Reinforcement Learning Enhanced LLMs: A Survey
Shuhe Wang♠ , Shengyu Zhang♣ , Jie Zhang⋆ , Runyi Hu▲ , Xiaoya Li♦
Tianwei Zhang▲ , Jiwei Li♣ , Fei Wu♣ , Guoyin Wang, Eduard Hovy♠

Abstract

arXiv:2412.10400v3 [cs.CL] 24 Feb 2025

Reinforcement learning (RL) enhanced large
language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Depsite the effectiveness in
improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies,
and optimization techniques. This complexity
poses challenges for researchers and practitioners in developing a systematic understanding of
RL-enhanced LLMs. Moreover, the absence of
a comprehensive survey summarizing existing
research on RL-enhanced LLMs has limited
progress in this domain, hindering further advancements.
In this work, we are going to make a systematic
review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to
consolidate and analyze the rapidly growing
research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of
RL; (2) introduce popular RL-enhanced LLMs;
(3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF)
and Reinforcement Learning from AI Feedback
(RLAIF); and (4) explore Direct Preference
Optimization (DPO), a set of methods that bypass the reward model to directly use human
preference data for aligning LLM outputs with
human expectations. We will also point out
current challenges and deficiencies of existing
methods and suggest some avenues for further
improvements. Project page of this work can
be found at our latest repo.
♠
The University of Melbourne, ♣ Zhejiang University,
CFAR and IHPC, A*STAR, Singapore, ▲ Nanyang Technological University, ♦ University of Washington
Email: shuhewang@student.unimelb.edu.au
Project page of this work can be found
at:
https://github.com/ShuheWang1998/
Reinforcement-Learning-Enhanced-LLMs-A-Survey
* The latest update was on Feb. 24, 2025 (Version 2).
⋆

1

Introduction

Recently, represented by DeepSeek-R1 (DeepSeekAI et al., 2025), reinforcement learning (RL) enhanced large language models (LLMs) (OpenAI,
2023, 2024a; Team et al., 2024b; GLM et al., 2024;
Adler et al., 2024; Yang et al., 2024a; AI et al.,
2024; Team et al., 2025), have demonstrated remarkable performance (Wang et al., 2023b; Wan
et al., 2023; Sun et al., 2023c,b; Giray, 2023;
Zhang, 2023; Long, 2023; Sun, 2023; Gao et al.,
2023; Paranjape et al., 2023; Sun et al., 2023a; Diao
et al., 2023; Wang et al., 2023a; Zhang et al., 2023b;
Sun et al., 2023d; Liu et al., 2024d; Yao et al., 2024;
Liu et al., 2024c; Lee et al., 2024; Kambhampati,
2024; Wang et al., 2024c), attracting widespread
attention from the research community. A key factor contributing to this success is the integration
of reinforcement learning techniques, which have
proven to be a powerful approach to enhancing
LLM capabilities.
The process of training LLMs using RL can be
divided into three main steps: (1) Reward Model
Training: before fine-tuning, a reward model (or
reward function) is trained to approximate human
preferences and evaluate different LLM outputs;
(2) Preference-Based Fine-Tuning: during each
fine-tuning iteration, the LLM generates multiple
responses for a given instruction, each of which
is scored using the trained reward model; and (3)
Policy Optimization: using RL optimization techniques, the model’s weights are updated based on
preference scores, improving response generation.
Incorporating reinforcement learning into LLMs allows models to adjust dynamically based on varied
preference scores, rather than being confined to a
single predetermined answer. This enables them to
produce well-structured, contextually appropriate
responses. Furthermore, RL enables direct training
on human preferences, enhancing the LLM’s ability to generate creative, high-quality outputs that

align more closely with human expectations.
Although reinforcement learning (RL) is highly
effective in enhancing LLM capabilities, its implementation remains highly complex, requiring
sophisticated algorithms, reward modeling strategies, and optimization techniques. This complexity
poses challenges for researchers and practitioners
in developing a systematic understanding of RLenhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research
on RL-enhanced LLMs has limited progress in this
domain, hindering further advancements and making it difficult to consolidate knowledge and drive
innovation in the field.
To address this issue, in this paper, we provide
a systematic review of both the challenges and
opportunities involved in reinforcement learningenhanced LLMs. Specifically, we aim to identify
the key obstacles in applying reinforcement learning to LLMs and offer a comprehensive analysis of
recent efforts to overcome them:
(1) At the reward model training level, we analyze the evolution of reward models, including
Reinforcement Learning from Human Feedback
(RLHF) and Reinforcement Learning from AI
Feedback (RLAIF). Our discussion covers their
effectiveness, limitations, evaluation methods, and
challenges, such as bias in human annotations, generation of out-of-distribution content, and issues
related to human interpretability.
(2) At preference fine-tuning level, we explore
approaches for aligning LLMs with human preferences, including direct preference optimization
(DPO) and its variations. Additionally, we examine
the impact of preference data collection, different
optimization functions, and strategies designed to
maintain safety throughout the alignment process.
By presenting a comprehensive overview of RLenhanced LLMs, this survey aims to bridge the
knowledge gap in the field, attempting to consolidate and analyze the rapidly growing research in
this field, helping researchers understand the current landscape, challenges, and advancements. The
rest of this survey is organized as follows:
• Section 2 presents the basics of reinforcement
learning (RL) along with key terminologies,
and outlines how the RL pipeline is adapted
for LLMs.
• Section 3 introduces popular and powerful
LLMs enhanced by reinforcement learning.

• Section 4 outlines the process of reinforcement learning from human feedback (RLHF),
a training method that integrates reinforcement learning with human feedback to align
LLMs with human values, preferences, and
expectations.
• Section 5 reviews research on reinforcement
learning from AI feedback (RLAIF), which
presents a promising alternative or complement to RLHF by utilizing AI systems to provide feedback on the outputs of the LLM being trained, offering advantages in scalability,
consistency, and cost-effectiveness.
• Section 6 provides an analysis of the challenges associated with RLHF and RLAIF.
• Section 7 discusses research on direct preference optimization (DPO), a series of methods
that bypasses the reward model and directly
utilizes human preference data to align LLM
outputs with human expectations.
• Section 8 summarizes the current challenges
and discusses opportunities for further improvement..

2

Basics: Reinforcement Learning for
LLMs

In this section, we first detail the basics of reinforcement learning (RL) along with key terminologies,
and then outline how the RL pipeline is adapted for
LLMs.
2.1

Basics of Reinforcement Learning

Reinforcement Learning (RL) is a key approach
in machine learning, focusing on how an agent
engages with its environment to maximize cumulative rewards. Unlike supervised learning, which
depends on labeled data, and unsupervised learning, which uncovers patterns in unlabeled data, RL
emphasizes learning through direct feedback via
trial and error. Below, we sequentially describe
basic definitions and general pipeline of RL.
2.1.1 Basic Definitions
Here, we use the training example in Figure 1 to illustrate the full process of RL. In this example, our
goal is to train a robot to move from the bottom-left
corner of a square to the top-right corner. Additionally, each grid cell has a reward score, and we aim
to maximize the robot’s total score. Before delving

Figure 1: An example of the full process of RL. Training Objective: The goal is to train a robot to navigate from
the bottom-left corner of a square to the top-right corner. Each grid cell is assigned a reward score, and the objective
is to maximize the robot’s overall score. General Pipeline of RL: The agent begins in an initial state s0 , and at
each time step t, it selects an action at based on its current state st . In response, the environment transitions to a
new state st+1 , and the agent receives a reward rt .

into the training process, we first introduce some
relevant terms:
• Agent: An agent is the entity we train to make
correct decisions. In this example, our goal is
to train the robot to make movement decisions,
so the robot is the agent.
• Environment: The environment is the external system that the agent interacts with.
For our example, as the trained robot (agent)
moves within the grid, the grid serves as the
environment.
• State: The state represents the agent’s position at each time t. For instance, at the beginning, at time t0 , the robot (agent) starts at
the bottom-left corner, so the state at time t0
is the bottom-left corner, represented by the
coordinates (0, 0).
• Action(s): Actions represent the possible
choices available to the agent within the environment at each time t. For example, at the
start, at time t0 , the robot (agent) can choose
to move right or up, making these two actions
available to the agent at t0 .
• Reward(s): Rewards are the signals or feedback provided by the environment to the agent
based on the action it takes at each time t. For
instance, at time t0 , the robot (agent) would
receive a reward of +5 points for moving right,
or a penalty of -1 point for moving up.
• Policy: A policy is a set of decision-making
strategies that helps the agent choose an action

at each time t. In practice, at time t0 , the policy represents a probability distribution that
directs the robot (agent) to move right or up
in order to maximize its cumulative rewards.
2.1.2 General Pipeline of RL
We have defined key terminologies used in RL,
and in this section, we will continue to detail the
general pipeline of RL.
As illustrated in Figure 1, the general reinforcement learning (RL) pipeline can be represented as
a Markov Decision Process (MDP). Formally, the
agent begins in an initial state s0 , and at each time
step t, it selects an action at based on its current
state st . In response, the environment transitions to
a new state st+1 , and the agent receives a reward
rt . This cycle continues, with the agent’s objective being to maximize the cumulative rewards it
accumulates over time.
Mapping into the specific example in Figure 1, at
the initial time t0 , the robot starts at the bottom-left
corner, denoted by the position (state) s0 . As time
progresses, at each time step t, the robot chooses an
action at (either moving up or moving right). This
action causes the robot to transition from its current
position st to a new position st+1 , while earning a
reward tt . This cycle of movement and reward collection continues until the robot reaches the desired
position (state) at the top-right corner, achieving
the goal of maximum cumulative rewards.
2.2

RL for LLMs

We have outlined the general framework of RL
above; now we will delve into the process of finetuning LLMs using RL. This approach aims to align

Figure 2: The framework of RL for LLMs proposed by Ouyang et al. (2022).

LLMs with desired behaviors, enhance their performance, and ensure that their outputs are both
effective and dependable.
In reinforcement learning (RL), there are six key
components:agent, environment, state, action, reward, and policy. To apply RL for fine-tuning
large language models (LLMs), the first step is
to map these components to the LLM framework.
LLMs are highly proficient at next-token prediction, where they take a sequence of tokens as input
and predict the next token based on the given context. From an RL perspective, we can view the
LLM itself as the policy. The current textual sequence represents the state, and based on this state,
the LLM generates an action—the next token. This
action updates the state, creating a new state that
incorporates the newly added token. After generating a complete textual sequence, a reward is
determined by assessing the quality of the LLM’s
output using a pre-trained reward model.
Figure 2 illustrates the specific RL framework
for LLMs as proposed by (Ouyang et al., 2022).
Ouyang et al. (2022) starts with an instructiontuned model trained through supervised learning,
enabling it to generate structured responses to human instructions. Then, Ouyang et al. (2022) applies the following two steps:
Step 1: Collect comparison data, and train
a reward model. Ouyang et al. (2022) collects

a dataset of comparisons between outputs of the
instruction-tuned model, where labelers indicate
which output they prefer for a given input. Then,
the collected dataset is used to train a reward model
(RM) to predict the human-preferred output.
Step 2: Optimize a policy against the reward
model using PPO. Ouyang et al. (2022) leverages
the output of the RM as a scalar reward, and finetunes the instruction-tuned model to optimize this
reward using the PPO algorithm (Schulman et al.,
2017).

3

Popular LLMs Enhanced by RL

Recent popular LLMs with strong capabilities almost all leverage reinforcement learning (RL) to
further enhance their performance during the posttraining process. The RL methods adopted by these
models can be typically divided into two main lines:
1. Traditional RL approaches, such as Reinforcement Learning from Human Feedback (RLHF)
and Reinforcement Learning from AI Feedback
(RLAIF). These methods require training a reward
model and involve a complex and often unstable
process, using algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017) to optimize the policy model. Models like InstructGPT
(Ouyang et al., 2022), GPT-4 (OpenAI, 2023), and
Claude 3 (Anthropic, 2024) follow this approach.
2. Simplified approaches, such as Direct Prefer-

RL Enhanced LLMs

Organization

# Params

RL Methods

671B-A31B

RL through CoT

Kimi-k1.5 (Team et al., 2025)

-

RL through CoT

o1 (OpenAI, 2024b)

-

RL through CoT

Hermes 3 (Teknium et al., 2024)

8B, 70B, 405B

DPO

Athene-70B (Nexusflow, 2024)

70B

RLHF

Starling-7B (Zhu et al., 2024)

7B

RLAIF, PPO

Gemma2 (Team et al., 2024b)

2B, 9B, 27B

RLHF

Qwen2 (Yang et al., 2024a)

(0.5-72)B, 57B-A14B

DPO

Llama 3 (Dubey et al., 2024)

8B, 70B, 405B

DPO

Nemotron-4 340B (Adler et al., 2024)

340B

DPO, RPO

ChatGLM (GLM et al., 2024)

6B, 9B

ChatGLM-RLHF

DeepSeek-V2 (Liu et al., 2024a)

236B-A21B

GRPO

Phi-3 (Abdin et al., 2024)

3.8B, 7B, 14B

DPO

Zephyr (HuggingFaceH4, 2024)

141B-A39B

ORPO

Reka (Team et al., 2024c)

7B, 21B

RLHF, PPO

Claude 3 (Anthropic, 2024)

-

RLAIF

InternLM2 (Cai et al., 2024)

1.8B, 7B, 20B

RLHF, PPO

Gemini (Team et al., 2023)

-

RLHF

GPT-4 (OpenAI, 2023)

-

RLHF, PPO, RBRM

1.3B, 6B, 175B

RLHF, PPO

DeepSeek-R1 (DeepSeek-AI et al., 2025)

Instruct-GPT (Ouyang et al., 2022)

Table 1: An overview of RL Enhanced LLMs. The format ‘141B-A39B’ refers to MoE models with 141B total and
39B active parameters.

ence Optimization (DPO) (Rafailov et al., 2024)
and Reward-aware Preference Optimization (RPO)
(Adler et al., 2024). These methods discard the
reward model, offering a stable, performant, and
computationally efficient solution. Models like
Llama 3 (Dubey et al., 2024), Qwen 2 (Yang et al.,
2024a), and Nemotron-4 340B (Adler et al., 2024)
follow this approach. In this section, we provide
a detailed description of each model, starting with
a brief overview of these RL enhanced LLMs and
followed by an explanation of how RL is applied in
their post-training process. An overview of these
RL Enhanced LLMs is shown in Tab 1.
3.1

DeepSeek-R1

DeepSeek-R1 (DeepSeek-AI et al., 2025), developed by DeepSeek, is a state-of-the-art reasoning
model that achieves performance comparable to
OpenAI’s o1 (OpenAI, 2024b) series models. This

work pioneers the application of pure reinforcement learning (RL) to enhance language model
reasoning, emphasizing self-evolution rather than
relying solely on supervised training data.
During the training stage, DeepSeek-R1 undergoes an alternating training process that integrates
supervised fine-tuning (SFT) (Zhang et al., 2023a)
and reinforcement learning (RL) across four key
stages: (1) Initial Cold Start SFT, begining with the
collection of thousands of high-quality, readabilityfocused long-CoT datasets to fine-tune DeepSeekV3-Base, establishing a strong foundation for subsequent RL training; (2) First Reasoning-Oriented
RL Stage, using the methodology named largescale reasoning-focused RL to further fine-tune
the model, and the resulting checkpoint is used to
generate additional SFT data for the next training
phase; (3) Second SFT Stage, refining the model
further by incorporating both reasoning and non-

reasoning data. For the reasoning data, it is generated using rejection sampling from the RL checkpoint of the previous stage. For the non-reasoning
data, the DeepSeek-V3 pipeline is used to integrate
data for writing, factual QA, self-cognition, and
translation, including parts of the DeepSeek-V3
SFT dataset; (4) Second RL Stage for All Scenarios, leverating the final RL stage to further align
the model with human preferences, improving its
helpfulness, harmlessness, and reasoning abilities.
DeepSeek-R1 is an open-source model, made
publicly available to the research community to
support further advancements in the field. The release process first introduces DeepSeek-R1-Zero, a
model derived from DeepSeek-V3-Base, trained using large-scale RL without supervised fine-tuning
(SFT). This initial model demonstrates remarkable
reasoning improvements, with its pass@1 score on
AIME 2024 increasing from 15.6% to 71.0%. With
majority voting, the score further rises to 86.7%,
matching OpenAI-o1-0912. To address issues such
as poor readability and language mixing, and to
further improve reasoning capabilities, the authors
introduce DeepSeek-R1. This enhanced model integrates a small amount of cold-start data and follows
a multi-stage training pipeline, achieving performance on par with OpenAI-o1-1217.
3.2

Kimi-k1.5

Kimi-k1.5 (Team et al., 2025), developed by Moonshot AI, is a multi-modal large language model
(LLM) that marks a major breakthrough in scaling
reinforcement learning (RL). The model introduces
a novel approach by emphasizing long-context scaling, extending the RL context window to 128k, and
refining policy optimization techniques.
For training, Kimi-k1.5 undergoes a four-stage
training process consisting of: (1) Pre-training,
(2) Vanilla supervised fine-tuning (SFT), (3) LongCoT supervised fine-tuning, and (4) Reinforcement
learning (RL). The key innovation lies in the reinforcement learning (RL) phase, where the authors
develop a high-quality RL prompt set to guide the
model toward robust reasoning while mitigating potential risks such as reward hacking and overfitting
to superficial patterns. This prompt set is designed
with three essential properties: (1) Diverse coverage, ensuring exposure to a broad range of reasoning challenges; (2) Balanced difficulty, providing
a mix of easy, moderate, and complex reasoning
tasks; and (3) Accurate evaluability, allowing precise measurement of reasoning performance.

While long-CoT (Chain-of-Thought) models exhibit strong reasoning performance, they tend to
consume more tokens at test time compared to standard short-CoT LLMs. To tackle this long2short
challenge, the authors propose four key methods to
transfer long-CoT reasoning capabilities to shortCoT models: (1) Model Merging, combining a
long-CoT model with a shorter model by averaging their weights; (2) Shortest Rejection Sampling,
generating multiple responses using the long-CoT
model and selecting the shortest correct response
for SFT training; (3) Direct Preference Optimization (DPO), constructing pairwise preference data,
where shorter correct solutions are treated as positive samples and longer solutions as negative samples, optimizing the model through DPO training;
and (4) Long2Short RL, implementing a two-phase
RL training approach, where the model first undergoes standard RL training, and then a length
penalty is applied, and the maximum response
length is reduced, encouraging more concise reasoning while maintaining performance.
Kimi-k1.5 delivers state-of-the-art reasoning performance across various benchmarks and modalities, rivaling OpenAI’s o1 (OpenAI, 2024b). Moreover, the introduction of long2short techniques significantly enhances short-CoT models, achieving
up to 550% improvement over existing models such
as GPT-4o (Hurst et al., 2024) and Claude 3.51 .
3.3

InstructGPT

InstructGPT (Ouyang et al., 2022) is a series of
language models fine-tuned from GPT-3 (Brown
et al., 2020) by OpenAI, using human feedback
to better align with human intent. The series includes models in three sizes: 1.3 B, 6 B, and 175
B parameters. The model is first fine-tuned using
supervised learning with prompts collected from
the OpenAI API or written by labelers and corresponding labeler demonstrations, then further refined using reinforcement learning from human
feedback (RLHF). Human evaluations reveal that
InstructGPT outputs are preferred over GPT-3. Notably, the 1.3B parameter InstructGPT model is
favored over the 175B GPT-3, despite having 100
times fewer parameters. Additionally, InstructGPT
demonstrates improved truthfulness and reduced
toxic outputs, with minimal performance trade-offs
on public NLP datasets.
1

https://www.anthropic.com/news/
claude-3-5-sonnet

Before applying reinforcement learning (RL),
the authors train a 6B reward model (RM) initialized from the supervised fine-tuned (SFT) model,
with the final unembedding layer removed. This
RM is trained using comparison data ranked by labelers. During the RL phase, they fine-tune the SFT
model to optimize the scalar reward output from
the RM using the PPO algorithm (Schulman et al.,
2017). To address performance regressions on public NLP datasets, they experiment with mixing pretraining gradients with PPO gradients, resulting in
models known as PPO-ptx.
3.4

GPT-4

GPT-4 (OpenAI, 2023), developed by OpenAI, is
a large multimodal model that can process both
image and text inputs to produce text outputs. It
excels at understanding and generating natural language, particularly in complex and nuanced scenarios. Evaluations show that GPT-4 performs exceptionally well on a range of human-designed exams,
often surpassing the majority of human test takers.
Additionally, it outperforms earlier large language
models and most state-of-the-art systems, which
frequently rely on benchmark-specific training or
hand-engineered solutions.
GPT-4 leverages RLHF methods, as outlined
in InstructGPT (Ouyang et al., 2022) which we
have describe in Sec 3.3, in the post-training alignment stage. To steer the models more effectively
towards appropriate refusals at a finer level, the
authors further use a zero-shot GPT-4 classifier as
the rule-based reward model (RBRM). This RBRM
provides an additional reward signal to the GPT-4
policy model during PPO fine-tuning on a subset
of training prompts. The RBRM takes a prompt
(optional), the policy model’s output, and a humanwritten rubric (e.g., a set of rules in multiple-choice
style) as input, then classifies the output according to the rubric. Through this approach, GPT-4
is rewarded for refusing harmful content and for
appropriately responding to known-safe prompts.
3.5

Gemini

Gemini (Team et al., 2023) represents a family
of advanced multimodal models developed by
Google, distinguished by their impressive capabilities. The initial version, Gemini 1.0, comes in three
sizes—Ultra, Pro, and Nano—ranging from large
to small in terms of performance. Each size is tailored to address specific computational constraints
and application needs. Notably, Gemini Ultra, the

most powerful variant, achieves state-of-the-art results in 30 out of 32 benchmarks and is the first
model to attain human expert-level performance on
MMLU (Hendrycks et al., 2020), while setting new
records across all 20 multimodal benchmarks.
Gemini implements a post-training process that
utilizes an optimized feedback loop, collecting
human-AI interactions to drive continuous improvement in key performance areas. During the posttraining’s RLHF phase, an iterative approach is
adopted wherein reinforcement learning (RL) incrementally enhances the reward model (RM). Concurrently, the RM undergoes continuous refinement
through systematic evaluation and data collection.
This dynamic interplay promotes ongoing advancement in both RL and RM, leading to progressively
improved performance over time.
3.6

InternLM2

InternLM2 (Cai et al., 2024) is an open-source series of large language models developed by Shanghai AI Laboratory, available in three sizes: 1.8B,
7B, and 20B. The model demonstrates superior
performance across six dimensions and 30 benchmarks, including long-context modeling and openended subjective evaluations, thanks to innovative
pre-training and optimization techniques.
To further enhance alignment, InternLM2 employs a novel strategy called Conditional Online
Reinforcement Learning from Human Feedback
(COOL RLHF) with the use of PPO. This approach
addresses two key challenges. The first is preference conflict, where it is difficult to satisfy two
preferences, such as helpfulness and harmlessness,
simultaneously. The second challenge is reward
hacking, which becomes more problematic as the
model’s scale increases and its policy becomes
more powerful. COOL RLHF introduces a Conditional Reward mechanism that reconciles diverse
preferences by allowing a single reward model to
dynamically adjust its focus based on specific conditional prompts, effectively integrating multiple
preferences. Additionally, COOL RLHF incorporates a multi-round Online RLHF strategy with two
distinct pathways: a Fast Path for immediate, targeted improvements and a Slow Path for long-term,
comprehensive refinement of the reward model.
This approach enables the model to quickly adapt
to new human feedback while reducing the risk of
reward hacking.

3.7

Claude 3

Claude 3 (Anthropic, 2024) is a family of large multimodal models developed by Anthropic, which
demonstrates strong performance across benchmark evaluations. It comprises three models with
varying abilities and speeds: the largest, Claude
3 Opus; the mid-sized, Claude 3 Sonnet; and the
smallest, Claude 3 Haiku. The Claude 3 models
show strong benchmark performance, setting new
standards in reasoning, math, and coding. Claude
3 Opus achieves state-of-the-art results on evaluations such as GPQA (Rein et al., 2023), MMLU
(Hendrycks et al., 2020), and MMMU (Yue et al.,
2024). Claude 3 Haiku matches or surpasses
Claude 2 in most text tasks, while Sonnet and Opus
perform significantly better.
The authors use a technique called Constitutional
AI (Bai et al., 2022) to align Claude 3 with human
values during reinforcement learning (RL). In the
RL stage, Constitutional AI follows a process similar to RLHF, but instead of human preferences
for harmlessness, it uses AI feedback, known as
RLAIF. Specifically, it distills language model interpretations of a set of rules and principles into a
hybrid human/AI preference model (PM), using human labels for helpfulness and AI labels for harmlessness. Afterwards, they fine-tune the supervised
learning model using RL with this PM, resulting in
a policy trained by RLAIF.
3.8

Zephyr 141B-A39B

Zephyr 141B-A39B (HuggingFaceH4, 2024) is the
newest addition to the Zephyr (Tunstall et al., 2023)
series of language models, developed through a collaboration between Argilla, KAIST, and Hugging
Face. This model is a Mixture of Experts (MoE)
with a total of 141 billion parameters, 39 billion of
which are active, fine-tuned from Mixtral-8x22Bv0.1 (Mistral AI, 2024).
Zephyr 141B-A39B employs a novel alignment
algorithm known as Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024). ORPO is
a straightforward, unified alignment approach that
discourages the model from adopting undesired
generation styles during supervised fine-tuning.
Notably, ORPO does not require an SFT warmup phase, a reward model, or a reference model,
making it highly resource-efficient. The method
works by adding an odds ratio-based penalty to
the standard SFT negative log-likelihood loss, enabling the model to distinguish between preferred

and non-preferred response styles.
3.9

DeepSeek-V2

DeepSeek-V2 (Liu et al., 2024a), developed by
DeepSeek-AI, is a powerful Mixture-of-Experts
(MoE) language model designed for economical
training and efficient inference. It features innovative architectures such as Multi-head Latent Attention (MLA) and DeepSeekMoE. With 236 billion
total parameters, of which 21 billion are activated
per token, it supports a context length of up to 128K
tokens. The model is pre-trained on a high-quality,
multi-source corpus of 8.1 trillion tokens. Evaluations show that DeepSeek-V2, along with its chat
versions, maintains top-tier performance among
open-source models, despite having only 21 billion
activated parameters.
DeepSeek-V2 is optimized using Group Relative
Policy Optimization (GRPO) (Shao et al., 2024)
during the RL phase to reduce training costs. Unlike traditional RL methods that use a critic model
of similar size to the policy model, which increases
training expenses, GRPO foregoes the critic model
and estimates the baseline from scores computed on
a group of outputs for the same question. Additionally, a two-stage RL training strategy is employed:
the first stage focuses on reasoning alignment, and
the second on human preference alignment, as the
authors find these stages exhibit distinct characteristics.
3.10

ChatGLM

ChatGLM (GLM et al., 2024), developed by Zhipu
AI, represents an evolving series of large language
models. The latest version in this series is GLM-4,
which includes variants such as GLM-4, GLM-4Air, and GLM-4-9B. These models are pre-trained
on a dataset of over 10 trillion tokens, predominantly in Chinese and English, and are subsequently post-trained through a combination of supervised fine-tuning (SFT) and RLHF to achieve
advanced alignment quality. Evaluation results indicate that GLM-4 rivals or even surpasses GPT4 (OpenAI, 2023) on general benchmarks like
MMLU, and demonstrates superior performance in
Chinese-specific alignments as measured by AlignBench (Liu et al., 2023b).
The reinforcement learning phase involves the
ChatGLM-RLHF (Hou et al., 2024) pipeline,
which enhances alignment with human preferences.
This pipeline comprises three primary components:
gathering human preference data, training a re-

ward model, and optimizing policy models. To
support large-scale training, ChatGLM-RLHF includes methods to reduce reward variance for stable
training, leverages model parallelism with fused
gradient descent, and applies regularization constraints to prevent catastrophic forgetting in large
language models. Experimental results confirm
that ChatGLM-RLHF yields substantial improvements in alignment-focused tasks compared to the
supervised fine-tuned version of ChatGLM.
3.11

Nemotron-4 340B

Nemotron-4 340B (Adler et al., 2024) is a family of models released by NVIDIA, consisting
of Nemotron-4-340B-Base, Nemotron-4-340BInstruct, and Nemotron-4-340B-Reward. The
Nemotron-4-340B-Base model is trained on 9 trillion tokens from a high-quality dataset. In the
alignment process to develop Nemotron-4-340BInstruct, over 98% of the data used is synthetically
generated by the model. Evaluations demonstrate
that these models perform competitively with openaccess models across a broad range of evaluation
benchmarks.
During the preference fine-tuning phase, both
DPO (Rafailov et al., 2024) and a new alignment
algorithm, Reward-aware Preference Optimization
(RPO), are employed to improve the model through
multiple iterations. RPO addresses a limitation
in DPO, where the quality difference between selected and rejected responses is not considered,
leading to overfitting and the forgetting of valuable
responses. RPO uses an implicit reward from the
policy network to approximate this gap, enabling
the model to better learn from and retain superior
feedback.
3.12

Llama 3

Llama 3 (Dubey et al., 2024), developed by Meta,
is a collection of open-source foundational language models available in sizes of 8 billion, 70
billion, and 405 billion parameters. It is trained on
a significantly larger corpus consisting of approximately 15 trillion multilingual tokens, a notable
increase compared to the 1.8 trillion tokens used for
Llama 2 (Touvron et al., 2023). Extensive empirical evaluations demonstrate that Llama 3 achieves
performance comparable to leading models, such
as GPT-4 (OpenAI, 2023), across a diverse range
of tasks.
The post-training process for aligning Llama 3
with human feedback involves six rounds of iter-

ative refinement. Each round includes supervised
fine-tuning (SFT) followed by DPO, with the final model being an average of the outputs from all
rounds. For each round, a reward model (RM) is
trained on newly collected preference annotation
data, targeting a wide range of capabilities built
upon the pre-trained checkpoint. After SFT, DPO
is applied to further optimize the SFT models, using recent preference data batches obtained from
the best-performing models of previous rounds. To
enhance the stability of DPO training, two key adjustments are implemented: masking out formatting tokens in the DPO loss and introducing regularization via an NLL (negative log-likelihood)
loss.
3.13

Qwen2

Qwen2 (Yang et al., 2024a), developed by Alibaba,
is a series of large language models ranging from
0.5 billion to 72 billion parameters in dense configurations, as well as a Mixture-of-Experts variant with 57 billion parameters, of which 14 billion are activated per token. It is pre-trained on a
high-quality, large-scale dataset containing over 7
trillion tokens, covering a wide array of domains
and languages. Extensive evaluations show that
Qwen2 outperforms most prior open-weight models, including its predecessor Qwen1.5, and delivers competitive results across a range of benchmarks, including language understanding, generation, multilingual proficiency, coding, mathematics,
and reasoning.
The preference fine-tuning process for Qwen2
consists of two main stages: offline and online
learning. In the offline stage, Qwen2 is optimized using DPO, which aims to maximize the
likelihood difference between two responses to the
same prompt, based on a pre-compiled preference
dataset. In the online stage, the model improves
continuously in real-time by utilizing preference
pairs selected by the reward model from multiple
responses generated by the current policy model.
Additionally, the Online Merging Optimizer (Lu
et al., 2024) is employed to minimize alignment
costs.
3.14

Gemma 2

Gemma 2 (Team et al., 2024b), developed by
Google, is the latest addition to the Gemma family of lightweight, state-of-the-art open models,
with sizes ranging from 2 billion to 27 billion parameters. The model incorporates several well-

established modifications to the Transformer architecture, including interleaving local-global attentions (Beltagy et al., 2020) and group-query attention (Ainslie et al., 2023). Experiments demonstrate that these models deliver the best performance for their size and even provide competitive
alternatives to models 2-3 times larger.
Similar to Gemma 1.1 (Team et al., 2024a), during the post-training RLHF phase, the authors use
a high-capacity model as an automatic rater to
tune hyperparameters and mitigate reward hacking
(Amodei et al., 2016; Skalse et al., 2022). However,
unlike Gemma 1.1, they employ a reward model
that is an order of magnitude larger than the policy
model. This reward model is specifically designed
to focus on conversational capabilities, with an emphasis on multi-turn interactions.
3.15

Starling-7B

Starling-7B (Zhu et al., 2024) is a strong 7-billionparameter chat model developed by UC Berkeley,
focused on alignment with human preferences for
helpfulness and harmlessness. It is fine-tuned from
Openchat-3.5 (Wang et al., 2024a) using RLAIF
on a high-quality preference dataset called Nectar,
which comprises 3.8 million pairwise comparisons
generated by prompting GPT-4 to rank responses.
As a result, the model’s score on MT-Bench improves from 7.81 to 8.09, its score on AlpacaEval
increases from 88.51% to 91.99%, and its human
evaluation ELO on Chatbot Arena (Chiang et al.,
2024) rises from 1072 to 1087.
The authors introduce several improvements to
the PPO algorithm during the RLAIF process to enhance training stability and robustness. First, they
introduce a constant positive reward for length control to prevent excessive verbosity. This adjustment
helps address the issue where a highly negative reward from the reward model during the early stages
can cause the policy model to become overly verbose after only a few gradient updates. Second,
they pretrain the critic model to reduce early performance drops due to a randomly initialized critic.
Third, they conduct full parameter tuning on both
the actor and critic models, as opposed to tuning
only the top four layers, to maximize performance
improvements during the reinforcement learning
stage.
3.16

o1

OpenAI’s o1 (OpenAI, 2024b) is a newly developed large language model optimized for complex

reasoning, utilizing reinforcement learning for its
training. Before producing responses, o1 engages
in an extensive internal thought process, enabling
it to excel across various reasoning tasks. The
model significantly surpasses GPT-4o (OpenAI,
2024a) in many challenging tasks: ranks in the 89th
percentile on Codeforces for competitive programming, places among the top 500 participants in the
AIME for mathematics, and surpasses PhD-level
accuracy in scientific benchmarks such as GPQA.
The training of o1 involves a large-scale reinforcement learning algorithm that emphasizes productive thinking through a detailed chain of thought
(CoT) (Wei et al., 2023), implemented with high
data efficiency. To preserve the model’s unfiltered
reasoning ability, no policy compliance or user preference training is applied to its internal thought processes, which also provides a unique opportunity to
understand the model’s raw thought process. This
approach allows o1 to refine its strategies, correct
errors, and deconstruct complex problems during
training. Notably, the model’s performance improves with increased training compute and with
more extensive test-time computation.
3.17

Others

Reka Core, Flash, and Edge: Team et al.
(2024c) are powerful multimodal language models
developed from scratch by Reka. Reka Edge and
Reka Flash are dense models with 7B and 21B parameters, respectively, outperforming many larger
models and offering exceptional performance for
their compute class. The flagship model, Reka
Core, competes with leading models like GPT-4v,
Gemini, and Claude 3 in both automated and blind
human evaluations. During post-training, following supervised fine-tuning, Reka models undergo
multiple rounds of RLHF using PPO to enhance
alignment further.
Phi-3: Abdin et al. (2024) is a series of language
models introduced by Microsoft, comprising phi-3mini, phi-3-small, and phi-3-medium. Remarkably,
the smallest model, phi-3-mini, is trained on 3.3
trillion tokens yet contains only 3.8 billion parameters, making it compact enough for deployment on
a mobile device. Despite its relatively small size,
phi-3-mini demonstrates performance comparable
to larger models like Mixtral 8x7B and GPT-3.5,
achieving 69% on MMLU and a score of 8.38 on
MT-bench in both academic benchmarks and internal testing. During post-training, the authors

Original Data Distribution (378K)

Filtered Data Distribution (80K)

2.

36.2%
(29682)

0)

03

25.9%
(98000)

(2

25.9%
(98000)

5%

employ DPO to guide phi-3 away from undesired
behavior by treating those outputs as “rejected” responses.

1.9% (7221)
8.8%
33.9%
4.92.3% (850
Athene-70B: Nexusflow (2024) is a powerful
4)
%
(7221)
(27785)
(18
548
10.4%
25.9%
)
13.2%
chat model fine-tuned from Llama-3-70B (Dubey
(8504)
(98000)
(50000)
8.2%
(6709)
et al., 2024), developed by Nexusflow. It achieves
an impressive Arena-Hard-Auto score of 77.8%,
Figure 1 | The composition chart of the Skywork-Reward preference data selections before and
placing it close to leading proprietary models like
Figure
3:selection
The composition
of the Skywork-Reward. The
after applying
data
and filtering operations.
GPT-4o (79.2%) and Claude-3.5-Sonnet (79.3%). figure is copied from Liu et al. (2024b).
used in our data mixture (section 3.1), the data selection and filtering techniques employed
This marks a significant leap from its predecessor,
to optimize its composition (section 3.2), and the training objective that guides the reward
model’s learning process (section 3.3). Our methodology aims to enhance the effectiveness
Llama-3-70B-Instruct, which scored 46.6%. This
ingmodeling
responses
based ontransparency
factors such
as quality
and on solely
of reward
while maintaining
and accessibility
by focusing
progress is attributed to Nexusflow’s targeted postpublicly available preference data. We visualize the composition chart of the Skywork-Reward
relevance.
This
feedback
is
then
used
to
train
a
repreference data selections in fig. 1.
training approach, which enhances the model’s perward model that predicts the quality of the outputs
formance. Specifically, Nexusflow curates high3.1. Dataset Mixture
and serves as the reward function in the RL process;
quality preference data based on internal benchExisting research (Dong et al., 2024; Jiang et al., 2023; Touvron et al., 2023) frequently leverages a
and (2) Preference Optimization Using Human
mixture of preference datasets from multiple sources to train reward models. These datasets
mark evaluations covering instruction following,
typicallyFeedback,
contain between
several hundred
thousand
to over amodel
million samples.
where
the trained
reward
guidesFor instance,
Llama 2 (Touvron et al., 2023) employs approximately 1.5 million publicly available preference
coding, creative writing, and multilingual tasks.
theaugmented
optimization
of theinternally
LLM’s
outputs
to maximize
data points,
with 1.4 million
generated
samples,
for reward model training.
This data is then used for targeted RLHF, resultA substantial portion of the public data originates from StackExchange, with the remainder
predicted
rewards,
aligning
the
LLM’s
behavior
capturing attributes such as helpfulness, harmlessness, and general human preferences. In
ing in substantial performance gains over Llama-3a similar
vein, human
Dong et al.preferences.
(2024) assemble aBelow,
more diverse
by aggregating samples
with
wedataset
will illustrate
from eight distinct sources, producing a collection of around 700K preference pairs. Notably,
70B-Instruct.
these
two
components
via
recent
research
studies.
approximately 90% f the responses in this dataset are generated by various
LLMs, with more

than half of the annotations sourced from GPT-3.5 and GPT-4. This growing reliance on LLM-

Hermes 3: Teknium et al. (2024) is a series of
generated data underscores the increasing trend toward using automated systems for large-scale
4.1 Collecting Human Feedback to Train
preference labeling in reward model development. We present the statistics of the Skywork
neutrally-aligned generalist instruction and toolReward Preference
data collections
in table 1.
Reward
Model
use models with advanced reasoning and creative
Skywork-Reward
(Liu et al.,
SkyworkA lightweight
yet high-quality data composition
Our2024b).
objective is to construct
a more lightweight
capabilities, developed by Nous Research. It is
preference
data
collection
that
not
only
reduces
the
overall
data
requirements
but also targets imporReward
is
a
carefully
designed
dataset
containfinetuned from Llama 3.1 (Dubey et al., 2024) in
tant abilities and domains that RLHF seeks to optimize, such as math and code. Additionally, we
ing 80,000
high-quality
preference
pairs,
curated
focus exclusively
on publicly
available data to ensure
transparency,
reproducibility,
and to enable
8B, 70B, and 405B variants and the largest model,
broader adoption of our methodologies without reliance on proprietary or internal datasets.
through effective data selection and filtering strateThis strategy has resulted in the creation of the following dataset mixture, which we introduce
Hermes 3 405B, sets the state-of-the-art perforbelow with
a brief
overview
of each
gies.
As
shown
in included
Figuredataset.
3, the original dataset,
mance among open-weight models across several
with
378,000
preference
pairs,
is significantly
redataset comprising only 10K
public benchmarks. Hermes is trained on diverse • HelpSteer2 (Wang et al., 2024e) is a compact preference
fined into a compact, high-quality dataset of 80,000
synthetic reasoning tasks and creative applications
4
pairs. Despite being significantly smaller than
such as role playing and writing. It is designed to
precisely and neutrally follow system and instruc- existing datasets, it achieves exceptional quality
through rigorous cleaning, consistency checks,
tion prompts, unlike many commercial models that
may decline instructions for moral reasons. To fur- model-based scoring to filter out low-quality samples, and manual reviews. Covering a diverse
ther align Hermes, the authors leverage DPO and
range of tasks such as instruction following, code
train a LoRA (Hu et al., 2021) adapter instead of
fine-tuning the entire model, significantly reduc- generation, and multilingual handling, SkyworkReward serves as the foundation for models like
ing GPU memory usage for both the reference and
Skywork-Reward-Gemma-27B, which excel on
trained models.
benchmarks2 . By enabling language models to
better understand human preferences, Skywork4 RLHF: Reinforcement Learning from
Reward helps LLMs become more accurate and
Human Feedback
useful in real-world applications.
Reinforcement learning from human feedback
(RLHF) is a training approach that combines re- TÜLU-V2-mix (Ivison et al., 2023). TÜLU-V2mix is designed to enhance instruction-following
inforcement learning (RL) with human feedback to
capabilities in large language models, offering a
align LLMs with human values, preferences, and
expectations. RLHF consists of two main compo- diverse dataset that improves the model’s generalization and execution abilities across multi-domain
nents: (1) Collecting Human Feedback to Train
tasks. It covers a wide range of tasks, including
Reward Model, where human evaluators provide
2
feedback on the LLM’s outputs by scoring or rankhttps://huggingface.co/spaces/allenai/reward-bench

question answering, code generation, translation,
and multi-turn conversations, with a strong emphasis on multilingual adaptability and handling
complex real-world scenarios. Skywork-Reward,
on the other hand, is designed to align models with
human preferences using preference pairs, helping
models learn to generate user-preferred responses,
such as fluent and coherent text. While TÜLU-V2mix excels in generalization across a wide range of
tasks, Skywork-Reward specializes in optimizing
user-centric outputs. Together, they address complementary goals for advancing language model
capabilities.

4.2

Preference Optimization Using Human
Feedback

Once the reward model is trained, it is used to
guide the fine-tuning of the original LLM through
reinforcement learning. The main objective is to
improve the LLM’s behavior based on the predicted
rewards, making it more likely to generate outputs
that align with human preferences. Recent research
(Ouyang et al., 2022; Yuan et al., 2023; Dong et al.,
2024; Ahmadian et al., 2024) has shown that this
process can be broken down into two key steps:
(1) Rewarding: In this step, the LLM generates
multiple outputs in response to a given instruction.
Each output is then passed through the trained reward model, which assigns a scalar score that approximates human preferences.
(2) Policy Optimization: In this step, the LLM is
fine-tuned by adjusting its parameters to maximize
the predicted reward, using the Proximal Policy Optimization (PPO) (Schulman et al., 2017) or Trust
Region Policy Optimization (TRPO) (Schulman,
2015) algorithm.
These two steps—rewarding and policy optimization—can be iterated, meaning that the process of
generating outputs, rewarding them with the trained
reward model, and fine-tuning the LLM to maximize rewards can be repeated multiple times. With
each iteration, the LLM’s performance improves
as it refines its behavior to better align with human
preferences. This iterative cycle allows the LLM
to continuously adapt and optimize its responses,
ultimately leading to more effective and aligned
outputs.

5

RLAIF: Reinforcement Learning from
AI Feedback

Reinforcement learning from AI feedback (RLAIF)
serves as a promising alternative or supplement to
RLHF that leverages AI systems—often more powerful or specialized LLMs (e.g., GPT-4 (OpenAI,
2024a))—to provide feedback on the outputs of
the LLM being trained. This approach provides
benefits such as scalability, consistency, and cost
efficiency while minimizing reliance on human
evaluators. Below, we explore several methods
for substituting human feedback with AI feedback
in reinforcement learning, highlighting approaches:
(1) Distilling AI Feedback to Train Reward Model,
(2) Prompting LLMs As a Reward Function, and
(3) Self-Rewarding.
5.1

Distilling AI Feedback to Train Reward
Model

Beyond manually collected data, distilling datasets
from pre-trained LLMs presents an efficient alternative. By leveraging the outputs of powerful LLMs
like GPT-4, researchers can build a bridge between
manual curation and autonomous evaluation.
UltraFeedback (Cui et al., 2023). UltraFeedback is a large-scale AI feedback dataset aimed
at improving the performance and alignment of
large language models (LLMs). It includes over 1
million high-quality GPT-4 feedback annotations
across 250,000 user-assistant interactions, focusing
on key dimensions like instruction adherence, accuracy, honesty, and usefulness. The dataset was created by collecting 60,000 diverse instructions, generating responses using 17 different models, and
leveraging GPT-4 for detailed critiques and scoring, wherein chain-of-thought reasoning is used to
reduce bias.
Magpie. Xu et al. (2024b) introduce a selfsynthesis method that leverages the autoregressive
nature of aligned LLMs. By utilizing predefined
templates as prompts, the model autonomously generates user queries and corresponding responses,
eliminating the need for manual intervention or
initial seed questions. Specifically, as shown in Figure 4, aligned LLMs (e.g., Llama-3-Instruct model)
is employed to synthesize 4 million instructionresponse pairs, subsequently filtering the dataset to
retain 300,000 high-quality pairs. These pairs were
then used to fine-tune the Llama-3-8B-Base model.
Remarkably, the fine-tuned model achieved per-

30%

AlpacaEval 2

Step 1

<|start_header_id|>user
<|end_header_id|>

Step 2

<|start_header_id|>user
<|end_header_id|>

What materials should I
use to build a nest?

Response
Building a nest! That’s a
wonderful project! ……

What materials should I
use to build a nest?
<|start_header_id|>
assistant<|end_header_id|>

LLM

Instruction: What materials
should I use to build a nest?
Response: Building a nest!
MAGPIE That’s a wonderful project!
……

Length Control Win Rate

Instruction

(Length Control)

25.08
25%
22.92
22.66
20%

SFT + DPO
SFT + RLHF

18.36

SFT Only

14.62

15%

10.90
10%

9.94 9.91 9.73

5%

“Other birds collect twigs for their nests. Magpies acquire jewels for theirs.”

Filters

SFT

ix
T
k
ct
es
r
ct
at
o
Pr tru e-Ai dbac ldCh stru erm V2 M reGP
i
a
e Wi
ie- Ins
nH ulu
l In
Sh
gp a-3- agp traFe
T
vo Ope
a
E
l
m
M
M
Ll a

U

Figure
figure
illustrates the
of self-synthesizing
LLMs
Figure1:4:This
Magpie
self-synthesizes
dataprocess
from aligned
LLMs. The figureinstruction
is borroweddata
fromfrom
Xu etaligned
al. (2024b).
(e.g., Llama-3-8B-Instruct) to create a high-quality instruction dataset. In Step 1, we input only the
pre-query template into the aligned LLM and generate an instruction along with its response using
auto-regressive
generation.
In StepLlama-3-8B2, we use a combination
a post-query
template
another
formance
comparable
to the official
as a robust of
resource
for improving
the and
fairness
and
pre-query
template
to
wrap
the
instruction
from
Step
1,
prompting
the
LLM
to
generate
the
query
Instruct model, which had undergone training on
reliability of reward models, with its data openly
formillion
the second
turn.through
This completes
construction
of the instruction
dataset.
M AGPIE efficiently
10
examples
supervisedthe
fine-tuning
accessible
for research
and development.
generates
diverse
and
high-quality
instruction
data.
Our
experimental
results
show that M AGPIE
and reinforcement learning with human feedback.
outperforms other public datasets for aligning Llama-3-8B-base.
5.2 Prompting LLMs As a Reward Function
Besides, models fine-tuned with Magpie excelled
on alignment benchmarks such as AlpacaEval, sur- As reward model training becomes more sophis[14, 26,models
64, 65,trained
66], which
is both
and labor-intensive
[37]. In contrast,
the second
ticated,
a natural progression
is to employ
LLMs
passing
on other
opentime-consuming
datasets and
type
of
method
uses
LLMs
to
produce
synthetic
instructions
[16,
31,
46,
47,
53,
55,
58,
59].
Although
themselves as evaluators in the loop of reinforcepreference optimization methods.
these methods reduce human effort, its success heavily
depends
on prompt engineering and the careful
ment
learning.
HelpSteer2
et al.,questions.
2024d). The
HelpSteer2
selection of(Wang
initial seed
diversityis of synthetic data tends to decrease as the dataset size
with LLMs
(ELLM)
Rewards
(Du
an
efficient,
open-source
grows.
Despite
ongoing preference
efforts, the dataset
scalablecomcreationExploring
of high-quality
and diverse
instruction
datasets
continues
to be challenging
problem. samples, et al., 2023). ELLM is a method that integrates
prising
approximately
10,000 comparison
LLMs
reinforcement
learning
enhance
designed
to
train
high-performance
reward
mod- at
Is it possible to synthesize high-quality
instructions
scalewith
by directly
extracting
data(RL)
fromtoadvanced
exploration
during three
the pretraining
phase. Figure
6
els.
TheLLMs
dataset
is built using
responses
aligned
themselves?
A typical
input generto an aligned
LLM contains
key components:
the preshowcases
the
overall
pipeline:
the
agent’s
curated
by
various
models
(including
GPT-3.5,
Claude,
query template, the query, and the post-query template. For instance, an input to Llama-2-chat could
rent statetemplate
is transformed
into a is
natural
language
and
multi-dimensional
be “others)
[INST]and
Hi!features
[/INST]
”, where [INST] annois the pre-query
and [/INST]
the post-query
description,
is input
into
LLM.the
The
LLM
template.
templates
are predefined
the creators
of thewhich
aligned
LLMs
tothe
ensure
correct
tations
suchThese
as fluency,
relevance,
creativity,byand
prompting
of thepairs
models.
We observe
when we
input exploration
the pre-query
template
aligned
thenonly
generates
goals
based ontothis
state
safety.
Preference
are crafted
based onthat
human
LLMs
such
as
Llama-3-Instruct,
they
self-synthesize
a
user
query
due
to
their
auto-regressive
nature.
description, such as specific actions or target locaor automated evaluations, enabling fine-grained
Our preliminary
experiments
indicate that
these random
userRL
queries
are of high
qualitythese
and goals,
great
tions. The
agent attempts
to achieve
alignment
for reward
models. Through
rigorous
diversity,
suggesting
that
the
abilities
learned
during
the
alignment
process
are
effectively
utilized.
data cleaning and optimization, HelpSteer2 deliv- and rewards are provided by the environment upon
goalmethod
completion.
This approach
improves
exploers
high-quality
in adeveloped
compact format.
It
Based
on theseannotations
findings, we
a self-synthesis
to construct
high-quality
instruction
scale,the
named
M AGPIE
(as illustrated
1). Unlikeby
existing
approach
ration efficiency
guidingmethods,
the agentour
toward
areas
isdatasets
releasedatunder
CC-BY-4.0
license,
fostering in Figure
does
not
rely
on
prompt
engineering
or
seed
questions.
Instead,
it
directly
constructs
instruction
of
the
state
space
that
are
likely
to
be
valuable,
the accessibility.
data by prompting aligned LLMs with a pre-querywithout
template
for sampling
instructions.
WeELLM
applied
requiring
pre-designed
rewards.
is
OffsetBias
al., 2024). OffsetBias
a
this method(Park
to theetLlama-3-8B-Instruct
and isLlama-3-70B-Instruct
models,
creating
two
instruction
particularly useful in sparse-reward environments.
meticulously
designed
aimed-Pro,
at mitigatdatasets: M AGPIE
-Air dataset
and M AGPIE
respectively.
Compared to traditional methods, ELLM signifiing biases in reward models, constructed using recantlyusing
improves
exploration
covering
Our M AGPIE-Air and M AGPIE-Pro datasets were created
206 and
614 GPUefficiency,
hours, respectively,
sponses generated by diverse models, including
without requiring any human intervention or API more
access
to productionbehaviors
LLMs like
Addicommon-sense
andGPT-4.
providing
betGPT-3.5,
Claude,two
andmulti-turn
open-source
models datasets, M AGPIE-Air-MT and M AGPIE-Pro-MT,
tionally, GPT-4,
we generated
instruction
ter initialization for downstream tasks.
like
Llama
2. As
shown inof
Figure
5, OffsetBias
which
contain
sequences
multi-turn
instructions and responses. The statistics and advantages
systematically
addresses
six identified
types, ones
Reward
Design with Language
Models
(RDLM).
of our instruction
datasets
comparedbias
to existing
are summarized
in Table 1.
We perform
a
namely,
content,
style,
informativeness,
safety,
creKwon
et
al.
(2023)
leverage
a
LLM
like
GPT-3
comprehensive analysis of the generated data, allowing practitioners to filter and select data instances
ativity,
and datasets
length. for
Based
on this,according
comparison
to particular
simplify reward
from these
fine-tuning
to their
needs. function design in reinforcesamples
are
generated
through
attribute-controlled
ment
learning
by allowing
users
define desired
To compare M AGPIE data with other public instruction datasets (e.g.,
ShareGPT
[10],toWildChat
[64],
prompts
and
multi-model
outputs.
These
samples
behaviors
through
natural
language
descriptions.
Evol Instruct [58], UltraChat [16], OpenHermes [49], Tulu V2 Mix [24]) and various preference
are
annotated
with with
multi-dimensional
scores
Specifically,
users provide a task
description
or
tuning
strategies
UltraFeedback
[13],and
we fine-tune
the Llama-3-8B-Base
model
with each
preference
labels
to
highlight
or
neutralize
biases,
a
few
examples,
and
the
LLM
generates
reward
dataset and assess the performance of the resultant models on LLM alignment benchmarks such as
enabling
fine-grained
alignment. OffsetBias
signals
evaluating
the agent’s
behavior
against
AlpacaEval
2 [33], Arena-Hard
[32], and serves
WildBench
[34].by
Our
results show
that models
fine-tuned
with M AGPIE achieve superior performance, even surpassing the official Llama-3-8B-Instruct model
on AlpacaEval, which was fine-tuned with over 10 million data points for supervised fine-tuning
(SFT) and follow-up feedback learning. Not only does M AGPIE excel in SFT alone compared to
prior public datasets that incorporate both SFT and preference optimization (e.g., direct preference

nforcement Learning with Large Language Models

1
Zihan Wang 2 Cédric Colas 3 4 Trevor Darrell 1 Pieter Abbeel 1
Figure 5: Identified bias types and examples in OffsetBias. The figure is borrowed from Park et al. (2024).
Abhishek Gupta 2 Jacob Andreas 3

s typically
well-shaped
ated exploby rewardtransitions,
fits in large
d novelty is
describe a
ledge from
his method,
rewards an
a language
the agent’s
le language
ents toward
eful behave loop. We
nvironment
r, showing
er coverage
pretraining
rmance on

You see trees,
cows, grass,
table, and
bushes. You have
wood in your
inventory. You
feel hungry,
thirsty, and
sleepy.

Prompt:
What should
you do next?

LLM

1. Cut down the tree.
2. Craft a pickaxe.
3. Eat cow.
4. Sleep.
. . .
k. Build a wood house.

Figure 1: ELLM uses a pretrained large language model
Figure 6: The figure is copied from (Du et al., 2023).
(LLM) to suggest plausibly useful goals in a task-agnostic
way. Building on LLM capabilities such as contextsensitivity and common-sense, ELLM trains RL agents to
these
producing
reward
code,
pursuecriteria.
goals thatInstead
are likelyofmeaningful
without
requiring
direct human
intervention.
Prompt values
is illustrative;
see full
RDLM
outputs
direct reward
that the
RL
prompt
and
goal
format
in
Appendix
D.
agent uses for policy optimization. This method
complex tasks in practice, RL agents may therefore need to
learn some behaviors in the absence of externally-defined
rewards. What should they learn?

is ideal for tasks where user goals are clear but
manually designing a reward function is complex.
While ELLM focuses on guiding exploration during pretraining by generating meaningful goals,
RDLM emphasizes task-specific reward generation
to streamline complex reward design and achieve
better agent alignment with human intent.
Eureka (Ma et al., 2023). Eureka is an algorithm
that leverages LLMs to automatically generate and
optimize reward function code for reinforcement
learning tasks. In Figure 7, first, a coding LLM
like GPT-4 is used to generate initial reward function code based on task descriptions. This code is

Published as a conference paper at ICLR 2024

Figure 2: E Figure
UREKA7:takes
unmodified
environment
codeisand
language
as context to
The overall
pipeline
of Eureka.source
The figure
borrowed
fromtask
Madescription
et al. (2023).
zero-shot generate executable reward functions from a coding LLM. Then, it iterates between reward sampling,
GPU-accelerated reward evaluation, and reward reflection to progressively improve its reward outputs.

domain expertise to construct task prompts or learn only simple skills, leaving a substantial gap in
achieving human-level dexterity (Yu et al., 2023; Brohan et al., 2023).
On the other hand, reinforcement learning (RL) has achieved impressive results in dexterity (Andrychowicz et al., 2020; Handa et al., 2023) as well as many other domains-if the human
designers can carefully construct reward functions that accurately codify and provide learning signals
for the desired behavior; likewise, many real-world RL tasks admit sparse rewards that are difficult
for learning, necessitating reward shaping that provides incremental learning signals. Despite their
fundamental importance, reward functions are known to be notoriously difficult to design in practice (Russell & Norvig, 1995; Sutton & Barto, 2018); a recent survey conducted finds 92% of polled
reinforcement learning researchers and practitioners report manual trial-and-error reward design and
89% indicate that their designed rewards are sub-optimal (Booth et al., 2023) and lead to unintended
behavior (Hadfield-Menell et al., 2017).
Given the paramount
design,The
we figure
ask whether
is possible
to develop
Figure 8: importance
An overviewof
ofreward
Text2Reward.
is copiedit from
Xie et al.
(2023). a universal
reward programming algorithm using state-of-the-art coding LLMs, such as GPT-4. Their remarkable
abilities in code writing, zero-shot generation, and in-context learning have previously enabled
then
iteratively
refined using
evolutionary
strateadvanced
skill
learning,
such this
as robotics
effective
programmatic
agents
(Shinn et al.,
2023; Wang
et al.,
2023a).
Ideally,
reward tasks,
designin
algorithm
achieve
human-level
generation
capabilities
that scale to a broad spectrum of
gies,
whereshould
candidate
reward
functionsreward
are evaluchallenging
scenarios.
tasks,
including
dexterity,
automate
theRL
tedious
ated
based
on how
well they
guide the
agenttrial-and-error procedure without human supervision,
Text2Reward
(Xie et al., 2023). Text2Reward is
and
yet
be
compatible
with
human
oversight
safety and alignment.
toward task success. The process evolves the to
re-assure
a
framework
that
leverages large language models
ward
functionsEvolution-driven
to improve their quality
andREward
effec- Kit for Agent (E UREKA
We introduce
Universal
), a novel reward design
to
automatically
generate
dense and interpretable
tiveness.
is particularly
effective
tasks
algorithmEureka
powered
by coding LLMs
withinthe
following contributions:
requiring complex or highly specific reward defini- reward function code from natural language task descriptions,
enabling
efficient
shaping across
1. Achieves
performance
on reward
design across
a diverse
suitereward
of 29 open-sourced
tions,
such as human-level
advanced robotic
skills. Its focus
on
diverse RL tasks.
As shown
in Figure
8, the proRL code
environments
thatmakes
include
10 distinct
robot morphologies,
including
quadruped,
quadcopter,
reward
optimization
it suitable
for scecess
starts
with
users
providing
a
task
description
biped,
manipulator,
as
well
as
several
dexterous
hands;
see
Fig.
1.
Without
any
task-specific
narios where precise reward shaping is critical. By
in natural
language,
whichthat
is input
into an LLM
prompting
reward
E UREKA
autonomously
generates
rewards
outperform
experto
utilizing
LLMs’orability
to templates,
generate and
refine code,
human rewards on 83% of the tasks and realizes generate
an average
normalized
improvement
of
52%.
executable reward code. This code often
Eureka evolves reward functions that effectively
includes
task-specific
logic and
integrate
exter2. Solves
dexterous
manipulation
tasks that
bymay
manual
reward
guide
RL agents.
Experiments
demonstrate
that were previously not feasible
nal libraries
for complex
functionalities.
The genengineering.
Wehuman-designed
consider pen spinning,
a five-finger
hand needs
to rapidly rotate
a
Eureka
outperforms
rewards in
in which
erated
reward
function
is
then
used
RL
to
guide
pen
in
pre-defined
spinning
configurations
for
as
many
cycles
as
possible.
Combining
E
UREKA
83% of tested tasks, with an average performance
thefirst
agent’s
Text2Reward
with curriculum
we its
demonstrate
for the
time behavior.
rapid pen Additionally,
spinning maneuvers
on a
improvement
of 52%,learning,
showcasing
potential for
simulated anthropomorphic Shadow Hand (see Figure
1 bottom).
supports
iterative refinement of the reward code
3. Enables a new gradient-free in-context learning approach to reinforcement learning from
human feedback (RLHF) that can generate more performant and human-aligned reward functions
2

Generative Verifiers: Reward Modeling as Next-Token Prediction

Problem

Finetuned Veriﬁer

Token Probability
“Is the answer correct (Yes/No)?”

GenRM

Yes

0.4

r

No

Solution

Other tokens
“Let’s verify step by step.”
Problem

Average

Finetuned Veriﬁer

Veriﬁcation CoT1

No

0.2

GenRM-CoT

…

Yes

0.9

Veriﬁcation CoTN

Yes

0.8

Solution

r

Figure 3 | An illustration of generative verifiers, namely GenRM and GenRM-CoT. Given a question and a
Figure 9: Illustration of GenRM. The figure is copied from Zhang et al. (2024a).
candidate solution, GenRM directly finetunes an LLM to answer the question ‘Is the answer correct (Yes/No)?’
RLAIF vs. RLHF
via SFT on the next-token response corresponding to either ‘Yes’ or ‘No’. During inference, the verifier score is
obtained by extracting the probability of the ‘Yes’ token (4). In comparison, GenRM-CoT finetunes a LLM to
the prompt
instructs
theasLLM
on how
to rate
a generation.
Prompt to Reward
directly
provides
scores
reward
signals,
bypassproduce verification chain-of-thought
(CoT) rationale before yielding the final Yes/No token. At test-time, we
1-10
Then,
the
likelihood
of
each
score
token
between
1 and 10
the needthe
foraverage
a reward
model. The
RL enabling
policy
sample multiple CoT rationales and use majority voting ing
to compute
probability
of ‘Yes’,
s------is
computed,
the
likelihoods
are
normalized
to
a
probability
Reward
SFT
- - - - - - - - additional inference-compute for better
is optimized
using these rewards, typically with alGenRM-CoT
to utilize
verification.
RL model
Model
-------distribution, a weighted score is calculated as s(y|x) =
General
P10 like Proximal Policy Optimization (PPO).
gorithms
Purpose
(i|y, x), and finally the score is again normalized
LLM
i=1 iPLLM-as-a-Judge,
Our results show that GenRM
outperforms discriminative
RMs,
and self-consistency
on
This
enables
automated,
scalable,
Reinforcement
to approach
the range [−1,
1]. Additional
details
on the and
prompting
Learning
algorithmic string manipulation and math reasoninghigh-quality
tasks
(Figure
1).
Best-of-N
performance
further
feedback
generation,
effectively
technique can
be found
in the Appendix
D. align-

improves with GenRM-CoT that uses majority-voting,ing
nearly
matching
performance
oracle while
verifier
RL agent
behavior
with taskwith
objectives
RL is then
conducted verifier
in a similar
manner to from
canonical
on algorithmic tasks. On GSM8K, when using a Gemma2-9B
GenRM-CoT
on solutions
reducing
reliance
on
human
annotations.
Figure
4:
In
direct-RLAIF
(d-RLAIF),
the
off-the-shelf
RLAIF,
where
the
direct
score
is
used
as
reward
instead
Figure 10:
figure
is borrowed
Lee et al. (2023).
Gemini
1.0The
Pro,
we observe
anfrom
improvement
from 73% → 93.4% in terms of the number of problems
LLM is directly used to provide rewards during RL, circumof a RM score.
GenRM.
Zhang et trained
al. (2024a)
re-define verificasolved,
GPT-4
and Gemini
Furthermore,
GenRM-CoT
on grade-school
math
ventingsurpassing
the issue of RM
“staleness”
and the1.5
timePro.
consuming
bymore
treating
it as a textcompetition
generation task,
lever-in
problems
easy-to-hard generalization, solvingtion
17%
high-school
problems
process ofexhibit
RM training.
2.3. Evaluation
through human
feedback,
enabling
opti- Moreover,
MATH500
(Lightman
et al., 2023)
withfurther
Best-of-32.
we find
that generative
scale
more
aging large
language
models toverifiers
produce
validaWeoutputs
evaluate
oureasoning
results
with
threeLLM-as-a-Judge
metrics
-"yes"
AI Labeler
mization.
This
method
excels
at
providing
flexfavorably
than
discriminative
verifiers
as
we
increase
model
capacity,
and
outperform
tion
and
chains,
such
as
2.2. Reinforcement Learning from AI Feedback
Alignment,
Rate, and verifiers
Harmless
Rate.
interpretable
rewards across
diverse
tasks, voting.
asible,
we scale
inference-time
compute
withRL
majority
Overall,
generative
hold
or "no"
with Win
explanations.
As shown
in significant
Figure 9,
2.2.1.
C
ANONICAL
RLAIF
particularly
in
robotics
and
manipulation.
Unlike
potential for improving the reasoning capabilities of LLMs.
Labeler Alignment
measures
the accuracy
AI-labeled
thisAIapproach
integrates
verification
into theofgenEureka,
evolving
and optimizing
rewardRLAIF
function
preferences
with
respect
to
human
preferences.
For
We
describe
our adaptation
of the canonical
setup erative
capabilities of LLMs, enabling them to as-a single
a softcandidate
AI-labeledanswers
preference
first converted to
code
through
LLMs and
evolutionary
algorithms,
Unless otherwise
mentioned,
RLAIF
is carried out sessexample,
2.below.
Preliminaries
and explain
in aistransparent
a
binary
representation
(e.g.
[0.6,
0.4]
→
[1, 0]). Then, a
using
this
method.
Text2Reward emphasizes creating human-readable
and interpretable manner. By framing verification
score
of
1
is
assigned
if
the
label
agrees
with
the human
An
autoregressive
language
model
anand
outputas
sequence
y = (prediction,
𝑦1 , 𝑦2 , . . . , 𝑦GenRM
a input context
𝑇 ) given eliminates
that
integrates
external
next-token
re- z
Areward
rewardcode
model
(RM)
is trained
on generates
thelibraries
LLM-generated
preference
and
0
otherwise.
The
alignment
accuracy
acc
x preference
(e.g., math
problem)
bythe
predicting
tokens
one at aliance
time, on
based
on thediscriminative
previously generated
tokens.
supports
iterative
refinement
via human
feedback.
labels
following
methodology
in Appendix
traditional
models
and
encan
be
expressed
as
follows:
Assuming
that
the language
model
is parameterized
by 𝜃, the conditional probability distribution of
A.2.
Since
ourmethods
approach
produces
soft labels
(e.g. design,
[0.6, 0.4]), hances
While
both
aim
to automate
reward
reasoning accuracy. Experimental results
generating
a
sequence
y
given
context
x
is
we
train the
RMin
with
a cross-entropy
lossreward
on the logic
softmax
Eureka
excels
optimizing
complex
demonstrate its ability D
to outperform conventional
of
scores generated
by the RM.
The softmax priorconverts
X
fortheadvanced
skills, whereas
Text2Reward
1
methods,
showcasing
its
tasks
AI
H
𝑇
1potential
[arg maxin
Pi,j
= prequirzacc =
the RM scores into a probability distribution. We note Ö
that
i ],
itizes flexibility, interpretability, and 𝑝adaptability
D
j
(y
|
x)
=
𝑝
(
𝑦
|
x
,
𝑦
)
(1)
interpretability, and scalable
𝜃 𝑡logical<𝑡reasoning, i=1
training a RM on a dataset of AI labels can𝜃be viewed as a ing
for
a
broad
range
of
tasks.
𝑡 =1 performance.
form of model distillation.

with
the we
convention
𝑦<1 = ∅ and learning
y<𝑡 = ( 𝑦to
, 𝑦𝑡 −1 ). For ease of notation, we define 𝑝𝜃 ( 𝑦𝑡 |AIx) := D×2
Finally,
conduct reinforcement
1 , 𝑦train
2 , . . .the
Self-Rewarding
D is the size of the preference dataset, P ∈ R
RLAIF. Lee et al. (2023) replace human feed- 5.3where
the RM to
assign
to
𝑝𝜃RLAIF
( 𝑦𝑡 | ypolicy
For ausing
vocabulary
size
𝑀 , rewards
the probability
of predicting the 𝑡 -th token 𝑦𝑡 , 𝑝𝜃H( 𝑦𝑡 | Dx),
<𝑡 , x).model,
is
the
matrix
of soft AI preferences, and p ∈ R is the
back in
RL withasAI-generated
feedback
by lever- The self-rewarding
enables𝑝𝜃the
responses,
described
in Appendix
A.3.
ismodel
determined
using
a softmax
with temperature
𝛾 on corresponding
logit scores vector
𝑧 ofmechanism
all
the tokens:
( 𝑦containing
| x) = ele𝑡 LLM
of human
preferences,
exp( 𝑧𝑡 /𝛾LLMs.
)
aging
The process begins with generating
to autonomously
assess
and
refine
its
own
perforÍ𝑀
, where 𝑧𝑡 = logit𝜃 ( 𝑦𝑡 | x, y<𝑡 ). Higher values
of
temperature
𝛾
introduce
more
randomness,
ments 0 or 1 to denote whether the first or second response
𝑖 /𝛾 ) outputs for a given task, such as text
𝑖candidate
=1 exp( 𝑧D
2.2.2.
IRECT-RLAIF ( D -RLAIF)
mance,
addressing
the cost, scalability,
and
adaptis preferred,
respectively.
while setting temperature 𝜏 = 0 makes the output deterministic,
which
corresponds
to greedy
decoding.
summarization or dialogue generation. These outOne issue with RLAIF is that the reward model may become ability limitations of existing RL methods.
Win Rate evaluates the end-to-end quality of two policies
puts are
paired
and
fed intoInantheLLM,
“stale”
as the
policy
is trained.
typicalwhich
setup, evalthe RM
by measuring how often one policy is preferred by human
them
provides
preferences
(e.g.,
selectisuates
trained
on and
generations
sampled
from the
initial
policy. Self-Refined LLM. Song et al. (2023) leverage
annotators over another. Given an input and two generations,
ingthe
thepolicy
better
output)the
or generated
assigns scores
basedbecome
on
LLMs to automatically generate reward functions 3
As
is trained,
trajectories
human annotators select their preferred generation. The
increasingly
out-of-distribution
the dataset
the used
RM was for deep reinforcement learning (DRL) tasks and
task-specific
criteria. This from
feedback
is then
percentage of instances where policy A is preferred over
trained
suboptimal
performance
(Bai et al., introduces a self-optimization mechanism to iterto trainon,a leading
reward tomodel
that predicts
the quality
policy B is referred to as the “win rate of A vs. B”. A 50%
2022a).
One and
solution
is tothe
conduct
iterativeInRLAIF,
where a atively refine these functions. The process begins
of outputs
guides
RL agent.
its streamwin rate indicates that A and B are equally preferred.
new RM is periodically trained on the latest policy, though
lined variant, d-RLAIF (see Figure 10), the LLM
with
the LLM
generating
an initial
reward
functionthat are
Harmless
Rate
measures the
percentage
of responses
this is a time consuming process.

We introduce direct-RLAIF (d-RLAIF) - a simple alternative
to canonical RLAIF that directly uses LLM feedback as the
reward signal in RL. D-RLAIF addresses the RM staleness
issue, as the off-the-shelf LLM directly scores generated

considered harmless by human evaluators. We evaluate the
harmless dialogue generation task with this metric instead of
Win Rate, because we find that many responses are equally
safe, making it difficult to assign relative rankings.

Self-Instruction creation
Generated
new prompts

Seed model
(for t=1)

Instruction following training

Generate
responses

Generate
rewards

Preference
pairs
select

DPO
training

Next iteration model

Figure 1: Self-Rewarding
Language
Models.
Our
self-alignment
method
consists of two
Figure 11: The overview
of SRLM.
The figure
is copied
from Yuan et
al. (2024).
steps: (i) Self-Instruction creation: newly created prompts are used to generate candidate
responses from model Mt , which also predicts its own rewards via LLM-as-a-Judge prompting.
(ii)Review
Instruction
following
pairs
selected
from the
generated
data,
the user’s question and
the correspondingtraining:
response using the preference
additive 5-point
sions, are
including
relevance,
coverage,
usefulness,
scoring system described below. Points are accumulated based on the satisfaction of each
criterion: are used for training via DPO, resulting in model M
which
procedure
canto
t+1 . This whole
clarity, and professionalism,
assigning
a score
- Add be
1 point
if the response
is relevant and in
provides
some information
related instruction
to
then
iterated
resulting
both
improved
following
and
reward
modeling
ability.
each response based on these criteria. Utilizing
the user’s inquiry, even if it is incomplete or contains some irrelevant content.
- Add another point if the response addresses a substantial portion of the user’s question,
but does not completely resolve the query or provide a direct answer.
these scores, preference pairs are constructed, con- Award a third point if the response answers the basic elements of the user’s question in a
useful way, regardless of whether it seems to have been written by an AI Assistant or if it
sisting of a preferred response and a dispreferred
has elements typically found in blogs or search results.
Starting
aresponse
seedis clearly
model,
inan each
iteration
there is a process of Self-Instruction creation
- Grant a fourthfrom
point if the
written from
AI Assistant’s
perspective,
addressing the user’s question directly and comprehensively, and is well-organized and
response.
These
pairscreated
are used
for Direct
Preferwhereby
responses
areconciseness
generated
by the
model for
newly
prompts,
and
are
helpful, even ifcandidate
there is slight room for
improvement in clarity,
or focus.
- Bestow a fifth point for a response that is impeccably tailored to the user’s question
ence
Optimization
(DPO),
improving
its
ability
to
by an assigned
AI Assistant, without
extraneous information,
reflecting
expert knowledge,
and The latter is implemented via LLM-as-a-Judge
then
rewards
by
that
same
model.
demonstrating a high-quality, engaging, and insightful answer.
generate
high-quality
responses.
Through
iterative
prompting, which can also be seen as an instruction following task. A preference dataset is
User: <INSTRUCTION_HERE>
refinement,
themodel
model is
progressively
its
built
from the generated data, and the next iteration
of the
trained viaenhances
DPO, ee
<response><RESPONSE_HERE></response>
performance. Figure 12 provides a detailed explaFigure
1.
After examining the user’s instruction and the response:
of theetprompts
usedseed
by the
modelfine-tuned
to evaluate
In- Briefly
our justify
experiments,
start with a Llama 2 70Bnation
[Touvron
al., 2023]
model
your total score, up towe
100 words.
- Conclude with the score using the format: “Score: <total points>”
candidate
responses.
Experimental
results
demononRemember
Open
Assistant [Köpf et al., 2023], and then perform the above training scheme.
We
to assess from the AI Assistant perspective, utilizing web search knowledge as
strate
that
fine-tuning
Llama
2
70B
using
SRLM
necessary.
To evaluate
the
responsedoes
in alignment
withinstruction
this additive scoring model,
we’ll
find
that
not
only
the
following
performance
improve
from
Self-Rewarding
systematically attribute points based on the outlined criteria.
over three
outperforms
several
state-ofLLM alignment compared to the baseline seed model,
butiterations
importantly
the reward
modeling
Figure 2: LLM-as-a-Judge
prompt
for our LLM
to actimproves
as a reward model
and
the-art
models,
including
GPT-4
and
Claude
2, on
ability,
which
is
no
longer
fixed,
as
well.
This
means
that
the
model
during
iterative
provide self-rewards
for its own for
modelLLM
generations.
model is initially
withisseed
Figure
12: Prompt
as The
a judge.
The trained
figure
training data of how
to performat
well a
at this
task, and
then improves at to
this task
further
training
is
able,
given
iteration,
provide
a
higher
quality
preference
dataset
to
itself
benchmarks like AlpacaEval 2.0, showcasing its
through our self-rewarding
trainingetprocedure.
borrowed
from Yuan
al. (2024).
than in the previous iteration. While this effect
likely saturates
in real-world
settings, it
effectiveness
in improving
instruction-following
provides
the
intriguing
ofare obtaining
reward
models
(and hence LLMs) that are
Section 2.2), following
Xu et
al. [2023], discardingpossibility
the pair if their scores
the same. These
and
general
task
performance.
pairs can be used for training with a preference tuning algorithm. We use DPO [Rafailov
superior
to ones that could have been trained from the original human-authored seed data
et al., 2023].
based
on natural language task descriptions. The
alone.
2.4 Overall
Self-Alignment
Algorithm
reward
function
is then
applied to RL training, and
Generative Judge via Self-generated ConIterative
Training performance
Our overall procedureis
trains
a series of modelsFeedback
M , . . . , M where
the
agent’s
evaluated.
trastive Judgments (Con-J). Ye et al. (2024)
each successive model t uses augmented training data created by the t − 1 model. We thus
2
Self-Rewarding
Language
define AIFT(M
) to mean AI Feedback Training data
created using model M . Models
from this evaluation is fed back into the LLM, en- propose a self-rewarding mechanism with selfabling
it to We
dynamically
adjust
and
the
Model Sequence
define the models, and
the training
data improve
they use as follows:
generated contrastive judgments, allowing LLMs
Our
first
to a base
language model, and a small amount
M : approach
Basefunction
pretrained LLM
fine-tuning. access
reward
inwitha noassumes
closed-loop
manner.
Com-pretrained
to evaluate and refine their outputs by providing
: Initialized with M , then fine-tuned
on the IFT+EFT
data using
SFT. build a model that aims to possess two skills
ofMhuman-annotated
seed
data. seedWe
then
pared
to Eureka
Text2Reward,
this
detailed, natural language rationales. As shown in
M : Initialized
with M ,and
then trained
with AIFT(M ) data
using approach
DPO.
simultaneously:
M : Initialized with M , then trained with AIFT(M ) data using DPO.
eliminates
the need for external optimization algo- Figure 13, unlike traditional scalar reward models
This iterative training resembles the procedure used in Pairwise Cringe Optimization and
specifically1.
is
Iterative intervention.
DPO, introduced
in Xu et al.given
[2023]; however,
an external that
rithms
ortermed
manual
that output
a single
numerical
score,
GeneraInstruction
following:
a prompt
describes
a user
request,
the the
ability
to
fixed reward model was used in that work.
generate a high quality, helpful (and harmless)
tive Judgeresponse.
compares candidate outputs and gener4
Self-Rewarding
Language
Models the
(SRLM).
atesgenerate
positive and
negative
evaluations
with accomcreation:
ability to
and
evaluate
new instruction2. Self-Instruction
Yuan et al.
(2024) examples
introduce to
a novel
approach
panyingset.
explanations in natural language. This
following
add to
its own training
where LLMs act as both the generator and eval- enables the model to assess why one output is
These
used so that
thesystem.
modelAscan preferable
perform to
self-alignment,
i.e.,
they are the
uator to skills
create aare
self-contained
learning
another, providing
interpretability
and
components
used
to
iteratively
train
itself
using
AI
Feedback
(AIF).
shown in Figure 11, the model begins by generat- aligning its decisions with nuanced human prefering new prompts (instructions)
and multiple
candi- candidate
ences. The
framework
is also
usingitself
DPO
Self-instruction
creation consists
of generating
responses
and
thentrained
the model
judging
their derived
quality,from
i.e., existing
it acts as
itsthereby
own reward
model, replacing
the need
an external
date
responses
data,
on human-labeled
preference
data,for
where
the LLM
one. This
is implemented
via the LLM-as-a-Judge
mechanism
[Zhengcontrastive
et al., 2023b],
i.e., by
creating
a diverse
and comprehensive
set of train- is prompted
to produce
rationales
for
formulating
the
evaluation
of
responses
as
an
instruction
following
task.
This
self-created
ing samples. Subsequently, the model evaluates
paired outputs. These self-generated evaluations
AIF
data is used
a trainingscorset. serve as both the reward signal and the basis for
these preference
candidate responses
usingas
a structured
ing
to determine their
quality. The
evaliterative
enablingbythebuilding
model toaimprove
Ourmechanism
overall self-alignment
procedure
is an
iterative
one,refinement,
which proceeds
series
uation
encompasses
dimenits alignment
with task
objectives autonomously.
of suchframework
models, with
the aimmultiple
that each
improves
over the last.
Importantly,
because the
model can both improve its generation ability, and act as its own reward model through the
same generation mechanism, this means the reward model itself can improve through these
iterations, deviating from standard practices where the reward model is fixed [Ouyang et al.,
1
th

t

T

t

0
1

0

2

1

1

3

2

2

Figure 13: Illustration of a scalar reward model and the proposed Con-J. The figure is copied from Ye et al. (2024).

In experiments, the Generative Judge achieved performance comparable to scalar reward models in
aligning outputs with human preferences but excelled in interpretability and robustness to dataset
biases. By leveraging contrastive judgments, the
model demonstrated enhanced adaptability to tasks
requiring multi-faceted reasoning and improved its
capacity for transparent decision-making.

6

Analysis of RLHF/RLAIF

While RLHF and RLAIF are effective methods
for aligning LLMs with desired behaviors, there
are still challenges that require careful analysis.
These include addressing out-of-distribution issues
between the trained reward models and the aligned
LLMs, ensuring the interpretability of the model
for humans, and maintaining safety and evaluation
benchmarks to train robust reward models. In this
section, we discuss recent works that tackle these
challenges and provide strategies for overcoming
them.
6.1

Out of Distribution (OOD)

Out-of-distribution (OOD) issues present a significant challenge in reward modeling, particularly
when the reward model and the large language
model (LLM) are trained independently. This separation can lead to inconsistencies in the knowledge and decision-making frameworks of the two
models, potentially causing the reward model to
encounter unfamiliar scenarios or fail to generalize
effectively. Addressing OOD challenges is critical

for ensuring that reward models (RMs) perform
reliably across diverse inputs.
Lou et al. (2024) point out that RMs often struggle when encountering OOD inputs, exhibiting a
dangerous tendency toward overconfidence. This
overconfidence stems from the models’ reliance on
training data distributions, which may not account
for the variability of real-world environments. They
emphasized that traditional RMs lack mechanisms
to quantify and act on uncertainty. By introducing
uncertainty quantification, the proposed approach
enables RMs to distinguish between "known" and
"unknown" regions in the data space, ensuring
more cautious and robust decision-making. Moreover, the integration of contrastive learning and regularization techniques further enhances the RM’s
ability to handle OOD scenarios.
Yang et al. (2024b) find that reward models failing to generalize preferences when input texts contain novel combinations of known patterns or previously unseen linguistic structures. To address this
limitation, they proposed Generalizable Reward
Model (GRM), which regularizes the hidden states
of RMs during training, ensuring they preserve the
underlying language understanding of LLMs. Additionally, a text-generation loss is introduced to
maintain the balance between preference learning
and the core generative capabilities of LLMs. The
result is a reward model that is more adaptable to
diverse inputs.

ArmoRM

Gating Layer

Tokens

✓

✓

Regression Layer

…

Decoder Layer

Decoder Layer

Response

Token Embedding

Prompt

Helpfulness
Correctness
Coherence
Complexity
Verbosity

✅ 0.8×
✅ 0.6×
⬜ 0×
⬜ 0×
❌ -0.2×

Score

Figure 1:Figure
Architecture
of ourof
reward
model.
consists
of an LLM
backbone,
regression
14: Overview
ArmoRM.
TheItfigure
is borrowed
from
Wang etaal.
(2024b). layer for
multi-objective reward modeling, and a gating layer that outputs coefficients to scalarize the reward
objectives into a scalar score.
This framework has achieved tremendous success in the post-training of ChatGPT [Ouyang et al.,
2022] and Claude [Bai et al., 2022]. These ideas also extend to other approaches, such as rejection
sampling fine-tuning [Dong et al., 2023; Gulcehre et al., 2023] and iterative direct preference learning
[Xiong et al., 2023; Guo et al., 2024; Xie et al., 2024]. In these approaches, the intermediate policy is
typically iteratively deployed to collect new responses, uses the reward model to label the responses,
and fine-tunes the model on the newly collected preference data. In all of these RLHF frameworks,
the capacity of the reward model is crucial as it directly affects the quality of the aligned LLMs.
The most popular
modeling approach
based
on is
the
maximum
estimation (MLE)
Figureward
15: Illustration
of QRM. isThe
figure
copied
from likelihood
Dorka (2024).
of the Bradley-Terry (BT) model [Bradley and Terry, 1952]. Despite its widespread use, the BT
model is rather limited in the capacity of capturing the complicated human preference [Munos et al.,
2023; Swamy
et al., 2024; Ye et al., 2024]. In addition
the as
capacity
issue,for
common
RMs, likelearn6.2 Human
Interpretability
scoretoused
feedback
reinforcement
the BT model, are typically black-box models that ing.
output
scores
or
preferences
without
providing
This mixture-of-experts approach
effectively
Human
interpretability is aexplanations,
crucial aspect
of reward
human-interpretable
making
it subjectseparates
to the widely
observed
phenomenon
of scores
reward to be
the objectives, allowing the
hacking
et al.,
2022; Singhal
al., 2023; Chen et al., 2024], where the aligned LLMs generate
modeling,
as [Skalse
it enables
researchers
andetpractitionmore clearly attributed to specific input features
high-reward
responses
(rated
by
the
RM)
not align with actual human preferences [Gao
ers to understand and trust the decisions madethat
by do or
goals,
thusofimproving
both interpretability
et
al.,
2023;
Lin
et
al.,
2023;
Coste
et
al.,
2023].
A
notable
example
this is the verbosity
bias, where and
the model. Reward models often produce discrete
transparency.
aligned LLMs produce longer-than-necessary responses because the RM favors length, regardless of
scores to evaluate LLM outputs, but the rationale
quality [Singhal et al., 2023; Wang et al., 2024a; Chen et al., 2024].

behind these scores is not always transparent. En- Quantile Reward Models (QRM). Dorka
In interpretability
this work, we aim
to enhance
rewardthat
models
making them more interpretable [Molnar, 2020]
hancing
is vital
for ensuring
the by (2024)
observe that traditional reward models typiand steerable [Wong et al., 2021]. Using the aforementioned verbosity bias as an example, suppose
alignment process is comprehensible and reliable, cally produce a single point estimate for rewards,
the RM’s output is decomposable, meaning that it assigns a high score to a response due to two
particularly
sensitive
where
which
theirweability
capture
the may
diversity
factors:in40%
for itsapplications
helpfulness and
60%human
for its length.
In limits
this case,
can seetothat
the RM
preferences
play the
a central
role.bias. Furthermore, if theand
human
In contrast,
suffer from
verbosity
RMcomplexity
is steerable,ofwe
could preferences.
adjust its decisionmaking process to base its scoring 100% on helpfulness.
would be
regardless
response length,
theyThis
proposed
QRM,
whichofleverages
quantile rethus
mitigating
the
verbosity
bias.
Enhancing
the
interpretability
of
RMs
also
allows
humans
to
ArmoRM. Wang et al. (2024b) argue that cur- gression to estimate the full distribution of rewards,
verify whether RMs have similar internal decision processes to humans when acting as proxies for
rent reward models often conflate different objec- allowing for a richer representation of human feedhuman preferences. We believe that this human-AI interaction process could ensure that RMs are
tives, consistent
making itwith
difficult
to discern which aspects
back. Figure 15 illustrates the architecture of the
human values and preferences, making RM-aligned LLMs more reliable and robust.
of the input data influence their scoring. To ad- QRM: the LLM backbone processes the prompt
dress this, they proposed the ArmoRM (Absolute
and response, producing two types of embeddings:
2
Rating Multi-Objective Reward Model). As illus- one for the gating network (prompt embedding) and
trated in Figure 14, the model processes a context
another for the quantile regression layers (promptand multiple candidate responses, evaluating them
response embedding). The quantile regression layacross interpretable dimensions such as honesty, ers estimate the reward distribution for various
safety, verbosity, and relevance. Each dimension
attributes, such as helpfulness and harmlessness.
is assessed by a dedicated sub-model that gener- Meanwhile, the gating network assigns weights to
ates individual scores. These scores are then dy- these attribute distributions. These weighted disnamically weighted by a gating network, which
tributions are then combined to produce the final
adapts to the context and produces a final reward
reward distribution. This approach is particularly

effective in handling noisy labels and conflicting
preferences, as it models such uncertainties as distinct modes within the reward distribution. By estimating a complete distribution, QRMs enhance
interpretability in decision-making, such as focusing on lower quantiles for risk-averse tasks or upper
quantiles for exploration.
General Preference Representation Model
(GPM). Zhang et al. (2024c) emphasize the importance of structured preference representations
in improving interpretability. The proposed preference representation learning method enhances
interpretability by embedding human preferences
into a latent space, which provides a structured and
transparent way to model complex relationships.
Instead of relying on traditional point-based scoring systems, this approach maps preferences into
a continuous space, where each dimension represents a specific attribute or characteristic, such as
relevance or coherence. This allows for clear explanations of why certain responses are preferred
based on their positions within the space. For example, a response might rank higher due to its conciseness, and this preference can be directly traced
to its alignment with the "conciseness" dimension
in the latent space. Unlike traditional methods,
which struggle with intransitive or cyclic preferences, preference embeddings naturally capture
these nuanced relationships. By visualizing or interpreting how responses relate to one another across
multiple dimensions, the method avoids forcing a
linear ranking and instead reflects the true complexity of human feedback. Additionally, the latent
representations adapt dynamically to different contexts, making it possible to explain preferences
based on the specific attributes relevant to the situation. For instance, a humorous response might
be preferred in one scenario, while informativeness could dominate in another, and the model can
attribute the preference to these varying factors.
6.3

Safety

Safety is a critical consideration for reward models,
especially when dealing with potentially harmful
or biased inputs. As reward models guide the optimization of LLMs, their handling of sensitive or
adversarial content plays a significant role in ensuring that the outputs generated by LLMs align
with ethical and safety standards. This subsection
explores the challenges and recent advancements
in enhancing the safety of reward models.

Safe RLHF (Dai et al., 2023). When aligning
LLMs with human values, Safe RLHF emphasizes
both helpfulness and harmlessness. Safe RLHF
uses a structured method to balance these two objectives by decoupling human preference annotations into two distinct objectives: a reward model
for helpfulness and a cost model for harmlessness.
This decoupling is achieved by independently annotating helpfulness and harmlessness on collected
response data, with each response evaluated separately for these aspects.
In the Safe RL phase, the method seeks to maximize expected rewards (helpfulness) while enforcing cost constraints (harmlessness) through a Lagrangian approach, where an unconstrained objective can be formulated as:
min max[−JR (θ) + λ · JC (θ)],
θ

λ≥0

(1)

where JR (θ) is the reward objective, JC (θ) is
the cost objective, and λ is dynamically adjusted
as a Lagrange multiplier to balance helpfulness
and harmlessness adaptively during training. The
method iteratively updates both model parameters
θ and the Lagrange multiplier λ, with each round
of Safe RLHF training adjusting λ to reflect recent
feedback on the safety constraints.
Quantized Reward Konditioning (Quark). Lu
et al. (2022) provide a framework Quark for addressing harmful content by equipping reward models with mechanisms to identify and unlearn unsafe
outputs. The "unlearning" aspect of the Quark algorithm is reflected in its ability to adjust the generative tendencies of a language model through reinforcement learning, gradually "forgetting" undesirable traits such as toxicity, repetition, or negative
sentiment. The algorithm evaluates generated samples using a reward function, marking low-quantile
samples as traits that the model needs to suppress,
and progressively weakens these tendencies during
the fine-tuning process through conditional generation. Additionally, the reinforcement learning
objective incorporates both the attenuation of lowquantile tendencies and the enhancement of highreward objectives, reducing the model’s reliance on
undesirable traits. By leveraging reward quantiles
to guide the process, Quark effectively "unlearns"
existing biases in the model, ultimately enabling
the generation of high-quality text that aligns with
desired goals.
Constitutional AI.

Bai et al. (2022) introduce a

Figure 16: The overall framework of Quark. The figure is copied from Lu et al. (2022).
Response
Helpful RLHF 
Model

Generate
Generate Responses

Responses

toto“Red
“Red Teaming”

Teaming”

Prompts
Eliciting
Harmful

Prompts
Eliciting 

PairsSamples
of Samples

Generate
Generate Responses

Responses

“Red Teaming”

Teaming”

toto“Red
Prompts
Eliciting
Harmful

Prompts
Eliciting 

Samples
Harmful
Samples

Constitutional AI Feedback

for Self-Improvement

Finetuned

SL-CAI

Model

Critique
Revision

Finetuned

Preference

Model (PM)

RLAIF

Training

with 

PM + SL-CAI 

Models

Final

RL-CAI

Model

Figure 1 We show the basic steps of our Constitutional AI (CAI) process, which consists of both a superFigure
17: The(SL)
overview
Constitutional
AI (CAI)
figure is borrowed
from(RL)
Bai etstage,
al. (2022).
vised
learning
stage, of
consisting
of the steps
at theprocess.
top, andThe
a Reinforcement
Learning
shown
as the sequence of steps at the bottom of the figure. Both the critiques and the AI feedback are steered by
a small set of principles drawn from a ‘constitution’. The supervised stage significantly improves the initial
novel
approach
to guiding
AI behavior
through
17 of
illustrates
this dual-phase
model,
and gives
some control
over the
initial prebehavior atFigure
the start
the RL phase,
addressingframework
potential
exploration
problems.
The RL
significantly improves
performance
andsupervised
reliability. learning phase, the
defined
principles,
referred
to stage
as a "constitution,"
in detail.
In the

enabling the training of harmless and transparent
model progressively learns to identify and rectify
AI assistants without relying heavily on human- undesirable traits in its responses through self1 Introduction
labeled data. The central idea is that AI can self- feedback. In the reinforcement learning phase,
assess
and refine
outputs
based on
principreference
model even
evaluates
theAIgenerated
We would
like toitstrain
AI ystems
thatthese
remain
helpful, ahonest,
and harmless,
as some
capabilitiesrereach
or exceed
human-level
performance.
suggestssponses,
that we will
need to develop
techniquesability
that dotonot
ples,
ensuring
safety
and alignment
with This
desired
strengthening
the model’s
prorely on
humans
to supervise
aspects
of AI behavior,
thatoutputs
can be used
automatically
test and enhance
goals.
The
process
involves all
two
key phases:
a su- and
duce
thattoalign
with constitutional
princirobustness to harmful behaviors. We also aim to develop methods that encode desirable AI behavior in a
pervised learning phase and a reinforcement learn- ples while maintaining transparency. This framesimple and transparent form, and that make it easier to understand and evaluate AI decision making.
ing phase. During the supervised phase, the model
work ensures the AI remains non-evasive, engagIn this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it
generates
initial responses, critiques them based
ing directly with sensitive or harmful prompts by
to train a non-evasive and relatively harmless AI assistant, without any human feedback labels for harms.
onThe
constitutional
principles,
and refines
its outputs,
explaining
why they
are problematic
than
method therefore
improves
upon, and
partially replaces
reinforcement
learning
from humanrather
feedback
which
are then
used
to fine-tune
model.‘RL-CAI’
In the isavoiding
By leveraging
minimal
[Christiano
et al.,
2017].
The wthe
assistant
preferred them.
by crowdworkers
over those
trainedmanual
with
previously collected
[Bai
et al.,the
2022,
Ganguli
et al., 2022]
human feedback
labels for
harmfulness.
chose
reinforcement
learning
phase,
model
generates
oversight
and applying
clear
rules, thisWe
approach
the
term
‘constitutional’
because
we
are
able
to
train
less
harmful
systems
entirely
through
the
specification
multiple responses to prompts, which are evaluated
presents an innovative way to reduce harmful outof a short list of principles or instructions, i.e. a constitution. But we are also employing this terminology to
byemphasize
a preference
model
trained
to
align
with
the
conputs
and
precise
that when developing and deploying a general
AI while
system,enhancing
we cannottransparency
avoid choosing
some
set ofbestitutional
as or
a implicit.
havioral control in AI systems.
principlesguidelines.
to govern it,These
even ifevaluations
they remainserve
hidden
reward
signal
to
optimize
the
model
further.
Our motivations for developing this technique were: (1) to study simple possibilities for using AI systems to
help supervise other AIs, and thus scale supervision, (2) to improve on our prior work training a harmless AI
assistant by eliminating evasive responses, reducing tension1 [Bai et al., 2022, Glaese et al., 2022] between
helpfulness and harmlessness and encouraging the AI to explain its objections to harmful requests, (3) to
make the principles governing AI behavior, and their implementation, more transparent, and (4) to reduce
iteration time by obviating the need to collect new human feedback labels when altering the objective. Let us
discuss these motivations in more detail.

Figure 2: Two-Stage
Annotation
Process.
The first stage
involves
evaluating
of a
Figure 18: Annotation
process
of BeaverTails.
The figure
is copied
from Ji etthe
al. harmlessness
(2024).
QA pair across 14 harm categories, subsequently determining the safety meta-label. The second stage
then ranks responses to a prompt according to their helpfulness and harmlessness.
• Within this dataset, 44.64% were assigned the safe meta-label, while the remaining 55.36% were
categorized under the unsafe meta-label.
• We acquired 361,903 pairs of human-preference annotations separately for the helpfulness and
harmlessness metrics of the responses. The inter-crowdworker agreement rate: safety meta-label =
81.68%, helpfulness preference = 62.39% and harmlessness = 60.91%.
Additionally, we solicited crowdworkers to assign confidence scores to their annotations, applicable
to both the classification and response-ranking tasks. The confidence spectrum extended from “very
uncertain” and “uncertain” to “certain” and “very certain”, corresponding to a scale of 0 to 3.
3.2 Data Collection and Annotation Process
Generating QA pairs Our study involves a collection of over 28k red-team prompts derived
Figure
1:ED
The
RBR
is ofcombined
with
the(RBR).
helpful-only
RMis score
during
training.
Figure
The
rule-based
rewards
copied
from Mu
et al.
(2024).
from the
HH19:
R
-Toverview
EAM dataset
[18] and
[56].
GivenThe
thefigure
dialogical
nature
ofRL
these
datasets, we
specifically selected the first question that initiated the interaction between the human and the AI
assistant.
Theseresponse
questions
were
meticulously
crafted by
Ganguli
et al. to
be both
provocative
and
The
appropriate
type
for
a given user
request
varies
content
policy
category
- wesuch
define
BeaverTails
(Ji
et al., 2024).
BeaverTails
is test
a for
rather
thanby
general
guidelines.
These
rules,
as
intentionally
deceptive,
serving
as
a
rigorous
a
language
model’s
ability
to
handle
harmful
this mapping as the behavior policy. To combat overrefusals, we include content policy categories
large-scale,
high-quality
question-answer
dataset
"Refusalsperceived
should include
an apology
but
not sound
prompts
designed
to elicit
unsafe responses.
For questions
overly
terse orline
incomplete,
that
capture
the safety
boundary
within a content
policy area: the as
often
complex
between
designed
to
enhance
the
safety
and
utility
of
large
judgmental,"
are
broken
into
simple
binary
propowe incorporated
contextual
information
count
what’s
consideredadditional
acceptable
or unacceptable
for a during
model pre-processing.
to engage with. The
For average
example,word
users
may
language
models
(LLMs).
As
displayed
in
Figsitions,
like
whether
the
response
includes
an
apol(using the
/\b\w+\b/)
prompt
13.61. material without asking the model to directly
request
thatregex
the model
classify for
texteach
that is
about isharmful
ure 18, this dataset
uniquely
separates
annotations
ogy or avoids
judgmental
language.
A Grader
LLM
generate
harmful
content.
In
these
cases,
the behavior
policy
may require
theper
model
to comply.
We then new
prompt
the Alpaca-7B
model
[27]
to generate
multiple
unique
responses
question
across
of "helpfulness" and "harmlessness" for question- evaluates each response against these propositions
the set of 7.7k unique questions (chosen from the previously mentioned set of red-team prompts) for
answer pairs, providing distinct perspectives on
assigns probabilities, which are then combined
B EAVERTAILS-30k. To ensure generation diversityand
and
enrich the range of outputs, we modulate
these
crucial attributes.
It as
comprises
meta- with
antoexisting
helpful-only
reward
model
(RM)
4the
Rule-Based
Rewards
for safety
Safety
sampling
parameters
follows:
temperature
is set
1.5, and
the maximum
token
length
is
labels
for
333,963
Q&A
pairs
and
361,903
pairs
to
create
a
total
reward.
As
shown
in
Figure
19,
limited to 512, with top_k and top_p values configured at 30 and 0.95, respectively. We measurethis
the
ofthis
expert
comparison
data forthe
both
helpfulness
and (RBRs),
combined
function
is used
reinforcement
In
section,
we describe
Rule-Based
Rewards
ourreward
proposed
toinbuilding
safety
average
word
count
(using
regex
/\b\w+\b/)
and
observe
a mean
ofapproach
61.38
words
per response
harmlessness
metrics
The
dataset based
spans diverse
learning,
ensuring policy.
that the We
model
aligns
with code
both
reward
functions
for30k
RL
training
on a content
and behavior
also
provide
across
the
resultant
responses.
2
real-world
scenarios,
including
everyday
inquiries,
safety
policies
and
helpfulness
goals
without
being
and
example
synthetic
data
for
fitting
the
reward
combination
models
described
in
this
section
. To
Two-Stage Annotation Process In an effort to annotate our dataset with human-preference data
motivate
ourwe
approach,
given
content
and
behavior
policy,
consider
what
researchers
must
do at
to
professional
domains,
ethical
challenges,
and
crossoverly
cautious.
Unlike
RLHF
or RLAIF,
which
efficiently,
engaged
a team
of over
70
crowdworkers
(annotators)
- all
of whom
possess
prepare
labeling
for and
safety
data
annotators.
The
haveannotations
to write
a list
of natural
cultural
contexts,instructions
enabling
researchers
to refine
relies
onofresearchers
collecting/generating
synthetic
datasets
to
least
a college-level
education
a proficient
command
English. The
provided
by
language
rules for
defining
a good
completion
and train
scoring
completions
with
undesirable
features,
the
crowdworkers
will
be re-evaluated
by
the quality
control
department,
which
maintains
regular
LLM
behavior
more
effectively.
Unlike
existing
a reward
model, RBR
directly
integrates
the
taking
great
care
to
ensure
that
instructions
are
specific
enough
that
different
annotators
will
produce
communication
with the
research
team to advanensure alignment.
task of annotating
a QA pair
in the
datasets, BeaverTails
provides
significant
rules intoThe
the learning
process. RLAIF’s
synthetic
the
same
judgements.
For
example,
consider
collecting
data, built
that from
scores
completions
from
B EAVER
T
AILSofdataset
involves
a two-stage
annotation
process.
tages
in terms
scale and
annotation
granularity,
datasets
general
guidelines,
can1-7.
lose For
deaaiming
requesttothat
should
be
hard-refused,
a
simplified
version
of
a
rule
in
our
example
can
be:
"rank
a core
for
exploring through
tail orarequire
extensive updates
as policies
evolve.
During thebecome
first
theresource
QA pair and
is annotated
multi-classification
process
involving
14
completions
withstage,
a short
apology
statement of inability
highest at 7, deduct
1 point
for each
LLM safety
and alignment
within
the community.
In contrast,
RBR provides fine-grained
control To
by
harm
categories
(see
Sec.
3.3),
leading
to
the
assignment
of
a
corresponding
safety
meta-label.
undesirable refusal quality (such as judgemental language) that is present, if the refusal contains
applying
rules
dynamically
during
training,
makfacilitate the
QA-moderation
task at
during
LLMs deployment
(seehave
Sec.to4.1),
we advocate
for examples.
assessing
disallowed
content
rank it lowest
1". Researchers
often also
provide
illustrative
the
harmlessness
of
a
QA
pair
from
a
risk
neutralization
perspective,
rather
than
relying
solely
on the
ing
it
more
precise
and
adaptable.
Experimental
Rule-Based
Rewards
(RBR)
(Mu
et
al.,
2024).
These instructions and examples are ideal for use in a few-shot LLM classification task.
toxicity
ofdesigned
individual
utterances
the QA
pair provided
content
moderation
results
show thatbyRBR
achieves
superiorsystems.
perforRBR is ascore
method
to make
LLMswithin
safer and
In
oura helpful
observations,
demonstrate
higher
asked
classify
specific,
individual
For
QA pair
beLLMs
classified
as harmless
andaccuracy
receive
awhen
safe
it
be confirmed
as
mance,
withmeta-label,
an F1 to
score
ofmust
97.1
compared
to 91.7
more
bytorelying
on explicit,
detailed
rules
tasks,
such asacross
determining
whether
a text contains
apology, compared to general, multilayered tasks
risk-neutral
all 14 harm
categories
by the annotators.
such as rating completions given a large content and behavior policy as input. To leverage this strength,
The second stage involves providing the annotators with a single prompt and multiple corresponding
we simplified these complex policies into a series of individual binary tasks, termed propositions.
responses, each pre-labeled with a safety meta-label from the first stage. The annotators are then
We then established a set of rules that determine when combinations of these propositions’ truth
values are desired or undesired. This framework allows us to accurately rank completions using

FigureFigure
1: The
method of the R
EWARD
B ENCH evaluation
Each
prompt
accompanied
20:scoring
The prompt-choice-rejection
triplets
of RewardBench.
The figure is suite.
copied from
Lambert
et al.is(2024).
by a chosen and rejected completion which are independently rated by a reward model.
for a human-feedback baseline, effectively balancing safety and usefulness in LLM outputs.

models, such as lack of transparency, reliance on
proprietary systems like GPT-4, and high evalu3 Background
ation costs. Its primary motivation is to provide
6.4 Reward Model Evaluation
a reliable and accessible alternative for evaluating language
model
acrossdoing
diverseRLHF,
tasks, is colReward
Modeling(Lambert
The first
step
of training
model,
andoutputs
therefore
RewardBench
et al.,
2024).
Reward- a reward
including text
generation,areasoning,
and factual
lectingBench
preference
data frombenchmark
a groupdesigned
of human
Individuals
presented
with prompts,
is a comprehensive
to labelers.
accuracy.
Unlike
traditional
approaches
that
dereward models,
which
addresses
lack
x, akinevaluate
to a question
or task,
and
asked the
to choose
between a set of completions, yi , answering
the
pend
on
closed-source
evaluators,
Prometheus
2
of
targeted,
standardized
evaluation
methodologies.
request. The most common case is for only two completions to be shown with measurement of
the research community with a transIt covers
diverse
domains, including
reasonpreference,
such
as win-loss-tie
or achat,
Likert
scale empowers
indicating
the magnitude of preference between
parent
and
reproducible
framework, enabling indeing,
and
safety,
and
introduces
a
novel
promptcompletions (Bai et al., 2022a), though other methods
for
labeling
exist,
such as ranking in a batch
pendent evaluations without sacrificing quality or
choice-rejection
triplet
dataset
structure
(see
Figof 4+ answers (Ouyang et al., 2022). The resultingconsistency.
data is transformed
a set of prompt-chosenThe innovationinto
of Prometheus
2 lies in
ure 20). This structure enables precise assessrejected
trios, where the chosen completion is preferred
over
the
rejected
completion
for training.
its
design
as
a
dedicated
evaluation
model
trained
ment of a reward model’s ability to align with
Training
a reward model involves training a classifier to predict the human preference probability,
human preferences by recognizing and prioritiz- on high-quality datasets that include both direct
∗
p , between
two answers,
modeled
byincludes
a Bradley-Terry
model
(Bradley
and (see
Terry,
1952):
scoring and
pairwise
ranking tasks
Figure
21).
ing high-quality
outputs.as
The
benchmark
This dual-task
framework ensures the model can
challenging test cases, such as out-of-distribution
∗
exp(r
(x,
y
))
handle
nuanced
such as subtle gram∗
1distinctions,
queries and fine-grained
like factual
p (ydifferences,
.
(1)
1 ≻ yx ∣ x) =
∗ matical errors or logical
∗ inconsistencies,
which are
exp(r
inaccuracies or logical inconsistencies. It also
pro- (x, y1 )) + exp(r (x, y2 ))
poses systematic evaluation metrics, such as rejec- critical for robust LM evaluations. Additionally,
Prometheus
incorporates likelihood
alignment techniques
Then, tion
estimate
the parameters
of the
RM by
optimizing
the2 maximum
loss as follows:
propensity,
which measures
a model’s
abilrθ (x,yrejectedto
) −closely
rθ (x,ymimic
chosen ) human preferences, achieving
ity =
to reject
content.
Empirical
L(θ, D)
E(x,ylow-quality
[log(1
+ e stud)].For language models, the RM is
chosen ,yrejected )∼D
state-of-the-art performance in agreement with huies
within
RewardBench
analyze
various
reward
often implemented by appending a linear layer toman
predict
one logit evaluations.
or removing
final decoding
and proprietary
Its the
systematic
models trained through methods like maximum
layers and replacing them with a linear layer. At approach
inference
time,theamodel
trained
reward model
enables
to outperform
existing returns a
likelihood estimation (MLE) and direct
preference
r(x,y
1)
evaluators,
providing accurate,
consisscalar, optimization
such that P(DPO).
(y1 ≻These
y2 ∣ x)
∝ reveal
e
(whichopen-source
intuitively
is the probability
that the
completion
studies
critical
tent,
and
interpretable
assessments.
would insights,
be a preferred
is trained
indirectly via the pairwise loss). Thus, a win between
includingresponse,
the models’but
limitations
in re-

completions
y1 and y2 outputs,
is achieved
when r(x, to
y1 ) > r(x, y2 ).
jecting problematic
their susceptibility

7 Direct Preference Optimization (DPO)
training data distribution in reasoning tasks, and
performance
variability
in
instruction
adherence.
Direct Preference Optimization Direct Preference
Optimization solves the RLHF problem withBy making the dataset and codebase publicly avail- While effective, RLHF or RLAIF is often mired
out needing
to learn a separate reward model. It achieves
thisdue
bytoreparameterizing
the preferencein complexity
the challenges of reinforceable, RewardBench not only provides reproducible
based reward
function using only the policy models
(Rafailov
al., 2023)
implicitofreward
used
ment
learning et
algorithms
andThe
the necessity
an
tools for the research community but also sets a
in DPO
isstandard
a function
of the
policy
model probabilities
(i.e.
thereward
model
being
trained),
π(y∣x), a
accurately
trained
model.
Recent
research
new
for reward
model
evaluation.
has turned
Optimization
regularization constant, β, the base model probabilities,
πreftowards
(y∣x),Direct
and Preference
a partition
function Z(x):
Prometheus 2 (Kim et al., 2024b) Prometheus
(DPO), which bypasses the reward model by di2 is an open-source evaluation model developedπ(y∣x)
rectly using human preference data to fine-tune
r(x,
y)
=
β
log
+ β log Z(x).
(2)
to address key challenges in assessing language
LLMs. DPO reframes the objective from reward
π (y∣x)
ref

Given two completions to a prompt, we compare the rewards r(x, y1 ) and r(x, y2 ) as follows, where
π(y ∣x)
π(y ∣x)
the score is computed via the log ratios of π: log π (y1 ∣x) > log π (y2 ∣x) .
ref

1

ref

2

Instruction
In the field of software development, what is the meaning and significance of “Containerization”? Also, explain the role of Docker in containerization.

Response

A

Containerization, in software, is somewhat like putting
things in a box. It means you take your software and all
its parts and put it in a container. Docker is a tool that
helps with this. It helps to put the software in containers
and makes it easy to use them. [...]

Pairwise Ranking

A

vs

B

+

Containerization in software development refers to the
process of packaging up an application along with all its
related configurations files, libraries, and dependencies
required to run, into a standalone unit or a ‘container’. [...]

Response

B

Evaluation Criteria

Does the response accurately use 

specific industry terminologies and jargons?

A

+

Direct Assessment
Does the response use simple language and

explanation that are easy to understand for a beginner?

Verbal Feedback

Verbal Feedback

Both response attempt to convey the fundamental concept of containerization,
but with varying degrees of clarity and technical details. Response A approaches
the concept by likening containerization to “putting things in a box”, a metaphor
that while easy to understand, lacks the precision and industry-specific [...] 



The response effectively uses simple and accessible language to explain
containerization and Docker, which is great for beginners. The analogy of putting
things in a box is particularly helpful as it visually illustrates the concept of [...]



It can be concluded that Response B is better than Response A.

However, the response could be improved by briefly mentioning why
containerization is significant, such as its benefits in ensuring that software runs
consistently across different computing environments. It loses a point for not 

fully addressing the significance of containerization in the broader context of
software development, which could add valuable insight for the reader.

Scoring Decision

Scoring Decision

On the other hand, Response B more effectively employs technical jargon such

as “packaging”, “configuration files”, “libraries”, and “dependencies”. [...] 



B

Figure Figure
2: Comparison
direct assessment
pairwise ranking.
responses
considered
decent under
21: The of
dual-task
frameworkand
of Prometheus
2. TheBoth
figure
is copiedcould
frombe
Kim
et al. (2024b).
the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.

3 Methodology
maximization
to preference optimization, and offers a straightforward and potentially more robust
We propose
a new recipe
for training
a unified
pathway
for aligning
LLM outputs
with human
exevaluator
LM
based
on
merging
the
weights
of
pectations. This section delves into the methodolomodels
trained forDPO,
directexploring
assessment
pairwise
gies
underpinning
howand
approaches
ranking.
We
begin
with
background
on
direct
aslike SLiC-HF, β-DPO, sDPO, and others leverage
sessment
and
pairwise
ranking
for
evaluator
LMs
preference data to enhance LLM alignment without
(Section
3.1, of3.2),
followed
the construction
the
overhead
traditional
RLby
frameworks.
process of our training data (Section 3.3). Finally,
we present
our methods to train the state-of-the-art
7.1
SLiC-hf
evaluator LM, Prometheus 2 models (Section 3.4).
SLiC-HF (Zhao et al., 2023) leverages Sequence
Likelihood Calibration to optimize LLMs based
3.1 Direct Assessment
on human feedback without relying on rewardDirect reinforcement
assessment is mapping
instruction
and
based
learning,an
using
human iprefresponse
r intoa simpler,
a scalar contrastive
value scoresetup.
s, such
as
erence
data
This
f
:
(i,
r)
→
7
s
where
s
∈
R.
For
the
scorisdirect
achieved by using a rank calibration loss to dising range,
we use positive
a 1-5 Likert
scale scoring.
tinguish
between
and negative
sequences.
Prior
works
have
identified
several
recipes
to
Given an input sequence x, SLiC-HF pairs
human+
align the scores
provided
by evaluator
LMs
(sLM
preferred
sequences
y (positive)
with
less
pre-)
and thesequences
scores assigned
by humans
(shuman
). For
ferred
y − (negative)
and
encourages
instance,
al. (2023a)
Zheng etoal.y +(2023)
the
modelLiu
to et
assign
higherand
likelihood
over
−
that it is crucial
to add a reference
yhave
. shown
The calibration
loss function,
Lcal (θ) an=
− |x)),
swer a as
input
toθ (y
the+ evaluator
LM
to
maximize
incormax(0,
β−
log P
|x) + log P
(y
θ
the correlation
sLMβ and
shuman
. Also,
porates
a marginbetween
parameter
to ensure
adequate
Zheng et al.
(2023)preferred
and Ye et
al.non-preferred
(2023) showed
separation
between
and
sethat
prompting
the
language
model
to
write
verbal
quences.
feedback
vr before
s also
improves
correlation
SLiC-HF
employs
two
primarytheapproaches:
between
s
and
s
.
Lastly,
Ye
et
al. (2023)
LM
SLiC-HF-direct andhuman
SLiC-HF-sample-rank.
SLiCand
Kim
et
al.
(2023)
showed
that
by
HF-direct uses raw human feedback dataexplicitly
(without
integrating
evaluation
users
can define
filtering
or ranking)
to criteria
calibrate, the
likelihood
of
the
standards
for
model
assessment,
ensuring
evalsequences directly. This direct application min-

uations
are flexible but
to specific
needsfrom
rather
than
imizes complexity
may suffer
out-ofgeneric
qualities.
Specifically,
e
is
represented
as
distribution examples if the feedback data does not
amatch
scoremodel
rubricoutputs.
including
a description for the criSLiC-HF-sample-rank,
on
teria
itselfhand, ainvolves
set of descriptions
for each candiscore
the other
generating multiple
between
the scoring
range.input,
This is
expressed
date sequences
for a given
then
selectingas:the
best onefusing: a(i,
ranking
r, a, e)or
7→reward
(vr , s)model. In this
direct
(1)
approach, the candidates are generated by sampling
where s ∈ {1, 2, 3, 4, 5}
and ranking, often employing a pairwise ranking
3.2
Ranking
modelPairwise
that has been
trained to predict human preferences. ranking is mapping an instruction i and
Pairwise
two pair of responses (rm , rn ) into either i or j,
7.2 as
DPO
such
fpair : (i, rm , rn ) 7→ s where s ∈ {m, n}.
Similar
to directDPO
assessment,
prior
works
have
Similar to Slic-hf,
(Rafailov
et al.,
2024)
byidentified
integrating
a reference
answer
a and
passes the that
iterative
sampling
complexities
of RLHF
verbal
feedback
vrm ,rn intooptimization
the evaluation
pipeline
by utilizing
closed-form
with
a simis
et al., objective
2023; Li that
et al.,
2023b,a).
plecrucial
binary (Zheng
classification
models
prefIn
addition,
to support granular assessment under
erences
directly.
custom
criterion,
we add which
the evaluation
In contrast
to RLHF,
typicallycriteria
trains ea
as
input to
the evaluator
LM (Ye
et al., 2023;
Kim
separate
reward
model, DPO
implicitly
optimizes
et
To preference
the best of our
knowledge,
we are
foral.,
the2023).
desired
function
by adjusting
the first
to directly.
study suchThis
fine-grained
evaluation
policy
is achieved
throughina
pairwise
ranking settings.
This where
is expressed
as:
re-parameterization
approach,
the model’s
outputsf approximate
an optimal policy under the
pair : (i, rm , rn , a, e) 7→ (vrm ,rn , s)
(2)
Bradley-Terry model—a commonly used statistical
where s ∈ {m, n}
model for paired preference data. A key insight in
In pairwise
the evaluation
criteria
e do
DPO
is using aranking,
closed-form
expression
to directly
not
include
set of policy
descriptions
forofeach
score;
represent
theaoptimal
in terms
the learned
instead,
only
the
description
of
the
evaluation
cripreference probabilities. The derived policy forterion
itself.
Also,
it
is
noteworthy
that
the
verbal
mula avoids iterative policy updates and instead
feedback
and
relies on avrclassification-style
loss, which is comm ,rn compares the commonalities
differences
between
r
and
r
concerning
e.
m likelihood
n
puted by comparing the
of preferred and

dis-preferred responses. The binary cross-entropy
loss between these likelihoods serves as the primary optimization metric, ensuring that the model
output aligns with human preferences in a stable
manner.
7.3

β-DPO

Although DPO has gained attention as a streamlined alternative to RLHF, the static nature of
DPO’s β parameter—a hyperparameter governing
the balance between model preference alignment
and retention of original model traits—limits its
robustness across diverse data qualities. The βDPO (Wu et al., 2024a) method introduces a dynamic calibration mechanism for the β parameter
by leveraging batch-level data quality assessments.
A batch-specific β adjustment responds to the informativeness of the pairwise data in each batch.
Specifically, β is adapted based on the mean reward discrepancy within each batch: for closely
matched pairs (low gap), β is decreased to enable more assertive updates, while for more distinct pairs (high gap), β is increased to temper the
updates, thus avoiding overfitting. To implement
this, the β parameter for each batch is computed as
βbatch = [1 + α(Ei∈batch [Mi ] − M0 )]β0 , where Mi
is the individual reward discrepancy, M0 is a baseline threshold updated via a moving average, and
α scales the discrepancy’s impact. Additionally,
β-DPO incorporates a filtering mechanism guided
by β, selecting the top 80% most informative samples within each batch by estimating the reward
discrepancy distribution.
7.4

sDPO

Another problem of Traditional DPO is to use entire preference datasets in a single step, aligning
models by comparing their outputs against a single reference model. In contrast, sDPO (Kim et al.,
2024a) partitions these datasets and feeds them into
the training process incrementally. This method allows each training step to use a more aligned model
from the prior step as the reference, creating a progressively refined alignment path.
sDPO begins with a SFT base model that serves
as the initial reference model. At each step, a portion of the preference data is used to align the target model, and the aligned model from the previous step becomes the reference model for the next.
This iterative setup allows the reference model’s
alignment quality to gradually improve, offering
a progressively higher standard, or lower bound,

for each subsequent alignment step. sDPO modifies the DPO loss by introducing an evolving lower
bound through the increasingly aligned reference
models. The objective of each step’s training is to
maximize the preference score by differentiating
the target model’s log probability ratios for chosen
versus rejected responses relative to the reference
model. This approach creates an internal progression from easier to more challenging preference
optimization, akin to curriculum learning. Additionally, sDPO suggests an easy-to-hard partitioning strategy for preference data, where early chunks
consist of data on which the model performs well,
helping stabilize early alignment and intensify difficulty as the steps advance, thus reinforcing the
alignment through a structured optimization path.
7.5

RSO

RSO (Liu et al., 2023a) centers on the development
of Statistical Rejection Sampling Optimization, designed to refine language model alignment with
human preferences by addressing data distribution
limitations inherent in SLiC and DPO. RSO begins
by constructing a reward-ranking model based on
a human preference dataset, which provides pairwise comparisons of output quality. This rewardranking model then guides the statistical rejection
sampling process, allowing the system to generate
response pairs that closely approximate an optimal
target policy. Unlike SLiC, which samples pairs
from a SFT policy, RSO selects candidate pairs
through a controlled rejection sampling approach.
This approach first samples from the SFT policy
and then probabilistically accepts or rejects samples based on how closely they match the desired
distribution according to the reward-ranking model.
The sampling mechanism emphasizes accuracy by
progressively recalculating the acceptance criteria,
thus continuously refining the sampled distribution
toward the optimal policy. RSO then fits the model
to these preference-labeled pairs using tailored loss
functions, such as hinge or sigmoid-norm, to ensure alignment without relying on explicit reinforcement learning structures.
7.6

GPO

GPO (Tang et al., 2024) aligns large models with
human feedback by optimizing over offline datasets.
The core methodology in GPO is creating a generalized framework for offline preference optimization
by using a family of convex functions to parameterize loss functions. Existing methods such as DPO

Figure 22: Overview of sDPO where preference datasets are divided to be used in multiple steps. The figure is
borrowed from Kim et al. (2024a).

Figure 23: RSO fits a pairwise reward-ranking model
from human preference data. The figure is borrowed
from Liu et al. (2023a).

and SLiC are claimed as specific instances of this
general approach, depending on the convex function chosen (e.g., logistic for DPO and hinge for
SLiC). GPO further extends to variants by allowing
flexibility in the convex function, defining a broad
range of preference optimization strategies with
distinct regularization strengths. GPO provides a
Taylor expansion around ρθ = 0 to approximate
and analyze the loss functions. This approximation reveals that the GPO loss dynamically balances preference optimization and regularization
by adapting to the chosen convex function’s properties. For instance, by choosing a function with a
rapidly decaying tail, GPO enforces stronger regularization, constraining the learned policy closer
to the reference model. In contrast, slower decaying functions lead to more flexible policies with
potentially greater divergence from the reference
policy, which could increase model expressiveness
but may require more careful tuning of the regularization coefficient, β.
7.7

DRO

DRO (Richemond et al., 2024) aims to improve
LLM alignment by using single-trajectory data
rather than traditional, costly preference data. Central to the DRO framework is the construction of
a single, quadratic objective function that approx-

Figure 24: A simplified illustration of reward modeling
and online iterative RLHF. The figure is borrowed from
Dong et al. (2024).

Figure 25: Illustration of our implementation of iterative
direct preference learning. The figure is borrowed from
Dong et al. (2024).

imates optimal policy and value functions in the
single-trajectory setting. Here, the primary goal
is to avoid pairwise preferences and instead use a
direct feedback score (like a thumbs-up or thumbsdown). DRO begins by defining a regularized objective function where the policy optimization is
guided by a KL divergence term, maintaining consistency with a reference policy, and incorporates a
reward signal for each single trajectory. The DRO
loss function is crafted as a sum of squared residuals between the observed reward and a computed
expected value adjusted by the policy and reference
terms. Additionally, DRO uses an iterative process where gradient updates are applied to both the
policy and value function parameters to minimize

empirical loss. This setup includes a regularization
parameter to balance the policy updates against the
reference model’s stability.

8

Analysis of DPO

While the simplicity and efficiency of DPO make
it an appealing choice, its practical implementation reveals challenges and opportunities for improvement. This section delves into the safety implications of DPO, particularly in how it handles
harmful outputs, and explores DPO variants, which
aim to optimize the trade-off between minimizing
harmful content and maintaining generative diversity. We reveal studies that highlight the theoretical
and practical considerations that define the effectiveness and limitations of DPO-based methods in
achieving safe, reliable, and high-interpretability
LLMs.
8.1

Safety

D2 O (Duan et al., 2024).

D2 O is designed to
align LLMs with human values by training on negative examples, such as harmful or ethically problematic outputs. It optimizes a distribution-level
Bradley-Terry preference model, which contrasts
the model’s responses with the negative samples
and encourages the model to reduce harmfulness
without introducing harmful biases from positive
responses. The optimization process in D2 O avoids
catastrophic forgetting—a common problem when
the model is forced to only minimize negative outputs—which can lead to the model forgetting how
to generate useful, informative content. This is
achieved by progressively sampling self-generated
responses during training and maximizing the difference between these and the human-annotated
negative samples, maintaining a balance between
exploration and the minimization of harmful content. D2 O demonstrates that it upper bounds the
effectiveness of previous methods like Instancelevel DPO. This means D2 O minimizes the negative content while enhances the model’s ability to
explore diverse responses, improving robustness
and response quality without overfitting to negative
samples.
NPO (Zhang et al., 2024b). NPO builds on principles of preference optimization by utilizing only
negative samples to refine unlearning in language
models. NPO minimizes a loss function that selectively decreases model confidence on data designated for unlearning. This loss function is de-

rived from the DPO but focuses solely on discouraging specific outputs instead of comparing both
preferred and less preferred responses. In implementation, the NPO loss adaptively weights each
gradient step, reducing the influence of already
unlearned samples by lowering their gradient contributions through a weight, which approaches zero
as the model confidence on undesirable samples
declines, slowing divergence and preventing catastrophic collapse.
8.2

Variations of DPO

DNO (Rosset et al., 2024). DNO operates
through a batched on-policy structure, which allows iterative self-improvement of the model based
on a Nash equilibrium concept. Each iteration involves the model learning a regression-based objective, where it aims to maximize the likelihood of
responses preferred over competing outputs in a sequence of "self-play" rounds. Pairs of responses (or
outputs) are generated from model outputs on specific prompts, ranked by a preference function that
estimates "win-rates." High-margin pairs—where
one response is significantly preferred—are retained to focus training on clear improvements.
To maintain stability and computational efficiency,
DNO implements a filtering strategy, ensuring that
only preference pairs with a high margin of preference are selected for training.
SPPO (Wu et al., 2024b). SPPO reformulates
language model optimization as a constant-sum
two-player game, where the goal is to identify a
Nash equilibrium policy through iterative updates.
Each policy update in SPPO uses a multiplicative
weight approach, a framework adapted from game
theory, specifically designed to approximate Nash
equilibria. The method proceeds by sampling responses for a given prompt and using a preference
model to assign win probabilities, indicating which
responses are preferred. In each iteration, SPPO
refines the policy by adjusting the probability distribution over responses based on observed preferences, ensuring responses with higher preference
win rates are increasingly favored.
The SPPO objective function optimizes over
each response’s probability weight to approximate an ideal Nash equilibrium. It avoids the direct computation of log-partition factors—used in
traditional preference optimization methods like
DPO—by approximating these factors with a constant, which could help reduce variance in policy

Figure 26: Safe RLHF pipeline compared to conventional RLHF method. The figure is borrowed from Dai et al.
(2023).

Figure 27: Illustration of DPO and D2 O Comparison. The figure is borrowed from Duan et al. (2024).

Figure 28: Illustration of how SPO is applied where
we are able to query the preference function online
and where we are given a fixed dataset. The figure is
borrowed from Swamy et al. (2024).

updates.
SPO (Swamy et al., 2024). SPO is rooted in the
concept of the Minimax Winner from social choice
theory, a solution concept that SPO employs to
handle complex preference aggregation tasks. At
the core, SPO frames RLHF as a two-player zerosum game where, conventionally, two policies are
pitted against each other in a "dueling" setup. How-

ever, SPO simplifies this to a single-agent, self-play
mechanism that approximates the Minimax Winner.
To accomplish this, SPO uses a preference function
that compares two trajectories and assigns a score
based on the proportion of times one trajectory is
preferred over the other. This score then serves as a
reward signal, which the agent optimizes. By leveraging the symmetry of the preference-based zerosum game, the process converges robustly even
without requiring explicit adversarial or competitive training.
DPOP (Pal et al., 2024). DPOP is designed to
address a failure mode of DPO when fine-tuning
LLMs on preference data with low edit distances.
It is found that DPO can unintentionally decrease
the likelihood of preferred responses in such cases
due to its focus on relative probabilities between
preferred and dispreferred completions. To overcome this, DPOP augments the standard DPO loss
with a corrective penalty term that ensures the log-

Figure 29: Illustration of DPOP avoiding a failure mode of DPO. The figure is borrowed from Pal et al. (2024).

likelihood of preferred completions does not fall
below the reference model’s likelihood. The full
DPOP loss function combines a standard DPO term
with a regularization term that penalizes the reduction in probability of the preferred completion.
This modification forces the model to retain a high
probability for preferred responses, mitigating the
risk of performance degradation observed in DPO,
especially when the edit distance between completion pairs is small.
TDPO (Zeng et al., 2024). TDPO refines the
DPO framework by optimizing at the token level
rather than the sentence level, addressing divergence efficiency and content diversity. TDPO formulates text generation as a Markov Decision Process, where each token is treated as an action within
a sequence. TDPO introduces token-wise KL divergence constraints, employing forward KL divergence to regulate token-level generation while
maintaining diversity. By extending the BradleyTerry model to the token level, TDPO leverages the
Regret Preference Model to compute preference
probabilities for each token pair. The loss function
incorporates both forward and reverse KL divergence terms, achieving a balance between alignment with human preferences and generative diversity. Two variants, TDPO1 and TDPO2, differ in
how they handle the KL divergence, with TDPO2
introducing a parameter α to fine-tune the divergence balance between preferred and dispreferred
responses.
8.3

Human Interpretability

ΨPO (Azar et al., 2024). ΨPO optimizes a policy by maximizing a non-linear function of the

preference probabilities, expressed as Ψ(p∗ (y ≻
y ′ |x)), where Ψ is a non-decreasing function,
while maintaining proximity to a reference policy
through KL-divergence regularization. By setting
Ψ to the identity function, Identity-Preference Optimization (IPO) is proposed as a practical version of
ΨPO that directly learns from preferences without
needing a reward model and without relying on
the Bradley-Terry assumption. IPO avoids overfitting by ensuring that policy optimization remains
regularized towards the reference policy, even in
the presence of deterministic or nearly deterministic preferences. The method employs a simple
yet effective empirical loss function, derived from
root-finding problems, which can be optimized via
gradient descent.
Unpacking DPO and PPO (Ivison et al., 2024).
Unpacking DPO and PPO investigate PPO and
DPO, and finds that PPO’s online nature allows for
dynamic adaptation and significant performance
improvements in complex domains such as reasoning and coding, where iterative feedback is essential, whereas DPO is computationally more efficient but limited in its flexibility due to its reliance
on static data. The comparative analysis suggests
that preference quality, reward model size, and
training algorithm choice significantly influence
downstream performance, with PPO generally outperforming DPO in multi-task, generalist settings,
but DPO showing strong results in tasks requiring
less complex adaptation.
Iterative Preference Learning from Human
Feedback (Xiong et al., 2024). Iterative preference learning from human feedback formulates

RLHF as a reverse-KL regularized contextual bandit problem, where the objective is to maximize
human feedback alignment while ensuring that the
learned policy does not deviate too far from the pretrained model, as captured by a KL divergence term.
Theoretical analysis reveals that the reverse-KL
constraint introduces a stochastic optimal policy,
which addresses the challenge of balancing exploration with fidelity to the pretrained policy, a key
issue in real-world alignment. In offline learning,
pessimism is applied by conservatively estimating
the reward, using uncertainty bounds derived from
concentration inequalities, which guarantees sample efficiency. The online iterative learning setting
is based on batch hybrid learning, where human
feedback is incorporated incrementally, and exploration is controlled via uncertainty-based exploration strategies. This study derives finite-sample
theoretical guarantees for both offline and online
settings, showing that the proposed methods, such
as the iterative DPO with pessimistic reward estimation and multi-step rejection sampling, outperform
existing approaches in terms of sample efficiency
and alignment performance. Furthermore, the analysis highlights the trade-off between exploration
and exploitation, proving that strategic exploration
during online learning enhances the model’s ability
to generalize to out-of-distribution data, while also
minimizing the KL divergence to the initial policy
Insights into Alignment (Saeidi et al., 2024). Insights into alignment reveal that DPO faces challenges related to overfitting and inefficient learning,
particularly in the absence of a regularization mechanism. IPO addresses these by introducing a regularization term to smooth the preference function,
effectively balancing the alignment with generalization across tasks. KTO (Ethayarajh et al., 2024),
inspired by prospect theory, eliminates the need
for paired preferences by treating each response as
either desirable or undesirable, simplifying the optimization process and reducing computational complexity. Lastly, CPO (Guo et al., 2024) improves
DPO by removing the reference model during training, reducing memory consumption and enabling
larger-scale model fine-tuning with fewer resources,
while still maintaining alignment through a combination of maximum-likelihood and preference loss.
Theoretically, these methods trade off the complexity of RL-based feedback for a more direct and
efficient alignment process, though they require
careful attention to regularization and preference

sampling to prevent model bias or poor generalization, especially in diverse task domains.
Is DPO Superior to PPO for LLM Alignment
(Xu et al., 2024a)? Theoretical analysis (Xu
et al., 2024a) reveals that DPO, by directly optimizing policies based on preference pairs, sidesteps
the need for an explicit reward model, instead framing the reward as a log ratio of policy probabilities.
However, this approach exposes DPO to significant risks of out-of-distribution bias, as it lacks the
regularizing influence of a reward function, leading to potentially skewed policy distributions when
preference data does not cover the full model output space. In contrast, PPO mitigates such issues
by leveraging a learned reward model, which introduces a KL divergence regularization term that
constrains the model’s policy updates, preventing
excessive divergence from the reference policy and
ensuring better generalization across diverse input
distributions. The study proves that PPO’s solutions are a proper subset of DPO’s, meaning any
optimal solution under PPO can also be a solution
under DPO, but DPO may produce biased solutions in cases where distribution shifts exist. Moreover, PPO’s performance is significantly enhanced
through key techniques like advantage normalization, large batch sizes, and exponential moving
average updates for the reference model, which stabilize training and improve convergence, especially
in complex tasks such as code generation.

9

Conclusion

This paper surveys the most up-to-date state of
knowledge on reinforcement learning enhanced
LLMs, attempting to consolidate and analyze the
rapidly growing research in this field. We make a
systematic review of the literature, including the basics of RL, popular RL-enhanced LLMs, studies on
two reward model-based RL techniques—RLHF
and RLAIF—and works focused on bypassing the
reward model to directly align LLM outputs with
human expectations through DPO. We hope this
work will help researchers understand the current
challenges and advancements, and motivate further
endeavors to address the deficiencies of current
RL-enhanced LLMs.

References
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed
Awadallah, Ammar Ahmad Awan, Nguyen Bach,

Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat
Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv
preprint arXiv:2404.14219.
Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh,
Pallab Bhattacharya, Annika Brundyn, Jared Casper,
Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al.
2024. Nemotron-4 340b technical report. arXiv
preprint arXiv:2406.11704.
Arash Ahmadian, Chris Cremer, Matthias Gallé,
Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,
Ahmet Üstün, and Sara Hooker. 2024. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint
arXiv:2402.14740.
01. AI, :, Alan Wake, Albert Wang, Bei Chen, C. X.
Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, Fan Zhou,
Feng Hu, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun
Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming
Song, Mou Li, Peng Liu, Qichen Hu, Shawn Wang,
Shijun Zhou, Shiyong Li, Tianhang Zhu, Wen Xie,
Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren,
Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen
Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan
Liu, and Zirui Zhang. 2024. Yi-lightning technical
report.
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury
Zemlyanskiy, Federico Lebron, and Sumit Sanghai.
2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895–
4901.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul
Christiano, John Schulman, and Dan Mané. 2016.
Concrete problems in ai safety. arXiv preprint
arXiv:1606.06565.
Anthropic. 2024. Claude 3 family.
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko,
and Daniele Calandriello. 2024. A general theoretical paradigm to understand learning from human
preferences. In International Conference on Artificial Intelligence and Statistics, pages 4447–4455.
PMLR.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073.
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners.
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,
Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi
Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297.
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,
Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E
Gonzalez, et al. 2024. Chatbot arena: An open platform for evaluating llms by human preference. arXiv
preprint arXiv:2403.04132.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. arXiv
preprint arXiv:2310.01377.
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo
Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang.
2023. Safe rlhf: Safe reinforcement learning from
human feedback. arXiv preprint arXiv:2310.12773.
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,
Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong
Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue,
Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu,
Chenggang Zhao, Chengqi Deng, Chenyu Zhang,
Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji,
Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo,
Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang,
Han Bao, Hanwei Xu, Haocheng Wang, Honghui
Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li,
Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang
Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.
Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai
Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai
Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong
Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan
Zhang, Minghua Zhang, Minghui Tang, Meng Li,
Miaojun Wang, Mingming Li, Ning Tian, Panpan
Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen,
Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan,
Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen,
Shanghao Lu, Shangyan Zhou, Shanhuang Chen,
Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng
Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing
Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,
T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu,
Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao
Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan

Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin
Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li,
Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,
Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang,
Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang
Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng
Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,
Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,
Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,
Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You,
Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu,
Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu,
Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan,
Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean
Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,
Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song,
Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu
Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan,
Xiang Liu, and Tong Zhang. 2023. Active prompting with chain-of-thought for large language models.
arXiv preprint arXiv:2302.12246.

Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997.
Louie Giray. 2023. Prompt engineering with chatgpt:
a guide for academic writers. Annals of biomedical
engineering, 51(12):2629–2633.
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. 2024. Chatglm: A family
of large language models from glm-130b to glm-4 all
tools. arXiv preprint arXiv:2406.12793.
Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun,
Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou,
Yankai Lin, et al. 2024. Controllable preference optimization: Toward controllable multi-objective alignment. arXiv preprint arXiv:2402.19085.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.
Jiwoo Hong, Noah Lee, and James Thorne. 2024.
Reference-free monolithic preference optimization
with odds ratio. arXiv preprint arXiv:2403.07691.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,
Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo,
Caiming Xiong, and Tong Zhang. 2024. Rlhf workflow: From reward modeling to online rlhf, 2024.
https://arxiv.org/abs/2405.07863.

Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang,
Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang,
Hongning Wang, Jie Tang, et al. 2024. Chatglmrlhf: Practices of aligning large language models with
human feedback. arXiv preprint arXiv:2404.00934.

Nicolai Dorka. 2024. Quantile regression for distributional reward models in rlhf. arXiv preprint
arXiv:2409.10164.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685.

Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta,
and Jacob Andreas. 2023. Guiding pretraining in
reinforcement learning with large language models.
In International Conference on Machine Learning,
pages 8657–8677. PMLR.

HuggingFaceH4. 2024. Zephyr-orpo-141b-a35b-v0.1.
https://huggingface.co/HuggingFaceH4/zephyrorpo-141b-A35b-v0.1.

Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing
Xie, and Ning Gu. 2024. Negating negatives: Alignment without human positive samples via distributional dispreference optimization. arXiv preprint
arXiv:2403.03419.

Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford,
et al. 2024. Gpt-4o system card. arXiv preprint
arXiv:2410.21276.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783.

Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu,
Valentina Pyatkin, Nathan Lambert, Noah A. Smith,
Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpacking dpo and ppo: Disentangling best practices for
learning from preference feedback. arXiv preprint
arXiv:2406.09279.

Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model
alignment as prospect theoretic optimization. arXiv
preprint arXiv:2402.01306.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen

Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi,
Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. 2023. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint
arXiv:2311.10702.

Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi
Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou
Wang, and Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of llm via a humanpreference dataset. Advances in Neural Information
Processing Systems, 36.
Subbarao Kambhampati. 2024. Can large language
models reason and plan? Annals of the New York
Academy of Sciences, 1534(1):15–18.
Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo
Kim, Yunsu Kim, Sanghoon Kim, and Chanjun Park.
2024a. sdpo: Don’t use your data all at once. arXiv
preprint arXiv:2403.19270.
Seungone Kim, Juyoung Suk, Shayne Longpre,
Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham
Neubig, Moontae Lee, Kyungjae Lee, and Minjoon
Seo. 2024b. Prometheus 2: An open source language
model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535.
Minae Kwon, Sang Michael Xie, Kalesha Bullard, and
Dorsa Sadigh. 2023. Reward design with language
models. arXiv preprint arXiv:2303.00001.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison,
LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
et al. 2024. Rewardbench: Evaluating reward
models for language modeling. arXiv preprint
arXiv:2403.13787.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie
Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif: Scaling
reinforcement learning from human feedback with ai
feedback. arXiv preprint arXiv:2309.00267.
Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre
Barut, Kai-Wei Chang, and Chengwei Su. 2024. Can
small language models help large language models
reason better?: Lm-guided chain-of-thought. arXiv
preprint arXiv:2404.03414.
Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang,
Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong
Ruan, Damai Dai, Daya Guo, et al. 2024a.
Deepseek-v2: A strong, economical, and efficient
mixture-of-experts language model. arXiv preprint
arXiv:2405.04434.
Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu,
and Yahui Zhou. 2024b. Skywork-reward: Bag of
tricks for reward modeling in llms. arXiv preprint
arXiv:2410.18451.
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe
Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi
Yang, Denny Zhou, et al. 2024c. Best practices and
lessons learned on synthetic data.
Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang,
and Yew-Soon Ong. 2024d. Large language models
as evolutionary optimizers. pages 1–8.

Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman,
Mohammad Saleh, Peter J. Liu, and Jialu Liu. 2023a.
Statistical rejection sampling improves preference
optimization. arXiv preprint arXiv:2309.06657.
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang,
Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023b. Alignbench:
Benchmarking chinese alignment of large language
models. arXiv preprint arXiv:2311.18743.
Jieyi Long. 2023. Large language model guided tree-ofthought. arXiv preprint arXiv:2305.08291.
Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie,
and Junge Zhang. 2024. Uncertainty-aware reward
model: Teaching reward models to know what is
unknown. arXiv preprint arXiv:2410.00847.
Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin,
and Chang Zhou. 2024. Online merging optimizers
for boosting rewards and mitigating tax in alignment.
arXiv preprint arXiv:2405.17931.
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,
Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
and Yejin Choi. 2022. Quark: Controllable text
generation with reinforced unlearning. Advances
in neural information processing systems, 35:27591–
27609.
Yecheng Jason Ma, William Liang, Guanzhi Wang, DeAn Huang, Osbert Bastani, Dinesh Jayaraman, Yuke
Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large
language models. arXiv preprint arXiv:2310.12931.
Mistral AI. 2024.
Mixtral-8x22b-v0.1.
https://huggingface.co/mistralai/Mixtral-8x22Bv0.1.
Tong Mu, Alec Helyar, Johannes Heidecke, Joshua
Achiam, Andrea Vallone, Ian Kivlichan, Molly Lin,
Alex Beutel, John Schulman, and Lilian Weng. 2024.
Rule based rewards for language model safety. arXiv
preprint arXiv:2411.01111.
Nexusflow. 2024. Athene-llama3-70b: Advancing openweight chat models.
OpenAI. 2023. Gpt-4 technical report.
OpenAI.
2024a.
Hello,
https://openai.com/index/hello-gpt-4o/.

GPT-4o.

OpenAI. 2024b.
O-1: Optimization for language models with continuous integration.
https://openai.com/o1/.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744.

Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.
2024. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint
arXiv:2402.13228.

Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong
Fang, Zhan Shu, and Lei Ma. 2023. Self-refined
large language model as automated reward function
designer for deep reinforcement learning in robotics.
arXiv preprint arXiv:2309.06687.

Bhargavi Paranjape, Scott Lundberg, Sameer Singh,
Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. 2023. Art: Automatic multistep reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014.

Hao Sun. 2023. Reinforcement learning in the era of
llms: What is essential? what is needed? an rl
perspective on rlhf, prompting, and beyond. arXiv
preprint arXiv:2310.06147.

Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung
Kim, and Sanghyuk Choi. 2024. Offsetbias: Leveraging debiased data for tuning evaluators. arXiv
preprint arXiv:2407.06551.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neural Information Processing Systems, 36.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2023. Gpqa: A
graduate-level google-proof q&a benchmark. arXiv
preprint arXiv:2311.12022.
Pierre Harvey Richemond, Yunhao Tang, Daniel
Guo, Daniele Calandriello, Mohammad Gheshlaghi
Azar, Rafael Rafailov, Bernardo Avila Pires, et al.
2024. Offline regularised reinforcement learning for
large language models alignment. arXiv preprint
arXiv:2405.19107.
Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and
Tengyang Xie. 2024. Direct nash optimization:
Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715.
Amir Saeidi, Shivanshu Verma, and Chitta Baral.
2024.
Insights into alignment: Evaluating
dpo and its variants across multiple tasks, 2024.
https://api.semanticscholar.org/CorpusID269303161.
John Schulman. 2015. Trust region policy optimization.
arXiv preprint arXiv:1502.05477.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open
language models. arXiv preprint arXiv:2402.03300.
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,
and David Krueger. 2022. Defining and characterizing reward gaming. Advances in Neural Information
Processing Systems, 35:9460–9471.

Hao Sun, Alihan Hüyük, and Mihaela van der Schaar.
2023a. Query-dependent prompt evaluation and optimization with offline inverse rl.
Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan,
Shuhe Wang, Tianwei Zhang, Jiwei Li, Fei Cheng,
Lingjuan Lyu, Fei Wu, and Guoyin Wang. 2023b.
Pushing the limits of chatgpt on nlp tasks.
Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. 2023c.
Text classification via large language models. arXiv
preprint arXiv:2305.08377.
Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang,
Fei Wu, Jiwei Li, Tianwei Zhang, and Guoyin Wang.
2023d. Sentiment analysis through llm negotiations.
arXiv preprint arXiv:2311.01876.
Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. 2024. A minimaximalist approach to reinforcement learning from
human feedback. arXiv preprint arXiv:2401.04056.
Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng,
Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko,
Bernardo Ávila Pires, and Bilal Piot. 2024. Generalized preference optimization: A unified approach to
offline alignment. arXiv preprint arXiv:2402.05749.
Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie
Millican, et al. 2023.
Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805.
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
Juliette Love, et al. 2024a. Gemma: Open models
based on gemini research and technology. arXiv
preprint arXiv:2403.08295.
Gemma Team, Morgane Riviere, Shreya Pathak,
Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak
Shahriari, Alexandre Ramé, et al. 2024b. Gemma 2:
Improving open language models at a practical size.
arXiv preprint arXiv:2408.00118.
Kimi Team, Angang Du, Bofei Gao, Bowei Xing,
Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun
Xiao, Chenzhuang Du, Chonghua Liao, Chuning

Tang, Congcong Wang, Dehao Zhang, Enming Yuan,
Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda
Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao
Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao,
Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu,
Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia
Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang,
Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan,
Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang,
Weihao Gao, Weimin Xiong, Weiran He, Weixiao
Huang, Wenhao Wu, Wenyang He, Xianghui Wei,
Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing
Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li,
Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie
Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang,
Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida
Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng
Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025.
Kimi k1.5: Scaling reinforcement learning with llms.
Reka Team, Aitor Ormazabal, Che Zheng, Cyprien
de Masson d’Autume, Dani Yogatama, Deyu Fu,
Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai
Pham, et al. 2024c. Reka core, flash, and edge: A series of powerful multimodal language models. arXiv
preprint arXiv:2404.12387.
Ryan Teknium, Jeffrey Quesnelle, and Chen Guang.
2024. Hermes 3 technical report. arXiv preprint
arXiv:2408.11857.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and finetuned chat models.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. arXiv preprint
arXiv:2310.16944.

Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying
Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.
2023. Gpt-re: In-context learning for relation extraction using large language models. arXiv preprint
arXiv:2305.02105.
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li,
Sen Song, and Yang Liu. 2024a. Openchat: Advancing open-source language models with mixed-quality
data. In The Twelfth International Conference on
Learning Representations.
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao,
and Tong Zhang. 2024b. Interpretable preferences
via multi-objective reward modeling and mixture-ofexperts. arXiv preprint arXiv:2406.12845.
Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong,
and Yangqiu Song. 2024c. Rethinking the bounds of
llm reasoning: Are multi-agent discussions the key?
arXiv preprint arXiv:2402.18272.
Shuhe Wang, Beiming Cao, Shengyu Zhang, Xiaoya Li,
Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy.
2023a. Sim-gpt: Text similarity via gpt annotated
data. arXiv preprint arXiv:2312.05603.
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,
Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.
2023b. Gpt-ner: Named entity recognition via large
language models. arXiv preprint arXiv:2304.10428.
Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi
Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang,
Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev.
2024d. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint
arXiv:2406.08673.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.
Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu,
Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan
He. 2024a. β-dpo: Direct preference optimization
with dynamic β. arXiv preprint arXiv:2407.08639.
Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. 2024b. Self-play preference optimization for language model alignment,
2024b. https://arxiv.org/abs/2405.00675.
Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,
Qian Luo, Victor Zhong, Yanchao Yang, and Tao
Yu. 2023. Text2reward: Automated dense reward
function generation for reinforcement learning. arXiv
preprint arXiv:2309.11489.
Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang,
Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang.
2024. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under
kl-constraint. In Forty-first International Conference
on Machine Learning.

Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin
Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and
Yi Wu. 2024a. Is dpo superior to ppo for llm
alignment? a comprehensive study. arXiv preprint
arXiv:2404.10719.

Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023a. Instruction tuning
for large language models: A survey. arXiv preprint
arXiv:2308.10792.

Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and
Bill Yuchen Lin. 2024b. Magpie: Alignment data
synthesis from scratch by prompting aligned llms
with nothing. arXiv preprint arXiv:2406.08464.

Yifan Zhang. 2023. Meta prompting for agi systems.
arXiv preprint arXiv:2311.11482.

An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan
Li, Dayiheng Liu, Fei Huang, et al. 2024a. Qwen2
technical report. arXiv preprint arXiv:2407.10671.
Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and
Tong Zhang. 2024b. Regularizing hidden states enables learning generalizable reward model for llms.
arXiv preprint arXiv:2406.10216.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
2024. Tree of thoughts: Deliberate problem solving
with large language models. Advances in Neural
Information Processing Systems, 36.
Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu.
2024. Beyond scalar reward model: Learning generative judge from preference data. arXiv preprint
arXiv:2410.03742.
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho,
Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
2024. Self-rewarding language models. arXiv
preprint arXiv:2401.10020.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023. Rrhf:
Rank responses to align language models with
human feedback without tears. arXiv preprint
arXiv:2304.05302.
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,
Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: A
massive multi-discipline multimodal understanding
and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9556–9567.
Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning
Yang, Haifeng Zhang, and Jun Wang. 2024. Tokenlevel direct preference optimization. arXiv preprint
arXiv:2404.11999.
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran
Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024a.
Generative verifiers: Reward modeling as next-token
prediction. arXiv preprint arXiv:2408.15240.
Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024b.
Negative preference optimization: From catastrophic
collapse to effective unlearning. arXiv preprint
arXiv:2404.05868.

Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, and
Quanquan Gu. 2024c. General preference modeling
with preference representations for aligning language
models. arXiv preprint arXiv:2410.02197.
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,
George Karypis, and Alex Smola. 2023b. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,
Mohammad Saleh, and Peter J. Liu. 2023. Slic-hf:
Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425.
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu,
Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and
Jiantao Jiao. 2024. Starling-7b: Improving helpfulness and harmlessness with rlaif. In First Conference
on Language Modeling.

