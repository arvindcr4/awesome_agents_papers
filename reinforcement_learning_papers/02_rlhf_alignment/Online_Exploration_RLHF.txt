Towards Efficient Online Exploration for Reinforcement Learning
with Human Feedback
Gen Liâˆ—â€ 

Yuling Yanâˆ—â€¡

arXiv:2509.22633v1 [stat.ML] 26 Sep 2025

September 29, 2025

Abstract
Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm
for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine
both the reward model and the policy in a data-efficient manner. By examining existing optimism-based
exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower
bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated
by this insight, we propose a new exploration scheme that directs preference queries toward reducing
uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit
model of RLHF, we establish regret bounds of order T (Î²+1)/(Î²+2) , where Î² > 0 is a hyperparameter that
balances reward maximization against mitigating distribution shift. To our knowledge, this is the first
online RLHF algorithm with regret scaling polynomially in all model parameters.

Keywords: Reinforcement learning from human feedback (RLHF), online exploration, principle of optimism, preference data

1

Introduction

Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural
language tasks, yet aligning their behavior with human preferences remains a central challenge. A widely
adopted solution is reinforcement learning with human feedback (RLHF), which fine-tunes a pretrained LLM
using human preference data (Bai et al., 2022; Christiano et al., 2017; Ziegler et al., 2019). The standard
RLHF pipeline involves three stages: (i) supervised fine-tuning (SFT) on human-written demonstrations to
produce a baseline model; (ii) training a reward model from human preference comparisons (Bradley and
Terry, 1952); and (iii) optimizing the LLM with reinforcement learning against the learned reward. This
framework has been instrumental in the success of instruction-following LLMs such as InstructGPT (Ouyang
et al., 2022) and ChatGPT (OpenAI, 2023), enabling models to produce responses that are more helpful,
safe, and aligned with human expectations.
Despite this progress, most existing RLHF implementations are offline (Azar et al., 2024; Rafailov et al.,
2024; Zhao et al., 2023): the preference data is collected once from static policies, and the reward model is
trained on this fixed dataset (Ivison et al., 2023; Shi et al., 2025; Zhu et al., 2024). While effective, offline
RLHF has inherent limitationsâ€”It cannot adaptively explore the enormous space of natural language, leading
to inefficient use of expensive human feedback. In contrast, online RLHF offers a more powerful alternative:
the policy iteratively collects new preference data, updates the reward model, and improves itself based
on these updates (Chen et al., 2024; Dong et al., 2024; Feng et al., 2025; Guo et al., 2024; Rosset et al.,
2024; Xiong et al., 2023). This interactive loop has the potential to greatly improve both alignment quality
âˆ— The authors contributed equally.

â€  Department of Statistics and Data Science, The Chinese University of Hong Kong, Hong Kong; Email: genli@cuhk.edu.hk.
â€¡ Department of Statistics, University of Wisconsin-Madison, WI 53706, USA; Email: yuling.yan@wisc.edu.

1

and sample efficiency. However, realizing this potential requires principled approaches to exploration, i.e.,
deciding which comparisons to query in order to most effectively reduce uncertainty in reward estimation.
A natural candidate for encouraging and guiding exploration is the principle of optimism (Lai and Robbins, 1985; Lattimore and SzepesvÃ¡ri, 2020), which acts as if the environment is more optimistic than
currently estimated, within the limits of statistical uncertainty based on all data that has been observed so
far. It is usually implemented by adding an uncertainty-based bonus to reward or value estimates, thereby
prioritizing actions whose values are uncertain but potentially high. This has yielded provably efficient algorithms in standard RL (see e.g., Azar et al. (2017); Jin et al. (2018); Russo and Van Roy (2013); Zanette and
Brunskill (2019)). However, extending this principle to RLHF introduces new difficulties, where feedback
comes not as a single reward but as a difference between rewards of two actions. The key challenge is to
determine the action pairs with the large uncertainties most relevant to policy improvement. A few recent
works achieved important progress towards designing sample-efficient online RLHF algorithms based on the
optimism principle (Cen et al., 2025; Xie et al., 2025; Zhang et al., 2025). However the existing theoretical
guarantees still exhibit exponential dependency on certain model parameters, which potentially leads to
inefficient exploration.
With this context, this paper makes contribution towards designing efficient online exploration schemes
for RLHF with provable guarantees. By analyzing the existing algorithms in the seminal works (Cen et al.,
2025; Xie et al., 2025; Zhang et al., 2025), we discuss their inadequacy in exploring the action pairs with
the large uncertainties most relevant to policy improvement, and construct lower bounds to show that the
exponential dependency on certain parameters is unavoidable in their regret. Based on these insights, we
propose a new exploration scheme for RLHF that adopts a different sampling protocol, and establish a regret
bound that depends polynomially on all model parameters.

2

Model set-up

Preliminaries. In RLHF, the prompt space X refers to the collection of all possible inputs or queries that
a user might provide to the model. The answer (or action) space A is the set of all possible outputs the
model can generate in reply to a given prompt. A language model is a policy Ï€ : X â†’ âˆ†(A) that defines a
probability distribution Ï€(Â· | x) over A conditioned on a prompt x âˆˆ X , specifying how likely the model is
to produce each potential response. The pipeline of RLHF starts with supervised fine-tuning (SFT), where
a reference policy Ï€ref : X â†’ âˆ†(A) is obtained by fine-tuning a pre-trained LLM on a dataset of prompts
paired with high-quality answers written by humans. SVT provides an initialization that stabilizes and
improves the effectiveness of the subsequent training stages that aligns the LLM with human preferences.
Reward modeling. To translate human preferences into a trainable objective, one need to model how
an oracle (e.g., a human annotator) rank two answers a1 and a2 given prompt x. Following a line of prior
works (e.g., Cen et al. (2025); Xie et al. (2025); Zhang et al. (2025)), we assume that preferences follow the
Bradley-Terry model (Bradley and Terry, 1952)
P(a1 â‰» a2 | x) =

exp(râ‹† (x, a1 ))
= Ïƒ (râ‹† (x, a1 ) âˆ’ râ‹† (x, a2 )) .
exp(râ‹† (x, a1 )) + exp(râ‹† (x, a2 ))

(2.1)

Here râ‹† : X Ã— A â†’ [0, rmax ] is an underlying reward function of an answer given a prompt, a1 â‰» a2 means
the answer a1 is preferred compared to a2 , and Ïƒ(x) = (1 + eâˆ’x )âˆ’1 is the sigmoid function. We also define
a policy Ï€HF to characterize human preference:
exp(râ‹† (x, a))
.
â‹†
â€²
aâ€² âˆˆA exp(r (x, a ))

Ï€HF (a | x) = P

The reward function is unknown and can be learned from e.g., an offline dataset D = {(xi , ai+ , aiâˆ’ )} comprised
of independent preference data samples using maximum likelihood estimation (MLE):
X

arg max â„“(r, D) where â„“(r, D) :=
log Ïƒ r(xi , ai+ ) âˆ’ r(xi , aiâˆ’ ) ,
(2.2)
r

D

where a preference data sample denoted by (x, a+ , aâˆ’ ) means that a+ â‰» aâˆ’ given prompt x.
2

RL fine-tuning. Given a reward model r, we seek to fine-tune the policy Ï€ to balance reward maximization
with maintaining similarity to the original model Ï€ref from the SFT stage. Towards this, we define the KLregularized reward objective


J(Ï€, r; Ï€cal ) := Exâˆ¼Ï Eaâˆ¼Ï€(Â· | x) [r(x, a)] âˆ’ Eaâˆ¼Ï€cal (Â· | x) [r(x, a)] âˆ’ Î²KL Ï€(Â· | x) âˆ¥ Ï€ref (Â· | x) .
(2.3)
Here Ï is the prompt distribution, and Î² > 0 is the regularization parameter reflecting the strength of the
KL regularization. In practice, Î² is typically chosen to be small; for instance, in InstructGPT (Ouyang
et al., 2022) the optimal value is reported to be around 0.01 and 0.02. This objective function includes a
calibration policy Ï€cal to eliminate the shift ambiguity of the reward function, as two reward functions r(x, a)
and r(x, a) + c(x) lead to the same preference model (2.1). Given any reward function r, the optimal policy
Ï€r := arg maxÏ€ J(Ï€, r; Ï€cal ) admits a closed-form expression (Rafailov et al., 2024)
Ï€r (a | x) =

Ï€ref (a | x) exp(r(x, a)/Î²)
Zr (x)

(2.4)

P
where Zr (x) = a Ï€ref (a|x) exp(r(x, a)/Î²) is the normalizing factor. Notice that the selection of Ï€cal does not
affect the optimal policy Ï€r given the reward function r. Our target is the optimal policy Ï€ â‹† that maximizes
the objective (2.3) under the true reward function r = râ‹† , namely
Ï€ â‹† := arg max J(Ï€, râ‹† ; Ï€cal ).
Ï€

(2.5)

Offline RLHF. The above framework leads to offline RLHF methods that relies on the preference dataset
D for training. Initial approaches (Christiano et al., 2017; Ouyang et al., 2022) first estimate a reward
function rb based on the preference dataset D using MLE, then optimize the KL-regularized objective (2.3)
with respect to rb. Another approach introduced by Rafailov et al. (2024) condensed these two steps into one
single step, known as direct preference optimization (DPO), which optimizes
 

i
i
X
| x)
Ï€(y+
| x) 
Ï€(yâˆ’
log Ïƒ Î² log
max
âˆ’
log
.
i | x)
i | x)
Ï€
Ï€ref (y+
Ï€ref (yâˆ’
D
The above objective avoids explicitly estimating the reward function, which can be obtained by expressing
the reward function r in the MLE formulation (2.2) with the associated optimal policy Ï€r using the closedform expression (2.4). However, as discussed in e.g., Xie et al. (2025); Zhang et al. (2025), the efficiency
of offline RLHF is limited by the coverage of the offline dataset D, and online exploration with active data
collection is necessary to achieve sample efficiency.
Online RLHF. We consider reward learning and policy learning iteratively, where in the t-th iteration we
use the current policy Ï€ (t) , obtained from previous iterations, to sample new data and subsequently update
both the reward estimate and the policy. This setup enables online exploration in RLHF by refining the
reward model and policy in tandem as new preference data is collected. We aim to minimize the regret
R(T ) :=

T
X


J(Ï€ â‹† ; râ‹† , Ï€cal ) âˆ’ J(Ï€ (t) ; râ‹† , Ï€cal ) .

(2.6)

t=1

It is worth mentioning that the choice of Ï€cal does not affect the regret. We define the following function J â‹†
that measures the optimal objective value for a given reward r:
J â‹† (r; Ï€cal ) := max J(Ï€, r; Ï€cal ) = J(Ï€r , r; Ï€cal ).
Ï€

(2.7)

This function plays an important role in the exploration algorithms.

3

RLHF with online exploration

Three recent algorithms for online RLHF are most closely related to this work: VPO (Cen et al., 2025),
XPO (Xie et al., 2025), and SELM (Zhang et al., 2025). In this section, we first analyze and discuss these
approaches, and then introduce our proposed exploration scheme.
3

3.1

Inadequacy of existing approaches

We begin by reviewing the procedure and intuition behind VPO (Cen et al., 2025). Fix a calibration policy
Ï€cal and an initial policy Ï€ (1) . For t = 1, 2, . . . , T , the t-th iteration of VPO consists of the following steps:
1. Sample a prompt xt âˆ¼ Ï and two answers at1 , at2 âˆ¼ Ï€ (t) (Â· | xt ). Query the preference oracle to obtain
pairwise comparison at+ â‰» atâˆ’ . Update the preference dataset D(t) = D(tâˆ’1) âˆª {(xt , at+ , atâˆ’ )}.
2. Update the reward model r(t+1) and the policy Ï€ (t+1) using the updated preference dataset D(t) :
r(t+1) =

(3.1a)

â„“(r, D(t) ) + Î±J â‹† (r; Ï€cal ),

arg max
r:X Ã—Aâ†’[0,rmax ]

(3.1b)

Ï€ (t+1) = arg max J(Ï€, r(t+1) ; Ï€cal ),
Ï€

where Î± > 0 is a regularization parameter, and step (3.1b) admits closed-form solution (2.4).
To illustrate the rationale behind VPO, consider the bandit case with no prompt. Step (3.1a) applies the
optimism principle, encouraging exploration based on the uncertainty in estimating the reward difference
between each action a and the calibration policy Ï€cal . Formally, it can be viewed as the Lagrangian form of
the constrained optimization problem
max Eaâˆ¼Ï€ [r(a)] âˆ’ Eaâˆ¼Ï€cal [r(a)] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref )
r,Ï€

s.t.

â„“(r, D(t) ) â‰¥ max â„“(r, D(t) ) âˆ’ B
r

for some B > 0. After the change of variable râ€² (a) = r(a) âˆ’ Eaâˆ¼Ï€cal [r(a)], this becomes
max
Eaâˆ¼Ï€ [râ€² (a)] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref )
â€²
r ,Ï€

s.t.

â„“(râ€² , D(t) ) â‰¥ max
â„“(râ€² , D(t) ) âˆ’ B,
â€²
r

Eaâˆ¼Ï€cal [râ€² (a)] = 0.

Here, the constraint set can be interpreted as a confidence region reflecting the uncertainty in estimating
each râ€² (a) from D(t) . Consequently, the updated policy Ï€ (t+1) depends both on the true reward gap r(a) âˆ’
Eaâˆ¼Ï€cal [r(a)] and on the uncertainty in estimating this gap for each action a âˆˆ A.
For intuition, suppose Ï€cal = 1a0 for some a0 âˆˆ A, and assume that the true reward gaps are small.
In this case, Ï€ (t+1) favors actions with higher estimation uncertainty relative to a0 , i.e., those a where the
estimate of r(a) âˆ’ r(a0 ) is most uncertain. However, comparing two actions a1 , a2 âˆ¼ Ï€ (t+1) reduces the
uncertainty between them, rather than the (potentially larger) uncertainty relative to a0 . This misalignment
can lead to inefficient exploration, as illustrated in the following example.
Example 1. Consider the bandit setting with three actions A = {a0 , a1 , a2 }, where the true rewards are
râ‹† (a0 ) = 1 and râ‹† (a1 ) = râ‹† (a2 ) = 0. Let the reference policy Ï€ref be uniform over A, and the calibration
policy be Ï€cal (a1 ) = Ï€cal (a2 ) = p and Ï€cal (a0 ) = 1 âˆ’ 2p for some 0 â‰¤ p < 1/4.
The following proposition shows that VPO may fail to explore efficiently in this setting The proof can
be found in Appendix A.
Proposition 1. Consider the setup in Example 1. Let the initial policy Ï€ (1) of VPO be the uniform distribution over A. Assume that rmax /Î² â‰¥ 3. For any Î± > 0, with probability at least 4/(9e), we have
J(Ï€ â‹† , râ‹† ; Ï€cal ) âˆ’ J(Ï€ (t) , râ‹† ; Ï€cal ) â‰¥

1
2

holds for any 1 < t â‰¤ exp(rmax /Î²)/2.
Letâ€™s discuss the idea behind Proposition 1 with Ï€cal = 1a0 . If the calibration action a0 is not visited
during the first t iterations, then Ï€ (t+1) will continue to favor a1 and a2 , since both gaps r(a1 ) âˆ’ r(a0 )
and r(a2 ) âˆ’ r(a0 ) remain highly uncertain. In particular, we establish that Ï€ (t+1) (a0 ) â‰¤ exp(âˆ’rmax /Î²),
which is exponentially small, implying that a0 is unlikely to be sampled in iteration t + 1. As a result, with
constant probability, a0 will not be sampled within the first O(exp(rmax /Î²)) iterations, and the resulting
highly suboptimal policy incurs linear regret over an exponentially long horizon. This example highlights
an algorithmic drawback: although VPO acknowledges uncertainty in the reward gaps between a1 and a0
(and between a2 and a0 ), it continues to encourage sampling a1 and a2 , leading primarily to comparisons
between them that fail to reduce their uncertainty relative to a0 .
4

Algorithm 1: Uncertainty-based RLHF exploration.
Input: initial policies Ï€ (0) , Ï€ (1) , regularizaton parameters {Î±t }tâ‰¥1 .
2 for t = 1 to T do
3
Sample a prompt xt âˆ¼ Ï and two answers at1 âˆ¼ Ï€ (tâˆ’1) (Â· | xt ), at2 âˆ¼ Ï€ (t) (Â· | xt ).
4
Query the preference oracle to obtain pairwise comparison at+ â‰» atâˆ’ and update the preference
dataset D(t) = D(tâˆ’1) âˆª {(xt , at+ , atâˆ’ )}.
5
Update the reward model r(t+1) and the policy Ï€ (t+1) using D(t) :

1

r(t+1) =

â„“(r, D(t) ) + Î±t J â‹† (r; Ï€ (t) ),

arg max

(3.2a)

r:X Ã—Aâ†’[0,rmax ]

Ï€ (t+1) = arg max J(Ï€, r(t+1) ; Ï€ (t) ).
Ï€

(3.2b)

where the policy update (3.2b) admits closed-form solution (2.4).
6

Output: {Ï€ (t) : 1 â‰¤ t â‰¤ T }

3.2

Our approach: exploration based on uncertainty

A natural modification to address the issue above is to change the sampling scheme so that at1 âˆ¼ Ï€ (t) and
at2 âˆ¼ Ï€cal . The intuition is that Ï€ (t) encourages to explore actions with higher estimation uncertainty relative
to the actions favored by the calibration policy Ï€cal . To effectively reduce this uncertainty, it is sensible to
compare one action drawn from Ï€ (t) with another drawn from Ï€cal . Indeed, the XPO and SELM algorithms
(Xie et al., 2025; Zhang et al., 2025) can be viewed as taking Ï€cal = Ï€ref .
However, if the fixed calibration policy Ï€cal is highly suboptimal for reward maximization (for example, if
it concentrates on a few low-reward actions), then the comparison will almost always favor at1 âˆ¼ Ï€ (t) against
at2 âˆ¼ Ï€cal , yielding little useful information. This issue is illustrated in the following example.
Example 2. Consider the bandit setting with three actions A = {a0 , a1 , a2 }, where the true rewards are
râ‹† (a0 ) = 0, râ‹† (a1 ) = rmax and râ‹† (a2 ) = rmax âˆ’ 2. Let the reference policy be Ï€ref (a0 ) = 1 âˆ’ 2/Îº, Ï€ref (a1 ) =
Ï€ref (a2 ) = 1/Îº for any Îº â‰¥ 4.
The following result shows that, when Îº is large (as we will see in Assumption 1, this corresponds to the
case where the reference policy deviates from human preference), this modified sampling schemes can lead
to inefficient exploration in this setting. The proof is deferred to Appendix B.
Proposition 2. Consider the setup in Example 2. Assume that Î² â‰¤ 1 and Îº â‰¤ exp(rmax /Î²). For any initial
policy Ï€ (1) and any Î± > 0, with probability at least 1/64, the modified exploration scheme which samples
at1 âˆ¼ Ï€ (t) and at2 âˆ¼ Ï€ref satisfies
J(Ï€ â‹† , râ‹† ; Ï€ref ) âˆ’ J(Ï€ (t) , râ‹† ; Ï€ref ) â‰¥ 0.01
for any 1 < t â‰¤ min{Îº, exp(rmax )/2}.
This lower bound suggests that relying on a fixed calibration policy can lead to inefficient exploration
over an exponentially long horizon. We will come back to this example in Section 4 after presenting our
algorithm and theoretical guarantees. This observation motivates us to update the calibration policy in each
iteration adaptively.
Uncertainty-based exploration. We propose an exploration scheme where the calibration policy evolves
with the iterations. In the t-th iteration, instead of a fixed Ï€cal , we use Ï€ (t) as the calibration policy when
optimizing r(t+1) and Ï€ (t+1) :
r(t+1) =

arg max

â„“(r, D(t) ) + Î±t J â‹† (r; Ï€ (t) ),

r:X Ã—Aâ†’[0,rmax ]

Ï€

(t+1)

= arg max J(Ï€, r(t+1) ; Ï€ (t) ).
Ï€

5

The key advantage is that Ï€ (t) improves over time, guiding exploration away from uninformative comparisons.
Since Ï€ (t) emphasizes actions with higher uncertainty relative to Ï€ (tâˆ’1) , it is natural to compare at1 âˆ¼ Ï€ (tâˆ’1)
and at2 âˆ¼ Ï€ (t) . This yields preference data that more directly reduces uncertainty, leading to more efficient
exploration. Our full exploration scheme is summarized in Algorithm 1.

4

Theoretical results

We establish theoretical guarantees for Algorithm 1 under the multi-armed bandit setting (i.e., X = âˆ…) with
A = |A|. We begin with a general regret bound, whose proof is deferred to Section 5.
Theorem 1. Let Î±t > A log T be non-decreasing in t. There exists a universal constant C > 0 such that,
with probability at least 1 âˆ’ O(T âˆ’10 ), the cumulative regret of running Algorithm 1 for T iterations satisfies
R(T ) â‰¤ Crmax A

2

p

2
+ CA2 Î±T rmax
Î±
t
t=1


 Î²

1
1
X
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Ï€HF (a+ )
Î²+1
Î±T rmax
.
Î±T rmax , T
min
Ï€HF (aâˆ’ )
Ï€ref (a+ )
â‹†

T log T + C

+ C(rmax + log T )

T
X
Armax log T

(4.1)

r â‹† (a+ )â‰¥r (aâˆ’ )

We now discuss the implications of Theorem 1. When Î² = 0, which corresponds to the case where only
reward maximization matters, the regret bound (4.1) simplifies to
r
âˆš 
t
3/2 3/2
2
e
R(T ) = O (A rmax + A rmax ) T
when
Î±t â‰ A log T +
.
Armax
When Î² > 0, the performance of the exploration algorithm becomes more intricate due to the trade-off
between reward maximization and similarity to the reference policy. To interpret the general regret bound
in this regime, we introduce the following assumption to capture the interaction between human preference
Ï€HF and the reference policy Ï€ref .
Assumption 1. There exists Îº, Ï„ â‰¥ 1 such that, for any action pair (a+ , aâˆ’ ),
Ï€HF (a+ )
â‰¥Ï„
Ï€HF (aâˆ’ )

=â‡’

Ï€ref (a+ )
â‰¥ Îºâˆ’1 .
Ï€ref (aâˆ’ )

Intuitively, Assumption 1 requires that whenever a+ is substantially more preferred than aâˆ’ under human
preference, the reference policy does not assign disproportionately higher weight to aâˆ’ than to a+ . This
is reasonable, since Ï€ref is obtained from the SFT step, where a pretrained LLM is fine-tuned on human
demonstrations already broadly aligned with preference. The quantities Îº and Ï„ capture the degree of
alignment between Ï€ref and Ï€HF , and their size reflects the influence of the reference policy on RLHF. We
note that the illustrative Example 1 satisfies Assumption 1 with Îº, Ï„ = O(1), and the parameter Îº in
Example 2 is consistent with the Îº here. Under this assumption, we obtain the following simplified regret
bound, whose proof is deferred to Appendix D.
Proposition 3. Suppose that Assumption 1 holds. Let
1

Î±t = A log T + t Î²+2

r

max

Î² 
 Î²+2

Îº

Then with probability at least 1 âˆ’ O(T âˆ’10 ), we have
Î²+1

R(T ) â‰² (Ï„ + ÎºÎ² T Î²+2 ) poly(A, rmax , log T ),
where the degree of the polynomial factor does not depend on Î².

6

Î²+1

 Î²+2
log T
.
A(rmax + log T )

Remark 1. When Îº is large, namely the reference policy deviates significantly from the human preference,
it is natural to choose a small KL regularization parameter Î² to reduce the influence of the reference policy.
In this regime, Algorithm 1 remains robust, since the regret bound scales only with ÎºÎ² . By contrast, the
lower bound in Proposition 2 suggests that the sampling protocols in prior works (Xie et al., 2025; Zhang
et al., 2025) would incur regret at least linear in Îº. This demonstrates that our strategy accommodates
scenarios with small Î², where the reference policy is poorly aligned with human preference.
Remark 2. In Appendix E, we present an alternative assumption linking human preference and the reference
policy, together with the corresponding regret guarantee.
Î²+1

Proposition 3 establishes a regret bound of order O(T Î²+2 ), with only polynomial dependence on the other
parameters. This stands in sharp contrast to
âˆš prior works (Cen et al., 2025; Xie et al., 2025; Zhang et al.,
2025), which achieved the more standard O( T ) regret but at the cost of exponential dependence on terms
such as rmax /Î². We conjecture that, for RLHF, eliminating exponential dependence inevitably requires a
slower rate in T , with the exponent governed by Î². This trade-off is intuitive: online exploration primarily
serves to learn human preference, and as the regularization parameter Î² increases, greater emphasis is placed
on preserving similarity to the reference measure. This constraint naturally slows convergence.

5

Proof of Theorem 1

5.1

Step 1: regret decomposition

In view of the optimality of r(t) (cf. equation (3.2a)), we have
â„“(r(t) , D(tâˆ’1) ) + Î±t J â‹† (r(t) ; Ï€ (tâˆ’1) ) â‰¥ â„“(râ‹† , D(tâˆ’1) ) + Î±t J â‹† (râ‹† ; Ï€ (tâˆ’1) ).
Rearrange terms to get

1  (t) (tâˆ’1)
â„“(r , D
) âˆ’ â„“(râ‹† , D(tâˆ’1) ) â‰¥ J â‹† (râ‹† ; Ï€ (tâˆ’1) ) âˆ’ J â‹† (r(t) ; Ï€ (tâˆ’1) )
Î±t
(i)

= max J(Ï€, râ‹† ; Ï€ (tâˆ’1) ) âˆ’ max J(Ï€, r(t) ; Ï€ (tâˆ’1) )
Ï€

Ï€

(ii)

(5.1)

â‰¥ J(Ï€ â‹† , râ‹† ; Ï€ (tâˆ’1) ) âˆ’ J(Ï€ (t) , r(t) ; Ï€ (tâˆ’1) ).

Here step (i) follows from the definition of J â‹† (cf. equation (2.7)), while step (ii) follows from the optimality
of Ï€ (t) (cf. equation (3.2b)). This allows us to reach the following decomposition:
Regrett := J(Ï€ â‹† , râ‹† ; Ï€ (tâˆ’1) ) âˆ’ J(Ï€ (t) , râ‹† ; Ï€ (tâˆ’1) )


â‰¤ Î±tâˆ’1 â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) ) + J(Ï€ (t) , r(t) ; Ï€ (tâˆ’1) ) âˆ’ J(Ï€ (t) , râ‹† ; Ï€ (tâˆ’1) ) .
{z
}
|
{z
} |

(5.2)

=:Î³t

=:Î¸t

In view of the definition of J (cf. equation (2.3)), we can further decompose
Î³t = Eaâˆ¼Ï€(t) [r(t) (a)] âˆ’ Eaâˆ¼Ï€(tâˆ’1) [r(t) (a)] âˆ’ Eaâˆ¼Ï€(t) [râ‹† (a)] + Eaâˆ¼Ï€(tâˆ’1) [râ‹† (a)]
= r(t) (at2 ) âˆ’ r(t) (at1 ) âˆ’ râ‹† (at2 ) + râ‹† (at1 ) + Î¾t
where Î¾t is the martingale difference sequence
Î¾t = Eaâˆ¼Ï€(t) [r(t) (a)] âˆ’ r(t) (at2 ) âˆ’ Eaâˆ¼Ï€(tâˆ’1) [r(t) (a)] + r(t) (at1 )
âˆ’ Eaâˆ¼Ï€(t) [râ‹† (a)] + râ‹† (at2 ) + Eaâˆ¼Ï€(tâˆ’1) [râ‹† (a)] âˆ’ râ‹† (at1 ).
Therefore we have
Regret =

T
X
t=1

Regrett â‰¤

T
X

Î¸t +

T
X

Î¾t +

T
X

t=1

t=1

t=1

| {z }

| {z }

|

=:Î¸

=:Î¾

7

|r(t) (at2 ) âˆ’ r(t) (at1 ) âˆ’ râ‹† (at2 ) + râ‹† (at1 )| .
{z

=:Î¶

}

(5.3)

It is straightforward to bound the second term Î¾. Notice that |Î¾t | â‰¤ 8rmax holds deterministically for any
1 â‰¤ t â‰¤ T . By the Azuma-Hoeffding inequality, with probability exceeding 1 âˆ’ O(T âˆ’10 ) we have
Î¾=

T
X

Î¾t â‰¤ C1 rmax

p

(5.4)

T log T

t=1

for some universal constant C1 > 0. In what follows, we bound the other two terms Î¸ and Î¶.

5.2

Step 2: bounding likelihood ratios

To bound Î¸, we need to analyze the regularized MLE. Notice that
Î¸t =

t
X
Ïƒ(r(t) (xi , ai+ ) âˆ’ r(t) (xi , aiâˆ’ ))
â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) )
.
= Î±tâˆ’1
log
Î±t
Ïƒ(râ‹† (xi , ai+ ) âˆ’ râ‹† (xi , aiâˆ’ ))
i=1

The following lemma is crucial for the subsequent analysis. The proof can be found in Appendix C.1.
Lemma 1. For any given reward function r : A â†’ [0, rmax ] and any 1 â‰¤ t â‰¤ T , define
âˆ†t (r) :=

t
X

log

i=1

t

Ïƒ(râ‹† (ai+ ) âˆ’ râ‹† (aiâˆ’ )) X
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) .
âˆ’
i
i
Ïƒ(r(a+ ) âˆ’ r(aâˆ’ ))
i=1

There exists some universal constant C2 > 1 such that for any fixed r, with probability at least 1 âˆ’ Î´,
v
u t
uX

log T
log t
rmax KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) log
|âˆ†t (r)| â‰¤ C2 t
+ C2 rmax log
.
Î´
Î´
i=1
Equipped with the concentration bounds in Lemma 1, we can use the standard covering argument to
derive an uniform upper bound, whose proof is deferred to Appendix C.2.
Lemma 2. There exists some universal constant C3 > 0 such that with probability exceeding 1 âˆ’ O(T âˆ’9 ),
â„“(r, D(t) ) âˆ’ â„“(râ‹† , D(t) ) â‰¤ âˆ’

t

1X
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + C3 Armax log T
2 i=1

holds for any r : A â†’ [0, rmax ] and 1 â‰¤ t â‰¤ T .
As an immediate consequence of Lemma 2, with probability exceeding 1 âˆ’ O(T âˆ’9 ),
â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) ) â‰¤ C3 Armax log T
holds for any 1 â‰¤ t â‰¤ T . Therefore
Î¸=

T
X
t=1

5.3

Î¸t =

T
X
â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) )

Î±t

t=1

â‰¤

T
X
C3 Armax log T

Î±t

t=1

.

Step 3: bounding reward errors

We first notice that

 (i)
Î±tâˆ’1 â„“(r(t) , D(tâˆ’1) ) âˆ’ â„“(râ‹† , D(tâˆ’1) ) â‰¥ max J(Ï€, râ‹† ; Ï€ (tâˆ’1) ) âˆ’ max J(Ï€, r(t) ; Ï€ (tâˆ’1) )
Ï€

Ï€

(ii)

â‰¥ J(Ï€ (t) , râ‹† ; Ï€ (tâˆ’1) ) âˆ’ J(Ï€ (t) , r(t) ; Ï€ (tâˆ’1) )

(iii)

= Eaâˆ¼Ï€(t) [r(t) (a) âˆ’ râ‹† (a)] âˆ’ Eaâˆ¼Ï€(tâˆ’1) [r(t) (a) âˆ’ râ‹† (a)]
8

(5.5)

(5.6)

â‰¥ âˆ’4rmax .

Here step (i) is an intermediate step of (5.1); step (ii) follows from the optimality of Ï€ (t) (cf. (3.2b)); step
(iii) follows from the definition of J (cf. (2.3)). This combined with Lemma 2 implies that
t
X

(5.7)


KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(t) (ai1 ) âˆ’ r(t) (ai2 ))

i=1



â‰¤ âˆ’2 â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) ) + 2C3 Armax log T â‰¤ C4 Î±t rmax ,
as long as Î±t â‰¥ A log T and C4 â‰¥ 8 + 2C3 . This implies that for any t âˆˆ [T ] and any action pair (a+ , aâˆ’ ),

C4 Î±t rmax
,
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))âˆ¥Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) â‰¤
Nt (a+ , aâˆ’ )

(5.8)

where Nt (a+ , aâˆ’ ) is the number of comparison for (a+ , aâˆ’ ) up to time t. This motivates us to decompose Î¶
according to whether Nt (a+ , aâˆ’ ) â‰« Î±t rmax : let Ï„ := 100C4 Î±T rmax and denote by tn (a+ , aâˆ’ ) the time of the
n-th comparison for (a+ , aâˆ’ ), we have
NT (a+ ,aâˆ’ )

X

2

Î¶ â‰¤ 2Ï„ A rmax +
r â‹† (a

+

X

)â‰¥r â‹† (a

r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ ) ,

n=Ï„

âˆ’)

{z

|

=:Î¶(a+ ,aâˆ’ )

}

where we denote by tn (a+ , aâˆ’ ) the time of the n-th comparison for (a+ , aâˆ’ ), and the first summation is taken
over all action pairs (a+ , aâˆ’ ) satisfying râ‹† (a+ ) â‰¥ râ‹† (aâˆ’ ). To bound each Î¶(a+ , aâˆ’ ), we need the following
technical lemma. The proof can be found in Appendix C.3.
Lemma 3. Consider any action pair (a+ , aâˆ’ ) and time t0 such that Nt0 (a+ , aâˆ’ ) â‰¥ Ï„ . There exists universal
constant C5 > 0 such that, for any t0 â‰¤ t1 < t2 â‰¤ T , with probability exceeding 1 âˆ’ O(T âˆ’10 ) we have
1/Î²

Nt2 (a+ , aâˆ’ ) âˆ’ Nt1 (a+ , aâˆ’ ) â‰¤ C5

t2
X
1
Ï€ref (aâˆ’ ) h
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))âˆ¥Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) Î²
Ï€ (a )
t=t1 +1 ref +
i
p
1
+ Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) Î² + C5 T log T .

Equipped with Lemma 3, we can bound each Î¶(a+ , aâˆ’ ) using both density ratios regarding human
feedback Ï€HF (a+ )/Ï€HF (aâˆ’ ), and regarding the reference policy Ï€ref (aâˆ’ )/Ï€ref (a+ ). The proof is deferred to
Appendix C.4.
Lemma 4. There exists universal constant C6 > 0 such that, for any action pair (a+ , aâˆ’ ), with probability
exceeding 1 âˆ’ O(T âˆ’9 ) we have
(
)

 Î²
1
1
Ï€HF (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î¶(a+ , aâˆ’ ) â‰¤ C6 (rmax + log T ) min
Î±T rmax , T
Î±T rmax
Ï€HF (aâˆ’ )
Ï€ref (a+ )


ANT (a+ , aâˆ’ ) log T p
+ C6
+ T log T rmax .
Î±T
This immediately implies that


p
AT log T
2
2
Î¶ â‰¤ 2Ï„ A rmax + C6
+ A T log T rmax
Î±T


 Î²

1
1
X
Ï€HF (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
+ C6 (rmax + log T )
min
Î±T rmax , T
Î±T rmax .
Ï€HF (aâˆ’ )
Ï€ref (a+ )
â‹†
â‹†

(5.9)

r (a+ )â‰¥r (aâˆ’ )

Putting the regret decomposition (5.3) and the bounds (5.4), (5.5) and (5.9) collectively yields the desired
regret bound (4.1).
9

6

Discussion

In this paper, we investigated the problem of efficient exploration in online RLHF. By a careful analysis of
the existing optimism-based exploration strategies, we identified a conceptual drawback in their sampling
protocol, and we proved lower bounds to show that they can lead to inefficient exploration. We then proposed
our algorithm that explicitly targets uncertainty in reward differences most relevant for policy improvement.
Under a multi-armed bandit setup of RLHF, we establish regret bounds of order T (Î²+1)/(Î²+2) , which scales
polynomially in all model parameters.
Our work opens several avenues for future investigation. An immediate question is whether the rate
T (Î²+1)/(Î²+2) is minimax optimal, or if faster rates can be achieved. Another important direction is to
refine the dependence on parameters such as A and rmax , which may be improved with sharper analysis
or alternative exploration schemes. Finally, our theoretical results are restricted to the bandit setting;
extending the analysis to richer environments that incorporate a prompt space would be an exciting step
toward bridging theory and practice in online RLHF.

Acknowledgements
G. Li is supported in part by the Chinese University of Hong Kong Direct Grant for Research and the Hong
Kong Research Grants Council ECS 2191363.

A

Proof of Proposition 1

For each t â‰¥ 1, define the event
Et := {no a0 is sampled in the first t samples}.
We will show that for any t â‰¥ 1,
P(Et ) â‰¥

2(tâˆ’1)
4
1 âˆ’ exp(âˆ’rmax /Î²)
.
9

(A.1)

Conditional on Et , it can be seen that â„“(r, D(t) ) only depends on r(a1 ) âˆ’ r(a2 ). Now we study when we
fix r(a1 ) âˆ’ r(a2 ) â‰¡ Î´ such that â„“(r, D(t) ) is fixed, when is J(Ï€, r; Ï€cal ) maximized over both Ï€ and r. By
symmetry, we can assume without loss of generality that Î´ â‰¥ 0. We can compute
J(Ï€, r; Ï€cal ) = Eaâˆ¼Ï€ [r(a)] âˆ’ Eaâˆ¼Ï€cal [r(a)] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref )
= [Ï€(a1 ) âˆ’ p][r(a1 ) âˆ’ r(a0 )] + [Ï€(a2 ) âˆ’ p][r(a2 ) âˆ’ r(a0 )] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref )
= [Ï€(a1 ) + Ï€(a2 ) âˆ’ 2p][r(a1 ) âˆ’ r(a0 )] âˆ’ Î´[Ï€(a2 ) âˆ’ p] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref ).
For fixed Ï€, we check which reward function r maximizes J(Ï€, r; Ï€cal ).
â€¢ When Ï€(a1 ) + Ï€(a2 ) > 2p, we know that
max J(Ï€, r; Ï€cal ) = rmax [Ï€(a1 ) + Ï€(a2 ) âˆ’ 2p] âˆ’ Î´[Ï€(a2 ) âˆ’ p] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref ),
r

(A.2)

which is maximized at r(a1 ) = rmax , r(a2 ) = rmax âˆ’ Î´ and r(a0 ) = 0.
â€¢ When Ï€(a1 ) + Ï€(a2 ) < 2p, we know that
max J(Ï€, r; Ï€cal ) = (rmax âˆ’ Î´)[2p âˆ’ Ï€(a1 ) âˆ’ Ï€(a2 )] âˆ’ Î´[Ï€(a2 ) âˆ’ p] âˆ’ Î²KL(Ï€ âˆ¥ Ï€ref ),
r

which is maximized at r(a1 ) = Î´, r(a2 ) = 0 and r(a0 ) = rmax .

10

(A.3)

In addition, for any policy Ï€ such that Ï€(a1 ) + Ï€(a2 ) < 2p, by considering another policy Ï€ â€² defined as
Ï€ â€² (a1 ) = 2p âˆ’ Ï€(a2 ) and Ï€ â€² (a2 ) = 2p âˆ’ Ï€(a1 ), we have
max J(Ï€ â€² , r; Ï€cal ) âˆ’ max J(Ï€, r; Ï€cal )
r

r

= rmax [Ï€ â€² (a1 ) + Ï€ â€² (a2 ) âˆ’ 2p] âˆ’ Î´[Ï€ â€² (a2 ) âˆ’ p] âˆ’ Î²KL(Ï€ â€² âˆ¥ Ï€ref )
âˆ’ (rmax âˆ’ Î´)[2p âˆ’ Ï€(a1 ) âˆ’ Ï€(a2 )] + Î´[Ï€(a2 ) âˆ’ p] + Î²KL(Ï€ âˆ¥ Ï€ref )
= Î²[KL(Ï€ âˆ¥ Ï€ref ) âˆ’ KL(Ï€ â€² âˆ¥ Ï€ref )].
Here the first relation follows from (A.2), (A.3) and the fact that Ï€ â€² (a1 ) + Ï€ â€² (a2 ) > 2p. Let x = Ï€(a1 ) and
y = Ï€(a2 ). Let
f (x, y) := KL(Ï€ âˆ¥ Ï€ref ) âˆ’ KL(Ï€ â€² âˆ¥ Ï€ref )
= x log x + y log y + (1 âˆ’ x âˆ’ y) log(1 âˆ’ x âˆ’ y) âˆ’ (2p âˆ’ x) log(2p âˆ’ x)
âˆ’ (2p âˆ’ y) log(2p âˆ’ y) âˆ’ (1 âˆ’ 4p + x + y) log(1 + x + y âˆ’ 4p).
By elementary analysis, it is straightforward to check that f (x, y) > 0 for any x, y > 0 satisfying x + y < 2p.
Therefore we have
max J(Ï€ â€² , r; Ï€cal ) > max J(Ï€, r; Ï€cal ).
r

r

Therefore in order to maximize â„“(r, D ) + Î±J (r; Ï€cal ), the following statement always holds regardless of
the value of Î´:

r(t+1) (a0 ) = 0, max r(t+1) (a1 ), r(t+1) (a2 ) = rmax .
(t)

â‹†

This immediately implies that
Ï€ (t+1) (a0 ) =

1
exp(r(t+1) (a0 )/Î²)
â‰¤
.
(t+1)
2 + exp(rmax /Î²)
exp(r
(a0 )/Î²) + exp(r(t+1) (a1 )/Î²) + exp(r(t+1) (a2 )/Î²)

Therefore conditional on Et , we know that
P(Et+1 |Et ) â‰¥ 1 âˆ’ Ï€ (t+1) (a0 )

2


â‰¥

1
1 + exp(âˆ’rmax /Î²)

2

2
â‰¥ 1 âˆ’ exp(âˆ’rmax /Î²) .

This relation, together with

4
,
9
establishes the statement (A.1). This immediately implies that, for any t â‰¤ exp(rmax /Î²)/2,
P(E0 ) = Ï€ (1) (a1 ) + Ï€ (1) (a2 )

P(Et ) â‰¥

2

=

2(tâˆ’1)
exp(rmax /Î²)
4
4
4
1 âˆ’ exp(âˆ’rmax /Î²)
â‰¥ 1 âˆ’ exp(âˆ’rmax /Î²)
â‰¥
â‰¥ 0.16.
9
9
9e

Finally, when Et holds, we have
J(Ï€ â‹† ; râ‹† , Ï€cal ) âˆ’ J(Ï€ (t) ; râ‹† , Ï€cal ) = Ï€ â‹† (a0 ) âˆ’ Ï€ (t) (a0 ) âˆ’ Î²KL(Ï€ â‹† âˆ¥Ï€ref ) + Î²KL(Ï€ (t) âˆ¥Ï€ref ).
We have
Ï€ â‹† (a0 ) =

exp(1/Î²)
,
exp(1/Î²) + 2

Ï€ â‹† (a1 ) = Ï€ â‹† (a2 ) =

1
.
exp(1/Î²) + 2

Therefore we have
KL(Ï€ â‹† âˆ¥ Ï€ref ) = log 3 + Ï€ â‹† (a0 ) log Ï€ â‹† (a0 ) + Ï€ â‹† (a1 ) log Ï€ â‹† (a1 ) + Ï€ â‹† (a2 ) log Ï€ â‹† (a2 )
exp(1/Î²)
exp(1/Î²)
2
1
log
+
log
exp(1/Î²) + 2
exp(1/Î²) + 2 exp(1/Î²) + 2
exp(1/Î²) + 2
exp(1/Î²)
= log 3 + Î² âˆ’1
âˆ’ log[exp(1/Î²) + 2].
exp(1/Î²) + 2
= log 3 +

11

In addition, when E tâˆ’1 happens, we know that
KL(Ï€ (t) âˆ¥ Ï€ref ) = log 3 + Ï€ (t) (a0 ) log Ï€ (t) (a0 ) + Ï€ (t) (a1 ) log Ï€ (t) (a1 ) + Ï€ (t) (a2 ) log Ï€ (t) (a2 )


Ï€ (t) (a1 ) + Ï€ (t) (a2 )
â‰¥ log 3 + Ï€ (t) (a0 ) log Ï€ (t) (a0 ) + Ï€ (t) (a1 ) + Ï€ (t) (a2 ) log
2
(t)


1
âˆ’
Ï€
(a
)
0
= log 3 + Ï€ (t) (a0 ) log Ï€ (t) (a0 ) + 1 âˆ’ Ï€ (t) (a0 ) log
2
(i)

(ii)

â‰¥ log 3 âˆ’ log 2 âˆ’ 0.16.
Here step (i) uses Jensenâ€™s inequality for convex function f (x) = x log x; step (ii) holds since the function
g(x) = x log x + (1 âˆ’ x) log(1 âˆ’ x)/2 is monotonically decreasing for 0 < x < 1/3, and we have
Ï€ (t) (a0 ) â‰¤

1
1
â‰¤
â‰¤ 0.046
2 + exp(rmax /Î²)
2 + exp(3)

provided that rmax /Î² â‰¥ 3. We have
J(Ï€ â‹† ; râ‹† , Ï€cal ) âˆ’ J(Ï€ (t) ; râ‹† , Ï€cal ) = Ï€ â‹† (a0 ) âˆ’ Ï€ (t) (a0 ) âˆ’ Î²KL(Ï€ â‹† âˆ¥ Ï€ref ) + Î²KL(Ï€ (t) âˆ¥ Ï€ref )

â‰¥ Î² log exp(1/Î²) + 2 âˆ’ (log 2 + 0.16)Î² âˆ’ 0.046
â‰¥ 1/2,
where the last relation holds for any Î² > 0.

B

Proof of Proposition 2

Let T = min{Îº, exp(rmax )/2}, and define the events

A := at2 = a0 for all 1 â‰¤ t â‰¤ T
and

E := {at1 â‰» at2 or at1 = at2 for all 1 â‰¤ t â‰¤ T }.

We can check that when Îº â‰¥ 5,
P(A) = [Ï€ref (a0 )]T â‰¤ (1 âˆ’ 2Îºâˆ’1 )Îº â‰¥

1
.
16

Conditional on A, we know that when rmax â‰¥ 1,
P(E | A) â‰¥

T  exp(r
exp(rmax )/2
 exp(r
1
max âˆ’ 1)
max âˆ’ 1)
â‰¥
â‰¥ .
1 + exp(rmax âˆ’ 1)
1 + exp(rmax âˆ’ 1)
4

Conditional on A and Et , for any 1 â‰¤ t â‰¤ T âˆ’ 1, all the preference data in D(t) are of form at1 â‰» at2 . In this
case, it is straightforward to check that the reward function that maximizes â„“(r, D(t) ) + Î±J â‹† (r; Ï€ref ) is
r(t+1) (a0 ) = 0,

r(t+1) (a1 ) = r(t+1) (a2 ) = rmax .

This immediately implies that
Ï€ (t+1) (a0 ) =

Îºâˆ’2
,
Îº âˆ’ 2 + 2 exp(rmax /Î²)

Ï€ (t+1) (a1 ) = Ï€ (t+1) (a2 ) =

exp(rmax /Î²)
.
Îº âˆ’ 2 + 2 exp(rmax /Î²)

On the other hand, we know that
Ï€ â‹† (a0 ) =

Îºâˆ’2
,
Îº âˆ’ 2 + exp(rmax /Î²) + exp((rmax âˆ’ 2)/Î²)
12

exp(rmax /Î²)
,
Îº âˆ’ 2 + exp(rmax /Î²) + exp((rmax âˆ’ 2)/Î²)
exp((rmax âˆ’ 2)/Î²)
Ï€ â‹† (a2 ) =
.
Îº âˆ’ 2 + exp(rmax /Î²) + exp((rmax âˆ’ 2)/Î²)

Ï€ â‹† (a1 ) =

For any 2 â‰¤ t â‰¤ T , we first lower bound
J(Ï€ â‹† ; râ‹† , Ï€ref ) âˆ’ J(Ï€ (t) ; râ‹† , Ï€ref ) â‰¥ J(Ï€Î¸â‹† ; râ‹† , Ï€ref ) âˆ’ J(Ï€1 ; râ‹† , Ï€ref )

(B.1)

for any Î¸â‹† âˆˆ [0, 1], where we define Ï€Î¸ := Î¸Ï€ (t) + (1 âˆ’ Î¸)Ï€ â‹† , and the above relation follows from the optimality
of Ï€ â‹† . Recall the definition
J(Ï€; râ‹† , Ï€ref ) = Ï€(a1 )rmax + Ï€(a2 )(rmax âˆ’ 2) âˆ’ Î²

2
X

Ï€(ai ) log

i=0

Ï€(ai )
,
Ï€ref (ai )

we can compute
ï£¹
râ‹† (a0 ) âˆ’ Î² log[Ï€(a0 )/Ï€ref (a0 )] âˆ’ Î²
ï£º
ï£¯
âˆ‡Ï€ J(Ï€; râ‹† , Ï€ref ) = ï£° râ‹† (a1 ) âˆ’ Î² log[Ï€(a1 )/Ï€ref (a1 )] âˆ’ Î² ï£»
râ‹† (a2 ) âˆ’ Î² log[Ï€(a2 )/Ï€ref (a2 )] âˆ’ Î²
ï£®

and

âˆ‡2Ï€ J(Ï€; râ‹† , Ï€ref ) = âˆ’Î²diag {Ï€(a0 ), Ï€(a1 ), Ï€(a2 )}

âˆ’1

.

It is straightforward to check that
ï£¹
ï£® ï£¹
1
0
ï£º
ï£¯ ï£º
ï£¯
âˆ‡Ï€ J(Ï€ (t) ; râ‹† , Ï€ref ) = ï£° 0 ï£» + const Â· ï£° 1 ï£» .
1
âˆ’1
ï£®

(B.2)

Since Ï€ (t) (a0 ) < Ï€ â‹† (a0 ), Ï€ (t) (a1 ) < Ï€ â‹† (a1 ) and Ï€ (t) (a2 ) > Ï€ â‹† (a2 ), we know that for any Î¸ âˆˆ [0, Î¸â‹† ]

âˆ’1
âˆ‡2Ï€ J(Ï€Î¸ ; râ‹† , Ï€ref ) âª° âˆ’Î²diag Ï€ (t) (a0 ), Ï€ (t) (a1 ), Î¸â‹† Ï€ (t) (a2 ) + (1 âˆ’ Î¸â‹† )Ï€ â‹† (a2 )
.

(B.3)

Therefore we have
(i)

J(Ï€Î¸â‹† ; râ‹† , Ï€ref ) âˆ’ J(Ï€1 ; râ‹† , Ï€ref ) â‰¥ Î¸â‹† âˆ‡Ï€ J(Ï€ (t) ; râ‹† , Ï€ref )âŠ¤ (Ï€ â‹† âˆ’ Ï€ (t) )

Î²Î¸â‹†2 â‹†
âˆ’1 â‹†
âˆ’
(Ï€ âˆ’ Ï€ (t) )âŠ¤ diag Ï€ (t) (a0 ), Ï€ (t) (a1 ), Î¸â‹† Ï€ (t) (a2 ) + (1 âˆ’ Î¸â‹† )Ï€ â‹† (a2 )
(Ï€ âˆ’ Ï€ (t) )
2
(ii)
Î²Î¸â‹†2 â‹†
9
â‰¥ Î¸â‹† [Ï€ (t) (a2 ) âˆ’ Ï€ â‹† (a2 )] âˆ’
[Ï€ (a0 ) + Ï€ â‹† (a1 ) + Ï€ (t) (a2 )/Î¸â‹† ]
2
16
9 â‹† (t)
Î²Î¸â‹†2
â‹† (t)
â‹†
= Î¸ [Ï€ (a2 ) âˆ’ Ï€ (a2 )] âˆ’ Î²Î¸ Ï€ (a2 ) âˆ’
[1 âˆ’ Ï€ â‹† (a2 )]
32
2




9
Î²Î¸â‹†
Î²Î¸â‹†2
â‹† (t)
= 1 âˆ’ Î² Î¸ Ï€ (a2 ) âˆ’ 1 âˆ’
Î¸â‹† Ï€ â‹† (a2 ) âˆ’
32
2
2


â‹†
â‹†2
(iii)
3
9
Î²Î¸
Î²Î¸ (iv) 15 â‹† (t)
Î¸â‹†2
â‰¥
âˆ’ Î²+
Î¸â‹† Ï€ (t) (a2 ) âˆ’
â‰¥
Î¸ Ï€ (a2 ) âˆ’
.
4 32
8
2
32
2

(B.4)

Here step (i) follows from the Taylor expansion and (B.3); step (ii) utilizes (B.2) and as well as the following
relations
Ï€ (t) (a0 ) â‰¤ Ï€ â‹† (a0 ) â‰¤ 2Ï€ (t) (a0 ), Ï€ (t) (a1 ) â‰¤ Ï€ â‹† (a1 ) â‰¤ 2Ï€ (t) (a1 )
and when Î² â‰¤ 1,
Ï€ â‹† (a2 ) â‰¤

2
1
Ï€ (t) (a2 ) â‰¤ Ï€ (t) (a2 );
exp(2/Î²) + 1
4
13

(B.5)

steps (iii) and (iv) follows from (B.5) and Î² â‰¤ 1. When Îº â‰¤ exp(rmax Î²), we have
Ï€ (t) (a2 ) =

exp(rmax /Î²)
exp(rmax /Î²)
1
â‰¥
â‰¥ .
Îº âˆ’ 2 + 2 exp(rmax /Î²)
3 exp(rmax /Î²) âˆ’ 2
3

(B.6)

By taking (B.1), (B.4) and (B.6) collectively, we have
(Ï€ â‹† ; râ‹† , Ï€ref ) âˆ’ J(Ï€ (t) ; râ‹† , Ï€ref ) â‰¥

25
5 â‹† Î¸â‹†2
Î¸ âˆ’
â‰¥
> 0.01
32
2
2048

where we take Î¸â‹† = 5/32.

C

Proof of auxiliary lemmas

C.1

Proof of Lemma 1

We first express
Xi := log

Ïƒ(râ‹† (ai+ ) âˆ’ râ‹† (aiâˆ’ ))
Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))
Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
i
i
i
i
=
1{a
â‰»
a
}
log
+
1{a
â‰º
a
}
log
.
1
2
1
2
Ïƒ(r(ai+ ) âˆ’ r(aiâˆ’ ))
Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
Ïƒ(r(ai2 ) âˆ’ r(ai1 ))

It is straightforward to check that




Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))
i
i i
i
+
P
a
â‰º
a
|a
,
a
log
E Xi ai1 , ai2 = P ai1 â‰» ai2 |ai1 , ai2 log
1
2
1
2
Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
Ïƒ(r(ai2 ) âˆ’ r(ai1 ))
â‹† i
â‹† i
Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
Ïƒ(r (a1 ) âˆ’ r (a2 ))
â‹† i
â‹† i
+
Ïƒ(r
(a
)
âˆ’
r
(a
))
log
= Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) log
2
1
Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
Ïƒ(r(ai2 ) âˆ’ r(ai1 ))

â‹† i
â‹† i
i
i
= KL Ïƒ(r (a1 ) âˆ’ r (a2 )) âˆ¥ Ïƒ(r(a1 ) âˆ’ r(a2 )) .
and


|Xi | â‰¤ log 1 + exp(âˆ’r(ai+ ) + r(aiâˆ’ )) â‰¤ 2rmax .

In addition, we can compute the variance
2


Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))
âˆ’
log
Var Xi ai1 , ai2 = Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 )) log
Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
Ïƒ(r(ai2 ) âˆ’ r(ai1 ))

2
Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))
Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
= Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 )) log
âˆ’
log
Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
Ïƒ(r(ai2 ) âˆ’ r(ai1 ))

2
= Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 )) r(ai1 ) âˆ’ r(ai2 ) âˆ’ râ‹† (ai1 ) + râ‹† (ai2 ) .
In view of Lemma 5, we have

KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 ))
1
â‰¥ Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 ))
4
n


2 o
Â· min |r(ai1 ) âˆ’ r(ai2 ) âˆ’ râ‹† (ai1 ) + râ‹† (ai2 )|, r(ai1 ) âˆ’ r(ai2 ) âˆ’ râ‹† (ai1 ) + râ‹† (ai2 )

â‰¥


2
1
Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 ))Ïƒ(râ‹† (ai2 ) âˆ’ râ‹† (ai1 )) r(ai1 ) âˆ’ r(ai2 ) âˆ’ râ‹† (ai1 ) + râ‹† (ai2 ) ,
16rmax

where the last step follows from |r(ai1 ) âˆ’ r(ai2 ) âˆ’ râ‹† (ai1 ) + râ‹† (ai2 )| â‰¤ 4rmax . Therefore we have


Var Xi ai1 , ai2 â‰¤ 16rmax KL Ïƒ(râ‹† (ai+ ) âˆ’ râ‹† (aiâˆ’ )) âˆ¥ Ïƒ(r(ai+ ) âˆ’ r(aiâˆ’ )) .
In addition, we have the following deterministic bound
t
X


2
Var Xi ai1 , ai2 â‰¤ 16trmax
.

i=1

14

(C.1)

By the Freedmanâ€™s inequality (cf. Lemma 6), for any fixed r, with probability exceeding 1 âˆ’ Î´,
|âˆ†t (r)| â‰¤

t
X



Xi âˆ’ E Xi ai1 , ai2

i=1

v
u t
uX

log t
log t
â‰¤ C2 t
rmax KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) log
+ C2 rmax log
Î´
Î´
i=1
for some sufficiently large constant C2 > 0.

C.2

Proof of Lemma 2

For any fixed r : A â†’ [Â±rmax ], with probability exceeding 1 âˆ’ Î´ we have
v
u t
uX
(i)

log T
log T
|âˆ†t (r)| â‰¤ C2 t
+ C2 rmax log
rmax KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) log
Î´
Î´
i=1
(ii) 1 X

â‰¤

2

i


log T
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + 2C22 rmax log
.
Î´

Here step (i) follows from Lemma 1, and step (ii) utilizes the AM-GM inequality. This immediately implies
that
â„“(r, D(t) ) âˆ’ â„“(râ‹† , D(t) ) =

t
X

Ïƒ(r(ai+ ) âˆ’ r(aiâˆ’ ))
Ïƒ(râ‹† (ai+ ) âˆ’ râ‹† (aiâˆ’ ))

log

i=1

=âˆ’

â‰¤âˆ’

t
X


KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) âˆ’ âˆ†t (r)

i=1
t
X


log T
1
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + 2C22 rmax log
.
2 i=1
Î´

(C.2)

Then we explore the Lipschitzness continuity of the above functionals of r. For any two fixed reward
functions r, râ€² : A â†’ [Â±rmax ], we have
â„“(r, D

(t)

â€²

) âˆ’ â„“(r , D

(t)

) =

â‰¤

t
X
i=1
t
X

log[Ïƒ(r(ai+ ) âˆ’ r(aiâˆ’ ))] âˆ’ log[Ïƒ(râ€² (ai+ ) âˆ’ râ€² (aiâˆ’ ))]
|r(ai+ ) âˆ’ r(aiâˆ’ ) âˆ’ râ€² (ai+ ) + râ€² (aiâˆ’ )| â‰¤ 2T âˆ¥r âˆ’ râ€² âˆ¥âˆž ,

(C.3)

i=1

where the penultimate step follows from d log(Ïƒ(x))/dx = Ïƒ(âˆ’x) â‰¤ 1. Similarly, for any x, y, Î´ âˆˆ R, we have


1 âˆ’ Ïƒ(y + Î´)
Ïƒ(y + Î´)
+ (1 âˆ’ Ïƒ(x)) log
KL Ïƒ(x) âˆ¥ Ïƒ(y) âˆ’ KL Ïƒ(x) âˆ¥ Ïƒ(y + Î´) = Ïƒ(x) log
Ïƒ(y)
1 âˆ’ Ïƒ(y)
â‰¤ Ïƒ(x)|Î´| + (1 âˆ’ Ïƒ(x))|Î´| = |Î´|.
This implies that
t
X


KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 ))

i=1

âˆ’

t
X

KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(râ€² (ai1 ) âˆ’ râ€² (ai2 ))

i=1

15



â‰¤ 2âˆ¥r âˆ’ râ€² âˆ¥âˆž .

(C.4)

Let NÎµ be an Îµ-net of [âˆ’rmax , rmax ]A (or equivalently, the function space of r : A â†’ [Â±rmax ]) under the
â„“âˆž norm such that |NÎµ | â‰¤ (2rmax /Îµ)A . By standard union bound argument and (C.2), with probability
exceeding 1 âˆ’ Î´,
â„“(r, D(t) ) âˆ’ â„“(râ‹† , D(t) ) â‰¤ âˆ’

t

|NÎµ | log T
1X
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + 2C22 rmax log
(C.5)
2 i=1
Î´

holds for any r âˆˆ NÎµ . This implies that for any r : A â†’ [Â±rmax ], there exists r0 âˆˆ NÎµ such that âˆ¥r âˆ’ râ€² âˆ¥ â‰¤ Îµ,
hence
(i)

â„“(r, D(t) ) âˆ’ â„“(râ‹† , D(t) ) â‰¤ â„“(r0 , D(t) ) âˆ’ â„“(râ‹† , D(t) ) + 2T Îµ
(ii)

â‰¤ âˆ’

t

|NÎµ | log T
1X
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r0 (ai1 ) âˆ’ r0 (ai2 )) + 2C22 rmax log
+ 2T Îµ
2 i=1
Î´

(iii)

â‰¤ âˆ’

t

1X
|NÎµ | log T
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + 2C22 rmax log
+ 4T Îµ
2 i=1
Î´

t

1X
KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(ai1 ) âˆ’ r(ai2 )) + C3 Armax log T.
â‰¤ âˆ’
2 i=1

(iv)

Here step (i) utilizes (C.3); step (ii) follows from r0 âˆˆ NÎµ and the uniform concentration bound (C.5); step
(iii) uses (C.4); step (iv) holds as long as C3 â‰« 2C22 , where we let Îµ = Armax /T and Î´ = T âˆ’10 . This
completes the proof.

C.3

Proof of Lemma 3

When Nt (a+ , aâˆ’ ) â‰¥ 100C4 Î±t rmax , we have

1
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) â‰¤
.
100

(C.6)

Now we assert that r(t) (aâˆ’ ) âˆ’ r(t) (a+ ) < 0.5 for any t â‰¥ t0 . This is because, if r(t) (aâˆ’ ) âˆ’ r(t) (a+ ) â‰¥ 0.5, we
have


KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) = KL Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) âˆ¥ Ïƒ(r(t) (aâˆ’ ) âˆ’ r(t) (a+ ))

1
.
â‰¥ KL Ïƒ(0) âˆ¥ Ïƒ(0.5) >
100
Here we use the fact that râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ) â‰¤ 0. This contradicts with (C.6). Hence we have
(C.7)

r(t) (aâˆ’ ) âˆ’ r(t) (a+ ) < 0.5.
Let p := Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) and q := Ïƒ(r(t) (aâˆ’ ) âˆ’ r(t) (a+ )). We have
(i)

(ii)

exp(r(t) (aâˆ’ ) âˆ’ r(t) (a+ )) â‰¤ 3Ïƒ(r(t) (aâˆ’ ) âˆ’ r(t) (a+ )) = 3q â‰¤ 6p + KL(p âˆ¥ q)
â‹†

â‹†

â‹†

â‹†

= 6Ïƒ(r (aâˆ’ ) âˆ’ r (a+ )) + 24KL Ïƒ(r (a+ ) âˆ’ r (aâˆ’ )) âˆ¥ Ïƒ(r

(t)

(a+ ) âˆ’ r

(t)

(C.8)

(aâˆ’ )) .

Here step (i) follows from (C.7), while step (ii) holds trivially when q â‰¤ 2p, and when q > 2p we have
KL(p âˆ¥ q) = p log

p
1âˆ’p
(q âˆ’ p)2
1
+ (1 âˆ’ p) log
â‰¥
â‰¥ q.
q
1âˆ’q
2q
8

Finally, for any t0 â‰¤ t1 < t2 â‰¤ T , we can upper bound
Nt2 (a+ , aâˆ’ ) âˆ’ Nt1 (a+ , aâˆ’ ) â‰¤

t2
X

Xi

where

i=t1 +1

16

Xi := 1{aâˆ’ is sampled in the i-th iteration}.

It is straightforward to check that Xi âˆ’ E[Xi |Fiâˆ’1 ] is a martingale difference sequence, and by the AzumaHoeffding inequality, with probability exceeding 1 âˆ’ O(T âˆ’100 ) we have
t2

X


p
e T log T
Xi âˆ’ E[Xi |Ï€ (i) , Ï€ (iâˆ’1) ] â‰¤ C

i=t1 +1

e > 0. In addition, we have
for some universal constant C
E[Xi |Ï€ (i) , Ï€ (iâˆ’1) ] â‰¤

Ï€ (i) (aâˆ’ )
Ï€ (iâˆ’1) (aâˆ’ )
+ (iâˆ’1)
.
(i)
Ï€
(aâˆ’ ) + Ï€ (iâˆ’1) (a+ )
âˆ’ ) + Ï€ (a+ )

Ï€ (i) (a

For each t âˆˆ [T ], we have
Ï€ref (aâˆ’ ) exp(r(t) (aâˆ’ )/Î²)
Ï€ (t) (aâˆ’ )
(i)
=
(t)
(t)
Ï€ (aâˆ’ ) + Ï€ (a+ )
Ï€ref (aâˆ’ ) exp(r(t) (aâˆ’ )/Î²) + Ï€ref (a+ ) exp(r(t) (a+ )/Î²)

 
Ï€ref (aâˆ’ )
exp r(t) (aâˆ’ ) âˆ’ r(t) (a+ ) /Î²
Ï€ref (a+ )
i1/Î²
Ï€ref (aâˆ’ ) h
6Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) + 24KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ ))
.
â‰¤
Ï€ref (a+ )
â‰¤

Here step (i) utilizes (2.4), while step (ii) follows from (C.8). Hence we have
Nt2 (a+ , aâˆ’ ) âˆ’ Nt1 (a+ , aâˆ’ ) â‰¤ 2

t2
X

p
Ï€ (t) (aâˆ’ )
e T log T
+ 2C
(t)
(t)
Ï€ (aâˆ’ ) + Ï€ (a+ )
t=t +1
1

1/Î²

â‰¤ C5

t2
X
1
Ï€ref (aâˆ’ ) h
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))âˆ¥Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) Î²
Ï€ (a )
t=t1 +1 ref +
i
p
1
+ Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) Î² + C5 T log T

for some sufficiently large constant C5 > 0.

C.4

Proof of Lemma 4

Let t0 be the first iteration such that

Nt0 (a+ , aâˆ’ ) â‰¥ min


1
NT (a+ , aâˆ’ ), 100C4 Î±T rmax .
2

(C.9)

In what follows, we establish the desired result under two different cases: NT (a+ , aâˆ’ ) being larger or smaller
than c0 exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))Î±T rmax for some sufficiently large constant c0 > 0.
Case 1.

When NT (a+ , aâˆ’ ) â‰¤ c0 exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))Î±T rmax , it is straightforward to show that

2
Î¶(a+ , aâˆ’ ) â‰¤ NT (a+ , aâˆ’ )rmax â‰¤ C6 exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))Î±T rmax
= C6

Ï€HF (a+ )
2
Î±T rmax
.
Ï€HF (aâˆ’ )

(C.10)

In addition, we have
Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) â‰¤ exp(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) â‰¤

c0 Î±T rmax
.
NT (a+ , aâˆ’ )

In addition, for any t0 â‰¤ t â‰¤ T , we can use (5.8) to show that

C4 Î±t rmax
2C4 Î±T rmax
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))âˆ¥Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) â‰¤
â‰¤
.
Nt (a+ , aâˆ’ )
NT (a+ , aâˆ’ )
17

(C.11)

By taking t1 = t0 âˆ’ 1 and t2 = T in Lemma 3, we have
(i)

NT (a+ , aâˆ’ ) â‰¤ 2[NT (a+ , aâˆ’ ) âˆ’ Nt0 âˆ’1 (a+ , aâˆ’ )]
(ii)
p
Ï€ref (aâˆ’ )  C5 max{c0 , 2C4 }Î±T rmax 1/Î²
+ 2C5 T log T .
â‰¤ 4T
Ï€ref (a+ )
NT (a+ , aâˆ’ )
Here step (i) follows from the definition of t0 (cf. (C.9)), while step (ii) uses the above two bounds and
Lemma 3 with t1 = t0 âˆ’ 1 and t2 = T . This immediately implies that
 Î²

p
1
Ï€ref (aâˆ’ ) Î²+1
(Î±T rmax ) Î²+1 + C7 T log T
NT (a+ , aâˆ’ ) â‰¤ C7 T
Ï€ref (a+ )
for some sufficiently large constant C7 > 0. This leads to

 Î²
Î²+2
1
p
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î±T rmax
Î¶(a+ , aâˆ’ ) â‰¤ NT (a+ , aâˆ’ )rmax â‰¤ C7 T
+ C7 T log T rmax .
Ï€ref (a+ )
Case 2.

(C.12)

When NT (a+ , aâˆ’ ) > c0 exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))Î±T rmax , we have
(i) 2
1
NT (a+ , aâˆ’ ) â‰¤ [NT (a+ , aâˆ’ ) âˆ’ Nt0 âˆ’1 (a+ , aâˆ’ )]
c0
c0
1/Î²
h
 2C Î± r
1/Î² i 2C p
(ii) 2C
Ï€ref (aâˆ’ )
4 T max
5
5
â‰¤
Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))1/Î² +
+
T
T log T
c0
Ï€ref (a+ )
NT (a+ , aâˆ’ )
c0
(iii) 4
2C5 p
Ï€ref (aâˆ’ )
â‰¤
exp(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))1/Î² +
T log T .
max{C5 , 2C4 C5 /c0 }1/Î² T
c0
Ï€ref (a+ )
c0

exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))Î±T rmax â‰¤

Here step (i) follows from the definition of t0 (cf. (C.9)); step (ii) utilizes Lemma 3 with t1 = t0 âˆ’ 1 and
t2 = T , as well as (C.11); step (iii) holds since Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) â‰¤ exp(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )) and
2C4 Î±T rmax
2C4 Î±T rmax
2C4
â‰¤
â‰¤
exp(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ )).
â‹†
â‹†
NT (a+ , aâˆ’ )
c0 exp(r (a+ ) âˆ’ r (aâˆ’ ))Î±T rmax
c0
This immediately implies that for some sufficiently large constant C8 > 0, we have
âˆš
Î²
 Ï€ (a )T  Î²+1
T log T
Ï€HF (a+ )
ref âˆ’
+ C8
= exp(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) â‰¤ C8
.
Ï€HF (aâˆ’ )
Ï€ref (a+ )Î±T rmax
Î±T rmax

(C.13)

Similar to (C.1), we can show that


KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ ))âˆ¥Ïƒ(r(a+ ) âˆ’ r(aâˆ’ )) = KL Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))âˆ¥Ïƒ(r(aâˆ’ ) âˆ’ r(a+ ))
(a)

1
Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))[1 âˆ’ Ïƒ(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))][r(a+ ) âˆ’ r(aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )]2
16rmax
(b)
1
exp(râ‹† (aâˆ’ ) âˆ’ râ‹† (a+ ))[r(a+ ) âˆ’ r(aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )]2 .
â‰¥
64rmax
â‰¥

Here step (a) follows from Lemma 5; step (b) makes use of the fact that râ‹† (aâˆ’ ) â‰¤ râ‹† (a+ ). Hence we have
[r(a+ ) âˆ’ r(aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )]2
â‰¤ 64rmax


Ï€HF (a+ )
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(a+ ) âˆ’ r(aâˆ’ )) .
Ï€HF (aâˆ’ )

(C.14)

In addition, we have
t
X

 (i)


KL Ïƒ(râ‹† (ai1 ) âˆ’ râ‹† (ai2 )) âˆ¥ Ïƒ(r(t) (ai1 ) âˆ’ r(t) (ai2 )) â‰¤ âˆ’2 â„“(r(t) , D(t) ) âˆ’ â„“(râ‹† , D(t) ) + 2C3 Armax log T

i=1

18

(ii)

â‰¤ 2Î±t Î³t + 2C3 Armax log T
Here step (i) follows from Lemma 2, while step (ii) utilizes (5.6) and the definition of Î³t (cf. (5.2)). This
immediately implies that
 2Î±t Î³t + 2C3 Armax log T
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(t) (a+ ) âˆ’ r(t) (aâˆ’ )) â‰¤
.
(C.15)
Nt (a+ , aâˆ’ )
Therefore for any 1 â‰¤ n1 < n2 â‰¤ NT (a+ , aâˆ’ ), we have
2
 X
n2
1
r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )
n2 âˆ’ n1 n=n
1

n2
(i) X
â‰¤
[r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )]2
n=n1
(ii)

â‰¤ 64rmax

n2

Ï€HF (a+ ) X
KL Ïƒ(râ‹† (a+ ) âˆ’ râ‹† (aâˆ’ )) âˆ¥ Ïƒ(r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ))
Ï€HF (aâˆ’ ) n=n
1

(iii) 128r
max Î±T Ï€HF (a+ )

â‰¤

n1

n2
X

Ï€HF (aâˆ’ ) n=n

2
Î³tn + 128C3 Armax
log T

1

n2 âˆ’ n1 Ï€HF (a+ )
.
n1 Ï€HF (aâˆ’ )

(C.16)

Here step (i) uses the Cauchy-Schwarz inequality; step (ii) follows from (C.14); step (iii) utilizes (C.15) and
the fact that {Î±t } is monotonically increasing. Following the same analysis as in (5.3) and (5.4), we know
that
n2
n2
n2
X
X
X
r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )
Î³tn â‰¤
Î¾tn +
n=n1

n=n1

n=n1

â‰¤ C1 rmax

p

(n2 âˆ’ n1 ) log T +

n2
X

r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ ) .

(C.17)

n=n1

Taking (C.16) and (C.17) collectively and let n2 = 2n1 , we know that for any n1 â‰¤ NT (a+ , aâˆ’ )/2,
 X
2
2n1
r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )
n=n1
(iii)

2n
Ï€HF (a+ ) X1 (tn )
r (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )
Ï€HF (aâˆ’ ) n=n
1
!
r
Ï€HF (a+ )
log T
+ 128rmax
n1 Î±T C1 rmax
+ C3 Armax log T .
Ï€HF (aâˆ’ )
n1

â‰¤ 128rmax Î±T

This self-bounding relation implies that
2n1
X

r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ ) â‰¤ 256rmax Î±T

n=n1

v
u
u
Ï€HF (a+ )
+ t256rmax
n1
Ï€HF (aâˆ’ )
â‰¤ 400rmax Î±T

r
Î±T C1 rmax

Ï€HF (a+ )
Ï€HF (aâˆ’ )

!
log T
+ C3 Armax log T .
n1

p
Ï€HF (a+ )
Armax log T
+ C1 rmax n1 log T + C3 n1
,
Ï€HF (aâˆ’ )
Î±T

where the last relation follows from the AM-GM inequality. By using the above relation recursively, we have
âŒˆlog T âŒ‰ NT (a+ ,aâˆ’ )/2kâˆ’1

Î¶(a+ , aâˆ’ ) â‰¤

X

X

r(tn ) (a+ ) âˆ’ r(tn ) (aâˆ’ ) âˆ’ râ‹† (a+ ) + râ‹† (aâˆ’ )

k=1 n=NT (a+ ,aâˆ’ )/2k

19

â‰¤ C9 rmax

p
Ï€HF (a+ )
Armax log T
Î±T log T + C9 rmax NT (a+ , aâˆ’ ) log T + C9 NT (a+ , aâˆ’ )
Ï€HF (aâˆ’ )
Î±T

(C.18)

for some sufficiently large constant C9 > 0. On the other hand, taking (C.18) and (C.13) collectively yields
 Î²

1
1
p
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î±T rmax
log T + C9 rmax NT (a+ , aâˆ’ ) log T
Î¶(a+ , aâˆ’ ) â‰¤ C8 C9 T
Ï€ref (a+ )
Armax log T
+ C9 NT (a+ , aâˆ’ )
.
Î±T

(C.19)

By putting (C.10), (C.12), (C.18) and (C.19) together, we have
(
)

 Î²
1
1
Ï€HF (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î¶(a+ , aâˆ’ ) â‰¤ C6 (rmax + log T ) min
Î±T rmax , T
Î±T rmax
Ï€HF (aâˆ’ )
Ï€ref (a+ )


ANT (a+ , aâˆ’ ) log T p
+ C6
+ T log T rmax
Î±T
always holds for some universal constant C6 > 0.

D

Proof of Proposition 3

Under Assumption 1, we know that for any action pair (a+ , aâˆ’ ),

min


 Î²

1
1
1
1
Î²

Ï€HF (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î²+1
Î±T rmax
.
Î±T rmax , T
â‰¤ max Ï„ Î±T rmax , (ÎºT ) Î²+1 Î±TÎ²+1 rmax
Ï€HF (aâˆ’ )
Ï€ref (a+ )

Therefore we have
T
X
p
Armax log T
R(T ) â‰¤ Crmax A T log T + C
+ 2C(rmax + log T )A2 Ï„ Î±T rmax
Î±
t
t=1
2

1

Î²

1

Î²+1
.
+ C(rmax + log T )A2 (ÎºT ) Î²+1 Î±TÎ²+1 rmax

By taking
1

Î±t = A log T + t Î²+2

r

max

Î² 
 Î²+2

Îº

Î²+1

 Î²+2
log T
,
A(rmax + log T )

we can achieve
R(T ) â‰² (rmax + log T )A3 Ï„ rmax log T + rmax A2
+ (rmax + log T )

Î²+1
Î²+2

2
Î²+2

rmax Îº

1

Î²
Î²+2

A

2Î²+3
Î²+2

2Î²+2

Î²+3

p

T log T
Î²+1

1

T Î²+2 (log T ) Î²+2
Î²

Î²+1

1

Î²+2
+ (rmax + log T ) Î²+2 A Î²+2 Ï„ rmax
Îºâˆ’ Î²+2 (log T ) Î²+2 T Î²+2
2Î²+3

Î²

1

1

Î²

Î²+1
+ (rmax + log T )A Î²+1 Îº Î²+1 (log T ) Î²+1 rmax
T Î²+1
Î²+1

2
2
A3 Ï„ log2 T.
â‰² Ï„ A3 rmax
log2 T + T Î²+2 ÎºÎ² rmax

E

Another assumption and the regret bound

As an alternaive to Assumption 1, we can also impose the following assumption to capture the relation
between human preference Ï€HF and the reference policy Ï€ref .
Assumption 2. There exists some quantity Âµ > 0 such that, for any action pair (a+ , aâˆ’ ),
Ï€HF (a+ )
Ï€ref (a+ )
â‰¤Âµ
.
Ï€HF (aâˆ’ )
Ï€ref (aâˆ’ )
20

The quantity Âµ measures the deviation of human preference from the reference policy. Under Assumption 2, we have

 Î²

1
1
Ï€HF (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
Î±T rmax
min
Î±T rmax , T
Ï€HF (aâˆ’ )
Ï€ref (a+ )


 Î²

1
1
Ï€ref (a+ )
Ï€ref (aâˆ’ ) Î²+1 Î²+1
Î²+1
â‰¤ min Âµ
Î±T rmax , T
Î±T rmax .
Ï€ref (aâˆ’ )
Ï€ref (a+ )


Î²

Î²+1

â‰¤ (ÂµT ) 2Î²+1 (Î±T rmax ) 2Î²+1 .
Putting the above relation with (4.1), we have
R(T ) â‰² rmax A2

p

T log T +

T
X
Armax log T
t=1

Î±t

2
+ A2 Î±T rmax

Î²

Î²+1

+ (rmax + log T )A2 (ÂµT ) 2Î²+1 (Î±T rmax ) 2Î²+1 .
By taking
Î²+1

Î±t = A + t 3Î²+2

Î²
2Î²+1
 3Î²+2
rmax  3Î²+2
log T
,
Âµ
A(rmax + log T )

we have
2Î²+1

Î²

R(T ) â‰² T 3Î²+2 Âµ 3Î²+2 poly(A, rmax , log T ).

F

Technical lemmas

Lemma 5. For any x, Î´ âˆˆ R, we have
KL(Ïƒ(x)âˆ¥Ïƒ(x + Î´)) â‰¥

1
Ïƒ(x) (1 âˆ’ Ïƒ(x)) min{|Î´|, Î´ 2 }.
4

Proof. Let fx (t) := KL(Ïƒ(x)âˆ¥Ïƒ(x + t)). We have
Ïƒ(x)
1 âˆ’ Ïƒ(x)
+ (1 âˆ’ Ïƒ(x)) log
Ïƒ(x + t)
1 âˆ’ Ïƒ(x + t)


1 âˆ’ Ïƒ(x + t)
1 âˆ’ Ïƒ(x)
Ïƒ(x)
Â·
+ log
= Ïƒ(x) log
1 âˆ’ Ïƒ(x)
Ïƒ(x + t)
1 âˆ’ Ïƒ(x + t)

1 + exp(x + t)
= log
âˆ’ Ïƒ(x)t = log 1 + Ïƒ(x)(et âˆ’ 1) âˆ’ Ïƒ(x)t.
1 + exp(x)

fx (t) = Ïƒ(x) log

Then we have
fxâ€² (t) =

Ïƒ(x)et
Ïƒ(x) (1 âˆ’ Ïƒ(x)) (et âˆ’ 1)
âˆ’
Ïƒ(x)
=
.
1 + Ïƒ(x)(et âˆ’ 1)
1 + Ïƒ(x)(et âˆ’ 1)

For any t > 0, we can check that
 1
fxâ€² (t) > Ïƒ(x) (1 âˆ’ Ïƒ(x)) 1 âˆ’ eâˆ’t â‰¥ Ïƒ(x) (1 âˆ’ Ïƒ(x)) min {t, 1} ,
2
and for any t âˆˆ (0, 1) we have

fxâ€² (t) < Ïƒ(x) (1 âˆ’ Ïƒ(x)) et âˆ’ 1 â‰¤ 2Ïƒ(x) (1 âˆ’ Ïƒ(x)) t.
This immediately implies that for Î´ > 0,
Z Î´
KL (Ïƒ(x)âˆ¥Ïƒ(x + Î´)) = fx (Î´) âˆ’ fx (0) =
0

21

fxâ€² (t)dt

Z Î´
1
Ïƒ(x) (1 âˆ’ Ïƒ(x))
min {t, 1} dt
2
0
(a) 1
â‰¥ Ïƒ(x) (1 âˆ’ Ïƒ(x)) min{Î´, Î´ 2 }.
4
RÎ´
RÎ´
Here step (a) holds since 0 min{t, 1}dt = Î´ 2 /2 for Î´ â‰¤ 1, and 0 min{t, 1}dt = Î´ âˆ’ 1/2 â‰¥ Î´/2 for Î´ > 1.
For Î´ < 0, we can use the same argument to show that
â‰¥

KL (Ïƒ(x)âˆ¥Ïƒ(x + Î´)) â‰¥

1
Ïƒ(x) (1 âˆ’ Ïƒ(x)) min{âˆ’Î´, Î´ 2 }.
4

This completes the proof.
The following lemma provides a user-friendly version of Freedmanâ€™s inequality (the Bernstein inequality
for martingale differences) (Freedman, 1975; Tropp, 2011).
Lemma 6. Consider a filtration {Fi }iâ‰¥0 and random variables {Xi }iâ‰¥1 obeying
|Xi | â‰¤ R

and

E[Xi |Fiâˆ’1 ] = 0

for all i â‰¥ 1.

Pn

Define Wn = i=1 E[Xi2 |Fiâˆ’1 ], and suppose that Wn â‰¤ Ïƒ 2 holds deterministically for some given quantity
Ïƒ > 0. Then for any positive integer m â‰¥ 1, with probability exceeding 1 âˆ’ Î´ we have
s


n
X
2m 4
2m
Ïƒ2
Xi â‰¤ 8 max Wn , m log
+ R log
.
2
Î´
3
Î´
i=1
Proof. See Li et al. (2021, Section A).

References
Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. (2024). A
general theoretical paradigm to understand learning from human preferences. In International Conference
on Artificial Intelligence and Statistics, pages 4447â€“4455. PMLR.
Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In
International Conference on Machine Learning, pages 263â€“272. PMLR.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D.,
Henighan, T., et al. (2022). Training a helpful and harmless assistant with reinforcement learning from
human feedback. arXiv preprint arXiv:2204.05862.
Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika, 39(3/4):324â€“345.
Cen, S., Mei, J., Goshvadi, K., Dai, H., Yang, T., Yang, S., Schuurmans, D., Chi, Y., and Dai, B. (2025).
Value-incentivized preference optimization: A unified approach to online and offline RLHF. In The Thirteenth International Conference on Learning Representations.
Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. (2024). Self-play fine-tuning converts weak language models
to strong language models. In International Conference on Machine Learning, pages 6621â€“6642. PMLR.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement
learning from human preferences. Advances in neural information processing systems, 30.
Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang,
T. (2024). Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863.
Feng, Y., Kwiatkowski, A., Zheng, K., Kempe, J., and Duan, Y. (2025). Pilaf: Optimal human preference
sampling for reward modeling. arXiv preprint arXiv:2502.04270.
22

Freedman, D. A. (1975). On tail probabilities for martingales. The Annals of Probability, pages 100â€“118.
Guo, S., Zhang, B., Liu, T., Liu, T., Khalman, M., Llinares, F., Rame, A., Mesnard, T., Zhao, Y., Piot, B.,
et al. (2024). Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792.
Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J., Wadden, D., Smith, N. A.,
Beltagy, I., et al. (2023). Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv
preprint arXiv:2311.10702.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? In Advances
in Neural Information Processing Systems, pages 4863â€“4873.
Lai, T. L. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in applied
mathematics, 6(1):4â€“22.
Lattimore, T. and SzepesvÃ¡ri, C. (2020). Bandit algorithms. Cambridge University Press.
Li, G., Cai, C., Chen, Y., Gu, Y., Wei, Y., and Chi, Y. (2021). Is Q-learning minimax optimal? a tight
sample complexity analysis. arXiv preprint arXiv:2102.06548.
OpenAI (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in
neural information processing systems, 35:27730â€“27744.
Rafailov, R., Hejna, J., Park, R., and Finn, C. (2024). From r to Qâˆ— : Your language model is secretly a
Q-function. In First Conference on Language Modeling.
Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. (2024). Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715.
Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic exploration.
Advances in Neural Information Processing Systems, 26.
Shi, R., Song, M., Zhou, R., Zhang, Z., Fazel, M., and Du, S. S. (2025). Understanding the performance gap
in preference learning: A dichotomy of rlhf and dpo. arXiv preprint arXiv:2505.19770.
Tropp, J. (2011). Freedmanâ€™s inequality for matrix martingales. Electronic Communications in Probability,
16:262â€“270.
Xie, T., Foster, D. J., Krishnamurthy, A., Rosset, C., Awadallah, A. H., and Rakhlin, A. (2025). Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient RLHF. In
The Thirteenth International Conference on Learning Representations.
Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. (2023). Iterative
preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv
preprint arXiv:2312.11456.
Zanette, A. and Brunskill, E. (2019). Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In International Conference on Machine Learning,
pages 7304â€“7312. PMLR.
Zhang, S., Yu, D., Sharma, H., Zhong, H., Liu, Z., Yang, Z., Wang, S., Awadalla, H. H., and Wang, Z.
(2025). Self-exploring language models: Active preference elicitation for online alignment. Transactions
on Machine Learning Research.
Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. (2023). Slic-hf: Sequence likelihood
calibration with human feedback. arXiv preprint arXiv:2305.10425.

23

Zhu, B., Frick, E., Wu, T., Zhu, H., Ganesan, K., Chiang, W.-L., Zhang, J., and Jiao, J. (2024). Starling-7b:
Improving helpfulness and harmlessness with rlaif. In First Conference on Language Modeling.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G.
(2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.

24

