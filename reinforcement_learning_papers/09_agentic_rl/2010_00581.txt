Emergent Social Learning via Multi-agent Reinforcement Learning

Kamal Ndousse 1 Douglas Eck 2 Sergey Levine 2 3 Natasha Jaques 2 3

arXiv:2010.00581v3 [cs.LG] 22 Jun 2021

Abstract
Social learning is a key component of human
and animal intelligence. By taking cues from the
behavior of experts in their environment, social
learners can acquire sophisticated behavior and
rapidly adapt to new circumstances. This paper
investigates whether independent reinforcement
learning (RL) agents in a multi-agent environment
can learn to use social learning to improve their
performance. We find that in most circumstances,
vanilla model-free RL agents do not use social
learning. We analyze the reasons for this deficiency, and show that by imposing constraints
on the training environment and introducing a
model-based auxiliary loss we are able to obtain generalized social learning policies which
enable agents to: i) discover complex skills that
are not learned from single-agent training, and
ii) adapt online to novel environments by taking
cues from experts present in the new environment.
In contrast, agents trained with model-free RL or
imitation learning generalize poorly and do not
succeed in the transfer tasks. By mixing multiagent and solo training, we can obtain agents that
use social learning to gain skills that they can
deploy when alone, even out-performing agents
trained alone from the start.

1. Introduction
Social learningâ€”learning by observing the behavior of other
agents in the same environmentâ€”enables both humans and
animals to discover useful behaviors that would be difficult
to obtain through individual exploration, and to adapt rapidly
to new circumstances (Henrich & McElreath, 2003; Laland,
2004). For example, fish are able to locate safe sources of
food in new environments by observing where other members
1 OpenAI, San Francisco,

CA, USA 2 Google Research â€“
Brain team, Mountain View, CA, USA 3 UC Berkeley, Berkeley, CA, USA. Correspondence to: Kamal Ndousse <kamal.ndousse@gmail.com>.
Proceedings of the 38ğ‘¡ â„ International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

of their species congregate (Laland, 2004). Humans are
able to learn from one another without direct access to the
experiences or memories of experts. Rather, the interactions
that constitute social learning are mediated directly by the
environment. A novice may gain the ability to solve a task
merely by watching a self-interested expert complete the
same task, and without an explicit incentive to mimic the
expert. Beyond simple imitation, social learning allows
individual humans to â€œstand on the shoulders of giantsâ€ and
develop sophisticated behaviors or innovations that would
be impossible to discover from scratch within one lifetime.
In fact, social learning may be a central feature of humansâ€™
unprecedented cognitive and technological development
(Humphrey, 1976; Boyd et al., 2011).
Contemporary techniques for transmitting skills between
agents in AI systems are much more limited. For example,
Imitation Learning (IL) depends on manually curated expert
demonstration trajectories. This requires a system extrinsic
to the learning agent (i.e., a developer) to (1) identify tasks of
interest, (2) find and instrument task experts, and (3) collect
demonstration data. The resulting agent is brittle, only able
to replicate skills within the collected demonstrations, and
otherwise vulnerable to compounding error (Ross et al.,
2011; Reddy et al., 2019). In contrast, humans and animals
capable of social learning are able to identify experts at tasks
relevant to their own interests and learn from those experts
without the intervention of an external system. In novel
circumstances, social learning enables adapting online to
the new task, which IL cannot do.
In this work, we investigate whether RL agents can learn
to learn from the cues of other agents. We focus on two
hypotheses related to the benefits of social learning: H1)
Social learning can discover more complex policies that
would be difficult to discover through costly individual exploration, and H2) Social learning can be used to adapt
online to a new environment, when the social learning policy encodes how to learn from the cues of experts. We
study a setting representative of real-world human environments: multi-agent reinforcement learning (MARL) where
completely independent agents pursue their own rewards
with no direct incentive to teach or learn from one another
(decentralized training and decentralized execution, with no
shared parameters or state). This setting is representative
of real-world multi-agent problems such as autonomous

driving, in which there are other cars on the road but the
drivers are not motivated to teach a learning agent, and may
or may not have relevant expertise. Novice agents are able
to see the experts in their partial observations, but must
learn the social learning behavior that would allow them to
make use of that information. In these challenging scenarios,
we find that vanilla model-free RL agents fail to use social
learning to improve their performance.

Farebrother et al., 2018; Packer et al., 2018), we believe these
results are a promising first step on a new path for enhancing
generalization. In real-world tasks, such as autonomous
driving or robotics, there are many human experts who know
how to perform useful tasks, but individual exploration can
be costly, inefficient, error-prone, or unsafe. Social learning
could enable agents to adapt online using cues from these
experts, without resorting to clumsy or costly exploration.

Since social learning does not occur by default, we thoroughly investigate the environment conditions and agent
abilities which are needed for it to emerge, and propose both
a training environment and model architecture that facilitate
social learning.

2. Related Work
There is a rich body of work on imitation learning, which
focuses on training an agent to closely approximate the
behavior of a curated set of expert trajectories (Pomerleau,
1989; Schaal, 1999; Billard et al., 2008; Argall et al., 2009).
For example, Behavioral Cloning (e.g. Bain & Sammut
(1995); Torabi et al. (2018)) uses supervised learning to
imitate how the expert maps observations to actions. Because
imitation learning can be brittle, it can also be combined
with RL (e.g. Guo et al. (2019); Lerer & Peysakhovich
(2019)), or make use of multiple experts (e.g. Cheng et al.
(2020)). Third-person imitation learning (e.g. Shon et al.
(2006); Calinon et al. (2007); Stadie et al. (2017)) enables
agents to learn to copy an expertâ€™s policy when observing
the expert from a third-person perspective.

We show that when social learning does occur, it enables
agents to learn more complex behavior than agents trained
alone and even the experts themselves.
Within each episode, social learners selectively follow experts only when doing so is rewarding â€” which pure imitation learning cannot do. To prevent social learners from
becoming reliant on the presence of experts, we interleave
training with experts and training alone. Social learners
trained in this manner make use of what they learned from experts to improve their performance in the solo environment,
even out-performing agents that were only ever trained solo.
This demonstrates that agents can acquire skills through
social learning that they could not discover alone, and which
are beneficial even when experts are no longer present.

Our work shares some similarities with third-person imitation learning, in that our agents are never given access to
expertâ€™s observations and therefore only have a third-person
view. However, we do not assume access to a curated set of
expert trajectories. Rather, our agents co-exist in a multiagent environment with other agents with varying degrees of
expertise, who are not always within their partially observed
field of view. We do not assume experts are incentivized
to teach other agents (as in e.g. Omidshafiei et al. (2019);
Christiano et al. (2017)). Since agents are not given privileged access to information about other agents, they do not
explicitly model other agents (as in e.g. Albrecht & Stone
(2018); Jaques et al. (2018)), but must simply treat them as
part of the environment. Finally, unlike in imitation learning,
we do not force our agents to copy the expertsâ€™ policy; in
fact, we show that our agents eventually learn to out-perform
the experts with which they learned.

We show for the first time that social learning improves
generalization for RL agents. When evaluated in novel
environments with different experts, social learners use cues
from the new experts to adapt online, achieving high zeroshot transfer performance. They significantly out-perform
agents trained alone, agents that do not learn social learning,
and imitation learning. Imitation learning leads to similar
transfer performance as the original experts, whereas social
learning enables agents to actively seek information about
the new environment from a new set of experts and adjust
their policy accordingly. The difference between imitation
learning and social learning can best be understood using
the following analogy: Teach someone to fish, and they will
learn how to fish. Teach someone to acquire skills from
other agents, and they can learn how to fish, hunt, swim, and
any other skills they require.

Our work is distinct from Inverse RL (IRL) (e.g. Ng et al.
(2000); Ramachandran & Amir (2007); Ziebart et al. (2008);
Hadfield-Menell et al. (2016)), because our agents share a
single environment with the experts which has a consistent
reward function, and thus do not need to infer the reward
function from expert trajectories.1 However, we do consider
learning from sub-optimal experts, which was studied by
Jacq et al. (2019) in the context of IRL.

The contributions of this work are to show how learning
social learningâ€”specifically, learning how to learn from
other agents using RLâ€”can lead to improved generalization to novel environments, reduce the cost of individual
exploration, and enhance performance. We show how both
algorithm and environment design are critical to facilitating
social learning, and provide details for how to achieve it.
Since deep RL often generalizes poorly (Cobbe et al., 2019;

1Note that novices and experts do not share rewards; if an expert
receives a reward the novice does not benefit.

2

Learning online from other agents has been studied in the
context of autonomous driving, where Landolfi & Dragan
(2018) trained cars which copied the policy of human drivers
on the road when the agent had a high degree of uncertainty.
However, they assume access to privileged information about
other carsâ€™ states and policies. Sun et al. (2019) make similar
assumptions, but use other cars to update agentsâ€™ beliefs; e.g.
about the presence of pedestrians in occluded regions.

policy ğœ‹ ğ‘˜ that optimizes
h Ã the total expected
i discounted future
âˆ
ğ‘¡ ğ‘Ÿ ğ‘˜ | ğ‘  , given starting state
reward: ğ½ (ğœ‹ ğ‘˜ ) = E ğœ‹
ğ›¾
0
ğ‘¡=0
ğ‘¡+1
ğ‘ 0 and discount factor ğ›¾ âˆˆ [0, 1]. Note that agents are
trained independently, cannot directly observe other agentsâ€™
observations or actions, and do not share parameters. To
simplify notation, when we discuss the learning objectives
for a single novice we forego the superscript notation.

Most closely related to our work is that of (Borsa et al.,
2019), who train model-free RL agents in the presence of a
scripted expert. In the partially observed exploration tasks
they consider, they find that that novice agents learning from
experts outperform solitary novices only when experts use
information that is hidden from the novices. In our work,
experts do not have access to privileged information about
the environment; rather they are distinguished by their task
skill. We analyze why it is difficult for RL agents to benefit
from expert demonstrations in sparse reward environments,
and propose a method to solve it. With our proposed learning
algorithm, novices learning from unprivileged experts do
outperform solitary learners. Unlike Borsa et al. (2019), we
show for the first time that social learning allows agents to
transfer effectively to unseen environments with new experts,
resulting in better generalization than solo learners.

3.1. Model-Free Sparse Reward Social Learning
To understand how experts present in the same environment
could provide cues that enhance learningâ€”and how modelfree RL could fail to benefit from those cuesâ€”consider an
example environment in which agents must first pick up a
key, and use it to pass through a door to reach a goal. In this
sparse-reward, hard exploration environment, agents only
receive a reward for reaching the goal; the rest of the time,
their reward is 0. Assume there is a boundless supply of
keys, and after an agent passes through the door it closes â€”
then other agents cannot modify the environment to make
the task easier. Instead, an expert agent can demonstrate
a novel state ğ‘ Ëœ that is difficult to produce through random
exploration: opening the door. Ideally, we would like novice
agents to learn from this demonstration by updating their
internal representation to model ğ‘ Ëœ, and eventually learn how
to produce this state.

Finally, our approach uses a model-based auxiliary loss,
which predicts the next state given the current state, to better
enable agents to learn transition dynamics from trajectories
in which they received no reward. The idea of using auxiliary
losses in deep RL has been explored in a variety of works (e.g.
Ke et al. (2019); Weber et al. (2017); Shelhamer et al. (2016);
Jaderberg et al. (2016)). Model-based RL (MBRL) has also
been applied to the multi-agent context (e.g. Krupnik et al.
(2020)). Hernandez-Leal et al. (2019) use predicting the
actions of other agents as an auxiliary task, and show that it
improves performance on multi-agent tasks. However, they
do not study social learning, transfer or generalization, and
assume privileged access to other agentsâ€™ actions.

However, if the novice does not receive any reward as a result
of observing ğ‘ Ëœ, model-free RL receives little benefit from the
expertâ€™s behavior. Assume that the novice agent is learning
policy ğœ‹ ğœƒ with a policy-gradient objective on a trajectory
including the demonstrated state ğ‘ Ëœ ğ‘˜ , ğ‘˜ âˆˆ (0, ğ‘‡ âˆ’ 1):

âˆ‡ ğœƒ ğ½ (ğœƒ) =

ğ‘‡
âˆ’1
âˆ‘ï¸

âˆ‡ ğœƒ log ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘ ğ‘¡ )ğ‘…ğ‘¡

(1)

ğ‘¡=0

Ã
0
where ğ‘…ğ‘¡ = ğ‘‡ğ‘¡0 =ğ‘¡+1 ğ›¾ ğ‘¡ âˆ’ğ‘¡âˆ’1 ğ‘Ÿ ğ‘¡ 0 is the total reward received
over the course of the trajectory. If the agent receives
0 reward during the episode in which the demonstration
occurred (e.g. it does not reach the goal), we can see
Ã
0
that âˆ€ğ‘¡ âˆˆ (0, ğ‘‡), ğ‘…ğ‘¡ = ğ‘‡ğ‘¡0 =ğ‘¡+1 ğ›¾ ğ‘¡ âˆ’ğ‘¡âˆ’1 (0) = 0. Therefore
Ãğ‘‡ âˆ’1
âˆ‡ ğœƒ ğ½ (ğœƒ) = ğ‘¡=0 âˆ‡ ğœƒ log ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘ ğ‘¡ ) (0) = 0, and the novice
receives no gradients from the expertâ€™s demonstration which
allow it to update its policy.

3. Learning Social Learning
We focus on Multi-Agent Partially Observable Markov Decision Process (MA-POMDP) environments defined by the
tuple hS, A, T , R, I, ğ‘i, where ğ‘ is the number of agents.
The shared environment state is ğ‘  âˆˆ S. I is an inspection
function that maps ğ‘  to each agentâ€™s partially observed view
of the world, ğ‘  ğ‘˜ . At each timestep ğ‘¡, each agent ğ‘˜ chooses
a discrete action ğ‘ ğ‘¡ğ‘˜ âˆˆ A. Agents act simultaneously and
there is no notion of turn-taking. Let A ğ‘ be the joint action
space.The transition function depends on the joint action
space: T : S Ã— A ğ‘ Ã— S â†’ [0, 1]. The reward function is
the same across agents, but each agent receives its own individual reward ğ‘Ÿ ğ‘¡ğ‘˜ = R (ğ‘ ğ‘¡ , ğ‘ ğ‘¡ğ‘˜ ). Each agent ğ‘˜ is attempting to
selfishly maximize its own, individual reward by learning a
3

Temporal Difference (TD) Learning could temporarily mitigate this issue, but as we show in the Appendix, this
ability quicklyh deteriorates. Consider
Q-learning, in which
i
Ãâˆ ğ‘¡
ğ‘„(ğ‘, ğ‘ ) = E ğœ‹
ğ‘¡=0 ğ›¾ ğ‘Ÿ ğ‘¡+1 | ğ‘, ğ‘  models the total expected
reward from taking action ğ‘ in state ğ‘ . As the agent continues
to receive 0 rewards during training, all ğ‘„ values will be
driven toward zero. Even if the agent observes a useful
novel state such as ğ‘ Ëœ, as ğ‘„(ğ‘, ğ‘ ) â†’ 0, âˆ€ğ‘ âˆˆ A, ğ‘  âˆˆ S, the

Q-learning objective becomes:
ğ‘„( ğ‘ Ëœ, ğ‘) = ğ‘Ÿ + ğ›¾ max
ğ‘„(ğ‘  0, ğ‘ 0) = 0 + ğ›¾0 = 0
0
ğ‘

(2)

Thus, the objective forces the value of ğ‘„( ğ‘ Ëœ, ğ‘) to be zero.
In this case, modeling transitions into or out of state ğ‘ Ëœ is
not required in order to produce an output of zero, since all
other Q-values are already zero. In both cases, we can see
that before a model-free RL agent receives a reward from
the environment, it will have difficulty modeling novel states
or transition dynamics. This makes learning from the cues
of experts particularly difficult.

Figure 1. PPO + auxiliary loss deep neural network architecture.
Convolution layers extract information about the state from pixels,
which is fed into a Fully Connected (FC) layer, and a recurrent
LSTM. The yellow shaded box shows the components of the model
which learn the RL policy ğœ‹ and value function ğ‘‰ (ğ‘ ). The green
shaded box shows the components of the model dedicated to
computing the auxiliary loss, which predicts the next state from
the current state.

Learning to model the expertâ€™s policy ğœ‹ ğ¸ (ğ‘ ğ¸ |ğ‘  ğ¸ ) would
likely help the novice improve performance on the task.
However, the novice does not have explicit access to the
expertâ€™s states or actions. From its perspective, the other
agentâ€™s policy is simply a part of the environment transition
dynamics. While the true state transition function ğ‘ ğ‘¡+1 =
T (ğ‘ ğ‘¡ , ğ‘ ğ‘¡ğ‘ , ğ‘ ğ‘¡ğ¸ ) depends on both the noviceâ€™s own action ğ‘ ğ‘¡ğ‘ ,
and the expertâ€™s policy (since ğ‘ ğ‘¡ğ¸ = ğœ‹ ğ¸ (ğ‘ ğ‘¡ğ¸ )), the novice is
ğ‘
only able to observe ğ‘(ğ‘ ğ‘¡+1
|ğ‘ ğ‘¡ğ‘ , ğ‘ ğ‘¡ğ‘ ). Therefore, the novice
can only obtain knowledge of the expertâ€™s policy through
correctly modeling the state transitions it observes. Since,
as we have argued above, the novice will struggle to model
state transitions in the absence of external reward, it will
also have difficulty modeling another agentâ€™s policy.

actions in order to accurately predict the next state. It is able
to do this without ever being given explicit access to the
other agentâ€™s states or actions, or even any labels indicating
what elements of the environment constitute other agents.
We refer to our approach as social learning with an auxiliary
predictive loss, and hypothesize that it will improve agentsâ€™
ability to learn from the cues of experts. Note, however,
that simply modeling the other agentâ€™s policy does not force
these independent agents to copy the actions of other agents
(unlike in imitation learning). When an independent social
learner receives a low reward for following the policy of
another agent, it can learn to avoid following that agent in
future.

3.2. Social Learning with Auxiliary Losses
To mitigate this issue, we propose augmenting the novice
agent with a model-based prediction loss. Specifically,
we append additional layers ğœƒ ğ´ to the policy networkâ€™s
encoding of the current state, ğ‘“ ğœƒ (ğ‘ ğ‘¡ ), as shown in Figure 1.
We introduce an unsupervised mean absolute error (MAE)
auxiliary loss to train the network to predict the next state
ğ‘ ğ‘¡+1 , given the current state ğ‘ ğ‘¡ and action ğ‘ ğ‘¡ :
ğ‘‡

ğ‘ Ë†ğ‘¡+1 = ğ‘“ ğœƒ ğ´ (ğ‘ ğ‘¡ , ğ‘ ğ‘¡ );

ğ½=

1 âˆ‘ï¸
|ğ‘ ğ‘¡+1 âˆ’ ğ‘ Ë†ğ‘¡+1 |
ğ‘‡ ğ‘¡=0

(3)

This architecture allows gradients from the auxiliary loss
to contribute to improving ğ‘“ ğœƒ (ğ‘ ğ‘¡ ). Figure 2 shows example
state predictions generated by the auxiliary layers of a trained
agent, demonstrating that this architecture enables effective
learning of the transition dynamics.
We can now see that if the novel demonstration state is in a
trajectory, ğ‘ Ëœ ğ‘˜ âˆˆ (0, ğ‘‡), the term | ğ‘ Ëœ ğ‘˜ âˆ’ ğ‘ Ë† ğ‘˜ | will be part of the
objective. It will not be 0 unless the agent learns to perfectly
predict the novel demonstration state. Therefore, cues from
the expert will provide gradients that allow the novice to
improve its representation of the world, even if it does not
receive any reward from the demonstration. This architecture
also implicitly improves the agentâ€™s ability to model other
agentsâ€™ policies, since it must correctly predict other agentsâ€™

4

To optimize our agents, we test both TD-learning (deep
Q-learning) and policy-gradient methods, and find that Proximal Policy Optimization (PPO) (Schulman et al., 2017)
provides better performance and stability, and is most able to
benefit from social learning. We use Generalized Advantage
Estimation (GAE) (Schulman et al., 2016) to train the PPO
value function. As shown in Figure 1, our agents use convolution layers to learn directly from a pixel representation of the
state ğ‘ ğ‘¡ . Because the environments under investigation are
partially observed and non-Markov, we use a recurrent policy parameterized by a Long Short-Term Memory (LSTM)
network (Hochreiter & Schmidhuber, 1997) to model the
history of observations in each episode. LSTM hidden states
are stored in the experience replay buffer. Following the
recommendations of Andrychowicz et al. (2020), we recalculate the stored state advantage values used to compute the
value function loss between mini-batch gradient updates. In
addition, we recalculate and update the stored hidden states
as suggested by Kapturowski et al. (2018) for off-policy
RL. A detailed description of the network architecture and
hyperparameters are given in an appendix.

Figure 2. Examples of future states ğ‘ Ë†ğ‘¡+1 predicted by the network given state ğ‘ ğ‘¡ (â€œobserved stateâ€), and each of the possible movement
actions ğ‘ ğ‘¡ . Most predictions are highly accurate, indicating the network has learned to effectively model transition dynamics. The
transition for the â€˜do nothingâ€™ action is less accurate because it is infrequently chosen by the agent.

3.3. Social Learning Environments

for navigating to the incorrect goal. In Appendix 7.2 we
conduct experiments to assess the effect of the exploration
penalty, and find that with no exploration penalty agents do
not learn to rely on social learning.

Humans and animals are most likely to rely on social learning
when individual learning is difficult or unsafe (Henrich &
McElreath, 2003; Laland, 2004). Further, individuals prefer
to learn from others that they perceive to be highly successful
or competent, which is known as prestige bias (JimÃ©nez &
Mesoudi, 2019). Cues or signals associated with prestige
have shown to be important to both human and animal social
learning (Barkow et al., 1975; Horner et al., 2010).

Prestige cues are implemented in Goal Cycle signal through
agents changing color as they collect rewards over the course
of each episode. At time ğ‘¡, the color of an agent is a
linear combination of the red and blue RGB color vectors:
colorğ‘¡ = blue Â· ğ‘Ëœğ‘¡ + red Â· (1 âˆ’ ğ‘Ëœğ‘¡ ), where ğ‘Ëœğ‘¡ = sigmoid(ğ‘ ğ‘¡ ),
and ğ‘ ğ‘¡ is a reward-dependent prestige value given by:
(
ğ›¼ğ‘ ğ‘ ğ‘¡âˆ’1 + ğ‘Ÿ ğ‘¡ , ğ‘Ÿ ğ‘¡ â‰¥ 0
ğ‘ğ‘¡ =
(4)
0,
otherwise.

Motivated by these two ideas, we introduce a novel environment specifically designed to encourage social learning by making individual exploration difficult and expensive, and introducing prestige cues. The code for both
the environment and social learning agents is available at:
https://github.com/kandouss/marlgrid. In
Goal Cycle (Figure 3a), agents are rewarded for navigating
between several goal tiles in a certain order, and penalized
for deviating from that order. The goal tiles are placed
randomly and are visually indistinguishable, so it is not
possible for an agent to identify the correct traversal order
without potentially incurring an exploration penalty. When
the penalty is large, this becomes a hard exploration task.

Thus the color of each agent changes from red to blue as
it collects rewards (slowly decaying with constant ğ›¼ğ‘ , but
reverts to red if it incurs a penalty.
The Goal Cycle environments are challenging to master
because there are many more incorrect than correct goal
traversal sequences. When the penalty is large, the returns
for most traversal orders are negative, so mistake-prone
agents learn to avoid goal tiles altogether (after the first goal
in an episode, which always gives a reward of +1). We can
obtain agents that perform well in high penalty environments
using a curriculum: we train them initially in low-penalty
environments and gradually increase the penalty.

Agents can achieve high rewards by efficiently traversing
between the goals in the correct order, and skilled agents are
able to identify the correct order while incurring minimal
penalties. Agents can accomplish this either by trial and
error, or by observing other agents. In practice, since the
behavior of other agents can be unpredictable and potentially
non-stationary, agents more easily learn to solve the task
directly through trial and error. But by adjusting the penalty
for navigating to goals in the wrong order, we can penalize
individual exploration and thereby encourage social learning.

3.4. Social Learning in New Environments

In all the Goal Cycle variants discussed here, agents receive
a reward of +1 for navigating to the first goal they encounter
in an episode, and +1 for any navigating to any subsequent
goals if in the correct order. They receive a penalty of âˆ’1.5
5

A central thesis of this work is that social learning can
help agents adapt more rapidly to novel environments. This
could be particularly useful because deep RL often fails
to generalize to even slight modifications of the training
environment. Therefore, we test the zero-shot transfer
performance of agents pre-trained in 3-Goal Cycle in three
new environments. The first is Goal Cycle with 4 goals. This
variant is significantly harder, because while there are two
possible cycles between three goals, there are six ways to

(a) Goal Cycle

(b) Four Rooms

(c) Maze

Figure 3. Goal Cycle (a) is a 13x13 environment in which the positions of goal tiles (yellow) and obstacles (brown) are randomized at the
start of each episode. Agents receive rewards for repeatedly traversing the goal tiles in a certain order and penalties for deviations from the
correct order. However, the correct order is not visible. Agents always receive a reward from the first goal tile they encounter in an episode.
We test whether agents trained in Goal Cycle can generalize to Four Rooms (b), and Maze (c); larger environments with walls hiding the
location of a single goal.

cycle between four goals (there are (ğ‘› âˆ’ 1)! distinct traversal
orders, since the cycle can be entered at any goal). Thus,
even optimal agents must incur higher penalties to identify
the correct traversal order in each episode.

does not need to learn about transition dynamics or other
agentsâ€™ policies. Finally, we compare to imitation learning
(IL), which is an upper bound on the performance that can
be obtained when learning from other agents with direct
access to their ground truth state and action information
(which is not provided to the other methods).

We also test transfer to the 17x17 Four Rooms (in Figure
3b), and 19x19 Maze environments (Figure 3c). In both
environments, the goal and obstacles are randomly generated
each episode, and agents must navigate around walls to find
the goal within a time limit. These environments represent
challenging transfer tasks, because navigating the maze is
significantly more complex than navigation in Goal Cycle,
and agents are unlikely to have encountered walls in Four
Rooms while training in Goal Cycle (Dennis et al., 2020).

4.1. H1: Learning Social Learning
Figure 4 compares Goal Cycle performance of agents trained
alone (solo) to agents trained in the presence of experts
(social). Solo agents, even with the predictive auxiliary
loss (solo ppo + aux pred), are not able to discover the
strategy of reaching the goals in the correct order; instead,
all random seeds converged to a strategy of stopping after
reaching a single goal (with a maximum return of 1). Even
when trained in the presence of experts, agents without an
auxiliary loss (social ppo (vanilla))

4. Experiments
To establish that independent agents with the auxiliary predictive loss described in Sec. 3.2 can use social learning
to improve their own performance, we compare the performance of agents with auxiliary predictive losses learning
in an environment shared with experts (social ppo + aux
pred) to that of agents with the same architecture but trained
alone (Solo PPO + aux pred).

failed in the same way, with all seeds receiving a maximum
reward of 1. This supports our hypothesis that model-free
RL by itself has difficulty benefiting from the cues of experts
in sparse reward environments.
In contrast, social agents trained with auxiliary predictive
or reconstructive losses were able to achieve higher performance. This confirms hypothesis H1, that when successful,
social learning can enable agents to learn more complex
behavior than when learning alone. While the reconstruction
loss helps agents learn from experts, still higher performance
is obtained with the model-based prediction loss. In contrast
to the reconstruction loss, the prediction loss helps agents
learn representations that capture the transition dynamics
and (implicitly) other agentsâ€˜ policies. Note that in our
experiments, solo agents with auxiliary predictive losses
performed much worse than social learners, showing that

We also compare social learning performance (in the presence of experts) across two ablations. The first uses the same
architecture with no auxiliary loss, which corresponds to
standard PPO. The second replaces the next-state-prediction
objective of Equation 3 with a simple autoencoder reconstruction objective that predicts
Ãthe current state, ğ‘ Ë†ğ‘¡ = ğ‘“ ğœƒ ğ´ (ğ‘ ğ‘¡ , ğ‘ ğ‘¡ ).
The loss is thus ğ½ = ğ‘‡1 ğ‘‡ğ‘¡=0 |ğ‘ ğ‘¡ âˆ’ ğ‘ Ë†ğ‘¡ |. We refer to this as
social learning with an auxiliary reconstruction loss (Solo
PPO + aux rec).
Unlike the auxiliary prediction loss, this reconstruction loss
6

Figure 5. The effect of expert skill on social learning. With auxiliary predictive losses, novice agents benefit from social learning in
the presence of near-optimal experts (green) or imperfect experts
(yellow), surpassing the performance of solo agents in both cases.
However, agents learning from imperfect experts achieve lower
performance, and only 1/5 random seeds exceed the performance
of the imperfect experts.

Figure 4. While learning in the presence of experts, only agents
with an auxiliary loss that enables them to model expert cues (with
either a predictive (pred) or reconstructive (rec) auxiliary loss)
succeed in using social learning to improve performance. None
of the seeds for solo agents are able to solve the task. Further, no
vanilla PPO seeds solve the task, showing that social learning does
not occur automatically with RL. In contrast, 4 of 5 seeds with an
auxiliary reconstructive loss (social ppo + aux rec) are able to
exceed the performance of the experts present in their environment.
Faded dotted lines show the performance of individual random
seeds and bolded lines show the mean of 5 seeds. The final
performance is bimodal, with some novice seeds achieving expertlevel performance and others failing to learn entirely. Since the
normality assumption is violated, we refrain from using confidence
intervals.

social learning to exceed the performance of agents trained
alone. However, novices trained with optimal experts more
frequently learn to exceed the performance of the experts
with which they are trained.
4.2. H2: Adapting Online to New Environments
We hypothesize that using social learning to adapt online
within an episode will allow agents to achieve higher performance when transferred to a new environment with experts.
To evaluate this hypothesis, we investigate how well agents
pre-trained in 3-Goal Cycle can generalize to three new
environments: a 4 goal variant of the Goal Cycle environment, Four Rooms, and Maze. Figure 6 shows the zero-shot
transfer performance of agents trained in 3-Goal Cycle to
each new environment. Agents trained alone (which have
not learned to rely on social learning) perform poorly, likely
because they were never able to obtain good exploration
policies during training, due to the high cost of exploration
in the training environment.

good performance can depend on learning effectively from
expert cues in sparse reward hard-exploration environments.
Interestingly, the majority of random seeds with predictive
losses are actually able to exceed the performance of both the
experts and the imitation learning (IL) methods. Unlike IL,
which requires direct access to expertsâ€™ states and actions,
social learners reach equivalent performance only through
learning from their partially observed view of experts via
RL. The reason that social learners can actually exceed the
performance of the experts themselves is that they learn an
improved policy that leverages social information. During
each episode, social learners wait and observe while the
expert explores the goals to find the correct order. Only
then do they follow the expertâ€™s cues. Thus, they allow
the expert to incur the exploration penalties of Goal Cycle
and use social learning to follow when it is safe. Despite
being a non-cooperative strategy, this is reminiscent of social
learning in the animal kingdom (Laland, 2018).
Learning from Sub-Optimal Experts. In real-world environments, not all other agents will have optimal performance.
Therefore, we test whether it is still possible to learn from
imperfect experts. The results in Figure 5 show that while expert performance does affect the novicesâ€™ final performance,
even when learning from imperfect experts novices can use

7

Imitation learning agents perform poorly across all three
transfer environments, under-performing the original 3Goal experts in every case. These results demonstrate
the brittleness of imitation learning, which can only fit the
expert trajectories, and generalizes poorly to states outside
of the training distribution (Ross et al., 2011; Reddy et al.,
2019). In contrast, agents trained with social PPO + aux
pred loss employ a generalized social learning policy that
encodes how to adapt online based on expert cues. Thus,
these agents use social learning to achieve high performance
in the new environments with a new set of experts. For
example, in the Maze environment they can follow the cues
of the Maze experts to more closely approximate the optimal

(a) Goal Cycle (4 goals, with experts)

(b) Four Rooms (with expert)

(c) Maze (with expert)

Figure 6. Zero-shot transfer performance in novel environments with expert demonstrators. Each bar shows the mean of the multiple
seeds, with the performance of each seed shown as a black â€˜xâ€™. The experts and 3-goal solo learners were also trained with auxiliary
predictive losses. Error bars show 95% confidence intervals for the estimated means. Agents which have learned to rely on social learning
benefit from the cues of novel experts in the new environment., and easily out-perform agents trained alone, agents training with imitation
learning, and in some cases, the experts from which they originally learned. Videos: tinyurl.com/SociAPL

policy, rather than continuing to rely on learning from 3-Goal
experts.

learning agents by changing the distribution of training tasks
to include episodes in which the experts are not present. The
green curves in Figure 7 show the performance of a checkpoint switched from entirely social training environments to
a mix of solo and social environments as training proceeds.
The agent is able to retain good performance in solo 3-goal
environments as well as 4-goal environments with experts,
indicating that it is learning to opportunistically take advantage of expert cues while building individual expertise in
the 3-goal environment. In fact, agents trained with social
ppo + aux pred perform better in solo transfer environments
than agents exclusively trained in the solo environment (see
Figure 7b), because agents trained alone never discover the
optimal strategy due to the difficulty of exploring in this
environment. This demonstrates that not only does social
learning enable agents to discover skills that they could not
learn by themselves (again supporting hypothesis H1), but
that they are able to retain this knowledge to improve their
individual performance even when experts are not present.

It is worth noting that the original 3-Goal experts transfer
poorly to both 4-Goal Cycle and Maze. In 4-Goal, they
receive large negative scores, repeatedly incurring a penalty
for navigating to the goals in the incorrect order. This
illustrates the brittleness of an RL policy which is overfit
to the training environment. In both environments, social
learning agents achieve better performance on the transfer
task than the original experts, suggesting social learning may
be a generally beneficial strategy for improving transfer in RL.
Taken together, these results provide support for hypothesis
H2, and suggest social learning can enable RL agents to
adapt online (within an episode) to new environments by
taking cues from other agents.
4.3. Social Learning Helps Solo Learning
A potential downside to learning to rely on the cues of
experts is that agents could fail to perform well when experts
are removed from the environment. For novice agents
always trained with experts, there appears to be a trade-off
between social learning and individual learning. As shown
in Figure 7, novice social ppo + aux pred agents initially
learn to solve the Goal Cycle task with individual exploration,
but eventually they overfit to the presence of experts and
become reliant on expert cues to the detriment of solo
performance. We can see this because the performance in
the 3-goal environment in the presence of experts increases
(7a), while the performance in the 3-goal environment when
alone drops (7b). Observing the agentâ€™s behavior reveals
that it has learned to follow the cues of experts when they
are present in the environment, but refrain from individual
exploration when experts are not present. Given the high
cost of individual exploration in this environment, this a safe
but conservative strategy.

5. Conclusions
In this work we investigated how model-free deep RL can
benefit from the presence of expert agents in the environment. We design an environment to elicit social learning
by increasing the cost of individual exploration and introducing prestige cues. However, we find that even in this
environment, model-free agents fail to use social learning to
reach optimal performance. By adding a model-based auxiliary loss, which requires implicitly modeling other agentsâ€™
policies, we are able to train agents to use social learning.
When deployed to novel environments, these agents use
their social learning policies to adapt online using the cues
of novel experts, and perform well in zero-shot transfer
tasks. In contrast, agents trained alone or with imitation
learning cannot adapt to the new task. Further, by mixing
social and solo training, we obtain social learning agents that

We find that we can improve the solo performance of social
8

(a) Training with experts

(b) Transfer to solo 3-Goal

(c) Transfer to 4-Goal with experts

Figure 7. Orange lines: performance throughout training for one of the social ppo + aux pred seeds from Figure 4 evaluated in three
Goal Cycle environments: (a) the training environment, 3-goal Goal Cycle with experts; (b) a solo version of the training environment (no
experts); and (c) 4-Goal Cycle, with experts. The agent initially relies mostly on individual exploration as indicated by good zero-shot
transfer performance in the solo environment. After about 500k episodes the agent becomes reliant on cues from the 3-Goal experts, and
its performance in solo 3-goal Goal Cycle degrades.
Green lines: performance of the same agent if the distribution of training
environments is changed from 100% social to 75% solo and 25% social after about 500k episodes. As training proceeds the agent
retains the capacity to solve the solo 3-goal environment while learning to use cues from expert behavior when they are available. The
performance of this agent in the solo environment actually exceeds that of agents trained exclusively in the solo environment. The agent
retains the capacity to use social learning when possible, with performance in the social training environment (a) and 4-goal Goal Cycle
with experts (c) comparable to the orange agent (for which 100% of training episodes included experts).

6. Acknowledgements

actually have higher performance in the solo environment
than agents trained exclusively in the solo environment. Our
results demonstrate that social learning not only enables
agents to learn complex behavior that they do not discover
when trained alone, but that it can allow agents to generalize
better when transferred to novel environments.

We are grateful to the OpenAI Scholars program for facilitating the collaboration that led to this paper. We would also
like to thank Ken Stanley, Joel Lehman, Bowen Baker, Yi
Wu, Aleksandra Faust, Max Kleiman-Weiner, and DJ Strouse
for helpful discussions related to this work, and Alethea
Power for providing compute resources that facilitated our
experiments.

5.1. Limitations and Future Work
Our social learning experiments focus on exploratory navigation tasks. Future work includes extending this to other
domains such as manipulation, to scenarios in which experts
pursue different goals than the novices, and to scenarios
with multiple experts that employ a variety of strategies.
We found that training in a mixture of social and solitary
environments can permit novice social learning agents to develop effective strategies for both social and individual task
variants, and notably that the resulting individual skill far
exceeds that of a solitary novice. However, in this work we
do not thoroughly explore different strategies for augmenting solitary with social experience. Further research could
clarify the circumstances in which adding social experiences
could aid solitary task performance, and see the development
of algorithms to facilitate this for arbitrary tasks. Finally, we
assume that prestige cues which indicate how well experts
perform in the environment are visible to social learners.
Future research could investigate how to accurately predict
another agentâ€™s performance when prestige cues are not
available.

References
Albrecht, S. V. and Stone, P. Autonomous agents modelling
other agents: A comprehensive survey and open problems.
Artificial Intelligence, 258:66â€“95, 2018.
Andrychowicz, M., Raichuk, A., StaÅ„czyk, P., Orsini, M.,
Girgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin,
O., Michalski, M., Gelly, S., and Bachem, O. What
matters in on-policy reinforcement learning? a largescale empirical study. arXiv preprint arXiv:2006.05990,
2020.
Argall, B. D., Chernova, S., Veloso, M., and Browning, B.
A survey of robot learning from demonstration. Robotics
and autonomous systems, 57(5):469â€“483, 2009.
Bain, M. and Sammut, C. A framework for behavioural
cloning. In Machine Intelligence 15, pp. 103â€“129, 1995.

9

Barkow, J. H., Akiwowo, A. A., Barua, T. K., Chance, M.,
Chapple, E. D., Chattopadhyay, G. P., Freedman, D. G.,
Geddes, W., Goswami, B., Isichei, P., et al. Prestige and
culture: a biosocial interpretation [and comments and
replies]. Current Anthropology, 16(4):553â€“572, 1975.

Biewald, L. Experiment tracking with weights and biases,
2020. URL https://www.wandb.com/. Software
available from wandb.com.

Hadfield-Menell, D., Russell, S. J., Abbeel, P., and Dragan, A.
Cooperative inverse reinforcement learning. In Advances
in neural information processing systems, pp. 3909â€“3917,
2016.

Billard, A., Calinon, S., Dillmann, R., and Schaal, S. Survey:
Robot programming by demonstration. Handbook of
robotics, 59(BOOK_CHAP), 2008.

Henrich, J. and McElreath, R. The evolution of cultural
evolution. Evolutionary Anthropology: Issues, News, and
Reviews, 12(3):123â€“135, May 2003.

Borsa, D., Heess, N., Piot, B., Liu, S., Hasenclever, L.,
Munos, R., and Pietquin, O. Observational learning
by reinforcement learning. In Proceedings of the 18th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 1117â€“1124. International Foundation
for Autonomous Agents and Multiagent Systems, 2019.

Hernandez-Leal, P., Kartal, B., and Taylor, M. E. Agent modeling as auxiliary task for deep reinforcement learning. In
Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 15,
pp. 31â€“37, 2019.
Hochreiter, S. and Schmidhuber, J. Long short-term memory.
Neural computation, 9(8):1735â€“1780, 1997.

Boyd, R., Richerson, P. J., and Henrich, J. The cultural niche:
Why social learning is essential for human adaptation.
Proceedings of the National Academy of Sciences, 108
(Supplement_2):10918â€“10925, June 2011.

Horner, V., Proctor, D., Bonnie, K. E., Whiten, A., and
de Waal, F. B. Prestige affects cultural learning in chimpanzees. PloS one, 5(5):e10625, 2010.

Calinon, S., Guenter, F., and Billard, A. On learning,
representing, and generalizing a task in a humanoid robot.
IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics), 37(2):286â€“298, 2007.

Humphrey, N. K. The social function of intellect. In Growing
points in ethology, pp. 303â€“317. Cambridge University
Press, 1976.

Cheng, C.-A., Kolobov, A., and Agarwal, A. Policy
improvement from multiple experts. arXiv preprint
arXiv:2007.00795, 2020.

Jacq, A., Geist, M., Paiva, A., and Pietquin, O. Learning
from a learner. In International Conference on Machine
Learning, pp. 2990â€“2999, 2019.

Chevalier-Boisvert, M., Willems, L., and Pal, S. Minimalistic gridworld environment for openai gym. https://
github.com/maximecb/gym-minigrid, 2018.

Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo,
J. Z., Silver, D., and Kavukcuoglu, K. Reinforcement
learning with unsupervised auxiliary tasks. arXiv preprint
arXiv:1611.05397, 2016.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. In Advances in Neural Information
Processing Systems, pp. 4299â€“4307, 2017.

Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega,
P. A., Strouse, D., Leibo, J. Z., and de Freitas, N. Intrinsic
social motivation via causal influence in multi-agent rl.
arXiv preprint arXiv:1810.08647, 2018.

Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman,
J. Quantifying generalization in reinforcement learning.
In International Conference on Machine Learning, pp.
1282â€“1289, 2019.

JimÃ©nez, Ã. V. and Mesoudi, A. Prestige-biased social
learning: current evidence and outstanding questions.
Palgrave Communications, 5(1):1â€“12, 2019.

Dennis, M., Jaques, N., Vinitsky, E., Bayen, A., Russell,
S., Critch, A., and Levine, S. Emergent complexity and
zero-shot transfer via unsupervised environment design.
arXiv preprint arXiv:2012.02096, 2020.

Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and
Dabney, W. Recurrent experience replay in distributed
reinforcement learning. In International conference on
learning representations, 2018.

Farebrother, J., Machado, M. C., and Bowling, M. Generalization and regularization in dqn. arXiv preprint
arXiv:1810.00123, 2018.

Ke, N. R., Singh, A., Touati, A., Goyal, A., Bengio, Y.,
Parikh, D., and Batra, D. Learning dynamics model in
reinforcement learning by incorporating the long term
future. arXiv preprint arXiv:1903.01599, 2019.

Guo, X., Chang, S., Yu, M., Tesauro, G., and Campbell, M.
Hybrid reinforcement learning with expert state sequences.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 3739â€“3746, 2019.

10

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. International Conference on Learning Representations, 12 2014.

learning. In Proceedings of the fourteenth international
conference on artificial intelligence and statistics, pp.
627â€“635. JMLR Workshop and Conference Proceedings,
2011.

Krupnik, O., Mordatch, I., and Tamar, A. Multi-agent
reinforcement learning with multi-step generative models.
In Conference on Robot Learning, pp. 776â€“790, 2020.
Laland, K. N. Social learning strategies. Animal Learning
& Behavior, 32(1):4â€“14, February 2004.

Schaal, S. Is imitation learning the route to humanoid robots?
Trends in cognitive sciences, 3(6):233â€“242, 1999.

Laland, K. N. Darwinâ€™s unfinished symphony: How culture
made the human mind. Princeton University Press, 2018.

Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,
P. High-dimensional continuous control using generalized
advantage estimation. In International Conference on
Learning Representations (ICLR), 2016.

Landolfi, N. C. and Dragan, A. D. Social cohesion in
autonomous driving. In 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp.
8118â€“8125. IEEE, 2018.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Lerer, A. and Peysakhovich, A. Learning existing social
conventions via observationally augmented self-play. In
Proceedings of the 2019 AAAI/ACM Conference on AI,
Ethics, and Society, pp. 107â€“114, 2019.

Shelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T.
Loss is its own reward: Self-supervision for reinforcement
learning. arXiv preprint arXiv:1612.07307, 2016.

Ng, A. Y., Russell, S. J., et al. Algorithms for inverse
reinforcement learning. In Icml, volume 1, pp. 2, 2000.

Shon, A., Grochow, K., Hertzmann, A., and Rao, R. P.
Learning shared latent structure for image synthesis and
robotic imitation. In Advances in neural information
processing systems, pp. 1233â€“1240, 2006.

Omidshafiei, S., Kim, D.-K., Liu, M., Tesauro, G., Riemer,
M., Amato, C., Campbell, M., and How, J. P. Learning to
teach in cooperative multiagent reinforcement learning.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 6128â€“6136, 2019.

Stadie, B. C., Abbeel, P., and Sutskever, I. Third-person
imitation learning. arXiv preprint arXiv:1703.01703,
2017.

Packer, C., Gao, K., Kos, J., KrÃ¤henbÃ¼hl, P., Koltun, V., and
Song, D. Assessing generalization in deep reinforcement
learning. arXiv preprint arXiv:1810.12282, 2018.

Sun, L., Zhan, W., Chan, C.-Y., and Tomizuka, M. Behavior
planning of autonomous cars with social perception. In
2019 IEEE Intelligent Vehicles Symposium (IV), pp. 207â€“
213. IEEE, 2019.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Wallach, H.,
Larochelle, H., Beygelzimer, A., d'AlchÃ©-Buc, F., Fox, E.,
and Garnett, R. (eds.), Advances in Neural Information
Processing Systems 32, pp. 8024â€“8035. Curran Associates,
Inc., 2019.

Torabi, F., Warnell, G., and Stone, P. Behavioral cloning
from observation. arXiv preprint arXiv:1805.01954, 2018.
Weber, T., RacaniÃ¨re, S., Reichert, D. P., Buesing, L., Guez,
A., Rezende, D. J., Badia, A. P., Vinyals, O., Heess, N.,
Li, Y., et al. Imagination-augmented agents for deep
reinforcement learning. arXiv preprint arXiv:1707.06203,
2017.
Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.
Maximum entropy inverse reinforcement learning. In
Aaai, volume 8, pp. 1433â€“1438. Chicago, IL, USA, 2008.

Pomerleau, D. A. Alvinn: An autonomous land vehicle
in a neural network. In Advances in neural information
processing systems, pp. 305â€“313, 1989.
Ramachandran, D. and Amir, E. Bayesian inverse reinforcement learning. In Ä²CAI, volume 7, pp. 2586â€“2591,
2007.
Reddy, S., Dragan, A. D., and Levine, S. Sqil: Imitation
learning via reinforcement learning with sparse rewards.
arXiv preprint arXiv:1905.11108, 2019.
Ross, S., Gordon, G., and Bagnell, D. A reduction of imitation learning and structured prediction to no-regret online

11

7. Appendix
7.1. Auxiliary prediction loss
Figure 8 shows the training curve for the next-state auxiliary
prediction loss of Equation 3, for the five agent seeds plotted
in Figure 4 with auxiliary prediction losses. The curve
shows that the agent is effectively learning to predict the next
state with low mean absolute error. However, because the
agentâ€™s policy is changing at the same time, the prediction
problem is non-stationary, which means that the loss does
not always decrease. If the agent discovers a new behavior,
the model will be required to predict new state transitions
not previously experienced.

Figure 9. Next-state auxiliary prediction loss (in Mean Absolute
Error (MAE) over the course of training the social ppo + aux rec
agents shown in Figure 4.

experiment, the transfer task experts are accompanied by
two non-expert distractor agents that take random movement
actions. As shown by the maze transfer results in Figure 10,
our method is robust to the presence of these distractions
and still outperforms the baselines.

Figure 8. Next-state auxiliary prediction loss (in Mean Absolute
Error (MAE) over the course of training the social ppo + aux rec
agents shown in Figure 4.

7.2. Training without an exploration penalty
Figure 10. Distractors

The Goal Cycle environments include a penalty for reaching
the goals in the incorrect order. We conducted an additional
experiment to investigate whether social learning emerges
effectively without the exploration penalty. Figure 9 compares the transfer results of agents trained in Goal Cycle
with the standard exploration penalty of ğ‘ = âˆ’1.5, to agents
trained with no penalty (ğ‘ = 0). Without the penalty, agents
in Goal Cycle lack an incentive to learn from experts in the
environment. The solitary policies they learn mirror those
of the three-goal experts, reflected by similar performance
in the four-goal transfer environment. This mirrors the finding that animals engage in social learning when individual
exploration is most costly or dangerous (Laland, 2004).

7.4. Imitation learning details
The imitation learning agents shown in Section 4 were trained
with behavior cloning(Bain & Sammut, 1995). The model
architecture is similar to that of the textscsocial ppo + aux
rec agents, but layers specific to the value function and next
state prediction were unused. The policy parameters ğœƒ were
optimized with Adam(Kingma & Ba, 2014) to minimize the
loss
ğ¿ BC = ED âˆ’ log ğœ‹ ğœƒ (ğ‘ ğ‘’ğ‘¥ ğ‘ğ‘’ğ‘Ÿ ğ‘¡ |ğ‘  ğ‘‘ğ‘’ğ‘šğ‘œ ),
where ğ· is a dataset of expert trajectories consisting of
states ğ‘  ğ‘‘ğ‘’ğ‘šğ‘œ and expert actions ğ‘ ğ‘’ğ‘¥ ğ‘ğ‘’ğ‘Ÿ ğ‘¡ . Figure 11 shows
the returns of imitation learning agents during training.

7.3. Identifying the experts
In most of our experiments, we assume that social learners
are trained in the presence of two high-performing expert
agents. We conduct an additional experiment to determine
if social learners can learn effectively in the presence of
both high-performing experts, and non-experts. In this new

7.5. Q-learning learning in sparse reward environments

12

Before a Temporal Difference (TD) learning agent has
received any reward, it will be difficult for it to learn
to model transition dynamics. Consider as an example

Figure 11. Returns over the course of training imitation learning
agents in the 3 goal Goal Cycle environment with experts.
Figure 12. We use a penalty curriculum to obtain experts in environments where exploration is expensive. In the scenario visualized
here, an agent trained for 81920 episodes in Goal Cycle environments with penalty of 0.5, then continued with a penalty of
1.0

deep Q-learning, in which the Q-function is parameterized by a neural network which encodes the state using a
function ğ‘“ ğœƒ (ğ‘ ). Assume the network is randomly initialized such that all Q-values are small, random values; i.e.
âˆ€ğ‘ âˆˆ A, ğ‘  âˆˆ S, ğ‘„(ğ‘, ğ‘ ) = ğœ– âˆ¼ N (0, 0.1). Assume that
the agent has not yet learned to navigate to the goal, and
has received zero rewards so far during training. Therefore,
when the agent observes the experience (ğ‘ , ğ‘, ğ‘Ÿ = 0, ğ‘  0), the
Q-learning objective is:
ğ½ (ğœƒ) = (ğ‘Ÿ + ğ›¾ max
ğ‘„(ğ‘  0, ğ‘ 0) âˆ’ ğ‘„(ğ‘ , ğ‘)) 2
0
ğ‘

= (0 + ğ›¾ğœ–1 âˆ’ ğœ–2 ) 2

(5)
(6)

In effect, this induces a correlation between ğ‘„(ğ‘  0, ğ‘ 0) and
ğ‘„(ğ‘ , ğ‘), and consequently ğ‘“ ğœƒ (ğ‘  0) and ğ‘“ ğœƒ (ğ‘ ), as a result of
observing the state transition ğ‘  â†’ ğ‘  0. However, as the agent
continues to receive zero reward, all Q-values will be driven
toward zero. Once this occurs, even if the agent observes a
useful novel state such as ğ‘ Ëœ, our equation becomes:
(ğ‘Ÿ + ğ›¾ max
ğ‘„(ğ‘  0, ğ‘ 0) âˆ’ ğ‘„( ğ‘ Ëœ, ğ‘)) 2 = (0 âˆ’ ğ‘„( ğ‘ Ëœ, ğ‘)) 2
0
ğ‘

(7)
Figure 13. Sample 19x19 maze environment.

such that the objective forces the value of ğ‘„( ğ‘ Ëœ, ğ‘) to be zero.
In this case, modeling transitions into or out of state ğ‘ Ëœ is
not required in order to produce an output of zero, since all
other Q-values are already zero.

When the penalty for individual exploration in Goal Cycle
environments is large, agents are unable to learn effective
strategies. We used a penalty curriculum to obtain experts
for such tasks as shown in Figure 12.

7.6. Environment details
The environments used in this paper were originally based on
Minigrid (Chevalier-Boisvert et al., 2018). The partial states
constituting agent observations are 27Ã—27 RGB images
corresponding to 7Ã—7 grid tiles. There are 7 possible
actions, though only three actions (rotate left, rotate right,
move forward) are relevant in the experiments discussed
in this paper. Agents are unable to see or move through
the obstructive tiles that clutter their environments, and
obstructed regions of their partial views appear as purple.
However, agents can both see and move through goal tiles
as well as other agents.

7.7. Network architecture details

13

The value, policy, and auxiliary task networks share three
convolution layers, a fully connected layer, and an LSTM
layer. Values and policies are computed with two fully connected layers, and the prediction-based auxiliary branch has
a fully connected layer followed by transposed convolution
layers that mirror the input convolution layers. The convolution and transposed convolution use leaky ReLU activation
functions; all other layers use tanh activation functions.

â€¢ Shared input layers:

batch size
mini-batches per batch
mini-batch num trajectories
mini-batch trajectory length
hidden state/advantage update interval
return discount ğ›¾
GAE-ğœ†
PPO clip ratio
KL target
KL hard limit

â€“ Conv (3, 32) 3x3 filters, stride 3, padding 0,
â€“ Conv (32, 64) 3x3 filters, stride 1, padding 0,
â€“ Conv (64, 64) 3x3 filters, stride 1, padding 0,
â€“ FC (576, 192),
â€“ LSTM (192, 192)
â€¢ Value MLP:
â€“ FC (192, 64),
â€“ FC (64, 64),
â€“ FC (64, 1)

For each mini-batch iteration, the loss used to update agent
parameters is

â€¢ Policy MLP:

ğ¿ total = ğ¿ ğœ‹ (ğœƒ) + ğ‘ ğ‘‰ Â· ğ¿ ğ‘‰ (ğœƒ) + ğ‘ aux Â· ğ¿ aux (ğœƒ) âˆ’ ğ‘ ent Â· ğ¿ ent (ğœƒ),

â€“ FC (192, 64),
â€“ FC (64, 64),
â€“ FC (64, 7)

where the policy loss ğ¿ ğœ‹ is computed with PPO-clip (Schulman et al., 2017) and GAE (Schulman et al., 2016), the value
loss ğ¿ ğ‘‰ is the mean squared error of the values estimated
for each step in the trajectory, ğ¿ ent (ğœƒ) is the policy entropy
bonus, and ğ¿ aux (ğœƒ) is the auxiliary prediction loss (Equation
3). The loss scaling coefficients used in our experiments are
ğ‘ ğ‘‰ = 0.1, ğ‘ ent = 1e âˆ’ 5, and ğ‘ aux = 3.

â€¢ Auxiliary state prediction layers:
â€“ FC (192 + 7, 576),
â€“ DeConv (64, 64) 3x3 filters, stride 1, padding 0,
â€“ DeConv (64, 32) 3x3 filters, stride 1, padding 0,
â€“ DeConv (32, 3) 3x3 filters, stride 3, padding 0

The prestige decay constant ğ›¼ğ‘ used for the Goal Cycle
environments (i.e. Equation 4) was 0.99.
In general we sought hyperparameters that enable stable
training. We experimented with mini-batch sizes varying
from 32 to 1025 trajectories and found training to be more
stable with larger mini-batches. Training was less stable
with learning rates higher than 1e âˆ’ 4.

Altogether there are 668555 parameters. We experimented
with smaller (357291-parameter) networks but did not observe a significant performance difference. The networks
were sized to roughly saturate the available (desktop) compute resources.

We randomized seeds for both the network parameter initialization and environment generation for each trial of each
experiment.

7.8. Hyperparameters
Each agent uses a single Adam optimizer (Kingma & Ba,
2014) to update its parameters. Each of the novice agents
was trained with a learning rate of 1e âˆ’ 4. The expert agents
were trained with a learning rate of 1e âˆ’ 5. Weights for
all agents in the generalization experiments as well as the
imperfect experts in textscsocial ppo + aux rec (imperfect
experts) were kept frozen and not updated.
Each parameter update consists of 20 sequential mini-batch
updates with the same batch of rollouts (128 episodes).
Each mini-batch consists of a uniform random sample of
trajectories from that batch. Hidden states are stored alongside the trajectories, and the initial hidden state for each
mini-batch trajectory is retrieved from these stored values.
Hidden states and advantage values for the entire batch are
re-calculated every 2 mini-batches. The mini-batch iteration
ceases if KL(ğœ‹, ğœ‹ğ‘Ÿ ğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ ) exceeds a target of 0.01. If for any
mini-batch the estimated divergence exceeds a hard limit of
0.03, the update terminates and all changes to the network
parameters and optimizer state are reverted.

128 episodes
20
512
16
2 minibatches
0.993
0.97
0.2
0.01
0.03

7.9. Compute
The experiments in this paper were performed primarily on
a desktop computer with an AMD Ryzen 3950x CPU and
two Nvidia GTX 1080TI GPUs, as well as g4dn.8xlarge instances provisioned on Amazon AWS. Either system can run
two or three trials simultaneously, each consisting of three
agents training together in a shared environment. Collecting
experience and updating parameters were comparably time
consuming, and a single 1.5M episode (375M step) 3-agent
training run took about 30 hours. We used Ubuntu 18.04
with python3.8 and all neural networks are implemented in
PyTorch v1.6 (Paszke et al., 2019). Training metrics were
logged with Weights and Biases (Biewald, 2020).

14

