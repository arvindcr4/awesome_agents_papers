On the Approximation of Cooperative Heterogeneous
Multi-Agent Reinforcement Learning (MARL) using Mean
Field Control (MFC)

arXiv:2109.04024v3 [cs.LG] 8 May 2022

Washim Uddin Mondal

wmondal@purdue.edu

Lyles School of Civil Engineering,
School of Industrial Engineering,
Purdue University,
West Lafayette, IN, 47907, USA

Mridul Agarwal

agarw180@purdue.edu

School of Electrical and Computer Engineering,
Purdue University,
West Lafayette, IN, 47907, USA

Vaneet Aggarwal

vaneet@purdue.edu

School of Industrial Engineering,
School of Electrical and Computer Engineering,
Purdue University,
West Lafayette, IN, 47907, USA

Satish V. Ukkusuri

sukkusur@purdue.edu

Lyles School of Civil Engineering,
Purdue University,
West Lafayette, IN, 47907, USA ∗

Abstract
Mean field control (MFC) is an effective way to mitigate the curse of dimensionality of
cooperative multi-agent reinforcement learning (MARL) problems. This work considers a
collection of Npop heterogeneous agents that can be segregated into K classes such that the
k-th class contains Nk homogeneous agents. We aim to prove approximation guarantees of
the MARL problem for this heterogeneous system by its corresponding MFC problem. We
consider three scenarios where the reward and transition dynamics of all agents are respectively taken to be functions of (1) joint state and action distributions across all classes, (2)
individual distributions of each class, and (3) marginal distributions of the entire population. We show that, in these cases,√the K-class
MARL problem can be approximated by
√
hp
p iP 1
|X |+ |U | P √
√
Nk ), e2 = O( |X | + |U|
MFC with errors given as e1 = O( Npop
k
k Nk )

h

i
p
p
√
P
, respectively, where A, B are
|X | + |U| NA
Nk + √ B
and e3 = O
k∈[K]
pop
Npop

some constants and |X |, |U| are the sizes of state and action spaces of each agent. Finally, we design a Natural Policy Gradient (NPG) based algorithm that, in the three cases
stated above, can converge to an optimal MARL policy within O(ej ) error with a sample
complexity of O(ej−3 ), j ∈ {1, 2, 3}, respectively.

∗. This work was presented in part at the NeurIPS Workshop on Cooperative AI, Dec. 2021.
The current version is published in the Journal of Machine Learning Research 23(129): 1–46, 2022.

1

Keywords: multi-agent learning, heterogeneous systems, mean-field control, approximation guarantees, policy gradient algorithm

1. Introduction
The control of a large number of interacting agents is a common problem in social science and engineering with applications in finance, smart grids, transportation, wireless
networks, epidemic control, etc. (Schwartz, 2014; Zhang et al., 2021). A common approach
for decision making in such environments is multi-agent reinforcement learning (MARL).
In cooperative MARL, the target is to design a sequence of decision rules or a policy that
instructs the agents how to select actions based on their observed state of the environment
such that the long-term collective reward is maximized. The joint state and action spaces
of the agents, however, increase exponentially with the size of the population. This makes
the computation of reward maximizing policy an incredibly challenging pursuit, especially
when the number of agents is large.
To overcome the exponential blow-up of joint state and action spaces in collaborative
MARL, several computationally efficient approaches have been proposed, including Independent Q-learning (IQL) (Tan, 1993), centralized training with decentralized execution
(CTDE) (Rashid et al., 2020; Sunehag et al., 2018; Son et al., 2019; Rashid et al., 2018),
and mean-field control (MFC) (Angiuli et al., 2020). IQL forces the environment to be nonstationary and thus its global convergence cannot be shown in general (Zhu et al., 2019).
Global convergence for CTDE-type algorithms is also not known. On the other hand, the
core idea of MFC is that, if the population size is infinite and the agents are homogeneous,
then one can draw accurate inferences about the population by studying only one representative agent (Bensoussan et al., 2018). The assumption of homogeneity, however, does
not go hand-in-hand with many scenarios of practical interest. For example, ride-hailing
services typically offer multiple types of vehicles and drivers, each with different accommodation capacity, driving behavior, searching behavior and preferred travel range. If the
profit earned per unit time is considered as reward, then each type of vehicle/driver will
possess a distinct reward function and thus the system as a whole cannot be homogeneous.
It is evident from the above discussion that there are no scalable approaches in the
literature to solve the problem of heterogeneous MARL with global convergence guarantees.
The goal of our paper is to bridge this gap. In particular, we consider a population of Npop
heterogeneous agents that can be partitioned into K classes such that k-th class consists
of Nk homogeneous agents. In other words, the agents in each class are assumed to have
identical reward function and state transition dynamics. However, those functions are
different in different classes. In this framework, we prove that MARL can be approximated
as a K-class MFC problem and obtain the approximation error as a function of different
class sizes. We further develop an algorithm to solve the K-class MFC problem and with
the help of our approximation result, show that it efficiently converges to a provably nearoptimal policy of heterogeneous MARL.
K-class MFC can be depicted as a generalization of traditional MFC-based approach
which as stated before, assumes all agents to be identical. Homogeneity enforces the impact
of the population on any agent to be summarized by the state and action distributions
of the entire population. In contrast, K-class MFC does not allow such simplification.
2

The agents in such a case, not only influence other agents from the same class but their
influence extends to agents from other classes as well. Due to the inter-class interaction,
the influence of the whole population must be summarized either via joint state and action
distributions over all classes or via the collection of distributions of each individual classes.
The analysis of a K-class MFC, as a result, turns out to be very different from that of a
single class/traditional MFC.
1.1 Key Contributions:
We analyse the above heterogeneous system under two generic setups. In the first case, the
reward and transition functions of all agents are assumed to be functions of joint state and
action distributions across all classes while in the second scenario, those are taken to be
functions of state and action distributions of each individual classes. We prove that, in the
first case, the Npop -agent RL
can be approximated by the K-class MFC problem
hpproblem
p i 1 P
√
within an error of e1 = O( |X | + |U | Npop k∈[K] Nk ) where Nk is the population

size of k-th class, k ∈ {1, · · · , K} , [K] and |X |, |U | denote the size of state and action
spaces of individual agents,
respectively.
In the second case, the approximation error is
hp
p iP
√1
proven to be e2 = O( |X | + |U |
k∈[K] Nk ).

h
p
p i 1
|X | + |U | √
For single class of agents, the approximation error reduces to O
Npop

which matches a recent result of (Gu et al., 2020). It is worthwhile to point out that our
proof methods are distinct from that used in (Gu et al., 2020). In particular, at the heart
of our approximation, lies a novel inequality on independent random variables bounded in
[0, 1] with constrained parameters (Lemma 11 of Appendix A). This, in conjugation with
two important observations about state and action evolution of the agents, establishes our
preliminary results. In contrast, (Gu et al., 2020) utilises a well-known property of subGaussian variables. Although for K = 1, both our bound and that suggested in (Gu et al.,
2020) are of the same order, our bounds possess smaller leading constant terms1 .
We also consider a special case where the reward and transition dynamics are functions
of aggregate state and action distributions
of the entire
 population. In this case,the aph
p i A P
p
√
where
|X | + |U | Npop k∈[K] Nk + √ B
proximation error reduces to e3 = O
Npop

A, B are some constants.

Finally, extending the approach in (Liu et al., 2020), we develop a natural policygradient (NPG) based algorithm for MFC, which, combined with the approximation results
between MARL and MFC, shows that the proposed NPG algorithm converges to the optimal MARL policy within O(ej ) error with a sample complexity of O(e−3
j ), j ∈ {1, 2, 3} for
the three cases, respectively.

1. We note that the authors of (Gu et al., 2020) had an incorrect result when we first posted our version on arXiv in Sept 2021 (https://arxiv.org/pdf/2109.04024.pdf), and the error was detailed in our
arXiv version. The authors of (Gu et al., 2020) fixed the error in the final version, acknowledging our
manuscript.

3

2. Related Work
Approaches for RL: Tabular algorithms such as Q-learning (Watkins and Dayan, 1992)
and SARSA (Rummery and Niranjan, 1994) were the earliest approaches to solve RL problems. However, they are not suitable for large state-action space due to their huge memory
requirement. Recently, Deep Q-network (DQN) (Mnih et al., 2015) and policy-gradient
based algorithms (Mnih et al., 2016) have shown promising results in terms of scalability. Although these algorithms can handle large state-space due to neural network (NN)
based architecture, the approach is not scalable to multiple agents. Further, the guarantees
of these algorithms either require the underlying Markov Decision Processes to be linear
(Jin et al., 2020), of low Bellman rank (Jiang et al., 2017), or the scaling of parameters of
NNs to be increasing with time (Wang et al., 2019) - all of which are restrictive assumptions
and may not hold for general MARL.
Use of MFC for MARL problems: MFC has found its application in various
MARL setups. For example, it has been used in traffic signal control (Wang et al., 2020),
management of power grids (Chen et al., 2016), ride-sharing (Al-Abbasi et al., 2019), and
epidemic control (Watkins et al., 2016), among others.
Learning Algorithms for MFC: To solve homogeneous MFC problems, several
learning algorithms have been proposed. For example, model-free Q-learning algorithms
have been suggested in (Angiuli et al., 2020; Gu et al., 2020; Carmona et al., 2019b) while
(Carmona et al., 2019a) designed a policy-gradient based method. Recently, (Pasztor et al.,
2021) proposed a model-based algorithm for MFC. All of these works are appropriate only
for homogeneous MFC.
Theoretical Relation between MARL and MFC: It is well known that when
the number of agents approaches infinity, the limiting behaviour of homogeneous MARL
is described by MFC (Lacker, 2017). However, it was proven only recently (Gu et al.,
2020)pthat for a finite Npop number of agents, MARL is approximated by MFC within
O(1/ Npop ) error margin. Our work is the first to provide such approximation bound for
the heterogeneous MARL.
Mean Field Games: Alongside MFC, mean field games (MFG) has garnered attention in the mean-field community. MFG analyses an infinite population of non-cooperative
homogeneous agents. The target is to identify the Nash equilibrium (NE) of the game
and design learning algorithms that converge to such an equilibrium (Guo et al., 2019;
Elie et al., 2020; Yang et al., 2018; Agarwal et al., 2022).

3. Model for Heterogeneous Cooperative MARL
We consider K classes of agents where the agents belonging to each class are identical and
interchangeable. The population sizePof k-th class, where k ∈ {1, · · · , K} , [K] is Nk , while
the total population size is Npop , k∈[K] Nk . Also, N , {Nk }k∈[K]. Let X , U be (finite)
state and action spaces of each agent. At time t ∈ {0, 1, · · · }, j-th agent belonging to k-th
t,N
class possesses a state xt,N
j,k ∈ X and takes an action uj,k ∈ U . As a consequence, it receives

t,N
a reward rj,k
and its state changes to xt+1,N
following some transition probability law. In
j,k
t,N
t,N
general rj,k
is a function of (xt,N
j,k , uj,k ), i.e, the state and action of the concerned agent at
time t, as well as the joint states and actions of all the agents at time t which are denoted

4

N
by xN
t and ut , respectively. Mathematically,
t,N
t,N
N
N
rj,k
= r̃k (xt,N
j,k , uj,k , xt , ut )

(1)

Note that the function r̃k (·, ·, ·, ·) is identical for all agents of k-th class. This is due to
the fact that the agents of a certain class are homogeneous. Recall that the agents belonging
N
to a given class are interchangeable as well. Thus if µN
t , ν t are empirical joint distributions
of states and actions of all agents at time t, i.e, ∀x ∈ X , ∀u ∈ U , ∀k ∈ [K],
µN
t (x, k) ,

Nk
X

1
Npop

νN
t (u, k) ,

δ(xt,N
j,k = x),

(2)

j=1

1
Npop

Nk
X

δ(ut,N
j,k = u)

(3)

j=1

where δ(·) is an indicator function, then, for some function rk , we can rewrite (1) as
t,N
t,N
N
N
rj,k
= rk (xt,N
j,k , uj,k , µt , ν t , Npop )

(4)

Note that the output of rk , in general, is dependent on the total number of agents, Npop .
Moreover, if, for an arbitrary set A, the collection of all distributions over A is denoted as
N
P(A), then µN
t ∈ P(X × [K]), and ν t ∈ P(U × [K]).
N
We shall now show that (1) can be also written in an alternate form. Let, µ̄N
t , ν̄ t be
N
N
such that µ̄t (., k) and ν̄ t (., k) are state and action distributions of the agents of k-th class,
K
K
N
i.e., µ̄N
t ∈ P (X ) , P(X ) × · · · × P(X ), ν̄t ∈ P (U ), and ∀x ∈ X , ∀u ∈ U , ∀k ∈ [K]
N

µ̄N
t (x, k) ,

k
1 X
δ(xt,N
j,k = x),
Nk

(5)

j=1
N

ν̄ N
t (u, k) ,

k
1 X
δ(ut,N
j,k = u)
Nk

(6)

j=1

With this notation, for some r̄k , we can rewrite (1) as
t,N
t,N
N
N
rj,k
= r̄k (xt,N
j,k , uj,k , µ̄t , ν̄ t , N)

(7)

Note that the output of r̄k is, in general, dependent on N, i.e., the population size of
each of the classes. Similar to (1), the state transition law in general can be written as
t,N
N
N
xt+1,N
∼ P̃k (xt,N
j,k
j,k , uj,k , xt , ut ),

(8)

for some function P̃k . Using the same argument as used in (4) and (7), we can express (8)
in the following two equivalent forms for some functions Pk and P̄k .
t,N
N
N
xt+1,N
∼ Pk (xt,N
j,k
j,k , uj,k , µt , ν t , Npop ),

t,N
N
N
xt+1,N
∼ P̄k (xt,N
j,k
j,k , uj,k , µ̄t , ν̄ t , N)

(9)
(10)

To proceed with the analysis, we need to assume one of the following assumptions to be
true.
5

Assumption 1 (a) ∀k ∈ [K], the outputs of rk , Pk are independent of the last argument
Npop . To simplify notations, Npop can be dropped as argument from both the functions.
(b)|rk (x, u, µ1 , ν 1 )| ≤ MR

(c)|rk (x, u, µ1 , ν 1 ) − rk (x, u, µ2 , ν 2 )| ≤ LR [|µ1 − µ2 |1 + |ν 1 − ν 2 |1 ]

(d)|Pk (x, u, µ1 , ν 1 ) − Pk (x, u, µ2 , ν 2 )|1 ≤ LP [|µ1 − µ2 |1 + |ν 1 − ν 2 |1 ]

∀x ∈ X , ∀u ∈ U , ∀µ1 , µ2 ∈ P(X × [K]), ∀ν 1 , ν 2 ∈ P(U × [K]), ∀k ∈ [K]. The terms
MR , LR , LP denote some positive constants. The function |.|1 indicates L1 -norm.
Assumption 2 (a) ∀k ∈ [K], the outputs of r̄k , P̄k are independent of the last argument
N. For simplifying notations, N can be dropped as argument from both the functions.
(b)|r̄k (x, u, µ̄1 , ν̄ 1 )| ≤ M̄R

(c)|r̄k (x, u, µ̄1 , ν̄ 1 ) − r̄k (x, u, µ̄2 , ν̄ 2 )| ≤ L̄R [|µ̄1 − µ̄2 |1 + |ν̄ 1 − ν̄ 2 |1 ]

(d)|P̄k (x, u, µ̄1 , ν̄ 1 ) − P̄k (x, u, µ̄2 , ν̄ 2 )|1 ≤ L̄P [|µ̄1 − µ̄2 |1 + |ν̄ 1 − ν̄ 2 |1 ]
∀x ∈ X , ∀u ∈ U , ∀µ̄1 , µ̄2 ∈ P K (X ), ∀ν̄ 1 , ν̄ 2 ∈ P K (U ), ∀k ∈ [K]. The terms M̄R , L̄R and L̄P
are constants.
Assumptions 1(a), 2(a) state that the influence of the population on individual agents
is summarized by the state and action distributions only and it does not vary with the
scale of the population. In particular, Assumption 1(a) dictates that such influence is
conveyed through joint state and action distributions across all classes which makes the
reward and transition functions invariant to Npop . In contrast, Assumption 2(a) presumes
that the joint influence of the whole population can be segregated based on the class it
originated from. This makes the reward and transition law invariant to the population size
of each individual class. For single class of agents (i.e., when K = 1), both assumptions
are identical. Scale invariance is one of the fundamental assumptions in the mean-field
literature (Carmona and Delarue, 2018; Gu et al., 2020; Angiuli et al., 2020).
Assumptions 1(b), 2(b) state that the reward functions are bounded while Assumptions
1(c), 2(c) and 1(d), 2(d) dictate that the reward functions and the transition probabilities
are Lipschitz continuous w. r. t. their respective state and action distribution arguments.
These assumptions are common in the literature (Carmona and Delarue, 2018; Gu et al.,
2020; Angiuli et al., 2020).
It is worthwhile to mention that for given rk ’s and Pk ’s satisfying Assumption 1, one can
define equivalent r̄k ’s and P̄k ’s that satisfy Assumption 2 and vice versa. For example, in
appendix P, we exhibit that if rk ’s and Pk ’s satisfy Assumption 1 with Lipschitz constants
LR , LP respectively, then we can define equivalent r̄k ’s and P̄k ’s that satisfy Assumption 2
−1
−1
with constants LP θ −1
M , LQ θ M respectively where θ M , maxk∈[K] {Npop /Nk }. Note that the
modified ‘constants’ are dependent on the population sizes of different classes. Therefore, if
we have an approximation bound for Assumption 2, by injecting the values of the modified
constants into the expression of that bound, we can obtain a bound for Assumption 1. In
appendix P, however, we demonstrate that such translated bounds are, in general, loose.
This is primarily because, in the derivation of the bound for Assumption 2, the Lipschitz
constants are not treated as functions of the population sizes. Therefore, it cannot account
6

for any stringent inequality that might be applicable due to the special structure of the
translated functions. We can similarly argue why a translation from Assumption 2 to
Assumption 1 may not produce a tight result. In summary, although the approximation
result derived for one of the above assumptions can be cast, with slight modifications, as an
approximation result for the other assumption, in general, such translated results are loose.
To derive tighter bounds, it is therefore necessary to produce analysis for each of these
assumptions separately. We shall establish our approximation result first with Assumption
1 and then with Assumption 2.

4. Policy, Value Function and Mean Field Limit under Assumption 1
4.1 Policy and Value Function
N
Recall that the distributions µN
t and ν t defined by (2), (3) are elements of P(X × [K])
and P(U × [K]) respectively. Therefore, presuming Assumption 1 to be true, the reward
function rk for k-th class of agents can be described as a map of the following form, rk :
X × U × P(X × [K]) × P(U × [K]) → R. Similarly, the transition law Pk can be described
as, Pk : X × U × P(X × [K]) × P(U × [K]) → P(X ).
A time-dependent decision rule πkt for k-th class of agents is a map, πkt : X ×P(X ×[K]) →
P(U ). In simple words, a decision rule πkt states with what probability a certain action u ∈ U
should be selected by any agent of k-th class at time t, given its own state and the state
distribution across all classes at time t. A policy π , {(πkt )k∈[K]}t∈{0,1,··· } is defined as a
sequence of decision rules over all classes of agents. For a policy π and given initial states
xN
0 , the infinite-horizon γ ∈ [0, 1)-discounted value of the policy π for j-th agent of k-th
class is defined as
#
"∞
X
t,N
N
N
N
γ t rk (xt,N
(11)
(xN
vj,k
0 , π) = E
j,k , uj,k , µt , ν t ) ,
t=0

t+1,N
t,N
t,N
N
N
N
t
∼ Pk (xt,N
where the expectation is taken over ut,N
j,k , uj,k , µt , ν t ).
j,k ∼ πk (xj,k , µt ), xj,k
N
N
N
Also, µN
t , and ν t are obtained from xt and ut respectively. The average infinite-horizon
discounted value of policy π is defined as

v N (xN
0 , π) ,

1
Npop

Nk
X X

N
(xN
vj,k
0 , π)

(12)

k∈[K] j=1

In the next subsection, we discuss how to compute the mean-field limit of the empirical
value function v N . The following two observations will be useful in many of our forthcoming
analyses.
N
Observation 1 {ut,N
j,k }j∈[Nk ],k∈[K] are independent conditioned on xt , ∀t ∈ {0, 1, · · · }.
′
′
Specifically, for a given policy π, and ∀j ∈ [Nk ], ∀j ∈ [Nk′ ], ∀k, k ∈ [K],
t,N
t,N N
t,N
N
N
P(ut,N
j,k , uj ′ ,k ′ |xt ) = P(uj,k |xt )P(uj ′ ,k ′ |xt )
N
Observation 2 {xt+1,N
}j∈[Nk ],k∈[K] are independent conditioned on xN
t , ut , ∀t ∈ {0, 1, · · · }.
j,k

7

4.2 Mean Field Limit for K Classes
In the mean-field limit, i.e., when Nk → ∞, ∀k ∈ [K], it is enough to consider a representative for each of the classes. The state and action of the representative of k-th class at
time t are indicated as xtk ∈ X and utk ∈ U respectively. The joint distribution of states and
actions of all classes of agents are symbolized as µt ∈ P(X × [K]) and ν t ∈ P(U × [K]).
If Assumption 1 holds, then the reward and the transition probability law of the representative of k-th class at time t can be expressed as, rk (xtk , utk , µt , ν t ) and Pk (xtk , utk , µt , ν t )
respectively. For a given policy, π , {π t }t∈{0,1,... } , π t , {(πkt )k∈[K]}, where {πkt }t∈{0,1,... } is
a sequence of decision rules for k-th class, the action distribution at time t can be obtained
as follows.
ν t = ν MF (µt , π t ) , {νkMF (µt , π t )}k∈[K] ,
X
πkt (x, µt )µt (x, k)
νkMF (µt , π t ) ,

(13)

x∈X

Using the definition of ν MF , the evolution of the state distribution can be written as
µt+1 =P MF (µt , π t ) , {PkMF (µt , π t )}k∈[K],
XX

PkMF (µt , π t ) ,
µt (x, k)πkt (x, µt )(u) × Pk x, u, µt , ν MF (µt , π t )

(14)

x∈X u∈U

Finally, the average reward of k-th class is computed as
XX
rkMF (µt , π t ) ,
µt (x, k)πkt (x, µt )(u) × rk (x, u, µt , ν MF (µt , π t ))

(15)

x∈X u∈U

For a given initial state distribution µ0 , and a policy π, the infinite-horizon γ-discounted
average reward is
v MF (µ0 , π) =

∞
X X

γ t rkMF (µt , π t )

(16)

k∈[K] t=0

In the following section, we show how well the function v N , given by (12) can be approximated by v MF as the population sizes, N and the cardinality of state and action spaces,
indicated by |X | and |U | respectively, become large.

5. MFC as an Approximation of MARL with Assumption 1
To establish the approximation result, we need to restrict the policies to a set Π such that
the following assumption holds.
Assumption 3 Every policy π , {(πkt )k∈[K]}t∈{0,1,··· } in Π is such that, ∀x ∈ X ,∀µ1 , µ2 ∈
P(X × [K]), ∀k ∈ [K]
|πkt (x, µ1 ) − πkt (x, µ2 )|1 ≤ LQ |µ1 − µ2 |1
for some positive real LQ .
8

Assumption 3 states that the decision rules πkt , associated with any policy in Π are
Lipschitz continuous w. r. t. the state distribution argument. Such assumption holds in
practice because the decision rules are commonly realised by Neural Networks possessing
bounded weights (Pasztor et al., 2021). Below we state our first result.
N
Theorem 1 Let xN
0 be the initial states and µ0 be their corresponding distribution. If v
MF
is the empirical value function given by (12) and v
is its mean-field limit defined in (16),
then for any policy, π ∈ Π, the following inequality holds if γSP < 1 and Assumptions 1, 3
are true.


X
p
p
C
1
R
MF

|U |
Nk 
v N (xN
(µ0 , π) ≤
0 , π) − v
1−γ
Npop
k∈[K]




h

i
X
p
p
p
1 
1
SR
1

+ CP
|X | + |U |
Nk ×
−
SP − 1
Npop
1 − γSP
1−γ
k∈[K]

(17)

where SR , MR (1 + LQ ) + LR (2 + LQ ), SP , (1 + LQ ) + LP (2 + LQ ), CR , MR + LR ,
CP , 2 + L P .
N , can be approximated by
Theorem 1 dictates that the empirical value function,
 vP
√ 
1
its mean-field limit, v MF , within an error margin of O Npop
k∈[K] Nk . In a special
p
case, where the number of agents in each classes are equal, the error is O( K/Npop ).
Additionally, Theorem 1 also dictates how the error varies as a functionof the state-action
p 
p
|X | + |U | .
cardinality. For example, given other things as constant, the error is O
The implication of this result is profound. It essentially assures that, if we can come
up with an algorithm to compute the optimal MFC policy, then the obtained policy is
guaranteed to be close to the optimal MARL policy. In practice, an MFC problem is much
easier to solve than a MARL problem, primarily because in MFC, we are needed to keep
track of only one representative agent from each class. Therefore, if the number of agents
is large and individual state-action spaces are relatively small, MFC can be utilized as an
easier route to obtain an approximate MARL solution. However, Theorem 1 also suggests
that the error of approximation increases with the number of classes, K. As a consequence,
if the level of heterogeneity in the population is too high, then MFC may not be a good
approximation of MARL.

5.1 Proof Outline
A detailed proof of Theorem 1 is provided in Appendix A. Here we present a brief outline.
Step 0: v N and v MF respectively are time-discounted average rewards for a finite agent
system and that of an infinite agent system. To estimate their difference, we need to evaluate
the difference between mean rewards of these systems at a given time t.
Step 1: To achieve that, we introduce an intermediate system X whose state-action
evolutions are identical to the N-agent system upto time t, but after that, it follows the
9

update process of an infinite agent system. Our first task is to bound the difference between
the average reward of system X and that of the N-agent system at time t (Lemma 13).
Step 2: The next task is to estimate the difference between the average reward of system
X and the mean-field reward at time t. Using the continuity of mean-field reward function
(Lemma 9), this difference can be bounded by a multiple of the difference between the
empirical state distribution, µN
t of the N-agent system and the mean-field distribution, µt .
Step 3: The difference between µN
t and µt can be obtained in a recursive manner. To
achieve this, we introduce another intermediate system Y whose state, action distributions
upto time t − 1 are same as the N-agent system, but after that, those evolve following
mean-field updates. First, we evaluate the difference between µN
t and the state distribution
of the system Y at time t (Lemma 14).
Step 4: Using the continuity of mean-field state-transition function (Lemma 10), the
difference between µt and the state distribution of system Y at t is upper bounded by a
multiple of the difference between µN
t−1 and µt−1 .
Step 5: Combining the above results, the difference between the average finite-agent
reward and the mean-field reward at time t can be written as a function of t.
Step 6: Taking a γ-discounted sum of the these estimate errors over t, we arrive at the
desired result.

6. MFC as an Approximation of MARL with Assumption 2
We shall now discuss how well the empirical value function is approximated by its meanfield counterpart if Assumption 2 is true. The empirical state and action distributions of
N
k-th class at time t are denoted as µ̄N
t (·, k), ν̄ t (·, k) and defined by (5), (6) respectively.
K
N
K
K
Clearly, µ̄N
t ∈ P (X ) and ν̄ t ∈ P (U ) where P (·) , P(·) × · · · × P(·).
The reward function, r̄k and the transition probability law, P̄k of k-th class of agents
are defined to be functions of the following forms, r̄k : X × U × P K (X ) × P K (U ) → R and
P̄k : X × U × P K (X ) × P K (U ) → P(X ). Similarly, a policy π̄ , {(π̄kt )k∈[K]}t∈{0,1,··· } is
defined as a sequence of collection of decision rules, π̄kt where π̄kt : X × P K (X ) → P(U ).
Similar to Assumption 3, we restrict the policies to a set Π̄ such that the decision rules
associated with each elements of Π̄ are Lipschitz continuous. This is formally expressed as
follows.
Assumption 4 Every policy π̄ , {(π̄kt )k∈[K]}t∈{0,1,··· } in Π̄ is such that, ∀x ∈ X ,∀µ̄1 , µ̄2 ∈
P K (X ), ∀k ∈ [K]
|π̄kt (x, µ̄1 ) − π̄kt (x, µ̄2 )|1 ≤ L̄Q |µ̄1 − µ̄2 |1

for some positive real L̄Q .
For initial states xN
0 , the empirical value of a given policy π̄ is defined as follows.
#
"∞
Nk
X
1 X X
t,N t,N
t
N
N
N N
γ r̄k (xj,k , uj,k , µ̄t , ν̄ t )
E
v̄ (x0 , π̄) =
(18)
Npop
t=0

k∈[K] j=1

t+1,N
t,N
t,N
N
N
N
t
∼ P̄k (xt,N
where the expectation is taken over ut,N
j,k , uj,k , µ̄t , ν̄ t ).
j,k ∼ π̄k (xj,k , µ̄t ), xj,k
MF denotes the mean-field limit of v̄ N , then
N
N
N
Also, µ̄N
t , ν̄ t are obtained from xt , ut . If v̄
the following approximation result holds.

10

K
Theorem 2 If xN
0 are initial states and µ̄0 ∈ P (X ) is the resulting distribution, then
under Assumptions 2, 4, ∀π̄ ∈ Π̄,



X
p
C̄
1
R
MF
√ 
v̄ N (xN
(µ̄0 , π̄) ≤
|U | 
0 , π̄) − v̄
1−γ
Nk
k∈[K]


h



i
X
p
p
1
1
S̄R
1 

√
|X | + |U |
−
×
+C̄P
1−γ
1 − γ S̄P
S̄P − 1
Nk

(19)

k∈[K]

whenever γ S̄P < 1 where v̄ N (·, ·) denotes the empirical value function defined in (18) and
v̄ MF (·, ·) is its mean-field limit. The other terms are given as follows: C̄R , M̄R + L̄R ,
C̄P , 2 + K L̄P , S̄R , M̄R (1 + L̄Q ) + L̄R (2 + K L̄Q ), and S̄P , (1 + K L̄Q ) + K L̄P (2 + K L̄Q ).
Therefore, Theorem 2 asserts
error
in approximating the value function v̄ N by
iP
hpthat the
p
√1
its mean-field limit, v̄ MF , is O( |X | + |U |
k∈[K] Nk ). Note that, Theorem 1 gives a
tighter bound than Theorem 2 (if Lipschitz constants are same in both the cases). This can
be attributed to the fact that the difference between two joint distributions µ, µ′ (which is
used to bound the approximation error in Theorem 1) is, in general, less than the difference
between the resulting collection of distributions of all classes µ̄, µ̄′ (which is used to bound
the approximation error in Theorem 2).

7. Improved Results when Transition and Reward Functions Depend on
Aggregate Distributions
In this section, the transition and reward functions (and thus, the decision rules associated
with the policies) are assumed to be Lipschitz continuous functions of aggregate/marginal
state and action distributions of the entire population. It is easy to see that, this assumption
is stronger than Assumption 1 and 2 as any Lipschitz continuous function of the marginal
distributions is necessarily a Lipschitz continuous function of both the joint distributions
and the collection of distributions of each classes, with the same Lipschitz parameter. We
shall demonstrate that such stronger assumption leads to improved approximation result.
Mathematically, if the reward and state transition functions are indicated as rk ’s, and Pk ’s,
a generic policy is denoted as π , {(πkt )k∈[K] }t∈{0,1,··· } , and the class of policies is defined
by Π, then our assumption can be stated as follows.
Assumption 5 (a) The reward functions, transition dynamics and the decision rules are
of the following form.
rk : X × U × P(X ) × P(U ) → R

Pk : X × U × P(X ) × P(U ) → P(X )

πkt : X × P(X ) → P(U )
11

∀µ1 , µ2 ∈ P(X × [K]), ∀ν 1 , ν 2 ∈ P(U × [K]), ∀x ∈ X , ∀u ∈ U , ∀k ∈ [K], ∀π =
{(πkt )k∈[K] }t∈{0,1,··· } ∈ Π,
(b)|rk (x, u, µ1 [X ], ν 1 [U ])| ≤ MR

(c)|rk (x, u, µ1 [X ], ν 1 [U ]) − rk (x, u, µ2 [X ], ν 2 [U ])| ≤ LR [|µ1 [X ] − µ2 [X ]|1 + |ν 1 [U ] − ν 2 [U ]|1 ]

(d)|Pk (x, u, µ1 [X ], ν 1 [U ]) − Pk (x, u, µ2 [X ], ν 2 [U ])|1 ≤ LP [|µ1 [X ] − µ2 [X ]|1 + |ν 1 [U ] − ν 2 [U ]|1 ]
(e)|πkt (x, µ1 [X ]) − πkt (x, µ2 [X ])|1 ≤ LQ |µ1 [X ] − µ2 [X ]|1

where µ[X ], ν[U ] are marginal distributions on X , U resulting from µ, ν and MR , LR , LP , LQ
are some constants.
Theorem 3 Assume xN
0 to be the initial states and µ0 their corresponding joint distribution. If γSP < 1, and Assumption 5 holds, then for any arbitrary policy, π ∈ Π,
CR p
1
|U | p
1−γ
Npop






hp
′′
′
X
p
p i γCP
S
 SR 
+
|X | + |U |
Nk  + p R 
1−γ
Npop
Npop
k∈[K]




h



i
′′
′
X
p
p
p
S
S
γ
γ
SR
|X | + |U |
Nk  + p P 
−
× P 
+ CP
SP − 1
1 − γSP
1−γ
Npop
Npop
MF
v N (xN
(µ0 , π) ≤
0 , π) − v

k∈[K]

(20)

′ ,
where v N denotes the empirical value function and v MF is its mean-field limit. Also, SR
′′
′
′′
MR + LR , SR , MR LQ + LR (1 + LQ ), SP , 1 + LP , and SP , LQ + LP (1 + LQ ). The
terms SR , SP , CR , CP are defined in Theorem 1.

Theorem 3 states that the error in approximating
the empirical
value function, v N , by 
its
h
i
p
p
√
P
K
√B
mean-field limit, v MF , can be written as O
|X | + |U | NA
k=1 Nk +
pop

Npop

where A, B are some constants. It is easy to show that the approximation error suggested
by Theorem 3 is strictly better than the errors given by Theorem 1, 2. Intuitively, if the
reward and transition functions only depend on the marginal distributions, not on the
joint distributions, then those functions overlook the heterogeneity of the agents and treat
component of the error which
the whole population holistically. This leads to the √ 1
Npop

matches the error of a single class system. However, the reward and transition functions
(and hence, the decision rules) themselves are different for different classes. This variation
enforces the other part of the error to align towards a general heterogeneous system.

8. Global Convergence of MARL using Natural Policy Gradient
Algorithm
The previous sections showed that a K-class heterogeneous MARL can be approximated as
a K-class MFC. This section develops a Natural Policy Gradient (NPG) based algorithm for
12

K-class MFC that can obtain policies with guaranteed optimality gaps for heterogeneous
MARL. We limit our discussion to the category of systems that satisfy the same set of
assumptions as used in Theorem 1. For other assumptions, one can replicate similar result
and the processes have been briefly described in sections 8.1 and 8.2.
Let the policies in the set Π be parametrized by Φ. Without loss of generality, we can
restrict the set Π to comprise of only stationary policies (Puterman, 2014). To simplify
k}
notations, we denote a stationary policy with the parameter Φ as π Φ , {πΦ
k∈[K] , where
k
πΦ ’s are stationary decision rules for each class. In a K-class MFC, we need to track
only one representative agent from each class. The k-th representative takes its action
uk by observing its own state xk and the joint distribution µ. If x , {xk }k∈[K] and
u , {uk }k∈[K] , then K-class MFC can effectively be described as a single agent RL problem
with (x, µ) ∈ X K × P(X × [K]) and u ∈ U K as its state and action respectively. However,
such a system comes with the additional advantage that the actions uk ’s are conditionally
independent given x. It will be clear from our later result (Theorem 5) that, this prevents
the complexity of the problem from being an exponential function of K.
For arbitrary µ ∈ P(X × [K]), x ∈ X K , and u ∈ U K , denote the Q-value and the
advantage value associated with the policy π Φ as QΦ (x, µ, u) and AΦ (x, µ, u) respectively.
The precise definition of Q-function is as follows.
#
"
∞
X X

t
t
t
γ rk xk , uk , µt , ν t x0 = x, µ0 = µ, u0 = u
(21)
QΦ (x, µ, u) = EΦ
k∈[K] t=0

k (xt , µ ), xt ∼ P (xt−1 , ut−1 , µ
where the expectation is computed over utk ∼ πΦ
k k
t−1 , ν t−1 ),
t
k
k
k
∀t ∈ {1, 2, · · · }, ∀k ∈ [K]. Moreover, ∀t ∈ {1, 2, · · · }, µt = P MF (µt−1 , π Φ ), ν t =
ν MF (µt , π Φ ) where P MF (·, ·), ν MF (·, ·) are given by (14), (13) respectively. Incorporating
(21), we now define the advantage function as follows

AΦ (x, µ, u) = QΦ (x, µ, u) − E [QΦ (x, µ, u)] ,
k (x , µ), ∀k ∈ [K].
where the expectation is over uk ∼ πΦ
k
∗
MF
Define vMF (µ0 ) , supΦ∈Rd v (µ0 , π Φ ), µ0 ∈ P(X × [K]), where v MF (·, ·) is the meanfield value function given by (16) and Rd denotes the space of Φ. Consider a sequence of
parameters {Φj }Jj=1 that is recursively calculated by following the natural policy gradient
(NPG) (Kakade, 2001; Liu et al., 2020; Agarwal et al., 2021) update as described below.

Φj+1 = Φj + ηwj , wj , arg minw∈Rd L Φj (w, Φj )
ζµ0

(22)
Φ

where η is the learning rate. The definitions of the function L Φj and the distribution ζµ0j
ζµ0

are provided below. Define the following function ∀Φ, Φ′ ∈ Rd ,
h
2
Y
k
Lζ Φ′ (w, Φ) , E(x,µ,u)∼ζ Φ′ AΦ (x, µ, u) − (1 − γ)wT ∇Φ log
πΦ
(xk , µ)(uk )
µ0

µ0

k∈[K]

(23)

′

and ζµΦ0 (x, µ, u) , (1 − γ)

∞
X

γ τ P(xτ = x, µτ = µ, uτ = u|x0 = x, µ0 = µ, u0 = u, π Φ′ )

τ =0

(24)
13

It is evident from (22) that at each NPG update, one needs to solve a stochastic minimization problem to find the update direction. This sub-problem can be solved by another stochastic gradient descent algorithm with the update equation wj,l+1 = wj,l − αhj,l
(Liu et al., 2020), where α is the learning rate and the update direction hj,l is defined as:
!
Y
Y
1
k
T
k
Â
(x,
µ,
u)
∇
log
πΦ
(xk , µ)(uk )
hj,l , wj,l
∇Φj log
πΦ
(x
,
µ)(u
)
−
Φ
Φ
k
k
j
j
j
j
1−γ
k∈[K]

k∈[K]

Φ

where x = {xk }k∈[K] , u = {uk }k∈[K], (x, µ, u) are sampled from ζµ0j and ÂΦj is an unbiased
estimator of AΦ . The details of procuring the samples and the unbiased estimate is provided
in Algorithm 2 which is based on Algorithm 3 of (Agarwal et al., 2021). In Algorithm 1,
we summarize the NPG-based procedure to obtain the optimal MFC policy.
Algorithm 1 Natural Policy Gradient for K-class MFC
Input: η, α: Learning rates, J, L: Number of execution steps
w0 , Φ0 : Initial parameters, µ0 : Initial state distribution
Initialization: Φ ← Φ0
1: for j ∈ {0, 1, · · · , J − 1} do
2:
wj,0 ← w0
3:
for l ∈ {0, 1, · · · , L − 1} do
Φ
4:
Sample (x, µ, u) ∼ ζµ0j and ÂΦj (x, µ, u) using Algorithm 2
5:
Compute hj,l using (25)
wj,l+1 ← wj,l − αhj,l
6:
end for
1 PL
7:
wj ←
wj,l
L l=1
8:
Φj+1 ← Φj + ηwj
9: end for
Output: {Φ1 , · · · , ΦJ }: Policy parameters
Following Theorem 4.9 of (Liu et al., 2020), we can now state the global convergence
result of NPG as given below. For the result to hold, the following Assumptions need to
be satisfied. These assumptions are similar to Assumptions 2.1, 4.2, 4.4 respectively in
(Liu et al., 2020).
Assumption 6 ∀Φ ∈ Rd , ∀µ0 ∈ P(X × [K]), the matrix Fµ0 (Φ) − χId is positive semidefinite for some χ > 0 where Fµ0 (Φ) is defined as,


T 



Y
Y


k
k
πΦ
(xk , µ)(uk )
Fµ0 (Φ) , E(x,µ,u)∼ζµΦ  ∇Φ log
∇Φ log
πΦ
(xk , µ)(uk ) 
0



k∈[K]

k∈[K]

Assumption 7 ∀Φ ∈ Rd , ∀µ ∈ P(X × [K]), ∀xk ∈ X , ∀uk ∈ U , ∀k ∈ [K],
∇Φ log

Y

k
(xk , µ)(uk )
πΦ

k∈[K]

≤G
1

14

Φ

Algorithm 2 Algorithm to sample (x, µ, u) ∼ ζµ0j and ÂΦj (x, µ, u)

k }
Input: µ0 : Initial joint state distribution, π Φj , {πΦ
: Policy,
j k∈[K]
{Pk (., ., ., .)}k∈[K] : Transition laws, {rk (., ., ., .)}k∈[K] : Reward functions,
θ , {θk }k∈[K] : Prior probabilities of different classes.

1: Sample x0 , {x0k }k∈[K] ∼ µ0 .

k (x0 , µ ), ∀k ∈ [K].
2: Sample u0 , {u0k }k∈[K] ∼ π Φj (x0 , µ0 ) i.e., sample u0k ∼ πΦ
0
k
j

3: ν 0 ← ν MF (µ0 , π Φj ) where ν MF is defined in (13).
4: t ← 0

5: FLAG ← FALSE

6: while FLAG is FALSE do

FLAG ← TRUE with probability 1 − γ.
Execute SystemUpdate
9: end while
10: T ← t
11: Accept (xT , µT , uT ) as a sample.
7:

8:

12: V̂Φj ← 0, Q̂Φj ← 0
13: FLAG ← FALSE
14: SumRewards ← 0

15: while FLAG is FALSE do

FLAG ← TRUE with probability 1 − γ.
17:
Execute SystemUpdate
P
18:
SumRewards ← SumRewards + k∈[K] θk rk (xtk , utk , µt , ν t )
19: end while
16:

20: With probability 12 , V̂Φj ← SumRewards. Otherwise Q̂Φj ← SumRewards.
21: ÂΦj (xT , µT , uT ) ← 2(Q̂Φj − V̂Φj ).

Output: (xT , µT , uT ) and ÂΦj (xT , µT , uT )
Procedure SystemUpdate:
1: Execute the actions ut , {utk }k∈[K] .
t+1
t+1
2: Transition to xt+1 , {xk }k∈[K] following xk ∼ Pk (xtk , utk , µt , ν t ), ∀k ∈ [K].
3: µt+1 ← P MF (µt , π Φj ) where P MF is defined in (14).
t+1
t+1
k (xt+1 , µ
∼ πΦ
4: Sample ut+1 , {uk }k∈[K] ∼ π Φj (xt+1 , µt+1 ) i.e., sample uk
t+1 ),
k
j
∀k ∈ [K].
5: ν t+1 ← ν MF (µt+1 , π Φj ) where ν MF is defined in (13).
6: t ← t + 1
EndProcedure

15

for some positive constant G.
Assumption 8 ∀Φ1 , Φ2 ∈ Rd , ∀µ ∈ P(X × [K]), ∀xk ∈ X , ∀uk ∈ U , ∀k ∈ [K],
∇Φ1 log

Y

k∈[K]

k
πΦ
(xk , µ)(uk ) − ∇Φ2 log
1

Y

k
πΦ
(xk , µ)(uk )
2

k∈[K]

≤ M |Φ1 − Φ2 |1
1

for some positive constant M .
Assumption 9 ∀Φ ∈ Rd , ∀µ0 ∈ P(X × [K]), the following holds true

∗
∗
LζµΦ∗ (wΦ
, Φ) ≤ ǫbias , wΦ
, arg minw∈Rd LζµΦ (w, Φ)
0

0

where Φ∗ is the parameter associated with an optimal policy.
Lemma 4 If {Φj }Jj=1 are computed following Algorithm 1, and Assumptions 6−9 are satisfied, then for appropriate choices of η, α, J, L,
√
J
ǫbias
1 X MF
∗
v (µ0 , π Φj ) ≤
+ ǫ,
vMF (µ0 ) −
J
1−γ
j=1

for arbitrary initial state distribution µ0 ∈ P(X × [K]) and initial parameter Φ0 . The
sample complexity of Algorithm 1 is O(ǫ−3 ). The parameter ǫbias is a constant.
The parameter ǫbias measures the capacity of parametrization. For rich neural network
based policies, we can assume ǫbias to be small (Liu et al., 2020).
Lemma 4 states that, with a sample complexity of O(ǫ−3 ), Algorithm 1 can approximate
the optimal mean-field value function with an error margin of ǫ. Combining this with
Theorem 1, we obtain the following result.
Theorem 5 Let xN
0 be the initial states and µ0 their associated distribution. If the paramJ
eters {Φj }j=1 are obtained by following Algorithm 1, then under Assumptions 1, 3, and the
set of assumptions used in Lemma 4, the following inequality holds for appropriate choices
of η, α, J, L if γSP < 1
√
J
ǫbias
1 X MF
v (µ0 , π Φj ) ≤
+ Ce1
sup v N (µ0 , πΦ ) −
J
1−γ
Φ∈Rd
j=1
(25)
hp
p i 1 X p
where e1 ,
|X | + |U |
Nk
Npop
k∈[K]

where SP is defined in Theorem 1, C is a constant and the parameter ǫbias is defined in
Lemma 4. The sample complexity of the process is O(e−3
1 ).

Theorem 5 states that, with a sample complexity of O(e1−3 ), Algorithm 1 generates a
policy which is within O(e1 ) error of the optimal heterogeneous MARL policy.
Note that both time and space complexity of the sampling step in Algorithm 1 is O(K).
In contrast, if NPG is directly applied to MARL, those complexities increase to O(Npop ).
Therefore, MFC based NPG provides an advantage of the order of Npop /K in comparison
to MARL based NPG.
In the following subsections, we shall establish results similar to Theorem 5 for the set
of assumptions used in Theorem 2 and 3.
16

8.1 NPG with Assumption 2 and 4
If a multi-agent system satisfies Assumption 2, 4, and the set of stationary policies, Π̄
is parametrized by Φ ∈ Rd , then similar to Algorithm 1, an NPG-based algorithm can be
made to obtain its global optimal policy within Π̄. Let this algorithm be denoted as NPG2,4 .
Algorithm NPG2,4 is identical to Algorithm 1 except the joint distribution µ ∈ P(X ×[K]) in
Algorithm 1 is replaced by µ̄ ∈ P K (X ), in NPG2,4 . To show its global convergence, we need
to assume a set of assumptions that are identical to those used in Lemma 4, except the joint
distributions in all those assumptions must be replaced by the collection of distributions
over all classes. Let this set of assumptions be denoted as ASMP2,4 .
Following the same line of argument as is used in Theorem 5, we can derive the result
stated below.
K
Theorem 6 Let xN
0 be the initial states and µ̄0 ∈ P (X ) their associated distribution.
If the parameters {Φj }Jj=1 are obtained by following NPG2,4 , then under Assumptions 2,
4, and ASMP2,4 , the following inequality holds for appropriate choices of the Algorithm
parameters, η, α, J, L if γ S̄P < 1.

sup v̄

N

1
(xN
0 , π̄Φ ) −

J
X

v̄

MF

(µ̄0 , π̄ Φj ) ≤

J
j=1
hp
p i X 1
√
where e2 ,
|X | + |U |
Nk
Φ∈Rd

√

ǫbias
+ C̄e2
1−γ

(26)

k∈[K]

where v̄ N is the empirical value function of the N-agent system, v̄ MF is its mean-field limit,
π̄Φ is the stationary decision rules associated with the policy π̄ Φ , S̄P is defined in Theorem
2, C̄ is a constant and the parameter ǫbias is a measure of the capacity of parametrization.
The sample complexity of the process is O(e−3
2 ).
Theorem 6 states that, with a sample complexity of O(e−3
2 ), Algorithm NPG2,4 can
approximate the empirical value function of MARL within an error margin of O(e2 ).
8.2 NPG with Assumption 5
If a multi-agent system satisfies Assumption 5, and the set of stationary policies, Π is
parametrized by Φ ∈ Rd , then similar to Algorithm 1, an NPG-based algorithm can be
made to obtain its global optimal policy within Π. Let this algorithm be denoted as NPG5 .
Algorithm NPG5 is identical to Algorithm 1 except the joint distribution µ ∈ P(X × [K]) in
Algorithm 1 must be replaced by µ[X ] ∈ P(X ), in NPG5 . To show its global convergence,
we need to assume a set of assumptions that are same as those used in Lemma 4, except the
joint distributions in those assumptions must be replaced by marginal distributions. Let
this set of assumptions be denoted as ASMP5 . Following the same line of argument as is
used in Theorem 5, we can derive the result stated below.
Theorem 7 Let xN
0 be the initial states and µ0 ∈ P(X × [K]) their associated joint distribution. If the parameters {Φj }Jj=1 are obtained by following NPG5 , then under Assumptions
17

5, and ASMP5 , the following inequality holds for appropriate choices of the Algorithm parameters, η, α, J, L if γSP < 1.
√
J
ǫbias
1 X MF
v (µ0 , π Φj ) ≤
+ e3
J
1
−γ
d
Φ∈R
j=1
#
"
K
hp
p i
B
A Xp
|X | + |U |
Nk + p
where e3 ,
Npop
Npop
k=1
sup v N (xN
0 , πΦ ) −

(27)

where v N is the empirical value function of the N-agent system, v MF is its mean-field limit,
πΦ is the stationary decision rules associated with policy π Φ , SP is defined in Theorem 1,
A, B are constants and the parameter ǫbias is a measure of the capacity of parametrization.
The sample complexity of the process is O(e−3
3 ).
Theorem 7 states that, with O(e−3
3 ) sample complexity, Algorithm NPG5 can approximate the empirical value function of MARL within an error margin of O(e3 ).

9. Conclusions
In this paper, we prove that a K-class heterogeneous cooperative MARL problem can be approximated by its associated MFC problem. We also provide estimates of the approximation
error as a function of class sizes for various set of assumptions. Finally, we propose a natural
policy gradient based algorithm that approximates the optimal MARL policy in a sample
efficient manner. Exchangeability among agents is one of the most important assumptions
in MFC-type analyses. It allows the influence of the whole population to be summarized
by the state-action distribution. In many scenarios of practical interest, however, agents
interact only with certain number of neighbouring agents. As a result, the presumption of
exchangeability may only hold locally. Establishing MFC-type approximation for system
with limited agent exchangeability is an important direction to pursue in the future.

18

A. Proof of Theorem 1
The following results are needed to prove the theorem. The proofs of Lemma 8-14 are
relegated to Appendix D-J respectively.
A.1 Continuity Lemmas
Lemma 8 If ν MF (·, ·) is defined by (13), then ∀µ, µ′ ∈ P(X × [K]) and ∀π = {πk }k∈[K]
where πk ’s are decision rules satisfying Assumption 3, the following inequality holds.
|ν MF (µ, π) − ν MF (µ′ , π)|1 ≤ (1 + LQ )|µ − µ′ |1

(28)

Lemma 9 If rkMF (·, ·) satisfies (15), then ∀µ, µ′ ∈ P(X × [K]) and ∀π = {πk }k∈[K] where
πk ’s are decision rules satisfying Assumption 3, the following inequality holds.
X

k∈[K]

|rkMF (µ, π) − rkMF (µ′ , π)| ≤ SR |µ − µ′ |1

(29)

where SR , MR (1 + LQ ) + LR [2 + LQ ]
Lemma 10 If P MF (·, ·) is defined by (14), then ∀µ, µ′ ∈ P(X × [K]) and ∀π = {πk }k∈[K]
where πk ’s denote decision rules satisfying Assumption 3, the following inequality holds.
|P MF (µ, π) − P MF (µ′ , π)|1 ≤ SP |µ − µ′ |1

where SP , (1 + LQ ) + LP [2 + LQ ]

(30)

Lemma 8-10 essentially state that the average reward function, rkMF (·, ·) defined by
(15) and the state and action evolution operators P MF (·, ·), ν MF (·, ·) defined by (14), (13)
respectively are Lipschitz continuous. These lemmas will be important in deriving the main
result.
A.2 Approximation Lemmas
Recall that our primary goal is to prove that the value functions generated by a certain
policy in a finite agent system can be well approximated by those generated by the same
policy in the mean-field limit. As a precursor to this grand target, in this section, we discuss
how various components of the value functions themselves behave when the population sizes
become large. Lemma 11 serves as a key ingredient in many of the forthcoming lemmas.
Lemma 11
P If ∀m ∈ [M ], {Xm,n }n∈[N ] are independent random variables bounded within
[0, 1] with m∈[M ] E[Xm,n ] = 1, ∀n ∈ [N ] and {Cm,n }m∈[M ],n∈[N ] ∈ R are constants obeying
|Cm,n | ≤ C, ∀m ∈ [M ], ∀n ∈ [N ], then the following holds.
M
X

m=1

E

N
X

n=1



√
Cm,n Xm,n − E[Xm,n ] ≤ C M N

19

(31)

Below we state our first approximation result. Essentially, Lemma 12 provides an estimate of the difference between the empirical action distributions, ν N
t and the action
distribution that would have been obtained by following the mean-field action evolution
operator ν(·, ·), defined by (13), in a finite agent system.

N
Lemma 12 If {µN
t , ν t }t∈{0,1,··· } are empirical joint state and action distributions induced
by the policy π = {π t }t∈{0,1,··· } , then the following inequality holds ∀t ∈ {0, 1, · · · }.


X
p
p
1 
MF
E|ν N
(µN
Nk  |U |
(32)
t −ν
t , π t )|1 ≤
Npop
k∈[K]

Lemma 13 (stated below) bounds the error between the empirical average reward and
the reward obtained by following the mean-field averaging process quantified by (15).

N
Lemma 13 If {µN
t , ν t }t∈{0,1,··· } are empirical joint state and action distributions induced
by the policy π = {π t }t∈{0,1,··· } , then the following holds ∀t ∈ {0, 1, · · · }.


Nk
X
X
X
X
p
p
1
1 
N
E
rk (xt,N
, ut,N
, µN
rkMF (µN
Nk 
t , π t ) ≤ CR |U |
t , νt ) −
j,k
j,k
Npop
Npop
k∈[K] j=1

k∈[K]

k∈[K]

(33)

where CR = MR + LR .
Finally, Lemma 14 computes an upper bound on the error between the empirical state
distribution, µN
t+1 and the distribution that would have been obtained by following the
mean-field state distribution evolution operator P (·, ·), defined by (14) in a finite agent
system.
Lemma 14 If {µN
t }t∈{0,1,··· } are empirical joint state distributions induced by the policy
π = {π t }t∈{0,1,··· } , then the following inequality holds ∀t ∈ {0, 1, · · · }.


hp
i
X
p
p
1 
MF
E µN
(µN
(34)
|X | + |U |
Nk 
t+1 − P
t , π t ) 1 ≤ CP
Npop
k∈[K]

where CP = 2 + LP .

A.3 Proof of the Theorem
We are now ready to prove the theorem. Using (11), (12), and (16), we can write,
MF
v N (xN
(µ0 , π) ≤ J1 + J2
0 , π) − v

(35)

where the first term J1 is defined as follows:
J1 ,

∞
X
t=0

(a)

≤

γtE

1
Npop

Nk
X X

k∈[K] j=1



t,N
N
N
rk (xt,N
j,k , uj,k , µt , ν t ) −



1 X p 
CR p
|U |
Nk
1−γ
Npop
k∈[K]

20

X

k∈[K]

rkMF (µN
t , πt)

The inequality (a) follows from Lemma 13. The second term, J2 is given as follows:
J2 ,
≤
(a)

=

≤

∞
X

γt

t=0

k∈[K]

rkMF (µt , π t ) −

∞
X

X

γt

t=0

k∈[K]

∞
X

X



rkMF (µt , π t ) − E rkMF (µN
t , πt)

X



E rkMF (µt , π t ) − rkMF (µN
t , πt )

γt

t=0

k∈[K]

∞
X

X

γt

t=0

k∈[K]

X

k∈[K]



E rkMF (µN
t , πt)

(36)

(b)

E rkMF (µt , π t ) − rkMF (µN
t , π t ) ≤ SR

∞
X
t=0

γ t E µN
t − µt 1

!

Equation (a) holds because the sequence {µt }t∈{0,1,··· } is deterministic. Inequality (b)
is due to Lemma 9. Observe that, ∀t ≥ 0 the following holds,


MF
MF
N
(37)
µN
µN
E µN
t , π t − µt+1 1
t , πt 1 + E P
t+1 − µt+1 1 ≤ E µt+1 − P

The first term can be upper bounded by invoking Lemma 14. Using Lemma 10, the
second term can be upper bounded as follows:


MF
MF
(µt , π t ) 1 ≤ SP (E|µN
µN
E P MF µN
t − µt |1 ) (38)
t , πt − P
t , π t − µt+1 1 = E P
Recall that, µ0,N = µ0 . Therefore,

E µN
t+1 − µt+1 1 ≤ CP
≤ CP

hp
hp

|X | +
|X | +

p

i
|U |

p

i
|U |

1
Npop





1 
Npop

X p

k∈[K]

X p

k∈[K]

Clearly, J2 is upper bounded as follows,
J2 ≤ CP



SR
SP − 1

h

p
p i
|X | + |U |

1
Npop

This completes the proof of (17).






Nk  + S P E µ N
t − µt 1
Nk 

X p

k∈[K]





Nk 

SPt+1 − 1
SP − 1





!

1
1
−
1 − γSP
1−γ

(39)



B. Proof of Theorem 2
The collection of empirical state and action distributions of all classes at time t are denoted
K
K
N
as µ̄N
t ∈ P (X ) and ν̄ t ∈ P (U ) respectively and their mean-field counterparts are
µ̄t , ν̄ t . The prior probability of k-th class, k ∈ [K] will be denoted as θk = Nk /Npop and
θ , {θk }k∈[K] .
21

B.1 Mean-field equations
For a policy π̄ , {π̄ t }t∈{0,1,··· } , {(π̄kt )k∈[K] }t∈{0,1,··· } , the mean-field action distribution is
updated as,
ν̄ t = ν̄ MF (µ̄t , π̄ t ) , {ν̄kMF (µ̄t , π̄ t )}k∈[K]
X
ν̄kMF (µ̄t , π̄ t ) ,
π̄kt (x, µ̄t )µ̄t (x, k)

(40)

x∈X

Similarly, the state distribution is updated as,
µ̄t+1 = P̄ MF (µ̄t , π̄ t ) , {P̄kMF (µ̄t , π̄ t )}k∈[K]
XX

P̄k x, u, µ̄t , ν̄ MF (µ̄t , π̄ t ) × µ̄t (x, k)π̄kt (x, µ̄t )(u)
P̄kMF (µ̄t , π̄ t ) ,

(41)

x∈X u∈U

Finally, the average reward of k-th class are computed as,
r̄kMF (µ̄t , π̄ t ) ,

XX

x∈X u∈U

r̄k (x, u, µ̄t , ν̄ MF (µ̄t , π̄ t )) × µ̄t (x, k)π̄kt (x, µ̄t )(u)

(42)

For an initial state distribution µ̄0 , and a policy π̄, the infinite-horizon γ-discounted
average reward in the mean-field limit is,
v̄ MF (µ̄0 , π̄) =

X

θk

k∈[K]

∞
X

γ t r̄kMF (µ̄t , π̄ t )

(43)

t=0

B.2 Helper Lemmas
The following results are necessary to prove the theorem. The proofs of Lemma 15, and 16
have been relegated to Appendix K, and L respectively.
Lemma 15 The following inequalities hold ∀µ̄, µ̄′ ∈ P K (X ) and ∀π̄ = {π̄k }k∈[K] where
π̄k ’s are decision rules satisfying Assumption 4.
(a) |ν̄ MF (µ̄, π̄) − ν̄ MF (µ̄′ , π̄)|1 ≤ (1 + K L̄Q )|µ̄ − µ̄′ |1
X
(b)
θk |r̄kMF (µ̄, π̄) − r̄kMF (µ̄′ , π̄)| ≤ S̄R |µ̄ − µ̄′ |1
k∈[K]

(c) |P̄ MF (µ̄, π̄) − P̄ MF (µ̄′ , π̄)|1 ≤ S̄P |µ̄ − µ̄′ |1

where S̄R , M̄R (1 + L̄Q ) + L̄R (2 + K L̄Q ) and S̄P , (1 + K L̄Q ) + K L̄P (2 + K L̄Q ).
N
Lemma 16 If {µ̄N
t , ν̄ t }t∈{0,1,··· } are the collections of empirical state and action distributions of each classes induced by policy π̄ = {π̄ t }t∈{0,1,··· } , then the following inequalities

22

hold true ∀t ∈ {0, 1, · · · }.


MF

(a) E|ν̄ N
(µ̄N
t − ν̄
t , π̄ t )|1 ≤

(b) E

Nk
X X

1

Npop

k∈[K] j=1

X

k∈[K]


1 p
√
|U |
Nk

t,N
N
N
r̄k (xt,N
j,k , uj,k , µ̄t , ν̄ t ) −





X

(44)

θk r̄kMF (µ̄N
t , π̄ t )

k∈[K]

(45)

p
1
√  |U |
≤ C̄R 
Nk
k∈[K]


hp
X 1
p i
MF

√ 
(c) E µ̄N
(µ̄N
|X | + |U |
t+1 − P̄
t , π̄ t ) 1 ≤ C̄P
Nk
X

(46)

k∈[K]

where C̄R , M̄R + L̄R , and C̄P , 2 + K L̄P .
B.3 Proof of the Theorem

We are now ready to prove the theorem. Using (18) and (43), we can write,
MF
v̄ N (xN
(µ̄0 , π̄) ≤ J1 + J2
0 , π̄) − v̄

(47)

where the first term J1 is defined as follows:
J1 ,

∞
X

t

γE

Nk
X X

1

t,N
N
N
r̄k (xt,N
j,k , uj,k , µ̄t , ν̄ t ) −

Npop
t=0
k∈[K] j=1


X
(a) C̄
p
1
R 
√  |U |
≤
1−γ
Nk

X

θk r̄kMF (µ̄N
t , π̄ t )

k∈[K]

k∈[K]

The inequality (a) follows from Lemma 16. The second term, J2 is given as follows:
J2 ,

∞
X

γt

t=0

≤
(a)

∞
X
t=0

≤ S̄R

θk r̄kMF (µ̄t , π̄ t ) −

X

θk E r̄kMF (µ̄t , π̄ t ) − r̄kMF (µ̄N
t , π̄ t )

k∈[K]

γt

X

X

k∈[K]
∞
X
t=0

t

γ E µ̄N
t − µ̄t 1

k∈[K]



θk E r̄kMF (µ̄N
t , π̄ t )
(48)

!

Inequality (a) is due to Lemma 15. Observe that, ∀t ≥ 0 the following holds,


MF
N
MF
E µ̄N
µ̄N
µ̄N
t+1 − µ̄t+1 1 ≤ E µ̄t+1 − P̄
t , π̄ t 1 + E P̄
t , π̄ t − µ̄t+1 1
23

(49)

The first term can be upper bounded by invoking Lemma 16. Using Lemma 15, the
second term can be upper bounded as follows:


MF
MF
(µ̄t , π̄ t ) 1 ≤ S̄P (E|µ̄N
µ̄N
E P̄ MF µ̄N
t − µ̄t |1 ) (50)
t , π̄ t − P̄
t , π̄ t − µ̄t+1 1 = E P̄
Recall that, µ̄0,N = µ̄0 . Therefore,



hp
i X 1
p

√  + S̄P E µ̄N
E µ̄N
|X | + |U | 
t+1 − µ̄t+1 1 ≤ C̄P
t − µ̄t 1
Nk
k∈[K]


!
t+1
hp
p i X 1
−
1
S̄
P
√ 
|X | + |U | 
≤ C̄P
S̄P − 1
Nk

(51)

k∈[K]

Clearly, J2 is upper bounded as follows,
J2 ≤ C̄P



S̄R
S̄P − 1

h

p

|X | +

p





i X 1
1
1
√ 
|U | 
−
1−γ
1 − γ S̄P
Nk
k∈[K]

C. Proof of Theorem 3
The following results are required to prove the theorem. The proofs of Lemma 17, 18
are given in Appendix M, N respectively. We define mean-field state, action distribution
evolution functions P MF (·, ·), ν MF (·, ·) and the class-average reward functions rkMF (·, ·)’s by
(14), (13), (15), respectively.
C.1 Helper Lemmas
Lemma 17 The following inequalities hold ∀µ, µ′ ∈ P(X ×[K]) and ∀π = {πk }k∈[K] where
πk ’s denote Lipschitz continuous decision rules with parameter LQ .
(a) |ν MF (µ, π)[U ] − ν MF (µ′ , π)[U ]|1 ≤ |ν MF (µ, π) − ν MF (µ′ , π)|1
(b)

X

k∈[K]

≤ |µ − µ′ |1 + LQ |µ[X ] − µ′ [X ]|1

′
′′
|rkMF (µ, π) − rkMF (µ′ , π)| ≤ SR
|µ − µ′ |1 + SR
|µ[X ] − µ′ [X ]|1

(c) |P MF (µ, π)[X ] − P MF (µ′ , π)[X ]|1 ≤ |P MF (µ, π) − P MF (µ′ , π)|1

≤ SP′ |µ − µ′ |1 + SP′′ |µ[X ] − µ′ [X ]|1

′ , M +L , S ′′ , M L +L (1+L ), S ′ , 1+L , and S ′′ , L +L (1+L ).
where SR
R
R
R Q
R
Q
P
Q
P
Q
R
P
P
′ + S ′′ = S
′ + S ′′ = S
Note that, SR
and
S
where
S
,
S
are
defined
in
(29),
(30)
R
P
R
P
R
P
P
respectively.

Similar to Lemma 12, 13, and 14, we can derive the approximation results as follows.

24

N
Lemma 18 If {µN
t , ν t }t∈{0,1,··· } are the empirical joint state and action distributions
induced by the policy π = {π t }t∈{0,1,··· } , then the following inequalities hold true ∀t ∈
{0, 1, · · · }.
MF
(a) E|ν N
(µN
t [U ] − ν
t , π t )[U ]|1 ≤ p
N

1 p
|U |
Npop

(52)

k
X
1 X X
CR p
t,N
N
N
MF
N
p
|U |
rk (xt,N
,
u
,
µ
[X
],
ν
[U
])
−
r
(µ
,
π
)
(b) E
≤
t
t
t
k
t
j,k
j,k
Npop
Npop

k∈[K] j=1

k∈[K]

(53)

CP
MF
(c) E µN
(µN
t+1 [X ] − P
t , π t )[X ] 1 ≤ p
Npop

hp

|X | +

p

i
|U |

(54)

where CR , CP are same as defined in Lemma 13, 14 respectively.
C.2 Proof of the Theorem
Following the proof of Theorem 1, we can write,
MF
v N (xN
(µ0 , π) ≤ J1 + J2
0 , π) − v

(55)

where the first term J1 is defined as follows:
J1 ,

∞
X
t=0

γ tE

1
Npop

Nk
X X

k∈[K] j=1

t,N
N
N
rk (xt,N
j,k , uj,k , µt [X ], ν t [U ]) −

X

rkMF (µN
t , πt)

k∈[K]

(56)

(a)

CR p
1
≤
|U | p
1−γ
Npop

The inequality (a) follows from Lemma 18. The second term, J2 is given as follows:
J2 ,

∞
X

γt

t=0

≤
(a)

≤

∞
X

rkMF (µt , π t ) −

X

E rkMF (µt , π t ) − rkMF (µN
t , πt)

k∈[K]

γt

X

X

k∈[K]



E rkMF (µN
t , πt)

t=0

k∈[K]

∞
X

 ′
N
′′
γ t SR
E µN
t − µt 1 + SR E µt [X ] − µt [X ] 1

t=0

(57)

Inequality (a) is due to Lemma 17. Observe that, ∀t ≥ 0 the following holds,
′′
N
′
SR
E µN
t+1 − µt+1 1 + SR E µt+1 [X ] − µt+1 [X ] 1


′′
N
MF
′
MF
µN
≤ SR
E µN
µN
t , π t [X ] 1
t+1 − P
t , π t 1 + SR E µt+1 [X ] − P


′
′′
MF
E P MF µN
+ SR
µN
t , π t − µt+1 1 + SR E P
t , π t [X ] − µt+1 [X ] 1

25

(58)

The first two terms can be upper bounded by invoking Lemma 14 and 18 respectively.
Utilising Lemma 17, the last two term can be upper bounded as follows:


MF
E P MF µN
µN
t , π t [X ] − µt+1 [X ] 1 ≤ E P
t , π t − µt+1 1

MF
(µt , π t ) 1
= E P MF µN
t , πt − P

(59)

′′
N
≤ SP′ (E|µN
t − µt |1 ) + SP (E|µt [X ] − µt [X ]|1 )

Therefore, (58) can be rewritten as,
′′
′
N
SR
E µN
t+1 − µt+1 1 + SR E µt+1 [X ] − µt+1 [X ] 1




hp
′′
X p
p i S′
S
≤ CP
|X | + |U |  R 
Nk  + p R 
Npop
Npop
k∈[K]


′
N
+
S
E
µ
[X
]
−
µ
[X
]
+ SR SP′ E µN
−
µ
t
t 1
P
t
t
1

(60)

′ + S ′′ . Similarly, one can show that,
where SR = SR
R

′′
N
SP′ E µN
t+1 − µt+1 1 + SP E µt+1 [X ] − µt+1 [X ] 1




hp
′′
X p
p i S′
S
|X | + |U |  P 
Nk  + p P 
≤ CP
Npop
Npop
k∈[K]


′
N
−
µ
+
S
E
µ
[X
]
−
µ
[X
]
+ SP SP′ E µN
t
t 1
P
t
t
1

(61)

Recall that µN
0 = µ0 . Combining the above results, we therefore obtain,
′′
′
N
SR
E µN
t+1 − µt+1 1 + SR E µt+1 [X ] − µt+1 [X ] 1




(
hp
′′
′
X
p i
p
S
 SR 
≤ CP
|X | + |U |
Nk  + p R 
Npop
Npop
k∈[K]




 t
)
′′
′
X
p
S
S
−
1
S
P
Nk  + p P 
+ SR  P 
Npop
SP − 1
Npop

(62)

k∈[K]

Clearly, J2 is upper bounded as follows,






hp
′
′′
X
p
p i
S
γ
 SR 
|X | + |U |
Nk  + p R 
J2 ≤ CP
1−γ
Npop
Npop
k∈[K]






h

i
′′
′
X
p
p
p
SP 
γ
SR
γ
SP 


+ CP
−
|X | + |U |
Nk + p
SP − 1
Npop
1 − γSP
1−γ
Npop
k∈[K]
This completes the proof of the Theorem.

26

D. Proof of Lemma 8
The following chain of inequalities hold true.
|ν MF (µ, π) − ν MF (µ′ , π)|1
X
=
|νkMF (µ, π) − νkMF (µ′ , π)|1
k∈[K]

X X

=

k∈[K] x∈X

µ(x, k)πk (x, µ) −

X X X

=

k∈[K] u∈U x∈X

≤
≤

X XX

k∈[K] u∈U x∈X

X X

k∈[K] x∈X

≤

X X

k∈[K] x∈X

x∈X

1

X

µ′ (x, k)πk (x, µ′ )(u)

x∈X

µ(x, k)πk (x, µ)(u) − µ′ (x, k)πk (x, µ′ )(u)

X X

X

πk (x, µ)(u)

u∈U

µ′ (x, k)

k∈[K] x∈X
(a)

µ′ (x, k)πk (x, µ′ )

µ(x, k)πk (x, µ)(u) −

|µ(x, k) − µ′ (x, k)|
+

X

X

u∈U

|πk (x, µ)(u) − πk (x, µ′ )(u)|

|µ(x, k) − µ′ (x, k)| + LQ |µ − µ′ |1

X X

µ′ (x, k)

k∈[K] x∈X

(b)

= (1 + LQ ) |µ − µ′ |1

Inequality (a) follows from Assumption 3 and the fact that πk (x, µ) is a distribution.
Finally, equality (b) uses the fact that µ′ is a distribution. This concludes the result.

E. Proof of Lemma 9
Note that,
X

|rkMF (µ, π) − rkMF (µ′ , π)|

k∈[K]

≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

|rk (x, u, µ, ν MF (µ, π)) − rk (x, u, µ′ , ν MF (µ′ , π))| × µ(x, k)πk (x, µ)(u)
|rk (x, u, µ′ , ν MF (µ′ , π))| × |µ(x, k)πk (x, µ)(u) − µ′ (x, k)πk (x, µ′ )(u)|

Utilising Assumption 1(c), and the facts that µ, πk (x, µ) are probability distributions,
the first term can be upper bounded by the following expression,

LR |µ − µ′ |1 + |ν MF (µ, π) − ν MF (µ′ , π)|1 ≤ LR [1 + (1 + LQ )] |µ − µ′ |1
27

Lemma 8 is applied to derive the above inequality. Utilising Assumption 1(b), the second
term can be upper bounded by the following quantity:
X XX

MR

k∈[K] x∈X u∈U

(a)

|µ(x, k)πk (x, µ)(u) − µ′ (x, k)πk (x, µ′ )(u)| ≤ MR (1 + LQ )|µ − µ′ |1

Inequality (a) can be proved using identical arguments as used in Lemma 8. This
concludes the result.

F. Proof of Lemma 10
Note that,
|P MF (µ, π) − P MF (µ′ , π)|1 =
≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

X

k∈[K]

|PkMF (µ, π) − PkMF (µ′ , π)|1

|Pk (x, u, µ, ν MF (µ, π)) − Pk (x, u, µ′ , ν MF (µ′ , π))|1 × µ(x, k)πk (x, µ)(u)
|Pk (x, u, µ′ , ν MF (µ′ , π))|1 × |µ(x, k)πk (x, µ)(u) − µ′ (x, k)πk (x, µ′ )(u)|

Utilising Assumption 1(d), and the facts that µ, πk (x, µ) are probability distributions,
the first term can be upper bounded by the following expression,

LP |µ − µ′ |1 + |ν MF (µ, π) − ν MF (µ′ , π)|1 ≤ LP [1 + (1 + LQ )] |µ − µ′ |1

Lemma 8 is applied to derive the above inequality. Note that, |Pk (x, u, µ′ , ν(µ′ , π))|1 =
1. Therefore, the second term can be bounded by the following quantity.
X XX

k∈[K] x∈X u∈U

(a)

|µ(x, k)πk (x, µ)(u) − µ′ (x, k)πk (x, µ′ )(u)| ≤ (1 + LQ )|µ − µ′ |1

Inequality (a) can be proved using identical arguments as used in Lemma 8. This
concludes the result.

G. Proof of Lemma 11
Let, Ym,n , Xm,n − E[Xm,n ], ∀m ∈ [M ], ∀m ∈ [N ]. We need the following results to prove
Lemma 11.
2 ] ≤ E[X
Proposition 19 ∀m ∈ [M ], ∀n ∈ [N ], E[Ym,n
m,n ].

Proof For random variables Xm,n ∈ [0, 1], note that,
2
2
E[Ym,n
] = E[Xm,n
] − (E[Xm,n ])2

≤ E[Xm,n ] − (E[Xm,n ])2 ≤ E[Xm,n ]

28

P
PN
2
2
2
Proposition 20 ∀m ∈ [M ], E[ N
n=1 Cm,n Ym,n ] ≤ C
n=1 E[Ym,n ].
Proof Using the independence of Ym,n ’s, we deduce, ∀m ∈ [M ],

E

"N
X

Cm,n Ym,n

n=1

=E

" N N
X X

#2

Cm,n1 Cm,n2 Ym,n1 Ym,n2

n1 =1 n2 =1

=

N
X

2
Cm,n
E

n=1
N
(a) X

=

n=1

≤ C2



2
Ym,n



+2

N
N
X
X

#

Cm,n1 Cm,n2 E[Ym,n1 ]E[Ym,n2 ]

n1 =1 n2 >n1

 2 
2
Cm,n
E Ym,n

N
X

2
]
E[Ym,n

n=1

Equality (a) uses the fact that E[Ym,n ] = 0, ∀m ∈ [M ], ∀n ∈ [N ].
We are now ready to prove Lemma 11. Note that,
M
X

E

m=1
(a) √

≤

N
X

Cm,n Ym,n

n=1

M


M
X


m=1

E

"N
X

n=1

√
≤C M

( M N
XX

√
≤C M

(N M
XX

(b)

(c)

m=1 n=1

√
= C MN

Cm,n Ym,n

n=1 m=1

 2 
E Ym,n
E [Xm,n ]

) 21

#2  21



) 21

Result (a) is a consequence of Cauchy-Schwarz inequality, and (b), (c) follow from
Proposition 20, and 19 respectively. This concludes the result.
29

H. Proof of Lemma 12
Using the definition of L1 -norm, we get:
MF
E νN
(µN
t −ν
t , πt) 1 =

X X

k∈[K] u∈U

=

1
Npop

MF
E νN
(µN
t (u, k) − ν
t , π t )(u, k)

X X

E

Nk
X
j=1

k∈[K] u∈U

δ(ut,N
j,k = u) −

Nk
X

N
πkt (xt,N
j,k , µt )(u)

j=1

Recall from Observation 1 that, the random variables ut,N
j,k ’s are independent conditioned
N
on xt . Also, it is easy to check the following relations,
i
i
h
X h t,N
t,N
N
N
t
N
=1
(x
,
µ
)(u)
and
E
δ(u
=
u)
x
=
π
E δ(ut,N
=
u)
x
t
t
t
k j,k
j,k
j,k
u∈U

Using Lemma 11, we therefore conclude:

MF
E νN
(µN
t −ν
t , πt) 1 ≤

1
Npop




X p

k∈[K]



Nk 

p

|U |

I. Proof of Lemma 13
Note that,
1

rkMF (µN
t , πt) =

Npop
1

=

Npop

Nk X X
X
j=1 x∈X u∈U

Nk X
X
j=1 u∈U

t,N
N
MF
t
rk (x, u, µN
(µN
t ,ν
t , π t )) × πk (x, µt )(u)δ(xj,k = x)

t,N
N
N MF
t
rk (xt,N
(µN
t , π t )) × πk (xj,k , µt )(u)
j,k , u, µt , ν

We can upper bound the LHS of (33) by J1 + J2 where J1 is defined as follows.

J1 ,

1
Npop
1

≤

Npop

(a)

1

≤

E

Nk
X X

k∈[K] j=1

E

Npop

Nk
X X

k∈[K] j=1
Nk
X X

k∈[K] j=1

t,N
t,N t,N
N
N
N MF
rk (xt,N
(µN
t , π t ))
j,k , uj,k , µt , ν t ) − rk (xj,k , uj,k , µt , ν

t,N
t,N t,N
N
N
N MF
rk (xt,N
(µN
t , π t ))
j,k , uj,k , µt , ν t ) − rk (xj,k , uj,k , µt , ν



X
p
p
L
R 
MF
LR E ν N
(µN
Nk  |U |
t −ν
t , πt) 1 ≤
Npop
(b)

k∈[K]

30

Inequality (a) follows from Assumption 1(c) whereas inequality (b) follows from Lemma
12. The term J2 is defined below.
J2 , E

≤

1
Npop
1

Npop

Nk X
X X

h
i
t,N
t,N
N MF
N
t
N
rk (xt,N
,
u,
µ
,
ν
(µ
,
π
))
×
δ(u
=
u)
−
π
(x
,
µ
)(u)
t
t
t
k j,k
t
j,k
j,k

Nk
X X

i
h
t,N
t,N
N
N MF
N
t
(x
,
µ
)(u)
rk (xt,N
,
u,
µ
,
ν
(µ
,
π
))
×
δ(u
=
u)
−
π
t
t
t
t
k j,k
j,k
j,k

k∈[K] j=1 u∈U

X

u∈U

E

k∈[K] j=1

N
Recall from Observation 1 that ut,N
j,k ’s are independent conditioned on xt . Therefore,
N
∀u ∈ U , δ(ut,N
j,k = u)’s are independent, conditioned on xt . Moreover,

i
h
N
N
= πkt (xt,N
x
E δ(ut,N
=
u)
t
j,k , µt )(u), ∀u ∈ U ,
j,k
i
X h t,N
=1
E δ(uj,k = u) xN
t
u∈U

MF
and |rk (x, u, µN
(µN
t , π t ))| ≤ MR , ∀x ∈ X , ∀u ∈ U
t ,ν

Using Lemma 11, we therefore get,


X
p
p
p
MR
MR 
J2 ≤ √
|U | ≤
Nk  |U |
N
N
k∈[K]

This concludes the result.

J. Proof of Lemma 14
Note that the LHS of (34) can be upper bounded as follows.
X X

MF
LHS =
E µN
µN
t+1 (x, k) − Pk
t , π t (x)
k∈[K] x∈X

=

X X

k∈[K] x∈X

1
Npop

E

Nk
X

δ(xt+1,N
= x)
j,k

j=1

−
≤ J1 + J2 + J3

Nk X
X

t,N
N
N MF
πkt (xt,N
(µN
t , π t ))(x)
j,k , µt )(u)Pk (xj,k , u, µt , ν

j=1 u∈U

The first term, J1 is defined as follows:
J1 ,

X X

k∈[K] x∈X

1
Npop

E

Nk
X
j=1

δ(xt+1,N
= x) −
j,k

31

Nk
X
j=1

t,N
N
N
Pk (xt,N
j,k , uj,k , µt , ν t )(x)

N
Recall from observation 2 that xt+1,N
’s are independent conditional on xN
t , ut . Also,
j,k
i
h 

t,N
N
N
N
N
= Pk (xt,N
E δ xt+1,N
=
x
x
,
u
t
t
j,k , uj,k , µt , ν t )(x), ∀x ∈ X
j,k
i

X h  t+1,N
N
=1
and
E δ xj,k
= x xN
,
u
t
t
x∈X

Applying Lemma 11, we can conclude that,




hp
X
X
p
p
p
p i
1 
1 
J1 ≤
Nk  |X | ≤
Nk 
|X | + |U |
Npop
Npop
k∈[K]

k∈[K]

The second term, J2 is defined as follows,
J2 ,

1

X X

Npop

k∈[K] x∈X

≤

1
Npop

E

Nk
X
j=1

Nk X
X X

k∈[K] j=1 x∈X

t,N
t,N t,N
N
N
N MF
Pk (xt,N
(µN
t , π t ))(x)
j,k , uj,k , µt , ν t )(x) − Pk (xj,k , uj,k , µt , ν

t,N
t,N t,N
N
N
N MF
E Pk (xt,N
(µN
t , π t ))(x)
j,k , uj,k , µt , ν t )(x) − Pk (xj,k , uj,k , µt , ν

(a)

MF
≤ LP ν N
(µN
t −ν
t , πt) 1




hp
X
X
(b) L
p
p
p
p i
L
P 
P 
≤
Nk  |U | ≤
Nk 
|X | + |U |
Npop
Npop
k∈[K]

k∈[K]

Relation (a) is a consequence of Assumption 1(d) and the inequality (b) follows from
Lemma 12. Finally,
J3 =

X X

E

k∈[K] x∈X

1
Npop
−

(a)

≤

1
Npop




X p

k∈[K]

Nk h
X

t,N
N MF
Pk (xt,N
(µN
t , π t ))(x)
j,k , uj,k , µt , ν

j=1

X

#

t,N
N
N MF
πkt (xt,N
(µN
t , π t ))(x)
j,k , µt )(u)Pk (xj,k , u, µt , ν

u∈U



Nk 

p

|X | ≤

1
Npop




X p

k∈[K]



Nk 

hp

|X | +

p

|U |

i

Inequality (a) is a result of Lemma 11 and the facts that {ut,N
j,k }j,k ’s are independent
N
conditioned on xt and
i
h
t,N
N MF
N
N
E Pk (xt,N
,
u
,
µ
,
ν
(µ
,
π
))(x)
x
t
t
t
t
j,k
j,k
X
t,N
N
N MF
(µN
=
πkt (xt,N
t , π t ))(x), ∀x ∈ X ,
j,k , µt )(u)Pk (xj,k , u, µt , ν
u∈U

X

x∈X

E

h

t,N
N
N MF
Pk (xt,N
(µN
t , π t ))(x) xt
j,k , uj,k , µt , ν

This concludes the result.
32

i

=1

K. Proof of Lemma 15
K.1 Proof of Proposition (a)
Following similar line of argument as used in the proof of Lemma 8, we obtain,
|ν̄ MF (µ̄, π̄) − ν̄ MF (µ̄′ , π̄)|1
X
=
|ν̄kMF (µ̄, π̄) − ν̄kMF (µ̄′ , π̄)|1
k∈[K]

X X X

=

k∈[K] u∈U x∈X

≤
≤

X XX

k∈[K] u∈U x∈X

X X

k∈[K] x∈X

µ̄(x, k)π̄k (x, µ̄)(u) −

X X

≤

X X

k∈[K] x∈X

X

x∈X

π̄k (x, µ̄)(u)

u∈U

µ̄′ (x, k)

k∈[K] x∈X
(a)

µ̄′ (x, k)π̄k (x, µ̄′ )(u)

µ̄(x, k)π̄k (x, µ̄)(u) − µ̄′ (x, k)π̄k (x, µ̄′ )(u)

|µ̄(x, k) − µ̄′ (x, k)|
+

X

X

u∈U

π̄k (x, µ̄)(u) − π̄k (x, µ̄′ )(u)

|µ̄(x, k) − µ̄′ (x, k)| + L̄Q |µ̄ − µ̄′ |1

X X

µ̄′ (x, k)

k∈[K] x∈X

(b)


= 1 + K L̄Q |µ̄ − µ̄′ |1

Inequality (a) follows from Assumption 4 and the fact that π̄k (x, µ̄) is a distribution
∀x ∈ X , ∀k ∈ [K]. Equality (b) uses the fact that µ̄′ (., k) is a distribution ∀k ∈ [K].
K.2 Proof of Proposition (b)
Note that,
X

θk |r̄kMF (µ̄, π̄) − r̄kMF (µ̄′ , π̄)|

k∈[K]

≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

|r̄k (x, u, µ̄, ν̄ MF (µ̄, π̄)) − r̄k (x, u, µ̄′ , ν̄ MF (µ̄′ , π̄))| × θk µ̄(x, k)π̄k (x, µ̄)(u)
|r̄k (x, u, µ̄′ , ν̄ MF (µ̄′ , π̄))| × θk |µ̄(x, k)π̄k (x, µ̄)(u) − µ̄′ (x, k)π̄k (x, µ̄′ )(u)|

Utilising Assumption 2(c), and the facts that θ, µ̄(·, k), πk (x, µ̄) are probability distributions ∀k ∈ [K], ∀x ∈ X , the first term can be upper bounded by the following expression,



L̄R |µ̄ − µ̄′ |1 + |ν̄ MF (µ̄, π̄) − ν̄ MF (µ̄′ , π̄)|1 ≤ L̄R 1 + (1 + K L̄Q ) |µ̄ − µ̄′ |1
33

Proposition (a) is used to derive the above inequality. Applying assumption 2(b), the
second term can be upper bounded by the following quantity.
X XX
θk |µ̄(x, k)π̄k (x, µ̄)(u) − µ̄′ (x, k)π̄k (x, µ̄′ )(u)|
M̄R
k∈[K] x∈X u∈U

≤ M̄R

X X

k∈[K] x∈X

θk |µ̄(x, k) − µ̄′ (x, k)|

+ M̄R

X X

k∈[K] x∈X
(a)

≤ M̄R

(b)

≤ M̄R

X X

k∈[K] x∈X

X X

k∈[K] x∈X

X

π̄k (x, µ̄)(u)

u∈U

θk µ̄′ (x, k)

X

u∈U

π̄k (x, µ̄)(u) − π̄k (x, µ̄′ )(u)

θk |µ̄(x, k) − µ̄′ (x, k)| + M̄R L̄Q |µ̄ − µ̄′ |1
|µ̄(x, k) − µ̄′ (x, k)| + M̄R L̄Q |µ̄ − µ̄′ |1

X X

θk µ̄′ (x, k)

k∈[K] x∈X

X

θk

k∈[K]

(c)

= M̄R (1 + L̄Q )|µ̄ − µ̄′ |1

Inequality (a) follows from Assumption 4 and the fact that π̄k (x, µ̄) is a distribution
∀x ∈ X , ∀k ∈ [K] while result (b) is derived from the fact that µ̄′ (·, k) is a distribution and
θk ≤ 1, ∀k ∈ [K]. Finally, equality (c) holds because θ is a distribution. This proves the
proposition.
K.3 Proof of Proposition (c)
Note that,
|P̄ MF (µ̄, π̄) − P̄ MF (µ̄′ , π̄)|1 =
≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

X

k∈[K]

|P̄kMF (µ̄, π̄) − P̄kMF (µ̄′ , π̄)|1

|P̄k (x, u, µ̄, ν̄ MF (µ̄, π̄)) − P̄k (x, u, µ̄′ , ν̄ MF (µ̄′ , π̄))|1 × µ̄(x, k)π̄k (x, µ̄)(u)
|P̄k (x, u, µ̄′ , ν̄ MF (µ̄′ , π̄))|1 × |µ̄(x, k)π̄k (x, µ̄)(u) − µ̄′ (x, k)π̄k (x, µ̄′ )(u)|

Using Assumption 2(d) and the facts that µ̄(·, k), π̄k (x, µ̄) are probability distributions
∀x ∈ X , ∀k ∈ [K], the first term can be upper bounded by the following expression,



K L̄P |µ̄ − µ̄′ |1 + |ν̄ MF (µ̄, π̄) − ν̄ MF (µ̄′ , π̄)|1 ≤ K L̄P 1 + (1 + K L̄Q ) |µ̄ − µ̄′ |1

Proposition (a) is applied to derive the above inequality. Note that, |P̄k (x, u, µ̄′ , ν̄(µ̄′ , π̄))|1 =
1. Therefore, the second term can be upper bounded by the following quantity.
X XX

k∈[K] x∈X u∈U

(a)

|µ̄(x, k)π̄k (x, µ̄)(u) − µ̄′ (x, k)π̄k (x, µ̄′ )(u)| ≤ (1 + K L̄Q )|µ̄ − µ̄′ |1

Inequality (a) can be established by following identical arguments as used in Proposition
(a). This concludes the result.
34

L. Proof of Lemma 16
L.1 Proof of Proposition (a)
Using the definition of L1 -norm, we get:
MF
E ν̄ N
(µ̄N
t − ν̄
t , π̄ t ) 1 =

X X

k∈[K] u∈U

MF
E ν̄ N
(µ̄N
t (u, k) − ν̄
t , π̄ t )(u, k)

Nk
Nk
X X 1
X
X
t,N
N
=
δ(uj,k = u) −
πkt (xt,N
E
j,k , µ̄t )(u)
Nk
j=1
j=1
k∈[K] u∈U


X 1
(a)
p
√  |U |
≤
Nk
k∈[K]

Inequality (a) follows from Lemma 11. This concludes the proposition.
L.2 Proof of Proposition (b)
Note that,
θk r̄kMF (µ̄N
t , π̄ t )


Nk X X
1 X
Nk
t,N
N
MF
t
r̄k (x, u, µ̄N
(µ̄N
=
t , ν̄
t , π̄ t )) × π̄k (x, µ̄t )(u)δ(xj,k = x)
Npop Nk
j=1 x∈X u∈U

=

1
Npop

Nk X
X
j=1 u∈U

t,N
N
N MF
t
r̄k (xt,N
(µ̄N
t , π̄ t )) × π̄k (xj,k , µ̄t )(u)
j,k , u, µ̄t , ν̄

We can upper bound the LHS of (45) by J1 + J2 where J1 is defined as follows.
Nk
X X
1
t,N t,N
N
N MF
r̄k (xt,N
, ut,N
, µ̄N
(µ̄N
J1 ,
E
t , ν̄ t ) − r̄k (xj,k , uj,k , µ̄t , ν̄
t , π̄ t ))
j,k
j,k
Npop
k∈[K] j=1

Npop

(a)

1

≤

Nk
X X

1

≤

k∈[K] j=1

Npop

(b)

≤ L̄R

t,N
t,N t,N
N
N
N MF
E r̄k (xt,N
(µ̄N
t , π̄ t ))
j,k , uj,k , µ̄t , ν̄ t ) − r̄k (xj,k , uj,k , µ̄t , ν̄

Nk
X X

k∈[K] j=1

 X

k∈[K]

MF
L̄R E ν̄ N
(µ̄N
t − ν̄
t , π̄ t ) 1

1 p
√
|U |
Nk
35

Inequality (a) follows from Assumption 2(c) while inequality (b) follows from Proposition
(a). The term J2 is defined as
J2 , E

Nk X
i
h
1 X X
t,N
t,N
N
N MF
N
t
(x
,
µ̄
)(u)
r̄k (xt,N
,
u,
µ̄
,
ν̄
(µ̄
,
π̄
))
×
δ(u
=
u)
−
π̄
t
t
t
t
k j,k
j,k
j,k
Npop
k∈[K] j=1 u∈U

≤

1
Npop

X

Nk
X X

E

k∈[K] j=1

u∈U

i
h
t,N
t,N
N
N MF
N
t
(x
,
µ̄
)(u)
r̄k (xt,N
,
u,
µ̄
,
ν̄
(µ̄
,
π̄
))
×
δ(u
=
u)
−
π̄
t
t
t
t
k j,k
j,k
j,k

Using similar argument as used in Lemma 13, we therefore get,


X
p
p
M̄R
1
√  |U |
J2 ≤ √
|U | ≤ M̄R 
Nk
N
k∈[K]

This concludes the result.

L.3 Proof of Proposition (c)
Note that the LHS of (46) can be upper bounded by the following quantity..
X X

MF
E µ̄N
µ̄N
t+1 (x, k) − P̄k
t , π̄ t (x)
k∈[K] x∈X

Nk
X
X X 1
δ(xt+1,N
= x)
E
=
j,k
Nk
j=1

k∈[K] x∈X

−

Nk X
X
j=1 u∈U

t,N
N
N MF
π̄kt (xt,N
(µ̄N
t , π̄ t ))(x) ≤ J1 + J2 + J3
j,k , µ̄t )(u)P̄k (xj,k , u, µ̄t , ν̄

The first term, J1 is defined as follows:
Nk
Nk
X X 1
X
X
t+1,N
t,N
N
N
J1 ,
δ(xj,k
= x) −
E
P̄k (xt,N
j,k , uj,k , µ̄t , ν̄ t )(x)
Nk
k∈[K] x∈X

j=1

j=1

Using similar argument as used in Lemma 14 to bound J1 , we get,




hp
X 1
X 1
p
p i
√  |X | ≤ 
√ 
J1 ≤ 
|X | + |U |
Nk
Nk
k∈[K]

k∈[K]

The second term, J2 is defined as follows,

Nk
Nk
X
X
X X 1
t,N t,N
N
N
N MF
E
P̄k (xj,k , uj,k , µ̄t , ν̄ t )(x) −
P̄k (xt,N
, ut,N
(µ̄N
J2 ,
t , π̄ t ))(x)
j,k
j,k , µ̄t , ν̄
Nk
k∈[K] x∈X

j=1

j=1

(a)

MF
≤ K L̄P ν̄ N
(µ̄N
t − ν̄
t , π̄ t ) 1




hp
X 1
X 1
(b)
p
p i
√  |U | ≤ K L̄P 
√ 
|X | + |U |
≤ K L̄P 
Nk
Nk
k∈[K]

k∈[K]

36

Relation (a) is a result of Assumption 2(d) and the inequality (b) follows from Proposition (a). Finally,

J3 =

X X

k∈[K] x∈X

"
Nk
1 X
t,N
N MF
P̄k (xt,N
(µ̄N
E
t , π̄ t ))(x)
j,k , uj,k , µ̄t , ν̄
Nk
j=1

−
(a)



≤

X

k∈[K]

X

#

t,N
N
N MF
πkt (xt,N
(µ̄N
t , π̄ t ))(x)
j,k , µ̄t )(u)P̄k (xj,k , u, µ̄t , ν̄

u∈U




hp
X 1
p i
1 p
√
√ 
|X | ≤ 
|X | + |U |
Nk
Nk
k∈[K]

Inequality (a) is a result of Lemma 11. This concludes the result.

M. Proof of Lemma 17
M.1 Proof of Proposition (a)
The following chain of inequalities hold true.
|ν MF (µ, π)[U ] − ν MF (µ′ , π)[U ]|1
=

X X

k∈[K] x∈X

≤
=

X X

k∈[K] x∈X

µ(x, k)πk (x, µ[X ]) −
µ(x, k)πk (x, µ[X ]) −

X X X

u∈U k∈[K] x∈X

≤

X X

k∈[K] x∈X

|µ(x, k) − µ (x, k)|
X X

k∈[K] x∈X
(a)

≤

X X

k∈[K] x∈X

µ′ (x, k)πk (x, µ′ [X ])

k∈[K] x∈X

X

1

µ′ (x, k)πk (x, µ′ [X ])

x∈X

µ(x, k)πk (x, µ[X ])(u) −
′

+

X X

X

1

X

= |ν(µ, π) − ν(µ′ , π)|1

µ′ (x, k)πk (x, µ′ [X ])(u)

x∈X

πk (x, µ[X ])(u)

u∈U

µ′ (x, k)

X

u∈U

|πk (x, µ[X ])(u) − πk (x, µ′ [X ])(u)|

|µ(x, k) − µ′ (x, k)| + LQ |µ[X ] − µ′ [X ]|1

X X

µ′ (x, k)

k∈[K] x∈X

(b)

= |µ − µ′ |1 + LQ |µ[X ] − µ′ [X ]|1

Result (a) follows from Lipschitz continuity of πkt and the fact that πk (x, µ) is a probability distribution. Finally, inequality (b) uses the fact that µ′ is a distribution. This
concludes the result.
37

M.2 Proof of Proposition (b)
Note that,
X
|rkMF (µ, π) − rkMF (µ′ , π)|
k∈[K]

≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

|rk (x, u, µ[X ], ν(µ, π)[U ]) − rk (x, u, µ′ [X ], ν(µ′ , π)[U ])| × µ(x, k)πk (x, µ[X ])(u)

|rk (x, u, µ′ [X ], ν(µ′ , π)[U ])| × |µ(x, k)πk (x, µ[X ])(u) − µ′ (x, k)πk (x, µ′ [X ])(u)|

Using the Lipschitz continuity of rk , and the facts that µ, πk (x, µ) are distributions,
the first term can be upper bounded by the following expression,

LR |µ[X ] − µ′ [X ]|1 + |ν MF (µ, π)[U ] − ν MF (µ′ , π)[U ]|1
≤ LR |µ − µ′ |1 + LR (1 + LQ )|µ[X ] − µ′ [X ]|1

Proposition (a) is used to derive the above inequality. Utilising similar logic as used in
Proposition (a), we can upper bound the second term by the following quantity:
MR |µ − µ′ |1 + MR LQ |µ[X ] − µ′ [X ]|1
M.3 Proof of Proposition (c)
The proof is similar to that of Proposition (b). Note that,
X
|P MF (µ, π) − P MF (µ′ , π)|1 =
|PkMF (µ, π) − PkMF (µ′ , π)|1
k∈[K]

≤
+

X XX

k∈[K] x∈X u∈U

X XX

k∈[K] x∈X u∈U

|Pk (x, u, µ[X ], ν MF (µ, π)[U ]) − Pk (x, u, µ′ [X ], ν MF (µ′ , π)[U ])|1 µ(x, k)πk (x, µ[X ])(u)

|Pk (x, u, µ′ [X ], ν MF (µ′ , π)[U ])|1 × |µ(x, k)πk (x, µ[X ])(u) − µ′ (x, k)πk (x, µ′ [X ])(u)|

Using the Lipschitz continuity of Pk and the facts that µ, πk (x, µ) are distributions, the
first term can be upper bounded by the following expression,


LP |µ[X ] − µ′ [X ]|1 + |ν MF (µ, π)[U ] − ν MF (µ′ , π)[U ]|1
≤ LP |µ − µ′ |1 + LP (1 + LQ )|µ[X ] − µ′ [X ]|1

Proposition (a) is used to derive the above inequality. Utilising similar logic as used in
Proposition (a), and the fact that |Pk (x, u, µ′ [X ], ν MF (µ′ , π)[U ])|1 = 1, ∀x ∈ X , ∀u ∈ U , we
can bound the second term by the following quantity:
|µ − µ′ |1 + LQ |µ[X ] − µ′ [X ]|1
This concludes the result.
38

N. Proof of Lemma 18
N.1 Proof of Proposition (a)
Using the definition of L1 -norm, we get:
MF
E νN
(µN
t [U ] − ν
t , π t )[U ] 1

=

X

E

u∈U

=

X

k∈[K]

1
Npop

X

u∈U

E

νN
t (u, k) −
Nk
X X

k∈[K] j=1

X

ν MF (µN
t , π t )(u, k)

k∈[K]

δ(ut,N
j,k = u) −

Nk
X X

N
πkt (xt,N
j,k , µt )(u)

k∈[K] j=1

Using Lemma 11, we conclude the proposition.
N.2 Proof of Proposition (b)
Using similar argument as used in Lemma 13, we can bound the LHS of (53) by J1 + J2
where
J1 ,

≤

1
Npop
1
Npop

E

Nk
X X

k∈[K] j=1
Nk
X X

k∈[K] j=1

t,N
t,N t,N
N
N
N
MF
rk (xt,N
(µN
t , π t )[U ])
j,k , uj,k , µt [X ], ν t [U ]) − rk (xj,k , uj,k , µt [X ], ν

t,N
t,N t,N
N
N
N
MF
E rk (xt,N
(µN
t , π t )[U ])
j,k , uj,k , µt [X ], ν t [U ]) − rk (xj,k , uj,k , µt [X ], ν

(a)

MF
≤ LR E ν N
(µN
t [U ] − ν
t , π t )[U ] 1
(b)
LR p
≤ p
|U |
Npop

Inequality (a) follows from the Lipschitz continuity of rk while (b) is a consequence of
proposition (a). The second term, J2 is as follows,

J2 ,

(a)

1
Npop

E

Nk X
X X

N
MF
rk (xt,N
(µN
t , π t )[U ])
j,k , u, µt [X ], ν

k∈[K] j=1 u∈U

MR p
|U |
≤ p
Npop

i
h
t,N
N
t
(x
,
µ
[X
])(u)
× δ(ut,N
=
u)
−
π
t
k j,k
j,k

Inequality (a) can be proved using Lemma 11.
39

N.3 Proof of Proposition (c)
Note that the LHS of (54) can be upper bounded as follows,
MF
E µN
(µN
t+1 [X ] − P
t , π t )[X ] 1

=

X

µN
t+1 (x, k) −

1 X
E
Npop

(

X

E

x∈X

=

k∈[K]

Nk
X X

k∈[K] j=1

x∈X

−
≤ J1 + J2 + J3

X

k∈[K]


PkMF µN
t , π t (x)

δ(xt+1,N
= x)
j,k

X

)

t,N
N
N
MF
πkt (xt,N
(µN
t , π t )[U ])(x)
j,k , µt [X ])(u)Pk (xj,k , u, µt [X ], ν

u∈U

The first term is defined as:
J1 ,

1
Npop

X

E

Nk
X X

k∈[K] j=1

x∈X

δ(xt+1,N
= x) −
j,k

Nk
X X

t,N
t,N
Pk (xt,N
[X ], ν t,N [U ])(x)
j,k , uj,k , µ

k∈[K] j=1

Applying Lemma 11, we can conclude that,
J1 ≤ p

hp
p i
1 p
1
|X | ≤ p
|X | + |U |
Npop
Npop

The second term, J2 is as follows,
J2 ,

≤

1
Npop

1
Npop

X

x∈X

E

Nk n
X X

t,N
N
N
P̃k (xt,N
j,k , uj,k , µt [X ], ν t [U ])(x)

k∈[K] j=1

Nk X
X X

o
t,N
N
MF
N
− P̃k (xt,N
,
u
,
µ
[X
],
ν
(µ
,
π
)[U
])(x)
t
t
t
j,k
j,k
t,N
N
N
E P̃k (xt,N
j,k , uj,k , µt [X ], ν t [U ])(x)

k∈[K] j=1 x∈X

t,N
N
MF
− P̃k (xt,N
(µN
t , π t )[U ])(x)
j,k , uj,k , µt [X ], ν
(a)

MF
≤ LP ν N
(µN
t [U ] − ν
t , π t )[U ] 1
(b)
LP p
|U |
≤ p
Npop
p i
L P hp
|X | + |U |
≤p
Npop

40

Inequality (a) is due to Lipschitz continuity of Pk and (b) follows from Proposition (a).
Finally,
"
Nk
X
1 X X
t,N
N
N
J3 =
E
Pk (xt,N
j,k , uj,k , µt [X ], ν(µt , π t )[U ])(x)−
Npop
x∈X
k∈[K] j=1
#
X
t,N
N
N
N
−
πkt (xt,N
j,k , µt [X ])(u)Pk (xj,k , u, µt [X ], ν(µt , π t )[U ])(x)
u∈U

Applying Lemma 11, we finally obtain, J3 ≤ √ 1

Npop

p
|X | ≤ √ 1

Npop

hp

|X | +

p

i
|U | .

O. Proof of Theorem 5
Note that the LHS of (25) can be upper bounded as,
J

∗
∗
(µ0 ) + vMF
(µ0 ) −
LHS ≤ sup v N (µ0 , πΦ ) − vMF
Φ∈Rd

1 X MF
v (µ0 , π Φj )
T
j=1

hp
p i 1 P
√
|X | + |U | Npop
Using Theorem 1, the first term can be bounded by C ′
k∈[K] Nk
√
for some constant C ′ . Using Lemma 4, the second term
ǫbias /(1 − γ) + ǫ
i by P
hp be bounded
p
√
1
|X | + |U | Npop
with a sample complexity O(ǫ−3 ). Choosing ǫ = C ′
k∈[K] Nk , we
obtain the result as in the statement of the theorem.

P. Loose Bounds
In this section, we shall demonstrate that one can derive loose bounds for multi-agent
systems satisfying Assumption 1, 3 using Theorem 2. Similarly, loose bounds for systems
satisfying Assumption 2 and 4 can be derived using Theorem 1.
P.1 Loose Bound Using Theorem 1
Consider a multi-agent system satisfying Assumptions 2 and 4. We shall use the notations
of Theorem 2. Let, θ , {θk }k∈[K] be prior probabilities of different classes. If r̄k ’s and P̄k ’s
are given reward and transition functions of the system, then one can define rk ’s and Pk ’s
such that, ∀x ∈ X , ∀u ∈ U , ∀µ̄ ∈ P K (X ), ∀ν̄ ∈ P K (U ) and ∀k ∈ [K],
r̄k (x, u, µ̄, ν̄) = rk (x, u, µ, ν),
P̄k (x, u, µ̄, ν̄) = Pk (x, u, µ, ν)
where µ, ν are uniquely defined as, µ , {θk µ̄(., k)}k∈[K] and ν , {θk ν̄(., k)}k∈[K] . Clearly,
µ ∈ Pθ (X × [K]) where Pθ (X × [K]) is the collection of distributions over X × [K] such
that the marginal distribution over [K] derived from each of its elements is θ. Similarly,
ν ∈ Pθ (U × [K]). Also, for every policy π̄ , {(π̄kt )k∈[K] }t∈{0,1,··· } , one can define π ,
{(πkt )k∈[K] }t∈{0,1,··· } such that, ∀x ∈ X , ∀µ̄ ∈ P K (X ) and ∀k ∈ [K],
π̄kt (x, µ̄) = πkt (x, µ)
41

Note that, the following inequality holds ∀µ, µ′ ∈ Pθ (X × [K]), ∀ν, ν ′ ∈ Pθ (U × [K]),
∀x ∈ X , ∀u ∈ U , and ∀k ∈ [K]
|rk (x, u, µ, ν) − rk (x, u, µ′ ,ν ′ )| = |r̄k (x, u, µ̄, ν̄) − r̄k (x, u, µ̄′ , ν̄ ′ )|


≤ L̄R |µ̄ − µ̄′ |1 + |ν̄ − ν̄ ′ |1
X


θk−1 |µ(., k) − µ′ (., k)|1 + |ν(., k) − ν ′ (., k)|1 (63)
= L̄R
k∈[K]



′
′
≤ L̄R θ −1
M |µ − µ |1 + |ν − ν |1

−1 ′
−1
−1
′
where we have, θ −1
M , max{θk }k∈[K] , µ̄ , {θk µ(., k)}k∈[K] , µ̄ , {θk µ (., k)}k∈[K] ,
ν̄ , {θk−1 ν(., k)}k∈[K] , and ν̄ ′ , {θk−1 ν ′ (., k)}k∈[K] . Similarly, ∀µ, µ′ ∈ Pθ (X ×[K]), ∀ν, ν ′ ∈
Pθ (U × [K]), ∀x ∈ X , ∀u ∈ U , ∀k ∈ [K], ∀t ∈ {0, 1, · · · },


′
′
|Pk (x, u, µ, ν) − Pk (x, u, µ′ , ν ′ )|1 ≤ L̄P θ −1
(64)
M |µ − µ |1 + |ν − ν |1
′
|πkt (x, µ) − πkt (x, µ′ )|1 ≤ L̄Q θ −1
M |µ − µ |1

(65)

Hence, the given system can equivalently be thought as a multi-agent system satisfying
−1
−1
Assumptions 1 and 3 with parameters M̄R , L̄R θ −1
M , L̄P θ M and L̄Q θ M . Using Theorem 1,
the approximation error bound for this translated system can be expressed as follows.
K
Theorem 21 Let xN
0 be the initial states and µ̄0 ∈ P (X ) their corresponding distribution.
If v̄ N denotes the empirical value function and v̄ MF is its mean-field limit, then for any
policy, π̄ ∈ Π̄, the following inequality holds


X
p
p
C̄
(θ)
1
R
MF

v̄ N (xN
(µ̄0 , π̄) ≤
|U |
Nk 
0 , π̄) − v̄
1−γ
Npop
k∈[K]




h

X p
p i 1
p
S̄R (θ)
1
1


+ C̄P (θ)
−
|X | + |U |
Nk ×
Npop
1 − γ S̄P (θ) 1 − γ
S̄P (θ) − 1
k∈[K]

(66)

whenever γ S̄P (θ) < 1 where the parameters are defined as follows,
C̄R (θ) , M̄R + L̄R θ −1
M
C̄P (θ) , 2 + L̄P θ −1
M
−1
−1
S̄R (θ) , M̄R (1 + L̄Q θ −1
M ) + L̄R θ M (2 + L̄Q θ M )
−1
−1
S̄P (θ) , (1 + L̄Q θ −1
M ) + L̄P θ M (2 + L̄Q θ M )

One can verify that the bound (66) is weaker than the bound provided by Theorem 2.
P.2 Loose Bound Using Theorem 2
Consider a multi-agent system satisfying Assumptions 1 and 3. We shall use the notations
of Theorem 1. Let, θ , {θk }k∈[K] be prior probabilities of different classes. If rk ’s and Pk ’s
42

are given reward and transition functions of the system, then one can define r̄k ’s and P̄k ’s
such that, ∀x ∈ X , ∀u ∈ U , ∀µ ∈ P(X × [K]), ∀ν ∈ P(U × [K]) and ∀k ∈ [K],
rk (x, u, µ, ν) = r̄k (x, u, µ̄, ν̄),
Pk (x, u, µ, ν) = P̄k (x, u, µ̄, ν̄)
where µ̄, ν̄ are uniquely defined as, µ̄ , {θk−1 µ(., k)}k∈[K] and ν̄ , {θk−1 ν(., k)}k∈[K] .
Clearly, µ̄ ∈ P K (X ), ν̄ ∈ P K (U ). Also, for every policy π , {(πkt )k∈[K] }t∈{0,1,··· } , one can
define π̄ , {(π̄kt )k∈[K]}t∈{0,1,··· } such that, ∀x ∈ X , ∀k ∈ [K], and ∀µ ∈ P(X × [K]),
πkt (x, µ) = π̄kt (x, µ̄)
Note that, the following inequality holds ∀µ̄, µ̄′ ∈ P K (X ), ∀ν̄, ν̄ ′ ∈ P K (U ), ∀x ∈ X ,
∀u ∈ U , and ∀k ∈ [K]
|r̄k (x, u, µ̄, ν̄) − r̄k (x, u, µ̄′ , ν̄ ′ )| = |rk (x, u, µ, ν) − rk (x, u, µ′ , ν ′ )|


≤ LR |µ − µ′ |1 + |ν − ν ′ |1
X


θk |µ̄(., k) − µ̄′ (., k)|1 + |ν̄(., k) − ν̄ ′ (., k)|1 (67)
= LR
k∈[K]

≤ LR |µ̄ − µ̄′ |1 + |ν̄ − ν̄ ′ |1




where µ , {θk µ̄(., k)}k∈[K] , µ′ , {θk µ̄′ (., k)}k∈[K] , ν , {θk ν̄(., k)}k∈[K] , ν ′ , {θk ν̄ ′ (., k)}k∈[K] .
Similarly, ∀µ̄, µ̄′ ∈ P K (X ), ∀ν̄, ν̄ ′ ∈ P K (U ), ∀x ∈ X , ∀u ∈ U , ∀k ∈ [K], ∀t ∈ {0, 1, · · · },


(68)
|P̄k (x, u, µ̄, ν̄) − P̄k (x, u, µ̄′ , ν̄ ′ )|1 ≤ LP |µ̄ − µ̄′ |1 + |ν̄ − ν̄ ′ |1
|π̄kt (x, µ̄) − π̄kt (x, µ̄′ )|1 ≤ LQ |µ̄ − µ̄′ |1

(69)

Hence, the given system can equivalently be thought as a multi-agent system satisfying Assumptions 2 and 4 with parameters MR , LR , LP and LQ . Using Theorem 2, the
approximation error bound for this translated system can be expressed as follows.
Theorem 22 If xN
0 be initial states and µ0 ∈ P(X × [K]) its resulting distribution, then
∀π ∈ Π,


X
p
CR
1
MF
√ 
v N (xN
(µ0 , π) ≤
|U | 
0 , π) − v
1−γ
Nk
k∈[K]


(70)
h



p
p i X 1
1
1
SR
√ ×
−
|X | + |U | 
+CP
SP − 1
1 − γSP
1−γ
Nk
k∈[K]

whenever γSP < 1 where v N (·, ·) denotes the empirical value function and v MF (·, ·) is its
mean-field limit. The other terms are given as follows: CR , MR + LR , CP , 2 + KLP ,
SR , MR (1 + LQ ) + LR (2 + KLQ ), and SP , (1 + KLQ ) + KLP (2 + KLQ ).
Clearly, the bound provided by (70) is weaker than the bound suggested in Theorem 1.
43

References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of
policy gradient methods: Optimality, approximation, and distribution shift. Journal of
Machine Learning Research, 22(98):1–76, 2021.
Mridul Agarwal, Vaneet Aggarwal, Arnob Ghosh, and Nilay Tiwari. Reinforcement learning
for mean-field game. Algorithms, 15(3):73, 2022.
Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed modelfree algorithm for ride-sharing using deep reinforcement learning. IEEE Transactions on
Intelligent Transportation Systems, 20(12):4714–4727, 2019.
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Laurière. Unified reinforcement qlearning for mean field game and control problems. arXiv preprint arXiv:2006.13912,
2020.
Alain Bensoussan, Tao Huang, and Mathieu Laurière. Mean field control and mean field
game models with several populations. Minimax Theory and its Applications, 3(2):173–
209, 2018.
René Carmona and François Delarue. Probabilistic Theory of Mean Field Games with Applications II: Mean Field Games with Common Noise and Master Equations, volume 84.
Springer, 2018.
René Carmona, Mathieu Laurière, and Zongjun Tan. Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods. arXiv preprint arXiv:1910.04295,
2019a.
René Carmona, Mathieu Laurière, and Zongjun Tan. Model-free mean-field reinforcement
learning: mean-field MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802,
2019b.
Yue Chen, Ana Bušić, and Sean P Meyn. State estimation for the individual and the
population in mean field control with application to demand dispatch. IEEE Transactions
on Automatic Control, 62(3):1138–1149, 2016.
Romuald Elie, Julien Perolat, Mathieu Laurière, Matthieu Geist, and Olivier Pietquin. On
the convergence of model free learning in mean field games. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pages 7143–7150, 2020.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-Field Controls with Q-learning
for Cooperative MARL: Convergence and Complexity Analysis. arXiv:2002.04131
[cs, math, stat], October 2020. URL http://arxiv.org/abs/2002.04131. arXiv:
2002.04131.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. Advances
in Neural Information Processing Systems, 32:4966–4976, 2019.
44

Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire.
Contextual decision processes with low bellman rank are pac-learnable. In International
Conference on Machine Learning, pages 1704–1713. PMLR, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory,
pages 2137–2143. PMLR, 2020.
Sham M Kakade. A natural policy gradient. Advances in neural information processing
systems, 14, 2001.
Daniel Lacker. Limit theory for controlled mckean–vlasov dynamics. SIAM Journal on
Control and Optimization, 55(3):1641–1672, 2017.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variancereduced) policy gradient and natural policy gradient methods. In NeurIPS, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for
deep reinforcement learning. In International conference on machine learning, pages
1928–1937. PMLR, 2016.
Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient Model-Based Multi-Agent
Mean-Field Reinforcement Learning. arXiv:2107.04050 [cs, stat], July 2021. URL
http://arxiv.org/abs/2107.04050. arXiv: 2107.04050.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.
John Wiley & Sons, 2014.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multiagent reinforcement learning. In International Conference on Machine Learning, pages
4295–4304. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems,
volume 37. Citeseer, 1994.
Howard M Schwartz. Multi-agent machine learning: A reinforcement approach. John Wiley
& Sons, 2014.
45

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran:
Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pages 5887–5896. PMLR, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al.
Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS’18), volume 3, pages 2085–2087, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In
Proceedings of the tenth international conference on machine learning, pages 330–337,
1993.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods:
Global optimality and rates of convergence. In International Conference on Learning
Representations, 2019.
Xiaoqiang Wang, Liangjun Ke, Zhimin Qiao, and Xinghua Chai. Large-scale traffic signal
control using a novel multiagent reinforcement learning. IEEE transactions on cybernetics, 51(1):174–187, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292,
1992.
Nicholas J Watkins, Cameron Nowzari, Victor M Preciado, and George J Pappas. Optimal resource allocation for competitive spreading processes on bilayer networks. IEEE
Transactions on Control of Network Systems, 5(1):298–307, 2016.
Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha. Learning deep
mean field games for modeling large population behavior. In International Conference
on Learning Representations (ICLR), 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A
selective overview of theories and algorithms. Handbook of Reinforcement Learning and
Control, pages 321–384, 2021.
Changxi Zhu, Ho-fung Leung, Shuyue Hu, and Yi Cai. A Q-values sharing framework for
multiple independent Q-learners. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems, pages 2324–2326, 2019.

46

