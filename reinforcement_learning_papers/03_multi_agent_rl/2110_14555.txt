arXiv:2110.14555v1 [cs.LG] 27 Oct 2021

V-Learning—A Simple, Efficient, Decentralized Algorithm
for Multiagent RL
Chi Jin
Princeton University
chij@princeton.edu

Qinghua Liu
Princeton University
qinghual@princeton.edu

Yuanhao Wang
Princeton University
yuanhao@princeton.edu

Tiancheng Yu
MIT
yutc@mit.edu

October 28, 2021

Abstract
A major challenge of multiagent reinforcement learning (MARL) is the curse of multiagents, where
the size of the joint action space scales exponentially with the number of agents. This remains to be a
bottleneck for designing efficient MARL algorithms even in a basic scenario with finitely many states
and actions. This paper resolves this challenge for the model of episodic Markov games. We design a
new class of fully decentralized algorithms—V-learning, which provably learns Nash equilibria (in the
two-player zero-sum setting), correlated equilibria and coarse correlated equilibria (in the multiplayer
general-sum setting) in a number of samples that only scales with maxi∈[m] Ai , where Ai is theQnumber
m
of actions for the ith player. This is in sharp contrast to the size of the joint action space which is i=1 Ai .
V-learning (in its basic form) is a new class of single-agent RL algorithms that convert any adversarial
bandit algorithm with suitable regret guarantees into a RL algorithm. Similar to the classical Q-learning
algorithm, it performs incremental updates to the value functions. Different from Q-learning, it only
maintains the estimates of V-values instead of Q-values. This key difference allows V-learning to achieve
the claimed guarantees in the MARL setting by simply letting all agents run V-learning independently.

1 Introduction
A wide range of modern artificial intelligence challenges can be cast as multi-agent reinforcement learning (MARL) problems, in which agents learn to make a sequence of decisions in the presence of other
agents whose decisions will influence the outcome and can adapt to the strategies of the agents. Modern
MARL systems have achieved significant success recently on a rich set of traditionally challenging tasks,
including the game of GO [44, 45], Poker [8], real-time strategy games [51, 34], decentralized controls or
multiagent robotics systems [6], autonomous driving [41], as well as complex social scenarios such as hideand-seek [4]. While single-agent RL has been the focus of recent intense theoretical study, MARL has been
comparatively underexplored, which leaves several fundamental questions open even in the basic model of
Markov games [42] with finitely many states and actions.

1

Table 1: A summary of sample complexities for V-learning under different settings. Here H is the length
of each episode, S is the number of states, A = maxi∈[m] Ai is the largest number of actions for each agent,
ǫ is the error tolerance. An information theoretical lower bound for all objectives is Ω(H 3 SA/ǫ2 ) [22, 2].
Multiplayer General-sum

Objective
Nash Equilibria

Two-player Zero-sum

-

Õ(H 5 SA/ǫ2 )

PPAD-complete

Coarse Correlated Equilibria

Õ(H 5 SA/ǫ2 )

Correlated Equilibria

Õ(H 5 SA2 /ǫ2 )

One such unique challenge of MARL is the curse of multiagents—let Ai be the number of actions for
the ith player, then the number
Qm of possible joint actions (as well as the number of parameters to specify a
Markov game) scales with i=1 Ai , which grows exponentially with the number of agents m. This remains
to be a bottleneck even for the best existing algorithms for learning Markov games. In fact, a majority of
these algorithms adapt the classical single-agent algorithms, such as value iteration or Q-learning,
into the
Q
multiagent setting [3, 30], whose sample complexity scales at least linearly with respect to m
A
.
This is
i
i=1
prohibitively large in practice even for fairly small multiagent applications, say only ten agents are involved
with ten actions available for each agent.
Another challenge of the MARL is to design decentralized algorithms. While a centralized algorithm
requires the existence of a centralized controller which gathers all information and jointly optimizes the policies of all agents, a decentralized algorithm allows each agent to only observe her own actions and rewards
while optimizing her own policy independently. Decentralized algorithms are often preferred over centralized algorithms in practice since (1) decentralized algorithms are typically cleaner, easier to implement; (2)
decentralized algorithms are more versatile as the individual learners are indifferent to the interaction and
the number of other agents; and (3) they are also faster due to less communication required. While several
provable decentralized MARL algorithms have been developed [see, e.g., 57, 40, 13], they either have only
asymptotic guarantees or work only under certain reachability assumptions (see Section 1.1). The existing provably efficient algorithms for general Markov games (without further assumptions) are exclusively
centralized algorithms [2, 55, 30].
This motivates us to ask the following open question:
Can we design decentralized MARL algorithms that break the curse of multiagents?
This paper addresses both challenges mentioned above, and provide the first positive answer to this
question in the basic setting of tabular episodic Markov games. We propose a new class of single-agent RL
algorithms—V-learning, which converts any adversarial bandit algorithm with suitable regret guarantees
into a RL algorithm. Similar to the classical Q-learning algorithm, V-learning also performs incremental
updates to the values. Different from Q-learning, V-learning only maintains the V-value functions instead
of theQQ-value functions. We remark that the number of parameters of Q-value functions in MARL is
O(S m
i=1 Ai ), where S is the number of states, while the number of parameters of V-value functions is
only O(S). This key difference allows V-learning to be readily extended to the MARL setting by simply
letting all agents run V-learning independently, which gives a fully decentralized algorithm.
We consider the standard learning objectives in the game theory—Nash equilibrium (NE), correlated
2

equilibrium (CE) and coarse correlated equilibrium (CCE). Except for the task of finding a NE in multiplayer
general-sum games which is PPAD-complete even for matrix games, we prove that V-learning finds a NE
for two-player zero-sum Markov games within Õ(H 5 SA/ǫ2 ) episodes, where H is the length of each
episode, A = maxi∈[m] Ai is the maximum number of actions of all agents, and ǫ is the error tolerance for
the objective. We further prove that for multiplayer general-sum Markov games, V-learning coupled with
suitable adversarial bandit algorithms is capable of finding a CCE within Õ(H 5 SA/ǫ2 ) episodes, and a CE
within Õ(H 5 SA2 /ǫ2 ) episodes. All sample complexities mentioned above do not grow with the number of
the agents, which thus break the curse of multiagents (See Table 1 for a summary).

1.1 Related work
In this section, we focus our attention on theoretical results for the tabular setting, where the numbers of
states and actions are finite. We acknowledge that there has been much recent work in RL for continuous
state spaces [see, e.g., 21, 23, 56, 24, 55, 25], but this setting is beyond our scope.
Markov games. Markov Game (MG), also known as stochastic game [42], is a popular model in multiagent RL [28]. Early works have mainly focused on finding Nash equilibria of MGs under strong assumptions, such as known transition and reward [29, 17, 15, 53], or certain reachability conditions [52, 54] (e.g.,
having access to simulators [20, 43, 58]) that alleviate the challenge in exploration.
A recent line of works provide non-asymptotic guarantees for learning two-player zero-sum tabular MGs
without further structural assumptions. Bai and Jin [2] and Xie et al. [55] develop the first provably-efficient
learning algorithms in MGs based on optimistic value iteration. Liu et al. [30] improves upon these works
and achieve best-known sample complexity for finding an ǫ-Nash equilibrium—O(H 3 SA1 A2 /ǫ2 ) episodes.
For multiplayer general-sum tabular MGs, Liu et al. [30] is the only existing work that provides nonasymptotic guarantees in the exploration setting. It proposes centralized model-based algorithms based on
value-iteration, and shows that
(although computationally inefficient), CCE and CE can be
Q Nash equilibria
2 ) episodes. Note this result suffers from the curse of multiagents.
all learned within O(H 4 S 2 m
A
/ǫ
j
j=1
V-learning—initially coupled with the FTRL algorithm as adversarial bandit subroutine—is firstly proposed in the conference version of this paper [3], for finding Nash equilibria in the two-player zero-sum
setting. During the preparation of this draft, we note two very recent independent works [47, 32], whose
results partially overlap with the results of this paper in the multiplayer general-sum setting. In particular,
Mao and Başar [32] use V-learning with stablized online mirror descent as adversarial bandit subroutine,
and learn ǫ-CCE in O(H 6 SA/ǫ2 ) episodes, where A = maxj∈[m] Aj . This is one H factor larger than
what is required in Theorem 6 of this paper. Song et al. [47] considers similar V-learning style algorithms
for learning both ǫ-CCE and ǫ-CE. For the latter objective, they require O(H 6 SA2 /ǫ2 ) episodes which is
again one H factor larger than what is required in Theorem 7 of this paper. Song et al. [47] also considers
Markov potential games, which is beyond the scope of this paper. We remark that both parallel works have
not presented V-learning as a generic class of algorithms which can be coupled with any adversarial bandit
algorithms with suitable regret guarantees in a black-box fashion.
Strategic games. Strategic game is one of the most basic game forms studied in the game theory literature
[37]. It can be viewed as Markov games without state and transition. The fully decentralized algorithm that
breaks the curse of multiagents is known in the setting of strategic games. By independently running noregret (or no-swap-regret) algorithm for all agents, one can find Nash Equilibria (in the two-player zero-sum
setting), correlated equilibria and coarse correlated equilibria (in the multiplayer general-sum setting) in a

3

number of samples that only scales with maxi∈[m] Ai [10, 16, 5]. However, such successes do not directly
extend to the Markov games due to the additional temporal structures involving both states and transition.
In particular, there is no computationally efficient no-regret algorithm for Markov games [38, 3].
Extensive-form games. There is another long line of research on MARL based on the model of extensiveform games (EFG) [see, e.g., 26, 14, 59, 7, 8, 9]. EFGs can be viewed as special cases of Markov games
where any state at the hth step can be reached from only one state at the (h − 1)th step (due to tree structure
of the game). Therefore, results on learning EFGs do not directly imply results for learning MGs.
Decentralized MARL There is a long line of empirical works on decentralized MARL [see, e.g., 31,
18, 49, 39, 46]. A majority of these works focus on the cooperative setting. They additionally attack the
challenge where each agent can only observe a part of the underlying state, which is beyond the scope of this
paper. For theoretical results, Zhang et al. [57] consider the cooperative setting while Sayin et al. [40] study
the two-player zero-sum Markov games. Both develop decentralized MARL algorithms but provide only
asymptotic guarantees. Daskalakis et al. [13] analyze the convergence rate of independent policy gradient
method in episodic two-player zero-sum MGs. Their result requires the additional reachability assumptions
(concentrability) which alleviates the difficulty of exploration.
Single-agent RL There is a rich literature on reinforcement learning in MDPs [see e.g. 19, 36, 1, 11, 48,
22, 24]. MDPs are special cases of Markov games, where only a single agent interacts with a stochastic environment. For the tabular episodic setting with nonstationary dynamics and no simulators, the best sample
complexity achieved by existing model-based and model-free algorithms are Õ(H 3 SA/ǫ2 ) (achieved by
value iteration [1]) and Õ(H 4 SA/ǫ2 ) (achieved by Q-learning [22]), respectively, where S is the number of
states, A is the number of actions, H is the length of each episode. Both of them (nearly) match the lower
bound Ω(H 3 SA/ǫ2 ) [19, 35, 22].

2 Preliminaries
We consider the model of Markov Games (MG) [42] (also known as stochastic games in the literature) in its
most generic—multiplayer general-sum form. Formally, we denote an tabular episodic MG with m players
m
by a tuple MG(H, S, {Ai }m
i=1 , P, {ri }i=1 ), where H, S denote the length of each episode and the state
space with |S| = S. Ai denotes the action space for the ith player and |Ai | = Ai . We let a := (a1 , · · · , am )
denote the (tuple of) joint actions by all m players, and A = A1 × . . . × Am . P = {Ph }h∈[H] is a collection
of transition matrices, so that Ph (·|s, a) gives the distribution of the next state if actions a are taken at state s
at step h, and ri = {ri,h }h∈[H] is a collection of reward functions for the ith player, so that ri,h (s, a) ∈ [0, 1]
gives the deterministic reward received by the ith player if actions a are taken at state s at step h. 1 We
remark that since the relation among the rewards of different agents can be arbitrary, this model of MGs
incorporates both cooperation and competition.
In each episode, we start with a fixed initial state s1 . 2 At each step h ∈ [H], each player i observes state
sh ∈ S, picks action ai,h ∈ Ai simultaneously, observes the actions played by other players,3 and receives
1

Our results directly generalize to random reward functions, since learning transitions is more difficult than learning rewards.
While we assume a fixed initial state for notational simplicity, our results readily extend to the setting where the initial state is
sampled from a fixed initial distribution.
3
We assume the knowledge of other players’ actions for the convenience of later definitions. We remark that the V-learning
algorithm introduced in this paper does not require knowing the actions of other players.
2

4

her own reward ri,h (sh , ah ). Then the environment transitions to the next state sh+1 ∼ Ph (·|sh , ah ). The
episode ends when sH+1 is reached.

Policy, value function A (random) policy πi of the ith player is a set of H maps πi := πi,h : Ω × (S ×
A)h−1 × S → Ai h∈[H] , where πi,h maps a random sample ω from a probability space Ω and a history
of length h—say τh := (s1 , a1 , · · · , sh ), to an action in Ai . To execute policy πi , we first draw a random
sample ω at the beginning of the episode. Then, at each step h, the ith player simply takes action πi,h (ω, τh ).
We note here ω is shared among all steps h ∈ [H]. ω encodes both the correlation among steps and the
individual randomness of each step. We further say a policy πi is deterministic if πi,h (ω, τh ) = πi,h (τh )
which is independent of the choice of ω.

An important subclass of policy is Markov policy, which can be defined as πi := πi,h : Ω × S →
Ai h∈[H] . Instead of depending on the entire history, a Markov policy takes actions only based on the
current state. Furthermore, the randomness in each step of Markov
policy is independent. Therefore, when

:=
it is clear from the context, we write Markov policy as πi
πi,h : S → ∆Ai h∈[H] , where ∆Ai denotes
the simplex over Ai . We also use notation πi,h (a|s) to denote the probability of the ith agent taking action a
at state s at step h.
A joint (potentially correlated) policy is a set of policies {πi }m
i=1 , where the same random sample ω is
shared among all agents, which we denote as π = π1 ⊙π2 ⊙. . .⊙πm . We also denote π−i = π1 ⊙. . . πi−1 ⊙
πi+1 ⊙ . . . ⊙ πm to be the joint policy excluding the ith player. A special case of joint policy is the product
policy where the random sample has special form ω = (ω1 , . . . , ωm ), and for any i ∈ [m], πi only uses the
randomness in ωi , which is independent of remaining {ωj }j6=i , which we denote as π = π1 × π2 × . . . × πm .
π (s ) as the expected cumulative reward that the ith player will receive
We define the value function Vi,1
1
if the game starts at initial state s1 at the 1st step and all players follow joint policy π:
hP
i
H
π (s ) := E
Vi,1
(1)
r
(s
,
a
)
s
1
π
1 .
h
h=1 i,h h
where the expectation is taken over the randomness in transition and the random sample ω in policy π.

Best response and strategy modification For any strategy π−i , the best response of the ith player is defined as a policy of the ith player which is independent of the randomness in π−i , and achieves the highest
value for herself conditioned on all other players deploying π−i . In symbol, the best response is the maxiπ ′ ×π
†,π
mizer of maxπi′ Vi,1i −i (s1 ) whose value we also denote as Vi,1 −i (s1 ) for simplicity. By its definition, we
know the best response can always be achieved at deterministic policies.
A strategy modification φi for the ith player is a set of maps φi := {φi,h : (S × A)h−1 × S × Ai → Ai },4
where φi,h can depend on the history τh and maps actions in Ai to different actions in Ai . For any policy of
the ith player πi , the modified policy (denoted as φi ⋄πi ) changes the action πi,h (ω, τh ) under random sample
ω and history τh to φi (τh , πi,h (ω, τh )). For any joint policy π, we define the best strategy modification of
(φ ⋄π )⊙π
the ith player as the maximizer of maxφi Vi,1 i i −i (s1 ).
Different from the best response, which is completely independent of the randomness in π−i , the best
strategy modification changes the policy of the ith player while still utilizing the shared randomness among
πi and π−i . Therefore, the best strategy modification is more powerful than the best response: formally one
π ′ ×π
(φ ⋄π )⊙π
can show that maxφi Vi,1 i i −i (s1 ) ≥ maxπi′ Vi,1i −i (s1 ) for any policy π.
4
Here, we only introduce the deterministic strategy modification for simplicity of notation, which is sufficient for discussion
in the context of this paper. The random strategy modification can also be defined by introducing randomness in φi which is
independent of randomness in πi and π−i . It can be shown that the best strategy modification can always be deterministic.

5

2.1 Learning objectives
A special case of Markov game is Markov Decision Process (MDP). One can show there always exists an
optimal policy π ⋆ = argmaxπ V1π (s1 ). Denote the value of the optimal policy as V ⋆ . The objective of
learning MDPs is to find an ǫ-optimal policy π, which satisfies V1⋆ (s1 ) − V1π (s1 ) ≤ ǫ.
For Markov games, there are three common learning objectives in the game theory literature—Nash
Equilibrium, Correlated Equilibrium (CE) and Coarse Correlated Equilibrium (CCE).
First, a Nash equilibrium is defined as a product policy where no player can increase her value by
changing only her own policy. Formally,
†,π

π )(s ) =
Definition 1 (Nash Equilibrium). A product policy π is a Nash equilibrium if maxi∈[m] (Vi,1 −i − Vi,1
1
†,π

π )(s ) ≤ ǫ.
0. A product policy π is an ǫ-approximate Nash equilibrium if maxi∈[m] (Vi,1 −i − Vi,1
1

We remark that, except for the special case of two-player zero-sum Markov games where reward r2,h =
−r1,h 5 for any h ∈ [H], the Nash equilibrium in general has been proved PPAD-hard to compute [12].
Therefore, we only present results for finding Nash equilibria in two-player zero-sum MGs in this paper.
Second, a coarse correlated equilibrium is defined as a joint (potentially correlated) policy where no
player can increase her value by playing a different independent strategy. In symbol,
†,π

π )(s ) =
Definition 2 (Coarse Correlated Equilibrium). A joint policy π is a CCE if maxi∈[m] (Vi,1 −i − Vi,1
1
†,π

π )(s ) ≤ ǫ.
0. A joint policy π is a ǫ-approximate CCE if maxi∈[m] (Vi,1 −i − Vi,1
1

The only difference between Definition 1 and Definition 2 is that Nash equilibrium requires the policy π
to be a product policy while CCE does not. Thus, it is clear that CCE is a relaxed notion of Nash equilibrium,
and a Nash equilibrium is always a CCE.
Finally, a correlated equilibrium is defined as a joint (potentially correlated) policy where no player can
increase her value by using a strategy modification. In symbol,
(φ ⋄πi )⊙π−i

Definition 3 (Correlated Equilibrium). A joint policy π is a CE if maxi∈[m] maxφi (Vi,1 i
(φ ⋄πi )⊙π−i

0. A joint policy π is a ǫ-approximate CE if maxi∈[m] maxφi (Vi,1 i

π )(s ) ≤ ǫ.
− Vi,1
1

π )(s ) =
− Vi,1
1

In Markov games, we also have that a Nash equilibrium is a CE, and a CE is a CCE (see Proposition 9
in Appendix A for more details).

3 V-Learning Algorithm
In this section, we introduce V-learning algorithm as a new class of single-agent RL algorithms, which
converts any adversarial bandit algorithm with suitable regret guarantees into a RL algorithm. We also
present its theoretical guarantees for finding a nearly optimal policy in the single-agent setting.

3.1 Training algorithm
To begin with, we describe the V-learning algorithm (Algorithm 1). It maintains a value Vh (s), a counter
Nh (s), and a policy πh (·|s) for each state s and step h, and initializes them to be the max value, 0, and uniform distribution respectively. V-learning also instantiates S × H different adversarial bandit algorithms—
one for each (s, h) pair. At each step h in each episode k, the algorithm performs three major steps:
5

Technically, to ensure r2,h ∈ [0, 1], we choose r2,h = 1 − r1,h . We note that adding a constant to the reward function has no
effect on the equilibria, which is our learning objective.

6

Algorithm 1 V- LEARNING
1: Initialize: for any (s, a, h), Vh (s) ← H + 1 − h, Nh (s) ← 0, πh (a|s) ← 1/A.
2: for episode k = 1, . . . , K do

3:
4:
5:
6:
7:
8:
9:

receive s1 .
for step h = 1, . . . , H do
take action ah ∼ πh (·|sh ), observe reward rh and next state sh+1 .
t = Nh (sh ) ← Nh (sh ) + 1.
Ṽh (sh ) ← (1 − αt )Ṽh (sh ) + αt (rh + Vh+1 (sh+1 ) + βt ).
Vh (sh ) ← min{H + 1 − h, Ṽh (sh )}.
H−rh −Vh+1 (sh+1 )
) on (sh , h)th adversarial bandit.
πh (·|sh ) ← A DV BANDIT U PDATE (ah ,
H

Protocol 2 A DVERSARIAL BANDIT A LGORITHM
1: Initialize: for any b, θ1 (b) ← 1/B.
2: for step t = 1, . . . , T do
3:
adversary chooses loss ℓt .
4:
take action bt ∼ θt , observe noisy bandit-feedback ℓ̃t (bt ).
5:
θt+1 ← A DV BANDIT U PDATE (bt , ℓ̃t (bt )).
• Policy execution (Line 5-6): the algorithm takes action ah according to the maintained πh , then
observes the reward rh and the next state sh+1 , and increases the counter Nh (sh ) by 1.
• V -value update (Line 7-8): the algorithm performs incremental update to the value function:
Ṽh (sh ) ← (1 − αt )Ṽh (sh ) + αt (rh + Vh+1 (sh+1 ) + βt )

(2)

here αt is the learning rate, and βt is the bonus to promote optimism (and exploration). The choices
of both quantities will be specified later. Next, we simply update Vh as a truncated version of Ṽh .
H−r +Vh+1 (sh+1 )
H

h
• Policy update (Line 9): the algorithm feeds the action ah and its “loss”
th
(sh , h) adversarial bandit algorithm, and receives the updated policy πh (·|sh ).

to the

Throughout this paper, we will always use the following learning rate αt . We also define an auxiliary
sequence {αit }ti=1 based on the learning rate, which will be frequently used across the paper.
αt =

H +1
,
H +t

α0t =

t
Y

(1 − αj ),

j=1

αit = αi

t
Y

(1 − αj ).

(3)

j=i+1

We remark that our incremental update (2) bears significant similarity to Q-learning, and our choice of
learning rate is precisely the same as the choice in Q-learning [22]. However, a key difference is that the
V-learning algorithm maintains V-value functions instead of Q-value functions. This is crucial when extendingQ
V-learning to the multiplayer setting where the number of parameters of Q-value functions becomes
O(HS m
i=1 Ai ) while the number of parameters of V-value functions is only O(HS). Since V-learning
does not use action-value functions, it resorts to adversarial bandit algorithms to update its policy.

7

Algorithm 3 E XECUTING O UTPUT P OLICY π̂ OF V- LEARNING
1: sample k ← Uniform([K]).
2: for step h = 1, . . . , H do
3:
4:
5:

observe sh , and set t ← Nhk (sh ).
set k ← khi (sh ), where i ∈ [t] is sampled with probability αit .
take action ah ∼ πhk (·|sh ).

A DV BANDIT U PDATE subroutine: Consider a multi-arm bandit problem with adversarial loss, where
we denote the action set by B with |B| = B. At round t, the learner picks a strategy (distribution over
actions) θt ∈ ∆B , and the adversary chooses a loss vector ℓt ∈ [0, 1]B . Then the learner takes an action bt that is sampled from distribution θt , and receives a noisy bandit-feedback ℓ̃t (bt ) ∈ [0, 1] where
E[ℓ̃t (bt )|ℓt , bt ] = ℓt (bt ). Then, the adversarial bandit algorithm performs updates based on bt and ℓ̃t (bt ), and
outputs the strategy for next round θt+1 , which we abstract as θt+1 ← A DV BANDIT U PDATE (bt , ℓ̃t (bt ))
(see Protocol 2).

3.2 Output policy
We define the final output policy π̂ of V-learning by how to execute this policy (see Algorithm 3). Let
V k , N k , π k be the value, counter and policy maintained by V-learning algorithm at the beginning of episode
k. The output policy maintains a scalar k, which is initially uniformly sampled from [K]. At each step
i
h, after observing sh , π̂ plays a mixture of policy {πhk (·|sh )}ti=1 with corresponding probability {αit }ti=1
defined in (3). Here t = Nhk (sh ) is the number of times sh is visited at step h at the beginning of episode k,
and ki is short for khi (sh ) which is the index of the episode when sh is visited at step h for the ith time. After
that, π̂ sets k to be the index khi (sh ) whose policy is just played within the mixture, and continue the same
process for the next step. This mixture form of output policy π̂ is mainly due to the incremental updates
of V-learning. One can show that, if omitting the optimistic bonus, V1K (s1 ) computed in the V-learning
algorithm is a stochastic estimate of the value of policy π̂.
We remark that π̂ is not a Markov policy, but a general random policy (see Definition in Section 2),
which can be written as a set of maps {πh : Ω × S h → Ai }. The choice of action at each step h depends
on a joint randomness ω ∈ Ω which is shared among all steps, and the history of past states (s1 , . . . , sh ). In
Section 6, we will further introduce a simple monotone technique that allows V-learning to output a Markov
policy in both the single-agent and the two-player zero-sum setting.

3.3 Single-agent guarantees
We first state our requirement for the adversarial bandit algorithm used in V-learning, which is to have a
high probability weighted external regret guarantee as follows. The weights {αit }ti=1 are defined in (3).
Assumption 1. For any t ∈ N and any δ ∈ (0, 1), with probability at least 1 − δ, we have
max

θ∈∆B

t
X
i=1

αit [hθi , ℓi i − hθ, ℓi i] ≤ ξ(B, t, log(1/δ)).

(4)

P
We further assume the existence of an upper bound Ξ(B, t, log(1/δ)) ≥ tt′ =1 ξ(B, t′ , log(1/δ))where (i)
ξ(B, t, log(1/δ)) is non-decreasing in B for any t, δ; (ii) Ξ(B, t, log(1/δ)) is concave in t for any B, δ.
8

Assumption 1 can be satisfied by modifying many existing algorithms with unweighted external regret to the weighted setting. In particular, we prove that the Follow-the-Regularized-Leader
(FTRL) algop
rithm (Algorithm 5) satisfiespthe Assumption 1 with bounds ξ(B, t, log(1/δ)) ≤ O( HB log(B/δ)/t)
and Ξ(B, t, log(1/δ)) ≤ O( HBt log(B/δ)). The H factor comes into the bounds because our choice of
weights {αit } in (3) involves H. We refer readers to Appendix F for more details.
We are now ready to introduce the theoretical guarantees of V-learning for finding near-optimal policies
in the single-agent setting.
Theorem 4. Suppose subroutine A DV BANDIT U PDATE satisfies Assumption 1. For any δ ∈ (0, 1) and
K
K ∈ N, let ι = log(HSAK/δ).
p Choose learning rate αt according to (3) and bonus {βt }t=1 so that
P
t
i
3
H ι/t) for any t ∈ [K]. Then, with probability at least 1 − δ, after
i=1 αt βi = Θ(Hξ(A, t, ι) +
running Algorithm 1 for K episodes, we have the output policy π̂ by Algorithm 3 satisfies
p
V1⋆ (s1 ) − V1π̂ (s1 ) ≤ O((H 2 S/K) · Ξ(A, K/S, ι) + H 5 Sι/K).
5), we can choose
In particular,
p when instantiating subroutine A DV BANDIT U PDATE by FTRL (Algorithm
p
βt = c · H 3 Aι/t for some absolute constant c, where V1⋆ (s1 ) − V1π̂ (s1 ) ≤ O( H 5 SAι/K).

Theorem 4 characterizes how fast the suboptimality of π̂ decreases with respect to the total number of
episode K. In particular, to obtain an ǫ-optimal output policy π̂, we only need to use a number of episodes
K = Õ(H 5 SA/ǫ2 ). This is H 2 factor larger than the information-theoretic lower bound Ω(H 3 SA/ǫ2 ) in
this setting [22]. We remark that one extra H factor is due to the incremental update and the use of learning
rate in (3) which is exactly the same for Q-learning algorithm [22]. The other H factor can be potentially
improved by using refined first-order regret bound (counterparts of Bernstein concentration) in V-learning.
While V-learning seems to be no better than classical value iteration or Q-learning in the single-agent
setting, its true power starts to show up in the multiagent setting: Value iteration and Q-learning require
highly nontrivial efforts to adapt them to the multiagent setting, and by design they suffer from the curse
of multiagents [3, 30]. In the following sections, we will show that V-learning can be directly extended to
the multiagent setting by simply letting all agents run V-learning independently. Furthermore, V-learning
breaks the curse of multiagents.

4 Two-player Zero-sum Markov Games
In this section, we provide the sample efficiency guarantee for V-learning to find Nash equilibria in twoplayer zero-sum Markov games.

4.1 Finding Nash equilibria
In the two-player zero-sum setting, we have two agents whose rewards satisfy r1,h = −r2,h for any h ∈ [H].
Our algorithm is simply that both agents run V-learning (Algorithm 1) independently with learning rate αt
as specified in (3). Each player j will uses her own set of bonus {βj,t } that depends on the number of her
actions and will be specified later. To execute the output policy, both agents simply execute Algorithm 3
independently using their own intermediate policies computed by V-learning.
We have the following theorem for V-learning. For clean presentation, we denote A = maxj∈[2] Aj .
Theorem 5. Suppose subroutine A DV BANDIT U PDATE satisfies Assumption 1. For any δ ∈ (0, 1) and
th
K ∈ N, let ι = log(HSAK/δ). Choose learning rate αt according to (3) and bonus {βj,t }K
t=1 of the j
9

p
P
player so that ti=1 αit βj,i = Θ(Hξ(Aj , t, ι) + H 3 ι/t) for any t ∈ [K]. Then, with probability at least
1 − δ, after running Algorithm 1 for K episodes, let π̂1 , π̂2 be the output policies by Algorithm 3 for each
player, then we have the product policy π̂ = π̂1 × π̂2 satisfies
p
†,π̂
π̂
max[Vj,1 −j (s1 ) − Vj,1
(s1 )] ≤ O((H 2 S/K) · Ξ(A, K/S, ι) + H 5 Sι/K).
j∈[2]

p
When instantiating A DV BANDIT U PDATE by FTRL (Algorithm 5), we can choose βj,t = c · H 3 Aj ι/t
p
†,π̂
π̂ (s )] ≤ O( H 5 SAι/K).
for some absolute constant c, which leads to maxj∈[2] [Vj,1 −j (s1 ) − Vj,1
1

Theorem 5 claims that, to find an ǫ−approximate Nash equilibrium, we only need to use a number of
episodes K = Õ(H 5 SA/ǫ2 ), where A = maxj∈[2] Aj . In contrast, value iteration or Q-learning based
algorithms require at least Ω(H 3 SA1 A2 /ǫ2 ) episodes to find Nash equilibria [3, 30]. Furthermore, Vlearning is a fully decentralized algorithm. To our best knowledge, V-learning is the only algorithm up
to today that achieves sample complexity linear in A for finding Nash equilibrium in two-player zero-sum
Markov games.
We remark that V-learning only performs O(1) operations and calls subroutine A DV BANDIT U PDATE
once every time a new sample is observed. As long as the adversarial bandit algorithm used in V-learning is
computationally efficient (which is the case for FTRL), V-learning itself is also computationally efficient.

5 Multiplayer General-sum Markov Games
In multiplayer general-sum games, finding Nash equilibria is computationally hard in general (which is
technically PPAD-complete [12]). In this section, we focus on finding two commonly-used alternative
notions of equilibria in the game theory—coarse correlated equilibria (CCE), and correlated equilibria (CE).
Both are relaxed notions of Nash equilibira.

5.1 Finding coarse correlated equilibria
The algorithm for finding CCE is again running V-learning (Algorithm 1) independently for each agent j
with learning rate αt (as specified in (3)) and bonus {βj,t } (to be specified later). The major difference
from the case of finding Nash equilibria is that CCE and CE require the output policy to be joint correlated
policy. We achieve this correlation by feeding the same random seed to all agents at the very beginning
when they execute the output policy according to Algorithm 3. That is, while training can be done in
the fully decentralized fashion, we require one round of communication at the beginning of the execution
to broadcast the shared random seed. After that, each agent can simply execute her own output policy
independently. During the execution, since the states visited are shared among all agents, shared random
seed allows the same index i to be sampled across all agents in the Step 4 of Algorithm 3 at every step. We
denote this correlated joint output policy as π̂ = π̂1 ⊙ . . . ⊙ π̂m .
We remark that to specify a correlated policy in general, we need to specify the
for taking
Qprobability
m
all action combinations (a1 , . . . , am ) for each (s, h). This requires at least Ω(HS j=1 Aj ) space, which
grows exponentially with the number of agents m. The way V-learning specifies the joint policy only
requires agents to store their
own intermediate counters and policies computed during training. This only
Pm
takes a total of O(HSK( j=1 Aj )) space, which scales only linearly with the number of agents. Our
approach dramatically improve over the former approach in space complexity when the number of agents is
large.
We now present the guarantees for V-learning to learn a CCE as follows. Let A = maxj∈[m] Aj .
10

Theorem 6. Suppose subroutine A DV BANDIT U PDATE satisfies Assumption 1. For any δ ∈ (0, 1) and
K
th
K ∈ N, let ι =Plog(mHSAK/δ). Choose learning
p rate αt according to (3) and bonus {βj,t }t=1 of the j
t
i
3
player so that i=1 αt βj,i = Θ(Hξ(Aj , t, ι) + H ι/t) for any t ∈ [K]. Then, with probability at least
1 − δ, after all the players running Algorithm 1 for K episodes, let π̂j be the output policy by Algorithm 3
for the j th player, then we have the joint policy π̂ = π̂1 ⊙ . . . ⊙ π̂m satisfies
p
†,π̂
π̂
max [Vj,1 −j (s1 ) − Vj,1
(s1 )] ≤ O((H 2 S/K) · Ξ(A, K/S, ι) + H 5 Sι/K).
j∈[m]

p
When instantiating A DV BANDIT U PDATE by FTRL (Algorithm 5), we can choose βj,t = c · H 3 Aj ι/t
p
†,π̂
π̂ (s )] ≤ O( H 5 SAι/K).
for some absolute constant c, which leads to maxj∈[m] [Vj,1 −j (s1 ) − Vj,1
1
Theorem 6 claims that, to find an ǫ−approximate CCE, V-learning only needs to use a number of
episodes K = Õ(H 5 SA/ǫ2 ), where A = maxj∈[m] Aj . This is in sharp contrast to the prior results
for multiplayer
Markov games, which use value iteration based algorithms, and require at least
Q general-sum
2 ) episodes [30]. As a result, V-learning is the first algorithm that breaks the curse of
Ω(H 4 S 2 ( m
A
)/ǫ
i=1 i
multiagents for finding CCE in Markov games.

5.2 Finding correlated equilibria
The algorithm for finding CE is almost the same as the algorithm for finding CCE except that we now
require a different A DV BANDIT U PDATE subroutine, which has the following high probability weighted
swap regret guarantee.
Assumption 2. For any t ∈ N and any δ > 0, with probability at least 1 − δ, we have
max
ψ∈Ψ

t
X
i=1

αit [hθi , li i − hψ ⋄ θi , li i] ≤ ξsw (B, t, log(1/δ)).

(5)

Pt
′
We assume the existence of an upper bound Ξsw (B, t, log(1/δ)) ≥
t′ =1 ξsw (B, t , log(1/δ))where (i)
ξsw (B, t, log(1/δ)) is non-decreasing in B for any t, δ; (ii) Ξsw (B, t, log(1/δ)) is concave in t for any B, δ.
Here Ψ denotes the set {ψ : B → B} which consists of all maps from actions in B to actions in B.
Meanwhile, for any θ ∈ ∆B , the term ψ ⋄ θ ∈ ∆B denotes the distribution over actions where ψ ⋄ θ(b) =
P
′
b′ :ψ(b′ )=b θ(b ). We note that bounded swap regret is a stronger requirement compared to bounded external
regret as in (4), since by maximizing over a subset of functions in Ψ which map all actions in B to one single
action, we recover the external regret by (5).
Assumption 2 can be satisfied by modifying many existing algorithms with external regret to the swap
regret setting. In particular, we prove that the Follow-the-Regularized-Leader for swap regret
p (FTRL swap)
algorithm (Algorithm 6) satisfiesp
Assumption 2 with bounds ξsw (B, t, log(1/δ))
≤
O(B
H log(B/δ)/t)
√
and Ξsw (B, t, log(1/δ)) ≤ O(B Ht log(B/δ)). Both bounds have one extra B factor comparing to the
counterparts in external regret. We refer readers to Appendix G for more details.
We now present the guarantees for V-learning to learn a CCE as follows. Let A = maxj∈[m] Aj .
Theorem 7. Suppose subroutine A DV BANDIT U PDATE satisfies Assumption 2. For any δ ∈ (0, 1) and
th
K ∈ N, let ι =Plog(mHSAK/δ). Choose learning
rate αt according to (3) and bonus {βj,t }K
t=1 of the j
p
t
player so that i=1 αit βj,i = Θ(Hξsw (Aj , t, ι) + H 3 ι/t) for any t ∈ [K]. Then, with probability at least
11

1 − δ, after all the players running Algorithm 1 for K episodes, let π̂j be the output policy by algorithm 3
for the j th player, then we have the joint policy π̂ = π̂1 ⊙ . . . ⊙ π̂m satisfies
p
φ ⋄π̂
π̂
max max[Vj,1j (s1 ) − Vj,1
(s1 )] ≤ O((H 2 S/K) · Ξsw (A, K/S, ι) + SH 5 ι/K).
j∈[m] φj

p
When instantiating A DV BANDIT U PDATE by FTRL swap (Algorithm 6), we can choose βj,t = c·Aj H 3 ι/t
p
φ ⋄π̂
π̂ (s )] ≤ O(A H 5 Sι/K).
for some absolute constant c, which leads to maxj∈[m] maxφj [Vj,1j (s1 ) − Vj,1
1
Theorem 7 claims that, to find an ǫ−approximate CE, V-learning only needs to use a number of episodes
K = Õ(H 5 SA2 /ǫ2 ), where A = maxj∈[m] Aj . It has an extra A multiplicative factor comparing to
the sample complexity of finding CCE, since CE is a subset of CCE thus finding CE is expected to be
more difficult. Nevertheless, the sample complexity
presented here is far better than value iteration based
Qm
4
2
algorithm, which requires at least Ω(H S ( i=1 Ai )/ǫ2 ) episodes for finding CE [30]. V-learning is also
the first algorithm that breaks the curse of multiagents for finding CE in Markov games.

6 Monotonic V-Learning
In the previous sections, we present the V-learning algorithm whose output policy (Algorithm 3) is a nested
mixture of Markov polices. Storing such a output policy requires O(HSAj K) space for the j th player.
In Section 5, we argue this approach has a significant advantage over directly storing a general correlated
policy when the number of agents is large. Nevertheless, this space complexity can be undesirable when the
number of agents is small.
In this section, we introduce a simple monotonic techique to V-learning, which allows each agent to
output a Markov policy when finding Nash equilibria in the two-player zero-sum setting. Storing a Markov
policy only takes O(HSAj ) space for the j th player. A similar result for the single-agent setting can be
immediately obtained by setting the second player in the Markov game to be a dummy player with only a
single action to choose from.
Monotonic update Monotonic V-learning is almost the same as V-learning with only the Line 8 in Algorithm 1 changed to
Vh (sh ) ← min{H + 1 − h, Ṽh (sh ), Vh (sh )},
(6)
This step guarantees Vh (sh ) to monotonically decrease at each step. This is helpful because in two-player
zero-sum Markov games, all Nash equilibria share a unique value which we denote as V ⋆ . By design, we
can prove that the V-values maintained in V-learning are high probability upper bounds of V ⋆ (Lemma 18).
This monotonic update allows our V-value estimates to always get closer to V ⋆ after each update, which
improves the accuracy of our V-value estimates.
Markov output policy For an arbitrary fixed (s, h) ∈ S × [H], let t1 be the last episode when the value
V1,h (s) is updated (i.e., strictly decreases), and let t2 be the last episode when the value V2,h (s) is updated.
Then the output policy for this (s, h) has the following form.
π̃1,h (·|s) :=

t2
X

ki
αit2 π1,h
(·|s),

π̃2,h (·|s) :=

t1
X
i=1

i=1

12

i

k
αit1 π2,h
(·|s),

(7)

where ki denotes the index of episode when state s is visited at step h is visited for the ith time. Recall
k (·|s) is the policy maintained by the j th player at the beginning of the k th episode when she runs
that πj,h
V-learning. That is, the new output policy is simply the weighted average of policies computed in the
V-learning at each (s, h) pair. Clearly, the policies π̃1 and π̃2 defined by (7) are Markov policies.
We remark that although the execution of π̃1 and π̃2 can be fully decentralized, in (7) the computation
of π̃1,h (·|s) depends on t2 while the computation of π̃2,h (·|s) depends on t1 . That is, two players need to
communicate at the end the indexes of the most recent episodes when their V -values are updated. As a
result, monotonic V-learning is not fully decentralized.
Theorem 8. Monotonic V-learning with output policy π̃ = π̃1 × π̃2 as specified by (7) has the same theoretical guarantees as Theorem 5 with the same choices of hyperparamters.
Theorem 8 asserts that V-learning can be modified to output Markov policies when finding Nash equilibria of two-player zero-sum Markov games. As a special case, the same technique and results directly apply
to the single-agent setting.

7 Conclusion
In this paper, we develop the first decentralized algorithm that breaks the curse of multiagents for learning
general Markov games. Behind this new result is a new class of single-agent RL algorithms—V-learning,
which converts any adversarial bandit algorithm with suitable regret guarantees into a RL algorithm. A
remarkable advantage of V-learning is its effortless extension to the multiagent setting while having much
preferred theoretical guarantees over existing methods: by simply running V-learning independently for all
agents, we find Nash equilibria (for two-player zero-sum games), CCE, and CE in a number of samples
that
Q scales with only maxj∈[m] Aj , in contrast to existing algorithms whose number of samples scales with
j∈[m] Aj .

References
[1] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 263–272. JMLR. org, 2017.
[2] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. arXiv
preprint arXiv:2002.04017, 2020.
[3] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. In Advances
in Neural Information Processing Systems, 2020.
[4] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. In International Conference on Learning
Representations, 2020.
[5] Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning
Research, 8(6), 2007.
[6] Manuele Brambilla, Eliseo Ferrante, Mauro Birattari, and Marco Dorigo. Swarm robotics: a review
from the swarm engineering perspective. Swarm Intelligence, 7(1):1–41, 2013.
13

[7] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top
professionals. Science, 359(6374):418–424, 2018.
[8] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885–890, 2019.
[9] Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for
extensive-form correlated equilibrium. Advances in Neural Information Processing Systems, 33, 2020.
[10] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university press,
2006.
[11] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, pages
5713–5723, 2017.
[12] Constantinos Daskalakis. On the complexity of approximating a nash equilibrium. ACM Transactions
on Algorithms (TALG), 9(3):23, 2013.
[13] Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021.
[14] Andrew Gilpin and Tuomas Sandholm. Finding equilibria in large sequential games of imperfect
information. In Proceedings of the 7th ACM conference on Electronic commerce, pages 160–169,
2006.
[15] Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor. Journal of the ACM
(JACM), 60(1):1–16, 2013.
[16] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127–1150, 2000.
[17] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003.
[18] Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning, pages 2961–2970. PMLR, 2019.
[19] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.
[20] Zeyu Jia, Lin F Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019.
[21] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low bellman rank are pac-learnable. In International Conference on Machine
Learning, pages 1704–1713. PMLR, 2017.
[22] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient?
In Advances in Neural Information Processing Systems, pages 4868–4878, 2018.
14

[23] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning
with linear function approximation. In Conference on Learning Theory, pages 2137–2143, 2020.
[24] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. Advances in Neural Information Processing Systems, 2021.
[25] Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state
spaces. arXiv preprint arXiv:2106.03352, 2021.
[26] Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive
form. Games and economic behavior, 4(4):528–552, 1992.
[27] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. 2018.
[28] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine
learning proceedings 1994, pages 157–163. Elsevier, 1994.
[29] Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pages 322–
328, 2001.
[30] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, pages 7001–7010. PMLR,
2021.
[31] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic
for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
[32] Weichao Mao and Tamer Başar. Provably efficient reinforcement learning in decentralized general-sum
markov games. arXiv preprint arXiv:2110.05682, 2021.
[33] Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.
In Advances in Neural Information Processing Systems, pages 3168–3176, 2015.
[34] OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
[35] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
[36] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value
functions. arXiv preprint arXiv:1402.0635, 2014.
[37] Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT press, 1994.
[38] Goran Radanovic, Rati Devidze, David Parkes, and Adish Singla. Learning to collaborate in markov
decision processes. In International Conference on Machine Learning, pages 5261–5270, 2019.
[39] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pages 4295–4304. PMLR, 2018.
[40] Muhammed O Sayin, Kaiqing Zhang, David S Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized q-learning in zero-sum markov games. arXiv preprint arXiv:2106.02748, 2021.
15

[41] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
[42] Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095–
1100, 1953.
[43] Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial Intelligence and Statistics, pages 2992–3002. PMLR, 2020.
[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the
game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
[45] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017.
[46] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning, pages 5887–5896. PMLR, 2019.
[47] Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large
number of players sample-efficiently? arXiv preprint arXiv:2110.04184, 2021.
[48] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC modelfree reinforcement learning. In International Conference on Machine Learning, pages 881–888, 2006.
[49] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
[50] Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games.
In International Conference on Machine Learning, pages 10279–10288. PMLR, 2021.
[51] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
[52] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, pages 4987–4997, 2017.
[53] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in
constrained saddle-point optimization. arXiv e-prints, pages arXiv–2006, 2020.
[54] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. arXiv
preprint arXiv:2102.04540, 2021.

16

[55] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneousmove markov games using function approximation and correlated equilibrium. arXiv preprint
arXiv:2002.07066, 2020.
[56] Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pages
10978–10989. PMLR, 2020.
[57] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multiagent reinforcement learning with networked agents. In International Conference on Machine Learning, pages 5872–5881. PMLR, 2018.
[58] Kaiqing Zhang, Sham M Kakade, Tamer Başar, and Lin F Yang. Model-based multi-agent rl in zerosum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461, 2020.
[59] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems, 20:1729–
1736, 2007.

17

Algorithm 4 E XECUTING P OLICY π̂hk
1: for step h′ = h, h + 1, . . . , H do
2:
3:
4:

A

observe sh′ , and set t ← Nhk′ (sh′ ).
set k ← khi ′ (sh′ ), where i ∈ [t] is sampled with probability αit .
take action ah′ ∼ πhk′ (·|sh′ ).

Notations and Basic Lemmas

A.1 Notations
In this subsection, we introduce some notations that will be frequently used in appendixes. Recall that we
use V k , N k , π k to denote the value, counter and policy maintained by V-learning algorithm at the beginning
of the episode k.
We also introduce a new policy π̂hk for a single agent (defined by its execution in Algorithm 4), which
can be viewed as a part of the output policy in Algorithm 3. The definition of π̂hk is very similar to π̂ except
two differences: (1) π̂hk is a policy for step h, . . . , H while π̂ is a policy for step 1, . . . , H; (2) in π̂ the initial
value of k is sampled uniformly at random from [K] at the very beginning while in π̂hk the initial value of k
is given.
We remark that π̂hk is a non-Markov policy that does not depends on history before to the hth step. In
′
symbol, we can express this class of policy as πj := {πj,h′ : Ω × (S × A)h −h × S → Aj }H
h′ =h . We call
this class of policy the policy starting from the hth step, and denote it as Πh . Similar to Section 2, we can
also define joint policy π = π1 ⊙ . . . ⊙ πm and product policy π = π1 × . . . × πm for policies in Πh . We
can also define value Vhπ (s) for joint policy π ∈ Πh as
i
hP
H
π (s) := E
′
′
′
s
=
s
.
)
,
a
(s
Vi,h
r
′
π
h
h
h
h =h i,h
π ′ ×π−i

This allows us to define the corresponding best response of π−i as the maximizer of maxπi′ ∈Πh Vi,hi

(s).

†,π
We also denote this maximum value as Vi,h −i (s). We define the strategy modification for policies starting
′
from the hth step as φi := {φi,h′ : (S × A)h −h × S × Ai → Ai }H
h′ =h , and denote the set of such strategy

modification as Φh .
Finally, for simplicity of notation, we define two operators P and D as follows:
(
Ph [V ](s, a) = Es′ ∼Ph (·|s,a) [V (s′ )],
Dπ [Q](s) = Ea∼π(·|s)[Q(s, a)],

(8)

for any value function V , Q and any one-step Markov policy π.

A.2 Basic lemmas
We first present a proposition which clarify the relations among the three different kind of equilibria. In
particular, we show that, similar to strategic games, we also have Nash ⊂ CE ⊂ CCE in Markov games.
Proposition 9 (Nash ⊂ CE ⊂ CCE). In Markov games, any ǫ-approximate Nash equilibrium is an ǫapproximate CE, and any ǫ-approximate CE is an ǫ-approximate CCE.

18

Proof. We prove two claims separately.
For Nash ⊂ CE, let π = π1 × π2 × · · · πm be an ǫ-approximate Nash equilibrium, then
(φ ⋄πi )×π−i

max Vi,1 i
φi

π ′ ×π−i

(a)

Vi,1i
(s1 ) = max
′
πi

(b)

π
(s1 ) ≤ Vi,1
(s1 ) + ǫ,

Step (a) is because that π is a product policy, where the randomness of different agents are completely
independent. In this case, maximizing over strategy modification φi is equivalent to maximizing over a
new independent policy. Step (b) directly follows from π being an ǫ-approximate Nash equilibrium. By
definition, this proves that π is also an ǫ-approximate CE.
For CE ⊂ CCE, let π = π1 ⊙ π2 ⊙ · · · πm be an ǫ-approximate CE, then we have
π ′ ×π−i

Vi,1i
max
′
πi

(c)

(φ ⋄πi )⊙π−i

(s1 ) ≤ max Vi,1 i
φi

(d)

π
(s1 ) ≤ Vi,1
(s1 ) + ǫ,

Step (c) is because by definition of strategy modification φi := {φi,h : (S × A)h−1 × S × Ai → Ai }, we can
consider a subset of strategy modification φ′i := {φ′i,h : (S × A)h−1 × S → Ai } which modifies the policy
ignoring whatever the action πi takes. It is not hard to see that maxmizing over the strategy modification in
this subset is equivalent to maximizing over a new independent policy πi′ . Therefore, maximizing over all
strategy modification is greater or equal to maximizing over πi′ . Finally, step (d) follows from π being an
ǫ-approximate CE. By definition, this proves that π is also an ǫ-approximate CCE.
Next, we present some basic lemmas that will be used in the proofs of different theorems. We start by
introducing some useful properties of sequence {αit } defined in (3).
Lemma 10. ([22, Lemma 4.1],[50, Lemma 2]) The following properties hold for αit :
1. √1t ≤

Pt

i=1

P
αi
αi
2
√t ≤ √
and 1t ≤ ti=1 it ≤ 2t for every t ≥ 1.
t
i

Pt
2H
i 2
2. maxi∈[t] αit ≤ 2H
i=1 (αt ) ≤ t for every t ≥ 1.
t and
P∞ i
1
3.
t=i αt = 1 + H for every i ≥ 1.

Finally, we have the following lemma which express the Ṽ maintained in V-learning in the form of
weighted sum of earlier updates.
Lemma 11. Consider an arbitrary fixed (s, h, k) tuple. Let t = Nhk (s) denote the number of times s is
visited at step h at the beginning of episode k, and suppose s was previously visited at episodes k 1 , . . . , k t <
k at the h-th step. Then the two V-values Ṽ and V in Algorithm 1 satisfy the following equation:
k
(s) = α0t (H − h + 1) +
Ṽj,h

t
X
i=1

i
h
i
i
ki
(skh+1 ) + βj,i ,
αit rj,h (s, akh ) + Vj,h+1

j ∈ [m].

(9)

Proof. The proof follows directly from the update rule in Line 7 Algorithm 1. Note that α0t is equal to zero
for any t > 1 and equal to one for t = 0.

19

B Proofs for Computing CCE in General-sum MGs
In this section, we give complete proof of Theorem 6. To avoid repeatedly state the condition of Theorem 6
in each lemma, we will use
• Condition of the adversarial bandit sub-procudure (Assumption 1) and
p
Pt
th
i
• Set the bonus {βj,t }K
H 3 ι/t) for any
t=1 of the j player so that
i=1 αt βj,i = Θ(Hξ(Aj , t, ι) +
t ∈ [K].
throughout the whole section.
The following Lemma is a direct consequence of Assumption 1, which will play an important role in
our later analysis.
Lemma 12. Under Assumption 1, the following event is true with probability at least 1 − δ: for any
(s, h, k) ∈ S × [H] × [K], let t = Nhk (s) and suppose s was previously visited at episodes k1 , . . . , kt < k
at the h-th step, then for all j ∈ [m]
max
µ

t
X
i=1

αit Dµ×πki

−j,h



ki
rj,h + Ph Vj,h+1



(s) −

t
X
i=1



ki
(s) ≤ Hξ(Aj , t, ι),
αit Dπki rj,h + Ph Vj,h+1
h

where ι = log(mHSAK/δ).
Proof. By Assumption 1 and the adversarial bandit update step in Algorithm 1, we have that with probability
at least 1 − δ, for any (s, h, k) ∈ S × [H] × [K],
!
!
t
t
ki
ki
X
X
H
−
r
−
P
V
H
−
r
−
P
V
j,h
h
j,h
h
j,h+1
j,h+1
αit Dµ×πki
(s)−
(s) ≤ ξ(Aj , t, ι),
αit Dπki
max
µ
−j,h
H
H
h
i=1

i=1

which implies the desired result by simple algebraic transformation.
Then we show V is actually an optimistic estimation of the value function of player j’th best response
to the output policy.
Lemma 13 (Optimism). For any δ ∈ (0, 1], with probability at least 1 − δ, for any (s, h, k, j) ∈ S × [H] ×
†,π̂ k

−j,h
k (s) ≥ V
(s).
[K] × [m], Vj,h
j,h

Proof of Lemma 13. We prove by backward induction. The claim is trivially satisfied for h = H + 1.
†,π̂ k

−j,h
k (s) ≥ V
(s) because
Suppose it is true for h + 1, consider a fixed state s. It suffices to show Ṽj,h
j,h
k
k
k
Vj,h (s) = min{Ṽj,h (s), H − h + 1}. Let t = Nh (s) and suppose s was previously visited at episodes

20

k1 , . . . , kt < k at the h-th step. Then using Lemma 11,
k
(s) = α0t H +
Ṽj,h

t
X
i=1

t
(i) X

≥

αit Dπki
h

i=1

(ii)

≥ max
µ

(iii)

t
X

≥ max
µ

(iv)

≥ max
µ

h
i
i
i
ki
(skh+1 ) + βj,i
αit rj,h (s, akh ) + Vj,h+1


ki
rj,h + Ph Vj,h+1

αit Dµ×πki

i=1
t
X
i=1
t
X

−j,h

(s) +

t
X
i=1

αit βj,i − O

r

H 3ι
t

!
r

t


X
ki
αit βj,i − O
(s) +
rj,h + Ph Vj,h+1

αit Dµ×πki

−j,h

αit Dµ×πki

−j,h

i=1



i=1

H 3ι
t

!

− Hξ(Aj , t, ι)



ki
(s)
rj,h + Ph Vj,h+1



i

k
†,π̂−j,h+1
rj,h + Ph Vj,h+1



(v)

†,π̂ k

(s) ≥ Vj,h −j,h (s)

where (i) is by martingale concentration and Lemma 2, (ii) is by Lemma 12, (iii) is by the definition of
βj,i , and (iv) is by induction hypothesis.
k
is non-Markov policy, and
Finally, we remark that (v) is not directly from Bellman equation since π̂−j,h
the best reponse of a non-Markov policy is not necessary a Markov policy. We prove (v) as follows. Recalls
definitions for policies in Πh as in Appendix A, by the definition, we have
k
µ×π̂−j,h

†,π̂ k

Vj,h −j,h (s) = max Vj,h
µ∈Πh

(a)

= max max

µh µ(h+1):H

(b)

≤ max
µh

(c)

= max
µh

=max
µ

t
X

i=1
t
X

i=1
t
X
i=1

t
X
i=1

αit Ea∼µ ×πki
h

−j,h

αit Ea∼µ ×πki
h



αit Ea∼µ ×πki
h



−j,h

−j,h

αit Dµ×πki

−j,h





i



i



k
µ(h+1):H ,π̂−j,h+1
(s, a, s′ )
rj,h (s, a) + Es′ Vj,h+1

rj,h (s, a) + Es′ max

µ(h+1):H

k
µ(h+1):H ,π̂−j,h+1
(s, a, s′ )
Vj,h+1



i

k
†,π̂−j,h+1
(s′ )
rj,h (s, a) + Es′ Vj,h+1

i

k
†,π̂−j,h+1
rj,h + Ph Vj,h+1



(s)

π
where Vj,h+1
(s, a, s′ ) for policy π ∈ Πh is defined as:
i
hP
H
′ .
π
′
′
′
(s, a, s′ ) := Eπ
)
s
=
s,
a
=
a,
s
=
s
,
a
(s
r
Vi,h+1
h
h
h+1
h
h
h′ =h+1 h
i

k
k
Step (a) uses the relation between π̂−j,h
and {π̂−j,h+1
}i . Step (b) pushes max inside summation and expeci

k
tation. Step (c) is because the Markov nature of Markov game and that {π̂−j,h+1
}i are policies that does not
depend on history at step h, we know the maximization over µ(h+1):H is achieved at policies in Πh+1 . This
finishes the proof.

To proceed with the analysis, we need to introduce two pessimistic V-estimations V and V that are
defined similarly as Ṽ and V . Formally, let t = Nhk (s) denote the number of times s is˜ visited at step h
21

at the beginning of episode k, and suppose s was previously visited at episodes k1 , . . . , kt < k at the h-th
step. Then
t
i
h
X
i
i
i
k
(10)
αit rj,h (s, akh ) + V kj,h+1 (skh+1 ) − βj,i ,
(s) =
Vj,h
˜
i=1

k
V kj,h (s) = max{0, Vj,h
(s)},
(11)
˜
for any player j ∈ [m] and k ∈ [K]. We emphasize that V and V are defined only for the purpose of
˜ agent, nor do the agents need to maintain
analysis. Neither do they influence the decision made by each
these quantities when running V-learning.
π̂ k

Equipped with the lower estimations, we are ready to lower bound Vj,hh .
Lemma 14 (Pessimism). For any δ ∈ (0, 1], with probability at least 1 − δ, the following holds for any
π̂ k

(s, h, k, j) ∈ S × [H] × [K] × [m] and any player j, V kj,h (s) ≤ Vj,hh (s).

Proof of Lemma 14. We prove by backward induction. The claim is trivially satisfied for h = H + 1.
π̂ k

k (s) ≤ V h (s) because V k (s) =
Suppose it is true for h + 1, consider a fixed state s. It suffices to show Vj,h
j,h
j,h
˜
k (s), 0}. Let t = N k (s) and suppose s was previously visited at episodes k 1 , . . . , k t < k at the
max{Vj,h
h
˜ Then by Equation 10,
h-th step.

k
(s) =
Vj,h

˜

t
X

i=1
t
(i) X

h
i
i
i
i
αit rj,h (s, akh ) + V kj,h+1 (skh+1 ) − βj,i

t


X
i
rh + Ph V kj,h+1 (s) −
αit βj,i + O

≤

αit Dπki

i=1
t
(ii) X

≤



i
αit Dπki rh + Ph V kj,h+1 (s)

≤

αit Dπki

h

i=1
t
(iii) X

i=1

r

H 3ι
t

!

h

i=1

h



ki
π̂h
rh + Ph Vj,h+1 (s)

π̂ k

= Vj,hh (s)
where (i) is by martingale concentration, (ii) is by the definition of βj,i , and (iii) is by induction hypothesis.
To prove Theorem 6, it remains to bound the gap

PK

k
k
k=1 maxj (V1,j − V 1,j )(s1 ).

k := V k (sk ) − V k (sk ) ≥ 0. The non-negativity
Proof of Theorem 6. Consider player j, we define δj,h
j,h h
j,h h
k . Let
here is a simple consequence of the update rule and induction. We want to bound δhk := maxj δj,h

k
nkh = Nhk skh and suppose skh was previously visited at episodes k1 , . . . , k nh < k at the h-th step. Now by

22

k (sk ) and V k (sk ),
the update rule of Vj,h
j,h h
h
k
k
δj,h
=Vj,h
(skh ) − V kj,h (skh )
k

≤α0nk H +
h

nh
X

h

αink

h

i=1
k

=α0nk H +
h

nh
X
i=1

i

i

k
Vj,h+1
− V kj,h+1



i

i
skh+1 + 2βj,i

ki
αink δj,h+1
+ O(Hξ(Aj , nkh , ι) +
h

q

H 3 ι/nkh )

p
where in the last step we have used i=1 αit βj,i = Θ(Hξ(Aj , t, ι) + H 3 ι/t).
Now by taking maximum w.r.t. j on both sides and notice ξ(B, t, ι) is non-decreasing in B, we have
Pt

k

δhk ≤α0nk H +
h

nh
X
i=1

i

k
αink δh+1
+ O(Hξ(A, nkh , ι) +
h

q

H 3 ι/nkh ).

Summing the first two terms w.r.t. k,
K
X
k=1

k

nh
K X
X
k=1 i=1

i

α0nk H =
h
K
(i) X

k
≤
αink δh+1
h

K
X
k=1

′

k
δh+1

k ′ =1

n
o
HI nkh = 0 ≤ SH,
∞
X

′
nk (ii)

αi h ≤

′

i=nkh +1



1+

1
H

X
K

k
.
δh+1

k=1

where (i) is by changing the order of summation and (ii) is by Lemma 10. Putting them together,
K
X
k=1

δhk =

K
X
k=1

k

α0nk H +
h


≤HS + 1 +

nh
K X
X

k
+
αink δh+1

1
H

k
δh+1
+

k=1 i=1
X
K

i

h

k=1

K
X
k=1

K
X
k=1

O(Hξ(A, nkh , ι) +

O(Hξ(A, nkh , ι) +

q

q

H 3 ι/nkh )

H 3 ι/nkh )

Recursing this argument for h ∈ [H] gives
K
X
k=1

δ1k ≤ eSH 2 + e

H X
K
X
h=1 k=1

O(Hξ(A, nkh , ι) +

q

H 3 ι/nkh )

By pigeonhole argument,
K
X
k=1

K

(Hξ(A, nkh , ι) +

q

!
H 3ι
Hξ(A, n, ι) +
n
s
n=1


q
X
K
K
3
≤O (1)
HΞ(A, Nh (s) , ι) + H Nh (s)ι

H 3 ι/nkh ) =O (1)

h (s)
X NX

s

r



√
≤O HSΞ(A, K/S, ι) + H 3 SKι ,
23

where in the last step we have used concavity.
Finally take the sum w.r.t. h ∈ [H] we have
K
X
k=1



√
k
max[V1,j
− V k1,j ](s1 ) ≤ O H 2 SΞ(A, K/S, ι) + H 5 SKι ,
j

which implies
†,π̂

π̂
max [Vj,1 −j (s1 ) − Vj,1
(s1 )] ≤ O((H 2 S/K) · Ξ(A, K/S, ι) +

j∈[m]

C

p

H 5 Sι/K).

Proofs for Computing CE in General-sum MGs

In this section, we give complete proof of Theorem 7. To avoid repeatedly state the condition of Theorem 7
in each lemma, we will use
• Condition of the adversarial bandit sub-procudure (Assumption 2) and
p
Pt
th
i
• Set the bonus {βj,t }K
H 3 ι/t) for any
t=1 of the j player so that
i=1 αt βj,i = Θ(Hξsw (Aj , t, ι) +
t ∈ [K].
throughout the whole section.
We begin with a swap regret version of Lemma 12.
Lemma 15. The following event is true with probability at least 1 − δ: for any (s, h, k) ∈ S × [H] × [K],
let t = Nhk (s) and suppose s was previously visited at episodes k1 , . . . , kt < k at the h-th step, then for all
j ∈ [m]
max
φj

t
X
i=1

αit Dφ ⋄πki ×πki
j

j,h

−j,h

h

t


i
X
ki
ki
(s) ≤ Hξsw (Aj , t, ι),
αit Dπki rj,h + Ph Vj,h+1
(s) −
rj,h + Ph Vj,h+1
h

i=1

where ι = log(KHS/δ).
Proof. By Assumption 2 and the adversarial bandit update step in Algorithm 1, we have that with probability
at least 1 − δ, for any (s, h, k) ∈ S × [H] × [K],
!
!
t
t
ki
ki
X
X
H − rj,h + Ph Vj,h+1
H − rj,h + Ph Vj,h+1
i
i
αt Dφ ⋄πki ×πki
max
αt Dπki
(s) −
(s)
j
φj
H
H
−j,h
j,h
h
i=1

i=1

≤ξsw (Aj , t, ι).

We begin with proving V is actually an optimistic estimation of the value function under best response.
Lemma 16 (Optimism). For any δ ∈ (0, 1), with probability at least 1 − δ, the following holds for any
k )⊙π̂ k
(φj ⋄π̂j,h
−j,h

k (s) ≥ max V
(s, h, k, j) ∈ S × [H] × [K] × [m], Vj,h
φj j,h

24

(s).

Proof of Lemma 16. We prove by backward induction. The claim is trivially satisfied for h = H + 1.
(φj ⋄π̂ k )⊙π̂ k

−j,h
j,h
k (s) ≥ max V
(s)
Suppose it is true for h + 1, consider a fixed state s. It suffices to show Ṽj,h
φj j,h
k
k
k
because Vj,h (s) = min{Ṽj,h (s), H − h + 1}. Let t = Nh (s) and suppose s was previously visited at
episodes k1 , . . . , kt < k at the h-th step. Then using Lemma 11,

k
Ṽj,h
(s) = α0t (H − h + 1) +
t
(i) X

≥

i=1

(ii)

i=1

φj

(iii)

h

t
X

≥ max
φj

(iv)

≥ max
φj

r

!

αit D(φ ⋄πki )×πki

αit D(φ ⋄πki )×πki



ki
(s)
rh + Ph Vj,h+1

i=1
t
X
i=1

i=1

H 3ι
t

t


X
ki
αit βj,i − O
rh + Ph Vj,h+1 (s) +

i=1
t
X

j

j

j,h

−j,h

−j,h

j,h

i=1

i

αit D(φ ⋄πki )×πki
j

−j,h

j,h

k )⊙π̂ k
(φj ⋄π̂j,h
−j,h

≥ max Vj,h
φj

h
i
i
i
ki
αit rj,h (s, akh ) + Vj,h+1
(skh+1 ) + βj,i

t


X
ki
αit βj,i − O
(s) +
αit Dπki rj,h + Ph Vj,h+1

≥ max

(v)

t
X

i

k
k
)⊙π̂−j,h+1
(φ′j ⋄π̂j,h+1
V
rh + Ph max
j,h
φ′j

!

r

H 3ι
t

!

− Hξ(Aj , t, ι)

(s)

(s)

where (i) is by martingale concentration and Lemma 2, (ii) is by Lemma 15, (iii) is by the definition of
βj,i , and (iv) is by induction hypothesis. Finally, (v) follows from a similar reasoning as in the proof of
Lemma 13, which we omit here.
π̂ k

We still need to lower bound Vj,hh . To do this, we estimate V and V defined by Equation 10 and
Equation 11. These quantities are indeed the lower bounds we need.˜
Lemma 17 (Pessimism). For any δ ∈ (0, 1), with probability at least 1 − δ, the following holds for any
π̂ k

(s, h, k, j) ∈ S × [H] × [K] × [m], V kj,h (s) ≤ Vh h (s).

Proof of Lemma 17. We prove by backward induction. The claim is trivially satisfied for h = H + 1.
π̂ k

k (s) ≤ V h (s) because V k (s) =
Suppose it is true for h + 1, consider a fixed state s. It suffices to show Vj,h
j,h
j,h
˜
k (s), 0}. Let t = N k (s) and suppose s was previously visited
1 , . . . , k t < k at the
max{Vj,h
at
episodes
k
h
˜

25

h-th step. Then by equation (10),
t
h
i
X
i
i
i
k
αit rj,h (s, akh ) + V kj,h+1 (skh+1 ) − βj,i
(s) =
Vj,h
˜
i=1

t


X
i
rh + Ph V kj,h+1 (s) −
αit βj,i + O

t
(i) X

≤

αit Dπki

i=1
t
(ii) X

≤



i
αit Dπki rh + Ph V kj,h+1 (s)

≤

αit Dπki

h

i=1
t
(iii) X

i=1

r

H 3ι
t

!

h

h

i=1



ki
π̂h
rh + Ph Vj,h+1
(s)

π̂ k

= Vj,hh (s)
where (i) is by martingale concentration, (ii) is by the definition of βj,i , and (iii) is by induction hypothesis.
To prove Theorem 7, it remains to bound the gap

PK

k
k
k=1 maxj (V1,j − V 1,j )(s1 ).

k := V k (sk ) − V k (sk ) ≥ 0. The non-negativity
Proof of Theorem 7. Consider player j, we define δj,h
j,h h
j,h h
k . Let
here is a simple consequence of the update rule and induction. We want to bound δhk := maxj δj,h

k
nkh = Nhk skh and suppose skh was previously visited at episodes k1 , . . . , k nh < k at the h-th step. Now by
k (sk ) and V k (sk ),
the update rule of Vj,h
j,h h
h
k
k
δj,h
=Vj,h
(skh ) − V kj,h (skh )
k

≤α0nk H +
h

nh
X

αink

h

i=1
k

=α0nk H +
h

nh
X
i=1

h

i

i

k
Vj,h+1
− V kj,h+1

i
 i 
skh+1 + 2βj,i

ki
+ O(Hξsw (Aj , nkh , ι) +
αink δj,h+1
h

q

H 3 ι/nkh )

p
where in the last step we have used i=1 αit βj,i = Θ(Hξsw (Aj , t, ι) + H 3 ι/t).
Now by taking maximum w.r.t. j on both sides and notice ξsw (B, t, ι) is non-decreasing in B, we have
Pt
k

δhk ≤α0nk H +
h

nh
X
i=1

i

k
αink δh+1
+ O(Hξsw (A, nkh , ι) +
h

q

H 3 ι/nkh ).

Summing the first two terms w.r.t. k,
K
X
k=1

α0nk H =
h

K
X
k=1

n
o
HI nkh = 0 ≤ SH,

k

nh
K X
X

 K

K
∞
′
X
(i) X
1 X k
nkh (ii)
ki
i
k′
δh+1 .
αnk δh+1 ≤
δh+1
αi ≤ 1 +
h
H
′
k=1
k=1 i=1
k ′ =1
i=nk +1
h

26

where (i) is by changing the order of summation and (ii) is by Lemma 10. Putting them together,
K
X

δhk =

K
X
k=1

k=1

k

α0nk H +
h


nh
K X
X

i

k
αink δh+1
+

k=1 i=1
X
K

1
≤HS + 1 +
H

h

k
+
δh+1

k=1

K
X

k=1
K
X
k=1

O(Hξsw (A, nkh , ι) +

O(Hξsw (A, nkh , ι) +

q

q

H 3 ι/nkh )

H 3 ι/nkh )

Recursing this argument for h ∈ [H] gives
K
X
k=1

δ1k ≤ eSH 2 + e

H X
K
X
h=1 k=1

O(Hξsw (A, nkh , ι) +

q
H 3 ι/nkh )

By pigeonhole argument,
K
X

K

(Hξsw (A, nkh , ι) +

k=1

q

!
3ι
H
Hξsw (A, n, ι) +
H 3 ι/nkh ) =O (1)
n
s
n=1


q
X
K
K
3
≤O (1)
HΞsw (A, Nh (s) , ι) + H Nh (s)ι
h (s)
X NX

r

s



√
≤O HSΞsw (A, K/S, ι) + H 3 SKι ,

where in the last step we have used concavity.
Finally take the sum w.r.t. h ∈ [H] we have
K
X
k=1



√
k
max[V1,j
− V k1,j ](s1 ) ≤ O H 2 SΞsw (A, K/S, ι) + H 5 SKι ,
j

which implies
†,π̂

π̂
max [Vj,1 −j (s1 ) − Vj,1
(s1 )] ≤ O((H 2 S/K) · Ξsw (A, K/S, ι) +

j∈[m]

D

p

H 5 Sι/K).

Proofs for MDPs and Two-player Zero-sum MGs

In this section, we prove the main theorems for V-learning in the setting of single-agent (MDPs) and twoplayer zero-sum MGs.
Proof of Theorem 5. To begin with, we notice an equivalent definition of two-player zero-sum MGs is that
the reward function satisfies r1,h = 1 − r2,h for all h ∈ [H]. The reason we use this definition instead of the
common version r1,h = −r2,h is we want to make it consistent with our assumption that the reward function
takes value in [0, 1] for any player. Although this definition does not satisfy the zero-sum condition, its Nash
equilibria are the same as those of the zero-sum version because adding a constant to the reward function of
player 2 per step will not change the dynamics of the game.
27

In order to show π̂ = π̂1 × π̂2 is an approximate Nash policy, it suffices to control
π1 ,π̂2
π̂1 ,π2
max V1,1
(s1 ) − min V1,1
(s1 ).
π1

π2

Since r1,h = 1 − r2,h for all h ∈ [H], with probability at least 1 − δ
π1 ,π̂2
π̂1 ,π2
max V1,1
(s1 ) − min V1,1
(s1 )
π1
π2


π̂1 ,π2
π1 ,π̂2
= max V1,1 (s1 ) − H − max V2,1 (s1 )
π1
π2

 

π1 ,π̂2
π̂1 ,π2
π̂1 ⊙π̂2
π̂1 ⊙π̂2
= max V1,1 (s1 ) − V1,1
(s1 ) + max V2,1 (s1 ) − V2,1
(s1 )
π1
π2
p
≤O((H 2 S/K) · Ξ(A, K/S, ι) + H 5 Sι/K),

where the last inequality follows from Theorem 6. The reason we can use Theorem 6 here is the precondition
of Theorem 5 is a special case of the precondition of Theorem 6.
Proof of Theorem 4. Since MDPs is a subclass of two-player zero-sum MGs by simply choosing the action
set of the second player to be a singleton, it suffices to only prove Theorem 5, from which the single-agent
guarantee, Theorem 4 trivially follows.

E Proofs for Monotonic V-learning
In this section, we prove Theorem 8. The algorithm is V-learning with monotonic update, and the setting we
consider is two-player zero-sum Markov games. As before, we assume r1,h (s, a) = 1 − r2,h (s, a) for all
s, a, h. The reason for assuming r1,h (s, a) = 1 − r2,h (s, a) instead of r1,h (s, a) = −r2,h (s, a) can be found
in Appendix D.
For two player zero-sum MGs, we can define its minimax value function (Nash value function) by the
following Bellman equations

⋆
⋆

Vj,h (s) = maxπj,h minπ−j,h Dπj,h ×π−j,h [Qj,h ](s),
⋆
(12)
](s, a),
Q⋆j,h(s, a) = rj,h (s, a) + Ph [Vj,h+1

 ⋆
Vj,H+1 (s) = Q⋆j,H+1 (s, a) = 0.
Lemma 18 (Optimism of V-estimates). With probability at least 1 − δ, for any (s, h, k, j) ∈ S × [H] ×
[K] × [2],
†,π̃
k
k
⋆
Ṽj,h
(s) ≥ Vj,h
(s) ≥ Vj,h −j (s) ≥ Vj,h
(s),
(13)
⋆ is the minimax (Nash) value function defined above.
where Vj,h

k (s) ≥ V k (s) is straightforward by the update rule of V-learning, and
Proof of Lemma 18. Note that Ṽj,h
j,h
†,π̃

⋆ (s) directly follows from the definition of minimax value function. Therefore, we only
Vj,h −j (s) ≥ Vj,h
need to prove the second inequality. We do this by backward induction.
†,π̃−j
k
The claim is true for h = H + 1. Assume for any s and k, Vj,h+1
(s) ≥ Vj,h+1
(s). For a fixed
k
(s, h, k) ∈ S × [H] × [K], let t = Nh (s) and suppose s was previously visited in episode k1 , . . . , kt < k

28

at the h-th step. By Bellman equation,
†,π̃
Vj,h −j (s) ≤α0t (H − h + 1) + max
µ

≤α0t (H − h + 1) + max
µ

≤α0t (H − h + 1) +
≤α0t (H − h + 1) +

t
X

i=1
t
X
i=1

t
X

i=1
t
X

αit Dµ×πki




†,π̃−j
rj,h + Ph Vj,h+1
(s)

αit Dµ×πki




ki
(s)
rj,h + Ph Vj,h+1

−j,h

−j,h

i=1



ki
(s) + Hξ(Aj , t, ι)
αit Dπki rj,h + Ph Vj,h+1
h

h

i
αit rj,h (s, ah ) + Vj,h+1 (sh+1 ) + O
ki

ki

ki

r

2H 3 ι
t

!

+ Hξ(Aj , t, ι)

where the second inequality follows from our induction hypothesis and the monotonicity of V k , the third
inequality follows from Lemma 12, and the last one follows from martingale concentration as well as Lemma
k (s). Note that
10. By Lemma 11 and the precondition of Theorem 8, we know the RHS is no larger than Ṽj,h
V k can be equivalently defined as
t
k
(s), H − h + 1},
(s) = min{min Ṽj,h
Vj,h
t∈[k]

†,π̃

−j
k (s) ≥ V
we conclude Vj,h
(s) for any k ∈ [K].
j,h

Now we are ready to prove Theorem 8.
Proof of Theorem 8. By the monotonicity of V and Lemma 18
†,π̃2
π̃1 ×π2
V1,1
(s1 ) − min V1,1
(s1 )
π2


†,π̃2
†,π̃1
(s1 ) − H − V2,1
(s1 )
= V1,1

K
K
≤ V1,1
(s1 ) + V2,1
(s1 ) − H
K

1 X k
k
≤
V1,1 (s1 ) + V2,1
(s1 ) − H
K
k=1

K

1 X k
k
≤
Ṽ1,1 (s1 ) + Ṽ2,1
(s1 ) − H ,
K
k=1

where the first equality follows from the definition of two-player zero-sum game, i.e., r1,h = 1 − r2,h .
k (sk ) + Ṽ k (sk ) − (H − h + 1). The
Now we can mimic the proof of Theorem 6. Define δhk := Ṽ1,h
2,h h
h
non-negativity here follows from Lemma 18 as below
k
k
⋆
⋆
Ṽ1,h
(skh ) + Ṽ2,h
(skh ) − (H − h + 1) ≥ V1,h
(skh ) + V2,h
(skh ) − (H − h + 1) = (H − h + 1) − (H − h + 1) = 0.


k
Let nkh = Nhk skh and suppose skh was previously visited at episodes k1 , . . . , k nh < k at the h-th step. By
29

Algorithm 5 FTRL for Weighted External Regret (FTRL)
1: Initialize: for any b ∈ B, θ1 (b) ← 1/B.
2: for episode t = 1, . . . , K do
3:
4:
5:

Take action bt ∼ θt (·), and observe loss ˜lt (bt ).
ˆlt (b) ← ˜
lt (bt )I{bt = b}/(θP
t (b) + γt ) for all b ∈ B.
θt+1 (b) ∝ exp[−(ηt /wt ) · ti=1 wi ˆli (b)]

Lemma 11 and the fact that r1,h = 1 − r2,h for all h, we have
k
k
δhk =Ṽ1,h
(skh ) + Ṽ2,h
(skh ) − (H − h + 1)
k

=2α0nk H +
h

nh
X
i=1

αink

h

k

=2α0nk H +
h

nh
X
i=1

i
h i
 i 
k
ki
skh+1 − (H − h) + β1,i + β2,i
Ṽ1,h+1
− Ṽ2,h+1
i

k
+ O(Hξ(A, nkh , ι) +
αink δh+1
h

q

H 3 ι/nkh )

p
P
where in the last step we used ti=1 αit βj,i = Θ(Hξ(Aj , t, ι) + H 3 ι/t).
The remaining steps follow exactly the same as the proof of Theorem 6. As a result, we obtain
1
†,π̃2
π̃1 ×π̃2
V1,1
(s1 ) − min V1,1
(s1 ) ≤
π̃2
K

K 

X
k
k
Ṽ1,1
(s1 ) + Ṽ2,1
(s1 ) − H
k=1

H 2S
· Ξ(A, K/S, ι) +
K

≤O

r

H 5 Sι
K

!

,

which completes the proof.

F

Adversarial Bandit with Weighted External Regret

In this section, we present a Follow-the-Regularized-Leader (FTRL) style algorithm that achieves low
weighted (external) regret for the adversarial bandit problem. Although FTRL is a classial algorithm in the
adversarial bandit literature, we did not find a good reference of FTRL with changing step size, weighted
regret and high probability bound. For completeness of this work, we provide detailed derivations here.
We present the FTRL algorithm in Algorithm 5. In Corollary 19, we prove that FTRL satisfies the
Assumption 1 with good regret bounds. Recall that B is the number of actions, and our normalization
condition requires loss ˜lt ∈ [0, 1]B for any t.
q
−1
Qt
H log B
Corollary 19. By choosing hyperparameter wt = αt
and
η
=
γ
=
(1
−
α
)
t
t
i
i=2
Bt , FTRL
(Algorithm 5) satisfies Assumption 1 with
p
p
Ξ(B, t, log(1/δ)) = 20 HBt log(B/δ)
ξ(B, t, log(1/δ)) = 10 HB log(B/δ)/t,
To prove Corollary 19, we show a more general weighted regret guarantee which works for any set of
i t
weights {wi }∞
i=1 in addition to {αt }i=1 . In particular, a general weighted regret is defined as
R(t) = max
⋆
θ

t
X
i=1

30

wi hθi − θ ⋆ , li i

(14)

Theorem 20. For any t ≤ K, following Algorithm 5, if ηi ≤ 2γi and ηi is non-increasing for all i ≤ t, let
ι = log(B/δ) , then with probability 1 − 3δ, we have
v
u
t
t
t
X
u X
X
1
wt log B B
γi wi + t2ι
+
ηi wi + max wi ι + B
R(t) ≤
wi2 + max wi ι/γt .
i≤t
ηt
2
2 i≤t
i=1

i=1

i=1

We postpone the proof of theorem 20 to the end of this section. We first show how to obtain Corollary
19 from Theorem 20.
Proof of Corollary 19. The weights {wt }K
t=1 we choose satisfy a nice property: for any t we have
wi
αi
= jt .
wj
αt
We prove this for i ≤ j and the other case is similar. By definition,
j
wi
αi Y
=
(1 − αk ),
wj
αj
k=i+1

and

j
αi Y
(1 − αk ).
=
αj
αjt
k=i+1

αit

We can easily verify that the RHS are the same.
−1
Qt
P
into
Define R̃(t) := maxθ∈∆B ti=1 αit [hθi , ℓi i − hθ, ℓi i]. By plugging wt = αt
i=2 (1 − αi )
Theorem 20, and using the property above, we have the regret guarantee

R̃(t) ≤

αt log B B
+
ηt
2

By choosing ηt = γt =

q

t
X
i=1

1
ηi αit + αt ι + B
2

t
X
i=1

v
u
t
u X
2
i
γi α + t2ι
αi + αt ι/γt .
t

t

i=1

H log B
and using Lemma 10, we can further upper bound the regret by
Bt

s
t
X
3p
Bt
(H + 1) log B
αi
√t
+
R̃(t) ≤
HB log B
H +t
H log B 2
t
i=1
v
s
u
t
X
2 (H + 1) ι
(H + 1) ι u
Bt
+
+ t2ι
αit +
2 (H + t)
(H + t)
H log B
i=1
r
r
r
r
HB log B
HB log B Hι
Hι
HB log B
≤2
+3
+
+2
+2
t
t
t
t
t
p
≤9 HBι/t + Hι/t.

To further simplify the above upper bound, consider two cases:
p
p
• If Hι/t ≤ 1, Hι/t ≥ Hι/t and thus R̃(t) ≤ 10 HBι/t.
31

p
• If Hι/t ≥ 1, p HBι/t ≥ 1 ≥ R̃(t) where the last step is by the definition of R̃(t). Therefore we
have R̃(t) ≤ HBι/t.
p
Combining the two cases above gives R̃(t) ≤
p 10 HBι/t.
P
which is non-decreasing in B. Since tt′ =1 ξ(B, t, log(1/δ)) ≤
√
√Finally, we pick ξ(B, t, log(1/δ)) := 10 HBι/t,
20 HBtι, we choose Ξ(B, t, log(1/δ)) = 20 HBtι, which is concave in t.
To prove Theorem 20, we first note that the weighted regret (14) can be decomposed into three terms
t
X
i=1

⋆

wi hθi − θ , li i =
=

t
X
i=1

t
X
i=1

|

wi hθi − θ ⋆ , li i

t
t
E X
E
D
D
E X
D
wi θi , li − ˆli +
wi θ ⋆ , ˆli − li
wi θi − θ ⋆ , ˆli +

{z

i=1

|

}

(A)

{z

}

(B)

i=1

|

{z

(15)

}

(C)

The rest of this section is devoted to bounding three terms above. We begin with the following useful
lemma adapted from Lemma 1 in [33], which is crucial in achieving high probability guarantees.
Lemma 21. For any sequence of coefficients c1 , c2 , . . . , ct s.t. ci ∈ [0, 2γi ]B is Fi -measurable, we have
with probability 1 − δ,
t
E
D
X
wi ci , ˆli − li ≤ max wi ι.
i≤t

i=1

Proof. Define w = maxi≤t wi . By definition,
wi ˆli (b) =

wi ˜li (b) I {bi = b}
wi ˜li (b) I {bi = b}
≤
θi (b) + γi
θ (b) + wi l̃i (b)I{bi =b} γ
i

w

i

2γi wi l̃i (b)I{bi =b}
(i) w
2γi wi ˜li (b) I {bi = b}
w
wθi (b)
log 1 +
≤
=
2γi 1 + γi wi l̃i (b)I{bi =b}
2γi
wθi (b)
wθi (b)

!

z
where (i) follows from 1+z/2
≤ log (1 + z) for all z ≥ 0.
Defining the sum
wi
wi D ˆ E
ci , li , Si =
hci , li i ,
Ŝi =
w
w
we have
!!#
"
h
 i
X ci (b)
2γi wi ˜li (b) I {bi = b}
log 1 +
Ei exp Ŝi ≤ Ei exp
2γi
wθi (b)
b
!#
"
Y
(i)
ci (b) wi ˜li (b) I {bi = b}
≤ Ei
1+
wθi (b)
b
"
#
X ci (b) wi ˜li (b) I {bi = b}
= Ei 1 +
wθi (b)
b

= 1 + Si ≤ exp (Si )

32

where (i) follows from z1 log (1 + z2 ) ≤ log (1 + z1 z2 ) for any 0 ≤ z1 ≥ 1 and z2 ≥ −1. Note that here
we are using the condition ci (b) ≤ 2γi for all b ∈ [B].
Equipped with the above bound, we can now prove the concentration result.
#
#
#
"
" t
" t


X
X
B
P
Ŝi − Si ≥
Ŝi − Si ≥ ι = P exp
δ
i=1
i=1
##
"
" t

X
δ
≤ Et exp
Ŝi − Si
B
i=1
#
#
"
" t−1
h

i

X
δ
≤ Et−1 exp
Ŝi − Si Et exp Ŝt − St
B
i=1
##
"
" t−1

X
δ
Ŝi − Si
≤ Et−1 exp
B
i=1

δ
.
B
We conclude the proof by taking a union bound.
≤ ··· ≤

With Lemma 21, we can bound the three terms (A),(B) and (C) in (15) separately as below.
Lemma 22. For any t ∈ [K], suppose ηi ≤ 2γi for all i ≤ t. Then with probability at least 1 − δ, for any
θ ⋆ ∈ ∆B ,
t
t
E w log B B X
D
X
1
t
⋆ ˆ
ηi wi + max wi ι.
+
wi θi − θ , li ≤
ηt
2
2 i≤t
i=1

i=1

Proof. We use the standard analysis of FTRL with changing step size, see for example Exercise 28.13 in
[27]. Notice the essential step size is ηt /wt ,
t
X
i=1

t
E
D
E w log B 1 X
D
t
ηi wi θi , ˆli2
+
wi θi − θ ⋆ , ˆli ≤
ηt
2

≤

wt log B 1
+
ηt
2

(i) w log B
t

≤

≤

ηt

+

1
2

i=1
t X
X

i=1 b∈B
t X
X

wt log B B
+
ηt
2

i=1 b∈B
t
X

ηi wi ˆli (b)
ηi wi li (b) +

ηi wi +

i=1

1
max wi ι
2 i≤t

1
max wi ι
2 i≤t

where (i) is by using Lemma 21 with ci (b) = ηi for any b. The any-time guarantee is justifed by taking
union bound.
Lemma 23. For any t ∈ [K], with probability 1 − δ,
t
X
i=1

v
u
t
t
E
D
X
u X
γi wi + t2ι
wi θi , li − ˆli ≤ B
wi2 .
i=1

33

i=1

Proof. We further decopose it into
t
X
i=1

t
t
E
D
E X
D
E X
D
wi θi , Ei ˆli − ˆli .
wi θi , li − Ei ˆli +
wi θi , li − ˆli =
i=1

i=1

The first term is bounded by
t
X
i=1


t
E X
D
wi θi , li −
wi θi , li − Ei ˆli =


θi
li
θi + γi
i=1


t
t
X
X
γi
li ≤ B
wi θi ,
γi wi .
=
θ i + γi
i=1

i=1

To bound the second term, notice
E X
D
I {bt = b} X
≤
I {bi = b} = 1,
θi , ˆli ≤
θi (b)
θi (b) + γi
b∈B

b∈B

D

E

thus {wi θi , Ei ˆli − ˆli }ti=1 is a bounded martingale difference sequence w.r.t. the filtration {Fi }ti=1 . By
Azuma-Hoeffding,
v
t D
t
E u
u X
X
t
ˆ
ˆ
θi , Ei li − li ≤ 2ι
wi2 .
i=1

i=1

Lemma 24. For any t ∈ [K], with probability 1 − δ, for any θ ⋆ ∈ ∆B , if γi is non-increasing in i,
t
X
i=1

E
D
wi θ ⋆ , ˆli − li ≤ max wi ι/γt .
i≤t

B
Proof. Define a basis {ej }B
j=1 of R by

(
1 if a = j
ej (b) =
0 otherwise
Then for all the j ∈ [B], we can apply Lemma 21 with ci = γt ej . Sincee ci (b) ≤ γt ≤ γi , the condition
in Lemma 21 is satisfied. As a result,
t
X
i=1

E
D
wi ej , ˆli − li ≤ max wi ι/γt .
i≤t

Since any θ ⋆ is a convex combination of {ej }B
j=1 , by taking the union bound over j ∈ [B], we have
t
X
i=1

E
D
wi θ ⋆ , ˆli − li ≤ max wi ι/γt .
i≤t

34

Algorithm 6 FTRL for Weighted Swap Regret (FTRL swap)
1: Initialize: for any b ∈ B, θ1 (b) ← 1/B.
2: for episode t = 1, . . . , K do
3:
4:
5:
6:
7:

Take action bt ∼ θt (·), and observe loss ˜lt (bt ).
for each action b ∈ B do
ˆlt (·|b) ← θt (b)˜lt (bt )I{bt = ·}/(θt (·) + γt ).
P
θ̃t+1 (·|b) ∝ exp[−(ηt /wt )P
· ti=1 wi ˆli (·|b)]
Set θt+1 such that θt+1 (·) = a θt+1 (b)θ̃t+1 (·|b).

Finally we are ready to prove Theorem 20.
Proof of Theorem 20. Note the conditions in Lemma 22 and Lemma 24 are satisfied by assumptions. Recall
the regret decomposition (15). By bounding (A) in Lemma 22, (B) in Lemma 23 and (C) in Lemma 24,
with probability 1 − 3δ, we have that
v
u
t
t
t
X
u X
X
1
wt log B B
t
wi2 + max wi ι/γt .
γi wi + 2ι
ηi wi + max wi ι + B
+
R(t) ≤
i≤t
ηt
2
2 i≤t
i=1

i=1

G

i=1

Adversarial Bandit with Weighted Swap Regret

In this section, we adapt Follow-the-Regularized-Leader (FTRL) algorithm that achieves low weighted swap
regret for the adversarial bandit problem. We follow a similar technique presented in [5] which adapts
external regret algorithms to swap regret algorithms for the unweighted case.
We present the FTRL swap algorithm in Algorithm 6. Different from FTRL (Algorithm 5), FTRL swap
maintains an additional B × B matrix θ̃t (·|·), and uses its eigenvector when taking actions. The matrix will
be updated similarly to FTRL, with a subtle difference that the loss estimator here ℓ̂t (·|b) is θt (b) times the
loss estimator ℓ̂t (·) in the FTRL algorithm (Line 4 in Algorithm 5).
In Corollary 25, we prove that FTRL swap satisfies the Assumption 2 with good swap regret bounds.
Recall that B is the number of actions, and our normalization condition requires loss ˜lt ∈ [0, 1]B for any t.
q
−1
Qt
H log B
Corollary 25. By choosing hyperparameter wt = αt
and ηt = γt =
,
i=2 (1 − αi )
t
FTRL swap (Algorithm 6) satisfies Assumption 2 with
p
p
ξsw (B, t, log(1/δ)) = 10B H log(B 2 /δ)/t, Ξsw (B, t, log(1/δ)) = 20B Ht log(B 2 /δ)
Again, we prove Corollary 25 by showing a more general weighted swap regret guarantee which works
i t
for any set of weights {wi }∞
i=1 in addition to {αt }i=1 . A general weighted swap regret is defined as
Rswap (t) := min
ψ∈Ψ

t
X
i=1

wi [hθi , li i − hψ ⋄ θi , li i].

(16)

Theorem 26. For any t ≤ K, following Algorithm 6, if ηi ≤ 2γi and ηi is non-increasing for all i ≤ t, let
ι = log(B 2 /δ), then with probability 1 − 3δ, we have
v
u
t
t
t
X
u X
X
wt B log B B
1
Rswap (t) ≤
γi wi + B t2ι
+
ηi wi + max wi ι + B
wi2 + B max wi ι/γt
i≤t
ηt
2
2 i≤t
i=1

i=1

35

i=1

We postpone the proof of Theorem 26 to the end of this section. We show first how Theorem 26 directly
implies Corollary 25.
Proof of Corollary 25. As shown in the proof of Corollary 19, the weights {wt }K
t=1 we choose satisfies a
nice property: for any t we have
αi
wi
= jt .
wj
αt
−1
Qt
P
Define R̃swap (t) := maxψ∈Ψ ti=1 αit [hθi , li i−hψ⋄θi , li i]. Plugging our choice of wi = αt
i=2 (1 − αi )
into Theorem 26, we have
t

R̃swap (t) ≤

αt B log B B X
1
ηi αit + αt ι
+
ηt
2
2
i=1
v
u
t
t
X
u X
2
i
t
αit + Bαt ι/γt .
γi αt + B 2ι
+B
i=1

i=1

By choosing ηt = γt =

q

H log B
and using Lemma 10, we can further upper bound the swap regret by
t

r
t
X
3B p
t
(H + 1) B log B
αi
√t
+
R̃swap (t) ≤
H log B
H +t
H log B
2
t
i=1
v
u
r
t
u X
2
(H + 1) ι
t
(H + 1) ι
t
i
+ B 2ι
αt + B
+
2 (H + t)
(H + t)
H log B
i=1
r
r
r
r
H log B Hι
Hι
H log B
log B
≤2B H
+ 3B
+
+ 2B
+ 2B
t
t
t
t
t
p
≤9B Hι/t + Hι/t.

To further simplify the above upper bound, consider two cases:
p
p
• If Hι/t ≤ 1, Hι/t ≥ Hι/t and thus R̃swap (t) ≤ 10B Hι/t.
p
• If Hι/t ≥ 1, B Hι/tp
≥ 1 ≥ R̃swap (t) where the last step is by the definition of R̃swap (t). Therefore
we have R̃swap (t) ≤ B Hι/t.
p
Combine the above two cases, R̃swap (t) ≤ 10B pHι/t.
Finally,
:= 10B Hι/t, which is non-decreasing in B.
√ On the other hand,
Pt we pick ξsw (B, t, log(1/δ)) √
since t′ =1 ξsw (B, t, log(1/δ)) ≤ 20B Htι, we choose Ξsw (B, t, log(1/δ)) = 20B Htι, which is concave in t.
To prove Theorem 26, we again first decompose the swap regret. We first note that by Line 7 of Algorithm 6, we have:
X
wi hθi , li i =
wi hθ̃i (·|b), θi (b)li (·)i.
b∈B

36

On the other hand, by the definition of strategy modification Ψ, we have
min
ψ∈Ψ

t
X
i=1

wi hψ ⋄ θi , li i =

X
b∈B

min

θ ⋆ (·|b)

t
X
i=1

wi θi (b) · hθ ⋆ (·|b), li (·)i.

Therefore, we have the following decomposition of the swap regret
Rswap (t) := min
ψ∈Ψ

t
X
i=1

t
XX

wi [hθi , li i − hψ ⋄ θi , li i] =

b∈B i=1

wi [hθ̃i (·|b) − θ ⋆ (·|b), θi (b)li i]

t
t
D
E XX
D
E
XX
wi θ̃i (·|b) − θ ⋆ (·|b), ˆli (·|b) +
wi θ̃i (·|b), θi (b)li (·) − ˆli (·|b)
=
b∈B i=1

|

b∈B i=1

{z

}

(A)

t
E
D
XX
+
wi θ ⋆ (·|b), ˆli (·|b) − θi (b)li (·)

|

{z

(B)

}

(17)

b∈B i=1

|

{z

}

(C)

For the remaining proof, we bound term (A), (B), (C) separately in Lemma 27, Lemma 28, Lemma 29.
Lemma 27. For any t ∈ [K], suppose ηi ≤ 2γi for all i ≤ t. The with probability 1 − δ, for any θ ⋆ ,
t
XX
b∈B i=1

t
D
E w B log B B X
1
t
ηi wi + max wi ι.
+
wi θ̃i (·|b) − θ ⋆ (·|b), ˆli (·|b) ≤
ηt
2
2 i≤t
i=1

Proof. Similar to Lemma 22, we have,
t
X
i=1

t
D
E w log B 1 X
D
E
t
wi θ̃i (·|b) − θ ⋆ (·|b), ˆli (·|b) ≤
ηi wi θ̃i (·|b), ˆli2 (·|b)
+
ηt
2
i=1

=

≤
Summing over b and using the fact that
t
XX
b∈B i=1

t
1XX

wt log B
+
ηt
2

wt log B 1
+
ηt
2

i=1 b′ ∈B
t X
X

θ 2 (b)˜li2 (bi )I{bi = b′ }
ηi wi θ̃i (b′ |b) i
(θi (b′ ) + γi )2
ηi wi

i=1 b′ ∈B

θ̃i (b′ |b)θi (b) ˆli (b′ |b)
θi (b′ )
θi (b)

P

′
′
b∈B θ̃i (b |b)θi (b) = θi (b ),

t X
D
E w B log B 1 X
ˆli (b′ |b)
t
wi θ̃i (·|b) − θ ⋆ (·|b), ˆli (·|b) ≤
+
ηi wi
ηt
2
θi (b)
′
(i) w B log B
t

≤

≤

ηt

+

1
2

wt B log B B
+
ηt
2
37

i=1 b ∈B
t X
X

i=1 b′ ∈B
t
X

 1
ηi wi li b′ + max wi ι
2 i≤t

ηi wi +

i=1

1
max wi ι
2 i≤t

′

|b)
where (i) is by using Lemma 21 with ci (b) = ηi . Notice the quantity l̂iθ(bi (b)
actually doesn’t depend on b,
so it is well-defined even after we take the summation with respect to b. The any-time guarantee is justified
by taking union bound.

Lemma 28. For any t ∈ [K], with probability 1 − δ ,
t
XX
b∈B i=1

v
u
t
t
D
E
X
u X
t
ˆ
wi2 .
γi wi + B 2ι
wi θ̃i (·|b), θi (b)li (·) − li (·|b) ≤ B
i=1

i=1

Proof. We further decompose it into
t
X
i=1

t
t
D
E
D
E X
D
E X
wi θ̃i (·|b), Ei ˆli (·|b) − ˆli (·|b) .
wi θ̃i (·|b), θi (b)li (·) − Ei ˆli (·|b) +
wi θ̃i (·|b), θi (b)li (·) − ˆli (·|b) =
i=1

i=1

The first term is bounded by
t
X
i=1

wi

D


t
E X
ˆ
θ̃i (·|b), θi (b)li (·) − Ei li (·|b) =
wi θi (b) θ̃i (·|b), (1 −


θi (·)
)li (·)
θi (·) + γi
i=1


t
X
γi
wi θi (b) θ̃i (·|b),
li .
=
θi (·) + γi
i=1

So by taking the sum with respect to b, we have
t
XX
b∈B i=1

wi

D


t
E XX
θ̃i (·|b), θi (b)li (·) − Ei ˆli (·|b) ≤
wi θi (b) θ̃i (·|b),
≤

b∈B i=1
t
XX

γi
li
θi (·) + γi



wi γi li (b′ )

b′ ∈B i=1
t
X

≤B

γi wi .

i=1

To bound the second term, notice θ̃i (b′ |b)θi (b) ≤ θi (b′ ) for any b, b′ ∈ B,
D
E X
X 
I {bt = b′ }
θ̃i (·|b), ˆli (·|b) ≤
θ̃i (b′ |b)θi (b)
≤
I bi = b′ = 1,
′
θ
(b
)
+
γ
i
i
′
′
b ∈B

b ∈B

D
E
thus {wi θ̃i (·|b), Ei ˆli (·|b) − ˆ
li (·|b) }ti=1 is a bounded martingale difference sequence w.r.t. the filtration
{Fi }ti=1 . By Azuma-Hoeffding,
t
X
i=1

wi

D

v
t
E u
u X
θ̃i (·|b), Ei ˆli (·|b) − ˆli (·|b) ≤ t2ι
wi2 .
i=1

The proof is completed by taking the summation with respect to b and a union bound.
38

Lemma 29. For any t ∈ [K], suppose γi is non-increasing in i, then with probability 1 − δ, and any θ ⋆ ,
t
XX
b∈B i=1

D
E
wi θ ⋆ (·|b), ˆli (·|b) − θi (b)li (·) ≤ B max wi ι/γt .
i≤t

Proof. The proof follows from Lemma 24 and taking the summation with respect to b.
Finally, we are ready to prove Theorem 26.
Proof of Theorem 26. Recall the decomposition of swap regret (17). We bound (A) in Lemma 27, (B) in
Lemma 28 and (C) in Lemma 29. Putting everything together, we have
v
u
t
t
t
X
X
u X
1
wt B log B B
t
γi wi + B 2ι
ηi wi + max wi ι + B
+
Rswap (t) ≤
wi2 + B max wi ι/γt .
i≤t
ηt
2
2 i≤t
i=1

i=1

39

i=1

