Causal Mean Field Multi-Agent Reinforcement
Learning
Hao MaÂ§â€  Zhiqiang Puâˆ—â€  Yi Panâ€  Boyin Liuâˆ—â€  Junlong Gaoâ€¡ Zhenyu Guoâ€¡
âˆ— School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China
Â§ School of Nanjing, University of Chinese Academy of Sciences, Beijing, China
â€  Institute of Automation, Chinese Academy of Sciences, Beijing, China
â€¡ Alibaba Group, Hangzhou, China

arXiv:2502.14200v1 [cs.AI] 20 Feb 2025

{mahao2021, zhiqiang.pu, yi.pan, liuboyin2019}@ia.ac.cn, {junlong.gjl, zhenyu.gz}@alibaba-inc.com

Abstractâ€”Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A
framework named mean-field reinforcement learning (MFRL)
could alleviate the scalability problem by employing the Mean
Field Theory to turn a many-agent problem into a two-agent
problem. However, this framework lacks the ability to identify
essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions,
though environments are nonstationary. Therefore, we propose
an algorithm called causal mean-field Q-learning (CMFQ) to
address the scalability problem. CMFQ is ever more robust
toward the change of the number of agents though inheriting the
compressed representation of MFRLâ€™s action-state space. Firstly,
we model the causality behind the decision-making process of
MFRL into a structural causal model (SCM). Then the essential
degree of each interaction is quantified via intervening on the
SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted
sum of all behavioral information according to their causal
effects. We test CMFQ in a mixed cooperative-competitive game
and a cooperative game. The result shows that our method has
excellent scalability performance in both training in environments
containing a large number of agents and testing in environments
containing much more agents.

I. I NTRODUCTION
Multi-agent reinforcement learning (MARL) has achieved
remarkable success in some challenging tasks. e.g., video
games [1], [2]. However, training a large number of agents
remains a challenge in MARL. The main reasons are 1) the
dimensionality of joint state-action space increases exponentially as agent number increases, and 2) during the training
for a single agent, the policies of other agents keep changing,
causing the nonstationarity problem, whose severity increases
as agent number increases [3]â€“[5].
Existing works generally use the centralized training and
decentralized execution paradigm to mitigate the scalability
problem via mitigating the nonstationarity problem [6]â€“[9].
Curriculum learning and attention techniques are also used
to improve the scalability performance [10], [11]. However,
above methods focus mostly on tens of agents. For largescale multi-agent system (MAS) contains hundreds of agents,
studies in game theory [12] and mean-field theory [13], [14]
offers a feasible framework to mitigate the scalability problem.
Under this framework, [14] propose an algorithm called meanfield Q-learning (MFQ), which replaces joint action in joint

Q-function with average action, assuming that the entire agentwise interactions could be simplified into the mean of local
pairwise interactions. That is, MFQ reduces the dimensionality
of joint state-action space with a merged agent. However, this
approach ignores the importance differences of the pairwise
interactions, resulting in the poor robustness. Nevertheless,
one of the drawbacks to mean field theory is that it does not
properly account for fluctuations when few interactions exist
[15] (e.g., the average action may change drastically if there
are only two adjacent agents). Ref. [16] attempt to improve the
representational ability of the merged agent by assign weight
to each pairwise interaction by its attention score. However,
the observations of other agents are needed as input, making
this method not practical enough in the real world. In addition,
the attention score is essentially a correlation in feature space,
which seems unconvincing. On the one hand, an agent pays
more attention to another agent not simply because of the
higher correlation. On the other hand, it may be inevitable that
the proximal agents will be assigned high weight just because
of the high similarity of their observation.
In this paper, we want to discuss a better way to represent
the merged agent. We propose an algorithm named causal
mean-field Q-learning (CMFQ) to address the shortcoming
of MFQ in robustness via causal inference. Research in
psychology reveals that humans have a sense of the logic of
intervention and will employ it in a decision-making context
[17]. This suggests that by allowing agents to intervene in the
framework of mean-field reinforcement learning (MFRL), they
could have the capacity to identify more essential interactions
as humans do. Inspired by this insight, we assume that
different pairwise interactions should be assigned different
weights, and the weights could be obtained via intervening.
We introduce a structural causal model (SCM) that represents
the invariant causal structure of decision-making in MFRL.
We intervene on the SCM such that the corresponding effect
of specific pairwise interaction can be presented by comparing
the difference before and after the intervention. Intuitively, the
intervening enable agents to ask â€œwhat if the merged agent
was replaced with an adjacent agentâ€ as illustrated in Fig.1.
In practice, the pairwise interactions could be embodied as
actions taken between two agents, therefore the intervention
also performs on the action in this case.

CMFQ is based on the assumption that the joint Q-function
could be factorized into local pairwise Q-functions, which
mitigates the dimension curse in the scalability problem.
Moreover, CMFQ alleviates another challenge in the scalability problem, namely nonstationarity, by focusing on crucial
pairwise interactions. Identifying crucial interactions is based
on causal inference instead of attention mechanism. Surprisingly, the scalability performance of CMFQ is much better
than the attention-based method [16]. The reasons will be
discussed in experiments section. As causal inference only
needs local pairwise Q-functions, CMFQ is practical in realworld applications, which are usually partially observable.
We evaluate CMFQ in the cooperative predator-prey game
and mixed cooperative-competitive battle game. The results
illustrate that the scalability of CMFQ significantly outperforms all the baselines. Furthermore, results show that agents
controlled by CMFQ emerge with more advanced collective intelligence. Supplemental materials could be found at
https://sites.google.com/view/cmfq.
This paper aims to alleviate the scalability problem in
MARL. In summary, our contributions include:
We analyze the bottleneck of MFRL in solving the scalability problem. By decomposing the scalability problem
into 1) the dimensionality of joint state-action space
increases exponentially as agent number increases, and 2)
the non-stationarity increases as agent number increases,
we could find that MFRL solves the first problem, while
the second problem remains largely unresolved. Hence
MFQ exhibits a strong scalability during training, but a
poor scalability during execution. That is, if we increase
the number of agents during execution, MFQ will fail
rapidly.
â€¢ We propose an algorithm named CMFQ to further alleviate the second problem, thus significantly increases
the robustness of MFQ. MFQ characterizes population
behavioral information by averaging actions of agents,
then obtains an average merged agent which lacks representational ability. CMFQ quantifies the importance
degree of each agent by counterfactual inference. Then
more reasonable and causality-aware merged agents could
be obtained, enabling agents to robustly concentrate on
agents that truly matter. Consequently, CMFQ exhibits
impressive scalability during both training and execution.
â€¢ CMFQ demonstrates a promising and flexible framework for incorporating causal inference into MFRL. The
method to calculate causal effects is very flexible. New
algorithms could be obtained by reasonably modifying
the causal module in the framework.

â€¢

II. R ELATED W ORK
The scalability problem has been widely investigated in
current literatures. Ref. [14] propose the framework of MFRL
that increases scalability by reducing the action-state space.
Several works in a related area named mean-field game also
proves that using a compact representation to characterize

The imaginary of agent ğ‘–
ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘–4
ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘–3
ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘–2
ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘‘ ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘–1
ğ‘–2

What if â€¦

ğ’Š

ğŸ
ğ’‚ğ’•âˆ’ğŸ

ğ’Š

ğŸ
ğ’‚ğ’•âˆ’ğŸ

ğ…â€²ğ’•ğ’Š
ğ…â€²ğ’Šğ’•

ğ‘–1

ğ…ğ’•ğ’Š

ğ‘–

ğ…â€²ğ’Šğ’•

agent ğ‘–â€™s
neighborhood

ğ…â€²ğ’•ğ’Š

ğ‘–

ğ’ŠğŸ
ğ‘–2
ğ’‚ğ’•âˆ’ğŸ
ğ’ŠğŸ
ğ‘–2
ğ’‚ğ’•âˆ’ğŸ

ğ’ŠğŸ
ğ’‚ğ’•âˆ’ğŸ

ğ‘¡

ğ‘–2

ğ‘¡

ğ’ŠğŸ
ğ‘–1
ğ’‚ğ’•âˆ’ğŸ

ğ‘–3
ğ’Š

ğŸ‘
ğ’‚ğ’•âˆ’ğŸ

ğ‘–

ğ‘–

ğ‘–

ğ‘–

ğ‘¡
ğ‘¡

ğ’Š
à´¥ğ’•âˆ’ğŸ
ğ’‚

ğ‘–4
ğ’Š

ğŸ’
ğ’‚ğ’•âˆ’ğŸ

ğ‘¡

ğ‘¡

ğ‘–

My policy would change
dramatically if the merged agent
were replaced with ğ‘–2 . I should
pay more attention to ğ‘–2 !

Fig. 1. Blue agents and orange agents belong to different teams. The purple
agent denote a merged agent that simply average all agents in agent iâ€™s
neighborhood. The diagram on the left shows a scenario in which the central
agent i interacts with many agents, ik denotes the kth agent in the observation
of agent i. In the framework of MFRL, the scenario is transferred to the
diagram in the middle, in which an merged agent is used to characterize
all the agents in the central agentâ€™s observation. Our method further enables
the central agent to learn to ask â€œwhat ifâ€. When it asks this question, it
can imagine the scenario illustrated in the right diagram. The central agent
can hypothetically replace the action of the merged agent in MFRL with the
action of a neighborhood agent, and if this replacement will cause dramatic
changes in policy, it means this neighborhood agent is potentially important.
Thus central agent should pay more attention to the interaction with this
neighborhood agent.

population information helps solve scalability problem [18],
[19].
Several works were proposed to improve MFQ. Ref. [20]
proposed a weighted mean-field assigning different weights to
neighbor actions according to the correlations of the hand-craft
agent attribute set, which is difficult to generalize to different
environments. Ref. [16] calculate the weights with attention
score. The observations of other agents are needed to calculate
the attention scores, making its practicality not satisfactory.
Our work is also closely related to recent development
in causal inference. Researches indicate that once the SCM,
which implicitly contains the causal relationships between
variables, is constructed, we can obtain the causal effect by
intervening. The causal inference has already been exploited
for communication pruning [21], solving credit assignment
problem [7], [22], demonstrating the potential of causal inference in reinforcement learning [23]â€“[25]. Ref. [26] and
[27] further proved that SCM could be equally replaced with
NCM under certain constraints, enabling us to ask â€œwhat ifâ€
by directly intervening on neural network.
III. P RELIMINARY
This section discusses the concepts of the stochastic game,
mean-field reinforcement learning, and causal inference.
A. Stochastic Game
A N-player stochastic game could be formalized as G =<
S, A, P, r, N, Î³ >, in which N agents in the environment take
action a âˆˆ A = Ã—Ni=1 Ai to interact with other agents and
the environment. Environment will transfer according to the
transition probability P(sâ€² | s, a) : S Ã— A Ã— S â†’ [0, 1], then every
agent obtains its reward ri (s, ai ) : S Ã— Ai â†’ R and Î³ âˆˆ [0, 1]
is the discount factor. Agent makes decision according to
its policy Ï€i (s) : S â†’ â„¦(Ai ), where â„¦(Ai ) is a probability
distribution over agent iâ€™s action space Ai .

Imaginary of agent ğ‘–
ğ’Š

ğ·ğ¼ğ‘†ğ‘‡ ğ…ğ’•ğ’Š |ğ…â€² ğ’• is large!
I should pay more
attention to ğ‘–2

agent ğ‘–â€™s
neighborhood

ğ…â€²ğ’•ğ’Š

ğ’Š

ğŸ
ğ’‚ğ’•âˆ’ğŸ

ğ‘–

ğ‘–2

ğ‘°ğ’ ğ’‚ğ’ˆğ’†ğ’ğ’•ğ’” ğ’Šâ€² ğ’” ğ’ğ’†ğ’Šğ’ƒğ’ğ’“ğ’‰ğ’ğ’ğ’…
ğ‘ ğ‘¡

Causal
Module

ğ‘‡ğ¸ğ‘¡ğ‘–,2

ğ‘‡ğ¸ğ‘¡ğ‘–,ğ‘

Causal
Module

ğ‘–

ğ‘–,ğ‘
ğ‘ğ‘¡âˆ’1

ğœ”ğ‘¡ğ‘–,2

ğ‘–

ğœ”ğ‘¡ğ‘–,ğ‘

Q
twork

ğ‘–
ğ‘à·”ğ‘¡âˆ’1

Q
Network

ğ‘ ğ‘¡
ğ‘–,ğ‘˜
ğ‘ğ‘¡âˆ’1

ğ‘–,ğ‘˜
ğ‘‡ğ¸ğ‘¡âˆ’1
ğ‘–
ğ‘à´¤ğ‘¡âˆ’1

Softmax

ğœ‹ğ‘¡ğ‘–

Softmax

ğ‘ ğ‘¡
Q
Network

ğ‘–
ğ‘à´¤ğ‘¡âˆ’1

Q
Network

Softmax

ğ‘–,ğ‘˜
ğ‘‡ğ¸ğ‘¡âˆ’1

ftmax

(b)

ğœ‹ğ‘¡ğ‘–

Fig. 2. (a) is CMFQâ€™s architecture. Each neighborhood agent is assigned
a weight according to its causal effect to the policy of the central agent.
(b) is the causal module. It calculate the KL divergence between the two
policies that the merged agent is represented by the average action and the
kth neighborhood agent action respectively. A large KL divergence means the
kth neighborhood agent might be ignored in the merged agent represented by
the average action, hence it should be assigned a higher weight to form a
better merged agent.

The joint Q-function of agent i is parameterized by Î¸i and
takes s and a. It is updated as
h
2 i
Li (Î¸i ) = Es,a,r,sâ€² Qi (s, a; Î¸i ) âˆ’ y ,

(1)
y = r + Î³ max Qi sâ€² , a; Î¸âˆ’
i
aâ€²

It is proven that Ï€ti will converge eventually [14].
Q
Network

C. Causal Inference

ğ‘–

(a)

ğ‘–,ğ‘˜
ğ‘ğ‘¡âˆ’1

Q
Network

Softmax

ğ‘–,2
ğ‘ğ‘¡âˆ’1

ğœ”ğ‘¡ğ‘–,1

Softmax

ğ‘‡ğ¸ğ‘¡ğ‘–,1

Weighted Average

Causal
Module

Normalization

ğ‘–,1
ğ‘ğ‘¡âˆ’1

information of agents in the neighborhood of agent i. Finally,
the policy of the central agent i is determined by pairwise
Q-function

i

exp Î²Qti st , ati , aÌ„tâˆ’1
i
i
i

Ï€t at | s, aÌ„t =
(3)
i
i
âˆ‘aâ€²i âˆˆA i exp Î²Qti st , aâ€² , aÌ„tâˆ’1

The data-driven statistical learning method lacks the identification of causality which is quite a vital part of composing human acknowledge. The SCM established with human
knowledge is needed to represent the causality among all
the variables we consider. An SCM is a 4-tuple M =<
U, V, F, P(U) >. U = {U1 ,U2 , Â· Â· Â· ,Um } is the set of exogenous
variables which are determined by factors outside the model.
V = {V1 ,V2 , Â· Â· Â· ,Vn } is the set of endogenous variables that
are determined by other variables. F is a set of functions
{ fV1 , fV2 , Â· Â· Â· , fVn } such that fV j maps PaV j âˆª UV j to V j . where
UV j âŠ† U is all the exogenous variables directly point to V j and
PaV j âŠ† V\V j is all the endogenous variables directly point
to V j . That is, V j = fV j (PaV j , UV j ) for j = 0, 1, Â· Â· Â· , n. P(U)
is the probability distribution function over the domain of
U. The causal mechanism in SCM M induced an acyclic
graph G , which uses a direct arrow to present a direct effect
between variables as shown in Fig.3. Intervention is performed
through an operator called do(x), which directly deletes fX and
replaces it with a constant X = x, while the rest of the model
keeps unchanged. The equation defines the post-intervention
distribution
PM (y|do(x)) â‰œ PMx (y)
(4)
where Mx is the SCM after performing do(x). Once we obtain
the post-intervention distribution, one may measure the causal
effect by comparing it with the pre-intervention distribution.
A common measure is the average causal effect.

i

where Î¸âˆ’
i is updated by with Î¸i every C steps and set fixed
until the next C steps finish.
B. Mean Field Reinforcement Learning
Mean field approximation turns a many-agent problem into
a two-agent problem by mapping the joint action space to
a single action space. The joint action Q function is firstly
factorized considering only local pairwise interactions, then
pairwise interactions are approximated using the mean-field
theory



1
Qi s, a1 , a2 , . . . , aN = i âˆ‘ Qi s, ai , ak
N kâˆˆN(i)
(2)

i
i i
â‰ˆ Q s, a , aÌ„
where N i = |N(i)|. N(i) is the set of agent iâ€™s neighboring
agents. Interactions between central agent i and its neighbors
are reduced to the interaction between the central agent and
an abstract agent, which is presented by average behavior

E[Y |do(x0â€² )] âˆ’ E[Y |do(x0 )]

(5)

where x0â€² and x0 are two different interventions. The causal
effect may also be measured by the experimental Risk Ratio
[28]
E[Y |do(x0â€² )]
(6)
E[Y |do(x0 )]
IV. M ETHOD
A. Counterfactual Policy
To answer â€œwhat ifâ€œ questions raised in Introduction(I),
counterfactual inference need to be performed on the policy
of central agent. For ease of understanding, we construct an
SCM which reveals relations among all variables of interest. In
i
the setting of MFRL, mean action aÌ„tâˆ’1
and state st determine
i
i
the policy Ï€t (Â·| st , aÌ„tâˆ’1 ) of agent i. As the key relation we
concern is how the merged interaction affects Ï€ti , the SCM is
j
constructed center on Ï€t as illustrated in Fig.3(b). Note that
the SCM is derived from the definitions in stochastic game

ğ‘ â‰” ğ‘“ğ‘ ğ‘ˆğ‘

ğ‘

ğ‘‹

ğ‘Œ

ğ‘

ğ‘‹

ğ‘Œ


Qi s,ai ,aÌŒi as QiaÌŒ . Then Q j (s, a1 , a2 , Â· Â· Â· , aN ) is approximated
using mean-field theory and considering the causality-aware
weights

ğ‘‹ â‰” ğ‘“ğ‘‹ ğ‘, ğ‘ˆğ‘‹
ğ‘Œ â‰” ğ‘“ğ‘Œ ğ‘‹, ğ‘ˆğ‘Œ

Qi (s, a1 , a2 , Â· Â· Â· , aN ) =

ğ‘ˆğ‘ , ğ‘ˆğ‘‹ , ğ‘ˆğ‘Œ ~ğ’©(0,1)

ğ‘‘ğ‘œ ğ‘¥0

ğ‘ğ‘¡ğ‘–

ğ‘–
ğ‘à´¤ğ‘¡âˆ’1



1
= âˆ‘ wi,k QiaÌŒ +âˆ‡aÌŒi QiaÌŒ Â· Î´ai,k + Î´ai,k Â·âˆ‡2aÌƒi,k QiaÌƒk Â· Î´ai,k
2
kâˆˆN(i)
#
"
 
= QiaÌŒ +âˆ‡aÌŒi QiaÌŒ Â· âˆ‘ wi,k Î´ai,k + âˆ‘ wi,k Ris,ai ai,k

ğ‘ğ‘¡ğ‘–

ğ‘–
ğ‘à´¤ğ‘¡âˆ’1

= QiaÌŒ +

(a)
ğ‘ ğ‘¡ ~ğ‘ƒ(âˆ™ |ğ‘ ğ‘¡âˆ’1 , ğ‘ğ‘¡âˆ’1 )

ğ‘ ğ‘¡

1
ğ‘˜
à· ğ‘ğ‘¡âˆ’1
ğ‘ğ‘–

ğ‘ ğ‘¡

wi,k Ris,ai

âˆ‘

(9)

kâˆˆN(i)

kâˆˆN(i)

ğ‘–
ğ‘ğ‘¡ğ‘– ~ğœ‹ğ‘¡ğ‘– âˆ™ |ğ‘ ğ‘¡ , ğ‘à´¤ğ‘¡âˆ’1
ğ‘–
ğ‘à´¤ğ‘¡âˆ’1
â‰”

âˆ‘ wi,k Qiak
kâˆˆN(i)

 
ai,k â‰ˆ QiaÌŒ

kâˆˆN(i)

ğ‘˜âˆˆğ‘(ğ‘–)

ğ‘˜
ğ‘‘ğ‘œ ğ‘ğ‘¡âˆ’1

ğ‘˜
ğ‘˜
ğ‘˜
ğ‘ğ‘¡âˆ’1
~ğœ‹ğ‘¡âˆ’1
âˆ™ |ğ‘ ğ‘¡âˆ’1 , ğ‘à´¤ğ‘¡âˆ’2

(b)
Fig. 3. (a) is a canonical SCM, when do(x0 ) is performed on X, all causes of
X will be broken and keep all variable constant but only change X to x0 . (b)
i
is the SCM of MFRL, the do-calculus on aÌ„tâˆ’1
follows the same procedure.

and MFRL. Formally, the causal effect of acting ak on Ï€ti is
qualified as follow.
i,k
i
i
T Eti,k = KL(Ï€ti (Â· | st , ati , aÌ„tâˆ’1
), Ï€ti (Â· | st , ati , do(aÌ„tâˆ’1
= atâˆ’1
))) (7)
i,k
where atâˆ’1
is the action of the kth agent in the neighborhood
of agent i. For unknown distributions, the causal effects are
quantified using the difference in statistics before and after the
intervention as Eq.(5) and Eq.(6). As the policies in Eq.(7)
are known, we can utilize the KL divergence to quantify
causal effects, because the essential idea of treatment effect
is to measure the change in distribution after do-calculus.
i,k
i
Ï€ti (Â· | st , ati , do(aÌ„tâˆ’1
= atâˆ’1
)) is the counterfactual policy. We
could distinguish nontrivial interactions according to their
causal effects. Because a large KL divergence means that the
preferred action in the policy of plain average merged agent
could be a bad choice in the counterfactual policy, which
implies a large potential threat of this interaction.
It is worth noting that not all neural networks are capable
of causal inference [26]. As a neural network learned by
interacting with the environment, Ï€ti lies on the second layer
of Pearl Causal Hierarchy [29], and naturally contains both
the causality between agent-wise interaction and the causality
between agent-environment interaction. It is sufficient for
estimating the causal effect of certain interaction.

B. Improving MFQ with Causal Effect
In MFRL, we assume that different pairwise Q-functions
should be assigned different weights depending on their potential influences on the policy of central agent. Hence, the
factorization of Eq.(2) should be revised to



Qi s, at1 , at2 , . . . , atN = âˆ‘ wi,k Qi s, ai , ai,k
(8)
kâˆˆN(i)

where N(i) is the set of agent
 iâ€™s adjacent agents. For simplicity, we denote Qi s, ai , ai,k as Qiak , Qi s, ai , aÌƒi,k as QiaÌƒk , and

where Î´ai,k = ai,k âˆ’ aÌŒi and aÌŒi = âˆ‘kâˆˆN(i) wi,k ai,k , hence
âˆ‘k wi,k Î´ai,k = 0. In the second-order term, aÌƒi,k = aÌŒi + Îµi,k Î´ai,k ,
Îµi,k âˆˆ (0, 1). Ris,ai ai,k denotes the first-order Taylor expansionâ€™s Lagrange remainder which
 is bounded by [âˆ’L, L] in the
condition that the Qi s, ai , ai,k function is L-smoothed. The
remainder is a value fluctuating around zero. As [14] discussed
in their work, under the assumption that fluctuations caused
by adjacent agents tend to cancel each other, the remainder
could be neglected.
Once causal effects of pairwise interactions are known,
the next question is how to to improve the representational
capacity of the merged agent. Both linear methods, e.g.,
weighted sum, or nonlinear methods, e.g., encoding with a
neural network, might be useful. However, to ensure the
merged agentâ€™s reasonability, we prefer a representation in
the linear space formed by adjacent agentsâ€™ action vectors.
An intuitive method that can induce reasonable output is a
weighted sum. In practice, we find that weighted sum using
respective causal effects as weight is enough to effectively
improve the representational capacity of average action

Ï€ti

i
ati | st , aÌŒtâˆ’1



i
exp Î²Qti st , ati , aÌŒtâˆ’1

=



i
âˆ‘aâ€² i âˆˆA i exp Î²Qti st , aâ€² i , aÌŒtâˆ’1
i,k
âˆ‘ wti,k atâˆ’1

i
aÌŒtâˆ’1
=

 ,
(10)

kâˆˆN(i)

wti,k =

T Eti,k + Îµ


i,k
âˆ‘kâˆˆN(i) T Et + Îµ

(11)

where subscripts are used to denote time steps. T Eti,k is
i,k
calculated according to Eq.(7). Each atâˆ’1
is encoded in one
hot vector. Hence the weighted sum returns a reasonable
representation in the linear space formed by the actions of
neighborhoods. Moreover, the representation is close to essential actions, emphasizing high-potential impact interactions. A
term Îµ was introduced to smooth the weight distribution across
all adjacent agents, avoiding additional nonstationarity during
training. Besides, the naive mean-field approximation could be
achieved when Îµ â†’ âˆ.

   
   

   

   

   

   

   

 : L Q  5 D W H

 7 R W D O  5 H Z D U G

The Q-function Qi update using the following loss function
similar with Eq.(1)
h
 2 i
Li (Î¸i ) = Es,a,r,sâ€² Qi s, ai , aÌŒi ; Î¸i âˆ’ y
(12)

   
   



i
y = r + Î³ max Qi sâ€² , aâ€² , aÌŒi ; Î¸âˆ’
i
â€²i

 $ W W H Q W L R Q  0 ) 4
 0 ) 4
 & 0 ) 4
 , 4 /
 5 D Q G R P

  

(13)

 

 

   

A. Mixed cooperative-competitive game
Task Setting. In this task, agents are separated into two
groups, each containing N agents. Every agent tries to survive
and annihilate the other group. Ultimately the team with more
agents surviving wins. Each agent obtains partial observation
of the environment and knows the last actions other agents
took. Agents will be punished when moving and attacking to
lead agents to act efficiently. Agents are punished when dead
and only rewarded when killing the enemy. The reward setting
requires the agent to cooperate efficiently with teammates
to annihilate enemies. In the experiments, we train CMFQ,
IQL, MFQ, and Attention-MFQ in the setting of N = 64, then
we change N from 64 to 400 to investigate the scalability
of CMFQ. The concrete reward values are set as follow:
rattack = âˆ’0.1, rmove = âˆ’0.005, rdead = âˆ’0.1, rkill = 5. We train
every algorithm in self-play paradigm.
Quantitative Results and Analysis. As illustrated in
Fig.4(b), we compare CMFQ with Attention-MFQ, MFQ, and
IQL. We do not choose [20] as a baseline because it is a
correlation-based algorithm identical to Attention-MFQ. We
assume that the attention-based method is a more challenging baseline. Moreover, in addition to these algorithms, we
also set ablation algorithms named Random to verify that
the performance improvement of CMFQ is not caused by
randomization. Random follows the same pipeline as CMFQ
but returns a random causal effect for each interaction. Fig.4(a)
shows the learning curve of all algorithms. We can see
that the total rewards of all algorithms converge to a stable
value, empirically demonstrating the training scalability of our
algorithm.
To compare the performance of each algorithm, we put
trained algorithms in the test environment that N = 64, and
let them battle against each other. Fig.4(b) shows that MFQ
performs better than IQL but worse than Attention-MFQ, indicating that the mean-field approximation mitigates the scalability problem in this task. However, the simply averaging as
MFQ is not a good representation of the population behavioral
information. In order to improve its representational ability for

   

   

(a) Total reward during training.

   
   
   
   

 , 4 /

 0 ) 4  $ W W H Q W L R Q  0 ) 4  & 0 ) 4

 5 D Q G R P

(b) Performance comparisons.

CMFQ64
MFQ64
Attention-MFQ64

   

 Y V   , 4 / 

   

 Y V   0 ) 4
 Y V   $ W W H Q W L R Q  0 ) 4

   

   

   
   

 : L Q  5 D W H

 : L Q  5 D W H

We evaluate CMFQ in two tasks: a mixed cooperativecompetitive battle game and a cooperative predator-prey game.
In the battle task, we compare CMFQ with independent Qlearning (IQL) [30], MFQ [14], and Attention-MFQ [16] to
investigate the effectiveness and scaling capacity of CMFQ.
We further verify the effectiveness of CMFQ in another task.
In the predator-prey task, we compare CMFQ with MFQ and
Attention-MFQ. Our experiment environment is MAgent [31].

   

   

 ( S L V R G H

a

V. E XPERIMENTS

   

 Y V   , 4 / 
 Y V   0 ) 4
 Y V   $ W W H Q W L R Q  0 ) 4
 Y V   & 0 ) 4
 Y V   5 D Q G R P

   

   
   
   

   

   
   
 0 ) 464

 0 ) 4100

 0 ) 4400

(c) Test Scalability curve.

   

= 0.001

= 0.01

=1

(d) Ablation experiments of Îµ.

Fig. 4. Win rate during execution. (a) demonstrates the curves of total reward
during training for each algorithm. (b) shows the results that algorithms battle
against each other. the horizontal axis is divided into five groups by algorithms,
and within each group there are five bars representing the win rate of the
algorithm on the horizontal axis. (c) shows win rates of algorithms in the
label against MFQ algorithms which are on the horizontal axis. (d) shows the
win rate of CMFQ with different Îµ against other algorithms.

large-scale scenarios, it is necessary to assign different weights
to different agents. Moreover, CMFQ outperforms AttentionMFQ during the test, verifying the correctness of our hypothesis that correlation-based weighting is insufficient to catch
the essential interactions properly, while the intervention fills
this gap by giving agents the ability to ask the counterfactual
question about â€œwhat ifâ€.
We further investigate the test scalability of CMFQ, MFQ,
and Attention-MFQ. Firstly, we train these three algorithms
in 64 vs. 64 scenario with self-play, denoted as CMFQ64 ,
MFQ64 , Attention-MFQ64 respectively, and further train the
MFQ algorithm in 100 vs. 100 and 400 vs. 400 scenarios, denoted as MFQ100 and MFQ400 . Then, allow CMFQ64 , MFQ64 ,
and Attention-MFQ64 to battle against MFQ64 , MFQ100 and
MFQ400 in environments 64 vs. 64, 100 vs. 100, 400 vs. 400
respectively, that is, letting CMFQ, MFQ, and Attention-MFQ
control more agents than they were trained, to reveal the test
scalability of the algorithms. As shown in Fig.4(c), the test
scalability of MFQ is the worst, which means that we need
to retrain MFQ when the number of agents increases and
the test scalability of Attention-MFQ is slightly better. The
test scalability of CMFQ is significantly better than both of
them. Furthermore, CMFQ achieves win rates of nearly 100%
against MFQ100 and 32% against MFQ400 .
Ablations. We set two ablation experiments. The first one
to ablate the effectiveness of causal effects in CMFQ. As
illustrated in Fig.4(b), the performance of Random is inferior
to MFQ, verifying the validity of causal effect in CMFQ. The
other one is ablation for Îµ. As we analyze in IV-B, Îµ is an
adjustable parameter in the interval [0, +âˆ]. As Îµ increases,
the effect of each interaction becomes smoother and eventually

    

Besiege

    

Besiege CMFQ

    
    

Teaming up

    

    
    
    

  Ã®

     Ã®

  Ã®

     Ã®

  Ã®

    

MFQ
Attention-MFQ

    

 $ Y H U D J H  5 H Z D U G

 $ Y H U D J H  5 H Z D U G

 $ Y H U D J H  5 H Z D U G

    

    

g up

    

CMFQ
MFQ

Attention-MFQ

    

    
    
    

  Ã®

     Ã®

  Ã®

     Ã®

  Ã®

    

  Ã®

     Ã®

  Ã®

     Ã®

  Ã®

(a) MFQ controls prey. (b) Attention-MFQ con- (c) CMFQ controls prey.
trols prey.

(a)

Fig. 7. Total reward of predators during execution changes when the
number of agents increases. 1Ã— denotes N predator =20, N prey =40, 4Ã— demotes
N predator =80, N prey =160 and so on. All algorithms are trained in the 1Ã—
environment.

(b)

Fig. 5. Visualization of CMFQ vs MFQ in 64 vs 64 environment. Red squares
denote CMFQ, and blue squares denote MFQ, the vertical bar on the left side
of the square indicates its health point, and the surrounding circular area
indicates its attack range. When agent attacks, an arrow will be extended to
point at the attack target.

    

    

 $ W W H Q W L R Q  0 ) 4
 0 ) 4
 & 0 ) 4

    
    
    
   
 

 
    
     
     
     

    
     

 $ W W H Q W L R Q  0 ) 4
 0 ) 4
 & 0 ) 4

   

 7 R W D O  5 H Z D U G

 7 R W D O  5 H Z D U G

    

     

 

   

   

   

   

   

 ( S L V R G H

(a) Total reward of Predator.

     

 

   

   

   

   

   

 ( S L V R G H

(b) Total reward of Prey.

Fig. 6. Total reward during training.

CMFQ equal to MFQ when Îµ â†’ +âˆ. From the Fig.4(d), we
can see that as we adjust Îµ from 0.001 to 1, the learning curve
of CMFQ always converges, and in the test environment, win
rates of CMFQ always outperform other baselines. When Îµ is
relatively large, the win rate is close to that of MFQ.
Visualization Analysis. As illustrated in Fig.5(a), CMFQ
learns the tactic of besieging, while MFQ tends to confront
frontally. The results in Fig.5(b) indicate the tricky issue in
mixed cooperative-competitive game: agents need to cooperate
with their teammates to kill enemies, whereas only the agent
who hits a fatal attack gets the biggest reward rkill , driving
agents hesitating to attack first. When there are few agents, the
policies of MFQ and CMFQ tend to be conservative. However,
CMFQ presents more advanced tactics: agents learn the trick
of teaming up in the mixed cooperative-competitive game.
When an agent chooses to attack, the adjacent teammates
will arrive to help, achieving the maximum reward with the
smallest cost of health. Moreover, Fig.5(b) also shows that
attacks of CMFQ are more focused than baselines. CMFQ
can discriminate key interactions and have a more accurate
timing of attacks, while MFQ lacks this discriminatory ability
and thus keeps attacking.
B. Cooperative game
Task Setting. In this task, agents are divided into predator
and prey. Prey move 1.5 times faster than predators, and their
task is to avoid predators as much as possible. Predators are

four times larger than prey and can attack but not yield any
damage. Predators only get rewarded when they are close
to prey. Therefore, to gain the reward, they must cooperate
with other predators and try to surround prey with their
size advantage. In our experiments, to test the scalability of
the CMFQ, we first train MFQ, CMFQ, and Attention-MFQ
employing the self-play paradigm in a scenario involving 20
predators and 40 prey, and then test them in environments
involving (20 predators, 40 prey), (80 predators, 160 prey),
(180 predators, 360 prey) respectively. The reward are set as
follow: rattack = âˆ’0.2, rsurround = 1, rbe surrounded = âˆ’1.
Quantitative Results and Analysis. We compare CMFQ
with MFQ and Attention-MFQ. First, we investigate their
training scalability in (20 predators, 40 prey), as shown in
Fig.6(a) and Fig.6(b), all of them converge to a stable reward
total reward, verifying their training scalability. Then, we
enlarge the number of agents during execution to investigate
their test scalability. To demonstrate the scalability gap of
different algorithms, we allow the algorithms to execute in an
adversarial form, which means that one algorithm controls the
predator and another controls the prey. For the environment,
we change the number of agents to 1x, 4x, and 9x of the
number in the training environment.
Because the reward rbe surrounded of prey and the reward
rsurround of predator are zero-sum and cooperation exists
mainly among predators, we use the total reward of predators
to indicate each algorithmâ€™s performance. The results are
shown in Fig.7. Total rewards in specific environment indicate
the train scalability, since a higher total reward means agents
learn better policy during training. Trends of lines are related
to test scalability, and a more flat line indicates the better test
scalability of the algorithm. We can see that the total reward
of Attention-MFQ is higher than that of MFQ, and the trend
is similar to that of MFQ. In comparison, the total reward of
CMFQ is higher than that of both MFQ and Attention- MFQ,
and the trend is ever more flat, indicating that CMFQ has
better scalability.
Visualization Analysis. The results that the trained CMFQ
and Attention-MFQ controls predators are shown in Fig.8. In
the the environment that N predator =20, N prey =40, both CMFQ
and Attention-MFQ perform similarly. Predators learn two
strategies: four predators cooperating to surround the prey
in an open area; two or three predators surrounding the

Broader impact. CMFQ comprehensively alleviating the
scalability problem. This brings very practical benefits: In
environments where the observed dimension does not change
with the number of agents, multiplying the number of agents
will no longer force us to retrain the model, thanks to the
robustness of CMFQ. Besides, we can train our models in
simpler environments and use them in more complex environments to reduce the training overhead.
VII. ACKNOWLEDGEMENTS

Fig. 8.
Visualization of cooperative predator prey game. The first
row is results of CMFQ, the second row is results of Attention-MFQ.
N predator =20,N prey =40 for the left column, N predator =40,N prey =80 for the
middle column, N predator =180,N prey =360 for the last column. Red squares
are predators while blue squares are prey, the grey squares are obstacles. All
images are obtained 400 steps after the game begin.

prey with the help of obstacles. In the environment that
N predator =40, N prey =80, when the number of agents increases,
predators controlled by Attention-MFQ are more dispersed
than predators controlled by CMFQ. Besides, Attention-MFQ
has more predators idle than CMFQ. Predators controlled by
CMFQ gather on map edges, because it is more efficient
to surround prey with the help of map edges. In addition,
predators controlled by CMFQ learn an advanced strategy to
drive prey to map edges then take advantage of the terrain
to surround them. In the environment that N predator =180,
N prey =360, the advanced strategy is also presented. Moreover,
predators controlled by CMFQ master the skill to utilize the
bodies of still teammates who have captured prey as obstacles.
Thus, predators controlled by CMFQ present a high degree of
aggregation and environmental adaptability.
VI. C ONCLUSIONS AND D ISCUSSIONS
This paper aims at scalability problem in large-scale MAS.
Firstly, We inherit the framework of MFRL which significantly
reduce the dimensionality of joint state-action space. To further
handle the intractable nonstationarity when the number of
agent is large, we propose an SCM to model the decisionmaking process, and enable agents to identify the more crucial
interactions via intervening on the SCM. Finally a causalityaware representation of population behavioral information
could be obtained by the weighted sum of the action of each
agent according to its causal effect. Experiments in two tasks
reveal the excellent scalability of CMFQ.
Limitation and future work. Despite the significant improvement that CMFQ brings to the robustness of MFQ,
we contend that there is still much to explore in the causal
inference module itself. Specifically, we question what other
do-calculus techniques may be feasible beyond replacing the
average action with a specific action. We leave this exploration
as future work to develop more robust and interpretable
algorithms.

This work was supported by the National Key Research and Development Program of China under Grant
2020AAA0103404, the Beijing Nova Program under Grant
20220484077, the National Natural Science Foundation of
China under Grant 62073323, and Alibaba Group through
Alibaba Innovative Research (AIR) Program.
R EFERENCES
[1] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al., â€œGrandmaster level in starcraft ii using multi-agent reinforcement learning,â€
Nature, vol. 575, no. 7782, pp. 350â€“354, 2019.
[2] B. Wu, â€œHierarchical macro strategy model for moba game ai,â€ in
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33,
no. 01, 2019, pp. 1206â€“1213.
[3] K. P. Sycara, â€œMultiagent systems,â€ AI magazine, vol. 19, no. 2, pp.
79â€“79, 1998.
[4] K. Zhang, Z. Yang, and T. BasÌ§ar, â€œMulti-agent reinforcement learning:
A selective overview of theories and algorithms,â€ Handbook of reinforcement learning and control, pp. 321â€“384, 2021.
[5] S. Gronauer and K. Diepold, â€œMulti-agent deep reinforcement learning:
a survey,â€ Artificial Intelligence Review, Apr. 2021.
[6] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster,
and S. Whiteson, â€œQmix: Monotonic value function factorisation for
deep multi-agent reinforcement learning,â€ in International conference
on machine learning. PMLR, 2018, pp. 4295â€“4304.
[7] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
â€œCounterfactual multi-agent policy gradients,â€ in Proceedings of the
AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.
[8] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
â€œMulti-agent actor-critic for mixed cooperative-competitive environments,â€ Advances in neural information processing systems, vol. 30,
2017.
[9] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
â€œValue-decomposition networks for cooperative multi-agent learning,â€
arXiv preprint arXiv:1706.05296, 2017.
[10] Q. Long, Z. Zhou, A. Gupta, F. Fang, Y. Wu, and X. Wang, â€œEvolutionary population curriculum for scaling multi-agent reinforcement
learning,â€ arXiv preprint arXiv:2003.10423, 2020.
[11] S. Iqbal and F. Sha, â€œActor-attention-critic for multi-agent reinforcement
learning,â€ in International conference on machine learning. PMLR,
2019, pp. 2961â€“2970.
[12] L. E. Blume, â€œThe statistical mechanics of strategic interaction,â€ Games
and economic behavior, vol. 5, no. 3, pp. 387â€“424, 1993.
[13] H. E. Stanley, Phase transitions and critical phenomena. Clarendon
Press, Oxford, 1971, vol. 7.
[14] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, â€œMean
field multi-agent reinforcement learning,â€ in International conference
on machine learning. PMLR, 2018, pp. 5571â€“5580.
[15] D. I. Uzunov, Introduction to the theory of critical phenomena: mean
field, fluctuations and renormalization. World Scientific, 1993.
[16] B. Wang, S. Li, X. Gao, and T. Xie, â€œWeighted mean field reinforcement
learning for large-scale uav swarm confrontation,â€ Applied Intelligence,
pp. 1â€“16, 2022.
[17] S. A. Sloman and D. Lagnado, â€œCausality in thought,â€ Annual review
of psychology, vol. 66, pp. 223â€“247, 2015.
[18] X. Guo, A. Hu, R. Xu, and J. Zhang, â€œLearning mean-field games,â€
Advances in Neural Information Processing Systems, vol. 32, 2019.

[19] S. Perrin, M. LaurieÌ€re, J. PeÌrolat, M. Geist, R. EÌlie, and O. Pietquin,
â€œMean field games flock! the reinforcement learning way,â€ in Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence. California: International Joint Conferences on Artificial
Intelligence Organization, Aug. 2021.
[20] T. Wu, W. Li, B. Jin, W. Zhang, and X. Wang, â€œWeighted mean-field
multi-agent reinforcement learning via reward attribution decomposition,â€ in International Conference on Database Systems for Advanced
Applications. Springer, 2022, pp. 301â€“316.
[21] Z. Ding, T. Huang, and Z. Lu, â€œLearning individually inferred communication for multi-agent cooperation,â€ Advances in Neural Information
Processing Systems, vol. 33, pp. 22 069â€“22 079, 2020.
[22] S. Omidshafiei, D.-K. Kim, M. Liu, G. Tesauro, M. Riemer, C. Amato,
M. Campbell, and J. P. How, â€œLearning to teach in cooperative multiagent reinforcement learning,â€ in Proceedings of the AAAI Conference
on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 6128â€“6136.
[23] J. Pearl, â€œThe seven tools of causal inference, with reflections on
machine learning,â€ Communications of the ACM, vol. 62, no. 3, pp.
54â€“60, 2019.
[24] â€”â€”, â€œDirect and indirect effects,â€ Probabilistic and Causal Inference:
The Works of Judea Pearl, p. 373, 2001.
[25] J. Peters, D. Janzing, and B. SchoÌˆlkopf, Elements of causal inference:
foundations and learning algorithms. The MIT Press, 2017.
[26] K. Xia, K.-Z. Lee, Y. Bengio, and E. Bareinboim, â€œThe causal-neural
connection: Expressiveness, learnability, and inference,â€ Advances in
Neural Information Processing Systems, vol. 34, pp. 10 823â€“10 836,
2021.
[27] M. ZecÌŒevicÌ, D. S. Dhami, P. VelicÌŒkovicÌ, and K. Kersting, â€œRelating
graph neural networks to structural causal models,â€ arXiv preprint
arXiv:2109.04173, 2021.
[28] J. Pearl, â€œCausal inference,â€ in Proceedings of Workshop on Causality:
Objectives and Assessment at NIPS 2008, ser. Proceedings of Machine
Learning Research, I. Guyon, D. Janzing, and B. SchoÌˆlkopf, Eds.,
vol. 6. Whistler, Canada: PMLR, 12 Dec 2010, pp. 39â€“58. [Online].
Available: https://proceedings.mlr.press/v6/pearl10a.html
[29] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard, â€œOn pearlâ€™s
hierarchy and the foundations of causal inference,â€ in Probabilistic and
Causal Inference: The Works of Judea Pearl, 2022, pp. 507â€“556.
[30] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
J. Aru, and R. Vicente, â€œMultiagent cooperation and competition with
deep reinforcement learning,â€ PloS one, vol. 12, no. 4, p. e0172395,
2017.
[31] L. Zheng, J. Yang, H. Cai, M. Zhou, W. Zhang, J. Wang, and Y. Yu,
â€œMagent: A many-agent reinforcement learning platform for artificial
collective intelligence,â€ in Proceedings of the AAAI conference on
artificial intelligence, vol. 32, no. 1, 2018.

u could be the eigenvalue of âˆ‡2 Q (x), then Eq.(16) can be
convert to

A PPENDIX
The pseudocode of CMFQ is listed below.
Algorithm 1 Causal Mean Field Q-learning
Input: Initialize state s0 ; QÎ¸i , QÎ¸âˆ’ , aÌŒi0 for all agent i âˆˆ
i
{1, 2, Â· Â· Â· , N}; trajectory length M;
while in the training loop do
for t = 0, 1, Â· Â· Â· , M do
for i = 1, 2, Â· Â· Â· , N do
i ) with average
Calculate policy Ï€ti (Â· | st , aÌ„tâˆ’1
merged agent;
Calculate causal effect for every neighborhood
agent by Eq.(7);
i
Obtain a new merged agent aÌŒtâˆ’1
and a new
i
i
policy Ï€t (Â· | st , aÌŒtâˆ’1 ) by Eq.(10);
end for
Sample joint action a = [a1 , a2 , Â· Â· Â· , aN ] from
1
2
[Ï€t , Ï€t , Â· Â· Â· , Ï€tN ]
obtain the next state st+1 and the reward r =
1 , aÌŒ2 , Â· Â· Â· , aÌŒN ];
[r1 , r2 , Â· Â· Â· , rN ] and merged agent aÌŒ = [aÌŒtâˆ’1
tâˆ’1
tâˆ’1
Store transition < st , a, r, st+1 , aÌŒ > in replay buffer;
end for
for i = 1, 2, Â· Â· Â· , N do
Sample a minibatch transition from replay buffer;
Calculate Li and update Î¸i by Eq.(12);
Updata target network by Î¸âˆ’
i = Î¸i after every C
updates of Î¸i ;
end for
end while
As s, ai in Qi (s, ai , ai,k ) are fixed parameter in the derivation
of Eq.(9), for simplicity, the pairwise Q-function Qi (s, ai , ai,k )
can be rewrite as Q(ak ) in the following. We assume that
ak is a one-hot encoding for n actions, to make Q(ak ) more
general, we replace the discrete ak (ak âˆˆ RN ) by a continuous
x (x âˆˆ RN ) which donâ€™t violate the domain of the parameterized Q-function. Given the Q (x) is L-smooth, then for any two
points x, y âˆˆ dom (Q) âŠ† RN , there exists a Lipschitz constant
L âˆˆ [0, +âˆ) that
âˆ¥âˆ‡Q(x) âˆ’ âˆ‡Q(y)âˆ¥2 < Lâˆ¥x âˆ’ yâˆ¥2

(14)

By the first order Taylor expansion with Lagrange remainder, we have
âˆ‡Q (y) = âˆ‡Q (x) + âˆ‡2 Q (x) Â· u + R (u)

(15)

R(u)
where u = y âˆ’ x, limuâ†’0 âˆ¥uâˆ¥
= 0. Assume x Ì¸= y, then we
2
can reform the first order Taylor expansion

âˆ¥âˆ‡2 Q(x) Â· uâˆ¥2
âˆ¥âˆ‡Q(y) âˆ’ âˆ‡Q(x) âˆ’ R(u)âˆ¥2
=
âˆ¥uâˆ¥2
âˆ¥uâˆ¥2
âˆ¥âˆ‡Q(y) âˆ’ âˆ‡Q(x)âˆ¥2 âˆ¥R(u)âˆ¥2
(16)
â‰¤
+
âˆ¥uâˆ¥2
âˆ¥uâˆ¥2
âˆ¥R(u)âˆ¥2
â‰¤ L+
, âˆ€x, y âˆˆ dom(Q), x Ì¸= y
âˆ¥uâˆ¥2

âˆ¥âˆ‡2 Q(x) Â· uâˆ¥2
âˆ¥Î»uâˆ¥2
âˆ¥R(u)âˆ¥2
=
=| Î» |â‰¤ L +
âˆ¥uâˆ¥2
âˆ¥uâˆ¥2
âˆ¥uâˆ¥2

(17)

Obviously, we can obtain the bound of Î», Î» âˆˆ [âˆ’L, L]. âˆ‡2 Q (x)
is a real symmetric matrix, so there exist an orthogonal

matrix U to diagonalize âˆ‡2 Q (x) such that U T âˆ‡2 Q (x) U =
Î› â‰œ diag [Î»1 , Î»2 , . . . , Î»N ]. Then the bound of Ris,ai (ai,k ) can be
derived as follow
 
1
Ris,ai (ai,k ) = Î´ai,k Â·âˆ‡2 Q ak Â· Î´ai,k
2
iT h
i
1h
= U Â· Î´ai,k Î› U Â· Î´ai,k
(18)
2
h
i
N
2
1
= âˆ‘ Î»n U Â· Î´ai,k
2 n=1
n
h
i2
N
âˆ’Lâˆ¥U Â· Î´ai,k âˆ¥2 â‰¤ âˆ‘ Î»n U Â· Î´ai,k â‰¤ Lâˆ¥U Â· Î´ai,k âˆ¥2
n

n=1

(19)



where U Â· Î´ai,k n refers to the nth element of vector U Â· Î´ai,k .
âˆ¥U Â· Î´ai,k âˆ¥2 = âˆ¥Î´ai,k âˆ¥2 = (ai,k âˆ’ aÌŒi )T (ai,k âˆ’ aÌŒi )
T

T

T

T

= ai,k ai,k + aÌŒi aÌŒi âˆ’ aÌŒi ai,k âˆ’ aÌŒi ai,k = 2(1 âˆ’ aÌŒin ) â‰¤ 2
(20)
where ai,k is a one-hot encoding action, aÌŒin denotes the nth
element in aÌŒi . Finally, according to Eq.(18) Eq.(19) Eq.(20),
the bound of Ris,ai (ai,k ) is [âˆ’L, L].
To further analyze the reasons why CMFQ is more effective
than Attention-MFQ empirically, we randomly select an agent
in the mixed cooperative-competitive game task and visualize
its weight. Some interesting observations can be made from
Fig.9(a). First of all, it makes sense that the agents on the
front line will be given high weights because they are battling.
Secondly, the weights of agents at the edge of the front line are
relatively small, possibly because these agents can cooperate
with nearby teammates to attack an enemy due to their position
advantages, so they are in a relatively dominant state. In
addition, agents at the very edge of the front line are given
higher weights, even if they are out of combat. This is because
they are in a position to flank their opponents and work with
their teammates to surround the opponents. In Fig.9(b), we
observe a result consistent with the analysis in our paper.
That is, the attention-based method uses the attributes of other
agents to calculate the attention scores, and observation is an
important part of the attributes, so it tends to give high weight
to the agents nearby because their observations are similar.
To further investigate the applicability of CMFQ, we perform an experiment on another environment named multiagent particle environment (MPE). As the dimensionality of
action-state space will change as the initial number of agent
changes, making it difficult to verify scalability, but we believe
that CMFQâ€™s scalability performance has been adequately
validated in previous experiments. For MPE, we tested the
predator prey task in MPE when the number of agents was

  

 

 $ W W H Q W L R Q  0 ) 4
 0 ) 4
 & 0 ) 4

 $ W W H Q W L R Q  0 ) 4
 0 ) 4
 & 0 ) 4

  

 $ Y H U D J H  5 H Z D U G  S H U  6 W H S

 $ Y H U D J H  5 H Z D U G  S H U  6 W H S

 

 

 

 

  
  
  
   
   
   

 

 

  

  

  

  

 ( S L V R G H

   

   

   

(a) Average reward of Predator.

 

  

  

  

  

 ( S L V R G H

   

   

   

(b) Average reward of Prey.

Fig. 10. Average reward during training.

(a) The weights obtained by CMFQ.

(b) The weights obtained by Attention-MFQ.
Fig. 9. The two figures visualize the mixed cooperative-competitive task,
where each agent in the blue team in (a) is controlled by CMFQ and each
agent in the blue team in (b) is controlled by Attention-MFQ. Each agent in
the red team is controlled by MFQ. We label the agents in the blue team whose
weights are visualized in green. The number above the blue agent represents
the normalized weight given by the green agent to the pairwise interaction
between them. Due to space constraints, the integer bits of all weights are
omitted

the same as that in the training environment, and compared it
with V-B to see whether the same conclusions could be drawn
in the two environments.
Task Setting. There are 20 predators, 40 preys, and 20
obstacles. Predator gets rcollide = 10 if it collide with prey.
Prey gets rbe collided = âˆ’10 if it collided with predator. The
speed of prey is 1.3 times of that of predator. In order to
make preys learn to leverage obstacles instead of running to
infinity, we manually draws an area. If preys go beyond this
area, they will get penalty rbound which will be aggravate as the
distance preys go beyond this area increase, until rbound = âˆ’10.
We trained MFQ, CMFQ and Attention-MFQ in the self-play
paradigm. The training curve is shown in Fig.10.
Quantitative Results and AnalysisIn the test phase, we

controlled 20 Predators and 40 prey with different algorithms
respectively, test 10 times and calculated the average reward
of each algorithm, as shown in Table.I. First, the average
reward of MFQ is lower than CMFQ and Attention-MFQ,
regardless of whether it controls predators or preys. This
indicates that the representational ability of average merged
agent is insufficient. Secondly, when MFQ controls prey, the
average predator reward of CMFQ is higher than AttentionMFQ, indicating that the weight obtained by CMFQ was more
representational. Finally, in the comparison between CMFQ
and Attention-MFQ, CMFQ outperforms Attention-MFQ in
both predator reward and prey reward, further confirms the
superiority of CMFQ. In the task that the number of agents in
testing was the same as that in the training, We compare the
performance of MFQ, CMFQ, and Attention-MFQ and come
to the same conclusion consistent with V-B, empirically certify
the applicability of CMFQ.
Predator
MFQ
CMFQ
MFQ
Attention-MFQ
CMFQ
Attention-MFQ

Predator reward
4.23
6.68
4.01
6.05
3.15
3.02

Prey
CMFQ
MFQ
Attention-MFQ
MFQ
Attention-MFQ
CMFQ

Prey reward
-8.64
-13.05
-8.47
-12.46
-11.07
-4.23

TABLE I
R ESULTS THAT LET TWO DIFFERENT ALGORITHM CONTROL PREDATORS
AND PREYS RESPECTIVELY. P REDATOR REWARD IS THE AVERAGE
REWARD A PREDATOR OBTAIN EVERY STEP. P REY REWARD IS THE
AVERAGE REWARD A PREY OBTAIN EVERY STEP.

