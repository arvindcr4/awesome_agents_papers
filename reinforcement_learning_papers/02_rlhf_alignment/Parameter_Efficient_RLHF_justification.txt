Paper: Parameter_Efficient_RLHF.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Parameter Efficient Reinforcement Learning from Human Feedback Hakim Sidahmed∗1 , Samrat Phatale∗1 , Alex Hutcheson2 , Zhuonan Lin2 , Zhang Chen2 , Zac Yu2 , Jarvis Jin2 , Simral Chaudhary1 , Roman Komarytsia2 , Christiane Ahlheim2 , Yonghao Zhu2 , Bowen Li2 , Saravanan Ganesh2 , Bill Byrne2 , Jessica Hoffmann1 , Hassan Mansoor1 , Wei Li1 , Abhinav Rastogi1 , Lucas Dixon1 1 Google DeepMind, 2 Google {hsidahmed, samratph, ldixon}@google.com  Abstract  arXiv:2403.10704v2 [cs.LG] 12 Sep 2024  While Reinforcement Learning from Human Feedback (RLHF) effectively aligns pretrained Large Language and ...

Specific Evidence for Classification:
- "Parameter Efficient Reinforcement Learning from Human Feedback Hakim Sidahmed∗1 , Samrat Phatale∗1 , Alex Hutcheson2 , Zhuonan Lin2 , Zhang Chen2 , Zac Yu2 , Jarvis Jin2 , Simral Chaudhary1 , Roman Komarytsia2 , Christiane Ahlheim2 , Yonghao Zhu2 , Bowen Li2 , Saravanan Ganesh2 , Bill Byrne2 , Jessica Hoffmann1 , Hassan Mansoor1 , Wei Li1 , Abhinav Rastogi1 , Lucas Dixon1 1 Google DeepMind, 2 Google {hsidahmed, samratph, ldixon}@google.com  Abstract  arXiv:2403.10704v2 [cs.LG] 12 Sep 2024  While Reinforcement Learning from Human Feedback (RLHF) effectively aligns pretrained Large Language and VisionLanguage Models (LLMs, and VLMs) with human preferences, its computational cost and complexity hamper its wider adoption."
- "To alleviate some of the computational burden of fine-tuning, parameter efficient methods, like LoRA (Hu et al., 2021) were introduced."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
