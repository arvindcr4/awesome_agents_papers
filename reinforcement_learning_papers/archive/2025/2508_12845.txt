CAMAR: Continuous Actions Multi-Agent Routing
Artem Pshenitsyn1,2 , Aleksandr Panov1,2 , Alexey Skrynnik1,2
1

arXiv:2508.12845v2 [cs.AI] 17 Nov 2025

CogAI Lab, Moscow, Russia
2
MIRAI, Moscow, Russia
pshenitsyn@cogailab.com, skrynnikalexey@gmail.com

Abstract
Multi-agent reinforcement learning (MARL) is a powerful
paradigm for solving cooperative and competitive decisionmaking problems. While many MARL benchmarks have been
proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly
for multi-agent pathfinding in environments with continuous
actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000
environment steps per second. We also propose a three-tier
evaluation protocol to better track algorithmic progress and
enable deeper analysis of performance. In addition, CAMAR
allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test
scenarios and benchmarking tools to ensure reproducibility
and fair comparison. Experiments show that CAMAR presents
a challenging and realistic testbed for the MARL community.
Code â€” https://github.com/AIRI-Institute/CAMAR.git

Introduction
Multi-agent reinforcement learning (MARL) has shown
strong results in cooperative and competitive settings, and
many recent studies explore how MARL can solve tasks that
need coordination in complex environments [1, 2, 3, 4, 5, 6].
One important group of such tasks is multi-agent pathfinding
(MAPF), where several agents must reach their goals without collisions. Classic MAPF is usually studied on discrete
grids, but real robots move in continuous space, follow dynamics, and must plan smooth paths. This continuous version
of MAPF is important for many domains such as warehouse
logistics, drone swarm coordination, and other systems where
many robots must move safely and efficiently.
Learnable methods have recently become effective for
MAPF and cooperative navigation [7, 5, 6, 4]. However, many
existing environments still use grid worlds or discrete actions
that do not match real robot behavior. In practice, robots must
plan and coordinate while avoiding both static and dynamic
obstacles, a long-standing problem in robotics [8, 9, 7, 10].
Many MARL environments simplify this problem too much
by using low-dimensional maps or unrealistic movement
models.

Goal

Agent

Agent

Goal

Figure 1: An example scenario from the proposed CAMAR
benchmark. Agents are represented as filled circles. Each
agent aims to reach its goal while avoiding collisions. The
small arrows for the red and green agents indicate segments
of paths generated by RRT*, providing guidance for the RL
algorithms.
High-fidelity simulators like Gazebo1 , Isaac Sim2 , AirSim [11], and Flightmare [12] offer realistic physics, but they
focus on robot control and perception, not on large-scale
MARL. These tools often run slower and cannot simulate
hundreds of agents at once. To study large-scale continuous
navigation, we need environments that combine high speed
with continuous dynamics, run efficiently on GPU, and allow
testing both learned and planning-based methods in a single
benchmark.
We identify three main gaps in current MARL environments. First, many use discrete action spaces that cannot
represent smooth motion [13, 14, 15]. Second, while some
environments do support continuous states and actions, they
do not scale to large numbers of agents and obstacles [16].
Third, other environments can scale but offer simple tasks
that do not require strong coordination or realistic navigation
skills [17, 18, 19].
1
2

https://gazebosim.org/home
https://developer.nvidia.com/isaac/sim

To bridge the gap between multi-robot systems and MARL
research, we introduce the CAMAR (Continuous Actions
Multi-Agent Routing) Benchmark. Specifically, we make the
following contributions:
â€¢ We introduce CAMAR, an extremely fast environment
with GPU acceleration support (using JAX), achieving
speeds exceeding 100,000 steps per second. It is designed
for multi-agent navigation and collision avoidance tasks
in continuous state and action spaces.
â€¢ We propose an evaluation protocol that includes both training and holdout task instances, as well as a suite of metrics
and performance indicators to assess agentsâ€™ generalization capabilities.
â€¢ We provide strong baselines for benchmarking, including state-of-the-art MARL algorithms and classical path
planning methods commonly used in robotics, and conduct an extensive experimental study to evaluate their
performance across diverse scenarios.

Related Work
To compare existing MARL environments, we evaluate them
in Table 1 across key features such as continuous control,
GPU support, scalability, and usability. An extended version
of the Table 1 with descriptions of comparison features and
detailed analysis of each environment are provided in the
Appendix H.
Prominent benchmarks include SMAC [20, 17, 19],
Jumanji [21], POGEMA [14, 15], MPE [22, 18], and
VMAS [16]. SMAC enables testing of strategic behavior
but uses discrete actions and scales poorly. Jumanji supports
GPUs and procedural generation but is not focused on navigation. POGEMA handles large-agent navigation with procedural maps but lacks continuous control. MPE, while foundational, does not scale to many agents; VMAS adds physical
realism but still struggles with performance and scalability.
Many environments lack evaluation protocols and suffer
from slow training due to CPU-GPU bottlenecks. Simulators like Gazebo [23], Webots [24], and ARGoS [25] offer
realistic continuous dynamics but are not optimized for efficient MARL training. These gaps highlight the need for a
new benchmark supporting scalable, high-performance multiagent learning.

CAMAR Environment
CAMAR is designed for continuous-space planning tasks
in multi-agent environments. In this environment, multiple
agents move toward their goals while avoiding both static obstacles and other moving agents. The simulation happens in a
fully continuous two-dimensional space, without any predefined grids. Agents interact by applying forces, which control
their movement through a simple and computationally efficient dynamic model. This approach makes the environment
more realistic and easier to scale for many agents.
Dynamic Model & Action Space A key part of CAMAR is
its collision model. Similar to MPE [22, 18] and VMAS [16],
CAMAR uses a force-based system. Agents receive repulsive
forces from nearby agents and obstacles. The collision force

applied to agent i from object j is calculated using a smooth
contact model, as shown in the equation below.
ï£±


âˆ’(âˆ¥âˆ†âƒ—
xij (t)âˆ¥âˆ’dmin )
ï£´
âˆ†âƒ—
xij (t)
collision
ï£´
âƒ—
k
fij
(t) = f0 âˆ¥âˆ†âƒ—xij (t)âˆ¥ k log 1 + e
,
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²if âˆ¥âˆ†âƒ—x (t)âˆ¥ < d ;
ij
min
ï£´
ï£´
ï£´
collision
ï£´
ï£´
fâƒ—ij
(t) = 0,
ï£´
ï£´
ï£³otherwise.
Here, contact force f0 regulates the magnitude of the repulsive force, penetration softness k controls the smoothness
of the contact dynamics, âˆ†âƒ—xij (t) is a displacement vector
between agent i and an object j at time t, dmin defines the
minimum allowable distance before collision is activated.
When two objects overlap, the force grows smoothly without sudden changes. When there is no overlap, the collision
force is zero. This smooth behavior helps keep agent movement more realistic and stable. The total collision force acting
on agent i is calculated by summing forces from all nearby
P collision
objects: fâƒ—ic (t) = j fâƒ—ij
(t).
The full environment state is updated using the collision
force and agentsâ€™ actions. CAMAR supports multiple types of
dynamic models. Currently, we provide two built-in models:
HolonomicDynamic and DiffDriveDynamic.
HolonomicDynamic This model is simple and similar
to the one used in MPE [22, 18]. Each agent has a position
and velocity. The agent moves by applying a 2D force. The
next state is calculated using the semi-implicit Euler method,
as described in the equation below.
ï£±
fâƒ—a (t)+fâƒ—c (t)
ï£´
ï£´
âƒ—vi (t + dt) = (1
âˆ’ damping)âƒ—vi (t) + i m i dt
ï£´
(
ï£´
ï£²
âƒ—vi (t + dt), if âˆ¥âƒ—vi (t + dt)âˆ¥ < max_speed
âƒ—vi (t + dt) :=
âƒ—
vi (t+dt)
ï£´
ï£´
âˆ¥âƒ—
vi (t+dt)âˆ¥ Â· max_speed, otherwise.
ï£´
ï£´
ï£³pos
âƒ— i (t + dt) = pos
âƒ— i (t) + âƒ—vi (t + dt)dt
Here, fâƒ—ia (t) is the 2D action force of agent i, damping
- scalar in the range [0, 1) that controls velocity decay over
time, m is the agent mass for applying forces, max_speed
regulates maximum speed of an agent preserving direction
but limiting speed, dt is the time step duration between updates.
DiffDriveDynamic Differential-drive robot model is another built-in dynamic in CAMAR. Each agent has a 2D poâƒ— i (t) and a heading angle Î¸i (t). The agent chooses
sition pos
a 2D action: one value for linear speed uai (t) and one for
angular speed Ï‰ia (t). The motion is updated based on this
action using equation below.
ï£±
ui (t) = clip(uai (t), âˆ’max_u, max_u)
ï£´
ï£´
ï£´
a
ï£´
ï£´
ï£²Ï‰i (t) = clip(Ï‰i (t), âˆ’max_w, max_w)
âƒ—vi (t) = [ui (t) cos(Î¸i (t)); ui (t) sin(Î¸i (t)] +
ï£´
ï£´
ï£´
âƒ— (t + dt) = pos
âƒ— i (t) + âƒ—vi (t)dt
pos
ï£´
ï£´
ï£³ i
Î¸i (t + dt) = Î¸i (t) + Ï‰i (t)dt

fâƒ—ic (t)
m dt

ry
Co
nt
.O
bs
Co
er
nt
. A vati
on
GP ctio
s
ns
U
Su
p
Sc
ala port
bi
lit
Pa
rti y >5
al l
00
Ag
He y ob
en
se
ter
r
ts
og
va
b
e
Pe
ne
le
rfo
o
rm us a
ge
an
nt
ce
s
>1
Py
0K
th
SP
on
S
ba
Pr
se
oc
d
ed
Re ural
g
qu
ire ene
rat
sg
io
en
era n
Ev
liz
at i
alu
on
ati
on
Te
sts
pr
ot
&
CI ocol
Py
s
PI
Li
s te
d

Re

po
sit
o

Environment / Simulator
RWare (Jumanji) [21]
SMAC [20]
SMACv2 [17]
SMAX (JaxMARL) [19]
MPE [22, 18]
MPE (JaxMARL) [19]
POGEMA [14]
VMAS 4 [16]

link
link
link
link
link
link
link
link

âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“

âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ—
âœ“

âœ“
âœ—
âœ—
âœ“
âœ—
âœ“
âœ—
âœ“

âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ—

âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“

âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“

âœ—
âœ—
âœ—
âœ“
âœ—/âœ“3
âœ—/âœ“3
âœ“
âœ—/âœ“4

âœ“
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“

âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ—

âœ“
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—/âœ“4

âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ—

âœ“
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“

âœ“
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“

Gazebo [23]
Webots [24]
ARGoS [25]

link
link
link

âœ“
âœ“
âœ“

âœ“
âœ“
âœ“

âœ“
âœ“
âœ—

âœ—
âœ—
âœ“

âœ“
âœ“
âœ“

âœ“
âœ“
âœ“

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ“
âœ“
âœ“

âœ—
âœ—
âœ—

CAMAR (Ours)

link

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Table 1: Comparison of multi-agent reinforcement learning (MARL) environments and simulators. Each row corresponds to a
specific environment or a particular implementation of it. The columns indicate key properties, including support for continuous
observations and actions, GPU acceleration, scalability beyond 500 agents, partial observability, heterogeneous agents, and
whether the simulator can exceed 10K simulation steps per second (SPS). Additional columns specify if the environment is
implemented fully in Python, supports procedural generation, requires generalization across different maps or tasks, includes
evaluation protocols, and provides built-in tests or continuous integration (CI). The â€œRepositoryâ€ column contains links to the
official source code for each environment. CAMAR, listed at the bottom, is our proposed environment.
Here, max_u and max_w are constraints on agent velocities.

agent

obs_2

agent
window

Observations Each agent in CAMAR receives a local observation centered around itself. The size of the observation
window can be set by the user. This observation system is
inspired by LIDAR sensors but avoids using ray tracing. Instead, CAMAR provides a simple and efficient vector-based
observation.
Each agent observes nearby objects using a penetrationbased vector representation, which ensures smooth and continuous observations. For every object in the environment
(either an agent or a static landmark), the observation is computed as a normalized vector pointing from the agent to the
object. If the object is far away, outside the agentâ€™s sensing window, the observation becomes a zero vector. This
method avoids discontinuities and helps agents better generalize across different object sizes.

3

SPS decreases gracefully with many agents and obstacles.
VMAS is a framework consisting of many different scenarios,
while some scenarios run efficiently with a speed exceeding 10K
SPS, other, complex ones donâ€™t; the same applies for generalization.
4

obs_1
obs_3

obstacle

obs_1
obs_2
observation = obs_3
0
â€¦

agent
obstacle

Figure 3: LIDAR-inspired vector observations in CAMAR.
Each agent detects nearby objects using penetration vectors,
and receives a normalized goal direction.

ï£±
âƒ— 
âˆ†âƒ—oj = âƒ—oj âˆ’ pos
ï£´
ï£´

ï£±
ï£´
ï£´
window+Rj
ï£´
ï£´
âˆ†âƒ—
o
Â·
1
âˆ’
,
ï£´
j
ï£´
ï£´
âˆ¥âƒ—
oj âˆ¥
ï£´
ï£´
ï£²
ï£²
âƒ— = if âˆ¥âˆ†âƒ—oj âˆ¥ âˆ’ R âˆ’ Rj < window
obs_j
ï£´
âƒ—0,
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ï£´
ï£´
otherwise.
ï£´
ï£´
ï£´
âƒ—
ï£³ âƒ—
obs_j
obs_j := window
This observation is computed for each agent in a fully
vectorized manner. Afterward, only the top max_obs closest
objects are kept to form the final observation vector. Here, âƒ—oj
is the 2D position of object j, R is the agent radius, Rj the

radius of object j, window is a parameter that sets how far
the agent can sense objects nearby.
In addition to obstacle information, each agent also gets an
ego-centric vector pointing to its goal. This vector is clipped,
normalized and concatenated to the final observation. This
structure helps agents understand both their surroundings and
the direction they need to move.
Circle-Based Discretization One simple way to simulate
a world is to use geometry rules to check if objects overlap.
Ray tracing is a common method for detecting collisions
based on the shapes of objects. However, ray tracing is hard
to implement in a way that is fast on a GPU. Simple versions
of ray tracing are slow and better suited for CPU simulations.
Another method is to discretize the world and only check
collisions with nearby objects. Based on the dynamic model
described above, it is enough to calculate the distance between objects.
In CAMAR, every object is represented as a circle. This
choice has several advantages. Checking the distance between
two circles is simple and fast. It does not require special cases
like ray tracing does. It also avoids the complexity of handling
different shapes like rectangles or polygons. Because of this,
the simulation can easily run on GPUs with many agents at
the same time. This design allows CAMAR to simulate largescale multi-agent tasks efficiently and with high performance.
Map Generators Although every object in CAMAR is
represented as a circle, it is still possible to create complex
and detailed maps. A complex structure can be made by
combining many smaller circles close together. The more
circles that are used, the more accurate the map becomes.
This method allows users to simulate walls, tunnels, mazes,
and other complicated shapes even though the basic element
is always a circle.
CAMAR includes several types of built-in maps. It also
gives users the ability to add custom map generators, even if
they are not compatible with JAX just-in-time compilation
[27]. A custom map generator can be connected easily by
using the string_grid or batched_string_grid formats.
This flexibility allows users to design many kinds of environments, from simple random setups to complex and realistic
maps.
The current set of built-in maps and generators includes
(see Fig. 4):
4a random_grid: A map where obstacles, agents, and
goals are placed randomly on a grid with a predefined
size.
4b labmaze_grid: Maps generated using LabMaze5 [28]
- maze generator with connected rooms.
4c movingai: Integrated two-dimensional maps from
the MovingAI benchmark [26], adapted for continuous planning tasks. They can also be used in a
batched_string_grid manner.
4d caves_cont: A continuous type of map where caves
are generated using Perlin noise, a common method in
video games for creating realistic and varied landscapes.
5

https://github.com/google-deepmind/labmaze

4e string_grid: A grid map based on a text layout. Obstacles are placed according to characters in a string, similar to the MovingAI benchmark [26]. Agent and goal
positions can be fixed or random, depending on the free
cells in the string.
4f batched_string_grid:
Similar
to
string_grid, but supports different obstacle
layouts across parallel environments. This allows training
on multiple map variations at once.
Reward Function CAMAR uses a scalar reward for each
agent at every time step. This reward is the sum of four terms:
a goal reward, a collision penalty, a movement-based reward,
and a collective success reward:
ri (t) = rall_g (t) + ron_gi (t) + rcollisioni (t) + rg_disti (t)
The terms are defined as follows:
ï£±
ï£´
rall_g (t) = +0.5, if âˆ€i : âƒ—xi (t) âˆ’ âƒ—xgi âˆ¥ â‰¤ Rg ;
ï£´
ï£´
ï£´
ï£´
ï£²ron_gi (t) = +0.5, if âˆ¥âƒ—xi (t) âˆ’ âƒ—xgi âˆ¥ â‰¤ Rg ;
rcollisioni (t) = âˆ’1, if âˆƒj : âˆ¥âˆ†âƒ—xij (t)âˆ¥ < dmin ;
ï£´
ï£´
ï£´
rg_disti (t) = +shapingÂ·
ï£´

ï£´
ï£³
Â· âˆ¥âƒ—xi (t âˆ’ dt) âˆ’ âƒ—xgi âˆ¥ âˆ’ âˆ¥âƒ—xi (t) âˆ’ âƒ—xgi âˆ¥
Here, âƒ—xi (t) is the position of agent i at time t, âƒ—xgi is the
goal position for agent i, Rg is the distance threshold to
count as reaching the goal (goal radius), âˆ†âƒ—xij (t) is the vector
between agent i and another object j, dmin is the minimal
distance between agent i and object j (for circle objects
dmin = Ri + Rj where Ri and Rj are radii), shaping is
a user-defined coefficient that controls the strength of the
movement-based term.
To support cooperation, the environment gives an extra
reward when all agents reach their goals. In this case, each
agent receives an additional reward of +0.5.
Heterogeneous agents Additionally, CAMAR supports
heterogeneous agents in both size and dynamics. All map
generators can produce agents with different properties, making it possible to study diverse multi-agent systems inspired
by real-world scenarios.
For
example,
some
agents
can
use
HolonomicDynamic,
while
others
follow
DiffDriveDynamic. These agents operate together in
a shared space, interact with the same obstacles, and must
coordinate their movements despite having different control
rules (Fig. 5). Each agent follows its own dynamics model,
but all agents contribute to a single global simulation.
CAMAR also supports agents with different sizes. Each
agent can have its own radius, which affects how it moves
and avoids collisions. This adds complexity to coordination
and planning.
Metrics The evaluation P
metrics are defined as follows: the
N
success rate is SR = N1 i=1 1{âˆ¥âƒ—xi (T ) âˆ’ âƒ—xgoal âˆ¥ â‰¤ Rg };
P
N
the flowtime is FT = N1 i=1 ti ; the makespan is MS =
maxi=1,...,N ti ; and the coordination is CO = 1 âˆ’ NC
Ã—T .
Here, N is the number of agents, T is the maximum episode

(a) random_grid (b) labmaze_grid

(c) movingai

(d) caves_cont

(e) string_grid (f) batched_string_grid

Figure 4: A rich collection of maps for multi-agent planning in continuous spaces in CAMAR: support for both continuous and
grid landscapes together with MovingAI collection [26].
agent

ğ’—ğ’Š ğ’• + ğ’…ğ’• = ğŸ âˆ’ ğ’…ğ’‚ğ’ğ’‘ğ’Šğ’ğ’ˆ ğ’—ğ’Š ğ’• +

ğ’‡ğ’Šğ’‚ (ğ’•) + ğ’‡ğ’Šğ’„ (ğ’•)
ğ’…ğ’•
ğ’

ğ’‘ğ’ğ’”ğ’Š ğ’• + ğ’…ğ’• = ğ’‘ğ’ğ’”ğ’Š ğ’• + ğ’—ğ’Š ğ’• + ğ’…ğ’• ğ’…ğ’•

obstacle
agent
agent

include detailed training and evaluation scripts and provide
all protocol maps in the public CAMAR repository.
These protocols help the community better track progress
on continuous multi-agent pathfinding and identify which algorithms generalize well to more realistic conditions. Sample
efficiency curves, metric-vs-agent-count plots, and performance profiles are supported and recommended for deeper
analysis.

ğ’—ğ’Š ğ’• = [ğ’–ğ’Šğ’‚ ğ’• ğœğ¨ğ¬ ğœ½ğ’Š ğ’• ; ğ’–ğ’Šğ’‚ ğ’• ğ¬ğ¢ğ§(ğœ½ğ’Š ğ’• )
ğ’‘ğ’ğ’”ğ’Š ğ’• + ğ’…ğ’• = ğ’‘ğ’ğ’”ğ’Š ğ’• + ğ’—ğ’Š ğ’• ğ’…ğ’•
ğœ½ğ’Š ğ’• + ğ’…ğ’• = ğœ½ğ’Š ğ’• + ğğ’‚ğ’Š ğ’• ğ’…ğ’•

agent
obstacle

Figure 5: Illustration of heterogeneous agents with different sizes and dynamics supported by CAMAR. Blue agents
are governed by HolonomicDynamic, while green agents
follow DiffDriveDynamic. All agents navigate a shared
environment while avoiding gray obstacles.

length, ti is the time step when agent i first reaches its goal,
ti = T for unfinished agents, C is the total per-agent-pertimestep number of collisions over the episode.
Evaluation Protocols To support rigorous and reproducible benchmarking, CAMAR includes a standardized
suite of evaluation protocols inspired by and extending prior
work on cooperative MARL evaluation [29]. We adapt that
framework for continuous multi-agent pathfinding, focusing
on generalization across both agent count and map structure.
We propose three evaluation tiers: Easy, Medium, and
Hard, each targeting a different level of generalization. Easy
evaluates performance on unseen start and goal positions
using the same map type and number of agents as during
training. Medium tests generalization to maps with similar structure but a different number of agents and obstacle
parameters. Hard measures generalization to fully unseen
map types from the MovingAI street collection, often with a
different number of agents than during training.
Each tier follows a defined training and evaluation setup
using introduced metrics, aggregated by the Interquartile
Mean (IQM) with 95% confidence intervals (CI95 ) for fair
comparison across methods and difficulty levels.
All experiments use fixed JAX [27] random seeds for
reproducibility. In total, each protocol involves training multiple models and running thousands of evaluation episodes. We

Experimental Evaluation
In this section, we evaluate both the scalability and benchmarking capabilities of our environment. We begin by training and testing a set of popular MARL algorithms, as well
as classical non-learnable and hybrid methods. These experiments show that the environment supports a wide range
of navigation and coordination strategies. We also present
results from a simple heterogeneous-agent scenario to demonstrate support for heterogeneous MARL research. Finally, we
measure the performance of the simulator in terms of simulation speed, and compare it with VMAS using a shared
experimental setup.

Experimental Setup
We evaluate 6 MARL algorithms: IPPO, MAPPO, IDDPG,
MADDPG, ISAC [30], and MASAC. In addition, we include
2 non-learnable baselines, RRT+PD and RRT*+PD, and 6 hybrid methods: RRT*+IPPO, RRT*+MAPPO, RRT*+IDDPG,
RRT+MADDPG, RRT*+ISAC, and RRT*+MASAC.
All methods are evaluated on two procedurally generated
map types: random_grid and labmaze_grid, each
with 6 versions that vary in obstacle density and agent count
(8 or 32)6 . For labmaze_grid, an additional connection
probability ranging from 0.4 to 1.0 is used to test different
maze complexities. Generation details for all training and
evaluation maps are provided in Appendix D.
The â€œindependentâ€ variants (IPPO [1], IDDPG, ISAC)
train each agent using its own policy and value function.
These methods do not use centralized critics or information
sharing across agents. In contrast, the multi-agent versions
(MAPPO, MADDPG, MASAC) use centralized critics during
training to improve coordination. All approaches use parame6

Our current Medium-tier protocol includes tasks with 8 and 32
agents, but we plan to extend it in future versions to support larger
agent populations and more complex settings as methods advance.

random_grid

labmaze_grid

Algorithm

SR â†‘

FT â†“

MS â†“

CO â†‘

SR â†‘

FT â†“

MS â†“

CO â†‘

IPPO
MAPPO
IDDPG
MADDPG
ISAC
MASAC

0.410Â±0.001
0.830Â±0.001
0.335Â±0.001
0.041Â±0.000
0.115Â±0.001
0.281Â±0.001

1695Â±10
984Â±5
1851Â±10
2508Â±12
2523Â±14
1843Â±11

160.0Â±0.0
151.4Â±0.3
160.0Â±0.0
160.0Â±0.0
160.0Â±0.0
160.0Â±0.0

1.000Â±0.000
1.000Â±0.000
1.000Â±0.000
0.913Â±0.001
1.000Â±0.000
0.856Â±0.001

0.213Â±0.013
0.568Â±0.004
0.167Â±0.000
0.027Â±0.000
0.047Â±0.000
0.105Â±0.001

2104Â±14
1484Â±8
2772Â±14
2745Â±12
2808Â±12
2098Â±12

160.0Â±0.0
160.0Â±0.0
160.0Â±0.0
160.0Â±0.0
160.0Â±0.0
160.0Â±0.0

1.000Â±0.000
1.000Â±0.000
0.996Â±0.000
0.854Â±0.001
1.000Â±0.000
0.781Â±0.001

RRT*+IPPO
RRT*+MAPPO
RRT*+IDDPG
RRT*+MADDPG
RRT*+ISAC
RRT*+MASAC

0.420Â±0.001
0.828Â±0.001
0.280Â±0.001
0.037Â±0.000
0.143Â±0.000
0.054Â±0.000

1426Â±9
971Â±5
2181Â±12
2953Â±15
2618Â±13
2511Â±14

160.0Â±0.0
150.4Â±0.3
160.0Â±0.0
160.1Â±0.0
160.0Â±0.0
160.0Â±0.0

1.000Â±0.000
1.000Â±0.000
1.000Â±0.000
0.984Â±0.000
1.000Â±0.000
1.000Â±0.000

0.511Â±0.001
0.556Â±0.001
0.189Â±0.000
0.037Â±0.000
0.058Â±0.000
0.034Â±0.000

1316Â±6
1326Â±7
2635Â±14
2918Â±14
2749Â±13
2854Â±15

160.0Â±0.0
160.0Â±0.0
160.0Â±0.0
160.1Â±0.0
160.0Â±0.0
160.0Â±0.0

0.999Â±0.000
0.999Â±0.000
0.997Â±0.000
0.969Â±0.000
1.000Â±0.000
0.994Â±0.000

RRT*+PD
RRT+PD

0.678Â±0.002
0.413Â±0.014

2010Â±59
2440Â±264

160.0Â±0.0
160.0Â±0.0

0.997Â±0.000
0.788Â±0.021

0.692Â±0.004
0.528Â±0.021

1807Â±49
2049Â±251

160.0Â±0.0
160.0Â±0.0

0.971Â±0.002
0.558Â±0.025

Table 2: Extended performance comparison across different algorithms in the random_grid and labmaze_grid environments including integrated RRT* with off-policy algorithms additionally. Reported metrics are SR (Success Rate), FT (Flowtime),
MS (Makespan), and CO (Coordination), each shown as IQMÂ±CI95 . Confidence intervals are symmetric for clarity and computed
using 1K bootstrapped samples. Arrows indicate the direction of improvement: â†‘ denotes higher is better, â†“ indicates lower is
better. Tan boxes highlight the best-performing approach. The CO metric is not colored here for visibility.
ter sharing, meaning that agents use the same neural network
weights.
Each algorithm is trained for 20M (IPPO, MAPPO) and
2M (IDDPG, MADDPG, ISAC, MASAC) steps per scenario.
The training is done independently for each of the 12 map
variations. After training, we evaluate the models on both
seen and unseen tasks to test their generalization. In total, we
train 532 models and evaluate them across 5184 tasks, with
1000 episodes per task. The experiments were run on a single
NVIDIA H100 GPU and took around 1000 hours in total.
For the non-learning baselines, we use RRT+PD and
RRT*+PD. These methods use classical planning algorithms
to generate a path to the goal for each agent. Each path is
generated using either RRT (with 50,000 iterations) [31] or
RRT* (with 3000 iterations). The agent then follows the path
using a simple PD controller.
We also evaluate hybrid methods where agents receive
additional RRT* information during training and evaluation.
At the start of each episode, RRT* generates sample paths
from the goal to the agentâ€™s position. These paths and their
estimated costs are included in the agentâ€™s observation, enabling the policy to learn from approximate cost-to-go values
without invoking RRT* at every step.
To evaluate simulator performance, we measure simulation
speed in steps per second (SPS) on a 20Ã—20 random_grid
map with 0.3 obstacle density (120 obstacles). We vary the
number of agents and parallel environments to assess CAMARâ€™s scalability. For fair comparison, we benchmark CAMAR against VMAS [16] using identical map size, agent
count, and a single NVIDIA H100 GPU.

Benchmark
The main results are shown in Table 2. On the
random_grid map, MAPPO reaches the highest success

rate with strong coordination. Adding planning improves efficiency. RRT*+MAPPO gives faster routes than MAPPO,
and RRT*+IPPO improves over IPPO. The classic RRT*+PD
baseline reaches high success without learning, but shows
low coordination because it plans for each agent alone.
For off-policy algorithms, results are mixed. RRT* improves ISAC and IDDPG slightly, but weakens MASAC and
MADDPG. These methods use a centralized critic and must
process long input vectors that include RRT* features. This
makes training unstable and harms generalization. Among
MARL baselines, IPPO and IDDPG reach good success but
slower flowtime than MAPPO. MASAC and MADDPG fail
in both maps.
The labmaze_grid setting is harder due to narrow corridors and sparse rewards. All MARL baselines drop in success. RRT*+PD performs best, which shows the value of
full-path planning when learning signals are weak. The simpler RRT+PD planner gives worse paths but still beats many
MARL baselines in success.
More detailed analysis of these results and hybrid methods
is given in Appendix D.

Heterogeneous Agents
To demonstrate support for heterogeneous teams, we extend
the give_way task so that two agents must pass through a
narrow corridor where only the smaller red agent can enter
the central chamber. The larger blue agent must wait.
We compare IPPO and MAPPO with shared policies to
their heterogeneous versions, where each agent has its own
model. As shown in Fig. 6b, HetIPPO performs better, but
HetMAPPO fails, likely because the centralized critic cannot
handle the larger and more diverse input space.
This experiment shows that CAMAR can model agents
with different sizes and abilities and is suitable for studying

50K
CAMAR
VMAS

2.7K

32

4
16

25000
3000
4000
5000
6000

200

562K

161K

4160 obstacles

Steps per Second

245K
133K
68K
36K
23K
12K
6K
2800

32 agents

105K
71K
55K
46K
36K
28K
22K
18K

Number of obstacles

992

2240
3520
4800
6080
7360
8640

800

Number of Agents

0

15420

1398
100
200
300
400
500
600
700

We study how CAMAR scales when we increase the number of parallel environments, agents, and obstacles. The full
results are shown in Fig. 7. Visual examples of CAMAR
and VMAS running the same scenario are provided in the
appendix E.
We first fix the number of agents at 32 and measure the
simulation speed when we increase the number of parallel environments from 5 to 6000+. VMAS scales roughly linearly
but stays below 10 000 steps per second (SPS) at 6000 environments. In contrast, CAMAR rises quickly to about 1000
environments and then stays close to 50 000 SPS even as we
add more parallel environments. This shows that CAMAR
can support fast and stable training with many vectorized
environments.
Next, we fix the number of environments at 2000 and
increase the number of agents from 4 to 128. CAMAR keeps
more than 100 000 SPS when the agent count is below 16
and remains above 10 000 SPS even at 128 agents. VMAS
begins near 20 000 SPS with 4 agents but drops to about 500
SPS at 128 agents. In this setting, CAMAR is up to 20 times
faster when many agents act together.
We also test extreme conditions by keeping 4160 fixed and
increasing the number of agents up to 800. The speed goes
down as more agents are added, but CAMAR still maintains
about 1400 SPS at 800 agents. Since every agent produces
one observation per step, the total amount of data remains
high.
Finally, we test how obstacle count alone affects speed.
We fix the number of agents at 32 and increase the number
of obstacles up to 9920. CAMAR still reaches about 15 000
SPS in this most cluttered setting. This shows that the system
remains robust even in very dense maps.
In summary, CAMAR achieves around 50 000 SPS on
complex scenarios with many obstacles and 32 agents. The
speed depends mainly on the number of objects in the scene.
In other tasks it reaches up to 161 000 SPS with 32 agents
and 960 obstacles, and up to 562 000 SPS when simulating a
single agent with 4160 obstacles. These results confirm that
CAMAR can operate above 100 000 SPS. Visualisations of
the test scenes are shown in the appendix E. Further comparisons with other MARL environments are also given in the
appendix E.

Number of Agents

(a) CAMAR vs VMAS with increasing number of environments,
agents, and obstacles.

1

Scalability Analysis

35K
20K

Number of Parallel Environments

Steps per Second

coordination in heterogeneous teams. More details are in
Appendix D.

50K

2.7K

5
5
100000

Figure 6: Example of heterogeneous agent coordination (a).
Success rates of algorithms are shown in (b).

CAMAR
VMAS

960

(b) SR results.

40K
30K
20K
10K

110K
95K
80K
65K

128

0.5
0.7
0.5
0.0

64

60K

IPPO
HetIPPO
MAPPO
HetMAPPO

Steps per Second

SR
Steps per Second

(a) hetero_give_way scenario

Algorithm

(b) CAMAR handles up to 800 agents, maintaining 1400 SPS and
high observation throughput.

Figure 7: CAMAR achieves robust scalability in multi-agent
simulation, surpassing VMAS in performance across varied
settings and supporting efficient simulation of large agent
populations with high observation rates.

Acknowledgments
The study was supported by the Ministry of Economic Development of the Russian Federation (agreement No. 139-15-2025-013, dated June 20, 2025, IGK
000000C313925P4B0002).

Conclusion
This paper introduces CAMAR, a high-performance benchmark for continuous-space multi-agent reinforcement learning. CAMAR combines realistic dynamics with efficient
simulation, supporting over 100 000 steps per second using
JAX [27]. It includes a diverse set of navigation tasks, a
standardized evaluation protocol with built-in metrics, and a
range of strong baselines from both classical, learning-based,
and hybrid methods. These components enable reliable, scalable, and reproducible evaluation of MARL algorithms.

References
[1] Christian Schroeder De Witt, Tarun Gupta, Denys
Makoviichuk, Viktor Makoviychuk, Philip HS Torr,
Mingfei Sun, and Shimon Whiteson. Is independent
learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.
[2] Matteo Bettini, Ajay Shankar, and Amanda Prorok. Heterogeneous multi-robot reinforcement learning. In AAMAS, 2023.

[3] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao,
Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative multi-agent games.
Advances in neural information processing systems,
35:24611â€“24624, 2022.
[4] Mehul Damani, Zhiyao Luo, Emerson Wenzel, and
Guillaume Sartoretti. Primal _2: Pathfinding via reinforcement and imitation multi-agent learning-lifelong.
IEEE Robotics and Automation Letters, 6(2):2666â€“
2673, 2021.
[5] Alexey Skrynnik, Anton Andreychuk, Maria Nesterova,
Konstantin Yakovlev, and Aleksandr Panov. Learn to
follow: Decentralized lifelong multi-agent pathfinding
via planning and learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 38, pages
17541â€“17549, 2024.
[6] Anton Andreychuk, Konstantin Yakovlev, Aleksandr
Panov, and Alexey Skrynnik. Mapf-gpt: Imitation learning for multi-agent pathfinding at scale. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 39, pages 23126â€“23134, 2025.
[7] Brian Angulo, Aleksandr Panov, and Konstantin
Yakovlev. Policy optimization to learn adaptive motion primitives in path planning with dynamic obstacles.
IEEE Robotics and Automation Letters, 8(2):824â€“831,
2022.
[8] Weinan Chen, Wenzheng Chi, Sehua Ji, Hanjing Ye,
Jie Liu, Yunjie Jia, Jiajie Yu, and Jiyu Cheng. A survey of autonomous robots and multi-robot navigation:
Perception, planning and collaboration. Biomimetic
Intelligence and Robotics, page 100203, 2024.
[9] Binyu Wang, Zhe Liu, Qingbiao Li, and Amanda Prorok. Mobile robot path planning in dynamic environments through globally guided reinforcement learning. IEEE Robotics and Automation Letters, 5(4):6932â€“
6939, 2020.
[10] Vassilissa Lehoux-Lebacque, Tomi Silander, Christelle
Loiodice, Seungjoon Lee, Albert Wang, and Sofia
Michel. Multi-agent path finding with real robot dynamics and interdependent tasks for automated warehouses.
In ECAI 2024, pages 4393â€“4401. IOS Press, 2024.
[11] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish
Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and service
robotics: Results of the 11th international conference,
pages 621â€“635. Springer, 2017.
[12] Yunlong Song, Selim Naji, Elia Kaufmann, Antonio
Loquercio, and Davide Scaramuzza. Flightmare: A
flexible quadrotor simulator. In Conference on Robot
Learning, pages 1147â€“1157. PMLR, 2021.
[13] Georgios Papoudakis, Filippos Christianos, Lukas
SchÃ¤fer, and Stefano V Albrecht. Benchmarking multiagent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint arXiv:2006.07869, 2020.
[14] Alexey Skrynnik, Anton Andreychuk, Konstantin
Yakovlev, and Aleksandr I Panov. Pogema: partially

observable grid environment for multiple agents. arXiv
preprint arXiv:2206.10944, 2022.
[15] Alexey Skrynnik, Anton Andreychuk, Anatolii
Borzilov, Alexander Chernyavskiy, Konstantin
Yakovlev, and Aleksandr Panov. Pogema: A benchmark
platform for cooperative multi-agent pathfinding. In
The Thirteenth International Conference on Learning
Representations, 2025.
[16] Matteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and
Amanda Prorok. Vmas: A vectorized multi-agent simulator for collective robot learning. In International
Symposium on Distributed Autonomous Robotic Systems, pages 42â€“56. Springer, 2022.
[17] Benjamin Ellis, Jonathan Cook, Skander Moalla,
Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan,
Jakob Foerster, and Shimon Whiteson. Smacv2: An
improved benchmark for cooperative multi-agent reinforcement learning. Advances in Neural Information
Processing Systems, 36:37567â€“37593, 2023.
[18] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI
Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments.
Advances in neural information processing systems, 30,
2017.
[19] Alexander Rutherford, Benjamin Ellis, Matteo Gallici,
Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt,
Alexandra Souly, et al. Jaxmarl: Multi-agent rl environments in jax. arXiv preprint arXiv:2311.10090, 2023.
[20] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS
Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge.
arXiv preprint
arXiv:1902.04043, 2019.
[21] ClÃ©ment Bonnet, Daniel Luo, Donal Byrne, Shikha
Surana, Sasha Abramowitz, Paul Duckworth, Vincent
Coyette, Laurence I Midgley, Elshadai Tegegn, Tristan Kalloniatis, et al. Jumanji: a diverse suite of scalable reinforcement learning environments in jax. arXiv
preprint arXiv:2306.09884, 2023.
[22] Igor Mordatch and Pieter Abbeel. Emergence of
grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908, 2017.
[23] Nathan Koenig and Andrew Howard. Design and use
paradigms for gazebo, an open-source multi-robot simulator. In 2004 IEEE/RSJ international conference on
intelligent robots and systems (IROS)(IEEE Cat. No.
04CH37566), volume 3, pages 2149â€“2154. Ieee, 2004.
[24] Olivier Michel. Cyberbotics ltd. webotsâ„¢: professional
mobile robot simulation. International Journal of Advanced Robotic Systems, 1(1):5, 2004.
[25] Carlo Pinciroli, Vito Trianni, Rehan Oâ€™Grady, Giovanni Pini, Arne Brutschy, Manuele Brambilla, Nithin
Mathews, Eliseo Ferrante, Gianni Di Caro, Frederick

Ducatelle, et al. Argos: a modular, parallel, multiengine simulator for multi-robot systems. Swarm intelligence, 6:271â€“295, 2012.
[26] Nathan R Sturtevant. Benchmarks for grid-based
pathfinding. IEEE Transactions on Computational Intelligence and AI in Games, 4(2):144â€“148, 2012.
[27] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas,
Skye Wanderman-Milne, et al. Jax: composable
transformations of python+ numpy programs. GitHub
repository, 2018.
[28] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom
Ward, Marcus Wainwright, Heinrich KÃ¼ttler, Andrew
Lefrancq, Simon Green, VÃ­ctor ValdÃ©s, Amir Sadik,
et al. Deepmind lab. arXiv preprint arXiv:1612.03801,
2016.
[29] Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock,
Roland Dubb, Siddarth Singh, and Arnu Pretorius. Towards a standardised performance evaluation protocol
for cooperative marl. Advances in Neural Information
Processing Systems, 35:5510â€“5521, 2022.
[30] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic
actor. In International conference on machine learning,
pages 1861â€“1870. Pmlr, 2018.
[31] Steven LaValle. Rapidly-exploring random trees: A new
tool for path planning. Research Report 9811, 1998.
[32] Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash
Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De
Fabritiis, and Vincent Moens. Torchrl: A data-driven
decision-making library for pytorch, 2023.
[33] Matteo Bettini, Amanda Prorok, and Vincent Moens.
Benchmarl: Benchmarking multi-agent reinforcement
learning. Journal of Machine Learning Research,
25(217):1â€“10, 2024.
[34] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers,
SÃ£o Paulo, Brazil, May 8-12, 2017, Revised Selected
Papers 16, pages 66â€“83. Springer, 2017.
[35] Joseph Suarez. Pufferlib: Making reinforcement learning libraries and environments play nice. arXiv preprint
arXiv:2406.12905, 2024.
[36] Alexander Rutherford, Michael Beukman, Timon Willi,
Bruno Lacerda, Nick Hawes, and Jakob Nicolaus Foerster. No regrets: Investigating and improving regret
approximations for curriculum discovery. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024.
[37] Eugene Vinitsky, Nathan LichtlÃ©, Xiaomeng Yang,
Brandon Amos, and Jakob Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. Advances in Neural
Information Processing Systems, 35:3962â€“3974, 2022.

[38] Jingtian Yan, Zhifei Li, William Kang, Yulun Zhang,
Stephen Smith, and Jiaoyang Li. Advancing mapf
towards the real world: A scalable multi-agent realistic testbed (smart). arXiv preprint arXiv:2503.04798,
2025.

Appendix Contents
Sections
Contents
Appendix A Evaluation Protocol
Appendix B

Code Examples

Appendix C

Discussion of Limitations

Appendix D

Extended Benchmark Results

Appendix E

Scalability Analysis Details

Appendix F

Future Work and Directions

Appendix G

Implementation Details

Appendix H

Extended Related Work

Appendix A â€” Evaluation Protocol
Evaluation Suite Overview. To make results easy to compare and fully reproducible, CAMAR adopts the â€œStandardised Performance Evaluation Protocol for Cooperative
MARLâ€ by [29] and extends it to continuous multi-agent
path-finding. We keep the core ideas, fixed training budgets,
multiple random seeds, and strict uncertainty estimates, but
add path-finding-specific stress-tests:
â€¢ Axis 1: Agent count. We vary the number of agents to
see whether a method still works when the team grows.
â€¢ Axis 2: Map difficulty. We change obstacle density and
map geometry to check if the learned policy still solves
harder layouts.
Three difficulty tiers.
â€¢ Easy uses the same map, agent count, and obstacle parameters for training and testing. Only the random seeds
that place starts and goals change. This tells us whether
an algorithm can solve the problem at all.
â€¢ Medium trains on one map setting but evaluates on 12
variants that share the same map types while changing
agent count and obstacle density. This probes generalization within the same domain and includes testing generalization across agents. The current protocol is designed
for reproducibility and fair comparison across methods.
However, since the environment scales efficiently, we plan
to extend the Medium tier in future releases to include
a wider range of agent counts and task complexities as
MARL methods progress.
â€¢ Hard trains on any user-chosen mapsâ€”except the MovingAI street setâ€”and then tests on those unseen street maps
with new agent counts. This is a near-real-world stress
test.

Standardized Evaluation Suite for CAMAR
Input. A set of maps M, task sets Tm , and a pool of
algorithms A.
1. Default settings
â€¢ Training steps T : 2M (off-policy) or 20M (on-policy).
â€¢ Independent runs R: 3 seeds.
â€¢ Evaluation episodes E: 1 000 per interval.
â€¢ Evaluation intervals I (based on available resources):
every 10K-100K steps (off-policy) or 100K-1000K
steps (on-policy).
2. Metrics
â€¢ Returns G for sample-efficiency plots.
â€¢ Success Rate (SR), Flowtime (FT), Makespan (MS),
and Coordination (CO).
â€¢ Per-task: mean Gat over E episodes with 95% CIs.
â€¢ Per-protocol: build an (RÃ—|T |) matrix of normalised
returns, then report IQM, optimality gap, probability of
improvement, and performance profiles (all with 95%
bootstrap CIs).
3. Three difficulty tiers
1. Easy: train and test on the same map/agent count. 12
models Ã— 3 seeds.
2. Medium: train on one map, test on all 12 maps and
aggregate. 12 models Ã— 3 seeds.
3. Hard: train on any maps (except MovingAI), test on
MovingAI [26] street maps with new agent counts. Single model (preferably 3 seeds).
4. Reporting checklist
â€¢ Hyper-parameters, network sizes, and compute budget.
â€¢ Map generation settings for each tier.
â€¢ Final IQM Â±95% CI for SR, FT, MS, CO (mandatory).
Sample-efficiency curves are recommended for Easy
and Medium, optional for Hard. In Hard also plot each
metric against the number of agents to test the ability
to scale.

Each tier follows the default budget: 2M steps for offpolicy and 20M steps for on-policy algorithms, three random
seeds, and evaluations every 10K-100K or 100K-1000K steps.
We keep the JAX seed fixed at 5 and split it to generate all
evaluation keys, which makes every run exactly repeatable.
Reporting and analysis. We require final IQM Â± 95% CI
of Success Rate, Flowtime, Makespan, and Coordination.
Sample-efficiency curves are strongly recommended for Easy
and Medium and optional for Hard if resources allow. For
the Hard tier, we also ask for plots of each metric versus the
number of agents because scalability is a key question.

Appendix B â€” Code Examples
The CAMAR library provides a simple and flexible interface
for building custom multi-agent pathfinding environments.
Below we show two examples that demonstrate how to create environments using different map generators and agent
dynamics.

Example 1 (Fig. 8) shows how to create an environment
with the random_grid map. It uses the string-based API
to pass all configuration options directly. This includes
the number of agents, the size range for agents and goals,
and the dynamic model to use. We set the dynamic to
HolonomicDynamic, and also customize the observation
window and the shaping factor. This example shows how to
use heterogeneous sizes for agents and goals, which can help
simulate more realistic scenarios.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

from camar import camar_v0
env1 = camar_v0(
map_generator="random_grid",
dynamic="HolonomicDynamic",
pos_shaping_factor=2.0,
window=0.25, # obs window
max_obs=10, # max num of obj in obs
map_kwargs={
"num_agents": 16,
"agent_rad_range": (0.01, 0.05 ),
"goal_rad_range": (0.001, 0.005),
},
dynamic_kwargs={
"mass": 2.0,
}
)

Figure 8: Example 1. Creating a CAMAR environment using
random grid maps and holonomic agents. The environment
is loaded using the string-based API, which supports configuration via YAML or inline dictionary.

Example 2 (Fig. 9) shows how to use maps from the MovingAI benchmark [26]. These maps are loaded by name using
the map generator function movingai(). This example also
shows how to use heterogeneous agent dynamics by combining different models (e.g. DiffDriveDynamic and
HolonomicDynamic). This is done using the class-based
API with MixedDynamic. The number of agents of each type
is specified, and the total is passed to the environment. This
allows testing how different types of agents can cooperate in
complex environments.
These examples can be used as templates for building
new scenarios in CAMAR. Users can adjust map parameters,
agent settings, or dynamic models to suit their research needs.
The CAMAR design supports quick changes to both environment structure and agent behavior using only a few lines of
code.

Appendix C â€” Limitations
CAMAR offers fast and flexible simulation, but there are still
important gaps. First, all built-in scenarios use static obstacles. We do not yet provide maps with moving obstacles, so
agents cannot train on fully dynamic scenes. While the engine
supports different sizes for agents, goals, and landmarks, the
built-in map generators do not include variations in landmark
sizes. We believe that adding new maps with obstacles of

Easy

Medium

Hard

Purpose

Test that the method can solve the
problem without testing generalization.

Test that the method can solve the
problem and generalise across similar map types including varying
number of agents.

Test that the method can generalise
to near-real-world settings.

How to train and
evaluate

Train on map_X (see Appendix).

Train on map_X (see Appendix).

Evaluate on the same map_X.

Evaluate on all 12 maps.

Train on any map collection excluding MovingAI
Evaluate on MovingAI street collection with varying agent counts

Repeat for all 12 maps

Repeat for all 12 maps

Number of models
Number of evals

12 trained models Ã— 3 seeds
12 evaluations Ã— 1K episodes

12 trained models Ã— 3 seeds
144 evaluations Ã— 1K episodes

Report

Metrics - if needed
sample-efficiency
curves
strongly recommended

-

Metrics - mandatory
sample-efficiency curves - if
needed

1 trained model
30 evaluations Ã— 1K episodes Ã—
varying number of agents
Metrics - mandatory
sample-efficiency curves - if resources allow
metrics vs num_agents - mandatory

Table 3: Overview on 3-tier evaluation protocols presenting the purpose, number of trained models and evaluations for each
protocol

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

from camar import camar_v0
from camar.maps import movingai
from camar.dynamics import MixedDynamic,
DiffDriveDynamic, HolonomicDynamic
dynamics_batch = [
DiffDriveDynamic(mass=1.0),
HolonomicDynamic(mass=10.0),
]
# 8 diffdrive + 24 holonomic = 32 total
num_agents_batch = [8, 24]
mixed_dynamic = MixedDynamic(
dynamics_batch=dynamics_batch,
num_agents_batch=num_agents_batch,
)
map_generator = movingai(
map_names=["street/Denver_0_1024",
"bg_maps/AR0072SR"],
num_agents=sum(num_agents_batch),
)

makes it easy to extend in this way.
Second, although we support two built-in dynamics models
(HolonomicDynamic and DiffDriveDynamic), and
allow mixing them to build heterogeneous teams, both use
basic integration schemes: either explicit or non-explicit Euler. To achieve stable simulation with larger time steps (for
example, dt = 0.1), a frameskip must be applied. Without
frameskip, simulation becomes unstable with large dt values,
and a smaller integration step like dt = 0.005 must be used
together with frameskip = 20. Adding more stable integration methods, such as Runge-Kutta, could improve stability
and allow efficient simulation with fewer steps. For example,
using Runge-Kutta with dt = 0.05 and frameskip = 2 (for a
total of 8 integration steps per forward pass) might offer a
better trade-off between speed and accuracy.
Third, there is no built-in communication mechanism between agents. Adding message passing would help study how
algorithms use shared information.

Appendix D â€” Extended Benchmark
env2 = camar_v0(
map_generator=map_generator,
dynamic=mixed_dynamic,
pos_shaping_factor=2.0,
window=0.25,
max_obs=10,
)

Figure 9: Example 2. Creating a CAMAR environment with
MovingAI maps [26] and mixed agent dynamics. This example demonstrates the class-based API for explicitly specifying
heterogeneous agent behavior.
different sizes and dynamic behavior is an important direction for future work. Fortunately, CAMARâ€™s modular design

Performance Analysis
Detailed Benchmark Analysis The main evaluation
compares success rate (SR), flowtime (FT), makespan
(MS), and coordination (CO) on two map types. On
random_grid, MAPPO gives the highest SR with strong
CO. RRT*+MAPPO reaches similar SR but with lower FT
and MS, which shows that planning improves movement efficiency. RRT*+IPPO gives smaller gains but still improves
over IPPO. The classic RRT*+PD baseline reaches high SR
but low CO because it plans for each agent alone and does
not react to dynamic agent movement.
Hybrid methods with off-policy algorithms show mixed
behavior. RRT* improves ISAC and IDDPG in SR and in
some cases CO. But RRT* weakens MASAC and MADDPG,
where SR is almost zero. These algorithms use a centralized

critic with long input vectors, which include observations
and RRT* samples from all agents. This leads to unstable
training.
IPPO and IDDPG reach good SR but slower FT than
MAPPO. ISAC gives high CO but low SR. MASAC and
MADDPG fail to explore and reach very low SR. Additional
experiments and analysis on DDPG and SAC can be found
below.
The labmaze_grid map is harder because of sparse
rewards and narrow corridors. All MARL baselines drop in
SR. RRT*+PD performs best in SR due to full-path planning,
but CO is still low. The simple RRT+PD baseline gives worse
paths and lower SR than RRT*+PD, but in some cases it still
outperforms MARL baselines in SR.
These results show a clear trade-off. Planning gives strong
reliability and better path efficiency. MARL gives better coordination but fails in sparse reward maps. Hybrid methods
give only small gains. One reason is that the current policy
networks are small and cannot process the large inputs that
combine local observations with RRT* samples. Stronger
model architectures may improve this integration.
1.00
Fraction of runs with score >

Fraction of runs with score >

1.00
0.75
0.50

IPPO
MAPPO
IDDPG
MADDPG
ISAC
MASAC

0.25
0.00
0.0

0.2

0.75
0.50

RRT*+MASAC
RRT*+IPPO
RRT*+MAPPO
RRT*+MADDPG
RRT*+ISAC
RRT*+IDDPG

0.25
0.00

0.4
0.6
Normalized return ( )

0.8

1.0

0.0

0.2

0.4
0.6
Normalized return ( )

0.8

1.0

Figure 10: Performance profiles based on normalized return.
Each line shows the fraction of evaluation runs that achieved
a score above a given threshold Ï„ .

1.00
Fraction of runs with score >

Fraction of runs with score >

1.00
0.75
0.50

IPPO
MAPPO
IDDPG
MADDPG
ISAC
MASAC

0.25
0.00
0.0

0.2

0.75
0.50

RRT*+MASAC
RRT*+IPPO
RRT*+MAPPO
RRT*+MADDPG
RRT*+ISAC
RRT*+IDDPG

0.25
0.00

0.4
0.6
Success rate ( )

0.8

1.0

0.0

0.2

0.4
0.6
Success rate ( )

0.8

1.0

Figure 11: Performance profiles based on success rate. Each
line shows the fraction of evaluation runs that achieved SR
above a threshold Ï„ .
Aggregate Analysis and Performance Profiles We also
report aggregate return metrics across all evaluated methods
in Fig.12. These charts, produced using the MARL - EVAL
toolkit[29], show median, interquartile mean (IQM), mean
return, and optimality gap, with normalized scores. Among
MARL baselines, MAPPO performs best across all metrics,
followed by IPPO. ISAC and IDDPG achieve lower scores
and are less consistent, while MADDPG performs the worst
and shows the largest optimality gap. Among hybrid methods,
RRT*+MAPPO and RRT*+IPPO achieve strong results that

match or exceed their baselines. However, RRT*+MASAC
and RRT*+MADDPG perform poorly. This may be caused
by the larger input space, especially for centralized critics
that struggle with high-dimensional observations.
To further evaluate consistency and reliability, we present
performance profiles in Fig.10 and Fig.11. These plots show
the fraction of evaluation runs where each algorithm achieves
a score above a given threshold. The results confirm our
earlier findings: MAPPO and IPPO maintain good performance across tasks, while MADDPG and MASAC drop
quickly. RRT*+MAPPO and RRT*+IPPO remain strong
among hybrid approaches, while RRT*+MADDPG and
RRT*+MASAC again show poor results.
These charts together highlight the differences in generalization ability, robustness, and effectiveness across MARL
and hybrid methods. They also support our earlier analysis
of SR, FT, MS, CO (Table 2). Overall, these findings show
that RRT* integration can improve performance for some
methods, but not all. The effect depends on the algorithm and
how it handles the added input complexity.
Sample Efficiency. Figure 13 shows the mean success rate
evolving during training, computed with the MARL - EVAL
toolkit [29]. MAPPO learns the fastest, rising above 0.60
after roughly 3M steps and then remaining stable. MASAC
improves quickly at first but plateaus below 0.45. IPPO starts
slowly yet keeps improving and nearly reaches the MASAC
curve near the end of training. IDDPG attains a modest
plateau early and then changes little. ISAC climbs slightly in
the first million steps and then flattens out, while MADDPG
stays under 0.15 for the entire run.
Algorithm

SR â†‘

FT â†“

MS â†“

CO â†‘

Ep. Return â†‘

IDDPG (SC)
ISAC (SC)
MADDPG (SC)
MADDPG (NSC)
MASAC (SC)
MASAC (NSC)

0.77
0.21
0.00
0.00
0.26
0.44

406
1060
1241
1259
813
780

134
160
160
160
159
160

0.996
0.999
0.997
0.998
0.996
1.000

266
92
-1.1
-0.9
67
193

Table 4: Performance comparison of multi-agent DDPG and
SAC variants on random_grid with 8 agents. SC: shared
critic, NSC: not shared critic.
DDPG and SAC analysis We further investigated the weak
performance of multi-agent DDPG and SAC algorithms in
our benchmark. Specifically, we observed that both MADDPG and MASAC achieved lower success rates and higher
episode lengths compared to their independent counterparts
(IDDPG and ISAC). To understand these results, we considered two possible factors that could negatively affect learning
with centralized critics.
First, the credit assignment problem becomes harder as the
number of agents increases. In both MADDPG and MASAC,
a single centralized critic must estimate value based on the
combined actions and states of all agents. The criticâ€™s target
is a simple mean across agents in this version, which can blur
important agent-specific differences. This makes learning
slow and unstable.

MASAC
ISAC
MADDPG
IDDPG
MAPPO
IPPO

Median

IQM

0.84 0.90 0.96

RRT*+MASAC
RRT*+ISAC
RRT*+MADDPG
RRT*+IDDPG
RRT*+MAPPO
RRT*+IPPO

Mean

0.88 0.92 0.96 1.00 0.85 0.90 0.95 1.00
Normalized return

Median

IQM

0.84 0.90 0.96

Mean

0.84 0.90 0.96
0.84 0.90 0.96
Normalized return

Optimality Gap

0.05 0.10 0.15
Optimality Gap

0.06 0.12 0.18

Figure 12: Normalized return scores. Top: MARL baselines. Bottom: RRT* hybrid methods. Metrics include median, interquartile
mean (IQM), mean, and optimality gap. Higher values are better except for optimality gap. Each bar shows 95% confidence
intervals.
IPPO
MAPPO
IDDPG
MADDPG
ISAC
MASAC

0.6

Success rate

0.5
0.4
0.3
0.2
0.1
0.0
0

1

2

3

4
Timesteps

5

6

7

1e6

Figure 13: Sample-efficiency curves: mean success rate versus environment steps. Computed using MARL - EVAL.

Second, the input size of the centralized critic grows with
the number of agents. For example, in our setting with 8
agents, the critic receives a long concatenated vector of
all observations and actions. This high-dimensional input
makes the learning problem more difficult, especially for
relatively small networks like MLPs. Previously discussed
performance of RRT*+MASAC and RRT*+MADDPG also
supports this hypothesis. These hybrid methods show much
lower returns and success rates compared to RRT*+IPPO or
RRT*+MAPPO. This drop in performance is likely caused by
the added RRT* information increasing the input size even
further, making the learning task more difficult for centralized
critics.
To test these ideas, we trained new versions of MADDPG
and MASAC where the critic network is no longer shared
between agents. Instead, each agent has its own critic (without parameter sharing), while still using centralized training.
The results are shown in Table 4. For MADDPG, there was
no improvement: both the shared and non-shared versions

failed to learn the task, with success rate remaining at 0.0.
This further supports the hypothesis that MADDPG struggles
with large input vectors and credit assignment, even when
the critics are separated. In contrast, MASAC improved with
non-shared critics: the success rate rose from 0.26 to 0.44 and
return nearly tripled. This suggests that SAC can better tolerate large input sizes, likely due to its entropy regularization
and smoother policy updates. However, even this improved
MASAC variant still performs worse than ISAC, which does
not face centralized credit assignment at all. This shows that
credit assignment remains a challenge in MASAC, despite its
robustness to large inputs. Notably, the poor performance of
RRT*+MADDPG and RRT*+MASAC in earlier experiments
aligns with these findings.
Together, these findings confirm that both factorsâ€”the
size of the critic input and the difficulty of shared credit
assignmentâ€”can reduce the performance of centralized offpolicy MARL methods. We include these results in Table 4.

Benchmark Maps
CAMAR includes three benchmark map types designed to
evaluate navigation and coordination in multi-agent scenarios.
These maps differ in layout, complexity, and agent configurations.
random_grid map is a 20 Ã— 20 grid with randomly
placed rectangular obstacles (with circle-based discretization). Agents are initialized in free cells, and goals are placed
at randomly chosen locations. The scenario uses holonomic
dynamics and includes parameters for obstacle density, agent
radius, goal radius, and others. During benchmarking, we
generate six different random_grid instances: three maps
with 8 agents and three with 32 agents. Each of these variations is evaluated under three different obstacle densities
(0.0, 0.05, 0.15), which define how cluttered the map becomes. These tasks are relatively simple but still useful to

60K

(b) VMAS

The hetero_give_way map tests heterogeneous multiagent coordination. It is based on a simple corridor scenario where one larger agent cannot enter the central
chamber, while the smaller agent can pass through. This
setup forces agents to learn implicit turn-taking or yielding behavior. Although the dynamics remain the same
(HolonomicDynamic), this configuration highlights differences in agent capabilities. Basic algorithms like MAPPO
and IPPO struggle in this task. Nonetheless, CAMAR supports such heterogeneous settings out of the box and can be
used to advance research in coordination strategies for agents
with diverse sizes, dynamics, and behaviors.

Appendix E â€” Extended Scalability Analysis
CAMAR vs VMAS This section provides visualizations of
scenarios of CAMAR and VMAS. Fig. 7a and Fig. 14 use the
same map generation settings for both CAMAR and VMAS
to allow a fair comparison under matched conditions.
Comparing with Other Environments. In Table 5, we
compare CAMAR to other environments with GPU support
like JaxMARL and Jumanji. CAMAR is faster than VMAS
and the MPE baselines, and it reaches speeds close to the
best GPU simulators. For example, CAMAR runs at 338K
SPS with 4 agents and 50K SPS with 32 agents. Some JaxMARL environments reach higher throughputâ€”for example,
SMAX achieves up to 1.2M SPSâ€”but their performance
drops more quickly as the number of agents grows. JaxNav,
for instance, cannot run with more than 32 agents. While
SMAX and RWARE show strong throughput, they use simplified settings like open space or discrete grids and do not
support procedurally generated obstacle layouts. CAMAR,
in contrast, supports continuous control and dense obstacles,
which brings more realism. It trades a small drop in speed for
a richer environment that better matches real-world MAPF
challenges.

Appendix F â€” Future Work
We highlight several research directions that can be explored
using CAMAR:

2.7K

200

110K
95K
80K
65K

CAMAR
VMAS

50K
35K
20K

2.7K

4
16

32

Steps per Second

Number of Parallel Environments

Number of Agents

128

Heterogeneous Support

CAMAR
VMAS

64

(a) CAMAR

40K
30K
20K
10K

55
100000

labmaze_grid is based on procedural maze generation. It creates more structured maps using multiple rectangular rooms and corridors. The parameter extra_connection_probability controls how
many connections exist between rooms: 1.0 means the map is
fully connected, while lower values introduce more isolated
areas and bottlenecks. As with the random grid, we evaluate 6
instances: three maps with 8 agents and three with 32 agents,
each under three different connection probabilities (0.4, 0.65,
1.0). The dynamics are the same as in the random grid, allowing consistent comparison across layouts. These tasks
provide harder challenges for coordination and navigation.
These benchmark maps are scalable, fast to simulate, and
allow flexible control over structure and difficulty. Together,
they form a comprehensive testbed for MAPF.

50K

25000
3000
4000
5000
6000

Steps per Second

test multi-agent pathfinding (MAPF) algorithms.

Figure 14: Scalability comparison between CAMAR and
VMAS across different settings, evaluating the impact of
increasing parallel environments, agent count, and obstacles
on simulation speed.
Agents
CAMAR
VMAS
JaxNav
SMAX
RWARE
MPE-2
MPE-1120

4

8

16

32

64

128

338K
16K
159K
0.8M
1.4M
0.6M
11.8K

189K
10K
61K
1.0M
1.1M
0.8M
11.2K

99K
5.5K
13.6K
1.2M
0.8M
1.2M
11.9K

50K
2.7K
353
0.8M
0.5M
0.7M
11.1K

25K
1.2K
0.3M
0.3M
0.5M
9.7K

13K
447
0.1M
0.1M
0.2M
7.7K

Table 5: SPS vs number of agents. Comparison across environments with GPU support. MPE-2 uses the default simpletag configuration with 2 circle obstacles, while MPE-1120
uses 1120 circles matching the number of circle obstacles in
CAMAR. â€˜-â€˜ indicates that the environment was too slow to
even finish episode.

1. CAMAR enables testing with large agent populations,
supporting the development of scalable multi-agent reinforcement learning (MARL) methods. Future work can
focus on improving how algorithms handle hundreds of
agents operating in dense, dynamic environments.
2. CAMAR provides a natural platform for studying interagent communication. Large-scale coordination often requires agents to exchange information with nearby teammates, which remains an open challenge. The environment offers the necessary tools to explore such localized
communication mechanisms.
3. CAMAR includes realistic continuous navigation tasks,
making it suitable for integrating classical planning approaches (RRT, RRTâˆ— ) with learning-based agents. Combining long-term planning with reinforcement learning

could improve both sample efficiency and robustness.
4. CAMAR supports agents with different dynamics and
properties, enabling research on heterogeneous teams. We
plan to extend the current list of dynamics. For the moment, we include differential-drive and holonomic robots,
but we are going to support more options, such as car-like
and 2D quadrotor dynamics.
5. CAMARâ€™s fully modular design allows users to easily
redefine observation and reward functions through wrapping. We plan to introduce additional observation types,
including simulated 2D LiDAR, to further diversify sensing modalities for navigation tasks.
6. Another direction of future extensions is to extend classical baselines beyond the current RRT+PD and RRTâˆ— setup
by adding widely used multi-robot navigation methods
such as PRM+PD (Probabilistic roadmap for global planning with PD controller), ORCA, and MPPI. Similarly,
we will expand learning-based baselines to include RNNbased algorithms and architectures using graph attention
or transformers, going beyond the standard MLP-based
backbones.
7. We aim to incorporate post-generation connectivity
checks to ensure that all agents can reach their goals.
While computationally expensive, this process will improve the reliability and fairness of evaluation.
8. CAMAR can be extended beyond multi-agent pathfinding.
Future releases will include new task types such as pickup-and-delivery, cooperative transportation, and dynamic
obstacle scenarios, making CAMAR more general testbed
for large-scale MARL research.
We hope CAMAR will support the community in studying
these and other open problems in multi-agent learning and
planning.

Appendix G â€” Implementation Details
Integrations
CAMAR is designed to work easily with modern reinforcement learning tools. It follows the Gymnax interface, which
is already familiar to many researchers working with RL
environments on top of JAX [27].
We also provide a wrapper for TorchRL [32], which allows
users to integrate CAMAR into PyTorch-based pipelines.
In addition, CAMAR supports integration with BenchMARL [33], a framework for evaluating MARL algorithms.
These integrations make it easy to use CAMAR with popular
RL frameworks.

Vectorized setup
To maintain high simulation speed, CAMAR uses efficient
vectorized operations based on JAX [27]. Agents with the
same dynamic model are grouped together and updated in
parallel. For agent sizes, two cases are supported: if all agents
have the same radius, it is treated as a constant during JAX
compilation [27], giving faster simulation. If agents have
different sizes, their radii are passed as vectors being part of
the environment state, which still allows efficient processing
and supports randomized sizes during training.

Appendix H â€” Extended Related Work
Many multi-agent reinforcement learning (MARL) benchmarks have been developed in recent years, each focusing
on different challenges such as coordination, navigation, or
multi-agent planning. A key ability for agents in robotics is
to move and adapt in complex environments. Some recent
environments explore these abilities, but they differ widely
in important properties.
These differences include whether the environment supports continuous spaces, GPU acceleration, and high simulation throughput. Some platforms support only discrete
actions, while others offer more realistic continuous control.
Environments also vary in their ability to handle large groups
of agents or support partial observability.
Other practical features also matter for researchers. Some
environments are fully implemented in Python and support
flexible task generation, which makes them easier to extend
and use in large experiments. Others may lack documentation
or require heavy customization.
Finally, only a few MARL environments offer standard
evaluation protocols, automated tests, or package installation
via PyPi. These features are useful for ensuring fair comparisons between methods, improving reproducibility, and
making environments easier to maintain. In our work, we aim
to address these points by extending existing protocols and
providing a high-performance, flexible platform.
To sum up, our benchmark builds on the strengths of existing environments and aims to complement ongoing efforts in MARL research. Over the years, a range of benchmarks has helped researchers tackle challenges in coordination, planning, and navigation. Well-known examples include SMAC [20, 17, 19], Jumanji [21], POGEMA [14, 15],
MPE [22, 18], and VMAS, each of which introduces useful
features for different kinds of tasks.
SMAC [20, 17, 19] is popular for testing strategic decisionmaking, but it uses discrete actions and does not scale well for
large environments. Jumanji [21] supports GPU acceleration
and procedural generation, but its focus is not on navigation
or planning. POGEMA [14, 15] is strong in grid-based navigation and procedural tasks with many agents, but it does
not use continuous states or actions, which are important in
robotics.
MPE [22, 18] has played an important role in early MARL
research, but it cannot scale to hundreds of agents efficiently.
VMAS [16] builds on MPE [22, 18] by adding its physics
and continuous dynamics, making it better for robotics. However, it can still be slow and difficult to scale to larger agent
populations or complex maps.
Many environments also lack evaluation protocols. This
makes it difficult to compare algorithms fairly and limits the
reproducibility of results. Performance issues also appear
in environments that depend heavily on CPU-GPU communication, which slows down training and makes large-scale
experiments harder.
Finally, popular robotics simulators such as Gazebo [23],
Webots [24], and ARGoS [25] are powerful tools for realistic physics-based simulation and robotic deployment. Each
of these frameworks serves specific use cases focused on

ry
Co
nt
.O
bs
Co
er
nt
. A vati
on
GP ctio
s
ns
U
Su
p
Sc
ala port
bi
lit
Pa
rti y >5
al l
00
Ag
He y ob
en
se
ter
r
ts
og
va
b
e
Pe
ne
le
rfo
o
rm us a
ge
an
nt
ce
s
>1
Py
0K
th
SP
on
S
ba
Pr
se
oc
d
ed
Re ural
g
qu
ire ene
rat
sg
io
en
era n
Ev
liz
at i
alu
on
ati
on
Te
sts
pr
ot
&
CI ocol
Py
s
PI
Li
s te
d

Re

po
sit
o

Environment / Simulator
Waterworld (SISL) [34]
RWare [13]
RWare (Jumanji) [21]
RWare (Pufferlib) [35]
Trash Pickup (Pufferlib) [35]
SMAC [20]
SMACv2 [17]
SMAX (JaxMARL) [19]
MPE [22, 18]
MPE (JaxMARL) [19]
JaxNav (JaxMARL) [36]
Nocturne [37]
POGEMA [14]
VMAS 4 [16]
SMART [38]

link
link
link
link
link
link
link
link
link
link
link
link
link
link
link

âœ“
âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“

âœ“
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“

âœ—
âœ—
âœ“
âœ—
âœ—
âœ—
âœ—
âœ“
âœ—
âœ“
âœ“
âœ—
âœ—
âœ“
âœ—

âœ—
âœ—
âœ—
âœ“
âœ“
âœ—
âœ—
âœ—
âœ“
âœ“
âœ—
âœ“
âœ“
âœ—
âœ“

âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“

âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ—
âœ—
âœ“
âœ—

âœ“
âœ—
âœ—
âœ“
âœ“
âœ—
âœ—
âœ“
âœ—/âœ“3
âœ—/âœ“3
âœ—
âœ—
âœ“
âœ—/âœ“4
âœ—

âœ“
âœ“
âœ“
âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“
âœ—

âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ—
âœ—

âœ—
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“
âœ“
âœ“
âœ—
âœ—
âœ“
âœ—/âœ“4
âœ“

âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ—
âœ“
âœ“
âœ—
âœ—

âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
âœ—

âœ“
âœ“
âœ“
âœ“
âœ“
âœ—
âœ—
âœ“
âœ“
âœ“
âœ“
âœ—
âœ“
âœ“
âœ—

Gazebo [23]
Webots [24]
ARGoS [25]

link
link
link

âœ“
âœ“
âœ“

âœ“
âœ“
âœ“

âœ“
âœ“
âœ—

âœ—
âœ—
âœ“

âœ“
âœ“
âœ“

âœ“
âœ“
âœ“

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ—
âœ—
âœ—

âœ“
âœ“
âœ“

âœ—
âœ—
âœ—

CAMAR (Ours)

link

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

âœ“

Table 6: Comparison of multi-agent reinforcement learning (MARL) environments and simulators. Each row corresponds to a
specific environment or a particular implementation of it. The columns indicate key properties, including support for continuous
observations and actions, GPU acceleration, scalability beyond 500 agents, partial observability, heterogeneous agents, and
whether the simulator can exceed 10K simulation steps per second (SPS). Additional columns specify if the environment is
implemented fully in Python, supports procedural generation, requires generalization across different maps or tasks, includes
evaluation protocols, and provides built-in tests or continuous integration (CI). The â€œRepositoryâ€ column contains links to the
official source code for each environment. CAMAR, listed at the bottom, is our proposed environment.
accurate physical modeling and hardware-oriented evaluation. In contrast, our environment, CAMAR, is designed to
complement these tools by focusing on large-scale MARL
and benchmarking, particularly for continuous MAPF tasks.
Rather than replacing existing simulators, CAMAR extends
the set of available research tools, bridging the gap between
large-scale MARL studies and robotics-oriented simulation.
We compare MARL environments and robotic simulators
using the following criteria (Tables 1, 6):

older environments like SMAC [20, 17] and MPE [22, 18]
do not support GPU-based simulation.
Scalability >500 Agents When the number of agents
grows, decision-making becomes harder. A good benchmark
should scale well to hundreds or thousands of agents. Environments like POGEMA [14, 15] and Trash Pickup [35] can
run with thousands and even millions of agents. Others, like
VMAS, MPE [22, 18], and SMAC [20, 17], are limited to
much smaller groups.

Continuous Observations and Actions Robots usually operate in continuous state and action spaces, so itâ€™s important
for benchmarks to reflect these conditions. Environments like
VMAS and MPE [22, 18] use continuous actions and observations, while others, like Trash Pickup [35] or POGEMA [15],
use discrete representations, which limit realism in robotics
simulations.

Partially Observable In most real-world scenarios, agents
see only part of the environment. This is known as partial observability and is a common feature in almost every RL environment. SMAC [20, 17], RWare [13], and
POGEMA [14, 15] support this feature by limiting each
agentâ€™s view.

GPU Support In multi-agent environments, each agent
often has its observations and rewards. This can lead to
heavy data transfers between the CPU and the GPU, especially when training deep RL models. Benchmarks like
Jumanji [21] and SMAX [19] are initially built with GPU acceleration, helping to reduce training time. In contrast, many

Heterogeneous Agents In real life, agents and robots can
have different sensors, shapes, or goals. Benchmarks like
VMAS and SMAC [20, 17] allow agents to behave differently, making cooperation more complex. Others, like
POGEMA [14, 15] or RWare [13], usually involve homogeneous agents.

Performance >10K Steps/s High simulation speed is essential when agents need millions of interactions to learn.
For example, the Pufferlib [35] environment can run with a
speed of up to 1M steps per second. This allows researchers
to train and develop models more quickly. Environments like
SMAC [20, 17] or VMAS can be slower, especially when
running with many agents.
Python Based Using Python makes it easier for researchers
to understand and modify the environment. SMAX [19],
VMAS, and POGEMA [14, 15] are implemented entirely
in Python. This helps with faster development and integration
into learning frameworks.
Procedural Generation Procedural generation helps create diverse tasks that reduce the chance of overfitting.
POGEMA [14, 15] and Jumanji [21] use this method to generate complex tasks automatically. Other environments, like
RWare [13], use fixed layouts that limit diversity.
Requires Generalization Only a few environments offer separate training and testing tasks, making it difficult
to test whether agents generalize well to new situations.
POGEMA [14, 15] and SMACv2 [17] include test scenarios
that allow researchers to check generalization. Many others
use the same tasks during both training and testing.
Evaluation Protocols To compare algorithms fairly, we
need well-defined test cases and metrics. Benchmarks like
SMACv2 [17] and Jumanji [21] include evaluation protocols
to make results reproducible and meaningful. Many other
environments do not have standard evaluation tools or test
setups.
Tests & CI CI pipeline and testing suite are vital for collaborative development and maintenance of open-source
projects. These practices help ensure code reliability and
facilitate contributions from the whole research community.
PyPi Listed When an environment is available on PyPi, it
becomes easier for others to install and use. Benchmarks like
Jumanji [21], and Trash Pickup [35] are available on PyPi.
This lowers the barrier to entry and helps increase adoption
in the research community.
Here are detailed descriptions and analysis of each environment used for the comparison in the Tables 1, 6:
Waterworld (SISL) â€” The Waterworld environment, part
of the SISL (Stanford Intelligent Systems Laboratory) suite
[34], is a continuous control benchmark where multiple
agents move in a bounded two-dimensional space to collect moving targets (â€œfoodâ€) while avoiding harmful objects
(â€œpoisonâ€). Agents have continuous observations and actions,
and their motion dynamics are simple and lightweight, which
allows high simulation speeds for training.
The environment places a small number of obstacles randomly in the scene, with the default setup containing only
one obstacle. This limited variation does not require agents
to generalize across different maps in a meaningful way. Waterworld does not support heterogeneous agents, evaluation
protocols, or complex multi-stage tasks, and the scenario
remains structurally the same across runs.

Multi-Robot Warehouse (RWare) â€” The Multi-Robot
Warehouse environment [13] simulates robots moving in a
warehouse to collect and deliver requested goods. The layout
is grid-based, and agents must coordinate to navigate around
shelves and other robots. The default version is implemented
in Python using a discrete state and action space. It supports
partial observability but does not provide procedural generation: the warehouse layout is fixed. As a result, agents do not
need to generalize across different maps.
Several re-implementations of RWare exist. The Jumanji
version [21] rewrites the environment in JAX, which improves simulation speed and allows hardware acceleration
on GPUs, but keeps the original fixed-layout design and task
structure. The PufferLib version [35] modifies the environment to support large-scale parallel simulation.
None of the RWare versions support heterogeneous agents,
continuous control, or evaluation protocols. Despite this,
RWare remains a widely used benchmark for discrete-space
multi-agent pathfinding and cooperative delivery tasks.
Trash Pickup (PufferLib) â€” The Trash Pickup environment [35] is a grid-based multi-agent task where agents move
around a map to collect pieces of trash and deliver them to
designated drop-off locations. It is implemented in C with
a Python API for controlling agents, which allows efficient
simulation. The environment uses discrete state and action
spaces and supports partial observability.
The layout is fixed, and trash positions follow a predefined spawn pattern. While trash locations may vary between
episodes, the underlying map structure remains the same.
This means the environment does not require generalization
across different maps. The design supports scalability to more
than 500 agents.
The Trash Pickup benchmark does not provide continuous
control, heterogeneous agents, or formal evaluation protocols.
However, it offers a simple and repeatable cooperative task
that can be scaled to large numbers of agents, making it
a useful test case for studying coordination efficiency in
discrete environments.
StarCraft Multi-Agent Challenge (SMAC) â€” The StarCraft Multi-Agent Challenge [20] is one of the most widely
used benchmarks in the MARL community. In SMAC, a
team of StarCraft II units controlled by independent agents
must cooperate to defeat an opposing team. The environment
is partially observable, and by default uses discrete action
spaces. The underlying maps are fixed, which allows agents
to solve the tasks without relying on observation inputs by
simply memorizing the optimal action sequence for each map.
This significantly reduces the need for generalization across
scenarios.
SMACv2 â€” SMACv2 [17] addresses one of the key limitations of SMAC by introducing randomly generated maps
with random positions of units, which prevents memorization
of fixed action sequences and forces agents to generalize their
policies across start positions. The rest of the environment
remains the same as SMAC, with partially observable states
and discrete actions by default. Like SMAC, SMACv2 does
not include an evaluation protocol, but it can be evaluated

using the protocol proposed in [29].
SMAX (JaxMARL) â€” SMAX [19] is a JAX-based reimplementation of SMACv2 that leverages hardware acceleration for faster simulation. In addition to performance improvements, SMAX introduces the option to switch from
discrete to continuous action spaces, making it more flexible
for testing different types of MARL algorithms.
Multi-Agent Particle Environment (MPE) â€” The MultiAgent Particle Environment [22, 18] is a lightweight 2D simulator for testing cooperative, competitive, and communication
multi-agent tasks. Agents and landmarks are represented as
simple geometric shapes, and their interactions follow basic physical dynamics. MPE supports both continuous and
discrete action spaces, and scenarios can be either fully or
partially observable. Procedural generation is not used in the
default scenarios, although obstacle or landmark positions
can be randomized in some tasks. This means generalization
across different maps is not strictly required. The environment does not include a built-in evaluation protocol.
MPE (JaxMARL) â€” The JaxMARL re-implementation of
MPE [19] offers hardware acceleration through JAX, which
enables much faster simulation compared to the original
Python implementation. Functionally, it mirrors MPE in
terms of available scenarios, physics, and agent capabilities. It supports both continuous and discrete action spaces.
Like the original MPE, it does not include procedural map
generation by default and does not provide an evaluation
protocol.
JaxNav (JaxMARL) â€” JaxNav [36] is a navigationfocused benchmark implemented within the JaxMARL
framework. It places agents in a continuous 2D space where
they must navigate to goal locations while avoiding static
obstacles. The environment supports both continuous and
discrete action spaces. Maps are fixed in the default setup, so
agents do not need to generalize to unseen layouts. JaxNav is
implemented in JAX for hardware acceleration but does not
provide an evaluation protocol.
Nocturne â€” Nocturne [37] is a 2D partially observed driving simulator implemented in C++ for performance, with a
Python API for training and evaluation. It focuses on realistic
autonomous driving scenarios sourced from the Waymo Open
Dataset. The environment provides a set of benchmark tasks
in which agents control autonomous vehicles interacting with
traffic participants whose trajectories are taken from recorded
data. This setup captures complex multi-agent interactions
requiring coordination, prediction of other agentsâ€™ intentions,
and handling of partial observability.
Nocturne does not use procedurally generated maps, as all
scenes are drawn from fixed datasets. While the simulator
benefits from its C++ implementation, it does not provide
GPU acceleration, which limits its throughput for large-scale
MARL experiments.
POGEMA â€” The Partially Observable Grid Environment
for Multiple Agents [14] is a grid-based benchmark for
partially observable multi-agent pathfinding. Agents operate with ego-centric local observations and must reach in-

dividual goals while avoiding collisions. A key feature of
POGEMA is its procedural map generation, producing diverse layoutsâ€”random maps, mazes, and warehouse-style
scenariosâ€”that require agents to generalize to unseen environments. It also offers integrated evaluation protocols with
standardized metrics, enabling consistent comparison across
reinforcement learning, planning, and hybrid methods. Implemented in Python, POGEMA supports scalable multi-agent
experiments with both classical and learning-based policies.
VMAS â€” The Vectorized Multi-Agent Simulator for Collective Robot Learning [16] is a PyTorch-based, vectorized
2D physics framework designed for efficient multi-robot
benchmarking. It includes a modular interface for defining
custom scenarios, alongside a set of built-in multi-robot tasks,
that each involve a relatively small number of agents. VMAS
stands out for its GPU-accelerated, batch-simulated environments. It supports inter-agent communication and sensors
(e.g., LIDAR).
SMART â€” Scalable Multi-Agent Realistic Testbed [38]
is a physics-based simulator tailored for bridging MultiAgent Path Finding (MAPF) algorithms and real-world performance. Designed for scalability, SMART supports simulation of thousands of agents, enabling evaluation of large-scale
deployments. However, its simulation speed is relatively low
(around 335 SPS for 100 agents), which can limit large-scale
reinforcement learning experiments. The platform targets
both academic researchers and industrial users who lack access to extensive physical robot fleets, offering a realistic
environment to test MAPF performance under near-real conditions.
Gazebo â€” Gazebo is a widely used open-source 3D
robotics simulator that integrates tightly with ROS. It supports detailed physics simulation, realistic sensors (e.g., LIDAR, cameras), and high-quality rendering. While Gazebo
allows multi-robot experimentation and moderate scalability,
it does not support GPU acceleration for simulation logic GPU use is limited to rendering and sensor visuals only. This
limitation can make simulations of large multi-agent systems
slow or resource-heavy.
Webots â€” Webots is a commercial-grade 3D robotics simulator offering a wide array of built-in robot models, sensors, and actuator libraries. It provides realistic physics simulation, high-quality graphics, and ROS support. Webots
is well-suited for prototyping and academic or industrial
robotic research. However, from a multi-agent perspective,
itâ€™s not optimized for running large numbers of agents at
scaleâ€”simulation speed tends to drop significantly as agent
count grows, making massive multi-robot evaluations challenging.
ARGoS â€” ARGoS [25] is an open-source, modular simulator designed specifically for swarm robotics and large-scale
multi-agent systems. Its architecture supports running thousands of simple agents efficiently by combining multiple
physics engines and leveraging parallel computation. ARGoS is customizable, allowing researchers to define their
own robots, sensors, actuators, etc.

