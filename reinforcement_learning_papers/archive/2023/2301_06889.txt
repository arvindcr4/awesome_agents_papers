Published in Transactions on Machine Learning Research (05/2023)

Mean-Field Control based Approximation of Multi-Agent
Reinforcement Learning in Presence of a Non-decomposable
Shared Global State
Washim Uddin Mondal

wmondal@purdue.edu

arXiv:2301.06889v2 [cs.LG] 26 May 2023

School of IE and CE, Purdue University

Vaneet Aggarwal

vaneet@purdue.edu

School of IE and ECE, Purdue University

Satish V. Ukkusuri

sukkusur@purdue.edu

Lyles School of Civil Engineering, Purdue University
Reviewed on OpenReview: https: // openreview. net/ forum? id= ZME2nZMTvY

Abstract
Mean Field Control (MFC) is a powerful approximation tool to solve large-scale MultiAgent Reinforcement Learning (MARL) problems. However, the success of MFC relies on
the presumption that given the local states and actions of all the agents, the next (local)
states of the agents evolve conditionally independent of each other. Here we demonstrate
that even in a MARL setting where agents share a common global state in addition to their
local states evolving conditionally independently (thus introducing a correlation between
the state transition processes of individual agents), the MFC can still be applied as a good
approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be
expressed as a collection hof local states iof the agents. We compute the approximation error
p
p
as O(e) where e = âˆš1N
|X | + |U| . The size of the agent population is denoted by
the term N , and |X |, |U| respectively indicate the sizes of (local) state and action spaces
of individual agents. The approximation error is found to be independent of the size of
the shared global state space. We further demonstrate that in a special case if the reward
and state transition functions are independent
of the action distribution of the population,
âˆš
|X |

then the error can be improved to e = âˆšN . Finally, we devise a Natural Policy Gradient
based algorithm that solves the MFC problem with O(Ïµâˆ’3 ) sample complexity and obtains
a policy that is within O(max{e, Ïµ}) error of the optimal MARL policy for any Ïµ > 0.

1

Introduction

Adaptive decision-making by a large number of cooperative autonomous entities in the presence of a changing
environment is a frequently appearing theme in many areas of modern human endeavor such as transportation, telecommunications, and internet networks. For example, consider the ride-hailing service provided by
a large fleet of vehicles. Not only those vehicles are needed to be strategically placed to allow the maximum
number of passengers to be served but such a formidable task needs to be performed even in the presence
of spatio-temporal variation of passenger demand. In this and many other similar scenarios, the central
question is how a large number of agents can learn to cooperatively achieve a desired target. The framework
of cooperative multi-agent reinforcement learning (MARL) has been developed to answer such questions.
However, in comparison to the single-agent learning framework, the task of cooperative MARL is significantly more challenging since as the number of agents increases, the size of the joint state space increases
exponentially.
1

Published in Transactions on Machine Learning Research (05/2023)

Several heuristic approaches have been designed to circumvent the aforementioned curse of dimensionality.
Based on how the agents are trained, these approaches can be primarily classified into two categories. In
Independent Q Learning (IQL) (Tan, 1993), the agents are trained independently. In contrast, centralized
training with decentralized execution (CTDE) based methods (Rashid et al., 2018) train the agents in a
centralized fashion. Both approaches avoid the problem of state-space explosion by restricting the policies of
each agent to depend only on its local state. Despite having empirical success, none of the above approaches
can be shown to have an optimality guarantee. Another approach that has recently emerged as an excellent
approximation tool for cooperative MARL with theoretical optimality guarantee is called mean-field control
(MFC). It works on the premise that in an infinite collection of homogeneous and exchangeable agents, the
behavior of an arbitrarily chosen representative accurately reflects the behavior of the entire population.
The guarantee of MFC as an approximation tool of MARL primarily relies on the law of large numbers
which dictates that the empirical average of a large number of independent and identically distributed
random variables, with high probability, is very close to their mean value. To utilize this property, it is
commonly assumed in the mean-field literature, often implicitly, that the agents are each associated with a
local state that evolves conditionally independently of each other (Gu et al., 2021). Unfortunately, in many
of the practical scenarios, such an assumption might appear to be too restrictive. As an example, consider
the ride-sharing problem discussed before. Assume that the local state perceived by a vehicle is the location
of the potential riders in its immediate vicinity. If two vehicles are located far apart, their local states might
be assumed to evolve conditionally independently. However, if they are so close such that their pickup areas
intersect, then the presumption of independent evolution of states might not hold.
Does MFC-based approximation still hold if the local state evolution processes of different agents are correlated? This is one of the most crucial questions that need to be addressed if mean-field-based techniques
are to be adopted in a wide array of real-world MARL problems. In this paper, we establish that if each
agent, in addition to their conditionally independently evolving local states, also possesses a common global
state, the mean-field approach can still be shown to be a good approximation of MARL. The global state is
assumed to be non-decomposable i.e., in general, it cannot be expressed as a collection of some agent-specific
local components. Note that, as the global state is common to all agents, the combined state perceived by
each agent can now no longer be considered to be evolving independently. As a result, the techniques that
are commonly applied to show mean-field approximations can no longer be directly used. We address this
challenge by showing that instead of naively applying the previously developed methods, if the problems are
transformed to an equivalent but slightly different representation, then we can find new variables that evolve
conditionally independently of each other and help us break the correlation barrier.

1.1

Our Contribution

We consider a network comprising N number of agents, each associated with a local state space of size
|X |, a non-decomposable global state space of size |G| and an action space of size |U| (all are assumed to
be of finite size). Given the global state, local states, and actions of all agents at time t, local states at
t + 1 are presumed to evolve conditionally independently of each other. The combined (local, global) states
of each agent, however, do not evolve conditionally independently. We prove that the mean-field controlbased approximation results can be applied even
presence of such a correlation. We quantify the
hp in the
p i
|X | + |U| . Note that the expression of e is independent
approximation error to be O(e) where e â‰œ âˆš1N
of |G|, the size of the global state-space. In a special case where the reward function and both the local
and global state transition functions are assumed to not depend âˆšon the action distribution of the agent
population, the approximation error is shown to improve to e =
size of the action space.

|X |
âˆš
N

i.e., it becomes independent of the

In traditional MFC (with no non-decomposable global state), one of the crucial
the MARLâˆš steps in proving
N
MFC approximation error is upper bounding the term E|ÂµN
âˆ’
Âµ
|
as
O(1/
N
)
where
Âµ
,
and
Âµt are the
1
t
t
t
state-distributions of the N -agent and the infinite agent systems respectively at time t. The key intuition in
proving this bound comes from the fact that ÂµN
t (x) can be written as the average of the random variables
i
{Î´(xit = x)}N
where
x
is
the
state
of
the
ith
agent at time t. Moreover, the above-mentioned random
t
i=1
2

Published in Transactions on Machine Learning Research (05/2023)

variables are independent conditioned on ÂµN
tâˆ’1 . This allows one to use the law of large numbers and obtain
the desired bound.
If we follow
in our setting (with a non-decomposable global state), we will end up with
P the same footsteps
N
the term x,g E|ÂµN
(x)Î´(g
=
g)
âˆ’ Âµt (x)Î´(gt = g)| where gtN and gt are the non-decomposable states in the
t
t
N
N -agent and the infinite agent systems respectively. Note that, we can write ÂµN
t (x)Î´(gt = g) as the average
N
i
N
N
of the random variables {Î´(xt = x, gt = g)}i=1 . Due to the common factor Î´(gt = g), the above-mentioned
random variables are now correlated, and hence the law of large numbers can no longer be applied. This is
essentially the primary challenge addressed by our paper. We invent new techniques to prove approximation
guarantees even in the presence of a non-decomposable state. The details are provided in section 6.
Finally, we develop a natural policy gradient (NPG) based algorithm and using the result of (Liu et al.,
2020), and our own approximation guarantee, establish that the proposed algorithm generates a policy that
is within O(max{e, Ïµ}) error of the optimal MARL policy for any Ïµ > 0. Moreover, the sample complexity
bound for obtaining such a solution is shown to be O(Ïµâˆ’3 ).

1.2

Related Works

Single Agent Learning: Pioneering works in the single agent learning setup includes various finite
state/tabular algorithms such as Q-learning (Watkins and Dayan, 1992) and SARSA (Rummery and Niranjan, 1994). Despite having theoretical guarantees, these algorithms did not get adopted in large-scale
applications due to the huge memory requirement. Recently, neural network (NN) based Deep Q Learning
(DQL) (Mnih et al., 2015) and policy gradient-based algorithms (Mnih et al., 2016) have garnered popularity
due to the large expressive powers of NNs. Nevertheless, due to the exponential increase in the size of the
joint state space with the number of agents, these algorithms are far from being the panacea for large-scale
MARL problems.
CTDE-based Approaches for MARL: As stated before, the idea behind centralized training and decentralized execution (CTDE) based approaches is to restrict the policies to solely take the local state of
the associated agent as an input. Depending on how such local policies are trained, various algorithms
have been constructed. For example, VDN (Sunehag et al., 2017), trains the local policies by minimizing
the Bellman error corresponding to the sum of individual Q-functions of each agent. QMIX (Rashid et al.,
2018), on the other hand, computes the Bellman error corresponding to the state-dependent weighted sum
of the Q-functions of each agent. Various other CTDE-based algorithms are WQMIX (Rashid et al., 2020),
QTRAN (Son et al., 2019) etc. Parallel to CTDE, IQL-based algorithms have also garnered popularity in
large-scale MARL (Wei et al., 2019). Alongside these heuristics, there have been some recent efforts to
theoretically characterize the efficacy of the local policies Qu et al. (2020); Lin et al. (2021); Mondal et al.
(2022c). However, none of them include a common global state.
Mean-Field Control (MFC): MFC is a relatively recent development that solves MARL with theoretical
optimality guarantees. (Gu et al., 2021) exhibited that if all the agents are homogeneous and exchangeable,
MFC can be used as a good approximation of an N -agent problem. Later, similar approximation results
were proved for K-class of heterogeneous agents (Mondal et al., 2022a) and non-exchangeable agents (Mondal
et al., 2022b). Moreover, various model-based (Pasztor et al., 2021) and model-free (Angiuli et al., 2022)
algorithms have been developed to solve the MFC problem. We would like to point out that our framework
is closely aligned with the framework of MFC with common noise (Motte and Pham, 2022; Carmona et al.,
2019). However, (Motte and Pham, 2022) only considers open-loop policies which are essentially sequences of
actions, rather than state-to-action maps (also known as closed-loop policies). On the other hand, although
(Carmona et al., 2019) do consider closed-loop policies, they do not show the convergence between MARL
and MFC as a function of the number of agents. Empirically, MFC has found its application in a diverse
range of scenarios, including epidemic management (Watkins et al., 2016), congestion control (Wang et al.,
2020), and ride-sharing (Al-Abbasi et al., 2019).
Beyond Mean-Field Control: The presumption of homogeneity of the agents turns out to be too restrictive in many practical scenarios. Graphon mean-field control (Caines and Huang, 2019) is an emerging new
3

Published in Transactions on Machine Learning Research (05/2023)

area that attempts to do away with the presumption of homogeneity. However, as explained in (Mondal
et al., 2022b), such approaches also come with their own set of restrictions.
Mean-Field Games: Similar to MFC, mean-field games (MFG) attempt to characterize the behavior of
an infinite number of agents in a non-cooperative setup. In contrast to MFC, the goal of MFG is to identify
the Nash equilibrium of the system (Elie et al., 2020; Yang et al., 2017).

2

MARL with Shared Global State

We consider a collection of N interacting agents each with a local state space X and an action space U. At
instant t, the local state and action of the ith agent are respectively indicated by xit , uit . In addition to the
local state, at time t, each agent also observes a global state gtN whose realizations are from the global state
N
space, G. The collection of local states and actions of all agents are expressed as xN
t , ut respectively. Given
N
N
N
the tuple (xt , gt , ut ), the local state of ith agent at time t + 1 is given by the following transition law,
N
N
N
N
N
N
xit+1 âˆ¼ Pi (xN
t , gt , ut ). Similarly, the transition law for the global state is given as gt+1 âˆ¼ PG (xt , gt , ut ).
i
N
N
N
N
N
It is assumed that the random variables {{xt+1 }i=1 , gt+1 } are independent, conditioned on {xt , gt , ut }.
N
N
N
N
At time t, the (expected) reward received by the ith agent is denoted as ri (xN
t , gt , ut ). Let Âµt , Î½ t indicate
the empirical state and action distributions of N agents at instant t which are defined respectively as follows.
ÂµN
t (x) â‰œ

N

1 X
Î´ xit = x , âˆ€x âˆˆ X
N i=1

(1)

Î½N
t (u) â‰œ

N

1 X
Î´ uit = u , âˆ€u âˆˆ U
N i=1

(2)

where Î´(Â·) is an indicator function. We assume the agents to be homogeneous and exchangeable. Hence, the
reward and state transition functions can be written as follows.
N
N
i
i
N
N
N
ri (xN
t , gt , ut ) = r(xt , ut , Âµt , gt , Î½ t )

(3)

N
N
i
i
N
N
N
Pi (xN
t , gt , ut ) = P (xt , ut , Âµt , gt , Î½ t )
N
N
N
N
N
PG (xN
t , gt , ut ) = PG (Âµt , gt , Î½ t )

(4)
(5)

where r, P, PG are given as follows: r : X Ã—U Ã—âˆ†(X )Ã—G Ã—âˆ†(U) â†’ R, P : X Ã—U Ã—âˆ†(X )Ã—G Ã—âˆ†(U) â†’ âˆ†(X )
and PG : âˆ†(X ) Ã— G Ã— âˆ†(U) â†’ âˆ†(G). The symbol âˆ†(S) defines the probability simplex defined over the set
S. Note that (5) is written with a slight abuse of notations.
A policy Ï€t is a function of the form, Ï€t : X Ã— âˆ†(X ) Ã— G â†’ âˆ†(U). In simple terms, a policy is a recipe for
the agents to (probabilistically) choose actions based on their current local states, the empirical local state
distribution of all the agents, and the global state1 . Let Ï€ â‰œ {Ï€t }tâˆˆ{0,1,Â·Â·Â· } be a sequence of policies. The
N
value generated by the sequence Ï€ corresponding to the initial state (xN
0 , g0 ) is defined as follows.
"âˆž
#
N
X
1 X
N
N
t
N
N
N
E
Î³ ri (xt , gt , ut )
VN (x0 , g0 , Ï€) â‰œ
N i=1
t=0
(6)
"âˆž
#
N
X
1 X
N
N
=
E
Î³ t r(xit , uit , ÂµN
t , gt , Î½ t )
N i=1
t=0
where the expectation is taken over all trajectories generated by the policy sequence Ï€ starting from the
N
initial states (xN
0 , g0 ) and Î³ âˆˆ (0, 1) is the discount factor. The target of MARL is to compute a policy
N
âˆž
sequence that maximizes VN (xN
â‰œ Î Ã—Î Ã—Â· Â· Â· where Î 
0 , g0 , Â·) over the set of admissible policy sequences Î 
is the set of admissible policies. In the next section, we shall discuss the mean-field control (MFC) framework
that can be used to approximately solve the MARL problem.
1 Here we implicitly assume that all agents execute the same policy. This is primarily because the agents are presumed to
have the same reward function and state-transition function (Gu et al., 2021; Pasztor et al., 2021).

4

Published in Transactions on Machine Learning Research (05/2023)

3

Mean-Field Control (MFC) Framework

In this setting, we consider the size of the agent population to be infinite. Due to homogeneity, we can
arbitrarily select a representative agent whose local state and action at time t are defined as xt , and ut
respectively. In addition, let gt indicate the global state at time t. Let Âµt , Î½ t be the distributions of local
states and actions at time t over the infinite agent population. For a given sequence Ï€ = {Ï€t }tâˆˆ{0,1,Â·Â·Â· } ,
define the following.
X
Î½ t = Î½ MF (Âµt , gt , Ï€t ) â‰œ
Ï€t (x, Âµt , gt )Âµt (x)
(7)
xâˆˆX

The above relation demonstrates how the action distribution Î½ t can be obtained from the local state distribution Âµt and the global state gt . Now we quantitatively describe how Âµt+1 , the local state distribution at
t + 1, can be obtained from Âµt an gt .
XX
Âµt+1 =
P (x, u, Âµt , gt , Î½ MF (Âµt , gt , Ï€t )) Ã— Ï€t (x, Âµt , gt )(u)Âµt (x) â‰œ P MF (Âµt , gt , Ï€t )
(8)
xâˆˆX uâˆˆU

Define Î»t â‰œ PG (Âµtâˆ’1 , gtâˆ’1 , Î½ tâˆ’1 ), t â‰¥ 1 and Î»0 â‰œ 1(g0 ) where 1(g0 ) indicates a one-hot vector with a
nonzero element at the position corresponding to g0 . Intuitively, Î»t denotes the conditional distribution of
gt given (Âµtâˆ’1 , gtâˆ’1 , Î½ tâˆ’1 ). Note that the following relation holds âˆ€t â‰¥ 0.
Î»t+1 = PGMF (Âµt , gt , Ï€t ) â‰œ PG (Âµt , gt , Î½ MF (Âµt , gt , Ï€t ))
Finally, the average reward at time t is expressed as follows.
XX
rMF (Âµt , gt , Ï€t ) =
Ï€t (x, Âµt , gt )(u) Ã— Âµt (x) Ã— r(x, u, Âµt , gt , Î½ MF (Âµt , gt , Ï€t ))

(9)

(10)

xâˆˆX uâˆˆU

The mean-field value generated by Ï€ = {Ï€t }tâˆˆ{0,1,Â·Â·Â· } for a local state distribution Âµ0 and global state g0 is
expressed as follows.
Vâˆž (Âµ0 , g0 , Ï€) =

âˆž
X



Î³ t E rMF (Âµt , gt , Ï€t )

(11)

t=0

where the expectation is obtained over {gt }tâˆˆ{1,2Â·Â·Â· } where gt+1 âˆ¼ PGMF (Âµt , gt , Ï€t ), Âµt+1 = P MF (Âµt , gt , Ï€t ),
t â‰¥ 0. The goal of MFC is to maximize the function Vâˆž (Âµ0 , g0 , Â·) over the set of admissible policy sequences,
Î âˆž . In the following section, we show that the optimal value of MARL is approximately equal to its
associated optimal MFC value.

4

Approximation Result

We shall first dictate some assumptions that are needed to establish the main result.
Assumption 1. The functions r, P and PG are assumed to follow the following relations âˆ€Âµ1 , Âµ2 âˆˆ âˆ†(X ),
âˆ€Î½ 1 , Î½ 2 âˆˆ âˆ†(U), âˆ€x âˆˆ X , âˆ€u âˆˆ U, and âˆ€g âˆˆ G
(a) |r(x, u, Âµ1 , g, Î½ 1 )| â‰¤ MR
(b) |r(x, u, Âµ1 , g, Î½ 1 ) âˆ’ r(x, u, Âµ2 , g, Î½ 2 )| â‰¤ LR {|Âµ1 âˆ’ Âµ2 |1 + |Î½ 1 âˆ’ Î½ 2 |1 }
(c) |P (x, u, Âµ1 , g, Î½ 1 ) âˆ’ P (x, u, Âµ2 , g, Î½ 2 )|1 â‰¤ LP {|Âµ1 âˆ’ Âµ2 |1 + |Î½ 1 âˆ’ Î½ 2 |1 }
(d) |PG (Âµ1 , g, Î½ 1 ) âˆ’ PG (Âµ2 , g, Î½ 2 )|1 â‰¤ LG {|Âµ1 âˆ’ Âµ2 |1 + |Î½ 1 âˆ’ Î½ 2 |1 }
The constants LR , LP are arbitrary positive numbers. The function | Â· |1 denotes the L1 -norm.
5

Published in Transactions on Machine Learning Research (05/2023)

Assumption 1(a) states that the reward function is bounded within the finite interval [âˆ’MR , MR ]. On the
other hand, assumption 1(b), 1(c), and 1(d) respectively dictates that the reward function, r, the local state
transition function, P and the global state transition function, PG are all Lipschitz continuous with respect
to their local state distribution and action distribution arguments. Such assumptions are common in the
literature (Pasztor et al., 2021; Hinderer, 2005; Gu et al., 2021). We would like to point out that although
the state and action distributions are treated as continuous variables, in an N -agent problem, they can only
take a finite number of values in their respective probability simplexes. In the MFC problem, however, these
variables can be arbitrary. Therefore, while comparing the N -agent and the MFC problem, we are implicitly
extending the domain of definition for the reward and the state transition functions.
Assumption 2. The set of admissible policies, Î  is such that any Ï€ âˆˆ Î  satisfies the following inequality
âˆ€x âˆˆ X , âˆ€Âµ1 , Âµ2 âˆˆ âˆ†(X ), and âˆ€g âˆˆ G.
|Ï€(x, Âµ1 , g) âˆ’ Ï€(x, Âµ2 , g)| â‰¤ LQ |Âµ1 âˆ’ Âµ2 |1
Assumption 2 states that the set of admissible policies is selected such that each of its elements is Lipschitz
continuous with respect to their local state distribution argument. Such assumption typically holds for
neural network (NN) based policies with bounded weights (Mondal et al., 2022a; Pasztor et al., 2021; Cui
and Koeppl, 2021).
We are now ready to state the main result. The proof of the theorem stated below is relegated to Appendix
A.
Theorem 1. Let x0 â‰œ {xi0 }iâˆˆ{1,Â·Â·Â· ,N } and g0 be the initial states and Âµ0 denote the empirical distribution
of x0 . If Assumption 1 holds and the set of admissible policies, Î  satisfies Assumption 2, then the following
relation is true whenever Î³SP < 1.
| sup VN (x0 , g0 , Ï€) âˆ’ sup Vâˆž (Âµ0 , g0 , Ï€)|
Ï€

Ï€

r
p !
MR + LR |U|
1
|U| MR LG Î³
âˆš +
â‰¤ sup |VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤
1
âˆ’
Î³
N (1 âˆ’ Î³)2
Ï€
N
"
#

 


p i
CP
MR SG
1
Î³MR SG
1 hp
1
âˆš
+
|X
|
+
|U|
+ SR
âˆ’
âˆ’
Ã—
SP âˆ’ 1
SP âˆ’ 1
1 âˆ’ Î³SP
1âˆ’Î³
(1 âˆ’ Î³)2
N
where SP â‰œ 1 + 2LP + LQ (1 + LP ), SR â‰œ MR + 2LR + LQ (MR + LR ), SG â‰œ LG (2 + LQ ) and CP â‰œ 2 + LP .
Suprema are performed over the class of all admissible policy sequences, Î âˆž .
Theorem 1 states that if the discount factorÎ³ is hsufficiently small,
then under assumptions 1 and 2, the
p
p i
1
âˆš
optimal N -agent value function is at most O
|X | + |U| error away from the optimal mean-field
N
value function. In other words, if the number of agents, N is large and the sizes of local states and actions
of individual agents is sufficiently small, then an optimal solution of MFC well approximates the optimal
solution of a MARL problem as described in section 2. Interestingly, notice that the approximation error
does not depend on the size of global state space |G|. Thus, the approximation error can be kept small even
when |G| is dramatically large or even potentially infinite.
Let, Ï€ MF
âˆˆ Î âˆž be an admissible policy sequence that solves the MFC problem with Ïµ accuracy. Note that,
Ïµ
| sup VN (x0 , g0 , Ï€) âˆ’ VN (x0 , g0 , Ï€ MF
Ïµ )| â‰¤ | sup VN (x0 , g0 , Ï€) âˆ’ sup Vâˆž (Âµ0 , g0 , Ï€)|
Ï€
Ï€
Ï€
{z
}
|
â‰œJ1
MF
MF
+ | sup Vâˆž (Âµ0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€ MF
Ïµ )| + |Vâˆž (Âµ0 , g0 , Ï€ Ïµ ) âˆ’ VN (x0 , g0 , Ï€ Ïµ )|
|
{z
}
Ï€
|
{z
}
â‰œJ3
â‰œJ2

The terms J1 , J3 can be bounded by Theorem 1 while J2 can be bounded by Ïµ. This result suggests that if we
can come up with a way to approximately solve the MFC problem, then that solution must be a good proxy
6

Published in Transactions on Machine Learning Research (05/2023)

for the optimal MARL solution. Before discussing the algorithmic aspects of solving the MFC problem, we
would like to first discuss a special case where the approximation error can be lower in comparison to that
stated in Theorem 1.

5

Improvement in a Special Case

In this section, we shall demonstrate that if certain structural restrictions are imposed on the reward and
state transition functions, then the approximation error stated in Theorem 1 can be improved further. In
particular, the assumption imposed on the stated functions can be mathematically described as follows.
Assumption 3. The following relations hold âˆ€Âµ âˆˆ âˆ†(X ), âˆ€Î½ âˆˆ âˆ†(U), âˆ€x âˆˆ X , âˆ€g âˆˆ G, âˆ€u âˆˆ U,
(a) r(x, u, Âµ, g, Î½) = r(x, u, Âµ, g),
(b) P (x, u, Âµ, g, Î½) = P (x, u, Âµ, g),
(c) PG (Âµ, g, Î½) = PG (Âµ, g)
Note that the above relations are stated with a slight abuse of notations.
Assumption 3 dictates that the reward function, r, the local state transition function, P , and the global
state transition function, PG are independent of the action distribution of the population. We would like to
point out that the functions r and P may still depend on the action of the associated agent even if they are
independent of the actions of others. In Theorem 2, we discuss the implication of Assumption 3. The proof
of Theorem 2 is relegated to Appendix N.
Theorem 2. Let x0 â‰œ {xi0 }iâˆˆ{1,Â·Â·Â· ,N } and g0 be the initial states and Âµ0 indicate the empirical distribution of
x0 . If assumptions 1 and 3 hold and the set of admissible policies, Î  obeys assumption 2, then the following
relation is true whenever Î³QP < 1,
p


|X |
MR
1
âˆš + âˆš
| sup VN (x0 , g0 , Ï€) âˆ’ sup Vâˆž (Âµ0 , g0 , Ï€)| â‰¤
1âˆ’Î³
Ï€
Ï€
N
N
#

"


MR L G
Î³MR LG
2
1
1
Ã—
+ QR
âˆ’
âˆ’
QP âˆ’ 1
QP âˆ’ 1
1 âˆ’ Î³QP
1âˆ’Î³
(1 âˆ’ Î³)2
where QP â‰œ 1 + LP + LQ and QR â‰œ MR (1 + LQ ) + LR . Suprema are computed over the set of all admissible
policy sequences, Î âˆž .
Theorem 2 dictates that if the reward, r, and state transition functions, P and PG are independent of
the action
âˆš  distribution of the entire agent population, then the approximation error can be improved to
O

|X |
âˆš
N

. Therefore, in addition to large global state space, such system can also afford to have large

action space without compromising the MFC based approximation accuracy.

6

Proof Outline

Here we present a brief outline of the proof of Theorem 1. The proof of Theorem 2 is similar. In order to
describe the proof steps, the infinite agent value function given in (11) needs to be written in an equivalent
but slightly different representation.
6.1

An Equivalent Representation

Let us focus on the expected infinite-agent reward at time t, denoted by the expression E[rMF (Âµt , gt , Ï€t )].
Note that, starting from Âµ0 , g0 , the sequence of local state distribution and global states {(Âµl , gl )}, l âˆˆ
{1, Â· Â· Â· , t} can be recursively obtained as follows.
Âµl+1 = P MF (Âµl , gl , Ï€l )
gl+1 âˆ¼ PGMF (Âµl , gl , Ï€l ),
7

(12)
l âˆˆ {0, Â· Â· Â· , t âˆ’ 1}

(13)

Published in Transactions on Machine Learning Research (05/2023)

where P MF , PGMF are defined in (8), (9) respectively. Note that (12) is a deterministic relation. Therefore, if
we fix a particular realization of the global states {gl }, l âˆˆ {1, Â· Â· Â· , t âˆ’ 1}, then, by recursively applying (12),
Âµl+1 can be written as follows, âˆ€l âˆˆ {0, Â· Â· Â· , t âˆ’ 1}.
Âµl+1 = PÌƒ MF (Âµ0 , g0:l , Ï€0:l )
â‰œ P MF (Â·, gl , Ï€l ) â—¦ P MF (Â·, glâˆ’1 , Ï€lâˆ’1 ) â—¦ Â· Â· Â· â—¦ P MF (Â·, g1 , Ï€1 ) â—¦ P MF (Â·, g0 , Ï€0 )(Âµ0 )

(14)

where â—¦ indicates function composition, g0:l â‰œ {g0 , Â· Â· Â· , gl } and Ï€0:l â‰œ {Ï€0 , Â· Â· Â· , Ï€l }. On the other hand, recursively using (13), the probability of occurrence of a particular realisation of the sequence g1:t â‰œ {g1 , Â· Â· Â· , gt }
can be written as follows.
P(g1:t |Âµ0 , g0 , Ï€0:tâˆ’1 ) â‰œ PGMF (Âµ0 , g0 , Ï€0 )(g1 ) Ã— Â· Â· Â· Ã— PGMF (Âµtâˆ’1 , gtâˆ’1 , Ï€tâˆ’1 )(gt )

(15)

For notational convenience, we denote this conditional joint probability as PÌƒGMF (Âµ0 , g0:tâˆ’1 , Ï€0:tâˆ’1 )(gt ). Invoking (14), this can be alternatively written as follows.
PÌƒGMF (Âµ0 , g0:tâˆ’1 , Ï€0:tâˆ’1 )(gt ) = PGMF (Âµ0 , g0 , Ï€0 )(g1 ) Ã— PGMF (PÌƒ MF (Âµ0 , g0:0 , Ï€0:0 ), g1 , Ï€1 )(g2 )
Ã— Â· Â· Â· Ã— PGMF (PÌƒ MF (Âµ0 , g0:tâˆ’2 , Ï€0:tâˆ’2 ), gtâˆ’1 , Ï€tâˆ’1 )(gt )

(16)

Using these notations, we can now define rÌƒMF (Âµ0 , g0 , Ï€0:t ) â‰œ E[rMF (Âµt , gt , Ï€t )] as follows.
rÌƒMF (Âµ0 , g0 , Ï€0:t ) â‰œ

X

PÌƒGMF (Âµ0 , g0:tâˆ’1 , Ï€0:tâˆ’1 )(gt ) Ã— rMF (PÌƒ MF (Âµ0 , g0:tâˆ’1 , Ï€0:tâˆ’1 ), gt , Ï€t )

(17)

1:t

P
where 1:t is the summation operation over g1:t âˆˆ G t for t â‰¥ 1. For t = 0, we define rÌƒMF (Âµ0 , g0 , Ï€0:0 ) â‰œ
rMF (Âµ0 , g0 , Ï€0 ).
6.2

Proof Outline

With the new representation defined above, we are now ready to sketch a brief outline of the proof.
Step 0: Recall from the definitions (6) and (11) that both N -agent and infinite agent value functions are
Î³-discounted sum of expected rewards. To obtain their difference corresponding to a given policy sequence,
Ï€ = {Ï€t }tâˆˆ{0,1,Â·Â·Â· } , we therefore must calculate the difference between the expected N -agent reward at time t
and the expected infinite agent reward at time t. Mathematically, this difference can be expressed as follows.

âˆ†Rt â‰œ

N


1 X  i i N N N 
E r(xt , ut , Âµt , gt , Î½ t ) âˆ’ E rMF (Âµt , gt , Ï€t )
N i=1

where all the notations are the same as used in sections 2 and 3.
Step 1: We upper bound âˆ†Rt as âˆ†Rt â‰¤ âˆ†Rt1 + âˆ†Rt2 . The first term is given as follows.
N

âˆ†Rt1 â‰œ E

We show that âˆ†Rt1 = O

1 X
N
N
MF
N
r(xit , uit , ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’ r
t , gt , Ï€t )
N i=1

p

|U|/N in Lemma 13 (Appendix A.3). The second term âˆ†Rt2 is the following.
N
MF
âˆ†Rt2 â‰œ E[rMF (ÂµN
(Âµt , gt , Ï€t )]
t , gt , Ï€t )] âˆ’ E[r

8

(18)

Published in Transactions on Machine Learning Research (05/2023)

N
MF
N
Step 2: Using the definition of rÌƒMF in section 6.1, we can write, E[rMF (ÂµN
(ÂµN
t , gt , Ï€t )] = E[rÌƒ
t , gt , Ï€t:t )]
and E[rMF (Âµt , gt , Ï€t )] = rÌƒMF (Âµ0 , g0 , Ï€0:t ) = E[rÌƒMF (Âµ0 , g0 , Ï€0:t )]. We now further bound âˆ†Rt2 as follows.
N
MF
âˆ†Rt2 = E[rÌƒMF (ÂµN
(Âµ0 , g0 , Ï€0:t )]
t , gt , Ï€t:t )] âˆ’ E[rÌƒ
(a)

=

tâˆ’1
X

N
MF
N
E[rÌƒMF (ÂµN
(ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E[rÌƒ
k , gk , Ï€k:t )]

k=0

â‰¤

(19)

tâˆ’1
X

N
MF
N
E[rÌƒMF (ÂµN
(ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E[rÌƒ
k , gk , Ï€k:t )]
|
{z
}
k=0
2
â‰œâˆ†Rk,t

N
In the above inequality, we implicitly use the convention that ÂµN
0 = Âµ0 , g0 = g0 . Equality (a) is essentially
a telescoping series.
2
Step 3: We shall now focus on the first term of âˆ†Rk,t
. Note that,
N
E[rÌƒMF (ÂµN
k+1 , gk+1 , Ï€k+1:t )]
  MF N

N
N
N
= E E rÌƒ (Âµk+1 , gk+1
, Ï€k+1:t ) ÂµN
k , gk , Î½ k
ï£® ï£®
ï£¹ï£¹
X
(a)
N
N
N
N
N
N ï£»ï£»
rÌƒMF (ÂµN
= E ï£°E ï£°
k+1 , g, Ï€k+1:t )PG (Âµk , gk , Î½ k )(g) Âµk , gk , Î½ k
gâˆˆG

ï£®
= Eï£°

ï£¹
X

N
N
N
ï£»
rÌƒMF (ÂµN
k+1 , g, Ï€k+1:t )PG (Âµk , gk , Î½ k )(g)

gâˆˆG
N
N
N
N
N
where (a) follows from the fact that ÂµN
k+1 , gk+1 are conditionally independent given Âµk , gk , Î½ k and gk+1 âˆ¼
N
N
PG (ÂµN
k , gk , Î½ k ).
2
Step 4: Using Lemma 9 (stated in Appendix A.2), we can expand the second term of âˆ†Rk,t
as follows.

"
E[rÌƒ

MF

N
(ÂµN
k , gk , Ï€k:t )] = E

#
X

N
MF
N
MF
N
PG (ÂµN
(ÂµN
(P MF (ÂµN
k , gk , Î½
k , gk , Ï€k ))(g) Ã— rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t )

gâˆˆG

Step 5: Lemma 8 (Appendix A.2) shows that rÌƒMF (Â·, g, Ï€k+1:t ) is Lipschitz continuous with a (k, t)-dependent
Lipschitz parameter for any g, Ï€k+1:t . Therefore, one can write the following.

MF
N
N
MF
N
rÌƒMF (ÂµN
(P MF (ÂµN
(ÂµN
k+1 , g, Ï€k+1:t ) âˆ’ rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t ) = O Âµk+1 âˆ’ P
k , gk , Ï€k ) 1
The leading constants in the above bound are (k, t) dependent. In Lemma 12 (Appendix A.3), we further
show that,


p i
1 hp
MF
N
N
âˆš
|X
|
+
|U|
E ÂµN
âˆ’
P
(Âµ
,
g
,
Ï€
)
=
O
k 1
k+1
k
k
N
N
N
Using Assumption 1(d), one can write the following for any choice of ÂµN
k , gk , Î½ k and Ï€k .
N
N
N
N
MF
N
N
MF
N
|PG (ÂµN
(ÂµN
(ÂµN
k , gk , Î½ k ) âˆ’ PG (Âµk , gk , Î½
k , gk , Ï€k ))|1 â‰¤ LG Î½ k âˆ’ Î½
k , gk , Ï€k ) 1

Moreover, in Lemma 11 (Appendix A.3), we show that,
p
E

MF
N
Î½N
(ÂµN
k âˆ’Î½
k , gk , Ï€k ) 1 = O

9

|U|
âˆš
N

!

Published in Transactions on Machine Learning Research (05/2023)

Combining all these results with the expansions obtained in steps 3 and 4, we finally conclude that,


p i
1 hp
2
âˆ†Rk,t
=O âˆš
|X | + |U|
N
where the leading constants are (k, t)-dependent.
2
Step 6: Substituting the bound of âˆ†Rk,t
in (19), we can obtain a bound for âˆ†Rt2 which, combined with
the previously obtained bound for âˆ†Rt1 , yields a bound for âˆ†Rt . We finally obtain the difference
Pâˆž between
N -agent and infinite agent value corresponding to Ï€ by computing the upper bound of the sum t=0 Î³ t âˆ†Rt .
We would like to point out here that the leading coefficient of âˆ†Rt turns out to be an exponential function
of t. In order for the sum to converge, one must have Î³ to be sufficiently small.

Step 7: We establish the desired result by observing that | supÏ€ VN (x0 , g0 , Ï€) âˆ’ supÏ€ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤
supÏ€ |VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)| where the suprema are calculated over the class of all admissible policy
sequences Î âˆž .
We would like to conclude this section by pointing out how our proof technique fundamentally differs from
the techniques used in the existing papers such as Mondal et al. (2022a); Gu et al. (2021) where the common
global state is not considered. Note that in absence2 of the shared state-space G, the infinite agent state
distributions {Âµt }tâˆˆ{0,1,Â·Â·Â· } are all deterministic and the error âˆ†Rt2 defined in (18) can be bounded as,
MF
âˆ†Rt2 â‰œ E[rMF (ÂµN
(Âµt , Ï€t )]
t , Ï€t )] âˆ’ E[r
(a)

MF
â‰¤ E rMF (ÂµN
(Âµt , Ï€t ) = O E|ÂµN
t , Ï€t ) âˆ’ r
t âˆ’ Âµt |1

(20)


Relation (a) is proven by establishing that rMF (Â·, Ï€t ) is Lipschitz continuous. The error term E|ÂµN
t âˆ’ Âµt |1
shown in (20) is bounded by establishing a recursion on t. Unfortunately, such simplified recursion does not
hold once the global state space is introduced. In our analysis, we rather need to keep track of the whole
trajectory induced by the policy sequence, Ï€k:t for k < t. The need to trace out convoluted trajectories and
the lack of any simplified recursion makes our analysis much more difficult and incompatible with the proof
techniques available in the literature.

7

Algorithm to solve MFC

In this section, we present a natural policy gradient (NPG) based algorithm to obtain an optimal policy for
the mean-field control (MFC) problem. As clarified in section 3, the size of the agent population is presumed
to be infinite in an MFC framework and we can arbitrarily select any agent to be a representative. Assume
that at time t the representative agent chooses an action ut âˆˆ U after observing its local state xt âˆˆ X , the
global state gt âˆˆ G, and the distribution of local states of all agents denoted by Âµt âˆˆ âˆ†(X ). The goal of
MFC, therefore, reduces to maximizing the discounted expected reward of this representative agent. Clearly,
it is a single agent Markov Decision Process (MDP) with a state space of X Ã— âˆ†(X ) Ã— G and action space U.
Without loss of generality, we can assume that the optimal policy sequence is stationary (Puterman, 2014,
Theorem 6.2.12). Note that the collection of admissible policies is denoted by the set Î . We assume that
the elements of Î  are characterized by a dâˆ’dimensional parameter Î¦ âˆˆ Rd . In the forthcoming discussion, a
policy with the parameter Î¦ will be denoted as Ï€Î¦ whereas, with a slight abuse of notations, the stationary
sequence generated by it will also be denoted as Ï€Î¦ . Let the Q-value corresponding to the policy Ï€Î¦ be
defined as follows âˆ€x, Âµ, g, u.
" âˆž
#
X
t
QÎ¦ (x, Âµ, g, u) â‰œ E
Î³ r(xt , ut , Âµt , gt , Î½ t ) x0 = x, Âµ0 = Âµ, g0 = g, u0 = u
(21)
t=0

where the quantities Âµt and Î½ t are recursively calculated by applying (8) and (7) respectively. Moreover, the
expectation is taken over xt+1 âˆ¼ P (xt , ut , Âµt , gt , Î½ t ), ut âˆ¼ Ï€Î¦ (xt , Âµt , gt ), and gt+1 âˆ¼ PG (Âµt , gt , Î½ t ), âˆ€t â‰¥ 0.
2 This can be considered a special case of our model by imposing |G| = 1.

10

Published in Transactions on Machine Learning Research (05/2023)

The advantage function corresponding to Ï€Î¦ is defined as follows.
AÎ¦ (x, Âµ, g, u) â‰œ QÎ¦ (x, Âµ, g, u) âˆ’ E[QÎ¦ (x, Âµ, g, uâ€² )]

(22)

where the expectation is over uâ€² âˆ¼ Ï€Î¦ (x, Âµ, g).
âˆ—
Let Âµ0 âˆˆ âˆ†(X ) be the distribution of initial local states and g0 be the initial global state. Define Vâˆž
(Âµ0 , g0 ) â‰œ
supÎ¦âˆˆRd Vâˆž (Âµ0 , g0 , Ï€Î¦ ) to be the optimal mean-field value obtained over the collection of admissible policies,
Î  corresponding to the initial state (Âµ0 , g0 ). Assume that {Î¦j }Jj=1 is a sequence of dâˆ’dimensional parameters
generated by the NPG algorithm (Liu et al., 2020; Agarwal et al., 2021) as follows.

Î¦j+1 = Î¦j + Î·wj , wj â‰œ arg minwâˆˆRd LÎ¶ Î¦j

(w, Î¦j )

(23)

(Âµ0 ,g0 )

where Î· > 0 is the learning parameter. The function LÎ¶ Î¦j

(Âµ0 ,g0 )

Î¦

and the occupancy measure Î¶(Âµj ,g0 ) are defined
0

below.
LÎ¶ Î¦â€²

(Âµ0 ,g0 )

â€²

(w, Î¦) â‰œE(x,Âµ,g,u)âˆ¼Î¶ Î¦â€²

(Âµ0 ,g0 )

Î¦
Î¶(Âµ
(x, Âµ, g, u) â‰œ
0 ,g0 )

âˆž
X

h
2 i
AÎ¦ (x, Âµ, g, u) âˆ’ (1 âˆ’ Î³)wT âˆ‡Î¦ log Ï€Î¦ (x, Âµ, g)(u)
,

(24)

Î³ Ï„ P(xÏ„ = x, ÂµÏ„ = Âµ, gÏ„ = g, uÏ„ = u|x0 = x, Âµ0 = Âµ, g0 = g, u0 = u, Ï€ Î¦â€² )(1 âˆ’ Î³)

Ï„ =0

(25)
Note that in order to calculate the update direction, wj at the j-th NPG update (23), we must solve another
optimization problem over Rd . The second minimization problem can be handled via a stochastic gradient
(Â·, Î¦) can be obtained
descent (SGD) approach. Particularly, for a given Î¦j âˆˆ Rd , the minimizer of LÎ¶ Î¦j
(Âµ0 ,g0 )

via the following SGD updates: wj,l+1 = wj,l âˆ’ Î±hj,l (Liu et al., 2020) where Î± > 0 is the learning parameter
for the sub-problem and hj,l , the gradient at the l-th iteration is given as follows.
!
1
T
hj,l â‰œ wj,l
âˆ‡Î¦j log Ï€Î¦j (x, Âµ, g)(u) âˆ’
AÌ‚Î¦ (x, Âµ, g, u) âˆ‡Î¦j log Ï€Î¦j (x, Âµ, g)(u)
(26)
1âˆ’Î³ j
Î¦

where the tuple (x, Âµ, g, u) is sampled from the occupancy measure Î¶Î»0j , and AÌ‚Î¦j (x, Âµ, g, u) denotes a unbiased estimator of AÎ¦j (x, Âµ, g, u). The sampling procedure and the method to obtain the estimator has been
described in Algorithm 2 in Appendix V. It is to be clarified that Algorithm 2 is based on Algorithm 3 of
(Agarwal et al., 2021). The whole NPG procedure is summarized in Algorithm 1.
Based on the result (Theorem 4.9) of (Liu et al., 2020), in Lemma 1, we establish the convergence of
Algorithm 1 and characterize its sample complexity. However, the following assumptions are needed to
prove the Lemma. The assumptions stated below are respectively similar to Assumptions 2.1, 4.2, 4.4 of
(Liu et al., 2020).
Assumption 4. âˆ€Î¦ âˆˆ Rd , âˆ€(Âµ0 , g0 ) âˆˆ âˆ†(X ) Ã— G, for some Ï‡ > 0, F(Âµ0 ,g0 ) (Î¦) âˆ’ Ï‡Id is positive semi-definite
where F(Âµ0 ,g0 ) (Î¦) is defined as stated below.
h
i
T
F(Âµ0 ,g0 ) (Î¦) â‰œE(x,Âµ,g,u)âˆ¼Î¶ Î¦
{âˆ‡Î¦ Ï€Î¦ (x, Âµ, g)(u)} Ã— {âˆ‡Î¦ log Ï€Î¦ (x, Âµ, g)(u)}
(Âµ0 ,g0 )

Assumption 5. âˆ€Î¦ âˆˆ Rd , âˆ€Âµ âˆˆ âˆ†(X ), âˆ€x âˆˆ X , âˆ€g âˆˆ G, âˆ€u âˆˆ U, the following holds
|âˆ‡Î¦ log Ï€Î¦ (x, Âµ, g)(u)|1 â‰¤ G
for some positive constant G.
Assumption 6. âˆ€Î¦1 , Î¦2 âˆˆ Rd , âˆ€Âµ âˆˆ âˆ†(X ), âˆ€x âˆˆ X , âˆ€g âˆˆ G, âˆ€u âˆˆ U, the following holds,
|âˆ‡Î¦1 log Ï€Î¦1 (x, Âµ, g)(u) âˆ’ âˆ‡Î¦2 log Ï€Î¦2 (x, Âµ, g)(u)|1 â‰¤ M |Î¦1 âˆ’ Î¦2 |1
for some positive constant M .
11

Published in Transactions on Machine Learning Research (05/2023)

Algorithm 1 Natural Policy Gradient
1: Input: Î·, Î±: Learning rates, J, L: Number of execution steps

w0 , Î¦0 : Initial parameters,
Âµ0 : Initial local state distribution
g0 : Initial global state
2: Initialization: Î¦ â† Î¦0
3: for j âˆˆ {0, 1, Â· Â· Â· , J âˆ’ 1} do
4:
wj,0 â† w0
5:
for l âˆˆ {0, 1, Â· Â· Â· , L âˆ’ 1} do
Î¦
6:
Sample (x, Âµ, g, u) âˆ¼ Î¶(Âµj ,g0 ) and AÌ‚Î¦j (x, Âµ, g, u) using Algorithm 2
0
7:
Compute hj,l using (26)
8:
wj,l+1 â† wj,l âˆ’ Î±hj,l
9:
end for
1 PL
wj,l
10:
wj â†
L l=1
11:
Î¦j+1 â† Î¦j + Î·wj
12: end for
13: Output: {Î¦1 , Â· Â· Â· , Î¦J }: Policy parameters

Assumption 7. âˆ€Î¦ âˆˆ Rd , âˆ€(Âµ0 , g0 ) âˆˆ âˆ†(X ) Ã— G,
LÎ¶ Î¦âˆ—

(Âµ0 ,g0 )

âˆ—
âˆ—
(wÎ¦
, Î¦) â‰¤ Ïµbias , wÎ¦
â‰œ arg minwâˆˆRd LÎ¶ Î¦

(Âµ0 ,g0 )

(w, Î¦)

where Î¦âˆ— is the parameter of the optimal policy.
Remark 1. Note that Assumption 7 is trivially satisfied with Ïµbias = 2MR /(1 âˆ’ Î³). Assume that, U = {1, 2},
and a restricted class of parameterized policies is defined as follows.


exp(Î¦)
1
Ï€Î¦ (x, Âµ, g) =
1 + exp(Î¦) 1 + exp(Î¦)

T

where (x, Âµ, g) âˆˆ X Ã— âˆ†(X ) Ã— G, and Î¦ âˆˆ [âˆ’Î¾, Î¾] for some constant, Î¾ > 0. One can check that, for the set of
policies stated above, assumptions 4 âˆ’ 6 are satisfied with Ï‡ = exp(2Î¾)/(1 + exp(Î¾))4 , G = 1, and M = 1/4.
We are now ready to state the convergence result which is a direct application of Theorem 4.9 of (Liu et al.,
2020).
Lemma 1. Let {Î¦j }Jj=1 be the sequence of policy parameters obtained from Algorithm 1. If Assumptions 4âˆ’7




(1âˆ’Î³)2 Ï‡2
1
1
1
hold, then the following inequality holds for Î· = 4G
2 M M , Î± = 4G2 , J = O
(1âˆ’Î³)2 Ïµ , L = O (1âˆ’Î³)4 Ïµ2 ,
R
âˆ—
Vâˆž
(Âµ0 , g0 ) âˆ’

âˆš
J

Ïµbias
1X 
E Vâˆž (Âµ0 , g0 , Ï€Î¦j ) â‰¤
+ Ïµ,
J j=1
1âˆ’Î³

for arbitrary initial parameter Î¦0 , initial local state distribution Âµ0 âˆˆ âˆ†(X ) and initial global state g0 . The
parameters {MR , Ï‡, G, M } are defined in Assumptions 1, 4, 5, 6 respectively. The term Ïµbias is a positive
constant. The sample complexity of Algorithm 1 is O(Ïµâˆ’3 ).
The term Ïµbias introduced in Lemma 1 is termed as the expressivity error of the policy class Î  parameterized
by the dâˆ’dimensional parameter. For dense neural network-based policies, Ïµbias appears to be small (Liu
et al., 2020).
âˆ—
Lemma 1 states that Algorithm 1 can approximate the optimal mean-field value, Vâˆž
(Âµ0 , g0 ) for any (Âµ0 , g0 ) âˆˆ
âˆ’3
âˆ†(X ) Ã— G with an error bound of Ïµ, and a sample complexity of O(Ïµ ). The constant term hiding in O(Â·)
is dependent on the parameters {MR , Ï‡, G, M } defined in Assumptions 1, 4, 5, 6. Combining Lemma 1 with
Theorem 1, we now arrive at the following result.

12

Published in Transactions on Machine Learning Research (05/2023)

Theorem 3. Let x0 â‰œ {xi0 }iâˆˆ{1,Â·Â·Â· ,N } and g0 be the initial local and global states respectively and Âµ0 be the
empirical distribution of x0 . Assume that {Î¦j }Jj=1 are the policy parameters generated from Algorithm 1
corresponding to the initial condition (Âµ0 , g0 , Î¦0 ), and and the set of policies, Î  follows Assumption 2. If
assumptions 1, 2, 4 - 7 are true, then, âˆ€Ïµ > 0, the following inequality holds for the choice of the parameters
{Î·, Î±, J, L} stated in Lemma 1.
âˆš
J

Ïµbias
1X 
E Vâˆž (Âµ0 , g0 , Ï€Î¦j ) â‰¤
+ C max{e, Ïµ}
J j=1
1âˆ’Î³
p i
1 hp
where VNâˆ— (x0 , g0 ) = sup VN (x0 , g0 , Ï€Î¦ ), e â‰œ âˆš
|X | + |U|
N
Î¦âˆˆRd
VNâˆ— (x0 , g0 ) âˆ’

whenever Î³SP < 1 where SP is defined in Theorem 1. The parameter, C is a constant and the parameter
Ïµbias is defined in Lemma 1. The sample complexity of the process is O(Ïµâˆ’3 ).
Proof. Note that,
VNâˆ— (x0 , g0 ) âˆ’

J
J
1X
1X
âˆ—
âˆ—
Vâˆž (Âµ0 , g0 , Ï€Î¦j ) â‰¤ |VNâˆ— (x0 , g0 ) âˆ’ Vâˆž
Vâˆž (Âµ0 , g0 , Ï€Î¦j )
(Âµ0 , g0 )| + Vâˆž
(Âµ0 , g0 ) âˆ’
J j=1
J j=1

Applying Theorem 1, the first term can be bounded by C â€² e for some constant C â€² . The second term can be
âˆš
bounded by Ïµbias /(1 âˆ’ Î³) + Ïµ with a sample complexity of O(Ïµâˆ’3 ) (Lemma 1). Using C = 2 max{C â€² , 1}, we
conclude.
Theorem 3 states that Algorithm 1 generates a policy such that its associated N âˆ’agent value is at most
O(max{e, Ïµ}) error away from the optimal N âˆ’agent value. Additionally, it also guarantees that such a
policy can be obtained with a sample complexity of O(Ïµhâˆ’3 ) where Ïµ is an
arbitrarily chosen positive number.
p i âˆš
p
|X | + |U| / N . However, if we assume that
We would like to point out that in Theorem 3, e =
Assumption 3 holds in addition to all the assumptions mentioned in Theorem 3, then using Theorem 2, one
can similarly show that the average difference betweenp
the optimal
MARL value and the sequence of values
âˆš
generated by Algorithm 1 is O(max{e, Ïµ}) where e = |X |/ N and such an error can be achieved with a
sample complexity of O(Ïµâˆ’3 ) where Ïµ > 0 is a tunable parameter.

8

Experiments

For numerical experiment, we shall consider a variant of the model described in Subramanian and Mahajan
(2019). The model consists of N collaborative firms operated by a single company. All of them produces the
same product but with different quality. At time t, the i-th firm may decide to improve the current quality
of its product (denoted by xit ) by investing Î»R amount of money. The action corresponding to investment is
denoted as uit = 1 whereas no investment is indicated as uit = 0. Clearly, the action space is U = {0, 1}. On
the other hand, the state space is assumed to be X = {0, 1, Â· Â· Â· , Q âˆ’ 1}. The state transition model of the
i-th firm is as follows.
ï£± i
if uit = 0
ï£²x t



i
N

(27)
xt+1 =
ÂµÌ„
ï£³xit + Ï‡ Q âˆ’ 1 âˆ’ xit 1 âˆ’ t
elsewhere
Q
N
i
where Ï‡ is a uniform random variable in [0, 1], ÂµN
t is the empirical distribution of xt â‰œ {xt }iâˆˆ{1,Â·Â·Â· ,N } and
N
N
ÂµÌ„t is the mean of Âµt . The intuition for (27) is that the product quality does not change when no investment
is made and it improves probabilistically otherwise. However, the improvement also depends on the average
N
product quality, ÂµÌ„N
t of the economy. Specifically, it is harder to improve the quality when ÂµÌ„t is high. The

13

Published in Transactions on Machine Learning Research (05/2023)

Figure 1: The error as a function of N . The bold line and half-width of the shaded region respectively
denote the mean and standard deviation of the error obtained over 25 experiments conducted with different
random seeds. The chosen model parameters are as follows: Î»0 = 1, Î»1 = 0.5, Î²R = 0.5, Î»R = 0.5, Q = 10.
term (1 âˆ’ ÂµÌ„N
t /Q) signifies the resistance to improvement. The reward experienced by the i-th firm at time
t is expressed as follows.
N
N i
N
i
r(xit , uit , ÂµN
t , Î±t ) = Î±t xt âˆ’ Î²R ÂµÌ„t âˆ’ Î»R ut

(28)

The first term Î±tN xit is the revenue earned by the i-th firm. One can interpret Î±tN as the price per unit
quality at time t. Clearly, Î±tN plays the role of shared global state. In (Subramanian and Mahajan, 2019),
Î±tN was taken to be a constant. Here we assume it to be linearly dependent on the average product quality
at t âˆ’ 1 i.e., Î±tN = Î»0 (1 âˆ’ Î»1 (ÂµÌ„N
tâˆ’1 /Q)) where Î»0 , Î»1 are arbitrary positive constants. The intuition is that
the average quality at t âˆ’ 1 influences the price at t. Specifically, as ÂµÌ„tâˆ’1 rises, the firms can charge less price
N
per quality, Î±tN . The second term, Î²R ÂµÌ„N
t denotes the cost incurred due to the average product quality, ÂµÌ„t .
âˆ—
i
Finally, the third term, Î»R ut indicates the investment cost. Let Ï€ be the policy given by the Algorithm 1.
We define the error as,
error â‰œ |VN (x0 , Î±0 , Ï€ âˆ— ) âˆ’ Vâˆž (Âµ0 , Î±0 , Ï€ âˆ— )|
where x0 = {xi0 }iâˆˆ{1,Â·Â·Â· ,N } is the initial local states, Âµ0 is its empirical distribution and Î±0 is the initial
global price. Moreover, VN and Vâˆž are defined via (6) and (11) respectively. Fig. 1 plots the error as a
function of N . We can observe3 that the error decreases with N .

9

Conclusions

In this paper, we introduce a MARL framework where the agents, in addition to their local states that
transitions conditionally independently, also possess a shared global state. As a result, the combined state
transition processes of each agent becomes correlated with each other. Our contribution is to show that
mean-field based approximations are valid even in presence of such correlation. We obtain the expression
for the approximation error as a function of different parameters of the model and surprisingly observe that
it is not dependent on the size of the global state-space. Furthermore, we designed an algorithm that approximately obtains an optimal solution to the MARL problem. Although our work shows approximation
3 The code can be accessed at: https://github.itap.purdue.edu/Clan-labs/MeanFieldwithGlobalState

14

Published in Transactions on Machine Learning Research (05/2023)

results in presence of correlated evolution, the presumed structure of correlation is not in the most generic
form. Specifically, we assumed that the states of each agent can be segregated into two partâˆ’one evolving
conditionally independently and the other being identical to every agent. Whether mean-field control techniques work in presence of more general form of correlated evolution is an important question that needs to
be investigated in the future.

A

Proof of Theorem 1

The following helper lemmas are needed to establish the main result.
A.1

Continuity Lemmas

In the following lemmas, Ï€ âˆˆ Î  is an arbitrary policy and Âµ, ÂµÌ„ âˆˆ âˆ†(X ) are arbitrary local state distributions.
Lemma 2. If Î½ MF (Â·, Â·, Â·) is defined by (7), then the following relation holds âˆ€g âˆˆ G.
|Î½ MF (Âµ, g, Ï€) âˆ’ Î½ MF (ÂµÌ„, g, Ï€)| â‰¤ (1 + LQ )|Âµ âˆ’ ÂµÌ„|1
Lemma 3. If P MF (Â·, Â·, Â·) is defined by (8), then the following relation holds âˆ€g âˆˆ G.
|P MF (Âµ, g, Ï€) âˆ’ P MF (ÂµÌ„, g, Ï€)|1 â‰¤ SP |Âµ âˆ’ ÂµÌ„|1
where SP â‰œ 1 + 2LP + LQ (1 + LP ).
Lemma 4. If PGMF (Â·, Â·, Â·) is defined by (9), then the following relation holds âˆ€g âˆˆ G.
|PGMF (Âµ, g, Ï€) âˆ’ PGMF (ÂµÌ„, g, Ï€)|1 â‰¤ SG |Âµ âˆ’ ÂµÌ„|1
where SG â‰œ LG (2 + LQ ).
Lemma 5. If rMF (Â·, Â·, Â·) is defined by (10), then the following relation holds âˆ€g âˆˆ G.
|rMF (Âµ, g, Ï€) âˆ’ rMF (ÂµÌ„, g, Ï€)| â‰¤ SR |Âµ âˆ’ ÂµÌ„|1
where SR â‰œ MR + 2LR + LQ (M + LR ).
The lemmas stated above exhibit that the functions Î½ MF (Â·, Â·, Â·), P MF (Â·, Â·, Â·), PGMF (Â·, Â·, Â·) and rMF (Â·, Â·, Â·) defined
in section 3 are Lipschitz continuous. Proofs of Lemma 2 âˆ’ 5 are relegated to Appendix Bâˆ’E.
A.2

Continuity of Some Relevant Functions

Let {Ï€l }lâˆˆ{0,1,Â·Â·Â· } âˆˆ Î âˆž be an arbitrary policy sequence and {gl }lâˆˆ{0,1,Â·Â·Â· } âˆˆ G âˆž be a given sequence of
global states. Recall the definition of P MF given in (8). For every Âµl âˆˆ âˆ†(X ), we define the following.
PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) â‰œ P MF (Â·, gl+r , Ï€l+r ) â—¦ P MF (Â·, gl+râˆ’1 , Ï€l+râˆ’1 ) â—¦ Â· Â· Â· â—¦ P MF (Â·, gl , Ï€l )(Âµl )

(29)

where â—¦ denotes function composition and l, r âˆˆ {0, 1, Â· Â· Â· }. Note that, using r = 0 in (29), we get
PÌƒ MF (Âµl , gl:l , Ï€l:l ) = P MF (Âµl , gl , Ï€l ). Similarly, using the definition of PGMF given in (9), we define the
following âˆ€Âµl âˆˆ âˆ†(X ).
PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 ) â‰œPGMF (Âµl , gl , Ï€l )(gl+1 ) Ã— PGMF (PÌƒ MF (Âµl , gl:l , Ï€l:l ), gl+1 , Ï€l+1 )(gl+2 )Ã—
Â· Â· Â· Ã— PGMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )(gl+r+1 )

(30)

Finally, using the definition of rMF given in (10), we define the following âˆ€Âµl âˆˆ âˆ†(X ).
rÌƒMF (Âµl , gl , Ï€l:l+r )
ï£± P
ï£´
rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ),
ï£²
l+1:l+r
=
ï£´
ï£³rMF (Âµ , g , Ï€ )
if r = 0
l
l l

if r â‰¥ 1

(31)
15

Published in Transactions on Machine Learning Research (05/2023)

P
r
where
l+1:l+r indicates a summation operation over {gl+1 , Â· Â· Â· , gl+r } âˆˆ G . We prove the continuity
property of these newly defined functions in the following lemmas. Proofs of Lemma 6 âˆ’ 8 are relegated to
Appendix Fâˆ’H.
Lemma 6. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl:l+r âˆˆ G r+1 and âˆ€Ï€l:l+r âˆˆ
Î r+1 .
|PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ SPr+1 |Âµl âˆ’ ÂµÌ„l |1
where SP is defined in Lemma 3.
Lemma 6 establishes the Lipschitz continuity of PÌƒ MF with respect to its first argument.
Lemma 7. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl:l+r âˆˆ G r+1 and âˆ€Ï€l:l+r âˆˆ
Î r+1 .
X
PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r ) 1 â‰¤ SG (1 + SP + Â· Â· Â· + SPr )|Âµl âˆ’ ÂµÌ„l |1
l+1:l+r
r
where
l+1:l+r indicates a summation operation over {gl+1 , Â· Â· Â· , gl+r } âˆˆ G for r â‰¥ 1 and an identity
operation for r = 0. The terms SP , SG are defined in Lemma 3 and 4 respectively.

P

Lemma 7 establishes the Lipschitz continuity of PÌƒGMF with respect to its first argument. Finally we establish
the Lipschitz continuity of rÌƒMF with respect to its first argument in the following lemma.
Lemma 8. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl âˆˆ G and âˆ€Ï€l:l+r âˆˆ Î r .



MR SG
r
r
MF
MF
(SP âˆ’ 1) + SR SP |Âµl âˆ’ ÂµÌ„l |1
|rÌƒ (Âµl , gl , Ï€l:l+r ) âˆ’ rÌƒ (ÂµÌ„l , gl , Ï€l:l+r )| â‰¤
SP âˆ’ 1
where SP , SG and SR are defined in Lemma 3, 4 and 5 respectively.
Finally, we prove an important property of the function rÌƒMF that directly follows from its definition.
Lemma 9. The following relations hold âˆ€l âˆˆ {0, 1, Â· Â· Â· }, âˆ€r âˆˆ {1, 2, Â· Â· Â· }, âˆ€Âµl âˆˆ âˆ†(X ), âˆ€gl âˆˆ G and
âˆ€Ï€l:l+r âˆˆ Î r .
X
rÌƒMF (Âµl , gl , Ï€l:l+r ) =
rÌƒMF (P MF (Âµl , gl , Ï€l ), gl+1 , Ï€l+1:l+r )PGMF (Âµl , gl , Ï€l )(gl+1 )
gl+1 âˆˆG

where P MF and PGMF are defined in (8) and (9) respectively.
Proof of Lemma 9 is relegated to Appendix I.
A.3

Approximation Lemmas

First we would like to state an important result that will be useful in proving the following lemmas.
Lemma
P10. If âˆ€m âˆˆ {1, Â· Â· Â· , M }, {Xmn }nâˆˆ{1,Â·Â·Â· ,N } are independent random variables that lie in [0, 1], and
satisfy mâˆˆ{1,Â·Â·Â· ,M } E[Xmn ] â‰¤ 1, âˆ€n âˆˆ {1, Â· Â· Â· , N }, then the following holds,
M
X
m=1

E

N
X

(Xmn âˆ’ E[Xmn ]) â‰¤

âˆš
MN

(32)

n=1

N
N
In the following, Ï€ â‰œ {Ï€t }tâˆˆ{0,1,Â·} is an arbitrary sequence of policies, {ÂµN
t , Î½ t , gt } denote the empirical
state distribution, action distribution and global state of N agent system at time t and {xit , uit } are the state
and action of ith agent at time t induced by Ï€ from initial states x0 , g0 .

Lemma 11. The following inequality holds âˆ€t âˆˆ {0, 1, Â· Â· Â· }.
1 p
MF
N
|U|
E Î½N
(ÂµN
t âˆ’Î½
t , gt , Ï€t ) 1 â‰¤ âˆš
N
16

(33)

Published in Transactions on Machine Learning Research (05/2023)

Lemma 12. The following inequality holds âˆ€t âˆˆ {0, 1, Â· Â· Â· }.
p i
CP hp
MF
N
E ÂµN
(ÂµN
|X | + |U|
t+1 âˆ’ P
t , gt , Ï€t ) 1 â‰¤ âˆš
N

(34)

where CP â‰œ 2 + LP .
Lemma 13. The following inequality holds âˆ€t âˆˆ {0, 1, Â· Â· Â· }.
E

1 X
LR p
MR
N
N
MF
N
+âˆš
|U|
r(xit , uit , ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’ r
t , gt , Ï€t ) â‰¤ âˆš
N i=1
N
N

The proofs of Lemma 10 âˆ’ 13 are relegated to Appendix Jâˆ’M.
A.4

Proof of the Theorem

Let, Ï€ = {Ï€t }tâˆˆ{0,1,Â·Â·Â· } be an arbitrary policy sequence. We shall use the notations introduced in section
A.3. Additionally, we shall consider {Âµt , gt } as the local state distribution and the global state of the infinite
agent system at time t. Consider the following.
|VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)|
â‰¤

âˆž
X

Î³tE

t=0

âˆž
X
1 X
N
N
MF
N
N
N
MF
r(xit , uit , ÂµN
,
g
,
Î½
)
âˆ’
r
(Âµ
,
g
,
Ï€
)
+
Î³ t |E[rMF (ÂµN
(Âµt , gt , Ï€t )]|
t
t
t
t
t
t
t , gt , Ï€t )] âˆ’ E[r
|
{z
}
N i=1
t=0
â‰œJt

(a)

â‰¤

p

MR + LR |U|
1âˆ’Î³

!

âˆž
X

1
âˆš +
Î³ t Jt
N
t=0
(35)

Inequality (a) follows from Lemma 13. Note that, using the definition (31), we can write the following.
N
MF
Jt = E[rÌƒMF (ÂµN
(Âµ0 , g0 , Ï€0:t )]
t , gt , Ï€t:t )] âˆ’ E[rÌƒ

â‰¤

tâˆ’1
X

N
MF
N
E[rÌƒMF (ÂµN
(ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E[rÌƒ
k , gk , Ï€k:t )]

k=0
tâˆ’1
(a) X

â‰¤

ï£®
N
ï£°
E[rÌƒMF (ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E

N
MF
N
N
ï£»
rÌƒMF (P MF (ÂµN
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

gâˆˆG

k=0

â‰¤

ï£¹
X

ï£®
ï£¹
X
  MF N

N
N
N
N
MF
N
N
ï£»
E E rÌƒ (Âµk+1 , gk+1
, Ï€k+1:t ) xN
âˆ’ Eï£°
rÌƒMF (P MF (ÂµN
k , gk , uk
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

tâˆ’1
X

gâˆˆG

k=0

ï£® ï£®

ï£¹ï£¹
tâˆ’1
X
X
(b)
N
N
N
N
N
N ï£»ï£»
=
E ï£°E ï£°
rÌƒMF (ÂµN
k+1 , g, Ï€k+1:t )PG (Âµk , gk , Î½ k )(g) xk , gk , uk
k=0

gâˆˆG

ï£®
âˆ’ Eï£°

ï£¹
X

N
MF
N
N
ï£»
rÌƒMF (P MF (ÂµN
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

gâˆˆG

â‰¤

tâˆ’1 X
X

N
N
N
MF
N
MF
N
N
E rÌƒMF (ÂµN
(P MF (ÂµN
k+1 , g, Ï€k+1:t )PG (Âµk , gk , Î½ k )(g) âˆ’ rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

k=0 gâˆˆG

â‰¤

tâˆ’1
X

1
2
Jk,t
+ Jk,2

k=0

17

Published in Transactions on Machine Learning Research (05/2023)

N
where we use the notation that ÂµN
0 = Âµ0 and g0 = g0 . Inequality (a) follows from Lemma 9 whereas
N
N
N
(b) is a result of the fact that ÂµN
and
g
are
conditionally independent given xN
k+1
k+1
k , gk , uk . Moreover,
N
N
N
N
1
gk+1 âˆ¼ PG (Âµk , gk , Î½ k ). The first term Jk,t can be bounded as follows.

X

1
Jk,t
â‰œ



MF
N
N
N
N
E rÌƒMF (ÂµN
(P MF (ÂµN
k+1 , g, Ï€k+1:t ) âˆ’ rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t ) Ã— PG (Âµk , gk , Î½ k )(g)

gâˆˆG
(a)



â‰¤

MR SG
SP âˆ’ 1

(b)






MF
N
N
N
N
(SPtâˆ’kâˆ’1 âˆ’ 1) + SR SPtâˆ’kâˆ’1 Ã— E ÂµN
(ÂµN
k+1 âˆ’ P
k , gk , Ï€k ) 1 Ã— PG (Âµk , gk , Î½ k ) 1
{z
}
|
=1

â‰¤ CP

MR SG
SP âˆ’ 1



(SPtâˆ’kâˆ’1 âˆ’ 1) + SR SPtâˆ’kâˆ’1



p i
1 hp
|X | + |U|
Ã—âˆš
N

2
Inequality (a) follows from Lemma 8 whereas (b) results from Lemma 12. Using (9), the second term Jk,t
can be bounded as follows.

X

2
Jk,t
â‰œ



N
N
N
N
MF
N
MF
N
E PG (ÂµN
(ÂµN
(P MF (ÂµN
k , gk , Î½ k )(g) âˆ’ PG (Âµk , gk , Î½
k , gk , Ï€k ))(g) Ã— rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t )

gâˆˆG
(a)

N
N
N
N
MF
N
â‰¤ MR Ã— E PG (ÂµN
(ÂµN
k , gk , Î½ k ) âˆ’ PG (Âµk , gk , Î½
k , gk , Ï€k )) 1
r
(c)
(b)
|U|
MF
N
(ÂµN
â‰¤ MR Ã— LG Ã— E Î½ N
k âˆ’Î½
k , gk , Ï€k ) 1 â‰¤ MR LG
N

Inequality (a) is a consequence of Assumption 1(a) and the definition of rÌƒMF given in (31). Inequality (b)
follows from Assumption 1(d). Finally, (c) is a consequence of Lemma 11. Combining, we obtain,


Jt â‰¤ CP

MR SG
+ SR
SP âˆ’ 1



SPt âˆ’ 1
SP âˆ’ 1




âˆ’

MR SG
SP âˆ’ 1

r
 
p i
1 hp
|U|
t Ã—âˆš
|X | + |U| + MR LG
t
N
N

Substituting in (35), we obtain the following result.

r
p !
MR + LR |U|
1
Î³
|U|
âˆš + MR LG
|VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤
1âˆ’Î³
N (1 âˆ’ Î³)2
N

 



p i
CP
MR SG
1
1
Î³MR SG
1 hp
âˆš
+
+ SR
âˆ’
âˆ’
Ã—
|X
|
+
|U|
SP âˆ’ 1
SP âˆ’ 1
1 âˆ’ Î³SP
1âˆ’Î³
(1 âˆ’ Î³)2
N

We conclude by noting that | supÏ€ VN (x0 , g0 , Ï€) âˆ’ supÏ€ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤ supÏ€ |VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)|
where the suprema are taken over the set of all admissible policy sequences Î âˆž .
18

Published in Transactions on Machine Learning Research (05/2023)

B

Proof of Lemma 2

Note the inequalities stated below.

(a)

X

|Î½ MF (Âµ, g, Ï€) âˆ’ Î½ MF (ÂµÌ„, g,Ï€)|1 =

Ï€(x, Âµ, g)Âµ(x) âˆ’

xâˆˆX

=

X X
XX

Ï€(x, ÂµÌ„, g)ÂµÌ„(x)

xâˆˆX

Ï€(x, Âµ, g)(u)Âµ(x) âˆ’

xâˆˆX

uâˆˆU

â‰¤

X

X

1

Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)

xâˆˆX

|Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)|

xâˆˆX uâˆˆU

â‰¤

X

|Âµ(x) âˆ’ ÂµÌ„(x)|

xâˆˆX

X
uâˆˆU

|
(b)

â‰¤ |Âµ âˆ’ ÂµÌ„|1 +

Ï€(x, Âµ, g)(u) +

X

X
xâˆˆX

{z

=1

ÂµÌ„(x)

X

|Ï€(x, Âµ, g)(u) âˆ’ Ï€(x, ÂµÌ„, g)(u)|

uâˆˆU

}

ÂµÌ„(x) LQ |Âµ âˆ’ ÂµÌ„|1

xâˆˆX

|

{z

=1

}

(c)

= (1 + LQ )|Âµ âˆ’ ÂµÌ„|1

Inequality (a) follows from the definition of Î½ MF (Â·, Â·) as given in (7). On the other hand, (b) is a consequence
of Assumption 2 and the fact that Ï€(x, Âµ, g) is a valid probability distribution. Finally, (c) uses the fact that
ÂµÌ„ is a distribution. This concludes the lemma.

C

Proof of Lemma 3

Observe that,
|P MF (Âµ, g, Ï€) âˆ’ P MF (ÂµÌ„, g, Ï€)|1
(a)

=

XX

P (x, u, Âµ, g, Î½ MF (Âµ, g, Ï€))Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ P (x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)

xâˆˆX uâˆˆU

1

â‰¤ J1 + J2

Equality (a) follows from the definition of P MF (Â·, Â·, Â·) as depicted in (8). The term J1 satisfies the following
bound.
XX

J1 â‰œ

P (x, u, Âµ, g, Î½ MF (Âµ, g, Ï€)) âˆ’ P (x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))

Ã— Ï€(x, Âµ, g)(u)Âµ(x)
1

xâˆˆX uâˆˆU
(a)

X

 X
â‰¤ LP |Âµ âˆ’ ÂµÌ„|1 + |Î½ MF (Âµ, g, Ï€) âˆ’ Î½ MF (ÂµÌ„, g, Ï€)|1 Ã—
Âµ(x)
Ï€(x, Âµ, g)(u)
xâˆˆX

|
(b)

â‰¤ LP (2 + LQ )|Âµ âˆ’ ÂµÌ„|1
19

uâˆˆU

{z

=1

}

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) is a consequence of Assumption 1(c) whereas (b) follows from Lemma 2, and the fact that
Ï€(x, Âµ, g), Âµ are probability distributions. The second term, J2 obeys the following bound.
J2 â‰œ

XX
xâˆˆX uâˆˆU

X

â‰¤

|P (x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))|1 Ã—|Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)|
|
{z
}
=1

|Âµ(x) âˆ’ ÂµÌ„(x)|

xâˆˆX

X
uâˆˆU

|
(a)

â‰¤ |Âµ âˆ’ ÂµÌ„|1 +

Ï€(x, Âµ, g)(u) +

X

X

ÂµÌ„(x)

xâˆˆX

{z

=1

X

|Ï€(x, Âµ, g)(u) âˆ’ Ï€(x, ÂµÌ„, g)(u)|

uâˆˆU

}
(b)

ÂµÌ„(x) LQ |Âµ âˆ’ ÂµÌ„|1 = (1 + LQ )|Âµ âˆ’ ÂµÌ„|1

xâˆˆX

{z

|

=1

}

Inequality (a) results from Assumption 2 and the fact that Ï€(x, Âµ, g) is a probability distribution whereas
(b) utilizes the fact that ÂµÌ„ is a distribution. This concludes the result.

D

Proof of Lemma 4

Note the following relations.
(a)

|PGMF (Âµ, g, Ï€) âˆ’ PGMF (ÂµÌ„, g, Ï€)|1 = |PG (Âµ, g, Î½ MF (Âµ, g, Ï€)) âˆ’ PGMF (ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))|1
(b)
(c)

â‰¤ LG |Âµ âˆ’ ÂµÌ„|1 + |Î½ MF (Âµ, g, Ï€) âˆ’ Î½ MF (ÂµÌ„, g, Ï€)|1 â‰¤ LG (2 + LQ )|Âµl âˆ’ ÂµÌ„l |1
Equality (a) follows from the definition (9) while (b) follows from Assumption 1(d). Finally, (c) is a result
of Lemma 2.

E

Proof of Lemma 5

Observe that,
|rMF (Âµ, g, Ï€) âˆ’ rMF (ÂµÌ„, g, Ï€)|1
(a)

XX

=

r(x, u, Âµ, g, Î½ MF (Âµ, g, Ï€))Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ r(x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)

xâˆˆX uâˆˆU

1

â‰¤ J1 + J2
Equality (a) follows from the definition of rMF (Â·, Â·, Â·) as depicted in (10). The term J1 satisfies the following
bound.
J1 â‰œ

XX

r(x, u, Âµ, g, Î½ MF (Âµ, g, Ï€)) âˆ’ r(x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€)) Ã— Ï€(x, Âµ, g)(u)Âµ(x)

xâˆˆX uâˆˆU

X
(b)

 X
â‰¤ LR |Âµ âˆ’ ÂµÌ„|1 + |Î½ MF (Âµ, g, Ï€) âˆ’ Î½ MF (ÂµÌ„, g, Ï€)|1 Ã—
Âµ(x)
Ï€(x, Âµ, g)(u) â‰¤ LR (2 + LQ )|Âµ âˆ’ ÂµÌ„|1

(a)

xâˆˆX

|

20

uâˆˆU

{z

=1

}

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) is a consequence of Assumption 1(b) whereas (b) follows from Lemma 2, and the fact that
Ï€(x, Âµ, g), Âµ are probability distributions. The second term, J2 obeys the following bound.
XX

J2 â‰œ

|r(x, u, ÂµÌ„, g, Î½ MF (ÂµÌ„, g, Ï€))| Ã— |Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)|

xâˆˆX uâˆˆU
(a)

â‰¤ MR

X

|Âµ(x) âˆ’ ÂµÌ„(x)|

xâˆˆX

X

Ï€(x, Âµ, g)(u) +MR

uâˆˆU

â‰¤ MR |Âµ âˆ’ ÂµÌ„|1 + MR

X

ÂµÌ„(x)

xâˆˆX

{z

|
(b)

X

=1

X

|Ï€(x, Âµ, g)(u) âˆ’ Ï€(x, ÂµÌ„, g)(u)|

uâˆˆU

}
(c)

ÂµÌ„(x) LQ |Âµ âˆ’ ÂµÌ„|1 = MR (1 + LQ )|Âµ âˆ’ ÂµÌ„|1

xâˆˆX

|

{z

=1

}

Inequality (a) results from Assumption 1(a) where (b) follows from Assumption 2 and the fact that Ï€(x, Âµ, g)
is a probability distribution. Finally, (c) utilizes the fact that ÂµÌ„ is a valid distribution. This concludes the
result.

F

Proof of Lemma 6

Fix l âˆˆ {0, 1, Â· Â· Â· }. We shall prove the lemma via induction on r. Note that, for r = 0, we have,
(a)

|PÌƒ MF (Âµl , gl:l , Ï€l:l ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l , Ï€l:l )|1 = |P MF (Âµl , gl , Ï€l ) âˆ’ P MF (ÂµÌ„l , gl , Ï€l )|1 â‰¤ SP |Âµl âˆ’ ÂµÌ„l |1
Inequality (a) follows from Lemma 3. Assume that the lemma holds for some r âˆˆ {0, 1, Â· Â· Â· }. We shall
demonstrate below that the relation holds for r + 1 as well. Note that,
|PÌƒ MF (Âµl , gl:l+r+1 , Ï€l:l+r+1 ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r+1 , Ï€l:l+r+1 )|1
= |P MF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 ) âˆ’ P MF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )|1
(a)

(b)

â‰¤ SP |PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ SPr+2 |Âµl âˆ’ ÂµÌ„l |1

Inequality (a) follows from Lemma 3 and (b) is a consequence of the induction hypothesis.

G

Proof of Lemma 7

Fix l âˆˆ {0, 1, Â· Â· Â· }. We shall prove the lemma via induction on r. Note that, for r = 0, we have,
(a)

|PÌƒGMF (Âµl , gl:l , Ï€l:l ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l , Ï€l:l )|1 = |PGMF (Âµl , gl , Ï€l ) âˆ’ PGMF (ÂµÌ„l , gl , Ï€l )|1 â‰¤ SG |Âµl âˆ’ ÂµÌ„l |1

(36)

Equality (a) follows from Lemma 4. Assume that the lemma holds for some r âˆˆ {0, 1, Â· Â· Â· }. We shall now
show that the lemma holds for r + 1 as well. Note that,
X

PÌƒGMF (Âµl , gl:l+r+1 , Ï€l:l+r+1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r+1 , Ï€l:l+r+1 ) 1

l+1:l+r+1
(a)

=

X

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )PGMF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )

l+1:l+r+1

âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )(gl+r+1 )PGMF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )

21

1

â‰¤ J1 + J2

Published in Transactions on Machine Learning Research (05/2023)

Equality (a) follows from (30). The first term J1 can be upper bounded as follows.
X

J1 â‰œ

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )

l+1:l+r+1

Ã— PGMF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 ) âˆ’ PGMF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )
(a)

â‰¤ SG

X

PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r )

l+1:l+r+1
(b)

X

â‰¤ SG SPr+1 |Âµl âˆ’ ÂµÌ„l |1

1

1

Ã— PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )

(c)

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 ) = SG SPr+1 |Âµl âˆ’ ÂµÌ„l |1

l+1:l+r+1

Inequality (a) follows from Lemma 4 while (b) results from Lemma 6. Finally, (c) can be shown following
the definition (30). The second term J2 can be bounded as follows.
J2 â‰œ

X
l+1:l+r+1

=

|P MF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )|1
{z
}
| G
=1

Ã— |PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )(gl+r+1 )|
X
(a)
|PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ SG (1 + SP + Â· Â· Â· + SPr )|Âµl âˆ’ ÂµÌ„l |1
l+1:l+r+1

Inequality (a) follows from induction hypothesis. This concludes the lemma.

H

Proof of Lemma 8

Note that the result readily follows for r = 0 from Lemma 5. Therefore, we assume r â‰¥ 1.
|rÌƒMF (Âµl , gl , Ï€l:l+r ) âˆ’ rÌƒMF (ÂµÌ„l , gl , Ï€l:l+r )|
(a) X
|rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )
â‰¤
l+1:l+r

âˆ’ rMF (PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )| â‰¤ J1 + J2

Inequality (a) follows from the definition (31). The first term can be bounded as follows.

J1 â‰œ

X

|rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )|

l+1:l+r

Ã— |PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )|
X
(a)
|PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )|1
â‰¤ MR
l+1:l+râˆ’1
(b)

â‰¤ MR SG (1 + SP + Â· Â· Â· + SPrâˆ’1 )|Âµl âˆ’ ÂµÌ„l |1 =

22



MR SG
SP âˆ’ 1



(SPr âˆ’ 1)|Âµl âˆ’ ÂµÌ„l |1

Published in Transactions on Machine Learning Research (05/2023)

The bound (a) can be proven using Assumption 1(a) and the definition of rMF given in (10). The bound (b)
follows from Lemma 7. The term J2 can be bounded as follows.
J2 â‰œ

X

PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )

l+1:l+r

Ã— |rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r ) âˆ’ rMF (PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )|
(a) X
â‰¤
PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) Ã— SR |PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )|
l+1:l+r
(b)

â‰¤ SR SPr |Âµl âˆ’ ÂµÌ„l |1 Ã—

X

(c)

PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) = SR SPr |Âµl âˆ’ ÂµÌ„l |1

l+1:l+r

Inequality (a) follows from Lemma 5 while (b) results from Lemma 6. Finally, (c) can be proven from the
definition (30). This concludes the lemma.

I

Proof of Lemma 9
rÌƒMF (Âµl , gl , Ï€l:l+r )
X
=
rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )
l+1:l+r
(a)

X

=

rMF (PÌƒ MF (P MF (Âµl , gl , Ï€l ), gl+1:l+râˆ’1 , Ï€l+1:l+râˆ’1 ), gl+r , Ï€l+r )

l+1:l+r

Ã— PÌƒGMF (P MF (Âµl , gl , Ï€l ), gl+1:l+râˆ’1 , Ï€l+1:l+râˆ’1 )(gl+r )PGMF (Âµl , gl , Ï€l )(gl+1 )
X
=
rÌƒMF (P MF (Âµl , gl , Ï€l ), gl+1 , Ï€l+1:l+r )PGMF (Âµl , gl , Ï€l )(gl+1 )
gl+1 âˆˆG

Equality (a) follows from the definitions (29) and (30).

J

Proof of Lemma 10

Let Ymn â‰œ Xmn âˆ’ E[Xmn ], âˆ€m âˆˆ {1, Â· Â· Â· , M }, âˆ€n âˆˆ {1, Â· Â· Â· , N }. Note that, as Xmn âˆˆ [0, 1], we have,
2
2
E[Ymn
] = E[Xmn
] âˆ’ [E[Xmn ]]2 â‰¤ E[Xmn ]. Using independence of {Ymn }nâˆˆ{1,Â·Â·Â· ,N } , for any given m âˆˆ
{1, Â· Â· Â· , M }, we get,

E

"N
X

#2
Ym,n

=E

n=1

" N N
X X

#
Ym,n1 Ym,n2 =

n1 =1 n2 =1

N
X

N
N
N
X
X
X
 2 
 2 
E Ym,n
+2
E[Ym,n1 ]E[Ym,n2 ] =
E Ym,n
n1 =1 n2 >n1

n=1

n=1

Using the above relation, we finally obtain the following.

M
X
m=1

E

N
X
n=1

(a) âˆš

Ym,n â‰¤

M

ï£±
M
ï£²X
ï£³

m=1

E

"N
X

Ym,n

#2 ï£¼ 12
ï£½

âˆš
M

=

ï£¾

n=1

( M N
XX
m=1 n=1

âˆš
=

M

(N M
XX
n=1 m=1

23

) 21
 2 
E Ym,n
) 21
E [Xm,n ]

â‰¤

âˆš
MN

Published in Transactions on Machine Learning Research (05/2023)

K

Proof of Lemma 11

Notice the following relations.
MF
N
(ÂµN
E Î½N
t âˆ’Î½
t , gt , Ï€t ) 1
(a)

= E Î½N
t âˆ’

X

N
N
Ï€t (x, ÂµN
t , gt )Âµt (x)

xâˆˆX

1

##

" "
=E E

X

Î½N
t (u) âˆ’

"

N
N
Ï€t (x, ÂµN
t , gt )(u)Âµt (x)

N
xN
t , gt

xâˆˆX

uâˆˆU

"

X

##
N
X
1
1 X
i
N
N
N
N
i
Î´(ut = u) âˆ’
Ï€t (x, Âµt , gt )(u)
Î´(xt = x) xt , gt
=E
E
N i=1
N
i=1
uâˆˆU
xâˆˆX
##
"
"
N
N
X
(c) 1 p
1 X
1 X
N
N
i
i
N
N
â‰¤ âˆš
Î´(ut = u) âˆ’
Ï€t (xt , Âµt , gt )(u) xt , gt
|U|
=E
E
N i=1
N i=1
N
(b)

X

N
X

uâˆˆU

Equality (a) follows from the definition of Î½ MF (Â·, Â·, Â·) given in (7) while (b) is a consequence of the definitions
i
N
of ÂµN
t , Î½ t . Finally, (c) uses Lemma 10. Specifically, it utilises the facts that, {ut }iâˆˆ{1,Â·Â·Â· ,N } are conditionally
N
N
independent given xt , gt and the following holds
h
i
N
N
E Î´(uit = u) xN
= Ï€t (xit , ÂµN
t , gt
t , gt )(u),
i
X h
N
E Î´(uit = u) xN
=1
t , gt
uâˆˆU

âˆ€i âˆˆ {1, Â· Â· Â· , N }, âˆ€u âˆˆ U. This concludes the lemma.

L

Proof of Lemma 12

Notice the following decomposition.
MF
N
E ÂµN
(ÂµN
t+1 âˆ’ P
t , gt , Ï€t ) 1
(a)

= E ÂµN
t+1 âˆ’

X X

N
MF
N
â€²
N
N
N
P (xâ€² , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))Ï€t (x , Âµt , gt )(u)Âµt (x)

xâ€² âˆˆX uâˆˆU

1

N
X

N
X X
1 X
1
(b) X
N
MF
N
N
â€²
N
â€²
Î´(xit+1 = x) âˆ’
P (xâ€² , u, ÂµN
,
g
,
Î½
(Âµ
,
g
,
Ï€
))(x)Ï€
(x
,
Âµ
,
g
)(u)
Î´(xit = xâ€² )
=
E
t
t
t
t
t
t
t
N i=1
N
â€²
i=1
x âˆˆX uâˆˆU

xâˆˆX

=

X
xâˆˆX

E

N
X

N
1 XX
1
N
MF
N
i
N
N
Î´(xit+1 = x) âˆ’
P (xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))(x)Ï€t (xt , Âµt , gt )(u)
N i=1
N i=1
uâˆˆU

â‰¤ J1 + J2 + J3
Equality (a) uses the definition of P MF (Â·, Â·, Â·) as shown in (8) and equality (b) uses the definition of ÂµN
t . The
term J1 obeys the following.
N
N
X
1 X X
N
N
E
Î´(xit+1 = x) âˆ’
P (xit , uit , ÂµN
t , gt , Î½ t )(x)
N
i=1
i=1
xâˆˆX
" " N
##
N
X
X
X
(a) 1 p
1
N
N
N
N
N
=
E E
Î´(xit+1 = x) âˆ’
P (xit , uit , ÂµN
â‰¤ âˆš
|X |
t , gt , Î½ t )(x) xt , gt , ut
N
N
i=1
i=1

J1 â‰œ

xâˆˆX

24

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) is obtained from Lemma 10. In particular, it uses the facts that {xit+1 }iâˆˆ{1,Â·Â·Â· ,N } are condiN
N
tionally independent given {xN
t , gt , ut }, and the following relations hold
i
h
N
N
N
N
= P (xit , uit , ÂµN
,
g
,
u
E Î´(xit+1 = x) xN
t , gt , Î½ t )(x),
t
t
t
i
X h
N
N
=1
E Î´(xit+1 = x) xN
t , gt , ut
xâˆˆX

âˆ€i âˆˆ {1, Â· Â· Â· , N }, and âˆ€x âˆˆ X . The second term satisfies the following bound.
J2 â‰œ

N
N
X
1 X X
N
N
N
MF
N
E
P (xit , uit , ÂµN
P (xit , uit , ÂµN
(ÂµN
t , gt , Î½ t )(x) âˆ’
t , gt , Î½
t , gt , Ï€t ))(x)
N
i=1
i=1
xâˆˆX
N

â‰¤

1 X
N
N
i
i
N
N
MF
N
E P (xit , uit , ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’ P (xt , ut , Âµt , gt , Î½
t , gt , Ï€t )) 1
N i=1

(a)

(b) L p
P
MF
N
|U|
â‰¤ LP E Î½ N
(ÂµN
t âˆ’Î½
t , gt , Ï€t ) 1 â‰¤ âˆš
N

Inequality (a) is a consequence of Assumption 1(c) while (b) follows from Lemma 11. Finally, the term, J3
can be upper bounded as follows.
J3 â‰œ

N
1 X X
N
MF
N
P (xit , uit , ÂµN
(ÂµN
E
t , gt , Î½
t , gt , Ï€t ))(x)
N
i=1
xâˆˆX

âˆ’

N X
X

N
MF
N
i
N
N
P (xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))(x)Ï€t (xt , Âµt , gt )(u)

i=1 uâˆˆU
(a)

1 p
|X |
â‰¤ âˆš
N

Inequality (a) is a result of Lemma 10. In particular, it uses the facts that, {uit }iâˆˆ{1,Â·Â·Â· ,N } are conditionally
N
independent given xN
t , gt , and the following relations hold
h
i X
N
MF
N
N
N
MF
N
i
N
N
E P (xit , uit , ÂµN
(ÂµN
=
P (xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))(x) xt , gt
t ,Î½
t , gt , Ï€t ))(x)Ï€t (xt , Âµt , gt )(u),
uâˆˆU

X

h
E

N
MF
N
N
N
P (xit , uit , ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))(x) xt , gt

i

=1

xâˆˆX

âˆ€i âˆˆ {1, Â· Â· Â· , N }, and âˆ€x âˆˆ X . This concludes the Lemma.

M

Proof of Lemma 13

Observe the following decomposition.
E

1 X
N
N
MF
N
r(xit , uit , ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’ r
t , gt , Ï€t )
N i=1
N

= E

XX
1 X
N
N
N
MF
N
N
N
N
r(xit , uit , ÂµN
r(x, u, ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’
t , gt , Î½
t , gt , Ï€t ))Ï€t (x, Âµt , gt )(u)Âµt (x)
N i=1

(b)

N
1 X

(a)

xâˆˆX uâˆˆU

=E

=E

N i=1
N
1 X

N i=1

N
N
r(xit , uit , ÂµN
t , gt , Î½ t ) âˆ’

XX

1
N
MF
N
N
N
r(x, u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))Ï€t (x, Âµt , gt )(u)

N i=1

xâˆˆX uâˆˆU
N

N
N
r(xit , uit , ÂµN
t , gt , Î½ t ) âˆ’

N
X

Î´(xit = x)

1 XX
N
MF
N
i
N
N
r(xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))Ï€t (xt , Âµt , gt )(u) â‰¤ J1 + J2
N i=1
uâˆˆU

25

Published in Transactions on Machine Learning Research (05/2023)

Equation (a) uses the definition of rMF (Â·, Â·, Â·) as shown in (10). Inequality (b) uses the definition of ÂµN
t . The
term, J1 , obeys the following.
J1 â‰œ
â‰¤

N
N
X
1 X
N
N
N
MF
N
E
r(xit , uit , ÂµN
,
g
,
Î½
)
âˆ’
r(xit , uit , ÂµN
(ÂµN
t
t
t
t , gt , Î½
t , gt , Ï€t ))
N i=1
i=1
N
1 X
N
N
i
i
N
N
MF
N
E
r(xit , uit , ÂµN
(ÂµN
t , gt , Î½ t ) âˆ’ r(xt , ut , Âµt , gt , Î½
t , gt , Ï€t ))
N i=1

(a)

(b) L p
R
MF
N
|U|
â‰¤ LR E Î½ N
(ÂµN
t âˆ’Î½
t , gt , Ï€t ) 1 â‰¤ âˆš
N

Inequality (a) results from Assumption 1(b), whereas (b) is a consequence of Lemma 11. The term, J2 , obeys
the following.
N
N X
X
1 X
N
MF
N
N
MF
N
i
N
N
r(xit , uit , ÂµN
(ÂµN
E
r(xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t )) âˆ’
t , gt , Î½
t , gt , Ï€t ))Ï€t (xt , Âµt , gt )(u)
N i=1
i=1 uâˆˆU
" " N
X
1
N
MF
N
= E E
r(xit , uit , ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))
N
i=1
##
N X
X
i
N
N
MF
N
N
i
N
N
N
N
âˆ’
r(xt , u, Âµt , gt , Î½ (Âµt , gt , Ï€t ))Ï€t (xt , Âµt , gt )(u) xt , gt

J2 â‰œ

i=1 uâˆˆU

" "

=

M
E E
N

N
X

N
MF
N
r0 (xit , uit , ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))

i=1

âˆ’

N X
X

##
N
MF
N
i
N
N
r0 (xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))Ï€t (xt , Âµt , gt )(u)

N
xN
t , gt

i=1 uâˆˆU

(a) M

â‰¤ âˆš

R

N

where r0 (Â·, Â·, Â·, Â·) â‰œ r(Â·, Â·, Â·, Â·)/MR . Inequality (a) follows from Lemma 10. In particular, it uses the fact that
N
{uit }iâˆˆ{1,Â·Â·Â· ,N } are conditionally independent given xN
t , gt , and the following relations hold.
N
MF
N
|r0 (xit , uit , ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t ))| â‰¤ 1,
h
i X
N
MF
N
N
N
N
MF
N
i
N
N
E r0 (xit , uit , ÂµN
(ÂµN
=
r0 (xit , u, ÂµN
(ÂµN
t , gt , Î½
t , gt , Ï€t )) xt , gt
t , gt , Î½
t , gt , Ï€t ))Ï€t (xt , Âµt , gt )(u)
uâˆˆU

âˆ€i âˆˆ {1, Â· Â· Â· , N }, âˆ€u âˆˆ U.

N

Proof of Theorem 2

The following results are needed to prove the theorem.
N.1

Continuity Lemmas

In the following lemmas, Ï€ âˆˆ Î  is an arbitrary policy and Âµ, ÂµÌ„ âˆˆ âˆ†(X ) are arbitrary local state distributions.
Lemma 14. If P MF (Â·, Â·, Â·) is defined by (8), then the following relation holds âˆ€g âˆˆ G.
|P MF (Âµ, g, Ï€) âˆ’ P MF (ÂµÌ„, g, Ï€)|1 â‰¤ QP |Âµ âˆ’ ÂµÌ„|1
where QP â‰œ 1 + LP + LQ .
Lemma 15. If rMF (Â·, Â·, Â·) is defined by (10), then the following relation holds âˆ€g âˆˆ G.
|rMF (Âµ, g, Ï€) âˆ’ rMF (ÂµÌ„, g, Ï€)| â‰¤ QR |Âµ âˆ’ ÂµÌ„|1
where QR â‰œ MR (1 + LQ ) + LR .
26

Published in Transactions on Machine Learning Research (05/2023)

The proofs of Lemma 14 âˆ’ 15 are relegated to Appendix Oâˆ’P. In the following three lemmas, we show
the continuity of the functions defined in Appendix A.2. Proofs of the following lemmas are relegated to
Appendix Qâˆ’S.
Lemma 16. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl:l+r âˆˆ G r+1 and âˆ€Ï€l:l+r âˆˆ
Î r+1 .
|PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ Qr+1
P |Âµl âˆ’ ÂµÌ„l |1
where QP is defined in Lemma 14.
Lemma 17. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl:l+r âˆˆ G r+1 and âˆ€Ï€l:l+r âˆˆ
Î r+1 .
X
PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r ) 1 â‰¤ LG (1 + QP + Â· Â· Â· + QrP )|Âµl âˆ’ ÂµÌ„l |1
l+1:l+r

P
r
where
l+1:l+r indicates a summation operation over {gl+1 , Â· Â· Â· , gl+r } âˆˆ G for r â‰¥ 1 and an identity
operation for r = 0. The term QP is defined in Lemma 14.
Lemma 18. The following relations hold âˆ€l, r âˆˆ {0, 1, Â· Â· Â· }, âˆ€Âµl , ÂµÌ„l âˆˆ âˆ†(X ), âˆ€gl âˆˆ G and âˆ€Ï€l:l+r âˆˆ Î r .



MR L G
MF
MF
r
r
(QP âˆ’ 1) + QR QP |Âµl âˆ’ ÂµÌ„l |1
|rÌƒ (Âµl , gl , Ï€l:l+r ) âˆ’ rÌƒ (ÂµÌ„l , gl , Ï€l:l+r )| â‰¤
QP âˆ’ 1
where QP and QR are defined in Lemma 14 and 15 respectively.
N.2

Approximation Lemmas

We use the same notation introduced in A.3.
Lemma 19. The following inequality holds âˆ€t âˆˆ {0, 1, Â· Â· Â· }.
2 p
MF
N
|X |
(ÂµN
E ÂµN
t+1 âˆ’ P
t , gt , Ï€t ) 1 â‰¤ âˆš
N

(37)

Lemma 20. The following inequality holds âˆ€t âˆˆ {0, 1, Â· Â· Â· }.

E

1 X
MR
N
MF
N
r(xit , uit , ÂµN
(ÂµN
t , gt ) âˆ’ r
t , gt , Ï€t ) â‰¤ âˆš
N i=1
N

The proofs of Lemma 19 âˆ’ 20 are relegated to Appendix Tâˆ’U.
N.3

Proof of the Theorem

Let, Ï€ = {Ï€t }tâˆˆ{0,1,Â·Â·Â· } be an arbitrary policy sequence. We shall use the notations introduced in section
A.3. Additionally, we shall consider {Âµt , gt } as the local state distribution and the global state of the infinite
agent system at time t. Consider the following.
|VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)|
â‰¤

âˆž
X
t=0

âˆž
X
1 X
N
MF
N
N
N
MF
r(xit , uit , ÂµN
,
g
)
âˆ’
r
(Âµ
,
g
,
Ï€
)
+
Î³ t |E[rMF (ÂµN
(Âµt , gt , Ï€t )]|
t
t
t
t
t
t , gt , Ï€t )] âˆ’ E[r
|
{z
}
N i=1
t=0





Î³tE

â‰œJt
(a)

â‰¤

MR
1âˆ’Î³

âˆž
X

1
âˆš +
Î³ t Jt
N
t=0
(38)
27

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) follows from Lemma 20. Note that, using the definition (31), we can write the following.
N
MF
Jt = E[rÌƒMF (ÂµN
(Âµ0 , g0 , Ï€0:t )]
t , gt , Ï€t:t )] âˆ’ E[rÌƒ

â‰¤

tâˆ’1
X

N
MF
N
E[rÌƒMF (ÂµN
(ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E[rÌƒ
k , gk , Ï€k:t )]

k=0
tâˆ’1
(a) X

â‰¤

ï£®
N
ï£°
E[rÌƒMF (ÂµN
k+1 , gk+1 , Ï€k+1:t )] âˆ’ E

â‰¤

N
MF
N
N
ï£»
rÌƒMF (P MF (ÂµN
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

gâˆˆG

k=0
tâˆ’1
X

ï£¹
X


 
N
N
N
N
E E rÌƒMF (ÂµN
k+1 , gk+1 , Ï€k+1:t ) xk , gk , uk

k=0

ï£®
âˆ’ Eï£°

ï£¹
X

N
MF
N
N
ï£»
rÌƒMF (P MF (ÂµN
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

gâˆˆG

ï£® ï£®

ï£¹ï£¹
tâˆ’1
X
X
(b)
N
N
N
N
N ï£»ï£»
=
E ï£°E ï£°
rÌƒMF (ÂµN
k+1 , g, Ï€k+1:t )PG (Âµk , gk )(g) xk , gk , uk
gâˆˆG

k=0

ï£®
âˆ’ Eï£°

ï£¹
X

N
MF
N
N
ï£»
rÌƒMF (P MF (ÂµN
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk , Ï€k )(g)

gâˆˆG

â‰¤

tâˆ’1 X
X

N
N
MF
N
N
N
(P MF (ÂµN
E rÌƒMF (ÂµN
k+1 , g, Ï€k+1:t )PG (Âµk , gk )(g) âˆ’ rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t )PG (Âµk , gk )(g)

k=0 gâˆˆG

â‰¤

tâˆ’1 X
X

MF
N
N
N
rÌƒMF (ÂµN
(P MF (ÂµN
k+1 , g, Ï€k+1:t ) âˆ’ rÌƒ
k , gk , Ï€k ), g, Ï€k+1:t ) Ã— PG (Âµk , gk )(g)

k=0 gâˆˆG
tâˆ’1
(c) X



â‰¤

k=0
tâˆ’1 
(d) X

â‰¤

k=0


=

MR LG
QP âˆ’ 1



MR LG
QP âˆ’ 1
 



(Qtâˆ’kâˆ’1
âˆ’ 1) + QR Qtâˆ’kâˆ’1
P
P



N
N
MF
N
(ÂµN
Ã— E ÂµN
k+1 âˆ’ P
k , gk , Ï€k ) 1 Ã— PG (Âµk , gk ) 1
|
{z
}
=1

2
QP âˆ’ 1



2 p
(Qtâˆ’kâˆ’1
âˆ’ 1) + QR Qtâˆ’kâˆ’1
Ã—âˆš
|X |
P
P
N



MR LG
1 p
t
+ QR QP âˆ’ 1 âˆ’ MR LG t Ã— âˆš
|X |
QP âˆ’ 1
N

N
where we use the notation that ÂµN
0 = Âµ0 and g0 = g0 . Inequality (a) follows from Lemma 9 whereas (b) is
N
N
N
N
a consequence of the fact that Âµk+1 and gk+1 are conditionally independent given xN
k , gk , uk . Moreover,
N
N
N
gk+1 âˆ¼ PG (Âµk , gk ). Inequality (c) follows from Lemma 18 while (d) is a consequence of Lemma 19.
Substituting in (35), we obtain the following result.



MR
1
âˆš
|VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤
1âˆ’Î³
N

 

 


2
MR SG
1
1
MR LG
Î³
1 p
+
+ QR
âˆ’
âˆ’
Ã—âˆš
|X |
2
QP âˆ’ 1
QP âˆ’ 1
1 âˆ’ Î³QP
1âˆ’Î³
QP âˆ’ 1 (1 âˆ’ Î³)
N

We conclude by noting that | supÏ€ VN (x0 , g0 , Ï€) âˆ’ supÏ€ Vâˆž (Âµ0 , g0 , Ï€)| â‰¤ supÏ€ |VN (x0 , g0 , Ï€) âˆ’ Vâˆž (Âµ0 , g0 , Ï€)|
where the suprema are taken over the set of all admissible policy sequences Î âˆž .
28

Published in Transactions on Machine Learning Research (05/2023)

O

Proof of Lemma 14

Observe that,
(a)

|P MF (Âµ, g, Ï€) âˆ’ P MF (ÂµÌ„, g, Ï€)|1 =

XX

P (x, u, Âµ, g)Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ P (x, u, ÂµÌ„, g)Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)

xâˆˆX uâˆˆU

1

â‰¤ J1 + J2
Equality (a) follows from the definition of P MF (Â·, Â·, Â·) as depicted in (8). The term J1 satisfies the following
bound.
XX
P (x, u, Âµ, g) âˆ’ P (x, u, ÂµÌ„, g) Ã— Ï€(x, Âµ, g)(u)Âµ(x)
J1 â‰œ
1

xâˆˆX uâˆˆU
(a)

â‰¤ LP |Âµ âˆ’ ÂµÌ„|1 Ã—

X
xâˆˆX

Âµ(x)

X

(b)

Ï€(x, Âµ, g)(u) = LP |Âµ âˆ’ ÂµÌ„|1

uâˆˆU

|

{z

}

=1

Inequality (a) is a consequence of Assumption 1(c) whereas (b) follows from the fact that Ï€(x, Âµ, g), Âµ are
probability distributions. The second term, J2 obeys the following bound.
XX
J2 â‰œ
|P (x, u, ÂµÌ„, g)|1 Ã—|Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)|
|
{z
}
xâˆˆX uâˆˆU

X

â‰¤

=1

|Âµ(x) âˆ’ ÂµÌ„(x)|

xâˆˆX

X

Ï€(x, Âµ, g)(u) +

(a)

â‰¤ |Âµ âˆ’ ÂµÌ„|1 +

xâˆˆX

uâˆˆU

{z

|
X

X

=1

ÂµÌ„(x)

X

|Ï€(x, Âµ, g)(u) âˆ’ Ï€(x, ÂµÌ„, g)(u)|

uâˆˆU

}
(b)

ÂµÌ„(x) LQ |Âµ âˆ’ ÂµÌ„|1 = (1 + LQ )|Âµ âˆ’ ÂµÌ„|1

xâˆˆX

|

{z

=1

}

Inequality (a) results from Assumption 2 and the fact that Ï€(x, Âµ, g) is a probability distribution whereas
(b) utilizes the fact that ÂµÌ„ is a distribution. This concludes the result.

P

Proof of Lemma 15

Observe that,
(a)

|rMF (Âµ, g, Ï€) âˆ’ rMF (ÂµÌ„, g, Ï€)|1 =

XX

r(x, u, Âµ, g)Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ r(x, u, ÂµÌ„, g)Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)

xâˆˆX uâˆˆU

1

â‰¤ J1 + J2
Equality (a) follows from the definition of rMF (Â·, Â·, Â·) as depicted in (10). The term J1 satisfies the following
bound.
J1 â‰œ

XX

(a)

r(x, u, Âµ, g) âˆ’ r(x, u, ÂµÌ„, g) Ã— Ï€(x, Âµ, g)(u)Âµ(x) â‰¤ LR |Âµ âˆ’ ÂµÌ„|1 Ã—

xâˆˆX uâˆˆU

X
xâˆˆX

|
(b)

= LR |Âµ âˆ’ ÂµÌ„|1

29

Âµ(x)

X

Ï€(x, Âµ, g)(u)

uâˆˆU

{z

=1

}

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) is a consequence of Assumption 1(b) whereas (b) follows from the fact that Ï€(x, Âµ, g), Âµ are
probability distributions. The second term, J2 obeys the following bound.
XX
J2 â‰œ
|r(x, u, ÂµÌ„, g)| Ã— |Ï€(x, Âµ, g)(u)Âµ(x) âˆ’ Ï€(x, ÂµÌ„, g)(u)ÂµÌ„(x)|
xâˆˆX uâˆˆU
(a)

â‰¤ MR

X

|Âµ(x) âˆ’ ÂµÌ„(x)|

xâˆˆX

X

Ï€(x, Âµ, g)(u) +MR

uâˆˆU

â‰¤ MR |Âµ âˆ’ ÂµÌ„|1 + MR

X

ÂµÌ„(x)

xâˆˆX

{z

|
(b)

X

=1

X

|Ï€(x, Âµ, g)(u) âˆ’ Ï€(x, ÂµÌ„, g)(u)|

uâˆˆU

}
(c)

ÂµÌ„(x) LQ |Âµ âˆ’ ÂµÌ„|1 = MR (1 + LQ )|Âµ âˆ’ ÂµÌ„|1

xâˆˆX

|

{z

=1

}

Inequality (a) results from Assumption 1(a) where (b) follows from Assumption 2 and the fact that Ï€(x, Âµ, g)
is a probability distribution. Finally, (c) utilizes the fact that ÂµÌ„ is a valid distribution. This concludes the
result.

Q

Proof of Lemma 16

Fix l âˆˆ {0, 1, Â· Â· Â· }. We shall prove the lemma via induction on r. Note that, for r = 0, we have,
(a)

|PÌƒ MF (Âµl , gl:l , Ï€l:l ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l , Ï€l:l )|1 = |P MF (Âµl , gl , Ï€l ) âˆ’ P MF (ÂµÌ„l , gl , Ï€l )|1 â‰¤ QP |Âµl âˆ’ ÂµÌ„l |1
Inequality (a) follows from Lemma 14. Assume that the lemma holds for some r âˆˆ {0, 1, Â· Â· Â· }. We shall
demonstrate below that the relation holds for r + 1 as well. Note that,
|PÌƒ MF (Âµl , gl:l+r+1 , Ï€l:l+r+1 ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r+1 , Ï€l:l+r+1 )|1
= |P MF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 ) âˆ’ P MF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )|1
(a)

(b)

â‰¤ QP |PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ Qr+2
P |Âµl âˆ’ ÂµÌ„l |1

Inequality (a) follows from Lemma 14 and (b) is a consequence of the induction hypothesis.

R

Proof of Lemma 17

Fix l âˆˆ {0, 1, Â· Â· Â· }. We shall prove the lemma via induction on r. Note that, for r = 0, we have,
|PÌƒGMF (Âµl , gl:l , Ï€l:l ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l , Ï€l:l )|1 = |PGMF (Âµl , gl , Ï€l ) âˆ’ PGMF (ÂµÌ„l , gl , Ï€l )|1
(39)

(a)

= |PG (Âµl , gl ) âˆ’ PG (ÂµÌ„l , gl )|1 â‰¤ LG |Âµl âˆ’ ÂµÌ„l |1
Equality (a) follows from Assumption 1(d). Assume that the lemma holds for some r âˆˆ {0, 1, Â· Â· Â· }. We shall
now show that the lemma holds for r + 1 as well. Note that,
X
PÌƒGMF (Âµl , gl:l+r+1 , Ï€l:l+r+1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r+1 , Ï€l:l+r+1 ) 1
l+1:l+r+1
(a)

=

X

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )PGMF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )

l+1:l+r+1

âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )(gl+r+1 )PGMF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )

30

1

â‰¤ J1 + J2

Published in Transactions on Machine Learning Research (05/2023)

Equality (a) follows from (30). The first term J1 can be upper bounded as follows.
X

J1 â‰œ

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )

l+1:l+r+1

Ã— PGMF (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 ) âˆ’ PGMF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )
1
X
=
PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )
l+1:l+r+1

Ã— PG (PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ), gl+r+1 ) âˆ’ PG (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 )
(a)

X

â‰¤ LG

PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒ MF (Âµl , gl:l+r , Ï€l:l+r )

l+1:l+r+1
(b)

X

â‰¤ LG Qr+1
P |Âµl âˆ’ ÂµÌ„l |1

1

1

Ã— PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 )

(c)

PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 ) = LG Qr+1
P |Âµl âˆ’ ÂµÌ„l |1

l+1:l+r+1

Inequality (a) follows from Assumption 1(d) while (b) results from Lemma 16. Finally, (c) can be shown
following the definition (30). The second term J2 can be bounded as follows.
J2 â‰œ

X
l+1:l+r+1

|PGMF (PÌƒ MF (ÂµÌ„l , gl:l+r , Ï€l:l+r ), gl+r+1 , Ï€l+r+1 )|1
|
{z
}

=1
MF
Ã— |PÌƒG (Âµl , gl:l+r , Ï€l:l+r )(gl+r+1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )(gl+r+1 )|

=

(a)

X

|PÌƒGMF (Âµl , gl:l+r , Ï€l:l+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+r , Ï€l:l+r )|1 â‰¤ LG (1 + QP + Â· Â· Â· + QrP )|Âµl âˆ’ ÂµÌ„l |1

l+1:l+r+1

Inequality (a) follows from induction hypothesis. This concludes the lemma.

S

Proof of Lemma 18

Note that the result readily follows for r = 0 from Lemma 15. Therefore, we assume r â‰¥ 1.
|rÌƒMF (Âµl , gl , Ï€l:l+r ) âˆ’ rÌƒMF (ÂµÌ„l , gl , Ï€l:l+r )|
(a) X
â‰¤
|rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )
l+1:l+r

âˆ’ rMF (PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )| â‰¤ J1 + J2
Inequality (a) follows from the definition (31). The first term can be bounded as follows.
X

J1 â‰œ

|rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )|

l+1:l+r

(a)

â‰¤ MR

Ã— |PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )|
X
|PÌƒGMF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ) âˆ’ PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )|1
l+1:l+râˆ’1

(b)

â‰¤ MR LG (1 + QP + Â· Â· Â· + Qrâˆ’1
P )|Âµl âˆ’ ÂµÌ„l |1 =

31



MR LG
QP âˆ’ 1



(QrP âˆ’ 1)|Âµl âˆ’ ÂµÌ„l |1

Published in Transactions on Machine Learning Research (05/2023)

The bound (a) can be proven using Assumption 1(a) and the definition of rMF given in (10). The bound (b)
follows from Lemma 17. The term J2 can be bounded as follows.
X
J2 â‰œ
PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r )
l+1:l+r

(a)

â‰¤

Ã— |rMF (PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r ) âˆ’ rMF (PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ), gl+r , Ï€l+r )|
X
PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) Ã— QR |PÌƒ MF (Âµl , gl:l+râˆ’1 , Ï€l:l+râˆ’1 ) âˆ’ PÌƒ MF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )|
l+1:l+r

(b)

â‰¤ QR QrP |Âµl âˆ’ ÂµÌ„l |1 Ã—

(c)

X

PÌƒGMF (ÂµÌ„l , gl:l+râˆ’1 , Ï€l:l+râˆ’1 )(gl+r ) = QR QrP |Âµl âˆ’ ÂµÌ„l |1

l+1:l+r

Inequality (a) follows from Lemma 15 while (b) results from Lemma 16. Finally, (c) can be proven from the
definition (30). This concludes the lemma.

T

Proof of Lemma 19

Notice the following decomposition.
MF
N
E ÂµN
(ÂµN
t+1 âˆ’ P
t , gt , Ï€t ) 1
(a)

= E ÂµN
t+1 âˆ’

X X

N
â€²
N
N
N
P (xâ€² , u, ÂµN
t , gt )Ï€t (x , Âµt , gt )(u)Âµt (x)

xâ€² âˆˆX uâˆˆU

1

N
X

N
X X
1 X
1
(b) X
N
â€²
N
â€²
Î´(xit+1 = x) âˆ’
P (xâ€² , u, ÂµN
,
g
)(x)Ï€
(x
,
Âµ
,
g
)(u)
Î´(xit = xâ€² )
=
E
t
t
t
t
N i=1
N
â€²
i=1
x âˆˆX uâˆˆU

xâˆˆX

=

X

E

xâˆˆX

N
X

N
1 XX
1
N
i
N
N
Î´(xit+1 = x) âˆ’
P (xit , u, ÂµN
t , gt )(x)Ï€t (xt , Âµt , gt )(u) â‰¤ J1 + J2
N i=1
N i=1
uâˆˆU

Equality (a) uses the definition of P MF (Â·, Â·, Â·) as shown in (8) and equality (b) uses the definition of ÂµN
t . The
term J1 obeys the following.
N
N
X
1 X X
N
E
Î´(xit+1 = x) âˆ’
P (xit , uit , ÂµN
t , gt )(x)
N
i=1
i=1
xâˆˆX
" " N
##
N
X
X
X
(a) 1 p
1
i
i
i
N
N
N
N
N
â‰¤ âˆš
=
E E
Î´(xt+1 = x) âˆ’
P (xt , ut , Âµt , gt )(x) xt , gt , ut
|X |
N
N
i=1
i=1

J1 â‰œ

xâˆˆX

Inequality (a) is obtained applying Lemma 10, and the facts that {xit+1 }iâˆˆ{1,Â·Â·Â· ,N } are conditionally indeN
N
pendent given {xN
t , gt , ut }, and the following relations hold
h
i
N
N
N
E Î´(xit+1 = x) xN
,
g
,
u
= P (xit , uit , ÂµN
t
t
t
t , gt )(x),
i
X h
N
N
E Î´(xit+1 = x) xN
=1
t , gt , ut
xâˆˆX

âˆ€i âˆˆ {1, Â· Â· Â· , N }, and âˆ€x âˆˆ X . The second term satisfies the following bound.
N
N X
X
(a) 1 p
1 X X
i
i
N
N
N
i
N
N
E
P (xt , ut , Âµt , gt )(x) âˆ’
P (xit , u, ÂµN
|X |
J2 â‰œ
t , gt )(x)Ï€t (xt , Âµt , gt )(u) â‰¤ âˆš
N
N
i=1
i=1
xâˆˆX

uâˆˆU

32

Published in Transactions on Machine Learning Research (05/2023)

Inequality (a) is a result of Lemma 10. In particular, it uses the facts that, {uit }iâˆˆ{1,Â·Â·Â· ,N } are conditionally
N
independent given xN
t , gt , and the following relations hold
h
i X
N
N
N
i
N
N
E P (xit , uit , ÂµN
=
P (xit , u, ÂµN
t , gt )(x) xt , gt
t )(x)Ï€t (xt , Âµt , gt )(u),
uâˆˆU

X

i
h
N
N
N
=1
E P (xit , uit , ÂµN
t , gt )(x) xt , gt

xâˆˆX

âˆ€i âˆˆ {1, Â· Â· Â· , N }, and âˆ€x âˆˆ X . This concludes the Lemma.

U

Proof of Lemma 20

Observe the following decomposition.
E

1 X
N
MF
N
r(xit , uit , ÂµN
(ÂµN
t , gt ) âˆ’ r
t , gt , Ï€t )
N i=1

(a)

= E

N

XX
1 X
N
N
N
N
N
r(xit , uit , ÂµN
r(x, u, ÂµN
t , gt ) âˆ’
t , gt )Ï€t (x, Âµt , gt )(u)Âµt (x)
N i=1
xâˆˆX uâˆˆU

(b)

=E

N
X

N
XX
1
1 X
N
N
N
N
N
r(xit , uit , ÂµN
,
g
)
âˆ’
r(x,
u,
Âµ
,
g
)Ï€
(x,
Âµ
,
g
)(u)
Î´(xit = x)
t
t
t
t
t
t
t
N i=1
N i=1
xâˆˆX uâˆˆU

N
1
1 XX
N
N
i
N
N
r(xit , uit , ÂµN
,
g
)
âˆ’
r(xit , u, ÂµN
t
t
t , gt )Ï€t (xt , Âµt , gt )(u)
N i=1
N i=1
uâˆˆU
" " N
##
N
X
XX
1
i
i
N
N
i
N
N
i
N
N
N
N
= E E
r(xt , ut , Âµt , gt ) âˆ’
r(xt , u, Âµt , gt )Ï€t (xt , Âµt , gt )(u) xt , gt
N
i=1
i=1 uâˆˆU
##
" " N
N X
X
X
(c) M
M
R
N
N
i
i
N
N
i
N
N
i
N
N
â‰¤ âˆš
r0 (xt , ut , Âµt , gt ) âˆ’
r0 (xt , u, Âµt , gt )Ï€t (xt , Âµt , gt )(u) xt , gt
E E
=
N
N
i=1
i=1

=E

N
X

uâˆˆU

Equation (a) uses the definition of rMF (Â·, Â·) as shown in (10) while inequality (b) uses the definition of ÂµN
t .
The function r0 is given as, r0 (Â·, Â·, Â·, Â·) â‰œ r(Â·, Â·, Â·, Â·)/MR . Finally, relation (c) follows from Lemma 10. In
N
particular, it uses the fact that {uit }iâˆˆ{1,Â·Â·Â· ,N } are conditionally independent given xN
t , gt , and the following
relations hold.
N
|r0 (xit , uit , ÂµN
t , gt )| â‰¤ 1,
h
i X
N
N
N
N
i
N
N
E r0 (xit , uit , ÂµN
,
g
)
x
,
g
=
r0 (xit , u, ÂµN
t
t
t
t
t , gt )Ï€t (xt , Âµt , gt )(u)
uâˆˆU

âˆ€i âˆˆ {1, Â· Â· Â· , N }, âˆ€u âˆˆ U.

33

Published in Transactions on Machine Learning Research (05/2023)

V

Sampling Procedure

Algorithm 2 Sampling Algorithm
1: Input: Âµ0 , g0 , Ï€ Î¦j , P , PG , r
2: Sample u0 âˆ¼ Ï€Î¦j (x0 , Âµ0 , g0 )
3: Î½ 0 â† Î½ MF (Âµ0 , g0 , Ï€Î¦j ) where Î½ MF is defined in (7).
4: t â† 0
5: FLAG â† FALSE
6: while FLAG is FALSE do
7:
FLAG â† TRUE with probability 1 âˆ’ Î³.
8:
Execute Update
9: end while
10: T â† t
11: Accept (xT , ÂµT , gT , uT ) as a sample.
12: VÌ‚Î¦j â† 0, QÌ‚Î¦j â† 0
13: FLAG â† FALSE
14: SumRewards â† 0
15: while FLAG is FALSE do
16:
FLAG â† TRUE with probability 1 âˆ’ Î³.
17:
Execute Update
18:
SumRewards â† SumRewards + r(xt , ut , Âµt , gt , Î½ t )
19: end while
20: With probability 21 , VÌ‚Î¦j â† SumRewards. Otherwise QÌ‚Î¦j â† SumRewards.
21: AÌ‚Î¦j (xT , ÂµT , gT , uT ) â† 2(QÌ‚Î¦j âˆ’ VÌ‚Î¦j ).
22: Output: (xT , ÂµT , gT , uT ) and AÌ‚Î¦j (xT , ÂµT , gT , uT )

Procedure Update:
23: xt+1 âˆ¼ P (xt , ut , Âµt , gt , Î½ t ).
24: gt+1 âˆ¼ PG (xt , ut , Âµt , gt , Î½ t ).
25: Âµt+1 â† P MF (Âµt , Ï€Î¦j ) where P MF is defined in (8).
26: ut+1 âˆ¼ Ï€Î¦j (xt+1 , Âµt+1 , gt+1 )
27: Î½ t+1 â† Î½ MF (Âµt+1 , gt+1 , Ï€Î¦j )
28: t â† t + 1

EndProcedure

34

Published in Transactions on Machine Learning Research (05/2023)

References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22
(98):1â€“76, 2021.
Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-free algorithm for
ride-sharing using deep reinforcement learning. IEEE Transactions on Intelligent Transportation Systems,
20(12):4714â€“4727, 2019.
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu LauriÃ¨re. Unified reinforcement q-learning for mean field
game and control problems. Mathematics of Control, Signals, and Systems, pages 1â€“55, 2022.
Peter E Caines and Minyi Huang. Graphon mean field games and the gmfg equations: Îµ-nash equilibria. In
2019 IEEE 58th conference on decision and control (CDC), pages 286â€“292. IEEE, 2019.
RenÃ© Carmona, Mathieu LauriÃ¨re, and Zongjun Tan. Model-free mean-field reinforcement learning: meanfield mdp and mean-field q-learning. arXiv preprint arXiv:1910.12802, 2019.
Kai Cui and Heinz Koeppl. Learning graphon mean field games and approximate nash equilibria. arXiv
preprint arXiv:2112.01280, 2021.
Romuald Elie, Julien Perolat, Mathieu LauriÃ¨re, Matthieu Geist, and Olivier Pietquin. On the convergence of
model free learning in mean field games. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pages 7143â€“7150, 2020.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-field controls with Q-learning for cooperative
MARL: convergence and complexity analysis. SIAM Journal on Mathematics of Data Science, 3(4):1168â€“
1196, 2021.
Karl Hinderer. Lipschitz continuity of value functions in markovian decision processes. Mathematical Methods
of Operations Research, 62:3â€“22, 2005.
Yiheng Lin, Guannan Qu, Longbo Huang, and Adam Wierman. Multi-agent reinforcement learning in
stochastic networked systems. Advances in Neural Information Processing Systems, 34:7825â€“7837, 2021.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced) policy
gradient and natural policy gradient methods. Advances in Neural Information Processing Systems, 33:
7624â€“7636, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. nature, 518(7540):529â€“533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928â€“1937. PMLR, 2016.
Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V Ukkusuri. On the approximation
of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean field control (mfc).
Journal of Machine Learning Research, 23(129):1â€“46, 2022a.
Washim Uddin Mondal, Vaneet Aggarwal, and Satish Ukkusuri. Can mean field control (mfc) approximate cooperative multi agent reinforcement learning (marl) with non-uniform interaction? In The 38th
Conference on Uncertainty in Artificial Intelligence, 2022b.
Washim Uddin Mondal, Vaneet Aggarwal, and Satish Ukkusuri. On the near-optimality of local policies in
large cooperative multi-agent reinforcement learning. Transactions on Machine Learning Research, 2022c.
URL https://openreview.net/forum?id=t5HkgbxZp1.
35

Published in Transactions on Machine Learning Research (05/2023)

MÃ©dÃ©ric Motte and HuyÃªn Pham. Mean-field markov decision processes with common noise and open-loop
controls. The Annals of Applied Probability, 32(2):1421â€“1458, 2022.
Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient model-based multi-agent mean-field reinforcement learning. arXiv preprint arXiv:2107.04050, 2021.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &
Sons, 2014.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning for
networked systems with average reward. Advances in Neural Information Processing Systems, 33:2074â€“
2086, 2020.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In
International Conference on Machine Learning, pages 4295â€“4304. PMLR, 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. Advances in neural information
processing systems, 33:10199â€“10210, 2020.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37.
Citeseer, 1994.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
conference on machine learning, pages 5887â€“5896. PMLR, 2019.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field games. In
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages
251â€“259, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for
cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the
tenth international conference on machine learning, pages 330â€“337, 1993.
Xiaoqiang Wang, Liangjun Ke, Zhimin Qiao, and Xinghua Chai. Large-scale traffic signal control using a
novel multiagent reinforcement learning. IEEE transactions on cybernetics, 51(1):174â€“187, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279â€“292, 1992.
Nicholas J Watkins, Cameron Nowzari, Victor M Preciado, and George J Pappas. Optimal resource allocation for competitive spreading processes on bilayer networks. IEEE Transactions on Control of Network
Systems, 5(1):298â€“307, 2016.
Hua Wei, Chacha Chen, Guanjie Zheng, Kan Wu, Vikash Gayah, Kai Xu, and Zhenhui Li. Presslight:
Learning max pressure control to coordinate traffic signals in arterial network. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1290â€“1298, 2019.
Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha. Learning deep mean field games
for modeling large population behavior. arXiv preprint arXiv:1711.03156, 2017.

36

