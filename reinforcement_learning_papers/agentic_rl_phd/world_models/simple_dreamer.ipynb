{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_dreamer.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/danijar/dreamerv3\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Paper: \"World Models\" (Ha & Schmidhuber, 2018) & \"DreamerV3\" (Hafner et al., 2023)\n",
    "# Category: Model-Based RL\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Vision Model (V) / Encoder\n",
    "        # Compresses high-dim observation to latent vector\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # 2. Memory Model (M) / RSSM (Recurrent State Space Model)\n",
    "        # Predicts next latent state given current state and action\n",
    "        # z_{t+1} = f(z_t, a_t, h_t)\n",
    "        self.rnn = nn.GRUCell(latent_dim + action_dim, latent_dim)\n",
    "        \n",
    "        # 3. Controller (C) / Policy\n",
    "        # Takes latent state (not pixels) and outputs action\n",
    "        self.controller = nn.Linear(latent_dim, action_dim)\n",
    "        \n",
    "    def dream(self, initial_state, horizon=10):\n",
    "        \"\"\"\n",
    "        Dreaming: Simulating the future in the latent space.\n",
    "        Used to train the controller without real environment interaction.\n",
    "        \"\"\"\n",
    "        current_state = initial_state\n",
    "        dream_trajectory = []\n",
    "        \n",
    "        for _ in range(horizon):\n",
    "            # Agent chooses action based on hallucination\n",
    "            action = self.controller(current_state)\n",
    "            \n",
    "            # World Model predicts what happens next\n",
    "            # (Simplified: typically involves mixing deterministic RNN state + stochastic latent state)\n",
    "            next_state = self.rnn(torch.cat([current_state, action], dim=1))\n",
    "            \n",
    "            dream_trajectory.append((current_state, action, next_state))\n",
    "            current_state = next_state\n",
    "            \n",
    "        return dream_trajectory\n",
    "\n",
    "# Note: DreamerV3 adds 'Symlog' scaling, discrete autoencoders, and KL balancing.\n",
    "# This is a simplified \"Ha & Schmidhuber\" style architecture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}