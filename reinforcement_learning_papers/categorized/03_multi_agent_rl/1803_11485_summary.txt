Here is the extracted information:

**PAPER:** QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning
**TITLE:** QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning
**ARXIV_ID:** 1803.11485v2
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** QMIX is a novel value-based method that can train decentralized policies in a centralized end-to-end fashion. It employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. QMIX structurally enforces that the joint-action value is monotonic in the per-agent values, allowing for tractable maximization of the joint action-value in off-policy learning and guaranteeing consistency between the centralized and decentralized policies.
**KEY_CONTRIBUTIONS:**
* QMIX can represent a much richer class of action-value functions than existing methods like VDN.
* QMIX allows for efficient use of extra state information during training.
* QMIX outperforms existing value-based multi-agent reinforcement learning methods in decentralized unit micromanagement tasks in StarCraft II.
* QMIX can learn to protect vulnerable units and position units to avoid enemy attacks.
* QMIX can learn complex strategies like focus firing and unit positioning.
