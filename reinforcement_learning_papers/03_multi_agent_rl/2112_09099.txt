Decentralized Mean Field Games
Sriram Ganapathi Subramanian1, 3 , Matthew E. Taylor2, 4 , Mark Crowley1 , Pascal Poupart1, 3
1

University of Waterloo, Waterloo, Ontario, Canada
University of Alberta, Edmonton, Alberta, Canada
3
Vector Institute, Toronto, Ontario, Canada
4
Alberta Machine Intelligence Institute (Amii), Edmonton, Alberta, Canada
s2ganapa@uwaterloo.ca, matthew.e.taylor@ualberta.ca, mcrowley@uwaterloo.ca, ppoupart@uwaterloo.ca

arXiv:2112.09099v5 [cs.MA] 13 Apr 2022

2

Abstract
Multiagent reinforcement learning algorithms have not been
widely adopted in large scale environments with many agents
as they often scale poorly with the number of agents. Using
mean field theory to aggregate agents has been proposed as a
solution to this problem. However, almost all previous methods in this area make a strong assumption of a centralized
system where all the agents in the environment learn the same
policy and are effectively indistinguishable from each other.
In this paper, we relax this assumption about indistinguishable
agents and propose a new mean field system known as Decentralized Mean Field Games, where each agent can be quite
different from others. All agents learn independent policies
in a decentralized fashion, based on their local observations.
We define a theoretical solution concept for this system and
provide a fixed point guarantee for a Q-learning based algorithm in this system. A practical consequence of our approach
is that we can address a ‘chicken-and-egg’ problem in empirical mean field reinforcement learning algorithms. Further,
we provide Q-learning and actor-critic algorithms that use the
decentralized mean field learning approach and give stronger
performances compared to common baselines in this area. In
our setting, agents do not need to be clones of each other
and learn in a fully decentralized fashion. Hence, for the first
time, we show the application of mean field learning methods in fully competitive environments, large-scale continuous
action space environments, and other environments with heterogeneous agents. Importantly, we also apply the mean field
method in a ride-sharing problem using a real-world dataset.
We propose a decentralized solution to this problem, which is
more practical than existing centralized training methods.

1

Introduction

Most multiagent reinforcement learning (MARL) algorithms
are not tractable when applied to environments with many
agents (infinite in the limit) as these algorithms are exponential in the number of agents (Busoniu, Babuska, and De Schutter 2006). One exception is a class of algorithms that use the
mean field theory (Stanley 1971) to approximate the many
agent setting to a two agent setting, where the second agent
is a mean field distribution of all agents representing the average effect of the population. This makes MARL algorithms
Copyright © 2022, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

tractable since, effectively, only two agents are being modelled. Lasry and Lions (2007) introduced the framework of a
mean field game (MFG), which incorporates the mean field
theory in MARL. In MARL, the mean field can be a population state distribution (Huang 2010) or action distribution
(Yang et al. 2018) of all the other agents in the environment.
MFGs have three common assumptions. First, each agent
does not have access to the local information of the other
agents. However, it has access to accurate global information
regarding the mean field of the population. Second, all agents
in the environment are independent, homogeneous, and indistinguishable. Third, all agents maintain interactions with
others only through the mean field. These assumptions (especially the first two) severely restrict the potential of using
mean field methods in real-world environments. The first assumption is impractical, while the second assumption implies
that all agents share the same state space, action space, reward
function, and have the same objectives. Further, given these
assumptions, prior works use centralized learning methods,
where all agents learn and update a shared centralized policy.
These two assumptions are only applicable to cooperative environments with extremely similar agents. Theoretically, the
agent indices are omitted since all agents are interchangeable
(Lasry and Lions 2007). We will relax the first two assumptions. In our case, the agents are not interchangeable and can
each formulate their own policies during learning that differs
from others. Also, we will not assume the availability of the
immediate global mean field. Instead, agents only have local
information and use modelling techniques (similar to opponent modelling common in MARL (Hernandez-Leal, Kartal,
and Taylor 2019)) to effectively model the mean field during the training process. We retain the assumption that each
agent’s impact on the environment is infinitesimal (Huang
2010), and hence agents calculate best responses only to the
mean field. Formulating best responses to each individual
agent is intractable and unnecessary (Lasry and Lions 2007).
The solution concepts proposed by previous mean field
methods have been either the centralized Nash equilibrium
(Yang et al. 2018) or a closely related mean field equilibrium (Lasry and Lions 2007). These solution concepts are
centralized as they require knowledge of the current policy of all other agents or other global information, even in
non-cooperative environments. Verifying their existence is
infeasible in many practical environments (Neumann 1928).

This work presents a new kind of mean field system, Decentralized Mean Field Games (DMFGs), which uses a decentralized information structure. This new formulation of
the mean field system relaxes the assumption of agents’ indistinguishability and makes mean field methods applicable
to numerous real-world settings. Subsequently, we provide a
decentralized solution concept for learning in DMFGs, which
will be more practical than the centralized solution concepts
considered previously. We also provide a fixed point guarantee for a Q-learning based algorithm in this system.
A ‘chicken-and-egg’ problem exists in empirical mean
field reinforcement learning algorithms where the mean field
requires agent policies, yet the policies cannot be learned
without the global mean field (Yang and Wang 2020). We
show that our formulation can address this problem, and we
provide practical algorithms to learn in DMFGs. We test our
algorithms in different types of many agent environments.
We also provide an example of a ride-sharing application that
simulates demand and supply based on a real-world dataset.

2

Background

Stochastic Game: An N -player stochastic (Markovian) game can be represented as a tuple
hS, A1 , . . . , AN , r1 , . . . , rN , p, γi, where S is the state
space, Aj represents the action space of the agent
j ∈ {1, . . . , N }, and rj : S ×A1 ×· · ·×AN →
− R represents
the reward function of j. Also, p : S×A1 ×· · ·×AN →
− Ω(S)
represents the transition probability that determines the next
state given the current state and the joint action of all agents.
Here Ω(S) is a probability distribution over the state space.
In the stochastic game, agents aim to maximize the expected
discounted sum of rewards, with discount factor γ ∈ [0, 1).
Each agent j in the stochastic game formulates a policy, which is denoted by π j : S −
→ Ω(Aj ) where the
joint policy is represented as π = [π 1 , . . . , π N ] for all
s ∈ S. Given an initial state s, the value function of the
agent j under the joint policy π can be represented as
P∞
v j (s|π) = t=0 γ t E[rtj |s0 = s, π]. The solution concept is
the Nash equilibrium (NE) (Hu and Wellman 2003). This is
represented by a joint policy π ∗ = [π∗1 , . . . , π∗N ], such that,
−j
j
j
for all π j , v j (s|π∗j , π −j
∗ ) ≥ v (s|π , π ∗ ), for all agents
j ∈ {1, . . . , N } and all states s ∈ S. The notation π −j
∗
represents the joint policy of all agents except the agent j.
Mean Field Game: MFG was introduced as a framework
to solve the stochastic game when the number of agents N is
very large (Lasry and Lions 2007; Huang et al. 2006). In this
setting, calculating the best response to each individual opponent is intractable, so each agent responds
to the aggregated
P
N

1(sj =s)

t
j=1
state distribution µt (s) , limN →
, known as
−∞
N
the mean field. Here 1 denotes the indicator function, i.e.
1(x) = 1, if x is true and 0 otherwise. Let µ , {µt }∞
t=0 .
MFG assumes that all agents are identical (homogeneous),
indistinguishable, and interchangeable (Lasry and Lions
2007). Given this assumption, the environment changes to
a single-agent stochastic control problem where all agents
share the same policy (Saldi, Basar, and Raginsky 2018).
Hence, π 1 = · · · = π N = π. The theoretical formulation
focuses on a representative player, and the solution of this

player (optimal policy) obtains the solution for the entire
system. The value function ofPa representative agent can be

∞
t
given as V (s, π, µ) , Eπ
t=0 γ r(st , at , µt )|s0 = s ,
where s and a are the state and action of the representative agent, respectively. The transition dynamics is represented as P (st , at , µt ), where the dependence is on the mean
field distribution. All agents share the same reward function
r(st , at , µt ). The action comes from the policy πt (at |st , µt ).
The solution concept for the MFG is the mean field
equilibrium (MFE). A tuple (π ∗M F G , µ∗M F G ) is a mean
field equilibrium if, for any policy π, an initial state s ∈
S and given mean field µ∗M F G , V (s, π ∗M F G , µ∗M F G ) ≥
V (s, π, µ∗M F G ). Additionally, µ∗M F G is the mean field obtained when all agents play the same π ∗M F G at each s ∈ S.
Mean Field Reinforcement Learning (MFRL): MFRL,
introduced by Yang et al. (2018), is another approach for
learning in stochastic games with a large number of agents.
Here, the empirical mean action is used as the mean field,
which each agent then uses to update its Q-function and
Boltzmann policy. Each agent is assumed to have access to
the global state, and it takes local action from this state. In
MFRL, the Q-function of the agent j is updated as,
Qj (st , ajt , µat )
= (1 − α)Qj (st , ajt , µat ) + α[rtj + γv j (st+1 )]

(1)

where
v j (st+1 ) =

X

π j (ajt+1 |st+1 , µat )Qj (st+1 , ajt+1 , µat ) (2)

ajt+1

µat =

1 X j j
a , a ∼ π j (·|st , µat−1 )
N j t t

π j (ajt |st , µat−1 ) = P

(3)

exp(−β̂Qj (st , ajt , µat−1 )
0

0

ajt ∈Aj

exp(−β̂Qj (st , ajt , µat−1 ))

(4)
where st is the global old state, st+1 is the global resulting
state, rtj is the reward of j at time t, v j is the value function of
j, N is the total number of agents, and β̂ represents the Boltzmann parameter. The action aj is assumed to be discrete and
represented using one-hot encoding. Like stochastic games,
MFRL uses the NE as the solution concept. The global mean
field µat captures the action distribution of all agents. To address this global limitation, Yang et al. (2018) specify mean
field calculation over certain neighbourhoods for each agent.
However, an update using Eq. 3 requires each agent to have
access to all other agent policies or the global mean field to
use the centralized concept (NE). We omit an expectation in
Eq. 2 since Yang et al. (2018) guaranteed that their updates
will be greedy in the limit with infinite exploration (GLIE).
From Eq. 3 and Eq. 4, it can be seen that the current action
depends on the mean field and the mean field depends on the
current action. To resolve this ‘chicken-and-egg’ problem,
Yang et al. (2018) simply use the previous mean field action
to decide the current action, as in Eq. 4. This can lead to a
loss of performance since the agents are formulating best
responses to the previous mean field action µat−1 , while they
are expected to respond to the current mean field action µat .

3

Related Work

MFGs were first proposed in Huang, Caines, and Malhamé
(2003), while a comprehensive development of the system
and principled application methods were given later in Lasry
and Lions (2007). Subsequently, learning algorithms were
proposed for this framework. Subramanian and Mahajan
(2019) introduce a restrictive form of MFG (known as stationary MFG) and provide a model-free policy-gradient (Sutton
et al. 1999; Konda and Tsitsiklis 1999) algorithm along with
convergence guarantees to a local Nash equilibrium. On similar lines, Guo et al. (2019) provide a model-free Q-learning
algorithm (Watkins and Dayan 1992) for solving MFGs, also
in the stationary setting. The assumptions in these works are
difficult to verify in real-world environments. Particularly,
Guo et al. (2019) assume the presence of a game engine (simulator) that accurately provides mean field information to
all agents at each time step, which is not practical in many
environments. Further, other works depend on fictitious play
updates for the mean field parameters (Hadikhanloo and Silva
2019; Elie et al. 2020), which involves the strong assumption
that opponents play stationary strategies. All these papers use
the centralized setting for the theory and the experiments.
Prior works (Gomes, Mohr, and Souza 2010; Adlakha, Johari, and Weintraub 2015; Saldi, Basar, and Raginsky 2018)
have established the existence of a (centralized) mean field
equilibrium in the discrete-time MFG under a discounted cost
criterion, in finite and infinite-horizon settings. Authors have
also studied the behaviour of iterative algorithms and provided theoretical analysis for learning of the non-stationary
(centralized) mean field equilibrium in infinite-horizon settings (Wi˛ecek and Altman 2015; Wi˛ecek 2020; Anahtarci,
Kariksiz, and Saldi 2019). We provide similar guarantees in
the decentralized setting with possibly heterogeneous agents.
Yang et al. (2018) introduces MFRL that uses a mean field
approximation through the empirical mean action, and provides two practical algorithms that show good performance
in large MARL settings. The approach is model-free and
the algorithms do not need strong assumptions regarding the
nature of the environment. However, they assume access to
global information that needs a centralized setting for both
theory and experiments. MFRL has been extended to multiple types (Subramanian et al. 2020) and partially observable
environments (Subramanian et al. 2021). However, unlike us,
Subramanian et al. (2020) assumes that agents can be divided
into a finite set of types, where agents within a type are homogeneous. Subramanian et al. (2021) relaxes the assumption of
global information in MFRL, however, it still uses the Nash
equilibrium as the solution concept. Further, that work contains some assumptions regarding the existence of conjugate
priors, which is hard to verify in real-world environments.
Additional related work is provided in Appendix L.

4

Decentralized Mean Field Game

The DMFG model is specified by hS, A, p, R, µ0 i, where
S = S 1 × · · · × S N represents the state space and A =
A1 × · · · × AN represents the joint action space. Here, S j
represents the state space of an agent j ∈ {1, . . . , N } and
Aj represents the action space of j. As in MFGs, we are

considering the infinite population limit of the game, where
the set of agents N , satisfy N →
− ∞. Similar to the MFG
formulation in several prior works (Lasry and Lions 2007;
Huang et al. 2006; Saldi, Basar, and Raginsky 2018), we will
specify that both the state and action spaces are Polish spaces.
Particularly, Aj for all agents j, is a compact subset of a finite
dimensional Euclidean space <d with the Euclidean distance
norm || · ||. Since all agents share the same environment,
for simplicity, we will also assume that the state spaces of
all the agents are the same S 1 = · · · = S N = S and are
locally compact. Since the state space is a complete separable
metric space (Polish space), it is endowed with a metric
dX . The transition function p : S × Aj × P(S) →
− P(S)
determines the next state of any j given the current state
and action of j, and the probability distribution of the state
in the system (represented by the mean field). The reward
function is represented as a set R = {R1 , . . . , RN }, where,
Rj : S × Aj × P(S) →
− [0, ∞) is the reward function of j.
Recall that a DMFG has two major differences as compared to MFG and MFRL. 1) DMFG does not assume that the
agents are indistinguishable and homogeneous (agent indices
are retained). 2) DMFG does not assume that each agent can
access the global mean field of the system. However, each
agents’ impact on the environment is infinitesimal, and therefore all agents formulate best responses only to the mean
field of the system (no per-agent modelling is required).
As specified, in DMFG, the transition and reward functions
for each agent depends on the mean field of the environment,
represented by µ , (µt )t≥0 , with the initial mean field represented as µ0 . For DMFG the mean field can either
correspond
P
N

1(sj =s)

t
j=1
to the state distribution µt (s) , limN →
, or
−∞
N
a
the action distribution µt as in Eq. 3 (for discrete settings, or
a mixture of Dirac measures for continuous spaces as used
in Anahtarci, Kariksiz, and Saldi (2019)). Without a loss of
generality, we use the mean field as the state distribution µt
(represented by P(S)), as done in prior works (Lasry and
Lions 2007; Elliott, Li, and Ni 2013). However, our setting
and theoretical results will hold for the mean field as action
distribution µat as well.
In the DMFG, each agent j will not have access to the true
mean field of the system and instead use appropriate techniques to actively model the mean field through exploration.
The agent j holds an estimate of the actual mean field represented by µjt . Let µj , (µjt )t≥0 . Let, M be used to denote
the set of mean fields {µj ∈ P(S)}. A Markov policy for an
agent j is a stochastic kernel on the action space Aj given the
immediate local state (sjt ) and the agent’s current estimated
mean field µjt , i.e. πtj : S × P(S) →
− P(Aj ), for each t ≥ 0.
Alternatively, a non-Markov policy will depend on the entire
state-action history of game play. We will use Πj to denote
a set of all policies (both Markov and non-Markov) for the
agent j. Let sjt represent the state of an agent j at time t
and ajt represent the action of j at t. Then an agent j tries
to maximize the objective function given by the following
equation (where rj denotes the immediate reward obtained
by the agent j and β ∈ [0, 1) denotes the discount factor),
j P∞
(5)
Jµj (π j ) , Eπ [ t=0 β t rj (sjt , ajt , µt )].

In line with prior works (Yang et al. 2018; Subramanian
et al. 2020), we assume that each agent’s sphere of influence is restricted by its neighbourhood, where it conducts
exploration. Using this assumption, we assert that, after a
finite t, the mean field estimate will accurately reflect the
true mean field for j, in its neighbourhood denoted as N j .
This assumption specifies that agents have full information in
their neighbourhood, and they can use modelling techniques
to obtain accurate mean field information within the neighbourhood (also refer to flocking from Perrin et al. (2021)).
Assumption 1. There exists a finite time T and a neighbourhood N j , such that for all t > T , the mean field estimate of
an agent j ∈ 1, . . . , N satisfies (∀sj ∈ N j ) µj (sj ) = µ(sj ).
Also, ∀sj ∈ N j , we have, p(·|sjt , ajt , µjt ) = p(·|sjt , aj , µt )
and rj (·|sjt , ajt , µjt ) = rj (·|sjt , ajt , µt ).
Let us define a set Φ : M →
− 2Π as Φ(µj ) = {π j ∈
j
j
Π : π is optimal for µ }. Conversely, for j, we define a
mapping Ψ : Π →
− M as, given a policy π j ∈ Πj , the mean
field state estimate µj , Ψ(π j ) can be constructed as,
j

µjt+1 (·) =

R

j

p(·|sjt , ajt , µt )P π (ajt |sjt , µjt )µjt (sjt ).
S×Aj
(6)
j
Here P π is a probability measure induced by π j . Later (in
Theorem 1) we will prove that restricting ourselves to Markov
j
policies is sufficient in a DMFG, and hence P π = π j .
Now, we can define the decentralized mean field equilibrium (DMFE) which is the solution concept for this game.
Definition 1. The decentralized mean field equilibrium of
an agent j is represented as a pair (π∗j , µj∗ ) ∈ Πj × M if
π∗j ∈ Φ(µj∗ ) and µj∗ = Ψ(π∗j ). Here π∗j is the best response
to µj∗ and µj∗ is the mean field estimate of j when it plays π∗j .
The important distinction between DMFE and centralized
concepts, such as NE and MFE, is that DMFE does not rely
on the policy information of other agents. MFE requires all
agents to play the same policy, and NE requires all agents to
have access to other agents’ policies. DMFE has no such constraints. Hence, this decentralized solution concept is more
practical than NE and MFE. In Appendix F, we summarize
the major differences between the DMFG, MFG and MFRL.

5

Theoretical Results

We provide a set of theorems that will first guarantee the
existence of the DMFE in a DMFG. Further, we will show
that a simple Q-learning update will converge to a fixed point
representing the DMFE. We will borrow relevant results from
prior works in centralized MFGs in our theoretical guarantees.
Particularly, we aim to adapt the results and proof techniques
in works by Saldi, Basar, and Raginsky (2018), Lasry and
Lions (2007), and Anahtarci, Kariksiz, and Saldi (2019) to
the decentralized setting. The statements of all our theorems
are given here, while the proofs are in Appendices A – E.
Similar to an existing result from centralized MFG (Lasry
and Lions 2007), in the DMFG, restricting policies to only
Markov policies would not lead to any loss of optimality. We
use ΠjM to denote the set of Markov policies for the agent j.

Theorem 1. For any mean field, µ ∈ M, and an agent
j ∈ {1, . . . , N }, we have,
sup Jµj (π j ) = sup Jµj (π j ).

π j ∈Πj

(7)

π j ∈ΠjM

Next, we show the existence of a DMFE under a set of
assumptions similar to those previously used in the centralized MFG (Lasry and Lions 2007; Saldi, Basar, and Raginsky
2018). The assumptions pertain to bounding the reward function and imposing restrictions on the nature of the mean field
(formal statements in Appendix B). We do not need stronger
assumptions than those previously considered for the MFGs.
Theorem 2. An agent j ∈ {1, . . . , N } in the DMFG admits
a decentralized mean field equilibrium (π∗j , µj∗ ) ∈ Πj × M.
We use C to denote a set containing bounded functions in
S. Now, we define a decentralized mean field operator (H),
H : C × P(S) 3 (Qj , µj )
(8)
→
− (H1 (Qj , µj ), H2 (Qj , µj )) ∈ C × P(S)
where
H1 (Qj , µj )(sjt , ajt ) , rj (sjt , ajt , µt )
+β

(9)

R

Qjmaxaj (sjt+1 , aj , µjt+1 )p(sjt+1 |sjt , ajt , µt )
S

H2 (Qj , µj )(·)
R


, S×Aj p · |sjt , π j (sjt , Qjt , µjt ), µt µjt (s)
(10)
for an agent j. Here, π j is a maximiser of the operator H1 .
For the rest of the theoretical results, we consider a set
of assumptions different from those needed for Theorem 2.
Here we assume that the reward and transition functions
are Lipschitz continuous with a suitable Lipschitz constant.
The Lipschitz continuity assumption is quite common in the
mean field literature (Yang et al. 2018; Lasry and Lions 2007;
Subramanian et al. 2020). We also consider some further
assumptions regarding the nature of gradients of the value
function (formal statements in Appendix C). All assumptions
are similar to those considered before for the analysis in
the centralized MFGs (Lasry and Lions 2007; Anahtarci,
Kariksiz, and Saldi 2019; Huang 2010). First, we provide
a theorem regarding the nature of operator H. Then, we
provide another theorem showing that H is a contraction.
Theorem 3. The decentralized mean field operator H is
well-defined, i.e., this operator maps C × P(S) to itself.
Theorem 4. Let B represent the space of bounded functions
in S. Then the mapping H : C × P(S) →
− C × P(S) is a
contraction in the norm of B(S).
Since H is a contraction, by the Banach fixed point theorem (Shukla, Balasubramanian, and Pavlović 2016), we can
obtain that fixed point using the Q iteration algorithm given
by Alg. 1. We provide this result in the next theorem.
Theorem 5. Let the Q-updates in Algorithm 1 converge to
(Qj∗ , µj∗ ) for an agent j ∈ {1, . . . , N }. Then, we can construct a policy π∗j from Qj∗ using the relation,
π∗j (sj ) = arg max
Qj∗ (sj , aj , µj∗ ).
(11)
j
j
a ∈A

Algorithm 1: Q-learning for DMFG
1: For each agent j ∈ {1, . . . , N }, start with initial Q-

function Qj0 and the initial mean field state estimate µj0
j

j

2: while (Qjn , µjn ) 6= (Qn−1 , µn−1 ) do

(Qjn+1 , µjn+1 ) = H(Qjn , µjn )

3:
4: end while
j
j
5: Return the fixed point (Q∗ , µ∗ ) of H

since it depends on the current mean field estimate, unlike
MFRL which used the previous global mean field in Eq. 4.
We provide a neural network-based Q-learning implementation for our update equations, namely Decentralized Mean
Field Game Q-learning (DMFG-QL), and an actor-critic implementation, Decentralized Mean Field Game Actor-Critic
(DMFG-AC). Detailed description of the algorithms are in
Appendix H (see Algs. 2 and 3). A complexity analysis is in
Appendix K, and hyperparameter details are in Appendix J.

7
Then the pair (π∗j , µj∗ ) is a DMFE.
Hence, from the above results, we have proved that a
DMFG admits a DMFE, and an iterative algorithm using
Q-updates can arrive at this equilibrium. This provides a
fixed point for Alg. 1, to which the Q-values converge.

6

Algorithms

We will apply the idea of decentralized updates to the modelfree MFRL framework. We modify the update equations
in MFRL (Section 2) and make them decentralized, where
agents only observe their local state and do not have access
to the immediate global mean field. Our new updates are:
Qj (sjt , ajt , µj,a
t )
j
j j
= (1 − α)Qj (sjt , ajt , µj,a
t ) + α[rt + γv (st+1 )]

(12)

where
v j (sjt+1 ) =

X

j
j,a
j j
π j (ajt+1 |sjt+1 , µj,a
t+1 )Q (st+1 , at+1 , µt+1 )

ajt+1

(13)
j,a
j j
µj,a
t = f (st , µ̂t−1 )

(14)

exp(−β̂Q (sjt , ajt , µj,a
t ))
and π j (ajt |sjt , µj,a
)
=
P
0
t
j
j
0
exp(−β̂Q (st , ajt , µj,a
t ))
aj ∈Aj
j

t

(15)
Here, sjt is the local state and µj,a
t is the mean field action
estimate for the agent j at time t and µ̂j,a
t−1 is the observed
local mean field action of j at t − 1. Other variables have
the same meaning as Eq. 1 – Eq. 4. In Eq. 14, the mean field
estimate for j is updated using a function of the current state
and the previous local mean field. Opponent modelling techniques commonly used in MARL (Hernandez-Leal, Kartal,
and Taylor 2019) can be used here. We use the technique
of He and Boyd-Graber (2016), that used a neural network
to model the opponent agent(s). In our case, we use a fully
connected neural network (2 Relu layers of 50 nodes and an
output softmax layer) to model the mean field action. The
network takes the current state and previous mean field action
as inputs and outputs the estimated current mean field. This
network is trained using a mean square error between the
estimated mean field action from the network (Eq. 14) and
the observed local mean field (local observation of actions of
other agents µ̂j,a
t ) after action execution. The policy in Eq. 15
does not suffer from the ‘chicken-and-egg’ problem of Eq. 4

Experiments and Results

In this section we study the performance of our algorithms.
The code for experiments has been open-sourced (Subramanian 2021). We provide the important elements of our
domains here, while the complete details are in Appendix I.
The first five domains belong to the MAgent environment
(Zheng et al. 2018). We run the experiments in two phases,
training and execution. Analogous to experiments conducted
in previous mean field studies (Yang et al. 2018; Subramanian et al. 2020), all agents train against other agents playing
the same algorithm for 2000 games. This is similar to multiagent training using self-play (Shoham, Powers, and Grenager
2003). The trained agents then enter into an execution phase,
where the trained policies are simply executed. The execution is run for 100 games, where algorithms may compete
against each other. We consider three baselines, independent
Q-learning (IL) (Tan 1993), mean field Q-learning (MFQ)
(Yang et al. 2018), and mean field actor-critic (MFAC) (Yang
et al. 2018). Each agent in our implementations learns in a
decentralized fashion, where it maintains its own networks
and learns from local experiences. This is unlike centralized
training in prior works (Yang et al. 2018; Guo et al. 2019). We
repeat experiments 30 times, and report the mean and standard deviation. Wall clock times are given in Appendix M.
First, we consider the mixed cooperative-competitive Battle game (Zheng et al. 2018). This domain consists of two
teams of 25 agents each. Each agent is expected to cooperate
with agents within the team and compete against agents of the
other team to win the battle. For the training phase, we plot
the cumulative rewards per episode obtained by the agents
of the first team for each algorithm in Fig. 1(a). The performance of the second team is also similar (our environment
is not zero-sum). From the results, we see that DMFG-QL
performs best while the others fall into a local optimum and
do not get the high rewards. The DMFG-AC algorithm comes
second. It has been noted previously (Yang et al. 2018) that
Q-learning algorithms often perform better compared to their
actor-critic counterparts in mean field environments. MFQ
and MFAC (using the previous mean field information) performs poorly compared to DMFG-QL and DMFG-AC (using
the current estimates). Finally, IL loses out to others due to its
independent nature. In execution, one team trained using one
algorithm competes against another team from a different
algorithm. We plot the percentage of games won by each algorithm in a competition against DMFG-QL and DMFG-AC.
A game is won by the team that kills more of its opponents.
The performances are in Fig. 1(b) and (c), where DMFG-QL
performs best. In Appendix G, we show that DFMG-QL can
accurately model the true mean field in the Battle game.

(a) Training
(a) Training

(b) Execution

Figure 3: Gather results

(b) Execution vs. DMFG-QL

(c) Execution vs. DMFG-AC

Figure 1: Battle results. In (a) solid lines show average and
shaded regions represent standard deviation. In (b) and (c)
bars are average and black lines represent standard deviation.

The second domain is the heterogeneous Combined Arms
environment. This domain is a mixed setting similar to Battle
except that each team consists of two different types of agents,
ranged and melee, with distinct action spaces. Each team has
15 ranged and 10 melee agents. This environment is different
from those considered in Subramanian et al. (2020), which
formulated each team as a distinct type, where agents within
a team are homogeneous. The ranged agents are faster and
attack further, but can be killed quickly. The melee agents are
slower but are harder to kill. We leave out MFQ and MFAC
for this experiment, since both these algorithms require the
presence of fully homogeneous agents. The experimental
procedure is the same as in Battle. From the results we see
that DMFG-QL performs best in both phases (see Fig. 2).

(a) Training

the maximum performance. In competitive environments, actively formulating the best responses to the current strategies
of opponents is crucial for good performances. Predictably,
the MFQ and MFAC algorithms (relying on previous information) lose out. For execution, we sample (at random) six
agents from each of the five algorithms to make a total of 30.
We plot the percentage of games won by each algorithm in a
total of 100 games. A game is determined to have been won
by the agent obtaining the most rewards. Again, DMFG-QL
shows the best performance during execution (Fig. 3(b)).
The next domain is a fully cooperative Tiger-Deer environment. In this environment, a team of tigers aims to kill deer.
The deer are assumed to be part of the environment moving
randomly, while the tigers are agents that learn to coordinate
with each other to kill the deer. At least two tigers need to attack a deer in unison to gain large rewards. Our environment
has 20 tigers and 101 deer. In the training phase, we plot the
average reward obtained by the tigers (Fig. 4(a)). The performance of MFQ almost matches that of DMFG-QL and the
performance of DMFG-AC matches MFAC. In a cooperative
environment, best responses to actively changing strategies
of other agents are not as critical as in competitive environments. Here all agents aid each other and using the previous
time information (as done in MFQ and MFAC) does not
hurt performance as much. For execution, a set of 20 tigers
from each algorithm execute their policy for 100 games. We
plot the average number of deer killed by the tigers for each
algorithm. DMFG-QL gives the best performance (Fig. 4(b)).

(b) Execution

Figure 2: Combined Arms results
(a) Training

Next is the fully competitive Gather environment. This contains 30 agents trying to capture limited food. All the agents
compete against each other for capturing food and could resort to killing others when the food becomes scarce. We plot
the average rewards obtained by each of the five algorithms in
the training phase (Fig. 3(a)). DMFG-QL once again obtains

(b) Execution

Figure 4: Tiger-Deer results
The next domain is the continuous action Waterworld domain, first introduced by Gupta, Egorov, and Kochenderfer
(2017). This is also a fully cooperative domain similar to

(a) Training

(b) Execution

(a) # of Vehicles

(b) Maximum Pickup Delay

(c) Capacity

(d) Single Day Test

Figure 5: Waterworld results

Tiger-Deer, where a group of 25 pursuer agents aim to capture a set of food in the environment while actively avoiding
poison. The action space corresponds to a continuous thrust
variable. We implement DMFG-AC in this domain, where
the mean field is a mixture of Dirac deltas of actions taken by
all agents. The experimental procedure is the same as TigerDeer. For this continuous action space environment, we use
proximal policy optimization (PPO) (Schulman et al. 2017)
and deep deterministic policy gradient (DDPG) (Lillicrap
et al. 2016), as baselines. We see that DMFG-AC obtains the
best performance in both phases (refer to Fig. 5(a) and (b)).
Our final environment is a real-world Ride-pool Matching
Problem (RMP) introduced by Alonso-Mora et al. (2017).
This problem pertains to improving the efficiency of vehicles
satisfying ride requests as part of ride-sharing platforms such
as UberPool. In our environment, ride requests come from the
open source New York Yellow Taxi dataset (NYYellowTaxi
2016). The road network (represented as a grid with a finite
set of nodes or road intersections) contains a simulated set of
vehicles (agents) that aim to serve the user requests. Further
details about this domain are in Appendix I. We consider two
baselines in this environment. The first is the method from
Alonso-Mora et al. (2017), which used a constrained optimization (CO) approach to match ride requests to vehicles.
This approach is hard to scale and is myopic in assigning
requests (it does not consider future rewards). The second
baseline is the Neural Approximate Dynamic Programming
(NeurADP) method from Shah, Lowalekar, and Varakantham
(2020), which used a (centralized) DQN algorithm to learn
a value function for effective mapping of requests. This approach assumes all agents are homogenous (i.e., having the
same capacity and preferences), which is impractical. To
keep comparisons fair, we consider a decentralized version of
NeurADP as our baseline. Finally, we implement DMFG-QL
for this problem where the mean-field corresponds to the
distribution of ride requests at every node in the environment.
Similar to prior approaches (Lowalekar, Varakantham, and
Jaillet 2019; Shah, Lowalekar, and Varakantham 2020), we
use the service rate (total percentage of requests served) as
the comparison metric. We train NeurADP and DMFG-QL
using a set of eight consecutive days of training data and test
all the performances in a previously unseen test set of six
days. The test results are reported as an average (per day)
of performances in the test set pertaining to three different
hyperparameters. The first is the capacity of the vehicle (c)

Figure 6: Results for the ride-sharing experiment. For (a), (b)
and (c), we start with a prototypical configuration of c=10, τ
= 580, and N = 100, and then vary the different parameters.
Figures (a), (b) and (c) share the same legend given in (a).
varied from 8 to 12, the second is the maximum allowed
waiting time (τ ) varied from 520 seconds to 640 seconds, and
the last is the number of vehicles (N ), varied from 80 to 120.
The results in Figs. 6(a-c) show that DMFG-QL outperforms
the baselines in all our test cases. The mean field estimates in
DMFG-QL help predict the distribution of ride requests in the
environment, based on which agents can choose ride requests
strategically. If agents choose orders that lead to destinations
with a high percentage of requests, they will be able to serve
more requests in the future. Thus, DMFG-QL outperforms
the NeurADP method (which does not maintain a mean field).
We note that the service rate for all algorithms is low in this
study (10% – 30%), since we are considering fewer vehicles
compared to prior works (Shah, Lowalekar, and Varakantham 2020) due to the computational requirements of being
decentralized. In practice our training is completely parallelizable and this is not a limitation of our approach. Also, from
Fig. 6(c), an increase in the number of vehicles, increases the
service rate. In Fig. 6(d) we plot the performance of the three
algorithms for a single test day (24 hours — midnight to midnight). During certain times of the day (e.g., 5 am), the ride
demand is low, and all approaches satisfy a large proportion
of requests. However, during the other times of the day, when
the demand is high, the DMFG-QL satisfies more requests
than the baselines, showing its relative superiority.

8

Conclusion

In this paper, we relaxed two strong assumptions in prior
work on using mean field methods in RL. We introduced the
DMFG framework, where agents are not assumed to have
global information and are not homogeneous. All agents learn
in a decentralized fashion, which contrasts with centralized

procedures in prior work. Theoretically, we proved that the
DMFG will have a suitable solution concept, DMFE. Also,
we proved that a Q-learning based algorithm will find the
DMFE. Further, we provided a principled method to address
the ‘chicken-and-egg’ problem in MFRL, and demonstrated
performances in a variety of environments (including RMP).
For future work, we would like to extend our theoretical
analysis to the function approximation setting and analyze the
convergence of policy gradient algorithms. Empirically, we
could consider other real-world applications like autonomous
driving and problems on demand and supply optimization.

Acknowledgements
Resources used in preparing this research at the University
of Waterloo were provided by the province of Ontario and
the government of Canada through CIFAR, NRC, NSERC
and companies sponsoring the Vector Institute. Part of this
work has taken place in the Intelligent Robot Learning (IRL)
Lab at the University of Alberta, which is supported in part
by research grants from the Alberta Machine Intelligence Institute (Amii); a Canada CIFAR AI Chair, CIFAR; Compute
Canada; and NSERC.

References
Adlakha, S.; Johari, R.; and Weintraub, G. Y. 2015. Equilibria
of dynamic games with many players: Existence, approximation, and market structure. Journal of Economic Theory, 156:
269–316.
Alonso-Mora, J.; Samaranayake, S.; Wallar, A.; Frazzoli, E.;
and Rus, D. 2017. On-demand high-capacity ride-sharing via
dynamic trip-vehicle assignment. Proceedings of National
Academy of Science USA, 114(3): 462–467.
Anahtarci, B.; Kariksiz, C. D.; and Saldi, N. 2019. Value
Iteration Algorithm for Mean-field Games. arXiv preprint
arXiv:1909.01758.
Bartle, R. G. 2014. The elements of integration and Lebesgue
measure. John Wiley & Sons.
Busoniu, L.; Babuska, R.; and De Schutter, B. 2006. Multiagent reinforcement learning: A survey. In 2006 9th International Conference on Control, Automation, Robotics and
Vision, 1–6. IEEE.
Cardaliaguet, P.; and Hadikhanloo, S. 2017. Learning in mean
field games: the fictitious play. ESAIM: Control, Optimisation
and Calculus of Variations, 23(2): 569–591.
Carmona, R.; Laurière, M.; and Tan, Z. 2019a. Linearquadratic mean-field reinforcement learning: convergence of
policy gradient methods. arXiv preprint arXiv:1910.04295.
Carmona, R.; Laurière, M.; and Tan, Z. 2019b. Modelfree mean-field reinforcement learning: mean-field MDP and
mean-field Q-learning. arXiv preprint arXiv:1910.12802.
Elie, R.; Pérolat, J.; Laurière, M.; Geist, M.; and Pietquin, O.
2020. On the Convergence of Model Free Learning in Mean
Field Games. In AAAI, 7143–7150.
Elliott, R.; Li, X.; and Ni, Y.-H. 2013. Discrete time meanfield stochastic linear-quadratic optimal control problems.
Automatica, 49(11): 3222–3233.

Fu, Z.; Yang, Z.; Chen, Y.; and Wang, Z. 2019. Actor-Critic
Provably Finds Nash Equilibria of Linear-Quadratic MeanField Games. In ICLR.
Gomes, D. A.; Mohr, J.; and Souza, R. R. 2010. Discrete
time, finite state space mean field games. Journal de mathématiques pures et appliquées, 93(3): 308–328.
Guo, X.; Hu, A.; Xu, R.; and Zhang, J. 2019. Learning
mean-field games. In NeurIPS, 4966–4976.
Gupta, J. K.; Egorov, M.; and Kochenderfer, M. 2017. Cooperative multi-agent control using deep reinforcement learning.
In AAMAS, 66–83. Springer.
Hadikhanloo, S.; and Silva, F. J. 2019. Finite mean field
games: fictitious play and convergence to a first order continuous mean field game. Journal de Mathématiques Pures et
Appliquées, 132: 369–397.
Hajek, B.; and Raginsky, M. 2019. Statistical learning theory.
Lecture Notes, 387.
He, H.; and Boyd-Graber, J. L. 2016. Opponent Modeling in
Deep Reinforcement Learning. In ICML, volume 48, 1804–
1813. JMLR.org.
Hernandez-Leal, P.; Kartal, B.; and Taylor, M. E. 2019. A
survey and critique of multiagent deep reinforcement learning. Journal of Autonomous Agents and Multi-Agent Systems
(JAAMAS), 33(6): 750–797.
Hernández-Lerma, O.; and Lasserre, J. B. 2012. Discretetime Markov control processes: basic optimality criteria,
volume 30. Springer Science & Business Media.
Hinderer, K. 1970. Decision models. In Foundations of
Non-stationary Dynamic Programming with Discrete Time
Parameter, 78–83. Springer.
Hu, J.; and Wellman, M. P. 2003. Nash Q-learning for generalsum stochastic games. Journal of machine learning research,
4(Nov): 1039–1069.
Huang, M. 2010. Large-population LQG games involving a
major player: the Nash certainty equivalence principle. SIAM
Journal on Control and Optimization, 48(5): 3318–3353.
Huang, M.; Caines, P. E.; and Malhamé, R. P. 2003. Individual and mass behaviour in large population stochastic
wireless power control problems: centralized and Nash equilibrium solutions. In 42nd IEEE International Conference
on Decision and Control. IEEE.
Huang, M.; Malhamé, R. P.; Caines, P. E.; et al. 2006. Large
population stochastic dynamic games: closed-loop McKeanVlasov systems and the Nash certainty equivalence principle.
Communications in Information & Systems, 6(3): 221–252.
Kakutani, S. 1941. A generalization of Brouwer’s fixed point
theorem. Duke mathematical journal, 8(3): 457–459.
Kizilkale, A. C.; and Caines, P. E. 2013. Mean Field Stochastic Adaptive Control. IEEE Trans. Autom. Control., 58(4):
905–920.
Konda, V. R.; and Tsitsiklis, J. N. 1999. Actor-Critic Algorithms. In NeurIPS. The MIT Press.
Lasry, J.-M.; and Lions, P.-L. 2007. Mean field games.
Japanese journal of mathematics, 2(1): 229–260.

Lattimore, T.; and Szepesvári, C. 2020. Bandit algorithms.
Cambridge University Press.
Li, M.; Qin, Z.; Jiao, Y.; Yang, Y.; Wang, J.; Wang, C.; Wu,
G.; and Ye, J. 2019. Efficient Ridesharing Order Dispatching
with Mean Field Multi-Agent Reinforcement Learning. In
The World Wide Web Conference, WWW 2019. ACM.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;
Tassa, Y.; Silver, D.; and Wierstra, D. 2016. Continuous
control with deep reinforcement learning. In ICLR.
Lowalekar, M.; Varakantham, P.; and Jaillet, P. 2019. ZAC:
A Zone Path Construction Approach for Effective Real-Time
Ridesharing. In ICAPS, 528–538. AAAI Press.
Mguni, D.; Jennings, J.; and de Cote, E. M. 2018. Decentralised Learning in Systems With Many, Many Strategic
Agents. In AAAI, 4686–4693. AAAI Press.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,
J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,
A. K.; Ostrovski, G.; et al. 2015. Human-level control through
deep reinforcement learning. Nature, 518(7540): 529.
Nash, J. F. 1951. Non-Cooperative Games. Princeton University Press.
Neumann, J. v. 1928. Zur theorie der gesellschaftsspiele.
Mathematische annalen, 100(1): 295–320.
NYYellowTaxi. 2016. New York yellow taxi dataset. http://
www.nyc.gov/html/tlc/html/about/triprecorddata.shtml. [Online; accessed 19-June-2021].
Perrin, S.; Laurière, M.; Pérolat, J.; Geist, M.; Élie, R.; and
Pietquin, O. 2021. Mean Field Games Flock! The Reinforcement Learning Way. In IJCAI, 356–362.
Puterman, M. L. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in
Probability and Statistics. Wiley. ISBN 978-0-47161977-2.
Ropke, S.; and Cordeau, J. 2009. Branch and Cut and Price
for the Pickup and Delivery Problem with Time Windows.
Transportation Science, 43(3): 267–286.
Saldi, N.; Basar, T.; and Raginsky, M. 2018. Discretetime risk-sensitive mean-field games.
arXiv preprint
arXiv:1808.03929.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
In arXiv preprint arXiv:1707.06347.
Shah, S.; Lowalekar, M.; and Varakantham, P. 2020. Neural
Approximate Dynamic Programming for On-Demand RidePooling. In AAAI, 507–515. AAAI Press.
Shoham, Y.; Powers, R.; and Grenager, T. 2003. Multi-agent
reinforcement learning: a critical survey. Technical report,
Technical report, Stanford University.
Shukla, S.; Balasubramanian, S.; and Pavlović, M. 2016.
A generalized Banach fixed point theorem. Bulletin of
the Malaysian Mathematical Sciences Society, 39(4): 1529–
1539.
Stanley, H. E. 1971. Phase transitions and critical phenomena.
Clarendon.
Subramanian, J.; and Mahajan, A. 2019. Reinforcement
Learning in Stationary Mean-field Games. In AAMAS, 251–
259. IFAAMAS.

Subramanian, S. G. 2021. Decentralized Mean Field Games.
https://github.com/Sriram94/DMFG.
Subramanian, S. G.; Poupart, P.; Taylor, M. E.; and Hegde,
N. 2020. Multi Type Mean Field Reinforcement Learning.
In AAMAS. IFAAMAS.
Subramanian, S. G.; Taylor, M. E.; Crowley, M.; and Poupart,
P. 2021. Partially Observable Mean Field Reinforcement
Learning. In AAMAS. ACM.
Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y.
1999. Policy Gradient Methods for Reinforcement Learning
with Function Approximation. In NIPS, 1057–1063. The
MIT Press.
Tan, M. 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML.
Tanaka, K.; Hori, T.; and Wang, H. O. 2003. A multiple
Lyapunov function approach to stabilization of fuzzy control
systems. IEEE Transactions on fuzzy systems, 11(4): 582–
589.
Terry, J. K.; Black, B.; Jayakumar, M.; Hari, A.; Santos,
L.; Dieffendahl, C.; Williams, N. L.; Lokesh, Y.; Sullivan,
R.; Horsch, C.; and Ravi, P. 2020. PettingZoo: Gym for
Multi-Agent Reinforcement Learning. In arXiv preprint
arXiv:2009.14471.
Wang, Z.; Qin, Z. T.; Tang, X.; Ye, J.; and Zhu, H. 2018.
Deep Reinforcement Learning with Knowledge Transfer for
Online Rides Order Dispatching. In ICDM, 617–626. IEEE
Computer Society.
Watkins, C. J.; and Dayan, P. 1992. Q-learning. Machine
Learning, 8(3-4): 279–292.
Wi˛ecek, P. 2020. Discrete-time ergodic mean-field games
with average reward on compact spaces. Dynamic Games
and Applications, 10(1): 222–256.
Wi˛ecek, P.; and Altman, E. 2015. Stationary anonymous
sequential games with undiscounted rewards. Journal of
optimization theory and applications, 166(2): 686–710.
Xu, Z.; Li, Z.; Guan, Q.; Zhang, D.; Li, Q.; Nan, J.; Liu, C.;
Bian, W.; and Ye, J. 2018. Large-Scale Order Dispatch in OnDemand Ride-Hailing Platforms: A Learning and Planning
Approach. In KDD. ACM.
Yang, J.; Ye, X.; Trivedi, R.; Xu, H.; and Zha, H. 2017. Deep
Mean Field Games for Learning Optimal Behavior Policy of
Large Populations. CoRR, abs/1711.03156.
Yang, Y.; Luo, R.; Li, M.; Zhou, M.; Zhang, W.; and Wang, J.
2018. Mean Field Multi-Agent Reinforcement Learning. In
ICML. PMLR.
Yang, Y.; and Wang, J. 2020. An Overview of Multi-Agent
Reinforcement Learning from Game Theoretical Perspective.
CoRR, abs/2011.00583.
Yin, H.; Mehta, P. G.; Meyn, S. P.; and Shanbhag, U. V.
2014. Learning in Mean-Field Games. IEEE Trans. Autom.
Control., 59(3): 629–644.
Zheng, L.; Yang, J.; Cai, H.; Zhou, M.; Zhang, W.; Wang,
J.; and Yu, Y. 2018. MAgent: A many-agent reinforcement
learning platform for artificial collective intelligence. In
AAAI.

A

Proof of Theorem 1

For the proof, we will follow the idea of using discounted
occupancy measures, as discussed in Puterman (1994). Let us
consider an agent j ∈ {1, . . . , N }. Let µ ∈ M and π j ∈ Πj
be arbitrary. Then from the Ionescu-Tulcea theorem (see
Theorem 3.3 in Lattimore and Szepesvári (2020)), a mean
field distribution µ on S and a policy π j defines a unique
j
probability measure P π on S × Aj .
We will begin with a definition of a discounted occupancy
measure for some mean field µ. Consider an arbitrary policy
j
π j ∈ Πj . Then, the discounted occupancy measure νµπ ∈
F(S × A) induced by this policy on the set S × Aj with
mean field µ is given by the equation
j

ν π (sj , aj |µ) =

∞
X

β t P π (sjt = sj , ajt = aj |µt = µ).

(16)
The discounted occupancy measure assigns a measure to
the given state-action pair, which is the infinite discounted
sum of the process that has a mean field of µ and follows the
policy π j , with the agent j hitting at various times the state
sj and taking the action aj at that state.
The discounted measure has a property that the value function can be represented as a product of the immediate reward
and the discounted occupancy measure. To prove this, consider (from Eq. 5),
j P∞
Jµj (π j ) = Eπ [ t=0 β t rj (sjt , ajt , µt )]

R
j

j

j

(s ∈S,a ∈A )

P∞

j

j
t π
j j
t=0 β E [r (st , at , µt )

I(sj = sjt , aj = ajt , µt = µ)]
1

=

R
(sj ∈S,aj ∈Aj )

P∞

t

πj

t=0 β E

j

j

j

[r (s , a , µ)

I((sjt = sj , ajt = aj , µt = µ)]

R
2

=

rj (sj , aj , µ)
(sj ∈S,aj ∈Aj )
P∞ t πj
j
j
j
j
t=0 β E [I(st = s , at = a , µt = µ)]

=

R

=

R

j

j

ν π̂ (sj , aj |µ) = ν π (sj , aj |µ)

(18)

Proof. For a mean field µ, we define
R an joccupancy measure
πj j
over the state space ν̃ (s |µ) , aj ν π (sj , aj |µ).
Now consider a Markov policy π̂ j as follows,
πj

j

j

,a |µ)
π̂ j (aj |sj , µ) = ν ν̃ π(s
,
j
(sj |µ)

j

if ν̃ π (sj , µ) 6= 0
(19)

and

j

t=0

=

such that

rj (sj , aj , µ)
(sj ∈S,aj ∈Aj )
P∞ t πj j
j
j
j
t=0 β P (st = s , at = a |µt = µ)

j

π̂ j (aj |sj , µ) = π0j (aj ),

if ν̃ π (sj , µ) = 0

Here, π0j (aj ) ∈ ΠjM is an arbitrary policy.
Consider,
j

ν̃ π̂ (sj |µ) =
=

R
aj

R

j

aj

ν π̂ (sj , aj |µ)

P∞

j
t π̂ j j
j
j
t=0 β P (st = s , at = a |µt = µ)

1 P∞
t π̂ j j
j
t=0 β P (st = s |µt = µ)

=

= ν0 (sj ) +

P∞

t π̂ j j
j
t=1 β P (st = s |µt = µ)

= ν0 (sj ) + β

P∞

t−1
t=1 β

R

R
s0j ∈S

a0j ∈Aj

j

P π̂ [sjt−1 = s0j , ajt−1 = a0j |µt−1 = µ0 ]p(sj |s0j , a0j , µ0 )
= ν0 (sj ) + β

P∞

t−1
t=1 β

R
s0j ∈S

j

P π̂ [st−1 = s0 |µt−1 = µ0 ]

R
a0j ∈Aj

π̂ j (a0j |s0j , µ0 )p(sj |s0j , a0j , µ0 )

= ν0 (sj )

j

(sj ∈S,aj ∈Aj )

rj (sj , aj , µ)ν π (sj , aj |µ)

(17)
Here the notation I(x) is an indicator function, which
equals 1 if the x is true and 0 if x is false. The sum of all
indicators for all the state-action pairs at a given t equals 1.
The step (1) uses this property of the indicator function, to
drop the time indices of the reward function. The step (2)
is from Lebesgue’s dominated convergence theorem (Bartle
2014).
Now we state a lemma needed for our proof.
Lemma 1. For an agent j ∈ {1, . . . , N } and policy π j ,
given mean field µ, there exists a Markov policy π̂ j ∈ ΠjM

+β

R
s0j ∈S

R
a0j ∈Aj

P∞

t=1 β

t−1

j

P π̂ [sjt−1 = s0j |µt−1 = µ0 ]

π̂ j (a0j |s0j , µ0 )p(sj |s0j , a0j , µt−1 = µ0 )

2

= ν0 (sj ) + β

R

j

s0j ∈S

R
a0j ∈Aj

ν̃ π̂ (s0j |µ0 )

π̂ j (a0j |s0j , µ0 )p(sj |s0j , a0j , µ0 )

(20)
Here ν0 (s ) is the initial distribution of the state of the
agent j. To obtain expression (2), see that this follows from
expression (1).
j

j

From Eq. 22 and Eq. 23 we find that ν π̂ (sj , aj |µ) =
πj j
ν (s , aj |µ), due to the property of the Markov policy π̂ j .

Now consider,
P∞
j
j
ν̃ π (sj |µ) = t=0 β t P π (sjt = sj |µt = µ)
= ν0 (sj ) +

P∞

t πj j
j
t=1 β P (st = s |µt = µ)

= ν0 (sj ) + β

P∞

t=1 β

t−1

R

Theorem 1. For any mean field, µ ∈ M, and an agent
j ∈ {1, . . . , N }, we have,

R
s0j ∈S

a0j ∈Aj

sup Jµj (π j ) = sup Jµj (π j ).

π j ∈Πj

j

P π [sjt−1 = s0j , ajt−1 = a0j |µt−1 = µ0 ]p(sj |s0j , a0j , µ0 )
= ν0 (sj ) + β

R

R

s0j ∈S

a0j ∈Aj

P∞

t=1 β

t−1

j

P π [sjt−1 = s0j , ajt−1 = a0j |µt−1 = µ0 ]p(sj |s0j , a0j , µ0 )
1

= ν0 (sj )
+β
2

R

R

j

s0j ∈S

a0j ∈Aj

R

R

j

= ν0 (s ) + β

ν π (s0j , a0j |µ0 )p(sj |s0j , a0j , µ0 )

a0j ∈Aj

0j

0

1

0j

0

j

0j

0j

0

j

ν̃ π (s0j |µ0 )
s0j ∈S

π̂ j (a0j |s0j , µ0 )p(sj |s0j , a0j , µ0 )
(21)
The (1) is from Eq. 16 and (2) is from Eq. 19. From both
Eq. 21 and Eq. 20, we find that both the discounted state
occupation frequency can be recursively expressed as a term
depending on the state occupation frequency at the previous
time step and a few other terms that are the same for both the
Eq. 20 and Eq. 21. Since the initial state distribution is the
j
same for both policies, we can conclude that ν̃ π̂ (sj |µ) =
j
ν̃ π (sj |µ).
Now, consider
P∞
j
j
ν π̂ (sj , aj |µ) = t=0 β t P π̂ (sjt = sj , ajt = aj |µt = µ)
P∞

t π̂ j j
j j
j
t=0 β P (at = a |st = s , µt = µ)
j

P π̂ (sjt = sj |µt = µ)
P∞

t j j
j j
j
t=0 β π̂ (at = a |st = s , µt = µ)
j

P π̂ (sjt = sj |µt = µ)
(22)
Also, from Eq. 19, we have
j

j

ν π (sj , aj |µ) = π̂ j (aj |sj , µ) × ν̃ π (sj |µ)
j

= π̂ j (aj |sj , µ) × ν̃ π̂ (sj |µ)
(23)
= π̂ j (aj |sj , µ)
P∞

P∞

(sj ∈S,aj ∈Aj )

M

a0j ∈Aj

j

j
t π̂
j
t=0 β P (st = s |µt = µ)

t j j j
π̂ j j
j
t=0 β π̂ (a |s , µ)P (st = s |µt = µ)

rj (sj , aj , µ)ν π (sj , aj |µ)
j

≤ supπ̂j ∈Πj

πj

R

=

R

R

0j

j

(sj ∈S,aj ∈Aj )

π̂ (a |s , µ )ν̃ (s |µ )p(s |s , a , µ )
= ν0 (sj ) + β

=

R

=

j

=

Proof. Now, we aim to show that supπj ∈Πj Jµj (π j ) =
supπj ∈Πj Jµj (π j ). Since ΠjM ⊂ Πj , we know that
M
supπj ∈Πj Jµj (π j ) ≤ supπj ∈Πj Jµj (π j ). To show the equalM
ity we aim to prove supπj ∈Πj Jµj (π j ) ≥ supπj ∈Πj Jµj (π j )
M
as well.
To show this result we use Lemma 1. Consider a policy π j
j
j
and a Markov policy π̂ j , such that νµπ̂ = νµπ . Now using the
Eq. 17, we have
Jµj (π j ) =

s0j ∈S

π j ∈ΠjM

rj (sj , aj , µ)ν π̂ (sj , aj |µ)

R
(sj ∈S,aj ∈Aj )

j

rj (sjt , ajt , µt )ν π̂ (sj , aj |µ)

= supπ̂j ∈Πj Jµj (π̂ j )
M

(24)
Here (1) is from Lemma 1. Now, from Eq. 24, and taking
supermum on both sides, we get that supπj ∈Πj Jµj (π j ) ≤
supπj ∈Πj Jµj (π j ), which concludes our proof.
M

B

Proof of Theorem 2

The proof of this theorem follows the Theorem 1 in Saldi,
Basar, and Raginsky (2018). We aim to extend it to the decentralized setting as mentioned in Section 5.
Before starting the proof, we will state some assumptions,
Assumption 2. The reward function for all agents j ∈
{1, . . . , N } is bounded and continuous.
Let us define a function w : S →
− [1, ∞), such that there
exists an increasing sequence of compact subsets {Kn }n≥1
of S where
lim
inf w(s) = ∞.
(25)
n→
− ∞ s∈S\Kn
That is, the value of w(s) gets close to infinity in this limit.
Here, the w can be regarded as a continuous moment function
(Saldi, Basar, and Raginsky 2018).
Assumption 3. There exists a positive α such that the following holds (for all agents j ∈ {1, . . . , N }),

Z
w(sjt+1 )p(sjt+1 |sjt , ajt , µt ) ≤ αw(sjt )

sup
(aj ,µj )∈Aj ×P (S)

S

(26)
Also, consider two different moment functions w and v,
then α satisfies,
sup(aj ,µj )∈Aj ×P (S)

R

R

S

Assumption 5. There exist a γ ≥ 1 and a positive scalar R
such that for all t ≥ 0, we define Mt , γ t R, then
sup
(aj ,µ)∈Aj ×P (S)

rj (sjt , ajt , µt ) ≤ Mt w(sjt )

(30)

(βα)k−t Mk

(35)

Here the constants α and Mk are obtained from Assumption 3 and Assumption 5 respectively.
It follows that the following equation holds,
Lt = Mt + (βα)Lt+1

(36)

Let Cw (S) denote the Banach space of all real valued
bounded measurable functions on S with a finite w-norm.
Let us define a set
t
Cw
(S) , {u ∈ Cw (S) : ||u||w ≤ Lt }

(37)

t

+β

R

i
j
j
j
j j
u
(s
)p(s
|s
,
a
,
µ
)
t
t
t
t+1
t+1
S
(38)

where uj : S →
− <
We will first prove the following lemma.
Lemma 2. For all t ≥ 0 and a given mean field µ, the
t+1
t
operator Ttµ maps Cw
(S) into Cw
(S). Also, this operator
will satisfy
||Ttµ u − Ttµ x||w ≤ αβ||u − x||w

(39)

for any u, x ∈ Cw (S).
t+1
Proof. Let u ∈ Cw
(S). We have the following relation,

||Ttµ u||w ≤ sup(sj ,aj )∈S×Aj
t

r j (sjt ,ajt ,µt )+β

≤ sup(sj ,aj )∈S×Aj
t

t

R

u(sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
S
w(sjt )

Mt w(sjt )+βαLt+1 w(sjt )
w(sjt )

S

= Mt + βαLt+1

Further, we define another set
Tw (S) , {µ ∈ P(S) : ||µ||w < ∞}
Also, for t ≥ 0, we define
Z
n
o
Twt (S) , µ ∈ Tw (S) :
w(s)µ(s) ≤ αt M
S

∞
X
k=t

t

Let Bw (S) be the Banach space of all real valued measurable functions on S with a finite w-norm.
Also, for any signed measure µ in the space of S, the
w-norm can be defined as
Z
||µ||w ,
sup
g(s)µ(s)
(31)
g∈Bw (S):||g||w ≤1

Lt ,

(29)

Now, we will state a few definitions required for our proof.
For any function g : S →
− <, we define the w-norm as:
|g(s)|
||g||w , sup
s∈S w(s)

For each t ≥ 0, we will define

For an agent j ∈ {1, . . . , N }, let us consider an operator
h
Ttµ uj (st ) = maxaj ∈Aj r(sjt , ajt , µt )

w(sjt+1 )p(sjt+1 |sjt , ajt , µt )
S

v(sjt+1 )p(sjt+1 |sjt , ajt , µt )) ≤ α|w(sjt ) − v(sjt )|
S
(27)
Assumption 4. The initial mean field µ0 satisfies
Z
w(sj )µ0 (sj ) , M < ∞
(28)
−

where the constant M is obtained from Assumption 4. Like
our notation P(S), we are using the notation P(S × Aj ) to
denote the probability of a state-action pair. Let us consider an
element ν ∈ P(S ×Aj ), and use the notation ν1 , ν(·×Aj )
to denote the state marginal of ν. Now we define a new set,
n
o
Twt (S × Aj ) , ν ∈ P(S × Aj ) : ν1 ∈ Twt (S) . (34)

(32)

(33)

= Lt
(40)
The second last step is from Assumption 5 and Assumption 3. Also, we use the bound on the w-norm from Eq. 37.
The last step is from Eq. 36. This proves the first statement.

To prove the second statement, without loss of generality,
let us assume that u0 ≥ x0 . Now consider,
||Ttµ u − Ttµ x||w =
sup(sj )∈S

u(sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
S
j
w(st )

max j r j (sjt ,ajt ,µt )+β
at

t

−r j (sjt ,ajt ,µt )−β

max j β

R
S

at

= sup(sj )∈S

R

j,µ
Let J j,µ
, (J∗,t
)t≥0 denote the optimal value function
∗
for an agent j ∈ {1, . . . , N }. This function is guaranteed to
be continuous by Assumption 2.

Lemma 3. For any µ, the optimal point J j,µ
∗ , for an agent
j ∈ {1, . . . , N }, belongs to the Banach space C.
Proof. Let π be a Markov policy. At any time t ≥ 0, we have

R

j,µ
J∗,t
(s)

x(sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
S
w(sjt )

u(sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt )

t

−

R



x(sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
S
w(sjt )

max j β
at

= sup(sj )∈S

R
S

≤ sup(sj )∈S

P∞

k−t

Eπ (rj (sjk , ajk , µk )|sjt = sj )

≤

P∞

k−t

Mk Eπ (v(sk )|sjt = sj )

≤

P∞

k−t

Mk αk−t v(sj )

k=t β
k=t β

k=t β

(46)

= Lt v(sj )

p(sjt+1 |sjt ,ajt ,µt )(u(sjt+1 )−x(sjt+1 ))
w(sjt )

t

=

The second step is from Assumption 5. The third step is
from Assumption 3. Hence, the function J j,µ
∗,t belongs to the
t
set Cw
(S) from the Eq. 37.

αβ (u(sjt )−x(sjt ))

Let us define another set D as

w(sjt )

t

t
j
D , Π∞
t=0 Tw (S × A )

= αβ||u − x||w
(41)
We apply Assumption 3 and the last step is from Eq. 30.
This proves the second part of the lemma as well.
t
Let us define a new set C from Cw
(S) as follows:
t
C , Π∞
t=0 Cw (S)

Also, let us assign the following metric to C:
∞
X
ρ(u, v) ,
σ −t ||ut − vt ||w

(42)

(43)

t=0

where σ > 0 and the following assumption holds.
Assumption 6. The variables σ, α, and β satisfy ασβ < 1.
This assumption guarantees that the metric C is complete
w.r.t ρ, and ρ(u, v) < ∞ for all u, v ∈ C.
Using the operators {Ttµ }t≥0 , let us define a new operator
µ
T :C→
− C as follows (for all t ≥ 0).
(T µ u)t = Ttµ ut+1

π j ∈Πj

Before proving further results, we restate a result proved
previously in non-homogeneous stochastic processes (Hinderer 1970). We will adapt the result to pertain to an agent
j ∈ {1, . . . , N }. For any E-valued random element x, we
use the notation L(x) to denote the distribution of x (i.e.
L(x) ∈ P (E)).
Consider a variable ν ∈ D. From the operator T µ , we will
define another operator T ν , where the relation is such that
µt = νt,1 . Recall that the subscript here refers to the state
marginal of ν (refer Eq. 34). From the result of T µ , we know
that the operator T ν is well-defined and has a unique fixed
point. Now, we can state the following result.
Lemma 4. For an agent j ∈ {1, . . . , N }, for any ν ∈ D, the
collection of value functions J j,ν
∗ is the unique fixed point
of the operator T ν . Furthermore, π j ∈ M is optimal if and
only if
j

νtπ ({(sjt , ajt ) : rj (sjt , ajt , νt,1 )+

(44)

From Lemma 2 we know that T µ is a well-defined operator
that maps C to itself. Also, from Eq. 39, T µ is a contraction
operator on C with constant of contraction αβσ < 1 (from
Assumption 6). Now, T µ will have a unique fixed point by
the Banach fixed point theorem in C.
Now, we move to proving another lemma. Consider a mean
field µ, the optimal value function for an agent j can be given
by
∞
hX
i
j
j,µ j
J∗,t
(s ) = sup Eπ
β t rj (sjt , ajt , µt )|sj0 = sj .
t=0

(45)

(47)

β

R
S

j,ν
j,ν
J∗,t+1
(sjt+1 )p(sjt+1 |sjt , ajt , νt,1 ) = Ttν J∗,t+1
(sjt )})

=1
(48)
j

where νtπ = L(sjt , ajt ).
Proof. See Hinderer (1970).
Now, let us define a set-valued mapping τ : D →
−
∞
2|P(S×A) | , for an agent j as follows:
τ (ν) = C(ν) ∩ B(ν)

(49)

where
0
C(ν) , {ν 0 ∈ P(S × Aj ) : ν0,1
= µ0 and
0
νt+1,1
(·) =

R

(50)
p(·|sj , aj , νt,1 )νt (sj , aj )}
S×Aj

and
n
B(ν) , ν 0 ∈ P(S × Aj ) : ∀t ≥ 0,

νt0 ( (sjt , ajt ) : rj (sjt , ajt , νt,1 )+

(51)

R

o
j,ν
β S J∗,t+1
(sjt+1 )p(sjt+1 |sjt , ajt , νt,1 ) = 1
Now in the next result we show that, the image of D under
τ is contained in 2D .
Proposition 1. For any ν ∈ D, we have τ (ν) ⊂ D
Proof. Fix a ν ∈ D. To prove the result, it is sufficient to
prove that C(ν) ⊂ D. Let ν 0 ∈ C(ν). We aim to prove by
0
induction that νt,1
∈ Pvt (S) for all t ≥ 0. The claim trivially
0
holds for t = 0 as ν0,1
= µ0 . Assume that the claim holds
for t and consider t + 1. We have

R
S

0
w(sjt )νt+1,1
(sjt+1 )

=

R

≤

R

R
S×A

S

w(sjt+1 )p(sjt+1 |sjt , ajt , νt,1 )νt (sjt , ajt )

(52)

αw(sjt )νt,1 (sjt )
S

≤ αt+1 M
The third step is from Assumption 3 and the last step is
from the fact that νt,1 ∈ Pvt (S). Hence, we can conclude that
0
νt+1,1
∈ Pvt+1 (S).
Now we can say that ν ∈ D is a fixed point of τ if
ν ∈ τ (ν). The following lemma connects the mean field
equilibrium and the fixed points of τ .
Lemma 5. Suppose the set valued mapping τ has a fixed
point ν j = (νtj )t≥0 for an agent j ∈ {1, . . . , N }. Consider
a Markov policy for the agent j as π j = (πtj )t≥0 , which is
j
obtained by factoring as νtj (sjt , ajt ) = νt,1
(sjt )πtj (ajt |sjt ), and
j
let ν j1 = (νt,1
)t≥0 . Then the pair (π j , ν j1 ) is a decentralized
mean field equilibrium.
Proof. If ν j ∈ τ (ν j ), then the corresponding Markov policy
π j satisfies the Eq. 48 for ν j . Thus, by Lemma 4, π j ∈
Φ(ν j1 ). Further, since ν j ∈ C(ν j ), we have Ψ(π j ) = ν j1 ,
which completes the proof.
From the Lemma 5, it can be seen that the set valued mapping (operator) τ having a fixed point is sufficient to guarantee the existence of the decentralized mean field equilibrium.
Like several results in centralized multiagent systems (Nash
1951) and centralized mean field games (Lasry and Lions
2007; Huang et al. 2006; Saldi, Basar, and Raginsky 2018),
we will use the Kakutani’s fixed point theorem (Kakutani

1941), to guarantee the existence of a fixed point for the operator τ . This theorem requires the set on which the set valued
mapping τ operates to be a non-empty, compact and convex
subset of some Euclidean space. Further, the operator τ (ν j )
is required to be non-empty and convex for all ν j . Finally,
the operator τ should have a closed graph. Given these three
conditions, we can conclude that τ has a fixed point using
the Kakutani’s fixed point theorem.
For the first condition, we need to show that the set on
which ν j resides is non-empty, compact and convex, i.e. we
need to show that D is non-empty, compact and convex. First,
note that the function w can be expressed as a continuous
moment function and hence the corresponding set Tvt (S) is
guaranteed to be compact (Hernández-Lerma and Lasserre
2012). As a consequence, the set Tvt (S × Aj ) is tight, since
the action space Aj is compact. Also, since the set Tvt (S ×
Aj ) is closed, it is compact. Therefore, the set D is also
compact. From Assumption 3 and Assumption 5 we can
show that a line segment between any two points in D lies
in D and hence the set D is convex. These assumptions also
guarantee that the set D is non-empty.
For the third condition, we need to show that τ (ν j ) is
non-empty and convex for any ν j ∈ D. Now, from Lemma 4
we know that B(ν j ) is non-empty and hence τ (ν j ) is nonempty. Also, we can show that each of the sets C(ν j ) and
B(ν j ) is convex (see Saldi, Basar, and Raginsky (2018)) and
hence, their intersection is convex. This makes the set τ (ν j )
convex.
Lemma 6. Using Assumptions 2—6, the graph of τ , i.e. the
set
Gr(τ ) , {(ν, E) ∈ D × D : E ∈ τ (ν)}
(53)
is closed.
Proof. See Proposition 3.9 in Saldi, Basar, and Raginsky
(2018) for complete proof.
Now, we are ready to give the final result.
Lemma 7. Using Assumptions 2—6, for an agent j ∈
{1, . . . , N }, there exists a fixed point ν j of the set valued
mapping τ : D →
− 2D . Then, the pair (π j , ν j1 ) is a decentralized mean field equilibrium, where π j is the policy of agent
j and ν j1 is its mean field estimate constructed according to
Lemma 5.
Proof. Using the Lemma 6 and our previous results, we have
proved that D is non-empty, compact and convex. Further, the
set valued mapping τ has a closed graph, is non-empty and
convex. Hence, by Kakutani’s fixed point theorem (Kakutani
1941), τ has a fixed point. Thus, from Lemma 5 this fixed
point is the decentralized mean field equilibrium.
Theorem 2. An agent j ∈ {1, . . . , N } in the DMFG admits
a decentralized mean field equilibrium (π∗j , µj∗ ) ∈ Πj × M.
Proof. Our result follows from Lemma 7.

C

Proof of Theorem 3

In this and the subsequent theorems, we follow the theoretical
results and proofs in the work by Anahtarci, Kariksiz, and
Saldi (2019), and extend them to the decentralized setting.
First, we will start with some definition for the norms, similar to our previous proofs. Consider an agent j ∈ {1, . . . , N }.
Let w : S × Aj →
− < be a continuous weight function. For
any measurable function v : S × Aj →
− <, the w-norm of v
is defined as,
|v(sj , aj )|
.
j
j
sj ,aj w(s , a )

||v||w , sup

(54)

Also, for any measurable function u : S →
− <, the wmax norm is defined as
||v||wmax , sup
sj

u(sj )
.
wmax (sj )

(55)

Now we will state a set of assumptions needed for our
results.
Assumption 7. For all agents j ∈ {1, . . . , N }, the reward
function is continuous, and it satisfies the following Lipschitz
bounds (for some Lipschitz constants L1 and L2 ):
||rj (·, ·, µt ) − rj (·, ·, µ̂t )||w ≤ L1 W1 (µt , µ̂t ),

∀µt , µ̂t
(56)
where W1 is the Wasserstein distance of order 1. Also,
sup(aj ,µt )∈Aj ×P (S) |rj (sjt , ajt , µt ) − r(ŝjt , ajt , µt )|
t

≤ L2 dX (sjt , ŝjt ),

∀sjt , ŝjt

supsj ∈S W1 (p(·|sjt , ajt , µt ), p(·|sjt , âjt , µ̂t ))
∀µt , µ̂t , ∀ajt , âjt .
(58)

Also, we have,

≤ K2 (dX (sjt , ŝjt ) + ||ajt − âjt ||),

∀sjt , ŝjt , ∀ajt , âjt .
(59)
j
Assumption 9. The action space A is convex for all agents
j ∈ {1, . . . , N }.
Assumption 10. For all agents j and all time t, there exist non-negative real numbers M and α such that for each
(sjt , ajt , µ) ∈ S × Aj × P(S), we have
rj (sjt , ajt , µ) ≤ M

(60)

wmax (sjt+1 )p(sjt+1 |sjt , ajt , µ) ≤ αw(sjt , ajt )

(61)

where ||g||Lip is defined as
||g||Lip ,

|g(x) − g(y)|
dX (x, y)
(x,y)∈S×S
sup

(63)

Here g ∈ C(S). The finiteness of ||g||Lip guarantees that
g is Lipschitz continuous with constant ||g||Lip .
Also, let us define B(S, K) to be the set of all real valued
measurable functions in S with wmax -norm less than K. We
use Lip(S, K) to denote the set of all Lipschitz continuous
functions with ||g||Lip < ∞.
Finally, we define an operator F : S × Lip(S) × P(S) ×
Aj →
− < as
F : S × Lip(S) × P(S) × Aj 3 (sjt , vtj , µt , ajt )
→
− rj (sjt , ajt , µt ) + β

R

vtj (sjt+1 )p(sjt+1 |sjt , ajt , µt ) ∈ <
(64)
Assumption 12. Let F be the set of non-negative functions
in


L2 
M 
Lip S,
∩ B S,
(65)
1 − βK2
1 − βα
For an agent j ∈ {1, . . . , N }, for any v ∈ F, µ ∈ P(S),
and sj ∈ S, F (sj , v j , µ, ·) is ρ-strongly convex; that is
F (sj , v j , µ, ·) is differentiable and its gradient ∇F satisfies
S

+∇F (sjt , vtj , µt , âjt )T · (ajt − âjt ) + ρ2 ||ajt − âjt ||2

(66)

for some ρ > 0 and for all ajt , âjt ∈ Aj .
Assumption 13. For an agent j ∈ {1, . . . , N }, the gradient
∇F (sjt , vtj , µt , ajt ) : S × F × P(S) × Aj →
− < satisfies the
following Lipshitz bound (for some Lipschitz constant KF ):
supaj ∈Aj ||∇F (sjt , vtj , µt , ajt ) − ∇F (ŝjt , v̂tj , µ̂t , ajt )||
t

(67)
for every sjt , ŝjt ∈ S; v j , v̂tj ∈ F, µt , µ̂t ∈ P(S).
Assumption 12 and Assumption 13 present a constraint on
the change in the value function during each of the updates.
These assumptions are generally considered in literature establishing the existence of Lyapunov functions that guarantee
the presence of local and global optimal points (maxima or
minima) as the case may be (Tanaka, Hori, and Wang 2003).
We also need to assume the following for all the constants,
Assumption 14. The following relation holds
L2
k , max{βα + KρF K1 , β 1−βK
+
2

(68)

S

Assumption 11. The product of constants βα < 1.

(62)

≤ KF (dX (sjt , ŝjt ) + ||vtj − v̂tj ||wmax + W1 (µjt , µ̂jt ))

supµ∈P (S) W1 (p(·|sjt , ajt , µt ), p(·|ŝjt , âjt , µt ))

Z

Lip(S) , {g ∈ C(S) : ||g||Lip < ∞}

F (sjt , vtj , µt , ajt ) ≥ F (sjt , vtj , µt , âjt )

(57)
Assumption 8. For an agent j ∈ {1, . . . , N }, the transition
function p(·|sj , aj , µ) is weakly continuous in (sj , aj , µ) and
satisfies the following Lipschitz bounds (for some Lipschitz
constants K1 and K2 ):

≤ K1 (||ajt − âjt || + W1 (µt , µ̂t )),

Let C(S) denote the set of real-valued continuous functions on S. Let Lip(S) denote the set of all Lipshitz continuous function on S, i.e.,



KF
ρ +1



K1 + K2 + KρF } < 1

For the rest of the proof we will also apply Assumption 1.
Consider a mean field µ, the optimal value function for an
agent j is given by the Eq. 45. Using the same procedure as
the result in Lemma 4, this optimal value function can be
shown to be a unique fixed point of a Bellman operator Tµ
that is a contraction on the wmax -norm with a constant of
contraction βα (from Assumption 11). Now we can write the
following equation,
h
J∗j,µ (sjt ) = maxaj ∈Aj rj (sjt , ajt , µt )+
β

R

i
j,µ j
j
j
j
J
(s
)p(s
|s
,
a
,
µ
)
∗
t
t
t+1
t+1 t
S

From Assumption 1, Assumption 12 and Assumption 13,
we know that there exists a unique maximiser π j (sjt , Qjt , µjt )
for the function F represented as,
rj (sjt , ajt , µt )
+β

R

R

i
β S J∗j,µ (sjt+1 )p(sjt+1 |sjt , ajt , µt ) =
rj (sjt , π j (ajt |sjt , µjt ), µt )

R

+β

S

J∗j,µ (sjt+1 )p(sjt+1 |sjt , ajt , µt )

(70)
The objective is to obtain this optimal policy. Towards the
same we use the Q-functions, where the optimal Q-function
can be defined as
j
j
Qj,∗
µ (st , at )

= r (sjt , ajt , µt ) + β

R

This maximser makes the gradient of F , 0.
∇F (sjt , Qjµ,max , µt , π j (sjt , Qjt , µjt )) = 0

Theorem 3. The decentralized mean field operator H is
well-defined, i.e. this operator maps C × P(S) to itself.

Proof. We first apply Assumption 1. Now, consider the decentralized mean field operator as defined in Eq. 8. To prove
the given theorem, we know that H2 (Qj , µ) ∈ P(S). We
need to prove that H1 (Qj , µ) ∈ C. Let us consider an element (Qj , µ) ∈ C × P(S). Then we have,

supsj ,aj

H1 (Qj ,µ)(sjt ,ajt )
w(sjt ,ajt )

t

= supsj ,aj
t

The optimal value function given by J∗j,µ is the maximum
of the Q-function given by Qj,∗
µ,max . Hence, we can rewrite
Eq. 71 as follows

+β

S

Qjmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )

t

r j (sjt ,ajt ,µ)
w(sjt ,ajt )

+β supsj ,aj
t

S

Qjmax (sjt+1 ,ajt ,µt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

t

≤M

j
j
, HQj,∗
µ (st , at )

(72)
Here, the operator H is the optimality operator for the
Q-functions. Our objective is to prove that this operator is a
contraction and has a unique fixed point given by Q∗ .
First, let us define a set of all bounded Q-functions for an
agent j ∈ {1, . . . , N }:
n
M
C , Qj : S × Aj →
− [0, ∞); ||Qjmax ||w ≤ 1−βα
and
L2
||Qjmax ||Lip ≤ 1−βK
2

S

w(sjt ,ajt )

R

j
j
j
j
j
Qj,∗
µ,maxaj (st+1 , a )p(st+1 |st , at , µt )

R

t

≤ supsj ,aj
t

r j (sjt ,ajt ,µ)+β

j
j
j
j j
Qj,∗
µ (st , at ) = r (st , at , µt )

R

(75)

This shows that the maximiser for the operator H2 in Eq. 8
is unique, under the considered assumptions. Now, we are
ready to prove the required theorem.

(71)
J j,µ p(sjt+1 |sjt , ajt , µt )
S ∗

(74)

.

t

j

Qjµ,maxaj (sjt+1 , aj )p(sjt+1 |sjt , ajt , µt )

= F (sjt , Qjµ,max , µt , ajt ).

, Tµ J∗j,µ (sjt )
(69)
Let us assume that this maximization is caused by a policy π j (aj |sj , µj ), which will be the optimal policy. In other
words,
h
maxaj ∈Aj rj (sjt , ajt , µt )+

S

(73)
o

R
+β||Qjmax ||wmax supsj ,aj
t
t

S

wmax (sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

≤ M + βα||Qjmax ||w
M
≤ M + βα 1−βα
M
= 1−βα

(76)
We used the Assumption 10 and the fact that ||Qj ||wmax ≤
||Qj ||w .

D

Also, we have,
|H1 (Qj , µ)max (sjt ) − H1 (Qj , µ)max (ŝjt )|
= maxaj ∈Aj [rj (sjt , ajt , µt )
+β

R
S

Qjmax (sjt+1 , aj , µjt+1 )p(sjt+1 |sjt , ajt , µt )]

− maxaj ∈Aj [rj (ŝjt , ajt , µt )
+β

R
S

Qjmax (sjt+1 , aj , µjt+1 )p(sjt+1 |ŝjt , ajt , µt )]

Theorem 4. Let B represent the space of bounded functions
in S. Then the mapping H : C × P(S) →
− C × P(S) is a
contraction in the norm of B(S).
In this section, we will continue to use the set of assumptions and definitions we introduced in Appendix C.
Proof. Fix any (Qj , µ) and (Q̂j , µ̂) in C × P(S), let us consider,
||H1 (Qj , µ) − H1 (Q̂j , µ̂)||w
= supsj ,aj
t

≤ supaj ∈Aj

r (sjt , ajt , µt ) − rj (ŝjt , ajt , µt )

−

t

j

h rj (sj ,aj ,µt )+β
t

+β supaj ∈Aj

R

Proof of Theorem 4

R

t

S

Qjmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

Qjmax (sjt+1 , aj , µjt+1 )p(sjt+1 |sjt , ajt , µt )
S

R

−r j (sjt ,ajt ,µ̂t )−β

R
S

Qjmax (sjt+1 , aj , µjt+1 )p(sjt+1 |ŝjt , ajt , µt )
S

≤ L2 dX (s, ŝ) + βK2 ||Qjmax ||Lip dX (s, ŝ)

t

L2
≤ 1−βK
dX (s, ŝ)
2

(77)
Here we are using the Assumption 7 and Assumption 8.
This proves that H1 (Qj , µ) ∈ C and concludes our proof.

R
S

+β supsj ,aj
t

−

w(sjt ,ajt )

|r j (sjt ,ajt ,µt )−r j (sjt ,ajt ,µ̂t )|
w(sjt ,ajt )

≤ supsj ,aj
t

Q̂jmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µ̂t ) i

Qjmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

t

R
S

Q̂jmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µ̂t )
w(sjt ,ajt )

≤ L1 W1 (µ, µ̂)

R
S

+β supsj ,aj
t

−

R
S

Qjmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

t

Q̂jmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

R
S

+β supsj ,aj
t

−

R
S

Q̂jmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

t

Q̂jmax (sjt+1 ,aj ,µjt+1 )p(sjt+1 |sjt ,ajt ,µ̂t )
w(sjt ,ajt )

≤ L1 W1 (µ, µ̂) + β||Qjmax − Q̂jmax ||wmax

R
supsj ,aj
t

S

wmax (sjt+1 )p(sjt+1 |sjt ,ajt ,µt )
w(sjt ,ajt )

t

W1 (p(·|sjt ,ajt ,µt ),p(·|sjt ,ajt ,µ̂t ))
w(sjt ,ajt )
t

+β||Q̂jmax ||Lip supsj ,aj
t

L2
≤ L1 W1 (µ, µ̂) + βα||Qj − Q̂j ||w + β 1−βK
K1 W1 (µ, µ̂)
2
(78)
First we apply Assumption 7. In the last step we apply
Assumption 8 and Assumption 10.

Now consider the distance between H2 (Qj , µ) and
H2 (Q̂j , µ̂). First, we will consider the difference between the
unique maximiser π j (sj , Qj , µj ) of H1 (Qj , µ)(sj , aj ) and
the unique maximiser π j (sj , Q̂j , µ̂j ) of H1 (Q̂j , µ̂)(sj , aj )
with respect to the action (using the Assumption 1). Let us
consider the function,
F : S × C × P(S) × A
→
− rj (sjt , ajt , µt ) + β

R

j

3 (sjt , vtj , µt , ajt )

Now, consider the W1 distance between H2 (Qj , µ) and
H2 (Q̂j , µ̂).
W1 (H2 (Qj , µ), H2 (Q̂j , µ̂))

(79)
This is ρ-strongly convex by Assumption 12, hence it
satisfies the following equation (Hajek and Raginsky 2019),

−

R

R
j

S×A

S

g(sjt+1 )

S

g(sjt+1 )

R

≤ sup||g||Lip ≤1

R
j

S×A

g(sjt+1 )

S


p sjt+1 |sjt , π j (sjt , Qjt , µjt ), µt µt (sjt )
−

(81)

Also, the term ajt + bjt = π j (sjt+1 , Q̂jt , µ̂jt ) is the unique
maximiser of F (sjt+1 , Q̂jmax,t , µ̂t , ·). Now, using Assumption 12 and Eq. 80 we have,

S×Aj


p sjt+1 |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t µ̂t (sjt )

[∇F (sjt , vtj , µt , ajt +bjt )−∇F (sjt , vtj , µt , ajt )]T ·rtj ≥ ρ||bjt ||2

∇F (sjt , Qjmax,t , µt , π j (sjt , Qjt , µjt )) = 0

R


p sjt+1 |sjt , π j (sjt , Qjt , µjt ), µt µt (sjt )

v(sjt+1 )p(sjt+1 |sjt , ajt , µt ) ∈ <
S

(80)
For any ajt , bjt ∈ Aj and for any sjt ∈ S, we can use the
notation ajt = π j (sjt , Qjt , µjt ) and bj = π j (sjt+1 , Q̂jt , µ̂jt ) −
π j (sjt , Qjt , µjt ).
Now, ajt = π j (sjt , Qjt , µjt ) is the unique maximiser of the
strongly convex function F (sjt , Qjmax,t , µt , ·), we have

R

= sup||g||Lip ≤1

R

R
S×Aj

S

g(sjt+1 )


p sjt+1 |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t µt (sjt )

R

+ sup||g||Lip ≤1

−∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt )T · bjt

R
S×Aj

S

g(sjt+1 )


p sjt+1 |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t µt (sjt )

= −∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt )T · bjt
+∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt + bjt )T · bjt ≥ ρ||bjt ||2
(82)
Similarly, using the Assumption 13 we also have,
−∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt )T · bjt

−

R
j

S×A

S

g(sjt+1 )


p sjt+1 |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t µ̂t (sjt )
(1)

≤

= −∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt )T · bjt
+∇F (sjt , Qjmax,t , µt , ajt )T · bjt

R

R

sup||g||Lip ≤1
S×A
j

R
S

g(sjt+1 )


p sjt+1 |sjt , π j (sjt , Qjt , µjt ), µt µt (sjt )

≤ ||bjt ||||∇F (sjt , Qjmax,t , µt , ajt )

−

R

R
j

S×A

−∇F (sjt+1 , Q̂jmax,t , µ̂t , ajt )||

S

g(sjt+1 )p sjt+1 |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t



µt (sjt ) + (K2 + KρF )W1 (µt , µ̂t )

≤ KF ||bjt ||(dX (sjt , sjt+1 ) + ||Qjmax,t − Q̂jmax,t ||wmax

≤

+W1 (µt , µ̂t ))

R



W
p · |sjt , π j (sjt , Qjt , µjt ), µt ,
1
S×A
j

p · |sjt , π j (sjt , Q̂jt , µ̂jt ), µ̂t

≤ KF ||bjt ||(dX (sjt , sjt+1 ) + ||Qjt − Q̂jt ||w
(83)
π j (sjt+1 , Q̂jt , µ̂jt ) − π j (sjt , Qjt , µjt )
≤ KρF (dX (sjt+1 , sjt ) + ||Qjt − Q̂jt ||w + W1 (µt , µ̂t ))
(84)

µt (sjt )



+ K2 + KρF W1 (µt , µ̂t )

+W1 (µt , µ̂t ))
Therefore, from the above two equations,



(2)



≤ KρF K1 ||Qjt − Q̂jt ||w + W1 (µt , µ̂t )


+K1 W1 (µt , µ̂t ) + K2 + KρF W1 (µt , µ̂t )
(85)

In the above derivation, (1) and (2) are obtained from
Assumption 8 and Eq. 84 (also refer Anahtarci, Kariksiz, and
Saldi (2019)). Combining Eq. 78 and Eq. 85, and applying
Assumption 14, the required result is proved. The constant of
contraction is k defined in Assumption 14.

E

Proof of Theorem 5

Theorem 5. Let the Q-updates in Algorithm 1 converge to
(Qj∗ , µj∗ ) for an agent j ∈ {1, . . . , N }. Then, we can construct a policy π∗j from Qj∗ which is expressed as
π∗j (sj ) = arg max
Qj∗ (sj , aj , µj∗ ).
j
j
a ∈A

Then the pair (π∗j , µj∗ ) is a DMFE.
Proof. From the Theorem 4 we know that (Qj∗ , µj∗ ) is a fixed
point of H. Using the Assumption 1, we consider a µ∗ , where
µ∗ (sj ) = µj∗ (sj ), for all states sj ∈ N j . Hence, we can
construct the following equations.
Qj∗ (sjt , ajt , µj∗ ) = rj (sjt , ajt , µ∗,t )
+β

µj∗,t+1 (·) =

R
S

MFRL
Yang et al.
(2018)

MFG
Lasry and
Lions
(2007)

DMFG
(ours)

Game
formulation

Stochastic
Game

Mean
Field
Game

Decentralized
Mean Field
Game

Reward
function

Same for
all agents

Same for
all agents

Can be different

Action
space

Discrete
only

Can
be
continuous

Can be continuous

Action
space

Same for
all agents

Same for
all agents

Can be different

Solution
Concept

Nash Equilibrium
(centralized)

Mean
Field Equilibrium
(centralized)

Decentralized
Mean Field
Equilibrium

Complexity
of obtaining
the mean
field

Exponential Exponential Constant in
in agents
in agents
agents

Theoretical
Guarantees

Needs
very
strong assumptions

Needs
weak assumptions

Needs weak
assumptions

Number of
agents

Should be
large but finite

Can
be
infinite in
the limit

Can be infinite in the
limit

Mean Field
Information

Global

Global

Local

Agents

Identical
and homogeneous

Identical
and homogeneous

Nonidentical

Policy

Stationary

Can
be
nonstationary

Can be nonstationary

State
Observable

Global

Local

Local

Qj∗,max (sjt+1 , aj , µj∗,t+1 )p(sjt+1 |sjt , ajt , µ∗,t )
(86)

Z
S×Aj

p(·|sjt , ajt , µ∗,t )π∗j (ajt |sjt , µj∗,t )µ∗,t (sjt )

(87)
From Eq. 87 and Assumption 1, we can construct the mean
field estimate of agentj, µj∗ . Now, the above equations imply
that π∗j ∈ Φ(µj∗ ) and µj∗ = Ψ(π∗j ). Hence, (π∗j , µj∗ ) is a
decentralized mean field equilibrium.

F

Method

Differences Between Mean Field Settings

Table 1 captures the differences between the three mean field
frameworks, MFRL, MFG, and DMFG. Particularly, we show
that the DMFG is different from other frameworks introduced
previously, and it is more practical than prior methods as it
relaxes some strong assumptions in them.
There are several disadvantages in using a centralized
solution concept like Nash equilibrium (or, by extension,
the mean field equilibrium) in multiagent systems. These
are listed as follows: 1) Centralized nature of the equilibrium, though practical systems have a decentralized information structure. 2) The equilibrium computation is almost
intractable for more than two agents (Neumann 1928). 3)
Needs strong assumptions to give theoretical guarantees of
convergence of learning systems in general sum games (Hu
and Wellman 2003), even in the stationary case. 4) The equilibrium requires agreement between agents even in the competitive case. 5) It is hard to verify this point in practice. Our
decentralized solution concept (DMFE) is more practical than
the Nash equilibrium and the mean field equilibrium, since
it does not suffer these limitations noted for the centralized
methods.

Table 1: Table to capture the differences between Mean Field
Reinforcement Learning (MFRL), Mean Field Games (MFG),
and Decentralized Mean Field Games (DMFG).

G

Mean Field Modelling in DMFG-QL

In this section, we plot the mean square error between the
estimated mean field action (µj,a
t ) (from the neural network
representing Eq. 14) and the true observed local mean field
action (µ̂j,a
t ) to show that the true local mean field action can
be accurately modelled by the DMFG-QL algorithm. We will
use the Battle game for this illustration.

Algorithm 3 provides the pseudo-code of the DMFG-AC
algorithm. This algorithm uses the policy as the actor, and the
value function as the critic, as is common in practice (Konda
and Tsitsiklis 1999). We choose to use a stochastic actor that
does not depend on the mean field. This is done to provide an
algorithm that can work independent of the mean field during
execution. Since the critic (not used during execution) can be
dependent on the mean field, we additionally parameterize
the value function with the mean field. The modified updates
can be seen in Algorithm 3. For both Algorithm 2 and Algorithm 3 we initialize the estimated mean field distribution to
a uniform distribution (arbitrarily).

I

Figure 7: This figure shows the average (per agent) mean
square error (MSE) between the true local mean field action
and the estimated mean field action from the neural network
in each episode (sum of MSE in the 500 steps) of the Battle
game training experiment. The results show that the MSE
steadily reduces, and hence DMFG-QL is able to accurately
model the true mean field. The result shows the average of
10 runs (negligible standard deviation).
Fig. 7 shows the mean square errors (MSE) between the
true mean field action and the estimated mean field action,
averaged per agent, for each episode of the Battle game. The
MSE converges to a small value close to 0 during training.
This shows that the DMFG-QL algorithm can accurately
model the mean field, which contributes to its superior performance in many of our experimental domains. As stated in
Section 6, DMFG-QL algorithm formulates best responses to
the current mean field action. This contrasts with approaches
such as MFQ that formulate best responses to the previous
mean field, which leads to a loss in performance. We start the
plot in Fig 7 from episode 200 (instead of 0), since the first
few episodes have a very high MSE, that simply skews the
axis of the resulting graph.

H

Algorithm Pseudo-Code

Algorithm 2 provides the pseudo-code of the DMFG-QL
algorithm. Here, the neural networks are used as the function
approximator for the Q-function. In addition, we include the
use of a second target network and a replay buffer for training,
as done in the popular Deep Q-learning (DQN) algorithm
(Mnih et al. 2015). Also, similar to Yang et al. (2018), we
specify that the policies satisfy the GLIE assumption and
hence we use the max operator over the Q-value of the next
state (i.e. choose the action that maximizes the Q value)
to calculate the temporal difference (T.D.) error instead of
maintaining a distinct value function as in Eq. 13.

Experimental Details

In this section, we describe each of our game domains in
detail, including the reward functions. We also provide the
implementation details of our algorithms, especially the hyperparameters. We also discuss the wall-clock times of our
algorithmic implementations. Each episode for the Petting
Zoo environments has a maximum of 500 steps. In our implementation of MFQ and MFAC, the agents learn in a decentralized fashion with independent networks and use the
previous mean field of the local neighbourhood instead of the
global mean field.
The first 5 domains are obtained from the Petting Zoo
environment (Terry et al. 2020), and the game parameters are
mostly left unchanged from those given in Terry et al. (2020).
In these games, as agents can die during game play, we use
the agent networks saved in the last available episode during
training for the execution experiments. Complete details of
these domains are given in each of the sub-sections below.

Battle Domain
This is the first domain from the Petting Zoo environment
(Terry et al. 2020) that is mixed cooperative-competitive.
This domain was originally defined in the MAgents simulator
(Zheng et al. 2018). We have two teams of 25 agents, each
learning to cooperate against the members of the same team
and compete against the members of its opponent team. The
agent gets rewarded for attacking and killing agents of the
opposing team. At the same time, the agent is penalized for
being attacked. In our implementation, each agent learns
using its local observation and cannot get global information.
Each agent has a view range of a circular radius of 6 units
around it. Most rewards are left as defaults, as given in Terry
et al. (2020). The agents get a penalty of -0.005 for each
step (step reward) and a penalty of -0.1 for being killed.
The agents get a reward of 5 for attacking an opponent and
a reward of 10 for killing an opponent. There is an attack
penalty of -0.1 (penalty for attacking anything). Agents start
with a specific number of hitpoints (HP) that are damaged
upon being attacked. Agents lose 2 HP when attacked and
recover an HP of 0.1 for every step that they are not attacked.
They start with 10 HP, and when this HP is completely lost,
the agent dies. Each agent can view a circular range of 6 units
around it (view range) and can attack in a circular range of
1.5 units around it (attack range). The action space of each
agent is a discrete set of 21 values, which corresponds to

Algorithm 2: Q-learning for Decentralized Mean Field Games
(DMFG-QL)

Algorithm 3: Actor-Critic for Decentralized Mean Field
Game (DMFG-AC)

1: Initialize the Q-functions (parameterized by weights)

1: Initialize the V -function or critic (parameterized by

Qφj , Qφj_ , for all agents j ∈ {1, . . . , N }
2: Initialize the mean field estimate (parameterized by
weights) µj,a
η for each agent j ∈ {1, . . . , N }
j,a
3: Initialize the estimated mean field µ0,η for each j to a
uniform distribution
4: Initialize the total steps (T) and total episodes (E)
j
5: Obtain the current state st
6: while Episode < E do
7:
while Step < T do
8:
For each agent j, obtain action ajt from the policy
induced by Qφj with the current estimated mean
action µj,a
t,η and the exploration rate β̂ according to
Eq. 15
9:
Execute the joint action at = [a1t , . . . , aN
t ]. Observe
the rewards rt = [rt1 , . . . , rtN ] and the next state
st+1 = [s1t+1 , . . . , sN
t+1 ]
10:
For each agent j, obtain the current observed local
mean action (µ̂j,a
t )
11:
Update the parameter η of the mean field network
using a mean square error between the observed
mean action µ̂j,a
and the current estimated mean
t
action µj,a
for
all
j
t,η
For each j, obtain the mean field estimates µj,a
12:
t+1,η
for the next state sjt+1 using the mean field network
according to Eq. 14
j,a
13:
For each j, store hsjt , ajt , rtj , sjt+1 , µj,a
t,η , µt+1,η i in
replay buffer B
Set the next mean field estimate as the current
14:
j,a
mean field estimate µj,a
t,η = µt+1,η and the next
state as the current state sjt = sjt+1 for all agents
j ∈ {1, . . . , N }
15:
end while
16:
while j = 1 to N do
17:
Sample a minibatch of K experiences
j,a
hsjt , ajt , rtj , sjt+1 , µj,a
t,η , µt+1,η i from B
j
Set y j = rt + γ maxaj Qφj_ (sjt+1 , ajt+1 , µj,a
18:
t+1,η )
t+1
according to Eq. 12
19:
Update the Q network by minimizing the loss
P j
1
2
L(φj ) = K
(y − Qφj (sjt , ajt , µj,a
t,η ))
20:
end while
21:
Update the parameters of the target network for each
agent j with learning rate τ ;

weights) Vφj and the policy or actor (parameterized by
weights) πθj for all agents j ∈ 1, . . . , N
2: Initialize the mean field estimate (parameterized by
weights) µj,a
η for each agent j ∈ {1, . . . , N }
j,a
3: Initialize the estimated mean field µ0,η for each j to a
uniform distribution
4: Initialize the total episodes (E)
j
5: Obtain the current state st
6: while Episode < E do
7:
For each agent j, obtain action ajt from the policy πθj
at the current state sjt
8:
Execute the joint action at = [a1t , . . . , aN
t ]. Observe
the rewards rt = [rt1 , . . . , rtN ] and the next state
st+1 = [s1t+1 , . . . , sN
t+1 ]
9:
For each agent j, obtain the current observed local
mean field action (µ̂j,a
t )
10:
Update the parameter η of the mean field network
using a mean square error between the observed mean
j,a
action µ̂j,a
t and the current estimated mean action µt,η
for all j
11:
For each j, obtain the mean field estimates µj,a
t+1,η
for the next state sjt+1 using the mean field network
according to Eq. 14
12:
Set y j = rtj + γVφj (sjt+1 , µj,a
t+1,η ) as the T.D. target
according to Eq. 12
13:
For each j, update the critic by minimizing the loss
2
L(φj ) = (y j − Vφj (sjt , µj,a
t,η ))
14:
For each j, update the actor using the log loss J (θj ) =
log πθj (ajt |sjt )L(φj )
15:
Set the next mean field estimate as the current mean
j,a
field estimate µj,a
t,η = µt+1,η and the next state as the
current state sjt = sjt+1 for all agents j ∈ {1, . . . , N }
16: end while

φj_ ← τ φj + (1 − τ )φj_
22: end while

taking moving and attacking actions in the environment. In
this game, we assume that agents can get perfect information
about actions of other agents at a distance of 6 units from
themselves and no information beyond this point. In the
execution phase, a game is considered to be won by a team

that kills more opponent agents. If both teams kill the same
number of agents, the team having the higher cumulative
reward is determined as the winner.

Gather Domain
In this environment, all agents try to capture limited food
available in the environment and gain rewards. Each food
needs to be attacked before it can be captured (takes 5 attacks
to capture food). This is a fully competitive game, where all
agents try to outcompete others in the environment and gain
more food for themselves. Agents can also kill other agents in
the environment by attacking them (just a single attack). Each
agent gets a step penalty of -0.01, an attack penalty (penalty
for attack) of -2 and a death penalty (punishment for dying)
of -20. Also, each agent gets a reward of 20 for attacking
food and a reward of 60 for capturing the food. There are a
total of 30 agents learning in our environment. The action
space of each agent is a set of 33 values. The agents have a

view range of 7 and an attack range of 1. All other conditions
and rewards are the same as the Battle game.

Combined Arms Domain
The Combined Arms environment is a heterogeneous largescale team game with two types of agents in each team. The
first type is a ranged agent, which can move fast and attack
agents situated further away but has fewer HP. The second
type is the melee agent that can only attack close-by agents
and move slowly; however, they have more HP. The reward
function for the agents is the same as that given in the battle
game. The action space of the melee agents is a discrete set
of 9 values, while the action space of the ranged agents is a
discrete set of 25 values. The actions correspond to moving
in the environment and attacking neighbouring agents. The
ranged agents have a maximum HP of 3, and the melee
agents have a maximum HP of 10. The maximum HP is
the limit after which the HP of any agent cannot increase
in this environment. Like the battle domain, agents lose 2
HP for each time they are attacked and gain 0.1 HP for each
step without being attacked. In our experiments, each team
consists of 25 agents, with 15 ranged and 10 melee agents
at the start. The view range is 6 and the attack range is 1 for
melee agents. The view range is 6 and attack range is 2 for
ranged agents. All other conditions and rewards are the same
as the Battle game. To create the mean field we choose to use
the action space of the type which has the larger number of
actions (i.e. we use the action space of the ranged agents).

Tiger-Deer Domain
In the tiger-deer environment, tigers are the learning agents,
cooperating with each other to kill deer in the environment.
At least two tigers need to attack a deer together to get high
rewards. The tigers start with an HP of 10 and gain an HP
of 8 whenever they kill deer. The tigers lose an HP of 0.021
at each step they do not eat a deer and die when they lose
all the HP. In this game, the deer move randomly in the
environment, and start with an HP of 5 and lose 1 HP upon
attack. The tiger gets a reward of +1 for attacking a deer
alongside another tiger. In this game, each tiger is assumed to
get perfect information about the actions of other tigers at a
distance of 4 units from itself and no information beyond this
point. The view range of the tiger is 4 and the attack range is
1. The tigers also get a shaping reward of 0.2 for attacking a
deer alone. All other rewards and conditions are the same as
the Battle game.

Waterworld Domain
The Waterworld domain was first introduced as a part of the
Stanford Intelligent Systems Laboratory (SISL) environments
by Gupta, Egorov, and Kochenderfer (2017). We use the same
domain adapted by the Petting Zoo environment (Terry et al.
2020). This is a continuous action space environment, where
a group of pursuer agents aim to capture food and avoid
poison. These pursuers are the learning agents, while both
food and poison move randomly in the environment. This is a
cooperative environment where pursuer agents need to work
together to capture food. At least two agents need to attack

a food particle together to be able to capture it. The action
is a two-element vector, where the first element corresponds
to horizontal thrust and the second element corresponds to
vertical thrust. The agents choose to use a thrust to make
themselves move in a particular direction with a desired
speed. The action values are in the range [-1, +1]. Our domain
contains 25 pursuer agents, 25 food particles and 10 poison
particles. The food is not destroyed but respawned upon
capture. The agents get a reward of +10 upon capturing food
and a penalty of -1 for encountering poison. If a single agent
encounters food alone, it gets a shaping reward of +1. Each
time an agent applies thrust, the agent obtains a penalty of
thrust penalty × ||action||, where the value of thrust penalty
is -0.5. Since this is a cooperative environment, each agent
is allowed access to the global mean field action which is
composed of actions of all agents in the environment, at each
time step. This is done to simplify this complex domain.

Ride-Sharing Domain
In this domain, our problem formulation and environment are
the same as that described in Shah, Lowalekar, and Varakantham (2020). The demand distribution is obtained from the
publicly available New York Yellow Taxi Dataset (NYYellowTaxi 2016). The overall approach follows six steps. First,
the user requests are obtained from the dataset. Second, sets
of feasible trips are generated (by an oracle) using the approach in Alonso-Mora et al. (2017). These feasible trips
keep the action space from exploding. Third, the feasible actions are scored by the individual agents using their respective
value functions. Fourth, a mapping of requests takes place
by checking different constraints. Fifth, the final mapping is
used to update the rewards for the individual agents. Sixth,
the motion of vehicles is simulated until the next decision
epoch.
We consider a maximum of 120 vehicles in our experiments. Since we are learning in a decentralized fashion, each
vehicle maintains its own network and is computationally
intensive. However, in practical applications, the training can
be parallelized across agents and the computational demands
will not be a limitation of our proposed setting.
Our goal in this experiment is to implement the mean field
algorithm on a real-world problem and compare the performance to other state-of-the-art approaches. The experimental
setup is along the lines of Alonso-Mora et al. (2017); Shah,
Lowalekar, and Varakantham (2020), where we restrict ourselves to the street networks of Manhattan, where the vast
majority of requests are contained. The New York Yellow
Taxi dataset contains information about ride requests at different times of the day during a given week. Similar to Shah,
Lowalekar, and Varakantham (2020), we use the pickup and
drop-off locations, pickup times and travel times from the
dataset. The dataset has about 330,000 requests per day. In
our experiments, the taxis are assigned a random location at
the start and react to incoming ride requests. We train the
networks using data pertaining to 8 weekdays and validate it
on a single day as done in Shah, Lowalekar, and Varakantham
(2020). We assume all vehicles have similar capacities for
simplicity, although our decentralized set-up is completely
applicable to environments with very different types of vehi-

cles as against prior work that relied on centralized training
(Lowalekar, Varakantham, and Jaillet 2019; Shah, Lowalekar,
and Varakantham 2020).
In the experiments, a single day of training corresponds to
one episode. Each episode has 1440 steps, where each step
(decision unit) is considered to span one minute. The initial
location of vehicles at the beginning of an episode (single
day of training) is random. The state, action space and reward
function in our system is the same as that in Shah, Lowalekar,
and Varakantham (2020). The state of the system can be described as a tuple (ct , ut ), where ct is the location of each
vehicle j denoted by cjt = (pj , t, Lj ) representing its trajectory. This captures the current location and a list of future
locations that each vehicle visits. The user request i at the
time t is represented as uit = (oi , ei ) which corresponds to
the origin and destination of the requests. The action for each
agent is to provide a score for all the requests with the objective of assigning the user requests to itself. The user request
should satisfy the constraints at the vehicle level (maximum
allowed wait time and capacity limits) and the constraints
at the system level (each request is only assigned to a single agent). Since these constraints are environmental, we
continue to use a central agent that performs the constraints
check before assigning requests to individual agents. Each
agent gets a reward based on the proportion of requests in its
feasible action set that it is able to satisfy, as done in Shah,
Lowalekar, and Varakantham (2020).

J

Hyperparameters and Implementation
Details

The implementation of DMFG-QL, IL and MFQ almost use
the same hyperparameters, with the learning rate set as α =
10−2 . The temperature for the Boltzmann policy is set as
0.1. Additionally, we also conduct epsilon greedy exploration
which is decayed from 20% to 1% during the training process.
The discount factor γ is equal to 0.9. The replay buffer size
is 2 × 105 , and the agents use a mini-batch size of 64. The
target network is updated at the end of every episode.
Our implementations of DMFG-AC and MFAC almost
use the same hyperparameters, where the learning rate of
the critic is 10−2 and the learning rate of the actor is 10−4 .
The mean field network of the DMFG-AC uses a learning
rate of 10−2 . The discount factor is the same as the other
three algorithms. In our implementation of MFAC, we do not
use replay buffers unlike the implementation of Yang et al.
(2018). We found this version of the actor-critic algorithm
using the current updates (instead of delayed updates through
the replay buffer) is more stable and performs better than
the implementation of Yang et al. (2018) in our experiments.
Additionally, our implementations of both MFAC and DMFGAC use complete decentralization during execution where
the actors only need to use their local states (mean field does
not need to be maintained anymore). Also since a separate
network is being maintained for the stochastic policy (actor)
we do not use the Boltzmann policy for the actor-critic based
methods (MFAC and DMFG-AC).
For the continuous action space Waterworld environment,
every component of the obtained estimated mean field is

normalized to be in the range [−1, 1] (the range of the action
values) and we do not use a softmax for the output layer (since
this a mixture of Dirac deltas as discussed in Section 7).
Most hyperparameters are the same or closely match those
used by prior works (Yang et al. 2018; Subramanian et al.
2020; Guo et al. 2019; Subramanian et al. 2021) in many
agent environments.
For the ride-sharing experiments, DMFG-QL uses the
same network architecture as given in Shah, Lowalekar, and
Varakantham (2020) for the NeurADP algorithm. The implementation of CO and NeurADP uses the implementation provided by Shah, Lowalekar, and Varakantham (2020) except
that all agents are fully decentralized as mentioned before.
Unlike the approach in Shah, Lowalekar, and Varakantham
(2020), each of our agents train their independent neural network using their local experiences. This network learns a
suitable value function that can assign an appropriate score
to each of the ride requests. This is done for both our implementations of NeurADP and DMFG-QL. For our baseline of
CO (Alonso-Mora et al. 2017), we used the implementation
from Shah, Lowalekar, and Varakantham (2020), which used
the immediate rewards as the score for the given requests
along with a small bonus term pertaining to the time taken
to process a request (faster processing of requests is encouraged). The mean field for the DMFG-QL implementation
is obtained by processing a distribution of the ride requests
across every node in the environment at each step. This mean
field is made available to all agents during both training and
testing. Also, we use a slightly different architecture for estimating the mean field in this domain (3 Relu layers of 50
nodes and an output softmax layer).
The DDPG hyperparameters are based on Lillicrap et al.
(2016) and the PPO hyperparameters are based on Schulman
et al. (2017). DDPG uses the learning rate of the actor as
0.001 and that of the critic as 0.002 and a discount factor of
0.9. We use the soft replacement strategy with learning rate
0.01. The batch size is 32. The PPO implementation uses
the same batch size and discount factor. The actor learning
rate is 0.0001 and critic learning rate is 0.0002. Independent
PPO uses a single thread implementation, since the data
correlations are already being broken by the non-stationary
nature of the environment induced by the multiple agents.
This is also computationally efficient.
We use a set of 30 random seeds (1 – 30) for all training
experiments and a new set of 30 random seeds (31 – 60) for
all execution experiments.

K

Complexity Analysis

A tabular version of our DMFG-QL algorithm is guaranteed
to be linear in the total number of states, polynomial in the
total number of actions, and constant in the number of agents.
These guarantees are the same for both space complexity and
time complexity. Comparatively, a tabular version of mean
field Q-learning (MFQ) algorithm from Yang et al. (2018)
has a space complexity that is linear in the number of states,
polynomial in the number of actions and exponential in the
number of agents. This is due to Eq. 3 requiring each agent
to maintain Q-tables of all other agents. The time complexity
is also the same as the space complexity in this case, since

each entry in the table may need to be accessed in the worst
case.

L

More Related Work

Mean field games have been used in the inverse reinforcement learning paradigm, where the objective is to determine
a suitable reward function given expert demonstrations (Yang
et al. 2017). Model-based learning solutions have also been
proposed for mean field games (Kizilkale and Caines 2013;
Yin et al. 2014), though these methods suffer from restrictive
assumptions on the model and the analysis does not generalize to other models. Methods such as Cardaliaguet and
Hadikhanloo (2017) perform a very computationally expensive computation using the full knowledge of the environment, which does not scale to large real-world environments.
The work by Fu et al. (2019) provides a mean field actorcritic algorithm along with theoretical analysis in the linear
function approximation setting, unlike other works which
only analyze the tabular setting (Guo et al. 2019; Yang et al.
2018). However, it is not clear if this algorithm is strong empirically since it does not contain empirical experiments that
illustrate its performance. Further, this work considered the
linear-quadratic setting that contains restrictions on the type
of reward function. Mguni, Jennings, and de Cote (2018) explore the connection between MARL and mean field games
in the model-free setting.
The mean field setting under the cooperative case can be
handled using a mean field control model (Carmona, Laurière, and Tan 2019a). In the linear-quadratic setting, Carmona, Laurière, and Tan (2019a) prove that policy-gradient
methods converge to local equilibrium. In further work, the
same authors prove that Q-learning algorithms also converge
(Carmona, Laurière, and Tan 2019b).
In the experiments, we consider a real-world application
of our methods on the Ride-Pool Matching Problem (RMP)
as originally defined in Alonso-Mora et al. (2017). This is
the problem considered by top ride-sharing platforms such as
UberPool and Lyft-Line. The problem studies the efficiency
of accepting ride requests by individual vehicles in such a way
that the vehicles make more money (cater to more requests)
per trip and the ride-sharing platform improves its ability
to serve more orders. Previous approaches to solving the
RMP problem have used standard optimization techniques
(Ropke and Cordeau 2009). However, these methods are not
scalable to large environments. One example from this class
of methods is the zone path construction approach (ZAC)
(Lowalekar, Varakantham, and Jaillet 2019). The ZAC solves
for an optimization objective where the environment is abstracted into zones and each zone path represents a set of trips.
The available vehicles are assigned to suitable zone paths using the ZAC algorithm. Another proposed solution was to
make greedy assignments (Alonso-Mora et al. 2017), which
considers maximizing the immediate returns and not the long
term discounted gains traditionally studied in RL. Prior work
has also considered RL approaches for this problem (Xu et al.
2018; Wang et al. 2018). However, these works consider very
restrictive settings, which do not model multiagent interactions between the different vehicles. The work by Li et al.

(2019) used a mean field approach for order dispatching, however, it assumes vehicles serve only one order at a time. The
recent work by Shah, Lowalekar, and Varakantham (2020)
introduced a very general formulation for the RMP where
vehicles of arbitrary capacity are designed to serve batches
of requests, with the whole solution scalable to thousands of
vehicles and requests. They introduced a Deep Q-Network
(DQN) (Mnih et al. 2015) based RL approach (called neural
approximate dynamic programming or NeurADP) that learns
efficient assignment of ride requests. However, the proposed
approach assumes a set of identical vehicles and uses centralized training, where all vehicles learn the same policy
using centralized updates. This is not practical in real-world
environments, where the individual vehicles are typically
heterogeneous (many differences in vehicular capacity and
preferences). We propose a mean field based decentralized
solution to this problem, which is more practical than the
approach by Shah, Lowalekar, and Varakantham (2020).

M

Wall Clock Times

The training for all the experiments on the simulated Petting
Zoo domains was run on a 2 GPU virtual machine with 16
GB GPU memory per GPU. We use Nvidia Volta-100 (V100)
GPUs for all these experiments. The CPUs use Skylake as the
processor microarchitecture. We have a CPU memory of 178
GB. The Battle, Combined Arms and Gather experiments
take an average of 5 days wall clock time to complete for
all the considered algorithms. The Tiger-Deer experiments
take an average of 4 days wall clock time to complete and
the Waterworld experiments take an average of 2 days wall
clock time to complete.
The majority of our experiments on the RMP problem
were run on a virtual machine with 4 Ampere-100 GPUs with
a GPU memory of 40 GB each. The CPUs use Skylake as the
processor microarchitecture. Each training takes an average
of 5 days to complete execution.

N

Statistical Significance Tests

We run a statistical significance test for all the main results
in our paper. In the MAgent environments, for the training
results we conduct an unpaired two-sided t-test at the last
episode (2000) of training. For the execution results, we conduct a Fischer’s exact test for the average performances. The
tests are between the best performing algorithm (DMFGQL or DMFG-AC) and the best performing baseline (IL,
MFQ, MFAC). In the ride-sharing experiments we conduct
an unpaired two-sided t-test for the average and standard
deviation of performances in Figure 6. The test is conducted
between DMFG-QL and NeurADP (second best performing
algorithm). We report the p-values for all the tests. As convention, we treat all p-values less than 0.05 as statistically
significant outcomes.
From the results in Table 2, we see that the better performance given by our algorithms (DMFG-QL or DMFG-AC) is
statistically significant in all domains except the Tiger-Deer
domain. As noted in Section 7, the MFQ and MFAC algorithms perform as well as the DMFG-QL and DMFG-AC
algorithms in this cooperative domain. Though we observe

Experimental Training
Domain
(p-value)

Testing (pvalue)

Statistically
Significant

Battle

p < 0.01

p < 0.01

Yes

Combined
Arms

p < 0.01

p < 0.01

Yes

Gather

p < 0.01

p < 0.01

Yes

Tiger-Deer

p < 0.5

p < 0.7

No

Waterworld

p < 0.01

p < 0.01

Yes

Table 2: Statistical significance of our results in simulated
experiments.
# of
Vehicles
(N )

Maximum
Pickup
Delay
(τ )
(sec)

Capacity
(c)

p-value

Statistically
Significant

80

580

10

p < 0.01

Yes

100

580

10

p < 0.01

Yes

120

580

10

p < 0.01

Yes

100

520

10

p < 0.01

Yes

100

640

10

p < 0.01

Yes

100

580

8

p < 0.01

Yes

100

580

12

p < 0.02

Yes

Table 3: Statistical significance of our results in the ridesharing domain.
that DMFG-QL gives the best overall average performance
in both training and execution in the Tiger-Deer environment,
the results are not statistically significant as noted from the
p-values in Table 2.
The statistical significance results for the ride-sharing experiment in Table 3 shows that our observations regarding
the superior performance of DMFG-QL are statistically significant.

