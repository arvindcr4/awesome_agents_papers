arXiv:2510.22570v1 [cs.RO] 26 Oct 2025

C URRICULUM -BASED I TERATIVE S ELF -P LAY FOR S CALABLE
M ULTI -D RONE R ACING ∗

Onur Akgün
Department of Mechatronics Engineering
Faculty of Engineering
Turkish-German University
Istanbul, Türkiye
akgun@tau.edu.tr

A BSTRACT
The coordination of multiple autonomous agents in high-speed, competitive environments represents
a significant engineering challenge. This paper presents CRUISE (Curriculum-Based Iterative SelfPlay for Scalable Multi-Drone Racing), a reinforcement learning framework designed to solve this
challenge in the demanding domain of multi-drone racing. CRUISE overcomes key scalability
limitations by synergistically combining a progressive difficulty curriculum with an efficient self-play
mechanism to foster robust competitive behaviors. Validated in high-fidelity simulation with realistic
quadrotor dynamics, the resulting policies significantly outperform both a standard reinforcement
learning baseline and a state-of-the-art game-theoretic planner. CRUISE achieves nearly double
the planner’s mean racing speed, maintains high success rates, and demonstrates robust scalability
as agent density increases. Ablation studies confirm that the curriculum structure is the critical
component for this performance leap. By providing a scalable and effective training methodology,
CRUISE advances the development of autonomous systems for dynamic, competitive tasks and serves
as a blueprint for future real-world deployment.
Keywords autonomous drone racing · self-play · curriculum learning · deep reinforcement learning · multi-agent
systems · adaptive racing strategies

1

Introduction

The effective coordination of multiple autonomous agents is a fundamental challenge in modern engineering, with
critical applications in logistics automation, autonomous transportation, and environmental monitoring. These realworld scenarios often involve dynamic, competitive, or collaborative elements where agents must make split-second
decisions in cluttered environments. Among these challenges, high-speed, competitive multi-agent racing serves as a
particularly demanding benchmark, pushing the limits of agility, safety, and scalable coordination Moon et al. (2019);
Foehn et al. (2022). While reinforcement learning (RL) has shown champion-level performance for single agents
Kaufmann et al. (2023), scaling these successes to the multi-agent domain remains a significant hurdle due to issues of
non-stationarity, computational complexity, and safe exploration Wang et al. (2020).
Developing and testing control policies for such high-stakes physical systems presents significant practical barriers.
The process involves prohibitive hardware costs, substantial safety risks to equipment and personnel, and the difficulty
of creating perfectly reproducible test conditions. Consequently, high-fidelity simulation has become an essential
and standard tool in the engineering design life-cycle Guerra et al. (2019); Song et al. (2021). It enables the initial
design, rigorous validation, and comprehensive benchmarking of complex multi-agent control policies before real-world
deployment. However, even within simulation, training robust and scalable policies efficiently remains an open problem,
necessitating novel frameworks that can manage complexity and reduce sample requirements.
∗

This paper is currently under review at the journal Engineering Applications of Artificial Intelligence.

To address this gap, this paper introduces CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone
Racing), a novel training framework designed to produce high-performance, scalable policies for complex multi-agent
coordination tasks. CRUISE synergistically integrates two key principles: (1) a progressive curriculum that simplifies
the learning problem by gradually increasing the number of opponents and task difficulty, and (2) an efficient iterative
self-play mechanism that fosters robust competitive behaviors by training against an evolving set of frozen, highperforming opponent policies. This combined approach is specifically designed to enhance sample efficiency and ensure
scalability—two of the most critical factors for complex robotics applications.
We validate the CRUISE framework within the challenging domain of multi-drone racing, using a high-fidelity simulator
with realistic quadrotor dynamics. 2 . Our results show that CRUISE policies substantially outperform both a standard
RL baseline and a state-of-the-art game-theoretic planner in terms of racing speed, success rate, and collision avoidance
(Sec. 4). The contributions of this work are therefore threefold. First, we propose a novel and generalizable training
framework, CRUISE, that integrates curriculum learning with iterative self-play for efficient and scalable multi-agent
coordination. Second, we provide comprehensive empirical validation of the framework, including ablation studies
that confirm the critical role of the curriculum structure in achieving high performance. Finally, we support open and
reproducible science by providing the full open-source release of our code, environments, and pretrained models 3
The remainder of this paper details the CRUISE methodology (Sec. 3), presents the empirical results (Sec. 4), discusses
their implications (Sec. 4.6), and offers a conclusion and directions for future work (Sec. 5, Sec. 6).

2

Related Works

Autonomous multi-drone racing research spans both planning-based and learning-based methods. Game-theoretic
planners Spica et al. (2020); Di et al. (2023) have been developed to address the competitive aspects of racing. Notably,
Sensitivity Enhanced Iterative Best Response (SE-IBR) Wang et al. (2020), which extends game theory to 3D racing,
serves as a key state-of-the-art planning baseline in our work. However, these planners often face scalability challenges
with an increasing number of agents and rely on accurate dynamics models. In parallel, Reinforcement Learning (RL)
has excelled in single-agent drone racing, with policies achieving champion-level performance against human pilots
Kaufmann et al. (2018); Loquercio et al. (2019); Kaufmann et al. (2023) and demonstrating competitiveness with
optimal control Song et al. (2023). However, applying Multi-Agent RL (MARL) directly to this domain introduces
significant scalability and sample efficiency hurdles due to the fast, adversarial physical interactions. Standard RL
frameworks (e.g., Stable-Baselines3 Raffin et al. (2021), used for our VANILLA baseline) typically struggle in MARL
without substantial enhancements.
To tackle these challenges, the community has explored innovations across multiple axes. Concurrent work has focused
on achieving time-optimal, aggressive flight through sophisticated reward engineering, demonstrating impressive
sim-to-real transfer by training the full multi-agent system directly Wang et al. (2025b). Other advances have improved
agent capabilities by enhancing sample efficiency through the design of information-dense, structured observations for
navigation Xu et al. (2024), or by developing novel communication architectures like the Graph Diffusion Network to
improve coordination in cooperative tasks Jiang et al. (2025). A distinct paradigm is Evolutionary Robotics, which uses
Evolutionary Algorithms to optimize the parameters of a pre-defined, often simpler, controller rather than learning a
complex policy from scratch, a technique demonstrated effectively in real-world swarm formations Stolfi and Danoy
(2024). While powerful, these approaches focus on optimizing agent perception, communication, or the parameters of a
fixed control law, rather than the learning process for emergent strategy.
Perhaps the most relevant direction to our work is the use of curriculum learning (CL) and self-play (SP), which have
proven effective in domains ranging from simulated sports Lin et al. (2023) to strategy games Silver et al. (2017);
Huynh et al. (2024). In robotics, curriculum-based approaches have been diverse. For cooperative tasks, curricula
have been implemented via staged reward functions that guide agents through pre-defined sub-tasks like formation and
tracking Wang et al. (2025a). This concept has been extended further by designing complex, theoretically-grounded
rewards based on classical flocking models, which implicitly encode a curriculum by guiding agents toward a provably
stable equilibrium Guo et al. (2025). Other curricula have focused on improving generalization for a single agent, either
by creating versatile, generalist policies via Multi-Task RL Xing et al. (2024) or by automatically generating novel race
tracks Xing et al. (2025) or enabling diverse missions without retraining through goal-conditioning Kim et al. (2024).
Sample efficiency has also been a direct target, with methods that strategically re-sample challenging initial states to
focus the learning process Messikommer et al. (2024).
2
A supplementary video detailing our method and demonstrating its performance is available at
https://drive.google.com/file/d/1k7necen2DgIxaYT2alKK8-b20sE_AyDA/view
3
Full implementation details, including all hyperparameters, are available at
https://doi.org/10.5281/zenodo.17256943

2

zB
4
zW

3
yB

yW
xW
2

xB

1

Figure 1: Quadrotor reference frames and rotor configuration. The world frame axes (xW , yW , zW ) are fixed, with
zW pointing upward. The body frame (xB , yB , zB ) is attached to the drone’s center of mass with xB pointing forward.
Blue arrows indicate thrust direction, while curved arrows show rotor rotation direction.
The CRUISE framework is positioned distinctly from these diverse approaches. It addresses the unique challenge of
scalable learning for emergent competitive strategy. Our work investigates whether complex, adversarial behaviors like
blocking and overtaking can be learned end-to-end, without the aid of explicit hierarchical planners, heavily engineered
reward curricula Wang et al. (2025a), or pre-defined control laws Stolfi and Danoy (2024). Crucially, unlike methods
that encode the solution into a complex, theoretically-grounded reward function Guo et al. (2025), CRUISE utilizes a
simple, sparse reward and instead engineers the learning process itself. We demonstrate that a structured curriculum,
implemented through iterative self-play, provides a powerful and scalable methodology to solve the severe exploration
problem in head-to-head competition, fostering the emergence of sophisticated tactics. This focus on scalable learning
within a competitive multi-agent context is a distinct and complementary contribution to the goal of single-agent task
generalization Xing et al. (2024). This positions our work as a contribution to understanding how to structure the
learning process to enable scalable, emergent strategy in complex, competitive multi-agent domains.

3

Methodology

This section details the CRUISE framework, beginning with the drone modeling and control architecture before
describing our core contribution: a reinforcement learning approach integrating a structured curriculum with iterative
self-play.
3.1

Drone Dynamics and Controller Design

We model the drone using standard quadrotor dynamics and employ a hierarchical controller that allows the RL policy
to operate at a high level of abstraction.
3.1.1

Notation and Dynamics

We define a world frame W (+zW up) and a body frame B (+xB forward), illustrated in Fig. 1. We use standard
notation for position pW B , velocity vW B , Euler angles Θ = [ϕ, θ, ψ]T , body angular velocity ω B = [p, q, r]T , mass
m, gravity gW , inertia matrix J, collective thrust T (acting along −zB , so TB = [0 0 T ]T ), and body torque τ B .
ṗW B = vW B

(1)

Θ̇ = WΘ (Θ)ω B
1
v̇W B = gW + RW B (Θ)TB
m
ω̇ B = J−1 (τ B − ω B × (Jω B ))

(2)
(3)
(4)

where RW B (Θ) is the body-to-world rotation matrix and WΘ (Θ) maps body rates to Euler rates.
3.1.2

Hierarchical Controller

A hierarchical controller translates the reference velocity vref (from policy action via Eq. 11) to motor commands (thrust
T , torques τ B ).
3

Velocity Tracking Controller (Outer Loop) This loop computes desired acceleration ades ∈ R3 from velocity error
ev = vref − vW B via a PD law:
ades = Kv ev + Kvd ėv
(5)
where Kv , Kvd are diagonal gains; ėv is approximated using finite differences.
Low-Level Controller (Inner Loop) The inner loop uses ades to compute an intermediate commanded acceleration
acmd :
acmd = Kp,pos ades − Kd,pos vW B
(6)
where Kp,pos , Kd,pos are diagonal gains. Collective thrust T is then derived from acmd ’s vertical component, compensating for gravity:
T = mg + m(acmd )z
(7)
Desired roll ϕdes and pitch θdes are computed from acmd ’s lateral components using a simplified mapping:
ϕdes = −(acmd )y /g

,

θdes = (acmd )x /g

(8)

T

With desired yaw ψdes = 0, the desired attitude Θdes = [ϕdes , θdes , ψdes ] is formed. Body torques τ B are then
computed by a PD attitude controller:
τ B = Kp,att (Θdes − Θ) − Kd,att ω B

(9)

where Θ, ω B are current attitude and body rates, and Kp,att , Kd,att are gains.
All specific PD gains (e.g., Kv , Kp,att ) were tuned for stability and are provided in our code repository.
3.2

Racing Agent Design and Training

This subsection details our core contribution: the CRUISE training framework integrating Proximal Policy Optimization
(PPO) Schulman et al. (2017) with curriculum learning and iterative self-play.
3.2.1

State and Action Space

State Space Each drone agent i observes a comprehensive state vector ot,i providing proprioceptive and exteroceptive
information:
T
ot,i = [pTrel,i , dnorm,i , vnorm,i
, vproj,i , hTi , pTnorm,i ,

sin(∆ψi ), cos(∆ψi ), PTopp,i , DTopp,i ]T

(10)

The components (detailed in our code repository) include: normalized relative position (prel,i ) and distance (dnorm,i ) to
the target gate; normalized velocity (vnorm,i ); projected velocity (vproj,i ); one-hot target gate index (hi ); normalized
world position (pnorm,i ); yaw difference to gate orientation (sin(∆ψi ), cos(∆ψi )); and relative positions/distances to
opponents (Popp,i , Dopp,i ). Normalization uses environment-dependent factors (dmax , vmax ).
Action Space The policy outputs a continuous, normalized desired acceleration araw
∈ [−1, 1]3 . This is processed
t
k
by clipping to [−1, 1], scaling by a curriculum-dependent agility coefficient α , and integrating using timestep ∆t to
desired
produce the reference velocity vt+1
for the low-level controller (Eq. 5):
desired
vt+1
= vW B,t + (αk · clip(araw
t , −1, 1)) · ∆t

(11)

This allows the policy to focus on high-level maneuvering commands.
3.2.2

Reward Function

The reward function Rk (st , at ) shapes agent behavior during curriculum stage k by combining multiple objectives,
summarized in Table {1}.
Table 1: Summary of Reward Function Component Objectives
Component
Rprox
Rprog
Ralign
Rspeed
Rover
Rcoll

Objective / Purpose
Encourage proximity to the target gate.
Reward incremental progress towards the target.
Penalize misalignment with the direct path to the target.
k
Encourage speed near the curriculum target vmin
.
Bonus for overtaking another drone.
Penalty for colliding with other drones.

4

The total reward is a weighted sum:
Rk (st , at ) = wprox Rprox + wprog Rprog − walign Ralign
+ wspeed Rspeed + wover Rover − wcoll Rcoll

(12)

Specific weights (wprox , . . . , wcoll ) and reward component parameters (e.g., a in Eq. 13, β in Eq. 14) are curriculumdependent coefficients, detailed in our code repository. The individual components are defined below. In these equations,
vt represents the drone’s current velocity vector, and ut is the unit vector pointing from the drone’s position to the
center of the target gate. The term I(·) is the indicator function:
Rprox (st ) = 2 ·

e−a·xt − e−a
−1
1 − e−a

Rprog (st ) = β · (dt−1 − dt )

1 − v t · u t
∥vt ∥∥ut ∥
Ralign (st ) =
0

Rspeed (st ) =

if ∥vt ∥ > ϵ

(13)
(14)

(15)

otherwise

k
k
vt − vmin
, if vt ≤ vmin
k
k
vmin − vt , if vt > vmin

(16)

For Rover , a bonus is awarded when agent i overtakes agent j. Overtaking is detected when the projection projtij =
(xj − xi ) · v̂i (of j’s relative position from i, onto i’s velocity unit vector v̂i ) changes from negative at t − 1 to positive
at t:
X


t
Rover,i (st ) =
rover · I (projt−1
(17)
ij < 0) ∧ (projij > 0)
j̸=i

Rcoll,i (st ) = I(∃j ̸= i : ∥pW B,i − pW B,j ∥ < δ)
3.2.3

(18)

Curriculum Learning Implementation

To facilitate efficient and robust policy learning, we employ a five-stage curriculum (k = 1, . . . , 5), where task
difficulty and realism are systematically increased. Each stage k is defined by environment and reward parameters Θk :
 k
k
k
k
Θk = vmin
, αk , ckenable , wcoll
, gtol
, wover
(19)
k
where vmin
is the minimum reference velocity target (m/s), αk action scaling (agility coefficient), ckenable indicates if
k
k
k
collision penalties are active, wcoll
the collision penalty magnitude, gtol
allowable gate passage tolerance (m), and wover
the reward for overtaking. Parameter values for each stage are detailed in Table 2.

Table 2: Curriculum Learning Stage Parameters (Qualitative Trends)
k
Stage
Name
Timesteps vmin
6
1
Basics
1 × 10
1.0
2
Intermediate
3 × 106
3.0
3
Advanced
6 × 106
5.0
4
Advanced II
1 × 107
7.0
5
Advanced III 2 × 107 10.0

k
k
k
αk ckenable wcoll
gtol
wover
2.0 False 0.0 0.5 0.0
3.0 True 0.25 0.3 0.1
4.0 True 0.5 0.25 0.2
6.0 True* 0.6 0.2 0.2
7.5 True* 0.7 0.2 0.2

Note: *From Stage 4 onward, collisions not only incur a penalty but also terminate the episode.

Curriculum Design and Progression Our curriculum is designed to progressively build the necessary skills for
high-speed racing by guiding the agent through five distinct stages (see Table 2 for specific parameter values). The
stages are as follows:
• Stage 1 (Basics): Focuses on fundamental navigation. By using a low speed and disabling collision penalties,
this stage allows the agent to learn the core task of flying through gates without being prematurely punished
for early mistakes, thus solving the initial exploration problem.
5

• Stage 2 (Intermediate): Introduces penalties for colliding with static obstacles (i.e., the track walls) while
increasing speed. This stage begins to shape the agent’s policy to balance speed with safety and precision.
• Stage 3 (Advanced): Further increases speed and penalties, refining the agent’s ability to fly aggressively
while respecting the track boundaries.
• Stage 4 (Advanced II): Critically, collisions are now configured to be terminal events. This strict condition
forces the agent to develop a truly robust policy, as even a single error ends the attempt.
• Stage 5 (Advanced III): Pushes the agent to its limits with maximum speed and the tightest tolerances. This
stage polishes the policy, ensuring it is capable of handling the most extreme conditions before facing dynamic
opponents.
init
To ensure a smooth adaptation as difficulty increases, policy weights are transferred after each stage (θk+1
= θk ),
allowing the agent to build directly on prior competencies. The progression incrementally increases difficulty towards
5
the system’s dynamic limits (e.g., vmin
= 10.0 m/s), balancing safe training with high-performance evaluation. Upon
completion of this single-agent curriculum, the trained policy is then used to initialize the iterative self-play phase for
multi-agent racing, where agents learn to handle the complexity of dynamic opponents.

3.2.4

Self-Play Methodology

To train competitive policies efficiently and scalably (O(n) complexity), we employ an iterative self-play mechanism,
detailed in Algorithm 1.
Training and Opponent Update A single active policy πθa is trained via PPO against n − 1 frozen opponent policies
{πϕi }. These opponents are periodically updated by copying the active policy’s weights (ϕi ← θa for all i) if πθa ’s win
rate w against them exceeds a predefined threshold τ . The win rate w is computed over neval evaluation episodes as:

neval 
1 X
(j)
(j)
I pa > max pk
(20)
w=
k
neval j=1
(j)

(j)

where pa and pk are progress metrics (gates passed) for the active agent and opponent k respectively in evaluation
episode j, and I(·) is the indicator function. This mechanism ensures that πθa continually strives against increasingly
competent versions of itself. Specific evaluation frequencies (Teval in Alg. 1) and the win rate threshold (τ ) are
hyperparameters detailed in our code repository.
Algorithm 1 Iterative Self-Play Training within Curriculum
1: Initialize active policy πθa and opponent policies {πϕi }n−1
i=1 with identical parameters
2: ϕi ← θa for i ∈ {1, 2, ..., n − 1}
▷ Initial sync
3: for each curriculum stage k = 1 to K do
4:
Set environment parameters Θk from Table 2
5:
for timestep t = 1 to Nk do
▷ Nk : budget per stage
6:
Collect experience tuple (st , at , rt , st+1 ) using πθa against frozen {πϕi }
7:
Update πθa using PPO and collected experience
8:
if t (mod Teval ) = 0 then
▷ Periodic evaluation
9:
Evaluate πθa against current {πϕi } over M episodes
10:
Calculate win rate w based on pa (Eq. 20)
11:
if w ≥ τ then
▷ Check mastery threshold
12:
Update opponent policies: ϕi ← θa for all i
▷ Sync opponents
13:
end if
14:
end if
15:
end for
16: end for

Self-Play Dynamics This iterative process inherently handles the non-stationarity of evolving opponents. Self-play,
operating within each curriculum stage, creates a synergistic learning dynamic: agents adapt to increasingly harder tasks
(via curriculum) while simultaneously improving against more competent opponents (via self-play). This is crucial for
developing robust and competitive behaviors.
6

Drone 0

Drone 1

Drone 2

Drone 3

3

4
5

2

−10

1

z [m]

4

0

2

5

0

−10

−5

x [m

]

−5

10

0

5

y [m]

10

Figure 2: Representative trajectories of four CRUISE-trained drones navigating the Ring Track. The track features
five gates (black rectangles) in a circular layout with alternating heights. Distinct colors show individual drone paths,
illustrating coordinated high-speed navigation.

4

Results and Discussion

This section empirically evaluates CRUISE, detailing the experimental setup, metrics, baselines, quantitative results,
and an ablation study on the curriculum’s impact.
4.1

Experimental Setup

CRUISE was evaluated with N = {2, 3, 4} drones in a custom Gymnasium multi-agent environment with realistic
quadrotor dynamics (Sec. 3). Two tracks tested different capabilities: a Ring Track for basic 3D maneuvering and
speed (Fig. 2), and a more complex Figure-Eight Track with intersections for coordination (Fig. 3). Each configuration
ran for 100 episodes with randomized starts for robustness.
4.2

Performance Metrics

Performance was quantified by three primary metrics (averaged over successful episodes): Mean Velocity (m/s)
indicating agility, Lap Completion Time (s), and Success Rate (%) measuring collision-free completion.
4.3

Baseline Methods

CRUISE is compared against two baselines:
7

Drone 0

Drone 1

Drone 2

Drone 3

4

5
6

1

z [m]

3
2
1
0

2

−10

3
−5
0

−5

y[
m]

]

m
x[

5

0
5

10

Figure 3: Representative trajectories of four CRUISE-trained drones on the more complex Figure-Eight Track. The six
gates form intersecting loops, requiring intricate coordination at crossing points.
• SE-IBR: To benchmark our framework against the state of the art in competitive MARL, we compare it against
Sensitivity Enhanced Iterative Best Responsee (SE-IBR) Wang et al. (2020). While methods like MAPPO are
standard for cooperative tasks, our zero-sum drone racing environment is fundamentally competitive. SE-IBR,
a game-theoretic planner, is therefore a more theoretically appropriate and challenging benchmark. As no
public code was available, we re-implemented the method based on its described iterative best response (IBR)
mechanism and opponent trajectory inference for 3D racing.
• VANILLA: To isolate and validate the contribution of our curriculum, we use VANILLA baseline. This agent
uses the same PPO algorithm and final reward function as CRUISE, but is trained from scratch only on the
final Stage 5 task (see Table 2), without any of the intermediate curriculum stages. This baseline therefore
serves as a direct and intuitive ablation of the curriculum structure, allowing us to measure the performance of
an agent that does not benefit from the guided learning sequence.
4.4

Quantitative Evaluation and Comparison

Tables 3 and 4 present the quantitative comparison, showing CRUISE’s superior performance across both tracks and
drone densities.
On the Ring Track (Table 3), CRUISE achieved superior mean velocities (4.3 m/s to 4.4 m/s) and lap times (13.3 s to
13.6 s) over the slower SE-IBR (≈2.3 m/s) and VANILLA (2.8 m/s for N=2). CRUISE maintained high success rates
(91 % to 100 %), while SE-IBR’s safety dropped at N=4 (50 %) and VANILLA failed for N ≥ 3.
On the more challenging Figure-Eight Track (Table 4), CRUISE again demonstrated superior speed (mean velocity
3.4 m/s to 3.5 m/s, lap times 16.1 s to 16.7 s) over SE-IBR (1.6 m/s to 1.8 m/s) and VANILLA (1.8 m/s to 2.1 m/s).
8

CRUISE also achieved high, consistent success rates (97 % to 98 %), while VANILLA’s dropped sharply with density.
SE-IBR maintained high safety (90 % to 100 %) but at significantly lower speeds than CRUISE. These results show
CRUISE robustly balances speed and safety in complex coordination scenarios.
Table 3: Performance comparison on the Ring Track. Values are mean ± std dev over 100 episodes.
Method
2 drones
CRUISE
SE-IBR
VANILLA
3 drones
CRUISE
SE-IBR
VANILLA
4 drones
CRUISE
SE-IBR
VANILLA

Lap Time
(s)

Velocity
(m/s)

Success
Rate (%)

13.3 ± 0.1
26.0 ± 1.0
20.9 ± 0.0

4.4 ± 0.0
2.3 ± 0.1
2.8 ± 0.0

100
100
50

13.4 ± 0.2
26.2 ± 0.7
N/A

4.4 ± 0.1
2.2 ± 0.1
N/A

91.3
90
0

13.6 ± 0.5
26.1 ± 0.8
N/A

4.3 ± 0.2
2.3 ± 0.1
N/A

93.5
50
0

Table 4: Performance comparison on the Figure-Eight Track. Values are mean ± std dev over 100 episodes.
Method
2 drones
CRUISE
SE-IBR
VANILLA
3 drones
CRUISE
SE-IBR
VANILLA
4 drones
CRUISE
SE-IBR
VANILLA

4.5

Lap Time
(s)

Velocity
(m/s)

Success
Rate (%)

16.1 ± 0.3
30.9 ± 0.8
27.4 ± 0.1

3.5 ± 0.1
1.8 ± 0.1
2.1 ± 0.0

98
90
100

16.2 ± 1.7
34.0 ± 3.7
28.3 ± 0.2

3.5 ± 0.4
1.7 ± 0.2
2.0 ± 0.0

98
90
43.3

16.7 ± 0.7
36.6 ± 6.4
31.1 ± 0.9

3.4 ± 0.1
1.6 ± 0.3
1.8 ± 0.1

97
100
30

Ablation Study: Impact of Curriculum Learning

The curriculum’s contribution was assessed by measuring mean velocity after each training stage. For all tracks and
drone counts (N = 2, 3, 4), velocity consistently increased with curriculum progression (Figs. 4 and 5), confirming that
the curriculum effectively scaffolds skill acquisition for high-speed flight. The VANILLA baseline’s poor performance
(Tables 3 and 4) further highlights the curriculum’s necessity.
4.6

Discussion

The experimental results validate CRUISE’s core hypotheses: that a structured curriculum is essential for sample
efficiency and that this foundation enables scalable learning of competitive strategies through self-play.
The curriculum is the key to solving the exploration problem. This is most clearly demonstrated by the VANILLA
baseline, which serves as a direct ablation of the curriculum. Its complete failure to converge in scenarios with three or
more agents (Tables 3, 4) underscores the severity of the exploration challenge in this domain. By breaking the problem
down into manageable stages, CRUISE ensures the agent first masters fundamental flight control before tackling the
complexities of high-speed, multi-agent interaction. This guided process makes efficient use of the sample budget,
leading to continuous and effective learning where a direct approach fails.
CRUISE enables the emergence of robust, non-greedy strategies. A crucial insight is that the final policies do not simply
maximize velocity. Instead, agents converge to effective racing speeds (e.g., ≈4.4 m/s on Ring) that are well below the
maximum reference velocity (10 m/s) available in the final curriculum stage. This demonstrates that the framework
successfully balanced competing reward terms, fostering a sophisticated policy that prioritizes consistency and collision
avoidance over raw speed, leading to higher success rates.
9

Effect of Curriculum Stage on Mean Velocity
Ring Track
2 Drones
3 Drones
4 Drones

4.5

3.5

Ring Track
(Length: 58.9m)
2

10

3
5

3.0
Y (m)

Mean Velocity (m/s)

4.0

1

0
−5

2.5

4
−10

5
−10

−5

0

5

10

X (m)

2.0
1

2

3

4

5

Curriculum Stage
Figure 4: Ablation study on the Ring Track: Mean velocity (m/s) vs. curriculum stage for N = 2, 3, 4 drones.
Error bars indicate std dev (100 trials). Inset shows track layout. Increasing velocity across stages demonstrates the
curriculum’s benefit.
The synergy between the curriculum and self-play stabilizes multi-agent learning. Our simplified iterative self-play
mechanism, which uses frozen opponents, proves highly effective. We posit this is because the single-agent curriculum
provides such a strong policy initialization. Agents enter the self-play phase as already-competent pilots, allowing the
learning to focus on refining opponent interaction rather than struggling with basic flight. This strong prior mitigates
the non-stationarity problem inherent in MARL and allows for a stable, scalable training process. Furthermore, the
progressively additive nature of the curriculum, where skills are built upon rather than replaced, inherently mitigates
catastrophic forgetting, ensuring foundational competencies are retained and strengthened throughout training.
4.7

Limitations and Broader Context

While CRUISE demonstrates a robust and scalable training methodology, we identify three key areas for future work.
First, the sim-to-real gap remains the primary hurdle. This work serves as a foundational study in simulation. Deploying
these high-speed, interactive policies on physical hardware will require addressing challenges such as unmodeled
dynamics, sensor noise, and communication latency. A systematic investigation into domain randomization and robust
controller design for sim-to-real transfer is the most critical next step.
Second, the reward function is hand-crafted. While effective, the design of the reward components and their respective
weights required significant domain expertise and empirical tuning. This is a common challenge in applying RL to
complex robotics tasks. Future work could explore automating this process through techniques like inverse reinforcement
learning (IRL) from expert demonstrations or population-based methods that co-evolve reward functions alongside
policies.
10

Effect of Curriculum Stage on Mean Velocity
Figure Eight Track

4.0

2 Drones
3 Drones
4 Drones

3.5

2.5
Figure Eight Track
(Length: 56.8m)

2.0

6

5.0

1.5

3

2.5

Y (m)

Mean Velocity (m/s)

3.0

0.0

5

2

−2.5

1.0

4

−5.0
−10

0.5

1

−5

0

5

10

X (m)

1

2

3

4

5

Curriculum Stage
Figure 5: Ablation study on the Figure-Eight Track: Mean velocity (m/s) vs. curriculum stage for N = 2, 3, 4 drones.
Error bars show std dev (100 trials). Inset shows track layout. Performance increases consistently with curriculum
stage.

Finally, the self-play mechanism can be extended. Our iterative approach with a single updating policy is efficient and
stable but represents one point in a wide spectrum of self-play strategies. Exploring more advanced schemes, such as
training against a diverse league of past policies or using population-based training, could potentially lead to more
generalizable and unpredictable strategies, further pushing the boundaries of competitive multi-agent performance.

5

Conclusion

This paper presented CRUISE, a framework that successfully addresses the critical challenges of sample efficiency
and scalability in competitive multi-drone racing. We demonstrated that a structured curriculum is not merely an
enhancement but a necessity for solving the severe exploration problem inherent to this complex domain, a finding
validated by our ablation studies. This curriculum provides a crucial foundation of single-agent competency, upon
which an efficient iterative self-play mechanism builds emergent, coordinated strategies for crowded environments.
The resulting policies significantly outperform standard RL and game-theoretic baselines, exhibiting a robust balance
between speed and safety. Ultimately, CRUISE provides a blueprint for structuring the learning process itself to unlock
complex, emergent behaviors in challenging multi-agent robotic systems. To facilitate further research, our full source
code and pretrained models are publicly available at https://doi.org/10.5281/zenodo.17256943.
11

6

Future Work

Building on this foundation, our future work will proceed along three primary axes. First and foremost is bridging
the sim-to-real gap, which involves deploying and validating these policies on physical quadrotors while addressing
challenges like dynamics mismatch and sensor noise. Second, we aim to reduce the reliance on hand-crafted rewards by
exploring techniques like inverse reinforcement learning to automate the reward engineering process. Finally, we will
investigate more advanced self-play schemes, such as training against a diverse league of past policies, to foster even
more generalizable and unpredictable competitive strategies.

References
Di, J., Chen, S., Li, P., Wang, X., Ji, H., Kang, Y., 2023. A cooperative-competitive strategy for autonomous multidrone
racing. IEEE Transactions on Industrial Electronics .
Foehn, P., Brescianini, D., Kaufmann, E., Cieslewski, T., Gehrig, M., Muglikar, M., Scaramuzza, D., 2022. Alphapilot:
Autonomous drone racing. Autonomous Robots 46, 307–320.
Guerra, W., Tal, E., Murali, V., Ryou, G., Karaman, S., 2019. Flightgoggles: Photorealistic sensor simulation for
perception-driven robotics using photogrammetry and virtual reality, in: 2019 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), IEEE. pp. 6941–6948.
Guo, Y., Xu, D., Wang, C., Li, J., Long, H., 2025. An invulnerable leader–follower collision-free unmanned aerial
vehicle flocking system with attention-based multi-agent reinforcement learning. Engineering Applications of
Artificial Intelligence 160, 111797.
Huynh, N.M., Cao, H.G., Wu, I., et al., 2024. Multi-agent training for pommerman: Curriculum learning and
population-based self-play approach. arXiv preprint arXiv:2407.00662 .
Jiang, Z., Zhang, C., Shi, Z., Song, G., 2025. Graph diffusion network for multi-agent reinforcement learning in drone swarm exploration. Engineering Applications of Artificial Intelligence 158, 111322. URL:
https://www.sciencedirect.com/science/article/pii/S0952197625013247, doi:https://doi.org/
10.1016/j.engappai.2025.111322.
Kaufmann, E., Bauersfeld, L., Loquercio, A., Müller, M., Koltun, V., Scaramuzza, D., 2023. Champion-level drone
racing using deep reinforcement learning. Nature 620, 982–987.
Kaufmann, E., Loquercio, A., Ranftl, R., Dosovitskiy, A., Koltun, V., Scaramuzza, D., 2018. Deep drone racing:
Learning agile flight in dynamic environments, in: Conference on Robot Learning, PMLR. pp. 133–145.
Kim, H., Choi, J., Do, H., Lee, G.T., 2024. A fully controllable uav using curriculum learning and goal-conditioned
reinforcement learning: From straight forward to round trip missions. Drones 9, 26.
Lin, F., Huang, S., Pearce, T., Chen, W., Tu, W.W., 2023. Tizero: Mastering multi-agent football with curriculum
learning and self-play. arXiv preprint arXiv:2302.07515 .
Loquercio, A., Kaufmann, E., Ranftl, R., Dosovitskiy, A., Koltun, V., Scaramuzza, D., 2019. Deep drone racing: From
simulation to reality with domain randomization. IEEE Transactions on Robotics 36, 1–14.
Messikommer, N., Song, Y., Scaramuzza, D., 2024. Contrastive initial state buffer for reinforcement learning, in: 2024
IEEE International Conference on Robotics and Automation (ICRA), IEEE. pp. 2866–2872.
Moon, H., Martinez-Carranza, J., Cieslewski, T., Faessler, M., Falanga, D., Simovic, A., Scaramuzza, D., Li, S.,
Ozo, M., De Wagter, C., et al., 2019. Challenges and implemented technologies used in autonomous drone racing.
Intelligent Service Robotics 12, 137–148.
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N., 2021. Stable-baselines3: Reliable
reinforcement learning implementations. Journal of Machine Learning Research 22, 1–8. URL: http://jmlr.org/
papers/v22/20-1364.html.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., 2017. Proximal policy optimization algorithms. arXiv
preprint arXiv:1707.06347 .
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton,
A., et al., 2017. Mastering the game of go without human knowledge. nature 550, 354–359.
Song, Y., Naji, S., Kaufmann, E., Loquercio, A., Scaramuzza, D., 2021. Flightmare: A flexible quadrotor simulator, in:
Conference on Robot Learning, PMLR. pp. 1147–1157.
Song, Y., Romero, A., Müller, M., Koltun, V., Scaramuzza, D., 2023. Reaching the limit in autonomous racing: Optimal
control versus reinforcement learning. Science Robotics 8, eadg1462.
12

Spica, R., Cristofalo, E., Wang, Z., Montijano, E., Schwager, M., 2020. A real-time game theoretic planner for
autonomous two-player drone racing. IEEE Transactions on Robotics 36, 1389–1403.
Stolfi, D.H., Danoy, G., 2024. Evolutionary swarm formation: From simulations to real world robots. Engineering
Applications of Artificial Intelligence 128, 107501.
Wang, H., Li, J., Tao, H., Liu, J., Li, C., Wang, K., Xu, M., 2025a. Autonomous dynamic formation for maritime
target tracking using multi-agent reinforcement learning. Engineering Applications of Artificial Intelligence 154,
110904. URL: https://www.sciencedirect.com/science/article/pii/S0952197625009042, doi:https:
//doi.org/10.1016/j.engappai.2025.110904.
Wang, X., Zhou, J., Feng, Y., Mei, J., Chen, J., Li, S., 2025b. Dashing for the golden snitch: Multi-drone time-optimal
motion planning with multi-agent reinforcement learning, in: 2025 IEEE International Conference on Robotics and
Automation (ICRA), IEEE. pp. 16692–16698.
Wang, Z., Taubner, T., Schwager, M., 2020. Multi-agent sensitivity enhanced iterative best response: A real-time game
theoretic planner for drone racing in 3d environments. Robotics and Autonomous Systems 125, 103410.
Xing, J., Geles, I., Song, Y., Aljalbout, E., Scaramuzza, D., 2024. Multi-task reinforcement learning for quadrotors.
IEEE Robotics and Automation Letters .
Xing, J., Wang, H., Messikommer, N., Scaramuzza, D., 2025. Environment as policy: Generative curriculum learning
for autonomous racing, in: 7th Robot Learning Workshop: Towards Robots with Human-Level Abilities. URL:
https://openreview.net/forum?id=UyZhgI4wLa.
Xu, J., Hu, P., Alrifaee, B., 2024. Sigmarl: A sample-efficient and generalizable multi-agent reinforcement learning
framework for motion planning, in: 2024 IEEE 27th International Conference on Intelligent Transportation Systems
(ITSC), IEEE. pp. 768–775.

13

