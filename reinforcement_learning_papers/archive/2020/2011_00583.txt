arXiv:2011.00583v5 [cs.MA] 13 Aug 2025

Game-Theoretic Multiagent
Reinforcement Learning
Suggested Citation: Yaodong Yang 1,∗,† , Chengdong Ma 2,∗ , Zihan Ding, Stephen McAleer,
Chi Jin, Jun Wang and Tuomas Sandholm (2025), « Game-Theoretic Multiagent Reinforcement Learning », : Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.

Yaodong Yang
Peking University, China
yaodong.yang@pku.edu.cn
Chengdong Ma
Peking University, China
chengdong.ma@stu.pku.edu.cn
Zihan Ding
Princeton University, USA
zihand@princeton.edu
Stephen McAleer
Carnegie Mellon University, USA
smcaleer@cs.cmu.edu
Chi Jin
Princeton University, USA
chij@princeton.edu
Jun Wang
University College London, UK
jun.wang@cs.ucl.ac.uk
Tuomas Sandholm
Carnegie Mellon University, USA
sandholm@cs.cmu.edu

Table des matières

1 Introduction
1.1 Deep Reinforcement Learning . . . . . . . . . . . . . . . .
1.2 Multiagent Reinforcement Learning . . . . . . . . . . . . . .

3

3
6

2 Single-Agent RL
11
2.1 Problem Formulation: Markov Decision Process . . . . . . . 11
2.2 Justification of Reward Maximisation . . . . . . . . . . . . . 13
2.3 Solving Markov Decision Processes . . . . . . . . . . . . . 13
3 Multi-Agent RL
17
3.1 Stochastic Game . . . . . . . . . . . . . . . . . . . . . . . 17
3.2 Extensive-Form Game . . . . . . . . . . . . . . . . . . . . . 31
4 Grand Challenges of MARL
4.1 Combinatorial Complexity . . . . . . . . . . . . . . . . . . .
4.2 Multi-Dimensional Learning Objectives . . . . . . . . . . . .
4.3 The Challenges of Non-Stationarity and Adaptive Opponents
4.4 Scalability Issue when N ≫ 2 . . . . . . . . . . . . . . . .

45

45
46
48
49

5 A Survey of MARL Surveys
51
5.1 Taxonomy of MARL Algorithms . . . . . . . . . . . . . . . . 51
5.2 A Survey of Surveys . . . . . . . . . . . . . . . . . . . . . . 56

6 Learning in Identical-Interest Games
59
6.1 Stochastic Team Games . . . . . . . . . . . . . . . . . . . 59
6.2 Dec-POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . 65
6.3 Networked Multiagent MDPs . . . . . . . . . . . . . . . . . 66
6.4 Stochastic Potential Games . . . . . . . . . . . . . . . . . . 69
7 Learning in Zero-Sum Games
71
7.1 Minimax Optimization . . . . . . . . . . . . . . . . . . . . . 72
7.2 Discrete-Action Normal-Form Games . . . . . . . . . . . . . 77
7.3 Continuous-Action Normal-Form Games . . . . . . . . . . . 80
7.4 Stochastic Games . . . . . . . . . . . . . . . . . . . . . . . 89
7.5 Extensive-Form Games . . . . . . . . . . . . . . . . . . . . 98
7.6 Online Markov Decision Processes . . . . . . . . . . . . . . 117
7.7 Team Games . . . . . . . . . . . . . . . . . . . . . . . . . 120
8 Learning in General-Sum Games
122
8.1 Solutions by Mathematical Programming . . . . . . . . . . . 122
8.2 Solutions by Value-Based Methods . . . . . . . . . . . . . . 125
8.3 Solutions by Two-Timescale Analysis . . . . . . . . . . . . . 127
8.4 Solutions by Policy-Based Methods . . . . . . . . . . . . . . 128
9 Learning in Games when N → +∞
131
9.1 Non-cooperative Mean-Field Game . . . . . . . . . . . . . . 133
9.2 Cooperative Mean-Field Control . . . . . . . . . . . . . . . 137
9.3 Mean-Field MARL . . . . . . . . . . . . . . . . . . . . . . . 139
10 Future Directions

143

Bibliography

148

Game-Theoretic Multiagent
Reinforcement Learning
Yaodong Yang 1,∗,† , Chengdong Ma 2,∗ , Zihan Ding3 , Stephen McAleer4 ,
Chi Jin5 , Jun Wang6 and Tuomas Sandholm7
1 Institute for Artificial Intelligence, Peking University, Beĳing, China,

yaodong.yang@pku.edu.cn
2 Institute for Artificial Intelligence, Peking University, Beĳing, China,
chengdong.ma@stu.pku.edu.cn
3 Department of Electrical and Computer Engineering, Princeton University,
Princeton, NJ, USA, zihand@princeton.edu
4 Carnegie Mellon University, PA, USA, smcaleer@cs.cmu.edu
5 Department of Electrical and Computer Engineering, Princeton University,
Princeton, NJ, USA, chĳ@princeton.edu
6 UCL Centre for Artificial Intelligence, University College London, UK,
jun.wang@cs.ucl.ac.uk
7 Carnegie Mellon University, PA, USA, sandholm@cs.cmu.edu

ABSTRACT
Tremendous advances have been made in multiagent reinforcement
learning (MARL). MARL corresponds to the learning problem
in a multiagent system in which multiple agents learn simultaneously. It is an interdisciplinary field of study with a long history
that includes game theory, machine learning, stochastic control,
psychology, and optimization. Despite great successes in MARL,
there is a lack of a self-contained overview of the literature that
covers game-theoretic foundations of modern MARL methods and
summarizes the recent advances. The majority of existing surveys
are outdated and do not fully cover the recent developments since
2010. In this work, we provide a monograph on MARL that
Yaodong Yang 1,∗,† , Chengdong Ma 2,∗ , Zihan Ding, Stephen McAleer, Chi Jin, Jun Wang and
Tuomas Sandholm (2025), « Game-Theoretic Multiagent Reinforcement Learning », : Vol. xx,
No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.
©2025 Yaodong Yang et al. * Equal contribution. † Corresponding author.

2

covers both the fundamentals and the latest developments on the
research frontier. The goal of this monograph is to provide a
self-contained assessment of the current state-of-the-art MARL
techniques from a game-theoretic perspective. We expect this
work to serve as a stepping stone for both new researchers who
are about to enter this fast-growing field and experts in the field
who want to obtain a panoramic view and identify new directions
based on recent advances.

1
Introduction

1.1

Deep Reinforcement Learning

Deep Reinforcement Learning (RL) is a subfield of artificial intelligence that
combines deep learningLeCun et al. (2015) and reinforcement learningSutton
and Barto (1998), enabling machines to make decisions and solve problems.
It has seen significant advancements in recent years and has been applied to
various domains including video gamesMnih et al. (2015); Silver et al. (2016,
2017a, 2018), roboticsKober et al. (2013); Akkaya et al. (2019), financeDeng
et al. (2016); Liu et al. (2018), energyDegrave et al. (2022), transportationWei
et al. (2018); Haydari and Yılmaz (2020), etc. In RL, an agent learns to
maximize the expected cumulative reward over time by interacting with an
environment and receiving feedback in the form of rewards.
A Car Driving Example.

To illustrate the key components of decisionmaking process in RL, let us consider the real-world example of controlling
a car to drive safely through an intersection. At each time step, a robot car
can move by steering, accelerating, and braking. The goal is to safely exit
the intersection and reach the destination (with possible decisions of going
straight or turning left/right into another lane). Therefore, in addition to being
able to detect objects, such as traffic lights, lane markings, and other cars (by
3

Introduction

4
Many agents

Agent

…

…
Action

State,
Reward

State,
Reward

Action

State,
Reward
Action

Action

Environment

State,
Reward

Environment

Figure 1.1 – Diagram of a single-agent MDP (left) and a multiagent MDP (right).

converting data to knowledge), we aim to find a steering policy that can control
the car to make a sequence of maneuvers to achieve the goal (making decisions
based on the knowledge gained). In a decision-making setting such as this,
two additional challenges arise:
1. First, during the decision-making process, at each time step, the robot
car should consider not only the immediate value of its current action but
also the consequences of its current action in the future. For example,
in the case of driving through an intersection, it would be detrimental to
have a policy that chooses to steer in a “safe” direction at the beginning
of the process if it would eventually lead to a car crash later on.
2. Second, to make each decision correctly and safely, the car must also
consider other cars’ behavior and act accordingly. Human drivers, for
example, often predict in advance other cars’ movements and then take
strategic moves in response (like giving way to an oncoming car or
accelerating to merge into another lane).
The need for an adaptive decision-making framework, together with
the complexity of addressing multiple interacting learners, has led to the
development of multiagent RL (Fig. 1.1), which will be discussed later 1.2.
A Short History of RL.

RL is a sub-field of machine learning, where agents
learn how to behave optimally based on a trial-and-error procedure during
their interaction with the environment. Unlike supervised learning, which

1.1. Deep Reinforcement Learning

5

takes labeled data as the input (for example, an image labeled with cats), RL is
goal-oriented: it constructs a learning model that learns to achieve the optimal
long-term goal by improvement through trial and error, with the learner having
no labeled data to obtain knowledge from. The word “reinforcement” refers to
the learning mechanism since the actions that lead to satisfactory outcomes
are reinforced in the learner’s set of behaviours.
Historically, the RL mechanism was originally developed based on studying
cats’ behaviour in a puzzle box Thorndike (1898). Minsky (1954) first proposed
the computational model of RL in his Ph.D. thesis and named his resulting
analog machine the stochastic neural-analog reinforcement calculator. Several
years later, he first suggested the connection between dynamic programming
Bellman (1952) and RL Minsky (1961). In 1972, Klopf (1972) integrated
the trial-and-error learning process with the finding of temporal difference
(TD) learning from psychology. TD learning quickly became indispensable
in scaling RL for larger systems. Based on dynamic programming and TD
learning, Watkins and Dayan (1992) laid the foundations for present-day RL
using the Markov decision process (MDP) and proposing the famous Q-learning
method as the solver. As a dynamic programming method, the original Qlearning process inherits Bellman’s “curse of dimensionality” Bellman (1952),
which strongly limits its applications when the number of state variables is
large. To overcome such a bottleneck, Bertsekas and Tsitsiklis (1996) proposed
approximate dynamic programming methods based on neural networks. More
recently, Mnih et al. (2015) from DeepMind made a significant breakthrough
by introducing the deep Q-learning (DQN) architecture, which leverages
the representation power of DNNs for approximate dynamic programming
methods. DQN has demonstrated human-level performance on 49 Atari games.
Since then, deep RL techniques have become common in machine learning/AI
and have attracted considerable attention from the research community.
RL originates from an understanding of animal behaviour where animals
use trial-and-error to reinforce beneficial behaviours, which they then perform
more frequently. During its development, computational RL incorporated
ideas such as optimal control theory and other findings from psychology that
help mimic the way humans make decisions to maximise the long-term profit
of decision-making tasks. As a result, RL methods can naturally be used to
train a computer program (an agent) to a performance level comparable to that
of a human on certain tasks. The earliest success of RL methods against human

Introduction

6

players can be traced back to the game of backgammon Tesauro (1995). More
recently, the advancement of applying RL to solve sequential decision-making
problems was marked by the remarkable success of the AlphaGo series Silver
et al. (2016, 2017a, 2018), a self-taught RL agent that beats top professional
players of the game Go, a game whose search space (10761 possible games) is
even greater than the number of atoms in the universe 1.
In fact, the majority of successful RL applications, such as those for the
game Go 2 and StrategoPerolat et al. (2022), robotic control Kober et al. (2013),
and autonomous driving Shalev-Shwartz et al. (2016), naturally involve the
participation of multiple AI agents, which probe into the realm of multiagent
RL.
1.2

Multiagent Reinforcement Learning

A Short History of MARL.

The development of MARL from the 1980s to
the early 2000s laid the foundation for modern methodologies, primarily driven
by the extension of RL techniques to multi-agent settings. Early works were
strongly rooted in game theory, particularly stochastic games, which extended
Markov decision processes (MDPs) to settings involving multiple interacting
agents. This shift established the groundwork for MARL by introducing key
concepts such as equilibrium-based strategies, coordination, and scalability. A
significant milestone in this period was the introduction of Minimax Q-learning
by Littman Littman (1994), which formalized the application of Q-learning to
zero-sum games, offering a way to model competitive interactions between
agents. Hu and Wellman Hu et al. (1998); Hu and Wellman (2003) expanded
this framework to Nash Q-learning, a general-sum extension that accounted for
both competitive and cooperative agents by incorporating Nash equilibria as the
solution concept. This approach, however, faced challenges, particularly with
convergence in environments with multiple equilibria, which necessitated the
development of advanced solution techniques. In parallel, joint-action learning
frameworks like those proposed by Claus and Boutilier Claus and Boutilier
1. There are an estimated 1082 atoms in the universe. If one had one trillion computers,
each processing one trillion states per second for one trillion years, one could only reach 1043
states.
2. AlphaGo can also be treated as a multiagent technique if we consider the opponent in
self-play as another agent.

1.2. Multiagent Reinforcement Learning

7

(1998a) focused on cooperation in multi-agent environments, advancing
methods such as joint-action value functions and optimistic exploration for
coordination in shared-reward settings. These frameworks addressed the
need for agents to effectively collaborate while managing the complexities of
dynamic environments. The period also saw key algorithmic advancements
aimed at tackling the distinct challenges of non-stationarity and scalability.
The problem of non-stationarity, where the environment continually changes
due to simultaneous policy learning by multiple agents, was addressed by
methods such as WoLF (Win or Learn Fast) Policy Hillclimbing by Bowling
and Veloso Bowling and Veloso (2001), which adapted learning rates to balance
rationality and convergence in mixed cooperative-competitive environments.
Additionally, strategies like Brownian bandits and Replicator Dynamics Tuyls
et al. (2003) were introduced to explore dynamic multi-agent interactions and
their effects on learning in non-stationary environments. Scalability remained
a pressing challenge, as the joint-action space in MARL grew exponentially
with the number of agents, making the use of full state-action tables impractical.
Sparse Q-learning provided a partial solution by reducing the complexity of
value function representations Kok and Vlassis (2004).
Despite these advances, several core challenges persisted. Non-stationarity
remained a significant hurdle due to the continuously changing policies of
other agents, often requiring adaptations such as opponent modeling Hu
(1999) and recency-based exploration Ballard and Zhu (2002). Moreover, the
presence of multiple Nash equilibria in general-sum games further complicated
convergence, requiring sophisticated coordination strategies among agents
to select appropriate equilibria. Key works from this era, such as Minimax
Q-learning (Littman, 1994) Littman (1994), Nash Q-learning Hu et al. (1998);
Hu and Wellman (2003), and joint-action RL Claus and Boutilier (1998a),
played pivotal roles in shaping the field. These foundational contributions
provided the theoretical groundwork for MARL, highlighting its potential
in both competitive and cooperative scenarios, while also identifying the
persistent challenges that would drive subsequent research. Early empirical
evaluations, conducted in controlled environments such as grid-world, predatorprey scenarios, and cooperative tasks, demonstrated the practical strengths
of these techniques but also revealed the theoretical gaps in convergence and
scalability. These foundational works paved the way for future advancements,
as they not only contributed to the theoretical development of MARL but also

8

Introduction

highlighted persistent issues that continue to shape the direction of research in
the field.
Multiagent Reinforcement Learning (MARL) extends the concept of singleagent RL to situations where multiple agents are present and interact with each
other and their environment. In these scenarios, each agent must take into
account the actions of other agents when making decisions, making MARL
a challenging area of research. The goal in MARL is for each agent to learn
its own policy while adapting to the changing policies of other agents. This
requires the development of algorithms that can handle the complexity and
non-stationarity of multiagent systems. MARL has been applied in various
fields such as autonomous robots, networked systems, and cyber-physical
systems, and has seen successful results in cooperative navigation, formation
control, traffic control, and energy management.
When designing MARL algorithms, it is critical to consider equilibria,
which are the outcomes of interactions between agents that are stable, meaning
no single agent has the incentive to change its behaviour. The mathematical
concept of equilibria traces back to the origin of game theory, which is
widely considered to start from the founding book “Theory of Games and
Economic behaviour” in 1944, by a mathematician and physicist named John
von Neumann and economist Oskar Morgenstern. Their approach to game
theory involved defining games as mathematical models consisting of players,
actions, and payoffs. The concept of mixed strategies was introduced, which
allows players to randomize their actions, and the minimax theorem, shows that
in any two-player zero-sum game, there is always a strategy that minimizes
the maximum possible loss. Another famous concept, Nash equilibrium, was
introduced by John Nash in his 1950 paper “Non-Cooperative Games”, for
which he was later awarded the Nobel Prize in Economics. Nash developed
the concept as a generalization of the classical equilibrium concept used in
economics, which is based on the assumption of perfect competition and
rational behaviour. A Nash equilibrium is a set of strategies where no player
can improve their outcome by changing their strategy, assuming that the other
players’ strategies remain the same. It serves as a fundamental concept in game
theory and is widely used to analyze a wide range of multiagent interaction
scenarios, boosting the development of theoretical game theory. Notably, in
some cases, the focus of study shifts from player strategies to programs. For
example, one approach to achieving cooperation in the one-shot Prisoner’s

1.2. Multiagent Reinforcement Learning

9

Dilemma is program equilibrium Tennenholtz (2004), where players submit
programs instead of strategies. These programs are then allowed to read each
other’s source code in order to decide on their actions. As a mathematical
framework for analyzing interactions between agents, game theory provides a
solution to this challenge by allowing us to develop MARL algorithms with
theoretical equilibrium properties and convergence guarantees. For example,
game theoretic algorithms for two-player zero-sum games have provable
convergence guarantees and have been shown to achieve astonishing practical
achievements, like beating top human players in various games such as GoSilver
et al. (2017b), pokerBrown and Sandholm (2018), and StrategoPerolat et al.
(2022). Multiplayer video games such as StarCraft IIVinyals et al. (2019c) and
Dota2Berner et al. (2019a) have also been tackled by MARL.
Recent Advances in MARL

One popular testbed of MARL is StarCraft
II Vinyals et al. (2017), a multi-player real-time strategy computer game that
has its own professional league. In this game, each player has only limited
information about the game state, and the dimension of the search space is
orders of magnitude larger than that of go (1026 possible choices for every
move). The design of effective RL methods for StarCraft II was once believed to
be a long-term challenge for AI Vinyals et al. (2017). However, a breakthrough
was accomplished by AlphaStar in 2019 Vinyals et al. (2019b), which has
exhibited grandmaster-level skills by ranking above 99.8% of human players.
Another prominent video game-based test-bed for MARL is Dota2, a
zero-sum game played by two teams, each composed of five players. From
each agent’s perspective, in addition to the difficulty of incomplete information
(similar to StarCraft II), Dota2 is more challenging, in the sense that both
cooperation among team members and competition against the opponents must
be considered. The OpenAI Five AI system Pachocki et al. (2018) demonstrated
superhuman performance in Dota2 by defeating world champions in a public
e-sports competition.
In addition to StarCraft II and Dota2, Jaderberg et al. (2019) and Baker et al.
(2019a) showed human-level performance in capture-the-flag and hide-andseek games, respectively. Although the games themselves are less sophisticated
than either StarCraft II or Dota2, it is still non-trivial for AI agents to master
their tactics, so the agents’ impressive performance again demonstrates the
efficacy of MARL. Interestingly, both authors reported emergent behaviours

10

Introduction

induced by their proposed MARL methods that humans can understand and
are grounded in physical theory.
Another achievement of MARL worth mentioning is its application to
the poker game Texas hold’ em, which is a multi-player extensive-form game
with incomplete information accessible to the player. Heads-up (namely,
two-player) no-limit hold’em has more than 6 × 10161 information states. Only
recently have ground-breaking achievements in the game been made, thanks to
MARL. Two independent programs, DeepStack Moravcik et al. (2017) and
Libratus Brown and Sandholm (2018), are able to beat professional human
players. Even more recently, Libratus was upgraded to Pluribus Brown and
Sandholm (2019) and showed remarkable performance by winning over one
million dollars from five elite human professionals in a no-limit setting.
More recently, MARL techniques based on Neural Replicator DynamicsOmidshafiei et al. (2019) have achieved expert-level performance in the
very large game of StrategoPerolat et al. (2022). Also in 2022, Cicero achieved
human-level performance in the press version of Diplomacy by using gametheoretic MARL combined with large language models(FAIR)† et al. (2022).
MARL techniques have also been applied to large language models, where redteam and blue-team language models engage in a game-theoretic interaction to
enhance the security detection of large language models Ma et al. (2023) and
align them with human preferences Wang et al. (2024).
In conclusion, MARL requires a deep understanding of equilibria and
the interactions between agents. Game theory provides a powerful tool for
studying multiagent systems and developing MARL algorithms with nice
theoretical properties and convergence guarantees. By combining game theory
and deep RL, it is possible to scale MARL algorithms to complex multiagent
systems and achieve superhuman performance.
For a deeper understanding of RL and MARL, mathematical notation and
deconstruction of the concepts are needed. In the next section, we provide
mathematical formulations for these concepts, starting from single-agent RL
and progressing to multiagent RL methods.

2
Single-Agent RL

Through trial and error, an RL agent attempts to find the optimal policy
to maximise its long-term reward. This process is formulated by Markov
Decision Processes.
2.1

Problem Formulation: Markov Decision Process

Definition 2.1 (Markov Decision Process). An MDP can be described by a

tuple of key elements ⟨S, A, P, R, γ⟩.
— S: the set of environmental states.
— A: the set of agent’s possible actions.
— P : S × A → ∆(S): for each time step t ∈ N, given agent’s action
a ∈ A, the transition probability from a state s ∈ S to the state in the
next time step s′ ∈ S.
— R : S × A × S → R: the reward function that returns a scalar value to
the agent for a transition from s to s′ as a result of action a. The rewards
have absolute values uniformly bounded by Rmax .
— γ ∈ [0, 1] is the discount factor that represents the value of time.
11

Single-Agent RL

12

At each time step t, the environment has a state st . The learning agent
observes this state 1 and executes an action at . The action makes the environment transition into the next state st+1 ∼ P (·|st , at ), and the new environment
returns an immediate reward R(st , at , st+1 ) to the agent. The reward function can be also written as R : S × A → R, which is interchangeable with
R : S × A × S → R (see Van Otterlo and Wiering (2012), page 10). The goal
of the agent is to solve the MDP: to find the optimal policy that maximises the
reward over time. Mathematically, one common objective is for the agent to find
a Markovian (i.e., the input depends on only the current state) and stationary
(i.e., function form is time-independent) policy function 2 π : S → ∆(A), with
∆(·) denoting the probability simplex, which can guide it to take sequential
actions such that the discounted cumulative reward is maximised:
Est+1



X

γ t R (st , at , st+1 ) at ∼ π (· | st ) , s0  .
∼P (·|s ,a )
t

(2.1)

t

t≥0

Another common mathematical objective of an MDP is to maximise the
time-average reward:
"

lim Est+1 ∼P (·|st ,at )

T →∞

−1
1 TX
R(st , at , st+1 ) at ∼ π (· | st ) , s0 ,
T t=0

#

(2.2)

which we do not consider in this work and refer to Mahadevan (1996) for a
full analysis of the objective of time-average reward.
Based on the objective function of Eq. (2.1), under a given policy π, we
can define the state-action function (namely, the Q-function, which determines
the expected return from undertaking action a in state s) and the value function
(which determines the return associated with the policy in state s) as:


X
Qπ (s, a) = Eπ  γ t R (st , at , st+1 ) a0 = a, s0 = s , ∀s ∈ S, a ∈ A
t≥0

(2.3)


X
V π (s) = Eπ  γ t R (st , at , st+1 ) s0 = s , ∀s ∈ S

(2.4)

t≥0

1. The agent can only observe part of the full environment state. The partially observable
setting is introduced in Definition 3.6 as a special case of Dec-PODMP.
2. Such an optimal policy exists as long as the transition function and the reward function
are both Markovian and stationary Feinberg (2010).

2.2. Justification of Reward Maximisation

13

where Eπ is the expectation under the probability measure Pπ over the set
of infinitely long state-action trajectories τ = (s0 , a0 , s1 , a1 , ...) and where
Pπ is induced by state transition probability P , the policy π, the initial
state s and initial action a (in the case of the Q-function). The connection
between the Q-function and value function is V π (s) = Ea∼π(·|s) [Qπ (s, a)]
and Qπ = Es′ ∼P (·|s,a) [R(s, a, s′ ) + V π (s′ )].
2.2

Justification of Reward Maximisation

The current model for RL, as given by Eq. (2.1), suggests that the expected
value of a single reward function is sufficient for any problem we want our
“intelligent agents” to solve. The justification for this idea is deeply rooted
in the von Neumann-Morgenstern (VNM) utility theory Von Neumann and
Morgenstern (2007). This theory essentially proves that an agent is VNMrational if and only if there exists a real-valued utility (or, reward) function
such that every preference of the agent is characterised by maximising the
single expected reward. The VNM utility theorem is the basis for the wellknown expected utility theory Schoemaker (2013), which essentially states that
rationality can be modelled as maximising an expected value. Specifically,
the VNM utility theorem provides both necessary and sufficient conditions
under which the expected utility hypothesis holds. In other words, rationality
is equivalent to VNM-rationality, and it is safe to assume an intelligent entity
will always choose the action with the highest expected utility in any complex
scenario.
Admittedly, it was accepted long before that some of the assumptions on
rationality could be violated by real decision-makers in practice Gigerenzer
and Selten (2002). In fact, those conditions are rather taken as the “axioms”
of rational decision-making. In the case of the multi-objective MDP, we are
still able to convert multiple objectives into a single-objective MDP with the
help of a scalarisation function through a two-timescale process; we refer to
Roĳers et al. (2013) for more details.
2.3

Solving Markov Decision Processes

One commonly used notion in MDPs is the (discounted-normalised)
occupancy measure µπ (s, a), which uniquely corresponds to a given policy π

Single-Agent RL

14

and vice versa (Syed et al., 2008, Theorem 2), defined by


µπ (s, a) = Est ∼P,at ∼π (1 − γ)


X

γ t 1(st =s∧at =a)  .

t≥0

= (1 − γ)

X

γ t Pπ (st = s, at = a),

(2.5)

t≥0

where 1 is an indicator function. Note that in Eq. (2.5), P is the state transitional
probability, and Pπ is the probability of specific state-action pairs when
following stationary policy π. The physical meaning of µπ (s, a) is a probability
measure that counts the expected discounted number of visits to the individual
P
admissible state-action pairs. Correspondingly, µπ (s) = a µπ (s, a) is the
discounted state visitation frequency, i.e., the stationary distribution of the
Markov process induced by π. With the occupancy measure, we can write Eq.
1
(2.4) as an inner product of V π (s) = 1−γ
µπ (s, a), R(s, a) . This implies
that solving an MDP can be regarded as solving a linear program (LP) of
maxµ µ(s, a), R(s, a) , and the optimal policy is then
π ∗ (a|s) = µ∗ (s, a)/µ∗ (s)

(2.6)

However, this method is not practical in the case of a large-scale LP with
millions of variables Papadimitriou and Tsitsiklis (1987). When the state-action
space of an MDP is continuous, LP formulation cannot help solve either.
In the context of optimal control Bertsekas (2005), the classical dynamicprogramming approaches, such as policy iteration and value iteration, can also
be applied to solve for the optimal policy that maximises Eq. (2.3) and Eq.
(2.4), but these approaches require knowledge of the exact form of the model:
the transition function P (·|s, a), and the reward function R(s, a, s′ ).
On the other hand, in the setting of RL, the agent learns the optimal policy
by a trial-and-error process during its interaction with the environment rather
than using prior knowledge of the model. RL algorithms can be categorised
into two types: value-based methods and policy-based methods.
Value-Based Methods

For all MDPs with finite states and actions, there exists at least one
deterministic stationary optimal policy Szepesvári (2010); Sutton and Barto
(1998). Value-based methods are introduced to find the optimal Q-function Q∗

2.3. Solving Markov Decision Processes

15

that maximises Eq. (2.3). Correspondingly, the optimal policy can be derived
from the Q-function by taking the greedy action of π ∗ = arg maxa Q∗ (s, a).
The classic Q-learning algorithm Watkins and Dayan (1992) approximates Q∗
by Q̂ and updates its value via temporal-difference learning Sutton (1988).
temporal difference error

z

Q̂(st , at ) ← Q̂(st , at ) +
|

{z

}

new value

|

{z

}

α
|{z}

·

{

Rt + γ · max Q̂(st+1 , a) − Q̂(st , at )
a∈A

learning rate

old value

}|

|

{z

}

|

{z

}

old value

temporal difference target

(2.7)
Theoretically, given the Bellman optimality operator H∗ , defined by
∗

(H Q)(s, a) =

X

′





′

P (s |s, a) R(s, a, s ) + γ max Q(s, b) ,
b∈A

s′

(2.8)

which is a contraction mapping and the optimal Q-function is the unique 3
fixed point, i.e., H∗ (Q∗ ) = Q∗ . The Q-learning algorithm draws random
samples of (s, a, R, s′ ) in Eq. (2.7) to approximate Eq. (2.8), but is still
guaranteed to converge to the optimal Q-function Szepesvári and Littman
(1999) under the assumptions that the state-action sets are discrete and finite
and are visited an infinite number of times. Munos and Szepesvári (2008)
extended the convergence result to a more realistic setting by deriving the
high probability error bound for an infinite state space with a finite number of
samples.
Recently, Mnih et al. (2015) applied neural networks as a function approximator for the Q-function in updating Eq. (2.7). Specifically, DQN optimises
the following equation:
"

min E(st ,at ,Rt ,st+1 )∼D
θ

2 #

Rt + γ max Qθ− (st+1 , a) − Qθ (st , at )
a∈A

.

(2.9)
The neural network parameters θ are fitted by drawing i.i.d. samples from the
replay buffer D and then updating in a supervised learning fashion. Qθ− is a
slowly updated target network that helps stabilise training. The convergence
property and finite sample analysis of DQN have been studied by Yang et al.
(2019b).
3. Note that although the optimal Q-function is unique, its corresponding optimal policies
may have multiple candidates.

Single-Agent RL

16
Policy-Based Methods

Policy-based methods are designed to directly search over the policy space
to find the optimal policy π ∗ . One can parameterise the policy expression
π ∗ ≈ πθ (·|s) and update the parameter θ in the direction that maximises the
cumulative reward θ ← θ + α∇θ V πθ (s) to find the optimal policy. However,
the gradient will depend on the unknown effects of policy changes on the state
distribution. The famous policy gradient (PG) theorem Sutton et al. (2000)
derives an analytical solution that does not involve the state distribution, that
is:
h

∇θ V πθ (s) = Es∼µπθ (·),a∼πθ (·|s) ∇θ log πθ (a|s) · Qπθ (s, a)

i

(2.10)

where µπθ is the state occupancy measure under policy πθ and ∇ log πθ (a|s)
is the updating score of the policy. When the policy is deterministic and the
action set is continuous, one obtains the deterministic policy gradient (DPG)
theorem Silver et al. (2014) as
i

h

∇θ V πθ (s) = Es∼µπθ (·) ∇θ πθ (a|s) · ∇a Qπθ (s, a) a=π (s) .
θ

(2.11)

A classic implementation of the PG theorem is REINFORCE Williams
PT
i−t r to estimate Qπθ .
(1992), which uses a sample return Rt =
i
i=t γ
Alternatively, one can use a model of Qω (also called critic) to approximate
the true Qπθ and update the parameter ω via TD learning. This approach gives
rise to the famous actor-critic methods Konda and Tsitsiklis (2000); Peters and
Schaal (2008). Important variants of actor-critic methods include trust-region
methods Schulman et al. (2015, 2017), PG with optimal baselines Weaver
and Tao (2001); Zhao et al. (2011), soft actor-critic methods Haarnoja et al.
(2018), and deep deterministic policy gradient (DDPG) methods Lillicrap et al.
(2015).

3
Multi-Agent RL

In the multiagent scenario, much like in the single-agent scenario, each
agent is still trying to solve the sequential decision-making problem through
a trial-and-error procedure. The difference is that the evolution of the environmental state and the reward function that each agent receives is now
determined by all agents’ joint actions (see Figure 1.1). As a result, agents
need to take into account and interact with not only the environment but also
other learning agents. A decision-making process that involves multiple agents
is usually modeled through stochastic games (covered below in section 3.1) or
extensive-form games (covered in section 3.2).
3.1
3.1.1

Stochastic Game
Problem Formulation

Definition 3.1 (Stochastic Game). A stochastic gameShapley (1953); Littman

(1994) can be regarded as a multi-player 1 extension to the MDP in Definition 2.1.
Therefore, it is also defined by a set of key elements ⟨N, S, {Ai }i∈{1,...,N } , P, {Ri }i∈{1,...,N } ,
1. Player is a common word used in game theory; the agent is more commonly used in
machine learning. We do not discriminate between their usages in this work. The same holds
for strategy vs policy and utility/payoff vs reward. Each pair refers to the game theory usage vs
machine learning usage.

17

Multi-Agent RL

18

traffic intersection

game scenario

Yield

Rush

Yield

(0, 0)

(1, 2)

Rush

(2, 1)

(0, 0)

normal-form game

Figure 3.1 – A snapshot of stochastic time in the intersection example. The scenario
is abstracted such that there are two cars, with each car taking one of
two possible actions: to yield or to rush. The outcome of each joint
action pair is represented by a normal-form game, with the reward value
for the row player denoted in red and that for the column player denoted
in black. The Nash equilibria (NE) of this game are (rush, yield) and
(yield, rush). If both cars maximize their own reward selfishly without
considering the others, they will end up in an accident.

— N : the number of agents, N = 1 degenerates to a single-agent MDP,
N ≫ 2 is referred as many-agent cases in this paper.
— S: the set of environmental states shared by all agents.
— Ai : the set of actions of agent i. We denote A := A1 × · · · × AN .
— P : S ×A → ∆(S): for each time step t ∈ N, given agents’ joint actions
a ∈ A , the transition probability from state s ∈ S to state s′ ∈ S in the
next time step.
— Ri : S × A × S → R: the reward function that returns a scalar value
to the i − th agent for a transition from (s, a) to s′ . The rewards have
absolute values uniformly bounded by Rmax .
— γ ∈ [0, 1] is the discount factor that represents the value of time.
We use the superscript of (·i , ·−i ) (for example, a = (ai , a−i )), when it is
necessary to distinguish between agent i and all other N − 1 opponents.
Ultimately, the stochastic game (SG) acts as a framework that allows
simultaneous moves from agents in a decision-making scenario 2. The game
can be described sequentially, as follows: At each time step t, the environment
has a state st , and given st , each agent executes its action ait , simultaneously
with all other agents. The joint action from all agents makes the environment
transition into the next state st+1 ∼ P (·|st , at ); then, the environment
determines an immediate reward Ri (st , at , st+1 ) for each agent. As seen
2. Extensive-form games allow agents to take sequential moves; the full description can be
found in (Shoham and Leyton-Brown, 2008, Chapter 5).

3.1. Stochastic Game

19

in the single-agent MDP scenario, the goal of each agent i is to solve the
SG. In other words, each agent aims to find a behavioral policy (or, a mixed
strategy 3 in game theory terminology Osborne and Rubinstein (1994)), that is,
π i ∈ Π i : S → ∆(Ai ) that can guide the agent to take sequential actions such
that the discounted cumulative reward 4 in Eq. (3.1) is maximized. Here, ∆(·)
is the probability simplex on a set. In game theory, π i is also called a pure
strategy (vs a mixed strategy) if ∆(·) is replaced by a Dirac measure.
i −i
V π ,π (s)=Es
i
i
−i
−i (·|s )
t
t+1 ∼P (·|st ,at ),at ∼π (·|st ),a ∼π


P

γ t Rti (st ,at ,st+1 ) ait ,s0
t≥0


.

(3.1)
Comparison of Eq. (3.1) with Eq. (2.4) indicates that the optimal policy of
each agent is influenced by not only its own policy but also the policies of the
other agents in the game. This scenario leads to fundamental differences in
the solution concept between single-agent RL and multiagent RL.
3.1.2

Solving Stochastic Games

Given the definitions of SGs defined above, we will primarily introduce
some solutions for SGs in this section, especially with MARL methods. More
detailed discussions about advanced algorithms for solving SGs are provided
later in Sec. 7.4. An SG can be considered as a sequence of normal-form
games, which are games that can be represented in a matrix. Take the original
intersection scenario as an example (see Figure 3.1). A snapshot of the SG
at time t (stage game) can be represented as a normal-form game in a matrix
format. The rows correspond to the action set A1 for agent 1, and the columns
correspond to the action set A2 for agent 2. The values of the matrix are the
rewards given for each of the joint action pairs. In this scenario, if both agents
care only about maximizing their own possible reward with no consideration of
other agents (the solution concept in a single-agent RL problem) and choose the
3. A behavioral policy refers to a function map from the history (s0 , ai0 , s1 , ai1 , ..., st−1 )
to an action. The policy is typically assumed to be Markovian such that it depends on only
the current state st rather than the entire history. A mixed strategy refers to a randomization
over pure strategies (for example, the actions). In SGs, the behavioral policy and mixed policy
are exactly the same. In extensive-form games, they are different, but if the agent retains
the history of previous actions and states (has perfect recall), each behavioral strategy has a
realization-equivalent mixed strategy, and vice versa Kuhn (1950a).
4. Similar to single-agent MDP, we can adopt the objective of time-average rewards.

Multi-Agent RL

20

action to rush, they will reach the outcome of crashing into each other. Clearly,
this state is unsafe and is thus sub-optimal for each agent, despite the fact that
the possible reward was the highest for each agent when rushing. Therefore, to
solve an SG and truly maximize the cumulative reward, each agent must take
strategic actions with consideration of others when determining their policies.
Unfortunately, in contrast to MDPs, which have polynomial time-solvable
linear programming formulations, solving SGs usually involves applying
Newton’s method for solving nonlinear programs. However, there are two
special cases of two-player general-sum discounted-reward SGs that can still
be written as LPs (Shoham and Leyton-Brown, 2008, Chapter 6.2) 5. They are
as follows:
— single-controller SG: the transition dynamics are determined by a single
player, i.e., P (·|a, s) = P (·|ai , s) if the i-th index in the vector a is
a[i] = ai , ∀s ∈ S, ∀a ∈ A .
— separable reward state independent transition (SR-SIT) SG: the states
and the actions have independent effects on the reward function and the
transition function depends on only the joint actions, i.e., ∃α : S →
R, β : A → R such that these two conditions hold: 1) Ri (s, a) =
α(s) + β(a), ∀i ∈ {1, ..., N }, ∀s ∈ S, ∀a ∈ A , and 2) P (·|s′ , a) =
P (·|s, a), ∀a ∈ A , ∀s, s′ ∈ S.
Value-Based MARL Methods

The single-agent Q-learning update in Eq. (2.7) still holds in the multiagent case. In the t-th iteration, for each agent i, given the transition data

(st , at , Ri , st+1 ) t≥0 sampled from the replay buffer, it updates only the
value of Q(st , at ) and keeps the other entries of the Q-function unchanged.
Specifically, we have

Qi (st ,at )←Qi (st ,at )+α·

Ri +γ·evali



Qi (s

t+1 ,·)


i∈{1,...,N }


−Qi (st ,at )

.

(3.2)

Compared to Eq. (2.7), the max operator is changed to another operator

evali {Qi (st+1 , ·)}i∈{1,...,N } in Eq. (3.2) to reflect the fact that each agent
5. According to Filar and Vrieze (2012) [Section 3.5], single-controller SG is solvable in
polynomial time only under zero-sum cases rather than general-sum cases, which contradicts
the result in Shoham and Leyton-Brown (2008) [Chapter 6.2], and we believe Shoham and
Leyton-Brown (2008) made a typo.

3.1. Stochastic Game

21

can no longer consider only itself but must evaluate the situation of the stage
game at time step t + 1 by considering all agents’ interests, as represented
by the set of their Q-functions. Then, the optimal policy can be solved by

solvei {Qi (st+1 , ·)}i∈{1,...,N } = π i,∗ . Therefore, we can further write the
evaluation operator as
evali



Qi (st+1 ,·)


i∈{1,...,N }



n

=V i st+1 , solvei {Qi (st+1 ,·)}i∈{1,...,N }

o

.

(3.3)

In summary, solvei returns agent i′ s part of the optimal policy at some
equilibrium point (not necessarily corresponding to its largest possible reward),
and evali gives agent i’s expected long-term reward under this equilibrium,
assuming all other agents agree to play the same equilibrium. In Chapter 8,
we will further demonstrate how to leverage eval and solve when using valuebased and policy-based algorithms, illustrating their practical applications in
equilibrium computation and policy optimization.
Policy-Based MARL Methods

The value-based approach suffers from the curse of dimensionality due
to the combinatorial nature of multiagent systems (for further discussion, see
Section 4.1). This characteristic necessitates the development of policy-based
algorithms with function approximations. Specifically, each agent learns its
own optimal policy πθi i : S → ∆(Ai ) by updating the parameter θi of, for
example, a neural network. Let θ = (θi )i∈{1,...,N } represent the collection
Q
of policy parameters for all agents, and let πθ := i∈{1,...,N } πθi i (ai |s) be
the joint policy. To optimize the parameter θi , the policy gradient theorem
in Section 2.3 can be extended to the multiagent context. Given agent i’s
P

i
objective function J i (θ) = Es∼P,a∼πθ
t≥0 γt Rt , we have:
h

i

∇θi J i (θ) = Es∼µπθ (·),a∼πθ (·|s) ∇θi log πθi (ai |s) · Qi,πθ (s, a) .

(3.4)

Considering a continuous action set with a deterministic policy, we have
the multiagent deterministic policy gradient (MADDPG) Lowe et al. (2017)
written as
h

i

∇θi J i (θ) = Es∼µπθ (·) ∇θi log πθi (ai |s) · ∇ai Qi,πθ (s, a) a=π (s) . (3.5)
θ

Note that in both Eqs. (3.4) & (3.5), the expectation over the joint policy πθ
implies that other agents’ policies must be observed; this is often a strong
assumption for many real-world applications.

Multi-Agent RL

22
Solution Concept–Nash Equilibrium

Game theory plays an essential role in multiagent learning by offering
so-called solution concepts that describe the outcomes of a game by showing
which strategies will finally be adopted by players. Many types of solution
concepts exist for MARL (see Section 4.2), among which the most famous
is probably the Nash equilibrium (NE) in non-cooperative game theory Nash
(1951). The word “non-cooperative” does not mean agents cannot collaborate
or have to fight against each other all the time, it merely means that each agent
maximizes its own reward independently and that agents cannot group into
coalitions to make collective decisions.
In a normal-form game, the NE characterizes an equilibrium point of the
joint strategy profile (π 1,∗ , ..., π N,∗ ), where each agent acts according to their
best response to the others. The best response produces the optimal outcome
for the player once all other players’ strategies have been considered. Player
i’s best response 6 to π −i is a set of policies in which the following condition
is satisfied.
π i,∗ ∈ Br(π −i ) :=

n

o

arg max Eπ̂i ,π−i Ri (ai , a−i ) .


π̂∈∆(Ai )

(3.6)

NE states that if all players are perfectly rational, none of them will have the
motivation to deviate from their best response π i,∗ given others are playing
π −i,∗ . Note that NE is defined in terms of the best response, which relies
on relative reward values, suggesting that the exact values of rewards are
not required for identifying NE. In fact, NE is invariant under positive affine
transformations of a player’s reward functions. By applying Brouwer’s fixed
point theorem, Nash (1951) proved that a mixed-strategy NE always exists for
any games with a finite set of actions. In the example of driving through an
intersection in Figure 3.1, the NE are (yield, rush) and (rush, yield).
For an SG, one commonly used equilibrium is a stronger version of the NE,
called the Markov perfect NE Maskin and Tirole (2001), which is defined by:
Definition 3.2 (Nash Equilibrium for Stochastic Game). A Markovian strat-

egy profile π ∗ = (π i,∗ , π −i,∗ ) is a Markov perfect NE of a SG defined in
Definition 3.1 if the following condition holds, with the requirement that the
6. Best responses may not be unique; if a mixed-strategy best response exists, there must be
at least one best response that is also a pure strategy.

3.1. Stochastic Game

23

randomness in all players’ strategies remains independent.
Vπ

i,∗ ,π −i,∗

i

(s) ≥ V π ,π

−i,∗

(s),

∀s ∈ S, ∀π i ∈ Π i , ∀i ∈ {1, ..., N }. (3.7)

“Markovian” means the Nash policies are measurable with respect to a
particular partition of possible histories (usually referring to the last state).
The word “perfect” means that the equilibrium is also subgame-perfect Selten
(1965) regardless of the starting state. Considering the sequential nature
of SGs, these assumptions are necessary, while still maintaining generality.
Hereafter, the Markov perfect NE will be referred to as NE.
A mixed-strategy NE 7 always exists for both discounted and averagereward 8 SGs Filar and Vrieze (2012), though they may not be unique. In fact,
checking for uniqueness is N P -hard Conitzer and Sandholm (2008). With the
NE as the solution concept of optimality, we can re-write Eq. (3.3) as:
evaliNash




n
o
Qi (st+1 , ·) i∈{1,...,N } = V i st+1 , Nashi {Qi (st+1 , ·)}i∈{1,...,N }
.
(3.8)

In the above equation, Nashi (·) = π i,∗ computes the NE of agent i’s strategy,

and V i s, {Nashi }i∈{1,...,N } is the expected payoff for agent i from state s
onwards under this equilibrium. Eq. (3.8) and Eq. (3.2) form the learning
steps of Nash Q-learning Hu et al. (1998). This process essentially leads
to the outcome of a learned set of optimal policies that reach NE for every
single-stage game encountered. In the case when NE is not unique, Nash-Q
adopts hand-crafted rules for equilibrium selection (e.g., all players choose
the first NE). Furthermore, similar to normal Q-learning, the Nash-Q operator
defined in Eq. (3.9) is also proved to be a contraction mapping, and the
stochastic updating rule provably converges to the NE for all states when the
NE is unique:

(HNash Q)(s,a)=

P

s′

P (s′ |s,a)

R(s,a,s′ )+γ·evaliNash



Qi (s

t+1 ,·)


i∈{1,...,N }

.

(3.9)

7. Note that this is different from a single-agent MDP, where a single, “pure” strategy
optimal policy always exists. A simple example is the rock-paper-scissors game, where none of
the pure strategies is the NE and the only NE is to mix between the three equally.
8. Average-reward SGs entail more subtleties because the limit of Eq. (2.2) in the multiagent
setting may be a cycle and thus not exist. Instead, NE is proved to exist on a special class of
irreducible SGs, where every stage game can be reached regardless of the adopted policy.

Multi-Agent RL

24

NEXPTIME-hard
P

PSPACE-hard

PPAD

NP-hard

NP

PPAD-hard

PSPACE

NEXPTIME

Figure 3.2 – The landscape of different complexity classes. Relevant examples
are 1) solving the NE in a two-player zero-sum game, P -complete
Neumann (1928), 2) solving the NE in a general-sum game, P P ADhard Daskalakis et al. (2009), 3) checking the uniqueness of the NE,
N P -hard Conitzer and Sandholm (2008), 4) checking whether a purestrategy NE exists in a stochastic game, P SP ACE-hard Conitzer and
Sandholm (2008), and 5) solving Dec-POMDP, N EXP T IM E-hard
Bernstein et al. (2002).

The process of finding a NE in a two-player general-sum game can be
formulated as a linear complementarity problem (LCP), which can then be
solved using the Lemke-Howson algorithm Shapley (1974). However, the
exact solution for games with more than three players is unknown. The known
theoretical result is that solving three-player zero-sum games is believed to
be P P AD-hard Daskalakis and Papadimitriou (2005). In fact, the process of
finding the NE is computationally demanding. Even in the case of two-player
games, the complexity of solving the NE is P P AD-hard (polynomial parity
arguments on directed graphs) Daskalakis et al. (2009); Chen and Deng (2006);
therefore, in the worst-case scenario, the solution could take time that is
exponential in relation to the game size. This complexity 9 prohibits any brute
9. The class of N P -complete is not suitable to describe the complexity of solving the NE
because the NE is proven to always exist Nash (1951), while a typical N P -complete problem –
the traveling salesman problem (TSP), for example – searches for the solution to the question:
“Given a distance matrix and a budget B, find a tour that is cheaper than B, or report that none
exists Daskalakis et al. (2009).”

3.1. Stochastic Game

25

force or exhaustive search solutions unless P = N P (see Figure 3.2). As we
would expect, the NE is much more difficult to solve for general SGs, where
determining whether a pure-strategy NE exists is P SP ACE-hard. Even if the
SG has a finite-time horizon, the calculation remains N P -hard Conitzer and
Sandholm (2008). When it comes to approximation methods to ϵ-NE, the best
known polynomially computable algorithm can achieve ϵ = 0.3393 on bimatrix
games Tsaknakis and Spirakis (2007); its approach is to turn the problem of
finding NE into an optimization problem that searches for a stationary point.
Furthermore, communication complexity can be used as a lower bound for
the required learning time or cost Conitzer and Sandholm (2004a), which is
general for any learning algorithm and provides communication complexity for
various solution concepts in game theory, including Nash equilibrium, iterated
dominant strategies (both strict and weak), and backward induction Conitzer
and Sandholm (2004a). This is a very important principle that many works
have followed to generate lower bounds for equilibrium finding and MARL.
3.1.3

Special Types of Stochastic Games

To summarise the solutions to SGs, one can think of the “master” equation:
Normal-form game solver + MDP solver = SG solver,
which was first summarised by Bowling and Veloso (2000) (in Table 4). The
first term refers to solving an equilibrium (NE) for the stage game encountered
at every time step. It assumes the transition and reward function is known.
The second term refers to applying a RL technique (such as Q-learning) to
model the temporal structure in the sequential decision-making process. It
assumes to only receive observations of the transition and reward function.
The combination of the two gives a solution to SGs, where agents reach a
certain type of equilibrium at each and every time step during the game.
Since solving general SGs with NE as the solution concept for the normalform game is computationally challenging, researchers instead aim to study
special types of SGs that have tractable solution concepts. In this section, we
provide a brief summary of these special types of games.
Definition 3.3 (Special Types of Stochastic Games). Given the general form

of SG in Definition 3.1, we have the following special cases:

Multi-Agent RL

26

Partially-Observable Stochastic Games
Stochastic Games

Team Games

Dec-MDP

MDP

POMDP

Dec-POMDP

Figure 3.3 – Venn diagram of different types of games in the context of POSGs. The
intersection of SG and Dec-POMDP is the team game. In the upper-half
SG, we have MDP ⊂ team games ⊂ potential games ⊂ identical-interest
games ⊂ SGs. In the bottom-half Dec-POMDP, we have MDP ⊂ team
games ⊂ Dec-MDP ⊂ Dec-POMDPs, and MDP ⊂ POMDP ⊂ DecPOMDP. We refer to Sections (3.1.3 & 3.1.4) for detailed definitions of
these games.

3.1. Stochastic Game

27

— normal-form game/repeated game: |S| = 1, see the example in Figure
3.1. These games have only a single state. Though not theoretically
grounded, it is practically easier to solve a small-scale SG.
— identical-interest setting 10: agents share the same learning objective,
which we denote as R. Since all agents are treated independently,
each agent can safely choose the action that maximizes its own reward.
As a result, single-agent RL algorithms can be applied safely, and a
decentralized method is developed. Several types of SGs fall into this
category.
— team games/fully cooperative games/multiagent MDP (MMDP):
agents are assumed to be homogeneous and interchangeable, so
importantly, they share the same reward function 11, R = R1 =
R2 = · · · = RN .
— team-average reward games/networked multiagent MDP (MMDP): agents can have different reward functions, but they share
P
i
the same objective, R = N1 N
i=1 R .
— stochastic potential games: agents can have different reward
functions, but their mutual interests are described by a shared
potential function R = ϕ, defined as ϕ : S × A → R such
that ∀(ai , a−i ), (bi , a−i ) ∈ A , ∀i ∈ {1, ..., N }, ∀s ∈ S and the
following equation holds:
Ri (s,(ai ,a−i ))−Ri (s,(bi ,a−i ))=ϕ(s,(ai ,a−i ))−ϕ(s,(bi ,a−i )).

(3.10)

Games of this type are guaranteed to have a pure-strategy NE
Mguni (2020). Moreover, potential games degenerate into team
games if one chooses the reward function to be a potential function.
— zero-sum setting: agents share opposite interests and act competitively,
and each agent optimizes against the worst-case scenario. The NE in a
zero-sum setting can be solved using a linear program (LP) in polynomial
10. In some of the literature on this topic, identical-interest games are equivalent to team
games. Here, we refer to this type of game as a more general class of games that involve a
shared objective function that all agents collectively optimize, although their individual reward
functions can still be different.
11. In some of the literature on this topic (for example, Wang and Sandholm (2003b)), agents
are assumed to receive the same expected reward in a team game, which means in the presence
of noise, different agents may receive different reward values at a particular moment.

Multi-Agent RL

28

time because of the minimax theorem developed by Neumann (1928).
The idea of min-max values is also related to robustness in machine
learning. We can subdivide the zero-sum setting as follows:
— two-player constant-sum games: R1 (s, a, s′ ) + R2 (s, a, s′ ) =
c, ∀(s, a, s′ ), where c is a constant and usually c = 0. For cases
when c ̸= 0, one can always subtract the constant c for all payoff
entries to make the game zero-sum.
— two-team competitive games: two teams compete against each
other, with team sizes N1 and N2 . Their reward functions are:
{R1,1 , ..., R1,N1 , R2,1 , ..., R2,N2 }.
Team members within a team share the same objective of either
R1 =

X

R1,i /N1 ,

i∈{1,...,N1 }

or
R2 =

X

R2,j /N2 ,

j∈{1,...,N2 }

and R1 + R2 = 0.
— harmonic games: Any normal-form game can be decomposed
into a potential game plus a harmonic game Candogan et al.
(2011). A harmonic game (for example, rock-paper-scissors) can
be regarded as a general class of zero-sum games with a harmonic
property. Let ∀p ∈ A be a joint pure-strategy profile, and let
A[−i] = {q ∈ A : q i ̸= pi , q −i = p−i } be the set of strategies
that differ from p on agent i; then, the harmonic property is:
X

X

Ri (p) − Ri (q) = 0,


∀p ∈ A .

i∈{1,...,N } q∈A [−i]

— linear-quadratic (LQ) setting: the transition model follows linear
dynamics, and the reward function is quadratic with respect to the states
and actions. Compared to a black-box reward function, LQ games offer a
simple setting. For example, actor-critic methods are known to facilitate
convergence to the NE of zero-sum LQ games Al-Tamimi et al. (2007).
Again, the LQ setting can be subdivided as follows:

3.1. Stochastic Game

29
1

— two-player zero-sum LQ games: Q ∈ R|S| , U 1 ∈ R|A | and
2
W 2 ∈ R|A | are the known cost matrices for the state and action spaces, respectively, while the matrices A ∈ R|S|×|S| , B ∈
1
2
R|S|×|A | , C ∈ R|S|×|A | are usually unknown to the agent:
st+1 = Ast + Ba1t + Ca2t ,

s0 ∼ P0 ,

1

R (a1t , a2t ) = −R2 (a1t , a2t )


X
T
T
= −Es ∼P  sT Qst + a1 U 1 a1 − a2 W 2 a2  .
0

t

0

t

t

t

t

t≥0

(3.11)
— multi-player general-sum LQ games: the difference with respect
to a two-player game is that the summation of the agents’ rewards
does not necessarily equal zero:
st+1 = Ast + Bat ,

s0 ∼ P0 ,



X
T
Ri (a) = −Es ∼P  sT Qi st + ai U i ai  .
0

t

0

t

t

(3.12)

t≥0

3.1.4

Partially Observable Settings

A partially observable stochastic game (POSG) assumes that agents have
no access to the exact environmental state but only an observation of the true
state through an observation function. Formally, this scenario is defined by:
Definition 3.4 (partially-observable stochastic games). A POSG is defined

by the set ⟨N, S, {Ai }i∈{1,...,N } , P, {Ri }i∈{1,...,N } , γ, {Oi }i∈{1,...,N } , O⟩. In
|

{z

}

newly added

addition to the SG defined in Definition 3.1, POSGs add the following terms:
— Oi : an observation set for each agent i. The joint observation set is
defined as O := O1 × · · · × ON .
— O : S × A → ∆(O ): an observation function O(o|a, s′ ) denotes the
probability of observing o ∈ O given the action a ∈ A , and the new
state s′ ∈ S from the environment transition.
Each agent’s policy now changes to π i ∈ Π i : O → ∆(Ai ).

30

Multi-Agent RL

Although the added partial-observability constraint is common in practice
for many real-world applications, theoretically it exacerbates the difficulty of
solving SGs. Even in the simplest setting of a two-player fully cooperative
finite-horizon game, solving a POSG is N EXP -hard (see Figure 3.2), which
means it requires super-exponential time to solve in the worst-case scenario
Bernstein et al. (2002). However, the benefits of studying games in the partially
observable setting come from the algorithmic advantages. Notably, recent
advancements in weakly-revealing POSGs Liu et al. (2022b) show that even
when different players have their own local observations, these games can
still be provably learned with polynomial sample complexity. This mitigates
some of the computational challenges traditionally associated with POSGs,
making learning more tractable in certain structured settings. Centralizedtraining-with-decentralized-execution methods Oliehoek et al. (2016); Lowe
et al. (2017); Foerster et al. (2017a); Rashid et al. (2018); Yang et al. (2020)
have achieved many empirical successes, and together with DNNs, they hold
great promise.
A POSG is one of the most general classes of games. An important
subclass of POSGs is decentralized partially observable MDP (Dec-POMDP),
where all agents share the same reward. Formally, this scenario is defined as
follows:
Definition 3.5 (Dec-POMDP). A Dec-POMDP is a special type of POSG

defined in Definition 3.4 with R1 = R2 = · · · = RN .
Dec-POMDPs are related to single-agent MDPs through the partial observability condition, and they are also related to stochastic team games through the
assumption of identical rewards. In other words, versions of both single-agent
MDPs and stochastic team games are particular types of Dec-POMDPs (see
Figure 3.3).
Definition 3.6 (Special types of Dec-POMDPs). The following games are

special types of Dec-POMDPs.
— partially observable MDP (POMDP): there is only one agent of interest,
N = 1. This scenario is equivalent to a single-agent MDP in Definition
2.1 with a partial-observability constraint.
— decentralized MDP (Dec-MDP): the agents in a Dec-MDP have joint
full observability. That is, if all agents share their observations, they

3.2. Extensive-Form Game

31

can recover the state of the Dec-MDP unanimously. Mathematically, we
have ∀o ∈ O , ∃s ∈ S such that P(St = s|O t = o) = 1.
— fully cooperative stochastic games: assuming each agent has full
observability, ∀i = {1, ..., N }, ∀oi ∈ Oi , ∃s ∈ S such that P(St =
s|Ot = oi ) = 1. The fully-cooperative SG from Definition 3.3 is a type
of Dec-POMDP.
We conclude Section 3 by presenting the relationships between the many
different types of POSGs through a Venn diagram in Figure 3.3.
3.2

Extensive-Form Game

3.2.1

Problem Formulation

An SG assumes that a game is represented as a large table in each stage
where the rows and columns of the table correspond to the actions of the two
players 12. Based on the big table, SGs model the situations in which agents
act simultaneously and then receive their rewards. Nonetheless, for many realworld games, players take actions alternately. Poker is one class of games in
which who plays first has a critical role in the players’ decision-making process.
Games with alternating actions are naturally described by an extensive-form
game (EFG) Osborne and Rubinstein (1994); Von Neumann and Morgenstern
(1945) through a tree structure. Recently, Kovarik et al. (2019) has made a
significant contribution in unifying the framework of EFGs and the framework
of POSGs.
Figure 3.4 shows the game tree of two-player Kuhn poker Kuhn (1950b). In
Kuhn poker, the dealer has three cards, a King, Queen, and Jack (King>Queen>Jack),
each player is dealt one card (the orange nodes in Figure 3.4), and the third
card is put aside unseen. The game then develops as follows.
— Player one acts first; he/she can check or bet.
—
—
—

If player one checks, then player two decides to check or bet.
If player two checks, then the higher card wins 1$ from the other
player.
If player two bets, then player one can fold or call.

12. A multi-player game is represented as a high-dimensional tensor in an SG.

Multi-Agent RL

32

If player one folds, then player two wins 1$ from player one.

—

If player one calls, then the higher card wins 2$ from the other

—
player.
—

If player one bets, then player two decides to fold or call.

—

If player two folds, then player one wins 1$ from player two.

—

If player two calls, then the higher card wins 2$ from the other
player.

An important feature of EFGs is that they can handle imperfect information
for multi-player decision-making. In the example of Kuhn poker, the players
do not know which card the opponent holds. However, unlike Dec-POMDP,
which also models imperfect information in the SG setting but is intractable to
solve, EFG, represented in an equivalent sequence form, can be solved by an
LP in polynomial time in terms of game states Koller and Megiddo (1992). In
the next section, we first introduce EFG and then consider the sequence form
of EFG.
Definition 3.7 (Extensive-form Game). An (imperfect-information) EFG can

be described by a tuple of key elements, which is written as
⟨N, A, H, T, {Ri }i∈{1,...,N } , χ, ρ, P, {Si }i∈{1,...,N } ⟩.
— N : the number of players. Some EFGs involve a special player
called “chance”, which has a fixed stochastic policy that represents the
randomness of the environment. For example, the chance player in Kuhn
poker is the dealer, who distributes cards to the players at the beginning.
— A: the (finite) set of all agents’ possible actions.
— H: the (finite) set of non-terminal choice nodes.
— T: the (finite) set of terminal choice nodes, disjoint from H.
— χ : H → 2|A| is the action function that assigns a set of valid actions to
each choice node.
— ρ : H → {1, ..., N } is the player indicating function that assigns, to
each non-terminal node, a player who is due to choose an action at that
node.
— P : H × A → H ∪ T is the transition function that maps a choice node
and an action to a new choice/terminal node such that ∀h1 , h2 ∈ H and
∀a1 , a2 ∈ A, if P (h1 , a1 ) = P (h2 , a2 ), then h1 = h2 and a1 = a2 .

3.2. Extensive-Form Game

33

p1 jack

p2 queen

check

p1 queen

p2 king

bet

check

p1 king

p2 jack

bet

check

p2 king

bet

check

p2 jack

bet

check

p2 queen

bet

check

check bet

bet

check bet

fold call

fold call

check bet

check bet

fold call

fold

check
-1

+1

fold

call

-1

-2

-2

call

bet

fold

call

-1

+1

-2

fold

call

check

-1

+1

fold

call

-1

fold call

-2

+1

+1

fold

call

-2

-1

chance node
terminal node

+2

+2

bet

-1

-2

player one node

player one information set

+1

+1

fold

call

-1

+2

+2

+1

+1

fold

call

-1

+2

+2

player two node
player two information set

Figure 3.4 – Game tree of two-player Kuhn poker. Each node (i.e., circles, squares
and rectangles) represents the choice of one player, each edge represents
a possible action, and the leaves (i.e., diamond) represent final outcomes
over which each player has a reward function (only player one’s reward is
shown in the graph since Kuhn poker is a zero-sum game). Each player
can observe only their own card; for example, when player one holds a
Jack, it cannot tell whether player two is holding a Queen or a King, so
the choice nodes of player one in each of the two scenarios stay within
the same information set.

34

Multi-Agent RL

— Ri : T → R is a real-valued reward function for player i on the terminal
node. Kuhn poker is a zero-sum game since R1 + R2 = 0.
— Si : a set of equivalence classes/partitions Si = (S1i , ..., Ski i ) for agent i on
{h ∈ H : ρ(h) = i} with the property that ∀j ∈ {1, ..., k i }, ∀h, h′ ∈ Sji ,
we have χ(h) = χ(h′ ) and ρ(h) = ρ(h′ ). The set Sji is also called an
information state. The physical meaning of the information state is that
the choice nodes of an information state are indistinguishable. In other
words, the set of valid actions and agent identities for the choice nodes
within an information state are the same; one can thus use χ(Sji ), ρ(Sji )
to denote χ(h), ρ(h), ∀h ∈ Sji .
The inclusion of the information sets in EFG helps to model the imperfect
information cases in which players have only partial or no knowledge about
their opponents. In the case of Kuhn poker, each player can only observe
their own card. For example, when player one holds a Jack, it cannot tell
whether player two is holding a Queen or a King, so the choice nodes of player
one under each of the two scenarios (Queen or King) stay within the same
information set. Perfect-information EFGs (e.g., go or chess) are a special case
where the information set is a singleton, i.e., |Sji | = 1, ∀j, so a choice node
can be equated to the unique history that leads to it. Imperfect-information
EFGs (e.g., Kuhn poker or Texas hold’em) are those in which there exists
i, j such that |Sji | ≥ 1, so the information state can represent more than one
possible history. However, with the assumption of perfect recall (described
later), the history that leads to an information state is still unique.
Recent studies have explored the problem of steering no-regret learning
agents towards desirable equilibria in EFG. Some works address the challenge of
using nonnegative payments to guide players toward predetermined outcomes.
A study shows that steering is impossible if the mediator’s total budget remains
finite, regardless of the game format, but demonstrates that vanishing average
payments can still facilitate steering. Additionally, constant per-round payments
allow steering when players’ full strategies are observable at each round. In
contrast, when only trajectories through the game tree are observable, steering
is generally infeasible with constant per-round payments in EFG, though it
remains possible in normal-form games or when the per-round budget can grow
with time, maintaining vanishing average payments. Zhang et al. (2023a,b)

3.2. Extensive-Form Game
3.2.2

35

Normal-Form Representation

A (simultaneous-move) NFG can be equivalently transformed into an
imperfect-information EFG 13 Shoham and Leyton-Brown (2008) [Chapter 5].
Specifically, since the choices of actions by other agents are unknown to the
central agent, this could potentially lead to different histories (triggered by
other agents) that can be aggregated into one information state for the central
agent.
In the other direction, an imperfect-information EFG can also be transformed into an equivalent NFG in which the pure strategies of each agent
Q
i are defined by the Cartesian product S i ∈Si χ(Sji ), which is a complete
j
specification 14 of which action to take at every information state of that agent.
In the Kuhn poker example, one pure strategy for player one can be checkbet-check-fold-call-fold; altogether, player one has 26 = 64 pure strategies,
corresponding to 3 × 23 = 24 pure strategies for the chance node and 26 = 64
pure strategies for player two. The mixed strategy of each player is then a
distribution over all its pure strategies. In this way, the NE in NFG in Eq.
(3.6) can still be applied to the EFG, and the NE of an EFG can be solved
in two steps: first, convert the EFG into an NFG; second, solve the NE of
the induced NFG by means of the Lemke-Howson algorithm Shapley (1974).
If one further restricts the action space to be state-dependent and adopts the
discounted accumulated reward at the terminal node, then the EFG recovers to
an SG. While the NE of an EFG can be solved through its equivalent normal
form, the computational benefit can be achieved by dealing with the extensive
form directly; this motivates the adoption of the sequence-form representation
of EFGs.

13. This transformation is not unique, but they share the same equilibria as the original game.
This transformation makes the payoff matrix very large, and the actual scalability of many
optimization algorithms for large-scale games is often limited by the large payoff matrix of the
game. Some researchers have proposed acceleration methods to improve solving scalability
Farina and Sandholm (2022). Moreover, this transformation from NFG to EFG does not hold
for perfect-information EFGs.
14. One subtlety of the pure strategy is that it designates a decision at each choice node,
regardless of whether it is possible to reach that node given the other choice nodes.

Multi-Agent RL

36
3.2.3

Sequence-Form Representation

Solving EFGs via the NFG representation, though universal, is inefficient
because the size of the induced NFG is exponential in the number of information
states. In addition, the NFG representation does not consider the temporal
structure of games. One way to address these problems is to operate on the
sequence form of the EFG, also known as the realization-plan representation,
the size of which is only linear in the number of game states and is thus
exponentially smaller than that of the NFG. Importantly, this approach enables
polynomial-time solutions to EFGs Koller and Megiddo (1992).
In the sequence form of EFGs, the main focus shifts from mixed strategies
to behavioral strategies in which, rather than randomizing over complete
pure strategies, the agents randomize independently at each information state

S i ∈ Si , i.e., π i : Si → ∆ χ(S i ) . With the help of behavioral strategies, the
key insight of the sequence form is that rather than building a player’s strategy
around the notion of pure strategies that can be exponentially many, one can
build the strategy based on the paths in the game tree from the root to each
node.
In general, the expressive power of behavioral strategy and mixed strategy are non-comparable. However, if the game has perfect recall, which
intuitively 15 means that each agent remembers all his historical moves in
different information states precisely, then the behavioral strategy and mixed
strategy are somehow equivalent. Specifically, suppose all choice nodes in an
information state share the same history that led to them (otherwise the agent
can distinguish between the choice nodes). In that case, the well-known Kuhn’s
theorem Kuhn (1950a) guarantees that the expressive power of behavioral
strategies and that of mixed strategies coincide in the sense that they induce
the same probability of outcomes for games of perfect recall. As a result,
the set of NE does not change if one considers only behavioral strategies.
15. More formally, on the path from the root node to a decision node h ∈ Sti of player i,
list in chronological order which information sets of i were encountered, i.e., Sti ∈ Si , and
what action player i took at that information set, i.e., ait ∈ χ(Sti ). If one calls this list of
i
(S0i , ai0 , ..., St−1
, ait−1 , Sti ) the experience of player i in reaching node h ∈ Sti , then the game
has perfect recall if and only if, for all players, any nodes in the same information set have
the same experience. In other words, there exists one and only one experience that leads to
each information state and the decision nodes in that information state; because of this, all
perfect-information EFGs are games of perfect recall.

3.2. Extensive-Form Game

37

In fact, the sequence-form representation is primarily useful for describing
imperfect-information EFGs of perfect recall, written as:
Definition 3.8 (Sequence-form Representation). The sequence-form rep-

resentation of an imperfect-information EFG, defined in Definition 3.7, of
perfect recall is described by a tuple of elements written as:
i

(N, Σ, {Gi }i∈{1,...,N } , {π i }i∈{1,...,N } , {µπ }i∈{1,...,N } , {C i }i∈{1,...,N } )
(3.13)
where
— N : the number of agents, including the chance node, if any, denoted by
c.
i
i
— Σ= N
i=1 Σ : where Σ is the set of sequences available to agent i.
A sequence of actions of player i, σ i ∈ Σ i , defined by a choice node
h ∈ H ∪ T, is the ordered set of player i’s actions that have been taken
from the root to node h. Let ∅ be the sequence that corresponds to the
root node.
Note that other players’ actions are not part of agent i’s sequence. In the
example of Kuhn poker, Σ c = {∅, Jack, Queen, King, Jack-Queen, JackKing, Queen-Jack, Queen-King, King-Jack, King-Queen}, Σ 1 = {∅,
check, bet, check-fold, check-bet}, and Σ 2 = {∅, check, bet, fold, call}.

Q

— π i : Si → ∆ χ(S i ) is the behavioral policy that assigns a probability
of taking a valid action ai ∈ χ(S i ) at an information state S i ∈ Si .
This policy randomizes independently over different information states.
In the example of Kuhn poker, each player has six information states;
their behavioral strategy is therefore a list of six independent probability
distributions.


i

— µπ : Σ i → [0, 1] is the realization plan that provides the realization
Q
i
probability, i.e., µπ (σ i ) = c∈σi π i (c), that a sequence σ i ∈ Σ i would
arise under a given behavioral policy π i of player i. In the Kuhn poker
case, the realization probability that player one chooses the sequence of
1
check and then fold is µπ (check-fold) = π 1 (check) × π 1 (fold).
Based on the realization plan, one can recover the underlying behavioral
strategy 16 (an idea similar to Eq. (2.6)). To do so, we need three
16. Empirically, it is often the case that working on the realization plan of a behavioral
strategy is more computationally friendly than working on the behavioral strategy directly.

Multi-Agent RL

38

additional pieces of notation. Let Seq : Si → Σi return the sequence
σ i ∈ Σ i that leads to a given information state S i ∈ Si . Since the game
assumes perfect recall, Seq(S i ) is known to be unique. Let σ i ai denote a
sequence that consists of the sequence σ i followed by the single action ai .
i
Since there are many possible actions ai to choose, let Ext : Σ i → 2Σ
denote the set of all possible sequences that extend the given sequence
by taking one additional action. It is trivial to see that sequences that
include a terminal node cannot be extended, i.e., Ext(T ) = ∅. Finally,
we can write the behavioral policy π i for an information state S i as
i

µπ Seq(S i )ai
 ,
π a ∈ χ(S ) = πi
µ Seq(S i )
i

i



i 

∀S i ∈ Si , ∀ Seq(S i )ai ∈ Ext Seq(S i ) .




(3.14)

— Gi : Σ → R is the reward function for agent i given by Gi (σ) = Ri (T )
if a terminal node T ∈ T is reached when each player plays their part
of the sequence in σ ∈ Σ, and Gi (σ) = 0 if non-terminal nodes are
reached.
Note that since each payoff that corresponds to a terminal node is stored
only once in the sequence-form representation (due to the perfect recall,
each terminal node has only one sequence that leads to it), compared
to the normal-form representation, which is a Cartesian product overall
information sets for each agent and is thus exponential in size, the
sequence form is only linear in the size of the EFG. In the example of
Kuhn poker, the normal-form representation is a tensor with 64×64×32
elements, while in the sequence-form representation, since there are 30
terminal nodes and each node has only one unique sequence leading to
it, the payoff tensor has only 30 elements (plus ∅ for each player).
i

— C i : is a set of linear constraints on the realization probability of µπ .
i
Under the notations of Seq and Ext defined in the bullet points of µπ ,
we know the realization plan must meet the condition that
i



i



X

µπ ∅ = 1, µπ σ i ≥ 0, ∀σ i ∈ Σ i
i



µπ Seq(S i ) =

σ i ∈Ext



i

µπ σ i , ∀S i ∈ Si .


Seq(S i )



(3.15)

3.2. Extensive-Form Game

39
i

The first constraint requires that µπ is a proper probability distribution.
In addition, the second constraint in Eq. (3.15) indicates that in order
for a realization plan to be valid to recover a behavioral strategy, at each
information state of agent i, the probability of reaching that information
state must equal the summation of the realization probabilities of all
the extended sequences. In the example of Kuhn poker, we have C 1 for
1
1
1
player one by µπ (check) = µπ (check-fold) + µπ (check-call).
3.2.4

Solving Extensive-Form Games

Based on the definition of EFG above and its different forms of representation, we will primarily introduce some solutions for EFGs in this section. More
detailed discussions about advanced algorithms for solving EFGs are provided
later in Sec. 7.5. In the sequence-form EFG, given a joint (behavioral) policy
π = (π 1 , ..., π N ), we can write the realization probability of agents reaching
a terminal node T ∈ T, assuming the sequence that leads to the node T is σT ,
in which each player, including the chance player, follows its own path σTi as
µπ σT =


i

Y

µπ σTi .


(3.16)

i∈{1,...,N }

The expected reward for agent i, which covers all possible terminal nodes
following the joint policy π, is thus given by Eq. (3.17).
Ri (π) =

X

µπ σT · Gi (σT ) =


T ∈T

X

µπ σT · Ri (T ).


(3.17)

T ∈T

If we denote the expected reward by Ri (π) for simplicity, then the solution
concept of NE for the EFG can be written as
Ri (π i,∗ , π −i,∗ ) ≥ Ri (π i , π −i,∗ ), for any policy π i of agent i and for all i.
(3.18)
Perfect-Information Games

Every finite perfect-information EFG has a pure-strategy NE Zermelo and
Borel (1913). Since players take turns and every agent sees everything that
has occurred thus far, it is unnecessary to introduce randomness or mixed
strategies into the action selection. However, the NE can be too weak of a

40

Multi-Agent RL

solution concept for the EFG. In contrast to that in NFGs, the NE in EFGs can
represent non-credible threats, which represent the situation where the Nash
strategy is not executed as claimed if agents truly reach that decision node.
A refinement of the NE in the perfect-information EFG is a subgame-perfect
equilibrium (SPE). The SPE rules out non-credible threats by picking only the
NE that is the best response at every subgame of the original game.
The fundamental principle in solving the SPE is backward induction,
which identifies the NE from the bottom-most subgame and assumes those NE
will be played as considered increasingly large trees. Specifically, backward
induction can be implemented through a depth-first search algorithm on the
game tree, which requires time that is only linear in the size of the EFG. In
contrast, finding NE in NFG is known to be P P AD-hard, let alone the NFG
representation is exponential in the size of an EFG.
In the case of two-player zero-sum EFGs, backward induction needs to
propagate only a single payoff from the terminal node to the root node in the
game tree. Furthermore, due to the strictly opposing interests between players,
one can further prune the backward induction process by recognizing that
certain subtrees will never be reached in NE, even without examining those
subtree nodes 17, which leads to the well-known Alpha-Beta-Pruning algorithm
(Shoham and Leyton-Brown, 2008, Chapter 5.1). For games with very deep
game trees, such as Chess or GO, a common approach is to search only nodes
up to certain depths and use an approximate value function to estimate those
nodes’ value without rolling outing to the end Silver et al. (2016).
Finally, backward induction can identify one NE in linear time; yet, it does
not provide an effective way to find all NE. A theoretical result suggests that
finding all NE in a two-player perfect-information EFG (not necessarily zerosum) requires O(|T|3 ), which is still tractable (Shoham and Leyton-Brown,
2008, Theorem 5.1.6).
Imperfect-Information Games

By means of the sequence-form representation, one can write the solution
of a two-player EFG as an LP. Given a fixed behavioral strategy of player two,
2
in the form of realization plan µπ , the best response for player one can be
17. This occurs, for example, in the case that the worst case of one player in one subgame is
better than the best case of that player in another subgame.

3.2. Extensive-Form Game

41

written as




max

X

µπ1 σ 1 ∈Σ 1

µ

π1





σ1 





X

g1 σ1, σ2 µ

π2





σ2 

σ 2 ∈Σ 2

subject to the constraints in Eq. (3.15). In NE, player one and player two form
1
2
a mutual best response. However, if we treat both µπ and µπ as variables,
then the objective becomes nonlinear. The key to address this issue is to adopt
the dual form of the LP Koller and Megiddo (1996), which is written as
min v0
s.t. vI(σ1 ) −

X
I ′ ∈I Ext(σ 1 )

vI ′ ≥


X





g 1 σ 1 , σ 2 µπ

2





σ2 ,

∀σ 1 ∈ Σ 1

σ 2 ∈Σ 2

(3.19)
where I : Σ i → Si is a mapping function that returns the information set 18
encountered when the final action in σ i was taken. With slight abuse of notation,

we let I Ext(σ 1 ) 19 denote the set of final information states encountered in
2
the set of the extension of σ i . The variable v0 represents, given µπ , player
1
one’s expected reward under its own realization plan µπ , and vI ′ can be
considered as the part of this expected utility in the subgame starting from
information state I ′ . Note that the constraint needs to hold for every sequence
of player one.
2
In the dual form of best response in Eq. (3.19), if one treats µπ as
2
an optimizing variable rather than a constant, which means µπ must meet
the requirements in Eq. (3.15) to be a proper realization plan, then the LP

18. Recall that this information set is unique under the assumption of perfect recall.
19. Recall that Ext(σ 1 ) is the set of all possible sequences that extend σ 1 one step ahead.

Multi-Agent RL

42

formulation for a two-player zero-sum EFG can be written as follows.
min v0

(3.20)
X

s.t. vI(σ1 ) −

X

vI ′ ≥





g 1 σ 1 , σ 2 µπ


2



∀σ 1 ∈ Σ 1

σ2 ,

σ 2 ∈Σ 2



I ′ ∈I Ext(σ 1 )

(3.21)
µ

π2

π2



∅ = 1, µ


2

σ



≥ 0, ∀σ 2 ∈ Σ 2

2

2

X

µπ Seq(S 2 ) =

σ 2 ∈Ext

(3.22)

µπ σ , ∀S 2 ∈ S2 .

Seq(S 2 )

2

(3.23)



Player two’s realization plan is now selected to minimize player one’s expected
utility. Based on the minimax theorem Von Neumann and Morgenstern
(1945), we know this process will lead to a NE. Notably, though the zero-sum
EFG and zero-sum SG (see the formulation in Eq. (7.1)) both adopt the LP
formulation to solve the NE and can be solved in polynomial time, the size of
the representation for the game itself is very different. If one chooses first to
transform the EFG into an NFG presentation and then solve it by LP, then the
time complexity would in fact become exponential in the size of the original
EFG.
The solution to a two-player general-sum EFG can also be formulated
using an approach similar to that used for the zero-sum EFG. The difference
is that there will be no objective function such as Eq. (3.20) since in the
general-sum context, one agent’s reward can no longer be determined based on
the other player’s reward. The LP with only Eqs. (3.21 - 3.23) thus becomes a
constraint satisfaction problem. Specifically, one would need to repeat Eqs.
(3.21 - 3.23) twice to consider each player independently. One final subtlety
required in solving the two-player general-sum EFG is to ensure v 1 and v 2 are
bounded 20, a complementary slackness condition must be further imposed;
we have ∀σ 1 ∈ Σ 1 (vice versa ∀σ 2 ∈ Σ 2 for player two):
1



µπ (σ 1 )

v1

P

−
I (σ 1 )

 P
 v1′ −
2

I ′ ∈I Ext(σ 1 )

I

2



g 1 (σ 1 ,σ 2 )µπ (σ 2 )
σ ∈Σ 2

=0.

(3.24)

The above condition indicates that for each player, either the sequence σ i is
i
never played, i.e., µπ (σ i ) = 0, or all sequences that are played by that player
20. Since the constraints are linear, they remain satisfied when both v 1 and v 2 are increased
by the same constant to any arbitrarily large values.

3.2. Extensive-Form Game

43

with positive probability must induce the same expected payoff such that v i
takes arbitrarily large values, thus being bounded. Eqs. (3.21 - 3.23), together
with Eq. (3.24), turns the solution to the NE into an LCP problem that can be
solved by the generalized Lemke-Howson method Lemke and Howson (1964).
Although in the worst case, polynomial time complexity cannot be achieved, as
can for zero-sum games, this approach is still exponentially faster than running
the Lemke-Howson method to solve the NE in a normal-form representation.
For a perfect-information EFG, recall that the SPE is a more informative
solution concept than NE. Extending SPE to the imperfect-information scenario
is therefore valuable. However, such an extension is non-trivial because a
well-defined notion of a subgame is lacking. However, for EFGs with perfect
recall, the intuition of subgame perfection can be effectively extended to a new
solution concept, named the sequential equilibrium (SE) Kreps and Wilson
(1982), which is guaranteed to exist and coincides with the SPE if all players
in the game have perfect information. We refer the readers to Chapter 7 for
discussions about advanced algorithms for solving EFGs.
Beyond defining equilibrium concepts, a crucial line of research has focused
on optimizing the sample complexity required to learn these equilibria. Recent
research has focused on optimizing the sample complexity of learning Nash
equilibria (NE) in imperfect-information games (IIGs). Kozuno et al. (2021)
introduces the Implicit Exploration Online Mirror Descent (IXOMD) algorithm
for two-player
zero-sum IIGs, achieving a high-probability convergence rate
√
of O(1/ T ), where T is the number of games played. This model-free
algorithm performs updates along sampled trajectories, ensuring computational
efficiency. Building on this, Bai et al. (2022) presents the Balanced Online
Mirror Descent (BOMD) and Balanced Counterfactual Regret Minimization
(B-CFR) algorithms, which improve the sample complexity to Õ((XA +
Y B)/ϵ2 ), where X, Y are the number of information sets and A, B are the
number of actions. This improves over previous bounds and matches the
information-theoretic lower bound up to logarithmic factors. Fiegel et al.
(2023) provides a problem-independent lower bound of Õ(H(AX + BY)/ϵ2 )
for learning optimal strategies in zero-sum IIGs. They propose two FTRL
algorithms: Balanced FTRL, which matches this lower bound but requires prior
knowledge of the information set structure, and Adaptive FTRL, which adapts
the regularization dynamically, achieving Õ(H 2 (AX + BY)/ϵ2 ). These work
indicate how to improve the efficiency of IIG algorithms and promote their

44

Multi-Agent RL

practical application.
Recent work has further expanded the scope of equilibrium concepts in
EFGs, particularly by introducing new classes of equilibria such as communication and certification equilibria Zhang and Sandholm (2020). These models
augment the game with a mediator capable of sending and receiving messages
to and from the players, with the added capability of remembering the messages.
In contrast to standard correlated equilibrium, the optimality of these new
equilibrium notions can be computed in polynomial time, even in the context of
extensive-form games. Some researchers explore these equilibrium concepts
computationally, showing that the complexity of solving such equilibria is often
driven by the mediator’s imperfect recall Zhang and Sandholm (2022). Their
work also generalizes previous algorithms, including the polynomial-time
solution for Bayes-Nash equilibria in automated mechanism design Conitzer
and Sandholm (2004b), and the correlation DAG algorithm for optimal correlation Zhang et al. (2022a). By leveraging a mediator-augmented game model,
they provide a scalable framework for computing these equilibria, particularly
in settings where players cannot lie about their information but may remain
silent, which they define as full-certification equilibria. This framework not
only enhances our understanding of the computational landscape of EFGs
but also offers practical insights for both automated decision-making and
game-theoretic analysis in multi-agent systems.
In contrast, the problem of finding an optimal extensive-form correlated
equilibrium (CCE) remains computationally intractable. While CCE is a
natural extension of correlated equilibrium to extensive-form games, it is
NP-hard to compute an optimal CCE, particularly when aiming to maximize
welfare or achieve other desirable outcomes. This hardness stems from the
complex dependencies between players’ strategies and the mediator’s actions,
which are difficult to manage in the context of imperfect recall and the intricate
structure of extensive-form games. As a result, unlike the communication and
certification equilibrium models, the computation of an optimal CCE requires
significantly more sophisticated methods and remains a major challenge in
game-theoretic research Zhang et al. (2022a). Thus, while recent advances
have made significant strides in providing polynomial-time algorithms for
some equilibrium concepts, the computational complexity of finding optimal
correlated equilibria in extensive-form games continues to be a fundamental
obstacle in the field.

4
Grand Challenges of MARL

Compared to single-agent RL, multiagent RL is a general framework that
better matches the broad scope of real-world AI applications. However, due to
the existence of multiple agents that learn simultaneously, MARL methods
pose more theoretical challenges, in addition to those already present in singleagent RL. Compared to classic MARL settings where there are usually two
agents, solving a many-agent RL problem is even more challenging. As a
1 combinatorial complexity, O
2 multi-dimensional learning
matter of fact, O
3 the issue of non-stationarity all result in the majority of
objectives, and O
4 only two players,
MARL algorithms being capable of solving games with O
in particular, two-player zero-sum games. In this section, I will elaborate on
each of the grand challenges in many-agent RL.
4.1

Combinatorial Complexity

In the context of multiagent learning, each agent has to consider the other
opponents’ actions when determining the best response; this characteristic is
deeply rooted in each agent’s reward function and for example is represented
by the joint action a in their Q-function Qi (s, a) in Eq. (3.2). The size of the
joint action space, |A|N , grows exponentially with the number of agents and
thus largely constrains the scalability of MARL methods. Furthermore, the
45

46

Grand Challenges of MARL

combinatorial complexity is worsened by the fact that solving a NE in game
theory is P P AD-hard, even for two-player games. Therefore, for multi-player
general-sum games (neither team games nor zero-sum games), it is non-trivial
to find an applicable solution concept.
One common way to address this issue is by assuming specific factorized
structures on action dependency such that the reward function or Q-function
can be significantly simplified. For example, a graphical game assumes an
agent’s reward is affected by only its neighboring agents, as defined by the graph
from Kearns (2007). This assumption leads to a polynomial-time solution for
the computation of a NE in specific tree graphs Kearns et al. (2013), though
the scope of applications is limited beyond this specific scenario.
Recent progress has also been made toward leveraging particular neural
network architectures for Q-function decomposition Sunehag et al. (2018);
Rashid et al. (2018); Yang et al. (2020). In addition to the fact that these methods
work only for the team-game setting, the majority of them lack theoretical
backing. There remain open questions to answer, such as understanding the
representational power (the approximation error) of the factorized Q-functions
in a multiagent task and how factorization itself can be learned from scratch.
4.2

Multi-Dimensional Learning Objectives

Compared to single-agent RL, where the only goal is to maximize the
learning agent’s long-term reward, the learning goals in MARL are naturally
multi-dimensional, as the objective of all agents are not necessarily aligned by
one metric. Bowling and Veloso (2001, 2002) proposed to classify the goals
of the learning task into two types: rationality and convergence. Rationality
ensures an agent takes the best possible response to the opponents when they
are stationary, and convergence ensures the learning dynamics eventually lead
to a stable policy against a given class of opponents. Reaching both rationality
and convergence gives rise to reaching the NE.
In terms of rationality, the NE characterizes a fixed point of a joint optimal
strategy profile from which no agents would be motivated to deviate as long
as they are all perfectly rational. However, in practice, an agent’s rationality
can easily be bound by either cognitive limitations and/or the tractability of
the decision problem. In these scenarios, the rationality assumption can be
relaxed to include other types of solution concepts, such as the recursive

4.2. Multi-Dimensional Learning Objectives

47

reasoning equilibrium, which results from modelling the reasoning process
recursively among agents with finite levels of hierarchical thinking (for example,
an agent may reason in the following way: I believe that you believe that
I believe ...) Wen et al. (2018, 2019); best response against a target type
of opponent Powers and Shoham (2005b); the mean-field game equilibrium,
which describes multiagent interactions as a two-agent interaction between each
agent itself and the population mean Guo et al. (2019); Yang et al. (2018b,a);
evolutionary stable strategies, which describe an equilibrium strategy based
on its evolutionary advantage of resisting invasion by rare emerging mutant
strategies Maynard Smith (1972); Tuyls and Nowé (2005); Tuyls and Parsons
(2007); Bloembergen et al. (2015); Stackelberg equilibrium Zhang et al.
(2019a), which assumes specific sequential order when agents take decisions;
and the robust equilibrium (also called the trembling-hand perfect equilibrium
in game theory), which is stable against adversarial disturbance Li et al.
(2019b); Goodfellow et al. (2014b); Yabu et al. (2007).
In terms of convergence, although most MARL algorithms are contrived to
converge to the NE, the majority either lack a rigorous convergence guarantee
Zhang et al. (2019b), potentially converge only under strong assumptions such
as the existence of a unique NE Littman (2001b); Hu and Wellman (2003), or
are provably non-convergent in all cases Mazumdar et al. (2019a). Zinkevich
et al. (2006) identified the non-convergent behavior of value-iteration methods
in general-sum SGs and instead proposed an alternative solution concept to the
NE – cyclic equilibria – that value-based methods converge to. The concept
of no regret (also called the Hannan consistency in game theory Hansen et al.
(2003)), measures convergence by comparison against the best possible strategy
in hindsight. This was also proposed as a new criterion to evaluate convergence
in zero-sum self-plays Bowling (2005); Hart and Mas-Colell (2001); Zinkevich
et al. (2008). In two-player zero-sum games with a non-convex non-concave
loss landscape (training GANs Goodfellow et al. (2014a)), gradient-descentascent methods are found to reach a Stackelberg equilibrium Lin et al. (2019);
Fiez et al. (2019) or a local differential NE Mazumdar et al. (2019b) rather
than the general NE.
Finally, although the above solution concepts account for convergence,
building a convergent objective for MARL methods with DNNs remains an
uncharted area. This is partly because the global convergence result of a
single-agent deep RL algorithm, for example, neural policy gradient methods

48

Grand Challenges of MARL

Wang et al. (2019); Liu et al. (2019) and neural TD learning algorithms Cai
et al. (2019b), has not been extensively studied yet.
4.3

The Challenges of Non-Stationarity and Adaptive Opponents

The most well-known challenge of multiagent learning versus singleagent learning is probably the non-stationarity issue. Since multiple agents
concurrently improve their policies according to their own interests, from each
agent’s perspective, the environmental dynamics become non-stationary and
challenging to interpret when learning. This problem occurs because the agent
itself cannot tell whether the state transition – or the change in reward – is an
actual outcome due to its own action or if it is due to its opponent’s explorations.
Although learning independently by completely ignoring the other agents can
sometimes yield surprisingly powerful empirical performance Papoudakis
et al. (2020); Matignon et al. (2012), this approach essentially harms the
stationarity assumption that supports the theoretical convergence guarantee of
single-agent learning methods Tan (1993). As a result, the Markovian property
of the environment is lost, and the state occupancy measure of the stationary
policy in Eq. (2.5) no longer exists. For example, the convergence result of
single-agent policy gradient methods in MARL is provably non-convergent in
simple linear-quadratic games Mazumdar et al. (2019b).
The non-stationarity issue can be further aggravated by TD learning, which
occurs with the replay buffer that most deep RL methods currently adopt Lin
(1992); Foerster et al. (2017b). In single-agent TD learning (see Eq. (2.9)),
the agent bootstraps the current estimate of the TD error, saves it in the replay
buffer, and samples the data in the replay buffer to update the value function. In
the context of multiagent learning, since the value function for one agent also
depends on other agents’ actions, the bootstrap process in TD learning also
requires sampling other agents’ actions, which leads to two problems. First, the
sampled actions barely represent the full behaviour of other agents’ underlying
policies across different states. Second, an agent’s policy can change during
training, so the samples in the replay buffer can quickly become outdated.
Therefore, the dynamics that yielded the data in the agent’s replay buffer must
be constantly updated to reflect the current dynamics in which it is learning.
This process further exacerbates the non-stationarity issue.
In general, the non-stationarity issue forbids the reuse of the same mathe-

4.4. Scalability Issue when N ≫ 2

49

matical tool for analysing single-agent algorithms in the multiagent context.
However, one exception exists: the identical-interest game in Definition 3.3.
In such settings, each agent can safely perform selfishly without considering
other agents’ policies since the agent knows the other agents will also act in
their own interest. The stationarity is thus maintained, so single-agent RL
algorithms can still be applied.
Beyond non-stationarity, an additional challenge in competitive games
arises due to the adaptivity of opponents He et al. (2016), who may actively
exploit weaknesses in the learner’s policy. Unlike in cooperative settings,
where all agents aim for shared goals, in competitive environments, adversarial
opponents can adjust their strategies dynamically to counter the learner’s
decisions Ganzfried and Sandholm (2011); Schadd et al. (2007). This means
that a learning agent must not only improve its own policy but also anticipate
and react to an ever-changing opponent strategy, making the learning process
even more unstable. In extreme cases, naive learning algorithms can be
systematically exploited, leading to poor performance or even divergence.
4.4

Scalability Issue when N ≫ 2

Combinatorial complexity, multi-dimensional learning objectives, and
the issue of non-stationarity all result in the majority of MARL algorithms
being capable of solving games with only two players, in particular, two-player
zero-sum games Zhang et al. (2019b). As a result, solutions to general-sum
settings with more than two agents (for example, the many-agent problem)
remain an open challenge. This challenge may be addressed from all three
perspectives of multiagent intelligence: game theory, which provides realistic
and tractable solution concepts to describe learning outcomes of a many-agent
system; RL algorithms, which offer provably convergent learning algorithms
that can reach stable and rational equilibria in the sequential decision-making
process; and finally deep learning techniques, which provide the learning
algorithms with expressive function approximators.

Grand Challenges of MARL

50

Table 4.1 – Common assumptions on the level of local knowledge made by MARL
algorithms.
Categories
0
1
2
3
4
5
6

Assumptions

Examples

Each agent observes the reward of his selected action.
Each agent observes the rewards of all possible actions.
Each agent observes others’ selected actions.
Each agent observes others’ reward values.
Each agent knows others’ exact policies.
Each agent knows others’ exact reward functions.
Each agent knows the equilibrium of the stage game.

Bandit
Blackjack, Full-information Bandit
Chess, Texas Hold ’em
Texas Hold ’em, Street Fighter
?
Tic-Tac-Toe, Rock-Paper-Scissor
Prisoner’s Dilemma, Rock-Paper-Scissor

5
A Survey of MARL Surveys

In this section, we provide a non-comprehensive review of MARL algorithms. To begin, we introduce different taxonomies that can be applied
to categorise prior approaches. Given multiple high-quality, comprehensive
surveys on MARL methods already exist, a survey of those surveys is provided.
Based on the proposed taxonomy, we review related MARL algorithms, covering works on identical interest games, zero-sum games, and games with an
infinite number of players. This section is written to be selective, focusing on
the algorithms that have theoretical guarantees and less focus on those with
only empirical success or those that are purely driven by specific applications.
5.1

Taxonomy of MARL Algorithms

One significant difference between the taxonomy of single-agent RL
algorithms and MARL algorithms is that in the single-agent setting, since the
objective is single and consistent (i.e. in terms of reward maximization), the
taxonomy is driven mainly by the type of solution Kaelbling et al. (1996); Li
(2017), for example, model-free vs model-based, on-policy vs off-policy, TD
learning vs Monte-Carlo methods. By contrast, in the multiagent setting, due
to the existence of multiple learning objectives (see Section 4.2), the taxonomy
is driven mainly by the type of problem rather than the solution. In fact, asking
51

52

A Survey of MARL Surveys

the right question for MARL algorithms is itself a research problem, which
is referred to as the problem problem Balduzzi et al. (2018b); Shoham et al.
(2007).
Based on Stage Games Types.

Since the solution concept varies considerably according to the game type, one principal component of the MARL
taxonomy is the nature of stage games. A common division 1 includes team
games (more generally, potential games), zero-sum games (more generally,
harmonic games), and a mixed setting of the two games, namely, general-sum
games. Other types of “exotic” games, such as mean-field games Lasry and
Lions (2007), that originate from non-game-theoretical research domains exist
and have recently attracted tremendous attention. Based on the type of stage
game, the taxonomy can be further enriched by how many times they are played.
A repeated game is where one stage game is played repeatedly without considering the state transition, and the learning cost in the learnability framework
Conitzer and Sandholm (2003) is measured by the losses the learning agent
accrues (rather than the number of rounds) Conitzer and Sandholm (2003). An
SG is a sequence of stage games, which can be infinitely long, with the order of
the games to play determined by the state-transition probability. Since solving
a general-sum SG is at least P SP ACE-hard Conitzer and Sandholm (2008),
MARL algorithms usually have a clear boundary on what types of games they
can solve. For general-sum games, there are few MARL algorithms that have a
provable convergence guarantee without strong, even unrealistic, assumptions
(e.g., the NE is unique) Shoham et al. (2007); Zhang et al. (2019b).
Based on Level of Local Knowledge.

The assumption on the level of
local knowledge, i.e., what agents can and cannot know during training and
execution time, is another major component to differentiate MARL algorithms.
Having access to different levels of local knowledge leads to different local
behaviours by agents and various levels of difficulty in developing theoretical
analysis.

1. Such a division is complementary because any multi-player normal-form game can
be decomposed into a potential game Monderer and Shapley (1996) plus a harmonic game
Candogan et al. (2011) (also see Definition 3.3); in the two-player case, it corresponds to a
team game plus a zero-sum game.

5.1. Taxonomy of MARL Algorithms

53

One can think of some simplest cases by extending the bandit problems
in online learning literature to multiagent settings, where each agent can
observe the reward of its selected action or rewards of all possible actions. The
latter one is more informative than the former. Beyond observing the rewards
for selected actions, knowing the entire reward function can be even more
informative (i.e., access to rewards for unselected actions and other agents’
rewards). Apart from the reward functions, the actions of the other agents
can be observable to the current agent in perfect information games, which
provides additional information for it to make decisions. Knowledge of the
agents’ exact policy/reward function forms is a much stronger assumption than
being able to observe their sampled actions/rewards. Therefore, being aware
of other agents’ policies are usually unlikely for complex games in practice,
with examples like Texas Hold ’em and Chess. There are also extreme cases
on the two ends. One is the case that the agent can observe nothing apart from
itself, and another is that the agent knows the equilibrium point, i.e., the direct
answer of the game.
Based on Learning Paradigms.

In addition to various levels of local
knowledge, MARL algorithms can be classified based on the learning paradigm,
as shown in Figure 5.1. For example, the 4th learning paradigm addresses
multiagent problems by building a single-agent controller, which takes the
joint information from all agents as inputs and outputs the joint policies for
all agents. In this paradigm, agents can exchange any information with any
other opponent through the central controller. The information that can be
exchanged depends on the assumptions about the level of local knowledge
describe in the last paragraph. The 5th paradigm allows agents to exchange
information with other agents only during training; during execution, each
agent has to act in a decentralised manner, making decisions based on its own
observations only. The 6th category can be regarded as a particular case of
paradigm 5 in that agents are assumed to be interconnected via a (time-varying)
network such that information can still spread across the whole network if
agents communicate with their neighbours. The most general case is paradigm
2, where agents are fully decentralised, with no information exchange of any
kind allowed at any time, and each agent executes its own policy. Relaxation
of paradigm 2 yields the 1st and the 3rd paradigm, where the agents, although
they cannot exchange information, share a single set of policy parameters, or,

A Survey of MARL Surveys

54

Agent
1
Environoment

Agent
2

Agent
2

Agent
4

Agent
N

(1)

(2)

Agent
1

Agent
1

Agent
2

Environoment

Agent
N

Agent
N

(4)

(5)

Environoment

Agent
4

Agent
2

Agent
N

Agent
3
Agent
4

(3)
Agent
1

Agent
3
Environoment

Agent
4

Agent
2

...

Agent
2

...

...

Central
Controller

Agent
1

Agent
3

...

Environoment

Environoment

...

...
Agent
N

Agent
1

Agent
3

Agent
N

Agent
3
Agent
4

(6)

Figure 5.1 – Common learning paradigms of MARL algorithms. (1) Independent
learners with shared policy. (2) Independent learners with independent
policies (i.e., denoted by the difference in wheels). (3) Independent
learners with shared policy within a group. (4) One central controller
controls all agents: agents can exchange information with any other
agents at any time. (5) Centralised training with decentralised execution
(CTDE): only during training, agents can exchange information with
others; during execution, they act independently. (6) Decentralised
training with networked agents: during training, agents can exchange
information with their neighbours in the network; during execution, they
act independently.

within a pre-defined group, share a single set of policy parameters.
Based on Five AI Agendas.

In order for MARL researchers to be specific
about the problem being addressed and the associated evaluation criteria,
Shoham et al. (2007); Sandholm (2007) identified five coherent agendas for
MARL studies, each of which has a clear motivation and success criterion.
Though proposed more than a decade ago, these five distinct goals are still
useful in evaluating and categorising recent contributions. We, therefore,
choose to incorporate the five agendas into the taxonomy of MARL algorithms,
as described in Table 5.1.
The following sections are organized around the five agendas in Table 5.1.
Specifically, the focus will be on investigating how game-theoretic models
can be constructed in both cooperative and noncooperative scenarios, with an

5.1. Taxonomy of MARL Algorithms

55

Table 5.1 – Summary of the five agendas for multiagent learning research Shoham
et al. (2007); Sandholm (2007).
ID

Agenda

Description

1

Computational

To develop efficient methods that can find solutions (strategy profiles) to games according
to given solution concepts. Examples: Berger
(2007); Leyton-Brown and Tennenholtz (2005)

2

Descriptive

To develop formal models of learning
that agree with the behaviors of people/animals/organizations. Examples: Erev
and Roth (1998); Camerer et al. (2002)

3

Learning algorithms in
equilibrium

To ensure that learning strategies themselves
form an equilibrium, where no agent can benefit by unilaterally deviating. This addresses
challenges such as unknown payoff matrices
and the fragility of equilibria when agents fail
to follow prescribed strategies. For example,
in a repeated prisoner’s dilemma, agents using
learning algorithms that converge to a mutual
cooperation equilibrium would achieve higher
payoffs than those deviating to always-defect
strategies.

4

Prescriptive, cooperative

To develop distributed learning algorithms for
games with intra-team collaboration characteristics. In this agenda, there is rarely a role for
equilibrium analysis since the agents have no
motivation to deviate from the prescribed algorithm. Examples: Claus and Boutilier (1998b)

5

Prescriptive, noncooperative

To develop effective methods for obtaining
a “high reward” in a given environment, for
example, an environment with a selected class
of opponents. Examples: Powers and Shoham
(2005a,b)

A Survey of MARL Surveys

56

emphasis on designing equilibrium learning algorithms. These discussions
aim to provide a comprehensive understanding of strategy selection and
optimization in multi-agent systems, drawing on both theoretical foundations
and practical applications.
5.2

A Survey of Surveys

A multiagent system (MAS) is a generic concept that could refer to many
different domains of research across different academic subjects; general
overviews are given by Weiss (1999), Wooldridge (2009), and Shoham
and Leyton-Brown (2008). Due to the many possible ways of categorising
multiagent (reinforcement) learning algorithms, it is impossible to have a
single survey that includes all relevant works considering all directions of
categorisations. In the past two decades, there has been no lack of survey papers
that summarise the current progress of specific categories of multiagent learning
research. In fact, there are so many that these surveys themselves deserve a
comprehensive review. Before proceeding to review MARL algorithms based
on the proposed taxonomy in Section 5.1, in this section, I provide an overview
of relevant surveys that study multiagent systems from the machine learning,
in particular, the RL, perspective.
One of the earliest studies that surveyed MASs in the context of machine
learning/AI was published by Stone and Veloso (2000): the research works up
to that time were summarised into four major scenarios considering whether
agents were homogeneous or heterogeneous and whether or not agents were
allowed to communicate with each other. Shoham et al. (2007) considered
the game theory and RL perspective and introspectively asked the question
of “if multiagent learning is the answer, what is the question?”. Upon failing
to find a single answer, Shoham et al. (2007) proposed the famous five AI
agendas for future research work to address. Stone (2007) tried to answer
Shoham’s question by emphasising that MARL can be more broadly framed
than through game theoretic terms, and he noted that how to apply the MARL
technique remains an open question, rather than being an answer, in contrast to
the suggestion of Shoham et al. (2007). The survey of Tuyls and Weiss (2012)
also reflected on Stone’s viewpoint; they believed that the entanglement of only
RL and game theory is too narrow in its conceptual scope, and MARL should
embrace other ideas, such as transfer learning Taylor and Stone (2009), swarm

5.2. A Survey of Surveys

57

intelligence Kennedy (2006), and co-evolution Tuyls and Parsons (2007).
Panait and Luke (2005) investigated the cooperative MARL setting; instead
of considering only reinforcement learners, they reviewed learning algorithms
based on the division of team learning (i.e., applying a single learner to search
for the optimal joint behaviour for the whole team) and concurrent learning (i.e.,
applying one learner per agent), which includes broader areas of evolutionary
computation, complex systems, etc. Matignon et al. (2012) surveyed the
solutions for fully-cooperative games only; in particular, they focused on
evaluating independent RL solutions powered by Q-learning and its many
variants. Due to the fact that Q-learning does not require an environmental
model and can be used online, it is very suitable for repeated games with
unknown opponents Sandholm and Crites (1996). Jan’t Hoen et al. (2005)
conducted an overview with a similar scope; moreover, they extended the
work to include fully competitive games in addition to fully cooperative
games. Buşoniu et al. (2010), to the best of my knowledge, presented the first
comprehensive survey on MARL techniques, covering both value iterationbased and policy search-based methods, together with their strengths and
weaknesses. In their survey, they considered not only fully cooperative or
competitive games but also the effectiveness of different algorithms in the
general-sum setting. Nowé et al. (2012), in the 14th chapter, addressed the
same topic as Buşoniu et al. (2010) but with a much narrower coverage of
multiagent RL algorithms.
Tuyls and Nowé (2005) and Bloembergen et al. (2015) both surveyed the
dynamic models that have been derived for various MARL algorithms and
revealed the deep connection between evolutionary game theory and MARL
methods. We refer to Table 1 in Tuyls and Nowé (2005) for a summary of this
connection.
Hernandez-Leal et al. (2017) provided a different perspective on the taxonomy of how existing MARL algorithms cope with the issue of non-stationarity
induced by opponents. On the basis of the opponent and environment characteristics, they categorised the MARL algorithms according to the type of
opponent modelling.
Da Silva and Costa (2019) introduced a new perspective of reviewing
MARL algorithms based on how knowledge is reused, i.e., transfer learning.
Specifically, they grouped the surveyed algorithms into intra-agent and interagent methods, which correspond to the reuse of knowledge from experience

58

A Survey of MARL Surveys

gathered from the agent itself and that acquired from other agents, respectively.
Most recently, deep MARL techniques have received considerable attention.
Nguyen et al. (2020) surveyed how deep learning techniques were used to
address the challenges in multiagent learning, such as partial observability,
continuous state and action spaces, and transfer learning. OroojlooyJadid and
Hajinezhad (2019) reviewed the application of deep MARL techniques in fully
cooperative games: the survey on this setting is thorough. Hernandez-Leal
et al. (2019) summarised how the classic ideas from traditional MAS research,
such as emergent behaviour, learning communication, and opponent modelling,
were incorporated into deep MARL domains, based on which they proposed a
new categorisation for deep MARL methods. Zhang et al. (2019b) performed
a selective survey on MARL algorithms that have theoretical convergence
guarantees and complexity analysis. To the best of my knowledge, their review
is the only one to cover more advanced topics such as decentralised MARL
with networked agents, mean-field MARL, and MARL for stochastic potential
games.
On the application side, Müller and Fischer (2014) surveyed 152 realworld applications in various sectors powered by MAS techniques. CamposRodriguez et al. (2017) reviewed the application of multiagent techniques
for automotive industry applications, such as traffic coordination and route
balancing. Derakhshan and Yousefi (2019) focused on real-world applications
for wireless sensor networks, Shakshuki and Reid (2015) studied multiagent
applications for the healthcare industry, and Kober et al. (2013) investigated
the application of robotic control and summarised profitable RL approaches
that can be applied to robots in the real world.

6
Learning in Identical-Interest Games

The majority of MARL algorithms assume that agents collaborate with
each other to achieve shared goals. In this setting, agents are usually considered
homogeneous and play an interchangeable role in the environmental dynamics.
In a two-player normal-form game or repeated game, for example, this means
the payoff matrix is symmetrical.
6.1

Stochastic Team Games

One benefit of studying identical interest games is that single-agent RL
algorithms with a theoretical guarantee can be safely applied. For example,
in the team game 1 setting, since all agents’ rewards are always the same, the
Q-functions are identical among all agents. As a result, one can simply apply
the single-agent RL algorithms over the joint action space a ∈ A , equivalently,
Eq. (3.3) can be written as
evali









Qi (st+1 , ·) i∈{1,...,N } = V i st+1 , arg max Qi st+1 , a . (6.1)
a∈A

Littman (1994) first studied this approach in SGs. However, one issue with
this approach is that when multiple equilibria exist (e.g., a normal-form game
1. The terms Markov team games, stochastic team games, and dynamic team games are
interchangeably used across different domains of the literature.

59

Learning in Identical-Interest Games

60

with reward R =

 0, 0 2, 2 

), unless the selection process is coordinated
2, 2 0, 0
among agents, the agents’ optimal policy can end up with a worse scenario
even though their value functions have reached the optimal values. To address
this issue, Claus and Boutilier (1998a) proposed to build belief models about
other agents’ policies. Similar to fictitious play Berger (2007), each agent
chooses actions in accordance with its belief about the other agents. Empirical
effectiveness, as well as convergence, have been reported for repeated games;
however, the convergent equilibrium may not be optimal. In solving this
problem, Wang and Sandholm (2003b) proposed optimal adaptive learning
(OAL) methods that provably converge to the optimal NE almost surely in
any team SG. The main novelty of OAL is that it learns the game structure
by building so-called weakly acyclic games that eliminate all the joint actions
with sub-optimal NE values and then applies adaptive play Young (1993) to
address the equilibrium selection problem for weakly acyclic games specifically.
Following this approach, Arslan and Yüksel (2016) proposed decentralised
Q-learning algorithms that, under the help of two-timescale analysis Leslie
et al. (2003), converge to an equilibrium policy for weakly acyclic SGs. To
avoid sub-optimal equilibria for weakly acyclic SGs, Yongacoglu et al. (2019)
further refined the decentralised Q-learners and derived theorems with stronger
almost-surely convergence guarantees for optimal policies.
Solutions via Q-function Factorisation

Another vital reason that team games have been repeatedly studied is that
solving team games is a crucial step in building distributed AI (DAI) Huhns
(2012); Gasser and Huhns (2014). The logic is that if each agent only needs
to maintain the Q-function of Qi (s, ai ), which depends on the state and local
action ai , rather than joint action a, then the combinatorial nature of multiagent
problems can be avoided. Unfortunately, Tan (1993) previously noted that such
independent Q-learning methods do not converge in team games. Lauer and
Riedmiller (2000) reported similar negative results; however, when the state
transition dynamics are deterministic, independent learning through distributed
Q-learning can still obtain a convergence guarantee. No additional expense is
needed in comparison to the non-distributed case for computing the optimal
policies.

6.1. Stochastic Team Games

61

Factorised MDPs Boutilier et al. (1999) are an effective way to avoid
exponential blowups. For a coordination task, if the joint-Q function can be
naturally written as
Q = Q1 (a1 , a2 ) + Q2 (a2 , a4 ) + Q3 (a1 , a3 ) + Q4 (a3 , a4 ),
then the nested structure can be exploited. For example, Q1 and Q3 are
irrelevant in finding the optimal a4 ; thus, given a4 , Q1 becomes irrelevant
for optimising a3 . Given a3 , a4 , one can then optimise a1 , a2 . Inspired by
this result, Guestrin et al. (2002b,a); Kok and Vlassis (2004) studied the idea
of coordination graphs, which combine value function approximation with
a message-passing scheme by which agents can efficiently find the globally
optimal joint action.
However, the coordination graph may not always be available in real-world
applications; thus, the ideal approach is to let agents learn the Q-function
factorisation from the tasks automatically. Deep neural networks are an
effective way to learn such factorisations. Specifically, the scope of the
problem is then narrowed to the so-called decentralisable tasks in the Dec
POMDP setting, that is, ∃ Qi i∈{1,...,N } ∀o ∈ O , a ∈ A , the following
condition holds.


arg max Qπ (o, a) = 



a

arg maxa1 Q1 (o1 , a1 )
..
.
arg maxaN

QN (oN , aN )



.


(6.2)

Eq. (6.2) suggests that a task is decentralisable only if the local maxima on
the individual value function per every agent amounts to the global maximum
on the joint value function. Different structural constraints, enforced by
particular neural architectures, have been proposed to satisfy this condition.
For example, VDN Sunehag et al. (2018) maintains an additivity structure
P
i i i
by making Qπ (o, a) := N
adopts
i=1 Q (o , a ). QMIX Rashid et al. (2018)
∂Qπ (o,a)
a monotonic structure by means of a mixing network to ensure ∂Q
i (oi ,ai ) ≥
0, ∀i ∈ {1, ..., N }. QTRAN Son et al. (2019) introduces a more rigorous
learning objective on top of QMIX that proves to be a sufficient condition for
Eq. (6.2). However, these structure constraints heavily depend on specially
designed neural architectures, which makes understanding the representational
power (i.e., the approximation error) of the above methods almost infeasible.
Another drawback is that the structure constraint also damages agents’ efficient

Learning in Identical-Interest Games

62

Recursive Reasoning Step

a

⇢0 i a i |s
<latexit sha1_base64="YfEiQmgWA7DE6xGzDf3XZI1BYjs=">AAACM3icdVDLSsNAFJ3UV62vqEs3g1WoC0sigi4LunBZwT6giWEynbRDJw9mboQS+wN+jRsX+ifiTty6d+mk7cK2eGDgcM69d+49fiK4Ast6NwpLyyura8X10sbm1vaOubvXVHEqKWvQWMSy7RPFBI9YAzgI1k4kI6EvWMsfXOV+64FJxePoDoYJc0PSi3jAKQEteeaRI/uxl1mj++yUjxzBAqiQMcePWDmS9/pw4pllq2qNgReJPSVlNEXdM3+cbkzTkEVABVGqY1sJuBmRwKlgo5KTKpYQOiA91tE0IiFTbja+ZoSPtdLFQSz1iwCP1b8dGQmVGoa+rgwJ9NW8l4v/eflENfN/5odz+0Bw6WY8SlJgEZ2sE6QCQ4zzAHGXS0ZBDDUhVHJ9EaZ9IgkFHXNJR2XPB7NImmdV26rat+fl2vU0tCI6QIeogmx0gWroBtVRA1H0hJ7RK3ozXowP49P4mpQWjGnPPpqB8f0Ldpmrhw==</latexit>

/

.. ⇡ a s
.
i
i
ai0 ⇡0 a |s

i

aik 2 ⇡k
<latexit sha1_base64="g9M0JWbFW3j/1SRCbZXItt0KNcU=">AAACFnicdVDLSgMxFL3js9ZX1aWbYBHcWGaKoCspuHFZwT6kHUsmzbShSWZIMkIZ+hVuXOivuBO3bv0Tl2baWdgWDwQO59ybnJwg5kwb1/12VlbX1jc2C1vF7Z3dvf3SwWFTR4kitEEiHql2gDXlTNKGYYbTdqwoFgGnrWB0k/mtJ6o0i+S9GcfUF3ggWcgINlZ6wI+sl47Oq5NeqexW3CnQMvFyUoYc9V7pp9uPSCKoNIRjrTueGxs/xcowwumk2E00jTEZ4QHtWCqxoNpPp4En6NQqfRRGyh5p0FT9u5FiofVYBHZSYDPUi14m/udlN+q599NALOQx4ZWfMhknhkoyixMmHJkIZR2hPlOUGD62BBPF7I8QGWKFibFNFm1V3mIxy6RZrXhuxbu7KNeu89IKcAwncAYeXEINbqEODSAg4Ble4c15cd6dD+dzNrri5DtHMAfn6xdRfKAl</latexit>

<latexit sha1_base64="aYxib1cqzouGMIb9ZTQGOEdxcOE=">AAACOnicdZC7SgNBFIZn4y3GW9TSZjAIEU3YDYKWAS0sI5gLJOsyO5lNhsxemDkrhDUP4dPYWOhj2NqJrYWls8kWJuKBgY//XOac340EV2Cab0ZuaXlldS2/XtjY3NreKe7utVQYS8qaNBSh7LhEMcED1gQOgnUiyYjvCtZ2R5dpvn3PpOJhcAvjiNk+GQTc45SAlpziSS/iTjKq1CZ3CZ/0BPOgTFLED1idYo0VLUs+GMKxUyyZVXMa+C9YGZRQFg2n+N3rhzT2WQBUEKW6lhmBnRAJnAo2KfRixSJCR2TAuhoD4jNlJ9OjJvhIK33shVK/APBU/d2REF+pse/qSp/AUC3mUvG/XDpRzf2fuP7CPuBd2AkPohhYQGfreLHAEOLUR9znklEQYw2ESq4vwnRIJKGg3S5oq6xFY/5Cq1a1zKp1c1aqX2Wm5dEBOkRlZKFzVEfXqIGaiKJH9IRe0KvxbLwbH8bnrDRnZD37aC6Mrx/iHa42</latexit>

<latexit sha1_base64="fXsO6W2Kr52HKcDwC/eFo7vx5J4=">AAACCnicbZC7SgNBFIbPxluMt6ilzWgQIkjYtdEyoIVlBHOBbAyzk9lkyOyFmbNCWFOKja9iY6GIrU9gZ+ejOLkUmvjDwMd/zuHM+b1YCo22/WVlFhaXlleyq7m19Y3Nrfz2Tk1HiWK8yiIZqYZHNZci5FUUKHkjVpwGnuR1r38+qtdvudIiCq9xEPNWQLuh8AWjaKx2ft+NRTu1hzepGLqS+1ikIyR3RLtKdHt41M4X7JI9FpkHZwqF8rF//w0AlXb+0+1ELAl4iExSrZuOHWMrpQoFk3yYcxPNY8r6tMubBkMacN1Kx6cMyaFxOsSPlHkhkrH7eyKlgdaDwDOdAcWenq2NzP9qzQT9s1YqwjhBHrLJIj+RBCMyyoV0hOIM5cAAZUqYvxLWo4oyNOnlTAjO7MnzUDspOXbJuTJpXMBEWdiDAyiCA6dQhkuoQBUYPMATvMCr9Wg9W2/W+6Q1Y01nduGPrI8fM3mcxg==</latexit>
sha1_base64="EfL1Aw5buNvLhLfAQFlxmRgx6EE=">AAACCnicbZC7TsMwFIYdrqXcAowshgqpLFXCAmMlGBiLRC9SEyLHdVqrzkX2CVIVMrPwKiwMIMTKE7DxNjhtBmj5JUuf/nOOjs/vJ4IrsKxvY2l5ZXVtvbJR3dza3tk19/Y7Kk4lZW0ai1j2fKKY4BFrAwfBeolkJPQF6/rjy6LevWdS8Ti6hUnC3JAMIx5wSkBbnnnkJNzLrPwu47kjWAB1UiB+wMqRfDiCU8+sWQ1rKrwIdgk1VKrlmV/OIKZpyCKggijVt60E3IxI4FSwvOqkiiWEjsmQ9TVGJGTKzaan5PhEOwMcxFK/CPDU/T2RkVCpSejrzpDASM3XCvO/Wj+F4MLNeJSkwCI6WxSkAkOMi1zwgEtGQUw0ECq5/iumIyIJBZ1eVYdgz5+8CJ2zhm017Bur1rwq4
sha1_base64="hjjfFYl6QE9sJaG1StQeKnz7oEs=">AAACCnicbZC7SgNBFIZn4y3G26qlzWgQIkjYtdEyoIVlBHOB7BpmJ7PJkNkLM2cDYU0pNr6KjYIiWvoEduLLOJuk0OgPAx//OYcz5/diwRVY1qeRm5tfWFzKLxdWVtfWN8zNrbqKEklZjUYikk2PKCZ4yGrAQbBmLBkJPMEaXv80qzcGTCoehZcwjJkbkG7IfU4JaKtt7joxb6fW6CrlI0cwH0okQ3yNlSN5twcHbbNola2x8F+wp1CsHPo3X4PHt2rb/HA6EU0CFgIVRKmWbcXgpkQCp4KNCk6iWExon3RZS2NIAqbcdHzKCO9rp4P9SOoXAh67PydSEig1DDzdGRDoqdlaZv5XayXgn7gpD+MEWEgni/xEYIhwlgvucMkoiKEGQiXXf8W0RyShoNMr6BDs2ZP/Qv2obFtl+0KncYYmyqMdtIdKyEbHqILOURXVEEW36B49oWfjzngwXozXSWvOmM5so18y3r8BtIeepg==</latexit>

<latexit sha1_base64="v6xVz1RbnV1VYMdSU0sdvKCFJYU=">AAACFHicdVDLSgMxFL1TX7W+Rl26CRbBVZkRQZcFXbisYB/QjiWTZtrQJDMkGaEM/Qg3LvRX3Ilb9/6JSzPtLGyLBwKHc+5NTk6YcKaN5307pbX1jc2t8nZlZ3dv/8A9PGrpOFWENknMY9UJsaacSdo0zHDaSRTFIuS0HY5vcr/9RJVmsXwwk4QGAg8lixjBxkpt/Jixad/ru1Wv5s2AVolfkCoUaPTdn94gJqmg0hCOte76XmKCDCvDCKfTSi/VNMFkjIe0a6nEguogm8WdojOrDFAUK3ukQTP170aGhdYTEdpJgc1IL3u5+J+X36gX3s9CsZTHRNdBxmSSGirJPE6UcmRilDeEBkxRYvjEEkwUsz9CZIQVJsb2WLFV+cvFrJLWRc33av79ZbV+W5RWhhM4hXPw4QrqcAcNaAKBMTzDK7w5L8678+F8zkdLTrFzDAtwvn4BA8GffQ==</latexit>

i

a i |s, ai

2

ai |s, a i

ak i 1 ⇢k i 1 a i |s, ai

a0 i ⇢0 i a i |s

a 1 i ⇢1

ai1 ⇡1i ai |s, a i

ai2 ⇡2i ai |s, a i

aik ⇡ki ai |s, a i

s

s

s

a i

a i

a i

Oi

Oi

Oi

Level 1

Level 2

<latexit sha1_base64="609iJVpj636NCGPnCMPfJta3EFk=">AAACFXicdVDLSgMxFL1TX7W+qi7dBIvgxjIjBV0WdOGygn1AO5ZMmmlDk8yQZIQy9CfcuNBfcSduXfsnLs20s7AtHggczrk3OTlBzJk2rvvtFNbWNza3itulnd29/YPy4VFLR4kitEkiHqlOgDXlTNKmYYbTTqwoFgGn7WB8k/ntJ6o0i+SDmcTUF3goWcgINlbq4L77mF6wab9ccavuDGiVeDmpQI5Gv/zTG0QkEVQawrHWXc+NjZ9iZRjhdFrqJZrGmIzxkHYtlVhQ7aezvFN0ZpUBCiNljzRopv7dSLHQeiICOymwGellLxP/87Ib9cL7aSCW8pjw2k+ZjBNDJZnHCROOTISyitCAKUoMn1iCiWL2R4iMsMLE2CJLtipvuZhV0rqsem7Vu69V6rd5aUU4gVM4Bw+uoA530IAmEODwDK/w5rw4786H8zkfLTj5zjEswPn6BXSan7Q=</latexit>

<latexit sha1_base64="kKSlYWmM25OAMo1hxTKvjwyREZ4=">AAACDXicbZC7SwNBEMbn4ivGV9TSZjEKETTc2WgZ0MIygnlALoa9zV6yuPdgd04IZ0obG/8VGwtFbO3t7PxT3DwKTfxg4cc3M8zO58VSaLTtLyszN7+wuJRdzq2srq1v5De3ajpKFONVFslINTyquRQhr6JAyRux4jTwJK97N2fDev2WKy2i8Ar7MW8FtBsKXzCKxmrn91zVi9qpPbhOj8TAldzHIh0xuSPaVaLbw4N2vmCX7JHILDgTKJQP/ftvAKi0859uJ2JJwENkkmrddOwYWylVKJjkg5ybaB5TdkO7vGkwpAHXrXR0zYDsG6dD/EiZFyIZub8nUhpo3Q880xlQ7Onp2tD8r9ZM0D9tpSKME+QhGy/yE0kwIsNoSEcozlD2DVCmhPkrYT2qKEMTYM6E4EyfPAu145Jjl5xLk8Y5jJWFHdiFIjhwAmW4gApUgcEDPMELvFqP1rP1Zr2PWzPWZGYb/sj6+AH23p2u</latexit>
sha1_base64="+zHCE7TyX8Rbfts57ErQD1qMKv4=">AAACDXicbZC7TsMwFIadcivlFmBksShIZaBKWGCsBANjkehFakLluE5j1bnIPkGqQl6AhVdhYQAhVnY23ga3zQAtv2Tp03/O0fH5vURwBZb1bZSWlldW18rrlY3Nre0dc3evreJUUtaisYhl1yOKCR6xFnAQrJtIRkJPsI43upzUO/dMKh5HtzBOmBuSYcR9Tgloq28eOTKI+5mV32WnPHcE86FGpowfsHIkHwZw0jerVt2aCi+CXUAVFWr2zS9nENM0ZBFQQZTq2VYCbkYkcCpYXnFSxRJCR2TIehojEjLlZtNrcnysnQH2Y6lfBHjq/p7ISKjUOPR0Z0ggUPO1iflfrZeCf+FmPEpSYBGdLfJTgSHGk2jwgEtGQYw1ECq5/iumAZGEgg6wokOw509ehPZZ3bbq9o1VbVwVcZTRATpENWSjc9RA16iJWoiiR/SMXtGb8WS8GO/Gx6y1ZBQz++iPjM8f+fSbdw==</latexit>
sha1_base64="bjhOn6sUAfnlL6oWE5poc8qb9TI=">AAACDXicbZA7SwNBFIVn4yvGV9TSZjAKETTs2mgZ0MIygnlANi6zk9lkyOyDmbuBsG5pY+NfEcRCCbb2duKfcfIoNPHAwMe593LnHjcSXIFpfhmZhcWl5ZXsam5tfWNzK7+9U1NhLCmr0lCEsuESxQQPWBU4CNaIJCO+K1jd7V2M6vU+k4qHwQ0MItbySSfgHqcEtOXkD2zZDZ3ETG+TE57agnlQJGPGd1jZkne6cOTkC2bJHAvPgzWFQvnYu//uPw8rTv7Tboc09lkAVBClmpYZQSshEjgVLM3ZsWIRoT3SYU2NAfGZaiXja1J8qJ029kKpXwB47P6eSIiv1MB3dadPoKtmayPzv1ozBu+8lfAgioEFdLLIiwWGEI+iwW0uGQUx0ECo5PqvmHaJJBR0gDkdgjV78jzUTkuWWbKudRqXaKIs2kP7qIgsdIbK6ApVUBVR9ICe0Ct6Mx6NF2NovE9aM8Z0Zhf9kfHxA3f7n44=</latexit>

<latexit sha1_base64="thW0T5sDJbLU1fn0dVLiwwCu1Ec=">AAACFHicbZC7SgNBFIbPxluMt6ilzWAQIsawa6NlQAvLCOYCSQyzk9lkyOyFmbNCWFP6ADa+io2FIrYWdnY+ipNLoYk/DHz85xzOnN+NpNBo219WamFxaXklvZpZW9/Y3Mpu71R1GCvGKyyUoaq7VHMpAl5BgZLXI8Wp70pec/vno3rtlistwuAaBxFv+bQbCE8wisZqZ4+aqhe2E2d4kxyLYVNyD/N0zOSO6AIxbGwluj08bGdzdtEei8yDM4VcqeDdfwNAuZ39bHZCFvs8QCap1g3HjrCVUIWCST7MNGPNI8r6tMsbBgPqc91KxkcNyYFxOsQLlXkBkrH7eyKhvtYD3zWdPsWenq2NzP9qjRi9s1YigihGHrDJIi+WBEMySoh0hOIM5cAAZUqYvxLWo4oyNDlmTAjO7MnzUD0pOnbRuTJpXMBEadiDfciDA6dQgksoQwUYPMATvMCr9Wg9W2/W+6Q1ZU1nduGPrI8fC6egYQ==</latexit>
sha1_base64="Vo3rLb/SxKzsgfHjt9bkDQffgNQ=">AAACFHicbZDLSsNAFIYn9VbrrerSzWARKmpJ3OiyoAuXFewFmhgm00kzOLkwcyKUmIdw46u4caGIWxfufBunbRba+sPAx3/O4cz5vURwBab5bZQWFpeWV8qrlbX1jc2t6vZOR8WppKxNYxHLnkcUEzxibeAgWC+RjISeYF3v7mJc794zqXgc3cAoYU5IhhH3OSWgLbd6ZMsgdjMrv81OeG4L5kOdTBg/YHWMNWtb8mEAh261ZjbMifA8WAXUUKGWW/2yBzFNQxYBFUSpvmUm4GREAqeC5RU7VSwh9I4MWV9jREKmnGxyVI4PtDPAfiz1iwBP3N8TGQmVGoWe7gwJBGq2Njb/q/VT8M+djEdJCiyi00V+KjDEeJwQHnDJKIiRBkIl13/FNCCSUNA5VnQI1uzJ89A5bVhmw7o2a83LIo4y2kP7qI4sdIaa6Aq1UBtR9Iie0St6M56MF+Pd+Ji2loxiZhf9kfH5Aw69nio=</latexit>
sha1_base64="cSWNcDNTKjRshx3UQUIhvcIyVmw=">AAACFHicbZC7SgNBFIZn4y3G26qlzWAQIsawa6NlQAvLCOYCSQyzk9lkyOyFmbOBsG7pA9j4IhY2KRSxtbATX8bJJoVGfxj4+M85nDm/EwquwLI+jczC4tLySnY1t7a+sbllbu/UVBBJyqo0EIFsOEQxwX1WBQ6CNULJiOcIVncG55N6fcik4oF/DaOQtT3S87nLKQFtdcyjluwHndhObuJjnrQEc6FAUsa3WBWxZm1L3uvDYcfMWyUrFf4L9gzy5aJ79zV8HFc65kerG9DIYz5QQZRq2lYI7ZhI4FSwJNeKFAsJHZAea2r0icdUO06PSvCBdrrYDaR+PuDU/TkRE0+pkefoTo9AX83XJuZ/tWYE7lk75n4YAfPpdJEbCQwBniSEu1wyCmKkgVDJ9V8x7RNJKOgcczoEe/7kv1A7KdlWyb7SaVygqbJoD+2jArLRKSqjS1RBVUTRPXpCz+jFeDDGxqvxNm3NGLOZXfRLxvs3jLWiQQ==</latexit>

<latexit sha1_base64="Rq8EZ8MmegtItEmbbsZR/fL633s=">AAACFXicdVDLSgMxFL1TX7W+qi7dBIvgxjIjBV0WdOGygn1AO5ZMmmlDk8yQZIQy9CfcuNBfcSduXfsnLs20s7AtHggczrk3OTlBzJk2rvvtFNbWNza3itulnd29/YPy4VFLR4kitEkiHqlOgDXlTNKmYYbTTqwoFgGn7WB8k/ntJ6o0i+SDmcTUF3goWcgINlbq4L73mF6wab9ccavuDGiVeDmpQI5Gv/zTG0QkEVQawrHWXc+NjZ9iZRjhdFrqJZrGmIzxkHYtlVhQ7aezvFN0ZpUBCiNljzRopv7dSLHQeiICOymwGellLxP/87Ib9cL7aSCW8pjw2k+ZjBNDJZnHCROOTISyitCAKUoMn1iCiWL2R4iMsMLE2CJLtipvuZhV0rqsem7Vu69V6rd5aUU4gVM4Bw+uoA530IAmEODwDK/w5rw4786H8zkfLTj5zjEswPn6BXZJn7U=</latexit>

<latexit sha1_base64="Nr4hK/Vg2YgSg5sZgbJEnDq5oUs=">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBS8GHa96DGgB48RzAOSNfROZpMhM7PLzKwQlnyEFw+KePV7vPk3Th4HTSxoKKq66e6KUsGN9f1vb2V1bX1js7BV3N7Z3dsvHRw2TJJpyuo0EYluRWiY4IrVLbeCtVLNUEaCNaPhzcRvPjFteKIe7ChlocS+4jGnaJ3UxMecj7tBt1T2K/4UZJkEc1Kunl7EBABq3dJXp5fQTDJlqUBj2oGf2jBHbTkVbFzsZIalSIfYZ21HFUpmwnx67picOaVH4kS7UpZM1d8TOUpjRjJynRLtwCx6E/E/r53Z+DrMuUozyxSdLYozQWxCJr+THteMWjFyBKnm7lZCB6iRWpdQ0YUQLL68TBqXlcCvBPcujVuYoQDHcALnEMAVVOEOalAHCkN4hld481LvxXv3PmatK9585gj+wPv8AaHzkIU=</latexit>
sha1_base64="yovDPZ91hhc80/tgFClkE/BH5VU=">AAAB7nicbVA9SwNBEJ2LXzF+RS1tFoNgFW5ttAxoYRnBJEJyhr3NXLJkb+/Y3RPCkR9hY6GIrb/Hzn/jJrlCEx8MPN6bYWZemEphrO9/e6W19Y3NrfJ2ZWd3b/+genjUNkmmObZ4IhP9EDKDUihsWWElPqQaWRxK7ITj65nfeUJtRKLu7STFIGZDJSLBmXVShz3mYtqn/WrNr/tzkFVCC1KDAs1+9as3SHgWo7JcMmO61E9tkDNtBZc4rfQygynjYzbErqOKxWiCfH7ulJw5ZUCiRLtSlszV3xM5i42ZxKHrjJkdmWVvJv7ndTMbXQW5UGlmUfHFoiiTxCZk9jsZCI3cyokjjGvhbiV8xDTj1iVUcSHQ5ZdXSfuiTv06vfNrjZsijjKcwCmcA4VLaMAtNKEFHMbwDK/w5qXei/fufSxaS14xcwx/4H3+ACjQj24=</latexit>
sha1_base64="MgwRdUksmCiNCh+jaUtNXa897+M=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBqPoxbDrRY8BFTxGMA9I1jA7mU2GzM4uM71CWPMRXjwo4tWP8eTNv3HyOGi0oKGo6qa7K0ikMOi6X05uYXFpeSW/Wlhb39jcKm7v1E2casZrLJaxbgbUcCkUr6FAyZuJ5jQKJG8Eg4ux37jn2ohY3eIw4X5Ee0qEglG0UoPeZWLU8TrFklt2JyB/iTcjpcrBSXj0cPVR7RQ/292YpRFXyCQ1puW5CfoZ1SiY5KNCOzU8oWxAe7xlqaIRN342OXdEDq3SJWGsbSkkE/XnREYjY4ZRYDsjin0z743F/7xWiuG5nwmVpMgVmy4KU0kwJuPfSVdozlAOLaFMC3srYX2qKUObUMGG4M2//JfUT8ueW/ZubBqXMEUe9mAfjsGDM6jANVShBgwG8AjP8OIkzpPz6rxNW3PObGYXfsF5/wa9h5IZ</latexit>

<latexit sha1_base64="LfdokDIBLoecuI60kL7PaBTugVY=">AAACFHicdVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuCLlxWsA9ox5JJM22YJDMkGaEM/Qg3LvRX3Ilb9/6JSzPtLGyLBwKHc+5NTk6QcKaN6347pbX1jc2t8nZlZ3dv/6B6eNTWcaoIbZGYx6obYE05k7RlmOG0myiKRcBpJ4hucr/zRJVmsXwwk4T6Ao8kCxnBxkod/MgGWTQdVGtu3Z0BrRKvIDUo0BxUf/rDmKSCSkM41rrnuYnxM6wMI5xOK/1U0wSTCI9oz1KJBdV+Nos7RWdWGaIwVvZIg2bq340MC60nIrCTApuxXvZy8T8vv1EvvJ8FYimPCa/9jMkkNVSSeZww5cjEKG8IDZmixPCJJZgoZn+EyBgrTIztsWKr8paLWSXti7rn1r37y1rjtiitDCdwCufgwRU04A6a0AICETzDK7w5L8678+F8zkdLTrFzDAtwvn4BZZGfuA==</latexit>

<latexit sha1_base64="NY/bIdAKpJmH2+LzmviRtuEq6PE=">AAACOHicdZC7SgNBFIZn4y3G26qlzWAQEtCwK4KWAS0sI5gLJOsyO5lNhsxemDkrhDXP4NPYWOhz2NmJrZWls8kWJuKBgY//XOac34sFV2BZb0ZhaXllda24XtrY3NreMXf3WipKJGVNGolIdjyimOAhawIHwTqxZCTwBGt7o8ss375nUvEovIVxzJyADELuc0pAS65Z7cXcTUeTu5RPeoL5UCEZ4gesjrHGEy1LPhhC1TXLVs2aBv4Ldg5llEfDNb97/YgmAQuBCqJU17ZicFIigVPBJqVeolhM6IgMWFdjSAKmnHR60gQfaaWP/UjqFwKeqr87UhIoNQ48XRkQGKrFXCb+l8smqrn/Uy9Y2Af8CyflYZwAC+lsHT8RGCKcuYj7XDIKYqyBUMn1RZgOiSQUtNclbZW9aMxfaJ3WbKtm35yV61e5aUV0gA5RBdnoHNXRNWqgJqLoET2hF/RqPBvvxofxOSstGHnPPpoL4+sH5uatww==</latexit>

<latexit sha1_base64="TbiNGgqKNpzlAB/7RIEZeg5HAIc=">AAACOHicdZC7SgNBFIZn4y3G26qlzWAQEtCwGwQtA1pYRjAXSNYwO5lNhsxemDkrhDXP4NPYWOhz2NmJrZWls8kWJsEDAx//ucw5vxsJrsCy3o3cyura+kZ+s7C1vbO7Z+4fNFUYS8oaNBShbLtEMcED1gAOgrUjyYjvCtZyR1dpvvXApOJhcAfjiDk+GQTc45SAlnpmuRvxXlKd3Cd80hXMgxJJET9idYo1nmlZ8sEQyj2zaFWsaeBlsDMooizqPfOn2w9p7LMAqCBKdWwrAichEjgVbFLoxopFhI7IgHU0BsRnykmmJ03wiVb62AulfgHgqfq3IyG+UmPf1ZU+gaFazKXif7l0opr7P3H9hX3Au3QSHkQxsIDO1vFigSHEqYu4zyWjIMYaCJVcX4TpkEhCQXtd0FbZi8YsQ7Nasa2KfXterF1npuXRETpGJWSjC1RDN6iOGoiiJ/SMXtGb8WJ8GJ/G16w0Z2Q9h2gujO9fgEGtig==</latexit>

<latexit sha1_base64="(null)">(null)</latexit>

<latexit sha1_base64="OwsedO/aZM2lY25YD1mglnKDFLY=">AAACEHicdVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuCLtxZ0T6gHUsmzbShSWZIMkIZ+gluXOivuBO3/oF/4tJMOwvb4oHA4Zx7k5MTxJxp47rfTmFldW19o7hZ2tre2d0r7x80dZQoQhsk4pFqB1hTziRtGGY4bceKYhFw2gpGV5nfeqJKs0g+mHFMfYEHkoWMYGOl+9tH1itX3Ko7BVomXk4qkKPeK/90+xFJBJWGcKx1x3Nj46dYGUY4nZS6iaYxJiM8oB1LJRZU++k06gSdWKWPwkjZIw2aqn83Uiy0HovATgpshnrRy8T/vOxGPfd+GoiFPCa89FMm48RQSWZxwoQjE6GsHdRnihLDx5Zgopj9ESJDrDAxtsOSrcpbLGaZNM+qnlv17s4rteu8tCIcwTGcggcXUIMbqEMDCAzgGV7hzXlx3p0P53M2WnDynUOYg/P1C78Hnbw=</latexit>

<latexit sha1_base64="PVGIk3ZvTz8t3j+00T50LIiSO0A=">AAACFnicbZC7SgNBFIbPeo1RY9TSZjAIEUzc1ULLgBaWEcwFkhhmJ7PJkNkLM2eFsAZ8BxtfxcZCEVux822cXApN/GHg4z/ncOb8biSFRtv+thYWl5ZXVlNr6fWNzcxWdnunqsNYMV5hoQxV3aWaSxHwCgqUvB4pTn1X8prbvxjVa3dcaREGNziIeMun3UB4glE0VjtbaKpe2E76BWd4mxTEsCm5h3k6ZnJP9BExbGwluj08bGdzdtEei8yDM4Vc6fg08wAA5Xb2q9kJWezzAJmkWjccO8JWQhUKJvkw3Yw1jyjr0y5vGAyoz3UrGZ81JAfG6RAvVOYFSMbu74mE+loPfNd0+hR7erY2Mv+rNWL0zluJCKIYecAmi7xYEgzJKCPSEYozlAMDlClh/kpYjyrK0CSZNiE4syfPQ/Wk6NhF59qkcQkTpWAP9iEPDpxBCa6gDBVg8AjP8Apv1pP1Yr1bH5PWBWs6swt/ZH3+APXNoAY=</latexit>
sha1_base64="s60FJ83mo5MXdPfxZbjt1nzXZys=">AAACFnicbZDLSsNAFIYnXmu9RV26GSxCBVsSN7os6MJlBXuBNpbJdNIMnVyYORFKzFO48VXcuFDErbjzbZymWWjrDwMf/zmHM+d3Y8EVWNa3sbS8srq2Xtoob25t7+yae/ttFSWSshaNRCS7LlFM8JC1gINg3VgyEriCddzx5bTeuWdS8Si8hUnMnICMQu5xSkBbA7PWl340SMc1O7tLazzrC+ZBleSMH7A6xZq1LfnIh5OBWbHqVi68CHYBFVSoOTC/+sOIJgELgQqiVM+2YnBSIoFTwbJyP1EsJnRMRqynMSQBU06an5XhY+0MsRdJ/ULAuft7IiWBUpPA1Z0BAV/N16bmf7VeAt6Fk/IwToCFdLbISwSGCE8zwkMuGQUx0UCo5PqvmPpEEgo6ybIOwZ4/eRHaZ3Xbqts3VqVxVcRRQofoCFWRjc5RA12jJmohih7RM3pFb8aT8WK8Gx+z1iWjmDlAf2R8/gBbPZ7W</latexit>
sha1_base64="KsjTEH21cKvfL84tzaPJY8VoR9Y=">AAACFnicbZC7SgNBFIZn4y1GjVFLm8EgRDBxVwstA1pYRjAXyMYwO5lNhsxemDkrhHWx8BFsfAEfwsZCEVux80HsnVwKTfxh4OM/53Dm/E4ouALT/DJSc/MLi0vp5czK6lp2PbexWVNBJCmr0kAEsuEQxQT3WRU4CNYIJSOeI1jd6Z8O6/VrJhUP/EsYhKzlka7PXU4JaKudK9qyF7TjftFKruIiT2zBXCiQEeMbrPaxZm1L3u3BXjuXN0vmSHgWrAnkywdH2dvvu8dKO/dpdwIaecwHKohSTcsMoRUTCZwKlmTsSLGQ0D7psqZGn3hMteLRWQne1U4Hu4HUzwc8cn9PxMRTauA5utMj0FPTtaH5X60ZgXvSirkfRsB8Ol7kRgJDgIcZ4Q6XjIIYaCBUcv1XTHtEEgo6yYwOwZo+eRZqhyXLLFkXOo0zNFYabaMdVEAWOkZldI4qqIooukdP6AW9Gg/Gs/FmvI9bU8ZkZgv9kfHxA8ewoiI=</latexit>

<latexit sha1_base64="uYufwldEp1Q9hQTvjLDGJxdpgQg=">AAACFHicdVDLSgMxFL1TX7W+qi7dBIvgqswUoS4LunBZwT6gHUsmzbShSWZIMkIZ+hFuXOivuBO37v0Tl2baWdgWDwQO59ybnJwg5kwb1/12ChubW9s7xd3S3v7B4VH5+KSto0QR2iIRj1Q3wJpyJmnLMMNpN1YUi4DTTjC5yfzOE1WaRfLBTGPqCzySLGQEGyt18KD2mLLZoFxxq+4caJ14OalAjuag/NMfRiQRVBrCsdY9z42Nn2JlGOF0VuonmsaYTPCI9iyVWFDtp/O4M3RhlSEKI2WPNGiu/t1IsdB6KgI7KbAZ61UvE//zshv10vtpIFbymPDaT5mME0MlWcQJE45MhLKG0JApSgyfWoKJYvZHiIyxwsTYHku2Km+1mHXSrlU9t+rdX1Uat3lpRTiDc7gED+rQgDtoQgsITOAZXuHNeXHenQ/nczFacPKdU1iC8/ULBduffw==</latexit>

<latexit sha1_base64="YdDGQ30EVswBQqIw/bE3BURGEc8=">AAACOHicdZC7SgNBFIZn4y3G26qlzWAQEtCwK4KWAS0sI5gLJDHMTmaTITO7y8xZIax5Bp/GxkKfw85ObK0snU22MBEPDHz85zLn/F4kuAbHebNyS8srq2v59cLG5tb2jr2719BhrCir01CEquURzQQPWB04CNaKFCPSE6zpjS7TfPOeKc3D4BbGEetKMgi4zykBI/XscifivcSd3CV80hHMhxJJET9gfYwNnhhZ8cEQyj276FScaeC/4GZQRFnUevZ3px/SWLIAqCBat10ngm5CFHAq2KTQiTWLCB2RAWsbDIhkuptMT5rgI6P0sR8q8wLAU/V3R0Kk1mPpmUpJYKgXc6n4Xy6dqOf+Tzy5sA/4F92EB1EMLKCzdfxYYAhx6iLuc8UoiLEBQhU3F2E6JIpQMF4XjFXuojF/oXFacZ2Ke3NWrF5lpuXRATpEJeSic1RF16iG6oiiR/SEXtCr9Wy9Wx/W56w0Z2U9+2gurK8ffnStiQ==</latexit>

<latexit sha1_base64="YoL6T8x9yeecPWqD+hHItotmbmU=">AAAB7XicbZDLSgMxFIbP1Futt1GXboJFcGOZcaM7K25cVrAXaMeaSTNtbCYZkoxQhj6Dblwo4tb3EHwBd76N6WWhrT8EPv7/HHLOCRPOtPG8bye3sLi0vJJfLaytb2xuuds7NS1TRWiVSC5VI8SaciZo1TDDaSNRFMchp/WwfzHK6/dUaSbFtRkkNIhxV7CIEWysVcM32REbtt2iV/LGQvPgT6F49vnxcAsAlbb71epIksZUGMKx1k3fS0yQYWUY4XRYaKWaJpj0cZc2LQocUx1k42mH6MA6HRRJZZ8waOz+7shwrPUgDm1ljE1Pz2Yj87+smZroNMiYSFJDBZl8FKUcGYlGq6MOU5QYPrCAiWJ2VkR6WGFi7IEK9gj+7MrzUDsu+V7Jv/KK5XOYKA97sA+H4MMJlOESKlAFAnfwCM/w4kjnyXl13ialOWfaswt/5Lz/ABKnkbM=</latexit>
sha1_base64="TEn/TdjgNlUm/yJ59XV4omr87/A=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBC8GHa96DHixWME84BkDb2T2WTM7MwyMyuEJf/gxYMiXv0fb/6Nk8dBEwsaiqpuuruiVHBjff/bW1ldW9/YLGwVt3d29/ZLB4cNozJNWZ0qoXQrQsMEl6xuuRWslWqGSSRYMxreTPzmE9OGK3lvRykLE+xLHnOK1kkNfMjP+bhbKvsVfwqyTII5KcMctW7pq9NTNEuYtFSgMe3AT22Yo7acCjYudjLDUqRD7LO2oxITZsJ8eu2YnDqlR2KlXUlLpurviRwTY0ZJ5DoTtAOz6E3E/7x2ZuOrMOcyzSyTdLYozgSxikxeJz2uGbVi5AhSzd2thA5QI7UuoKILIVh8eZk0LiqBXwnu/HL1eh5HAY7hBM4ggEuowi3UoA4UHuEZXuHNU96L9+59zFpXvPnMEfyB9/kDadWO/g==</latexit>
sha1_base64="mtTisajUYuRxKQW//mCXgqALL64=">AAAB7XicbZDLSgMxFIbP1Futt6pLN8EidGOZcaM7K25cVrAXaMeSSTNtbCYZkoxQhj6Dblwo4tb3UHwBdz6Ie9PLQlt/CHz8/znknBPEnGnjul9OZmFxaXklu5pbW9/Y3Mpv79S0TBShVSK5VI0Aa8qZoFXDDKeNWFEcBZzWg/75KK/fUqWZFFdmEFM/wl3BQkawsVYNX6eHbNjOF9ySOxaaB28KhdOPt7tW8fu90s5/tjqSJBEVhnCsddNzY+OnWBlGOB3mWommMSZ93KVNiwJHVPvpeNohOrBOB4VS2ScMGru/O1IcaT2IAlsZYdPTs9nI/C9rJiY88VMm4sRQQSYfhQlHRqLR6qjDFCWGDyxgopidFZEeVpgYe6CcPYI3u/I81I5KnlvyLt1C+QwmysIe7EMRPDiGMlxABapA4Abu4RGeHOk8OM/Oy6Q040x7duGPnNcfjgqTjw==</latexit>

<latexit sha1_base64="NTtciCwc6VEi2XRpAz+JZELjkQQ=">AAACGXicdVDLSgMxFL3js9ZX1aWbYBHctMyIoMuCG5cV7APasWTSTBuaZIYkI5RhfsONC/0Vd+LWlX/i0kw7C9vigcDhnHuTkxPEnGnjut/O2vrG5tZ2aae8u7d/cFg5Om7rKFGEtkjEI9UNsKacSdoyzHDajRXFIuC0E0xuc7/zRJVmkXww05j6Ao8kCxnBxkp9/JjWWDZIJzUvG1Sqbt2dAa0SryBVKNAcVH76w4gkgkpDONa657mx8VOsDCOcZuV+ommMyQSPaM9SiQXVfjrLnKFzqwxRGCl7pEEz9e9GioXWUxHYSYHNWC97ufifl9+oF95PA7GUx4Q3fspknBgqyTxOmHBkIpTXhIZMUWL41BJMFLM/QmSMFSbGllm2VXnLxayS9mXdc+ve/VW10ShKK8EpnMEFeHANDbiDJrSAQAzP8Apvzovz7nw4n/PRNafYOYEFOF+/sr6haQ==</latexit>

<latexit sha1_base64="(null)">(null)</latexit>

<latexit sha1_base64="(null)">(null)</latexit>

<latexit sha1_base64="YoL6T8x9yeecPWqD+hHItotmbmU=">AAAB7XicbZDLSgMxFIbP1Futt1GXboJFcGOZcaM7K25cVrAXaMeaSTNtbCYZkoxQhj6Dblwo4tb3EHwBd76N6WWhrT8EPv7/HHLOCRPOtPG8bye3sLi0vJJfLaytb2xuuds7NS1TRWiVSC5VI8SaciZo1TDDaSNRFMchp/WwfzHK6/dUaSbFtRkkNIhxV7CIEWysVcM32REbtt2iV/LGQvPgT6F49vnxcAsAlbb71epIksZUGMKx1k3fS0yQYWUY4XRYaKWaJpj0cZc2LQocUx1k42mH6MA6HRRJZZ8waOz+7shwrPUgDm1ljE1Pz2Yj87+smZroNMiYSFJDBZl8FKUcGYlGq6MOU5QYPrCAiWJ2VkR6WGFi7IEK9gj+7MrzUDsu+V7Jv/KK5XOYKA97sA+H4MMJlOESKlAFAnfwCM/w4kjnyXl13ialOWfaswt/5Lz/ABKnkbM=</latexit>
sha1_base64="TEn/TdjgNlUm/yJ59XV4omr87/A=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBC8GHa96DHixWME84BkDb2T2WTM7MwyMyuEJf/gxYMiXv0fb/6Nk8dBEwsaiqpuuruiVHBjff/bW1ldW9/YLGwVt3d29/ZLB4cNozJNWZ0qoXQrQsMEl6xuuRWslWqGSSRYMxreTPzmE9OGK3lvRykLE+xLHnOK1kkNfMjP+bhbKvsVfwqyTII5KcMctW7pq9NTNEuYtFSgMe3AT22Yo7acCjYudjLDUqRD7LO2oxITZsJ8eu2YnDqlR2KlXUlLpurviRwTY0ZJ5DoTtAOz6E3E/7x2ZuOrMOcyzSyTdLYozgSxikxeJz2uGbVi5AhSzd2thA5QI7UuoKILIVh8eZk0LiqBXwnu/HL1eh5HAY7hBM4ggEuowi3UoA4UHuEZXuHNU96L9+59zFpXvPnMEfyB9/kDadWO/g==</latexit>
sha1_base64="mtTisajUYuRxKQW//mCXgqALL64=">AAAB7XicbZDLSgMxFIbP1Futt6pLN8EidGOZcaM7K25cVrAXaMeSSTNtbCYZkoxQhj6Dblwo4tb3UHwBdz6Ie9PLQlt/CHz8/znknBPEnGnjul9OZmFxaXklu5pbW9/Y3Mpv79S0TBShVSK5VI0Aa8qZoFXDDKeNWFEcBZzWg/75KK/fUqWZFFdmEFM/wl3BQkawsVYNX6eHbNjOF9ySOxaaB28KhdOPt7tW8fu90s5/tjqSJBEVhnCsddNzY+OnWBlGOB3mWommMSZ93KVNiwJHVPvpeNohOrBOB4VS2ScMGru/O1IcaT2IAlsZYdPTs9nI/C9rJiY88VMm4sRQQSYfhQlHRqLR6qjDFCWGDyxgopidFZEeVpgYe6CcPYI3u/I81I5KnlvyLt1C+QwmysIe7EMRPDiGMlxABapA4Abu4RGeHOk8OM/Oy6Q040x7duGPnNcfjgqTjw==</latexit>

<latexit sha1_base64="YoL6T8x9yeecPWqD+hHItotmbmU=">AAAB7XicbZDLSgMxFIbP1Futt1GXboJFcGOZcaM7K25cVrAXaMeaSTNtbCYZkoxQhj6Dblwo4tb3EHwBd76N6WWhrT8EPv7/HHLOCRPOtPG8bye3sLi0vJJfLaytb2xuuds7NS1TRWiVSC5VI8SaciZo1TDDaSNRFMchp/WwfzHK6/dUaSbFtRkkNIhxV7CIEWysVcM32REbtt2iV/LGQvPgT6F49vnxcAsAlbb71epIksZUGMKx1k3fS0yQYWUY4XRYaKWaJpj0cZc2LQocUx1k42mH6MA6HRRJZZ8waOz+7shwrPUgDm1ljE1Pz2Yj87+smZroNMiYSFJDBZl8FKUcGYlGq6MOU5QYPrCAiWJ2VkR6WGFi7IEK9gj+7MrzUDsu+V7Jv/KK5XOYKA97sA+H4MMJlOESKlAFAnfwCM/w4kjnyXl13ialOWfaswt/5Lz/ABKnkbM=</latexit>
sha1_base64="TEn/TdjgNlUm/yJ59XV4omr87/A=">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBC8GHa96DHixWME84BkDb2T2WTM7MwyMyuEJf/gxYMiXv0fb/6Nk8dBEwsaiqpuuruiVHBjff/bW1ldW9/YLGwVt3d29/ZLB4cNozJNWZ0qoXQrQsMEl6xuuRWslWqGSSRYMxreTPzmE9OGK3lvRykLE+xLHnOK1kkNfMjP+bhbKvsVfwqyTII5KcMctW7pq9NTNEuYtFSgMe3AT22Yo7acCjYudjLDUqRD7LO2oxITZsJ8eu2YnDqlR2KlXUlLpurviRwTY0ZJ5DoTtAOz6E3E/7x2ZuOrMOcyzSyTdLYozgSxikxeJz2uGbVi5AhSzd2thA5QI7UuoKILIVh8eZk0LiqBXwnu/HL1eh5HAY7hBM4ggEuowi3UoA4UHuEZXuHNU96L9+59zFpXvPnMEfyB9/kDadWO/g==</latexit>
sha1_base64="mtTisajUYuRxKQW//mCXgqALL64=">AAAB7XicbZDLSgMxFIbP1Futt6pLN8EidGOZcaM7K25cVrAXaMeSSTNtbCYZkoxQhj6Dblwo4tb3UHwBdz6Ie9PLQlt/CHz8/znknBPEnGnjul9OZmFxaXklu5pbW9/Y3Mpv79S0TBShVSK5VI0Aa8qZoFXDDKeNWFEcBZzWg/75KK/fUqWZFFdmEFM/wl3BQkawsVYNX6eHbNjOF9ySOxaaB28KhdOPt7tW8fu90s5/tjqSJBEVhnCsddNzY+OnWBlGOB3mWommMSZ93KVNiwJHVPvpeNohOrBOB4VS2ScMGru/O1IcaT2IAlsZYdPTs9nI/C9rJiY88VMm4sRQQSYfhQlHRqLR6qjDFCWGDyxgopidFZEeVpgYe6CcPYI3u/I81I5KnlvyLt1C+QwmysIe7EMRPDiGMlxABapA4Abu4RGeHOk8OM/Oy6Q040x7duGPnNcfjgqTjw==</latexit>

<latexit sha1_base64="OwsedO/aZM2lY25YD1mglnKDFLY=">AAACEHicdVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuCLtxZ0T6gHUsmzbShSWZIMkIZ+gluXOivuBO3/oF/4tJMOwvb4oHA4Zx7k5MTxJxp47rfTmFldW19o7hZ2tre2d0r7x80dZQoQhsk4pFqB1hTziRtGGY4bceKYhFw2gpGV5nfeqJKs0g+mHFMfYEHkoWMYGOl+9tH1itX3Ko7BVomXk4qkKPeK/90+xFJBJWGcKx1x3Nj46dYGUY4nZS6iaYxJiM8oB1LJRZU++k06gSdWKWPwkjZIw2aqn83Uiy0HovATgpshnrRy8T/vOxGPfd+GoiFPCa89FMm48RQSWZxwoQjE6GsHdRnihLDx5Zgopj9ESJDrDAxtsOSrcpbLGaZNM+qnlv17s4rteu8tCIcwTGcggcXUIMbqEMDCAzgGV7hzXlx3p0P53M2WnDynUOYg/P1C78Hnbw=</latexit>

<latexit sha1_base64="OwsedO/aZM2lY25YD1mglnKDFLY=">AAACEHicdVDLSgMxFL1TX7W+qi7dBIvgqsyIoMuCLtxZ0T6gHUsmzbShSWZIMkIZ+gluXOivuBO3/oF/4tJMOwvb4oHA4Zx7k5MTxJxp47rfTmFldW19o7hZ2tre2d0r7x80dZQoQhsk4pFqB1hTziRtGGY4bceKYhFw2gpGV5nfeqJKs0g+mHFMfYEHkoWMYGOl+9tH1itX3Ko7BVomXk4qkKPeK/90+xFJBJWGcKx1x3Nj46dYGUY4nZS6iaYxJiM8oB1LJRZU++k06gSdWKWPwkjZIw2aqn83Uiy0HovATgpshnrRy8T/vOxGPfd+GoiFPCa89FMm48RQSWZxwoQjE6GsHdRnihLDx5Zgopj9ESJDrDAxtsOSrcpbLGaZNM+qnlv17s4rteu8tCIcwTGcggcXUIMbqEMDCAzgGV7hzXlx3p0P53M2WnDynUOYg/P1C78Hnbw=</latexit>

···

Level k

F gure 6 1 – Graph ca mode of he eve -k reason ng mode Wen e a (2019) The
red par s he equ va en graph ca mode for he mu agen earn ng
prob em The b ue par corresponds o he recurs ve reason ng s eps
Subscr p a∗ s ands for he eve of h nk ng no he me s ep The
opponen po c es are approx ma ed by ρ− The om ed eve -0 mode
cons ders opponen s ha are fu y random sed Agen i ro s ou he
recurs ve reason ng abou opponen s n s m nd (b ue area) In he
recurs on agen s w h h gher- eve be efs ake he bes response o he
ower- eve agen s The h gher- eve mode s conduc a he compu a ons
ha he ower- eve mode s have done e g he eve -2 mode con a ns
he eve -1 mode by n egra ng ou π0 (a s)

exploration during training To mitigate these issues Yang et al (2020)
proposed Q-DPP which eradicates the structure constraints by approximating
the Q-function through a determinantal point process (DPP) Kulesza and
Taskar (2012) DPP pushes agents to explore and acquire diverse behaviours;
consequen y eads o na ura decompos on of he o n Q-func on w h no
need for a priori structure constraints In fact VDN/QMIX/QTRAN prove to
be the exceptional cases of Q-DPP
So ut ons v a Mu t agent Soft Learn ng

In s ng e-agen RL he process of find ng he op ma po cy can be equ vaen y ransformed n o a probab s c nference prob em on a graph ca mode

6.1. Stochastic Team Games

63

Levine (2018). The pivotal insight is that by introducing an additional binary
random variable P (O = 1|st , at ) ∝ exp(R(st , at )), which denotes the optimality of the state-action pair at time step t, one can draw an equal connection
between searching the optimal policies by RL methods and computing the
marginal probability of p(Oti = 1) by probabilistic inference methods, such
as message passing or variational inference Jordan et al. (1999); Beal (2003);
Wainwright et al. (2008). This equivalence between optimal control and
probabilistic inference also holds in the multiagent setting Wen et al. (2018);
Grau-Moya et al. (2018); Tian et al. (2019); Wen et al. (2019); Shi et al. (2019).
In the context of SG (see the red part in Figure 6.1),the optimality
 variable for

each agent i is defined by p Oti = 1|Ot−i = 1, τti ∝ exp ri st , ait , a−i
,
t
i −i
which implies that the optimality of trajectory τti = (s0 , ai0 , a−i
0 , ..., st , at , at )
depends on whether agent i acts according to its best response against other
agents, and Ot−i = 1 indicates that all other agents are perfectly rational and
attempt to maximise their rewards. Therefore, from each agent’s perspective,
−i
i
its objective becomes maximising p(O1:T
= 1|O1:T
= 1). As we assume no
knowledge of the optimal policies and the model of the environment, we treat
states and actions as latent variables and apply variational inference Blei et al.
(2017) to approximate this objective, which leads to
−i
i
max J(πθ ) = log p(O1:T
= 1|O1:T
= 1)
θi

≥

T
X





Es∼P (·|s,a),a∼πθ (s) ri st , ait , a−i
+ H πθ (ait , a−i
t
t |st )




.

t=1

(6.3)
One major difference from traditional RL is the additional entropy term 2
in Eq. (6.3).h Under this new objective, the valuei function is written as

i −i
V i (s) = Eπθ Qi (st , ait , a−i
t ) − log πθ (at , at |st ) , and the corresponding
optimal Bellman operator is
h

Hsoft Qi s, ai , a−i ≜ ri s, ai , a−i + γ · Es′ ∼P (·|s,a) log






X

i

Qi s′ , a .

a

(6.4)


P
This process is called soft learning because log a exp Q(s, a) ≈ maxa Q s, a .
One substantial benefit of developing a probabilistic framework for multiagent learning is that it can help model the bounded rationality Simon (1972).
2. Soft learning is also called maximum-entropy RL Haarnoja et al. (2018).

Learning in Identical-Interest Games

64

Instead of assuming perfect rationality and agents reaching NE, bounded rationality accounts for situations in which rationality is compromised; it can be
constrained by either the difficulty of the decision problem or the agents’ own
cognitive limitations. One intuitive example is the psychological experiment
of the Keynes beauty contest Keynes (1936), in which all players are asked
to guess a number between 0 and 100 and the winner is the person whose
number is closest to the 1/2 of the average number of all guesses. Readers
are recommended to pause here and think about which number you would
guess. Although the NE of this game is 0, the majority of people guess a
number between 13 and 25 Coricelli and Nagel (2009), which suggests that
human beings tend to reason only by 1-2 levels of recursion in strategic games
Camerer et al. (2004), i.e., “I believe how you believe how I believe”.
Wen et al. (2018) developed the first MARL powered reasoning model
that accounts for bounded rationality, which they called probabilistic recursive
reasoning (PR2). The key idea of PR2 is that a dependency structure is
assumed when splitting the joint policy πθ , written by










πθ ai , a−i |s = πθi i ai |s ρ−i
a−i |s, ai
θ−i



(PR2, Level-1),

(6.5)

that is, the opponent is considering how the learning agent is going to affect its
actions, i.e., a Level-1 model. The unobserved opponent model is approximated
by a best-fit model ρθ−i when optimising Eq. (6.3). In the team game setting,
since agents’ objectives are fully aligned, the optimal ρϕ−i has a closed-form

solution ρ−i
(a−i |s, ai ) ∝ exp Qi (s, ai , a−i ) − Qi (s, ai ) . Following the
ϕ−i
direction of recursive reasoning, Tian et al. (2019) proposed an algorithm
named ROMMEO that splits the joint policy by


















πθ ai , a−i |s = πθi i ai |s, a−i ρ−i
a−i |s
θ−i

(ROMMEO, Level-1),
(6.6)
in which a Level-1 model is built from the learning agent’s perspective. GrauMoya et al. (2018); Shi et al. (2019) introduced a Level-0 model where no
explicit recursive reasoning is considered.




πθ ai , a−i |s = πθi i ai |s ρ−i
a−i |s
θ−i



(Level-0).

(6.7)

However, they generalised the multiagent soft learning framework to include
the zero-sum setting. Wen et al. (2019) recently proposed a mixture of hierarchy
Level-k models in which agents can reason at different recursion levels, and

6.2. Dec-POMDPs

65

higher-level agents make the best response to lower-level agents (see the blue
part in Figure 6.1). They called this method generalised recursive reasoning
(GR2).
πki (aik |s) ∝
Z

a−i
k−1



πki (aik |s, a−i
k−1 ) ·

h

aik−2

|

Z

i

−i
i
i
i
i
ρ−i
k−1 (ak−1 |s, ak−2 )πk−2 (ak−2 |s) dak−2

{z

opponents of level k-1 best responds to agent i of level k-2



da−i
k−1 . (GR2, Level-K).

}

(6.8)
In GR2, practical multiagent soft actor-critic methods with convergence
guarantee were introduced to make large-K reasoning tractable.
6.2

Dec-POMDPs

Dec-POMDP is a stochastic team game with partial observability. However,
optimally solving Dec-POMDPs is a challenging combinatorial problem that is
N EXP -complete Bernstein et al. (2002). As the horizon increases, the doubly
exponential growth in the number of possible policies quickly makes solution
methods intractable. Most of the solution algorithms for Dec-POMDPs,
including the above VDN/QMIX/QTRAN/Q-DPP, are based on the learning
paradigm of centralised training with decentralised execution (CTDE) Oliehoek
et al. (2016). CTDE methods assume a centralised controller that can access
observations across all agents during training. A typical implementation is
through a centralised critic with a decentralised actor Lowe et al. (2017). In
representing agents’ local policies, stochastic finite-state controllers and a
correlation device are commonly applied Bernstein et al. (2009). Through
this representation, Dec-POMDP can be formulated as non-linear programmes
Amato et al. (2010); this process allows the use of a wide range of off-theshelf optimisation algorithms. Szer et al. (2005); Dibangoye et al. (2016);
Dibangoye and Buffet (2018) introduced the transformation from Dec-POMDP
into a continuous-state MDP, named the occupancy-state MDP (oMDP). The
occupancy state is essentially a distribution over hidden states and the joint
histories of observation-action pairs. In contrast to the standard MDP, where
the agent learns an optimal value function that maps histories (or states) to
real values, the learner in oMDP learns an optimal value function that maps

Learning in Identical-Interest Games

66

occupancy states and joint actions to real values (they call the corresponding
policy a plan). These value functions in oMDP are piece-wise linear and
convex. Importantly, the benefit of restricting attention on the occupancy state
is that the resulting algorithms are guaranteed to converge to a near-optimal
plan for any finite Dec-POMDP with a probability of one, while traditional RL
methods, such as REINFORCE, may only converge towards a local optimum.
In addition to CTDE methods, famous approximation solutions to DecPOMDP include the Monte Carlo policy iteration method Wu et al. (2010),
which enjoys linear-time complexity in terms of the number of agents, planning
by maximum-likelihood methods Toussaint et al. (2008); Wu et al. (2013),
which easily scales up to thousands of agents, and a method that decentralises
POMDP by maintaining shared memory among agents Nayyar et al. (2013).
6.3

Networked Multiagent MDPs

A rapidly growing area in the optimisation domain for addressing decentralised learning for cooperative tasks is the networked multiagent MDP (networked MDP). In the context of networked MDP, agents are considered heterogeneous rather than homogeneous; they have different reward functions but still
P
i
′
form a team to maximise the team-average reward R = N1 N
i=1 R (s, a, s ).
Furthermore, in networked MDP, the centralised controller is assumed to be
non-existent; instead, agents can only exchange information with their neighbours in a time-varying communication network defined by Gt = ([N ], Et ),
where Et represents the set of all communicative links between any two of the
N neighbouring agents at time step t. The states and joint actions are assumed
to be globally observable, but each agent’s reward is only locally observable to
itself. Compared to stochastic team games, this setting is believed to be more
realistic for real-world applications and can be scaled up to large-scale systems
Qu et al. (2020, 2022); Du et al. (2022b); Ma et al. (2024); Fan et al. (2024)
such as smart grids Dall’Anese et al. (2013) or transport management Adler
and Blue (2002).
The cooperative goal of the agents in networked MDP is to maximise the
team average cumulative discounted reward obtained by all agents over the
network, that is,
N
hX
i
1 X
max
E
γ t Rti (st , at ) .
(6.9)
π N
i=1
t≥0

6.3. Networked Multiagent MDPs

67

Accordingly, under the joint policy π =
is defined as
Qπ (s, a) =

N
1 X

N i=1

Q

i i
i∈{1,...,N } π (a |s), the Q-function



X
Ea ∼π(·|s ),s ∼P (·|s ,a )  γ t Ri (st , at ) s0 = s, a0 = a
t

t

t

t

t

t

t≥0

(6.10)
To optimise Eq. (6.10), the optimal Bellman operator is written as
N
1 X
′
′
.
Ri (s, a)+γ·Es′ ∼P (·|s,a) max
Q
s
,
a
H
Q (s, a) =
a′ ∼A
N i=1
(6.11)
However, since agents can know only their own reward, they do not share the
estimation of the Q function but rather maintain their own copy. Therefore,
from each agent’s perspective, the individual optimal Bellman operator is
written as





networked MDP











Hnetworked MDP,i Qi (s, a) = Ri (s, a) + γ · Es′ ∼P (·|s,a) max
Qi s′ , a′
′
a ∼A





.

(6.12)
To solve the optimal joint policy π ∗ , the agents must reach consensus over the
global optimal policy estimation, that is, if Q1 = · · · = QN = Q∗ , we know




Hnetworked MDP Q∗ (s, a) =

N 

1 X
Hnetworked MDP,i Qi .
N i=1

(6.13)

To satisfy Eq. (6.13), Zhang et al. (2018b) proposed a method based on neural
fitted-Q iteration (FQI) Riedmiller (2005) in the batch RL setting Lange et al.
(2012). Specifically, let Fθ denote the parametric function class of neural
networks that approximate Q-functions, let D = {(sk , aik , s′k )} be the replay
buffer that contains all the transition data available to all agents, and let {Rki }
be the local reward known only to each agent. The objective of FQI can be
written as
N
K h
i2
1 X
1 X
yki −f (sk , ak ; θ) , with yki = Rki +γ·max Qik (s′k , a).
f ∈Fθ N
a∈A
2K j=1
i=1
(6.14)
i
In each iteration, K samples are drawn from D. Since yk is known only to
each agent i, Eq. (6.14) becomes a typical consensus optimisation problem
(i.e., consensus must be reached for θ) Nedic and Ozdaglar (2009). Multiple

min

Learning in Identical-Interest Games

68

effective distributed optimisers can be applied to solve this problem, including
 i
1 PK
the DIGing algorithm Nedic et al. (2017). Let g i (θi ) = 2K
j=1 yk −
2

f (sk , ak ; θ) , α be the learning rate, and G([N ], El ) be the topology of the
network in the lst iteration; the DIGing algorithm designs the gradient updates
for each agent i as
i
θl+1
=

N
X

El (i, j) · θlj − α · ρil ,

j=1

ρil+1 =

N
X





 

i
El (i, j) · ρjl + ∇g i θl+1
− ∇g i θli .

(6.15)

j=1

Intuitively, Eq. (6.15) implies that if all agents aim to reach a consensus on θ,
they must incorporate a weighted combination of their neighbours’ estimates
into their own gradient updates. However, due to the usage of neural networks,
the agents may not reach an exact consensus. Zhang et al. (2018b) also
studied the finite-sample bound in a high-probability sense that quantifies the
generalisation error of the proposed neural FQI algorithm.
The idea of reaching consensus can be directly applied to solving Eq. (6.9)
via policy-gradient methods. Zhang et al. (2018c) proposed an actor-critic
algorithm in which the global Q-function is approximated individually by each
agent. On the basis of Eq. (3.4), the critic of Qi,πθ (s, a) is modelled by another
neural network parameterised by ω i , i.e., Qi (·, ·; ω i ), and the parameter ω i is
updated as
i
ωt+1
=

N
X



Et (i, j) · ωtj + α · δtj · ∇ω Qjt (ωtj )



(6.16)

j=1

where δtj = Rtj + γ · maxa∈A Qjt (s′t , a; ωtj ) − Qjt (s′t , a; ωtj ) is the TD error.
Similar to Eq. (6.15), the update in Eq. (6.16) is a weighted sum of all
the neighbouring gradients. The same group of authors later extended this
approach to cover the continuous-action space in which a deterministic policy
gradient method of Eq. (3.5) is applied Zhang et al. (2018a). Moreover, Zhang
et al. (2018c) and Zhang et al. (2018a) applied a linear function approximation
to achieve an almost sure convergence guarantee. Following this thread, Suttle
et al. (2019) and Zhang and Zavlanos (2019) extended the actor-critic method
to an off-policy setting, rendering more data-efficient MARL algorithms.

6.4. Stochastic Potential Games
6.4

69

Stochastic Potential Games

The potential game (PG) first appeared in Monderer and Shapley (1996).
The physical meaning of Eq. (3.10) is that if any agent changes its policy
unilaterally, the changes in reward will be represented on the potential function
shared by all agents. A PG is guaranteed to have a pure-strategy NE – a
desirable property that does not generally hold in normal-form games. Many
efforts have since been dedicated to finding the NE of (static) PGs Lã et al.
(2016), among which fictitious play Berger (2007) and generalised weakened
fictitious play Leslie and Collins (2006) are probably the most common
solutions.
Generally, stochastic PGs (SPGs) 3 can be regarded as the “single-agent
component” of a multiagent stochastic game Candogan et al. (2011) since all
agents’ interests in SPGs are described by a single potential function. However,
the analysis of SPGs is exceptionally sparse. Zazo et al. (2015) studied an SPG
with deterministic transition dynamics in which agents consider only openloop policies 4. In fact, generalising a PG to the stochastic setting is further
complicated because agents must now execute policies that depend on the state
and consider the actions of other players. In this setting, González-Sánchez
and Hernández-Lerma (2013) investigated a type of SPG in which they derive
a sufficient condition for NE, but it requires each agent’s reward function to
be a concave function of the state and the transition function to be invertible.
Macua et al. (2018) studied a general form of SPG where a closed-loop NE
can be found. Although they demonstrated the equivalence between solving
the closed-loop NE and solving a single-agent optimal control problem, the
agents’ policies must depend only on disjoint subsets of components of the
state. Notably, both González-Sánchez and Hernández-Lerma (2013) and
Macua et al. (2018) proposed centralised methods; optimisation over the joint
action space surely results in a combinatorial complexity when solving the
SPGs. In addition, they do not consider an RL setting in which the system is a
priori unknown.
3. As with team games, stochastic PG is also called dynamic PG or Markov PG.
4. Open loop means that agents’ actions are a function of time only. By contrast, close-loop
policies take into account the state. In deterministic systems, these policies can be optimal and
coincide in value. For a stochastic system, an open-loop strategy is unlikely to be optimal since
it cannot adapt to state transitions.

70

Learning in Identical-Interest Games

The work of Mguni (2020) is probably the most comprehensive treatment
of SPGs in a model-free setting. Similar to Macua et al. (2018), the authors
revealed that the NE of the PG in pure strategies could be found by solving
a dual-form MDP, but they reached the conclusion without the disjoint state
assumption: the transition dynamics and potential function must be known.
Specifically, they provided an algorithm to estimate the potential function
based on the reward samples. To avoid combinatorial explosion, they also
proposed a distributed policy-gradient method based on generalised weakened
fictitious play Leslie and Collins (2006) that has linear-time complexity.
Recently, Mazumdar et al. (2018) studied the dynamics of gradient-based
learning on potential games. They found that in a general superclass of
potential games named Morse-Smale games Hirsch (2012), the limit sets of
competitive gradient-based learning with stochastic updates are attractors
almost surely, and those attractors are either local Nash equilibria or non-Nash
locally asymptotically stable equilibria but not saddle points.

7
Learning in Zero-Sum Games

Zero-sum games are games where gains for one group of players correspond
to losses for another group of players, so that the total utilities sum to zero. In
this chapter we mainly focus on two-player zero-sum games, but briefly review
zero-sum games with more than two players at the end of this chapter.
In two-player zero-sum games, each player wants to choose a strategy
such that their opponent’s best response against that strategy achieves minimal
expected utility. This notion is captured in the following LP which can be used
to efficiently solve small matrix games.
U1∗

(7.1)

R1 (a1 , a2 ) · π 2 (a2 ) ≤ U1∗ , ∀a1 ∈ A1

(7.2)

min
s.t.

X
a2 ∈A2

X

π 2 (a2 ) = 1

(7.3)

π 2 (a2 ) ≥ 0, ∀a2 ∈ A2

(7.4)

a2 ∈A2

Eq. (7.1) is considered from the min-player’s perspective. One can also
derive a dual-form LP from the max-player’s perspective. In discrete games,
the minimax theorem Von Neumann and Morgenstern (1945), shown in Eq.
71

Learning in Zero-Sum Games

72

(7.5) is a consequence of the strong duality theorem of LP 1 Matousek and
Gärtner (2007).
h

min max E R π 1 , π 2
π1

π2

i

h

= max min E R π 1 , π 2
π2

π1

i

(7.5)

As a result of the minimax theorem, playing a Nash equilibrium (NE) is an
unambiguous solution concept for a player. By playing a NE they maximize
their reward even if the opponent knows their strategy. Playing a NE also
guarantees receiving the NE value of the game (for example, tying in tic-tactoe). As a result, algorithms that seek to find NE in large two-player zero
sum games have achieved superhuman performance on ChessCampbell et al.
(2002), GoSilver et al. (2016), and PokerBrown and Sandholm (2018).
In games that are larger than those that can be solved with an LP, learning
algorithms are used to iteratively converge to a (potentially approximate)
NE. In the following sections, we will use the following terms to refer to
different convergence rates in literature: “linear” indicates a rate of Õ(log 1ϵ ),
“sublinear” is slower than “linear” and it’s rate is Õ( ϵ1p ) with p ≥ 1, and
“superlinear” means faster than “linear” with a rate of Õ(log(log 1ϵ )), where ϵ
means eventually the algorithm will be ϵ−approximately close to the optimal
solution.
7.1

Minimax Optimization

Minimax optimization is one of the important and well-studied topic in the
optimization fields, which is to optimize the function f (x, y) in a dual-objective
manner, for two variables x ∈ Rm and y ∈ Rn :
min max f (x, y)

x∈Rm y∈Rn

(7.6)

where f (x, y) : Rm × Rn → R is not necessarily convex-concave. f (x, y) is
convex-concave if f (·, y) is convex for all y ∈ Rn and f (x, ·) is concave for
all x ∈ Rm .
Different conventional approaches are proposed to solve this minimax
optimization problem: Proximal Point (PP)Martinet (1970); Rockafellar (1976)
1. Solving zero-sum games is equivalent to solving a LP; Dantzig (1951) also proved the
correctness of the other direction, that is, any LP can be reduced to a zero-sum game, though
some degenerate solutions need careful treatments Adler (2013).

7.1. Minimax Optimization

73

is an early proposed method for minimax optimization problem. It has the
update rule as following:
df (xt+1 , yt+1 )
dx
df (xt+1 , yt+1 )
yt+1 = yt + ηht+1 , ht+1 =
dy
xt+1 = xt − ηgt+1 , gt+1 =

(7.7)
(7.8)

where η is the learning rate controlling the step size of the update. As we can
see, it is an implicit algorithm since f (xt+1 , yt+1 ) is unknown before the update.
The implementation of PP requires an inverse of matrix (I + η∇x f )−1 and
(I − η∇y f )−1 , which may not be computationally affordable in practice. PP
method is proven to converge linearly to NE for bilinear cases like normal-form
games or strongly-convex-strongly-concave casesMokhtari et al. (2020a).
Gradient Descent Ascent (GDA) is a basic type of algorithm where the
min-player takes the gradient descent and the max-player takes the gradient
ascent, as its name says. GDA has the update rule as following:
df (xt , yt )
dx
df (xt , yt )
yt+1 = yt + ηht , ht =
dy
xt+1 = xt − ηgt , gt =

(7.9)
(7.10)

where η is the learning rate. GDA achieves average-iterate convergence
guarantee with a rate O(1/ϵ2 ) under L-Lipschitz or l-smooth convex-concave
casesNemirovski (2004); and it has a rate O(κ2 log 1ϵ ) for strongly-convexstrongly-concave cases, κ = µl for l-smooth and µ-strongly-convex-stronglyconcave. However, GDA does not have last-iterate convergence guarantee,
there are cases where GDA has cyclic or divergent behaviours instead of
converges, and it fails in simple bilinear casesCheung and Piliouras (2019) and
non-convex or non-concave cases.
Optimistic Gradient Descent Ascent (OGDA)Daskalakis et al. (2017):
zt+1 = zt − 2ηF (zt ) + ηF (zt−1 )

(7.11)

where zt = (xt , yt ), F (zt ) = ( df (xdxt ,yt ) , − df (xdyt ,yt ) ), and η is the learning rate.
OGDA achieves average-iterate convergence guarantee with a rate O(1/ϵ)
for l-smooth convex-concave casesMokhtari et al. (2020b), Golowich et al.
(2020) also prove OGDA to have a last-iterate convergence rate of O(1/ϵ9 ) for

Learning in Zero-Sum Games

74

convex-concave cases. OGDA is further proved to achieve a linear convergence
rate of O(κ log 1ϵ ) for l-smooth and µ-strongly-convex-strongly-concave cases,
where κ = µl Mokhtari et al. (2020a).
Stochastic GDA (SGDA) is the algorithm to be applied in the case when
oracle gradient is not accessible, but only sample estimation of the gradient
is provided. It follows the same setting as stochastic gradient descent versus
1 PM
gradient descent: ∇x f (x, y) ≈ M
i=1 G(x, y, ξi ), where {ξi }i∈[M ] are data
samples and G is the gradient on the sample. SGDA converges in l-smooth
3
and nonconvex-µ-strongly-concave case with a rate of Õ( κϵ4 )Lin et al. (2020),
where κ = µl . It also converges for nonconvex-concave cases with a rate of
Õ( ϵ18 ).
Two-time-scale GDAHeusel et al. (2017) applies an asymmetric learning
rate for the the max- and min-player, which can be written as following:
df (xt , yt )
dx
df (xt , yt )
yt+1 = yt + ηy ht , ht =
dy
xt+1 = xt − ηx gt , gt =

(7.12)
(7.13)

where ηx , ηy are different learning rates for two sides. It is proven to converge in
2
l-smooth and nonconvex-µ-strongly-concave case with a rate of Õ( κϵ2 )Lin et al.
(2020), where κ = µl . It also converges for nonconvex-concave cases with a rate
of Õ( ϵ16 ). Two-timescale Stochastic GDA (two-timescale SGDA)Daskalakis
et al. (2020) is an extension of two-time-scale GDA to two-player SG setting,
with reinforcement learning method like REINFORCE for gradient estimation,
therefore the gradient is not oracle and suffers from sample estimation variances.
However, the method is provably convergent to NE with finite samples.
γ-GDAJin et al. (2020a) can be viewed as a special case of Two-time-scale
GDA, and it’s update rule is as following:
η
df (xt , yt )
gt , gt =
γ
dx
df (xt , yt )
yt+1 = yt + ηht , ht =
dy

xt+1 = xt −

(7.14)
(7.15)

where the max-player has the learning rate η and the min-player has a slower
learning rate γη through dividing it by γ > 1. It’s proven that γ-GDA does not
necessarily converge to local NE. By taking γ → ∞, we have the ∞-GDA

7.1. Minimax Optimization

75

algorithm, which has a infinite slow update of the min-player compared with
the max-player. In this case, the max-player will always be approximately
close to an oracle at the time scale of the min-player, therefore the algorithm is
also called GD with Max-Oracle (GDMax). GDMax is proven to converge to
stationary points called local minimax, which is a superset of the set of local
NEJin et al. (2020a).
Multiplicative Weights Update (MWU), also known as Hedge:
t

xt+1
= xti P
i

eηgi

t
j∈[m] xj e

, gt =
ηg t
j

df (xt , y t )
dx

t
e−ηhj
df (xt , y t )
t+1
t
t
yj = yj P
,
h
=
t
t −ηhi
dy
i∈[n] yi e

(7.16)

(7.17)

where xti , yjt are the i-th and j-th entry of probabilistic density in the row and
column player’s strategies at time t, in spaces of size m and n. η is the learning
rate. Similar as vanilla GDA, vanilla MWU is found to have a cyclic behaviour
and fail to converge even in simple bilinear casesBailey and Piliouras (2018),
therefore it does not have a last-iterate convergence guarantee.
Optimistic Multiplicative Weights Update (OMWU)Daskalakis and Panageas
(2018a), also known as Optimistic Hedge:
t−1
t
e2ηgi −ηgi
t+1
t
xi = xi P
t−1 , ∀i ∈ [A],
t
t e2ηgj −ηgj
x
j∈[m] j
t

yjt+1 = yjt P

(7.18)

t−1

e−2ηhj +ηhj

, ∀i ∈ [B],

(7.19)

,g
=
dx
dx
t, yt)
t−1 , y t−1 )
df
(x
df
(x
ht =
, ht−1 =
dy
dy

(7.20)

with g t =

t −2ηhti +ηht−1
i
i∈[n] yi e
df (xt , y t ) t−1 df (xt−1 , y t−1 )

(7.21)

where xti , yjt are the i-th and j-th entry of probabilistic density in the row
and column player’s strategies at time t, in spaces of size m and n. η is the
learning rate as in MWU. OMWU is proved to achieve a linear convergence
rate for bilinear cases when the equilibrium is uniqueDaskalakis and Panageas
(2018a).

Learning in Zero-Sum Games

76

A unification of OGDA and OMWU is called Optimistic Mirror Descent
Ascent (OMDA)Wei et al. (2020), which can be written as:
zt = arg min{η⟨z, F (zt−1 )⟩ + Dψ (z, zbt )}

(7.22)

zd
t+1 = arg min{η⟨z, F (zt )⟩ + Dψ (z, zbt )}

(7.23)

z∈Z

z∈Z

where zt = (xt , yt ), F (zt ) = ( df (xdxt ,yt ) , − df (xdyt ,yt ) ), Z = X × Y, Dψ (u, v) is
the Bregman divergence in mirror descent. To see that OMDA is a unification
of OGDA and OMWU, it can be shown that Dψ (u, v) = 12 ||u − v||2 for OGDA
and Dψ (u, v) = KL(u, v) for OMWU. The readers can try to prove that by
specifying different Bregman divergence, the update rules in Eq. (7.22) and
(7.23) will recover the rules of OMDA and OMWU.
Extra-Gradient (EG)Korpelevich (1976) method can be interpreted as an
approximation of PP methodMokhtari et al. (2020a).
zt+ 1 = zt − ηF (zt ),

(7.24)

zt+1 = zt − ηF (zt+ 1 )

(7.25)

2

2

where zt = (xt , yt ), F (zt ) = ( df (xdxt ,yt ) , − df (xdyt ,yt ) ), and η is the learning rate.
EG achieves average-iterate convergence guarantee with a rate O(1/ϵ) for lsmooth convex-concave casesMokhtari et al. (2020b), Daskalakis and Panageas
(2018a) also prove EG to have a last-iterate convergence rate of O(1/ϵ2 ) for
convex-concave cases. EG is further proved to achieve a linear convergence
rate of O(κ log 1ϵ ) for both bilinear cases and l-smooth and µ-strongly-convexstrongly-concave cases, where κ = µl Mokhtari et al. (2020a). It is an open
problem whether EG can achieve last-iterate convergence guaranteeLiang
and Stokes (2019); Mokhtari et al. (2020a). As an approximation of PP
method, EG is found to have a faster convergence rate than OGDA for bilinear
problemsMokhtari et al. (2020a).
The algorithms introduced in this section are essentially important for
solving equilibria in games, especially in two-player zero-sum games. We
will see that the minimax optimization is exactly the objective for solving NE
when NE exists in a normal-form game. Moreover, the procedure of solving
minimax optimization problem will serve as a subroutine for more complex
games with multiple steps, including stochastic games (SGs), extensive-form
games (EFGs), etc. We will see more about this in the following sections.

7.2. Discrete-Action Normal-Form Games
7.2

77

Discrete-Action Normal-Form Games

Now we consider two-player zero-sum games, i.e., fully competitive games.
The minimax optimization lies at the core of the solution of this problem.
By plugging f (x, y) = x⊺ Ay in Eq. (7.6), finding the NE in normal-form
games given the payoff function actually follows the minimax optimization
formulation:
min max x⊺ Ay
(7.26)
x∈△A y∈△B

where A ∈ R|A| × R|B| is the payoff matrix, △A ∈ R|A| , △B ∈ R|B| are
probability simplicies on the the discrete action spaces A and B for two
players. For simplicity, we may denote A = |A| and B = |B| in the following
paragraphs. We will start with some relatively simple algorithms based on the
best response subroutine for finding NE in two-player zero-sum normal-form
games.
Self-Play (SP)Fudenberg et al. (1998) is provably convergent for multiplayer potential game, but not for zero-sum games. SP has the update rule,
xt+1
= Bri (xt−i ), ∀i ∈ [n]
i

(7.27)

where Bri (·) is the best-response subroutine for a given (joint) strategy, and it
returns an optimal strategy for the i-th player, n is the number of players in the
game. It is easy to see that SP does not even converge for simple two-player
zero-sum games like Rock-Paper-Scissors.
Fictitious Play (FP)Brown (1951) is one of the earliest proposed algorithm
for solving games in an iterative procedure, which can be written as:
xbti = Bri (xt−i =

X
1 τ =t−1
Pr(aτ−i = a, a ∈ A))
t τ =0

1
1
xt+1
= (1 − )xti + xbti , ∀i ∈ [n]
i
t
t

(7.28)
(7.29)

where Bri (·) is the best-response subroutine for a given (joint) strategy, and
it returns an optimal strategy for the i-th player, n is the number of players
in the game. FP achieves ϵ-NE with a convergence rate of (1/ϵ)Ω(A) for a
normal-form game with action space ADaskalakis and Pan (2014).
Double Oracle (DO)McMahan et al. (2003) is another algorithm with
iterative best-response procedure. Different from FP, which is based on the

Learning in Zero-Sum Games

78

best response of the historical average strategy of opponents, DO iteratively
solves the best response of the opponents’ Nash equilibrium strategy on a
restricted strategy set, and keeps adding new best response strategy to the
strategy set. It can be written as,
τ |τ = 0, 1, . . . , t − 1}))
xbti = Bri (Nash−i ({xd
−i

cτ |τ = 0, 1, . . . , t}), ∀i ∈ [n]
xt+1
= Nash({x
i
i

(7.30)
(7.31)

where Nash−i (·) will return the Nash strategy on the input restricted set of
strategies for players except for i, Bri (·) is the best-response subroutine for
player i, and n is the number of players in the game. The DO has convergence
rate of O(|A|) for a normal-form game with action space A.
Apart from these algorithms based on best response, there are some other
types of algorithms simultaneously updating the strategies for two players to
find the NE in games, which will be detailed as following.
Replicator Dynamics (RD) is another type of algorithm with continuous
gradient flow. The symmetric version of RD for symmetric games with payoff
matrix A has the update rule as following:
dxk
= xk [(Ax)k − x⊺ Ax]
dt

(7.32)

where xk represents the k-th component of the probabilistic distribution of the
strategy for each single player in the game. Since the game is
Asymmetric Replicator Dynamics for two-player games with asymmetric
payoff matrices A, B, A⊺ ̸= B,
dxk
= xk [(Ay)k − x⊺ Ay],
dt

dyk
= yk [(x⊺ B)k − x⊺ By]
dt

(7.33)

where xk , yk represent the k-th component of the probabilistic distributions of
strategies for row and column players, respectively. (more see Bloembergen
et al. (2015))
As shown in Eq. (7.26), finding NE in normal-form games is a sub-problem
of minimax optimization, with a bilinear structure (therefore convex-concave).
The methods introduced in above Sec. 7.1 can directly be applied for solving
NE in normal-form games. For example, we can derive the update rule for
MWU method on normal-form games by plugging f (x, y) = x⊺ Ay into Eq.

7.2. Discrete-Action Normal-Form Games

79

(7.16) and (7.17):
t
eη(Ay )i
1
t+1
t
1
xi = xi P
t ) , xi =
t
η(Ay
j
A
j∈[m] xj e
⊺ t

e−η(A x )j
1
1
⊺ xt ) , yj =
t
−η(A
i
B
i∈[n] yi e

yjt+1 = yjt P

(7.34)

and the corresponding two-player OMWU algorithm will be:
t
t−1
e2η(Ay )i −η(Ay )i
t+1
t
, ∀i ∈ [A]
xi = xi P
t 2η(Ayt )j −η(Ayt−1 )j
j∈[m] xj e
⊺ t
⊺ t−1
e−2η(A x )j +η(A x )j
t+1
t
yj = yj P
, ∀j ∈ [B]
t −2η(A⊺ xt )i +η(A⊺ xt−1 )i
i∈[n] yi e

(7.35)

Similarly, methods including GDA and OGDA have the corresponding formulas
for normal-form games.
As introduced before, the MWU algorithm is also known as HedgeLittlestone
and Warmuth (1994); Freund and Schapire (1997), which is originated from
online learning for regret minimization. It applies an exponentially weighted
function to derive a new strategy:
π t (ai )e−ηRt (ai )
1
π t+1 (ai ) = PK
, π 1 (·) = .
−ηR
(a
)
t
t
j
K
j=1 π (aj )e

(7.36)

where ai is the i-th component within the action space. Notice that Rt in
above equation is the regret instead of reward at time step t. In computing Eq.
(7.36), Hedge needs access to the full information of the reward values for all
actions, including those that are not selected. For the above two-player setting,
the player at each side optimizes its strategy following the regret-minimization
principle. Specifically, the regret is defined as: Rt = −Ayt for the first player
and Rt = A⊺ xt for the second player.
√
The Hedge algorithm as Eq. (7.34) achieves a regret of O( T log A)
for multi-player general-sum game, with each player having action space of
size ACesa-Bianchi and Lugosi (2006). EXP3 Auer et al. (1995) extended
the Hedge algorithm for a partial information game in which the player
knows only the reward of the chosen action (i.e., a bandit version) and
has to estimate the loss of the actions that it does not select. Brown et al.
(2017) augmented the Hedge algorithm with a tree-pruning technique based

Learning in Zero-Sum Games

80

on dynamic thresholding. Gordon (2007) developed Lagrangian hedging,
which unifies no-regret algorithms, including both regret matching and Hedge,
through a class of potential functions. Optimistic Hedge (or called OMWU)
provably achieves O(N log A log4 T ) regret for N-player general-sum games,
with each player having action space of size ADaskalakis et al. (2021).
For regret minimization, one can also apply Blackwell’s approachability
theorem Blackwell et al. (1956) to minimize the regret independently on each
information set, also known as Regret Matching (RM) Hart and Mas-Colell
(2001). As we are most concerned with positive regret, denoted by ⌊·⌋+ , we
have ∀S ∈ Si , ∀a ∈ χ(S), the strategy of player i at time t + 1 as Eq. (7.37).



⌊Regit S, a ⌋+



 P
i

a∈χ(S) ⌊Regt S, a ⌋+
1



|χ(S)|

i
πt+1
(S, a) =


if

i
a∈χ(S) ⌊Regt (S, a)⌋+ > 0

P

otherwise
(7.37)

In the standard CFR algorithm, for each information set, Eq. (7.37) is
used to compute action probabilities in proportion to the positive cumulative
regrets.
We recommend to read the referenceCesa-Bianchi and Lugosi (2006) for a
comprehensive overview of no-regret algorithms.

7.3

Continuous-Action Normal-Form Games

Recently, the challenge of training generative adversarial networks (GANs)
Goodfellow et al. (2014a) has ignited tremendous research interest in understanding policy gradient methods in two-player continuous games, specifically,
games with a continuous station-action space and nonconvex-nonconcave
loss landscape. In GANs, two neural network parameterised models – the
generator G and the discriminator D – play a zero-sum game. In this game,
the generator attempts to generate data that “look” authentic such that the
discriminator cannot tell the difference from the true data; on the other hand,
the discriminator tries not to be deceived by the generator. The loss function

7.3. Continuous-Action Normal-Form Games

81

in this scenario is written as


min max f θG , θD =

(7.38)

θG ∈Rd θD ∈Rd



h

Ex∼pdata log DθD x

i

h



+ Ez∼p(z) log 1 − DθD GθG (z)

i



where θG and θD represent neural networks parameters and z is a random
signal, serving as the input to the generator. In searching for the NE, one naive
approach is to update both θG and θD by simultaneously implementing the
Gradient Descent Ascent (GDA) updates with the same step size in Eq. (7.38).
This approach is equivalent to a MARL algorithm in which both agents are
applying policy-gradient methods. With trivial adjustments to the step size
Bowling and Veloso (2002); Bowling (2005); Zhang and Lesser (2010), GDA
methods can work effectively in two-player two-action (thus convex-concave)
games. However, in the nonconvex-nonconcave case, where the minimax
theorem no longer holds, GDA with equal stepsize can converge to limit
cycles or even diverge in a general setting, and GDA can exhibit oscillation
in case of nonconvexity Zhang et al. (2020a). Flokas et al. (2021) further
proved that if the hidden game is strictly convex-concave then vanilla GDA
converges not merely to local Nash, but typically to the von-Neumann solution.
If the game lacks strict convexity properties, GDA may fail to converge to any
equilibrium. Therefore, in order to solve the above problems, more variants of
GDA have been developed. Lin et al. (2020) presented the convergence results
of the two-time-scale GDA algorithm for solving nonconvex-concave minimax
problems, showing that the algorithm can find a stationary point efficiently.
Kalogiannis et al. (2021) complemented some negative results which are
finding a NE in some settings can be shown to be CLS-hard by designing
a modified GDA that converges locally to NE. Deng and Mahdavi (2021)
proposed local Stochastic Gradient Descent Ascent (local SGDA), where the
primal and dual variables can be trained locally and averaged periodically to
significantly reduce the number of communications. Sharma et al. (2022)
proved that Local SGDA has order-optimal sample complexity for several
classes of nonconvex-concave and nonconvex-nonconcave minimax problems,
and also enjoys linear speedup with respect to the number of clients. Li (2021)
showed that for quadratic or nearly quadratic nonconvex-strongly-concave
functions under some assumptions, two-time-scale GDA and SGDA with
appropriate stepsizes achieve a linear convergence rate. Zhang et al. (2022c)

82

Learning in Zero-Sum Games

proved that alternating gradient descent-ascent (Alt-GDA) achieves a nearoptimal local convergence rate for strongly convex-strongly concave (SCSC)
problems while simultaneous GDA converges at a much slower rate.
Based on the recent research results above, GDA methods are notoriously
flawed from three aspects. First, GDA algorithms may not converge at all
Balduzzi et al. (2018a); Mertikopoulos et al. (2018); Daskalakis and Panageas
(2018b), resulting in limit cycles 2 in which even the time average 3 does not
coincide with NE Mazumdar et al. (2019a). Second, there exist undesired
stable stationary points for the GDA algorithms that are not local optima of
the game Adolphs et al. (2019); Mazumdar et al. (2019a). Third, there exist
games whose equilibria are not the attractors of GDA methods at all Mazumdar
et al. (2019a). These problems are partly caused by the intransitive dynamics
(e.g., a typical intransitive game is rock-paper-scissors game) that are inherent
in zero-sum games Omidshafiei et al. (2020); Balduzzi et al. (2018a) and
the fact that each agent may have a non-smooth objective function. In fact,
even in simple linear-quadratic games, the reward function cannot satisfy the
smoothness condition 4 globally, and the games are surprisingly not convex
either Fazel et al. (2018); Mazumdar et al. (2019a); Zhang et al. (2019c).
Three mainstream approaches have been followed to develop algorithms
that have at least a local convergence guarantee. One natural idea is to
make the inner loop solvable at a reasonably high level and then focus on a
simpler type of game. In other words, the algorithm tries to find a stationary

point of the function Φ(·) := maxθD ∈Rd f ·, θD , instead of Eq. (7.38). For
example, by considering games with a nonconvex and (strongly) concave loss
landscape, Lin et al. (2019); Nouiehed et al. (2019a); Rafique et al. (2018);
Thekumparampil et al. (2019); Lu et al. (2020a); Kong and Monteiro (2019)
presented an affirmative answer that GDA methods can converge to a stationary

point in the outer loop of optimising Φ(·) := maxθD ∈Rd f ·, θD . Based on
this understanding, they developed various GDA variants that apply the “best
response” in the inner loop while maintaining an inexact gradient descent in
2. Limit cycle is a terminology in the study of dynamical systems, which describes
oscillatory systems. In game theory, an example of limit cycles in the strategy space can be
found in Rock-Paper-Scissor game.
3. In two-player two-action games, Singh et al. (2000) showed that the time average payoffs
would converge to an NE value if their policies do not.
4. A differentiable function is said to be smooth if the gradients of the function are
continuous.

7.3. Continuous-Action Normal-Form Games

83

the outer loop. We refer to Lin et al. (2019) [Table 1] for a detailed summary
of the time complexity of the above methods.
The second mainstream idea is to shift the equilibrium of interest from the
NE, which is induced by simultaneous gradient updates, to the Stackelberg
equilibrium, which is a solution concept in leader-follower (i.e., alternating
update) games. Jin et al. (2019b) introduced the concept of the local Stackelberg equilibrium, named local minimax, based on which he established the
connection to GDA methods by showing that all stable limit points of GDA
are exactly local minimax points. Fiez et al. (2019) also built connections
between the NE and Stackelberg equilibrium by formulating the conditions
under which attracting points of GDA dynamics are Stackelberg equilibria
in zero-sum games. When the loss function is bilinear, theoretical evidence
was found that alternating updates converge faster than simultaneous GDA
methods Zhang and Yu (2019).
The third mainstream idea is to analyse the loss landscape from a gametheoretic perspective and design corresponding algorithms that mitigate oscillatory behaviour. Compared to the previous two mainstream ideas, which helped
generate more theoretical insights than applicable algorithms, works within
this stream demonstrate strong empirical improvements in training GANs.
Mescheder et al. (2017) investigated the game Hessian and identified that
issues on the eigenvalues trigger the limit cycles. As a result, they proposed
a new type of update rule based on consensus optimisation, together with a
convergence guarantee to a local NE in smooth two-player zero-sum games.
Adolphs et al. (2019) leveraged the curvature information of the loss landscape
to propose algorithms in which all stable limit points are guaranteed to be local
NEs. Similarly, Mazumdar et al. (2019b) took advantage of the differential
structure of the game and constructed an algorithm for which the local NEs are
the only attracting fixed points. Zhang et al. (2020a) introduced a “smoothing”
scheme which can be combined with GDA to stabilize the oscillation and
ensure convergence to a stationary solution. Yang et al. (2022) established new
convergence results for alternating GDA and smoothed GDA under the mild
assumption that the objective satisfies the Polyak-Lojasiewicz (PL) condition
about one variable. He et al. (2022) proposed GDA with anderson mixing
(GDA-AM) which can achieve global convergence for bilinear problems under
mild conditions. GAD-AM views the GDA dynamics as a fixed-point iteration and solves it using Anderson Mixing to converge to the local minimax.

Learning in Zero-Sum Games

84

Nouiehed et al. (2019b) proposed a multi-step GDA (MGDA) algorithm that
finds an ϵ–first order stationary point of the game in iterations. Lee and
Kim (2021b) proposed a new semi-anchoring (SA) technique for the MGDA
method. This makes the MGDA method find a stationary point of a structured
nonconvex-nonconcave composite minimax problem. Barazandeh et al. (2021)
showed that a simple multi-step proximal gradient descent-ascent algorithm
converges to ϵ–first order Nash equilibrium of the min-max game. Finally,
some studies have proved some characteristics of GDA. Mladenovic et al.
(2021) studied GDA flow and proved global convergence under mild assumptions. Fiez and Ratliff (2020) studied the role that a finite timescale separation
parameter has on the learning dynamics in GDA. Vlatakis-Gkaragkounis et al.
(2019) proved that GDA dynamics can exhibit a variety of behaviors antithetical
to convergence to the game theoretically meaningful min-max solution in some
specific instances. While studying the properties of GDA, the results in Doan
(2022) enhanced the convergence properties of its discrete-time counterpart.
In addition, Daskalakis et al. (2017); Mertikopoulos et al. (2018) addressed
the issue of limit cycling behaviour in training GANs by proposing to apply
the technique of optimistic mirror descent (OMD), which is initially proposed
by Rakhlin and Sridharan (2013). OMD achieves the last-iterate convergent
guarantee in bilinear convex-concave games. Specifically, at each time step,
OMD adjusts the gradient of that time step by considering the opponent policy
at the next time step. Let Mt+1 be the predictor of the next iteration gradient 5;
we can write OMD as follows.








θG,t+1 = θG,t + α · ∇θG ,t f θG , θD + MθG ,t+1 − MθG ,t
θD,t+1 = θG,t − α · ∇θD ,t f θG , θD + MθD ,t+1 − MθD ,t

(7.39)

In fact, the pivotal idea of opponent prediction in OMD, developed in the
optimisation domain, resembles the idea of approximate policy prediction in
the MARL domain Zhang and Lesser (2010); Foerster et al. (2018a).
Due to the advantages of OMD, many variants continue to improve this
method, such as Mertikopoulos et al. (2018) proved that OMD converges in
all coherent problems by taking an “extra-gradient” step. Vyas and Azizzadenesheli (2022) proposed optimistic competitive gradient optimization (OCGO),
an optimistic variant, for which they show convergence rate to saddle points in
5. In practice, it is usually set as the last iteration gradient.

7.3. Continuous-Action Normal-Form Games

85

α-coherent class of functions. Daskalakis et al. (2017) established that a variant of the widely used Gradient Descent/Ascent procedure, called Optimistic
Gradient Descent/Ascent (OGDA), exhibits last-iterate convergence to saddle points in unconstrained convex-concave min-max optimization problems.
BoHERE!HERE!hm et al. (2022) rediscovered a method related to OGDA,
for the deterministic and the
√ stochastic problem they showed a convergence
rate of O(1/k) and O(1/ k), Schäfer et al. (2020) proposed competitive
mirror descent (CMD): a general method for solving such problems based
on first order information that can be obtained by automatic differentiation.
Huang et al. (2021b) proposed an accelerated stochastic mirror descent ascent
(VR-SMDA) method based on the variance reduced technique. Huang et al.
(2021c) proposed a distributed mirror descent algorithm for computing a Nash
equilibrium and proved its convergence with suitably selected diminishing
step-sizes for a strictly convex-concave cost function. Wibisono et al. (2022)
proposed and analyze the alternating mirror descent algorithm, in which
each player takes turns to take action following the mirror descent algorithm
for constrained optimization. Some work further reveals the properties of
OMD, such as Anagnostides et al. (2022d) revealed that OMD either reaches
arbitrarily close to a Nash equilibrium, or it outperforms the robust price of
anarchy in efficiency. Anagnostides et al. (2022c) showed that when OMD
does not reach arbitrarily close to a NE, the cumulative regret of both players
is not only negative, but decays linearly with time. Gao and Pavel (2021)
proposed a second-order extension of the continuous-time game-theoretic
mirror descent dynamics which converges to mere variationally stable states.
Böhm (2022) proved novel convergence results which matching the 1/k rate
for the best iterate in terms of the squared operator norm recently shown for
the EG for a generalized version of the OGDA. Anagnostides and Panageas
(2022) found that OGD and its variants are instances of proportional integral
derivative (PID) control.
The other approach is extragradient (EG), EG (Korpelevich,1976) is one of
the most popular methods for solving saddle point and variational in equalities
problems (VIP). there remain important open questions about convergence of
EG. Gorbunov et al. (2022) derived the first last-iterate O(1/k) convergence
rate for EG for monotone and Lipschitz VIP without any additional assumptions
on the operator. Enrich et al. (2019) proposed an additional variance reduction
mechanism for EG to obtain speed-ups in smooth convex games. Luo and

86

Learning in Zero-Sum Games

Tran-Dinh unified and established a best-convergence rate of two variants
of the extragradient method for approximating a solution of a co-monotone
inclusion constituted by the sum of two operators. Liao et al. (2021) studied a
class of stochastic minimax methods and develop a communication-efficient
distributed stochastic extragradient algorithm, LocalAdaSEG, with an adaptive
learning rate suitable for solving convex-concave minimax problems in the
Parameter-Server model. Lee and Kim (2021a) proposed a two-time-scale
variant of the EG, named EG+, with a slow O(1/k) rate on the squared
gradient norm, where k denotes the number of iterations. Cen et al. (2021)
developed provably efficient extragradient methods to find the quantal response
equilibrium (QRE)—which are solutions to zero-sum two-player matrix games
with entropy regularization—at a linear rate. Li et al. (2021) presented an
analysis of the same-sample Stochastic EG (SEG) method with constant step
size. He et al. (2021) proposed AGE, an alternating extra-gradient method
with nonlinear gradient extrapolation. It estimates the lookahead step using
a nonlinear mixing of past gradient sequences. Du et al. (2022a) presented
a stochastic accelerated gradient EG (AG-EG) that achieves such a relatively
mature characterization of optimality in saddle-point optimization. Fasoulakis
et al. (2021) showed that its EG algorithm reaches first an η 1/ρ -approximate
Nash equilibrium, with ρ > 1. Finally, Azizian et al. (2020a) proved that EG
achieves the optimal rate for a wide class of algorithms with any number of
extrapolations.
In addition to the above three categories of methods, there are many other
gradient based methods. Loizou et al. (2021) introduced the expected cocoercivity condition and provided the first last-iterate convergence guarantees
of SGDA and stochastic consensus optimization (SCO) under this condition for
solving a class of stochastic variational inequality problems that are potentially
non-monotone. Loizou et al. (2020) proposed a novel unbiased estimator
for the stochastic Hamiltonian gradient descent (SHGD) and showed that
SHGD converges linearly to the neighbourhood of a stationary point. Qi
et al. (2021) proposed the adaptive Composite Gradients (ACG) method,
linearly convergent in bilinear games under suitable settings. Xu et al. (2021)
proposed a zeroth-order alternating randomized gradient projection (ZO-AGP)
and its iteration complexity to obtain an ϵ-stationary point is bounded by
O(ϵ−4 ). Wu et al. (2019) improved CS-GAN with natural gradient-based
latent optimisation and showed that it improves adversarial dynamics. Balduzzi

7.3. Continuous-Action Normal-Form Games

87

et al. (2018a) decomposed the second-order dynamics into two components.
The first is related to potential games, which reduce to gradient descent on
an implicit function; the second relates to Hamiltonian games, a new class
of games that obey a conservation law, akin to conservation laws in classical
mechanical systems. Mazumdar et al. (2018) proposed a general framework
for competitive gradient-based learning, which sheds light on the issue of
convergence to non-Nash strategies in general-sum and zero-sum games which
have no relevance to the underlying game.
There are some other methods that provide us with a new perspective.
For example, Grnarova et al. (2021) optimized a different objective that
circumvents the min-max structure using the notion of duality gap from game
theory. The convergence and stability properties of the method proposed in
Schaefer and Anandkumar (2019) remain robust under strong interactions
between players, even without adapting the step size. Franci and Grammatico
(2021) proposed a stochastic relaxed forward-backward (SRFB) algorithm
for GANs, and demonstrated convergence to an exact solution as more data
becomes available. Hsieh et al. (2020) conducted an in-depth study of a
comprehensive class of state-of-the-art algorithms and prevalent heuristics
for non-convex / non-concave problems. Perolat et al. (2021) investigated
how adapting the reward—by adding a regularization term—can yield strong
convergence guarantees in monotone games. Letcher (2018) addressed and
solved these games using a new algorithm called Stable Opponent Shaping
(SOS), which inherits strong convergence guarantees from lookahead and the
shaping capacity from Learning with Opponent-Learning Awareness (LOLA).
To accelerate the calculation of equilibrium, Yoon and Ryu (2022) showed
that these accelerated algorithms exhibit what we call the merging path (MP)
property. Azizian et al. (2020b) used matrix iteration theory to characterize
acceleration in smooth games.
Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning
problems formulated as games, instead treating them as simultaneous play
games and adopting the Nash equilibrium solution concept. Fiez et al. (2020),
Fiez et al. (2019) deviated from this paradigm and provide a comprehensive
study of learning in Stackelberg games.These works provided insights into the
optimization landscape of zero-sum games by establishing connections between
Nash and Stackelberg equilibria along with the limit points of simultaneous

88

Learning in Zero-Sum Games

gradient descent.
A central obstacle in the optimization of continuous zero-sum games is the
rotational dynamics that hinder their convergence. Existing methods typically
employ intuitive, carefully hand-designed mechanisms for controlling such
rotations. some works take a novel approach to address this issue by casting
min-max optimization as a physical system, such as Hemmat et al. (2020)
leveraged tools from physics to introduce LEAD (Least-Action Dynamics),
a second-order optimizer for min-max games. Bailey and Piliouras (2019)
established a formal and robust connection between multiagent systems and
Hamiltonian dynamics. Ibrahim et al. (2020) gave a linear lower bound
for n-player differentiable games, by using the spectral properties of the
update operator. Domingo-Enrich et al. (2020) proved a law of large numbers
that relates particle dynamics to mean-field dynamics. Ha and Kim (2022)
presented a spectral analysis and provide a geometric explanation of how and
when it actually improves the convergence around a stationary point.
Thus far, the most promising results are probably those of Bu et al. (2019)
and Zhang et al. (2019c), which reported the first results in solving zero-sum
LQ games with a global convergence guarantee. Specifically, Zhang et al.
(2019c) developed the solution through projected nested-gradient methods,
while Bu et al. (2019) solved the problem through a projection-free Stackelberg
leadership model. Both of the models achieve a sublinear rate for convergence.
These results are essentially related to game dynamics, recent advancements
in learning dynamics for games have addressed critical challenges in achieving
efficient regret minimization under various game-theoretic settings. Notably,
significant progress has been made in bounding the swap regret in generalsum multiplayer games. A novel combination of optimistic regularized
learning with self-concordant barriers enables near-optimal bounds on swap
regret, bypassing complex frameworks like higher-order smoothness, and
improving existing results to achieve bounds proportional to second-order
path lengths Anagnostides et al. (2022b). In dynamic multi-agent settings,
where the underlying repeated games evolve over time, researchers have
extended the convergence analysis of optimistic gradient descent (OGD) to
time-varying games. This framework provides a refined equilibrium gap and
regret bounds, particularly under strong convexity-concavity, with applications
to meta-learning and variation-dependent regret bounds Anagnostides et al.
(2024). Furthermore, new algorithms address regret minimization in general

7.4. Stochastic Games

89

convex games with arbitrary compact strategy sets, achieving exponential
improvements in regret bounds via an optimistic follow-the-regularized-leader
approach. These dynamics leverage self-concordant regularizers and proximal
oracles to maintain computational efficiency while generalizing previous results
Farina et al. (2022a). In addition, recent work extends equilibrium computation
to continuous-action games via a modified double oracle algorithm suitable
for multi-player settings. By maintaining fixed-cardinality pure strategy sets
and avoiding the need for exact metagame solving or global best-response
computation, this approach significantly reduces memory and computational
requirements, even in high-dimensional action spaces. Martin and Sandholm
(2024) Collectively, these contributions advance our understanding of dynamic
learning and equilibrium computation across a broad spectrum of gametheoretic frameworks.
7.4

Stochastic Games

Stochastic games (SGs)Shapley (1953), also known as Markov games
(MGs)Littman (1994), can be defined in the following way. An infinite-horizon
discounted version of multi-player general-sum SG/MG is denoted as a tuple
N
(N, S, {Ai }N
i=1 , P, {ri }i=1 , γ). N is the number of players in the game. S
is the state space, {Ai }N
i=1 are the action spaces for each player i ∈ [N ]
respectively. P is the state transition distribution, and P(·|s, a1 , . . . , aN ) :
S × {Ai }N
i=1 → Pr(S) is the distribution of the next state given the current
state s and joint actions (a1 , . . . , aN ). ri : S × Ai → R is the reward function
for the i-th player. γ ∈ [0, 1] is the discount factor. The stochastic policy of
the i-th player is defined as a probabilistic simplex: π(s) ∈ △Ai , s ∈ S.
For the two-player zero-sum SG/MG, it can be defined as a tuple (N, S, A, B, P, r, γ),
with the only difference that A, B are the action spaces for the max-player
and min-player respectively. The transition function P and r are changed
accordingly. In the zero-sum setting, the reward is the gain for the max-player
and the loss for the min-player due to the zero-sum payoff structure. Similar to
single-agent MDP, value-based methods aim to find an optimal value function,
which in the context of zero-sum SGs, corresponds to the minimax NE of the
game.
The MG can be regarded as a direct extension of Markov Decision Process
(MDP) for single agent to a multiagent setting, where the Markov property

Learning in Zero-Sum Games

90

for the agent in MDP is now preserved for the joint transition of all players in
MG. An extensive-form game (EFG) can be viewed as the MG with a special
tree structure. Although MG can also be viewed in a tree structure perspective
(always transferring from a parent node into its child node with the same state
and action spaces), it does not require so due to the Markov property. Strictly
speaking, MG is a subset of EFG, but this does not hurt to bring up methods
specifically designed for MG or EFG to achieve better sample efficiency. We
will see this with detailed discussions in the following sections.
7.4.1

Tabular Stochastic Games

From a theoretical view, we survey the existing methods for tabular SG/MG
with finite and discrete spaces, for both the state and the action. For simplicity,
we denote the action space size A = |A| (and B = |B| for second player in
the game), and the state space size S = |S|, where | · | denotes the cardinality
of the countable set. The horizon of the game is H. The tabular case for
two-player zero-sum SGs means A and B are finite and small.
In two-player zero-sum SGs with discrete states and actions, we know
1 2
1 2
V 1,π ,π = −V 2,π ,π , and by the minimax theorem Von Neumann and Mor1 2
genstern (1945), the optimal value function is V ∗ = maxπ2 minπ1 V 1,π ,π =
1 2
minπ1 maxπ2 V 1,π ,π . In each stage game defined by Q1 = −Q2 , the optimal
value can be solved by a matrix zero-sum game through a linear program in Eq.
(7.1). Shapley (1953) introduced the first value-iteration method, written as

(HShapley V )(s) =

min

max

π 1 ∈∆(A1 ) π 2 ∈∆(A2 )

h

i

Ea1 ∼π1 ,a2 ∼π2 ,s′ ∼P R1 (s, a1 , a2 )+γ·V (s′ ) ,
(7.40)

and proved HShapley is a contraction mapping (in the sense of the infinity norm)
in solving two-player zero-sum SGs. In other words, assuming the transitional
dynamics and reward function are known, the value-iteration method will
generate a sequence of value functions {Vt }t≥0 that asymptotically converges
to the fixed point V ∗ , and the corresponding policies will converge to the NE
policies π ∗ = (π 1,∗ , π 2,∗ ).
If the exact NE can not be found, we will introduce the concept of ϵapproximate NE, which represent a solution close to true NE by a distance of ϵ.
The ϵ-approximate NE for N -player is a product policy π = (π 1 , π 2 , . . . , π N )

7.4. Stochastic Games

91

satisfying:


bi

−i

i

−i



max max V i,π ,π (s1 ) − V i,π ,π (s1 ) ≤ ϵ

i∈[N ]

(7.41)

πbi

This definition covers the case of two players.
In contrast to Shapley’s model-based value-iteration method, Littman
(1994) proposed a model-free Q-learning method – Minimax-Q – that extends
the classic Q-learning algorithm defined in Eq. (3.2) to solve zero-sum SGs.
Specifically, in Minimax-Q, Eq. (3.3) can be equivalently written as
eval1





Q1 (st+1 , ·) = −eval2 Q2 (st+1 , ·)

=

min

max

π 1 ∈∆(A1 ) π 2 ∈∆(A2 )

h

i

Ea1 ∼π1 ,a2 ∼π2 Q1 (st+1 , a1 , a2 ) .
(7.42)

The Q-learning update rule of Minimax-Q is exactly the same as that in Eq.
(3.2). Minimax-Q can be considered an approximation algorithm for computing
the fixed point Q∗ of the Bellman operator of Eq. (3.9) through stochastic
sampling. Importantly, it assumes no knowledge about the environment.
Szepesvári and Littman (1999) showed that under similar assumptions to
those for Q-learning Watkins and Dayan (1992), the Bellman operator of
Minimax-Q is a contraction mapping operator, and the stochastic updates made
by Minimax-Q eventually lead to a unique fixed point that corresponds to the
NE value. In a nutshell, Minimax-Q is an algorithm provably convergent to
NE for two-player zero-sum SGs Szepesvári and Littman (1999).
In addition to the tabular-form Q-function in Minimax-Q, various Qfunction approximators have been developed. For example, Lagoudakis
and Parr (2003) studied the factorised linear architectures for Q-function
representation. Yang et al. (2019b) adopted deep neural networks and derived
a rigorous finite-sample error bound. Zhang et al. (2018b) also derived
a finite-sample bound for linear function approximators in the competitive
M-MDPs. Ding et al. (2022) shows empirical evidence with a DRL approach
for achieving policies that are hard to exploit in two-player Atari games.
Nash Q-learningHu and Wellman (2003) is one of the earliest work solving
general-sum stochastic games, including zero-sum SG/MG. Nash Q-learning
provably converges to NE for general-sum games under the assumption that
the NE is unique for each stage game during learning process. The update rule

Learning in Zero-Sum Games

92

of Nash Q-learning on a sample (s, a1 , a2 , r, s′ ) follows:
∗
Q∗h (s, a1 , a2 ) = rh (s, a1 , a2 ) + Vh+1
(s′ ), ∀h ∈ [H]

(πh1,∗ , πh2,∗ ) = Nash(Qh (s, ·, ·))
∗
Vh+1
(s′ ) = πh1,∗ (·|s)⊺ Qh+1 (s′ , ·, ·)πh2,∗ (·|s)
Qh (s, a1 , a2 ) = αQ∗h (s, a1 , a2 ) + (1 − α)Qh (s, a1 , a2 )

(7.43)
(7.44)
(7.45)
(7.46)

where Nash is a NE solving subroutine on the payoff matrices as Nash(Q) =
arg maxπ1 arg minπ2 (π 1 )⊺ Qπ 2 , Q∗ is the target value and Q is the current
estimation. It takes a soft update with a learning rate α for each sample
(s, a1 , a2 , r, s′ ). A variant of Nash Q-learning called QVI-MDVSSSidford et al.
(2020) with the generative model for querying arbitrary samples and special
3
) for two-player zero-sum
variance reduction techniques proves Õ( H ϵSAB
2
SG/MG. For the exploration setting, the convergence rate of an optimistic
version of Nash Q-learningBai et al. (2020) for achieving an ϵ-approximate NE
5
) for two-player zero-sum SG/MG. As a counterpart, in singleis Õ( H ϵSAB
2
agent RL, Q-learning with upper confidence bound achieves a convergence
4
rate of Ω̃( H ϵ2SA )Jin et al. (2018a), which matches with the lower bound of the
model-free single-agent RL setting.
Nash Value Iteration (Nash-VI) is the model-based version of Nash Qlearning, which combines the Value Iteration (model-based algorithm in
single-agent RL) with the NE operator on a matrix. The update rule of Nash
Value Iteration is similar to Nash Q-learning:
∗
Q∗h (s, a1 , a2 ) = rh (s, a1 , a2 ) + (Ph Vh+1
)(s, a1 , a2 ), ∀h ∈ [H]
(7.47)

(πh1,∗ (·|s), πh2,∗ (·|s)) = Nash(Qh (s, ·, ·))
Vh∗ (s) = πh1,∗ (·|s)⊺ Qh (s, ·, ·)πh2,∗ (·|s)
Qh (s, a1 , a2 ) = Q∗h (s, a1 , a2 )

(7.48)
(7.49)
(7.50)

where Nash is a NE solving subroutine on the payoff matrices, Q∗ is the
target value and Q is the current estimation. As a model-based algorithms, the
transition function Ph , ∀h ∈ [H] needs to be estimated before the above update,
which is different from the per-sample update in Nash Q-learning. In the
generative model setting (no consideration for exploration), a variant of Nash3
VIZhang et al. (2020b) is proved to achieve Õ( H ϵSAB
) sample complexity for
2
finding ϵ-approximate NE in two-player zero-sum SG/MG.

7.4. Stochastic Games

93

Value Iteration with Upper/Lower Confidence Bound (VI-ULCB)Bai and
Jin (2020) is an extension of Upper Confidence Bound Value Iteration
(UCBVI)Azar et al. (2017) in single-agent RL to the multiagent setting,
with upper confidence bound as exploration bonus. VI-ULCB is proven to
4 2
converge to an ϵ-approximate NE within Õ( H Sϵ2 AB ) steps, which has as
higher rate on S but lower rate on H compared with Nash Q-learning. The
2
) for
theoretical lower bound for two-player zero-sum SG/MG is Ω( H S(A+B)
ϵ2
3

model-based algorithms, and Ω( H S max{A,B})
) for model-free algorithms.
ϵ2
Q-learning type algorithms (e.g., Nash Q-learning) are model-free; while value
iteration type algorithms, e.g. VI-ULCB, are model-based.
As another variant of Nash-VI for the exploration setting, Optimistic Nash
Value Iteration (Optimistic Nash-VI)Liu et al. (2021) is proposed to further
improved the previous upper bounds. Nash-VI adopts an optimistic value
estimation, but with inclusion of an auxiliary bonus apart from the bonus in
upper/lower confidence bound for exploration, compared with VI-ULCB, to
3
), which removes a S term in UCBVI.
achieve a convergence rate of Õ( H ϵSAB
2
Moreover, Nash-VI finds Coarse Correlated Equalibirum (CCE) for multiplayer
N A
H 4 S 2 Πi=1
i
).
general-sum games with a convergence rate of Õ(
ϵ2
Nash V-learningBai et al. (2020) combines the algorithm Follow-TheRegularized-Leader (FTRL) with Q-learning in RL for two-player zero-sum
games. With an optimistic value estimation for exploration, Nash V-learning
6
is proven to have a convergence rate of Õ( H S(A+B)
). As a model-free
ϵ2
algorithm, Nash V-learning is tight compared with the lower bound except for
the dependency of the H term.
V-learningJin et al. (2021a) is an algorithm breaking the curse of multiagents, which is seen in above VI-ULCB or Nash-VI algorithms with a
N A in action spaces. It applies a decentralized learndependency of Πi=1
i
ing manner for each agent to estimate V value instead of Q value, since
V value only has a dimensional dependency of O(S) while using the Q
N A ). V-learning achieves ϵ−NE in two-player zero-sum
value is O(SΠi=1
i
5
Markov games for Õ( H S max{A,B}
) steps. Furthermore, V-learning can find
ϵ2
5

) steps. As a
ϵ−Coarse Correlated Equilibrium (CCE) in Õ( H S max{A,B}
ϵ2
concurrent work, CCE-V-learningSong et al. (2021) proves the same rate
for the multi-player general-sum SG setting. Meanwhile, the information
theoretical lower bound for getting ϵ-NE in two-player zero-sum SGs is

Learning in Zero-Sum Games

94
3

Ω̃( H S(A+B)
)Domingues et al. (2021); Zhang et al. (2020b). More disϵ2
cussions about general-sum settings see Chapter 8. A summarization of
value-based methods for multi-player zero-sum SGs is shown in Table 7.1.
Table 7.1 – Convergence guarantee for tabular two-player zero-sum stochastic games.
Type

Model Free

Model Based

Algorithm
Training Scheme
Optimistic Nash
Q-learningHu and
centralized
Wellman (2003);
Bai et al. (2020)
Nash V-learningBai
decentralized
et al. (2020)
V-learningJin et al.
decentralized
(2021a)
VI-ULCBBai and
centralized
Jin (2020)
Nash-VILiu et al.
centralized
(2021)

Lower
BoundDomingues
et al. (2021); Zhang
et al. (2020b)

7.4.2

Sample Complexity
5

Õ( H ϵSAB
)
2
6

Õ( H S(A+B)
)
ϵ2
5

Õ( H S max{A,B}
)
ϵ2
4

2

Õ( H Sϵ2 AB )
3

Õ( H ϵSAB
)
2
3

Ω̃( H S(A+B)
)
ϵ2

Linear Function Approximation

In recent years, there are also research beyond the tabular settings, which
only considers finite and discrete cases. As a relatively simple class in function
approximation settings, linear function approximation are studied in both
single-agent MDP Jin et al. (2020b) and SGs Xie et al. (2020); Chen et al.
(2021); Jin et al. (2021b).
Definition 7.1. (Linear function class) The linear function class F is a set of

functions linear in the features of state-action pairs. Specifically, for ∀h ∈ [H],
Fh := {ϕ⊺h θ|θ ∈ Bd (R)}

(7.51)

where ϕh : S × A × B → Bd (1) is the feature mapping from the state-action
pair (actions for two players) to a d-dimensional vector in unit ball. Bd (R) is
a d-dimensional ball with radius R.

7.4. Stochastic Games

95

Xie et al. (2020) proposes an algorithm based on an optimistic version of
Least Square Value Iteration, which is a theoretically proved algorithm for
single-agent MDP Jin et al. (2020b), and the algorithm is called Optimistic
Minimax Value Iteration (OMNI-VI) due to the change of single-agent Bellman
operator to a Nash/Minimax
Bellman operator. The proof shows that the
√
algorithm achieves Õ( d3 H 3 T ) regret for finding NE in two-player zero-sum
3 4
MGs, which corresponds to the sample complexity of Õ( d ϵH
2 )
Nash-UCRL Chen et al. (2021) is proposed to apply the “optimism-in-faceof-uncertainty” principle
in two-player zero-sum MGs as well, and it is proven
√
to achieve Õ(dH T ) regret under linear function approximation for finding
CCE of the game, where T is the total steps in the game and d is the linear
2 3
function dimension. It corresponds to the sample complexity of Õ( d ϵH
2 ) This
result is tighter than the results by Xie et √
al. (2020), and it matches with the
information theoretic lower bound Ω(dH T ) for two-player zero-sum MGs
with linear structures.
7.4.3

General Function Approximation

To establish understanding towards more general function approximation
methods like neural networks, which is proven to have universal function approximation capability with infinitely wide layers, researchers have investigated
MGs with general function approximation.
GOLF-with-ExploiterJin et al. (2021b) is a model-based algorithm for
two-player zero-sum tabular MGs and MGs with linear or kernel function
approximation. The results are proved within these function classes since
the Bellman Eluder (BE) dimensionJin et al. (2020b) is as low as Õ(d). It
applies an optimistic estimation for updating policy and exploiter within a
confidence set, and the exploiter is adopted in the exploration process. GOLFwith-Exploiter is proven to achieve a ϵ-approximate NE in zero-sum MGs with
2
NF
), where d comes from the BE dimension
a convergence rate of Õ( H d log
ϵ2
of the function class, NF = |F| is the cardinality of function class F, H is
the horizon length of an episode by default.
Optimistic Nash Elimination for Markov Games (ONEMG)Huang et al.
√
(2021a) is proven to achieve Õ(H dE K log NF ) regret in K episodes of
learning, which also has Õ( ϵ12 ) sample complexity. dE is the proposed
Minimax Eluder dimension especially for the zero-sum game setting, as a

Learning in Zero-Sum Games

96

counterpart of Bellman Eluder dimension in single-agent RL setting, H is the
horizon length of an episode by default. NF is the covering number of the
general function class F, therefore the rate has a logarithm dependence on the
cardinality of the function class. The function class F here is the function
approximation of the Q∗ , which represents the Q value for Nash equilibrium.
For a game with horizon H for each episode, the function class can be
disentangled to be F = F1 × . . . FH , Fh ⊂ {f : S × A1 × A2 → [0, 1]}. For
example, a parameterized NE value function (with parameters θ) at timestep
h is assumed to be an instance within the function class Fh : Q∗θ ∈ Fh . In
order to achieve the above regret guarantee, the realizability assumption and
the completeness assumption are assumed to be true, which are detailed as
following:
Assumption 1. (Realizability). Q∗h ∈ Fh , ∀h ∈ [H].
Assumption 2. (Completeness). Th f ∈ Fh , ∀f ∈ Fh+1 h ∈ [H], where Th

is the min-max Bellman operator:
Th Qh+1 (s, a, b) = r(s, a, b) + γEs′ ∼P(·|s,a,b) [max min Qh+1 (s′ , ·, ·)]
µ

ν

(7.52)
Decentralized Optimistic hypeRpolicy mIrror deScent (DORIS) Zhan
et al. (2022) is a decentralized policy optimization algorithm for zerosum MGs with general function approximation, and it is proven to achieve
√
Õ(HVmax KdBEE log N ) regret for K episodes, where Vmax is the maximum value of V -function, dBEE is another type of dimension for quantifying
the complexity of function class called Bellman Evaluation Eluder dimension,
H is the horizon length. The proof of the rate requires the realizability and
generalized completeness
assumptions. Similar as ONEMG introduced above,
√
the rate also has a K dependence on the K and logarithm dependence on
the cardinality N of the function class.
7.4.4

Turn-Based Stochastic Games

An important class of games that lie in the middle of SG and EFG is
the two-player zero-sum turn-based SG (2-TBSG). In TBSG, the state space
is split between two agents, S = S1 ∪ S2 , S1 ∩ S2 = ∅, and in every time
step, the game is in exactly one of the states, either S1 or S2 . Two players

7.4. Stochastic Games

97

alternate taking turns to make decisions, and each state is controlled 6 by only
one of the players π i : Si → Ai , i = 1, 2. The state then transitions into
the next state with probability P : Si × Ai → Sj , i, j = 1, 2. Given a joint
policy π =h (π 1 , π 2 ), the first player seeks
i to maximise the value function

P∞ t
π(s)
V
=E
t=0 γ R st , π(st ) |s0 = s , while the second player seeks to
minimize it, and the saddle point is the NE of the game.
Research on 2-TBSG leads to many important finite-sample bounds,
i.e., how many samples one would need before reaching the NE at a given
precision, for understanding multiagent learning algorithms. Hansen et al.
(2013) extended Ye (2005, 2010)’s result from single-agent MDP to 2-TBSG
and proved that the strongly polynomial time complexity of policy iteration
algorithms also holds in the context of 2-TBSG if the payoff matrix is fully
accessible. In the RL setting, in which the transition model is unknown,
Sidford et al. (2018, 2020) provided a near-optimal Q-learning algorithm that

computes an ϵ-optimal strategy with high-probability given O (1 − γ)−3 ϵ−2
samples from the transition function for each state-action pair. This result of
polynomial-time sample complexity is remarkable since it was believed to
hold for only single-agent MDPs. Recently, Jia et al. (2019) showed that if
the transition model can be embedded in some state-action feature space, i.e.,
P
′
′
∃ψk (s′ ) such that P (s′ |s, a) = K
k=1 ϕk (s, a)ψk (s ), ∀s ∈ S, (s, a) ∈ S×A,
then the sample complexity of the two-player Q-learning algorithm towards

finding an ϵ-NE is only linear to the number of features O K/(ϵ2 (1 − γ)4 ) .
All the above works focus on the offline domain, where they assume that
there exists an oracle that can unconditionally provide state-action transition
samples. Wei et al. (2017) studied an online setting in an averaged-reward
two-player SG. They achieved a polynomial sample-complexity bound if the
opponent plays an optimistic best response, and a sublinear regret round against
an arbitrary opponent.
Robust reinforcement learning attempts to learn minimax optimal policies
in the face of environment perturbationsMorimoto and Doya (2005); Pinto
et al. (2017). Robust RL is usually formulated as a stochastic game, where the
adversary is able to perturb something, such as the action or transition dynamics,
every timestep. Usually the adversary is assumed to be able to perturb within
some pre-defined uncertainty set that stays the same every timestep. For
6. Note that since the game is turned based, the Nash policies are deterministic.

Learning in Zero-Sum Games

98

example, the adversary might be able to add noise to a continuous action in an
adversarial directionTessler et al. (2019).
7.5

Extensive-Form Games

As briefly introduced in Section 3.2.4, zero-sum EFG with imperfect
information can be efficiently solved via LP in sequence form representations
Koller and Megiddo (1992, 1996). However, these approaches are limited
to solving only small-scale problems (e.g., games with O(107 ) information
states). In fact, considerable additional effort is needed to address real-world
games (e.g., limit Texas hold’em, which has O(1018 ) game states); to name
a few, Monte Carlo Tree Search (MCTS) techniques 7 Cowling et al. (2012);
Browne et al. (2012); Silver et al. (2016), isomorphic abstraction techniques
Billings et al. (2003); Gilpin and Sandholm (2006a), and iterative (policy)
gradient-based approaches Gordon (2007); Gilpin et al. (2007); Zinkevich
(2003).
A central idea of iterative methods for EFGs is minimising regret 8. A
learning rule achieves no-regret, also called Hannan consistency in game
theoretical terms Hannan (1957), if, intuitively speaking, against any set of
opponents it yields a payoff that is no less than the payoff the learning agent
could have obtained by playing any one of its pure strategies in hindsight.
Recall the reward function under a given policy π = (π i , π −i ) in Eq. (3.17);
the (average) regret of player i is defined by:
RegiT =

T h
i
X
1
Ri (π i , πt−i ) − Ri (πti , πt−i ) .
max
T πi t=1

(7.53)

A no-regret algorithm satisfies RegiT → 0 as T → ∞ with probability 1. When
Eq. (7.53) equals zero, all agents are acting with their best response to others,
which essentially forms a NE. Therefore, one can regard regret as a type of
“distance” to NE. As one would expect, the single-agent Q-learning procedure
7. Notably, though MCTS methods such as UCT Kocsis and Szepesvári (2006) work
remarkably well in turn-based EFGs, such as GO and chess, they cannot converge to a NE
trivially in (even perfect-information) simultaneous-move games Schaeffer et al. (2009). See a
rigorous treatment for remedy in Lisy et al. (2013).
8. One can regard minimising regret as one solution concept for multiagent learning
problems, similar to the reward maximisation in single-agent learning.

7.5. Extensive-Form Games

99

can be shown to be Hannan consistent in a stochastic game against opponents
playing stationary policies Shoham and Leyton-Brown (2008) [Chapter 7]
since the optimal Q-function guarantees the best response. In contrast, the
Minimax-Q algorithm in Eq. (7.42) is not Hannan consistent because if the
opponent plays a sub-optimal strategy, Minimax-Q is unable to exploit the
opponent due to the over-conservativeness in terms of over-estimating its
opponents.
An important result about regret states is that in a zero-sum game at time
T , if both players’ average regret is less than ϵ, then their average strategy
constitutes a 2ϵ-NE of the game (Zinkevich et al., 2008, Theorem 2). In
general-sum games, the average strategy of the ϵ-regret algorithm will reach
an ϵ-coarse correlated equilibrium of the game (Michael, 2020, Theorem
6.3.1). This result essentially implies that regret-minimising algorithms (or,
algorithms with Hannan consistency) applied in a self-play manner Zhang et al.
(2024) can be used as a general technique to approximate the NE of zero-sum
games. Building upon this finding, two families of methods are developed,
namely, fictitious play types of methods Berger (2007) and counterfactual regret
minimization Zinkevich et al. (2008), which lay the theoretical foundations for
modern techniques to solve real-world games.
It is worth noting that for EFG, a challenging setting involving imperfect
information and sequential moves, accelerated dynamics for extensive-form
correlated equilibria (EFCE) Farina et al. (2022b) and extensive-form coarse
correlated equilibria (EFCCE) have been developed. These methods significantly improve convergence rates, introducing a refined perturbation analysis
of Markov chains and efficient fixed-point characterizations Anagnostides et al.
(2022a).
7.5.1

Model-Free Approaches

The four families of model-free techniques for solving extensive-form
games—tree-based methods, Double Oracle methods McMahan et al. (2003),
Fictitious Play Berger (2007), and policy-gradient-based methods—are closely
interconnected, each addressing specific limitations of the others. Tree-based
methods, such as Counterfactual Regret Minimization (CFR) Zinkevich et al.
(2008), serve as a foundation by providing precise solutions for small-scale
problems but face scalability challenges due to the exponential growth of the

Learning in Zero-Sum Games

100

game tree. Double Oracle methods build upon this foundation by iteratively
expanding a reduced strategy space, allowing efficient computation in larger
games while still leveraging tree structures for solving subgames. Fictitious
Play Brown (1951) introduces a simpler iterative best-response approach that
converges to Nash equilibria under specific conditions, offering a balance
between computational simplicity and convergence guarantees in certain
settings. Policy-gradient-based methods extend these approaches further by
parameterizing strategies and optimizing them directly through gradient ascent,
making them suitable for continuous or extremely large strategy spaces where
explicit enumeration is infeasible. Together, these techniques represent a
progression from exact solutions to scalable approximations, each tailored to
different problem complexities and requirements.
Counterfactual Regret Minimization (CFR)

Another family of methods achieve Hannan consistency by directly minimising the regret, in particular, a special kind of regret named counterfactual
regret (CFR) Zinkevich et al. (2008). Unlike FP methods, which are developed
from the stochastic approximation perspective and generally have asymptotic
convergence guarantees, CFR methods are established on the framework of
online learning and online convex optimisation Shalev-Shwartz et al. (2011),
which makes analysing the speed of convergence, i.e., the regret bound, to the
NE possible.
The key insight from CFR methods is that in order to minimize the
total regret in Eq. (7.53) to approximate the NE, it suffices to minimize
the immediate counterfactual regret at the level of each information state.
Mathematically, Zinkevich et al. (2008) [Theorem 3] shows that the sum of
the immediate counterfactual regret over all encountered information states
provides an upper bound for the total regret in Eq. (7.53), i.e.,
RegiT ≤

X

n

o

max RegiT,imm (S), 0 , ∀i.

(7.54)

S∈Si

To fully describe RegiT,imm (S), we need two additional notations. Let
µπ (σS → σT ) denote, given agents’ behavioural policies π, the realisation
probability of going from the sequence σS 9, which leads to the information
9. Recall that for games of perfect recall, the sequence that leads to the information state,

7.5. Extensive-Form Games

101

state S ∈ Si to its extended sequence σT , which continues from S and reaches
the terminal state T . Let v̂ i (π, S) be the counterfactual value function, i.e.,
the expected reward of agent i in non-terminal information state S, which is
written as
v̂ i π, S =


X

µπ

−i

σs µπ (σs → σT )Ri (T ).


(7.55)

s∈S,T ∈T

Note that in Eq. (7.55), the contribution from player i in realising σs is
excluded; we treat whatever action current player i needs to reach state s as
i
having a probability of one, that is, µπ (σs ) = 1. The motivation is that now

one can make the value function v̂ i π, S “counterfactual” simply by writing
the consequence of player i not playing action a in the information state S as

v̂ i (π|S→a , S) − v̂ i (π, S) , in which π|S→a is a joint strategy profile identical
to π, except player i always chooses action a when information state S is
encountered. Finally, based on Eq. (7.55), the immediate counterfactual regret
can be expressed as
RegiT,imm (S) = max RegiT (S, a)

(7.56)

a∈χ(S)

where
1
RegiT (S, a) =

T 
X

T t=1



v̂ i (πt |S→a , S) − v̂ i (πt , S) .

(7.57)

Note that the T in Eq. (7.55) is different from that in Eq. (7.57).
Since minimizing the immediate counterfactual regret minimizes the overall regret, we can find an approximate NE by choosing a specific behavioral
policy π i (S) that minimizes Eq. (7.57). To this end, one can apply Blackwell’s approachability theorem Blackwell et al. (1956) to minimize the regret
independently on each information set, also known as regret matching Hart and
Mas-Colell (2001). As we are most concerned with positive regret, denoted by
⌊·⌋+ , we have ∀S ∈ Si , ∀a ∈ χ(S), the strategy of player i at time T + 1 as
Eq. (7.58).

including all the choice nodes within that information state, is unique.

Learning in Zero-Sum Games

102




⌊RegiT S, a ⌋+



 P
i

a∈χ(S) ⌊RegT S, a ⌋+
1



|χ(S)|

πTi +1 (S, a) =


if

i
a∈χ(S) ⌊RegT (S, a)⌋+ > 0

P

otherwise
(7.58)

In the standard CFR algorithm, for each information set, Eq. (7.37) is used
to compute action probabilities in proportion to the positive cumulative regrets.
In addition to regret matching, another online learning tool that minimizes
regret is Hedge Freund and Schapire (1997); Littlestone and Warmuth (1994),
in which an exponentially weighted function is used to derive a new strategy,
which is
πt (ak )e−ηRt (ak )
1
πt+1 (ak ) = PK
, π1 (·) = .
(7.59)
−ηR
(a
)
t
j
K
j=1 πt (aj )e
In computing Eq. (7.59), Hedge needs access to the full information of the
reward values for all actions, including those that are not selected. EXP3
Auer et al. (1995) extended the Hedge algorithm for a partial information
game in which the player knows only the reward of the the chosen action
(i.e., a bandit version) and has to estimate the loss of the actions that it does
not select. Brown et al. (2017) augmented the Hedge algorithm with a treepruning technique based on dynamic thresholding. Gordon (2007) developed
Lagrangian hedging, which unifies no-regret algorithms, including both regret
matching and Hedge, through a class of potential functions. We recommend
Cesa-Bianchi and Lugosi (2006) for a comprehensive overview of no-regret
algorithms.
No-regret algorithms, under the framework of online learning, offer a
natural way to study the regret bound (i.e., how fast the regret decays with
time).√For example, CFR and its variants ensure a counterfactual regret bound
of O( T ) 10, as a result√of Eq. (7.54), the convergence rate for the total regret
is upper bounded by O( T · |S|), which is linear in the number of information
states. In other words, the average policy of applying
CFR-type methods in a
√
two-player zero-sum EFG generates an O(|S|/ T )-approximate NE after T
10. According to
√Zinkevich (2003), any online convex optimization problem can be made to
incur RegT = Ω( T ).

7.5. Extensive-Form Games

103

steps through self-play 11.
Compared with the LP approach (recall Eq. (3.23)), which is applicable
only for small-scale EFGs, the standard CFR method can be applied to limit
Texas hold’em with as many as 1012 states. CFR+ , the fastest implementation
of CFR, can solve games with up to 1014 states Tammelin et al. (2015).
However, CFR methods still have a bottleneck in that computing Eq. (7.55)
requires a traversal of the entire game tree to the terminal nodes in each iteration.
Pruning the sub-optimal paths in the game tree is a natural solution Brown and
Sandholm (2015a); Brown et al. (2017); Brown and Sandholm (2017a). Many
CFR variants have been developed to improve computational efficiency further.
Lanctot et al. (2009) integrated Monte Carlo sampling with CFR (MCCFR) to
significantly reduce the per iteration time cost of CFR by traversing a smaller
sampled portion of the tree. Burch et al. (2012) improved MCCFR by sampling
only a subset of a player’s actions, which provides even faster convergence
rate in games that contain many player actions. Gibson et al. (2012); Schmid
et al. (2019) investigated the sampling variance and proposed MCCFR variants
with a variance reduction module. Johanson et al. (2012b) introduced a more
accurate MCCFR sampler by considering the set of outcomes from the chance
node, rather than sampling only one outcome, as in all previous methods. Xu
et al. (2024a) extended these developments by exploring the minimization
of weighted counterfactual regret using an optimistic Online Mirror Descent
(OMD) framework. This led to the proposal of a novel CFR variant, PDCFR+,
which unifies the principles of PCFR+ and Discounted CFR (DCFR). By
doing so, PDCFR+ effectively mitigates the negative effects of dominated
actions while leveraging predictions to consistently accelerate convergence in
a principled manner.
Deep CFR

Apart from Monte Carlo methods, function approximation
methods have also been introduced Waugh et al. (2014); Jin et al. (2018b). The
idea of these methods is to predict regret directly, and the no-regret algorithm
then uses these predictions in place of the true regret to define a sequence of
policies. Deep CFRBrown et al. (2019) uses neural networks to approximate
11. The self-play assumption can in fact be released. Johanson et al. (2012a) shows that
in two-player zero-sum games, as long as both agents minimize their regret, not necessarily
through the same
√ algorithm, their time-average policies will converge to NE with the same
regret bound O( T ). An example is to let a CFR player play against a best-response opponent.

104

Learning in Zero-Sum Games

CFR. Deep CFR iteratively samples trajectories like in MC-CFR and adds
the MC-CFR estimators for information sets encountered to a large replay
buffer. Then a regret network is trained on the replay buffer to approximate
the cumulative counterfactual regret at an information set. This regret network
is then used as a policy in the next iteration.
Although Deep CFR outperforms NFSP on poker games, the variance
of the MC-CFR estimator (due to importance sampling) causes the neural
network to become unstable. To partially fix this issue, DREAMSteinberger
et al. (2020) uses a learned history-value function as a baselineSchmid
et al. (2019) to reduce variance. However, since the importance sampling
term from MC-CFR remains, the variance in DREAM counterfactual regret
targets become very large in games with long horizons. To achieve very low
variance, ESCHERMcAleer et al. (2022a) directly estimates counterfactual
regret with a history advantage function. ESCHER is guaranteed to converge
to an approximate Nash equilibrium with high probability and is shown to
outperform DREAM and NFSP in large games. ARMACGruslys et al. (2020)
predicts history advantages conditioned on the policy that generated each
iteration and is competitive with NFSP. One potential downside of Deep-CFRbased methods opposed to last-iterate approaches is the need to output an
average policy either via checkpointing policies or training an average policy
network.

Policy Gradient

Self-play is a model-free policy-gradient method where two policies take
policy gradient updates against each other simultaneously. Although self-play
does not converge to a Nash equilibriumHennes et al. (2019), it has been shown
to scale up to large video gamesBerner et al. (2019b).
Interestingly, there exists a hidden equivalence between model-free policybased/actor-critic MARL methods and the CFR algorithm Jin et al. (2018b);
Srinivasan et al. (2018). In particular, if we consider the counterfactual value
function in Eq. (7.55) to be explicitly dependent on the action a that player i
P
chooses at state S, in which we have v̂ i (π, S) = a∈χ(S) π i (S, a)q̂ i (π, S, a),
then it is shown in Srinivasan et al. (2018) [Section 3.2] that the Q-function in
P t i

′
standard MARL Qi,π (s, a) = Es′ ∼P,a∼π
t γ R (s, a, s )|s, a differs from

7.5. Extensive-Form Games

105

q̂ i (π, S, a) in CFR only by a constant of the probability of reaching S, that is,
q̂ i π, S, a
.
s, a = P
π −i σ
s
s∈S µ


i,π

Q



(7.60)

Subtracting a value function on both sides of Eq. (7.60) leads to the fact
that the counterfactual regret of RegiT (S, a) in Eq. (7.56) differs from the
advantage function in MARL, i.e., Qi,π (s, ai , a−i ) − V i,π (s, a−i ), only by a
constant of the realisation probability. As a result, the multiagent actor-critic
algorithm Foerster et al. (2018b) can be formulated as a special type of CFR
method, thus sharing a similar convergence guarantee and regret bound in
two-player zero-sum games.
Neural Replicator Dynamics (NeuRD)Hennes et al. (2019) approximates
replicator dynamics in normal form games via a modified policy gradient
update. Since replicator dynamics is equivalent to HedgeKleinberg et al.
(2009), if one runs NeuRD independently as a counterfactual-regret minimizer
in CFR, this algorithm approximates CFR with Hedge, which is known to
converge to a Nash equilibrium in self play. The NeuRD update can also be
used as a policy gradient update in self play. The resulting algorithm does not
have theoretical guarantees but has demonstrated approximate convergence in
some games.
Friction-Follow-the-Regularized-Leader (F-FoReL)Perolat et al. (2021,
2022) modifies the reward function of a game by adding a relative entropy
term comparing the current policy and a reference policy. By modifying the
reward in this way, any no-regret algorithm achieves last-iterate convergence
to a modified equilibrium. The full algorithm carries out this procedure
multiple times, where the reference policy is updated every iteration to be the
output of the last iteration. This procedure converges to an approximate Nash
equilibrium in the last iterate in normal-form games. In extensive-form games,
the NeuRD update is used to update the policy and the reward is modified
at every information set. Although there are not convergence guarantees for
F-FoReL in extensive-form games, F-FoReL has achieved the impressive result
of beating top humans at the very large game of StrategoPerolat et al. (2022).
This is a model-free deep reinforcement learning method, without search, that
learns to master Stratego through self-play from scratch. This regularization
method called "reward transformation" has given rise to a series of modern
EFG solving methods that do not require search Meng et al. (2023); ?.

Learning in Zero-Sum Games

106

Algorithm 1 Policy Space Response Oracles (PSRO)McMahan et al. (2003);
Lanctot et al. (2017)
i
i∈N S , the meta-game payoff
M, ∀S ∈ S, and meta-policy π i = UNIFORM(Si ).

1: Initialise: the “high-level” policy set S =

Q

2: for iteration t ∈ {1, 2, ...} do:

for each player i ∈ N do:
4:
Compute the meta-policy πt by meta-game solver S(Mt ).
5:
Find a new policy against others by Oracle: Sti = Oi (πt−i ).
6:
Expand Sit+1 ← Sit ∪ {Sti } and update meta-payoff Mt+1 .
7:
terminate if: Sit+1 = Sit , ∀i ∈ N .
8: Return: π and S.
3:

Similar to F-FoReL, Magnetic Mirror Descent (MMD)Sokota et al. (2022)
is a policy-gradient method that uses regularization to converge in self-play.
MMD is similar to mirror descent except that is adds an additional policy
regularization term. MMD is guaranteed to converge in the last iterate to a QRE
in normal-form games. Using the sequence form and dilated entropy as the
divergence, MMD also converges in the last iterate to a QRE in extensive-form
games. In deep RL experiments, the authors show that a behavioral-strategy
update without convergence guarantees outperforms NFSP on large games.
Another policy-gradient approache is Actor Critic Hedge (ACH)Fu et al.
(2021). ACH is an actor-critic method where the critic is an information-state
advantage function and the policy is given by Hedge. Experiments show ACH
is competitive with NeuRD, RPG, and QPG.
Double Oracle

In solving real-world zero-sum games, such as Go or StarCraft, since the
number of pure strategies can be very large, one feasible approach instead
is to focus on meta-games. A meta-game is constructed by considering
sets of policies for each player, called populations. Populations of policies
implicitly define a normal-form meta game where the actions correspond to
policies in a population and the payoffs correspond to the expected value
when the corresponding policies are played against each other. Ideally, small
populations can be found such that some mixture of them will approximate a

7.5. Extensive-Form Games

107

Nash equilibrium in the full game.
The double oracle algorithm (DO)McMahan et al. (2003) iteratively
builds a meta-game by computing a NE of the meta-game and then adding
best responses for both players to the opponent’s meta-distribution. Policy
Space Response Oracles (PSRO)Neu et al. (2017) is a direct extension of
double oracle McMahan et al. (2003) that uses an RL subroutine as an
approximate best response. Specifically, one can write PSRO and its variations
in Algorithm 1, which essentially involves an iterative two-step process of
solving for the meta-policy first (e.g., Nash over the meta-game), and then
based on the meta-policy, finding a new better-performing policy, against
the opponent’s current meta-policy, to augment the existing population. The
meta-policy solver, denoted as S(·), computes a joint meta-policy profile
π based on the current payoff M where different solution concepts can be
adopted (e.g., NE). Finding a new policy is equivalent to solving a single-player
optimisation problem given opponents’ policy sets S−i and meta-policies π −i ,
which are fixed and known. One can regard a new policy as given by an
Oracle, denoted by O. In two-player zero-sum cases, an oracle represents
P
O1 (π 2 ) = {S 1 : S 2 ∈S2 π 2 (S 2 ) · ϕ(S 1 , S 2 ) > 0}. Finally, after a new
policy is found, the payoff table M is expanded, and the missing entries are
filled by running new game simulations. The above two-step process loops
over each player at each iteration, and it terminates if no new policies can be
found for any players.
Algorithm 1 is a general framework, with appropriate choices of metagame solver S and oracle O, it can represent solvers for different types of
meta-games. For example, it is trivial to see that FP/GWFP is recovered when
S = UNIFORM(·) and Oi = Bri (·)/Briϵ (·). The double oracle McMahan
et al. (2003) and PSRO methods Lanctot et al. (2017) refer to the cases when
the meta-solver computes NE.
PSRO has advantages and disadvantages compared to other approaches
such as CFR-based approaches and policy-gradient-based approaches. One
advantage is that PSRO can potentially terminate a lot faster than other
approaches. If PSRO happens to add pure strategies that support a Nash
equilibrium of the game in few iterations then PSRO can terminate very
early. Intuitively, games that are highly transitive or that require many nonstrategic skills might be good candidates for PSRO. For example, Starcraft
requires low-level skills that do not need to be mixed over, and a PSRO-based

108

Learning in Zero-Sum Games

method was able to beat expert humansVinyals et al. (2019a). Also, PSRO
provides a natural measure of approximate exploitability, which can be useful
when evaluating progress in large games. A few downsides of PSRO are as
follows. First, PSRO requires training best responses sequentially, which can
be time-consuming. Second, PSRO can arbitrarily increase exploitability from
one iteration to the next, which can be a problem when terminating early.
Third, PSRO might need to expand all pure strategies in the game, which is
exponential in the number of information sets.

Pipeline PSROMcAleer et al. (2020) solves the first problem by pretraining multiple best responses simultaneously while maintaining PSRO
convergence guarantees. Anytime PSROMcAleer et al. (2022b) is a version
of PSRO that does not increase exploitability. It does so by adaptively
modifying the restricted distribution via a no-regret algorithm against the
opponent best response while the opponent best response is training. The
resulting restricted distribution approximates the least-exploitable restricted
distribution, which only gets less exploitable as more policies are added to
the population. To address the final problem, Neural Extensive-Form Double
Oracle (NXDO)McAleer et al. (2021) constructs an extensive-form restricted
game instead of a normal-form restricted game. Instead of sampling one
policy at the root of the game and then continuing to play from that policy,
NXDO allows players to switch between policies in the population from one
information set to another. As a result, convergence is guaranteed in a number
of iterations that is linear in the number of information sets, as opposed to
exponential for PSRO. Despite these improvements, PSRO and NXDO still
lack regret bounds or rates of convergence besides the trivial bound when all
strategies are added. Also, after the restricted distribution becomes strong
enough, training the best response from scratch via RL can be challenging.
Regret-Minimizing Double Oracle provides a unified theoretical framework to
analyze these methods Tang et al. (2023, 2024), including their convergence
rates and sample complexity. This framework enables extending ODO to
extensive-form games while determining its sample complexity and highlights
the exponential sample complexity of XDO McAleer et al. (2021) due to the
decaying stopping threshold of restricted games.

7.5. Extensive-Form Games

109

Fictitious Play

Fictitious play (FP) Berger (2007) is one of the oldest learning procedures
in game theory that is provably convergent for zero-sum games, potential
games, and two-player n-action games with generic payoffs. Although the
algorithm guarantees convergence to the NE, the convergence is not efficient
in the worst case Daskalakis and Pan (2014). In FP, each player maintains a
belief about the empirical mean of the opponents’ average policy, based on
which the player selects the best response. With the best response defined in
Eq. (3.6), we can write the FP updates as:
t−1



1  i 1 i,∗
1 X  −i
i
−i
i
1 aτ = a, a ∈ A , πt+1
= 1−
π + a , ∀i.
ai,∗
t ∈ Br πt =
t τ =0
t t t t

(7.61)

In the FP scheme, each agent is oblivious to the other agents’ reward; however, they need full access to their own payoff matrix in the
stage game. In the continuous case with an infinitesimal learning rate of

1/t → 0, Eq. (7.61) is equivalent to dπt /dt ∈ Br πt − πt in which

Br(πt ) = Br(πt−1 ), ..., Br(πt−N ) . Viossat and Zapechelnyuk (2013)
proved that continuous FP leads to no regret and is thus Hannan consistent. If
the empirical distribution of each πti converges in FP, then it converges to a
NE 12.
Although standard discrete-time FP is not Hannan consistent (Cesa-Bianchi
and Lugosi, 2006, Exercise 3.8), various extensions have been proposed that
guarantee such a property; see a full list summarised in Hart (2013) [Section
10.9]. Smooth FP Fudenberg and Kreps (1993); Fudenberg and Levine (1995)
is a stochastic variant of FP (thus also called stochastic FP) that considers a
smooth ϵ-best response in which the probability of each action is a softmax
function of that action’s utility/reward against the historical frequency of the
opponents’ play. In smooth FP, each player’s strategy is a genuine mixed
strategy. Let Ri (ai1 , πt−i ) be the expected reward of player i’s action ai1 ∈ Ai
under opponents’ strategy π −i ; the probability of playing ai1 in the best response
12. Note that the convergence in Nash strategy does not necessarily mean the agents will
receive the expected payoff value at NE. In the example of Rock-Paper-Scissor games, agents’
actions are still miscorrelated after convergence, flipping between one of the three strategies,
though their average policies do converge to (1/3, 1/3, 1/3).

Learning in Zero-Sum Games

110

is written as
exp

Briλ (πt−i ) := P|Ai |



1 i
λR

k=1 exp





ai1 , πt−i

1 i
λR





aik , πt−i

 .

(7.62)

Benaim and Faure (2013) verified the Hannan consistency of the smooth best
response with the smoothing parameter λ being time dependent and vanishing
asymptotically. In potential games, smooth FP is known to converge to a
neighbourhood of the set of NE Hofbauer and Sandholm (2002). Recently,
Swenson and Poor (2019) showed a generic result that in almost all N × 2
potential games, smooth FP converges to the neighbourhood of a pure-strategy
NE with a probability of one.
In fact, “smoothing” the cumulative payoffs before computing the best
response is crucial to designing learning procedures that achieve Hannan
consistency Kaniovski and Young (1995). One way to achieve such smoothness
is through stochastic smoothing or adding perturbations 13. For example, the
smooth best response in Eq. (7.62) is a closed-form solution if one perturbs
the cumulative reward by an additional entropy function, that is,
π i,∗ ∈ Br(π −i ) =

n

o

arg max Eπ̂i ,π−i Ri + λ · log(π̂) .


π̂∈∆(Ai )

(7.63)

Apart from smooth FP, another way to add perturbation is the sampled FP in
which during each round, the player samples historical time points using a
randomised sampling scheme, and plays the best response to the other players’
moves, restricted to the set of sampled time points. Sampled FP is shown to be
Hannan consistent when used with Bernoulli sampling Li and Tewari (2018).
Among the many extensions of FP, the most important is probably generalised weakened FP (GWFP) Leslie and Collins (2006), which releases the
standard FP by allowing both approximate best response and perturbed average
strategy updates. Specifically, if we write the ϵ-best response of player i as






Ri Brϵ (π −i ), π −i ≥



sup Ri π, π −i − ϵ.
π∈∆(Ai )

(7.64)

then the GWFP updating steps change from Eq. (7.61) to








i
i
πt+1
= 1 − αt+1 πti + αt+1 Briϵ (π −i ) + Mt+1
,

∀i.

(7.65)

13. The physical meaning of perturbing the cumulative payoff is to consider the incomplete
information about what the opponent has been playing, variability in their payoffs, and
unexplained trembles.

7.5. Extensive-Form Games

111

GWFP is Hannan consistent if αt → 0, ϵt → 0, αt = ∞ when t → ∞, and
 Pk−1 i+1 i+1
Pk−1 i+1
{Mt } meets limt→∞ supk
M
s.t. i=t
α
≤ T = 0.
i=t α
It is trivial to see that GWFP recovers FP when αt = 1/t, ϵt = 0, Mt = 0.
GWFP is an important extension of FP in that it provides two key components
for bridging game theoretic ideas with RL techniques. With the approximate
best response (highlighted in blue, also named as the “weakened” term),
this approach allows one to adopt a model-free RL algorithm, such as deep
Q-learning, to compute the best response. Moreover, the perturbation term
(highlighted in red, also named as the “generalised” term) enables one to
incorporate policy exploration; if one applies an entropy term as the perturbation
in addition to the best response (in which the smooth FP in Eq. (7.63) is
also recovered), the scheme of maximum-entropy RL methods Haarnoja
et al. (2018) is recovered. In fact, the generalised term also accounts for the
perturbation that comes from the fact the beliefs are not updated towards the
exact mixed strategy π −i but instead towards the observed actions Benaım and
Hirsch (1999). As a direct application, Perolat et al. (2018) implemented the
GWFP process through an actor-critic framework Konda and Tsitsiklis (2000)
in the MARL setting.
Brown’s original version of FP Berger (2007) describes alternating updates
by players; yet, the modern usage of FP involves players updating their beliefs
simultaneously Berger (2007). In fact, Heinrich et al. (2015) only recently
proposed the first FP algorithm for EFG using the sequence-form representation.
The extensive-form FP is essentially an adaptation of GWFP from NFG to
EFG based on the insight that a mixture of normal-form strategies can be
implemented by a weighted combination of behavioural strategies that have
the same realisation plan (recall Section 3.2.3). Specifically, let π and β be
two behavioural strategies, Π and B be the two realisation-equivalent mixed
strategies 14, and α ∈ R+ ; then, for each information state S, we have
P



αµβ (σS )
β(S)
−
π(S)
, ∀S ∈ S,
(1 − α)µπ (σS ) + αµβ (σS )
(7.66)
π/β
where σS is the sequence leading to S, µ (σS ) is the realisation probability
of σS under a given policy, and π̃(S) defines a new behaviour that is realisation

π̃(S) = π(S) +

14. Recall that in games with perfect recall, Kuhn’s theorem Kuhn (1950a) suggests that the
behavioural strategy and mixed strategies are equivalent in terms of the realisation probability
of different outcomes.

Learning in Zero-Sum Games

112

equivalent to the mixed strategy (1 − α)Π + αB. The extensive-form FP
essentially iterates between Eq. (7.64), which computes the ϵ-best response,
and Eq. (7.66), which updates the old behavioural strategy with a step size of
α. Note that these two steps must iterate over all information states of the game
in each iteration. Similar to the normal-form FP in Eq. (7.61), extensive-form
FP generates a sequence of {πt }t≥1 that provably converges to the NE of a
zero-sum game under self-play if the step size α goes to zero asymptotically.
As a further enhancement, Heinrich and Silver (2016) implemented neural
fictitious self-play (NFSP), in which the best response step is computed by deep
Q-learning Mnih et al. (2015) and the policy mixture step is computed through
supervised learning. NFSP requires the storage of large replay buffers of past
experiences; Lockhart et al. (2019) removes this requirement by obtaining
the policy mixture for each player through an independent policy-gradient
step against the respective best-responding opponent. All these amendments
help make extensive-form FP applicable to real-world games with large-scale
information states.
7.5.2

Search

In this section we briefly outline techniques for performing search in
two-player zero-sum games. The previous sections in this chapter covered
model-free techniques. These techniques are useful for obtaining a blueprint
policy which roughly approximates a Nash equilibrium. However, usually these
blueprint policies are not exact enough to achieve superhuman performance.
To achieve superhuman performance on games such as chess, go, and poker,
search was required.
Search in two-player zero-sum extensive-form games refers to any technique
that improves its blueprint policy at test time against an opponent. At a
given information set, a search technique will perform subgame solving to
approximately solve the remaining subgame. Hopefully, the resulting strategy
at that information set will be better than the blueprint policy.
Perfect-Information Search

In perfect-information games, to make a decision at a state, all that is
needed is to consider all future states reachable from that state. As a result,
most perfect-information search techniques have a recursive structure. The

7.5. Extensive-Form Games

113

simplest example is min-max exhaustive searchRussell (2010). Min-max
search recursively solves all subgames nested inside a given subgame. The
simplest subgame and the base case are subgames that only contain the terminal
nodes. These subgames are solved by assuming that the max player will play
the maximal action or the min player will play the minimal action. In subgames
that are not terminal nodes, the value computed by lower level subgames are
passed up, and the player then chooses the best action of the available actions.
The resulting policy is the Nash equilibrium of that subgame, because at each
decision point, each player acted min-max optimally.
Of course, exhaustive search can only work in games as small as tic-tac-toe.
Many techniques exist for scaling up this idea to large games. Instead of
performing exhaustive search, these methods truncate the search by substituting
the value of a subgame with a heuristic based on that state. For example,
in chess this heuristic could be based on the number of pieces and the
position. These techniques have lead to superhuman performance at chess and
checkersCampbell et al. (2002); Schaeffer et al. (1996).
More recently, deep learning techniques have been used to find heuristics
for even larger games. AlphaGoSilver et al. (2016) uses a deep neural network
to approximate the value of a state for a given player. This heuristic is used in
Monte-Carlo Tree Search (MCTS)Browne et al. (2012) as a substitute for the
value of a subgame at a particular state. AlphaGo trained the value network
and policy using a large amount of human data. AlphaZeroSilver et al. (2017a)
was able to learn a value and policy network from scratch by training it on
rollouts from self-play games of the prior version of itself.
Imperfect-Information Search

Imperfect-information search is considerably harder than perfect-information
search. This is because the information needed to act in an information set
does not just depend on states that can be reached from the current state, but
also on everywhere else in the game tree.
As a result, subgame solving in imperfect information games must either
construct gadget games or reason about public belief states. Gadget games are
games that are constructed at a subgame so that once solved, the strategy found
from solving the gadget game can be used for that subgame in the original game.
Unsafe subgame solving methodsGanzfried and Sandholm (2015); Gilpin and

114

Learning in Zero-Sum Games

Sandholm (2006b, 2007a); Billings et al. (2003) have no guarantees on whether
the exploitability of the new strategy will be lower than the original blueprint
strategy. Safe subgame solvingBurch et al. (2014) constructs gadget games in a
way that the opponent can opt out and receive the utility of the blueprint strategy.
As a result, by solving this type of subgame, the new strategy is guaranteed
to be less exploitable than before. Max-margin subgame solvingMoravcik
et al. (2016) maximizes the difference in exploitability between the blueprint
and the new strategy. Safe and nested subgame solving improves max-margin
subgame solving further by considering all subgames in conjunction and was
shown to achieve superhuman performance in pokerBrown and Sandholm
(2017b). However, current subgame-solving techniques typically analyze the
entire common-knowledge closure of the player’s current information set,
encompassing all nodes within which it is common knowledge that the current
node lies. While effective in games with relatively simple information structures
like poker, these methods are impractical for games with complex information
structures, where the common-knowledge closure becomes infeasibly large to
enumerate or approximate. To address this limitation, recent work introduced
k-KLSS (order-k knowledge-limited subgame solving), a novel subgamesolving approach that leverages low-order knowledge instead of relying on
the common-knowledge closure Zhang and Sandholm (2021a). This method
enables agents to prune unreachable nodes upon encountering an information
set, significantly reducing the game tree size relative to traditional methods.
Methods based on public belief states such as DeepStackMoravcik et al.
(2017) and ReBeLBrown et al. (2020) keep an explicit distribution over all
possible hidden states and perform search in this space.
Imperfect-Information Subgame Solving

Imperfect-information subgame solving has undergone significant advancements, especially through the development of core algorithms, real-time
techniques, and scalable refinements. Key theoretical foundations include
decomposition methods such as CFR-D Burch and Bowling (2013), which
offers memory-efficient solutions with full-game consistency guarantees, addressing computational limits in large imperfect-information games (IIGs)
Burch et al. (2014); Burch and Bowling (2013). Real-time solving methods,
particularly nested solving Brown and Sandholm (2017b), allow dynamic

7.5. Extensive-Form Games

115

refinement of strategies during games, significantly reducing exploitability
in real-time settings like poker. Techniques such as depth-limited solving
Brown et al. (2018) provide a balance between decision quality and memory
usage, crucial for handling large state-space domains while ensuring safety
against adversary exploitation. Additionally, methods like subgame margin
help manage exploitability during transitions between precomputed strategies
and refined subgames Moravcik et al. (2016), further improving the robustness
of subgame-solving algorithms.
Despite substantial progress, scalability and abstraction remain significant
challenges. Abstraction refinement techniques, such as lossless or dynamic
refinements Moravcik et al. (2016); Ganzfried and Sandholm (2014), ensure
that simplified representations of games do not compromise the accuracy of
subgame solutions. Approaches like Earth Mover’s Distance in clustering
help optimize state aggregation but require careful balancing to avoid oversimplification Ganzfried and Sandholm (2014). Furthermore, frameworks like
Generative Subgame Solving Ge et al. (2024) and Opponent-Limited Subgame
Solving Liu et al. (2023) provide scalability by reducing subgame tree sizes,
a crucial step for handling high-dimensional games and adversarial setups.
These frameworks have shown potential for large-scale games like adversarial
team scenarios, but their applicability beyond poker remains underexplored,
with generalizability to other domains like economic models and security
games still facing hurdles.
The real-world application of these techniques, particularly in poker, has
validated their effectiveness, with systems like Libratus Sandholm (2010) and
DeepStack Moravcik et al. (2017) showcasing superhuman performance by
combining subgame refinement and recursive CFR. However, when extending
these techniques to non-poker domains, such as adversarial team games or
sequential auctions Sandholm (2010), scalability and the control of exploitability remain unresolved challenges. Further advancements in imperfect-recall
solutions Ganzfried and Sandholm (2014); Šustr et al. (2020), opponent modeling Milec et al. (2021), and methods to control safety and exploitability are
crucial for broadening the applicability of subgame-solving algorithms Liu
et al. (2022a); Šustr et al. (2020). As these methods evolve, the challenge will
be to integrate them into more diverse, real-world settings while managing the
computational complexity and ensuring robust performance across domains.

116

Learning in Zero-Sum Games

Game Abstraction in Imperfect-Information Games

Game abstraction plays a vital role in solving large-scale, imperfectinformation games by simplifying the computational complexity involved in
reasoning about vast strategy spaces Sandholm (2015). The key objective
is to reduce the game’s complexity while preserving enough information to
make effective decisions. Early research focused on three main types of
abstraction: state Li et al. (2006), action Gilpin (2009), and information
abstractions Kroer and Sandholm (2014a). State abstraction groups similar
states together to reduce the number of possibilities agents need to consider,
making it computationally feasible to analyze large games like poker Gilpin
and Sandholm (2008). Action abstraction reduces the set of available actions to
the most strategic moves, helping agents focus on key decisions. Information
abstraction, such as imperfect-recall methods Kroer and Sandholm (2014a),
simplifies hidden information, making it more manageable for agents to
compute strategies. While temporal abstractions Xu et al. (2024b) remain less
explored, they hold promise for simplifying decision-making over extended
horizons.
Modern approaches in game abstraction have focused on balancing between
lossless and lossy abstractions Gilpin (2009). Lossless abstractions maintain
the full strategic fidelity of the game Gilpin and Sandholm (2007b), but
they are often computationally infeasible for large-scale settings due to the
vastness of strategy spaces. On the other hand, lossy abstractions reduce the
game’s complexity at the expense of some strategic accuracy. Despite this
trade-off, lossy abstractions Sandholm and Singh (2012) have been equipped
with rigorous theoretical guarantees, such as bounds on error propagation and
exploitability Kroer and Sandholm (2018), ensuring that the abstraction does
not overly compromise the quality of the strategies derived from it. Additionally,
dynamic and adaptive refinement techniques Brown and Sandholm (2015b);
Bard et al. (2014) have been introduced to improve abstraction methods in
real-time. These techniques, such as simultaneous abstraction and equilibrium
finding, dynamically refine abstractions as the game progresses, leading to
more efficient computations and higher-quality solutions.
The scalability of these abstraction methods has been significantly enhanced through unified frameworks that combine various types of abstraction
into a single, scalable system Li et al. (2006). These frameworks ensure that dif-

7.6. Online Markov Decision Processes

117

ferent abstraction methods can be applied cohesively across different domains,
providing both computational efficiency and theoretical guarantees. Notable examples include systems that integrate Nash equilibrium solvers with abstraction
pipelines, enabling the solution of massive strategy spaces in complex games
like poker Sandholm (2015); Kroer and Sandholm (2014b). Furthermore,
abstraction methods have shown practical success in real-world applications,
particularly in poker, where techniques like Libratus have demonstrated superhuman performance by continuously refining abstractions throughout games
Sandholm (2010). Despite these advances, challenges remain, particularly
in multi-agent and non-zero-sum games, where abstraction methods struggle
to provide guarantees on solution quality. Future research will likely focus
on addressing these limitations and extending abstraction techniques to more
dynamic, multi-agent environments, as well as exploring under-researched
areas like temporal abstractions and long-horizon decision-making.
7.6

Online Markov Decision Processes

A common situation in which online learning techniques are applied is in
stateless games, where the learning agent faces an identical decision problem
in each trial (e.g., playing a multi-arm bandit in the casino). However, realworld decision problems often occur in a dynamic and changing environment.
Such an environment is commonly captured by a state variable which, when
incorporated into online learning, leads to an online MDP. Online MDP EvenDar et al. (2009); Yu et al. (2009); Auer et al. (2009), also called adversarial
MDP 15, focuses on the problem in which the reward and transition dynamics
can change over time, i.e., they are non-stationary and time-dependent.
In contrast to an ordinary stochastic game, the opponent/adversary in
an online MDP is not necessarily rational or even self-optimising. The aim
of studying online MDP is to provide the agent with policies that perform
well against every possible opponent (including but not limited to adversarial
opponents), and the objective of the learning agent is to minimize its average
loss during the learning process. Quantitatively, the loss is measured by how
15. The word “adversarial” is inherited from the online learning literature, i.e., stochastic
bandit vs adversarial bandit Auer et al. (2002). Adversary means there exists a virtual adversary
(or, nature) who has complete control over the reward function and transition dynamics, and the
adversary does not necessarily maintain a fully competitive relationship with the learning agent.

Learning in Zero-Sum Games

118

worse off the agent is compared to the best stationary policy in retrospect. The
expected regret is thus different from Eq. (7.53) (unless in repeated games)
and is written as
RegT =

T
hX

i
1
sup Eπ
Rt s∗t , a∗t − Rt st , at
T π∈Π
t=1

(7.67)

where Eπ denotes the expectation over the sequence of (s∗t , a∗t ) induced by the
stationary policy π. Note that the reward function sequence and the transition
kernel sequence are given by the adversary, and they are not influenced by the
retrospective sequence (s∗t , a∗t ).
The goal is to find a no-regret algorithm that can satisfy RegT → 0 as
T → ∞ with probability 1. A sufficient condition that ensures the existence
of no-regret algorithms for online MDPs is the oblivious assumption – both
the reward functions and transition kernels are fixed in advance, although they
are unknown to the learning agent. This scenario is in contrast to the stateless
setting in which no-regret is achievable, even if the opponent is allowed to be
adaptive/non-oblivious: they can choose the reward function and transition
kernels in accordance to (s0 , a0 , ..., st ) from the learning agent. In short, Yu
et al. (2009); Mannor and Shimkin (2003) demonstrated that in order to achieve
sub-linear regret, it is essential that the changing rewards are chosen obliviously.
Furthermore, Yadkori et al. (2013) showed with the example of an online
shortest path problem that there does not exist a polynomial-time solution (in
terms of the size of the state-action space) where both the reward functions
and transition dynamics are adversarially chosen, even if the adversary is
oblivious (i.e., it cannot adapt to the other agent’s historical actions). Most
recently, Ortner et al. (2020); Cheung et al. (2020) investigated online MDPs
where the transitional dynamics are allowed to change slowly (i.e., the total
variation does not exceed a specific budget). Therefore, the majority of existing
no-regret algorithms for online MDP focus on an oblivious adversary for the
reward function only. The nuances of different algorithms lie in whether the
transitional kernel is assumed to be known to the learning agent and whether
the feedback reward that the agent receives is in the full-information setting or
in the bandit setting (i.e., one can only observe the reward of a taken action).
Two design principles can lead to no-regret algorithms that solve online
MDPs with an oblivious adversary controlling the reward function. One is to
leverage the local-global regret decomposition result Even-Dar et al. (2005,

7.6. Online Markov Decision Processes

119

2009) [Lemma 5.4], which demonstrates that one can in fact achieve no regret
globally by running a local regret-minimisation algorithm at each state; a
similar result is observed for the CFR algorithm described in Eq. (7.56). Let
µ∗ (·) denote the state occupancy induced by policy π ∗ ; we then obtain the
decomposition result by
RegT =

X
s∈S

µ∗ (s)

T X
X



π ∗ (a | s) − πt (a | s) Qt s, a .


(7.68)

t=1 a∈A

|

{z

}

local regret in state s with reward function Qt (s, ·)

Under full knowledge of the transition function and full-information feedback
about the reward, Even-Dar et al. (2009) proposed the famous MDP-Expert
(MDP-E) algorithm, which adopts Hedge Freund and Schapire (1997) as the
p
regret minimizer and achieves O( τ 3 T ln |A|) regret, where τ is the bound
on the mixing time of MDP 16. For comparison, the theoretical lower bound
for regret in a fixed MDP (i.e., no adversary perturbs the reward function) is
p
Ω( |S||A|T ) 17 Auer et al. (2009). Interestingly, Neu et al. (2017) showed
that there in fact exists an equivalence between TRPO methods Schulman et al.
(2015) and MDP-E methods. Under bandit feedback, Neu et al. (2010) analysed
p
MDP-EXP3, which achieves a regret bound of O( τ 3 T |A| log |A|/β), where
β is a lower bound on the probability of reaching a certain state under a given
policy.
√ Later, Neu et al. (2014) removed the dependency on β and achieved
O( T log T ) regret. One major advantage of local-global design principle is
that it can work seamlessly with function approximation methods Bertsekas
and Tsitsiklis (1996). For example, Yu et al. (2009) eliminated the requirement
of knowing the transition kernel by incorporating Q-learning methods; their
proposed Q-follow the perturbed leader√(Q-FPL) method achieved O(T 2/3 )
regret. OPPO algorithm achieves O( d2 H 3 T ) regret Cai et al. (2019a),
where . d is the feature dimension, H is the episode horizon, and T is the total
number of steps. Note that OPPO requires the full information of the reward
functions to be available, but the transition dynamics are unknown. In contrast,
Jin et al. (2019a) first addresses the problem of learning online MDPs with
unknown transition dynamics and bandit feedback.
16. Roughly, it can be considered as the time that a policy needs to reach the stationary status
in MDPs. See a precise definition in Even-Dar et al. (2009) [Assumption 3.1].
17. This lower bound has recently been achieved by Azar et al. (2017) up to a logarithmic
factor.

Learning in Zero-Sum Games

120

Apart from the local-global decomposition principle, another design
principle is to formulate the regret minimisation problem as an online linear
optimisation (OLO) problem and then apply gradient-descent type methods.
Specifically, since the regret in Eq. (7.68) can be further written as the inner
P
product of RegT = Tt=1 ⟨µ∗ − µt , Rt ⟩, one can run the gradient descent
method by
n
o
1
µt+1 = arg max µ, Rt − D µ|µt ,
(7.69)
µ∈U
η
where U = µ ∈ ∆S×A : a µ(s, a) = s′ ,a′ P (s|s′ , a′ )µ(s′ , a′ ) is the
set of all valid stationary distributions 18, where D denotes a certain form of
divergence and the policy can be extracted by πt+1 (a|s) = µt+1 (s, a)/µ(s).
One significant advantage of this type of method is that it can flexibly handle
different model constraints and extensions. If one uses Bregman divergence
as D, then online mirror descent is recovered Nemirovsky and Yudin (1983)
and is guaranteed to achieve a nearly optimal regret for OLO problems Srebro
et al. (2011). Zimin and Neu (2013) and Dick et al. (2014) adopted a relative
entropy for D; the subsequent online relative entropy policy search (O-REPS)
p
algorithm achieves an O( τ T log(|S||A|)) regret in the full-information
p
setting and an O( T |S||A| log(|S||A|)) regret in the bandit setting. For
p
comparison, the aforementioned MDP-E algorithm achieves O( τ 3 T ln |A|)
p
and O( τ 3 T |A| log |A|/β), respectively. When the transition dynamics are
unknown to the agent, Rosenberg and Mansour (2019) extended O-REPS by
incorporating the classic idea of optimism in the face of uncertainty in Auer
p
et al. (2009), and the induced UC-O-REPS algorithm achieved O(|S| |A|T )
regret.


7.7

P

P

Team Games

Team games are games in which members of the same team share the
same utility functions. In two-team zero-sum games, two teams compete in
a zero-sum game. One solution concept in zero-sum team games is called
TMECorCelli and Gatti (2018); Farina et al. (2018); Zhang and Sandholm
(2021b); Zhang et al. (2022b). In TMECor, players on the same team are
18. In the online MDP literature, it is generally assumed that every policy reaches its
stationary distribution immediately; see the policy mixing time assumption in Yu et al. (2009)
[Assumption 2.1].

7.7. Team Games

121

allowed to coordinate before playing, and since it involves cooperation within
the team, cooperative MARL algorithms can be used to approximate this
solution concept McAleer et al. (2023). A related solution concept is an
equilibrium where team members are not able to coordinate, which is known
as a team-maxmin equilibrium (TME) strategyvon Stengel and Koller (1997);
Basilico et al. (2017); Zhang and An (2020a,b); Kalogiannis et al. (2021).
A TME yields the maximum expected utility for the team players against a
best-responding team opponent. The TMECor solution concept has several
advantages over TME. First, the team is guaranteed at least as much expected
utility under TMECor than under TMECelli and Gatti (2018). Second, the
TMECor objective is convex while the TME objective is not convex. Lastly,
in general, finding a TMECor strategy is NP-hard and inapproximableCelli
and Gatti (2018). However, many real-world scenarios might not permit
communication.

8
Learning in General-Sum Games

Solving general-sum stochastic games (SGs) entails an entirely different
level of difficulty than solving team games or zero-sum games. In a static
two-player normal-form game, finding the Nash equilibrium (NE) is known to
be P P AD-complete Chen and Deng (2006).
8.1

Solutions by Mathematical Programming

To solve a two-player general-sum discounted stochastic game with discrete
states and discrete actions, Filar and Vrieze (2012) [Chapter 3.8] formulated
the problem as a nonlinear program; the matrix form is written as follows:
minV,π f (V, π) =

P2

T
i=1 1|S|

h



(a) π 2 (s)T R1 (s) + γ ·
h

s.t.



Ri (π) + γ · P(π)V i

′
1 ′
s′ P(s |s)V (s )

P

i

i



≤ V 1 (s)1T|A1 | ,

′
2 ′
1
2
s′ P(s |s)V (s ) π (s) ≤ V (s)1|A2 | ,
π 1 (s)T 1|A1 | = 1, ∀s ∈ S
π 2 (s)T 1|A2 | = 1, ∀s ∈ S

(b) R2 (s) + γ ·
(c) π 1 (s) ≥ 0,
(d) π 2 (s) ≥ 0,

Vi−

P

∀s ∈ S
∀s ∈ S

(8.1)
where
122

8.1. Solutions by Mathematical Programming

123

— V = V i : i = 1, 2 is the vector of agents’ values over all states,
V i = V i (s) : s ∈ S is the value vector for the i-th agent.
— π = π i : i = 1, 2 and π i = π i (s) : s ∈ S , where the policy π i (s) =
π i (a|s) : a ∈ Ai is the vector representing the stochastic policy in
state s ∈ S for the i-th agent.
— Ri (s) = Ri s, a1 , a2 : a1 ∈ A1 , a2 ∈ A2 is the reward matrix for
the ith agent in state s ∈ S. The rows correspond to the actions of the
second agent, and the columns correspond to those of the first agent.

With
a slight abuse of notation,
we use Ri (π) = Ri π 1 , π 2 =
D
E
π 2 (s)T Ri (s)π 1 (s) : s ∈ S to represent the expected reward vector
over all states under joint policy π.






— P(s′ |s) = P (s′ |s, a) : a = a1 , a2 , a1 ∈ A1 , a2 ∈ A2 is a matrix
representing the probability of transitioning from the current state
s ∈ S to the next state s′ ∈ S. The rows represent the actions of
the second agent, and the columns represent those of the first agent.

1, π2
With
a
slight
abuse
of
notation,
we
use
P(π)
=
P
π
=
h
i
2
T
′
1
′
π (s) P(s |s)π (s) : s ∈ S, s ∈ S to represent the expected transition probability over all state pairs under joint policy π.




This is a nonlinear programme because the inequality constraints in the
optimisation problem are quadratic in V and π. The objective function in
Eq. (8.1) aims to minimise the TD error for a given policy π over all states,
similar to the policy evaluation step in the traditional policy iteration method,
and the constraints of (a) and (b) in Eq. (8.1) act as the policy improvement
step, which satisfies the equation when the optimal value function is achieved.
Finally, constraints (c) and (d) ensure the policy is properly defined.
Although the NE is proved to exist in general-sum SGs in the form of
stationary strategies, solving Eq. (8.1) in the two-player case is notoriously
challenging. First, Eq. (8.1) has a non-convex feasible region; second, only
the global optimum 1 of Eq. (8.1) corresponds to the NE of SGs, while the
common gradient-descent type of methods can only guarantee convergence to
a local minimum. Apart from the efforts by Filar and Vrieze (2012), Breton
et al. (1986) [Chapter 4] developed a formulation that has nonlinear objectives
but linear constraints. Furthermore, Dermed and Isbell (2009) formulated the
1. Note that in the zero-sum case, every local optimum is global.

Learning in General-Sum Games

124

NE solution as multi-objective linear program. Herings et al. (2004); Herings
and Peeters (2010) proposed an algorithm in which a homotopic path between
the equilibrium points of N independent MDPs and the N -player SG is traced
numerically. This approach yields a NE point of the stochastic game of interest.
However, all these methods are tractable only in small-size SGs with at most
tens of states and only two players.
Apart from the concept of NE defined before, we will also use another two
solution concepts in the general-sum SGs, which are correlated equilibrium
(CE) and coarse correlated equilibrium (CCE), as generalization of NE. These
concepts will be defined as following.
Definition 8.1. (Correlated Equilibrium) A correlated equilibrium (CE) in

multi-player general-sum SGs is defined as a joint (correlated) policy π :=
{πh (s) ∈ △Ai , s ∈ S}, and it satisfies the following relationship at each
time-step:
max max V i,ϕ⋄π (s) ≤ V i,π , ∀s ∈ S,

i∈[N ] ϕ∈Φi

(8.2)

where the strategy modification ϕ := {ϕs : Ai → Ai }s∈S ∈ Φi is a function
set for player i at each time-step, changing the action of the player. Specifically,
by ϕ ⋄ π it changes the policy π from choosing a = (a1 , . . . , aN ) at state s and
a certain time-step, to actions (a1 , · · · , ai−1 , ϕs (ai ), ai+1 , . . . , aN ) insteadLiu
et al. (2021).
Definition 8.2. (Coarse Correlated Equilibrium) A coarse correlated equilib-

rium (CCE) in multi-player general-sum SGs is defined as a joint (correlated)
policy π := {πh (s) ∈ △Ai , s ∈ S}, and it satisfies the following relationship
at each time-step:
max max V i,πbi ,π−i (s) ≤ V i,π , ∀s ∈ S

i∈[N ]

πbi

(8.3)

.
It can be seen that CCE is a generalized version of NE, and NE is just the
case when the correlated policy in CCE can be factorized as the product of
individual policy for each player.
There is a relationship between NE, CE and CCE as shown in Theorem 8.1,
for which the proof will be omitted.

8.2. Solutions by Value-Based Methods

125

Theorem 8.1. For general-sum SGs, {NE}⊆{CE}⊆{CCE}.

Mathematical programming methods, particularly linear programming,
have been crucial for solving equilibria in general-sum extensive-form games.
Solutions such as Extensive-Form Correlated Equilibria (EFCE) Farina et al.
(2022b) and Stackelberg Extensive-Form Correlated Equilibria (SEFCE)
offer strong theoretical guarantees but face scalability challenges due to
the exponential complexity of game trees Cermak et al. (2016). Recent
advancements include innovations like correlation-directed acyclic graphs
(DAGs) Zhang et al. (2022a), which help manage the computational complexity
of EFCE. Despite these improvements, the high computational demands of
these techniques limit their applicability in large-scale games, requiring further
research to improve scalability without sacrificing solution quality.
8.2

Solutions by Value-Based Methods

A series of value-based methods have been proposed to address generalsum SGs. A majority of these methods adopt classic Q-learning Watkins
and Dayan (1992) as a centralised controller, with the differences being what
solution concept the central Q-learner should apply to guide the agents to
converge in each iteration. For example, the Nash-Q learner in Eqs. (3.8
& 3.9) applies NE as the solution concept, the correlated-Q learner adopts
correlated equilibrium Greenwald et al. (2003), and the friend-or-foe learner
considers both cooperative (see Eq. (6.1)) and competitive equilibrium (see Eq.
(7.42)) Littman (2001a). Although many algorithms come with convergence
guarantees, the corresponding assumptions are often overly restrictive to be
applicable in general. When Nash-Q learning was first proposed Hu et al.
(1998), it required the NE of the SG be unique such that the convergence
property could hold. Though strong, this assumption was still noted by
Bowling (2000) to be insufficient to justify the convergence of the Nash-Q
algorithm. Later, Hu and Wellman (2003) corrected her convergence proof by
tightening the assumption even further; the uniqueness of the NE must hold
for every single stage game encountered during state transitions. Years later, a
strikingly negative result by Zinkevich et al. (2006) concluded that the entire
class of value-iteration methods could be excluded from consideration for
computing stationary equilibria, including both NE and correlated equilibrium,

Learning in General-Sum Games

126

in general-sum SGs. Unlike those in single-agent RL, the Q values in the
multiagent case are inherently defective for reconstructing the equilibrium
policy.
There is a variant of Nash-VI algorithm for the exploration setting in multiplayer SGs, called Optimistic Nash Value Iteration (Optimistic Nash-VI)Liu
et al. (2021), which is provably finding Coarse Correlated Equalibirum (CCE)
N A
H 4 S 2 Πi=1
i
for multiplayer general-sum SGs with a convergence rate of Õ(
).
ϵ2
Similar as in single-agent RL, value iteration (VI) is a model-based type algorithm as a counterpart of model-free Q-learning. In multiagent setting, Nash-VI
and its variants are also model-based, which first estimates the transition model
P(s′ |s, a) and leveraging the NE, CE or CCE solving subroutine on payoff
matrices to achieve the corresponding equilibria on multi-step SGs. However,
N A , which
Nash-VI has a product dependency on the action space as Πi=1
i
is referred as the “curse of multiagents”. This will bring large computation
when the number of players is huge. To break the “curse of multiagents”,
V-learningJin et al. (2021a) is proposed. In V-learning, each agent estimates the
V value instead of Q value, since V value only has a dimensional dependency
N A ). V-learning finds ϵ−CCE in
of O(S) while using the Q value is O(SΠi=1
i
multi-player general-sum SGs in Õ(
H 5 S(max

A )2

H 3 S max

A

H 5 S maxi∈[N ] Ai
) steps, and finds ϵ−CE in
ϵ2

i∈[N ] i
) steps. There are some concurrent work proposed at this
Õ(
ϵ2
time, Optimistic V-learning with Stabilized Online Mirror Descent (V-learning
OMD)Mao and Başar (2022), CCE-V-learning and CE-V-learningSong et al.
(2021) prove the similar rates as V-learning with slightly worse dependence
on the horizon H for the multi-player general-sum SGs. The information
theoretical lower bound of multi-player general-sum SGs is proved to be
i∈[N ] i
) for NE, CE and CCEJin et al. (2021a); Mao and Başar
Ω̃(
ϵ2
(2022). A summarization of value-based methods for multi-player general-sum
SGs is shown in Table 8.1. Among them, Jin et al. (2021a) achieves tighter
sample complexity bounds, which is why we only include its complexity results
in this table.
Value-based methods have been slower to mature in the context of generalsum games. Recent work focuses on adapting neural networks to approximate
enforceable payoff frontiers (EPF) Song et al. (2022), particularly for Stackelberg problems. These methods aim to reduce computational costs while
maintaining effective value propagation in games with complex strategies.

8.3. Solutions by Two-Timescale Analysis

127

However, significant work remains to refine these methods, particularly in
handling the complexity of value functions in non-cooperative settings and
improving their practical efficiency in dynamic environments.
Table 8.1 – Convergence guarantee for tabular multi-player general-sum stochastic
games.

Algorithm
Nash-VILiu et al. (2021)
V-learningJin et al. (2021a)

Solution Concept
NE, CE, CCE
CCE
CE

Lower BoundJin et al. (2021a);
Mao and Başar (2022)
8.3

NE, CE, CCE

Sample Complexity
N A
H 4 S 2 Πi=1
i
)
ϵ2
5
H S maxi∈[N ] Ai
)
Õ(
ϵ2
H 5 S(maxi∈[N ] Ai )2
)
Õ(
ϵ2
H 3 S maxi∈[N ] Ai
Ω̃(
)
ϵ2

Õ(

Solutions by Two-Timescale Analysis

In addition to the centralised Q-learning approach, decentralised Q-learning
algorithms have recently received considerable attention because of their
potential for scalability. Although independent learners have been accused
of having convergence issues Tan (1993), decentralised methods have made
substantial progress with the help of two-timescale stochastic analysis Borkar
(1997) and its application in RL Borkar (2002).
Two-timescale stochastic analysis is a set of tools certifying that, in a
system with two coupled stochastic processes that evolve at different speeds, if
the fast process converges to a unique limit point for any particular fixed value
of the slow process, we can, quantitatively, analyse the asymptotic behaviour
of the algorithm as if the fast process is always fully calibrated to the current
value of the slow process Borkar (1997). As a direct application, Leslie et al.
(2003); Leslie and Collins (2005) noted that independent Q-learners with
agent-dependent learning rates could break the symmetry that leads to the
non-convergent limit cycles; as a result, they can converge almost surely to
the NE in two-player collaboration games, two-player zero-sum games, and
multi-player matching pennies. Similarly, Prasad et al. (2015) introduced a
two-timescale update rule that ensures the training dynamics reach a stationary
local NE in general-sum SGs if the critic learns faster than the actor. Later,
Perkins et al. (2015) proposed a distributed actor-critic algorithm that enjoys
provable convergence in solving static potential games with continuous actions.

128

Learning in General-Sum Games

Similarly, Arslan and Yüksel (2016) developed a two-timescale variant of
Q-learning that is guaranteed to converge to an equilibrium in SGs with weakly
acyclic characteristics, which generalises potential games. Other applications
include developing two-timescale update rules for training GANs Heusel et al.
(2017) and developing a two-timescale algorithm with guaranteed asymptotic
convergence to the Stackelberg equilibrium in general-sum Stackelberg games.
Two-timescale analysis has emerged as an important tool for improving
the convergence speed and sample efficiency in general-sum extensive-form
games. This method differentiates between fast and slow timescales in the
learning process, making it particularly useful in settings with bandit feedback,
such as the K-EFCE framework Song et al. (2022). By decoupling the update
rates of different game variables, these methods enable more efficient learning
processes, especially in large, multi-agent environments. However, balancing
the timescales for optimal performance and ensuring convergence across
different agent types remain ongoing challenges.
8.4

Solutions by Policy-Based Methods

Convergence to NE via direct policy search has been extensively studied;
however, early results were limited mainly by stateless two-player two-action
games Singh et al. (2000); Bowling and Veloso (2002); Bowling (2005);
Conitzer and Sandholm (2007); Abdallah and Lesser (2008); Zhang and Lesser
(2010). Recently, GAN training has posed a new challenge, thereby rekindling
interest in understanding the policy gradient dynamics of continuous games
Nagarajan and Kolter (2017); Heusel et al. (2017); Mescheder et al. (2018,
2017).
Analysing gradient-based algorithms through dynamic systems Shub
(2013) is a natural approach to yield more significant insights into convergence
behaviour. However, a fundamental difference is observed when one attempts
to apply the same analysis from the single-agent case to the multiagent
case because the combined dynamics of gradient-based learning schemes in
multiagent games do not necessarily correspond to a proper gradient flow –
a critical premise for almost sure convergence to a local minimum. In fact,
the difficulty of solving general-sum continuous games is exacerbated by the
usage of deep networks with stochastic gradient descent. In this context, a
key equilibrium concept of interest is the local NE Ratliff et al. (2013) or

8.4. Solutions by Policy-Based Methods

129

differential NE Ratliff et al. (2014), defined as follows.
Definition 8.3 (Local Nash Equilibrium). For an N -player continuous game

denoted by {ℓi : Rd → R}i∈{1,...,N } with each agent’s loss ℓi being twice continuously differentiable, the parameters are w = (w1 , ..., wn ) ∈ Rd , and each
P
player controls wi ∈ Rdi , i di = d. Let ξ(w) = (∇w1 ℓ1 , . . . , ∇wn ℓn ) ∈
Rd be the simultaneous gradient of the losses w.r.t. the parameters of the
respective players, and let H(w) := ∇w · ξ(w)⊤ be the (d × d) Hessian
matrix of the gradient, written as
∇2w1 ℓ1
 ∇2w ,w ℓ2
2
1

H(w) = 
..

.


∇2wn ,w1 ℓn

∇2w1 ,w2 ℓ1
∇2w2 ℓ2
∇2wn ,w2 ℓn


· · · ∇2w1 ,wn ℓ1
· · · ∇2w2 ,wn ℓ2 


..

.
···

∇2wn ℓn

where ∇2wi ,wj ℓk is the (di ×dj ) block of 2nd-order derivatives. A differentiable
NE for the game is w∗ if ξ(w∗ ) = 0 and ∇2wi ℓi ≻ 0, ∀i ∈ {1, ..., N };
furthermore, this result is a local NE if det H(w∗ ) ̸= 0.
A recent result by Mazumdar et al. (2018) suggested that gradient-based
algorithms can almost surely avoid a subset of local NE in general-sum games;
even worse, there exist non-Nash stationary points. As a tentative treatment,
Balduzzi et al. (2018a) applied Helmholtz decomposition 2 to decompose the
game Hessian H(w) into a potential part plus a Hamiltonian part. Based
on the decomposition, they designed a gradient-based method to address
each part and combined them into symplectic gradient adjustment (GDA),
which is able to find all local NE for zero-sum games and a subset of local
NE for general-sum games. More recently, Chasnov et al. (2019) separately
considered the cases of 1) agents with oracle access to the exact gradient ξ(w)
and 2) agents with only an unbiased estimator for ξ(w). In the first case, they
provided asymptotic and finite-time convergence rates for the gradient-based
learning process to reach the differential NE. In the second case, they derived
concentration bounds guaranteeing with high probability that agents will
converge to a neighbourhood of a stable local NE in finite time. In the same
2. This approach is similar in ideology to the work by Candogan et al. (2011), where they
leverage the combinatorial Hodge decomposition to decompose any multi-player normal-form
game into a potential game plus a harmonic game. However, their equivalence is an open
question.

130

Learning in General-Sum Games

framework, Fiez et al. (2019) studied Stackelberg games in which agents take
turns to conduct the gradient update rather than acting simultaneously and
established the connection under which the equilibrium points of simultaneous
gradient descent are Stackelberg equilibria in zero-sum games. Mertikopoulos
and Zhou (2019) investigated the local convergence of no-regret learning and
found local NE is attracting under gradient play if and only if a NE satisfies a
property known as variational stability. This idea is inspired by the seminal
notion of evolutionary stability observed in animal populations Smith and
Price (1973).
Finally, it is worth highlighting that the above theoretical analysis of the
performance of gradient-based methods on stateless continuous games cannot
be taken for granted in SGs. The main reason is that the assumption on the
differentiability of the loss function required in continuous games may not hold
in general-sum SGs. As clearly noted by Mazumdar et al. (2019a); Fazel et al.
(2018); Zhang et al. (2019c), even in the extreme setting of linear-quadratic
games, the value functions are not guaranteed to be globally smooth (w.r.t.
each agent’s policy parameter).

9
Learning in Games when N → +∞

As detailed in Section 4, designing learning algorithms in a multiagent
system with N ≫ 2 is a challenging task. One major reason is that the
solution concept, such as Nash equilibrium, is difficult to compute in general
due to the curse of dimensionality of the multiagent problem itself. However,
if one considers a continuum of agents with N → +∞, then the learning
problem becomes surprisingly tractable. The intuition is that one can effectively
transform a many-body interaction problem into a two-body interaction problem
(i.e., agent vs the population mean) via mean-field approximation.
The idea of mean-field approximation, which considers the behaviour of
large numbers of particles where individual particles have a negligible impact
on the system, originated from physics. Important applications include solving
Ising models 1 Weiss (1907); Kadanoff (2009), or more recently, understanding
1. An Ising model is a model used to study magnetic phase transitions under different
system temperatures. In a 2D Ising model, one can imagine the magnetic spins are laid out on
a lattice, and each spin can have one of two directions, either up or down. When the system
temperature is high, the direction of the spins is chaotic, and when the temperature is low, the
directions of the spins tend to be aligned. Without the mean-field approximation, computing
the probability of the spin direction is a combinatorial hard problem; for example, in a 5 × 5
2D lattice, there are 225 possible spin configurations. A successful approach to solving the
Ising model is to observe the phase change under different temperatures and compare it against
the ground truth.

131

Learning in Games when N → +∞

132

Learning in games

N-player
Stochastic Game
Em
piri
cal
av

N→+∞

Mean-field/
Mckean-Vlasov
dynamics

era
g

e fo

Nash equilibrium
of N-player game

N→+∞
r st
ates
/act
ion
s

of f
init
e

N

Mean-field Game
≠
Mean-field MARL
≠
Mean-field Control

Learning in games

Figure 9.1 – Relations of mean-field learning algorithms in games with large N .

the learning dynamics of over-parameterised deep neural networks Lu et al.
(2020b); Song et al. (2018); Sirignano and Spiliopoulos (2020); Hu et al.
(2019). In the game theory and MARL context, mean-field approximation
essentially enables one to think of the interactions between every possible
permutation of agents as an interaction between each agent itself and the
aggregated mean effect of the population of the other agents, such that the
N -player game (N → +∞) turns into a “two”-player game. Moreover, under
the law of large numbers and the theory of propagation of chaos Gärtner (1988);
McKean (1967); Sznitman (1991), the aggregated version of the optimisation
problem in Eq. (9.3) asymptotically approximates the original N -player game.
The assumption in the mean-field regime that each agent responds only to
the mean effect of the population may appear rather limited initially; however,
for many real-world applications, agents often cannot access the information
of all other agents but can instead know the global information about the
population. For example, in high-frequency trading in finance Cardaliaguet
and Lehalle (2018); Lehalle and Mouzouni (2019), each trader cannot know
every other trader’s position in the market, although they have access to the
aggregated order book from the exchange. Another example is real-time
bidding for online advertisements Guo et al. (2019); Iyer et al. (2014), in which
participants can only observe, for example, the second-best prize that wins the
auction but not the individual bids from other participants.
There is a subtlety associated with types of games in which one applies
the mean-field theory. If one applies the mean-field type theory in non-

9.1. Non-cooperative Mean-Field Game

133

cooperative 2 games, in which agents act independently to maximize their
own individual reward, and the solution concept is NE, then the scenario
is usually referred to as a mean-field game (MFG) Jovanovic and Rosenthal
(1988); Huang et al. (2006); Lasry and Lions (2007); Guéant et al. (2011).
If one applies mean-field theory in cooperative games in which there exists
a central controller to control all agents cooperatively to reach some Pareto
optimality, then the situation is usually referred to as mean-field control (MFC)
Bensoussan et al. (2013); Andersson and Djehiche (2011), or McKean-Vlasov
dynamics (MKV) control. If one applies the mean-field approximation to solve
a standard SG through MARL, specifically, to factorize each agent’s reward
function or the joint-Q function, such that they depend only on the agent’s
local state and the mean action of others, then it is called mean-field MARL
(MF-MARL) Yang et al. (2018b); Subramanian et al. (2020); Zhou et al. (2019).
Despite the difference in the applicable game types, technically, the differences among MFG/MFC/MF-MARL can be elaborated from the perspective
of the order in which the equilibrium is learned (optimised) and the limit as
N → +∞ is taken Carmona et al. (2013). MFG learns the equilibrium of the
game first and then takes the limit as N → +∞, while MFC takes the limit
first and optimises the equilibrium later. MF-MARL is somewhat in between.
The mean-field in MF-MARL refers to the empirical average of the states
and/or actions of a finite population; N does not have to reach infinity, though
the approximation converges asymptotically to the original game when N is
large. This result is in contrast to the mean-field in MFG and MFC, which
is essentially a probability distribution of states and/or actions of an infinite
population (i.e., the Mckean-Vlasov dynamics). Before providing more details,
we summarise the relationships of MFG, MFC, and MF-MARL in Figure 9.1.
Readers are recommended to revisit their differences after finishing reading
the below subsections.
9.1

Non-cooperative Mean-Field Game

MFGs have been widely studied in different domains, including physics,
economics, and stochastic control Carmona et al. (2018); Guéant et al. (2011).
2. Note that the word “non-cooperative” does not mean agents cannot collaborate to
complete a task, it means agents cannot collude to form a coalition: they have to behave
independently.

134

Learning in Games when N → +∞

An intuitive example to quickly illustrate the idea of MFG is the problem of
when does the meeting start Guéant et al. (2011). For a meeting in the real
world, people often schedule a calendar time t in advance, and the actual start
time T depends on when the majority of participants (e.g., 90%) arrive. Each
participant plans to arrive at τ i , and the actual arrival time, τ̃ i = τ i + σ i ϵi ,
is often influenced by some uncontrolled factors σ i ϵi , ϵi ∼ N (0, 1), such as
weather or traffic. Assuming all players are rational, they do not want to
be later than either t or T ; moreover, they do not want to arrive too early
and have to wait. The cost function of each individual can be written as


ci (t, T, τ̃ i ) = E α⌊τ̃ i − t⌋+ + β⌊τ̃ i − T ⌋+ + γ⌊T − τ̃ i ⌋+ , where α, β, γ are
constants. The key question to ask is when is the best time for an agent to
arrive, as a result, when will the meeting actually start, i.e., what is T ?
The challenge of the above problem lies in the coupled relationship between
T and τ i ; that is, in order to compute T , we need to know τ i , which is based on
T itself. Therefore, solving the time T is essentially equivalent to finding the
fixed point, if it exists, of the stochastic process that generates T . In fact, T can
be effectively computed through a two-step iterative process, and we denote
as Γ 1 and Γ 2 . At Γ 1 , given the current 3 value of T , each agent solves their
optimal arrival time τ i by minimising their cost Ri (t, T, τ̃ i ). At Γ 2 , agents
calibrate the new estimate of T based on all τ i values that were computed
in Γ 1 . Γ 1 and Γ 2 continue iterating until T converges to a fixed point, i.e.,
Γ 2 ◦ Γ 1 (T ∗ ) = T ∗ . The key insight is that the interaction with other agents
is captured simply by the mean-field quantity. Since the meeting starts only
when 90% of the people arrive, if one considers a continuum of players with
N → +∞, T becomes the 90th quantile of a distribution, and each agent
can easily find the best response. This result contrasts to the cases of a finite
number of players, in which the ordered statistic is intractable, especially when
N is large (but still finite).
Approximating an N -player SG by letting N → +∞ and letting each
player choose an optimal strategy in response to the population’s macroscopic
information (i.e., the mean field), though analytically friendly, is not costfree. In fact, MFG makes two major assumptions: 1) the impact of each
player’s action on the outcome is infinitesimal, resulting in all agents being
3. At time step 0, it can be a random guess. Since the fixed point exists, the final convergence
result is irrelevant to the initial guess.

9.1. Non-cooperative Mean-Field Game

135

identical, interchangeable, and indistinguishable; 2) each player maintains weak
interactions with others only through a mean field, denoted by Li ∈ ∆|S||A| ,
which is essentially a population state-action joint distribution
i



−i

−i

P



L = µ (·), α (·) =

j̸=i 1(s

lim

j = ·)

N −1

N →+∞

P

,

j̸=i 1(a

j = ·) 

N −1

(9.1)
where sj and aj player j’s local state 4 and local action. Therefore, for SGs that
do not share the homogeneity assumption 5 and weak interaction assumption,
MFG is not an effective approximation. Furthermore, since agents have no
identity in MFG, one can choose a representative agent (the agent index is thus
omitted) and write the formulation 6 of the MFG as


V s, π,

∞
Lt t=0





:= E

"∞
X

#
t

γ R (st , at , Lt ) s0 = s

t=0

subject to st+1 ∼ P (st , at , Lt ) , at ∼ πt (st ) .

(9.2)

Each agent applies a local policy 7 πt : S → ∆(A), which assumes the
population state is not observable. Note that both the reward function and the
transition dynamics depend on the sequence of the mean-field terms {Lt }∞
t=0 .
From each agent’s perspective, the MDP is time-varying and is determined by
all other agents.
The solution concept in MFG is a variant of the (Markov perfect) NE
named the mean-field equilibrium, which is a pair of {πt∗ , L∗t }t≥0 that satisfies
two conditions: 1) for fixed L∗ = {L∗t }, π ∗ = {πt∗ } is the optimal policy,
that is, V (s, π ∗ , L∗ ) ≥ V (s, π, L∗ ), ∀π, s; 2) L∗ matches with the generated
mean field when agents follow π ∗ . The two-step iteration process in the
4. Note that in mean-field learning in games, the state is not assumed to be global. This is
different from Dec-POMDP, in which there exists an observation function that maps the global
state to the local observation for each agent.
5. In fact, the homogeneity in MFG can be relaxed to allow agents to have (finite) different
types Lacker and Zariphopoulou (2019), though within each type, agents must be homogeneous.
6. MFG is more commonly formulated in a continuous-time setting in the domain of optimal
control, where it is typically composed by a backward Hamilton-Jacobi-Bellman equation
(e.g., the Bellman equation in RL is its discrete-time counterpart) that describes the optimal
control problem of an individual agent and a forward Fokker-Planck equation that describes the
dynamics of the aggregate distribution (i.e., the mean field) of the population.
7. A general non-local policy π(s, L) : S × ∆|S||A| → ∆(A) is also valid for MFG, and it
makes the learning easier by assuming L is fully observable.

136

Learning in Games when N → +∞

meeting start-time example applied in MFG is then expressed as Γ 1 (Lt ) = πt∗
and Γ 2 (Lt , πt∗ ) = Lt+1 , and it terminates when Γ 2 ◦ Γ 1 (L) = L = L∗ .
Mean-field equilibrium is essentially a fixed point of MFG, its existence for
discrete-time 8 discounted MFGs has been verified by Saldi et al. (2018) in
the infinite-population limit N → +∞ and also in the partially observable
setting Saldi et al. (2019). However, these works consider the case where the
mean field in MFG includes only the population state. Recently, Guo et al.
(2019) demonstrated the existence of NE in MFG, taking into account both the
population states and actions distributions. In addition, they proved that if Γ 1
and Γ 2 meet small parameter conditions Huang et al. (2006), then the NE is
unique in the sense of L∗ . In terms of uniqueness, a common result is based
on assuming monotonic cost functions Lasry and Lions (2007). In general,
MFGs admit multiple equilibria Nutz et al. (2020); the reachability of multiple
equilibria is studied when the cost functions are anti-monotonic Cecchin et al.
(2019) or quadratic Delarue and Tchuendom (2020).
Based on the two-step fixed-point iteration in MFGs, various model-free
RL algorithms have been proposed for learning the NE. The idea is that in the
step Γ 1 , one can approximate the optimal πt given Lt through single-agent
RL algorithms 9 such as (deep) Q-learning Guo et al. (2019); Anahtarcı et al.
(2019); Anahtarci et al. (2020), (deep) policy-gradient methods Guo et al.
(2020); Elie et al. (2020); Subramanian and Mahajan (2019); uz Zaman et al.
(2020), and actor-critic methods Yang et al. (2019a); Fu et al. (2019). Then, in
step Γ 2 , one can compute the forward Lt+1 by sampling the new πt directly
or via fictitious play Cardaliaguet and Hadikhanloo (2017); Hadikhanloo and
Silva (2019); Elie et al. (2019). A surprisingly good result is that the sample
complexity of both value-based and policy-based learning methods for MFG
shares the same order of magnitude as those of single-agent RL algorithms
Guo et al. (2020). However, one major subtlety of these learning algorithms for
MFGs is how to obtain stable samples for Lt+1 . For example, Guo et al. (2020)
discovered that applying a softmax policy for each agent and projecting the
8. The existence of equilibrium in continuous-time MFGs is widely studied in the area of
stochastic control Huang et al. (2006); Lasry and Lions (2007); Carmona and Delarue (2013);
Carmona et al. (2015b, 2016); Cardaliaguet et al. (2015); Fischer et al. (2017); Lacker (2018,
2015), though it may be of less interest to RL researchers.
9. Since agents in MFG are homogeneous, if the representative agent reaches convergence,
then the joint policy is the NE. Additionally, given Lt , the MDP to the representative agent is
stationary.

9.2. Cooperative Mean-Field Control

137

mean-field quantity on an ϵ-net with finite cover help to significantly stabilize
the forward propagation of Lt+1 .
9.2

Cooperative Mean-Field Control

MFC maintains the same homogeneity assumption and weak interaction
assumption as MFG. However, unlike MFG, in which each agent behaves independently, there is a central controller that coordinates all agents’ behaviours in
the context of MFC. In cooperative multiagent learning, assuming each agent
observes only a local state, the central controller maximises the aggregated
accumulative reward:
sup
π

X

N
1 X
Est+1 ∼P,at ∼π
γ t Ri (st , at ) s0 = s .
N i=1
t

(9.3)

Solving Eq. (9.3) is a combinatorial problem. Clearly, the sample complexity
of applying the Q-learning algorithm grows exponentially in N Even-Dar and
Mansour (2003). To avoid the curse of dimensionality in N , MFC Carmona
et al. (2018); Gu et al. (2019) pushes N → +∞, and under the law of large
numbers and the theory of propagation of chaos Gärtner (1988); McKean
(1967); Sznitman (1991), the optimisation problem in Eq. (9.3), in the view of
a representative agent, can be equivalently written as
X

sup E
π

t



γ R̃(st , at , µt , αt ) s0 ∼ µ

t

subject to st+1 ∼ P (st , at , µt , αt ) , at ∼ πt (st , µt ) .

(9.4)

in which (µt , αt ) is the respective state and action marginal distribution
P
of the mean-field quantity, µt (·) = limN →+∞ N
1(sit = ·)/N , αt (·) =
P
P i=1
i
s∈S µt (s) · πt (s, µt ) (·), and R̃ = limN →+∞
i R /N. The MFC approach
is attractive not only because the dimension of MFC is independent of N , but
also because MFC has shown to approximate the original cooperative game in
terms of both game values and optimal strategies Lacker (2017); Motte and
Pham (2019).
Although the MFC formulation in Eq. (9.4) appears similar to the MFG
formulation in Eq. (9.2), their underlying physical meaning is fundamentally
different. As is illustrated in Figure 9.1, the difference is which operation is
performed first: learning the equilibrium of the N -player game or taking the

138

Learning in Games when N → +∞

limit as N → +∞. In the fixed-point iteration of MFG, one first assumes Lt
is given and then lets the (infinite) number of agents find the best response to
Lt , while in MFC, one assumes an infinite number of agents to avoid the curse
of dimensionality in cooperative MARL and then finds the optimal policy for
each agent from a central controller perspective. In addition, compared to
mean-field NE in MFG, the solution concept of the central controller in MFC
is the Pareto optimum 10, an equilibrium point where no individual can be
better off without making others worse off. It is worth noting that when there
are multiple equilibria and the agent may have different preferences between
them, learning to play Pareto optimal strict Nash equilibrium is helpful Wang
and Sandholm (2003a). Finally, other differences between MFG and MFC can
be found in Carmona et al. (2013).
In MFC, since the marginal distribution of states serves as an input in
the agent’s policy and is no longer assumed to be known in each iteration
(in contrast to MFG), the dynamic programming principle no longer holds
in MFC due to its non-Markovian nature Andersson and Djehiche (2011);
Buckdahn et al. (2011); Carmona et al. (2015a). That is, MFC problems are
inherently time-inconsistent. A counter-example of the failure of standard
Q-learning in MFC can be found in Gu et al. (2019). One solution is to
learn MFC by adding common noise to the underlying dynamics such that
all existing theory on learning MDP with stochastic dynamics can be applied,
such as Q-learning Carmona et al. (2019b). In the special class of linearquadratic MFCs, Carmona et al. (2019a) studied the policy-gradient method
and its convergence, and Luo et al. (2019) explored an actor-critic algorithm.
However, this approach of adding common noise still suffers from high sample
complexity and weak empirical performance Gu et al. (2019). Importantly,
applying dynamic programming in this setting lacks rigorous verifications,
leaving aside the measurability issues and the existence of a stationary optimal
policy.
Another way to address the time inconsistency in MFCs is to consider an
enlarged state-action space Laurière and Pironneau (2014); Pham and Wei
(2016, 2018, 2017); Djete et al. (2019); Gu et al. (2019). This technique is also
called “lift up”, which essentially means to lift up the state space and the action
space into their corresponding probability measure spaces in which dynamic
10. The Pareto optimum is a subset of NE.

9.3. Mean-Field MARL

139

programming principles hold. For example, Gu et al. (2019); Motte and Pham
(2019) proposed to lift the finite state-action space S and A to a compact
state-action space embedded in Euclidean space denoted by C := ∆(S) × H

and H := h : S → ∆(A) , and the optimal Q-function associated with the
MFC problem in Eq. (9.4) is

QC (µ, h) = sup E
π

"∞
X

#
t



γ R̃ st , at , µt , αt s0 ∼ µ, u0 ∼ α, at ∼ πt , ∀(µ, h) ∼ C.

t=0

(9.5)

The physical meaning of H is the set of all possible local policies h : S → ∆(A)
over all different states. Note that after lift up, the mean-field term µt in πt
of Eq. (9.4) no longer exists as an input to h. Although the support of each
h is |∆(A)||S| , it proves to be the minimum space under which the Bellman
equation can hold. The Bellman equation for QC : C → R is






QC (µ, h) = R µ, h + γ sup QC Φ(µ, h), h̃

(9.6)

h̃∈H

where R and Φ are the reward function and transition dynamics written as


XX



XX

R µ, h =

R̃ s, a, µ, α(µ, h) · µ(s) · h(s)(a)



(9.7)



(9.8)

s∈S a∈A

Φ µ, h =

P s, a, µ, α(µ, h) · µ(s) · h(s)(a)

s∈S a∈A

with α(µ, h)(·) := s∈S µ(s) · h(s)(·) representing the marginal distribution
of the mean-field quantity in action. The optimal value function is V ∗ (µ) =

maxh∈H QC µ, h . Since both µ and h are probability distributions, the
difficulty of learning MFC then changes to how to deal with continuous state
and continuous action inputs to QC (µ, h), which is still an open research
question. Gu et al. (2020) tried to discretise the lifted space C through ϵ-net and
then adopted the kernel regression on top of the discretisation; impressively,
the sample complexity of the induced Q-learning algorithm is independent of
the number of agents N .
P

9.3

Mean-Field MARL

The scalability issue of multiagent learning in non-cooperative generalsum games can also be alleviated by applying the mean-field approximation

Learning in Games when N → +∞

140

directly to each agent’s Q-function Zhou et al. (2019); Yang et al. (2018b);
Subramanian et al. (2020). In fact, Yang et al. (2018b) was the first to combine
mean-field theory with the MARL algorithm. The idea is to first factorise
the Q-function using only the local pairwise interactions between agents (see
Eq. (9.9)) and then apply the mean-field approximation; specifically, one can
write the neighbouring agent’s action ak as the sum of the mean action āj
P
and a fluctuation term δaj,k , i.e., ak = āj + δaj,k , āj = N1j k ak , in which
N (j) is the set of neighbouring agents of the learning agent j with its size
being N j = |N j |. With the above two processes, we can reach the mean-field
Q-function Qj (s, aj , āj ) that approximates Qj (s, a) as follows


1 X j
Qj s, a = j
Q s, aj , ak
N
k



1 X j
= j
Q s, aj , āj + ∇āj Qj s, aj , āj · δaj,k
N

(9.9)

k



1 j,k
δa · ∇2ãj,k Qj s, aj , ãj,k · δaj,k
2




1 X j,k
= Qj s, aj , āj + ∇āj Qj s, aj , āj ·
δa
Nj
k



1 X
j,k
2
j
j
j,k
j,k
+
δa
·
∇
Q
s,
a
,
ã
·
δa
j,k
ã
2N j
k


1 X j
Rs,aj ak
= Qj s, aj , āj +
2N j
k

≈ Qj s, aj , āj .
+

(9.10)

(9.11)

(9.12)

The second term in Eq. (9.11) is zero by definition, and the third term can
be bounded if the Q-function is smooth, and it is neglected on purpose. The
mean-field action āj can be interpreted as the empirical distribution of the
actions taken by agent j’s neighbours. However, unlike the mean-field quantity
in MFG or MFC, this quantity does not have to assume an infinite population
of agents, which is more friendly for many real-world tasks, although a large N
can reduce the approximation error between ak and āj due to the law of large
numbers. In addition, the mean-field term in MF-MARL does not include the
state distribution, unlike MFG or MFC.
Based on the mean-field Q-function, one can write the Q-learning update
as

9.3. Mean-Field MARL

141

h



i
Qjt+1 s, aj , āj = 1 − α Qjt s, aj , āj + α Rj + γvtj,MF s′
h
 X j j ′ j
i
vtj,MF s′ =
πt a | s , ā · Eāj (a−j )∼π−j Qjt s′ , aj , āj .
t

(9.13)

aj

The mean action āj depends on aj , j ∈ N (j), which itself depends on the mean
action. The chicken-and-egg problem is essentially the time inconsistency that
also occurs in MFC. To avoid coupling between aj and āj , Yang et al. (2018b)
proposed a filtration such that in each stage game {Qt }, the mean action āj is
P
computed first using each agents’ current policies, i.e., āj = N1j k ak , ak ∼
πtk , and then given āj , each agent finds the best response by
πtj



exp βQjt s, aj , āj

.

aj | s, āj = P
j
j ′ , āj )
exp
βQ
(s,
a
j
j
t
a ∈A

(9.14)

For large β, the Boltzmann policy in Eq. (9.14) proves to be a contraction
mapping, which means the optimal action aj is unique given āj ; therefore, the
chicken-and-egg problem is resolved 11.
MF-Q can be regarded as a modification of the Nash-Q learning algorithm
Hu and Wellman (2003), with the solution concept changed from NE to meanfield NE (see the definition in MFG). As a result, under the same conditions,
which include the strong assumption that there
h exists a unique NE at ievery stage
game encountered, HMF Q(s, a) = Es′ ∼p R(s, a) + γv MF (s′ ) proves to
be a contraction operator. Furthermore, the asymptotic convergence of the
MF-Q learning update in Eq. (9.13) has also been established.
Considering only pairwise interactions in MF-Q may appear rather limited.
However, it has been noted that the pairwise approximation of the agent and
its neighbours, while significantly reducing the complexity of the interactions
among agents, can still preserve global interactions between any pair of agents
Blume (1993). In fact, such an approach is widely adopted in other machine
learning domains, for example, factorisation machines Rendle (2010) and
learning to rank Cao et al. (2007). Based on MF-Q, Li et al. (2019a) solved the
real-world taxi order dispatching task for Uber China and demonstrated strong
11. Coincidentally, the techniques of fixing the mean-field term first and adopting the
Boltzmann policy for each agent were discovered by Guo et al. (2019) in learning MFGs at the
same time.

142

Learning in Games when N → +∞

empirical performance against humans. Subramanian and Mahajan (2019)
extended MF-Q to include multiple types of agents and applied the method
to a large-scale predator-prey simulation scenario. Ganapathi Subramanian
et al. (2020) further relaxed the assumption that agents have access to exact
cumulative metrics regarding the mean-field behaviour of the system, and
proposed partially observable MF-Q that maintains a distribution to model the
uncertainty regarding the mean field of the system.

10
Future Directions

MARL Theory.

In contrast to the remarkable empirical success of MARL
methods, developing theoretical understandings of MARL techniques are
very much under-explored in the literature. Although many early works have
been conducted on understanding the convergence property and the finitesample bound of single-agent RL algorithms Bertsekas and Tsitsiklis (1996),
extending those results into multiagent, even many-agent, settings seem to
be non-trivial. Furthermore, it has become a common practice nowadays
to use DNNs to represent value functions in RL and multiagent RL. In fact,
many recent remarkable successes of multiagent RL benefit from the success
of deep learning techniques Vinyals et al. (2019d); Baker et al. (2019b);
Pachocki et al. (2018). Therefore, there are pressing needs to develop theories
that could explain and offer insights into the effectiveness of deep MARL
methods. Overall, we believe there is an approximate ten-year gap between the
theoretical developments of single-agent RL and multiagent RL algorithms.
Learning the lessons from single-agent RL theories and extending them into
multiagent settings, especially understanding the incurred difficulty due to
involving multiple agents, and then generalising the theoretical results to
include DNNs could probably act as a practical road map in developing MARL
theories. Along this thread, we recommend the work of Zhang et al. (2019b)
143

144

Future Directions

for a comprehensive summary of existing MARL algorithms that come with
theoretical convergence guarantee.
Safe and Robust MARL.

Although RL provides a general framework for
optimal decision making, it has to incorporate certain types of constraints
when RL models are truly to be deployed in the real-world environment. We
believe it is critical to firstly account for MARL with robustness and safety
constraints; one direct example is on autonomous driving. At a very high
level, robustness refers to the property that an algorithm can generalise and
maintain robust performance in settings that are different from the training
environment Morimoto and Doya (2005); Abdullah et al. (2019). And safety
refers to the property that an algorithm can only act in a pre-defined safety
zone with minimum times of violations even during training time Garcıa
and Fernández (2015). In fact, the community is still at the early stage of
developing theoretical frameworks to encompass either robust or safe constraint
in single-agent settings. In the multiagent setting, the problem could only
become more challenging because the solution now requires to take into
account the coupling effect between agents, especially those agents that have
conflict interests Li et al. (2019b). In addition to opponents, one should also
consider robustness towards the uncertainty of environmental dynamics Zhang
et al. (2020c), which in turn will change the behaviours of opponents and pose
a more significant challenge.
Model-Based MARL.

Most of the algorithms we have introduced in this
monograph are model-free, in the sense that the RL agent does not need to
know how the environment works and it can learn how to behave optimally
through purely interacting with the environment. In the classic control domain,
model-based approaches have been extensively studied in which the learning
agent will first build an explicit state-space “model” to understand how the
environment works in terms of state-transition dynamics and reward function,
and then learn from the “model”. The benefit of model-based algorithms
lies in the fact that they often require much fewer data samples from the
environment Deisenroth and Rasmussen (2011). The MARL community
has initially come up with model-based approaches, for example the famous
R-MAX algorithm Brafman and Tennenholtz (2002), nearly two decades ago.
Surprisingly, the developments along the model-based thread halted ever since.

145

Given the impressive results that model-based approaches have demonstrated
on single-agent RL tasks Schrittwieser et al. (2020); Hafner et al. (2019a,b),
model-based MARL approaches deserves more attention from the community.
Multiagent Meta-RL.

Throughout this monograph, we have introduced
many MARL applications; each task needs a bespoke MARL model to solve.
A natural question to ask is whether we can use one model that can generalise
across multiple tasks. For example, Terry et al. (2020) has put together almost
one hundred MARL tasks, including Atari, robotics, and various kinds of
board games and pokers into a Gym API. An ambitious goal is to develop
algorithms that can solve all of the tasks in one or a few shots. This requires
multiagent meta-RL techniques.
Meta-learning aims to train a generalised model on a variety of learning
tasks, such that it can solve new learning tasks with few or without additional
training samples. In the context of equilibrium finding and learning to play
games, a promising direction was explored by Harris et al. (2022), who
introduced meta-learning for game-theoretic settings. They provided the first
meta-learning guarantees for fundamental classes of games, such as twoplayer zero-sum games, general-sum games, and Stackelberg games. Their
framework leverages natural notions of similarity between sequences of games,
achieving faster convergence rates to equilibria compared to solving games
in isolation, while recovering single-game guarantees for arbitrary sequences.
Notably, their experiments with poker endgames demonstrated significant
efficiency gains, often by an order of magnitude, when applying meta-learning
techniques to games with varying stack sizes. Additionally, Anagnostides
et al. (2024) extended the meta-learning framework to the analysis of noregret learning algorithms, such as optimistic gradient descent (OGD), in
dynamic game environments. Their results highlighted how meta-learning
could refine variation-dependent regret bounds and enable sharper convergence
guarantees for time-varying zero-sum and general-sum games. This work
underscores the potential of meta-learning to adapt to dynamically changing
games, providing novel insights into learning equilibria efficiently in settings
where game parameters evolve over time.
Fortunately, Finn et al. (2017) has proposed a general meta-learning
framework – MAML – that is compatible with any model trained with gradientdescent based methods. Although MAML works well on supervised learning

146

Future Directions

tasks, developing meta-RL algorithms seems to be highly non-trivial Rothfuss
et al. (2018), and introducing the meta-learning framework on top of MARL is
even an uncharted territory. We expect multiagent meta-RL to be a challenging
yet fruitful research topic, since making a group of agents master multiple
games necessarily requires agents to automatically discover their identities and
roles when playing different games; this itself is a hot research idea. Besides,
the meta-learner in the outer loop would need to figure out how to compute
the gradients with respect to the entire inner-loop subroutine, which must be a
MARL algorithm such as multiagent policy gradient method or mean-field
Q-learning, and, this would probably lead to exciting enhancements to the
existing meta-learning framework.
Foundation Models for MARL.

Foundation models for decision making,
especially within the scope of multiagent reinforcement learning, symbolize the confluence of foundational models and sequential decision-making
conceptsYang et al. (2023). Foundation models, typically pretrained on vast
datasets through self-supervised learning, exhibit remarkable knowledge transfer capabilities to an array of downstream tasks. These models are being
increasingly employed for tasks requiring sophisticated control, long-term
reasoning, search, and planning - all essential components of sequential
decision-making processes. However, unique challenges arise when these
models interact with external entities or agents, such as learning from external
feedback, adapting to unique modalities, and performing long-term reasoning
and planning.
Sequential decision-making, including reinforcement learning, imitation
learning, planning, search, and optimal control, has typically relied on taskspecific or tabula rasa environments with limited prior knowledge. While these
methods have achieved commendable feats, they often grapple with issues
related to generalization and sample efficiency due to their start-from-scratch
approach, which omits broad knowledge from vision, language, or other
datasets.
Recently, a growing interest has been seen in merging the principles
of foundation models and sequential decision making. This shift is fueled
by the recognition that broad datasets can also greatly benefit sequential
decision making models, similar to how they’ve benefitted foundation models.
Large-scale vision and language models’ success has inspired the creation of

147

bigger datasets for learning multi-model, multi-task, and generalist interactive
agents. Simultaneously, foundation models have started branching into complex
problems involving long-term reasoning or multiple interactions. This mutually
beneficial integration has immense potential to leverage foundational models’
world knowledge to enhance sequential decision-making processes, ultimately
leading to faster task-solving and better generalization. The burgeoning
research area holds promising solutions for existing challenges and continues
to open up new avenues for exploration and innovation.

Bibliography

Abdallah, S. and V. Lesser (2008), ‘A multiagent reinforcement learning algorithm
with non-linear dynamics’. Journal of Artificial Intelligence Research 33, 521–549.
Abdullah, M. A., H. Ren, H. B. Ammar, V. Milenkovic, R. Luo, M. Zhang, and
J. Wang (2019), ‘Wasserstein robust reinforcement learning’. arXiv preprint
arXiv:1907.13196.
Adler, I. (2013), ‘The equivalence of linear programs and zero-sum games’. International Journal of Game Theory 42(1), 165–177.
Adler, J. L. and V. J. Blue (2002), ‘A cooperative multi-agent transportation management and route guidance system’. Transportation Research Part C: Emerging
Technologies 10(5-6), 433–454.
Adolphs, L., H. Daneshmand, A. Lucchi, and T. Hofmann (2019), ‘Local saddle
point optimization: A curvature exploitation approach’. In: The 22nd International
Conference on Artificial Intelligence and Statistics. pp. 486–495.
Akkaya, I., M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A.
Paino, M. Plappert, G. Powell, R. Ribas, et al. (2019), ‘Solving rubik’s cube with a
robot hand’. arXiv preprint arXiv:1910.07113.
Al-Tamimi, A., F. L. Lewis, and M. Abu-Khalaf (2007), ‘Model-free Q-learning
designs for linear discrete-time zero-sum games with application to H-infinity
control’. Automatica 43(3), 473–481.
Amato, C., D. S. Bernstein, and S. Zilberstein (2010), ‘Optimizing fixed-size stochastic
controllers for POMDPs and decentralized POMDPs’. Autonomous Agents and
Multi-Agent Systems 21(3), 293–320.

148

Bibliography

149

Anagnostides, I., G. Farina, C. Kroer, A. Celli, and T. Sandholm (2022a), ‘Faster
no-regret learning dynamics for extensive-form correlated and coarse correlated
equilibria’. arXiv preprint arXiv:2202.05446.
Anagnostides, I., G. Farina, C. Kroer, C.-W. Lee, H. Luo, and T. Sandholm (2022b),
‘Uncoupled learning dynamics with swap regret in multiplayer games’. Advances in
Neural Information Processing Systems 35, 3292–3304.
Anagnostides, I., G. Farina, I. Panageas, and T. Sandholm (2022c), ‘Optimistic Mirror
Descent Either Converges to Nash or to Strong Coarse Correlated Equilibria in
Bimatrix Games’. arXiv preprint arXiv:2203.12074.
Anagnostides, I. and I. Panageas (2022), ‘Frequency-Domain Representation of FirstOrder Methods: A Simple and Robust Framework of Analysis’. In: Symposium on
Simplicity in Algorithms (SOSA). pp. 131–160.
Anagnostides, I., I. Panageas, G. Farina, and T. Sandholm (2022d), ‘On Last-Iterate
Convergence Beyond Zero-Sum Games’. arXiv preprint arXiv:2203.12056.
Anagnostides, I., I. Panageas, G. Farina, and T. Sandholm (2024), ‘On the convergence
of no-regret learning dynamics in time-varying games’. Advances in Neural
Information Processing Systems 36.
Anahtarcı, B., C. D. Karıksız, and N. Saldi (2019), ‘Fitted Q-learning in mean-field
games’. arXiv preprint arXiv:1912.13309.
Anahtarci, B., C. D. Kariksiz, and N. Saldi (2020), ‘Q-Learning in Regularized
Mean-field Games’. arXiv preprint arXiv:2003.12151.
Andersson, D. and B. Djehiche (2011), ‘A maximum principle for SDEs of mean-field
type’. Applied Mathematics & Optimization 63(3), 341–356.
Arslan, G. and S. Yüksel (2016), ‘Decentralized Q-learning for stochastic teams and
games’. IEEE Transactions on Automatic Control 62(4), 1545–1558.
Auer, P., N. Cesa-Bianchi, Y. Freund, and R. E. Schapire (1995), ‘Gambling in a
rigged casino: The adversarial multi-armed bandit problem’. In: Proceedings of
IEEE 36th Annual Foundations of Computer Science. pp. 322–331.
Auer, P., N. Cesa-Bianchi, Y. Freund, and R. E. Schapire (2002), ‘The nonstochastic
multiarmed bandit problem’. SIAM journal on computing 32(1), 48–77.
Auer, P., T. Jaksch, and R. Ortner (2009), ‘Near-optimal regret bounds for reinforcement
learning’. In: Advances in neural information processing systems. pp. 89–96.
Azar, M. G., I. Osband, and R. Munos (2017), ‘Minimax Regret Bounds for Reinforcement Learning’. In: International Conference on Machine Learning. pp.
263–272.

150

Bibliography

Azizian, W., I. Mitliagkas, S. Lacoste-Julien, and G. Gidel (2020a), ‘A tight and
unified analysis of gradient-based methods for a whole spectrum of differentiable
games’. In: International Conference on Artificial Intelligence and Statistics. pp.
2863–2873.
Azizian, W., D. Scieur, I. Mitliagkas, S. Lacoste-Julien, and G. Gidel (2020b),
‘Accelerating smooth games by manipulating spectral shapes’. In: International
Conference on Artificial Intelligence and Statistics. pp. 1705–1715.
Bai, Y. and C. Jin (2020), ‘Provable self-play algorithms for competitive reinforcement
learning’. In: International conference on machine learning. pp. 551–560.
Bai, Y., C. Jin, S. Mei, and T. Yu (2022), ‘Near-optimal learning of extensive-form
games with imperfect information’. In: International Conference on Machine
Learning. pp. 1337–1382.
Bai, Y., C. Jin, and T. Yu (2020), ‘Near-optimal reinforcement learning with self-play’.
Advances in neural information processing systems 33, 2159–2170.
Bailey, J. P. and G. Piliouras (2018), ‘Multiplicative weights update in zero-sum games’.
In: Proceedings of the 2018 ACM Conference on Economics and Computation. pp.
321–338.
Bailey, J. P. and G. Piliouras (2019), ‘Multi-agent learning in network zero-sum games
is a Hamiltonian system’. arXiv preprint arXiv:1903.01720.
Baker, B., I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I.
Mordatch (2019a), ‘Emergent tool use from multi-agent autocurricula’. arXiv
preprint arXiv:1909.07528.
Baker, B., I. Kanitscheider, T. M. Markov, Y. Wu, G. Powell, B. McGrew, and I.
Mordatch (2019b), ‘Emergent Tool Use From Multi-Agent Autocurricula’. CoRR
abs/1909.07528.
Balduzzi, D., S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel (2018a),
‘The mechanics of n-player differentiable games’. In: International Conference on
Machine Learning. pp. 354–363.
Balduzzi, D., K. Tuyls, J. Perolat, and T. Graepel (2018b), ‘Re-evaluating evaluation’.
In: Advances in Neural Information Processing Systems. pp. 3268–3279.
Ballard, D. and S. Zhu (2002), ‘Overcoming Non-Stationarity in Uncommunicative
Learning’.
Barazandeh, B., D. A. Tarzanagh, and G. Michailidis (2021), ‘Solving a class of
non-convex min-max games using adaptive momentum methods’. In: ICASSP 20212021 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). pp. 3625–3629.

Bibliography

151

Bard, N., M. Johanson, and M. Bowling (2014), ‘Asymmetric abstractions for
adversarial settings’. In: Proceedings of the 2014 international conference on
Autonomous agents and multi-agent systems. pp. 501–508.
Basilico, N., A. Celli, G. D. Nittis, and N. Gatti (2017), ‘Team-maxmin equilibrium:
efficiency bounds and algorithms’. In: Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence. pp. 356–362.
Beal, M. J. (2003), Variational algorithms for approximate Bayesian inference.
University of London, University College London (United Kingdom).
Bellman, R. (1952), ‘On the theory of dynamic programming’. Proceedings of the
National Academy of Sciences of the United States of America 38(8), 716.
Benaim, M. and M. Faure (2013), ‘Consistency of vanishingly smooth fictitious play’.
Mathematics of Operations Research 38(3), 437–450.
Benaım, M. and M. W. Hirsch (1999), ‘Mixed equilibria and dynamical systems
arising from fictitious play in perturbed games’. Games and Economic Behavior
29(1-2), 36–72.
Bensoussan, A., J. Frehse, P. Yam, et al. (2013), Mean field games and mean field
type control theory, Vol. 101. Springer.
Berger, U. (2007), ‘Brown’s original fictitious play’. Journal of Economic Theory
135(1), 572–578.
Berner, C., G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi,
Q. Fischer, S. Hashme, C. Hesse, et al. (2019a), ‘Dota 2 with Large Scale Deep
Reinforcement Learning’. arXiv preprint arXiv:1912.06680.
Berner, C., G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi,
Q. Fischer, S. Hashme, C. Hesse, et al. (2019b), ‘Dota 2 with large scale deep
reinforcement learning’. arXiv preprint arXiv:1912.06680.
Bernstein, D. S., C. Amato, E. A. Hansen, and S. Zilberstein (2009), ‘Policy iteration
for decentralized control of Markov decision processes’. Journal of Artificial
Intelligence Research 34, 89–132.
Bernstein, D. S., R. Givan, N. Immerman, and S. Zilberstein (2002), ‘The complexity
of decentralized control of Markov decision processes’. Mathematics of operations
research 27(4), 819–840.
Bertsekas, D. P. (2005), ‘The dynamic programming algorithm’. Dynamic Programming and Optimal Control; Athena Scientific: Nashua, NH, USA pp. 2–51.
Bertsekas, D. P. and J. N. Tsitsiklis (1996), Neuro-dynamic programming. Athena
Scientific.

152

Bibliography

Billings, D., N. Burch, A. Davidson, R. Holte, J. Schaeffer, T. Schauenberg, and D.
Szafron (2003), ‘Approximating game-theoretic optimal strategies for full-scale
poker’. In: ĲCAI, Vol. 3. p. 661.
Blackwell, D. et al. (1956), ‘An analog of the minimax theorem for vector payoffs.’.
Pacific Journal of Mathematics 6(1), 1–8.
Blei, D. M., A. Kucukelbir, and J. D. McAuliffe (2017), ‘Variational inference: A
review for statisticians’. Journal of the American statistical Association 112(518),
859–877.
Bloembergen, D., K. Tuyls, D. Hennes, and M. Kaisers (2015), ‘Evolutionary dynamics
of multi-agent learning: A survey’. Journal of Artificial Intelligence Research 53,
659–697.
Blume, L. E. (1993), ‘The statistical mechanics of strategic interaction’. Games and
economic behavior 5(3), 387–424.
Böhm, A. (2022), ‘Solving Nonconvex-Nonconcave Min-Max Problems exhibiting
Weak Minty Solutions’. arXiv preprint arXiv:2201.12247.
Borkar, V. S. (1997), ‘Stochastic approximation with two time scales’. Systems &
Control Letters 29(5), 291–294.
Borkar, V. S. (2002), ‘Reinforcement learning in Markovian evolutionary games’.
Advances in Complex Systems 5(01), 55–72.
Boutilier, C., T. Dean, and S. Hanks (1999), ‘Decision-theoretic planning: Structural
assumptions and computational leverage’. Journal of Artificial Intelligence Research
11, 1–94.
Bowling, M. (2000), ‘Convergence problems of general-sum multiagent reinforcement
learning’. In: ICML. pp. 89–94.
Bowling, M. (2005), ‘Convergence and no-regret in multiagent learning’. In: Advances
in neural information processing systems. pp. 209–216.
Bowling, M. and M. Veloso (2000), ‘An analysis of stochastic game theory for multiagent reinforcement learning’. Technical report, Carnegie-Mellon Univ Pittsburgh
Pa School of Computer Science.
Bowling, M. and M. Veloso (2001), ‘Rational and convergent learning in stochastic
games’. In: International joint conference on artificial intelligence, Vol. 17. pp.
1021–1026.
Bowling, M. and M. Veloso (2002), ‘Multiagent learning using a variable learning
rate’. Artificial Intelligence 136(2), 215–250.
BoHERE!HERE!hm, A., M. Sedlmayer, E. R. Csetnek, and R. I. Bot (2022), ‘Two
Steps at a Time—Taking GAN Training in Stride with Tseng’s Method’. SIAM
Journal on Mathematics of Data Science 4(2), 750–771.

Bibliography

153

Brafman, R. I. and M. Tennenholtz (2002), ‘R-max-a general polynomial time
algorithm for near-optimal reinforcement learning’. Journal of Machine Learning
Research 3(Oct), 213–231.
Breton, M., J. A. Filar, A. Haurle, and T. A. Schultz (1986), ‘On the computation
of equilibria in discounted stochastic dynamic games’. In: Dynamic games and
applications in economics. Springer, pp. 64–87.
Brown, G. W. (1951), ‘Iterative solution of games by fictitious play’. Activity analysis
of production and allocation 13(1), 374–376.
Brown, N., A. Bakhtin, A. Lerer, and Q. Gong (2020), ‘Combining deep reinforcement learning and search for imperfect-information games’. Advances in Neural
Information Processing Systems 33, 17057–17069.
Brown, N., C. Kroer, and T. Sandholm (2017), ‘Dynamic thresholding and pruning
for regret minimization’. In: Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 31.
Brown, N., A. Lerer, S. Gross, and T. Sandholm (2019), ‘Deep counterfactual regret
minimization’. In: International Conference on Machine Learning. pp. 793–802.
Brown, N. and T. Sandholm (2015a), ‘Regret-Based Pruning in Extensive-Form
Games.’. In: NIPS. pp. 1972–1980.
Brown, N. and T. Sandholm (2017a), ‘Reduced space and faster convergence in
imperfect-information games via pruning’. In: International conference on machine
learning. pp. 596–604.
Brown, N. and T. Sandholm (2017b), ‘Safe and nested subgame solving for imperfectinformation games’. Advances in neural information processing systems 30.
Brown, N. and T. Sandholm (2018), ‘Superhuman AI for heads-up no-limit poker:
Libratus beats top professionals’. Science 359(6374), 418–424.
Brown, N. and T. Sandholm (2019), ‘Superhuman AI for multiplayer poker’. Science
365(6456), 885–890.
Brown, N., T. Sandholm, and B. Amos (2018), ‘Depth-limited solving for imperfectinformation games’. Advances in neural information processing systems 31.
Brown, N. and T. W. Sandholm (2015b), ‘Simultaneous abstraction and equilibrium
finding in games’.
Browne, C. B., E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen,
S. Tavener, D. Perez, S. Samothrakis, and S. Colton (2012), ‘A survey of monte
carlo tree search methods’. IEEE Transactions on Computational Intelligence and
AI in games 4(1), 1–43.
Bu, J., L. J. Ratliff, and M. Mesbahi (2019), ‘Global Convergence of Policy Gradient
for Sequential Zero-Sum Linear Quadratic Dynamic Games’. arXiv pp. arXiv–1911.

154

Bibliography

Buckdahn, R., B. Djehiche, and J. Li (2011), ‘A general stochastic maximum principle
for SDEs of mean-field type’. Applied Mathematics & Optimization 64(2), 197–216.
Burch, N. and M. Bowling (2013), ‘Cfr-d: Solving imperfect information games using
decomposition’. arXiv preprint arXiv:1303.4441 pp. 1–15.
Burch, N., M. Johanson, and M. Bowling (2014), ‘Solving imperfect information
games using decomposition’. In: Twenty-eighth AAAI conference on artificial
intelligence.
Burch, N., M. Lanctot, D. Szafron, and R. G. Gibson (2012), ‘Efficient Monte
Carlo counterfactual regret minimization in games with many player actions’. In:
Advances in Neural Information Processing Systems. pp. 1880–1888.
Buşoniu, L., R. Babuška, and B. De Schutter (2010), ‘Multi-agent reinforcement
learning: An overview’. In: Innovations in multi-agent systems and applications-1.
Springer, pp. 183–221.
Cai, Q., Z. Yang, C. Jin, and Z. Wang (2019a), ‘Provably efficient exploration in policy
optimization’. arXiv preprint arXiv:1912.05830.
Cai, Q., Z. Yang, J. D. Lee, and Z. Wang (2019b), ‘Neural temporal-difference learning
converges to global optima’. In: Advances in Neural Information Processing Systems.
pp. 11315–11326.
Camerer, C. F., T.-H. Ho, and J.-K. Chong (2002), ‘Sophisticated experience-weighted
attraction learning and strategic teaching in repeated games’. Journal of Economic
theory 104(1), 137–188.
Camerer, C. F., T.-H. Ho, and J.-K. Chong (2004), ‘A cognitive hierarchy model of
games’. The Quarterly Journal of Economics.
Campbell, M., A. J. Hoane Jr, and F.-h. Hsu (2002), ‘Deep blue’. Artificial intelligence
134(1-2), 57–83.
Campos-Rodriguez, R., L. Gonzalez-Jimenez, F. Cervantes-Alvarez, F. AmezcuaGarcia, and M. Fernandez-Garcia (2017), ‘Multiagent Systems in Automotive
Applications’. Multi-agent Systems p. 43.
Candogan, O., I. Menache, A. Ozdaglar, and P. A. Parrilo (2011), ‘Flows and decompositions of games: Harmonic and potential games’. Mathematics of Operations
Research 36(3), 474–503.
Cao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li (2007), ‘Learning to rank: from
pairwise approach to listwise approach’. In: Proceedings of the 24th international
conference on Machine learning. pp. 129–136.
Cardaliaguet, P., F. Delarue, J.-M. Lasry, and P.-L. Lions (2015), ‘The master
equation and the convergence problem in mean field games’. arXiv preprint
arXiv:1509.02505.

Bibliography

155

Cardaliaguet, P. and S. Hadikhanloo (2017), ‘Learning in mean field games: the
fictitious play’. ESAIM: Control, Optimisation and Calculus of Variations 23(2),
569–591.
Cardaliaguet, P. and C.-A. Lehalle (2018), ‘Mean field game of controls and an
application to trade crowding’. Mathematics and Financial Economics 12(3),
335–363.
Carmona, R. and F. Delarue (2013), ‘Probabilistic analysis of mean-field games’.
SIAM Journal on Control and Optimization 51(4), 2705–2734.
Carmona, R., F. Delarue, et al. (2015a), ‘Forward–backward stochastic differential
equations and controlled McKean–Vlasov dynamics’. The Annals of Probability
43(5), 2647–2700.
Carmona, R., F. Delarue, et al. (2018), Probabilistic Theory of Mean Field Games
with Applications I-II. Springer.
Carmona, R., F. Delarue, and A. Lachapelle (2013), ‘Control of McKean–Vlasov
dynamics versus mean field games’. Mathematics and Financial Economics 7(2),
131–166.
Carmona, R., F. Delarue, D. Lacker, et al. (2016), ‘Mean field games with common
noise’. The Annals of Probability 44(6), 3740–3803.
Carmona, R., D. Lacker, et al. (2015b), ‘A probabilistic weak formulation of mean
field games and applications’. The Annals of Applied Probability 25(3), 1189–1231.
Carmona, R., M. Laurière, and Z. Tan (2019a), ‘Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods’. arXiv preprint
arXiv:1910.04295.
Carmona, R., M. Laurière, and Z. Tan (2019b), ‘Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning’. arXiv preprint
arXiv:1910.12802.
Cecchin, A., P. D. Pra, M. Fischer, and G. Pelino (2019), ‘On the convergence problem
in mean field games: a two state model without uniqueness’. SIAM Journal on
Control and Optimization 57(4), 2443–2466.
Celli, A. and N. Gatti (2018), ‘Computational Results for Extensive-Form Adversarial
Team Games’. In: AAAI Conference on Artificial Intelligence (AAAI). pp. 965–972.
Cen, S., Y. Wei, and Y. Chi (2021), ‘Fast policy extragradient methods for competitive
games with entropy regularization’. Advances in Neural Information Processing
Systems 34, 27952–27964.
Cermak, J., B. Bosansky, K. Durkota, V. Lisy, and C. Kiekintveld (2016), ‘Using
correlated strategies for computing stackelberg equilibria in extensive-form games’.
In: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30.

156

Bibliography

Cesa-Bianchi, N. and G. Lugosi (2006), Prediction, learning, and games. Cambridge
university press.
Chasnov, B., L. J. Ratliff, E. Mazumdar, and S. A. Burden (2019), ‘Convergence analysis of gradient-based learning with non-uniform learning rates in non-cooperative
multi-agent settings’. arXiv preprint arXiv:1906.00731.
Chen, X. and X. Deng (2006), ‘Settling the complexity of two-player Nash equilibrium’.
In: 2006 47th Annual IEEE Symposium on Foundations of Computer Science
(FOCS’06). pp. 261–272.
Chen, Z., D. Zhou, and Q. Gu (2021), ‘Almost optimal algorithms for twoplayer markov games with linear function approximation’. arXiv preprint
arXiv:2102.07404.
Cheung, W. C., D. Simchi-Levi, and R. Zhu (2020), ‘Reinforcement learning for
non-stationary markov decision processes: The blessing of (more) optimism’.
ICML.
Cheung, Y. K. and G. Piliouras (2019), ‘Vortices instead of equilibria in minmax
optimization: Chaos and butterfly effects of online learning in zero-sum games’. In:
Conference on Learning Theory. pp. 807–834.
Claus, C. and C. Boutilier (1998a), ‘The dynamics of reinforcement learning in
cooperative multiagent systems’. AAAI/IAAI 1998(746-752), 2.
Claus, C. and C. Boutilier (1998b), ‘The dynamics of reinforcement learning in
cooperative multiagent systems’. AAAI/IAAI 1998, 746–752.
Conitzer, V. and T. Sandholm (2003), ‘BL-WoLF: A framework for loss-bounded learnability in zero-sum games’. In: Proceedings of the 20th International Conference
on Machine Learning (ICML-03). pp. 91–98.
Conitzer, V. and T. Sandholm (2004a), ‘Communication complexity as a lower bound
for learning in games’. In: International Conference on Machine Learning (ICML).
Banff, Alberta, Canada, pp. 185–192.
Conitzer, V. and T. Sandholm (2004b), ‘Self-interested automated mechanism design
and implications for optimal combinatorial auctions’. In: Proceedings of the 5th
ACM Conference on Electronic Commerce. pp. 132–141.
Conitzer, V. and T. Sandholm (2007), ‘AWESOME: A general multiagent learning
algorithm that converges in self-play and learns a best response against stationary
opponents’. Machine Learning 67(1-2), 23–43.
Conitzer, V. and T. Sandholm (2008), ‘New complexity results about Nash equilibria’.
Games and Economic Behavior 63(2), 621–641.

Bibliography

157

Coricelli, G. and R. Nagel (2009), ‘Neural correlates of depth of strategic reasoning
in medial prefrontal cortex’. Proceedings of the National Academy of Sciences
106(23), 9163–9168.
Cowling, P. I., E. J. Powley, and D. Whitehouse (2012), ‘Information set monte carlo
tree search’. IEEE Transactions on Computational Intelligence and AI in Games
4(2), 120–143.
Da Silva, F. L. and A. H. R. Costa (2019), ‘A survey on transfer learning for multiagent
reinforcement learning systems’. Journal of Artificial Intelligence Research 64,
645–703.
Dall’Anese, E., H. Zhu, and G. B. Giannakis (2013), ‘Distributed optimal power flow
for smart microgrids’. IEEE Transactions on Smart Grid 4(3), 1464–1475.
Dantzig, G. (1951), ‘A proof of the equivalence of the programming problem and
the game problem, in “Activity analysis of production and allocation”(ed. TC
Koopmans), Cowles Commission Monograph, No. 13’.
Daskalakis, C., M. Fishelson, and N. Golowich (2021), ‘Near-optimal no-regret
learning in general games’. Advances in Neural Information Processing Systems 34,
27604–27616.
Daskalakis, C., D. J. Foster, and N. Golowich (2020), ‘Independent policy gradient
methods for competitive reinforcement learning’. Advances in neural information
processing systems 33, 5527–5540.
Daskalakis, C., P. W. Goldberg, and C. H. Papadimitriou (2009), ‘The complexity of
computing a Nash equilibrium’. SIAM Journal on Computing 39(1), 195–259.
Daskalakis, C., A. Ilyas, V. Syrgkanis, and H. Zeng (2017), ‘Training gans with
optimism’. arXiv preprint arXiv:1711.00141.
Daskalakis, C. and Q. Pan (2014), ‘A counter-example to Karlin’s strong conjecture
for fictitious play’. In: 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science. pp. 11–20.
Daskalakis, C. and I. Panageas (2018a), ‘Last-iterate convergence: Zero-sum games
and constrained min-max optimization’. arXiv preprint arXiv:1807.04252.
Daskalakis, C. and I. Panageas (2018b), ‘The limit points of (optimistic) gradient
descent in min-max optimization’. In: Advances in Neural Information Processing
Systems. pp. 9236–9246.
Daskalakis, C. and C. H. Papadimitriou (2005), ‘Three-player games are hard’. In:
Electronic colloquium on computational complexity, Vol. 139. pp. 81–87.

158

Bibliography

Degrave, J., F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds,
R. Hafner, A. Abdolmaleki, D. de Las Casas, et al. (2022), ‘Magnetic control
of tokamak plasmas through deep reinforcement learning’. Nature 602(7897),
414–419.
Deisenroth, M. and C. E. Rasmussen (2011), ‘PILCO: A model-based and dataefficient approach to policy search’. In: Proceedings of the 28th International
Conference on machine learning (ICML-11). pp. 465–472.
Delarue, F. and R. F. Tchuendom (2020), ‘Selection of equilibria in a linear quadratic
mean-field game’. Stochastic Processes and their Applications 130(2), 1000–1040.
Deng, Y., F. Bao, Y. Kong, Z. Ren, and Q. Dai (2016), ‘Deep direct reinforcement
learning for financial signal representation and trading’. IEEE transactions on
neural networks and learning systems 28(3), 653–664.
Deng, Y. and M. Mahdavi (2021), ‘Local stochastic gradient descent ascent: Convergence analysis and communication efficiency’. In: International Conference on
Artificial Intelligence and Statistics. pp. 1387–1395.
Derakhshan, F. and S. Yousefi (2019), ‘A review on the applications of multiagent
systems in wireless sensor networks’. International Journal of Distributed Sensor
Networks 15(5), 1550147719850767.
Dermed, L. M. and C. L. Isbell (2009), ‘Solving stochastic games’. In: Advances in
Neural Information Processing Systems. pp. 1186–1194.
Dibangoye, J. and O. Buffet (2018), ‘Learning to Act in Decentralized Partially
Observable MDPs’. In: International Conference on Machine Learning. pp. 1233–
1242.
Dibangoye, J. S., C. Amato, O. Buffet, and F. Charpillet (2016), ‘Optimally solving DecPOMDPs as continuous-state MDPs’. Journal of Artificial Intelligence Research
55, 443–497.
Dick, T., A. Gyorgy, and C. Szepesvari (2014), ‘Online learning in Markov decision
processes with changing cost sequences’. In: International Conference on Machine
Learning. pp. 512–520.
Ding, Z., D. Su, Q. Liu, and C. Jin (2022), ‘A Deep Reinforcement Learning Approach
for Finding Non-Exploitable Strategies in Two-Player Atari Games’. arXiv preprint
arXiv:2207.08894.
Djete, M. F., D. Possamaï, and X. Tan (2019), ‘McKean-Vlasov optimal control: the
dynamic programming principle’. arXiv preprint arXiv:1907.08860.
Doan, T. (2022), ‘Convergence Rates of Two-Time-Scale Gradient Descent-Ascent
Dynamics for Solving Nonconvex Min-Max Problems’. In: Learning for Dynamics
and Control Conference. pp. 192–206.

Bibliography

159

Domingo-Enrich, C., S. Jelassi, A. Mensch, G. Rotskoff, and J. Bruna (2020), ‘A
mean-field analysis of two-player zero-sum games’. Advances in neural information
processing systems 33, 20215–20226.
Domingues, O. D., P. Ménard, E. Kaufmann, and M. Valko (2021), ‘Episodic
reinforcement learning in finite mdps: Minimax lower bounds revisited’. In:
Algorithmic Learning Theory. pp. 578–598.
Du, S. S., G. Gidel, M. I. Jordan, and C. J. Li (2022a), ‘Optimal extragradient-based
bilinearly-coupled saddle-point optimization’. arXiv preprint arXiv:2206.08573.
Du, Y., C. Ma, Y. Liu, R. Lin, H. Dong, J. Wang, and Y. Yang (2022b), ‘Scalable
model-based policy optimization for decentralized networked systems’. In: 2022
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp.
9019–9026.
Elie, R., J. Pérolat, M. Laurière, M. Geist, and O. Pietquin (2019), ‘Approximate
fictitious play for mean field games’. arXiv preprint arXiv:1907.02633.
Elie, R., J. Pérolat, M. Laurière, M. Geist, and O. Pietquin (2020), ‘On the Convergence
of Model Free Learning in Mean Field Games.’. In: AAAI. pp. 7143–7150.
Enrich, C. D., S. Jelassi, C. Domingo-Enrich, D. Scieur, A. Mensch, and J. Bruna
(2019), ‘Extragradient with player sampling for faster Nash equilibrium finding’.
arXiv preprint arXiv:1905.12363.
Erev, I. and A. E. Roth (1998), ‘Predicting how people play games: Reinforcement
learning in experimental games with unique, mixed strategy equilibria’. American
economic review pp. 848–881.
Even-Dar, E., S. M. Kakade, and Y. Mansour (2005), ‘Experts in a Markov decision
process’. In: Advances in neural information processing systems. pp. 401–408.
Even-Dar, E., S. M. Kakade, and Y. Mansour (2009), ‘Online Markov decision
processes’. Mathematics of Operations Research 34(3), 726–736.
Even-Dar, E. and Y. Mansour (2003), ‘Learning rates for Q-learning’. Journal of
machine learning Research 5(Dec), 1–25.
(FAIR)†, M. F. A. R. D. T., A. Bakhtin, N. Brown, E. Dinan, G. Farina, C. Flaherty,
D. Fried, A. Goff, J. Gray, H. Hu, et al. (2022), ‘Human-level play in the game
of Diplomacy by combining language models with strategic reasoning’. Science
378(6624), 1067–1074.
Fan, W., Z. Yu, C. Ma, C. Li, Y. Yang, and X. Zhang (2024), ‘Towards Efficient
Collaboration via Graph Modeling in Reinforcement Learning’. arXiv preprint
arXiv:2410.15841.

160

Bibliography

Farina, G., I. Anagnostides, H. Luo, C.-W. Lee, C. Kroer, and T. Sandholm (2022a),
‘Near-optimal no-regret learning dynamics for general convex games’. Advances in
Neural Information Processing Systems 35, 39076–39089.
Farina, G., A. Celli, N. Gatti, and T. Sandholm (2018), ‘Ex ante coordination and
collusion in zero-sum multi-player extensive-form games’. In: Advances in Neural
Information Processing Systems. pp. 9638–9648.
Farina, G., A. Celli, A. Marchesi, and N. Gatti (2022b), ‘Simple uncoupled no-regret
learning dynamics for extensive-form correlated equilibrium’. Journal of the ACM
69(6), 1–41.
Farina, G. and T. Sandholm (2022), ‘Fast payoff matrix sparsification techniques for
structured extensive-form games’. In: Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 36. pp. 4999–5007.
Fasoulakis, M., E. Markakis, Y. Pantazis, and C. Varsos (2021), ‘Forward Looking Best-Response Multiplicative Weights Update Methods’. arXiv preprint
arXiv:2106.03579.
Fazel, M., R. Ge, S. Kakade, and M. Mesbahi (2018), ‘Global Convergence of Policy
Gradient Methods for the Linear Quadratic Regulator’. In: International Conference
on Machine Learning. pp. 1467–1476.
Feinberg, E. A. (2010), ‘Total expected discounted reward MDPs: existence of optimal
policies’. Wiley Encyclopedia of Operations Research and Management Science.
Fiegel, C., P. Ménard, T. Kozuno, R. Munos, V. Perchet, and M. Valko (2023),
‘Adapting to game trees in zero-sum imperfect information games’. In: International
Conference on Machine Learning. pp. 10093–10135.
Fiez, T., B. Chasnov, and L. Ratliff (2020), ‘Implicit learning dynamics in stackelberg
games: Equilibria characterization, convergence analysis, and empirical study’. In:
International Conference on Machine Learning. pp. 3133–3144.
Fiez, T., B. Chasnov, and L. J. Ratliff (2019), ‘Convergence of learning dynamics in
stackelberg games’. arXiv preprint arXiv:1906.01217.
Fiez, T. and L. Ratliff (2020), ‘Gradient descent-ascent provably converges to
strict local minmax equilibria with a finite timescale separation’. arXiv preprint
arXiv:2009.14820.
Filar, J. and K. Vrieze (2012), Competitive Markov decision processes. Springer
Science & Business Media.
Finn, C., P. Abbeel, and S. Levine (2017), ‘Model-Agnostic Meta-Learning for Fast
Adaptation of Deep Networks’. In: ICML.
Fischer, M. et al. (2017), ‘On the connection between symmetric n-player games and
mean field games’. The Annals of Applied Probability 27(2), 757–810.

Bibliography

161

Flokas, L., E.-V. Vlatakis-Gkaragkounis, and G. Piliouras (2021), ‘Solving min-max
optimization with hidden structure via gradient descent ascent’. arXiv preprint
arXiv:2101.05248.
Foerster, J., R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch
(2018a), ‘Learning with opponent-learning awareness’. In: Proceedings of the
17th International Conference on Autonomous Agents and MultiAgent Systems. pp.
122–130.
Foerster, J., G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson (2017a), ‘Counterfactual Multi-Agent Policy Gradients’. arXiv preprint arXiv:1705.08926.
Foerster, J., N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson
(2017b), ‘Stabilising experience replay for deep multi-agent reinforcement learning’.
In: Proceedings of the 34th International Conference on Machine Learning-Volume
70. pp. 1146–1155.
Foerster, J. N., G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson (2018b),
‘Counterfactual Multi-Agent Policy Gradients’. In: S. A. McIlraith and K. Q.
Weinberger (eds.): Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press.
Franci, B. and S. Grammatico (2021), ‘Training generative adversarial networks via
stochastic Nash games’. IEEE Transactions on Neural Networks and Learning
Systems.
Freund, Y. and R. E. Schapire (1997), ‘A decision-theoretic generalization of on-line
learning and an application to boosting’. Journal of computer and system sciences
55(1), 119–139.
Fu, H., W. Liu, S. Wu, Y. Wang, T. Yang, K. Li, J. Xing, B. Li, B. Ma, Q. Fu, et al.
(2021), ‘Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information
Game’. In: International Conference on Learning Representations.
Fu, Z., Z. Yang, Y. Chen, and Z. Wang (2019), ‘Actor-critic provably finds Nash
equilibria of linear-quadratic mean-field games’. arXiv preprint arXiv:1910.07498.
Fudenberg, D., F. Drew, D. K. Levine, and D. K. Levine (1998), The theory of learning
in games, Vol. 2. MIT press.
Fudenberg, D. and D. M. Kreps (1993), ‘Learning mixed equilibria’. Games and
economic behavior 5(3), 320–367.
Fudenberg, D. and D. Levine (1995), ‘Consistency and cautious fictitious play’.
Journal of Economic Dynamics and Control.
Ganapathi Subramanian, S., M. E. Taylor, M. Crowley, and P. Poupart (2020), ‘Partially
Observable Mean Field Reinforcement Learning’. arXiv e-prints pp. arXiv–2012.

162

Bibliography

Ganzfried, S. and T. Sandholm (2011), ‘Game theory-based opponent modeling in
large imperfect-information games’. In: The 10th International Conference on
Autonomous Agents and Multiagent Systems-Volume 2. pp. 533–540.
Ganzfried, S. and T. Sandholm (2014), ‘Potential-aware imperfect-recall abstraction
with earth mover’s distance in imperfect-information games’. In: Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 28.
Ganzfried, S. and T. Sandholm (2015), ‘Endgame solving in large imperfectinformation games’. In: Workshops at the Twenty-Ninth AAAI Conference on
Artificial Intelligence.
Gao, B. and L. Pavel (2021), ‘Second-Order Mirror Descent: Convergence in Games
Beyond Averaging and Discounting’. arXiv preprint arXiv:2111.09982.
Garcıa, J. and F. Fernández (2015), ‘A comprehensive survey on safe reinforcement
learning’. Journal of Machine Learning Research 16(1), 1437–1480.
Gärtner, J. (1988), ‘On the McKean-Vlasov limit for interacting diffusions’. Mathematische Nachrichten 137(1), 197–248.
Gasser, R. and M. N. Huhns (2014), Distributed Artificial Intelligence: Volume II,
Vol. 2. Morgan Kaufmann.
Ge, Z., Z. Xu, T. Ding, W. Li, and Y. Gao (2024), ‘Efficient subgame refinement for
extensive-form games’. Advances in Neural Information Processing Systems 36.
Gibson, R. G., M. Lanctot, N. Burch, D. Szafron, and M. Bowling (2012), ‘Generalized
Sampling and Variance in Counterfactual Regret Minimization.’. In: AAAI.
Gigerenzer, G. and R. Selten (2002), Bounded rationality: The adaptive toolbox. MIT
press.
Gilpin, A. (2009), Algorithms for abstracting and solving imperfect information games.
Carnegie Mellon University.
Gilpin, A., S. Hoda, J. Pena, and T. Sandholm (2007), ‘Gradient-based algorithms for
finding Nash equilibria in extensive form games’. In: International Workshop on
Web and Internet Economics. pp. 57–69.
Gilpin, A. and T. Sandholm (2006a), ‘Finding equilibria in large sequential games of
imperfect information’. In: Proceedings of the 7th ACM conference on Electronic
commerce. pp. 160–169.
Gilpin, A. and T. Sandholm (2006b), ‘A Texas Hold’em poker player based on
automated abstraction and real-time equilibrium computation’. In: Proceedings
of the fifth international joint conference on Autonomous agents and multiagent
systems. pp. 1453–1454.

Bibliography

163

Gilpin, A. and T. Sandholm (2007a), ‘Better automated abstraction techniques
for imperfect information games, with application to Texas Hold’em poker’. In:
Proceedings of the 6th international joint conference on Autonomous agents and
multiagent systems. pp. 1–8.
Gilpin, A. and T. Sandholm (2007b), ‘Lossless abstraction of imperfect information
games’. Journal of the ACM (JACM) 54(5), 25–es.
Gilpin, A. and T. Sandholm (2008), ‘Expectation-Based Versus Potential-Aware Automated Abstraction in Imperfect Information Games: An Experimental Comparison
Using Poker.’. In: AAAI. pp. 1454–1457.
Golowich, N., S. Pattathil, C. Daskalakis, and A. Ozdaglar (2020), ‘Last iterate is
slower than averaged iterate in smooth convex-concave saddle point problems’. In:
Conference on Learning Theory. pp. 1758–1784.
González-Sánchez, D. and O. Hernández-Lerma (2013), Discrete–time stochastic
control and dynamic potential games: the Euler–Equation approach. Springer
Science & Business Media.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A.
Courville, and Y. Bengio (2014a), ‘Generative adversarial nets’. In: NIPS. pp.
2672–2680.
Goodfellow, I. J., J. Shlens, and C. Szegedy (2014b), ‘Explaining and harnessing
adversarial examples’. arXiv preprint arXiv:1412.6572.
Gorbunov, E., N. Loizou, and G. Gidel (2022), ‘Extragradient method: O (1/K)
last-iterate convergence for monotone variational inequalities and connections with
cocoercivity’. In: International Conference on Artificial Intelligence and Statistics.
pp. 366–402.
Gordon, G. J. (2007), ‘No-regret algorithms for online convex programs’. In: Advances
in Neural Information Processing Systems. pp. 489–496.
Grau-Moya, J., F. Leibfried, and H. Bou-Ammar (2018), ‘Balancing two-player
stochastic games with soft q-learning’. arXiv preprint arXiv:1802.03216.
Greenwald, A., K. Hall, and R. Serrano (2003), ‘Correlated Q-learning’. In: ICML,
Vol. 3. pp. 242–249.
Grnarova, P., Y. Kilcher, K. Y. Levy, A. Lucchi, and T. Hofmann (2021), ‘Generative
minimization networks: Training GANs without competition’. arXiv preprint
arXiv:2103.12685.
Gruslys, A., M. Lanctot, R. Munos, F. Timbers, M. Schmid, J. Perolat, D. Morrill, V.
Zambaldi, J.-B. Lespiau, J. Schultz, et al. (2020), ‘The Advantage Regret-Matching
Actor-Critic’. arXiv preprint arXiv:2008.12234.

164

Bibliography

Gu, H., X. Guo, X. Wei, and R. Xu (2019), ‘Dynamic programming principles for
learning MFCs’. arXiv preprint arXiv:1911.07314.
Gu, H., X. Guo, X. Wei, and R. Xu (2020), ‘Q-Learning for Mean-Field Controls’.
arXiv preprint arXiv:2002.04131.
Guéant, O., J.-M. Lasry, and P.-L. Lions (2011), ‘Mean field games and applications’.
In: Paris-Princeton lectures on mathematical finance 2010. Springer, pp. 205–266.
Guestrin, C., D. Koller, and R. Parr (2002a), ‘Multiagent planning with factored
MDPs’. In: NIPS. pp. 1523–1530.
Guestrin, C., M. Lagoudakis, and R. Parr (2002b), ‘Coordinated reinforcement
learning’. In: ICML, Vol. 2. pp. 227–234.
Guo, X., A. Hu, R. Xu, and J. Zhang (2019), ‘Learning mean-field games’. In:
Advances in Neural Information Processing Systems. pp. 4966–4976.
Guo, X., A. Hu, R. Xu, and J. Zhang (2020), ‘A General Framework for Learning
Mean-Field Games’. arXiv preprint arXiv:2003.06069.
Ha, J. and G. Kim (2022), ‘On Convergence of Lookahead in Smooth Games’. In:
International Conference on Artificial Intelligence and Statistics. pp. 4659–4684.
Haarnoja, T., A. Zhou, P. Abbeel, and S. Levine (2018), ‘Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor’. In:
International Conference on Machine Learning. pp. 1861–1870.
Hadikhanloo, S. and F. J. Silva (2019), ‘Finite mean field games: fictitious play and
convergence to a first order continuous mean field game’. Journal de Mathématiques
Pures et Appliquées 132, 369–397.
Hafner, D., T. Lillicrap, J. Ba, and M. Norouzi (2019a), ‘Dream to Control: Learning
Behaviors by Latent Imagination’. In: International Conference on Learning
Representations.
Hafner, D., T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson (2019b),
‘Learning latent dynamics for planning from pixels’. In: International Conference
on Machine Learning. pp. 2555–2565.
Hannan, J. (1957), ‘Approximation to Bayes risk in repeated play’. Contributions to
the Theory of Games 3, 97–139.
Hansen, N., S. D. Müller, and P. Koumoutsakos (2003), ‘Reducing the time complexity
of the derandomized evolution strategy with covariance matrix adaptation (CMAES)’. Evolutionary computation 11(1), 1–18.
Hansen, T. D., P. B. Miltersen, and U. Zwick (2013), ‘Strategy iteration is strongly
polynomial for 2-player turn-based stochastic games with a constant discount factor’.
Journal of the ACM (JACM) 60(1), 1–16.

Bibliography

165

Harris, K., I. Anagnostides, G. Farina, M. Khodak, Z. S. Wu, and T. Sandholm (2022),
‘Meta-learning in games’. arXiv preprint arXiv:2209.14110.
Hart, S. (2013), Simple adaptive strategies: from regret-matching to uncoupled
dynamics, Vol. 4. World Scientific.
Hart, S. and A. Mas-Colell (2001), ‘A reinforcement procedure leading to correlated
equilibrium’. In: Economics Essays. Springer, pp. 181–200.
Haydari, A. and Y. Yılmaz (2020), ‘Deep reinforcement learning for intelligent
transportation systems: A survey’. IEEE Transactions on Intelligent Transportation
Systems 23(1), 11–32.
He, H., J. Boyd-Graber, K. Kwok, and H. Daumé III (2016), ‘Opponent modeling in
deep reinforcement learning’. In: International conference on machine learning.
pp. 1804–1813.
He, H., S. Zhao, Y. Xi, and J. Ho (2021), ‘AGE: Enhancing the Convergence on GANs
using Alternating extra-gradient with Gradient Extrapolation’. In: NeurIPS 2021
Workshop on Deep Generative Models and Downstream Applications.
He, H., S. Zhao, Y. Xi, J. Ho, and Y. Saad (2022), ‘GDA-AM: ON THE EFFECTIVENESS OF SOLVING MIN-IMAX OPTIMIZATION VIA ANDERSON MIXING’.
In: International Conference on Learning Representations.
Heinrich, J., M. Lanctot, and D. Silver (2015), ‘Fictitious self-play in extensive-form
games’. In: International Conference on Machine Learning. pp. 805–813.
Heinrich, J. and D. Silver (2016), ‘Deep reinforcement learning from self-play in
imperfect-information games’. arXiv preprint arXiv:1603.01121.
Hemmat, R. A., A. Mitra, G. Lajoie, and I. Mitliagkas (2020), ‘Lead: Least-action
dynamics for min-max optimization’. arXiv preprint arXiv:2010.13846.
Hennes, D., D. Morrill, S. Omidshafiei, R. Munos, J. Perolat, M. Lanctot, A. Gruslys,
J.-B. Lespiau, P. Parmas, E. Duenez-Guzman, et al. (2019), ‘Neural Replicator
Dynamics’. arXiv preprint arXiv:1906.00190.
Herings, P. J.-J. and R. Peeters (2010), ‘Homotopy methods to compute equilibria in
game theory’. Economic Theory 42(1), 119–156.
Herings, P. J.-J., R. J. Peeters, et al. (2004), ‘Stationary equilibria in stochastic games:
Structure, selection, and computation’. Journal of Economic Theory 118(1), 32–60.
Hernandez-Leal, P., M. Kaisers, T. Baarslag, and E. M. de Cote (2017), ‘A survey of
learning in multiagent environments: Dealing with non-stationarity’. arXiv preprint
arXiv:1707.09183.
Hernandez-Leal, P., B. Kartal, and M. E. Taylor (2019), ‘A survey and critique of
multiagent deep reinforcement learning’. Autonomous Agents and Multi-Agent
Systems 33(6), 750–797.

166

Bibliography

Heusel, M., H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter (2017), ‘Gans
trained by a two time-scale update rule converge to a local nash equilibrium’. In:
Advances in neural information processing systems. pp. 6626–6637.
Hirsch, M. W. (2012), Differential topology, Vol. 33. Springer Science & Business
Media.
Hofbauer, J. and W. H. Sandholm (2002), ‘On the global convergence of stochastic
fictitious play’. Econometrica 70(6), 2265–2294.
Hsieh, Y.-P., P. Mertikopoulos, and V. Cevher (2020), ‘The limits of min-max
optimization algorithms: convergence to spurious non-critical sets’. arXiv preprint
arXiv:2006.09065.
Hu, J. (1999), Learning in dynamic noncooperative multiagent systems. University of
Michigan.
Hu, J. and M. P. Wellman (2003), ‘Nash Q-learning for general-sum stochastic games’.
Journal of machine learning research 4(Nov), 1039–1069.
Hu, J., M. P. Wellman, et al. (1998), ‘Multiagent reinforcement learning: theoretical
framework and an algorithm.’. In: ICML, Vol. 98. pp. 242–250.
Hu, K., Z. Ren, D. Siska, and L. Szpruch (2019), ‘Mean-field Langevin dynamics and
energy landscape of neural networks’. arXiv preprint arXiv:1905.07769.
Huang, B., J. D. Lee, Z. Wang, and Z. Yang (2021a), ‘Towards general function
approximation in zero-sum markov games’. arXiv preprint arXiv:2107.14702.
Huang, F., X. Wu, and H. Huang (2021b), ‘Efficient mirror descent ascent methods
for nonsmooth minimax problems’. Advances in Neural Information Processing
Systems 34, 10431–10443.
Huang, M., R. P. Malhame, P. E. Caines, et al. (2006), ‘Large population stochastic
dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty
equivalence principle’. Communications in Information & Systems 6(3), 221–252.
Huang, S., J. Lei, Y. Hong, U. V. Shanbhag, and J. Chen (2021c), ‘No-regret distributed
learning in subnetwork zero-sum games’. arXiv preprint arXiv:2108.02144.
Huhns, M. N. (2012), Distributed Artificial Intelligence: Volume I, Vol. 1. Elsevier.
Ibrahim, A., W. Azizian, G. Gidel, and I. Mitliagkas (2020), ‘Linear lower bounds
and conditioning of differentiable games’. In: International conference on machine
learning. pp. 4583–4593.
Iyer, K., R. Johari, and M. Sundararajan (2014), ‘Mean field equilibria of dynamic
auctions with learning’. Management Science 60(12), 2949–2970.

Bibliography

167

Jaderberg, M., W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castaneda,
C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, et al. (2019), ‘Humanlevel performance in 3D multiplayer games with population-based reinforcement
learning’. Science 364(6443), 859–865.
Jan’t Hoen, P., K. Tuyls, L. Panait, S. Luke, and J. A. La Poutre (2005), ‘An overview
of cooperative and competitive multiagent learning’. In: International Workshop
on Learning and Adaption in Multi-Agent Systems. pp. 1–46.
Jia, Z., L. F. Yang, and M. Wang (2019), ‘Feature-Based Q-Learning for Two-Player
Stochastic Games’. arXiv pp. arXiv–1906.
Jin, C., Z. Allen-Zhu, S. Bubeck, and M. I. Jordan (2018a), ‘Is Q-learning provably
efficient?’. Advances in neural information processing systems 31.
Jin, C., T. Jin, H. Luo, S. Sra, and T. Yu (2019a), ‘Learning adversarial MDPs with
bandit feedback and unknown transition’. arXiv preprint arXiv:1912.01192.
Jin, C., Q. Liu, Y. Wang, and T. Yu (2021a), ‘V-Learning–A Simple, Efficient,
Decentralized Algorithm for Multiagent RL’. arXiv preprint arXiv:2110.14555.
Jin, C., Q. Liu, and T. Yu (2021b), ‘The Power of Exploiter: Provable Multi-Agent
RL in Large State Spaces’. arXiv preprint arXiv:2106.03352.
Jin, C., P. Netrapalli, and M. Jordan (2020a), ‘What is local optimality in nonconvexnonconcave minimax optimization?’. In: International conference on machine
learning. pp. 4880–4889.
Jin, C., P. Netrapalli, and M. I. Jordan (2019b), ‘What is Local Optimality in
Nonconvex-Nonconcave Minimax Optimization?’. arXiv pp. arXiv–1902.
Jin, C., Z. Yang, Z. Wang, and M. I. Jordan (2020b), ‘Provably efficient reinforcement
learning with linear function approximation’. In: Conference on Learning Theory.
pp. 2137–2143.
Jin, P., K. Keutzer, and S. Levine (2018b), ‘Regret minimization for partially observable
deep reinforcement learning’. In: International Conference on Machine Learning.
pp. 2342–2351.
Johanson, M., N. Bard, N. Burch, and M. Bowling (2012a), ‘Finding optimal abstract
strategies in extensive-form games’. Proceedings of the AAAI Conference on
Artificial Intelligence 26(1).
Johanson, M., N. Bard, M. Lanctot, R. G. Gibson, and M. Bowling (2012b), ‘Efficient Nash equilibrium approximation through Monte Carlo counterfactual regret
minimization.’. In: AAMAS. pp. 837–846.
Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul (1999), ‘An introduction
to variational methods for graphical models’. Machine learning 37, 183–233.

168

Bibliography

Jovanovic, B. and R. W. Rosenthal (1988), ‘Anonymous sequential games’. Journal of
Mathematical Economics 17(1), 77–87.
Kadanoff, L. P. (2009), ‘More is the same; phase transitions and mean field theories’.
Journal of Statistical Physics 137(5-6), 777.
Kaelbling, L. P., M. L. Littman, and A. W. Moore (1996), ‘Reinforcement learning: A
survey’. Journal of artificial intelligence research 4, 237–285.
Kalogiannis, F., E.-V. Vlatakis-Gkaragkounis, and I. Panageas (2021), ‘Teamwork
makes von Neumann work: Min-Max Optimization in Two-Team Zero-Sum Games’.
arXiv preprint arXiv:2111.04178.
Kaniovski, Y. M. and H. P. Young (1995), ‘Learning dynamics in games with stochastic
perturbations’. Games and economic behavior 11(2), 330–363.
Kearns, M. (2007), ‘Graphical games’. Algorithmic game theory 3, 159–180.
Kearns, M., M. L. Littman, and S. Singh (2013), ‘Graphical models for game theory’.
arXiv preprint arXiv:1301.2281.
Kennedy, J. (2006), ‘Swarm intelligence’. In: Handbook of nature-inspired and
innovative computing. Springer, pp. 187–219.
Keynes, J. M. (1936), The General Theory of Employment, Interest and Money.
Macmillan. 14th edition, 1973.
Kleinberg, R., G. Piliouras, and É. Tardos (2009), ‘Multiplicative updates outperform
generic no-regret learning in congestion games’. In: Proceedings of the forty-first
annual ACM symposium on Theory of computing. pp. 533–542.
Klopf, A. H. (1972), Brain function and adaptive systems: a heterostatic theory, No.
133. Air Force Cambridge Research Laboratories, Air Force Systems Command,
United . . . .
Kober, J., J. A. Bagnell, and J. Peters (2013), ‘Reinforcement learning in robotics: A
survey’. The International Journal of Robotics Research 32(11), 1238–1274.
Kocsis, L. and C. Szepesvári (2006), ‘Bandit based monte-carlo planning’. In:
European conference on machine learning. pp. 282–293.
Kok, J. R. and N. Vlassis (2004), ‘Sparse cooperative Q-learning’. In: Proceedings of
the twenty-first international conference on Machine learning. p. 61.
Koller, D. and N. Megiddo (1992), ‘The complexity of two-person zero-sum games in
extensive form’. Games and economic behavior 4(4), 528–552.
Koller, D. and N. Megiddo (1996), ‘Finding mixed strategies with small supports in
extensive form games’. International Journal of Game Theory 25(1), 73–92.
Konda, V. R. and J. N. Tsitsiklis (2000), ‘Actor-critic algorithms’. In: Advances in
neural information processing systems. pp. 1008–1014.

Bibliography

169

Kong, W. and R. D. Monteiro (2019), ‘An accelerated inexact proximal point method
for solving nonconvex-concave min-max problems’. arXiv pp. arXiv–1905.
Korpelevich, G. M. (1976), ‘The extragradient method for finding saddle points and
other problems’. Matecon 12, 747–756.
Kovarik, V., M. Schmid, N. Burch, M. Bowling, and V. Lisy (2019), ‘Rethinking
formal models of partially observable multiagent decision making’. arXiv preprint
arXiv:1906.11110.
Kozuno, T., P. Ménard, R. Munos, and M. Valko (2021), ‘Learning in two-player
zero-sum partially observable Markov games with perfect recall’. Advances in
Neural Information Processing Systems 34, 11987–11998.
Kreps, D. M. and R. Wilson (1982), ‘Reputation and imperfect information’. Journal
of economic theory 27(2), 253–279.
Kroer, C. and T. Sandholm (2014a), ‘Extensive-form game imperfect-recall abstractions with bounds’. arXiv Prepr. http//arxiv. org/abs/1409.3302.
Kroer, C. and T. Sandholm (2014b), ‘Extensive-Form Game Imperfect-Recall Abstractions With Bounds’. arXiv.
Kroer, C. and T. Sandholm (2018), ‘A Unified Framework for Extensive-Form Game
Abstraction with Bounds’. In: AI3 Workshop at ĲCAI.
Kuhn, H. W. (1950a), ‘Extensive games’. Proceedings of the National Academy of
Sciences of the United States of America 36(10), 570.
Kuhn, H. W. (1950b), ‘A simplified two-person poker’. Contributions to the Theory of
Games 1, 97–103.
Kulesza, A. and B. Taskar (2012), ‘Determinantal point processes for machine
learning’. arXiv preprint arXiv:1207.6083.
Lã, Q. D., Y. H. Chew, and B.-H. Soong (2016), Potential Game Theory. Springer.
Lacker, D. (2015), ‘Mean field games via controlled martingale problems: existence
of Markovian equilibria’. Stochastic Processes and their Applications 125(7),
2856–2894.
Lacker, D. (2017), ‘Limit theory for controlled McKean–Vlasov dynamics’. SIAM
Journal on Control and Optimization 55(3), 1641–1672.
Lacker, D. (2018), ‘On the convergence of closed-loop Nash equilibria to the mean
field game limit’. arXiv preprint arXiv:1808.02745.
Lacker, D. and T. Zariphopoulou (2019), ‘Mean field and n-agent games for optimal
investment under relative performance criteria’. Mathematical Finance 29(4),
1003–1038.

170

Bibliography

Lagoudakis, M. G. and R. Parr (2003), ‘Learning in zero-sum team markov games
using factored value functions’. In: Advances in Neural Information Processing
Systems. pp. 1659–1666.
Lanctot, M., K. Waugh, M. Zinkevich, and M. Bowling (2009), ‘Monte Carlo sampling
for regret minimization in extensive games’. In: Advances in neural information
processing systems. pp. 1078–1086.
Lanctot, M., V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat, D. Silver, and
T. Graepel (2017), ‘A unified game-theoretic approach to multiagent reinforcement
learning’. In: Advances in Neural Information Processing Systems. pp. 4190–4203.
Lange, S., T. Gabel, and M. Riedmiller (2012), ‘Batch reinforcement learning’. In:
Reinforcement learning. Springer, pp. 45–73.
Lasry, J.-M. and P.-L. Lions (2007), ‘Mean field games’. Japanese journal of
mathematics 2(1), 229–260.
Lauer, M. and M. Riedmiller (2000), ‘An algorithm for distributed reinforcement
learning in cooperative multi-agent systems’. In: ICML.
Laurière, M. and O. Pironneau (2014), ‘Dynamic programming for mean-field type
control’. Comptes Rendus Mathematique 352(9), 707–713.
LeCun, Y., Y. Bengio, and G. Hinton (2015), ‘Deep learning’. nature 521(7553),
436–444.
Lee, S. and D. Kim (2021a), ‘Fast extra gradient methods for smooth structured
nonconvex-nonconcave minimax problems’. Advances in Neural Information Processing Systems 34, 22588–22600.
Lee, S. and D. Kim (2021b), ‘Semi-anchored multi-step gradient descent ascent
method for structured nonconvex-nonconcave composite minimax problems’. arXiv
preprint arXiv:2105.15042.
Lehalle, C.-A. and C. Mouzouni (2019), ‘A mean field game of portfolio trading and
its consequences on perceived correlations’. arXiv preprint arXiv:1902.09606.
Lemke, C. E. and J. T. Howson, Jr (1964), ‘Equilibrium points of bimatrix games’.
Journal of the Society for Industrial and Applied Mathematics 12(2), 413–423.
Leslie, D. S., E. Collins, et al. (2003), ‘Convergent multiple-timescales reinforcement
learning algorithms in normal form games’. The Annals of Applied Probability
13(4), 1231–1251.
Leslie, D. S. and E. J. Collins (2005), ‘Individual Q-learning in normal form games’.
SIAM Journal on Control and Optimization 44(2), 495–514.
Leslie, D. S. and E. J. Collins (2006), ‘Generalised weakened fictitious play’. Games
and Economic Behavior 56(2), 285–298.

Bibliography

171

Letcher, A. (2018), ‘Stability and exploitation in differentiable games’. Ph.D. thesis,
Master’s thesis, University of Oxford. Accessed: 2020-06-23.
Levine, S. (2018), ‘Reinforcement learning and control as probabilistic inference:
Tutorial and review’. arXiv preprint arXiv:1805.00909.
Leyton-Brown, K. and M. Tennenholtz (2005), ‘Local-effect games’. In: Dagstuhl
Seminar Proceedings.
Li, C. J., Y. Yu, N. Loizou, G. Gidel, Y. Ma, N. L. Roux, and M. I. Jordan (2021),
‘On the convergence of stochastic extragradient for bilinear games with restarted
iteration averaging’. arXiv preprint arXiv:2107.00464.
Li, H. (2021), ‘On the Complexity of Nonconvex-Strongly-Concave Smooth Minimax
Optimization Using First-Order Methods’. Ph.D. thesis, Massachusetts Institute of
Technology.
Li, L., T. J. Walsh, and M. L. Littman (2006), ‘Towards a unified theory of state
abstraction for MDPs.’. AI&M 1(2), 3.
Li, M., Z. Qin, Y. Jiao, Y. Yang, J. Wang, C. Wang, G. Wu, and J. Ye (2019a), ‘Efficient
ridesharing order dispatching with mean field multi-agent reinforcement learning’.
In: The World Wide Web Conference. pp. 983–994.
Li, S., Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell (2019b), ‘Robust multiagent reinforcement learning via minimax deep deterministic policy gradient’.
In: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. pp.
4213–4220.
Li, Y. (2017), ‘Deep reinforcement learning: An overview’. arXiv preprint
arXiv:1701.07274.
Li, Z. and A. Tewari (2018), ‘Sampled fictitious play is Hannan consistent’. Games
and Economic Behavior 109, 401–412.
Liang, T. and J. Stokes (2019), ‘Interaction matters: A note on non-asymptotic
local convergence of generative adversarial networks’. In: The 22nd International
Conference on Artificial Intelligence and Statistics. pp. 907–915.
Liao, L., L. Shen, J. Duan, M. Kolar, and D. Tao (2021), ‘Local AdaGrad-Type
Algorithm for Stochastic Convex-Concave Minimax Problems’. arXiv preprint
arXiv:2106.10022.
Lillicrap, T. P., J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D.
Wierstra (2015), ‘Continuous control with deep reinforcement learning’. arXiv
preprint arXiv:1509.02971.
Lin, L.-J. (1992), ‘Self-improving reactive agents based on reinforcement learning,
planning and teaching’. Machine learning 8, 293–321.

172

Bibliography

Lin, T., C. Jin, and M. Jordan (2020), ‘On gradient descent ascent for nonconvexconcave minimax problems’. In: International Conference on Machine Learning.
pp. 6083–6093.
Lin, T., C. Jin, and M. I. Jordan (2019), ‘On gradient descent ascent for nonconvexconcave minimax problems’. arXiv preprint arXiv:1906.00331.
Lisy, V., V. Kovarik, M. Lanctot, and B. Bosansky (2013), ‘Convergence of Monte
Carlo tree search in simultaneous move games’. In: Advances in Neural Information
Processing Systems. pp. 2112–2120.
Littlestone, N. and M. K. Warmuth (1994), ‘The weighted majority algorithm’.
Information and computation 108(2), 212–261.
Littman, M. L. (1994), ‘Markov games as a framework for multi-agent reinforcement
learning’. In: Machine learning proceedings 1994. Elsevier, pp. 157–163.
Littman, M. L. (2001a), ‘Friend-or-foe Q-learning in general-sum games’. In: ICML,
Vol. 1. pp. 322–328.
Littman, M. L. (2001b), ‘Value-function reinforcement learning in Markov games’.
Cognitive Systems Research 2(1), 55–66.
Liu, B., Q. Cai, Z. Yang, and Z. Wang (2019), ‘Neural Proximal/Trust Region Policy
Optimization Attains Globally Optimal Policy’. arXiv preprint arXiv:1906.10306.
Liu, M., C. Wu, Q. Liu, Y. Jing, J. Yang, P. Tang, and C. Zhang (2022a), ‘Safe opponentexploitation subgame refinement’. Advances in Neural Information Processing
Systems 35, 27610–27622.
Liu, Q., C. Szepesvári, and C. Jin (2022b), ‘Sample-efficient reinforcement learning
of partially observable markov games’. Advances in Neural Information Processing
Systems 35, 18296–18308.
Liu, Q., T. Yu, Y. Bai, and C. Jin (2021), ‘A sharp analysis of model-based reinforcement
learning with self-play’. In: International Conference on Machine Learning. pp.
7001–7010.
Liu, W., H. Fu, Q. Fu, and Y. Wei (2023), ‘Opponent-limited online search for
imperfect information games’. In: International Conference on Machine Learning.
pp. 21567–21585.
Liu, X.-Y., Z. Xiong, S. Zhong, H. Yang, and A. Walid (2018), ‘Practical deep reinforcement learning approach for stock trading’. arXiv preprint arXiv:1811.07522.
Lockhart, E., M. Lanctot, J. Pérolat, J.-B. Lespiau, D. Morrill, F. Timbers, and K.
Tuyls (2019), ‘Computing approximate equilibria in sequential adversarial games
by exploitability descent’. arXiv preprint arXiv:1903.05614.

Bibliography

173

Loizou, N., H. Berard, G. Gidel, I. Mitliagkas, and S. Lacoste-Julien (2021), ‘Stochastic
gradient descent-ascent and consensus optimization for smooth games: Convergence
analysis under expected co-coercivity’. Advances in Neural Information Processing
Systems 34, 19095–19108.
Loizou, N., H. Berard, A. Jolicoeur-Martineau, P. Vincent, S. Lacoste-Julien, and I.
Mitliagkas (2020), ‘Stochastic hamiltonian gradient methods for smooth games’.
In: International Conference on Machine Learning. pp. 6370–6381.
Lowe, R., Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch (2017), ‘Multiagent actor-critic for mixed cooperative-competitive environments’. In: Advances
in Neural Information Processing Systems. pp. 6379–6390.
Lu, S., I. Tsaknakis, M. Hong, and Y. Chen (2020a), ‘Hybrid block successive
approximation for one-sided non-convex min-max problems: algorithms and
applications’. IEEE Transactions on Signal Processing.
Lu, Y., C. Ma, Y. Lu, J. Lu, and L. Ying (2020b), ‘A mean-field analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from
depth’. arXiv preprint arXiv:2003.05508.
Luo, Y. and Q. Tran-Dinh, ‘Extragradient-Type Methods for Co-Monotone RootFinding Problems’.
Luo, Y., Z. Yang, Z. Wang, and M. Kolar (2019), ‘Natural actor-critic converges globally
for hierarchical linear quadratic regulator’. arXiv preprint arXiv:1912.06875.
Ma, C., A. Li, Y. Du, H. Dong, and Y. Yang (2024), ‘Efficient and scalable reinforcement learning for large-scale network control’. Nature Machine Intelligence pp.
1–15.
Ma, C., Z. Yang, M. Gao, H. Ci, J. Gao, X. Pan, and Y. Yang (2023), ‘Red teaming
game: A game-theoretic framework for red teaming language models’. arXiv
preprint arXiv:2310.00322.
Macua, S. V., J. Zazo, and S. Zazo (2018), ‘Learning Parametric Closed-Loop
Policies for Markov Potential Games’. In: International Conference on Learning
Representations.
Mahadevan, S. (1996), ‘Average reward reinforcement learning: Foundations, algorithms, and empirical results’. Machine learning 22(1-3), 159–195.
Mannor, S. and N. Shimkin (2003), ‘The empirical Bayes envelope and regret minimization in competitive Markov decision processes’. Mathematics of Operations
Research 28(2), 327–345.
Mao, W. and T. Başar (2022), ‘Provably efficient reinforcement learning in decentralized general-sum markov games’. Dynamic Games and Applications pp.
1–22.

174

Bibliography

Martin, C. and T. Sandholm (2024), ‘Simultaneous incremental support adjustment
and metagame solving: An equilibrium-finding framework for continuous-action
games’. arXiv preprint arXiv:2406.08683.
Martinet, B. (1970), ‘Regularisation, d’inéquations variationelles par approximations
succesives’. Revue Francaise d’informatique et de Recherche operationelle.
Maskin, E. and J. Tirole (2001), ‘Markov perfect equilibrium: I. Observable actions’.
Journal of Economic Theory 100(2), 191–219.
Matignon, L., G. J. Laurent, and N. Le Fort-Piat (2012), ‘Independent reinforcement
learners in cooperative Markov games: a survey regarding coordination problems’.
The Knowledge Engineering Review 27(1), 1–31.
Matousek, J. and B. Gärtner (2007), Understanding and using linear programming.
Springer Science & Business Media.
Maynard Smith, J. (1972), ‘On evolution’.
Mazumdar, E., L. J. Ratliff, and S. Sastry (2018), ‘On the convergence of gradient-based
learning in continuous games’. arXiv preprint arXiv:1804.05464.
Mazumdar, E., L. J. Ratliff, S. Sastry, and M. I. Jordan (2019a), ‘Policy Gradient in
Linear Quadratic Dynamic Games Has No Convergence Guarantees’.
Mazumdar, E. V., M. I. Jordan, and S. S. Sastry (2019b), ‘On finding local nash
equilibria (and only local nash equilibria) in zero-sum games’. arXiv preprint
arXiv:1901.00838.
McAleer, S., G. Farina, M. Lanctot, and T. Sandholm (2022a), ‘ESCHER: Eschewing
Importance Sampling in Games by Computing a History Value Function to Estimate
Regret’. arXiv preprint arXiv:2206.04122.
McAleer, S., G. Farina, G. Zhou, M. Wang, Y. Yang, and T. Sandholm (2023), ‘TeamPSRO for learning approximate TMECor in large team games via cooperative
reinforcement learning’. Advances in Neural Information Processing Systems 36,
45402–45418.
McAleer, S., J. Lanier, P. Baldi, and R. Fox (2021), ‘XDO: A double oracle algorithm
for extensive-form games’. Advances in Neural Information Processing Systems
(NeurIPS).
McAleer, S., J. Lanier, R. Fox, and P. Baldi (2020), ‘Pipeline PSRO: A Scalable
Approach for Finding Approximate Nash Equilibria in Large Games’. arXiv preprint
arXiv:2006.08555.
McAleer, S., K. Wang, J. B. Lanier, M. Lanctot, P. Baldi, T. Sandholm, and R. Fox
(2022b), ‘Anytime PSRO for Two-Player Zero-Sum Games’.

Bibliography

175

McKean, H. P. (1967), ‘Propagation of chaos for a class of non-linear parabolic equations’. Stochastic Differential Equations (Lecture Series in Differential Equations,
Session 7, Catholic Univ., 1967) pp. 41–57.
McMahan, H. B., G. J. Gordon, and A. Blum (2003), ‘Planning in the presence of cost
functions controlled by an adversary’. In: Proceedings of the 20th International
Conference on Machine Learning (ICML-03). pp. 536–543.
Meng, L., Z. Ge, W. Li, B. An, and Y. Gao (2023), ‘Efficient last-iterate convergence
algorithms in solving games’. arXiv preprint arXiv:2308.11256.
Mertikopoulos, P., B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G.
Piliouras (2018), ‘Optimistic mirror descent in saddle-point problems: Going the
extra (gradient) mile’. arXiv preprint arXiv:1807.02629.
Mertikopoulos, P. and Z. Zhou (2019), ‘Learning in games with continuous action sets
and unknown payoff functions’. Mathematical Programming 173(1-2), 465–507.
Mescheder, L., A. Geiger, and S. Nowozin (2018), ‘Which training methods for GANs
do actually converge?’. arXiv preprint arXiv:1801.04406.
Mescheder, L., S. Nowozin, and A. Geiger (2017), ‘The numerics of gans’. In:
Advances in Neural Information Processing Systems. pp. 1825–1835.
Mguni, D. (2020), ‘Stochastic Potential Games’. arXiv preprint arXiv:2005.13527.
Michael,
D. (2020),
‘Algorithmic Game Theory Lecture Notes’.
http://www.cs.jhu.edu/ mdinitz/classes/AGT/Spring2020/Lectures/lecture6.pdf.
Milec, D., O. Kubíček, and V. Lisỳ (2021), ‘Continual Depth-limited Responses for Computing Counter-strategies in Sequential Games’. arXiv preprint
arXiv:2112.12594.
Minsky, M. (1961), ‘Steps toward artificial intelligence’. Proceedings of the IRE 49(1),
8–30.
Minsky, M. L. (1954), Theory of neural-analog reinforcement systems and its
application to the brain model problem. Princeton University.
Mladenovic, A., I. Sakos, G. Gidel, and G. Piliouras (2021), ‘Generalized Natural
Gradient Flows in Hidden Convex-Concave Games and GANs’. In: International
Conference on Learning Representations.
Mnih, V., K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A.
Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. (2015), ‘Human-level
control through deep reinforcement learning’. Nature 518(7540), 529.
Mokhtari, A., A. Ozdaglar, and S. Pattathil (2020a), ‘A unified analysis of extragradient and optimistic gradient methods for saddle point problems: Proximal point
approach’. In: International Conference on Artificial Intelligence and Statistics. pp.
1497–1507.

176

Bibliography

Mokhtari, A., A. E. Ozdaglar, and S. Pattathil (2020b), ‘Convergence Rate of O(1/k)
for Optimistic Gradient and Extragradient Methods in Smooth Convex-Concave
Saddle Point Problems’. SIAM Journal on Optimization 30(4), 3230–3251.
Monderer, D. and L. S. Shapley (1996), ‘Potential games’. Games and economic
behavior 14(1), 124–143.
Moravcik, M., M. Schmid, N. Burch, V. Lisy, D. Morrill, N. Bard, T. Davis, K.
Waugh, M. Johanson, and M. Bowling (2017), ‘DeepStack: Expert-level artificial
intelligence in heads-up no-limit poker’. Science 356(6337), 508–513.
Moravcik, M., M. Schmid, K. Ha, M. Hladik, and S. Gaukrodger (2016), ‘Refining
subgames in large imperfect information games’. In: Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 30.
Morimoto, J. and K. Doya (2005), ‘Robust reinforcement learning’. Neural computation
17(2), 335–359.
Motte, M. and H. Pham (2019), ‘Mean-field Markov decision processes with common
noise and open-loop controls’. arXiv preprint arXiv:1912.07883.
Müller, J. P. and K. Fischer (2014), ‘Application impact of multi-agent systems and
technologies: A survey’. In: Agent-oriented software engineering. pp. 27–53.
Munos, R. and C. Szepesvári (2008), ‘Finite-time bounds for fitted value iteration’.
Journal of Machine Learning Research 9(May), 815–857.
Nagarajan, V. and J. Z. Kolter (2017), ‘Gradient descent GAN optimization is locally
stable’. In: Advances in neural information processing systems. pp. 5585–5595.
Nash, J. (1951), ‘Non-cooperative games’. Annals of mathematics pp. 286–295.
Nayyar, A., A. Mahajan, and D. Teneketzis (2013), ‘Decentralized stochastic control
with partial history sharing: A common information approach’. IEEE Transactions
on Automatic Control 58(7), 1644–1658.
Nedic, A., A. Olshevsky, and W. Shi (2017), ‘Achieving geometric convergence for
distributed optimization over time-varying graphs’. SIAM Journal on Optimization
27(4), 2597–2633.
Nedic, A. and A. Ozdaglar (2009), ‘Distributed subgradient methods for multi-agent
optimization’. IEEE Transactions on Automatic Control 54(1), 48–61.
Nemirovski, A. (2004), ‘Prox-method with rate of convergence O (1/t) for variational
inequalities with Lipschitz continuous monotone operators and smooth convexconcave saddle point problems’. SIAM Journal on Optimization 15(1), 229–251.
Nemirovsky, A. S. and D. B. Yudin (1983), ‘Problem complexity and method efficiency
in optimization.’.

Bibliography

177

Neu, G., A. Antos, A. György, and C. Szepesvári (2010), ‘Online Markov decision
processes under bandit feedback’. In: Advances in Neural Information Processing
Systems. pp. 1804–1812.
Neu, G., A. Gyorgy, C. Szepesvari, and A. Antos (2014), ‘Online Markov Decision
Processes Under Bandit Feedback’. IEEE Transactions on Automatic Control 3(59),
676–691.
Neu, G., A. Jonsson, and V. Gómez (2017), ‘A unified view of entropy-regularized
markov decision processes’. NIPS.
Neumann, J. v. (1928), ‘Zur theorie der gesellschaftsspiele’. Mathematische annalen
100(1), 295–320.
Nguyen, T. T., N. D. Nguyen, and S. Nahavandi (2020), ‘Deep reinforcement learning
for multiagent systems: A review of challenges, solutions, and applications’. IEEE
transactions on cybernetics.
Nouiehed, M., M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn (2019a), ‘Solving
a class of non-convex min-max games using iterative first order methods’. In:
Advances in Neural Information Processing Systems. pp. 14934–14942.
Nouiehed, M., M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn (2019b), ‘Solving
a Class of Non-Convex Min-Max Games Using Iterative First Order Methods’.
In: H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.): Advances in Neural Information Processing Systems, Vol. 32.
Curran Associates, Inc.
Nowé, A., P. Vrancx, and Y.-M. De Hauwere (2012), ‘Game theory and multi-agent
reinforcement learning’. In: Reinforcement Learning. Springer, pp. 441–470.
Nutz, M., J. San Martin, X. Tan, et al. (2020), ‘Convergence to the mean field game
limit: a case study’. The Annals of Applied Probability 30(1), 259–286.
Oliehoek, F. A., C. Amato, et al. (2016), A concise introduction to decentralized
POMDPs, Vol. 1. Springer.
Omidshafiei, S., D. Hennes, D. Morrill, R. Munos, J. Perolat, M. Lanctot, A. Gruslys,
J.-B. Lespiau, and K. Tuyls (2019), ‘Neural Replicator Dynamics’. arXiv preprint
arXiv:1906.00190.
Omidshafiei, S., K. Tuyls, W. M. Czarnecki, F. C. Santos, M. Rowland, J. Connor, D.
Hennes, P. Muller, J. Perolat, B. De Vylder, et al. (2020), ‘Navigating the Landscape
of Games’. arXiv preprint arXiv:2005.01642.
OroojlooyJadid, A. and D. Hajinezhad (2019), ‘A review of cooperative multi-agent
deep reinforcement learning’. arXiv preprint arXiv:1908.03963.
Ortner, R., P. Gajane, and P. Auer (2020), ‘Variational regret bounds for reinforcement
learning’. In: Uncertainty in Artificial Intelligence. pp. 81–90.

178

Bibliography

Osborne, M. J. and A. Rubinstein (1994), A course in game theory. MIT press.
Pachocki, J., G. Brockman, J. Raiman, S. Zhang, H. Pondé, J. Tang, F. Wolski, C.
Dennison, R. Jozefowicz, P. Debiak, et al. (2018), ‘OpenAI Five, 2018’. URL
https://blog. openai. com/openai-five.
Panait, L. and S. Luke (2005), ‘Cooperative multi-agent learning: The state of the art’.
AAMAS 11(3), 387–434.
Papadimitriou, C. H. and J. N. Tsitsiklis (1987), ‘The complexity of Markov decision
processes’. Mathematics of operations research 12(3), 441–450.
Papoudakis, G., F. Christianos, L. Schäfer, and S. V. Albrecht (2020), ‘Comparative
evaluation of multi-agent deep reinforcement learning algorithms’. arXiv preprint
arXiv:2006.07869.
Perkins, S., P. Mertikopoulos, and D. S. Leslie (2015), ‘Mixed-strategy learning with
continuous action sets’. IEEE Transactions on Automatic Control 62(1), 379–384.
Perolat, J., B. De Vylder, D. Hennes, E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T.
Connor, N. Burch, T. Anthony, et al. (2022), ‘Mastering the game of Stratego with
model-free multiagent reinforcement learning’. Science 378(6623), 990–996.
Perolat, J., R. Munos, J.-B. Lespiau, S. Omidshafiei, M. Rowland, P. Ortega, N.
Burch, T. Anthony, D. Balduzzi, B. De Vylder, et al. (2021), ‘From Poincaré
recurrence to convergence in imperfect information games: Finding equilibrium via
regularization’. In: International Conference on Machine Learning. pp. 8525–8535.
Perolat, J., B. Piot, and O. Pietquin (2018), ‘Actor-critic fictitious play in simultaneous
move multistage games’. In: International Conference on Artificial Intelligence and
Statistics. pp. 919–928.
Peters, J. and S. Schaal (2008), ‘Natural actor-critic’. Neurocomputing 71(7-9),
1180–1190.
Pham, H. and X. Wei (2016), ‘Discrete time McKean–Vlasov control problem: a
dynamic programming approach’. Applied Mathematics & Optimization 74(3),
487–506.
Pham, H. and X. Wei (2017), ‘Dynamic programming for optimal control of stochastic
McKean–Vlasov dynamics’. SIAM Journal on Control and Optimization 55(2),
1069–1101.
Pham, H. and X. Wei (2018), ‘Bellman equation and viscosity solutions for meanfield stochastic control problem’. ESAIM: Control, Optimisation and Calculus of
Variations 24(1), 437–461.
Pinto, L., J. Davidson, R. Sukthankar, and A. Gupta (2017), ‘Robust Adversarial
Reinforcement Learning’. arXiv preprint arXiv:1703.02702.

Bibliography

179

Powers, R. and Y. Shoham (2005a), ‘Learning against opponents with bounded
memory.’. In: ĲCAI, Vol. 5. pp. 817–822.
Powers, R. and Y. Shoham (2005b), ‘New criteria and a new algorithm for learning in
multi-agent systems’. In: Advances in neural information processing systems. pp.
1089–1096.
Prasad, H., P. LA, and S. Bhatnagar (2015), ‘Two-timescale algorithms for learning
Nash equilibria in general-sum stochastic games’. In: Proceedings of the 2015
International Conference on Autonomous Agents and Multiagent Systems. pp.
1371–1379.
Qi, H., F. Li, S. Tan, and X. Zhang (2021), ‘Training Generative Adversarial Networks
with Adaptive Composite Gradient’. arXiv preprint arXiv:2111.05508.
Qu, G., Y. Lin, A. Wierman, and N. Li (2020), ‘Scalable multi-agent reinforcement learning for networked systems with average reward’. Advances in Neural
Information Processing Systems 33, 2074–2086.
Qu, G., A. Wierman, and N. Li (2022), ‘Scalable reinforcement learning for multiagent
networked systems’. Operations Research 70(6), 3601–3628.
Rafique, H., M. Liu, Q. Lin, and T. Yang (2018), ‘Non-Convex Min-Max Optimization:
Provable Algorithms and Applications in Machine Learning’. arXiv pp. arXiv–1810.
Rakhlin, S. and K. Sridharan (2013), ‘Optimization, learning, and games with
predictable sequences’. Advances in Neural Information Processing Systems 26.
Rashid, T., M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson
(2018), ‘QMIX: Monotonic value function factorisation for deep multi-agent
reinforcement learning’. arXiv preprint arXiv:1803.11485.
Ratliff, L. J., S. A. Burden, and S. S. Sastry (2013), ‘Characterization and computation
of local Nash equilibria in continuous games’. In: 2013 51st Annual Allerton
Conference on Communication, Control, and Computing (Allerton). pp. 917–924.
Ratliff, L. J., S. A. Burden, and S. S. Sastry (2014), ‘Genericity and structural
stability of non-degenerate differential Nash equilibria’. In: 2014 American Control
Conference. pp. 3990–3995.
Rendle, S. (2010), ‘Factorization machines’. In: 2010 IEEE International Conference
on Data Mining. pp. 995–1000.
Riedmiller, M. (2005), ‘Neural fitted Q iteration–first experiences with a data efficient
neural reinforcement learning method’. In: ECML. pp. 317–328.
Rockafellar, R. T. (1976), ‘Augmented Lagrangians and applications of the proximal
point algorithm in convex programming’. Mathematics of operations research 1(2),
97–116.

180

Bibliography

Roĳers, D. M., P. Vamplew, S. Whiteson, and R. Dazeley (2013), ‘A survey of multiobjective sequential decision-making’. Journal of Artificial Intelligence Research
48, 67–113.
Rosenberg, A. and Y. Mansour (2019), ‘Online Convex Optimization in Adversarial
Markov Decision Processes’. In: International Conference on Machine Learning.
pp. 5478–5486.
Rothfuss, J., D. Lee, I. Clavera, T. Asfour, and P. Abbeel (2018), ‘ProMP: Proximal
Meta-Policy Search’. In: International Conference on Learning Representations.
Russell, S. J. (2010), Artificial intelligence a modern approach. Pearson Education,
Inc.
Saldi, N., T. Basar, and M. Raginsky (2018), ‘Markov–Nash Equilibria in Mean-Field
Games with Discounted Cost’. SIAM Journal on Control and Optimization 56(6),
4256–4287.
Saldi, N., T. Başar, and M. Raginsky (2019), ‘Approximate Nash equilibria in partially
observed stochastic games with mean-field interactions’. Mathematics of Operations
Research 44(3), 1006–1033.
Sandholm, T. (2007), ‘Perspectives on Multiagent Learning’. Artificial Intelligence
171, 382–391.
Sandholm, T. (2010), ‘The state of solving large incomplete-information games, and
application to poker’. Ai Magazine 31(4), 13–32.
Sandholm, T. (2015), ‘Abstraction for Solving Large Incomplete-Information Games’.
In: AAAI Conference on Artificial Intelligence (AAAI). Senior Member Track.
Sandholm, T. and R. Crites (1996), ‘Multiagent Reinforcement Learning in the Iterated
Prisoner’s Dilemma’. Biosystems 37, 147–166. Special issue on the Prisoner’s
Dilemma. Early version in ĲCAI-95 Workshop on Adaptation and Learning in
Multiagent Systems.
Sandholm, T. and S. Singh (2012), ‘Lossy stochastic game abstraction with bounds’. In:
Proceedings of the 13th ACM Conference on Electronic Commerce. pp. 880–897.
Schadd, F., S. Bakkes, and P. Spronck (2007), ‘Opponent Modeling in Real-Time
Strategy Games.’. In: GAMEON. pp. 61–70.
Schaefer, F. and A. Anandkumar (2019), ‘Competitive Gradient Descent’. In: H.
Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(eds.): Advances in Neural Information Processing Systems, Vol. 32. Curran
Associates, Inc.
Schaeffer, J., R. Lake, P. Lu, and M. Bryant (1996), ‘Chinook the world man-machine
checkers champion’. Ai Magazine 17(1), 21–21.

Bibliography

181

Schaeffer, M. S. N. S. J., N. Shafiei, et al. (2009), ‘Comparing UCT versus CFR in
simultaneous games’.
Schäfer, F., A. Anandkumar, and H. Owhadi (2020), ‘Competitive mirror descent’.
arXiv preprint arXiv:2006.10179.
Schmid, M., N. Burch, M. Lanctot, M. Moravcik, R. Kadlec, and M. Bowling
(2019), ‘Variance reduction in monte carlo counterfactual regret minimization
(VR-MCCFR) for extensive form games using baselines’. In: Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 33. pp. 2157–2164.
Schoemaker, P. J. (2013), Experiments on decisions under risk: The expected utility
hypothesis. Springer Science & Business Media.
Schrittwieser, J., I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A.
Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. (2020), ‘Mastering atari, go,
chess and shogi by planning with a learned model’. Nature 588(7839), 604–609.
Schulman, J., S. Levine, P. Abbeel, M. Jordan, and P. Moritz (2015), ‘Trust region
policy optimization’. In: International conference on machine learning. pp. 1889–
1897.
Schulman, J., F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017), ‘Proximal
policy optimization algorithms’. arXiv preprint arXiv:1707.06347.
Selten, R. (1965), ‘Spieltheoretische behandlung eines oligopolmodells mit nachfrageträgheit: Teil i: Bestimmung des dynamischen preisgleichgewichts’. Zeitschrift
für die gesamte Staatswissenschaft/Journal of Institutional and Theoretical Economics (H. 2), 301–324.
Shakshuki, E. M. and M. Reid (2015), ‘Multi-agent system applications in healthcare:
current technology and future roadmap.’. In: ANT/SEIT. pp. 252–261.
Shalev-Shwartz, S. et al. (2011), ‘Online learning and online convex optimization’.
Foundations and trends in Machine Learning 4(2), 107–194.
Shalev-Shwartz, S., S. Shammah, and A. Shashua (2016), ‘Safe, multi-agent, reinforcement learning for autonomous driving’. arXiv preprint arXiv:1610.03295.
Shapley, L. S. (1953), ‘Stochastic games’. Proceedings of the national academy of
sciences 39(10), 1095–1100.
Shapley, L. S. (1974), ‘A note on the Lemke-Howson algorithm’. In: Pivoting and
Extension. Springer, pp. 175–189.
Sharma, P., R. Panda, G. Joshi, and P. Varshney (2022), ‘Federated minimax optimization: Improved convergence analyses and algorithms’. In: International Conference
on Machine Learning. pp. 19683–19730.

182

Bibliography

Shi, W., S. Song, and C. Wu (2019), ‘Soft policy gradient method for maximum
entropy deep reinforcement learning’. In: Proceedings of the 28th International
Joint Conference on Artificial Intelligence. pp. 3425–3431.
Shoham, Y. and K. Leyton-Brown (2008), Multiagent systems: Algorithmic, gametheoretic, and logical foundations. Cambridge University Press.
Shoham, Y., R. Powers, and T. Grenager (2007), ‘If multi-agent learning is the answer,
what is the question?’. Artificial intelligence 171(7), 365–377.
Shub, M. (2013), Global stability of dynamical systems. Springer Science & Business
Media.
Sidford, A., M. Wang, X. Wu, L. Yang, and Y. Ye (2018), ‘Near-optimal time and
sample complexities for solving Markov decision processes with a generative
model’. In: Advances in Neural Information Processing Systems. pp. 5186–5196.
Sidford, A., M. Wang, L. Yang, and Y. Ye (2020), ‘Solving discounted stochastic
two-player games with near-optimal time and sample complexity’. In: International
Conference on Artificial Intelligence and Statistics. pp. 2992–3002.
Silver, D., A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. (2016),
‘Mastering the game of Go with deep neural networks and tree search’. nature
529(7587), 484.
Silver, D., T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot,
L. Sifre, D. Kumaran, T. Graepel, et al. (2017a), ‘Mastering chess and shogi
by self-play with a general reinforcement learning algorithm’. arXiv preprint
arXiv:1712.01815.
Silver, D., T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot,
L. Sifre, D. Kumaran, T. Graepel, et al. (2018), ‘A general reinforcement learning
algorithm that masters chess, shogi, and Go through self-play’. Science 362(6419),
1140–1144.
Silver, D., G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller (2014),
‘Deterministic policy gradient algorithms’. In: ICML. pp. 387–395.
Silver, D., J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T.
Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. v. d.
Driessche, T. Graepel, and D. Hassabis (2017b), ‘Mastering the game of Go without
human knowledge’. Nature 550(7676), 354–359.
Simon, H. A. (1972), ‘Theories of bounded rationality’. Decision and organization
1(1), 161–176.
Singh, S. P., M. J. Kearns, and Y. Mansour (2000), ‘Nash Convergence of Gradient
Dynamics in General-Sum Games.’. In: UAI. pp. 541–548.

Bibliography

183

Sirignano, J. and K. Spiliopoulos (2020), ‘Mean field analysis of neural networks: A
law of large numbers’. SIAM Journal on Applied Mathematics 80(2), 725–752.
Smith, J. M. and G. R. Price (1973), ‘The logic of animal conflict’. Nature 246(5427),
15–18.
Sokota, S., R. D’Orazio, J. Z. Kolter, N. Loizou, M. Lanctot, I. Mitliagkas, N.
Brown, and C. Kroer (2022), ‘A Unified Approach to Reinforcement Learning,
Quantal Response Equilibria, and Two-Player Zero-Sum Games’. arXiv preprint
arXiv:2206.05825.
Son, K., D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi (2019), ‘Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning’.
In: International Conference on Machine Learning. pp. 5887–5896.
Song, M., A. Montanari, and P. Nguyen (2018), ‘A mean field view of the landscape
of two-layers neural networks’. Proceedings of the National Academy of Sciences
115, E7665–E7671.
Song, Z., S. Mei, and Y. Bai (2021), ‘When Can We Learn General-Sum Markov
Games with a Large Number of Players Sample-Efficiently?’. arXiv preprint
arXiv:2110.04184.
Song, Z., S. Mei, and Y. Bai (2022), ‘Sample-efficient learning of correlated equilibria
in extensive-form games’. Advances in Neural Information Processing Systems 35,
4099–4110.
Srebro, N., K. Sridharan, and A. Tewari (2011), ‘On the universality of online mirror
descent’. In: Advances in neural information processing systems. pp. 2645–2653.
Srinivasan, S., M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos, and M.
Bowling (2018), ‘Actor-critic policy optimization in partially observable multiagent
environments’. In: Advances in neural information processing systems. pp. 3422–
3435.
Steinberger, E., A. Lerer, and N. Brown (2020), ‘DREAM: Deep Regret minimization with Advantage baselines and Model-free learning’. arXiv preprint
arXiv:2006.10410.
Stone, P. (2007), ‘Multiagent learning is not the answer. It is the question’. Artificial
Intelligence 171(7), 402–405.
Stone, P. and M. Veloso (2000), ‘Multiagent systems: A survey from a machine
learning perspective’. Autonomous Robots 8(3), 345–383.
Subramanian, J. and A. Mahajan (2019), ‘Reinforcement learning in stationary meanfield games’. In: Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems. pp. 251–259.

184

Bibliography

Subramanian, S. G., P. Poupart, M. E. Taylor, and N. Hegde (2020), ‘Multi Type Mean
Field Reinforcement Learning’. arXiv preprint arXiv:2002.02513.
Sunehag, P., G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M.
Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al. (2018), ‘Value-Decomposition
Networks For Cooperative Multi-Agent Learning Based On Team Reward’. In:
Proceedings of the 17th International Conference on Autonomous Agents and
MultiAgent Systems. pp. 2085–2087.
Šustr, M., M. Schmid, M. Moravčík, N. Burch, M. Lanctot, and M. Bowling (2020),
‘Sound search in imperfect information games’. arXiv preprint arXiv:2006.08740.
Suttle, W., Z. Yang, K. Zhang, Z. Wang, T. Basar, and J. Liu (2019), ‘A Multi-Agent
Off-Policy Actor-Critic Algorithm for Distributed Reinforcement Learning’. arXiv
preprint arXiv:1903.06372.
Sutton, R. S. (1988), ‘Learning to predict by the methods of temporal differences’.
Machine learning 3(1), 9–44.
Sutton, R. S. and A. G. Barto (1998), Reinforcement learning: An introduction, Vol. 1.
MIT press Cambridge.
Sutton, R. S., D. A. McAllester, S. P. Singh, and Y. Mansour (2000), ‘Policy gradient
methods for reinforcement learning with function approximation’. In: Advances in
neural information processing systems. pp. 1057–1063.
Swenson, B. and H. V. Poor (2019), ‘Smooth Fictitious Play in N× 2 Potential Games’.
In: 2019 53rd Asilomar Conference on Signals, Systems, and Computers. pp.
1739–1743.
Syed, U., M. Bowling, and R. E. Schapire (2008), ‘Apprenticeship learning using
linear programming’. In: Proceedings of the 25th international conference on
Machine learning. pp. 1032–1039.
Szepesvári, C. (2010), ‘Algorithms for reinforcement learning’. Synthesis lectures on
artificial intelligence and machine learning 4(1), 1–103.
Szepesvári, C. and M. L. Littman (1999), ‘A unified analysis of value-function-based
reinforcement-learning algorithms’. Neural computation 11(8), 2017–2060.
Szer, D., F. Charpillet, and S. Zilberstein (2005), ‘MAA*: A Heuristic Search
Algorithm for Solving Decentralized POMDPs’.
Sznitman, A.-S. (1991), ‘Topics in propagation of chaos’. In: Ecole d’été de probabilités de Saint-Flour XIX—1989. Springer, pp. 165–251.
Tammelin, O., N. Burch, M. Johanson, and M. Bowling (2015), ‘Solving heads-up limit
Texas Hold’em’. In: Twenty-Fourth International Joint Conference on Artificial
Intelligence.

Bibliography

185

Tan, M. (1993), ‘Multi-agent reinforcement learning: Independent vs. cooperative
agents’. In: Proceedings of the tenth international conference on machine learning.
pp. 330–337.
Tang, X., S. M. McAleer, Y. Yang, et al. (2023), ‘Regret-minimizing double oracle
for extensive-form games’. In: International Conference on Machine Learning. pp.
33599–33615.
Tang, X., C. Wang, C. Ma, I. Bogunovic, S. McAleer, and Y. Yang (2024), ‘SampleEfficient Regret-Minimizing Double Oracle in Extensive-Form Games’. arXiv
preprint arXiv:2411.00954.
Taylor, M. E. and P. Stone (2009), ‘Transfer learning for reinforcement learning
domains: A survey.’. Journal of Machine Learning Research 10(7).
Tennenholtz, M. (2004), ‘Program equilibrium’. Games and Economic Behavior
49(2), 363–373.
Terry, J. K., B. Black, M. Jayakumar, A. Hari, L. Santos, C. Dieffendahl, N. L.
Williams, Y. Lokesh, R. Sullivan, C. Horsch, and P. Ravi (2020), ‘PettingZoo: Gym
for Multi-Agent Reinforcement Learning’. arXiv preprint arXiv:2009.14471.
Tesauro, G. (1995), ‘Temporal difference learning and TD-Gammon’. Communications
of the ACM 38(3), 58–68.
Tessler, C., Y. Efroni, and S. Mannor (2019), ‘Action robust reinforcement learning
and applications in continuous control’. In: International Conference on Machine
Learning. pp. 6215–6224.
Thekumparampil, K. K., P. Jain, P. Netrapalli, and S. Oh (2019), ‘Efficient algorithms
for smooth minimax optimization’. In: Advances in Neural Information Processing
Systems. pp. 12680–12691.
Thorndike, E. L. (1898), ‘Animal intelligence: an experimental study of the associative
processes in animals.’. The Psychological Review: Monograph Supplements 2(4), i.
Tian, Z., Y. Wen, Z. Gong, F. Punakkath, S. Zou, and J. Wang (2019), ‘A regularized opponent model with maximum entropy objective’. arXiv preprint arXiv:1905.08087.
Toussaint, M., L. Charlin, and P. Poupart (2008), ‘Hierarchical POMDP Controller
Optimization by Likelihood Maximization.’. In: UAI, Vol. 24. pp. 562–570.
Tsaknakis, H. and P. G. Spirakis (2007), ‘An optimization approach for approximate
Nash equilibria’. In: International Workshop on Web and Internet Economics. pp.
42–56.
Tuyls, K. and A. Nowé (2005), ‘Evolutionary game theory and multi-agent reinforcement learning’.
Tuyls, K. and S. Parsons (2007), ‘What evolutionary game theory tells us about
multiagent learning’. Artificial Intelligence 171(7), 406–416.

186

Bibliography

Tuyls, K., K. Verbeeck, and T. Lenaerts (2003), ‘A selection-mutation model for
q-learning in multi-agent systems’. In: Proceedings of the second international
joint conference on Autonomous agents and multiagent systems. pp. 693–700.
Tuyls, K. and G. Weiss (2012), ‘Multiagent learning: Basics, challenges, and prospects’.
Ai Magazine 33(3), 41–41.
uz Zaman, M. A., K. Zhang, E. Miehling, and T. Başar (2020), ‘Approximate
equilibrium computation for discrete-time linear-quadratic mean-field games’. In:
2020 American Control Conference (ACC). pp. 333–339.
Van Otterlo, M. and M. Wiering (2012), ‘Reinforcement learning and markov decision
processes’. In: Reinforcement Learning. Springer, pp. 3–42.
Vinyals, O., I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A.
Dudzik, A. Huang, P. Georgiev, R. Powell, et al. (2019a), ‘AlphaStar: Mastering
the real-time strategy game StarCraft II’. DeepMind Blog.
Vinyals, O., I. Babuschkin, M. W. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, H. D.
Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka,
A. Huang, L. Sifre, T. Cai, P. J. Agapiou, M. Jaderberg, S. A. Vezhnevets, R.
Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, L. T. Paine,
C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K.
McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C.
Apps, and D. Silver (2019b), ‘Grandmaster level in StarCraft II using multi-agent
reinforcement learning’. Nature pp. 1–5.
Vinyals, O., I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung,
D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. (2019c), ‘Grandmaster level in
StarCraft II using multi-agent reinforcement learning’. Nature 575(7782), 350–354.
Vinyals, O., I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung,
D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. (2019d), ‘Grandmaster level in
StarCraft II using multi-agent reinforcement learning’. Nature 575(7782), 350–354.
Vinyals, O., T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A.
Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, et al. (2017), ‘StarCraft II: A
new challenge for reinforcement learning’. arXiv preprint arXiv:1708.04782.
Viossat, Y. and A. Zapechelnyuk (2013), ‘No-regret dynamics and fictitious play’.
Journal of Economic Theory 148(2), 825–842.
Vlatakis-Gkaragkounis, E.-V., L. Flokas, and G. Piliouras (2019), ‘Poincaré Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex
Non-Concave Zero-Sum Games’. In: H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett (eds.): Advances in Neural Information
Processing Systems, Vol. 32. Curran Associates, Inc.

Bibliography

187

Von Neumann, J. and O. Morgenstern (1945), Theory of games and economic behavior.
Princeton University Press Princeton, NJ.
Von Neumann, J. and O. Morgenstern (2007), Theory of games and economic behavior
(commemorative edition). Princeton university press.
von Stengel, B. and D. Koller (1997), ‘Team-maxmin equilibria’. Games and Economic
Behavior 21(1-2), 309–321.
Vyas, A. and K. Azizzadenesheli (2022), ‘Competitive Gradient Optimization’. arXiv
preprint arXiv:2205.14232.
Wainwright, M. J., M. I. Jordan, et al. (2008), ‘Graphical models, exponential families,
and variational inference’. Foundations and Trends® in Machine Learning 1(1–2),
1–305.
Wang, L., Q. Cai, Z. Yang, and Z. Wang (2019), ‘Neural Policy Gradient Methods:
Global Optimality and Rates of Convergence’. In: International Conference on
Learning Representations.
Wang, M., C. Ma, Q. Chen, L. Meng, Y. Han, J. Xiao, Z. Zhang, J. Huo, W. J. Su,
and Y. Yang (2024), ‘Magnetic Preference Optimization: Achieving Last-iterate
Convergence for Language Model Alignment’. arXiv preprint arXiv:2410.16714.
Wang, X. and T. Sandholm (2003a), ‘Learning near-Pareto-optimal conventions in
polynomial time’. Advances in Neural Information Processing Systems 16.
Wang, X. and T. Sandholm (2003b), ‘Reinforcement learning to play an optimal Nash
equilibrium in team Markov games’. In: NIPS. pp. 1603–1610.
Watkins, C. J. and P. Dayan (1992), ‘Q-learning’. Machine learning 8(3-4), 279–292.
Waugh, K., D. Morrill, J. A. Bagnell, and M. Bowling (2014), ‘Solving games with
functional regret estimation’. arXiv preprint arXiv:1411.7974.
Weaver, L. and N. Tao (2001), ‘The optimal reward baseline for gradient-based reinforcement learning’. In: Proceedings of the Seventeenth conference on Uncertainty
in artificial intelligence. pp. 538–545.
Wei, C.-Y., Y.-T. Hong, and C.-J. Lu (2017), ‘Online reinforcement learning in
stochastic games’. In: Advances in Neural Information Processing Systems. pp.
4987–4997.
Wei, C.-Y., C.-W. Lee, M. Zhang, and H. Luo (2020), ‘Linear Last-iterate Convergence
in Constrained Saddle-point Optimization’. arXiv e-prints pp. arXiv–2006.
Wei, H., G. Zheng, H. Yao, and Z. Li (2018), ‘Intellilight: A reinforcement learning
approach for intelligent traffic light control’. In: Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. pp.
2496–2505.

188

Bibliography

Weiss, G. (1999), Multiagent systems: a modern approach to distributed artificial
intelligence. MIT press.
Weiss, P. (1907), ‘L’hypothèse du champ moléculaire et la propriété ferromagnétique’.
Wen, Y., Y. Yang, R. Luo, and J. Wang (2019), ‘Modelling Bounded Rationality in
Multi-Agent Interactions by Generalized Recursive Reasoning’. ĲCAI.
Wen, Y., Y. Yang, R. Luo, J. Wang, and W. Pan (2018), ‘Probabilistic Recursive
Reasoning for Multi-Agent Reinforcement Learning’. In: International Conference
on Learning Representations.
Wibisono, A., M. Tao, and G. Piliouras (2022), ‘Alternating Mirror Descent for
Constrained Min-Max Games’. arXiv preprint arXiv:2206.04160.
Williams, R. J. (1992), ‘Simple statistical gradient-following algorithms for connectionist reinforcement learning’. Machine learning 8(3-4), 229–256.
Wooldridge, M. (2009), An introduction to multiagent systems. John Wiley & Sons.
Wu, F., S. Zilberstein, and X. Chen (2010), ‘Rollout sampling policy iteration for
decentralized POMDPs’. In: Proceedings of the Twenty-Sixth Conference on
Uncertainty in Artificial Intelligence. pp. 666–673.
Wu, F., S. Zilberstein, and N. R. Jennings (2013), ‘Monte-Carlo expectation maximization for decentralized POMDPs’. In: Twenty-Third International Joint Conference
on Artificial Intelligence.
Wu, Y., J. Donahue, D. Balduzzi, K. Simonyan, and T. Lillicrap (2019), ‘Logan: Latent
optimisation for generative adversarial networks’. arXiv preprint arXiv:1912.00953.
Xie, Q., Y. Chen, Z. Wang, and Z. Yang (2020), ‘Learning zero-sum simultaneousmove markov games using function approximation and correlated equilibrium’. In:
Conference on learning theory. pp. 3674–3682.
Xu, H., K. Li, B. Liu, H. Fu, Q. Fu, J. Xing, and J. Cheng (2024a), ‘Minimizing
Weighted Counterfactual Regret with Optimistic Online Mirror Descent’. arXiv
preprint arXiv:2404.13891.
Xu, L., D. Perez-Liebana, and A. Dockhorn (2024b), ‘Strategy Game-Playing with
Size-Constrained State Abstraction’. In: 2024 IEEE Conference on Games (CoG).
pp. 1–8.
Xu, Z., J. Shen, Z. Wang, and Y. Dai (2021), ‘Zeroth-order alternating randomized
gradient projection algorithms for general nonconvex-concave minimax problems’.
arXiv preprint arXiv:2108.00473.
Yabu, Y., M. Yokoo, and A. Iwasaki (2007), ‘Multiagent planning with tremblinghand perfect equilibrium in multiagent POMDPs’. In: Pacific Rim International
Conference on Multi-Agents. pp. 13–24.

Bibliography

189

Yadkori, Y. A., P. L. Bartlett, V. Kanade, Y. Seldin, and C. Szepesvári (2013),
‘Online learning in Markov decision processes with adversarially chosen transition
probability distributions’. In: Advances in neural information processing systems.
pp. 2508–2516.
Yang, J., A. Orvieto, A. Lucchi, and N. He (2022), ‘Faster single-loop algorithms for
minimax optimization without strong concavity’. In: International Conference on
Artificial Intelligence and Statistics. pp. 5485–5517.
Yang, J., X. Ye, R. Trivedi, H. Xu, and H. Zha (2018a), ‘Learning Deep Mean Field
Games for Modeling Large Population Behavior’. In: International Conference on
Learning Representations.
Yang, S., O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans (2023), ‘Foundation
models for decision making: Problems, methods, and opportunities’. arXiv preprint
arXiv:2303.04129.
Yang, Y., R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang (2018b), ‘Mean Field
Multi-Agent Reinforcement Learning’. In: International Conference on Machine
Learning. pp. 5567–5576.
Yang, Y., Y. Wen, L. Chen, J. Wang, K. Shao, D. Mguni, and W. Zhang (2020),
‘Multi-Agent Determinantal Q-Learning’.
Yang, Z., Y. Chen, M. Hong, and Z. Wang (2019a), ‘Provably global convergence of
actor-critic: A case for linear quadratic regulator with ergodic cost’. In: Advances
in Neural Information Processing Systems. pp. 8353–8365.
Yang, Z., Y. Xie, and Z. Wang (2019b), ‘A theoretical analysis of deep Q-learning’.
arXiv preprint arXiv:1901.00137.
Ye, Y. (2005), ‘A new complexity result on solving the Markov decision problem’.
Mathematics of Operations Research 30(3), 733–749.
Ye, Y. (2010), ‘The simplex method is strongly polynomial for the Markov decision
problem with a fixed discount rate’.
Yongacoglu, B., G. Arslan, and S. Yüksel (2019), ‘Learning Team-Optimality
for Decentralized Stochastic Control and Dynamic Games’. arXiv preprint
arXiv:1903.05812.
Yoon, T. and E. K. Ryu (2022), ‘Accelerated Minimax Algorithms Flock Together’.
arXiv preprint arXiv:2205.11093.
Young, H. P. (1993), ‘The evolution of conventions’. Econometrica: Journal of the
Econometric Society pp. 57–84.
Yu, J. Y., S. Mannor, and N. Shimkin (2009), ‘Markov decision processes with
arbitrary reward processes’. Mathematics of Operations Research 34(3), 737–757.

190

Bibliography

Zazo, S., S. Valcarcel Macua, M. Sánchez-Fernández, and J. Zazo (2015), ‘Dynamic
Potential Games in Communications: Fundamentals and Applications’. arXiv pp.
arXiv–1509.
Zermelo, E. and E. Borel (1913), ‘On an application of set theory to the theory of the
game of chess’. In: Congress of Mathematicians. pp. 501–504.
Zhan, W., J. D. Lee, and Z. Yang (2022), ‘Decentralized Optimistic Hyperpolicy
Mirror Descent: Provably No-Regret Learning in Markov Games’. arXiv preprint
arXiv:2206.01588.
Zhang, B. and T. Sandholm (2020), ‘Small Nash equilibrium certificates in very large
games’. Advances in Neural Information Processing Systems 33, 7161–7172.
Zhang, B. and T. Sandholm (2021a), ‘Subgame solving without common knowledge’.
Advances in Neural Information Processing Systems 34, 23993–24004.
Zhang, B. and T. Sandholm (2022), ‘Polynomial-time optimal equilibria with a
mediator in extensive-form games’. Advances in Neural Information Processing
Systems 35, 24851–24863.
Zhang, B. H., G. Farina, I. Anagnostides, F. Cacciamani, S. M. McAleer, A. A. Haupt,
A. Celli, N. Gatti, V. Conitzer, and T. Sandholm (2023a), ‘Steering No-Regret
Learners to a Desired Equilibrium’. arXiv preprint arXiv:2306.05221.
Zhang, B. H., G. Farina, I. Anagnostides, F. Cacciamani, S. M. McAleer, A. A. Haupt,
A. Celli, N. Gatti, V. Conitzer, and T. Sandholm (2023b), ‘Steering no-regret
learners to optimal equilibria’.
Zhang, B. H., G. Farina, A. Celli, and T. Sandholm (2022a), ‘Optimal correlated
equilibria in general-sum extensive-form games: Fixed-parameter algorithms,
hardness, and two-sided column-generation’. In: Proceedings of the 23rd ACM
conference on economics and computation. pp. 1119–1120.
Zhang, B. H., G. Farina, and T. Sandholm (2022b), ‘Team Belief DAG Form: A
Concise Representation for Team-Correlated Game-Theoretic Decision Making’.
arXiv preprint arXiv:2202.00789.
Zhang, B. H. and T. Sandholm (2021b), ‘Team Correlated Equilibria in
Zero-Sum Extensive-Form Games via Tree Decompositions’. arXiv preprint
arXiv:2109.05284.
Zhang, C. and V. Lesser (2010), ‘Multi-agent learning with policy prediction’. In:
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 24.
Zhang, G., Y. Wang, L. Lessard, and R. B. Grosse (2022c), ‘Near-optimal local
convergence of alternating gradient descent-ascent for minimax optimization’. In:
International Conference on Artificial Intelligence and Statistics. pp. 7659–7679.

Bibliography

191

Zhang, G. and Y. Yu (2019), ‘Convergence of gradient methods on bilinear zero-sum
games’. arXiv preprint arXiv:1908.05699.
Zhang, H., W. Chen, Z. Huang, M. Li, Y. Yang, W. Zhang, and J. Wang (2019a), ‘Bilevel Actor-Critic for Multi-agent Coordination’. arXiv preprint arXiv:1909.03510.
Zhang, J., P. Xiao, R. Sun, and Z. Luo (2020a), ‘A Single-Loop Smoothed Gradient
Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems’. In: H.
Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (eds.): Advances in
Neural Information Processing Systems, Vol. 33. pp. 7377–7389, Curran Associates,
Inc.
Zhang, K., S. Kakade, T. Basar, and L. Yang (2020b), ‘Model-based multi-agent rl
in zero-sum markov games with near-optimal sample complexity’. Advances in
Neural Information Processing Systems 33, 1166–1178.
Zhang, K., T. Sun, Y. Tao, S. Genc, S. Mallya, and T. Basar (2020c), ‘Robust
Multi-Agent Reinforcement Learning with Model Uncertainty’. Advances in Neural
Information Processing Systems 33.
Zhang, K., Z. Yang, and T. Basar (2018a), ‘Networked multi-agent reinforcement
learning in continuous spaces’. In: 2018 IEEE CDC. pp. 2771–2776.
Zhang, K., Z. Yang, and T. Başar (2019b), ‘Multi-agent reinforcement learning: A
selective overview of theories and algorithms’. arXiv preprint arXiv:1911.10635.
Zhang, K., Z. Yang, and T. Basar (2019c), ‘Policy optimization provably converges
to Nash equilibria in zero-sum linear quadratic games’. In: Advances in Neural
Information Processing Systems. pp. 11602–11614.
Zhang, K., Z. Yang, H. Liu, T. Zhang, and T. Başar (2018b), ‘Finite-Sample Analysis
For Decentralized Batch Multi-Agent Reinforcement Learning With Networked
Agents’. arXiv preprint arXiv:1812.02783.
Zhang, K., Z. Yang, H. Liu, T. Zhang, and T. Basar (2018c), ‘Fully Decentralized
Multi-Agent Reinforcement Learning with Networked Agents’. In: International
Conference on Machine Learning. pp. 5872–5881.
Zhang, R., Z. Xu, C. Ma, C. Yu, W.-W. Tu, S. Huang, D. Ye, W. Ding, Y. Yang, and Y.
Wang (2024), ‘A Survey on Self-play Methods in Reinforcement Learning’. arXiv
preprint arXiv:2408.01072.
Zhang, Y. and B. An (2020a), ‘Computing Team-Maxmin Equilibria in Zero-Sum
Multiplayer Extensive-Form Games’. In: AAAI Conference on Artificial Intelligence
(AAAI). pp. 2318–2325.
Zhang, Y. and B. An (2020b), ‘Converging to Team-Maxmin Equilibria in Zero-Sum
Multiplayer Games’. In: International Conference on Machine Learning (ICML).

192

Bibliography

Zhang, Y. and M. M. Zavlanos (2019), ‘Distributed off-policy actor-critic reinforcement
learning with policy consensus’. In: 2019 IEEE 58th Conference on Decision and
Control (CDC). pp. 4674–4679.
Zhao, T., H. Hachiya, G. Niu, and M. Sugiyama (2011), ‘Analysis and improvement
of policy gradient estimation’. In: Advances in Neural Information Processing
Systems. pp. 262–270.
Zhou, M., Y. Chen, Y. Wen, Y. Yang, Y. Su, W. Zhang, D. Zhang, and J. Wang (2019),
‘Factorized Q-learning for large-scale multi-agent systems’. In: Proceedings of the
First International Conference on Distributed Artificial Intelligence. pp. 1–7.
Zimin, A. and G. Neu (2013), ‘Online learning in episodic Markovian decision
processes by relative entropy policy search’. In: Advances in neural information
processing systems. pp. 1583–1591.
Zinkevich, M. (2003), ‘Online convex programming and generalized infinitesimal
gradient ascent’. In: Proceedings of the 20th international conference on machine
learning (icml-03). pp. 928–936.
Zinkevich, M., A. Greenwald, and M. L. Littman (2006), ‘Cyclic equilibria in Markov
games’. In: Advances in Neural Information Processing Systems. pp. 1641–1648.
Zinkevich, M., M. Johanson, M. Bowling, and C. Piccione (2008), ‘Regret minimization in games with incomplete information’. In: Advances in Neural Information
Processing Systems (NeurIPS).

