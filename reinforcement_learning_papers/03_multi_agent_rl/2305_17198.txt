arXiv:2305.17198v2 [cs.LG] 18 Jan 2024

A Model-Based Solution to the Offline Multi-Agent
Reinforcement Learning Coordination Problem
Paul Barde

Jakob Foerster

Mila - Quebec AI Institute
McGill University
Montreal, Canada
bardepau@mila.quebec

University of Oxford
Oxford, United Kingdom

Derek Nowrouzezahrai

Amy Zhang

Mila - Quebec AI Institute
McGill University
Montreal, Canada

The University of Texas at Austin
Meta AI - FAIR
Austin, USA

ABSTRACT

1

Training multiple agents to coordinate is an essential problem with
applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning
(MARL) methods are online and thus impractical for real-world
applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when
available, doing so gives rise to what we call the offline coordination problem. Specifically, we identify and formalize the strategy
agreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two issues at which current offline MARL algorithms fail.
Concretely, we reveal that the prevalent model-free methods are
severely deficient and cannot handle coordination-intensive offline
multi-agent tasks in either toy or MuJoCo domains. To address
this setback, we emphasize the importance of inter-agent interactions and propose the very first model-based offline MARL method.
Our resulting algorithm, Model-based Offline Multi-Agent Proximal
Policy Optimization (MOMA-PPO) generates synthetic interaction
data and enables agents to converge on a strategy while fine-tuning
their policies accordingly. This simple model-based solution solves
the coordination-intensive offline tasks, significantly outperforming the prevalent model-free methods even under severe partial
observability and with learned world models.

Multi-agent problems are ubiquitous in real-world scenarios, including traffic control, distributed energy management, multi-robot
coordination, auctions, marketplaces, and social networks [6, 8, 10,
13, 15, 16, 21, 25, 42, 44, 53, 58]. This makes the development of
efficient multi-agent algorithms a crucial research area in artificial
intelligence and machine learning with substantial implications in
various fields including robotics, game theory, economics, and social
sciences [35, 37, 51, 60, 68]. However, existing methods are mostly
online and require interacting with the environment throughout
learning which often makes them costly or even dangerous for realworld applications [34]. In contrast, offline Reinforcement Learning
(offline RL) obviates the need for interactions with the environment
as it allows learning from existing datasets that do not have to be
collected by experts. It is therefore well suited to tasks where: a) we
cannot afford to materialize the situation in practice, b) it is unfeasible to build a simulator, and c) there exist datasets of realizations
of such situations. Consequently, we hypothesize that offline multiagent approaches will be key for tackling real-world multi-agent
problems. Let us imagine for instance trying to understand how
autonomous actors (i.e., governments, international organizations,
industries, etc.) must maneuver to reduce the severity of a worldwide pandemic while preventing economic collapse. It goes without
saying that: a) starting pandemics is not a viable way to gain realworld practice; b) building a simulator is a colossal task that would
suggest emulating our society and its economy; and c) the impact
of past decisions (such as implementing lockdown policies, travel
restrictions, and vaccination campaigns) on the unfolding of the
pandemics and the economy is well-documented. Hopefully, these
records can be used to derive new strategies in the future.

KEYWORDS
Multi-Agent Learning; Coordination; Offline Reinforcement Learning; Model-Based Reinforcement Learning; Deep Learning; World
Model
ACM Reference Format:
Paul Barde, Jakob Foerster, Derek Nowrouzezahrai, and Amy Zhang. 2024.
A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. In Proc. of the 23rd International Conference on
Autonomous Agents and Multiagent Systems (AAMAS 2024), Auckland, New
Zealand, May 6 â€“ 10, 2024, IFAAMAS, 17 pages.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2024), N. Alechina, V. Dignum, M. Dastani, J.S. Sichman (eds.), May 6 â€“ 10, 2024,
Auckland, New Zealand. Â© 2024 International Foundation for Autonomous Agents and
Multiagent Systems (www.ifaamas.org).

INTRODUCTION

The offline coordination challenge. In general, actors such as
individuals, organizations, robots, software processes, or cars are
self-governed and ultimately act autonomously. However, during
the offline learning phase, it is reasonable to assume that learners can share pieces of information, which makes the Centralized
Training Decentralized Execution (CTDE) formulation a natural
approach for the offline training regime. Unfortunately, as shown in
this work, simply combining CTDE MARL and offline RL methods
on a dataset of multi-agent interactions does not necessarily yield
policies that perform well together. This is what we call the offline

b

b

optimum
kin

pic
a

g

a

b

g a

pickin

data collection
dataset
policy update
strategy update

(a) online learning

(b) offline learning

Figure 1: Comparing online learning and offline learning in policy space to illustrate the offline coordination problem.
(a) During online learning, agents continuously interact using their current policies and collect new data that informs the
next update of the co-evolution from (ğœ‹10, ğœ‹20 ) to (ğœ‹1ğ‘ , ğœ‹2ğ‘ ). (b) During offline learning, agents cannot collect new data and thus,
they can only estimate updates from the dataset of interactions (here collected with ğœ‹1D and ğœ‹ 2D ). To reach an optimal strategy
agents must (1) agree on which optimum to target between â˜…ğ‘ or â˜…ğ‘ , â€“ i.e. solve strategy agreement â€“, and (2) respectively derive
â˜…ğ‘—
â˜…ğ‘—
the policy corresponding to that strategy (ğœ‹1 and ğœ‹2 ), â€“ i.e. solve strategy fine-tuning.
coordination problem: 1) multi-agent solutions require agents to
coordinate, that is to act coherently as a group such that individual
behaviors combine into an efficient team strategy; 2) learning to
coordinate is particularly challenging in the absence of interactions
during training.
In this work, we propose that multi-agent coordination can be
decomposed into two distinct challenges. First, since most multiagent tasks allow multiple optimal team strategies [5] â€“ some of
which may only vary on how they break symmetries present in the
task [24] â€“ agents must collectively select one strategy over another
such that they individually converge toward coherent behaviors.
We refer to this coordination challenge as Strategy Agreement (SA).
Additionally, for a chosen team strategy, agents have to precisely
calibrate and adjust their behaviors to one another in order to reach
the corresponding optimal group behavior. We call this Strategy
Fine-Tuning (SFT). During online learning, agents continuously interact together in the environment, therefore, changes due to one
agentâ€™s local optimization directly impact other agentsâ€™ experiences
and teammates are able to adapt: coordination occurs through interactive trial and error. Conversely, when learning from a fixed
dataset of interactions, agents cannot interact with other agents to
generate new data that explicitly measures the outcomes of current
policies in the environment (i.e., which global strategy is being
chosen and how individual behaviors blend together). Therefore, it
is difficult for offline learners to coordinate. Figure 1 illustrates this
in policy space: (a) During online learning, agents continuously
interact to collect up-to-date data that inform the optimization of
both individual policies and team strategy. This allows agents to
converge to a global optimum by co-adapting and improving on
each otherâ€™s updates. (b) In the offline setting however, agents must
independently decide towards which of the two optima they aim
to converge (i.e., strategy agreement between ğœ‹ğ‘–â˜…ğ‘ or ğœ‹ğ‘–â˜…ğ‘ , ğ‘– = 1, 2).
Assuming they pick â˜…ğ‘, they must then derive their corresponding
optimal policy (i.e., strategy fine-tuning toward ğœ‹ğ‘–â˜…ğ‘ , ğ‘– = 1, 2). Crucially, offline agents must rely only on interactions present in the
dataset (corresponding to policies ğœ‹ğ‘–D , ğ‘– = 1, 2), thus likely without
interactions corresponding to their past or current policies.

Current methods [27, 43, 64] deal with offline MARL by simply
extending single-agent offline RL to the multi-agent setting. To do
so, they either consider that agents are independent learners, or
they leverage the CTDE paradigm. Provided that the dataset has
enough coverage, it is in theory possible for CTDE agents to learn
the different optimal value functions and policies. Yet, as we will see
in Section 6, agents may still fail to agree on which team strategy
to pick. We highlight such offline coordination failure in an offline
version of the well-established Iterated Coordination Game [5].
The crux of offline MARL is therefore that learners cannot interact
together in the environment to collect new data that measures how
individual policies blend together and what are the outcomes of the
current team strategy. This relates to the absence of interactions
during learning and is not a centralization issue. Thus, we motivate
the need for generating synthetic data that measure the outcomes
of the teamâ€™s current strategy to allow for coordinated agents.
We propose MOMA-PPO, a simple model-based approach that
generates synthetic interactions. We show that it allows for offline coordination even in complex Multi-Agent MuJoCo (MAMuJoCo) [45] tasks with partial observability and learned world model.
Across tasks and domains, MOMA-PPO significantly outperforms
the offline MARL baselines. Our contributions are:
â€¢ identifying and defining the offline coordination problem, an
issue that has been overlooked by the offline MARL community;
â€¢ proposing benchmarks in the form of new datasets and partially
observable multi-agent tasks that test for offline coordination;
â€¢ showing that current methods, which are all model-free, fail at
offline coordination even in simple environments;
â€¢ suggesting a link between the offline coordination problem
and the lack of inter-agent interactions during learning that is
inherent to model-free offline approaches;
â€¢ proposing to address the coordination problem with the firstever model-based offline MARL method;
â€¢ experimentally validating the benefits of coordinating by interacting through a world model.

2

BACKGROUND

We consider the formalism of Dec-POMDP [41] with states ğ‘ ğ‘¡ âˆˆ S.
There are ğ‘– = 1, ..., ğ‘ğ´ agents that partially observe this state from
their stochastic observation function ğ‘ ğ‘¡ğ‘– âˆ¼ ğ‘‚ ğ‘– (ğ‘  ğ‘– |ğ‘ ğ‘¡ ) and choose
action ğ‘ğ‘–ğ‘¡ âˆˆ Ağ‘– at every time step. Each agent has an actionobservation history given by â„ğ‘–ğ‘¡ = {ğ‘  0ğ‘– , ğ‘ğ‘–0, ...ğ‘ ğ‘¡ğ‘– } and picks its action
ğ‘ğ‘–ğ‘¡ using its stochastic policy ğœ‹ğ‘– (ğ‘ğ‘– |â„ğ‘–ğ‘¡ ). The environmentâ€™s transition depends on the joint action ğ‘ğ‘¡ such that ğ‘ ğ‘¡ +1 âˆ¼ ğ‘ƒ (ğ‘  â€² |ğ‘ ğ‘¡ , ğ‘ğ‘¡ ). The
game is fully cooperative so all agents receive the same reward and
ğ‘Ÿğ‘¡ğ‘– = ğ‘Ÿğ‘¡ = ğ‘Ÿ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) âˆˆ R âˆ€ğ‘–. The agentâ€™s objective is therefore to maxÃ
imize the expected team return ğ½ = Eğœ ğ‘…(ğœ) where ğ‘…(ğœ) = ğ‘¡ ğ›¾ ğ‘¡ ğ‘Ÿğ‘¡
with discount factor ğ›¾ and ğœ = {ğ‘  0, ğ‘ 0, ğ‘Ÿ 0, ...ğ‘  ğ¹ } is a trajectory with
absorbing state ğ‘  ğ¹ . Note that some trajectories can become arbitrarily long in which case we truncate them and use the value of the
last state as an estimation of the return-to-go.
We adopt the CTDE framework [17, 38], where at training time,
individual observations, policies, and value functions are available
to all the agents. For simplicity, we assume access to the global
state ğ‘ ğ‘¡ and the observation functions ğ‘‚ ğ‘– . Yet, this is not a requirement and it is always possible to define the global state as the
concatenation of the agentsâ€™ observations, i.e., ğ‘ ğ‘¡ = {ğ‘ ğ‘¡1, ..., ğ‘ ğ‘¡ğ‘ğ´ }. In
most problems, which are special cases of Dec-POMDPs referred
to as Dec-MDPs, such concatenation will fully observe the environment. At execution, agents must act independently and the
joint policy is approximated by individual decentralized policies as
Ãğ‘ ğ´
ğ‘ğ‘¡ âˆ¼ ğœ‹ (ğ‘|ğ‘ ğ‘¡ ) â‰ˆ ğ‘–=0
ğœ‹ğ‘– (ğ‘ğ‘– |â„ğ‘–ğ‘¡ ).
Finally, we consider Offline Learning [20, 34], where agents have
only access to a fixed dataset of trajectories D and cannot collect
additional interactions with the environment during learning.

3

METHOD

In this work, we propose MOMA-PPO, a Dyna-like [55] modelbased approach to multi-agent CTDE offline learning that relies
on PPO [50]. The method can be decomposed into two steps: 1)
learning a world model from the dataset, and 2) using the world
model to train the agentsâ€™ policies.

3.1

Learning a centralized world model
ensemble

MOMA-PPO leverages the CTDE assumptions to learn centralized
models that predict the next state, reward, and termination condition from the current state and actions. When learning in an
approximate world model, RL agents might learn to exploit the
world modelâ€™s reconstruction inaccuracies to reap more rewards in
simulation, eventually producing incoherent behaviors that perform
poorly in the real world [22]. One way to avoid this is to penalize
the agents for going into regions of the state-action space where
the world model is uncertain about its predictions [67]. Learning an
ensemble of models enables estimating the world modelâ€™s epistemic
uncertainty due to the limited amount of learning data in the offline
dataset. Each model comprises two diagonal Gaussians N (ğœ‡ğ‘‡ , ğœğ‘‡2 )
and N (ğœ‡ğ‘Ÿ , ğœğ‘Ÿ2 ) that respectively model the next state ğ‘  â€² and the reward ğ‘Ÿ . Models also predict whether or not the next state is terminal
using a Bernoulli distribution Bern(ğ‘ğ‘‘ ). Distributionsâ€™ ğœ‡ğ‘‡ , ğœ‡ğ‘… , ğœğ‘‡ ,
ğœğ‘… , and ğ‘ğ‘‘ are parametrized by neural networks conditioned on

the current global state ğ‘  and the joint action ğ‘. The parameters
are learned from the offline dataset D using Gaussian negative loglikelihood for N (ğœ‡ğ‘‡ , ğœğ‘‡2 ) and N (ğœ‡ğ‘Ÿ , ğœğ‘Ÿ2 ), and binary cross-entropy
for Bern(ğ‘ğ‘‘ ). In practice, we train ğ‘ğ‘š = 7 models and keep the best
ğ‘ = 5 based on their average validation accuracy across the next
states and rewards [67]. We estimate the epistemic uncertainty of
the reward using the variance of the predicted rewards across the
ensemble:
Ãğ‘
Ãğ‘
2
ğ‘ŸË†ğ‘š
ğ‘š=1 (ğ‘ŸË†ğ‘š âˆ’ ğ‘ŸÂ¯)
,
ğ‘ŸÂ¯ = ğ‘š=1 .
ğœ–ğ‘Ÿ =
ğ‘ âˆ’1
ğ‘
We also estimate the epistemic uncertainty of the general prediction by concatenating the next state and the reward and computing
the Frobenius norm of the ensemble covariance matrix:
ğœ–ğ‘” = ||cov(ğ‘¥ğ‘– , ğ‘¥ ğ‘— )|| ğ¹ ,
Ãğ‘
(ğ‘¥Ë†ğ‘–,ğ‘š âˆ’ ğ‘¥Â¯ğ‘— )(ğ‘¥Ë† ğ‘—,ğ‘š âˆ’ ğ‘¥Â¯ğ‘– )
cov(ğ‘¥ğ‘– , ğ‘¥ ğ‘— ) = ğ‘š=1
,
ğ‘ âˆ’1
where ğ‘¥ğ‘– and ğ‘¥ ğ‘— are components of the vector resulting from the
concatenation of the predicted next state vector ğ‘ Ë†â€² and the predicted
reward scalar ğ‘ŸË†. At this point, we define a world model based on
the ensemble such that:
ğ‘ Ë†ğ‘¡ +1, ğ‘ŸÂ¯ğ‘¡ , ğ‘“Â¯ğ‘¡ , ğœ–ğ‘Ÿ,ğ‘¡ , ğœ–ğ‘”,ğ‘¡ âˆ¼ M (Â·|ğ‘ ğ‘¡ , ğ‘ğ‘¡ ),
where ğ‘“Â¯ğ‘¡ is a mask equal to 0, if the model predicts that we reached
an absorbing state, and 1 otherwise. ğ‘ŸÂ¯ğ‘¡ is the mean predicted reward
across the ensemble and ğ‘“Â¯ğ‘¡ results from a majority vote between
the members of the ensemble. The mean state would likely be outof-distribution and lack the structure of real states which would
impede learning and evaluation. Therefore, ğ‘ Ë†ğ‘¡ +1 is instead sampled
uniformly amongst each ensemble memberâ€™s next state. Finally, to
avoid unrealistic values, ğ‘ŸÂ¯ğ‘¡ and ğ‘ Ë†ğ‘¡ +1 are clipped to the minimum
bounding box of the offline dataset while uncertainties estimations
are limited to a specified threshold ğ‘™ğœ– :
min ğ‘Ÿ â‰¤ ğ‘ŸÂ¯ğ‘¡ â‰¤ max ğ‘Ÿ

ğ‘ŸâˆˆD

ğ‘ŸâˆˆD

min ğ‘ ğ‘– â‰¤ ğ‘ Ë†ğ‘¡ +1,ğ‘– â‰¤ max ğ‘ ğ‘–

ğ‘ ğ‘– âˆˆ D

âˆ€ğ‘– âˆˆ [0, ğ‘] | S âŠ‚ Rğ‘ ,

ğ‘ ğ‘– âˆˆ D

and ğœ–ğ‘Ÿ â‰¤ ğ‘™ğœ– , ğœ–ğ‘” â‰¤ ğ‘™ğœ– .

3.2

Model-based Offline Multi-Agent PPO
(MOMA-PPO)

Once M has been trained on the
offline dataset D, it can be used
to train online reinforcement learning algorithms in a Dyna-like manner. Here, we use MAPPO, a CTDE
multi-agent version of PPO [65].
The synthetic data used to train
the PPO policies is collected by sampling states from the offline dataset Figure 2: Model-based
D and using the current policies rollouts generation (blue)
ğœ‹ğ‘– alongside the world model M to from datasetâ€™s states
generate PPOâ€™s training rollouts of (grey). Red denotes early
size ğ‘˜. Terminating a rollout when termination and ğ‘˜ = 3.
the world model uncertainty exceeds ğ‘™ğœ– allows for adaptative rollout length and avoids training the

policies on unfeasible data. Rollouts are always terminated with a
timeout mask ğœğ‘¡ such that:
ğœğ‘¡ = 1 âˆ’ I(ğ‘¡ = ğ‘˜ âˆ’ 1 âˆª ğœ–ğ‘”,ğ‘¡ â‰¥ ğ‘™ğœ– ),
where I is the indicator function. Note that the timeout mask ğœğ‘¡ is
different from the model-predicted mask ğ‘“ğ‘¡ which is solely related
to the environment and indicates terminal absorbing states that
occur for instance when the agent dies or reaches the goal. On the
PPO side, we adapt the generalized advantage estimation [49] to
account for the timeouts ğœğ‘¡ and ensure that there is no accumulation
across rollouts while computing returns. We use the value of the
last state as an estimation of the return-to-go (see Appendix A).
The generation of length ğ‘˜ = 3 rollouts is illustrated in Figure 2
where it can be seen that a rollout is interrupted early because the
world model was unreliable when generating the associated state
(shown in red).
In addition to the adaptive rollout length, the model epistemic
uncertainty is used to penalize the agents so they avoid exploiting
the world model in poorly reconstructed regions of the state space.
The final uncertainty-penalized reward is given by:
ğ‘ŸËœğ‘¡ = ğ‘ŸÂ¯ğ‘¡ âˆ’ ğœ†ğ‘Ÿ ğœ–ğ‘Ÿ âˆ’ ğœ†ğ‘” ğœ–ğ‘” ,
where ğœ–ğ‘Ÿ and ğœ–ğ‘” are hyperparameters that weigh the severity of the
penalty.
Algorithm 1 MOMA-PPO
Require: offline dataset D, world model M, rollout horizon ğ‘˜, rollout
batch size ğ‘, uncertainty penalty coefficients ğœ†ğ‘Ÿ and ğœ†ğ‘” , uncertainty
threshold ğ‘™ğœ– , MAPPO agents.
Initialize MAPPO policies ğœ‹ğ‘– and value function ğ‘‰ .
for epoch 1, 2, . . . do
âŠ² Generate synthetic data
Initialize an empty rollout buffer R â† âˆ….
for 1, 2, . . . , ğ‘ (in parallel) do
ğ‘
Sample history â„ğ‘¡ = {â„ğ‘–ğ‘¡ }ğ‘–=1ğ´ from D.
for ğ‘— = ğ‘¡, ğ‘¡ + 1, . . . , ğ‘¡ + ğ‘˜ âˆ’ 1 do
Sample ğ‘ğ‘–ğ‘— âˆ¼ ğœ‹ğ‘– (ğ‘ğ‘– |â„ğ‘–ğ‘— ) âˆ€ğ‘–.
Sample ğ‘ Ë†ğ‘— +1 , ğ‘ŸÂ¯ğ‘— , ğ‘“Â¯ğ‘— , ğœ–ğ‘Ÿ,ğ‘— , ğœ–ğ‘”,ğ‘— âˆ¼ M (Â· |ğ‘  ğ‘— , ğ‘ ğ‘— ).
Compute ğ‘ŸËœ ğ‘— = ğ‘ŸÂ¯ğ‘— âˆ’ ğœ†ğ‘Ÿ ğœ–ğ‘Ÿ âˆ’ ğœ†ğ‘” ğœ–ğ‘” .
Compute ğœ ğ‘— = 1 âˆ’ I( ğ‘— = ğ‘¡ + ğ‘˜ âˆ’ 1 âˆª ğœ–ğ‘”,ğ‘— â‰¥ ğ‘™ğœ– )
Add sample (â„ ğ‘— , ğ‘ ğ‘— , ğ‘ŸËœ ğ‘— , ğ‘“Ë†ğ‘— , ğœ ğ‘— , ğ‘ Ë†ğ‘— +1 ) to R.
Get â„ğ‘–ğ‘— +1 from â„ğ‘–ğ‘— , ğ‘ğ‘–ğ‘— and ğ‘ Ë†ğ‘— +1 . Set ğ‘  ğ‘— +1 = ğ‘ Ë†ğ‘— +1 .
âŠ² Train agents
Use synthetics rollouts in R to train policies ğœ‹ğ‘– and value
function ğ‘‰ with MAPPO.
return multi-agent policies ğœ‹ğ‘– .

Practical considerations and limitations. As stated in Section 2, we assume CTDE and that the global state ğ‘ ğ‘¡ fully observes
the environment, therefore we do not equip the world model with
memory. The agents, on the other hand, only have access to partial observations and must rely on action-observation histories â„ğ‘–ğ‘¡ .
In practice, we restrict action-observation histories to 10 steps in
the past (either from the dataset or from the generated rollouts)
and process them with one layer of self-attention [59] followed
by one layer of soft-attention [1]. The resulting embeddings are
concatenated to the agentâ€™s current state, and for simplicity, we
abuse notation by denoting this â€œmemory enhanced" state with â„ğ‘–ğ‘¡ .

In MAPPO our centralized value function uses the QMIX valuedecomposition [47] with ğ‘¤ ğ‘– (ğ‘ ğ‘¡ ) and ğ‘ (ğ‘ ğ‘¡ ) provided by a learnable
Ã
neural network: ğ‘‰ (ğ‘ ğ‘¡ ) â‰œ ğ‘– ğ‘¤ ğ‘– (ğ‘ ğ‘¡ )ğ‘‰ ğ‘– (â„ğ‘–ğ‘¡ ) + ğ‘ (ğ‘ ğ‘¡ ).
Finally, it is important to note that the task of the MOMA-PPO
agents is quite different from the task of the agents that generated
the dataset. First, the MOMA-PPO agentsâ€™ initial state distribution
is now the datasetâ€™s state distribution. Then the reward of the task
has been altered to account for the model uncertainty. Last but
not least, agents are only allowed to stray ğ‘˜ steps away from the
datasetâ€™s coverage. While this restriction mitigates world model
abuse, it can also prevent the agents from discovering goals that
are further away from the offline data. Our resulting model-based
offline multi-agent method is illustrated in Algorithm 1 and more
details are provided in Appendix A.

4

RELATED WORK

Multi-agent coordination. Coordination has been a challenge
of interest since the early works on cooperative MARL [5, 7, 11, 12,
36] and has consistently been a central focus of the multi-agent
literature [26, 33, 38, 69]. While different works consider different
aspects of coordination â€“ such as behavior predictability and synchronous sub-policy selection [48], structured team exploration
[40] or the emergence of communication and cooperative guiding
[2, 32, 63] â€“ our definition of coordination is closest to the seminal
work of [5]. Indeed, we consider coordination in terms of agents
agreeing to individually follow the same team strategy (that is a
policy over joint actions) and finetuning their behavior to one another in tasks where multiple distinct optimal team strategies exist.
A similar notion of coordination has been used in the zero-shot coordination problem investigated by [24] where agents are trained so
that they are able to perform with agents they have never seen before. Yet, while their focus is on deriving standardized coordinated
strategies that can generalize to unseen teammates, coordination is
still learned through online interactions.
Coordination with teammates without direct interactions is often
referred to as ad-hoc coordination [4, 54]. Recent works assume
access to offline demonstrations of the teammatesâ€™ behaviors. These
can be used to guide the agentâ€™s self-play training toward adopting
the appropriate equilibrium (or â€œsocial conventionsâ€) of its future
teammates [33, 57]. Similarly, [9] uses offline data to learn a model of
the teammateâ€™s behavior and use it to train the agent to coordinate
with that ally. In ad-hoc coordination, teammatesâ€™ behaviors are
fixed and can be estimated a priori from the dataset: the learner
merely has to identify the team strategy and adopt it. Conversely,
in our offline coordination setting, all the agents are learning and
therefore have unknown changing behaviors: they must identify
the different potential team strategies and agree on which one to
follow (strategy agreement). Simultaneously, they must finetune
their behaviors to one another in order to reach this team policy
(strategy fine-tuning). All this without being able to interact with
other agents or the environment.
Offline MARL. Recent works have been investigating offline
solutions to the MARL problem. All of these methods build on
model-free single-agent approaches and constrain the policy to stay
in the datasetâ€™s distribution by using either SARSA-like schemes

(such as ICQ [64] and IQL [30]) or policy regularization (such as
CQL [31] and TD3+BC [19]).
Some methods investigate specific modifications to improve performance in the multi-agent setting. For instance, in the decentralized setting, MABCQ [27] enforces an optimistic importance
sampling modification that assumes that independent agents will
strive toward similar high-rewarding states, yet since this does not
discriminate between which high-rewarding state to favor, the strategy agreement issue remains. For discrete actions spaces problems,
[56] propose a Transformer-based approach that learns a centralized teacher and distills its policy into independent student policies.
Finally, OMAR [43] proposes to alleviate miscoordination failure in
offline MARL by adding a zeroth order optimization method on top
of multi-agent CQL, achieving state-of-the-art performance on a
variety of tasks. We share these worksâ€™ goal of learning coordinated
and efficient multi-agent teams in the offline setting. Yet, we believe
that interacting learners and agents are essential to coordination
and therefore take a different approach by focusing on model-based
methods rather than model-free ones.
Offline model-based RL. Model-based approaches have been
investigated in the single-agent offline RL setting. Notoriously,
MOPO [67] proposed to learn an ensemble-based world model
and use it to generate rollouts from the offline dataset to train a
SAC agent [23]. They also proposed an uncertainty-based reward
penalty to prevent the learner from exploiting the model. MOReL
[28] takes a similar approach but prevents model abuse by learning
a pessimistic MDP in which states that are outside of the dataset
coverage become absorbing terminal states. COMBO [66] proposed
a similar but more conservative version of MOPO by using CQL
instead of SAC and learning on both generated and datasetâ€™s states.
Finally, ROMI [61] also uses model-free offline RL to derive a policy
from a model-based augmented offline dataset, yet they enforce
additional conservatism by learning a reverse policy and dynamics
model to generate rollouts that lead to target states contained in
the dataset. This mitigates against generating rollouts outside of
the datasetâ€™s coverage.
We believe that offline RL algorithms are ill-suited to learn on
non-stationary data such as the one generated by updating policies
be it in a world model or in a real environment. Therefore, our
method MOMA-PPO uses an online RL algorithm instead of an
offline one. Additionally, to enforce conservatism and avoid world
model exploitation we use both uncertainty penalty and early rollout termination. In that sense, MOMA-PPO unifies what would
be multi-agent extensions of MOPO and MOReL. Yet, instead of
penalizing aleatoric uncertainty as in MOPO, we focus on epistemic
uncertainty and estimate it by monitoring the coherence between
the different models in the ensemble. Section 6.3 reports the benefits
of using the epistemic uncertainty (due to the finite amount of training data) over the aleatoric uncertainty (from the environmentâ€™s
stochasticity). Also, MOMA-PPOâ€™s early rollout termination is done
with timeouts rather than with MOReLâ€™s terminal states meaning
that it does not penalize agents while still avoiding using unfeasible
rollouts to train them. Finally, in contrast with MOReL and MOPO
which use off-policy algorithms, MOMA-PPO is based on MAPPO,
an on-policy method that has reliably achieved state-of-the-art
performance in MARL tasks [65]. It is therefore well suited to the

multi-agent setting in which slight changes in a teammateâ€™s policy
can drastically impact the overall group behavior and quickly make
previous interactions obsolete.
Model-based multi-agent RL. In the online setting, modelbased approaches aim to improve sample efficiency by reducing
the number of interactions with the environment. Therefore, these
methods use off-policy schemes and focus on how and when to
collect additional environment data for refining the world model
and the policies [62, 70, 71]. In offline MARL, sample efficiency is
not a consideration since the environment data has already been
collected offline and additional samples are not an option. Yet, as
we show here, model-based approaches can benefit multi-agent
coordination in the offline setting by allowing multiple learners to
interact through the world model.

5

BASELINES, ENVIRONMENTS, TASKS, AND
DATASETS
ğ‘2

ğ‘1

â†

â†’

â†

1,1

0,0

â†’

0,0

1,1

(a)

(b)

(c)

Figure 3: Environments illustrations. (a) Pay off matrix of the
Iterated Coordination Game. (b) Two-agent Reacher, red and
blue agents respectively control the torque on ğœƒ 1 and ğœƒ 2 . (c)
Four-agent Ant, each agent controls a different limb (shown
with different colors). In PO tasks, agents only observe the
limb they control while the torso observations â€“ in white â€“
are available only to the yellow agent.

5.1

Baselines

We compare with a large and varied array of baselines. First, we
consider a centralized training and centralized execution approach
by observing the global state and selecting the joint action. We
use IQL [30], a state-of-the-art single-agent model-free offline RL
algorithm for this setting. Considering centralized execution gives
an upper bound on what can be achieved in terms of strategy
agreement. Indeed, a single learner controls all the agents and can
thus choose for the whole team the strategy to adopt.
Then, we extend IQL [30] to the multi-agent setting by using the
QMIX value decomposition on the ğ‘„ and ğ‘‰ value networks. This
gives MAIQL, a very competitive model-free CTDE offline MARL
algorithm that should allow for fine-tuning with additional online
data after training. We refer to the finetuned version as MAIQLft and follow IQLâ€™s finetuning procedure [30]: MAIQL-ft is first
trained to convergence on the offline data and then finetuned for
the same number of training steps by progressively introducing
additional interaction data. At the end of finetuning, the replay
buffer contains as many offline interactions as online ones. For
completeness, we also consider MATD3+BC the CTDE multi-agent
version of [19] with QMIX.

Recent literature in model-free MARL [14, 39] and notably in the
offline setting [43], advocates for decentralized value functions and
independent learners. Therefore, we consider several independent
learner approaches, starting with Independent Behavioral Cloning
(IBC). Despite its simplicity, BC [46] produces surprisingly efficient baselines for Imitation Learning and offline RL [3, 52]. Finally,
we consider the independent learners extensions to [31] and [19],
respectively ICQL and ITD3+BC, as well as the state-of-the-art
model-free offline MARL method, OMAR [43].

5.2

Offline Iterated Coordination Game

To illustrate the strategy agreement coordination challenge, we
propose an offline version of the Iterated Coordination Game [5]
presented in Figure 3 (a). Agents must pick the same direction
in order to succeed and we investigate three offline datasets of
interactions (see Table 1). In the most favorable dataset, data is
collected by coordinated agents that select the same option of going
right most of the time. In the less favorable setting, agent 1 goes
left most of the time while agent 2 is more likely to go right. In the
neutral setting, agents act uniformly. However, each dataset does
contain both coordinated and uncoordinated behaviors in which
agents simultaneously choose the same â€“ respectively, different â€“
directions.
It is therefore straightforward for a centralized critic to learn
that ğ‘„ (â†’, â†’) = ğ‘„ (â†, â†) = 1 while ğ‘„ (â†’, â†) = ğ‘„ (â†, â†’) = 0,
regardless of the specific dataset. Yet, decentralized actors remain
unaware of whether they should go left or right since both strategies
are equivalent and actors have no way of consistently picking one
over the other.
Table 1: Policies used to collect the datasets in the Iterated Coordination Game and resulting average scores of the datasets

favorable

ğ‘ƒ (ğ‘ 1 =â†’)

ğ‘ƒ (ğ‘ 2 =â†’)

Avg. Score

0.75

0.75

0.623

neutral

0.5

0.5

0.502

unfavorable

0.25

0.75

0.375

functions: all-observant (FO: every agent fully observes the environment), independent (PO: each agent only sees the target and the
velocity and angle of the joint it controls), and leader-only (PO: both
agents observes the two joints but only the red agent observes the
targetâ€™s position). Note that with PO no agent observes the target to
fingertip vector. Thus, in leader-only, only the leader can estimate
whether or not the fingertip matches the target. In independent, no
agent has enough information to estimate this, yet both observe
the target position. These tasks are very challenging and require
agents to agree on following a specific convention (either clockwise
or counter-clockwise arm bend) to reach a given target location
and get rewards.
Four-agent Ant. Similarly, we use a MAMuJoCo-like decomposition [45] of the D4RL [18] offline ant task to make it multi-agent:
each individual limb (composed of two joints) is controlled by a
different agent. For the offline datasets, we use the single-agent
D4RL datasets and consider two types of observation functions.
For fully observable tasks, every agent observes the whole robot:
torso observations (i.e., vertical position, orientation, angular and
translational velocities) and the observations of all the limbs (i.e.,
angle and angular velocity of each joint). For the partially observable tasks, agents only observe the limb they control, and the torso
observations are only made available to the yellow limb agent. PO
tasks are very challenging in this case because only the yellow
agent knows if the ant is moving in the correct direction and it
must therefore learn to â€œsteer" the whole robot.
Table 2 details all the datasetsâ€™ score distributions as well are the
reference agentsâ€™ returns â€“ expert and random â€“ that are used to
normalize performance. Note that ant datasets are from D4RL [18]
(single agent datasets that we split into multi-agent observations
and actions) while we generated the two-agent reacher mixture-ofexpert dataset using MAPPO.
Table 2: Normalized measures of datasetsâ€™ scores distributions and normalization performances.
Scores
Datasets

5.3

reacher

Offline MAMuJoCo

Building upon D4RL [18] and MAMuJoCo [45], we propose offline
multi-agent continuous control tasks with various datasets and full
or partial observability.
Two-agent Reacher with a mixture-of-expert dataset. To investigate strategy agreement in a more complex continuous control
setting, we propose a two-agent version of the Reacher environment as shown in Figure 3 (b). The offline dataset is collected as
follows: in the first stage, we train online MAPPO on the fully
observable two-agent Reacher task (every agent observes all the
joint angles and velocities as well as the target position â€“ in black
â€“ and the target to fingertip â€“ in green â€“ vector). Depending on
the seed of the run, teams converge to counter-clockwise (ğœƒ 2 â‰¥ 0
as in Figure 3 (b)) or clockwise (ğœƒ 2 â‰¤ 0) arm bends. Thus we can
build a mixture-of-expert dataset by combining equal proportions
of demonstrations from clockwise and counter-clockwise teams.
Finally, we explore the impact of Full Observability (FO) versus Partial Observability (PO) by considering three types of observation

ant

6

expert-mix

min

mean

median

max

expert

(%)

(%)

(%)

(%)

return

return

38.8

100.0

98.9

152.3

-4.237

-11.145

3879.7

-325.6

random

-3

6.4

7.2

10.3

medium

-4.8

80.2

95.1

107.2

full-replay

-22.4

72.0

77.8

134.3

expert

-32.8

117.4

129.4

142.5

random

RESULTS

Table 3: Teamsâ€™ performances on the Iterated Coordination Game. MOMA-PPO is the only decentralized execution
method to solve it for all the datasets.

IQL

MAIQL

IBC

MOMA-PPO

fav.

1. Â± 0.

1. Â± 0.

1. Â± 0.

1. Â± 0.

neutral

1. Â± 0.

0.9 Â± 0.1

0.55 Â± 0.11

1. Â± 0.

unfav.

1. Â± 0.

0. Â± 0.

0. Â± 0.

1. Â± 0.

Table 4: Teamsâ€™ performances on two-agent Reacher with mixture-of-experts dataset for different observation functions. Scores
are normalized with expert and random performances. (a) Independent learners fail on datasets that contain a mixture of
incompatible experts while MOMA-PPO (and to some extent MAIQL) are able to coordinate agents. (b) Current model-free
methods are unable to adapt agentsâ€™ behaviors while MOMA-PPO significantly outperforms the baseline across all settings.
(a) Two-agent Reacher, mixture-of-experts dataset and full/partial observability
Algorithms
Tasks
FO

IQL
1.07 Â± 0.01

CTDE

model-based (ours)
independent learners

CTDE

MAIQL

MATD3+BC

IBC

ITD3+BC

ICQL

IOMAR

MOMA-PPO

0.96 Â± 0.05

1.04 Â± 0.01

1.02 Â± 0.01

0.78 Â± 0.00

0.48 Â± 0.06

0.73 Â± 0.01

1.07 Â± 0.01

independent

0.92 Â± 0.04

0.59 Â± 0.03

0.76 Â± 0.04

0.30 Â± 0.11

0.46 Â± 0.04

0.45 Â± 0.02

0.95 Â± 0.06

leader-only

0.80 Â± 0.05

0.73 Â± 0.02

0.84 Â± 0.02

0.48 Â± 0.04

0.31 Â± 0.05

0.39 Â± 0.02

1.00 Â± 0.01

all-observant

PO

model-free
centralized

(b) Four-agent Ant, various datasets and full/partial observability
Algorithms
Tasks

FO

PO

model-free
centralized

CTDE

model-based (ours)
independent learners

CTDE

IQL

MAIQL

MAIQL-ft

IBC

ITD3+BC

ICQL

IOMAR

MOMA-PPO

ant-random

0.12 Â± 0.00

0.28 Â± 0.01

0.28 Â± 0.03

0.31 Â± 0.00

0.22 Â± 0.02

0.08 Â± 0.00

0.08 Â± 0.00

0.52 Â± 0.07

ant-medium

0.97 Â± 0.02

0.85 Â± 0.02

0.81 Â± 0.02

0.84 Â± 0.01

1.04 Â± 0.00

0.88 Â± 0.12

1.10 Â± 0.03

1.29 Â± 0.06

ant-full-replay

1.22 Â± 0.02

0.77 Â± 0.21

0.95 Â± 0.13

1.20 Â± 0.01

1.33 Â± 0.01

1.21 Â± 0.02

1.30 Â± 0.00

1.42 Â± 0.07

ant-expert

1.26 Â± 0.01

1.24 Â± 0.00

1.06 Â± 0.07

1.24 Â± 0.00

1.25 Â± 0.02

0.73 Â± 0.15

1.16 Â± 0.01

1.49 Â± 0.01

ant-random

0.31 Â± 0.00

0.34 Â± 0.04

0.31 Â± 0.00

0.31 Â± 0.00

0.17 Â± 0.02

0.21 Â± 0.02

0.42 Â± 0.05

ant-medium

0.14 Â± 0.02

0.11 Â± 0.01

0.17 Â± 0.01

0.22 Â± 0.05

0.09 Â± 0.02

0.06 Â± 0.01

0.54 Â± 0.19

ant-full-replay

0.18 Â± 0.02

-0.07 Â± 0.10

0.21 Â± 0.02

0.20 Â± 0.01

0.09 Â± 0.01

0.11 Â± 0.02

0.46 Â± 0.10

ant-expert

-0.16 Â± 0.01

-0.23 Â± 0.02

0.05 Â± 0.04

0.16 Â± 0.00

0.11 Â± 0.03

0.10 Â± 0.01

0.18 Â± 0.00

The experimental procedure such as hyperparameters, training
routine, and raw learning curves are detailed in Appendix B. All
algorithms are trained to convergence and we used 10 seeds for
the Iterated Coordination Game and 3 seeds for MAMuJoCo tasks.
Tables are normalized and report the mean evaluation performance
and the standard error of the mean across seeds. Evaluation is done
for 100 episodes using the greedy policies (no sampling).

partial observability1 . Again, CTDE methods â€“ particularly MAIQL
â€“ tend to fare better than independent learners. Interestingly, IBC
fares best among independent learners. Finally, our model-based
CTDE approach MOMA-PPO solves strategy agreement and performs
on par with centralized execution (IQL), a setting that sidesteps
the strategy agreement issue altogether. MOMA-PPO matches or
significantly outperforms all other baselines.

6.1

6.2

Strategy Agreement

Table 3 reports the results for the offline Iterated Coordination
Game and validates most of our intuitions about strategy agreement: the centralized execution (IQL) and model-based (MOMAPPO) approaches are able to coordinate agents regardless of the
datasets. On the other hand, independent BC agents imitate the
dataset behavior and therefore coordinate only if the dataset majorly demonstrates coordination. Surprisingly, the CTDE model-free
approach MAIQL is able to break symmetry and coordinate agents
in the neutral dataset. We hypothesize that small numerical errors
in the centralized value approximation have the team favor one
equivalent strategy over the other. Unfortunately, the conservatism
of model-free methods forces agents to stay close to the demonstrated behaviors and prevails over this brittle symmetry-breaking
mechanism in the unfavorable â€“ i.e., uncoordinated â€“ dataset.

Table 4 (a) confirms that these insights on strategy agreement
hold in the more complex two-agent Reacher environment: modelfree methods struggle with strategy agreement, especially under

Strategy Fine-Tuning

From Table 4 (b) one can investigate how the different offline methods cope with strategy finetuning. First, IQL performs on par with
the other model-free methods which suggests that centralized execution (single-agent) vs. decentralized execution (multi-agent) is
less a consideration for strategy fine-tuning than it is for strategy agreement. Yet, this also highlights that model-free methods
(even when centralized) are unable to perform strategy fine-tuning. Indeed, they are surpassed by our model-based method, MOMA-PPO.
This latter generates additional synthetic experiences that allow for
strategy fine-tuning in addition to strategy agreement. For modelfree methods, independent learners tend to outperform CTDE ones
(which echoes [43] observations). Additionally, comparing IQL with
MAIQL performance does highlight that offline multi-agent coordination is more challenging in varied datasets (i.e. medium or
full-replay that display both coordinated and uncoordinated behaviors) than in uniformly coordinated datasets (i.e. expert).
1 https://sites.google.com/view/moma-ppo shows independent learners converge to

incompatible conventions.

6.3

exploded after a few world-model steps. Also, the use of uncertainty penalty (ğœ†ğ‘” ) and adaptive rolloutsâ€™ length (ğ‘™ğœ– ) are necessary
to achieve satisfactory results. Finally, varying the rolloutâ€™s maximum length from 5 to 50 did not significantly impact performance.

1.4
1.2

return

1.0

2 https://sites.google.com/view/moma-ppo shows rollouts with and without â€œsteering"

behavior.

Aleatoric - w/ adaptive rollouts - lambda = 1
Aleatoric - w/ adaptive rollouts - lambda = 5
Aleatoric - w/o adaptive rollouts - lambda = 1
Aleatoric - w/o adaptive rollouts - lambda = 5
Epistemic (MOMA-PPO, ours)

0.8
0.6
0.4
0.2
0.0

0

20000

40000
60000
training step

80000

100000

Ant-full-replay-PO
Aleatoric - w/ adaptive rollouts - lambda = 1
Aleatoric - w/ adaptive rollouts - lambda = 5
Aleatoric - w/o adaptive rollouts - lambda = 1
Aleatoric - w/o adaptive rollouts - lambda = 5
Epistemic (MOMA-PPO, ours)

1.0
0.8
0.6
0.4
0.2
0.0

Ablations

We validate our design choice of using epistemic uncertainty over
aleatoric. To do so we replace MOMA-PPOâ€™s penalty with the
aleatoric uncertainty penalty of MOPO [67]. For a fair comparison, we followed [67]â€™s parameter search procedure (i.e., coefficient
values of 1 and 5). Additionally, we investigated aleatoric penalty
both with and without the use of adaptive rollout length (which is
based on epistemic uncertainty). The resulting learning curves on
Ant-full-replay with full and partial observability are displayed in
Figure 4. It appears that using epistemic uncertainty always significantly outperforms using aleatoric uncertainty except in the case
of full observability and adaptive rollouts where the comparison
is not statistically significant. This validates the soundness of our
choice regarding the use of epistemic uncertainty over aleatoric
uncertainty in deterministic environments.
Appendix B.2 reports additional ablations, which main insights
we discuss here. First, using the ground-truth simulator yielded
higher scores suggesting that the performance of MOMA-PPO can
be further improved by learning a more accurate world model. Then,
not clipping the generated next-states to the datasetâ€™s bounding box
prevented generating length 10 rollouts as the statesâ€™ magnitude

Ant-full-replay-FO

1.6

return

In partially observable (PO) tasks, model-free methods are unable
to adapt the behaviors demonstrated in the datasets and they result
in teams that run in circles because the yellow agent (the only one
to observe the torsoâ€™s headings and velocities) fails to correct the
other limbsâ€™ motions. Conversely, with MOMA-PPO, the yellow
agent steers the ant toward the correct direction, and the teams
reach very satisfactory performances provided that the datasets
have enough coverage to learn a world model that can simulate
diverse and robust behaviors (cf. the lower performance for the
expert dataset).2
Finally, the poor performance of MAIQL-ft suggests that IQLâ€™s
finetuning abilities might not carry over to the multi-agent setting
(even though we used the ground-truth simulator to generate the
rollouts). While there might be multiple causes, we hypothesize that
it is mainly due to MAIQLâ€™s instability since it required intensive
hyperparameters finetuning and filtering out collapsed runs for ant
tasks. We believe that MAIQLâ€™s instability is exacerbated by the
induced non-stationarity of the training data when augmenting it
with online interactions. Unlike online methods, offline algorithms
are designed to learn on fixed datasets and are thus ill-equipped to
deal with data continually collected by changing policies. Similarly,
experiments that used MAIQL instead of MAPPO for MOMA (i.e.
MOMA-IQL) quickly led to unstable learning and exploding losses.
In conclusion, our results show that current model-free offline
MARL methods fail at offline coordination. Crucially, this deficiency
remains even in very simple domains such as the Iterated Coordination Game. Also, it sometimes leads to underperforming basic imitation learning (i.e., IBC) or the datasetsâ€™ average score (see Table 2).
Conversely, our model-based approach is able to coordinate agents
even under severe partial observability and with learned world
models by restoring inter-agent interactions throughout learning.

0.2
0.4

0

20000

40000
60000
training step

80000

100000

Figure 4: Comparision between using epistemic uncertainty
reward penalty (MOMA-PPO) vs. aleatoric uncertainty reward penalty (MOPO-like). Mean and standard error of the
mean on three seeds.

7

DISCUSSION AND CONCLUSION

This work explores coordination in offline MARL and highlights the
failures of current model-free methods. For instance, they struggle
in the presence of multiple equivalent but incompatible optimal
team strategies (strategy agreement), or when partial observability requires the team to adapt the behaviors demonstrated in the
dataset (strategy fine-tuning). In such scenarios, prevalent modelfree methods might even fail to match the datasetâ€™s performance
and fall short compared to behavioral cloning. To address these
problems, we propose MOMA-PPO which is, to our knowledge, the
first model-based offline MARL approach. Our method is able to
coordinate teams of offline learners and significantly outperforms
model-free alternatives. Interestingly, it also outperforms the fully

centralized model-free method IQL [30] even though this latter completely bypasses the strategy agreement problem. This suggests
that model-free methods, even when fully centralized, are unable to
deal with strategy fine-tuning. We also observe in our experiments
that the single-agent approach of fine-tuning offline methods (i.e.
IQL) with online interactions [30] fails in the multi-agent setting
(i.e. MAIQL). Our model-based approach succeeds at both multiagent strategy agreement and strategy fine-tuning problems which
goes to show that the benefits of offline model-based approaches
over offline model-free ones [67] hold in the multi-agent setting.
This might indicate that world models may generalize better than
values networks in offline learningâ€™s limited data regime. Finally, for
tasks that require adapting the teamâ€™s behavior, dataset coverage
might be more desirable than demonstrated performance. Indeed,
methods fare better on random datasets than on expert ones.
MOMA-PPOâ€™s successes put forward the benefits of model-based
methods that leverage online policy optimization. Nevertheless,
our approach of coordinating agents through a world model is general and some might be interested in extending it to more sampleefficient policy learning algorithms (i.e., MOMA-SAC). Also, the
dataset bounding box clipping and adaptive rollouts developed for
MOMA-PPO might benefit the single-agent setting.
Finally, this work aims to pinpoint an overlooked issue in the
offline MARL community and proposes a new avenue of modelbased solutions that shifts away from conventional model-free
approaches. Therefore, we look forward to future work that will
further analyze model-based offline MARL approaches and scale
them to more domains and datasets.

ACKNOWLEDGMENTS
The authors would like to thank Luis Pineda, Hengyuan Hu, and
Eugene Vinitsky for their help, insightful discussions, and advice.
This research was enabled in part by support provided by Calcul
QuÃ©bec and the Digital Research Alliance of Canada. This work
was partially conducted while Paul was interning at Meta AI - FAIR.

REFERENCES
[1] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International
Conference on Learning Representations, ICLR 2015.
[2] Paul Barde, Tristan Karch, Derek Nowrouzezahrai, ClÃ©ment Moulin-Frier, Christopher Pal, and Pierre-Yves Oudeyer. 2022. Learning to Guide and to be Guided in
the Architect-Builder Problem. In International Conference on Learning Representations. https://openreview.net/forum?id=swiyAeGzFhQ
[3] Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Chris Pal, and Derek
Nowrouzezahrai. 2020. Adversarial soft advantage fitting: Imitation learning
without policy optimization. Advances in Neural Information Processing Systems
33 (2020), 12334â€“12344.
[4] Samuel Barrett, Peter Stone, and Sarit Kraus. 2011. Empirical evaluation of ad
hoc teamwork in the pursuit domain. In The 10th International Conference on
Autonomous Agents and Multiagent Systems-Volume 2. 567â€“574.
[5] Craig Boutilier. 1996. Planning, learning and coordination in multiagent decision
processes. In TARK, Vol. 96. Citeseer, 195â€“210.
[6] Justin Boyan and Michael Littman. 1993. Packet routing in dynamically changing
networks: A reinforcement learning approach. Advances in neural information
processing systems 6 (1993).
[7] Ronen Brafman and Moshe Tennenholtz. 2002. Efficient learning equilibrium.
Advances in Neural Information Processing Systems 15 (2002).
[8] Wilfried Brauer and Gerhard WeiÃŸ. 1998. Multi-machine scheduling-a multiagent learning approach. In Proceedings International Conference on Multi Agent
Systems (Cat. No. 98EX160). IEEE, 42â€“48.
[9] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel,
and Anca Dragan. 2019. On the utility of learning about humans for human-ai
coordination. Advances in neural information processing systems 32 (2019).

[10] Lars-Erik Cederman. 1997. Emergent actors in world politics: how states and nations
develop and dissolve. Vol. 2. Princeton University Press.
[11] Georgios Chalkiadakis and Craig Boutilier. 2003. Coordination in multiagent
reinforcement learning: A Bayesian approach. In Proceedings of the second international joint conference on Autonomous agents and multiagent systems. 709â€“716.
[12] Caroline Claus and Craig Boutilier. 1998. The dynamics of reinforcement learning
in cooperative multiagent systems. AAAI/IAAI 1998, 746-752 (1998), 2.
[13] Vincent P Crawford and Joel Sobel. 1982. Strategic information transmission.
Econometrica: Journal of the Econometric Society (1982), 1431â€“1451.
[14] Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. 2020. Is independent
learning all you need in the starcraft multi-agent challenge? arXiv preprint
arXiv:2011.09533 (2020).
[15] Kurt Dresner and Peter Stone. 2004. Multiagent traffic management: A
reservation-based intersection control mechanism. In Autonomous Agents and
Multiagent Systems, International Joint Conference on, Vol. 3. Citeseer, 530â€“537.
[16] K Fischer, N Kuhn, HJ Muller, JP Muller, and M Pischel. 1993. Sophisticated and
distributed: The transportation domain. In Proceedings of 9th IEEE Conference on
Artificial Intelligence for Applications. IEEE, 454.
[17] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 32.
[18] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. 2020.
D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint
arXiv:2004.07219 (2020).
[19] Scott Fujimoto and Shixiang Shane Gu. 2021. A minimalist approach to offline
reinforcement learning. Advances in neural information processing systems 34
(2021), 20132â€“20145.
[20] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep reinforcement learning without exploration. In International conference on machine
learning. PMLR, 2052â€“2062.
[21] Stephen Grand, Dave Cliff, and Anil Malhotra. 1997. Creatures: Artificial life
autonomous software agents for home entertainment. In Proceedings of the first
international conference on Autonomous agents. 22â€“29.
[22] David Ha and JÃ¼rgen Schmidhuber. 2018. World models. arXiv preprint
arXiv:1803.10122 (2018).
[23] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In International conference on machine learning. PMLR, 1861â€“
1870.
[24] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. 2020. â€œOtherPlayâ€ for Zero-Shot Coordination. In International Conference on Machine Learning. PMLR, 4399â€“4410.
[25] Jun Huang, Nicholas R Jennings, and John Fox. 1995. An agent architecture
for distributed medical care. In Intelligent Agents: ECAI-94 Workshop on Agent
Theories, Architectures, and Languages Amsterdam, The Netherlands August 8â€“9,
1994 Proceedings 1. Springer, 219â€“232.
[26] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro
Ortega, Dj Strouse, Joel Z Leibo, and Nando De Freitas. 2019. Social Influence as
Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning. In International Conference on Machine Learning. 3040â€“3049.
[27] Jiechuan Jiang and Zongqing Lu. 2021. Offline decentralized multi-agent reinforcement learning. arXiv preprint arXiv:2108.01832 (2021).
[28] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
2020. Morel: Model-based offline reinforcement learning. Advances in neural
information processing systems 33 (2020), 21810â€“21823.
[29] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. International Conference on Learning Representations (2015).
[30] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. 2021. Offline Reinforcement
Learning with Implicit Q-Learning. arxiv (2021).
[31] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conservative q-learning for offline reinforcement learning. Advances in Neural Information
Processing Systems 33 (2020), 1179â€“1191.
[32] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. 2017. MultiAgent Cooperation and the Emergence of (Natural) Language. In International
Conference on Learning Representations. https://openreview.net/forum?id=
Hk8N3Sclg
[33] Adam Lerer and Alexander Peysakhovich. 2019. Learning existing social conventions via observationally augmented self-play. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society. 107â€“114.
[34] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv
preprint arXiv:2005.01643 (2020).
[35] Jiaoyang Li, Zhe Chen, Yi Zheng, Shao-Hung Chan, Daniel Harabor, Peter J
Stuckey, Hang Ma, and Sven Koenig. 2021. Scalable rail planning and replanning:
Winning the 2020 flatland challenge. In Proceedings of the International Conference
on Automated Planning and Scheduling, Vol. 31. 477â€“485.

[36] Michael L Littman et al. 2001. Friend-or-foe Q-learning in general-sum games.
In ICML, Vol. 1. 322â€“328.
[37] Haotian Liu and Wenchuan Wu. 2021. Online multi-agent reinforcement learning
for decentralized inverter-based volt-var control. IEEE Transactions on Smart
Grid 12, 4 (2021), 2980â€“2990.
[38] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
2017. Multi-agent actor-critic for mixed cooperative-competitive environments.
In Advances in Neural Information Processing Systems. 6379â€“6390.
[39] Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. 2021. Contrasting centralized and decentralized critics in multi-agent reinforcement learning.
arXiv preprint arXiv:2102.04402 (2021).
[40] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. 2019.
Maven: Multi-agent variational exploration. Advances in Neural Information
Processing Systems 32 (2019).
[41] Ranjit Nair, Milind Tambe, Makoto Yokoo, David Pynadath, and Stacy Marsella.
2003. Taming decentralized POMDPs: Towards efficient policy computation for
multiagent settings. In IJCAI, Vol. 3. Citeseer, 705â€“711.
[42] Praveen Palanisamy. 2020. Multi-agent connected autonomous driving using
deep reinforcement learning. In 2020 International Joint Conference on Neural
Networks (IJCNN). IEEE, 1â€“7.
[43] Ling Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. 2022. Plan better amid
conservatism: Offline multi-agent reinforcement learning with actor rectification.
In International Conference on Machine Learning. PMLR, 17221â€“17237.
[44] Liviu Panait and Sean Luke. 2005. Cooperative multi-agent learning: The state of
the art. Autonomous agents and multi-agent systems 11, 3 (2005), 387â€“434.
[45] Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin BÃ¶hmer, and Shimon Whiteson. 2021. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information
Processing Systems 34 (2021), 12208â€“12221.
[46] Dean A Pomerleau. 1991. Efficient training of artificial neural networks for
autonomous navigation. Neural computation 3, 1 (1991), 88â€“97.
[47] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
Foerster, and Shimon Whiteson. 2018. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference
on Machine Learning. PMLR, 4295â€“4304.
[48] Julien Roy, Paul Barde, FÃ©lix Harvey, Derek Nowrouzezahrai, and Chris Pal.
2020. Promoting coordination through policy regularization in multi-agent deep
reinforcement learning. Advances in Neural Information Processing Systems 33
(2020), 15774â€“15785.
[49] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
2015. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 (2015).
[50] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[51] Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. 2021. Offline Reinforcement
Learning for Autonomous Driving with Safety and Exploration Enhancement.
arXiv preprint arXiv:2110.07067 (2021).
[52] Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and
J Andrew Bagnell. 2021. Feedback in imitation learning: The three regimes of
covariate shift. arXiv preprint arXiv:2102.02872 (2021).
[53] Randall Steeb, Stephanie Cammarata, Frederick A Hayes-Roth, Perry W
Thorndyke, and Robert E Wesson. 1981. Distributed intelligence for air fleet
control. Technical Report. RAND CORP SANTA MONICA CA.
[54] Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein. 2010. Ad hoc
autonomous agent teams: Collaboration without pre-coordination. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 24. 1504â€“1509.
[55] Richard S Sutton. 1990. Integrated architectures for learning, planning, and
reacting based on approximating dynamic programming. In Machine learning
proceedings 1990. Elsevier, 216â€“224.
[56] Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola.
2022. Offline Multi-Agent Reinforcement Learning with Knowledge Distillation.
Advances in Neural Information Processing Systems 35 (2022), 226â€“237.
[57] Mycal Tucker, Yilun Zhou, and Julie Shah. 2020. Adversarially guided self-play
for adopting social conventions. arXiv preprint arXiv:2001.05994 (2020).
[58] LÃ¡szlÃ³Z Varga, Nick R Jennings, and David Cockburn. 1994. Integrating intelligent
systems into a cooperating community for electricity distribution management.
Expert Systems with Applications 7, 4 (1994), 563â€“579.
[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[60] Eugene Vinitsky, Nathan LichtlÃ©, Xiaomeng Yang, Brandon Amos, and Jakob
Foerster. 2022. Nocturne: a scalable driving benchmark for bringing multi-agent
learning one step closer to the real world. arXiv preprint arXiv:2206.09889 (2022).
[61] Jianhao Wang, Wenzhe Li, Haozhe Jiang, Guangxiang Zhu, Siyuan Li, and
Chongjie Zhang. 2021. Offline reinforcement learning with reverse model-based
imagination. Advances in Neural Information Processing Systems 34 (2021), 29420â€“
29432.

[62] DaniÃ«l Willemsen, Mario Coppola, and Guido CHE de Croon. 2021. MAMBPO:
Sample-efficient multi-robot reinforcement learning using learned world models.
In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
IEEE, 5635â€“5640.
[63] Mark Woodward, Chelsea Finn, and Karol Hausman. 2020. Learning to interactively learn and assist. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 34. 2535â€“2543.
[64] Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang,
Jun Yang, and Qianchuan Zhao. 2021. Believe What You See: Implicit Constraint
Approach for Offline Multi-Agent Reinforcement Learning. NeurIPS (2021).
[65] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen,
and Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent
games. Advances in Neural Information Processing Systems 35 (2022), 24611â€“24624.
[66] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and
Chelsea Finn. 2021. Combo: Conservative offline model-based policy optimization.
Advances in neural information processing systems 34 (2021), 28954â€“28967.
[67] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey
Levine, Chelsea Finn, and Tengyu Ma. 2020. Mopo: Model-based offline policy
optimization. Advances in Neural Information Processing Systems 33 (2020), 14129â€“
14142.
[68] Chi Zhang, Sanmukh R Kuppannagari, Chuanxiu Xiong, Rajgopal Kannan, and
Viktor K Prasanna. 2019. A cooperative multi-agent deep reinforcement learning framework for real-time residential load scheduling. In Proceedings of the
International Conference on Internet of Things Design and Implementation. 59â€“69.
[69] Chongjie Zhang and Victor Lesser. 2013. Coordinating multi-agent reinforcement
learning with limited communication. In Proceedings of the 2013 international
conference on Autonomous agents and multi-agent systems. 1101â€“1108.
[70] Qizhen Zhang, Chris Lu, Animesh Garg, and Jakob Foerster. 2022. Centralized
Model and Exploration Policy for Multi-Agent RL. In Proceedings of the 21st
International Conference on Autonomous Agents and Multiagent Systems (Virtual
Event, New Zealand) (AAMAS â€™22). International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC, 1500â€“1508.
[71] Weinan Zhang, Xihuai Wang, Jian Shen, and Ming Zhou. 2021. Model-based Multiagent Policy Optimization with Adaptive Opponent-wise Rollouts. International
Joint Conference on Artificial Intelligence (2021).

A

REPRODUCIBILITY DETAILS

The following section focuses on reproducibility and goes into detail about the implementations and experimental procedures.

A.1

Methods implementation

MOMA-PPO entropy bonus and action penalty. For offline methods based on online RL algorithm, exploration is an important
component (cf. TD3â€™s exploration strategy) so we used an entropy bonus for PPO defined as:
#
#
#
#
#
#

entropy bonus
dimensions are batch , act_dim , n_agents .
Instead of using closed form entropy ,
we estimate it with E ( - pi log pi ) were
the expectation is sampled over pi_old
so we have to correct with pi_new / pi_old ( which is ratio !)

# we do two losses
# ( clipped / not clipped like with the actor loss )
surrogate_entropy = - ( ratio * new_policy ) . mean ( 0 )
clipped_entropy = - ( clipped_ratio * new_policy ) . mean ( 0 )
entropy = torch . min ( surrogate_entropy , clipped_entropy )
self . entropy_alpha + = self . ppo_entropy_bonus_coeff * ( self . ppo_entropy_target - entropy ) . detach ()
self . entropy_alpha . data . clamp_min_ ( 0 .)
# minus sign because we minimize the expressions
# i.e. max ( ent ) = min ( - ent )
entropy_bonus = - entropy * self . entropy_alpha

with an entropy bonus coefficient of 0.001 and an entropy target of -6 and -4 for respectively Ant and Reacher tasks. Since entropy computation
can become numerically unstable for squashed actions close to the Tanh bounds, PPO uses action penalty instead of Tanh squashing to keep
actions close to the -1, 1 range:
delta = (1. - actions . abs () )
action_bound_error = (( delta < 0 .) . to ( torch . float32 ) * delta ** 2 ) . sum (1 , keepdim = True )
surrogate_action_bound_error = ( ratio * action_bound_error ) . mean ( 0 )
clipped_action_bound_error = ( clipped_ratio * action_bound_error ) . mean ( 0 )
action_bound_error = torch . max ( surrogate_action_bound_error , clipped_action_bound_error )
action_penalty = self . ppo_action_penalty_coeff * action_bound_error

with an action penalty coefficient of 1.
General Advantage Estimation. We show below how we modified the general advantage estimation to account for rollout termination
(indicated by time_out_masks).
# initial ( end ) running returns is the next state value
running_returns = next_values [ - 1 ] * masks [ - 1 ]
# initial ( end ) advantage is 0 because no difference in value and return
running_advants = 0
for t in reversed ( range (0 , len ( rewards ) ) ) :
# We are going reverse so if done , only reward because end of
# episode if timeout , we stop accumulation and use value as
# bootstrap like in initialization
running_returns = rewards [ t ] + self . discount * masks [ t ] * ( running_returns * time_out_masks [ t ] + ( 1 . time_out_masks [ t ] ) * next_values [ t ] * masks [ t ])
returns [t] = running_returns
## No accumulation here and timeout doesn 't influence next_state value
running_delta = rewards [ t ] + ( self . discount * next_values [ t ] * masks [ t ] ) - values [ t ]
## if timeout running_advants goes back to running_delta because
# we do not have extra rewards to estimate it
# ( cf initialization above )
running_advants = running_delta + ( self . discount * self . lamda * running_advants * masks [ t ] ) * time_out_masks [t]
advants [t] = running_advants

Memory module for partial observability. To handle partial observability we use observation-action histories â„ğ‘–ğ‘¡ of size ten (i.e. the
ten past observation-action pairs). These histories are processed with self-attention followed by soft-attention to yield embeddings of size
ğ‘’â„ = 128 that are concatenated to the current state before being fed to the policy and value networks. We use a first linear layer to encode
â„ğ‘–ğ‘¡ to Rğ‘’â„ and add positional encodings [59]. Query, Key, and Value networks are linear layers and we follow [59]â€™s Scaled Dot-Product
Attention with skip connection and layer-norm. Finally, the resulting self-attentions are aggregated using soft-attention with the soft-key
network being a bias-less linear layer and the soft-queries are ğ‘’â„ normally initialized trainable parameters.
Note that policy and value networks use (and backprop through) the same memory module but target networks have their own target
memory module (that tracks the memory module with Polyak updates just like regular target networks). The memory learning rate is 1e âˆ’4
for all the algorithms.
MAIQL.. We provide IQL [30] learning rules here for completeness. Value learning is done with SARSA Bellman on ğ‘’ expectile of ğ‘„ instead
of mean ğ‘„ (this latter is a special case where ğ‘’ = 0.5). To ensure that the expectile is computed only from the action selection distribution
and is not influenced by the randomness of the environmentâ€™s transitions, IQL uses a state-only value function ğ‘‰ that marginalizes over
future transitions:
h
i
ğ¿ğ‘‰ (ğœ“ ) = Eğ‘ ,ğ‘âˆ¼D L2ğ‘’ (ğ‘„ğœƒË† (ğ‘ , ğ‘) âˆ’ ğ‘‰ğœ“ (ğ‘ ))

(1)

 
2
= Eğ‘ ,ğ‘âˆ¼D |ğ‘’ âˆ’ I ğ‘„ğœƒË† (ğ‘ , ğ‘) âˆ’ ğ‘‰ğœ“ (ğ‘ ) < 0 | ğ‘„ğœƒË† (ğ‘ , ğ‘) âˆ’ ğ‘‰ğœ“ (ğ‘ )
,

2
ğ‘Ÿ (ğ‘ , ğ‘) + ğ›¾ğ‘‰ğœ“ (ğ‘  â€² ) âˆ’ ğ‘„ğœƒ (ğ‘ , ğ‘)
.

ğ¿ğ‘„ (ğœƒ ) = Eğ‘ ,ğ‘,ğ‘Ÿ,ğ‘  â€² âˆ¼D

Policy extraction is done with Advantage Weighted Regression (AWR):
h
 

i
ğ¿ğœ‹ (ğœ™) = E (ğ‘ ,ğ‘)âˆ¼D âˆ’ exp ğ›½ ğ‘„ğœƒË† (ğ‘ , ğ‘) âˆ’ ğ‘‰ğœ“ (ğ‘ ) log ğœ‹ğœ™ (ğ‘|ğ‘ ) .

(2)

(3)

We make IQL multi-agent (i.e. MAIQL) by leveraging the CTDE formalism and using QMIX value-decomposition [47] for both ğ‘„ and ğ‘‰ :
âˆ‘ï¸
ğ‘‰ğœ“ (ğ‘ ) =
ğ‘¤ğ‘‰ğ‘– (ğ‘ )ğ‘‰ğœ“ ğ‘– (â„ğ‘– ) + ğ‘ğ‘‰ (ğ‘ ),
ğ‘–

ğ‘„ğœƒ (ğ‘ ) =

âˆ‘ï¸

ğ‘–
ğ‘¤ğ‘„
(ğ‘ )ğ‘„ğœƒ ğ‘– (â„ğ‘– ) + ğ‘ğ‘„ (ğ‘ ).

(4)

ğ‘–

And the target network:
ğ‘„ğœƒË† (ğ‘ ) =

âˆ‘ï¸

ğ‘–
ğ‘¤Ë† ğ‘„
(ğ‘ )ğ‘„ğœƒË†ğ‘– (ğ‘  ğ‘– ) + ğ‘Ë†ğ‘„ (ğ‘ ).

(5)

ğ‘–

Similarly, the joint policy is assumed to decompose as:
ğœ‹ (ğ‘|ğ‘ ) â‰œ

Ã–

ğœ‹ğ‘– (ğ‘ğ‘– |â„ğ‘– ).

(6)

ğ‘–

Injecting this into Eq. 3 one gets:
ï£®
ï£¹

 âˆ‘ï¸
Ã–
ï£¯
ï£º
ğ‘–
ğ¿ğœ‹ (ğœ™) = E (ğ‘ ,ğ‘)âˆ¼D ï£¯ï£¯âˆ’ğµ(ğ‘ )
exp ğ›½ (ğ‘¤Ë† ğ‘„
(ğ‘ )ğ‘„ğœƒË†ğ‘– (â„ğ‘– , ğ‘ğ‘– ) âˆ’ ğ‘¤ğ‘‰ğ‘– (ğ‘ )ğ‘‰ğœ“ ğ‘– (â„ğ‘– ))
log ğœ‹ğœ™ ğ‘— (ğ‘ ğ‘— |â„ ğ‘— ) ï£ºï£º ,
ï£¯
ï£º
ğ‘–
ğ‘—
ï£°
ï£»
ğµ(ğ‘ ) = exp(ğ‘Ë†ğ‘„ (ğ‘ ) âˆ’ ğ‘ğ‘‰ (ğ‘ )).

(7)

Finetuning MAIQL: MAIQL-ft. We follow [30]â€™s finetuning procedure and use the ground truth simulator to generate the rollout.
However, the rollouts are still generated using MOMAâ€™s Dyna-like approach described in Section 3 rather than generating length 1000
rollouts from the initial state distribution. Indeed, we consider the model-based offline setting and not the offline-to-online finetuning setting.
Therefore it is unfeasible to assume that the taskâ€™s ground-truth initial state distribution is known or that it is possible to learn a world
model that remains accurate over 1000 simulation steps.
Opensource baselines. For the baseline implementations, we followed the official repositories at:
â€¢ https://github.com/sfujim/TD3_BC,
â€¢ https://github.com/ling-pan/OMAR/,
â€¢ and https://github.com/ikostrikov/implicit_q_learning.

A.2

Hyperparameters, Tuning, and Training

Unless specified otherwise, networks are two-layers 256 units ReLu MLPs. We use Adam optimizer [29] with default hyperparameters (except
learning rates) and a batch size of 256. We clip gradient norms to 1.
World model learning. World models use four layers and 1024 units. World models use a learning rate of 3e âˆ’5 and are trained for 3e6
steps.
Policy learning. MAPPO learning rate was finetuned on MAMUJOCO online task
halfcheetah-v2_2x3_full with a grid search across [1e âˆ’6, 5e âˆ’6, 1e âˆ’5, 5e âˆ’5, 1e âˆ’4, 5e âˆ’4, 1e âˆ’3 ]. We kept the value of 5e âˆ’5 for MOMA-PPO
for all experiments. PPO uses rollouts length of 1000, 5 epochs per update, 2000 transitions between updates, a clip value of 0.2, a ğœ† value of
0.98, and a critic loss coefficient of 0.5.
MAIQL showed quite unstable and we had to finetune its learning rate extensively depending on the tasks. We tried the range
[1e âˆ’4, 3e âˆ’4, 1e âˆ’5, 5e âˆ’5, 1e âˆ’6 ] on ant-expert for MAIQL and MAIQL-ft and kept 3e âˆ’4 . For the results on ant-expert with full observability, we
had to discard one collapsed unstable seed for MAIQL and retrain another seed. For reacher tasks we tried [5e âˆ’5, 1e âˆ’4, 3e âˆ’4, 5e âˆ’4, 1e âˆ’3 ] and
kept 3e âˆ’4 . IQL and MAIQL use an expectile value of 0.7 and an AWR temperature of 3.
All other methods use their default learning rate of 3e âˆ’4 . ITD3+BC uses a BC regularization parameter of 2.5, a policy frequency update
of 2, a policy noise of 0.2, and a noise clip value of 0.5. ICQL uses a coefficient of 1 (and so does IOMARâ€™s CQL component) for all tasks
except for ant-expert tasks where we had to tune it between [0.1, 0.5, 1, 5] and kept a value of 5 (same for IOMAR). Additionally, CQL uses an
LSE temperature of 1, 10 sampled actions, and a sample noise level of 0.2. Finally, IOMAR uses a coefficient of 1, 2 iterations, a ğœ‡ value of 0, a
ğœ value of 2, 20 samples, and 5 elites.
We use default values for other hyperparameters and we report them here for consistency. We use a discount factor of 0.99 and a target
update coefficient of 0.005 for the Polyak averaging. TD3 (and thus CQL and OMAR) policies are deterministic with Tanh squashing while
IQL and PPO use Gaussians with state-dependent variance. IQL uses Tanh squashing while PPO does not (see below).
Model-free methods are trained for 1e6 learning steps while MOMA-PPO is trained for 1e5 learning steps which correspond to roughly
8
2e collected interactions from the world model.

A.3

Compute

Our longest MOMA-PPO training took 6 days on a Tesla P100-PCIE-12GB GPU. For comparison our longest IOMAR run took 38 hours
on a Tesla V100-SXM2-32GB GPU and our longest MAIQL run took 40 hours on a Tesla V100-SXM2-32GB GPU and five days on a Tesla
P100-PCIE-12GB GPU for MAIQL-ft. Training a world model took at most three days on a Tesla P100-PCIE-12GB GPU. Note that training
times are also impacted by how often we estimate performance for the learning curves and we do it much more often for MOMA-PPO (5
episodes every 50 learning steps) than for other methods (10 episodes every 5000 learning steps).

B

RAW RESULTS

This section focuses on transparency and provides the raw results of the experiments.

B.1

Learning Curves
Reacher-mix-FO
1.0

return

0.5
0.0
IQL
MAIQL
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.5
1.0
1.5

0.0

0.2

0.4
0.6
training step

0.8

1.0
1e6

Reacher-mix-independent
1.00
0.75

return

0.50
0.25
0.00

MAIQL
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.25
0.50
0.75

0.0

0.2

0.4
0.6
training step

0.8

1.0
1e6

Reacher-mix-leader-only
1.00
0.75

return

0.50
0.25
0.00
0.25

MAIQL
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.50
0.75
0.0

0.2

0.4
0.6
training step

0.8

1.0
1e6

Figure 5: Learning Curves for two-agent Reacher. Mean and standard error of the mean on three seeds.
Figues 5 and 6 show the learning curves for the Reacher and Ant environments respectively. We use 5 episodes to evaluate MOMA-PPO
every 50 learning steps and 10 episodes every 5000 learning steps to evaluate the other methods (this is why MOMA-PPO curves might
look noisier). We used a smoothing factor of .8 for all curves. We report the mean over three seeds and the shaded area represents Â± the
standard error of the mean. We train MOMA-PPO for 1e5 training steps because it is on-policy while the off-policy methods are trained for
1e6 training steps. MAIQL-ft training is 2e6 steps (half offline and half online fine-tuning).

Ant-random-FO

0.6

return

0.5
0.4

Ant-random-PO
0.45
0.40
0.35
return

IQL
MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.7

0.30

0.3

0.25

0.2

0.20

0.1

MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.15

0.00

0.25

0.50

0.75 1.00 1.25
training step

1.50

1.75

2.00
1e6

0.00

0.25

0.50

Ant-medium-FO

0.6

1.0

0.4

0.8

IQL
MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.6
0.4
0.2
0.00

0.25

0.50

0.75 1.00 1.25
training step

1.50

1.75

return

return

1.2

0.2

0.2

2.00
1e6

0.00

0.25

0.50

0.75 1.00 1.25
training step

1.50

0.8

IQL
MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.6
0.4
0.2
1.50

1.75

return

return

0.4

1.0

0.75 1.00 1.25
training step

0.0
0.2

2.00
1e6

0.00

0.25

0.50

0.75 1.00 1.25
training step

1.50

0.2

1.0

0.1

0.8

IQL
MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.6
0.4
0.2
0.25

0.50

0.75 1.00 1.25
training step

1.50

1.75

2.00
1e6

return

1.2

0.00

1.75

2.00
1e6

Ant-expert-PO
MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

0.3

1.4

return

0.2

Ant-expert-FO

1.6

2.00
1e6

MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

1.2

0.50

1.75

Ant-full-replay-PO
0.6

0.25

2.00
1e6

0.0

1.4

0.00

1.75

MAIQL
MAIQL-ft
IBC
ITD3+BC
ICQL
IOMAR
MOMA-PPO

Ant-full-replay-FO

1.6

1.50

Ant-medium-PO

0.8

1.4

0.75 1.00 1.25
training step

0.0
0.1
0.2
0.00

0.25

0.50

0.75 1.00 1.25
training step

1.50

1.75

2.00
1e6

Figure 6: Learning curves for four-agent Ant. Mean and standard error of the mean on three seeds.

B.2

Ablations

Ant-expert-FO
1.75

0.5

1.25
1.00

return

return

MOMA-PPO
MOMA-PPO-gt

0.6

1.50

0.75

0.4
0.3
0.2

0.50
0.25
0.00

Ant-expert-PO

0.7

0.1

MOMA-PPO
MOMA-PPO-gt
0.00

0.25

0.50

0.75 1.00 1.25
world model step

1.50

1.75

0.0

2.00
1e8

0.00

0.25

0.50

0.75 1.00 1.25
world model step

1.50

1.75

2.00
1e8

Figure 7: Impact of using the ground truth world model vs. the learned world model. â€œgt" stands for â€œground truth" and means
that the corresponding runs use the ground truth world model. The learning curves are with respect to generated transitions
(either from the ground truth or the learned world model). Here we show each individual run instead of the usual mean and
standard error of the mean.

Table 5: Mean scores and standard error of the mean at the end of training with and without the use of the ground truth
simulator. Evaluations are over 100 episodes.
MOMA-PPO

(FO)

(PO)

MOMA-PPO-gt

ant-random

0.52 Â± 0.07

1.17 Â± 0.03

ant-medium

1.29 Â± 0.06

1.68 Â± 0.03

ant-full-replay

1.42 Â± 0.07

1.66 Â± 0.03

ant-expert

1.49 Â± 0.01

1.71 Â± 0.04

ant-random

0.42 Â± 0.05

0.47 Â± 0.01

ant-medium

0.54 Â± 0.19

0.81 Â± 0.20

ant-full-replay

0.46 Â± 0.10

0.84 Â± 0.07

ant-expert

0.18 Â± 0.00

0.43 Â± 0.06

Figure 7 and Table 5 compare using a learned world model with having access to the ground truth simulator to generate the rollouts. It
appears that MOMA-PPO performance can be further improved provided that we learn better world models.

Ant-full-replay-FO

Ant-full-replay-PO
1.0

1.5

MOMA-PPO-epsilon_g
MOMA-PPO-epsilon_r
MOMA-PPO-rollouts
MOMA-PPO-gt
MOMA-PPO

0.8
0.6
return

return

1.0
0.5

0.4
0.2

MOMA-PPO-epsilon_g
MOMA-PPO-epsilon_r
MOMA-PPO-rollouts
MOMA-PPO-gt
MOMA-PPO

0.0
0

20000

40000
60000
training step

80000

0.0
0.2

100000

0

20000

40000
60000
training step

80000

100000

Figure 8: Impact of the different uncertainty-based techniques to prevent model exploitation by the learning algorithm. â€œgt"
stands for â€œground truth" and means that the corresponding runs use the ground truth world model. â€œepsilon_g" and â€œepsilon_r"
respectively means that we set ğœ†ğ‘” = 0 and ğœ†ğ‘Ÿ = 0. â€œrollouts" means that we removed the adaptive rollouts component (i.e. ğ‘™ğœ– = âˆ).
Mean and standard error of the mean on three seeds.
Figure 8 shows ablations on MOMA-PPO. It appears that ğœ†ğ‘Ÿ penalty can be removed without hurting performance, indeed it is already
encapsulated in ğœ†ğ‘” . The other components of MOMA-PPO such as ğœ†ğ‘” uncertainty penalty on the reward, or the adaptive rollouts that
terminate if the uncertainty crosses a threshold, are mandatory to reach satisfactory performance. Note that running experiments without
clipping the world model predictions to the datasetsâ€™ bounding box was impossible as the magnitude of the predicted state exploded after
a few rollout steps. This could be mitigated by reducing the exploration of the PPO policies so that the agents stay closer to the dataset
distribution where the world model does not predict such extreme states.

Ant-full-replay-FO

Ant-full-replay-PO
1.0

1.5

MOMA-PPO-5
MOMA-PPO-10
MOMA-PPO-25
MOMA-PPO-50
MOMA-PPO-gt-10

0.8

1.0
return

return

0.6

0.5
MOMA-PPO-5
MOMA-PPO-10
MOMA-PPO-25
MOMA-PPO-50
MOMA-PPO-gt-10

0.0
0

20000

40000
60000
training step

80000

100000

0.4
0.2
0.0
0

20000

40000
60000
training step

80000

100000

Figure 9: Impact of different maximum rollout lengths when generating the synthetic interactions. MOMA-PPO-x indicates
a maximum rollout length of x. â€œgt" stands for â€œground truth" and means that the corresponding runs use the ground truth
world model. Mean and standard error of the mean on three seeds.
Figure 9 shows MOMA-PPO trainings for different maximum rollout lengths. It appears that varying the maximum length from 5 to 50
does not significatively impact MOMA-PPO performance on the tasks we investigated.

