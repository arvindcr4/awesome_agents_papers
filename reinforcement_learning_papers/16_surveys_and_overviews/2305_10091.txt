arXiv:2305.10091v1 [cs.AI] 17 May 2023

Multi-Agent Reinforcement Learning: Methods,
Applications, Visionary Prospects, and Challenges
ZIYUAN ZHOU and GUANJUN LIUâˆ— , Tongji University, China
YING TANG, Rowan University, USA
Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However,
current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This
paper aims to review methods and applications and point out research trends and visionary prospects for the
next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this
paper outlines the corresponding research methods and their limitations on safety, robustness, generalization,
and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we
believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest
that considering human interaction is essential for the practical application of MARL in various societies.
Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.
CCS Concepts: â€¢ General and reference â†’ Surveys and overviews; â€¢ Computing methodologies â†’
Robotic planning; â€¢ Hardware â†’ Safety critical systems; â€¢ Security and privacy â†’ Human and societal
aspects of security and privacy.
Additional Key Words and Phrases: Multi-agent Reinforcement Learning, Smart Transportation, Smart Education, Smart manufacturing, Unmanned Aerial Vehicles, Financial Trade, Network Security, Intelligent
Information System, Robustness, Safety, Generalization, Ethical Constraint
ACM Reference Format:
Ziyuan Zhou, Guanjun Liu, and Ying Tang. 2023. Multi-Agent Reinforcement Learning: Methods, Applications,
Visionary Prospects, and Challenges. 1, 1 (May 2023), 43 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

Reinforcement Learning (RL) is extensively explored due to its tremendous potential in solving
sequence decision tasks [88, 107, 129, 131, 168, 169, 197, 216, 218]. Kaelbling et al. pointed out in
1996 [76] that RL will be widely used in game playing and robotics. Mnih et al. [130] propose Deep
Reinforcement Learning (DRL) to combine reinforcement learning with reasoning ability and Deep
Learning (RL) with representative capacity, and the performance of the trained agent outperformed
that of human players in various Atari games. Silver et al. use RL to solve Go games in 2007[180]
and propose AlphaGo leveraging deep neural networks and Monte Carlo tree search in 2016 [179].
In robotics, DRL also achieves outstanding developments such as quadrupedal movement [92, 233].
The latest ChatGPT is well-known worldwide and makes use of RL-related technology. In the 20
âˆ— corresponding author

Authorsâ€™ addresses: Ziyuan Zhou; Guanjun Liu, Department of Computer Science, Tongji University, Shanghai, China,
{ziyuanzhou,liuguanjun}@tongji.edu.cn; Ying Tang, Department of Electrical and Computer Engineering, Rowan University,
Glassboro, New Jersey, USA, 08028, tang@rowan.edu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Â© 2023 Association for Computing Machinery.
XXXX-XXXX/2023/5-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
, Vol. 1, No. 1, Article . Publication date: May 2023.

2

Z. Zhou, G. Liu and Y. Tang

Table 1. The difference between this paper and other related reviews.

Work
[52, 78, 219]
[102]
[136]
[213]
[20]
[242]
[254]
[37]
[39]
[62]
[87, 201, 247]
[225]
Ours

SARL

MARL

Scope
Applications

Trustworthy

Human

Future Internet
Model-based
Large Population
Decentralized
Communication
Causal
Safety
Robustness
Generalization
Comprehensive
Brief

Brief
Comprehensive

years since DRL was proposed, there has been a continuous rise in research interest in games and
robotics. Visionary applications of RL are summarized in [76].
Multi-Agent Reinforcement Learning (MARL) research is advancing significantly based on
the issues of poor scalability and non-stationary and has shown remarkable success in a range
of applications. We summarize the relevant research on MARL in nine domains, involved in
engineering and science.
However, despite the impressive achievements, it is still necessary to construct trustworthy
MARL to apply it to real-world tasks better. Consequently, one of the most critical topics we need
to focus on in the next 10 to 20 years is how to establish a trustworthy MARL. As stated in [225], the
intrinsic safety, robustness, and generalization of RL still need to improve, making it challenging to
realize accurate general intelligence. While it mainly focuses on the single-agent domain. Compared
to Single-agent Reinforcement Learning (SARL), MARL requires consideration not only of individual
policy trustworthiness but also of the reliability of team interaction policies. As the number of
agents increases, the complexity of team policies also increases, which increases the difficulty of
researching trustworthy MARL. Currently, there is a portion of research on trustworthy MARL, but
it is still in the early stages. To promote the development of this field, we conduct a comprehensive
investigation of trustworthy multi-agent reinforcement learning from four aspects, including safety,
robustness, generalization, and learning with ethical constraints.
By integrating human aspects, it is necessary to take into consideration not just agent collaboration but also the interaction between intelligent physical information systems and human
civilization. In relation to MARL for human-machine interaction, we present four challenges: nonMarkovian due to human intervention, diversity of human behavior, complex heterogeneity, and
scalability of multi-human and multi-machines.
The difference between this paper and other related reviews are listed in Table 1. The outline of
this paper is shown in Fig. 1. The rest of this survey is organized as follows. In Section 2, we give a
relevant definition of MARL and summarize typical research methods. Section 3 shows the specific
application scenarios of MARL. Section 4 summarizes the definition, related research, and limitations
of trustworthy MARL. In Section 5, we point out the challenges faced by human-compatible MARL.
Section 6 concludes the whole paper.
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

3

Methods
Single-agent Reinforcement Learning

Multi-agent Reinforcement Learning

Applications of Multi-agent Reinforcement Learning
Smart Transportation
Traffic light control

Smart Manufacturing

Auto-driving

Job shop
scheduling

Industrial robots

Portfolio
management

Trading strategy
optimization

Unmanned Aerial Vehicles
Environmental
monitoring

Cluster control

Financial Trade

Collaborative
transportation

Programming
generation

Risk
management

Network Security

Intelligent Information System
Natural language
processing

Preventive
maintenance

Recommend
system

Intrusion detection

Resource optimization

Smart Education

RL for Science

Public Health and Intelligent Medical Diagnosis
Medical image
processing

COVID-19

Disease
diagnosis

Trustworthy
Visionary Prospects of Multi-agent Reinforcement Learning
Safe MARL
Optimization

Generalization in MARL
Formal methods

Multi-tasks transfer

Robustness in MARL
Test

Sim2Real

Others

Learning with Ethical Constraint
Privacy

Train

Fairness

Transparency

Human-Machine Intercation
Challenges on Human-Compatible Multi-agent Reinforcement Learning
Non-stationarity

Diversity

Complex Heterogeneity

Scalability

Fig. 1. The outline of this survey

2
2.1

METHODS
Single-agent Reinforcement Learning

The RL agent aims to maximize the total discounted expected reward by trial-and-error interactions
with the environment. Markov Decision Process (MDP) helps define models for sequential decisions.
Definition 1 (MDP). A Markov decision process can be formulated by a 5-tuple âŸ¨S, A, ğ‘…, ğ‘, ğ›¾âŸ©,
where S is the environmental-state set, A is the space of agent actions, ğ‘… : S Ã— A Ã— S â†’ R is the
reward obtained by the agent for transition to state ğ‘  â€² by doing action ğ‘ in state ğ‘  â€², R is the set of
real numbers, ğ‘ : S Ã— A â†’ Î” (S) is the transition probability from state ğ‘  âˆˆ S to state ğ‘  â€² âˆˆ S
given the action ğ‘, and ğ›¾ âˆˆ [0, 1] is the discount factor over time.
Solving MDP is to learn a policy ğœ‹ : S â†’ Î” (A) that maximizes the expected reward over time,
where Î” (Â·) is the probability simplex. The state-action (Q-function) and value functions are as
"âˆ
#
âˆ‘ï¸
ğ‘„ ğœ‹ (ğ‘ , ğ‘) = Eğœ‹
ğ›¾ ğ‘¡ ğ‘… (ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ‘ 0 = ğ‘, ğ‘  0 = ğ‘ ) ,
(1)
ğ‘¡ =0

ğ‘‰ğœ‹ (ğ‘ ) = Eğœ‹

"âˆ
âˆ‘ï¸

#
ğ‘¡

ğ›¾ ğ‘… (ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ‘  0 = ğ‘ ) ,

(2)

ğ‘¡ =0

, Vol. 1, No. 1, Article . Publication date: May 2023.

4

Z. Zhou, G. Liu and Y. Tang

where ğ‘… (ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘ ğ‘¡ +1 ) is an immediate reward environment returned when the agent executes action
ğ‘ğ‘¡ at time step ğ‘¡ to make the state transit from ğ‘ ğ‘¡ to ğ‘ ğ‘¡ +1 . Many techniques to solve MDP are divided
into value-based and policy-based methods. The most popular value-based method is Q-Learning
[189] which approximates the optimal Q-function ğ‘„ âˆ— by ğ‘„Ëœ and updates its value via TD as follows,


ğ‘„Ëœ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) â† ğ‘„Ëœ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) + ğ›¼ ğ‘…ğ‘¡ + ğ›¾ ğ‘šğ‘ğ‘¥ (ğ‘ ğ‘¡ +1, ğ‘) âˆ’ ğ‘„Ëœ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ )
(3)
ğ‘ âˆˆA

where ğ›¼ is the learning rate. The optimal policy ğœ‹âˆ— is derived from greedy action, i.e., ğœ‹âˆ— =
ğ‘ğ‘Ÿğ‘” ğ‘šğ‘ğ‘¥ ğ‘„ âˆ— (ğ‘ , ğ‘). Mnih et al. [130, 131] propose Deep Q-Network (DQN) combining deep neural
ğ‘
networks with Q-Learning, which minimizes the following loss function:
"
2#
L (ğœƒ ) = E âŸ¨ğ‘ ğ‘¡ ,ğ‘ğ‘¡ ,ğ‘…ğ‘¡ ,ğ‘ ğ‘¡ +1 âŸ©âˆ¼D ğ‘…ğ‘¡ + ğ›¾ ğ‘šğ‘ğ‘¥ ğ‘„ğœƒ âˆ’ (ğ‘ ğ‘¡ +1, ğ‘) âˆ’ ğ‘„ğœƒ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ )
,
(4)
ğ‘ âˆˆA

where ğœƒ and ğœƒ âˆ’ are parameters of Q-Network and target network fitted by a mini-batch of tuples
âŸ¨ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘…ğ‘¡ , ğ‘ ğ‘¡ +1 âŸ© sampled from replay buffer D.
The main idea of policy-based methods is to find optimal policy ğœ‹âˆ— by searching policy space
directly. Policy Gradient (PG) theorem [189] is as


â–½ğœƒ J (ğœƒ ) = Eğ‘ âˆ¼ğœ‡ğœ‹ğœƒ ,ğ‘âˆ¼ğœ‹ğœƒ â–½ğœƒ ğ‘„ ğœ‹ğœƒ (ğ‘ , ğ‘) ğ‘™ğ‘œğ‘” ğœ‹ğœƒ (ğ‘|ğ‘ ) ,
(5)
where ğœ‡ğœ‹ğœƒ is the state occupancy measure under policy ğœ‹ğœƒ . The Deterministic Policy Gradient
(DPG) theorem is used in continuous action space,


â–½ğœƒ J (ğœƒ ) = Eğ‘ âˆ¼ğœ‡ğœ‹ğœƒ â–½ğœƒ ğœ‹ğœƒ (ğ‘|ğ‘ ) â–½ğ‘ ğ‘„ ğœ‹ğœƒ (ğ‘ , ğ‘) |ğ‘=ğœ‹ğœƒ (ğ‘ ) .
(6)
2.2

Multi-agent Reinforcement Learning

Each agent in a Multi-Agent System (MAS) solves sequential decision problems via trial-and-error
contact with the environment. However, it is more complex than a single-agent scenario because
the next state and reward returned by the environment are based on all agentsâ€™ joint actions,
making the environment non-Markovian for any agent. Stochastic Game (SG) can be used to model
multi-agent sequential decision problems.
Definition 2 (SG). A Stochastic game can be represented as a tuple
ğ‘ , S, A 1, Â· Â· Â· , A ğ‘ , ğ‘… 1, Â· Â· Â· , ğ‘… ğ‘ , ğ‘, ğ›¾ ,
where ğ‘ is the number of agents, S is the state set of the environment, Ağ‘– is the action space
of the agent ğ‘–, ğ‘…ğ‘– : S Ã— A 1 Ã— Â· Â· Â· Ã— A ğ‘ Ã— S â†’ R is the reward function of the agent ğ‘–, ğ‘ :
S Ã— A 1 Ã— Â· Â· Â· Ã— A ğ‘ â†’ Î” (S) is the transition probability based on the joint action ğ’‚ and ğ›¾ âˆˆ [0, 1]
is the discount factor over time.
The state-action and value functions in multi-agent scenarios are defined like Eqs. (1) and (2),
respectively.
"âˆ
#
âˆ‘ï¸
ğ‘¡ ğ‘–
ğ‘„ ğœ‹ ğ‘– ,ğœ‹ âˆ’ğ‘– (ğ‘ , ğ’‚) = Eğœ‹ ğ‘– ,ğœ‹ âˆ’ğ‘–
ğ›¾ ğ‘… (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ’‚ 0 = ğ’‚, ğ‘  0 = ğ‘ ) ,
(7)
ğ‘¡ =0

ğ‘‰ğœ‹ ğ‘– ,ğœ‹ âˆ’ğ‘– (ğ‘ ) = Eğœ‹ ğ‘– ,ğœ‹ âˆ’ğ‘–

"âˆ
âˆ‘ï¸

#
ğ‘¡ ğ‘–

ğ›¾ ğ‘… (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ‘  0 = ğ‘ ) ,

(8)

ğ‘¡ =0


where ğœ‹ ğ‘– , ğœ‹ âˆ’ğ‘– is used
 to distinguish the policy between the agent ğ‘– and the other agents, similarly,
we can use ğ‘ğ‘– , ğ‘ âˆ’ğ‘– to represent the joint action ğ’‚. The common solving SG can be divided into
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

5

learning cooperation and learning communication according to whether communication between
agents is involved in the execution process.
2.2.1 Learning cooperation. The typical approach for learning cooperation involves centralized
training and decentralized execution (CTDE), utilizing global or communication information
during the training process while only using the observation information of the current agent
during the execution phase. It also includes value-based [161, 181, 188, 191, 208] and policy-based
[114, 119, 144, 164] MARL methods.
Value-based MARL: The updated rule of Eq. (3) is suitable for the multi-agent scenario:


ğ‘„Ëœ ğ‘– (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ ) â† ğ‘„Ëœ ğ‘– (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ ) + ğ›¼ ğ‘…ğ‘– + ğ‘šğ‘ğ‘¥ ğ›¾ {ğ‘„Ëœ ğ‘— (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ )} ğ‘— âˆˆ {1,...,ğ‘ } âˆ’ ğ‘„Ëœ ğ‘– (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ ) .
(9)
ğ‘ğ‘– âˆˆAğ‘–

Tampuu et al. [191] first extend the DQN to the multi-agent scenario equipping an independent
DQN for each agent, i.e., only considering the agentâ€™s interaction with the environment. The
experimental results demonstrate that this fully distributed training can produce good results for
simple MAS but that it is difficult to converge for complex tasks and that there is a credit assignment
issue. Sunehag et al. [188] overcome these issues by introducing a Value Decomposition Network
(VDN) based on the CTDE. An optimal linear-valued decomposition is trained from the team reward
function with VDN, and during execution, each agent uses an implicit value function based only on
partial observations to make decisions. However, this decomposition is linear and can only apply
to small-scale scenarios. Rashid et al. [161] use an end-to-end Q-Mixing Network (QMIX) to train
decentralized policies following the advantages of VND. QMIX is a complex non-linear network
that constrains the joint Q-function monotonic on the Q-function of each agent. This ensures the
consistency of centralized and decentralized policies and simplifies the solution for maximizing the
joint action-value function in offline policy learning. Son et al. [181] develop an innovative MARL
factorization technique called QTRAN that eliminates the structural restriction and uses a novel
technique to convert the initial joint action-value function into a simple decomposition function.
Although the decomposition of QTRAN is more complex computationally, it covers a broader range
of MARL activities as compared to VDN and QMIX. An approximate QTRAN performs hard in
complex domains with online data collecting and requires two extra soft regularizations [125].
As a result, effective scalability is still a challenge for cooperative MARL. Wang et al. [208] use a
duplex dueling network structure (QPLEX) to decompose the joint action-value function into an
action-value function for each agent to address this challenge. It is made easier to learn actionvalue functions with a linear decomposition structure by reformulating the Individual-Global-Max
(IGM) consistency as a restriction on the value range of the advantage function, which is a strong
scalability value-based MARL technique.
Policy-based MARL: The state of the environment is determined by the action of all agents
in the multi-agent scenario. The value-based method is challenging to train due to the unstable
environment, and the variance of the policy-based method gets more prominent as the number of
agents increases. Lowe et al. [119] proposed a variant of the actor-critic method in a multi-agent
scenario - multi-agent deep deterministic policy gradient (MADDPG), which considers the action
strategies of other agents in the process of reinforcement learning training for each agent, and
Only individual information is considered during the testing phase. The multi-agent deterministic
policy gradient can be written as



â–½ğœƒ ğ‘– J ğ‘– (ğœƒ ) = Eğ‘ âˆ¼ğœ‡ğ… ğ‘– â–½ğœƒ ğ‘– ğ‘™ğ‘œğ‘” ğœ‹ğœƒ ğ‘– ğ‘ğ‘– |ğ‘  â–½ğ‘ğ‘– ğ‘„ ğœ‹ğœƒ ğ‘– (ğ‘ , ğ’‚) | ğ’‚=ğ…ğœƒ (ğ‘ ) .
(10)
ğœƒ

However, as the number of agents increases, the estimation error in the critic network also increases,
making it difficult to scale MADDPG to larger environments. To address this limitation, researchers
have proposed attention mechanisms that allow agents to focus dynamically on relevant information.
, Vol. 1, No. 1, Article . Publication date: May 2023.

6

Z. Zhou, G. Liu and Y. Tang

For example, the MAAC [63], G2ANet [114] and HAMA [164] algorithms use graph structures to
model agent relationships and employ attention mechanisms to weigh their relevance. This approach
has shown promising results in environments with a large number of agents. Another challenge
in MAS is the need to adapt to changes in collaborative policies. The FACMAC algorithm [144]
addresses this issue by incorporating a centralized strategy gradient estimation to optimize joint
action spaces. This method has been shown to outperform MADDPG and QMIX in environments
with large-scale continuous actions.
Mean-Field-based MARL: The above methods are all based on the CTDE training framework,
effectively addressing the problem of non-Markovian environments in fully decentralized training
frameworks and the problem of high computational complexity in fully centralized training frameworks. However, existing MARL methods are usually limited to a small number of agents, and
scalability remains a challenging issue. Yang et al. [227] propose mean-field reinforcement learning
(MFRL), which approximates the interaction between individuals as the interaction between individuals and the average effect of the whole group or neighboring individuals and the convergence
of Nash equilibrium solutions is analyzed. Ganapathi et al. [32] extended MFRL to multiple types of
domains and proposed the MTMFQ method. Multiple types relax a core assumption in mean-field
games, which is that all agents in the environment are using almost identical strategies and have
the same goals. Then they further relaxed the assumption of MFRL and extended it to partially
observable domains, assuming that agents can only observe information from a fixed neighborhood
or from other agents based on random distances [33]. Zhang et al. [243] apply mean-field theory
to the value function decomposition-based MARL framework and proposed the MFVDN method,
which solves the problems of homogenous agents, limited representation, and inability to execute
with local information decentralized in MFRL.
2.2.2 Learning communication. The purpose of learning communication is for agents to learn
when, with which agents, and what information to communicate, which can be categorized as
reinforced and differentiable according to [254].
Reinforced: Foerster et al. [28] use DQN with a recurrent network to handle partial observability
called RIAL. Kilinc et al. [86] improve a DDPG algorithm enhanced by a communication medium
including a concurrent learning mechanism that allows agents to decide if their private observations
need to be shared with others. To maximize communication efficiency, Huang et al. [55] propose a
network named ETCNet, that uses RL to find the optimal communication protocol within bandwidth
constraints. The bandwidth is minimized due to messages being sent only when necessary. Gupta et
al. [44] introduce a central agent observing every observation with multiple agents only receiving
local observations and no communication. The central agent determines the message each agent
needs to make better decisions based on global observations, avoiding central solving of the entire
problem.
Differentiable: Sukhbaatar et al. [184] develop a neural model CommNet that lets the agents
communicate continuously for fully cooperative tasks. Agents learn both their policy and communication way during training. To maintain effective communication, Peng et al. [146] propose a
multi-agent Bidirectionally-Coordinated Network (BiCNet) with a vectorized actor-critic formulation. They demonstrate that BiCNet can learn advanced coordination methods without supervision.
To learn abstract representations of the interaction of agents, Jiang et al. [67] propose graph convolution RL that leverages graph convolution to adapt to the underlying dynamics of the graph,
with relation kernels capturing the interaction of agents. Wang et al.[211] devise a novel approach
entitled IMAC, which addresses the challenges of constrained-bandwidth communication in MARL.
IMAC optimizes resource usage, minimizes needless connections, and allows smooth communication protocols and schedules. It uses low-entropy messages that stick to bandwidth limits and
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

7

Table 2. Correspondence between smart transportation and RL methods.

Applications
Traffic light
control
Smart
Transportation
Auto-driving

Papers
[99]
[220]
[255]
[232]
[250]
[8]
[61]
[251]

Methods
DQN-based [131]
MADDPG-based [119]
Game theoretic
Dynamic coordination graph
Auto-driving simulation platform
DQN-based [131]
AC-based [88]
AC-based [129]

SA/MA
SA
MA
MA
MA
MA
MA
SA
MA

merges the information bottleneck principle with a weight-based scheduler to produce a practical
protocol. Using an attention mechanism is insufficient as it overlooks dynamic communication and
the correlation between agentsâ€™ connections. To tackle this issue, Du et al. [24] propose a method
that utilizes a normalizing flow to encode the correlation between agentsâ€™ interactions, allowing
for direct learning of the dynamic communication topology. This methodology proves effective in
cooperative navigation and adaptive traffic control tasks. Niu et al. [137] leverage a graph-attention
mechanism to determine the most pertinent agent of messages and the most suitable means of
delivery.
Overall, these algorithms aim to improve the scalability and non-stationary of MAS, allowing
agents to learn from the experiences of other agents and achieve better performance in complex
environments.
3

APPLICATIONS OF MULTI-AGENT REINFORCEMENT LEARNING

Through MARL, agents are able to learn and communicate with each other, thereby achieving
more efficient task completion and better decision-making results. This method is widely used in
engineering and science, for example, in smart transportation, unmanned aerial vehicles, intelligent
information system, public health and intelligent medical diagnosis, smart manufacturing, financial
trade, network security, smart education, and RL for science.
3.1

Smart Transportation

Smart transportation makes use of advanced technologies like the Internet of Things (IoT) and
AI to increase safety, improve transportation efficiency, and reduce its negative environmental
effects. In MARL-based smart transportation, we describe two known scenarios: traffic light control
and auto-driving and present the role of humans in these intelligent systems. The correspondence
between this application and RL methods is shown in Table 2.
Traffic light control: Li et al. [99] use DQN to obtain the optimal policy in sight of the variety
of the control action and the state and demonstrate the potential of DRL in traffic light control.
However, the control of traffic lights needs to consider the situation of multiple intersections. Wu
et al. [220] combine MADDPG with Long-short-term Memory (LSTM) for multi-intersection traffic
light control coordination. The use of LSTM is appropriate to address the environmental instability
forced on by partial observable states. They take into account both the cars and the pedestrians
waiting to cross the street. Zhu et al. [255] propose a Bi-hierarchical Game-theoretic (BHGT) to
solve network-wide traffic signal control problems. They evaluate the state of the network-wide
traffic based on the collection data of trips. The experiment shows that BHGT efficiently reduces
the network-wide travel delay.
, Vol. 1, No. 1, Article . Publication date: May 2023.

8

Z. Zhou, G. Liu and Y. Tang

Table 3. Correspondence between unmanned aerial vehicles and RL methods.

Applications
Cluster control
Unmanned Aerial
Vehicles

Environmental
monitoring
Collaborative
transportation

Papers
[124]
[207]
[210]
[148]
[75]
[204]
[134]
[66, 74]
[177]

Methods
DQN-based [131]
DDPG-based [107]
MADDPG-based [119]
DQN-based [131]
DQN-based [131]
TRPO-based [168]
DQN-based [131]
MAAC-based [63]
MADDPG-based [119]

SA/MA
SA
MA
MA
MA
MA
MA
MA
MA
MA

Auto-driving: Chao et al. [232] simulate the dynamic topography during vehicle interactions
using a dynamic coordination graph and put forward two fundamental learning strategies to
coordinate the driving actions for a fleet of vehicles. Additionally, they propose a number of
extension mechanisms in order to adapt to the complex scenario with any number of vehicles.
Zhou et al. [250] build an autonomous driving simulation platform to realize more realistic and
diverse interactions. Bhalla et al. [8] propose two novel centralized training based on DQN and a
memory block to execute decentralized, which achieve better cumulative reward in autonomous
driving. Huang et al. [61] propose a sample efficient DRL framework including imitative expert
priors and RL. The agent learns expert policy from the prior human knowledge and is guided by
minimizing the KL divergence between the policy of the agent and the imitative expert. Zhou et al.
[251] propose a MARL framework composed of a brand-new local reward and scheme for sharing
parameters for lane-changing decision makings.
As a system that involves both physical and digital components, it requires the active participation
and cooperation of humans to achieve its full potential. Humans play a crucial role in the operation
and management of systems for transportation, from designing and building infrastructure to using
and maintaining vehicles to making decisions about routing and scheduling. Thus, the success of
smart transportation ultimately depends on how well it can integrate and leverage the capabilities
of both humans and machines in a seamless and effective manner. However, the current state of
research on MARL-based smart transportation is without adequately address the decision priority
between human control and intelligent algorithms. Given the continuously evolving nature of
both human behavior and city traffic, situations such as traffic accidents and surges in vehicles can
make it challenging to manage traffic jams solely through traffic signal control. In such scenarios,
human intervention becomes necessary. Similarly, in instances where self-driving cars encounter
hazardous situations that were not anticipated during training, relinquishing control to the human
driver is critical. Defining the optimal decision priority between humans and agents remains an
unresolved issue.
3.2

Unmanned Aerial Vehicles

In MARL-based Unmanned Aerial Vehicles (UAVs) applications, we describe three known scenarios:
cluster control [124, 158, 207, 210, 222â€“224], environmental monitoring [75, 134, 148, 204], and
collaborative transportation [66, 74, 177]. The correspondence between this application and RL
methods is shown in Table 3.
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

9

Cluster control: Maciel-Pearson et al. [124] make use of DRL to improve the ability of UAVs
to automatically navigate when the environments are various. The approach uses a double stateinput strategy that combines positional information with feature maps from the current scene.
This approach is tested and shown to outperform other DQN variants and has the ability to
navigate through multiple unknown environments and extreme weather conditions. A two-stage
RL method is proposed by Wang et al. [207] for multi-UAV collision avoidance to address the issues
of high variance and low reproducibility, where supervised training is in the first stage and policy
gradient is in the next stage. Wang et al. [210] propose a trajectory control method according to
MARL, which introduced a low-complexity approach to optimize the offloading decisions of the
user equipment given the trajectories of UAVs. The results show that the proposed approach has
promising performance.
Environmental monitoring: Pham et al. [148] propose a distributed MARL algorithm to
achieve complete coverage of an unfamiliar area while minimizing overlapping fields of view. Julian
and Kochenderfer [75] present two DRL approaches for controlling teams of UAVs to monitor
wildfires. The approaches accommodate the problem with uncertainty and high dimensionality
and allow the UAV to accurately track the wildfire expansions and outperform existing controllers.
The approaches scale with different numbers of UAVs and generalize to various wildfire shapes.
Walker et al. [204] propose a method for indoor target-finding by combining Partially Observable
MDP (POMDP) and DRL. The framework consists of two stages: planning and control. Global
planning is done using an online POMDP solver, while local control is done using Deep RL. Mou et
al. [134] propose a hierarchical UAV swarm architecture based on the DRL algorithm for solving
the 3D irregular terrain surface coverage problem. A geometric approach is used to divide the 3D
terrain surface into weighted 2D patches. A coverage trajectory algorithm is designed for low-level
follower UAVs to achieve specific coverage tasks within patches. For high-level leader UAVs, a
swarm DQN algorithm is proposed to choose patches, which integrates Convolutional Neural
Networks (CNNs) and mean embedding methods to address communication limitations.
Collaborative transportation: Jeon et al. [74] design a UAV logistics delivery service environment using Unity to evaluate MADRL-based models, and Jo [66] propose a fusion-multi-actorattention-critic (F-MAAC) model based on the MAAC. It is shown from the results that F-MAAC
outperformed MAAC in terms of the total number of deliveries completed during a specific period
and the total number of deliveries completed over the same distance. Our previous work [177]
develops a virtual platform for multi-UAVs collaborative transport using AirSim [174] and proposed
recurrent-MADDPG with domain randomization technique to achieve MARL sim2real transfer.
By utilizing MARL, UAV systems can make autonomous decisions and collaborations in various
scenarios, leading to more efficient task completion. However, existing works do not consider the
command and interaction between ground workstations and operators for UAV systems, and the
robustness and safety of MARL are deficient. When a UAV encounters interference and cannot
make the correct decisions, it can cause serious harm to human society. Considering the interaction
between intelligent UAV systems and humans to achieve more efficient and safer UAV systems is
one of the goals in future 10-20 years.
3.3

Intelligent Information System

MARL has tremendous potential for applications in intelligent information systems, including
natural language processing (NLP) [13, 83, 98, 104, 120, 183, 195, 226], programming generation
[26, 104, 178], and recommender systems [40, 51, 72, 231, 245]. Techniques based on SARL have
been studied in NLP and programming generation, and we will summarize these studies and point
out the significant advantages of MARL in these applications. The correspondence between this
application and RL methods is shown in Table 4.
, Vol. 1, No. 1, Article . Publication date: May 2023.

10

Z. Zhou, G. Liu and Y. Tang

Table 4. Correspondence between intelligent information system and RL methods.

Applications
Natural
language
processing
Intelligent
Information
System

Programming
generation
Recommender
system

Papers
[98]
[226]
[120]
[83]
[104]
[140]
[94]
[178]
[26]
[51, 72, 231]
[245]
[40]

Methods
REINFORCE-based [218]
AC-based [88]
DQN-based [131]
REINFORCE,AC,DQN [88, 131, 218]
AC-based [88]
PPO-based [169]
REINFORCE [218]
PPO-based [169]
DQN-based [131]
MADDPG-based [119]
Learning communication
IQL-based [191]

SA/MA
SA
SA
SA
SA
SA
SA
SA
SA
SA
MA
MA
MA

Natural language processing: Li et al. [98] describe how RL can be applied to chatbot dialogue
generation to predict the impact of current actions on future rewards. By utilizing a policy gradient
approach to optimize long-term rewards defined by the developer, the model learns all possible
strategies for speaking in an infinite action space, resulting in more interactive and consistent
conversation generation for chatbots. Yang et al. [226] combine multi-task learning and RL to
present a personalized dialog system called MRPDG. Three kinds of rewards are used to guide the
model to produce highly rewarded dialogs. In order to address the problems of sparse rewards and
few successful dialogues, Lu et al. [120] propose two complex methods for hindsight experience
replay. During the RL training process, chatbot agents can be made to generate more authentic
dialogues by introducing human-relevant evaluation metrics. Chen et al. [13] present a framework
called "companion teaching" in which a human teacher guides the machine in real-time during
the learning process and uses example actions of the teacher to improve policy learning. Su et al.
[183] present two approaches to address the challenge of measuring rewards in real-world dialogue
system applications. Keneshloo et al. [83] use RL to solve the effects of sequence-to-sequence
exposure bias and inconsistency between training and test measurements. Li et al. [104] propose a
new framework that includes a generator and an evaluator for learning from data. The generator is
a learning model for paragraph generation, and the evaluator is a matching model used to provide
a reward function for RL. Large language models may produce fake or useless outputs. Ouyang
et al. [140] introduce RL with human feedback to fine-tune GPT-3 to reduce unwanted outputs
and propose a language model called instructGPT. They believe that it is important to use human
feedback to make the output of large language models close to human intention.
Programming generation: Le et al. [94] design a program synthesis framework called CodeRL
that uses pre-trained language models and RL to generate programs.Shojaee et al. [178] integrate
a pre-trained programming language model with PPO to optimize the model through execution
feedback and present a new code generation framework called PPOCoder. Software testing is
essential for quality assurance but expensive and time-consuming. Esnaashari et al. [26] propose
a new method using a memetic algorithm with RL as a local search that outperforms traditional
evolutionary or heuristic algorithms in speed, coverage, and evaluation.
MARL has advantages over SARL in NLP and programming generation due to its stronger
collaboration ability and adaptability. In NLP, MARL can be used for tasks such as chatbots and
text translation. In these tasks, multiple agents can work together to learn the knowledge and skills
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

11

Table 5. Correspondence between public health and intelligent medical diagnosis and RL methods.

Applications
COVID-19
Public Health and
Intelligent Medical
Diagnosis

Medical image
processing
Disease diagnosis

Papers
[84, 85, 90]
[249]
[14, 65]
[2, 96, 202, 202, 248]
[106, 122]
[80]
[109, 110, 193]
[159]

Methods
DQN-based [131]
DDPG-based [107]
DQN-based [131]
DQN-based [131]
A3C-based [129]
AC-based [88]
DQN-based [131]
DQN-based [131]

SA/MA
SA
SA
SA
MA
MA
MA
SA
MA

of a conversational system, thereby improving its performance and interaction experience. For
programming generation, MARL is usually more suitable for scenarios that require the generation
of complex systems or large-scale software. This is because in MARL, each agent can be responsible
for generating a part of the code, and the whole system can be built through collaboration. This
approach can improve the efficiency and quality of the generated code and can reduce the repetition
and error rate of the code.
Recommender system: He et al. [51] propose a MARL method with communication restrictions
to address sub-optimal global strategies due to the lack of cooperation among optimization teams.
Zhang et al. [245] propose a novel dynamic, collaborative recommendation method utilizing
MARL for recommending academic collaborators, optimizing collaborator selection from different
similarity measures. To improve communication efficiency on Twitter-like social networking, Gui
et al. [40] propose a MARL by combining dozens of more historical tweets to choose a set of users.
Jin et al. [72] propose a method for optimizing bids using MARL to achieve specific goals, such
as maximizing revenue and return on investment for real-time advertising. The method uses a
clustering approach to assign strategic bidding agents to each advertiser cluster and proposes
a practical distributed coordinated multi-agent bidding to balance competition and cooperation
among advertisers. Li and Tong [231] propose a social MARL framework named MATR, where one
agent captures the dynamic preferences of users while the other exploits social networks to reduce
data sparsity and cold starts. The state representation module aims to learn from social networks
and user rating matrices, using trust inference and feature aggregation modeling to optimize the
use of social networks.
MARL has many advantages in intelligent information processing, but the lack of robustness
and transparency prevents MARL decisions from being trusted by humans. In order to apply MARL
to the real world, it is first necessary to improve its trustworthiness, and in addition, RL with
human feedback needs to be further considered to make the generated language more realistic, the
programming more efficient, and the recommended content more attractive.
3.4

Public Health and Intelligent Medical Diagnosis

MARL is widely explored and applied in public health and intelligent medical diagnosis. For example,
MARL can be applied in COVID-19 prediction and management, medical image processing, and
disease diagnosis to improve disease prevention, diagnosis, and treatment efficiency and accuracy.
The correspondence between this application and RL methods is shown in Table 5.
COVID-19 prediction and diagnosis: Khalilpourazari et al. [84, 85] present the Hybrid Qlearning-based algorithm (HQLA) as a solution to predict the COVID-19 pandemic. HQLA accurately
reflects the future trend in France and Quebec, Canada. Furthermore, their analysis also provides
, Vol. 1, No. 1, Article . Publication date: May 2023.

12

Z. Zhou, G. Liu and Y. Tang

critical insights into pandemic growth and factors that policymakers should consider when making
social measures. Kumar et al. [90] utilize two learning algorithms, DL and RL, to forecast COVID-19,
where LSTM is used to forecasts newly affected individuals, losses, and cures in the coming days,
and DQN is suggested for optimizing predictive outcomes based on symptoms. Zheng et al. [249]
propose developing MDPs to model the oxygen flow trajectory and health outcomes of COVID-19
patients. Using Deep Deterministic Policy Gradient (DDPG), an optimal oxygen control policy is
obtained for each patient, resulting in a reduced mortality rate.
Regarding the prediction and diagnosis of COVID-19, existing studies are based on SARL. compared with SARL, MARL can be responsible for different tasks, such as virus transmission model
prediction and clinical diagnosis, separately and then complete the task through communication and
collaboration. In addition, the COVID-19 epidemic develops rapidly and is influenced by multiple
factors, and MARL can better handle the uncertainty and complexity. Therefore, we believe that
MARL has excellent potential in this area.
Medical image processing: X-ray images have become crucial for expediting the diagnostics
of COVID-19. Jalali et al. [65] propose an ensemble of CNNs to differentiate COVID-19 patients
from non-patients according to an automated X-ray image. The selective ensemble approach utilizes
DQN to heighten model accuracy while reducing the required classifiers. Chen et al. [14] suggest an
RL-based detection framework to quickly and effectively diagnose COVID-19. They build a mixed
loss, enabling efficient detection of the virus. Additionally, they propose a prediction framework that
allows for integrating multiple detection frameworks through parameter sharing. This allows for the
prediction of disease progression without the need for additional training. Allioui et al. [2] develop
a new method for more efficient automatic image segmentation that employs MARL. This approach
addresses mask extraction difficulties and uses a modified version of the DQN to identify masks in
CT images of COVID-19 patients. MARL can be used for interactive image segmentation, where
each voxel is an agent with a shared behavior policy to reduce exploration space and dependence
among voxels. [106] is for the field of medical image segmentation, considering clinical criteria,
using MARL to solve the problem, reducing the exploration space, and using a sharing strategy to
capture the dependencies between pixels; While [122] is for interactive image segmentation, using
MDP and MARL models to model iterative segmentation, introducing a boundary-based reward
function to update the segmentation strategy. Zheng et al. [248] use a MARL approach to prostate
localization in Magnetic Resonance (MR) images. They create a communication environment by
sharing convolutions and maintaining independent action policy via distinct fully connected layers
for each agent. Anatomical landmark detection is crucial in medical image analysis. Vlontzos et
al.[202] present a novel approach using MARL to detect multiple landmarks simultaneously. This
theory suggests that the positioning of anatomical landmarks in human anatomy is interdependent
and not random. It can accommodate ğ¾ agents to detect ğ¾ different landmarks with implicit intercommunication. Leroy et al. [96] develop a communicative MARL framework, aiding in detecting
landmarks in MR images. In contrast to [202], agent communication is explicit. Kasseroller et al.
[80] propose a solution to the long inference time caused by DQN-based methods being limited
to a discrete action space. They recommend using a continuous action space to allow the agent
to move smoothly in any direction with varying step sizes, resulting in fewer required steps and
increased landmark identification accuracy.
Disease diagnosis: Ling et al. [109, 110] propose an RL-based method to improve clinical
diagnostic inferencing. This approach can extract clinical concepts, integrate external evidence,
and identify accurate diagnoses, which is especially beneficial in cases with limited annotated data.
The system uses a DQN architecture and a reward function to optimize accuracy during training.
Tang et al. [193] introduce a new neural symptom checker that employs an ensemble model. They
incorporate an RL framework to develop inquiry and diagnosis policies as MDPs without using
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

13

Table 6. Correspondence between smart manufacturing and RL methods.

Applications

Job shop scheduling

Smart Manufacturing
Industrial robots
Preventive
maintenance

Papers
[212]
[246]
[73]
[152, 153]
[240]
[126]
[1]
[192]
[89]
[93]
[182]
[163]

Methods
QMIX-based [161]
PPO-based [169]
DDPG-based [107]
PPO-based [169]
DQN-based [131]
IQL-based [191]
PPO-based [169]
DQN-based [131]
MADDPG-based [119]
AC-based [88]
MADDPG-based [119]
PPO-based [169]

SA/MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA

previous approximation methods. Furthermore, they develop a model for each anatomical section
reflective of the practices of various hospital departments. This new approach offers improved
user experience and significant enhancements in disease prediction accuracy over current models.
Rajesh et al. [159] created the IMRLDPTR system, which uses mobile agents to collect data from
multiple sources and generates rule sets for different disease categories.
MARL has many benefits in public health and intelligent medical diagnosis, such as the ability
to handle highly complex tasks and to consider the interaction of multiple factors and variables.
However, MARL also has some drawbacks, such as low transparency of the learning process and
decision results, making it difficult to understand the decision process and behavior of the model. In
addition, the robustness of MARL is poor, and the decisions are sensitive to perturbations. Therefore,
the above drawbacks must be addressed when applying MARL to this field.
3.5

Smart Manufacturing

Smart manufacturing is the integration of advanced technologies, e.g., IoT, AI, and so on, into the
manufacturing process to optimize the production process. As for smart manufacturing, MARL is a
promising approach. In the context of smart manufacturing, MARL can be utilized as a tool for
production scheduling, shop industrial robot control, quality control, and equipment maintenance
to achieve an intelligent and efficient production process [97]. The correspondence between this
application and RL methods is shown in Table 6.
Job shop scheduling is a key challenge in smart manufacturing because it involves complex
decision-making processes and resource allocation problems. Traditional approaches are usually
based on rules or static algorithms, but these approaches frequently fall short of adjusting to
the changing production environment. In recent years, MARL has been introduced to job shop
scheduling to improve the efficiency and accuracy of shop floor task scheduling by learning and
adapting strategies from a progressively changing environment. In the resource preemption that
addresses the high-dimensional action space problem. A MARL algorithm for job scheduling is
proposed in [212]. In the algorithm, the environment is modeled as a Markov decision process
which is decentralized and partially observable. And every job is regarded as an agent which
selects the available robot. Zhang et al. [246] propose a multi-agent manufacturing system for
efficient and autonomous personalized order processing in a changeable workshop environment.
The manufacturing equipment is built as an agent with an AI scheduler, which generates excellent
, Vol. 1, No. 1, Article . Publication date: May 2023.

14

Z. Zhou, G. Liu and Y. Tang

production strategies in sight of the workshop state and is periodically trained through the PPO
algorithm [169]. This algorithm can tackle resource or task disturbances and obtain solutions that
satisfy different performance metrics. Jing et al. [73] address the flexible job shop scheduling issues
by utilizing a graph-based MARL with centralized learning decentralized execution. The approach
uses a directed acyclic graph to simulate the flexible job shop scheduling issues and predicts the
connection probability among edges to adjust the scheduling strategy. Popper et al. [152, 153] use
MARL to deal with the issues of flexible job shop scheduling with multiple objectives. Zhang et al.
[240] propose a new model called DeepMAG for flexible job shop scheduling according to MARL.
DeepMAG provides each machine and job with an agent, and they work together to find the best
action. In Industry 4.0, a user-friendly MARL tool for the job shop scheduling problem is designed
in [126], which provides users with the chance to communicate with the learning algorithms. Users
can either maintain the optimal schedule produced by Q-Learning or change it to meet constraints.
Industrial robots have a growing amount of influence on industrial manufacturing. However,
with the increasing complexity of production tasks, it is often difficult for individual robots to
complete tasks effectively. MARL is widely used in smart manufacturing robots. Agrawal et al. [1]
propose a framework based on MARL that integrates job scheduling and navigation control for an
autonomous mobile robot-operated shop floor. To address the challenge of increasing demands for
customization and rapid product iterations, Tan et al. [192] propose a multi-agent model for the
industrial robot assembly process, and the communication of agents which have real-time data
acquisition and fusion is studied. Besides, they also propose an excellent algorithm for planning
and scheduling industrial robot assembly using a MARL approach. Krnjaic et al. [89] use MARL to
optimize order-picking systems in commercial warehouses. The goal is to improve efficiency and
flexibility while minimizing resource constraints. The MARL framework is applicable to various
configurations of warehouses and allows agents to learn how to cooperate optimally with one
another. Lan et al. [93] explore the use of MARL to optimize coordination in a multi-robot pickand-place system for smart manufacturing.
Preventive maintenance: With the increasing scale and productivity of the manufacturing
industry, how to design useful preventive maintenance strategies to guarantee the steady operation
of production systems has become a vital issue in the manufacturing field. The MARL approach has
provided a new idea to address this issue. Due to the problem of action space explosion, traditional
RL methods are difficult to be applied directly. Therefore, [182] adopts a MARL-based approach in
a manufacturing system to model every machine as a collaborative intelligence and implements
adaptive learning through the multi-agent value decomposition Actor-Critic algorithm to obtain an
efficient and cost-reasonable preventive maintenance strategy. [163] present a multi-agent approach
using RL to coordinate maintenance scheduling and dynamically assign tasks to technicians with
various skills under the uncertainty of multiple machine failures.
MARL shows potential applications in smart manufacturing and achieves some stunning results.
However, this approach has challenges in scalability and is difficult to scale to situations with
a high number of agents. It also suffers from poor generalization, which makes it difficult to be
applied well to real scenarios. In addition, smart manufacturing is a task that involves humancomputer interaction, so human behavior and human-computer priority switching need to be
considered when applying MARL. All these factors need to be fully considered when designing
and implementing MARL algorithms to ensure the reliability and applicability of the models.
3.6

Financial Trade

Financial trading is a challenging activity that requires fast judgment and adjustment to continuously
changing market conditions. Single-agent approaches and DL techniques from the past are no
longer adequate to meet market expectations. MARL offers a fresh idea for tackling the difficulties in
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

15

Table 7. Correspondence between financial trade and RL methods.

Applications
Portfolio
management

Financial Trade

Trading strategy
optimization

Risk management

Papers
[123, 150]
[60, 95, 175]
[156]
[157]
[143]
[79]
[7]
[6]
[34]
[49]

Methods
MADDPG-based [119]
DQN-based [131]
MFRL-based [227]
MADDPG-based [119]
DQN-based [131]
Double-Q-based [197]
DDPG-based [107]
Multi-agent System
PPO-based [169]
DQN-based [131]

SA/MA
MA
MA
MA
MA
MA
MA
MA
MA
MA
MA

financial trade by combining collaboration and competition among various agents. We summarize
the applications of MARL in financial trade from the perspectives of portfolio management [60, 95,
123, 150, 175], trading strategy optimization [79, 143, 156, 157], and risk management [6, 34, 49].
The correspondence between this application and RL methods is shown in Table 7.
Portfolio management: In portfolio management, MARL can help investors better optimize
asset allocation and improve returns. Multiple agents make investment decisions and are trained to
achieve optimal investment portfolios and returns. For a portfolio of 10 equities on the Vietnam
stock market, Pham et al. [150] use MARL to create an automatic hedging strategy. They develop
a simulator including transaction fees, taxes, and settlement dates for training the RL agent. The
agent can get knowledge of trading and hedging to minimize losses and maximize earnings. It also
protected portfolios and generated positive profits in case of a systematic market collapse. Lee et al.
[95] propose a new investment strategy called a MARL-based portfolio management system (MAPS)
that uses a cooperative system of independent "investor" agents to create a diversified portfolio. The
agents are trained to act in a variety of ways and maximize their return using a thought-out loss
function. To address the scalability and re-usability in RL-based portfolio management, Huang and
Tanaka [60] propose a MARL-based system with Evolving Agent Module (EAM) and the Strategic
Agent Module (SAM). EAM generates signal-comprised information for a particular asset using a
DQN agent. In contrast, SAM uses a PPO agent for portfolio optimization by connecting to multiple
EAMs to reallocate corresponding assets. Ma et al. [123] introduce a new MARL for optimizing
financial portfolio management. The algorithm employs two agents to study the best trading policy
for two distinct categories of stock trends, with a trend consistency factor that takes into account
the consistency of stocks within a portfolio. Besides, the reward function now includes a novel TC
regularization, which is based on the trend consistency factor value. The algorithm dynamically
alternates between the two agents in order to obtain the best portfolio strategy based on the state of
the market. Shavandi and Khedmati[175] propose a MARL framework that leverages the collective
intelligence of expert traders on various periods. The DQN and a hierarchical structure are used in
the framework to train the agents.
Trading strategy optimization: In the financial markets, developing an effective trading strategy is always a challenging issue. Traditionally, trading strategies are usually designed by individuals
or teams based on their experience and skills, but there are many limitations in this approach. With
the continuous advance in AI methods, MARL is widely applied in the optimization of trading
strategies. It allows multiple agents to collaborate and compete to learn and improve strategies,
leading to better trading results. [156] and [157] worked by Qiu et al use MFRL [227] and MADDPG
, Vol. 1, No. 1, Article . Publication date: May 2023.

16

Z. Zhou, G. Liu and Y. Tang

Table 8. Correspondence between network security and RL methods.

Applications
Intrusion
detection
Network Security
Resource
optimization

Papers
[17, 118, 172, 173]
[54]
[132]
[103, 186, 190]
[145]
[135]

Methods
DQN-based [131]
DQN-based [131]
SARSA-based [91]
DQN-based [131]
MADDPG-based [119]
Double-Q, AC [88, 197]

SA/MA
MA
SA
MA
MA
MA
MA

[119] to optimize energy trading and market strategies, respectively. Patel [143] applies MARL to
place limit orders to optimize market-making. The MARL framework consists of a macro-agent
that decides whether to buy, sell, or hold an asset and a micro-agent that places limited orders
within the order book. A model-free method is proposed by Karpe et al.[79]. It uses the Double
Deep Q-Learning algorithm [197], which is trained in a multi-agent realistic market simulation
environment. The approach involves configuring a historical order book simulation environment
with multiple agents and evaluating the simulation with real market data. Bao [7] proposes a MARL
method to formulate stock trading strategies for investment banks with multiple clients. The method
aims to balance revenue and fairness among clients with different order sizes and requirements.
The proposed scheme uses RL to adapt trading strategies to complex market environments and
uses MAS to optimize individual revenues.
Risk management: Risk management is always a crucial part of business and organization
management. Compared with traditional SARL, MARL can help enterprises and organizations better
manage risk and reduce potential losses and risks. Bajo et al. [6] discuss the need for innovative
tools to help small to medium enterprises predict risks and manage inefficiencies and create a
multi-agent system that uses advanced reasoning to detect situations with risks and offer decision
support. Ganesh et al. [34] use a simulation to study how RL can be utilized for training market
maker agents in a dealer market. The RL agent learns to manage inventory and adapt to market
price trends while also learning about the pricing strategies of its competitors. They also propose
and test reward formulations to create risk-averse RL-based market makers. He et al. [49] propose
a new approach to train a trading agent using RL by using a multi-agent virtual market model
consisting of multiple generative adversarial networks. The model creates simulated market data
that takes into account how the action of the agent affects the state of the market. A backtest of the
China Shanghai Shenzhen 300 stock index futures in 2019 shows that the trained agent has a 12
percent higher profit and a low risk of loss.
3.7 Network Security
Network security is an important issue facing society today, where attackers use various techniques
and means to compromise computer systems and networks, threatening the security of individuals,
organizations, and nations. MARL is a promising approach that can be used in the field of network
security, with major applications in intrusion detection [54, 118, 118, 132, 172, 173] and network
resource optimization [103, 135, 145, 186, 190]. The correspondence between this application and
RL methods is shown in Table 8.
Intrusion detection: Intrusion detection is one of the critical aspects to protect network
security [70]. However, traditional intrusion detection systems may have limitations in the face of
complex and variable network attacks. MARL is an effective solution that can be used to enhance
the accuracy and robustness of intrusion detection through collaborative learning and mutual
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

17

communication. Servin and Kudenko [172] present a MARL-based intrusion detection method
that enables the identification and prediction of normal and abnormal states in a network through
learning and interaction between distributed sensors and decision-making intelligence. Sethi et al.
[173] propose an intrusion detection system according to MARL with attention mechanisms for
efficient detection and classification of advanced network attacks. A DRL algorithm is proposed by
Hsu and Matsuoka [54] for the anomaly network intrusion detection systems, which can update
itself to detect new types of network traffic behavior. The system is tested on two benchmark
datasets and a real campus network log and compared to three classic machine learning methods
and two related published results. The model is capable of processing large amounts of network
traffic in real time. Safa and Ridha [132] propose a new adversarial MARL approach-based Deep
SARSA [91] for intrusion detection in dynamic environments. The proposed algorithm addresses
the problem of imbalanced distribution datasets by improving the detection of minority classes,
which can improve classifier performance. Louati et al. [118] propose an intelligent and distributed
intrusion detection system using the MAS based on parallel ML algorithms. Chowdhary et al. [17]
propose a MARL framework for an adversarial game in a software-defined network-managed cloud
environment. This model takes into account the dynamic nature of the network and minimal impact
on service availability.
Resource optimization: Suzuki and Harada [190] propose a safe MARL to optimize network
resources efficiently even during significant changes in network demands. This method uses DRL
algorithms to learn the relationship between network demand patterns and optimal allocation
in advance. Safety considerations and multi-agent techniques are developed to reduce constraint
violations and improve scalability, respectively. Sun et al. [186] propose a dynamic controller
workload balancing scheme based on MARL to address the time-consuming or under-performing
issues of iterative optimization algorithms. Peng and Shen[145] explore multi-dimensional resource
management for UAVs in vehicular networks, and the problem is formulated as a distributive
optimization problem that can be addressed by the MADDPG method [119]. Li et al. [103] propose
a MARL approach to address resource-balancing challenges within complex transportation networks. The traditional solutions leveraging combinatorial optimization face challenges due to high
complexity, uncertainty, and non-convex business constraints. The proposed approach introduces
a cooperative mechanism for state and reward design, resulting in better transportation which is
more efficient and effective. Naderializadeh et al.[135] propose a distributed resource management
and interference mitigation mechanism for wireless networks using MARL. In the network, each
transmitter is equipped with a DRL agent responsible for selecting the user to serve and determining
the transmission power to utilize based on delayed observations from its associated users and
neighboring agents.
MARL has excellent potential in the field of network security, especially when dealing with
complex network attacks and defense strategies. However, there are some shortcomings of MARL
in the network security domain. One of the main problems is insufficient training data and performance issues. The behaviors of attackers are usually covert and small in number, so obtaining
reliable training data is a challenge. In addition, attackers may use adversarial samples to spoof
MARL models, leading to model failure. Therefore, it is necessary to address the robustness and
generalization problem of MARL in addition to improving its performance. The correspondence
between this application and RL methods is shown in Table 9.
3.8

Smart Education

Smart education uses the IoT and AI to digitize learning processes and offer individualized learning
experiences and support depending on the learning styles and features of specific students. Sensors
can be used to capture studentsâ€™ learning behaviors and data. Communication enables real-time
, Vol. 1, No. 1, Article . Publication date: May 2023.

18

Z. Zhou, G. Liu and Y. Tang

Table 9. Correspondence between smart education and science and RL methods.

Applications
Smart Education
RL for Science

Papers
[48, 194]
[31, 112]
[171]
[22]
[5]

Methods
DQN-based [131]
DQN-based [131]
DDPG-based [107]
AC-based [88]
PPO-based [169]

SA/MA
SA
SA
SA
SA
MA

interaction between students and teachers, as well as collaborative learning among students. AI
can be used to analyze learning behavior, offer personalized learning, and evaluate teaching. Scene
reconstruction, experiment simulation, and remote teaching are made easier by virtual reality
technology. In MARL-based smart education, we summarize the existing techniques [31, 48, 112,
194]. Education 4.0 intends to incorporate AI technology into each stage of student self-regulated
learning to increase interest and effectiveness during the process [19, 46, 170]. Tang and Hare
[194] create an adaptive tutoring game that allows students to personalize their learning without
the guidance of teachers. In order to optimize student learning, this system uses a Petri net
graph structure to monitor studentsâ€™ progress in the game and an RL agent to adaptively change
system behavior in response to student performance. Then they apply Petri Nets and hierarchical
reinforcement learning algorithm to personalized student assistance based on the above game
[48]. The algorithm can assist teachers in giving students in-game instruction and feedback that is
specifically tailored to them, allowing them to gradually master complex knowledge and skills by
breaking down the tasks in games into several stages. The algorithm can help educators provide
customized support and feedback to students in games and gradually master complex knowledge
and skills by dividing the tasks in games into multiple levels. [112] and [31] both monitor student
learning progress using data gathered by sensors and offer students personalized learning advice
using RL techniques.
Smart Education based on MARL can enhance teaching efficiency, save time, and ultimately, better
learning outcomes for students. However, the collection of daily behavioral data from students
is required by smart education, which presents privacy concerns. Additionally, since the core of
intelligent education is human, its purpose is to assist teachers in teaching and students in learning.
As a result, it necessitates prioritizing switching according to different scenarios, such as when
there are discrepancies between the assessment of the teacher and AI for the level of knowledge
mastery. Improper prioritization switching may lead to reduced educational effectiveness and poor
student experiences. Therefore, how to conduct reasonable prioritization switching is a problem
that needs to be explored.
3.9

RL for Science

Recently, AI for science has been a popular topic, and AI is highly regarded as a critical tool
in achieving scientific progress [127]. RL has demonstrated significant scientific potential, with
particular promise in chemistry, physics, and materials research. RL has proven instrumental in
solving challenges like exploring uncharted physical phenomena. The correspondence between this
application and RL methods is shown in Table 9. Seo et al. [171] utilize RL to control feedforward
ğ›½ in the KSTAR tokamak. Degrave et al. [22] introduce an innovative RL approach that enables
the magnetic control system of a tokamak fusion device to learn autonomously, achieving precise
control over various plasma configurations, significantly reducing design efforts, and representing
a pioneering application of RL to the fusion domain. Bae et al. [5] introduce a scientific MARL
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

19

Safe MARL
Optimization
[38][111][121]

Formal methods
[25][176]

Fig. 2. Categories of Safety in MARL

(SciMARL) for discovering wall models in turbulent flow simulations, dramatically reducing computational cost while reproducing key flow quantities and offering unprecedented capabilities for
simulating turbulent flows. RL scientific research offers more possibilities, and we believe that RL
will have a wider range of scientific applications in the future.
4

VISIONARY PROSPECTS

Although MARL has shown superior performance in many domains, some issues, such as safety,
robustness, and generalization, limit the application of MARL in the real world. We believe that
maximizing the superiority of MARL in practical applications in the future needs to first address
these issues and need to consider the moral constraints of human society. This section reviews the
current state of research in four areas: safety, robustness, generalization, and ethical constraints,
and discusses the gaps that need to be addressed in future research.
4.1

Safety of Multi-agent Reinforcement Learning

The increasing popularity of MARL has brought attention to the need to ensure the safety of these
systems. In MARL, the actions of one agent can potentially cause harm to the task or other agents
involved. Therefore, there is a pressing need to develop safe MARL approaches. To achieve safety
in MARL, one common approach is to add constraints to the training process. By incorporating
safety constraints, agents are encouraged to avoid unsafe actions that could lead to task failure or
harm to other agents. There have been numerous reviews on the safety of RL, as summarized in
[35], [39], and [225]. However, there is currently no systematic review of the safety of MARL, and
there is relatively little research on this topic. In this section, we give a definition of safe MARL
which is used in [38].
Definition 3 (Safe MARL). A multi-agent constrained stochastic game can be modeled as the
tuple
ğ‘ , S, A 1, Â· Â· Â· , A ğ‘ , ğ‘…, C 1, Â· Â· Â· , C ğ‘ , ğ’„ 1, Â· Â· Â· , ğ’„ ğ‘ , ğ‘, ğ›¾ ,
â‰¤ğ‘
where ğ‘… : S Ã— A 1 Ã— Â· Â· Â· Ã— A ğ‘ Ã— S â†’ R is the joint reward function, Cğ‘– = {ğ¶ ğ‘–ğ‘— }ğ‘–1â‰¤
is a set of cost
ğ‘— â‰¤ğ‘šğ‘–

function of agent ğ‘– (ğ‘šğ‘– is the number of cost functions of agent ğ‘–), ğ¶ ğ‘–ğ‘— : S Ã— A 1 Ã— Â· Â· Â· Ã— A ğ‘ Ã— S â†’ R
â‰¤ğ‘
is the cost function, and ğ’„ ğ‘– = {ğ‘ ğ‘–ğ‘— }ğ‘–1â‰¤
âˆˆ R is cost-constraining values.
ğ‘— â‰¤ğ‘šğ‘–
The goal of agents is to maximize the expected total reward while trying to satisfy the safety
constraint of each agent,
"âˆ
#
âˆ‘ï¸
ğ‘¡
J (ğ…) = Eğ…
ğ›¾ ğ‘… (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ‘  0 = ğ‘ ) ,
ğ‘¡ =0

ğ‘ .ğ‘¡ .Jğ‘—ğ‘– (ğ…) = Eğ…

"âˆ
âˆ‘ï¸

#
ğ›¾ ğ‘¡ ğ¶ ğ‘–ğ‘— (ğ‘ ğ‘¡ , ğ’‚ğ‘¡ , ğ‘ ğ‘¡ +1 |ğ‘  0 = ğ‘ )

â‰¤ ğ‘ ğ‘–ğ‘— ,

(11)

ğ‘¡ =0

âˆ€ğ‘— = 1, Â· Â· Â· , ğ‘šğ‘– .
, Vol. 1, No. 1, Article . Publication date: May 2023.

20

Z. Zhou, G. Liu and Y. Tang

We then summarize relevant research from two perspectives: optimization and formal methods, as
shown in Fig.2.
4.1.1 Optimization. Gu et al. [38] introduce Multi-Agent Constrained Policy Optimization (MACPO)
and MAPPO-Lagrangian to devise safety MARL algorithms. These algorithms aim to meet safety
constraints while concurrently enhancing rewards by integrating theories from constrained policy
optimization and multi-agent trust region learning, yielding strong theoretical guarantees. Furthermore, the authors have established a benchmark suite, Safe Multi-Agent MuJoCo, to evaluate
the efficacy of their approaches, which exhibit performance levels comparable to baselines and
persistently comply with safety constraints. Lu et al. [121] propose a method called Safe Decentralized Policy Gradient (Safe Dec-PG) to solve a distributed RL problem where agents work together
with safety constraints. The method is decentralized and considers coupled safety constraints
while ensuring a measurable convergence rate. It can also solve other decentralized optimization
problems. Liu et al. [111] propose a novel algorithm CMIX that can be used for MARL in a partially observable environment with constraints on both peak and average reward. CMIX enables
CTDE and outperforms existing algorithms in maximizing the global reward function subject to
constraints. The algorithm is evaluated on two scenarios, including a blocker game and a vehicular
network routing problem, demonstrating its ability to satisfy both peak and average constraints,
which has not been achieved before in a CTDE learning algorithm.
4.1.2 Formal methods. MARL is being used in safety-critical applications, but current methods
do not guarantee safety during learning. To address this, two approaches for safe MARL have
been presented in [25]: centralized shielding monitors actions of all agents and corrects unsafe
actions, while factored shielding uses multiple shields to monitor subsets of agents concurrently.
Both approaches ensure safety without sacrificing policy quality, but factored shielding is larger
numbers of agents. Sheebaelhamd et al. [176] improve the MADDPG framework for multi-agent
control problems with safety constraints. A safety mechanism is integrated into the deep policy
network to avoid in-feasibility problems in the action correction step, which guarantee constraint
satisfaction using exact penalty functions. Empirical results show that this approach reduces
constraint violations, enabling safety during learning.
4.1.3 Limitations of current methods. Although there has been some progress in researching the
safety of MARL, there are still some limitations. First, the existing approach to MARL safety is
designed for small numbers of agents and may not be applicable to large-scale systems. Second,
most existing research on MARL safety assumes that the environment is static and unchanging. In
real-world applications, however, the environment is often dynamic and unpredictable, which can
pose additional safety risks. Finally, in order to apply MARL to human society, it is necessary to
add constraints to protect human safety. Furthermore, human interactions lead to a non-Markov
environment. Hence, MARL which accounts for the safety of large-scale human society, is a
challenging and significant research direction for the future.
4.2

Robustness of Multi-agent Reinforcement Learning

The robustness of DL in classification tasks has a series of studies [36, 58, 69, 71, 142]. RL is
a sequential decision problem, where misclassification at a one-time step is not equivalent to
expecting the minimum reward. In MARL, a decision failure of any agent can lead to team task
failure, which makes the study of robustness MARL challenging. Furthermore, MARL faces various
challenges in real-world applications, such as uncertainty in the environment, uncertainty policies
of other agents, and sensor noise. All these factors may cause the trained models to perform poorly
or fail. Therefore, it is crucial to improve the robustness of MARL, which will help ensure that the
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

21

Robustness in MARL
Test
Data poisoning
[221]

Backdoor attack
[15][209][214]

Adversarial attack
[41][57][100][108][149]
Train

State observations
[47][50][206][253]

Actions
[29][56][101][151][185]

Rewards and models
[215][221][241]

Adversarial policy
[42][43]

Communications
[187]

Fig. 3. Categories of Robustness in MARL

models can operate stably and reliably in various situations. The following are related definitions
of robust MARL. We use the definition of [253] and [241].
Definition 4 (Robustness against state observations perturbation). A state-adversarial stochastic
game can be defined as a tuple
S, A 1, . . . , A ğ‘ , B 1, . . . , B ğ‘€ , ğ‘… 1, . . . , ğ‘… ğ‘ , ğ‘, ğ›¾
where B ğ‘— is the uncertainty set of adversarial states of agent ğ‘—, and ğ‘€ is the number of attacked
agents such that ğ‘€ â‰¤ ğ‘ .

Given the joint policy ğ… : S â†’ PD A 1 Ã— Â· Â· Â· Ã— A ğ‘ and the joint adversarial perturbation
ğ’— : S â†’ B 1 Ã— Â· Â· Â· Ã— B ğ‘€ , The Bellman equation with fixed ğ… and ğ’— is as follows,


âˆ‘ï¸
âˆ‘ï¸ 
ğ‘‰Ë†âˆ—ğ‘– (ğ‘ ) = ğ‘šğ‘ğ‘¥ ğ‘šğ‘–ğ‘›
ğ… (ğ’‚|ğ‘ , ğ’— (ğ‘ ))
ğ‘ (ğ‘  â€² |ğ‘ , ğ’‚) ğ‘…ğ‘– (ğ‘ , ğ’‚, ğ‘  â€²) + ğ›¾ ğ‘‰Ë†âˆ—ğ‘– (ğ‘  â€²) ,
(12)
ğœ‹ ğ‘– ( Â· |ğ‘ ) ğ‘£
ğ’‚ âˆˆA 1 Ã—Â·Â·Â·Ã—A ğ‘

ğ‘  â€² âˆˆS

Definition 5 (Robustness against model perturbation). A model-adversarial stochastic game can be
defined as the tuple
Ë†ğ›¾ ,
ğ‘ , S, A 1, Â· Â· Â· , A ğ‘ , RÌ‚ 1, Â· Â· Â· , RÌ‚ ğ‘ , ğ’‘,
where RÌ‚ğ‘– and ğ’‘Ë† are the uncertainty sets of reward functions and transition probabilities, respectively.
The Bellman-type equation is as follows:

âˆ‘ï¸
âˆ‘ï¸ 
ğ‘‰Ë†âˆ—ğ‘– (ğ‘ ) = ğ‘šğ‘ğ‘¥ ğ‘šğ‘–ğ‘›
ğ… (ğ’‚|ğ‘ )
ğ‘Ë† (ğ‘  â€² |ğ‘ , ğ’‚) ğ‘…Ë†ğ‘– (ğ‘ , ğ’‚, ğ‘  â€²) + ğ›¾ ğ‘‰Ë†âˆ—ğ‘– (ğ‘  â€²)
ğœ‹ ğ‘– ( Â· |ğ‘ ) ğ‘…Ë†ğ‘– âˆˆ RÌ‚ğ‘– ,ğ‘Ë† âˆˆğ’‘Ë†

ğ’‚ âˆˆA 1 Ã—Â·Â·Â·Ã—A ğ‘

(13)

ğ‘  â€² âˆˆS

Currently, research on the robustness of MARL is being pursued from both attacks and defense.
Attacks research aims to identify stronger perturbations to test the robustness of MARL models,
while defense aims to develop MARL algorithms that are robust to perturbations.
4.2.1 Testing. : As shown in Fig. 3, similar to DL, the robustness testing methods for MARL can be
classified into three categories: adversarial attacks, backdoor attacks, and data poisoning.
Data poisoning: Wu et al. [221] discuss how an attacker can modify rewards in a dataset
used for offline MARL to encourage each agent to adopt a harmful target policy with minimal
modifications. The attacker can establish the target policy as a Markov perfect dominant strategy
equilibrium, which is a strategy that rational agents will adopt. The article explores the effectiveness
of attacks on various MARL agents and their cost compared to separate single-agent attacks. It also
examines the relationship between dataset structure and attack cost and highlights the need for
future research on defense in offline MARL.
Adversarial attacks: Lin et al. [108] show that Cooperative MARL (c-MARL) is vulnerable to
attacks on a single agent. By manipulating agent observations, the attacker reduces the overall team
, Vol. 1, No. 1, Article . Publication date: May 2023.

22

Z. Zhou, G. Liu and Y. Tang

reward. The proposed attack strategy involves training a policy network to induce the victim agent
to take an incorrect action and utilizing targeted adversarial attack methods to compel the agent to
take that action. Experiments demonstrate a significant reduction in team reward and winning rate.
Guo et al. [41] discuss the potential vulnerabilities of c-MARL algorithms to adversarial attacks and
the importance of testing their robustness before deployment in safety-critical applications. The
authors propose MARLSafe, a comprehensive testing framework that considers state, action, and
reward robustness to address this. Experimental results on the SMAC environment demonstrate
the low robustness of advanced c-MARL algorithms in all aspects. Hu and Zhang [57] propose
a sparse adversarial attack on c-MARL systems to test their robustness. The attack is trained
using MARL with regularization and is shown to significantly decrease performance when only
a few agents are attacked at a few timesteps. This highlights the need for more robust cMARL
algorithms. Pham et al. [149] introduce a novel model-based approach for evaluating the robustness
of c-MARL agents against adversarial states. They demonstrate the superiority of their approach
over existing baselines by crafting more robust adversarial state perturbations and employing a
victim-agent selection strategy. Through experiments on multi-agent MuJoCo benchmarks, they
demonstrate that the approach is effective by achieving a reduction in total team rewards. Li et al.
[100] propose the Adversarial Minority Influence attack, which introduces an adversarial agent
that influences other cooperative victims to achieve worst-case cooperation. The attack addresses
the complexity and cooperative nature of c-MARL by characterizing and maximizing the influence
from the adversary to the victims. The proposed approach is demonstrated to be superior to existing
methods in various simulation environments.
Backdoor attack: Chen et al. [15] introduce a novel backdoor attack framework, known as
MARNet, which is specifically designed for c-MARL scenarios. MARNet comprises three primary
modules: trigger design, action poisoning, and reward hacking, all of which work together to
manipulate the actions and rewards of poisoned agents. The framework is evaluated on two popular
c-MARL algorithms, VDN [188] and QMIX [161], in two commonly used c-MARL games. The
experimental results demonstrate that MARNet outperforms baselines from SARL backdoor attacks,
reducing the utility under attack by up to 100%. Although fine-tuning is employed as a defense
mechanism against MARNet, it is not entirely effective in eliminating the impact of the attack. Wang
et al. [214] investigate research on the backdoor attack for DRL-based Autonomous Vehicles (AVs)
controllers. They develop a trigger based on traffic physics principles. Experiments are conducted
on both single-lane and two-lane circuits, and they demonstrate that the attack can cause a crash
or congestion when triggered while maintaining normal operating performance. These findings
underscore the importance of robust security measures in AVs controller design. Wang et al. [209]
examine backdoor attacks in MARL systems and put forward a technique called BACKDOORL to
detect and prevent such attacks.
4.2.2 Training. : Robustness testing and training in MARL are still in the early stages of research.
Therefore, we summarize robustness training methods from five aspects: state observation, action,
reward and model, adversarial policy, and communication.
State Observations: In our previous work[253], we combine a policy gradient function and
an action loss function, along with a regularized action loss term, to develop a new objective
function for training actors in mean-field actor-critic reinforcement learning [227] that improves
its robustness. Furthermore, we define State-Adversarial Stochastic Game (SASG) and discuss its
properties. Due to the traditional solution concepts do not always exist in SASG, [47] and [50]
introduce a new solution concept called robust agent policy and develop a Robust Multi-Agent
Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents. Wang et al.
[206] propose a training framework for c-MARL to address the weakness of agents to adversarial
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

23

attacks. The framework generates adversarial attacks on agent observations to help them learn a
robust cooperative policy. The attacker selects an agent to attack and outputs a perturbation vector.
The victim policy is then trained against the attacker. Experimental results demonstrate that the
generated attacks improve the robustness against observation perturbations.
Actions: Foerster et al. [29] consider how the policies adopted by different agents in the environment interact with each other and affect the learning process of all agents. They propose Learning
with Opponent-Learning Awareness (LOLA), a framework that takes into account the influence
of one agentâ€™s policy on the expected parameter update of the other agents through a specific
term. The method leads to the emergence of cooperation in the iterated dilemma of prisoners
and convergence to the Nash equilibrium in repeated matching pennies. The extension of the
policy gradient estimator enables efficient computation of LOLA, making it suitable for handling
large parameters and input spaces that use nonlinear function approximators. Li et al. [101] design
MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) to train MARL agents with
continuous actions that can handle robustness issues in complex environments. M3DDPG adds a
minimax component to MADDPG [119] and employs multi-agent adversarial learning (MAAL) to
optimize the learning objective. Through experiment evaluation in four multi-agent environments,
the proposed algorithm surpasses existing baselines in terms of performance. Based on [101], Sun
et al. [185] apply the convex relaxation of neural networks instead of MAAL to apply the convex
relaxation of neural networks to overcome computationally difficult, which enables robustness in
interacting with agents that have significantly different behaviors and achieves a certified bound of
the original optimization problem. To overcome the computational difficulties of MAAL, Sun et al.
[185] utilize the convex relaxation technique to guarantee robustness in the interaction of agents
with varying actions and yield a certified bound for the original optimization problem. Phan et
al. [151] propose a value decomposition scheme that trains competing teams of varying sizes to
improve resilience against arbitrary agent changes. By doing so, RADAR offers a more versatile
and flexible approach to MARL that can adapt to changing agent behavior and system conditions.
According to [56], in order to enhance robustness, non-adversarial agents should collaborate and
make decisions based on correlated equilibrium rather than acting independently. The authors
introduce new approaches to encourage agents to learn and follow correlated equilibrium while
maintaining the benefits of decentralized execution.
Rewards and models: Wang and Zou [215] propose a sample-based approach to estimate the
uncertainty set of a misspecified MDP in model-free robust RL. They develop robust Q-learning
and robust TDC algorithms that converge to optimal or stationary points without additional
conditions on the discount factor. The algorithms also have similar convergence rates as their
vanilla counterparts and can be extended to other RL algorithms. Zhang et al. [241] focus on
the problem of MARL in situations where there is uncertainty in the model, such as inaccurate
knowledge of reward functions. They model this as a robust Markov game, where agents aim to
find policies that lead to equilibrium points that are robust to model uncertainty. They present
a novel solution concept known as robust Nash equilibrium and a Q-learning algorithm that
guarantees convergence. Additionally, policy gradients are derived, and an actor-critic algorithm
that uses function approximation is developed to effectively tackle large state-action spaces. Wu et
al. [221] introduce linear programs that can efficiently address the attack problem and analyze the
connection between the characteristics of datasets and the minimal attack cost.
Adversarial policy: Guo et al. [42] propose Backdoor Detection in MARL systems, using Policy
Cleanse to detect and mitigate Trojan agents and their trigger actions. Besides, they also design
a machine unlearning-based approach to effectively mitigate the detected backdoors. In contrast
to previous techniques that rely on the zero-sum assumption, the recent work by Guo et al. [43]
proposes a novel approach that resets the optimization objective and employs a new surrogate
, Vol. 1, No. 1, Article . Publication date: May 2023.

24

Z. Zhou, G. Liu and Y. Tang

Generalization in MARL
Multi-tasks transfer
Hierarchies
[11][199][238]

Meta-learning
[105]

Decentralized Learning
[139][236]

Sim2Real
Domain randomization
[10][177]

Real data
[45]

Others
[196][239]

Fig. 4. Categories of Generalization in MARL

optimization function. This method has been shown through experiments to significantly enhance
the ability of adversarial agents to exploit weaknesses in a given game and take advantage of any
inherent unfairness in the game mechanics. Moreover, agents that are trained adversarially against
this approach have demonstrated a greater level of resistance against adversarial attacks. Overall,
these findings suggest that the proposed approach represents a promising direction for improving
the robustness and fairness of game-playing AI agents.
Communication: A certifiable defense mechanism is proposed by Sun et al. [187], which employs a message-ensemble policy to merge several message sets with random ablations. Theoretical
analysis indicates that this mechanism can withstand various adversarial communications.
4.2.3 Limitations of current methods. Current research on MARL robustness leaves much to be
desired. First, the recent research only focuses on one of the states, actions, or policies and needs
to consider the robustness of a combination of multiple aspects. Second, there needs to be more
team robustness evaluation metrics. It is insufficient to test the robustness of MARL only by way
of attacks because it can only cover some possible perturbations. In addition, existing studies
tend to ignore the impact of non-cooperative and malicious behaviors caused by human factors
on robustness, which is also an issue that needs further research. Therefore, further in-depth
integration of robust MARL with multiple perturbation types, verifiable robustness evaluation
metrics, and robustness with human intervention must be considered in the future.
4.3

Generalization of Multi-agent Reinforcement Learning

Within the domain of MARL, generalization pertains to the capacity of agents to transfer their
learned knowledge and skills from a specific environment or scenario to novel and diverse ones
without necessitating significant modifications or retraining. Several surveys have investigated
generalization in RL [87, 201, 225, 247]. In the generalization of SARL, various techniques such as
domain randomization [133, 160, 165], causal inference [82, 167, 237], and meta-learning [3, 27, 77]
have been employed to address generalization issues. However, compared to single-agent settings,
research on the generalization of MARL remains relatively scarce. In this regard, we provide an
overview of pertinent work from two perspectives, namely multi-task learning, and sim2real, as
shown in Fig. 4.
4.3.1 Multi-tasks transfer. The goal of multi-task learning is to improve the generalization ability
of a model by incorporating knowledge from related tasks as a form of inductive bias. In order to
learn shared and task-specific representations and improve overall performance and efficiency in
complicated domains entails training a model to carry out several tasks at once.
Hierarchies: To address the issue of generalization to unknown opponents in multi-agent
games, Vezhnevets et al. [199] propose a hierarchical agent architecture grounded in game theory,
which enables credit assignment across hierarchy levels and achieves better generalization to unseen
opponents than conventional baselines. Carion et al. [11] propose a structured prediction method
to assign agents to tasks that uses coordination inference procedures and scoring models. Zhang et
al. [238] propose an offline multi-task collaborative reinforcement learning algorithm called ODIS,
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

25

which is able to extract universal coordination skills from offline multi-task data, enabling better
generalization in handling multi-task coordination problems. Specifically, the ODIS algorithm has
a two-step process for improving the generalization and performance of c-MARL tasks. First, it
extracts coordination skills from offline data that are applicable across different tasks. It then uses
these skills to differentiate between different agent behaviors. Second, it trains a coordination policy
that selects the most effective coordination skills using the CTDE paradigm. The effectiveness of
ODIS is demonstrated in experiments where it significantly improves generalization to unseen
tasks, achieving superior performance in various cooperative MARL benchmarks. Importantly, the
ODIS algorithm achieves these results using only limited sources of offline data.
Meta-learning: Liang et al. [105] present a Self-adaptive Meta-learning (SAML) framework that
employs gradient-based methods to combine individual task policies into a unified policy capable of
adapting to new tasks. Experimental results demonstrate that SAML outperforms baseline methods
in terms of efficiency and continuous adaptation.
Decentralized learning: Omidshafiei et al. [139] tackle the challenge of multi-task MARL
with partial observability and limited communication. They introduce a decentralized single-task
learning approach that can be synthesized into a unified policy for multiple correlated tasks without
the need for explicit indication of task identity. The work by Zeng et al. [236] presents a novel
mathematical framework for addressing multi-task RL problems using a policy gradient method.
Specifically, the authors propose a decentralized entropy-regularized policy gradient method for
solving these problems. The efficacy of the proposed method is evaluated through experimental
results on both small-scale and large-scale multi-task RL problems. The findings demonstrate that
the proposed approach offers promising performance for tackling complex multi-task RL problems.
4.3.2 Sim2Real. To train MARL agents, simulations are often used due to their efficiency and ease
of implementation. However, a significant challenge arises when attempting to transfer policies
learned in simulation to the real world, as differences between the two environments can lead to a
performance gap. To address this issue, researchers have been investigating methods for Sim2Real
transfer, which aim to minimize the performance gap between simulation and the real world. These
methods typically involve fine-tuning policies in the real world, using domain randomization to
increase the generalization of policies learned in simulation, or combining real data to achieve
better results.
Domain randomization: Candela et al. [10] create a simulation platform for autonomous
driving and use the MAPPO with domain randomization to enable the transfer of policies from
simulation to reality. In our previous work [177], we developed a simulation platform for multiUAV transport, utilizing domain randomization to facilitate the transfer from simulation to reality.
Additionally, we formulated a non-stationary variant of Markov games and established the efficacy
of RNNs in addressing non-stationary Markov games.
Real data: Gurevich et al. [45] present a novel approach for implementing homogeneous MAS
by transferring data between real and simulated robots. Their method involves designing a deep
neural network architecture called CR-Net, which can simulate the motion of individual robots in
this system. To train the CR-Net in a simulated environment, they generate synthetic data using
a generative model trained on real data from one robot. The effectiveness of their approach is
validated by testing the RL models trained using this method on real ground and underwater
vehicles, which showed successful policy transfer from simulation to reality.
4.3.3 Others. The generalization to unexplored state-action pairs is considered in [196], which uses
tensors of low CP-rank to model the transition and reward functions. Zhang et al. [239] propose
a novel multi-task actor-critic paradigm based on a share critic with knowledge transfer to solve
heterogeneous state-action learning problems.
, Vol. 1, No. 1, Article . Publication date: May 2023.

26

Z. Zhou, G. Liu and Y. Tang

Learning with Ethical Constraint
Privacy
State and action
[18][113][166][198][200][205]

Environment
[141][252]

Reward function
[30][116][154]

Fairness
SARL
[12][23][64][162][217]

MARL
[229][234]

Transparency
MARL
[59][68][256]

Explainability
[53][138][244]

Interpretability
[81][117][128][235]

Fig. 5. Categories of MARL with Ethical Constraint

4.3.4 Limitations of current methods. Current research in multi-agent learning has mainly focused
on generalization in the context of cyber-physical systems, which considers the abstraction of agents
to unknown agents and the differences between virtual and real-world environments. However,
the functionality of human social systems is multifaceted, and human behavior is highly diverse,
making the consideration of the generalization of interactions with humans a crucial research area
for MAS. For instance, in intelligent transportation systems, traffic signal control algorithms based
on MARL must generalize over different cities and time periods. Similarly, in smart education,
personalized education assistance based on MARL needs to consider individual differences in living
environments and personality traits to develop tailored learning plans for students. Hence, MARL
which accounts for the generalization of human behavior, is a promising and challenging research
direction for the future.
4.4

Learning with Ethical Constraint

As AI technology continues to evolve, it is increasingly important to consider the ethical implications
of AI systems [4]. MARL systems involve the interaction of multiple agents whose actions can
have significant real-world. Therefore, it is critical to ensure that the design and training of MARL
systems take ethical considerations into account. We summarize research related to the ethical
constraints of MARL in terms of privacy protection, fairness, and transparency, as shown in Fig.5.
4.4.1 Privacy protection. Privacy protection is a long-standing issue extensively discussed in
machine learning. Some of the main topics and techniques studied in this area include differential privacy, federated learning, cryptography, trusted execution environments, and ML-specific
approaches[21]. The research on privacy protection in RL is still in its early stages. We outline relevant studies in the following areas: the privacy of state and action, environment, reward function,
and MARL scenario.
State and action: Venkitasubramaniam [198] proposes an MDP to explore the development of
controller actions while satisfying privacy requirements. They analyze the balance between the
achievable privacy level and system utility using analytical methods. The optimization problem is
formulated as a Bellman equation which owns the convex reward functions for a certain category
of MDPs, and as a POMDP with belief-dependent rewards for the general MDP with privacy
constraints. Differentially private algorithms are used in protecting reward information by Wang
et al .[205] for RL in continuous spaces. The authors propose a method for protecting the value
function approximator, which is realized by incorporating functional noise iterative into the training.
They provide rigorous privacy guarantees and gain insight into the approximate optimality of the
algorithm. Experiments show improvement over existing approaches. Vietri et al. [200] use the
notion of Joint Differential Privacy (JDP) and a private optimism-based learning method to address
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

27

the privacy problem for episodic RL. Chowdhury et al. [18] design two frameworks, i.e., policy
optimization and value iteration, and not only consider the JDP but Local Differential Privacy (LDP)
in finite horizon tabular MDP to minimize regret. The previous text describes the use of differential
privacy as a means of protecting sensitive user data in RL. There are also other methods of protecting
user privacy, such as cryptographic techniques. Sakuma et al. [166] use a homomorphic encryption
algorithm to realize the privacy protection of distributed RL. They divide private information based
on time and observation, design a sarsa privacy protection method based on random actions for
these two division methods, and extend these to Q-learning based on greedy and ğœ–-greedy action
selections. A new privacy-preserving RL method is proposed by Liu et al. [113] named Preyer to
provide treatment options for patients while protecting their privacy. Preyer is composed of an
innovative encrypted data format, a secure mechanism for plaintext length management, and a
privacy-preserving RL with experience replay.
Environment: Pan et al. [141] first investigate the privacy in RL environment. They propose two
methods based on genetic algorithms and shadow policies, respectively. Zhou [252] first discusses
how to achieve privacy protection in finite-horizon MDPs, which have large state and action spaces.
The author proposes two privacy-preserving RL algorithms according to value iteration and policy
optimization and proves that they can achieve sub-linear regret performance while ensuring privacy
protection.
Reward: Fu et al. [30] and Prakash et al. [154] investigate the problem of how to preserve
the privacy of reward functions in reinforcement learning by employing adversarial reward and
inverse reinforcement learning techniques. Liu et al. [116] studies privacy-preserving RL using
dissimulation to hide the true reward function. Two models are presented and evaluated through
computational and human experiments, showing that resulting policies are deceptive and make it
more difficult for observers to determine the true reward function.
MARL: The differential privacy is used by Ye et al. [229] in the field of multi-agent planning
for the first time to protect agent privacy. Based on differential privacy, they propose a new
strong privacy-preserving planning method, which can not only ensure strong privacy but also
control communication overhead. Yuan et al. [234] delve into the issue of integrating Cooperative
Intelligence (CI) to enhance the efficiency of communication networks, which is hampered by
privacy concerns and practical limitations in communication. In response, the authors present a
Privacy-preserving scheme based on MARL (PP-MARL) that employs a HE-friendly architecture.
Experiment results indicate that PP-MARL exhibits better performance in privacy protection and
reduced overhead compared to state-of-the-art approaches. Nonetheless, preserving privacy in
CI-enabled communication networks remains a formidable challenge, especially when the number
of agents involved is subject to variation or the system scales up. The research on privacy protection
for MARL is still limited, and some studies have explored the use of differential privacy techniques
to enhance the performance of MARL [16, 228] or against malicious advise [230].
4.4.2 Fairness. Fairness in machine learning refers to the concern that machine learning models
and algorithms should not discriminate or create bias against certain groups of people based on
their protected characteristics, such as race, gender, age, religion, etc. The review paper [147]
provides a comprehensive summary of existing techniques. However, research on fairness in RL is
still limited, and we provide an overview from both single-agent and multi-agent perspectives.
SARL: Jabbari et al.[64] first consider the fairness in RL and demonstrate that an algorithm
conforming to their fairness constraint requires an exponential amount of time to achieve a nontrivial approximation to the optimal policy. To overcome this challenge, they introduce a polynomial
time algorithm that satisfies an approximate form of the fairness constraint. Weng et al. [217]
address the issue of complete unfairness for some users or stakeholders by using a social welfare
, Vol. 1, No. 1, Article . Publication date: May 2023.

28

Z. Zhou, G. Liu and Y. Tang

function encoded with fairness. Chen et al. [12] introduce a novel approach to incorporate fairness
in actor-critic RL for network optimization problems. By considering the shape of the fairness
utility function and past reward statistics, their proposed algorithm adjusts the rewards using a
weight factor that is dependent on both of these factors. Ren et al. [162] propose a novel framework
to obtain optimum and relative fairness solutions in space applications, including a new image
quality representation method, a finite MDP model, and an algorithm based on RL. Deng et al. [23]
propose an RL algorithm that enforces stepwise fairness constraints to ensure group fairness at
every time step.
MARL: Jiang and Lu [68] propose a hierarchical RL model, named FEN, which is aimed at both
obtaining fairness and efficiency objectives. FEN decomposes fairness for each agent and utilizes a
structure with a high-level controller and multiple sub-policies to avoid multi-objective conflict. The
study by Zimmer et al. [256] also focuses on the two aspects of fairness and efficiency. They propose
a generic neural network architecture to address this problem, which consists of two sub-networks
specifically designed to consider the two aspects of fairness and can be implemented in centralized
training and decentralized execution or fully decentralized MARL settings. In multi-intersection
scenarios, Huang et al. [59] propose a novel fairness-aware model-based MARL (FM2Light) to deal
with unfair control with superior reward design.
4.4.3 Transparency. Transparency is essential for building reliable MARL decision systems. Decisionmaking interactions among multiple agents are very complex and difficult to understand and explain.
Without a transparent understanding of the interactions and decision-making processes among
agents, the reliability and trustworthiness of the system are affected. Therefore, studying the
transparency of MARL is an important direction. We summarize it in terms of both explainability
and interpretability.
Explainability refers to the ability of a model in machine learning to provide a rationale for its
outputs that can be easily comprehended and trusted by humans [155, 203]. Heuillet et al. [53] use
a game theory concept of shapley values to explain the contribution of one agent in MARL and
use Monte Carlo sampling to approximate shapley values to overcome the high overhead. This
method provides an explanation for the model but can not give the precise reason why the action
is taken by the agent. Ohana et al. [138] also use shapley values to understand the model behavior
and explain local feature contributions. Zhang et al. [244] propose a framework composed of a
variational autoencoder and graph neural networks to encode the interactions between pairs of
agents.
Interpretability refers to the ability of a human to understand and explain the inner workings
of a machine learning model [155, 203]. Kazhdan et al. [81] develop a library named MARLeME
which uses symbolic models to improve the interpretability of MARL. It can be employed across a
broad spectrum of existing MARL systems and has potential applications in safety-critical domains.
Liu et al. [117] propose a novel interpretable architecture based on soft decision trees with recurrent
structure. Milani et al. [128] propose two frameworks (IVIPER and MAVIPER) to extract interpretable
coordination policies of MARL in sight of the decision tree. Zabounidis et al. [235] incorporate
interpretable concepts from domain experts into MARL models trained. This approach improves
interpretability, allows experts to understand which high-level concepts are used by the policy, and
intervenes to improve performance.
MARL for decision transparency involves not only the transparency of single-agent decisions
but also the study of complex interactions among multiple agents. Currently, although there have
some related research works, it is still relatively small, and more research works are needed to
explore how to make MARL more transparent for better application to real-world problems.
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

5

29

CHALLENGES ON HUMAN-COMPATIBLE MULTI-AGENT REINFORCEMENT
LEARNING

The Human-Cyber-Physical System (HCPS) is developed based on the Cyber-Physical System (CPS)
and integrates computer science, automatic technology, communication science, etc [9, 115]. The
applications of MARL summarized in Section 3 of this paper are typical of HCPS. Humans are seen
as an essential component of HCPS. Therefore, the design of MARL algorithms needs to take into
account the human factor. In addition to the challenges of scalability and non-stationary, MARL in
HCPS faces many additional challenges due to the interactions between humans, physical systems,
and computer systems.
5.1

Non-stationarity due to Human Intervention

Non-stationarity refers to the dynamic changes in the environment or the behavior of agents over
time. The existing MARL is based on SG, where the number of agents remains constant during the
training process. Currently, research on non-stationarity in MARL is limited to the CPS level, only
considering the non-stationarity caused by changes in agent policies on the overall environment[].
However, in HCPS, humans interact continuously with the CPS, and human behavior can affect the
dynamic changes in the CPS system. In addition, the reward function for MARL agents is defined
by human experts. Human needs will change with social progress, and the reward function for
MARL agents will change accordingly. This is also an essential factor causing non-stationarity in
HCPS. How to design stable MARL algorithms against human intervention is a vital challenge.
5.2

Diversity of Human Behavior

Human behavior is diverse due to the influence of different geographies, cultures, and beliefs.
In HCPS, MARL needs to model human behavior in order to better achieve intelligence in interaction with humans. The quality of understanding human behavior predominantly affects the
user experience of CPS. For example, in intelligent education, MARL agents need to understand
student behavior well to better recommend personalized services for different students. However,
the diversity of behavior makes this process very challenging. The current research for modeling
human behavior is limited to the societal level and only takes into account human behavior, not the
possible influence of machine intelligence on human behavior. How to consider the influence of
machines on human behavior in the process of modeling human behavior is a significant challenge.
5.3

Complex Heterogeneity of HCPS

The complexity of HCPS manifests itself in various aspects, including human heterogeneity, physical
system heterogeneity, cyber system heterogeneity, and temporal heterogeneity. Human heterogeneity refers to the diversity of human behavior and the different roles played by humans in systems
with different functions. Physical system heterogeneity refers to the variety of sensors used, such
as GPS and cameras in UAV transportation systems. Cyber system heterogeneity is composed of
various software, hardware, and algorithms, which require the integration of multiple intelligent
algorithms due to the complexity of multi-source information and multi-task decision-making.
This cannot be achieved by a single end-to-end algorithm. Finally, temporal heterogeneity is when
making decisions; MARL agents require defining different time intervals based on the actual situation at each time step. How to design MARL algorithms to handle the decision-making process of
complex heterogeneous HCPS is an enormous challenge.
, Vol. 1, No. 1, Article . Publication date: May 2023.

30

5.4

Z. Zhou, G. Liu and Y. Tang

Scalability of Multi-human and Multi-machine

HCPS is a complex system of multi-human and multi-machine coexistence. Thus, MARL used for
intelligent decision-making should have strong scalability, and the agent here should have a broad
concept that includes both humans and intelligent machines. However, as the number of agents
increases, the joint action space of agents grows exponentially, which makes the scalability of MARL
algorithms poor. Existing research only focuses on the scalability of the number of machines without
considering human factors. Designing scalable multi-agent reinforcement learning algorithms that
are suitable for complex and heterogeneous HCPS is a significant challenge.
6

CONCLUSION

This paper summarizes the fundamental methods of MARL and reviews its relevant research in
various fields, such as smart transportation, unmanned aerial vehicles, intelligent information
system, public health and intelligent medical diagnosis, smart manufacturing, financial trade,
network security, smart education, and RL for science. In order to better serve human society,
it is necessary to develop a trustworthy MARL. Therefore, we define trustworthy MARL from
the perspectives of safety, robustness, generalization, and ethical constraints and summarize the
current research and limitations in these areas. Finally, we discuss the additional challenges when
considering HCPS in MARL, which is crucial for its practical application in human society. We hope
this paper can provide a comprehensive review of various research approaches and application
scenarios, encouraging and promoting the application of MARL in human societies for better
service to humans.
REFERENCES
[1] Akash Agrawal, Sung Jun Won, Tushar Sharma, Mayuri Deshpande, and Christopher McComb. 2021. A MULTI-AGENT
REINFORCEMENT LEARNING FRAMEWORK FOR INTELLIGENT MANUFACTURING WITH AUTONOMOUS
MOBILE ROBOTS. Proceedings of the Design Society 1 (2021), 161â€“170. https://doi.org/10.1017/pds.2021.17
[2] Hanane Allioui, Mazin Abed Mohammed, Narjes Benameur, Belal Al-Khateeb, Karrar Hameed Abdulkareem, Begonya
Garcia-Zapirain, Robertas DamaÅ¡eviÄius, and Rytis MaskeliuÌ„nas. 2022. A Multi-Agent Deep Reinforcement Learning
Approach for Enhancement of COVID-19 CT Image Segmentation. Journal of Personalized Medicine 12, 2 (2022).
https://doi.org/10.3390/jpm12020309
[3] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. 2020. Meta Reinforcement Learning for Simto-real Domain Adaptation. In 2020 IEEE International Conference on Robotics and Automation (ICRA). 2725â€“2731.
https://doi.org/10.1109/ICRA40945.2020.9196540
[4] Mona Ashok, Rohit Madan, Anton Joha, and Uthayasankar Sivarajah. 2022. Ethical framework for Artificial Intelligence
and Digital technologies. International Journal of Information Management 62 (2022), 102433. https://doi.org/10.
1016/j.ijinfomgt.2021.102433
[5] H. Jane Bae and Petros Koumoutsakos. 2022. Scientific multi-agent reinforcement learning for wall-models of
turbulent flows. Nature Communications 13, 1 (17 Mar 2022), 1443. https://doi.org/10.1038/s41467-022-28957-7
[6] Javier Bajo, MarÃ­a L. Borrajo, Juan F. De Paz, Juan M. Corchado, and MarÃ­a A. Pellicer. 2012. A multi-agent system for
web-based risk management in small and medium business. Expert Systems with Applications 39, 8 (2012), 6921â€“6931.
https://doi.org/10.1016/j.eswa.2012.01.001
[7] Wenhang Bao. 2019. Fairness in multi-agent reinforcement learning for stock trading. arXiv preprint arXiv:2001.00918
(2019).
[8] Sushrut Bhalla, Sriram Ganapathi Subramanian, and Mark Crowley. 2020. Deep Multi Agent Reinforcement Learning
for Autonomous Driving. In Advances in Artificial Intelligence, Cyril Goutte and Xiaodan Zhu (Eds.). Springer
International Publishing, Cham, 67â€“78.
[9] Alexandros Bousdekis, Dimitris Apostolou, and Gregoris Mentzas. 2020. A human cyber physical system framework
for operator 4.0 â€“ artificial intelligence symbiosis. Manufacturing Letters 25 (2020), 10â€“15. https://doi.org/10.1016/j.
mfglet.2020.06.001
[10] Eduardo Candela, Leandro Parada, Luis Marques, Tiberiu-Andrei Georgescu, Yiannis Demiris, and Panagiotis Angeloudis. 2022. Transferring Multi-Agent Reinforcement Learning Policies for Autonomous Driving using Simto-Real. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 8814â€“8820. https:
, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

31

//doi.org/10.1109/IROS47612.2022.9981319
[11] Nicolas Carion, Nicolas Usunier, Gabriel Synnaeve, and Alessandro Lazaric. 2019. A Structured Prediction Approach
for Generalization in Cooperative Multi-Agent Reinforcement Learning. In Advances in Neural Information Processing
Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran
Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/3c3c139bd8467c1587a41081ad78045e-Paper.pdf
[12] Jingdi Chen, Yimeng Wang, and Tian Lan. 2021. Bringing Fairness to Actor-Critic Reinforcement Learning for
Network Utility Optimization. In IEEE INFOCOM 2021 - IEEE Conference on Computer Communications. 1â€“10. https:
//doi.org/10.1109/INFOCOM42981.2021.9488823
[13] Lu Chen, Runzhe Yang, Cheng Chang, Zihao Ye, Xiang Zhou, and Kai Yu. 2017. On-line Dialogue Policy Learning
with Companion Teaching. In Proceedings of the 15th Conference of the European Chapter of the Association for
Computational Linguistics: Volume 2, Short Papers. Association for Computational Linguistics, Valencia, Spain, 198â€“204.
https://aclanthology.org/E17-2032
[14] Siying Chen, Minghui Liu, Pan Deng, Jiali Deng, Yi Yuan, Xuan Cheng, Tianshu Xie, Libo Xie, Wei Zhang, Haigang
Gong, Xiaomin Wang, Lifeng Xu, Hong Pu, and Ming Liu. 2022. Reinforcement Learning Based Diagnosis and
Prediction for COVID-19 by Optimizing a Mixed Cost Function From CT Images. IEEE Journal of Biomedical and
Health Informatics 26, 11 (2022), 5344â€“5354. https://doi.org/10.1109/JBHI.2022.3197666
[15] Yanjiao Chen, Zhicong Zheng, and Xueluan Gong. 2022. MARNET: Backdoor Attacks against Value-Decomposition
Multi-Agent Reinforcement Learning. https://openreview.net/forum?id=-VsGCG_AQ69
[16] Zishuo Cheng, Dayong Ye, Tianqing Zhu, Wanlei Zhou, Philip S Yu, and Congcong Zhu. 2022. Multi-agent reinforcement learning via knowledge transfer with differentially private noise. International Journal of Intelligent Systems 37,
1 (2022), 799â€“828.
[17] Ankur Chowdhary, Dijiang Huang, Abdulhakim Sabur, Neha Vadnere, Myong Kang, and Bruce Montrose. 2021.
SDN-based Moving Target Defense using Multi-agent Reinforcement Learning. In Proceedings of the first International
Conference on Autonomous Intelligent Cyber defense Agents (AICA 2021). Paris, France, 15â€“16.
[18] Sayak Ray Chowdhury and Xingyu Zhou. 2022. Differentially Private Regret Minimization in Episodic Markov
Decision Processes. Proceedings of the AAAI Conference on Artificial Intelligence 36, 6 (Jun. 2022), 6375â€“6383. https:
//doi.org/10.1609/aaai.v36i6.20588
[19] Monica Ionita Ciolacu, Leon Binder, and Heribert Popp. 2019. Enabling IoT in Education 4.0 with BioSensors from
Wearables and Artificial Intelligence. In 2019 IEEE 25th International Symposium for Design and Technology in Electronic
Packaging (SIITME). 17â€“24. https://doi.org/10.1109/SIITME47687.2019.8990763
[20] Kai Cui, Anam Tahir, Gizem Ekinci, Ahmed Elshamanhory, Yannick Eich, Mengguang Li, and Heinz Koeppl. 2022. A Survey on Large-Population Systems and Scalable Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2209.03859
(2022).
[21] Emiliano De Cristofaro. 2020. An overview of privacy in machine learning. arXiv preprint arXiv:2005.08679 (2020).
[22] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds,
Roland Hafner, Abbas Abdolmaleki, Diego de las Casas, Craig Donner, Leslie Fritz, Cristian Galperti, Andrea Huber,
James Keeling, Maria Tsimpoukelli, Jackie Kay, Antoine Merle, Jean-Marc Moret, Seb Noury, Federico Pesamosca,
David Pfau, Olivier Sauter, Cristian Sommariva, Stefano Coda, Basil Duval, Ambrogio Fasoli, Pushmeet Kohli, Koray
Kavukcuoglu, Demis Hassabis, and Martin Riedmiller. 2022. Magnetic control of tokamak plasmas through deep
reinforcement learning. Nature 602, 7897 (01 Feb 2022), 414â€“419. https://doi.org/10.1038/s41586-021-04301-9
[23] Zhun Deng, He Sun, Zhiwei Steven Wu, Linjun Zhang, and David C Parkes. 2022. Reinforcement Learning with
Stepwise Fairness Constraints. arXiv preprint arXiv:2211.03994 (2022).
[24] Yali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and Haifeng Zhang. 2021. Learning
Correlated Communication Topology in Multi-Agent Reinforcement Learning. In Proceedings of the 20th International
Conference on Autonomous Agents and MultiAgent Systems (Virtual Event, United Kingdom) (AAMAS â€™21). International
Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 456â€“464.
[25] Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Amato, RÃ¼diger Ehlers, Ufuk Topcu, and Lu Feng. 2021. Safe
multi-agent reinforcement learning via shielding. arXiv preprint arXiv:2101.11196 (2021).
[26] Mehdi Esnaashari and Amir Hossein Damia. 2021. Automation of software test data generation using genetic
algorithm and reinforcement learning. Expert Systems with Applications 183 (2021), 115446. https://doi.org/10.1016/j.
eswa.2021.115446
[27] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation of Deep
Networks. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 1126â€“1135. https://proceedings.mlr.press/v70/
finn17a.html
[28] Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. 2016. Learning to Communicate
with Deep Multi-Agent Reinforcement Learning. In Advances in Neural Information Processing Systems, D. Lee,

, Vol. 1, No. 1, Article . Publication date: May 2023.

32

Z. Zhou, G. Liu and Y. Tang

M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc. https://proceedings.neurips.
cc/paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf
[29] Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. 2017.
Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326 (2017).
[30] Justin Fu, Katie Luo, and Sergey Levine. 2017. Learning robust rewards with adversarial inverse reinforcement
learning. arXiv preprint arXiv:1710.11248 (2017).
[31] Siyong Fu. 2022. A Reinforcement Learning-Based Smart Educational Environment for Higher Education. International
Journal of e-Collaboration (IJeC) 19, 6 (2022), 1â€“17.
[32] Sriram Ganapathi Subramanian, Pascal Poupart, Matthew E. Taylor, and Nidhi Hegde. 2020. Multi Type Mean Field
Reinforcement Learning. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent
Systems (Auckland, New Zealand) (AAMAS â€™20). International Foundation for Autonomous Agents and Multiagent
Systems, Richland, SC, 411â€“419.
[33] Sriram Ganapathi Subramanian, Matthew E. Taylor, Mark Crowley, and Pascal Poupart. 2021. Partially Observable
Mean Field Reinforcement Learning. In Proceedings of the 20th International Conference on Autonomous Agents and
MultiAgent Systems (Virtual Event, United Kingdom) (AAMAS â€™21). International Foundation for Autonomous Agents
and Multiagent Systems, Richland, SC, 537â€“545.
[34] Sumitra Ganesh, Nelson Vadori, Mengda Xu, Hua Zheng, Prashant Reddy, and Manuela Veloso. 2019. Reinforcement
learning for market making in a multi-agent dealer market. arXiv preprint arXiv:1911.05892 (2019).
[35] Javier GarcÃ­a, Fern, and o FernÃ¡ndez. 2015. A Comprehensive Survey on Safe Reinforcement Learning. Journal of
Machine Learning Research 16, 42 (2015), 1437â€“1480. http://jmlr.org/papers/v16/garcia15a.html
[36] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples.
arXiv preprint arXiv:1412.6572 (2014).
[37] St John Grimbly, Jonathan Shock, and Arnu Pretorius. 2021. Causal multi-agent reinforcement learning: Review and
open problems. arXiv preprint arXiv:2111.06721 (2021).
[38] Shangding Gu, Jakub Grudzien Kuba, Munning Wen, Ruiqing Chen, Ziyan Wang, Zheng Tian, Jun Wang, Alois Knoll,
and Yaodong Yang. 2021. Multi-agent constrained policy optimisation. arXiv preprint arXiv:2110.02793 (2021).
[39] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. 2022. A
review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330 (2022).
[40] Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua Zhou, and Xuanjing Huang. 2019. Mention
Recommendation in Twitter with Cooperative Multi-Agent Reinforcement Learning (SIGIRâ€™19). Association for
Computing Machinery, New York, NY, USA, 535â€“544. https://doi.org/10.1145/3331184.3331237
[41] Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, and Simin Li. 2022. Towards Comprehensive Testing on
the Robustness of Cooperative Multi-Agent Reinforcement Learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops. 115â€“122.
[42] Junfeng Guo, Ang Li, and Cong Liu. 2022. Backdoor detection in reinforcement learning. arXiv preprint arXiv:2202.03609
(2022).
[43] Wenbo Guo, Xian Wu, Sui Huang, and Xinyu Xing. 2021. Adversarial Policy Learning in Two-player Competitive
Games. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 3910â€“3919. https://proceedings.mlr.press/v139/
guo21b.html
[44] Nikunj Gupta, G Srinivasaraghavan, Swarup Kumar Mohalik, and Matthew E Taylor. 2021. Hammer: Multi-level
coordination of reinforcement learning agents via learned messaging. arXiv preprint arXiv:2102.00824 (2021).
[45] Anton Gurevich, Eran Bamani, and Avishai Sintov. 2022. Real-to-Sim-to-Real: Learning Models for Homogeneous
Multi-Agent Systems. https://eranbamani.github.io/eranbamani (2022).
[46] Bernhard Haderer and Monica Ciolacu. 2022. Education 4.0: Artificial Intelligence Assisted Task- and Time Planning
System. Procedia Computer Science 200 (2022), 1328â€“1337. https://doi.org/10.1016/j.procs.2022.01.334 3rd International
Conference on Industry 4.0 and Smart Manufacturing.
[47] Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, and Fei Miao. 2022. What is the Solution for State
Adversarial Multi-Agent Reinforcement Learning? arXiv preprint arXiv:2212.02705 (2022).
[48] Ryan Hare and Ying Tang. 2022. Petri Nets and Hierarchical Reinforcement Learning for Personalized Student
Assistance in Serious Games. In 2022 International Conference on Cyber-Physical Social Intelligence (ICCSI). 733â€“737.
https://doi.org/10.1109/ICCSI55536.2022.9970680
[49] Fei-Fan He, Chiao-Ting Chen, and Szu-Hao Huang. 2023. A multi-agent virtual market model for generalization in
reinforcement learning based trading strategies. Applied Soft Computing 134 (2023), 109985. https://doi.org/10.1016/j.
asoc.2023.109985
[50] Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. 2023. Robust Multi-Agent Reinforcement
Learning with State Uncertainties. https://openreview.net/forum?id=Rl4ihTreFnV

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

33

[51] Xu HE, Bo An, Yanghua Li, Haikai Chen, Rundong Wang, Xinrun Wang, Runsheng Yu, Xin Li, and Zhirong Wang.
2020. Learning to Collaborate in Multi-Module Recommendation via Multi-Agent Reinforcement Learning without
Communication. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys
â€™20). Association for Computing Machinery, New York, NY, USA, 210â€“219. https://doi.org/10.1145/3383313.3412233
[52] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. 2018. Is multiagent deep reinforcement learning the
answer or the question? A brief survey. learning 21 (2018), 22.
[53] Alexandre Heuillet, Fabien Couthouis, and Natalia DÃ­az-RodrÃ­guez. 2022. Collective eXplainable AI: Explaining
Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning With Shapley Values. IEEE
Computational Intelligence Magazine 17, 1 (2022), 59â€“71. https://doi.org/10.1109/MCI.2021.3129959
[54] Ying-Feng Hsu and Morito Matsuoka. 2020. A Deep Reinforcement Learning Approach for Anomaly Network
Intrusion Detection System. In 2020 IEEE 9th International Conference on Cloud Networking (CloudNet). 1â€“6. https:
//doi.org/10.1109/CloudNet51028.2020.9335796
[55] Guangzheng Hu, Yuanheng Zhu, Dongbin Zhao, Mengchen Zhao, and Jianye Hao. 2020. Event-triggered multi-agent
reinforcement learning with communication under limited-bandwidth constraint. arXiv preprint arXiv:2010.04978
(2020).
[56] Yizheng Hu, Kun Shao, Dong Li, Jianye HAO, Wulong Liu, Yaodong Yang, Jun Wang, and Zhanxing Zhu. 2021.
Robust Multi-Agent Reinforcement Learning Driven by Correlated Equilibrium. https://openreview.net/forum?id=
JvPsKam58LX
[57] Yizheng Hu and Zhihua Zhang. 2022. Sparse adversarial attack in multi-agent reinforcement learning. arXiv preprint
arXiv:2205.09362 (2022).
[58] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. 2017. Adversarial attacks on neural
network policies. arXiv preprint arXiv:1702.02284 (2017).
[59] Xingshuai Huang, Di Wu, and Benoit Boulet. 2023. Fairness-Aware Model-Based Multi-Agent Reinforcement Learning
for Traffic Signal Control. https://openreview.net/forum?id=sy0PqUr2fq9
[60] Zhenhan Huang and Fumihide Tanaka. 2022. MSPM: A modularized and scalable multi-agent reinforcement learningbased system for financial portfolio management. Plos one 17, 2 (2022), e0263689.
[61] Zhiyu Huang, Jingda Wu, and Chen Lv. 2022. Efficient Deep Reinforcement Learning With Imitative Expert Priors for
Autonomous Driving. IEEE Transactions on Neural Networks and Learning Systems (2022), 1â€“13. https://doi.org/10.
1109/TNNLS.2022.3142822
[62] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha, Dinh Thai Hoang, and Dusit
Niyato. 2022. Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning. IEEE
Transactions on Artificial Intelligence 3, 2 (2022), 90â€“109. https://doi.org/10.1109/TAI.2021.3111139
[63] Shariq Iqbal and Fei Sha. 2019. Actor-Attention-Critic for Multi-Agent Reinforcement Learning. In Proceedings of
the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika
Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2961â€“2970. https://proceedings.mlr.press/v97/iqbal19a.html
[64] Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. 2017. Fairness in Reinforcement
Learning. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 1617â€“1626. https://proceedings.mlr.press/v70/
jabbari17a.html
[65] Seyed Mohammad Jafar Jalali, Milad Ahmadian, Sajad Ahmadian, Abbas Khosravi, Mamoun Alazab, and Saeid
Nahavandi. 2021. An oppositional-Cauchy based GSK evolutionary algorithm with a novel deep ensemble reinforcement learning strategy for COVID-19 diagnosis. Applied Soft Computing 111 (2021), 107675. https:
//doi.org/10.1016/j.asoc.2021.107675
[66] Sangwoo Jeon, Hoeun Lee, Vishnu Kumar Kaliappan, Tuan Anh Nguyen, Hyungeun Jo, Hyeonseo Cho, and Dugki
Min. 2022. Multiagent Reinforcement Learning Based on Fusion-Multiactor-Attention-Critic for Multiple-UnmannedAerial-Vehicle Navigation Control. Energies 15, 19 (2022). https://doi.org/10.3390/en15197426
[67] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. 2020. Graph Convolutional Reinforcement Learning. In
International Conference on Learning Representations. https://openreview.net/forum?id=HkxdQkSYDB
[68] Jiechuan Jiang and Zongqing Lu. 2019. Learning Fairness in Multi-Agent Systems. In Advances in Neural Information
Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/10493aa88605cad5ab4752b04a63d172-Paper.
pdf
[69] Yang Jiao, Kai Yang, and Dongjin Song. 2022. Distributed Distributionally Robust Optimization with Non-Convex
Objectives. arXiv preprint arXiv:2210.07588 (2022).
[70] Yang Jiao, Kai Yang, Dongjing Song, and Dacheng Tao. 2022. TimeAutoAD: Autonomous Anomaly Detection With
Self-Supervised Contrastive Loss for Multivariate Time Series. IEEE Transactions on Network Science and Engineering
9, 3 (2022), 1604â€“1619. https://doi.org/10.1109/TNSE.2022.3148276

, Vol. 1, No. 1, Article . Publication date: May 2023.

34

Z. Zhou, G. Liu and Y. Tang

[71] Yang Jiao, Kai Yang, Tiancheng Wu, Dongjin Song, and Chengtao Jian. 2022. Asynchronous Distributed Bilevel
Optimization. arXiv preprint arXiv:2212.10048 (2022).
[72] Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. 2018. Real-Time Bidding with Multi-Agent
Reinforcement Learning in Display Advertising. In Proceedings of the 27th ACM International Conference on Information
and Knowledge Management (Torino, Italy) (CIKM â€™18). Association for Computing Machinery, New York, NY, USA,
2193â€“2201. https://doi.org/10.1145/3269206.3272021
[73] Xuan Jing, Xifan Yao, Min Liu, and Jiajun Zhou. 2022. Multi-agent reinforcement learning based on graph convolutional
network for flexible job shop scheduling. Journal of Intelligent Manufacturing (12 Oct 2022). https://doi.org/10.1007/
s10845-022-02037-5
[74] Hyungeun Jo, Hoeun Lee, Sangwoo Jeon, Vishnu Kumar Kaliappan, Tuan Anh Nguyen, Dugki Min, and Jae-Woo Lee.
2023. Multi-agent Reinforcement Learning-Based UAS Control for Logistics Environments. In The Proceedings of the
2021 Asia-Pacific International Symposium on Aerospace Technology (APISAT 2021), Volume 2, Sangchul Lee, Cheolheui
Han, Jeong-Yeol Choi, Seungkeun Kim, and Jeong Ho Kim (Eds.). Springer Nature Singapore, Singapore, 963â€“972.
[75] Kyle D Julian and Mykel J Kochenderfer. 2019. Distributed wildfire surveillance with autonomous aircraft using deep
reinforcement learning. Journal of Guidance, Control, and Dynamics 42, 8 (2019), 1768â€“1778.
[76] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. 1996. Reinforcement learning: A survey. Journal of
artificial intelligence research 4 (1996), 237â€“285.
[77] Christos Kaplanis, Murray Shanahan, and Claudia Clopath. 2018. Continual Reinforcement Learning with Complex
Synapses. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.). PMLR, 2497â€“2506. https://proceedings.mlr.press/v80/
kaplanis18a.html
[78] Sanyam Kapoor. 2018. Multi-agent reinforcement learning: A report on challenges and approaches. arXiv preprint
arXiv:1807.09427 (2018).
[79] MichaÃ«l Karpe, Jin Fang, Zhongyao Ma, and Chen Wang. 2021. Multi-Agent Reinforcement Learning in a Realistic
Limit Order Book Market Simulation. In Proceedings of the First ACM International Conference on AI in Finance
(New York, New York) (ICAIF â€™20). Association for Computing Machinery, New York, NY, USA, Article 30, 7 pages.
https://doi.org/10.1145/3383455.3422570
[80] Klemens Kasseroller, Franz Thaler, Christian Payer, and Darko Å tern. 2021. Collaborative Multi-agent Reinforcement
Learning for Landmark Localization Using Continuous Action Space. In Information Processing in Medical Imaging,
Aasa Feragen, Stefan Sommer, Julia Schnabel, and Mads Nielsen (Eds.). Springer International Publishing, Cham,
767â€“778.
[81] Dmitry Kazhdan, Zohreh Shams, and Pietro Lio. 2020. MARLeME: A Multi-Agent Reinforcement Learning Model
Extraction Library. In 2020 International Joint Conference on Neural Networks (IJCNN). 1â€“8. https://doi.org/10.1109/
IJCNN48605.2020.9207564
[82] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard SchÃ¶lkopf, Michael C
Mozer, Chris Pal, and Yoshua Bengio. 2019. Learning neural causal models from unknown interventions. arXiv
preprint arXiv:1910.01075 (2019).
[83] Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and Chandan K. Reddy. 2020. Deep Reinforcement Learning for
Sequence-to-Sequence Models. IEEE Transactions on Neural Networks and Learning Systems 31, 7 (2020), 2469â€“2489.
https://doi.org/10.1109/TNNLS.2019.2929141
[84] Soheyl Khalilpourazari and Hossein Hashemi Doulabi. 2021. Using reinforcement learning to forecast the spread of
COVID-19 in France. In 2021 IEEE International Conference on Autonomous Systems (ICAS). 1â€“8. https://doi.org/10.
1109/ICAS49788.2021.9551174
[85] Soheyl Khalilpourazari and Hossein Hashemi Doulabi. 2022. Designing a hybrid reinforcement learning based
algorithm with application in prediction of the COVID-19 pandemic in Quebec. Annals of Operations Research 312, 2
(01 May 2022), 1261â€“1305. https://doi.org/10.1007/s10479-020-03871-7
[86] Ozsel Kilinc and Giovanni Montana. 2018. Multi-agent deep reinforcement learning with extremely noisy observations.
arXiv preprint arXiv:1812.00922 (2018).
[87] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim RocktÃ¤schel. 2023. A Survey of Zero-Shot Generalisation in
Deep Reinforcement Learning. J. Artif. Int. Res. 76 (feb 2023), 64 pages. https://doi.org/10.1613/jair.1.14174
[88] Vijay Konda and John Tsitsiklis. 1999. Actor-Critic Algorithms. In Advances in Neural Information Processing
Systems, S. Solla, T. Leen, and K. MÃ¼ller (Eds.), Vol. 12. MIT Press. https://proceedings.neurips.cc/paper/1999/file/
6449f44a102fde848669bdd9eb6b76fa-Paper.pdf
[89] Aleksandar Krnjaic, Jonathan D Thomas, Georgios Papoudakis, Lukas SchÃ¤fer, Peter BÃ¶rsting, and Stefano V Albrecht.
2022. Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers.
arXiv preprint arXiv:2212.11498 (2022).

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

35

[90] R. Lakshmana Kumar, Firoz Khan, Sadia Din, Shahab S. Band, Amir Mosavi, and Ebuka Ibeke. 2021. Recurrent
Neural Network and Reinforcement Learning Model for COVID-19 Prediction. Frontiers in Public Health 9 (2021).
https://doi.org/10.3389/fpubh.2021.744100
[91] Valery Kuzmin. 2002. Connectionist Q-learning in robot control task. In Scientific proceedings of riga technical
university, Vol. 5. Citeseer, 88â€“98.
[92] Hang Lai, Weinan Zhang, Xialin He, Chen Yu, Zheng Tian, Yong Yu, and Jun Wang. 2022. Sim-to-Real Transfer for
Quadrupedal Locomotion via Terrain Transformer. arXiv preprint arXiv:2212.07740 (2022).
[93] Xi Lan, Yuansong Qiao, and Brian Lee. 2021. Towards Pick and Place Multi Robot Coordination Using Multi-agent
Deep Reinforcement Learning. In 2021 7th International Conference on Automation, Robotics and Applications (ICARA).
85â€“89. https://doi.org/10.1109/ICARA51699.2021.9376433
[94] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. CodeRL: Mastering
Code Generation through Pretrained Models and Deep Reinforcement Learning. In Advances in Neural Information
Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,
Inc., 21314â€“21328. https://proceedings.neurips.cc/paper_files/paper/2022/file/8636419dea1aa9fbd25fc4248e702da4Paper-Conference.pdf
[95] Jinho Lee, Raehyun Kim, Seok-Won Yi, and Jaewoo Kang. 2020. MAPS: Multi-Agent reinforcement learning-based
Portfolio management System. arXiv preprint arXiv:2007.05402 (2020).
[96] Guy Leroy, Daniel Rueckert, and Amir Alansary. 2020. Communicative Reinforcement Learning Agents for Landmark
Detection in Brain Images. In Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-oncology,
Seyed Mostafa Kia, Hassan Mohy-ud Din, Ahmed Abdulkadir, Cher Bass, Mohamad Habes, Jane Maryam Rondina,
Chantal Tax, Hongzhi Wang, Thomas Wolfers, Saima Rathore, and Madhura Ingalhalikar (Eds.). Springer International
Publishing, Cham, 177â€“186.
[97] Chengxi Li, Pai Zheng, Yue Yin, Baicun Wang, and Lihui Wang. 2023. Deep reinforcement learning in smart
manufacturing: A review and prospects. CIRP Journal of Manufacturing Science and Technology 40 (2023), 75â€“101.
https://doi.org/10.1016/j.cirpj.2022.11.003
[98] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning
for dialogue generation. arXiv preprint arXiv:1606.01541 (2016).
[99] Li Li, Yisheng Lv, and Fei-Yue Wang. 2016. Traffic signal timing via deep reinforcement learning. IEEE/CAA Journal
of Automatica Sinica 3, 3 (2016), 247â€“254. https://doi.org/10.1109/JAS.2016.7508798
[100] Simin Li, Jun Guo, Jingqiao Xiu, Pu Feng, Xin Yu, Jiakai Wang, Aishan Liu, Wenjun Wu, and Xianglong Liu. 2023.
Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. arXiv preprint
arXiv:2302.03322 (2023).
[101] Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. 2019. Robust Multi-Agent Reinforcement
Learning via Minimax Deep Deterministic Policy Gradient. Proceedings of the AAAI Conference on Artificial Intelligence
33, 01 (Jul. 2019), 4213â€“4220. https://doi.org/10.1609/aaai.v33i01.33014213
[102] Tianxu Li, Kun Zhu, Nguyen Cong Luong, Dusit Niyato, Qihui Wu, Yang Zhang, and Bing Chen. 2022. Applications
of Multi-Agent Reinforcement Learning in Future Internet: A Comprehensive Survey. IEEE Communications Surveys
& Tutorials 24, 2 (2022), 1240â€“1279. https://doi.org/10.1109/COMST.2022.3160697
[103] Xihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu. 2019. A cooperative multi-agent reinforcement learning
framework for resource balancing in complex logistics network. arXiv preprint arXiv:1903.00714 (2019).
[104] Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li. 2017. Paraphrase generation with deep reinforcement learning.
arXiv preprint arXiv:1711.00279 (2017).
[105] Wenqian Liang, Ji Wang, Weidong Bao, Xiaomin Zhu, Qingyong Wang, and Beibei Han. 2022. Continuous self-adaptive
optimization to learn multi-task multi-agent. Complex & Intelligent Systems 8, 2 (April 2022), 1355â€“1367.
[106] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, and Ya Zhang. 2020.
Iteratively-Refined Interactive 3D Medical Image Segmentation With Multi-Agent Reinforcement Learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
[107] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).
[108] Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. 2020. On the Robustness
of Cooperative Multi-Agent Reinforcement Learning. In 2020 IEEE Security and Privacy Workshops (SPW). 62â€“68.
https://doi.org/10.1109/SPW50608.2020.00027
[109] Yuan Ling, Sadid A. Hasan, Vivek Datla, Ashequl Qadir, Kathy Lee, Joey Liu, and Oladimeji Farri. 2017. Diagnostic
Inferencing via Improving Clinical Concept Extraction with Deep Reinforcement Learning: A Preliminary Study.
In Proceedings of the 2nd Machine Learning for Healthcare Conference (Proceedings of Machine Learning Research,
Vol. 68), Finale Doshi-Velez, Jim Fackler, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (Eds.). PMLR,
271â€“285. https://proceedings.mlr.press/v68/ling17a.html

, Vol. 1, No. 1, Article . Publication date: May 2023.

36

Z. Zhou, G. Liu and Y. Tang

[110] Yuan Ling, Sadid A. Hasan, Vivek Datla, Ashequl Qadir, Kathy Lee, Joey Liu, and Oladimeji Farri. 2017. Learning
to Diagnose: Assimilating Clinical Narratives using Deep Reinforcement Learning. In Proceedings of the Eighth
International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Asian Federation of Natural
Language Processing, Taipei, Taiwan, 895â€“905. https://aclanthology.org/I17-1090
[111] Chenyi Liu, Nan Geng, Vaneet Aggarwal, Tian Lan, Yuan Yang, and Mingwei Xu. 2021. CMIX: Deep Multi-agent
Reinforcement Learning with Peak and Average Constraints. In Machine Learning and Knowledge Discovery in
Databases. Research Track, Nuria Oliver, Fernando PÃ©rez-Cruz, Stefan Kramer, Jesse Read, and Jose A. Lozano (Eds.).
Springer International Publishing, Cham, 157â€“173.
[112] Su Liu, Ye Chen, Hui Huang, Liang Xiao, and Xiaojun Hei. 2018. Towards Smart Educational Recommendations with
Reinforcement Learning in Classroom. In 2018 IEEE International Conference on Teaching, Assessment, and Learning
for Engineering (TALE). 1079â€“1084. https://doi.org/10.1109/TALE.2018.8615217
[113] Ximeng Liu, Robert H. Deng, Kim-Kwang Raymond Choo, and Yang Yang. 2021. Privacy-Preserving Reinforcement
Learning Design for Patient-Centric Dynamic Treatment Regimes. IEEE Transactions on Emerging Topics in Computing
9, 1 (2021), 456â€“470. https://doi.org/10.1109/TETC.2019.2896325
[114] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. 2020. Multi-Agent Game Abstraction
via Graph Attention Neural Network. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020),
7211â€“7218. https://doi.org/10.1609/aaai.v34i05.6211
[115] Zhiming Liu and Ji Wang. 2020. Human-cyber-physical systems: concepts, challenges, and research opportunities.
Frontiers of Information Technology & Electronic Engineering 21 (2020), 1535â€“1553.
[116] Zhengshang Liu, Yue Yang, Tim Miller, and Peta Masters. 2021. Deceptive reinforcement learning for privacypreserving planning. arXiv preprint arXiv:2102.03022 (2021).
[117] Zichuan Liu, Yuanyang Zhu, Zhi Wang, and Chunlin Chen. 2022. MIXRTs: Toward Interpretable Multi-Agent
Reinforcement Learning via Mixing Recurrent Soft Decision Trees. arXiv preprint arXiv:2209.07225 (2022).
[118] Faten Louati, Farah Barika Ktata, and Ikram Amous Ben Amor. 2022. A Distributed Intelligent Intrusion Detection
System based on Parallel Machine Learning and Big Data Analysis.. In SENSORNETS. 152â€“157.
[119] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. 2017. Multi-Agent Actor-Critic
for Mixed Cooperative-Competitive Environments. In Advances in Neural Information Processing Systems, I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates,
Inc. https://proceedings.neurips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf
[120] Keting Lu, Shiqi Zhang, and Xiaoping Chen. 2019. Goal-Oriented Dialogue Policy Learning from Failures. Proceedings of
the AAAI Conference on Artificial Intelligence 33, 01 (Jul. 2019), 2596â€“2603. https://doi.org/10.1609/aaai.v33i01.33012596
[121] Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer BaÅŸar, and Lior Horesh. 2021. Decentralized policy gradient descent
ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 35. 8767â€“8775.
[122] Chaofan Ma, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, and Ya Zhang. 2021. Boundary-Aware
Supervoxel-Level Iteratively Refined Interactive 3D Image Segmentation With Multi-Agent Reinforcement Learning.
IEEE Transactions on Medical Imaging 40, 10 (2021), 2563â€“2574. https://doi.org/10.1109/TMI.2020.3048477
[123] Cong Ma, Jiangshe Zhang, Zongxin Li, and Shuang Xu. 2023. Multi-agent deep reinforcement learning algorithm
with trend consistency regularization for portfolio management. Neural Computing and Applications 35, 9 (01 Mar
2023), 6589â€“6601. https://doi.org/10.1007/s00521-022-08011-9
[124] Bruna G Maciel-Pearson, Letizia Marchegiani, Samet Akcay, Amir Atapour-Abarghouei, James Garforth, and Toby P
Breckon. 2019. Online deep reinforcement learning for autonomous UAV navigation and exploration of outdoor
environments. arXiv preprint arXiv:1912.05684 (2019).
[125] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. 2019. MAVEN: Multi-Agent Variational
Exploration. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf
[126] Yailen MartÃ­nez JimÃ©nez, Jessica Coto Palacio, and Ann NowÃ©. 2020. Multi-Agent Reinforcement Learning Tool for
Job Shop Scheduling Problems. In Optimization and Learning, BernabÃ© Dorronsoro, Patricia Ruiz, Juan Carlos de la
Torre, Daniel Urda, and El-Ghazali Talbi (Eds.). Springer International Publishing, Cham, 3â€“12.
[127] Qinghai Miao, Min Huang, Yisheng Lv, and Fei-Yue Wang. 2022. Parallel Learning between Science for AI and AI for
Science: A Brief Overview and Perspective. In 2022 Australian & New Zealand Control Conference (ANZCC). 171â€“175.
https://doi.org/10.1109/ANZCC56036.2022.9966863
[128] Stephanie Milani, Zhicheng Zhang, Nicholay Topin, Zheyuan Ryan Shi, Charles Kamhoua, Evangelos E. Papalexakis,
and Fei Fang. 2023. MAVIPER: Learning Decision Tree Policies for Interpretable Multi-agent Reinforcement Learning.
In Machine Learning and Knowledge Discovery in Databases, Massih-Reza Amini, StÃ©phane Canu, Asja Fischer, Tias
Guns, Petra Kralj Novak, and Grigorios Tsoumakas (Eds.). Springer Nature Switzerland, Cham, 251â€“266.

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

37

[129] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,
and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The
33rd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 48), Maria Florina
Balcan and Kilian Q. Weinberger (Eds.). PMLR, New York, New York, USA, 1928â€“1937. https://proceedings.mlr.press/
v48/mniha16.html
[130] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[131] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
learning. nature 518, 7540 (2015), 529â€“533.
[132] Safa Mohamed and Ridha Ejbali. 2021. ADVERSARIAL MULTI-AGENT REINFORCEMENT LEARNING ALGORITHM
FOR ANOMALY NETWORK INTRUSION DETECTION SYSTEM. International Journal on Information Technologies &
Security 13, 3 (2021).
[133] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. 2015. Ensemble-CIO: Full-body dynamic motion planning
that transfers to physical humanoids. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). 5307â€“5314. https://doi.org/10.1109/IROS.2015.7354126
[134] Zhiyu Mou, Yu Zhang, Feifei Gao, Huangang Wang, Tao Zhang, and Zhu Han. 2021. Deep Reinforcement Learning
Based Three-Dimensional Area Coverage With UAV Swarm. IEEE Journal on Selected Areas in Communications 39, 10
(2021), 3160â€“3176. https://doi.org/10.1109/JSAC.2021.3088718
[135] Navid Naderializadeh, Jaroslaw J. Sydir, Meryem Simsek, and Hosein Nikopour. 2021. Resource Management in
Wireless Networks via Multi-Agent Deep Reinforcement Learning. IEEE Transactions on Wireless Communications 20,
6 (2021), 3507â€“3523. https://doi.org/10.1109/TWC.2021.3051163
[136] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. 2020. Deep Reinforcement Learning for Multiagent
Systems: A Review of Challenges, Solutions, and Applications. IEEE Transactions on Cybernetics 50, 9 (2020), 3826â€“3839.
https://doi.org/10.1109/TCYB.2020.2977374
[137] Yaru Niu, Rohan Paleja, and Matthew Gombolay. 2021. Multi-Agent Graph-Attention Communication and Teaming.
In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (Virtual Event,
United Kingdom) (AAMAS â€™21). International Foundation for Autonomous Agents and Multiagent Systems, Richland,
SC, 964â€“973.
[138] Jean Jacques Ohana, Steve Ohana, Eric Benhamou, David Saltiel, and Beatrice Guez. 2021. Explainable AI (XAI)
Models Applied to the Multi-agent Environment of Financial Markets. In Explainable and Transparent AI and MultiAgent Systems, Davide Calvaresi, Amro Najjar, Michael Winikoff, and Kary FrÃ¤mling (Eds.). Springer International
Publishing, Cham, 189â€“207.
[139] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. 2017. Deep Decentralized
Multi-task Multi-Agent Reinforcement Learning under Partial Observability. In Proceedings of the 34th International
Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh
(Eds.). PMLR, 2681â€“2690. https://proceedings.mlr.press/v70/omidshafiei17a.html
[140] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow
instructions with human feedback. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730â€“27744. https://proceedings.
neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf
[141] Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. 2019. How You Act Tells a Lot:
Privacy-Leaking Attack on Deep Reinforcement Learning. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems (Montreal QC, Canada) (AAMAS â€™19). International Foundation for
Autonomous Agents and Multiagent Systems, Richland, SC, 368â€“376.
[142] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016.
The Limitations of Deep Learning in Adversarial Settings. In 2016 IEEE European Symposium on Security and Privacy
(EuroS&P). 372â€“387. https://doi.org/10.1109/EuroSP.2016.36
[143] Yagna Patel. 2018. Optimizing market making using multi-agent reinforcement learning. arXiv preprint
arXiv:1812.10252 (2018).
[144] Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Boehmer,
and Shimon Whiteson. 2021. FACMAC: Factored Multi-Agent Centralised Policy Gradients. In Advances in Neural
Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan
(Eds.), Vol. 34. Curran Associates, Inc., 12208â€“12221. https://proceedings.neurips.cc/paper_files/paper/2021/file/
65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf

, Vol. 1, No. 1, Article . Publication date: May 2023.

38

Z. Zhou, G. Liu and Y. Tang

[145] Haixia Peng and Xuemin Shen. 2021. Multi-Agent Reinforcement Learning Based Resource Management in MECand UAV-Assisted Vehicular Networks. IEEE Journal on Selected Areas in Communications 39, 1 (2021), 131â€“141.
https://doi.org/10.1109/JSAC.2020.3036962
[146] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang. 2017. Multiagent
bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games.
arXiv preprint arXiv:1703.10069 (2017).
[147] Dana Pessach and Erez Shmueli. 2022. A Review on Fairness in Machine Learning. ACM Comput. Surv. 55, 3, Article
51 (feb 2022), 44 pages. https://doi.org/10.1145/3494672
[148] Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Luan Van Nguyen. 2018. Cooperative and Distributed
Reinforcement Learning of Drones for Field Coverage. CoRR abs/1803.07250 (2018). arXiv:1803.07250 http://arxiv.
org/abs/1803.07250
[149] Nhan H Pham, Lam M Nguyen, Jie Chen, Hoang Thanh Lam, Subhro Das, and Tsui-Wei Weng. 2022. Evaluating
Robustness of Cooperative MARL: A Model-based Approach. arXiv preprint arXiv:2202.03558 (2022).
[150] Uyen Pham, Quoc Luu, and Hien Tran. 2021. Multi-agent reinforcement learning approach for hedging portfolio
problem. Soft Computing 25, 12 (01 Jun 2021), 7877â€“7885. https://doi.org/10.1007/s00500-021-05801-6
[151] Thomy Phan, Lenz Belzner, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, and Claudia Linnhoff-Popien. 2021.
Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition. Proceedings of the AAAI
Conference on Artificial Intelligence 35, 13 (May 2021), 11308â€“11316. https://doi.org/10.1609/aaai.v35i13.17348
[152] Jens Popper, William Motsch, Alexander David, Teresa Petzsche, and Martin Ruskowski. 2021. Utilizing MultiAgent Deep Reinforcement Learning For Flexible Job Shop Scheduling Under Sustainable Viewpoints. In 2021
International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME). 1â€“6.
https://doi.org/10.1109/ICECCME52200.2021.9590925
[153] Jens Popper and Martin Ruskowski. 2022. Using Multi-Agent Deep Reinforcement Learning For Flexible Job Shop
Scheduling Problems. Procedia CIRP 112 (2022), 63â€“67. https://doi.org/10.1016/j.procir.2022.09.039 15th CIRP
Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021.
[154] Kritika Prakash, Fiza Husain, Praveen Paruchuri, and Sujit Gujar. 2022. How Private Is Your RL Policy? An Inverse RL
Based Analysis Framework. Proceedings of the AAAI Conference on Artificial Intelligence 36, 7 (Jun. 2022), 8009â€“8016.
https://doi.org/10.1609/aaai.v36i7.20772
[155] Erika Puiutta and Eric M. S. P. Veith. 2020. Explainable Reinforcement Learning: A Survey. In Machine Learning and
Knowledge Extraction, Andreas Holzinger, Peter Kieseberg, A Min Tjoa, and Edgar Weippl (Eds.). Springer International
Publishing, Cham, 77â€“95.
[156] Dawei Qiu, Jianhong Wang, Zihang Dong, Yi Wang, and Goran Strbac. 2022. Mean-Field Multi-Agent Reinforcement
Learning for Peer-to-Peer Multi-Energy Trading. IEEE Transactions on Power Systems (2022), 1â€“13. https://doi.org/10.
1109/TPWRS.2022.3217922
[157] Dawei Qiu, Jianhong Wang, Junkai Wang, and Goran Strbac. 2021. Multi-Agent Reinforcement Learning for Automated
Peer-to-Peer Energy Trading in Double-Side Auction Market.. In IJCAI. 2913â€“2920.
[158] Huaxin Qiu and Haibin Duan. 2020. A multi-objective pigeon-inspired optimization approach to UAV distributed
flocking among obstacles. Information Sciences 509 (2020), 515â€“529. https://doi.org/10.1016/j.ins.2018.06.061
[159] Thota Radha Rajesh and Surendran Rajendran. 2022. Intelligent Multi-Agent Reinforcement Learning Based Disease
Prediction and Treatment Recommendation Model. In 2022 International Conference on Augmented Intelligence and
Sustainable Systems (ICAISS). 216â€“221. https://doi.org/10.1109/ICAISS55157.2022.10010747
[160] Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. 2016. Epopt: Learning robust neural
network policies using model ensembles. arXiv preprint arXiv:1610.01283 (2016).
[161] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. 2020. Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. J. Mach.
Learn. Res. 21, 1, Article 178 (jan 2020), 51 pages.
[162] Lili Ren, Xin Ning, and Zheng Wang. 2022. A competitive Markov decision process model and a recursive
reinforcement-learning algorithm for fairness scheduling of agile satellites. Computers & Industrial Engineering 169
(2022), 108242. https://doi.org/10.1016/j.cie.2022.108242
[163] Marcelo Luis Ruiz RodrÃ­guez, Sylvain Kubler, Andrea de Giorgio, Maxime Cordy, JÃ©rÃ©my Robert, and Yves Le Traon.
2022. Multi-agent deep reinforcement learning based Predictive Maintenance on parallel machines. Robotics and
Computer-Integrated Manufacturing 78 (2022), 102406. https://doi.org/10.1016/j.rcim.2022.102406
[164] Heechang Ryu, Hayong Shin, and Jinkyoo Park. 2020. Multi-Agent Actor-Critic with Hierarchical Graph Attention
Network. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 7236â€“7243. https://doi.org/
10.1609/aaai.v34i05.6214
[165] Fereshteh Sadeghi, Alexander Toshev, Eric Jang, and Sergey Levine. 2018. Sim2Real Viewpoint Invariant Visual
Servoing by Recurrent Control. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

39

(CVPR).
[166] Jun Sakuma, Shigenobu Kobayashi, and Rebecca N. Wright. 2008. Privacy-Preserving Reinforcement Learning. In
Proceedings of the 25th International Conference on Machine Learning (Helsinki, Finland) (ICML â€™08). Association for
Computing Machinery, New York, NY, USA, 864â€“871. https://doi.org/10.1145/1390156.1390265
[167] Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard SchÃ¶lkopf, Michael C
Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. 2021. Learning neural causal models with active
interventions. arXiv preprint arXiv:2109.02429 (2021).
[168] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust Region Policy
Optimization. In Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 37), Francis Bach and David Blei (Eds.). PMLR, Lille, France, 1889â€“1897. https://proceedings.
mlr.press/v37/schulman15.html
[169] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 (2017).
[170] Clara Schumacher and Dirk Ifenthaler. 2021. Investigating prompts for supporting studentsâ€™ self-regulation â€“
A remaining challenge for learning analytics approaches? The Internet and Higher Education 49 (2021), 100791.
https://doi.org/10.1016/j.iheduc.2020.100791
[171] Jaemin Seo, Y-S Na, B Kim, CY Lee, MS Park, SJ Park, and YH Lee. 2021. Feedforward beta control in the KSTAR
tokamak by deep reinforcement learning. Nuclear Fusion 61, 10 (2021), 106010.
[172] Arturo Servin and Daniel Kudenko. 2008. Multi-Agent Reinforcement Learning for Intrusion Detection: A Case Study
and Evaluation. In Multiagent System Technologies, Ralph Bergmann, Gabriela Lindemann, Stefan Kirn, and Michal
PÄ›chouÄek (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 159â€“170.
[173] Kamalakanta Sethi, Y. Venu Madhav, Rahul Kumar, and Padmalochan Bera. 2021. Attention based multi-agent
intrusion detection systems using reinforcement learning. Journal of Information Security and Applications 61 (2021),
102923. https://doi.org/10.1016/j.jisa.2021.102923
[174] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. 2018. AirSim: High-Fidelity Visual and Physical
Simulation for Autonomous Vehicles. In Field and Service Robotics, Marco Hutter and Roland Siegwart (Eds.). Springer
International Publishing, Cham, 621â€“635.
[175] Ali Shavandi and Majid Khedmati. 2022. A multi-agent deep reinforcement learning framework for algorithmic trading
in financial markets. Expert Systems with Applications 208 (2022), 118124. https://doi.org/10.1016/j.eswa.2022.118124
[176] Ziyad Sheebaelhamd, Konstantinos Zisis, Athina Nisioti, Dimitris Gkouletsos, Dario Pavllo, and Jonas Kohler. 2021. Safe
Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces. arXiv preprint arXiv:2108.03952
(2021).
[177] Haoran Shi, Guanjun Liu, Kaiwen Zhang, Ziyuan Zhou, and Jiacun Wang. 2022. MARL Sim2real Transfer: Merging
Physical Reality With Digital Virtuality in Metaverse. IEEE Transactions on Systems, Man, and Cybernetics: Systems
(2022), 1â€“11. https://doi.org/10.1109/TSMC.2022.3229213
[178] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based Code Generation
using Deep Reinforcement Learning. arXiv preprint arXiv:2301.13816 (2023).
[179] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,
Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural
networks and tree search. nature 529, 7587 (2016), 484â€“489.
[180] David Silver, Richard S Sutton, and Martin MÃ¼ller. 2007. Reinforcement Learning of Local Shape in the Game of Go..
In IJCAI, Vol. 7. 1053â€“1058.
[181] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. 2019. QTRAN: Learning to Factorize
with Transformation for Cooperative Multi-Agent Reinforcement Learning. In Proceedings of the 36th International
Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan
Salakhutdinov (Eds.). PMLR, 5887â€“5896. https://proceedings.mlr.press/v97/son19a.html
[182] Jianyu Su, Jing Huang, Stephen Adams, Qing Chang, and Peter A. Beling. 2022. Deep multi-agent reinforcement
learning for multi-level preventive maintenance in manufacturing systems. Expert Systems with Applications 192
(2022), 116323. https://doi.org/10.1016/j.eswa.2021.116323
[183] Pei-Hao Su, Milica GaÅ¡iÄ‡, and Steve Young. 2018. Reward estimation for dialogue policy optimisation. Computer
Speech & Language 51 (2018), 24â€“43. https://doi.org/10.1016/j.csl.2018.02.003
[184] Sainbayar Sukhbaatar, arthur szlam, and Rob Fergus. 2016. Learning Multiagent Communication with Backpropagation. In Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2016/file/
55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf
[185] Chuangchuang Sun, Dong-Ki Kim, and Jonathan P. How. 2022. ROMAX: Certifiably Robust Deep Multiagent
Reinforcement Learning via Convex Relaxation. In 2022 International Conference on Robotics and Automation (ICRA).

, Vol. 1, No. 1, Article . Publication date: May 2023.

40

Z. Zhou, G. Liu and Y. Tang

5503â€“5510. https://doi.org/10.1109/ICRA46639.2022.9812321
[186] Penghao Sun, Zehua Guo, Gang Wang, Julong Lan, and Yuxiang Hu. 2020. MARVEL: Enabling controller load
balancing in software-defined networks with multi-agent reinforcement learning. Computer Networks 177 (2020),
107230. https://doi.org/10.1016/j.comnet.2020.107230
[187] Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi, Sumitra Ganesh, and Furong Huang.
2023. Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=dCOL0inGl3e
[188] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc
Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. 2018. Value-Decomposition Networks
For Cooperative Multi-Agent Learning Based On Team Reward. In Proceedings of the 17th International Conference
on Autonomous Agents and MultiAgent Systems (Stockholm, Sweden) (AAMAS â€™18). International Foundation for
Autonomous Agents and Multiagent Systems, Richland, SC, 2085â€“2087.
[189] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.
[190] Akito Suzuki and Shigeaki Harada. 2020. Safe Multi-Agent Deep Reinforcement Learning for Dynamic Virtual
Network Allocation. In GLOBECOM 2020 - 2020 IEEE Global Communications Conference. 1â€“7. https://doi.org/10.
1109/GLOBECOM42002.2020.9348210
[191] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul
Vicente. 2017. Multiagent cooperation and competition with deep reinforcement learning. PLOS ONE 12, 4 (04 2017),
1â€“15. https://doi.org/10.1371/journal.pone.0172395
[192] Qingmeng Tan, Yifei Tong, Shaofeng Wu, and Dongbo Li. 2019. Modeling, planning, and scheduling of shop-floor
assembly process with dynamic cyber-physical interactions: a case study for CPS-based smart industrial robot
production. The International Journal of Advanced Manufacturing Technology 105, 9 (01 Dec 2019), 3979â€“3989.
https://doi.org/10.1007/s00170-019-03940-7
[193] Kai-Fu Tang, Hao-Cheng Kao, Chun-Nan Chou, and Edward Y Chang. 2016. Inquire and diagnose: Neural symptom
checking ensemble using deep reinforcement learning. In NIPS workshop on deep reinforcement learning.
[194] Ying Tang and Ryan Hare. 2022. Evaluation of an AI-assisted Adaptive Educational Game System. In 2022 Spring
ASEE Middle Atlantic Section Conference. ASEE Conferences, Newark, New Jersey. https://peer.asee.org/40052.
[195] VÃ­ctor Uc-Cetina, NicolÃ¡s Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius Weber, and Stefan Wermter. 2023.
Survey on reinforcement learning for language processing. Artificial Intelligence Review 56, 2 (01 Feb 2023), 1543â€“1575.
https://doi.org/10.1007/s10462-022-10205-5
[196] Pascal Van Der Vaart, Anuj Mahajan, and Shimon Whiteson. 2021. Model based multi-agent reinforcement learning
with tensor decompositions. arXiv preprint arXiv:2110.14524 (2021).
[197] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep reinforcement learning with double q-learning. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 30.
[198] Parv Venkitasubramaniam. 2013. Privacy in stochastic control: A Markov Decision Process perspective. In 2013 51st
Annual Allerton Conference on Communication, Control, and Computing (Allerton). 381â€“388. https://doi.org/10.1109/
Allerton.2013.6736549
[199] Alexander Vezhnevets, Yuhuai Wu, Maria Eckstein, RÃ©mi Leblond, and Joel Z Leibo. 2020. OPtions as REsponses:
Grounding behavioural hierarchies in multi-agent reinforcement learning. In Proceedings of the 37th International
Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal DaumÃ© III and Aarti Singh
(Eds.). PMLR, 9733â€“9742. https://proceedings.mlr.press/v119/vezhnevets20a.html
[200] Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Steven Wu. 2020. Private Reinforcement Learning with
PAC and Regret Guarantees. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 119), Hal DaumÃ© III and Aarti Singh (Eds.). PMLR, 9754â€“9764. https://proceedings.
mlr.press/v119/vietri20a.html
[201] Nelson Vithayathil Varghese and Qusay H. Mahmoud. 2020. A Survey of Multi-Task Deep Reinforcement Learning.
Electronics 9, 9 (2020). https://doi.org/10.3390/electronics9091363
[202] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert, and Bernhard Kainz. 2019. Multiple
Landmark Detection Using Multi-agent Reinforcement Learning. In Medical Image Computing and Computer Assisted
Intervention â€“ MICCAI 2019, Dinggang Shen, Tianming Liu, Terry M. Peters, Lawrence H. Staib, Caroline Essert, Sean
Zhou, Pew-Thian Yap, and Ali Khan (Eds.). Springer International Publishing, Cham, 262â€“270.
[203] George A. Vouros. 2022. Explainable Deep Reinforcement Learning: State of the Art and Challenges. ACM Comput.
Surv. 55, 5, Article 92 (dec 2022), 39 pages. https://doi.org/10.1145/3527448
[204] Ory Walker, Fernando Vanegas, Felipe Gonzalez, and Sven Koenig. 2020. Multi-UAV Target-Finding in Simulated
Indoor Environments using Deep Reinforcement Learning. In 2020 IEEE Aerospace Conference. 1â€“9. https://doi.org/10.
1109/AERO47225.2020.9172262

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

41

[205] Baoxiang Wang and Nidhi Hegde. 2019. Privacy-Preserving Q-Learning with Functional Noise in Continuous
Spaces. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
6646b06b90bd13dabc11ddba01270d23-Paper.pdf
[206] Chenghe Wang, Yuhang Ran, Lei Yuan, Yang Yu, and Zongzhang Zhang. 2023. Robust Multi-Agent Reinforcement
Learning against Adversaries on Observation. https://openreview.net/forum?id=eExA3Mk0Dxp
[207] Dawei Wang, Tingxiang Fan, Tao Han, and Jia Pan. 2020. A Two-Stage Reinforcement Learning Approach for
Multi-UAV Collision Avoidance Under Imperfect Sensing. IEEE Robotics and Automation Letters 5, 2 (2020), 3098â€“3105.
https://doi.org/10.1109/LRA.2020.2974648
[208] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. 2021. {QPLEX}: Duplex Dueling Multi-Agent
Q-Learning. In International Conference on Learning Representations. https://openreview.net/forum?id=Rcmk0xxIQV
[209] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. 2021. Backdoorl: Backdoor attack
against competitive reinforcement learning. arXiv preprint arXiv:2105.00579 (2021).
[210] Liang Wang, Kezhi Wang, Cunhua Pan, Wei Xu, Nauman Aslam, and Lajos Hanzo. 2021. Multi-Agent Deep Reinforcement Learning-Based Trajectory Planning for Multi-UAV Assisted Mobile Edge Computing. IEEE Transactions
on Cognitive Communications and Networking 7, 1 (2021), 73â€“84. https://doi.org/10.1109/TCCN.2020.3027695
[211] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. 2020. Learning Efficient Multi-agent
Communication: An Information Bottleneck Approach. In Proceedings of the 37th International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 119), Hal DaumÃ© III and Aarti Singh (Eds.). PMLR, 9908â€“9918.
https://proceedings.mlr.press/v119/wang20i.html
[212] Xiaohan Wang, Lin Zhang, Tingyu Lin, Chun Zhao, Kunyu Wang, and Zhen Chen. 2022. Solving job scheduling
problems in a resource preemption environment with multi-agent reinforcement learning. Robotics and ComputerIntegrated Manufacturing 77 (2022), 102324. https://doi.org/10.1016/j.rcim.2022.102324
[213] Xihuai Wang, Zhicheng Zhang, and Weinan Zhang. 2022. Model-based multi-agent reinforcement learning: Recent
progress and prospects. arXiv preprint arXiv:2203.10603 (2022).
[214] Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, and Saif Eddin Jabari. 2021. Stop-and-Go: Exploring
Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems. IEEE Transactions on
Information Forensics and Security 16 (2021), 4772â€“4787. https://doi.org/10.1109/TIFS.2021.3114024
[215] Yue Wang and Shaofeng Zou. 2021. Online Robust Reinforcement Learning with Model Uncertainty. In Advances in Neural Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 7193â€“7206. https://proceedings.neurips.cc/paper/2021/file/
3a4496776767aaa99f9804d0905fe584-Paper.pdf
[216] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. 2016. Dueling network
architectures for deep reinforcement learning. In International conference on machine learning. PMLR, 1995â€“2003.
[217] Paul Weng. 2019. Fairness in reinforcement learning. arXiv preprint arXiv:1907.10323 (2019).
[218] Ronald J. Williams. 1992. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.
Springer US, Boston, MA, 5â€“32. https://doi.org/10.1007/978-1-4615-3618-5_2
[219] Annie Wong, Thomas BÃ¤ck, Anna V. Kononova, and Aske Plaat. 2022. Deep multiagent reinforcement learning:
challenges and directions. Artificial Intelligence Review (19 Oct 2022). https://doi.org/10.1007/s10462-022-10299-x
[220] Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu. 2020. Multi-Agent
Deep Reinforcement Learning for Urban Traffic Light Control in Vehicular Networks. IEEE Transactions on Vehicular
Technology 69, 8 (2020), 8243â€“8256. https://doi.org/10.1109/TVT.2020.2997896
[221] Young Wu, Jermey McMahan, Xiaojin Zhu, and Qiaomin Xie. 2022. Reward Poisoning Attacks on Offline Multi-Agent
Reinforcement Learning. arXiv preprint arXiv:2206.01888 (2022).
[222] Cheng Xu, Ming Xu, and Chanjuan Yin. 2020. Optimized multi-UAV cooperative path planning under the complex
confrontation environment. Computer Communications 162 (2020), 196â€“203. https://doi.org/10.1016/j.comcom.2020.
04.050
[223] D. Xu and G. Chen. 2022. Autonomous and cooperative control of UAV cluster with multi-agent reinforcement
learning. The Aeronautical Journal 126, 1300 (2022), 932â€“951. https://doi.org/10.1017/aer.2021.112
[224] Dan Xu and Gang Chen. 2022. The research on intelligent cooperative combat of UAV cluster with multi-agent
reinforcement learning. Aerospace Systems 5, 1 (01 Mar 2022), 107â€“121. https://doi.org/10.1007/s42401-021-00105-x
[225] Mengdi Xu, Zuxin Liu, Peide Huang, Wenhao Ding, Zhepeng Cen, Bo Li, and Ding Zhao. 2022. Trustworthy
reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability. arXiv preprint
arXiv:2209.08025 (2022).
[226] Min Yang, Weiyi Huang, Wenting Tu, Qiang Qu, Ying Shen, and Kai Lei. 2021. Multitask Learning and Reinforcement
Learning for Personalized Dialog Generation: An Empirical Study. IEEE Transactions on Neural Networks and Learning
Systems 32, 1 (2021), 49â€“62. https://doi.org/10.1109/TNNLS.2020.2975035

, Vol. 1, No. 1, Article . Publication date: May 2023.

42

Z. Zhou, G. Liu and Y. Tang

[227] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. 2018. Mean Field Multi-Agent
Reinforcement Learning. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.). PMLR, 5571â€“5580. https://proceedings.
mlr.press/v80/yang18d.html
[228] Dayong Ye, Tianqing Zhu, Zishuo Cheng, Wanlei Zhou, and Philip S. Yu. 2022. Differential Advising in Multiagent
Reinforcement Learning. IEEE Transactions on Cybernetics 52, 6 (2022), 5508â€“5521. https://doi.org/10.1109/TCYB.
2020.3034424
[229] Dayong Ye, Tianqing Zhu, Sheng Shen, Wanlei Zhou, and Philip S. Yu. 2022. Differentially Private Multi-Agent
Planning for Logistic-Like Problems. IEEE Transactions on Dependable and Secure Computing 19, 2 (2022), 1212â€“1226.
https://doi.org/10.1109/TDSC.2020.3017497
[230] Dayong Ye, Tianqing Zhu, Wanlei Zhou, and Philip S. Yu. 2020. Differentially Private Malicious Agent Avoidance in
Multiagent Advising Learning. IEEE Transactions on Cybernetics 50, 10 (2020), 4214â€“4227. https://doi.org/10.1109/
TCYB.2019.2906574
[231] Li Yinggang and Tong Xiangrong. 2022. Social Recommendation System Based on Multi-agent Deep Reinforcement
Learning. In 2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS). 371â€“377.
https://doi.org/10.1109/CCIS57298.2022.10016386
[232] Chao Yu, Xin Wang, Xin Xu, Minjie Zhang, Hongwei Ge, Jiankang Ren, Liang Sun, Bingcai Chen, and Guozhen
Tan. 2020. Distributed Multiagent Coordinated Learning for Autonomous Driving in Highways Based on Dynamic
Coordination Graphs. IEEE Transactions on Intelligent Transportation Systems 21, 2 (2020), 735â€“748. https://doi.org/
10.1109/TITS.2019.2893683
[233] Chen Yu, Weinan Zhang, Hang Lai, Zheng Tian, Laurent Kneip, and Jun Wang. 2022. Multi-embodiment Legged
Robot Control as a Sequence Modeling Problem. arXiv preprint arXiv:2212.09078 (2022).
[234] Tingting Yuan, Hwei-Ming Chung, and Xiaoming Fu. 2022. PP-MARL: Efficient Privacy-Preserving MARL for
Cooperative Intelligence in Communication. arXiv preprint arXiv:2204.12064 (2022).
[235] Renos Zabounidis, Joseph Campbell, Simon Stepputtis, Dana Hughes, and Katia P. Sycara. 2023. Concept Learning
for Interpretable Multi-Agent Reinforcement Learning. In Proceedings of The 6th Conference on Robot Learning
(Proceedings of Machine Learning Research, Vol. 205), Karen Liu, Dana Kulic, and Jeff Ichnowski (Eds.). PMLR, 1828â€“
1837. https://proceedings.mlr.press/v205/zabounidis23a.html
[236] Sihan Zeng, Malik Aqeel Anwar, Thinh T. Doan, Arijit Raychowdhury, and Justin Romberg. 2021. A decentralized
policy gradient approach to multi-task reinforcement learning. In Proceedings of the Thirty-Seventh Conference on
Uncertainty in Artificial Intelligence (Proceedings of Machine Learning Research, Vol. 161), Cassio de Campos and
Marloes H. Maathuis (Eds.). PMLR, 1002â€“1012. https://proceedings.mlr.press/v161/zeng21a.html
[237] Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina
Precup. 2020. Invariant Causal Prediction for Block MDPs. In Proceedings of the 37th International Conference on
Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal DaumÃ© III and Aarti Singh (Eds.). PMLR,
11214â€“11224. https://proceedings.mlr.press/v119/zhang20t.html
[238] Fuxiang Zhang, Chengxing Jia, Yi-Chen Li, Lei Yuan, Yang Yu, and Zongzhang Zhang. 2023. Discovering Generalizable
Multi-agent Coordination Skills from Multi-task Offline Data. In The Eleventh International Conference on Learning
Representations. https://openreview.net/forum?id=53FyUAdP7d
[239] Gengzhi Zhang, Liang Feng, and Yaqing Hou. 2021. Multi-task Actor-Critic with Knowledge Transfer via a Shared
Critic. In Proceedings of The 13th Asian Conference on Machine Learning (Proceedings of Machine Learning Research,
Vol. 157), Vineeth N. Balasubramanian and Ivor Tsang (Eds.). PMLR, 580â€“593. https://proceedings.mlr.press/v157/
zhang21b.html
[240] Jia-Dong Zhang, Zhixiang He, Wing-Ho Chan, and Chi-Yin Chow. 2023. DeepMAG: Deep reinforcement learning
with multi-agent graphs for flexible job shop scheduling. Knowledge-Based Systems 259 (2023), 110083. https:
//doi.org/10.1016/j.knosys.2022.110083
[241] Kaiqing Zhang, TAO SUN, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. 2020. Robust Multi-Agent
Reinforcement Learning with Model Uncertainty. In Advances in Neural Information Processing Systems, H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 10571â€“10583. https://
proceedings.neurips.cc/paper/2020/file/774412967f19ea61d448977ad9749078-Paper.pdf
[242] Kaiqing Zhang, Zhuoran Yang, and Tamer BaÅŸar. 2021. Decentralized multi-agent reinforcement learning with
networked agents: recent advances. Frontiers of Information Technology & Electronic Engineering 22, 6 (01 Jun 2021),
802â€“814. https://doi.org/10.1631/FITEE.1900661
[243] Tianhao Zhang, Qiwei Ye, Jiang Bian, Guangming Xie, and Tie-Yan Liu. 2021. MFVFD: A Multi-Agent Q-Learning
Approach to Cooperative and Non-Cooperative Tasks. In Proceedings of the Thirtieth International Joint Conference on
Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org,
500â€“506. https://doi.org/10.24963/ijcai.2021/70

, Vol. 1, No. 1, Article . Publication date: May 2023.

Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges

43

[244] Xianjie Zhang, Yu Liu, Xiujuan Xu, Qiong Huang, Hangyu Mao, and Anil Carie. 2021. Structural relational inference
actor-critic for multi-agent reinforcement learning. Neurocomputing 459 (2021), 383â€“394. https://doi.org/10.1016/j.
neucom.2021.07.014
[245] Yang Zhang, Chenwei Zhang, and Xiaozhong Liu. 2017. Dynamic Scholarly Collaborator Recommendation via
Competitive Multi-Agent Reinforcement Learning (RecSys â€™17). Association for Computing Machinery, New York, NY,
USA, 331â€“335. https://doi.org/10.1145/3109859.3109914
[246] Yi Zhang, Haihua Zhu, Dunbing Tang, Tong Zhou, and Yong Gui. 2022. Dynamic job shop scheduling based on deep
reinforcement learning for multi-agent manufacturing systems. Robotics and Computer-Integrated Manufacturing 78
(2022), 102412. https://doi.org/10.1016/j.rcim.2022.102412
[247] Wenshuai Zhao, Jorge PeÃ±a Queralta, and Tomi Westerlund. 2020. Sim-to-Real Transfer in Deep Reinforcement
Learning for Robotics: a Survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI). 737â€“744.
https://doi.org/10.1109/SSCI47803.2020.9308468
[248] Chenyang Zheng, Xiangyu Si, Lei Sun, Zhang Chen, Linghao Yu, and Zhiqiang Tian. 2021. Multi-agent reinforcement
learning for prostate localization based on multi-scale image representation. In International Symposium on Artificial
Intelligence and Robotics 2021, Vol. 11884. SPIE, 487â€“494.
[249] Hua Zheng, Jiahao Zhu, Wei Xie, and Judy Zhong. 2021. Reinforcement learning assisted oxygen therapy for
COVID-19 patients under intensive care. BMC Medical Informatics and Decision Making 21, 1 (17 Dec 2021), 350.
https://doi.org/10.1186/s12911-021-01712-6
[250] Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, IMAN
FADAKAR, Zheng Chen, Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves, Zhengbang Zhu, Yihan Ni,
Nhat Nguyen, Mohamed Elsayed, Haitham Ammar, Alexander Cowen-Rivers, Sanjeevan Ahilan, Zheng Tian, Daniel
Palenicek, Kasra Rezaee, Peyman Yadmellat, Kun Shao, dong chen, Baokuan Zhang, Hongbo Zhang, Jianye Hao,
Wulong Liu, and Jun Wang. 2021. SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous
Driving. In Proceedings of the 2020 Conference on Robot Learning (Proceedings of Machine Learning Research, Vol. 155),
Jens Kober, Fabio Ramos, and Claire Tomlin (Eds.). PMLR, 264â€“285. https://proceedings.mlr.press/v155/zhou21a.html
[251] Wei Zhou, Dong Chen, Jun Yan, Zhaojian Li, Huilin Yin, and Wanchen Ge. 2022. Multi-agent reinforcement learning
for cooperative lane changing of connected and autonomous vehicles in mixed traffic. Autonomous Intelligent Systems
2, 1 (2022), 5.
[252] Xingyu Zhou. 2022. Differentially Private Reinforcement Learning with Linear Function Approximation. Proc. ACM
Meas. Anal. Comput. Syst. 6, 1, Article 8 (feb 2022), 27 pages. https://doi.org/10.1145/3508028
[253] Ziyuan Zhou and Guanjun Liu. 2022. Romfac: A robust mean-field actor-critic reinforcement learning against
adversarial perturbations on states. arXiv preprint arXiv:2205.07229 (2022).
[254] Changxi Zhu, Mehdi Dastani, and Shihan Wang. 2022. A survey of multi-agent reinforcement learning with
communication. arXiv preprint arXiv:2203.08975 (2022).
[255] Yiting Zhu, Zhaocheng He, and Guilong Li. 2022. A bi-Hierarchical Game-Theoretic Approach for Network-Wide
Traffic Signal Control Using Trip-Based Data. IEEE Transactions on Intelligent Transportation Systems 23, 9 (2022),
15408â€“15419. https://doi.org/10.1109/TITS.2022.3140511
[256] Matthieu Zimmer, Claire Glanois, Umer Siddique, and Paul Weng. 2021. Learning Fair Policies in Decentralized
Cooperative Multi-Agent Reinforcement Learning. In Proceedings of the 38th International Conference on Machine
Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 12967â€“12978.
https://proceedings.mlr.press/v139/zimmer21a.html

, Vol. 1, No. 1, Article . Publication date: May 2023.

