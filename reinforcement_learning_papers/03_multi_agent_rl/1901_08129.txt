arXiv:1901.08129v2 [cs.AI] 11 Apr 2025

The Multi-Agent Reinforcement Learning in MalmÖ
(MARLÖ) Competition

Diego Perez-Liebana
Andre Kramer

Katja Hofmann
Sam Devlin

Sharada Prasanna Mohanty
Noboru Kuno
Raluca D. Gaina
Daniel Ionita ∗

Abstract
Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward
settings and different opponent types. The Multi-Agent Reinforcement Learning
in MalmÖ (MARLÖ) competition is a new challenge that proposes research in this
domain using multiple 3D games. The goal of this contest is to foster research in
general agents that can learn across different games and opponent types, proposing
a challenge as a milestone in the direction of Artificial General Intelligence.

1 Introduction
Learning in multi-agent settings is one of the fundamental problems in AI research. Independently
learning agents can result in non-stationarity, and the presence of adversarial agents can hamper exploration and consequently the learning progress. Multi-agent settings can be approached as Stochastic N -player Games (SGs) [9], where each player interacts with the game environment by observing
state observations, sending actions that in turn affect the state of the environment (and other players) and receiving rewards. Reinforcement Learning (RL) is a common approach to learning in
SGs [12, 5] and promises solutions that could be general and applicable to learning in any game.
RL for multi-agent settings has a long and fruitful research tradition [10, 1]. The goal of a reinforcement learner is formally to maximize its long-term cumulative reward. Games can be competitive
(e.g., zero-sum games where one player’s reward is the inverse of its opponent’s), collaborative (all
rewards are shared), or general-reward. The latter are the most realistic for many real-world applications but also notoriously challenging. Even in more restricted purely competitive and collaborative
settings, the challenges of learning in the presence of other agents are far from solved. Current solutions only scale empirically in tasks restricted to relatively small environments or with simplifying
assumptions.
Recent research progress in multi-agent RL have shown rapid progress in tackling some key challenges [3, 2, 6, 11], suggesting that increased research efforts could lead to further breakthroughs.
Genralization beyond individual tasks and opponent types is an area with high need and potential
for further research. In single agent RL, there is a clear risk to overfitting to individual tasks and
specific opponent types. This problem is gradually being addressed in single-agent tasks, but most
current empirical work in multi-agent RL is focused on few individual tasks with a single learning
agent.
Diego Perez-Liebana, Raluca D. Gaina and Daniel Ionita are with the Game AI group, Queen Mary University of London (UK). Katja Hofmann, Noboru Kuno, Andre Kramer and Sam Devlin are with Microsoft
Research (UK). Sharada Prasanna Mohanty is with École Polytechnique Fédérale de Lausanne (Switzerland).
∗

Preprint. Work in progress.

2 MARLÖ: Multi-Agent Reinforcement Learning in MalmÖ
The Multi-Agent Reinforcement Learning in MalmÖ (MARLÖ) competition is a new challenge
that proposes research on multi-agent RL using multiple games. Participants would create learning
agents that will be able to play multiple 3D games within Minecraft as defined in the MalmÖ
platform. The MARLÖ competition will run for the first time in 2018 and will feature 3 games2 :
Mob Chase: A collaborative game in which two or more agents and a mob wander a small meadow
for a limited amount of time. The agents can either catch the mob - cornering it, leaving no escape
path available - or leave the pen through one of the exits. Capturing the mob gives players a high
reward (1 point) while exiting provides a smaller one (0.2). This game is inspired by the variant of
the stag hunt presented in [13].
Build Battle: A competitive and collaborative game where two teams of agents compete to build a
given cuboid structure within a time limit. Agents receive 0.2 points for correctly placing a block or
removing an incorrectly placed block, and −0.2 points for incorrectly placing a block or removing
a correctly placed block.
Treasure Hunt: A competitive and collaborative game played in an underground dungeon. Each
team is formed by collectors (who can pick treasures) and fighters (who fight foes). The goal is to
retrieve a treasure while surviving the enemy entities. All agents on a winning team receive 0.25
points if their collector gets the treasure, and 0.5 points if the collector reaches the exit (losing teams
receive the negation of these points). If anybody in the team dies, all agents on the team receive −1
points. The game ends when the collector player reaches the exit, when a player dies or when the
time runs out.
All games are parameterizable, providing a task space in which potentially endless variants of each
domain can be created. Examples of these parameters are weather, block types, number and position of entities, and size of the playing area. Participants can train their agents in any of the possible
instances of each game (known as tasks). The final evaluation will be performed in particular customizations designed by the organizers and not revealed before the competition deadline.
The participants will be provided an extensive starter kit3 , with all necessary instructions to download the framework, develop and execute their agents locally for testing in the games included in the
benchmark. The starter kit will also include a set of simple tasks and the code for default challenge
agents (to compete against, but also as examples for participants to develop their own entries). This
approach has been proven successful in previous competitions [7, 4, 8], helping entrants to quickly
get started with the challenge and easily continue to make iterative changes to their approach.
The final rankings of the competition are computed by means of a play-off tournament. Each stage
in the tournament features the 3 games mentioned here, at least 1 task for each one of them and N
teams playing a round-robin league. The agents must therefore show proficiency in multiple games
to move to the next round of the play-off. Each league will have its own ranking table, in which
entries are sorted by the sum of scores obtained in all tasks. The top entries of each group progress
to the next stage, until reaching the final league, which determines the competition winner.

3 Conclusions
The MARLÖ competition aims at encouraging research in multi-agent reinforcement learning. In
particular, it proposes a challenge in which researchers must attempt approaches that generalize well
across games, tasks and opponent types. Not only submitted agents are tested in different games,
but these games are also highly parameterizable, with the final configuration of tasks not known
to participants beforehand. Additionally, all entries play against multiple agents in the tournament,
requiring the agents not to overfit to their opponents.
Research is therefore encouraged and supported by this competition in multiple ways: it makes a
set of multi-agent learning tasks publicly available, with a low barrier of entry, but enough room
for increasing task difficulty as progress is made. It also creates shared baselines and an evaluation
2
3

https://www.crowdai.org/challenges/marlo-2018
https://github.com/crowdAI/marlo-single-agent-starter-kit/

2

setup that eases comparison between approaches. Finally, it increases awareness of the multi-agent
RL challenges and provides a platform for testing and sharing the progress in the field.
We believe that this is the right way at the right time to invigorate the research community and drive
progress in this exciting and important area. The future of the competition will bring different games
with new dynamics, richer interactions and further challenges for multi-agent RL reserach.

3

References
[1] Busoniu, L., Babuska, R., and De Schutter, B. (2006). Multi-agent reinforcement learning:
A survey. In Control, Automation, Robotics and Vision, 2006. ICARCV’06. 9th International
Conference on, pages 1–6. IEEE.
[2] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2017a). Counterfactual
multi-agent policy gradients. arXiv preprint arXiv:1705.08926.
[3] Foerster, J., Nardelli, N., Farquhar, G., Afouras, T., Torr, P. H., Kohli, P., and Whiteson, S.
(2017b). Stabilising experience replay for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 1146–1155.

[4] Laboratory, S. N. B. (2017). Learning to Run. https://www.crowdai.org/challenges/nips-2017-learning-to-ru
[Online; accessed 11-February-2018].
[5] Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning.
In Machine Learning Proceedings 1994, pages 157–163.
[6] Lowe, R., WU, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. (2017). Multi-Agent
Actor-Critic for Mixed Cooperative-Competitive Environments. In Advances in Neural Information Processing Systems 30, pages 6382–6393.
[7] Perez-Liebana, D., Samothrakis, S., Togelius, J., Lucas, S. M., and Schaul, T. (2016). General
Video Game AI: Competition, Challenges and Opportunities. In Thirtieth AAAI Conference on
Artificial Intelligence, pages 4335–4337.
[8] Research,
M. (2017).
The Malmo Collaborative AI Challenge .
https://www.microsoft.com/en-us/research/academic-program/collaborative-ai-challenge/.
[Online; accessed 12-February-2018].
[9] Shapley, L. S. (1953). Stochastic games. Proceedings of the National Academy of Sciences,
39(10), 1095–1100.
[10] Stone, P. and Veloso, M. (2000). Multiagent systems: A survey from a machine learning
perspective. Autonomous Robots, 8(3), 345–383.
[11] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot,
M., Sonnerat, N., Leibo, J. Z., Tuyls, K., and Graepel, T. (2017). Value-Decomposition Networks
For Cooperative Multi-Agent Learning. arXiv preprint arXiv:1706.05296.
[12] Tan, M. (1993). Multi-agent reinforcement learning: independent versus cooperative agents.
In Proceedings of the Tenth International Conference on International Conference on Machine
Learning, pages 330–337.
[13] Yoshida, W., Dolan, R. J., and Friston, K. J. (2008). Game theory of mind. PLOS Computational Biology, 4(12), 1–14.

4

