Here is the extracted information:

**PAPER**: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning
**TITLE**: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning
**RESEARCH_METHOD**: 03_multi_agent_rl
**METHOD_DESCRIPTION**: The paper revisits two common practices in cooperative multi-agent reinforcement learning (MARL): value decomposition and policy sharing. It argues that these practices can be problematic in certain scenarios and proposes alternative approaches, including policy gradient methods and auto-regressive policy learning.
**KEY_CONTRIBUTIONS**:
* The paper shows that value decomposition methods can fail to represent the underlying payoff structure in certain games, such as the XOR game.
* It proves that policy gradient methods can converge to an optimal solution in these games.
* The paper proposes an auto-regressive policy learning approach, which can learn multi-modal behaviors and discover interesting emergent behaviors.
* It evaluates the proposed approaches on several MARL testbeds, including StarCraft Multi-Agent Challenge and Google Research Football, and shows that they can outperform existing methods in certain scenarios.
