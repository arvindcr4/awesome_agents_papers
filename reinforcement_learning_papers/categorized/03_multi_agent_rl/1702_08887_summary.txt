PAPER: 1702_08887.pdf
TITLE: Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning
ARXIV_ID: 1702.08887v3
RESEARCH_METHOD: 03_multi_agent_rl
METHOD_DESCRIPTION: This paper proposes two methods for stabilising experience replay in deep multi-agent reinforcement learning. The first approach uses a multi-agent variant of importance sampling to naturally decay obsolete data. The second approach conditions each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. The fingerprint is designed to capture the changing policies of other agents over time, allowing the model to generalise between best responses to different policies.
KEY_CONTRIBUTIONS:
- Proposed two methods for stabilising experience replay in deep multi-agent reinforcement learning: multi-agent importance sampling and conditioning on a fingerprint.
- Demonstrated the effectiveness of these methods in a decentralised variant of StarCraft unit micromanagement, a challenging multi-agent benchmark problem.
- Showed that the fingerprint approach can improve performance over the importance sampling approach, especially in feed-forward models.
- Highlighted the potential of these methods for addressing non-stationarity in multi-agent reinforcement learning and their applicability to a broader range of problems.
