Published as a conference paper at ICLR 2022

T RUST R EGION P OLICY O PTIMISATION IN
M ULTI -AGENT R EINFORCEMENT L EARNING

arXiv:2109.11251v2 [cs.AI] 4 Apr 2022

Jakub Grudzien Kuba1,2âˆ—, Ruiqing Chen3,âˆ— , Muning Wen4 , Ying Wen4 ,
Fanglei Sun3 , Jun Wang5 , Yaodong Yang6,â€ 
1
University of Oxford, 2 Huawei R&D UK, 3 ShanghaiTech University,
4
Shanghai Jiao Tong University 5 University College London
6
Institute for AI, Peking University & BIGAI
â€ 
Corresponding to: yaodong.yang@pku.edu.cn

A BSTRACT
Trust region methods rigorously enabled reinforcement learning (RL) agents to
learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning
(MARL), the property of monotonic improvement may not simply apply; this is
because agents, even in cooperative games, could have conflicting directions of
policy updates. As a result, achieving a guaranteed improvement on the joint
policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to cooperative MARL. Central
to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent
Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they
need any restrictive assumptions on decomposibility of the joint value function.
Most importantly, we justify in theory the monotonic improvement property of
HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent
MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all
tested tasks, thereby establishing a new state of the art.

1

I NTRODUCTION

Policy gradient (PG) methods have played a major role in recent developments of reinforcement
learning (RL) algorithms (Silver et al., 2014; Schulman et al., 2015a; Haarnoja et al., 2018). Among
the many PG variants, trust region learning (Kakade & Langford, 2002), with two typical embodiments of Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015a) and Proximal Policy
Optimisation (PPO) (Schulman et al., 2017) algorithms, offer supreme empirical performance in
both discrete and continuous RL problems (Duan et al., 2016; Mahmood et al., 2018). The effectiveness of trust region methods largely stems from their theoretically-justified policy iteration
procedure. By optimising the policy within a trustable neighbourhood of the current policy, thus
avoiding making aggressive updates towards risky directions, trust region learning enjoys the guarantee of monotonic performance improvement at every iteration.
In multi-agent reinforcement learning (MARL) settings (Yang & Wang, 2020), naively applying
policy gradient methods by considering other agents as a part of the environment can lose its effectiveness. This is intuitively clear: once a learning agent updates its policy, so do its opponents;
this however changes the loss landscape of the learning agent, thus harming the improvement effect
from the PG update. As a result, applying independent PG updates in MARL offers poor convergence property (Claus & Boutilier, 1998). To address this, a learning paradigm named centralised
training with decentralised execution (CTDE) (Lowe et al., 2017b; Foerster et al., 2018; Zhou et al.,
2021) was developed. In CTDE, each agent is equipped with a joint value function which, during
âˆ—
First two authors contribute equally. Code is available at https://github.com/cyanrain7/
Trust-Region-Policy-Optimisation-in-Multi-Agent-Reinforcement-Learning.

1

Published as a conference paper at ICLR 2022

training, has access to the global state and opponentsâ€™ actions. With the help of the centralised value
function that accounts for the non-stationarity caused by others, each agent adapts its policy parameters accordingly. As such, the CTDE paradigm allows a straightforward extension of single-agent
PG theorems (Sutton et al., 2000; Silver et al., 2014) to multi-agent scenarios (Lowe et al., 2017b;
Kuba et al., 2021; Mguni et al., 2021). Consequently, fruitful multi-agent policy gradient algorithms
have been developed (Foerster et al., 2018; Peng et al., 2017; Zhang et al., 2020; Wen et al., 2018;
2020; Yang et al., 2018).
Unfortunately, existing CTDE methods offer no solution of how to perform trust region learning in
MARL. Lack of such an extension impedes agents from learning monotonically improving policies
in a stable manner. Recent attempts such as IPPO (de Witt et al., 2020a) and MAPPO (Yu et al.,
2021) have been proposed to fill such a gap; however, these methods are designed for agents that are
homogeneous (i.e., sharing the same action space and policy parameters), which largely limits their
applicability and potentially harm the performance. As we show in Proposition 1 later, parameter
sharing could suffer from an exponentially-worse suboptimal outcome. On the other hand, although
IPPO/MAPPO can be practically applied in a non-parameter sharing way, it still lacks the essential
theoretical property of trust region learning, which is the monotonic improvement guarantee.
In this paper, we propose the first theoretically-justified trust region learning framework in MARL.
The key to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. With the advantage decomposition lemma, we introduce a multi-agent policy
iteration procedure that enjoys the monotonic improvement guarantee. To implement such a procedure, we propose two practical algorithms: Heterogeneous-Agent Trust Region Policy Optimisation
(HATRPO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO). HATRPO/HAPPO
adopts the sequential update scheme, which saves the cost of maintaining a centralised critic for
each agent in CTDE. Importantly, HATRPO/HAPPO does not require homogeneity of agents, nor
any other restrictive assumptions on the decomposibility of the joint Q-function (Rashid et al., 2018).
We evaluate HATRPO and HAPPO on benchmarks of StarCraftII and Multi-Agent MuJoCo against
strong baselines such as MADDPG (Lowe et al., 2017a), IPPO (de Witt et al., 2020b) and MAPPO
(Yu et al., 2021); results clearly demonstrate its state-of-the-art performance across all tested tasks.

2

P RELIMINARIES

In this section, we first introduce problem formulation and notations for MARL, and then briefly
review trust region learning in RL and discuss the difficulty of extending it to MARL. We end by
surveying existing MARL work that relates to trust region methods and show their limitations.
2.1

C OOPERATIVE MARL P ROBLEM F ORMULATION AND N OTATIONS

We consider a Markov game (Littman, 1994), which is defined by a tuple hN
Q,nS, A, P, r, Î³i. Here,
N = {1, . . . , n} denotes the set of agents, S is the finite state space, A = i=1 Ai is the product
of finite action spaces of all agents, known as the joint action space, P : S Ã— A Ã— S â†’ [0, 1] is the
transition probability function, r : S Ã— A â†’ R is the reward function, and Î³ âˆˆ [0, 1) is the discount
factor. The agents interact with the environment according to the following protocol: at time step t âˆˆ
N, the agents are at state st âˆˆ S; every agent i takes an action ait âˆˆ Ai , drawn from its policy Ï€ i (Â·|st ),
1
n
which together with other
Qnagentsâ€™ actions gives a joint action at = (at , . . . , at ) âˆˆ A, drawn from the
joint policy Ï€(Â·|st ) = i=1 Ï€ i (Â·i |st ); the agents receive a joint reward rt = r(st , at ) âˆˆ R, and move
to a state st+1 with probability P (st+1 |st , at ). The joint policy Ï€, the transition probabililty function
P , and the initial state distribution Ï0 , induce a marginal state distribution at time t, denoted by ÏtÏ€ .
Pâˆ t t
state value function
We define an (improper) marginal state distribution ÏÏ€ ,
t=0 Î³ ÏÏ€ . The

Pâˆ t
and the state-action value function are defined: VÏ€ (s) , Ea0:âˆ âˆ¼Ï€,s1:âˆ âˆ¼P
Î³ rt s0 = s and
t=0
 Pâˆ t

QÏ€ (s, a) , Es1:âˆ âˆ¼P,a1:âˆ âˆ¼Ï€
t=0 Î³ rt s0 = s, a0 = a . The advantage function is written
as AÏ€ (s, a) , QÏ€ (s, a) âˆ’ VÏ€ (s). In this paper, we consider a fully-cooperative setting where all
agents share the same reward function, aiming to maximise the expected total reward:
J(Ï€) , Es0:âˆ âˆ¼Ï0:âˆ
,a0:âˆ âˆ¼Ï€
Ï€

"âˆ
X

#
t

Î³ rt .

t=0

Throughout this paper, we pay close attention to the contribution to performance from different
subsets of agents. Before proceeding to our methods, we introduce following novel definitions.
2

Published as a conference paper at ICLR 2022

Definition 1. Let i1:m denote an ordered subset {i1 , . . . , im } of N , and let âˆ’i1:m refer to its complement. We write ik when we refer to the k th agent in the ordered subset. Correspondingly, the
multi-agent state-action value function is defined as

i

h

i1:m
QÏ€
s, ai1:m , Eaâˆ’i1:m âˆ¼Ï€âˆ’i1:m QÏ€ s, ai1:m , aâˆ’i1:m ,

and for disjoint sets j1:k and i1:m , the multi-agent advantage function is






AiÏ€1:m s, aj1:k , ai1:m , QjÏ€1:k ,i1:m s, aj1:k , ai1:m âˆ’ QjÏ€1:k s, aj1:k .

(1)

Hereafter, the joint policies Ï€ = (Ï€ 1 , . . . , Ï€ n ) and Ï€Ì„ = (Ï€Ì„ 1 , . . . , Ï€Ì„ n ) shall be thought of as the
â€œcurrentâ€, and the â€œnewâ€ joint policy that agents update towards, respectively.
2.2

T RUST R EGION A LGORITHMS IN R EINFORCEMENT L EARNING

Trust region methods such as TRPO (Schulman et al., 2015a) were proposed in single-agent RL
with an aim of achieving a monotonic improvement of J(Ï€) at each iteration. Formally, it can be
described by the following theorem.
Theorem 1. (Schulman et al., 2015a, Theorem 1) Let Ï€ be the current policy and Ï€Ì„ be
the next candidate policy. We define LÏ€ (Ï€Ì„) = J(Ï€) + Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì„ [AÏ€ (s, a)] , Dmax
KL (Ï€, Ï€Ì„) =
maxs DKL (Ï€(Â·|s), Ï€Ì„(Â·|s)) . Then the inequality of

(2)
J(Ï€Ì„) â‰¥ LÏ€ (Ï€Ì„) âˆ’ CDmax
KL Ï€, Ï€Ì„
holds, where C =

4Î³ maxs,a |AÏ€ (s,a)|
.
(1âˆ’Î³)2

The above theorem states that as the distance between the current policy Ï€ and a candidate policy Ï€Ì„
decreases, the surrogate LÏ€ (Ï€Ì„), which involves only the current policyâ€™s state distribution, becomes
an increasingly accurate estimate of the actual performance metric J(Ï€Ì„). Based on this theorem, an
iterative trust region algorithm is derived; at iteration k + 1, the agent updates its policy by

Ï€k+1 = arg max LÏ€k (Ï€) âˆ’ CDmax
KL (Ï€k , Ï€) .
Ï€

Such an update guarantees a monotonic improvement of the policy, i.e., J(Ï€k+1 ) â‰¥ J(Ï€k ). To implement this procedure in practical settings with parameterised policies Ï€Î¸ , Schulman et al. (2015a)
approximated the KL-penalty with a KL-constraint, which gave rise to the TRPO update of
Î¸k+1 = arg max LÏ€Î¸k (Ï€Î¸ ),
Î¸

subject to Esâˆ¼ÏÏ€Î¸ [DKL (Ï€Î¸k , Ï€Î¸ )] â‰¤ Î´.

(3)

k

At each iteration k + 1, TRPO constructs a KL-ball BÎ´ (Ï€Î¸k ) around the policy Ï€Î¸k , and optimises
Ï€Î¸k+1 âˆˆ BÎ´ (Ï€Î¸k ) to maximise LÏ€Î¸k (Ï€Î¸ ). By Theorem 1, we know that the surrogate objective
LÏ€Î¸k (Ï€Î¸ ) is close to the true reward J(Ï€Î¸ ) within BÎ´ (Ï€Î¸k ); therefore, Ï€Î¸k leads to improvement.
Furthermore, to save the cost on Esâˆ¼ÏÏ€Î¸ [DKL (Ï€Î¸k , Ï€Î¸ )] when computing Equation (3), Schulman
k
et al. (2017) proposed an approximation solution to TRPO that uses only first order derivatives,
known as PPO. PPO optimises the policy parameter Î¸k+1 by maximising the PPO-clip objective of





Ï€Î¸ (a|s)
Ï€Î¸ (a|s)
AÏ€Î¸k (s, a), clip
, 1 Â±  AÏ€Î¸k (s, a) .
LPPO
Ï€Î¸ (Ï€Î¸ ) = Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k min
k
k
Ï€Î¸k (a|s)
Ï€Î¸k (a|s)

(4)

with 1 +  or 1 âˆ’ , depending on whether or not the ratio
The clip operator replaces the ratio Ï€Ï€Î¸Î¸ (a|s)
(a|s)
k
is beyond the threshold interval. This effectively enables PPO to control the size of policy updates.
2.3

L IMITATIONS OF E XISTING T RUST R EGION M ETHODS IN MARL

Extending trust region methods to MARL is highly non-trivial. One naive approach is to equip all
agents with one shared set of parameters and use agentsâ€™ aggregated trajectories to conduct policy
optimisation at every iteration. This approach was adopted by MAPPO (Yu et al., 2021) in which
the policy parameter Î¸ is optimised by maximising the objective of
LMAPPO
(Ï€Î¸ ) =
Ï€Î¸
k

n
X






Ï€Î¸ (ai |s)
Ï€Î¸ (ai |s)
Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k min
A
(s,
a),
clip
,
1
Â±

A
(s,
a)
.
Ï€
Ï€Î¸
k
k
Ï€Î¸k (ai |s) Î¸k
Ï€Î¸k (ai |s)
i=1
(5)

3

Published as a conference paper at ICLR 2022

Unfortunately, this simple approach has significant drawbacks. An obvious demerit is that parameter
sharing requires that all agents have identical action spaces, i.e., Ai = Aj , âˆ€i, j âˆˆ N , which limits
the class of MARL problems to solve. Importantly, enforcing parameter sharing is equivalent to
putting a constraint Î¸i = Î¸j , âˆ€i, j âˆˆ N on the joint policy space. In principle, this can lead to a
suboptimal solution. To elaborate, we demonstrate through an example in the following proposition.
Proposition 1. Letâ€™s consider a fully-cooperative game with an even number of agents n, one state,
and the joint action space {0, 1}n , where the reward is given by r(0n/2 , 1n/2 ) = r(1n/2 , 0n/2 ) = 1,
âˆ—
and r(a1:n ) = 0 for all other joint actions. Let J âˆ— be the optimal joint reward, and Jshare
be the
optimal joint reward under the shared policy constraint. Then
âˆ—
Jshare
2
= n.
Jâˆ—
2

For proof see Appendix B. In the above example, we show that parameter sharing can lead to a
suboptimal outcome that is exponentially worse with the increasing number of agents. We also
provide an empirical verification of this proposition in Appendix F.
Apart from parameter sharing, a more general approach to extend trust region methods in
MARL is to endow all agents with their own parameters, and at each iteration k + 1, agents
construct trust regions of {BÎ´ (Ï€Î¸i i )}iâˆˆN , and optimise their objectives {LÏ€Î¸k (Ï€Î¸i i Ï€Î¸âˆ’iâˆ’i )}iâˆˆN .
k

k

Admittedly, this approach can still be supported by the current
HATRPO
MAPPO implementation (Yu et al., 2021) if one turns off paMAPPO
& HAPPO
& IPPO
rameter sharing, thus distributing the summation in Equation
(5) to all agents. However, such an approach cannot offer a
rigorous guarantee of monotonic improvement during training.
In fact, agentsâ€™ local improvements in performance can jointly
lead to a worse outcome. For example, in Figure 1, we design a
single-state differential game where two agents draw their actions from Gaussian distributions with learnable means Âµ1 , Âµ2
Figure 1: Example of a two- and unit variance, and the reward function is r(a1 , a2 ) = a1 a2 .
player differentiable game with The failure of MAPPO-style approach comes from the fact
r(a1 , a2 ) = a1 a2 . We initialise that, although the reward function increases in each of the
two Gaussian policies with Âµ1 = agentsâ€™ (one-dimensional) update directions, it decreases in the
âˆ’0.25, Âµ2 = 0.25. The purple joint (two-dimensional) update direction.
intervals represent the KL-ball of
Î´ = 0.5. Individual trust region Having seen the limitations of existing trust region methods in
updates (red) decrease the joint re- MARL, in the following sections, we first introduce a multiturn, whereas our sequential up- agent policy iteration procedure that enjoys theoreticallydate (blue) leads to improvement. justified monotonic improvement guarantee. To implement
this procedure, we propose HATRPO and HAPPO algorithms,
which offer practical solutions to apply trust region learning in MARL without the necessity of assuming homogeneous agents while still maintaining the monotonic improvement property.

3

M ULTI -AGENT T RUST R EGION L EARNING

The purpose of this section is to develop a theoretically-justified trust region learning procedure in
the context of multi-agent learning. In Subsection 3.1, we present the policy iteration procedure with
monotonic improvement guarantee, and in Subsection 3.2, we analyse its properties during training
and at convergence. Throughout the work, we make the following regularity assumptions.
Assumption 1. There exists Î· âˆˆ R, such that 0 < Î·  1, and for every agent i âˆˆ N , the policy
space Î i is Î·-soft; that means that for every Ï€ i âˆˆ Î i , s âˆˆ S, and ai âˆˆ Ai , we have Ï€ i (ai |s) â‰¥ Î·.
3.1

T RUST R EGION L EARNING IN MARL WITH M ONOTONIC I MPROVEMENT G UARANTEE

We start by introducing a pivotal lemma which shows that the joint advantage function can be decomposed into a summation of each agentâ€™s local advantages. Importantly, this lemma offers a
critical intuition behind the sequential policy-update scheme that our algorithms later apply.
Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a
joint policy Ï€, for any state s, and any agent subset i1:m , the below equations holds.
m
 X

i
i1:m
AÏ€
s, ai1:m =
AÏ€j s, ai1:jâˆ’1 , aij .
j=1

4

Published as a conference paper at ICLR 2022

For proof see Appendix C.2. Notably, Lemma 1 holds in general for cooperative Markov games,
with no need for any assumptions on the decomposibility of the joint value function such as those in
VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) or Q-DPP (Yang et al., 2020).
Lemma 1 indicates an effective approach to search for the direction of performance improvement
(i.e., joint actions with positive advantage values) in multi-agent learning. Specifically, let agents
take actions sequentially by following an arbitrary order i1:n , assuming agent i1 takes an action aÌ„i1
such that Ai1 (s, aÌ„i1 ) > 0, and then, for the rest m = 2, . . . , n, the agent im takes an action aÌ„im
such that Aim (s, aÌ„i1:mâˆ’1 , aÌ„im ) > 0. For the induced joint action aÌ„, Lemma 1 assures that AÏ€Î¸ (s, aÌ„)
is positive, thus the performance is guaranteed to improve. To formally extend the above process
into a policy iteration procedure with monotonic improvement guarantee, we need the following
definitions.
Qmâˆ’1
Definition 2. Let Ï€ be a joint policy, Ï€Ì„ i1:mâˆ’1 = j=1 Ï€Ì„ ij be some other joint policy of agents
i1:mâˆ’1 , and Ï€Ì‚ im be some other policy of agent im . Then



LiÏ€1:m Ï€Ì„ i1:mâˆ’1 , Ï€Ì‚ im , Esâˆ¼ÏÏ€ ,ai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aim âˆ¼Ï€Ì‚im AiÏ€m s, ai1:mâˆ’1 , aim .
Note that, for any Ï€Ì„ i1:mâˆ’1 , we have



i1:m
LÏ€
Ï€Ì„ i1:mâˆ’1 , Ï€ im = Esâˆ¼ÏÏ€ ,ai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aim âˆ¼Ï€im AiÏ€m s, ai1:mâˆ’1 , aim



= Esâˆ¼ÏÏ€ ,ai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 Eaim âˆ¼Ï€im AiÏ€m s, ai1:mâˆ’1 , aim
= 0.

(6)

Building on Lemma 1 and Definition 2, we can finally generalise Theorem 1 of TRPO to MARL.
Lemma 2. Let Ï€ be a joint policy. Then, for any joint policy Ï€Ì„, we have
J(Ï€Ì„) â‰¥ J(Ï€) +

n
X
 i1:m i1:mâˆ’1 im 

im
im
LÏ€
Ï€Ì„
, Ï€Ì„
âˆ’ CDmax
KL (Ï€ , Ï€Ì„ ) .
m=1

For proof see Appendix C.2. This lemma provides an idea about how a joint policy can be improved.
Namely, by Equation (6), we know that if any agents were to set the values of the above summands
im
im
LiÏ€1:m (Ï€Ì„ i1:mâˆ’1 , Ï€Ì„ im ) âˆ’ CDmax
KL (Ï€ , Ï€Ì„ ) by sequentially updating their policies, each of them can
always make its summand be zero by making no policy update (i.e., Ï€Ì„ im = Ï€ im ). This implies
that any positive update will lead to an increment in summation. Moreover, as there are n agents
making policy updates, the compound increment can be large, leading to a substantial improvement.
Lastly, note that this property holds with no requirement on the specific order by which agents
make their updates; this allows for flexible scheduling on the update order at each iteration. To
summarise, we propose the following Algorithm 1. We want to highlight that the algorithm is
Algorithm 1 Multi-Agent Policy Iteration with Monotonic Improvement Guarantee
1: Initialise the joint policy Ï€0 = (Ï€01 , . . . , Ï€0n ).
2: for k = 0, 1, . . . do
3:
Compute the advantage function AÏ€k (s, a) for all state-(joint)action pairs (s, a).
4Î³
4:
Compute  = maxs,a |AÏ€k (s, a)| and C = (1âˆ’Î³)
2.
5:
Draw a permutaion i1:n of agents at random.
6:
for m = 1 : n do
h


i
i1:mâˆ’1
im
im
im
7:
Make an update Ï€k+1
= arg maxÏ€im LiÏ€1:m
Ï€k+1
, Ï€ im âˆ’ CDmax
KL (Ï€k , Ï€ ) .
k
8:
end for
9: end for

markedly different from naively applying the TRPO update, i.e., Equation (3), on the joint policy of
all agents. Firstly, our Algorithm 1 does not update the entire joint policy at once, but rather update
each agentâ€™s individual policy sequentially. Secondly, during the sequential update, each agent has a
unique optimisation objective that takes into account all previous agentsâ€™ updates, which is also the
key for the monotonic improvement property to hold.
3.2

T HEORETICAL A NALYSIS

Now we justify by the following theorm that Algorithm 1 enjoys monotonic improvement property.
5

Published as a conference paper at ICLR 2022

âˆ

Theorem 2. A sequence (Ï€k )k=0 of joint policies updated by Algorithm 1 has the monotonic improvement property, i.e., J(Ï€k+1 ) â‰¥ J(Ï€k ) for all k âˆˆ N.
For proof see Appendix C.2. With the above theorem, we finally claim a successful introduction of
trust region learning to MARL, as this generalises the monotonic improvement property of TRPO.
Moreover, we take a step further to study the convergence property of Algorithm 1. Before stating
the result, we introduce the following solution concept.
Definition 3. In a fully-cooperative game, a joint policy Ï€âˆ— = (Ï€âˆ—1 , . . . , Ï€âˆ—n ) is a Nash equilibrium
(NE) if for every i âˆˆ N , Ï€ i âˆˆ Î i implies J (Ï€âˆ— ) â‰¥ J Ï€ i , Ï€âˆ—âˆ’i .
NE (Nash, 1951) is a well-established game-theoretic solution concept. Definition 3 characterises
the equilibrium point at convergence for cooperative MARL tasks. Based on this, we have the
following result that describes Algorithm 1â€™s asymptotic convergence behaviour towards NE.
Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to
âˆ
begin the update, a sequence (Ï€k )k=0 of joint policies generated by the algorithm, in a cooperative
Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium.
For proof see Appendix C.3. In deriving this result, the novel details introduced by Algorithm 1
played an important role. The monotonic improvement property (Theorem 2), achieved through the
multi-agent advantage and the sequential update scheme, provided us with a guarantee on the convergence of the return. Furthermore, randomisation of the update order assured that, at convergence,
none of the agents is incentified to make an update. The proof is finalised by excluding a possibility
that the algorithm converges at non-equilibrium points.

4 P RACTICAL A LGORITHMS
When implementing Algorithm 1 in practice, large state and action spaces could prevent agents from
designating policies Ï€ i (Â·|s) for each state s separately. To handle this, we parameterise each agentâ€™s
policy Ï€Î¸i i by Î¸i , which, together with other agentsâ€™ policies, forms a joint policy Ï€Î¸ parametrised
by Î¸ = (Î¸1 , . . . , Î¸n ). In this section, we develop two deep MARL algorithms to optimise the Î¸.
4.1

HATRPO

im
im
Computing Dmax
KL Ï€Î¸ im , Ï€Î¸ im



in Algorithm 1 is challenging; it requires evaluating the KL-

k

divergence for all states at each iteration.
Similar to TRPO, one can ease this maximal KL
im
im
divergence penalty Dmax
KL Ï€Î¸ im , Ï€Î¸ im by replacing it with the expected KL-divergence constraint
Esâˆ¼ÏÏ€Î¸

k

k
h
i
im
(Â·|s) â‰¤ Î´ where Î´ is a threshold hyperparameter, and the expectation
(Â·|s),
Ï€
DKL Ï€ im
im
Î¸ im

Î¸k

can be easily approximated by stochastic sampling. With the above amendment, we propose practical HATRPO algorithm in which, at every iteration k + 1, given a permutation of agents i1:n , agent
im
imâˆˆ{1,...,n} sequentially optimises its policy parameter Î¸k+1
by maximising a constrained objective:
im
Î¸k+1
= arg max E

i

i

i
Î¸ m

sâˆ¼ÏÏ€Î¸ ,a 1:mâˆ’1 âˆ¼Ï€ 1:mâˆ’1
,aim âˆ¼Ï€ m
i
i

Î¸ im

k

Î¸

subject to Esâˆ¼ÏÏ€Î¸


k

1:mâˆ’1
k+1


 im
AÏ€Î¸ (s, ai1:mâˆ’1 , aim ) ,

im
DKL Ï€ im
im (Â·|s), Ï€Î¸ im (Â·|s)
Î¸k

k



â‰¤ Î´.

(7)

To compute the above equation, similar to TRPO, one can apply a linear approximation to the
objective function and a quadratic approximation to the KL constraint; the optimisation problem in
Equation (7) can be solved by a closed-form update rule as
s

2Î´
(Hkim )âˆ’1 gkim ,
(8)
gkim (Hkim )âˆ’1 gkim


im
where Hkim = âˆ‡2Î¸im Esâˆ¼ÏÏ€Î¸ DKL Ï€ im
is the Hessian of the expected KLi
im (Â·|s), Ï€Î¸ im (Â·|s)
Î¸ im =Î¸ m
im
Î¸k+1
= Î¸kim + Î±j

k

Î¸k

k

divergence, gkim is the gradient of the objective in Equation (7), Î±j < 1 is a positive coefficient that
is found via backtracking line search, and the product of (Hkim )âˆ’1 gkim can be efficiently computed
with conjugate gradient algorithm.
h
i
The last missing piece for HATRPO is to estimate Eai1:mâˆ’1 âˆ¼Ï€i1:mâˆ’1 ,aim âˆ¼Ï€im AiÏ€mÎ¸ s, ai1:mâˆ’1 , aim ,
Î¸k+1

Î¸ im

k

which poses new challenges because each agentâ€™s objective has to take into account all previous
6

Published as a conference paper at ICLR 2022

agentsâ€™ updates, and the size of input vaires. Fortunately, with the following proposition, we can
efficiently estimate this objective by employing a joint advantage estimator.
Qn
Proposition 2. Let Ï€ = j=1 Ï€ ij be a joint policy, and AÏ€ (s, a) be its joint advantage function.
Qmâˆ’1
Let Ï€Ì„ i1:mâˆ’1 = j=1 Ï€Ì„ ij be some other joint policy of agents i1:mâˆ’1 , and Ï€Ì‚ im be some other policy
of agent im . Then, for every state s,


Eai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aim âˆ¼Ï€Ì‚im AiÏ€m s, ai1:mâˆ’1 , aim
h Ï€Ì‚ im (aim |s)
 Ï€Ì„ i1:mâˆ’1 (ai1:mâˆ’1 |s)
i
= Eaâˆ¼Ï€
âˆ’
1
A
(s,
a)
.
(9)
Ï€
Ï€ im (aim |s)
Ï€ i1:mâˆ’1 (ai1:mâˆ’1 |s)
For proof see Appendix D.1. One benefit of applying Equation (9) is that agents only need to maintain a joint advantage estimator AÏ€ (s, a) rather than one centralised critic for each individual agent
(e.g., unlike CTDE methods such as MADDPG). Another practical benefit one can draw is that,
given an estimator AÌ‚(s, a) of the advantage function AÏ€Î¸k (s, a), for example GAE (Schulman et al.,
2015b), we can estimate Eai1:mâˆ’1 âˆ¼Ï€i1:mâˆ’1 ,aim âˆ¼Ï€im
Î¸

 Ï€ im (aim |s)

Î¸
âˆ’1
Ï€Î¸im
(aim |s)
k



i1:mâˆ’1
k+1


M i1:m s, a ,

Î¸ im

h

AiÏ€mÎ¸

k

where M i1:m =

s, ai1:mâˆ’1 , aim

i

with an estimator of


Ï€Ì„ i1:mâˆ’1 (ai1:mâˆ’1 |s)
AÌ‚ s, a .
i1:mâˆ’1
i1:mâˆ’1
Ï€
(a
|s)

(10)

Notably, Equation (10) aligns nicely with the sequential update scheme in HATRPO. For agent im ,
since previous agents i1:mâˆ’1 have already made their updates, the compound policy ratio for M i1:m
in Equation (10) is easy to compute. Given a batch B of trajectories with length T , we can estimate
the gradient with respect to policy parameters (derived in Appendix D.2) as follows,
T

gÌ‚kim =

1 X X i1:m
i
M
(st , at )âˆ‡Î¸im log Ï€Î¸im
im (at |st ) Î¸ im =Î¸ im .
|B| Ï„ âˆˆB t=0
k

The term âˆ’1 Â· M i1:m (s, a) of Equation (10) is not reflected in gÌ‚kim , as it only introduces a constant
with zero gradient. Along with the Hessian of the expected KL-divergence, i.e., Hkim , we can update
im
Î¸k+1
by following Equation (8). The detailed pseudocode of HATRPO is listed in Appendix D.3.
4.2

HAPPO

To further alleviate the computation burden from Hkim in HATRPO, one can follow the idea of PPO
in Equation (4) by considering only using first order derivatives. This is achieved by making agent
im
im choose a policy parameter Î¸k+1
which maximises the clipping objective of
"
Esâˆ¼ÏÏ€Î¸ ,aâˆ¼ Ï€Î¸k min
k

i
Ï€Î¸im
im (a |s)
i
Ï€ im
im (a |s)

M

i1:m


(s, a) , clip

Î¸k

i
Ï€Î¸im
im (a |s)
i
Ï€ im
im (a |s)

!#

i1:m
,1 Â±  M
(s, a)
.

(11)

Î¸k

The optimisation process can be performed by stochastic gradient methods such as Adam (Kingma
& Ba, 2014). We refer to the above procedure as HAPPO and Appendix D.4 for its full pseudocode.
4.3

R ELATED W ORK

We are fully aware of previous attempts that tried to extend TRPO/PPO into MARL. Despite empirical successes, none of them managed to propose a theoretically-justified trust region protocol
in multi-agent learning, or maintain the monotonic improvement property. Instead, they tend to
impose certain assumptions to enable direct implementations of TRPO/PPO in MARL problems.
For example, IPPO (de Witt et al., 2020a) assume homogeneity of action spaces for all agents and
enforce parameter sharing. Yu et al. (2021) proposed MAPPO which enhances IPPO by considering a joint critic function and finer implementation techniques for on-policy methods. Yet, it still
suffers similar drawbacks of IPPO due to the lack of monotonic improvement guarantee especially
when the parameter-sharing condition is switched off. Wen et al. (2021) adjusted PPO for MARL
by considering a game-theoretical approach at the meta-game level among agents. Unfortunately, it
can only deal with two-agent cases due to the intractability of Nash equilibrium. Recently, Li & He
(2020) tried to implement TRPO for MARL through distributed consensus optimisation; however,
they enforced the same ratio Ï€Ì„ i (ai |s)/Ï€ i (ai |s) for all agents (see their Equation (7)), which, similar
7

Published as a conference paper at ICLR 2022

2c_vs_64zg

0.6
0.4
0.2
0.0

0.5

1.0

Environment Steps

1.5

(a) 2c-vs-64zg (hard)

2.01e7

0.8
0.6
0.4
0.2
0.0

0.0

0.5

1.0

Environment Steps

corridor

1.0
Evaluate Winning Rate

0.8

0.0

3s5z

1.0
Evaluate Winning Rate

Evaluate Winning Rate

1.0

1.5

(b) 3s5z (hard)

2.01e7

0.8
0.6
0.4

HATRPO
HAPPO
MAPPO

0.2
0.0

0.0

0.5

1.0

Environment Steps

1.5

2.01e7

(c) corridor (super hard)

Figure 2: Performance comparisons between HATRPO/HAPPO and MAPPO on three SMAC tasks.
Since all methods achieve 100% win rate, we believe SMAC is not sufficiently difficult to discriminate the capabilities of these algorithms, especially when non-parameter sharing is not required.
to parameter sharing, largely limits the policy space for optimisation. Moreover, their method comes
with a Î´/n KL-constraint threshold that fails to consider scenarios with large agent number.
One of the key ideas behind our HATRPO/HAPPO is the sequential update scheme. A similar
idea of multi-agent sequential update was also discussed in the context of dynamic programming
(Bertsekas, 2019) where artificial â€œin-betweenâ€ states have to be considered. On the contrary, our
sequential update sceheme is developed based on Lemma 1, which does not require any artificial assumptions and hold for any cooperative games. Furthermore, Bertsekas (2019) requires to maintain
a fixed order of updates that is pre-defined for the task, whereas the order in HATRPO/MAPPO can
be randomised at each iteration, which also offers desirable convergence property, as stated in Proposition 3 and also verified through ablation studies in Appendix F. The idea of sequential update also
appeared in principal component analysis; in EigenGame (Gemp et al., 2020) eigenvectors, represented as players, maximise their own utility functions one-by-one. Although EigenGame provably
solves the PCA problem, it is of little use in MARL, where a single iteration of sequential updates
is insufficient to learn complex policies. Furthermore, its design and analysis rely on closed-form
matrix calculus, which has no extension to MARL.
Lastly, we would like to highlight the importance of the decomposition result in Lemma 1. This
result could serve as an effective solution to value-based methods in MARL where tremendous
efforts have been made to decompose the joint Q-function into individual Q-functions when the
joint Q-function are decomposable (Rashid et al., 2018). Lemma 1, in contrast, is a general result
that holds for any cooperative MARL problems regardless of decomposibility. As such, we think of
it as an appealing contribution to future developments on value-based MARL methods.

5 E XPERIMENTS AND R ESULTS
We consider two most common benchmarksâ€”StarCraftII Multi-Agent Challenge (SMAC)
(Samvelyan et al., 2019) and Multi-Agent MuJoCo (de Witt et al., 2020b)â€”for evaluating MARL
algorithms. All hyperparameter settings and implementations details can be found in Appendix E.
StarCraftII Multi-Agent Challenge (SMAC). SMAC contains a set of StarCraft maps in which
a team of ally units aims to defeat the opponent team. IPPO (de Witt et al., 2020a) and MAPPO
(Yu et al., 2021) are known to achieve supreme results on this benchmark. By adopting parameter
sharing, these methods achieve a winning rate of 100% on most maps, even including the maps that
have heterogeneous agents. Therefore, we hypothesise that non-parameter sharing is not necessarily
required and the trick of sharing policies is sufficient to solve SMAC tasks. We test our methods
on two hard maps and one super-hard; results on Figure 2 confirm that SMAC is not sufficiently
difficult to show off the capabilities of HATRPO/HAPPO when compared against existing methods.
Multi-Agent MuJoCo. In comparison to SMAC, we believe Mujoco enviornment provides a more
suitable testing case for our methods. MuJoCo tasks challenge a robot to learn an optimal way
of motion; Multi-Agent MuJoCo models each part of a robot as an independent agent, for example, a leg for a spider or an arm for a swimmer. With the increasing variety of the body parts,
modelling heterogeneous policies becomes necessary. Figure 3 demonstrate that, in all scenarios,
HATRPO and HAPPO enjoy superior performance over those of parameter-sharing methods: IPPO
and MAPPO, and also outperform non-parameter sharing MADDPG (Lowe et al., 2017b) both in
terms of reward values and variance. It is also worth noting that the performance gap between HATRPO and its rivals enlarges with the increasing number of agents. Meanwhile, we can observe that
HATRPO outperforms HAPPO in almost all tasks; we believe it is because the hard KL constraint
8

Published as a conference paper at ICLR 2022

3000
2000
1000
0.2

0.4

0.6

Environment steps

0.8

1000
0
0.0

0.2

0.4

0.6

Environment steps

0.8

(b) 4x2-Agent Ant
HalfCheetah 3x2

3000
2000
1000
0
0.2

0.4

0.6

Environment steps

0.8

5000
4000
3000
2000
1000
0
0.0

0.2

0.4

0.6

Environment steps

0.8

Walker 3x2

1000
0
0.2

0.4

0.6

Environment steps

0.8

2000
1000
0
0.2

0.4

0.6

Environment steps

0.8

(g) 2x3-Agent Walker

(h) 3x2-Agent Walker

Humanoid 17x1

HumanoidStandup 17x1

Average Episode Reward

800
700
600
500
400
300
200
100
0.0

3000

0.0

0.2

0.4

0.6

Environment steps

0.8

1.0
1e7

(j) 17x1-Agent Humanoid

160
1e3
140

5000
4000

60
40
20
0.6

Environment steps

1.0
1e7

2000
1000
0
0.2

0.4

0.6

0.8

Environment steps

1.0
1e7

(f) 6x1-Agent HalfCheetah

0.8

HATRPO
HAPPO
MAPPO
IPPO
MADDPG

5000
4000
3000
2000
1000
0
0.0

0.2

0.4

0.6

Environment steps

0.8

1.0
1e7

ManyAgentSwimmer 10x2

250

80

0.4

0.8

(i) 6x1-Agent Walker

100

0.2

0.6

3000

1.0
1e7

120

0.0

0.4

Environment steps

Walker 6x1

4000

1.0
1e7

0.2

HATRPO
HAPPO
MAPPO
IPPO
MADDPG

6000

0.0

Average Episode Reward

0.0

0.0

Average Episode Reward

Average Episode Reward

2000

0

1.0
1e7

(e) 3x2-Agent HalfCheetah

3000

1000

HalfCheetah 6x1

Walker 2x3

4000

2000

(c) 8x1-Agent Ant

(d) 2x3-Agent HalfCheetah
5000

3000

1.0
1e7

6000

1.0
1e7

HATRPO
HAPPO
MAPPO
IPPO
MADDPG

4000

Average Episode Reward

Average Episode Reward

4000

Average Episode Reward

2000

HalfCheetah 2x3

5000

Average Episode Reward

3000

(a) 2x4-Agent Ant
6000

0.0

4000

1.0
1e7

Ant 8x1

5000

Average Episode Reward

4000

0.0

Average Episode Reward

Ant 4x2
Average Episode Reward

Average Episode Reward

Ant 2x4

1.0
1e7

(k) 17x1-Agent HumanoidStandup

200
150
100
50
0
50
100
150

0.0

0.2

0.4

0.6

Environment steps

0.8

1.0
1e7

(l) 10x2-Agent Swimmer

Figure 3: Performance comparison on multiple Multi-Agent MuJoCo tasks. HAPPO and HATRPO
consistently outperform their rivals, thus establishing a new state-of-the-art algorithm for MARL.
The performance gap enlarges with increasing number of agents.
in HATRPO, compared to the clipping version in HAPPO, relates more closely to Algorithm 1 that
attains monotonic improvement guarantee.

6

C ONCLUSION

In this paper, we successfully apply trust region learning to multi-agent settings by proposing the first
MARL algorithm that attains theoretically-justified monotonical improvement property. The key to
our development is the multi-agent advantage decomposition lemma that holds in general with no
need for any assumptions on agents sharing parameters or the joint value function being decomposable. Based on this, we introduced two practical deep MARL algorithms: HATRPO and HAPPO.
Experimental results on both discrete and continuous control tasks (i.e., SMAC and Multi-Agent
Mujoco) confirm their state-of-the-art performance. For future work, we will consider incorporating
the safety constraint into HATRPO/HAPPO and propose rigorous safety-aware MARL solutions.

9

Published as a conference paper at ICLR 2022

R EFERENCES
Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning.
arXiv:1910.00120, 2019.

arXiv preprint

Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020a.
Christian SchroÌˆder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip H. S. Torr, Wendelin
BoÌˆhmer, and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. CoRR, abs/2003.06709, 2020b.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International conference on machine learning,
pp. 1329â€“1338. PMLR, 2016.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: Pca as a nash
equilibrium. arXiv preprint arXiv:2010.00554, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861â€“1870. PMLR, 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
preprint arXiv:2108.08612, 2021.
Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv e-prints, pp. arXivâ€“
2010, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157â€“163. Elsevier, 1994.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 6382â€“6393, 2017a.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 6382â€“6393, 2017b.
A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra.
Benchmarking reinforcement learning algorithms on real-world robots. In Conference on robot
learning, pp. 561â€“591. PMLR, 2018.
David H Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jennings, and Jun Wang. Learning in nonzero-sum stochastic games with potentials. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 7688â€“7699. PMLR, 18â€“24 Jul
2021.
10

Published as a conference paper at ICLR 2022

John Nash. Non-cooperative games. Annals of mathematics, pp. 286â€“295, 1951.
P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionallycoordinated nets for learning to play starcraft combat games. arxiv 2017. arXiv preprint
arXiv:1703.10069, 2017.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295â€“4304. PMLR, 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889â€“1897. PMLR,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation.
arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387â€“395. PMLR, 2014.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085â€“2087, 2018.
R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems 12,
volume 12, pp. 1057â€“1063. MIT Press, 2000.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.
Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. In International Conference on Learning Representations,
2018.
Ying Wen, Yaodong Yang, and Jun Wang. Modelling bounded rationality in multi-agent interactions
by generalized recursive reasoning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 414â€“421. International
Joint Conferences on Artificial Intelligence Organization, 7 2020. Main track.
Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A gametheoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021.
Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, and
Jun Zhu. Tianshou: a highly modularized deep reinforcement learning library. arXiv preprint
arXiv:2107.14171, 2021.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In International Conference on Machine Learning, pp. 5571â€“5580.
PMLR, 2018.
11

Published as a conference paper at ICLR 2022

Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757â€“10766. PMLR, 2020.
Chao Yu, A. Velu, Eugene Vinitsky, Yu Wang, A. Bayen, and Yi Wu. The surprising effectiveness
of mappo in cooperative, multi-agent games. ArXiv, abs/2103.01955, 2021.
Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun
Wang. Bi-level actor-critic for multi-agent coordination. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pp. 7325â€“7332, 2020.
Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang,
Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent
reinforcement learning. arXiv preprint arXiv:2106.07551, 2021.

12

Published as a conference paper at ICLR 2022

Appendices
A Preliminaries

14

A.1 Definitions and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

A.2 Proofs of Preliminary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

B Proof of Proposition 1

16

C Derivation and Analysis of Algorithm 1

17

C.1 Recap of Existing Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

C.2 Analysis of Training of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . .

17

C.3 Analysis of Convergence of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . .

19

D HATRPO and HAPPO

22

D.1 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22

D.2 Derivation of the gradient estimator for HATRPO . . . . . . . . . . . . . . . . . .

22

D.3 Pseudocode of HATRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

D.4 Pseudocode of HAPPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

E Hyper-parameter Settings for Experiments

24

F Ablation Experiments

26

G Ablation Study on Non-Parameter Sharing MAPPO/IPPO

26

H Multi-Agent Particle Environment Experiments

27

13

Published as a conference paper at ICLR 2022

A

P RELIMINARIES

A.1

D EFINITIONS AND A SSUMPTIONS

Assumption 1. There exists Î· âˆˆ R, such that 0 < Î·  1, and for every agent i âˆˆ N , the policy
space Î i is Î·-soft; that means that for every Ï€ i âˆˆ Î i , s âˆˆ S, and ai âˆˆ Ai , we have Ï€ i (ai |s) â‰¥ Î·.
Definition 3. In a fully-cooperative game, a joint policy Ï€âˆ— = (Ï€âˆ—1 , . . . , Ï€âˆ—n ) is a Nash equilibrium
(NE) if for every i âˆˆ N , Ï€ i âˆˆ Î i implies J (Ï€âˆ— ) â‰¥ J Ï€ i , Ï€âˆ—âˆ’i .
Definition 4. Let X be a finite set and p : X â†’ R, q : X â†’ R be two maps. Then, the notion of
distance between p and q that we adopt is given by ||p âˆ’ q|| , maxxâˆˆX |p(x) âˆ’ q(x)|.
A.2

P ROOFS OF P RELIMINARY R ESULTS

Lemma 3. Every agent iâ€™s policy space Î i is convex and compact under the maximum norm.
Proof. We start from proving convexity od the policy space: we prove that, for any two policies
Ï€ i , Ï€Ì„ i âˆˆ Î i , for every Î± âˆˆ [0, 1], Î±Ï€ i + (1 âˆ’ Î±)Ï€Ì„ i âˆˆ Î i . Clearly, for all s âˆˆ S and ai âˆˆ Ai , we
have
Î±Ï€ i (ai |s) + (1 âˆ’ Î±)Ï€Ì„ i (ai |s) â‰¥ Î±Î· + (1 âˆ’ Î±)Î· = Î·.

Also, for every state s,
Xh

i
X i i
X i i
Î±Ï€ i (ai |s) + (1 âˆ’ Î±)Ï€Ì„ i (ai |s) = Î±
Ï€ (a |s) + (1 âˆ’ Î±)
Ï€Ì„ (a |s) = Î± + 1 âˆ’ Î± = 1,

ai

ai

ai

which establishes convexity.
For compactness, we first prove that Î i is closed.
âˆ
Let Ï€ki k=0 be a convergent sequence of policies of agent i. Let Ï€ i be the limit. We will prove that

Ï€ i is a policy. First, by Assumption 1, for any k âˆˆ N, s âˆˆ S, and ai âˆˆ Ai , we have Ï€ki ai |s â‰¥ Î·.
Hence,


Ï€ i ai |s = lim Ï€ki ai |s â‰¥ lim Î· â‰¥ Î·.
kâ†’âˆ
kâ†’âˆ

P
i
i
Furtheremore, for any k and s, we have ai Ï€k a |s = 1. Hence
X
X
 X


Ï€ i ai |s =
lim Ï€ i ai |s = lim
Ï€ i ai |s = lim 1 = 1.
ai

kâ†’âˆ

ai

kâ†’âˆ

kâ†’âˆ

ai

With these two conditions satisfied, Ï€ i is a policy, which proves the closure.
Further, for all policies Ï€ i , states s and actions a, |Ï€ i (ai |s)| â‰¤ 1. This means that ||Ï€ i ||max â‰¤ 1,
which proves boundedness. Hence, Î i is compact.
Lemma 4 (Continuity of ÏÏ€ ). The improper state distribution ÏÏ€ is continuous in Ï€.
Proof. First, let us show that for any t âˆˆ N, the distribution ÏtÏ€ is continous in Ï€. We will do it
by induction. This obviously holds when t = 0, as Ï0 does not depend on policy. Hence, we can
assume that for some t âˆˆ N, the distribution ÏtÏ€ is continuous in Ï€. Let us now consider two policies
Ï€ and Ï€Ì‚. Let s0 âˆˆ S. We have
0
t+1 0
Ït+1
Ï€ (s ) âˆ’ ÏÏ€Ì‚ (s ) =

X
s

ÏtÏ€ (s)

X

Ï€(a|s)P (s0 |s, a) âˆ’

a

s

XX t

ÏÏ€ (s)Ï€(a|s) âˆ’ ÏtÏ€Ì‚ (s)Ï€Ì‚(a|s) P (s0 |s, a)
=
s

â‰¤

a

XX
s

X

ÏtÏ€ (s)Ï€(a|s) âˆ’ ÏtÏ€Ì‚ (s)Ï€Ì‚(a|s) P (s0 |s, a)

a

14

ÏtÏ€Ì‚ (s)

X
a

Ï€Ì‚(a|s)P (s0 |s, a)

Published as a conference paper at ICLR 2022

=

XX
s

ÏtÏ€ (s)Ï€(a|s) âˆ’ ÏtÏ€ (s)Ï€Ì‚(a|s) + ÏtÏ€ (s)Ï€Ì‚(a|s) âˆ’ ÏtÏ€Ì‚ (s)Ï€Ì‚(a|s) P (s0 |s, a)

a

â‰¤

XX

â‰¤

XX
s

a

=

X

ÏtÏ€ (s)

s

ÏtÏ€ (s) |Ï€(a|s) âˆ’ Ï€Ì‚(a|s)| + Ï€Ì‚(a|s) ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s)



a


ÏtÏ€ (s)||Ï€ âˆ’ Ï€Ì‚|| + Ï€Ì‚(a|s)||ÏtÏ€ âˆ’ ÏtÏ€Ì‚ ||
X

s

||Ï€ âˆ’ Ï€Ì‚|| +

a

X

||ÏtÏ€ âˆ’ ÏtÏ€Ì‚ ||

X

s

Ï€Ì‚(a|s)

a

= |A| Â· ||Ï€ âˆ’ Ï€Ì‚|| + |S| Â· ||ÏtÏ€ âˆ’ ÏtÏ€Ì‚ ||.

Hence, we obtain
t
t
||Ït+1
âˆ’ Ït+1
(12)
Ï€
Ï€Ì‚ || â‰¤ |A| Â· ||Ï€ âˆ’ Ï€Ì„|| + |S| Â· ||ÏÏ€ âˆ’ ÏÏ€Ì‚ ||.
Using the base case, taking the limit as Ï€Ì„ â†’ Ï€, we get that the right-hand-side of Equation (12)
converges to 0, which proves that every Ït+1
is continuous in Ï€, and finishes the inductive step.
Ï€

We can now prove that the total marginal state distribution ÏÏ€ is continous in Ï€. To do that, letâ€™s
(1âˆ’Î³)
log[ 4 ]
. Equivalently, we choose T such
take an arbitrary  > 0, and a natural T such that T >
log Î³
T

2Î³
that 1âˆ’Î³
< 2 . Let Ï€ and Ï€Ì‚ be two policies. We have

|ÏÏ€ (s) âˆ’ ÏÏ€Ì‚ (s)| =

âˆ
X

Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s)



t=0

=

T
âˆ’1
X

âˆ
X


Î³ tâˆ’T ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s)
Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) + Î³ T

t=0

â‰¤

T
âˆ’1
X

t=T

Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) + Î³

âˆ
X
T

t=0

â‰¤

T
âˆ’1
X

Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) + Î³ T

t=0

=

T
âˆ’1
X

â‰¤

âˆ
X

2Î³ tâˆ’T

t=T

Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) +

t=0
T
âˆ’1
X

Î³ tâˆ’T ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s)

t=T

ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) +

t=0

T
âˆ’1
X
2Î³ T

<
Î³ t ÏtÏ€ (s) âˆ’ ÏtÏ€Ì‚ (s) +
1âˆ’Î³
2
t=0

T
âˆ’1
X


â‰¤
||ÏtÏ€ âˆ’ ÏtÏ€Ì‚ || + .
2
2
t=0

(13)

Now, by continuity of each of ÏtÏ€ , for t = 0, 1, . . . , T âˆ’ 1, we have that there exists a Î´ > 0 such that

||Ï€ âˆ’ Ï€Ì‚|| < Î´ implies ||ÏtÏ€ âˆ’ ÏtÏ€Ì‚ || < 2T
. Taking such Î´, by Equation (13), we get ||ÏÏ€ âˆ’ ÏÏ€Ì‚ || â‰¤ ,
which finishes the proof.
Lemma 5 (Continuity of QÏ€ ). Let Ï€ be a policy. Then QÏ€ (s, a) is Lipschitz-continuous in Ï€.
Proof. Let Ï€ and Ï€Ì‚ be two policies. Then we have
|QÏ€ (s, a) âˆ’ QÏ€Ì‚ (s, a)|
!
=

r(s, a) + Î³

XX
s0

=Î³

XX
s0

â‰¤Î³
=Î³
â‰¤Î³

r(s, a) + Î³

s0

P (s |s, a)Ï€Ì‚(a|s)QÏ€Ì‚ (s, a)

a

P (s0 |s, a) [Ï€(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€Ì‚ (s, a)]

P (s0 |s, a) |Ï€(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€Ì‚ (s, a)|
P (s0 |s, a) |Ï€(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€ (s, a) + Ï€Ì‚(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€Ì‚ (s, a)|

a

XX
s0

âˆ’

a

0

a

XX
s0

P (s |s, a)Ï€(a|s)QÏ€ (s, a)

!
XX

a

XX
s0

0

P (s0 |s, a) (|Ï€(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€ (s, a)| + |Ï€Ì‚(a|s)QÏ€ (s, a) âˆ’ Ï€Ì‚(a|s)QÏ€Ì‚ (s, a)|)

a

15

Published as a conference paper at ICLR 2022

=Î³

XX
a

+Î³

XX
s0

â‰¤Î³

P (s0 |s, a)|Ï€(a|s) âˆ’ Ï€Ì‚(a|s)| Â· |QÏ€ (s, a)|

s0

XX

a

P (s0 |s, a)||Ï€ âˆ’ Ï€Ì‚|| Â· Qmax

s0

a

+Î³

XX
s0

P (s0 |s, a)Ï€Ì‚(a|s) |QÏ€ (s, a) âˆ’ QÏ€Ì‚ (s, a)|

P (s0 |s, a)Ï€Ì‚(a|s)||QÏ€ âˆ’ QÏ€Ì‚ ||

a

= Î³ Qmax Â· |A| Â· ||Ï€ âˆ’ Ï€Ì‚|| + Î³||QÏ€ âˆ’ QÏ€Ì‚ ||
Hence, we get
||QÏ€ âˆ’ QÏ€Ì‚ || â‰¤ Î³ Qmax Â· |A| Â· ||Ï€ âˆ’ Ï€Ì‚|| + Î³||QÏ€ âˆ’ QÏ€Ì‚ ||,
which implies
||QÏ€ âˆ’ QÏ€Ì‚ || â‰¤

Î³ Qmax Â· |A| Â· ||Ï€ âˆ’ Ï€Ì‚||
.
1âˆ’Î³

(14)

max Â·|A|
Equation (14) establishes Lipschitz-continuity with a constant Î³ Q1âˆ’Î³
.

Corollary 1. From Lemma 5 we obtain that the following functions are Lipschitz-continuous in Ï€:
P
the state value function VÏ€ = a Ï€(a|s)QÏ€ (s, a),
the advantage function AÏ€ (s, a) = QÏ€ (s, a) âˆ’ VÏ€ (s),
and the expected total reward J(Ï€) = Esâˆ¼Ï0 [VÏ€ (s)].
Lemma 6. Let Ï€ and Ï€Ì‚ be policies. The quantity Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì‚ [AÏ€ (s, a)] is continuous in Ï€.
Proof. We have
Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì‚ [AÏ€ (s, a)] =

XX
s

ÏÏ€ (s)Ï€Ì‚(a|s)AÏ€ (s, a).

a

Continuity follows from continuity of each ÏÏ€ (s) (Lemma 4) and AÏ€ (s, a) (Corollary 1).
Corollary 2 (Continuity in MARL). All the results about continuity in Ï€ extend to MARL. Policy Ï€
can be replaced with joint policy Ï€; as Ï€ is Lipschitz-continuous in agent iâ€™s policy Ï€ i , the above
continuity results extend to conitnuity in Ï€ i . Thus, we will quote them in our proofs for MARL.

B

P ROOF OF P ROPOSITION 1

Proposition 1. Letâ€™s consider a fully-cooperative game with an even number of agents n, one state,
and the joint action space {0, 1}n , where the reward is given by r(0n/2 , 1n/2 ) = r(1n/2 , 0n/2 ) = 1,
âˆ—
and r(a1:n ) = 0 for all other joint actions. Let J âˆ— be the optimal joint reward, and Jshare
be the
optimal joint reward under the shared policy constraint. Then
âˆ—
2
Jshare
= n.
Jâˆ—
2

Proof. Clearly J âˆ— = 1. An optimal joint policy in this case is, for example, the deterministic policy
with joint action (0n/2 , 1n/2 ).
Now, let the shared policy be (Î¸, 1âˆ’Î¸), where Î¸ determines the probability that an agent takes action
0. Then, the expected reward is




J(Î¸) = Pr a1:n = (0n/2 , 1n/2 ) Â· 1 + Pr a1:n = (1n/2 , 0n/2 ) Â· 1 = 2 Â· Î¸n/2 (1 âˆ’ Î¸)n/2 .

16

Published as a conference paper at ICLR 2022

In order to maximise J(Î¸), we must maximise Î¸(1 âˆ’ Î¸), or equivalently,
artithmetic-geometric means inequality, we have

p
Î¸(1 âˆ’ Î¸). By the

p
Î¸ + (1 âˆ’ Î¸)
1
Î¸(1 âˆ’ Î¸) â‰¤
= ,
2
2
where the equality holds if and only if Î¸ = 1 âˆ’ Î¸, that is Î¸ = 12 . In such case we have
 
1
2
âˆ—
Jshare = J
= 2 Â· 2âˆ’n/2 Â· 2âˆ’n/2 = n ,
2
2
which finishes the proof.

C

D ERIVATION AND A NALYSIS OF A LGORITHM 1

C.1

R ECAP OF E XISTING R ESULTS

Lemma 7 (Performance Difference). Let Ï€Ì„ and Ï€ be two policies. Then, the following identity
holds,
J(Ï€Ì„) âˆ’ J(Ï€) = Esâˆ¼ÏÏ€Ì„ ,aâˆ¼Ï€Ì„ [AÏ€ (s, a)] .
Proof. See Kakade & Langford (2002) (Lemma 6.1) or Schulman et al. (2015a) (Appendix A).
Theorem 1. (Schulman et al., 2015a, Theorem 1) Let Ï€ be the current policy and Ï€Ì„ be
the next candidate policy. We define LÏ€ (Ï€Ì„) = J(Ï€) + Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì„ [AÏ€ (s, a)] , Dmax
KL (Ï€, Ï€Ì„) =
maxs DKL (Ï€(Â·|s), Ï€Ì„(Â·|s)) . Then the inequality of

(2)
J(Ï€Ì„) â‰¥ LÏ€ (Ï€Ì„) âˆ’ CDmax
KL Ï€, Ï€Ì„
holds, where C =

4Î³ maxs,a |AÏ€ (s,a)|
.
(1âˆ’Î³)2

Proof. See Schulman et al. (2015a) (Appendix A and Equation (9) of the paper).
C.2

A NALYSIS OF T RAINING OF A LGORITHM 1

Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a
joint policy Ï€, for any state s, and any agent subset i1:m , the below equations holds.
m

 X
i
i1:m
AÏ€j s, ai1:jâˆ’1 , aij .
AÏ€
s, ai1:m =
j=1

Proof. By the definition of multi-agent advantage function,
i1:m
AÏ€
(s, ai1:m ) = QiÏ€1:m
(s, ai1:m ) âˆ’ VÏ€Î¸ (s)
Î¸
Î¸
m h
i
X
i1:kâˆ’1
i1:k
i1:kâˆ’1
=
QiÏ€1:k
(s,
a
)
âˆ’
Q
(s,
a
)
Ï€
Î¸
Î¸

=

k=1
m
X

AiÏ€kÎ¸ (s, ai1:kâˆ’1 , aik ),

k=1

which finishes the proof.
Note that a similar finding has been shown in Kuba et al. (2021).
Lemma 8. Let Ï€ =

Qn

i=1 Ï€

i

and Ï€Ì„ =

Qn

i=1 Ï€Ì„

i

Dmax
KL (Ï€, Ï€Ì„) â‰¤

be joint policies. Then
n
X
i=1

17

i
i
Dmax
KL Ï€ , Ï€Ì„



Published as a conference paper at ICLR 2022

Proof. For any state s, we have
DKL (Ï€(Â·|s), Ï€Ì„(Â·|s)) = Eaâˆ¼Ï€ [log Ï€(a|s) âˆ’ log Ï€Ì„(a|s)]
!
!#
"
n
n
Y
Y
= Eaâˆ¼Ï€ log
Ï€ i (ai |s) âˆ’ log
Ï€Ì„ i (ai |s)
i=1

= Eaâˆ¼Ï€

" n
X

i=1
i

i

log Ï€ (a |s) âˆ’

i=1

=

n
X

n
X

#
i

i

log Ï€Ì„ (a |s)

i=1

n

 X

Eai âˆ¼Ï€i ,aâˆ’i âˆ¼Ï€âˆ’i log Ï€ i (ai |s) âˆ’ log Ï€Ì„ i (ai |s) =
DKL Ï€ i (Â·|s), Ï€Ì„ i (Â·|s) .

i=1

i=1

Now, taking maximum over s on both sides yields
Dmax
KL (Ï€, Ï€Ì„) â‰¤

n
X


i
i
Dmax
KL Ï€ , Ï€Ì„ ,

i=1

as required.
Lemma 2. Let Ï€ be a joint policy. Then, for any joint policy Ï€Ì„, we have
n
X

 i1:m i1:mâˆ’1 im 
im
im
J(Ï€Ì„) â‰¥ J(Ï€) +
LÏ€
Ï€Ì„
, Ï€Ì„
âˆ’ CDmax
KL (Ï€ , Ï€Ì„ ) .
m=1

Proof. By Theorem 1
J(Ï€Ì„) â‰¥ LÏ€ (Ï€Ì„) âˆ’ CDmax
KL (Ï€, Ï€Ì„)
= J(Ï€) + Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì„ [AÏ€ (s, a)] âˆ’ CDmax
KL (Ï€, Ï€Ì„)
which by Lemma 1 equals
" n
#
X

im
i1:mâˆ’1 im
= J(Ï€) + Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì„
AÏ€ s, a
,a
âˆ’ CDmax
KL (Ï€, Ï€Ì„)
m=1

and by Lemma 8 this is at least
#
" n
n
X
X

i1:mâˆ’1 im
im
im
im
â‰¥ J(Ï€) + Esâˆ¼ÏÏ€ ,aâˆ¼Ï€Ì„
AÏ€ s, a
,a
âˆ’
CDmax
KL (Ï€ , Ï€Ì„ )
m=1

= J(Ï€) +
= J(Ï€) +

n
X
m=1
n
X

m=1

n

 X
im
im
Esâˆ¼ÏÏ€ ,ai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aim âˆ¼Ï€Ì„im AiÏ€m s, ai1:mâˆ’1 , aim âˆ’
CDmax
KL (Ï€ , Ï€Ì„ )
m=1
i1:m
LÏ€
Ï€Ì„ i1:mâˆ’1 , Ï€Ì„


im


im
im
âˆ’ CDmax
KL (Ï€ , Ï€Ì„ ) .

m=1

âˆ

Theorem 2. A sequence (Ï€k )k=0 of joint policies updated by Algorithm 1 has the monotonic improvement property, i.e., J(Ï€k+1 ) â‰¥ J(Ï€k ) for all k âˆˆ N.
Proof. Let Ï€0 be any joint policy. For every k â‰¥ 0, the joint policy Ï€k+1 is obtained from Ï€k by
Algorithm 1 update; for m = 1, . . . , n,
h


i
i1:mâˆ’1
im
im
im
Ï€k+1
= arg max LiÏ€1:m
Ï€k+1
, Ï€ im âˆ’ CDmax
.
KL Ï€k , Ï€
k
Ï€ im

18

Published as a conference paper at ICLR 2022

By Theorem 1, we have
J(Ï€k+1 ) â‰¥ LÏ€k (Ï€k+1 ) âˆ’ CDmax
KL (Ï€k , Ï€k+1 ),
which by Lemma 8 is lower-bounded by
n
X
im
im
â‰¥ LÏ€k (Ï€k+1 ) âˆ’
CDmax
KL (Ï€k , Ï€k+1 )
m=1

= J(Ï€k ) +

n 
X


i1:mâˆ’1
im
im
max im
LiÏ€1:m
(Ï€
,
Ï€
)
âˆ’
CD
(Ï€
,
Ï€
)
,
KL
k+1
k+1
k
k+1
k

(15)

m=1
im
and as for every m, Ï€k+1
is the argmax, this is lower-bounded by
n

X
i1:mâˆ’1
im
im
â‰¥ J(Ï€k ) +
LiÏ€1:m
(Ï€k+1
, Ï€kim ) âˆ’ CDmax
KL (Ï€k , Ï€k ) ,
k
m=1

which, as mentioned in Definition 2, equals
n
X
= J(Ï€k ) +
0 = J(Ï€k ),
m=1

where the last inequality follows from Equation (6). This proves that Algorithm 1 achieves monotonic improvement.
C.3

A NALYSIS OF C ONVERGENCE OF A LGORITHM 1

Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to
âˆ
begin the update, a sequence (Ï€k )k=0 of joint policies generated by the algorithm, in a cooperative
Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium.
âˆ

Proof. Step 1 (convergence). Firstly, it is clear that the sequence (J(Ï€k ))k=0 converges as, by
max
Â¯
Theorem 2, it is non-decreasing and bounded above by R
1âˆ’Î³ . Let us denote the limit by J. For every
k, we denote the tuple of agents, according to whose order the agents perform the sequential updates,
by ik1:n , and we note that ik1:n kâˆˆN is a random process. Furthermore, we know that the sequence
of policies (Ï€k ) is bounded, so by Bolzano-Weierstrass Theorem, it has at least one convergent
subsequence. Let Ï€Ì„ be any limit point of the sequence (note that the set of limit points is a random
âˆ
set), and Ï€kj j=0 be a subsequence converging to Ï€Ì„ (which is a random subsequence as well). By
continuity of J in Ï€ (Corollary 1), we have



Â¯
J(Ï€Ì„) = J lim Ï€kj = lim J Ï€kj = J.
(16)
jâ†’âˆ

jâ†’âˆ

For now, we introduce an auxiliary definition.
Definition 5 (TR-Stationarity). A joint policy Ï€Ì„ is trust-region-stationary (TR-stationary) if, for
every agent i,




i
i
Ï€Ì„ i = arg max Esâˆ¼ÏÏ€Ì„ ,ai âˆ¼Ï€i AiÏ€Ì„ (s, ai ) âˆ’ CÏ€Ì„ Dmax
,
KL Ï€Ì„ , Ï€

Ï€i
4Î³
where CÏ€Ì„ = (1âˆ’Î³)2 , and  = maxs,a |AÏ€Ì„ (s, a)|.

We will now establish the TR-stationarity of any limit point joint policy Ï€Ì„ (which, as stated above,
is a random variable). Let Ei0:âˆ
[Â·] denote the expected value operator under the random process
1:n
4Î³k
(i0:âˆ
).
Let
also

=
max
|A
k
s,a
Ï€k (s, a)|, and Ck = (1âˆ’Î³)2 . We have
1:n
0 = lim Ei0:âˆ
[J(Ï€k+1 ) âˆ’ J(Ï€k )]
1:n
kâ†’âˆ

â‰¥ lim Ei0:âˆ
[LÏ€k (Ï€k+1 ) âˆ’ Ck Dmax
KL (Ï€k , Ï€k+1 )] by Theorem 1
1:n
kâ†’âˆ
h k  k 
 k k i
i
i1
i
i1
â‰¥ lim Ei0:âˆ
LÏ€1k Ï€k+1
âˆ’ Ck Dmax
Ï€k1 , Ï€k+1
KL
1:n
kâ†’âˆ

by Equation (15) and the fact that each of its summands is non-negative
19

Published as a conference paper at ICLR 2022

Now,we consider an arbitrary limit point Ï€Ì„ from the (random) limit set, and a (random) subsequence
âˆ
Ï€kj j=0 that converges to Ï€Ì„. We get
 k  k 
 k

k
i1 j
i1 j
i1 j
i1 j
max
0 â‰¥ lim Ei0:âˆ
L
Ï€
âˆ’
C
D
Ï€
,
Ï€
.
Ï€kj
kj KL
kj +1
kj
kj +1
1:n
jâ†’âˆ

As the expectation is taken of non-negative random variables, and for every i âˆˆ N and k âˆˆ N, with
k
some positive probability pi , we have i1j = i (because every permutation has non-zero probability),
the above is bounded from below by
i

h
max
i
i
i
i
,
Ï€
,
Ï€
L
(Ï€
)
âˆ’
C
D
pi lim max
k
k
Ï€
KL
j
j
k
i
jâ†’âˆ Ï€

j

which, as Ï€kj converges to Ï€Ì„, equals to
 i i

max
i
i
pi max
L
(Ï€
)
âˆ’
C
D
Ï€Ì„
,
Ï€
â‰¥ 0, by Equation (6).
Ï€Ì„
Ï€Ì„
KL
i
Ï€

For convergence of LiÏ€k (Ï€ i ) we used Definition 2 combined with Lemma 6, for convergence
j
of Ckj we used Corollary 1, and the convergence of Dmax
KL follows from continuity of DKL and
max. This
proves
that,
for
any
limit
point
Ï€Ì„
of
the
random
process (Ï€k ) induced by Algorithm 1,


i
i
maxÏ€i LiÏ€Ì„ (Ï€ i ) âˆ’ CÏ€Ì„ Dmax
= 0, which is equivalent with Definition 5.
KL Ï€Ì„ , Ï€
Step 2 (dropping the penalty term). Now, we have to prove that TR-stationary points are NEs of
cooperative Markov games. The main step is to prove the following statement: a TR-stationary joint
policy Ï€Ì„, for every state s âˆˆ S, satisfies


Ï€Ì„ i = arg max Eai âˆ¼Ï€i AiÏ€Ì„ (s, ai ) .
(17)
Ï€i

We will use the technique of the proof by contradiction. Suppose that there is a state s0 such that
there exists a policy Ï€Ì‚ i with




(18)
Eai âˆ¼Ï€Ì‚i AiÏ€Ì„ (s0 , ai ) > Eai âˆ¼Ï€Ì„i AiÏ€Ì„ (s0 , ai ) .
i
Let us parametrise the policies Ï€ according to the template
i

Ï€ (Â·|s0 ) =

xi1 , . . . , xidi âˆ’1 , 1 âˆ’

i
dX
âˆ’1

xij



j=1
i
i
i
where the values of xj (j = 1, . . . , d âˆ’ 1) are such that Ï€ (Â·|s0 ) is a valid probability distribution.

Then we can rewrite our quantity of interest (the objective of Equation (17) as
i

i

j=1

h=1

âˆ’1
dX
âˆ’1

 dX


Eai âˆ¼Ï€i AiÏ€Ì„ (s0 , ai ) =
xij Â· AiÏ€Ì„ s0 , aij + (1 âˆ’
xih )AiÏ€Ì„ s0 , aidi

=

i
dX
âˆ’1





xij AiÏ€Ì„ s0 , aij âˆ’ AiÏ€Ì„ s0 , aidi + AiÏ€Ì„ s0 , aidi ,

j=1

which is an affine function of the policy parameterisation. It follows that its gradient (with respect to
xi ) and directional derivatives are constant in the space of policies at state s0 . The existance of policy
Ï€Ì‚ i (Â·|s0 ), for which Inequality (18) holds, implies that the directional derivative in the direction from
Ï€Ì„ i (Â·|s0 ) to Ï€Ì‚ i (Â·|s0 ) is strictly positive. We also have

âˆ‚DKL (Ï€Ì„ i (Â·|s0 ), Ï€ i (Â·|s0 ))
âˆ‚  i
=
(Ï€Ì„ (Â·|s0 ))T (log Ï€Ì„ i (Â·|s0 ) âˆ’ log Ï€ i (Â·|s0 ))
i
i
âˆ‚xj
âˆ‚xj

âˆ‚ 
âˆ’(Ï€Ì„ i )T log Ï€ i (omitting state s0 for brevity)
=
âˆ‚xij
!
di âˆ’1
dX
i âˆ’1
âˆ‚ X
âˆ‚ i
i
i
i
=âˆ’ i
Ï€Ì„k log xk âˆ’
Ï€Ì„ log 1 âˆ’
xk
âˆ‚xj
âˆ‚xij di
k=1

k=1

Ï€Ì„ji
Ï€Ì„di i
=âˆ’ i +
Pdi âˆ’1 i
xj
1 âˆ’ k=1
xk
=âˆ’

Ï€Ì„ji
Ï€Ì„di i
+
= 0, when evaluated at Ï€ i = Ï€Ì„ i ,
Ï€ji
Ï€di i
20

(19)

Published as a conference paper at ICLR 2022

which means that the KL-penalty has zero gradient at Ï€Ì„ i (Â·|s0 ). Hence, when evaluated at Ï€ i (Â·|s0 ) =
Ï€Ì„ i (Â·|s0 ), the objective



ÏÏ€Ì„ (s0 )Eai âˆ¼Ï€i AiÏ€Ì„ (s0 , ai ) âˆ’ CÏ€Ì„ DKL Ï€Ì„ i (Â·|s0 ), Ï€ i (Â·|s0 )
has a strictly positive directional derivative in the direction of Ï€Ì‚ i (Â·|s0 ). Thus, there exists a policy
Ï€
ei (Â·|s0 ), sufficiently close to Ï€Ì„ i (Â·|s0 ) on the path joining it with Ï€Ì‚ i (Â·|s0 ), for which



ÏÏ€Ì„ (s0 )Eai âˆ¼eÏ€i AiÏ€Ì„ (s0 , ai ) âˆ’ CÏ€Ì„ DKL Ï€Ì„ i (Â·|s0 ), Ï€
ei (Â·|s0 ) > 0.
Let Ï€âˆ—i be a policy such that Ï€âˆ—i (Â·|s0 ) = Ï€
ei (Â·|s0 ), and Ï€âˆ—i (Â·|s) = Ï€Ì„ i (Â·|s) for states s 6= s0 . As for
these states we have




ÏÏ€Ì„ (s)Eai âˆ¼Ï€âˆ—i AiÏ€Ì„ (s, ai ) = ÏÏ€Ì„ (s)Eai âˆ¼Ï€Ì„i AiÏ€Ì„ (s, ai ) = 0, and DKL (Ï€Ì„ i (Â·|s), Ï€âˆ—i (Â·|s)) = 0,
it follows that


 i
i
i
i
i
LiÏ€Ì„ (Ï€âˆ—i ) âˆ’ CÏ€Ì„ Dmax
ei (Â·|s0 )
Ï€ i AÏ€Ì„ (s0 , a ) âˆ’ CÏ€Ì„ DKL Ï€Ì„ (Â·|s0 ), Ï€
KL (Ï€Ì„ , Ï€âˆ— ) = ÏÏ€Ì„ (s0 )Eai âˆ¼e
i
i
> 0 = LiÏ€Ì„ (Ï€Ì„ i ) âˆ’ CÏ€Ì„ Dmax
KL (Ï€Ì„ , Ï€Ì„ ),

which is a contradiction with TR-stationarity of Ï€Ì„. Hence, the claim of Equation (17) is proved.
Step 3 (optimality). Now, for a fixed joint policy Ï€Ì„ âˆ’i of other agents, Ï€Ì„ i satisfies




Ï€Ì„ i = arg max Eai âˆ¼Ï€i AiÏ€Ì„ (s, ai ) = arg max Eai âˆ¼Ï€i QiÏ€Ì„ (s, ai ) , âˆ€s âˆˆ S,
Ï€i

Ï€i

which is the Bellman optimality equation (Sutton & Barto, 2018). Hence, for a fixed joint policy
Ï€Ì„ âˆ’i , the policy Ï€Ì„ i is optimal:
Ï€Ì„ i = arg max J(Ï€ i , Ï€Ì„ âˆ’i ).
Ï€i

As agent i was chosen arbitrarily, Ï€Ì„ is a Nash equilibrium.

21

Published as a conference paper at ICLR 2022

D

HATRPO AND HAPPO

D.1

P ROOF OF P ROPOSITION 2
Qn
Proposition 2. Let Ï€ = j=1 Ï€ ij be a joint policy, and AÏ€ (s, a) be its joint advantage function.
Q
mâˆ’1
Let Ï€Ì„ i1:mâˆ’1 = j=1 Ï€Ì„ ij be some other joint policy of agents i1:mâˆ’1 , and Ï€Ì‚ im be some other policy
of agent im . Then, for every state s,


Eai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aim âˆ¼Ï€Ì‚im AiÏ€m s, ai1:mâˆ’1 , aim
h Ï€Ì‚ im (aim |s)
 Ï€Ì„ i1:mâˆ’1 (ai1:mâˆ’1 |s)
i
= Eaâˆ¼Ï€
âˆ’
1
A
(s,
a)
.
(9)
Ï€
Ï€ im (aim |s)
Ï€ i1:mâˆ’1 (ai1:mâˆ’1 |s)
Proof. We have
 i1:m i1:m

Ï€Ì„ i1:mâˆ’1 (ai1:mâˆ’1 |s)
Ï€Ì„
(a
|s)
AÏ€ (s, a) âˆ’ i1:mâˆ’1 i1:mâˆ’1 AÏ€ (s, a)
= Eaâˆ¼Ï€
Ï€ i1:m (ai1:m |s)
Ï€
(a
|s)

 i1:m i1:m
(a
|s)
Ï€Ì„
i1:m
âˆ’i1:m
AÏ€ (s, a
,a
)
= Eai1:m âˆ¼Ï€i1:m ,aâˆ’i1:m âˆ¼Ï€âˆ’i1:m
Ï€ i1:m (ai1:m |s)

 i1:mâˆ’1 i1:mâˆ’1
Ï€Ì„
(a
|s)
i1:mâˆ’1
âˆ’i1:mâˆ’1
âˆ’ Eai1:mâˆ’1 âˆ¼Ï€i1:mâˆ’1 ,aâˆ’i1:mâˆ’1 âˆ¼Ï€âˆ’i1:mâˆ’1
AÏ€ (s, a
,a
)
Ï€ i1:mâˆ’1 (ai1:mâˆ’1 |s)


= Eai1:m âˆ¼Ï€Ì„i1:m ,aâˆ’i1:m âˆ¼Ï€âˆ’i1:m AÏ€ (s, ai1:m , aâˆ’i1:m )


âˆ’ Eai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 ,aâˆ’i1:mâˆ’1 âˆ¼Ï€âˆ’i1:mâˆ’1 AÏ€ (s, ai1:mâˆ’1 , aâˆ’i1:mâˆ’1 )



= Eai1:m âˆ¼Ï€Ì„i1:m Eaâˆ’i1:m âˆ¼Ï€âˆ’i1:m AÏ€ (s, ai1:m , aâˆ’i1:m )



âˆ’ Eai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 Eaâˆ’i1:mâˆ’1 âˆ¼Ï€âˆ’i1:mâˆ’1 AÏ€ (s, ai1:mâˆ’1 , aâˆ’i1:mâˆ’1 )


= Eai1:m âˆ¼Ï€Ì„i1:m AiÏ€1:m (s, ai1:m )


âˆ’ Eai1:mâˆ’1 âˆ¼Ï€Ì„i1:mâˆ’1 AiÏ€1:mâˆ’1 (s, ai1:mâˆ’1 ) ,
which, by Lemma 1, equals


= Eai1:m âˆ¼Ï€Ì„i1:m AiÏ€1:m (s, ai1:m ) âˆ’ AiÏ€1:mâˆ’1 (s, ai1:mâˆ’1 )


= Eai1:m âˆ¼Ï€Ì„i1:m AiÏ€m (s, ai1:mâˆ’1 , aim ) .

D.2

D ERIVATION OF THE GRADIENT ESTIMATOR FOR HATRPO
"

im
Ï€Î¸im
|s)
im (a

âˆ‡Î¸im Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k

!

#
i1:m

âˆ’1 M
(s, a)
im |s)
Ï€Î¸im
im (a
k
#
"
"
#
im
Ï€Î¸im
|s) i1:m
im (a
= âˆ‡Î¸im Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k im
M
(s, a) âˆ’ âˆ‡Î¸im Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k M i1:m (s, a)
k
k
Ï€Î¸im (aim |s)
k
"
#
im
âˆ‡Î¸im Ï€Î¸im
|s) i1:m
im (a
= Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k
M
(s, a)
im |s)
k
Ï€Î¸im
im (a
k
"
#
im
Ï€Î¸im
(a
|s)
im
im
im
i1:m
= Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k im
âˆ‡Î¸im log Ï€Î¸im (a |s)M
(s, a) .
k
Ï€Î¸im (aim |s)
k

k

Evaluated at Î¸

im

= Î¸kim , the above expression equals
h
i
im
Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸k M i1:m (s, a)âˆ‡Î¸im log Ï€Î¸im
|s) Î¸im =Î¸im ,
im (a
k

k

which finishes the derivation.
22

Published as a conference paper at ICLR 2022

D.3

P SEUDOCODE OF HATRPO

Algorithm 2 HATRPO
1: Input: Stepsize Î±, batch size B, number of: agents n, episodes K, steps per episode T , possible

steps in line search L, line search acceptance threshold Îº.
2: Initialize: Actor networks {Î¸0i , âˆ€i âˆˆ N }, Global V-value network {Ï†0 }, Replay buffer B
3: for k = 0, 1, . . . , K âˆ’ 1 do
4:
Collect a set of trajectories by running the joint policy Ï€Î¸k = (Ï€Î¸11 , . . . , Ï€Î¸nn ).
k

5:
6:
7:
8:
9:
10:
11:

k

Push transitions {(oit , ait , oit+1 , rt ), âˆ€i âˆˆ N , t âˆˆ T } into B.
Sample a random minibatch of B transitions from B.
Compute advantage function AÌ‚(s, a) based on global V-value network with GAE.
Draw a random permutation of agents i1:n .
Set M i1 (s, a) = AÌ‚(s, a).
for agent im = i1 , . . . , in do
Estimate the gradient of the agentâ€™s maximisation objective
B P
T

P
gÌ‚kim = B1
âˆ‡Î¸im log Ï€Î¸im
aitm | oitm M i1:m (st , at ).
im
b=1 t=1

k

k

Use the conjugate gradient algorithm to compute the update direction
xikm â‰ˆ (HÌ‚kim )âˆ’1 gkim ,
im
where HÌ‚k is the Hessian of the average KL-divergence


B P
T
P
im
im
im
im
1
(Â·|o
)
.
D
),
Ï€
Ï€
(Â·|o
i
i
KL
t
t
BT
Î¸ m
Î¸ m
b=1 t=1

12:

13:

14:

k

Estimate the maximal step size allowing
sfor meeting the KL-constraint
2Î´
.
Î²Ì‚kim â‰ˆ
im T
(xÌ‚k ) HÌ‚kim xÌ‚ikm
Update agent im â€™s policy by
im
Î¸k+1
= Î¸kim + Î±j Î²Ì‚kim xÌ‚ikm ,
where j âˆˆ {0, 1, . . . , L} is the smallest such j which improves the sample loss by at least
ÎºÎ±j Î²Ì‚kim xÌ‚ikm Â· gÌ‚kim , found by the backtracking line search.
Ï€ im
(aim |oim )
i
Î¸ m
k+1
i1:m+1
Compute M
(s, a) = Ï€im (aim |oim ) M i1:m (st , at ). //Unless m = n.
i
Î¸ m
k

15:
16:

end for
Update V-value network by following formula:
2
B P
T 
P
1
Ï†k+1 = arg minÏ† BT
VÏ† (st ) âˆ’ RÌ‚t
b=1 t=0

17: end for

23

Published as a conference paper at ICLR 2022

D.4

P SEUDOCODE OF HAPPO

Algorithm 3 HAPPO
1: Input: Stepsize Î±, batch size B, number of: agents n, episodes K, steps per episode T .
2: Initialize: Actor networks {Î¸0i , âˆ€i âˆˆ N }, Global V-value network {Ï†0 }, Replay buffer B
3: for k = 0, 1, . . . , K âˆ’ 1 do
4:
Collect a set of trajectories by running the joint policy Ï€Î¸k = (Ï€Î¸11 , . . . , Ï€Î¸nn ).
k

5:
6:
7:
8:
9:
10:
11:

k

Push transitions {(oit , ait , oit+1 , rt ), âˆ€i âˆˆ N , t âˆˆ T } into B.
Sample a random minibatch of B transitions from B.
Compute advantage function AÌ‚(s, a) based on global V-value network with GAE.
Draw a random permutation of agents i1:n .
Set M i1 (s, a) = AÌ‚(s, a).
for agent im = i1 , . . . , in do
im
Update actor im with Î¸k+1
, the argmax of the PPO-Clip objective
!
 im im im

im im
im
B P
T
P
Ï€ im (at |ot )
Ï€ im (at |ot )
1
i1:m
i1:m
Î¸
Î¸
(st , at ), clip Ï€im aim |oim , 1 Â±  M
(st , at ) .
min Ï€im aim |oim M
BT
( t t )
( t t )
i
i
b=1 t=0
Î¸ m
Î¸ m
k

k

im im
Ï€ im
|o )
im (a

12:

Î¸

i1:m
(s, a). //Unless m = n.
Compute M i1:m+1 (s, a) = Ï€ik+1
m (aim |oim ) M
i
Î¸ m
k

13:
14:

end for
Update V-value network by following formula:
2
B P
T 
P
1
Ï†k+1 = arg minÏ† BT
VÏ† (st ) âˆ’ RÌ‚t
b=1 t=0

15: end for

E

H YPER - PARAMETER S ETTINGS FOR E XPERIMENTS
hyperparameters

value

hyperparameters

value

hyperparameters

value

critic lr
gamma
gain
actor network
hypernet embed
activation

5e-4
0.99
0.01
mlp
64
ReLU

optimizer
optim eps
hidden layer
num mini-batch
max grad norm
hidden layer dim

Adam
1e-5
1
1
10
64

stacked-frames
batch size
training threads
rollout threads
episode length
use huber loss

1
3200
32
8
400
True

Table 1: Common hyperparameters used in the SMAC domain.

Algorithms

MAPPO

HAPPO

HATRPO

actor lr
ppo epoch
kl-threshold
ppo-clip
accept ratio

5e-4
5
/
0.2
/

5e-4
5
/
0.2
/

/
/
0.06
/
0.5

Table 2: Different hyperparameters used for MAPPO, HAPPO and HATRPO in the SMAC
The implementation of MADDPG is adopted from the Tianshou framework (Weng et al., 2021), all
hyperparameters left unchanged at the origin best-performing status.
24

Published as a conference paper at ICLR 2022

hyperparameters

value

hyperparameters

value

hyperparameters

value

critic lr
gamma
gain
std y coef
std x coef
activation

5e-3
0.99
0.01
0.5
1
ReLU

optimizer
optim eps
hidden layer
actor network
max grad norm
hidden layer dim

Adam
1e-5
1
mlp
10
64

num mini-batch
batch size
training threads
rollout threads
episode length
eval episode

1
4000
8
4
1000
32

Table 3: Common hyperparameters used for IPPO, MAPPO, HAPPO, HATRPO in the Multi-Agent
MuJoCo domain
Algorithms

IPPO

MAPPO

HAPPO

HATRPO

actor lr
ppo epoch
kl-threshold
ppo-clip
accept ratio

5e-6
5
/
0.2
/

5e-6
5
/
0.2
/

5e-6
5
/
0.2
/

/
/
[1e-4,1.5e-4,7e-4,1e-3]
/
0.5

Table 4: Different hyperparameters used for IPPO, MAPPO, HAPPO and HATRPO in the MultiAgent MuJoCo domain.
hyperparameters

value

hyperparameters

value

hyperparameters

value

actor lr
critic lr
gamma
tau
start-timesteps
epoch

1e-3
1e-3
0.99
5e-2
25000
200

optimizer
exploration noise
step-per-epoch
step-per-collector
update-per-step
hidden-sizes

Adam
0.1
50000
2000
0.05
[256,256]

buffer size
batch size
training num
test num
n-step
episode length

1e6
200
16
10
1
1000

Table 5: Hyper-parameter used for MADDPG in the Multi-Agent MuJoCo domain
task

value

task

value

task

value

Ant(2x4)
HalfCheetah(2x3)
Walker(2x3)
Humanoid(17x1)

1e-4
1e-4
1e-3
7e-4

Ant(4x2)
HalfCheetah(3x2)
Walker(3x2)
Humanoid-Standup(17x1)

1e-4
1e-4
1e-4
1e-4

Ant(8x1)
HalfCheetah(6x1)
Walker(6x1)
Swimmer(10x2)

1e-4
1e-4
1e-4
1.5e-4

Table 6: Parameter kl-threshold used for HATRPO in the Multi-Agent MuJoCo domain
hyperparameters

value

hyperparameters

value

hyperparameters

value

lr
gamma
gain
max grad norm
hidden layer dim

7e-4
0.99
0.01
10
64

optimizer
optim eps
hidden layer
actor network
activation

Adam
1e-5
1
rnn
ReLU

num mini-batch
eval episode
training threads
rollout threads
episode length

1
32
1
128
25

Table 7: Common hyperparameters used for MAPPO, HAPPO in the Multi-Agent Particle Environment

25

Published as a conference paper at ICLR 2022

F

A BLATION E XPERIMENTS

In this section, we conduct ablation study to investigate the importance of two key novelties that our
HATRPO introduced; they are heterogeneity of agentsâ€™ parameters and the randomisation of order
of agents in the sequential update scheme. We compare the performance of original HATRPO with
a version that shares parameters, and with a version where the order in sequential update scheme is
fixed throughout training. We run the experiments on two MAMuJoCo tasks (2-agent & 6-agent).

Walker 2x3

Walker 6x1
Average Episode Reward

Average Episode Reward

5000
4000
3000
2000
1000
0
0.0

0.2

0.4

0.6

Environment steps

0.8

4000
3000
2000
1000
0

1.0
1e7

HATRPO (original)
HATRPO (shared parameter)
HATRPO (no random order)
MAPPO

5000

0.0

(a) 2-Agent Walker

0.2

0.4

0.6

Environment steps

0.8

1.0
1e7

(b) 6-Agent Walker

Figure 4: Performance comparison between original HATRPO, and its modified versions: HATRPO
with parameter sharing, and HATRPO without randomisation of the sequential update scheme.
The experiments reveal that, although the modified versions of HATRPO still outperform baselines
(represented by MAPPO), their deviation from the theory harms performance. In particular, parameter sharing introduces extra variance to training, harms the monotonic improvement property
(Theorem 2 assumes heterogeneity), and causes HATRPO to converge to suboptimal policies. The
suboptimality is more severe in the task with more agents, as suggested by Proposition 1. Similarly,
fixed order in the sequential update scheme negatively affected the performance at convergence
(especially in the task with 6 agents), as suggested by Proposition 3. We conclude that the fine performance of HATRPO relies strongly on the close connection between theory an implementation.
The connection becomes increasingly important with the growing number of agents.

G

A BLATION S TUDY ON N ON -PARAMETER S HARING MAPPO/IPPO

We verify that heterogeneous-agent trust region algorithms (represented by HATRPO) achieve superior performance to, originally homogeneous, IPPO/MAPPO algorithms with the parameter-sharing
function switched off.

Walker 3x2

5000

Average Episode Reward

Average Episode Reward

Halfcheetah 3x2
4000
3000
2000
1000
0
0.0

0.2

0.4

0.6

Environment steps

0.8

3000
2000
1000
0

1.0
1e7

(a) 3-Agent HalfCheetah

MAPPO (No share param.)
IPPO (No share param.)
HATRPO

4000

0.0

0.2

0.4

0.6

Environment steps

0.8

1.0
1e7

(b) 3-Agent Walker

Figure 5: Performance comparison between HATRPO and MAPPO/IPPO without parameter sharing. HATRPO significantly outperforms its counterparts.

26

Published as a conference paper at ICLR 2022

H

M ULTI -AGENT PARTICLE E NVIRONMENT E XPERIMENTS

We verify that heterogeneous-agent trust region algorithms (represented here by HAPPO) quickly
solve cooperative MPE tasks.

Spread

Reference
Average Episode Reward

Average Episode Reward

60
80
100
120
140
160
180
200
220

0.00

0.25

0.50

0.75

1.00

1.25

1.50

Environment steps

1.75

2.00
1e7

MAPPO
HAPPO

0
50
100
150
200
250
0.0

(a) Simple Spread

0.5

1.0

1.5

2.0

Environment steps

2.5

(b) Simple Reference

Figure 6: Performance comparison between MAPPO and HAPPO on MPE.

27

3.0
1e6

