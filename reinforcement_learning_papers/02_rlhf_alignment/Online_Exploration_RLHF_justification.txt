Paper: Online_Exploration_RLHF.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback Gen Li∗†  Yuling Yan∗‡  arXiv:2509.22633v1 [stat.ML] 26 Sep 2025  September 29, 2025  Abstract Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward mod...

Specific Evidence for Classification:
- "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback Gen Li∗†  Yuling Yan∗‡  arXiv:2509.22633v1 [stat.ML] 26 Sep 2025  September 29, 2025  Abstract Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences."
- "In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
