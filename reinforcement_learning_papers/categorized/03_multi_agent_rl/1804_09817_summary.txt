PAPER: 1804_09817.pdf
TITLE: Multiagent Soft Q-Learning
ARXIV_ID: 1804.09817v1
RESEARCH_METHOD: 03_multi_agent_rl
METHOD_DESCRIPTION: This paper proposes a novel method called Multiagent Soft Q-Learning for cooperative continuous games. The method combines the benefits of centralized training and decentralized execution, using a central critic and multiple distributed policies. It uses Soft Q-Learning, which is a type of Q-Learning that uses a soft maximum operator to avoid the bias introduced by the max operator. The method is shown to achieve better coordination and convergence to better local optima in the joint action space compared to existing methods like MADDPG.
KEY_CONTRIBUTIONS:
* Proposes a novel method for cooperative continuous games that combines centralized training and decentralized execution
* Uses Soft Q-Learning to avoid the bias introduced by the max operator in traditional Q-Learning
* Achieves better coordination and convergence to better local optima in the joint action space compared to existing methods
* Provides a theoretical analysis of the method and its properties
* Evaluates the method on a simple continuous game and shows its effectiveness in achieving better coordination and convergence.
