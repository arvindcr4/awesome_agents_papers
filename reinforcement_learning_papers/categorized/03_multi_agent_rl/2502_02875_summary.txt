Here is the extracted information in the desired format:

**PAPER:** Heterogeneous Value Decomposition Policy Fusion for Multi-Agent Cooperation
**TITLE:** Heterogeneous Value Decomposition Policy Fusion for Multi-Agent Cooperation
**ARXIV_ID:** 2502.02875v1 [cs.MA]
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** This paper proposes a Heterogeneous Policy Fusion (HPF) scheme for cooperative multi-agent reinforcement learning. HPF integrates the strengths of various value decomposition (VD) methods by constructing a composite policy set and adaptively synthesizing a composite policy from this set to interact with the environment. The method aims to balance representation ability and training efficiency in VD algorithms.

**KEY_CONTRIBUTIONS:**

* The paper proposes a novel HPF scheme that combines the benefits of different VD methods to enhance the training efficiency of cooperative multi-agent reinforcement learning.
* HPF constructs a composite policy set by extending two distinct types of VD policies into a policy set and integrating the interaction policy based on their value function estimates.
* The method uses an instructive constraint between the utility functions of the composite policy to correct the learned factorized utilities and provide a more accurate estimation potential.
* The paper evaluates the effectiveness of HPF through experiments on various cooperative tasks, including matrix games, predator-prey environments, and the StarCraft Multi-Agent Challenge.
