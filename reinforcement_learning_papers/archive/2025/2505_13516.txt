HALO: Hierarchical Autonomous Logic-Oriented
Orchestration for Multi-Agent LLM Systems

arXiv:2505.13516v1 [cs.MA] 17 May 2025

Zhipeng Hou∗
Nanjing University of Posts and Telecommunications
japhonehou@gmail.com
Junyi Tang
Nanjing University of Posts and Telecommunications
b23012214@njupt.edu.cn

Yipeng Wang
Chongqing University
yipengxw@gmail.com

Abstract
Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios. Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability as
well as flexibility in complex interaction environments and leading to subpar performance on highly specialized and expert-level tasks. To address these issues,
we introduce HALO, a multi-agent collaboration framework based on a hierarchical reasoning architecture. Specifically, we incorporate a high-level planning
agent for task decomposition, mid-level role-design agents for subtask-specific
agent instantiation, and low-level inference agents for subtask execution. Particularly, subtask execution is reformulated as a structured workflow search problem,
where Monte Carlo Tree Search (MCTS) systematically explores the agentic action
space to construct optimal reasoning trajectories. Additionally, as the majority
of users lack expertise in prompt engineering, we leverage an Adaptive Prompt
Refinement module to transform raw queries into task-specific prompts. Empirical
evaluations on Code Generation (HumanEval), General Reasoning (MMLU), and
Arithmetic Reasoning (MATH) benchmark datasets highlight the effectiveness of
HALO, yielding a 14.4% average improvement over state-of-the-art baselines.
Notably, HALO achieves up to 13.3% performance gain on the Moral Scenarios
subject in the MMLU benchmark and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark, indicating its advanced proficiency in
tackling highly specialized and expert-level tasks. The code repository is available
at https://github.com/23japhone/HALO.

1

Introduction

Large Language Models (LLMs), such as OpenAI o3 [1] and Deepseek R1 [2], have demonstrated
remarkable capabilities in both language understanding and reasoning. These advancements unlock
tremendous potential for LLM-based multi-agent systems to address a broad spectrum of downstream
tasks, including code generation [3, 4], mobile device control [5, 6], video gaming [7] and opendomain question answering [8, 9]. Consequently, the formulation of effective agentic systems is
crucial to fully harnessing the capabilities of LLMs across diverse downstream applications.
∗

First author and corresponding author, correspondence to japhonehou@gmail.com

Preprint. Under review.

User Query
Which of the following is true about Cushing's Syndrome?
A. It is due to a deficiency of cortisol hormone
B. Enlarged extremities are commonly seen
C. Osteoporosis is not a feature
D. Moon face and buffalo hump are characteristic of the disease

“To tackle this complex clinical question, I formulated the first subtask: evaluate the defining
characteristics of the syndrome and determine which of the given options accurately reflects its
clinical profile.
This subtask requires a blend of literature-based inquiry, statistical insight, and domain-specific
medical expertise. Consequently, I have decomposed the objective accordingly and prepared it for
delegation. The task will now be handed off to the next layer of specialized agents, each selected
for their ability to contribute domain-relevant perspectives.”

Step 1

:fail
:continue
:success
score:0.64

score:0.75

score:0.85

Step 2

extract

Prompt Refinement Agent
Task Type: Knowledge Assessment
Core Intent: Answer a multiple-choice question about clinical domin
Key Details: The question relates to Cushing's Syndrome with
options provided: A. ..., B. ..., C. ..., D. ...

High-level
Planning Agent

refine

Final Prompt

Question:
```
Which of the following is true about Cushing's Syndrome?
```

;

C. ...

;

From Step 1, the top-scoring
agent at the node continues
to the next step.

score:0.83

score:0.56

Agent Role: Endocrinology Expert

Task Type: Knowledge Assessment
Core Intent: Answer a multiple-choice question about clinical domin
Key Details: The question relates to Cushing's Syndrome with
options provided: A. ..., B. ..., C. ..., D. ...
Final Prompt: Evaluate the clinical knowledge question below and
select the correct answer by choosing the corresponding letter:

Options:
```
A. ... ; B. ...
```

Agent Role: Medical Researcher
Target: Conduct a comprehensive review of current
biomedical literature on Cushing’s Syndrome to extract evidence
regarding its pathophysiology ...
Final Prompt: As a Medical Researcher, your objective is to
conduct an evidence-based investigation into the characteristics ...

D. ...

Adapative Prompt Refinement

Mid-level Roledesign agent

score:0.67

Step 3

Target: Apply domain-specific medical expertise to
interpret the clinical relevance of each option and determine
which statement best aligns with ...
Final Prompt: As an Endocrinology Expert, your responsibility is
to evaluate the diagnostic accuracy of the provided statements ...

Agent Role: Clinical Data Analyst
Target: Analyze symptom prevalence, patient data
patterns, and epidemiological studies to determine which of the
four options most accurately reflects clinical trends ...
Final Prompt: As a Clinical Data Analyst, your task is to examine
Cushing’s Syndrome through the lens of real-world clinical data ...

Hierarchical Reasoning Stack

The highest-scoring agent
continues, iterating until task
completion or depth limit.

score:0.93

score:0.76

score:0.86

Workflow Search Engine

Figure 1: The overview of HALO framework. HALO consists of three modules: (1) Adaptive Prompt
Refinement (Section 3.2), where user queries are refined into high-quality and LLM-comprehensible
prompts; (2) Hierarchical Reasoning Stack (Section 3.3), which is responsible for task decomposition,
role instantiation, and subtask execution; and (3) Workflow Search Engine (Section 3.4), which
explores multi-agent collaboration and constructs optimal workflows. Green paths denote optimal
reasoning trajectories, while red paths are pruned during search.

However, existing agentic systems often struggle to maintain robust performance in complex interaction environments and expert-level tasks. This limitation arises from predefined agent-role
design spaces [5, 10] and static communication workflows [11, 12], which heavily depend on expert
insight and manually-designed policies. Meanwhile, as the majority of users lack expertise in prompt
engineering, poorly formulated queries have greatly hindered the comprehension of agents, ultimately leading to inefficient task execution. Together, these challenges underscore two fundamental
problems: (1) how can agentic systems self-organize and coordinate in unfamiliar environments
with minimal manual intervention; and (2) how can user queries be refined to improve the overall
efficiency and effectiveness of multi-agent collaboration?
In response, we introduce HALO, a Hierarchical Autonomous Logic-Oriented Orchestration framework focused on addressing complex interaction environments and expert-domain reasoning tasks.
To this end, HALO incorporates an extensible agent-role instantiation mechanism and a dynamic
communication architecture to replace the rigidity of predefined role spaces and static workflows.
Additionally, HALO employs a prompt engineering module for user query refinement. The overview
of HALO is illustrated in Figure 1.
Specifically, HALO functions in a three-stage paradigm. The first stage is Adaptive Prompt
Refinement (detail in Section 3.2), where the raw user query is transformed into a high-quality
and LLM-comprehensible prompt. This module comprises four collaborative agents responsible for
different phase of prompt refinement, ranging from query parsing to final synthesis. The second stage
is Hierarchical Reasoning Stack (detail in Section 3.3), which employs a three-tier multi-agent
collaboration architecture. At the top layer, a high-level planning agent decomposes the overall task
into a sequence of subtasks. At the middle layer, mid-level role-design agents dynamically instantiate
specialized agents tailored to the requirement of each subtask. At the bottom layer, low-level inference
agents are responsible for executing each subtask through cooperation mechanisms. The third stage
is Workflow Search Engine (detail in Section 3.4), which explores low-level inference agents
collaboration and constructs optimal workflows. We reformulate subtask execution as a workflow
search process, where Monte Carlo Tree Search (MCTS) [13] guides the exploration over action
spaces. Each node in the search tree corresponds to an agent-generated response or an intermediate
reasoning step, while edges denote possible transitions between reasoning states. Together, these
nodes and edges form a path that represents a candidate reasoning trajectory for each subtask.
Extensive experiments (detail in Section 4.2) demonstrate that HALO outperforms strong baselines
across Code Generation (HumanEval) [14], General Reasoning (MMLU) [15], and Arithmetic
2

Reasoning (MATH) [16] benchmarks, yielding a 14.4% average improvement. Notably, HALO
achieves up to 13.3% performance gain on the Moral Scenarios subject in MMLU and up to 19.6%
performance gain on the Algebra subarea in MATH, underscoring its advanced proficiency in highly
specialized and expert-level tasks.
The key contributions of this work are as follows:
• We introduce a novel framework named HALO for task-oriented agent collaboration in
three stages, marking a significant advancement beyond the limitations of expert insight and
manually-designed policies.
• Experiments across three diverse tasks show that our method outperforms state-of-the-art
baselines, confirming the effectiveness and adaptability of HALO across complex interaction
environments and expert-domain reasoning tasks.

2

Related work

2.1

Prompt optimization

Prompt design underpins how LLM agents interpret instructions and coordinate actions. Recent
work automates this process to reduce manual intervention and improve modularity. Promptor [17]
decomposes goals into structured, role-specific prompts, enabling agents to focus on individual
tasks. CAMEL [12] integrates a task-planner agent that generates role prompts to guide multi-agent
interactions through instruction-driven simulation. While these methods improve modularity and task
decomposition, current agentic systems still depend on handcrafted prompt templates or task-specific
engineering, which limits generalization across domains. Recent frameworks like ComfyAgent [18]
begin to incorporate prompt generation as part of evolving workflows. Nonetheless, the challenge of
designing prompts that are both reusable and responsive to runtime context changes remains largely
open, especially in scenarios where agent roles or task objectives are not predefined.
2.2

Role design in LLM-based architecture

Role design has become a central theme in recent multi-agent architectures. Manual role assignment
is a common strategy for organizing agent behavior. MetaGPT [19], for instance, aligns agents with
real-world job titles and governs their behavior through standard operating procedures. This static
role configuration offers clarity and reproducibility, especially in well-defined domains. However,
fixed roles often struggle in open-ended or dynamic settings where agent responsibilities must evolve
in response to task changes. To address this, newer frameworks introduce role generation mechanisms
that allow agents to create, adapt, or inherit roles dynamically. TPTU [20] and DyLAN [21] adopt
hierarchical or layered structures where agents can delegate, transform, or refine responsibilities as
collaboration progresses. These approaches mark a shift toward greater flexibility, although existing
agentic systems still operate within partially predefined boundaries.
2.3

Cooperation optimization strategies

As the scale and complexity of LLM-based multi-agent systems grow, optimizing inter-agent cooperation becomes increasingly critical. Broadly, current systems adopt either centralized or decentralized paradigms. Centralized frameworks, such as AgentLaboratory [22], ScoreFlow [23],
and WORKFLOW-LLM [24], rely on controller agents to manage scheduling, communication, and
output aggregation. While these architectures offer clear oversight, they can suffer from scalability
issues and become bottlenecks in real-time or asynchronous settings. In contrast, decentralized
approaches seek to distribute control and decision-making across agents. Some utilize peer-to-peer
communication or identity-based protocols [25], while others apply game-theoretic negotiation [26].
These designs offer greater autonomy and robustness in unstructured environments. Recently, several
frameworks reframe cooperation as a structured search process. AFlow [27], for instance, explores
layered task workflows through dynamic graph traversal rather than relying on fixed execution paths.
Such methods enable agents to adaptively construct interaction plans based on task-specific signals
or peer responses, highlighting a promising direction for building more context-sensitive agent teams.
Notably, some researchs apply reinforcement learning to refine cooperation strategies [28–32].
3

Table 1: Comparative analysis of HALO and existing representative LLM-based multi-agent frameworks across five key dimensions.
Method

Structure

ScoreFlow [23]
CAMEL [12]
AgentVerse [33]
MetaGPT [19]
DyLAN [21]
ComfyAgent [18]
AgentLaboratory [22]
TPTU [20]
AFlow [27]
WORKFLOW-LLM [24]
Game-theoretic Workflow [26]
HALO (Ours)

Central + Score-based
Feedback Triplet
Hierarchical Tree
SOP-guided Workflow
DAG + Feedback Loop
Modular Workflow
Centralized Scheduler
Hierarchical Planner
Multi-layer DAG
Centralized Workflow
Decentralized Negotiation
Hierarchical Structure + MCTS

Multi-Role

Role Assignment

Dynamic Structure

Team Optimization

✗
Manual
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓

✗
Manual
Generated
Manual
Manual + Gen
Generated
Manual
Manual + Gen
Manual + Gen
Manual
Adaptive
Adaptive

✗
✗
✗
✗
✓
✓
✗
✓
✓
✗
✓
✓

✓
✗
✗
✗
✓
✗
✓
✗
✓
✓
✓
✓

A comparative summary of these cooperation frameworks across five dimensions is provided in
Table 1. In contrast, HALO introduces a hierarchical reasoning architecture and reformulates subtask
execution as a structured reasoning search. This design offers a balanced pathway between modular
coordination and dynamic adaptability, particularly suited for complex interaction environments.

3

HALO framework

3.1

Problem formulation

In the proposed workflow search process, a workflow W is composed of a sequence of subtasks
{T1 , T2 , . . . , TK }, where each subtask Tk is handled by a series of role-specialized LLM-based
(1) (2)
agents Ak = {ak , ak , . . . }. We reformulate subtask execution as the workflow search process
over the workflow space S, where each candidate workflow W ∈ S represents a unique instantiation
of subtasks and multi-agent interactions. Given a user query Q expressed in natural language, the
objective is to construct an optimal reasoning workflow W ∗ that generates an expected answer Ŷ
aligned with the user query Q. The optimization problem of HALO is defined as follows:
W ∗ = arg max Value(Q, W)
W∈S

(1)

where Value(Q, W) evaluates the effectiveness of workflow W in addressing the user query Q.
The overall framework of HALO is illustrated in Figure 1 and the algorithm is presented in Algorithm 1. Next, we elaborate on each component of the three-stage paradigm.
3.2

Adaptive Prompt Refinement

As the majority of users lack expertise in prompt engineering, user queries Q are often loosely
structured or ambiguous in intents. To address this limitation and enhance the reliability of downstream reasoning, we propose an Adaptive Prompt Refinement module at the beginning of the HALO
framework, which refines the raw user query Q into a structured and LLM-comprehensible prompt
Q∗ . This module comprises four collaborative agents, each formalized as a functional mapping,
denoted by P1 to P4 . These agents are responsible for different stages of prompt refinement, ranging
from query parsing to final synthesis (detail in Appendix A). The process is formulated as follows.
The process begins with the Task Parser Agent P1 , which semantically analyzes the original query Q
to extract three essential components: the task type T , the core intent I, and the key details D. These
components are then assembled into a structured triplet:
F = P1 (Q) = (T , I, D)

(2)

This structured task representation F serves as a global semantic context throughout the entire
reasoning workflow.
Based on this structured task representation F and raw user query Q, the Prompt Template Agent P2
constructs an initial prompt frame Q0 that includes a reformulated task description, clear reasoning
objectives, bounded input conditions, and the explicit output format:
Q0 = P2 (Q, F)
4

(3)

Algorithm 1 Algorithm of HALO
Input: User query Q, prompt refinement agents P1 ∼ P4 , planning agent Aplan , role-design agent
Arole , inference agent pool Ak , subtask budget K
Output: Final answer Ŷ
1: F ← P1 (Q)
▷ Parse task type T , intent I, and details D
2: Q0 ← P2 (Q, F)
▷ Build initial prompt template
3: Qopt ← P3 (Q0 , F)
▷ Optimize with prompting strategies
4: Q∗ ← P4 (Qopt , F)
▷ Synthesize final prompt
5: H0 ← ∅
▷ Initialize subtask execution history
6: for k = 1 to K do
7:
Tk ← Aplan (Q∗ , F, Hk−1 )
▷ Decompose query into subtask
8:
if Tk = stop then
9:
break
▷ Terminate if planning agent signals task completion
10:
end if
11:
Ak ← Arole (Tk , Q∗ , F)
▷ Generate role-specific agents
(i)
12:
for each agent ak ∈ Ak do
(i)
(i)
13:
yk ← ak (Tk , Q∗ , F)
▷ Generate intermediate output by executing subtask Tk
(i) (i)
(i)
14:
ℓk , vk ← Evaluate(yk )
▷ Evaluate status label and quality score for output
(i) (i) (i)
15:
MCTS_Backpropagate(ak , ℓk , vk )
▷ Propagate execution feedback
16:
end for
17:
Hk ← Hk−1 ∪ {Tk , Ŷk }
▷ Update subtask execution history
18:
if early-stop(Hk ) is True then
19:
break
▷ Terminate if early-stopping mechanism is triggered
20:
end if
21: end for
(i)
22: Ŷ ← Aggregate({yk })
▷ Select final answer
23: return Ŷ
To further refine the initial prompt template, the Prompt Optimization Agent P3 incorporates slowthinking prompting strategies [34–38] as well as tool calling [39] instructions, generating an optimized
prompt Qopt :
Qopt = P3 (Q0 , F)
(4)
Finally, the Prompt Generator Agent P4 synthesizes the optimized structure into the final refined
prompt Q∗ , which serves as the entry point for downstream multi-agent reasoning:
Q∗ = P4 (Qopt , F)
3.3

(5)

Hierarchical Reasoning Stack

Following the refined prompt Q∗ , HALO proceeds to the Hierarchical Reasoning Stack module,
which comprises three collaborative layers, including the high-level planning agent Aplan for task
decomposition, the mid-level role-design agents Arole for dynamic agent instantiation, and the
(1) (2)
low-level inference agents Ak = {ak , ak , . . . } for subtask execution.
At the top layer, the high-level planning agent Aplan receives the refined prompt Q∗ and the global
structured task representation F. Based on this information, it decomposes the overall task into a
sequence of subtasks {T1 , T2 , . . . , TK } (detail in Appendix B). Instead of performing full decomposition in advance, Aplan adopts a step-wise strategy, generating one subtask at a time and iteratively
updating its decomposition policy based on the execution history of preceding subtasks:
Tk = Aplan (Q∗ , F, Hk−1 )

(6)

where Hk−1 denotes the execution history of preceding subtasks for the subtask Tk .
Each generated subtask Tk is delegated to the mid-level role-design agents Arole , which instantiate
(1) (2)
a set of specialized LLM-based agents Ak = {ak , ak , . . . } to execute the subtask Tk . This
instantiation process is jointly guided by the semantics of the subtask Tk , the refined prompt Q∗ , and
5

Selection

Expansion

Backpropagation

Simulation

Figure 2: The illustration of how Monte Carlo Tree Search (MCTS) guides multi-agent reasoning
through selection, expansion, simulation, and backpropagation stages. Each node represents an agent
and edge transitions are guided by execution outcomes as well as evaluation feedback.
(i)

the global structured task representation F, ensuring that each generated agent ak is well aligned
(i)
(i)
with the subtask Tk requirements. For each generated agent ak , a role-specific system prompt ρk is
assigned to govern its behavior within the subtask Tk (detail in Appendix C):
(i)

(i)

ak = Arole (Tk , Q∗ , F) ⇒ ρk

(1)

(7)
(2)

For each subtask Tk , the low-level inference agents Ak = {ak , ak , . . . } receives the subtask
Tk , the refined prompt Q∗ , and the global structured task representation F. They are subsequently
engaged in collaborative reasoning, resulting in a set of intermediate outputs:
(1)

(2)

Yk = {yk , yk , . . . } = Ak (Tk , Q∗ , F)

(8)

To further improve efficiency and avoid unnecessary subtask generation, HALO integrates an earlystopping mechanism within the high-level planning agent Aplan . This design is inspired by the
Byzantine Consensus theory [40], which states that at least 3p + 1 agents are required to tolerate
p faulty agents in a single round of communication. Following this principle, HALO terminates
the reasoning process if at least 66% of the completed subtasks in the execution history Hk yield a
consistent answer Ŷ . Additionally, the reasoning process will also be terminated when the maximum
number of permitted subtasks has been reached.
3.4

Workflow Search Engine

To dynamically explore multi-agent collaboration and adaptively construct the optimal workflow,
HALO leverages a Workflow Search Engine module based on Monte Carlo Tree Search (MCTS) [13].
The workflow search process formalizes multi-agent inference as a tree-structured action space,
(i)
where each node represents a role-specific agent ak executing a subtask Tk and each edge denotes
communication transitions between agents. Together, these nodes and edges form a path that
corresponds to a candidate reasoning trajectory for each subtask Tk . During each search iteration,
HALO follows the standard four-stage optimized MCTS paradigm (Selection, Expansion, Simulation,
and Backpropagation), as illustrated in Figure 2.
(i)

Selection. HALO recursively selects the best agent ak with the UCT algorithm [41] and adds it to
the trajectory:
s
(i)
vk
log N
(i)
UCT(ak ) = (i) + α
(9)
(i)
nk
nk
(i)

(i)

(i)

(i)

where vk is the score value of agent ak , nk is the number of visits to the agent ak , N is the
(i−1)
number of visits to the parent agent ak
, and α is an exploration coefficient.
(i)

Expansion. Given a selected agent ak from the selection phase, if it contains untried actions,
(i+1)
HALO expands the search tree by instantiating a new role-specific agent ak
as its child.
6

Table 2: Performance comparison of HALO against competitive baselines across three benchmarks.
Metrics include pass@1 (%) for HumanEval, accuracy (%) for MMLU as well as MATH, and Avg.
(%) for the mean performance over three runs. All methods are executed with GPT-4o.
Baseline Type

Method

Benchmarks

Structure

Avg.

HumanEval

MMLU

MATH

Single-agent

ReAct [11]

Monolithic Sequential Reasoning Flow

69.1

57.6

29.2

52.0

Static MAS

CAMEL [12]
LLM-Debate [43]

Feedback Triplet
Fully Connected Bipartite

72.4
73.7

64.3
66.3

31.9
32.6

56.2
57.5

Dynamic MAS

DyLAN [21]
AgentVerse [33]
ADAS [44]
HALO (Ours)

DAG + Feedback Loop
Hierarchical Tree
Search-Based Dynamic Graph Structure
Hierarchical Structure + MCTS

81.7
75.2
82.4
95.2

70.1
67.5
72.8
81.6

35.2
34.4
36.9
58.9

62.3
59.0
64.0
78.6

(i+1)

Simulation. The agent ak
initiates a simulated reasoning trajectory for subtask Tk , where a
(i+2) (i+3)
sequence of additional agents ãk
, ãk
, . . . are engaged to emulate hypothetical future steps
(j)
(j)
along the reasoning path. Each simulated agent ãk generates an intermediate output ỹk :
(j)

(j)

ỹk = ãk (Tk , Q∗ , F)

(10)

(j)
The output ỹk is evaluated by two auxiliary agents. The judging agent assigns a status label
(j)
ℓ̃k ∈ {success, f ail, continue}, indicating whether the subtask is completed. The scoring agent
(j)
computes a quality score value ṽk ∈ [0, 1] to reflect the effectiveness of the generated output (detail

in Appendix B).

Backpropagation. Inspired by the value aggregation strategy in CoAT [42], HALO updates the
evaluation scores of all traversed nodes along the search path by incorporating simulation feedback.
To enhance sensitivity to task completion status, we additionally introduce a reward signal adjustment
mechanism based on the judgment outcome.
(j)

(j)

Let λ(ℓ̃k ) be the impact factor associated with the status label ℓ̃k , where λ reflects the reward or
(i)
penalty of a simulation result. The updated value of a node ak is computed as follows:
P
(i)
(i)
(j † )
(j)
vk · nk + λ(ℓ̃k ) +
ṽk
(i)∗

vk

(j)

=

(i)

ãk ∈Child(ak )
(i)

(11)

(i)

nk + |Child(ak )|

(j † )

(j † )

where ℓ̃k denotes the terminal status label of the simulated leaf node agent ãk
(i)
trajectory and Child(ak ) refers to the set of its simulated children.

4

Experiments

4.1

Experimental setup

along the reasoning

Code generation. We use the HumanEval [14] dataset, which contains 164 Python programming
problems and corresponding unit tests. Unit tests are used to validate the correctness of generated
codes. We report the pass@1 metric to assess code accuracy.
General reasoning. We use the MMLU [15] dataset, which spans 57 subjects with 15,908 questions.
Each question is presented in multiple-choice format with four options. Due to the large number of
questions, we randomly sample 13% of the total dataset according to the subject-wise distribution.
We report the accuracy metric to measure the correctness of answers.
Arithmetic reasoning. We use the MATH [16] dataset, which consists of 12,500 math problems
across 7 subareas with 5 difficulty levels. Likewise, we randomly sample 500 math problems
according to the subarea-wise and level-wise distribution. We report the accuracy metric to measure
the proportion of correct answers.
7

Table 3: Performance comparison on five abstract subjects selected from the MMLU dataset. Metrics
are reported as accuracy (%) averaged over three runs.
Baseline Type

Subjects

Method
Abstract Algebra

College Physics

Formal Logic

High School Mathematics

Moral Scenarios

Single-agent

ReAct [11]

44.2

46.5

40.3

44.7

37.6

Static MAS

CAMEL [12]
LLM-Debate [43]

50.4
51.1

54.7
54.6

49.8
48.3

51.9
52.8

44.2
45.1

Dynamic MAS

DyLAN [21]
AgentVerse [33]
ADAS [44]
HALO (Ours)

52.9
53.6
55.7
69.7

60.4
59.4
62.4
77.3

51.2
50.3
52.6
66.8

58.8
57.1
60.0
75.5

49.6
48.7
51.3
64.6

Baselines. We compare HALO with six competitive baselines across three categories, including
single-agent frameworks (ReAct [11]), static multi-agent systems (CAMEL [12], LLM-Debate [43]),
and dynamic multi-agent orchestration methods (DyLAN [21], AgentVerse [33], ADAS [44]).
HALO setup. We implement HALO and conduct these experiments by using GPT-4o [45], with
the random seed of 10, temperature setting of 0.8, and max tokens limit of 2048. To ensure fair
comparison, we use the same number of few-shot examples across all methods and merely equip
HALO with the code interpreters as tools in the code generation task (detail in Appendix D).
4.2

Main results

We conduct extensive experiments to compare HALO against six baselines on three tasks and report
the results based on GPT-4o in Table 2. Additionally, we present performances of executing five
abstract subjects from the MMLU in Table 3 and three computationally intensive subareas from the
MATH in Figure 3. Based on these results, we derive the following key observations.
HALO proposes the hierarchical reasoning architecture that overcomes the limitations of
cognitive overload. Unlike frameworks such as ReAct [11], which require a single agent to simultaneously manage planning, reasoning, and reflection, HALO distributes these responsibilities
across the hierarchical reasoning architecture dedicated to task decomposition, role instantiation, and
subtask inference. This design enables more focused agent behavior and reduces cognitive overload.
Specifically, as shown in Table 2, HALO achieves significant improvements over ReAct across all
benchmarks, including a 26.1% gain in HumanEval pass@1 (95.2% vs. 69.1%), 24.0% in MMLU
accuracy (81.6% vs. 57.6%), and 29.7% in MATH accuracy (58.9% vs. 29.2%). On average, HALO
improves performance by 26.6% (78.6% vs. 52.0%), highlighting the advantages of hierarchical
reasoning architecture over monolithic single-agent reasoning.
HALO leverages adaptive agent instantiation and search-based workflow exploration that enhance the granularity of task execution. Compared to static MAS approaches such as CAMEL [12]
and LLM-Debate [43], which depend on fixed agent roles and handcrafted workflows, as well as
dynamic MAS systems like DyLAN [21], AgentVerse [33], and ADAS [44], which lack fine-grained
alignment between tasks and agents, HALO introduces adaptive agent instantiation and MCTS-driven
workflow exploration. This design allows HALO to dynamically instantiate appropriate agent roles
and iteratively refine execution trajectories based on real-time feedback. As a result in Table 2, HALO
outperforms the strongest baseline (ADAS) by 14.6% on average (78.6% vs. 64.0%), and consistently
achieves the highest performance across all benchmarks, with improvements of 12.8% on HumanEval
pass@1 (95.2% vs. 82.4%), 8.8% on MMLU accuracy (81.6% vs. 72.8%), and 22.0% on MATH
accuracy (58.9% vs. 36.9%). These results underscore the effectiveness of HALO in orchestrating
multi-agent reasoning at scale.
HALO excels in handling highly complex and expert-level reasoning tasks. To further evaluate the
capability of HALO in solving professionally demanding tasks, we conduct fine-grained comparisons
on difficult subdomains from MMLU and MATH datasets, as reported in Table 3 and Figure 3. HALO
consistently outperforms all baselines on these challenging problems. On the abstract MMLU subjects,
HALO achieves substantial improvements over the strongest baseline (ADAS [44]), with an average
accuracy of 70.8% compared to 56.4%, marking a 14.4% gain. Similarly, on the computationally
intensive MATH subareas, HALO reaches an average accuracy of 43.9%, significantly surpassing
8

HALO (Ours)

41.2
21.1

AgentVerse

18.2

24.5
24.4

20.7

DyLAN

18.4

LLM-Debate

17.8
18.8

25.5
21.2
22.3

22.1
17.9
18.3
20.6
15.6
17.2

CAMEL
ReAct
0

10

20

90

27.6

Performance (%)

ADAS

100

46.4
44.1

Score (%)

Geometry
Algebra
Precalculus
30

40

HALO (full)
w/o Adaptive Prompt Refinement module
w/o task decomposition layer

95.2
90.3
83.8

80

81.6

67.3
58.9

60

40

Figure 3: Performance comparison on three computationally intensive subareas selected from
the MATH dataset. Metrics are reported as
accuracy (%) averaged over three runs.

73.3

70
54.1

50
50

78.6

75.4 73.5

44.7

HumanEval

MMLU

MATH

Avg.

Figure 4: Ablation study of removing the Adaptive Prompt Refinement module and the highlevel planning agent on GPT-4o across three
benchmarks.

ADAS at 24.4%. These results demonstrate the strength of HALO in tackling highly specialized and
non-trivial reasoning tasks.
4.3

Ablation study

To evaluate the impact of HALO components, we conduct ablation studies by individually removing
components to measure performances on HumanEval, MMLU, and MATH. Results are summarized
in Figure 4.
Effectiveness of Adaptive Prompt Refinement. We examine the necessity of the Adaptive Prompt
Refinement module. By removing this component and directly passing the raw user query Q into
downstream agents, we observe a clear degradation across all benchmarks. As shown in Figure 4,
performance drops by 5.3% on average, with MMLU suffering the most (81.6% → 75.4%). This
result highlights the importance of structured prompt construction in enhancing task understanding
and aligning reasoning trajectories with user intent.
Effectiveness of the high-level planning agent. We remove the high-level planning agent and
treat the refined user query Q∗ as a single-step task without iterative decomposition. As shown in
Figure 4, this leads to an average performance drop of 11.3% across all benchmarks. Notable drops
on HumanEval (95.2% → 83.8%) and MATH (58.9% → 44.7%) demonstrate that removing task
decomposition impairs reasoning coherence, highlighting its critical role in HALO.

5

Conclusion

In this work, we introduce HALO, a multi-agent collaboration framework dedicated to tackling
complex interaction environments and expert-domain reasoning tasks. HALO functions in a threestage paradigm. In the Adaptive Prompt Refinement, user queries are refined into structured prompts
to enhance downstream reasoning. In the Hierarchical Reasoning Stack, HALO delegates downstream
processes to three specialized layers, overcoming the limitations of cognitive overload. In the
Workflow Search Engine, HALO leverages the search-based workflow exploration to construct
optimal collaborative workflows. Experimental results demonstrate that HALO achieves significant
performance improvements compared to the competitive baselines. Additionally, we find that
performance can be further enhanced through the injection of long-term memory mechanisms and
external knowledge integration, providing new directions for future work.

Acknowledgments
This work was conducted independently without any institutional or financial support.
9

References
[1] A. El-Kishky, A. Wei, A. Saraiva, B. Minaiev, D. Selsam, D. Dohan, F. Song, H. Lightman, I. Clavera, J. Pachocki et al., “Competitive programming with large reasoning models,” arXiv preprint arXiv:2502.06807,
2025.
[2] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi,
X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu,
B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo,
G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo,
J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao,
K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang,
M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du,
R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang,
S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao,
W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie,
X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen,
X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu,
Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma,
Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X.
Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren,
Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li,
Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang, “Deepseek-r1: Incentivizing reasoning
capability in llms via reinforcement learning,” 2025. [Online]. Available: https://arxiv.org/abs/2501.12948
[3] L. Zhong, Z. Wang, and J. Shang, “Debug like a human: A large language model debugger via verifying
runtime execution step-by-step,” arXiv preprint arXiv:2402.16906, 2024.
[4] N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, and S. Yao, “Reflexion: language agents with
verbal reinforcement learning,” in Thirty-seventh Conference on Neural Information Processing Systems,
2023. [Online]. Available: https://openreview.net/forum?id=vAElhFcKW6
[5] J. Wang, H. Xu, H. Jia, X. Zhang, M. Yan, W. Shen, J. Zhang, F. Huang, and J. Sang, “Mobile-agent-v2:
Mobile device operation assistant with effective navigation via multi-agent collaboration,” arXiv preprint
arXiv:2406.01014, 2024.
[6] Y. Li, C. Zhang, W. Yang, B. Fu, P. Cheng, X. Chen, L. Chen, and Y. Wei, “Appagent v2: Advanced agent
for flexible mobile interactions,” arXiv preprint arXiv:2408.11824, 2024.
[7] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, “Voyager: An
open-ended embodied agent with large language models,” arXiv preprint arXiv:2305.16291, 2023.
[8] R. Li, C. Xu, Z. Guo, B. Fan, R. Zhang, W. Liu, Y. Zhao, W. Gong, and E. Wang, “Ai-vqa: visual question
answering based on agent interaction with interpretability,” in Proceedings of the 30th ACM International
Conference on Multimedia, 2022, pp. 5274–5282.
[9] Z. Wang, H. Zhang, C.-L. Li, J. M. Eisenschlos, V. Perot, Z. Wang, L. Miculicich, Y. Fujii, J. Shang, C.-Y.
Lee et al., “Chain-of-table: Evolving tables in the reasoning chain for table understanding,” arXiv preprint
arXiv:2401.04398, 2024.
[10] Q. Zeng, Q. Yang, S. Dong, H. Du, L. Zheng, F. Xu, and Y. Li, “Perceive, reflect, and plan: Designing llm
agent for goal-directed city navigation without instructions,” arXiv preprint arXiv:2408.04168, 2024.
[11] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “React: Synergizing reasoning and
acting in language models,” in International Conference on Learning Representations (ICLR), 2023.
[12] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative agents for" mind"
exploration of large language model society,” Advances in Neural Information Processing Systems, vol. 36,
pp. 51 991–52 008, 2023.
[13] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez,
S. Samothrakis, and S. Colton, “A survey of monte carlo tree search methods,” IEEE Transactions on
Computational Intelligence and AI in games, vol. 4, no. 1, pp. 1–43, 2012.
[14] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,
G. Brockman et al., “Evaluating large language models trained on code,” arXiv preprint arXiv:2107.03374,
2021.

10

[15] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, “Measuring massive
multitask language understanding,” arXiv preprint arXiv:2009.03300, 2020.
[16] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, “Measuring
mathematical problem solving with the math dataset,” arXiv preprint arXiv:2103.03874, 2021.
[17] J. Shen, J. J. Dudley, J. Zheng, B. Byrne, and P. O. Kristensson, “Promptor: A conversational and
autonomous prompt generation agent for intelligent text entry techniques,” 2023. [Online]. Available:
https://arxiv.org/abs/2310.08101
[18] X. Xue, Z. Lu, D. Huang, Z. Wang, W. Ouyang, and L. Bai, “Comfybench: Benchmarking llm-based
agents in comfyui for autonomously designing collaborative ai systems,” 2024. [Online]. Available:
https://arxiv.org/abs/2409.01392
[19] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou et al.,
“Metagpt: Meta programming for multi-agent collaborative framework,” arXiv preprint arXiv:2308.00352,
vol. 3, no. 4, p. 6, 2023.
[20] J. Ruan, Y. Chen, B. Zhang, Z. Xu, T. Bao, H. Mao, Z. Li, X. Zeng, R. Zhao et al., “Tptu: Task planning
and tool usage of large language model-based ai agents,” in NeurIPS 2023 Foundation Models for Decision
Making Workshop, 2023.
[21] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, “A dynamic llm-powered agent network for task-oriented
agent collaboration,” in First Conference on Language Modeling, 2024.
[22] S. Schmidgall, Y. Su, Z. Wang, X. Sun, J. Wu, X. Yu, J. Liu, Z. Liu, and E. Barsoum, “Agent laboratory:
Using llm agents as research assistants,” arXiv preprint arXiv:2501.04227, 2025.
[23] Y. Wang, L. Yang, G. Li, M. Wang, and B. Aragam, “Scoreflow: Mastering llm agent workflows via
score-based preference optimization,” arXiv preprint arXiv:2502.04306, 2025.
[24] S. Fan, X. Cong, Y. Fu, Z. Zhang, S. Zhang, Y. Liu, Y. Wu, Y. Lin, Z. Liu, and M. Sun, “Workflowllm:
Enhancing workflow orchestration capability of large language models,” arXiv preprint arXiv:2411.05451,
2024.
[25] B. Liu, X. Li, J. Zhang, J. Wang, T. He, S. Hong, H. Liu, S. Zhang, K. Song, K. Zhu et al., “Advances and
challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe
systems,” arXiv preprint arXiv:2504.01990, 2025.
[26] W. Hua, O. Liu, L. Li, A. Amayuelas, J. Chen, L. Jiang, M. Jin, L. Fan, F. Sun, W. Wang, X. Wang,
and Y. Zhang, “Game-theoretic llm: Agent workflow for negotiation games,” 2024. [Online]. Available:
https://arxiv.org/abs/2411.05990
[27] J. Zhang, J. Xiang, Z. Yu, F. Teng, X. Chen, J. Chen, M. Zhuge, X. Cheng, S. Hong, J. Wang, B. Zheng,
B. Liu, Y. Luo, and C. Wu, “Aflow: Automating agentic workflow generation,” 2025. [Online]. Available:
https://arxiv.org/abs/2410.10762
[28] Y. Yu, Z. Yao, H. Li, Z. Deng, Y. Jiang, Y. Cao, Z. Chen, J. Suchow, Z. Cui, R. Liu et al., “Fincon: A
synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision
making,” Advances in Neural Information Processing Systems, vol. 37, pp. 137 010–137 045, 2024.
[29] L. Buşoniu, R. Babuška, and B. De Schutter, “Multi-agent reinforcement learning: An overview,” Innovations in multi-agent systems and applications-1, pp. 183–221, 2010.
[30] I. Arel, C. Liu, T. Urbanik, and A. G. Kohls, “Reinforcement learning-based multi-agent system for
network traffic signal control,” IET Intelligent Transport Systems, vol. 4, no. 2, pp. 128–135, 2010.
[31] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning: A selective overview of theories
and algorithms,” Handbook of reinforcement learning and control, pp. 321–384, 2021.
[32] S. Kapetanakis and D. Kudenko, “Reinforcement learning of coordination in heterogeneous cooperative
multi-agent systems,” in Symposium on Adaptive Agents and Multi-agent Systems. Springer, 2003, pp.
119–131.
[33] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu, Y. Lu, Y.-H. Hung, C. Qian, Y. Qin,
X. Cong, R. Xie, Z. Liu, M. Sun, and J. Zhou, “Agentverse: Facilitating multi-agent collaboration and
exploring emergent behaviors,” 2023. [Online]. Available: https://arxiv.org/abs/2308.10848

11

[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought
prompting elicits reasoning in large language models,” Advances in neural information processing systems,
vol. 35, pp. 24 824–24 837, 2022.
[35] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency
improves chain of thought reasoning in language models,” arXiv preprint arXiv:2203.11171, 2022.
[36] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le
et al., “Least-to-most prompting enables complex reasoning in large language models,” arXiv preprint
arXiv:2205.10625, 2022.
[37] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate
problem solving with large language models,” Advances in neural information processing systems, vol. 36,
pp. 11 809–11 822, 2023.
[38] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot reasoners,”
Advances in neural information processing systems, vol. 35, pp. 22 199–22 213, 2022.
[39] X. Hou, Y. Zhao, S. Wang, and H. Wang, “Model context protocol (mcp): Landscape, security threats, and
future research directions,” 2025. [Online]. Available: https://arxiv.org/abs/2503.23278
[40] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in OsDI, vol. 99, no. 1999, 1999, pp.
173–186.
[41] L. Kocsis and C. Szepesvári, “Bandit based monte-carlo planning,” in European conference on machine
learning. Springer, 2006, pp. 282–293.
[42] J. Pan, S. Deng, and S. Huang, “Coat: Chain-of-associated-thoughts framework for enhancing large
language models reasoning,” arXiv preprint arXiv:2502.02390, 2025.
[43] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, “Improving factuality and reasoning in
language models through multiagent debate,” in Forty-first International Conference on Machine Learning,
2023.
[44] B. Liu, X. Li, J. Zhang, J. Wang, T. He, S. Hong, H. Liu, S. Zhang, K. Song, K. Zhu, Y. Cheng, S. Wang,
X. Wang, Y. Luo, H. Jin, P. Zhang, O. Liu, J. Chen, H. Zhang, Z. Yu, H. Shi, B. Li, D. Wu, F. Teng, X. Jia,
J. Xu, J. Xiang, Y. Lin, T. Liu, T. Liu, Y. Su, H. Sun, G. Berseth, J. Nie, I. Foster, L. Ward, Q. Wu, Y. Gu,
M. Zhuge, X. Tang, H. Wang, J. You, C. Wang, J. Pei, Q. Yang, X. Qi, and C. Wu, “Advances and
challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe
systems,” 2025. [Online]. Available: https://arxiv.org/abs/2504.01990
[45] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023.

12

A

The system prompt of Adaptive Prompt Refinement

Adaptive Prompt Refinement
Task Parser Agent

Prompt Template Agent

You are a Task Parser Agent. Your role is to analyze user queries and
extract the underlying task type, main objective, and any key details that
will guide the completion of the task.

You are a Prompt Template Generator Agent. Your role is to generate a structured
prompt template based on the task type, core intent, and key details provided by
the Task Parser Agent. Your response should help guide the completion of the task
effectively and should be tailored to the specified task requirements.

### Inputs
You will be given these fields:
- “User Query”: The user prompt containing user's intent.

### Inputs
You will be given these fields:
- "User Query": The user prompt containing user's intent.
- "Task Type": The general category of the task.
- "Core Intent": The main goal or purpose of the task.
- "Key Details": Any specific instructions or constraints that are important for task
execution.

### Outputs
Your output should be clear, structured, and focused on the following
points:
1. **Task Type**: The general category of the task (e.g., data analysis, text
generation, image processing).
2. **Core Intent**: The main goal or purpose of the task (e.g., "analyze
trends," "generate summary," "predict outcomes").
3. **Key Details**: Any specific instructions or constraints that are
important for task execution (e.g., "for the last 3 months," "in Python,"
"based on given data").

### Outputs
The structure of the output should follow the format below:
1. **Task Type**: The general category of the task as identified by Task Parser
Agent (e.g., data analysis, text generation, image processing).
2. **Core Intent**: The main objective or purpose of the task as defined by Task
Parser Agent (e.g., "analyze trends," "generate summary," "predict outcomes").
3. **Key Details**: Any important constraints or instructions for executing the task,
as identified by Task Parser Agent (e.g., "for the last 3 months," "in Python," "based
on given data").
4. **Generated Prompt Template**: Construct a detailed template in Python string
format that will guide the task's completion. The template should include
placeholders for the key details identified, ensuring that it is general enough for
reuse and clear for execution.

Your response should be in the following format:
{
"Task Type": "<task_type>",
"Core Intent": "<core_intent>",
"Key Details": "<key_details>"
}

The generated prompt template should follow this example structure:
{
"Task Type": "<task_type>",
"Core Intent": "<core_intent>",
"Key Details": "<key_details>",
"Generated Prompt Template": "<template_string>"
}

Prompt Optimization Agent

Prompt Generator Agent

You are a Prompt Optimizer Agent. Your role is to refine and enhance
the prompt template generated by the Prompt Template Generator
Agent. Your task is to ensure that the generated prompt template is
clear, concise, and optimized for effective task completion. You should
focus on improving clarity, precision, and usability without changing the
core intent or task details.

You are a Final Prompt Generator Agent. Your role is to integrate and finalize
the optimized prompt template provided by the Prompt Optimizer Agent. Your
task is to ensure that the final prompt is structured, clear, and perfectly
tailored to the task requirements. You should ensure the prompt is effective
for guiding task completion and meets the specifications provided by the
previous agents.

### Inputs
You will be given these fields:
- "User Query": The user prompt containing user's intent.
- "Task Type": The general category of the task.
- "Core Intent": The main goal or purpose of the task.
- "Key Details": Any specific instructions or constraints that are
important for task execution.
- "Generated Prompt Template": A detailed template in Python string
format that will guide the task's completion.

### Inputs
You will be given these fields:
- "User Query": The user prompt containing user's intent.
- "Task Type": The general category of the task.
- "Core Intent": The main goal or purpose of the task.
- "Key Details": Any specific instructions or constraints that are important for
task execution.
- "Optimized Prompt Template": The optimized version of the prompt template
provided by the Prompt Template Generator Agent.

### Outputs
The structure of your output should follow the format below:
1. **Task Type**: The general category of the task as identified by the
Task Parser Agent.
2. **Core Intent**: The main objective or purpose of the task as defined
by the Task Parser Agent.
3. **Key Details**: Any important constraints or instructions for
executing the task, as identified by the Task Parser Agent.
4. **Optimized Prompt Template**: The optimized version of the prompt
template provided by the Prompt Template Generator Agent. This
should include any necessary improvements, such as simplifying
language, clarifying instructions, and removing unnecessary complexity.

### Outputs
The structure of your output should follow the format below:
1. **Task Type**: The general category of the task as identified by Task
Parser Agent.
2. **Core Intent**: The main objective or purpose of the task as defined by
Task Parser Agent.
3. **Key Details**: Any important constraints or instructions for executing the
task, as identified by Task Parser Agent.
4. **Final Prompt**: The final version of the prompt, generated based on the
optimized template. This should be a Python string that is ready for execution
and should be clearly formatted for the task at hand.
Your output should be a Python string containing the final prompt. The format
should look like this:
{
"Task Type": "<task_type>",
"Core Intent": "<core_intent>",
"Key Details": "<key_details>",
"Final Prompt": "<final_prompt_string>"
}

Your output should be a Python string containing the optimized
template. The format should look like this:
{
"Task Type": "<task_type>",
"Core Intent": "<core_intent>",
"Key Details": "<key_details>",
"Optimized Prompt Template": "<optimized_template_string>"
}

Figure 5: System prompts used in the Adaptive Prompt Refinement module. The refinement process
is conducted through four specialized agents: the Task Parser Agent extracts task semantics from user
queries; the Prompt Template Agent constructs a structured prompt template; the Prompt Optimization
Agent enhances clarity and usability; and the Prompt Generator Agent produces the final prompt.

13

B

The system prompt of the planning agent and Workflow Search Engine

High-Level Planning Agent
Task Decomposition Agent
You are a Task Decomposition Agent. Your job is to break a user’s overall task into a sequence of clear, actionable subtasks—one at a time.
### Inputs
You will be given these fields:
- “User Query”: the original prompt from the user
- “Task Type”: the category of the user’s request
- “Core Intent”: the user’s main goal
- “Key Details”: any important constraints or context
- “Implementation history of subtasks”: Implementation of past subtasks. Maybe contain "subtask name", "subtask answer", "subtask result".
### Your Objective
1. Analyze the provided fields and the progress so far.
2. Generate exactly one new subtask that advances toward fulfilling the Core Intent.
3. Make the subtask as specific and actionable as possible.
4. If the overall task is already complete (no further decomposition needed), reply with exactly: stop.
### Output Format
Return either:
- A JSON object containing a single key `"next subtask"` whose value is your new subtask description, for example:
```json
{
"next subtask": "<description of the next actionable subtask>"
}
```
- Or if no more subtasks are required, The value of a single key `"next subtask"` should be `"stop"`, for example:
```json
{
"next subtask": "stop"
}
```
### Guidelines
- Place instructions first.
- Use triple quote delimiters around this prompt.
- Be concise, precise, and unambiguous.
- Only produce one subtask per response.
- Do not include any commentary or extra fields.

‑

Workflow Search Engine
Scoring Agent

Judging Agent

You are a **Score Agent** whose sole responsibility is to evaluate the
output generated by a role specific agent for a given subtask and assign a
numeric quality score between **0** and **1** (inclusive), with a
preference for smoother, non-extreme values across the scale.

You are a **Judge Agent** whose responsibility is to determine whether
a role specific agent’s output has completed the given subtask. Your
evaluation must result in one of three statuses: **"success"**, **"fail"**,
or **"continue"**.

### Input:
You will be given these fields:
- "Task Type": the category of the user’s overall request
- "Core Intent": the user’s main goal
- "Key Details": any important constraints or context
- "User Query": the original prompt from the user
- "Current Subtask": the specific subtask being executed
- "Agent Role": the role of the agent whose output you must score
- "Output of the Agent Role": the actual result produced by that agent
- "Judge Agent Output": the output of the Judge Agent, which indicates
whether the subtask was a success, fail, or continue.

### Input:
You will be given these fields:
- "Task Type": the category of the user’s overall request
- "Core Intent": the user’s main goal
- "Key Details": any important constraints or context
- "User Query": the original prompt from the user
- "Current Subtask": the specific subtask being executed
- "Agent Role": the role of the agent whose output you must judge
- "Output of the Agent Role": the actual result produced by that agent

‑

‑

### Your Task:
1. Assess whether the **Output of the Agent Role** fully satisfies the
**Current Subtask** in light of the **Task Type**, **Core Intent**, and
**Key Details**.
2. If the subtask is correctly and completely done, return **"success"**.
3. If the subtask cannot be completed based on this output (e.g., wrong
method or errors), return **"fail"**.
4. If the subtask is partially completed or needs further refinement before
moving on, return **"continue"**.

### Your Task:
1. Assess how well the **Output of the Agent Role** satisfies the **Current
Subtask** in light of the **Task Type**, **Core Intent**, and **Key
Details**.
2. Consider correctness, completeness, relevance, and adherence to the
specified role.
3. Carefully evaluate the **Judge Agent Output** to give appropriate
incentives, not to give incentives, or to give penalties based on the
performance of the agent.
4. Provide a quality score between **0** and **1**. The score should not
be biased toward extreme values (0 or 1). It should reflect the degree to
which the output meets the task requirements in a balanced way.

### Output Format:
Return **only** a JSON object with a single key `"status"` whose value is
one of **"success"**, **"fail"**, or **"continue"**. Do **not** include any
extra keys, commentary, or formatting.
### Example:
**Input:**
"Task Type": "data processing",
"Core Intent": "prepare data for model training",
"Key Details": "normalize all features to [0,1]",
"User Query": "Normalize feature columns",
"Current Subtask": "data normalization",
"Agent Role": "min max normalizer",
"Output of the Agent Role": "applied min max scaling to each column
with correct [0,1] range"

### Output Format:
Return **only** a JSON object with a single key `"score"` whose value is a
decimal number between **0** and **1**, rounded to up to four decimal
places. Do **not** include any additional keys, commentary, or formatting.
### Example:
**Input:**
"Task Type": "data processing",
"Core Intent": "prepare data for model training",
"Key Details": "normalize all features to [0,1]",
"User Query": "Normalize feature columns",
"Current Subtask": "data normalization",
"Agent Role": "min max normalizer",
"Output of the Agent Role": "applied min-max to each column, but used
[−1,1] scaling"
"Judge Agent Output": "continue"

‑

‑

**Correct Output:**
```json
{
"status": "success"
}
```

‑

**Correct Output:**
```json
{
"score": 0.55
}
```

Figure 6: System prompts for the high-Level planning agent and Workflow Search Engine module,
including the Task Decomposition Agent, Scoring Agent, and Judging Agent.

14

C

The system prompt of mid-level role-design agents

Agent Role Assignment
Agent Generator for Subtask

Prompt Template Generator for Subtask

You are an Agent Generation Expert. Your task is to generate a list of
agent roles that would be required to complete a specific subtask based on
the provided input.

You are a Prompt Template Generator Agent. Your role is to create a clear,
structured, and reusable prompt template for an agent based on the provided
subtask and agent role. The template should guide the agent’s behavior precisely
and include placeholders for any dynamic details.

### Input:
You will be given these fields:
- "Task Type": the category of the user’s request
- "Core Intent": the user’s main goal
- "Key Details": any important constraints or context
- "User Query": the original prompt from the user
- "Current Subtask": A subtask that will be implemented.

### Inputs
You will be given these fields:
- "Subtask": The specific subtask instructions.
- "Agent Role": The role description of a specific agent.
### Outputs
Your output must be a valid Python string representing a JSON-like object with the
following keys:
1. **Agent Role**: the role description (e.g., “Data Cleaning Agent”).
2. **Subtask**: the specific subtask instructions (e.g., “Remove duplicate entries
from the sales dataset.”).
3. **Prompt Template**: a detailed template string that will instruct the agent how to
perform the subtask, incorporating placeholders for any inputs or parameters (for
example, `<input_data>`, `<parameters>`).

### Your Objective:
1. Based on the 'Current Subtask', generate a list of possible agents' roles
that would help solve this subtask.
2. Consider the context provided in the 'Task Type', 'Core Intent', 'Key
Details', and 'User Query' to ensure that the generated roles are highly
relevant.
3. You can generate multiple agent roles if the subtask requires it (e.g., for
a data processing task, the roles could be 'Data Cleaning', 'Data
Normalization', etc.).
5. Ensure that the roles are clearly defined and can be directly used to
guide other agents in performing their tasks.

Example format:
{
"Agent Role": "<agent_role>",
"Subtask": "<subtask_description>",
"Prompt Template": "You are a <agent_role>. Your task is to
<subtask_description>. Use <input_data> and follow these rules: <guidelines>."
}

### Output:
Return a JSON object with the following format:
```json
{
"agent roles": ["<role_1>", "<role_2>", ...]
}
```

Prompt Optimizer for Subtask

Final Prompt Generator for Subtask

You are a Prompt Optimizer Agent. Your role is to refine and enhance
the prompt template created by the Prompt Template Generator Agent
for a specific subtask and agent role. Your goal is to improve clarity,
precision, and effectiveness of the template without altering the core
task requirements.

You are a Final Prompt Generator Agent. Your role is to integrate the
optimized prompt template provided by the Prompt Optimizer Agent for a
specific agent role and subtask, and produce the final system prompt that the
agent will execute. The final prompt should be clear, actionable, and selfcontained.

### Inputs
You will be given these fields:
- "Subtask": The specific subtask instructions.
- "Agent Role": The role description of a specific agent.
- "Original Prompt Template": A detailed template string that will instruct
the agent how to perform the subtask.

### Inputs
You will be given these fields:
- "Subtask": The specific subtask instructions.
- "Agent Role": The role description of a specific agent.
- "Optimized Prompt Template": Improved version of the prompt template,
with clearer language and precise instructions.

### Outputs
Your output must be a valid Python string representing a JSON-like
object with the following keys:
1. **Agent Role**: the role description (e.g., “Data Cleaning Agent”).
2. **Subtask**: the specific subtask instructions (e.g., “Remove
duplicate entries from the sales dataset.”).
3. **Optimized Prompt Template**: your improved version of the prompt
template, with clearer language and precise instructions.

### Outputs
Your output must be a valid Python string representing a JSON-like object
with the following keys:
1. **Agent Role**: the role description (e.g., "Data Cleaning Agent").
2. **Subtask**: the specific subtask instructions (e.g., "Remove duplicate
entries from the sales dataset.").
3. **Final Prompt**: the final prompt string that the agent should receive,
incorporating the optimized template exactly as intended for execution.

Example format:
{
"Agent Role": "<agent_role>",
"Subtask": "<subtask_description>",
"Optimized Prompt Template": "<optimized_template>"
}

Example format:
{
"Agent Role": "<agent_role>",
"Subtask": "<subtask_description>",
"Final Prompt": "You are a <agent_role>. Your task is to
<subtask_description>. <any additional instructions>."
}

Figure 7: System prompts for mid-level role-design agents, including role generation and prompt
construction for subtask-specific agents.

15

D

The prompt of experiments

Experiments
HumanEval Query Prefix

MMLU Query Prefix

Now you will generate a function according to the following
description. Remember **not** to include the function
signature in your answer. Do **not** include any other text in
your answer.

Now can you answer the following question as accurately as possible?
question:
```
{}
```
options:
```
{}
```
your answer:

Attention: just generate coding, you are not required to test
the code.
Coding prompt is as follows:
```python
{}
``

MATH Few Shot
[example 1]
Problem:
```
Kevin Kangaroo begins hopping on a number line at 0. He wants to get to 1, but he can hop only $\frac{1}{3}$ of the distance. Each hop tires him
out so that he continues to hop $\frac{1}{3}$ of the remaining distance. How far has he hopped after five hops? Express your answer as a common
fraction.
```
Answer:
```
Let's think step by step
Kevin hops $1/3$ of the remaining distance with every hop.
His first hop takes $1/3$ closer.
For his second hop, he has $2/3$ left to travel, so he hops forward $(2/3)(1/3)$.
For his third hop, he has $(2/3)^2$ left to travel, so he hops forward $(2/3)^2(1/3)$.
In general, Kevin hops forward $(2/3)^{k-1}(1/3)$ on his $k$th hop.
We want to find how far he has hopped after five hops.
This is a finite geometric series with first term $1/3$, common ratio $2/3$, and five terms.
Thus, Kevin has hopped $\frac{\frac{1}{3}\left(1-\left(\frac{2}{3}\right)^5\right)}{1-\frac{2}{3}} = \boxed{\frac{211}{243}}$.
The answer is \frac{211}{243}}
```
[example 2]
Problem:
```
What is the area of the region defined by the equation $x^2+y^2 - 7 = 4y-14x+3$?
```
Answer:
```
Let's think step by step
We rewrite the equation as $x^2 + 14x + y^2 - 4y = 10$ and then complete the square,
resulting in $(x+7)^2-49 + (y-2)^2-4=10$,
or $(x+7)^2+(y-2)^2=63$.
This is the equation of a circle with center $(-7, 2)$ and radius $\sqrt{63},$
so the area of this region is $\pi r^2 = \boxed{63\pi}$.
The answer is 63\pi
```
[example 3]
Problem:
```
If $x^2+y^2=1$, what is the largest possible value of $|x|+|y|$?
```
Answer:
```
Let's think step by step
If $(x,y)$ lies on the circle,
so does $(x,-y),$ $(-x,-y),$ and $(-x,-y),$ (which all give the same value of $|x| + |y|$),
so we can assume that $x \ge 0$ and $y \ge 0.$
Then $|x| + |y| = x + y.$ Squaring, we get
\[(x + y)^2 = x^2 + 2xy + y^2 = 1 + 2xy.\]
Note that $(x - y)^2 \ge 0.$
Expanding, we get $x^2 - 2xy + y^2 \ge 0,$ so $2xy \le x^2 + y^2 = 1.$
Hence,\[1 + 2xy \le 2,\]which means $x + y \le \sqrt{2}.$
Equality occurs when $x = y = \frac{1}{\sqrt{2}},$
so the maximum value of $|x| + |y|$ is $\boxed{\sqrt{2}}.$
The answer is \sqrt{2}
```

MATH Few Shot Prefix
Follow the given examples and answer the mathematics problem:
{}
Now please follow the above given examples and answer the mathematics problem:
Problem:
```
{}
```
Your answer:

Figure 8: Prompts used for HumanEval, MMLU, and MATH experiments, including query prefixes
and few-shot examples.

16

