Multi-agent Reinforcement Learning: A
Comprehensive Survey

arXiv:2312.10256v2 [cs.MA] 3 Jul 2024

Dom Huh1 and Prasant Mohapatra1,3
1
University of California, Davis
{dhuh, pmohapatra}@ucdavis.edu
3
University of South Florida

Abstract
Multi-agent systems (MAS) are widely prevalent and crucially important in numerous real-world applications, where multiple agents must make decisions to achieve
their objectives in a shared environment. Despite their ubiquity, the development of
intelligent decision-making agents in MAS poses several open challenges to their
effective implementation. This survey examines these challenges, placing an emphasis on studying seminal concepts from game theory (GT) and machine learning
(ML) and connecting them to recent advancements in multi-agent reinforcement
learning (MARL), i.e. the research of data-driven decision-making within MAS.
Therefore, the objective of this survey is to provide a comprehensive perspective
along the various dimensions of MARL, shedding light on the unique opportunities that are presented in MARL applications while highlighting the inherent
challenges that accompany this potential. Therefore, we hope that our work will
not only contribute to the field by analyzing the current landscape of MARL but
also motivate future directions with insights for deeper integration of concepts
from related domains of GT and ML. With this in mind, this work delves into a
detailed exploration of recent and past efforts of MARL and its related fields and
describes prior solutions that were proposed and their limitations, as well as their
applications.

1

Introduction

Multi-agent reinforcement learning (MARL) has long been recognized as a pivotal domain in artificial
intelligence (AI), promising dynamic solutions for complex tasks within multi-agent systems (MAS)
that involve multiple goal-oriented decision-making, i.e. control, agents. The importance of devising
such solutions is evident, as it enables the realization of a wide array of real-world applications, where
the consideration of the existence of other intelligent agents is required. The process of developing
these agents is largely centered around facilitating the emergence of not only decision-making abilities
but also an adept understanding of social dynamics in a data-driven manner. Hence, with proper
modeling and learning methods, these agents strive to leverage the multi-agent nature of their shared
environment to achieve their individual and collective goals.
Alongside this focus on social behaviors and their connection to an agent’s decision-making capabilities, the motivation to integrate concepts from related domains such as game theory (GT) and machine
learning (ML) becomes critical, as GT and ML provide a rich background and broader perspective
to the problem posed by MARL. However, it remains equally important to concurrently study the
distinctive challenges that arise under the MARL paradigm Stone and Veloso [2000], Bernstein et al.
[2013] to fully understand the field’s unique intricacies and promote potential breakthroughs.
In the past decade, there has been a significant interest in MARL efforts, poised to endow the
desirable behavioral qualities that characterize intelligent social agents through data-driven learning

processes Gronauer and Diepold [2022]. These learning processes have ranged from assuming
classical perspectives of full rationality to embodying models of bounded rationality Shoham et al.
[2007], wherein agents progressively refine their behaviors in a myopic and iterative manner over
time and experience. A common theme of this survey is emphasizing the increasing incorporation of
the realism and its complexities that pervades real-world MAS applications into our MARL solutions.
And despite the many successes in MARL in these prior efforts, many open challenges persist,
paving the way for future research endeavors. The central purpose of this survey is to provide a
comprehensive view of these efforts, concretely staging the current state of MARL research from a
holistic perspective.
The structure of this survey is as follows. In Section 3, we define the problem statement of learning
optimal control within MAS, and discuss seminal concepts from foundational fields of MARL
such as GT and ML – all of which provide the foundation to the recent ideas studied in MARL.
In Section 4, we discuss the unique benefits and challenges of learning in a MAS and explore
the learning pathologies that plague the MARL paradigms. Lastly, in Section 5, the prospects of
MARL are studied, such as MARL-specific simulation, training paradigms, communication methods,
the challenges of multi-agent credit assignment and ad-hoc team-play, social learning, and agent
modeling, and a detailed discussion regarding the recent efforts associated to these prospects.

2

Related Surveys

There exists a rich literature surveying various research topics in MARL, from general overviews
Stone and Veloso [2000], Shoham et al. [2003], Busoniu et al. [2008], Bloembergen et al. [2015],
Gronauer and Diepold [2022] to specific topics such as dealing with non-stationarity Hernandez-Leal
et al. [2017], independent learners Matignon et al. [2012], multi-agent learning Panait and Luke
[2005], Tuyls and Weiss [2012], communication Luong et al. [2019], Zhu et al. [2024], ad-hoc
team-play Mirsky et al. [2022], agent modeling Baarslag et al. [2016], Albrecht and Stone [2018],
safety Lasota et al. [2017] and knowledge transfer Silva and Costa [2019]. However, many of these
perspectives are constrained, excluding important aspects that come only from GT and ML viewpoints,
thereby lacking the context needed to fully realize the current developments and limitations of MARL.
In contrast, our article presents a more modern and comprehensive survey that aims to provide a
holistic view of the challenges inherent and unique to learning control within multi-agent environments. Additionally, we provide context to these challenges by weaving together the perspectives
from GT and ML into a new unified view to present a novel understanding of the distinctive nature of
the MARL problem.

3

Background

In this section, we formalize the basis of the MARL problem statement and define its learning goals
and solution concepts. We additionally explore related fields, i.e. GT and ML, that contribute to a
deeper understanding of the study of MARL.
3.1

Multi-agent Environment

A MAS consists of a population of decision-making agents that exist within a shared environment, as
illustrated in Figure 1. These agents observe their environment and communicate with one another to
perform actions that align with their objectives. This observation and communication amongst agents
is constrained by decentralization.
Definition 1 (Decentralization) In a decentralized setting, each agent is capable of perceiving its
environment, communicating with other agents, and taking action autonomously.
There exist two forms of decentralization. Natural decentralization is the limitations imposed
by physical realities, like communication range, while artificial decentralization involves specific
requirements to improve tractability, such as communication bandwidth Whiteson [2020]. More
generally, each agent is defined and inhibited by a set constraint.
The agents can devise strategies to make decisions to reach their defined goals. This causal association
between stimuli, action, and objective is what we refer to as the agent’s behavior Matarić [1994]. The
2

Figure 1: A visualization of a generalized multi-agent system following the iterative control process.
environment then responds to the agent’s actions, transitioning the current setting to the next state
and providing the agents with feedback signals. This response from the environment is referred to as
the model. This process between the agents and their environment forms a closed-looped interaction
that iterates until a terminal condition is met.
The MAS setting described here is widely prevalent in real-world applications, encompassing autonomous vehicles Shalev-Shwartz et al. [2016], Zhou et al. [2020], internet marketing Jin et al.
[2018a], multi-robot control Matarić [1997], networking applications such as optimizing communication networks Luong et al. [2019] and traffic control Calvo and Dusparic [2018], Chu et al. [2019],
and multiplayer game playing Samvelyan et al. [2019].
3.2

Stochastic Game

We now introduce a formal representation of the MAS setting described in Section 3.1, called the
stochastic game Shapley [1953], Bowling and Veloso [2001], where the term “game" refers to the
interactions between strategic agents. This framework serves as the basis for a wide range of multiagent applications Busoniu et al. [2010], and is related to other models of games, as seen in Figure
2.
Definition 2 (Stochastic Game) A stochastic game is a 5-tuple (N, S, A, r, T ) where these elements
are defined as:
• N is the set of n agents.
• S is the set of (global) states.
• A = A0 × A1 × · · · × An is the joint action space, where Ai is the action space of agent i.
• r = r0 × r1 × · · · × rn is the joint reward function.
• T : S × A × S 7→ P (S) is the state transition operator which maps a state-action pair to
the probability of next states.
The state defines the global setting and configuration of the environment. To transition from one state
to another, each agent i uses their policy πi : S 7→ Ai — a functional representation of a composite
of the agent’s behaviors that is expressed as a mapping from its perceived state to action — to make
decisions, otherwise known as their strategy. Generally, a policy returns a probability distribution
over the action space conditioned on the state.
X
π(a|s) = 1
a∈A

The joint policy π : S 7→ A is defined as a mapping to the joint action space, commonly achieved
by concatenating the local actions computed by each agent’s policies. We introduce the concept of
an information set as an aggregate state, where it encapsulates all information available to an agent
during its decision-making process.
The state transition function T returns the probability of reaching certain states from a given stateaction pair. As each state holds the Markov property, i.e. every state is sufficient to infer the future,
3

Figure 2: Models of Games: The overview of different models of multi-agent interactions is illustrated,
from normal-form games to variations of stochastic games. We note that Markov Decision Processes
(MDP) and POMDP are not models of a game but are included for a complete illustration. The
following figure was adapted from Albrecht et al. [2024].
the state transition function is sufficiently conditioned on the current state and action.
T (s, a, s′ ) = P (st+1 = s′ |st = s, at = a)

(1)

Using the stochastic game framework as a basis, we can build and introduce additional concepts to
align with various real-world scenarios.
Sequential and Macro-Actions In many multi-agent applications, agents may perform actions
that occur asynchronously, meaning either agents take turns performing their actions, or their actions
take different duration of time. In cases where agents take turns, we distinguish this form of
decision-making as sequential and is represented as an extensive-form game, otherwise, we refer to
settings where agents’ actions are done at the same time as simultaneous and use the stochastic game
framework. In the latter case where actions have varying duration, we turn to an alternate framework
called the macro-action stochastic game Amato et al. [2019], Xiao et al. [2022]. This framework
removes the assumption of synchronized primitive-action execution, which assumes that all actions
take the same period of time. Hence, the notion of macro-actions is introduced, where macro-actions
are temporally-extended actions that can vary in duration. Consequently, this also introduces a notion
of macro-observation.
Imperfect Information It may be intractable for agents to directly perceive the (global) state of the
environment. Instead, agents may only have access to observations. Thereby, we introduce the joint
observation space O = O0 × · · · × On that accounts for the notion of imperfect information Shoham
and Leyton-Brown [2008]. Observations do not necessarily satisfy the Markovian property and may
contain limited insights regarding the true state of the environment. In fact, even joint observational
histories h = {o0 , o1 , . . . ot } can be insufficient to infer the state st of the environment. However,
the contrapositive is true, where the emission transition function E(o, s) defines the mapping from
states to corresponding joint observations that can be potentially induced.
E(o, s) = P (o|s)

(2)

The concept of imperfect information encapsulates the limits of information agents are restricted to,
and help enforce the constraints of decentralization. Imperfect information is strongly related to two
other important ideas of information limitations: partial observability and incomplete information.
Partial observability constrains agents’ access to a subset of the state that can be obscured by noise.
In settings with incomplete information, agents do not have common knowledge about the game that
is being played, which leads to uncertainties regarding various aspects of the state. A framework that
takes these concepts of information limitation into account is called partially-observable stochastic
games (POSG), and if agents hold a belief, or type, over these uncertainties, this is called a Bayesian
game Harsanyi [1967].
Reward Function Within all models of decision-making, the reward function plays an central role
in guiding the behaviors of agents. The reward function provides immediate feedback to agents,
evaluating the state and/or action of the agents with respects to their objective in the form of a scalar
4

value. The common goal in optimizing control in MAS is to maximize this reward signal, thereby
instilling a degree of rationality.
Definition 3 (Rationality) Rational agents take actions that will maximize the expected cumulative
reward, known as the expected return, they receive over time.
In certain multi-agent applications, it can be difficult to define individualized reward functions for
each agent, but it may be trivial to define a single reward function for all agents. This difficulty
relates to the nature of the task, often derived from an inability to define proper multi-agent credit
assignment. In such applications, we use the decentralized MDP (Dec-MDP), where a centralized
reward function returns a collective score shared by all agents.
Nature of Interaction An important factor to keep in mind is the relationships between the agents’
actions and decisions and how they interact with one another, which we refer to as the nature of
interactions. In each setting, the interplay between each agent’s behaviors can lead to different
dynamics and outcomes and may require varying considerations, notably to the definition of the
reward function.
• Cooperative setting - All agents share the aligning goal.
• Adversarial setting - Agents have dichotomous goals.
• Mixed setting - Agents have varying goals.
In a cooperative setting, all agents share aligning goals, where the reward function is typically
designed to promote collaboration and joint success. In an adversarial setting, groups of agents may
have dichotomous goals, meaning agents that aim to maximize their own reward can impede the
achievement of other agents’ goals. In a mixed setting, agents have varying goals, and this introduces
additional complexities as agents may have aligning, conflicting, or overlapping interests at various
points in time. These interactions can be realized with inherent and artificial structures placed within
the task definition.
There exists a deep interconnection between the reward function and the nature of interactions, as
the reward signals directly guide the emergent behaviors and outcomes within a MAS. For instances,
the rewards of each agent may be computed by a function of not only their own policies but also the
policies of others. This concept of multi-agent reward dependency is formalized with aggregative
games.
Social Context The purpose of social context is to ensure mutually consistency between agents’
actions within an shared environment, and is defined by social convention and role assignment Busoniu
et al. [2008]. Social conventions are established through social constructs to often define preferences
for joint action profiles and is used to resolve conflicting action selections. Role assignments restrict
and define the actions available for each agent as well as influence their rationality and objectives.
The properties and conditions of these elements are largely considered task-specific, however, all
MARL solutions must take into account these unique characteristics and demands of each situation.
Networked Games To tractably perform certain multi-agent tasks, agents must communicate
with one another. This is often achieved through defined communication channels and form a
communication network G(s) that is associated to each state s. This element G(s) = (V, E), where
vertices V correspond to the agents and an edge (i, j) ∈ E exists if agents i and j can communicate,
is appended to the stochastic game framework, forming the networked stochastic game.
Coordination The outcome of an agent’s actions can often depend on the actions of other agents.
This dependency of behaviors and the corresponding consequences of their actions between agents is
encapsulated by the concept of coordination. There are two common approaches of embedding the
idea of coordination into a game paradigm, through a coordination graph or interactions.
• Coordination graphs specify the coordination dependencies between agents’ actions in form
of a graph, where the nodes represent agents and the edges represent these dependencies
Nair et al. [2005], Guestrin et al. [2001], Kok et al. [2005]. This concept is expressed with
action-graph games Jiang et al. [2011]. With coordination graphs, we can factorize the global
5

utility into a set of local payoff and utility functions. Each payoff function is associated with
a subset of agents and can be interpreted as (hyper)-edges in a graph where the nodes are
agents Amato and Oliehoek [2014], whereas the utility functions reflect individual agent’s
utility. With this factorization, maximum-a-posteriori estimation techniques, such as variable
elimination Guestrin et al. [2001], max-plus Vlassis et al. [2004], or Q-learning Kok et al.
[2005], can be used to compute the optimal joint action. More recent extensions leverage
deep neural networks to model different components of the factorized value function Böhmer
et al. [2020] or the coordination graph itself Li et al. [2021].
• Similar to coordination graphs, interactions broadly define specific conditions in which
coordination should occur Koller and Milch [2003]. However, unlike coordination graphs,
we can specify restriction of these interactions for only defined for certain states or stateactions Melo and Veloso [2009, 2011], De Hauwere et al. [2010]. Additionally, these
interactions are classified as strategic compliments or substitute Monaco and Sabarwal
[2016], where interactions can produce mutual reinforcements or discouragements.
Thereby, the concept of coordination is illustrated naturally through “social" networks, thereby
often studied within network games as defined in classic GT. We note that network games are not
networked stochastic game, where the former specifies coordination dependencies and the latter
defines communication channels.
Return A state-action trajectory τ = {s0 , a0 , s1 , a1 , . . . , sT } is sampled from the dynamics model
distribution pπ (τ ) following a joint policy π, where:
T
Y
pπ (τ ) = P (s0 )
π(at |st )T (st , at , st+1 )
(3)
t=0

With this trajectory, we can compute an important quantity called the return, which calculates the
future utility of a given state. Formally, the return G(i,t) at each time step for an agent i, sometimes
called the agent’s gain, is the cumulative future discounted reward.
T
X
′
G(i,t) (τ ) =
γ t ri (st′ , ai,t′ )
(4)
t′ =t

where γ ∈ [0, 1] is the discount factor that enforces a diminishing value for more distant rewards.
Value Function The value function Vπi (st |π) and the Q-value function Qπi (st , ai,t |π) map the
state st and state-action pair (st , ai,t ) to the expected return for an agent i given some joint policy π,
and are commonly used to develop solutions for optimizing controls in MAS.
Vπi (st |π) = Eτ ∼pπ (τ |st ) [G(i,t) (τ )]
(5)
Qπi (st , ai,t |π) = Eτ ∼pπ (τ |st ,ai,t ) [G(i,t) (τ )]
(6)
Vπi (st |π) = Ea∼π(a|st ) [Qπi (st , ai,t |π)]
(7)
The advantage function Aπi (st , ai,t |π) measures the benefit of taking action ai,t in state st for agent
i under the joint policy π. It quantifies how much better or worse it is to choose action ai,t compared
to the average expected return of all actions in that state, similar to the concept of regret Jin et al.
[2018b]. Hence, the advantage function is defined as the difference between the Q-value function
Qπi (st , ai,t |π) and the state-value function Vπi (st |π).
Aπi (st , ai,t |π) = Qπi (st , ai,t |π) − Vπi (st |π)
(8)
MAS Objective The objective Ji (·) of agent i is expressed as maximizing the expected return,
where trajectories τ are sampled from the dynamics model distribution pπ (τ ) following a joint policy
π.
Ji (π) = Eτ ∼pπ (τ ) [G(i,0) (τ )]
(9)
In practice, the expectation of the return can be approximated using the Monte Carlo sampling or
through temporal difference estimation. Such empirical estimates can result in return estimates
with high variance, resulting in greater learning complexity, a central and persistent issue in MARL
solutions.
We state the general form of the MAS optimization objective, which we describe as maximizing the
expected return for all agents.
maximize Ji (π), ∀i ∈ N
(10)
6

3.3

Game Theory

The field of GT, often studied within the domains of economics, provides a formal context that
analyzes and conceptualizes strategic interactions among multiple agents within a market Osborne
[2009]. The term “market" will be interchangeably used with the idea of a shared environment. We
examine the MAS optimization objective from a game theoretic perspective to understand solution
concepts relating to learning goals and social principles used to identify the joint behaviors that are
considered desirable or interesting Myerson [1985]. For further study into GT, we refer readers to the
following resource Osborne [2009].
Utility and Prospect Theory Under the lens of utility theory Fishburn et al. [1979], Shoham and
Leyton-Brown [2008], we study how individuals make choices by quantifying their preferences,
defining an preference relation. We define the utility function u(·) to reflect the individual’s subjective
evaluation of their overall satisfaction, assigning a numerical value to each outcome and obeys the
axioms of preference. Utility theory affirms the sufficiency of scalar value representation of the
agent’s preferences Shoham and Leyton-Brown [2008].
In many real-world applications, agents face uncertainty. The causes of this uncertainty stem not only
from the environment dynamics but also from the behaviors of other agents. Under such uncertain
settings, the agents’ decision-making processes should account for these unpredictabilities. We
introduce this notion as risk and the domain of prospect theory, a study of decision-making under
risk Kahneman and Tversky [2013], Tversky and Kahneman [1992], Prashanth et al. [2022]. By
taking risk into account, agents are capable of weighing their intrinsic preferences and objectives
against their unknowns, defining the agents’ aversion to losses, their reference points, and diminishing
sensitivity Kahneman and Tversky [2013]. Hence, through prospect theory, we gain insights into
behaviors driven by risk, where “losses loom larger than gains" thereby illustrating the behaviors
that come with loss-averse and gain-seeking decisions, and remains a large focus in behavioral GT
Camerer [1997]. There exists a diminishing sensitivity to these losses and gains, meaning that the
impact of a change diminishes with the distance from the reference point. Reference points are a
concept introduced by prospect theory that represents the status quo and argues that the value of
an agent is defined by the final utility positions, as stated in traditional utility theory, may not paint
the whole picture, and instead, it is important to view their value in terms of gains and losses, i.e.
the agent’s value on a relative scale. However, the behaviors driven by risk defined by prospect
theory become less relevant under repeated market settings, as experience within a market reduces
uncertainty Loomes et al. [2003]. We note that (expected) utility theory does account for uncertainty
in its own manner, where the solutions derived from utility theory are considered risk-neutral but this
faces violations in many risk-sensitive games Kahneman and Tversky [2013].
Generally, risk-sensitive decision-making can be modeled with chance constraints, which are statistical constraints to averse based on a probability of a high utility loss at specified risk-tolerance levels,
or value-at-risk (VaR) and expected shortfall (i.e. conditional VaR (CVaR)), which place a similar
constraint which is now specified by quantiles of the utility distribution, i.e. the tail risk.
Incentives and Mechanisms Incentives provide context to the unique behaviors that come along
with asymmetries in information and actions between agents in market settings, which are commonly
studied under contract theory and auctions Salanié [2005]. For instance, agents may be unequally
informed about the various parts of the market, which can be reflective of the agent’s roles or the
social context, and this can lead to adverse selections, moral hazards, and nonverifiability Laffont and
Martimort [2009]. Adverse selection refers to situations where the agent with less information may
make decisions that are unfavorable or risky, whereas moral hazard refers to the tendency of agents to
take greater risks when they are insured or protected because they know they are shielded from some
of the consequences. Lastly, nonverifiability refers to situations where the quality or performance
of goods or services exchanged cannot be easily verified by the parties involved. Together, these
concepts underscore the critical role of incentives in aligning behaviors and outcomes in markets
characterized by information asymmetries. Incentives influence how agents gather and disclose
information, manage risks, and fulfill contractual obligations, thereby shaping the efficiency and
effectiveness of market interactions.
We now consider how these games and their rules are constructed, i.e. game form, to induce certain
desired outcomes and perhaps more importantly, avoid undesirable outcomes. This effort is studied
under the domain of mechanism design Hurwicz and Reiter [2006] and accounts for both incentive
7

and feasibility constraints, as well as the distribution of agent preferences, particularly distinguishing
between whether they are public or private. Mechanism design remains a key focus in GT research and
plays a crucial role in both theory and practice, offering frameworks to improve market functionality,
regulatory policy, and organizational design in contexts of dynamic markets.
Solution Concepts and Equilibrium Solution concepts refer to states of equilibria and are described by meaningful or interesting properties used to evaluate joint policies, otherwise known as
strategy profiles. Pareto efficiency is a general criterion commonly used to evaluate strategy profiles.
′
A strategy profile π Pareto dominates another strategy profile π if:
′

∀i ∈ N : ui (π ) ≥ ui (π)
′

∃i ∈ N : ui (π ) > ui (π)

(11)

′

In other words, no agent using strategy profile π can be better off without making another agent
worse off by using π. We define Pareto improvement as any adjustments to a strategy profile that
′
makes the resulting strategy profile more Pareto efficient, i.e. π → π .
Definition 4 (Pareto Efficiency) A strategy profile π ∗ is a Pareto efficient solution if it is not Pareto
dominated by any other strategy profiles.
Pareto efficiency focuses on maximizing overall welfare, which is the sum of the all agent’s utilities.
However, Pareto efficiency does not emphasize individual rationality or collective stability. Therefore,
Pareto efficient solution may not necessarily be a good measure if the truly optimal solution requires
agents to act against their self-interest or deviate from some locally desirable strategies. Importantly,
Pareto optimality does not address the issue of fairness or equality, as some individuals may benefit
more than others in the pursuit of maximizing overall welfare Shoham and Leyton-Brown [2008].
The concept of best response provides an alternative perspective in analyzing strategic interactions in
multi-agent systems that aligns more with concepts of individual rationality and collective stability.
Given a strategy profile π = {π0 , π1 , . . . , πi , . . . , πN }, a best response strategy πi′ for agent i is
defined by:
∀πi : ui (Jπ−i , πi′ K) ≥ ui (Jπ−i , πi K)
(12)

where π−i = π \ πi denotes the strategy profile without agent i and Jπ1 , π2 K is the strategy profile of
π1 , π2 . In other words, the best response strategy of an agent is one that maximizes its utility given
the strategies chosen by other players.
The solution concept of a Nash equilibrium (NE) π ∗ applies this notion of best response to the
∗
collective, where for all agents i, πi∗ is the best response to π−i
.
∗
∗
∀πi : u(Jπ−i
, πi∗ K) ≥ u(Jπ−i
, πi K)

(13)

Definition 5 (Nash Equilibrium) A Nash equilibrium (NE) defines a state where no individual agent
can increase its expected return by unilaterally deviating from their policy Nash [1951].
Hence, within a Nash equilibrium, all of the agents’ strategy is the best response to the other agents’
strategy.
Unfortunately, similar to Pareto efficient solutions, NE is not unique, and determining the differences
between sample equilibria in terms of their social behaviors is unclear. In fact, the behaviors exhibited
by sample equilibria can vary starkly, where their comparison can be computed using other concepts
of efficiency such as coordination ratio Koutsoupias and Papadimitriou [1999] or Price of Stability
Anshelevich et al. [2008]. While NE remains a popular criterion for multi-agent decision-making
under uncertainty, computing this equilibrium may be computationally intractable in complex games.
Moreover, achieving desired joint behaviors is not guaranteed through this approach Shoham et al.
[2003], Matignon et al. [2012]. Nonetheless, NE does represent stable points akin to saddle points
within the optimization landscape.
To address the intractability of computing a strict NE, ϵ-Nash equilibrium relaxes the requirements
by allowing the agent to deviate if it improves its expected returns by more than some value ϵ.
∗
∗
u(Jπ−i
, πi∗ K) ≥ u(Jπ−i
, πi K) − ϵ, ∀πi

8

(14)

Correlated equilibrium (CE) is an important generalization of NE that adds the notion of correlating
strategies among agents. This considers the existence of signals that coordinate agents’ actions
Aumann [1974], as well as the introduction of a correlating distribution over the strategies of all
agents. In cases where such correlating signals do not necessarily affect the joint strategies and cause
no agents to deviate regardless of the information provided by the signals, we call the following
optimization state a coarse correlated equilibrium (CCE). As mentioned, NE is a special case of CE,
where the correlating distribution of the agents’ strategies is a product of independent distributions.
The concept of perfect equilibrium Bielefeld [1988] refines the idea of NE in a different manner —
by imposing additional requirements of consistency and mutual optimality. This solution concept
describes an optimization state where agents’ strategies are mutually consistent and take into account
the possibility of off-equilibrium actions, requiring agents to choose strategies that are robust against
such deviations. In fact, numerous additional refinements and solution concepts exist that take into
account asymmetric roles and information Başar and Olsder [1998], situations involving imperfect
information Shoham and Leyton-Brown [2008], or even games with sequential dynamics Kreps and
Wilson [1982] and non-stationary considerations Daskalakis et al. [2022], Kim et al. [2022].
Equilibrium Analysis and Computation The process of computing the optimal decisions for
multiple agents is referred to as equilibrium computation Daskalakis [2022] and has been studied
under the framework of optimization and variational inequalities Harker and Pang [1990], Kovalev
et al. [2023]. Equilibrium computation aims to find specific points of interest, known as equilibria,
within an optimization landscape that spans the strategy space of multiple agents. Formally, equilibria
represent stable or “optimal" solutions where the dynamical system reaches a balanced or steady
state. Equilibria can be described as being local or global, indicating whether the state is a locally
optimal solution or the best solution across the entire landscape. The two important properties we
must consider for equilibrium computation are their existence and tractability. The existence of an
equilibrium largely depends on the utility function of each agent, and whether it is concave with
respects to their actions, where in nonconcave games, the existence of equilibrium is at risk. The
tractability of equilibrium refers to the complexity of solving for an equilibrium and this remains a
significant challenge to be compute efficient even in nonconcave games.
Equilibrium Complexity Even with the existences of an equilibrium solution Nash [1951], the
complexity of computing this equilibrium must be discussed. This discussion is most aptly had under
the pretense of a total search problem (i.e. PPAD) rather than as a decision problem (i.e. P/NP).
Although, computing an equilibrium can be classified an NP-hard decision problem Gilboa and Zemel
[1989], Conitzer and Sandholm [2008], Nisan et al. [2007], Albrecht et al. [2024].
Definition 6 (PPAD) Polynomial parity argument for directed graphs (PPAD) consists of problems
that can be reduced to the following (End-Of-The-Line) problem:
Given a directed graph with vertices have at most one predecessor and/or one successor, and a source
vertex s, where s has no predecessor, find a vertex t with no predecessor or no successor, such that
s ̸= t. Let there be a polynomial-time function that returns the predecessor and successor of all
vertices in this graph.
where PPAD-completeness is shown by reducing the End-Of-The-Line problem. To support the
hardness of PPAD problems, Daskalakis et al. [2009] proposes the following question: How can one
hope to devise an efficient algorithm that telescopes exponentially long paths in every implicitly given
graph?
Computing the Nash equilibrium is proven to be a PPAD-complete Daskalakis et al. [2009], Chen
et al. [2007], further affirming its difficulty and potential intractability.
Learning Dynamics While these solution concepts of equilibrium give context for stability and
optimality, they do not provide insights into the process of reaching such states. The concept
of learning dynamics bridges this disconnect by detailing the procedure for reaching equilibrium.
Additionally, learning dynamics also helps analyze the transition into equilibrium itself, which can be
equally or more important to gain insights into task-specific understandings, and the processes that
come after achieving equilibrium, where within that steady state, whether continual lifelong learning
and further adaptations may come into the equation. In general, there exist two fundamental learning
9

dynamics within the process of equilibrium computation: best-response dynamics and no-regret
dynamics.
Best-response dynamics directly optimize to converge to an NE or one of its refinements. A seminal
algorithm of best-response learning dynamics is fictitious play (FP), where all agents iteratively
compute the best response to other agents’ strategies, or most specifically, the uniform distribution
over the past strategies of the other agents Robinson [1951].
We summarize the seminal realizations and recent advancements in best-response dynamics learning:
• There are improvements to the idea of FP that include improved robustness to perturbations
in the form of stochastic FP Fudenberg and Kreps [1993] as well as uncertainty within
these models of other agents and update these beliefs through Bayesian updating, known
as rational and Bayesian learning Albrecht et al. [2024], Jordan [1991], Kalai and Lehrer
[1993]. Specifically, Bayesian learning utilizes the value of information Chalkiadakis and
Boutilier [2003], which considers how actions will influence future behaviors and devise
more accurate beliefs.
• FP algorithms are typically used in normal-form games but can be realized in extension-form
games using behavioral strategies, known as extensive-form fictitious play (XFP), or through
approximate best response and sample-based learning, i.e. ML approach to XFP, known as
Fictitious Self-Play Heinrich et al. [2015].
• Double oracle approach adopts the same iterative procedure as fictitious play, but instead,
agents now compute a meta-NE to a restricted game that is maintained and expanded by the
past best-response strategies McMahan et al. [2003], Adam et al. [2021].
• Policy Space Response Oracles, Deep Cognitive Hierarchy Lanctot et al. [2017], and
Extensive-Form Double Oracle McAleer et al. [2021] incorporate and build upon the double
oracle algorithm with the use of ML techniques, and address computational issues that exist
when extending double oracle to extensive form games, promote generalization and prevent
overfitting to specific equilibrium Bighashdel et al. [2024], Lanctot et al. [2017].
• Value iteration Shapley [1953] can be applied over the joint-action space, resulting in
solutions such as Minimax Q-learning Littman [1994], Nash Q-Learning Hu and Wellman
[2003], Correlated Q-learning Greenwald and Hall [2003], and Friend-or-Foe Q-learning
Littman [2001b].
• Replicator dynamics Maynard Smith [1976] adapts the concept of evolutionary dynamics to
achieve best-response policies Tuyls et al. [2006].
• Infinitesimal gradient ascent (IGA) Singh et al. [2000] utilizes gradient learning to optimize
agent’s policy with respect to their utility. Similar methods and their extensions to improve
convergence and other theoretical properties include using variable step sizes Bowling
and Veloso [2002], Bowling [2004], proximal point optimization Mokhtari et al. [2019],
momentum Gidel et al. [2019], extra-gradientKorpelevich [1976], and optimistic gradients
Rakhlin and Sridharan [2013], Daskalakis and Panageas [2018], Wei et al. [2021].
In no-regret learning dynamics, the aim is instead to minimize regret, a measure of how much an
agent would have gained in utility if they had chosen a different strategy often in retrospect Hart and
Mas-Colell [2000]. Intuitively, regret can be quantified by the average cost between the utility of
the best possible strategy profile and the actual utility of the chosen strategy profile. This notion of
regret is called external regret, where comparisons of decisions are performed using an expert offline
strategy. On the other hand, internal, or swap, regret takes a more online approach, compared to a
modified strategy that swaps out certain actions with others from the original strategy. There exists a
well-known connection between no-regret and Nash equilibrium Zinkevich et al. [2007], where an
ϵ-Nash equilibrium is a profile that achieves an upper bound regret of ≤ ϵ. Usually, the tractability
of computing and verifying best-response can be compromised with games with high complexity,
and therefore no-regret dynamics approaches are an attractive alternative as they scale very well
when using domain-specific abstractions, such as in Poker AI applications Zinkevich et al. [2007],
Tammelin [2014]. We summarize the seminal realizations and recent advancements in no-regret
dynamics learning:
• Regret matching serves as the foundation for regret minimization algorithms, and is achieved
through repeated self-play that computes the strategy iterates based on a distribution of
10

normalized positive regret Blackwell [1956], Hart and Mas-Colell [2000]. Convergence to a
stable solution, i.e. sample equilibrium, is achieved by taking the average overall strategy
iterates.
• Counterfactual regret minimization (CFR) extends regret matching to extensive form games
with counterfactual regret, which accounts for the sequential nature of actions in extensive
form games Zinkevich et al. [2007]. To ensure sufficient coverage over the game tree for CFR
updates, a sampling approach using external or chance sampling Lanctot [2013] or a more
extensive search, like in CFR+ Tammelin [2014], Bowling et al. [2015], must be considered.
Recent advancements, such as CFR+ and advantage-based regret minimization Jin et al.
[2018b], have demonstrated performance improvements with resetting and positive clipping
negative cumulative regret to zero, which can be a form of “optimism under uncertainty".
• Follow-the-Leader (FTL) is another popular regret-minimizing technique studied more in
online learning, and FTL constructs online strategies to follow the actions with minimal
loss over past rounds, i.e. the best “expert" Shalev-Shwartz et al. [2012]. However, a
naive implementation of FTL is unstable, where a natural solution to this instability is
to append a time-varying regularization term, known as Follow-the-Regularized-Leader
(FoReL). Importantly, the choice of regularization term leads to different regret bounds.
Another method to address this instability and avoid overfitting from FoReL is to use the
subgradient method with the proximal term as the Bregman divergence, known as mirror
descent Orabona [2023]. For both FoReL and mirror descent methods, continued efforts to
seek improvements are made to the regret bounds, with techniques as simple as gradient
clipping Cutkosky [2019].
• Multiplicative weights update/hedge algorithm generalizes FTL, as it now maintains weights
that are updated with a non-infinite learning rate rather than selecting the single “expert"
Freund and Schapire [1999], Arora et al. [2012]. The learning rate can be adaptive to ensure
consistent low regret on easy and hard instances with the doubling trick or using both FTL
and hedge periodically De Rooij et al. [2014].
• IGA and its extensions achieve both best-response and no-regret under certain conditions
Zinkevich [2003], and with the advent of deep learning in MAS applications, gradient
methods have become quite popular.
Connection to MARL The literature on GT is extensive, with ongoing efforts investigating diverse
challenges in strategic interactions among multiple agents. However, the significance of MARL
lies in the integration of data-driven considerations into these strategic decision-making processes.
So, unlike traditional GT, MARL leverages insights from data and statistics to manage complex
markets where agents’ behaviors can be shaped by data-driven models. This approach enables MARL
to effectively address dynamic, uncertain, and large-scale scenarios that traditional GT often finds
challenging to handle.
3.4

Machine Learning

The field of ML represents a crucial domain within AI, with the objective of constructing data-driven
solutions that excel in pattern recognition using statistical models James et al. [2013]. Central to
ML are two primary tasks: data collection and data analysis. Data collection defines the process of
gathering and managing data which constructs the target data distribution, whereas the task of data
analysis encompasses the methods of statistical inference using this data distribution. Hence, the data
plays a pivotal role in the ML process, defining the underlying distribution on which ML models rely
their statistics upon.
This significance has become glaringly apparent in real-world ML applications, where the integrity of
data is often tested by strategic agents aiming to manipulate statistics or exploit the predictive models
themselves Zrnic et al. [2022]. Unfortunately, this multi-agent dynamic is canonically neglected in
ML literature but has more recently gained traction in the emerging fields of strategic classification
and adversarial ML, which develop solutions regarding the cycle between the development and
deployment of ML models and the post-hoc response of strategic agents influencing the model, which
can potentially adversarially “attack" the model and “pollute" the data. Another popular stream of
research that blends GT into ML is in generative modeling, notably with the training of generativeadversarial networks (GAN). GANs employ a min-max optimization scheme to train an generative
11

model with an discriminator model that classifies between true and generated data. However, an
naive implementation without proper GT considerations leads to chaotic and oscillatory learning
Daskalakis et al. [2018]. In essence, the efficacy of real-world ML hinges on robust data practices
and the ability to navigate the complex landscape of strategic interactions in diverse scenarios.
3.4.1

Deep Learning

Deep learning is a popular and general approach in contemporary AI research, with its foundations
relying on the use of artificial neural networks (ANNs). ANNs have showcased remarkable proficiency
in a wide variety of general applications Goodfellow et al. [2016].
Concretely, ANNs define a set of parameters θ that act as a function that processes and transforms
data using layers of linear and non-linear operations. We optimize these parameters with respect to a
defined cost function J(θ) using methods of statistical learning and numerical optimization, such as
stochastic gradient descent (SGD) James et al. [2013].
θ = θ − ∇θ J(θ)

(15)

A key attribute of a deep learning approach is its ability to develop a scalable end-to-end solution that
can capture complex distributions underlying many real-world applications. Additionally, we avoid
the need to handcraft features and instead allow the optimization to learn curated latent representation
tailored toward the task at hand in a data-driven manner. Hence, the emphasis is on extracting
patterns, relationships, and insights directly from data, rather than relying heavily on predefined rules
or models, although such efforts are not necessarily orthogonal.
In the context of MARL, integrating these deep learning methods into MARL solutions has shown
promise for developing expressive and adaptive multi-agent systems for complex tasks, where agent’s
behaviors are defined using a composite of ANN representations and optimized in an end-to-end
manner. For example, we can represent the elements of a stochastic game, such as the policy, value
function, or model, using ANNs to optimize the joint behavior. The central constraints in such
methods reside with the need for large-scale data collection and the high computational cost required
for training.
3.4.2

Reinforcement Learning

The domain of reinforcement learning (RL) focuses on learning how to make decisions guided by
reward signals Sutton and Barto [2018] and serves as the general foundation of learning controls. A
caveat of traditional RL research is that it is normally studied under a single-agent setting, i.e. using
the MDP framework.
Predictions and Control We define the process of sequential decision-making as control. Control
contrasts the concept of predictions, as predictions are static inferences that have no causal relationship
to future inputs and predictions. Formally, predictions are independent and identically distributed
(IID), whereas no such assumptions are made for control. Within RL frameworks, control is learned
within an environment whose dynamics the agent has limited knowledge of Recht [2019], however
predictions still play a major role in this capability, whether it is incorporated implicitly or explicitly,
to ensure that the agent has an understanding of the world it resides.
Value Function Approximation A value-based approach for RL estimates the value function
to approximate the optimal policy. Hence, value-based solutions often do not define an explicit
policy and instead, use an implicit policy computed using the value function, π(a|s) ∝ Q(s, a).
These Q-learning methods Watkins [1989], Watkins and Dayan [1992] utilize approximate dynamic
programming by applying the concept of policy iterations, as shown in Figure 3, with the Bellman
(expectation) equations Bellman [1957], a set of recursive definitions of the value functions.
Qπ (st , at ) =Eτ ∼pπ (τ |st ,at ) [Gt (τ )]
=r(st , at )+
Est+1 ∼T (st ,at ,st+1 ),at+1 ∼π(a|st+1 ) [Qπ (st+1 , at+1 )]

(16)

We define the Bellman backup operator Bπ on our value function estimate Q such that Q = Bπ Q is
equivalent to Equation 16. By iteratively applying the Bellman backup operator Bπ , Bπ is shown to
12

Figure 3: Policy Iteration: The process of policy iteration consists of an iterative cycle of policy
E
I
evaluation (shown as −
→) and policy improvement (shown as →
− ). Policy evaluation computes the
value function for current policy whereas policy improvement updates current policy with respect to
evaluated value function. The following figure was taken and modified from Sutton and Barto [2018].
be a contraction mapping, thereby according to the Banach fixed-point theorem, Q is guaranteed to
converge to a unique fixed point that corresponds to the true Q-function Qπ for the policy π Sutton
and Barto [2018].
By employing an optimal (implicit) policy denoted as π ∗ (a|s) = δ(argmaxa∈A Q(s, a)), the need
for policy evaluation, a process that scales poorly with the state-action space, can be circumvented.
This approach, known as value iteration, leverages the Bellman optimality equation in the form of the
Bellman backup Bπ to minimize the Bellman residual error.
Qπ∗ (st , ut ) = r(st , ut ) + Est+1 ∼T (st ,ut ,st+1 ) [max Qπ∗ (st+1 , a)]
a∈A

(17)

Value function approximation methods have come with several improvements over the past decade,
including:
• Extension to deep neural networks representation for continuous state space and usage of
experience replay to stabilize off-policy learning Mnih et al. [2013].
• Distributional value function representation Bellemare et al. [2017], Dabney et al. [2018b,a],
Yang et al. [2019] for more expressive value approximations.
• Mitigate overoptimism/maximization bias with double learning Van Hasselt et al. [2016],
clipped double Q-learning, and target policy smoothing Fujimoto et al. [2018b].
• Utilization of a dueling architecture Wang et al. [2016b] to help figure out which states are
valuable without having to learn the effect of each action for each state.
• Improvements to experience replay to prioritize important transitions Schaul et al. [2015], to
perform better with sparse rewards Andrychowicz et al. [2017], and to work with recurrent
networks Hausknecht and Stone [2015], Kapturowski et al. [2018].
• Improved exploration through randomized modeling Fortunato et al. [2017], intrinsic rewards
Badia et al. [2020b,a], maximum entropy framework Haarnoja et al. [2017].
• Handle continuous action spaces by assuming a deterministic policy Lillicrap et al. [2015],
parameterizing the Q-function as a well-defined convex function Gu et al. [2016], or utilizing
sampling methods Kalashnikov et al. [2018] in order to compute the Bellman update.
• Learn on datasets of offline experiences while managing distributional mismatch and continuing improvements beyond behaviors seen in dataset Levine et al. [2020] by appending a
behavior regularization term to the standard RL training Fujimoto and Gu [2021], regularizing overestimation with conservatism Kumar et al. [2020], utilizing probabilistic regression
Kostrikov et al. [2021], Ma et al. [2021], and a model estimate Yu et al. [2022b]. This
application is known as batch or offline RL.
• Integration of the listed improvements through the years Hessel et al. [2017], Kapturowski
et al. [2022].
Value-based approaches are commonly used in MARL algorithms, as they offer a powerful and
expressive solution to learning and producing a control policy in an off-policy approach, thereby
being sample-efficient.
13

Policy Gradient A policy-based approach of RL directly optimizes the objective stated in Equation
10. A common approach is to perform gradient ascent along the objective using the policy gradient,
pioneered by the REINFORCE (REward Increment = Nonnegative Factor × Offset Reinforcement
× Characteristic Eligibility) algorithm Williams [1988, 1992].
∇θπ J(π) = ∇θπ Eτ ∼pπ (τ ) [G0 (τ ) − b(τ )]
= Eτ ∼pπ (τ ) [

T
X
(Gt (τ ) − b(τ ))∇θπ log(π(at |st ))]

(18)

t=0

where θπ represents the parameters of the policy π and b(·) is the reinforcement baseline function
used to stabilize the approximated reinforcement Gt (τ )1 . As the expectation is computed through
Monte Carlo sampling, it can often lead to high variance despite providing an unbiased estimate of
the expected return. Hence, as noted by Williams [1992], Schulman et al. [2018], the choice of b(·)
can improve convergence and performance by mitigating this variance.
Notable improvements to this vanilla policy gradient consist of:
• Learn on data from various policies, i.e. an off-policy approach, with policy gradient
methods by using importance sampling corrections Jie and Abbeel [2010], Degris et al.
[2012] that enables training on a more diverse dataset in a sample-efficient manner.
• Mitigate dominating gradients on the parameter level by reparameterizing loss under a
probabilistic manifold using Fisher information matrix Kakade [2001] with natural policy
gradient, or Kronecker-factored approximated curvature Wu et al. [2017].
• Utilize surrogate objective that guarantees monotonic improvements, conditioned on a
trust-region optimization Kakade and Langford [2002], Schulman et al. [2017a,b].
Unlike value-based approaches, policy-based approaches are not commonly practiced in MARL,
largely due to the sample inefficiencies from its on-policy requirements and high variance. However,
in cases where samples can be generated cheaply, policy-based approaches can perform very well.
Actor-Critic Methods Actor-critic algorithms provide an integration of policy-based and valuebased methods Sutton et al. [1999a]. Instead of estimating the value function of an implicit policy, we
model the value function for an explicitly defined policy thereby minimizing Bellman residual using
Equation 16 and learning from a policy gradient. With access to a value estimate, temporal difference
(TD) learning or n-step learning can be utilized instead of Monte Carlo sampling to approximate the
offset reinforcement. While these bootstrap methods do introduce bias, the variance is considerately
lowered as updates are no longer dependent on the entire trajectories but only on a subset of the
trajectory Sutton and Barto [2018]. Additionally, the value function can also improve the quality of
the policy gradient by stabilizing the return estimate with a new baseline Schulman et al. [2018].
Much of the improvements to the actor-critic methods follow the advancements from value-based
and policy-based approaches, and also consist of:
• The use of generalized baseline functions Schulman et al. [2018] and action-dependent
control variate Gu et al. [2017], Liu et al. [2018], Tucker et al. [2018] to stabilize the variance
of policy gradient.
• Distributed training paradigms for actor-critic methods through asynchronous learning
between multiple parallel actors Mnih et al. [2016], Espeholt et al. [2018].
• Stabilized learning of solution with shared parameters between actor and critic by using
phasic learning and representation regularization Cobbe et al. [2020], Huh [2021].
• Extending efforts from policy-based and value-based methods to an actor-critic framework,
such as experience replay Wang et al. [2016a] and soft Q-learning Haarnoja et al. [2018,
2019].
1

We highlight that the return Gt (τ ) here removed the notation of which agent it is referring to, as we are
only considering one agent in this scenario.

14

Challenges
Computational complexity
Non-stationarity
Coordination
Performance Evaluation

Pathologies
Stochasticity, Deception
Moving-target problem
Miscoordination, Relative Overgeneralization, Alter-Exploration Problem
n/a

Table 1: A table of MARL challenges and their learning pathologies.
Model-based Approaches With a model-based approach, an explicit estimate of the transition
dynamics T and/or reward probabilities is maintained. These estimates can be used for planning
without an explicit policy or for a more sample-efficient policy improvement Sutton [1990]. Methods
that do not maintain such estimates are referred to as a model-free approach. While model-based
approaches have shown success in highly complex tasks, these methods can be more challenging as
accurately and comprehensively representing the transition dynamics and reward function, especially
over long time horizons, is non-trivial and difficult.
There have been notable improvements in model-based approaches, which include:
• Integrate model learning and policy optimization with deep learning methods Ha and
Schmidhuber [2018], Weber et al. [2018], Hafner et al. [2020, 2022, 2023].
• Mitigate distributional mismatch using online data collection Ross et al. [2011], uncertainty
estimation Deisenroth and Rasmussen [2011], Feinberg et al. [2018] via Bayesian neural
network Blundell et al. [2015], bootstrap ensembling Chua et al. [2018], Kurutach et al.
[2018], Clavera et al. [2018], Luo et al. [2021], Buckman et al. [2018], or dropout Gal et al.
[2017].
• Model-based learning on imperfect information Watter et al. [2015], Zhang et al. [2019].
Successor Representation Successor representation (SR) is an alternative approach to the modelbased and model-free approach, where it disentangles the state transitions from reward estimation by
maintaining a state occupancy function M (·) for a given policy π Dayan [1993].
Mπ (s, s′ ) = Eτ ∼pπ (τ |s0 =s) [

T
X

γ t 1(st = s′ )]

(19)

t=0

where 1 is the characteristic function. The state occupancy function captures the notion of environmental affordance, caching statistics relating to which future states are possible from a given
state. Similar to the Bellman equations, we can derive a recursive definition for the state occupancy
function.
Mπ (s, s′ ) = 1(st = s′ ) + γEst+1 ∼pπ (τ |st =s) [M (st+1 , s′ )]
This factorization provides a compact and structured representation of the model for efficient and
adaptive learning in complex environments Kulkarni et al. [2016]. Generally, SR methods alleviate
the cost of learning a complex environment model while still being adaptive to distal reward changes,
given the disentanglement of the reward function from the state transition.
Recently, SR has reemerged with a new utility in RL that can help capture different aspects of the
RL problem, despite the initial idea proposed several decades ago. Notably, a grounded formulation
to extend SR with deep learning methods Blier et al. [2021], Touati and Ollivier [2021] and the
utilization of passive data to learn latent and useful features with SR Ghosh et al. [2023] are some
recent applications that have captivated some new interest in this technique.
Foundation Model for Control Foundation models are ML models that are pre-trained on largescale data to be adapted for diverse downstream tasks and have largely been successful and practiced
in recent ML research Bommasani et al. [2022]. This sentiment for foundation models is echoed in RL
applications, where leveraging pre-trained models can also offer advantages, especially considering
RL’s training inefficiencies. However, questions regarding its effective implementation remain at
large.
The recent efforts of developing foundational models for RL has been in general navigation Shah
et al. [2023a,b], Baker et al. [2022], robotic manipulation Walke et al. [2023] and other broad robotic
15

applications Collaboration et al. [2023]. These works have largely been motivated by practices in
large-language model training methodologies and offline RL Chebotar et al. [2023] and creating
multi-modal prompting to integrate successes from the research of computer vision and natural
language. The key insight with foundation models is recognizing that across tasks, there are shared
traits and skills that are required in many of these tasks that do not necessarily require re-learning for
each task. Instead, learning these aspects in a unified manner rather than individually and myopically
within a single task can lead to not only an amortized cost of learning but also more robust and
superior behaviors.

4

Learning in a Multi-agent Environment

The transition from single-agent RL to learning within a multi-agent stochastic game setting promises
numerous opportunities but consequently is tied to challenging difficulties that require paradigmaltering considerations Resnick [2005]. In contrast to single-agent control systems, where one agent
interacts with its environment, a multi-agent setting involves managing the decision-making and
learning processes of multiple entities that can interact with each other and their shared environment.
Facilitating stable, adaptive, and social behaviors within a multi-agent control learning process is
non-trivial, and comes with several intricacies that converge to necessitate desired solutions. In this
section, we cover the positive and negative aspects that are unique to MARL, largely highlighting
its potential and consequences in the hopes of providing meaningful context and a deeper look into
these concepts.
4.1

Benefits of MARL

MARL directly addresses the optimization problem of developing multiple decision-making agents
within a shared environment. As a result, these algorithms are specifically designed to consider
the complexities inherent to multi-agent dynamics during training. Therefore, the optimization can
produce valuable behaviors that span from adaptive social learning Ndousse et al. [2021] to the
emergence of coalitions that enable intelligent interactions and improve robustness against dynamic
changes within the population Busoniu et al. [2008].
This perspective of approaching control optimization that MARL studies also enables the realization
of applications that could not be accurately modeled in single-agent RL alone Matarić [1994],
particularly in scenarios that involve multiple adaptive agents that can influence the environment or
other agents Lanctot et al. [2017]. The context of training under MAS features additionally introduces
the chance to capitalize on the unique prospects and structures of the multi-agent nature, which can
enhance the efficiency and depth of training through various means discussed in the later parts of this
survey.
4.2

Challenges of MARL

The task of MARL is not only faced with the already challenging objective of optimizing control
but is further riddled with the inclusion of multiple interacting agents. Throughout this paper, we
investigate four central challenges — computational complexity, non-stationarity, coordination, and
performance evaluation — that are interdependent and coupled to various learning pathologies.
Definition 7 (Learning Pathology) Learning pathology refers to undesirable or sub-optimal behaviors that can emerge within MARL learning dynamics. As listed in Palmer [2020], these include
stochasticity, deception, the moving-target problem, miscoordination, relative overgeneralization,
and the alter-exploration problem.
Much of the problematic intricacies of MARL stem from a mixture of these challenges, as they tend
to be deeply interconnected.
4.2.1

Computational Complexity

As mentioned in Section 3.3, optimizing for an equilibrium solution is theoretically demonstrated as
challenging, where the cost of modeling multiple agents can easily scale intractably. This is seen
as the joint state-action space scales exponentially with respect to the number of agents Qu et al.
16

[2021], the complexity of the optimization problem and the computational cost required to render the
optimization impractical to learn any useful joint behaviors. Other key factors, such as the pathologies
of stochasticity and deception, play a major role in contributing to this complexity.
Stochasticity In many real-world MAS applications, stochastic rewards or transitions pervade,
arising from factors such as noise or unobservable elements in the state space. When a game is
inherently stochastic, the challenge of identifying the sources responsible for the stochasticity can
lead to learning instabilities. This, in turn, requires agents to accumulate more experience to discern
and adapt to these sources of uncertainty Palmer [2020].
Deception The pathology of deception involves a game that contains certain states that have a
high local reward but low return. In other words, there exist settings where a trade-off should be
made from immediate reward for long-term success. For many MARL algorithms, especially those
derived from maximum-based learners Kapetanakis and Kudenko [2003], Matignon et al. [2012],
over-estimation is a key challenge that leads to early convergence to sub-optimal equilibrium.
In general, to address the challenge of computational complexity in MARL applications, it becomes
even more crucial to design our optimization model carefully such that the modeled components are
not susceptible to fail to converge or to find good approximate equilibrium states during learning. With
these considerations, we need to balance the trade-offs between solution complexity, computational
efficiency, and learning performance to effectively devise a tractable MARL solution.
4.2.2

Non-stationarity

Informally, a stationary process can be classified by its underlying distribution’s invariance under
time shifts. In a multi-agent setting, the decisions of each agent impact the transition dynamics of
the environment. Consequently, from the perspective of each agent, the other agents’ behaviors are
inherent components of the dynamics. Therefore, whenever these external agents adapt and alter their
behaviors, the underlying model distribution in the perspective of the agent also changes, rendering
it non-stationary Hernandez-Leal et al. [2017]. Importantly, this non-stationarity arises not from a
stochastic process that can be easily approximated, such as white-noise Gaussian, but rather from the
structured learning process of the external agents Daskalakis [2022].
This non-stationarity is a critical deviation from the fundamental assumption made by conventional
single-agent RL algorithms Choi et al. [1999]. The absence of the stationary property destabilizes the
optimization process and contributes to the pathology of the moving-target problem. This problem
arises from the fact that what an agent has learned and needs to learn is dependent on other agents’
evolving behaviors Tuyls and Weiss [2012]. Hence, the learning landscape the agents are optimizing
over is in flux at each update step. This setting can make learning infeasible for agents to properly
converge to a stable behavior Papoudakis et al. [2019]. Therefore, it is essential to consider extensions
of methods that can effectively account for this non-stationarity to develop stable algorithms for
MARL.
4.2.3

Coordination

One of the unique aspects of developing a multi-agent solution is the ability of the agents to work
together to achieve their goals. As each agent makes decisions based on its local observations in a
shared environment, they can heavily benefit from coordinating their actions to achieve a joint strategy
that maximizes the collective return and avoids unintended interference to mitigate diminishing returns
Cai et al. [2007].
However, achieving successful coordination is difficult, notably when agents have limited information
about the environment and the behaviors of other agents. Addressing this challenge of coordination
has been a significant research question in MARL that studies how agents can engage with one
another in various manners depending on their settings while also effectively succeeding in their
local tasks. Specifically, these efforts towards successful coordination are directly associated with the
learning pathologies of miscoordination and relative overgeneralization.
Miscoordination The pathology of miscoordination is also known as the Pareto-selection problem
and can be observed when two or more incompatible Pareto-optimal equilibria are present. As a
consequence, the agents can potentially choose an action from different equilibria due to improper
17

a
b
c
a
1
−2
0
b −2
1
0
0
0
0.5
c
Table 2: Payoff Matrix of a Climbing Game: Motivated by the climbing game proposed in Kapetanakis
and Kudenko [2003], this game consists of two agents, each with three possible actions (a, b, c).

coordination and therefore harming their performance. For example, given a bi-matrix game defined
in Table 2, there exists two equilibria, (a, a), (b, b), where either both agents choose action a or b.
However, these equilibria are incompatible, since the strategy profiles (a, b) and (b, a) come from
either equilibria, however, if they are intermixed, neither resulting strategies are optimal.
′

Definition 8 (Incompatiable Equilibria) Two equilibria π and π are incompatible if and only if,
′

′

′

∃i πi ̸= πi , Ji ([πi , π−i ]) < Ji (π )
′

where [πi , π−i ] signifies a strategy profile using agent i action from π and the other agent’s action
′
from π .
Relative Overgeneralization The pathology of relative overgeneralization occurs in games where,
as a result of a shadowed equilibrium, the agents converge upon a sub-optimal Nash Equilibrium that
is Pareto-dominated by at least one other Nash Equilibrium.
′

Definition 9 (Shadowed Equilibrium) An equilibrium π is shadowed by another one π if there
exists an agent that receives a low return by unilaterally deviating from this equilibrium and if this
return is lower than the minimal return when deviating from the other equilibrium Palmer [2020].
′

∃i ∃π̄ J([π̄i , π−i ]) < min J([π̄j , π−j ])
j

For instance, in the bi-matrix game shown in Table 2, while (a, a), (b, b) are both Pareto-optimal
equilibria, a miscoordination penalty of −2 is associated with both of them. No such penalty exists
with action c. Hence, both equilibria are shadowed by (c, c), as the expected gain if one agent deviates
unilaterally from either equilibrium is inferior to the lowest expected gain if one agent deviates
unilaterally from (c, c). Hence, agents can be drawn to sub-optimal but wide peaks in the return space
due to a greater likelihood of achieving beneficial collaboration. This derives as a form of action
shadowing.

Figure 4: Visualization of relative overgeneralization in a two-player game from Wei and Luke
[2016].
18

Definition 10 (Action Shadowing) Action shadowing is a phenomenon wherein one individual
action seems more favorable than another, despite the potential greater return of the second action
Fulda and Ventura [2007]. Concretely, a sub-optimal policy may result in a higher average payoff
when paired with arbitrary actions chosen by other agents, leading to utility values of optimal actions
being underestimated.
The pathology of relative overgeneralization is visualized in Figure 4 over a continuous action space.
The x and y axes represent the actions of agents i and j respectively. The z axis represents the reward
for each joint action. The reward space is structured where action iM can lead to the optimal reward,
however, due to miscoordination being less severely punished for actions approaching iN , the agents
are drawn towards the sub-optimal Nash equilibrium.
Exploration The challenge of exploration in MARL describes the issue of how to effectively
explore unknown environments to collect valuable experiences that benefit the agents’ learning
the most Sutton and Barto [2018]. To address this challenge, a balance between exploration and
exploitation must be struck, where agents must decide whether it is more valuable to take the actions
that they know would lead to good returns or take the actions they have not tried yet that may lead to
even greater returns or at the very least reduce the uncertainty regarding those actions. An improper
balance on either side can result in an incomplete coverage over the state-action space, leading to
sub-optimal convergence. This challenge is especially exacerbated in intricate environments with
sparse and delayed reward signals, noisy transitions, long horizons, and non-stationary dynamics Hao
et al. [2023]. In addition, in light of the inherent delicacy involved in optimizing and coordinating
multi-agent systems under the influence of shadowed equilibria and miscoordination penalties, such
exploration can increase the likelihood of deceptive transitions and introduce instabilities within the
learning dynamics Matignon et al. [2012]. For instance, other agents may adapt to these exploratory
actions too abruptly even though they may lead to a shadowing equilibrium. We call this pathology
the alter-exploration problem. Another key issue noted by prior efforts is the lazy agent problem Liu
et al. [2023] when certain agents learn a good policy but some agents have less incentive to continue
to explore and learn themselves, as their actions may negatively affect the already high-performing
agents. For example, as discussed in Sunehag et al. [2017], consider the scenario of training a soccer
team with the number of goals as the team’s reward signal. If certain players are more proficient
scorers than others, it becomes evident that when the less skilled player takes a shot, the outcome is
less favorable on average. Consequently, the weaker player learns to avoid taking shots Hausknecht
[2016].
Traditionally, simple exploration methods, such as ϵ-greedy Sutton and Barto [2018] or noise
perturbation Fujimoto et al. [2018a], can be employed for random action selection, however, such
naive methods can lead to unintentional and indiscriminate exploration which can be inefficient in
complex learning tasks with exploration challenges. While exploration remains an open challenge
with much room for improvement, there exists more studied and developed exploration methods, as
follows:
• Uncertainty-oriented Exploration: With a lack of knowledge regarding certain actions,
agents can incorporate this uncertainty into the decision-making process when tackling
this balance between exploration and exploitation. A common heuristic to employ is the
principle of ”Optimism in the Face of Uncertainty", where agents are incentivized to explore
state-action pairs with high epistemic uncertainty. Epistemic uncertainty represents the
errors that arise from insufficient and inaccurate knowledge about the environment whereas
aleatoric uncertainty represents the inherent randomness of the environment. Typically,
this approach requires some modeling of these uncertainties. A common approach is to
parameterize the solution as a distribution Zhu et al. [2020], Sun et al. [2021], Zhao et al.
[2022] to be able to properly express the stochasticity of the environment and leveraging a
classic exploration technique utilizing these estimates.
• Intrinsic Motivation-oriented Exploration: An alternative approach is to incorporate a
meta-task of exploration by introducing and designing intrinsic rewards for agents. These
rewards can be to minimize prediction errors regards the environment Zheng et al. [2021],
motivated by novelty of states Mahajan et al. [2020], Iqbal and Sha [2021], Liu et al. [2021]
or driven by information gain Houthooft et al. [2017]. For instance, in Du et al. [2019], the
individual intrinsic reward is learned and used to update an agent’s policy to maximize the
team reward.
19

• Multi-agent Exploration: In a multi-agent setting, we face the challenge of not only
complexity but also miscoordination (i.e. alter-exploration problem). This issue requires
some level of coordinated exploration Hao et al. [2023], as exploratory actions of one
agent can affect the learning of others Palmer et al. [2018], and in certain multi-agent
tasks, efficient exploration requires a degree of global planning as opposed to pure local
exploration Brafman and Tennenholtz [2003].
4.2.4

Performance Evaluation

Assessing the performance of a MARL algorithm is a multifaceted challenge. Firstly, we highlight
that the success of one agent’s policy is intricately linked to the policies of other agents, which
renders individual assessments unclear to properly interpret. This is compounded by the problem of
establishing unbiased and useful metrics that quantify inherently qualitative social behaviors, such as
measuring the quality of communication and coordination Havrylov and Titov [2017], Jaques et al.
[2019], Bogin et al. [2019], Lowe et al. [2019] and determining the appropriate evaluations dependent
on the roles assigned to all agents (i.e. oligopoly with leader-follower structure).
Furthermore, we can focus on more egalitarian criteria, ie. concentrating on the agents that are
most struggling Zhang and Shah [2014] as opposed to an more utilitarian approach such as social
welfare. Hence, it also becomes important to understand multi-agent credit assignment and discern
the individual impact of each agent in terms of the coalition’s utility, especially where there exists
only a global reward structure, e.g. within a Dec-MDP. A classic example is the Shapley value, which
quantifies and captures the notion of marginal contribution by averaging all possible combinations of
the marginalized population’s achieved utility Shapley [1952]. More recently, a popular approach
in deep MARL is to utilize an advantage function, which compares the current Q-values to a
counterfactual Foerster et al. [2017a], Li et al. [2022], or to utilize value function decomposition
Sunehag et al. [2017] that marginalizes the contribution of each agent.
From a theoretical perspective, determining which solution concepts would lead to optimal behaviors
and strategies, as studied in game theory literature, remains unclear and often task-specific Bergerson
[2021]. Even so, as many studied solution concepts prove prohibitively costly to explicitly measure
and optimize with complex and dynamic tasks, the research for definable metrics that capture the
nuances of diverse MAS settings persists as an ongoing and open challenge.

5

Prospects of MARL

In this section, we cover the unique properties of learning controls in a multi-agent environment that
can help promote the benefits or address the underlying issues of the MARL approach 4.
5.1

Simulating MARL Tasks

A lingering concern for MARL algorithms is its learning complexity Daskalakis et al. [2022]. Often,
in order to acquire valuable behaviors, it requires significant computational resources. This constraint
arises from various factors, namely sample inefficiency and a brittle optimization landscape Gu et al.
[2017], van Hasselt et al. [2018], Tucker et al. [2018], Henderson et al. [2019].
Sample efficiency is broken down into two factors: the number of environment interactions required
by each agent to learn and generalize and the cost associated with each interaction Huh and Mohapatra
[2024a]. Hence, it is important to emphasize the pivotal role that simulators play in this equation, as
they can enhance the learning process by yielding higher-quality samples through greater accessibility,
stability, accuracy, and precision of the retrieved data.
Parallelized and Vectorized Processing Over the past several decades, the processing capabilities
of massively parallelizable processors, such as graphic processing units (GPUs) and tensor processing
units (TPUs), have advanced significantly. This progress has opened up exciting possibilities for
leveraging this technology to simulate highly complex tasks effectively, minimizing the cost of
simulating a large number of trajectories. These processors are specifically designed to handle parallel
computations efficiently by executing tasks simultaneously on a large scale, enabling simulations
to run orders of magnitude faster compared to conventional CPU-based implementations. There
have been notable efforts to develop task-specific Dalton et al. [2019] and general physics-based
20

Figure 5: A general diagram of a holonic/multilevel simulation taken from Tchappi et al. [2018].
simulators, such as IsaacSims Makoviychuk et al. [2021], Brax Freeman et al. [2021], and MuJoCo
XLA that leverage GPUs. Many of these simulators primarily concentrate on addressing single-agent
problems, with some addressing their extension to MAS Chen et al. [2022], Gu et al. [2023], Huh
and Mohapatra [2024a], Rutherford et al. [2023]. In turn, there exists promising potential to further
optimize and extend the usage of vectorized processing to MAS scenarios Lan et al. [2021], Lechner
et al. [2023].
Multilevel Simulation Many complex multi-agent tasks can be factorized into a hierarchical
structure using a multilevel simulation paradigm, as an effort to manage its complexity. Multilevel
simulation introduces several organizing levels that encapsulate various individual components into
monolithic abstractions Ghosh [1986]. These levels are defined in distinctive manners, integrating
microscopic to macroscopic attributes Haman et al. [2017]. Such modeling is defined in a holonic
paradigm Tchappi et al. [2018] (see Figure 5), where holons are used as these abstractions and holons
are defined as stable self-similar structures that behave as both an entity and an organization. Holons
satisfy three important conditions: holons are stable, autonomous, and cooperative with one another.
However, a holonic organization of a task is often difficult to properly define.
Open Source Environments There exists a wide range of libraries that are used as common
benchmarks for MARL research. Table 3 provides descriptions of some of the more widely recognized
environments.
5.2

MARL Training Schemes

In this part, we explore various training paradigms used in and unique to MARL applications.
Specifically, we look into the centralized training paradigms and the use of off-policy learning.
5.2.1

Centralized Training

There exists a spectrum of agent representations in training and execution to combat the scalability
and complexity issues of learning joint strategies. This spectrum includes three main categories:
centralized training and centralized execution (CTCE), decentralized training and decentralized
execution (DTDE), and centralized training and decentralized execution (CTDE) Lowe et al. [2020].
The idea of centralization couples components of agents’ behaviors to provide a more complete state
of information to work with or to decrease the complexity of the task by distributing the workload
over multiple agents. Importantly, leveraging some level of centralization poses a good solution for
handling non-stationarity, as each agent now will have access to global information to account for the
changes in other agents’ behaviors.
CTCE The CTCE paradigm entails a fully centralized approach that involves mapping a collection
of local observations from each agent to distributions over individual action spaces. In this case,
21

Name
Multi-agent Particle Environment
(MPE) Lowe et al. [2020]
StarCraft Multi-Agent Challenge (SMAC)Samvelyan
et al. [2019], Ellis et al.
[2022], Phan et al. [2023]
PettingZoo Terry et al. [2021]
MA-Gym Koul [2019]
MAgent Zheng et al. [2017]
Level-based Foraging (LBF) and
Robot Warehouse (RWARE)
Christianos et al. [2020]
Google Research Football Kurach et al. [2020]
Overcooked Carroll et al. [2020]

Description
Various social tasks focused on communication
within a particle world setting.
Cooperative StarCraft decentralized micromanagement scenarios.

A collection of different MARL tasks and libraries.
Multi-agent tasks in a grid-world setting.
A collection of many-agent tasks.
Customizable grid-world foraging task and simulation warehouse with robots moving and delivering
product in gridworld
Simulated soccer game using physics-based 3D simulator.
Human-AI coordination on multiplayer video-game
task.
Vectorized Multi-Agent Simulator Various MARL tasks using a vectorized Pytorch(VMAS) Bettini et al. [2022]
based 2D physics engine
IsaacTeams Huh and
Various physics-based MARL tasks using GPUMohapatra [2024a]
accelerated IsaacSim platform.
JaxMARL Rutherford et al. [2023] Various physics-based MAS tasks using GPUaccelerated Brax platform.
Table 3: List of open-source environments

we essentially reduced the MAS control problem into a single-agent RL optimization over the
concatenated observations and combinatorial joint action space. While CTCE provides expressive
and complete representations of a MAS Gupta et al. [2017b] and performs well against non-CTCE
methods Yu et al. [2022a], the assumptions of decentralization are largely compromised. This is
because, during execution, decentralized agents must only make decisions based on their local
observations and do not have access to the global information it was trained on. Therefore, nontrivial
and unnatural adjustments must be made to convert the joint policy from a centralized executor to a
decentralized executors, such as masking the other agent’s information to prevent information leakage.
CTCE approaches also fail to address the curse of dimensionality problem Gronauer and Diepold
[2022], otherwise expressed as the exponential scaling caused by the joint state-action space of MAS.
DTDE On the other side of the spectrum, DTDE proposes a fully decentralized approach that
adheres to all decentralization constraints in all aspects of training and execution. We note that
this does not necessarily mean the agents cannot perceive nor is aware of other agent’s existence,
where this is known as independent learners (IL) vs. joint action learner (JAL) as defined in Claus
and Boutilier [1998], although IL can be considered an extreme form of DTDE. Prior efforts Claus
and Boutilier [1998], Tan [1997], Lauer and Riedmiller [2000], Tampuu et al. [2015], Jaderberg
et al. [2019] have demonstrated that IL with standard RL algorithms does demonstrate the ability to
converge to an equilibrium in particular and fine-tuned settings. A key challenge in the DTDE training
scheme, as notably emphasized, is non-stationarity. This challenge is exacerbated by the absence of
centralization, leading to a potential loss of mutual information among agents’ behaviors, which, in
turn, can give rise to various learning pathologies Palmer [2020]. To mitigate these pathologies, a
widely adopted strategy involves inducing optimism Matignon et al. [2007], Palmer et al. [2018], Lyu
and Amato [2020], through hysteric learning or leniency. This approach restricts the reduction of
value estimations, thereby alleviating the impact of other agents’ exploration strategies and promoting
exploration beyond equilibria that can easily trap agents without the added optimism.
• Hysteric learning: While DTDE has demonstrated success in deterministic settings, independent learning has struggled to replicate such achievements in stochastic settings. A
significant stumbling block has been the tendency to overestimate the value function, a
consequence of the inherent stochasticity, resulting in sub-optimal solutions, as stated in
Matignon et al. [2007]. To address this issue, hysteric Q-learning Matignon et al. [2007] was
22

introduced to provide an optimistic update function that assigns greater weight to positive
experiences, particularly beneficial in cooperative multi-agent scenarios. This is achieved
through the use of two learning rates, denoted as α and β. The larger learning rate, α,
is applied when updating Q-values following positive value updates, while β is utilized
otherwise.
• Leniency: Alternatively, another method to adjust the degree of optimism during the
learning process is leniency Palmer et al. [2018]. Leniency effectively allows for the
forgiveness or disregard of suboptimal actions taken by teammates that result in low rewards
during initial exploration, taking in the form of lenient Q-value updates and lenient-based
exploration. Over time, this optimism exhibited by lenient agents is gradually reduced as
they encounter and revisit state-action pairs. Consequently, agents become less lenient in
situations frequently encountered, while retaining their optimistic outlook in unexplored
territories. This shift towards average-based reward learning from maximum-based, helps
lenient agents steer clear of suboptimal joint policies, especially in environments where
rewards are subject to stochastic fluctuations. Empirically, leniency shows higher learning
stability compared to hysteretic learning, primarily due to temperature-enabled leniency
at different stages of estimation maturity. The leniency decay allows for a more faithful
representation of domain dynamics during later stages of training, where it is probable that
teammate policies become stable and near-optimal, assuming the rate of decay is appropriate
and value maturity is synchronized across all states.
Like CTCE, DTDE also demonstrates a significant issue of scalability, as a distributed solution
requires each agent to not only be represented individually but also require their own set of samples
for learning. As agents are not granted any access to the global state, which can be pivotal not only
for sample efficiency but also for good performance Gupta et al. [2017b].
CTDE CTDE provides a middle-ground by centralizing certain variables during training that still
enable decentralized execution of agents Kraemer and Banerjee [2016]. This approach strikes a
balance between the advantages of centralization while maintaining the constraints set by natural
and artificial decentralization during execution. CTDE is commonly practiced by using a centralized
value function.
Centralization of the value function allows all agents access to comprehensive state information during
training, without violating decentralization constraints during execution since the value function is not
required for decision-making. This facilitates enhanced learning, efficient updates, and coordination
between actor and critic, promoting improved policy convergence Foerster et al. [2017a], Lowe
et al. [2020]. Additionally, single-agent RL algorithms can naturally be extended, similar to DTDE,
however, using a shared critic model (i.e. MAPG Samvelyan et al. [2019], MADDPG Lowe et al.
[2020] and MAPPO Yu et al. [2022a]).
Like CTCE, a centralized value function remains prone to scalability issues. As the number of agents
grows, its representation needs to handle a larger or more intricate state space. This can lead to
significant computational costs and difficulties in defining the concise state space. Another critical
challenge is the centralized-decentralized mismatch. Since the value function is shared among agents,
sub-optimal policies from one agent can have a detrimental impact on the policy learning of other
agents, causing catastrophic miscoordination Wang et al. [2020b]. Largely, this increased variance in
learning a shared critic remains a long-standing challenge.
Parameter Sharing A commonly used approach to implement centralization is through parameter
sharing, where different agents share representation modes Gupta et al. [2017a]. This allows each
agent to update the same parameters, potentially leading to a more efficient and richer learning process.
In a sense, all agents can aggregate their experience and learn much faster in a more memory-efficient
manner Fu et al. [2022]. However, it is important to note that this can more likely lead to homogeneity
in behaviors and introduce instability, especially when dealing with highly diverse agents, as it
transforms the problem into a difficult multi-task optimization problem. Parameter sharing among
such agents, especially if they are heterogeneous, becomes a nontrivial task.
Subtask Sharing Another approach to centralization involves the use of global subtasks. In many
MAS, tasks can be decomposed into subtasks universally defined amongst all agents. As a result,
each agent’s task can be decomposed, where these global subtasks can be assigned accordingly to
23

each agent. To achieve this, the process of task decomposition needs to consider how the subtasks
can be defined properly. While subtasks can be defined using domain knowledge Spanoudakis and
Moraitis [2010], more generalizable decomposition methods, such as RODE Wang et al. [2020a] and
LDSA Yang et al. [2022], have been introduced. The core idea behind both approaches is to learn
embeddings over the actions or trajectories and perform clustering to define subtasks. A significant
challenge in these approaches is ensuring the subtasks’ definitions are distinct Yang et al. [2022].
5.2.2

Off-policy Learning

To improve the sample efficiency of MARL training, agents can learn from experiences that come
from different policies in some manner Sutton and Barto [2018], Silver et al. [2014], and this is known
as off-policy learning. Typically, off-policy approaches rely on storing samples in an experience replay
buffer. However, proper implementation of experience replay in a multi-agent setting is non-trivial
due to the non-stationary dynamics of the environment that can render past experiences obsolete,
as other agents’ behaviors change, learning with their prior behaviors may be out of distribution.
Previous efforts Foerster et al. [2017b] account for these discrepancies through two methods. The
first approach utilizes importance sampling to correct the policy updates, however, there remain
questions about the tractability of computing the importance weightings and its large and unbounded
variance. Extensions to alleviate these issues, including truncation, do reduce variance, however,
introduce additional bias. Fingerprinting, on the other hand, appends contextual information regarding
the current stage of learning of the agents in the environment to the samples that are stored in the
experience replay buffer, to disambiguate the age of the sample. Both approaches prove to stabilize
the experience replay sufficiently.
Aside from the depreciation of samples, other issues have been addressed, such as ensuring concurrency of experience sampling Omidshafiei et al. [2017] and detecting to manage miscoordination and
relative over-generalization with off-policy learning by using variable learning rate to accommodate
for exploratory actions Palmer et al. [2019], Lyu and Amato [2020]. Traditional experience replay
mechanisms, such as priority replay Schaul et al. [2016], have been experimented with MARL.
However, a naive application may deteriorate convergence and performance due to the noisy reward
and the continuous behavior changes of coexisting agents, causing a priority bias. Hence, a lenient
reward function is modeled Zheng et al. [2018] to correct the priority bias.
5.2.3

Offline Learning

The practice of offline learning with MARL adopts much of the same ideas from single-agent offline
RL, existing in the form of behavior regularization and conservatism Pan et al. [2022], but with further
considerations required to mitigate underlying issues that come along with the mixture of offline
RL and MAS, such as the propagation of extrapolation error Yang et al. [2021b] and agent-wise
imbalances within the offline data Tian et al. [2023].
5.3

Agent Awareness

Agents can exhibit different degrees of awareness of other agents, which can be classified into
three categories: independent, tracking, and agent-aware Busoniu et al. [2008]. The selection of
an awareness level involves unique considerations, advantages, and challenges, contingent upon
the specific nature of the interaction and the task. At each level, a trade-off between stability and
adaptability is made.
Definition 11 (Stability and Adaptability) Stability focuses on achieving convergence to a stationary policy π, while adaptability aims to maintain or improve performance in the face of changes in
other agents’ behaviors.
Definition 12 (Stationary Policy) A policy π i is stationary, for any state si and time-steps t and t′ ,
where t ̸= t′ ,
|πti (a|sit ) − πti′ (a|sit′ )| ≤ ϵ, ∀a ∈ A
While stability and adaptability are not necessarily dichotomous objectives, the balance between
the two helps illustrate the extent to which coordination is emphasized. Effectively addressing this
coordination problem requires agents to skillfully navigate these intricacies and strike the right
balance between focusing on stability and adaptability.
24

• Independent agents disregard the notion of coordination entirely. Moreover, their focus lies
solely on converging to stable behaviors rather than adapting to the actions of other agents in
their environment. In cooperative, adversarial, and mixed settings, such methods are referred
to as coordination-free, opponent-independent, and agent-independent respectively, and
each has demonstrated empirical success under restricted problem settings Littman [2001a],
Lauer and Riedmiller [2000], Hernandez-Leal et al. [2017]. However, these independent
methods often result in sub-optimal outcomes or even failure to achieve desired goals as
coordination becomes crucial in many scenarios to anticipate and respond strategically to
the actions of other agents.
• Tracking agents prioritize adaptability over stability, placing a greater priority on coordination rather than learning a stable individual behavior. With a tracking approach, agents
continuously adjust their strategies based on the observed behavior of other agents. Empirically, agent-tracking methods rely on agent modeling to guide the agents’ action selection
process Robinson [1951], Weinberg and Rosenschein [2004]. However, the stability of the
joint behavior may be compromised. The constant adaptation can lead to non-stationary
behaviors, as the agents respond not only to changes in the environment but also to the
changing strategies of other agents.
• Agent-aware agents strive to achieve a balance between stability and adaptability by being
conscious of the other agents’ strategies while preserving their individuality. Previous
studies have explored approaches such as "Adapt When Everyone is Stationary, Otherwise
Move to Equilibrium" (AWESOME) Conitzer and Sandholm [2003] or "Win or Learn
Fast" (WoLF) Bowling [2004] to determine when to adapt or maintain their local strategy,
mostly just by adjusting the learning rate or incorporating the other agent’s anticipated
learning to one another Foerster et al. [2018]. These concepts primarily address handling the
non-stationarity of the optimization problem, but this heightened awareness also establishes
a well-rounded foundation for fostering social behaviors that lead to stable and successful
coordination.
Learning with Awareness When learning, agents can take into account the behaviors and information regarding other agents Foerster et al. [2017c]. One way this can be achieved is by extrapolating
gradient updates for each agents, thereby performing a one-step look-ahead over the learning over all
agents, which is known as extragradient Korpelevich [1976]. To reduce complexity of extrapolation
step of gradient update, we can instead use only a sample subset of agents in many-agent settings
Jelassi et al. [2020]. Similarly, LOLA Foerster et al. [2017c] takes a similar approach, however,
extrapolates only the other agents using a second order correction term with a Taylor expansion
approximation.
5.4

Multi-agent Credit Assignment

For many MARL applications, it may be intractable to define local rewards for each agent, hence
necessitating the use of the Dec-MDP framework. In such settings, a global reward is instead
provided, which represents the collective’s utility. However, with this measure, it is unclear the
direct contributions and local performances of each agent. This problem is known as multi-agent
credit assignment (MACA) Agogino and Tumer [2008]. We distinguish MACA from the traditional
credit assignment problem associated with the casual aspects of sequential decision-making, where
the actions themselves are evaluated on their impact Sutton and Barto [2018]. In recent efforts, the
challenge of MACA, as briefly mentioned in Section 4.2.4, is addressed in two approaches: difference
rewards and value factorization.
Difference Rewards Difference rewards aim to capture an agent’s contribution from a global
performance measure by shaping a local reward signal that isolates the utility of individual agent’s
actions by removing the utility of other agents Agogino and Tumer [2004]. While in some applications,
such as air traffic flow management Tumer and Agogino [2007], this isolation is possible, generally,
forming a theoretical setting that removes agents individually may be impossible. Hence, the
marginalization of individual agents is often estimated by comparing them against the average actions,
known as aristocrat utility Wolpert and Tumer [2001]. To extend to the realm of deep RL, COMA
Foerster et al. [2017a] leverages the concept of the advantage function to achieve the same effects.
25

Value Factorization Value factorization decomposes a global value into local values for each agent
Guestrin et al. [2001]. A simple implementation of this is known as a value decomposition network
(VDN) Sunehag et al. [2017], where the sum of the learned local value functions can be treated as
the global value. With the many extensions of VDN that have been proposed in the past decade, the
standard constraints of Individual-Global-Max (IGM) Rashid et al. [2018] serve as the theoretical
basis for guaranteeing and maintaining consistency between the global Q and local qi value estimates.


argmax q0 (τ0 , a0 )
a0




..
argmax Q(s, a) = 
(20)

.


a
argmax qN (τN , aN )
aN

where τi is the observation-action history of agent i. However, these constraints have been shown
to restrict the expressiveness of the value function representations Mahajan et al. [2020], leading to
sub-optimal value approximations and poor explorations. Hence, it remains a open research challenge
to improve these limitations of value decomposition while trying to adhere to IGM.
5.5

Communication

Communication is a powerful capability of high interest within MARL literature that enables agents
to exchange and propagate information between one another, leading them to behave as a collective
rather than a collection of independent individuals Zhu et al. [2024]. In many multi-agent tasks, communication proves vital to coordinate the behavior of multiple agents to achieve optimal performance
Foerster et al. [2016], especially under settings with imperfect information and partial observability
Zaïem and Bennequin [2019].
However, an efficient and practical implementation of a communication mechanism presents several
key challenges, necessitating consideration of not only what information to communicate Sukhbaatar
et al. [2016], Foerster et al. [2016], but also how Shao et al. [2022], when Singh et al. [2018], and
with whom Jiang and Lu [2018] to communicate. In Zhu et al. [2024], the topic of communication is
broken down and categorized over 9 dimensions on its implementation, so we recommend readers
refer to this resource for more in-depth analysis.
5.5.1

Communication Infrastructure

A communication graph is introduced to define which agents each agent can communicate with,
alongside the use of the networked stochastic game framework. The restriction placed on the existence
of the graph’s edges is bounded by the constraints of decentralization and requires solutions that
address issues including limited range Huh and Mohapatra [2023], limited bandwidth Foerster
et al. [2016], noisy communication channels Freed et al. [2020], and contentions with shared
communication mediums Kim et al. [2019] such as a proxy. A consideration for each is understanding
their practicality, which is dependent on the nature of the task, as well as their shortcomings.
Proxy While decentralization prohibits centralized executors, a centralized communication medium
is not prohibited. The role of a proxy is to serve as a coordinator and message aggregator. It gathers
local observations or messages from agents in the environment, subsequently broadcasting messages
to each of them Kong et al. [2017]. Alternatively, it can connect nearby agents who opt to participate
in a communication group, facilitating the sharing of coordinated messages with each group member
Jiang and Lu [2018]. Canonically, a proxy solely acts as a communication medium, having no direct
effect on the environment. A key challenge of a proxy is its design. Solutions that use proxies must
consider their efficiency, ensuring that sufficient communication is achieved for the task at hand while
also managing the computation load and expressiveness of the proxy.
Networked communication In contrast to relying on a proxy for inter-agent communication, the
networked communication protocol consists of agents that pass and receive messages directly to and
from other agents Zhang et al. [2018], Chu et al. [2020]. While this form of communication may
seem most fitting for a decentralized setting, its dynamic nature and lack of structure can lead to poor
performance and scalability issues, especially when each agent has limited compute resources and is
required to process many agents’ messages.
26

Implicit Communication Agents can also communicate with one another without explicit means,
such as through stigmergy Grassé [1959]. The concept of stigmery defines the influence agents have
through their actions on one another, often through environmental changes or some other form of
stimuli. More formally, stigmery describes the influence of the persisting environmental effects of
prior behaviors on behaviorsHolland and Melhuish [1999]. We categorize the idea of stigmergy based
on the intent/form of the stigmergic actions, the responses to the stigmergic behaviors, and the impact
of stigmergic actions.
• Sematectonic and marker-based stigmergy Wilson [2000], Marsh and Onof [2008] distinguish whether the stigmergic actions were directly aligned with true objectives of the agents
or rather, to solely stigmergize. The intent and the actual actions are often considered when
classifying the two forms of stigmergy.
• Quantitative and qualitative stigmergy Theraulaz and Bonabeau [1999] differentiate whether
the response to the stigmergic actions is an intensification of the resulting stimulus or triggering different stimuli, leading to a self-organization process. A self-organizing process refers
to a set of dynamical local mechanisms, which through their applications and interactions,
causes emergent global structures and behaviors.
• Active and passive stigmergy Holland and Melhuish [1999] refer to the effects and outcomes
of the stigmergic actions. Active stigmergy directly affects the agents, influencing the
observations, actions, and parameters (e.g. frequency, latency, duration, intensity). However, passive stigmergy is more indirect and subtle, perhaps leading to no changes to any
observations, actions, or their parameters, but only to changes to the outcome.
Historically, a central focus of stigmery in MARL has been deriving optimization algorithms, such as
ant colony optimization Dorigo and Blum [2005], that are inspired by these concepts. We propose that
these patterns and behaviors of stigmergy should be further studied towards other forms of integration
into our MAS, including, but not limited to, how to induce stigmergic behaviors and quantify and
evaluate them.
5.5.2

Communication Representation

In this section, we look into the various forms of communication mechanisms that are used in practice,
namely those that were realized using deep learning techniques such as graph neural networks (GNN).
Graph Neural Networks Facilitating rich communication among agents requires a scalable framework that can naturally process information within the communication graph in an expressive manner.
GNNs are a fundamental tool for handling non-Euclidean data, especially when dealing with information naturally occurring in graph structures.
Concretely, GNNs learn to map input data to latent representations that can be used in subsequent
tasks. GNNs generate these latent embeddings by iteratively performing the following operations:
message computation, propagation, and aggregation. A visualization of these operations is provided
in Figure 6. Together, these three operations can be collectively described as a graph convolution Kipf
and Welling [2017]. Graph convolutions can be performed iteratively, increasing the receptive field
and allowing GNNs to capture more global information as the number of message passing rounds
increase, i.e. increase the number of GNN layers.
There have been several advancements with GNN algorithms, specifically to increase expressiveness,
improve scalability, and importantly, compensate for the over-smoothing problem Xu et al. [2018],
where learning on densely connected graphs often converged to redundant node embeddings. Current
MARL research utilizes GNNs as the de-facto communication mechanism, such as graph convolution
network (GCN) Kipf and Welling [2017] in CommNet Sukhbaatar et al. [2016] and BiCNet Peng
et al. [2017], and graph attention network (GAT) Brody et al. [2022] in DGN Jiang et al. [2020]
and ATOCJiang and Lu [2018]. While many of these GNN models were initially designed for
prediction-based tasks such as supervised classification, there have been efforts to bridge these tools
into the domain of control such that novel mechanisms are tailored to the intricacies of MAS (e.g.
dynamic role assignments Shao et al. [2022] and limited communication Kim et al. [2019]), although
this remains an open problem in MARL.
27

Figure 6: A visualization of the three operations of graph convolutions.

5.5.3

Learning to Communicate

Learning communication involves considering the following three aspects of communication: the
content of the outgoing messages, how the incoming messages are incorporated and the communication policy Zhu et al. [2024]. The optimization itself can be devised to learn all these aspects together
or individually and makes use of explicit and/or implicit feedback. This means the communication
learning dynamics can use additional feedback signals, such as social influence Jaques et al. [2019],
which may be optimized in conjunction with the MARL training. A common practice is to seamlessly
integrate the two learning dynamics of communication and control, often with differentiable modeling
and backpropagation Foerster et al. [2016], Sukhbaatar et al. [2016], Freed et al. [2020].
Message Computation To initiate communication, each agent must compute messages to broadcast.
The content of the message can vary from encoded or non-encoded information regarding the current
and/or past local observations, actions, rewards, beliefs, objectives, previously received messages, or
any other accessible information regarding the agents and the tasks. This can also include imagined
information, such as future imagined trajectories/behaviors or intentions Kim et al. [2020]. To embed
the listed information, an explicit approach is often employed through an auto-encoding process. On
the other hand, although not completely orthogonal to the explicit learning approach, an end-to-end
learning process is an alternative trained using the MARL learning dynamics without any such
grounding. The learning representation for the content can be either discrete symbols or continuous
values Foerster et al. [2016].
In addition to the content itself, the concept of language is postulated to be an important aspect of
generalizable coordinating behaviors, such as learning a lingua franca Lin et al. [2021]. An interesting
property of language often studied is compositionality Lazaridou et al. [2018], which refers to the
ability to produce complex meanings by combining simpler linguistic elements and symbols in
systematic ways.
Communication Policy A communication policy defines the properties of the edges in the communication graph, managing the following: the senders and recipients of all messages and the frequency
of the message transmissions. This policy must thereby consider and adhere to the constraints of
decentralized communication. If a proxy is used, the purpose of the proxy is to handle the operations
of the communication policy.
The edges of the communication graph can be statically or dynamically defined. A static implementation can be fixed or make use of some heuristics Huh and Mohapatra [2023], whereas a dynamically
defined solution is more involved. A natural approach is to make use of gating mechanisms Jiang
and Lu [2018] and communication scheduling modules Kim et al. [2019] to dictate the formation of
28

the edges in a learnable fashion. This enables us to design communication paradigms dynamically
dependent on the needs and constraints of the task. For example, we can regularize the communication
overhead with penalty terms that directly impact the parameters of the gating mechanism Hu et al.
[2020].
Another consideration is the frequency of communication between agents. A common assumption is
allowing all agents to communicate at every time step, however, this may not be necessary and in
such cases, the over-communication can be detrimental in not only cost but also effectiveness due
to the greater reliance and need for more expressive and consistent communication measures. To
mitigate this issue, communication can be less frequent by setting a more sparse communication
cycle Shao et al. [2022] or utilizing a mechanism that enables agents to be more selective, such as the
gating mechanism or communication scheduling.
Message Integration Lastly, we discuss message integration, which refers to how each agent
processes and utilizes the messages they receive. A popular approach is to use message aggregation
operations, as observed with GNN architectures, involving either a weighted or non-weighted
summation over all incoming messages. While maintaining parameters for the weighting over the
incoming messages is a great option to increase expressiveness in the aggregation process Zhang
et al. [2021], Li et al. [2021], learning the weighting may lead to challenges notably in generalization,
as performance may suffer when dealing with ad-hoc team-play scenarios if there is a lack of
adaptability.
5.5.4

Evaluating Communication

In recent efforts relating to communication, studies have explored evaluation metrics to grow our
understanding and quantify the quality of the communication between agents Lowe et al. [2019].
These efforts range from taking a broader view by observing the changes in performance (i.e. agent’s
rewards or task success rate) under changes in communication methods to a more granular scope,
where explicit metrics are defined that can account for the varying reasons for the impact achieved by
communication. A broader view is often taken when communication is vital for the task at hand, often
in the form of referential games, a common mode of game used to study the aspect of communication
in MARL. Referential games can be thought of as a form of Lewis signaling game, where agents
are each assigned the roles of speaker and listeners, and the speaker agent must communicate to the
listener agents to complete their tasks Lewis [2008]. In many cases, the speaker agent has access to
some private information not privied to the listener agents, making their communication significant.
These specific roles designated to the agents result in a strong reliance on the communication system
in place for the success of the task.
The metrics of communication can be divided into two classifications: positive signaling and positive
listening Lowe et al. [2019]. Positive signaling quantifies some statistical dependence of the messages
on the agents’ observations or actions. Positive listening, on the other hand, is measured by the
change in behavior of an agent if its incoming messages are obscured or omitted. Previous studies
that focus on devising positive signaling metrics aim towards quantifying the alignment between the
agent’s messages and some inherent component relating to the agent, often defined using mutual
information Jaques et al. [2019], Bogin et al. [2019]. For instance, speaker consistency (SC) measures
the mutual information (MI) between an agent’s messages and its actions Jaques et al. [2019], whereas
context independence (CI) instead measures the MI between the agent’s messages and predefined
task concepts. Intuitively, SC provides insights into how much uncertainty is reduced regarding an
agent’s action given its messages, and CI enforces the notion of language compositionality, where the
content of the messages is induced to be related to inherent concepts within the environment.
On the other hand, the guiding principle for measuring positive listening has been to quantify the
causal influence of an agent’s message on another agent’s behavior. Similarly, this computation
can be achieved with MI, where in practice, we quantify the alignment between an agent’s message
to the actions of receiving agents Lowe et al. [2019], Jaques et al. [2019]. Usually, these metrics
consider the “one-step" behavior, meaning the causal influence of a message is referenced against the
immediate response of an agent, such as its next action, where a multi-step behavior can lead to more
accurate measures Eccles et al. [2019]. In terms of future directions, it remains unclear the definitive
relationship between these metrics, the concepts of positive listening and positive signaling, the true
quality of communication, and their interactions as well as a more theoretical justification for the bias
these approaches provide Eccles et al. [2019].
29

5.6

Modeling Other Agents

An essential capability for agents is the ability to reason about the behaviors of other agents, which
can be achieved by constructing models of other agents (MOA). This process is often referred to as
agent modeling, or more traditionally known as opponent modeling. We divide our discussion of
agent modeling into three parts: the representation of MOA, the optimization paradigm used with
MOA, and how MOAs are utilized.
Representation of MOA MOA can encompass a wide array of properties of other agents, including
their observations, actions, goals, beliefs, and more intricate components such as intentions or agent
types Hong et al. [2018], Raileanu et al. [2018]. MOA can also contain information regarding entire
coalitions Erdogan and Veloso [2011]. Empirical applications of MOA can be accomplished using
deep learning models tailored to represent the specific properties they embody and how the MOA is
intended to be integrated He et al. [2016]. Bayesian game is a common game mode used to represent
MOA through a belief space, i.e. type Harsanyi [1967]. The belief space of each agent contains any
information which is not regarded as common knowledge including its private knowledge, which
can hold local information regarding other agents. These beliefs, however, face the challenge of
approximating uncertainty as agents must contend with incomplete information about the environment
and the behaviors of other agents. Addressing this challenge often involves employing probabilistic
techniques to estimate and reason about uncertainties within the belief space Huh and Mohapatra
[2024b]. Another interesting concept with MOA is the theory of mind, where agents engage in
recursive reasoning about the states of other agents Premack and Woodruff [1978], Yu et al. [2022c].
In practice, a nested reasoning approach is often approximated using belief nesting down to a fixed
recursion depth, which can be implemented with game tree search techniques Carmel and Markovitch
[1996]. In practice, there exists a large inspiration from model-based single-agent RL (MBRL),
and there remains much work to incorporate the unique aspects of MAS into such MBRL methods
Nashed and Zilberstein [2022].
Learning MOA Here, we discuss two approaches to optimize and learn information about other
agents, in the form of discriminative and generative learning. The discriminative learning approach
comprises training MOA to classify and predict explicit properties of other agents, such as their
observations or policies, typically through methods of maximum likelihood estimation Huh and
Mohapatra [2024b]. On the other hand, generative learning approaches rely more on maximum
a posterior, where the goal is to model the joint distribution of observed data and latent variables,
enabling the generation of realistic samples from the learned distribution. By capturing the underlying
structure of the data, generative learning facilitates a deeper understanding of the relationships
between different properties of other agents, allowing for more nuanced inference and decisionmaking in multi-agent environments Erdogan and Veloso [2011], Nashed and Zilberstein [2022]. The
learning process of MOA also must consider how it is integrated with the MARL training, which can
be done separately or simultaneously, and what additional data or assumptions are required.
Using MOA MOA can be utilized in various manners, such as guiding the agent’s decision-making
process, i.e. planning and recursive reasoning, or helping construct a more accurate understanding of
the environment the agent is presiding in Huh and Mohapatra [2024b]. An important consideration
is the trust and robustness of utilizing these models, as agents may make incorrect or inaccurate
predictions. An interesting application that remains an open research topic is an adversarial attempt
to trick agents through deceptive actions to promote misleading synergies Albrecht and Stone [2018].
5.7

Ad-Hoc Team-Play

Learning in multi-agent systems may be faced with a distributional mismatch resulting from unseen
behaviors from other agents. The concept of ad-hoc team-play (AHTP) challenges the learned
behaviors of agents to work with unknown partners who are capable of contributing to the task Stone
et al. [2010]. For its evaluation, it is common to define a period of ad-hoc interactions between
the agents to acclimate and devise their new joint strategy but assume that agents have no prior
coordination before this ad-hoc interaction and also that agents have no direct control over other
agents Mirsky et al. [2022]. If we assume no ad-hoc interactions, we refer to this as zero-shot
coordination (ZSC) Treutlein et al. [2021]. The notion of AHTP/ZSC often arises in settings of
human-AI coordination Nekoei et al. [2023], Yu et al. [2023], where the AI agents must interact
30

with humans to achieve a task. Typically in such settings, the AI agents and the humans have not
interacted with each other previously. In general, we note that despite the use of the term “teammate"
and “team-play", these agents are not necessarily cooperative but can be adversarial or mixed.
A crucial aspect of achieving good AHTP behaviors is to avoid arbitrary conventions that often arise
in traditional MARL training Carroll et al. [2020]. This can be achieved by either training agents on a
pool of diverse policies, known as population-based training (PBT) Canaan et al. [2019], or removing
grounded beliefs of the agents that rely on arbitrary social conventions by utilizing off-belief learning,
which assumes the prior behaviors of others were derived from fixed random policies, but their
future actions will be computed with actual behavioral policies Hu et al. [2021]. Another option for
addressing AHTP is for agents to learn to identify the behaviors of the other agents such that they
can properly adjust their strategy to its current environment Chen et al. [2020]. Similarly, agents can
instead learn models of the other agents for fast and efficient teamwork in the absence of explicit prior
coordination Barrett et al. [2017], Santos et al. [2021]. However, this approach is sample inefficient,
as agents would need to learn these additional capabilities effectively.
To quantify the AHTP capability, there exist two popular metrics: cross-play and adaptation regret
Nekoei et al. [2023]. Cross-play is a static measure of the performance of agents with their new
teammates and is often visualized through a cross-play matrix. Similarly, adaptation regret measures
the cross-play performance but compares it to the performance achieved with the old teammates, i.e.
the regret. The adaptation regret can be viewed under an adaptation curve, which helps view how
quickly agents adapt to their new teammates. Similar to PBT, how to define the pool of “diverse"
policies more optimally remains a question and often is generated using multiple independent runs of
training with the same or different MARL algorithms.
AHTP remains an especially difficult challenge when dealing with heterogeneous agents, open
environments with variation in the number of agents in the environment, imperfect information,
unreliable nor robust communication mechanisms, highly adaptive, irrational or risk-averse agents,
and diverse nature of interactions Mirsky et al. [2022], Guan et al. [2023], Nekoei et al. [2023].
5.8

Social Learning

The sharing of learned behaviors amongst agents is a powerful mechanism that can improve the
efficiency and adaptability of a population’s knowledge Silva and Costa [2019]. This process is
referred to as social learning Ndousse et al. [2021] and encompasses the concept of knowledge transfer,
which manifests through two central approaches: intra-agent transfer, where knowledge is transferred
and reused from different domains, and inter-agent transfer, where agents share knowledge within
the same task and setting. While both forms of knowledge transfer play crucial roles in facilitating
efficient social learning and adaptation within MAS, the focus will be on inter-agent transfer. Within
the framework of inter-agent transfer, it is also important to consider the role assignments within the
populations. In this work, we define three common role assignments: advisor/advisee, teacher/student,
and mentor/observer Silva and Costa [2019].
• In the advisor/advisee relationship, the advisor receives requests from the advisee and
observes their state, offering valuable information without presuming anything about the
internal representation of agents.
• The interaction between a teacher and student is similar to that of an advisor/advisee, but
in this case, certain assumptions are made, allowing for more informed designs for any
information exchange.
• As for the mentor/observer relationship, the observer aims to emulate the behavior of the
mentor, thereby learning from the mentor’s expertise and experience.
Concretely, we explore three forms of inter-agent transfer: action advising, reward shaping, and
knowledge distillation. Each of these approaches offers unique ways for agents to leverage the
knowledge of their peers and improve their performance in the collective endeavor.
Action Advising The fundamental concept of action advising (AA) revolves around an experienced
agent providing recommendations on the next best actions to take to a less experienced agent Guo
et al. [2023], Omidshafiei et al. [2018]. In many cases, one agent can offer action suggestions to
another, even when the internal representation of the other agents remains unknown. In practice, its
31

implementation must consider how this AA process is initiated, i.e. by the advisor and/or advisee.
Da Silva et al. [2020], Fachantidis et al. [2017], Amir et al. [2016], and how the advice is incorporated
with some form of option learning, referring to whether or not the suggested policy should be followed
Sutton et al. [1999b], Yang et al. [2021a]. However, an effective approach that generalizes this AA
capability that benefits MARL training remains an open challenge.
Reward Shaping Derived from the motivations of potential functions in single-agent RL Ng et al.
[1999], reward shaping in MARL enables agents to influence the reward signals of other agents,
typically in the form of an auxiliary signal that provides further learning guidance Gupta et al.
[2017a]. Reward shaping approaches are particularly valuable in scenarios where rewards are sparse,
as they allow for a learnable method to devise more informative learning signals Wang et al. [2022].
However, similar to potential functions, it is important to take caution when using such methods, as
the behaviors of agents are highly dependent and influenced by the reward function, thereby it may
be pivotal in some settings to maintain some level of invariance to these auxiliary reward signals.
Knowledge Distillation The process of knowledge distillation (KD) involves transferring knowledge from teacher agents to student agents Buciluǎ et al. [2006], Hinton et al. [2015]. Typically, KD
is primarily used for model compression, where the capabilities of a larger model or multiple models
are distilled into a single/smaller model for parameter efficiency and potential performance benefits,
or adaptation to a new state and/or action space, where a teacher model is initially trained on a more
complete state-action space and a student model must make use of a more restricted state-action
space Czarnecki et al. [2019], Lai et al. [2020]. In MARL applications, KD can further leverage the
multi-agent nature, through structural relations distillation, where the relations between multi-agents’
features are preserved Tseng et al. [2022].

6

Concluding Remarks

While the challenges within MARL have been extensively studied and assessed, the existing methodologies for acquiring multi-agent behaviors fall short of fully harnessing the myriad opportunities
within a MAS. Despite substantial progress, particularly in unique areas of learning in a MAS, i.e.
its prospects, and the fundamental challenges of MARL, there remain open research challenges that
demand further exploration and refinement.
The intricacies of MARL extend beyond individual agent behavior to encompass the dynamic interactions unfolding within complex environments. Other properties of certain applications, such as open
environments, human-robot interactions, and heterogeneous agents, merit additional considerations,
for instance, how to handle the unbounded and evolving nature of open environments, safety and
proper coordination with human-robot interactions, and manage the diverse capabilities, behaviors,
and learning speeds within heterogeneous populations.
To conclude our discussion of MARL, our exploration of MARL discussed both the progress made
and the avenues yet to be fully explored. The multifaceted nature of multi-agent interactions within
dynamic environments demands ongoing research and refinement of methodologies to unlock the full
potential of MARL in harnessing the complexities inherent in a MAS.

References
L. Adam, R. Horčík, T. Kasl, and T. Kroupa. Double oracle algorithm for computing equilibria in
continuous games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 5070–5077, 2021.
A. K. Agogino and K. Tumer. Unifying temporal and structural credit assignment problems. In
Autonomous agents and multi-agent systems conference, 2004.
A. K. Agogino and K. Tumer. Analyzing and visualizing multiagent rewards in dynamic and
stochastic domains. Autonomous Agents and Multi-Agent Systems, 17:320–338, 2008.
S. V. Albrecht and P. Stone. Autonomous agents modelling other agents: A comprehensive survey and
open problems. Artificial Intelligence, 258:66–95, may 2018. doi: 10.1016/j.artint.2018.01.002.
URL https://doi.org/10.1016%2Fj.artint.2018.01.002.
32

S. V. Albrecht, F. Christianos, and L. Schäfer. Multi-agent reinforcement learning: Foundations and
modern approaches. 2024. URL https://www.marl-article.com.
C. Amato and F. A. Oliehoek. Scalable planning and learning for multiagent pomdps: Extended
version, 2014.
C. Amato, G. Konidaris, L. P. Kaelbling, and J. P. How. Modeling and planning with macro-actions
in decentralized pomdps. Journal of Artificial Intelligence Research, 64:817–859, 2019.
O. Amir, E. Kamar, A. Kolobov, and B. Grosz. Interactive teaching strategies for agent training. In In
Proceedings of IJCAI 2016, 2016.
M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
P. Abbeel, and W. Zaremba. Hindsight experience replay. arXiv preprint arXiv:1707.01495, 2017.
E. Anshelevich, A. Dasgupta, J. Kleinberg, É. Tardos, T. Wexler, and T. Roughgarden. The price
of stability for network design with fair cost allocation. SIAM Journal on Computing, 38(4):
1602–1623, 2008.
S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and
applications. Theory of Computing, 8(6):121–164, 2012. doi: 10.4086/toc.2012.v008a006. URL
https://theoryofcomputing.org/articles/v008a006.
R. J. Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics, 1(1):67–96, 1974. ISSN 0304-4068. doi: https://doi.org/10.1016/0304-4068(74)90037-8.
URL https://www.sciencedirect.com/science/article/pii/0304406874900378.
T. Baarslag, M. Hendrikx, K. Hindriks, and C. Jonker. Learning about the opponent in automated
bilateral negotiation: a comprehensive survey of opponent modeling techniques. Autonomous
Agents and Multi-Agent Systems, 30, 09 2016. doi: 10.1007/s10458-015-9309-1.
A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell. Agent57:
Outperforming the atari human benchmark, 2020a.
A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman, M. Arjovsky,
A. Pritzel, A. Bolt, and C. Blundell. Never give up: Learning directed exploration strategies,
2020b.
B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and
J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos, 2022.
S. Barrett, A. Rosenfeld, S. Kraus, and P. Stone. Making friends on the fly: Cooperating with new
teammates. Artificial Intelligence, 242:132–171, 2017. ISSN 0004-3702. doi: https://doi.org/
10.1016/j.artint.2016.10.005. URL https://www.sciencedirect.com/science/article/
pii/S0004370216301266.
T. Başar and G. J. Olsder. Dynamic noncooperative game theory. SIAM, 1998.
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
70:449–458, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/bellemare17a.
html.
R. Bellman. Dynamic programming princeton university press princeton. New Jersey Google Scholar,
pages 24–73, 1957.
S. Bergerson. Multi-agent inverse reinforcement learning: Suboptimal demonstrations and alternative
solution concepts, 2021.
D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of decentralized control of markov
decision processes. CoRR, abs/1301.3836, 2013. URL http://arxiv.org/abs/1301.3836.
M. Bettini, R. Kortvelesy, J. Blumenkamp, and A. Prorok. Vmas: A vectorized multi-agent simulator
for collective robot learning. The 16th International Symposium on Distributed Autonomous
Robotic Systems, 2022.
33

R. S. Bielefeld. Reexamination of the perfectness concept for equilibrium points in extensive games,
1988. URL https://doi.org/10.1007/978-94-015-7774-8_1.
A. Bighashdel, Y. Wang, S. McAleer, R. Savani, and F. A. Oliehoek. Policy space response oracles:
A survey, 2024.
D. Blackwell. An analog of the minimax theorem for vector payoffs. 1956.
L. Blier, C. Tallec, and Y. Ollivier. Learning successor states and goal-dependent values: A mathematical viewpoint, 2021.
D. Bloembergen, K. Tuyls, D. Hennes, and M. Kaisers. Evolutionary dynamics of multi-agent
learning: A survey. Journal of Artificial Intelligence Research, 53:659–697, 08 2015. doi:
10.1613/jair.4818.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks,
2015.
B. Bogin, M. Geva, and J. Berant. Emergence of communication in an interactive world with
consistent speakers, 2019.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji,
A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon,
J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman,
S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang,
T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh,
M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent,
X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa,
S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko,
G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan,
R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang,
W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang,
X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the opportunities and risks of foundation
models, 2022.
M. Bowling. Convergence and no-regret in multiagent learning. Advances in neural information
processing systems, 17, 2004.
M. Bowling and M. Veloso. An analysis of stochastic game theory for multiagent reinforcement
learning. 08 2001.
M. Bowling and M. Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence,
136(2):215–250, 2002. ISSN 0004-3702. doi: https://doi.org/10.1016/S0004-3702(02)00121-2.
URL https://www.sciencedirect.com/science/article/pii/S0004370202001212.
M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold’em poker is solved.
Science, 347(6218):145–149, 2015.
R. I. Brafman and M. Tennenholtz. Learning to coordinate efficiently: A model-based approach.
Journal of Artificial Intelligence Research, 19:11–23, jul 2003. doi: 10.1613/jair.1154. URL
https://doi.org/10.1613%2Fjair.1154.
S. Brody, U. Alon, and E. Yahav. How attentive are graph attention networks?, 2022.
C. Buciluǎ, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541,
2006.
J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-efficient reinforcement learning
with stochastic ensemble value expansion. CoRR, abs/1807.01675, 2018. URL http://arxiv.
org/abs/1807.01675.
34

L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
38(2):156–172, 2008. doi: 10.1109/TSMCC.2007.913919.
L. Busoniu, R. Babuvska, and B. D. Schutter. Multi-agent reinforcement learning: An overview.
2010. URL https://api.semanticscholar.org/CorpusID:17136625.
W. Böhmer, V. Kurin, and S. Whiteson. Deep coordination graphs, 2020.
C. Cai, C. Yang, Q. Zhu, and Y. Liang. Collision avoidance in multi-robot systems. pages 2795–2800,
2007. doi: 10.1109/ICMA.2007.4304002.
J. A. Calvo and I. Dusparic. Heterogeneous multi-agent deep reinforcement learning for traffic lights
control. 2018. URL https://api.semanticscholar.org/CorpusID:57661298.
C. F. Camerer. Progress in behavioral game theory. Journal of economic perspectives, 11(4):167–188,
1997.
R. Canaan, J. Togelius, A. Nealen, and S. Menzel. Diverse agents for ad-hoc cooperation in hanabi.
In 2019 IEEE Conference on Games (CoG), pages 1–8. IEEE, 2019.
D. Carmel and S. Markovitch. Incorporating opponent models into adversary search. pages 120–125,
01 1996.
M. Carroll, R. Shah, M. K. Ho, T. L. Griffiths, S. A. Seshia, P. Abbeel, and A. Dragan. On the utility
of learning about humans for human-ai coordination, 2020.
G. Chalkiadakis and C. Boutilier. Coordination in multiagent reinforcement learning: A bayesian
approach. In Proceedings of the second international joint conference on Autonomous agents and
multiagent systems, pages 709–716, 2003.
Y. Chebotar, Q. Vuong, A. Irpan, K. Hausman, F. Xia, Y. Lu, A. Kumar, T. Yu, A. Herzog, K. Pertsch,
K. Gopalakrishnan, J. Ibarz, O. Nachum, S. Sontakke, G. Salazar, H. T. Tran, J. Peralta, C. Tan,
D. Manjunath, J. Singht, B. Zitkovich, T. Jackson, K. Rao, C. Finn, and S. Levine. Q-transformer:
Scalable offline reinforcement learning via autoregressive q-functions, 2023.
S. Chen, E. Andrejczuk, Z. Cao, and J. Zhang. Aateam: Achieving the ad hoc teamwork by employing
the attention mechanism. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):
7095–7102, Apr. 2020. doi: 10.1609/aaai.v34i05.6196. URL https://ojs.aaai.org/index.
php/AAAI/article/view/6196.
X. Chen, X. Deng, and S.-H. Teng. Settling the complexity of computing two-player nash equilibria,
2007.
Y. Chen, Y. Yang, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. M. McAleer, H. Dong, and S.-C. Zhu.
Towards human-level bimanual dexterous manipulation with reinforcement learning. 2022. URL
https://openreview.net/forum?id=D29JbExncTP.
S. Choi, D.-Y. Yeung, and N. Zhang. An environment model for nonstationary reinforcement
learning. 12, 1999. URL https://proceedings.neurips.cc/paper_files/paper/1999/
file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf.
F. Christianos, L. Schäfer, and S. V. Albrecht. Shared experience actor-critic for multi-agent reinforcement learning. 2020.
T. Chu, J. Wang, L. Codecà, and Z. Li. Multi-agent deep reinforcement learning for large-scale traffic
signal control, 2019.
T. Chu, S. Chinchali, and S. Katti. Multi-agent reinforcement learning for networked system control,
2020.
K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
trials using probabilistic dynamics models, 2018.
35

C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems.
page 746–752, 1998.
I. Clavera, J. Rothfuss, J. Schulman, Y. Fujita, T. Asfour, and P. Abbeel. Model-based reinforcement
learning via meta-policy optimization, 2018.
K. Cobbe, J. Hilton, O. Klimov, and J. Schulman. Phasic policy gradient, 2020.
E. Collaboration, A. Padalkar, A. Pooley, A. Mandlekar, A. Jain, A. Tung, A. Bewley, A. Herzog,
A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Garg, A. Brohan, A. Raffin, A. Wahid, B. BurgessLimerick, B. Kim, B. Schölkopf, B. Ichter, C. Lu, C. Xu, C. Finn, C. Xu, C. Chi, C. Huang, C. Chan,
C. Pan, C. Fu, C. Devin, D. Driess, D. Pathak, D. Shah, D. Büchler, D. Kalashnikov, D. Sadigh,
E. Johns, F. Ceola, F. Xia, F. Stulp, G. Zhou, G. S. Sukhatme, G. Salhotra, G. Yan, G. Schiavi,
G. Kahn, H. Su, H.-S. Fang, H. Shi, H. B. Amor, H. I. Christensen, H. Furuta, H. Walke, H. Fang,
I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Abou-Chakra, J. Kim, J. Peters, J. Schneider,
J. Hsu, J. Bohg, J. Bingham, J. Wu, J. Wu, J. Luo, J. Gu, J. Tan, J. Oh, J. Malik, J. Booher,
J. Tompson, J. Yang, J. J. Lim, J. Silvério, J. Han, K. Rao, K. Pertsch, K. Hausman, K. Go,
K. Gopalakrishnan, K. Goldberg, K. Byrne, K. Oslund, K. Kawaharazuka, K. Zhang, K. Rana,
K. Srinivasan, L. Y. Chen, L. Pinto, L. Fei-Fei, L. Tan, L. Ott, L. Lee, M. Tomizuka, M. Spero,
M. Du, M. Ahn, M. Zhang, M. Ding, M. K. Srirama, M. Sharma, M. J. Kim, N. Kanazawa,
N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf, N. D. Palo, N. M. M. Shafiullah, O. Mees,
O. Kroemer, P. R. Sanketi, P. Wohlhart, P. Xu, P. Sermanet, P. Sundaresan, Q. Vuong, R. Rafailov,
R. Tian, R. Doshi, R. Martín-Martín, R. Mendonca, R. Shah, R. Hoque, R. Julian, S. Bustamante,
S. Kirmani, S. Levine, S. Moore, S. Bahl, S. Dass, S. Sonawani, S. Song, S. Xu, S. Haldar,
S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S. Welker, S. Tian, S. Dasari, S. Belkhale, T. Osa,
T. Harada, T. Matsushima, T. Xiao, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell,
V. Jain, V. Vanhoucke, W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Wang, X. Zhu, X. Li, Y. Lu,
Y. Chebotar, Y. Zhou, Y. Zhu, Y. Xu, Y. Wang, Y. Bisk, Y. Cho, Y. Lee, Y. Cui, Y.-H. Wu, Y. Tang,
Y. Zhu, Y. Li, Y. Iwasawa, Y. Matsuo, Z. Xu, and Z. J. Cui. Open x-embodiment: Robotic learning
datasets and rt-x models, 2023.
V. Conitzer and T. Sandholm. Awesome: A general multiagent learning algorithm that converges in
self-play and learns a best response against stationary opponents, 2003.
V. Conitzer and T. Sandholm. New complexity results about nash equilibria. Games and
Economic Behavior, 63(2):621–641, 2008. ISSN 0899-8256. doi: https://doi.org/10.
1016/j.geb.2008.02.015. URL https://www.sciencedirect.com/science/article/pii/
S0899825608000936. Second World Congress of the Game Theory Society.
A. Cutkosky. Artificial constraints and hints for unbounded online learning. In A. Beygelzimer and
D. Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99
of Proceedings of Machine Learning Research, pages 874–894. PMLR, 25–28 Jun 2019. URL
https://proceedings.mlr.press/v99/cutkosky19a.html.
W. M. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and M. Jaderberg. Distilling
policy distillation. In The 22nd international conference on artificial intelligence and statistics,
pages 1331–1340. PMLR, 2019.
F. L. Da Silva, P. Hernandez-Leal, B. Kartal, and M. E. Taylor. Uncertainty-aware action advising
for deep reinforcement learning agents. In Proceedings of the AAAI conference on artificial
intelligence, volume 34, pages 5792–5799, 2020.
W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit quantile networks for distributional
reinforcement learning. pages 1096–1105, 2018a.
W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning
with quantile regression. 2018b.
S. Dalton, I. Frosio, and M. Garland. Gpu-accelerated atari emulation for reinforcement learning.
CoRR, abs/1907.08467, 2019. URL http://arxiv.org/abs/1907.08467.
C. Daskalakis. Equilibrium computation and machine learning, 2022.
36

C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization, 2018.
C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a nash
equilibrium. SIAM Journal on Computing, 39(1):195–259, 2009. doi: 10.1137/070699652. URL
https://doi.org/10.1137/070699652.
C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training gans with optimism, 2018. URL
https://arxiv.org/abs/1711.00141.
C. Daskalakis, N. Golowich, and K. Zhang. The complexity of markov equilibrium in stochastic
games, 2022.
P. Dayan. Improving generalization for temporal difference learning: The successor representation.
Neural Computation, 5(4):613–624, 1993. doi: 10.1162/neco.1993.5.4.613.
Y.-M. De Hauwere, P. Vrancx, and A. Nowé. Learning multi-agent state space representations. In
Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems:
volume 1-Volume 1, pages 715–722, 2010.
S. De Rooij, T. Van Erven, P. D. Grünwald, and W. M. Koolen. Follow the leader if you can, hedge if
you must. The Journal of Machine Learning Research, 15(1):1281–1316, 2014.
T. Degris, M. White, and R. S. Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012.
M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. pages 465–472, 2011.
M. Dorigo and C. Blum. Ant colony optimization theory: A survey. Theoretical computer science,
344(2-3):243–278, 2005.
Y. Du, L. Han, M. Fang, T. Dai, J. Liu, and D. Tao. Liir: Learning individual intrinsic reward in
multi-agent reinforcement learning. 2019.
T. Eccles, Y. Bachrach, G. Lever, A. Lazaridou, and T. Graepel. Biases for emergent communication
in multi-agent reinforcement learning. Advances in neural information processing systems, 32,
2019.
B. Ellis, S. Moalla, M. Samvelyan, M. Sun, A. Mahajan, J. N. Foerster, and S. Whiteson. Smacv2:
An improved benchmark for cooperative multi-agent reinforcement learning, 2022. URL https:
//arxiv.org/abs/2212.07489.
C. Erdogan and M. Veloso. Action selection via learning behavior patterns in multi-robot domains.
In Proc. International Joint Conference on Artificial Intelligence, pages 192–197. Citeseer, 2011.
L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
I. Dunning, S. Legg, and K. Kavukcuoglu. Impala: Scalable distributed deep-rl with importance
weighted actor-learner architectures, 2018.
A. Fachantidis, M. E. Taylor, and I. Vlahavas. Learning to teach reinforcement learning agents.
Machine Learning and Knowledge Extraction, 1(1):21–42, 2017.
V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E. Gonzalez, and S. Levine. Model-based value
estimation for efficient model-free reinforcement learning, 2018.
P. C. Fishburn, P. C. Fishburn, et al. Utility theory for decision making. 1979.
J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy
gradients, 2017a.
J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr, P. Kohli, and S. Whiteson. Stabilising
experience replay for deep multi-agent reinforcement learning. 70:1146–1155, 06–11 Aug 2017b.
URL https://proceedings.mlr.press/v70/foerster17b.html.
37

J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning, 2016.
J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with
opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017c.
J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with
opponent-learning awareness, 2018.
M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis,
O. Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017.
B. Freed, G. Sartoretti, J. Hu, and H. Choset. Communication learning via backpropagation in discrete
channels with unknown noise. Proceedings of the AAAI Conference on Artificial Intelligence, 34
(05):7160–7168, Apr. 2020. doi: 10.1609/aaai.v34i05.6205. URL https://ojs.aaai.org/
index.php/AAAI/article/view/6205.
C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax – a differentiable
physics engine for large scale rigid body simulation, 2021.
Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games and
Economic Behavior, 29(1-2):79–103, 1999.
W. Fu, C. Yu, Z. Xu, J. Yang, and Y. Wu. Revisiting some common practices in cooperative
multi-agent reinforcement learning, 2022.
D. Fudenberg and D. M. Kreps. Learning mixed equilibria. Games and economic behavior, 5(3):
320–367, 1993.
S. Fujimoto and S. S. Gu. A minimalist approach to offline reinforcement learning, 2021.
S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods, 2018a.
S. Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods, 2018b.
N. Fulda and D. Ventura. Predicting and preventing coordination problems in cooperative q-learning
systems. page 780–785, 2007.
Y. Gal, J. Hron, and A. Kendall. Concrete dropout, 2017.
D. Ghosh, C. Bhateja, and S. Levine. Reinforcement learning from passive data via latent intentions,
2023.
S. Ghosh. On the concept of dynamic multi-level simulation. In Proceedings of the 19th annual
symposium on Simulation, pages 201–205, 1986.
G. Gidel, R. A. Hemmat, M. Pezeshki, R. Le Priol, G. Huang, S. Lacoste-Julien, and I. Mitliagkas.
Negative momentum for improved game dynamics. In The 22nd International Conference on
Artificial Intelligence and Statistics, pages 1802–1811. PMLR, 2019.
I. Gilboa and E. Zemel. Nash and correlated equilibria: Some complexity considerations. Games
and Economic Behavior, 1(1):80–93, 1989. ISSN 0899-8256. doi: https://doi.org/10.1016/
0899-8256(89)90006-7. URL https://www.sciencedirect.com/science/article/pii/
0899825689900067.
I. Goodfellow, Y. Bengio, and A. Courville.
deeplearningarticle.org.

Deep learning.

2016.

http://www.

P.-P. Grassé. La reconstruction du nid et les coordinations interindividuelles chez bellicositermes
natalensis et cubitermes sp. la théorie de la stigmergie: Essai d’interprétation du comportement des
termites constructeurs. Insectes sociaux, 6:41–80, 1959.
A. Greenwald and K. Hall. Correlated-q learning. page 242–249, 2003.
38

S. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: A survey. Artif. Intell.
Rev., 55(2):895–943, feb 2022. ISSN 0269-2821. doi: 10.1007/s10462-021-09996-w. URL
https://doi.org/10.1007/s10462-021-09996-w.
S. Gu, T. Lillicrap, I. Sutskever, and S. Levine. Continuous deep q-learning with model-based
acceleration. pages 2829–2838, 2016.
S. Gu, T. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine. Q-prop: Sample-efficient policy
gradient with an off-policy critic, 2017.
S. Gu, J. G. Kuba, Y. Chen, Y. Du, L. Yang, A. Knoll, and Y. Yang. Safe multi-agent reinforcement
learning for multi-robot control. Artificial Intelligence, page 103905, 2023.
C. Guan, L. Zhang, C. Fan, Y. Li, F. Chen, L. Li, Y. Tian, L. Yuan, and Y. Yu. Efficient human-ai
coordination via preparatory language-based convention. arXiv preprint arXiv:2311.00416, 2023.
C. Guestrin, D. Koller, and R. Parr.
Multiagent planning with factored mdps.
14, 2001. URL https://proceedings.neurips.cc/paper_files/paper/2001/file/
7af6266cc52234b5aa339b16695f7fc4-Paper.pdf.
Y. Guo, J. Campbell, S. Stepputtis, R. Li, D. Hughes, F. Fang, and K. Sycara. Explainable action
advising for multi-agent reinforcement learning. In 2023 IEEE International Conference on
Robotics and Automation (ICRA), pages 5515–5521. IEEE, 2023.
A. Gupta, C. Devin, Y. Liu, P. Abbeel, and S. Levine. Learning invariant feature spaces to transfer
skills with reinforcement learning, 2017a.
J. K. Gupta, M. Egorov, and M. J. Kochenderfer. Cooperative multi-agent control using deep
reinforcement learning. 2017b.
D. Ha and J. Schmidhuber.
Recurrent world models facilitate policy evolution.
pages 2451–2463. Curran Associates, Inc., 2018.
URL https://papers.nips.cc/
paper/7512-recurrent-world-models-facilitate-policy-evolution.
https://
worldmodels.github.io.
T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based
policies. pages 1352–1361, 2017.
T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor, 2018.
T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta,
P. Abbeel, and S. Levine. Soft actor-critic algorithms and applications, 2019.
D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination, 2020.
D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models, 2022.
D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models,
2023.
I. T. Haman, V. C. Kamla, S. Galland, and J. C. Kamgang. Towards an multilevel agent-based model
for traffic simulation. Procedia Computer Science, 109:887–892, 2017.
J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and Z. Wang. Exploration in deep
reinforcement learning: From single-agent to multiagent domain. IEEE Transactions on Neural
Networks and Learning Systems, pages 1–21, 2023. doi: 10.1109/tnnls.2023.3236361. URL
https://doi.org/10.1109%2Ftnnls.2023.3236361.
P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity
problems: a survey of theory, algorithms and applications. Mathematical programming, 48(1):
161–220, 1990.
39

J. C. Harsanyi. Games with incomplete information played by “bayesian” players, i–iii part i. the
basic model. Management science, 14(3):159–182, 1967.
S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127–1150, 2000.
M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. 2015.
M. J. Hausknecht. Cooperation and communication in multiagent deep reinforcement learning. PhD
thesis, The University of Texas at Austin, 2016.
S. Havrylov and I. Titov. Emergence of language with multi-agent games: Learning to communicate
with sequences of symbols, 2017.
H. He, J. Boyd-Graber, K. Kwok, and H. Daumé III. Opponent modeling in deep reinforcement
learning. In International conference on machine learning, pages 1804–1813. PMLR, 2016.
J. Heinrich, M. Lanctot, and D. Silver. Fictitious self-play in extensive-form games. In International
conference on machine learning, pages 805–813. PMLR, 2015.
P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement
learning that matters, 2019.
P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. M. de Cote. A survey of learning in multiagent
environments: Dealing with non-stationarity. CoRR, abs/1707.09183, 2017. URL http://arxiv.
org/abs/1707.09183.
M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,
M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning, 2017.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
O. Holland and C. Melhuish. Stigmergy, self-organization, and sorting in collective robotics. Artificial
life, 5(2):173–202, 1999.
Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, and C.-Y. Lee. A deep policy inference q-network
for multi-agent systems, 2018.
R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. D. Turck, and P. Abbeel. Vime: Variational
information maximizing exploration, 2017.
G. Hu, Y. Zhu, D. Zhao, M. Zhao, and J. Hao. Event-triggered multi-agent reinforcement learning
with communication under limited-bandwidth constraint. arXiv preprint arXiv:2010.04978, 2020.
H. Hu, A. Lerer, B. Cui, D. Wu, L. Pineda, N. Brown, and J. Foerster. Off-belief learning, 2021.
J. Hu and M. P. Wellman. Nash q-learning for general-sum stochastic games. J. Mach. Learn. Res., 4
(null):1039–1069, dec 2003. ISSN 1532-4435.
D. Huh. Mix and mask actor-critic methods. ArXiv, abs/2106.13037, 2021. URL https://api.
semanticscholar.org/CorpusID:235691945.
D. Huh and P. Mohapatra. Decentralized multi-agent filtering, 2023.
D. Huh and P. Mohapatra. Isaacteams: Extending gpu-based physics simulator for multi-agent
learning. 1 2024a.
D. Huh and P. Mohapatra. Representation learning for efficient deep multi-agent reinforcement
learning, 2024b.
L. Hurwicz and S. Reiter. Designing economic mechanisms. Cambridge university press, 2006.
S. Iqbal and F. Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforcement
learning, 2021.
40

M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castañ eda, C. Beattie, N. C.
Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver,
D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3d multiplayer games
with population-based reinforcement learning. Science, 364(6443):859–865, may 2019. doi:
10.1126/science.aau6249. URL https://doi.org/10.1126%2Fscience.aau6249.
G. James, D. Witten, T. Hastie, R. Tibshirani, et al. An introduction to statistical learning, volume
112. Springer, 2013.
N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. A. Ortega, D. Strouse, J. Z. Leibo, and
N. de Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning,
2019.
S. Jelassi, C. Domingo-Enrich, D. Scieur, A. Mensch, and J. Bruna. Extragradient with player
sampling for faster nash equilibrium finding. In Proceedings of the International Conference on
Machine Learning, 2020.
A. X. Jiang, K. Leyton-Brown, and N. A. Bhat. Action-graph games. Games and Economic Behavior,
71(1):141–173, 2011.
J. Jiang and Z. Lu. Learning attentional communication for multi-agent cooperation, 2018.
J. Jiang, C. Dun, T. Huang, and Z. Lu. Graph convolutional reinforcement learning, 2020.
T. Jie and P. Abbeel. On a connection between importance sampling and the likelihood ratio
policy gradient. 23, 2010. URL https://proceedings.neurips.cc/paper/2010/file/
35cf8659cfcb13224cbd47863a34fc58-Paper.pdf.
J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang. Real-time bidding with multi-agent reinforcement learning in display advertising. oct 2018a. doi: 10.1145/3269206.3272021. URL
https://doi.org/10.1145%2F3269206.3272021.
P. Jin, K. Keutzer, and S. Levine. Regret minimization for partially observable deep reinforcement
learning. In International conference on machine learning, pages 2342–2351. PMLR, 2018b.
J. Jordan. Bayesian learning in normal form games. Games and Economic Behavior, 3(1):60–81,
1991. ISSN 0899-8256. doi: https://doi.org/10.1016/0899-8256(91)90005-Y. URL https:
//www.sciencedirect.com/science/article/pii/089982569190005Y.
D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk. In Handbook of
the fundamentals of financial decision making: Part I, pages 99–127. World Scientific, 2013.
S. M. Kakade. A natural policy gradient. Advances in neural information processing systems, 14,
2001.
S. M. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. 2002.
URL https://api.semanticscholar.org/CorpusID:31442909.
E. Kalai and E. Lehrer. Rational learning leads to nash equilibrium. Econometrica: Journal of the
Econometric Society, pages 1019–1045, 1993.
D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan,
V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic
manipulation. arXiv preprint arXiv:1806.10293, 2018.
S. Kapetanakis and D. Kudenko. Improving on the reinforcement learning of coordination in
cooperative multi-agent systems. 04 2003.
S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in
distributed reinforcement learning. 2018.
S. Kapturowski, V. Campos, R. Jiang, N. Rakićević, H. van Hasselt, C. Blundell, and A. P. Badia.
Human-level atari 200x faster, 2022.
41

D. Kim, S. Moon, D. Hostallero, W. J. Kang, T. Lee, K. Son, and Y. Yi. Learning to schedule
communication in multi-agent reinforcement learning, 2019.
D.-K. Kim, M. Riemer, M. Liu, J. N. Foerster, M. Everett, C. Sun, G. Tesauro, and J. P. How.
Influencing long-term behavior in multiagent reinforcement learning, 2022.
W. Kim, J. Park, and Y. Sung. Communication in multi-agent reinforcement learning: Intention
sharing. In International Conference on Learning Representations, 2020.
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks, 2017.
J. R. Kok, E. J. Hoen, B. Bakker, and N. Vlassis. Utile coordination: Learning interdependencies
among cooperative agents. In EEE Symp. on Computational Intelligence and Games, Colchester,
Essex, pages 29–36, 2005.
D. Koller and B. Milch. Multi-agent influence diagrams for representing and solving games. Games
and economic behavior, 45(1):181–221, 2003.
X. Kong, B. Xin, F. Liu, and Y. Wang. Revisiting the master-slave architecture in multi-agent deep
reinforcement learning, 2017.
G. M. Korpelevich. The extragradient method for finding saddle points and other problems. 1976.
URL https://api.semanticscholar.org/CorpusID:118602977.
I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning, 2021.
A. Koul. ma-gym: Collection of multi-agent environments based on openai gym. https://github.
com/koulanurag/ma-gym, 2019.
E. Koutsoupias and C. Papadimitriou. Worst-case equilibria. In Annual symposium on theoretical
aspects of computer science, pages 404–413. Springer, 1999.
D. Kovalev, A. Beznosikov, A. Sadiev, M. Persiianov, P. Richtárik, and A. Gasnikov. Optimal
algorithms for decentralized stochastic variational inequalities, 2023. URL https://arxiv.org/
abs/2202.02771.
L. Kraemer and B. Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing, 190:82–94, 2016. ISSN 0925-2312. doi: https://doi.org/10.1016/
j.neucom.2016.01.031. URL https://www.sciencedirect.com/science/article/pii/
S0925231216000783.
D. M. Kreps and R. Wilson. Sequential equilibria. Econometrica: Journal of the Econometric Society,
pages 863–894, 1982.
T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning,
2016.
A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement
learning, 2020.
K. Kurach, A. Raichuk, P. Stańczyk, M. Zajac,
˛ O. Bachem, L. Espeholt, C. Riquelme, D. Vincent,
M. Michalski, O. Bousquet, and S. Gelly. Google research football: A novel reinforcement learning
environment, 2020.
T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy
optimization, 2018.
J.-J. Laffont and D. Martimort. The theory of incentives: the principal-agent model. In The theory of
incentives. Princeton university press, 2009.
K.-H. Lai, D. Zha, Y. Li, and X. Hu. Dual policy distillation. arXiv preprint arXiv:2006.04061, 2020.
T. Lan, S. Srinivasa, H. Wang, and S. Zheng. Warpdrive: Extremely fast end-to-end deep multi-agent
reinforcement learning on a gpu, 2021.
42

M. Lanctot. Monte Carlo sampling and regret minimization for equilibrium computation and
decision-making in large extensive form games. University of Alberta (Canada), 2013.
M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Perolat, D. Silver, and T. Graepel. A
unified game-theoretic approach to multiagent reinforcement learning, 2017.
P. A. Lasota, T. Fong, and J. A. Shah. A survey of methods for safe human-robot interaction. Found.
Trends Robotics, 5:261–349, 2017. URL https://api.semanticscholar.org/CorpusID:
207179253.
M. Lauer and M. A. Riedmiller. An algorithm for distributed reinforcement learning in cooperative
multi-agent systems. page 535–542, 2000.
A. Lazaridou, K. M. Hermann, K. Tuyls, and S. Clark. Emergence of linguistic communication from
referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018.
M. Lechner, L. Yin, T. Seyde, T.-H. Wang, W. Xiao, R. Hasani, J. Rountree, and D. Rus. Gigastep one billion steps per second multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, 2023. URL https://openreview.net/forum?id=UgPAaEugH3.
S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and
perspectives on open problems. CoRR, abs/2005.01643, 2020. URL https://arxiv.org/abs/
2005.01643.
D. Lewis. Convention: A philosophical study. John Wiley & Sons, 2008.
S. Li, J. K. Gupta, P. Morales, R. Allen, and M. J. Kochenderfer. Deep implicit coordination graphs
for multi-agent reinforcement learning, 2021.
Y. Li, G. Xie, and Z. Lu. Difference advantage estimation for multi-agent policy gradients. 162:
13066–13085, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/li22w.html.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
T. Lin, J. Huh, C. Stauffer, S. N. Lim, and P. Isola. Learning to ground multi-agent communication
with autoencoders. Advances in Neural Information Processing Systems, 34:15230–15242, 2021.
M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. page 157–163,
1994.
M. L. Littman. Value-function reinforcement learning in markov games. Cognitive Systems Research,
2(1):55–66, 2001a. ISSN 1389-0417. doi: https://doi.org/10.1016/S1389-0417(01)00015-8. URL
https://www.sciencedirect.com/science/article/pii/S1389041701000158.
M. L. Littman. Friend-or-foe q-learning in general-sum games. page 322–328, 2001b.
B. Liu, Z. Pu, Y. Pan, J. Yi, Y. Liang, and D. Zhang. Lazy agents: a new perspective on solving sparse
reward problem in multi-agent reinforcement learning. In International Conference on Machine
Learning, pages 21937–21950. PMLR, 2023.
H. Liu, Y. Feng, Y. Mao, D. Zhou, J. Peng, and Q. Liu. Action-depedent control variates for policy
optimization via stein’s identity, 2018.
I.-J. Liu, U. Jain, R. A. Yeh, and A. G. Schwing. Cooperative exploration for multi-agent deep
reinforcement learning, 2021.
G. Loomes, C. Starmer, and R. Sugden. Do anomalies disappear in repeated markets? The Economic
Journal, 113(486):C153–C166, 2003.
R. Lowe, J. Foerster, Y.-L. Boureau, J. Pineau, and Y. Dauphin. On the pitfalls of measuring emergent
communication, 2019.
R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed
cooperative-competitive environments, 2020.
43

Y. Luo, H. Xu, Y. Li, Y. Tian, T. Darrell, and T. Ma. Algorithmic framework for model-based deep
reinforcement learning with theoretical guarantees, 2021.
N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and D. I. Kim. Applications of
deep reinforcement learning in communications and networking: A survey. IEEE Communications
Surveys and Tutorials, 21(4):3133–3174, 2019. doi: 10.1109/COMST.2019.2916583.
X. Lyu and C. Amato. Likelihood quantile networks for coordinating multi-agent reinforcement
learning, 2020.
Y. Ma, D. Jayaraman, and O. Bastani. Conservative offline distributional reinforcement learning. 34:
19235–19247, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/
file/a05d886123a54de3ca4b0985b718fb9b-Paper.pdf.
A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson. Maven: Multi-agent variational exploration,
2020.
V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,
A. Allshire, A. Handa, and G. State. Isaac gym: High performance gpu-based physics simulation
for robot learning, 2021.
L. Marsh and C. Onof. Stigmergic epistemology, stigmergic cognition. Cognitive Systems Research,
9(1-2):136–149, 2008.
M. J. Matarić. Learning to behave socially. 1994.
M. J. Matarić. Reinforcement learning in the multi-robot domain. Autonomous Robots, 4:73–83,
1997. URL https://api.semanticscholar.org/CorpusID:14610547.
L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Hysteretic q-learning : an algorithm for decentralized
reinforcement learning in cooperative multi-agent teams. pages 64–69, 2007. doi: 10.1109/IROS.
2007.4399095.
L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Independent reinforcement learners in cooperative
markov games: a survey regarding coordination problems. The Knowledge Engineering Review,
27(1):1–31, 2012. doi: 10.1017/S0269888912000057.
J. Maynard Smith. Evolution and the theory of games. American scientist, 64(1):41–45, 1976.
S. McAleer, J. B. Lanier, K. A. Wang, P. Baldi, and R. Fox. Xdo: A double oracle algorithm for
extensive-form games. Advances in Neural Information Processing Systems, 34:23128–23139,
2021.
H. B. McMahan, G. J. Gordon, and A. Blum. Planning in the presence of cost functions controlled
by an adversary. In Proceedings of the 20th International Conference on Machine Learning
(ICML-03), pages 536–543, 2003.
F. S. Melo and M. Veloso. Learning of coordination: Exploiting sparse interactions in multiagent
systems. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 2, pages 773–780. Citeseer, 2009.
F. S. Melo and M. Veloso. Decentralized mdps with sparse interactions. Artificial Intelligence, 175
(11):1757–1789, 2011.
R. Mirsky, I. Carlucho, A. Rahman, E. Fosong, W. Macke, M. Sridharan, P. Stone, and S. V. Albrecht.
A survey of ad hoc teamwork research, 2022.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning, 2016.
A. Mokhtari, A. Ozdaglar, and S. Pattathil. A unified analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach, 2019.
44

A. J. Monaco and T. Sabarwal. Games with strategic complements and substitutes. Economic Theory,
62:65–91, 2016.
R. B. Myerson. An introduction to game theory. 1985.
northwestern.edu/research/math/papers/623.pdf.

URL https://www.kellogg.

R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked distributed pomdps: A synthesis
of distributed constraint optimization and pomdps. Proceedings of the National Conference on
Artificial Intelligence, 1:133–139, 01 2005.
J. F. Nash. Non-cooperative games. Classics in Game Theory, 1951. URL https://api.
semanticscholar.org/CorpusID:264793164.
S. Nashed and S. Zilberstein. A survey of opponent modeling in adversarial domains. Journal of
Artificial Intelligence Research, 73:277–327, 2022.
K. K. Ndousse, D. Eck, S. Levine, and N. Jaques. Emergent social learning via multi-agent reinforcement learning. In International conference on machine learning, pages 7991–8004. PMLR,
2021.
H. Nekoei, X. Zhao, J. Rajendran, M. Liu, and S. Chandar. Towards few-shot coordination: Revisiting
ad-hoc teamplay challenge in the game of hanabi. In Conference on Lifelong Learning Agents,
pages 861–877. PMLR, 2023.
A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and
application to reward shaping. In Icml, volume 99, pages 278–287, 1999.
N. Nisan, T. Roughgarden, E. Tardos, and V. V. Vazirani. Algorithmic game theory. 2007.
S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent
reinforcement learning under partial observability, 2017.
S. Omidshafiei, D.-K. Kim, M. Liu, G. Tesauro, M. Riemer, C. Amato, M. Campbell, and J. P. How.
Learning to teach in cooperative multiagent reinforcement learning, 2018.
F. Orabona. A modern introduction to online learning, 2023.
M. J. Osborne. An introduction to game theory. 2009.
G. Palmer. Independent Learning Approaches: Overcoming Multi-Agent Learning Pathologies In
Team-Games. PhD thesis, University of Liverpool, 2020.
G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani. Lenient multi-agent deep reinforcement
learning, 2018.
G. Palmer, R. Savani, and K. Tuyls. Negative update intervals in deep multi-agent reinforcement
learning, 2019.
L. Pan, L. Huang, T. Ma, and H. Xu. Plan better amid conservatism: Offline multi-agent reinforcement
learning with actor rectification. In International conference on machine learning, pages 17221–
17237. PMLR, 2022.
L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous Agents
and Multi-Agent Systems, 11:387–434, 2005. URL https://api.semanticscholar.org/
CorpusID:19706.
G. Papoudakis, F. Christianos, A. Rahman, and S. V. Albrecht. Dealing with non-stationarity in
multi-agent deep reinforcement learning, 2019.
P. Peng, Y. Wen, Y. Yang, Q. Yuan, Z. Tang, H. Long, and J. Wang. Multiagent bidirectionallycoordinated nets: Emergence of human-level coordination in learning to play starcraft combat
games. arXiv preprint arXiv:1703.10069, 2017.
T. Phan, F. Ritz, P. Altmann, M. Zorn, J. Nüßlein, M. Kölle, T. Gabor, and C. Linnhoff-Popien.
Attention-based recurrence for multi-agent reinforcement learning under stochastic partial observability, 2023.
45

L. Prashanth, M. C. Fu, et al. Risk-sensitive reinforcement learning via policy gradient search.
Foundations and Trends® in Machine Learning, 15(5):537–693, 2022.
D. Premack and G. Woodruff. Does the chimpanzee have a theory of mind? Behavioral and Brain
Sciences, 1(4):515–526, 1978. doi: 10.1017/S0140525X00076512.
G. Qu, A. Wierman, and N. Li. Scalable reinforcement learning for multi-agent networked systems,
2021.
R. Raileanu, E. Denton, A. Szlam, and R. Fergus. Modeling others using oneself in multi-agent
reinforcement learning. In International conference on machine learning, pages 4257–4266.
PMLR, 2018.
A. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences, 2013.
T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. Foerster, and S. Whiteson. Qmix: Monotonic
value function factorisation for deep multi-agent reinforcement learning, 2018.
B. Recht. A tour of reinforcement learning: The view from continuous control. Annual Review of
Control, Robotics, and Autonomous Systems, 2(1):253–279, 2019.
M. Resnick. Beyond the centralized mindset–explorations in massively-parallel microworlds. 08
2005.
J. J. Robinson. An iterative method of solving a game. Classics in Game Theory, 1951.
S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured prediction
to no-regret online learning, 2011.
A. Rutherford, B. Ellis, M. Gallici, J. Cook, A. Lupu, G. Ingvarsson, T. Willi, A. Khan, C. S. de Witt,
A. Souly, S. Bandyopadhyay, M. Samvelyan, M. Jiang, R. T. Lange, S. Whiteson, B. Lacerda,
N. Hawes, T. Rocktaschel, C. Lu, and J. N. Foerster. Jaxmarl: Multi-agent rl environments in jax.
arXiv preprint arXiv:2311.10090, 2023.
B. Salanié. The economics of contracts: a primer. MIT press, 2005.
M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J. Rudner, C.-M. Hung, P. H. S.
Torr, J. Foerster, and S. Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043,
2019.
P. Santos, J. Ribeiro, A. Sardinha, and F. Melo. Ad hoc teamwork in the presence of non-stationary
teammates. 09 2021.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay, 2016.
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization,
2017a.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms, 2017b.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation, 2018.
D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine. GNM: A General Navigation Model to
Drive Any Robot. In International Conference on Robotics and Automation (ICRA), 2023a. URL
https://arxiv.org/abs/2210.03370.
D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine. ViNT: A
foundation model for visual navigation. In 7th Annual Conference on Robot Learning, 2023b.
URL https://arxiv.org/abs/2306.14846.
46

S. Shalev-Shwartz, S. Shammah, and A. Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving, 2016.
S. Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and Trends®
in Machine Learning, 4(2):107–194, 2012.
J. Shao, Z. Lou, H. Zhang, Y. Jiang, S. He, and X. Ji. Self-organized group for cooperative multi-agent
reinforcement learning. Advances in Neural Information Processing Systems, 35:5711–5723, 2022.
L. S. Shapley. A value for n-person games. 1952. doi: 10.7249/P0295.
L. S. Shapley. Stochastic games*. Proceedings of the National Academy of Sciences, 39(10):1095–
1100, 1953. doi: 10.1073/pnas.39.10.1095. URL https://www.pnas.org/doi/abs/10.1073/
pnas.39.10.1095.
Y. Shoham and K. Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical
foundations. 2008.
Y. Shoham, R. Powers, and T. Grenager. Multi-agent reinforcement learning:a critical survey. 2003.
Y. Shoham, R. Powers, and T. Grenager. If multi-agent learning is the answer, what is the question?
Artificial Intelligence, 171:365–377, 05 2007. doi: 10.1016/j.artint.2006.02.006.
F. Silva and A. Costa. A survey on transfer learning for multiagent reinforcement learning systems.
Journal of Artificial Intelligence Research, 64, 03 2019. doi: 10.1613/jair.1.11396.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient
algorithms. 32(1):387–395, 22–24 Jun 2014. URL https://proceedings.mlr.press/v32/
silver14.html.
A. Singh, T. Jain, and S. Sukhbaatar. Learning when to communicate at scale in multiagent cooperative
and competitive tasks, 2018.
S. Singh, M. Kearns, and Y. Mansour. Nash convergence of gradient dynamics in general-sum games.
page 541–548, 2000.
N. Spanoudakis and P. Moraitis. Using aseme methodology for model-driven agent systems development. In International workshop on agent-oriented software engineering, pages 106–127. Springer,
2010.
P. Stone and M. Veloso. Multiagent systems: A survey from a machine learning perspective.
Autonomous Robots, 8, 05 2000. doi: 10.1023/A:1008942012299.
P. Stone, G. A. Kaminka, S. Kraus, and J. S. Rosenschein. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. page 1504–1509, 2010.
S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation,
2016.
W.-F. Sun, C.-K. Lee, and C.-Y. Lee. Dfac framework: Factorizing the value function via quantile
mixture for multi-agent distributional q-learning, 2021.
P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for cooperative
multi-agent learning, 2017.
R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. pages 216–224. Elsevier, 1990.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. 2018.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. 12, 1999a. URL https://proceedings.neurips.cc/
paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.
47

R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999b.
O. Tammelin. Solving large imperfect information games using cfr+, 2014.
A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente. Multiagent
cooperation and competition with deep reinforcement learning, 2015.
M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents, 1997.
I. H. Tchappi, S. Galland, V. C. Kamla, and J. C. Kamgang. A brief review of holonic multi-agent
models for traffic and transportation systems. Procedia computer science, 134:137–144, 2018.
J. Terry, B. Black, N. Grammel, M. Jayakumar, A. Hari, R. Sullivan, L. S. Santos, C. Dieffendahl,
C. Horsch, R. Perez-Vicente, et al. Pettingzoo: Gym for multi-agent reinforcement learning.
Advances in Neural Information Processing Systems, 34:15032–15043, 2021.
G. Theraulaz and E. Bonabeau. A brief history of stigmergy. Artificial life, 5(2):97–116, 1999.
Q. Tian, K. Kuang, F. Liu, and B. Wang. Learning from good trajectories in offline multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,
pages 11672–11680, 2023.
A. Touati and Y. Ollivier. Learning one representation to optimize all rewards, 2021.
J. Treutlein, M. Dennis, C. Oesterheld, and J. Foerster. A new formalism, method and open issues for
zero-shot coordination. In International Conference on Machine Learning, pages 10413–10423.
PMLR, 2021.
W.-C. Tseng, T.-H. J. Wang, Y.-C. Lin, and P. Isola. Offline multi-agent reinforcement learning with
knowledge distillation. Advances in Neural Information Processing Systems, 35:226–237, 2022.
G. Tucker, S. Bhupatiraju, S. Gu, R. E. Turner, Z. Ghahramani, and S. Levine. The mirage of
action-dependent baselines in reinforcement learning, 2018.
K. Tumer and A. Agogino. Distributed agent-based air traffic flow management. In Proceedings of
the 6th international joint conference on Autonomous agents and multiagent systems, pages 1–8,
2007.
K. Tuyls and G. Weiss. Multiagent learning: Basics, challenges, and prospects. Ai Magazine, 33:
41–52, 12 2012. doi: 10.1609/aimag.v33i3.2426.
K. Tuyls, P. J. T. Hoen, and B. Vanschoenwinkel. An evolutionary dynamical analysis of multi-agent
learning in iterated games. Autonomous Agents and Multi-Agent Systems, 12:115–153, 2006.
A. Tversky and D. Kahneman. Advances in prospect theory: Cumulative representation of uncertainty.
Journal of Risk and uncertainty, 5:297–323, 1992.
H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. 30(1),
2016.
H. van Hasselt, Y. Doron, F. Strub, M. Hessel, N. Sonnerat, and J. Modayil. Deep reinforcement
learning and the deadly triad, 2018.
N. Vlassis, R. Elhorst, and J. Kok. Anytime algorithms for multiagent decision making using
coordination graphs. 1:953–957 vol.1, 2004. doi: 10.1109/ICSMC.2004.1398426.
H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong,
A. He, V. Myers, K. Fang, C. Finn, and S. Levine. Bridgedata v2: A dataset for robot learning at
scale. In Conference on Robot Learning (CoRL), 2023.
L. Wang, Y. Zhang, Y. Hu, W. Wang, C. Zhang, Y. Gao, J. Hao, T. Lv, and C. Fan. Individual reward
assisted multi-agent reinforcement learning. In International Conference on Machine Learning,
pages 23417–23432. PMLR, 2022.
48

T. Wang, T. Gupta, A. Mahajan, B. Peng, S. Whiteson, and C. Zhang. Rode: Learning roles to
decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020a.
Y. Wang, B. Han, T. Wang, H. Dong, and C. Zhang. Off-policy multi-agent decomposed policy
gradients, 2020b.
Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample
efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016a.
Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network architectures
for deep reinforcement learning. pages 1995–2003, 2016b.
C. Watkins. Learning from delayed rewards. 01 1989.
C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8:279–292, 1992.
M. Watter, J. T. Springenberg, J. Boedecker, and M. A. Riedmiller. Embed to control: A locally
linear latent dynamics model for control from raw images. CoRR, abs/1506.07365, 2015. URL
http://arxiv.org/abs/1506.07365.
T. Weber, S. Racanière, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals,
N. Heess, Y. Li, R. Pascanu, P. Battaglia, D. Hassabis, D. Silver, and D. Wierstra. Imaginationaugmented agents for deep reinforcement learning, 2018.
C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Linear last-iterate convergence in constrained saddlepoint optimization, 2021.
E. Wei and S. Luke. Lenient learning in independent-learner stochastic cooperative games. Journal
of Machine Learning Research, 17(84):1–42, 2016. URL http://jmlr.org/papers/v17/
15-417.html.
M. Weinberg and J. Rosenschein. Best-response multiagent learning in non-stationary environments.
pages 506–513, 2004.
S. Whiteson. Factored value functions for cooperative multi-agent reinforcement learning. MIT
Embodied Intelligence Seminars, 2020.
R. J. Williams. Toward a theory of reinforcement-learning connectionist systems. Technical Report
NU-CCS-88-3, Northeastern University, College of Computer Science, 1988.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8(3–4):229–256, may 1992. ISSN 0885-6125. doi: 10.1007/BF00992696.
URL https://doi.org/10.1007/BF00992696.
E. O. Wilson. Sociobiology. Harvard University Press, 2000.
D. H. Wolpert and K. Tumer. Optimal payoff functions for members of collectives. Advances in
Complex Systems, 4(02n03):265–279, 2001.
Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation, 2017.
Y. Xiao, W. Tan, and C. Amato. Asynchronous actor-critic for multi-agent reinforcement learning.
Advances in Neural Information Processing Systems, 35:4385–4400, 2022.
K. Xu, C. Li, Y. Tian, T. Sonobe, K. ichi Kawarabayashi, and S. Jegelka. Representation learning on
graphs with jumping knowledge networks, 2018.
D. Yang, L. Zhao, Z. Lin, T. Qin, J. Bian, and T.-Y. Liu. Fully parameterized quantile function for
distributional reinforcement learning. Advances in neural information processing systems, 32:
6193–6202, 2019.
M. Yang, J. Zhao, X. Hu, W. Zhou, J. Zhu, and H. Li. Ldsa: Learning dynamic subtask assignment
in cooperative multi-agent reinforcement learning. Advances in Neural Information Processing
Systems, 35:1698–1710, 2022.
49

T. Yang, W. Wang, H. Tang, J. Hao, Z. Meng, H. Mao, D. Li, W. Liu, Y. Chen, Y. Hu, et al. An
efficient transfer learning framework for multiagent reinforcement learning. Advances in neural
information processing systems, 34:17037–17048, 2021a.
Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang, J. Yang, and Q. Zhao. Believe what you see:
Implicit constraint approach for offline multi-agent reinforcement learning. Advances in Neural
Information Processing Systems, 34:10299–10312, 2021b.
C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness of
ppo in cooperative, multi-agent games, 2022a.
C. Yu, J. Gao, W. Liu, B. Xu, H. Tang, J. Yang, Y. Wang, and Y. Wu. Learning zero-shot cooperation
with humans, assuming humans are biased. arXiv preprint arXiv:2302.01605, 2023.
T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn. Combo: Conservative offline
model-based policy optimization, 2022b.
X. Yu, J. Jiang, W. Zhang, H. Jiang, and Z. Lu. Model-based opponent modeling. Advances in Neural
Information Processing Systems, 35:28208–28221, 2022c.
M. S. Zaïem and E. Bennequin. Learning to communicate in multi-agent reinforcement learning : A
review, 2019.
C. Zhang and J. A. Shah.
Fairness in multi-agent sequential decision-making.
27,
2014.
URL https://proceedings.neurips.cc/paper_files/paper/2014/file/
792c7b5aae4a79e78aaeda80516ae2ac-Paper.pdf.
K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Başar. Fully decentralized multi-agent reinforcement
learning with networked agents, 2018.
M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Solar: Deep structured
representations for model-based reinforcement learning. pages 7444–7453, 2019.
Y. Zhang, Q. Yang, D. An, and C. Zhang. Coordination between individual agents in multi-agent
reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):
11387–11394, May 2021. doi: 10.1609/aaai.v35i13.17357. URL https://ojs.aaai.org/
index.php/AAAI/article/view/17357.
J. Zhao, M. Yang, Y. Zhao, X. Hu, W. Zhou, J. Zhu, and H. Li. Mcmarl: Parameterizing value
function via mixture of categorical distributions for multi-agent reinforcement learning, 2022.
L. Zheng, J. Yang, H. Cai, W. Zhang, J. Wang, and Y. Yu. Magent: A many-agent reinforcement
learning platform for artificial collective intelligence, 2017.
L. Zheng, J. Chen, J. Wang, J. He, Y. Hu, Y. Chen, C. Fan, Y. Gao, and C. Zhang. Episodic multi-agent
reinforcement learning with curiosity-driven exploration, 2021.
Y. Zheng, J. Hao, and Z. Zhang. Weighted double deep multiagent reinforcement learning in stochastic
cooperative environments, 2018.
M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao, W. Zhang, M. Alban, I. Fadakar, Z. Chen,
A. C. Huang, Y. Wen, K. Hassanzadeh, D. Graves, D. Chen, Z. Zhu, N. Nguyen, M. Elsayed,
K. Shao, S. Ahilan, B. Zhang, J. Wu, Z. Fu, K. Rezaee, P. Yadmellat, M. Rohani, N. P. Nieves,
Y. Ni, S. Banijamali, A. C. Rivers, Z. Tian, D. Palenicek, H. bou Ammar, H. Zhang, W. Liu, J. Hao,
and J. Wang. Smarts: Scalable multi-agent reinforcement learning training school for autonomous
driving, 2020.
C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep reinforcement learning with
communication. Autonomous Agents and Multi-Agent Systems, 38(1):4, 2024.
Z. Zhu, E. Biyik, and D. Sadigh. Multi-agent safe planning with gaussian processes. oct
2020. doi: 10.1109/iros45743.2020.9341169. URL https://doi.org/10.1109%2Firos45743.
2020.9341169.
50

M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. page
928–935, 2003.
M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with
incomplete information. 20, 2007. URL https://proceedings.neurips.cc/paper_files/
paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf.
T. Zrnic, E. Mazumdar, S. S. Sastry, and M. I. Jordan. Who leads and who follows in strategic
classification?, 2022.

51

