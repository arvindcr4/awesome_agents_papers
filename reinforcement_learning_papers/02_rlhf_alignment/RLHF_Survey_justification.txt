Paper: RLHF_Survey.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
A Survey of Reinforcement Learning from Human Feedback Timo Kaufmann  timo.kaufmann@ifi.lmu.de  LMU Munich, MCML Munich  Paul Weng  paul.weng@dukekunshan.edu.cn  Digital Innovation Research Center, Duke Kunshan University  Viktor Bengs  viktor.bengs@dfki.de  arXiv:2312.14925v3 [cs.LG] 28 Dec 2025  German Research Center for Artificial Intelligence (DFKI)  Eyke Hüllermeier  eyke@lmu.de  LMU Munich, MCML Munich, DFKI Kaiserslautern  Abstract Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engin...

Specific Evidence for Classification:
- "A Survey of Reinforcement Learning from Human Feedback Timo Kaufmann  timo.kaufmann@ifi.lmu.de  LMU Munich, MCML Munich  Paul Weng  paul.weng@dukekunshan.edu.cn  Digital Innovation Research Center, Duke Kunshan University  Viktor Bengs  viktor.bengs@dfki.de  arXiv:2312.14925v3 [cs.LG] 28 Dec 2025  German Research Center for Artificial Intelligence (DFKI)  Eyke Hüllermeier  eyke@lmu.de  LMU Munich, MCML Munich, DFKI Kaiserslautern  Abstract Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function."
- "Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
