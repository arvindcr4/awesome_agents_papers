Here is the extracted information:

**PAPER:** Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization
**TITLE:** Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization
**ARXIV_ID:** 2412.07639v2
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** This paper proposes a novel algorithm called In-Sample Sequential Policy Optimization (InSPO) for offline multi-agent reinforcement learning. InSPO uses sequential updates to avoid out-of-distribution (OOD) joint actions and introduces policy entropy to ensure comprehensive exploration of the dataset.
**KEY_CONTRIBUTIONS:**
* InSPO addresses the OOD joint actions issue and local optimum convergence issue in offline MARL.
* The algorithm uses sequential updates to avoid conflicting update directions and incorporates policy entropy to prevent premature convergence to local optima.
* InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE).
* Experimental results demonstrate the effectiveness of InSPO compared to current state-of-the-art offline MARL methods.
