Paper: RL_from_Human_Preferences.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
arXiv:1706.03741v4 [stat.ML] 17 Feb 2023  Deep Reinforcement Learning from Human Preferences Paul F Christiano OpenAI paul@openai.com Miljan Martic DeepMind miljanm@google.com  Jan Leike DeepMind leike@google.com Shane Legg DeepMind legg@google.com  Tom B Brown nottombrown@gmail.com  Dario Amodei OpenAI damodei@openai.com  Abstract For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of traject...

Specific Evidence for Classification:
- "arXiv:1706.03741v4 [stat.ML] 17 Feb 2023  Deep Reinforcement Learning from Human Preferences Paul F Christiano OpenAI paul@openai.com Miljan Martic DeepMind miljanm@google.com  Jan Leike DeepMind leike@google.com Shane Legg DeepMind legg@google.com  Tom B Brown nottombrown@gmail.com  Dario Amodei OpenAI damodei@openai.com  Abstract For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems."
- "In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
