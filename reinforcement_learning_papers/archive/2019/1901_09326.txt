Value Propagation for Decentralized Networked Deep
Multi-agent Reinforcement Learning
Chao Qu ∗1 , Shie Mannor2 , Huan Xu3,4 , Yuan Qi1 , Le Song1,4 , and Junwu Xiong1

arXiv:1901.09326v4 [cs.LG] 28 Sep 2019

1

Ant Financial Services Group
2
Technion
3
Alibaba Group
4
Georgia Institute of Technology

Abstract
We consider the networked multi-agent reinforcement learning (MARL) problem
in a fully decentralized setting, where agents learn to coordinate to achieve joint
success. This problem is widely encountered in many areas including traffic control,
distributed control, and smart grids. We assume each agent is located at a node
of a communication network and can exchange information only with its neighbors. Using softmax temporal consistency, we derive a primal-dual decentralized
optimization method and obtain a principled and data-efficient iterative algorithm
named value propagation. We prove a non-asymptotic convergence rate of O(1/T )
with nonlinear function approximation. To the best of our knowledge, it is the
first MARL algorithm with a convergence guarantee in the control, off-policy,
non-linear function approximation, fully decentralized setting.

1

Introduction

Multi-agent systems have applications in a wide range of areas such as robotics, traffic control,
distributed control, telecommunications, and economics. For these areas, it is often difficult or
simply impossible to predefine agents’ behaviour to achieve satisfactory results, and multi-agent
reinforcement learning (MARL) naturally arises [Bu et al., 2008, Tan, 1993]. For example, ElTantawy et al. [2013] model a traffic signal control problem as a multi-player stochastic game and
solve it with MARL. MARL generalizes reinforcement learning by considering a set of agents
(decision makers) sharing a common environment. However, multi-agent reinforcement learning
is a challenging problem since the agents interact with both the environment and each other. For
instance, independent Q-learning—treating other agents as a part of the environment—often fails
as the multi-agent setting breaks the theoretical convergence guarantee of Q-learning and makes
the learning process unstable [Tan, 1993]. Rashid et al. [2018], Foerster et al. [2018], Lowe et al.
[2017] alleviate such a problem using a centralized network (i.e., being centralized for training, but
decentralized during execution.). Its communication pattern is illustrated in the left panel of Figure 1.
Despite the great success of (partially) centralized MARL approaches, there are various scenarios,
such as sensor networks [Rabbat and Nowak, 2004] and intelligent transportation systems [Adler
and Blue, 2002] , where a central agent does not exist or may be too expensive to use. In addition,
privacy and security are requirements of many real world problems in multi-agent system (also in
many modern machine learning problems) [Abadi et al., 2016, Kurakin et al., 2016] . For instance,
in Federated learning [McMahan et al., 2016], the learning task is solved by a lose federation of
participating devices (agents) without the need to centrally store the data, which significantly reduces
∗

luoji.qc@antfin.com

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

privacy and security risk by limiting the attack surface to only the device. In the agreement problem
[DeGroot, 1974, Mo and Murray, 2017], a group of agents may want to reach consensus on a subject
without leaking their individual goal or opinion to others. Obviously, centralized MARL violates
privacy and security requirements. To this end, we and others have advocated the fully decentralized
approaches, which are useful for many applications including unmanned vehicles [Fax and Murray,
2002], power grid [Callaway and Hiskens, 2011], and sensor networks [Cortes et al., 2004]. For
these approaches, we can use a network to model the interactions between agents (see the right panel
of Figure 1). Particularly, We consider a fully cooperative setting where each agent makes its own
decision based on its local reward and messages received from their neighbors. Thus each agent
preserves the privacy of its own goal and policy. At the same time, through the message-passing all
agents achieve consensus to maximize the averaged cumulative rewards over all agents; see Equation
(3).
In this paper, we propose a new fully decentralized networked multi-agent deep reinforcement learning algorithm. Using softmax temporal consistency [Nachum et al., 2017, Dai
et al., 2018] to connect value and policy updates,
we derive a new two-step primal-dual decentralized reinforcement learning algorithm inspired
by a primal decentralized optimization method
[Hong et al., 2017] 2 . In the first step of each iteration, each agent computes its local policy, value Figure 1: Centralized network vs Decentralized
gradients and dual gradients and then updates network. Each blue node in the figure corresponds
only policy parameters. In the second step, each to an agent. In centralized network (left), the red
agent propagates to its neighbors the messages central node collects information for all agents,
based on its value function (and dual function) while in decentralized network (right), agents exand then updates its own value function. Hence changes information with neighbors.
we name the algorithm value propagation. It
preserves the privacy in the sense that no individual reward function is required for the network-wide collaboration. We approximate the local
policy, value function and dual function of each agent by deep neural networks, which enables
automatic feature generation and end-to-end learning.
Contributions: [1] We propose the value propagation algorithm and prove that it converges with the
rate O(1/T ) even with the non-linear deep neural network function approximation. To the best of
our knowledge, it is the first deep MARL algorithm with non-asymptotic convergence guarantee. At
the same time, value propagation can use off-policy updates, making it data efficient. When it reduces
to the single agent case, it provides a proof of [Dai et al., 2018] in the realistic setting; see remarks of
algorithm 1 in Section 3.3. [2] The objective function in our problem is a primal-dual decentralized
optimization form (see (8)), while the objective function in [Hong et al., 2017] is a primal problem.
When our method reduces to pure primal analysis, we extend [Hong et al., 2017] to the stochastic
and biased gradient setting which may be of independent interest to the optimization community. In
the practical implementation, we extend ADAM into the decentralized setting to accelerate training.

2

Preliminaries

MDP Markov Decision Process (MDP) can be described by a 5-tuple (S, A, R, P, γ): S is the finite
state space, A is the finite action space, P = (P (s0 |s, a))s,s0 ∈S,a∈A are the transition probabilities,
R = (R(s, a))s,s0 ∈S,a∈A are the real-valued immediate rewards and γ ∈ (0, 1) is the discount factor.
A policy is used to select actions in the MDP. In general, the policy is stochastic and denoted by π,
∗
where π(s
t , at ) is the conditional probability density at at associated with the policy. Define V (s) =
P
∞
t
∗
maxπ E[ t=0 γ R(st , at )|s0 = s] to be the optimal value function. It is known that V is the unique
fixed point of the Bellman optimality operator, V (s) = (T V )(s) := maxa R(s, a)+γEs0 |s,a [V (s0 )].
The optimal policy π ∗ is related to V ∗ by the following equation: π ∗ (s, a) = arg maxa {R(s, a) +
γEs0 |s,a V ∗ (s0 )}
2
The objective in Hong et al. [2017] is a primal optimization problem with constraint. Thus they introduce a
Lagrange multiplier like method to solve it (so they call it primal-dual method ). Our objective function is a
primal-dual optimization problem with constraint.

2

Softmax Temporal Consistency Nachum et al. [2017] establish a connection between value and
policy based reinforcement learning based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Particularly, the soft Bellman optimality is
as follows,

Vλ (s) = max Ea∼π(s,·) (R(s, a) + γEs0 |s,a Vλ (s0 )) + λH(π, s) ,
(1)
P π(s,·)
where H(π, s) = − a∈A π(s, a) log π(s, a) and λ ≥ 0 controls the degree of regularization. When
λ = 0, above equation reduces to the standard Bellman optimality condition. An important property
of soft Bellman optimality is the called temporal consistency, which leads to the Path Consistency
Learning.
Proposition 1. [Nachum et al., 2017]. Assume λ > 0. Let Vλ∗ be the fixed point of (1) and
πλ∗ be the corresponding policy that attains that maximum on the RHS of (1). Then, (Vλ∗ , πλ∗ )
is the unique (V, π) pair that satisfies the following equation for all (s, a) ∈ S × A : V (s) =
R(s, a) + γEs0 |s,a V (s0 ) − λ log π(s, a).
A straightforward way to apply temporal consistency is to optimize the following problem,
2
minV,π Es,a R(s, a) + γEs0 |s,a V (s0 ) − λ log π(s, a) − V (s) . Dai et al. [2018] get around the
double sampling problem of above formulation by introduce a primal-dual form
min max Es,a,s0 [(δ(s, a, s0 ) − V (s))2 ] − ηEs,a,s0 [(δ(s, a, s0 ) − ρ(s, a))2 ],
(2)
V,π

ρ

0

where δ(s, a, s ) = R(s, a) + γV (s0 ) − λ log π(s, a), 0 ≤ η ≤ 1 controls the trade-off between bias
and variance.
In the following discussion, we use k · k to denote the Euclidean norm over the vector, A0 stands for
the transpose of A, and denotes the entry-wise product between two vectors.

3

Value Propagation

In this section, we present our multi-agent reinforcement learning algorithm, i.e., value propagation.
To begin with, we extend the MDP model to the Networked Multi-agent MDP model following
the definition in [Zhang et al., 2018]. Let G = (N , E) be an undirected graph with |N | = N
agents (node). E represents the set of edges. (i, j) ∈ E means agent i and j can communicate
with each other through this edge. A networked multi-agent MDP is characterized by a tuple
(S, {Ai }i∈N , P, {Ri }i∈N , G, γ): S is the global state space shared by all agents (It could be partially
observed, i.e., each agent observes its own state S i , see our experiment). Ai is the action space
QN
of agent i, A = i=1 Ai is the joint action space, P is the transition probability, Ri denotes
the local reward function of agent i. We assume rewards are observed only locally to preserve
the privacy of the each agent’s goal. At each time step, agents observe st and make the decision
at = (at1 , at2 , ..., atN ). Then each agent just receives its own reward Ri (st , at ), and the environment
switches to the new state st+1 according to the transition probability. Furthermore, since each agent
make the decisions independently, it is reasonable to assume that the policy π(s, a) can be factorized,
QN
i.e., π(s, a) = i=1 π i (s, ai ) [Zhang et al., 2018]. We call our method fully-decentralized method,
since reward is received locally, the action is executed locally by agent, critic (value function) are
trained locally.
3.1

Multi-Agent Softmax Temporal Consistency

The goal of the agents is to learn a policy that maximizes the long-term reward averaged over the
agent, i.e.,
∞
N
X
1 X t
E
γ Ri (st , at ).
(3)
N i=1
t=0
In the following, we adapt the temporal consistency into the multi-agent version. Let Vλ (s) =

PN
E N1 i=1 Ri (s, a) + γEs0 |s,a Vλ (s0 ) + λH(π, s) , Vλ∗ be the optimal value function and πλ∗ (s, a) =
QN i∗
i
i=1 πλ (s, a ) be the corresponding policy. Apply the soft temporal consistency, we obtain that for
3

all (s, a) ∈ S × A, (Vλ∗ , πλ∗ ) is the unique (V, π) pair that satisfies
V (s) =

N
N
X
1 X
Ri (s, a) + γEs0 |s,a V (s0 ) − λ
log π i (s, ai ).
N i=1
i=1

(4)

A optimization problem inspired by (4) would be
min

{π i }N
i=1 ,V

E V (s) −

N
N
X
2
1 X
Ri (s, a) − γEs0 |s,a V (s0 ) + λ
log π i (s, ai ) .
N i=1
i=1

(5)

There are two potential issues of above formulation: First, due to the inner conditional expectation, it
would require two independent samples to obtain the unbiased estimation of gradient of V [Dann
et al., 2014]. Second, V (s) is a global variable over the network, thus can not be updated in a
decentralized way.
For the first issue, we introduce the primal-dual form of (5) as that in [Dai et al., 2018]. Using the
fact that x2 = maxν (2νx − ν 2 ) and the interchangeability principle [Shapiro et al., 2009] we have,
min

V,{π i }N
i=1

max 2Es,a,s0 [ν(s, a)
ν

N

1 X
(Ri (s, a)+γV (s0 )−V (s)−λN log π i (s, ai ) ]−Es,a,s [ν 2 (s, a)].
N i=1

Change the variable ν(s, a) = ρ(s, a) − V (s), the objective function becomes
N
N
2
2
1 X
1 X
(δi (s, a, s0 ) − V (s)) ] − Es,a,s0 [
(δi (s, a, s0 ) − ρ(s, a)) ],
N
i
ρ
N i=1
N i=1
V,{π }i=1
(6)
where δi = Ri (s, a) + γV (s0 ) − λN log π i (s, ai ).

min

3.2

max Es,a,s0 [

Decentralized Formulation

So far the problem is still in a centralized form, and we now turn to reformulating it in a decentralized
way. We assume that policy, value function, dual variable ρ are all in the parametric function class.
QN
Particularly, each agent’s policy is π i (s, ai ) := πθπi (s, ai ) and πθ (s, a) = i=1 πθπi (s, ai ). The
value function Vθv (s) is characterized by the parameter θv , while θρ represents the parameter of
ρ(s, a). Similar to [Dai et al., 2018], we optimize a slightly different version from (6).
min

θv ,{θπi }N
i=1

max Es,a,s0 [
θρ

N
N
2
2
1 X
1 X
(δi (s, a, s0 ) − V (s)) ] − ηEs,a,s0 [
(δi (s, a, s0 ) − ρ(s, a)) ],
N i=1
N i=1
(7)

where 0 ≤ η ≤ 1 controls the bias and variance trade-off. When η = 0, it reduces to the pure primal
form.
We now consider the second issue that V (s) is a global variable. To address this problem, we introduce
the local copy of the value function, i.e., Vi (s) for each agent i. In the algorithm, we have a consensus
update step, such that these local copies are the same, i.e., V1 (s) = V2 (s) = ... = VN (s) = V (s), or
equivalently θv1 = θv2 = ... = θvN , where θvi are parameter of Vi respectively. Notice now in (7),
there is a global dual variable ρ in the primal-dual form. Therefore, we also introduce the local copy
of the dual variable, i.e., ρi (s, a) to formulate it into the decentralized optimization problem. Now
the final objective function we need to optimize is
N

min

2
1 X
(δi (s, a, s0 ) − Vi (s)) ]
N i=1

max L(θV , θπ , θρ ) = Es,a,s0 [

N
{θvi ,θπi }N
i=1 {θρi }i=1

− ηE

s,a,s0

s.t. θv1 =, ..., = θvN , θρ1 =, ..., = θρN ,
0

i

i

N
2
1 X
(δi (s, a, s0 ) − ρi (s, a)) ],
[
N i=1

(8)

where δi = Ri (s, a) + γVi (s ) − λN log π (s, a ). We are now ready to present the value propagation
algorithm. In the following, for notational simplicity, we assume the parameter of each agent is a
4

scalar, i.e., θρi , θπi , θvi ∈ R. We pack the parameter together and slightly abuse the notation by
writing θρ = [θρ1 , ..., θρN ]0 , θπ = [θπ1 , ..., θπN ]0 , θV = [θv1 , ..., θvN ]0 . Similarly, we also pack the
stochastic gradient g(θρ ) = [g(θρ1 ), ..., g(θρn )]0 , g(θV ) = [g(θv1 ), ..., g(θvn )]0 .
3.3

Value propagation algorithm

Solving (8) even without constraints is not an easy problem when both primal and dual parts are
approximated by the deep neural networks. An ideal way is to optimize the inner dual problem and
find the solution θρ∗ = arg maxθρ L(θV , θπ , θρ ), such that θρ1 = ... = θρN . Then we can do the
(decentralized) stochastic gradient decent to solve the primal problem.
min
L(θV , θπ , θρ∗ ) s.t. θv1 = ... = θvN .
(9)
{θvi ,θπi }N
i=1

However in practice, one tricky issue is that we can not get the exact solution θρ∗ of the dual problem.
Thus, we do the (decentralized) stochastic gradient for Tdual steps in the dual problem and get an
approximated solution θ̃ρ in the Algorithm 1. In our analysis, we take the error ε generated from this
inexact solution into the consideration and analyze its effect on the convergence. Particularly, since
∇θV L(θV , θπ , θ̃ρ ) 6= ∇θV L(θV , θπ , θρ∗ ), the primal gradient is biased and the results in [Dai et al.,
2018, Hong et al., 2017] do not fit this problem.
α

α

In the dual update we do a consensus update θρt+1 = 21 D−1 L+ θρt − 2ρ D−1 A0 µtρ + 2ρ D−1 g(θρt )
using the stochastic gradient of each agent, where µρ is some auxiliary variable to incorporate the
communication, D is the degree matrix, A is the node-edge incidence matrix, L+ is sign-less graph
Laplacian. We defer the detail definition and the derivation of this algorithm to Appendix A.1 and
Appendix A.5 due to space limitation. After updating the dual parameters, we optimize the primal
parameters θv , θπ . Similarly, we use a mini-batch data from the replay buffer and then do a consensus
update on θv . The same remarks on ρ also hold for the primal parameter θv . Notice here we do not
need the consensus update on θπ , since each agent’s policy π i (s, ai ) is different than each other. This
update rule is adapted from a primal decentralized optimization algorithm [Hong et al., 2017]. Notice
even in the pure primal case, Hong et al. [2017] only consider the batch gradient case while our
algorithm and analysis include the stochastic and biased gradient case. In practicals implementation,
we consider the decentralized momentum method and multi-step temporal consistency to accelerate
the training; see details in Appendix A.2 and Appendix A.3.
Remarks on Algorithm 1. (1) In the single agent case, Dai et al. [2018] assume the dual problem
can be exactly solved and thus they analyze a simple pure primal problem. However such assumption
is unrealistic especially when the dual variable is represented by the deep neural network. Our
multi-agent analysis considers the inexact solution. This is much harder than that in [Dai et al., 2018],
since now the primal gradient is biased. (2) The update of each agent just needs the information of
the agent itself and its neighbors. See this from the definition of D, A, L+ in the appendix. (3) The
topology of the Graph G affects the convergence speed. In particular, the rate depends on σmin (A0 A)
and σmin (D), which are related to spectral gap of the network.

4

Theoretical Result

In this section, we give the convergence result on Algorithm 1. We first make two mild assumptions
on the function approximators f (θ) of Vi (s), π i (s, ai ), ρi (s, a).
Assumption 1. i) The function approximator f (θ) is differentiable and has Lipschitz continuous
gradient, i.e., k∇f (θ1 ) − ∇f (θ2 )k ≤ Lkθ1 − θ2 k, ∀θ1 , θ2 ∈ RK . This is commonly assumed in the
non-convex optimization. ii) The function approximator f (θ) is lower bounded. This can be easily
satisfied when the parameter is bounded, i.e., kθk ≤ C for some positive constant C.
In the following, we give the theoretical analysis for Algorithm 1 in the same setting of [Antos et al.,
2008, Dai et al., 2018] where samples are prefixed and from one single β-mixing off-policy sample
path. We denote L̂(θV , θπ ) = maxθρ L(θV , θπ , θρ ), s.t., θρ1 =, ..., = θρN
Theorem 1. Let the function approximators of Vi (s), π i (s, ai ) and ρi (s, a) satisfy Assumption 1,
snd denote the total training step be T . We solve the inner dual problem with a approximated
√
solution θ̃ρ = (θ̃ρ1 , ..., θ̃ρN )0 , such that k∇θV L(θV , θπ , θ̃ρ ) − ∇θV L(θV , θπ , θρ∗ )k ≤ c1 / T , and
√
k∇θπ L(θV , θπ , θ̃ρ ) − ∇θπ L(θV , θπ , θρ∗ )k ≤ c2 / T . Assume the variance of the stochastic gradient
5

Algorithm 1 Value Propagation
Input: Environment ENV, learning rate απ , αv , αρ , discount factor γ, number of step Tdual to train
dual parameter θρi , replay buffer capacity B, node-edge incidence matrix A ∈ RE×N , degree
matrix D, signless graph Laplacian L+ .
Initialization of θvi , θπi , θρi , µ0ρ = 0, µ0V = 0.
for t = 1, ..., T do
QN
sample trajectory s0:τ ∼ π(s, a) = i=1 π i (s, ai ) and add it into the replay buffer.
1. Update the dual parameter θρi
Do following dual update Tdual times:
i N
Random sample a mini-batch of transition (st , {ait }N
i=1 , st+1 , {rt }i=1 ) from the replay buffer.
for agent i = 1 to n do
Calculate the stochastic gradient g(θρt i ) of −η(δi (st , at , st+1 ) − ρi (st , at ))2 w.r.t. θρt i .
end for
// Do consensus update on θρ := [θρ1 , ..., θρN ]0
α
α
θρt+1 = 21 D−1 L+ θρt − 2ρ D−1 A0 µtρ + 2ρ D−1 g(θρt ), µt+1
= µtρ + α1ρ Aθρt+1
ρ
2. Update primal parameters θvi , θπi
i N
Random sample a mini-batch of transition (st , {ait }N
i=1 , st+1 , {rt }i=1 ) from the replay buffer.
for agent i = 1 to n do
Calculate the stochastic gradient g(θvt i ),g(θπt i ) of (δi (st , at , st+1 ) − Vi (st ))2 −
η(δi (st , at , st+1 ) − ρi (st , at ))2 , w.r.t. θvt i , θπt i
end for
// Do gradient decent on θπi : θπt+1
= θπt i − απ g(θπt i ) for each agent i.
i
// Do consensus update on θV := [θv1 , ..., θvN ]0 :
= µtV + α1v AθVt+1 .
θVt+1 = 12 D−1 L+ θVt − α2v D−1 A0 µtV − α2v D−1 g(θVt ), µt+1
V
end for
g(θ√V ), g(θπ ) and g(θρ ) (estimated by a single sample) are bounded by σ 2 , the size of the mini-batch
is T , the step size απ , αv , αρ ∝ L1 . Then value propagation in Algorithm 1 converges to the
stationary solution of L̂(θV , θπ ) with rate O(1/T ).
Remarks: (1) The convergence criteria and its dependence on the network structure are involved. We
defer the definition of them to the proof section in the appendix (Equation (44)). (2) We require that
the approximated dual solution θ̃ρ are not far from θρ∗ such that the estimation of the primal gradient
√
of θv and θπ are not far from the true one (the distance is less than O(1/ T )). Once the inner
dual problem is concave, we can get this approximated solution easily using vanilla decentralized
stochastic gradient method after at most T steps. If the dual problem is non-convex, we still can show
the dual problem converges to some stationary solution with rate O(1/T ) by our proof. (3) In the
theoretical analysis, the stochastic gradient estimated from the mini-batch (rather than the estimation
from a single sample ) is common in non-convex analysis, see the work [Ghadimi and Lan, 2016]. In
practice, a mini-batch of samples is commonly used in training deep neural network.

5

Related work

Among related work on MARL, the setting of [Zhang et al., 2018] is close to ours, where the authors
proposed a fully decentralized multi-agent Actor-Critic algorithm to maximize the expected timePT
Pn
average reward limT →∞ T1 E t=1 n1 i=1 rit . They provide the asymptotic convergence analysis
on the on-policy and linear function approximation setting. In our work, we consider the discounted
reward setup, i.e., Equation (3). Our algorithm includes both on-policy and off-policy setting thus
can exploit data more efficiently. Furthermore, we provide a convergence rate O( T1 ) in the nonlinear function approximation setting which is much stronger than the result in [Zhang et al., 2018].
Littman [1994] proposed the framework of Markov games which can be applied to collaborative
and competitive setting [Lauer and Riedmiller, 2000, Hu and Wellman, 2003]. These early works
considered the tabular case thus can not apply to real problems with large state space. Recent works
[Foerster et al., 2016, 2018, Rashid et al., 2018, Raileanu et al., 2018, Jiang et al., 2018, Lowe
et al., 2017] have exploited powerful deep learning and obtained some promising empirical results.
However most of them lacks theoretical guarantees while our work provides convergence analysis.
6

23.5
agent1
agent2
agent3

21.0

22.5

22.0

20.0

20.5

20.0

19.5

19.0

Value Propagation eta=0
Value Propagation eta=1
Value Propagation eta=0.1
Value Propagation eta=0.01
Centralized-PCL

18.5

21.5
18.0

0.0

2.5

5.0

7.5

10.0

12.5

15.0

17.5

state

20.0

0

250

500

750

1000

1250

Episodes

1500

1750

2000

Cumulative reward

Cumulative reward

value function

23.0

19.5

19.0

18.5

Value Propagation eta=0
Value Propagation eta=1
Value Propagation eta=0.1
Value Propagation eta=0.01
Centralized-PCL

18.0

17.5
0

250

500

750

1000

1250

1500

1750

2000

Episodes

Figure 2: Results on randomly sampled MDP. Left: Value function of different agents in value
propagation. In the figure, value functions of three agents are similar, which means agents get
consensus on value functions. Middle: Cumulative reward of value propagation (with different η)
and centralized PCL with 10 agents. Right : Results with 20 agents.
We emphasize that most of the research on MARL is in the fashion of centralized training and
decentralized execution. In the training, they do not have the constraint on the communication, while
our work has a network decentralized structure.

6

Experimental result

The goal of our experiment is two-fold: To better understand the effect of each component in the
proposed algorithm; and to evaluate efficiency of value propagation in the off-policy setting. To
this end, we first do an ablation study on a simple random MDP problem, we then evaluate the
performance on the cooperative navigation task [Lowe et al., 2017]. The settings of the experiment
are similar to those in [Zhang et al., 2018]. Some implementation details are deferred to Appendix
A.4 due to space constraints.
6.1

Ablation Study

In this experiment, we test effect of several components of our algorithm such as the consensus update,
dual formulation in a random MDP problem. Particularly we answer following three questions: (1)
Whether an agent can get consensus through message-passing in value propagation even when each
agent just knows its local reward. (2) How much performance does the decentralized approach
sacrifice comparing with centralized one? (3) What is the effect of the dual part in our formulation
(0 ≤ η ≤ 1 and η = 0 corresponds to the pure primal form)?
We compare value propagation with the centralized PCL. The centralized PCL means that there is
a central node to collect rewards of all agent, thus it can optimize the objective function (5) using
the single agent PCL algorithm [Nachum et al., 2017, Dai et al., 2018]. Ideally, value propagation
should converges to the same long term reward with the one achieved by the centralized PCL. In the
experiment, we consider a multi-agent RL problem with N = 10 and N = 20 agents, where each
agent has two actions. A discrete MDP is randomly generated with |S| = 32 states. The transition
probabilities are distributed uniformly with a small additive constant to ensure ergodicity of the MDP,
which is P(s0 |a, s) ∝ pass0 + 10−5 , pass0 ∼ U [0, 1]. For each agent i and each state-action pair (s, a),
the reward Ri (s, a) is uniformly sampled from [0, 4].
In the left panel of Figure 2, we verify that the value function vi (s) in value propagation reaches
the consensus through message-passing in the end of the training. Particularly, we randomly choose
three agent i, j, k and draw their value functions over 20 randomly picked states. It is easy to see
that value functions vi (s), vj (s), vk (s) over these states are almost same. This is accomplished by
the consensus update in value propagation. In the middle and right panel of Figure 2, we compare
the result of value propagation with centralized PCL and evaluate the effect of the dual part of value
propagation. Particularly, we pick η = 0, 0.01, 0.1, 1 in the experiment, where η = 0 corresponds to
the pure primal formulation. When η is too large (η = 1), the algorithm would have large variance
while η = 0 the algorithm has some bias. Thus value propagation with η = 0.1, 0.01 has better result.
We also see that value propagation (η = 0.1, 0.01) and centralized PCL converge to almost the same
value, although there is a gap between centralized and decentralized algorithm. The centralized PCL
converges faster than value propagation, since it does not need time to diffuse the reward information
over the network.
7

1.0
0.8
0.6
0.4

10

8

6

2

0.0

0

2.5

5.0

7.5

10.0

state

12.5

15.0

17.5

20.0

Value Propagation eta=0.01
Value Propagation eta=0.1
PCL without communication
Independent Q
MA-AC
Partial Value Propagation

4

0.2

0.0

Value Propagation eta=0.01
Value Propagation eta=0.1
PCL without communication
Independent Q
MA-AC
Partial Value Propagation

12

Cumulative reward

value function

8

agent1
agent2
agent3

1.2

0

250

500

750

1000

Episodes

1250

1500

1750

Cumulative reward

1.4

6

4

2

0

0

250

500

750

1000

1250

1500

1750

Episodes

Figure 3: Results on Cooperative Navigation task. Left: value functions of three random picked
agents (totally 16 agents) in value propagation. They get consensus. Middle : cumulative reward
of value propagation (eta=0.01 and eta=0.1), MA-AC and PCL without communication with agent
number N=8. Right: Results with agent number N=16. Our algorithm outperforms MA-AC and PCL
without communication. Comparing with the middle panel, the number of agent increases in the
right panel. Therefore, the problem becomes harder (more collisions). We see agents achieve lower
cumulative reward (averaged over agents) and need more time to find a good policy.
6.2 Cooperative Navigation task

The aim of this section is to demonstrate that the value propagation outperforms decentralized multiagent Actor-Critic (MA-AC)[Zhang et al., 2018], independent Q learning [Tan, 1993], the Multi-agent
PCL without communication. Here PCL without communication means each agent maintains its own
estimation of policy π i (s, ai ) and value function V i (s) but there is no communication Graph. Notice
that this is different from the centralized PCL in Section 6.1, where centralized PCL has a central
node to collect all reward information and thus do not need further communication. Note that the
original MA-AC is designed for the averaged reward setting thus we adapt it into the discounted case
to fit our setting. We test the value propagation in the environment of the Cooperative Navigation task
[Lowe et al., 2017], where agents need to reach a set of L landmarks through physical movement.
We modify this environment to fit our setting. A reward is given when the agent reaches its own
landmarks. A penalty is received if agents collide with other agents. Since the position of landmarks
are different, the reward function of each agent is different. Here we test the case the state is globally
observed and partially observed. In particular, we assume the environment is in a rectangular region
with size 2 × 2. There are N = 8 or N = 16 agents. Each agent has a single target landmark, i.e.,
L = N , which is randomly located in the region. Each agent has five actions which corresponds to
going up, down, left, right with units 0.1 or staying at the position. The agent has high probability
(0.95) to move in the direction following its action and go in other direction randomly otherwise.
The maximum length of each epoch is set to be 500 steps. When the agent is close enough to the
landmark, e.g., the distance is less than 0.1, we think it reaches the target and gets reward +5. When
two agents are close to each other (with distance less than 0.1), we treat this case as a collision and
a penalty −1 is received for each of the agents. The state includes the position of the agents. The
communication graph is generated as that in Section 6.1 with connectivity ratio 4/N . In the partially
observed case, the actor of each agent can only observe its own and neighbors’ states. We report the
results in Figure 3.
In the left panel of Figure 3, we see the value function vi (s) reaches consensus in value propagation.
In the middle and right panel of Figure 3, we compare value propagation with PCL without communication, independent Q learning and MA-AC. In PCL without communication, each agent maintains
its own policy, value function and dual function, which is trained by the algorithm SBEED [Dai et al.,
2018] with η = 0.01. Since there is no communication between agents, intuitively agents may have
more collisions in the learning process than those in value propagation. Similar augment holds for
the independent Q learning. Indeed, In the middle and right panel, we see value propagation learns
the policy much faster than PCL without communication. We also observe that value propagation
outperforms MA-AC. One possible reason is that value propagation is an off-policy method thus we
can apply experience replay which exploits data more efficiently than the on-policy method MA-AC.
We also test the performance of value propagation (result labeled as partial value propagation in
Figure 3) when the state information of actor is partially observed. Since the agent has limited
information, its performance is worse than the fully observed case. But it is better than the PCL
without communication (fully observed state).
8

References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pages 308–318. ACM, 2016.
Jeffrey L Adler and Victor J Blue. A cooperative multi-agent transportation management and route
guidance system. Transportation Research Part C: Emerging Technologies, 10(5-6):433–454,
2002.
András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71
(1):89–129, 2008.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
IEEE transactions on information theory, 52(6):2508–2530, 2006.
Lucian Bu, Robert Babu, Bart De Schutter, et al. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
38(2):156–172, 2008.
Duncan S Callaway and Ian A Hiskens. Achieving controllability of electric loads. Proceedings of
the IEEE, 99(1):184–199, 2011.
Federico S Cattivelli, Cassio G Lopes, and Ali H Sayed. Diffusion recursive least-squares for
distributed estimation over adaptive networks. IEEE Transactions on Signal Processing, 56(5):
1865–1877, 2008.
Jorge Cortes, Sonia Martinez, Timur Karatas, and Francesco Bullo. Coverage control for mobile
sensing networks. IEEE Transactions on robotics and Automation, 20(2):243–255, 2004.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International
Conference on Machine Learning, pages 1133–1142, 2018.
Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A
survey and comparison. The Journal of Machine Learning Research, 15(1):809–883, 2014.
Morris H DeGroot. Reaching a consensus. Journal of the American Statistical Association, 69(345):
118–121, 1974.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.
Samah El-Tantawy, Baher Abdulhai, and Hossam Abdelgawad. Multiagent reinforcement learning for
integrated network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale
application on downtown toronto. IEEE Transactions on Intelligent Transportation Systems, 14(3):
1140–1150, 2013.
J Alexander Fax and Richard M Murray. Information flow and cooperative control of vehicle
formations. In IFAC World Congress, volume 22, 2002.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, pages 2137–2145, 2016.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59–99, 2016.
Mingyi Hong. Decomposing linearly constrained nonconvex problems by a proximal primal dual
approach: Algorithms, convergence, and applications. arXiv preprint arXiv:1604.00543, 2016.
9

Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao. Prox-pda: The proximal primal-dual
algorithm for fast distributed nonconvex optimization and learning over networks. In International
Conference on Machine Learning, pages 1529–1538, 2017.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003.
Jiechuan Jiang, Chen Dun, and Zongqing Lu. Graph convolutional reinforcement learning for
multi-agent cooperation. arXiv preprint arXiv:1810.09202, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In In Proceedings of the Seventeenth International Conference on
Machine Learning. Citeseer, 2000.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994, pages 157–163. Elsevier, 1994.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pages 6379–6390, 2017.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
Yilin Mo and Richard M Murray. Privacy preserving average consensus. IEEE Transactions on
Automatic Control, 62(2):753–765, 2017.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems, pages 2775–2785, 2017.
Michael Rabbat and Robert Nowak. Distributed optimization in sensor networks. In Proceedings
of the 3rd international symposium on Information processing in sensor networks, pages 20–27.
ACM, 2004.
Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in
multi-agent reinforcement learning. arXiv preprint arXiv:1802.09640, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczyński. Lectures on stochastic programming: modeling and theory. SIAM, 2009.
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact first-order algorithm for decentralized
consensus optimization. SIAM Journal on Optimization, 25(2):944–966, 2015.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pages 330–337, 1993.
T Tieleman and G Hinton. Divide the gradient by a running average of its recent magnitude. coursera:
Neural networks for machine learning. Technical report, Technical Report. Available online:
https://zh. coursera. org/learn . . . .
Lin Xiao, Stephen Boyd, and Sanjay Lall. A scheme for robust distributed sensor fusion based on
average consensus. In Information Processing in Sensor Networks, 2005. IPSN 2005. Fourth
International Symposium on, pages 63–70. IEEE, 2005.
10

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar. Fully decentralized
multi-agent reinforcement learning with networked agents. International Conference on Machine
Learning, 2018.

11

A

Details on the Value Propagation

A.1

Topology of the Graph

Here, we explain the matrix in the Algorithm 1 which are closely related to the topology of the Graph,
which is left from the main paper due to the limit of the space.
• D = diag[d1 , ..., dN ] is the degree matrix, with di denoting the degree of node i.
• A is the node-edge incidence matrix: if e ∈ E and it connects vertex i and j with i > j, then
Aev = 1 if v = i, Aev = −1 if v = j and Aev = 0 otherwise.
• The signless incidence matrix B := |A|, where the absolute value is taken for each component of A.
/ E. Notice
• The signless graph Laplacian L+ = B T B. By definition L+ (i, j) = 0 if (i, j) ∈
the non-zeros element in A, L+ , the update just depends on each agent itself and its neighbor.
A.2

Practical Acceleration

The algorithm 1 trains the agent with vanilla gradient decent method with a extra consensus update. In
practice, the adaptive momentum gradient methods including Adagrad Duchi et al. [2011], Rmsprop
Tieleman and Hinton and Adam Kingma and Ba [2014] have much better performance in training
the deep neural network. We adapt Adam in our setting, and propose algorithm 2 which has better
performance than algorithm 1 in practice.
Algorithm 2 Accelerated value propagation
Input: Environment ENV, learning rate β1 , β2 ∈ [0, 1), αt , discount factor γ, a mixing matrix W ,
number of step Tdual to train dual parameter θρi , replay buffer capacity B.
Initialization of θvi , θπi , θρi , moment vectors m0vi = m0ρi = 0, wv0i = wρ0i = 0 .
for t = 1, ..., T do
QN
sample trajectory s0:τ ∼ π(s, a) = i=1 π i (s, ai ) and add it into the replay buffer.
// Update the dual parameter θρi
Do following update Tdual times:
i N
Random sample a mini-batch of transition (st , {ait }N
i=1 , st+1 , {rt }i=1 ) from the replay buffer.
for agent i = 1 to n do
Calculate the stochastic gradient g(θρt i ) of −η(δi (st , at , st+1 ) − ρi (st , at ))2 w.r.t. θρt i .
t
// update momentum parameters: mtρi = β1 mt−1
ρi + (1 − β1 )(−g(θρi ))
wρt i = β2 wρt−1
+ (1 − β2 )g(θρt i ) g(θρt i )
i
end for
// Do consensus update for each agent i
PN
mt i
t+ 1
t+ 1
θρi 2 = j=1 [W ]ij θρt j , θρt i = θρi 2 − αt q ρt
w i
ρ

// End the update of dual problem
// Update primal parameters θvi , θπi .
i N
Random sample a mini-batch of transition (st , {ait }N
i=1 , st+1 , {rt }i=1 ) from the replay buffer.
for agent i = 1 to n do
Calculate the stochastic gradient g(θvt i ),g(θπt i ) of (δi (st , at , st+1 ) − Vi (st ))2 −
η(δi (st , at , st+1 ) − ρi (st , at ))2 , w.r.t. θvt i , θπt i
//update the momentum parameter:
t
mtvi = β1 mt−1
v i + (1 − β1 )g(θv i )
wvt i = β2 wvt−1
+ (1 − β2 )g(θvt i ) g(θvt i )
i
// Using Adam to update θπi for each agent i.
// Do consensus update on θvi for each agent i:
PN
mt
t+ 1
t+ 1
θvi 2 = j=1 [W ]ij θvt j , θvt i = θvi 2 − αt q vti
w i
v

end for
end for

12

Mixing Matrix: In Algorithm 2, there is a mixing matrix W ⊂ RN ×N in the consensus update. As
its name suggests, it mixes information of the agent and its neighbors. This nonnegative matrix W
need to satisfy the following condition.
• W needs to be doubly stochastic, i.e., W T 1 = 1 and W 1 = 1.
• W respects the communication graph G, i.e., W (i, j) = 0 if (i, j) ∈
/ E.
• The spectral norm of W T (I − 11T /N )W is strictly smaller than one.
Here is one particular choice of the mixing matrix W used in our work which satisfies above
requirement called Metropolis weights Xiao et al. [2005].
−1

W (i, j) = 1 + max[d(i), d(j)] , ∀(i, j) ∈ E,
X
W (i, i) = 1 −
W (i, j), ∀i ∈ N ,

(10)

j∈N E(i)

where N E(i) = {j ∈ N : (i, j) ∈ E} is the set of neighbors of the agent i and d(i) = |N (i)| is the
degree of agent i. Such mixing matrix is widely used in decentralized and distributed optimization
Boyd et al. [2006], Cattivelli et al. [2008]. The update rule of the momentum term in Algorithm
PN
t+ 1
2 is adapted from Adam. The consensus (communication) steps are θρi 2 = j=1 [W ]ij θρt j and
PN
t+ 1
θvi 2 = j=1 [W ]ij θvt j .
A.3

Multi-step Extension on value propagation

The temporal consistency can be extended to the multi-step case Nachum et al. [2017], where the
following equation holds
V (s0 ) =

k−1
X

γ t Est |s0 ,a0:t−1 [R(st , at ) − λ log π(st , ait )] + γ k Esk |s0 ,a0:k−1 V (sk ).

t=0

Pk−1
Thus in the objective
function (8), we can replace δi by δi (s0:k , a0:k−1 ) = t=0 γ t Ri (st , at ) −

λN log π i (st , ait ) + γ k Vi (sk ) and change the estimation of stochastic gradient correspondingly in
Algorithm 1 and Algorithm 2 to get the multi-step version of vaue propagation . In practice, the
performance of setting k > 1 is better than k = 1 which is also observed in single agent case Nachum
et al. [2017], Dai et al. [2018]. We can tune k for each application to get the best performance.
A.4

Implementation details of the experiments

Ablation Study
The value function vi (s) and dual variable ρi (s, a) are approximated by two hidden-layer neural
network with Relu as the activation function where each hidden-layer has 20 hidden units. The policy
of each agent is approximated by a one hidden-layer neural network with Relu as the activation
function where the number of the hidden units is 32. The output is the softmax function to approximate
π i (s, ai ). The mixing matrix in Algorithm 2 is selected as the Metropolis Weights in (10). The graph
G is generated by randomly placing communication links among agents such that the connectivity
ratio is 4/N . We set γ = 0.9, λ = 0.01, learning rate α=5e-4. The choice of β1 , β2 are the default
value in Adam.
Cooperative Navigation task
The value function vi (s) is approximated by a two-hidden-layer neural network with Relu as the
activation function where inputs are the state information. Each hidden-layer has 40 hidden units.
The dual function ρ(s, a) is also approximated by a two-hidden-layer neural network, where the only
difference is that inputs are state-action pairs (s,a). The policy is approximated by a one-hidden-layer
neural network with Relu as the activation function. The number of the hidden units is 32. The
output is the softmax function to approximate π i (s, ai ). In all experiments, we use the multi-step
version of value propagation and choose k = 4. We choose γ = 0.95, λ = 0.01. The learning
rate of Adam is chosen as 5e-4 and β1 , β2 are default value in Adam optimizer. The setting of PCL
13

without communication is exactly same with value propagation except the absence of communication
network.
A.5

Consensus update in Algorithm 1

We now give details to derive the Consensus Update in Algorithm 1 with η = 1 to ease the exposition.
When η ∈ [0, 1), we just need to change variable and some notations, the result are almost same.
Here we use the primal update as an example, the derivation of the dual update is the same.
In the main paper section 3, we have shown that when η = 1, in the primal update, we basically solve
following problem.

min

{θvi ,θπi }N
i=1

2Es,a,s [ν ∗ (s, a)

N

1 X i
(R (s, a) + γVi (s0 ) − Vi (s) − λN log π i (s, a) ] − Es,a,s [ν ∗2 (s, a)],
N i=1

s.t., θv1 = ... = θvn .
(11)
here for simplicity we assume in the dual optimization, we have already find the optimal solution
ν ∗ (s, a). It can be any approximated solution of ν̃(s, a) which does not affect the derivation of the
update rule in primal optimization. In the later proof, we will show how this approximated solution
affects the convergence rate.
When we optimize w.r.t. θvi , we basically we solve a non-convex problem with the following form

min f (x) =
x

N
X

fi (xi ), s.t. x1 = ... = xN

(12)

i=1

Recall the definition of the node-edge incidence matrix A: if e ∈ E and it connects vertex i and j
with i > j, then Aev = 1 if v = i, Aev = −1 if v = j and Ae v = 0 otherwise. Thus by define
x = [x1 , ..., xN ]0 we have a equivalent form of (12)

min f (x) =
x

N
X

fi (xi ), s.t., Ax = 0

(13)

i=1

Notice the update of θπi is a special case of above formulation, since we do not have the constraint
x1 =, ..., = xN . Thus in the following, it suffice to analyze above formulation (13). We adapt the
Prox-PDA in Hong et al. [2017] to solve above problem. To keep the notation consistent with Hong
et al. [2017], we consider a more general problem

min f (x) =
x

N
X

fi (xi ), s.t., Ax = b.

i=1

In the following we denote ∇f (xt ) := [(∇x1 f (x1 ))T , ..., (∇xN f (xN ))T ]T where the superscript 0
means transpose. We denote gi (xi ) as an estimator of ∇xi f (xi ) and g(x) = [g1 (x1 ), ..., gN (xN )].
The update rule of Prox-PDA is
xt+1 = arg minhg(xt ), x − xt i + hµt , Ax − bi +
x

β
β
kAx − bk2 + kx − xt k2B T B
2
2

(14)

µt+1 = µt + β(Axt+1 − b)
(15)
where g(x ) is an estimator of ∇f (xt ). The signed graph Laplacian matrix L− is AT A. Now we
choose B := |A| as the signless incidence matrix. Using this choice of B, we have B T B = L+ ∈
RN ×N which is the signless graph Laplacian whose (i, i)th diagonal entry is the degree of node i
and its (i, j)th entry is 1 if e = (i, j) ∈ E, and 0 otherwise.
t

14

Thus
xt+1 = arg minhg(xt ), x − xt i + hµt , Ax − bi +
x

β T
β
x L− x + (x − xt )L+ (x − xt )
2
2

β T
x (L− + L+ )x − βxT L+ xt
2
= arg minhg(xt ), xi + hµt , Ax − bi + βxT Dx − βxT L+ xt ,
= arg minhg(xt ), xi + hµt , Ax − bi +

(16)

x
x

where D = diag[d1 , ..., dN ] is the degree matrix, with di denoting the degree of node i.
After simple algebra, we obtain
1 −1 T t
1 −1
1 −1 + t
D L x −
D A µ −
D g(xt ),
2
2β
2β
which is the primal update rule of the consensus step in the algorithm 1 (notice here the stepsize is
1/β)
xt+1 =

B

Convergence Proof of Value Propagation

B.1

Convergence on the primal update

In this section, we first give the convergence analysis of the value propagation (algorithm 1) on the
primal update. To include the effected of the inexact solution of dual optimization problem, we
denote g(xt ) = ∇f (xt ) + t , where t = εt + ε̃t is some error terms.
• εt is a zero mean random variable coming from the randomness of the stochastic gradient
g(xt ).
• ε̃t comes from the approximated solution of ν̃ in (11) or ρ̃ in (8) such
that k∇θv L(θV , θπ , θ̃ρ ) − ∇θv L(θV , θπ , θρ∗ )k ≤ ε̃t and k∇θπ L(θV , θπ , θ̃ρ ) −
∇θπ L(θV , θπ , θρ∗ )k ≤ ε̃t .
Before we begin the proof, we made some mild assumption on the function f (x).
Assumption 2. 1. The function f(x) is differentiable and has Lipschitz continuous gradient, i.e.,
k∇f (x) − ∇f (y)k ≤ kx − yk, ∀x, y ∈ RK .
2. Further assume that AT A + B T B < I. This assumption is always satisfied by our choice on A
and B. We have AT A + B T B < D < mini {di }I
3. There exists a constant δ > 0 such that ∃f > −∞, s.t., f (x) + 2δ kAx − bk2 ≥ f , ∀x. This
assumption is satisfied if we require the parameter space is bounded.
Lemma 1. Suppose the assumption 2 is satisfied, we have following inequality holds

kµt+1 − µt k2
3L2
3
3β
≤
kxt −xt−1 k2 + kt−1 −t k2 +
kB T B (xt+1 −xt )−(xt −xt−1 ) k2
β
βσmin
β
σmin
(17)
Proof. Using the optimality condition of (14), we obtain
∇f (xt ) + t + AT µt + βAT (Axt+1 − b) + βB T B(xt+1 − xt ) = 0

(18)

applying equation (15) we have
AT µt+1 = −∇f (xt ) − βB T B(xt+1 − xt ).
0

Note that from the fact that µ = 0, we have the variable lies in the column space of A.
r

µ =β

r
X

(Axt − b).

t=1

15

(19)

Let σmin denote the smallest non-zero eigenvalue of AT A, we have
1/2

σmin kµt+1 − µt k
≤kA(µt+1 − µt )k
≤k − ∇f (xt ) − t − βB T B(xt+1 − xt ) − (−∇f (xt−1 ) − t−1 − βB T B(xt − xt−1 ))k

(20)

=k∇f (xt−1 ) − ∇f (xt ) + (t−1 − t ) − βB T B((xt+1 − xt − (xt − xt−1 )))k.
Thus we have
kµt+1 − µt k2
β
1
≤
k∇f (xt−1 ) − ∇f (xt ) + (t−1 − t ) − βB T B((xt+1 − xt − (xt − xt−1 )))k2
(21)
1/2
βσmin

3L2
3
3β
≤
kxt − xt−1 k2 + kt−1 − t k2 +
kB T B (xt+1 − xt ) − (xt − xt−1 ) k2 ,
βσmin
β
σmin
where the second inequality holds from the fact that (a + b + c)2 ≤ 3a2 + 3b2 + 3c2 .

Lemma 2. Define Lβ (xt , µt ) = f (xt ) + hµt , Ax − bi + β2 kAx − bk2 + β2 kx − xt kB T B . Suppose
assumptions are satisfied, then the following is true for the algorithm
Lβ (xt+1 , µt+1 ) − Lβ (xt , µt )

β t+1
3L2
3
3β
kx
− xt k 2 +
kxt − xt−1 k2 + kt−1 − t k2 +
kB T B (xt+1 − xt ) − (xt − xt−1 ) k2
2
βσmin
β
σmin
+ ht , xt+1 − xt i
(22)

≤−

Proof. By the Assumptions AT A + B T B ≥ I, the objective function in (14) is strongly convex with
parameter β.
Using the optimality condition of xt+1 and strong convexity, we have for any x,
β
β
kx − xt k2B T B − (Lβ (xt+1 , µt ) + kxt+1 − xt k2B T B )
2
2
β
t+1
t
T
t+1
t
t+1
≥ h∇Lβ (x , µ ) + βB B(x
− x ), x − x i + kxt+1 − xk2
2

Lβ (x, µt ) +

16

(23)

Now we start to provide a upper bound of Lβ (xt+1 , µt+1 ) − Lβ (xt , µt ) .
Lβ (xt+1 , µt+1 ) − Lβ (xt , µt )
=Lβ (xt+1 , µt+1 ) − Lβ (xt+1 , µt ) + Lβ (xt+1 , µt ) − Lβ (xt , µt )
β
≤Lβ (xt+1 , µt+1 ) − Lβ (xt+1 , µt ) + Lβ (xt+1 , µt ) + kxt+1 − xt k2B T B − Lβ (xt , µt )
2
a kµt+1 − µt k
β
+ h∇Lβ (xt+1 , µt ) + βB T B(xt+1 − xt ), xt+1 − xt i − kxt+1 − xt k2
≤
β
2
(24)
t+1
t 2
b
β t+1
kµ
−
µ
k
t 2
t+1
t
≤ − kx
−x k +
+ ht , x
−x i
2
β
c
β
3L2
3
≤ − kxt+1 − xt k2 +
kxt − xt−1 k2 + kt−1 − t k2
2
βσmin
β
 2
3β
T
t+1
t
t
t−1
+
kB B (x
− x ) − (x − x ) k + ht , xt+1 − xt i,
σmin
where the inequality (a) holds from the update rule in (15) and a simple algebra from the expression
of Lβ (x, µ). Inequality (b) comes from the optimality condition of (14). Particularly, we have
g(xt ) + AT µt + βAT (Ax − b) + βB T B(x − xt ) = 0
replace g(xt ) by ∇f (xt ) + t , we have the result. The inequality (c) holds using the Lemma 1.

Lemma 3. Suppose Assumption 2 is satisfied, then the following condition holds.
β
(kAxt+1 − bk2 + kxt+1 − xt k2B T B )
2
L
L
β
≤ kxt+1 − xt k2 + kxt − xt−1 k2 + (kxt − xt−1 k2B T B + kAxt − bk2 )
2
2
2
β
− (k(xt − xt−1 ) − (xt+1 − xt )k2B T B + kA(xt+1 − xt )k2 ) − ht − t−1 , xt+1 − xt i
2

(25)

Proof. Using the optimality condition of xt+1 and xt in the update rule in (14), we obtain
hg(xt ) + AT µt + βAT (Axt+1 − b) + βB T B(xt+1 − xt ), xt+1 − xi ≤ 0, ∀x

(26)

hg(xt−1 ) + AT µt−1 + βAT (Axt − b) + βB T B(xt − xt−1 ), xt − xi ≤ 0, ∀x

(27)

and
Replacing g(xt ) by ∇f (xt ) + t and g(xt−1 ) by ∇f (xt−1 ) + t−1 , and using the update rule (15)
h∇f (xt ) + t + AT µt+1 + βB T B(xt+1 − xt ), xt+1 − xi ≤ 0, ∀x

(28)

h∇f (xt−1 ) + t−1 + AT µt + βB T B(xt − xt−1 ), xt − xi ≤ 0, ∀x

(29)

t

Now choose x = x in the first inequality and x = x
together, we obtain

t+1

in the second one, adding two inequalities


h∇f (xt )−∇f (xt−1 )+t −t−1 +AT (µt+1 −µt )+βB T B (xt+1 −xt )−(xt −xt−1 ) , xt+1 −xt i ≤ 0
(30)
Rearranging above terms, we have
hAT (µt+1 − µt ), xt+1 − xt i

(31)

≤ −h∇f (xt ) − ∇f (xt−1 ) + t − t−1 + βB T B (xt+1 − xt ) − (xt − xt−1 ) , xt+1 − xt i
17

We first re-express the lhs of above inequality.
hAT (µt+1 − µt ), xt+1 − xt i
=hβAT (Axt+1 − b), xt+1 − xt i
=hβ(Axt+1 − b), Axt+1 − b − (Axt − b)i

(32)

=βkAxt+1 − bk2 − βhAxt+1 − b, Axt − bi
β
= (kAxt+1 − bk2 − kAxt − bk2 + kA(xt+1 − xt )k2 )
2
Next, we bound the rhs of (31).

− h∇f (xt ) − ∇f (xt−1 ) + t − t−1 + βB T B (xt+1 − xt ) − (xt − xt−1 ) , xt+1 − xt i

= − h∇f (xt ) − ∇f (xt−1 ) + t − t−1 , xt+1 − xt i − βhB T B (xt+1 − xt ) − (xt − xt−1 ) , xt+1 − xt i
a L
1
k∇f (xt ) − ∇f (xt−1 )k2 − ht − t−1 , xt+1 − xt i
≤ kxt+1 − xt k2 +
2
2L

− βhB T B (xt+1 − xt ) − (xt − xt−1 ) , xt+1 − xt i
b L
L
≤ kxt+1 − xt k2 + kxt − xt−1 k2 − ht − t−1 , xt+1 − xt i
2
2

− βhB T B (xt+1 − xt ) − (xt − xt−1 ) , xt+1 − xt i
L
L
= kxt+1 − xt k2 + kxt − xt−1 k2 − ht − t−1 , xt+1 − xt i
2
2

β
t
t−1 2
+ kx − x kB T B − kxt+1 − xt k2B T B − k(xt − xt−1 ) − (xt+1 − xt )k2B T B ,
2
(33)
where the inequality (a) uses Cauchy-Schwartz inequality, (b) holds from the smoothness assumption
on f .
Combine all pieces together, we obtain
β
(kAxt+1 − bk2 + kxt+1 − xt k2B T B )
2
L
L
β
≤ kxt+1 − xt k2 + kxt − xt−1 k2 + (kxt − xt−1 k2B T B + kAxt − bk2 )
2
2
2
β
− (k(xt − xt−1 ) − (xt+1 − xt )k2B T B + kA(xt+1 − xt )k2 ) − ht − t−1 , xt+1 − xt i
2

(34)

Same with Hong et al. [2017], we define the potential function
cβ
(kAxt+1 − bk2 + kxt+1 − xt k2B T B )
2
Lemma 4. If Assumption 2 holds, we have following
Pc,β (xt+1 , xt , µt+1 ) = Lβ (xt+1 , µt+1 ) +

(35)

Pc,β (xt+1 , xt , µt+1 )
β
cL 2c + 1
3L2
cL
−
−
)kxt+1 − xt k2 + (
+
)kxt−1 − xt k2
2
2
2
βσmin
2
cβ
3βkB T Bk
c β
1
−(
−
)k(xt+1 − xt ) − (xt − xt−1 )k2B T B + ( + )kt−1 − t k2 + kt k2
2
σmin
4
3
2
(36)

≤Pc,β (xt , xt−1 , µt ) − (

18

Proof.
Pc,β (xt+1 , xt , µt+1 )
β
cL
cβ
(kxt − xt−1 kB T B + kAxt − bk2 ) − ( −
)kxt+1 − xt k2
≤Lβ (xt , µt ) +
2
2
2
3L2
cL
cβ
3βkB T Bk
+(
+
)kxt−1 − xt k2 − (
−
)k(xt+1 − xt ) − (xt − xt−1 )k2B T B
βσmin
2
2
σmin
3
+ kt − t−1 k2 + ht , xt+1 − xt i − cht − t−1 , xt+1 − xt i
β
cL
3L2
cL
β
)kxt+1 − xt k2 + (
+
)kxt−1 − xt k2
≤Pc,β (xt , xt−1 , µt ) − ( −
2
2
βσmin
2
cβ
3βkB T Bk
c
−(
−
)k(xt+1 − xt ) − (xt − xt−1 )k2B T B + kt−1 − t k2
2
σmin
4
1
1 t+1
β
t+1
t 2
2
t 2
+ ckx
− x k + kt k + kx
− x k + kt − t−1 k2
2
2
3
cL
2c
+
1
3L2
cL
β
−
)kxt+1 − xt k2 + (
+
)kxt−1 − xt k2
=Pcβ (xt , xt−1 , µt ) − ( −
2
2
2
βσmin
2
3βkB T Bk
1
c β
cβ
−
)k(xt+1 − xt ) − (xt − xt−1 )k2B T B + ( + )kt−1 − t k2 + kt k2
−(
2
σmin
4
3
2
(37)
where the second inequality holds from the Cauchy-Schwartz inequality.
We require that
3βkB T Bk
cβ
−
≥ 0,
2
σmin
which is satisfied when
c≥

6kB T Bk
σmin

(38)

We further require
β
cL 2c + 1
3L2
cL
−
−
)≥(
),
+
2
2
2
βσmin
2
which will be used later in the telescoping.
(

Thus we require
β ≥ 2cL + 2c + 1 +
1
and choose β ≥ CL + 2c+1
2 + 2

q

6L2
.
βσmin
2

(2cL + 2c + 1)2 + 24L
σmin

19

(39)

Now we do summation over both side of (36) and have
T
X
cL 2c + 1
3L2
cL
β
−
)kxt+1 − xt k2 − (
+
)kxt−1 − xt k2 ]
[( −
2
2
2
βσ
2
min
t=1
T
X
3βkB T Bk
cβ
−
)k(xt+1 − xt ) − (xt − xt−1 )k2B T B
+
(
2
σ
min
t=1

≤Pcβ (x1 , x0 , µ0 ) − Pcβ (xT +1 , xT , µT ) +

(40)

T
X
c β
1
[( + )kt−1 − t k2 + kt k2 ]
4
3
2
t=1

rearrange terms of above inequality.
T
−1
X

cl 2c + 1
3L2
cl
β
cl 2c + 1
β
− −
−
− )kxt+1 − xt k2 + ( − −
)kxT +1 − xT k2
2
2
2
βσ
2
2
2
2
min
t=1
(

≤Pcβ (x1 , x0 , µ0 ) − Pcβ (xT +1 , xT , µT ) + (

T
X
c β
1
3L2
cl
[( + )kt−1 − t k2 + kt k2 ]
+ )kx1 − x0 k2 +
βσmin
2
4
3
2
t=1
(41)

Next we show Pcβ is lower bounded
The following lemma is from Lemma 3.5 in Hong [2016], we present here for completeness.
Lemma 5. Suppose Assumption 2 are satisfied, and (c, β) are chosen according to (39) and (38).
Then the following state holds true
∃P s.t., Pcβ (xt+1 , xt , µt+1 ) ≥ P > −∞
Proof.
β
kAxt+1 − bk2
2
1
β
=f (xt+1 ) + hµt+1 , µt+1 − µt i + kAxt+1 − bk2
β
2
β
1
(kµt+1 k2 − kµt k2 + kµt+1 − µt k2 ) + kAxt+1 − bk2 .
=f (xt+1 ) +
2β
2

Lβ (xt+1 , µt+1 ) =f (xt+1 ) + hµt+1 , Axt+1 − bi +

(42)

Sum over both side, we obtain
T
X
t=1

Lβ (xt+1 , µt+1 ) =

T
X

 1
β
1
f (xt+1 )+ kAxt+1 −bk2 + kµt+1 −µt k2 + (kµT +1 k2 −kµ1 k2 )
2
2β
2β
t=1
(43)

By assumption 2, above sum is lower bounded, which implies that the sum of the potential function is
t+1
− bk2 + kxt+1 −
also lower bounded (Recall Pc,β (xt+1 , xt , µt+1 ) = Lβ (xt+1 , µt+1 ) + cβ
2 (kAx
t 2
x kB T B ) ). Thus we have
Pcβ (xt+1 , xt , µt+1 ) > −∞, ∀t > 0

In the next step, we are ready to provide the convergence rate. Following Hong [2016], we define the
convergence criteria
Q(xt+1 , µt+1 ) = k∇Lβ (xt+1 , µt )k2 + kAxt+1 − bk2
20

(44)

It is easy to see, when Q(xt+1 , µt ) = 0, ∇f (x) + AT µ = 0 and Ax = b, which are KKT condition
of the problem.
k∇Lβ (xt , µt−1 )k2
=k∇f (xt ) − ∇f (xt−1 ) + t − t−1 + AT (µt+1 − µt ) + βB T B(xt+1 − xt )k2
2

t

t−1 2

≤4L kx − x

t+1

k + 4kµ

t 2

T

2

T

t+1

− µ k kA Ak + 4β kB B(x

t

(45)

2

2

− x )k + 4kt − t−1 k

Using the proof in Lemma 1, we know there exist two positive constants c1 c2 c3 c4

Q(xt , µt−1 ) ≤ c1 kxt −xt+1 k2 +c2 kxt −xt−1 k2 +c3 kB T B (xt+1 −xt )−(xt −xt−1 ) k2 +c4 kt −t−1 k2 .
Using Lemma 4, we know there must exist a constant κ such that
T
−1
X

Q(xt , µt−1 )

t=1

≤κ(Pcβ (x1 , x0 , µ0 ) − Pcβ (xT +1 , xT , µT ) +
≤κ(Pcβ (x1 , x0 , µ0 ) − P +

T
−1
T
X
X
c β
1
[( + )kt−1 − t k2 + kt k2 ]) + c4
kt − t−1 k2
4
3
2
t=1
t=1

T
−1
T
X
X
1
c β
kt − t−1 k2
[( + )kt−1 − t k2 + kt k2 ]) + c4
4
3
2
t=1
t=1

(46)
Divide both side by T and take expectation
T
T
1 X
1
κ X c β
1
Q(xt , µt−1 ) ≤ κ(Pcβ (x1 , x0 , µ0 ) − P )+ [ ( + )Ekt−1 − t k2 + kt k2 ]
E
T t=1
T
T t=1 4
3
2
T −1

+

c4 X
Ekt − t−1 k2
T t=1
(47)

Now we bound the R.H.S. of above equation.
√
√
Recall we choose the mini-batch size T , t = εt + ε̃t and εt ≤ c1 / T
kt−1 −t k2 ≤ 2E(kt−1 k2 +kt k2 ) ≤ 4E(kεt k2 +kε̃t k2 +kεt−1 k2 +kε̃t−1 k2 ) ≤

8c1 8σ 2
+
(48)
T
T

Similarly we can bound kt k2 . Combine all pieces together, we obtain
T
1 X
1
1
E
Q(xt , µt−1 ) ≤ κ(Pcβ (x1 , x0 , µ0 ) − P ) + (κc5 + c6 σ 2 ),
T t=1
T
T
where c5 , c6 are some universal positive constants.
PT
Notice mint EQ(xt , µt−1 ) ≤ T1 E t=1 Q(xt , µt−1 ), we have mint EQ(xt , µt−1 ) ≤ (C + σ 2 )/T
where C is a universal positive constant.

B.2

Convergence on the dual update

If the dual objective function is non-convex, we just follow the exact analysis in our proof on the
primal problem. Notice the analysis on the dual update is easier than primal one, since we do not
21

have the error term ˜t . Therefore, we have the algorithm converges to stationary solution with rate
O(1/T )in criteria Q.
If the dual objective function is linear or convex, the update rule reduce to Extra [Hong, 2016, Shi
et al., 2015] the convergence result of stochastic setting can be adapted from the proof in [Shi et al.,
2015]. Since it is not the main contribution of this paper, we omit the proof here.

22

