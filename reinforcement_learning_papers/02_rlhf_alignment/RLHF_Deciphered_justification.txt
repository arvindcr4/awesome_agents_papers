Paper: RLHF_Deciphered.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
arXiv:2404.08555v2 [cs.LG] 16 Apr 2024  RLHF D ECIPHERED : A C RITICAL A NALYSIS OF R EINFORCEMENT L EARNING FROM H UMAN F EEDBACK FOR LLM S  Shreyas Chaudhari*1  Pranjal Aggarwal*2  Ashwin Kalyan5  Karthik Narasimhan3  Vishvak Murahari3 Ameet Deshpande3  Tanmay Rajpurohit4  Bruno Castro da Silva1  1  2  University of Massachusetts Amherst Department of Computer Science, Indian Institute of Technology, Delhi 3 Department of Computer Science, Princeton University 4 Georgia Tech 5 Independent Researcher * Equal Contribution schaudhari@cs.umass.edu, pranjal2041@gmail.com  A BSTRACT State-of-the-a...

Specific Evidence for Classification:
- "arXiv:2404.08555v2 [cs.LG] 16 Apr 2024  RLHF D ECIPHERED : A C RITICAL A NALYSIS OF R EINFORCEMENT L EARNING FROM H UMAN F EEDBACK FOR LLM S  Shreyas Chaudhari*1  Pranjal Aggarwal*2  Ashwin Kalyan5  Karthik Narasimhan3  Vishvak Murahari3 Ameet Deshpande3  Tanmay Rajpurohit4  Bruno Castro da Silva1  1  2  University of Massachusetts Amherst Department of Computer Science, Indian Institute of Technology, Delhi 3 Department of Computer Science, Princeton University 4 Georgia Tech 5 Independent Researcher * Equal Contribution schaudhari@cs.umass.edu, pranjal2041@gmail.com  A BSTRACT State-of-the-art large language models (LLMs) have become indispensable tools for various tasks."
- "A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
