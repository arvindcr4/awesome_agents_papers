Multi-level Advantage Credit Assignment
for Cooperative Multi-Agent Reinforcement Learning

arXiv:2508.06836v1 [cs.AI] 9 Aug 2025

Xutong Zhao
Mila - Quebec AI Institute
Polytechnique Montréal

Abstract
Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple
agents to achieve a common goal. A key challenge in MARL is credit assignment, which
involves assessing each agent’s contribution
to the shared reward. Given the diversity
of tasks, agents may perform different types
of coordination, with rewards attributed to
diverse and often overlapping agent subsets.
In this work, we formalize the credit assignment level as the number of agents cooperating to obtain a reward, and address scenarios with multiple coexisting levels. We
introduce a multi-level advantage formulation
that performs explicit counterfactual reasoning to infer credits across distinct levels. Our
method, Multi-level Advantage Credit Assignment (MACA), captures agent contributions
at multiple levels by integrating advantage
functions that reason about individual, joint,
and correlated actions. Utilizing an attentionbased framework, MACA identifies correlated
agent relationships and constructs multi-level
advantages to guide policy learning. Comprehensive experiments on challenging Starcraft
v1&v2 tasks demonstrate MACA’s superior
performance, underscoring its efficacy in complex credit assignment scenarios.

1

INTRODUCTION

Multi-agent systems (MAS) are inherently applicable
to a broad spectrum of real-world applications, ranging
from smart grid (Roesch et al., 2020) to swarm robotics
Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao,
Thailand. PMLR: Volume 258. Copyright 2025 by the author(s).

Yaqi Xie
School of Computer Science,
Carnegie Mellon University
(Hüttenrauch et al., 2017), and autonomous vehicles
(Shalev-Shwartz et al., 2016). Cooperative multi-agent
reinforcement learning (MARL) emerges as a general
learning framework for MAS with a common objective
among agents. Naively applying single-agent RL methods by regarding multiple agents as one single agent
with exponentially large joint action space is limited by
its scalability, and potentially leads to non-stationary
learning. A key challenge in MARL is multi-agent
credit assignment, the task of identifying the contribution of each individual agent’s action to the global
reward (Albrecht et al., 2023). Consider a warehouse
task where agents receive collective rewards for moving
objects. In this scenario, one agent A might collaborate
with agents B and C to carry a fridge, while another
agent, D, does nothing. Disentangling each agent’s
contribution to the total collective reward is crucial to
promoting effective cooperation strategies. However,
credit assignment is highly non-trivial given only the
joint actions and shared rewards. Real-world scenarios
often involve more complex multi-agent interactions,
rendering credit assignment more challenging.
Addressing the multi-agent credit assignment challenge
efficiently and scalably remains an open question. Prior
efforts to address it can be mainly categorized into: (1)
implicit methods, such as QMix (Rashid et al., 2018),
which learns a mixing network to decompose joint values, and (2) explicit methods, such as COMA (Foerster
et al., 2018), which performs counterfactual reasoning
to guide the learning of individual policies. However,
both lines of works only consider cooperation among a
certain fixed number of agents and overlook contributions from different or overlapping agent subsets.
Inspired by the study of human collaboration in cognitive neuroscience (Richerson et al., 2016), we propose a
multi-level formulation to model the credit assignment
in multi-agent cooperation. This approach considers
the general case where each agent collaborates with distinct subsects of agents. We formalize the type of cooperation into distinct levels, where each level corresponds
to the number of agents needed to obtain the reward

Multi-level Advantage Credit Assignment

(detailed in Section 3.3). For example, in the warehouse
scenario described above, agent A is involved in a 3-level
cooperation. Additionally, agent A may simultaneously
carry a backpack, resulting in both 1-level and 3-level
cooperations. This coexistence of multi-level cooperation underscores the necessity of our multi-level credit
assignment formulation, designed to explicitly recognize
and assign credit across different levels of cooperation.
More specifically, we propose a k-level counterfactual
credit assignment formulation, further extending it to a
general multi-level credit assignment to accommodate
coexisting levels. Inspired by Foerster et al. (2018), we
introduce a k-level counterfactual advantage function.
The advantage is defined as the discrepancy between
the state-action value of the taken joint action and a
counterfactual baseline, which marginalizes out the k
agents’ actions by their policies while keeping other
agents’ actions fixed. This formulation explicitly performs counterfactual reasoning that deduces the joint
contribution by the k-agent subset. Accounting for circumstances with coexisting different levels, we extend
k-level credit assignment to multi-level credit assignment by amalgamation of multiple k-level advantages
with different k’s. Since each k-level advantage function
is tailored to a specific level, the multi-level advantage
captures contributions across different levels.

robustly, particularly in the more demanding contexts
of the SMACv2 tasks. Additionally, ablation studies
showcase that every single level is essential in MACA’s
multi-level advantage. Theoretical analysis establishes
an in-depth understanding of the strong performance.

We then introduce Multi-level Advantage Credit
Assignment (MACA), an actor-critic based approach
to address the multi-agent credit assginment challenge
based on our multi-level counterfactual credit assignment formulation. MACA’s multi-level advantage is
constructed by three k-level advantages that respectively attribute contributions to individual actions,
joint actions, and actions taken by the subset of
strongly correlated agents. Although tasks may involve
a variety of credit assignment levels, MACA focuses
on the most significant ones. It adapts the transformer
encoder architecture (Vaswani et al., 2017), leveraging
its attention mechanism to model correlations among
agents (Zhang et al., 2022), which is further used to
construct the multi-level counterfactual advantage.
The dynamically determined agent correlation serves as
a basis for adaptive credit assignment at different states,
which promotes effective contribution estimation and
efficient learning across various scenarios.

Recent works on multi-agent credit assignment can be
categorized into two lines of approaches based on the
utilized assignment mechanism: (1) implicit methods,
which implicitly learns agent contribution via a decomposition of the joint value function, and (2) explicit
methods, which rely on some explicit mechanism to
infer agents’ contributions.

We evaluate MACA on two popular cooperative MARL
benchmarks, the StarCraft Multi-Agent Challenge
(SMAC) (Samvelyan et al., 2019), and the newly
extended SMACv2 benchmark, which introduces
significantly greater stochasticity (Ellis et al., 2022).
Tasks in both benchmarks cover various credit assignment types, making them ideal for testing multi-agent
credit assignments. Our empirical findings highlight
MACA’s ability to enhance performance effectively and

Our main contributions are summarized as follows:
• Inspired by human collaboration, we propose a
multi-level credit assignment formulation that recognizes and assigns credit to individual agents
across different levels of collaboration.
• Based on the formulation, we present MACA, an
actor-critic approach that addresses the multiagent credit assignment challenge by leveraging
multi-level counterfactual advantage functions to
capture contributions of individual actions, joint
actions, and actions by strongly correlated agents.
• Experiments on challenging Starcraft benchmarks
demonstrate MACA outperforms previous stateof-the-art. Notably, it shows more significant advantages in tasks of higher complexity.

2

RELATED WORK

Implicit Credit Assignment Most prior works
perform implicit credit assignment by learning a decomposition of the joint value function to per-agent value
functions (Sunehag et al., 2017; Rashid et al., 2018;
Son et al., 2019; Wang et al., 2020a). VDN (Sunehag
et al., 2017) assumes linearity and obtains the joint
value by summing over all individual values. QMix
(Rashid et al., 2018) lifts the linearity assumption by
learning a mixing neural network, but its monotonicity
constraints on the mixing weights also limit its expressiveness. HAPPO (Kuba et al., 2021a) is based on the
multi-agent advantage decomposition and employs a sequential policy update scheme, but the computational
complexity of sequential updates limits its scalability.
Explicit Credit Assignment One exemplar
family calculates difference rewards against a reward
baseline (Tumer and Agogino, 2007; Wolpert and
Tumer, 2001). COMA (Foerster et al., 2018) is an
actor-critic (AC) method implementing this idea
through a counterfactual advantage baseline. However,

Xutong Zhao, Yaqi Xie

it treats each agent as an independent entity without
considering interactions within the group, thus can be
inefficient in learning complex cooperative behaviours
(Papoudakis et al., 2020; Li et al., 2021). Other
AC methods such as MAPPO (Yu et al., 2021) and
MAA2C (Papoudakis et al., 2020) share a similar
formulation, but their advantage is only concerned
with joint actions’ contribution, without distinguishing
individual agents’ credits. Limited by their credit
assignment mechanism, these AC methods only deduce
contributions of the same set of agents, ignoring the
potential situations where one agent may contribute
through different or multiple overlapping agent subsets.
Other representative methods leverage the Shapley
Value (Shapley, 1953), but its computational complexity grows factorially with the number of agents
(Kumar et al., 2020), handicapping efficient training in
practice. Li et al. (2021) seeks to reduce computations
by approximating Shapley Value through Monte Carlo
sampling, but this formulation sacrifices certain properties of Shapley Value, such as symmetry. Other explicit
methods employ reward shaping. Li et al. (2022)
applies potential-based difference rewards, but learning
its reward model introduces more computational cost
compared to model-free approaches.
Attention in MARL Adopting transformers
or attention mechanisms (Vaswani et al., 2017) to
MARL has grown in popularity in recent years. Meng
et al. (2021) extends the Decision Transformer (Chen
et al., 2021) to the offline MARL regime. Baker
et al. (2019) uses an attention-based architecture to
learn permutation-invariant state representation and
demonstrates self-supervised autocurriculum in the
hide-and-seek task. Seraj et al. (2022) constructs a
heterogeneous graph-attention network architecture to
promote efficient multi-agent communication. Nayak
et al. (2023) also use the attention mechanism for
inter-agent communication, seeking to tackle the
2D navigation problem. However, few works have
leveraged attention-captured correlations to address
the credit assignment challenge in cooperative MARL.

3

PRELIMINARIES

3.1

Dec-POMDP

We consider the cooperative multi-agent setting as
a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek and Amato,
2016), which is formally defined as a tuple G =
⟨N , S, A, P, R, γ, O, Ω⟩ , where N = {1, . . . , n} is the
set of agents, S is the state space, A = A1 × · · · × An
is the joint action space, P : S × A × S → [0, 1] is
the transition probability, R : S × A × S → R is the

global reward function, γ ∈ [0, 1] is the discount factor, Ω = Ω1 × · · · × Ωn is the joint observation space,
O : S × N → Ω is the observation function. At each
timestep t, agents are in a state st ∈ S. Each agent
i ∈ N observes a local observation oti = O(st , i) ∈ Ωi ,
and selects an action ati ∈ Ai . All actions together
form the joint action at = [ati ]ni=1 . The environment
transitions to the next state st+1 ∼ P (s′ |st , at ), and
outputs a global reward r ∈ R shared across all agents.
Let Hi = (Ωi × Ai )∗ × Ωi represent the space of
agent i’s action-observation history. A trajectory is
a full joint history sequence τ = ⟨ot , at ⟩∞
t=0 . We use
π = π1 × · · · × πn to denote the joint policy, where
each πi (a′i |hti ) : Hi × Ai → [0, 1] is agent i’s policy
distribution that samples its action at every timestep.
The marginal state distribution dπ is induced by the
policy π and the transition probability P . For the simplicity of notations, in subsequent sections we assume
the Markov property and full observability, and we may
ignore superscripts/subscripts if the context is unambiguous. We use −i to denote all agents except agent i.
P∞
The discounted return is defined as Gt = k=0 γ k rt+k .
Based on agents’ joint policy, the state-value
function and action-value
 function are defined as
Vπ (s) = Es1:∞ ∼dπ ,a0:∞ ∼π G0 |s0 =s and Qπ (s, a) =
Es1:∞ ∼dπ ,a1:∞ ∼π G0 |s0 = s, a0 = a , respectively. By
definition Vπ (s) = Ea∼π [Qπ (s, a)]. The advantage
function is defined as Aπ (s, a) = Qπ (s, a) − Vπ (s).
The MARL objective is to find a joint policy
 that
maximizes the expected return J(θ) = Eτ G0 .
3.2

Policy Gradient Methods

In RL, policy gradient (PG) methods optimize the
policy π by performing gradient updates on the objective J(θ), where π is parameterized by θ. The
Policy Gradient Theorem (Sutton et al., 1999) provides the gradient with respect to θ as ∇θ J(θ) =
Es∼dπ ,a∼π [Q(s, a)∇θ log πθ (a|s)]. The MAPG theorem (Foerster et al., 2018; Zhang et al., 2018) extends
the single-agent policy gradient to
gθi = ∇θi J(θ) = Es∼dπ ,a∼π [Q(s, a)∇θi log πi (ai |s)]
(1)
In the AC framework, a critic learns the value function,
constructing the gradient to update the actor, i.e.
the policy. A popular variance-reduction approach
is to subtract the Q-value estimate by a baseline
function b (Weaver and Tao, 2013). One common
choice b(s) = V (s) gives the advantage function
Q(s, a) − V (s) by definition. We can show that any
action-independent baseline function preserves unbiasedness of the gradient estimate in expectation, i.e.,
Es∼dπ ,a∼π [bi ∇θi log πi (ai |s)] = 0 for any agent i ∈ N

Multi-level Advantage Credit Assignment

if bi does not depend on ai (see Appendix A for proof).

write a k-level counterfactual baseline

In this work, we follow the centralized training with
decentralized execution (CTDE) (Bernstein et al., 2002)
MARL learning paradigm. Extending AC approaches
to MARL, the CTDE formulation is usually centralized
critic and decentralized actors, where the critic learns
the value function based on the global state information
(and potentially joint actions), which is used to optimize
the policies following Equation (1).

bCF
i (s, a) = EaGi [Q(s, a)]

3.3

Credit Assignment Level

The concept of credit assignment level is inspired by
the coordination level in MARL tasks (Liu et al., 2022).
Let G ⊂ N denote a subset of |G| = k agents. Different
subsets are allowed to overlap. At each timestep t, we
use rG (st , at ) = rG (st , atG ) to denote the joint reward
that can only be obtained if the subset of k agents
cooperate, where atG = {ati |i ∈ G}. We then represent
the global reward as the sum of rewards
contributed by
P
all subsets of agents: r(st , at ) = G⊂N rG (st , at ). We
consider the number of agents k as a credit assignment
level. We then consider credit assignment and associated contributions with different levels as different
types. This formalism intends to cover complex situations that involve an individual agent in different credit
assignment types across timesteps, or even coexisting
different types at the same timestep.

4

METHOD

In this section, we introduce MACA, an actor-critic
MARL method that explicitly models multiple levels
of credit assignment.
This approach delineates
three distinct advantage functions, each designed to
assess contributions from individual actions, joint
actions, and actions taken by strongly correlated
partners. With multi-level contributions encoded by
the combination of these advantages, MACA provides
a comprehensive framework for addressing multi-agent
credit assignment challenges, accommodating the
diverse range of interactive behaviors among agents.
4.1

Multi-Level Counterfactual Formulation

k-Level Advantage Motivated by Foerster et al.
(2018), we consider the advantage in the MAPG estimator as an approach to counterfactually assessing
agents’ contributions. With a centralized critic learning Q(s, a), when we compute the advantage function
Ai (s, a) for each agent i, the actions marginalized out
in the Q-based baseline correspond to the agents whose
contributions we reason about. In particular we can

(2)

where aGi = {aj ∼ πj : j ∈ Gi } and Gi is an
arbitrary subset of |Gi | = k agents that satisfies
{i} ⊂ Gi ⊂ N . The resulting advantage Ai (s, a) =
Q(s, a) − bCF
i (s, a) provides a general form of counterfactual advantage that explicitly reasons about credit
assignment by the k-agent subset. For instance, COMA
computes the baseline bInd
i (s, a) = Eai ∼πi [Q(s, a)],
thereby reasoning about the individual action’s contribution. MAPPO/MAA2C estimates the baseline
bJnt
(s, a) = V (s) = Ea∼π [Q(s, a)]. Intuitively, this adi
vantage evaluates how much the joint action is better
or worse than the default behaviour. As all actions
are marginalized out, they do not differentiate each
particular agent’s contribution. It is trivial to show
that the bCF
encompasses both bInd
and bJnt
.
i
i
i
While the minimum-variance baseline b∗i (see Theorem A.2 ) is usually infeasible to compute in
complex MDPs, our k-level counterfactual baseline
bCF
also reduces the variance of MAPG estimates
i
and assists learning in practice. The rational is
clear when each k-level baseline is expressed as
Cov(Q,||∇i log πi ||2 )
bCF
= b∗i − E[||∇i log πi ||2 ]
(see Lemma A.3 ). The
i

2
term Cov Q, ||∇i log πi || typically becomes negative
as the gradient becomes smaller on actions with high
returns during the optimization process, leading to
the baseline being an optimistic baseline (Chung et al.,
2021), which justifies our design choice.
We consider one important subset Gi whose agents are
strongly correlated with agent i. We name this subset
the CorrSet Ci , and the corresponding baseline the
CorrSet baseline bCor
. We evaluate inter-agent correlai
tions conditioned on the state information so that bCor
i
is capable of adapting dynamically to different credit
assignment levels across timesteps. We discuss how we
determine Ci in detail in the following subsection.
Multi-Level Advantage As discussed in previous
sections, we consider the situation where one agent’s
contribution may simultaneously involve a mixture of
multiple credit assignment levels. We aim to reason
about each of them using a different k-level advantage
function, and combine them into one advantage, which
we call the multi-level counterfactual advantage. To
obtain such an advantage, due to the same action value
function used in all advantage functions, equivalently
we only need to compute corresponding k-level
baselines and a combination of them. In particular,
for each agent i, we compute the three most important
k-level baselines: (1) k = n: the joint action set
baseline bJnt , (2) k = 1: the individual action set
baseline bInd
i , and (3) k = |Ci |: a CorrSet baseline

Xutong Zhao, Yaqi Xie

Figure 1: MACA critic architecture. The sequence of agent observations (ot1 , . . . , otn ) inputs to an embedding
layer and a self-attention encoder, and the output passes through an MLP layer to produce the state embedding
zs . The attention weight matrix Ãt , joint actions at , and joint policy distribution π t pass through the function
t
ff use to obtain action distributions π̄Ct , π̄Ind
, corresponding to CorrSet and individual actions, respectively. The
embedding zs and action distributions are fed to a linear layer to get respective Q values, and the coefficients
[ψm ] weight them to obtain the final MACA baseline btM ACA .
bCor
. In principle, a multi-level advantage does not
i
limit the number of k-level advantage components.
We then obtain our MACA baseline by integrating
these three baselines by a weighted sum
bMACA
= ψiJnt bJnt + ψiInd bInd + ψiCor bCor
i

(3)

AMACA
= Q(s, a) − bMACA
i
i

(4)

where the weighting coefficients [ψ m ]m∈[3] ∈ ∆(3) are
state-dependent. Hence the resulting advantage function AMACA
reasons different credit assignment levels
i
via its multi-level baselines. We discuss how we learn
the coefficients using stochastic optimization in the
following subsection. As each of the k-level baselines
marginalizes out the action ai , the resulting MACA
baseline is also independent of ai , which preserves unbiasedness in policy gradient estimates (see Lemma A.1
). This property ensures MACA advantage is generally
compatible with the MAPG framework and applicable
to various algorithms. Lemma A.4 derives the convergence of MACA to a local optimal policy, by following
the convergence proof of single-agent actor-critic (Sutton et al., 1999; Konda and Tsitsiklis, 1999) subjecting
to the same assumptions. We hereby provide a proof
sketch. The detailed proof is deferred to Appendix A.
Proof sketch of Lemma A.4.
Since each k-level
baseline marginalizes out the action ai , the multi-level
baseline as a linear combination of all k-level baselines
is independent of ai . By Lemma A.1 it does not
introduce bias to MAPG estimates. Therefore the
multi-level baseline does not affect the convergence in
expectation. Writing the joint policy as the product
of individual policies, the remaining proof directly
follows Konda and Tsitsiklis (1999).

4.2

Attention-based Framework

In this section, we introduce our learning framework
that constructs the MACA advantage function. We
discuss how we utilize the self-attention mechanism to
quantify agent-wise relative correlation and construct
the CorrSet. We then present how we perform optimization to learn policies, values, and parameters that
compute the MACA baseline. The MACA architecture
is illustrated in Figure 1.
Critic Encoder Architecture The critic leverages
the multi-agent transformer encoder (Vaswani et al.,
2017; Wen et al., 2022). It inputs the sequence of agents’
observations (ot1 , . . . , otn ) to an embedding layer and M
encoding blocks. Each block consists of a self-attention
mechanism, a multi-layer perceptron (MLP) layer, and
residual connections and layer normalizations (Ba et al.,
2016) to avoid gradient vanishing. The output passes
through an MLP layer to produce a state embedding
zs . We compute the attention rollout weights (Abnar
and Zuidema, 2020) of the last layer, denoted as Ã
(referred to as attention weights for simplicity).
Value Estimation We aim to compute any k-level
counterfactual baseline bCF
i (s, a). To improve scalability and flexibility, we adopt a design that inputs
the joint action distribution parameters (Wierstra and
Schmidhuber, 2007) by bCF
i (s, a) = EaGi [Q(s, a)] =
Qϕ (s, π̄Gi ), where π̄Gi = EaGi [a] ∈ R|A| is the action/policy distribution, and Qϕ is the value approximation parameterized by ϕ. Regarding per-agent policies
in π̄Gi , the marginalized actions {aj : j ∈ Gi } correspond to the original policies, and all other policies are
one-hot with the probability of taken actions equal to 1.
We overabuse the same notation to denote the policy

Multi-level Advantage Credit Assignment

Table 1: Overview of advantage functions.
Algorithm

Update

Advantage function Ai (s, a)

COMA

Simultaneous

Q(s, a) − Eai ∼πi [Q(s, a)]

MAPPO

Simultaneous

Q(s, a) − Ea∼π [Q(s, a)]

IPPO

Simultaneous

Qi (s, ai ) − Eai ∼πi [Qi (s, ai )]

PPO-Sum

Simultaneous

Qi (s, ai ) − Eai ∼πi [Qi (s, ai )] ; Q =

PPO-Mix

Simultaneous

HAPPO

Sequential

∂Q
≥0
Qi (s, ai ) − Eai ∼πi [Qi (s, ai )] ; Q = fMix ([Qi ]), ∂Q
i
Qi−1 π̄j (aj |s)
( j=1 πj (aj |s) ) (Q(s, a) − Ea∼π [Q(s, a)])

MACA-Cor (ours)

Simultaneous

Q(s, a) − EaCi [Q(s, a)] ; {i} ⊂ Ci ⊂ N

MACA (ours)

Simultaneous

Q(s, a) − ψ 1 Ea∼π [Q(s, a)] + ψ 2 Eai ∼πi [Q(s, a)]

+ψ 3 EaCi [Q(s, a)] ; {i} ⊂ Ci ⊂ N , [ψ m ] ∈ ∆(3)

distribution whose joint action and individual action
are marginalized out by π̄iJnt and π̄iInd , respectively.
Note that π̄iJnt = π, the original joint policy.
It is worth noting that in general EaGi [Q(s, a)] ̸=
Q(s, π̄Gi ). While recent work has ignored this issue
(Zhou et al., 2020), we seek a sound solution that ensures equality. As per Jensen’s inequality (Jensen, 1906)
we feed the state embedding zs and π̄Gi through a linear
layer bCF
i (s, a) = Lin(zs , π̄Gi ). To validate this layer’s
expressivity, we investigate a transformer-decoder modeling option in ablation experiments in Section 5.3.
CorrSet Construction We utilize inter-agent relationships represented by the pre-computed attention
weights Ã to infer the CorrSet Ci . Note that this
component only relies on linear computations.
For each agent i, Ã ∈ Rn contains weights associated
with all n agents, each of which is denoted by Ãi,j , j ∈
N . The self-attention mechanism reasons correlations
among input tokens (Zhang et al., 2022). Intuitively,
higher attention weights indicate stronger correlations
between input tokens, and vice versa. Hence to identify
the subset of strongly correlated agents, we find agents
with high attention weights – i.e., let j ∈ Ci if Ãi,j ≥
σ, where σ ∈ [0, 1] is a thresholding hyperparameter.
To preserve unbiasedness in gradient estimates, we
additionally enforce i ∈ Ci regardless of Ãi,i as per
definition of k-level baseline. With the CorrSet Ci
settled, we can follow the procedure described above
Cor
to compute all baselines (bJnt , bInd
) and further
i , bi
the MACA advantage.
Training Losses We follow the standard actor-critic
training paradigm. We optimize the actors through the
MAPG as in Equation (1). We update value function
parameters ϕ on-policy by optimizing the mean-squared

P

i Qi

TD error over collected trajectories:

 
LV (ϕ) = Eτ ||V (st ) − sg rt + γV (st+1 ) ||22

 
LQ (ϕ) = Eτ ||Q(st , at ) − sg rt + γQ(st+1 , at+1 ) ||22
where V (st ) = Q(st , π) as discussed above, and sg(·)
indicates the stop-gradient operation.
To learn the weighting coefficients [ψiJnt , ψiInd , ψiCor ]
involved in the computation of MACA advantage
(Equation (4)), it is important to realize they indirectly
affect performance improvement. Since policy gradient
updates only rely on the value of advantage functions,
the MAPG objective is indifferentiable with respect to
coefficients ψ’s. Similarly the TD updates performed
on value functions do not optimize the weights either.
We hence consider the stochastic optimization setting.
We compute each coefficient by a linear layer Lin(zs ; η)
parameterized by η, and apply a softmax over all
coefficients to obtain a probability distribution. We
leverage the CMA-ES method (Hansen et al., 2019)
to optimize the performance difference between
policies after and before policy updates using MACA
advantages, that is, L(η) = −Eτ [R(θn+1 ) − R(θn )],
where R(θ) is the cumulative trajectory reward
achieved by policy parameterized by θ.

5

EXPERIMENTS

In this section, we present empirical experiments and
discuss the results. We first describe the experimental
setup. Then we compare the performance of MACA
and other state-of-the-art approaches. We also present
ablation studies to demonstrate the critical role of
MACA’s different components.

Xutong Zhao, Yaqi Xie

Task Type

MACA

MAPPO

IPPO

PPO-Mix

PPO-Sum

COMA

HAPPO

Steps

25m
5m_vs_6m
8m_vs_9m
10m_vs_11m
3s5z

homo.
homo.
homo.
homo.
hetero.

99.3(0.1)
87.0(2.0)
99.0(0.6)
100.0(0.0)
99.2(0.8)

99.5(0.3)
75.2(1.5)
94.5(1.9)
87.0(8.6)
96.5(0.9)

99.5(0.5)
78.0(0.9)
95.0(1.4)
98.8(0.7)
99.0(1.0)

25.0(3.4)
24.5(13.3)
57.0(8.1)
22.4(1.7)
97.5(0.8)

68.0(14.9)
67.0(3.2)
66.0(13.2)
69.0(16.7)
0.5(1.1)

0.0(0.0)
0.9(0.5)
1.0(0.6)
3.0(2.0)
0.5(1.1)

85.8(0.8)
76.7(3.0)
86.7(5.1)
93.3(1.7)
99.2(0.8)

3e6
8e6
8e6
8e6
8e6

protoss_5_vs_5
terran_5_vs_5
zerg_5_vs_5
protoss_10_vs_10
terran_10_vs_10
zerg_10_vs_10

hetero.
hetero.
hetero.
hetero.
hetero.
hetero.

79.0(3.4)
74.4(8.3)
63.4(5.5)
75.8(3.9)
75.0(5.2)
62.9(8.3)

56.5(4.6)
50.5(2.7)
42.3(1.4)
53.0(3.1)
40.0(5.2)
39.8(3.0)

54.2(2.7)
57.7(2.5)
37.5(3.6)
38.0(4.0)
34.7(0.8)
21.8(2.8)

61.1(2.7)
57.0(3.8)
41.5(4.4)
33.0(4.1)
37.6(3.3)
32.0(2.2)

32.5(6.8)
44.1(6.7)
32.8(5.5)
26.0(3.4)
33.1(6.6)
21.3(3.9)

2.0(1.5)
7.0(2.2)
4.5(0.9)
0.5(1.1)
2.5(1.4)
1.0(0.6)

50.0(2.5)
55.8(3.6)
42.5(1.4)
21.7(4.2)
17.5(3.8)
17.5(5.2)

1e7
1e7
1e7
1e7
1e7
1e7

5.1

Experimental Setup

SMAC

Task

SMACv2

Table 2: Mean evaluation win rate and standard deviation for different methods on SMAC v1&v2 tasks. A win
rate is marked in bold if it is within the critical region of the significance test.

Baseline Methods We compare MACA with SOTA
multi-agent credit assignment approaches. To promote
fair comparison, we adopt all methods into the MAPPO
learning framework. We ensure the only difference
among all methods is advantage function computation
unless otherwise specified. We use well-established
implementations of MAPPO/IPPO/HAPPO (Zhong
et al., 2023), transformer encoder for MACA
(Karpathy, 2023; Wen et al., 2022), and COMA/PPOMix/PPO-Sum (Papoudakis et al., 2020). We include
the following methods, whose corresponding advantage
functions are summarized in Table 1.
MAPPO optimizes all decentralized PPO (Schulman
et al., 2017) actors with a centralized critic. MAPPO
and IPPO are the SOTA on-policy MAPG algorithms
in SMACv1 tasks (Yu et al., 2021).
IPPO (Independent PPO) directly adopts PPO that
learns an individual critic for each actor. IPPO performs competitively with MAPPO across different environments (Papoudakis et al., 2020; Yu et al., 2021;
de Witt et al., 2020).
HAPPO (Kuba et al., 2021a) extends MAPPO by
the sequential policy update scheme. Experimental
results show competitive performance to MAPPO on
SMACv1&v2 (Zhong et al., 2023).
COMA (Foerster et al., 2018) performs counterfactual
reasoning by marginalizing out the current agent’s action in the baseline, as mentioned in previous sections.
PPO-Mix is built upon the value decomposition
method QMix (Rashid et al., 2018) that learns the
joint Q-value as a monotonic function of individual
Q values. PPO-Mix shares the same architecture
as FACMAC (Peng et al., 2021), but the skeleton
algorithm is the MAPPO rather than MADDPG to
ensure a fair comparison.
PPO-Sum is identical to PPO-Mix, except the value

decomposition method is VDN (Sunehag et al., 2017),
which represents the joint value function as the sum
of individual value functions. PPO-Sum is analogous
to DOP (Wang et al., 2020b), with MAPPO backbone
instead of MADDPG for the same reason above.
Evaluation We evaluate MACA on challenging
benchmark StarCraft Multi-Agent Challenge (SMAC)
(Samvelyan et al., 2019) (referred to as SMACv1
hereafter), and the recently proposed SMACv2 testbed
(Ellis et al., 2022). We evaluate methods on five tasks
from SMACv1, and six tasks from SMACv2, covering
a broad spectrum of agent types and task scenarios.
These tasks include both homogeneous and heterogeneous domains that ensure adequate coverage of task
diversity and complexity, enabling a comprehensive
empirical evaluation. We adopt the same hyperparameter settings from the original codebase and performed
coarse finetuning, as detailed in Appendix C. We
train each algorithm for 8M timesteps in SMACv1
tasks and 10M timesteps in SMACv2 tasks, with five
random seeds per task. During training, agents are
evaluated every 160k timesteps with 40 independent
runs. We report the mean values and the standard
deviation from evaluation win rates. We also conduct
experiments on three Multi-Agent Particle (Lowe
et al., 2017; Terry et al., 2021) (MPE) tasks following
the same protocol. We report the mean value and the
standard deviation of evaluation episodic returns.
5.2

Empirical Results

In this section, we present and discuss evaluation results
from experiments on SMACv1&v2 testbeds. Table 2
summarizes the evaluation win rates for all methods.
To determine the best performances, we perform a
two-sample t-test (Snedecor and Cochran, 1980) with
a significance level of 0.05 between the algorithm with
the maximum mean and each of the other algorithms
in every task. Win rates that are not significantly
different from the best performance are marked in bold.

Multi-level Advantage Credit Assignment
protoss_5_vs_5

terran_5_vs_5

80%

80%

60%

60%

zerg_5_vs_5
70%

40%

Win rate (%)

Win rate (%)

Win rate (%)

60%

40%

20%

20%

0%

0%

50%
40%
30%
20%
10%

0.0

0.2

0.4

0.6

0.8

1.0

0%
0.0

0.2

1e7

Steps

0.6

0.8

1.0

0.0

70%

60%

60%

50%
40%
30%
20%

60%

30%
20%

0%

0%
0.4

0.6

0.8

1.0

MACA

MAPPO

50%
40%
30%
20%
10%
0%

0.0

0.2

0.4

1e7

Steps

1.0

1e7

zerg_10_vs_10

40%

10%

0.8

70%

50%

10%

0.6

Steps

Win rate (%)

70%

Win rate (%)

80%

0.2

0.4

terran_10_vs_10

80%

0.0

0.2

1e7

Steps

protoss_10_vs_10

Win rate (%)

0.4

0.6

0.8

1.0

IPPO

0.0

0.2

0.4

1e7

Steps
PPO-Mix

0.6

0.8

1.0

1e7

Steps

PPO-Sum

COMA

HAPPO

Figure 2: Performance on the SMACv2 benchmark.

PPO-Mix generally performs well in SMACv2, but it
occasionally fails in some SMACv1 tasks, e.g. in 25m.
PPO-Sum shows large variances in SMACv1 tasks,
and in general downperforms PPO-Mix.
COMA empirically performs poorly across all tasks
in both SMACv1&v2. Such failure is consistent with

5m_vs_6m
Win rate (%)

80%
60%
40%
20%
0%

60%

MACA
MAPPO
MACA-Dec
MACA-Jnt
MACA-Cor

40%
20%
0%

0.0

0.2

0.4

0.6

0.8

Steps

1.0

0.0

1e7

0.2

0.4

0.6

Steps

protoss_10_vs_10

0.8

1e7

5m_vs_6m

80%
80%

Win rate (%)

In the SMACv1 benchmark, MACA achieves
a superior overall performance.
Although
MAPPO/IPPO/HAPPO performs competitively
in the majority of tasks, they show inferior performance in certain tasks; thus MACA is significantly
better than at least one of them in four out of five tasks.
Therefore, MACA demonstrates more robustness than
other methods in general.

protoss_10_vs_10
80%

Win rate (%)

Results in Table 2 and Figure 2 show that MACA
reaches stronger overall performance compared to the
SOTA in SMACv1 benchmark, and superior performance in more challenging SMACv2 benchmarks. Particularly in SMACv2 where MACA significantly outperforms all other methods, MACA gains higher sample
efficiency. MACA improves much faster as learning progresses, while other methods tend to reach convergence.
Such sample efficiency gain leads to higher final win
rates. Although MAPPO and PPO-Mix also demonstrate strong performance in two tasks, by our statistical test MACA still significantly outperforms both.

prior works (Papoudakis et al., 2020; Kuba et al., 2021b;
Wang et al., 2022). Wang et al. (2022) argues COMA’s
poor performance may be due to relative overgeneralization, a common game theoretic pathology that the
suboptimal actions are preferred (Wei et al., 2018).

Win rate (%)

Figure 2 shows the learning progress of all methods
in SMACv2 benchmarks. We present learning curves
in SMACv1 benchmarks, MPE results, and additional
discussions in Appendix B.

60%
40%
20%
0%

60%
40%

MACA
MACA-NoCor
MACA-NoInd
MACA-NoJnt

20%
0%

0.0

0.2

0.4

0.6

Steps

0.8

1.0

1e7

0.0

0.2

0.4

Steps

0.6

0.8

1e7

Figure 3: Ablation performance.
5.3

Ablations

In this section, we perform ablation experiments
on MACA’s key components. Results are shown in
Table 3 and Figure 3. We evaluate the following
variants on 5m_vs_6m and protoss_10_vs_10.
MACA-Dec is algorithmically equivalent to MACA,
with the linear layer replaced with a transformer decoder (Vaswani et al., 2017) to perform value estimation.
A more detailed illustration is presented in Figure 7

Xutong Zhao, Yaqi Xie

Table 3: Mean evaluation win rate and standard deviation for MACA and its ablation variants.
Task

MACA

MACA-Dec

MACA-Jnt

MACA-Cor

MACA-Ind

MACA-NoCor

MACA-NoInd

MACA-NoJnt

5m_vs_6m
protoss_10_vs_10

87.0(2.0)
75.8(3.9)

84.0(2.3)
64.7(5.1)

79.2(3.0)
55.8(15.2)

70.6(1.9)
59.2(11.7)

0.9(0.6)
0.3(0.9)

75.0(1.8)
52.5(6.0)

78.1(1.9)
61.9(6.7)

70.0(4.7)
65.0(7.6)

in Appendix B.5. This variant aims to validate the expressivity of linear value estimation in MACA. Results
show that MACA is comparable with MACA-Dec in
both tasks, suggesting that the linear layer is capable
of effectively estimating the value function.
To ablate the importance of the MACA advantage, we
remove different baseline components by enforcing their
corresponding weighting coefficients to 0 and applying
softmax to the remaining coefficients to obtain a valid
categorical distribution. MACA-Jnt only uses the joint
action set baseline bJnt . Since this variant is equivalent
to MAPPO with a transformer encoder-based critic,
we attempt to verify (1) the effect of transformer
encoder compared with MLP, and (2) the importance
of multi-level advantage. Results show that MACA-Jnt
reaches similar performance as MAPPO yet still downperforms MACA. This demonstrates that improvement
by only introducing a complex architecture is rather
limited. The MACA advantage plays a more important
role in performance improvement. MACA-Cor only
uses the CorrSet baseline bCor . This variant similarly
downperforms MACA, highlighting the importance of
the multi-level advantage.
MACA-NoCor, MACA-NoInd, and MACA-NoJnt remove the bCor , bInd , and bJnt , respectively, seeking to
evaluate individual components within the advantage
baseline. Results indicate that each advantage term is
essential and contributes to performance improvement.
In addition, we conducted ablation experiments on
MACA-Ind, the variant that only keeps the bInd baseline. Similar to the equivalence between MACA-Jnt
and MAPPO, MACA-Ind is essentially COMA, where
the only difference is its attention-based critic. MACAInd can also demonstrate the limited effect of only
introducing the attention encoder, which aligns with
conclusions from MACA-Jnt. However, both COMA
and MACA-Ind empirically perform poorly, making
the comparison not as informative as that between
MACA-Jnt and MAPPO.

6

CONCLUSIONS

In this work, we tackle the credit assignment problem
in cooperative MARL by considering different levels
of credit assignment explicitly. We formalize the
per-level advantage that counterfactually deduces
contributions over a specific level. We propose MACA,

an actor-critic method that constructs three different
counterfactual advantage functions to respectively infer
contributions from individual actions, joint actions,
and actions taken by strongly correlated partners.
MACA leverages a transformer-based architecture
to capture agents’ correlations via the attention
mechanism. With multi-level contributions encoded by
the combination of these advantages, MACA provides
a generic formulation to address credit assignment
challenges. Empirical evaluations on challenging
Starcraft benchmarks underscore MACA’s superior
performance, showcasing its efficacy in complex
cooperative MARL scenarios. Theoretical results
provide support for MACA’s strong performance.
In the future, it would be interesting to establish a connection between MACA and exploration methods via
the advantage function, aiming to enhance performance
across both regimes.
Limitations. This work addresses the multi-agent
credit assignment challenge in the cooperative setting,
where agents receive collective rewards. Other complex
settings such as the mixed cooperative-competitive
scenario could further complicate the problem, which
is beyond the scope of this work.

Acknowledgments
We appreciate the anonymous reviewers for their insightful comments and constructive suggestions that
improved the quality of this manuscript. We acknowledge the computational resources provided by Mila and
the Digital Research Alliance of Canada.
References
Abnar, S. and Zuidema, W. (2020). Quantifying
attention flow in transformers. arXiv preprint
arXiv:2005.00928.
Albrecht, S. V., Christianos, F., and Schäfer, L. (2023).
Multi-agent reinforcement learning: Foundations and
modern approaches. Massachusetts Institute of Technology: Cambridge, MA, USA.
Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer
normalization. arXiv preprint arXiv:1607.06450.
Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., and Mordatch, I. (2019). Emer-

Multi-level Advantage Credit Assignment

gent tool use from multi-agent autocurricula. arXiv
preprint arXiv:1909.07528.
Bernstein, D. S., Givan, R., Immerman, N., and Zilberstein, S. (2002). The complexity of decentralized
control of markov decision processes. Mathematics
of operations research, 27(4):819–840.
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,
Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.
(2021). Decision transformer: Reinforcement learning
via sequence modeling. Advances in neural information processing systems, 34:15084–15097.

Kuba, J. G., Wen, M., Meng, L., Zhang, H., Mguni,
D., Wang, J., Yang, Y., et al. (2021b). Settling the
variance of multi-agent policy gradients. Advances
in Neural Information Processing Systems, 34:13458–
13470.
Kumar, I. E., Venkatasubramanian, S., Scheidegger, C.,
and Friedler, S. (2020). Problems with shapley-valuebased explanations as feature importance measures.
In International Conference on Machine Learning,
pages 5491–5500. PMLR.

Chung, W., Thomas, V., Machado, M. C., and Le Roux,
N. (2021). Beyond variance reduction: Understanding the true impact of baselines on policy optimization. In International Conference on Machine Learning, pages 1999–2009. PMLR.

Li, J., Kuang, K., Wang, B., Liu, F., Chen, L., Wu,
F., and Xiao, J. (2021). Shapley counterfactual
credits for multi-agent reinforcement learning. In
Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining, pages 934–
942.

Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C.
(2022). Flashattention: Fast and memory-efficient
exact attention with io-awareness. Advances in neural information processing systems, 35:16344–16359.

Li, Y., Xie, G., and Lu, Z. (2022). Difference advantage
estimation for multi-agent policy gradients. In International Conference on Machine Learning, pages
13066–13085. PMLR.

de Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S.
(2020). Is independent learning all you need in
the starcraft multi-agent challenge? arXiv preprint
arXiv:2011.09533.
Ellis, B., Cook, J., Moalla, S., Samvelyan, M., Sun,
M., Mahajan, A., Foerster, J. N., and Whiteson, S.
(2022). Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv
preprint arXiv:2212.07489.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N.,
and Whiteson, S. (2018). Counterfactual multi-agent
policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32.
Hansen, N., Akimoto, Y., and Baudis, P.
(2019). CMA-ES/pycma on Github. Zenodo,
DOI:10.5281/zenodo.2559634.
Hüttenrauch, M., Šošić, A., and Neumann, G. (2017).
Guided deep reinforcement learning for swarm systems. arXiv preprint arXiv:1709.06011.
Jensen, J. L. W. V. (1906). Sur les fonctions convexes
et les inégalités entre les valeurs moyennes. Acta
mathematica, 30(1):175–193.
Karpathy, A. (2023). nanogpt. https://github.com/
karpathy/nanoGPT.
Konda, V. and Tsitsiklis, J. (1999). Actor-critic algorithms. Advances in neural information processing
systems, 12.
Kuba, J. G., Chen, R., Wen, M., Wen, Y., Sun, F.,
Wang, J., and Yang, Y. (2021a). Trust region policy
optimisation in multi-agent reinforcement learning.
arXiv preprint arXiv:2109.11251.

Liu, D., Shah, V., Boussif, O., Meo, C., Goyal, A., Shu,
T., Mozer, M., Heess, N., and Bengio, Y. (2022).
Stateful active facilitator: Coordination and environmental heterogeneity in cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2210.03022.
Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel,
O., and Mordatch, I. (2017). Multi-agent actor-critic
for mixed cooperative-competitive environments. Advances in neural information processing systems, 30.
Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W.,
Wen, Y., Zhang, H., Wang, J., and Xu, B. (2021).
Offline pre-trained multi-agent decision transformer:
One big sequence model tackles all smac tasks. arXiv
preprint arXiv:2112.02845.
Nayak, S., Choi, K., Ding, W., Dolan, S., Gopalakrishnan, K., and Balakrishnan, H. (2023). Scalable
multi-agent reinforcement learning through intelligent information aggregation. In International Conference on Machine Learning, pages 25817–25833.
PMLR.
Oliehoek, F. A. and Amato, C. (2016). A concise
introduction to decentralized POMDPs. Springer.
Papoudakis, G., Christianos, F., Schäfer, L., and Albrecht, S. V. (2020). Benchmarking multi-agent
deep reinforcement learning algorithms in cooperative tasks. arXiv preprint arXiv:2006.07869.
Peng, B., Rashid, T., Schroeder de Witt, C., Kamienny, P.-A., Torr, P., Böhmer, W., and Whiteson,
S. (2021). Facmac: Factored multi-agent centralised
policy gradients. Advances in Neural Information
Processing Systems, 34:12208–12221.

Xutong Zhao, Yaqi Xie

Rashid, T., Samvelyan, M., Schroeder, C., Farquhar,
G., Foerster, J., and Whiteson, S. (2018). Qmix:
Monotonic value function factorisation for deep multiagent reinforcement learning. In International conference on machine learning, pages 4295–4304. PMLR.
Richerson, P., Baldini, R., Bell, A. V., Demps, K.,
Frost, K., Hillis, V., Mathew, S., Newton, E. K.,
Naar, N., Newson, L., et al. (2016). Cultural group
selection plays an essential role in explaining human
cooperation: A sketch of the evidence. Behavioral
and Brain Sciences, 39:e30.
Roesch, M., Linder, C., Zimmermann, R., Rudolf, A.,
Hohmann, A., and Reinhart, G. (2020). Smart grid
for industry using multi-agent reinforcement learning.
Applied Sciences, 10(19):6900.
Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M.,
Torr, P. H., Foerster, J., and Whiteson, S. (2019).
The starcraft multi-agent challenge. arXiv preprint
arXiv:1902.04043.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
and Klimov, O. (2017). Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347.
Seraj, E., Wang, Z., Paleja, R., Patel, A., and Gombolay, M. (2022). Learning efficient diverse communication for cooperative heterogeneous teaming.
Technical report, Sandia National Lab.(SNL-NM),
Albuquerque, NM (United States).
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
(2016). Safe, multi-agent, reinforcement learning for autonomous driving.
arXiv preprint
arXiv:1610.03295.
Shapley, L. S. (1953). A value for n-person games.
Contribution to the Theory of Games, 2.
Snedecor, G. W. and Cochran, W. G. (1980). Statistical methods. iowa. Iowa State University Press.
Starkstein, SE, & Robinson, RG (1989). Affective
disorders and cerebral vascular disease. The British
Journal of Psychiatry, 154:170–182.
Son, K., Kim, D., Kang, W. J., Hostallero, D. E.,
and Yi, Y. (2019). Qtran: Learning to factorize
with transformation for cooperative multi-agent reinforcement learning. In International conference on
machine learning, pages 5887–5896. PMLR.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M.,
Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat,
N., Leibo, J. Z., Tuyls, K., et al. (2017). Valuedecomposition networks for cooperative multi-agent
learning. arXiv preprint arXiv:1706.05296.
Sutton, R. S., McAllester, D., Singh, S., and Mansour,
Y. (1999). Policy gradient methods for reinforcement
learning with function approximation. Advances in
neural information processing systems, 12.

Terry, J., Black, B., Grammel, N., Jayakumar, M.,
Hari, A., Sullivan, R., Santos, L. S., Dieffendahl, C.,
Horsch, C., Perez-Vicente, R., et al. (2021). Pettingzoo: Gym for multi-agent reinforcement learning.
Advances in Neural Information Processing Systems,
34:15032–15043.
Tumer, K. and Agogino, A. (2007). Distributed agentbased air traffic flow management. In Proceedings of
the 6th international joint conference on Autonomous
agents and multiagent systems, pages 1–8.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin,
I. (2017). Attention is all you need. Advances in
neural information processing systems, 30.
Wang, J., Ren, Z., Liu, T., Yu, Y., and Zhang, C.
(2020a). Qplex: Duplex dueling multi-agent qlearning. arXiv preprint arXiv:2008.01062.
Wang, J., Zhang, Y., Gu, Y., and Kim, T.-K. (2022).
Shaq: Incorporating shapley value theory into multiagent q-learning. Advances in Neural Information
Processing Systems, 35:5941–5954.
Wang, Y., Han, B., Wang, T., Dong, H., and Zhang, C.
(2020b). Off-policy multi-agent decomposed policy
gradients. arXiv preprint arXiv:2007.12322.
Weaver, L. and Tao, N. (2013). The optimal reward
baseline for gradient-based reinforcement learning.
arXiv preprint arXiv:1301.2315.
Wei, E., Wicke, D., Freelan, D., and Luke, S.
(2018). Multiagent soft q-learning. arXiv preprint
arXiv:1804.09817.
Wen, M., Kuba, J. G., Lin, R., Zhang, W., Wen, Y.,
Wang, J., and Yang, Y. (2022). Multi-agent reinforcement learning is a sequence modeling problem.
arXiv preprint arXiv:2205.14953.
Wierstra, D. and Schmidhuber, J. (2007). Policy gradient critics. In European Conference on Machine
Learning, pages 466–477. Springer.
Wolpert, D. H. and Tumer, K. (2001). Optimal payoff
functions for members of collectives. Advances in
Complex Systems, 4(02n03):265–279.
Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and
Wu, Y. (2021). The surprising effectiveness of ppo
in cooperative, multi-agent games. arXiv preprint
arXiv:2103.01955.
Zhang, F., Liu, B., Wang, K., Tan, V., Yang, Z., and
Wang, Z. (2022). Relational reasoning via set transformers: Provable efficiency and applications to marl.
Advances in Neural Information Processing Systems,
35:35825–35838.
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
(2018). Fully decentralized multi-agent reinforcement

Multi-level Advantage Credit Assignment

learning with networked agents. In International
Conference on Machine Learning, pages 5872–5881.
PMLR.
Zhong, Y., Kuba, J. G., Hu, S., Ji, J., and Yang, Y.
(2023). Heterogeneous-agent reinforcement learning.
arXiv preprint arXiv:2304.09870.
Zhou, M., Liu, Z., Sui, P., Li, Y., and Chung, Y. Y.
(2020). Learning implicit credit assignment for cooperative multi-agent reinforcement learning. Advances
in neural information processing systems, 33:11853–
11864.

Checklist
1. For all models and algorithms presented, check if
you include:
(a) A clear description of the mathematical setting, assumptions, algorithm, and/or model.
[Yes/No/Not Applicable] Yes.
(b) An analysis of the properties and complexity
(time, space, sample size) of any algorithm.
[Yes/No/Not Applicable] Yes.
(c) (Optional) Anonymized source code, with
specification of all dependencies, including
external libraries. [Yes/No/Not Applicable]
Not Applicable.
2. For any theoretical claim, check if you include:
(a) Statements of the full set of assumptions of all
theoretical results. [Yes/No/Not Applicable]
Yes.
(b) Complete proofs of all theoretical results.
[Yes/No/Not Applicable] Yes.
(c) Clear explanations of any assumptions.
[Yes/No/Not Applicable] Yes.
3. For all figures and tables that present empirical
results, check if you include:
(a) The code, data, and instructions needed to reproduce the main experimental results (either
in the supplemental material or as a URL).
[Yes/No/Not Applicable] Yes.
(b) All the training details (e.g., data splits,
hyperparameters, how they were chosen).
[Yes/No/Not Applicable] Yes.
(c) A clear definition of the specific measure or
statistics and error bars (e.g., with respect to
the random seed after running experiments
multiple times). [Yes/No/Not Applicable]
Yes.
(d) A description of the computing infrastructure
used. (e.g., type of GPUs, internal cluster,

or cloud provider). [Yes/No/Not Applicable]
Yes.
4. If you are using existing assets (e.g., code, data,
models) or curating/releasing new assets, check if
you include:
(a) Citations of the creator If your work uses
existing assets. [Yes/No/Not Applicable] Yes.
(b) The license information of the assets, if applicable. [Yes/No/Not Applicable] Yes.
(c) New assets either in the supplemental material or as a URL, if applicable. [Yes/No/Not
Applicable] Yes.
(d) Information about consent from data
providers/curators. [Yes/No/Not Applicable]
Yes.
(e) Discussion of sensible content if applicable,
e.g., personally identifiable information or offensive content. [Yes/No/Not Applicable] Not
Applicable.
5. If you used crowdsourcing or conducted research
with human subjects, check if you include:
(a) The full text of instructions given to participants and screenshots. [Yes/No/Not Applicable] Not Applicable.
(b) Descriptions of potential participant risks,
with links to Institutional Review Board (IRB)
approvals if applicable. [Yes/No/Not Applicable] Not Applicable.
(c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Yes/No/Not Applicable]
Not Applicable.

Xutong Zhao, Yaqi Xie

Multi-level Advantage Credit Assignment
for Cooperative Multi-Agent Reinforcement Learning:
Supplementary Materials

A

Theoretical Results

Lemma A.1. Action-independent baseline functions do not affect the bias of the policy gradient estimate in
expectation, i.e.,
gb = Es∼dπ ,a∼π [bi ∇θi log πi (ai |s)] = 0
for any agent i ∈ N if bi does not depend on ai .
Proof. We assume continuous action space. Proof for the discrete case is analogous.
gb = Es∼dπ ,a∼π [bi ∇θi log πi (ai |s)]
= Es∼dπ ,a−i ∼π−i [bi Eai ∼πi [∇θi log πi (ai |s)]] .
We have
Z
Eai ∼πi [∇θi log πi (ai |s)] =
πi (ai |s) ∇θi log πi (ai |s) dai
Ai
Z
Z
=
∇θi πi (ai |s) dai = ∇θi
πi (ai |s) dai = ∇θi 1 = 0
Ai

Ai

which concludes the proof.
Theorem A.2 (Minimum-variance baseline). Adapted from single-agent results by Chung et al. (2021).
The minimum-variance baseline for the MAPG estimator is


EaGi Q(s, a)||∇θi log πi (ai |s)||2
∗
bi (s, a−Gi ) =
EaGi [||∇θi log πi (ai |s)||2 ]
if the baseline is conditioned on the state s and actions by −Gi , where aGi = {aj ∼ πj : j ∈ Gi }, {i} ⊂ Gi ⊂ N ,
and −Gi = N \ Gi .
Proof. Assuming access to the Q-value for each state-action pair Q(s, a) = Qπ (s, a), the gradient is given in
Equation (1) as ∇θi J(θ) = Es∼dπ ,a∼π [Q(s, a)∇θi log πi (ai |s)] = Es∼dπ ,a∼π [gi ], where the estimator is gi =
(Q(s, a) − bi (s, a−Gi ))∇θi log πi (ai |s). We have




Vara∼π [gi ] = Vara−Gi EaGi [gi ] +
Ea−Gi VaraGi [gi ]
|
{z
}
{z
}
|
variance from given actions

variance from unknown actions

We now derive the baseline that minimizes the second term.


VaraGi (gi ) = EaGi ∥gi ∥2 − ∥EaGi [gi ] ∥2


= EaGi ∥gi ∥2 − ∥EaGi [Q(s, a)∇θi log πi (ai |s)] ∥2

Multi-level Advantage Credit Assignment

The equality holds by proof of Lemma A.1. Then we only need to consider the first item

 ∂

∂
EaGi ∥gi ∥2 = EaGi ∥Q(s, a)∇ log πi (ai |s)∥2 − 2 · Q(s, a)bi (s, a−Gi )∥∇ log πi (ai |s)∥2
∂b
∂b

+bi (s, a−Gi )2 ∥∇ log πi (ai |s)∥2




=2 bi (s, a−Gi ) · EaGi ∥∇ log πi (ai |s)∥2 − EaGi Q(s, a)∥∇ log πi (ai |s)∥2
The minimum variance can be obtained by setting the gradient above to 0, i.e.,


EaGi Q(s, a)||∇θi log πi (ai |s)||2
∗
.
bi (s, a−Gi ) =
EaGi [||∇θi log πi (ai |s)||2 ]

Lemma A.3. Adapted from single-agent results by Chung et al. (2021).
The minimum-variance baseline b∗i from Theorem A.2 and the baseline bi = EaGi [Q(s, a)] satisfies

CovaGi Q(s, a), ||∇θi log πi (ai |s)||2
∗
bi = bi −
.
EaGi [||∇θi log πi (ai |s)||2 ]
Proof. We have


EaGi Q(s, a)||∇θi log πi (ai |s)||2
EaGi [||∇θi log πi (ai |s)||2 ]


EaGi Q(s, a)||∇θi log πi (ai |s)||2
=
− bi + bi
EaGi [||∇θi log πi (ai |s)||2 ]




EaGi Q(s, a)||∇θi log πi (ai |s)||2 − EaGi [Q(s, a)] EaGi ||∇θi log πi (ai |s)||2
+ bi
=
EaGi [||∇θi log πi (ai |s)||2 ]

CovaGi Q(s, a), ||∇θi log πi (ai |s)||2
=
+ bi
EaGi [||∇θi log πi (ai |s)||2 ]

b∗i (s, a−Gi ) =

Lemma A.4. Adapted from Foerster et al. (2018).
For an actor-critic algorithm with a compatible TD(1) critic following a policy gradient
#
"
X
k
g = Es∼dπ ,a∼π
∇θk log πi (ai |s) (Q(s, a) − bi )
i

at each iteration k, where the baseline function bi does not depend on action ai ,
lim inf ||∇J|| = 0
k

w.p. 1.

Proof. By Lemma A.1, the per-agent baseline does not change the expected gradient, thereby not affecting the
convergence. Hence we have the policy gradient
"
#
X
g = Es∼dπ ,a∼π
∇θ log πi (ai |s) (Q(s, a) − bi )
i

= Es∼dπ ,a∼π

"
X

#
∇θ log πi (ai |s)Q(s, a)

i

"
= Es∼dπ ,a∼π ∇θ log

#
Y

πi (ai |s)Q(s, a)

i

= Es∼dπ ,a∼π [∇θ log π(a|s)Q(s, a)]

Xutong Zhao, Yaqi Xie

SMACv2

SMAC

Table 4: SMAC v1&v2 task scenarios with unit and task types.

Task

Ally Unit Types

Task Type

25m
5m_vs_6m
8m_vs_9m
10m_vs_11m
3s5z

Marine
Marine
Marine
Marine
Stalker,Zealot

homogeneous
homogeneous
homogeneous
homogeneous
heterogeneous

protoss_5_vs_5
terran_5_vs_5
zerg_5_vs_5
protoss_10_vs_10
terran_10_vs_10
zerg_10_vs_10

Stalker, Zealot, Colossus
Marine, Marauder, Medivac
Zergling, Hydralisk, Baneling
Stalker, Zealot, Colossus
Marine, Marauder, Medivac
Zergling, Hydralisk, Baneling

heterogeneous
heterogeneous
heterogeneous
heterogeneous
heterogeneous
heterogeneous

yielding the standard single-agent actor-critic policy gradient.
Konda and Tsitsiklis (1999) prove that an actor-critic following this gradient converges to a local maximum of the
expected return J(θ), given that:
1. the policy π is differentiable,
2. the update timescales for Q and π are sufficiently slow, and that π is updated sufficiently slower than Q, and
3. Q uses a representation compatible with π,
amongst several further assumptions. The policy parameterization (i.e., the joint-action agent is decomposed into
independent actors) is immaterial to convergence, as long as it remains differentiable. Note that a centralized
critic with access to the global state is essential for this proof to hold.

B

Experiment Details

B.1

Experimental Setup

SMACv1 contains a diverse set of battle scenarios in which a team of ally units aims to defeat the enemy
team. SMACv2 extends SMACv1 with procedurally generated scenarios, with team compositions and positions
spawned according to different probabilities, which introduces significantly more stochasticity that requires
agents’ generalization to unseen settings. We follow the default reward setting, which returns positive rewards for
damage dealt to the enemy, killing each enemy unit, and winning the battle. Tasks in both benchmarks embody
complicated multi-agent credit assignment challenges, due to diverse cooperative behaviours involved in defeating
the opponent. Table 4 provides an overview of SMAC v1&v2 task scenarios.
B.2

Computational Requirements

It is important to highlight that MACA utilizes the attention encoder module of the transformer, rather than
the entire transformer architecture. Besides self-attention’s internal computations (i.e., QKV projection, convex
combination), other operations within MACA’s critic, including policy distribution computation, value estimation,
and weighted sum, are all linear operations. MACA hence enjoys reasonable time and space complexity compared
to other baseline methods. In practice, MACA has memory and runtime on the same scale as other methods
despite its transformer module, as shown in Table 6. Moreover, MACA’s attention-based critic only affects
parameter updates, while the majority of time consumed lies in agent-environment interactions, where only
the actor networks are involved. As modern deep learning frameworks optimize the efficiency of attention (e.g.
FlashAttention (Dao et al., 2022)), the use of attention would not bottleneck MACA’s scalability.

Multi-level Advantage Credit Assignment

We conduct all our experiments entirely on CPUs in compute clusters with multiple nodes. All experiments were
run on a single CPU core. The main CPU model types are AMD Rome 7532 @ 2.40 GHz 256M cache L3 and
AMD Rome 7502 @ 2.50 GHz 128M cache L3. Each SMACv1/SMACv2/MPE job respectively took 24/48/12
CPU hours. The total number of CPU hours spent (including hyperparameter search) is 33,708.
B.3

Additional Results and Discussions

MPE

Table 5: Mean evaluation episodic returns and standard deviation for different methods on MPE tasks.
Task

MACA

MAPPO

IPPO

PPO-Mix

PPO-Sum

COMA

HAPPO

Steps

Spread
Reference
Speaker Listener

−61.95(1.44)
−12.04(0.45)
−9.17(0.53)

−67.96(4.22)
−12.19(0.43)
−9.10(0.53)

−67.01(2.40)
−29.63(1.16)
−9.41(0.58)

−106.56(0.92)
−27.84(2.64)
−21.26(2.52)

−84.29(1.14)
−34.40(0.79)
−26.06(1.55)

−109.09(3.61)
−39.71(1.76)
−31.80(2.94)

−72.25(2.05)
−12.44(0.43)
−9.06(0.51)

8e6
8e6
8e6

Spread

Reference

−50

−120
−140
−160
−180

−40

Episodic return

Episodic return

−100

Episodic return

−25

−20

−80

Speaker Listener

0

−60

−60

−80

−75
−100
−125
−150
−175

−200
−100

−200

−220
0

1

2

3

4

5

6

7

8

0

1

2

3

4

1e6

Steps
MACA

5

6

7

8

MAPPO

IPPO

0

1

2

3

4

1e6

Steps
PPO-Mix

PPO-Sum

5

6

7

COMA

8

1e6

Steps
HAPPO

Figure 4: Performance on the MPE benchmark.
5m_vs_6m

8m_vs_9m

10m_vs_11m

3s5z

25m

100%

100%

100%

100%

80%

80%

80%

80%

40%

60%
40%

60%
40%

20%

20%

20%

20%

0%

0%

0%

0%

0.0

0.2

0.4

Steps

0.6

0.8

0.0

0.2

1e7

0.4

0.6

MACA

0.8

0.0

1e7

Steps
MAPPO

0.2

0.4

0.6

Steps
IPPO

PPO-Mix

0.8

Win rate (%)

40%

60%

Win rate (%)

60%

Win rate (%)

Win rate (%)

Win rate (%)

80%

40%
20%
0%

0.0

0.2

1e7
PPO-Sum

60%

0.4

0.6

COMA

0.8

1e7

Steps

0

1

2

Steps

3

1e6

HAPPO

Figure 5: Performance on the SMAC benchmark.
On MPE tasks, multiple baseline methods exhibit strong performance. Despite continuous action space, MPE tasks
are easy because they involve similar cooperation types and simple state-action spaces, leading to limited evaluation
challenges. MACA marginally outperforms other methods unlike in the more challenging SMAC environments.
Table 6: Number of parameters in the critic model and experiment runtime. All values are reported in the case of
5m_vs_6m. MACA has memory and runtime requirements on the same scale as other methods.
Algorithm

Number of parameters

Runtime (hours)

MAPPO
IPPO
COMA
HAPPO
PPO-Mix
PPO-Sum
MACA

165,747
149,497
174,966
165,747
361,349
134,148
239,674

15.1
14.8
18.6
15.9
18.5
17.4
16.7

1

Xutong Zhao, Yaqi Xie

0.353 0.146 0.141 0.216 0.143
0.32

3

2

1

0.164 0.140 0.346 0.160 0.190

4

4

0.168 0.358 0.154 0.174 0.146

0.220 0.155 0.153 0.331 0.141

5

2

3

0.166 0.149 0.181 0.156 0.347

5

1

2

3

4

0.28
0.24
0.20
0.16

5

Figure 6: Example visualized ally agents’ coordinates (left) and corresponding attention weight (right) in
5m_vs_6m. The attention matrix shows that agents 1 and 4 have high scores, which aligns with the task map
where these agents are allied together.
B.4

Visualization

We present an exemplary visualization of ally coordinates and corresponding attention weights in Figure 6, which
shows that the attention map reflects meaningful inter-agent correlations.
B.5

Ablation Details

Figure 7: MACA critic architecture with transformer decoder.
MACA-Dec replaces the linear layer with a transformer decoder to perform value estimation. The architecture is
illustrated in Figure 7.

C

Hyperparameters

Multi-level Advantage Credit Assignment

Table 7: Common hyperparameters across tasks.
Hyperparameter

Value

state type
lr
actor hidden sizes
critic hidden sizes
actor num mini batch
critic num mini batch
gamma
entropy coef
use valuenorm
use linear lr decay
use proper time limits
activation
use feature normalization
initialization method
gain
use naive recurrent policy
use recurrent policy
num GRU layers
data chunk length
optim eps
weight decay
std x coef
std y coef
use clipped value loss
value loss coef
use max grad norm
max grad norm
use GAE
GAE lambda
use huber loss
use policy active masks
huber delta
action aggregation
share param

EP
0.0005
[64,64,64]
[128,128,128]
1
1
0.99
0.01
True
False
True
ReLU
True
orthogonal
0.01
False
True
1
10
1e-5
0
1
0.5
True
1
True
10.0
True
0.95
True
True
10.0
prod
True

Table 8: Hyperparameters for MAPPO.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.1
0.1
0.1

Table 9: Hyperparameters for IPPO.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.1
0.1
0.1

Xutong Zhao, Yaqi Xie

Table 10: Common hyperparameters for MACA across tasks.
Hyperparameter

Value

critic hidden sizes
n encode layer
n head
n embd
zs dim
bias
active fn
weight decay
betas
weight init
warmup epochs
v value loss coef
q value loss coef
att sigma σ

[64,64,64]
1
1
64
256
True
gelu
0.01
[0.9, 0.95]
TFixup
10
1.0
0.5
1.0/n

Table 11: Hyperparameters for MACA.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.05
0.1
0.1

Table 12: Hyperparameters for PPO-Mix.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.05
0.05
0.05

Table 13: Hyperparameters for PPO-Sum.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.05
0.05
0.05

Table 14: Hyperparameters for COMA.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

5
5
5

0.05
0.05
0.05

Table 15: Hyperparameters for HAPPO.
Task

ppo/critic epoch

clip param

SMAC
SMACv2
MPE

10
10
10

0.1
0.1
0.1

Multi-level Advantage Credit Assignment

Table 16: Hyperparameter sweep for MACA. Results demonstrate MACA’s robustness across ppo/critic epoch
and clip hyperparameter settings in multiple representative environments.
epoch/clip

25m

5m_vs_6m

protoss_5_vs_5

protoss_10_vs_10

10/0.05
10/0.075
10/0.1
10/0.125
5/0.05
5/0.1

99.3 (0.1)
99.3 (0.1)
99.5 (0.1)
99.1 (0.0)
99.3 (0.3)
99.1 (0.1)

87.0 (2.0)
85.5 (2.8)
82.5 (4.1)
80.6 (3.7)
83.4 (2.3)
80.2 (2.5)

76.1 (3.5)
77.5 (4.8)
79.0 (3.4)
78.2 (2.7)
70.6 (6.4)
76.3 (5.6)

70.1 (2.6)
72.4 (4.1)
75.8 (3.9)
74.6 (5.2)
67.9 (6.0)
71.5 (7.6)

Table 17: Hyperparameter sweep for MACA. Results demonstrate MACA’s robustness across thresholding
hyperparameter settings in multiple representative environments.
threshold σ

0.9/n

0.95/n

1.0/n

1.05/n

1.1/n

5m_vs_6m
protoss_10_vs_10

86.5 (1.5)
74.2 (4.4)

87.4 (1.2)
75.3 (4.6)

87.0 (2.0)
75.8 (3.9)

86.6 (0.9)
74.7 (3.6)

85.7 (3.3)
74.5 (5.0)

