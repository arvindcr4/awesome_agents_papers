The Option Keyboard
Combining Skills in Reinforcement Learning

arXiv:2106.13105v1 [cs.AI] 24 Jun 2021

Andr√© Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Ayg√ºn,
Philippe Hamel, Daniel Toyama, Jonathan Hunt, Shibl Mourad, David Silver, Doina Precup
{andrebarreto,borsa,shaobohou,gcomanici,eser}@google.com
{hamelphi,kenjitoyama,jjhunt,shibl,davidsilver,doinap}@google.com

DeepMind

Abstract
The ability to combine known skills to create new ones may be crucial in the
solution of complex reinforcement learning problems that unfold over extended
periods. We argue that a robust way of combining skills is to define and manipulate
them in the space of pseudo-rewards (or ‚Äúcumulants‚Äù). Based on this premise, we
propose a framework for combining skills using the formalism of options. We show
that every deterministic option can be unambiguously represented as a cumulant
defined in an extended domain. Building on this insight and on previous results
on transfer learning, we show how to approximate options whose cumulants are
linear combinations of the cumulants of known options. This means that, once we
have learned options associated with a set of cumulants, we can instantaneously
synthesise options induced by any linear combination of them, without any learning
involved. We describe how this framework provides a hierarchical interface to the
environment whose abstract actions correspond to combinations of basic skills.
We demonstrate the practical benefits of our approach in a resource management
problem and a navigation task involving a quadrupedal simulated robot.

1

Introduction

In reinforcement learning (RL) an agent takes actions in an environment in order to maximise the
amount of reward received in the long run [25]. This textbook definition of RL treats actions as
atomic decisions made by the agent at every time step. Recently, Sutton [23] proposed a new view
on action selection. In order to illustrate the potential benefits of his proposal Sutton resorts to the
following analogy. Imagine that the interface between agent and environment is a piano keyboard,
with each key corresponding to a possible action. Conventionally the agent plays one key at a time
and each note lasts exactly one unit of time. If we expect our agents to do something akin to playing
music, we must generalise this interface in two ways. First, we ought to allow notes to be arbitrarily
long‚Äîthat is, we must replace actions with skills. Second, we should be able to also play chords.
The argument in favour of temporally-extended courses of actions has repeatedly been made in the
literature: in fact, the notion that agents should be able to reason at multiple temporal scales is one of
the pillars of hierarchical RL [7, 18, 26, 8, 17]. The insight that the agent should have the ability to
combine the resulting skills is a far less explored idea. This is the focus of the current work.
The possibility of combining skills replaces a monolithic action set with a combinatorial counterpart:
by learning a small set of basic skills (‚Äúkeys‚Äù) the agent should be able to perform a potentially very
large number of combined skills (‚Äúchords‚Äù). For example, an agent that can both walk and grasp an
object should be able to walk while grasping an object without having to learn a new skill. According
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

to Sutton [23], this combinatorial action selection process ‚Äúcould be the key to generating behaviour
with a good mix of preplanned coherence and sensitivity to the current situation.‚Äù
But how exactly should one combine skills? One possibility is to combine them in the space of
policies: for example, if we look at policies as distribution over actions, a combination of skills can
be defined as a mixture of the corresponding distributions. One can also combine parametric policies
by manipulating the corresponding parameters. Although these are feasible solutions, they fail to
capture possible intentions behind the skills. Suppose the agent is able to perform two skills that can
be associated with the same objective‚Äîdistinct ways of grasping an object, say. It is not difficult
to see how combinations of the corresponding behaviours can completely fail to accomplish the
common goal. We argue that a more robust way of combining skills is to do so directly in the goal
space, using pseudo-rewards or cumulants [25]. If we associate each skill with a cumulant, we can
combine the former by manipulating the latter. This allows us to go beyond the direct prescription of
behaviours, working instead in the space of intentions.
Combining skills in the space of cumulants poses two challenges. First, we must establish a welldefined mapping between cumulants and skills. Second, once a combined cumulant is defined, we
must be able to perform the associated skill without having to go through the slow process of learning
it. We propose to tackle the former by adopting options as our formalism to define skills [26]. We
show that there is a large subset of the space of options, composed of deterministic options, in which
every element can be unambiguously represented as a cumulant defined in an extended domain.
Building on this insight, we extend Barreto et al.‚Äôs [3, 4] previous results on transfer learning to
show how to approximate options whose cumulants are linear combinations of the cumulants of
known options. This means that, once the agent has learned options associated with a collection
of cumulants, it can instantaneously synthesise options induced by any linear combination of them,
without any learning involved. Thus, by learning a small set of options, the agent instantaneously
has at its disposal a potentially enormous number of combined options. Since we are combining
cumulants, and not policies, the resulting options will be truly novel, meaning that they cannot, in
general, be directly implemented as a simple alternation of their constituents.
We describe how our framework provides a flexible interface with the environment whose abstract
actions correspond to combinations of basic skills. As a reference to the motivating analogy described
above, we call this interface the option keyboard. We discuss the merits of the option keyboard at the
conceptual level and demonstrate its practical benefits in two experiments: a resource management
problem and a realistic navigation task involving a quadrupedal robot simulated in MuJoCo [30, 21].

2

Background

As usual, we assume the interaction between agent and environment can be modelled as a Markov
decision process (MDP) [19]. An MDP is a tuple M ‚â° (S, A, p, r, Œ≥), where S and A are the state and
action spaces, p(¬∑|s, a) gives the next-state distribution upon taking action a in s, r : S √ó A √ó S 7‚Üí R
a
specifies the reward associated with the transition s ‚àí
‚Üí s0 , and Œ≥ ‚àà [0, 1) is the discount factor.
The objective
of the agent is to find a policy œÄ : S 7‚Üí A that maximises the expected return
P‚àû
Gt ‚â° i=0 Œ≥ i Rt+i , where Rt = r(St , At , St+1 ). A principled way to address this problem is to
use methods derived from dynamic programming, which usually compute the action-value function
of a policy œÄ as: QœÄ (s, a) ‚â° EœÄ [Gt |St = s, At = a] , where EœÄ [¬∑] denotes expectation over the
transitions induced by œÄ [19]. The computation of QœÄ (s, a) is called policy evaluation. Once œÄ has
been evaluated, we can compute a greedy policy
œÄ 0 (s) ‚àà argmaxa QœÄ (s, a) for all s ‚àà S.

(1)

0

It can be shown that QœÄ (s, a) ‚â• QœÄ (s, a) for all (s, a) ‚àà S √ó A, and hence the computation of œÄ 0 is
referred to as policy improvement. The alternation between policy evaluation and policy improvement
is at the core of many RL algorithms, which usually carry out these steps approximately. Here we will
use a tilde over a symbol to indicate that the associated quantity is an approximation (e.g., QÃÉœÄ ‚âà QœÄ ).
2.1

Generalising policy evaluation and policy improvement

Following Sutton and Barto [25], we call any signal defined as c : S √ó A √ó S 7‚Üí R a cumulant.
Analogously to the conventional value function QœÄ , we define QœÄc as the expected discounted sum of
2

cumulant c under policy œÄ [27]. Given a policy œÄ and a set of cumulants C, we call the evaluation of
œÄ under all c ‚àà C generalised policy evaluation (GPE) [2]. Barreto et al. [3, 4] propose an efficient
formP
of GPE based on successor features: they show that, given cumulants c1 , c2 , ..., cd , for any
c = i wi ci , with w ‚àà Rd ,
#
"‚àû
d
d
X X
X
œÄ
œÄ
k
Qc (s, a) ‚â° E
Œ≥
wi Ci,t+k |St = s, At = a =
wi QœÄci (s, a),
(2)
k=0

i=1

i=1

where Ci,t ‚â° ci (St , At , Rt ). Thus, once we have computed QœÄc1 , QœÄc1 , ..., QœÄcd , we can instantaneously
P
evaluate œÄ under any cumulant in the set C ‚â° {c = i wi ci | w ‚àà Rd }.
Policy improvement can also be generalised. In Barreto et al.‚Äôs [3] generalised policy improvement
(GPI) the improved policy is computed based on a set of value functions. Let QœÄc 1 , QœÄc 2 , ...QœÄc n be
the action-value functions of n policies œÄi under cumulant c, and let Qmax
(s, a) = maxi QœÄc i (s, a)
c
for all (s, a) ‚àà S √ó A. If we define
œÄ(s) ‚àà argmaxa Qmax
(s, a) for all s ‚àà S,
c

(3)

then QœÄc (s, a) ‚â• Qmax
(s, a) for all (s, a) ‚àà S √ó A. This is a strict generalisation of standard policy
c
improvement (1). The guarantee extends to the case in which GPI uses approximations QÃÉœÄc i [3].
2.2

Temporal abstraction via options

As discussed in the introduction, one way to get temporal abstraction is through the concept of
options [26]. Options are temporally-extended courses of actions. In their more general formulation,
options can depend on the entire history between the time t when they were initiated and the current
time step t + k, ht:t+k ‚â° st at st+1 ...at+k‚àí1 st+k . Let H be the space of all possible histories; a
semi-Markov option is a tuple o ‚â° (Io , œÄo , Œ≤o ) where Io ‚äÇ S is the set of states where the option can
be initiated, œÄo : H 7‚Üí A is a policy over histories, and Œ≤o : H 7‚Üí [0, 1] gives the probability that the
option terminates after history h has been observed [26]. It is worth emphasising that semi-Markov
options depend on the history since their initiation, but not before.

3

Combining options

In the previous section we discussed how several key concepts in RL can be generalised: rewards
with cumulants, policy evaluation with GPE, policy improvement with GPI, and actions with options.
In this section we discuss how these concepts can be used to combine skills.
3.1

The relation between options and cumulants

We start by showing that there is a subset of the space of options in which every option can be
unequivocally represented as a cumulant defined in an extended domain.
First we look at the relation between policies and cumulants. Given an MDP (S, A, p, ¬∑, Œ≥), we say
that a cumulant cœÄ : S √ó A √ó S 7‚Üí R induces a policy œÄ : S 7‚Üí A if œÄ is optimal for the MDP
(S, A, p, cœÄ , Œ≥). We can always define a cumulant cœÄ that induces a given policy œÄ. For instance, if
we make

0 if a = œÄ(s);
cœÄ (s, a, ¬∑) =
(4)
z otherwise,
where z < 0, it is clear that œÄ is the only policy that achieves the maximum possible value QœÄ (s, a) =
Q‚àó (s, a) = 0 on all (s, a) ‚àà S √ó A. In general, the relation between policies and cumulants is a
many-to-many mapping. First, there is more than one cumulant that induces the same policy: for
example, any z < 0 in (4) will clearly lead to the same policy œÄ. There is thus an infinite set of
cumulants CœÄ associated with œÄ. Conversely, although this is not the case in (4), the same cumulant
can give rise to multiple policies if more than one action achieves the maximum in (1).
Given the above, we can use any cumulant cœÄ ‚àà CœÄ to refer to policy œÄ. In order to extend this
possibility to options o = (Io , œÄo , Œ≤o ) we need two things. First, we must define cumulants in the
space of histories H. This will allow us to induce semi-Markov policies œÄo : H 7‚Üí A in a way that is
analogous to (4). Second, we need cumulants that also induce the initiation set Io and the termination
function Œ≤o . We propose to accomplish this by augmenting the action space.
3

Let œÑ be a termination action that terminates option o much like the termination function Œ≤o . We can
think of œÑ as a fictitious action and model it by defining an augmented action space A+ ‚â° A ‚à™ {œÑ }.
When the agent is executing an option o, selecting action œÑ immediately terminates it. We now
show that if we extend the definition of cumulants to also include œÑ we can have the resulting
cumulant induce not only the option‚Äôs policy but also its initiation set and termination function. Let
e : H √ó A+ √ó S 7‚Üí R be an extended cumulant. Since e is defined over the augmented action space,
for each h ‚àà H we now have a termination bonus e(h, œÑ, s) = e(h, œÑ ) that determines the value of
interrupting option o after having observed h. The extended cumulant e induces an augmented policy
œâe : H 7‚Üí A+ in the same sense that a standard cumulant induces a policy (that is, œâe is an optimal
policy for the derived MDP whose state space is H and the action space is A+ ; see Appendix A for
details). We argue that œâe is equivalent to an option oe ‚â° (Ie , œÄe , Œ≤e ) whose components are defined
as follows. The policy œÄe : H 7‚Üí A coincides with œâe whenever the latter selects an action in A. The
termination function is given by

e
1 if e(h, œÑ ) > maxa6=œÑ Qœâ
e (h, a),
(5)
Œ≤e (h) =
0 otherwise.
In words, the agent will terminate after h if the instantaneous termination bonus e(h, œÑ ) is larger than
the maximum expected discounted sum of cumulant e under policy œâe . Note that when h is a single
state s, no concrete action has been executed by the option yet, hence it terminates with œÑ immediately
after its initiation. This is precisely the definition of the initialisation set Ie ‚â° {s | Œ≤e (s) = 0}.
Termination functions like (5) are always deterministic. This means that extended cumulants e can
only represent options oe in which Œ≤e is a mapping H 7‚Üí {0, 1}. In fact, it is possible to show that
all options of this type, which we will call deterministic options, are representable as an extended
cumulant e, as formalised in the following proposition (proof in Appendix A):
Proposition 1. Every extended cumulant induces at least one deterministic option, and every deterministic option can be unambiguously induced by an infinite number of extended cumulants.
3.2

Synthesising options using GPE and GPI

In the previous section we looked at the relation between extended cumulants and deterministic
options; we now build on this connection to use GPE and GPI to combine options.
Let E ‚â° {e1 , e2 , ..., ed } be a set of extended cumulants. We know that ei : H √ó A+ √ó S 7‚Üí R is
associated with deterministic option oei ‚â° œâei . As with any other cumulant, P
the extended cumulants
ei can be linearly combined; it then follows that, for any w ‚àà Rd , e = i wi ei defines a new
deterministic option oe ‚â° œâe . Interestingly,
P the termination function of oe has the form (5) with
termination bonuses defined as e(h, œÑ ) = i wi ei (h, œÑ ). This means that the combined option oe
‚Äúinherits‚Äù its termination function from its constituents oei . Since any w ‚àà Rd defines an option oe ,
the set E can give rise to a very large number of combined options.
The problem is of course that for each w ‚àà Rd we have to actually compute the resulting option œâe .
This is where GPE and GPI come to the rescue. Suppose we have the values of options œâei under all
the cumulants e1 , e2 , ..., ed . With this information, and analogously to (2), we can use the fast form
of GPE provided by successor features to compute the value of œâej with respect to e:
X
œâe
œâe
Qe j (h, a) =
wi Qei j (h, a).
(6)
i

Now that we have all the options œâej evaluated under e, we can merge them to generate a new option
that does at least as well as, and in general better than, all of them. This is done by applying GPI over
œâe
the value functions Qe j :
œâe

œâÃÉe (h) ‚àà argmaxa‚ààA+ maxj Qe j (h, a).

(7)

œâe
œâe
e
From previous theoretical results we know that maxj Qe j (h, a) ‚â§ QœâÃÉ
e (h, a) ‚â§ Qe (h, a) for all
+
(h, a) ‚àà H √ó A [3]. In words, this means that, even though the GPI option œâÃÉe is not necessarily

optimal, following it will in general result in a higher return in terms of cumulant e than if the agent
were to execute any of the known options œâej . Thus, we can use œâÃÉe as an approximation to œâe that
requires no additional learning. It is worth mentioning that the action selected by the combined
option in (7), œâÃÉe (h), can be different from œâei (h) for all i‚Äîthat is, the resulting policy cannot, in
4

general, be implemented as an alternation of its constituents. This highlights the fact that combining
cumulants is not the same as defining a higher-level policy over the associated options.
In summary, given a set of cumulants
E, we can combine them by picking weights w and computing
P
the resulting cumulant e =
i wi ei . This can be interpreted as determining how desirable or
undesirable each cumulant is. Going back to the example in the introduction, suppose that e1 is
associated with walking and e2 is associated with grasping an object. Then, cumulant e1 + e2 will
reinforce both behaviours, and will be particularly rewarding when they are executed together. In
contrast, cumulant e1 ‚àí e2 will induce an option that avoids grasping objects, favouring the walking
behaviour in isolation and even possibly inhibiting it. Since the resulting option aims at maximising a
combination of the cumulants ei , it can itself be seen as a combination of the options oei .

4

Learning with combined options

Given a set of extended cumulants E, in
order to be able to combine the associated options using GPE and GPI one
only needs the value functions QE ‚â°
œâe
{QÃÉej i | ‚àÄ(i, j) ‚àà {1, 2, ..., d}2 }. The set
QE can be constructed using standard RL
methods; for an illustration of how to do it
with Q-learning see Algorithm 3 in App. B.

player

w
r ‚Ä≤ , s‚Ä≤ , Œ≥ ‚Ä≤
agent

a
OK

environment

r, s
temporal abstraction

Figure 1: OK mediates the interaction between player
As discussed, once QE has been computed and environment. The exchange of information between
we can use GPE and GPI to synthesise op- OK and the environment happens at every time step.
tions on the fly. In this case the newly- The interaction between player and OK only happens
generated options are fully determined by ‚Äúinside‚Äù the agent when the termination action œÑ is sethe vector of weights w ‚àà Rd . Concep- lected by GPE and GPI (see Algorithms 1 and 2).
tually, we can think of this process as an
interface between an RL algorithm and the environment: the algorithm selects a vector w, hands it
over to GPE and GPI, and ‚Äúwaits‚Äù until the action returned by (7) is the termination action œÑ . Once œÑ
has been selected, the algorithm picks a new w, and so on. The RL method is thus interacting with
the environment at a higher level of abstraction in which actions are combined skills defined by the
vectors of weights w. Returning to the analogy with a piano keyboard described in the introduction,
we can think of each option œâei as a ‚Äúkey‚Äù that can be activated by an instantiation of w whose only
non-zero entry is wi > 0. Combined options associated with more general instantiations of w would
correspond to ‚Äúchords‚Äù. We will thus call the layer of temporal abstraction between algorithm and
environment the option keyboard (OK). We will also generically refer to the RL method interacting
with OK as the ‚Äúplayer‚Äù. Figure 1 shows how an RL agent can be broken into a player and an OK.
Algorithm 1 shows a generic implementation of OK. Given a set of value functions QE and a vector
of weights w, OK will execute the actions selected by GPE and GPI until the termination action is
picked or a terminal state is reached. During this process OK keeps track of the discounted reward
accumulated in the interaction with the environment (line 6), which will be returned to the player
when the interaction terminates (line 10). As the options œâei may depend on the entire trajectory
since their initiation, OK uses an update function u(h, a, s0 ) that retains the parts of the history that
are actually relevant for decision making (line 8). For example, if OK is based on Markov options
only, one can simply use the update function u(h, a, s0 ) = s0 .
The set QE defines a specific instantiation of OK; once an OK is in place any conventional RL
method can interact with it as if it were the environment. As an illustration, Algorithm 2 shows
how a keyboard player that uses a finite set of combined options W ‚â° {w1 , w2 , ..., wn } can be
implemented using standard Q-learning by simply replacing the environment with OK. It is worth
pointing out that if we substitute any other set of weight vectors W 0 for W we can still use the same
OK, without the need to relearn the value functions in QE . We can even use sets of abstract actions
W that are infinite‚Äîas long as the OK player can deal with continuous action spaces [33, 24, 22].
Although the clear separation between OK and its player is instructive, in practice the boundary
between the two may be more blurry. For example, if the player is allowed to intervene in all interactions between OK and environment, one can implement useful strategies like option interruption [26].
Finally, note that although we have been treating the construction of OK (Algorithm 3) and its use
5

(Algorithms 1 and 2) as events that do not overlap in time, nothing keeps us from carrying out the
two procedures in parallel, like in similar methods in the literature [1, 32].
Algorithm 2 Q-learning keyboard player
Ô£±
OK
option keyboard
Ô£¥
Ô£≤
W
combined options
Require:
value functions
Ô£¥
Ô£≥ QE
Œ±, , Œ≥ ‚àà R hyper-parameters
1: create QÃÉ(s, w) parametrised by Œ∏ Q
2: select initial state s ‚àà S
3: repeat forever
4:
if Bernoulli()=1 then w ‚Üê Uniform(W)
5:
else w ‚Üê argmaxw0 ‚ààW QÃÉ(s, w0 )
6:
(s0 , r0 , Œ≥ 0 ) ‚Üê OK(s, w, QE , Œ≥)
7:
Œ¥ ‚Üê r0 + Œ≥ 0 maxw0 QÃÉ(s0 , w0 ) ‚àí QÃÉ(s, w)
8:
Œ∏ Q ‚Üê Œ∏ Q + Œ±Œ¥‚àáŒ∏Q QÃÉ(s, w) // update QÃÉ
9:
if s0 is terminal then select initial s ‚àà S
10:
else s ‚Üê s0

Algorithm 1 Option Keyboard (OK)
Ô£±
s‚ààS
current state
Ô£¥
Ô£≤
w ‚àà Rd
vector of weights
Require:
Q
value functions
Ô£¥
E
Ô£≥
Œ≥ ‚àà [0, 1) discount rate
1: h ‚Üê s; r 0 ‚Üê 0; Œ≥ 0 ‚Üê 1
2: repeat
P
œâe

3:
a ‚Üê argmaxa0 maxi [ j wj QÃÉej i (h, a0 )]
4:
if a 6= œÑ then
5:
execute action a and observe r and s0
6:
r0 ‚Üê r0 + Œ≥ 0 r
7:
if s0 is terminal Œ≥ 0 ‚Üê 0 else Œ≥ 0 ‚Üê Œ≥ 0 Œ≥
8:
h ‚Üê u(h, a, s0 )
9: until a = œÑ or s0 is terminal
10: return s0 , r 0 , Œ≥ 0

5

Experiments

We now present our experimental results illustrating the benefits of OK in practice. Additional details,
along with further results and analysis, can be found in Appendix C.
5.1

Foraging world

The goal in the foraging world is to manage a set of resources by navigating in a grid world and
picking up items containing the resources in different proportions. For illustrative purposes we will
consider that the resources are nutrients and the items are food. The agent‚Äôs challenge is to stay
healthy by keeping its nutrients within certain bounds. The agent navigates in the grid world using
the four usual actions: up, down, left, and right. Upon collecting a food item the agent‚Äôs nutrients are
increased according to the type of food ingested. Importantly, the quantity of each nutrient decreases
by a fixed amount at every step, so the desirability of different types of food changes even if no food is
consumed. Observations are images representing the configuration of the grid plus a vector indicating
how much of each nutrient the agent currently has (see Appendix C.1 for a technical description).
What makes the foraging world particularly challenging is the fact that the agent has to travel towards
the items to pick them up, adding a spatial aspect to an already complex management problem. The
dual nature of the problem also makes it potentially amenable to be tackled with options, since we
can design skills that seek specific nutrients and then treat the problem as a management task in
which actions are preferences over nutrients. However, the number of options needed can increase
exponentially fast. If at any given moment the agent wants, does not want, or does not care about
each nutrient, we need 3m options to cover the entire space of preferences, where m is the number of
nutrients. This is a typical situation where being able to combine skills can be invaluable.
As an illustration, in our experiments we used m = 2 nutrients and 3 types of food. We defined
a cumulant ei ‚àà E associated with each nutrient as follows: ei (h, a, s) = 0 until a food item is
consumed, when it becomes the increase in the associated nutrient. After a food item is consumed we
have that ei (h, a, s) = ‚àí1{a 6= œÑ }, where 1{¬∑} is the indicator function‚Äîthis forces the induced
option to terminate, and also illustrates how the definition of cumulants over histories h can be useful
(since single states would not be enough to determine whether the agent has consumed a food item).
We used Algorithm 3 in Appendix B to compute the 4 value functions in QE . We then defined a
8-dimensional abstract action space covering the space of preferences, W ‚â° {‚àí1, 0, 1}2 ‚àí {[0, 0]},
and used it with the Q-learning player in Algorithm 2. We also consider Q-learning using only the 2
options maximizing each nutrient and a ‚Äúflat‚Äù Q-learning agent that does not use options at all.
By modifying the target range of each nutrient we can create distinct scenarios with very different
dynamics. Figure 2 shows results in two such scenarios. Note how the relative performance of the
two baselines changes dramatically from one scenario to the other, illustrating how the usefulness
of options is highly context-dependent. Importantly, as shown by the results of the OK player, the
6

100

140

80

Q-Learning Player
Q-Learning Simple Options
Q-Learning

120
100
80
60
40

Average Episode Return

Average Episode Return

160

40
20
0
20

20
0

Q-Learning Player
Q-Learning Simple Options
Q-Learning

60

40
0

200000

400000

steps

600000

800000

1000000

0

200000

400000

steps

600000

800000

1000000

Figure 2: Results on the foraging world. The two plots correspond to different configurations of the
environment (see Appendix C.1). Shaded regions are one standard deviation over 10 runs.
ability to combine options in cumulant space makes it possible to synthesise useful behaviour from a
given set of options even when they are not useful in isolation.
5.2 Moving-target arena
As the name suggests, in the moving-target arena the goal is to get to a target region whose location
changes every time the agent reaches it. The arena is implemented as a square room with realistic
dynamics defined in the MuJoCo physics engine [30]. The agent is a quadrupedal simulated robot
with 8 actuated degrees of freedom; actions are thus vectors in [‚àí1, 1]8 indicating the torque applied to
each joint [21]. Observations are 29-dimensional vectors with spatial and proprioception information
(Appendix C.2). The reward is always 0 except when the agent reaches the target, when it is 1.
We defined cumulants in order to encourage the agent‚Äôs displacement in certain directions. Let v(h)
be the vector of (x, y) velocities of the agent after observing history h (the velocity is part of the
observation). Then, if we want the agent to travel at a certain direction w for k steps, we can define:

w> v(h) if length(h) ‚â§ k;
ew (h, a, ¬∑) =
(8)
‚àí1{a 6= œÑ } otherwise.
The induced option will terminate after k = 8 steps as a negative reward is incurred for all histories
of length greater than k and actions other than œÑ . It turns out that even if a larger number of directions
w is to be learned, we only need to compute two value functions for each cumulant ew . Since for
all ew ‚àà E we have that ew = w1 ex + w2 ey , where x = [1, 0] and y = [0, 1], we can use (2) to
œâ
œâ
decompose the value function of any option œâ as Qœâ
ew (h, a) = w1 Qex (h, a) + w2 Qey (h, a). Hence,
2
|QE | = 2|E|, resulting in a 2-dimensional space W in which w ‚àà R indicates the intended direction
of locomotion. Thus, by learning a few options that move along specific directions, the agent is
potentially able to synthesise options that travel in any direction.
For our experiments, we defined cumulants ew corresponding to the directions 0o , 120o , and 240o .
To compute the set of value functions QE we used Algorithm 3 with Q-learning replaced by the
deterministic policy gradient (DPG) algorithm [22]. We then used the resulting OK with both discrete
and continuous abstract-action spaces W. For finite W we adopted a Q-learning player (Algorithm 2);
in this case the abstract actions wi correspond to n ‚àà {4, 6, 8} directions evenly-spaced in the unit
circle. For continuous W we used a DPG player. We compare OK‚Äôs results with that of DPG applied
directly in the original action space and also with Q-learning using only the three basic options.
Figure 3 shows our results on the moving-target arena. As one can see by DPG‚Äôs results, solving
the problem in the original action space is difficult because the occurrence of non-zero rewards may
depend on a long sequence of lucky actions. When we replace actions with options we see a clear
speed up in learning, even if we take into account the training of the options. If in addition we allow
for combined options, we observe a significant boost in performance, as shown by the OK players‚Äô
results. Here we see the expected trend: as we increase |W| the OK player takes longer to learn but
achieves better final performance, as larger numbers of directional options allow for finer control.
These results clearly illustrate the benefits of being able to combine skills, but how much is the agent
actually using this ability? In Figure 3 we show a histogram indicating how often combined options
are used by OK to implement directions w ‚àà R2 across the state space (details in App. C.2). As
shown, for abstract actions w close to 0o , 120o and 240o the agent relies mostly on the 3 options
trained to navigate along these directions, but as the intended direction of locomotion gets farther from
7

35

Average return per episode

30
25
20

OK training

15

DPG Player
Q-Learning Player (8)
Q-Learning Player (6)
Q-Learning Player (4)
Q-Learning + Options
DPG

10
5
0

0

1

2

Steps

3

4

5
1e7

Figure 3: Left: Results on the moving-target arena. All players used the same keyboard, so they
share the same OK training phase. Shaded regions are one standard deviation over 10 runs. Right:
Histogram of options used by OK to implement directions w. Black lines are the three basic options.
these reference points combined options become crucial. This shows how the ability to combine skills
can extend the range of behaviours available to an agent without the need for additional learning.1
Even if one accepts the premise of this paper that skills should be combined in the space of cumulants,
it is natural to ask whether other strategies could be used instead of GPE and GPI. Although we are
not aware of any other algorithm that explicitly attempts to combine skills in the space of cumulants,
there are methods that do so in the space of value functions [29, 6, 13, 16]. Haarnoja et al. [13]
propose a way of combining skills based on entropy-regularised value functions.
P Given a set of
cumulants e1 , e2 , ..., ed , they propose to compute a skill associated with e = i wi ei as follows:
P
œâe
œâe
œâÃÇe (h) ‚àà argmaxa‚ààA+ j wj QÃÇej j (h, a), where QÃÇej j (h, a) are entropy-regularised value functions
and wj ‚àà [‚àí1, 1]. We will refer to this method as additive value composition (AVC).
How well does AVC perform as compared to GPE and GPI? In order to answer this question we
reran the previous experiments but now using œâÃÇe (h) as defined above instead of the option œâÃÉe (h)
computed through (6) and (7). In order to adhere more closely to the assumptions underlying AVC,
we also repeated the experiment using an entropy-regularised OK [14] (App. C.2). Figure 4 shows
the results. As indicated in the figure, GPE and GPI outperform AVC both with the standard and the
entropy-regularised OK. A possible explanation for this is given in the accompanying polar scatter
chart in Figure 4, which illustrates how much progress each method makes, over the state space, in all
directions w (App. C.2). The plot suggests that, in this domain, the directional options implemented
through GPI and GPE are more effective in navigating along the desired directions (also see [16]).

6

Related work

Previous work has used GPI and successor features, the linear form of GPE considered here, in the
context of transfer [3, 4, 5]. A crucialPassumption underlying these works is that the reward can
be well approximated as r(s, a, s0 ) ‚âà i wi ci (s, a, s0 ). By solving a regression problem, the agent
finds a w ‚àà Rd that leads to a good approximation of r(s, a, s0 ) and uses it to apply GPE and GPI
(equations (2) and (3), respectively). In terms of the current work, this is equivalent to having a
keyboard player that is only allowed to play one endless ‚Äúchord‚Äù. Through the introduction of a
termination action, in this work we replace policies with options that may eventually halt. Since
policies are options that never terminate, the previous framework is a special case of OK. Unlike in
the previous framework, with OK we can also chain a sequence of options, resulting in more flexible
behaviour. Importantly, this allows us to completely remove the linearity assumption on the rewards.
We now turn our attention to previous attempts to combine skills with no additional learning. As
discussed, one way to do so is to work directly in the space of policies. Many policy-based methods
first learn a parametric representation of a lower-level policy, œÄ(¬∑| s; Œ∏), and then use Œ∏ ‚àà Rd as the
actions for a higher-level policy ¬µ : S 7‚Üí Rd [15, 10, 12]. One of the central arguments of this paper
1
A video of the quadrupedal simulated robot being controlled by the DPG player can be found on the
following link: https://youtu.be/39Ye8cMyelQ.

8

35

Average return per episode

30
25
20

OK training

15

GPE and GPI + OK
GPE and GPI + ENT-OK
AVC + OK
AVC + ENT-OK
DPG

10
5
0

0

1

2

Steps

3

4

5
1e7

Figure 4: Left: Comparison of GPE and GPI with AVC on the moving-target arena. Results were
obtained by a DPG player using a standard OK and an entropy-regularised counterpart (ENT-OK).
We trained several ENT-OK with different regularisation parameters and picked the one leading to the
best AVC performance. The same player and keyboards were used for both methods. Shaded regions
are one standard deviation over 10 runs. Right: Polar scatter chart showing the average distance
travelled by the agent along directions w when combining options using the two competing methods.
is that combining skills in the space of cumulants may be advantageous because it corresponds to
manipulating the goals underlying the skills. This can be seen if we think of w ‚àà Rd as a way of
encoding skills and compare its effect on behaviour with that of Œ∏: although the option induced by
w1 + w2 through (6) and (7) will seek a combination of both its constituent‚Äôs goals, the same cannot
be said about a skill analogously defined as œÄ(¬∑| s; Œ∏ 1 + Œ∏ 2 ). More generally, though, one should
expect both policy- and cumulant-based approaches to have advantages and disadvantages.
Interestingly, most of the previous attempts to combine skills in the space of value functions are based
on entropy-regularised RL, like the already discussed AVC [34, 9, 11, 13]. Hunt et al. [16] propose a
way of combining skills which can in principle lead to optimal performance if one knows in advance
the weights of the intended combinations. They also extend GPE and GPI to entropy-regularised
RL. Todorov [28] focuses on entropy-regularised RL on linearly solvable MDPs. Todorov [29] and
da Silva et al. [6] have shown how, in this scenario, one can compute optimal skills corresponding
to linear combinations of other optimal skills‚Äîa property later explored by Saxe et al. [20] to
propose a hierarchical approach. Along similar lines, Van Niekerk et al. [31] have shown how
optimal value function composition can be obtained in entropy-regularised shortest-path problems
with deterministic dynamics, with the non-regularised setup as a limiting case.

7

Conclusion

The ability to combine skills makes it possible for an RL agent to learn a small set of skills and
then use them to generate a potentially very large number of distinct behaviours. A robust way of
combining skills is to do so in the space of cumulants, but in order to accomplish this one needs
to solve two problems: (1) establish a well-defined mapping between cumulants and skills and (2)
define a mechanism to implement the combined skills without having to learn them.
The two main technical contributions of this paper are solutions for these challenging problems. First,
we have shown that every deterministic option can be induced by a cumulant defined in an extended
domain. This novel theoretical result provides a way of thinking about options whose interest may
go beyond the current work. Second, we have described how to use GPE and GPI to synthesise
combined options on-the-fly, with no learning involved. To the best of our knowledge, this is the only
method to do so in general MDPs with performance guarantees for the combined options.
We used the above formalism to introduce OK, an interface to an RL problem in which actions
correspond to combined skills. Since OK is compatible with essentially any RL method, it can be
readily used to endow our agents with the ability to combine skills. In describing the analogy with a
keyboard that inspired our work, Sutton [23] calls for the need of ‚Äúsomething larger than actions, but
more combinatorial than the conventional notion of options.‚Äù We believe OK provides exactly that.

9

Acknowledgements
We thank Joseph Modayil for first bringing the subgoal keyboard idea to our attention, and also
for the subsequent discussions on the subject. We are also grateful to Richard Sutton, Tom Schaul,
Daniel Mankowitz, Steven Hansen, and Tuomas Haarnoja for the invaluable conversations that helped
us develop our ideas and improve the paper. Finally, we thank the anonymous reviewers for their
comments and suggestions.

References
[1] P. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI), 2017.
[2] A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with
generalized policy updates. Proceedings of the National Academy of Sciences, 117(48):30079‚Äì
30087, 2020.
[3] A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
Systems (NIPS), 2017.
[4] A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised
policy improvement. In Proceedings of the International Conference on Machine Learning
(ICML), 2018.
[5] D. Borsa, A. Barreto, J. Quan, D. J. Mankowitz, H. van Hasselt, R. Munos, D. Silver, and
T. Schaul. Universal successor features approximators. In International Conference on Learning
Representations (ICLR), 2019.
[6] M. da Silva, F. Durand, and J. PopovicÃÅ. Linear Bellman combination for control of character
animation. ACM Transactions on Graphics, 28(3):82:1‚Äì82:10, 2009.
[7] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In Advances in Neural Information
Processing Systems (NIPS), 1993.
[8] T. G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research, 13:227‚Äì303, 2000.
[9] R. Fox, A. Pakman, and N. Tishby. Taming the noise in reinforcement learning via soft updates.
In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2016.
[10] K. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta learning shared hierarchies. In
International Conference on Learning Representations (ICLR), 2018.
[11] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based
policies. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
[12] T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine. Latent space policies for hierarchical
reinforcement learning. In Proceedings of the International Conference on Machine Learning
(ICML), 2018.
[13] T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement learning for robotic manipulation. In IEEE International Conference on Robotics and
Automation (ICRA), 2018.
[14] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In Proceedings of the International
Conference on Machine Learning (ICML), 2018.
[15] N. Heess, G. Wayne, Y. Tassa, T. P. Lillicrap, M. A. Riedmiller, and D. Silver. Learning
and transfer of modulated locomotor controllers. CoRR, abs/1610.05182, 2016. URL http:
//arxiv.org/abs/1610.05182.
10

[16] J. J. Hunt, A. Barreto, T. P. Lillicrap, and N. Heess. Entropic policy composition with generalized
policy improvement and divergence correction. In Proceedings of the International Conference
on Machine Learning (ICML), 2019.
[17] L. P. Kaelbling. Hierarchical learning in stochastic domains: Preliminary results. In Proceedings
of the International Conference on Machine Learning (ICML), 2014.
[18] R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. In Proceedings of
the Conference on Advances in Neural Information Processing Systems (NIPS), 1997.
[19] M. L. Puterman. Markov Decision Processes‚ÄîDiscrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc., 1994.
[20] A. M. Saxe, A. C. Earle, and B. Rosman. Hierarchy through composition with multitask
LMDPS. In Proceedings of the International Conference on Machine Learning (ICML), 2017.
[21] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous
control using generalized advantage estimation. In Proceedings of the International Conference
on Learning Representations (ICLR), 2016.
[22] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy
gradient algorithms. In Proceedings of the International Conference on Machine Learning
(ICML), 2014.
[23] R. Sutton. Toward a new view of action selection: The subgoal keyboard. Slides presented
at the Barbados Workshop on Reinforcement Learning, 2016. URL http://barbados2016.
rl-community.org/RichSutton2016.pdf?attredirects=0&d=1.
[24] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems
(NIPS), 2000.
[25] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.
[26] R. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: a framework for
temporal abstraction in reinforcement learning. Artificial Intelligence, 112:181‚Äì211, 1999.
[27] R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde:
A scalable real-time architecture for learning knowledge from unsupervised sensorimotor
interaction. In International Conference on Autonomous Agents and Multiagent Systems
(AMAS), 2011.
[28] E. Todorov. Linearly-solvable Markov decision problems. In Advances in Neural Information
Processing Systems (NIPS), 2007.
[29] E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information
Processing Systems (NIPS), 2009.
[30] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In
Intelligent Robots and Systems (IROS), 2012.
[31] B. Van Niekerk, S. James, A. Earle, and B. Rosman. Composing value functions in reinforcement
learning. In Proceedings of the International Conference on Machine Learning (ICML), 2019.
[32] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu.
FeUdal networks for hierarchical reinforcement learning. In Proceedings of the International
Conference on Machine Learning (ICML), pages 3540‚Äì3549, 2017.
[33] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229‚Äì256, 1992.
[34] B. D. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, Carnegie Mellon University, 2010.

11

The Option Keyboard
Combining Skills in Reinforcement Learning
Supplementary Material
Andr√© Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Ayg√ºn,
Philippe Hamel, Daniel Toyama, Jonathan Hunt, Shibl Mourad, David Silver, Doina Precup
{andrebarreto,borsa,shaobohou,gcomanici,eser}@google.com
{hamelphi,kenjitoyama,jjhunt,shibl,davidsilver,doinap}@google.com

DeepMind

Abstract
In this supplement we give details of the theory and experiments that had to be left
out of the main paper due to space constraints. We prove our theoretical result,
provide a thorough description of the protocol used to carry out our experiments,
and present details of the algorithms. We also present additional empirical results
and analysis, as well as a more in-depth discussion of several aspects of OK. The
numbering of sections, equations, and figures resume from what is used in the
main paper, so we refer to these elements as if paper and supplement were a single
document.

A

Theoretical results

Proposition 1. Every extended cumulant induces at least one deterministic option, and every deterministic option can be unambiguously induced by an infinite number of extended cumulants.
Proof. We start by showing that every extended cumulant induces one or more deterministic options.
Let e : H √ó A+ √ó S 7‚Üí R be a cumulant defined in an MDP M ‚â° (S, A, p, ¬∑, Œ≥); our strategy will
be to define an extended MDP M + and a corresponding cumulant eÃÇ and show that maximising eÃÇ over
M + corresponds to a deterministic option in the original MDP M .
Since we need to model the termination of options, we will start by defining a fictitious absorbing
state s‚àÖ and let H+ ‚â° H ‚à™ {s‚àÖ }. Moreover, we will use the following notation for the last state in a
history: last(ht:t+k ) = st+k . Define M + ‚â° (H+ , A+ , pÃÇ, ¬∑, Œ≥) where
pÃÇ(has|h, a) = p(s|last(h), a) for all (h, a, s) ‚àà H √ó A √ó S,
pÃÇ(s‚àÖ |h, œÑ ) = 1 for all h ‚àà H, and
pÃÇ(s‚àÖ |s‚àÖ , a) = 1 for all a ‚àà A+ .

We can now define the cumulant eÃÇ for M + as follows:
eÃÇ(h, a, s) = e(h, a, s) for all (h, a, s) ‚àà H √ó A+ √ó S, and
eÃÇ(s‚àÖ , a, s‚àÖ ) = 0 for all a ‚àà A+ .
We know from the dynamic programming theory that maximising eÃÇ over M + has a unique optimal
value function Q‚àóeÃÇ [19]; we will use Q‚àóeÃÇ to induce the three components that define an option. First,
define the option‚Äôs policy œÄe : H ‚Üí A with œÄe (h) ‚â° argmaxa6=œÑ Q‚àóeÃÇ (h, a) (with ties broken
arbitrarily). Then, define the termination function as

1 if œÑ ‚àà argmaxa Q‚àóeÃÇ (h, a),
Œ≤e (h) ‚â°
0 otherwise.
12

Finally, let Ie ‚â° {s | Œ≤e (s) = 0} be the initiation set. It is easy to see that the option oe ‚â° (Ie , œÄe , Œ≤e )
is a deterministic option in the MDP M .
We now show that every deterministic option can be unambiguously induced by an infinite number of
extended cumulants. Given a deterministic option o specified in M and a negative number z < 0, our
strategy will be to define an augmented cumulant ez : H √ó A+ √ó S 7‚Üí R that will induce option o
using the construction above (i.e. from the optimal value function Q‚àóeÃÇz on M + ).
First we note a subtle point regarding the execution of options and the interaction between the
initiation set and the termination function. Whenever an option o is initiated in state s ‚àà Io , it first
executes a = œÄo (s) and only checks the termination function in the resulting state. This means that
an option o will always be executed for at least one time step. Similarly, an option that cannot be
initiated in state s does not need to terminate at this state (that is, it can be that s ‚àà
/ Io and Œ≤o (h) < 1,
with last(h) = s). Given a deterministic option o ‚â° (Io , œÄo , Œ≤o ), let
Ô£±
0 if a = œÑ, h ‚àà S and h ‚àà
/ Io ,
Ô£¥
Ô£≤
0 if a = œÑ, h ‚àà
/ S and Œ≤o (h) = 1,
ez (h, a, ¬∑) =
(9)
Ô£¥
Ô£≥ 0 if a = œÄo (h), and
z otherwise.
We use the same MDP extension M + ‚â° (H+ , A+ , pÃÇ, ¬∑, Œ≥) as described above and maximise the
extended cumulant eÃÇz . It should be clear that Q‚àóeÃÇz (h, a) = 0 only when the action a corresponds
to either a transition or a termination dictated by option o, and Q‚àóeÃÇz (h, a) < 0 otherwise. As such,
option o is induced by the set of cumulants {ez | z < 0} of infinite size.

B

Additional pseudo-code

In this section we present one additional pseudo-code as a complement to the material in the main
paper. Algorithm 3 shows a very simple way of building the set QE used by OK through Q-learning
and -greedy exploration. The algorithm uses fairly standard RL concepts. Perhaps the only detail
worthy of attention is the strategy adopted to explore the environment, which switches between
options with a given probability (1 in Algorithm 3). This is a simple, if somewhat arbitrary, strategy
to collect data, which can probably be improved. It was nevertheless sufficient to generate good
results in our experiments.

C

Details of the experiments

In this section we give details of the experiments that had to be left out of the main paper due to the
space limit.
C.1

Foraging world

C.1.1

Environment

We start by giving a more detailed description of the environment. The goal in the foraging world is
to manage a set of m ‚Äúnutrients‚Äù i by navigating in a grid world and picking up food items containing
these nutrients in different proportions. At every time step t the agent has a certain quantity of each
nutrient available, xit , and the desirability of nutrient i is a function of xit , di (xit ). For example, we
can have di (xit ) = 1 if xit is within certain bounds and di (xit ) = ‚àí1 otherwise. The quantity xit
decreases by a fixed amount li at each time step, regardless of what the agent does: x0it = xit ‚àí li .
The agent can increase xit by picking up one of the many food items available. Each item is of a
certain type j, which defines how much of each nutrient it provides. We can thus represent a food
type as a vector y j ‚àà Rm where yji indicates how much xit increases when the agent consumes
an item
P of that type. If the agent picks up an item of type j at time step t it receives a reward of
rt = i yji di (xit ), where xit = x0it‚àí1 + yji . If the agent does not pick up any items it gets a reward
of zero and xit = x0it‚àí1 . The environment is implemented as a grid, with agent and food items
occupying one cell each, and the usual four directional actions available. Observations are images
representing the configuration of the grid plus a vector of nutrients xt = [x1t , ..., xmt ].
The foraging world was implemented as a 12 √ó 12 grid with toroidal dynamics‚Äîthat is, the grid
‚Äúwraps around‚Äù connecting cells on opposite edges. We used m = 2 nutrients and 3 types of food:
13

Algorithm 3 Compute set QE with -greedy Q-learning
Ô£±
E = {e1 , e2 , ..., ed } cumulants
Ô£¥
Ô£¥
Ô£¥
Ô£≤ 1
probability of changing cumulant
2
exploration parameter
Require:
Ô£¥
Ô£¥
Œ±
learning
rate
Ô£¥
Ô£≥
Œ≥
discount rate
1: select an initial state s ‚àà S
2: k ‚Üê Uniform({1, 2, ..., d})
3: repeat
4:
if Bernoulli(1 )=1 then
5:
h‚Üês
6:
k ‚Üê Uniform({1, 2, ..., d})
7:
if Bernoulli(2 )=1 then a ‚Üê Uniform(A)
œâe
8:
else a ‚Üê argmaxb QÃÉek k (h, b)
9:
if a 6= œÑ then
10:
execute action a and observe s0
11:
h0 ‚Üê u(h, a, s0 )
12:
for i ‚Üê 1, 2, ..., d do
œâe
13:
a0 ‚Üê argmaxb QÃÉei i (h0 , b)
14:
for j ‚Üê 1, 2, ..., d do
15:
16:
17:
18:
19:
20:
21:
22:

œâe

// pick a random ek
// explore
// GPI

// e.g. u(h, a, s0 ) = has0
// update Q-values
// a0 = œâei (h0 )

œâe

Œ¥ ‚Üê ej (h, a, s0 ) + Œ≥ 0 QÃÉej i (h0 , a0 ) ‚àí QÃÉej i (h, a)
œâe
Œ∏ œâi ‚Üê Œ∏ œâi + Œ±Œ¥‚àáŒ∏œâi QÃÉej i (h, a)

s ‚Üê s0
else
for i ‚Üê 1, 2, ..., d do
for j ‚Üê 1, 2, ..., d do
œâe
Œ¥ ‚Üê ej (h, œÑ ) ‚àí QÃÉej i (h, œÑ )

// update values associated with termination

œâe

Œ∏ œâi ‚Üê Œ∏ œâi + Œ±Œ¥‚àáŒ∏œâi QÃÉej i (h, œÑ )

23: until stop criterion is satisfied
œâe
24: return QE ‚â° {QÃÉej i | ‚àÄ(i, j) ‚àà {1, 2, ..., d}2 }

y 1 = (1, 0), y 2 = (0, 1), and y 3 = (1, 1). Observations were image-like features of dimensions
12 √ó 12 √ó 3, where the last dimension indicates whether there is a food item of a certain type present
in a cell. The observations reflect the agent‚Äôs ‚Äúegocentric‚Äù view, i.e., the agent is always located at
the centre of the grid and is thus not explicitly represented. At every step the amount available of
each nutrient was decreased by li = 0.05, for i = 1, 2. The desirability functions di (xi ) used in the
experiments of Section 5.1 were:

Scenario
1

+1 x1 ‚â§ 10
d1 (x1 ) =
‚àí1 x1 > 10
Ô£±
Ô£≤‚àí1 x2 ‚â§ 5
d2 (x2 ) = +5 5 < x2 < 25
Ô£≥
‚àí1 x2 ‚â• 25
C.1.2

Scenario
2

+1 x1 ‚â§ 10
d1 (x1 ) =
‚àí1 x1 < 10
Ô£±
Ô£≤‚àí1 x2 ‚â§ 5
d2 (x2 ) = +5 5 < x2 < 15
Ô£≥
‚àí1 x2 ‚â• 15

Agent

Agents‚Äô architecture: All the agents used a multilayer perceptron (MLP) with the same architecture
to compute the value functions. The network had two hidden layers of 64 and 128 units with RELU
activations. Q-learning‚Äôs network had |A| = 4 output units corresponding to QÃÉ(s, a). The network
of the Q-learning player had |W| output units corresponding to QÃÉ(s, w), while OK‚Äôs network had
œâe
2 √ó 2 √ó |A+ | outputs corresponding to QÃÉej i (h, a) ‚àà QE .
14

(a) Arena

(b) Quadrupedal simulated robot

Figure 5: The moving-target arena
The states s used by Q-learning and the Q-learning player were 12 √ó 12 √ó 3 images plus a twodimensional vector x corresponding to the agent‚Äôs nutrients. The histories h used by OK were
s plus an indicator function signalling whether the agent has picked up a food item‚Äîthat is,
the update function u(h, a, s0 ) showing up in Algorithms 1 and 3 was defined as u(h, a, s0 ) =
[1{agent has picked up a food item}, s0 ].
Agents‚Äô training: As described in Section 5.1, in order to build OK we defined one cumulant
ei ‚àà E associated with each nutrient. We now explain in more detail how cumulants were defined. If
the agent picks up a food item of type j at time step t, ei (ht , at , ¬∑) = yji . After a food item is picked
up we have that ei (h, a, s) = ‚àí1{a 6= œÑ } for all h, a, and s‚Äîthat is, the agent gets penalised unless
it terminates the option. In all other situations ei = 0.
OK was built using Algorithm 3 with the cumulants ei ‚àà E, exploration parameters 1 = 0.2 and
2 = 0.1, and discount rate Œ≥ = 0.99. The agent interacted with the environment in episodes of
length 100. We tried the learning rates Œ± ‚àà L1 ‚â° {10‚àí1 , 10‚àí2 , 10‚àí3 , 10‚àí4 } and selected the OK
that resulted in the best performance using w = (1, 1) on a scenario with d1 (x) = d2 (x) = 1 for all
x. OK was trained for 5 √ó 106 steps, but visual inspection suggests that less than 10% of the training
time would lead to the same results.2
The Q-learning player was trained using Algorithm 2 with the abstract action set W ‚â° {‚àí1, 0, 1}2 ‚àí
{[0, 0]} described in the paper,  = 0.1 and Œ≥ = 0.99. All the agents interacted with the environment
in episodes of length 300. For all algorithms (flat Q-learning, Q-learning + options, and Q-learning
player) we tried learning rates Œ± in the set L1 above and picked the configuration that led to the
maximum return averaged over the last 100 episodes and 10 runs.
C.2
C.2.1

Moving-target arena
Environment

The environment was implemented using the MuJoCo physics engine [30] (see Figure 5). The arena
was defined as a bounded region [‚àí10, 10]2 and the targets were circles of radius 0.8. We used a
control time step of 0.2. The reward is always 0 except when the agent reaches the target, when it
gets a reward of 1. In this case both the agent and the target reappear in random locations in [‚àí5, 5]2 .
C.2.2

Agent

Agents‚Äô architecture: The network architecture used for the agents was identical to that used in
the experiments with the foraging world (Section C.1). Observations are 29-dimensional with the
agent‚Äôs current (x, y) position and velocity, its orientation matrix (3 √ó 3), a 2-dimensional vector
of distances from the agent to the current target, and two 8-dimensional vectors with angles and
2

Since the point of the experiment was not to make a case in favour of temporal abstraction, we did not
deliberately try to minimise the total number of sample transitions used to train the options.

15

velocities of each joint. The histories h used by OK were simply the length of the trajectory plus the
current state, that is, the update function u(h, a, s0 ) showing up in Algorithms 1 and 3 was defined in
order to compute ht:t+k = [k, st+k ]. As mentioned in the main paper, A ‚â° [‚àí1, 1]8 .
Agents‚Äô training: The set of value functions QE used by OK was built using Algorithm 3 with Qlearning replaced by deterministic policy gradient (DPG) [22]. Specifically, for each cumulant ei ‚àà E
we ran standard DPG and used the same data to evaluate the resulting policies on-line over the set of
cumulants E. The cumulants ei ‚àà E used were the ones described in Section 5.2, equation (8). During
training exploration was achieved by adding zero-mean Gaussian noise with standard deviation 0.1
to DPG‚Äôs policy. We used batches of 10 transitions per update, no experience replay, and a target
network. The discount rate used was Œ≥ = 0.9. The agent interacted with the environment in episodes
of length 300. We swept over learning rates Œ± ‚àà L2 ‚â° {10‚àí2 , 10‚àí3 , 3 √ó 10‚àí4 , 10‚àí4 , 10‚àí5 }, and
selected the OK that resulted in the best performance in a small set of evaluation vectors w with
w1 , w2 > 0 (that is, the directions used for evaluation did not correspond to those of the basic options
œâei ). OK was trained for 107 steps.
The Q-learning players were trained using Algorithm 2 with the discrete abstraction action set W
described in the paper,  = 0.1, and Œ≥ = 0.99. Updates were applied to batches of 10 sample
transitions. The DPG player was trained using the same implementation as the DPG used to build OK,
and the same value Œ≥ = 0.99 used by the Q-learning player. Note that, given a fixed w ‚àà R2 , in order
to compute the max operator appearing in (7) we need to sample actions a ‚àà R8 . We did so using a
simple cross-entropy Monte-Carlo method with 50 samples [35]. The same DPG implementation
was also used by the flat DPG as a baseline, the only difference being that it used the actions space
A ‚äÇ R8 instead of the abstract action space W ‚äÇ R2 . All the agents interacted with the environment
in episodes of length 1200. For all algorithms (Q-learning, Q-learning player, DPG, and DPG player)
we tried learning rates Œ± in the set L2 above and picked the configuration that led to the maximum
average return, averaged over 10 runs.
The results comparing GPE and GPI with AVC shown in Figure 4 were generated exactly as
explained above. In order to train the entropy-regularised OKs we used the soft actor-critic algorithm proposed by Haarnoja et al. [14]. We trained one OK for each regularisation parameter in
{0.001, 0.01, 0.03, 0.1, 0.3, 1.0} and selected the one leading to the best performance of AVC. In
addition to the implementation of the AVC option œâÃÇe (h) described in Section 5.2, we also tried a
P
œâe
‚Äúsoft‚Äù version in which œâÃÇe is a stochastic policy defined as œâÃÇe (a|h) ‚àù j wj QÃÇej j (h, a), where, as
œâe

before, QÃÇej j (h, a) are entropy-regularised value functions and wj ‚àà [‚àí1, 1]. The results with the
stochastic policy were slightly worse than the ones shown in Figure 4 (this is consistent with some
experiments reported by Haarnoja et al. [14]).
C.2.3

Experiments

In order to generate the histogram shown in Figure 3 we sampled 100 000 directions w ‚àà R2 from
a player with a uniformly random policy and inspected the value of the action a ‚àà R8 returned by
GPI (7). Specifically, we considered the selected action came from one of the three basic options œâei
if
œâe
œâe
mini‚àà{1,2,3},a‚ààAÃÉ Qe i (h, œâei (h) ‚àí Qe i (h, a)) ‚â§ 0.15,
(10)
P
where e = i wi ei and AÃÉ is the set of actions sampled through the cross-entropy sampling process
described in the previous section. If (10) was true we considered the action selected came from
the option œâei associated with the index i that minimises the left-hand side of (10); otherwise we
considered the action came from a combined option.
In order to generate the polar scatter chart shown in Figure 4 we sampled 10 000 pairs (s, w), with s
sampled uniformly at random from S and abstract actions w sampled from an isotropic Gaussian
distribution in Rd with unit variance (where d = 3 for AVC and d = 2 for GPE and GPI).3 Then,
for each pair (s, w), we ran the option resulting from (6) and (7) for 60 simulated seconds, without
3
As explained in Section 5.2, in our implementation of GPE and GPI we explored the fact that Qœâ
ew (h, a) =
œâ
2
w1 Qœâ
ex (h, a) + w2 Qey (h, a) for any option œâ and any direction w ‚àà R , where x = [1, 0] and y = [0, 1], to
only compute two value functions per cumulant e. This results in a two-dimensional abstract space w ‚àà R2 .
Since GPE is not part of AVC (that is, the option induced by a cumulant is not evaluated under other cumulants),
it is not clear how to carry out a similar decomposition in this case.

16

termination, and measured the distance travelled along the desired direction w (for w ‚àà R3 we first
projected the weights onto R2 using the decomposition discussed in Section 5.2). Each point in the
scatter chart defines a vector whose direction is the intended w and whose magnitude is the travelled
distance along that direction.

D

Discussion

In this section we take a closer look at some aspects of OK. We start with a thorough discussion
on how extended cumulants can be used to define deterministic options; we then analyse several
properties of GPE and GPI in more detail.
D.1

Defining options through extended cumulants

We have shown that every deterministic option o can be represented by an augmented policy œâe :
H 7‚Üí A+ , which in turn can be induced by an extended cumulant e : H √ó A+ √ó S 7‚Üí R (in fact, by
an infinite number of them). In order to provide some intuition on these relations, in this section we
give a few concrete examples of how to generate potentially useful options using extended cumulants.
We start by defining an option that executes a policy œÄ : S 7‚Üí A for k time steps and then terminates.
This can be accomplished using the following cumulant:
(
0 if length(h) ‚â§ k and a = œÄ(last(h));
0 if length(h) = k + 1 and a = œÑ ;
e(h, a, ¬∑) =
(11)
‚àí1 otherwise,
where length(h) is the length of history h, that is, length(ht,t+k ) = k + 1, and last(ht:t+k ) = st+k
(also see (8)). Note that if œÄ(s) = a for all s ‚àà S action a is repeated k times in sequence; for the
particular case where k = 1 we recover the primitive action a. Another instructive example is an
option that navigates to a goal state g ‚àà S and terminates once this state has been reached. We can
get such behaviour using the following extended cumulant:

1 if last(h) = g and a = œÑ ;
e(h, a, ¬∑) =
(12)
0 otherwise.
Note that the cumulant is non-zero only when the agent chooses to terminate in g. Yet another
possibility is to define a fixed termination bonus e(h, œÑ ) = z for all h ‚àà H, where z ‚àà R; in this case
the option will terminate whenever it is no longer possible to get more than z discounted units of e.
Even though working in the space of histories H is convenient at the conceptual level, in practice
the extended cumulants only have to be defined in a small subset of this space, which makes them
easy to be implemented. In order to implement (11), for example, one only needs to keep track of
the number of steps executed by the option and the last state and action experienced by the agent
(cf. Section C.2.2). The implementation of (12) is even simpler, requiring only the current state and
action. Obviously, one is not restricted to cumulants of these forms; other versions of e can define
interesting trade-offs between terminating and continuing.
As a final observation, note that, unlike with standard termination functions Œ≤o (h), (5) depends on
œâe
e
the value function Qœâ
e . This means that, when Qe is being learned, the termination condition may
change during learning. This can be seen as a natural way of incorporating Œ≤o (h) into the learning
process, and thus impose a form of consistency on the agent‚Äôs behaviour. When we define (5), we
are asking the agent to terminate in h if it cannot get more than e(h, œÑ ) (discounted) units of e; thus,
even if it is possible to do so, a sub-optimal agent that is not capable of achieving this should perhaps
indeed terminate.
D.2

GPE and GPI

The nature of GPE and GPI‚Äôs options: Given a set of cumulants E, GPE and GPI can be used to
compute an approximation of any option induced by a linear combination of the elements of this
set. Although this potentially gives rise to a very rich set of behaviours, not all useful combinations
of skills can be represented in this way. To illustrate this point, suppose that all cumulants e ‚àà E
take values in {0, 1}. In this case, when the weights w are nonnegative, it is instructive to think
17

of GPE and GPI as implementing something in between the AND and the OR logical operators, as
positive cumulants are rewarding in isolation but more so in combination. GPE and GPI cannot
implement a strict AND, for example, since this would require only rewarding the agent when all
cumulants are equal to 1. Van Niekerk et al. [31] present a related discussion in the context of
entropy-regularised RL.
The mechanics of GPE and GPI: There are two ways in which OK‚Äôs combined options can provide
benefits with respect to an agent that only uses single options. As discussed in Section 3.2, a combined
option constructed through GPE and GPI can be different from all its constituent options, meaning
that the actions selected by the former may not coincide with any of the actions taken by the latter
(including termination). But, even when the combined option could in principle be recovered as a
sequence of its constituents, having it can be very advantageous for the agent. To see why this is
so, it is instructive to think of GPE and GPI in this case as a way of automatically carrying out an
alternation of the single options that would otherwise have to be deliberately implemented by the
agent. This means that, in order to emulate combined options that are a sequence of single options,
a termination should occur at every point where the option achieving the maximum in (7) changes,
resulting in potentially many more decisions to be made by the agent.
Option discovery: As discussed in Section 3, the precise interface to an RL problem provided by OK
is defined by a set of extended cumulants E plus a set of abstract actions W. A natural question is then
how to define E and W. Although we do not have a definite answer to this question, we argue that
these definitions should aim at exploiting a specific structure in the RL problem. Many RL problems
allow for a hierarchical decomposition in which decisions are made at different levels of temporal
abstraction. For example, as illustrated in Section 5.2, in a navigation task it can be beneficial
to separate decisions at the level of intended locomotion (e.g., ‚Äúgo northeast‚Äù) from their actual
implementation (e.g., ‚Äúapply a certain force to a specific joint‚Äù). Most hierarchical RL algorithms
exploit this sort of structure in the problem; another type of structure that has received less attention
occurs when each hierarchical level can be further decomposed into distinct skills that can then be
combined (for example, the action ‚Äúgo northeast‚Äù can be decomposed into ‚Äúgo north‚Äù and ‚Äúgo east‚Äù).
In this context, the cumulants in E should describe the basic skills to be combined and the set W
should identify the combinations of these skills that are useful. Thus, the definition of E and W
decomposes the problem of option discovery into two well-defined objectives, which can potentially
make it more approachable.
The effects of approximation: Once E and W have been defined one has an interface to a RL
problem composed of a set of deterministic
options œâÃÉe . Each œâÃÉe is an approximation of the option
P
œâe induced by the cumulant e = i wi ei . Barreto et al. [3] have shown that it is possible to bound
œâei
œâÃÉe
e
Qœâ
e ‚àí Qe based on the quality of the approximations QÃÉej and the minimum distance between
e and the cumulants ei ‚àà E. Although this is a reassuring result, in the scenario studied here the
sub-optimality of the options œâÃÉe is less of a concern because it can potentially be circumvented by the
operation of the player. To see why this is so, note that a navigation option that slightly deviates from
the intended direction can be corrected by the other directional options (especially if it is prematurely
interrupted, which, as discussed in Section D.1, can be a positive side effect of using (5)). Although
it is probably desirable to have good approximations of the options intended in the design of E, the
player should be able to do well as long as the set of available options is expressive enough. This
suggests that the potential to induce a diverse set of options may be an important criterion in the
definition of the cumulants E, something previously advocated in the literature.

E

Additional results and analysis

We now present some additional empirical results that had to be left out of the main paper due to the
space limit.
E.1

Foraging World

In this section we will take a closer look at the results presented in the main paper and study stepby-step the behaviour induced by different desirability profiles of the nutrients. For each of these
regimes of desirabily, we will also inquire what the combined options will do and which of them
one would expect to be useful. In order to do this, we are going to consider various instantiations
of the absract action set W. As a reminder, the Q-learning player presented in the main paper
18

350

QP(3)-1,2,3,4

250
200
150

Q-Learning Player (8)
Q-Learning Simple Options
Q-Learning

100

0

200000

400000

steps

600000

800000

50
40

QP(3)-1,2,4

30
20

50
0

QP(3)-3

60

average episode return

average episode return

300

10

1000000

0

200000

(a) Scenario A1
1.06
1.04
1.02
1.00
0.98
0.96
0.94
3.15 0
3.10
3.05
3.00
2.95
2.90
2.85
2.80
0

400000

600000

steps

800000

1000000

(b) Scenario A2

2

4

6

8

10

2

4

6

8

10

1.06
1.04
1.02
1.00
0.98
0.96
0.94
0.94 0
0.96
0.98
1.00
1.02
1.04
1.06
0

(c) Scenario A1: di (xi )

2

4

6

8

10

2

4

6

8

10

(d) Scenario A2: di (xi )

Figure 6: Results on the foraging world using m = 2 nutrients and 3 types of food items: y 1 = (1, 0),
y 2 = (0, 1), and y 3 = (1, 1). Shaded regions are one standard deviation over 10 runs.

was trained using Algorithm 2 with the abstract action set W ‚â° {‚àí1, 0, 1}2 ‚àí {[0, 0]}. In the
following section, we will refer back to this agent as ‚ÄôQ-learning player (8)‚Äô, indicating the cardinality
of the set of absract action considered ‚Äì in this case, |W| = 8. In addition, we will consider in
our investigations W0 ‚â° {(1, 0), (0, 1)}, the set of basic options, and the following individual
combinations: w1 = (1, 1), w2 = (1, ‚àí1), w3 = (‚àí1, 1), and w4 = (‚àí1, ‚àí1). As in the main
paper, we refer to the instantiation of the Q-learning player that uses W0 as Q-learning player +
options (QO). Otherwise, we will use QP(n) to refer to a Q-learning player with n (combined) options.
Specifically, we adopt QP(3)-i to refer to the players using Wi ‚â° W0 ‚à™ {wi }. Finally, throughout
this study, we include a version of Q-learning (QL) that uses the original action space to serve as our
flat agent baseline. This agent does not use any form of abstraction and does not have access to the
trained options.
Most of the settings of the environment stay the same: we are going to be considering two types of
nutrients and three food items {y 1 = (1, 0), y 2 = (0, 1), y 3 = (1, 1)} available for pick up. The only
thing we are going to be varying is the desirability function associated with each of these nutrients.
We will see that this alone already gives rise to very interesting and qualitatively different learning
dynamics. In particular, we are going to be looking at four scenarios, slightly simpler than the ones
used in the paper, based on the same (pre-trained) keyboard QE . These scenarios should help the
reader to build some intuition for what a player based on this keyboard could achieve by combining
options under multiple changes in the desirability functions‚Äîas exemplified by the scenarios 1 and 2
in the main paper, Figure 2.
The simplest scenario one could consider is one where the desirability functions associated with each
nutrients are constant. In particular we will look at the scenario where both desirability functions
are positive (Figure 6(c)). In this case we benefit from picking up any of the two nutrients and the
most desirable item is item 3 which gives the agent a unit of each nutrient. As our keyboard was
trained for cumulants corresponding to W0 = {(1, 0), (0, 1)}, the trained primitive options would
be particularly suited for this task, as they are tuned to picking up the food items present in the
environment. Performance of the player and comparison with a Q-learning agent are reported in
Figure 6(a). The first thing to note is that our player can make effective use of the OK‚Äôs options and
converges very fast to a very good performance. Nevertheless, the Q-learning agent will eventually
catch-up and could possibly surpass our players‚Äô performance, as our policy for the ‚Äútrue‚Äù cumulant
induced by w‚àó = (1, 1) is possibly suboptimal. But this will require a lot more learning.
19

120

QP(3)-3

QP(3)-4
50

80

QP(3)-4

60

QP(3)-1,2

40

Q-Learning Player (8)
Q-Learning Simple Options
Q-Learning

20
0

0

200000

400000

steps

600000

800000

average episode return

average episode return

100

QP(3)-3
QP(3)-2

0

50

QP(3)-1
100

1000000

0

200000

(a) Scenario A3
1.06
1.04
1.02
1.00
0.98
0.96
0.94
2.0 0
1.5
1.0
0.5
0.0
0.5
1.0
0

400000

steps

600000

800000

1000000

(b) Scenario A4

2

4

6

8

10

2

4

6

8

10

1.0
0.5
0.0
0.5
1.0
50
4
3
2
1
0
1
0

(c) Scenario A3: di (xi )

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

(d) Scenario A4: di (xi )

Figure 7: Results on the foraging world using m = 2 nutrients and 3 types of food items: y 1 = (1, 0),
y 2 = (0, 1), and y 3 = (1, 1). Shaded regions are one standard deviation over 10 runs.

The second simple scenario we looked at is the one where one nutrient is desirable‚Äîd1 (x) > 0, ‚àÄx‚Äî
and the other one is not: d2 (x) < 0, ‚àÄx (Figure 6(d)). In this case only one of the trained options
will be useful, the one going for the nutrient that has a positive weight. But even this is one will
be suboptimal as it will pick up equally food item 1 (y 1 = (1, 0)) and 3 (y 3 = (1, 1)) although the
latter will produce no reward for the player. Moreover, sticking to this first option, which is the only
sensible policy available to QO, the player cannot avoid items of type 2 if they happen to be on their
path to collecting one of the desirable items (1 or 3). This accounts for the suboptimal level that this
player, based solely on the trained options, achieves in Figure 6(b). We can also see that the only
combined option that improved the performance of the player is w3 = (1, ‚àí1) which corresponds
exactly to the underlying reward scheme present in the environment at this time (this is akin to the
scenario discussed in Section 3.2, e1 ‚àí e2 , where the agent is encouraged to walk (e1 ) while avoiding
grasping objects (e2 )). By adding this synthesised option to the collection of primitive options, we
can see that the player Q(3)-3 achieves a considerably better asymptotic performance and maintains a
speedy convergence compared to our baseline (QL). It is also worth noting that, in the absence of any
information about the dynamics of the domain, we can opt for a range of diverse combinations, like
exemplified by QP(8), and let the player decide which of them is useful. This will mean learning
with a larger set of options, which will delay convergence. At the same time this version of the
algorithm manages to deal with both of these situations, and many more, as shown in Figures 7 and 8,
without any modification, being agnostic to the type of change in reward the player will encounter.
We hypothesise that this is representative of the most common scenario in practice, and this is why in
the main paper we focus our analysis on this scenario alone. Nevertheless, in this in-depth analysis,
we aim to understand what different combination of the same set of options would produce and in
which scenarios a player would be able to take advantage of these induced behaviours.
Next we are going to look at a slightly more interesting scenario, where the desirability function
changes over time as a function of the player‚Äôs inventory. An intuitive scenario is captured in A3
(Figure 7(c)) where for the second nutrient we will be considering a function that gives us positive
reward until the number of units of this nutrient reaches a critical level‚Äîin this particular scenario 5‚Äî,
when the reward associated with it changes sign. The first nutrient remains constant with a positive
value. We can think of the second nutrient here as something akin to certain types of food, like
‚Äùsugar‚Äù: at some point, if you have too much, it becomes undesirable to consume more. And thus you
would have to wait until the leakage rate li pushes this nutrient into a positive range before attempting
to pick it up again. Conceptually this is a combination of the scenarios we have explored previously
20

in Figure 6, but now the two situations would occur in the same episode. Results are presented in
Figure 7(a). As before, we can see that adding the synthesised option corresponding to w3 = (1, ‚àí1),
emulating the reward structure in the second part of the episode, gives us a considerable boost in
performance as compared to the primitive set W0 (QO). Moreover, we can see again that the player
converges considerably faster than the Q-learning agent which now encounters a switch in the reward
structure based on inventory. This change in the desirability function makes this scenario considerably
more challenging for the flat agent, while the player has the right level of abstraction to deal with this
problem effectively.
The fourth scenario considered in this section is a small modification of the one above, where both
nutrients have the ‚Äúsugar‚Äù profile: they both start positive and at one point become negative‚Äîsee
Figure 7(d). We consider different thresholds at which this switch happens for each nutrient, to
show that we can deal with asymmetries not present in pre-training. The results are shown in Figure
7(b). Now we can see that this small modification leads to a very different learning profile. The
first thing to notice is that the player based on only primitive options, QO, does very poorly in this
scenario. This is because this player can only act based on options that pick up items and due to
the length of our episodes (300) this player will find itself most of the time in the negative range
of these desirability functions. Moreover the player will be unable to escape this range as it will
continue to pick up items, resulting in more nutrients being added to its inventory, since these are
the only options available to it. On the other hand we can see that by considering combinations of
these options our players can do much better. In particular, given the above desirability functions,
we expect negative combinations to be helpful. And, indeed, when we add w2 , w3 or w4 to the set
of primitive options, we can see that the resulting players QP(3)-2,3,4 perform considerably better
than QO. Unsurprisingly, adding a positive-only combination, like w1 , does not help performance, as
even in the positive range this option would be suboptimal and will mimic the performance of the
primitive set (as already seen in scenario A1, Figure 6(a)). It is worth noting that in this case we are
on par with QL, but keep in mind that this scenario was chosen a bit adversarially against our OK
players. Remember this is a scenario where planning on top of the trained options alone would lead
to a very poor performance. Nevertheless we have seen that by considering combinations of options,
our OK players can achieve a substantially better performance. This is genuinely remarkable and
illustrate the power of this method in combining options in cumulant space: even if the primitive
options do not elicit a good plan, combined options synthesised on top of these primitive options can
lead to useful behaviour and near optimal performance.
Lastly, for convenience we included the two scenarios presented in the main paper, as well as their
desirability functions, in Figure 8. As in previous analysis, we include the performance of the QP(3)-i
players to illustrate how each of these combined options influences the performance. It is worth
noting that in all of these scenarios, including the ones in the main text, the same keyboard QE was
(re-)used and that player QP(8), which considers a diverse set of the proposed combinations, can
generally recover the best performance. This is an example of an agnostic player that learns to use the
most useful combined options in its set, depending on the dynamics it observes in the environment.
Moreover, in all of these scenarios we can clearly outperform or at least match the performance of
the flat agent, QL, and the agent that only uses basic options, QO. This shows that the proposed
hierarchical agent can effectively deal with structural changes in the reward function, by making use
of the combined behaviour produced by GPE and GPI (Section 2.1).
E.2

Moving-target arena

Figure 9 shows additional OK‚Äôs results on the moving-target arena as we change the length of the
options. We vary the options‚Äô lengths by changing the value of k in the definition of the cumulants (8).
As a reference, we also show the performance of flat DPG in the original continuous action space A.

References
[35] P. de Boer, D. Kroese, S. Mannor, and R. Rubinstein, A Tutorial on the Cross-Entropy Method
Annals of Operations Research, 134(1):19‚Äì67, 2005.

21

QP(3)-4

160

Q-Learning Player (9)
Q-Learning Simple Options
Q-Learning

Average Episode Return

140
120

QP(3)-3

100

QP(3)-2

80
60

QP(3)-1

40
20
0

0

200000

400000

steps

600000

800000

1000000

(a) Scenario 1

QP(3)-4

Q-Learning Player
Q-Learning Simple Options
Q-Learning

Average Episode Return

100

QP(3)-3

50

QP(3)-2
0

QP(3)-1

50

0

200000

400000

steps

600000

800000

1000000

(b) Scenario 2
5
4
3
2
1
0
1
50
4
3
2
1
0
1
0

5

10

15

20

25

30

5

10

15

20

25

30

5
4
3
2
1
0
1
50
4
3
2
1
0
1
0

(c) Scenario 1: di (xi )

5

10

15

20

25

30

5

10

15

20

25

30

(d) Scenario 2: di (xi )

Figure 8: Results on the foraging world using m = 2 nutrients and 3 types of items: y 1 = (1, 0),
y 2 = (0, 1), and y 3 = (1, 1). Shaded regions are one standard deviation over 10 runs.

22

35

Average Return Per Episode

30
25
20

Pretraining

15

Q-Learning Player (2 steps)
Q-Learning Player (4 steps)
Q-Learning Player (8 steps)
Q-Learning Player (16 steps)
DPG

10
5
0

0

1

2

steps

3

4

5
1e7

(a) Q-learning players using |W| = 8 combined options
35
30

mean_return_avg

25
20

Pretraining

15

DPG Player (2 steps)
DPG Player (4 steps)
DPG Player (8 steps)
DPG Player (16 steps)
DPG

10
5
0

0

1

2

steps

3

4

5
1e7

(b) DPG player

Figure 9: Results on the moving-target arena for options of different lengths. The number of steps
corresponds to the value of k in (8). Shaded regions are one standard deviation over 10 runs.

23

