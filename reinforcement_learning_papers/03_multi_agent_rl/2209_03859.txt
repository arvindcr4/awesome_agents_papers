1

A Survey on Large-Population Systems and
Scalable Multi-Agent Reinforcement Learning

arXiv:2209.03859v1 [cs.MA] 8 Sep 2022

Kai Cui, Anam Tahir, Gizem Ekinci, Ahmed Elshamanhory,
Yannick Eich, Mengguang Li and Heinz Koeppl

Abstract—The analysis and control of large-population systems
is of great interest to diverse areas of research and engineering,
ranging from epidemiology over robotic swarms to economics and
finance. An increasingly popular and effective approach to realizing sequential decision-making in multi-agent systems is through
multi-agent reinforcement learning, as it allows for an automatic
and model-free analysis of highly complex systems. However, the
key issue of scalability complicates the design of control and
reinforcement learning algorithms particularly in systems with
large populations of agents. While reinforcement learning has
found resounding empirical success in many scenarios with few
agents, problems with many agents quickly become intractable
and necessitate special consideration. In this survey, we will
shed light on current approaches to tractably understanding and
analyzing large-population systems, both through multi-agent
reinforcement learning and through adjacent areas of research
such as mean-field games, collective intelligence, or complex
network theory. These classically independent subject areas offer
a variety of approaches to understanding or modeling largepopulation systems, which may be of great use for the formulation
of tractable MARL algorithms in the future. Finally, we survey
potential areas of application for large-scale control and identify
fruitful future applications of learning algorithms in practical
systems. We hope that our survey could provide insight and
future directions to junior and senior researchers in theoretical
and applied sciences alike.
Index Terms—Large-scale multi-agent systems, reinforcement
learning, graph dynamical systems, mean-field games, swarm
intelligence.

I. I NTRODUCTION
N recent years, the automated design of sequential decisionmaking through reinforcement learning (RL) [1] – also
often referred to as approximate optimal control [2] – has
quite prominently found application in a varied set of application areas, ranging from complex video games [3]–[6]
over robotic systems [7], [8] such as autonomous cars [9] or
unmanned aerial vehicles (UAVs) [10]–[14] to finance [15],
[16] and recommender systems [17]. While RL remains a
very active research area of great importance, many important
practical applications consist of more than a single agent,

I

This work has been co-funded by the LOEWE initiative (Hesse, Germany)
within the emergenCITY center, the Distr@l-project BlueSwarm (Project
71585164) of the Hessian Ministry of Digital Strategy and Development, the
German Research Foundation (DFG) as part of projects B1 and C3 of the
Collaborative Research Center 1053 Multi-Mechanisms Adaptation for the
Future Internet, and the State of Hesse and HOLM as part of the "Innovations
in Logistics and Mobility" programme of the Hessian Ministry of Economics,
Energy, Transport and Housing (HA project no.: 1010/21-12).
The authors are with the Department of Electrical Engineering
and
Information
Technology,
Technische
Universität
Darmstadt, 64287 Darmstadt, Germany. (e-mail: {kai.cui,
heinz.koeppl}@tu-darmstadt.de).

thus also necessitating differentiation between a variety of
problem scenarios in between the fully-cooperative and fullycompetitive setting. Such multi-agent problems instead fall
into the subject area of so-called multi-agent reinforcement
learning (MARL) [18]. While MARL techniques sometimes
perform well empirically – typically for systems with a number
of agents in the range of few to dozens [6], [19], [20] – they
suffer from a variety of difficulties. For example, the nonuniqueness of learning goals, non-stationarity of other learning
agents, and concrete information structure of problems all
constitute potential obstacles complicating theoretical analysis
and a principled, rigorous design of MARL algorithms, see
also a multitude of prior surveys on controlling and learning
multi-agent systems [18], [21]–[23].
One of the perhaps most important practical shortcomings
is the issue of scalability to large state and action spaces in
the presence of many agents as well as to large numbers of
agents, the latter of which will constitute the main subject
of our survey. Irrespective of considering either competitive
or cooperative problems, with e.g. Nash and Pareto optima
as respective solution concepts, the control of multi-agent
systems suffers from the so-called combinatorial nature of
MARL [18], [23], sometimes also known as the curse of many
agents [24], [25]. Since the joint state and action spaces of
multi-agent systems grows exponentially with both the number
of agents and individual agent state and action spaces in the
presence of multiple agents, it has been long known that the
cooperative scenario suffers from a notorious computational
complexity issue [26]. The competitive general Nash equilibria
are similarly known to be intractable as well [27]. This issue
has conferred difficulties to multi-agent control both in the
theoretical understanding of MARL and in the principled
design of empirically effective general learning algorithms.
As a result, the complexity of general exact solutions gives
motivation to a diverse spectrum of approximate or specialized algorithms, tailored to certain subclasses of multi-agent
systems such as factorization approaches or limits of largepopulation systems discussed in this survey.
Still, large-population problems remain of significant interest to a great number of subjects in science and engineering.
In contrast to prior surveys, the motivation of our survey lies
instead in the learning of control for large-population systems
specifically, which are surprisingly ubiquitous in nature and
engineering. For examples of natural systems, see e.g. the
study of epidemics [28], robotic swarm systems [29], or
biological swarming such as in schools of fish [30], flocks
of birds [31], [32] and colonies of bacteria [33]. Accordingly,

2

we also find many use cases for large-population systems in
engineering applications, for which we point to Section IV.
Generally, such systems could be understood and studied
e.g. through the classical subjects of complex network theory
and collective intelligence, where the focus of the former
lies on decentralized systems on graphs, while for the latter
the consideration is on entities interacting through simple
localized interaction rules and nonetheless achieving global
properties without centralized knowledge of the system state
at any point [34]. In order to engineer and understand such
large-population systems, an automated and generic design
methodology via MARL is of practical interest.
In this survey, we give an account of MARL techniques and
associated methods specifically designed for scalability. We
begin by reviewing selected ideas from single-agent and multiagent reinforcement learning. We then move on to present
several recent promising approaches from the large-population
point of view. Here, we mention and very briefly give the
idea of the disjoint research areas of complex network theory,
mean-field games, and collective intelligence, many ideas of
which could be and have been useful for the current and
future development of scalable MARL. Finally, we finish by
reviewing a plentitude of applications for large-population
control. Due to the large amount of literature from the multiagent reinforcement learning community and adjacent fields,
we make no claims that our survey is exhaustive and discusses
all prior work. In particular, in this survey we shall focus
on analysis of large-scale population systems with decisionmaking mostly through the lens of learning, foregoing classical
control-theoretic approaches. Here, some differences between
reinforcement learning and control theory include the former’s
focus on automated but approximate sample-based techniques,
usually in discrete time, whereas multi-agent control theory
instead often focuses on solving specific multi-agent problems
in continuous time (e.g. formation flight [35]) with theoretical guarantees under a known model class. Note however,
that the intersection between techniques from both control
and reinforcement learning is substantial, see also e.g. [36]
for an account on reinforcement learning from the control
perspective, and [37] for a survey on control-theoretic multiagent decision-making. In the following, we will thus refer
to sequential decision-making, reinforcement learning and
control interchangeably. Overall, we try to give a high-level
overview of the discussed subject areas and avoid technical
details, which may instead be found in the variety of excellent
specialized surveys we will point towards. We hope that this
allows for accessibility of the topics, and that the presentation
of a variety of subjects related to large-population systems will
foster the exchange of ideas between mostly disjoint fields.
II. S EQUENTIAL D ECISION -M AKING
In this section we provide a short introductory review to
single-agent and multi-agent sequential decision-making, with
a focus on RL-based techniques.
A. Single-agent reinforcement learning
To begin with, sequential decision-making refers to finding a series of actions at each decision epoch in order to

maximize an objective function, typically by deciding actions
at each decision epoch depending on the current state of
the problem (closed-loop), as opposed to deciding all actions
in advance (open-loop). Therefore, in contrast to one-step
decision-making as in multi-armed bandits, where only one
decision is being made, this process considers the dynamics
of the problem. The standard framework for modeling such
problems is referred to as a Markov decision process (MDP,
[38]). MDPs consist of a state space, an action space, transition
function, and a reward function. At each time step the agent
/ decision-maker chooses an action. Based on the action and
the current state, the system moves to the next state according
to the transition probabilities. Finally, for each transition the
agent obtains numerical feedback via the reward function,
giving rise to the maximization objective of the agent in
various forms of expected cumulative rewards over future time
steps. Here, the concept of an optimal policy maximizing the
objective is introduced as the commonly desired solution of
RL algorithms. Depending on whether e.g. an infinite timehorizon discounted objective or a finite time-horizon objective
is chosen, such a policy may be a time-stationary or timevariant distribution over actions at any given state, describing
an agent’s choice of action at the current state [38].
Most of the solution methods go back to Bellman’s principle
of optimality [39]. These dynamic programming principles
introduce the concept of value functions, which describe
the optimal expected future cumulative reward that may be
achieved. With an appropriate choice of the objective such
as the infinite-horizon discounted or finite-horizon objective,
the value function will follow the so-called Bellman equation by expressing the value function in a recursive manner.
Under full knowledge of the transition and reward functions,
dynamic programming methods such as value iteration and
policy iteration solve the Bellman equation iteratively to obtain
the optimal value function, which will in turn give rise to
optimal decision-making at each time and state. These methods
conventionally exploit full knowledge of the model, and are
referred to as optimal control methods.
In contrast, reinforcement learning aims to solve MDPs
approximately and without directly accessing the transition
probabilities and reward function. This is used either to solve
problems with unknown models (e.g. when transition probabilities and reward function are defined implicitly through simulation), or to solve problems that are otherwise too complex
to solve exactly. The goal is to learn an optimal policy by trial
and error, sampling trajectories and using the corresponding
reward signal from the MDP as feedback. One of the most
well-known methods is the popular Q-Learning algorithm [40]
which performs value iteration without accessing the model,
sampling from the MDP instead. Such methods that store the
value function for each state-action pair are called tabular
methods. They remain limited to problems with small state and
action space, as the memory needed to store the values as well
as the number of state-action pairs the agent has to experience,
the sample complexity, grows quickly with increasing state and
action spaces. This effect is called the curse of dimensionality
[1] and is typically solved by techniques of modern deep
reinforcement learning.

3

Environment

Action 1

Action 2

Agent 1

Agent 2

..
.

Reward 1
Observation 1
Reward 2
Observation 1

<latexit sha1_base64="csVbf27Va8uZv6qb9x+smyfYvK8=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Ae0oWw2m3bpZjfsbgol9Ed48aCIV3+PN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJR8tUEdomkkvVC7CmnAnaNsxw2ksUxXHAaTeY3Od+d0qVZlI8mVlC/RiPBIsYwcZK3cE0lEZXhtWaW3cXQOvEK0gNCrSG1a9BKEkaU2EIx1r3PTcxfoaVYYTTeWWQappgMsEj2rdU4JhqP1ucO0cXVglRJJUtYdBC/T2R4VjrWRzYzhibsV71cvE/r5+a6NbPmEhSQwVZLopSjoxE+e8oZIoSw2eWYKKYvRWRMVaYGJtQHoK3+vI66VzVvet647FRa94VcZThDM7hEjy4gSY8QAvaQGACz/AKb07ivDjvzseyteQUM6fwB87nDwUOj18=</latexit>

Action N

Agent N

Reward N
Observation N

Fig. 1. Visualization of a general partially-observable stochastic game with
N agents, each with their own observations, rewards, and actions to take.

Here, one finds two common categories of reinforcement
learning through neural network approximations for tackling
bigger or even continuous state and action spaces. First, value
function approximation methods [41], [42] build upon tabular
methods like the aforementioned Q-Learning and attempt to
learn a parametrized approximation to the value function and
use these to obtain a policy. Meanwhile, policy-based methods
search directly over the policy space without learning the
value function for maximization of the objective. Most of
the latter algorithms are built on the policy gradient theorem
[43], [44], which formulates the derivative of the expected
cumulative reward with respect to the policy parameters. In
recent years, both methods have profited from the advances
in deep learning [45] and led to huge successes in agents
outperforming humans in board and computer games [6],
[42]. For a more comprehensive introduction to RL, see [1].
For excellent surveys on applied RL with robotics and deep
learning we refer the reader to [7] and [46] respectively.
B. Multi-agent reinforcement learning
Multi-agent reinforcement learning refers instead to sequential decision-making with multiple agents. These tasks are
more challenging than single-agent RL, as the transitions and
rewards for each agent are now additionally influenced by
other agents’ actions. MARL tasks are commonly divided into
the cooperative, competitive and mixed setting [18], [23].
In the cooperative setting, the agents work together to
reach a common goal. Cooperation is usually induced by
a common reward function that is shared by all agents. In
particular, this includes the case where each agent has its
own reward function and the common goal is to maximize
the average cumulative reward. As a consequence, at least for
full observability, the agents can be summarily coordinated
by a central decision-maker using standard single-agent RL
algorithms such as by learning a joint value function for joint
states and actions. These centralized models however suffer
from the combinatorial nature of MARL [18], [23], or curse
of many agents [24], [25], as the overall state and action
space increases exponentially with the number of agents. Apart
from the combinatorial nature, partial observability in the
presence of multiple agents leads to truly decentralized problems that cannot easily be solved by single-agent techniques.
Building on the ideas of partially-observable Markov decision

processes (POMDPs) these systems are usually modeled as
Decentralized POMDPs (Dec-POMDPs) [22]. It is known that
complexity in general is quite high for scenarios with even
just two decentralized agents [26], [47], [48]. In particular,
the general problem of Dec-POMDPs has been shown to be
NEXP-complete [26]. Exact solutions for such systems, while
possible, are therefore limited to very small systems due to
complexity constraints. Here, heuristic search methods have
been proposed in the past [49]. Other techniques include the
reformulation of multi-agent systems as certain single-agent
MDPs [50]. However, the general complexity issue remains
unchanged, fundamentally motivating the search for subclasses
of Dec-POMDPs that are more tractable to solve.
In the competitive setting, each agent has its own reward
function and acts selfishly to maximize only its own expected
cumulative reward. Typically, these tasks are modeled as zerosum games, i.e. in order for an agent to gain, another must
lose. The competition of agents is well described by the field
of game theory, and accordingly the most common solution
concept is that of a Nash equilibrium, i.e. a solution for all
agents where no agent can gain by single-handed deviating
from the solution [51]. Unfortunately, most of the literature is
focused on the two-player or two-team to achieve tractability,
which is not too interesting for large-population systems.
Finally, that leaves the third setting of mixed competitivecooperative scenarios, where in the most general case each
agent has an arbitrary but agent-unique reward function. A
common model is the partially-observable stochastic game,
where each agent may have their own observations, obtain
their own rewards and take actions in order to influence the
system state, though there are also a variety of other useful
models (cf. [18]). See also Fig. 1 for a visualization. Again,
the typical solution is the Nash equilibrium. However, similar
to the aforementioned cooperative case, the computational
complexity of Nash equilibria in the general case is high [27].
Exact solutions can be found as in the cooperative setting
via dynamic programming [52], though similarly remaining
limited to very small problem sizes due to the high complexity.
This complexity of multi-agent control is the primary reason
for the existence of a plethora of less theoretically-founded
algorithms and more specialized multi-agent system models,
each typically constrained to a subset of multi-agent problems
that are more amenable to a computational solution while
still capturing scenarios that are relevant in practice. In the
following, we will begin by pointing out a subset of common
learning-based MARL approaches. Following that, in the next
section we will provide more specialized multi-agent system
models based on graph factorizations and large-population limits. For more details on exact multi-agent control algorithms,
we point to specialized surveys [22], [47].
The most straightforward approach to extending reinforcement learning to a multi-agent system is by simply applying
single-agent reinforcement learning methods and ignoring the
multi-agent aspect of the problem. For example, under the
names of parameter sharing [53] and independent learning
[54], a single policy or value function is learned for all
assumed homogeneous agents, and by using proven singleagent RL algorithms such as proximal policy optimization

4

(PPO) [55] this has repeatedly been demonstrated to give
state-of-the-art performance on a diverse set of cooperative
multi-agent benchmark tasks [20], [56]–[58]. In particular the
parameter sharing approach is one way of handling large
numbers of agents tractably and even scale to unforeseen
or dynamic numbers of agents in a system. However, in
general convergence guarantees of approaches remain sparse,
and common benchmarks remain fixated on low numbers of
agents up to 10 [58], [20, Table 2]. One way to understand the
issue is that independent learning generates a non-stationary
single-agent problem for each agent. This leads to convergence
problems, especially e.g. when the number of agents is high.
A common approach to ameliorate some of the nonstationarity is the idea of centralized training with decentralized execution (CTDE), in which agents are trained offline
using centralized information but executed online in a decentralized manner. Since more information is shared during
training, the training phase is no longer limited by local
observations of each agent, leading to potential performance
improvements compared to fully decentralized training. Similar to standard RL, CTDE-based algorithms can be roughly
divided into approaches based on learning a policy directly,
and approaches based on learning value functions with certain
structural assumptions. For learning multi-agent value functions, Value Decomposition Networks (VDN) [59] focus on
the cooperative setting where a certain joint reward function
is to be maximized. They point out the phenomenon of the
so-called lazy agent, where one agent is active while the
other policies remain inefficient, which is resolved by learning
agent-wise value functions as an additive decomposition of
the joint value function. QMIX [60] improves upon VDN
by instead allowing any non-linear monotonic combination
of single agent value functions into the joint value function,
allowing a richer class of value functions to be represented and
improving upon state-of-the-art results in problems with up to
8 agents. Finally, in [61], two weighting functions were added
to the projection operator of QMIX, further expanding the
representation possibilities of the decomposed value function.
On the other hand, policy-based methods such as MADDPG
[62] are a popular approach for both cooperative and competitive multi-agent systems. Here, a centralized critic improves
policy gradient estimates by learning the joint value function
and taking into account the actions of the other agents, which
enables lower-variance policy-gradient estimates for complex
agent-wise coordination. Their proposed ensemble training
of policies improves the adaptability of the trained policies.
COMA [63] further adds a counterfactual baseline calculating
the advantage function for each agent using a centralized critic
for all agents, which efficiently addresses credit assignment
problem [18] in the multi-agent setting.
Finally, an orthogonal approach is to apply communication
in the context of MARL, which may improve cooperation
as part of the problem by allowing each agent to transmit relevant information to other agents. Initial works [64],
[65] propose using independent learning to learn how to
communicate while at the same time learning to cooperate
in terms of actions to solve a shared task. However, the
broadcasted messages that have been considered in these work

could hinder the decision-making by flooding the agents with
possibly irrelevant information and therefore cannot be scaled
to large-scale scenarios. As a way to prevent this, TarMAC
[66] introduces a signature-based soft attention mechanism,
where the messages are targeted to some agents using keys.
Similarly, ATOC [67] extends the actor-critic method by
an attention unit where the senders dynamically select their
collaborators from the team, and scales up to 50 agents in
both cooperative and competitive settings. [68] also proposes
to limit communication by information-theoretical regularizers
which minimizes the overall communication but maximizes
the message information. Therefore, the agents learn to act
independently most of the time and communicate when it is
crucial for cooperation.
While CTDE-based framework for solving MARL problems
show promising empirical results, especially by addressing the
non-stationary nature of the multi-agent systems, they may
still be difficult to extend to large number of agents due to the
nature of centralized critics, falling prey to the combinatorial
nature of MARL. Meanwhile, independent learning, parameter
sharing and communication-based techniques inherently scale
to many agents, but may be difficult to train. Here, the problem
of large-population MARL remains an important current and
future direction. Indeed, while there exist a variety of works
going towards large-population MARL such as [69]–[71], most
of them have in common an extensive manual engineering
of observations, policies, and especially rewards to achieve
useful results. A challenge here remains often how to solve
cooperative tasks, since intuitively in the presence of many
agents and a shared reward function, the reward learning signal
becomes increasingly noisy due to the difficulty of credit
assignment. In the next section, this survey will therefore
explore a number of approaches and subject areas related to
the analysis and learning of large-population systems.
III. L ARGE -P OPULATION S YSTEMS
While there exist a variety of important challenges in multiagent system control such as control for a variety of possible
information structures [98] or even the question of choosing an
objective – see also the existing surveys on general multi-agent
decision-making [18], [99] – developing more specialized
approaches for large-population systems seems appropriate
due to the general complexity of multi-agent control [26], [27].
For this purpose, we will give an overview of a diverse set
of subject areas concerned with specialized large-population
systems as well as their control, ranging from graph-based
factorizations and complex networks theory over mean-field
approximations to collective and swarm intelligence. Finally,
we close with a brief taxonomy on partial observability and
decentralization. For a brief overview of a selected subset of
frameworks for multi-agent control, see also Table I.
A. Graph-based methods
In order to tackle the combinatorial nature of MARL and
attain efficient and scalable algorithms, a popular direction
is to use graphs to represent sparse interactions in-between
agents, since in many practical problems, not all agents are

5

TABLE I
A SELECTED SUBSET OF RESEARCH AREAS AND RECENT ALGORITHMS OR MODELLING FRAMEWORKS FOR MULTI - AGENT SYSTEMS .
Category
RL

MARL

Framework
Q-Learning [40]
Value function approximation [41], [42]
Policy gradient methods [44], [55]
Independent Learning [54]
Parameter Sharing [53]
QMIX [60]
MADDPG [62]
COMA [63]
MAPPO [57]
Factored MDP [72]–[74]
Networked Distributed POMDP [75]–[78]
Factored Dec-POMDP [79]–[81]

Graphical
Deep coordination graphs [82]
Hyper-Graph CoNvolution MIX [83]
Collaborative graphical Bayesian game [84]

MFGs

Fixed-point iteration [85]–[87]
Fictitious Play [88], [89]
Online Mirror Descent [90], [91]
Graphical MFGs [92]–[94]
MFC MDP [95]–[97]

Methodology
Learns a tabular value function for optimal control with finite state and action spaces.
Scales to large / continuous state spaces by learning approximated value functions.
Scales to large / continuous state and action spaces by iteratively improving a policy.
Applies single-agent RL to each agent directly, ignoring the multi-agent aspect.
Scales to arbitrary numbers of agents by learning a single shared policy for all agents.
Decomposes and learns the joint-value as a monotonic combination of single-agent values.
Policy gradient method taking into account actions of other agents in a centralized critic.
Adds a counterfactual baseline for policy advantage estimation using a centralized critic.
PPO [55] via independent learning, achieves state-of-the-art results [20], [56]–[58].
Local interactions are represented as a graph, factorized transition model as DBN, reward
functions are additively decomposed into local reward functions.
Graphs represent local interactions with partial observability, transition / observation functions
are factorized agent-wise, value functions are decomposed additively into local value functions.
Partial observability combined with coordination graphs, factorized transition model as DBN,
reward function additively decomposed into local reward functions.
Hypergraphs used to represent the relationship between agents and incorporate graph convolution into the centralized training and decentralized execution structure.
Agents determine on-the-fly hyperedges with other agents, using the signals received from the
environment.
Agents can interact with different agents at every stage of the game, resulting in a non-stationary
interaction graph.
Solves contractive MFGs through application of fixed-point operator.
Dampens oscillations for non-contractive MFGs, solving potential MFGs.
Scales and speeds up fictitious play for potential MFGs.
Formulates more local MFGs for agents on graphs, interacting with neighbors.
Converts cooperative MARL into a high-dimensional single-agent MDP, circumventing the
combinatorial nature of cooperative MARL.

interacting with each other. Graphical modeling approaches
are highly general and allow one to model interaction between
agents that is significantly more sparse than in a standard
model. In the following, we will thus begin by presenting
various scalable algorithms based on graphs.
1) Factorized models: A common approach is to assume
factorized graph structures in the problem. The main idea
is to consider independence properties that transition, observation, and reward functions might have. Exploiting such
independencies, these functions can be written in the form of
smaller factors instead of functions over the high dimensional
joint action and state spaces [22]. For instance, consider a
multi-robot navigation problem. The observations of each
robot may depend only on its sensors and the local environment. Therefore the observation model can be compactly
represented as a product of smaller observation functions for
each robot. Factored models can be compactly represented
using networked graphs where the nodes and edges show
explicitly how the factors affect each other. The most straightforward way to introduce graph-based approaches to multiagent decision-making is thus to identify and exploit such
structure in the interactions of the agents a priori. In such
systems, even though the behavior of all the agents affects the
global reward, few agents directly interact with each other and
need to coordinate. These interactions can be represented as
coordination graphs (CG) [100], where each agent is a node
and the edges show their direct interactions. An example of
a coordination graph as compared to a fully connected graph

of the same population is illustrated in Fig. 2. With this graph
structure, the reward model is factorized into local smaller
reward functions. The interactions can also be represented
using a hypergraph instead of traditional graphs when pairwise
interactions are insufficient. We address these higher-order
interactions separately in Section III-A2. In the remainder
of this section, we give an overview of models exploiting
factorization via graph representations.
a) Factored MDPs: Factored MDPs are one of the
frameworks that employ graph-based factorization for scalability [72], [73]. In complex MDPs with large population,
the global state space grows exponentially in the number of
agents, therefore matrix representation of the state transition
model is impractical. Factored MDPs allow tractable representations of such MDPs by factorizing the state transition
model as a dynamic Bayesian network (DBN) [101]. This

v1

v1

<latexit sha1_base64="aE4dDuVXQZrrf0kF0tWgz+bcSOg=">AAACWXicfVBNS8NAEN3Er1q/7dHLYil4KomU6rHgxWNF+wFtKJvttF26yYbdSaGE/gSv+tvEP+OmLWIUfLDweG9mZ+aFiRQGPe/DcXd29/YPSoflo+OT07Pzi8uuUanm0OFKKt0PmQEpYuigQAn9RAOLQgm9cP6Q+70FaCNU/ILLBIKITWMxEZyhlZ4XI390XvXq3hr0L/G3pEq2aI8unNZwrHgaQYxcMmMGvpdgkDGNgktYlYepgYTxOZvCwNKYRWCCbL3ritasMqYTpe2Lka7Vnx0Zi4xZRqGtjBjOzG8vF7+9WmEUTu6DTMRJihDzzaRJKikqml9Ox0IDR7m0hHEt7LKUz5hmHG0+5RrlqUEV/f9nwc3yJVApaYo3G84kjINsBnIBWPAyFDaMlc3c/53wX9K9rfvNeuOpUW01t+mXyBW5JjfEJ3ekRR5Jm3QIJ1PySt7Iu/PpOm7JLW9KXWfbUyEFuJUvbv+10w==</latexit>

<latexit sha1_base64="aE4dDuVXQZrrf0kF0tWgz+bcSOg=">AAACWXicfVBNS8NAEN3Er1q/7dHLYil4KomU6rHgxWNF+wFtKJvttF26yYbdSaGE/gSv+tvEP+OmLWIUfLDweG9mZ+aFiRQGPe/DcXd29/YPSoflo+OT07Pzi8uuUanm0OFKKt0PmQEpYuigQAn9RAOLQgm9cP6Q+70FaCNU/ILLBIKITWMxEZyhlZ4XI390XvXq3hr0L/G3pEq2aI8unNZwrHgaQYxcMmMGvpdgkDGNgktYlYepgYTxOZvCwNKYRWCCbL3ritasMqYTpe2Lka7Vnx0Zi4xZRqGtjBjOzG8vF7+9WmEUTu6DTMRJihDzzaRJKikqml9Ox0IDR7m0hHEt7LKUz5hmHG0+5RrlqUEV/f9nwc3yJVApaYo3G84kjINsBnIBWPAyFDaMlc3c/53wX9K9rfvNeuOpUW01t+mXyBW5JjfEJ3ekRR5Jm3QIJ1PySt7Iu/PpOm7JLW9KXWfbUyEFuJUvbv+10w==</latexit>

v5

v2
(a)

v3
<latexit sha1_base64="sjZ9vaF0ns8VtjY+tAppgN4a33c=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuinoMePEY0TwgWcLspJMMmd1ZZnoDYckneNVvE3/GyQNxI1gwUFR1T3dXmEhh0PM+HXdnd2//oHBYPDo+OT0rlc/bRqWaQ4srqXQ3ZAakiKGFAiV0Ew0sCiV0wunj0u/MQBuh4lecJxBEbByLkeAMrfQyG9wMSlWv7q1A/xJ/Q6pkg+ag7DT6Q8XTCGLkkhnT870Eg4xpFFzCothPDSSMT9kYepbGLAITZKtdF7RmlSEdKW1fjHSl/u7IWGTMPAptZcRwYra9pfjj1XKjcPQQZCJOUoSYryeNUklR0eXldCg0cJRzSxjXwi5L+YRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhGGQTUDOAHNehsKGsbCZ+9sJ/yXt67p/V799vq027jbpF8gFuSRXxCf3pEGeSJO0CCdj8kbeyYfz5TpuwS2uS11n01MhObiVb3LltdU=</latexit>

v4
<latexit sha1_base64="3yHgLonKiho/vPBk0Bdxsgnoaq8=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQftobHdb/pr0DXSVCSOinxMDxx2i8jxbMYEuSSGTMI/BTDnGkUXMLSe8kMpIzP2AQGliYsBhPmq12XtGGVER0rbV+CdKX+7shZbMwijmxlzHBq/nqF+OM1KqNwfBvmIkkzhIR/TxpnkqKixeV0JDRwlAtLGNfCLkv5lGnG0ebjNSjPDKr4/z8rbl4sgUpJU73ZcCZhFOZTkHPAipejsGEsbebB34TXSfeqGVw3W4+tevu6TL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFdNi11g==</latexit>

v5

v2

<latexit sha1_base64="fr4y38yx+7fcEHxAS1EsjiGjdQE=">AAACWXicfVBNS8NAEN3Erxo/a49eFkvBU0nEr2PBi8eKVgttKJvttF3cZMPupFBCf4JX/W3in3HbBjEVfLDweG9mZ+ZFqRQGff/TcTc2t7Z3Krve3v7B4dFx9eTZqExz6HAlle5GzIAUCXRQoIRuqoHFkYSX6PVu4b9MQRuhkiecpRDGbJyIkeAMrfQ4HVwNjut+01+C/iVBQeqkQHtQdVr9oeJZDAlyyYzpBX6KYc40Ci5h7vUzAynjr2wMPUsTFoMJ8+Wuc9qwypCOlLYvQbpUf3fkLDZmFke2MmY4MeveQvzxGqVROLoNc5GkGULCV5NGmaSo6OJyOhQaOMqZJYxrYZelfMI042jz8RqUZwZV/P+fJTdfLIFKSVO+2XAmYRjmE5BTwJKXo7BhzG3mwXrCf8nzRTO4bl4+XNZb10X6FXJKzsg5CcgNaZF70iYdwsmYvJF38uF8uY5bcb1VqesUPTVSglv7BnbLtdc=</latexit>

<latexit sha1_base64="ua93kO6UiT7cJVmmscb6KtU5/RM=">AAACWXicfVBdSwJBFJ3dvsy+tB57GRKhJ9mVsB6FXnosyhR0kdnxqoOzO8vMXUEWf0Kv9duiP9NoS7QKHRg4nHPv3HtPmEhh0PM+HXdnd2//oHRYPjo+OT2rVM9fjUo1hw5XUuleyAxIEUMHBUroJRpYFErohrP7ld+dgzZCxS+4SCCI2CQWY8EZWul5PmwOKzWv4a1Bt4mfkxrJ8TisOu3BSPE0ghi5ZMb0fS/BIGMaBZewLA9SAwnjMzaBvqUxi8AE2XrXJa1bZUTHStsXI12rfzsyFhmziEJbGTGcmk1vJf569cIoHN8FmYiTFCHmP5PGqaSo6OpyOhIaOMqFJYxrYZelfMo042jzKdcpTw2q6P8/C262WgKVkqZ4s+FMwijIpiDngAUvQ2HDWNrM/c2Et8lrs+G3GjdPN7V2K0+/RC7JFbkmPrklbfJAHkmHcDIhb+SdfDhfruOW3PJPqevkPRekAPfiG3DytdQ=</latexit>

<latexit sha1_base64="fr4y38yx+7fcEHxAS1EsjiGjdQE=">AAACWXicfVBNS8NAEN3Erxo/a49eFkvBU0nEr2PBi8eKVgttKJvttF3cZMPupFBCf4JX/W3in3HbBjEVfLDweG9mZ+ZFqRQGff/TcTc2t7Z3Krve3v7B4dFx9eTZqExz6HAlle5GzIAUCXRQoIRuqoHFkYSX6PVu4b9MQRuhkiecpRDGbJyIkeAMrfQ4HVwNjut+01+C/iVBQeqkQHtQdVr9oeJZDAlyyYzpBX6KYc40Ci5h7vUzAynjr2wMPUsTFoMJ8+Wuc9qwypCOlLYvQbpUf3fkLDZmFke2MmY4MeveQvzxGqVROLoNc5GkGULCV5NGmaSo6OJyOhQaOMqZJYxrYZelfMI042jz8RqUZwZV/P+fJTdfLIFKSVO+2XAmYRjmE5BTwJKXo7BhzG3mwXrCf8nzRTO4bl4+XNZb10X6FXJKzsg5CcgNaZF70iYdwsmYvJF38uF8uY5bcb1VqesUPTVSglv7BnbLtdc=</latexit>

<latexit sha1_base64="ua93kO6UiT7cJVmmscb6KtU5/RM=">AAACWXicfVBdSwJBFJ3dvsy+tB57GRKhJ9mVsB6FXnosyhR0kdnxqoOzO8vMXUEWf0Kv9duiP9NoS7QKHRg4nHPv3HtPmEhh0PM+HXdnd2//oHRYPjo+OT2rVM9fjUo1hw5XUuleyAxIEUMHBUroJRpYFErohrP7ld+dgzZCxS+4SCCI2CQWY8EZWul5PmwOKzWv4a1Bt4mfkxrJ8TisOu3BSPE0ghi5ZMb0fS/BIGMaBZewLA9SAwnjMzaBvqUxi8AE2XrXJa1bZUTHStsXI12rfzsyFhmziEJbGTGcmk1vJf569cIoHN8FmYiTFCHmP5PGqaSo6OpyOhIaOMqFJYxrYZelfMo042jzKdcpTw2q6P8/C262WgKVkqZ4s+FMwijIpiDngAUvQ2HDWNrM/c2Et8lrs+G3GjdPN7V2K0+/RC7JFbkmPrklbfJAHkmHcDIhb+SdfDhfruOW3PJPqevkPRekAPfiG3DytdQ=</latexit>

(b)

v4
<latexit sha1_base64="3yHgLonKiho/vPBk0Bdxsgnoaq8=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQftobHdb/pr0DXSVCSOinxMDxx2i8jxbMYEuSSGTMI/BTDnGkUXMLSe8kMpIzP2AQGliYsBhPmq12XtGGVER0rbV+CdKX+7shZbMwijmxlzHBq/nqF+OM1KqNwfBvmIkkzhIR/TxpnkqKixeV0JDRwlAtLGNfCLkv5lGnG0ebjNSjPDKr4/z8rbl4sgUpJU73ZcCZhFOZTkHPAipejsGEsbebB34TXSfeqGVw3W4+tevu6TL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFdNi11g==</latexit>

v3
<latexit sha1_base64="sjZ9vaF0ns8VtjY+tAppgN4a33c=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuinoMePEY0TwgWcLspJMMmd1ZZnoDYckneNVvE3/GyQNxI1gwUFR1T3dXmEhh0PM+HXdnd2//oHBYPDo+OT0rlc/bRqWaQ4srqXQ3ZAakiKGFAiV0Ew0sCiV0wunj0u/MQBuh4lecJxBEbByLkeAMrfQyG9wMSlWv7q1A/xJ/Q6pkg+ag7DT6Q8XTCGLkkhnT870Eg4xpFFzCothPDSSMT9kYepbGLAITZKtdF7RmlSEdKW1fjHSl/u7IWGTMPAptZcRwYra9pfjj1XKjcPQQZCJOUoSYryeNUklR0eXldCg0cJRzSxjXwi5L+YRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhGGQTUDOAHNehsKGsbCZ+9sJ/yXt67p/V799vq027jbpF8gFuSRXxCf3pEGeSJO0CCdj8kbeyYfz5TpuwS2uS11n01MhObiVb3LltdU=</latexit>

Fig. 2. Visualization of (a) a fully connected graph of a system of 5 agents
labeled as vi and (b) a coordination graph depicting local interactions in the
system. The coordination graph allows factorization of the reward function
into local factors and provides tractable solutions.

6

graph also represents the local interactions by the childparent relations and can be interpreted as a coordination
graph. Another assumption is that the reward function can
be factored additively into localized reward functions which
leads to more compact representations. In factored MDPs,
similar approaches as in the case of MARL are proposed
for decision-making. Authors in [72] show that the global Qfunction can be additively decomposed over local functions
representing the underlying graph in factored MDPs. The
corresponding cost network is solved using variable elimination. This reduces the complexity to be exponential only in
the maximum number of interactions between agents instead
of the number of agents. As an alternative, the authors in
[74] propose payoff propagation algorithm (max-plus), that is
analogous to belief propagation in Bayesian networks. They
further introduce Sparse Cooperative Q-learning (SparseQ)
[102], where the agent- or edge-based local Q-functions are
updated considering its contribution to the global payoff. [73]
proposes approximate linear programming and dynamic programming algorithms for factored MDPs, which are also based
on variable elimination. Their dynamic programming approach
shows theoretical guarantees in terms of error minimization,
while the approximate linear programming algorithm is more
general in terms of its assumptions. For large factored MDPs,
the idea of “anonymous influence” is introduced in the paper
[103]. In graph-based problems, they focus on the influence
of variable sets rather than the identity of the variables.
Exploiting this anonymity, they manage to scale up variable
elimination and approximate linear programming. We note that
this anonymity is a recurring theme and is also used e.g. in the
formulation of mean-field models in Section III-B. The authors
in [104] consider planning via inference for graph-based
MDPs which is equivalent to factored MDP with individual
agent policies. They propose an approximate method based on
variational perturbation theory and attain a linear complexity
in the number of agents instead of exponential, which can be
scaled to larger population systems.
b) Partially-observed models: Networked Distributed
POMDPs (ND-POMDPs) [75] are one of the frameworks
that bring partial observability and coordination graphs together. The transition and observation functions are agentwise factorized, and the reward function can be additively
decomposed to local reward functions. In addition, it allows
a set of unaffectable environment states, which cannot be
manipulated by any agents. It is shown that, in this model,
the value function can also be decomposed into local value
functions. This decomposition also implies the locality of
interactions, that is, an agent should not affect the policy of
other agents unless they are neighbors. Exploiting the locality
of interactions, a number of constraint optimization problem
(COP) methods can be applied effectively. Authors in [75]
propose to optimize each agent’s policy with respect to its
neighbors’ policies in a distributed manner and propagate the
optimal childrens’ policies through parents in a tree-structured
network. [76] extends this to update the policies stochastically,
allowing neighboring agents to change their policies at the
same time and speeds up the convergence. Authors in [77]
consider the planning problem as an inference problem and,

using an expectation-maximization (EM) algorithm, achieve a
scalable approximation for multi-agents by decomposing the
global inference problem into parts involving subsets of agents.
The authors in [78] extend ND-POMDP with communication
between neighbors based on the coordination graph. This
allows distributed learning of the joint policy, while ensuring
the global convergence and scaling the learning complexity
potentially linearly to the number of agents. It should be noted
however, that in ND-POMDPs, the assumptions that lead to
value function decomposition and therefore efficient solutions,
also limit the applicability of the framework.
Factored Dec-POMDPs constitute another framework that
represents graph-based multi-agent systems and considers partial observability. Analogous to factored MDPs, the transition
and observation model can be represented by a DBN and the
reward function can be factored to local reward functions of
smaller sets of agents. Since it does not impose agent-wise
transition and observation independence as ND-POMDP, this
framework is more flexible and can be applied to a wide
range of problems. [79] proposes transfer planning which
decomposes the problem into smaller tasks and uses the value
function of those tasks as components of the factored value
function of the main problem. Although this approach does
not scale well with the time horizon, it shows promising
results scaling up to 1000 agents. Finally, [80], [81] propose
extensions to the EM algorithm for planning in factored
infinite-horizon DEC-POMDPs.
c) Other recent scalable methods: Some recent works
[105]–[107] use an underlying graph structure of agents or
states and apply the correlation decay method [108] to design
scalable, localized Q-functions for each agent, which only
depend on the local states, actions, and rewards of the agent
itself and its k-hop neighbors. They propose the Scalable Actor
Critic algorithm (SAC) which exploits these factorizations
to learn a localized policy for each agent via approximated
gradients. Another set of algorithms that make use of graphbased factorization are the distributed/consensus algorithms,
which have been extensively studied to solve one-stage static
optimization problems in a non-sequential manner. In these
models an underlying graph is used to communicate the
policy gradient estimate of each agent to the others in the
network to form a consensus. We refer the reader to [109]–
[116] for further results in this direction. RL for graph-based
factorization has also been used for multi-task systems, where
each agent has its own independent MDP but the goal is
to have a joint optimal policy in the end. In other words,
each agent has access to local tasks but wants to learn a
global policy. In [117], the authors assume states, actions, and
rewards to be local to each agent and then propose a distributed
actor-critic algorithm, which is a combination of TD-step
and consensus step. The authors in [118] use a time-varying
communication network to enable the agents to take local
actions only based on their local information and messages
received from the neighbors, such that the joint reward is
maximized. Their proposed solution is to use a decentralized
actor-critic framework with function approximation for each
agent. A similar solution is proposed in [119] for continuous
spaces, and solved using expected policy gradients [120].

7

v7

ge
pered

4-hy

<latexit sha1_base64="Q1btXOQmyYLRBbEK34U+rVp8WZE=">AAACWXicfVBNS8NAEN3Er1q/9ehlsRQ8lUSkeix48ahoP6CGstlM2qWbbNidFEroT/Cqv038M25qEWPBBwuP92Z2Zl6YSWHQ8z4cd2Nza3untlvf2z84PDo+Oe0ZlWsOXa6k0oOQGZAihS4KlDDINLAklNAPp3el35+BNkKlzzjPIEjYOBWx4Ayt9DQb3YyOG17LW4KuE39FGmSFh9GJ03mJFM8TSJFLZszQ9zIMCqZRcAmL+ktuIGN8ysYwtDRlCZigWO66oE2rRDRW2r4U6VL93VGwxJh5EtrKhOHE/PVK8cdrVkZhfBsUIs1yhJR/T4pzSVHR8nIaCQ0c5dwSxrWwy1I+YZpxtPnUm5TnBlXy/58VtyiXQKWkqd5sOJMQBcUE5Ayw4hUobBgLm7n/N+F10rtq+e3W9eN1o9NepV8j5+SCXBKf3JAOuScPpEs4GZNX8kbenU/XcWtu/bvUdVY9Z6QC9+wLerG12Q==</latexit>

v1
<latexit sha1_base64="aE4dDuVXQZrrf0kF0tWgz+bcSOg=">AAACWXicfVBNS8NAEN3Er1q/7dHLYil4KomU6rHgxWNF+wFtKJvttF26yYbdSaGE/gSv+tvEP+OmLWIUfLDweG9mZ+aFiRQGPe/DcXd29/YPSoflo+OT07Pzi8uuUanm0OFKKt0PmQEpYuigQAn9RAOLQgm9cP6Q+70FaCNU/ILLBIKITWMxEZyhlZ4XI390XvXq3hr0L/G3pEq2aI8unNZwrHgaQYxcMmMGvpdgkDGNgktYlYepgYTxOZvCwNKYRWCCbL3ritasMqYTpe2Lka7Vnx0Zi4xZRqGtjBjOzG8vF7+9WmEUTu6DTMRJihDzzaRJKikqml9Ox0IDR7m0hHEt7LKUz5hmHG0+5RrlqUEV/f9nwc3yJVApaYo3G84kjINsBnIBWPAyFDaMlc3c/53wX9K9rfvNeuOpUW01t+mXyBW5JjfEJ3ekRR5Jm3QIJ1PySt7Iu/PpOm7JLW9KXWfbUyEFuJUvbv+10w==</latexit>

v5

v1
<latexit sha1_base64="aE4dDuVXQZrrf0kF0tWgz+bcSOg=">AAACWXicfVBNS8NAEN3Er1q/7dHLYil4KomU6rHgxWNF+wFtKJvttF26yYbdSaGE/gSv+tvEP+OmLWIUfLDweG9mZ+aFiRQGPe/DcXd29/YPSoflo+OT07Pzi8uuUanm0OFKKt0PmQEpYuigQAn9RAOLQgm9cP6Q+70FaCNU/ILLBIKITWMxEZyhlZ4XI390XvXq3hr0L/G3pEq2aI8unNZwrHgaQYxcMmMGvpdgkDGNgktYlYepgYTxOZvCwNKYRWCCbL3ritasMqYTpe2Lka7Vnx0Zi4xZRqGtjBjOzG8vF7+9WmEUTu6DTMRJihDzzaRJKikqml9Ox0IDR7m0hHEt7LKUz5hmHG0+5RrlqUEV/f9nwc3yJVApaYo3G84kjINsBnIBWPAyFDaMlc3c/53wX9K9rfvNeuOpUW01t+mXyBW5JjfEJ3ekRR5Jm3QIJ1PySt7Iu/PpOm7JLW9KXWfbUyEFuJUvbv+10w==</latexit>

<latexit sha1_base64="fr4y38yx+7fcEHxAS1EsjiGjdQE=">AAACWXicfVBNS8NAEN3Erxo/a49eFkvBU0nEr2PBi8eKVgttKJvttF3cZMPupFBCf4JX/W3in3HbBjEVfLDweG9mZ+ZFqRQGff/TcTc2t7Z3Krve3v7B4dFx9eTZqExz6HAlle5GzIAUCXRQoIRuqoHFkYSX6PVu4b9MQRuhkiecpRDGbJyIkeAMrfQ4HVwNjut+01+C/iVBQeqkQHtQdVr9oeJZDAlyyYzpBX6KYc40Ci5h7vUzAynjr2wMPUsTFoMJ8+Wuc9qwypCOlLYvQbpUf3fkLDZmFke2MmY4MeveQvzxGqVROLoNc5GkGULCV5NGmaSo6OJyOhQaOMqZJYxrYZelfMI042jz8RqUZwZV/P+fJTdfLIFKSVO+2XAmYRjmE5BTwJKXo7BhzG3mwXrCf8nzRTO4bl4+XNZb10X6FXJKzsg5CcgNaZF70iYdwsmYvJF38uF8uY5bcb1VqesUPTVSglv7BnbLtdc=</latexit>

<latexit sha1_base64="n5UNqdgsLbnK66R1aFMrsBqrhEs=">AAACWXicfVFNSwMxEJ1dv2r91qOXYCl4KrtS1GPBi0dFq4IuJZtO22B2sySzhbL05Nmrnv0nnv0H4p8xrUVcBQcCj/eSzJs3caakpSB49/y5+YXFpcpydWV1bX1jc2v7yurcCGwLrbS5iblFJVNskySFN5lBnsQKr+P7k4l+PURjpU4vaZRhlPB+KntScHLUxbBz2NmsBY1gWuwvCGegBrM662x5rbuuFnmCKQnFrb0Ng4yighuSQuG4epdbzLi45328dTDlCdqomHods7pjuqynjTspsSn780XBE2tHSexuJpwG9rc2Ib+1eqkV9Y6jQqZZTpiKr069XDHSbDI560qDgtTIAS6MdGaZGHDDBbl8qnUmcks6+f/PklpMTJDWypZntoIr7EbFANUQqaQVJF0YY5d5+Dvhv+DqoBEeNprnzVqrufDy9vqwDRXYhT3YhxCOoAWncAZtENCHR3iCZ+/D9/yKX/1alO/NNrYDpfJ3PgFD1Lib</latexit>

v3

e

v6

dg
pere

<latexit sha1_base64="3yHgLonKiho/vPBk0Bdxsgnoaq8=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQftobHdb/pr0DXSVCSOinxMDxx2i8jxbMYEuSSGTMI/BTDnGkUXMLSe8kMpIzP2AQGliYsBhPmq12XtGGVER0rbV+CdKX+7shZbMwijmxlzHBq/nqF+OM1KqNwfBvmIkkzhIR/TxpnkqKixeV0JDRwlAtLGNfCLkv5lGnG0ebjNSjPDKr4/z8rbl4sgUpJU73ZcCZhFOZTkHPAipejsGEsbebB34TXSfeqGVw3W4+tevu6TL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFdNi11g==</latexit>

<latexit sha1_base64="ua93kO6UiT7cJVmmscb6KtU5/RM=">AAACWXicfVBdSwJBFJ3dvsy+tB57GRKhJ9mVsB6FXnosyhR0kdnxqoOzO8vMXUEWf0Kv9duiP9NoS7QKHRg4nHPv3HtPmEhh0PM+HXdnd2//oHRYPjo+OT2rVM9fjUo1hw5XUuleyAxIEUMHBUroJRpYFErohrP7ld+dgzZCxS+4SCCI2CQWY8EZWul5PmwOKzWv4a1Bt4mfkxrJ8TisOu3BSPE0ghi5ZMb0fS/BIGMaBZewLA9SAwnjMzaBvqUxi8AE2XrXJa1bZUTHStsXI12rfzsyFhmziEJbGTGcmk1vJf569cIoHN8FmYiTFCHmP5PGqaSo6OpyOhIaOMqFJYxrYZelfMo042jzKdcpTw2q6P8/C262WgKVkqZ4s+FMwijIpiDngAUvQ2HDWNrM/c2Et8lrs+G3GjdPN7V2K0+/RC7JFbkmPrklbfJAHkmHcDIhb+SdfDhfruOW3PJPqevkPRekAPfiG3DytdQ=</latexit>

v4
<latexit sha1_base64="3yHgLonKiho/vPBk0Bdxsgnoaq8=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQftobHdb/pr0DXSVCSOinxMDxx2i8jxbMYEuSSGTMI/BTDnGkUXMLSe8kMpIzP2AQGliYsBhPmq12XtGGVER0rbV+CdKX+7shZbMwijmxlzHBq/nqF+OM1KqNwfBvmIkkzhIR/TxpnkqKixeV0JDRwlAtLGNfCLkv5lGnG0ebjNSjPDKr4/z8rbl4sgUpJU73ZcCZhFOZTkHPAipejsGEsbebB34TXSfeqGVw3W4+tevu6TL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFdNi11g==</latexit>

v6
<latexit sha1_base64="n5UNqdgsLbnK66R1aFMrsBqrhEs=">AAACWXicfVFNSwMxEJ1dv2r91qOXYCl4KrtS1GPBi0dFq4IuJZtO22B2sySzhbL05Nmrnv0nnv0H4p8xrUVcBQcCj/eSzJs3caakpSB49/y5+YXFpcpydWV1bX1jc2v7yurcCGwLrbS5iblFJVNskySFN5lBnsQKr+P7k4l+PURjpU4vaZRhlPB+KntScHLUxbBz2NmsBY1gWuwvCGegBrM662x5rbuuFnmCKQnFrb0Ng4yighuSQuG4epdbzLi45328dTDlCdqomHods7pjuqynjTspsSn780XBE2tHSexuJpwG9rc2Ib+1eqkV9Y6jQqZZTpiKr069XDHSbDI560qDgtTIAS6MdGaZGHDDBbl8qnUmcks6+f/PklpMTJDWypZntoIr7EbFANUQqaQVJF0YY5d5+Dvhv+DqoBEeNprnzVqrufDy9vqwDRXYhT3YhxCOoAWncAZtENCHR3iCZ+/D9/yKX/1alO/NNrYDpfJ3PgFD1Lib</latexit>

v2
<latexit sha1_base64="ua93kO6UiT7cJVmmscb6KtU5/RM=">AAACWXicfVBdSwJBFJ3dvsy+tB57GRKhJ9mVsB6FXnosyhR0kdnxqoOzO8vMXUEWf0Kv9duiP9NoS7QKHRg4nHPv3HtPmEhh0PM+HXdnd2//oHRYPjo+OT2rVM9fjUo1hw5XUuleyAxIEUMHBUroJRpYFErohrP7ld+dgzZCxS+4SCCI2CQWY8EZWul5PmwOKzWv4a1Bt4mfkxrJ8TisOu3BSPE0ghi5ZMb0fS/BIGMaBZewLA9SAwnjMzaBvqUxi8AE2XrXJa1bZUTHStsXI12rfzsyFhmziEJbGTGcmk1vJf569cIoHN8FmYiTFCHmP5PGqaSo6OpyOhIaOMqFJYxrYZelfMo042jzKdcpTw2q6P8/C262WgKVkqZ4s+FMwijIpiDngAUvQ2HDWNrM/c2Et8lrs+G3GjdPN7V2K0+/RC7JFbkmPrklbfJAHkmHcDIhb+SdfDhfruOW3PJPqevkPRekAPfiG3DytdQ=</latexit>

v3
<latexit sha1_base64="sjZ9vaF0ns8VtjY+tAppgN4a33c=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuinoMePEY0TwgWcLspJMMmd1ZZnoDYckneNVvE3/GyQNxI1gwUFR1T3dXmEhh0PM+HXdnd2//oHBYPDo+OT0rlc/bRqWaQ4srqXQ3ZAakiKGFAiV0Ew0sCiV0wunj0u/MQBuh4lecJxBEbByLkeAMrfQyG9wMSlWv7q1A/xJ/Q6pkg+ag7DT6Q8XTCGLkkhnT870Eg4xpFFzCothPDSSMT9kYepbGLAITZKtdF7RmlSEdKW1fjHSl/u7IWGTMPAptZcRwYra9pfjj1XKjcPQQZCJOUoSYryeNUklR0eXldCg0cJRzSxjXwi5L+YRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhGGQTUDOAHNehsKGsbCZ+9sJ/yXt67p/V799vq027jbpF8gFuSRXxCf3pEGeSJO0CCdj8kbeyYfz5TpuwS2uS11n01MhObiVb3LltdU=</latexit>

<latexit sha1_base64="sjZ9vaF0ns8VtjY+tAppgN4a33c=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuinoMePEY0TwgWcLspJMMmd1ZZnoDYckneNVvE3/GyQNxI1gwUFR1T3dXmEhh0PM+HXdnd2//oHBYPDo+OT0rlc/bRqWaQ4srqXQ3ZAakiKGFAiV0Ew0sCiV0wunj0u/MQBuh4lecJxBEbByLkeAMrfQyG9wMSlWv7q1A/xJ/Q6pkg+ag7DT6Q8XTCGLkkhnT870Eg4xpFFzCothPDSSMT9kYepbGLAITZKtdF7RmlSEdKW1fjHSl/u7IWGTMPAptZcRwYra9pfjj1XKjcPQQZCJOUoSYryeNUklR0eXldCg0cJRzSxjXwi5L+YRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhGGQTUDOAHNehsKGsbCZ+9sJ/yXt67p/V799vq027jbpF8gFuSRXxCf3pEGeSJO0CCdj8kbeyYfz5TpuwS2uS11n01MhObiVb3LltdU=</latexit>

(a)

<latexit sha1_base64="Q1btXOQmyYLRBbEK34U+rVp8WZE=">AAACWXicfVBNS8NAEN3Er1q/9ehlsRQ8lUSkeix48ahoP6CGstlM2qWbbNidFEroT/Cqv038M25qEWPBBwuP92Z2Zl6YSWHQ8z4cd2Nza3untlvf2z84PDo+Oe0ZlWsOXa6k0oOQGZAihS4KlDDINLAklNAPp3el35+BNkKlzzjPIEjYOBWx4Ayt9DQb3YyOG17LW4KuE39FGmSFh9GJ03mJFM8TSJFLZszQ9zIMCqZRcAmL+ktuIGN8ysYwtDRlCZigWO66oE2rRDRW2r4U6VL93VGwxJh5EtrKhOHE/PVK8cdrVkZhfBsUIs1yhJR/T4pzSVHR8nIaCQ0c5dwSxrWwy1I+YZpxtPnUm5TnBlXy/58VtyiXQKWkqd5sOJMQBcUE5Ayw4hUobBgLm7n/N+F10rtq+e3W9eN1o9NepV8j5+SCXBKf3JAOuScPpEs4GZNX8kbenU/XcWtu/bvUdVY9Z6QC9+wLerG12Q==</latexit>

<latexit sha1_base64="fr4y38yx+7fcEHxAS1EsjiGjdQE=">AAACWXicfVBNS8NAEN3Erxo/a49eFkvBU0nEr2PBi8eKVgttKJvttF3cZMPupFBCf4JX/W3in3HbBjEVfLDweG9mZ+ZFqRQGff/TcTc2t7Z3Krve3v7B4dFx9eTZqExz6HAlle5GzIAUCXRQoIRuqoHFkYSX6PVu4b9MQRuhkiecpRDGbJyIkeAMrfQ4HVwNjut+01+C/iVBQeqkQHtQdVr9oeJZDAlyyYzpBX6KYc40Ci5h7vUzAynjr2wMPUsTFoMJ8+Wuc9qwypCOlLYvQbpUf3fkLDZmFke2MmY4MeveQvzxGqVROLoNc5GkGULCV5NGmaSo6OJyOhQaOMqZJYxrYZelfMI042jz8RqUZwZV/P+fJTdfLIFKSVO+2XAmYRjmE5BTwJKXo7BhzG3mwXrCf8nzRTO4bl4+XNZb10X6FXJKzsg5CcgNaZF70iYdwsmYvJF38uF8uY5bcb1VqesUPTVSglv7BnbLtdc=</latexit>

2-hy

v4
v2

v7

v5

(b)

3-hyperedge

Fig. 3. Comparison between a simple graph and a hypergraph, vi is the node
(vertex) label, whereas circles and lines constitute the hyperedges between
involved nodes. (a) Simple graph example. (b) Hypergraph example.

As we have pointed out, there is a large number of models
and solution methods that take advantage of graph-based
factorization, and many more we could not cover in this
survey. Although these models have great theoretical potential
to improve the scalability and it is acknowledged so, we note
that the empirical results presented in most works still remain
limited to smaller systems. Hence, we conclude that there is
a great deal to be explored and investigated in this area and
we imagine this would result in many new approaches and
extensions to the existing ones.
2) Complex network models: Apart from the diverse variety
of approximate and exact control algorithms for large systems
based on graph structure, the very active study of complex
networks [121] as well as dynamical processes on such complex networks [122], [123] concerns itself with understanding
real-world phenomena through graph-based models with nontrivial structure. Except for mean-field approximations on
large graphs – which we will discuss also in their nongraphical version in Section III-B – many works consider
formal or rigorous analyses of a variety of graph properties and
dynamical processes on graphs such as epidemics, magnetism
or opinion dynamics. For example, common subject matters of
complex network theory are small-world phenomena of graphs
[121], percolation theory considering connectivity of graphs
[122] or phase transitions and synchronization in dynamical
models used in statistical physics [123]. Since the theory of
complex networks has long been focused on understanding
complex large-population systems on graphs, it seems natural
that similar ideas would help in learning and controlling such
large-population systems.
In particular, we begin by pointing out an important emergent area of research dealing with higher-order interactions
beyond simple pairwise interaction. In recent years there has
been a great surge in interest in such higher-order dynamical models, e.g. for epidemiology [124], [125] or opinion
dynamics [126], [127]. For example, in social networks it
is natural to consider interactions that are inherently groupbased and lead beyond pairwise relations. On the topic of
higher-order interaction, there exist various excellent reviews
of common modeling methods [128]–[130], which we refer the
reader to. One of the most general ways of realizing higherorder interaction in a graphical manner is through hypergraphs,
which consist of vertices and hyperedges [131]. In contrast to
standard graphs, here hyperedges are allowed to connect more
than two nodes. See also Figure 3 for a visualization. Here, a
great number of existing models from complex network theory
could be explored to further improve the generality of existing

frameworks and appropriately model higher-order interactive
systems, which may well become increasingly important (e.g.
for large-scale social networks, computer networks etc.). We
thus imagine that future work could deal with higher-order
interactions, be it in directed or undirected graphs, weighted
or unweighted graphs and so on.
On the topic of controlled hypergraphical systems, initial
works such as DGN [132] have already made use of graph
neural networks to exploit the interactions between agents.
[82] present the framework Deep coordination graphs (DCG),
which shows that using hypergraphs can further explore the
representation of the relationship between agents. All these
studies take the topological structure among agents into account by incorporating the idea of graph convolution into
the centralized training and decentralized execution structure,
and manually define the graph edges. Whereas, in HyperGraph CoNvolution MIX (HGCN-MIX) [83], the agent uses
the environmental signals it receives to determine on-the-fly
the connections, and create hypergraphs, between itself and
other agents. [133] also use hypergraphs to model higherorder interaction between a group of players in order to
learn cooperative or competitive strategies in synchronization
(symmetric hedonic) games. A hypergraph-adapted actor-critic
framework has also been proposed recently in [134] to employ hypergraph convolution to the centralized training with
decentralized execution paradigm. Based on the interaction
structure of agents and the resulting hypergraphs they propose
two novel methods for efficient information feature extraction
and representation, thus leading to better collaboration policies
of agents. They have used various multi-agent games to show
the performance improvement of their models because of the
extraction and use of higher-order interactions. In [135], the
authors presented a multi-agent interacting system in the form
of a game and then learned its equilibria for the optimal
strategy. Based on the amount of information available to
each agent and the communication level, the authors learned
the optimal strategy of the game using numerous approaches
such as fictitious play, smooth fictitious play, regret matching,
reinforcement learning, etc.
Lastly, of great recent interest are also dynamical (adaptive) graph systems instead of dynamical systems on graphs,
since in practice it is seldom the case that networks remain
static and unchanging over time, see Fig. 4 for an exemplary visualization of both changing agents in the system
and changing interactions between agents. The authors in
[136] give a nice introduction to the concepts and properties
of adaptive networks. They also emphasize its importance
using applications which include but are not limited to social
networks, transportation networks, neural networks, and biological networks. In order to be more realistic and applicable to
real-world scenarios, multi-agent systems not only need to be
adaptive to the changing environment but also to the changing
interactions, actions, and connections in the system. A recent
survey [137] explains this need and highlights the existing
techniques from the point of view of smart cities and Internetof-Things. Another survey [138] presents an overview of the
various distributed strategies which enable connected agents to
interact locally and learn and adapt to changing traffic over the

8

<latexit sha1_base64="xnNb+Wd27ensFvvMBImDhIIc4jo=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbOgPS1Wv7q1Ad4m/IVWyQWtYdpqDkeJpBDFyyYzp+16CQcY0Ci5hURykBhLG39gE+pbGLAITZKtdF7RmlREdK21fjHSl/u3IWGTMPAptZcRwara9pfjr1XKjcPwQZCJOUoSYryeNU0lR0eXldCQ0cJRzSxjXwi5L+ZRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhFGQTUHOAHNehsKGsbCZ+9sJ75LObd2/qzeeG9VmY5N+gVyRa3JDfHJPmuSJtEibcDIh7+SDfDrfruMW3OK61HU2PRWSg1v5AW5ltdE=</latexit>

v1

v1

v1

<latexit sha1_base64="xnNb+Wd27ensFvvMBImDhIIc4jo=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbOgPS1Wv7q1Ad4m/IVWyQWtYdpqDkeJpBDFyyYzp+16CQcY0Ci5hURykBhLG39gE+pbGLAITZKtdF7RmlREdK21fjHSl/u3IWGTMPAptZcRwara9pfjr1XKjcPwQZCJOUoSYryeNU0lR0eXldCQ0cJRzSxjXwi5L+ZRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhFGQTUHOAHNehsKGsbCZ+9sJ75LObd2/qzeeG9VmY5N+gVyRa3JDfHJPmuSJtEibcDIh7+SDfDrfruMW3OK61HU2PRWSg1v5AW5ltdE=</latexit>

<latexit sha1_base64="xnNb+Wd27ensFvvMBImDhIIc4jo=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbOgPS1Wv7q1Ad4m/IVWyQWtYdpqDkeJpBDFyyYzp+16CQcY0Ci5hURykBhLG39gE+pbGLAITZKtdF7RmlREdK21fjHSl/u3IWGTMPAptZcRwara9pfjr1XKjcPwQZCJOUoSYryeNU0lR0eXldCQ0cJRzSxjXwi5L+ZRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhFGQTUHOAHNehsKGsbCZ+9sJ75LObd2/qzeeG9VmY5N+gVyRa3JDfHJPmuSJtEibcDIh7+SDfDrfruMW3OK61HU2PRWSg1v5AW5ltdE=</latexit>

<latexit sha1_base64="xnNb+Wd27ensFvvMBImDhIIc4jo=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbOgPS1Wv7q1Ad4m/IVWyQWtYdpqDkeJpBDFyyYzp+16CQcY0Ci5hURykBhLG39gE+pbGLAITZKtdF7RmlREdK21fjHSl/u3IWGTMPAptZcRwara9pfjr1XKjcPwQZCJOUoSYryeNU0lR0eXldCQ0cJRzSxjXwi5L+ZRpxtHmU6xRnhpU0f9/5txsuQQqJU3+ZsOZhFGQTUHOAHNehsKGsbCZ+9sJ75LObd2/qzeeG9VmY5N+gVyRa3JDfHJPmuSJtEibcDIh7+SDfDrfruMW3OK61HU2PRWSg1v5AW5ltdE=</latexit>

v5

<latexit sha1_base64="EBwbAFbU8n87tNRwcFTAcX2wTa8=">AAACWXicfVBNS8NAEN3Er1q/qj16WSwFTyWR+nEsePGoaFXQUDabSbt0kw27k0IJ/Qle9beJf8ZNW8RY8MHC472ZnZkXZlIY9LxPx11b39jcqm3Xd3b39g8ah0ePRuWaQ58rqfRzyAxIkUIfBUp4zjSwJJTwFI6vS/9pAtoIlT7gNIMgYcNUxIIztNL9ZHA+aLS8jjcHXSX+krTIEreDQ6f3GimeJ5Ail8yYF9/LMCiYRsElzOqvuYGM8TEbwoulKUvABMV81xltWyWisdL2pUjn6u+OgiXGTJPQViYMR+avV4o/XrsyCuOroBBpliOkfDEpziVFRcvLaSQ0cJRTSxjXwi5L+YhpxtHmU29TnhtUyf9/VtyiXAKVkqZ6s+FMQhQUI5ATwIpXoLBhzGzm/t+EV8njWce/6HTvuq1ed5l+jRyTE3JKfHJJeuSG3JI+4WRI3sg7+XC+XMetufVFqesse5qkArf5DXYxtdU=</latexit>

<latexit sha1_base64="EBwbAFbU8n87tNRwcFTAcX2wTa8=">AAACWXicfVBNS8NAEN3Er1q/qj16WSwFTyWR+nEsePGoaFXQUDabSbt0kw27k0IJ/Qle9beJf8ZNW8RY8MHC472ZnZkXZlIY9LxPx11b39jcqm3Xd3b39g8ah0ePRuWaQ58rqfRzyAxIkUIfBUp4zjSwJJTwFI6vS/9pAtoIlT7gNIMgYcNUxIIztNL9ZHA+aLS8jjcHXSX+krTIEreDQ6f3GimeJ5Ail8yYF9/LMCiYRsElzOqvuYGM8TEbwoulKUvABMV81xltWyWisdL2pUjn6u+OgiXGTJPQViYMR+avV4o/XrsyCuOroBBpliOkfDEpziVFRcvLaSQ0cJRTSxjXwi5L+YhpxtHmU29TnhtUyf9/VtyiXAKVkqZ6s+FMQhQUI5ATwIpXoLBhzGzm/t+EV8njWce/6HTvuq1ed5l+jRyTE3JKfHJJeuSG3JI+4WRI3sg7+XC+XMetufVFqesse5qkArf5DXYxtdU=</latexit>

v2

v2

v2
v6

v4
v3
<latexit sha1_base64="O3X2AAfALvRVSAkaNx04xKP9omc=">AAACWXicfVBNS8NAEN3Erxq/qj16WSwFTyXRoh4LXjwqWhU0lM122i7dZMPupFBCf4JX/W3in3HTBjEWfLDweG9mZ+ZFqRQGff/TcdfWNza3atvezu7e/kH98OjRqExz6HEllX6OmAEpEuihQAnPqQYWRxKeosl14T9NQRuhkgecpRDGbJSIoeAMrXQ/7Z/3602/7S9AV0lQkiYpcds/dLqvA8WzGBLkkhnzEvgphjnTKLiEufeaGUgZn7ARvFiasBhMmC92ndOWVQZ0qLR9CdKF+rsjZ7ExsziylTHDsfnrFeKP16qMwuFVmIskzRASvpw0zCRFRYvL6UBo4ChnljCuhV2W8jHTjKPNx2tRnhlU8f9/Vty8WAKVkqZ6s+FMwiDMxyCngBUvR2HDmNvMg78Jr5LHs3Zw0e7cdZrdTpl+jRyTE3JKAnJJuuSG3JIe4WRE3sg7+XC+XMetud6y1HXKngapwG18A3JLtdM=</latexit>

t=1

Optimize (coop.)

Optimize (coop.)
N →∞

Mean-field system

<latexit sha1_base64="O3X2AAfALvRVSAkaNx04xKP9omc=">AAACWXicfVBNS8NAEN3Erxq/qj16WSwFTyXRoh4LXjwqWhU0lM122i7dZMPupFBCf4JX/W3in3HTBjEWfLDweG9mZ+ZFqRQGff/TcdfWNza3atvezu7e/kH98OjRqExz6HEllX6OmAEpEuihQAnPqQYWRxKeosl14T9NQRuhkgecpRDGbJSIoeAMrXQ/7Z/3602/7S9AV0lQkiYpcds/dLqvA8WzGBLkkhnzEvgphjnTKLiEufeaGUgZn7ARvFiasBhMmC92ndOWVQZ0qLR9CdKF+rsjZ7ExsziylTHDsfnrFeKP16qMwuFVmIskzRASvpw0zCRFRYvL6UBo4ChnljCuhV2W8jHTjKPNx2tRnhlU8f9/Vty8WAKVkqZ6s+FMwiDMxyCngBUvR2HDmNvMg78Jr5LHs3Zw0e7cdZrdTpl+jRyTE3JKAnJJuuSG3JIe4WRE3sg7+XC+XMetud6y1HXKngapwG18A3JLtdM=</latexit>

<latexit sha1_base64="O3X2AAfALvRVSAkaNx04xKP9omc=">AAACWXicfVBNS8NAEN3Erxq/qj16WSwFTyXRoh4LXjwqWhU0lM122i7dZMPupFBCf4JX/W3in3HTBjEWfLDweG9mZ+ZFqRQGff/TcdfWNza3atvezu7e/kH98OjRqExz6HEllX6OmAEpEuihQAnPqQYWRxKeosl14T9NQRuhkgecpRDGbJSIoeAMrXQ/7Z/3602/7S9AV0lQkiYpcds/dLqvA8WzGBLkkhnzEvgphjnTKLiEufeaGUgZn7ARvFiasBhMmC92ndOWVQZ0qLR9CdKF+rsjZ7ExsziylTHDsfnrFeKP16qMwuFVmIskzRASvpw0zCRFRYvL6UBo4ChnljCuhV2W8jHTjKPNx2tRnhlU8f9/Vty8WAKVkqZ6s+FMwiDMxyCngBUvR2HDmNvMg78Jr5LHs3Zw0e7cdZrdTpl+jRyTE3JKAnJJuuSG3JIe4WRE3sg7+XC+XMetud6y1HXKngapwG18A3JLtdM=</latexit>

v7

MFC

<latexit sha1_base64="IyGMvrnkbWZNj+5D14QZ+fkYkxo=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQfXg+P637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFeCS11g==</latexit>

N -agent system

v3

v3

v6

<latexit sha1_base64="vjKAuUuoO5jiBXd21bamnLFmgx4=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbNgYlqpe3VuB7hJ/Q6pkg9aw7DQHI8XTCGLkkhnT970Eg4xpFFzCojhIDSSMv7EJ9C2NWQQmyFa7LmjNKiM6Vtq+GOlK/duRsciYeRTayojh1Gx7S/HXq+VG4fghyEScpAgxX08ap5KiosvL6Uho4CjnljCuhV2W8inTjKPNp1ijPDWoov//zLnZcglUSpr8zYYzCaMgm4KcAea8DIUNY2Ez97cT3iWd27p/V288N6rNxib9Arki1+SG+OSeNMkTaZE24WRC3skH+XS+XcctuMV1qetseiokB7fyA3Q+tdQ=</latexit>

<latexit sha1_base64="IyGMvrnkbWZNj+5D14QZ+fkYkxo=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQfXg+P637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFeCS11g==</latexit>

<latexit sha1_base64="vjKAuUuoO5jiBXd21bamnLFmgx4=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbNgYlqpe3VuB7hJ/Q6pkg9aw7DQHI8XTCGLkkhnT970Eg4xpFFzCojhIDSSMv7EJ9C2NWQQmyFa7LmjNKiM6Vtq+GOlK/duRsciYeRTayojh1Gx7S/HXq+VG4fghyEScpAgxX08ap5KiosvL6Uho4CjnljCuhV2W8inTjKPNp1ijPDWoov//zLnZcglUSpr8zYYzCaMgm4KcAea8DIUNY2Ez97cT3iWd27p/V288N6rNxib9Arki1+SG+OSeNMkTaZE24WRC3skH+XS+XcctuMV1qetseiokB7fyA3Q+tdQ=</latexit>

v7

v8

t=2

t=T

<latexit sha1_base64="IUjdkb+TNbp/7++bYD28yRhLBOk=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQf3g6P637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFfAq12A==</latexit>

<latexit sha1_base64="cxOgxnY4l64jWQEqGrOs54QZRCg=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQf3gyP637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFehe11w==</latexit>

<latexit sha1_base64="cxOgxnY4l64jWQEqGrOs54QZRCg=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQf3gyP637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFehe11w==</latexit>

<latexit sha1_base64="/YzgCdZjvgJjk8UhuaagJbutks0=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuiHpRAl48RjQPSBaZnXSSwdmdZaY3EJZ8glf9NvFnnDwQNwELBoqq7unuChMpDHrel+Nube/s7hX2iweHR8cnpfJp26hUc2hxJZXuhsyAFDG0UKCEbqKBRaGETvj2MPc7E9BGqPgFpwkEERvFYig4Qys9453/Wqp6dW8Bukn8FamSFZqvZafRHyieRhAjl8yYnu8lGGRMo+ASZsV+aiBh/I2NoGdpzCIwQbbYdUZrVhnQodL2xUgX6t+OjEXGTKPQVkYMx2bdm4u/Xi03Coe3QSbiJEWI+XLSMJUUFZ1fTgdCA0c5tYRxLeyylI+ZZhxtPsUa5alBFf3/Z87N5kugUtLkbzacSRgE2RjkBDDnZShsGDObub+e8CZpX9b96/rV01W1cb9Kv0DOyDm5ID65IQ3ySJqkRTgZkXfyQT6db9dxC25xWeo6q54KycGt/AArFbW3</latexit>

v4

v6

v4

<latexit sha1_base64="IyGMvrnkbWZNj+5D14QZ+fkYkxo=">AAACWXicfVBNS8NAEN3Erxq/9ehlsRQ8lUSKeix48ahoP6CGstlO26WbbNidFEroT/Cqv038M25qEGPBBwuP92Z2Zl6USmHQ9z8cd2Nza3untuvt7R8cHh2fnHaNyjSHDldS6X7EDEiRQAcFSuinGlgcSehFs7vC781BG6GSZ1ykEMZskoix4Ayt9DQfXg+P637TX4Guk6AkdVLiYXjitF9GimcxJMglM2YQ+CmGOdMouISl95IZSBmfsQkMLE1YDCbMV7suacMqIzpW2r4E6Ur93ZGz2JhFHNnKmOHU/PUK8cdrVEbh+DbMRZJmCAn/njTOJEVFi8vpSGjgKBeWMK6FXZbyKdOMo83Ha1CeGVTx/39W3LxYApWSpnqz4UzCKMynIOeAFS9HYcNY2syDvwmvk+5VM7huth5b9XarTL9GzskFuSQBuSFtck8eSIdwMiGv5I28O5+u49Zc77vUdcqeM1KBe/YFeCS11g==</latexit>

<latexit sha1_base64="vjKAuUuoO5jiBXd21bamnLFmgx4=">AAACWXicfVDLSgNBEJxdXzG+EnP0MhgCnsKuBPUY8OIxonlAsoTZSScZnN1ZZnoDYckneNVvE3/GyQNxE7BgoKjqnu6uMJHCoOd9Oe7e/sHhUeG4eHJ6dn5RKl92jEo1hzZXUuleyAxIEUMbBUroJRpYFErohm+PS787A22Eil9xnkAQsUksxoIztNLLbNgYlqpe3VuB7hJ/Q6pkg9aw7DQHI8XTCGLkkhnT970Eg4xpFFzCojhIDSSMv7EJ9C2NWQQmyFa7LmjNKiM6Vtq+GOlK/duRsciYeRTayojh1Gx7S/HXq+VG4fghyEScpAgxX08ap5KiosvL6Uho4CjnljCuhV2W8inTjKPNp1ijPDWoov//zLnZcglUSpr8zYYzCaMgm4KcAea8DIUNY2Ez97cT3iWd27p/V288N6rNxib9Arki1+SG+OSeNMkTaZE24WRC3skH+XS+XcctuMV1qetseiokB7fyA3Q+tdQ=</latexit>

approx.

<latexit sha1_base64="oTRRbY6Wv5avIjODK1pQV+kx00c=">AAACWXicfVDLagJBEJzdvIx5x2MuQ0TISXZFkhyFXHI0JD5AF5kdWx2c3VlmegVZ/IRck28L+ZmMD0JWIQUDRVX3dHeFiRQGPe/Lcff2Dw6PCsfFk9Oz84vLq+u2Uanm0OJKKt0NmQEpYmihQAndRAOLQgmdcPq09Dsz0Eao+A3nCQQRG8diJDhDK73OBrXBZdmreivQXeJvSJls0BxcOY3+UPE0ghi5ZMb0fC/BIGMaBZewKPZTAwnjUzaGnqUxi8AE2WrXBa1YZUhHStsXI12pfzsyFhkzj0JbGTGcmG1vKf56ldwoHD0GmYiTFCHm60mjVFJUdHk5HQoNHOXcEsa1sMtSPmGacbT5FCuUpwZV9P+fOTdbLoFKSZO/2XAmYRhkE5AzwJyXobBhLGzm/nbCu6Rdq/r31fpLvdyob9IvkBtyS+6ITx5IgzyTJmkRTsbknXyQT+fbddyCW1yXus6mp0RycEs/cFi10g==</latexit>

<latexit sha1_base64="oTRRbY6Wv5avIjODK1pQV+kx00c=">AAACWXicfVDLagJBEJzdvIx5x2MuQ0TISXZFkhyFXHI0JD5AF5kdWx2c3VlmegVZ/IRck28L+ZmMD0JWIQUDRVX3dHeFiRQGPe/Lcff2Dw6PCsfFk9Oz84vLq+u2Uanm0OJKKt0NmQEpYmihQAndRAOLQgmdcPq09Dsz0Eao+A3nCQQRG8diJDhDK73OBrXBZdmreivQXeJvSJls0BxcOY3+UPE0ghi5ZMb0fC/BIGMaBZewKPZTAwnjUzaGnqUxi8AE2WrXBa1YZUhHStsXI12pfzsyFhkzj0JbGTGcmG1vKf56ldwoHD0GmYiTFCHm60mjVFJUdHk5HQoNHOXcEsa1sMtSPmGacbT5FCuUpwZV9P+fOTdbLoFKSZO/2XAmYRhkE5AzwJyXobBhLGzm/nbCu6Rdq/r31fpLvdyob9IvkBtyS+6ITx5IgzyTJmkRTsbknXyQT+fbddyCW1yXus6mp0RycEs/cFi10g==</latexit>

<latexit sha1_base64="oTRRbY6Wv5avIjODK1pQV+kx00c=">AAACWXicfVDLagJBEJzdvIx5x2MuQ0TISXZFkhyFXHI0JD5AF5kdWx2c3VlmegVZ/IRck28L+ZmMD0JWIQUDRVX3dHeFiRQGPe/Lcff2Dw6PCsfFk9Oz84vLq+u2Uanm0OJKKt0NmQEpYmihQAndRAOLQgmdcPq09Dsz0Eao+A3nCQQRG8diJDhDK73OBrXBZdmreivQXeJvSJls0BxcOY3+UPE0ghi5ZMb0fC/BIGMaBZewKPZTAwnjUzaGnqUxi8AE2WrXBa1YZUhHStsXI12pfzsyFhkzj0JbGTGcmG1vKf56ldwoHD0GmYiTFCHm60mjVFJUdHk5HQoNHOXcEsa1sMtSPmGacbT5FCuUpwZV9P+fOTdbLoFKSZO/2XAmYRhkE5AzwJyXobBhLGzm/nbCu6Rdq/r31fpLvdyob9IvkBtyS+6ITx5IgzyTJmkRTsbknXyQT+fbddyCW1yXus6mp0RycEs/cFi10g==</latexit>

Cooperative Pareto optimum

v5

v5

<latexit sha1_base64="EBwbAFbU8n87tNRwcFTAcX2wTa8=">AAACWXicfVBNS8NAEN3Er1q/qj16WSwFTyWR+nEsePGoaFXQUDabSbt0kw27k0IJ/Qle9beJf8ZNW8RY8MHC472ZnZkXZlIY9LxPx11b39jcqm3Xd3b39g8ah0ePRuWaQ58rqfRzyAxIkUIfBUp4zjSwJJTwFI6vS/9pAtoIlT7gNIMgYcNUxIIztNL9ZHA+aLS8jjcHXSX+krTIEreDQ6f3GimeJ5Ail8yYF9/LMCiYRsElzOqvuYGM8TEbwoulKUvABMV81xltWyWisdL2pUjn6u+OgiXGTJPQViYMR+avV4o/XrsyCuOroBBpliOkfDEpziVFRcvLaSQ0cJRTSxjXwi5L+YhpxtHmU29TnhtUyf9/VtyiXAKVkqZ6s+FMQhQUI5ATwIpXoLBhzGzm/t+EV8njWce/6HTvuq1ed5l+jRyTE3JKfHJJeuSG3JI+4WRI3sg7+XC+XMetufVFqesse5qkArf5DXYxtdU=</latexit>

<latexit sha1_base64="S3vl3K7NivkKRSHPxyp24OpSoTk=">AAACWXicfVBNSwMxEM2uX7V+tXr0EiwFT2W3FPWiFLx4VLS20C4lm07b0OxmSWYLZelP8Kq/Tfwzph+IW8EHgcd7M5mZFyZSGPS8T8fd2t7Z3SvsFw8Oj45PSuXTV6NSzaHFlVS6EzIDUsTQQoESOokGFoUS2uHkfuG3p6CNUPELzhIIIjaKxVBwhlZ6xtt6v1Txat4S9C/x16RC1njsl51mb6B4GkGMXDJjur6XYJAxjYJLmBd7qYGE8QkbQdfSmEVggmy565xWrTKgQ6Xti5Eu1d8dGYuMmUWhrYwYjs2mtxB/vGpuFA5vgkzESYoQ89WkYSopKrq4nA6EBo5yZgnjWthlKR8zzTjafIpVylODKvr/z5ybLZZApaTJ32w4kzAIsjHIKWDOy1DYMOY2c38z4b/ktV7zr2qNp0alebdOv0DOyQW5JD65Jk3yQB5Ji3AyIm/knXw4X67jFtziqtR11j1nJAf37BstCLW4</latexit>

Optimize (comp.)

Optimize (comp.)

<latexit sha1_base64="Ctf4ETGquQ7LakQu71o7dDJf8EM=">AAACWXicfVDLSgNBEJxdXzG+Ej16GQwBT2FXgnpRAl48KuYFyRJmJ51kyOzOMtMbCEs+wat+m/gzTh6Iq2DBQFHVPd1dYSKFQc/7cNyt7Z3dvcJ+8eDw6PikVD5tG5VqDi2upNLdkBmQIoYWCpTQTTSwKJTQCacPS78zA22Eips4TyCI2DgWI8EZWukF75qDUsWreSvQv8TfkArZ4GlQdhr9oeJpBDFyyYzp+V6CQcY0Ci5hUeynBhLGp2wMPUtjFoEJstWuC1q1ypCOlLYvRrpSf3ZkLDJmHoW2MmI4Mb+9pfjtVXOjcHQbZCJOUoSYryeNUklR0eXldCg0cJRzSxjXwi5L+YRpxtHmU6xSnhpU0f9/5txsuQQqJU3+ZsOZhGGQTUDOAHNehsKGsbCZ+78T/kvaVzX/ulZ/rlca95v0C+ScXJBL4pMb0iCP5Im0CCdj8kreyLvz6TpuwS2uS11n03NGcnDPvgBvTrXa</latexit>

Competitive Nash equilibrium

approx.

MFG

Fig. 4. Visualization of an adaptive network over time. At time t = 2, the
connection (edge) between nodes v3 and v4 ends. Until time t = T , node v7
leaves the network and node v8 makes a connection with node v3 .

streaming network. These strategies can be non-cooperative,
centralized, incremental, consensus, or diffusion based and are
used for adaptation, learning, and optimization over networks.
In the factored MDPs, the assumption that the observation and
transition models are fully factorized limits the applicability
of the model to many real-life scenarios. A more generalized
model is proposed in [84], under the name of collaborative
graphical Bayesian games (CGBG). Here, agents may change
their interactions with neighbors at every stage. This results
in a non-stationary interaction graph. They propose to find a
solution for this Dec-POMDP at every stage instead of finding
individual optimal policies of full length. In [139], different
types of agents are considered, leading to further factorization
of the interaction graph through type-independence. In [140]
the authors present a task allocating problem in a multiagent system with dynamic neighborhoods. They proposed
four distributed strategies based on reinforcement learning,
while considering that the agents have limited resources and
connections with other agents may be lost or added at any
time. Their solutions can be applied to any problem where the
networks are dynamic and agents are self-organizing.
Especially in communication networks, using reinforcement
learning to find optimal routing paths in adaptive networks
is not a new direction, see e.g. [141]. In [142] the authors
embedded a reinforcement learning module at every node of an
adaptive switching network. The nodes use only local interactions to predict the network statistics and learn optimal routing
policies to minimize the delivery time of packets under varying
network loads. Some other older and recently proposed routing
algorithms include (predictive) Q-Routing based on Q learning
[143], [144], DRQ-Routing based on dual reinforcement Q
learning [145], Hierarchical Q-Routing based on simulated
annealing and hierarchy in the network [146], TD-Routing
based on temporal difference learning [147], and Ants-Routing
based on biological network of ants [148]–[151]. Mobile adhoc networks (MANETs) are also an example of adaptive,
distributed networks. In [152] the authors use collaborative reinforcement learning to enable agents in a MANET to optimize
network throughput online using feedback, without any prior
knowledge or central control. Q-routing based on Q learning
has also been proposed many times to find optimal routing
solutions in MANETs [153], [154]. A recent comprehensive
analysis of the reinforcement learning based centralized and
distributed routing schemes, combined with multi-optimality
routing criteria, for dynamic IoT networks is given [155].
Finally, adaptive networks are also a more realistic way of

Fig. 5. Pictorial scheme of approximation for mean-field games and meanfield control. The finite N -agent system is first approximated by a meanfield system, which is then solved through learning algorithms, thereby
circumventing the difficult solving of the finite system. The resulting solution
will be an approximately optimal solution in sufficiently large finite systems.

modeling epidemic spreading scenarios, which are frequent
objects of study in complex network theory, see also [28],
[156]. Authors in [157] have used an adaptive protection
scheme using graph protection strategies, proposing n-fitted
Q learning to train the model and find the optimal allocation
of protection resources to maximize the number of surviving
nodes. In [158] also the authors study the graph protection
problem by constructing minimum protective vertex covers
using reinforcement learning.
Thus, higher-order and adaptive networks are a natural and
realistic way to represent practical multi-agent problems. Overall, we believe that more work on the intersection of complex
network theory and MARL may be a way to scale MARL
algorithms to large populations with complex interactions.
B. Mean-field limits
A quite recent and related approach towards solving the
scalability problem is via combining MARL with mean-field
approximations and mean-field games (MFGs) [159]. Using
the idea of mean-field theory (classically on graphs, see
e.g. [160]), the many-agent problem is simplified for the
infinite-agent limit. Intuitively, in MFGs and their cooperative
counterpart called mean-field control (MFC) the interaction
between all agents is simplified to a two-body interaction
between the mass of all agents and the behavior of any representative single agent. Most importantly, this simplification
reduces the generally complex multi-agent problem into a fixed
point equation (competitive) or high-dimensional single-agent
problem (cooperative) with theoretical guarantees. This allows
one to circumvent the otherwise difficult control problem of
finite agent systems by solving the infinite agent system, which
will provide an approximately optimal solution in the finite
agent system, see also Fig. 5. In particular, the cooperative
MFC approach has only very recently been developed in great
generality [97] and deserves further investigation, as reducing
the combinatorial nature of MARL [18], [23] to the betterunderstood curse of dimensionality of single-agent RL [161]
could allow for effective learning of scalable control.
Apart from solving limiting mean-field formulations presented in the following through RL techniques, mean-field
approximations have also been applied directly to MARL.
In the seminal work of [159], the authors apply a pairwise

9

decomposition of value functions to the competitive graphbased scenario, where value functions of each agent are
assumed to depend only on the mean action of neighbors.
Similar algorithms have since been proposed to extend towards
partial observability [162] and heterogeneous agents [163],
[164]. In another work, combining the CTDE approach with
the effect of exponential decay using k-hop neighborhoods,
the authors in [165] propose localized training - decentralized
execution methods. Here the critic is not learned using global
information, but using information from agents with states
that are close in the sense of a network. However, so far
mean-field-based approximations for MARL either remain
limited to the cooperative case [165], or require very strong
assumptions in order to obtain theoretical convergence results
[159], motivating also the solution of the following limiting
mean-field systems with milder technical assumptions.
1) Mean-field games: Popularized by the seminal works
[166], [167] in the setting of stochastic differential games,
i.e. continuous-time dynamical systems described through
stochastic differential equations, competitive MFGs formally
assume infinitely many homogeneously modeled agents, which
will allow one to handle the mass of many agents through
statistical terms. Given that agents interact weakly with each
other – e.g. each agent depends continuously on the overall
state distribution of all other agents – it is often possible to
obtain both theoretically rigorous approximation results for
sufficiently large systems and at the same time more tractable
formulations of otherwise intractable problems. Parallel to the
law of large numbers, such an approach is typically asymptotically exact as the number of agents grows sufficiently large.
Here, many prior works consider stochastic differential games,
see also various reviews [168]–[170] on the very extensive
body of literature on MFGs. Only recently were MFGs extended in very general form to discrete time [171], which
allows a more accessible treatment and development of more
classical learning-based algorithms, thus constituting the main
focus of the review in our survey. The result is a framework
for analyzing otherwise intractable competitive many-agent
problems and finding approximate Nash equilibria.
In recent years, there has been an increasing focus on
using RL and other learning techniques to solve such MFGs
and MFC. Due to the extensive amount of literature, we
will keep it brief and point towards the recent survey [172]
for more details. Note that here, learning refers to both the
classical computation of equilibria in game theory as well
as modern deep RL. In contrast to finding analytic solutions,
which typically remain restricted to certain special cases and
require manual effort, the goal is to computationally solve
arbitrary MFGs. While MFGs also find application in solving inverse RL problems [173]–[175] or through related but
distinct mean-field approximations directly for MARL [159],
the vast majority of recent works considers algorithms for
solving MFGs, which could in turn be used indirectly through
model-based MARL to solve finite-sized large-population systems. The perhaps most straightforward approach is through
classical fixed-point iteration [166], where one iterates over
optimal policies given that all other agents play the previous
policy. However, unfortunately this tends to fail in discrete-

time and it is usually difficult to verify theoretical guarantees
[87]. Here, a recent approach approximates MFGs through
entropy regularization, giving solvable approximate equilibria
[85], [87], [176]–[178]. Another approach [88], [179], [180]
focuses on potential MFGs by adapting the classical fictitious
play algorithm [51]. Together with modern deep techniques
such as normalizing flows and deep RL [32], or via mirror
descent [90], [91], such algorithms are further able at the
same time to scale to large state spaces. Very recently, there
have also been efforts to formulate MFGs in an optimizationbased framework by using the linear program formulation of
MDPs [181]. Finally, a slightly different but related scenario
aims at correlated equilibria, which allow for conditioning on
correlating noise between agents and have also been studied in
the context of MFGs [182]. Here, the combination of policyspace response oracles [183] with MFGs can similarly find a
type of correlated equilibrium [184].
2) Mean-field control: In contrast to competitive MFGs
where agents attempt to selfishly optimize their own objective,
another important class of problems is described by the fullycooperative framework of MFC [169], [185]. In other words,
the goal is to find behavior for each agent that optimizes a
global objective, i.e. the solution concept shifts from Nash
equilibrium to either Pareto optimum or simply the optimization of a global cost function. As a simple illustration, such a
global cost function could for instance be given by the optimal
foraging of food in natural swarms such as ant or bee hives, or
by an optimal cell coverage in the engineering of UAV swarms
for mobile cell towers. Similar to MFGs, MFC has only
recently been considered in discrete time [96], [97]. Through
a dynamic programming principle on an enlarged state-action
space, one may reduce the MFC problem to a potentially
infinite-dimensional single-agent control problem [96], [97],
[186], [187], which will have a complexity independent of the
number of agents and may be more tractable.
In the cooperative MFC problem, finding decentralized
Pareto optimal policies has been the study of many recent
works [97], [188]–[190]. Here, a promising approach is to
formulate an equivalent high-dimensional or even infinitedimensional MFC Markov decision process (MDP), where one
finds a dynamic programming principle [96], [97], [186] and
a number of theoretically rigorous approximation properties
[189], [191], [192] for the MFC MDP and corresponding
finite-sized large-population systems, giving the MFC approach a theoretical basis for tractable MARL. An issue that
remains is that the MFC MDP typically has a function-valued
action space for continuous agent state spaces, for which only
limited preliminary studies have been performed [193]. Here,
a common idea is to discretize the continuous state space,
which somewhat avoids the problem of representation and
enables application of well-established tabular or approximate
RL algorithms, such as Deep Deterministic Policy Gradient
(DDPG, [194]) as proposed in [97]. Alternatively [96] uses
kernel regression methods to tackle the continuous state space
and reduce the action space to a finite space. The authors
provide an algorithm to solve the Bellman equations approximately. Still, finding efficient reinforcement algorithms for the
lifted MFC MDP remains a major challenge for solving MFC

10

y
1

y
1

1
0.8
0.6
0.4
0.2

0

1x

0

1x

0

Fig. 6. Visualization of the finite step graphon of a graph with 5 nodes (left)
and a limiting graphon (right). Graphons as continuous-domain versions of
adjacency matrices provide a tractable way of modeling large graph limits.

problems [187]. For linear-quadratic mean-field scenarios, the
convergence of policy gradient methods is shown by enforcing
the correct form of the policy [195]. In a similar fashion,
[196] proposed a natural actor-critic algorithm for hierarchical
linear quadratic problems, while [188], [189] propose policy
gradient methods for stationary and general MFC problems.
In contrast to the mentioned methods, the authors of [190]
suggest a model-based algorithm. Similar to the single-agent
case, the algorithm is sample-efficient compared to model-free
approaches.
Similar ideas have been formulated also for PDE-based
problems from the control community [197], [198], though
differences – to name some – include less focus on general
learning-based solutions and instead on particular problems,
as well as the assumption of PDE models instead of arbitrary
time-discrete per-agent dynamics. While there exists some
isolated prior work on learning PDE control via discretization
in both time and space [193], this connection as well as a
more tractable and specific design of reinforcement learning
algorithms in MFC beyond discretization remains unexplored.
3) Graphs and partial observability: MFGs and MFC
have since also been extended to interactions on graphs,
with the goal of modeling sparser interactions than all-toall. Interactions of agents should be only with a subset of
other agents, which can be formulated through their neighbors
on a graph, see also some early works [199], [200]. Here,
graphons as limits of large graphs [201] are used to describe
MFGs on graphs, which have been formulated for static
[202], [203] and dynamic agents, both in continuous and
discrete time [92], [94], [200], [204]. See also Fig. 6 for a
visualization. The associated graphon MFGs can be considered
a generalization of standard MFGs to neighbor interaction
on a graph [94], [204], [205], or alternatively via a double
limit instead as many populations of homogeneous agents,
where each node in a graph represents large populations of
homogeneous agents, each interacting with large populations
of other neighboring populations [92], [200]. Here, the state
of the art still remains limited to highly dense, undirected, and
static graphs apart from few exceptions of higher-order [206],
[207] or sparse graphs [208], [209]. Similarly, few works have
studied partially-observed mean-field systems. Some partiallyobserved MFGs have been formulated in continuous time
[210], [211] and with major-minor formulation which includes
an external agent not part of a homogeneous swarm [212]–

[215]. Very recently, a partially-observed system was also
analyzed in discrete time [216], though the partially-observed
system remains restricted to deterministic mean-field. Finally,
initial works have already appeared on analogous cooperative
partially-observed MFC [217]. However, the development of
both theory and learning algorithms for the aforementioned
settings has only begun and largely remains to be explored.
C. Collective swarm intelligence
The classical field of collective intelligence has been concerned with the study of group intelligence – as commonly
observed in nature – in form of collaborative interactions
between collectives of many individual agents that behave in
a decentralized manner [218], [219]. The way of collaboration enables the swarm to accomplish tasks that are beyond
the capabilities or even comprehension of their individuals.
Here, the rule of interaction among the swarm considers the
emergence of macroscopic intelligence arising from behavior
of microscopic agents, with agent interactions that are typically highly localized and homogeneous. Due to the highly
localized and homogeneous nature, such systems are scalable
to almost arbitrary numbers of agents and inherently robust
against failures, by redundancy of agents, as each agent can
be substituted by any other. Moreover, by the fact of being
decentralized, the information obtained by every agent through
interaction with direct neighbors is stored locally, and not
necessarily required globally. Therefore, in case of loss of
certain numbers of agents, the efficiency of the whole swarm
may drop but a given task is still solved to a sufficient degree.
In summary, swarms are scalable to any number of agents,
while maintaining their functionality without the need to define
how each agent interacts individually [29].
1) Reinforcement learning for swarm intelligence: Given
the increasing number of emerging application scenarios of
collective intelligence that may be handled through the engineering of swarm behavior, e.g. in swarms of UAVs [220],
[221], synthetic biology [222], or in social sciences [223], it is
only natural to consider the study of collective intelligence for
the control of large-population systems. In recent years, with
the advance of computational power and the availability of
large datasets, sophisticated deep neural network techniques
have achieved state-of-the-art performances across several
fields. With these advances, the field of collective intelligence
aims to adapt methods of the artificial intelligence field. For
instance, the fruitful intersection of collective intelligence with
deep learning has achieved renewed progress in both areas, see
[224] for a recent survey on this intersection.
In connection with decision-making and reinforcement
learning, the idea of self-organizing and modular, embodied
agents has allowed e.g. for the design of decentralized controllers, capable of dynamically self-assembling morphologies
in simulation and forming larger complex morphologies to
adapt to a variety of terrains [225]. Although there exists
a variety of possibilities for the taxonomy of collective intelligence, a common classification ranging from artificial
intelligence down to swarm robotics can be abstracted in
Fig. 7. Here, the field of swarm intelligence is considered

11

Artificial Intelligence (AI)
Collective Intelligence (CI)
Swarm Intelligence (SI)
Swarm Robotics (SR)

Fig. 7. Taxonomy of collective intelligence. Collective intelligence can be
understood as a subcategory of artificial intelligence, while swarm robotics as
part of swarm intelligence are a subfield of collective intelligence.

a subfield of collective intelligence based on the collective
behavior of decentralized and self-organized systems [226],
while swarm robotics is based on applying the paradigms
and methodologies of swarm intelligence to groups of simple
homogeneous embodied agents, coordinated in a distributed
and decentralized way to perform difficult swarm tasks.
The field of MARL is mostly concerned about environment
and algorithm settings directed to a simple number of agents
(2-10) at the micro-level that could find a place in different
application scenarios [20]. These scenarios are typically applications of MARL techniques on a microscopic level, which are
not sufficient to deploy to large populations. In contrast, the
field of collective intelligence relies on a much large number
of agents involving population sizes ranging from hundreds
to millions. In connection with recent advances in MARL,
[70] develops the MAgent platform to support development
of many-agent reinforcement learning environments that focus
on swarm intelligence applications and necessitate scalability.
Through interactions among many agents, the platform facilitates learning benchmarks for the study of MARL, specifically
the emergence of properties from the behavioral interaction
of large populations. In turn, other works such as Neural
MMO [71] have appeared, analogously focusing on further
competitive scenarios of many agents. See also discussions in
the recent survey [224]. However, it seems that so far, existing
works are sparse and remain limited to selfish formulations
with extensive reward engineering, defining suitable per-agent
rewards [227] and per-agent observations together with appropriate policy structures [69] to achieve desirable results, see
also e.g. the choice of cost function in the Battle scenario
of MAgent [70]. Here, future work could attempt to close
current gaps in learning how to minimize global swarm-level
objective functions in order to proceed towards engineering
artificial swarm intelligence.
2) Swarm intelligence for decision-making: On the other
hand, natural intelligent swarms such as ant or bee hives are
able to solve highly complex decision-making tasks such as
foraging, where under extreme decentralization the biological
agents are nonetheless capable of strong coordination. In
general, there are a variety of classically considered swarm
intelligence tasks [29], [34] that may not only serve as
benchmarks for large-population MARL methods and building
blocks for intelligent behavior, but also as a fountain of
ideas through the plethora of existing prior works attempting

to solve swarm intelligence through more classical or bioinspired methods. Thus, we believe that it may be of great
interest also for scalable large-population MARL to understand approaches from collective intelligence literature. In the
following, we describe a subset of such tasks and how agents
may organize and distribute themselves among the swarm in
a swarm-intelligent manner, though for space reasons we only
present the most important ideas and selected results. For more
detailed information, see also the excellent surveys [29], [34].
In terms of spatially-organizing behaviors [29], the perhaps
simplest task considered in swarm intelligence is that of
aggregation [228], [229]. In [230], a solution for the problem
of self-organized aggregation of many agents into clusters
is proposed, by using only a long-range binary sensor to
detect agents in their line of sight, inspired by observations
of cockroaches and bees in nature studied in [231]. Results
showed that the algorithm successfully aggregated at least a
thousand agents into a single cluster consistently. Somewhat
more complex, the tasks of chain formation and pattern formation consider the problem of connecting two points through
agents, and deploying in a regular and repetitive pattern
respectively. For the former, agents should form chains in a
self-organized manner, e.g. similar to foraging ants connecting
their nest with foraging areas [232]. In [233], exploration
and navigation strategies were introduced through evolutionary
algorithms for effectively finding the shortest path between
two points in an unknown environment. Furthermore, inspired
by natural swarms, works such as [234] use pheromones as
swarm communicating robots without any external communication infrastructure. For pattern formation, [235] developed
a scalable control mechanism that implements scalability,
where the desired large-scale self-organizing pattern formation emerges from pair-wise interactions between agent and
its nearest neighbor by following mesh abstractions called
virtual springs to compute virtual forces rules of motion, i.e.
repulsion or attraction such that the global behavior emerges
from these simple local interactions. For further biologicallyinspired methodologies in swarm intelligence, see also [236].
Beyond relatively simple spatial organization problems,
many more complex behavioral tasks are also commonly
considered in swarm intelligence, including but not limited
to navigation behaviors such as collective exploration and
transportation, or more abstract collective decision-making
such as decentralized task allocation and consensus finding.
For example, task allocation assumes that challenging tasks
are decomposed into a series of sub-tasks and split up among
homogeneous agents [237]. Consensus finding on the other
hand can be found in groups of ants using pheromones
to find shortest paths [238], and may be realized through
voting mechanisms [239] or specialized consensus algorithms
[240], which are an area of research in itself [241]. Similar
biological inspiration is also used in collective exploration
tasks, giving rise also to otherwise useful and well-known
optimization techniques such as ant colony optimization [242]
or particle swarm optimization [243] for navigation and global
optimization respectively. Finally, in particular for embodied
agents, coordinated motion, and collision avoidance are useful
constituents of complex behavior, see e.g. flocking in group

12

of birds or schooling in group of fish as impressive examples
of such self-organized coordinated motion [244]. Through
coordinated motion, animals gain several advantages, such as
a higher survival rate, more precise navigation, and reduced
energy consumption [245]. Here, biological inspiration from
swarm intelligence in birds and bugs has helped towards
advances such as scalable collision avoidance [246] and exploration [247] for swarms of UAVs respectively.
Overall, it seems that existing ideas from swarm intelligence and natural systems may help in designing appropriate
algorithms for large-population systems with local interaction.
More precisely, we feel that the usage of consensus algorithms
as a well-studied field of research may help in the design of
intelligent coordinated behavior through MARL, which has a
similar flavor to communication-based methods.
D. Partial observability and decentralization
Partial observability and decentralization are a key element
in large-population systems, both from the theoretic and applied point of view. Without partial observability – unless the
goal is to make control design of large systems more tractable
through reduction of the exponential state and action space and
avoiding a global solution – each agent must know the global
state of the entire system and may thus coordinate perfectly
through a global policy shared by all agents. Thus, there cannot
be decentralization. On the other hand, absence of decentralization implies that agents may coordinate centrally, reducing
to a centralized though potentially partially-observable singleagent problem. Meanwhile for applications, especially in large
systems, it may be unrealistic or too strict of a condition to
assume existence of a central planner or for all agents to
know the global state of the system. Therefore, we believe
that handling the different levels of partial observability and
decentralization – which we shall refer to as locality – is of
central importance and should be investigated more closely. In
particular, in this survey we would like to give a new view on
partial observability and decentralization by analyzing various
degrees of locality that may occur in systems. We hope that
this new point of view is able to identify both commonalities
between existing work as well as current gaps in the theory
of controlling large-population systems.
To systematically categorize and understand existing and
future works on partially observed, decentralized systems, we
introduce a differentiation between four different ideas of
locality. This will allow us to understand relations between existing works and current limitations. We shall do this formally
by identifying different ‘radii’ of interest around an agent in
terms of dynamical interactions, observations and objectives.
To begin, we consider a state-coupling radius A, which may
quantify the degree of locality in dynamical interaction, i.e.
how far away do other agents have to be, in order to not affect
a particular agent’s transitions. For example, consider spatial
epidemiological models on factored MDPs [72], where only
agents close to other infected agents may become infected by a
disease. Analogously, we believe it may sometimes make sense
to consider a separate action-coupling radius B, i.e. how far
out an agent’s action has the capability to affect other agent’s

transitions. In contrast to the first radius, it may be the case e.g.
in social networks, that agents are able to choose to interact
with a large quantity of agents should they wish to do so.
Next, to quantify the degree of partial observability, we define
an observation radius C by the amount of information that an
agent obtains from its neighborhood to take informed actions.
This quantity is related to the locality of information structure
[21], [98] according to which actions may be performed, and it
defines the level of partial observability in the system, though
we stress that this is not limited to spatial radii but could
also consider e.g. aggregate information or summary statistics
of the whole system state such as distributions in mean-field
games [171]. Lastly, we introduce the reward radius D, which
quantifies whether the interests of an agent lie in itself, or
also in other agents. For example, in a selfish epidemiological
large-population system of people, each agent may choose to
care only about their own infection, while in a socially-optimal
cooperative system such as in mean-field control [97], the goal
could be to minimize overall prevalence.
Finally, we introduce a number of other, more standard
categories such as the agent or behavior homogeneity, the
number of agents considered in experiments, or the desired
solution modalities such as competitive or cooperative. In
Table II, we give an exemplary overview of a few selected
techniques discussed in the prequel, together with a rough
classification of their degree of locality, though note that the
taxonomy omits various specifics of approaches and it is not
always straightforward to clearly classify a work. Instead,
the classifications can be seen more as a rough guideline.
Prior works already consider various degrees of locality, but
some observations can be made. In particular, most works
either attempt to handle highly localized actions or require
some degree of homogeneity. Furthermore, almost all scalable
approaches use graphs to decompose interactions effectively,
whereas in practice such a graphical neighborhood may not
always be easy to obtain or sufficiently realistic, see also the
discussion in Section III-A. A development of methods for
missing cases may provide potential future directions.
IV. A PPLICATIONS
In practice, systems with numbers of agents too large to
handle tractably through standard approaches are surprisingly
ubiquitous. In the following, we give an overview over selected
areas of applications that could highly profit from further
research in scalable MARL methods.
A. Distributed computing
One important area of application with high accessibility
(in terms of simulated data) is given by networked computers
and computing applications, including for example also video
games, where the advantages of MARL have been prominently
and repeatedly demonstrated in scenarios with up to e.g. 10
agents [3]–[6]. On the other hand, MARL in scenarios with
significantly larger population sizes such as in Neural MMO
[71] has not yet seen similar levels of success, and such benchmarks were only recently proposed. Here, future work towards
a better understanding and successful design of large-scale

13

TABLE II
A SELECTED OVERVIEW OF PARTIALLY- OBSERVABLE DECENTRALIZED SYSTEMS BASED ON FOUR DIFFERENT LOCALITY SCHEMES .

Title

State coupling
radius A

Action coupling
radius B

Observation
radius C

Reward
radius D

Number of
agents N

Coop. /
Comp.

Homog.

Factored MDPs [72]
Networked MARL [119]
Networked Distributed POMDPs [75]
Mean Field MARL [159], [162]
Mean Field Games [166], [167], [171]
Mean Field Control [95], [97]
Mean Field Decentralized MARL [165]
Scalable Actor Critic [105]–[107]

local
global
zero
global
global
global
local
local

local
local
zero
local
local
local
local
local

local
global
zero
varies
local
local
local
local

local
local
local
global
global
global
local
varies

finite
finite
finite
finite
infinite
infinite
infinite
finite

coop.
coop.
coop.
comp.
comp.
coop.
coop.
coop.

no
no
no
yes
yes
yes
yes
varies

multi-agent interaction is ongoing and could find application in
making real games more interactive. We imagine that similar
insights hold true also for other computing applications such
as peer-to-peer systems [248] and decentralized finance [249],
where automated game-theoretic analysis of user behavior
could help e.g. in finding successful system designs.
Apart from very specific computing applications, another
important use case of large-scale MARL may be the optimization of distributed computing itself. For example, reinforcement learning has long been used to find adaptive load
balancing algorithms [250]. Even to this day, the study of
load balancing remains an open problem, for example in
the presence of partial observability and delayed information
[251], where studies on systems with large populations of
servers and clients remain of importance and – in consideration
of today’s increasingly large-scale computing infrastructure
– continue to be investigated using e.g. mean-field analysis
[252] and learning [253]. Some related areas of research
include throughput optimization [254], cloud resource sharing
[255], [256] and edge computing [257] in systems with many
devices, where mean-field approximations are often already
used [255], [258]. However, such formulations are typically
used to analytically derive results, which has the disadvantages
of requiring extensive manual efforts and considering only
special cases. Here, a scalable and automated control algorithm
could enable solutions for more complex and realistic models.
Scalable solutions are also a necessity in communication
networks, where many agents (devices) are present. In [259],
RL has been proposed for a Network Inventory Manager
(NIM) to learn the changing network conditions and needs
of network devices. Network slicing [260] is believed to be
able to provide diversified services with distinct requirements
that the 5G-and-beyond systems need. Various algorithms of
deep reinforcement learning have been proposed to cater to
the scalability issue in the network slicing solution. We refer
the reader also to [261]–[264]. RL-based routing is also a
very promising and classical direction, especially in adaptive
networks with no prior information of the system, see [265]
and Section III-A for more details.
B. Cyber-physical systems
Cyber-physical systems constitute another highly important
and emerging subject area. Apart from the many applications

of single-agent learning in robotics [7], [8], more scalable
MARL methods could find further applications for swarms
of embodied agents. The market for unmanned aerial vehicles
(UAVs) is developing rapidly, and swarms of drones could
reach large-scale deployment in the near future [266], owing
to their great number of potential applications. In general,
swarms of terrestrial, marine and especially aerial drones could
thus be a key technology for tasks such as establishing communication networks for disaster management [267], performing efficient search-and-rescue missions [268] or delivering
packages [269]. However, deploying drone swarms in the real
world is associated with a variety of coordination challenges
[270], not seldom stemming from the complexity of real-world
environments. Here, automated and effective decision-making
for large swarms of drones remains yet an active area of
research with few general design methods [271].
Similarly, applications are not restricted to embodied, interactive agents and can long be found in other sectors of industry
such as energy [272], heating [273], and water distribution
[274]. These wide-scale critical infrastructure sectors have
similarly profited from developments in large-scale system
control, see e.g. recent works on power networks [275]–[277]
or smart heating [278]. In the future, critical infrastructure
could profit also from achieving the effective design of more
decentralized control solutions also in terms of resilience,
since secure, reliable electric power and water supply remain
of paramount importance to society. Here, scalable learning
methods could enable key technologies such as smart grids,
which may well be key to preparing and hardening the power
grid against future natural and man-made disasters [279].
C. Autonomous mobility and traffic control
Many real-world scenarios associated with autonomous mobility involve many agents and therefore require scalable learning methods. Especially following the increasing connectivity
of the vehicles and considering the highly dynamic and unpredictable nature of mobile systems. It constitutes one of the
most challenging application areas of MARL and has received
well-deserved attention from both academia and the industry
in the last decades. Important challenges of the area includes
safety constraints, standardized or compatible algorithms for
different vehicles and integration with existing systems, e.g.
manually controlled vehicles and human interaction. Here,

14

recent work employs e.g. graph-based models [280], [281]
and distributed RL-based methods [282] for scalability. For a
more detailed view of the latest works and open challenges in
autonomous mobility, we refer the reader to the recent surveys
[9], [283], [284].
Traffic control is another related application area that attracts attention as traffic congestion becomes more and more
problematic with the increasing population and proliferation
of private cars. This comes with the requirement of dealing
a large number of entities such as traffic lights, vehicles
and pedestrians, and therefore is an excellent potential application for MARL approaches. One way to benefit from
MARL in traffic control is adaptive traffic signal control. For
example, recent work utilizes actor-critic methods combined
with communication [285] and graph attention networks [286]
to help with partial observability in such problems, where
both algorithms are shown to be scalable to a large realworld traffic network of Monaco city. Another way to deal
with traffic congestion through MARL is traffic routing. We
refer the reader to recent works that propose mean-field based
approaches to this end [287]–[289].
D. Natural and social sciences
Lastly, foregoing control for a moment, the study of behavior of large-scale dynamical systems is quite classical,
such as through the mean-field theory originating in statistical
physics for the description of magnetic materials [290], which
has since also been used as a benchmark in large-population
MARL [159]. Analogous approaches are also often found
in social sciences through opinion dynamics on networks
of people [291], [292], or in particular through analyzing
general interacting particle systems on complex networks
[121]. Oftentimes, each agent in such models can be endowed
with decision-making capabilities, leading for example to
applications such as the analysis of crowd dynamics in the case
of building evacuations [293]–[295]. Apart from the social
sciences, one finds a variety of large-population scenarios also
in the related topics of economics and finance, for which we
refer to a variety of surveys [15], [16], [296], [297].
Another example of great relevance is the study of spread
and control of epidemics [28], which is not restricted to
biological epidemics but includes also e.g. malware spread on
computer networks [298]–[300], and is of interest now more
than ever due to the COVID-19 pandemic. Such systems can
be seen as multi-agent systems connected via complex and
adaptive networks, see for example [301]–[303] for work in
this direction. Recently, many works using RL for finding optimal decisions in epidemic situations have emerged. The works
in this direction include but are not limited to [304]–[307].
Here, graph-based approaches such as ones using graphons
have also been used to represent heterogeneous interactions
between players together with learning [94], [308].
V. F UTURE D IRECTIONS
Though there has already been some recent progress on the
design of scalable MARL techniques, we imagine that future
works could consider integrating ideas into MARL from a
variety of recent emerging research areas.

A. The limiting mean-field regime
Of particular note to large population systems are theories
of large systems and their infinite-sized limits. The theory of
MFGs as well as its intersection with modern deep RL and
collective intelligence remains in its infancy, both in order to
engineer large-population systems, and to understand naturally
occurring ones. Firstly, the development of learning algorithms
for such mean-field systems remains important. Only recently
have tractable learning algorithms been proposed for competitive MFGs [85], [89], though typically limited to systems
fulfilling certain conditions such as monotonicity structure
[90], [309] or contractivity [85], [87]. Moreover, specifically
the cooperative case of MFC has only been formulated in
discrete-time very recently [97] and learning algorithms going beyond discretization-based MFC MDPs [97] or locallyoptimal solutions [188] remain yet to be developed. We
imagine that further research into more tractable algorithms
such as ones based on function-valued reinforcement learning
[193] and a combination with more classical studies of PDE
system control [310] could improve the applicability of MFCbased reinforcement learning techniques to practical systems.
Secondly, the majority of MFG theories assume some sort
of weak all-to-all coupling in the population, i.e. each agent
has negligible but ever possibly present influence on any
other agent. One advantage and primary reason is that such
a construction allows one to derive mathematically rigorous
results with relative ease, giving rise to well-founded approximation guarantees at least for a large subclass of problems,
where weak all-to-all coupling indeed holds true. However,
in order for similar MFG-based approaches to be applied
to more decentralized systems without global information or
locally interacting systems such as embodied robot swarms
with physical collisions, it is necessary to better understand
MFGs with stronger locality, both in information structure and
interaction.
To this end, work on both partially observable MFGs [216]
and learning in e.g. graphical MFGs [92], [94] has only
yet begun and continues to remain of research interest, in
particular since (i) observing the full distribution of agent
states in large decentralized systems is usually unrealistic,
and (ii) current graphical frameworks still assume interaction
with infinitely many neighbors [92]. For partially observable
MFGs, one of the few works fusing partial observability with
mean-field reinforcement learning is [162], though remaining
limited to very strong assumptions and not solving a limiting
MFG. Meanwhile for graphical MFGs, on the one hand, less
mathematically rigorous mean-field equations from physical
and complex network communities such as the approximate
master equations and less detailed variants [311] could find
great value in enabling scalable MARL on more realistic,
bounded-degree network structures. On the other hand, rigorous approximation results are beginning to appear even for
highly sparse networks, though typically requiring a variety of
assumptions such as locally tree-like graphs [312] and are yet
to be generalized to controlled systems. Finally, the respective
cooperative counterparts of partially observable and graphical
MFGs remain to be explored.

15

B. Higher-order complex networks
Analogously, we believe that further work combining the
worlds of complex network sciences and graphical reinforcement learning may be of great interest, as there have been great
recent efforts in extending complex network theories to higherorder or adaptive networks. Apart from applying graphical
mean-field approximations as mentioned in the prequel, recent
developments in complex network theory consider more than
simple locally tree-like graphs and allow for clustering or
higher-order interactions, e.g. in the form of hypergraphs.
Hypergraphs have already started gaining attention since it has
been shown that simple pairwise interactions are insufficient
for representing complex, real systems [129]. For example,
[313] contribute to understanding the higher-order interactions
in complex systems using simplicial models and their relation
to hypergraphs. They also highlighted that pair-wise interactions are insufficient to accurately represent complex social
networks in which contagion occurs due to interactions in
different group sizes. While some intersecting work between
hypergraphs and MARL has been mentioned, we believe that
further exploration in this direction may be fruitful.
Additionally, modeling approaches for representing large
adaptive networks have emerged very recently and could be
applied in a control setting. This is important, since while
many real-life processes can be modeled through graphical
interactions, such interactions usually change over time. For
example, recently, [314] use a general idea of flip processes
to create time dependent trajectories in the space of graphons
and then study large graphs from the dynamical systems perspective. Here, further work could integrate control elements
with large adaptive graphs. Similarly, another approach could
be to continue research on dynamical processes on random geometric graphs, which have been used e.g. to study epidemics
[315] or synchronization of oscillators on graphs [316] and
allow for the linking of graph connections to an underlying
spatial structure. To obtain an understanding of the theory of
random geometric graphs, which could be used to study the
relations between spatial agents in a multi-agent system, we
refer the reader to [317], [318]. We believe that the use of
MARL to learn optimal policies in such scenarios is a very
fruitful direction and should be further investigated, perhaps
also by considering the recently emerging graph convolutional
networks [319] for reinforcement learning [132] together
with limiting graphon-based formulations [320]. Overall, a
combination of graphical modeling schemes with multi-agent
decision-making seems like a promising direction.

C. Intersectional and application-oriented work
Lastly, at the intersection of swarm intelligence and MARL
lies another promising approach to tractably engineering artificial swarm intelligence for systems with many interacting
agents. Work at this intersection has only begun and we
imagine that further work could lead to great advances in both
engineering and understanding swarm behavior, in particular
since so far, there appears to be no definitive toolchain or
framework for computationally designing intelligent swarm

systems [271]. Here, we imagine that learning-based approaches utilizing a number of powerful ideas from MARL,
MFGs, and complex graphical networks could provide such a
toolchain in the near future, or supplement existing ideas [69].
In particular, we imagine that automated design tools for
swarm robotics could enable a plethora of interesting applications for which current methods would necessitate an
extensive manual design of algorithms. For example, many
of the difficult and abstract subtasks in the study of swarm
intelligence mentioned in Section III-C and [29], [34] could
likely be handled by recent advances in scalable MARL.
Similarly, we believe that an application of approaches based
on limit theories could propel forward many aforementioned
fields of research, ranging from computing over cyber-physical
and embodied swarm systems to natural and social sciences.
VI. D ISCUSSION
The primary challenge we have discussed in detail is scalable analysis and understanding of large-population systems
in the context of MARL for automated design of sequential
decision-making. Indeed, in practice we often encounter systems that are too large to be tractably handled by standard
control and reinforcement learning techniques, as evidenced
also by the large amount of applications and ongoing research
in the field. While there has been some recent progress on
the issue of scalable learning in large-population systems,
important open research questions and synergies with existing
fields of research remain that could be addressed in future
works. In particular, we have given an overview of a variety
of long-standing subject areas dealing with large-scale systems
such as mean-field games, collective intelligence, and complex
networks. It is our belief that intersectional works will be
crucial to the further development of tractable MARL. We
hope that this survey proves useful in bridging the diverse set
of research subjects on large-scale multi-agent systems and
providing new perspectives for future works.
R EFERENCES
[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.
Cambridge, MA, USA: MIT press, 2018.
[2] D. Bertsekas, Reinforcement Learning and Optimal Control. Athena
Scientific, 2019.
[3] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,
M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser
et al., “Starcraft II: A new challenge for reinforcement learning,”
arXiv:1708.04782, 2017. [Online]. Available: https://arxiv.org/abs/
1708.04782
[4] N. Brown and T. Sandholm, “Superhuman AI for multiplayer poker,”
Science, vol. 365, no. 6456, pp. 885–890, 2019.
[5] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison,
D. Farhi, Q. Fischer, S. Hashme, C. Hesse et al., “Dota 2 with large
scale deep reinforcement learning,” arXiv:1912.06680, 2019. [Online].
Available: https://arxiv.org/abs/1912.06680
[6] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,
S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel et al.,
“Mastering atari, go, chess and shogi by planning with a learned
model,” Nature, vol. 588, no. 7839, pp. 604–609, 2020.
[7] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” Int. J. Robot. Res., vol. 32, no. 11, pp. 1238–1274,
2013.
[8] A. S. Polydoros and L. Nalpantidis, “Survey of model-based reinforcement learning: Applications on robotics,” J. Intell. Robot. Syst., vol. 86,
no. 2, pp. 153–173, 2017.

16

[9] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous
driving: A survey,” IEEE Trans. Transp. Syst., pp. 4909–4926, 2022.
[10] J. Tožička, B. Szulyovszky, G. de Chambrier, V. Sarwal, U. Wani, and
M. Gribulis, “Application of deep reinforcement learning to UAV fleet
control,” in Proc. SAI Intell. Syst. Conf., 2018, pp. 1169–1177.
[11] H. X. Pham, H. M. La, D. Feil-Seifer, and A. Nefian, “Cooperative
and distributed reinforcement learning of drones for field coverage,”
arXiv:1803.07250, 2018. [Online]. Available: https://arxiv.org/abs/
1803.07250
[12] Y. F. Chen, M. Everett, M. Liu, and J. P. How, “Socially aware motion
planning with deep reinforcement learning,” in Proc. IROS, 2017, pp.
1343–1350.
[13] D. Wang, T. Fan, T. Han, and J. Pan, “A two-stage reinforcement
learning approach for multi-UAV collision avoidance under imperfect
sensing,” IEEE Robot. Automat. Lett., vol. 5, no. 2, pp. 3098–3105,
2020.
[14] M. Everett, Y. F. Chen, and J. P. How, “Collision avoidance in
pedestrian-rich environments with deep reinforcement learning,” IEEE
Access, vol. 9, pp. 10 357–10 377, 2021.
[15] Y.-J. Hu and S.-J. Lin, “Deep reinforcement learning for optimizing
finance portfolio management,” in Proc. AICAI, 2019, pp. 14–20.
[16] A. Charpentier, R. Elie, and C. Remlinger, “Reinforcement learning in
economics and finance,” Comput. Econ., pp. 1–38, 2021.
[17] M. M. Afsar, T. Crump, and B. Far, “Reinforcement learning based
recommender systems: A survey,” ACM Comput. Surv., pp. 1–37, 2022.
[18] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning:
A selective overview of theories and algorithms,” in Handbook of
Reinforcement Learning and Control, K. G. Vamvoudakis, Y. Wan,
F. L. Lewis, and D. Cansever, Eds. Cham: Springer International
Publishing, 2021, pp. 321–384.
[19] P. Muller, S. Omidshafiei, M. Rowland, K. Tuyls, J. Perolat, S. Liu,
D. Hennes, L. Marris, M. Lanctot, E. Hughes et al., “A generalized
training approach for multiagent learning,” in Proc. ICLR, 2020, pp.
1–35.
[20] G. Papoudakis, F. Christianos, L. Schäfer, and S. V. Albrecht, “Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks,” in Proc. NeurIPS Track Datasets Benchmarks, 2021.
[21] A. Mahajan and M. Mannan, “Decentralized stochastic control,” Ann.
Oper. Res., vol. 241, no. 1, pp. 109–126, 2016.
[22] F. A. Oliehoek and C. Amato, A Concise Introduction to Decentralized
POMDPs. Springer, 2016.
[23] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
of multiagent deep reinforcement learning,” Autonomous Agents and
Multi-Agent Systems, vol. 33, no. 6, pp. 750–797, 2019.
[24] L. Wang, Z. Yang, and Z. Wang, “Breaking the curse of many agents:
Provable mean embedding Q-iteration for mean-field reinforcement
learning,” in Proc. ICML, 2020, pp. 10 092–10 103.
[25] C. Jin, Q. Liu, Y. Wang, and T. Yu, “V-learning–a simple, efficient,
decentralized algorithm for multiagent RL,” in Proc. ICLR 2022
Workshop Gamification Multiagent Solut., 2022.
[26] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The
complexity of decentralized control of Markov decision processes,”
Math. Oper. Res., vol. 27, no. 4, pp. 819–840, 2002.
[27] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou, “The complexity of computing a Nash equilibrium,” SIAM J. Comput., vol. 39,
no. 1, pp. 195–259, 2009.
[28] I. Z. Kiss, J. C. Miller, and P. L. Simon, Mathematics of Epidemics
on Networks: From Exact to Approximate Models. Springer, 2017,
vol. 46.
[29] M. Brambilla, E. Ferrante, M. Birattari, and M. Dorigo, “Swarm
robotics: A review from the swarm engineering perspective,” Swarm
Intell., vol. 7, no. 1, pp. 1–41, 2013.
[30] I. D. Couzin, J. Krause, R. James, G. D. Ruxton, and N. R. Franks,
“Collective memory and spatial sorting in animal groups,” J. Theor.
Biol., vol. 218, no. 1, pp. 1–11, 2002.
[31] A. Cavagna and I. Giardina, “Bird flocks as condensed matter,” Annu.
Rev. Condens. Matter Phys., vol. 5, no. 1, pp. 183–207, 2014.
[32] S. Perrin, M. Laurière, J. Pérolat, M. Geist, R. Élie, and O. Pietquin,
“Mean field games flock! The reinforcement learning way,” in Proc.
IJCAI, 2021, pp. 356–362.
[33] H. H. Wensink, J. Dunkel, S. Heidenreich, K. Drescher, R. E. Goldstein,
H. Löwen, and J. M. Yeomans, “Meso-scale turbulence in living fluids,”
PNAS, vol. 109, no. 36, pp. 14 308–14 313, 2012.
[34] H. Hamann, Swarm Robotics: A Formal Approach. Springer, 2018.

[35] Z. Lin, M. Broucke, and B. Francis, “Local control strategies for groups
of mobile autonomous agents,” IEEE Trans. Automat. Contr., vol. 49,
no. 4, pp. 622–629, 2004.
[36] B. Recht, “A tour of reinforcement learning: The view from continuous
control,” Ann. Rev. Contr. Robot. Auton. Syst., vol. 2, pp. 253–279,
2019.
[37] S. Knorn, Z. Chen, and R. H. Middleton, “Overview: Collective control
of multiagent systems,” IEEE Trans. Contr. Netw. Syst., vol. 3, no. 4,
pp. 334–347, 2015.
[38] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.
[39] R. Bellman, “Dynamic programming,” Science, vol. 153, no. 3731, pp.
34–37, 1966.
[40] C. J. Watkins and P. Dayan, “Q-learning,” Mach. Learn., vol. 8, no. 3,
pp. 279–292, 1992.
[41] L. Baird, “Residual algorithms: Reinforcement learning with function
approximation,” in Proc. ICML. Elsevier, 1995, pp. 30–37.
[42] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control through deep reinforcement learning,”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.
[43] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine Learning, vol. 8, no. 3,
pp. 229–256, 1992.
[44] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient
methods for reinforcement learning with function approximation,” in
Proc. NIPS, 1999, pp. 1057–1063.
[45] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.
521, no. 7553, pp. 436–444, 2015.
[46] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,
“Deep reinforcement learning: A brief survey,” IEEE Sig. Proc. Mag.,
vol. 34, no. 6, pp. 26–38, 2017.
[47] C. V. Goldman and S. Zilberstein, “Decentralized control of cooperative
systems: Categorization and complexity analysis,” J. Artif. Intell. Res.,
vol. 22, pp. 143–174, 2004.
[48] C. Amato, G. Chowdhary, A. Geramifard, N. K. Üre, and M. J.
Kochenderfer, “Decentralized control of partially observable Markov
decision processes,” in Proc. IEEE CDC, 2013, pp. 2398–2405.
[49] D. Szer, F. Charpillet, and S. Zilberstein, “MAA*: a heuristic search
algorithm for solving decentralized POMDPs,” in Proc. UAI, 2005, pp.
576–583.
[50] J. S. Dibangoye, C. Amato, O. Buffet, and F. Charpillet, “Optimally
solving Dec-POMDPs as continuous-state MDPs,” J. Artif. Intell. Res.,
vol. 55, pp. 443–497, 2016.
[51] D. Fudenberg and J. Tirole, Game theory. MIT press, 1991.
[52] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, “Dynamic programming for partially observable stochastic games,” in Proc. AAAI, vol. 4,
2004, pp. 709–715.
[53] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multiagent control using deep reinforcement learning,” in Proc. AAMAS,
2017, pp. 66–83.
[54] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative agents,” in Proc. ICML, 1993, pp. 330–337.
[55] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.
[Online]. Available: https://arxiv.org/abs/1707.06347
[56] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. Torr,
M. Sun, and S. Whiteson, “Is independent learning all you need in the
Starcraft multi-agent challenge?” arXiv:2011.09533, 2020. [Online].
Available: https://arxiv.org/abs/2011.09533
[57] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu,
“The surprising effectiveness of PPO in cooperative, multi-agent
games,” arXiv:2103.01955, 2021. [Online]. Available: https://arxiv.
org/abs/2103.01955
[58] W. Fu, C. Yu, Z. Xu, J. Yang, and Y. Wu, “Revisiting some common
practices in cooperative multi-agent reinforcement learning,” in Proc.
ICML, 2022, pp. 6863–6877.
[59] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
“Value-decomposition networks for cooperative multi-agent learning
based on team reward,” in Proc. AAMAS, vol. 17, 2018, pp. 2085–
2087.
[60] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and
S. Whiteson, “QMIX: Monotonic value function factorisation for deep
multi-agent reinforcement learning,” in Proc. ICML, 2018, pp. 4295–
4304.

17

[61] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted QMIX:
Expanding monotonic value function factorisation for deep multi-agent
reinforcement learning,” in Proc. NeurIPS, vol. 33, 2020, pp. 10 199–
10 210.
[62] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multiagent actor-critic for mixed cooperative-competitive environments,” in
Proc. NIPS, 2017, pp. 6382–6393.
[63] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
“Counterfactual multi-agent policy gradients,” in Proc. AAAI, vol. 32,
no. 1, 2018, pp. 2974–2982.
[64] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning
to communicate with deep multi-agent reinforcement learning,” in
Proc. NIPS, 2016, pp. 2145–2153.
[65] S. Sukhbaatar, A. Szlam, and R. Fergus, “Learning multiagent communication with backpropagation,” in Proc. NIPS, vol. 29, 2016, pp.
2244–2252.
[66] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and
J. Pineau, “TarMAC: Targeted multi-agent communication,” in Proc.
ICML, 2019, pp. 1538–1546.
[67] J. Jiang and Z. Lu, “Learning attentional communication for multiagent cooperation,” in Proc. NeurIPS, 2018, pp. 7265–7275.
[68] T. Wang, J. Wang, C. Zheng, and C. Zhang, “Learning nearly decomposable value functions via communication minimization,” in Proc.
ICLR, 2019, pp. 1–15.
[69] M. Hüttenrauch, S. Adrian, G. Neumann et al., “Deep reinforcement
learning for swarm systems,” J. Mach. Learn. Res., vol. 20, no. 54, pp.
1–31, 2019.
[70] L. Zheng, J. Yang, H. Cai, W. Zhang, J. Wang, and Y. Yu, “Magent:
a many-agent reinforcement learning platform for artificial collective
intelligence,” in Proc. AAAI, 2018, pp. 8222–8223.
[71] J. Suarez, Y. Du, I. Mordach, and P. Isola, “Neural MMO v1. 3: A
massively multiagent game environment for training and evaluating
neural networks,” in Proc. AAMAS, vol. 19, 2020, pp. 2020–2022.
[72] C. Guestrin, D. Koller, and R. Parr, “Multiagent planning with factored
MDPs,” in Proc. NIPS, vol. 14. MIT Press, 2001, pp. 1523–1530.
[73] C. Guestrin, D. Koller, R. Parr, and S. Venkataraman, “Efficient solution
algorithms for factored MDPs,” J. Artif. Intell. Res., vol. 19, pp. 399–
468, 2003.
[74] J. R. Kok and N. Vlassis, “Collaborative multiagent reinforcement
learning by payoff propagation,” J. Mach. Learn. Res., vol. 7, no. 65,
pp. 1789–1828, 2006.
[75] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo, “Networked
distributed POMDPs: A synthesis of distributed constraint optimization
and POMDPs,” in Proc. AAAI, 2005, p. 133–139.
[76] Y. Kim, R. Nair, P. Varakantham, M. Tambe, and M. Yokoo, “Exploiting locality of interaction in networked distributed POMDPs.” in Proc.
AAAI Spring Symp.: Distrib. Plan Sched. Manage., 2006, pp. 41–48.
[77] A. Kumar, S. Zilberstein, and M. Toussaint, “Scalable multiagent planning using probabilistic inference,” in Proc. IJCAI, 2011, p. 2140–2146.
[78] C. Zhang and V. Lesser, “Coordinated multi-agent reinforcement learning in networked distributed POMDPs,” in Proc. AAAI, 2011, pp. 764–
770.
[79] F. A. Oliehoek, S. Whiteson, M. T. Spaan et al., “Approximate solutions
for factored Dec-POMDPs with many agents.” in Proc. AAMAS, 2013,
pp. 563–570.
[80] J. K. Pajarinen and J. T. Peltonen, “Efficient planning for factored
infinite-horizon Dec-POMDPs,” in Proc. IJCAI, 2011, p. 325–331.
[81] F. Wu, S. Zilberstein, and N. R. Jennings, “Monte-carlo expectation
maximization for decentralized POMDPs,” in Proc. IJCAI, 2013, pp.
397–403.
[82] W. Böhmer, V. Kurin, and S. Whiteson, “Deep coordination graphs,”
in Proc. ICML, 2020, pp. 980–991.
[83] Y. Bai, C. Gong, B. Zhang, G. Fan, and X. Hou, “Value function
factorisation with hypergraph convolution for cooperative multi-agent
reinforcement learning,” arXiv:2112.06771, 2021. [Online]. Available:
https://arxiv.org/abs/2112.06771
[84] F. Oliehoek, M. Spaan, S. Whiteson, and N. Vlassis, “Exploiting
locality of interaction in factored Dec-POMDPs,” in Proc. AAMAS,
May 2008, pp. 517–524.
[85] X. Guo, A. Hu, R. Xu, and J. Zhang, “Learning mean-field games,” in
Proc. NeurIPS, 2019, pp. 4966–4976.
[86] B. Anahtarci, C. D. Kariksiz, and N. Saldi, “Q-learning in regularized
mean-field games,” Dyn. Games and Appl., pp. 1–29, 2022.
[87] K. Cui and H. Koeppl, “Approximately solving mean field games via
entropy-regularized deep reinforcement learning,” in Proc. AISTATS,
2021, pp. 1909–1917.

[88] P. Cardaliaguet and S. Hadikhanloo, “Learning in mean field games:
the fictitious play,” ESAIM Control Optim. Calc. Var., vol. 23, no. 2,
pp. 569–591, 2017.
[89] S. Perrin, J. Pérolat, M. Laurière, M. Geist, R. Elie, and O. Pietquin,
“Fictitious play for mean field games: Continuous time analysis and
applications,” in Proc. NeurIPS, vol. 33, 2020, pp. 13 199–13 213.
[90] J. Pérolat, S. Perrin, R. Elie, M. Laurière, G. Piliouras, M. Geist,
K. Tuyls, and O. Pietquin, “Scaling mean field games by online mirror
descent,” in Proc. AAMAS, vol. 21, 2022, pp. 1028–1037.
[91] M. Lauriere, S. Perrin, S. Girgin, P. Muller, A. Jain, T. Cabannes,
G. Piliouras, J. Perolat, R. Elie, O. Pietquin, and M. Geist, “Scalable
deep reinforcement learning algorithms for mean field games,” in Proc.
ICML, 2022, pp. 12 078–12 095.
[92] P. E. Caines and M. Huang, “Graphon mean field games and the GMFG
equations,” in Proc. IEEE CDC, 2018, pp. 4129–4134.
[93] D. Vasal, R. Mishra, and S. Vishwanath, “Sequential decomposition of
graphon mean field games,” in Proc. IEEE ACC, 2021, pp. 730–736.
[94] K. Cui and H. Koeppl, “Learning graphon mean field games and
approximate Nash equilibria,” in Proc. ICLR, 2022, pp. 1–31.
[95] H. Gu, X. Guo, X. Wei, and R. Xu, “Dynamic programming
principles for mean-field controls with learning,” arXiv:1911.07314,
2019. [Online]. Available: https://arxiv.org/abs/1911.07314
[96] ——, “Mean-field controls with Q-learning for cooperative MARL:
convergence and complexity analysis,” SIAM J. Math. Data Sci., vol. 3,
no. 4, pp. 1168–1196, 2021.
[97] R. Carmona, M. Laurière, and Z. Tan, “Model-free mean-field
reinforcement learning: mean-field MDP and mean-field Q-learning,”
arXiv:1910.12802, 2019. [Online]. Available: https://arxiv.org/abs/
1910.12802
[98] A. Mahajan, N. C. Martins, M. C. Rotkowitz, and S. Yüksel, “Information structures in optimal decentralized control,” in Proc. IEEE CDC,
2012, pp. 1291–1306.
[99] O. Hernández-Lerma and J. B. Lasserre, Discrete-time Markov control
processes: basic optimality criteria. Springer Science & Business
Media, 2012, vol. 30.
[100] C. Guestrin, M. Lagoudakis, and R. Parr, “Coordinated reinforcement
learning,” in Proc. ICML, vol. 2, 2002, pp. 227–234.
[101] C. Boutilier, R. Dearden, and M. Goldszmidt, “Exploiting structure in
policy construction,” in Proc. IJCAI, 1995, pp. 1104–1111.
[102] J. R. Kok and N. Vlassis, “Sparse cooperative Q-learning,” in Proc.
ICML, 2004, p. 61.
[103] P. Robbel, F. A. Oliehoek, and M. J. Kochenderfer, “Exploiting
anonymity in approximate linear programming: Scaling to large multiagent MDPs,” in Proc. AAAI Fall Symp. Ser., 2015.
[104] D. Linzner and H. Koeppl, “A variational perturbative approach to
planning in graph-based Markov decision processes,” in Proc. AAAI,
vol. 34, no. 05, 2020, pp. 7203–7210.
[105] Y. Lin, G. Qu, L. Huang, and A. Wierman, “Multi-agent reinforcement
learning in stochastic networked systems,” in Proc. NeurIPS, 2021, pp.
7825–7837.
[106] G. Qu, A. Wierman, and N. Li, “Scalable reinforcement learning of
localized policies for multi-agent networked systems,” in Proc. Learn.
Dyn. Contr., 2020, pp. 256–266.
[107] G. Qu, Y. Lin, A. Wierman, and N. Li, “Scalable multi-agent reinforcement learning for networked systems with average reward,” in Proc.
NeurIPS, vol. 33, 2020, pp. 2074–2086.
[108] D. Gamarnik, “Correlation decay method for decision, optimization,
and inference in large-scale networks,” in Theory Driven by Influential
Applications. INFORMS, 2013, pp. 108–121.
[109] A. Nedic and A. Ozdaglar, “Distributed subgradient methods for multiagent optimization,” IEEE Trans. Automat. Contr., vol. 54, no. 1, pp.
48–61, 2009.
[110] P. Varshavskaya, L. P. Kaelbling, and D. Rus, “Efficient distributed
reinforcement learning through agreement,” in Distributed Autonomous
Robotic Systems 8. Springer, 2009, pp. 367–378.
[111] A. Agarwal and J. C. Duchi, “Distributed delayed stochastic optimization,” in Proc. NIPS, 2011, pp. 873–881.
[112] D. Jakovetic, J. Xavier, and J. M. Moura, “Cooperative convex optimization in networked systems: Augmented Lagrangian algorithms
with directed gossip communication,” IEEE Trans. Sig. Proc., vol. 59,
no. 8, pp. 3889–3902, 2011.
[113] S.-Y. Tu and A. H. Sayed, “Diffusion strategies outperform consensus
strategies for distributed estimation over adaptive networks,” IEEE
Trans. Sig. Proc., vol. 60, no. 12, pp. 6217–6234, 2012.
[114] M. Hong and T.-H. Chang, “Stochastic proximal gradient consensus
over random networks,” IEEE Trans. Sig. Proc., vol. 65, no. 11, pp.
2933–2948, 2017.

18

[115] A. Nedic, A. Olshevsky, and W. Shi, “Achieving geometric convergence
for distributed optimization over time-varying graphs,” SIAM J. Optim.,
vol. 27, no. 4, pp. 2597–2633, 2017.
[116] A. Wang, T. Dong, and X. Liao, “Distributed optimal consensus
algorithms in multi-agent systems,” Neurocomputing, vol. 339, pp. 26–
35, 2019.
[117] P. Pennesi and I. C. Paschalidis, “A distributed actor-critic algorithm
and applications to mobile sensor network coordination problems,”
IEEE Trans. Automat. Contr., vol. 55, no. 2, pp. 492–497, 2010.
[118] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized
multi-agent reinforcement learning with networked agents,” in Proc.
ICML, 2018, pp. 5872–5881.
[119] K. Zhang, Z. Yang, and T. Basar, “Networked multi-agent reinforcement learning in continuous spaces,” in Proc. IEEE CDC, 2018, pp.
2771–2776.
[120] K. Ciosek and S. Whiteson, “Expected policy gradients,” in Proc. AAAI,
2018, pp. 2868–2875.
[121] R. van der Hofstad, Random Graphs and Complex Networks: Volume
1. USA: Cambridge University Press, 2016.
[122] R. Albert and A.-L. Barabási, “Statistical mechanics of complex
networks,” Rev. Mod. Phys., vol. 74, no. 1, p. 47, 2002.
[123] A. Barrat, M. Barthlemy, and A. Vespignani, Dynamical Processes on
Complex Networks. USA: Cambridge University Press, 2008.
[124] Á. Bodó, G. Y. Katona, and P. L. Simon, “SIS epidemic propagation
on hypergraphs,” Bull. Math. Biol., vol. 78, no. 4, pp. 713–735, 2016.
[125] N. W. Landry and J. G. Restrepo, “The effect of heterogeneity on
hypergraph contagion models,” Chaos, vol. 30, no. 10, p. 103117, 2020.
[126] J. Noonan and R. Lambiotte, “Dynamics of majority rule on hypergraphs,” Phys. Rev. E, vol. 104, no. 2, p. 024316, 2021.
[127] X.-J. Xu, S. He, and L.-J. Zhang, “Dynamics of the threshold model
on hypergraphs,” Chaos, vol. 32, no. 2, p. 023125, 2022.
[128] M. A. Porter, “Nonlinearity+ networks: A 2020 vision,” in Emerging
frontiers in nonlinear science. Springer, 2020, pp. 131–159.
[129] F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania,
J.-G. Young, and G. Petri, “Networks beyond pairwise interactions:
structure and dynamics,” Phys. Rep., vol. 874, pp. 1–92, 2020.
[130] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Arruda,
B. Franceschiello, I. Iacopini, S. Kéfi, V. Latora, Y. Moreno et al.,
“The physics of higher-order interactions in complex systems,” Nature
Phys., vol. 17, no. 10, pp. 1093–1098, 2021.
[131] D. Zhou, J. Huang, and B. Schölkopf, “Learning with hypergraphs:
clustering, classification, and embedding,” in Proc. NIPS, 2006, pp.
1601–1608.
[132] J. Jiang, C. Dun, T. Huang, and Z. Lu, “Graph convolutional reinforcement learning,” in Proc. ICLR, 2019, pp. 1–13.
[133] S. Simon and D. Wojtczak, “Synchronisation games on hypergraphs,”
in Proc. IJCAI, 2017, pp. 402–408.
[134] B. Zhang, Y. Bai, Z. Xu, D. Li, and G. Fan, “Efficient
cooperation strategy generation in multi-agent video games via
hypergraph neural network,” arXiv:2203.03265, 2022. [Online].
Available: https://arxiv.org/abs/2203.03265
[135] L. Rose, S. Lasaulce, S. M. Perlaza, and M. Debbah, “Learning
equilibria with partial information in decentralized wireless networks,”
IEEE Commun. Mag., vol. 49, no. 8, pp. 136–142, 2011.
[136] H. Sayama, I. Pestov, J. Schmidt, B. J. Bush, C. Wong, J. Yamanoi,
and T. Gross, “Modeling complex systems with adaptive networks,”
Comput. Math. Appl., vol. 65, no. 10, pp. 1645–1664, 2013.
[137] N. Nezamoddini and A. Gholami, “A survey of adaptive multi-agent
networks and their applications in smart cities,” Smart Cities, vol. 5,
no. 1, pp. 318–347, 2022.
[138] A. H. Sayed, “Adaptive networks,” Proc. IEEE, vol. 102, no. 4, pp.
460–497, 2014.
[139] F. Oliehoek, S. Whiteson, and M. Spaan, “Exploiting structure in
cooperative Bayesian games,” in Proc. UAI, August 2012, pp. 654–
664.
[140] N. Creech, N. C. Pacheco, and S. Miles, “Dynamic neighbourhood
optimisation for task allocation using multi-agent,” arXiv:2102.08307,
2021. [Online]. Available: https://arxiv.org/abs/2102.08307
[141] Z. Mammeri, “Reinforcement learning based routing in networks:
Review and classification of approaches,” IEEE Access, vol. 7, pp.
55 916–55 950, 2019.
[142] J. A. Boyan and M. L. Littman, “Packet routing in dynamically
changing networks: a reinforcement learning approach,” in Proc. NIPS,
1993, pp. 671–678.
[143] S. P. Choi and D.-Y. Yeung, “Predictive Q-routing: a memory-based
reinforcement learning approach to adaptive traffic control,” in Proc.
NIPS, 1995, pp. 945–951.

[144] D. Oužecki and D. Jevtić, “Reinforcement learning as adaptive network
routing of mobile agents,” in Proc. Int. Conv. MIPRO, 2010, pp. 479–
484.
[145] S. KUMAR, “Dual reinforcement Q-routing: An on-line adaptive
routing algorithm,” in Proc. Artif. Neural Netw. Eng. Conf., 1998, pp.
231–238.
[146] A. M. Lopez and D. R. Heisterkamp, “Simulated annealing based
hierarchical Q-routing: A dynamic routing protocol,” in Proc. Int. Conf.
Inf. Technol. New Gener., 2011, pp. 791–796.
[147] Y. T. Valdivia, M. M. Vellasco, and M. A. Pacheco, “An adaptive network routing strategy with temporal differences,” Inteligencia Artificial.
Revista Iberoamericana de Inteligencia Artificial, vol. 5, no. 12, pp.
85–91, 2001.
[148] H. Matsuo and K. Mori, “Accelerated ants routing in dynamic networks,” in Proc. Int. Conf. Softw. Eng. Artif. Intell. Netw. Parallel/Distrib. Comput., 2001, pp. 333–339.
[149] D. Subramanian, P. Druschel, and J. Chen, “Ants and reinforcement
learning: a case study in routing in dynamic networks,” in Proc. IJCAI,
1997, pp. 832–838.
[150] R. Schoonderwoerd, O. E. Holland, J. L. Bruten, and L. J. Rothkrantz,
“Ant-based load balancing in telecommunications networks,” Adaptive
Behav., vol. 5, no. 2, pp. 169–207, 1997.
[151] G. Di Caro and M. Dorigo, “Mobile agents for adaptive routing,” in
Proc. Hawaii Int. Conf. Syst. Sci., vol. 31, 1998, pp. 74–85.
[152] J. Dowling, E. Curran, R. Cunningham, and V. Cahill, “Using feedback
in collaborative reinforcement learning to adaptively optimize MANET
routing,” IEEE Trans. Syst. Man Cybern. A: Syst. and Humans, vol. 35,
no. 3, pp. 360–372, 2005.
[153] R. A. Haraty and B. Traboulsi, “MANET with the Q-routing protocol,”
in Proc. Int. Conf. Netw., 2012, pp. 187–192.
[154] A. Bitaillou, B. Parrein, and G. Andrieux, “New results on Q-routing
protocol for wireless networks,” in Proc. Int. Conf. Ad Hoc Netw., 2020,
pp. 29–43.
[155] P. Cong, Y. Zhang, Z. Liu, T. Baker, H. Tawfik, W. Wang, K. Xu,
R. Li, and F. Li, “A deep reinforcement learning-based multi-optimality
routing scheme for dynamic IoT networks,” Comput. Netw., vol. 192,
p. 108057, 2021.
[156] R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and A. Vespignani,
“Epidemic processes in complex networks,” Rev. Mod. Phys., vol. 87,
pp. 925–979, 08 2015.
[157] A. W. Wijayanto and T. Murata, “Learning adaptive graph protection
strategy on dynamic networks via reinforcement learning,” in Proc.
IEEE/WIC/ACM Int. Conf. Web Intell., 2018, pp. 534–539.
[158] ——, “Effective and scalable methods for graph protection strategies
against epidemics on dynamic networks,” Appl. Netw. Sci., vol. 4, no. 1,
pp. 1–31, 2019.
[159] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean
field multi-agent reinforcement learning,” in Proc. ICML, 2018, pp.
5571–5580.
[160] J. P. Gleeson, “Binary-state dynamics on complex networks: Pair
approximation and beyond,” Phys. Rev. X, vol. 3, p. 021004, 04 2013.
[161] A. Gosavi, “Reinforcement learning: A tutorial survey and recent
advances,” INFORMS J. Comput., vol. 21, no. 2, pp. 178–192, 2009.
[162] S. Ganapathi Subramanian, M. E. Taylor, M. Crowley, and P. Poupart,
“Partially observable mean field reinforcement learning,” in Proc.
AAMAS, vol. 20, 2021, pp. 537–545.
[163] S. Ganapathi Subramanian, P. Poupart, M. E. Taylor, and N. Hegde,
“Multi type mean field reinforcement learning,” in Proc. AAMAS,
vol. 19, 2020, pp. 411–419.
[164] S. G. Subramanian, M. E. Taylor, M. Crowley, and P. Poupart,
“Decentralized mean field games,” in Proc. AAAI, vol. 36, no. 9, 2022,
pp. 9439–9447.
[165] H. Gu, X. Guo, X. Wei, and R. Xu, “Mean-field multiagent reinforcement learning: A decentralized network approach,”
arXiv:2108.02731, 2021. [Online]. Available: https://arxiv.org/abs/
2108.02731
[166] M. Huang, R. P. Malhamé, and P. E. Caines, “Large population
stochastic dynamic games: closed-loop McKean-Vlasov systems and
the Nash certainty equivalence principle,” Commun. Inf. Syst., vol. 6,
no. 3, pp. 221–252, 2006.
[167] J.-M. Lasry and P.-L. Lions, “Mean field games,” Japanese J. Math.,
vol. 2, no. 1, pp. 229–260, 2007.
[168] Y. Achdou and I. Capuzzo-Dolcetta, “Mean field games: numerical
methods,” SIAM J. Numer. Anal., vol. 48, no. 3, pp. 1136–1162, 2010.
[169] A. Bensoussan, J. Frehse, P. Yam et al., Mean field games and mean
field type control theory. Springer, 2013, vol. 101.

19

[170] P. E. Caines, “Mean field games,” in Encyclopedia of systems and
control. Springer, 2021, pp. 1197–1202.
[171] N. Saldi, T. Basar, and M. Raginsky, “Markov–Nash equilibria in meanfield games with discounted cost,” SIAM J. Contr. Optim., vol. 56, no. 6,
pp. 4256–4287, 2018.
[172] M. Laurière, S. Perrin, M. Geist, and O. Pietquin, “Learning mean
field games: A survey,” arXiv:2205.12944, 2022. [Online]. Available:
https://arxiv.org/abs/2205.12944
[173] J. Yang, X. Ye, R. Trivedi, H. Xu, and H. Zha, “Learning deep mean
field games for modeling large population behavior,” in Proc. ICLR,
2018, pp. 1–15.
[174] Y. Chen, J. Liu, and B. Khoussainov, “Agent-level maximum
entropy inverse reinforcement learning for mean field games,”
arXiv:2104.14654, 2021. [Online]. Available: https://arxiv.org/abs/
2104.14654
[175] Y. Chen, L. Zhang, J. Liu, and S. Hu, “Individual-level inverse
reinforcement learning for mean field games,” in Proc. AAMAS, 2022,
p. 253–262.
[176] X. Guo, A. Hu, R. Xu, and J. Zhang, “A general framework for learning
mean-field games,” Math. Oper. Res., 2022.
[177] B. Anahtarci, C. D. Kariksiz, and N. Saldi, “Learning in discrete-time
average-cost mean-field games,” in Proc. IEEE CDC, 2021, pp. 3048–
3053.
[178] X. Guo, R. Xu, and T. Zariphopoulou, “Entropy regularization for mean
field games with learning,” Math. Oper. Res., 2022.
[179] D. Mguni, J. Jennings, and E. M. de Cote, “Decentralised learning in
systems with many, many strategic agents,” in Proc. AAAI, vol. 32,
2018, pp. 4686–4693.
[180] J. Bonnans, P. Lavigne, and L. Pfeiffer, “Generalized conditional gradient and learning in potential mean field games,” arXiv:2109.05785,
2021. [Online]. Available: https://arxiv.org/abs/2109.05785
[181] X. Guo, A. Hu, and J. Zhang, “MF-OMO: An optimization formulation
of mean-field games,” arXiv:2206.09608, 2022. [Online]. Available:
https://arxiv.org/abs/2206.09608
[182] L. Campi and M. Fischer, “Correlated equilibria and mean field games:
a simple model,” Math. Oper. Res., 2022.
[183] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat,
D. Silver, and T. Graepel, “A unified game-theoretic approach to
multiagent reinforcement learning,” in Proc. NIPS, vol. 30, 2017, pp.
1–14.
[184] P. Muller, M. Rowland, R. Elie, G. Piliouras, J. Perolat, M. Lauriere,
R. Marinier, O. Pietquin, and K. Tuyls, “Learning equilibria in meanfield games: Introducing mean-field PSRO,” in Proc. AAMAS, vol. 20,
2021, p. 926–934.
[185] D. Andersson and B. Djehiche, “A maximum principle for sdes of
mean-field type,” Appl. Math. Optim., vol. 63, no. 3, pp. 341–356,
2011.
[186] H. Pham and X. Wei, “Bellman equation and viscosity solutions for
mean-field stochastic control problem,” ESAIM Contr. Optim. Calc.
Var., vol. 24, no. 1, pp. 437–461, 2018.
[187] M. Motte and H. Pham, “Mean-field Markov decision processes with
common noise and open-loop controls,” Ann. Appl. Probab., vol. 32,
no. 2, pp. 1421–1458, 2022.
[188] J. Subramanian and A. Mahajan, “Reinforcement learning in stationary
mean-field games,” in Proc. AAMAS, vol. 18, 2019, pp. 251–259.
[189] W. U. Mondal, M. Agarwal, V. Aggarwal, and S. V. Ukkusuri, “On the
approximation of cooperative heterogeneous multi-agent reinforcement
learning (MARL) using mean field control (MFC),” J. Mach. Learn.
Res., vol. 23, no. 129, pp. 1–46, 2022.
[190] B. Pasztor, I. Bogunovic, and A. Krause, “Efficient model-based
multi-agent mean-field reinforcement learning,” arXiv:2107.04050,
2021. [Online]. Available: https://arxiv.org/abs/2107.04050
[191] K. Cui, A. Tahir, M. Sinzger, and H. Koeppl, “Discrete-time mean
field control with environment states,” in Proc. IEEE CDC, 2021, pp.
5239–5246.
[192] W. U. Mondal, V. Aggarwal, and S. V. Ukkusuri, “Can mean field
control (MFC) approximate cooperative multi agent reinforcement
learning (MARL) with non-uniform interaction?” arXiv:2203.00035,
2022. [Online]. Available: https://arxiv.org/abs/2203.00035
[193] Y. Pan, A.-m. Farahmand, M. White, S. Nabi, P. Grover, and
D. Nikovski, “Reinforcement learning with function-valued action
spaces for partial differential equation control,” in Proc. ICML, 2018,
pp. 3986–3995.
[194] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning.” in Proc. ICLR, 2016, pp. 1–14.

[195] R. Carmona, M. Laurière, and Z. Tan, “Linear-quadratic mean-field
reinforcement learning: convergence of policy gradient methods,”
arXiv:1910.04295, 2019. [Online]. Available: https://arxiv.org/abs/
1910.04295
[196] Y. Luo, Z. Yang, Z. Wang, and M. Kolar, “Natural actor-critic converges
globally for hierarchical linear quadratic regulator,” arXiv:1912.06875,
2019. [Online]. Available: https://arxiv.org/abs/1912.06875
[197] D. Milutinović and P. Lima, “Modeling and optimal centralized control
of a large-size robotic population,” IEEE Trans. Robot., vol. 22, no. 6,
pp. 1280–1285, 2006.
[198] T. Zheng, Q. Han, and H. Lin, “Transporting robotic swarms via meanfield feedback control,” IEEE Trans. Automat. Contr., vol. 67, no. 8,
pp. 4170–4177, 2021.
[199] S. Gao and P. E. Caines, “The control of arbitrary size networks of
linear systems via graphon limits: An initial investigation,” in Proc.
IEEE CDC, 2017, pp. 1052–1057.
[200] P. E. Caines and M. Huang, “Graphon mean field games and the GMFG
equations: ε-Nash equilibria,” in Proc. IEEE CDC, 2019, pp. 286–292.
[201] L. Lovász, Large networks and graph limits. Am. Math. Soc., 2012,
vol. 60.
[202] F. Parise and A. Ozdaglar, “Graphon games,” in Proc. ACM Conf. Econ.
Comput., 2019, pp. 457–458.
[203] R. Carmona, D. Cooney, C. Graves, and M. Lauriere, “Stochastic
graphon games: I. the static case,” Math. Oper. Res., vol. 47, no. 1, pp.
750–778, 2021.
[204] E. Bayraktar, S. Chakraborty, and R. Wu, “Graphon mean
field systems,” arXiv:2003.13180, 2020. [Online]. Available: https:
//arxiv.org/abs/2003.13180
[205] G. Bet, F. Coppini, and F. R. Nardi, “Weakly interacting oscillators on
dense random graphs,” arXiv:2006.07670, 2020. [Online]. Available:
https://arxiv.org/abs/2006.07670
[206] K. Cui, W. R. KhudaBukhsh, and H. Koeppl, “Motif-based mean-field
approximation of interacting particles on clustered networks,” Phys.
Rev. E, vol. 105, p. L042301, 2022.
[207] ——, “Hypergraphon mean field games,” arXiv:2203.16223, 2022.
[Online]. Available: https://arxiv.org/abs/2203.16223
[208] M. A. Gkogkas and C. Kuehn, “Graphop mean-field limits for
Kuramoto-type models,” SIAM J. Appl. Dyn. Syst., vol. 21, no. 1, pp.
248–283, 2022.
[209] D. Lacker and A. Soret, “A case study on stochastic games on large
graphs in mean field and sparse regimes,” Math. Oper. Res., vol. 47,
no. 2, pp. 1530–1565, 2021.
[210] M. Huang, P. E. Caines, R. P. Malhamé et al., “Distributed multi-agent
decision-making with partial observations: asymptotic Nash equilibria,”
in Proc. 17th Internat. Symp. MTNS, 2006, pp. 2725–2730.
[211] J. Huang and S. Wang, “A class of mean-field lqg games with
partial information,” arXiv:1403.5859, 2014. [Online]. Available:
https://arxiv.org/abs/1403.5859
[212] M. Nourian and P. E. Caines, “-Nash mean field game theory for
nonlinear stochastic dynamical systems with major and minor agents,”
SIAM J. Contr. Optim., vol. 51, no. 4, pp. 3302–3331, 2013.
[213] N. Şen and P. E. Caines, “Mean field games with partially observed
major player and stochastic mean field,” in Proc. IEEE CDC, 2014,
pp. 2709–2715.
[214] ——, “Mean field game theory with a partially observed major agent,”
SIAM J. Contr. Optim., vol. 54, no. 6, pp. 3174–3224, 2016.
[215] ——, “Mean field games with partial observation,” SIAM J. Contr.
Optim., vol. 57, no. 3, pp. 2064–2091, 2019.
[216] N. Saldi, T. Başar, and M. Raginsky, “Approximate Nash equilibria
in partially observed stochastic games with mean-field interactions,”
Math. Oper. Res., vol. 44, no. 3, pp. 1006–1033, 2019.
[217] T. Nie and K. Yan, “Extended mean-field control problem with partial
observation,” ESAIM Control Optim. Calc. Var., vol. 28, p. 17, 2022.
[218] D. H. Wolpert and K. Tumer, “An introduction to collective
intelligence,” arXiv:cs/9908014, 1999. [Online]. Available: https:
//arxiv.org/abs/cs/9908014
[219] D. H. Wolpert, K. R. Wheeler, and K. Tumer, “Collective intelligence
for control of distributed dynamical systems,” EPL, vol. 49, no. 6, p.
708, 2000.
[220] M. S. Innocente and P. Grasso, “Self-organising swarms of firefighting
drones: Harnessing the power of collective intelligence in decentralised
multi-robot systems,” J. Comput. Sci., vol. 34, pp. 80–101, 2019.
[221] J. Su, S. Yu, B. Li, and Y. Ye, “Distributed and collective intelligence
for computation offloading in aerial edge networks,” IEEE Trans.
Transp. Syst., 2022.

20

[222] R. Solé, D. R. Amor, S. Duran-Nebreda, N. Conde-Pueyo,
M. Carbonell-Ballestero, and R. Montañez, “Synthetic collective intelligence,” Biosystems, vol. 148, pp. 47–61, 2016.
[223] G. Elia and A. Margherita, “Can we solve wicked problems? a
conceptual framework and a collective intelligence system to support
problem analysis and solution design for complex social issues,”
Technol. Forecasting Soc. Change, vol. 133, pp. 279–286, 2018.
[224] D. Ha and Y. Tang, “Collective intelligence for deep learning: A
survey of recent developments,” arXiv:2111.14377, 2021. [Online].
Available: https://arxiv.org/abs/2111.14377
[225] D. Pathak, C. Lu, T. Darrell, P. Isola, and A. A. Efros, “Learning
to control self-assembling morphologies: a study of generalization via
modularity,” in Proc. NeurIPS, 2019.
[226] A. Iglesias, A. Gálvez, and P. Suárez, “Swarm robotics–a case study:
bat robotics,” in Nature-Inspired Computation and Swarm Intelligence.
Elsevier, 2020, pp. 273–302.
[227] A. Šošić, A. M. Zoubir, and H. Koeppl, “Reinforcement learning in a
continuum of agents,” Swarm Intell., vol. 12, no. 1, pp. 23–51, 2018.
[228] N. Cambier, V. Frémont, V. Trianni, and E. Ferrante, “Embodied
evolution of self-organised aggregation by cultural propagation,” in
Proc. Int. Conf. Swarm Intell., 2018, pp. 351–359.
[229] N. Correll and A. Martinoli, “Modeling and designing self-organized
aggregation in a swarm of miniature robots,” Int. J. Robot. Res., vol. 30,
no. 5, pp. 615–626, 2011.
[230] M. Gauci, J. Chen, W. Li, T. J. Dodd, and R. Groß, “Self-organized
aggregation without computation,” Int. J. Robot. Res., vol. 33, no. 8,
pp. 1145–1161, 2014.
[231] S. Camazine, J.-L. Deneubourg, N. R. Franks, J. Sneyd, G. Theraula,
and E. Bonabeau, “Self-organization in biological systems,” in SelfOrganization in Biological Systems. Princeton university press, 2020.
[232] J.-A. Meyer and S. W. Wilson, “Task differentiation in polistes wasp
colonies: a model for self-organizing groups of robots,” in From
Animals to Animats: Proceedings of the First International Conference
on Simulation of Adaptive Behavior. MIT Press, 1991, pp. 346–355.
[233] V. Sperati, V. Trianni, and S. Nolfi, “Self-organised path formation in
a swarm of robots,” Swarm Intell., vol. 5, no. 2, pp. 97–119, 2011.
[234] R. Fujisawa, S. Dobata, K. Sugawara, and F. Matsuno, “Designing
pheromone communication in swarm robotics: Group foraging behavior
mediated by chemical substance,” Swarm Intell., vol. 8, no. 3, pp. 227–
246, 2014.
[235] B. Shucker and J. K. Bennett, “Scalable control of distributed robotic
macrosensors,” in Distributed Autonomous Robotic Systems 6, 2007,
pp. 379–388.
[236] H. Oh, A. R. Shirazi, C. Sun, and Y. Jin, “Bio-inspired self-organising
multi-robot pattern formation: A review,” Robot. Auton. Syst., vol. 91,
pp. 83–100, 2017.
[237] W. Lee, N. Vaughan, and D. Kim, “Task allocation into a foraging
task with a series of subtasks in swarm robotic system,” IEEE Access,
vol. 8, pp. 107 549–107 561, 2020.
[238] S. Camazine, N. R. Franks, J. Sneyd, E. Bonabeau, J.-L. Deneubourg,
and G. Theraula, Self-Organization in Biological Systems. Princeton
University Press, 2001.
[239] M. A. Montes de Oca, E. Ferrante, A. Scheidler, C. Pinciroli, M. Birattari, and M. Dorigo, “Majority-rule opinion dynamics with differential
latency: a mechanism for self-organized collective decision-making,”
Swarm Intell., vol. 5, no. 3, pp. 305–327, 2011.
[240] C.-H. Yu, J. Werfel, and R. Nagpal, “Collective decision-making in
multi-agent systems by implicit leadership,” in Proc. AAMAS, vol. 3,
2010, p. 1189–1196.
[241] J. Qin, Q. Ma, Y. Shi, and L. Wang, “Recent advances in consensus
of multi-agent systems: A brief survey,” IEEE Trans. Ind. Electron.,
vol. 64, no. 6, pp. 4972–4983, 2016.
[242] M. Dorigo, M. Birattari, and T. Stutzle, “Ant colony optimization,”
IEEE Comput. Intell. Mag., vol. 1, no. 4, pp. 28–39, 2006.
[243] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in Proc.
Int. Conf. Neural Netw., vol. 4, 1995, pp. 1942–1948.
[244] A. Okubo, “Dynamical aspects of animal grouping: swarms, schools,
flocks, and herds,” Adv. Biophys., vol. 22, pp. 1–94, 1986.
[245] J. K. Parrish, S. V. Viscido, and D. Grunbaum, “Self-organized fish
schools: an examination of emergent properties,” Biol. Bull., vol. 202,
no. 3, pp. 296–305, 2002.
[246] R. Ourari, K. Cui, A. Elshamanhory, and H. Koeppl, “Nearestneighbor-based collision avoidance for quadrotors via reinforcement
learning,” in Proc. IEEE ICRA, 2022, pp. 293–300.
[247] K. N. McGuire, G. C. de Croon, and K. Tuyls, “A comparative study
of bug algorithms for robot navigation,” Robot. Auton. Syst., vol. 121,
p. 103261, 2019.

[248] P. Golle, K. Leyton-Brown, I. Mironov, and M. Lillibridge, “Incentives
for sharing in peer-to-peer networks,” in Proc. Int. Workshop Electron.
Commerce, 2001, pp. 75–87.
[249] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep
reinforcement learning,” in Proc. IntelliSys, 2017, pp. 905–913.
[250] A. Schaerf, Y. Shoham, and M. Tennenholtz, “Adaptive load balancing:
A study in multi-agent learning,” J. Artif. Intell. Res., vol. 2, pp. 475–
500, 1994.
[251] D. Lipshutz, “Open problem—load balancing using delayed information,” Stoch. Syst., vol. 9, no. 3, pp. 305–306, 2019.
[252] N. Gast and B. Van Houdt, “A refined mean field approximation,” ACM
SIGMETRICS Perform. Eval. Rev., vol. 46, no. 1, pp. 113–113, 2018.
[253] A. Tahir, K. Cui, and H. Koeppl, “Learning mean-field control
for delayed information load balancing in large queuing systems,”
arXiv:2208.04777, 2022. [Online]. Available: https://arxiv.org/abs/
2208.04777
[254] S. Kar, R. Rehrmann, A. Mukhopadhyay, B. Alt, F. Ciucu, H. Koeppl,
C. Binnig, and A. Rizk, “On the throughput optimization in large-scale
batch-processing systems,” Perform. Eval., vol. 144, p. 102142, 2020.
[255] A. F. Hanif, H. Tembine, M. Assaad, and D. Zeghlache, “Meanfield games for resource sharing in cloud-based networks,” IEEE/ACM
Trans. Netw., vol. 24, no. 1, pp. 624–637, 2015.
[256] R. Zheng, H. Wang, and M. De Mari, “Optimal computation offloading
with a shared mec center: A mean field game approach,” in Proc. IEEE
GLOBECOM Workshops, 2019, pp. 1–6.
[257] R. A. Banez, L. Li, C. Yang, L. Song, and Z. Han, “A mean-field-type
game approach to computation offloading in mobile edge computing
networks,” in Proc. ICC, 2019, pp. 1–6.
[258] W. R. KhudaBukhsh, S. Kar, B. Alt, A. Rizk, and H. Koeppl, “Generalized cost-based job scheduling in very large heterogeneous cluster
systems,” IEEE Trans. Parallel Distrib. Syst., vol. 31, no. 11, pp. 2594–
2604, 2020.
[259] M. Andrews, S. Borst, J. Lee, E. Martin-Lopez, and K. Palyutina,
“Tracking the state of large dynamic networks via reinforcement
learning,” in Proc. IEEE INFOCOM, 2020, pp. 416–425.
[260] X. Foukas, G. Patounas, A. Elmokashfi, and M. K. Marina, “Network
slicing in 5g: Survey and challenges,” IEEE Commun. Mag., vol. 55,
no. 5, pp. 94–100, 2017.
[261] F. Wei, G. Feng, Y. Sun, Y. Wang, and Y.-C. Liang, “Dynamic network
slice reconfiguration by exploiting deep reinforcement learning,” in
Proc. ICC, 2020, pp. 1–6.
[262] F. Wei, G. Feng, Y. Sun, Y. Wang, S. Qin, and Y.-C. Liang, “Network
slice reconfiguration by exploiting deep reinforcement learning with
large action space,” IEEE Trans. Netw. Service Manage., vol. 17, no. 4,
pp. 2197–2211, 2020.
[263] C. Qi, Y. Hua, R. Li, Z. Zhao, and H. Zhang, “Deep reinforcement
learning with discrete normalized advantage functions for resource
management in network slicing,” IEEE Commun. Lett., vol. 23, no. 8,
pp. 1337–1341, 2019.
[264] S. De Bast, R. Torrea-Duran, A. Chiumento, S. Pollin, and H. Gacanin,
“Deep reinforcement learning for dynamic network slicing in ieee
802.11 networks,” in Proc. IEEE INFOCOM Workshops, 2019, pp.
264–269.
[265] H. A. Al-Rawi, M. A. Ng, and K.-L. A. Yau, “Application of reinforcement learning to routing in distributed wireless networks: A review,”
Artif. Intell. Rev., vol. 43, no. 3, pp. 381–416, 2015.
[266] I. Kovalev, A. Voroshilova, and M. Karaseva, “Analysis of the current
situation and development trend of the international cargo UAVs
market,” in Proc. J. Phys.: Conf. Ser., vol. 1399, no. 5. IOP Publishing,
2019, p. 055095.
[267] D. Câmara, “Cavalry to the rescue: Drones fleet to help rescuers
operations over disasters scenarios,” in Proc. IEEE CAMA, 2014, pp.
1–4.
[268] Y. Karaca, M. Cicek, O. Tatli, A. Sahin, S. Pasli, M. F. Beser, and
S. Turedi, “The potential use of unmanned aircraft systems (drones) in
mountain search and rescue operations,” Am. J. Emerg. Med., vol. 36,
no. 4, pp. 583–588, 2018.
[269] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita,
I. Khalil, N. S. Othman, A. Khreishah, and M. Guizani, “Unmanned
aerial vehicles (UAVs): A survey on civil applications and key research
challenges,” IEEE Access, vol. 7, pp. 48 572–48 634, 2019.
[270] G. Chmaj and H. Selvaraj, “Distributed processing applications for
UAV/drones: A survey,” in Progress in Systems Engineering. Springer,
2015, pp. 449–454.
[271] M. Schranz, G. A. Di Caro, T. Schmickl, W. Elmenreich, F. Arvin,
A. Şekercioğlu, and M. Sende, “Swarm intelligence and cyber-physical

21

systems: Concepts, challenges and future trends,” Swarm Evol. Comput., vol. 60, p. 100762, 2021.
[272] Z. Zhang, D. Zhang, and R. C. Qiu, “Deep reinforcement learning for
power system applications: An overview,” CSEE J. Power Energy Syst.,
vol. 6, no. 1, pp. 213–225, 2019.
[273] A. Castelletti, G. Corani, A. Rizzolli, R. Soncinie-Sessa, and E. Weber,
“Reinforcement learning in the operational management of a water
system,” in Proc. IFAC Workshop on Model. Contr. in Environmental
Issues, 2002, pp. 325–330.
[274] S. Brandi, M. S. Piscitelli, M. Martellacci, and A. Capozzoli, “Deep
reinforcement learning to optimise indoor temperature control and
heating energy consumption in buildings,” Energy Build., vol. 224, p.
110225, 2020.
[275] F. Bagagiolo and D. Bauso, “Mean-field games and dynamic demand
management in power grids,” Dyn. Games Appl., vol. 4, no. 2, pp.
155–176, 2014.
[276] Z. Ma, D. S. Callaway, and I. A. Hiskens, “Decentralized charging
control of large populations of plug-in electric vehicles,” IEEE Trans.
Contr. Syst. Technol., vol. 21, no. 1, pp. 67–78, 2011.
[277] Z. Ma, N. Yang, S. Zou, and Y. Shao, “Charging coordination of plugin electric vehicles in distribution networks with capacity constrained
feeder lines,” IEEE Trans. Contr. Syst. Technol., vol. 26, no. 5, pp.
1917–1924, 2017.
[278] A. C. Kizilkale and R. P. Malhame, “Collective target tracking mean
field control for electric space heaters,” in Proc. Mediterranean Conf.
Contr. Automat., 2014, pp. 829–834.
[279] Y. Wang, C. Chen, J. Wang, and R. Baldick, “Research on resilience of
power systems under natural disasters—a review,” IEEE Trans. Power
Syst., vol. 31, no. 2, pp. 1604–1613, 2015.
[280] D. Troullinos, G. Chalkiadakis, I. Papamichail, and M. Papageorgiou,
“Collaborative multiagent decision making for lane-free autonomous
driving,” in Proc. AAMAS, vol. 20, 2021, pp. 1335–1343.
[281] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
G. Tan, “Distributed multiagent coordinated learning for autonomous
driving in highways based on dynamic coordination graphs,” IEEE
Trans. Transp. Syst., vol. 21, no. 2, pp. 735–748, 2019.
[282] C. Wu, K. Kumekawa, and T. Kato, “Distributed reinforcement learning
approach for vehicular ad hoc networks,” IEICE Trans. Commun.,
vol. 93, no. 6, pp. 1431–1442, 2010.
[283] A. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad,
“A survey of end-to-end driving: Architectures and training methods,”
IEEE Trans. Neural Netw. Learn. Syst., vol. 33, no. 4, pp. 1364–1384,
2020.
[284] L. M. Schmidt, J. Brosig, A. Plinge, B. M. Eskofier, and C. Mutschler,
“An introduction to multi-agent reinforcement learning and review
of its application to autonomous mobility,” arXiv:2203.07676, 2022.
[Online]. Available: https://arxiv.org/abs/2203.07676
[285] T. Chu, J. Wang, L. Codecà, and Z. Li, “Multi-agent deep reinforcement
learning for large-scale traffic signal control,” IEEE Trans. Transp.
Syst., vol. 21, no. 3, pp. 1086–1095, 2019.
[286] M. Wang, L. Wu, J. Li, and L. He, “Traffic signal control with
reinforcement learning based on region-aware cooperative strategy,”
IEEE Trans. Transp. Syst., 2021.
[287] T. Tanaka, E. Nekouei, A. R. Pedram, and K. H. Johansson, “Linearly
solvable mean-field traffic routing games,” IEEE Trans. Automat.
Contr., vol. 66, no. 2, pp. 880–887, 2020.
[288] T. Cabannes, M. Laurière, J. Perolat, R. Marinier, S. Girgin, S. Perrin,
O. Pietquin, A. M. Bayen, E. Goubault, and R. Elie, “Solving n-player
dynamic routing games with congestion: A mean-field approach,” in
Proc. AAMAS, vol. 21, 2022, pp. 1557–1559.
[289] K. Huang, X. Chen, X. Di, and Q. Du, “Dynamic driving and routing
games for autonomous vehicles on networks: A mean field game
approach,” Transp. Res. C Emerg. Technol., vol. 128, p. 103189, 2021.
[290] R. J. Glauber, “Time-dependent statistics of the Ising model,” J. Math.
Phys., vol. 4, no. 2, pp. 294–307, 1963.
[291] K. Sugishita, M. A. Porter, M. Beguerisse-Díaz, and N. Masuda,
“Opinion dynamics on tie-decay networks,” Phys. Rev. Res., vol. 3,
p. 023249, 06 2021.
[292] J. S. Juul and M. A. Porter, “Hipsters on networks: How a minority
group of individuals can lead to an antiestablishment majority,” Phys.
Rev. E, vol. 99, p. 022313, 02 2019.
[293] A. Tcheukam, B. Djehiche, and H. Tembine, “Evacuation of multi-level
building: Design, control and strategic flow,” in Proc. CCC, 2016, pp.
9218–9223.
[294] B. Djehiche, A. Tcheukam, and H. Tembine, “A mean-field game
of evacuation in multilevel building,” IEEE Trans. Automat. Contr.,
vol. 62, no. 10, pp. 5154–5169, 2017.

[295] A. Aurell and B. Djehiche, “Mean-field type modeling of nonlocal
crowd aversion in pedestrian crowd dynamics,” SIAM J. Contr. Optim.,
vol. 56, no. 1, pp. 434–455, 2018.
[296] R. Carmona, “Applications of mean field games in financial
engineering and economic theory,” arXiv:2012.05237, 2020. [Online].
Available: https://arxiv.org/abs/2012.05237
[297] A. Angiuli, J.-P. Fouque, and M. Lauriere, “Reinforcement learning for
mean field games, with applications to economics,” arXiv:2106.13755,
2021. [Online]. Available: https://arxiv.org/abs/2106.13755
[298] D. Bruneo, M. Scarpa, A. Bobbio, D. Cerotti, and M. Gribaudo,
“Markovian agent modeling swarm intelligence algorithms in wireless
sensor networks,” Perform. Eval., vol. 69, no. 3–4, p. 135–149, Mar.
2012.
[299] M. Gribaudo, D. Cerotti, and A. Bobbio, “Analysis of on-off policies
in sensor networks using interacting Markovian agents,” in Proc. Ann.
IEEE Int. Conf. PerCom, 2008, pp. 300–305.
[300] P. Van Mieghem, J. Omic, and R. Kooij, “Virus spread in networks,”
IEEE/ACM Trans. Netw., vol. 17, no. 1, pp. 1–14, 2009.
[301] Y. Xie, Z. Wang, J. Lu, and Y. Li, “Stability analysis and control
strategies for a new SIS epidemic model in heterogeneous networks,”
Appl. Math. Comput., vol. 383, p. 125381, 2020.
[302] K. Tran and G. Yin, “Optimal control and numerical methods for hybrid
stochastic SIS models,” Nonlinear Analysis: Hybrid Systems, vol. 41,
p. 101051, 2021.
[303] Z. Abbasi, I. Zamani, A. H. A. Mehra, M. Shafieirad, and A. Ibeas,
“Optimal control design of impulsive sqeiar epidemic models with
application to COVID-19,” Chaos, Solitons & Fractals, vol. 139, p.
110054, 2020.
[304] C. Liu, “A microscopic epidemic model and pandemic prediction
using multi-agent reinforcement learning,” arXiv:2004.12959, 2020.
[Online]. Available: https://arxiv.org/abs/2004.12959
[305] A. Q. Ohi, M. Mridha, M. M. Monowar, M. Hamid et al., “Exploring
optimal control of epidemic spread using reinforcement learning,” Sci.
Rep., vol. 10, no. 1, pp. 1–19, 2020.
[306] V. Kompella, R. Capobianco, S. Jong, J. Browne, S. Fox, L. Meyers,
P. Wurman, and P. Stone, “Reinforcement learning for optimization
of COVID-19 mitigation policies,” arXiv:2010.10560, 2020. [Online].
Available: https://arxiv.org/abs/2010.10560
[307] R. Capobianco, V. Kompella, J. Ault, G. Sharon, S. Jong, S. Fox,
L. Meyers, P. R. Wurman, and P. Stone, “Agent-based Markov modeling for improved COVID-19 mitigation policies,” J. Artif. Intell. Res.,
vol. 71, pp. 953–992, 2021.
[308] A. Aurell, R. Carmona, G. Dayanıklı, and M. Laurière, “Finite state
graphon games with applications to epidemics,” Dyn. Games and Appl.,
vol. 12, no. 1, pp. 49–81, 2022.
[309] S. Perrin, M. Laurière, J. Pérolat, R. Élie, M. Geist, and O. Pietquin,
“Generalization in mean field games by learning master policies,” in
Proc. AAAI, vol. 36, no. 9, 2022, pp. 9413–9421.
[310] P. D. Christofides and J. Chow, “Nonlinear and robust control of PDE
systems: Methods and applications to transport-reaction processes,”
Appl. Mech. Rev., vol. 55, no. 2, pp. B29–B30, 2002.
[311] J. P. Gleeson, “High-accuracy approximation of binary-state dynamics
on networks,” Phys. Rev. Lett., vol. 107, p. 068701, 08 2011.
[312] D. Lacker, K. Ramanan, and R. Wu, “Local weak convergence for
sparse networks of interacting processes,” arXiv:1904.02585, 2019.
[Online]. Available: https://arxiv.org/abs/1904.02585
[313] I. Iacopini, G. Petri, A. Barrat, and V. Latora, “Simplicial models of
social contagion,” Nature Commun., vol. 10, no. 1, pp. 1–9, 2019.
[314] F. Garbe, J. Hladkỳ, M. Šileikis, and F. Skerman, “From flip
processes to dynamical systems on graphons,” arXiv:2201.12272,
2022. [Online]. Available: https://arxiv.org/abs/2201.12272
[315] V. M. Preciado and A. Jadbabaie, “Spectral analysis of virus spreading
in random geometric networks,” in Proc. IEEE CDC, 2009, pp. 4802–
4807.
[316] A. Diaz-Guilera, J. Gómez-Gardenes, Y. Moreno, and M. Nekovee,
“Synchronization in random geometric graphs,” Int. J. Bifurcation
Chaos, vol. 19, no. 02, pp. 687–693, 2009.
[317] M. Penrose, Random Geometric Graphs. OUP Oxford, 2003, vol. 5.
[318] M. Walters, “Random geometric graphs,” Surv. Combinatorics, vol.
392, pp. 365–402, 2011.
[319] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A
comprehensive survey on graph neural networks,” IEEE Trans. Neural
Netw. Learn. Syst., vol. 32, no. 1, pp. 4–24, 2020.
[320] L. Ruiz, L. Chamon, and A. Ribeiro, “Graphon neural networks and
the transferability of graph neural networks,” in Proc. NeurIPS, vol. 33,
2020, pp. 1702–1712.

