arXiv:2305.00684v1 [cs.LG] 1 May 2023

On the Complexity of Multi-Agent Decision Making:
From Learning in Games to Partial Monitoring
Dylan J. Foster

Dean P. Foster

Noah Golowich∗

Alexander Rakhlin

dylanfoster@microsoft.com

dean@foster.net

nzg@mit.edu

rakhlin@mit.edu

May 1, 2023
Abstract
A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand
what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and
how these considerations change as we move from few to many agents. We study this question in a
general framework for interactive decision making with multiple agents, encompassing Markov games
with function approximation and normal-form games with bandit feedback. We focus on equilibrium
computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling
multiple agents that interact with an (unknown) environment. Our main contributions are:
• We provide upper and lower bounds on the optimal sample complexity for multi-agent
decision making based on a multi-agent generalization of the Decision-Estimation Coefficient,
a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to
our setting. Compared to the best results for the single-agent setting, our upper and lower
bounds have additional gaps. We show that no “reasonable” complexity measure can close
these gaps, highlighting a striking separation between single and multiple agents.
• We show that characterizing the statistical complexity for multi-agent decision making
is equivalent to characterizing the statistical complexity of single-agent decision making,
but with hidden (unobserved) rewards, a framework that subsumes variants of the partial
monitoring problem. As a consequence of this connection, we characterize the statistical
complexity for hidden-reward interactive decision making to the best extent possible.
Building on this development, we provide several new structural results, including 1) conditions under
which the statistical complexity of multi-agent decision making can be reduced to that of single-agent,
and 2) conditions under which the so-called curse of multiple agents can be avoided.

Contents
1 Introduction
1.1 Multi-agent interactive decision making (MA-DMSO) . . . . . . . . . . . . . . . . . . . . . .
1.2 MA-DMSO: Overview of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Hidden-reward interactive decision making (HR-DMSO) . . . . . . . . . . . . . . . . . . . . .
1.4 HR-DMSO: Overview of results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 MA-DMSO: Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.7 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
4
8
10
12
13
14
16

2 Equivalence of MA-DMSO and HR-DMSO frameworks

16

3 Upper and lower bounds on minimax rates

17

∗ Work done in part while interning at Microsoft Research.

1

3.1
3.2
3.3

HR-DMSO: Upper and lower bounds on minimax rates . . . . . . . . . . . . . . . . . . . . . .
HR-DMSO: Gaps between bounds and impossibility of tight characterizations . . . . . . . . .
Implications for MA-DMSO framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18
19
24

4 MA-DMSO: From multi-agent to single-agent
26
4.1 Bounding the MA-DEC for convex decision spaces . . . . . . . . . . . . . . . . . . . . . . . . 26
4.2 Bounding the MA-DEC for Markov games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5 MA-DMSO: On the curse of multiple agents

28

I

38

Examples

A MA-DMSO: Examples of instances
38
A.1 Additional examples of equilibria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
A.2 Additional examples of instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
A.3 Computing bounds on the DEC and minimax risk of multi-agent instances . . . . . . . . . . . 41

II

Proofs

51

B Technical tools
B.1 Information theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Topological lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.4 Minimax theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51
51
52
53
53

C Proofs for Section 2

53

D Proofs for Section 3
59
D.1 Proofs from Section 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
D.2 Proofs from Section 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
D.3 Proofs from Section 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
E Proofs for Section 4
69
E.1 Bounds for general games with convex decision spaces . . . . . . . . . . . . . . . . . . . . . . 70
E.2 Bounds for Markov games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
F Proofs for upper bounds from Section 5
F.1 The multi-agent exploration-by-optimization objective . . . . . . . . . . . . . . . . . . . . . .
F.2 Bounding the performance of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.3 The multi-agent parametrized information ratio . . . . . . . . . . . . . . . . . . . . . . . . . .
F.4 Relating the multi-agent information ratio and exploration-by-optimization objective . . . . .
F.5 Putting everything together: Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . .

77
77
79
81
84
86

G Proofs for lower bounds from Section 5
G.1 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.2 Proof of Theorem 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.3 Supplementary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

87
87
87
95

2

1

Introduction

Many of the most exciting frontiers for artificial intelligence are game-theoretic in nature, and involve
multiple agents with differing incentives interacting and making decisions in dynamic environments, either
in cooperation or in competition. Numerous recent approaches, adopting the framework of multi-agent
reinforcement learning (MARL), have achieved human-level performance in multi-agent game-playing domains
(Silver et al., 2016; Brown and Sandholm, 2018; Perolat et al., 2022; Kramár et al., 2022; Bakhtin et al., 2022),
and while there is great potential to apply MARL further in domains such as cybersecurity (Malialis and
Kudenko, 2015), autonomous driving (Shalev-Shwartz et al., 2016), and economic policy (Zheng et al., 2022),
sample-efficiency and reliability are obstacles for real-world deployment. Consequently, a central question is
to understand what modeling assumptions and algorithm design principles lead to robust, sample-efficient
learning guarantees. This issue is particularly salient in domains with high-dimensional feedback and decision
spaces, where the use of flexible models such as neural networks is critical.
For reinforcement learning in single-agent settings, an extensive line of research identifies modeling assumptions
(or, structural conditions) under which sample-efficient learning is possible (Russo and Van Roy, 2013; Jiang
et al., 2017; Sun et al., 2019; Wang et al., 2020; Du et al., 2021; Jin et al., 2021a; Foster et al., 2021). Notably,
Foster et al. (2021, 2022b, 2023) provide a notion of statistical complexity, the Decision-Estimation Coefficient
(DEC), which is both necessary and sufficient for low sample complexity, and leads to unified principles for
algorithm design. For multi-agent reinforcement learning, structural conditions for sample-efficient learning
have also received active investigation (Chen et al., 2022b; Li et al., 2022; Xie et al., 2020; Jin et al., 2022;
Huang et al., 2021; Zhan et al., 2022; Liu et al., 2022), drawing inspiration from the single agent setting.
However, insights from single agents do not always transfer to multiple agents in intuitive ways (Daskalakis
et al., 2022), and development has largely proceeded on a case-by-case basis. As such, the problem of
developing a unified understanding or necessary conditions for sample-efficient multi-agent reinforcement
learning remained open.
Contributions. We consider a general framework, Multi-Agent Decision Making with Structured Observations (MA-DMSO), which generalizes the single-agent DMSO framework of Foster et al. (2021) and subsumes
multi-agent reinforcement learning with general function approximation, as well as normal-form games with
bandit feedback and structured action spaces. We focus on centralized equilibrium computation, where a
centralized learning algorithm with control of all agents aims to compute an equilibrium by interacting with
the (unknown) environment. Our main results are:
• Complexity of multi-agent decision making. We introduce a new complexity measure, the MultiAgent Decision-Estimation Coefficient, generalizing the Decision-Estimation Coefficient of Foster et al.
(2021, 2023), and show that it leads to upper and lower bounds on the optimal sample complexity for
multi-agent decision making. Compared to the best results for the single-agent setting (Foster et al.,
2023), our upper and lower bounds have additional gaps, which we show that no (reasonable) complexity
measure can close.
• Complexity of hidden-reward decision making. We show that characterizing the statistical
complexity for multi-agent decision making is equivalent to characterizing the statistical complexity
of single-agent decision making, but with hidden (unobserved) rewards, a framework that we refer
to as Hidden-Reward Decision Making with Structured Observations (HR-DMSO). Leveraging this
connection, we characterize the statistical complexity of the HR-DMSO framework, which encompasses
PAC variants of the stochastic partial monitoring problem (Bartók et al., 2014), to the best extent
possible (for any reasonable complexity measure).
• Additional insights for multiple agents. Building on the results above, we provide a number of
new structural results and algorithmic insights for multi-agent decision making and RL, including 1)
general conditions under which the complexity of multi-agent decision making can be reduced to that
of single agent decision making, and 2) general conditions under which the so-called curse of multiple
agents (Jin et al., 2021b) can be removed.
Our results provide a foundation on which to develop a unified understanding of multi-agent reinforcement

3

learning and decision making, and highlight a number of exciting open problems.

1.1

Multi-agent interactive decision making (MA-DMSO)

We introduce a multi-agent generalization of the Decision Making with Structured Observations framework
of Foster et al. (2021), which we refer to as Multi-Agent Decision Making with Structured Observations
(MA-DMSO). The framework consists of T rounds of interaction between K agents and the environment. For
each round t = 1, 2, . . . , T :
1. The agents collectively select a joint decision π t ∈ Π, where Π is the joint decision space.
2. Each agent k ∈ [K] receives a reward rkt ∈ R ⊆ R and a pure observation ot◦ ∈ O◦ sampled via
t
(r1t , . . . , rK
, ot◦ ) ∼ M ? (π t ), where M ? : Π → ∆(RK × O◦ ) is the underlying model. We refer to R as the
t
reward space and to O◦ as the pure observation space. We call the tuple (r1t , . . . , rK
, ot◦ ) consisting of
all information revealed to agents on round t the full observation.
After the T rounds of interaction, the agents collectively output a joint decision π
b ∈ Π, which may be chosen
in an arbitrary fashion based on the data observed over the T rounds, and may be randomized according to a
distribution p ∈ ∆(Π). Their goal, which we formalize in the sequel, is to choose π
b to be an equilibrium (e.g.,
Nash or CCE) for the average reward function induced by M ? . The model M ? , which is formalized as a
probability kernel from decisions to full observations (Section 1.6), is unknown to the agents, and is to be
interpreted as the underlying environment.
The DMSO framework captures most online decision making problems in which a single agent interacts with
an unknown environment, and the MA-DMSO framework further generalizes it to capture a wide variety of
problems in multi-agent reinforcement learning. Examples include learning in normal-form games with bandit
feedback (Rakhlin and Sridharan, 2013; Foster et al., 2016; Heliou et al., 2017; Wei and Luo, 2018; Giannou
et al., 2021), where M ? represents the distribution over rewards for each entry in the game, and learning in
Markov games with function approximation (Chen et al., 2022b; Li et al., 2022; Xie et al., 2020; Jin et al.,
2022; Huang et al., 2021; Zhan et al., 2022; Liu et al., 2022), where M ? represents the underlying Markov
game. Additional examples include normal-form games with structured (e.g., convex-concave) rewards and
high-dimensional action spaces (Bravo et al., 2018; Maheshwari et al., 2022; Lin et al., 2021).
Realizability. While the model M ? is unknown, we make a standard realizability assumption.
Assumption 1.1 (Realizability for MA-DMSO). The agents have access to a model class M consisting of
probability kernels M : Π → ∆(RK × O◦ ) that contains the true model M ? .
For normal-form games, the class M encodes structure in the rewards (e.g., linearity or convexity) or decision
space, and for Markov games it encodes structure in transition probabilities or value functions. See Part I
of the appendix for examples, as well as Foster et al. (2021) for in the single-agent case where K = 1.
1.1.1

Equilibria

The goal of the agents in the MA-DMSO framework is to produce an equilibrium for the underlying game/model
M ? . We formalize the notion of equilibrium in a general fashion which encompasses several standard gametheoretic equilibria. To keep notation compact, we define O := RK × O◦ to be the full observation space,
t
and will write ot := (r1t , . . . , rK
, ot◦ ) to denote the (full) observation. For M ∈ M and π ∈ Π, let EM,π [·]
denote expectation under the process (r1 , . . . , rK , o◦ ) ∼ M (π); in light of our notation O = RK × O◦ , we will
sometimes denote this process as o ∼ M (π). For each k ∈ [K] and M ∈ M, define the mapping fkM : Π → R
by fkM (π) = EM,π [rk ], which denotes agent k’s expected reward under M when the joint decision π is played.
For each agent k, we assume they are given a deviation space Π0k , together with a switching function,
Uk : Π0k × Π → Π. Given a joint decision π ∈ Π, each agent k can choose a deviation πk0 ∈ Π0k , which will
have the effect that the joint policy played by agents is Uk (πk0 , π) instead of π. We aim for the output policy
π
b ∼ p produced in the MA-DMSO setup to have the property that no agent can significantly increase their

4

value by deviating. We quantify this via
Risk(T ) := Eπb∼p

"K
X

#
M?

M?

sup fk (Uk (πk0 , π)) − fk (π) .

0
0
k=1 πk ∈Πk

(1)

PK
For M ∈ M and π ∈ Π, we abbreviate hM (π) = k=1 supπk0 ∈Π0k fkM (Uk (πk0 , π)) − fkM (π), so that Risk(T ) :=
?
Eπb∼p [hM (b
π )]. The quantity hM (π) measures the sum of players’ incentives to deviate from the joint decision
π under M ; we say that π is an equilibrium for M if hM (π) = 0.
The notion (1) captures standard notions of equilibria, including Nash equilibria, correlated equilibria (CE),
and coarse correlated equilibria (CCE). As we have strived to make the setup in this section as general as
possible, we make two regularity assumptions to rule out other, potentially pathological notions of equilibria.
The first posits that equilibria exist, and the second asserts that each agent can always choose a deviation
that does not decrease their value.
Assumption 1.2 (Existence of equilibria). For any model M ∈ M, there exists π ∈ Π with hM (π) = 0.
Assumption 1.3 (Monotonicity of the optimal deviation). For any model M ∈ M, agent k ∈ [K], and joint
decision π ∈ Π, there is some deviation πk0 ∈ Π0k such that fkM (Uk (πk0 , π)) ≥ fkM (π).
Assumption 1.3 implies that, up to a factor of K, the notion of risk in (1) is equivalent to the maximal gain
any agent can achieve by deviating. Both assumptions are satisfied by Nash equilibria, CE, and CCE (see
Definitions 1.1, 1.2 and A.1).
Summarizing, the MA-DMSO framework captures the problem of equilibrium computation: the agents aim to
find an (ε-approximate) equilibrium π
b so that Risk(T ) ≤ ε, but the underlying game is unknown, so they must
gather information by interacting with it and exploring. We refer to the tuple M = (M, Π, O, {Π0k }k , {Uk }k )
as an instance for the MA-DMSO framework. The instance M specifies all information known a-priori to the
agents before the learning process begins.
Remark 1.1. As described, the MA-DMSO framework allows centralized learning protocols, in which a single
learning algorithm may control all agents in a centralized fashion (equivalently, unlimited communication and
coordination is permitted amongst agents throughout the learning process). Lower bounds against centralized
learning algorithms certainly apply to decentralized algorithms, being a special case of the former. However,
in general there may be gaps between the minimax sample complexity for centralized and decentralized
algorithms, and we leave a detailed investigation of decentralized multi-agent interactive decision-making for
future work.
Remark 1.2. Our presentation of the MA-DMSO framework captures settings in which (multi-agent)
learning algorithms are evaluated only on the proximity of output decision π
b to equilibrium, as opposed
to, say, the average proximity to equilibrium for the decisions played throughout the T rounds of learning.
In the single-agent setting, such guarantees are often referred as PAC (Probability Approximately Correct)
guarantees, as opposed to regret guarantees (Foster et al., 2023). It is fairly straightforward to extend many
of our results to the regret setting.
1.1.2

Examples of instances for MA-DMSO

We now highlight basic multi-agent bandit and MARL problems captured by the MA-DMSO framework.
We describe the structure of the decision space, deviation space, and switching functions that allow us to
capture concrete notions of equilibria, then give examples of instances M .
Examples of equilibria. In Definitions 1.1 and 1.2 below, we specify the decision spaces, deviation spaces,
and switching functions that can be used to capture Nash equilibria and coarse correlated equilibria (CCE);
see Appendix A.1 for further examples, including correlated equilibria (CE) and variants of CCE and CCE
which have been studied in the context of Markov games.
Definition 1.1 (Nash equilibrium instance). An MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k ) is a
Nash equilibrium (NE) instance if the following holds:
5

1. For sets Π1 , . . . , ΠK , we have Π = Π1 × · · · × ΠK .
2. For each k ∈ [K], we have Π0k = Πk .
3. For each k ∈ [K], π ∈ Π, and πk0 ∈ Π0k , it holds that Uk (πk0 , π) = (πk0 , π−k ).1
We say that the NE instance M is a two-player zero-sum NE instance if K = 2, and for all M ∈ M, π ∈ Π,
it holds that f1M (π) + f2M (π) = 0.
The notion of Nash equilibrium in Definition 1.1 encompasses, but goes well beyond the standard notion of
mixed Nash equilibria in normal-form games (e.g., (Nisan et al., 2007)). In particular, Definition 1.1 does
not assume that the decision spaces Πk are distributions over a pure action space of player k. Therefore,
it captures refined solution concepts including pure Nash equilibria in normal-form games (Daskalakis and
Papadimitriou, 2006) and Markov Nash equilibria in Markov games (Example 1.2). As a result of this
generality, an NE instance per Definition 1.1 is not guaranteed to satisfy Assumption 1.2, i.e., to have
equilibria; nevertheless, we will ensure that all examples of NE instances we consider are constructed in such
a way so that Assumption 1.2 is satisfied.
Definition 1.2 gives an analogue of Definition 1.1 which can capture the notion of (normal-form) coarse
correlated equilibria.
Definition 1.2 (Coarse correlated equilibrium instance). An instance M = (M, Π, O, {Π0k }k , {Uk }k ) for
MA-DMSO is a coarse correlated equilibrium (CCE) instance if the following holds:
1. For some sets Σ1 , . . . , ΣK (called pure decisions), we have Π = ∆(Σ1 × · · · × ΣK ). We will write
Σ := Σ1 × · · · × ΣK .
2. For each π ∈ Π and M ∈ M, it holds that M (π) = Eσ∼π [M (σ)]. Further, there is a measurable function
ϕ : O → Σ so that Po∼M (σ) (ϕ(o) = σ) = 1 for each M ∈ M and σ ∈ Σ (i.e., M (σ) reveals σ).
3. For each k ∈ [K], we have Π0k = Σk ∪ {⊥}.
4. For each k ∈ [K], π ∈ Π, and πk0 ∈ Π0k , it holds that
(
Iπk0 × π−k
Uk (πk0 , π) =
π

: πk0 6=⊥
,
: πk0 =⊥

where Iπk0 × π−k ∈ Π denotes the product distribution whereby agent k plays πk0 and the other agents
play according to their joint marginal under π ∈ Π.
In Definition 1.2, the inclusion of ⊥∈ Π0k corresponds to player k choosing not to deviate. This is necessary to
satisfy Assumption 1.3 since there can be distributions π ∈ Π so that if player k deviates to any fixed option
in Σk , their value decreases.2 We also remark that Definition 1.2 captures the notion of CCE in normal-form
games (with pure action sets Σk ); in Appendix A.1 we give an example of an instance capturing a slightly
different notion of CCE in Markov games.
Remark 1.3. We use the following convention throughout the paper, including in Item 2 of the above
definition: when convenient, we associate any singleton distribution with the element that the distribution
places its mass on. For instance, for a pure decision σ = (σ1 , . . . , σK ) ∈ Σ1 × · · · × ΣK in the context of
Definition 1.2, we will denote its corresponding singleton distribution Iσ ∈ ∆(Σ) = Π as just σ ∈ Π. In
addition, when possible, we use the convention that Σ denotes a pure decision set, whereas Π denotes a
decision set that may be pure or mixed (this will be clear from context).
1 We adopt the convention that π
−k = (π1 , . . . , πk−1 , πk+1 , . . .) and (πk , π−k ) = (π1 , . . . , πk , . . . , πK ).
2 In some contexts, coarse correlated equilibria are defined without such an option ⊥∈ Π0 ; in settings where the only goal is
k

to establish upper bounds, the addition of ⊥ does not make a material difference (since its only effect is to guarantee that the
suboptimality of a decision is non-negative), but since we aim to prove lower bounds as well, it is crucial to have the option
⊥∈ Π0k .

6

Examples of equilibria. We now provide concrete examples for the NE and CCE instances in Definitions 1.1
and 1.2; see Appendix A for additional examples (including CE) and discussion.
Example 1.1 (Learning Nash, and CCE in normal-form games). We begin by describing the problem of
learning in normal-form games with bandit feedback. Suppose that each player k ∈ [K] has a finite action
set Ak , with joint action set denoted by A = A1 × · · · × AK . Upon playing a joint action profile a ∈ A, the
(unknown) ground truth model M ? samples (r1 , . . . , rK ) ∼ M ? (a), where rk denotes the reward received by
player k. The goal is to compute a distribution over joint action profiles which is some type of equilibrium of
the game whose payoffs are given by expected rewards under M ? .Below we formally describe the MA-DMSO
instances corresponding to the problems of computing Nash equilibria and coarse correlated equilibria:
• To express the problem of Nash equilibrium computation, set Πk := ∆(Ak ) for each k, let Π = Π1 ×· · · ΠK
be the space of product distributions on A, and define Π0k , Uk as in Definition 1.1. Moreover, let R = [0, 1]
and O◦ = A, O = RK × O◦ . Let M be the class of models so that: (a) for all singleton distributions
Ia = Ia1 × · · · × IaK ∈ Π, M (Ia ) ∈ ∆(RK ) × {Ia }, and (b) for all π ∈ Π, M (π) = Ea∼π [M (Ia )]. In words,
M (π) samples an action profile a ∼ π (in particular, ak ∼ πk for each k), reveals the action profile a
sampled,3 as well as K [0, 1]-valued rewards drawn from an arbitrary distribution. Then the instance
?
M = (M, Π, O, {Π0k }k , {Uk }k ) is an NE instance per Definition 1.1. For π
b ∈ Π, hM (b
π ) measures the
?
sum of the players’ incentives to deviate from π
b under the true model M ? ; in particular, hM (b
π ) = 0 if and
?
?
only if π
b is a Nash equilibrium of the game whose payoff functions are given by a 7→ fkM (a) := EM ,a [rk ].
• To express the problem of CCE computation, set Π = ∆(A1 × · · · × AK ), and define Π0k , Uk as in
Definition 1.2 with Σk = Ak for each k. Moreover, let R = [0, 1], and O◦ = A, O = RK × O◦ . Let
M be the class of models so that: (a) for all singleton distributions Ia ∈ Π, M (Ia ) ∈ ∆(RK ) × {Ia },
and (b), for π ∈ Π, M (π) = Ea∼π [M (Ia )]. Then the instance M := (M, Π, O, {Π0k }k , {Uk }k ) is a CCE
?
instance per Definition 1.2. For π
b ∈ Π, hM (b
π ) measures the sum of players’ non-negative incentives
?
?
to deviate from π
b under the true model M ; in particular, hM (b
π ) = 0 if and only if π
b is a CCE of the
M?
M ? ,a
game whose payoff functions are given by a 7→ fk (a) := E
[rk ].
• The MA-DMSO framework can also express the problem of correlated equilibrium computation. We
use the same CCE instance as described in the previous point, but define Π0k , Uk slightly differently;
see Definition A.1 in Appendix A.
In the most basic (“finite-action”) version of the normal-form game setup, we allow M ? (a) to be arbitrary,
subject to the constraint that rk ∈ [0, 1], but assume that Ak := |Ak | < ∞ for all k. Beyond finite-action
normal-form games, the MA-DMSO framework captures structured normal-form games with bandit feedback
(equivalently, multi-agent variants of the structured bandit problem), in which the players’ action spaces are
large or infinite, but rewards have additional structure. Examples include linear, convex, or concave payoffs
(generalizing bandit convex optimization) (Bravo et al., 2018; Maheshwari et al., 2022; Lin et al., 2021), and
many others (Cui et al., 2022).
/
Example 1.2 (Learning Nash equilibria in Markov games). Next, we consider an episodic multi-agent
finite-horizon reinforcement learning setting, in which the unknown ground truth model M ? is a Markov
game. We focus on the problem of computing a Markov Nash equilibrium; the problems of computing variants
of CCE and CE are discussed in Appendix A.1.
Formally, each model M ∈ M defines a Markov game of the form M = (H, {Sh }h∈[H] , {Ak }k∈[K] ,
M
{PhM }h∈[H] , {Rk,h
}k∈[K],h∈[H] , d1 ), where H ∈ N denotes the horizon, Sh denotes the state space for layer h, Ak
denotes the action space for player k, A := A1 ×· · ·×AK denotes the joint action space, PhM : Sh ×A → ∆(Sh+1 )
M
denotes the probability transition kernel for layer h, Rk,h
: Sh × A → ∆(R) denotes player k’s reward
distribution for layer h, and d1 ∈ ∆(S1 ) denotes the initial state distribution. The transition kernel and
reward distributions are allowed to vary across models in M, but we assume that the state and action spaces,
horizon, and initial state distribution are the same for all models in M.
Each agent’s decision space Πk is the space of their randomized Markov policies πk = (πk,1 , . . . , πk,H ), where
πk,h : Sh → ∆(Ak ), and the joint decision space is Π = Π1 × · · · × ΠK . Given a joint decision π ∈ Π, an
3 We assume that the model reveals the action profile played for technical reasons (see Assumption 4.1); this is a very mild
assumption, satisfied in essentially all (centralized) settings, since agents know which action they play.

7

observation is drawn from M (π) according to the following process, called an episode. First, an initial state
is drawn according to s1 ∼ d1 . Then, for h ∈ [H], the following random variables are sampled in sequence:
M
• For all k ∈ [K], ak,h ∼ πk,h (sh ), and rk,h ∼ Rk,h
(sh , (a1,h , . . . , aK,h )).

• sh+1 ∼ PhM (·|sh , (a1,h , . . . , aK,h )).
The sequence τ = {(sh , (a1,h , . . . , aK,h ), (r1,h , . . . , rK,h )}h∈[H] of all states, actions, and rewards is called a
PH
trajectory. The distribution of (r1 , . . . , rK , o◦ ) ∼ M (π) is given by o◦ = τ and rk = h=1 rk,h . In particular,
PH
the pure observation space O◦ is the space of trajectories. We assume that h=1 rk,h ∈ [0, 1] with probability
1, meaning that R = [0, 1], and write O = RK × O◦ .
let Π0k , Uk be defined as in Definition 1.1. Then the instance M := (M, Π, O, {Π0k }k , {Uk }k ) is an NE instance
?
of MA-DMSO. For π
b ∈ Π, the value hM (b
π ) measures the sum of players’ incentives to deviate from π
b under
?
the true model M , where each agent can choose an arbitrary non-stationary Markov policy as their deviation.
?
In particular, hM (b
π ) = 0 if and only if π
b is a Markov Nash equilibrium of M ? (e.g., Daskalakis et al. (2022)).
A key question in (multi-agent) online reinforcement learning is to understand what structural properties of
the model class M permit efficient learnability. In the simplest case (known as the tabular case), the state
and action spaces Sh , A are all finite, and M consists of all models specified by arbitrary transitions PhM and
M
reward distributions Rk,h
with uniformly bounded support. By restricting M, our formulation also captures
a more complex settings that incorporate function approximation (Chen et al., 2022b; Li et al., 2022; Xie
et al., 2020; Jin et al., 2022; Huang et al., 2021; Zhan et al., 2022; Liu et al., 2022); see Appendix A.
/
We refer to Appendix A for additional examples and exposition.

1.2

MA-DMSO: Overview of results

We provide upper and lower bounds on the minimax sample complexity for the MA-DMSO framework using a
new complexity measure, the Multi-Agent Decision-Estimation Coefficient, which generalizes the Constrained
Decision-Estimation Coefficient introduced by Foster et al. (2023) in the single agent setting.
The Multi-Agent Decision-Estimation Coefficient. For probability measures P and Q with a common
dominating measure ν, define squared Hellinger distance by
r
2
Z r
dP
dQ
2
DH (P, Q) =
−
dν.
dν
dν
Consider an instance M = (M, Π, O, {Π0k }k , {Uk }k ) for the MA-DMSO framework, as well as a reference
model M : Π → ∆(O).4 For a scale parameter ε > 0, the Multi-Agent Decision-Estimation Coefficient for the
instance M with reference model M at scale ε is defined by


decε (M , M ) :=
inf
sup Eπ∼p [hM (π)] | Eπ∼q [DH2 M (π), M (π) ] ≤ ε2 ;
(2)
p,q∈∆(Π) M ∈M

whenever the set is empty, we adopt the convention that decε (M , M ) = 0.

Hq,ε (M ) := {M ∈ M | Eπ∼q [DH2 M (π), M (π) ] ≤ ε2 }

(3)

In addition, we define
decε (M ) :=

sup

decε (M , M ),

M ∈co(M)

where co(M) denotes the convex hull of the class M.
4 The reference model M may be arbitrary, and is not required to lie in M.

8

(4)

The interpretation of the definition (2), which is a min-max game, is as follows. The model M ∈ M selected
by max-player represents a worst-case choice for the underlying model. The joint distributions p, q ∈ ∆(Π)
selected by the min-player represent strategies for a centralized learning algorithm controlling all agents.
The distribution q ∈ ∆(Π) is an exploration
 distribution which acts as a strategy for acquiring information,
with the quantity Eπ∼q [DH2 M (π), M (π) ] acting as their average “information gain” (that is, the amount
information that allows to distinguish between M ∈ M and the reference model M ). The distribution
p ∈ ∆(Π) is an exploitation distribution which aims to be near equilibrium for the model M ∈ M selected by
the max-player, with Eπ∼p [hM (π)] representing the distance from equilibrium. Thus, to summarize, the value
(2) captures, for a best-case choice of p, q ∈ ∆(Π), the worst-case distance to equilibrium for p for models
M ∈ M that are “close” to M in the sense that their information gain under q is small.
For familiar readers, we recall that the (single-agent) constrained DEC generalizes the earlier offset DEC of
Foster et al. (2021) (which acts as a Lagrangian relaxation), and always leads to tighter guarantees (Foster
et al., 2023). Our definition (2) generalizes the so-called PAC variant of the constrained DEC in Foster et al.
(2023), as opposed the regret variant, which restricts to p = q.
Main results. The first of our results gives upper and lower bounds on the minimax sample complexity
for the MA-DMSO framework based on the Multi-Agent Decision-Estimation Coefficient. To state the result
in the simplest form, we assume that |M| < ∞; see Section 3 for more general results.
Theorem 1.1 (Informal version of Corollaries 3.1 and 3.2). For any instance M = (M, Π, O, {Π0k }k , {Uk }k )
for the MA-DMSO framework and T ∈ N:
• Upper bound: Under Assumption 1.1, there exists an algorithm that achieves
p

e
e
E[Risk(T )] ≤ O(1)
· decε(T ) (M ), where ε(T ) ≤ Θ
log|M|/T .

(5)

• Lower bound: For a worst-case model M ∈ M, any algorithm must have
e
E[Risk(T )] ≥ Ω(1)
· decε(T ) (M ),


e ε2 KT .
where ε(T ) solves decε (M ) ≥ Ω

(6)

This result shows that the MA-DEC is a fundamental limit for equilibrium computation in the MA-DMSO
framework, and is sufficient for low sample complexity whenever log|M| < ∞. The upper bound is an
immediate corollary of an upper bound given by Foster et al. (2023) in the single-agent setting, while the
lower bound requires a new approach; this is due to fundamental differences between the single and multiple
agents, which we highlight in the sequel.
To build intuition, let us start with a basic example. Suppose that M is a CCE instance consisting of
two-player A1 ×A2 normal-form games (that is, |A1 | = A1 and |A2 | =√A2 ) with bandit feedback (Example 1.1)
and Bernoulli noise. In this case, one can show that decε (M ) ∝ ε · A1 + A2 , so that the upper bound (5)
gives
r
(A1 + A2 ) log|M|
E[Risk(T )] .
,
T
or equivalently, (A1 +A2ε2) log|M| rounds of interaction are sufficient to find an ε-CCE. For this class, one can
e 1 · A2 ). We give more refined results (Section 5) which allow one to replace log|M| by
take log|M| . O(A

0
2
e A1 +A
, which is optimal.
maxk log|Πk | . log(A1 + A2 ), so that we achieve sample complexity O
ε2
Turning to lower bounds, for the same normal-form game instance M , one can choose ε(T ) &
that (6) gives
E[Risk(T )] &

√
A1 +A2
, so
T

A1 + A2
,
T


e A1 +A2 rounds of interaction are necessary to find an ε-CCE. Comparing the upper and
or equivalently, Ω
ε
lower bounds, there are two gaps. The first is the term log|M| appearing in the upper bound, which represents
9

the sample complexity required to perform statistical estimation with the class M, and in general scales
poorly with the number of agents. This can be refined (cf. Section 5), but is not possible to completely
remove in general, even in the single-agent setting; see Foster et al. (2021, 2023) and Section 3 for further
discussion.
The second gap is
lower bound;
√the difference between the values ε(T ) and ε(T ) appearing in the upper and

2
e
we set ε(T ) ∝ 1/ T , while ε(T ) is chosen to solve the fixed-point equation decε (M ) ≥ Ω ε T (we focus on
the case of constant K in this discussion). For normal-form games, this causes the lower bound to scale with
1
1
ε instead of ε2 . This gap is not present in the single-agent setting (Foster et al., 2023), where the best upper
and lower bounds based on the constrained DEC have ε(T ) ≈ ε(T ) (up to dependence on log|M|). We show
(Proposition 3.1) that for most parameter regimes,
decε(T ) (M ) . K 2 log|M| · decε(T ) (M )

1/2

,

i.e., the gap between the upper and lower bounds is no worse than quadratic generically. This gap turns out to
be fundamental: We show (Propositions 3.2 and 3.3) that there exist instances for which each bound (upper
and lower) is tight, and—somewhat surprisingly—the following result shows that no complexity measure
satisfying fairly general conditions can fully characterize the sample complexity of multi-agent decision making
e
beyond a quadratic gap, even when log|M| = O(1).
Theorem 1.2 (Informal version of Theorem 3.4). For any ε ∈ N, there exist two-player zero-sum Nash
equilibrium MA-DMSO instances M1 = (M1 , Π, O, {Π0k }k , {Uk }k ) and M2 = (M2 , Π, O, {Π0k }k , {Uk }k ) and
a one-to-one mapping E : M1 → M2 satisfying:
1. For all M ∈ M1 , fkM ≡ fkE (M ) for all k ∈ [2].
2. For all M, M 0 ∈ M1 and all π ∈ Π, DH2 (M (π), M 0 (π)) = DH2 (E (M )(π), E (M 0 )(π)).

e 1 rounds, yet any algorithm
3. There exists an algorithm that finds an ε-NE for any model in M1 using O
ε

e 12 rounds to find an ε-NE for a worst-case model in M2 .
requires Ω
ε
e
In addition, log|M1 | = log|M2 | = O(1).
Informally, this result states that if a complexity measure depends on the instance M only through 1) reward
functions and 2) pairwise Hellinger distances for models in M, then it cannot characterize the optimal sample
complexity for every instance beyond the gap in the prequel. In addition, the full result is not limited to
Hellinger distance, and applies to general f -divergences including KL- and χ2 -divergence. This rules out
tighter guarantees based on various variants of the DEC, as well as most other general-purpose complexity
measures for interactive decision making; see Section 3.2.2 for details.5
Theorem 1.2 (and Propositions 3.2 and 3.3) highlight a fundamental separation between the single and
multi-agent frameworks. In the single-agent setting, the constrained DEC characterizes, up to logarithmic
e
factors, the optimal number of samples required to learn an ε-optimal decision, as long as log|M| = O(1)
(Foster et al., 2023). For two or more agents, Theorem 1.2 and Propositions 3.2 and 3.3 rule out such a
characterization.

1.3

Hidden-reward interactive decision making (HR-DMSO)

To prove the results in the prequel, we establish a certain equivalence between the MA-DMSO framework
and another single-agent setting we refer to as Hidden-Reward Decision Making with Structured Observations
(HR-DMSO), which generalizes the single-agent DMSO framework (MA-DMSO with K = 1) by allowing
rewards to be hidden from the agent. This setting is of interest in its own right, and can be thought of
as a stochastic, PAC variant of the partial monitoring problem (Bartók et al., 2014). In what follows, we
introduce the framework, then show that 1) MA-DMSO can be viewed as a special case of the HR-DMSO
framework via a simple reduction, and 2) a converse holds, thus showing a sort of equivalence. We then
discuss implications for minimax rates in both frameworks.
5 Directly applying Theorem 1.2 to the constrained DEC presents complications due to M ∈ co(M); see App. 3.2.2.

10

Formally, the HR-DMSO framework proceeds in T rounds, where for each round t = 1, 2, . . . , T :
1. The learner selects a decision π t ∈ Π, where Π is the decision space, and gains (but does not observe)
?
reward f M (π t ).
2. The learner receives an observation ot ∈ O sampled via ot ∼ M ? (π t ), where M ? : Π → ∆(O) is the
underlying model. We refer to O as the observation space.
After this process finishes, the learner uses the data collected throughout the T rounds of interaction to
produce an output decision π
b ∈ Π, which may be randomized according to a distribution p ∈ ∆(Π). The
?
learner’s goal is to choose the decision π
b so as to maximize its (unobserved) reward f M (b
π ). Formally, writing
M
πM := arg maxπ∈Π f (π), we define the risk of an algorithm as:
?

?

Risk(T ) := Eπb∼p [f M (πM ? ) − f M (b
π )].
We assume that every model M is associated a (known) function f M : Π → R, where f M (π) specifies the
learner’s value under decision π ∈ Π when the underlying model is M . We make the following realizability
assumption, analogous to Assumption 1.1.
Assumption 1.4 (Realizability for HR-DMSO). The learner has access to a model class M consisting of
probability kernels M : Π → ∆(O) that contains the true model M ? .
We refer to the tuple H = (M, Π, O, {f M (·)}M ∈M ) as an instance for the HR-DMSO framework. It specifies
all of the information known to a learner a-priori before interacting with the model M ? ∈ M.
Remark 1.4. An equivalent formulation of the HR-DMSO framework would be to consider models M : Π →
∆(O × R) that specify joint distributions over observations and rewards and define f M (π) = EM ,π [r], but
only allow o to be observed by the learner under (o, r) ∼ M (π).
We refer to the tuple H = (M, Π, O, {f M (·)}M ∈M ) as an instance for the HR-DMSO framework. We
extend the constrained Decision-Estimation Coefficient of Foster et al. (2023) to HR-DMSO as follows. For
an instance H = (M, Π, O, {f M (·)}M ), reference model M : Π → ∆(O), and scale parameter ε > 0, the
constrained Decision-Estimation Coefficient is given by6


decε (H , M ) =
inf
sup Eπ∼p [f M (πM ) − f M (π) | Eπ∼q [DH2 M (π), M (π) ] ≤ ε2 .
(7)
p,q∈∆(Π) M ∈M

We define the Decision-Estimation Coefficient (DEC) of the instance H at scale ε to be
decε (H ) =

sup

decε (H , M ).

(8)

M ∈co(M)

This definition is identical to the constrained PAC DEC (Foster et al., 2023); this is natural, as the only
difference between the HR-DMSO framework and the DMSO framework (Foster et al., 2023) is that we relax
the constraint that the agent observes its reward.
Remark 1.5. The HR-DMSO framework is related to the partial monitoring problem (Bartók et al., 2014) .
While most work in partial monitoring considers regret guarantees (that is, cumulative suboptimality for
π 1 , . . . , π T ), we consider PAC guarantees (i.e., final suboptimality for π
b). An additional difference between
the two settings is that partial monitoring typically considers finite decision and observation spaces, while
we allow for large, structured spaces (formalized via the model class M), and aim for sample complexity
guarantees that reflect the intrinsic complexity of these spaces.
Remark 1.6 (Contrast with reward-free DMSO). Despite the similar name, the HR-DMSO framework is
distinct from the “reward-free” DMSO framework considered in the recent work of Chen et al. (2022a); in the
latter framework, which is specialized to Markov decision processes, a reward-function is given to the learner
explicitly, but only after the learning process ends.
6 Note that we use the same notation for the DEC in the HR-DMSO and MA-DMSO settings; we will typically use the letter
H to denote HR-DMSO instances and M to denote MA-DMSO instances to avoid ambiguity.

11

1.4

HR-DMSO: Overview of results

It is fairly immediate to see that the HR-DMSO framework generalizes the MA-DMSO framework. For
any MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k ) satisfying Assumption 1.3 and Assumption 1.2, by
choosing the value function f M (·) = −hM (·), the instance of the HR-DMSO framework specified by the
tuple H = (M, Π, O, {f M }M ) (recalling that O = O◦ × RK ) is statistically equivalent to M .7 In particular,
letting M(M , T ) denote the minimax risk for an instance M in the MA-DMSO framework, and let M(H , T )
denote the minimax risk for the corresponding HR-DMSO instance H (see Section 1.6 for formal definitions),
we have:
1. For all models M and ε > 0, decε (H , M ) = decε (M , M ).
2. For all T ∈ N, M(H , T ) = M(M , T ).
It is natural to ask whether the HR-DMSO framework is strictly more general than the MA-DMSO framework.
Indeed, by allowing rewards to be hidden, one might imagine that HR-DMSO can capture problems outside
of MA-DMSO, which forces rewards to be observed. The next result shows that this is not the case: any
HR-DMSO instance can be embedded in a two-player zero-sum NE instance for MA-DMSO, with minimal
increase in statistical complexity.
Theorem 1.3 (Informal version of Theorem 2.2). Consider any HR-DMSO instance specified by the tuple
H = (M, Π, O, {f M (·)}M ). For any δ > 0, there exists a two-player zero-sum NE MA-DMSO instance
f Π,
e O,
e Π0 , Uk ) (Definition 1.1) such that:
M = (M,
k
1. For all ε > 0, decε (H ) ≤ decε (M ) ≤ δ + decε+δ (H ).
2. For all T ∈ N, it holds that M(H , T ) ≤ M(M , T ) ≤ M(H , T ) + δ.
f ≤ log |M| + polylog(T, δ −1 ).
3. If M is finite, then log |M|
This result establishes that the MA-DMSO and HR-DMSO frameworks satisfy a sort of equivalence, and
shows that characterizing the minimax sample complexity for MA-DMSO is no easier than characterizing
the minimax sample complexity for the HR-DMSO framework. The proof proceeds by embedding a given
instance for the HR-DMSO framework into a two-player game: the first of the two agents in the game plays
the role of the HR-DMSO agent, and the second agent selects actions to ensure that optimal actions for the
original HR-DMSO instance are Nash equilibria for the new instance, and vice-versa. The key idea is that
even though rewards in the game are observed, by making the game polynomially large, we can ensure that
discovering them requires a prohibitively large amount of exploration, rendering them effectively hidden.
HR-DMSO: Minimax rates. To prove the multi-agent minimax rates in Theorems 1.1 and 1.2, we first
prove analogous bounds for the HR-DMSO framework, then use the equivalence above to extend them to
MA-DMSO. In particular, the following result provides our main sample complexity bounds for HR-DMSO,
generalizing Theorem 1.1.
Theorem 1.4 (Informal version of Theorems 3.1 to 3.3). For any instance H = (M, Π, O, {f M (·)}M ) for
the HR-DMSO framework and T ∈ N:
• Upper bound: Under Assumption 1.1, there exists an algorithm that achieves
e
E[Risk(T )] ≤ O(1)
· decε(T ) (H )
e
for all M ∈ M, where ε(T ) ≤ Θ

p

(9)


log|M|/T .

• Lower bound: For a worst-case model M ∈ M, any algorithm must have
e
E[Risk(T )] ≥ Ω(1)
· decε(T ) (H ),

(10)


e ε2 T .
where ε(T ) is the largest value ε > 0 such that decε (H ) ≥ Ω
7 It is essential for this reduction that the rewards in H

be hidden, since it is in general impossible to simulate a reward

whose mean is −hM (π) using samples from M (π).

12

In addition, no complexity measure that depends on the instance H only through the reward functions
{f M (·)}M ∈M and pairwise Hellinger distances for models M, M 0 ∈ M can characterize the optimal sample
complexity for every instance, beyond a quadratic gap.

1.5

MA-DMSO: Additional results

Beyond minimax rates, we provide a number of structural results for the MA-DMSO framework that we
believe to be of independent interest, including: (1) conditions under which the multi-agent DEC can be
controlled by the single-agent DEC, and (2) conditions under which the so-called curse of multiple agents can
be avoided. We now highlight these results.
From multi-agent to single-agent. We show that it is generically possible to upper bound the MA-DEC
in terms of the single-agent DEC for each player k. This result is most easily stated in terms of a multi-agent
analogue of the offset version of the DEC introduced in Foster et al. (2021). Specifically, we consider a regret
variant of the offset DEC that restricts p = q, coupling exploration and exploitation: For an instance M ,
reference model M , and scale parameter γ > 0, we define


r-decoγ (M , M ) := inf
sup Eπ∼p [hM (π)] − γ · Eπ∼p [DH2 M (π), M (π) ] .
(11)
p∈∆(Π) M ∈M

It follows
 immediately from the results of Foster et al. (2023) (see Proposition 4.1) that decε (M , M ) ≤
inf γ>0 r-decoγ (M , M ) ∨ 0 + γε2 , so upper bounds on r-decoγ (M ) yield upper bounds on decε (M ), which
can in turn be inserted into Theorem 1.1 to yield upper bounds on minimax risk. While it is also possible to
directly upper bound decε (M , M ) without going through r-decoγ (M , M ), using r-decoγ (·) is more convenient
and does not lead to any significant quantitative loss in the resulting upper bounds.
We prove an upper bound on the multi-agent DEC of the instance M , in terms of the (single-agent) DEC
fk , defined in terms of M . To define these model classes, for M ∈ M and
of K different model classes M
k ∈ [K], we first define an induced single-agent model M |k as follows: a pure observation drawn from M |k (π)
has the distribution of the pure observation o◦ when o◦ ∼ M (π), and the reward drawn from M |k (π) has the
distribution of rk when (r1 , . . . , rK ) ∼ M (π). In short, the model M |k is identical to M but simply ignores
fk is defined to have policy space Πk , so that
the rewards of all agents except k. Next, the model class M
f
f
fk , which is indexed by
models in Mk are mappings M : Πk → ∆(R × O◦ ). Finally, we define the class M
Π−k × M, as follows:
fk = {πk 7→ M |k (πk , π−k ) : π−k ∈ Π−k , M ∈ M} .
M

(12)

Theorem 1.5. Let M = (M, Π, O, {Π0k }k , {Uk }k ) be a NE MA-DMSO instance satisfying Assumption 4.1.
Then for any γ > 0, it holds that8
sup
M ∈co(M)

r-decoγ (M , M ) ≤

K
X

sup

fk , M k ).
r-decoγ/K (M

fk )
k=1 M k ∈co(M

This result allows us to bound the MA-DEC using standard bounds on the single-agent DEC (Foster et al.,
2021). For example, for normal-form games with bandit feedback, where each player has Ak actions, it yields
PK
supM ∈co(M) r-decoγ (M , M ) . K · k=1 Aγk . See Section 4 for refinements concerning Markov games.
The proof of Theorem 4.1 employs a novel fixed-point argument: For each agent k, if all other agents commit
to some joint distribution, this induces a single-agent DMSO instance, and it is natural for agent k to play
the strategy that minimizes the single-agent DEC for this instance. Using Kakutani’s fixed point theoerem,
we show that it is possible for all K agents to apply this strategy simultaneously.
8 Here, the notation r-deco
fk , M k ) refers to the single-agent DEC for the model class M
fk ; see Section 1.6.
(M
γ/K

13

On the curse of multiple agents. In multi-agent reinforcement learning, the curse of multiple agents
refers to the situation in which the sample complexity required to learn an equilibrium scales exponentially
in the number of players (Jin et al., 2021b). In general, our upper bounds on sample complexity for the
MA-DMSO framework (Theorem 1.1) suffer from the curse of multiple agents due to the presence of the
estimation complexity term log|M|. For example, in a K-player normal-form game with A actions per player,
one has log|M| ≈ AK (using an appropriate discretization of M). Our final result shows that it is possible
to avoid the curse of multiple agents by replacing the estimation complexity log|M| with the maximum
size maxk log|Π0k | for each player’s deviation set, which is usually polynomial in the number of agents; the
tradeoff is that the result scales with the MA-DEC for the MA-DMSO instance in which the model class M
is convexified via M ← co(M).
Theorem 1.6 (Informal version of Theorem 5.1). Let M = (M, Π, O, {Π0k }k , {Uk }k ) be a CCE instance
(Definition 1.2) or a CE instance (Definition A.1) of the MA-DMSO framework. Then, for any T ∈ N,
Algorithm 1 outputs π
b ∈ Π such that with probability at least 1 − δ,



γ
maxk |Π0k |
?
e
Risk(T ) = hM (b
π ) ≤ O(K)
· inf r-decoγ (co(M )) + · log
,
γ>0
T
δ
where we adopt the convention that co(M ) ≡ (co(M), Π, O, {Π0k }k , {Uk }k ).
0
In normal-form games with K players and A actions per player, we have decoγ (co(M )) . A
γ and maxk log|Πk | =
log(A), so this result gives
r
poly(K) · A
Risk(T ) .
.
T
More broadly, Theorem 1.6 shows that it is generically possible to avoid the curse of multiple agents for
convex classes, including structured classes of normal-form games with bandit feedback such as games with
linear or convex payoffs. In general though, it does not lead to tight guarantees for non-convex classes such
as Markov games, and is best thought of as complementary to results for this setting (Jin et al., 2021b; Song
et al., 2021; Mao and Basar, 2022). The result is proven by adapting the powerful exploration-by-optimization
algorithm from the single-agent setting (Lattimore, 2022; Foster et al., 2022b) so as to exploit the unique
feedback structure of the multi-agent setting. We refer to Section 5 for details, as well as additional results
which highlight settings in which the curse of multiple agents cannot be avoided in the sense of Theorem 1.6.

1.6

Preliminaries

Below we provide additional technical preliminaries which will be used throughout our proofs.
Probability kernels. For probability spaces (X , X ) and (Y, Y ), a probability kernel P (·|·) from (X , X )
to (Y, Y ) is a mapping P : Y × X → [0, 1] which satisfies (1) for all x ∈ X , P (·|x) is a probability measure
on (Y, Y ), and (2) for all Y ∈ Y , the mapping x 7→ P (Y |x) is measurable with respect to X . To simplify
notation we often denote probability kernels as P : X → ∆(Y).
MA-DMSO framework. We adopt the same formalism for probability spaces as in Foster et al. (2021,
2023). Decisions are associated with a measure space (Π, P), and observations are associated with the
measure space (O, O). In the MA-DMSO framework, pure observations are associated with the measure space
(O◦ , O◦ ) and rewards are associated with a measure space (R, R), and furthermore, we have O = O◦ × RK
and O = O◦ ⊗ R ⊗K . Formally, a model M (· | ·) is a probability kernel from (Π, P) to (O, O). We denote the
set of all models as M+ . Note that M+ depends on the measure spaces (Π, P), (O, O); when we wish to make
t
1
1
t
t
this dependence explicit, we will write M+
Π,O . The history up to time t is given by H = (π , o ), . . . , (π , o ).
We define
Ωt =

t
Y

Ft =

(Π × O),

i=1

T
O
i=1

so that Ht is associated with the space (Ωt , F t ).
14

(P ⊗ O),

We assume throughout the paper that R = [0, 1] (which implies in particular that hM (π) ∈ [0, K] for all
M, π) unless otherwise stated. To simplify notation, for each π ∈ Π and M ∈ M, we write hM
k (π) :=
PK
supπk0 ∈Πk fkM (Uk (πk0 , π)) − fkM (π), so that hM (π) = k=1 hM
(π).
k
The canonical single-agent instance. Given a decision space Π, an observation space O = O◦ × R, and
a model class M ⊂ (Π → ∆(O)), there is a canonical single-agent instance M = (M, Π, O, {Π0k }k , {Uk }k )
corresponding to the model class M: we take Π01 = Π and U1 (π10 , π) = π10 , which ensures that hM (π) =
maxπ0 ∈Π f M (π 0 ) − f M (π) for all π ∈ Π, M ∈ M. The single-agent instance M of the 1-player MA-DMSO
framework exactly captures the DMSO framework in Foster et al. (2021, 2023) for the model class M.
Furthermore, for any model M , we will write decε (M, M ) = decε (M , M ) (and similarly we will write
r-decoγ (M, M ) = r-decoγ (M , M ) for regret variant of the offset DEC introduced in Section 4); the quantity
decε (M, M ) = decε (M , M ) is identical to the constrained (PAC) DEC of the model class M as defined in
Foster et al. (2023), and the quantity r-decoγ (M, M ) = r-decoγ (M , M ) is identical to the offset (regret) DEC
of the model class M as defined in Foster et al. (2021).
HR-DMSO framework. As in the MA-DMSO framework, decisions are associated with a measure space
(Π, P), observations are associated with the measure space (O, O), and models M (· | ·) are probability kernels
from (Π, P) to (O, O). The history up to time t is given by Ht = (π 1 , o1 ), . . . , (π t , ot ), and is associated with
the space (Ωt , F t ) given by
t

Ω =

t
Y

F =
t

(Π × O),

i=1

T
O

(P ⊗ O).

i=1

We denote the set of all models as M+ . Unless stated otherwise, we will assume throughout that f M (π) ∈ [0, 1]
for all M ∈ M+ and π ∈ Π.
For a model M and decision π ∈ Π, EM,π [·] denotes expectation under the process o ∼ M (π). To simplify
?
notation, we often abbreviate g M (π) := f M (πM ) − f M (π), so that Risk(T ) = Eπb∼p [g M (b
π )].
Density ratios.

For both the MA-DMSO and HR-DMSO, we define


M (A | π)
∨ e.
V (M) := sup sup sup
M 0 (A | π)
M,M 0 ∈M π∈Π A∈O

(13)

Finiteness of V (M) is not necessary for our results to hold, but improves several of our bounds by a log(T )
factor.
Divergences. Total variation distance is given by
1
DTV (P, Q) = sup |P(A) − Q(A)| =
2
A∈F
and the Kullback Leibler divergence is given by

 R
dP
log dQ
dP,
DKL (P k Q) =
+∞,

Z
|dP − dQ|,

P  Q,
otherwise.

Minimax sample complexity. Formally, for T ∈ N, an algorithm (for either
 the HR-DMSO or MA-DMSO
frameworks) is a collection of probability kernels (p, q) = p(· | ·), {q t (· | ·)}Tt=1 , where each q t : Ωt−1 → ∆(Π)
is a probability kernel from (Ωt−1 , F t−1 ) to (Π, P), and p : ΩT → ∆(Π) is a probability kernel from (ΩT , F T )
to (Π, P). We let PM,(p,q) denote the law of (HT , π
b) under the process:
π t ∼ q t (· | Ht−1 ), ot ∼ M (· | π t ), ∀t ∈ [T ],

15

π
b ∼ p(· | HT ),

and we use EM,(p,q) to denote the corresponding expectation. Our main goal is to characterize the minimax
PAC sample complexity of an instance M = (M, Π, O, {Π0k }k , {Uk }k ) of the MA-DMSO framework or
H = (M, Π, O, {f M (·)}M ) of the HR-DMSO framework. The minimax sample complexities for both cases
are defined in an identical manner, spelled out below:
"K
#
X
?
M(M , T ) := inf sup EM ,(p,q) Eπb∼p(·|HT )
sup fkM (Uk (πk0 , π
b)) − fkM (b
π) ,
(p,q) M ? ∈M

M(H , T ) := inf

0

0

k=1 πk ∈Πk

?

?

?

sup EM ,(p,q) Eπb∼p(·|HT ) [f M (πM ? ) − f M (b
π )].

(p,q) M ? ∈M

1.7

Organization

This paper is organized as follows. First,Section 2 and Section 3 present our main results:
• Section 2 establishes a certain equivalence between the MA-DMSO and HR-DMSO.
• Section 3 establishes upper and lower bounds on the minimax rates for both frameworks based on the
Decision-Estimation Coefficient, and highlights barriers to obtaining sharper guarantees analogous to
those found in the basic DMSO framework (Foster et al., 2023).
Section 4 and Section 5 then present additional results concerning the MA-DMSO framework:
• Section 4 gives general conditions under which it is possible to bound the MA-DEC in terms of the
single-agent DEC.
• Section 5 gives conditions under which one can obtain sample complexity guarantees in the MA-DMSO
framework that avoid the so-called curse of multiple agents, as well as examples in which this is not
possible.
All proofs are deferred to the appendix. Further examples for both frameworks are given in Appendix A.
Additional notation. For an integer n ∈ N, we let [n] denote the set {1, . . . , n}. For a set X , we let ∆(X )
denote the set of all probability distributions over X . For x ∈ X , we use Ix ∈ ∆(X ) to denote the distribution
e
which places probability mass 1 on x. We adopt standard big-oh notation, and write f = O(g)
to denote
that f = O(g · max{1, polylog(g)}). We use . only in informal statements to emphasize the most relevant
aspects of an inequality. For a set X , let P(X ) denote the power set of i.e., the set of all subsets of X .

2

Equivalence of MA-DMSO and HR-DMSO frameworks

In this section, which forms the starting point for our main results, we show that the MA-DMSO and
HR-DMSO frameworks satisfy a certain statistical equivalence. First, in Theorem 2.1, we formalize the trivial
direction of this equivalence: namely, any instance of the MA-DMSO framework can be viewed as an instance
of the HR-DMSO framework. To state the result, recall that per our convention, the full observation space in
a MA-DMSO instance is denoted by O = O◦ × RK .
Theorem 2.1 (Reducing MA-DMSO to HR-DMSO). Consider any instance of the MA-DMSO framework
satisfying Assumption 1.3 and Assumption 1.2 and specified by the tuple M = (M, Π, O, {Π0k }k , {Uk }k ).
Then for some choice of value functions feM , the instance of the HR-DMSO framework specified by the tuple
H = (M, Π, O, {feM }M ), satisfies:
1. For all models M and ε > 0, decε (H , M ) = decε (M , M ).
2. For all T ∈ N, M(H , T ) = M(M , T ).
This result proceeds by choosing the value function feM (π) = K − hM (π). Note that for this reduction to
be admissible, it is critical that rewards are hidden: the function hM (π) is not observed directly in the
MA-DMSO framework, and as we will see, this is a source of fundamental hardness.

16

Theorem 2.1 is a fairly immediate result, and it is natural to imagine that the HR-DMSO framework might
truly be more general than the MA-DMSO framework, especially since rewards are observed in the latter. The
following result, which is the formal version of Theorem 1.3, shows that if one allows for small approximation,
any instance of the HR-DMSO framework can be embedded in a two-player, zero-sum NE instance for
MA-DMSO with minimal increase in complexity.
Theorem 2.2 (Reducing HR-DMSO to MA-DMSO). Consider any instance of the HR-DMSO framework
specified by the tuple H = (M, Π, O, {f M (·)}M ). Then for any V ∈ N, there is a two-player zero-sum NE
f Π,
e O,
e Π0 , Uk ) for the MA-DMSO framework (Definition 1.1) such that:
instance M = (M,
k
√
1. For all ε > 0, decε (M ) ≤ decε (H ) ≤ 6/ V + decε+(6/V )−1/2 (M ).
2. For all T ∈ N, it holds that M(M , T ) ≤ M(H , T ) ≤ M(M , T ) + O((T log(T )/V )1/4 ).
f is indexed by tuples (M, i) ∈ M × [V ]. In particular, if M is finite, then log |M|
f = log |M| + log V .
3. M
The main consequence of this result is that characterizing the minimax sample complexity for the MA-DMSO
is no easier than characterizing the minimax sample complexity for the HR-DMSO framework; this will allow
us to restrict our attention to the latter task for the results that follow. Let us make some additional remarks.
• As we increase the parameter V , the approximation to the minimax rate in Theorem 2.2 improves.
Choosing V = poly(T ) suffices for all settings of interest, the only tradeoff is that the size of the model
class M increases from log|M| to log|M| + log V . For the results we consider in subsequent sections,
this increase will be inconsequential (beyond log(T ) factors).
• Beyond preserving the minimax risk, both reductions preserve the value of the Decision-Estimation
Coefficient, which is a consequence of preserving rewards and Hellinger distances for models in the
class. This will become relevant for our results in the sequel (Section 3), where we show that the DEC
is closely connected to minimax risk, yet not completely equivalent.
• Both reductions are algorithmic in nature. For example, suppose that we start with a HR-DMSO
instance H and produce a MA-DMSO instance M via the reduction in Theorem 2.2. Then any
algorithm that achieves low risk for every model in M can be efficiently lifted to an algorithm for the
original class H .
Theorem 2.2 is proven by embedding a given instance H for the HR-DMSO framework into a two-player
zero game instance M , where the first of the two agents plays the role of the HR-DMSO agent. The key
properties of the embedding are that:
1. The second agent selects actions to ensure that near-optimal decisions for the original HR-DMSO
instance form Nash equilibria for the new instance, and vice-versa.
2. Even though rewards in the game instance M are observed, by increasing the size of the game (as a
function of the parameter V ), we can ensure that discovering an action with non-zero reward requires a
prohibitively large amount of exploration, rendering them hidden (up to small approximation error).

3

Upper and lower bounds on minimax rates

This section presents our results regarding minimax rates for the MA-DMSO and HR-DMSO frameworks. We
work in the HR-DMSO framework for the majority of the section, and give implications for the MA-DMSO
at the end, using the equivalence from Section 2. In more detail:
• In Section 3.1, we give upper and lower bounds on the minimax rates for interactive decision making in
the HR-DMSO framework, which scale with the constrained DEC.
• Next, we establish in Section 3.2 that, under mild regularity assumptions on the constrained DEC,
the upper and lower bounds on the minimax rate are separated by at most a polynomial factor
(ignoring the estimation error term); for most parameter regimes, the gap between the bounds is at
most quadratic. We then show—perhaps surprisingly—that neither the upper or lower bounds can be

17

improved, in that there are instances where each is nearly tight. In other words, in contrast to the
DMSO framework (Foster et al., 2023), in the HR-DMSO framework, the constrained DEC cannot not
give a characterization of the minimax sample complexity which is tight beyond a quadratic factor. We
show further that this gap is not limited to the constrained DEC, and in fact holds for an entire family of
complexity measures based on pairwise f -divergences between models. As a result, any characterization
of the minimax rate for HR-DMSO which is tight up to polylogarithmic factors must use a complexity
measure substantially different from those considered in recent works (Foster et al., 2021, 2022b, 2023).
• Finally, using the equivalence shown in the previous section, we establish (Section 3.3) that all of the
results above hold verbatim in the MA-DMSO framework.
All of the results in this section are presented in a general form. We refer to Part I of the appendix for
applications to specific instances of interest.

3.1

HR-DMSO: Upper and lower bounds on minimax rates

We now give upper and lower bounds on the minimax risk for the HR-DMSO framework. We obtain
upper bounds as an immediate corollary of regret bounds for the Estimation-to-Decisions+ (E2D+ for PAC)
algorithm from recent work of Foster et al. (2023). The E2D+ for PAC algorithm was introduced in the
(single-agent/non-hidden-reward) DMSO framework, where it leads to tight upper bounds on minimax risk
based on the constrained DEC (Foster et al., 2023). We observe that it provides identical guarantees for
the more general HR-DMSO framework without modification; this can be seen by inspecting the proof of
correctness of the E2D+ for PAC algorithm in Foster et al. (2023) and noting that it does not make use of the
fact that the learning agent observes the rewards r1 , . . . , rT . Further background on the algorithm may be
found in Appendix D.1.
Our main upper bound is stated for the case in which M is finite (|M| < ∞); more general guarantees for
infinite classes are given in Appendix D.1.

1
Theorem 3.1 (Minimax upper bound for HR-DMSO (Foster et al., 2023)). Fix δ ∈ 0, 10
and T ∈
M
N, and consider
any
instance
H
=
(M,
Π,
O,
{f
(·)}
).
Suppose
that
Assumption
1.4
holds.
Letting
M
q

+
ε(T ) := 16 dlogT2/δe · log |M|
δ , the E2D for PAC algorithm, when configured appropriately, guarantees that
with probability at least 1 − δ,

Risk(T ) ≤ decε(T ) (H ).
In addition, if f M (·) ∈ [0, R] for all M ∈ M and some R > 0, then the expected risk is bounded as
E[Risk(T )] ≤ decε(T ) (H ) + δR.
Before interpreting this result, we complement it with our main lower bound, Theorem 3.2, which shows
that the minimax risk for any algorithm is lower bounded by the constrained DEC for an appropriate choice
of the scale parameter ε > 0. The statement of this result uses the definition C(T ) := log(T ∧ V (M)). In
addition, we recall that g M (π) := f M (πM ) − f M (π).
Theorem 3.2 (Minimax lower bound for HR-DMSO). Consider any instance H = (M, Π, O, {f M (·)}M )
and write R := supπ∈Π,M ∈M g M (π). Given T ∈ N, let ε(T ) > 0 be chosen as large as possible such that
ε(T )2 · C(T ) · R · T ≤

1
· decε(T ) (H ).
8

(14)

Then for any algorithm, there exists a a model in M for which
E[Risk(T )] ≥

1
· decε(T ) (H ).
6

Understanding the bounds. We now give a sense for the behavior of the lower bound of Theorem 3.2
and the upper bound of Theorem 3.1 through several examples. For simplicity we consider the case that
R = supπ∈Π,M ∈M g M (π) = 1 (in the context of Theorem 3.2).
18

•

√

T -rates. Most of the classes studied
√ in the literature on bandits and reinforcement learning have the
property that the optimal rate is O( T ). Many of these problems have the property that rewards are
observed (i.e., they lie in the DMSO framework), but such rates also arise for problems in HR-DMSO
for which rewards are not observed; a notable example is locally observable finite partial
monitoring
p
problems (Bartók et al., 2014). For such classes, it holds that decε (H ) ∝ ε · Cprob , for some
problem-dependent constant Cprob > 0 reflecting the complexity of the model class M (see Foster
et al. (2021,
p 2023) for examples). In this case, by choosing a failure probability of δ = 1/T , we have
ε(T ) . log(T ) log(T |M|)/T , so that Theorem 3.1 gives an upper bound of
!
r
C
log
|M|
prob
e
E[Risk(T )] ≤ O
T
p
on the minimax risk. For lower bounds, if decε (H ) ∝ ε · Cprob , then the solution to the fixed point
p
equation (14) is ε(T ) & Cprob /(T · C(T )). This translates, via Theorem 3.2, into a lower bound of


Cprob
e
E[Risk(T )] ≥ Ω
T
on the minimax risk, which differs from the upper bound by a quadratic factor (ignoring the log |M|
factor). By the results of Foster et al. (2023),
q for the special
 case where rewards are observed (i.e., the
C
log
|M|
prob
e
DMSO framework), the upper bound of O
is the correct rate (up to the log |M| factor
T

and log T factors). We will show in the sequel that for general settings where rewards are not observed,
this is not necessarily the case, and the lower bound can be tight.
√
• Nonparametric rates. For nonparametric model classes, for which the optimal regret is ω( T ), it is
typically the case that decε (H ) ∝ ε1−ρ for some ρ ∈ (0, 1). For such problems, Theorem 3.1 yields

e (log |M|/T )(1−ρ)/2 on the minimax risk. In contrast, the best
an upper bound of E[Risk(T )] ≤ O
1
possible solution to the fixed point equation in (14) is
) &1/(T · C(T )) 1+ρ , which translates, via
 ε(T 1−ρ
e 1/T 1+ρ on the minimax risk. Here the lower
Theorem 3.2, into a lower bound of E[Risk(T )] ≥ Ω
2
≤ 2. By the
bound is off from the upper bound (ignoring the log |M| factor) by a power of 1+ρ
results of Foster et al. (2023), for the special case where rewards are observed, the upper bound of

e (log |M|/T )(1−ρ)/2 is the correct rate (up to the log |M| factor and log T factors).
O

We refer to Foster et al. (2023) for concrete examples exhibiting the growth rates sketched above for the
special case where rewards are observed (DMSO), and to Part I of the appendix for examples arising from
MA-DMSO.

3.2

HR-DMSO: Gaps between bounds and impossibility of tight characterizations

We now investigate the nature of the gap between the upper and lower bounds in Theorems 3.1 and 3.2. We
first give a generic bound on the gap, then show that it is not possible—in a fairly strong sense—to close the
gap further.
3.2.1

On the gap between the upper and lower bounds

Ignoring constant factors, the only difference between the upper and lower bounds of Theorems
q 3.1 and 3.2 is the

scale ε at which the DEC is computed. The upper bound of Theorem 3.1 uses scale ε(T ) = 8 dlogT2/δe · log|M|,
whereas the lower bound of Theorem 3.2 (with R = 1) uses the scale ε(T ), which is defined implicitly to be
as large as possible subject to the constraint ε(T )2 · C(T ) · T ≤ 18 · decε(T ) (H ). Thus, the size of the gap
between ε(T ) and ε(T ) controls the degree of tightness of these upper and lower bounds. In what follows,
we give a bound on the size of this gap that holds whenever the constrained DEC satisfies the following
regularity assumption.
19

Assumption 3.1 (Regularity). An instance H (of either HR-DMSO or MA-DMSO) is said to satisfy the
regularity condition with constants Creg , creg > 1 at scale ε ∈ (0, 2) if
decε (H ) ≤ c2reg · decε/Creg (H ).

Most natural classes satisfy Assumption 3.1 for some constants creg , Creg (in particular, the condition is
satisfied whenever decε (H ) ∝ εp for p < 2). We note that a similar assumption used in Foster et al. (2023)
to give upper bounds on the optimal rates attainable in the DMSO framework.
Under Assumption 3.1, the following result shows that our upper bound on minimax risk, which scales with
decε(T ) (H ), is bounded above by a quantity that is a polynomial of our lower bound, namely decε(T ) (H ).
Proposition 3.1. Suppose that an instance H (for either HR-DMSO or MA-DMSO) satisfies Assumption
c
log c
, ε(T )). Choose any β ≥ log(Cregreg
3.1 for some values Creg > creg > 1 and for all ε ∈ (ε(T ) · Creg
/creg ) . Then
reg
for any T ∈ N,
β

1

decε(T ) (H ) ≤ (C log 1/δ · log|M| · C(T ) · Creg /creg ) 1+β · decε(T ) (H ) 1+β .

We remark that Proposition 3.1 is a purely algebraic fact that makes no use of the structure of the DEC, and
in particular holds for instances of both the HR-DMSO and MA-DMSO frameworks. To make the result
concrete, we consider, we revisit each of the situations we discussed in Section 3.1, and describe how applying
Proposition 3.1 allows us to conclude that our upper and lower bounds are related by a polynomial factor.
√
p
• T -rates. Suppose that decε (H ) ∝ ε · Cprob , for some problem-dependent constant Cprob > 0. Then,
for any constant β > 1, there is a sufficiently large absolute constant Creg > 1 so that, for all ε > 0,
β
decε (H ) ≤ Creg
· decε/Creg (H ). It follows that Assumption 3.1 is satisfied with the constants Creg and
β/2

log c

creg := Creg (which satisfy β ≥ log(Cregreg
/creg ) ), and Proposition 3.1 gives that
β

1

e
1+β · dec
1+β .
decε(T ) (H ) ≤ O(log|M|)
ε(T ) (H )
Disregarding the estimation error and taking β → 1, we conclude that decε(T ) (H ) . decε(T ) (H )1/2−o(1) ,
i.e., there is a (roughly) quadratic gap between our upper and lower bounds.
• Nonparametric rates. Suppose that decε (H ) ∝ ε1−ρ for some ρ ∈ (0, 1). Then for any constant β > 1−ρ
1+ρ ,
β(1+ρ)

there is a sufficiently large constant Creg > 1 so that, for all ε > 0, decε (H ) ≤ Creg
β(1+ρ)/2

Thus, Assumption 3.1 is satisfied with the constants Creg and creg := Creg
log creg
log(Creg /creg ) , and Proposition 3.1 gives that
β

· decε/Creg (H ).

, which satisfy β ≥

1

e
1+β · dec
1+β .
decε(T ) (H ) ≤ O(log|M|)
ε(T ) (H )
1+ρ
1
Disregarding the estimation error and taking β → 1−ρ
1+ρ (so that 1+β →
2 ), we conclude that
1+ρ

decε(T ) (H ) . decε(T ) (H ) 2 −o(1) , i.e., the gap between the upper and lower bounds is smaller than

quadratic.

Of course, the arguments in Section 3.1 already allowed us to draw these conclusions directly; the purpose
here is to exhibit how this conclusion can obtained as a special case of the more general Proposition 3.1.
3.2.2

On tight characterizations for the minimax risk

It is natural to wonder whether the polynomial gap between our upper and lower bounds can be tightened to
give a characterization of the minimax risk up that is only loose by polylogarithmic factors. In this section,
we show that this is not possible in several senses.

20

Tightness of the upper and lower bounds. In Propositions 3.2 and 3.3, we give two instances H1
H2 , so that, up to log 1ε factors, we have both decε (H1 )  ε and decε (H2 )  ε. Despite having the same
behavior for the DEC, the minimax rates for the √
instances are different: For the instance H1 , the upper
bound from Theorem 3.1 is tight (M(H1 , T ) & 1/ T ), yet for H2 , the lower bound from Theorem 3.2 is
tight (M(H2 , T ) . log(T )/T ).
Proposition 3.2 (An instance where the upper bound is tight). For any sufficiently L, A ∈ N, there is an
instance H1 = (M, Π, O, {f M (·)}M ) with log |M| ≤ log(LA) and which satisfies the following properties:
p
1. For all T ≤ 2L/2 , the minimax rate for H1 is given by M(H1 , T ) = Θ( A/T ).
√
√
√
2. For all ε ∈ (2−L , 1/ A), it holds that c · ε A ≤ decε (H1 ) ≤ C · ε A, for some constants c, C > 0.
The instance H1 in Proposition 3.2 has model class given by a subclass of multi-armed bandit problems with
A arms and Bernoulli rewards, and the bounds in the proposition are an immediate consequence of prior
work. We provide a proof in Section 3.2 for completeness.
Proposition 3.3 (An instance where the lower bound is tight). For any sufficiently large L ∈ N and any
Cprob ≥ 1, there exists an instance H2 = (M, Π, O, {f M (·)}M ) with log |M| ≤ L2 , satisfying the following
properties:
1. For all T ≤ 2L , the minimax rate for the instance H2 is bounded as M(H2 , T ) ≤
√

2
8Cprob
log T
.
T

C

2. For all ε ≥ Cprob2·2L , we have √prob
· ε ≤ decε (H2 ) ≤ 2Cprob · ε. In particular, ε(T ) ≥ Ω
8·L



Cprob
T log(T )·L



as

long as T ≤ 2 /L .
L

3

In particular, for any T ∈ N, by choosing L = 100 log T , we have that for all ε ≥ Ω



1
Cprob ·T 100



, the

instance H2 satisfies, Ω(ε · Cprob / log 1ε ) ≤ decε (H2 ) ≤ O(ε · Cprob ), yet the minimax risk is bounded as
2
M(H2 , T ) ≤ O(Cprob
log(T )/T ).
Let us compare the instances for Proposition 3.2 and Proposition 3.3. First, note that for both instances, the
e
estimation complexity log|M| scales as O(1).
Thus:
√
e
• Theorem 3.1, using the radius ε(T ), yields an upper bound on the minimax risk of O(1/
T ), which is
tight for H1 .
e
), which is tight
• Theorem 3.2, using the radius ε(T ), yields a lower bound on the minimax risk of Ω(1/T
for H2 .
That is, the instance H1 establishes that our upper bound cannot be improved to use the radius ε(T ), and
the instance H2 establishes that our lower bound cannot be improved to use the radius ε(T ). More generally,
since decε (H1 ) and decε (H2 ) have the same behavior, yet H1 and H2 have different minimax rates, the
constrained DEC cannot give a tight characterization of the minimax risk for the HR-DMSO framework.
This contrasts the situation for the (reward-observed) DMSO framework in Foster et al. (2023), where the
e
constrained DEC characterizes the minimax rates up to logarithmic factors whenever log|M| = O(1).
We remark in passing that the instances constructed in Propositions 3.2 and 3.3 satisfy the regularity condition
of Assumption 3.1 for Creg , creg ≤ O(log T ) and all ε ≥ ε(T ). Thus, the regularity condition is not sufficient
to close the gap between the upper and lower bounds.
Ruling out more general characterizations. We now show that the gaps highlighted above are not
limited to the DEC, and are in fact intrinsic to a broad class of complexity measures. Our main result,
Theorem 3.3 shows that for any f -divergence D(· k ·) satisfying a mild assumption, it is possible to construct
two HR-DMSO instances H1 and H2 for which the minimax risk differs by a polynomial factor, yet 1) the
value functions associated with H1 and H2 are identical, and 2) the pairwise D(· k ·)-divergences between all
models in H1 and H2 are identical. In other words:
It is impossible to obtain a tight characterization for minimax risk that depends only on value functions and
pairwise f -divergences.
21

Definition 3.1 gives our main technical assumption regarding f -divergences: roughly speaking, it states that
the function defining the f -divergence exhibits at most polynomial growth near 0 and ∞.
Definition 3.1 (Bounded f -divergence). Consider a convex function φ : [0, ∞) → [0, ∞] so that φ(1) = 0
and φ(x) is finite for all x > 0, and let
 

dP
(15)
Dφ (P k Q) := EQ φ
dQ
denote the associated f -divergence for probability measures P and Q with P  Q. For constants α, β ≥ 0, we
say that φ is (α, β)-bounded if, for all x ≥ 1,
φ(x)
≤ β · xα .
x
In such a case, we say that the f -divergence Dφ is (α, β)-bounded.
φ(1/x) +

Essentially all commonly used f -divergences
satisfy Definition 3.1 for small values of α and β. For the
√
Hellinger divergence, we have φ(x) = ( x − 1)2 , so that DH2 (·, ·) is (0, 2)-bounded; for the KL-divergence,
we have φ(x) = x ln(x) + 1 − x, so that DKL (· k ·) is (0, 2)-bounded; and for the χ2 -divergence, we have
φ(x) = (x − 1)2 , so that Dχ2 (·, ·) is (1, 1)-bounded.
Remark 3.1 (Non-negativity of φ). We remark that often, when f -divergences are presented, it is assumed
that the function φ maps to [−∞, ∞] (as opposed to [0, ∞]). Assuming that φ maps to [0, ∞] is without loss of
generality, for the following reason. It is well-known that for any c ∈ R, and for any convex function φ satisfying
e
φ(1) = 0, letting φ(x)
= φ(x) + c · (x − 1), we have Dφ = Dφe. Thus, given any φ : [0, ∞) → [−∞, ∞], we may
choose any c ∈ −∂φ(1), so that 0 ∈ ∂(φ(x) + c · (x − 1)), which in particular implies that φ(x) + c · (x − 1) ≥ 0
for all x, and the f -divergence induced by φ(x) + c · (x − 1) is equivalent to Dφ .
Theorem 3.3. For some constants α, β ≥ 0, suppose Dφ is an (α, β)-bounded f -divergence. Then for
any T ∈ N,  ∈ (0, 1), and Cprob ≥ 1, there are instances H1 = (M1 , Π1 , O1 , {f1M (·)}M ∈M1 ), H2 =
(M2 , Π2 , O2 , {f2M (·)}M ∈M2 ) of the HR-DMSO framework, so that Π1 = Π2 , O1 = O2 , and there is a
one-to-one mapping E : M1 → M2 satisfying:
1. For all M ∈ M1 , f1M ≡ f2E (M ) .
2. For all M, M 0 ∈ M1 , and π ∈ Π1 , Dφ (M (π) k M 0 (π)) = Dφ (E (M )(π) k E (M 0 )(π)).
1/2+

3. There is some constant Cφ depending only on φ so that for all T 0 with T ≤ T 0 ≤ T 3/2−2 ·(Cφ Cprob ln T )−1 ,
it holds that

1/2+/(2α)

1/2
1
Cprob
Cprob
0
0
−2−2/
M(H1 , T ) ≤ + 2 ·
,
M(H2 , T ) ≥ 2
·
.
T
T
T
In the event that α = 0, the quantity (Cprob /T )1/2+/(2α) in the statement of Theorem 3.3 is to be
interpreted as 0. In particular, if D(· k ·) is the Hellinger divergence or the KL divergence, then we have
M(H1 , T 0 ) ≤ 1/T in Item 3, giving a quadratic separation. If D(· k ·) is the χ2 -divergence, then we have
M(H1 , T 0 ) ≤ O(1/T 1/2+/2 ), which leads to a smaller, yet still polynomial separation for any choice of the
constant  > 0.
Several variants of the DEC and related complexity measures depend only on the value functions f M (·)
(for M ∈ M) and pairwise f -divergences between models in the class M, and thus cannot provide a
characterization for minimax risk in the HR-DMSO framework that is tight up to polylogarithmic factors.
Below, we highlight a few notable examples.
• The distributional offset DEC (Foster et al., 2021; Chen et al., 2022a; Foster et al., 2023), is defined for
H = (M, Π, O, {f M (·)}M ) as:9



deco,rnd
(H ) = sup
inf
sup Eπ∼p [f M (πM ) − f M (π)] − γ · Eπ∼q EM ∼ν DH2 M (π), M (π) .
γ
ν∈∆(M) p,q∈∆(Π) M ∈M

9 We consider the PAC variant of the offset DEC here (Foster et al., 2023), but it is clear that our argument applies identically

to the regret version of the DEC (Foster et al., 2021).

22

Clearly, this definition depends only on value functions {f M }M ∈M and pairwise Hellinger distances for
models in M, and hence can only characterize minimax risk up to a quadratic factor.
• The offset DEC (Foster et al., 2021, 2023) is defined for H = (M, Π, O, {f M (·)}M ) as:
decoγ (H ) =

sup

inf


sup Eπ∼p [f M (πM ) − f M (π)] − γ · Eπ∼q [DH2 M (π), M (π) ].

M ∈co(M) p,q∈∆(Π) M ∈M

Note that decoγ (H ) depends on the divergence between models in M and those in co(M), which is
not covered by Theorem 3.3. However, Foster et al. (2023, Proposition D.2) show that decoγ (H ) ≤
o,rnd
(H ) are equivalent up to constant factors),
decγ/4 (H ) ≤ decoγ/4 (H ) (that is, decoγ (H ) and deco,rnd
γ
so it follows from the previous bullet point that this complexity measure can only characterize minimax
risk up to a quadratic factor.
• Foster et al. (2021, 2023) consider variants of the DEC that are applied to localized subsets of the model
class M. In particular, the following two notions of localization have been considered in Foster et al.
(2021): for some localization radius α > 0, a model class M, and a reference model M ,
Mα (M ) :={M ∈ M : f M (πM ) ≤ f M (πM ) + α}, and

M
M
M
M
M∞
α (M ) := M ∈ M : (f (πM ) − f (π)) − (f (πM ) − f (π)) ≤ α ∀π ∈ Π .
Since these definitions only depend on the value functions {f M }M ∈M , Theorem 3.3 implies that
incorporating localization into the variants of the DEC considered above cannot help to provide a
characterization of the minimax risk.
• The information ratio (Russo and Van Roy, 2014, 2018; Lattimore and György, 2021) was introduced to
bound the Bayesian regret for posterior sampling and a more general algorithm known as informationdirected sampling. The information ratio of a model class M is closely related to the DEC of the convex
hull of M; in particular, Foster et al. (2022b) showed that a parametrized version of the information
ratio of M is equivalent to the DEC of the convex hull of M, up to constant factors. As the DEC of
co(M) involves pairwise Hellinger distances between models in the convex hull of M, Theorem 3.3 does
not definitively rule it out as providing a characterization of minimax risk. However, the DEC of co(M)
is known to be exponentially larger than the minimax risk for many natural examples (e.g., tabular
reinforcement learning (Foster et al., 2022b)), so it seems unlikely to provide a tight characterization.
There are also variants of the information ratio which Theorem 3.3 does rule out: given a reference
model M ∈ M and a distribution µ ∈ ∆(M), one can define (Foster et al., 2021)
(Eπ∼p EM ∼µ [f M (πM ) − f M (π)])2
 .
p,q∈∆(Π) Eπ∼q EM ∼µ [DKL M (π) k M (π) ]

I(H , M , µ) := arg min

As this definition depends only on value functions and pairwise KL-divergences for models in M,
Theorem 3.3, no function of I(H , M , µ) (such as a worst-case version of the information ratio defined
by maxM ∈M maxµ∈∆(M) I(H , M , µ)) can provide a characterization of minimax risk.
• Note that in general, the constrained DEC decε (H ) = supM ∈co(M) decε (H , M ) depends on Hellinger
divergences between models in M and those in co(M), so Theorem 3.3 does not directly rule out a
characterization in terms of decε (H ). However, we have already ruled out such a characterization
separately in Propositions 3.2 and 3.3. Of course, the variant supM ∈M decε (H , M ), which restricts to
M ∈ M, only depends on the value functions and pairwise Hellinger divergences of models in M, and
hence is covered by Theorem 3.3.
Let us remark that one complexity measure not currently ruled out by our results is the generalized information
ratio considered in the work of Lattimore (2022) on adversarial partial monitoring, which uses an unnormalized
KL-like divergence based on the logarithmic barrier, and cannot be written in terms of f -divergences. The
upper and lower bounds on regret given by Lattimore (2022) are loose by poly(|Π|) factors, and as such we
find it to be unlikely that this complexity measure can give tight guarantees in the “large decision-space/model
class” regime where T  min{|M|, |Π|}, which is the focus of our work.
23

Remark 3.2. While this is out of scope for the present paper, we remark that it is possible to establish
similar impossibility results for the regret (as opposed to PAC) framework.

3.3

Implications for MA-DMSO framework

Up to this point, all of the results in this section concerned the HR-DMSO framework. Using Theorems 2.1
and 2.2, we can immediately derive analogous results for the MA-DMSO framework. In what follows, we state
these analogues (in particular, upper and lower bounds on minimax risk, and impossibility of tighter results),
all of which are corollaries the results in the prequel. We refer to Part I of the appendix for applications of
these results.
Upper and lower bounds on minimax risk. We begin by stating upper and lower bounds for the
minimax risk for instance of MA-DMSO in terms of the Multi-Agent DEC; these results are corollaries of
Theorems 3.1 and 3.2.

1
Corollary 3.1 (Minimax upper bound for MA-DMSO). Fix δ ∈ 0, 10
and T ∈ N, and consider any
0
M
K-player MA-DMSO instance
M
=
(M,
Π,
O,
{Π
}
,
{U
}
).
Suppose
that
f
k k
k k
k (·) ∈ [0, 1] for all k ∈ [K] and
q
M ∈ M, and let ε(T ) := 16

dlog 2/δe log |M|
· δ . Then we have
T

(16)

M(M , T ) ≤ decε(T ) (M ) + Kδ.

Proof of Corollary 3.1. Given an instance M of MA-DMSO, consider the instance H = (M, Π, O, {feM }M )
as per Theorem 2.1. We have that hM (·) ∈ [0, K] for all M ∈ M, meaning that feM (·) ∈ [−K + 1, 1] for
all M ∈ M under the construction in the proof of Theorem 2.1. By rescaling feM (·), the guarantee from
Theorem 3.1 ensures that M(H , T ) ≤ decε(T ) (H ) + Kδ, from which (16) follows using Theorem 2.1. We have
also used here that both M(H , T ) and decε (H ) scale linearly under rescaling of the value functions feM (·).
As we discuss further in Remark D.1, the high-probability guarantee from Theorem D.1 applies also in the
MA-DMSO setting, i.e., in the contex of Corollary 3.1.
Corollary 3.2 (Minimax lower bound for MA-DMSO). Consider any instance M = (M, Π, O, {Π0k }k , {Uk }k )
for the MA-DMSO framework with R = [0, 1]. Given T ∈ N, let ε(T ) > 0 be chosen as large as possible such
that ε(T )2 · C(T ) · K · T ≤ 18 · decε(T ) (M ). Then
M(M , T ) ≥

1
· decε(T ) (M ).
6

Corollary 3.2. Given an instance M of MA-DMSO, consider the instance H = (M, Π, O, {feM }M ) as per
Theorem 2.1. By definition of feM , we have that supπ∈Π,M ∈M supπ0 ∈Π feM (π 0 ) − feM (π) ≤ K. Then we have
M(M , T ) = M(H , T ) ≥ 16 · decε(T ) (H ) = 16 · decε(T ) (M ), where the two equalities use Theorem 2.1 and
the inequality uses Theorem 3.2.
As we have already remarked, Proposition 3.1, which bounds the gap between our upper and lower bounds
based on the DEC, already applies to instances of MA-DMSO whenever Assumption 3.1 is satisfied. In
particular, this means that whenever decε (M ) ∝ ε1−ρ for ρ ∈ [0, 1), we have
1−ρ

1−ρ

1+ρ

e 1+ρ log 2 |M|) · decε(T ) (M ) 2 .
decε(T ) (M ) ≤ O(K
Tightness of the gaps. Next, we provide analogues of Propositions 3.2 and 3.3 for the MA-DMSO. The
results construct MA-DMSO instances M1 (Proposition 3.4) and M2 (Proposition 3.5) that exhibit the
√ same
DEC behavior, in that decε (M1 )  ε and decε (M2 )  ε, yet have minimax rates: M(M1 , T ) & 1/ T and
M(M2 , T ) . log(T )/T . In particular, Proposition 3.4 below shows that in the upper bound Corollary 3.1,
the scale ε(T ) cannot be decreased, and Proposition 3.5 below shows that in the lower bound Corollary 3.2,
the scale ε(T ) cannot be increased.
24

Proposition 3.4. For any sufficiently large L, A ∈ N, there is an instance M1 = (M, Π, O, {Π0k }k , {Uk }k )
with log |M| ≤ log(LA) and which satisfies the following properties:
p
1. For all T ≤ 2L/2 , the minimax rate for the instance M1 is given by M(M1 , T ) = Θ( A/T ).
√
√
√
2. For all ε ∈ (2−L , 1/ A), it holds that c · ε A ≤ decε (M1 ) ≤ C · ε A, for some constants c, C > 0.
Proof of Proposition 3.4. We observe that the instance H = (M, Π, O, {f M (·)}M ) used to prove Proposition 3.2 immediately yields the 1-player instance of MA-DMSO given by M1 = (M, Π, O, Π01 , U1 ), with
Π01 = Π and U1 (π10 , π) = π10 , since rewards are observed under all models in M. The result then follows
immediately from Proposition 3.2.
Proposition 3.5. For any sufficiently large L ∈ N and any Cprob ≥ 1, there exists an instance M2 =
(M, Π, O, {Π0k }k , {Uk }k ) with log |M| ≤ O(L2 + log Cprob ), satisfying the following properties:
1. For all T ≤ 2L , the minimax rate for the instance M2 is bounded as M(M2 , T ) ≤
√

2
8Cprob
log T
.
T

C

2. For all ε ≥ Cprob2·2L , we have √prob
· ε ≤ decε (M2 ) ≤ 2Cprob · ε. In particular, ε(T ) ≥ Ω
8·L



Cprob
T log(T )·L



as

long as T ≤ 2 /L .
L

3

Proof of Proposition 3.5. Given L and Cprob , let H = (M, Π, O, {f M (·)}M ) be the instance given per
f Π,
e O,
e {Π0 }k , {Uk }k ) be the instance constructed per Theorem 2.2 for
Proposition 3.3. Next, let M2 = (M,
k
2
f = log |M| + log V ≤ O(L2 + log Cprob ). Using
the instance H with V = 100 · Cprob
· 22L . We have log |M|
the guarantees of Proposition 3.3 and Theorem 2.2, we have M(M2 , T ) ≤ M(H , T ) ≤
√
p
2
(which ensures that ε − 6/V ≥ Cprob2·2L ),
and for all ε ≥ Cprob
·2L

2
8Cprob
log T
T

for T ≤ 2L ,



p
√
C
√ prob · ε − 6/V ≤ decε−(6/V )−1/2 (H ) − 6/ V ≤ decε (M2 ) ≤ decε (H ) ≤ 2Cprob · ε.
8·L
p
C
1
Since ε ≥ Cprob
implies that 6/V ≤ ε/2, it follows that 2√prob
· ε ≤ decε (M2 ) ≤ 2Cprob · ε.
·2L
8L
Ruling out more general characterizations. Finally, we state an analogue of Theorem 3.3 for the
MA-DMSO framework, which shows that any complexity measure that dependence on the instance M only
through value functions and pairwise f -divergences can only characterize the minimax risk up to polynomial
factors.
Theorem 3.4. For some constants α, β ≥ 0, suppose that Dφ is an (α, β)-bounded f -divergence (Definition 3.1). Then for any T ∈ N,  > 0, and Cprob ≥ 1, there are instances M1 = (M1 , Π, O, {Π0k }k , {Uk }k ),
M2 = (M2 , Π, O, {Π0k }k , {Uk }k ) of the MA-DMSO framework, so that there is a one-to-one mapping
E : M1 → M2 satisfying:
1. For all M ∈ M1 , f1M ≡ f2E (M ) .
2. For all M, M 0 ∈ M1 , and π ∈ Π, Dφ (M (π) k M 0 (π)) = Dφ (E (M )(π) k E (M 0 )(π)).
1/2+

3. There is some constant Cφ depending only on φ so that for all T 0 with T ≤ T 0 ≤ T 3/2−2 ·(Cφ Cprob ln T )−1 ,
it holds that

1/2+/(2α)

1/2
1
Cprob
Cprob
0
0
−3−2/
M(M1 , T ) ≤ + 2 ·
, yet M(M2 , T ) ≥ 2
·
.
T
T
T
The proof uses the equivalence of Theorem 2.2 to translate the construction of HR-DMSO instances in
Theorem 3.3 to the MA-DMSO framework. Since Theorem 3.3 makes a claim about pairwise f -divergences
as opposed to the constrained DEC of the instance, decε (H ), we cannot apply Theorem 2.2 in an entirely
black-box manner, yet most of the reasoning from the proof of Theorem 2.2 carries over.
25

4

MA-DMSO: From multi-agent to single-agent

Having established upper and lower bounds on the minimax risk for the MA-DMSO framework based on the
Multi-Agent Decision-Estimation Coefficient, we spend the remainder of the paper providing structural results
which can be used to apply our main risk bounds to concrete settings of interest. To this end, in section we
provide generic results which allow the conditions under which the multi-agent DEC can be controlled by the
single-agent DEC, thereby allowing one to lift the plethora of existing results for the single-agent setting
(Foster et al., 2021, 2023) to multiple agents.
Induced single-agent model classes. Consider a Nash equilibrium instance M = (M, Π, O, {Π0k }k , {Uk }k )
for the MA-DMSO framework (Definition 1.1), recalling that Π = Π1 × · · · × ΠK . We will prove upper
bounds on the multi-agent DEC of the instance M in terms of the single-agent DEC for a collection of
fk defined based on M . To define the model classes M
fk , for M ∈ M
induced single-agent model classes M
and k ∈ [K], we first define a single-agent model M |k as follows: a pure observation drawn from M |k (π) has
the distribution of the pure observation o◦ when o◦ ∼ M (π), and the reward drawn from M |k (π) has the
distribution of rk when (r1 , . . . , rK ) ∼ M (π). In other words, the model M |k is identical to M but ignores
the rewards of all agents except k.
fk is defined to have policy space Πk , so that models in M
fk are mappings
The single-agent model class M
f : Πk → ∆(O◦ × R). In addition, M
fk is indexed by Π−k × M and its models are given as follows:
M
fk = {πk 7→ M |k (πk , π−k ) : π−k ∈ Π−k , M ∈ M} .
M

(17)

The intuition behind this definition is that for each agent k, if other agents commit to playing π−k , this induces
fk
a “single-agent” environment for k. If M ∈ M is the original environment, then the model M |k (·, π−k ) ∈ M
is precisely the induced single-agent environment for k (in a decentralized protocol in which each agent
observes its own reward but not the reward of other agents).
Offset Decision-Estimation Coefficient. The results in this section are most naturally stated in terms
of the offset variant of the DEC introduced in Foster et al. (2021)—specifically, the regret variant which
restricts to p = q (that is, exploration and exploitation are coupled). For an instance M , reference model M ,
and scale parameter γ > 0, we define


sup Eπ∼p [hM (π)] − γ · Eπ∼p [DH2 M (π), M (π) ] .
(18)
r-decoγ (M , M ) := inf
p∈∆(Π) M ∈M

We remark, via Foster et al. (2023), that this notion can be related to the constrained (PAC) DEC as follows.
Proposition 4.1 (Foster et al. (2023)). For all M ∈ M+ and ε > 0,

decε (M , M ) ≤ inf r-decoγ (M , M ) ∨ 0 + γε2 .
γ>0

(19)

Proposition 4.1 suffices to derive tight bounds on the constrained DEC for all of the examples we will consider.
It is also possible to relate the two complexity measures in the opposite direction, but this can lead to loose
results (Foster et al., 2023); this will not be necessary for our purposes.

4.1

Bounding the MA-DEC for convex decision spaces

Our first result considers a general class of instances in which agents’ decision spaces Πk satisfy a convexity
property, formally stated as Assumption 4.1.
Assumption 4.1 (Convexity of decision spaces). For each k ∈ [K], there is a finite set Ak (called the pure
decision set) so that Πk = ∆(Ak ). Furthermore, the following holds:
1. Each M ∈ M is linear in π, i.e., for π ∈ Π, M (π) = Eak ∼πk ∀k [M (a)], where we write a = (a1 , . . . , aK ).
2. There is a measurable function ϕ : O → A so that, for all a ∈ A and M ∈ M, Po∼M (a) (ϕ(o) = a) = 1,
i.e., M (a) reveals a.
26

This assumption is quite mild, and is satisfied whenever players 1) are allowed to randomize their actions, and
2) observe the resulting actions that are sampled at each round. In particular, this encompasses (structured)
normal-form games with bandit feedback
(see examples in Appendix A.3). To simplify notation, we will write
Q
A = A1 × · · · × AK and A−k = k0 6=k Ak0 .
Our main result for this subsection, Theorem 4.1, shows that for any M ∈ ∆(M), we can bound the
fk , M k ), of the K model classes
multi-agent DEC r-decoγ (M , M ) in terms of the single-agent DECs r-decoγ/K (M
fk and reference models M k .
M
Theorem 4.1 (Restatement of Theorem 1.5). Suppose that M = (M, Π, O, {Π0k }k , {Uk }k ) is an NE instance
of the MA-DMSO framework satisfying Assumption 4.1. Then for any γ > 0, it holds that
sup
M ∈co(M)

r-decoγ (M , M ) ≤

K
X

sup

fk , M k ).
r-decoγ/K (M

fk )
k=1 M k ∈co(M

This result is quite intuitive: It shows that the complexity of centralized equilibrium computation is no
larger than the complexity required for each agent to optimize their own reward in the face of a worst-case
environment induced by the other players. It is proven using the following fixed-point argument: For a given
fk , and it
agent k, if all other agents commit to a joint distribution, this induces a single-agent DMSO class M
is natural for agent k to play the strategy that minimizes the single-agent DEC for this class. This is not
enough to bound the MA-DEC as-is, because we need to specify a strategy for all agents, but by applying
Kakutani’s fixed point theoerem, we show that it is possible for all K agents to simultaneously minimize
their respective single-agent DECs with respect to the other agents’ strategies. Furthermore, we remark that
an immediate consequence of Theorem 4.1 is that the same upper bound on r-decoγ (M ) holds also when M is
a CCE or a CE instance, since Nash equilibria are always (coarse) correlated equilibria (see Appendix A.3).

As a concrete example, for the multi-armed bandit problem with A actions, we have r-decoγ (M) ≤ O A
γ
(Foster et al., 2021). Using Theorem 4.1, it follows that if M is the class of K-player normal-form games
with bandit feedback and Ak actions per player, then
PK
Ak
sup r-decoγ (M , M ) ≤ O(K) · k=1
.
γ
M ∈co(M)
 q P

K
Using Proposition 4.1, we conclude that decε (M ) ≤ O ε · K k=1 Ak . We refer to Appendix A.3 for
details, as well as additional examples, including structured normal-form games with linear or concave payoffs.
For many of these examples, the application of Theorem 4.1 leads to nearly tight bounds on r-decoγ (M ).
However, this is not always true: In Proposition A.11 (Appendix A.3.5), we show that there are instances M
fk ) is much larger than r-decoγ (M ).
for which r-decoγ (M

4.2

Bounding the MA-DEC for Markov games

While Assumption 4.1 is quite general, and holds for most standard normal-form game setups, a notable
setting that it does not capture is that of Markov games, where the joint decision space Π consists of
randomized non-stationary policies (formalized in Assumption 4.2 below).10 In this section, we provide an
analogous result specialized to this general, non-convex setting.
Assumption 4.2 (Markov game instance). The instance M = (M, Π, O, {Π0k }k , {Uk }k ) is such that for
QK
some H ∈ N, finite state space S, and finite joint action space A = k=1 Ak , each model M ∈ M is a
K-player, horizon-H Markov game with state space S and joint action space A (see Example 1.2). In addition,
for each k, the class Πk consists of non-stationary, randomized Markov policies, i.e.,
Πk = {(πk,1 , . . . , πk,H ) | πk,h : S → ∆(Ak ) ∀h ∈ [H]} .
10 One might try to satisfy Assumption 4.1 by convexifying each agent’s decision space Π

k ; however, in the setting of Markov
fk defined in (17) to be prohibitively large, since the policies π−k will now be mixtures
games, this will lead the model classes M
fk that result will in general scale with
of non-stationary Markov policies. In particular, the DEC of the induced model classes M
the DEC of the class of mixtures of MDPs, which is exponential even in the tabular setting (Foster et al., 2022b).

27

The finiteness of S and A in Assumption 4.2 is made for technical reasons, so as to enable the application
of fixed point theorems; our bounds in this section will not depend quantitatively on |S| or |A|, and we
anticipate that this assumption can be relaxed.
Under Assumption 4.2, we provide the following analogue of Theorem 4.1.
Theorem 4.2. There is a constant C > 0 so that the following holds. Suppose that M = (M, Π, O, {Π0k }k , {Uk }k )
is an NE instance of the MA-DMSO framework satisfying Assumption 4.2. Then for any γ > 0, it holds that
K

sup r-decoγ (M , M ) ≤
M ∈M

CKH log H X
fk , M k ).
+
sup r-decoγ/(CKH log H) (M
γ
fk
M k ∈M
k=1

As an example, for when M is a class of tabular MDPs with |S| = S and |A| = A, we have r-decoγ (M) ≤
poly(S,A,H)
(Foster et al., 2021). Theorem 4.2 then implies that for tabular Markov games with |S| ≤ S and
γ
|Ak | ≤ A, we have supM ∈M r-decoγ (M , M ) ≤ poly(S,A,H,K)
and via (19),
γ
sup decε (M , M ) ≤ ε ·

p

poly(S, A, H, K).

M ∈M

We remark that while Theorem 4.1 allows for improper reference models M ∈ co(M), Theorem 4.2 is
restricted to proper reference models M ∈ M, and hence is mainly useful in settings (such as tabular MGs)
in which proper estimators are available. See Appendix A.3 examples, as well as further details.

5

MA-DMSO: On the curse of multiple agents

A nuisance encountered frequently in the study of multi-agent reinforcement learning is poor scaling of
sample complexity with respect to the number of agents K. In particular, algorithms which directly estimate
the model M ? or agents’ Q-value functions typically incur sample complexity exponential in K, due to the
fact that both the model and agents’ Q-value functions require at least exp(K) parameters to specify; this
phenomenon has been called the curse of multiple agents (Jin et al., 2021b). In this section, we investigate the
curse of multiple agents in the MA-DMSO framework through the lens of the Multi-Agent Decision-Estimation
Coefficient.
We first remark that the upper bound on the minimax risk in terms of the DEC in our upper bound,
Theorem 3.1 (as well as the more general version, Theorem D.1), does indeed suffer from the curse of multiple
agents: even for very simple model classes such as K-player normal-form games, the estimation error log|M|
in Theorem 3.1 will scale exponentially in K (see examples in Appendix A.3 for details and discussion), and
therefore the upper bound in Theorem 3.1 will also scale exponentially in K, even though the MA-DEC is
not itself exponential. Note that our lower bound (Theorem 3.2) does not have exponential dependence on
K, since (a) the DEC typically scales as decε (M )  Cprob · ε, where the problem-dependent constant Cprob
depends only on the size of agents’ individual action sets, thus avoiding scaling exponential in K, and (b) the
bound of Theorem 3.2 does not include any term involving model estimation error (in particular, it does not
multiply the scale ε(T ) at which the DEC is evaluated).11
Evading the curse of multiple agents. Celebrated results in multi-agent (bandit) learning imply that
the curse of multiple agents is not necessary, at least for multi-player normal-form games with bandit feedback:
if each player runs an adversarial bandit no-regret algorithm, then the empirical average of their joint
action profiles over√T time steps approaches a (coarse) correlated equilibrium for the game at a rate of
poly(K, maxk Ak )/ T (e.g., Rakhlin and Sridharan (2013)), where Ak is the number of actions for player
k. Furthermore, a sequence of recent works has extended these results to the setting of Markov games (Jin
et al., 2021b; Song et al., 2021; Mao and Basar, 2022).
11 We recall that even in the single-agent setting, the appearance of the estimation error term in the upper bound, but not in
the lower bound, leads to a gap between them. Foster et al. (2023) emphasize that narrowing this gap is an important open
problem.

28

It is natural to wonder if it is possible to capture these results, which avoid exponential scaling with K,
through our framework and the Multi-Agent Decision-Estimation Coefficient. In light of the discussion above,
this question translates to asking whether the log |M| term in Theorem 3.1 (more generally, the term EstH (T )
in Theorem D.1, which can be controlled in terms of covering numbers), which results from estimation error,
can be decreased. Note that in general, as observed in Foster et al. (2021), the estimation error term log|M|
appearing in Theorem 3.1 cannot be removed completely, even in single-agent settings, but one might hope to
replace it with a weaker quantity. One possible avenue, if possible, would be to replace log|M| with log |FM |,
where FM denotes the induced class of value functions; this approach was explored for the single-agent
setting in (Foster et al., 2022a), where it leads to tighter guarantees for model-free reinforcement learning
settings. However, this approach is insufficient for the purpose of avoiding the curse of multiple agents, since
(an ε-cover of) the value function class F typically has size whose logarithm scales exponentially in K, even
for normal-form games with bandit feedback (Example 1.1).
In light of this discussion, perhaps most promising approach for evading the curse of multiple agents is to aim
for bounds that are analogous to Theorem 3.1, but replace the factor log|M| with the logarithm of the size of
the agents’ decision sets. Indeed, the logarithm of the size of the joint (pure) decision set typically does not
scale exponentially in K. For instance, for K-player normal-form games in which each player has A actions,
the number of pure action profiles is AK , so its logarithm is only linear in K; equivalently, one can look for
bounds which scale as the sum of the logarithms of the agents’ individual decision sets. In the single-agent
DMSO setting, Foster et al. (2021, 2022b) indeed obtain bounds that scale with log |Π|, as opposed to log |M|.
There is a cost to pay for this improvement, however: the upper bounds of Foster et al. (2021, 2022b) that
replace log|M| with log |Π| depend on the DEC of the convex hull of M, as opposed to the DEC of M itself.
Our upper bound. In Theorem 5.1 below, we provide an upper bound that replaces the factor log|M|
appearing in Theorem 3.1 with maxk log |Π0k |, at the cost of scaling with the MA-DEC for a convexified
version of the instance M . The quantity maxk log |Π0k | is equal to maxk log(|Σk | + 1) in the special case of
CCE instances (Definition 1.2), but is also small for CE instances (Definition A.1), as well as the following
more general notion of correlated equilibrium, which we refer to as a “generalized correlated equilibrium”.
Assumption 5.1 (Generalized correlated equilibrium). We say that an MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k )
satisfies the generalized correlated equilibrium assumption if the following holds: we have Π = ∆(Σ1 ×· · ·×ΣK ),
for finite sets Σ1 , . . . , ΣK , called pure decision sets. Furthermore, writing Σ := Σ1 × · · · × ΣK , the instance
M satisfies:
1. Each M ∈ M is linear in π, i.e., for π ∈ Π, M (π) = Eσ∼π [M (σ)].
2. The deviation functions Uk respect linearity in the sense that for all k ∈ [K], M ∈ M, and π ∈ Π, πk0 ∈
Π0k , we have fkM (Uk (πk0 , π)) = Eσ∼π [fkM (Uk (πk0 , σ))].
It is straightforward to check that both CCE instances (Definition 1.2) and CE instances (Definition A.1)
satisfy Assumption 5.1 as long as the pure decision sets Σk are all finite.
To state our result, for an instance M = (M, Π, O, {Π0k }k , {Uk }k ) of the MA-DMSO framework, we define
the convex hull of the instance M to be the instance co(M ) := (co(M), Π, O, {Π0k }k , {Uk }k ). We with the
results in the previous section, our guarantees are most naturally stated in terms of the regret variant of the
MA-DEC (r-decoγ ; cf. (18)).
Theorem 5.1. Suppose that M = (M, Π, O, {Π0k }k , {Uk }k ) is an MA-DMSO instance satisfying Assumption
5.1. Then, for any T ∈ N and δ ∈ (0, 1), there exists an algorithm ( MAExO; Algorithm 1 in Appendix F) which
produces π
b ∈ Π such that with probability at least 1 − δ,



K · maxk |Π0k |
γ
?
.
Risk(T ) = hM (b
π ) ≤ O(K) · inf r-decoγ (co(M )) + · log
γ>0
T
δ
We view this result as extending guarantees that replace log|M| by log|Π| in the single-agent setting (Foster
et al., 2021, 2022b); as with those prior results, the cost is that the DEC is applied to the convex hull of the

29

instance. For the problem of computing CCE in normal form games with K players and A actions per player,
0
we have decoγ (co(M )) . A
γ and maxk log|Πk | = log(A), so this result gives
r
Risk(T ) .

poly(K) · A
;
T

see Appendix A.3 for details and further examples. Theorem 5.1 shows that it is possible to avoid the curse
of multiple agents for convex classes, and leads to tight guarantees for structured classes of normal-form
games with bandit feedback, such as games with linear or convex payoffs. In general though, it does not lead
to tight guarantees non-convex classes such as Markov games. We prove the result by adapting the powerful
exploration-by-optimization algorithm from the single-agent setting (Lattimore, 2022; Foster et al., 2022b) in
a way that exploits the unique feedback structure of the multi-agent setting. One might wonder how the
guarantee of Theorem 5.1 compares to what one would obtain by having each agent k run the (single-agent)
fk
exploration-by-optimization algorithm of Foster et al. (2022b) separately (applied to the model class M
defined in (17)) and using the resulting regret bound of Foster et al. (2022b) for each agent to obtain an
approximate CCE. As we show in Proposition A.11, the guarantee of Theorem 5.1 can be arbitrarily better
than this alternative approach, since it involves the multi-agent DEC, r-decoγ (co(M )), which can be arbitrarily
fk )).
smaller than the DEC for the single-agent classes, r-decoγ (co(M
Extending the result to infinite decision sets. We next explain how to extend the guarantee of
Theorem 5.1 to the setting where the pure decision sets Σk and deviation sets Π0k are not finite. We will focus
on CCE instances: consider a MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k ) satisfying Assumption 5.1.
e k ⊆ Σk and Π
e 0 ⊆ Π0 for each k, and write Π
e = ∆(Σ
e1 × · · · × Σ
e K ) ⊂ Π. (As an example,
Consider subsets Σ
k
k
0
e
e
if M is a CCE instance, we will often take Πk = Σk ∪ {⊥}.) It is straightforward to see that the instance
f= (M, Π,
e k ). We now define a sense
e O, {Π
e 0 }k , {Uk }k ) satisfies Assumption 5.1 (with pure decision sets Σ
M
k
fis a good cover for M .
in which the instance M
f be defined as above. For ε ≥ 0, we say that that M
f is an ε-decision space cover
Definition 5.1. Let M , M
for M if
∀M ∈ M,

∀k ∈ [K],

e
∀e
π ∈ Π,

e 0k
∃e
πk0 ∈ Π

s.t.

max fkM (Uk (πk0 , π
e)) − fkM (Uk (e
πk0 , π
e)) ≤

0 ∈Π0
πk
k

ε
.
K

e 0 | denote the size of the largest deviation set in the smallest such cover, and
We let NΠ (M , ε) := maxk∈[K] |Π
k
define, for T ∈ N,
estΠ (M , T ) = inf {log NΠ (M , ε) + εT } .
ε≥0

f= (M,
e it follows
f Π,
e O, {Π
e 0 }k , {Uk }k ) be an ε-decision space cover for M . Note that, for any π
Let M
b ∈ Π,
k
from Definition 5.1 that
hM (b
π) =

K
X
k=1

max
fkM (Uk (πk0 , π
b)) − fkM (b
π) ≤
0
0

πk ∈Πk

K
X
k=1

max fkM (Uk (e
πk0 , π
b)) − fkM (b
π ) + ε.

0 ∈Π
ek
π
ek

Therefore, applying the algorithm of Theorem 5.1 to an appropriate decision space cover for the instance M
(for an appropriate choice of ε), we get the following result as an immediate corollary:
Corollary 5.1. Suppose that M = (M, Π, O, {Π0k }k , {Uk }k ) is a MA-DMSO instance satisfying Assumption
5.1. Then, for any T ∈ N and δ ∈ (0, 1), there exists an algorithm which produces π
b ∈ Π such that with
probability at least 1 − δ,
n
o
γ
?
Risk(T ) = hM (b
π ) ≤ O(K) · inf r-decoγ (co(M )) + · (estΠ (M , T ) + log(K/δ)) .
γ>0
T

30

Lower bounds for Nash equilibrium instances. Theorem 5.1 relies on the assumption that M is a
generalized correlated equilibrium instance (Assumption 5.1). To close the section, we complement this result
by showing that it is not possible to achieve analogous guarantees for Nash equilibria. First, in Proposition 5.1
we show such an impossibility result for K-player NE instances: We give an instance for which the upper
bound in Theorem 5.1 is polynomial in K, yet the minimax risk is exponential in K.
Proposition 5.1. There is a constant c0 > 0 so that the following holds. For any K ∈ N, there is a K-player
NE instance M = (M, Π, O, {Π0k }k , {Uk }k ) so that:
1. maxk |Π0k | = 2.
2. For all γ > 0, r-decoγ (co(M )) ≤ O(K/γ).
3. There is no algorithm that adaptively draws 2o(K) samples and outputs a policy with expected risk at
most c0 · K.
For the instance M in Proposition 5.1, we have |Π0k | = O(1) so a bound of the form in Theorem 5.1 would
2
e
imply that O(poly(K)/
) samples suffice to learn an -approximate Nash equilibrium; the lower bound
on sample complexity of 2Ω(K) from Proposition 5.1 rules this out. The proof of Proposition 5.1 follows
directly from well-known lower bounds on the query complexity of K-player Nash equilibria (Rubinstein,
2016; Babichenko, 2016; Chen et al., 2017).
For our last result Theorem 5.2, we go even further, and show that the impossibility of proving any variant of
Theorem 5.1 for NE instances persists even in the case when K = 2 and the game is zero-sum.
Theorem 5.2. There is a constant C0 > 0 so that the following holds. Fix any N ∈ N with N ≥ C0 and
 ∈ (1/N, 1). There is a two-player zero-sum NE instance M = (M, Π, O, {Π0k }k , {Uk }k ) such that the
following holds:
1. max{|Π01 |, |Π02 |} ≤ |Π| ≤ C0 · N 2 /2 .
2. For all γ ≥ C0 , r-decoγ (co(M )) ≤ .
3. There is no algorithm that adaptively draws
most 1/C0 .

√

N /C0 samples and outputs a policy with expected risk at

Observe that for the instance M in Theorem 5.2, we have log|Π| . log(N/), so a bound of the form in
Theorem 5.1 would imply that roughly log(N/)
samples suffice to learn an -approximate equilibrium. The

√
lower bound on sample complexity in Theorem 5.2, which shows that Ω( N ) samples are required, thus rules
out a guarantee of this type in a fairly strong sense.
We remark that the instance M constructed in Theorem 5.2, while an NE instance per Definition 1.1, does
not correspond to the standard notion of mixed Nash equilibrium in normal-form games (see the discussion
following Definition 1.1). Since the marginals of coarse correlated equilibria in two-player zero-sum games
constitute mixed Nash equilibria, Theorem 5.1 rules out a strengthening of Theorem 5.2 which constructs an
NE instance corresponding to the standard notion of mixed Nash equilibrium.
The proof of Theorem 5.2 is significantly more challenging (given prior work) than that of Proposition 5.1.
It uses the classical support estimation problem (e.g., Paninski (2008); Canonne (2020)) to construct an
instance for which the DEC is small but the minimax risk is large. This idea is natural, because the support
estimation problem has large model-estimation error, and the upper bound of Theorem D.1, which involves
the model estimation error, must be respected by the instance M . Using the support estimation problem as
a building block, we construct a class of two-player zero-sum games, which bears some resemblance to the
construction used in the proof of Theorem 2.2. However, the construction in the latter result does not ensure
that r-decoγ (co(M )) remains small, necessitating a more sophisticated approach. To ensure that r-decoγ (co(M ))
is small while maintaining a lower bound on minimax risk, we need to embed a few additional components in
the construction, namely the composition of a Reed-Solomon code and a randomness extractor. We refer the
reader to Appendix G for further details.

31

Acknowledgements
We thank Rob Schapire, Yunzong Xu, and Yanjun Han for helpful comments and discussions. NG is supported
at MIT by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Fellowship. AR acknowledges
support from ONR under grant N00014-20-1-2336 and ARO through award W911NF-21-1-0328.

32

References
Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for
bandit linear optimization. In Proc. of the 21st Annual Conference on Learning Theory (COLT), 2008.
Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. Near-optimal phi-regret learning in extensiveform games, 2022.
Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT,
volume 7, pages 1–122, 2009.
Yakov Babichenko. Query complexity of approximate nash equilibria. J. ACM, 63(4), oct 2016. ISSN
0004-5411.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. In Proceedings of the
34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA,
2020. Curran Associates Inc. ISBN 9781713829546.
Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff,
Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam
Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe,
Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play
in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Science, 378
(6624):1067–1074, 2022. doi: 10.1126/science.ade9097. URL https://www.science.org/doi/abs/10.1126/
science.ade9097.
Gábor Bartók, Dean P Foster, Dávid Pál, Alexander Rakhlin, and Csaba Szepesvári. Partial monitoring—classification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967–997,
2014.
Gerald Beer. Topologies on Closed and Closed Convex Sets. Kluwer Academic Publishers, 1993.
Mario Bravo, David Leslie, and Panayotis Mertikopoulos. Bandit learning in concave n-person games.
Advances in Neural Information Processing Systems, 31, 2018.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top
professionals. Science, 359(6374):418–424, 2018. doi: 10.1126/science.aao1733. URL https://www.science.
org/doi/abs/10.1126/science.aao1733.
Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in Machine
Learning, 8(3-4):231–357, 2015.
Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online linear
optimization with bandit feedback. In Conference on Learning Theory, pages 41–1. JMLR Workshop and
Conference Proceedings, 2012.
Sébastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimization. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 72–85, 2017.
Clément L Canonne. A survey on distribution testing: Your data is big. but is it blue? Theory of Computing,
pages 1–100, 2020.
Fan Chen, Song Mei, and Yu Bai. Unified algorithms for rl with decision-estimation coefficients: No-regret,
pac, and reward-free learning. arXiv preprint arXiv:2209.11745, 2022a.
Xi Chen, Yu Cheng, and Bo Tang. Well-Supported vs. Approximate Nash Equilibria: Query Complexity
of Large Games. In Christos H. Papadimitriou, editor, 8th Innovations in Theoretical Computer Science
Conference (ITCS 2017), volume 67 of Leibniz International Proceedings in Informatics (LIPIcs), pages 57:1–
57:9, Dagstuhl, Germany, 2017. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik. ISBN 978-3-95977-029-3.
doi: 10.4230/LIPIcs.ITCS.2017.57. URL http://drops.dagstuhl.de/opus/volltexte/2017/8163.

33

Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zero-sum linear
mixture markov games. In International Conference on Algorithmic Learning Theory, pages 227–261.
PMLR, 2022b.
Qiwen Cui, Zhihan Xiong, Maryam Fazel, and Simon S Du. Learning in congestion games with bandit
feedback. arXiv preprint arXiv:2206.01880, 2022.
Varsha Dani, Thomas P Hayes, and Sham Kakade. The price of bandit information for online optimization.
2007.
Constantinos Daskalakis and Christos H Papadimitriou. Computing pure nash equilibria in graphical games
via markov random fields. In Proceedings of the 7th ACM Conference on Electronic Commerce, pages
91–99, 2006.
Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of markov equilibrium in
stochastic games. arXiv preprint arXiv:2204.03991, 2022.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. International Conference on
Machine Learning, 2021.
Eyal Even-Dar, Yishay Mansour, and Uri Nadav. On the convergence of regret minimization dynamics in
concave games. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages
523–532, 2009.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM
symposium on Discrete algorithms, pages 385–394, 2005.
Dylan J Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Learning in games:
Robustness of fast convergence. Advances in Neural Information Processing Systems, 29, 2016.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487, 2021.
Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. A note on model-free
reinforcement learning with the decision-estimation coefficient. arXiv preprint arXiv:2211.14250, 2022a.
Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial
decision making. arXiv preprint arXiv:2206.13063, 2022b.
Dylan J. Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the
decision-estimation coefficient. arXiv preprint arXiv:2301.08215, 2023.
Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. On the rate
of convergence of regularized learning in games: From bandits and uncertainty to optimism and beyond.
Advances in Neural Information Processing Systems, 34:22655–22666, 2021.
Geoffrey J. Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. In Proceedings
of the 25th International Conference on Machine Learning, ICML ’08, page 360–367, New York, NY, USA,
2008. Association for Computing Machinery. ISBN 9781605582054.
Venkatesan Guruswami, Atri Ruda, and Madhu Sudan. Essential Coding Theory. 2022.
Amélie Heliou, Johanne Cohen, and Panayotis Mertikopoulos. Learning with bandit feedback in potential
games. Advances in Neural Information Processing Systems, 30, 2017.
Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in
zero-sum markov games. arXiv preprint arXiv:2107.14702, 2021.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning, pages 1704–1713, 2017.
34

Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems,
and sample-efficient algorithms. Neural Information Processing Systems, 2021a.
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efficient, decentralized
algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021b.
Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state
spaces. In International Conference on Machine Learning, pages 10251–10279. PMLR, 2022.
Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural
Information Processing Systems, 17:697–704, 2004.
János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R. McKee, Mateusz Malinowski, Thore
Graepel, and Yoram Bachrach. Negotiation and honesty in artificial intelligence methods for the board
game of Diplomacy. Nature Communications, 13(1):7214, December 2022. ISSN 2041-1723. doi: 10.
1038/s41467-022-34473-5. URL https://www.nature.com/articles/s41467-022-34473-5. Number: 1
Publisher: Nature Publishing Group.
Tor Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Mathematical
Statistics and Learning, 2(3):311–334, 2020.
Tor Lattimore. Minimax regret for partial monitoring: Infinite outcomes and rustichini’s regret. arXiv
preprint arXiv:2202.10997, 2022.
Tor Lattimore and Andras György. Mirror descent and the information ratio. In Conference on Learning
Theory, pages 2965–2992. PMLR, 2021.
Chris Junchi Li, Dongruo Zhou, Quanquan Gu, and Michael I Jordan. Learning two-player mixture markov
games: Kernel function approximation and correlated equilibrium. arXiv preprint arXiv:2208.05363, 2022.
Tianyi Lin, Zhengyuan Zhou, Wenjia Ba, and Jiawei Zhang. Optimal no-regret learning in strongly monotone
games with bandit feedback. arXiv preprint arXiv:2112.02856, 2021.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning
with self-play. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7001–7010. PMLR,
18–24 Jul 2021.
Qinghua Liu, Csaba Szepesvári, and Chi Jin. Sample-efficient reinforcement learning of partially observable
markov games. arXiv preprint arXiv:2206.01315, 2022.
Chinmay Maheshwari, Chih-Yuan Chiu, Eric Mazumdar, Shankar Sastry, and Lillian Ratliff. Zeroth-order
methods for convex-concave min-max problems: Applications to decision-dependent risk minimization. In
International Conference on Artificial Intelligence and Statistics, pages 6702–6734. PMLR, 2022.
Kleanthis Malialis and Daniel Kudenko. Distributed response to network intrusions using multiagent
reinforcement learning. Engineering Applications of Artificial Intelligence, 41:270–284, 2015. ISSN
0952-1976. doi: https://doi.org/10.1016/j.engappai.2015.01.013. URL https://www.sciencedirect.com/
science/article/pii/S095219761500024X.
Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against learners
in bayesian games. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference
on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 5221–5252. PMLR,
02–05 Jul 2022.
Weichao Mao and Tamer Basar. Provably efficient reinforcement learning in decentralized general-sum markov
games. Dynamic Games and Applications, pages 1–22, 2022.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic game theory, volume 1.
Cambridge University Press Cambridge, 2007.
Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT press, 1994.

35

Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE
Transactions on Information Theory, 54(10):4750–4755, 2008.
Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul
Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H.
Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers,
Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan
Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder
Singh, Demis Hassabis, and Karl Tuyls. Mastering the game of stratego with model-free multiagent
reinforcement learning. Science, 378(6623):990–996, 2022. doi: 10.1126/science.add4679. URL https:
//www.science.org/doi/abs/10.1126/science.add4679.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2014.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In
Advances in Neural Information Processing Systems (NIPS), pages 3066–3074, 2013.
J. B. Rosen. Existence and uniqueness of equilibrium points for concave n-person games. Econometrica, 33
(3):520–534, 1965.
Aviad Rubinstein. Settling the complexity of computing approximate two-player Nash equilibria. In Annual
Symposium on Foundations of Computer Science (FOCS), pages 258–265. IEEE, 2016.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.
In Advances in Neural Information Processing Systems, pages 2256–2264, 2013.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations
Research, 39(4):1221–1243, 2014.
Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. Operations
Research, 66(1):230–252, 2018.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. CoRR, abs/1610.03295, 2016. URL http://arxiv.org/abs/1610.03295.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature, 529(7587):484, 2016.
Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large number of
players sample-efficiently? arXiv preprint arXiv:2110.04184, 2021.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in
contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In
Conference on learning theory, pages 2898–2933. PMLR, 2019.
Salil Vadhan. Pseudorandomness. Foundations and Trends in Theoretical Computer Science, 7:1–336, 2012.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function
approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information
Processing Systems, 33, 2020.
Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning
Theory, pages 1263–1291. PMLR, 2018.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move
markov games using function approximation and correlated equilibrium. In Conference on learning theory,
pages 3674–3682. PMLR, 2020.
Wenhao Zhan, Jason D Lee, and Zhuoran Yang. Decentralized optimistic hyperpolicy mirror descent: Provably
no-regret learning in markov games. arXiv preprint arXiv:2206.01588, 2022.

36

Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher. The ai economist:
Taxation policy design via two-level deep multiagent reinforcement learning. Science Advances, 8(18):
eabk2607, 2022. doi: 10.1126/sciadv.abk2607. URL https://www.science.org/doi/abs/10.1126/sciadv.
abk2607.

37

Part I

Examples
A

MA-DMSO: Examples of instances

In this section of the appendix, we give examples of instances for the MA-DMSO framework, and apply our
results to derive upper and lower bounds on the minimax risk.
• In Appendix A.1 we give additional examples equilibria that can be captured in the MA-DMSO
framework, focusing on correlated equilibria and variants.
• In Appendix A.2 we give detailed examples of MA-DMSO instances, including normal-form games with
linear or concave payoffs (Appendix A.2.1) and Markov games (Appendix A.2.2).
• Finally, in Appendix A.3, we give bounds on the Multi-Agent Decision-Estimation Coefficient and
minimax risk for variance instances, including finite-action normal-form games (Appendix A.3.1),
structured normal-form games (Appendix A.3.2, Appendix A.3.3), and tabular Markov games (Appendix
A.3.4). In addition, in Appendix A.3.5, we give an instance which shows that the multi-agent to singleagent reduction in Theorem 4.1 can be loose in general.

A.1

Additional examples of equilibria

Definition A.1 below shows how we can use the MA-DMSO framework to capture the problem of (normalform) correlated equilibrium computation in games. The definition is similar to that of CCE instances
(Definition 1.2), except players’ deviation sets consist of mappings from their pure decision set to itself; these
mappings describe how the player deviates as a function of their pure decision.
Definition A.1 (Correlated equilibrium instance). We say that an MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k )
is a correlated equilibrium (CE) instance if the following holds:
1. For some finite sets Σ1 , . . . , ΣK (called pure decisions), we have Π = ∆(Σ1 × · · · × ΣK ). We write
Σ = Σ1 × · · · × ΣK .
2. For each π ∈ Π and M ∈ M, it holds that M (π) = Eσ∼π [M (σ)].
0
k
3. For k ∈ [K], we have Π0k = ΣΣ
k , i.e., Πk is the set of functions φ : Σk → Σk .

4. For each k ∈ [K], π ∈ Π, and φ ∈ Π0k , Uk (φ, π) ∈ ∆(Σ) is the distribution whose probability mass
function is given as follows:
∀σ ∈ Σ,

Uk (φ, π)(σ) = π ({(σk0 , σ−k ) ∈ Σ : φ(σk0 ) = σk }) .

In words, Uk (φ, π) is the distribution of (φ(σk ), σ−k ), for σ ∼ π.
Our next example considers notions of equilibria specialized to Markov games. Recall that Definitions 1.2
and A.1 describe instances that capture the notions of (coarse) correlated equilibria in normal-form games,
in which the pure actions belong to Σ = Σ1 × · · · × ΣK . In the setting of Markov games, often a slightly
different notion of (coarse) correlated equilibrium is used, whch we show is captured by Example A.1 below.
Example A.1 (Markov (coarse) correlated equilibria in Markov games). In Example 1.2, We will show
how to capture the problem of computing Markov coarse correlated equilibria (CCE) and Markov correlated
equilibria (CE) (e.g., Bai et al. (2020); Liu et al. (2021); Daskalakis et al. (2022)) in the MA-DMSO framework,
generalizing the notion of Markov Has equilibrium from Example 1.2. As in Example 1.2, we assume that
the class M consists of finite-horizon Markov games with horizon H ∈ N, state spaces Sh for h ∈ [H],
action spaces Ak for k ∈ [K], and distribution d1 ∈ ∆(S1 ), all of which are identical across all models in
the model class. The pure observation space O◦ consists of trajectories, and the reward space is R = [0, 1].
For both Markov CE and Markov CCE, the joint decision space is the set Π of Markov correlated policies,
38

namely policies π = (π1 , . . . , πH ), where each πh : Sh → ∆(A1 × · · · × AK ) specifies a mapping from states
to joint distributions over actions. For a model M and a joint decision π ∈ Π, an observation (trajectory)
o = {(sh , (a1,h , . . . , aK,h ), (r1,h , . . . , rK,h ))}h∈[H] is drawn as follows: first, s1 ∼ d1 , and then for h ∈ [H]:
• (a1,h , . . . , aK,h ) ∼ πh (sh ) and rk,h ∼ RkM (sh , (a1,h , . . . , aK,h )).
• sh+1 ∼ PhM (·|sh , (a1,h , . . . , aK,h )).
It remains to specify the deviation sets Π0k and switching functions Uk :
• For the case of Markov CCE, for each k ∈ [K], the deviation set Π0k is the set of deterministic Markov
0
0
0
policies for player k, which take the form πk0 = (πk,1
, . . . , πk,H
), where πk,h
: Sh → Ak . For a joint
0
0
policy π ∈ Π, Uk (πk , π) ∈ Π is the Markov correlated policy where player k plays according to πk,h
at
0
each state and all other players play according to π. In particular, denoting π
e := Uk (πk , π), we have
0
that π
eh (sh ) = πk,h
(sh ) × π−k,h (sh ), where π−k,h (sh ) denotes the marginal of πh (sh ) on the actions of
all players but k. Summarizing, for the MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k ), we have
?
that π
b ∈ Π, hM (b
π ) = 0 if and only if π
b is a Markov CCE of M ? .
• For the case of Markov CE, for each k ∈ [K], the deviation set Π0k is simply the set of tuples
φ = (φk,h,s )h∈[H],s∈Sh , where each φk,h,s : Ak → Ak is a function from Ak to itself. For a joint policy
π ∈ Π, Uk (φ, π) is the Markov correlated policy π
e defined as follows: the joint action distribution of π
e
at step h and state sh ∈ Sh is the distribution given by:
π
eh (s)(a) = πh (s)({(a0k , a−k ) ∈ A : φ(a0k ) = ak }),
for joint actions a ∈ A. In words, π
eh (s) is the distribution of (φ(a0k ), a−k ), for a ∼ πh (s). Summarizing,
?
for the MA-DMSO instance M = (M, Π, O, {Π0k }k , {Uk }k ) , we have that π
b ∈ Π, hM (b
π ) = 0 if and
?
only if π
b is a Markov CE of M .
Note that the instances constructed above are not special cases of the CCE or CE instances ( Definitions 1.2
and A.1) we consider for normal-form games. This is because the notions of Markov (C)CE discussed above
are more restrictive, forcing the joint decision π
b to be a (joint) Markov policy, as opposed to an arbitrary
distribution over joint policies. Nevertheless, as Example A.1 shows, the MA-DMSO framework is sufficiently
general to capture all of these notions of equilibria.
/

A.2

Additional examples of instances

In this section, we give additional examples of instances that capture standard equilibrium learning problems
found in the literature. We begin by describing examples of structured normal-form games in Appendix A.2.1,
and then consider multi-agent reinforcement learning problems in Appendix A.2.2.
A.2.1

Instances for bandits

In this section, we describe several instances of structured normal-form games, which may be thought of as
multi-agent generalization of structured bandit problem found in the single-agent setting. For each example
we consider, the models will have the following common structure (paralleling that of Example 1.1).
• Each agent k ∈ [K] will have a set Ak , referred to as its pure action set, and the joint policy space Π
will be a subset of ∆(A) = ∆(A1 × · · · × AK ) which contains all singleton distributions Ia .
• We will take R := [−1, 1] as the reward space and O◦ := A as the pure observation space.
• Let a class of mean reward functions F ⊆ (A → RK ) be given. We define the model class MF as
the set of models M : Π → ∆(RK × O◦ ) for which there is some (f1 , . . . , fK ) ∈ F so that: (a) for all
singleton distributions Ia ∈ Π, the distribution of (r1 , . . . , rK , o◦ ) ∼ M (Ia ) satisfies o◦ = a a.s. and
EM,Ia [rk ] = f M (Ia ) = fk (a), and (b) for all π ∈ Π, M (π) = Ea∼π [M (Ia )].
In words, MF consists of models M where (i) value functions fkM (·) are given by some element of F, and (ii)
observations reveal the action played (via the pure observation).

39

First, in Example A.2, we consider a normal-form game with linearly structured rewards, generalizing the
single-agent linear bandit problem (Dani et al., 2007; Abernethy et al., 2008; Bubeck et al., 2012). . This
example generalizes Example 1.1, which can be thought of as the special case where each player’s action set
is the set of standard unit vectors.
Example A.2 (Normal-form games with linear rewards). Fix K ∈ N; for each player k ∈ [K], Ak ⊂
Rdk for some dk ∈ N. Write d = d1 d2 · · · dK . Suppose that Θ1 , . . . , ΘK ⊂ Rd are convex sets so that
|ha1 ⊗ · · · ⊗ aK , θk i| ≤ 1 for all a1 ∈ A1 , . . . , aK ∈ AK , k ∈ [K], and θk ∈ Θk . Define F ⊂ (A → RK ) by
F = {(a1 , . . . , aK ) 7→ (ha1 ⊗ · · · ⊗ aK , θk i)k∈[K] : θ1 ∈ Θ1 , . . . , θK ∈ ΘK }. We can now consider the instances
corresponding to finding Nash equilibria, CE, and CCE for the class of games whose payoffs are given by
functions in F:
• We first treat Nash equilibria: suppose we set Πk = ∆(Ak ) for each k ∈ [K] and Π = Π1 ×· · ·×Πk , and define Π0k , Uk as in Definition 1.1. We define M = MF . Then the instance M = (M, Π, O, {Π0k }k , {Uk }k )
captures the problem of finding Nash equilibria in an unknown linear bandit game.
• Next we treat (C)CE: we set Π = ∆(A1 × · · · × AK ) and define Π0k , Uk as in Definition A.1 (respectively,
Definition 1.2). We define M = MF , so that the instance M = (M, Π, O, {Π0k }k , {Uk }k ) captures the
problem of finding (coarse) correlated equilibria in an unknown linear bandit game.
/
Next, Example A.3 treats the setting of concave games (with bandit feedback), which has received extensive
attention in the game theory literature (Rosen, 1965; Even-Dar et al., 2009), as well as machine learning
(Bravo et al., 2018; Maheshwari et al., 2022; Lin et al., 2021). It can also be viewed as a generalization of
the problem of single-player concave bandits (Kleinberg, 2004; Flaxman et al., 2005; Bubeck et al., 2017;
Lattimore, 2020).12
Example A.3 (Concave games). Given K ∈ N, for each k ∈ [K], let dk ∈ N and Ak ⊂ Rdk be a convex and
compact subset with nonempty interior. Set A := A1 × · · · × AK ⊂ Rd , where d = d1 + · · · + dK . Define
F ⊂ (A → RK ) by

F = f : A → [0, 1]K | ∀k ∈ [K], ∀a−k ∈ A−k , Ak 3 ak 7→ fk (ak , a−k ) is concave and 1-Lipschitz .
Above, 1-Lipschitzness is with respect to the `2 norm. We consider the following Nash and CCE instances:
• We first consider Nash equilibria: define Π0k , Uk as in Definition 1.1, and set Π = A, M = MF . Then
the instance M = (M, Π, O, {Π0k }k , {Uk }k ) captures the problem of finding Nash equilibria in concave
games, a classical problem (Rosen, 1965). In the two-player zero-sum case (namely, when K = 2 and
f1 (a) + f2 (a) = 0 for all a ∈ Π), the problem of bandit feedback which we cover has received extensive
attention (Bravo et al., 2018; Maheshwari et al., 2022; Lin et al., 2021).
• We next consider coarse correlated equilibria. Define Π := ∆(A1 × · · · × AK ), namely the space of
Borel measures on the compact set A1 × · · · × AK ⊂ Rd , and set M = MF . Furthermore define Π0k , Uk
as in Definition 1.2. Then the instance M = (M, Π, O, {Π0k }k , {Uk }k ) captures the problem of finding
coarse correlated equilibria in concave games; this has received less attention than Nash equilibria in
concave games., but has been studied recently in
Since the action sets Ak are infinite in this setting, it is not particularly natural to define a CE instance in
the sense of Definition A.1.
/
A.2.2

Instances for multi-agent reinforcement learning

We now give concrete examples of Markov game classes M. The first example considers the special case of
the instances for computing Markov Nash equilibria and Markov (coarse) correlated equilibria described in
Examples 1.2 and A.1 in which the Markov game under consider is tabular (i.e., has finite states and actions).
12 Often referred to as convex bandits, or zeroth-order convex optimization, since it is typically phrased in the form of loss
minimization, whereas we consider reward maximization.

40

Example A.4 (Equilibria in tabular Markov games). Fix parameters K, H ∈ N representing the number of
players and the horizon, finite action spaces Ak (of size Ak ∈ N) for each player k ∈ [K], and finite state
spaces Sh (each of size S ∈ N) at each step h ∈ [H]. The instances for each of the three types of equilibria
(Nash, CE, CCE) share the same observation space O: in particular, their pure observation space is O◦ , the
space of all possible H-step trajectories over the state and action spaces S1 , . . . , SH and A, and the reward
space is R = [0, 1].
We refer to the tabular setting as the model class M parametrized by all possible K-player Markov games with
horizon H, state spaces Sh , and action spaces Ak , so that the sum of each player’s rewards is bounded in [0, 1]
on any positive-probability trajectory.13 Then for the deviation and switching functions Π0k , Uk as described
in Example 1.2, the instance M = (M, Π, O, {Π0k }k , {Uk }k ) captures the problem of computing Markov Nash
equilibrium in an unknown tabular Markov game, and for Π0k , Uk as described in Example A.1 corresponding
to the notions of Markov CCE or Markov CE, respectively, the instance M = (M, Π, O, {Π0k }k , {Uk }k )
captures the problem of computing Markov CCE or Markov CE, respectively, in an unknown tabular Markov
game.
/
Example A.5 (Equilibria in linear mixture Markov games). Fix parameters K, H ∈ N representing the
number of players and the horizon, finite action spaces Ak for each k ∈ [K], and finite state spaces Sh for each
h ∈ [H].14 For a dimension parameter d ∈ N, we are given mappings φh : S × A × S → Rd , ψk,h : S × A → Rd
such that for all h ∈ [H] and k ∈ [K]
X
φh (sh+1 |sh , a) = 1 ∈ Rd , and kψk,h (sh , a)k2 ≤ 1
sh+1 ∈Sh+1

for all sh ∈ Sh , a ∈ A, sh+1 ∈ Sh+1 .15 The instances we construct have pure observation space O◦ given by
the set of all possible H-step trajectories over the action and state spaces A and Sh , and have reward space
R = [0, 1].
For some B ∈ N, the set of linear mixture Markov games is the model class M consisting of all K-player
Markov games M with horizon H, state spaces Sh , and action spaces Ak , for which there are vectors θhM ∈ Rd
satisfying kθhM k2 ≤ B and
PhM (sh+1 |sh , a) = hθhM , φh (sh+1 |sh , a)i,

M
Rk,h
(sh , a) = hθhM , ψk,h (sh , a)i

for all h ∈ [H], k ∈ [K], sh ∈ Sh , a ∈ A, sh+1 ∈ Sh+1 , and for which under any positive-probability trajectory,
PH
h=1 rk,h ∈ [0, 1]. (For simplicity, we assume the rewards are deterministic and equal to the quantity
M
Rk,h
(sh , a) defined above.)
For the deviation and switching functions Π0k , Uk as described in Example 1.2, the instance M = (M, Π, O, {Π0k }k , {Uk }k )
captures the problem of computing Markov Nash equilibrium in an unknown linear mixture Markov game,
and for Π0k , Uk as described in Example A.1 corresponding to the notions of Markov CCE or Markov CE,
respectively, the instance M = (M, Π, O, {Π0k }k , {Uk }k ) captures the problem of computing Markov CCE or
Markov CE, respectively, in an unknown linear mixture Markov game.
/

A.3

Computing bounds on the DEC and minimax risk of multi-agent instances

In this section, we apply our results from Sections 3 to 5 to (a) give bounds on the DEC of various MA-DMSO
instances, and (b) use these bounds on the DEC to derive bounds on the minimax risk for learning equilibria
in multi-agent interactive decision making.
13 This assumption allows us to take R = [0, 1].
14 We require the state spaces to be finite for technical reasons, but our bounds will not depend on the size of the state spaces.
15 The values of φ

H (sh+1 |sh , a) will not matter, so we may take SH+1 to be, e.g., the set consisting of single state.

41

A.3.1

Normal-form games with finite action spaces

We begin with perhaps the simplest example: finite-action normal-form games with bandit feedback. We
consider Nash, CE, and CCE instances, as described in Example 1.1. Let us fix K ∈ N along with action
sets A1 , . . . , AK for each of the K players, with joint action set A := A1 × · · · × AK . We write Ak := |Ak |
for k ∈ [K]. Let M NE , M CE , M CCE denote the NE, CE, and CCE instances, respectively, constructed in
Example 1.1. In this section, we bound the DEC of these instances; we begin with an upper bound on the
offset DEC, which immediately yields an upper bound on the constrained DEC via Proposition 4.1.
Proposition A.1. For any γ > 0, the instances M NE , M CE , M CCE defined above satisfy
r-decoγ (M CCE ) ≤ r-decoγ (M CE ) ≤ r-decoγ (M NE ) ≤

K·

PK

k=1 Ak

γ

.

Proof of Proposition A.1. Note that the instances M NE , M CE , M CCE share the same observation
space O, i.e., they have pure observation space O◦ = A and reward space R = [0, 1].16 Thus, let us
write M NE = (M, ΠNE , O, {(Π0k )NE }k , {UkNE }k ), M CE = (M, ΠCE , O, {(Π0k )CE }k , {UkCE }k ), and M CCE =
(M, ΠCCE , O, {(Π0k )CCE }k , {UkCCE }k ). To distinguish between the three different settings, we augment the
functions f M (·) and hM (·) with the superscripts NE/CE/CCE. For example, for the instance M NE , we have,
for M ∈ M, π ∈ ΠNE ,
fkM,NE (π) := EM,π [rk ],

and hM,NE (π) =

K
X
k=1

max

0 ∈(Π0 )NE
πk
k

fkM,NE (UkNE (πk0 , π)) − fkM,NE (π).

The functions hM,CE : ΠCE → R and hM,CCE : ΠCCE → R are defined analogously.
It holds that ΠCE = ΠCCE ; furthermore, for any M ∈ M and π ∈ ΠCE = ΠCCE , we have that hM,CCE (π) ≤
hM,CE (π). It immediately follows that r-decoγ (M CCE ) ≤ r-decoγ (M CE ). Next, note that ΠNE ⊂ ΠCE , and for
any π ∈ ΠNE and M ∈ M, we have that hM,NE (π) = hM,CE (π). Hence, for M ∈ co(M),


r-decoγ (M NE , M ) =
inf
sup Eπ∼p hM,NE (π) − γ · DH2 M (π), M (π)
p∈∆(ΠNE ) M ∈M


≥ inf
sup Eπ∼p hM,CE (π) − γ · DH2 M (π), M (π) = r-decoγ (M CE , M ).
p∈∆(ΠCE ) M ∈M

This establishes that

r-decoγ (M CCE ) ≤ r-decoγ (M CE ) ≤ r-decoγ (M NE ).

Q
It remains to upper bound r-decoγ (M NE ). For k ∈ [K], we write Πk := ∆(Ak ) and Π−k := k0 6=k Πk0 . For
fk ⊂ (Πk → ∆(R × O◦ )) as in Eq. (17); in particular:
each k ∈ [K], define the model class M
fk = {πk 7→ M |k (πk , π−k ) : π−k ∈ Π−k , M ∈ M}.
M
Next define the model class M0k ⊂ (Ak → ∆(R × {⊥})) by
M0k = {M : M (ak ) ∈ ∆(R × {⊥}) ∀ak ∈ Ak },
i.e., M (ak ) is allowed to be an arbitrary distribution over R × {⊥} for each ak . Proposition 5.2 of Foster et al.
fk ), and let M 0 ∈ co(M0 ) be the unique model so
(2021) shows that r-decoγ (M0k ) ≤ Aγk . Next, fix M ∈ co(M
k
16 Technically, the model class for the instance M NE only acts on product distributions in ΠNE = ∆(A ) × · · · × ∆(A ), as
1
K
opposed to ΠCCE = ΠCE = ∆(A) ⊃ ΠNE ; we will formally interpret the domain of M for the instance M NE as ΠNE to avoid
cluttering notation.

42

that the reward r ∼ M 0 (ak ) is distributed identically to the reward r ∼ M (ak ) for all ak ∈ Ak . Then we have
fk , M )
r-decoγ (M

=

inf

sup

p∈∆(Πk ) M ∈M,π−k ∈Π−k

Eπk ∼p

max
fkM,NE (πk0 , π−k ) − fkM,NE (πk , π−k ) − γ · DH2 M (πk , π−k ), M (πk )
0

inf

sup

p∈∆(Ak ) M ∈M,π−k ∈Π−k

Eak ∼p


≤

inf

sup Eak ∼p

p∈∆(Ak ) M 0 ∈M0

k



πk ∈Πk


≤



M,NE

max fk

a0k ∈Ak

(a0k , π−k ) − fkM,NE (ak , π−k ) − γ · DH2

M0

0
max
fk (a0k ) − fkM (ak ) − γ · DH2
0
ak ∈Ak


M (ak , π−k ), M (ak )





M (ak ), M (ak ) = r-decoγ (M0k , M 0 ),
0

0

where the first inequality follows since Ak ⊂ Πk (by identifying each action ak ∈ Ak with its indicator
distribution Iak ∈ Πk ), and the second inequality follows since for any M ∈ M, π−k ∈ Π−k , there is a
model M 0 ∈ M0k so that for all ak ∈ Ak , the distribution of the reward r ∼ M (ak , π−k ) is identical to the
distribution of r ∼ M 0 (ak ). Note that in the display above we have associated actions ak ∈ Ak with their
fk ) ≤ r-decoγ (M0 ) for all γ > 0.
singleton distribution Iak ∈ Πk , per our convention. It follows that r-decoγ (M
k
NE
Finally, by Theorem 4.1 applied to the instance M , we have that
r-decoγ (M NE ) ≤

K
X
k=1

fk ) ≤
r-decoγ/K (M

K
X
k=1

r-decoγ/K (M0k ) ≤

K·

PK

k=1 Ak

γ

.

Note that our application of Theorem 4.1 is valid since Assumption 4.1 is satisfied by the definition of M NE
in Example 1.1 (in particular, our assumption that M (Ia ) ∈ ∆(RK ) × {Ia }, i.e., that M reveals a, satisfies
the second point of Assumption 4.1).
Using Proposition A.1, we now bound the minimax rates for the instances M NE , M CE , M CCE . To simplify
matters slightly, we consider slightly simplified special cases of these instances in which the model class is
constrained to models which output rewards according to the Bernoulli distribution (i.e., the rewards are
{0, 1}-valued).17 Furthermore, we assume for simplicity that Ak ≥ 2 for all k. We denote the corresponding
MA-DMSO instances with Bernoulli rewards by M0NE , M0CE , M0CCE . First, we bound the minimax rate for
M0NE :
NE
Proposition A.2. There
which guarantees that with probability at
0
p is an algorithm for the instance M
−1
−1
least 1 − δ, Risk(T ) ≤ (maxk Ak ) · A · T · polylog(T, A, δ ), where A = A1 A2 · · · AK .

It is evident that the same upper bound on risk for M0NE in Proposition A.2 applies to M0CE , M0CCE since
for any decision π
b ∈ ΠNE ⊂ ΠCE = ΠCCE , we have hM,CCE (b
π ) ≤ hM,CE (b
π ) ≤ hM,NE (b
π ) (recall the definition of
M,NE
M,CE
M,CCE
h
,h
,h
in the proof of Proposition A.1).
Proof of Proposition
A.2.
The combination of Proposition A.1 and Proposition 4.1 yields that
q
PK
NE
decε (M0 ) ≤ ε · 2 K · k=1 Ak . Since, |O| ≤ 2K A (as rewards are assumed to be Bernoulli), the
class M satisfies Assumption D.2 with B = 2K A, and therefore Proposition D.1 gives that EstH (T, δ) =
O(est(M, T ) + log δ −1 ) · K 2 · log2 (maxk Ak ). Finally, by discretizing the reward means into multiples of ε2 ,
we see that N (M, ε) ≤ (1/ε2 )AK , which implies that est(M, T ) ≤ O(AK · log(T )). Therefore, Theorem D.1
combined with Theorem 2.1 gives that there is an algorithm with
r
r
p
EstH (T, δ)
maxk Ak · A
Risk(T ) ≤ K(A1 + · · · + AK ) ·
· polylog(T, 1/δ) ≤
· polylog(T, 1/δ, A).
T
T
Note that the upper bound of Proposition A.2 suffers from the curse of multiple agents: the number of joint
action profiles A is exponential in the number of agents K. It is a well-known result that such exponential
17 This restriction of the model class is essentially without loss of generality: given any model class with general reward
distributions in [0, 1], we can simulate samples from a model class with the same value functions fkM (·) and Bernoulli reward
distributions by, upon receiving rewards (r1 , . . . , rK ) ∼ M (π), replacing each rk with a sample rk0 ∼ Ber(rk ).

43

dependence on K is necessary for learning (e.g., Rubinstein (2016); see Proposition 5.1), while it is not
necessary for learning (coarse) correlated equilibria. We next show that our results in Section 5 allow us to
recover this improved (polynomial) bound for (coarse) correlated equilibria:
Proposition A.3. Fix any T ∈ N, δ ∈ (0, 1). There is an algorithm for the instance M0CE which produces
π
b ∈ ΠCE such that with probability at least 1 − δ,
r


K 4 maxk A2k
−1
Risk(T ) ≤
· polylog K, max Ak , δ
.
k
T
Furthermore, there is an algorithm for the instance M0CCE which produces π
b ∈ ΠCCE such that with probability
at least 1 − δ,
s
PK


K 3 k=1 Ak
−1
Risk(T ) ≤
· polylog K, max Ak , δ
.
k
T
5.1.
Proof of Proposition A.3. The statement of the proposition is an immediate consequence of Theorem
P
K K
k=1 Ak
For the instance M0CCE , we have that r-decoγ (co(M0CCE )) ≤ r-decoγ (co(M CCE )) = r-decoγ (M CCE ) ≤
,
γ
where we have used that the model class M is convex and Proposition A.1. Therefore, Theorem 5.1 gives
that there is an algorithm achieving
( P

)!
K
K k=1 Ak
γ
K · maxk Ak
Risk(T ) ≤O K · inf
+ · log
γ>0
γ
T
δ
s
PK


K 3 k=1 Ak
· polylog K, max Ak , δ −1 .
≤
k
T
Next, for the instance M0CE , the same upper bound on the DEC of co(M0CE ) holds, but the deviation sets are
k
larger: we have maxk |Π0k | = maxk |AA
k |, and so Theorem 5.1 gives
( P
!)!
K
k
K · maxk AA
K k=1 Ak
γ
k
Risk(T ) ≤O K · inf
+ · log
γ>0
γ
T
δ
r


K 4 maxk A2k
−1
≤
· polylog K, max Ak , δ
.
k
T

Lower bounds. Next we discuss lower bounds for the instances M0CCE , M0CE , M0NE . It is straightforward
to see that each of them embeds an instance of single-player maxk Ak -armed bandits, by restricting the model
class M to models for which the reward distribution depends only on the action taken by any single player
NE
CE
k. It then follows from
√ the proof of Proposition 5.3 of Foster et al. (2021) that decε (M0 ) ≥ decε (M0 ) ≥
CCE
decε (M0 ) ≥ Ω(ε maxk Ak ) for ε > 0; in fact, these lower bounds are obtained
by subclasses of M which
√
k Ak
have C(T ) = log(T ∧ V (M)) = O(1). Therefore, Theorem 3.2 (with ε(T ) = c max
, for sufficiently small
KT
CCE
CE
c > 0) together with Theorem 2.1 gives that for any of the instances M0 , M0 , M0NE , and any algorithm,
there is a model for which E[Risk(T )] ≥ Ω(maxk Ak /(KT )) under any of these three instances.
For the instance M0CCE , in the learnable regime
T > maxk Ak , this lower bound is off from the upper
p
CE
bound of Proposition
A.3
by
a
factor
of
T
/
max
k Ak · poly(K, maxk log Ak , log T ); for M0 , the gap
p
√
NE
increases
to T / max
p
√ k Ak · maxk Ak · poly(K, maxk log Ak , log T ),√and for M0 , the gap increases further
to T / maxk Ak · A · polylog(T, A). In all these cases, the factor of T in the gap is due to the impossibility
results discussed in Appendix 3.2.2, and the remaining terms are due to model estimation error appearing in
the upper bound but not the lower bound. In particular (up to a O(K) factor), there is no gap in the upper
and lower bounds we have computed on the MA-DEC for these instances.
44

A.3.2

Normal-form games with linear payoffs

In this section we bound the DEC and minimax regret for the linearly structured normal-form game instances
defined in Example A.2. In particular, fix action sets Ak ⊂ Rdk for each k ∈ [K], as well as convex sets
Θ1 , . . . , ΘK ⊂ Rd (with d = d1 · · · dK ) so that |ha1 ⊗· · ·⊗aK , θk i| ≤ 1 for all (a1 , . . . , aK ) ∈ A1 ×· · ·×AK , k ∈
[K], θk ∈ Θk . Let M NE , M CE , M CCE denote the NE, CE, and CCE instances constructed given the sets
Ak , Θk as in Example A.2. The below proposition bounds the (regret) offset DEC of these instances:
Proposition A.4. For any γ > 0, the instances M NE , M CE , M CCE defined above satisfy
r-decoγ (M CCE ) ≤ r-decoγ (M CE ) ≤ r-decoγ (M NE ) ≤

K·

PK

k=1 dk

γ

.

Proof of Proposition A.4. The proof is essentially identical to that of Proposition A.1, except that each
fk can be viewed as a single-agent linear bandit problem in d dimensions, allowing us
induced model class M
to use Proposition 6.1 of Foster et al. (2021) to bound the DEC for (single-player) linear bandits.
Using Proposition A.4, we now bound the minimax rates for the instances M NE , M CE , M CCE . As in the
previous subsection, to simplify matters, we restrict the instances so that the model class is constrained to
models which output (random) rewards that take values in {−1, 1} (recall that, for the linear bandit instances
defined in Example A.2, fkM (a) ∈ [−1, 1] for all M ∈ M, a ∈ A). Furthermore, we assume that for each k,
dk ≥ 2 and all θk ∈ Θk satisfy kθk k2 ≤ D and all ak ∈ Ak satisfy kak k2 ≤ D for some D > 0. It follows that
ka1 ⊗ · · · ⊗ aK k2 ≤ DK for all a1 ∈ A1 , . . . , ak ∈ Ak ; our bounds depend only logarithmically on D. We
denote the corresponding MA-DMSO instances with {−1, 1}-valued rewards by M0NE , M0CE , M0CCE . First, we
bound the minimax rate for M0NE :
Proposition A.5. For any T ∈ N, δ ∈ (0, 1),pthere is an algorithm for the instance M0NE which guarantees
that with probability at least 1 − δ, Risk(T ) ≤ (maxk dk ) · d · T −1 · polylog(T, d, δ −1 ), where d = d1 d2 · · · dK .
Proof of Proposition A.5. Analogous to our notation for finite-action normal-form games, let us write
0 NE
NE
M0NE = (M, ΠNE , O,
k ) }k , {Uk }k ). The combination of Proposition A.1 and Proposition 4.1 yields that
q {(ΠP
K
decε (M0NE ) ≤ 2ε · K · k=1 dk . For any π ∈ ΠNE ⊂ ∆(A), the distribution on O = RK × O◦ = RK × A
defined by ν(·|π) := Unif({−1, 1}K ) × π verifies that M satisfies Assumption D.2 with B = 2K , and
therefore Proposition D.1 gives that EstH (T, δ) = O(est(M, T ) + log δ −1 ) · K 2 . Finally, note that a product
of ε2 /DK -covers of Θk , for k ∈ [K], with respect to the Euclidean norm yields a ε-model class cover of
M in the sense of Definition D.1. Since each Θk has a ε2 /DK -cover of size O(DK+1 /ε2 )d , it follows that
N (M, ε) ≤ (DK+1 /ε2 )Kd , which implies that est(M, T ) ≤ O(K 2 d · log(T D)). Therefore, Theorem D.1
combined with Theorem 2.1 gives that there is an algorithm with
r
r
p
EstH (T, δ)
maxk dk · d
Risk(T ) ≤ K(d1 + · · · + dK ) ·
· polylog(T, 1/δ) ≤
· polylog(T, 1/δ, d).
T
T
As in the case of finite-action normal-form games, the upper bound in Proposition A.5 (which also applies
to the instances M0CE , M0CCE ) suffers from the curse of multiple agents. For the instance M0CCE , we obtain
improved bounds with minimax risk scaling only polynomially with K by appealing to our results in Section 5.
Proposition A.6. For any T ∈ N, δ ∈ (0, 1), there is an algorithm for the instance M0CCE which guarantees
that with probability at least 1 − δ,
r
K 5 · maxk {dk }
Risk(T ) ≤
· log(KDT /δ).
T
One might wonder whether a similar bound can be established for the instance M0CE . According to our
definition of M0CE (which is a CE instance per Definition A.1) we have |Π0k | = |Ak ||Ak | for each k, meaning
that the upper bound of Theorem 5.1 would yield a risk bound with polynomial dependence on |Ak |, which is
45

unacceptable in the linear bandit setting since Ak is often taken to be exponentially large or infinite. Even if
we were to attempt to use Corollary 5.1 to decrease the size of the deviation sets, the only choice of deviation
ek
e 0 := Σ
eΣ
set that works generically is Π
k
k , which has logarithm scaling exponentially in the dimension dk .
A more promising avenue is to consider notions of equilibria between CCE and CE (sometimes known as
Φ-equilibria), as in, e.g., Gordon et al. (2008); Anagnostides et al. (2022); Mansour et al. (2022); we leave
this direction for future work.
Proof of Proposition A.6. The proposition follows as a consequence of Corollary 5.1. Paralleling our
notation for normal-form games, let us write M0CCE = (M, ΠCCE , O, {(Π0k )CCE }k , {UkCCE }k ). Let us write
A1 ⊗ · · · ⊗ AK := {a1 ⊗ · · · ⊗ aK : a1 ∈ A1 , . . . , aK ∈ AK }. For each k ∈ [K], there is an ε/(KDK )-cover
with respect to the `2 -norm of Ak of size at most O(KDK+1 /ε)dk . Let us denote such a cover by Aek ⊆ Ak .
e CCE = ∆(A),
e and (Π
e 0 )CCE := Aek ∪ {⊥} for each k ∈ [K]. Consider any
Let us write Ae = Ae1 × · · · × AeK , Π
k
e
model M ∈ M. Note that, for any k ∈ [K], and ak ∈ Ak , there is some e
a0k ∈ Aek so that for all e
a ∈ A,
|fkM (Uk (ak , e
a)) − fkM (Uk (e
a0k , e
a))| =|he
a1 ⊗ · · · ⊗ ak ⊗ · · · ⊗ e
aK , θkM i − he
a1 ⊗ · · · ⊗ e
a0k ⊗ · · · ⊗ e
aK , θkM i|
≤kθkM k2 · DK−1 · ke
ak − e
a0k k2 ≤ ε/K,
fCCE := (M, Π
e CCE , O, {(Π
e 0 )CCE }k , {U CCE }k ) is a ε-decision space
which in particular implies that the instance M
0
k
k
CCE
CCE
cover for M
(per Definition 5.1). It therefore follows that estΠ (M
,
T
)
≤
K
·
max
k {dk } · log(KDT ). We
0
P
K·

K

d

k=1 k
have r-decoγ (co(M0CCE )) ≤ r-decoγ (co(M CCE )) = r-decoγ (M CCE ) ≤
by Proposition A.4 and convexity
γ
of the class M, which follows since the sets Θk are convex. By Corollary 5.1, we have that there is an
algorithm with
(
)
PK
K · k=1 dk
γ
Risk(T ) ≤O(K) · inf
+ · K · max{dk } · log(KDT /δ)
γ>0
k
γ
T
r
K 5 · maxk {dk }
≤
· log(KDT /δ).
T

Lower bounds. We now derive lower bounds for the instances M0CCE , M0CE , M0NE under the assumption
that Ak , Θk contain the unit `2 ball in their respective spaces.18 It follows from
√ the proof of Proposition 6.2
of Foster et al. (2021) that decε (M0NE ) ≥ decε (M0CE ) ≥ decε (M0CCE ) ≥ Ω(ε maxk dk ) for ε > 0, using the
fact that each of these instances embeds√an instance of single-player linear bandits in dimension maxk dk .
k dk
Therefore, Theorem 3.2 (with ε(T ) = c max
, for sufficiently small c > 0) together with Theorem 2.1
KT
gives that for any of the instances M0CCE , M0CE , M0NE , and any algorithm, there is a model for which
E[Risk(T )] ≥ Ω(maxk dk /(KT )) under any of these three instances. Similar considerations apply to the gaps
between the upper and lower bounds as discussed in Appendix A.3.1.
A.3.3

Concave (bandit) games

We now bound the DEC and minimax regret for the normal-form games wth concave rewards given in
Example A.3. Fix sets Ak ⊂ Rdk and the class F ⊂ (A → RK ) as described in Example A.3. We
assume that kak2 ≤ D for all a ∈ A, for some D > 0.; our bounds depend only logarithmically on D.
Let M NE = (M, ΠNE , O, {(Π0k )NE }k , {UkNE }k ) denote the NE instance constructed in Example A.3, and let
M CCE = (M, ΠCCE , O, {(Π0k )CCE }k , {UkCCE }k ) denote the CCE instance constructed in Example A.3.19 The
below proposition bounds the (regret) offset DEC of these instances:
18 Analogous lower bounds can be obtained under alternative action and parameter sets; for instance, if A each contains the
k
`1 ball and Θk each contains the `∞ ball, then we can embed the normal-form game setting from the previous subsection.
19 As we have done previously, we use the model class M for both instances M NE , M CCE , where it is understood that models
have domain appropriate for each instance.

46

Proposition A.7. For any γ > 0, the instances M NE , M CCE defined above satisfy
r-decoγ (M CCE ) ≤ r-decoγ (M NE ) ≤

K·

PK

4
k=1 dk

γ



· polylog max{dk }, D, γ .
k

Proof of Proposition A.7. The fact that r-decoγ (M CCE ) ≤ r-decoγ (M NE ) follows from the fact that ΠNE
may be identified as a subset of ΠCCE (namely, ΠNE consists of singleton distributions in ΠCCE ), in a similar manner to the proof of Proposition A.1. To prove the secondPupper bound, we will use Theorem 4.1
K
fk ), for M
fk defined as
applied to the instance M NE , which gives that r-decoγ (M NE ) ≤ k=1 r-decoγ/K (M
fk , we define the model class M0 ⊂ (Ak → ∆(R × {⊥}), by
in (17). In turn, to bound the DEC of M
k
0
M
Mk = {M : f (·) is concave}. Since, for any k ∈ [K], M ∈ MNE , a−k ∈ A−k , there is a model Mk0 ∈ M0k
so that, for all ak ∈ Ak , the distribution of r ∼ Mk0 (ak ) is the same as the distribution of rk ∼ M (ak , a−k ), it
fk ) ≤ r-deco (M0 ). Finally, Proposition 6.3 of Foster et al. (2021) (which is a restateholds that r-decoγ/K (M
k
γ/K
d4

ment of Theorem 3 of Lattimore (2020)) gives that, for all γ 0 > 0, r-decoγ 0 (M0k ) ≤ γk · polylog(dk , diam(Ak ), γ),
which yields that r-decoγ (M NE ) ≤

K

PK

4
k=1 dk

γ

· polylog (maxk {dk }, D, γ).

We now turn our attention to bounding the minimax risk. The model classes MNE , MCCE for our concave
game instances are extremeley large: any cover of MNE or MCCE in the sense of Definition D.1 must
have logarithm exponential in the dimensions dk , so the model-based guarantee from Theorem D.1 is not
particularly interesting, even in the case where K is small. Therefore, we turn directly to the policy-based
guarantees given in Section 5, and will prove a minimax risk upper bound for the instance M CCE . It turns out
that such an upper bound will immediately imply upper bounds for the instance M NE , under the following
assumption, specializing Even-Dar et al. (2009).
Assumption A.1 (Zero-sum socially concave). We say that a model class M is zero-sum socially concave
if for all M ∈ M, k ∈ [K] and ak ∈ Ak , the mapping A−k 3 a−k 7→ fkM (ak , a−k ) is a convex function and
PK
for all a ∈ A, k=1 fkM (a) = 0.
In the special case that the model M is a two-player zero-sum concave game (i.e., f1M + f2M ≡ 0), zero-sum
social concavity necessarily holds.
Proposition A.8. Then for any T ∈ N, δ ∈ (0, 1), there is an algorithm for the instance M CCE which
guarantees that with probability at least 1 − δ,


K 2 · maxk {d2.5
k }
√
Risk(T ) ≤
· polylog D, T, γ, max{dk }, K, 1/δ .
(20)
k
T
Suppose further that the model class M is zero-sum socially concave (i.e., it satisfies Assumption A.1). Then
there is an algorithm for the instance M NE which guarantes the same upper bound on risk in Eq. (20) with
probability at least 1 − δ.
Proof of Proposition A.8. For each k ∈ [K], there is an ε-cover with respect to the `2 -norm of Ak of size
e CCE := ∆(A),
at most O(D/ε)dk . Let us denote such a cover by Aek ⊂ Ak . Write Ae := Ae1 × · · · × AeK . Let Π
0 CCE
0
e
e
and (Πk )
:= Ak ∪ {⊥}. Note that, for any M ∈ M, k ∈ [K], and ak ∈ Ak , there is some e
ak ∈ Aek so that
e
for all e
a ∈ A,
|fkM (Uk (ak , e
a)) − fkM (Uk (e
a0k , e
a))| = |fkM (ak , e
a−k ) − fkM (e
a0k , e
a−k )| ≤ kak − e
a0k k2 ≤ ε,
fCCE := (M, Π
e CCE , O, {(Π
e 0 )CCE }k , {U CCE }k )
where the first inequality uses 1-Lipschitzness of fkM (·). Hence, the CCE instance M
k
k
CCE
CCE
is an ε-decision space cover for M
(per Definition 5.1). It follows that estΠ (M
, T ) ≤ maxk {dk } ·
log(DT ).
P
K· K

d4

k=1 k
Next, we have r-decoγ (co(M CCE )) = r-decoγ (M CCE ) ≤
· polylog(maxk {dk }, D, γ) by Proposition A.7
γ
and convexity of the class M (which follows since the convex combination of concave and 1-Lipschitz functions

47

is concave and 1-Lipschitz). By Corollary 5.1, for any T ∈ N and δ > 0, there is an algorithm which outputs
π
b ∈ ΠCCE so that with probability at least 1 − δ,
(
)
PK


K · k=1 d4k
γ
Risk(T ) ≤K · inf
+ · max{dk } · polylog D, T, γ, max{dk }, K, 1/δ
γ>0
k
k
γ
T


2
2.5
K · maxk {dk }
√
≤
· polylog D, T, γ, max{dk }, K, 1/δ .
k
T
Next we prove the upper bound for M NE . As we have done previously in this section, for π
b ∈ ΠCCE and
M,CCE
M ∈ M, we write h
(b
π ) to denote the suboptimality of π
b with respect to the instance M CCE , and for
NE
M,NE
π
b ∈ Π , we write h
(b
π ) to denote the suboptimality of π
b with respect to the instance M NE . Given
CCE
π
b ∈ Π , define b
a := Ea∼bπ [a] ∈ A. For each k ∈ [K] and M ∈ M, we have that
hM,NE (b
a) =

K
X
k=1

≤

K
X
k=1

=

K
X
k=1

max fkM (a0k , b
a−k ) − fkM (b
a)

a0k ∈Ak

max Ea−k ∼bπ [fkM (a0k , a−k )] − fkM (b
a)

a0k ∈Ak

max Ea−k ∼bπ [fkM (a0k , a−k )] − Ea∼bπ [fkM (a)] = hM,CCE (b
π ),

a0k ∈Ak

where
first inequality
PK the
PK followsMfrom social concavity and the second equality follows from the fact that
M
f
(b
a
)
=
0
=
b output by our algorithm for the instance
π [fk (a)]. Thus, given a decision π
k=1 k
k=1 Ea∼b
M CCE , we may simply output b
a = Ea∼bπ [a], which yields the same upper bound on risk.
Lower bounds. Assume that A = A1 × · · · × AK contains the unit `2 -ball. Then the instances M CCE , M NE
each embed a single-player linear bandit instance with dimension maxk dk (namely, by taking the subclass of
NE
F to consist of linear
√ functions in ak only), and so the lower bounds from Appendix A.3.2 give decε (M ) ≥
CCE
decε (M
) ≥ Ω(ε maxk dk ) and a minimax risk lower bound of Ω(maxk dk /(KT )). In this setting, even
the DEC lower bound (in the single-agent setting) is off from the upper bound implied by Proposition A.7
and Proposition 4.1 (Foster et al., 2021).
A.3.4

Tabular Markov games

We next give bounds on minimax risk for the instances corresponding to Markov Nash equilibria, Markov
CE, and Markov CCE in tabular Markov games, as described in Example A.4. Given H ∈ N, state spaces
Sh each of size S, action spaces Ak of size Ak := Ak , and an initial distribution d1 ∈ ∆(S1 ), let M NE =
(M, ΠNE , O, {(Π0k )NE }k , {UkNE }k ), M CE = (M, ΠCE , O, {(Π0k )CE }k , {UkCE }k ), and M CCE = (M, ΠCCE , O, {(Π0k )CCE }k , {UkCCE }k )
be the MA-DMSO instances corresponding to Markov Nash equilibria, Markov CE, and Markov CCE as
defined in Example A.4. Technically, the model class M for M NE acts on policies in ΠNE , whereas the
model class M for M CE and M CCE acts on policies in ΠCE = ΠCCE 6= ΠNE ; we will write the model class for
each instance as M and formally interpret its domain as the appropriate decision space, to avoid cluttering
notation.
In Proposition A.9 below, we begin with an upper bound on their offset DEC, which immediately yields an
upper bound on the constrained DEC via Proposition 4.1.
Proposition A.9. For any γ > 0, and any M ∈ M, the instances M NE , M CE , M CCE defined above satisfy
PK
27KH 3 log(H)S k=1 Ak
o
CCE
o
CE
o
NE
r-decγ (M
, M ) ≤ r-decγ (M , M ) ≤ r-decγ (M , M ) ≤
.
γ
Proof of Proposition A.9. As in the proof of Proposition A.1, we augment the functions f M (·) and hM (·)
with the superscripts NE/CE/CCE to distinguish between the value functions for models in the three different
48

instances. For example, for the instance M NE , we have, for M ∈ M, π ∈ ΠNE ,
"H
#
K
X
X
M,NE
M,π
M,NE
fkM,NE (UkNE (πk0 , π)) − fkM,NE (π).
fk
(π) := E
rk,h ,
h
(π) =
max
0
0
h=1

k=1

πk ∈(Πk )NE

The functions hM,CE : ΠCE → R and hM,CCE : ΠCCE → R are defined similarly.
We have ΠCE = ΠCCE ; furthermore, for any M ∈ M and π ∈ ΠCE = ΠCCE , we have that hM,CCE (π) ≤ hM,CE (π).
Thus r-decoγ (M CCE , M ) ≤ r-decoγ (M CE , M ). Next, note that we may identify ΠNE as a subset of ΠCE as
follows: for π = (π1 , . . . , πK ) ∈ ΠNE , we associate it to the joint Markov policy π
e = (e
π1 , . . . , π
eH ) ∈ ΠCE
where π
eh (sh ) is the product distribution π
eh (sh ) := π1,h (sh ) × · · · × πK,h (sh ). It is straightforward to see that,
for such π and any model M ∈ M, the distributions of M (π) and M (e
π ) are identical. Accordingly, with
slight abuse of notation, for π ∈ ΠNE , we denote its corresponding policy in ΠCE as π as well. Thus we have
hM,NE (π) = hM,CE (π), and for any M ∈ M, we have


r-decoγ (M NE , M ) =
inf
sup Eπ∼p hM,NE (π) − γ · DH2 M (π), M (π)
NE
p∈∆(Π ) M ∈M


≥ inf
sup Eπ∼p hM,CE (π) − γ · DH2 M (π), M (π) = r-decoγ (M CE , M ).
p∈∆(ΠCE ) M ∈M

It remains to upper bound r-decoγ (M NE ). For k ∈ [K], let Πk be the class of randomized Markov policies of
fk ⊂ (Πk → ∆(R × O◦ ))
player k (so that ΠNE = Π1 × · · · × ΠK ). For each k ∈ [K], define the model class M
as in (17):
fk = {πk 7→ M |k (πk , π−k ) : π−k ∈ Π−k , M ∈ M} .
M
Define M0k to be the model class consisting of all horizon-H Markov decision processes with action set Ak
and state spaces S1 , . . . , SH , and so that the sum of rewards under any trajectory that occurs with positive
probability is bounded in [0, 1]. Formally, the pure observation space of M0k is the space O◦0 of trajectories
{(sh , ak,h , rk,h )}h∈[H] , with sh ∈ Sh , ak,h ∈ Ak , rk,h ∈ R, its reward space is R = [0, 1], and its decision space
is Πk . Thus M0k ⊂ (Πk → ∆(R × O◦0 )). Proposition 5.4 of Foster et al. (2021) shows that for all M ∈ M0k ,
2
k
r-decoγ (M0k , M ) ≤ 26 H SA
.
γ
f∈M
fk . By definition of M
fk , we can find M ∈ M and π −k ∈ Π−k so that M
f(πk ) = M |k (πk , π −k )
Next, fix M
for all πk ∈ Πk . Let M 0 ∈ M0k be the unique model so that for all πk ∈ Πk , the marginal distribution
f(πk ) is identical to the distribution of the pure
of {(sh , ak,h , rk,h )}h∈[H] for a trajectory drawn from M
0
observation drawn from M (πk ). Such a model exists, since for each state sh ∈ Sh and action ak,h ∈ Ak , the
0
transition distribution PhM (·|sh , ak,h ) ∈ ∆(Sh+1 ) is defined as Eak0 ,h ∼πk0 ,h (sh ) ∀k0 6=k [PhM (·|sh , (ak,h , a−k,h ))]
M0
and the reward distribution Rk,h
(sh , ak,h ) ∈ ∆(R) is defined as Eak0 ,h ∼πk0 ,h (sh ) ∀k0 6=k [RhM (sh , (ak,h , a−k,h ))].
We now compute
(21)

fk , M
f)
r-decoγ (M

=

inf

sup

p∈∆(Πk ) M ∈M,π−k ∈Π−k

Eπk ∼p


≤

inf

sup Eπk ∼p

p∈∆(Πk ) M 0 ∈M0

k


M,NE
M,NE
0
2
f
max
fk
(πk , π−k ) − fk
(π) − γ · DH M (πk , π−k ), M (πk )
0


πk ∈Πk
M0

0
max
fk (πk0 ) − fkM (πk ) − γ · DH2
0
πk ∈Πk

0

0


M (πk ), M (πk )



= r-decoγ (M0k , M 0 ),
where the inequality follows since, via the same argument used to construct M 0 , for any M ∈ M, π−k ∈ Π−k ,
there is some M 0 ∈ M0k so that for any πk ∈ Πk , the marginal distribution of {(sh , ak,h , rk,h )}h∈[H] for
a trajectory drawn from M (πk , π−k ) is the same as the distribution of a trajectory drawn from M 0 (πk ).
In addition, we have applied
the data processing inequality for the Hellinger distance to conclude that

f(πk ) is an upper bound for the squared Hellinger distance between the marginal distribuDH2 M (πk , π−k ), M
f(πk ). Finally, by Theorem 4.2 applied to the instance
tions of {(sh , ak,h , rk,h )}h∈[H] under M (πk , π−k ) and M
49

M NE , we have that
K

sup r-decoγ (M NE , M ) ≤
M ∈M

CKH log H X
fk , M
fk )
+
sup r-decoγ/(CKH log H) (M
γ
fk ∈M
fk
M
k=1
K

CKH log H X
≤
+
sup r-decoγ/(CKH log H) (M0k , M 0k )
γ
M 0 ∈M0
k=1

≤

CKH log H
+
γ

K
X

k

k

26H 2 SAk ·

k=1

3

27KH log(H)S
≤
γ

CKH log H
γ

PK

k=1 Ak

.

Using Proposition A.9, we now bound the minimax rates for the instances M NE , M CE , M CCE . To simplify
?
matters, we assume that reward distributions are known. Formally, we fix some functions Rk,h
: Sh ×A → ∆(R)
?
M
(for k ∈ [K], h ∈ [H]) and restrict the model class M to models M ∈ M for which Rk,h
(sh , a) ≡ Rk,h
(sh , a) ∈
?
∆([0, 1/H]) for all M ∈ M. We also assume that Ak ≥ 2 for all k. With the functions Rk,h
fixed, let us
denote the resulting instances by M0NE , M0CE , M0CCE .20
NE
CE
CCE
Proposition A.10. There is an algorithm for
which guarantees
qeach of the instances M0 , M0 , M0

that with probability at least 1 − δ, Risk(T ) ≤

maxk Ak ·AS 3 H 4
· polylog(T, δ −1 , A, S, H).
T

Proof of Proposition A.10. Note that it suffices to bound the minimax risk for the instance M0NE , since
for any π
b ∈ ΠNE ⊂ ΠCE = ΠCCE , we have that hM,CCE (b
π ) ≤ hM,CE (b
π ) ≤ hM,NE (b
π ). (Recall the definition of
M,CCE
M,CE
M,NE
h
,h
,h
in the proof of Proposition A.9.) The combination of Proposition A.9 and Proposition 4.1
yields that, for any M ∈ M,
 v

u
K
u
X
decε (M0NE , M ) ≤ O ε · tKH 3 log(H)S
Ak  .
k=1

Because of the constraint that M ∈ M in the DEC upper bound, we need a proper estimation algorithm, i.e.,
c = M (in the context of Assumption D.1). To do so, we use the approach of layer-wise estimators
one with M
from Foster et al. (2021). Note that the model class M has the product structure M = M1 × · · · × MH ,
where each Mh is the set of transition kernels Sh × A → ∆(Sh+1 ), which is a convex set, thus satisfying
Assumption 7.2 of Foster et al. (2021). Furthermore, by gridding the transition densities into multiples
2
of ε2 , we have that N (Mh , ε) ≤ (1/ε2 )S A , and therefore, by Proposition 7.1 and Lemma A.16 of Foster
c = M and which has estimation error
et al. (2021), there is an estimation algorithm AlgEst with M
2
−1
EstH (T, δ) ≤ O(S AH) · polylog(S, H, δ , T ). Therefore, Theorem D.1 combined with Theorem 2.1 gives
that there is an algorithm with
v
r
u
K
u
X
EstH (T, δ)
Risk(T ) ≤tKH 3 log(H)S
Ak ·
· polylog(T, 1/δ)
T
k=1
r
maxk Ak · AS 3 H 4
≤
· polylog(T, δ −1 , A, S, H).
T

20 Essentially the same argument in Proposition A.10 allows us to upper bound the minimax risk for the original instances
M NE , M CE , M CCE , for which rewards are not known, but doing so requires a renormalization argument (and the loss of a factor
of H) to ensure rewards are always bounded in [0, 1], which we omit for brevity.

50

Lower bounds. It is straightforward to see that for any k, each of the instances M NE , M CE , M CCE
embeds an instance corresponding the class of single-player MDPs on state spaces Sh , action space Ak ,
and horizon H: in particular, take the subclass of M whose transitions and rewards only depend on
player k’s action at each step. Then it follows from the
√ proof of Proposition 5.8 of Foster et al. (2021)
NE
that dec√
) ≥ decε (M CE ) ≥ decε (M CCE ) ≥ Ω(ε SH · maxk Ak ). Therefore, Theorem 3.2 (with
ε (M
k Ak
ε(T ) = c SH·max
, for sufficiently small c > 0) together with Theorem 2.1 gives that for any of the instances
KT log T
CCE
CE
NE
e
M
, M , M , and any algorithm, there is a model for which E[Risk(T )] ≥ Ω(SH
· maxk Ak /(KT )).
A.3.5

A separation between multi-agent DEC and single-agent DEC

In the previous subsections, we bounded the multi-agent DEC, and thereby the minimax risk (via an
application of Theorem D.1 and Theorem 2.1), for several bandit problems. In all cases, our upper bound on
the multi-agent DEC (for CCE, CE, and Nash instances) followed via an application of Theorem 4.1 to upper
fk defined in Eq. (17). The next
bound the multi-agent DEC by the single-agent DEC of the model classes M
(straightforward) proposition shows that this approach is not tight in general, indicating that the multi-agent
DEC represents a fundamental complexity measure that is distinct from existing ones.
Proposition A.11. For any K, A ∈ N, there is a K-player MA-DMSO NE instance M = (M, Π, O, {Π0k }k , {Uk }k )
fk ) ≥ Ω(A/γ) for all γ > 0, where M
fk are defined as in (17).
so that r-decoγ (M ) = 0 but r-decoγ (M
Proof of Proposition A.11. Fix K, A ∈ N, and set Πk = {0, 1, . . . , A} for each k. Let R := [−1, 1] and
O◦ := A. Define F ⊆ (Π → RK ) to be the class of all tuples (f1 , . . . , fK ) with fk : Π → RK with the
property that for all π ∈ Π, if there is any k so that πk = 0, then fk0 (π) = 0 for all k 0 ∈ [K]. Set M := MF ,
and define Π0k , Uk as in Definition 1.1.
Define π0 = (0, . . . , 0). Since hM (π0 ) = 0 for all M ∈ M, it follows that r-decoγ (M ) = 0. On the other hand,
fk embeds a standard multi-armed bandit instance with A arms,
it is straightforward to see that each class M
fk ) ≥ Ω(A/γ) for all γ > 0.
meaning that by Proposition 5.3 of Foster et al. (2021), we have that r-decoγ (M

Part II

Proofs
B

Technical tools

B.1

Information theory

In this section we collect several technical lemmas which are used in our proofs.
Lemma B.1. Let (X , X ), (I, I ) be measure spaces. Suppose that for each i ∈ I, there are distributions
Pi , Pi0 ∈ ∆(X ), and Q ∈ ∆(I). Suppose further that there is a measurable function ϕ : X → I so that, for
each i ∈ I, Px∼Pi (ϕ(x) = i) = Px∼Pi0 (ϕ(x) = i) = 1. Then for any f -divergence D(· k ·), it holds that
D(Ei∼Q [Pi ] k Ei∼Q [Pi0 ]) = Ei∼Q [D(Pi k Pi0 )].
Proof of Lemma B.1. That D(Ei∼Q [Pi ] k Ei∼Q [Pi0 ]) ≤ Ei∼Q [D(Pi k Pi0 )] follows from convexity of D(· k ·).
To establish the opposite direction, our assumption on the function ϕ together with the data processing
inequality yields
D(Ei∼Q [Pi ] k Ei∼Q [Pi0 ]) ≥D(Ei∼Q [Ii × Pi ] k Ei∼Q [Ii × Pi0 ]) = Ei∼Q [D(Pi k Pi0 )],
where the final inequality follows from, e.g., Polyanskiy and Wu (2014).

51

Lemma B.2 (e.g., Polyanskiy and Wu (2014)). Let (X , X ) and (Y, Y ) be measure spaces, and let X × Y be
equipped with the product sigma-algebra X ⊗ Y . Let (x, y) be a pair of random variables on X × Y, distributed
according to some distribution Px,y . For any f -divergence D(· k ·), it holds that




Ex∼Px D Py|x k Py = Ey∼Py D Px|y k Px .
Lemma B.3 (Lemma B.5 of Foster et al. (2022b)). Let P, Q be probability distributions on a measure space
(X , X ). For any α ≥ 1, let Gα := {g : X → R : kgk∞ ≤ α}. Then

1 2
DH (P, Q) ≤ sup 1 − EP [eg ] · EQ [e−g ] + 4e−α .
2
g∈Gα
Lemma B.4 (e.g., Foster et al. (2022b)). Consider measure spaces (X , X ), (Y, Y ), and let (x, y) be a pair
of random variables distributed according to some distribution Px,y on (X × Y, X ⊗ Y ). Then




Ex∼Px DH2 Py|x , Py ≤ 4 · inf Ex∼Px DH2 Py|x , Q .
Q∈∆(Y)

Proof of Lemma B.4. Consider any Q ∈ ∆(Y). Using the fact that the Hellinger distance satisfies the
triangle inequality, we have





Ex∼Px DH2 Py|x , Py ≤Ex∼Px 2 · DH2 Py|x , Q + 2 · DH2 (Q, Py )



≤2 · Ex∼Px DH2 Py|x , Q + 2 · DH2 Q, Ex∼Px [Py|x ]


≤4 · Ex∼Px DH2 Py|x , Q ,
where the final inequality follows from convexity of the squared Hellinger distance.
Lemma B.5 (Donsker-Varadhan; see Polyanskiy and Wu (2014)). Let (X , X ) be a measure space, and let
P, Q be probability measures on (X , X ). Then
DKL (P k Q) = sup {EX∼P [h(X)] − log EX∼Q [exp(h(X))]} ,
h:X →R

where the supremum is over all (measurable) functions h : X → R satisfying EX∼Q [exp(h(X))] < ∞.
Lemma B.6. Let P, Q be probability measures on some probability space (Ω, F ). Consider some event E ∈ F
so that P(E) ≥ 1−δ, for some δ ∈ (0, 1). Suppose also that for all events F ∈ F , we have P(E ∩F) = Q(E ∩F).
Then DTV (P, Q) ≤ δ.
Proof of Lemma B.6. Choosing E 0 = Ω gives Q(E) = P(E) ≥ 1 − δ. Then for any event F ∈ F , we have
|P(F) − Q(F)| ≤|P(F ∩ E) − Q(F ∩ E)| + |P(F ∩ E c ) − Q(F ∩ E c )|
=|P(F ∩ E c ) − Q(F ∩ E c )| ≤ δ.

B.2

Concentration inequalities

Lemma B.7 (Lemma A.4 of Foster et al. (2021)). Let (Xt )t∈[T ] be any sequence of real-valued random
variables adapted to a filtration F t . Then with probability at least 1 − δ,
T
X
t=1

Xt ≤

T
X



log E eXt |F t−1 + log(1/δ).

t=1

52

B.3

Topological lemmas

The below lemma is a special case of the Berge maximum theorem.
Lemma B.8. Let U, V be compact subsets of Euclidean space, and consider any continuous function G :
U × V → R. Define C : V → P(U) by C(v) := arg minu∈U {G(u, v)}. Then C is upper hemicontinuous.
Proof of Lemma B.8. Consider any sequences un → u ∈ U, vn → v ∈ V so that un ∈ C(vn ) for all n. We
wish to show that u ∈ C(v), i.e., G(u, v) ≤ G(u0 , v) for all u0 ∈ U (which suffices to prove upper hemicontinuity
by compactness of V; see (Beer, 1993, Lemma 6.2.6)). To do so, fix any u0 ∈ U and  > 0. There exists N so
that for n ≥ N , we have |G(un , vn ) − G(u, v)| ≤  and |G(u0 , vn ) − G(u0 , v)| ≤ , by continuity of G. Then
G(u, v) ≤ G(un , vn ) +  ≤ G(u0 , vn ) +  ≤ G(u0 , v) + 2,
and by taking  → 0 we get that G(u, v) ≤ G(u0 , v).
The next lemma
Q is a straightforward
Q consequence of Kakutani’s fixed point theorem. In its statement, we
write X−k := k0 6=k Xk0 and X = k∈[K] Xk .
Lemma B.9. Suppose that X1 , . . . , XK are nonempty, compact, and convex subsets of Euclidean space.
Suppose that for each k ∈ [K] we are given an upper hemicontinuous function Fk : X → P(Xk ) so that, for
all x ∈ X , Fk (x) is nonempty, closed, and convex. Then there is some x ∈ X so that
x ∈ F1 (x) × · · · × FK (x).
Proof of Lemma B.9. Define F : X → P(X ) by F (x) := F1 (x) × · · · × FK (x). It is evident that for
each x ∈ X , F (x) is nonempty, closed, and convex. Furthermore, we claim that F is upper hemicontinuous.
To see this, consider any sequences xn → x and yn → y so that yn ∈ F (xn ) for each n ∈ N. Writing
yn = (yn,1 , . . . , yn,K ) and y = (y1 , . . . , yK ), by the product structure of F (xn ), we have that yn,k ∈ Fk (xn )
for each k ∈ [K]. By upper hemicontinuity of Fk and the fact that yn,k → yk , it holds that yk ∈ Fk (x). Thus
y ∈ F (x). By Kakutani’s fixed point theorem (Osborne and Rubinstein, 1994, Lemma 20.1), it holds that F
has a fixed point, namely some x ∈ X so that x ∈ F (x).

B.4

Minimax theorem

Theorem B.1 (Sion’s minimax theorem). Let X , Y be convex subsets of topological vector spaces, with X
compact. Let F : X × Y → R be a function such that (a) the mapping y 7→ F (x, y) is concave and upper
semicontinuous for all x ∈ X , and (b) the mapping x 7→ F (x, y) is convex and lower semicontinuous for all
y ∈ Y. Then
inf sup F (x, y) = sup inf F (x, y).

x∈X y∈Y

C

y∈Y x∈X

Proofs for Section 2

Proof of Theorem 2.1.
framework.

Consider an instance M = (M, Π, O◦ , R, {Π0k }k , {Uk }k ) of the MA-DMSO

For all models M and decisions π ∈ Π, define21
feM (π) := K − hM (π) = K −

K
X

sup {fkM (Uk (πk0 , π)) − fkM (π)} .

0
0
k=1 πk ∈Πk

21 The addition of K in the definition of feM (π) is for convenience, so as to ensure that if hM (π) ∈ [0, K] for all M, π, then the
same holds for feM (π).

53

Now fix any M ∈ M. By Assumption 1.2, there is some π ? ∈ Π so that hM (π ? ) = 0. By Assumption 1.3, it
holds that hM (π) ≥ 0 for all π ∈ Π. Then
sup feM (π 0 ) − feM (π) = K − (K − hM (π)) = hM (π).

π 0 ∈Π

(22)

Note that the instance H = (M, Π, O, {feM }M ) is well-defined since models in M are probability kernels
M : Π → O = O◦ × RK , and the observation space in the instance H is by definition O. Thus, the first
claimed point is follows from (22) since for any M , we have:
decε (M , M ) =

inf

sup

p,q∈∆(Π) M ∈H


=

inf

sup

p,q∈∆(Π) M ∈H

(23)

Eπ∼p [hM (π)]

q,ε (M )

Eπ∼p

q,ε (M )


M
0
M
e
e
sup f (π ) − f (π) = decε (H , M ).

π 0 ∈Π

Finally, we note that since the decision and (full) observation spaces of H , M are identical, the space of
?
algorithms (p, q) and distributions PM ,(p,q) are identical in the two frameworks. It follows from the definitions
of M(H , T ) and M(M , T ) that they are equal.
Proof of Theorem 2.2. Consider an instance H = (M, Π, O, {f M (·)}M ) and some V ∈ N. We first specify
the instance M by defining each of its components:
e k = ∆(Σk ) for k ∈ {1, 2}, and Π
e := Π
e1 × Π
e 2.
• Define Σ1 = Π and Σ2 = {0, 1, . . . , V }, Π
• We define Π0k , Uk for k ∈ [2] in the standard fashion for NE instances, per Definition 1.1; in particular,
e k = ∆(Σk ) for each k and Uk (π 0 , π) = (π 0 , π−k ).
set Π0k = Π
k
k
e = O◦ × R2 .
• Define O◦ := O ∪ {⊥}, R = [−1, 1], and set O
f is indexed by tuples (M, v) ∈ M × {1, 2, . . . , V } = M × [V ]. In particular, for each
• The model class M
fM,v ∈ M,
f which is defined as explained below. As the instance
such tuple (M, v), we have a model M
M we are constructing corresponds to that of computing mixed Nash equilibria in a game whose pure
action sets are Σ1 , Σ2 , we call elements of Σ1 × Σ2 pure decisions.
fM,v ((σ1 , 0)) is
– For pure decisions of the form (σ1 , 0) ∈ Σ1 × Σ2 , the distribution of (o◦ , r1 , r2 ) ∼ M
given by:
o◦ ∼ M (σ1 ) ∈ O ⊂ O◦ ,

r1 = r2 = 0.

– For pure decisions of the form (σ1 , i) ∈ Σ2 × Σ2 with i > 0, the distribution of (o◦ , r1 , r2 ) ∼
fM,v ((σ1 , i)) is given by:
M
(
−1
: i 6= v
o◦ =⊥,
r2 = −r1 =
M
g (σ1 ) : i = v.
e we can write π = π1 × π2 for πk ∈ ∆(Σk ) for k ∈ [2]. Then the
– For general decisions π ∈ Π,
fM,v (π) is the distribution of M
fM,v (σ), for σ = (σ1 , σ2 ) is distributed as: σk ∼ πk for
distribution M
k ∈ [2].
For reference later in the proof, we state a basic technical lemma, which is an immediate consequence of the
f
construction of M.
e and any M 0 = M
fM,v ∈ M,
f it holds that
Lemma C.1. For any π = π1 × π2 ∈ Π,
0

hM (π) ≥ π2 (Σ2 \{v}) · Eσ1 ∼π1 [g M (σ1 )] + π2 (Σ2 \{0, v}).
54

Proof of Lemma C.1. By considering the deviation π20 = v, we have
0

0

0

hM (π) ≥ sup {f2M (U2 (π20 , π)) − f2M (π)}
π20 ∈Π02
0

0

≥f2M (π1 × Iv ) − f2M (π1 × π2 )
=Eσ1 ∼π1 [g M (σ1 )] + π2 (Σ2 \{0, v}) − π2 (v) · Eσ1 ∼π1 [g M (σ1 )]
=π2 (Σ2 \{v}) · Eσ1 ∼π1 [g M (σ1 )] + π2 (Σ2 \{0, v}).

Bounding M(H , T ) by M(M , T ). Consider any algorithm (e
p, qe) which achieves M(M , T ). We have
QT e
Qt−1 e
e → ∆(Π)
e and qe = (e
e → ∆(Π)
e (we refer to Section 1.6
pe : t=1 (Π
× O)
q 1 , . . . , qeT ), with each qet : i=1 (Π
× O)
for background on how algorithms in the MA-DMSO framework and HR-DMSO framework are formalized).
Given (e
p, qe), we define an algorithm (p, q) for the instance H as follows. For any model M ∈ M, the
fM,v by only interacting with M ∈ M. The
algorithm attempts to simulate the interaction of (e
p, qe) with M
algorithm will store internal state, denoted by (e
π i , (e
oi , r1i , r2i )), for each i ∈ [T ], which store the “simulated”
fM,v . As a result of this internal state, our description
decisions and observations taken with respect to M
below does not explicitly identify the probability kernels p(·|·), q t (·|·). Since these kernels take as input the
entire history, there exist kernels p(·|·), q t (·|·) which produce exactly the same distribution over trajectories as
the below algorithm, but writing them down explicitly is somewhat cumbersome.
In particular, the distributions q t (for t ∈ [T ]) and p are defined (implicitly) as follows:
1. For t = 1, 2, . . . , T :
e
(a) Draw π
et ∼ qet (·|(e
π 1 , (e
o1 , r11 , r2t )), . . . , (e
π t−1 , (e
ot−1 , r1t−1 , r2t−1 ))), so that π t ∈ Π.
(b) Draw (σ1t , σ2t ) ∼ π
et .
(c) The distribution q t is defined (implicitly) by taking the decision σ1t ∈ Σ1 = Π.
(d) For use in choosing future decisions: as a function of the observation ot received after σ1t is played,
define
(
(ot , 0, 0)
: σ2t = 0
(e
ot , r1t , r2t ) =
(⊥, 1, −1) : σ2t > 0.
2. Finally, the distribution p is defined as the distribution of σ
b1 , where π
b ∼ pe(·|(e
π 1 , (e
o1 , r1t , r2t )), . . . , (e
π T , (e
oT , r1T , r2T )))
and σ
b1 ∼ π
b1 .
fM,0 which is defined identically to M
fM,v
To analyze this algorithm, for each M ∈ M, we introduce a model M
f
for any v ∈ [V ] except that MM,0 ((σ1 , i)) outputs (⊥, 1, −1) a.s. for any σ1 ∈ Σ1 , i ∈ [V ]. It is straightforward
to see that if there is some underlying model M ∈ M so that ot ∼ M (σ1t ) when the algorithm (p, q) defined
above is used, then the distribution of {(e
π t , (e
ot , r1t , r2t ))}Tt=1 defined above is exactly the distribution of the
f
M
,(p,
e q)
e
M,0
history under P
. We next appeal to the following claim, which states that we can pass from this
f
e q)
e
distribution to the distribution PMM,v? ,(p,
for some v ? ∈ [V ]:
Lemma C.2. There is an absolute constant C > 0 so that for any choice of algorithm (e
p, qe) and model
M ∈ M, there exists v ? ∈ [V ] so that:


p
f
f
e q)
e
e q)
e
1. DTV PMM,0 ,(p,
, PMM,v? ,(p,
≤ C T log(T )/V .
p
f
e q)
e
2. EMM,v? ,(p,
Eπb∼ep [b
π2 (v ? )] ≤ C T log(T )/V .

55

p
The proof of Lemma C.2 is provided following in the sequel. Let δ := C T log(T )/V , where C is the constant
from Lemma C.2. If δ > 1, then it is immediate that M(H , T ) ≤ M(M , T ) + O(δ), so we may assume
henceforth that δ ≤ 1. We then have:
e q)
e
EM,(p,q) Eσb1 ∼p [g M (b
σ1 )] = EMM,0 ,(p,
Eπb∼ep Eσb1 ∼bπ1 [g M (b
σ1 )]
f

e q)
e
≤ EMM,v? ,(p,
Eπb∼ep Eσb1 ∼bπ1 [g M (b
σ1 )] + δ
)#
"
( f
M
M,v ? (b
h
π
)
f
M
,(p,
e q)
e
?
≤ E M,v
Eπb∼ep min
,1
+δ
1−π
b2 (v ? )
√
√
f
f
e q)
e
≤ (1 + δ) · EMM,v? ,(p,
Eπb∼ep [hMM,v? (b
π )] + 2 δ
√
f
f
e q)
e
≤ EMM,v? ,(p,
Eπb∼ep [hMM,v? (b
π )] + 6 δ,
f

where the first inequality follows from the first point of Lemma C.2, the second inequality follows from
Lemma C.1, the second-to-last inequality uses √
the second
point of Lemma C.2 together with Markov’s
√
f
0
e q)
e
inequality to conclude that PMM,v? ,(p,
(b
π2 (v ? ) ≥ δ) < δ, and the final inequality uses that hM (π) ≤ 4 for
f π ∈ Π.
e
all M 0 ∈ M,
Taking a supremum over all models M ∈ M, we conclude that

√
f e q)
f
e
sup EM,(p,q) Eσb1 ∼p [g M (b
σ1 )] ≤ sup EM ,(p,
Eπb∼ep [hM (b
π )] + 6 δ.

M ∈M

f∈M
f
M

Bounding M(M , T ) by M(H , T ). Consider any algorithm (p, q) which achieves M(H , T ). We have
QT
Qt−1
p : t=1 (Π × O) → ∆(Π), and q = (q 1 , . . . , q T ), with each q t : i=1 (Π × O) → ∆(Π).
e ot ∈ O◦ , rt , rt ∈ R for each
We define an algorithm (e
p, qe) for the instance M as follows. Given π t ∈ O,
1 2
t ∈ [T ], we define
e
pe(·|(π 1 , (o1 , r11 , r21 )), . . . , (π T , (oT , r1T , r2T ))) ∈ ∆(Π)
to be the distribution obtained by sampling (σ1t , σ2t ) ∼ π t for each t, and taking the pure decision (b
σ , 0),
where σ
b is distributed according to p(·|(σ11 , o1 ), . . . , (σ1T , oT )). Similarly, define
e
qet (·|(π 1 , (o1 , r11 , r21 )), . . . , (π t−1 , (ot−1 , r1t−1 , r2t−1 ))) ∈ ∆(Π)
to be the distribution obtained by sampling (σ1i , σ2i ) ∼ π i for each i < t, and taking the pure decision (σ1t , 0),
where σ1t is distributed according to q t (·|(σ11 , o1 ), . . . , (σ1t−1 , ot−1 )). Since each qet is supported only on (pure)
fM,v , the distribution of
decisions in Σ1 × {0}, for any model M ∈ M and any v ∈ [V ], letting M 0 = M
T
T
t
t
M 0 ,(p,
e q)
e
t
t
{(σ1 , o ))}t=1 under P
is the same as the distribution of {(π , o )}t=1 under PM,(p,q) . Thus, we have
0

e q)
e
EM,(p,q) Eπb∼p [g M (b
π )] =EM ,(p,
E(bσ1 ,0)∼ep [g M (b
σ1 )]
"
M 0 ,(p,
e q)
e

=E

E(bσ1 ,0)∼ep

#
M0

M0

sup {f2 (U2 (π20 , (b
σ1 , 0))) − f2 ((b
σ1 , 0))}

π20 ∈Π02

 0

0 e q)
e
=EM ,(p,
E(bσ1 ,0)∼ep hM ((b
σ1 , 0)) ,
where above we have shortened p = p(·|(π 1 , o1 ), . . . , (π T , oT )) to denote the random variable under PM,(p,q) and
0 e q)
e
pe = pe(·|(π 1 , (o1 , r11 , r21 )), . . . , (π T , (oT , r1T , r2T ))) to denote the random variable under PM ,(p,
. This establishes
that M(H , T ) ≤ M(M , T ).
Bounding
decε (H ) by decε0 (M ). Consider any reference model M ∈ co(M). Given ε > 0, set ε0 :=
p
f) for some M
f ∈ co(M).
f For some distribution
ε + 6/V . We will upper bound decε (H , M ) by decε0 (M , M
ν ∈ ∆(M), we can write M (π) = EM ∼ν [M (π)] for all π ∈ Π. Define µ := ν × Unif([V ]) ∈ ∆(M × [V ]), and
f(π) := E(M,v)∼µ [M
fM,v (π)] for all π ∈ Π. Choose some pe, qe ∈ ∆(Π)
e so that
M
n
h 
i
o
f) = sup Eπ∼ep [hM 0 (π)] | Eπ∼eq DH2 M 0 (π), M
f(π) ≤ (ε0 )2 .
decε0 (M , M
f
M 0 ∈M

56

Define p ∈ ∆(Π) to be the distribution of σ1 where π = π1 × π2 ∼ pe and σ1 ∼ π1 . Similarly define q ∈ ∆(Π)
to be the distribution of σ1 where π = π1 × π2 ∼ qe and σ1 ∼ π1 . Now choose v ? ∈ [V ] as follows:
v ? := arg min {Eπ∼ep [π2 (v)] + Eπ∼eq [π2 (v)]} ,
v∈[V ]

where we have used the convention that π = π1 × π2 above. Then we have
Eπ∼ep [π2 (v ? )] + Eπ∼eq [π2 (v ? )] ≤

2
.
V

fM,v? . We now compute
Consider any model M ∈ M, and let M 0 := M
Eσ1 ∼p [g M (σ1 )] =Eπ∼ep Eσ1 ∼π1 [g M (σ1 )]


 M0
h (π) − π2 (Σ2 \{0, v ? })
,
1
≤Eπ∼ep min
π2 (Σ2 \{v ? })
 M0

h (π)
≤2V −1/2 + Eπ∼ep
1 − V −1/2
0

≤2V −1/2 + (1 + 2V −1/2 ) · Eπ∼ep [hM (π)],
where the first inequality uses Lemma C.1 and the second-to-last inequality uses Markov’s inequality to
conclude that Pπ∼ep [π2 (v ? ) > V −1/2 ] ≤ 2V −1/2 . Furthermore, we have
h 
i
h
h 
ii
f(π) ≤Eπ∼eq E(σ ,σ )∼π D2 M 0 ((σ1 , σ2 )), M
f((σ1 , σ2 ))
Eπ∼eq DH2 M 0 (π), M
H
1
2
h
h 
ii
f((σ1 , 0))
≤Eπ∼eq π2 (0) · Eσ1 ∼π1 DH2 M 0 ((σ1 , 0)), M
h
h 
ii
f((σ1 , v ? ))
+ Eπ∼eq π2 (v ? ) · Eσ1 ∼π1 DH2 M 0 ((σ1 , v ? )), M


+ Eπ∼eq π2 (Σ\{0, v ? }) · DH2 (Ber(0), Ber(1/V ))
h 
i
f((σ1 , 0)) + 2 · Eπ∼eq [π2 (v ? )] + 2/V
≤Eσ1 ∼q DH2 M 0 ((σ1 , 0)), M


≤Eσ1 ∼q DH2 M (π), M (π) + 6/V,
where the first equality uses convexity of the squared hellinger distance, the second inequality uses that
f is a mixture of M
fM,v with v ∼ Unif([V ]), and the third inequality uses that D2 (Ber(0), Ber(1/V )) ≤
M
H
2 · DTV (Ber(0), Ber(1/V )) = 2/V . Thus, it follows that

 2

decε (H , M ) ≤ sup Eσ1 ∼p [g M (π)] | Eσ1 ∼q DH
M (σ1 ), M (σ1 ) ≤ ε2
M ∈M
n
h 
i
o
0
f(π) ≤ ε2 + 6/V
≤ sup 2V −1/2 + (1 + 2V −1/2 ) · Eπ∼ep [hM (π)] | Eπ∼eq DH2 M 0 (π), M
f
M 0 ∈M

f)
≤2V −1/2 + (1 + 2V −1/2 ) · decε+(6/V )−1/2 (M , M
f).
≤6V −1/2 + decε+(6/V )−1/2 (M , M
f ∈ co(M).
f We will upper bound
Bounding decε (M ) by decε (H ). Next consider any reference model M
f
decε (M , M ) by decε (H , M ) for some M ∈ co(M). For some distribution µ ∈ ∆(M × [V ]), we have
f(π) = E(M,v)∼µ [M
fM,v (π)] for all π ∈ Π.
e Define M by letting ν ∈ ∆(M) to be the marginal of µ over M,
M
and then: M (π) := EM ∼ν [M (π)] for each π ∈ Π. Choose some p, q ∈ ∆(Π) so that


2
decε (H , M ) = sup Eπ∼p [g M (π)] | Eπ∼q [DH
M (π), M (π) ] ≤ ε2 .
M ∈M

57

e be the distribution of (σ1 , 0) where σ1 ∼ p, and qe ∈ ∆(Π)
e be the distribution of (σ1 , 0) where
Let pe ∈ ∆(Π)
0
fM,v , for any M = M
fM,v ∈ M,
f we have:
σ1 ∼ q. By definition of the models M
"
#
0

0

0

0

0

sup {f1M (U1 (π10 , π)) − f1M (π)} + sup {f2M (U2 (π20 , π)) − f2M (π)}

Eπ∼ep [hM (π)] =Eπ∼ep

π10 ∈Π01

π20 ∈Π02

"
=Eπ∼ep

#
M0

sup {f2 (U2 (π20 , π))}

π20 ∈Π02

=Eσ1 ∼p [g M (σ1 )].
f have r1 = r2 = 0 a.s. under such
Furthermore, since qe is supported entirely on Σ1 × {0} and all models in M
policies, it holds that
h 
i


f(π) = Eσ ∼q D2 M (π), M (π) ,
Eπ∼eq DH2 M 0 (π), M
H
1
which certifies that
n
o
h 
i
0
f(π) ≤ ε2 ≤ decε (H , M ).
Eπ∼ep [hM (π)] | Eπ∼eq DH2 M 0 (π), M

f) ≤ sup
decε (M , M
f
M 0 ∈M

fM,v ((π, 0)) and (M, v) ∼ µ.
for each π ∈ Π, M (π) to be the distribution of o◦ when (o◦ , r1 , r2 ) ∼ M
fM,v (for v ≥ 0)
Proof of Lemma C.2. We denote a history drawn according to any of the distributions M
t
t
t
t
T
t
t
1
1
t
t
t−1
by {(e
π , (e
o , r1 , r2 ))}t=1 . Furthermore, we abbreviate qe = qe (·|(e
π , (e
o , r1 , r2 )), . . . , (e
π , (e
ot−1 , r1t−1 , r2t−1 )))
T −1
T −1
1
1
t
t
T −1
T −1
0
fM,0 and choose
and pe = pe(·|(e
π , (e
o , r1 , r2 )), . . . , (e
π , (e
o , r1 , r2 )))). Define M := M
( T
)
X
?
M 0 ,(p,
e q)
e
t
M 0 ,(p,
e q)
e
v := arg min
E
Eπet ∼eqt [e
π2 (v)] + T · E
Eπb∼ep [b
π2 (v)] .
v∈[V ]

t=1

e satisfy PV π2 (v) ≤ 1 ensures that
Then the choice of v ? together with the fact that all π ∈ Π
v=1
T
X

0

0

e q)
e
e q)
e
EM ,(p,
Eπet ∼eqt [e
π2t (v ? )] + T · EM ,(p,
Eπb∼ep [b
π2 (v ? )] ≤

t=1

2T
.
V

(24)

fM,v? . Next, using (Foster et al., 2021, Lemma A.13),22 we have:
Write M 00 = M
" T
#
X

0
00
0
e q)
e
e q)
e
e q)
e
DH2 PM ,(p,
, PM ,(p,
=O(log T ) · EM ,(p,
DH2 (M 0 (e
π t ), M 00 (e
π t ))
t=1
M 0 ,(p,
e q)
e

" T
X

M 0 ,(p,
e q)
e

" T
X

≤O(log T ) · E

#
E(σ1t ,σ2t )∼eπt [DH2 (M 0 ((σ1t , σ2t )), M 00 ((σ1t , σ2t )))]

t=1

≤O(log T ) · E

#
t

?

E(σ1t ,σ2t )∼eπt [2 · I {σ2 = v }]

t=1


≤O

T log T
V


,

where the final inequality uses (24). Since p
total variation distance is bounded above by Hellinger distance,
0 e q)
00 e q)
e
e
it follows that DTV PM ,(p,
, PM ,(p,
≤ C T log(T )/V for some constant C > 0. Using this fact together
with (24), we see that
p
p
00 e q)
0 e q)
e
e
EM ,(p,
Eπb∼ep [b
π2 (v ? )] ≤ EM ,(p,
Eπb∼ep [b
π2 (v ? )] + C T log(T )/V ≤ 2T /V + C T log(T )/V ,
22 In particular, we apply this lemma to the sequence X , . . . , X
et , Xt+1 =
1
2T , where, for odd values of t we have Xt = π
(e
ot , r1t , r2t ), and use that the conditional distribution of π
et given the history up to step t − 1 is the same under the distributions
0 e q)
00 e q)
e
e
PM ,(p,
and PM ,(p,
since the algorithm (e
p, qe) is the same.

58

where the second inequality above uses (24).

D

Proofs for Section 3

D.1

Proofs from Section 3.1

D.1.1

Further details for upper bound

The upper bound from Theorem 3.1 is derived by appealing to the E2D+ for PAC algorithm from Foster
et al. (2023). In what follows, we give some background on the algorithm, as well as a more general upper
bound. In brief, the E2D+ for PAC algorithm proceeds as follows: The algorithm uses an online estimation
oracle, denoted by AlgEst (defined formally in Assumption D.1), which is given as input a model class M
and attempts to estimate the true model M ? ∈ M given data obtained from playing various decisions under
M ? . To generate each successive datapoint at iteration t, which will be fed to the estimation oracle AlgEst ,
the E2D+ for PAC algorithm solves the minimax problem in (7) to compute distributions pt , q t , where the
model M is set to be the output of the estimation oracle from the previous iteration. Then, a decision π t is
sampled from q t , and we observe the resulting observation ot ∼ M ? (π t ). The tuple (π t , ot ) is then be fed to
ct+1 . The algorithm’s output after T iterations is
the estimation oracle, which produces its next estimate M
t?
given by a sample from one of the distributions p , where t? ∼ [T ] is uniform. See Foster et al. (2023) for
further background.
Assumption D.1 (Estimation oracle for M). For each time t ∈ [T ], an online estimation oracle AlgEst
for the class M takes as input Ht−1 = (π 1 , o1 ), . . . , (π t−1 , ot−1 ) where oi ∼ M ? (π i ) and π i ∼ q i , for arbitrary
c ⊆ co(M), the oracle AlgEst
(adaptive) choices of the distributions q i ∈ ∆(Π). Then, for some class M
?
t
c
c
returns an estimator M ∈ M. We assume that if M ∈ M, the estimators produced by the algorithm satisfy
EstH (T ) :=

T
X

h 
i
ct (π t ) ≤ EstH (T, δ),
Eπt ∼qt DH2 M ? (π t ), M

t=1

with probability at least 1 − δ, where EstH (T, δ) is a known upper bound.
c in Assumption D.1 will be co(M), though in some cases it is possible
For most estimation oracles, the class M
to take it to be smaller (see Proposition A.10 for an example).

1
Theorem D.1 (Foster et al. (2023), Theorem 3.1; Upper bound for HR-DMSO). Fix δ ∈ 0, 10
and
T ∈ N, and consider any instance H = (M, Π, O, {f M (·)}M ). Suppose that Assumptions
1.4
and
D.1
hold


2T
δ
c ⊆ co(M), and let EstH := EstH
for the model class M and some class M
dlog 2/δe , 4dlog 2/δe . Letting
q
ε(T ) := 8 dlogT2/δe · EstH , E2D+ for PAC, with access to the oracle AlgEst , guarantees that with probability at
least 1 − δ,
Risk(T ) ≤ sup decε(T ) (H , M ) ≤ decε(T ) (H ).
c
M ∈M

If further f M (·) ∈ [0, R] for all M ∈ M and some R > 0, then the expected risk is bounded as E[Risk(T )] ≤
decε(T ) (H ) + Rδ.
c = co(M), but an inspection
We remark that Theorem D.1 is only stated in Foster et al. (2023) for the case M
c ⊆ co(M) in which AlgEst
of the proof shows that the same guarantee holds for an arbitrary subclass M
produces its predictions (with no modifications to the proof being necessary).
Theorem 3.1 follows from Theorem D.1 by noting that there exists an estimation oracle with EstH (T, δ) ≤
2 log(|M|/δ) for finite classes (Foster et al., 2023).
Remark D.1 (Analogue for MA-DMSO). Using the transformation of Theorem 2.1 (which does not change
the model class of the instance, and therefore preserves estimation error guarantees), there is an analogue
59

of Theorem D.1 for the multi-agent setting. In particular, for any instance M of MA-DMSO, under
Assumptions 1.1 and D.1, there is an algorithm that ensures with probability 1 − δ, Risk(T ) ≤ decε(T ) (M ).
Infinite model classes. As some of our applications in Appendix A involve infinite model classes M, we
next describe a simple way to bound the estimation error EstH (T, δ) for such classes, following the approach
in Foster et al. (2021).
Definition D.1 (Model class cover; Foster et al. (2021), Definition 3.2). A model class M0 ⊆ M is an
ε-cover for M if for all M ∈ M, there is M 0 ∈ M0 so that supπ∈Π DH2 (M (π), M 0 (π)) ≤ ε2 . Let N (M, ε)
denote the size of the smallest such cover M0 , and define

est(M, T ) := inf log N (M, ε) + ε2 T .
ε≥0

We will bound the estimation error EstH (T, δ) for a model class in terms of the quantity est(M, T ); to do so,
we need the following mild assumption.
Assumption D.2. Suppose that there is a kernel ν from (Π, P) to (O, O) so that M (π)  ν(π) for all
M ∈ M, π ∈ Π, and let mM (·|π) denote the density of M (·|π) with respect to ν(·|π). Furthermore, suppose
there is a constant B ≥ e so that
1. ν(O|π) ≤ B for all π ∈ Π.
2. supπ∈Π supo∈O mM (o|π) ≤ B for all M ∈ M.
Proposition D.1 below shows that the estimation error EstH (T, δ) scales with log B. This quantity is typically
small: for instance, it is a constant for standard multi-armed bandit problems (e.g., Bernoulli bandits and
Gaussian bandits), and is polylogarithmic in the size of the state and action spaces for reinforcement learning
problems with finite state and action spaces.
Proposition D.1 (Lemma A.16 of Foster et al. (2021)). Suppose Assumption D.2 holds. Fix T ∈ N, δ ∈
(0, e−1 ), and write bT = log(2B 2 T ). Then there is an algorithm AlgEst that guarantees that, with probability
1 − δ, we have
EstH (T ) ≤ O(bT · est(M, T ) + b2T log(δ −1 )),
i.e., we can take EstH (T, δ) = C · (bT · est(M, T ) + b2T log(δ −1 )) for some universal constant C.
D.1.2

Proof of Theorem 3.2

Proof of Theorem 3.2. Fix T ∈ N and an algorithm (p, q) = {q t (·|·), p(·|·)}Tt=1 . For each model M ∈ M+ ,
we use the abreviation PM ≡ PM,(p,q) , and write EM for the corresponding expectation. We also define
"
#
T
1X t
M
T
M
t−1
pM = E [p(·|H )],
qM = E
q (·|H ) .
T t=1
Choose ε(T ) as in the theorem statement, and write ε = ε(T ). Choose M ∈ co(M) so that decε (M) =
decε (M, M ).23 We will prove a lower bound on the expected risk in terms of decε (M, M ). Define


M := arg max Eπ∼pM [g M (π)] | Eπ∼qM [DH2 M (π), M (π) ] ≤ ε2 ,
M ∈M

where we recall that C(T ) := log(T ∧ V (M)). Note that if the HqM ,ε (M ) = ∅, then by definition
decε (M, M ) = 0 and the result follows. Thus, we may assume that HqM ,ε (M ) 6= ∅, and hence the
choice of M above is well-defined. Furthermore, the choice of M ensures that
Eπ∼pM [g M (π)] ≥ decε (M, M ) = decε (M).

(25)

23 If the supremum over M is not achievable, then we may apply the argument that follows for a sequence that achieves the

supremum.

60

By Lemma A.13 in Foster et al. (2021), we have24


DH2 PM , PM ≤ C(T ) · T · Eπ∼qM [DH2 M (π), M (π) ] ≤ C(T ) · T · ε2 .
Using the data processing inequality, it follows that
DH2 (pM , pM ) ≤ C(T ) · T · ε2 ≤

1
· decε (M),
8R

(26)

where the second inequality follows from the choice of ε = ε(T ).
Next, using Lemma A.11 in Foster et al. (2021) and the fact that g M (π) ∈ [0, R] for all π, we have
Eπ∼pM [g M (π)] ≤3 · Eπ∼pM [g M (π)] + 4R · DH2 (pM , pM ).
Combining the above display with (25) and (26) and rearranging, we see that
1
· decε (M) ≤ Eπ∼pM [g M (π)] = EM Eπ∼p(·|HT ) [g M (π)] = EM [Risk(T )],
6
which gives the desired lower bound on expected risk.

D.2

Proofs from Section 3.2

D.2.1

Proof of Proposition 3.1
dec

(M)

)
Proof of Proposition 3.1. Define ∆ := 8·ε(Tε(T
)2 ·C(T )·T . If ∆ ≥ 1, then we have ε(T ) ≥ ε(T ), so we may
assume from here on that ∆ < 1. Choose


log 1/∆
α=
≥ 1,
2 log(Creg /creg )

2
which in particular is the smallest positive integer so that (Creg
/c2reg )α ≥ 1/∆. Such α is well-defined by
our assumption that Creg > creg and since 1/∆ > 1. Applying Assumption 3.1 to ε = ε(T ) · (creg /Creg )j for
0 ≤ j < α, it follows that
α (M)
decε(T ) (M) ≤c2α
reg · decε(T )/Creg

2α
α (M)
≤∆ · Creg
· decε(T )/Creg

≤decε(T ) (M) ·

α (M)
decε(T )/Creg

α )2 · C(T ) · T
8 · (ε(T )/Creg

.

α
Hence ε(T ) ≥ ε(T )/Creg
, and so
α (M) ≥
decε(T ) (M) ≥ decε(T )/Creg

1
· decε(T ) (M).
c2α
reg

The definition of α and ε(T ) gives that

2α
Creg
Creg 1
Creg 83 · dlog 2/δe · EstH · C(T )
≤
·
≤
·
.
creg
creg ∆
creg
decε(T ) (M)
2α β
Our definition of β ensures that c2α
reg ≤ ((Creg /creg ) ) , meaning that, for some constant C,
β

β
β
β
−β
decε(T ) (M) ≤ c2α
· decε(T ) (M),
reg · decε(T ) (M) ≤ (C · Creg /creg ) · log 1/δ · EstH · C(T ) · decε(T ) (M)
24 In order to apply this result, we need to ensure that for all measurable sets E ⊆ O and all π ∈ Π, we have M (E|π) ≤ V (M).
M (E|π)

This follows from the definition of V (M) in (13) and the fact that M ∈ co(M).

61

and rearranging yields:
decε(T ) (M) ≤ C log 1/δ · EstH · C(T ) · Creg /creg

D.2.2

β
 1+β

1

· decε(T ) (M) 1+β .

Proof of Proposition 3.2 and Proposition 3.3

Proof of Proposition 3.2. We set Π = [A] and O = {0, 1}. For each δ > 0, we define a model class
Mδ ⊂ (Π → ∆(O)), as follows: Mδ = {Mδ,a : a ∈ [A]}, and define Mδ,a (π) = Ber(1/2 + δI {π = a}). We
SL
now set M = i=2 M2−i , from which it follows that |M| ≤ LA. Define f M (π) = EM,π [r], where r ∼ M (π).
Finally set H = (M, Π, O, {f M (·)}M ). Note that the instance H is actually a standard (non-hidden reward)
DMSO instance in the sense of Foster et al. (2021).
Since the model class M is a subclass of the class of all A-armed bandit problems, we have from Proposition
5.1 of Foster et al. (2021) and Proposition 4.1 (which
applies identically to HR-DMSO instances
in addition
√
√
to MA-DMSO instances) that decε (H ) ≤ O(ε A). Furthermore, we have M(H , T ) ≤ O( T A) (Audibert
and Bubeck, 2009) (up to logarithmic factors, this bound is also a consequence of, e.g., Theorem 3.1).
PA
For each δ > 0, write Hδ := (Mδ , Π, O, {f M (·)}M ). Also write M δ = A1 a=1 Mδ,a ∈ co(Mδ ). Since

for all π ∈ Π and a ∈ [A], DH2 M δ (π), Mδ,a (π) ≤ 4δ 2 , it is straightforward to see that dec4δ/√A (Hδ ) ≥
dec4δ/√A (Hδ , M δ ) ≥ Ω(δ). Since increasing the size of the model class cannot decrease the DEC, it follows
√
√
that, for all ε satisfying 1/ A > ε > 2−L , decε (H ) ≥ Ω(ε A). Finally, since the rewards are observed in the
instance H , we can use Theorem 2.1 of Fosterpet al. (2023) to conclude that for A at least some sufficiently
large constant, and T ≤ 2L/2 , M(H , T ) ≥ Ω( A/T ).
Proof of Proposition 3.3. Fix L to be larger than some universal constant (whose value will be
specified below), and consider any value for a constant Cprob ≥ 1. We define the following instance
H = (M, Π, O, {f M (·)}M ∈M ), with the individual components defined as follows:
• For 1 ≤ ` ≤ L, define α` := 1/L, N` = 2` , and δ` = (Cprob1 N` )2 .
QL
• Let V := `=1 [N` ], and set Π := V.
QL
• Let O = `=1 ([N` ] ∪ {⊥` }). For ease of notation we write O` := [N` ] ∪ {⊥` }.
• For o` ∈ O` and v` ∈ [N` ], define


1 − δ(N` − 1) : o` =⊥`
Pv` (o` ) := δ`
: o` ∈ [N` ]\{v` }


0
: o` = v` .
Then Pv` ∈ ∆(O` ).
• The class M is indexed by tuples v ∈ V; in particular, for each v = (v1 , . . . , v` ) ∈ V, there is a model
Mv , defined as follows. For π ∈ Π, Mv (π) ∈ ∆(O) is the following distribution which does not depend
on π: for o = (o1 , . . . , oL ) ∈ O,
Mv (π)(o) =

L
Y

Pv` (o` ).

`=1

Since the distribuiton Mv (π) does not depend on π, we will often drop the argument π and simply
write Mv ∈ ∆(O). Accordingly, the Hellinger distance between observation distributions of two models
M, M 0 ∈ co(M) will be denoted by DH2 (M, M 0 ).

62

• For all v ∈ V and π = (π1 , . . . , πL ) ∈ Π, the value function f : Π → [0, 1] is defined as follows:
f Mv (π) :=

L
X

α` · (1 − I {π` = v` }) .

`=1

PL
For convenience we write f`Mv (π) := (1 − I {π` = v` }), so that f Mv (π) = `=1 α` · f`Mv (π). It is clear
that for all v ∈ V there is some π (namely, any π so that π` 6= v` for all `) for which f Mv (π) = 1,
meaning that g Mv (π) = 1 − f Mv (π).
Upper bounding the minimax sample complexity. Fix some T ∈ N; we next upper bound M(H , T ).
Since the distribution over observations for all models in the class M does not depend on the decision, to
specify an algorithm (p, q) we need only to specify the distribution p, which is a mapping from T -tuples of
observations to distributions over decisions. To define p, we first define mappings p` : O`T → [N` ], as follows:
(
Unif(Π) : o`,1 = · · · = o`,T =⊥` ,
p` (o`,1 , . . . , o`,T ) :=
Io`,t
: t := arg min{s ∈ [T ] | o`,s 6=⊥` } exists.
In particular, p` outputs the first index of an observation which is not ⊥` ; if no such index exists, then p`
outputs the uniform distribution over [N` ]. Now we define
L

p(o1 , . . . , oT ) := (p` (o`,1 , . . . , o`,T ))`=1 ,
where we have written ot = (o1,t , . . . , oL,t ) for each T ∈ [T ].
We now upper bound the risk of the algorithm p. We abbreviate the distribution over histories under a given
model Mv ∈ M by PMv (·), and write EMv [·] for the corresponding expectation. For each ` ∈ [L], we have, for
all Mv ∈ M,
EMv Eπ∼p(o1 ,...,oT ) [1 − f`Mv (π)] ≤ (1 − δ` (N` − 1))T ·

1
,
N`

since the probability that there is no t ∈ [T ] so that o`,t 6=⊥` is (1 − δ` (N` − 1))T , and on the complement of
this event (so that such t exists), p` (o1 , . . . , oT ) puts all its mass on such o`,t 6= v` , so that f`Mv (π) = 1. Hence
Mv

E

Eπ∼p(o1 ,...,oT ) [g

Mv

(π)] ≤

L
X
α`
`=1

N`

T

· (1 − δ` (N` − 1)) ≤

L
X
α`
`=1

N`

·

1−

1
2
2Cprob
N`

!T
.

2
Given T ≤ NL , choose `? = `? (T ) ∈ [L] as large as possible so that T ≥ 2 log(N`? ) · 2Cprob
N`? , which gives

EMv Eπ∼p(o1 ,...,oT ) [g Mv (π)] ≤

`?
X
α`
`=1

≤

`?
X
`=1


T
L
X
2 log N`?
α`
· exp −
+
N`
T
N`
`=`? +1

α`
1
·
+
N` N`2?

L
X
`=`? +1

2
8Cprob
log T
α`
1
≤
≤
,
N`
N`?
T

where the final inequality uses that our choice of `? gives that N`? ≥ 8C 2 T log T ,
prob

Lower bounding the DEC. By the tensorization property of the squared Hellinger distance, we have, for
any two models Mv , Mu ∈ M,
 Y

L 
L 
L
Y
X
1
1
1
1 − DH2 (Mu , Mv ) =
1 − DH2 (Pv` , Pu` ) =
1 − · I {u` 6= v` } · 2δ` ≥ 1 −
δ` · I {u` 6= v` } ,
2
2
2
`=1

which implies that DH2 (Mu , Mv ) ≤ 2

`=1

PL

`=1

`=1 δ` · I {u` 6= v` }. Let v := (1, 1, . . . , 1) ∈ V, and set M := Mv .

63

√
Now consider any 2 ≥ ε ≥ 2δL . Choose `? = `? (ε) to be the smallest possible value of ` ∈ [L] so that
ε2 ≥ 2δ` . For each i ∈ [N`? ], define v i ∈ V by:
(
v ` : ` 6= `?
v i` =
i
: ` = `? ,

and write M i := Mvi . Then for all i ∈ [N`? ], we have DH2 M , M i ≤ 2δ`? ≤ ε2 . For any distribution
p ∈ ∆(Π), there must be some i? ∈ [N`? ] so that
i?

Eπ∼p [1 − f`M? (π)] = Pπ∼p (π`? = i? ) ≥ 1/N`? .
Therefore,
decε (M, M ) ≥

p
α`?
α`? Cprob
Cprob
√
= α`? Cprob δ`? ≥
·ε= √
· ε,
N` ?
8
8·L

(27)

where the final inequality uses that ε2 ≤ 8δ` since δ`+1 = δ` /4 for all ` < L and ε ≤ 2.
Upper bounding the DEC. Next we upper bound decε (M) for ε ∈ (0, 2); while not necessary for lower
bounding ε(T ), an upper bound on the decε (M) serves to ensure that the class M satisfies the regularity
condition of Assumption 3.1. This certifies that the instance H we construct satisfies the assumptions that
we use to upper and lower bounding minimax risk in terms of the DEC.
Consider any M ∈ co(M). We can write M = Ev∼µ [Mv ] for some distribution µ ∈ ∆(V). For each ` ∈ [L], let
QL
µ` ∈ ∆([N` ]) be the marginal of µ on [N` ] (recall that V = `=1 [N` ]). Since DH2 (Pu
, Pv` ) = 2δ` for u` 6= v` ,
`

any two distinct values v` , v`0 ∈ [N` ] satisfying DH2 (Eu` ∼µ` [Pu` ], Pv` ) ≤ ε2 and DH2 Eu` ∼µ` [Pu` ], Pv`0 ≤ ε2
must in turn satisfy




2δ` = DH2 Pv` , Pv`0 ≤ 2 · DH2 (Eu` ∼µ` [Pu` ], Pv` ) + 2 · DH2 Eu` ∼µ` [Pu` ], Pv`0 ≤ 4ε2 .
(28)
√
Now consider any ε ∈ ( δL , 2). Define ` = `(ε) to be the largest possible value of ` ∈ [L] so that ε2 < δ` /2. By
(28) it follows that for all ` ≤ `, there is at most a single value of v` ∈ [N` ] so that DH2 (Eu` ∼µ` [Pu` ], Pv ) ≤ ε2 .
Denote this value of v` by v ` if such a v` exists; if not, choose v ` ∈ [N` ] arbitrarily.

By the data processing inequality, for any v ∈ V and each ` ∈ [L], it holds that DH2 M , Mv ≥ DH2 (Eu` ∼µ` [Pu` ], Pv` ).

Thus, for each Mv ∈ M so that DH2 M , Mv ≤ ε2 , we must have that v` = v ` for all ` ≤ `.
Now choose any v ? ∈ V so that v`? 6= v ` for all ` ≤ `, and define p? ∈ ∆(Π) as follows:


p? := Unif v ∈ V | v` = v`? ∀` ≤ ` .
We may now compute:
L
X


α`
1
Eπ∼p? [g M (π)] | DH2 M , M ≤ ε2 ≤
≤
≤ 2Cprob ε,
N`
N`+1
M ∈M

decε (M, M ) ≤ sup

`=`+1

where the final inequality uses that ε2 ≥ δ` /8 by definition of ` and the fact that ε ≥

√

δL .

Bounding ε(T ). Consider any T ≤ NL /L3 , which ensures that (for sufficiently large L),
√
p
Cprob
2
√
≥
= 2δL ,
Cprob · NL
8 8 · L · C(T ) · T

(29)

where we have used that C(T ) ≤ C0 · log(T ) for some universal constant C0 . Recall that ε(T ) is defined to
be as large as possible so that ε(T )2 · C(T ) · T ≤ 81 · decε(T ) (M). Set


Cprob
Cprob
ε0 := √
≥Ω
,
T log(T ) · L
8 8 · L · C(T ) · T
which, using (29) and (27), satisfies ε20 · C(T ) · T ≤ 81 · decε0 (M), and thus ε(T ) ≥ ε0 .

64

D.2.3

Proof of Theorem 3.3

p
Proof of Theorem 3.3. Given any Cprob ≥ 1, fix N = d T /Cprob e. For real numbers δ, β ∈ (0, 1),
we will define instances H δ,β of the HR-DMSO framework. We will later choose H1 , H2 to be such
instances for certain choices of δ, β. For some model classes Mδ,β , each of size N , we will have, for all δ, β,
H δ,β = (Mδ,β , Π, O, {f M (·)}M ), i.e., the instances H δ,β share the same decision space, observation space,
and value functions. We next define these components:
• Π = [N ] and O = [N ] ∪ {⊥}.
δ,β
• For all δ, β, we have Mδ,β = {M1δ,β , . . . , MN
}. For i ∈ [N ] and π ∈ Π, Miδ,β (π) ∈ ∆(O) is the following
distribution, which does not depend on π:


1 − δ(N − 1) − β : j =⊥
δ,β
Mi (π)(j) = δ
: j ∈ [N ]\{i}


β
: j = i.

Since the distribution Miδ,β (π) does not depend on π, we will often drop the argument π and simply
write Miδ,β ∈ ∆(O).
δ,β

• For all δ, β and for all π ∈ Π, i ∈ [N ], the value function f Mi

: Π → [0, 1] is defined as follows:

δ,β

f Mi (π) := 1 − I {i = π} .
δ,β

Since the above value function does not depend on δ, β, we will simply write f i (π) := f Mi (π) and
δ,β
g i (π) := g Mi (π) = I {i = π}.
In the model Miδ,β , all decisions except decision i are optimal. Furthermore, we will always have β < δ,
meaning that, under Miδ,β , it is more likely to observe any given index in [N ]\{i} than it is to observe i.
Upper bounding the minimax risk. Next, for T ∈ N, we upper bound M(H δ,β , T ). Since the
distribution over observations for all models in the classes Mδ,β does not depend on the decision, to specify an
algorithm (p, q) we need only to specify the distribution p, which is a mapping from T -tuples of observations
to distributions over decisions. Furthermore, to specify the distribution over histories under a given model
δ,β
Miδ,β , we write EMi [·]. Now consider the algorithm p defined by:
(
Unif(Π) : o1 = · · · = oT =⊥
p(o1 , . . . , oT ) :=
(30)
Iot
: t := arg min {s ∈ [T ] | os 6=⊥} , exists.
In particular, p outputs the index of the first observation which is not ⊥; if no such index exists, then p
outputs the uniform distribution over decisions. To upper bound the expected risk of p, note that, for any
model Miδ,β , we have
δ,β

EMi Eπ∼p(o1 ,...,oT ) [g i (π)] ≤

(1 − δ(N − 1) − β)T
β
(1 − δ(N − 1))T
β
+
≤
+
,
N
β + δ(N − 1)
N
δ(N − 1)

(31)

where the first term on the right-hand side accounts for the case that o1 = · · · = oT =⊥, and the second term
gives the probability that, given that there exists s such that os 6=⊥, the index t of the first such observation
satisfies ot = i.
Lower bounding the minimax risk. We next lower bound the minimax risk for the instances H δ,β in
the following lemma; the proof is provided at the end of the section.
Lemma D.1. Fix any real numbers C ≥ 1 and  ∈ (0, 1), suppose δ ≤ 1/N , and write β = δ/C. The
minimax risk for the instance H δ,β is bounded below as follows: for S ≤ 1/δ 1− , M(H δ,β , S) ≥ 2N C1 2/ .

65

Computing Dφ . It is now straightforward to compute the Dφ -divergence between any two models in Mδ,β .
In particular, for i ∈ [N ], we have:
(
0
:i=j
δ,β
δ,β
Dφ (Mi k Mj ) =
(32)
β · φ(δ/β) + δ · φ(β/δ) : i 6= j.
Choosing δ, β. Let  ∈ (0, 1) and T ∈ N be as in the theorem statement. Since φ is assumed to be
(α, β)-bounded, we have that
φ(N /α ) · N −/α + φ(N −/α )
β · N
≤
≤ β0 · N ,
φ(2)/2 + φ(1/2)
φ(2)/2 + φ(1/2)
o
n
β
.
where we have written β 0 := max 1, φ(2)/2+φ(1/2)

(33)

For some constant C0 > 0 to be specified below, we choose
δ1 =

C0 ln T
,
(N − 1)T

β1 =

δ1
;
N /α

δ2 =

φ(N /α ) · N −/α + φ(N −/α )
· δ1 ,
φ(2)/2 + φ(1/2)

β2 =

δ2
.
2

(34)

The choices of δ1 , β1 , δ2 , β2 ensure that
β1 · φ(δ1 /β1 ) + δ1 · φ(β1 /δ1 ) = β2 · φ(δ2 /β2 ) + δ2 · φ(β2 /δ2 ),

(35)

which, together with (32), ensures that for all i, j ∈ [N ], Dφ (Miδ1 ,β1 k Mjδ1 ,β1 ) = Dφ (Miδ2 ,β2 k Mjδ2 ,β2 ).
Wrapping up. We set H1 = H δ1 ,β1 and H2 = H δ2 ,β2 , and correspondingly set M1 = Mδ1 ,β1 and
M2 = Mδ2 ,β2 . We define the one-to-one mapping E : M1 → M2 by the mapping that sends Miδ1 ,β1 7→ Miδ2 ,β2
for all i ∈ [N ]. It is clear that these definitions satisfy Item 1 and Item 2 of the proposition statement.
From (31), the expected risk of p against a worst-case model in Mδ1 ,β1 is bounded above as follows:
T
C0 ln T
β1
sup
E
[Risk(T )] ≤ 1 −
+
T
δ1 · (N − 1)
M ? ∈Mδ1 ,β1

T
C0 ln T
2
≤ exp −
+ 1+/α
T
N

1/2+/(2α)
Cprob
≤T −1 + 2 ·
,
T
p
where the final inequality holds as long as we choose C0 = 1; recall that N := d T /Cprob e. The above
display establishes the upper bound of Item 3.


M ? ,p

Next, for the lower bound, recall that (33) gives that δ2 ≤ β 0 · N  · δ1 , so
1

1

≥ 0 
≥
(β N · δ1 )1−
δ21−

(N 1− T )1−
N 1−4 · T
T 3/2−2
≥
≥
,

0
0
1/2+
2β C0 ln T
2β C0 Cprob ln T
2β 0 C0 C
ln T
prob


where the second-to-last inequality uses that T  ≤ N 2 · Cprob
. Thus, from Lemma D.1 with (δ, β) = (δ2 , β2 )

(so that C = 2), we have that for all T 0 ≤

M(H

δ2 ,β2

T 3/2−2
,
1/2+
2β 0 C0 Cprob ln T

1

1
1
) ≥ 1+2/ ·
≥ 2+2/ ·
N
2
2

s

T
Cprob

.

Thus, taking Cφ = 2β 0 C0 , the above inequality verifies the lower bound of Item 3.
66

δ,δ
Proof of Lemma D.1. Consider any algorithm p : OS → Π. Note that the distributions of M1δ,δ , . . . , MN
are all identical. Thus, there is some i ∈ [N ] so that
δ,δ

δ,δ

EMi Eπ∼p(o1 ,...,oS ) [I {π = i}] = EMi Eπ∼p(o1 ,...,oS ) [g i (π)] ≥ 1/N.
For λ ≥ 0, define

Sλ := (o1 , . . . , oS ) ∈ OS : |{t ∈ [S] : ot = i}| > λ .
Then the probability that (o1 , . . . , oS ) ∈ Sλ is bounded above as follows:
 
δ,δ
S
PMi ((o1 , . . . , oS ) ∈ Sλ ) ≤
· δ λ ≤ (Sδ)λ ≤ δ λ .
λ
Choosing λ = 2/ yields δ λ = δ 2 ≤ 1/N 2 , meaning that
 

δ,δ
EMi Eπ∼p(o1 ,...,oS ) I (o1 , . . . , oS ) 6∈ S2/ · I {π = i} ≥ 1/N − 1/N 2 ≥ 1/(2N ).
For any (o1 , . . . , oS ) 6∈ Sλ , we have that
Miδ,β ((o1 , . . . , oS ))
Miδ,δ ((o1 , . . . , oS ))

≥ (β/δ)λ ≤ 1/C λ .

Thus,

δ,β
δ,β
EMi Eπ∼p(o1 ,...,oS ) [g i (π)] ≥ EMi Eπ∼p(o1 ,...,oS ) [I {π = i} · I (o1 , . . . , oS ) 6∈ S2/ ]

δ,δ
≥ EMi Eπ∼p(o1 ,...,oS ) [I {π = i} · I (o1 , . . . , oS ) 6∈ S2/ ] · 1/C 2/
1
≥
.
2N C 2/

D.3

Proofs from Section 3.3

p
Proof of Theorem 3.4. Given Cprob ≥ 1, fix N = d T /Cprob e. Recall the definition of the instances H δ,β = (Mδ,β , Π, O, {f M (·)}M ) (for δ, β ∈ (0, 1)) of the HR-DMSO framework defined in the
proof of Theorem 3.3, where we have Π = [N ] and O = [N ] ∪ {⊥}. For each δ, β, we now define
fδ,β , Π,
e O,
e {Π0 }k , {Uk }k ) to be the instance of the (2-player) MA-DMSO framework constructed
M δ,β = (M
k
given the instance H δ,β per the construction in the proof of Theorem 2.2 with a value of V to be specified
e O,
e Π0 , Uk do not depend on δ, β. For clarity, we explicitly write out the definition of
below. In particular, Π,
k
δ,β
the components of M
in terms of the components of H δ,β :
e k = ∆(Σk ) for k ∈ {1, 2}, and Π
e =Π
e1 × Π
e 2.
• Define Σ1 = Π = [N ] and Σ2 = {0, 1, . . . , V }, Π
e k for
• Define Π0k , Uk for k ∈ [2] so that M δ,β is an NE instance (Definition 1.1); in particular, Π0k = Π
each k and Uk (πk0 , π) = (πk , π−k ).
e the reward space to be R = [−1, 1], and the
• Define the pure observation space to be O◦ := O ∪ {⊥},
2
e
full observation space to O := O◦ × R .
fδ,β is indexed by tuples (M, v) ∈ Mδ,β × {1, 2, . . . , V } = Mδ,β × [V ]. (Thus
• The model class M
fδ,β | = N V .) In particular, for each such tuple (M, v), we have a model M
fM,v , which is defined as
|M
follows:
67

fM,v ((σ1 , 0)) is
– For pure decisions of the form (σ1 , 0) ∈ Σ1 × Σ2 the distribution of (o◦ , r1 , r2 ) ∼ M
given by:
o◦ ∼ M (σ1 ) ∈ O ⊂ O◦ ,

r1 = r2 = 0.

– For pure decisions of the form (σ1 , i) ∈ Σ1 × Σ2 with i > 0, the distribution of (o◦ , r1 , r2 ) ∼
fM,v ((σ1 , i)) is given by:
M
(
−1
: i 6= v
e
o◦ = ⊥,
r2 = −r1 =
(36)
g M (σ1 ) : i = v,
where we recall that g M (σ1 ) = maxσ10 ∈Σ1 {f M (σ10 )} − f M (σ1 ).
e we can write π = π1 ×π2 for πk ∈ Π
e k for k ∈ [2]. Then the distribution
– For general decisions π ∈ Π,
f
f
MM,v (π) is the distribution of MM,v (σ) where σ = (σ1 , σ2 ) is distributed as: σk ∼ πk for k ∈ [2].
Next, let δ1 , δ2 be defined given T, N, , φ, α, as in the proof of Theorem 3.3 (in particular, they are
fδ1 ,β1
specified in (34)). We write M1 = M δ1 ,β1 and M2 = M δ2 ,β2 , and correspondingly write M1 = M
δ2 ,β2
f
and M2 = M
. Moreover, we define the mapping E : M1 → M2 in an analogous manner to the
δ,η
definition in the proof of Theorem 3.3. In particular, for each δ, β, we have Mδ,β = {M1δ,β , . . . , MN
}. First
δ1 ,β1
δ2 ,β2
δ1 ,β1
δ2 ,β2
define E0 : M
→ M
by E0 (Mi
) = Mi
, for i ∈ [N ] (exactly as was done in the proof of
fM,v ∈ M
fδ1 ,β1 = M1 (so that M ∈ Mδ1 ,β1 , v ∈ [V ]), define
Theorem 3.3. Then for each model of the form M
fM,v ) := M
fE (M ),v . We are now ready to verify the individual claims of the theorem:
E (M
0
fM,v ∈ M
fδ1 ,β1 (so that M ∈ Mδ1 ,β1 , v ∈ [V ]). For any (σ1 , σ2 ) ∈ Σ1 × Σ2 ,
Proof of Item 1. Consider any M
we have, by definition of E ,


: σ2 = 0
0
f
f
M
E (M
)
f2 M,v (σ1 , σ2 ) = f2 M,v (σ1 , σ2 ) = −1
: σ2 ∈ [V ]\{v}

 M
g (σ1 ) : σ2 = v,
which establishes Item 1 since all instances are 2-player 0-sum instances.
fM,v , M
fM 0 ,v0 ∈ M
fδ1 ,β1 (so that M, M 0 ∈ Mδ1 ,β1 , and
Proof of Item 2. Consider any two models M
0
v, v ∈ [V ]). For any π1 ∈ Π1 , we have that
fM,v (π1 , 0) k M
fM 0 ,v0 (π1 , 0)) = Dφ (M (π1 ) k M 0 (π1 ))
Dφ (M
fE (M ),v (π1 , 0) k M
fE (M 0 ),v0 (π1 , 0))
=Dφ (E0 (M )(π1 ) k E0 (M 0 )(π1 )) = Dφ (M
0
0
fM,v )(π1 , 0) k E (M
fM 0 ,v0 )(π1 , 0)),
=Dφ (E (M

(37)

fδ,β above, the second equality follows by Item 2
where the first and third equalities follow by definition of M
of Theorem 3.3 and the fact that our choice of δ1 , β1 , δ2 , β2 is identical to that in the proof of Theorem 3.3
(cf. Eq. (34) and Eq. (35)), and the fourth equality follows from definition of E .
fM,v (σ1 , σ2 ) and E (M
fM,v )(σ1 , σ2 ) =
Next, for any σ1 ∈ Σ1 and σ2 ∈ Σ2 \{0}, note that the distributions M
e
f
ME0 (M ),v (σ1 , σ2 ) are identical: the pure observation under both these distributions is ⊥ a.s., and the rewards
are given by (36), where we have noted that g M (σ1 ) = g E0 (M ) (σ1 ) for all σ1 ∈ Σ1 . It follows that for any
fM,v (π1 , π2 ) and E (M
fM,v )(π1 , π2 ) are identical. In a similar
π1 ∈ Π1 and σ2 ∈ ∆(Σ2 \{0}), the distributions M
f
fM 0 ,v0 )(π1 , π2 ) are identical.
manner, we have that for any such π1 , π2 , the distributions MM 0 ,v0 (π1 , π2 ) and E (M
Therefore,
fM,v (π1 , π2 ) k M
fM 0 ,v0 (π1 , π2 )) = Dφ (E (M
fM,v )(π1 , π2 ) k E (M
fM 0 ,v0 )(π1 , π2 )).
Dφ (M
68

(38)

Now consider any joint decision (π1 , π2 ) ∈ Π1 × Π2 . Let us write π2 = π2 (0) · I0 + (1 − π2 (0)) · π20 , where
f∈M
fδ,β (for any δ, β), the distributions M
f(π1 , 0) and M
f(π1 , π 0 )
π20 ∈ ∆(Σ2 \{0}). Since, for any model M
2
e and under the first, the
have disjoint support (namely, under the second, the pure observation is always ⊥,
e it follows from Lemma B.1 that for any two models M
f, M
f0 ∈ Mδ,β ,
pure observation is never ⊥),
f(π1 , π2 ) k M
f0 (π1 , π2 )) =π2 (0) · Dφ (M
f(π1 , 0) k M
f0 (π1 , 0))
Dφ (M
f(π1 , π20 ) k M
f0 (π1 , π20 )).
+ (1 − π2 (0)) · Dφ (M

(39)

Then for the decision (π1 , π2 ) ∈ Π1 × Π2 , with π20 defined as above, we have
fM,v (π1 , π2 ) k M
fM 0 ,v0 (π1 , π2 ))
Dφ (M
fM,v (π1 , 0) k M
fM 0 ,v0 (π1 , 0)) + (1 − π2 (0)) · Dφ (M
fM,v (π1 , π 0 ) k M
fM 0 ,v0 (π1 , π 0 ))
=π2 (0) · Dφ (M
2
2
fM,v )(π1 , 0) k E (M
fM 0 ,v0 )(π1 , 0)) + (1 − π2 (0)) · Dφ (E (M
fM,v )(π1 , π 0 ) k E (M
fM 0 ,v0 )(π1 , π 0 ))
=π2 (0) · Dφ (E (M
2
2
fM,v )(π1 , π2 ) k E (M
fM 0 ,v0 )(π1 , π2 )),
=Dφ (E (M
where the first and third equalities use (39), and the second equality uses (37) and (38). The above display
verifies Item 2.
Proof of Item 3. For each δ, β ∈ (0, 1), the construction of M δ,β given H δ,β according to the construction
in the proof of Theorem 2.2, together with the conclusion of Theorem 2.2, gives that, for all T 0 ∈ N,
M(M δ,β , T 0 ) ≤ M(H δ,β , T 0 ) ≤ M(M δ,β , T 0 ) + O((T 0 log(T 0 )/V )1/4 ).

(40)

Then Item 3 of Theorem 3.3, together with our choice of δ1 , β1 , δ2 , β2 to mimic that in the proof of Theorem 3.3,
1/2+
yields that for all T 0 with T ≤ T 0 ≤ T 3/2−2 · (Cφ Cprob ln T )−1
M(M1 , T 0 ) = M(M δ1 ,β1 , T 0 ) ≤M(H δ1 ,β1 , T 0 ) ≤

1
+2·
T



Cprob
T

1/2+/(2α)

(41)

M(M2 , T 0 ) = M(M δ2 ,β2 , T 0 ) ≥M(H δ2 ,β2 , T 0 ) − O((T 0 log(T 0 )/V )1/4 )
1/2

Cprob
− O((T 0 log(T 0 )/V )1/4 ).
≥2−2−2/ ·
T
Choosing V = T 100 · 28+8/ ensures that
0

−3−2/

M(M2 , T ) ≥ 2


·

Cprob
T

1/2
.

(42)

Together (41) and (42) verify Item 3.

E

Proofs for Section 4

Throughout this section, we consider an instance M = (M, Π, O, {Π0k }k , {Uk }k ) of MA-DMSO which is an
NE instance (Definition 1.1). It follows in particular that for any M ∈ M, π ∈ Π, we have
hM (π) =

K
X
k=1

hM
k (π) =

K
X

sup fkM (πk0 , π−k ) − fkM (π).

0
k=1 πk ∈Πk

69

E.1

Bounds for general games with convex decision spaces

Proof of Theorem 4.1. For each k ∈ [K] and π−k ∈ Π−k , define
fk (π−k ) := {πk 7→ M |k (πk , π−k ) : M ∈ M}.
M
fk in (17) that for each k ∈ [K], M
fk = S
f
It is straightforward from the definition of M
π−k ∈Π−k Mk (π−k ),
S
fk (π−k )) ⊆ co(M
fk ). For any π −k ∈ Π−k and M ∈ co(M), we denote
co(M
and therefore that
π−k ∈Π−k

fk (π −k )) by (M , π −k ). (In particular, (M , π −k ) is the model that sends
the corresponding element of co(M
πk 7→ M |k (πk , π −k ).) It then suffices to prove the following stronger result: for each M ∈ co(M),
r-decoγ (M , M ) ≤

K
X
k=1

sup
π −k ∈Π−k

(43)

fk , (M , π −k )).
r-decoγ/K (M

Next, note that for any M ∈ M, π−k ∈ Π−k , the value function for the model πk 7→ M |k (πk , π−k ) is given by
f M |k (πk ) = fkM (πk , π−k ), for π ∈ Π (this holds since the distribution of the reward under M |k (πk , π−k ) is
simply the distribution of agent k’s reward under M (πk , π−k )). Then for any M ∈ co(M), π −k ∈ Π−k , we
have
fk , (M , π −k ))
r-decoγ/K (M



γ
2
M
0
M
· DH M (πk , π−k ), M (πk , π −k )
= inf
sup Eπk ∼pk max
fk (πk , π−k ) − fk (πk , π−k ) −
0 ∈Π
πk
K
pk ∈∆(Πk ) M ∈M
k
π−k ∈Π−k


≥

inf

pk ∈∆(Πk )

sup
M ∈M
π−k ∈Π−k

Eπk ∼pk

ak ∼πk

γ
· DH2
max
fk (πk0 , π−k ) − fkM (ak , π−k ) −
0 ∈Π
πk
K
k
M

M (ak , π−k ), M (ak , π −k )


= inf

πk ∈Πk

sup
M ∈M
π−k ∈Π−k

Eak ∼πk

γ
· DH2
max
fk (πk0 , π−k ) − fkM (ak , π−k ) −
0 ∈Π
πk
K
k
M

M (ak , π−k ), M (ak , π −k )






,



(44)

where the inequality uses joint convexity of the squared Hellinger distance, and the final inequality uses
the fact that any distribution pk ∈ ∆(Πk ) may be replaced by the singleton distribution for the decision
π
ek := Eπk ∼pk [πk ], without changing the value of the expression.
Thus
fk , (M , π −k )) ≥ inf
r-decoγ/K (M

πk ∈Πk

sup
M ∈M
π−k ∈Π−k

h
i
γ
2
Eak ∼πk hM
(a
,
π
)
−
·
D
M
(a
,
π
),
M
(a
,
π
)
.
k
−k
k
−k
k
−k
k
H
K

Existence of fixed points. For each k ∈ [K], define the set-valued function Ck : Π → P(Πk ) by
h
i
γ
Ck (π) := arg min
sup
Eak ∼πk hM
· DH2 M (ak , π−k ), M (ak , π −k ) .
k (ak , π−k ) −
K
πk ∈Πk M ∈M,π−k ∈Π−k
Further, for π−k ∈ Π−k , M ∈ M, define the function GM,π−k : Πk × Π−k → R by
h
i
γ
GM,π−k (πk , π −k ) = Eak ∼πk hM
· DH2 M (ak , π−k ), M (ak , π −k ) .
k (ak , π−k ) −
K
Assumption 4.1 gives that for all ak , the map π −k 7→ M (ak , π −k ) is linear. It follows by the
 dominated
convergence theorem that for all M, π−k , ak , the function π −k 7→ DH2 M (ak , π−k ), M (ak , π −k ) is continuous.
Hence GM,π−k (πk , π −k ) is continuous in (πk , π −k ), and the function
e k (πk , π −k ) :=
G

sup
M ∈M,π−k ∈Π−k

GM,π−k (πk , π −k )

is also continuous in (πk , π −k ). Furthermore, since, for each π −k , the function GM,π−k (πk , π −k ) is linear in
e k (πk , π −k ) is convex in πk . It follows that Ck (π) = arg minπ ∈Π {G
e k (πk , π −k )} is a
πk (Assumption 4.1), G
k
k
e k and Lemma B.8, we
closed, nonempty, and convex subset of Πk for all π. Furthermore, by continuity of G
have that Ck (π) is upper hemicontinuous. By Lemma
QB.9, it follows that the mapping π 7→ C1 (π) × · · · × CK (π)
has a fixed point, namely some π ∈ Π so that π ∈ k∈[K] Ck (π).
70

Q
Applying the fixed point strategy. Let π ∈ k∈[K] Ck (π) be a fixed point of C1 × · · · × CK . Then


2
r-decoγ (M , M ) ≤ sup hM (π) − γ · DH
M (π), M (π)
M ∈M

(K
X

= sup
M ∈M

≤

K
X
k=1

≤

K
X
k=1

=

K
X
k=1

=

K
X
k=1

≤

K
X

)

2
hM
k (π) − γ · DH M (π), M (π)

k=1

n
o
γ
· DH2 M (π), M (π)
hM
k (π) −
K
M ∈M
sup

n
o
γ
2
hM
(π
,
π
)
−
·
D
M
(π
,
π
),
M
(π
,
π
)
k
−k
k
−k
k
−k
k
H
K
M ∈M,π−k ∈Π−k
sup

h
i
γ
2
(a
,
π
)
−
M
(a
,
π
)
·
D
M
(a
,
π
),
Eak ∼πk hM
k
−k
k
−k
k
−k
k
H
K
M ∈M,π−k ∈Π−k
sup

h
i
γ
Eak ∼πk hM
· DH2 M (ak , π−k ), M (ak , π −k )
k (ak , π−k ) −
πk ∈Πk M ∈M,π−k ∈Π−k
K
inf

sup

fk , (M , π −k )).
decγ/K (M

k=1

Above, we have used the following facts:
1. The second equality uses Assumption 4.1 to conclude that for all ak , πk , π −k , M, M ,
Po∼M (ak ,π−k ) (ϕ(o) = ak ) = 1,

Po∼M (ak ,π−k ) (ϕ(o) = ak ) = 1,

thus allowing us to apply Lemma B.1 to give that



DH2 M (π k , π−k ), M (π k , π −k ) = Eak ∼πk DH2 M (ak , π−k ), M (ak , π −k ) .
2. The third equality follows from the fact that π k ∈ Ck (π) for all k ∈ [K].
3. The final inequality follows from (44).

E.2

Bounds for Markov games

Here, we prove Theorem 4.2. The proof uses a number of technical lemmas which are stated and proven in
the sequel.
Proof of Theorem 4.2. As in the proof of Theorem 4.1, for each k ∈ [K] and π−k ∈ Π−k , we define
fk (π−k ) := {πk 7→ M |k (πk , π−k ) : M ∈ M}.
M
fk (π −k ) ⊆ M
fk by (M , π −k ). We
For any π −k ∈ Π−k and M ∈ M, we denote the corresponding element of M
0
will prove the following stronger result: there is some constant C > 0 so that for each M ∈ M,
K

r-decoγ (M , M ) ≤

C 0 KH log H X
fk , (M , π −k )).
+
sup r-decoγ/(C 0 KH log H) (M
γ
π −k ∈Π−k

(45)

k=1

Fix any  > 0. For each k ∈ [K], let Πk be a finite -cover of Πk in the sense that for all πk ∈ Πk , there is
some element πk ∈ Πk so that, for all M ∈ M, π−k ∈ Π−k ,
DH2 (M (πk , π−k ), M (πk , π−k )) ≤ 2 .
71

Furthermore, we require that the mapping πk 7→ πk is measurable with respect to the Borel σ-algebra on Πk .
By finiteness of S, Ak , it is straightforward to see that such a finite cover Πk exists. The size of the cover Πk
may depend on |S|, |A|, but this will not matter as |Πk | will not enter into our final bounds. (We introduce
discretization here only to ensure that Πk is compact when applying Lemma B.8.)
We collect a few basic properties of Πk in the below lemma, proved at the end of the section:
Lemma E.1. For any πk ∈ Πk , there is some πk ∈ Πk so that the following holds. For any M, M ∈ M,
π−k ∈ Π−k ,
 1

DH2 M (πk , π−k ), M (πk , π −k ) ≥ · DH2 M (πk , π−k ), M (πk , π −k ) − 22
3
M

|hM
k (πk , π−k ) − hk (πk , π−k )| ≤.
Existence of fixed points. Let C > 0 be the constant of Lemma E.4, and write γ 0 = γ/(CKH log H).
For each k ∈ [K], define the function Ck : Π → ∆(Πk ) by


0
2
sup
Eπk ∼pk hM
+  · kpk k22 ,
Ck (π) = arg min
k (πk , π−k ) − γ · DH M (πk , π−k ), M (πk , π −k )
pk ∈∆(Πk ) M ∈M,π−k ∈Π−k



where kpk k22 denotes the squared `2 norm of pk , interpreted as a vector in the Euclidean space R|Πk | .
Further, for π−k ∈ Π−k , M ∈ M, define the function GM,π−k : ∆(Πk ) × Π−k → R by


0
2
GM,π−k (pk , π −k ) = Eπk ∼pk hM
k (πk , π−k ) − γ · DH M (πk , π−k ), M (πk , π −k ) .
We may view π −k as an element of ∆(Ak )S×[H] , which is a subset of Euclidean space (since Ak , S are assumed
to be finite). Since there are finitely many states and actions, it follows from the dominated
convergence

theorem that for all M, πk , π−k , the function π −k 7→ DH2 M (πk , π−k ), M (πk , π −k ) is continuous. Hence
GM,π−k (pk , π −k ) is continuous in (pk , π −k ). Hence the function
e k (pk , π −k ) :=
G

sup
M ∈M,π−k ∈Π−k

GM,π−k (pk , π −k ) +  · kpk k22

e k (pk , π −k ) is strongly
is also continuous. Furthermore, GM,π−k (pk , π −k ) is linear in pk (for fixed π −k ), so G
e k (pk , π −k )} is a singleton for all π. Furthermore,
convex in pk (for fixed π −k ). Thus Ck (π) = arg minpk ∈∆(Πk ) {G

e
by continuity of Gk , compactness of ∆(Πk ) and Π−k , and Lemma B.8, we have that Ck (π) is upper
hemicontinuous, which means, by single-valuedness, it is actually continuous.
Given M ∈ M, π −k ∈ Π−k , note that the pure observation distribution of the model πk 7→ M (πk , π −k ) is
exactly that of an MDP, which we denote by M π−k : it has horizon H, state space S, action space Ak , and
rewards and transitions given by those of M when each agent k 0 6= k acts according to π k0 ,h (·|s) at each state
s and step h (to be precise, the rewards of M π−k are given by the rewards of agent k in M ). Note that the
space of randomized nonstationary policies of M π−k is Πk (using Assumption 4.2).
Since we do not assume convexity of Πk , elements pk ∈ ∆(Πk ) may not belong to Πk . We next introduce
a set of decisions in Πk which are “equivalent” to pk given a reference model M and a reference decision
π −k . In particular, for M ∈ M, π −k ∈ Π−k , and pk ∈ ∆(Πk ), let Π?M ,π−k (pk ) ⊂ Πk be the set of all policies
πk? ∈ Πk which satisfy Eq. (50) of Lemma E.2 for pk and πk 7→ M (πk , π −k ). Note that Π?M ,π−k (pk ) is a
nonempty convex set: as a subset of ∆(Ak )S×[H] , it is a product of sets (one for each factor of ∆(Ak )),
each of which is either a singleton or all of ∆(Ak ). It is straightforward from the definition that the map
(pk , π −k ) 7→ Π?M ,π−k (pk ) is upper hemicontinuous. Then Lemma E.4 gives that, for any M and π −k and pk , if
πk? ∈ Π?M ,π−k (pk ) is the corresponding policy in (50), then for γ > 0,
n
o
γ
?
sup
hM
· DH2 M (πk? , π−k ), M (πk? , π −k )
k (πk , π−k ) −
K
M ∈M, π−k ∈Π−k


1
0
2
≤ 0+
sup
Eπk ∼pk hM
(46)
k (πk , π−k ) − γ · DH M (πk , π−k ), M (πk , π −k ) .
γ
M ∈M,π−k ∈Π−k
72

Since the mapping π 7→ Ck (π) ∈ ∆(Πk ) is continuous, the composition Ck? (π) := Π?M ,π−k (Ck (π)) is upper
hemicontinuous. Thus, by Kakutani’s fixed point theorem (Osborne and Rubinstein, 1994, Lemma 20.1), the
?
set-valued mapping C ? (π) := C1? (π) × · · · × CK
(π) has a fixed point.
Applying the fixed point strategy. Let π ∈ Π be a fixed point for C ? , so that π k ∈ Ck? (π) for each
k ∈ [K]. Then
r-decoγ (M , M )

(K
X

≤ sup
M ∈M

≤

K
X
k=1

≤

K
X
k=1

)
hk (π) − γ · DH2
M


M (π), M (π)

k=1

n
o
γ
hM
· DH2 M (π k , π −k ), M (π k , π −k )
k (π k , π −k ) −
K
M ∈M
sup

n
o
γ
· DH2 M (π k , π−k ), M (π k , π −k )
hM
k (π k , π−k ) −
K
M ∈M, π−k ∈Π−k
sup

K

X


1
0
2
≤ 0+
sup
Eπk ∼Ck (π) hM
k (πk , π−k ) − γ · DH M (πk , π−k ), M (πk , π −k )
γ
M ∈M,π−k ∈Π−k

(47)

k=1

K
X


1
0
2
 + inf 
sup
Eπk ∼pk hM
≤ 0+
(48)
k (πk , π−k ) − γ · DH M (πk , π−k ), M (πk , π −k )
γ
pk ∈∆(Πk ) M ∈M,π−k ∈Π−k
k=1


K
X

1
γ0
0
2
2
M
≤ 0+
2 + γ · 2 + inf
sup
Eπk ∼pk hk (πk , π−k ) −
· DH M (πk , π−k ), M (πk , π −k ) .
γ
3
pk ∈∆(Πk ) M ∈M,π−k ∈Π−k
k=1

(49)

where (47) uses Eq. (46) and the fact that π k ∈ Ck? (π) = Π?M ,π (Ck (π)) for each k, and (48) uses the
−k
definition of Ck (π). Finally, (49) uses Lemma E.1, as follows: given any distribution pk ∈ ∆(Πk ), we
consider the distribution pk ∈ ∆(Πk ) which is given by pushing forward pk through the map πk 7→ πk
(here we use that πk 7→ πk is measurable to ensure that pek p is well-defined). Then by Lemma E.1, for all
M ∈ M, π−k ∈ Π−k , π −k ∈ Π−k , we have
M
Eπk ∼pk [hM
k (πk , π−k )] ≤Eπk ∼pk [hk (πk , π−k )] + 


γ0
−γ 0 · Eπk ∼pk [DH2 M (πk , π−k ), M (πk , π −k ) ] ≤ −
· Eπk ∼pk [DH2 M (πk , π−k ), M (πk , π −k ) ] + γ 0 · 22 .
3

By taking  → 0, we obtain that, for some constant C > 0,
CKH log H
r-decoγ (M , M ) ≤
+
γ

K
X

fk , (M , π −k ))
r-decoγ/(CKH log H) (M

k=1
K

≤

CKH log H X
fk , (M , π
+
sup r-decoγ/(CKH log H) (M
e−k )),
γ
π
e−k ∈Π−k
k=1

thus verifying (45).

73

E.2.1

Supporting lemmas

Proof of Lemma E.1. To establish the first property, we use the definition of Πk and the triangle inequality
for Hellinger distance to conclude that

DH2 M (πk , π−k ), M (πk , π −k )


≤3 · DH2 (M (πk , π−k ), M (πk , π−k )) + DH2 M (πk , π −k ), M (πk , π −k ) + DH2 M (πk , π−k ), M (πk , π −k )

≤3 · DH2 (M (πk , π−k ), M (πk , π−k )) + 22 ,
and rearranging gives the first claimed inequality of the lemma.
To prove the second inequality, we note that for each πk ∈ Πk , the cover element πk ∈ Πk satisfies the
following: for all M ∈ M, π−k ∈ Π−k
M

M
M


|hM
k (πk , π−k ) − hk (πk , π−k )| = |fk (πk , π−k ) − fk (πk , π−k )| ≤DH (M (πk , π−k ), M (πk , π−k )) ≤ .

The following lemma shows that for any MDP M and distribution p ∈ ∆(ΠRNS ), there exists a corresponding
randomized policy in ΠRNS which induces identical occupancies in M .
Lemma E.2. Consider any finite-horizon MDP M = (S, H, A, P, R, µ) with finite state and action spaces
S, A. Let ΠRNS denote the set of randomized nonstationary policies of M . Suppose p ∈ ∆(ΠRNS ) is a
distribution over ΠRNS with finite support. Consider any policy π ? ∈ ΠRNS so that:
0

X

∀a ∈ A, s ∈ S s.t.

,π
p(π 0 ) · dM
(s) > 0 :
h

π 0 ∈ΠRNS

,π
p(π) · dM
h (s)

X

πh? (a|s) =

π∈ΠRNS : p(π)>0

0

M ,π
0
(s)
π 0 ∈ΠRNS p(π ) · dh

P

· πh (a|s).
(50)

?

,π
Then for all states s ∈ S, dM
(s) =
h
P
M ,π
π∈ΠRNS p(π) · dh (s, a).
?

P

As a consequence, it follows that V1M ,π =

M ,π

π∈ΠRNS p(π) · dh

P

?

,π
(s), and for all (s, a) ∈ S × A, dM
(s, a) =
h

M ,π
.
π∈ΠRNS p(π) · V1

Proof of Lemma E.2. We drop the superscript M in all relevant quantities throughout the proof. We
use induction on h, noting that the base case h = 1 is immediate since dπ1 is identical for all π ∈ ΠRNS . Fix
p ∈ ∆(ΠRNS ), and let π ? be chosen as in Eq. (50). Assuming that the statement of the lemma holds at step
h − 1, we compute
X
?
?
?
dπh (s) =
dπh−1 (s0 ) · πh−1
(a0 |s0 ) · Ph−1 (s|s0 , a0 )
s0 ,a0 :
?
0
dπ
h−1 (s )>0

!
=

X

X

s0 ,a0 :

π0

0
p(π ) · dπh−1 (s0 )

0

·

X
π

p(π) · dπh−1 (s0 )
P
· πh−1 (a0 |s0 ) · Ph−1 (s|s0 , a0 )
0 ) · dπ 0 (s)
p(π
0
π
h−1

?
0
dπ
h−1 (s )>0

=

X

p(π) ·

X

=

X

π

dπh−1 (s0 ) · πh−1 (a0 |s0 ) · Ph−1 (s|s0 , a0 )

s0 ,a0 :
?
0
dπ
h−1 (s )>0

π

=

X

p(π) ·

X

dπh−1 (s0 ) · πh−1 (a0 |s0 ) · Ph−1 (s|s0 , a0 )

s0 ,a0

p(π) · dπh (s),

π

74

?

where the second-to-last inequality follows since if dπh−1 (s0 ) = 0, then (using the inductive hypothesis) for
all π, p(π) · dπh−1 (s0 ) = 0. The above chain of equalities then completes the inductive step. It then follows
P
?
immediately from the definition of π ? that dπh (s, a) = π∈ΠRNS p(π) · dπh (s, a).
The final statement regarding the value functions follows since, for all policies π,
V1π =

H
X

X

dπh (s, a) · rh (s, a).

h=1 (s,a)∈S×A

The remaining lemmas establish certain technical properties for the policy π ? ∈ ΠRNS constructed in
Lemma E.2.
Lemma E.3. There is a constant C > 0 so that the following holds. Consider any finite-horizon MDP
M = (S, H, A, P M , RM , µM ) with finite state and action spaces S, A. Let ΠRNS denote the set of randomized
nonstationary policies of M , and let p ∈ ∆(ΠRNS ) be a distribution of finite support. Consider any policy
π ? ∈ ΠRNS satisfying Eq. (50) for p. Then for any MDP M = (S, H, A, P M , RM , µM ),



Eπ∼p DH2 M (π), M (π) ≤ CH log H · DH2 M (π ? ), M (π ? ) .
Proof of Lemma E.3. For any π ∈ ΠRNS , a full observation (r, o◦ ) ∼ M (π) consists of the trajectory
(s1 , a1 , r1 , . . . , sH , aH , rH ), where s1 ∼ µM , sh+1 ∼ PhM (sh , ah ) for h ∈ [H − 1], rh ∼ RhM (sh , ah ) for h ∈ [H],
and ah ∼ πh (sh ) for h ∈ [H]. We use the notation τ1:h to denote the portion of a trajectory consisting of
(s1 , a1 , r1 , . . . , sh , ah , rh ).
We use PM,π to denote the distribution of the trajectory τH ∼ M (π), and PM ,π to denote the distribution of
the trajectory τH ∼ M (π). We use EM,π [·] and EM ,π [·] to denote the corresponding expectations. By Lemma
A.13 of Foster et al. (2021), it holds that, for some constant C > 0,


Eπ∼p DH2 M (π), M (π)
"H
#
X

M ,π
2
M ,π
M,π
DH P (sh |τ1:h−1 ), P (sh |τ1:h−1 )
≤C log(H) · Eπ∼p E
h=1
M ,π

+ C log(H) · Eπ∼p E

"H
X

#
DH2

M,π

P

(rh |τ1:h−1 , sh , ah ), P

M ,π

(rh |τ1:h−1 , rh , ah )



h=1

"H−1
#
X


DH2 PhM (sh , ah ), PhM (sh , ah )
=C log(H) · DH2 µM , µM + C log(H) · Eπ∼p EM ,π
h=1
M ,π

+ C log(H) · Eπ∼p E

"H
X

DH2

#

Rh (sh , ah ), Rh (sh , ah ) .
M

M

(51)

h=1

By Lemma E.2 and the definition of π ? , for each h ∈ [H], s ∈ S, a ∈ A, it holds that
 ,π

M ,π ?
Eπ∼p dM
(s, a).
h (s, a) = dh
Thus, we may replace the expectation over π ∼ p, (sh , ah ) ∼ M (π) in (51) with (sh , ah ) ∼ M (π ? ), and obtain


Eπ∼p DH2 M (π), M (π)
"H−1
#
X


2
M
M
M ,π ?
2
M
M
≤C log(H) · DH µ , µ + E
DH Ph (sh , ah ), Ph (sh , ah )
h=1
M ,π

+E

"H
X
?

#!
DH2


Rh (sh , ah ), Rh (sh , ah )
M

M

h=1

75

.

By Foster et al. (2021, Lemma A.9) and the data processing inequality, we have that:
"H−1
#
X


M ,π ?
2
M
M
E
DH Ph (sh , ah ), Ph (sh , ah ) ≤4H · DH2 M (π ? ), M (π ? ) ,
h=1

EM ,π

"H
X
?

#


DH2 RhM (sh , ah ), RhM (sh , ah ) ≤4H · DH2 M (π ? ), M (π ? ) ,

h=1



DH2 µM , µM ≤DH2 M (π ? ), M (π ? ) .
It then follows that, for some constant C > 0,



Eπ∼p DH2 M (π), M (π) ≤ C · H log(H) · DH2 M (π ? ), M (π ? ) ,
as desired.
Lemma E.4. There is a constant C > 0 so that the following holds. Consider any model class M consisting
of MDPs of fixed horizon H, finite state space S, finite action space A, and cumulative rewards bounded by
[0, 1]. Let ΠRNS be the class of randomized nonstationary policies. Consider any M ∈ M and finite-support
distribution p ∈ ∆(ΠRNS ), and let π ? ∈ ΠRNS denote any policy satisfying Eq. (50) for M and p. Then for
any γ > 0,


sup f M (πM ) − f M (π ? ) − γ · DH2 M (π ? ), M (π ? )
M ∈M



CH log H
γ
2
M
M
≤
+ sup Eπ∼p f (πM ) − f (π) −
· DH M (π), M (π) .
(52)
γ
CH log H
M ∈M
An immediate consequence of Lemma E.4 is that


sup f M (πM ) − f M (π) − γ · DH2 M (π), M (π)
inf
π∈ΠRNS M ∈M



CH log H
γ
2
M
M
≤
+
inf
sup Eπ∼p f (πM ) − f (π) −
· DH M (π), M (π) .
γ
CH log H
p∈∆(ΠRNS ) M ∈M
?
Proof of Lemma E.4. Consider any M ∈ M, finite-support p ∈ ∆(Π
RNS
 ), and let π be defined as in

the statement of the lemma. Lemma E.2 gives that f M (π ? ) = Eπ∼p f M (π) . Let C be the constant from
Lemma E.3, and let C 0 = C + 12 . Then for any γ > 0,

sup f M (πM ) − f M (π ? ) − C 0 H log H · γ · DH2 M (π ? ), M (π ? )



M ∈M



1
γ
≤ sup f M (πM ) − f M (π ? ) − C 0 H log H · γ · DH2 M (π ? ), M (π ? ) +
+ · DH2 M (π ? ), M (π ? )
2γ
2
M ∈M

1
= sup f M (πM ) − f M (π ? ) − CH log H · γ · DH2 M (π ? ), M (π ? ) +
2γ
M ∈M

 M
1
≤ sup Eπ∼p f (πM ) − f M (π) − γ · DH2 M (π), M (π) +
(53)
2γ
M ∈M




1
γ
1
≤ sup Eπ∼p f M (πM ) − f M (π) − γ · DH2 M (π), M (π) +
+ · DH2 M (π), M (π) +
2γ
2
2γ
M ∈M
h
i 1
γ
= sup Eπ∼p f M (πM ) − f M (π) − · DH2 M (π), M (π) + .
(54)
2
γ
M ∈M
where (53) uses Lemma E.3. The statement of the proposition follows by replacing γ with γ · C 0 H log H.

76

F

Proofs for upper bounds from Section 5

In this section we prove Theorem 5.1, which gives an upper bound for learning equilibria for CCE and
CE instances in the MA-DMSO framework in a way that avoids the curse of multiple agents, i.e., avoids
exponential scaling with the number of players K. In Appendix F.1, we describe the algorithm (Algorithm 1)
used to prove Theorem 5.1, which is based on the idea of exploration-by-optimization, used previously in
Foster et al. (2022b); Lattimore and György (2021). In Appendices F.2 to F.4 we prove Theorem 5.1; our
proofs roughly follow those of Foster et al. (2022b), but require some subtle modifications to account for the
multi-agent nature of our problem, as well as the more general notion of deviation sets Π0k that we study.

F.1

The multi-agent exploration-by-optimization objective

We begin by describing the algorithm, Multi-Agent Exploration-by-Optimization (MAExO; Algorithm 1) used
to prove Theorem 5.1. The algorithm is a multi-agent counterpart to the exploration-by-optimization (ExO+ )
algorithm given Foster et al. (2022b). At a high level, MAExO (as well as its precursor ExO+ ) is a variant
of EXP3, which applies the exponential weights algorithm to a sequence of reward estimators which act as
importance-weighted estimates for the true reward function. However, unlike EXP3 and ExO+ , MAExO does not
apply exponential weights to agents’ pure policies themselves, but rather to their potential deviations Π0k .
In particular, MAExO operates over T rounds of interaction with the environment. At each round t ∈ [T ], the
algorithm first computes, for each player k, a reference distribution qkt ∈ ∆(Π0k ) over their deviation space
Π0k , according to an exponential weights update given a sequence of vectors fbk1 , . . . , fbkt−1 constructed by the
algorithm in previous rounds (Line 4). Roughly speaking, for s ≤ t − 1, the entries fbks (πk0 ), πk0 ∈ Π0k , of these
vectors can be interpreted as the potential gain in value that agent k could receive by deviating to πk0 , given
adversarial choices of the other agents’ decisions. Accordingly, the reference distribution qkt will put more
mass on deviations which lead to larger gains in value.
Next, in Line 5, the players jointly solve an optimization problem. To define this optimization problem, we
introduce some notation. For each k ∈ [K], let Gk denote the set of all functions gk : Π0k × Σ × O → R, and
QK
QK
?
let G = G1 × · · · × GK . Given q ∈ k=1 ∆(Π0k ), η > 0, π ∈ Π, g ∈ G, π ? = (π1? , . . . , πK
) ∈ k=1 Π0k , and
M ∈ M, define
"K
#
X
Γq,η (π, g; π ? , M ) :=Eσ∼π
fkM (Uk (πk? , σ)) − fkM (σ)
(55)
k=1





K
1 X
η
Eσ∼π,o∼M (σ) Eπk0 ∼qk exp
· (gk (πk0 ; σ, o) − gk (πk? ; σ, o)) − 1 .
+ ·
η
π(σ)
k=1

With this definition, the optimization problem solved in Line 5 of MAExO is as follows:
(π t , g t ) ← arg min

supQ

π∈Π,g∈G M ∈M,π ? ∈

Γqt ,η (π, g; π ? , M ).

(56)

K
0
k=1 Πk

The interpretation of the objective (55) and the optimization problem (56) is as follows. Roughly speaking,
for each k ∈ [K], πk0 ∈ Π0k , σ ∈ Σ, o ∈ O, the value gk (πk0 ; σ, o) for g ∈ Gk can be interpreted as an estimate
of player k’s gain in value by deviating to πk0 under joint decision profile σ, under an unknown model M
which is “consistent with” the decision-observation pair (σ, o). Then, by solving (56), the algorithm wishes to
t
find a joint decision π t ∈ Π and estimator g t = (g1t , . . . , gK
) ∈ G, which, for each player k ∈ [K], satisfies the
following two properties:
• First, corresponding to the first term in (55), for a worst-case unknown model M and an unknown
deviation πk? , it should not be possible for player k to gain much value by deviating to πk? given the
policy π t . Here πk? should be interpreted as the best deviation in hindsight at the termination of the
algorithm.
• Second, corresponding to the second term in (55): π t and gkt should be chosen so that with high
probability under σ ∼ π t , gkt does not underestimate the value gain in deviating to πk? as compared to a
77

sample πk0 from the reference distribution qkt . The second term in (55) can be viewed as a term that
regularizes the adversarial choice of πk? , analogously to the term subtracting squared Hellinger distance
in the offset DEC (see (11)): in particular, if πk? has significantly high value under the estimate gk , then
this term will be very negative, canceling out the (potentially large) first term.
Given (π t , g t ) computed in Eq. (56), Algorithm 1 samples a decision σ t ∼ π t and receives an observation ot
from the true model. Finally, in Line 7, players construct their reward estimators fbkt (to be used in future
0
iterations t0 > t to construct qkt ) using gkt (·; σ t , ot ). Once all T rounds conclude, the algorithm outputs
the joint decision π
b which is the uniform average over the T pure decisions σ 1 , . . . , σ T . We remark that
Algorithm 1 is different from having each player run the exploration-by-optimization algorithm of Foster et al.
(2022b): in the latter, agents each individually optimize their own objective, in contrast to the optimization
problem in (56), which is solved for all agents simultaneously. This feature of MAExO allows us to obtain a
guarantee scaling with r-decoγ (co(M )), which can be arbitrarily smaller than what one obtains by using the
approach of Foster et al. (2022b) (see Proposition A.11).
In Definition F.1 below, we formalize the value of the minimax objective (56) computed in the course of
Algorithm 1.
Algorithm 1 Multi-Agent Exploration by Optimization (MAExO)
1: parameters: Learning rate η > 0.
2: Initialize fb0 (π 0 ) := 0 for all k ∈ [K], π 0 ∈ Π0 .
k

k

k

k

3: for t = 1, 2, . . . T do
4:

For each agent k ∈ [K], define qkt ∈ ∆(Π0k ) via exponential weights update: for πk0 ∈ Π0k ,
 P

t−1
exp η i=1 fbki (πk0 )
 P
.
qkt (πk0 ) := P
t−1 bi
00
π 00 ∈Π0 exp η
i=1 fk (πk )
k

5:

k

t
. The players jointly solve the following objective:
Define q t = q1t × · · · × qK

(π t , g t ) ← arg min

sup

π∈Π,g∈G M ∈M,π ? ∈Π0

Γqt ,η (π, g; π ? , M ).

6:

Sample σ t ∼ π t , play σ t , and observe ot ∼ M ? (σ t ).

7:

Each player k ∈ [K] constructs their reward estimator fbkt as follows: for πk0 ∈ Π0k ,
t

0

// Eq. (55)

t

t

g (π ; σ , o )
.
fbkt (πk0 ) = k kt t
π (σ )
8: return joint decision π
b := T1

PT

t=1 Iσ t .

Definition F.1 (Exploration-by-optimization objective). Consider any instance M = (M, Π, O, {Π0k }k , {Uk }k )
QK
satisfying Assumption 5.1. For any scale parameter η > 0 and distribution q ∈ k=1 ∆(Π0k ), define
exoη (M , q) =

inf

sup

π∈Π,g∈G M ∈M,π ? ∈QK

Γq,η (π, g; π ? , M ),

0
k=1 Πk

and let exoη (M ) := supq∈QK
0 exoη (M , q).
k=1 ∆(Π )
k

To prove Theorem 5.1, we first (Appendix F.2) bound the performance of Algorithm 1 in terms of exoη (M ).
Following this, in Appendix F.3 and Appendix F.4, we will upper bound exoη (M ) by decoγ (M ) for an
appropriate choice of γ, using a quantity we call the multi-agent (parametrized) information ratio as an
intermediary. Finally, in Appendix F.5, we put these pieces together and prove Theorem 5.1.
78

F.2

Bounding the performance of Algorithm 1

The following result bounds the performance of Algorithm 1 (namely, the quantity hM (b
π )) in terms of
exoη (M ).
?

Lemma F.1. For any η > 0, Algorithm 1 ensures that for all δ > 0, with probability at least 1 − δ,


K
K
X
2 X
K · |Π0k |
M?
M?
0
M?
h (b
π) =
max
fk (Uk (πk , π
b)) − fk (b
π ) ≤ exoη (M ) +
·
.
log
0 ∈Π0
πk
Tη
δ
k
k=1

k=1

Proof of Lemma F.1. For any πk? ∈ Π0k and player k ∈ [K], we define player k’s regret with respect to
the deviation πk? ∈ Π0k as follows:
T
X

Regk (πk? ) =


?
?
?
?
Eσt ∼πt [fkM (Uk (πk? , σ t )) − fkM (σ t )] = T · fkM (Uk (πk? , π
b)) − fkM (b
π) ,

t=1

where the second equality above uses the definition
b in Line 8 of Algorithm 1 and the second property in
PKof π
Assumption 5.1. Hence, it suffices to bound T1 · k=1 maxπk? ∈Π0k Regk (πk? ) to establish the statement of the
lemma.
Throughout the proof we use the following convention: for functions fk : Π0k → R (for instance, the reward
0
estimators fbkt defined in Line 7 of Algorithm 1), we will view fk as a vector in R|Πk | , whose coordinates
0
are the values of fk (πk0 ), for πk0 ∈ Π0k . Furthermore, for each πk0 ∈ Π0k , we write eπk0 ∈ R|Πk | to denote the
corresponding unit vector whose πk0 -th entry is 1 and all other entries are 0.
PT
By adding and subtracting t=1 heπk? , fbkt i, we obtain
Regk (πk? ) =

T
X

?

?

?

?

Eσt ∼πt [fkM (Uk (πk? , σ t )) − fkM (σ t )]

t=1

=

T
X

Eσt ∼πt [fkM (Uk (πk? , σ t )) − fkM (σ t )] +

t=1

T
X

heπk? , fbkt i −

t=1

T
X

heπk? , fbkt i.

(57)

t=1

By Lemma F.2 and the definition of the multiplicative weights updates for qkt in Line 4 of Algorithm 1, it
holds that
T
X
heπk? , fbkt i
t=1

≤

T
X

T

hqkt+1 , fbkt i −

t=1


1X
1
DKL (qkt+1 k qkt ) + DKL eπk? k qk1
η t=1
η

t
T
X
1X
log |Π0k |
≤
hqkt+1 , fbkt i −
DKL (qkt+1 k qkt ) +
.
η t=1
η
t=1

(58)

By Lemma B.5, we have that for each t ∈ [T ],


X
1
1
hqkt+1 , fbkt i − DKL (qkt+1 k qkt ) ≤ log 
qkt (πk0 ) · exp(η · fbkt (πk0 )) .
η
η
0
0
πk ∈Πk

Using the above together with (58) and (57), we obtain
Regk (πk? ) ≤

T
X

M?

Eσt ∼πt [fk (Uk (πk? , σ t )) − fk

t=1

+

M?



X
1
(σ t )] + log 
qkt (πk0 ) · exp(η · fbkt (πk0 ))
η
0
0
πk ∈Πk

T
X

log |Π0k |
−
heπk? , fbkt i.
η
t=1

(59)

79

Let F t denote the σ-algebra generated by (σ 1 , o1 , . . . , σ t , ot ) (where the random variables σ s , os are drawn
as in Algorithm 1). Note that F t is a filtration, and write Et [·] = E[·|F t ]. For each πk? ∈ Π0k , we define a
sequence of random variables, denoted {Xt (πk? )}t∈[T ] , by


X
Xt (πk? ) := log 
qkt (πk0 ) · exp(η · fbkt (πk0 )) − heπk? , η · fbkt i.
0 ∈Π0
πk
k

By Lemma B.7 and the union bound, with probability at least 1 − δ/K, it holds that for all πk? ∈ Π0k ,
T
X

Xt (πk? ) ≤

t=1

T
X

log Et−1 [e

?
)
Xt (πk


] + log

t=1

K · |Π0k |
δ


.

(60)

Note that π t , q t are both measurable with respect to F t−1 . Then, for any πk? ∈ Π0k and any t ∈ [T ], we may
compute
?

log Et−1 [eXt (πk ) ]






X

= log Et−1 exp log 



qkt (πk0 ) · exp(η fbkt (πk0 )) − η fbkt (πk? )

0 ∈Π0
πk
k

h
h
i
i
= log Eσt ∼πt Eot ∼M (σt ) Eπk0 ∼qkt exp(η fbkt (πk0 )) · exp(−η fbkt (πk? ))






η
η
t
0
t
t
t
?
t
t
·
g
(π
;
σ
,
o
)
·
exp
−
·
g
(π
;
σ
,
o
)
= log Eσt ∼πt Eot ∼M (σt ) Eπk0 ∼qkt exp
π t (σ t ) k k
π t (σ t ) k k



η
≤ Eσt ∼πt Eot ∼M (σt ) Eπk0 ∼qkt exp
· (gkt (πk0 ; σ t , ot ) − gkt (πk? ; σ t , ot )) − 1,
(61)
t
π (σ t )
where the final inequality uses that log(x) ≤ x − 1 for all x > 0.
By (59), (60), and (61), and a union bound over k ∈ [K], it follows that with probability at least 1 − δ, for all
?
?
π1? ∈ Π01 , . . . , πK
∈ Π0K , letting π ? = (π1? , . . . , πK
),
K
X

Regk (πk? )

k=1

≤

K
X
k=1



log
0
 log |Πk | +
η

T X
K
X



K·|Π0k |
δ

η


+

K
T X
X

 ?

?
Eσt ∼πt fkM (Uk (πk? , σ t )) − fkM (σ t )

t=1 k=1





1
η
t
0
t
t
t
?
t
t
Eσt ∼πt Eot ∼M (σt ) Eπk0 ∼qkt exp
·
(g
(π
;
σ
,
o
)
−
g
(π
;
σ
,
o
))
−
1
k k
k k
η
π t (σ t )
t=1 k=1

 X
K
T
2 X
K · |Π0k |
≤ ·
log
+
Γqt ,η (π t , g t ; π ? , M ? )
η
δ
t=1
k=1


K
T
0
X
2 X
K · |Πk |
f)
≤ ·
log
+
sup
Γqt ,η (π t , g t ; π
e? , M
η
δ
?
0
f
e ∈Π ,M ∈M
t=1 π
k=1


K
0
X
2
K · |Πk |
≤ ·
log
+ T · exoη (M ),
η
δ
+



k=1

where the second inequality uses the definition of Γqt ,η (π t , g t ; π ? , M ? ) in (55), and the final equality follows
since π t , g t are chosen so as to minimize the multi-agent exploration-by-optimization objective (Line 5 of
Algorithm 1).

80

Lemma F.2. Consider any d ∈ N, and let f 1 , . . . , f T ∈ Rd be an arbitrary sequence of vectors. For η > 0,
let q 1 , . . . , q T ∈ ∆d denote the exponential weights update iterates with step size η when the reward vectors are
given by f 1 , . . . , f T ; in particular, for t ∈ [T ]:
P
exp(η s≤t f s (i))
t
q (i) = Pd
.
(62)
P
s
j=1 exp(η
s≤t f (j))
Then for any q ∈ ∆d ,
T
X
t=1

hq, f t i ≤

T
X
t=1

T

hq t+1 , f t i −

1
1X
DKL (q t+1 k q t ) + DKL (q k q 1 ).
η t=1
η

Proof of Lemma F.2. By rearranging and telescoping, it suffices to show that, for each t ∈ [T ],
hq − q t+1 , f t i =

1
· (DKL (q k q t ) − DKL (q k q t+1 ) − DKL (q t+1 k q t )) .
η

To establish this inequality, we note that the multiplicative weight updates (62) are equivalent to the following
Pd
mirror descent updates with the negative entropy regularizer Φ(q) := i=1 qi · log qi :
∇Φ(pt+1 ) = ∇Φ(q t ) + η · f t ,

q t+1 =

pt+1
,
h1, pt+1 i

where 1 ∈ Rd denotes the all-ones vector. Using the fact that for all x, y, z ∈ ∆d (Eq. (4.1) of Bubeck (2015))
h∇Φ(y) − ∇Φ(x), x − zi = DKL (z k y) − DKL (z k x) − DKL (x k y)
with z = q, y = q t , x = q t+1 , we obtain
1
1
· (DKL (q k q t ) − DKL (q k q t+1 ) − DKL (q t+1 k q t )) = · h∇Φ(q t ) − ∇Φ(q t+1 ), q t+1 − qi
η
η
1
= · h∇Φ(q t ) − ∇Φ(pt+1 ), q t+1 − qi
η
=hf t , q − q t+1 i,
where in the second equality we have used that ∇Φ(q t+1 ) = ∇Φ(pt+1 ) − log (h1, pt+1 i) · 1.

F.3

The multi-agent parametrized information ratio

In this section, we introduce a multi-agent version of the parametrized information ratio of (Foster et al.,
2022b, Definition 3.1), and upper bound this information ratio by the DEC of the convex hull of M . In the
following section, we will upper bound exoη (M ) by this information ratio.
We first introduce some notation. We will wish to reason about the space of probability measures on
M × Π01 × · · · × Π0K . Since |M| may be infinite, to avoid measure-theoretic issues, we will slightly abuse
notation by letting ∆(M × Π01 × · · · × Π0K ) denote the set of finitely supported probability measures on
M × Π01 × · · · × Π0K . This convention ensures that for any function h : M × Π01 × · · · × Π0K → R and any
0
0
0 )∼µ [h(M, π , . . . , π
µ ∈ ∆(M × Π01 × · · · × Π0K ), E(M,π10 ,...,πK
1
K )] is well-defined.
Consider any k ∈ [K], a distribution µ ∈ ∆(M × Π01 × · · · × Π0K ), and a distribution π ∈ ∆(Σ). Let P denote
?
the law of the process (M, π1? , . . . , πK
) ∼ µ, σ ∼ π, and o ∼ M (σ). We introduce the following distributions,
depending on µ and k:
• Define the distribution µkpr ∈ ∆(Π0k ) by µkpr (πk0 ) = P(πk? = πk0 ), for πk0 ∈ Π0k .
• For each σ ∈ Σ and o ∈ O, define the distribution µkpo ∈ ∆(Π0k ) by µkpo (πk0 ; σ, o) = P(πk? = πk0 |(σ, o)),
for πk0 ∈ Π0k .
81

The distribution µkpr should be thought of as a prior distribution over the deviation πk? , and the distribution
µkpo (·; σ, o) should be thought of as a posterior distribution over πk? after observing the pure decision σ together
with an observation o ∼ M (σ).
Definition F.2 (Multi-agent information ratio). Given an instance M = (M, Π, O, {Π0k }k , {Uk }k ) which is
a generalized correlated equilibrium instance, the parametrized multi-agent information ratio of the instance
M is defined as
"K
#
X
M
?
M
? )∼µ
infrγ (M ) :=
sup
inf Eσ∼π E(M,π1? ,...,πK
fk (Uk (πk , σ)) − fk (σ)
µ∈∆(M×Π01 ×···×Π0K ) π∈Π

k=1

− γ · Eσ∼π Eo|σ

"K
X

#
DH2

µkpo (·; σ, o), µkpr (·)



.

k=1

In the above expression, when we write Uk (πk? , σ) and fkM (σ), we view σ ∈ Σ as an element of Π by associating
it with the singleton distribution on σ, recalling that Π = ∆(Σ).
Lemma F.3 upper bounds the multi-agent information ratio in terms of the multi-agent offset DEC of the
convex hull of a given instance.
Lemma F.3. Consider any instance M = (M, Π, O, {Π0k }k , {Uk }k ) which satisfies Assumption 5.1, and for
which co(M ) satisfies Assumption 1.3. Then for all γ > 0,
infrγ (M ) ≤ K · r-decoγ (co(M )).

Proof of Lemma F.3. We denote the pure decision sets of the instance M by Σ1 , . . . , ΣK , and the joint
decision set as Σ = Σ1 × · · · × ΣK . Fix a prior µ ∈ ∆(M × Π01 × · · · × Π0K ) and a distribution π ∈ ∆(Σ).
?
Recall our notation from above: let P denote the law of the process σ ∼ π, (M, π1? , . . . , πK
) ∼ µ, o ∼ M (σ).
k
0
?
0
k
0
?
0
For each k ∈ [K], let µpr (πk ) = P(πk = πk ) and µpo (πk ; σ, o) = P(πk = πk |(σ, o)).
Consider the value of the multi-agent information ratio given the choices for µ, π:
"K
#
"K
#
X
X

M
?
M
2
k
k
? )∼µ
Eσ∼π E(M,π1? ,...,πK
fk (Uk (πk , σ)) − fk (σ) − γ · Eσ∼π Eo|σ
DH µpo (·; σ, o), µpr (·) .
k=1

k=1

For each k ∈ [K], πk0 ∈ Π0k , and π ∈ Π, define M kπ0 (π) := Eµ [M (π)|πk? = πk0 ]. Further define M (π) = Eµ [M (π)].
Note that M kπ0 (σ) = Po|σ,πk0 and M (σ) = Po|σ .

k

k

To proceed, note that for each fixed σ ∈ Σ,
h 
i


Eo|σ DH2 µkpo (·; σ, o), µpr (·) =Eo|σ DH2 Pπk? |σ,o , Pπk?
h 
i
=Eo|σ DH2 Pπk? |σ,o , Pπk? |σ
h 
i
=Eπk? ∼µ DH2 Po|σ,πk? , Po|σ
i
h 
=Eπk? ∼µ DH2 M kπk? (σ), M (σ) ,

(63)

?
where the second equality follows since σ ∼ π and (π1? , . . . , πK
) ∼ µ are (marginally) independent, and the
third equality holds by Lemma B.2. Furthermore, we have that
M
?
M
M
?
M
?
? ∼µ Eµ [f
? )∼µ Eσ∼π [f
E(M,π1? ,...,πK
k (Uk (πk , σ)) − fk (σ)] =Eσ∼π Eπk
k (Uk (πk , σ)) − fk (σ)|πk ]
 Mk

Mk?
π?
π
=Eσ∼π Eπk? ∼µ fk k (Uk (πk? , σ)) − fk k (π)
 Mk

Mk?
π?
π
0
k
k
Eσ∼π fk (Uk (πk , σ)) − fk (σ) .
≤Eπk? ∼µ max
0
0

πk ∈Πk

82

(64)

Next, for any M 1 ∈ co(M), we have, for γ > 0,
r-decoγ (co(M ), M 1 )

= inf

sup

p∈∆(Π) M ∈co(M)

= inf

sup

p∈∆(Π) M ∈co(M)

≥ inf

sup

p∈∆(Π) M ∈co(M)

≥ inf

sup

p∈∆(Π) M ∈co(M)

≥

inf

sup

π∈∆(Σ) M ∈co(M)

Eπ∼p

"K 
X
k=1

Eπ∼p

"K
X
k=1

Eπ∼p

"K
X
k=1

K
X
k=1
K
X
k=1

fk (Uk (πk0 , π)) − fkM (π)
max
0 ∈Π0
πk
k



M

#
− γ · DH2

M (π), M 1 (π)


#

Eσ∼π [fk (Uk (πk0 , σ)) − fkM (σ)] − γ · DH2
max
0 ∈Π0
πk
k
M


Eσ∼π [M (σ)], Eσ∼π [M 1 (σ)]
#


Eσ∼π [fkM (Uk (πk0 , σ)) − fkM (σ)] − γ · Eσ∼π [DH2 M (σ), M 1 (σ) ]
max
0
0

πk ∈Πk



Eπ∼p Eσ∼π [fkM (Uk (πk0 , σ)) − fkM (σ)] − γ · Eπ∼p Eσ∼π DH2 M (σ), M 1 (σ)
max
0
0

πk ∈Πk



Eσ∼π [fkM (Uk (πk0 , σ)) − fkM (σ)] − γ · Eσ∼π DH2 M (σ), M 1 (σ) ,
max
0
0

πk ∈Πk

where the second equality follows from Assumption 5.1, the first inequality follows from convexity of squared
Hellinger distance, the second inequality follows from Jensen’s inequality, and the final inequality follows by
replacing any p ∈ ∆(Π) with the decision π := Eπ∼p [π] ∈ ∆(Σ) = Π.
By the above display, the following holds: for any M 1 ∈ co(M), there is some π ∈ Π so that, for each
M 2 ∈ co(M),
K
X
k=1

i
h


M2
M2
0
(U
(π
,
σ))
−
f
(σ)
− γ · Eσ∼π DH2 M 2 (σ), M 1 (σ) ≤ r-decoγ (co(M )).
max
E
f
k k
σ∼π
k
k
0
0

πk ∈Πk

(65)

Since we have assumed that co(M ) satisfies Assumption 1.3, the following holds: for each k ∈ [K], π ∈ Π,
and M 1 ∈ co(M), we have (again using Assumption 5.1)
i
h
max
Eσ∼π fkM 1 (Uk (πk0 , σ)) − fkM 1 (σ) = max
(66)
fkM 1 (Uk (πk0 , π)) − fkM 1 (π) ≥ 0.
0
0
0
0
πk ∈Πk

πk ∈Πk

Then, by (66) and (65), for each k ∈ [K], we have that for any M 1 ∈ co(M), there is π ∈ Π so that for each
M 2 ∈ co(M),
i
h


M2
M2
0
max
E
(U
(π
,
σ))
−
f
(σ)
− γ · Eσ∼π DH2 M 2 (σ), M 1 (σ) ≤ r-decoγ (co(M )).
(67)
f
σ∼π
k
k
k
k
0
0
πk ∈Πk

Next, choose π ∈ Π, given M 1 = M to ensure that (65) holds for all M 2 ∈ co(M). Then for each k ∈ [K]
and each πk? ∈ Π0k , choosing M 2 = M kπ? in (67),
k

 Mk

h 
i
Mk?
π?
π
0
2
k
k
k
M
≤ r-decoγ (co(M )).
max
E
f
(U
(π
,
σ))
−
f
(σ)
−
γ
·
E
D
? (σ), M (σ)
σ∼π
k
σ∼π
k
H
π
k
k
0
0
k

πk ∈Πk

Taking expectation over πk? ∼ µ and using (63) and (64), we obtain


E(M,πk? )∼µ Eσ∼π [fkM (Uk (πk? , σ)) − fkM (σ)] − γ · Eσ∼π Eo|σ DH2 µkpo (·; σ, o), µpr (·)

 Mk

h 
i
Mk?
π?
π
0
2
k
k
k
≤Eπk? ∼µ max
Eσ∼π fk (Uk (πk , σ)) − fk (σ) − γ · Eσ∼π DH M πk? (σ), M (σ)
0
0
πk ∈Πk

≤r-decoγ (co(M )).

83

Note that the choice of π depends only on M , and in particular it does not depend on k. Therefore, we may
sum the above display over k ∈ [K], to obtain
"K
"K
#
#
X
X

M
?
M
2
k
? )∼µ Eσ∼π
E(M,π1? ,...,πK
DH µpo (·; σ, o), µpr (·)
fk (Uk (πk , σ)) − fk (σ) − γ · Eσ∼π Eo|σ
k=1

k=1

≤K · r-decoγ (co(M )).
Using that the choice of µ ∈ ∆(M × Π01 × · · · × Π0K ) is arbitrary, we obtain that infrγ (M ) ≤ r-decoγ (co(M )),
as desired.

F.4

Relating the multi-agent information ratio and exploration-by-optimization
objective

In this section, we prove the following result, which upper bounds exoη (M ) by the multi-agent information
ratio of M , at scale 1/(8η).
Lemma F.4. Consider any instance M = (M, Π, O, {Π0k }k , {Uk }k ) satisfying Assumption 5.1. Then for all
η > 0,
exoη (M ) ≤ infr1/(8η) (M ).

Proof of Lemma F.4. Throughout the proof, we will denote the (finite) pure decision sets, as guaranteed
by Assumption
5.1, by Σ1 , . . . , ΣK , and the joint decision set by Σ := Σ1 × · · · × ΣK . Additionally, we write
Qk
Π0 := k=1 Π0k to denote the product of the deviation sets Π0k . We can write
exoη (M ) =

sup

inf

sup

Q
0 π∈∆(Σ), g∈G µ∈∆(M×Π0 )
q∈ K
k=1 ∆(Πk )

E(M,π? )∼µ [Γq,η (π, g; π ? , M )] .

For α ≥ max{1, 1/η} and ε ∈ (0, 1), define
Gα = {(g1 , . . . , gk ) ∈ G : kgk k∞ ≤ α ∀k ∈ [K]},

Pε = {π ∈ Π : π(σ) ≥ |Σ|−1 ∀σ}.

We will now use Sion’s minimax theorem (Theorem B.1), with X = Pε × Gα and Y = ∆(M × Π0 ), to
interchange the inf π∈Π,g∈G and the supµ∈∆(M×Π0 ) in the definition ot exoη (M ) above. We first check that
its preconditions hold:
• Let the set Pε have the standard topology induced from Π, so that Pε is compact, and let Gα have the
product topology. Tychanoff’s theorem yields that Gα is compact, and thus X = Pε × Gα is compact.
It is also clearly convex.
• Let us give Y = ∆(M × Π0 ) (which we recall is the space of finitely supportedR distributions on M × Π0 )
the weak topology, which is the coarsest topology so that the functional µ 7→ φdµ is continuous for all
bounded functions φ : M × Π0 → R.
• To establish the remaining preconditions, we need that the mapping (π, g, µ) 7→ E(M,π? )∼µ [Γq,η (π, g; π ? , M )]
is uniformly bounded for (π, g) ∈ Pε × Gα and µ ∈ ∆(M × Π0 ). This follows immediately from the
definition of Γq,η (π, g; π ? , M ) and the domains Pε and Gα .
• Clearly, the map µ 7→ E(M,π? ) [Γq,η (π, g; π ? , M )] is linear, and thus concave, for each π, g. Moreover, it
is continuous by boundedness of Γq,η (π, g; π ? , M ), and the fact that ∆(M × Π0 ) has the weak topology.
• By Lemma F.5, the map (π, g) 7→ E(M,π? )∼µ [Γq,η (π, g; π ? , M )] is convex in (π, g) for any fixed µ.
Furthermore, it is continuous by definition of the product topology and since π(σ) is uniformly bounded
below for π ∈ Pε .

84

Having verified all of the conditions for Theorem B.1 to apply, we now have:
exoη (M ) ≤

sup

q∈

=

inf

sup

E(M,π? )∼µ [Γq,η (π, g; π ? , M )]

sup

inf

E(M,π? )∼µ [Γq,η (π, g; π ? , M )] ,

0 π∈Pε ,g∈Gα µ∈∆(M×Π0 )
k=1 ∆(Πk )

QK

sup

Q
0 µ∈∆(M×Π0 ) π∈Pε ,g∈Gα
q∈ K
k=1 ∆(Πk )

(68)

where the inequality follows since we are restricting
to smaller sets Gα ⊂ G and Pε ⊂ Π in the infimum, and
QK
the equality uses Theorem B.1. Given q ∈ k=1 ∆(Π0k ), µ ∈ ∆(M × Π0 ), π ∈ Pε , consider the value of
inf E(M,π? )∼µ [Γq,η (π, g; π ? , M )]
"K
#
X
M
?
M
=E(M,π? )∼µ Eσ∼π
fk (U (πk , σ)) − fk (σ)
g∈Gα

k=1
K
X





η
0
?
· (gk (πk ; σ, o) − gk (πk ; σ, o)) − 1
E(M,π? )∼µ Eσ∼π,o∼M (σ) Eπk0 ∼qk exp
π(σ)
k=1
"K
#
X
M
?
M
=E(M,π? )∼µ Eσ∼π
fk (U (πk , σ)) − fk (σ)
1
+
inf
η g∈Gα

k=1

+

K
1X

η

k=1





η
E(M,π? )∼µ Eσ∼π,o∼M (σ) Eπk0 ∼qk exp
· (gk (πk0 ; σ, o) − gk (πk? ; σ, o)) − 1 ,
gk ∈Gk,α
π(σ)
inf

(69)

where we have used Gk,α to denote {gk ∈ Gk : kgk k∞ ≤ α}, so that Gα = G1,α × · · · × GK,α .
Let P be the law of the process (M, π ? ) ∼ µ, σ ∼ π, o ∼ M (σ), and define, for k ∈ [K], µkpr (πk0 ) = P(πk? = πk0 ),
and µkpo (πk0 ; σ, o) = P(πk? = πk0 |(σ, o)). For each k ∈ [K], the term corresponding to agent k in the second
term of (69) above can be rewritten as follows, using the definition of the posterior distribution µkpo (πk0 ; σ, o):




η
0
?
0
inf E(M,π? )∼µ Eσ∼π,o∼M (σ) Eπk ∼qk exp
· (gk (πk ; σ, o) − gk (πk ; σ, o)) − 1
gk ∈Gk,α
π(σ)








gk (πk? ; σ, o)
gk (πk0 ; σ, o)
0
= inf Eσ∼π Eo|σ Eπk ∼qk exp η ·
· Eπk? ∼µkpo (·;σ,o) exp −η ·
−1 .
gk ∈Gk,α
π(σ)
π(σ)
π(σ)
0
?
?
Given any gk ∈ Gk,ηα , we have that (πk0 , σ, o) 7→ π(σ)
η · gk (πk ; σ, o) and (πk , σ, o) 7→
η · gk (πk ; σ, o) both
belong to Gk,α , meaning that the above quantity is upper bounded by
h
i
inf Eσ∼π Eo|σ Eπk0 ∼qk [exp(gk (πk0 ; σ, o))] · Eπk? ∼µkpo (·;σ,o) [exp(−gk (πk? ; σ, o))] − 1 .
gk ∈Gk,ηα

This expression is equal to
Vk (π, q, µ) := Eσ∼π Eo|σ

inf

gk :Π0k →R,kgk k∞ ≤αη

n
o
Eπk0 ∼qk [exp(gk (πk0 ))] · Eπk? ∼µkpo (·;σ,o) [exp(−gk (πk? ))] − 1 .

By Lemma B.3, we have that for all π, q, µ,
Vk (π, q, µ) = − Eσ∼π Eo|σ

sup
gk :Π0k →R,kgk k∞ ≤αη

o
n
−Eπk0 ∼qk [exp(gk (πk0 ))] · Eπk? ∼µkpo (·;σ,o) [exp(−gk (πk? ))] + 1



1
· Eσ∼π Eo|σ DH2 µkpo (·; σ, o), qk + 4e−αη .
2
Combining (68), (69), and (70), we obtain the following upper bound:
(
"K
#
X
M
?
M
exoη (M ) ≤
sup
sup
inf E(M,π? )∼µ Eσ∼π
fk (U (πk , σ)) − fk (σ)
Q
≤−

q∈

K
0 µ∈∆(M×Π0 ) π∈Pε
k=1 ∆(Πk )

1
−
2η

K
X

Eσ∼π Eo|σ DH2


k=1

µkpo (·; σ, o), qk

k=1

85



4K −αη
+
·e
η

)
.

(70)

Since fkM ∈ [0, 1] for all k, M and DH2 (·, ·) ∈ [0, 2], it follows that we may replace the inf π∈Pε in the above
expression with inf π∈Π and pay an additive cost of Kε · (1 + 1/η), and so
(
"K
#
X
exoη (M ) ≤
sup
sup
inf E(M,π? )∼µ Eσ∼π
fkM (U (πk? , σ)) − fkM (σ)
Q
K
0 µ∈∆(M×Π0 ) π∈Π
k=1 ∆(Πk )

q∈

−

K
1 X

2η

k=1

k=1

)
 2 k
 4 −αη
Eσ∼π Eo|σ DH µpo (·; σ, o), qk + · e
+ Kε · (1 + 1/η) .
η

Since the above holds for any ε ∈ (0, 1) and α ≥ max{1, 1/η}, we may take the limits ε → 0, α → ∞ to get
(
"K
#
X
M
?
M
sup
inf E(M,π? )∼µ Eσ∼π
exoη (M ) ≤
sup
fk (U (πk , σ)) − fk (σ)
Q
K
0 µ∈∆(M×Π0 ) π∈Π
k=1 ∆(Πk )

q∈

−

K
1 X

2η

k=1


Eσ∼π Eo|σ DH2 µkpo (·; σ, o), qk

)


.

k=1

Next, for any choice of qk ∈ ∆(Π0k ), we have




Eσ∼π Eo|σ DH2 µkpo (·; σ, o), µkpr =Eσ∼π Eo|σ DH2 µkpo (·; σ, o), Eσ∼π Eo|σ [µpo (·; σ, o)]


≤4 · Eσ∼π Eo|σ DH2 µkpo (·; σ, o), qk ,
where the equality uses that, for πk0 ∈ Π0k , µkpr (πk0 ) = Eσ∼π Eo|σ [µpo (πk0 ; σ, o)] (by Bayes’ rule), and the
inequality uses Lemma B.4.
Hence, we have
(
exoη (M ) ≤

sup

inf

µ∈∆(M×Π0 ) π∈Π

E(M,π? )∼µ Eσ∼π

"K
X

#
fk (U (πk? , σ)) − fkM (σ)
M

k=1

)
K
 2 k

1 X
k
−
Eσ∼π Eo|σ DH µpo (·; σ, o), µpr
8η
k=1

=infr1/(8η) (M ),
as desired.
QK
Lemma F.5. For any fixed η > 0, q ∈ k=1 ∆(Π0k ), M ∈ M and π ? ∈ Π0 , the map (π, g) 7→ Γq,η (π, g; π ? , M )
is jointly convex with respect to (π, g) ∈ Π × G.
Proof of Lemma F.5. Fix any η, q, M, π ? as in the statement of the lemma. Recall the definition of
Γq,η (π, g; π ? , M ) in (55). Since convexity is preserved under summation, it suffices to show that, for each k,
the map from Π × Gk → R, given by
(π, gk ) 7→Eσ∼π [fkM (Uk (πk? , σ)) − fkM (σ)]




η
1
+ · Eσ∼π,o∼M (σ) Eπk0 ∼qk exp
· (gk (πk0 ; σ, o) − gk (πk? ; σ, o)) − 1
η
π(σ)
is convex. This follows directly from Lemma C.1 of Foster et al. (2022b).

F.5

Putting everything together: Proof of Theorem 5.1

The proof of Theorem 5.1 is a straightforward consequence of the lemmas proven previously in this section.
Proof of Theorem 5.1. Consider an instance M as in the statement of Theorem 5.1. By Lemma F.3 and
Lemma F.4, we have that, for any η > 0,
exoη (M ) ≤ infr1/(8η) (M ) ≤ K · r-deco1/(8η) (co(M )).

86

On the other hand, Lemma F.1 gives that for any η, δ > 0, Algorithm 1 run with the value η gives that with
probability at least 1 − δ,


2K
K · maxk |Π0k |
?
Risk(T ) = hM (b
π ) ≤ exoη (M ) +
· log
.
Tη
δ
Minimizing over η > 0 and substituting γ = 1/(8η) yields that there is a value of η for which Algorithm 1
yields risk upper bounded as



16γ
K · maxk |Π0k |
?
π ) ≤ K · inf r-decoγ (co(M )) +
Risk(T ) = hM (b
· log
,
γ>0
T
δ
which yields the claimed statement of Theorem 5.1.

G

Proofs for lower bounds from Section 5

G.1

Proof of Proposition 5.1

Proof of Proposition 5.1. Fix K ∈ N, and consider the K-player NE instance M of Example 1.1, where
Ak = {1, 2} for each k ∈ [K]. Certainly we have |Π0k | = |Ak | = 2 for all k. By Proposition A.1, we have
r-decoγ (M NE ) ≤ O(K/γ) for all γ > 0. Finally, Rubinstein (2016) implies that there is no algorithm which
?
M?
draws 2o(K) samples (each of which requires querying the true payoff function a 7→ (f1M (a), . . . , fK
(a)) once)
and outputs a c0 -approximate Nash equilibrium with probability at least 2/3, where c0 > 0 is a sufficiently
small universal constant; this yields the third claimed statement of Proposition 5.1.

G.2

Proof of Theorem 5.2

In this section, we prove Theorem 5.2. Before proving the result, we introduce some notation that will be
useful in the remainder of the section.

]
0
• For integers N ≥ N 0 ≥ 0, we let [N
N 0 denote the set of all subsets of [N ] = {1, 2, . . . , N } of size N .
• For positive integers n ≤ n0 let [n, n0 ] = {n, n + 1, . . . , n0 }.
• For sets X , Y, X t Y denotes the disjoint union of X and Y; it is formally defined as {(x, 0) : x ∈
X } ∪ {(y, 1) : y ∈ Y}.
• For finite sets X , Y, we let X Y denote the set of all functions φ : Y → X . Note that, in the case of
Y = [n] for some n ∈ N, the sets X n (which is the n-fold product of X ) and X [n] are in bijection. We
will at times slightly abuse notation by identifying these two sets.
• For a finite set X , let Unif(X ) denote the uniform distribution over X .
Proof of Theorem 5.2. Fix  > 0 and N ∈ N; by increasing the constant C0 in the statement of
the theorem, it is without loss of generality to assume that N is a multiple of 3. Set N1 = N/3 and
N2 = 2N/3 = N − N1 . Define
k = N,
which ensures that q k ≥

q=n=

2k
2N
=
,




]
and T2 := [N
N2 . We will now
e : T1 ∪ T2 → [q][n] t [q + 1, 2q][n+1,2n] so that Φ
e maps T1 to [q][n] and T2 to
define a random function Φ
[n+1,2n]
e
[q + 1, 2q]
. We will show that with positive probability, Φ satisfies certain conditions.
N
N1



for sufficiently large N . We write T1 :=

(71)
[N ]
N1



1. First, let Γ : T1 ∪ T2 → [q]k t [q]k denote a random function, defined as follows: Γ maps T1 to the
first copy of [q]k (uniformly at random), and T2 to the second copy of [q]k (uniformly at random). In
particular, for each S ∈ T1 ∪ T2 , Γ(S) are independent and chosen uniformly over their respective
copies of [q]k .
87

2. We next define a mapping Σ : [q]k t [q]k → [q][n] t [q + 1, 2q][n+1,2n] which maps the first copy of [q]k
into [q][n] and the second copy of [q]k into [q + 1, 2q][n+1,2n] according to the Reed-Solomon code of
Lemma G.6. (Here we have identified each of [q][n] and [q + 1, 2q][n+1,2n] with [q]n in the natural way.)
e = Σ ◦ Γ.
3. We then set Φ
e satisfies the following Conditions G.1
We next argue that there is some choice of Γ for which the resulting Φ
and G.2.
e ), Φ(T
e 0 )) ≥
Condition G.1. For each i ∈ {1, 2}, for all sets T , T 0 ∈ Ti with T 6= T 0 , it holds that dHam (Φ(T
q − k + 1.
√
Condition G.2. For any subset Q ⊂ [N ] with |Q| ≤ N ,


e )(a1 ) = a2 |Q ⊂ T ≤ 2/q
∀a1 ∈ [n], a2 ∈ [q], PT ∼Unif(T1 ) Φ(T
(72)


e )(a1 ) = a2 |Q ⊂ T ≤ 2/q.
∀a1 ∈ [n + 1, 2n], a2 ∈ [q + 1, 2q], PT ∼Unif(T2 ) Φ(T
(73)
To see that there exists such a choice for Γ, we make the following observations.
2
1. Since q k > 10 · NN1 whenever N is sufficiently large (by Eq. (71)), with probability at least 1 −

N 2 k
N1 /q > 9/10, the function Γ is injective. Conditioned on being injective, Lemma G.6 gives that
Condition G.1 holds, since the action of Σ on each of the copies of [q]k is defined to be that of a
Reed-Solomon code. Thus, Condition G.1 holds with probability at least 9/10 over the choice of Γ.
2. Consider any fixed choice of T ∈ T1 . Note that, for each coordinate a1 ∈ [n], the mapping T 7→
e )(a1 ) = Σ(Γ(T ))(a1 ), for T ∈ T1 , is distributed as a uniformly random function from T1 → [q]
Φ(T
(with respect to the randomness in Γ). This fact follows from the final sentence of Lemma G.6 and the
fact that Γ is a uniformly random function. Thus, by Lemma G.7 with N0 = N1 and a union bound
√
5N/6
2
N +1
over all n possible values of a1 , with
· 2−( N/6 )/(Cq ) over the choice of Γ, for
√ probability 1 − n · N
any subset Q ⊂ [N ] of size |Q| ≤ N , Eq. (72) holds. Similarly, an application of Lemma G.7 with
√
5N/6
2
N +1
N0 = N2 yields that with
· 2−( N/6 )/(Cq ) over the choice of Γ, for any subset
√ probability 1 − n · N
Q ⊂ [N ] of size |Q| ≤ N , Eq. (73) holds. Note that our choices of q, N,  ensure that, for the constant
C in Lemma G.7, as long as N is sufficiently large,


5N/6
3 log q ≤ 6 log(N/) ≤ 12 log(N ) ≤ N/6 − C ≤ log
− C,
N/6
meaning that it is valid to apply Lemma G.7. Finally, let us note that our choices for N, q ensure that
as long as N is sufficiently large,




√
5N/6
> Cq 2 · log(2n) + log(N N +1 ) + 5 ,
N/6
√
and therefore, (72) and (73) hold for all Q ⊂ [N ] with |Q| ≤ N , with probability at least 1 − 2−5 . In
particular, Condition G.2 holds with probability at least 1 − 2−5 over the random choice of Γ.
Summarizing the above points, with probability at least 1−1/10−2−5 > 0 over the choice of Γ, Conditions G.1
and G.2 both hold. We pick any such Γ for which both conditions hold, and set Φ = Σ ◦ Γ.
We are now ready to define the 2-player instance M = (M, Π, O, {Π0k }k , {Uk }k ).
Policy space. Let Π1 = {1, 2, . . . , 2n} and Π2 = {0, 1, . . . , 2q}, and write Π = Π1 × Π2 to denote the joint
policy space.
Deviation sets and switching functions. The deviation sets Π0k and switching function Uk are set as
in Definition 1.1 to make M a 2-player NE instance. To be concrete, we have Π0k = Πk for each k, and
Uk (πk0 , π) = (πk0 , π−k ).
88

Model class M. The class M is indexed by T1 ∪ T2 . Given a set T ∈ T1 ∪ T2 , we write the corresponding
model as MT . We will often consider the decomposition M = M1 t M2 , where M1 := {MT : T ∈ T1 } and
M2 := {MT : T ∈ T2 }. For each MT ∈ M, we need to specify the distributions o = (r1 , r2 , o◦ ) ∼ MT (π),
for each π ∈ Π. To do so, we first define a mapping B ? : T1 ∪ T2 → P([2n] × [2q]) ⊂ P(Π), as follows: recall
that Φ maps T1 to [q][n] and T2 to [q + 1, 2q][n+1,2n] . Then for T ∈ T1 ∪ T2 , define B ? (T ) ⊂ [2n] × [2q] ⊂ Π
by
(
{(i, Φ(T )(i)) : i ∈ [n]}
: MT ∈ M1
?
B (T ) =
{(i, Φ(T )(i)) : i ∈ [n + 1, 2n]} : MT ∈ M2 .
Note that here we view, for each set T in the domain of Φ, Φ(T ) as a function mapping either [n] → [q] (for
MT ∈ M1 ) or [n + 1, 2n] → [q + 1, 2q] (for MT ∈ M2 ).
We set the reward space to be R = [0, 1], and the pure observation space to be O◦ = [N ]. Now, for each
MT ∈ M and π ∈ Π, the full observation o = (r1 , r2 , o◦ ) ∼ MT (π) is drawn as follows:
• The pure observation o◦ ∈ O◦ is simply a uniformly random element of the set T .
• The rewards are deterministic, i.e., we have rk = fk T (π) for each k ∈ [K], a.s. Moreover, we define


: π ∈ Π1 × {0}
0
M
M
(74)
f1 T (π) = −f2 T (π) = 1
: π ∈ (Π1 × {1, 2, . . . , 2q})\B ? (T )


?
−δ : π ∈ B (T ),
M

where we set δ := 10−3 .
Establishing the claimed statements. It is immediate from definition of Π that |Π| = 2n · (2q + 1) =
O(N 2 /2 ), thus establishing the first claimed statement of the theorem. Next, Lemma G.1 below bounds
r-decoγ (co(M )), establishing the second claimed statement.
Lemma G.1. For any γ > 0, It holds that r-decoγ (co(M )) ≤ .
The proof of Lemma G.1 uses that Φ satisfies Condition G.1. Finally, the third claimed statement is established
by the following lemma.
Lemma
√ G.2. There is a constant C > 0 so that the following holds. For any algorithm that has at most
T ≤ N /C rounds of interaction, there is some model M ? ∈ M so that

?
? 
?
EM [Risk(T )] = EM hM (b
π ) > δ/100 = 10−5 .
Recall that above π
b denotes the output policy of the algorithm.
The proof of Lemma G.2 uses that Φ satisfies Condition G.2. It remains to prove Lemmas G.1 and G.2; we
do so in the remainder of this section.
Proof of Lemma G.1. For M ∈ co(M) and i ∈ [N ], let M [i] ∈ [0, 1] denote the probability P(r1 ,r2 ,o◦ )∼M (π) [o◦ =
i], for an arbitrary decision π ∈ Π (note that the choice of decision does not affect the distribution over the
pure observation o◦ ). For any M ∈ co(M), we define the set T (M ) ⊂ [N ] as follows:

T (M ) := i ∈ [N ] : M [i] ≥ 1/N ,
Now fix any M ∈ co(M). Define p? ∈ ∆(Π) as follows, as a function of M :
(
Unif({(1, 0), . . . , (n, 0)})
: |T (M )| ≥ N/2
?
p =
Unif({(n + 1, 0), . . . , (2n, 0)}) : |T (M )| < N/2.

89

We have that
r-decoγ (co(M ), M ) ≤

=

sup
M ∈co(M)

sup



Eπ∼p? hM (π) − γ · DH2 M (π), M (π)

Eπ∼p?

" 2
X

M ∈co(M)

k=1

#

fkM (πk0 , π−k ) − fkM (π) − γ · DH2 M (π), M (π)
max
0

πk ∈Πk


=

sup

E

π∼p?

M ∈co(M)

max f2 (π1 , π20 ) − f2M (π) − γ · DH2
π20 ∈Π2
M



M (π), M (π) ,

(75)

where the final equality follows because for all M ∈ co(M) and all π in the support of p? , maxπ10 ∈Π1 f1M (π10 , π2 ) =
0 = f1M (π).
Fix ν ∈ ∆(M) so that M = M ν (π) := EM ∼ν [M (π)] attains the supremum in Eq. (75). We consider the
following possibilities:
Case 1. Suppose first that |T (M )| ≥ N/2. We consider the following sub-cases:
1
1. First suppose that ν(M1 ) ≤ 1+δ
, where we recall that δ := 10−3 (Eq. (74)). Then for all π1 ∈ [n] and
π2 ∈ [2q], it holds that


1
1
f2M ν ((π1 , π2 )) ≤
·δ− 1−
= 0,
(76)
1+δ
1+δ
1
since f2MT ((π1 , π2 )) is only positive when π ∈ B ? (T ), which happens with probability at most 1+δ
under MT ∼ ν, as π1 ∈ [n]; moreover, when it is positive, it is δ, and when it is not positive, it is −1.
Using (76), since for all decisions π in the support of p? (which have π1 ∈ [n] in this sub-case), the
expression in (75) is bounded above by 0.

suppose that there
2. Next
P is some model M1T ∈ M1 so that ν(MT ) ≥ 14/15. Thus, we must have
P
M
[j]
≤
ν
j∈[N ]\T M ν [j] ≤ 15 . On the other hand, since |T (M )| ≥ N/2, we have
Pj∈T (M )\T
1
N
j∈T (M )\T M [j] ≥ N · 6 = 1/6. Thus, for any decision π ∈ Π,


DH2 M ν (π), M (π) ≥ (DTV M ν (π), M (π) )2 ≥ (1/6 − 1/15)2 = 1/100,
Thus, as long as γ ≥ 100, since f2M (π1 , π20 ) ≤ 1 for all π1 , π20 , if we recall that M = M ν is chosen to
maximize the expression in (75), we have that this expression is bounded above by 0.
1
3. In the remaining case, we must have ν(M1 ) ≥ 1+δ
, yet for each MT ∈ M1 , ν(MT ) < 14/15. Suppose
for the purpose of contradiction that




Mν
Mν
Mν
0
0
Eπ∼p? max
f2 (π1 , π2 ) − f2 (π) = Eπ∼p? max
(77)
f2 (π1 , π2 ) > .
0
0
π2 ∈Π2

π2 ∈Π2

Write I = {π1 ∈ [n] : maxπ20 ∈Π2 f2M ν (π1 , π20 ) ≥ 0}; since maxπ20 ∈Π2 f2M ν (π1 , π20 ) ≤ 1 for all π1 , (77) tells
us that |I| ≥ n. By construction, for each π1 ∈ I, there is at most one value of π2 ∈ [q] so that
f2M ν (π1 , π2 ) ≥ 0; let this value of π2 be denoted by π2? (π1 ), if such π2 exists given π1 , and otherwise set
π2? (π1 ) = −1.
Note that if f2M ν (π1 , π2 ) ≥ 0 for any π1 ∈ Π1 , then we must have that ν({MT ∈ M1 : Φ(T )(π1 ) =
1
π2 }) ≥ 1+δ
> 1 − δ. Therefore, for all π1 , if π2? (π1 ) > 0, then
ν({MT ∈ M1 : Φ(T )(π1 ) = π2? (π1 )}) > 1 − δ.
For each MT ∈ M1 , define
ζ(T ) := |{π1 ∈ I : Φ(T )(π1 ) 6= π2? (π1 )}| .
90

(78)

We have that
|I| −

X
MT ∈M1

ν(MT )ζ(T ) =

X

X

ν(MT ) · I {Φ(T )(π1 ) = π2? (π1 )} ≥ |I| · (1 − δ),

π1 ∈I MT ∈M1

0
where the inequality√uses (78). Thus, by Markov’s inequality,
√ for some subset M1 ⊂ M1 , it holds
0
0
that ν(M1 \M1 ) ≤
δ. Since ν(M1 ) ≥ 1 − δ, it follows that
1 , ζ(T ) ≤ |I| ·
√ all MT ∈ M√
√ δ and for
ν(M01 ) ≥ 1 − δ − δ ≥ 1 − 2 δ. Since 1 − 2 δ > 14/15 by our choice of δ = 10−3 , there must be at
least two distinct elements of M01 , which we denote by MT1 and MT2 .

To proceed, by definition of M01 , it holds that

√
|{π1 ∈ I : Φ(T1 )(π1 ) = Φ(T2 )(π1 ) = π2? (π1 )}| ≥ |I| · (1 − 2 δ) ≥ |I|/2 ≥ n/2.

It follows that dHam (Φ(T1 ), Φ(T2 )) ≤ n − n/2 = n(1 − /2), which contradicts Condition G.1, since
2N
n(1 − /2) = 2N
 · (1 − /2) <  − N + 1 = q − k + 1. Thus, (77) is false, and therefore the expression
in (75) corresponding to choosing M = M ν is bounded above by .
Case 2. Now suppose that |T (M )| < N/2. In this case an argument symmetric to that in the case that
|T (M )| ≥ N/2 may be applied to establish the same upper bound on the multi-agent DEC. (In particular, the
roles of M1 , M2 are swapped; the symmetry arises from the fact that sets in T1 have size N/3 = N/2 − N/6
whereas sets in T2 have size 2N/3 = N/2 + N/6.) Below we expand on the details for completeness.
1
1. If ν(M2 ) ≤ 1+δ
, then for all π1 ∈ [n + 1, 2n] and π2 ∈ [2q], f2M ν ((π1 , π2 )) ≤ 0, meaning that, since for
all decisions π in the support of p? , π2 ∈ [n + 1, 2n], the expression in (75) is non-positive.
P
2. Next suppose there is some model MT ∈ M2 so that ν(MT ) ≥ 14/15. We must have that j∈T \T (M ) M [j] ≤
|T \T (M )| · N1 . On the other hand, since for each i ∈ T we have MT [i] = 3/(2N ), we have
P
14
7
3
j∈T \T (M ) M ν [j] ≥ 15 · 2N · |T \T (M )| ≥ 5N · |T \T (M )|. Thus, for any π ∈ Π, since |T (M )| ≤ N/2
and |T | = 2N/3 (as MT ∈ M2 ),


2

2
7
1
DH2 M ν (π), M (π) ≥ DTV M ν (π), M (π)
≥ |T \T (M )| ·
−
5N
N


2
7
1
N
1
·
−
.
≥
=
6
5N
N
225

Thus, as long as γ ≥ 225, since f2M (π1 , π20 ) ≤ 1 for all π1 , π20 , the expression in (75) for M = M ν is
bounded above by 0.
1
, yet for each MT ∈ M2 , ν(MT ) < 14/15. In this
3. In the remaining case, we must have ν(M2 ) ≥ 1+δ
case, the expression in (75) for M = M ν is bounded above by , via an argument identical to the one in
Item 3 above where one replaces all intances of M1 with M2 .

Summarizing, we have shown that (75) is bounded above by  for an arbitrary choice of M , which completes
the proof of the lemma.
√
Proof of Lemma G.2. Fix any T ≤ N /C (for a constant C to be specified below), and consider any
algorithm (p, q) = {(q t (·|·), p(·|·)}Tt=1 . Recall that, for any model M , HT denotes the history of interaction
between the algorithm (p, q) and the model M , and is defined by HT = (π 1 , o1 ), . . . , (π T , oT ). HT is associated
with the measure space (ΩT , F T ). For each model M ∈ M, we use the abbreviate PM ≡ PM,(p,q) as the law of
HT , and write EM for the corresponding expectation. We will show the stronger statement that the algorithm
(p, q) has large risk for a uniformly random model M ? ∈ M; in particular,

? 
?
EM ? ∼Unif(M) EM hM (b
π ) > δ/100.
(79)
Clearly (79) implies the statement of Lemma G.2.
In order to prove Lemma G.2, we first prove a few intermediate results. To start, we define an additional
model M0 : the distribution of (r1 , r2 , o◦ ) ∼ M0 (π) are as follows:
91

• The rewards r1 , r2 are given as in (74) with B ? = ∅; in particular, rk = fkM0 (π) are deterministic with
(
0 : π ∈ Π1 × {0}
M0
M0
f1 (π) = −f2 (π) =
1 : π ∈ Π2 × {1, 2, . . . , 2q}.
• The pure observation o◦ ∈ [N ] is a uniformly random element of [N ].
M
0
M
0 ∈Π0 f
Next, recall that we write, for π ∈ Π, k ∈ {1, 2}, M ∈ M, hM
k (π) = maxπk
k (Uk (πk , π)) − fk (π).
k

Lemma G.3 below shows that for each i ∈ {1, 2}, under the model M0 , with constant probability either all
models in M1 or all models in M2 have high risk with respect to the algorithm’s output policy π
b.
Lemma G.3. There is some i ∈ {1, 2} (depending on the algorithm (p, q)) so that
PM0 (∀M ∈ Mi : hM (b
π ) ≥ δ) ≥

1
.
2

The proof of Lemma G.3 is provided at the end of this section. Since M0 is not in M, Lemma G.3 is not
enough to prove Lemma G.2; we will next use a series of change-of-measure arguments to reason about the
history of interaction when the true model is a uniformly random model in M . In particular, for each model
MT ∈ M, we define an intermediate model MT ,0 : the distribution of (r1 , r2 , o◦ ) ∼ MT ,0 (π) is as follows:
• The rewards (r1 , r2 ) are given identically to the rewards under M0 (π) (in particular, they are deterministic).
• The pure observation o◦ is a uniformly random element of T .
Lemma G.4 below shows that under a history drawn from MT ,0 for a uniformly random T ∼ Unif(Ti ),
with high probability the algorithm will not query any decision belonging to B ? (T ) ⊂ Π; furthermore, the
distribution of the history HT is close under M0 and under MT ,0 , again for a uniformly random T ∼ Unif(Ti ):
Lemma G.4. For each i ∈ {1, 2}, the following holds:
ET ∼Unif(Ti ) EMT ,0 [I {{π 1 , . . . , π T } ∩ B ? (T ) 6= ∅}] ≤

2T
1
+
.
q
100

(80)

1
.
100

(81)

Furthermore, for any measurable subset F ∈ F T of histories,
EM0 [I {HT ∈ F}] − ET ∼Unif(Ti ) EMT ,0 [I {HT ∈ F}] ≤
The proof of Lemma G.4 is provided at the end of this section.
Next, Lemma G.5 shows that if, for some model MT , the algorithm does not query any decision in B ? (T )
with high probability, then the distribution of histories under PMT ,0 and PMT are close.
Lemma G.5. Fix some model MT ∈ M so that PMT ,0 ({π 1 , . . . , π T } ∩ B ? (MT ) 6= ∅) ≤ η for some η > 0.
Then DTV (PMT ,0 , PMT ) ≤ η.
The proof of Lemma G.5 is provided at the end of this section. Given the above lemmas, we now establish
(79). Suppose for the purpose of contradiction that EM ∼Unif(M) EM [hM (b
π )] ≤ δ/100. Then by Markov’s
inequality, EM ∼Unif(M) EM [I {hM (b
π ) ≥ δ}] ≤ 1/100. Since Unif(M) is the uniform average of Unif(M1 ) and
Unif(M2 ), it follows that for each i ∈ {1, 2},
EM ∼Unif(Mi ) EM [I {hM (b
π ) ≥ δ}] ≤ 1/50.
We next note that Lemma G.3 gives that for some i? ∈ {1, 2},
PM0 (∀M ∈ Mi? : hM (b
π ) ≥ δ) ≥
92

1
.
2

(82)

By the conclusion (81) of Lemma G.4, it follows that
ET ∼Unif(Ti? ) EMT ,0 [I{∀M ∈ Mi? : hM (b
π ) ≥ δ}] ≥ 1/2 − 1/100.
Next, by the statement (80) of Lemma G.4 and using that 2T ≤
sufficiently large N ,

√

N and

√

(83)

√
N /q ≤ 1/ N ≤ 1/100 for

√
MT ,0

ET ∼Unif(Ti? ) E

1

?

T

[I {{π , . . . , π } ∩ B (MT ) 6= ∅}] ≤

N
1
1
+
≤
.
q
100
50

(84)

Now, for η = 1/7, let us write χ(T ) := I {EMT ,0 [I{{π 1 , . . . , π T } ∩ B ? (MT ) 6= ∅}] > η} ∈ {0, 1}; Eq. (84)
together with Markov’s inequality give that ET ∼Unif(Ti? ) [χ(T )] ≤ 1/7.
Next, Lemma G.5 gives that, for all T ∈ T1 ∪ T2 ,
PMT (∀M ∈ Mi? : hM (b
π ) ≥ δ) ≥ PMT ,0 (∀M ∈ Mi? : hM (b
π ) ≥ δ) − χ(T ) − η,
and taking expectation over T ∼ Unif(Ti? ) and using that ET ∼Unif(Ti? ) [χ(T )] ≤ 1/7 and the choice of
η = 1/7 gives that
PT ∼Unif(Ti? ) EMT [I {∀M ∈ Mi? : hM (b
π ) ≥ δ}]
≥PT ∼Unif(Ti? ) EMT ,0 [I {∀M ∈ Mi? : hM (b
π ) ≥ δ}] − 2/7 ≥ 1/2 − 1/100 − 2/7,
where the final inequality follows by Eq. (83). In particular, using that MT ∈ Mi? if T ∈ Ti? , we have
ET ∼Unif(Ti? ) EMT [I {hMT (b
π ) ≥ δ}] ≥ 1/2 − 1/100 − 2/7 > 1/5,
which contradicts Eq. (82), thus completing the proof.
e := Π1 × (Π2 \ {0}) = Π1 × {1, 2, . . . , 2q} ⊂ Π. First, we claim that for all
Proof of Lemma G.3. We write Π
M
e
π ∈ Π, and all M ∈ M, it holds that hM (π) = hM
1 (π) + h2 (π) ≥ δ. To see this, consider any M = MT ∈ M,
and we consider the following two cases:
• If π 6∈ B ? (T ), then f2M ((π1 , 0)) − f2M (π) = 0 − (−1) = 1.
• If π ∈ B ? (T ), then there must be some π10 ∈ Π1 with (π10 , π2 ) 6∈ B ? (T ), and so f1M ((π10 , π2 )) − f1M (π) =
1 − (−δ) = 1 + δ.
Next, note that
n



o
e ∪ ([n] × {0}) , PM0 π
e ∪ ([n + 1, 2n] × {0})
max PM0 π
b∈Π
b∈Π
≥ 1/2.


e ∪ ([n] × {0}) ≥ 1/2. Note that if M = MT ∈ M1 and π ∈ [n]×{0}, then
Let us first suppose that PM0 π
b∈Π

M
M
hM (π) ≥ hM
2 (π) = f2 ((π1 , Φ(T )(π1 ))) − f2 (π) = δ − 0 = δ. Moreover, the two bullet points above establish
M
e
that if π
b ∈ Π, then h (b
π ) ≥ 1 > δ. Thus, in this case, we have established that PM0 (∀M ∈ M1 , hM (b
π ) ≥ δ) ≥
1/2.


e ∪ ([n + 1, 2n] × {0}) ≥ 1/2, it follows inb a symmetric manner that,
In the other case, where PM0 π
b∈Π

PM0 (∀M ∈ M2 , hM (b
π ) ≥ δ) ≥ 1/2.
S
Proof of Lemma G.4. Fix any i ∈ {1, 2}. For a model M ∈ {M0 } ∪ T ∈Ti {MT ,0 }, consider a
draw of HT = (π 1 , (r11 , r21 , o1◦ ), . . . , π T , (r1T , r2T , oT◦ )) ∼ PM , where we have written out the full observations
ot = (r1t , r2t , ot◦ ). Since the distribution of the pure observations ot◦ ∼ M (π) does not depend on the policy
π, the distribution of HT is identical to the following one: first, o1◦ , . . . , oT◦ are drawn i.i.d. from M (π0 ) (for
an arbitrary decision π0 ), and then the decisions π t are chosen adaptively, π t ∼ q t (·|Ht−1 ), with the rewards
r1t , r2t being determined by π t .
93

For any T ∈ Ti , and for any t, t0 ∈ [T ] with t 6= t0 , we have PMT ,0 [ot◦ = ot◦ ] = 1/|T | ≤ 3/N . Thus
0
PMT ,0 ∃t 6= t0 : ot◦ = ot◦ ≤ T 2 · 3/N ≤ 1/100,
0

(85)

p
where the final inequality follows since T ≤ N/300 (as long as the constant C in the statement of Lemma G.2
0
is sufficiently large). Let E ∈ F T denote the event that for all t 6= t0 , ot◦ 6= ot◦ . The inequality (85) gives that
ET ∼Unif(Ti ) EMT ,0 [I {E}] ≥ 1 − 1/100.

(86)

In a similar manner, we also have that
EM0 [I {E}] ≥ 1 − 1/100.

(87)

Now, we may compute
ET ∼Unif(Ti ) EMT ,0 [I {π t ∈ B ? (T )} | E]
X 1
X
MT ,0
[I {π t ∈ B ? (T )} | o1:T
=
PMT ,0 (o1:T
◦ = ω1:T | E) · E
◦ = ω1:T ]
|Ti |
T ∈Ti

=

X
T ∈Ti

ω1 ,...,ωT ∈[N ]

1
|Ti |

X
ω1 ,...,ωT ∈T
distinct

X

=

ω1 ,...,ωT ∈[N ]
distinct

1
· EM0 [I {π t ∈ B ? (T )} | o1:T
◦ = ω1:T ]
Ni (Ni − 1) · · · (Ni − T + 1)

1
ET ∼Unif(Ti ) EM0 [I {π t ∈ B ? (T )} | o1:T
◦ = ω1:T , {ω1 , . . . , ωT } ⊂ T ]
N (N − 1) · · · (N − T + 1)
(88)

≤2/q,
where:
• The second equality uses that the distribution of HT conditioned on o1:T is identical under PM0 and
PMT ,0 .

N −1
Ni !
• The third equality switches the order of summation and uses that 1/|Ti | = N
= N (N −1)···(N
−Ni +1) ,
i
as well as the fact that the number of sets T containing any tuple ω1 , . . . , ωT ∈ [N ] of distinct integers
−1)···(N −Ni +1)
is (N −T )(N −T
.
(Ni −T )!
• The final inequality uses the fact that, for fixed ω1 , . . . , ωT , the distribution of T ∼ Unif(Ti )|{ω1 , . . . , ωT } ⊂
?
T is independent of the distribution of HT ∼ PM0 |o1:T
◦ = ω1:T . Moreover, the definition of B (T ) in
terms of Φ(T ) and the fact Φ satisfies Condition G.2 means that, for any fixed π = (π1 , π2 ) ∈ Π with
π1 > 0,
PT ∼Unif(Ti ) (π ∈ B ? (T )|{ω1 , . . . , ωT } ⊂ T ) = PT ∼Unif(Ti ) (Φ(T )(π1 ) = π2 |{ω1 , . . . , ωT } ⊂ T ) ≤ 2/q,
where√ we take Φ(T )(π1 ) = −1 if π1 is not in the domain of Φ(T ). (Here we have also used that
T ≤ N .) In particular, the above inequality holds with the random choice of π t ∼ PM0 |o1:T
◦ = ω1:T
replacing π.
Taking a union bound over all T values of t ∈ [T ] and applying (86), the first claim (80) of the lemma follows.
To show the second claim (81) of the lemma, we note that for any fixed subset F ∈ F T (not depending on
T ) the chain of equalities ending in (88) implies that
ET ∼Unif(Ti ) EMT ,0 [I {HT ∈ F} | E] = EM0 [I {HT ∈ F} | E],
Eq. (81) follows from the above equality combined with (86) and (87).

94

Proof of Lemma G.5. Let E denote the event that {π 1 , . . . , π T } ∩ B ? (MT ) = ∅. Consider any subset
F ⊂ F T of histories. Then
PMT ,0 (E ∩ F) = PMT (E ∩ F),
which follows since for any decition π 6∈ B ? (MT ), the distribution over the full observation o ∼ MT (π) and
o ∼ MT ,0 (π) is identical. The statement of the lemma then follows from Lemma B.6.

G.3

Supplementary lemmas

The following lemma, which is an elementary fact from coding theory, states the dimension and distance
properties of the Reed-Solomon code. To present it, we recall the definition of Hamming distance: for q, n ∈ N,
and w, w0 ∈ [q]n , we let dHam (w, w0 ) = |{i ∈ [n] : wi =
6 wi0 }| to be the number of positions at which w, w0
differ.
Lemma G.6 (Reed-Solomon code; Section 5.2 of Guruswami et al. (2022)). Fix any integers n, q, k satisfying
q ≥ k. Then there is a mapping Φ : [q]k → [q]n so that for any two vectors v, v 0 ∈ [q]k with v =
6 v 0 , it holds
0
that dHam (Φ(v), Φ(v )) ≥ n − k + 1.
Furthermore, Φ may be chosen so that if X ∈ [q]k is uniformly random, then for each i ∈ [n], the value
Φ(X)i ∈ [q] is uniformly random.
Lemma G.7 below shows that a certain type of randomness extractor exists.
Lemma G.7. There is a sufficiently large constant C ≥ 1 so that the following holds.
positive
 Consider any

[N ]
integers N, N0 , R, q with N0 ≤ 2N/3, R ≤ N/6 ≤ N0 − N/6, and 3 log q ≤ log 5N/6
−
C.
Let
Ψ
:
N0 → [q]
N/6
5N/6
2
be a uniformly random function. Then with probability at least 1 − N R+1 · 2−( N/6 )/(Cq ) over the choice of Ψ,
for all subsets Q ⊂ [N ] of size |Q| ≤ R, and all j ∈ [q],

PT ∼Unif ([N ]) (Ψ(T ) = j|Q ⊂ T ) ≤
N0

2
.
q


]
We clarify that the distribution of the uniformly random function Ψ : [N
N0 → [q] in the above lemma

]
statement is given as follows: for each S ∈ [N
N0 , Ψ(S) is an independent random variable, distributed
uniformly on [q].
Proof of Lemma G.7. Since R ≤ N/6 ≤ N0 − N/6 and N0 ≤ 2N/3, for any subset Q ⊂ [N ] of size


5N/6
]
|Q| ≤ R, the distribution of T ∼ Unif [N
N0 |Q ⊂ T puts mass at most 1/ N/6 on any subset T (such a

distribution is known as a flat k-source for some k ≥ log 5N/6
). By Vadhan (2012, Proposition 6.12) with
N/6

ε = 1/q, for a sufficiently large constant C, as long as 3 log q ≤ log 5N/6
− C, with probability at least
N/6
2
−(5N/6
/(Cq
)
1−2 N/6 )
over the choice of Ψ, it holds that, for any fixed Q of size at most R, the distribution of Ψ(T ),

]
with T ∼ Unif [N
N0 |Q ⊂ T , is 1/q-close (in total variation distance) to uniform on [q], which in particular

]
implies that Ψ(T ) = j with probability at most 2/q for any j ∈ [q] (again under T ∼ Unif [N
N0 |Q ⊂ T ).

PR
R+1
Taking a union bound over all r=0 N
possible sets Q, we obtain that Ψ satisfies the desired
R ≤ N
2
−(5N/6
R+1
property with probability at least 1 − N
· 2 N/6 )/(Cq ) .

95

