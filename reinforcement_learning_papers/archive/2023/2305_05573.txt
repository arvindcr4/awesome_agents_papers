An Algorithm for Adversary Aware Decentralized Networked
MARL
Soumajyoti Sarkar 1

arXiv:2305.05573v2 [cs.LG] 15 Jun 2023

California, United States
sarkar.soumajyoti@gmail.com

ABSTRACT
Decentralized multi-agent reinforcement learning (MARL) algorithms
have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to
canonical multi-agent Markov Decision Process (MDP) settings which
assume common reward functions over all agents. In this work, we
follow the existing work on collaborative MARL where agents in a
connected time varying network can exchange information among
each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing decentralized MARL algorithms where some agents can deviate from their usual consensus
update, who we term as adversarial agents. We then proceed to
provide an algorithm that allows non-adversarial agents to reach
a consensus in the presence of adversaries under a constrained setting.
ACM Reference Format:
Soumajyoti Sarkar 2 . 2023. An Algorithm for Adversary Aware Decentralized Networked MARL. In Proceedings of ACM Conference (Conferenceâ€™17).
ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

Multi-agent reinforcement learning constitutes a reinforcement learning scenario where multiple agents participate to jointly learn an
optimal policy and where optimality is deï¬ned in terms of some
objective [2]. In a collaborative setting, the agents have to work together with a goal to optimize a shared reward metric [12]. Multiagent environments are inherently non-stationary since the other
agents are free to change their behavior as they also learn and
adapt and this makes designing algorithms for MARL more complex that single agent systems. Our model diï¬€ers from the traditional collaborative and fully co-operative multi-agent systems
in that the agents in our system are heterogeneous and can have
diï¬€erent local reward functions. We work on sequential decisionmaking problems in which the agents repeatedly interact with their
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
2 This work has been done while the author was at Arizona State University and
prior to joining Amazon

environment and try to jointly optimize the long-term reward they
receive from the system, which depends on a sequence of decisions
made by them and the signals shared by other agents in the network.
Existing value-factorized based MARL approaches perform well
in various multi-agent cooperative environments under the centralized training and decentralized execution (CTDE) scheme, where
all agents in a team are trained together by the centralized value
network and each agent executes its policy independently [14, 15].
One of the existing issues in such systems is that in the centralized
training process, the environment for the team is partially observable and non-stationary. The observation and action information
of all the agents cannot represent the global states. What follows
is that the existing methods perform poorly and are highly sample
ineï¬ƒcient. In these regimes, regret minimization is a promising
approach as it performs well in partially observable and fully competitive settings. However, it tends to model others as opponents
and thus cannot work well under the CTDE scheme. In our work,
we set out to respect two constraints: ï¬rst, we want the agents to
be fully decentralized even in their training and second, we follow
the model free actor critic type algorithms for our collaborative
MARL setup as opposed to Q-learning type algorithms or centralized training with model based approaches [28].
The major diï¬€erence in the kind of algorithms we follow in this
paper with that of CTDE techniques is that our work follows the
studies on consensus based distributed optimization [19] where
the agents exchange parameters instead of their actions and states
with other agents [14]. There is no centralized training and each
agent performs the critic step independently. These consensus based
systems have been popular in many ï¬elds like sensor networks, social learning, co-ordination of vehicles and co-ordinated synchronous distributed optimization to name a few. However, most of
these studies evade the risk of adversaries or agents that deviate
from consensus updates. For example, in studies where humans
participate in decision making and social learning over time [4, 23],
presence of adversarial nodes could disrupt the learning dynamics. The study in [8] showed that a single adversarial agent can
persuade all the other agents in the network to implement policies that optimize an objective that it desires. It then becomes important to answer whether we can modify the existing consensus
based decentralized MARL algorithms to respond to the presence
of malicious agents who deviate from the consensus rule updates
or whether the non-malicious agents can still converge to the optimal solution in the presence of adversaries.
We study the setup where the rewards or the data for the agents
are not corrupted but instead where the adversarial agents do not
follow the consensus updates to obtain a joint optimal policy. We

consider the scenario where there could exist more than a single
adversarial agent in our setup. Section 2 provides the technical preliminaries of our networked MARL setup and Section 3 discusses
the adversarial settings and the adversary aware consensus MARL
algorithm.

2

DECENTRALIZED NETWORKED MARL

Our work is heavily inï¬‚uenced by the research on decentralized
multi-agent reinforcement learning done in [31]. The authors develop a consensus based reinforcement learning algorithm in a
fully decentralized and networked setting. We primarily extend
their work in a networked setting where malicious agents are present
under a constrained setup. Our networked multiagent MDP is constructed as follows: we have a state space S shared by all agents
N in a network, such that Ağ‘– , ğ‘– âˆˆ |N | is the action of agent ğ‘–. Let
Ãğ‘
A = ğ‘–=1
Ağ‘– be the joint action space of all agents. Following
ğ‘–
this, R : S Ã— A â†’ R is the local reward function of agent ğ‘– and
ğ‘ƒ : S Ã— A Ã— S â†’ [0, 1] is the state transition probability of the
MDP. We assume that the states are observable globally in order to
ensure co-operation among agents, but the rewards are observed
locally as a function of the neighbors of the agent. At time ğ‘¡, an
agent ğ‘– chooses an action ğ‘ğ‘–ğ‘¡ given state ğ‘ ğ‘¡ , following its own local
policy ğœ‹ ğ‘– : S Ã— Ağ‘– â†’ [0, 1], where ğœ‹ ğ‘– (ğ‘ , ğ‘ğ‘– ) represents the probability of choosing action ğ‘ğ‘– at state ğ‘ . In this setup, the joint policy
Ã
of all agents is given by ğœ‹ (ğ‘ , ğ‘) = ğ‘– âˆˆ N ğœ‹ ğ‘– (ğ‘ , ğ‘ğ‘– ).
For an agent ğ‘–, the local policy is parameterized by ğœ‹ğœƒğ‘– where ğœƒ ğ‘– âˆˆ
Ãğ‘ ğ‘–
ğ‘–
Î˜ are the parameters and the joint policy is given by Î˜ = ğ‘–=1
Î˜.
As is standard in the assumptions of actor-critic algorithms with
function approximation, the policy function ğœ‹ğœƒğ‘– (ğ‘ , ğ‘ğ‘– ) > 0 for any
ğœƒ ğ‘– âˆˆ Î˜ğ‘– . We assume that ğœ‹ğœƒğ‘– ğ‘– (ğ‘ , ğ‘ğ‘– ) is continuously diï¬€erentiable
with respect to the parameter ğœƒ ğ‘– over Î˜ğ‘– . For all ğœƒ âˆˆ Î˜, the transition matrix of the Markov chain {ğ‘ ğ‘¡ }ğ‘¡ â‰¥0 induced by policy ğœ‹ğœƒ , for
any ğ‘ , ğ‘  â€² âˆˆ S is given by
ğ‘ƒ ğœƒ (ğ‘  â€² |ğ‘ ) =

Ã•

(1)

It comes along with the assumption that {ğ‘ ğ‘¡ }ğ‘¡ â‰¥0 is irreducible and
aperiodic under any ğœ‹ğœƒ , and the chain of the state-action pair {(ğ‘ ğ‘¡ , ğ‘ğ‘¡ )}
has a stationary distribution ğ‘‘ğœƒ (ğ‘ ) Â· ğœ‹ğœƒ (ğ‘ , ğ‘) for all pairs.
Objective of the MARL: The collective objective of the agents
is to collaborate and ï¬nd a policy ğœ‹ğœƒ such that it maximizes the
average global long term rewards while only utilizing information
ğ‘–
that is local to the agents in the network. In that context, let ğ‘Ÿ ğ‘¡+1
denote the reward received by agent ğ‘– at time ğ‘¡. Then the goal of
all agents collectively is to optimize the following objectives:
ğ‘‡ âˆ’1
1 Ã• 1 Ã• ğ‘– 
E
ğ‘Ÿ ğ‘¡+1
ğ‘‡
ğ‘
ğ‘¡=0
ğ‘–âˆˆN
Ã•
ğ‘‘ğœƒ (ğ‘ )ğœ‹ğœƒ (ğ‘ , ğ‘) Â· ğ‘…(ğ‘ , ğ‘).

ğ‘šğ‘ğ‘¥ğœƒ ğ½ (ğœƒ) = ğ‘™ğ‘–ğ‘šğ‘‡

(2)

ğ‘  âˆˆ S,ğ‘âˆˆ A

where ğ‘…(ğ‘ , ğ‘) = ğ‘1 Â· ğ‘– âˆˆ N ğ‘…ğ‘– (ğ‘ , ğ‘) is the globally averaged reward
function. Following this, we have ğ‘…(ğ‘ , ğ‘) = E[ğ‘Ÿ ğ‘¡+1 |ğ‘ ğ‘¡ = ğ‘ , ğ‘ğ‘¡ =
Ã
ğ‘] where ğ‘Ÿ ğ‘¡ = ğ‘1 ğ‘– âˆˆ N ğ‘¡ğ‘¡ğ‘– . In that context of the symbols, the
Ã

Policy Gradient with MARL: To develop an algorithm for MARL,
we would apply the policy gradient theorem as mentioned in Theorem 3.1 in [31]. For any ğœƒ âˆˆ Î˜, let ğœ‹ğœƒ be a policy and ğ½ (ğœƒ) denote
the globally averaged return, we deï¬ne the local advantage function ğ´ğ‘–ğœƒ : S Ã— A â†’ R as ğ´ğ‘–ğœƒ (ğ‘ , ğ‘) = Qğœƒ (ğ‘ , ğ‘) âˆ’ ğ‘‰Ëœğœƒğ‘– (ğ‘ , ğ‘ âˆ’1 ), where Qğœƒ
and ğ´ğœƒ are the corresponding global action-value and advantage
functions and ğ‘‰Ëœğœƒğ‘– (ğ‘ , ğ‘ âˆ’1 ) is the local value function. The gradient
of the policy is given by

âˆ‡ğœƒ ğ‘– ğ½ (ğœƒ) = Eğ‘ âˆ¼ğ‘‘ğœƒ ,ğ‘âˆ¼ğœ‹ğœƒ [âˆ‡ğœƒ ğ‘– ğ‘™ğ‘œğ‘”ğœ‹ğœƒğ‘– ğ‘– (ğ‘ , ğ‘ğ‘– ) Â· ğ´ğ‘–ğœƒ (ğ‘ , ğ‘)]

(3)

3 ADVERSARY AWARE MARL ALGORITHM
3.1 Non Adversarial Setting
We ï¬rst present the existing work under a non-adversarial setting. Here, he actor-critic based consensus algorithm for the MARL
setup is as follows: we consider an agent speciï¬c local version of
the global action-value function ğ‘„ğœƒ which we denote as Q (ğœ” ğ‘– )
where we hide the state, action factors in the function which is
implicit and ğœ” âˆˆ Rğ‘ˆ , where the dimension ğ‘ˆ is signiï¬cantly less
than the joint state action space. In order to use the policy gradient
theorem discussed in the previous section to improve an agentsâ€™
policy, each agent shares its local parameters ğœ” ğ‘– with its neighbors on the network in order to reach an estimate of Qğœƒ that is
consensual among all agents in the network. Such distributed consensus algorithms have been proposed earlier [19] that guarantee
convergence of the local agent functions.

3.2 Adversary Aware Decentralized MARL
ğœ‹ğœƒ (ğ‘ , ğ‘) Â· ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘)

ğ‘âˆˆ A

=

global state value and action value functions can be denoted by
Ã
Q (ğ‘ , ğ‘) = ğ‘¡ E[ğ‘Ÿ ğ‘¡+1 âˆ’ ğ½ (ğœƒ) | ğ‘  0 = ğ‘ , ğ‘ 0 = ğ‘, ğœ‹ğœƒ ] and Vğœƒ (ğ‘ ) =
Ãğœƒ
ğ‘âˆˆ A ğœ‹ğœƒ (ğ‘ , ğ‘)Qğœƒ (ğ‘ , ğ‘).

In order to introduce the setup behind the MARL environment
where malicious agents are a part of the network, we ï¬rst deï¬ne
certain notations based on the setup of consensus based adversarial attacks in [27]. We consider an undirected network N =
ğ‘‰ (ğ‘ ), ğ¸ (ğ‘ ), where ğ‘‰ (ğ‘ ) = {ğ‘£1, 1 . . . , ğ‘£ğ‘› } and ğ¸ (ğ‘ ) denotes the
edges connecting pairs of nodes. Note that ğ‘£ğ‘– is just an exaggerated
notation to denote an agent in a graph as opposed to ğ‘– that we have
used in the previous sections but they refer to the same agent. Denoting Kğ‘– as the neighborhood vertices of ğ‘– âˆˆ ğ‘‰ (ğ‘ ), for any ğ‘Ÿ âˆˆ N,
a subset ğ‘† âŠ‚ ğ‘‰ (ğ‘ ) is said to be r-local, if |Kğ‘– âˆ©ğ‘† | â‰¤ ğ‘Ÿ âˆ€ğ‘£ğ‘– âˆˆ ğ‘‰ (ğ‘ )\ğ‘†.
That is to say, for r-local subset, there are at most ğ‘Ÿ nodes in the
neighborhood of any vertex from ğ‘‰ (ğ‘ ) \ ğ‘†. For the setup of adversarial attacks in the multi-agent system, we consider the following:
we consider that there are randomly chosen adversarial nodes in
the network such that for each node ğ‘£ğ‘– , there cannot be more than
ğ‘” fraction of its neighbors who are adversaries, where ğ‘” âˆˆ [0, 1).
This fraction ğ‘” is known to all nodes in the network, however the
regular nodes do not know which or if any of their neighbor nodes
are adversaries. Here we assume that the adversaries are restricted
to form an F-local set , where ğ¹ is a non-negative integer.

Algorithm 1 Adversary aware Networked actor-critic algorithm
1: Input: Initial values of the parameters ğœ‡0ğ‘– , ğœ” ğ‘–0 , ğœ”Ëœ ğ‘–0 , ğœƒ 0ğ‘– , âˆ€ğ‘– âˆˆ
N ; the initial state ğ‘  0 of the MDP and stepsizes {ğ›½ğœ”,ğ‘¡ }ğ‘¡ â‰¥0 and

{ğ›½ğœƒ,ğ‘¡ }ğ‘¡ â‰¥0, ğ¹, ğ‘‰ (ğ‘ )
2: Repeat:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

13:

for all ğ‘– âˆˆ N do
ğ‘– .
Observe state ğ‘ ğ‘¡+1 and reward ğ‘Ÿ ğ‘¡+1
ğ‘–
ğ‘–
ğ‘– .
Update ğœ‡ğ‘–+1 â† (1 âˆ’ ğ›½ğœ”,ğ‘¡ ) Â· ğœ‡ğ‘¡ + ğ›½ğœ”,ğ‘¡ Â· ğ‘Ÿ ğ‘¡+1
ğ‘–
ğ‘–
Select and execute action ğ‘ğ‘¡+1 âˆ¼ ğœ‹ ğ‘– (ğ‘ ğ‘¡+1, Â·).
ğœƒğ‘¡

end for
1 , . . . , ğ‘ ğ‘ ).
Observe joint actions ğ‘ğ‘¡+1 = (ğ‘ğ‘¡+1
ğ‘¡+1
for all ğ‘– âˆˆ N do
ğ‘– - ğœ‡ğ‘– + Q
ğ‘–
ğ‘–
Update ğ›¿ğ‘¡ğ‘– â† ğ‘Ÿ ğ‘¡+1
ğ‘¡+1 (ğœ” ğ‘¡ ) - Qğ‘¡ (ğœ” ğ‘¡ ).
ğ‘¡
ğ‘–
ğ‘–
ğ‘–
Critic Step: ğœ”Ëœ ğ‘¡ â† ğœ”ğ‘¡ + ğ›½ğœ”,ğ‘¡ Â· ğ›¿ğ‘¡ Â· âˆ‡ğœ” Qğ‘¡ (ğœ”ğ‘¡ğ‘– ).
Ã
Update Ağ‘¡ğ‘– â† Qğ‘¡ (ğœ”ğ‘¡ğ‘– ) - ğ‘ğ‘– âˆˆ Ağ‘– ğœ‹ğœƒ ğ‘– (ğ‘ ğ‘¡ , ğ‘ğ‘– Â· Q (ğ‘ ğ‘¡ , ğ‘ğ‘– ,
ğ‘¡

ğ‘ âˆ’1 ; ğœ”ğ‘¡ğ‘– )
Update ğœ“ğ‘¡ğ‘– â† âˆ‡ğœƒğ‘– log ğœ‹ ğ‘– ğ‘– (ğ‘ ğ‘¡ , ğ‘ğ‘–ğ‘¡ ).
ğœƒğ‘¡

ğ‘–
Actor Step: ğœƒğ‘¡+1
â† ğœƒğ‘¡ğ‘– + ğ›½ğœƒ,ğ‘¡ Â· Ağ‘¡ğ‘– Â· ğœ“ğ‘¡ğ‘– .
ğ‘–
15:
Send ğœ”ğ‘¡ to the neighbors { ğ‘— âˆˆ N : (ğ‘–, ğ‘—) âˆˆ Eğ‘¡ } over the
communication network Gğ‘¡
16:
end for
17:
for all ğ‘– âˆˆ N do
18:
Gather neighbors ğ¾ğ‘– from ğ‘‰ (ğ‘ )
19:
Gather ğœğ‘– (ğ‘¡) by removing the highest and lowest
ğ¹ states among ğ¾ğ‘–
Ã
ğ‘—
ğ‘–
20:
Consensus Step: ğœ”ğ‘¡+1
â† ğ‘— âˆˆğœğ‘– ğ‘ğ‘¡ (ğ‘–, ğ‘—) Â· ğœ”Ëœ ğ‘¡ .
21:
end for
22:
Update the iteration counter ğ‘¡ â† ğ‘¡ + 1.
23: Until Convergence
14:

3.3

Algorithm

The overall actor-critic algorithm is detailed in Algorithm 1. In the
actor step, each agent, each agent improves it policy as shown in
Line 14, where ğ›½ğœƒ,ğ‘¡ > 0 is the stepsize. Note that both the actor and
the critic steps can be executed in a decentralized fashion without
any centralized training. For the consensus step, one important
thing to note that since the agents aim to optimize the globally
averaged reward function ğ‘Ÿ , the agents share their local parameters ğœ” ğ‘– with their neighbors and this allows the agents to improve
their current policy using the policy gradient theorem. The parameter sharing involves a consensus update using the weight matrix
ğ¶ğ‘¡ = [ğ‘ğ‘¡ (ğ‘–, ğ‘—)] | N |Ã— | N | such that ğ‘ğ‘¡ (ğ‘–, ğ‘—) is the weight on the message sent by agent ğ‘— to agent ğ‘– at time ğ‘¡. An important restriction
in our model is that we only consider ğ‘ğ‘¡ (ğ‘–, ğ‘—) > 0 if agent ğ‘– and ğ‘—
are neighbors of each other in the network. Some discussions on
the choice of ğ‘ğ‘¡ (ğ‘–, ğ‘—) is in the later part of this section.
We modify Algorithm 1 in [31] to incorporate co-ordinated responses by regular nodes in the presence of adversaries. At each
ğ‘—
time step, the regular nodes {ğ‘£ğ‘– } gather ğœ”Ëœ ğ‘¡ for all ğ‘£ ğ‘— âˆˆ Kğ‘– and remove the highest and lowest ğ¹ states and the remaining nodes are
denoted by ğœğ‘– (ğ‘¡) âˆˆ Kğ‘– . The consensus action in the actor is then
to aggregate the parameters from the neighbors in ğœğ‘– (ğ‘¡). Note that,

these updates are only done by the regular nodes and the adversarial nodes aggregate the weights from the neighbors in any way
they wish. The dynamics of updates by the regular nodes are local
and decentralized since they do not require regular nodes to know
anything beyond the signals sent from their neighbors. As mentioned in [27], when the network N is time-invariant, the eï¬€ective
neighbor set ğœğ‘– (ğ‘¡) is only a function of the states of neighbors of
ğ‘£ğ‘– at time step ğ‘¡. This ï¬ltering is also closely connected to bandit
based ranking of arms where the distribution of the means of the
arms decide which arm would be picked, albeit here instead of picking one arm, the agent selects one or multiple arms [6]. The rest of
the consensus style algorithm in this decentralized environment is
the same as Algorithm 1 in [31].
The standard assumptions for the consensus matrix is deï¬ned
in Assumption 4.4 of [31] and while there can be many ways to
deï¬ne the weight, one popular way is to consider the notion of
Metropolis weights [29]
n
o âˆ’1
ğ‘ğ‘¡ (ğ‘–, ğ‘—) = 1 + ğ‘šğ‘ğ‘¥ [ğ‘˜ğ‘¡ (ğ‘–), ğ‘˜ğ‘¡ ( ğ‘—)]
, âˆ€(ğ‘–, ğ‘—) âˆˆ Eğ‘¡
Ã•
ğ‘ğ‘¡ (ğ‘–, ğ‘—) = 1 âˆ’
ğ‘ğ‘¡ (ğ‘–, ğ‘—)âˆ€ğ‘– âˆˆ N
ğ‘— âˆˆğœğ‘–

where Eğ‘¡ denotes the time varying edges and ğ‘˜ğ‘¡ (ğ‘–) is the degree
of the agent ğ‘– in the time varying network.

4 RELATED WORK
One of the assumptions in the process of social decision making is
that individuals following a learning trajectory (despite each individual having a limited memory) successfully converges to the best
option for the collective population. While individuals participate
in decision making where they associate diï¬€erent risk and rewards
in an uncertain environment, they also tend to incorporate beliefs
from their immediate neighbors in a networked environment, a
phenomenon that has played a critical role in co-operative multiagent systems [4]. This begets the question as to how resilient are
these consensus algorithms and how can agents adapt to decentralized training in the presence of malicious agents or adversaries. We
brieï¬‚y highly some notable studies done previously at the intersection of MARL and
MARL: The ï¬eld of MARL has evolved very rapidly over the past
few years. The collective goal of the multi-agent system is to either reach a stable and consensus state for all agents [7], or solve
a static optimization problem in a distributed fashion[19]. However, competition and collaboration always emerge between autonomous agents that learn by reinforcement over ï¬nite horizons.
Some of the common approaches on modeling and solving cooperative multi-agent reinforcement learning problems include: (1)
independent learners [10, 13], (2) fully observable critic [16], (3)
value function factorization [26], (4) consensus based RL [3], and
(5) learning to communicate [17].
Decentralized Networked MARL: The challenge with fully cooperative multi-agent systems is that fully cooperative systems
(Dec-POMDPs) are signiï¬cantly harder to solve than single agent
RL problems due to the combinatorial explosion in the joint action state spaces combining all agents. For this reason CTDE have

.

.

become popular. The prevailing MARL paradigm of centralised
training with decentralised execution (CTDE) [25,29,21] assumes a
training stage during which the learning algorithm can access data
from all agents to learn decentralised (locally-executable) agent
policies. CTDE algorithms such as COMA [9] learn powerful critic
networks conditioned on joint observations and actions of all agents.
Other extensions of MADDP include shared experience actor-critic
frameworks [5] for eï¬ƒcient MARL exploration by sharing experience amongst agents as opposed to MADDPG which only reinforces an agentâ€™s own explored actions. There have also been studies conducted on making these RL systems in networked systems
scalabale [20].
Decision making for social systems: There are two broad avenues of research on decision making in interactive environments
considering social systems. The ï¬rst group of studies focus on game
theoretic environments where agents have similar adaptation and
learning abilities and so the actions of each agent aï¬€ect the task
achievement of the other agents [1, 18, 21]. The payoï¬€ of the agents
in these games depend on whether they are purely collaborative
or competitive or a mix. In a recent study on combining Reinforcement Learning with Agent Based Modeling [25], the authors address the self-organizing dynamics of social segregation and explore the space of possibilities that emerge from considering different types of rewards. The second group of studies focuses on
social inï¬‚uence from other agents as the intrinsic factor for the decision making. Social inï¬‚uence has been known to be an intrinsic
factor in agentsâ€™ choices [23, 24], and in recent studies, there have
been attempts to propose a uniï¬ed mechanism for coordination in
MARL by rewarding agents for having causal inï¬‚uence over other
agentsâ€™ actions [11, 30]. Adversarial attacks during training can potentially impact the choices made by agents and impact the consensus algorithms as studied in [8] and this is important to solve since
attacks through social inï¬‚uence are a common tool for adversaries
[22].

5

CONCLUSION

In this work, we discussed a simple strategy for agents to mitigate the nuances of adversarial agents in fully decentralized MARL
where the agents are connected via a time varying network. We
relied on the algorithm of [31] and modiï¬ed the actor-critic algorithm in the presence of one or more adversaries. We did not
provide a formal argument for the convergence of the consensus
updates with our adversarial framework which we leave as one of
the immediate next steps of this work. A second direction where
this kind of work can be adapted is to use attention based actor
critic algorithms [? ] that modify the action value function by parameterizing it with agent speciï¬c attention weights. We can then
use reward values to optimize for the attention weights over the
time varying network.

REFERENCES
[1] Michael Bowling and Manuela Veloso. 2000. An analysis of stochastic game theory
for multiagent reinforcement learning. Technical Report. Carnegie-Mellon Univ
Pittsburgh Pa School of Computer Science.
[2] Lucian Busoniu, Robert Babuska, and Bart De Schutter. 2008. A comprehensive
survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man,
and Cybernetics, Part C (Applications and Reviews) 38, 2 (2008), 156â€“172.

[3] Lucas Cassano, Kun Yuan, and Ali H Sayed. 2020. Multiagent fully decentralized
value function learning with linear convergence rates. IEEE Trans. Automat.
Control 66, 4 (2020), 1497â€“1512.
[4] L Elisa Celis, Peter M Kraï¬€t, and Nisheeth K Vishnoi. 2017. A distributed learning dynamics in social groups. In Proceedings of the ACM Symposium on Principles of Distributed Computing. 441â€“450.
[5] Filippos Christianos, Lukas SchÃ¤fer, and Stefano Albrecht. 2020. Shared experience actor-critic for multi-agent reinforcement learning. Advances in neural
information processing systems 33 (2020), 10707â€“10717.
[6] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. 2006.
Action Elimination and Stopping Conditions for the Multi-Armed Bandit and
Reinforcement Learning Problems. Journal of machine learning research 7, 6
(2006).
[7] J Alexander Fax and Richard M Murray. 2004. Information ï¬‚ow and cooperative
control of vehicle formations. IEEE transactions on automatic control 49, 9 (2004),
1465â€“1476.
[8] Martin Figura, Yixuan Lin, Ji Liu, and Vijay Gupta. 2021. Resilient Consensusbased Multi-agent Reinforcement Learning. arXiv preprint arXiv:2111.06776
(2021).
[9] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artiï¬cial intelligence, Vol. 32.
[10] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras,
Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. 2017. Stabilising experience replay for deep multi-agent reinforcement learning. In International
conference on machine learning. PMLR, 1146â€“1155.
[11] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro
Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. 2019. Social inï¬‚uence
as intrinsic motivation for multi-agent deep reinforcement learning. In International conference on machine learning. PMLR, 3040â€“3049.
[12] Jelle R Kok and Nikos Vlassis. 2006. Collaborative multiagent reinforcement
learning by payoï¬€ propagation. Journal of Machine Learning Research 7 (2006),
1789â€“1828.
[13] Guillaume J Laurent, LaÃ«titia Matignon, Le Fort-Piat, et al. 2011. The world
of independent learners is not Markovian. International Journal of Knowledgebased and Intelligent Engineering Systems 15, 1 (2011), 55â€“64.
[14] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems 30 (2017).
[15] Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. 2021. Contrasting centralized and decentralized critics in multi-agent reinforcement learning. arXiv preprint arXiv:2102.04402 (2021).
[16] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, and Zhibo Gong. 2018. Modelling
the dynamic joint policy of teammates with attention multi-agent DDPG. arXiv
preprint arXiv:1811.07029 (2018).
[17] Igor Mordatch and Pieter Abbeel. 2018. Emergence of grounded compositional
language in multi-agent populations. In Proceedings of the AAAI conference on
artiï¬cial intelligence, Vol. 32.
[18] Kazuaki Nakayama, Masato Hisakado, and Shintaro Mori. 2017. Nash equilibrium of social-learning agents in a restless multiarmed bandit game. Scientiï¬c
reports 7, 1 (2017), 1937.
[19] Angelia Nedic and Asuman Ozdaglar. 2009. Distributed subgradient methods
for multi-agent optimization. IEEE Trans. Automat. Control 54, 1 (2009), 48â€“61.
[20] Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. 2020. Scalable multi-agent
reinforcement learning for networked systems with average reward. Advances
in Neural Information Processing Systems 33 (2020), 2074â€“2086.
[21] Paul B Reverdy, Vaibhav Srivastava, and Naomi Ehrich Leonard. 2014. Modeling
human decision making in generalized Gaussian multiarmed bandits. Proc. IEEE
102, 4 (2014), 544â€“571.
[22] Soumajyoti Sarkar, Paulo Shakarian, Mika Armenta, Danielle Sanchez, and Kiran Lakkaraju. 2019. Can social inï¬‚uence be exploited to compromise security:
An online experimental evaluation. In Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. 593â€“596.
[23] Soumajyoti Sarkar, Paulo Shakarian, Danielle Sanchez, Mika Armenta, and Kiran
Lakkaraju. 2020. Use of a controlled experiment and computational models to
measure the impact of sequential peer exposures on decision making. Plos one
15, 7 (2020), e0234875.
[24] Karl H Schlag. 1998. Why imitate, and if so, how?: A boundedly rational approach to multi-armed bandits. Journal of economic theory 78, 1 (1998), 130â€“156.
[25] Egemen Sert, Yaneer Bar-Yam, and Alfredo J Morales. 2020. Segregation dynamics with reinforcement learning and agent based modeling. Scientiï¬c reports 10,
1 (2020), 11771.
[26] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung
Yi. 2019. Qtran: Learning to factorize with transformation for cooperative multiagent reinforcement learning. In International conference on machine learning.
PMLR, 5887â€“5896.
[27] Shreyas Sundaram and Bahman Gharesifard. 2015. Consensus-based distributed
optimization with malicious nodes. In 2015 53rd Annual Allerton Conference on

Communication, Control, and Computing (Allerton). IEEE, 244â€“249.
[28] DaniÃ«l Willemsen, Mario Coppola, and Guido CHE de Croon. 2021. MAMBPO:
Sample-eï¬ƒcient multi-robot reinforcement learning using learned world models. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). IEEE, 5635â€“5640.
[29] Lin Xiao, Stephen Boyd, and Sanjay Lall. 2005. A scheme for robust distributed
sensor fusion based on average consensus. In IPSN 2005. Fourth International

Symposium on Information Processing in Sensor Networks, 2005. IEEE, 63â€“70.
[30] Annie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. 2021.
Learning latent representations to inï¬‚uence multi-agent interaction. In Conference on robot learning. PMLR, 575â€“588.
[31] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. 2018.
Fully decentralized multi-agent reinforcement learning with networked agents.
In International Conference on Machine Learning. PMLR, 5872â€“5881.

