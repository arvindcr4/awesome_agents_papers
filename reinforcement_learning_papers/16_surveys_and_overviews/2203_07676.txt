An Introduction to Multi-Agent Reinforcement Learning
and Review of its Application to Autonomous Mobility

arXiv:2203.07676v2 [cs.AI] 2 Aug 2022

Lukas M. Schmidt1∗ , Johanna Brosig1∗ , Axel Plinge1† , Bjoern M. Eskofier2 , and Christopher Mutschler1
Abstract— Many scenarios in mobility and traffic involve
multiple different agents that need to cooperate to find a joint
solution. Recent advances in behavioral planning use Reinforcement Learning to find effective and performant behavior strategies. However, as autonomous vehicles and vehicle-to-X communications become more mature, solutions that only utilize
single, independent agents leave potential performance gains
on the road. Multi-Agent Reinforcement Learning (MARL) is
a research field aiming to find optimal solutions for multiple
agents interacting with each other. This work gives an overview
of the field to researchers in autonomous mobility. We first
explain MARL and introduce important concepts. Then, we
discuss the central paradigms that underlie MARL algorithms
and give an overview of state-of-the-art methods and ideas in
each paradigm. With this background, we survey applications of
MARL in autonomous mobility scenarios and give an overview
of existing scenarios and implementations.

I. I NTRODUCTION
Artificial intelligence (AI) is increasingly being deployed
in many applications and devices operating in a connected
and cooperative environment. This is particularly relevant
in autonomous mobility and traffic scenarios, like the ones
shown in Fig. 1, where multiple agents need to interact to
find a common solution. As cooperation and communication
between the agents are important in these problems, singleagent solutions, such as reinforcement learning (RL), often
fall short of expectations. Instead, multi-agent reinforcement
learning (MARL) deals with multi-agent problems and aims
to find policies that help multiple vehicles reach their individual and common objectives.
MARL aims at finding optimal strategies for agents in settings where multiple agents interact in the same environment.
This allows a variety of new solutions that build on concepts
such as cooperation [1] or competition [2]. However, multiagent settings also introduce challenges, including potentially
missing or inadequate communication, difficulties in credit
assignment to agents, and environment non-stationarity. This
survey highlights key principles and paradigms in MARL
research and gives an overview of possible applications and
challenges for MARL in autonomous mobility.
The paper is structured as follows. Basics of RL are provided in Sec. II before central concepts of MARL in Sec. III.
Sec. IV summarizes state-of-the-art MARL algorithms and
explains core paradigms. Sec. V gives an overview of application fields. Sec. VI concludes.
1 Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Nuremberg, Germany. {firstname}.{lastname}@iis.fraunhofer.de
2 Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Erlangen,
Germany. bjoern.eskofier@fau.de
* Equal contribution.
† Corresponding Author.

Fig. 1: MARL applications: Intelligent warehouses and logistics, drone fleets for delivery and agriculture, traffic flow control, and automated driving (image source: Pexels/Pixabay).

II. BASICS : R EINFORCEMENT L EARNING
Single-agent RL is designed to solve a class of problems
known as Markov decision processes (MDPs). In an MDP, a
single agent interacts with an environment by taking certain
actions a depending on the state s of the environment. These
actions may change the state of the environment. The agent
receives a reward r and aims at maximizing the expected
return, i.e., the sum of rewards gained in the environment
discounted by a factor γ (that weights future rewards) by
finding an optimal policy π ∗ (i.e., the behavior; ∗ denotes
optimality) that maps states to actions. A single step in the
environment is known as a transition sample (s, a, r, s0 ) [3].
We describe an MDP by a quin-tuple (S, A, P, R, γ) where
S and A refer to state and action space, P denotes the transition probability from a state s to a successor state s0 given
an action a, R denotes the reward function received by the
agent for the transition from (s, a) to s0 , and γ ∈ [0; 1] is the
discount factor that trades instantaneous over future rewards.
Two major paradigms in RL are value-based and policy
gradient methods. Value-based methods, such as DQN [4],
estimate the utility of an action using a parametrized Qnetwork Qθ ; the policy is then formed by greedily following
the actions with the highest estimated values. Instead, policy
gradient methods directly optimize a parametrized policy πθ
to take actions that lead to a high reward more likely. Recent work in RL, e.g., PPO [5] combines both paradigms
into an actor-critic architecture that uses a value-based critic
to improve updates to the actor (or policy).

III. M ULTI -AGENT R EINFORCEMENT L EARNING
In MARL, multiple agents are concurrently optimized to
find optimal policies. Compared to the single-agent RL settings, this introduces several important differences and possibilities, which we want to highlight in this section.
1) Observability in MARL: In RL, observability describes
whether an agent can perceive the entire environment state
fully or partially. A classical MDP is defined to be fully observable: The agent directly perceives the environment state
s. Partial observability is described by a partially observable Markov decision processs (POMDPs) and modeled by
adding a (potentially lossy) observation function converting
an environment state s to an observation o. POMDPs often present more challenging, real-world applications. For
applied research, some scenarios (i.e., robots inside a controlled warehouse environment with enough sensors) can be
assumed to be fully observable. Most real-world mobility
applications, like traffic scenarios, are partially observable.
MARL provides the opportunity to combine partial observations from multiple agents to gather more information.
Several paradigms of MARL research, like communication
or cooperation, explicitly or implicitly use this opportunity
to improve performance. Many MARL methods use Recurrent Neural Networks (RNNs) to aggregate observations over
time into more complete observations [6].
2) Centrality: Different MARL settings can be distinguished by how much agents can communicate with each
other. In a fully decentralized problem, agents have no means
of communication, while a fully centralized setting allows
one central entity to perceive and control all agents. This
distinction affects whether the state and action spaces for all
agents are disjoint (decentralized) or joint (fully centralized).
Most MARL algorithms are neither decentralized nor fully
centralized; we explain different communication and interaction paradigms in Sec. IV.
3) Heterogeneous vs. Homogeneous agents: In general,
MARL does not require all agents to be similar. Heterogeneous agents can have different observation and action
spaces, like cars and traffic lights. However, in many MARL
environments, the agents are homogeneous, meaning they
have the same state and action space. For example, many
cars require similar behavior in an autonomous driving setting. For homogeneous agents, parameter sharing can be introduced to effectively jointly learn their policies. This is advantageous as the amount of trainable parameters is reduced,
allowing for shorter training times. More efficient training is
enabled as the experiences of all agents are used. However,
different behavior at execution time is still possible because
each agent has unique observations.
4) Cooperative vs. Competitive: An important difference
between MARL environments is how the goals of each agent
relate to each other. This can be divided into fully cooperative, fully competitive, and mixed cooperative-competitive.
In cooperative settings, all agents aim to achieve a common goal. For example, all agents want to reach their destination undamaged. Usually, all agents share a common reward function [7]. This work focuses on cooperative MARL

because autonomous mobility scenarios are best characterized by shared goals (e.g., reaching each agent’s destination
unharmed) rather than by competitive objectives.
By contrast, competitive settings, e.g., card or board
games, are usually modeled as zero-sum Markov games
where the return of all agents sums to zero [8]. Mixed settings combine cooperative and competitive agents that use
general-sum returns. For instance, in team games, agents
have to cooperate with their teammates while competing with
the opposing teams [8]. Littmann et al. [9] state that optimal
play can be guaranteed against an arbitrary opponent in competitive settings, whereas strong assumptions must be made
to guarantee convergence for cooperative environments.
5) Scalability: While simple MARL problems have a relatively small number of agents, many real-world mobility scenarios involve hundreds or thousands of individual
traffic participants. Modern simulators, like SUMO [10] or
CityFlow [11], can simulate entire neighborhoods or small
cities and model how hundreds of traffic lights affect the
traffic flow. Consequently, algorithms must scale to these
large agent numbers correctly and efficiently. For example, a
simple centralized entity that uses the joint observation and
action space of all agents would be impossible to train. In
contrast, independent agents (although less performant for a
smaller number of agents) could still operate. This problem
is known as scalability, and we discuss potential solutions in
Sec. IV.
6) Global vs. Local rewards: Single-agent RL environments have a single reward definition specifying the objective for the agent. Cooperative multi-agent settings can either
have a single global reward or multiple local rewards, one
for each agent [12]. By contrast, global rewards only depend
on the global state. They are easy to compute and specify
but often lead to inefficient training because agents are never
explicitly rewarded or penalized for their own actions [13].
Local rewards can reflect each agent’s contribution to the
cooperating group. This makes it easier to learn and improves convergence, especially in decentralized settings that
do not have other explicit cooperation [14]. However, it is
usually challenging to design and implement a reward function that correctly rewards individual agents for their contributions [15]. Algorithms that aim to solve the problem
of credit assignment described in Sec. IV-B explicitly learn
how global rewards can be disentangled into local rewards
to solve this.
IV. A LGORITHMS FOR MARL
We organize algorithms for MARL into four categories
based on their underlying paradigm, as illustrated in Fig. 2.
We order them from independent to fully connected agents.
The four paradigms described in the next paragraph are:
Fully decentralized algorithms, credit assignment, communicating agents, algorithms with centralized training and decentralized execution. All algorithms surveyed in this work
are summarized in Table I. In addition to paradigm and algorithm type, the table also lists available open-source implementations of these algorithms.

(a) Decentralized

(b) Credit assignment

(c) Communication

(d) CTDE

Fig. 2: Four categories of MARL approaches based on communication and cooperation: (a) In a decentralized setting, agents
(colored circles) operate and learn independently; (b) Credit assignment techniques use an explicit mechanism that assigns
the reward (grey circles) to individual agents; (c) Agents operate independently but have access to a communication medium
to exchange messages (black arrows). (d) In Centralized training with decentralized execution (CTDE), agents use a shared
critic (gray background) to improve performance during training. After training, the critic is no longer used, and agents
operate independently.
Algorithm Frameworks. There are many publicly available implementations of MARL algorithms, provided both
through author implementations and larger frameworks. RLLib [34] is an open-source reinforcement learning library
that aims at providing high-performance implementations for
RL algorithms. Many single-agent RL algorithms are implemented, which can be extended to the multi-agent setting. Moreover, some MARL-specific algorithms are available. EpyMARL is another framework comprising a wide
range of algorithms. However, only discrete actions are available [35]. Mava is designed for scalable MARL training and
execution. It allows for discrete and continuous actions [36].
MALib is specialized in scalable population-based MARL
[37]. Networked MARL (NMARL) is a repository of MARL
algorithms for networked system control [28]. Table II lists
the implementation framework, if available, as well.
A. Decentralized: Learning independently
The fully decentralized setting assumes that agents learn
entirely independently. Usually, multiple agents learn individual single-agent RL algorithms concurrently. To that end,
each agent has an independent observation, policy, and algorithm - the only interaction exists through the environment.
This kind of algorithm is often used as a baseline and includes independent Q-learning (IQL) [16], independent advantage actor critic (IA2C) [17] or independent proximal policy optimization (IPPO) [18]. Typically, independent agents
perform worse than optimized cooperating agents in centralized settings. However, the performance benefits of more
complicated approaches vary wildly between environments
- in some cases, independent agents are on par with or can
outperform cooperating agents [16], [18].
While it is easy to adapt single-agent RL algorithms to
these settings, decentralized environments violate the stationary Markov property (i.e., the probability transitions P ,
which define the system dynamics, change). In particular, all
other agents are considered part of the environment from an
agent’s perspective. Therefore, contrary to single-agent RL,
convergence is not guaranteed [38]. However, decentralized
algorithms do not need to deal with scalability issues which
is a frequent challenge in autonomous mobility scenarios.

B. Credit assignment
Most MARL environments do not explicitly disentangle
reward functions for different agents and use global rewards
instead. This is problematic during training [13]. Credit assignment methods aim to convert this global reward into an
estimated, per-agent local reward. For example, if vehicles
are trained to move autonomously, and a vehicle causes a
crash, it is desirable to only punish the perpetrator.
Credit assignment methods can be grouped into explicit
and implicit credit assignment methods. Explicit methods
estimate each agent’s contributions against a certain baseline. COMA [19] proposes a counterfactual baseline, where
the global reward is compared to the reward received if
the agent’s action is replaced by a default action. This approach omits correlations between the agents, leading to inefficiency in complex settings. To circumvent this problem,
Li et al. [20] introduce Shapley counterfactual credit assignment that guides the training by estimating the individual
credit for each agent using Shapley Values. Shapley Value
is a concept from cooperative game theory quantifying the
contribution of each player to the coalition [39].
Implicit credit assignment aims at learning how to decompose the shared reward function into individual value functions [40]. Sun et al. introduce a value decomposition network (VDN), which learns to decompose the shared value
function into individual value functions [21]. However, it assumes additive individual functions. QMIX [22] builds on
VDN and removes this limitation by decomposing the global
reward into an arbitrary non-linear combination of per-agent
value functions. To make the optimization tractable, QMIX
only constrains this combination to be monotonous with respect to the contribution of individual agents [22].
While this monotony constraint allows a decentralized execution, Mah et al. [23] argue that it does not hold for arbitrary Q-functions and makes exploration inefficient. They
introduce a hierarchical method, i.e., multi-agent variational
exploration (MAVEN), that introduces a central latent space
and diversifies exploration among agents [23]. However, recent work [24] argues that this exploration is still inadequate

TABLE I: We sort algorithms into the paradigms explained in Fig.2 according to their primary contribution. Some algorithms
can be assigned to multiple paradigms. Additionally, we note if algorithms make use of an explicit communication mechanism
between agents. The type is either value-based (V), policy-based (P), or actor-critic (AC).
Name

Year

Communication

Type

Framework

Independent Agents, Section IV-A
IQL [16]
IA2C [17]
IPPO [18]

1993
2020
2020

No
No
No

V
AC
AC

EpyMARL1 , RLLib2 , MALib4
EpyMARL1 , RLLib2 , MALib4
EpyMARL1 , RLLib2 , MALib4

Credit Assignment, Section IV-B
COMA [19]
Shapley counterfactual credit assignment [20]
VDN [21]
QMIX [22]
MAVEN [23]
Q-DPP [24]

2018
2021
2018
2018
2019
2020

No
No
No
No
No
No

AC
Value
V
V
Hybrid
V

EpyMARL1
-/EpyMARL1 , Mava3
EpyMARL1 , RLLib2 , Mava3 , MALib4
Author’s implementation5
Author’s implementation6

Communication, Section IV-C
RIAL [6]
DIAL [6]
CommNet [25]
ATOC [26]
IC3Net [27]
NeurComm [28]
Mean-field MARL [29]
Dec-Neural-AC [30]

2016
2016
2016
2018
2019
2020
2018
2021

Yes
Yes
Yes
Yes
Yes
Yes
No
No

V
V
P
AC
P
AC
V/AC
AC

Author’s implementation7
Mava3 , NMARL8
NMARL8
-/Author’s implementation9
NMARL8
Author’s implementation10
-/-

Centralized Training, Decentralized Execution, Section IV-D
MADDPG [31]
MAPPO [32]
Recurrent policy optimization [33]

2017
2021
2021

No
No
No

AC
AC
AC

EpyMARL1 , RLLib2 , MALib4 , Mava3
EpyMARL1 , Mava3
Authors implementation11

1 github.com/uoe-agents/epymarl
2 docs.ray.io/en/master/rllib/index.html

7 github.com/iassael/learning-to-communicate
8 github.com/cts198859/deeprl_network

3 github.com/instadeepai/Mava
4 github.com/sjtu-marl/malib
5 github.com/AnujMahajanOxf/MAVEN

9 github.com/IC3Net/IC3Net
10 github.com/mlii/mfrl
11 github.com/kargarisaac/macrpo

6 github.com/QDPP-GitHub/QDPP

and proposes a linear-time sampler that helps agents cover
orthogonal directions in state space during training.
C. Learning Communication
To allow agents to cooperate, a large body of work introduces an explicit communication layer to the algorithms.
This information exchange allows agents to widen their observation space and cooperate. We discuss algorithms that
use explicit communication between agents.
Foerster et al. [6] proposed two approaches for learning
communication between independent agents. In Reinforced
Inter-Agent Learning (RIAL), each agent has an additional
network to generate a communication message for the other
agents. For Differentiable Inter-Agent Learning (DIAL), the
gradients of the other agents are fed through the communication channel into the communication network. This allows for
end-to-end training across the agents. RIAL is only trainable
within an agent. In contrast to discrete connections in RIAL
and DIAL, CommNet [25] learns a continuous communication channel jointly with the policy. Through this channel, the
agents receive the summed transmission of the other agents.
Then, the input of each hidden layer for each agent is the
previous layer and the communication message.
In recent work, communication has been viewed more ambiguously. Communication introduces costs, especially as the

number of agents grows. Additionally, communication massively increases the amount of information available to each
agent, making it harder to identify crucial information [26].
Recent work has thus focused on different mechanisms to
decide if an agent should communicate at all.
Jiang and Lu [27] propose an attentional communication
model (ATOC) that uses local information to decide if an
agent should initiate communication. Agents form a dynamic
group of local communicating agents that can coordinate
with a shared communication medium. In a different line
of work, the individualized controlled continuous communication model (IC3Net) [27] uses recurrent neural network
(RNN) policies and allows agents to share the RNN’s hidden
state through a designated action. This is particularly useful
in competitive scenarios as agents can block communication.
Another approach to restricting communication is networked MARL. Contrary to attentional communication,
these networks limit the agents’ communication to their
local neighbors. Chu et al. [28] employ this concept and
additionally introduce a communication protocol called
NeurComm., It is similar to CommNet [25] but instead of
summing the received messages, NeurComm concatenates
them. Policy fingerprints supplement communication to
reduce non-stationarity [28].

Representing each agent as a node in a network enables
scaling to large agent populations. Yang et al. [29] propose
employing this mean-field formulation to solve scalability issues. Accordingly, each agent is only affected by the mean
effect of its local neighborhood. Compared to considering all
agents, the complexity of interactions is reduced significantly
for large agent populations. Decentralized neural actor-critic
algorithm (Dec-Neural-AC) [30] is another mean-field approach aiming at scalability.
D. Centralized Training with Decentralized Execution
Modern MARL approaches often use centralization during
training but create independent agents that run independently.
This setting is known as centralized training with decentralized execution (CTDE). CTDE maps well to real-world use
cases, where training can occur in simulation or with (potentially expensive) real-time communication, but agents should
operate independently after training.
Usually, CTDE algorithms use an actor-critic architecture,
which decomposes the policy and value estimates into explicit actor and critic networks. The critic is only used during
training and to improve the policy’s gradient updates. CTDE
actor-critic algorithms exploit this by training multiple agents
together with a shared critic. Only the independent actors are
needed during execution, allowing agents to operate without
communication after training.
Multi-agent deep deterministic policy gradient (MADDPG) [31] is an example of CTDE algorithms using peragent actors with policy networks to map agent-specific observations to agent-specific actions. The critic approximates
a centralized action-value function and receives the concatenated actions of all agents and additional global state information. This state information can include the concatenated
observations of all agents. The agent’s actions are included
to make the environment more stationary [31].
Off-policy methods, such as MADDPG, were commonly
considered to be more sample-efficient than on-policy methods. However, Yu et al. [32] propose the on-policy algorithm
multi-agent proximal policy optimization (MAPPO) that performs comparably to off-policy algorithms. MAPPO suggests
using global environment information as input to the critic
instead of a concatenation of all observations, which scales
with an increasing number of agents.
Recurrent Policy Optimization [33] is a recent method using a recurrent critic and a combinator that combines different agents’ trajectories into one meta-trajectory. This metatrajectory is used to train the critic and allows the approach
to learning better from agents’ interactions.
V. MARL IN AUTONOMOUS M OBILITY: A PPLICATIONS
AND C HALLENGES
Many real-world scenarios in autonomous mobility involve multiple agents. This presents an obvious potential
for MARL approaches to optimize how agents cooperate
and interact. Autonomous mobility scenarios face similar
key challenges. Large agent populations cause scalability and
credit assignment issues. As mobility scenarios are usually

highly dynamic, fast reaction times are required. We give an
overview of application domains of MARL in autonomous
mobility and discuss potential benefits, challenges, and recommendations for MARL-based methods. Table II lists different environments and their use-cases.
A. Traffic Control
Traffic congestion in cities is problematic as it causes pollution, financial loss and increases the risk of accidents [50].
Hence, controlling the traffic through traffic signals, line controls, or routing guidance has great potential to improve living conditions. Traffic involves many participants, including
traffic lights, vehicles, and pedestrians. MARL is an obvious
fit for the traffic control problem as it does not rely on limiting heuristics. Data from various sources, e.g., road surveillance cameras, location-based mobile services, and vehicle
tracking devices can be input into MARL models [11].
Adaptive traffic signal control is a prominent approach
to traffic control. A large number of agents, however, demand solutions to problems related to the scalability issue.
As centralized and CTDE approaches do not scale well for
large agent populations, decentralized approaches are an evident choice. However, convergence is not guaranteed (Section IV-A). To make the environment stationary from the
agents’ perspectives, Wang et al. [51] introduce graph attention networks to learn the dynamics of the neighbor intersection’s influences. Instead of attention networks, Chu et
al. [17] propose communication to the neighboring agents to
improve the convergence of decentralized training. In addition, a spatial discount factor is used to weigh neighboring
agents’ observation and reward signals.
In essence, all approaches assume that neighboring agents
are most relevant to controlling local traffic. However, training MARL models needs a lot of data, making the availability
of accurate simulators crucial. SUMO [10] and CityFlow [11]
provide macroscopic city-scale simulations that can efficiently model many different agents and their interactions.
B. Autonomous Vehicles
A different application directly controls multiple
autonomous vehicles through MARL. Compared to independent single-agent RL [52], this improves the ability of
agents to cooperate and achieve their individual objectives.
In this domain, performance gains can come from coordinated driving (e.g., by reducing wind resistance, traffic
jams, and optimizing road usage [28]), the ability to extend
the own perception with information from other agents
(through vehicle-to-vehicle (V2V) / vehicle-to-X (V2X)
communication [53]), or through efficient interactions in
intersection navigation scenarios [54].
A large variety of simulation and benchmark environments are available for research on autonomous vehicles.
The Highway-Env [45] allows to control multiple vehicles
and is particularly easy to set up and use. However, we found
that performance can drop below acceptable levels for largescale traffic simulations of many vehicles. BARK [43] is

TABLE II: Simulation environments for general-purpose MARL research and four application fields summarized in Sec. V.
Environments are characterized by either partial (P) or full (F) observability. Action spaces can either be discrete (D) or
continuous (C). Slashes (A/B) denote that the environment can be configured to have different characteristics.
Environment

Type

Obs.

Actions

Description

General Purpose
MAgent1 [41]
Multi-Agent Particle1 [2]

Comp
Coop / Comp

P
P

D
D/C

Multiple environments with a large number of agents
Multiple environments focused on communication

Traffic Control
CityFlow [11]
SUMO [10]

Coop / Comp
Coop / Comp

P/F
P/F

-/D/C

City traffic simulator
Traffic simulation

Autonomous Vehicles
SMARTS2 [42]
BARK3 [43]
MACAD4 [44]
highway-env5 [45]

Coop
Coop
Coop / Comp
Coop

P
P/F
P
P/F

C
D/C
D/C
D/C

Autonomous driving simulation platform
Autonomous driving scenarios
Multi-Agent Connected Autonomous Driving built based on CARLA
Autonomous driving and tactical decision-making tasks

Unmanned Aerial Vehicles
Gym PyBullet Drones6 [46]
AirSim7 [47]

Coop / Comp
Coop / Comp

P
P

C
C

Simulation environment for quadcopters with OpenAI gym API
High-fidelity simulation environment for UAVs and autonomous vehicles

Resource Optimization
MARO7 [48]
Flatland8 [49]

Coop
Coop

P
P/F

D/C
D

Resource optimization in industrial domains
Vehicle re-scheduling problem

1 github.com/Farama-Foundation/PettingZoo.
2 github.com/huawei-noah/SMARTS

5 github.com/eleurent/highway-env
6 github.com/utiasDSL/gym-pybullet-drones

3 github.com/bark-simulator/bark-ml
4 github.com/praveen-palanisamy/macad-gym

7 github.com/microsoft/maro
8 flatland.aicrowd.com/intro.html

an open-source simulation framework for autonomous vehicle research focusing on multi-agent scenarios with interactions between agents. BARK simulates various traffic scenarios and has a two-way interface to control cars hosted in
CARLA, a widely-used AV research simulation framework.
Similarly, SMARTS [42] provides a simulation environment
for diverse driving interactions, is compatible with the OpenAI gym interface, and offers multiple different observations.
This line of work presents unique challenges: Autonomous
vehicles must act in dynamic and unpredictable environments
like urban or highway traffic, and safety constraints must be
satisfied at any time. This limits the exploration crucial for
RL. Post-hoc extraction of safe policies trained in a simulator [52] can be used to assure safety, but these methods have
not yet been successfully applied to complex, multi-agent
control settings. Moreover, most MARL methods assume
identical or compatible algorithms. This requires manufacturers to agree on a common architecture and communication
standard to facilitate cross-brand cooperation of vehicles.
C. Unmanned Aerial Vehicles
Swarms of unmanned aerial vehicles (UAVs) are another
popular application field. UAVs are especially beneficial for
communication tasks [55] and applications where human
lives are endangered. Here, a (potentially large) network of
UAVs needs to be controlled to fulfill tasks like forest fire
surveillance [56], road traffic monitoring, [57] and air quality monitoring [58]. Moreover, UAVs can be used to provide
internet access in remote areas and enable wireless communication for applications like navigation and control [59].
Multiple works use MARL to plan paths and assign targets for UAVs, in a problem known as Multiple Target Assignment and Path Planning (MUTAPP). Here, MARL-based

methods can outperform classical optimization techniques,
like mixed-integer linear programming, because they can
handle dynamic environments and operate in a decentralized
way [60]. Qie et al. [60] propose a solution based on MADDPG to jointly optimize target assignment and path planning.
They propose a task abstraction layer that combines these
tasks into a common reward structure. Their algorithm can
effectively minimize the number of conflicts that arise in the
MUTAPP problem.
In general, UAVs present a challenging problem for almost
all algorithms. Typical environments for UAVs are highly dynamic [60] and require constant collision avoidance (and thus
quick reaction times) between individual agents. In addition,
UAVs have strict real-time constraints for their operation,
which limits complex central path planning and possible architectures for MARL algorithms. [60]. Another major challenge is the energy supply and the limited range of UAVs.
Jung et al. [61] apply a solution based on CommNet [25] to
coordinate the assignment of UAVs to charging towers and to
share energy between towers, optimizing power draw from
the electrical grid and minimizing operational costs.
Most research on UAVs uses custom, low-fidelity environments to simplify development [60], [61]. However, multiple
high-fidelity simulators are available that model UAV dynamics in more detail. AirSim [47] is a simulation platform
based on the Unreal Engine with a special focus on realistic simulation and hardware-in-the-loop research. It supports detailed rendered camera observations. In addition to
UAVs, AirSim is also able to simulate cars. Gym PyBullet
Drones [46] presents a physical simulator for research on
UAVs. In contrast to AirSim, Pybullet Drones features more
realistic physical effects like the ground effect and downwash, and it is compatible with OpenAI gym and RLlib.

D. Resource optimization
In another line of work, MARL is applied to resource
optimization and scheduling for mobility scenarios. This includes scheduling for trains [49] or ambulances, but also
taxi repositioning [62] and ride-sharing services [63], bike
repositioning, or container inventory management [48]. Li et
al. [63] propose to solve the assignment of ride requests to
specific drivers with MARL, which improves order response
rates.
A popular simulation used in the train scheduling and
routing domain is Flatland, which has been used in several
NeurIPS competitions [49]. Flatland provides a 2D-gridworld
of train tracks. Agents (i.e., trains) have to find optimal policies that are safe, performant, and robust to unforeseen circumstances since trains can break down. Observations can
be global or local, and Flatland also allows custom observation spaces. The Multi-Agent Resource Optimization platform [48] provides simulations, an RL toolkit, and utilities
for distributed computing for this domain. It supports diverse scenarios across multiple application domains, including the aforementioned container inventory management and
bike repositioning tasks, and includes visualizations of these
simulation environments.
VI. C ONCLUSION
In conclusion, we would like to note several challenges
and open problems in multi-agent reinforcement learning
(MARL) research for mobility scenarios. Chief among these
is a lack of explicit safety and interpretability affordances
in almost all current algorithms. This introduces real risk in
autonomous mobility scenarios, especially when autonomous
vehicles or UAVs are controlled. Safe training and verification methods that certify policy safety [52], [64], [65] must
be developed to deploy these methods in the real world [66].
A second challenge is integration with existing, manually
controlled systems. This is evident in domains like traffic
control or resource optimization, where human interaction
and intervention (e.g., through emergency vehicles or manually prioritized resources) can come unexpectedly for agents
that were only trained in a simulation. Effectively providing these interactions with human needs in simulation is an
unsolved problem due to the low sample efficiency of RL.
Finally, most simulation environments assume perfect,
high-bandwidth, latency-free communication. Despite recent
advances in communication standards like 5G [53], [67], this
is not a valid assumption in real-world scenarios. This makes
more centralized solutions, like centralized training with decentralized execution (CTDE), harder or impossible to train
in real-world situations. Recent methods that limit communication could be a possible solution to this problem.
Overall, however, MARL presents an exciting opportunity
to learn solutions for complex problems involving multiple
agents in an efficient, automated way.
ACKNOWLEDGEMENTS
This work was supported by the Bavarian Ministry for
Economic Affairs, Infrastructure, Transport and Technology

through the Center for Analytics-Data-Applications (ADACenter) within the framework of ”BAYERN DIGITAL II”.
B.M.E. gratefully acknowledges support of the German
Research Foundation (DFG) within the framework of the
Heisenberg professorship program (Grant ES 434/8-1).
R EFERENCES
[1] A. Oroojlooyjadid and D. Hajinezhad, “A review of cooperative multiagent deep reinforcement learning,” arXiv preprint 1908.03963, 2019.
[2] I. Mordatch and P. Abbeel, “Emergence of grounded compositional
language in multi-agent populations,” arXiv:1703.04908, 2017.
[3] R. S. Sutton and A. G. Barto, Reinforcement learning - an introduction,
ser. Adaptive computation and machine learning. MIT Press, 1998.
[4] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control through deep reinforcement learning,”
Nature, vol. 518, no. 7540, pp. 529–533, 2015.
[5] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.
[6] J. N. Foerster, Y. M. Assael, N. De Freitas, and S. Whiteson, “Learning to communicate with deep multi-agent reinforcement learning,” in
NeurIPS, 2016, pp. 2145–2153.
[7] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning:
A selective overview of theories and algorithms,” in Studies in Systems,
Decision and Control, 2021, vol. 325, pp. 321–384.
[8] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino,
M. Re, and S. Spanò, “Multi-agent reinforcement learning: A review
of challenges and applications,” Appl. Sciences, vol. 11, no. 11, 2021.
[9] M. L. Littman, “Value-function reinforcement learning in Markov
games,” Cognitive Systems Research, vol. 2, no. 1, pp. 55–66, 2001.
[10] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd,
R. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. Wießner, “Microscopic traffic simulation using sumo,” in The 21st IEEE International Conference on Intelligent Transportation Systems. IEEE, 2018.
[11] H. Zhang, S. Feng, C. Liu, Y. Ding, Y. Zhu, Z. Zhou, W. Zhang, Y. Yu,
H. Jin, and Z. Li, “Cityflow: A multi-agent reinforcement learning
environment for large scale city traffic scenario,” in World Wide Web
Conf., San Francisco, CA, 2019, pp. 3620–3624.
[12] J. Wang, Y. Zhang, T. K. Kim, and Y. Gu, “Shapley Q-value: a local
reward approach to solve global reward games,” in AAAI Conf. on
Artificial Intelligence, 2020, pp. 7285–7292.
[13] D. H. Wolpert and K. Tumer, “Optimal payoff functions for members
of collectives,” Adv. in Compl. Sys., vol. 4, no. 2/3, pp. 265–279, 2001.
[14] T. Balch, “Reward and diversity in multirobot foraging,” Work. Agents
Learning About, From and With other Agents, pp. 92–99, 1999.
[15] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
of multiagent deep reinforcement learning,” Autonomous Agents and
Multi-Agent Systems, vol. 33, no. 6, pp. 750–797, nov 2019.
[16] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative agents,” Int. Conf. on Machine Learning, pp. 330–337, 1993.
[17] T. Chu, J. Wang, L. Codeca, and Z. Li, “Multi-agent deep reinforcement learning for large-scale traffic signal control,” IEEE Trans. Intell.
Transportation Systems, vol. 21, no. 3, pp. 1086–1095, Mar. 2020.
[18] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. S.
Torr, M. Sun, and S. Whiteson, “Is independent learning all
you need in the starcraft multi-agent challenge?” arXiv preprint
arXiv:2011.09533, 2020.
[19] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in AAAI Conf. on
Artificial Intelligence, 2018, pp. 2974–2982.
[20] J. Li, K. Kuang, B. Wang, F. Liu, L. Chen, F. Wu, and J. Xiao, “Shapley counterfactual credits for multi-agent reinforcement learning,” in
ACM Intl. Conf. Knowl. Disc. and Data Min., 2021, pp. 934–942.
[21] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and
T. Graepel, “Value-decomposition networks for cooperative multiagent learning based on team reward,” in Intl. Jnt. Conf. Autonomous
Agents and Multiagent Systems, 2018, pp. 2085–2087.
[22] J. Foerster, S. Whiteson, T. Rashid, M. Samvelyan, C. Schroeder De
Witt, and G. Farquhar, “QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning,” arXiv preprint
arXiv:1803.11485v1, 2018.

[23] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “MAVEN:
multi-agent variational exploration,” in NeurIPS, 2019.
[24] Y. Yang, Y. Wen, J. Wang, L. Chen, K. Shao, D. Mguni, and W. Zhang,
“Multi-agent determinantal q-learning,” in Int. Conf. on Machine
Learning. PMLR, 2020, pp. 10 757–10 766.
[25] S. Sukhbaatar, A. Szlam, and R. Fergus, “Learning multiagent communication with backpropagation,” in Advances in Neural Information
Processing Systems, 2016, pp. 2252–2260.
[26] J. Jiang and Z. Lu, “Learning attentional communication for multiagent cooperation,” in NeurIPS, 2018, pp. 7254–7264.
[27] A. Singh, T. Jain, and S. Sukhbaatar, “Learning when to communicate
at scale in multiagent cooperative and competitive tasks,” in Int. Conf.
on Learning Representations, 2019.
[28] T. Chu, S. Chinchali, and S. Katti, “Multi-agent reinforcement learning
for networked system control,” arXiv preprint 2004.01339, 2020.
[29] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean
field multi-agent reinforcement learning,” in Int. Conf. on Machine
Learning, vol. 12, 2018, pp. 5571–5580.
[30] H. Gu, X. Guo, X. Wei, and R. Xu, “Mean-field multi-agent reinforcement learning: A decentralized network approach,” arXiv preprint
arXiv:2108.02731, 2021.
[31] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environments,” in NeurIPS, 2017, pp. 6380–6391.
[32] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu, “The
surprising effectiveness of ppo in cooperative, multi-agent games,”
arXiv preprint arXiv:2103.01955, 2021.
[33] E. Kargar and V. Kyrki, “MACRPO: multi-agent cooperative recurrent
policy optimization,” arXiv preprint 2109.00882, 2021.
[34] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg,
J. Gonzalez, M. Jordan, and I. Stoica, “RLlib: Abstractions for distributed reinforcement learning,” in Int. Conf. on Machine Learning,
2018, pp. 3053–3062.
[35] G. Papoudakis, F. Christianos, L. Schäfer, and S. V. Albrecht, “Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks,” arXiv preprint arXiv:2006.07869, 2020.
[36] A. Pretorius, K.-A. Tessera, A. P. Smit, C. Formanek, S. J. Grimbly,
K. Eloff, S. Danisa, L. Francis, J. Shock, H. Kamper, W. Brink, H. Engelbrecht, A. Laterre, and K. Beguir, “Mava: a research framework
for distributed multi-agent reinforcement learning,” arXiv preprint
arXiv:2107.01460, 2021.
[37] M. Zhou, Z. Wan, H. Wang, M. Wen, R. Wu, Y. Wen, Y. Yang,
W. Zhang, and J. Wang, “MALib: a parallel framework for
population-based multi-agent reinforcement learning,” arXiv preprint
arXiv:2106.07551, 2021.
[38] J. Hao, D. Huang, Y. Cai, and H. fung Leung, “The dynamics of reinforcement social learning in networked cooperative multiagent systems,” Eng. Applicat. of Artificial Intellig., vol. 58, pp. 111–122, 2017.
[39] L. Shapley, “A value for n-person games,” Ann. Math. Study28, Contributions to the Theory of Games, pp. 307–317, 1953.
[40] M. Zhou, Z. Liu, P. Sui, Y. Li, and Y. Y. Chung, “Learning implicit
credit assignment for cooperative multi-agent reinforcement learning,”
in Advances in Neural Information Processing Systems, Dec. 2020.
[41] L. Zheng, J. Yang, H. Cai, W. Zhang, J. Wang, and Y. Yu, “MAgent:
a many-agent reinforcement learning platform for artificial collective
intelligence,” in AAAI Conf. Artificial Intellig., 2018, pp. 8222–8223.
[42] M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao, W. Zhang,
M. Alban, I. Fadakar, Z. Chen, A. C. Huang, Y. Wen, K. Hassanzadeh, D. Graves, D. Chen, Z. Zhu, N. Nguyen, M. Elsayed, K. Shao,
S. Ahilan, B. Zhang, J. Wu, Z. Fu, K. Rezaee, P. Yadmellat, M. Rohani,
N. P. Nieves, Y. Ni, S. Banijamali, A. C. Rivers, Z. Tian, D. Palenicek,
H. bou Ammar, H. Zhang, W. Liu, J. Hao, and J. Wang, “SMARTS:
scalable multi-agent reinforcement learning training school for autonomous driving,” in Conf. on Robot Learning, Nov. 2020.
[43] J. Bernhard, K. Esterle, P. Hart, and T. Kessler, “BARK: open behavior
benchmarking in multi-agent environments,” in IEEE/RSJ Int. Conf.
Intelligent Robots and Systems, Las Vegas, NV, 2020, pp. 6201–6208.
[44] P. Palanisamy, “Multi-agent connected autonomous driving using deep
reinforcement learning,” in Int. Joint Conf. on Neural Networks, 2020.
[45] E. Leurent, “An environment for autonomous driving decisionmaking,” https://github.com/eleurent/highway-env, 2018.
[46] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig,
“Learning to fly—a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control,” in IEEE/RSJ
Int. Conf. on Intelligent Robots and Systems (IROS), 2021.

[47] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity
visual and physical simulation for autonomous vehicles,” in Intl. Conf.
Field and Serv. Robot., Zurich, Switzerland, 2017, pp. 621–635.
[48] X. Li, J. Zhang, J. Bian, Y. Tong, and T.-Y. Liu, “A cooperative
multi-agent reinforcement learning framework for resource balancing in complex logistics network,” in Int. Joint Conf. on Autonomous
Agents and Multiagent Systems, Mar. 2019.
[49] S. Mohanty, E. Nygren, F. Laurent, M. Schneider, C. Scheller,
N. Bhattacharya, J. Watson, A. Egli, C. Eichenberger, C. Baumberger, G. Vienken, I. Sturm, G. Sartoretti, and G. Spigler, “FlatlandRL: multi-agent reinforcement learning on trains,” arXiv preprint
arXiv:2012.05893, 2020.
[50] Z. Wang, H. Zhu, M. He, Y. Zhou, X. Luo, and N. Zhang, “Gan and
multi-agent DRL based decentralized traffic light signal control,” IEEE
Trans. Veh. Technol., 2021.
[51] M. Wang, L. Wu, J. Li, and L. He, “Traffic signal control with reinforcement learning based on region-aware cooperative strategy,” IEEE
Transactions on Intelligent Transportation Systems, 2021.
[52] L. M. Schmidt, G. Kontes, A. Plinge, and C. Mutschler, “Can you
trust your autonomous car? interpretable and verifiably safe reinforcement learning,” in IEEE Intelligent Vehicles Symp., Nagoya, Japan,
Jul. 2021, pp. 171–178.
[53] X. Ge, Z. Li, and S. Li, “5G software defined vehicular networks,”
arXiv preprint arXiv:1702.03675, 2017.
[54] Y. Wu, H. Chen, and F. Zhu, “Dcl-aim: Decentralized coordination
learning of autonomous intersection management for connected and
automated vehicles,” Transportation Research Part C: Emerging Technologies, vol. 103, pp. 246–260, 2019.
[55] J. Cui, Y. Liu, and A. Nallanathan, “The application of multi-agent
reinforcement learning in UAV networks,” in IEEE Int. Conf. on Communications Workshops. Shanghai, China: IEEE, May 2019.
[56] D. W. Casbeer, D. B. Kingston, R. W. Beard, and T. W. McLain,
“Cooperative forest fire surveillance using a team of small unmanned
air vehicles,” Int. J. Syst. Sci., vol. 37, no. 6, pp. 351–360, 2006.
[57] M. Elloumi, R. Dhaou, B. Escrig, H. Idoudi, and L. A. Saı̈dane, “Monitoring road traffic with a uav-based system,” in IEEE Wireless Communications and Networking Conf., 2018.
[58] Y. Yang, Z. Zheng, K. Bian, L. Song, and Z. Han, “Real-time profiling
of fine-grained air quality index distribution using UAV sensing,” IEEE
Internet Things J., vol. 5, no. 1, pp. 186–198, 2018.
[59] M. Mozaffari, W. Saad, M. Bennis, Y. Nam, and M. Debbah, “A tutorial on uavs for wireless networks: Applications, challenges, and
open problems,” IEEE Commun. Surv. Tutorials, vol. 21, no. 3, pp.
2334–2360, 2019.
[60] H. Qie, D. Shi, T. Shen, X. Xu, Y. Li, and L. Wang, “Joint optimization
of multi-uav target assignment and path planning based on multi-agent
reinforcement learning,” IEEE Access, vol. 7, pp. 146 264–146 272,
2019.
[61] S. Jung, W. J. Yun, J. Kim, J.-H. Kim, and F. Falcone, “Coordinated
multi-agent deep reinforcement learning for energy-aware UAV-based
big-data platforms,” mdpi.com, 2021.
[62] C. Liu, C. X. Chen, and C. Chen, “META: a city-wide taxi repositioning framework based on multi-agent reinforcement learning,” IEEE
Trans. on Intelligent Transportation Systems, 2021.
[63] M. Li, Z. T. Qin, Y. Jiao, Y. Yang, J. Wang, C. Wang, G. Wu, and
J. Ye, “Efficient ridesharing order dispatching with mean field multiagent reinforcement learning,” in World Wide Web Conference, San
Francisco, CA, 2019, pp. 983–994.
[64] G. Kontes, D. Scherer, T. Nisslbeck, J. Fischer, and C. Mutschler,
“High-speed collision avoidance using deep reinforcement learning
and domain randomization for autonomous vehicles,” in IEEE Int.
Conf. Intell. Transportation Systems, 2020.
[65] S. Rietsch, S.-Y. Huang, G. Kontes, A. Plinge, and C. Mutschler,
“Driver dojo: A benchmark for generalizable reinforcement learning
for autonomous driving,” arXiv preprint arXiv:2207.11432, 2022.
[66] B. Osinski, A. Jakubowski, P. Ziecina, P. Milos, S. Galias, C. Homoceanu, and H. Michalewski, “Simulation-based reinforcement learning
for real-world autonomous driving,” in IEEE Int. Conf. Robotics and
Automation, May 2020, pp. 6411–6418.
[67] M. Stahlke, T. Feigl, M. H. Castaneda Garcia, R. S. Gallacher, J. Seitz,
and C. Mutschler, “Transfer learning to adapt 5G fingerprint-based
localization across environments,” in IEEE Veh. Tech. Conf., 2022.

