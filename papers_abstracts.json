{
  "reinforcement_learning_papers/03_multi_agent_rl/MasRouter_Multi-Agent_Routing.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/03_multi_agent_rl/Revisiting_Cooperative_MARL.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning  Wei Fu 1 Chao Yu 2 Zelai Xu 2 Jiaqi Yang 3 Yi Wu 1 4  arXiv:2206.07505v2 [cs.AI] 7 Aug 2022  Abstract Many advances in cooperative multi-agent reinforcement learning (MARL) are based on two common design principles: value decomposition and parameter sharing. A typical MARL algorithm of this fashion decomposes a centralized Q-function into local Q-networks with parameters shared across agents. Such an algorithmic paradigm enables centralized training and decentralized execution (CTDE) and leads to efficient learning in practice. Despite all the advantages, we revisit these two principles and show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, value decomposition, and parameter sharing can be problematic and lead to undesired outcomes. In contrast, policy gradient (PG) methods with individual policies provably converge to an optimal solution in these cases, which"
  },
  "reinforcement_learning_papers/03_multi_agent_rl/Safe_Multi_Agent_RL.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning  Jiayu Chen 1 Le Xu 2 Wentse Chen 3 Jeff Schneider 3  1. Introduction  arXiv:2410.11234v3 [cs.LG] 11 Nov 2025  Abstract  The success of RL typically relies on large amounts of interactions with the environment. However, in real-world scenarios, such interactions can be unsafe or costly. As an alternative, offline RL (Levine et al., 2020; Chen et al., 2024a) leverages offline datasets of transitions, collected by a behavior policy, to train a policy. To avoid overestimation of the expected return for out-of-distribution states, which can mislead policy learning, model-free offline RL methods (Kumar et al., 2020; Wu et al., 2019) often constrain the learned policy to remain close to the behavior policy. However, acquiring a large volume of demonstrations from a high-quality behavior policy, can be expensive. This challenge has led to the development of offline modelbased reinforcement learning (MBRL) ap"
  },
  "reinforcement_learning_papers/03_multi_agent_rl/AgentNet_Decentralized_Multi-Agent.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/03_multi_agent_rl/Multi_Agent_RL.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "arXiv:2412.10256v2 [math.CO] 15 Apr 2025  THE BBDVW CONJECTURE FOR KAZHDAN\u2013LUSZTIG POLYNOMIALS OF LOWER INTERVALS GRANT T. BARKLEY AND CHRISTIAN GAETZ Abstract. Blundell, Buesing, Davies, Velic\u030ckovic\u0301, and Williamson (BBDVW) introduced the notion of a hypercube decomposition of an interval in Bruhat order. They conjectured a recursive formula in terms of this structure which, if shown for all intervals, would imply the Combinatorial Invariance Conjecture of Lusztig and Dyer, for Kazhdan\u2013Lusztig polynomials of the symmetric group. In this article, we prove implications between the BBDVW Conjecture and several other recurrences for hypercube decompositions, under varying hypotheses, which have appeared in the recent literature. As an application, we prove the BBDVW Conjecture for lower intervals [e, v], the first non-trivial class of intervals for which it has been established.  1. Introduction Kazhdan\u2013Lusztig polynomials [KL79] have long been studied for their deep connections to Hecke "
  },
  "reinforcement_learning_papers/03_multi_agent_rl/Zero_Sum_Positional_RL.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach  Anton Plaksin 1 Vitaly Kalev 2  arXiv:2405.02044v1 [cs.LG] 3 May 2024  Abstract  (see also Robust Adversarial RL (Pinto et al., 2017)), in which such uncertainty or disturbances are interpreted as actions of a second adversarial agent, and thus the problem is reduced to seeking the agents\u2019 policies robust to any opponent\u2019s actions.  Robust Reinforcement Learning (RRL) is a promising Reinforcement Learning (RL) paradigm aimed at training robust to uncertainty or disturbances models, making them more efficient for real-world applications. Following this paradigm, uncertainty or disturbances are interpreted as actions of a second adversarial agent, and thus, the problem is reduced to seeking the agents\u2019 policies robust to any opponent\u2019s actions. This paper is the first to propose considering the RRL problems within the positional differential game theory, which helps us to ob"
  },
  "reinforcement_learning_papers/03_multi_agent_rl/Multi_Agent_RL_Survey.pdf": {
    "category": "03_multi_agent_rl",
    "abstract": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning  Jiayu Chen 1 Le Xu 2 Wentse Chen 3 Jeff Schneider 3  1. Introduction  arXiv:2410.11234v3 [cs.LG] 11 Nov 2025  Abstract  The success of RL typically relies on large amounts of interactions with the environment. However, in real-world scenarios, such interactions can be unsafe or costly. As an alternative, offline RL (Levine et al., 2020; Chen et al., 2024a) leverages offline datasets of transitions, collected by a behavior policy, to train a policy. To avoid overestimation of the expected return for out-of-distribution states, which can mislead policy learning, model-free offline RL methods (Kumar et al., 2020; Wu et al., 2019) often constrain the learned policy to remain close to the behavior policy. However, acquiring a large volume of demonstrations from a high-quality behavior policy, can be expensive. This challenge has led to the development of offline modelbased reinforcement learning (MBRL) ap"
  },
  "reinforcement_learning_papers/07_model_based_rl/MuZero_Mastering_Atari.pdf": {
    "category": "07_model_based_rl",
    "abstract": "arXiv:1911.08265v2 [cs.LG] 21 Feb 2020  Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model Julian Schrittwieser,1\u2217 Ioannis Antonoglou,1,2\u2217 Thomas Hubert,1\u2217 Karen Simonyan,1 Laurent Sifre,1 Simon Schmitt,1 Arthur Guez,1 Edward Lockhart,1 Demis Hassabis,1 Thore Graepel,1,2 Timothy Lillicrap,1 David Silver1,2\u2217 1  2  DeepMind, 6 Pancras Square, London N1C 4AG. University College London, Gower Street, London WC1E 6BT. \u2217 These authors contributed equally to this work.  Abstract  Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman per"
  },
  "reinforcement_learning_papers/07_model_based_rl/Model_Based_RL_MLE.pdf": {
    "category": "07_model_based_rl",
    "abstract": "Model-based Reinforcement Learning for Parameterized Action Spaces  Renhao Zhang * 1 Haotian Fu * 1 Yilin Miao 1 George Konidaris 1  arXiv:2404.03037v3 [cs.LG] 24 May 2024  Abstract  or Robot Soccer (Hausknecht & Stone, 2016b).  We propose a novel model-based reinforcement learning algorithm\u2014Dynamics Learning and predictive control with Parameterized Actions (DLPA)\u2014for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.1  Compared to just discrete or continuous action space, Reinforcem"
  },
  "reinforcement_learning_papers/07_model_based_rl/Prioritized_Generative_Replay.pdf": {
    "category": "07_model_based_rl",
    "abstract": "TO APPEAR IN IEEE TRANSACTIONS ON BIOMETRICS, IDENTITY AND BEHAVIOR (T-BIOM)  1  CATFace: Cross-Attribute-Guided Transformer with Self-Attention Distillation for Low-Quality Face Recognition  arXiv:2401.03037v1 [cs.CV] 5 Jan 2024  Niloufar Alipour Talemi , Hossein Kashiani , and Nasser M. Nasrabadi , Fellow, IEEE Abstract\u2014Although face recognition (FR) has achieved great success in recent years, it is still challenging to accurately recognize faces in low-quality images due to the obscured facial details. Nevertheless, it is often feasible to make predictions about specific soft biometric (SB) attributes, such as gender, and baldness even in dealing with low-quality images. In this paper, we propose a novel multi-branch neural network that leverages SB attribute information to boost the performance of FR. To this end, we propose a cross-attribute-guided transformer fusion (CATF) module that effectively captures the long-range dependencies and relationships between FR and SB feature rep"
  },
  "reinforcement_learning_papers/07_model_based_rl/World_Models.pdf": {
    "category": "07_model_based_rl",
    "abstract": "World Models David Ha 1 Ju\u0308rgen Schmidhuber 2 3  arXiv:1803.10122v4 [cs.LG] 9 May 2018  Abstract We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io  1. Introduction Humans develop a mental model of the world based on what they are able to perceive with their limited senses. The decisions and actions we make are based on this internal model. Jay Wright Forrester, the father of system dynamics, described a m"
  },
  "reinforcement_learning_papers/07_model_based_rl/Bayes_Adaptive_MCT.pdf": {
    "category": "07_model_based_rl",
    "abstract": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning  Jiayu Chen 1 Le Xu 2 Wentse Chen 3 Jeff Schneider 3  1. Introduction  arXiv:2410.11234v3 [cs.LG] 11 Nov 2025  Abstract  The success of RL typically relies on large amounts of interactions with the environment. However, in real-world scenarios, such interactions can be unsafe or costly. As an alternative, offline RL (Levine et al., 2020; Chen et al., 2024a) leverages offline datasets of transitions, collected by a behavior policy, to train a policy. To avoid overestimation of the expected return for out-of-distribution states, which can mislead policy learning, model-free offline RL methods (Kumar et al., 2020; Wu et al., 2019) often constrain the learned policy to remain close to the behavior policy. However, acquiring a large volume of demonstrations from a high-quality behavior policy, can be expensive. This challenge has led to the development of offline modelbased reinforcement learning (MBRL) ap"
  },
  "reinforcement_learning_papers/07_model_based_rl/DreamerV3_Mastering_Diverse_Domains.pdf": {
    "category": "07_model_based_rl",
    "abstract": "Mastering Diverse Domains through World Models Danijar Hafner,12 Jurgis Pasukonis,1 Jimmy Ba,2 Timothy Lillicrap1  Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achie"
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Extracting_Training_Data_from_LLMs.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Interactive_Tools_Assist_LM_Agents_Security_Vulnerabilities.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/AgentPoison_Red-teaming_LLM_Agents.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/The_Secret_Sharer_Unintended_Memorization.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/DecodingTrust_Trustworthiness_GPT_Models.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Representation_Engineering_AI_Transparency.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/DataSentinel_Game-Theoretic_Detection_Prompt_Injection.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Big_Sleep_LLM_Vulnerabilities_Real-World.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Progent_Programmable_Privilege_Control.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/19_llm_security_and_redteaming/Privtrans_Privilege_Separation.pdf": {
    "category": "19_llm_security_and_redteaming",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Composing_Global_Optimizers_Algebraic_Objects.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Large_Language_Models_as_Optimizers.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Premise_Order_Matters_in_Reasoning_with_Large_Language_Models.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Chain-of-Verification_Reduces_Hallucination.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Symbolic_Regression_Learned_Concept_Library.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Chain-of-Thought_Reasoning_Without_Prompting.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Chain-of-Thought_Empowers_Transformers_to_Solve_Inherently_Serial_Problems.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Large_Language_Models_Cannot_Self-Correct_Reasoning_Yet.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Beyond_A-Star_Better_Planning_Transformers.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/18_llm_reasoning_and_prompting/Grokked_Transformers_are_Implicit_Reasoners.pdf": {
    "category": "18_llm_reasoning_and_prompting",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/11_meta_continual_rl/CRL_Easy_to_Hard.pdf": {
    "category": "11_meta_continual_rl",
    "abstract": "C URRICULUM R EINFORCEMENT L EARNING FROM E ASY TO H ARD TASKS I MPROVES LLM R EASONING Shubham Parashar1\u2217 Shurui Gui1\u2217 Xiner Li1\u2217 Hongyi Ling1 Sushil Vemuri2 Blake Olson1 Eric Li1 Yu Zhang1 James Caverlee1 Dileep Kalathil2 Shuiwang Ji1\u2020 1 Department of Computer Science & Engineering, Texas A&M University 2 Department of Electrical & Computer Engineering, Texas A&M University  arXiv:2506.06632v2 [cs.LG] 2 Nov 2025  A BSTRACT We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although eas"
  },
  "reinforcement_learning_papers/11_meta_continual_rl/RL_on_Pre_Training_Data.pdf": {
    "category": "11_meta_continual_rl",
    "abstract": "2025-09-26  Reinforcement Learning on Pre-Training Data Siheng Li1,3, \u2217 ,\u2020 , Kejiao Li1,\u2020 , Zenan Xu1,\u2020 , Guanhua Huang1 , Evander Yang1 , Kun Li1,3,\u2217 , Haoyuan Wu1 , Jiajia Wu1 , Zihao Zheng1 , Chenchen Zhang1 , Kun Shi1 , Kyrierl Deng1 , Qi Yi1 , Ruibin Xiong1 , Tingqiang Xu1,\u2217 , Yuhao Jiang1 , Jianfeng Yan1 , Yuyuan Zeng1 , Guanghui Xu1 , Jinbao Xue2 , Zhijiang Xu2 , Zheng Fang2 , Shuai Li2 , Qibin Liu2 , Xiaoxue Li2 , Zhuoyu Li2 , Yangyu Tao2 , Fei Gao2 , Cheng Jiang2 , Bo Chao Wang2 , Kai Liu2 , Jianchen Zhu2 , Wai Lam3 , Bo Zhou1,\u2021, Di Wang1 1 LLM Department, Tencent 2 HunYuan Infra Team 3 The Chinese University of Hong Kong B chaysezhou@tencent.com  The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling parad"
  },
  "reinforcement_learning_papers/11_meta_continual_rl/Warm_Start_RL.pdf": {
    "category": "11_meta_continual_rl",
    "abstract": "Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data Zhiyuan Zhou*1 , Andy Peng*1 , Qiyang Li1 , Sergey Levine1 , Aviral Kumar2  arXiv:2412.07762v3 [cs.LG] 2 Jul 2025  1 UC Berkeley, 2 Carnegie Mellon University  (* Equal Contribution)  The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. However, this is undesirable because training on diverse offline data is slow and expensive for large datasets, and should, in principle, also limit the performance improvement possible because of constraints or pessimism on offline data. In this paper, we show that retaining offline data is unnecessary as long as we use a properly-designed "
  },
  "reinforcement_learning_papers/11_meta_continual_rl/TTRL_Overview.pdf": {
    "category": "11_meta_continual_rl",
    "abstract": "TTRL: Test-Time Reinforcement Learning  TTRL: Test-Time Reinforcement Learning Yuxin Zuo\u22171,2 Kaiyan Zhang\u22171 Li Sheng1,2 Shang Qu1,2 Ganqu Cui2 Xuekai Zhu1 Haozhan Li1,2 Yuchen Zhang2 Xinwei Long1 Ermo Hua1 Biqing Qi2 Youbang Sun1 Zhiyuan Ma1 Lifan Yuan1 Ning Ding\u20201,2 Bowen Zhou\u20201,2 1 Tsinghua University 2 Shanghai AI Lab  Abstract This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models."
  },
  "reinforcement_learning_papers/11_meta_continual_rl/TTRL_Test_Time_RL.pdf": {
    "category": "11_meta_continual_rl",
    "abstract": "TTRL: Test-Time Reinforcement Learning  TTRL: Test-Time Reinforcement Learning Yuxin Zuo\u22171,2 Kaiyan Zhang\u22171 Li Sheng1,2 Shang Qu1,2 Ganqu Cui2 Xuekai Zhu1 Haozhan Li1,2 Yuchen Zhang2 Xinwei Long1 Ermo Hua1 Biqing Qi2 Youbang Sun1 Zhiyuan Ma1 Lifan Yuan1 Ning Ding\u20201,2 Bowen Zhou\u20201,2 1 Tsinghua University 2 Shanghai AI Lab  Abstract This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models."
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/ProShield_RL.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning Edwin Hamel-De le Court\u2217  Imperial College London, United Kingdom e.hamel-de-le-court@imperial.ac.uk  Gaspard Ohlmann\u2217  Mulhouse, France gaspard.ohlmann@outlook.com  arXiv:2510.15720v2 [cs.LG] 21 Oct 2025  ABSTRACT Safety is a major concern in reinforcement learning (RL): we aim at developing RL systems that not only perform optimally, but are also safe to deploy by providing formal guarantees about their safety. To this end, we introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free algorithm for safe reinforcement learning under cost constraints. ProSh augments the Constrained MDP state space with a risk budget and enforces safety by applying a shield to the agent\u2019s policy distribution using a learned cost critic. The shield ensures that all sampled actions remain safe in expectation. We also show that optimality is preserved when the environment is deterministic. Since ProSh is model-free, saf"
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/SCPO_Constrained.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "Published in Transactions on Machine Learning Research (04/2024)  State-wise Constrained Policy Optimization Weiye Zhao  weiyezha@andrew.cmu.edu  Robotics Institute Carnegie Mellon University  Rui Chen  ruic3@andrew.cmu.edu  arXiv:2306.12594v3 [cs.LG] 17 Jun 2024  Robotics Institute Carnegie Mellon University  Yifan Sun  yifansu2@andrew.cmu.edu  Robotics Institute Carnegie Mellon University  Feihan Li  feihanl@andrew.cmu.edu  Robotics Institute Carnegie Mellon University  Tianhao Wei  twei2@andrew.cmu.edu  Robotics Institute Carnegie Mellon University  Changliu Liu  cliu6@andrew.cmu.edu  Robotics Institute Carnegie Mellon University Reviewed on OpenReview: https: // openreview. net/ forum? id= NgK5etmhz9  Abstract Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many"
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/IP3O_Safe_RL.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "Educational impacts of generative artificial intelligence on learning and performance of engineering students in China Lei Fan*, Kunyang Deng and Fangxue Liu Department of Civil Engineering, Design School, Xi\u2019an Jiaotong-Liverpool University, Suzhou, China *Corresponding Author (Email: Lei.Fan@xjtlu.edu.cn) Abstract: With the rapid advancement of generative artificial intelligence (AI), its potential applications in higher education have attracted significant attention. This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education. Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students\u2019 learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption "
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/Risk_Sensitive_RL.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression  arXiv:2506.06954v2 [cs.LG] 7 Dec 2025  Clinton Enwerem, Aniruddh G. Puranic, John S. Baras, and Calin Belta Institute for Systems Research University of Maryland, College Park, United States {enwerem, puranic, baras, calin}@umd.edu  Abstract: Mainstream approximate action-value iteration reinforcement learning (RL) algorithms suffer from overestimation bias, leading to suboptimal policies in high-variance stochastic environments. Quantile-based action-value iteration methods reduce this bias by learning a distribution of the expected cost-to-go using quantile regression. However, ensuring that the learned policy satisfies safety constraints remains a challenge when these constraints are not explicitly integrated into the RL framework. Existing methods often require complex neural architectures or manual tradeoffs due to combined cost functions. To address this, we propose "
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/Probabilistic_Shielding.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "Probabilistic Shielding for Safe Reinforcement Learning Edwin Hamel-De le Court, Francesco Belardinelli, Alexander W. Goodall  arXiv:2503.07671v3 [stat.ML] 25 Mar 2025  Imperial College London {e.hamel-de-le-court, francesco.belardinelli, a.goodall22}@ic.ac.uk  Abstract In real-life scenarios, a Reinforcement Learning (RL) agent aiming to maximise their reward, must often also behave in a safe manner, including at training time. Thus, much attention in recent years has been given to Safe RL, where an agent aims to learn an optimal policy among all policies that satisfy a given safety constraint. However, strict safety guarantees are often provided through approaches based on linear programming, and thus have limited scaling. In this paper we present a new, scalable method, which enjoys strict formal guarantees for Safe RL, in the case where the safety dynamics of the Markov Decision Process (MDP) are known, and safety is defined as an undiscounted probabilistic avoidance property. Our "
  },
  "reinforcement_learning_papers/05_safe_constrained_rl/Constrained_RL_Average_Reward.pdf": {
    "category": "05_safe_constrained_rl",
    "abstract": "arXiv:2406.11481v3 [cs.LG] 17 Jul 2024  Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms  Vaneet Aggarwal Purdue University vaneet@purdue.edu Washim Uddin Mondal Indian Institute of Technology Kanpur wmondal@iitk.ac.in Qinbo Bai Purdue University bai113@purdue.edu  \fContents  1 Introduction 1.1 Chapter Organization . . . . . . . . . . . . . . . . . . . . 1.2 Some Useful Inequalities . . . . . . . . . . . . . . . . . .  3 5 6  2 Model-Based RL 2.1 Overall Model and Assumptions . . . . . . . . . . . . . . 2.2 Algorithms for Model-Based RL . . . . . . . . . . . . . . . 2.3 Regret Analysis and Constraint Violation for Optimism Based Approach . . . . . . . . . . . . . . . . . . . . . . . 2.4 Regret Analysis and Constraint Violation for Posterior Sampling Based Approach . . . . . . . . . . . . . . . . . . . . 2.5 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . 2.6 Notes and Open Problems . . . . . . . . . . . . . . . . .  "
  },
  "reinforcement_learning_papers/15_multi_objective_rl/Latent_Conditioned_PG_MORL.pdf": {
    "category": "15_multi_objective_rl",
    "abstract": "Curiosity-driven Exploration by Self-supervised Prediction  Deepak Pathak 1 Pulkit Agrawal 1 Alexei A. Efros 1 Trevor Darrell 1  arXiv:1705.05363v1 [cs.LG] 15 May 2017  Abstract In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\u2019s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrins"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Preference_Based_RL.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options  arXiv:2510.18713v2 [cs.LG] 11 Nov 2025  Joongkyu Lee Seoul National University jklee0717@snu.ac.kr  Seouh-won Yi Seoul National University uniqueseouh@snu.ac.kr  Min-hwan Oh Seoul National University minoh@snu.ac.kr  Abstract We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged\u2014motivated by PbRL\u2019s recent empirical success, particularly in aligning large language models (LLMs)\u2014most existing studies focus only on pairwise comparisons. A few recent works [92, 49, 76] have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve\u2014and can even deteriorate\u2014as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett\u2013Luce (PL) model for ranking feedback over action subsets and propos"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RL_with_Rubric_Anchors.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Reinforcement Learning with Rubric Anchors Zenan Huang\u2217 , Yihong Zhuang\u2217 , Guoshan Lu\u2217 , Zeyu Qin\u2217 , Haokai Xu\u2217 , Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, Junbo Zhao\u2020  arXiv:2508.12790v1 [cs.AI] 18 Aug 2025  Inclusion AI, Ant Group, Zhejiang University  Abstract Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI\u2019s o-series. In RLVR, rewards are derived from deterministic, programmatically verifiable signals\u2014such as passing unit tests in code generation or matching the correct numerical answer in mathematical reasoning. While effective, this requirement for unambiguous correctness largely confines RLVR to domains with clear, automatically checkable outcomes. To overcome this limitation, we extend the RLVR paradigm beyond strictly ve"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Online_Exploration_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback Gen Li\u2217\u2020  Yuling Yan\u2217\u2021  arXiv:2509.22633v1 [stat.ML] 26 Sep 2025  September 29, 2025  Abstract Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insi"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Multi_Turn_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Multi-turn Reinforcement Learning from Preference Human Feedback  arXiv:2405.14655v2 [cs.LG] 2 Dec 2024  Lior Shani \u2217 1 liorshani@google.com Oran Lang 1 Bilal Piot 2  Aviv Rosenberg \u2217 1 avivros@google.com  Daniele Calandriello 2 Idan Szpektor 1  Avital Zipori 1  Avinatan Hassidim 1  Asaf Cassel \u2217 1 3 acassel@mail.tau.ac.il Hila Noga 1  Orgad Keller 1  Yossi Matias 1  R\u00e9mi Munos 2  Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular sett"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/OpenAI_O1_Replication_Journey.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RLHF_Workflow.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Published in Transactions on Machine Learning Research (09/2024)  RLHF Workflow: From Reward Modeling to Online RLHF A Comprehensive Practical Alignment Recipe of Iterative Preference Learning Hanze Dong1\u2217 Han Zhao2  Wei Xiong2\u2217 Yingbo Zhou1  arXiv:2405.07863v3 [cs.LG] 12 Nov 2024  Caiming Xiong1\u2020 1  Salesforce AI Research  Bo Pang1\u2217 Nan Jiang2  Haoxiang Wang2\u2217 Doyen Sahoo1  Tong Zhang2\u2020 2  University of Illinois Urbana-Champaign  Reviewed on OpenReview: https://openreview.net/forum?id=a13aYUU9eU  Abstract We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative "
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DeepSeek_Janus_Pro_Multimodal.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RLHF_Deciphered.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "arXiv:2404.08555v2 [cs.LG] 16 Apr 2024  RLHF D ECIPHERED : A C RITICAL A NALYSIS OF R EINFORCEMENT L EARNING FROM H UMAN F EEDBACK FOR LLM S  Shreyas Chaudhari*1  Pranjal Aggarwal*2  Ashwin Kalyan5  Karthik Narasimhan3  Vishvak Murahari3 Ameet Deshpande3  Tanmay Rajpurohit4  Bruno Castro da Silva1  1  2  University of Massachusetts Amherst Department of Computer Science, Indian Institute of Technology, Delhi 3 Department of Computer Science, Princeton University 4 Georgia Tech 5 Independent Researcher * Equal Contribution schaudhari@cs.umass.edu, pranjal2041@gmail.com  A BSTRACT State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Y"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DeepSeek-R1_Reasoning_via_RL.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Direct_Preference_Optimization.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/R-Search_Multi-Step_Reasoning.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RLHF_Principled.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons Banghua Zhu\u2020 \u2020  Michael I. Jordan\u2020, \u2021  Jiantao Jiao\u2020, \u2021  Department of Electrical Engineering and Computer Sciences, UC Berkeley \u2021 Department of Statistics, UC Berkeley  arXiv:2301.11270v5 [cs.LG] 8 Feb 2024  Abstract We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the K-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotica"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RL_with_Foundation_Priors.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own  arXiv:2310.02635v4 [cs.RO] 11 Oct 2024  Weirui Ye123 Yunsheng Zhang23 Haoyang Weng1 Xianfan Gu2 Shengjie Wang123 Tong Zhang123 Mengchen Wang1 Pieter Abbeel4 Yang Gao123 \u2217 1 Tsinghua University, 2 Shanghai Qi Zhi Institute 3 Shanghai Artificial Intelligence Laboratory, 4 UC Berkeley  Abstract: Reinforcement learning (RL) is a promising approach for solving robotic manipulation tasks. However, it is challenging to apply the RL algorithms directly in the real world. For one thing, RL is data-intensive and typically requires millions of interactions with environments, which are impractical in real scenarios. For another, it is necessary to make heavy engineering efforts to design reward functions manually. To address these issues, we leverage foundation models in this paper. We propose Reinforcement Learning with Foundation Priors (RLFP) to utilize guidance and feedback from policy, value, a"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RLHF_Survey.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "A Survey of Reinforcement Learning from Human Feedback Timo Kaufmann  timo.kaufmann@ifi.lmu.de  LMU Munich, MCML Munich  Paul Weng  paul.weng@dukekunshan.edu.cn  Digital Innovation Research Center, Duke Kunshan University  Viktor Bengs  viktor.bengs@dfki.de  arXiv:2312.14925v3 [cs.LG] 28 Dec 2025  German Research Center for Artificial Intelligence (DFKI)  Eyke H\u00fcllermeier  eyke@lmu.de  LMU Munich, MCML Munich, DFKI Kaiserslautern  Abstract Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning provides a promising approach to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Guided_GRPO_Adaptive_Guidance.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RL_from_Human_Preferences.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "arXiv:1706.03741v4 [stat.ML] 17 Feb 2023  Deep Reinforcement Learning from Human Preferences Paul F Christiano OpenAI paul@openai.com Miljan Martic DeepMind miljanm@google.com  Jan Leike DeepMind leike@google.com Shane Legg DeepMind legg@google.com  Tom B Brown nottombrown@gmail.com  Dario Amodei OpenAI damodei@openai.com  Abstract For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent\u2019s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate th"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RLHF_Open_Problems.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback  arXiv:2307.15217v2 [cs.AI] 11 Sep 2023  Stephen Casper,\u2217 MIT CSAIL, scasper@mit.edu Xander Davies,\u2217 Harvard University Claudia Shi, Columbia University Thomas Krendl Gilbert, Cornell Tech J\u00e9r\u00e9my Scheurer, Apollo Research Javier Rando, ETH Zurich Rachel Freedman, UC Berkeley Tomasz Korbak, University of Sussex David Lindner, ETH Zurich Pedro Freire, Independent Tony Wang, MIT CSAIL Samuel Marks, Harvard University Charbel-Rapha\u00ebl Segerie, EffiSciences Micah Carroll, UC Berkeley Andi Peng, MIT CSAIL Phillip Christoffersen, MIT CSAIL Mehul Damani, MIT CSAIL Stewart Slocum, MIT CSAIL Usman Anwar, University of Cambridge Anand Siththaranjan, UC Berkeley Max Nadeau, Harvard University Eric J. Michaud, MIT Jacob Pfau, New York University Dmitrii Krasheninnikov, University of Cambridge Xin Chen, ETH Zurich Lauro Langosco, University of Cambridge Peter Hase, UNC Chapel Hill Erdem B\u0131y\u0131k, University of Southe"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RFT_Powers_Multimodal_Reasoning.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/ReMax_Simple.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "arXiv:2301.10505v1 [math.CA] 25 Jan 2023  Asymptotically uniform functions: a single hypothesis which solves two old problems Jean\u2013Pierre Gabriel\u2217 and Jean\u2013Paul Berrut\u2217 January 26, 2023  Abstract The asymptotic study of a time-dependent function f as the solution of a differential equation often leads to the question of whether its derivative f\u02d9 vanishes at infinity. We show that a necessary and sufficient condition for this is that f\u02d9 is what may be called asymptotically uniform. We generalize the result to higher order derivatives. We further show that the same property for f itself is also necessary and sufficient for its one-sided improper integrals to exist. On the way, the article provides a broad study of such asymptotically uniform functions. Math Subject Classification: 26A09, 26A12, 34A45 Keywords: asymptotically uniform function, vanishing of a derivative at infinity, vanishing of an integrand at infinity, Hadamard\u2019s lemma, Barba\u0306lat\u2019s lemma  1  Introduction  When does the d"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DPO_Direct_Preference_Optimization.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model  arXiv:2305.18290v3 [cs.LG] 29 Jul 2024  Rafael Rafailov\u2217\u2020  Archit Sharma\u2217\u2020  Stefano Ermon\u2020\u2021  Christopher D. Manning\u2020  Eric Mitchell\u2217\u2020 Chelsea Finn\u2020  \u2020  Stanford University \u2021 CZ Biohub {rafailov,architsh,eric.mitchell}@cs.stanford.edu  Abstract While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximi"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Contrastive_Rewards_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards  Wei Shen * 1 Xiaoying Zhang * 2 Yuanshun Yao 2 Rui Zheng 1 Hongyi Guo 3 Yang Liu 2  arXiv:2403.07708v2 [cs.CL] 14 Mar 2024  Abstract  2023a). Despite the successes, the effectiveness of RLHF relies heavily on the reward model (RM) used in the Proximal Policy Optimization (PPO) (Schulman et al., 2017) stage to guide the learning process.  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named contrastive rewards. Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as "
  },
  "reinforcement_learning_papers/02_rlhf_alignment/ReMax_Simple_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models  Ziniu Li 1 2 Tian Xu 3 4 Yushun Zhang 1 2 Zhihang Lin 1 Yang Yu 3 4 5 \u2020 Ruoyu Sun 1 6 2 \u2020 Zhi-Quan Luo 1 2  arXiv:2310.10505v4 [cs.LG] 16 May 2024  Abstract  >4 Hyper-parameters related to the Value Model Op mizer States of the Value Model  Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large Language Models (LLMs), typically paired with the Proximal Policy Optimization (PPO) algorithm. While PPO is a powerful method designed for general reinforcement learning tasks, it is overly sophisticated for LLMs, leading to laborious hyper-parameter tuning and significant computation burdens. To make RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. These properties are not exploited in PPO, making it less suitable for RLHF. Building on the renowned REINFORCE algorithm, ReM"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/s1_Simple_Test-Time_Scaling.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/STILL-2_Distilling_Reasoning.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Unpacking_DPO_and_PPO.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Parameter_Efficient_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Parameter Efficient Reinforcement Learning from Human Feedback Hakim Sidahmed\u22171 , Samrat Phatale\u22171 , Alex Hutcheson2 , Zhuonan Lin2 , Zhang Chen2 , Zac Yu2 , Jarvis Jin2 , Simral Chaudhary1 , Roman Komarytsia2 , Christiane Ahlheim2 , Yonghao Zhu2 , Bowen Li2 , Saravanan Ganesh2 , Bill Byrne2 , Jessica Hoffmann1 , Hassan Mansoor1 , Wei Li1 , Abhinav Rastogi1 , Lucas Dixon1 1 Google DeepMind, 2 Google {hsidahmed, samratph, ldixon}@google.com  Abstract  arXiv:2403.10704v2 [cs.LG] 12 Sep 2024  While Reinforcement Learning from Human Feedback (RLHF) effectively aligns pretrained Large Language and VisionLanguage Models (LLMs, and VLMs) with human preferences, its computational cost and complexity hamper its wider adoption. To alleviate some of the computational burden of fine-tuning, parameter efficient methods, like LoRA (Hu et al., 2021) were introduced. In this work, we empirically evaluate the setup of Parameter Efficient Reinforcement Learning from Human Feedback (PE-RLHF) that leverag"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/REBEL_Reinforcement_Learning.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "REBEL: Reinforcement Learning via Regressing Relative Rewards  arXiv:2404.16767v4 [cs.LG] 10 Dec 2024  Zhaolin Gao Cornell University zg292@cornell.edu Wenhao Zhan Princeton University wenhao.zhan@princeton.edu Kiant\u00e9 Brantley Harvard University kdbrantley@g.harvard.edu  Jonathan D. Chang Cornell University jdc396@cornell.edu Owen Oertell Cornell University ojo2@cornell.edu  Gokul Swamy Carnegie Mellon University gswamy@andrew.cmu.edu  Thorsten Joachims Cornell University tj@cs.cornell.edu  J. Andrew Bagnell Aurora Innovation, CMU bagnell2@andrew.cmu.edu  Jason D. Lee Princeton University jasonlee@princeton.edu  Wen Sun Cornell University ws455@cornell.edu  Abstract While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value netwo"
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DeepSeek_R1_Implications_for_AI.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DeepSeek_R1_Reasoning_Models_Faithful.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Qwen_QwQ_Reasoning_Model.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/RL_Fine-tuning_Instruction_Following.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/DeepSeekMath_GRPO.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/Sky-T1_Training_Small_Reasoning_LLMs.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/02_rlhf_alignment/MA_RLHF.pdf": {
    "category": "02_rlhf_alignment",
    "abstract": "Published as a conference paper at ICLR 2025  MA-RLHF: R EINFORCEMENT L EARNING FROM H U MAN F EEDBACK WITH M ACRO ACTIONS  arXiv:2410.02743v2 [cs.CL] 14 Feb 2025  Yekun Chai\u2217 Haoran Sun\u2217 Huang Fang Shuohuan Wang Yu Sun Hua Wu Baidu Inc. {chaiyekun,fanghuang,wangshuohuan}@baidu.com sunhaoran0402@gmail.com  A BSTRACT Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to preferred outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions \u2014 sequences of tokens or higher-level language constructs \u2014 into the learning process. By operating at higher level of abstraction, our approach reduces the temporal dis"
  },
  "reinforcement_learning_papers/12_generalization_procgen/Switching_Loss_Cost.pdf": {
    "category": "12_generalization_procgen",
    "abstract": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining  arXiv:2310.08566v2 [cs.LG] 26 May 2024  Licong Lin\u2217  Yu Bai\u2020\u00a7  Song Mei\u2021\u00a7  May 28, 2024 Abstract Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods \u2014 algorithm distillation and decision-pretrained transformers. First, assuming mode"
  },
  "reinforcement_learning_papers/12_generalization_procgen/EXPO_Stable_RL.pdf": {
    "category": "12_generalization_procgen",
    "abstract": "EXPO: Stable Reinforcement Learning with Expressive Policies  arXiv:2507.07986v2 [cs.LG] 15 Jul 2025  Perry Dong Stanford University  Qiyang Li UC Berkeley  Dorsa Sadigh Stanford University  Chelsea Finn Stanford University  A BSTRACT We study the problem of training and fine-tuning expressive policies with online reinforcement learning (RL) given an offline dataset. Training expressive policy classes with online RL present a unique challenge of stable value maximization. Unlike simpler Gaussian policies commonly used in online RL, expressive policies like diffusion and flow-matching policies are parameterized by a long denoising chain, which hinders stable gradient propagation from actions to policy parameters when optimizing against some value function. Our key insight is that we can address stable value maximization by avoiding direct optimization over value with the expressive policy and instead construct an on-the-fly RL policy to maximize Q-value. We propose EXpressive Policy Opt"
  },
  "reinforcement_learning_papers/06_curiosity_exploration/NeoRL_Efficient_Exploration.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "N EO RL: Efficient Exploration for Nonepisodic RL  arXiv:2406.01175v4 [cs.LG] 11 Feb 2025  Bhavya Sukhija\u2217, Lenart Treven, Florian D\u00f6rfler, Stelian Coros, Andreas Krause ETH Zurich, Switzerland  Abstract We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., adapt online and without resets. This setting is ubiquitous in the real world, where resetting is impossible or requires human intervention. We propose Nonepisodic Optimistic RL (N EO RL), an approach based on the principle of optimism in the face of uncertainty. N EO RL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy\u221a assumptions on the system, we provide a first-of-its-kind regret bound of O(\u0393T T ) for general nonlinear systems with Gaussian process dynamics. We compare N EO R"
  },
  "reinforcement_learning_papers/06_curiosity_exploration/Unsupervised_Data_Generation.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective from Model Shuncheng He1 , Hongchang Zhang1 , Jianzhun Shao1 , Yuhang Jiang1 , Xiangyang Ji1 1  arXiv:2506.19643v1 [cs.LG] 24 Jun 2025  Tsinghua University hesc16@mails.tsinghua.edu.cn  Abstract Offline reinforcement learning (RL) recently gains growing interests from RL researchers. However, the performance of offline RL suffers from the out-of-distribution problem, which can be corrected by feedback in online RL. Previous offline RL research focuses on restricting the offline algorithm in in-distribution even in-sample action sampling. In contrast, fewer work pays attention to the influence of the batch data. In this paper, we first build a bridge over the batch data and the performance of offline RL algorithms theoretically, from the perspective of model-based offline RL optimization. We draw a conclusion that, with mild assumptions, the distance between the state-action pair distribution generated by the"
  },
  "reinforcement_learning_papers/06_curiosity_exploration/RND_Exploration.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "E XPLORATION BY R ANDOM N ETWORK D ISTILLATION Yuri Burda\u2217 OpenAI  Harrison Edwards\u2217 OpenAI  Amos Storkey Univ. of Edinburgh  Oleg Klimov OpenAI  We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma\u2019s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access "
  },
  "reinforcement_learning_papers/06_curiosity_exploration/Curiosity_Driven_Exploration.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "2021-09-17  Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration Oliver Groth1,2 , Markus Wulfmeier1 , Giulia Vezzani1 , Vibhavari Dasagi1,3 , Tim Hertweck1 , Roland Hafner1 , Nicolas Heess1 and Martin Riedmiller1  arXiv:2109.08603v1 [cs.LG] 17 Sep 2021  1  DeepMind, 2 University of Oxford, 3 Queensland University of Technology  Curiosity-based reward schemes can present powerful exploration mechanisms which facilitate the discovery of solutions for complex, sparse or long-horizon tasks. However, as the agent learns to reach previously unexplored spaces and the objective adapts to reward new areas, many behaviours emerge only to disappear due to being overwritten by the constantly shifting objective. We argue that merely using curiosity for fast environment exploration or as a bonus reward for a specific task does not harness the full potential of this technique and misses useful skills. Instead, we propose to shift the focus towards retaining the b"
  },
  "reinforcement_learning_papers/06_curiosity_exploration/Curiosity_ICM.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "Curiosity-driven Exploration by Self-supervised Prediction  Deepak Pathak 1 Pulkit Agrawal 1 Alexei A. Efros 1 Trevor Darrell 1  arXiv:1705.05363v1 [cs.LG] 15 May 2017  Abstract In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\u2019s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrins"
  },
  "reinforcement_learning_papers/06_curiosity_exploration/Curiosity_Paper.pdf": {
    "category": "06_curiosity_exploration",
    "abstract": "Curiosity-driven Exploration by Self-supervised Prediction  Deepak Pathak 1 Pulkit Agrawal 1 Alexei A. Efros 1 Trevor Darrell 1  arXiv:1705.05363v1 [cs.LG] 15 May 2017  Abstract In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\u2019s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrins"
  },
  "reinforcement_learning_papers/16_applied_rl/GR00T_N1_Foundation_Model_Humanoid.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Autoformalizing_Euclidean_Geometry.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Lean-STaR_Interleave_Thinking_and_Proving.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/In-Context_Learning_Agent_Formal_Theorem-Proving.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Deep_Dive_Bio_Robotic.pdf": {
    "category": "16_applied_rl",
    "abstract": "arXiv:2408.03539v3 [cs.RO] 16 Sep 2024  Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes Chen Tang1,\u2217 , Ben Abbatematteo1,\u2217 , Jiaheng Hu1,\u2217 , Rohan Chandra2 , Roberto Mart\u0131\u0301n-Mart\u0131\u0301n1 , Peter Stone1,3 1  Department of Computer Science, The University of Texas at Austin, Austin, Texas 78712, United States; email: chen.tang@utexas.edu, abba@cs.utexas.edu, jiahengh@utexas.edu, robertomm@cs.utexas.edu, pstone@utexas.edu 2 Department of Computer Science, The University of Virginia, Charlottesville, Virginia 22904, United States; email: rohanchandra@virginia.edu 3 Sony AI \u2217 Equal Contribution  Xxxx. Xxx. Xxx. Xxx. YYYY. AA:1\u201342  Keywords  https://doi.org/10.1146/((please add article doi))  robotics, reinforcement learning, deep learning, learning for control, real-world applications  Copyright \u00a9 YYYY by the author(s). All rights reserved  Abstract Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has sh"
  },
  "reinforcement_learning_papers/16_applied_rl/Voyager_Open-Ended_Embodied_Agent.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Draft_Sketch_and_Prove_Formal_Theorem_Provers.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/ImProver_Agent-Based_Automated_Proof_Optimization.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/SLAC_Simulation-Pretrained_Latent_Action.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/LeanDojo_Theorem_Proving_Retrieval-Augmented.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/miniCTX_Neural_Theorem_Proving_Long-Contexts.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Autoformalization_with_Large_Language_Models.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Eureka_Human-Level_Reward_Design.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/DrEureka_Language_Model_Guided_Sim-To-Real.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/Gran_Turismo_Deep_Reinforcement_Learning.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/16_applied_rl/AlphaGeometry_Solving_Olympiad_Geometry.pdf": {
    "category": "16_applied_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/08_imitation_learning/Soft_Imitation_Learning.pdf": {
    "category": "08_imitation_learning",
    "abstract": "The Option Keyboard Combining Skills in Reinforcement Learning  arXiv:2106.13105v1 [cs.AI] 24 Jun 2021  Andr\u00e9 Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Ayg\u00fcn, Philippe Hamel, Daniel Toyama, Jonathan Hunt, Shibl Mourad, David Silver, Doina Precup {andrebarreto,borsa,shaobohou,gcomanici,eser}@google.com {hamelphi,kenjitoyama,jjhunt,shibl,davidsilver,doinap}@google.com  DeepMind  Abstract The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or \u201ccumulants\u201d). Based on this premise, we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning, we show how to"
  },
  "reinforcement_learning_papers/08_imitation_learning/Beyond_Expert_Performance_ILDE.pdf": {
    "category": "08_imitation_learning",
    "abstract": "Published as a conference paper at ICLR 2025  B EYOND -E XPERT P ERFORMANCE WITH L IMITED D EMONSTRATIONS : E FFICIENT I MITATION L EARNING WITH D OUBLE E XPLORATION  arXiv:2506.20307v1 [cs.LG] 25 Jun 2025  Heyang Zhao1\u2217 , Xingrui Yu23\u2217 , David M. Bossens23\u2217 , Ivor W. Tsang23\u2020 , Quanquan Gu1\u2020 1 Department of Computer Science, University of California, Los Angeles 2 IHPC, Agency for Science, Technology and Research, Singapore 3 CFAR, Agency for Science, Technology and Research, Singapore {hyzhao,qgu}@cs.ucla.edu {yu xingrui,david bossens,ivor tsang}@cfar.a-star.edu.sg  A BSTRACT Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert\u2019s behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome t"
  },
  "reinforcement_learning_papers/14_systems_modular_rl/ML_Agent_Autonomous_Engineering.pdf": {
    "category": "14_systems_modular_rl",
    "abstract": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering  arXiv:2505.23723v1 [cs.CL] 29 May 2025  Zexi Liu1 2 * 1  Jingyi Chai1 2 * Xinyu Zhu1 Shuo Tang1 Bo Zhang2 Lei Bai2 \u2020 Siheng Chen1 \u2020  Rui Ye1  Shanghai Jiao Tong University 2 Shanghai AI Laboratory * Equal Contribution \u2020 Corresponding Author ML-Agent: https://github.com/zeroxleo/ML-Agent  Abstract The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched"
  },
  "reinforcement_learning_papers/14_systems_modular_rl/RL_Optimization.pdf": {
    "category": "14_systems_modular_rl",
    "abstract": "ROLL  2025-06-13  Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library ROLL Team https://github.com/alibaba/ROLL  arXiv:2506.06122v1 [cs.LG] 6 Jun 2025  Abstract The remarkable success of Reinforcement Learning (RL) in advancing Large Language Models (LLMs) has spurred the development of efficient RL training frameworks. These frameworks, however, entail the coordinated management of multiple models and multi-stage pipelines, presenting challenges in efficiency, scalability, and usability. To respond, we introduce ROLL, an efficient, scalable, and user-friendly library designed for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters to three primary user groups: tech pioneers aiming for cost-effective, fault-tolerant large-scale training, developers requiring flexible control over training workflows, and researchers seeking agile experimentation. ROLL is built upon the following key modules to effectively ser"
  },
  "reinforcement_learning_papers/14_systems_modular_rl/RL_Machine_Learning_Engineering.pdf": {
    "category": "14_systems_modular_rl",
    "abstract": "Reinforcement Learning for Machine Learning Engineering Agents  arXiv:2509.01684v1 [cs.LG] 1 Sep 2025  Sherry Yang\u2217 Stanford University  Joy He-Yueya Stanford University  Percy Liang Stanford University  Abstract Existing agents for solving tasks such as ML engineering rely on prompting powerful language models. As a result, these agents do not improve with more experience. In this paper, we show that agents backed by weaker models that improve via reinforcement learning (RL) can outperform agents backed by much larger, but static models. We identify two major challenges with RL in this setting. First, actions can take a variable amount of time (e.g., executing code for different solutions), which leads to asynchronous policy gradient updates that favor faster but suboptimal solutions. To tackle variable-duration actions, we propose durationaware gradient updates in a distributed asynchronous RL framework to amplify high-cost but high-reward actions. Second, using only test split perfo"
  },
  "reinforcement_learning_papers/01_core_methods/MEPG_Ensemble_PG.pdf": {
    "category": "01_core_methods",
    "abstract": "arXiv:2303.08909v2 [cs.LG] 5 Oct 2024  Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning Takuya Kanazawa and Chetan Gupta Industrial AI Lab, Hitachi America, Ltd. R&D, Santa Clara, CA 95054, USA.  *Corresponding author(s). E-mail(s): takuya.kanazawa.cz@hitachi.com; Abstract Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design ch"
  },
  "reinforcement_learning_papers/01_core_methods/Trust_Region_Policy_Optimization.pdf": {
    "category": "01_core_methods",
    "abstract": "arXiv:1502.05477v5 [cs.LG] 20 Apr 2017  Trust Region Policy Optimization  John Schulman JOSCHU @ EECS . BERKELEY. EDU Sergey Levine SLEVINE @ EECS . BERKELEY. EDU Philipp Moritz PCMORITZ @ EECS . BERKELEY. EDU Michael Jordan JORDAN @ CS . BERKELEY. EDU Pieter Abbeel PABBEEL @ CS . BERKELEY. EDU University of California, Berkeley, Department of Electrical Engineering and Computer Sciences  Abstract We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as in"
  },
  "reinforcement_learning_papers/01_core_methods/Recursive_LS_Actor_Critic.pdf": {
    "category": "01_core_methods",
    "abstract": "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS  1  Recursive Least Squares Advantage Actor-Critic Algorithms  arXiv:2201.05918v2 [cs.LG] 14 Feb 2022  Yuan Wang, Chunyuan Zhang, Tianzong Yu, Meng Ma  Abstract\u2014As an important algorithm in deep reinforcement learning, advantage actor critic (A2C) has been widely succeeded in both discrete and continuous control tasks with raw pixel inputs, but its sample efficiency still needs to improve more. In traditional reinforcement learning, actor-critic algorithms generally use the recursive least squares (RLS) technology to update the parameter of linear function approximators for accelerating their convergence speed. However, A2C algorithms seldom use this technology to train deep neural networks (DNNs) for improving their sample efficiency. In this paper, we propose two novel RLS-based A2C algorithms and investigate their performance. Both proposed algorithms, called RLSSA2C and RLSNA2C, use the RLS method to train the critic netwo"
  },
  "reinforcement_learning_papers/01_core_methods/Deep_Q_Learning_Gradient.pdf": {
    "category": "01_core_methods",
    "abstract": "Deep Q-Learning with Gradient Target Tracking  arXiv:2503.16700v3 [cs.LG] 18 Jul 2025  Bumgeun Park\u2217 Electrical Engineering, KAIST j4t123@kaist.ac.kr  Taeho Lee \u2217 Electrical Engineering, KAIST eho0228@kaist.ac.kr  Donghwan Lee Electrical Engineering, KAIST donghwan@kaist.ac.kr  Abstract This paper introduces Q-learning with gradient target tracking, a novel reinforcement learning framework that provides a learned continuous target update mechanism as an alternative to the conventional hard update paradigm. In the standard deep Q-network (DQN), the target network is a copy of the online network\u2019s weights, held fixed for a number of iterations before being periodically replaced via a hard update. While this stabilizes training by providing consistent targets, it introduces a new challenge: the hard update period must be carefully tuned to achieve optimal performance. To address this issue, we propose two gradient-based target update methods: DQN with asymmetric gradient target tracking ("
  },
  "reinforcement_learning_papers/01_core_methods/Quantum_Advantage_Actor_Critic.pdf": {
    "category": "01_core_methods",
    "abstract": "Quantum Advantage Actor-Critic for Reinforcement Learning Michael Ko\u0308lle1\u2217 , Mohamad Hgog1\u2217 , Fabian Ritz1 , Philipp Altmann1 , Maximilian Zorn1 , Jonas Stein1 and Claudia Linnhoff-Popien1 1 Institute of Informatics, LMU Munich, Munich, Germany  arXiv:2401.07043v1 [quant-ph] 13 Jan 2024  michael.koelle@ifi.lmu.de  Keywords:  Quantum Computing, Quantum Reinforcement Learning, Advantage Actor-Critic  Abstract:  Quantum computing offers efficient encapsulation of high-dimensional states. In this work, we propose a novel quantum reinforcement learning approach that combines the Advantage Actor-Critic algorithm with variational quantum circuits by substituting parts of the classical components. This approach addresses reinforcement learning\u2019s scalability concerns while maintaining high performance. We empirically test multiple quantum Advantage Actor-Critic configurations with the well known Cart Pole environment to evaluate our approach in control tasks with continuous state spaces. Our re"
  },
  "reinforcement_learning_papers/01_core_methods/DQN_Playing_Atari.pdf": {
    "category": "01_core_methods",
    "abstract": "Playing Atari with Deep Reinforcement Learning  Volodymyr Mnih  Koray Kavukcuoglu  David Silver  arXiv:1312.5602v1 [cs.LG] 19 Dec 2013  Daan Wierstra  Alex Graves  Ioannis Antonoglou  Martin Riedmiller  DeepMind Technologies {vlad,koray,david,alex.graves,ioannis,daan,martin.riedmiller} @ deepmind.com  Abstract We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.  1  Introduction  Learning to control agents directly from high-dimensional sensory inputs like"
  },
  "reinforcement_learning_papers/01_core_methods/Policy_Shaped_Prediction.pdf": {
    "category": "01_core_methods",
    "abstract": "Policy-shaped prediction: avoiding distractions in model-based reinforcement learning  arXiv:2412.05766v1 [cs.LG] 8 Dec 2024  Miles Hutson Stanford University hutson@stanford.edu  Isaac Kauvar Stanford University ikauvar@stanford.edu  Nick Haber Stanford University nhaber@stanford.edu  Abstract Model-based reinforcement learning (MBRL) is a promising route to sampleefficient policy optimization. However, a known vulnerability of reconstructionbased MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods \u2014including DreamerV3 and DreamerPro\u2014 with a novel environment where background distractions are intricate, predictable, and useless for planning future actions"
  },
  "reinforcement_learning_papers/01_core_methods/Policy_Gradient_Definitive_Guide.pdf": {
    "category": "01_core_methods",
    "abstract": "arXiv:2401.13662v2 [cs.LG] 1 Mar 2024  The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations  Matthias Lehmann University of Cologne  A BSTRACT In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/ Matt00n/PolicyGrad"
  },
  "reinforcement_learning_papers/01_core_methods/Structured_RL_Combinatorial.pdf": {
    "category": "01_core_methods",
    "abstract": "Structured Reinforcement Learning for Combinatorial Decision-Making  arXiv:2505.19053v2 [cs.LG] 27 Oct 2025  Heiko Hoppe1  L\u00e9o Baty2 Louis Bouvier2 Axel Parmentier2 Maximilian Schiffer1 1 Technical University of Munich 2 \u00c9cole des Ponts {heiko.hoppe,schiffer}@tum.de {leo.baty,louis.bouvier,axel.parmentier}@enpc.fr  Abstract Reinforcement learning (RL) is increasingly applied to real-world problems involving complex and structured decisions, such as routing, scheduling, and assortment planning. These settings challenge standard RL algorithms, which struggle to scale, generalize, and exploit structure in the presence of combinatorial action spaces. We propose Structured Reinforcement Learning (SRL), a novel actor-critic paradigm that embeds combinatorial optimization-layers into the actor neural network. We enable end-to-end learning of the actor via Fenchel-Young losses and provide a geometric interpretation of SRL as a primal-dual algorithm in the dual of the moment polytope. Across si"
  },
  "reinforcement_learning_papers/01_core_methods/Deep_PG_No_Batch.pdf": {
    "category": "01_core_methods",
    "abstract": "Laminar and turbulence forced heat transfer convection correlations inside tubes. A review By: Gubran A.Q. Abdulrahmana,b , Sultan M. Alharbia a  Mechanical Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran 31261, Saudi Arabia b Interdisciplinary Research Center for Hydrogen Technology and Carbon Management (IRCHTCM), King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia  Abstract  This work proposes an extensive review of laminar and turbulent forced convective heat transfer correlations inside tubes by analyzing both experimental and computational research. Convective heat transfer is influenced by fluid turbulence and boundary layers, with geometry significantly impacting flow conditions. Nusselt number correlations quantifying the heat transfer coefficient are vital for various applications. Previous reviews are summarized regarding nanofluid heat transfer modeling approaches in circular tubes. Additionally, comparisons of tube shap"
  },
  "reinforcement_learning_papers/01_core_methods/Elastic_step_DQN.pdf": {
    "category": "01_core_methods",
    "abstract": "1  Deep Reinforcement Learning for Robotic Bipedal Locomotion: A Brief Survey  arXiv:2404.17070v6 [cs.RO] 9 Nov 2025  Lingfan Bao1 , Joseph Humphreys12 , Tianhu Peng1 and Chengxu Zhou1  Abstract\u2014Bipedal robots are gaining global recognition due to their potential applications and the rapid advancements in artificial intelligence, particularly through Deep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. This survey systematically categorises, compares, and analyses existing DRL frameworks for bipedal locomotion, organising them into end-to-end and hierarchical control schemes. Endto-end frameworks are evaluated based on their learning approaches, whereas hierarchical frameworks are examined in terms of their layered structures that integrate learning-based and traditional model-based methods. We provide a detailed evaluation of the composi"
  },
  "reinforcement_learning_papers/01_core_methods/DDPG_Continuous_Control.pdf": {
    "category": "01_core_methods",
    "abstract": "Published as a conference paper at ICLR 2016  C ONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING  arXiv:1509.02971v6 [cs.LG] 5 Jul 2019  Timothy P. Lillicrap\u2217, Jonathan J. Hunt\u2217, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver & Daan Wierstra Google Deepmind London, UK {countzero, jjhunt, apritzel, heess, etom, tassa, davidsilver, wierstra} @ google.com  A BSTRACT We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm wit"
  },
  "reinforcement_learning_papers/01_core_methods/Adaptive_QN_Network.pdf": {
    "category": "01_core_methods",
    "abstract": "Published as a conference paper at ICLR 2025  A DAPTIVE Q-N ETWORK : O N - THE - FLY TARGET S ELECTION FOR D EEP R EINFORCEMENT L EARNING The\u0301o Vincent1,2,\u2217 Fabian Wahren1,2 Jan Peters1,2,3 Boris Belousov1 Carlo D\u2019Eramo2,3,4 1 DFKI GmbH, SAIROL 2 Department of Computer Science, TU Darmstadt 3 Hessian.ai, TU Darmstadt 4 Center for AI and Data Science, University of Wu\u0308rzburg  arXiv:2405.16195v3 [cs.LG] 3 Mar 2025  A BSTRACT Deep Reinforcement Learning (RL) is well known for being highly sensitive to hyperparameters, requiring practitioners substantial efforts to optimize them for the problem at hand. This also limits the applicability of RL in real-world scenarios. In recent years, the field of automated Reinforcement Learning (AutoRL) has grown in popularity by trying to address this issue. However, these approaches typically hinge on additional samples to select well-performing hyperparameters, hindering sample-efficiency and practicality. Furthermore, most AutoRL methods are heavily "
  },
  "reinforcement_learning_papers/01_core_methods/Advantage_Actor_Critic_Reasoner.pdf": {
    "category": "01_core_methods",
    "abstract": "Advantage Actor-Critic with Reasoner: Explaining the Agent\u2019s Behavior from an Exploratory Perspective Muzhe Guo1 , Feixu Yu2 , Tian Lan2 , Fang Jin1 1  arXiv:2309.04707v1 [cs.AI] 9 Sep 2023  2  Department of Statistics, George Washington University, United States Department of Electrical and Computer Engineering, George Washington University, United States {muzheguo, fxyu, tlan, fangjin}@gwu.edu  Abstract Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage ActorCritic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor\u2019s act"
  },
  "reinforcement_learning_papers/01_core_methods/PPO_Proximal_Policy_Optimization.pdf": {
    "category": "01_core_methods",
    "abstract": "arXiv:1707.06347v2 [cs.LG] 28 Aug 2017  Proximal Policy Optimization Algorithms John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov OpenAI {joschu, filip, prafulla, alec, oleg}@openai.com Abstract We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \u201csurrogate\u201d objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, a"
  },
  "reinforcement_learning_papers/01_core_methods/SAC_Soft_Actor_Critic.pdf": {
    "category": "01_core_methods",
    "abstract": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor  arXiv:1801.01290v2 [cs.LG] 8 Aug 2018  Tuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1  Abstract Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework"
  },
  "reinforcement_learning_papers/01_core_methods/Action_Value_Gradient.pdf": {
    "category": "01_core_methods",
    "abstract": "Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers  arXiv:2411.15370v2 [cs.LG] 21 May 2025  Gautham Vasan12 Fahim Shariar12 1  Mohamed Elsayed12 Colin Bellinger3  Alireza Azimi\u221712  Martha White124  Jiamin He\u221712  A. Rupam Mahmood124  University of Alberta 2 Amii 3 National Research Council of Canada 4 CIFAR Canada AI Chair {vasan, mohamedelsayed, sazimi, jiamin12, fshahri1}@ualberta.ca colin.bellinger@nrc-cnrc.gc.ca {whitem, armahmood}@ualberta.ca  Abstract Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradie"
  },
  "reinforcement_learning_papers/01_core_methods/DLPA_Parameterized_Actions.pdf": {
    "category": "01_core_methods",
    "abstract": "Model-based Reinforcement Learning for Parameterized Action Spaces  Renhao Zhang * 1 Haotian Fu * 1 Yilin Miao 1 George Konidaris 1  arXiv:2404.03037v3 [cs.LG] 24 May 2024  Abstract  or Robot Soccer (Hausknecht & Stone, 2016b).  We propose a novel model-based reinforcement learning algorithm\u2014Dynamics Learning and predictive control with Parameterized Actions (DLPA)\u2014for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.1  Compared to just discrete or continuous action space, Reinforcem"
  },
  "reinforcement_learning_papers/01_core_methods/Rational_PG.pdf": {
    "category": "01_core_methods",
    "abstract": "Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning Rashmeet Kaur Nayyar and Siddharth Srivastava  arXiv:2412.16395v1 [cs.AI] 20 Dec 2024  Autonomous Agents and Intelligent Robots Lab, School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ, USA {rmnayyar, siddharths}@asu.edu  Abstract Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action representations to enable transfer and generalization remains a challenging open problem. This paper presents a novel approach for inventing, representing, and utilizing options, which represent temporally extended behaviors, in continual RL settings. Our approach addresses streams of stochastic problems characterized by long horizons, sparse rewards, and unknown transition and reward functions. Our approach continually learns and maintains an interpretable state abstraction, and uses it to invent high-level options with "
  },
  "reinforcement_learning_papers/01_core_methods/Adjoint_Matching.pdf": {
    "category": "01_core_methods",
    "abstract": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control Carles Domingo-Enrich1 , Michal Drozdzal1 , Brian Karrer1 , Ricky T. Q. Chen1  arXiv:2409.08861v5 [cs.LG] 7 Jan 2025  1  FAIR, Meta  Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoreticallysound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing meth"
  },
  "reinforcement_learning_papers/01_core_methods/Trust_SDAC.pdf": {
    "category": "01_core_methods",
    "abstract": "Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints  arXiv:2301.10923v2 [cs.LG] 24 Dec 2023  1  Dohyeong Kim1 , Kyungjae Lee2 , and Songhwai Oh1 Dep. of Electrical and Computer Engineering and ASRI, Seoul National University 2 Artificial Intelligence Graduate School, Chung-Ang University dohyeong.kim@rllab.snu.ac.kr, kyungjae.lee@ai.cau.ac.kr, songhwai@snu.ac.kr  Abstract In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance. Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints. To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a safe distributional actor-critic (SDAC). Our main contributions are as follows: 1) introducing a gradient integration meth"
  },
  "reinforcement_learning_papers/01_core_methods/Rational_Policy_Gradient.pdf": {
    "category": "01_core_methods",
    "abstract": "Robust and Diverse Multi-Agent Learning via Rational Policy Gradient  Niklas Lauffer1 Ameesh Shah1 Micah Carroll1  arXiv:2511.09535v1 [cs.AI] 12 Nov 2025  Sanjit A. Seshia1 Stuart Russell1 Michael Dennis2 1  UC Berkeley  2  Google Deepmind  Abstract Adversarial optimization algorithms that explicitly search for flaws in agents\u2019 policies have been successfully applied to finding robust and diverse policies in multi-agent settings. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to selfsabotage, blocking the completion of tasks and halting further learning. To address this, we introduce Rationality-preserving Policy Optimization (RPO), a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain rational\u2014that is, their policies are optimal with respect to some possible partner policy. To"
  },
  "reinforcement_learning_papers/01_core_methods/Distributional_Advantage_Actor_Critic.pdf": {
    "category": "01_core_methods",
    "abstract": "arXiv:1806.06914v1 [cs.LG] 10 Jun 2018  Distributional Advantage Actor-Critic  Shangda Li Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 shangdal@andrew.cmu.edu  Selina Bing Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 zbing@andrew.cmu.edu  Steven Yang Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 rujiay@andrew.cmu.edu  Abstract In traditional reinforcement learning, an agent maximizes the reward collected during its interaction with the environment by approximating the optimal policy through the estimation of value functions. Typically, given a state s and action a, the corresponding value is the expected discounted sum of rewards. The optimal action is then chosen to be the action a with the largest value estimated by value function. However, recent developments have shown both theoretical and experimental evidence of superior performance when value function is replaced with value distribution i"
  },
  "reinforcement_learning_papers/01_core_methods/RLA_Anticipation.pdf": {
    "category": "01_core_methods",
    "abstract": "R EINFORCEMENT L EARNING WITH A NTICIPATION : A H IERARCHICAL A PPROACH FOR L ONG -H ORIZON TASKS  arXiv:2509.05545v1 [cs.LG] 6 Sep 2025  A P REPRINT Yang Yu National Key Laboratory for Novel Software Technology, Nanjing University, China School of Artificial Intelligence, Nanjing University, China yuy@nju.edu.cn  A BSTRACT Solving long-horizon goal-conditioned tasks remains a significant challenge in reinforcement learning (RL). Hierarchical reinforcement learning (HRL) addresses this by decomposing tasks into more manageable sub-tasks, but the automatic discovery of the hierarchy and the joint training of multilevel policies often suffer from instability and can lack theoretical guarantees. In this paper, we introduce Reinforcement Learning with Anticipation (RLA), a principled and potentially scalable framework designed to address these limitations. The RLA agent learns two synergistic models: a low-level, goal-conditioned policy that learns to reach specified subgoals, and a high-l"
  },
  "reinforcement_learning_papers/01_core_methods/M2DQN_Acceleration.pdf": {
    "category": "01_core_methods",
    "abstract": "Laminar and turbulence forced heat transfer convection correlations inside tubes. A review By: Gubran A.Q. Abdulrahmana,b , Sultan M. Alharbia a  Mechanical Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran 31261, Saudi Arabia b Interdisciplinary Research Center for Hydrogen Technology and Carbon Management (IRCHTCM), King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia  Abstract  This work proposes an extensive review of laminar and turbulent forced convective heat transfer correlations inside tubes by analyzing both experimental and computational research. Convective heat transfer is influenced by fluid turbulence and boundary layers, with geometry significantly impacting flow conditions. Nusselt number correlations quantifying the heat transfer coefficient are vital for various applications. Previous reviews are summarized regarding nanofluid heat transfer modeling approaches in circular tubes. Additionally, comparisons of tube shap"
  },
  "reinforcement_learning_papers/04_hierarchical_rl/LDSC_Option_Discovery.pdf": {
    "category": "04_hierarchical_rl",
    "abstract": "Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning  arXiv:2503.19007v1 [cs.LG] 24 Mar 2025  Chak Lam Shek1 and Pratap Tokekar2 Abstract\u2014 Large Language Models (LLMs) have shown remarkable promise in reasoning and decision-making, yet their integration with Reinforcement Learning (RL) for complex robotic tasks remains underexplored. In this paper, we propose an LLM-guided hierarchical RL framework, termed LDSC, that leverages LLM-driven subgoal selection and option reuse to enhance sample efficiency, generalization, and multi-task adaptability. Traditional RL methods often suffer from inefficient exploration and high computational cost. Hierarchical RL helps with these challenges, but existing methods often fail to reuse options effectively when faced with new tasks. To address these limitations, we introduce a three-stage framework that uses LLMs for subgoal generation given natural language description of the task, a reusable option learning and selection m"
  },
  "reinforcement_learning_papers/04_hierarchical_rl/Goal_Space_Planning.pdf": {
    "category": "04_hierarchical_rl",
    "abstract": "A New View on Planning in Online Reinforcement Learning Kevin Roice* 1 , Parham Mohammad Panahi* 1 , Scott M. Jordan1 , Adam White1, 2 \u2020 , Martha White1, 2 \u2020  arXiv:2406.01562v1 [cs.LG] 3 Jun 2024  1  Department of Computing Science, University of Alberta 2 Alberta Machine Intelligence Institute (Amii) \u2020 Canada CIFAR AI Chair {roice, parham1, sjordan, amw8, whitem}@ualberta.ca  Abstract This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set o"
  },
  "reinforcement_learning_papers/17_surveys_and_overviews/Comprehensive_RL_Survey.pdf": {
    "category": "17_surveys_and_overviews",
    "abstract": "1  Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges arXiv:2411.18892v2 [cs.AI] 1 Feb 2025  Majid Ghasemi\u2020,1 , Amir Hossein Moosavi1 , and Dariush Ebrahimi1  Abstract\u2014Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the select"
  },
  "reinforcement_learning_papers/09_agentic_rl/TapeAgents_Holistic_Framework_Agent_Development.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Stable_RL_Reasoning.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Stable Reinforcement Learning for Efficient Reasoning  Muzhi Dai\u2662\u2217, Shixuan Liu\u2661\u2217, Qingyi Si\u2662\u2020, Huawei Technologies Co., Ltd. \u2661 Australian National University mzdai666@gmail.com, u6920173@anu.edu.au, siqingyi@huawei.com  GRPO + length penalty  GRPO-\u03bb 0.80  10000  0.6  7500 5000  0.5  16000 14000  0.75  12000  0.70  10000 0.65  8000  2500 0.4  0  20  40  Step  60  80  100  0  20  40  Step  60  80  100  Figure 1: Training process of GRPO+length penalty and our GRPO-\u03bb.  Abstract The success of Deepseek-R1 has drawn the LLM community\u2019s attention to reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1 outcome reward methods lack the capability to regulate the intermediate reasoning processes during chain-of-thought (CoT) generation, leading to severe overthinking phenomena. In response, recent studies have designed reward functions to reinforce models\u2019 behaviors in producing shorter yet correct completions. Nevertheless, we observe that these length-penalty reward fun"
  },
  "reinforcement_learning_papers/09_agentic_rl/Reflexion_Verbal_RL.pdf": {
    "category": "09_agentic_rl",
    "abstract": "arXiv:2303.11366v4 [cs.AI] 10 Oct 2023  Reflexion: Language Agents with Verbal Reinforcement Learning  Noah Shinn Northeastern University noahshinn024@gmail.com  Federico Cassano Northeastern University cassano.f@northeastern.edu  Edward Berman Northeastern University berman.ed@northeastern.edu  Ashwin Gopinath Massachusetts Institute of Technology agopi@mit.edu  Karthik Narasimhan Princeton University karthikn@princeton.edu  Shunyu Yao Princeton University shunyuy@princeton.edu  Abstract Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedb"
  },
  "reinforcement_learning_papers/09_agentic_rl/OS-Harm_Computer_Use_Safety.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Toolformer.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick  Jane Dwivedi-Yu  Roberto Dess\u00ec\u2020  Maria Lomeli  Luke Zettlemoyer  Nicola Cancedda  Roberta Raileanu Thomas Scialom  Meta AI Research \u2020 Universitat Pompeu Fabra  arXiv:2302.04761v1 [cs.CL] 9 Feb 2023  Abstract Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorpor"
  },
  "reinforcement_learning_papers/09_agentic_rl/ReAct_Reasoning_Acting.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Published as a conference paper at ICLR 2023  R E A C T : S YNERGIZING R EASONING AND ACTING IN L ANGUAGE M ODELS Shunyu Yao\u2217*,1 , Jeffrey Zhao2 , Dian Yu2 , Nan Du2 , Izhak Shafran2 , Karthik Narasimhan1 , Yuan Cao2 1  arXiv:2210.03629v3 [cs.CL] 10 Mar 2023  Department of Computer Science, Princeton University 2 Google Research, Brain team 1 {shunyuy,karthikn}@princeton.edu 2 {jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com  A BSTRACT While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as "
  },
  "reinforcement_learning_papers/09_agentic_rl/FireAct_Language_Agent_Fine-tuning.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/HippoRAG_Neurobiologically_Inspired_Long-Term_Memory.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Iterative_Reasoning_Preference_Optimization.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/StateFlow_Enhancing_LLM_Task-Solving.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/BrowseComp_Web_Browsing_Benchmark.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/WorkArena_Compositional_Planning.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/OSWorld_Multimodal_Agents_Benchmark.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Does_RL_Incentivize_Reasoning.pdf": {
    "category": "09_agentic_rl",
    "abstract": "November 25, 2025  Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? Yang Yue 1 \u2217 \u2020 , Zhiqi Chen 1 \u2217 , Rui Lu 1 , Andrew Zhao 1 , Zhaokai Wang 2 , Yang Yue 1 , Shiji Song 1 , and Gao Huang 1 B  arXiv:2504.13837v5 [cs.AI] 24 Nov 2025  1 LeapLab, Tsinghua University \u2217 Equal Contribution  2 Shanghai Jiao Tong University  \u2020 Project Lead  B Corresponding Author  Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously selfimprove, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at the current state of RLVR by systematically probing the reasoning capability "
  },
  "reinforcement_learning_papers/09_agentic_rl/SWE-agent_Agent-Computer_Interfaces.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Tree_of_Thoughts.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models  arXiv:2305.10601v2 [cs.CL] 3 Dec 2023  Shunyu Yao Princeton University  Dian Yu Google DeepMind  Thomas L. Griffiths Princeton University  Jeffrey Zhao Google DeepMind  Yuan Cao Google DeepMind  Izhak Shafran Google DeepMind  Karthik Narasimhan Princeton University  Abstract Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, \u201cTree of Thoughts\u201d (ToT), which generalizes over the popular \u201cChain of Thought\u201d approach to prompting language models, and enables exploration over coherent units of text (\u201cthoughts\u201d) that serve as intermediate steps toward "
  },
  "reinforcement_learning_papers/09_agentic_rl/Tau2-Bench_Conversational_Agents_Dual-Control.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/OSWORLD_Benchmarking_Multimodal_Agents.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/ARTIST_Agentic_Reasoning_Tool_Integration.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/AGUVIS_Unified_Pure_Vision_Agents_GUI.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/RL_Enhanced_LLMs_Survey.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Reinforcement Learning Enhanced LLMs: A Survey Shuhe Wang\u2660 , Shengyu Zhang\u2663 , Jie Zhang\u22c6 , Runyi Hu\u25b2 , Xiaoya Li\u2666 Tianwei Zhang\u25b2 , Jiwei Li\u2663 , Fei Wu\u2663 , Guoyin Wang, Eduard Hovy\u2660  Abstract  arXiv:2412.10400v3 [cs.CL] 24 Feb 2025  Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Depsite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consoli"
  },
  "reinforcement_learning_papers/09_agentic_rl/Voyager_Open_Ended_Agent.pdf": {
    "category": "09_agentic_rl",
    "abstract": "arXiv:2305.16291v2 [cs.AI] 19 Oct 2023  VOYAGER: An Open-Ended Embodied Agent with Large Language Models Guanzhi Wang1 2 # , Yuqi Xie3 , Yunfan Jiang4\u2217 , Ajay Mandlekar1\u2217 , Chaowei Xiao1 5 , Yuke Zhu1 3 , Linxi \u201cJim\u201d Fan1\u2020 # , Anima Anandkumar1 2\u2020 1 NVIDIA, 2 Caltech, 3 UT Austin, 4 Stanford, 5 UW Madison \u2217 Equal contribution \u2020 Equal advising # Corresponding authors https://voyager.minedojo.org  Abstract We introduce VOYAGER, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. VOYAGER consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. VOYAGER interacts with GPT-4 via blackbo"
  },
  "reinforcement_learning_papers/09_agentic_rl/WebShop_Scalable_Real-World_Web_Interaction.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/DSPy_Compiling_Declarative_Language_Model.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Teaching_Large_Language_Models_to_Self-Debug.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/DAPO_OpenSource_LLM_RL.pdf": {
    "category": "09_agentic_rl",
    "abstract": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale ByteDance Seed  Institute for AI Industry Research (AIR), Tsinghua University 3 The University of Hong Kong 4 SIA-Lab of Tsinghua AIR and ByteDance Seed 2  Full author list in Contributions  Abstract Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addi"
  },
  "reinforcement_learning_papers/09_agentic_rl/Is_Your_LLM_Secretly_a_World_Model_of_the_Internet.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/AutoGen_Multi-Agent_Conversation.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/WorkArena_Common_Knowledge_Work_Tasks.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/WebAgent-R1_Multi-Turn_RL.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Tree_Search_for_Language_Model_Agents.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Transformers_Decision_Makers.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning  Wei Fu 1 Chao Yu 2 Zelai Xu 2 Jiaqi Yang 3 Yi Wu 1 4  arXiv:2206.07505v2 [cs.AI] 7 Aug 2022  Abstract Many advances in cooperative multi-agent reinforcement learning (MARL) are based on two common design principles: value decomposition and parameter sharing. A typical MARL algorithm of this fashion decomposes a centralized Q-function into local Q-networks with parameters shared across agents. Such an algorithmic paradigm enables centralized training and decentralized execution (CTDE) and leads to efficient learning in practice. Despite all the advantages, we revisit these two principles and show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, value decomposition, and parameter sharing can be problematic and lead to undesired outcomes. In contrast, policy gradient (PG) methods with individual policies provably converge to an optimal solution in these cases, which"
  },
  "reinforcement_learning_papers/09_agentic_rl/Mind2Web_Generalist_Agent_for_the_Web.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/WebArena_Realistic_Web_Environment.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Dualformer_Controllable_Fast_and_Slow_Thinking.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/OpenScholar_Synthesizing_Scientific_Literature.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Survey_Evaluation_LLM-based_Agents.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/ReAct_Synergizing_Reasoning_and_Acting.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Adding_Error_Bars_to_Evals.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Generative_Agents_Simulacra.pdf": {
    "category": "09_agentic_rl",
    "abstract": "arXiv:2304.03442v2 [cs.HC] 6 Aug 2023  Generative Agents: Interactive Simulacra of Human Behavior Joon Sung Park  Joseph C. O\u2019Brien  Carrie J. Cai  Stanford University Stanford, USA joonspk@stanford.edu  Stanford University Stanford, USA jobrien3@stanford.edu  Google Research Mountain View, CA, USA cjcai@google.com  Meredith Ringel Morris  Percy Liang  Michael S. Bernstein  Google DeepMind Seattle, WA, USA merrie@google.com  Stanford University Stanford, USA pliang@cs.stanford.edu  Stanford University Stanford, USA msb@cs.stanford.edu  Figure 1: Generative agents are believable simulacra of human behavior for interactive applications. In this work, we demonstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can observe and intervene as agents plan their days, share news, form relationships, and coordinate group activities.  ABSTRACT Believable proxies of human behavior can empower interactive applications ranging from im"
  },
  "reinforcement_learning_papers/09_agentic_rl/VisualWebArena.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/DigiRL_Device_Control_Agents.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/SurCo_Learning_Linear_Surrogates.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/SWE-bench_Verified.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Paper2Agent_Research_Papers_as_AI_Agents.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/OpenHands_AI_Software_Developers.pdf": {
    "category": "09_agentic_rl",
    "abstract": "No text available."
  },
  "reinforcement_learning_papers/09_agentic_rl/Bootstrapped_Transformer.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Published as a conference paper at ICLR 2022  P ESSIMISTIC B OOTSTRAPPING FOR U NCERTAINTYD RIVEN O FFLINE R EINFORCEMENT L EARNING Chenjia Bai Harbin Institute of Technology  Lingxiao Wang Northwestern University  Zhuoran Yang Princeton University  baichenjia255@gmail.com  arXiv:2202.11566v1 [cs.LG] 23 Feb 2022  Zhihong Deng University of Technology Sydney  Animesh Garg University of Toronto Vector Institute, NVIDIA  Peng Liu Harbin Institute of Technology  Zhaoran Wang Northwestern University  A BSTRACT Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problems by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalizati"
  },
  "reinforcement_learning_papers/09_agentic_rl/Search_R1_Reasoning_Search.pdf": {
    "category": "09_agentic_rl",
    "abstract": "Published as a conference paper at COLM 2025  Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning  arXiv:2503.09516v5 [cs.CL] 5 Aug 2025  Bowen Jin1 , Hansi Zeng2 , Zhenrui Yue1 , Jinsung Yoon3 , Sercan O\u0308. Ar\u0131k3 , Dong Wang1 , Hamed Zamani2 , Jiawei Han1 1 Department of Computer Science, University of Illinois at Urbana-Champaign 2 Center for Intelligent Information Retrieval, University of Massachusetts Amherst 3 Google Cloud AI Research {bowenj4,zhenrui3,dwang24,hanj}@illinois.edu, {hzeng, zamani}@cs.umass.edu {jinsungyoon,soarik}@google.com  Abstract Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces S EAR"
  },
  "reinforcement_learning_papers/10_offline_rl/Decision_Transformer.pdf": {
    "category": "10_offline_rl",
    "abstract": "arXiv:2106.01345v2 [cs.LG] 24 Jun 2021  Decision Transformer: Reinforcement Learning via Sequence Modeling  Lili Chen\u2217,1 , Kevin Lu\u2217,1 , Aravind Rajeswaran2 , Kimin Lee1 , Aditya Grover2 , Michael Laskin1 , Pieter Abbeel1 , Aravind Srinivas\u2020,1 , Igor Mordatch\u2020,3 \u2217 equal contribution \u2020 equal advising 1  UC Berkeley  2  Facebook AI Research  3  Google Brain  {lilichen, kzl}@berkeley.edu  Abstract We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregres"
  },
  "reinforcement_learning_papers/10_offline_rl/Implicit_Q_Learning.pdf": {
    "category": "10_offline_rl",
    "abstract": "Journal of Machine Learning Research 21 (2020) 1-52  Submitted 1/20; Revised 8/20; Published 8/20  Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning Tabish Rashid\u2217  tabish.rashid@cs.ox.ac.uk  University of Oxford  Mikayel Samvelyan\u2217  mikayel@samvelyan.com  arXiv:2003.08839v2 [cs.LG] 27 Aug 2020  Russian-Armenian University  Christian Schroeder de Witt  cs@robots.ox.ac.uk  University of Oxford  Gregory Farquhar  gregory.farquhar@cs.ox.ac.uk  University of Oxford  Jakob Foerster  jnf@fb.com  Facebook AI Research  Shimon Whiteson  shimon.whiteson@cs.ox.ac.uk  University of Oxford  Editor: George Konidaris  Abstract  In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state inf"
  },
  "reinforcement_learning_papers/10_offline_rl/Offline_RL_Generative_Trajectory.pdf": {
    "category": "10_offline_rl",
    "abstract": "Preprint  O FFLINE R EINFORCEMENT L EARNING WITH G ENERA TIVE T RAJECTORY P OLICIES Xinsong Feng1 , Leshu Tang2 , Chenan Wang1 , Haipeng Chen1 1 William & Mary, 2 UCLA xfeng06@wm.edu, leshutang@ucla.edu, cwang33@wm.edu, hchen23@wm.edu  arXiv:2510.11499v1 [cs.LG] 13 Oct 2025  A BSTRACT Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. However, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. In this paper, we demonstrate that it is possible to bridge this gap. The key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models\u2014including diffusion, flow matching, and consistency models\u2014as specific instances of learning a continuous-time "
  },
  "reinforcement_learning_papers/10_offline_rl/CQL_Conservative_Q_Learning.pdf": {
    "category": "10_offline_rl",
    "abstract": "arXiv:2006.04779v3 [cs.LG] 19 Aug 2020  Conservative Q-Learning for Offline Reinforcement Learning  Aviral Kumar1 , Aurick Zhou1 , George Tucker2 , Sergey Levine1,2 1 UC Berkeley, 2 Google Research, Brain Team aviralk@berkeley.edu  Abstract Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-"
  },
  "reinforcement_learning_papers/10_offline_rl/Offline_RL_Optimization.pdf": {
    "category": "10_offline_rl",
    "abstract": "Published as a conference paper at ICLR 2025  OGB ENCH : B ENCHMARKING O FFLINE G OAL -C ONDITIONED RL Seohong Park1 Kevin Frans1 Benjamin Eysenbach2 1 University of California, Berkeley 2 Princeton University seohong@berkeley.edu  Sergey Levine1  arXiv:2410.20092v2 [cs.LG] 13 Feb 2025  A BSTRACT Offline goal-conditioned reinforcement learning (GCRL) is a major problem in reinforcement learning (RL) because it provides a simple, unsupervised, and domainagnostic way to acquire diverse behaviors and representations from unlabeled data without rewards. Despite the importance of this setting, we lack a standard benchmark that can systematically evaluate the capabilities of offline GCRL algorithms. In this work, we propose OGBench, a new, high-quality benchmark for algorithms research in offline goal-conditioned RL. OGBench consists of 8 types of environments, 85 datasets, and reference implementations of 6 representative offline GCRL algorithms. We have designed these challenging and reali"
  },
  "reinforcement_learning_papers/10_offline_rl/Offline_RL_Implementable.pdf": {
    "category": "10_offline_rl",
    "abstract": "Astronomy & Astrophysics manuscript no. PhD-Paper3-arXiv December 6, 2024  \u00a9ESO 2024  A \u039bCDM Extension Explaining the Hubble Tension and the Spatial Curvature \u2126 k,0 = \u22120.012 \u00b1 0.010 Measured by the Final PR4 of the Planck Mission Horst Foidl1, 2 * and Tanja Rindler-Daller1, 2, 3\u2020  arXiv:2412.04126v1 [astro-ph.CO] 5 Dec 2024  1  Institut f\u00fcr Astrophysik, Universit\u00e4tssternwarte Wien, Fakult\u00e4t f\u00fcr Geowissenschaften, Geographie und Astronomie, Universit\u00e4t Wien, T\u00fcrkenschanzstr.17, A-1180 Vienna, Austria 2 Vienna International School of Earth and Space Sciences, Universit\u00e4t Wien, Josef-Holaubek-Platz 2, A-1090 Vienna, Austria 3 Wolfgang Pauli Institut, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria December 6, 2024 ABSTRACT  The measurements of the CMB have determined the cosmological parameters with high accuracy, and the observation of the flatness of space have contributed to the status of the concordance \u039bCDM model. However, the cosmological constant \u039b, necessary to close the model t"
  }
}