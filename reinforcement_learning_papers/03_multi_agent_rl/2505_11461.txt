arXiv:2505.11461v2 [cs.LG] 28 May 2025

Signal attenuation enables scalable decentralized
multi-agent reinforcement learning over networks
Wesley A. Suttle

Vipul K. Sharma

Brian M. Sadler

Army Research Laboratory
U.S. Army DEVCOM
Adelphi, MD USA
wesley.a.suttle.ctr@army.mil

Industrial Engineering Dept.
Purdue University
West Lafayette, IN USA
sharm697@purdue.edu

Oden Institute
University of Texas, Austin
Austin, TX USA
brian.sadler@ieee.org

Abstract—Multi-agent reinforcement learning (MARL) methods typically require that agents enjoy global state observability,
preventing development of decentralized algorithms and limiting
scalability. Recent work has shown that, under assumptions
on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent,
enabling decentralization and scalability. Real-world applications
enjoying such decay properties remain underexplored, however,
despite the fact that signal power decay, or signal attenuation,
due to path loss is an intrinsic feature of many problems in
wireless communications and radar networks. In this paper, we
show that signal attenuation enables decentralization in MARL
by considering the illustrative special case of performing power
allocation for target detection in a radar network. To achieve this,
we propose two new constrained multi-agent Markov decision
process formulations of this power allocation problem, derive
local neighborhood approximations for global value function
and policy gradient estimates and establish corresponding error
bounds, and develop decentralized saddle point policy gradient
algorithms for solving the proposed problems. Our approach,
though oriented towards the specific radar network problem we
consider, provides a useful model for extensions to additional
problems in wireless communications and radar networks.
Index Terms—multi-agent reinforcement learning, radar networks, target detection, power allocation

I. I NTRODUCTION
Multi-agent reinforcement learning (MARL) has seen immense attention in recent years, from both theoretical [1]–[4]
and experimental [5]–[7] perspectives. Due to limitations of
the underlying multi-agent Markov decision process (MDP)
model, however, standard methods for MARL in networked
systems require global state observability at each agent [4].
This inhibits the development of truly decentralized MARL
methods where each agent only needs access to information
from its local neighborhood, thereby limiting the scalability
of such methods and preventing their application to realistic
problems. Fortunately, recent works [8]–[11] on scalable,
decentralized MARL have shown that, for problems where
inter-agent influence decays sufficiently quickly as the distance
between agents increases over their communication network,
use of only local neighborhood information at each agent
suffices to approximately solve the global problem. This
enables the development of truly decentralized methods that
scale well as the number of agents increases. However, despite

the theoretical advantages of these methods, real-world applications where the prerequisite decay properties hold remain
largely unexplored.
Signal power decay, or signal attenuation, due to path loss
is a well-known property of wireless communications [12]
and radar systems [13]. In problems where multiple agents
are widely dispersed over a geographic region, such as radar
networks [14], path loss naturally leads to decay of inter-agent
influence as distance between agents increases. For example,
when the performance metric at each agent is a function of the
power of received signals, such as the signal-to-interferenceplus-noise ratio (SINR), performance measurements at a given
agent are largely decoupled from behavior of other agents
that are sufficiently far away over the network. Due to this
inherent property of these systems, wireless communications
and radar networks provide promising candidates for realworld application of scalable, decentralized MARL methods
like [8]–[11] that rely on such decay properties for success.
In this paper, we examine the implications of signal attenuation for the development of scalable, decentralized MARL
approaches to the specific problem of performing power allocation for target detection in a radar network. Radar networks
are attractive for performing target detection and tracking
due to advantages arising from their spatial dispersion and
potential signal variety [15]. When determining power allocations in radar networks, maximizing power leads to improved
signal strength, but this conflicts with the need to achieve low
probability of intercept (LPI) and abide by resource constraints
[16]. Existing methods for LPI power allocation for target
detection in radar networks are centralized in that they require
global observability and global coordination between radars
[15]–[17], rendering them impractical in large networks for
similar reasons to the MARL methods discussed above.
In this work, we propose a MARL approach for performing decentralized power allocation for LPI target detection
in radar networks that mitigates these drawbacks. This is
achieved by leveraging the signal attenuation inherent in radar
networks to replace global observability and coordination
with local observability and coordination among neighboring
radars. Specifically, our contributions are as follows: (i) we
propose two new constrained multi-agent MDP formulations
of the problem of power allocation for target detection in

radar networks; (ii) we leverage signal attenuation properties
inherent in our setting to derive local approximations of
the policy gradient expressions used in our algorithms and
rigorously establish error bounds on these approximations; (iii)
we propose novel decentralized, policy gradient ascent-descent
algorithms for approximately solving the proposed problems.
Though we focus on radar networks in this work, our approach
can likely be extended to a broad range of applications in
wireless communication and radar networks.

j. The signal-to-interference-plus-noise ratio (SINR) of the
signal received at radar i is then given by

II. P ROBLEM F ORMULATION

To capture the underlying system model described above,
we use a constrained multi-agent Markov decision process
(CMAMDP) (S, A, p, N , {ri }i∈N , {ci }i∈N ), defined below.
Let the set of radars N = {1, . . . , n} correspond to the n
radars. Let S = S 1 × . . . × S n denote the joint state space,
where the ith component S i ⊂ Rk × Rm corresponds to set of
possible locations and movements of the target and radar i, i.e.,
si = (sitarget , siradar ) with sitarget ∈ Rk and siradar ∈ Rm .
When the target is moving in R3 and the radars are moving
in R2 , for example, we may take k = 9 and m = 6 and let
sitarget and siradar correspond to the positions, velocities, and
accelerations of the target and radar, respectively. Notice that,
for all i, j ∈ N , we have sitarget = sjtarget . Given the joint
state st ∈ S at time t, we assume that the Doppler steering
vectors, predesigned waveforms, and channel gain variances
corresponding to radar i are provided and that the goal of radar
i is to determine an appropriate transmission power level to
supply given its local state information sit . To capture this, let
A = A1 × . . . × An , where Ai = [0, amax ] denotes the set
of possible power allocations at radar i, for a predetermined,
finite, maximum power allocation amax > 0 over all radars.
Let the transition dynamics p : S × A → S capture the
movement of the target and radars over time. We assume in
this paper that the movements of the target and radars, and
therefore p, are independent of the transmission power allocations applied at the radars. Given joint state s ∈ S and joint
power allocation a ∈ S, let ri (s, a) = SIN Ri (s, a) denote
the SINR received at radar i obtained from equation (4) by
substituting the channel gains, cross-correlation coefficients,
and noise corresponding to s and applying allocation a, i.e.,

In this section, we first describe our system model for a
radar network with widely separated radars, a flying target, and
extended clutter. To enable LPI target detection and tracking
in this setting, we subsequently propose two different problem
formulations: (i) maximizing sum-of-SINRs subject to regional
power constraints; (ii) minimizing power consumption subject
to a minimal SINR threshold and regional power constraints.
A. Radar network model
We consider a radar network composed of a set N =
{1, . . . , n} networked radars. In the presence of a target, the
received signal for radar i ∈ N is given by [15], [16]
X
√
√
β ji aj y j + ω i ,
(1)
xi = αi ai y i +
i∈N \{i}

where y i = ψ i z i describes the transmitted signal from radar i

T
and z i = 1 ej2πfD,i . . . ej2π(N −1)fD,i is the Doppler
steering vector of radar i associated with the desired target,
fD,i denotes the normalized Doppler shift as seen by radar
i, N is the number of received pulses at each timestep, and
ψ i denotes the predesigned waveform transmitted from radar
i. Furthermore, the parameter αi denotes the desired channel
gain in the target direction, ai denotes the transmission power
of radar i, β ji describes the cross-channel gain between radars
i and j, and ω i denotes zero-mean white Gaussian noise.
We assume that αi ∼ CN (0, hτii ), β ij ∼ CN (0, cij (hτij +
d
hij )), and ω i ∼ CN (0, (σ i )2 ) for an i-dependent σ i > 0,
where cij hτij represents the variance of the channel gain for the
radar i-target-radar j path, cij hdij represents the variance of the
channel gain for the direct radar i-radar j path, and cij denotes
the cross-correlation coefficient between the ith and jth radars.
Note that cii = 1, so the variance of the channel gain for the
radar i-target-radar i path is simply hτii . The variances are
given by the radar range equations [13]
RCS 2
Gt Gr σij
λ
hτij =
, (2)
2
3
(4π) Ri Rj2

G′ G′ λ2
hdij = t 2r 2 ,
(4π) dij

SIN Ri =

where Gt and Gr are the radar main-lobe transmitting and
receiving gains, G′t and G′r are the radar side-lobe transmitting
RCS
and receiving gains, σij
is the radar cross section (RCS)
of the target between the ith and jth radars, λ denotes the
wavelength, Ri denotes the distance between radar i and the
target, and dij denotes the distance between radars i and

(4)

where σ i corresponds to the noise received at radar i. See
Figure 1 for an illustration of the radar network system model.
B. Constrained multi-agent MDP formulations

ri (s, a) =

(σ i (s, a))2 +

P

hτii (s)ai
 ,
d
τ
j
j∈N \{i} cji (s) hji (s) + hji (s) a
(5)

where
we
abuse
notation
to
let
hτii (s), hτji (s), hdji (s), cji (s), σ i (s, a), and σκi (s, a) correspond
to the expressions appearing in (4) when the system is in
state s and joint action a is selected. Similarly, define the
local neighborhood reward by
i
rκ
(s, a) =

(3)

(σ i )2 +

hτii ai
 ,
τ
d
j
j∈N \{i} cji hji + hji a

P

hτii (s)ai
i (s, a))2 +
(σκ

P

j∈Nκ (i)\{i} cji (s)


 ,
hdji (s) + hτji (s) aj
(6)

which captures the SINR received at radar i originating
within neighborhood Nκ (i), where σκi (s, a) denotes the noise
originating within Nκ (i). The expression in (6) will be crucial
in the theoretical results of Section III. Finally, let the cost
ci (s, a) denote the cost to radar i of applying power level
ai . We might simply take ci (s, a) = ai , for example, but our
approach accommodates general cost structures.

Fig. 1. Illustrations of radar network system model of Section II-A (left) and communication neighborhoods of Section II-B (right).

Let a fixed communication radius R > 0 between the
radars be given, and denote the undirected communication
graph between radars in state s ∈ S by G(s) = (N , E(s)),
where E(s) = {(i, j) | dij (s) ≤ R} and dij (s) denotes
the Euclidean distance between radars i and j when the
system is in state s. We note that R defines a user-specified
communications neighborhood, but that it does not necessarily
represent a hard limit on communications within the network.
We assume in this paper that the edge set E = E(s) remains
constant, i.e., that any movement of the radars leaves the
topology of the communication network unchanged, and will
henceforth suppress the dependence on s and simply write
G = (N , E). For a given positive integer κ ∈ N+ , let Nκ (i)
denote the κ-hop neighborhood of radar i with respect to
G. Note that, since G is undirected, the κ-hop neighborhood
relation is symmetric, i.e., i ∈ Nκ (j) if and only if j ∈ Nκ (i).
Finally, let Nκ−1 (i) = N \ Nκ (i) denote the set of all radars
outside i’s κ-hop neighborhood. See Figure 1 for an illustration
of the communication neighborhoods induced by a specified
communication radius R on a simple example.
Let κ ∈ N+ be fixed, and assume each radar i has access to
the state information sNκ (i) ∈ S Nκ (i) = {sj | j ∈ Nκ (i)}.
To each i, let there be associated a parameterized policy
class {πθi i : S Nκ (i) → ∆(Ai )}θi ∈Θi , where Θi ⊆ Rd is
the set of permissible policy parameters, for some positive
integer d. Denote the induced joint policy by πθ (a|s) =

T
Q
i
i Nκ (i)
∈Θ=
), where θ = (θ1 )T . . . (θn )T
i∈N πθ i (a |s
Θ1 × . . . × Θn is the stacked vector of each radar’s policy
parameters. Given policy πθ , under this formulation,
−1
h TX
i
1
Eπθ
ri (st , at ) ,
T →∞ T
t=0

Jri (θ) = lim

(7)

T −1

hX
i
1
Jci (θ) = lim
Eπθ
ci (st , at )
T →∞ T
t=0

(8)

capture the average expected SINR achieved and average
expected cost incurred, respectively, at radar i.

1) Sum-of-SINRs Maximization: Fix κ > 0. The sum-ofSINRs maximization problem is as follows.
X
max
Jri (θ)
θ ∈ Θ i∈N
X
(Pκmax )
s.t.
Jcj (θ) ≤ ui , ∀i ∈ N ,
j∈Nκ (i)

The objective of this problem is to maximize the sum over the
expected average SINRs over all radars while simultaneously
ensuring that expected average cost does not exceed “regional”
upper bounds at each radar.
2) Power Minimization with SINR Threshold: Fix κ > 0
and let γmin > 0 denote the minimum SINR value allowable
at each radar, where we assume that γmin is chosen such
that the thresholds at each radar in problem (Pκmin ) below
are achievable. The problem of minimizing overall power
consumption while respecting the SINR threshold is
X
min
Jci (θ)
θ ∈ Θ i∈N
Jri (θ) ≥ γmin , ∀i ∈ N ,

s.t.
X

Jcj (θ) ≤ ui ,

(Pκmin )

∀i ∈ N .

j∈Nκ (i)

The objective of this problem is to minimize the expected
average cost over all radars while simultaneously ensuring
that both (a) each radar achieves a minimal SINR value,
on average, and (b) expected average cost does not exceed
“regional” upper bounds at each radar.
III. S IGNAL ATTENUATION ENABLES DECENTRALIZATION
In this section, we examine how signal decay inherent
in the radar problem under consideration leads to natural
decentralization in MARL solution approaches. We first recall the MARL global observability issue that prevents true
decentralization in the general setting, then establish formal
properties of our problem that lead to natural decentralization.
We leverage the properties established in this section to

develop decentralized MARL algorithms for solving problems
(Pκmax ) and (Pκmin ) in the next section.
review a few key concepts from RL. Let r(s, a) =
PWe first
i
r
(s,
Pi∈N i a) denote the global reward and c(s, a) =
i∈N c (s, a) the global cost. Define the global averages
X
X
Jr (θ) =
Jri (θ),
Jc (θ) =
Jci (θ).
(9)
i∈N

i∈N

Fixing θ ∈ Θ, the action-value function corresponding to πθ
and the reward or cost function f ∈ {r, c} is given by
Qfθ (s, a) = Eπθ
=
=

X

"∞
X

#
f (st , at ) − Jf (θ) | s0 = s, a0 = a

(10)

t=0
"∞
#
X i
Eπθ
f (s, a) − Jf i (θ) | s0 = s, a0 = a

(11)

i∈N

t=0

X

Qfθ (s, a),

i

(12)

i∈N
i

where the Q function Qfθ (s, a) corresponding to f i is
i
Qθf (s, a) = Eπθ

"∞
X

#
i

f (s, a) − Jf i (θ) | s0 = s, a0 = a , (13)

t=0

for the ith reward or cost f i ∈ {ri , ci }.
A. MARL Global Observability Issue
Ideally, we would like each agent i to be able to compute
its contribution ∇θi Jf (θ) to the overall gradient ∇θ Jf (θ)
using only information from its local neighborhood Nκ (i), as
this would make the development of decentralized algorithms
easier. However, applying the classic policy gradient theorem
[18] to differentiate with respect to θi gives
i
h
∇θi Jf (θ) = Eπθ Qfθ (s, a)∇θi log πθi i (ai |si )
"
#
X fj
i
i i
= Eπθ
Qθ (s, a)∇θi log πθi (a |s ) .

(14)
(15)

j∈N

From equation (15), global information is clearly required to
j
estimate ∇θi Jf (θ), since each of the Qfθ (s, a) requires access
to the global state s and joint action a, and agent i needs
access to the Q functions of all other agents j ∈ N . This is
the crux of the global observability issue in policy gradient
methods for MARL. To address this issue, we next identify
properties of the radar problem we consider under which the
j
P
term j∈N Qfθ (s, a) can be replaced by an approximation
depending only on the information available to agent i within
its local neighborhood, Nκ (i).
B. Signal Attenuation and Decentralization
We start this section by stating several assumptions that
will be needed in the subsequent analysis. Assumption 1
provides a mechanism for ensuring that the radar network
provides adequate coverage of the region under consideration
through appropriate choice of g. Assumption 2 stipulates that
the local costs at each radar are independent of the costs at
all other radars. This is reasonable when costs depend only on
local power consumption, for example. Assumption 3 provides

minimal conditions ensuring that the radar range equations (2)(3) lead to well-conditioned local rewards (5). Assumptions 4
and 5 are commonly used in the RL [19]–[21] and MARL
[8], [9] literatures to enable analysis of policy gradient-based
methods. Assumption 6 bounds the effect that changes in a
given radar’s policy parameters can have on rewards received
by radars outside its κ-hop neighborhood, which is reasonable
when rewards decay as distance between agents increases.
We suspect that Assumption 6 can be proven to hold for our
setting, but leave this to future work.
Assumption 1 (Network Coverage). There exists a function
g : N+ × R+ → [1, ∞), strictly increasing in both entries,
such that, for all i ∈ N , all κ ∈ N+ , and any j ∈ Nκ−1 (i),
we have dij (s) ≥ g(κ, R), for all s ∈ S.
Remark 1. When κ = 1, taking g(κ, R) = R is a natural
choice in Assumption 1, since, for any j ∈ N1−1 (i), we have
dij (s) ≥ g(1, R) = R by definition of the neighbor relation
in G (see Section II-B). More generally, letting g(κ, R) = κR
satisfies Assumption 1 for a large class of network topologies,
such as when the radars are static and arranged in a grid
topology with grid cell side lengths R.
Assumption 2 (Pairwise Cost Independence). For each i ∈ N ,
we have the following two conditions: (i) ∇θi Jcj (θ) = 0, for
i
i
all j ∈ N \ {i}; (ii) Qcθ (s, a) = Qcθ (si , ai ), i.e., the value of
i
Qc depends on purely local state and action information.
Assumption 3 (Regularity Conditions). For all i, j ∈ N ,
RCS
we have inf s∈S Ri (s) ≥ 1, σ̄ RCS = sups∈S σij
(s) <
RCS
∞ and σ̄
> 0, c̄ = sups∈S cji (s) < ∞ and
c̄ > 0, σ = min{inf s,a σ i (s, a), inf s,a σκi (s, a)} > 0, and
¯
λ, Gt , Gr , G′t , G′r > 0.
Assumption 4 (Uniform Ergodicity). There exist ρ ∈ (0, 1)
and m ∈ R+ such that every joint policy πθ satisfies
dT V (dtπθ (·|s0 )||dθ (·)) ≤ mρt , for any
R s0 ∈ S andR for all t ≥
0, where dT V (q(·)||q ′ (·)) = supA | A q(x) dx − A q ′ (x) dx|
denotes the total variation distance between densities q, q ′ , and
dtπθ (·|s0 ) denotes the t-step state occupancy measure induced
by πθ over S given start state s0 .
Assumption 5 (Lipschitz Score Functions). For each i ∈ N ,
there exists Li > 0 such that ∇θi log πθi i (ai |si ) ≤ Li , for
all si ∈ S i , ai ∈ Ai .
Assumption 6 (Bounded Inter-agent Gradients). There exists
εκ > 0 such that, for each i ∈ N and all j ∈ Nκ−1 (i), we
have ∥∇θi Jrj (θ)∥ ≤ εκ , for all θ ∈ Θ.
We now establish properties of the radar problem enabling
decentralized solution of the CMAMDP problem. Due to the
form of the costs and rewards coupled with properties of the
SINR (4) and radar range equations (2) and (3), the CMAMDP
enjoys the following property.
Theorem 1. Let Assumptions 1, 2, 3, and 4 hold. For any
θ ∈ Θ, i ∈ N , sNκ (i) ∈ S Nκ (i) , aNκ (i) ∈ ANκ (i) , and f i ∈

{ri , ci }, we have


i
−1
−1
Qθf (sNκ (i) , sNκ (i) ), (aNκ (i) , aNκ (i) )


−1
i
−1
M |Nκ−1 (i)|
,
− Qfθ (sNκ (i) , s̄Nκ (i) ), (aNκ (i) , āNκ (i) ) ≤
g 2 (κ, R)
−1

−1

−1

−1

i

Proof. Assumption 2 renders the f = c case trivial, so we
give the proof only for the f i = ri case. Fix θ ∈ Θ, i ∈ N ,
and κ ∈ N+ . Let sNκ (i) ∈ S Nκ (i) , aNκ (i) ∈ ANκ (i) ,
−1
−1
−1
−1
−1
−1
sNκ (i) , s̄Nκ (i) ∈ S Nκ (i) , and aNκ (i) , āNκ (i) ∈ ANκ (i) .
−1
−1
Define s = (sNκ (i) , sNκ (i) ), s̄ = (sNκ (i) , s̄Nκ (i) ), a =
−1
−1
(aNκ (i) , aNκ (i) ), and ā = (aNκ (i) , āNκ (i) ).
i
By the definition of Qrθ , we have
i

i

Qrθ (s, a) − Qrθ (s̄, ā)
= Eπθ



∞
X

(16)


ri (st , at ) − Jri (θ) | s0 = s, a0 = a

− Eπθ



= Eπθ




ri (st , at ) − Jri (θ) | s0 = s̄, a0 = ā (17)


ri (st , at ) | s0 = s, a0 = a

∞

X
ri (st , at ) | s0 = s̄, a0 = ā ,

(18)

t=0

where
the
last
equality
holds
by
the
=
fact
that
Eπθ [Jri (θ) | s0 = s, a0 = a]
Eπθ [Jri (θ) | s0 = s̄, a0 = ā], since Assumption 4 implies
ergodicity of the Markov chain over S induced by πθ .
Recalling (6) and continuing from (18), we have
i
i
Qrθ (s, a) − Qrθ (s̄, s̄)

=

∞ h
X

(19)



Eπθ ri (st , at ) − rκi (st , at ) + rκi (st , at ) | s0 = s, a0 = a

t=0


i
− Eπθ ri (st , at ) | s0 = s̄, a0 = ā
=

∞ h
X

=

t=0 s′ ∈S a′ ∈A

− dtπθ (s′ |s0 = s̄, a0 = ā)
∞ X
(a) X

≤

X

≤

X

∞ X X
X

Giκ (s′ , a′ )

h

(26)

dtπθ (s′ |s0 = s, a0 = a) − dπθ (s′ )

t=0 s′ ∈S a′ ∈A

+ dtπθ (s′ |s0 = s̄, a0 = ā) − dπθ (s′ )
(c)

≤ sup Giκ (s′ , a′ )
s′ ,a′

∞ h
X

i

t=0
i
+ dT V (dtπθ (·|s0 = s̄, a0 = ā) || dπθ (·))
∞
X

(27)

dT V (dtπθ (·|s0 = s, a0 = a) || dπθ (·))

2mρt = sup Giκ (s′ , a′ )
s′ ,a′

t=0

2m
,
1−ρ

(28)
(29)

where inequality (a) follows by the triangle and CauchySchwarz inequalities and the fact that 0 ≤ πθ (a′ |s′ ) ≤ 1,
inequality (b) follows by adding and subtracting dθ (s′ ) and
applying the triangle inequality, (c) follows by taking suprema
and the definition of total variation distance, and (d) follows
by Assumption 4.
All that remains is to bound sups′ ,a′ Giκ (s′ , a′ ) . Fix s ∈
S, a ∈ A. Recall that
ri (s, a) =

hτii (s)ai
,
2
σ (s, a) + α

rκi (s, a) =

hτii (s)ai
,
(σκ (s, a))2 + β

(30)

where
α=

X



cji (s) hdji (s) + hτji (s) aj ,

(31)

j∈N \{i}

β=

X



cji (s) hdji (s) + hτji (s) aj .

(32)

j∈Nκ (i)\{i}

hτii (s)ai (α − β)
(σ 2 (s, a) + α)((σκi (s, a))2 + β)
(a)
hτii (s)ai |α − β|
= 2
|σ (s, a) + α||(σκi (s, a))2 + β|
(b) hτ (s)ai |α − β|
≤ ii
,
σ4

Giκ (s, a) =
(21)

(22)

t=0

=

Giκ (s′ , a′ ) · dtπθ (s′ |s0 = s, a0 = a)
− dtπθ (s′ |s0 = s̄, a0 = ā)

∞ X
(b) X

where (21) follows by the independence of a of the transition
dynamics p defined in Section II-B and the fact that rκi (s, a)
−1
−1
is independent of the value of sNκ (i) , aNκ (i) by definition.
Now notice that


Eπθ ri (st , at ) − rκi (st , at ) | s0 = s, a0 = a

(25)

In light of this, we can write

t=0

∞
X



t=0 s′ ∈S a′ ∈A

(20)



Eπθ ri (st , at ) − rκi (st , at ) | s0 = s, a0 = a


i
− Eπθ ri (st , at ) − rκi (st , at ) | s0 = s̄, a0 = ā ,

(24)


Giκ (s′ , a′ )πθ (a′ |s′ ) dtπθ (s′ |s0 = s, a0 = a)

s′ ,a′

t=0

− Eπθ

∞ X X
X

(d)

t=0
∞
X

i

≤ sup Giκ (s′ , a′ )

t=0
∞
X

i

Qrθ (s, a) − Qrθ (s̄, s̄)

−1

for all sNκ (i) , s̄Nκ (i) ∈ S Nκ (i) and all aNκ (i) , āNκ (i) ∈
−1
ANκ (i) , where M > 0 is a constant depending only on the
quantities in Assumptions 3 and 4.
i

expression holds for s0 = s̄, a0 = ā. Continuing from (21) and
(23), we have

Giκ (s′ , a′ )πθ (a′ |s′ )dtπθ (s′ |s0 = s, a0 = a), (23)

t=0 s′ ∈S a′ ∈A

where dtπθ denotes the t-step state distribution of Assumption
4 and Giκ (s′ , a′ ) = ri (s′ , a′ )−rκi (s′ , a′ ), and that an analogous

(33)
(34)
(35)

¯

where (a) follows by nonnegativity of hτii (s) and ai , and (b)
follows by nonnegativity of α and β and Assumption 3. By
(2) and Assumption 3,
hτii (s)ai =

RCS
Gt Gr σii
(s)λ2 ai
Gt Gr σ̄ RCS λ2 amax
≤
,
4
3
(4π) Ri (s)
(4π)3
(36)

whence it follows that

Combining (39), (40), and (46), we have
RCS 2 max

Giκ (s, a) ≤

Gt Gr σ̄
λ a
(4π)3 σ 4

|α − β|.

(37)

|α − β| ≤ c̄amax Nκ−1 (i)


G′t G′r λ2
Gt Gr σ̄ RCS λ2
+
,
(4π)2 g 2 (κ, R)
42 π 3 g 2 (κ, R)
(47)



¯

It remains to bound |α − β|. Notice that
X


α−β =
cji (s) hdji (s) + hτji (s) aj

which, combined with (37), gives us
(38)

j∈Nκ−1 (i)

X

≤ c̄amax

 d

hji (s) + hτji (s)

Gt Gr σ̄ RCS λ2 (amax )2 c̄
·
(4π)3
 ′ ′ 2

Gt Gr λ
Gt Gr σ̄ RCS λ2 Nκ−1 (i)
+
. (48)
(4π)2
42 π 3
g 2 (κ, R)

Giκ (s, a) ≤

(39)

j∈Nκ−1 (i)

so we need only bound hdji (s) and hτji (s). For hdji (s), notice
that, for j ∈ Nκ−1 (i), by (3) and Assumption 1 we have
hdji (s) =

G′t G′r λ2
G′t G′r λ2
≤
.
2
2
(4π) (dij (s))
(4π)2 g 2 (κ, R)

(40)

For hτji (s), we consider four possible cases that may arise
depending on the physical configuration of the radar network.
Fix j ∈ Nκ−1 (i).
Case 1: Rj (s) ≤ dij (s) ≤ Ri (s). In this case, we
know Ri (s)Rj (s) ≥ dij (s), whence
hτji (s) =

Gt Gr σ̄ RCS λ2
Gt Gr σ RCS (s)λ2
,
≤
2
2
3
(4π) Ri (s)Rj (s)
(4π)3 g 2 (κ, R)

(41)

where the inequality holds by Assumptions 1 and 3.
Case 2: Ri (s) ≤ dij (s) ≤ Rj (s). This case follows by
reasoning identical to Case 1.
Case 3: dij (s) ≤ min{Ri (s), Rj (s)}. In this case,
Ri (s)Rj (s) ≥ d2ij (s), whence
hτij (s) ≤

Gt Gr σ̄ R CSλ2 (a) Gt Gr σ̄ R CSλ2
≤
,
(4π)3 g 4 (κ, R)
(4π)3 g 2 (κ, R)

(42)

where (a) holds since g(κ, R) ≥ 1 by Assumption 1.
Case 4: dij (s) ≥ max{Ri (s), Rj (s)}. The triangle
inequality gives Ri (s) + Rj (s) ≥ dij (s), whence
Ri (s) ≥ g(κ, R) − Rj (s). If Rj ≥ 12 g(κ, R), then
hτij (s) =

Gt Gr σ RCS (s)λ2
Gt Gr σ RCS (s)λ2
≤
2
2
3
(4π) Ri (s)Rj (s)
(4π)3 Rj2 (s)

(43)

Finally, combining (29) with (48), we have
i

i

Qrθ (s, a) − Qrθ (s̄, s̄) ≤

M Nκ−1 (i)
,
g 2 (κ, R)

(49)

where
M=



2mGt Gr σ̄ RCS λ4 (amax )2 c̄ G′t G′r
Gt Gr σ̄ RCS
+
. (50)
(1 − ρ)(4π)3
(4π)2
42 π 3

In Theorem 1, the scalar M intuitively corresponds to the
maximum possible expected contribution of radar j to SIN Ri
before accounting for signal attenuation due to distance. When
κ = 1 and g(κ, R) = R, for example, M |N1−1 (i)|/R2 bounds
the maximum possible contribution to SIN Ri originating outside radar i’s immediate communication neighborhood N1 (i),
decayed by the square of the communication radius R. Furthermore, for general g, as κ > 1 increases Nκ−1 (i) will decrease
and g(κ, R) will increase, resulting in a tighter overall bound
M |Nκ−1 (i)|/g 2 (κ, R). This bound can be further tightened by
increasing the communication radius R.
We now proceed to the main result of this section, which
establishes bounds on the accuracy of gradient estimators
constructed using only local neighborhood information. We
first define local Q function approximations. Fix i ∈ N , and
−1
Nκ−1 (i)
let wi : S Nκ (i) × AP
→ [0, 1] be an arbitrary weighting
−1
−1
function satisfying
wi (s̄, ā) = 1. Let
s̄∈S Nκ (i) ,ā∈ANκ (i)
i
e f (sNκ (i) , aNκ (i) ) denote agent i’s local approximation of
Q
θi

Qf (s, a), for f i ∈ {ri , ci }, defined by
i

e f (sNκ (i) , aNκ (i) ) =
Q
θ


X
i
Qfθ (sNκ (i) , s̄), (aNκ (i) , ā) wi (s̄, ā).

(51)

−1
−1
s̄∈S Nκ (i) ,ā∈ANκ (i)

(44)

For any such weighting function wi , the following approximation result is implied by Theorem 1.

On the other hand, if Rj (s) < 12 g(κ, R), then Ri (s) ≥
g(κ, R) − Rj (s) > 21 g(κ, R). Thus, by the same argument
as above,

Theorem 2. Let the conditions of Theorem 1 and Assumption
5P hold. Fix i, j ∈ N , f i ∈ {ri , ci }, define f (s, a) =
i i i
i∈N f (s , a ), and let M be as in Theorem 1. We have:

Gt Gr σ̄ RCS λ2
≤
.
(4π)3 14 g 2 (κ, R)

Gt Gr σ̄ RCS λ2
hτji (s) ≤
.
(4π)3 14 g 2 (κ, R)

(45)

Taking the maximum over all four cases, we obtain
hτji (s) ≤

Gt Gr σ̄ RCS λ2
.
42 π 3 g 2 (κ, R)

(46)

i

i

−1

κ (i)|
e f (sNκ (i) , aNκ (i) ) − Qf (s, a)| ≤ M |N
,
(i) |Q
θ
θ
g 2 (κ,R)
for all s ∈ S, a ∈ A;
−1
M Li |Nκ
(j)|
d
d
i
i
(ii) h
(θ) − ∇θi Jf j (θ) ≤
, where h
(θ) =
g 2 (κ,R)
f jh
fj
i
j
f
e (sNκ (j) , aNκ (j) )∇θi log π i i (ai |si ) ;
Eπθ Q
θ
θ
ci (θ) − ∇ i Jf (θ) ≤
(iii) If Assumption 6 also holds, then h
θ
f
−1
M n̄Li |Nκ
(i)|
−1
c
hi (θ)
=
+
N (i) εκ ,
where

g 2 (κ,R)

κ

f

hP

j
e f Nκ (j) , aNκ (j) )∇θi log π i i (ai |si )
j∈Nκ (i) Qθ (s
θ
and n̄ = maxj∈N Nκ−1 (j) .

Eπθ

i

where (a) follows by Part (ii) and Assumption 6, and (b)
follows by the definition of n̄ in the statement of the theorem.

(iv) If
P

Assumption
6
also
holds,
then
P
jd
l
i
η
h
(θ)
−
η
∇
J
(θ)
i
l
θ f
j∈Nκ (i)
l∈N
fj
i
−1
P
P
j M L |Nκ (j)|
≤
+ j∈Nκ−1 (i) η j εκ , for all
j∈Nκ (i) η
g 2 (κ,R)
η ∈ Rn .

Proof. As in Theorem 1, the f i = ci case is trivial by
Assumption 2, so consider only the f i = ri case. Fix
wi , s ∈ S, and a ∈ A.

−1
−1
s̄∈S Nκ (i) ,ā∈ANκ (i)

g 2 (κ, R)

=

g 2 (κ, R)

.

(54)

Part (ii). By the policy gradient theorem [18],
i
h j
∇θi Jrj (θ) = Eπθ Qrθ (s, a)∇θi log πθi i (ai |si ) .

(55)

This means we can write
i
hc
(θ) − ∇θi Jrj (θ)
(56)
rj


 rj Nκ (j) Nκ (j)
j
r
i
i
i
e θ (s
,a
) − Qθ (s, a) ∇θi log πθi (a |s )
= Eπθ Q
(57)
h
i
(a)
j
j
e rθ (sNκ (j) , aNκ (j) ) − Qrθ (s, a) · ∇θi log π i i (ai |si )
≤ Eπθ Q
θ
(58)
"
#
−1
i
−1
(b)
M Nκ (j)
M L Nκ (j)
≤ Eπθ
· Li =
,
(59)
g 2 (κ, R)
g 2 (κ, R)

where (a) follows by Jensen’s inequality and the CauchySchwarz inequality, and (b) follows by Theorem 1 and
Assumption 5.
ci (θ), we have
Part (iii). By (15) and the definition of h
r
X
X
i (θ). (60)
ci (θ) =
∇θi Jr (θ) =
∇θi Jrj (θ), h
hc
r
rj
j∈N

j∈Nκ (i)

We thus have
cir (θ) − ∇ i Jr (θ)
h
θ

(61)


X 
i
hc
(θ) − ∇θi Jrj (θ) −
rj

≤

X

(a)

X
j∈Nκ (i)

X

i
hc
(θ) − ∇θi Jrj (θ) +
rj

∇θi Jrj (θ)

(62)

∥∇θi Jrj (θ)∥ (63)

−1
j∈Nκ
(i)

j∈Nκ (i)

≤

X
−1
j∈Nκ
(i)

j∈Nκ (i)

X
M Li Nκ−1 (j)
+
εκ
2
g (κ, R)
−1

(b) M Li n̄ N −1 (j)
κ
≤
g 2 (κ, R)

(64)

j∈Nκ

+

Nκ−1 (i) εκ ,

X

η j ∇θi Jrj (θ)

(66)

j∈N

h
i
i
(θ)
−
∇
η j hc
i Jr j (θ) −
j
θ
r

X

X

η j ∇θi Jrj (θ)

−1
j∈Nκ
(i)

j∈Nκ (i)

≤

X

(65)

ηj

X

i
(θ) − ∇θi Jrj (θ) +
hc
rj

η j ∥∇θi Jrj (θ)∥

−1
j∈Nκ
(i)

j∈Nκ (i)

(68)
≤

X
j∈Nκ (i)

M Nκ−1 (i)

M Nκ−1 (i)

−1
−1
s̄∈S Nκ (i) ,ā∈ANκ (i)

=

=

(b)

(53)
wi (s̄, ā)

i
η j hc
(θ) −
rj

j∈Nκ (i)

(a)

e ri (sNκ (i) , aNκ (i) ) − Qri (s, a)
Q
(52)
θ
θ


X
i
ri
Nκ (i)
Nκ (i)
ri
≤
w (s̄, ā) Qθ (s
, s̄), (a
, ā) − Qθ (s, a)

X

X

(67)

e ri , we have
Part (i). By the definition of Q
θ

≤

Part (iv). Fix η ∈ Rn . We know that

M Li Nκ−1 (j)
ηj
+
g 2 (κ, R)

X

η j εκ ,

(69)

−1
j∈Nκ
(i)

where (a) follows by the triangle and Cauchy-Schwarz inequalities and (b) follows by Part (ii) and Assumption 6.
Theorem 2 provides approximate policy gradient expressions that can be computed using only local neighborhood
information and establishes corresponding error bounds depending on the system model, communication radius, choice
of κ, policy design, and placement of radars within the
network. The bounds arise due to signal decay inherent in
the radar range equations (2)-(3), manifested in the reward
(5). These bounds provide a guide to the selection of κ,
design of g from Assumption 1, radar placement based on
the effective communication range R, properties of the system
model captured by M , properties of the parametric policy class
through the Lipschitz constants Li , and the number of radars
n. Importantly, the result demonstrates that, for appropriate
choice of design parameters given the underlying system, the
expressions presented in Theorem 2 provide good approximations of the desired policy gradients while using only local
neighborhood information. Armed with these policy gradient
expressions, we next turn to development of decentralized
MARL algorithms leveraging them to solve the problems
proposed in Section II-B.
IV. A LGORITHMS
In this section, we derive decentralized, policy gradientbased MARL methods for solving problems (Pκmax ) and
(Pκmin ) proposed in Section II-B. In each case, we solve
the problem using a decentralized, stochastic policy gradient descent-ascent procedure, or decentralized saddle point
policy gradient (D-SP-PG), on the Lagrangian relaxation
corresponding to the original problem. Performing gradient
ascent-descent on the Lagrangian relaxation is a standard
solution technique for constrained RL problems [22], while
our extension to the MARL setting is inspired by decentralized
approaches to optimization over networks [23], [24]. For
each of problems (Pκmax ) and (Pκmin ), we first formulate
the Lagrangian of the corresponding problem, then provide
gradient expressions and local approximations for each agent,
and finally state the corresponding decentralized algorithm.

Algorithm 1 SINR Maximization with Cost Constraints
1: Input: stepsizes αt , βt , ζt
ci
ci
2: Initialize: initialize s0 , θ0 , ν0 , set t = µc0 = µr0 = 0, and
i
i
er , Q
e c to 0, for all i ∈ N
set all entries of Q
3: for agent i ∈ N do
N (i)
4:
share sit with Nκ (i), receive st κ from Nκ (i)
N (i)
5:
take action ait ∼ πθi i (·|st κ )
t
6:
observe rti = ri (st , at ), cit = ci (st , at )
i
ci = (1 − ζ )µd
ci
7:
µc
t t−1 + ζt ct
t
ri + ζ ri
r i = (1 − ζ )µd
8:
µc
t

t

9:
10:
11:
12:
13:

t t

t−1

N (i)

share ait with Nκ (i), receive at κ from Nκ (i)
ci , sNκ (i) , aNκ (i) , ζ )
e ct i = UPDATE_Q(Q
e ci , cit , µc
Q
t
t
t
t
t−1
i
i
i
Nκ (i) Nκ (i)
r
r
i c
r
e
e
Q = UPDATE_Q(Q , r , µ , s
,a
,ζ )
t

t

t−1

t

t

t

t

N (i)
κ (i)
ci , ν i with N (i), receive
e rt i (sN
share Q
, at κ ), µc
κ
t
t
t
Nκ (j)
c
κ (j)
cj , ν j from N (i)
e rj (sN
Q
,
a
),
µ
κ
t
t
t
t
t
form estimates:

d
i
h
r,t =

X

Algorithm 2 Cost Minimization with SINR Constraints
1: Input: stepsizes αt , βt , ζt
ci
ci
2: Initialize: initialize s0 , θ0 , η0 , ν0 , set t = µc0 = µr0 = 0,
i
i
er , Q
e c to 0, for all i ∈ N
and set all entries of Q
3: for agent i ∈ N do
N (i)
4:
share sit with Nκ (i), receive st κ from Nκ (i)
N (i)
5:
take action ait ∼ πθi i (·|st κ )
t
6:
observe rti = ri (st , at ), cit = ci (st , at )
i
ci = (1 − ζ )µd
ci
7:
µc
t t−1 + ζt ct
t
ri + ζ ri
r i = (1 − ζ )µd
8:
µc
10:
11:
12:
13:

t

j

j∈Nκ (i)



update:
i
θt+1
= θti − αt

j∈Nκ (i)
i
νt+1
= νti − βt

X

i

u −

t

t

for all j ∈ Nκ (i)
14:

i
ν j hd
ci ,t

t

j
N (j)
N (i)
κ (i)
i
[
e rt (sN
h
=Q
, at κ )∇θi log πθi i (ait |st κ ),
t
r j ,t

update:
X

t

t

t−1

i
N (i)
N (i)
κ (i)
i
e ct (sN
hd
=Q
, at κ )∇θi log πθi i (ait |st κ )
t
ci ,t

t

d
i −
h
r,t

N (i)

κ (i)
ci , ν i with N (i), receive
e rt i (stNκ (i) , aN
share Q
), µc
κ
t
t
t
Nκ (j)
c
κ (j)
cj , ν j from N (i)
e rj (sN
Q
,
a
),
µ
κ
t
t
t
t
t
form estimates:

N (i)
κ (j)
e rt (stNκ (j) , aN
Q
)∇θi log πθi i (ait |st κ )
t

i
θt+1
= θti + αt

t t

t−1

share ait with Nκ (i), receive at κ from Nκ (i)
ci , sNκ (i) , aNκ (i) , ζ )
e ct i = UPDATE_Q(Q
e ci , cit , µc
Q
t
t
t
t
t−1
i
i
i
Nκ (i) Nκ (i)
r
r
i c
r
e
e
Q = UPDATE_Q(Q , r , µ , s
,a
,ζ )

i
κ (i)
i
e ct (stNκ (i) , atNκ (i) )∇θi log π i i (ait |sN
)
hd
=Q
t
θ
ci ,t

14:

t

t

9:


1+


i
ν j hd
−
ci ,t

X
j∈Nκ (i)

X

i
[
ηj h
r j ,t



j∈Nκ (i)



ri
γmin − µc
t

X cj 
i
νt+1
= νti + δt ui −
µct


cj
µc
t

i
ηt+1
= ηti + βt

j∈Nκ (i)

15:
t←t+1
16: end for

j∈Nκ (i)

15:
t←t+1
16: end for

A. Sum-of-SINRs Maximization
The Lagrangian of problem (Pκmax ) is given by
X
X

LQ (θ, ν) = Jr (θ) +
ν j uj −
Jck (θ) ,
j∈N

(70)

k∈Nκ (j)

i

P
where we recall that Jr (θ) =
j∈N Jr j (θ). In order to
solve (Pκmax ), our goal is to instead solve the problem
maxθ minν LQ (θ, ν) by alternating between gradient ascent
in θ and gradient descent in ν using stochastic approximates
of the gradient expressions
X
X
∇θi LQ (θ, ν) = ∇θi Jr (θ) −
νj
∇θi Jck (θ) (71)
j∈N

= ∇θi Jr (θ) −
X

i

N (i)
N (i)
κ (i)
κ (i)
e ft (sN
e f (sN
Q
, at κ ) = (1 − ζt )Q
, at κ )
t
t−1 t


i
di
Nκ (i)
κ (i)
e f (sN
+ ζt fti − µft + Q
,
a
)
t
t−1 t
i

i

e ft (sNκ (i) , aNκ (i) ) = Q
e f (sNκ (i) , aNκ (i) ), for all
Q
t−1
N (i)

(sNκ (i) , aNκ (i) ) ̸= (st κ
3: return

e ft
Q

N (i)

, at κ

)

i

k∈Nκ (j)

X

ν j ∇θi Jci (θ)

(72)

j∈Nκ (i)

∇ν i LQ (θ, ν) = ui −

Algorithm 3 UPDATE_Q
i
f i Nκ (i) Nκ (i)
e f , fti , µc
1: Input: Q
, at
, ζt
t , st
t−1
2: perform updates

Jcj (θ).

(73)

j∈Nκ (i)

Here equation (72) follows from Assumption 2 combined with
our assumption that the κ-hop neighbor relation is symmetric,
i.e., that i ∈ Nκ (j) if and only if j ∈ Nκ (i).
We can now present a decentralized learning algorithm for
approximately solving this problem, where each agent only

needs information available within its local κ-hop neighborhood. Using the approximations provided by Theorem 2, for
suitable choice of κ and R we have
X
i (θ),
ci (θ) −
∇θi LQ (θ, ν) ≈ h
ν j hc
(74)
r
ci
j∈Nκ (i)

∇ν i LQ (θ, ν) ≈ u −
i

X
j∈Nκ (i)

cj (θ),
µc

(75)

cj (θ) ≈ J j (θ) is a suitable approximation, such
where µc
c
as a cumulative or exponential moving average. Using these
expressions, we provide the D-SP-PG scheme for solving
(Pκmax ) in Algorithm 1.

B. Power Minimization with SINR Threshold
The Lagrangian of problem (Pκmin ) is given by
X
LR (θ, η, ν) = Jc (θ) +
η j (γmin − Jrj (θ))

V. C ONCLUSION
(76)

j∈N

−

X

X

ν j uj −

j∈N


Jck (θ) ,

we have presented the Q updates of Algorithm 3 in the
easily analyzable tabular form, which is only applicable to
finite state and action spaces. We emphasize that practical
variants leveraging standard neural network architectures for
Q function approximation can be substituted in Algorithm 3
to enable application to the continuous spaces of Section II.

(77)

k∈Nκ (j)

P
where we recall that Jc (θ) =
j∈N Jcj (θ). In order to
min
solve (Pκ ), we instead solve the saddle point problem
minθ maxη,ν LR (θ, η, ν) by alternating between stochastic
gradient descent in θ and ascent in η and ν. Differentiating
(76), (77) with respect to θi gives
X
∇θi LR (θ, η, ν) = ∇θi Jc (θ) −
η j ∇θi Jrj (θ)
(78)

In this work, we have considered the specific use-case of
power allocation for target detection in a radar network to
illustrate how signal strength decay properties inherent in
wireless communications and radar networks can be used
to develop decentralized, scalable MARL methods. Future
directions include convergence analysis of Algorithms 1 and
2, experimental evaluation of neural network-based versions of
our algorithms, extension of our approach to additional applications in communications and radar networks, and extension
of our approach to joint motion planning and power allocation
in target detection in mobile radar networks.

j∈N

+

X

ν

j∈N

= ∇θi Jci (θ) −

X

j

X

∇θi Jck (θ)

(79)

k∈Nκ (j)

X

j

η ∇θi Jrj (θ) +

j∈N

k

ν ∇θi Jci (θ)

k∈Nκ (i)

(80)


= 1+

X



ν j ∇θi Jci (θ) −

X

η j ∇θi Jrj (θ),

j∈N

k∈Nκ (i)

(81)
where the first equality holds by Assumption 2 combined with
our assumption that the κ-hop neighbor relation is symmetric,
i.e., that i ∈ Nκ (j) if and only if j ∈ Nκ (i). Finally,
differentiating with respect to η i and ν i yields
∇ηi LR (θ, η, ν) = γmin − Jri (θ),
X
Jcj (θ).
∇ν i LR (θ, η, ν) = ui −

(82)
(83)

j∈Nκ (i)

Our goal is again to perform the necessary updates in
a decentralized manner, with each agent only using locally
available information. Using the approximations provided by
Theorem 2, for suitable choice of κ and R we have

X 
X
i (θ) −
i (θ)
∇θi LR (θ, η, ν) ≈ 1 +
ν k hc
η j hc
ci
rj
k∈Nκ (i)

j∈Nκ (i)

(84)
r i (θ)
∇ηi LR (θ, η, ν) ≈ γmin − µc
X
cj (θ),
∇ν i LR (θ, η, ν) ≈ ui −
µc

(85)
(86)

j∈Nκ (i)

c
r i (θ) ≈ J l (θ) and µ
cj (θ) ≈ J j (θ) are suitable
where µc
c
r
approximations. Using these expressions, we provide the DSP-PG scheme for solving (Pκmin ) in Algorithm 2.
Remark 2. We note that, in order to lay the groundwork
for convergence analysis of our algorithms in future work,

R EFERENCES
[1] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar,
“Fully decentralized multi-agent reinforcement learning with networked
agents,” in International conference on machine learning. PMLR, 2018,
pp. 5872–5881.
[2] Wesley A Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang, Tamer
Başar, and Ji Liu, “A multi-agent off-policy actor-critic algorithm for
distributed reinforcement learning,” in 21st International Federation of
Automatic Control (IFAC) World Congress. IEEE, 2020, pp. 1549–1554.
[3] Krishna Chaitanya Kosaraju, Seetharaman Sivaranjani, Wesley A Suttle,
Vijay Gupta, and Ji Liu, “Reinforcement learning based distributed
control of dissipative networked systems,” IEEE Transactions on Control
of Network Systems, vol. 9, no. 2, pp. 856–866, 2021.
[4] Kaiqing Zhang, Zhuoran Yang, and Tamer Başar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,”
Handbook of reinforcement learning and control, pp. 321–384, 2021.
[5] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor, “A survey
and critique of multiagent deep reinforcement learning,” Autonomous
Agents and Multi-Agent Systems, vol. 33, no. 6, pp. 750–797, 2019.
[6] Sven Gronauer and Klaus Diepold, “Multi-agent deep reinforcement
learning: a survey,” Artificial Intelligence Review, vol. 55, no. 2, pp.
895–943, 2022.
[7] Changxi Zhu, Mehdi Dastani, and Shihan Wang, “A survey of multiagent deep reinforcement learning with communication,” Autonomous
Agents and Multi-Agent Systems, vol. 38, no. 1, pp. 4, 2024.
[8] Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li, “Scalable
multi-agent reinforcement learning for networked systems with average
reward,” Advances in Neural Information Processing Systems, vol. 33,
pp. 2074–2086, 2020.
[9] Guannan Qu, Adam Wierman, and Na Li, “Scalable reinforcement
learning for multiagent networked systems,” Operations Research, vol.
70, no. 6, pp. 3601–3628, 2022.
[10] Lijun Zhang, Lin Li, Wei Wei, Huizhong Song, Yaodong Yang, and Jiye
Liang, “Scalable constrained policy optimization for safe multi-agent
reinforcement learning,” Advances in Neural Information Processing
Systems, vol. 37, pp. 138698–138730, 2024.
[11] Mostafa M Shibl, Wesley A Suttle, and Vijay Gupta, “Scalable
natural policy gradient for general-sum linear quadratic games with
known parameters,” in 7th Annual Learning for Dynamics & Control
Conference. PMLR, 2025, pp. 1–14.
[12] Andrea Goldsmith, Wireless communications, Cambridge university
press, 2005.
[13] Mark A Richards, Fundamentals of radar signal processing, Mcgrawhill New York, 2005.
[14] Simon Haykin, “Cognitive radar: a way of the future,” IEEE signal
processing magazine, vol. 23, no. 1, pp. 30–40, 2006.

[15] Anastasios Deligiannis and Sangarapillai Lambotharan, “A bayesian
game theoretic framework for resource allocation in multistatic radar
networks,” in 2017 IEEE Radar Conference (RadarConf). IEEE, 2017,
pp. 0546–0551.
[16] Chenguang Shi, Sana Salous, Fei Wang, and Jianjiang Zhou, “Power
allocation for target detection in radar networks based on low probability
of intercept: A cooperative game theoretical strategy,” Radio Science,
vol. 52, no. 8, pp. 1030–1045, 2017.
[17] Luke Snow, Vikram Krishnamurthy, and Brian M Sadler, “Identifying
coordination in a cognitive radar network-a multi-objective inverse reinforcement learning approach,” in ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2023, pp. 1–5.
[18] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour, “Policy gradient methods for reinforcement learning with function
approximation,” Advances in neural information processing systems,
vol. 12, 1999.
[19] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu, “A finitetime analysis of two time-scale actor-critic methods,” Advances in
Neural Information Processing Systems, vol. 33, pp. 17617–17628, 2020.
[20] Xuyang Chen and Lin Zhao, “Finite-time analysis of single-timescale
actor-critic,” Advances in Neural Information Processing Systems, vol.
36, pp. 7017–7049, 2023.
[21] Wesley A Suttle, Amrit Bedi, Bhrij Patel, Brian M Sadler, Alec Koppel,
and Dinesh Manocha, “Beyond exponentially fast mixing in averagereward reinforcement learning via multi-level monte carlo actor-critic,”
in International Conference on Machine Learning. PMLR, 2023, pp.
33240–33267.
[22] Santiago Paternain, Luiz Chamon, Miguel Calvo-Fullana, and Alejandro
Ribeiro, “Constrained reinforcement learning has zero duality gap,”
Advances in Neural Information Processing Systems, vol. 32, 2019.
[23] Alec Koppel, Felicia Y Jakubiec, and Alejandro Ribeiro, “A saddle
point algorithm for networked online convex optimization,” IEEE
Transactions on Signal Processing, vol. 63, no. 19, pp. 5149–5164, 2015.
[24] Alec Koppel, Brian M Sadler, and Alejandro Ribeiro, “Proximity without consensus in online multiagent optimization,” IEEE Transactions
on Signal Processing, vol. 65, no. 12, pp. 3062–3077, 2017.

