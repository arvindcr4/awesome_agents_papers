Networked Multi-Agent Reinforcement Learning with Emergent
Communication
Shubham Gupta∗and Rishi Hazra∗ and Ambedkar Dukkipati
Department of Computer Science and Automation
Indian Institute of Science
Bangalore - 560012, India.
[shubhamg, rishihazra, ambedkar]@iisc.ac.in

arXiv:2004.02780v2 [cs.MA] 9 Apr 2020

Abstract

ment that can only be partially observed by each of them.
Further, we assume that these agents are interconnected via
a fixed network topology and that they have been endowed
with the ability to communicate with each other along the
edges of this network. The objective of agents is to learn a
communication protocol so as to cooperatively maximize
the rewards provided to them by the environment. This
problem setting has two interesting features:
1. Communication: Communication allows the agents
to augment their local observations with additional information that is necessary to achieve global cooperation. It also enables a more dynamic form of coordination among agents (for example, agents soliciting help from
their peers after entering a particular state) as opposed to
the well known centralized training, decentralized execution paradigm (Lowe et al., 2017) where the agents are
trained together but must act independently in a decentralized fashion post deployment.
2. Network of agents: We assume that agents can only
communicate along the edges of a fixed underlying network. We believe that this is a more practical scenario as
direct communication between all agents may not always
be possible due to constraints like geographical separation,
limited communication bandwidth and so on. Moreover,
having a fixed network allows us to study the relationship
between the emergent communication and the topology of
the underlying network (Section 5). We restrict our attention to discrete communication, i.e., agents communicate
by exchanging discrete symbols. This has been done to: (i)
facilitate analysis of emergent language; and (ii) conserve
communication bandwidth which is important for practical
applications.
Emergent communication has been studied both in the
context of referential games (Havrylov and Titov, 2017;
Mordatch and Abbeel, 2018) and for developing abstract
strategies (Cao et al., 2018; Gupta and Dukkipati, 2019).
While these approaches offer many insights into emergent
communication, their applicability is limited in practice.
More generic approaches, like TarMAC (Das et al., 2019)
which explores the utility of attention in deciding whether
a given pair of agents may communicate with each other or
not, have been proposed but they lack in-depth analysis of
communication. As opposed to (Das et al., 2019): (i) we
use discrete communication which helps us in performing
a detailed analysis of communication and, (ii) we only allow agents to communicate via an underlying network, i.e.,

Multi-Agent Reinforcement Learning (MARL) methods
find optimal policies for agents that operate in the presence of other learning agents. Central to achieving this is
how the agents coordinate. One way to coordinate is by
learning to communicate with each other. Can the agents
develop a language while learning to perform a common
task? In this paper, we formulate and study a MARL problem where cooperative agents are connected to each other
via a fixed underlying network. These agents can communicate along the edges of this network by exchanging discrete
symbols. However, the semantics of these symbols are not
predefined and, during training, the agents are required to
develop a language that helps them in accomplishing their
goals. We propose a method for training these agents using
emergent communication. We demonstrate the applicability
of the proposed framework by applying it to the problem of
managing traffic controllers, where we achieve state-of-theart performance as compared to a number of strong baselines. More importantly, we perform a detailed analysis of
the emergent communication to show, for instance, that the
developed language is grounded and demonstrate its relationship with the underlying network topology. To the best
of our knowledge, this is the only work that performs an in
depth analysis of emergent communication in a networked
MARL setting while being applicable to a broad class of
problems1 .

1

Introduction

Co-existing intelligent agents affect each other in nontrivial ways. Any change in one agent’s policy modifies the
other agent’s perception about the environment dynamics.
This turns the environment non-stationary and the learning problem becomes hard. Approaches that try to independently learn optimal behavior for agents do not perform
well in practice (Tan, 1993). Thus, it is important to develop
models that have been tailored towards training multiple
agents simultaneously. Multi-agent reinforcement learning
(MARL) provides the formalism to do so.
In this paper, we consider a multi-agent setting where a
certain number of cooperative agents co-exist in an environ∗ Equal Contribution
1 short paper accepted at AAMAS 2020

1

certain agents may never directly communicate with each
other, thus the agents must learn to perform well in a more
practical but also more constrained environment. This also
enables the study of the effect of underlying network topology on emergent language.
Many real world problems can be cast in this framework.
For example, consider the problem of intelligently managing traffic in a city. The nodes in the network (i.e., the
agents) correspond to traffic controllers and the edges correspond to roads. The controllers must act cooperatively to
ensure a smooth flow of traffic by maximizing an appropriate notion of reward. We present the traffic management
problem (Section 4) as a particular instantiation of the proposed abstract MARL problem (Section 3). Although we
provide a number of other concrete examples in Section 3,
for clarity of exposition and to be more concrete, in this paper, we only focus on the problem of intelligently managing
traffic as a case study. This problem was chosen because of
the easy availability of high quality simulators and because
it subsumes a number of interesting problems like routing
of network packets, air traffic control and so on.
Our main contributions are: formulation of the abstract
MARL problem with networked agents and emergent communication as stated above (Section 3), demonstration of
the effectiveness of proposed approach using traffic management as a case study (Section 4) and most importantly
analysis of the emergent communication (Section 5) to investigate: (i) utility of communication; (ii) grounding of
language (i.e., whether words in the language refer to physical actions); and (iii) interplay between the underlying network topology and emergent language.

2

ing the desired task (Sukhbaatar et al., 2016; Mordatch and
Abbeel, 2018; Cao et al., 2018; Gupta and Dukkipati, 2019;
Das et al., 2019). Some of these approaches use continuous
communication (Sukhbaatar et al., 2016; Das et al., 2019)
while others use discrete communication. We use emergent
discrete communication for reasons described in Section 1.
As opposed to existing methods, the emergent communication in our method is influenced by the structure of the
underlying network.
MARL over networks has also been studied in (Zhang
et al., 2018) in a fully decentralized training setup where
local parameters of the agents are shared through communication. However, (Zhang et al., 2018) assume that all agents
have full access to the global environment state while we allow partial observability. (Sukhbaatar et al., 2016) also use
communication in traffic networks, but: (i) we formulate
this as a network problem with traffic controllers as agents
as opposed to treating vehicles as agents; and (ii) we use
network restricted, discrete communication. Our approach
is very similar to DIAL (Foerster et al., 2016) which uses
a centralized training and a decentralized execution with
communication channels. However, we use an attention
mechanism in our setup to prioritize incoming messages in
addition to the use of Gumbel-Softmax (Jang et al., 2016)
which yields better quality gradients during training.
The simplest and one of the most widely used approaches
to tackle the traffic management problem is the Fixed-time
Control (Miller, 1963; Webster, 1958), which uses a predefined cycle for planning. Self-Organizing Traffic Light
Control (SOTL) (Cools et al., 2013) method switches the
traffic lights when the number of vehicles crosses a predefined threshold. These conventional traffic control methods rely on assumptions that do not hold in practice. In
the recent past, reinforcement learning methods have also
been used for dynamically adapting to the traffic conditions (Prabuchandran K.J. et al., 2014; Li et al., 2016;
Wei et al., 2018). However, these approaches consider the
agents to be independent of one another, thus leading to a
non-stationary environment.

Related Work

Independent training of agents in a MARL setting often results in poor performance (Tan, 1993). A straightforward
way to address this issue is to view the collection of agents
as a single meta-agent and then train this meta-agent using existing single agent reinforcement learning techniques.
However, such an approach is not scalable as the action
space of the meta-agent grows exponentially with the number of agents.
The centralized training, decentralized execution
paradigm (Lowe et al., 2017) avoids non-stationarity issues
during training by providing the global state information
to all agents. Once trained, these agents can be executed
independently of each other. There are two common issues
associated with this paradigm: (i) as the number of agents
grows, the centralized training step gets harder; and (ii) as
during the test time the agents have to act independently,
this strategy is not optimal in scenarios where a more dynamic form of context dependent coordination is required.
Decentralized training, decentralized execution approaches
(Wen et al., 2019) address the first issue but not the second
one.
Communication enables agents to exchange information
even during the deployment phase, thus providing a solution
to the problem of achieving dynamic coordination. Certain approaches use a fixed communication protocol (Zhang
et al., 2018) while others learn to communicate while solv-

3

Proposed Markov Games with
Emergent Communication

Markov games (Littman, 1994) are used for modeling
multi-agent environments. Let S be the set of all environment states and s(t) ∈ S be the state of environment
at time t. At time t, observation function fi : S → Oi
(t)
yields the observation, oi ∈ Oi , for agent i where Oi
is its observation space. Based on the observation, each
(t)
agent i chooses an action ai ∈ Ai using its policy πi ,
(t)
(t)
i.e., ai ∼ πi (·|oi ). The state is then updated using
the transition function T : S × A1 × · · · × AN × S →
[0, 1]. Agents receive rewards via the reward functions
ri : S × A1 × · · · × AN → R, i = 1, 2, . . . , N . The
goal is to find optimal policies πi∗ : Oi × Ai → [0, 1],
i = 1, 2, . . . , N that maximize the expected long term reP
(t)
(t)
ward, Ri = Eπ [ t γ t ri ] for each agent i. Here ri is
agent i’s reward at time t, γ ∈ (0, 1] is the discount factor
and, π = π1 × π2 × · · · × πN .
2

We model the problem as a Markov game with two additional assumptions: (i) let V = {1, 2, . . . , N } be the set of
agents, we assume that these agents are interconnected via
an underlying network with edge set E; and (ii) agents can
communicate with their immediate neighbors in the underlying network using messages drawn from a discrete space.
A message broadcasted by an agent at time t is received by
its neighbors at time t + 1. The observation space of agents
is augmented to also consider the messages received by it.
While agents maximize their own rewards, during training, we allow them to provide feedback on the quality of
received messages to their neighbors via gradients. This
provides incentives to the agents to send messages that help
their neighbors in maximizing their rewards thereby ensuring that the agents act in a cooperative manner. Communication between competitive agents is an interesting problem but it poses a different set of challenges (for example,
agents may mislead each other by conveying false information), hence, in order to perform a meaningful analysis of
emergent communication, we restrict ourselves to a cooperative setting in this paper.
There are numerous practical scenarios that can be modeled this way. Consider, for instance, an intelligent electrical distribution network: in this application, agents represent power stations and the underlying network corresponds
to the electrical grid. Each agent has a production capacity and it may choose to share the power generated by it
with a neighboring agent (action space). All agents observe
various attributes like power requirements, consumer demand and so on (observation space) and they have to meet
the local demand which changes stochastically. Rewards
ri measure the success of agents in meeting their local demands without wasting surplus power. As another application, consider a supply chain where interconnected warehouses (agents) have to manage their inventory to meet the
local demand. As before, agents can choose to ship goods
that they have in their inventory to their neighbors (action
space). The warehouses have to meet stochastically changing demands and must learn to communicate effectively in
order to share goods so that an appropriate level of inventory is maintained at each warehouse.
As mentioned earlier, in this paper, we study intelligent
traffic management as a concrete instantiation of the proposed MARL problem (Section 4). For all real world applications that we have mentioned above, transparency in
decision making is important and using discrete communication is a step in this direction as we believe that it is more
interpretable. Towards this end, we demonstrate that the
emergent language is grounded, i.e. messages correspond
to actions in Section 5. If agents communicate using discrete symbols, humans can potentially inspect and interpret
the conversation logs.

3.1

Figure 1: Traffic networks used in our experiments. On the
left is a 10 agent network (network 1) and on the right is a
28 agent network (network 2) that has 14 agents in each of
the sub-networks (A & B) connected by single 1-way lanes.

dence of modules on agent index to avoid clutter. An instantiation of this template for the traffic management problem
has been presented in Section 4.
Observation Encoder: This module encodes the observation of an agent into a form suitable for the other two
modules. We denote this module by fobs and its output by
(t)
(t)
(t)
hi , i.e., hi = fobs (oi ).
Communicator: The communicator module fcomm
takes the encoded observation and a history of received
(t)
messages as input and produces a message mi ∈ {0, 1}d
to be broadcasted as output. Note that the messages are
d-dimensional binary vectors. Aside from computing the
message to be sent, fcomm also processes the messages received from the neighbors to generate a vector q̄(t) ∈ Rd
that summarizes the received messages.
Action Selector: The action selector module fact takes
(t)
the output of observation encoder hi and an encoding of
(t)
received messages q̄i as input and produces a probability
(t)
distribution over actions in Ai as output. The action ai is
then sampled from this distribution.
These three modules jointly formulate the policy πi for
agent i that takes the current observation and messages from
all neighbors as input and produces an action along with a
message to be broadcasted to the neighbors as output. The
policies are then optimized to maximize the expected rewards as described above.

4

Intelligent Traffic Management

In this section, we first cast the problem of intelligently
managing traffic controllers in the general framework presented in Section 3 and then instantiate the solution template from Section 3.1 to solve this problem. As noted
earlier, in this context, the traffic junctions correspond to
agents and these agents are connected to each other via
roads which form the underlying network.
Simulator: We used a traffic simulator known as Simulation of Urban MObility (SUMO) (Krajzewicz et al., 2012)
to simulate the traffic flow. The two road networks with
which we experiment are given in Fig. 1. All roads (except the connectors in network 2) are two lane roads. While
the smaller 10 agent network (which we call network 1) allows us to easily study aspects like grounding of emergent
language, the bigger 28 agent network (network 2) has two
distinct structural communities and it yields further insights
into the relationship between network topology and emergent communication.
Observations: Each agent observes an image representation of the traffic junction obtained by cropping a square

Learning Policies with Communication

Here we propose a generic solution template for the abstract
problem defined above. The policy of each agent is composed of three modules: observation encoder, communicator and action selector. Below we describe each of these
modules for an arbitrary agent i in detail. While each agent
has its own copy of these modules, we suppress the depen3

(t−1)

for j ∈ Ui . The aggregate message encoding
(t)
q̄i is generated as follows:
i.e., mj

(t)

(t)

qi = W hi

h
i
(t)
(t) | (t−1)
αi = softmax qi mj
: j ∈ Ui
X (t) (t−1)
(t)
q̄i =
αij mj

(2)

j∈Ui

Figure 2: Permitted actions for 4-way and 3-way junctions.
[Left-hand traffic]
Here W is a learnable parameter. This attention mechanism
is in the same spirit as the query based attention used in
(t)
patch of size 140px centered at that agent from the sim- (Das et al., 2019). The received message summaries, q̄i ,
ulation window. The observation patches of neighboring are then aggregated using a LSTM to provide a message
(t)
(t)
agents do not overlap and have a considerable amount of history. This LSTM takes q̄i as input and produces q̂i
(t)
space between them which makes the problem challenging as output. At every fifth time-step, q̂i is passed on to the
due to severe partial observability. Within the observation action selector.
patch, a queue length of at most 8 vehicles (4 per lane) can
Action Selector: fact takes the outputs of observation
be observed. We do not explicitly provide any additional in- encoder and communicator as input. These vectors are
formation like queue length on different lanes to the agents passed through separate linear layers and the results are
as a powerful enough observation encoder can in principle added to obtain a single vector of size |Ai |. Softmax is apextract such information from the raw images.
plied on this vector to obtain a probability distribution over
Actions: Fig. 2 shows the action space for agents resid- actions.
ing on 4-way and 3-way junctions. Each allowed action
Training details: In all the modules, we use ReLU acticorresponds to a particular configuration of traffic lights at vation function after all linear layers unless it is followed
the junction (also called a phase). Although the number of by other activation functions like Softmax or Gumbeltraffic light combinations is much higher, all other configu- Softmax. As actions are taken every five seconds, the rerations are either not legal, or are unsafe.
wards given in (1) are also accumulated over this period.
Rewards: A vehicle is considered to be waiting at time Thus, if an action is taken at time t, the corresponding rePt+4 (t0 )
t if it is moving with a speed < 0.1m/s. The reward for
ward for this time-step is given by t0 =t ri . We use the
agent i at time t is computed as:
mean episode reward as the baseline and use the well known


X (t)
(t)
(t)
(t)
(t)
ri = − `i + wi −
dij + ei
(1) REINFORCE trick (Williams, 1992) to compute gradients
for running the policy gradients algorithm (Sutton et al.,
j
2000). We use Adam optimizer (Kingma and Ba, 2014)
−4
(t)
where, at the given junction, ` is the number of waiting with a learning rate of 10 .
i

(t)

vehicles, wi is the sum of waiting times of all the waiting
(t)
vehicles, dij is the delay calculated as the ratio of the average speed of vehicles in lane j and the maximum permitted
(t)
speed and, ei is the number of times emergency braking
was used. In our setting, we consider a linear combination
of all rewards as they roughly have the same scale.
Observation Encoder: We model fobs as a three-layered
convolution neural network (CNN) (Lecun et al., 1998).
As it is unreasonable to change the configuration of traffic lights very frequently, the actions are only taken once
every five seconds. However, to obtain sufficient information about the environment, the observation is recorded
at each second and the CNN output is aggregated using
a Long Short-Term Memory (LSTM) network (Hochreiter
and Schmidhuber, 1997) with hidden-size 64. The output of
LSTM after five time-steps is taken as a summary of agent’s
observation.
Communicator: Agents broadcast a message every sec(t)
ond. Output message mi is generated by passing the
output of LSTM from observation encoder at time t to a
fully connected layer followed by application of GumbelSoftmax (Jang et al., 2016). To aggregate received messages we use soft-attention. Let Ui be the set of neighbors
of node i in the network. At time t, agent i receives messages broadcasted by its neighbors at the (t−1)th time-step,

5

Experiments

We experimentally establish the following claims: (i) our
approach outperforms baseline methods; (ii) agents exchange meaningful information; (iii) emergent communication is grounded in the actions taken by the agents; and
(iv) network topology affects the nature of emergent communication.
Comparison with baselines: We compare our approach
with the following baselines:
(i) Fixed-time control: The agents periodically switch between actions in a round-robin fashion after every five steps.
This is how the presently deployed traffic controllers work.
(ii) Self-Organizing Traffic Light control (SOTL):
SOTL (Cools et al., 2013) switches between actions when
the queue length at an adjoining lane exceeds a predefined
threshold (fixed to five in our implementation). This simple
heuristic improves the performance of fixed time control.
(iii) Deep-Q Learning (DQN): Agents are training independently and each agent has its own deep Q-network. This
baseline justifies the need of a multi-agent setup.
(iv) IntelliLight: IntelliLight (Wei et al., 2018) uses more
elaborate observations that includes queue length, number
of vehicles and updated waiting time at the adjoining lanes
4

No. of Blind Agents
Reward

0
-65

1
-75

2
-170

Table 1: Effect of having different number of blind agents
on average reward post convergence. Having one blind
agent does not reduce the performance but having two blind
agents does reduce it. Hence, communication is meaningful.

vector (blank message. While this may seem very similar to
the DQN baseline, we wanted to ensure that the improved
performance of our method is not because of the use of a
different training algorithm (Q-learning vs policy gradient)
but because of communication. We not only noticed a drop
in convergence speed but also observed that post convergence rewards were lower as compared to the original setting (the difference was ≈ 85, also see Fig. 3).
(ii) We define an agent to be visually impaired (or blind)
if it does not use its local observation while taking an action. Note that a visually impaired agent can still receive
messages from its neighbors. We experimented with a setting where agent 4 in Fig. 1 (network 1) was made visually impaired. We observed that, after convergence, the
rewards were same as the rewards obtained in the original setup. This indicates that the visually impaired agent
learned to receive necessary information from its neighbors
through communication. To test this hypothesis further, we
additionally made agent 5 (which is a neighbor of agent
4) visually impaired as well. As expected, since neighboring agents have been made visually impaired, they can no
longer supplement each other’s missing information using
communication and hence the performance decreased. Table 1 summarizes these results.
Grounding in communication: One reason for preferring the usage of discrete symbols for communication over
continuous vectors is because we believe that it would be
easier for humans to interpret discrete communication. This
is because, in many cases, it may be possible to establish a
relationship between words in the emergent language and
physical entities/actions. An emergent language is said to
be grounded if it satisfies this property. In many real world
applications, like the intelligent traffic management problem, being able to interpret the process used for decision
making is highly desirable. Our analysis shows that the
emergent language in the proposed setup is grounded.
To establish this, we constructed a Pointwise Mutual
Information (PMI) (Church and Hanks, 1990) matrix for
each pair of agents. The rows of this matrix correspond
to the actions of one agent (say i) and the columns correspond to the message sent by the other agent (say j). Let
d
Pij ∈ R|Ai |×2 denote the PMI matrix for the agents i and
th
j as described above. A high value of Pij
kl indicates that k
action taken by ith agent bears a strong relationship with
lth word spoken by j th agent. For each pair of agents (i, j),
we computed the Singular Value Decomposition (SVD) of
T
the PMI matrix Pij = Uij Sij Vij , where Uij ∈ R|Ai |×k ,
d
Sij ∈ Rk×k and Vij ∈ R2 ×k and k ≤ |Ai | is a hyperparameter (we use k = 2). The rows of V can be interpreted
as representations of the words in the emergent language in
the context of actions taken by the receiving agent. Under

Figure 3: Comparing our method with the baselines (Section 5). Five independent trained models were executed to
collect episode rewards averaged over agents. The mean
and standard deviation over five runs have been reported.

of a junction in addition to its image representation. As the
action space used in (Wei et al., 2018) can lead to dangerous configurations, we used the action space specified in
Section 4 in our implementation of this baseline.
(v) Fixed communication protocol: We modified the
setup presented in Section 4 to use a fixed communication
protocol. Agents do not learn to communicate as part of the
training process, but rather share all the parameters needed
to compute rewards in (1) with their neighbors directly. This
baseline shows the utility of emergent communication.
Fig. 3 compares our approach with these baselines. It
can be seen that our method outperforms all baseline approaches. Also, fixed communication is better than no communication but it is not as good as the emergent communication.
Robustness: In practice, roads are often blocked due to
random events which changes the distribution of traffic. To
study the robustness of our approach to such perturbations,
we evaluated two types of trained models on test episodes,
in which a randomly chosen road was blocked at the beginning of each episode (in network 1): (i) a model trained on a
fixed network as described in Section 4 and, (ii) a model that
was simultaneously trained on 25 perturbed variants of the
same road network. In case (ii), one of the 25 perturbations
was randomly sampled for each training episode. While (i)
resulted in a reward of ≈ −177 points, (ii) performed considerably better, yielding a reward of ≈ −55 points. So,
the proposed model can be made more robust by following
the training procedure outlined in (ii). More interestingly,
(ii) also has a regularization effect which improves the performance of the model even when network is fixed during
testing.
Utility of communication: We provide a qualitative
analysis of the communication while following the guidelines presented in (Lowe et al., 2019). While the DQN and
fixed communication protocol baselines indicate that the
emergent communication is important, to further strengthen
this argument, we performed two additional experiments:
(i) We modified the setup presented in Section 4 to mask
all communication messages in the system with an all zeros
5

Figure 4: [Best viewed in colour] Word embeddings of
neighbour j (j ∈ Ui ) corresponding to actions of agent i
for a 3-way junction (indexed by {0, 1, 2}) and for a 4-way
junction (indexed by {0, 1, 2, 3}. The clusters can be interpreted as distinct words used to mean different actions.
3-way junctions have an action space of size 3 and 4-way
junctions have an action space of size 4.

Figure 6: [Best viewed in color]. Clustering of agents
in the 10-agent network (network 1) for different runs
[Fig.(a,b)]. The numbers denote the agents. Fig.(c) on the
right represents the clustering in the 28-agent network (network 2) with A & B denoting two 14-agent sub-networks.
The position of the numbers denote the mean of the clusters.

casting to agent 4. It is also consistent with our findings
in Fig. 5, where actions embeddings corresponding to the
neighbors overlap.
Experiments on larger networks: When agents are networked together, it is reasonable to expect that they will
be influenced more by those agents with whom they are
closely associated. While we have demonstrated that this
holds for agents that are sending messages to a common receiver (Fig. 6), more broadly, one can understand the role
played by communities in shaping the emergent communication.
To do so, we took a pair of networks, each having 14
agents and connect them by two single one-way lanea (28
agent network in Fig. 1). As before, we plotted the t-SNE
embeddings of the rows of tf-idf matrix (Fig. 6 (c)) and observed that two clusters (denoted by A and B), corresponding to the two structural communities in the network are
discovered. There is an overlapping region which we argue represents the common vocabulary used by agents from
both communities.

Figure 5: [Best viewed in color]. Action embeddings from
matrix U (orthonormal matrix) of agent 0 corresponding to
messages from all agents. As highlighted by the red circles, the action embeddings of agent 0 are overlapped for
neighbors (1, 3). The color bar represents different agents.

this representation, words will be similar if they lead to similar actions being taken by the receiving agent.
Figure 4 shows the t-SNE (van der Maaten and Hinton,
2008) plot of the rows of Vij matrix. Colors have been assigned to the points based on the action with which the word
has the highest PMI. It can be seen that distinct clusters
form, each corresponding to a different action. This signifies that neighboring agents use specific set of words to indicate actions, thus showing that the language is grounded.
Additionally, we plot the rows of Uij matrix (with k =
2), i.e., the action embeddings corresponding to the broadcasted messages. We set i = 0 and pair agent 0 with all
agents j = 0, 1, . . . , 9 to obtain different action embeddings for agent 0 using U0j . In Fig. 5, we plot these action
embeddings; different colors have been used for different
agents j = 0, 1, 2, . . . , 9. It can be observed that the points
corresponding to neighbors j = 1, 3 tend to be very close
to each other (highlighted using red circles in Fig. 5). This
implies that neighbors of agent 0 are consistent in their use
of messages for referring to actions.
Effect of network topology: We obtain a tf-idf matrix
where rows correspond to agents and columns correspond
to the words in the vocabulary. On plotting a t-SNE plot of
the rows of this matrix for agents in the ten node network
(Fig. 1), we noticed that agents that broadcast to the same
neighbor tend to be clustered together. For instance, from
Fig. 6 [(a), (b)], one can infer that the following groups are
formed: (i) agents 0 & 4 broadcasting to agent 1; (ii) agents
3 & 7 broadcasting to agent 4; (iii) agents 3 & 5 broad-

6

Conclusion

In this paper, we formulated a networked multi-agent reinforcement learning problem where cooperative agents communicate with each other using an emergent language. The
problem of intelligent traffic management was cast in the
general problem framework and empirical evaluations were
made to: (i) demonstrate the utility of emergent communication in optimizing traffic flow; (ii) understand the properties of emergent language; and (iii) show the relationship
between emergent communication and the underlying network topology. It would be interesting to extend this framework to dynamic graphs and we leave this for future work.

References
Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo,
Karl Tuyls, and Stephen Clark. 2018. Emergent Communication through Negotiation. In 6th International Conference on Learning Representations (ICLR).
Kenneth Ward Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, and Lexicography.
Comput. Linguist. 16, 1 (1990), 22–29.
6

Seung-Bae Cools, Carlos Gershenson, and Bart D’Hooghe.
Alan J. Miller. 1963. Settings for Fixed-Cycle Traffic Sig2013. Self-Organizing Traffic Lights: A Realistic Simunals. Journal of the Operational Research Society 14, 4
lation. Springer London, 45–55.
(1963), 373–386.
Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv
Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Igor Mordatch and Pieter Abbeel. 2018. Emergence of
2019. TarMAC: Targeted Multi-Agent Communication.
Grounded Compositional Language in Multi-Agent PopProceedings of the 36th International Conference on Maulations. AAAI Conference on Artificial Intelligence
chine Learning, PMLR 97 (2019), 1538–1546.
(2018).
Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and
Prabuchandran K.J., Hemanth Kumar A.N, and S. BhatnaShimon Whiteson. 2016. Learning to Communicate with
gar. 2014. Multi-agent reinforcement learning for traffic
Deep Multi-Agent Reinforcement Learning. In Proceedsignal control. 17th International IEEE Conference on
ings of the 30th International Conference on Neural InIntelligent Transportation Systems (ITSC) (2014), 2529–
formation Processing Systems (NIPS’16). 2145–2153.
2534.
Shubham Gupta and Ambedkar Dukkipati. 2019. On Voting Strategies and Emergent Communication. CoRR Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.
2016. Learning Multiagent Communication with Backabs/1902.06897 (2019).
propagation. In Advances in Neural Information ProcessSerhii Havrylov and Ivan Titov. 2017. Emergence of Laning Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg,
guage with Multi-agent Games: Learning to CommuniI. Guyon, and R. Garnett (Eds.). 2244–2252.
cate with Sequences of Symbols. In Advances in Neural
Information Processing Systems 30. 2149–2159.
Richard S. Sutton, David McAllester, Satinder Singh, and
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput. 9, 8 (1997), 1735–
1780.

Yishay Mansour. 2000. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In
Advances in Neural Information Processing Systems 13.
1057–1063.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical
Reparameterization with Gumbel-Softmax. (2016).
M. Tan. 1993. Multi-agent Reinforcement Learning - Independent vs Cooperative Agents. In Proceedings of
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method
the tenth International Conference on Machine Learning
for Stochastic Optimization. Proceedings of 3rd Interna(ICML). 330–337.
tional Conference on Learning Representations (ICLR)
(2014).
Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using t-SNE. Journal of Machine LearnDaniel Krajzewicz, Jakob Erdmann, Michael Behrisch, and
ing Research 9 (2008), 2579–2605. http://www.jmlr.org/
Laura Bieker-Walz. 2012. Recent Development and Appapers/v9/vandermaaten08a.html
plications of SUMO - Simulation of Urban MObility. International Journal On Advances in Systems and MeaF. V. Webster. 1958. Traffic signal settings. Paper No. 39,
surements 5, 3&4 (2012), 128–138.
Road Research Laboratory, England.
Yann Lecun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to docu- Hua Wei, Guanjie Zheng, Huaxiu Yao, and Zhenhui Li.
ment recognition. Proc. IEEE 86, 11 (1998), 2278–2324.
2018. IntelliLight: A Reinforcement Learning Approach
L. Li, Y. Lv, and F. Wang. 2016. Traffic signal timing via
deep reinforcement learning. IEEE/CAA Journal of Automatica Sinica 3, 3 (2016), 247–254.

for Intelligent Traffic Light Control. In Proceedings of
the 24th ACM SIGKDD International Conference on
Knowledge Discovery; Data Mining (KDD ’18). 2496–
2505.

Michael L. Littman. 1994. Markov Games As a Framework
for Multi-agent Reinforcement Learning. In Proceedings Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and
of the Eleventh International Conference on Machine
Wei Pan. 2019. Probabilistic Recursive Reasoning for
Learning. 157–163.
Multi-Agent Reinforcement Learning. In 7th International Conference on Learning Representations (ICLR).
Ryan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau,
and Yann Dauphin. 2019. On the Pitfalls of Measuring
Ronald J Williams. 1992. Simple Statistical Gradient
Emergent Communication. In Proceedings of the 18th InFollowing Algorithms for Connectionist Reinforcement
ternational Conference on Autonomous Agents and MulLearning. Machine Learning 8, 3-4 (1992), 229–256.
tiAgent Systems. 693–701.
Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and
Pieter Abbeel, and Igor Mordatch. 2017. Multi-Agent
Tamer Basar. 2018. Fully Decentralized Multi-Agent ReActor-Critic for Mixed Cooperative-Competitive Enviinforcement Learning with Networked Agents. Proceedronments. In Advances in Neural Information Processing
ings of the 35th International Conference on Machine
Systems 30. 6379–6390.
Learning, PMLR 80 (2018), 5872–5881.
7

