When is Mean-Field Reinforcement Learning Tractable and
Relevant?
Batuhan Yardim

Artur Goldman

Niao He

ETH ZÃ¼rich
ZÃ¼rich, Switzerland
yardima@ethz.ch

HSE University
Moscow, Russia
agoldman@hse.ru

ETH ZÃ¼rich
ZÃ¼rich, Switzerland
niao.he@inf.ethz.ch

arXiv:2402.05757v1 [cs.GT] 8 Feb 2024

ABSTRACT
Mean-field reinforcement learning has become a popular theoretical
framework for efficiently approximating large-scale multi-agent
reinforcement learning (MARL) problems exhibiting symmetry.
However, questions remain regarding the applicability of meanfield approximations: in particular, their approximation accuracy
of real-world systems and conditions under which they become
computationally tractable. We establish explicit finite-agent bounds
for how well the MFG solution approximates the true ğ‘ -player
game for two popular mean-field solution concepts. Furthermore,
for the first time, we establish explicit lower bounds indicating that
MFGs are poor or uninformative at approximating ğ‘ -player games
assuming only Lipschitz dynamics and rewards. Finally, we analyze
the computational complexity of solving MFGs with only Lipschitz
properties and prove that they are in the class of PPAD-complete
problems conjectured to be intractable, similar to general sum ğ‘
player games. Our theoretical results underscore the limitations
of MFGs and complement and justify existing work by proving
difficulty in the absence of common theoretical assumptions.

in for instance auctions [17], and cloud resource management [21].
For the mean-field analysis, the game dynamics with ğ‘ -players
must be symmetric (i.e., each player must be exposed to the same
rules) and anonymous (i.e., the effect of each player on the others
should be permutation invariant). Under this simplification, works
such as [1, 6, 12, 25, 27, 35, 36] and many others have analyzed
reinforcement learning (RL) algorithms in the MFG limit ğ‘ â†’ âˆ
to obtain a tractable approximation of many agent games, providing
learning guarantees under various structural assumptions.
Being a simplification, MFG formulations should ideally satisfy
two desiderata: (1) they should be relevant, i.e., they are good approximations of the original MARL problem and (2) they should be
tractable, i.e., they are at least easier than solving the original MARL
problem. In this work, we would like to understand the extent to
which MFGs satisfy these two requirements, and we aim to answer
two natural questions that remain understudied:
â€¢ When are MFGs good approximations of the finite player
games, when are they not? In particular, are polynomially
many agents always sufficient for mean-field approximation
to be effective?
â€¢ Is solving MFGs always computationally tractable, or more
tractable than directly solving the ğ‘ -player game? In particular, can MFGs be solved in polynomial or pseudo-polynomial
time?

KEYWORDS
Mean-Field Games; Computational Complexity; Approximation
ACM Reference Format:
Batuhan Yardim, Artur Goldman, and Niao He. 2024. When is Mean-Field
Reinforcement Learning Tractable and Relevant?. In Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS
2024), Auckland, New Zealand, May 6 â€“ 10, 2024, IFAAMAS, 26 pages.

1

INTRODUCTION

Multi-agent reinforcement learning (MARL) finds numerous impactful applications in the real world [21, 22, 28, 31, 32, 34]. Despite
the urgent need in practice, MARL remains a fundamental challenge, especially in the setting with large numbers of agents due to
the so-called â€œcurse of many agentsâ€ [33].
Mean-field games (MFG), a theoretical framework first proposed
by Lasry and Lions [19] and Huang et al. [16], permits the theoretical study of such large-scale games by introducing mean-field
simplification. Under certain assumptions, the mean-field approximation leads to efficient algorithms for the analysis of a particular
type of ğ‘ -agent competitive game where there are symmetries
between players and when ğ‘ is large. Such games appear widely
This work is licensed under a Creative Commons Attribution
International 4.0 License.
Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2024), N. Alechina, V. Dignum, M. Dastani, J.S. Sichman (eds.), May 6 â€“ 10, 2024,
Auckland, New Zealand. Â© 2024 International Foundation for Autonomous Agents and
Multiagent Systems (www.ifaamas.org).

1.1

Related Work

Mean-field RL has been studied in various mathematical settings.
In this work, we focus on two popular formulations in particular:
stationary mean-field games (Stat-MFG, see e.g. [1, 12]) and finitehorizon MFG (FH-MFG, see e.g. [25, 27]). In the Stat-MFG setting
the objective is to find a stationary policy that is optimal with
respect to its induced stationary distribution, while in the FH-MFG
setting, a finite-horizon reward is considered with a time-varying
policy and population distribution.
Existing results on MFG relevance/approximation. The approximation properties of MFGs have been explored by several
works in literature, as summarized in Table 1. Finite-agent approximation bounds have been widely analyzed in the case of stochastic
mean-field differential games [3, 4], albeit in the differential setting
and without explicit lower bounds. Recent works [1, 6] have established that Stat-MFG Nash equilibria (Stat-MFG-NE) asymptotically
approximate the NE of ğ‘ -player symmetric dynamic games under
continuity assumptions. The result by Saldi et al. [30], as the basis
of subsequent proofs, shows asymptotic convergence for a large
class of MFG variants and only requires continuity of dynamics
and rewards as well as minor technical assumptions such as compactness and a form of local Lipschitz continuity. However, such

asymptotic convergence guarantees leave the question unanswered
if the MFG models are realistic in real-world games. Many games
such as traffic systems, financial markets, etc. naturally exhibit large
ğ‘ , however, if ğ‘ must be astronomically large for good approximation, the real-world impact of the mean-field analysis will be
limited. Recently, [37] provided finite-agent approximation bounds
of a special class of stateless MFG, which assumes no state dynamics. We complement existing work on approximation properties of
both Stat-MFG and FH-MFG by providing explicit upper and lower
bounds for approximation.
Existing results on MFG tractability. The tractability of solving MFGs as a proxy for MARL has been also heavily studied in
the RL community under various classes of structural assumptions.
Since finding approximate Nash equilibria for normal form games is
PPAD-complete, a class believed to be computationally intractable
[5, 7], solving the mean-field approximation in many cases can be a
tractable alternative. We summarize recent work for computationally (or statistically) solving the two types of MFGs below, with an
in-depth comparison also provided in Table 2.
For Stat-MFG, under a contraction assumption RL algorithms
such as Q-learning [1, 38], policy mirror ascent [36], policy gradient methods [13], soft Q-learning [6] and fictitious play [35] have
been shown to solve Stat-MFG with statistical and computational
efficiency. However, all of these guarantees require the game to
be heavily regularized as pointed out in [6, 36], inducing a nonvanishing bias on the computed Nash. Moreover, in some works the
population evolution is also implicitly required to be contractive
under all policies (see e.g. [12, 36]), further restricting the analysis
to sufficiently smooth games. While [14] has proposed a method
that guarantees convergence to MFG-NE under differentiable dynamics, the algorithm converges only when initialized sufficiently
close to the solution. To the best of our knowledge, there are neither
RL algorithms that work without regularization nor evidence of difficulty in the absence of such strong assumptions: we complement
the line of work by showing that unless dynamics are sufficiently
smooth, Stat-MFG is both computationally intractable and a poor
approximation.
A separate line of work analyzes the finite horizon problem. In
this case, when the dynamics are population-independent and the
payoffs are monotone the problem is known to be tractable. Algorithms such as fictitious play [27] and mirror descent [25] have
been shown to converge to Nash in corresponding continuoustime equations. Recent work has also focused on the statistical
complexity of the finite-horizon problem in very general FH-MFG
problems [15], however, the algorithm proposed is in general computationally intractable. In terms of computational tractability and
the approximation properties, our work complements these results
by demonstrating that (1) when dynamics depend on the population
as well an exponential approximation lower bound exists, and (2)
in the absence of monotonicity, the FH-MFG is provably as difficult
as solving an ğ‘ -player game.
Finally, we note that there are several other settings and MFG
solution concepts have been analyzed. For instance, a certain class
of infinite horizon MFG has been shown to be equivalent to concave
utility RL, proving finite-time computational guarantees [10].

1.2

Our Contribution

In this work, we formalize and provide answers to the two aforementioned fundamental questions, first focusing on the approximation
properties of MFG in Section 3 and later on the computational
tractability of MFG in Section 4. Our contributions are summarized
as follows.
Firstly, we introduce explicit finite-agent approximation bounds
for finite horizon and stationary MFGs (Table 1) in terms of exploitability in the finite agent game. In both cases, we prove explicit
upper bounds which quantify how many agents a symmetric game
must have to be well-approximated by the MFG, which has been
absent in the literature to the best of our knowledge. Our approximation results only require a minimal Lipschitz continuity assumption
of the transition
 kernel and rewards. For FH-MFG, we prove a
O

(1âˆ’ğ¿ğ» )ğ» 2
âˆš
(1âˆ’ğ¿) ğ‘

upper bound for the exploitabilty where ğ¿ is the

Lipschitz modulus of the population evolution operator: the upper
bound exhibits an exponential dependence
 on the horizon ğ» . For

(1âˆ’ğ›¾ ) âˆ’3

âˆš
the Stat-MFG we show that a O
approximation bound
ğ‘
can be established, but only if the population evolution dynamics
are non-expansive. Next, for the first time, we establish explicit
lower bounds for the approximation proving the shortcomings of
the upper bounds are fundamental. For the FH-MFG, we show that
unless ğ‘ â‰¥ Î©(2ğ» ), an exploitability linear in horizon ğ» is unavoidable when deploying the MFG solution to the ğ‘ player game:
hence in general the MFG equilibrium becomes irrelevant quickly
as the problem horizon increases. For Stat-MFG we establish an
Î©(ğ‘ log2 ğ›¾ ) lower bound when the population dynamics are not
restricted to non-expansive population operators, showing that a
large discount factor ğ›¾ also rapidly deteriorates the approximation
efficiency. Our lower bounds indicate that in the worst case, the
number of agents required for the approximation can grow exponentially in the problem parameters, demonstrating the limitations
of the MFG approximation.
Finally, from the computational perspective, we establish that
both finite-horizon and stationary MFGs can be PPAD-complete
problems in general, even when restricted to certain simple subclasses (Table 2). This shows that both MFG problems are in general
as hard as finding a Nash equilibrium of ğ‘ -player general sum
games. Furthermore, our results imply that unless PPAD=P there
are no polynomial time algorithms for solving FH-MFG and StatMFG, a result indicating computational intractability.

2

MEAN-FIELD GAMES: DEFINITIONS,
SOLUTION CONCEPTS

Notation. Throughout this work, we assume S, A are finite sets.
For a finite set X, Î” X denotes the set of probability distributions
on X. The norm used will not fundamentally matter for our results,
we choose to equip Î” S , Î” A with the norm âˆ¥ Â· âˆ¥ 1 . We define the
ğ» âˆ’1 :
set of Markov policies Î  := {ğœ‹ : S â†’ Î” A }, Î ğ» := {{ğœ‹â„ }â„=0
ğ‘ := {{ğœ‹ ğ‘– }ğ» âˆ’1,ğ‘ : ğœ‹ ğ‘– âˆˆ Î , âˆ€â„}. For policies
ğœ‹â„ âˆˆ Î , âˆ€â„} and Î ğ»
â„ â„=0,ğ‘–=0
â„
ğœ‹, ğœ‹ â€² âˆˆ Î  denote âˆ¥ğœ‹ âˆ’ğœ‹ â€² âˆ¥ 1 = supğ‘  âˆˆ S âˆ¥ğœ‹ (Â·|ğ‘ ) âˆ’ğœ‹ â€² (Â·|ğ‘ )âˆ¥ 1 . We denote
ğ‘‘ (ğ‘¥, ğ‘¦) := 1 {ğ‘¥â‰ ğ‘¦ } for ğ‘¥, ğ‘¦ in A or S. For ğœ‹ âˆˆ Î  ğ‘ , ğœ‹ â€² âˆˆ Î , we define
(ğœ‹ â€², ğœ‹ âˆ’ğ‘– ) âˆˆ Î  ğ‘ as the policy profile where the ğ‘–-th policy has been
ğ‘ , ğœ‹ â€² âˆˆ Î  , we denote by
replaced by ğœ‹ â€² . Likewise, for ğœ‹ âˆˆ Î ğ»
ğ»

Work

MFG type

Key Assumptions

Approximation Rate (in Exploitability)

Carmona and Delarue, 2013
Saldi et al., 2018
Anahtarci et al., 2022
Cui and Koeppl, 2021
Yardim et al., 2023a

Othera
Stat-MFG
Stat-MFG
Otherc

Affine drift, Lipschitz derivatives
Continuity
Lipschitz ğ‘ƒ, ğ‘… + Regularized + Contractive Î“ğ‘ƒ
Continuity
Lipschitz ğ‘ƒ, ğ‘…

Theorem 3.2

FH-MFG

Lipschitz ğ‘ƒ, ğ‘…

O (ğ‘ âˆ’1/(ğ‘‘+4) ) (ğ‘‘ dimension of state space)
ğ‘œ (1) (asymptotic: convergence as ğ‘ â†’ âˆ)
ğ‘œ (1) (asymptotic: convergence as ğ‘ â†’ âˆ)
ğ‘œ (1) (asymptotic: convergence as ğ‘ â†’ âˆ)
âˆš
O ( 1/ ğ‘ )


ğ» 2 (1âˆ’ğ¿ğ» )
âˆš
O
, ğ¿ Lipschitz modulus of Î“ğ‘ƒ

Theorem 3.3
Theorem 3.5
Theorem 3.6

FH-MFG
Stat-MFG
Stat-MFG

Lipschitz ğ‘ƒ, ğ‘…
Lipschitz ğ‘ƒ, ğ‘… + Non-expansive Î“ğ‘ƒ
Lipschitz ğ‘ƒ, ğ‘…

Î©(ğ» ) unless ğ‘ â‰¥ Î©(2ğ» )
âˆš
O ( (1 âˆ’ ğ›¾ ) âˆ’3/ ğ‘ )
âˆ’1
Î©(ğ‘ âˆ’ log2 ğ›¾ ))

Otherb

(1âˆ’ğ¿) ğ‘

Table 1: Selected approximation results for MFG. Notes: a stochastic differential MFG, b infinite-horizon discounted setting
with non-stationary policies, c stateless/static MFG setting.

Work

MFG Type

Key Assumptions

Iteration/Sample Complexity result

Anahtarci et al., 2022
Geist et al., 2022
Perrin et al., 2020
PÃ©rolat et al., 2022
Zaman et al., 2023
Cui and Koeppl, 2021
Yardim et al., 2023a
Yardim et al., 2023b

Stat-MFG
Othera
FH-MFG
FH-MFG
Stat-MFG
Stat-MFG
Otherb
Stat-MFG

Lipschitz ğ‘ƒ, ğ‘… + Regularization + Contractive Î“ğ‘ƒ
Concave potential
Monotone ğ‘…, ğœ‡-independent ğ‘ƒ
Monotone ğ‘…, ğœ‡-independent ğ‘ƒ
Lipschitz ğ‘ƒ, ğ‘… + Regularization + Contractive Î“ğ‘ƒ
Lipschitz ğ‘ƒ, ğ‘… + Regularization
Monotone and Lipschitz ğ‘…
Lipschitz ğ‘ƒ, ğ‘… + Regularization + Contractive Î“ğ‘ƒ

e(ğœ€ âˆ’4| A | ) samples, O (log ğœ€ âˆ’1 ) iterations
O
O (ğœ€ âˆ’2 ) iterations
O (ğœ€ âˆ’1 ) (continuous time analysis)
O (ğœ€ âˆ’1 ) (continuous time analysis)
O (ğœ€ âˆ’4 ) samples
O (log ğœ€ âˆ’1 ) iterations
O (ğœ€ âˆ’2 ) samples (ğ‘ -player)
O (ğœ€ âˆ’2 ) samples (ğ‘ -player)

Theorem 4.9
Theorem 4.12
Theorem 4.14

Stat-MFG
FH-MFG
FH-MFG

Lipschitz ğ‘ƒ, ğ‘…
Lipschitz ğ‘ƒ, ğ‘… + ğœ‡-independent ğ‘ƒ
Linear ğ‘ƒ, ğ‘… + ğœ‡-independent ğ‘ƒ

PPAD-complete
PPAD-complete
PPAD-complete

Table 2: Selected results for computing MFG-NE from literature. In the assumptions column, contractive Î“ğ‘ƒ indicates that for
all ğœ‹ âˆˆ Î , Î“ğ‘ƒ (Â·, ğœ‹) is a contraction, and regularization indicates that a non-vanishing bias is present. Notes: a infinite-horizon,
population dependence through the discounted state distribution. b stateless/static MFG.

ğ‘ the policy profile where the ğ‘–-th playerâ€™s policy has
(ğœ‹ â€², ğœ‹ âˆ’ğ‘– ) âˆˆ Î ğ»
been replaced by ğœ‹ â€² . For any ğ‘ âˆˆ N â‰¥0 , [ğ‘ ] := {1, . . . , ğ‘ }.
MFGs introduce a dependence on the population distribution
over states of the rewards and dynamics. We will strictly consider
Lipschitz continuous rewards and dynamics, which is a common
assumption in literature [1, 12, 35, 36], formalized below.

Definition 2.1 (Lipschitz dynamics, rewards). For some ğ¿ â‰¥ 0, we
define the set of ğ¿-Lipschitz reward functions and state transition
dynamics as
n
Rğ¿ := ğ‘… : S Ã— A Ã— Î” S â†’[0, 1] : |ğ‘…(ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘…(ğ‘ , ğ‘, ğœ‡ â€² )|
o
â‰¤ ğ¿âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1, âˆ€ğ‘ , ğ‘, ğœ‡, ğœ‡ â€² ,
n
Pğ¿ := ğ‘ƒ : S Ã— A Ã— Î” S â†’Î” S : âˆ¥ğ‘ƒ (ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘ƒ (ğ‘ , ğ‘, ğœ‡ â€² )âˆ¥ 1
o
â‰¤ ğ¿âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1, âˆ€ğ‘ , ğ‘, ğœ‡, ğœ‡ â€² .
Moreover, we define the set of Lipschitz rewards and dynamics as
Ã
Ã
R := ğ¿â‰¥0 Rğ¿ , P := ğ¿â‰¥0 Pğ¿ respectively.

We note that there are interesting MFGs with non-Lipschitz dynamics and rewards, however, even the existence of Nash is not
guaranteed in this case. Lipschitz continuity is a minimal assumption under which solutions to MFG always exist, and as our aim is
to prove lower bounds and difficulty we will adopt this assumption.
Solving MFG with non-Lipschitz dynamics is more challenging
than Lipschitz continuous MFG (the latter being a subset of the
former), hence our difficulty results will apply.
Operators. We will define the useful population operators Î“ğ‘ƒ :
ğ»
Î” S Ã— Î  â†’ Î” S , Î“ğ‘ƒğ» : Î” S Ã— Î  â†’ Î” S , and Î›ğ»
ğ‘ƒ : Î” S Ã— Î ğ» â†’ Î” S as
âˆ‘ï¸
Î“ğ‘ƒ (ğœ‡, ğœ‹) :=
ğœ‡ (ğ‘ )ğœ‹ (ğ‘|ğ‘ )ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡),
ğ‘  âˆˆ S,ğ‘âˆˆ A
ğ»
Î“ğ‘ƒ (ğœ‡, ğœ‹) := Î“ğ‘ƒ (. . . Î“ğ‘ƒ (Î“ğ‘ƒ (ğœ‡, ğœ‹), ğœ‹) . . . ), ğœ‹),

|

{z

ğ» times

}


ğ» âˆ’1
Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ) := Î“ğ‘ƒ (. . . Î“ğ‘ƒ (Î“ğ‘ƒ (ğœ‡ 0 , ğœ‹ 0 ), ğœ‹ 1 ) . . . , ğœ‹â„âˆ’1 ) â„=0
|
{z
}
â„ times

ğ» âˆ’1 âˆˆ Î  , ğ‘ƒ âˆˆ P, ğœ‡ âˆˆ Î” .
for all ğ‘› âˆˆ N>0, ğœ‹ âˆˆ Î , ğœ‹ = {ğœ‹â„ }â„=0
0
ğ»
S

Finally, we will need the following Lipschitz continuity result
for the Î“ğ‘ƒ operator.
Lemma 2.2. [36, Lemma 3.2] Let ğ‘ƒ âˆˆ Pğ¾ğœ‡ for ğ¾ğœ‡ > 0 and
ğ¾ğ‘  := sup ğ‘ƒ (ğ‘ , ğ‘, ğœ‡ ) âˆ’ ğ‘ƒ (ğ‘  â€² , ğ‘, ğœ‡ ) 1 , ğ¾ğ‘ := sup ğ‘ƒ (ğ‘ , ğ‘, ğœ‡ ) âˆ’ ğ‘ƒ (ğ‘ , ğ‘ â€² , ğœ‡ ) 1 .
ğ‘ ,ğ‘  â€²
ğ‘,ğœ‡

ğ‘,ğ‘ â€²
ğ‘ ,ğœ‡

Then it holds for all ğœ‡, ğœ‡ â€² âˆˆ Î” S , ğœ‹, ğœ‹ â€² âˆˆ Î  that:
âˆ¥Î“ğ‘ƒ (ğœ‡, ğœ‹) âˆ’ Î“ğ‘ƒ (ğœ‡ â€², ğœ‹ â€² )âˆ¥ 1 â‰¤ ğ¿ğ‘ğ‘œğ‘,ğœ‡ âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 +

ğ¾ğ‘
âˆ¥ğœ‹ âˆ’ ğœ‹ â€² âˆ¥ 1,
2

where ğ¿ğ‘ğ‘œğ‘,ğœ‡ := (ğ¾ğœ‡ + ğ¾2ğ‘  + ğ¾2ğ‘ ) for all ğœ‹, ğœ‹ â€² âˆˆ Î , ğœ‡, ğœ‡ â€² âˆˆ Î” S .
In particular, in our settings, Lemma 2.2 indicates that Î“ğ‘ƒ is
always Lipschitz continuous if ğ‘ƒ âˆˆ P, a property which will become
significant for approximation analysis.
We will be interested in two classes of MFG solution concepts
that lead to different analyses: infinite horizon stationary MFG
Nash equilibrium (Stat-MFG-NE) and finite horizon MFG Nash
equilibrium (FH-MFG-NE). The first problem widely studied in literature is the stationary MFG equilibrium problem, see for instance
[1, 12, 13, 35, 36]. We formalize this solution concept below.
Definition 2.3 (Stat-MFG). A stationary MFG (Stat-MFG) is defined by the tuple (S, A, ğ‘ƒ, ğ‘…, ğ›¾) for Lipschitz dynamics and rewards
ğ‘ƒ âˆˆ P, ğ‘… âˆˆ R, discount factor ğ›¾ âˆˆ (0, 1). For any (ğœ‡, ğœ‹) âˆˆ Î” S Ã— Î ,
we define the ğ›¾-discounted infinite horizon expected reward as
"âˆ
#
âˆ‘ï¸
ğ›¾
ğ‘  0 âˆ¼ğœ‡, ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ )
ğ‘¡
ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡, ğœ‹) := E
ğ›¾ ğ‘…(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğœ‡) ğ‘  âˆ¼ğ‘ƒ (ğ‘  ,ğ‘ ,ğœ‡ ) .
ğ‘¡ +1

ğ‘¡

3.1

Approximation Analysis of FH-MFG

Firstly, we define the finite-player game that is approximately solved
by the FH-MFG-NE.
Definition 3.1 (ğ‘ -FH-SAG). An ğ‘ -player finite horizon SAG
(ğ‘ -FH-SAG) is determined by the tuple (ğ‘ , S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 ) such
that ğ‘ âˆˆ Z>0, ğ» âˆˆ Z>0 , ğ‘ƒ âˆˆ P, ğ‘… âˆˆ R, ğœ‡0 âˆˆ Î” S . For any ğœ‹ =
ğ‘ , we define the expected mean reward
{ğœ‹â„ğ‘– }â„=0,...,ğ» âˆ’1,ğ‘– âˆˆ [ğ‘ ] âˆˆ Î ğ»
and exploitability of player ğ‘– as
"ğ» âˆ’1
#
ğ‘—
ğ‘—
ğ‘— ğ‘—
âˆ‘ï¸
âˆ€ ğ‘—:ğ‘  0 âˆ¼ğœ‡ 0 , ğ‘â„ âˆ¼ğœ‹â„ (ğ‘ â„ )
ğ»,ğ‘ ,(ğ‘– )
Ã
(ğœ‹ ) := E
ğ½ğ‘ƒ,ğ‘…
ğ‘…(ğ‘ â„ğ‘– , ğ‘â„ğ‘– , b
ğœ‡â„ ) ğ‘  ğ‘— âˆ¼ğ‘ƒ (ğ‘  ğ‘— ,ğ‘ ğ‘— ,b
ğœ‡ ),b
ğœ‡ := 1
e ğ‘— ,
â„+1

ğœ‡ âˆ— = Î“ğ‘ƒ (ğœ‡ âˆ—, ğœ‹ âˆ— ),
(Stat-MFG-NE)

The second MFG concept that we will consider has a finite time
horizon, and is also common in literature [15, 20, 26, 27]. In this
case, the population distribution is permitted to vary over time,
and the objective is to find an optimal non-stationary policy with
respect to the population distribution it induces. We formalize this
problem and the corresponding solution concept below.
Definition 2.4 (FH-MFG). A finite horizon MFG problem (FHMFG) is determined by the tuple (S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 ) where ğ» âˆˆ Z>0 ,
ğ» âˆˆ Î  , ğ = {ğœ‡ }ğ» âˆ’1 âˆˆ
ğ‘ƒ âˆˆ P, ğ‘… âˆˆ R, ğœ‡0 âˆˆ Î” S . For ğœ‹ = {ğœ‹â„ }â„=0
ğ»
â„ â„=0
ğ»
Î” S , define the expected reward and exploitability as
#
"ğ» âˆ’1
âˆ‘ï¸
ğ‘  0 âˆ¼ğœ‡ 0 , ğ‘â„ âˆ¼ğœ‹â„ (ğ‘ â„ )
ğ»
ğ‘‰ğ‘ƒ,ğ‘… (ğ, ğœ‹ ) := E
ğ‘…(ğ‘ â„ , ğ‘â„ , ğœ‡â„ ) ğ‘  âˆ¼ğ‘ƒ (ğ‘  ,ğ‘ ,ğœ‡ ) ,
â„

â„

â„

â„=0
ğ»
ğ»
â€²
ğ»
ğ»
(ğœ‹ ) := max ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
Eğ‘ƒ,ğ‘…
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (Î›ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ ).
ğœ‹ â€² âˆˆÎ ğ»

Then, the FH-MFG Nash equilibrium is defined as:
ğ» âˆ’1
Policy ğœ‹ âˆ— = {ğœ‹â„âˆ— }â„=0
âˆˆ Î ğ» such that
ğ»
ğ» âˆ’1
({ğœ‹â„âˆ— }â„=0
) = 0.
Eğ‘ƒ,ğ‘…

â„

â„

â„

Eğ‘ƒ,ğ‘…

ğ‘

ğ‘— ğ‘ 
â„

ğ»,ğ‘ ,(ğ‘– ) â€² âˆ’ğ‘–
ğ»,ğ‘ ,(ğ‘– )
(ğœ‹ ) := max ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ ).
ğœ‹ â€² âˆˆÎ ğ»

Then, the ğ‘ -FH-SAG Nash equilibrium is defined as:
(ğ‘– ),âˆ— ğ» âˆ’1
ğ‘
}â„=0 âˆˆ Î ğ»
such that
ğ»,ğ‘ ,(ğ‘– )
ğ» âˆ’1
âˆ€ğ‘– : Eğ‘ƒ,ğ‘…
({ğœ‹â„âˆ— }â„=0
) = 0.
(ğ‘ -FH-SAG-NE)

ğ‘ -tuple of policies {ğœ‹â„

ğ›¾
ğ›¾
ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹ âˆ— ) = max ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹).
ğœ‹ âˆˆÎ 

â„+1

â„

â„=0
ğ»,ğ‘ ,(ğ‘– )

A policy-population pair (ğœ‡ âˆ—, ğœ‹ âˆ— ) âˆˆ Î” S Ã— Î  is called a Stat-MFG
Nash equilibrium if the two conditions hold:

Optimality:

APPROXIMATION PROPERTIES OF MFG

ğ‘¡

ğ‘¡ =0

Stability:

3

As established in literature, the reason the FH-MFG and Stat-MFG
problems are studied is the fact that they can approximate the NE
of certain symmetric games with ğ‘ players, establishing the main
relevance of the formulations in the real world. Such results are
summarized in Table 1.
In this section, we study how efficient this convergence is and
also related lower bounds. For these purposes, we first define the
corresponding finite-player game of each mean-field game problem: to avoid confusion, we call these games symmetric anonymous
dynamic games (SAG). Afterwards, for each solution concept, we
will first establish (1) an upper bound on the approximation error
(i.e. the exploitability) due to the mean-field, and (2) a lower bound
demonstrating the worst-case rate. We will present the main outlines of proofs, and postpone computation-intensive derivations to
the supplementary material of the paper.

ğ»,ğ‘ ,(ğ‘– )

If instead Eğ‘ƒ,ğ‘…
(ğœ‹ ) â‰¤ ğ›¿ for all ğ‘–, then ğœ‹ is called a ğ›¿-ğ‘ -FH-SAG
Nash equilibrium.
The above definition corresponds to a real-world problem as the
ğ»,ğ‘ ,(ğ‘– )
function ğ½ğ‘ƒ,ğ‘…
expresses the expected total payoff of each player:
hence a ğ›¿-ğ‘ -MFG-NE is a Nash equilibrium of a concrete ğ‘ -player
game in the traditional game theoretical sense. Also, note that now
in the definition transition probabilities and rewards depend on
b
ğœ‡â„ which is the F ({ğ‘ â„ğ‘– }ğ‘– ) = Fâ„ -measurable random vector of the
empirical state distribution at time â„ of all agents.
Firstly, we provide a positive result well-known in literature:
the ğ‘ -FH-SAG is approximately solved by the FH-MFG-NE policy.
Unlike some past works, we establish an explicit rate of convergence
in terms of ğ‘ and problem parameters.
Theorem 3.2 (Approximation of ğ‘ -FH-SAG). Let (S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 )
be a FH-MFG with ğ‘ƒ âˆˆ P, ğ‘… âˆˆ R and with a FH-MFG-NE ğœ‹ âˆ— âˆˆ Î ğ» ,
ğ‘ . Let ğ¿ > 0 be the
and for any ğ‘ âˆˆ N>0 let ğœ‹ âˆ—ğ‘ := (ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆˆ Î ğ»
| {z }
ğ‘ times

(FH-MFG-NE)

Lipschitz constant of Î“ğ‘ƒ in ğœ‡, and let Gğ‘ := (ğ‘ , S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 ) be
the corresponding ğ‘ -player game. Then:

ğ»,ğ‘ ,(ğ‘– )

(1) If ğ¿ = 1, then for all ğ‘– âˆˆ [ğ‘ ], Eğ‘ƒ,ğ‘…

3

(ğœ‹ âˆ—ğ‘ ) â‰¤ O ( âˆšğ» ), that
ğ‘

3
is, ğœ‹ âˆ—ğ‘ is a O ( âˆšğ» )-NE of Gğ‘ .
ğ‘



ğ»,ğ‘ ,(ğ‘– ) âˆ—
ğ» 2 (1âˆ’ğ¿ğ» )
âˆš
(2) If ğ¿ â‰  1, then for all ğ‘– âˆˆ [ğ‘ ], Eğ‘ƒ,ğ‘…
(ğœ‹ ğ‘ ) â‰¤ O
,
(1âˆ’ğ¿) ğ‘


ğ» 2 (1âˆ’ğ¿ğ» )
âˆš
that is, ğœ‹ âˆ—ğ‘ is a O
-NE of Gğ‘ .
(1âˆ’ğ¿) ğ‘

Proof. (sketch) Certain aspects of our proof will mirror the
techniques introduced by [30], although we establish an explicit
bound. We first bound the expected

 empirical population deviation
given by E[âˆ¥b
ğœ‡â„ âˆ’ğœ‡â„ğœ‹ âˆ¥ 1 ] = O âˆšğ¿

â„

with an inductive concentration

ğ‘

argument: at each step â„ + 1, given past states b
ğœ‡â„ , the empirical
distribution b
ğœ‡â„ is a sum of ğ‘ independent identically distributed
sub-Gaussian random variables. Next, by utilizing the Lipschitz
property of rewards and bounding deviation from the theoretical
rewards the result follows in two computational steps: (1) we show
ğ»,ğ‘ ,(1)
ğ» (Î›ğ» (ğœ‡ , ğœ‹ ), ğœ‹ ) â‰¤ O ( 1/âˆšğ‘ ), and simthat ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
ğ‘ƒ 0
ilarly (2) we show that for any policy sequence ğœ‹ â€² âˆˆ Î â„ , we have

Theorem 3.3 (Approximation lower bound for ğ‘ -FH-SAG).
There exists S, A and ğ‘ƒ âˆˆ P8, ğ‘… âˆˆ R 2, ğœ‡0 âˆˆ Î” S such that the
following hold:
(1) For each ğ» > 0, the FH-MFG defined by (S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 ) has
âˆ— (up to modifications on zero-probability
a unique solution ğœ‹ ğ»
sets),
(2) For any ğ», â„ > 0, in the
ğœ‡â„ âˆ’
it holds that Eğ» [âˆ¥b
 ğ‘ -FH-SAG
2ğ» } .
âˆ— ) âˆ¥ ] â‰¥ Î© min{ 1, âˆš
Î›ğ»
(ğœ‡
,
ğœ‹
ğ‘ƒ 0 ğ» â„ 1
ğ‘

(3) For any ğ», ğ‘ > 0 either ğ‘ â‰¥ Î©(2ğ» ), or for each player
ğ»,ğ‘ ,(ğ‘– ) âˆ—
âˆ— ) â‰¥ Î©(ğ» ).
ğ‘– âˆˆ [ğ‘ ] it holds that Eğ‘ƒ,ğ‘…
(ğœ‹ ğ» , . . . , ğœ‹ ğ»
Proof. (sketch) We provide the basic idea of the proof and leave
the cumbersome computations to the appendix. The proof is constructive: we construct an explicit FH-MFG where the statements
hold, depicted in Figure 1. The FH-MFG will have 6 states and
two actions defined as sets S = {ğ‘  Left, ğ‘  Right, ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB }
and A = {ğ‘ A, ğ‘ B }. We define the initial state distribution with
ğ 0 (ğ‘  Left ) = ğ 0 (ğ‘  Right ) = 1/2. The colored state transition probabilities are given by the function:

âˆš

ğ» (Î›ğ» (ğœ‡ , ğœ‹ ), ğœ‹ â€² ) â‰¤ O ( 1/ ğ‘ ). The reğ½ğ‘ƒ,ğ‘…
(ğœ‹ â€², ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
ğ‘ƒ 0
sult follows by definition of exploitability, with explicit constants
shown in the appendix.
â–¡
ğ»,ğ‘ ,(1)

Î“ğ‘ƒ in Theorem 3.2 is always ğ¿-Lipschitz in ğœ‡ for some ğ¿ by
âˆš
Lemma 2.2. When ğ¿ > 1, the upper bound O ( (1 + ğ¿ğ» )ğ» 2/ ğ‘ ) has an
exponential dependence on the Lipschitz constant of the operator
Î“ğ‘ƒ . However, for games with longer horizons, the upper bound
might require an unrealistic amount of agents ğ‘ to guarantee a
good approximation due to the exponential dependency. Next, we
establish a worst-case result demonstrating that this is not avoidable
without additional assumptions.

1 {ğ‘=ğ‘ A }

1 {ğ‘=ğ‘ B }

ğ‘  LA
ğ‘  LB
ğ‘  Right

ğ‘  Left
ğ‘  RA
ğ‘  RB

1 {ğ‘=ğ‘ A }

1 {ğ‘=ğ‘ B }

Figure 1: Visualization of the counterexample. All orange
edges have probability ğœ”ğœ€ (ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB )), green edges have
probability ğœ”ğœ€ (ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB )) independent of action taken.
Edges with probability 0 are not drawn.

ï£±
ï£´
1, ğ‘¥ > 1/2 + ğœ–
ï£´
ï£²
ï£´
.
ğœ”ğœ– (ğ‘¥) = 0, ğ‘¥ < 1/2 âˆ’ ğœ–
ï£´
ï£´
ï£´ 1 + ğ‘¥ âˆ’ 1/2 , ğ‘¥ âˆˆ [1/2 âˆ’ ğœ–, 1/2 + ğœ–]
ï£³2
2ğœ–
The uniform policy over all actions ğœ‹ âˆ— at all states will be the unique
FH-MFG-NE for all ğ» , and the mean-field population distribution
for all even â„ will be ğœ‡â„âˆ— (ğ‘  Left ) = ğœ‡â„âˆ— (ğ‘  Right ) = 1/2. However, for finite
ğ‘ , using an anti-concentration bound on the binomial, we can show
âˆš
that with probability at least 1/10, âˆ¥ğœ‡ 0âˆ— âˆ’ b
ğœ‡â„ âˆ¥ 1 â‰¥ 1/ ğ‘ . Using the fact
âˆ’1
1
that ğœ”ğœ– is (2ğœ–) -expansive in the interval [ /2 âˆ’ ğœ–, 1/2 + ğœ–], we can
then show that the empirical population distribution exponentially
âˆš
âˆ— âˆ’b
diverges from the mean-field, that is E[âˆ¥ğœ‡ 2â„
ğœ‡2â„ âˆ¥ 1 ] â‰¥ Î©( 5â„/ ğ‘ )
âˆš
until time ğ¾ := log5 ğ‘ . Moreover, with a series of concentration bounds, it can be shown that within an expected number of
O (log ğ‘ ) steps, all agents will converge to either ğ‘  Left or ğ‘  Right during even rounds. Only the colored transitions are defined to have
non-zero rewards, whose definition (provided in the supplementary) guarantees that the exploitability suffered scales linearly with
ğ» after ğ‘ agents concentrate on the same state in even steps. â–¡
This result shows that without further assumptions, the FH-MFG
solution might suffer from exponential exploitability in ğ» in the
ğ‘ -player game. In such cases, to avoid the concrete ğ‘ -player game
from deviating from the mean-field behavior too fast, either ğ» must
be small or ğ‘ƒ must be sufficiently smooth in ğœ‡. We note that the
typical assumption in the finite-horizon setting that ğ‘ƒ âˆˆ P0 (see
e.g. [10, 27]) avoids this lower bound since in this case Î“ğ‘ƒ (Â·, ğœ‹)
is simply multiplication by a stochastic matrix which is always
non-expansive (ğ¿ = 1). We also note at the expense of simplicity
a stronger counter-example inducing exploitability Î©(ğ» ) unless
ğ‘ â‰¥ Î©((ğ¿ âˆ’ ğœ–) ğ» ) for all ğœ– > 0 can be constructed, where ğ‘ƒ âˆˆ Pğ¿ .
A remark. The proof of Theorem 3.3 in fact suggests that for
finite ğ‘ and large horizon ğ» , there exists a time-homogenous policy
ğœ‹ â˜… âˆˆ Î  different than the FH-MFG solution such that for ğœ‹ â˜…
ğ» :=
ğ» âˆ’1 âˆˆ Î  , the time-averaged exploitability of ğœ‹ â˜… is small:
{ğœ‹ â˜… }â„=0
ğ»
ğ»
ğ»,ğ‘ ,(ğ‘– )

âˆ€ğ‘– âˆˆ [ğ‘ ] : ğ» âˆ’1 Eğ‘ƒ,ğ‘…

â˜…
âˆ’1 log ğ‘ ).
(ğœ‹ â˜…
2
ğ» , . . . , ğœ‹ ğ» ) â‰¤ O (ğ»

3.2

Approximation Analysis of Stat-MFG

Similarly, we introduce the ğ‘ -player game corresponding to the
Stat-MFG solution concept.
Definition 3.4 (ğ‘ -Stat-SAG). An ğ‘ -player stationary SAG (ğ‘ Stat-SAG) problem is defined by the tuple (ğ‘ , S, A, ğ‘ƒ, ğ‘…, ğ›¾) for Lipschitz dynamics and rewards ğ‘ƒ âˆˆ P, ğ‘… âˆˆ R, discount factor ğ›¾ âˆˆ (0, 1).
For any (ğœ‡, ğœ‹ ) âˆˆ Î” S Ã— Î  ğ‘ , the ğ‘ -player ğ›¾-discounted infinite horizon expected reward is defined as:
Ã
#
"âˆ
ğ‘—e ğ‘—
âˆ‘ï¸
ğ‘ 
ğ‘—
ğ›¾,ğ‘ ,(ğ‘– )
ğ‘— (ğ‘  ğ‘— ),b
â„
ğ‘¡
ğ‘– ğ‘–
ğ‘
âˆ¼ğœ‹
ğœ‡
:=
ğ‘¡
.
ğ½ğ‘ƒ,ğ‘…
(ğœ‡, ğœ‹ ) := E
ğ›¾ ğ‘…(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , b
ğœ‡ğ‘¡ ) ğ‘¡
ğ‘¡
ğ‘
ğ‘—

ğ‘¡ =0

ğœ‡ğ‘¡ )
ğ‘  0 âˆ¼ğœ‡,ğ‘ ğ‘¡ğ‘– +1 âˆ¼ğ‘ƒ (ğ‘ ğ‘¡ğ‘– ,ğ‘ğ‘–ğ‘¡ ,b

A policy profile-population pair (ğœ‡ âˆ—, ğœ‹ âˆ— ) âˆˆ Î” S Ã— Î  ğ‘ is called an
ğ‘ -Stat-SAG Nash equilibrium if:
ğ›¾,ğ‘ ,(ğ‘– )

ğ½ğ‘ƒ,ğ‘…

ğ›¾,ğ‘ ,(ğ‘– )

(ğœ‡ âˆ—, ğœ‹ âˆ— ) = max ğ½ğ‘ƒ,ğ‘…
ğœ‹ âˆˆÎ 

ğ›¾,ğ‘ ,(ğ‘– )

(ğœ‡ âˆ—, (ğœ‹, ğœ‹ âˆ—,âˆ’ğ‘– )). (ğ‘ -Stat-SAG-NE)
ğ›¾,ğ‘ ,(ğ‘– )

If instead ğ½ğ‘ƒ,ğ‘…
(ğœ‡ âˆ—, ğœ‹ âˆ— ) â‰¥ maxğœ‹ âˆˆÎ  ğ½ğ‘ƒ,ğ‘…
(ğœ‡ âˆ—, (ğœ‹, ğœ‹ âˆ—,âˆ’ğ‘– )) âˆ’ ğ›¿,
âˆ—
âˆ—
then we call ğœ‡ , ğœ‹ a ğ›¿-ğ‘ -Stat-SAG Nash equilibrium.

By the design of reward functions, this yields an exploitability of
Î©






âˆ’1
1 + 2ğ›¾ 2 + . . . + (2ğ›¾ 2 )ğ‘‡ âˆ’1
= Î© ğ‘ âˆ’ log2 ğ›¾
.
âˆš
ğ‘

The proof is postponed to the supplementary material.

â–¡

The result above shows that unless the relevant Î“ğ‘ƒ operator is
contracting in some potential, in general, the exploitability of the
Stat-MFG-NE in the ğ‘ -player game might be very large unless
the effective horizon (1 âˆ’ ğ›¾) âˆ’1 is small. Hence, in these cases, the
mean-field Nash equilibrium might be uninformative regarding the
true NE of the ğ‘ player game. In the case of Stat-MFG, our lower
bound is even stronger in the sense that the exploitability no longer
âˆš
decreases with O ( 1/ ğ‘ ) for large ğ›¾. For a sufficiently long effective
âˆ’1
horizon (1 âˆ’ ğ›¾) and large enough Lipschitz constant ğ¿, the rate
in terms of ğ‘ can be arbitrarily slow. Furthermore, if we take the
ergodic limit ğ›¾ â†’ 1, we will observe a non-vanishing exploitability
Î©(1) for all finite ğ‘ .

Theorem 3.5 (Approximation of ğ‘ -Stat-SAG). Let (S, A, ğ», ğ‘ƒ, ğ‘…, ğ›¾)
4 COMPUTATIONAL TRACTABILITY OF MFG
be a Stat-MFG and (ğœ‡ âˆ—, ğœ‹ âˆ— ) âˆˆ Î” S Ã— Î  be a corresponding Stat-MFGThe
next fundamental question for mean-field reinforcement learnNE. Furthermore, assume that Î“ğ‘ƒ (Â·, ğœ‹) is non-expansive in the â„“1
â€²
â€²
ing
will
be whether it is always computationally easier than finding
norm for any ğœ‹, that is, âˆ¥Î“ğ‘ƒ(ğœ‡, ğœ‹)
 âˆ’ Î“ğ‘ƒ (ğœ‡ , ğœ‹)âˆ¥ 1 â‰¤ âˆ¥ğœ‡ âˆ’ ğœ‡ âˆ¥ 1 . Then,
an
equilibrium
of a ğ‘ -player general sum normal form game. We
1
(ğœ‡ âˆ—, ğœ‹ âˆ— ) âˆˆ Î” S Ã— Î  ğ‘ is a O âˆš
Nash equilibrium for the ğ‘ -player
ğ‘
focus on the computational aspect of solving mean-field games in
game where ğœ‹ âˆ—ğ‘ := (ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ), that is, for all ğ‘–,
this section, and not statistical uncertainty: we assume we have


full knowledge of the MFG dynamics. We will show that unless
(1 âˆ’ ğ›¾) âˆ’3
ğ›¾,ğ‘ ,(ğ‘– ) âˆ— âˆ—
ğ›¾,ğ‘ ,(ğ‘– ) âˆ—
additional assumptions are introduced (as typically done in the
ğ½ğ‘ƒ,ğ‘…
(ğœ‡ , ğœ‹ ğ‘ ) â‰¥ max ğ½ğ‘ƒ,ğ‘…
(ğœ‡ , (ğœ‹, ğœ‹ âˆ—,âˆ’ğ‘–
))
âˆ’
O
.
âˆš
ğ‘
ğœ‹ âˆˆÎ 
ğ‘
form of contractivity or monotonicity), solving MFG can in general
be as hard as finding ğ‘ -player general sum Nash.
Proof. (sketch) Let (ğœ‡ âˆ—, ğœ‹ âˆ— ) be a Stat-MFG-NE. The proof method
We will prove that the problems are PPAD-complete, where
is very similar to the FH-MFG case: we first bound the expected
PPAD
is a class of computational problems studied in the seminal
âˆ—
âˆ—
deviation from the stable distribution ğœ‡ given by E[âˆ¥b
ğœ‡ âˆ’ ğœ‡ âˆ¥ 1 ].
work
by
Papadimitriou [24], containing the complete problem of
The truncated expected rewards can be controlled using similar
finding ğ‘ -player Nash equilibrium in general sum normal form
arguments to the FH-MFG case, and an application of the domigames and finding the fixed point of continuous maps [5, 7]. The
nated convergence theorem yields the exploitability for the infinite
class PPAD is conjectured to contain difficult problems with no
horizon discounted setting.
â–¡
polynomial time algorithms [2, 11], hence our results can be seen as
a proof of difficulty. Our results are significant since they imply that
We also establish an approximation lower bound for the ğ‘ -Statthe MFG problems studied in literature are in the same complexity
SAG. In this case, the question is if the non-expansive Î“ğ‘ƒ assumption
âˆš
class as general-sum ğ‘ -player normal form games or ğ‘ -player
1
is necessary for the optimal O ( / ğ‘ ) rate. The below results affirm
Markov games [8]. Once again, several computation-intensive asthis: in for Stat-MFG-NE with expansive Î“ğ‘ƒ , we suffer from an
âˆš
pects of our proofs will be postponed to the supplementary material.
1
exploitability of ğœ” ( / ğ‘ ) in the ğ‘ -agent case.
Due to a technical detail, we will prove the complexity results for
Theorem 3.6 (Lower bound for ğ‘ -Stat-SAG). For any ğ‘ âˆˆ
a subset of possible reward and transition probability functions. We
âˆš
N>0, ğ›¾ âˆˆ ( 1/ 2, 1) there exists S, A with |S| = 6, |A| = 2 and ğ‘ƒ âˆˆ
formalize this subset of possible rewards and dynamics as â€œsimpleâ€
P7, ğ‘… âˆˆ R 3 such that:
rewards/dynamics and also linear rewards, defined below.
(1) The Stat-MFG (S, A, ğ‘ƒ, ğ‘…, ğ›¾) has a unique NE ğœ‡ âˆ—, ğœ‹ âˆ— ,
Definition 4.1 (Simple/Linear Dynamics and Rewards). ğ‘… âˆˆ R
(2) For any ğ‘ and ğœ‹ âˆ—ğ‘ := (ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆˆ Î  ğ‘ , it holds that
and
ğ‘ƒ âˆˆ P are said to be simple if for any ğ‘ , ğ‘  â€² âˆˆ S, ğ‘ âˆˆ A,
âˆ’1
ğ›¾,ğ‘ ,(ğ‘– ) âˆ—
ğ›¾,ğ‘ ,(ğ‘– )
âˆ’ log2 ğ›¾ ).
ğ½ğ‘ƒ,ğ‘…
(ğœ‹ ğ‘ ) â‰¤ maxğœ‹ ğ½ğ‘ƒ,ğ‘…
(ğœ‹, ğœ‹ âˆ—,âˆ’ğ‘–
)
âˆ’
Î©(ğ‘
â€²
ğ‘
ğ‘ƒ (ğ‘  |ğ‘ , ğ‘, ğœ‡) and ğ‘…(ğ‘ , ğ‘, ğœ‡) are functions of ğœ‡ that are expressible
as finite combinations of arithmetic operations +, âˆ’, Ã—, Â·Â· and funcProof. (sketch) The counter-example will be similar to the case
tions max{Â·, Â·}, min{Â·, Â·} of coordinates of ğœ‡. They are called linear
in the FH-MFG, with minor modifications to make the Stat-MFGif ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘, ğœ‡) and ğ‘…(ğ‘ , ğ‘, ğœ‡) are linear functions of ğœ‡ for all ğ‘ , ğ‘, ğ‘  â€² .
NE unique. Intuitively,
due
to
the
same
anti-concentration
bound
as
âˆš
The set of simple rewards and dynamics are denoted by R Sim and
before for ğ‘‡ = log2 ğ‘ , at times ğ‘¡ = 0, 2, 4, . . . ,ğ‘‡ âˆ’ 1 the population
P Sim respectively, and the set of linear rewards and transitions are
ğ‘¡
deviation from ğœ‡ âˆ— can be lower bounded by E[âˆ¥b
ğœ‡ğ‘¡ âˆ’ğœ‡ âˆ— âˆ¥ 1 ] â‰¥ Î©( âˆš2 ).
denoted
R Lin, P Lin respectively.
ğ‘

A note on simple functions. We define simple functions as
above as in general there is no known efficient encoding of a Lipschitz continuous function as a sequence of bits. This is significant since a Turing machine accepts a finite sequence of bits as
input. To solve this issue, we prove a slightly stronger hardness
result that even games where ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘, ğœ‡), ğ‘…(ğ‘ , ğ‘, ğœ‡) are Lipschitz
functions with strong structure are PPAD-complete. Since we are
proving hardness, other larger classes of ğ‘ƒ, ğ‘… including P Sim, R Sim
will have similar intractability. See also arithmetic circuits with
max, min gates [9] for a similar idea.

4.1

The Complexity Class PPAD

The PPAD class is defined by the complete problem End-of-TheLine [7], whose formal definition we defer to the appendix as it is
not used in our proofs.
Definition 4.2 (PPAD, PPAD-hard, PPAD-complete). The class PPAD
is defined as all search problems that can be reduced to End-ofThe-Line in polynomial time. If End-of-The-Line can be reduced
to a search problem S in polynomial time, then S is called PPADhard. A search problem S is called PPAD-complete if it is both a
member of PPAD and it is PPAD-hard.
While End-of-the-Line defines the problem class PPAD, it is
hard to construct direct reductions to it. We will instead use two
problems that are known to be PPAD-complete (and hence can be
equivalently used to define PPAD): solving generalized circuits and
finding a NE for an ğ‘ -player general sum game.
Definition 4.3 (Generalized Circuits [8, 29]). A generalized circuit C = (V, G) is a finite set of nodes V and gates G. Each
gate ğº âˆˆ G is characterized by the tuple ğº (ğœƒ |ğ‘£ 1, ğ‘£ 2 |ğ‘£) where ğº âˆˆ
{ğº â†, ğº Ã—,+, G< }, ğœƒ âˆˆ Râ˜… is a parameter (possibly of length 0),
ğ‘£ 1, ğ‘£ 2 âˆˆ ğ‘‰ âˆª {âŠ¥} are the input nodes (with âŠ¥ indicating an empty input) and ğ‘£ âˆˆ ğ‘‰ it the output node of the gate. The collection of gates
G satisfies the property that if ğº 1 (ğœƒ |ğ‘£ 1, ğ‘£ 2 |ğ‘£), ğº 2 (ğœƒ â€² |ğ‘£ 1â€² , ğ‘£ 2â€² |ğ‘£ â€² ) âˆˆ ğº
are distinct gates, then ğ‘£ â‰  ğ‘£ â€² .
Such circuits define a set of constraints on values assigned to
each gate, and finding such an assignment will be the associated
computational problem for such a circuit desription. We formally
define the ğœ€-GCircuit problem to this end. ğœ€-GCircuit is a standard
complete problem for the class PPAD, and we will work with it for
our reductions. We will use the shorthand notation ğ‘¥ = ğ‘¦ Â± ğœ€ to
indicate that ğ‘¥ âˆˆ [ğ‘¦ âˆ’ ğœ€, ğ‘¦ + ğœ€] for ğ‘¥, ğ‘¦ âˆˆ R.
Definition 4.4 (ğœ€-GCircuit [29]). Given a generalized circuit C =
(V, G), a function ğ‘ : ğ‘‰ â†’ [0, 1] is called an ğœ€-satisfying assignment if:
â€¢ For every gate ğº âˆˆ G of the form ğº â† (ğœ ||ğ‘£) for ğœ âˆˆ 0, 1, it
holds that ğ‘ (ğ‘£) = ğœ Â± ğœ€,
â€¢ For every gate ğº âˆˆ G of the form ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£) for
ğ›¼, ğ›½ âˆˆ [âˆ’1, 1], it holds that
ğ‘ (ğ‘£) âˆˆ [max{min{0, ğ›¼ğ‘ (ğ‘£ 1 ) + ğ›½ğ‘ (ğ‘£ 2 )}}] Â± ğœ€,
â€¢ For every gate ğº âˆˆ G of the form ğº < (|ğ‘£ 1, ğ‘£ 1 |ğ‘£) it holds that
(
1 Â± ğœ€, ğ‘ (ğ‘£ 1 ) â‰¤ ğ‘ (ğ‘£ 2 ) âˆ’ ğœ€,
ğ‘ (ğ‘£) =
0 Â± ğœ€, ğ‘ (ğ‘£ 1 ) â‰¥ ğ‘ (ğ‘£ 2 ) + ğœ€.

The ğœ€-GCircuit problem is defined as follows:
Given generalized circuit C, find an ğœ€-satisfying assignment of C.
ğœ€-GCircuit is one of the prototypical hard instances of PPAD
problems as the result below suggests.
Theorem 4.5. [29] There exists ğœ€ > 0 such that ğœ€-GCircuit is
PPAD-complete.
In other words, ğœ€-GCircuit is representative of the most difficult
problem in PPAD which suggests intractability. The ğœ€-GCircuit
computational problem will be used in our proofs by reducing an
arbitrary generalized circuit into solving a particular MFG.
We will also use the general sum 2-player Nash computation
problem, which is the standard problem of finding an approximate
Nash equilibrium of a general sum bimatrix game.
Definition 4.6 (2-Nash). Given ğœ€ > 0, ğ¾1, ğ¾2 âˆˆ N>0 , payoff matrices ğ´, ğµ âˆˆ [0, 1] ğ¾1 ,ğ¾2 , find an approximate Nash equilibrium
(ğœ1, ğœ2 ) âˆˆ Î”ğ¾1 Ã— Î”ğ¾2 such that
âˆ‘ï¸ âˆ‘ï¸
âˆ‘ï¸ âˆ‘ï¸
max
ğ´ğ‘–,ğ‘— ğœ (ğ‘–)ğœ2 ( ğ‘—) âˆ’
ğ´ğ‘–,ğ‘— ğœ1 (ğ‘–)ğœ2 ( ğ‘—) â‰¤ ğœ€

ğœ âˆˆÎ”ğ¾1

max

ğœ âˆˆÎ”ğ¾2

ğ‘– âˆˆ [ğ¾1 ] ğ‘— âˆˆ [ğ¾2 ]

âˆ‘ï¸

âˆ‘ï¸

ğ‘– âˆˆ [ğ¾2 ] ğ‘âˆˆ [ğ¾2 ]

ğ‘– âˆˆ [ğ¾1 ] ğ‘— âˆˆ [ğ¾2 ]

ğµğ‘–,ğ‘— ğœ1 (ğ‘–)ğœ ( ğ‘—) âˆ’

âˆ‘ï¸

âˆ‘ï¸

ğµğ‘–,ğ‘— ğœ1 (ğ‘–)ğœ2 ( ğ‘—) â‰¤ ğœ€

ğ‘– âˆˆ [ğ¾1 ] ğ‘— âˆˆ [ğ¾2 ]

The following is the well-known result that even the 2-Nash general sum problem is PPAD-complete. In fact, any ğ‘ -player general
sum normal form game is PPAD-complete.
Theorem 4.7. [5] 2-Nash is PPAD-complete.

4.2

Complexity of Stat-MFG

Next, we provide our difficulty results for the Stat-MFG problem.
Notably, for Stat-MFG, the stability subproblem of finding a stable
distribution for a fixed policy ğœ‹ itself is PPAD-hard. Even without
considering the optimality conditions, finding a stable distribution
in general for a fixed policy is intractable, unless additional assumptions are introduced (e.g. Î“ğ‘ƒ is contractive or non-expansive). We
define the computational problem below and state the results.
Definition 4.8 (ğœ€-StatDist). Given finite state-action sets S, A,
simple dynamics ğ‘ƒ âˆˆ P Sim and policy ğœ‹, find ğœ‡ âˆ— âˆˆ Î” S such that
âˆ¥Î“ğ‘ƒ (ğœ‡ âˆ—, ğœ‹) âˆ’ ğœ‡ âˆ— âˆ¥ âˆ â‰¤ | Sğœ€ | .
The computational problem as described above is to find an approximate fixed point of Î“ğ‘ƒ (Â·, ğœ‹) which corresponds to an approximately stable distribution of policy ğœ‹. We show that ğœ€-StatDist is
PPAD-complete for some fixed constant ğœ€.
Theorem 4.9 (ğœ€-StatDist is PPAD-complete). For some ğœ€ > 0,
the problem ğœ€-StatDist is PPAD-complete.
Proof. (sketch) The reduction from ğœ€-StatDist to a fixed point
problem (or the Sperner problem [7]) is straightforward, showing
ğœ€-StatDist is in PPAD. The main challenge of the proof is showing ğœ€-StatDist is simultaneously PPAD-hard. This is achieved by
showing any ğœ€-GCircuit problem can be reduced to a ğœ€-StatDist
for some ğœ€ â€² . For simplicity, we reduce ğœ€-GCircuit to finding the
stable distribution of a transition kernel ğ‘ƒ (ğ‘  â€² |ğ‘ , ğœ‡). Given a generalized circuit C = (V, G), we construct a Stat-MFG that has one

base state ğ‘  base , one additional state ğ‘  ğ‘£ for each ğ‘£ âˆˆ V that is the
1 , ğµ := 1 . Also define the function
output of a gate. Let ğœƒ := 8ğ‘‰
4
ğ‘¢ğ›¼ (ğ‘¥) := max{0, min{ğ›¼, ğ‘¥ }} for any ğ›¼ âˆˆ [0, 1]. We present the construction and defer the analysis to the appendix: any gate of the
form ğº â† (ğœ ||ğ‘£), we will add one state ğ‘  ğ‘£ such that ğ‘ƒ (ğ‘  base |ğ‘  ğ‘£ , ğœ‡) = 1,
ğœğœƒ
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) = max{ğµ, ğœ‡ (ğ‘  ) } . For any weighted addition gate
base
ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£), we add a state ğ‘  ğ‘£ such that ğ‘ƒ (ğ‘  base |ğ‘  ğ‘£ , ğœ‡) = 1
ğ‘¢ (ğ›¼ ğœ‡ (ğ‘£ )+ğ›½ğœ‡ (ğ‘£ ) )
and ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) = ğœƒmax{ğµ,1ğœ‡ (ğ‘  )2} . Finally, for each comparibase
son gate ğº < (|ğ‘£ 1, ğ‘£ 1 |ğ‘£), also add a state ğ‘  ğ‘£ and define the transition
probabilities:
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) =

ğœƒğ‘ğœ€/8 (ğœƒ âˆ’1 ğœ‡ (ğ‘  1 ), ğœƒ âˆ’1 ğœ‡ (ğ‘  2 ))

,
max{ğµ, ğœ‡ (ğ‘  base )}
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  ğ‘£ , ğœ‡) = 0, ğ‘ƒ (ğ‘  base |ğ‘  ğ‘£ , ğœ‡) = 1,


where ğ‘ğœ€ (ğ‘¥, ğ‘¦) := ğ‘¢ 1 12 + ğœ€ âˆ’1 (ğ‘¥ âˆ’ ğ‘¦) . Once all gates are added,
the construction is completed by defining ğ‘ƒ (ğ‘  base |ğ‘  base, ğœ‡) = 1 âˆ’
Ã
â€²
ğ‘  â€² âˆˆ S ğ‘ƒ (ğ‘  |ğ‘  base , ğœ‡). Simple computation verifies that for any exact
stationary distribution ğœ‡ âˆ— of the above ğ‘ƒ, an exact assignment the
ğœ‡ âˆ— (ğ‘  )
the generalized circuit can be read by the map ğ‘£ â†’ ğ‘¢ 1 ( ğœƒ ğ‘£ ). â–¡
As a corollary, there is no polynomial time algorithm for ğœ€StatDist unless PPAD=P, which is conjectured to be not the case.
Corollary 4.10. There exists a ğœ€ > 0 such that there exists no
polynomial time algorithm for ğœ€-StatDist, unless P = PPAD.
Most notably, these results show that the stable distribution
oracle of [6] might be intractable to compute in general, and the
shared assumption that Î“ğ‘ƒ (Â·, ğœ‹) is contractive in some norm found
in many works [1, 35, 36] might not be trivial to remove without
sacrificing tractability.

4.3

Complexity of FH-MFG

We will show that finding an ğœ€ solution to the finite horizon problem
is also PPAD-complete, in particular even if we restrict our attention
to the case when ğ» = 2 and the transition probabilities ğ‘ƒ do not
depend on ğœ‡. We formalize the structured computational FH-MFG
problem.
Definition 4.11 ((ğœ€, ğ» )-FH-Nash). Given simple reward function
ğ‘… âˆˆ R Sim , transition matrix ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘), and initial distribution ğœ‡ 0 âˆˆ
ğ» âˆ’1 such that E ğ» ({ğœ‹ }ğ» âˆ’1 ) â‰¤
Î” S , find a time dependent policy {ğœ‹â„ }â„=0
â„ â„=0
ğ‘ƒ,ğ‘…
ğœ€/|S| .
Our result in the case of the finite horizon MFG problem is that
even in the case of ğ» = 2, the problem is PPAD-complete.
Theorem 4.12 ((ğœ€, 2)-FH-Nash is PPAD-complete). There exists
an ğœ€ > 0 such that the problem (ğœ€, 2)-FH-Nash is PPAD-complete.
Proof. (sketch) Once again, showing (ğœ€, 2)-FH-Nash is in PPAD
is simple: it follows from the fact that a FH-MFG-NE is a fixed
point of an easy-to-compute function (see e.g. [15]). To show that
(ğœ€, 2)-FH-Nash is also PPAD-hard, for an arbitrary generalized
circuit C = (V, G) we construct a FH-MFG whose ğ›¿-NE will be
ğ›¿ â€² -satisfying assignments for C for some ğ›¿â€™.
â–¡

These results for the FH-MFG show that the (weak) monotonicity
assumption present in works such as [25, 27] might also be necessary, as in the absence of any structural assumptions the problems
are provably difficult.
Finally, we also show that even if ğ‘…(ğ‘ , ğ‘, ğœ‡) is a linear function
of ğœ‡ for all ğ‘ , ğ‘ (that is, ğ‘… âˆˆ R Lin ), the intractability holds, although
not for fixed ğœ€. We define the linear computational problem below.
Definition 4.14 (ğ» -FH-Linear). Given ğœ€ > 0, linear reward function ğ‘… âˆˆ R Lin , transition matrix ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘), find a time dependent
ğ» âˆ’1 such that E ğ» ({ğœ‹ }ğ» âˆ’1 ) â‰¤ ğœ€.
policy {ğœ‹â„ }â„=0
â„ â„=0
ğ‘ƒ,ğ‘…
Theorem 4.15 (2-FH-Linear is PPAD-complete). The problem
2-FH-Linear is PPAD-complete.
Proof. (sketch) In this case, we provide a reduction from 2Nash. For a given 2-Nash instance ğ¾1, ğ¾2 âˆˆ N>0 with payoff matrices ğ´, ğµ âˆˆ [0, 1] ğ¾1 ,ğ¾2 , we construct an FH-MFG with one initial
state for each player and one additional state for each strategy
of each of the players, resulting in a FH-MFG with ğ¾1 + ğ¾2 + 2
1 , ğ‘  2 , ğ‘  1 , . . . , ğ‘  1 , ğ‘  2 , . . . , ğ‘  2 }. We set ğœ‡ (ğ‘  1 ) =
states, S := {ğ‘  base
0 base
ğ¾1 1
ğ¾2
base 1
2 ) = 1/2. The action set will consist of max{ğ¾ , ğ¾ } actions.
ğœ‡ 0 (ğ‘  base
1 2
1
In the first round, an agent starting from ğ‘  base
will be transitioned
to one of states ğ‘  11, . . . , ğ‘ ğ¾1 depending on the action picked receiving
1
2
zero reward, and likewise and agent starting from ğ‘  base
will tran2
2
sition to one of states ğ‘  1 , . . . , ğ‘ ğ¾ . In the second round, the agent
2
will receive a population-dependent reward regardless of the action
player, which is equal to the expected utility of an action (a linear
function). We postpone the cumbersome details relating to error
analysis and dealing with the case ğ¾1 â‰  ğ¾2 to the appendix.
â–¡
We emphasize that for 2-FH-Linear the accuracy ğœ€ is also an
input of the problem: hence the existence of a pseudo-polynomial
time algorithm is not ruled out.

5

DISCUSSION AND CONCLUSION

We provided novel results on when mean-field RL is relevant for
real-world applications and when it is tractable from a computational perspective. Our results differ from existing work by provably
characterizing cases where MFGs might have practical shortcomings. From the approximation perspective, we show clear conditions
and lower bounds on when the MFGs efficiently approximate realworld games. Computationally, we show that even simple MFGs
can be as hard as solving ğ‘ -player general sum games.
We emphasize that our results do not discard MFGs, but rather
identify potential bottlenecks (and conditions to overcome these)
when using mean-field RL to compute a good approximate NE.

ACKNOWLEDGMENTS
This project is supported by Swiss National Science Foundation
(SNSF) under the framework of NCCR Automation and SNSF Starting Grant. A.Goldman was supported by the ETH Student Summer
Research Fellowship.

REFERENCES
Corollary 4.13. There exists a ğœ€ > 0 such that there exists no
polynomial time algorithm for (ğœ€, 2)-FH-Nash, unless P= PPAD.

[1] Berkay Anahtarci, Can Deha Kariksiz, and Naci Saldi. 2022. Q-learning in regularized mean-field games. Dynamic Games and Applications (2022), 1â€“29.

[2] Paul Beame, Stephen Cook, Jeff Edmonds, Russell Impagliazzo, and Toniann
Pitassi. 1995. The relative complexity of NP search problems. In Proceedings of
the twenty-seventh annual ACM symposium on Theory of computing. Las Vegas,
Nevada, USA, 303â€“314.
[3] RenÃ© Carmona and FranÃ§ois Delarue. 2013. Probabilistic analysis of mean-field
games. SIAM Journal on Control and Optimization 51, 4 (2013), 2705â€“2734.
[4] RenÃ© Carmona, FranÃ§ois Delarue, et al. 2018. Probabilistic theory of mean field
games with applications I-II. Springer.
[5] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. 2009. Settling the complexity of
computing two-player Nash equilibria. Journal of the ACM (JACM) 56, 3 (2009),
1â€“57.
[6] Kai Cui and Heinz Koeppl. 2021. Approximately solving mean field games via
entropy-regularized deep reinforcement learning. In International Conference on
Artificial Intelligence and Statistics. PMLR, 1909â€“1917.
[7] Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. 2009.
The complexity of computing a Nash equilibrium. Commun. ACM 52, 2 (2009),
89â€“97.
[8] Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. 2023. The complexity of markov equilibrium in stochastic games. In The Thirty Sixth Annual
Conference on Learning Theory. PMLR, 4180â€“4234.
[9] Constantinos Daskalakis and Christos Papadimitriou. 2011. Continuous local
search. In Proceedings of the twenty-second annual ACM-SIAM symposium on
Discrete Algorithms. SIAM, 790â€“804.
[10] Matthieu Geist, Julien PÃ©rolat, Mathieu LauriÃ¨re, Romuald Elie, Sarah Perrin,
Oliver Bachem, RÃ©mi Munos, and Olivier Pietquin. 2022. Concave Utility Reinforcement Learning: The Mean-field Game Viewpoint. In Proceedings of the 21st
International Conference on Autonomous Agents and Multiagent Systems (Virtual
Event, New Zealand) (AAMAS â€™22). International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC, 489â€“497.
[11] Paul W Goldberg. 2011. A survey of PPAD-completeness for computing Nash
equilibria. arXiv preprint arXiv:1103.2709 (2011).
[12] Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. 2019. Learning mean-field
games. Advances in Neural Information Processing Systems 32 (2019).
[13] Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. 2022. A general framework
for learning mean-field games. Mathematics of Operations Research (2022).
[14] Xin Guo, Anran Hu, and Junzi Zhang. 2022. MF-OMO: An optimization formulation of mean-field games. arXiv preprint arXiv:2206.09608 (2022).
[15] Jiawei Huang, Batuhan Yardim, and Niao He. 2023. On the Statistical Efficiency
of Mean Field Reinforcement Learning with General Function Approximation.
arXiv preprint arXiv:2305.11283 (2023).
[16] Minyi Huang, Roland P MalhamÃ©, and Peter E Caines. 2006. Large population
stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash
certainty equivalence principle. Communications in Information & Systems 6, 3
(2006), 221â€“252.
[17] Krishnamurthy Iyer, Ramesh Johari, and Mukund Sundararajan. 2014. Mean field
equilibria of dynamic auctions with learning. Management Science 60, 12 (2014),
2949â€“2970.
[18] Rob Kaas and Jan M Buhrman. 1980. Mean, median and mode in binomial
distributions. Statistica Neerlandica 34, 1 (1980), 13â€“18.
[19] Jean-Michel Lasry and Pierre-Louis Lions. 2007. Mean field games. Japanese
journal of mathematics 2, 1 (2007), 229â€“260.
[20] Mathieu LauriÃ¨re, Sarah Perrin, Sertan Girgin, Paul Muller, Ayush Jain, ThÃ©ophile
Cabannes, Georgios Piliouras, Julien Pâ€™erolat, Romuald Elie, Olivier Pietquin, and
Matthieu Geist. 2022. Scalable Deep Reinforcement Learning Algorithms for
Mean Field Games. In International Conference on Machine Learning.
[21] Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk,
Ravi Iyer, and Tamer Basar. 2022. A Mean-Field Game Approach to Cloud
Resource Management with Function Approximation. In Advances in Neural
Information Processing Systems.
[22] LaÃ«titia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. 2007. Hysteretic
q-learning: an algorithm for decentralized reinforcement learning in cooperative
multi-agent teams. In 2007 IEEE/RSJ International Conference on Intelligent Robots
and Systems. IEEE, 64â€“69.
[23] Colin McDiarmid et al. 1989. On the method of bounded differences. Surveys in
combinatorics 141, 1 (1989), 148â€“188.
[24] Christos H Papadimitriou. 1994. On the complexity of the parity argument and
other inefficient proofs of existence. Journal of Computer and system Sciences 48,
3 (1994), 498â€“532.
[25] Julien PÃ©rolat, Sarah Perrin, Romuald Elie, Mathieu LauriÃ¨re, Georgios Piliouras,
Matthieu Geist, Karl Tuyls, and Olivier Pietquin. 2022. Scaling Mean Field Games
by Online Mirror Descent. In Proceedings of the 21st International Conference on
Autonomous Agents and Multiagent Systems. 1028â€“1037.
[26] Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. 2015. Approximate
dynamic programming for two-player zero-sum markov games. In International
Conference on Machine Learning. PMLR, 1321â€“1329.
[27] Sarah Perrin, Julien PÃ©rolat, Mathieu LauriÃ¨re, Matthieu Geist, Romuald Elie, and
Olivier Pietquin. 2020. Fictitious play for mean field games: Continuous time
analysis and applications. Advances in Neural Information Processing Systems 33

(2020), 13199â€“13213.
[28] Navid Rashedi, Mohammad Amin Tajeddini, and Hamed Kebriaei. 2016. Markov
game approach for multi-agent competitive bidding strategies in electricity market. IET Generation, Transmission & Distribution 10, 15 (2016), 3756â€“3763.
[29] Aviad Rubinstein. 2015. Inapproximability of Nash equilibrium. In Proceedings of
the forty-seventh annual ACM symposium on Theory of computing. 409â€“418.
[30] Naci Saldi, Tamer Basar, and Maxim Raginsky. 2018. Markovâ€“Nash equilibria in
mean-field games with discounted cost. SIAM Journal on Control and Optimization
56, 6 (2018), 4256â€“4287.
[31] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr,
Jakob Foerster, and Shimon Whiteson. 2019. The StarCraft Multi-Agent Challenge. In Proc. of the 18th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS 2019) (Montreal QC, Canada) (AAMAS â€™19). International Foundation for Autonomous Agents and Multiagent Systems, Richland,
SC, 2186â€“2188.
[32] Ali Shavandi and Majid Khedmati. 2022. A multi-agent deep reinforcement
learning framework for algorithmic trading in financial markets. Expert Systems
with Applications 208 (2022), 118124.
[33] Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. 2020. Breaking the curse of
many agents: Provable mean embedding Q-iteration for mean-field reinforcement
learning. In International conference on machine learning. PMLR, 10092â€“10103.
[34] Marco A. Wiering. 2000. Multi-agent reinforcement learning for traffic light control. In Machine Learning: Proceedings of the Seventeenth International Conference
(ICMLâ€™2000). 1151â€“1158.
[35] Qiaomin Xie, Zhuoran Yang, Zhaoran Wang, and Andreea Minca. 2021. Learning
while playing in mean-field games: Convergence and optimality. In International
Conference on Machine Learning. PMLR, 11436â€“11447.
[36] Batuhan Yardim, Semih Cayci, Matthieu Geist, and Niao He. 2023. Policy mirror
ascent for efficient and independent learning in mean field games. In International
Conference on Machine Learning. PMLR, 39722â€“39754.
[37] Batuhan Yardim, Semih Cayci, and Niao He. 2023. Stateless Mean-Field Games:
A Framework for Independent Learning with Large Populations. In Sixteenth
European Workshop on Reinforcement Learning.
[38] Muhammad Aneeq Uz Zaman, Alec Koppel, Sujay Bhatt, and Tamer Basar. 2023.
Oracle-free reinforcement learning in mean-field games along a single sample
path. In International Conference on Artificial Intelligence and Statistics. PMLR,
10178â€“10206.

A MFG APPROXIMATION RESULTS
A.1 Preliminaries
To establish explicit upper bounds on the approximation rate, we
will use standard concentration tools.
Definition A.1 (Sub-Gaussian). Random variablehğœ‰ is called subi

Gaussian with variance proxy ğœ 2 if âˆ€ğœ† âˆˆ R :
ğ‘’

ğœ†2 ğœ 2
2

E ğ‘’ ğœ† (ğœ‰ âˆ’E[ğœ‰ ] ) â‰¤

. In this case, we write ğœ‰ âˆˆ ğ‘†ğº (ğœ 2 ).

It is easy to show that if ğœ‰ âˆˆ ğ‘†ğº (ğœ 2 ), then ğ›¼ğœ‰ âˆˆ ğ‘†ğº (ğ›¼ 2 ğœ 2 ) for any
constant ğ›¼ âˆˆ R. Furthermore, if ğœ‰ 1, . . . , ğœ‰ğ‘› are independent random
Ã
Ã
variables with ğœ‰ğ‘– âˆˆ ğ‘†ğº (ğœğ‘–2 ), then ğ‘– ğœ‰ğ‘– âˆˆ ğ‘†ğº ( ğ‘– ğœğ‘–2 ). Finally, if
ğœ‰ is almost surely bounded in [ğ‘, ğ‘], then ğœ‰ğ‘– âˆˆ ğ‘†ğº ( (ğ‘ âˆ’ ğ‘) 2/4). We
also state the well-known Hoeffding concentration bound and a
corollary, Lemma A.3.
Lemma A.2 (Hoeffding ineqality [23]). Let ğœ‰ âˆˆ ğ‘†ğº (ğœ 2 ). Then
2

for any ğ‘¡ > 0 it holds that P (|ğœ‰ âˆ’ E [ğœ‰] | â‰¥ ğ‘¡) â‰¤ 2ğ‘’

âˆ’ ğ‘¡2
2ğœ

.


First assuming ğ‘ is even, we obtain by monotonicity ğ‘ğ‘˜ â‰¤
âˆš
1
1
ğ‘
2ğœ‹ğ‘˜ ğ‘˜+ 2 e âˆ’ğ‘˜ â‰¤ ğ‘˜! â‰¤ eğ‘˜ ğ‘˜+ 2 e âˆ’ğ‘˜ ,
ğ‘ /2 . Using the Stirling bound

ğ‘
we further upper bound ğ‘ğ‘/2 â‰¤ ğœ‹ğ‘’ âˆš2 , resulting in the bound
ğ‘
âˆš
âˆš i
h
âˆš

P ğ‘2 âˆ’ 2ğ‘ < ğ‘‹ < ğ‘2 + 2ğ‘ â‰¤ 2 âˆ’ğ‘ ğ‘ ğ‘ğ‘/2 â‰¤ ğœ‹ğ‘’ â‰¤ 9/10, since
âˆš
there are at most ğ‘ binomial coefficients being summed. Finally,

assume ğ‘ = 2ğ‘š + 1 is odd, then by the binomial formula 2ğ‘š+1
ğ‘š+1 =



2ğ‘š
2ğ‘š
2ğ‘š
2ğ‘š
2ğ‘’ âˆš
2
ğ‘š+1 + ğ‘š â‰¤ 2 ğ‘š â‰¤ ğœ‹ 2ğ‘š . Hence we have the bound on
âˆš
âˆš i
âˆš
h
the sum P ğ‘2 âˆ’ 2ğ‘ < ğ‘‹ < ğ‘2 + 2ğ‘ â‰¤ ğ‘’ ğœ‹ğ‘ âˆš 1 . It is easy to
ğ‘ âˆ’1

âˆš

verify that for ğ‘ â‰¥ 16, ğ‘’âˆš ğ‘ â‰¤ 9/10, and the case when ğ‘ < 16
ğœ‹ ğ‘ âˆ’1
and ğ‘ is odd follows by manual computation.
â–¡
Finally, we prove slightly more general upper bounds than presented in the main text that approximates the exploitability of an
approximate MFG-NE in a finite population setting. Hence we define
the following notions approximate FH-MFG and Stat-MFG.

Lemma A.3. Let ğœ‰ âˆˆ ğ‘†ğº (ğœ 2 ). Then
E [|ğœ‰ âˆ’ E [ğœ‰] |] â‰¤

âˆšï¸
2ğœ‹ğœ 2,

Definition A.5 (ğ›¿-FH-MFG-NE). Let (S, A, ğ», ğ‘ƒ, ğ‘…, ğœ‡0 ) be a FHMFG. Then, a ğ›¿-FH-MFG Nash equilibrium is defined as:



E (ğœ‰ âˆ’ E [ğœ‰]) 2 â‰¤ 4ğœ 2

Proof.

âˆ— ğ» âˆ’1
Policy ğœ‹ ğ›¿âˆ— = {ğœ‹ğ›¿,â„
}â„=0 âˆˆ Î ğ» such that

âˆ« âˆ
E [|ğœ‰ âˆ’ E [ğœ‰] |] =

P(|ğœ‰ âˆ’ E [ğœ‰] | â‰¥ ğ‘¡)ğ‘‘ğ‘¡

0

(ğ¼ )

âˆ« âˆ

â‰¤ 2

ğ‘’

2
âˆ’ ğ‘¡2
2ğœ

ğ‘‘ğ‘¡ =

âˆšï¸

ğ»
âˆ— ğ» âˆ’1
Eğ‘ƒ,ğ‘…
({ğœ‹ğ›¿,â„
}â„=0 ) â‰¤ ğ›¿.

2ğœ‹ğœ 2

Definition A.6 (ğ›¿-Stat-MFG-NE). Let (S, A, ğ‘ƒ, ğ‘…, ğ›¾) be a Stat-MFG.
A policy-population pair (ğœ‡ğ›¿âˆ— , ğœ‹ğ›¿âˆ— ) âˆˆ Î” S Ã— Î  is called a ğ›¿-Stat-MFG
Nash equilibrium if the two conditions hold:

0

Inequality (ğ¼ ) is true due to Lemma A.2. Likewise,
âˆ« âˆ


E (ğœ‰ âˆ’ E [ğœ‰]) 2 =
P((ğœ‰ âˆ’ E [ğœ‰]) 2 â‰¥ ğ‘¡)ğ‘‘ğ‘¡
0
âˆ« âˆ
âˆš
P(|ğœ‰ âˆ’ E [ğœ‰] | â‰¥ â„)ğ‘‘ğ‘¡
=
0
âˆ« âˆ
(ğ¼ ğ¼ )
âˆ’ â„
â‰¤ 2
ğ‘’ 2ğœ 2 ğ‘‘ğ‘¡ = 4ğœ 2

Stability:

ğœ‡ğ›¿âˆ— = Î“ğ‘ƒ (ğœ‡ğ›¿âˆ— , ğœ‹ğ›¿âˆ— ),
ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ğ›¿âˆ— , ğœ‹ğ›¿âˆ— ) â‰¥ max ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ğ›¿âˆ— , ğœ‹) âˆ’ ğ›¿.
ğ›¾

Optimality:

ğ›¾

ğœ‹ âˆˆÎ 

(ğ›¿-Stat-MFG-NE)

0

â–¡
Establishing lower bounds for the mean-field approximation
of the ğ‘ -player game will be more challenging as it will require
different tools. To establish lower bounds, we will need to use the
following anti-concentration result for the binomial distribution.
Lemma A.4 (Anti-concentration for binomial). Let ğ‘ âˆˆ N>0
and ğ‘‹ âˆ¼ Binom(ğ‘ , ğ‘) beh drawn from
a binomial distribution for
âˆš i
some ğ‘ âˆˆ [1/2, 1]. Then, P ğ‘‹ â‰¥ ğ‘2 + 2ğ‘

(ğ›¿-FH-MFG-NE)

1 .
â‰¥ 20

âˆš m
l
Ãğ‘
ğ‘ ğ‘˜
Proof. For ğ‘˜ 0 := ğ‘2 + 2ğ‘ , we will lower bound ğ‘˜=ğ‘˜
ğ‘ (1âˆ’
0 ğ‘˜
ğ‘
âˆ’ğ‘˜
ğ‘)
when ğ‘ is large enough. If ğ‘˜ 0 < âŒˆğ‘ ğ‘âŒ‰, then the probability in the statement above is bounded below trivially by 1/2 since
âŒŠğ‘ ğ‘âŒ‹ lower bounds the median of the binomial [18]. Otherwise, if
ğ‘˜ 0 â‰¥ âŒˆğ‘ ğ‘âŒ‰, then the function ğ‘ â†’ ğ‘ ğ‘˜ (1 âˆ’ ğ‘) ğ‘ âˆ’ğ‘˜ is increasing in
ğ‘ in the interval [0, ğ‘]. As 1/2 âˆˆh [0, ğ‘],âˆš it is then sufficient
to assume
âˆš i

ğ‘ = 1/2, and to upper bound P ğ‘2 âˆ’ 2ğ‘ < ğ‘‹ < ğ‘2 + 2ğ‘ by 9/10 as

the binomial probability mass is symmetric around ğ‘2 when ğ‘ = 1/2.

A.2

Upper Bound for FH-MFG: Extended Proof
of Theorem 3.2

Throughout this section we work with fixed ğ‘ƒ âˆˆ Pğ¾ğœ‡ and ğ‘… âˆˆ Rğ¿ğœ‡ .
For any X valued random variable ğ‘¥ denote L (ğ‘¥)(Â·) âˆˆ Î” X as the
distribution of ğ‘¥. We start by introducing some notation.
For given ğ‘… and ğ‘ƒ define the following constants:
ğ¿ğ‘  := sup ğ‘…(ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘…(ğ‘  â€², ğ‘, ğœ‡) ,
ğ‘ ,ğ‘  â€² ,ğ‘,ğœ‡

ğ¿ğ‘ := sup ğ‘…(ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘…(ğ‘ , ğ‘ â€², ğœ‡) ,
ğ‘ ,ğ‘,ğ‘ â€² ,ğœ‡

ğ¾ğ‘  := sup

ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘ƒ (Â·|ğ‘  â€², ğ‘, ğœ‡) ,

ğ‘ ,ğ‘  â€² ,ğ‘,ğœ‡

ğ¾ğ‘ := sup

ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘ƒ (Â·|ğ‘ , ğ‘ â€², ğœ‡) .

ğ‘ ,ğ‘,ğ‘ â€² ,ğœ‡

ğ‘… and ğ‘ƒ are bounded due to Definition 2.1, thus all constants
ğ¾ğ‘ , ğ¾ğ‘  , ğ¿ğ‘ , ğ¿ğ‘  are finite and well-defined, and it always holds that
ğ¾ğ‘  , ğ¾ğ‘ â‰¤ 2 and ğ¿ğ‘  , ğ¿ğ‘ â‰¤ 1. With the above definition of constants,
the more general Lipschitz condition holds: âˆ€ ğ‘ , ğ‘  â€² âˆˆ S, ğ‘, ğ‘ â€² âˆˆ A,

ğ» âˆ’1 = Î›ğ» (ğœ‡ , ğœ‹ ). Then for all â„ âˆˆ
be arbitrary and ğğœ‹ := {ğœ‡â„ğœ‹ }â„=0
ğ‘ƒ 0
{0, . . . , ğ» âˆ’ 1}, it holds that:
âˆšï¸‚
â„+1
â„âˆ’1
i 1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
h
ğœ‹
ğ¾ğ‘ âˆ‘ï¸ â„âˆ’ğ‘– âˆ’1
E âˆ¥b
ğœ‡â„ âˆ’ ğœ‡â„ğœ‹ âˆ¥ 1 â‰¤
|S|
+
ğ¿
Î”ğœ‹ğ‘– ,
1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
2ğ‘ 2ğ‘ ğ‘–=0 ğ‘ğ‘œğ‘,ğœ‡

ğœ‡, ğœ‡ â€² âˆˆ Î” S
âˆ¥ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘ƒ (Â·|ğ‘  â€², ğ‘ â€², ğœ‡ â€² )âˆ¥ 1 â‰¤ğ¾ğœ‡ âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 + ğ¾ğ‘  ğ‘‘ (ğ‘ , ğ‘  â€² )
+ ğ¾ğ‘ ğ‘‘ (ğ‘, ğ‘ â€² ),
â€²

â€²

â€²

|ğ‘…(ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘…(ğ‘  , ğ‘ , ğœ‡ )| â‰¤ğ¿ğœ‡ âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 + ğ¿ğ‘  ğ‘‘ (ğ‘ , ğ‘  â€² )
+ ğ¿ğ‘ ğ‘‘ (ğ‘, ğ‘ â€² ).

where Î”â„ := ğ‘1

We also introduce the shorthand notation for any ğ‘  âˆˆ S, ğ‘¢ âˆˆ
Î”A, ğœ‡ âˆˆ Î”S :
âˆ‘ï¸
ğ‘ƒ (Â·|ğ‘ , ğ‘¢, ğœ‡) :=
ğ‘¢ (ğ‘)ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡),
ğ‘âˆˆ A

âˆ‘ï¸

ğ‘…(ğ‘ , ğ‘¢, ğœ‡) :=

ğ‘¢ (ğ‘)ğ‘…(ğ‘ , ğ‘, ğœ‡).

Ã

ğ‘–
ğ‘– âˆ¥ğœ‹ â„ âˆ’ ğœ‹â„ âˆ¥ 1

Proof. The proof will proceed inductively over â„. First, for time
â„ = 0, we have
"
#
âˆšï¸‚
ğ‘
âˆ‘ï¸
1 âˆ‘ï¸
ğœ‹
E [âˆ¥b
ğœ‡0 âˆ’ ğœ‡0 âˆ¥ 1 ] =
E
(1 ğ‘–
âˆ’ ğœ‡ 0 (ğ‘ )) â‰¤ |S|
,
ğ‘ ğ‘–=1 {ğ‘ 0 =ğ‘  }
2ğ‘
ğ‘ âˆˆS

ğ‘âˆˆ A

By [36, Lemma C.1], it holds that
âˆ¥ğ‘ƒ (Â·|ğ‘ , ğ‘¢, ğœ‡) âˆ’ ğ‘ƒ (Â·|ğ‘  â€², ğ‘¢ â€², ğœ‡ â€² )âˆ¥ 1 â‰¤ğ¾ğœ‡ âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 + ğ¾ğ‘  ğ‘‘ (ğ‘ , ğ‘  â€² )
ğ¾ğ‘
+
âˆ¥ğ‘¢ âˆ’ ğ‘¢ â€² âˆ¥ 1,
2
|ğ‘…(ğ‘ , ğ‘¢, ğœ‡) âˆ’ ğ‘…(ğ‘  â€², ğ‘¢ â€², ğœ‡ â€² )| â‰¤ğ¿ğœ‡ âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 + ğ¿ğ‘  ğ‘‘ (ğ‘ , ğ‘  â€² )
ğ¿ğ‘
+
âˆ¥ğ‘¢ âˆ’ ğ‘¢ â€² âˆ¥ 1 .
(1)
2
We will define a new operator for tracking the evolution of the
population distribution over finite time horizons for a time-varying
ğ» âˆ’1 âˆˆ Î  :
policy âˆ€ğœ‹ = {ğœ‹â„ }â„=0
ğ»

where the last line is due to Lemma A.3 and the fact that 1 {ğ‘  ğ‘– =ğ‘  }
0
are bounded (hence subgaussian)
random
variables, and that in the
h
i

finite state space we have E 1 {ğ‘  ğ‘– =ğ‘  } = ğœ‡ 0 (ğ‘ ).
0
Next, denoting the ğœ-algebra induced by the random variables
({ğ‘ â„ğ‘– })ğ‘–,â„ â€² â‰¤â„ as Fâ„ , we have that:
h
i
ğœ‹
E âˆ¥b
ğœ‡â„+1 âˆ’ ğœ‡â„+1
âˆ¥ 1 |Fâ„
â‰¤ E [âˆ¥E [b
ğœ‡â„+1 |Fâ„ ] âˆ’ Î“ğ‘ƒ (b
ğœ‡â„ , ğœ‹ â„ )âˆ¥ 1 |Fâ„ ]
|
{z
}
(â–¡)

h
i
ğœ‹
+ E [âˆ¥b
ğœ‡â„+1 âˆ’ E [b
ğœ‡â„+1 |Fâ„ ] âˆ¥ 1 |Fâ„ ] + E âˆ¥Î“ğ‘ƒ (b
ğœ‡â„ , ğœ‹ â„ ) âˆ’ ğœ‡â„+1
âˆ¥ 1 |Fâ„
|
{z
} |
{z
}
(â–³)

Î“â„ğ‘ƒ (ğœ‡, ğœ‹ ) := Î“ğ‘ƒ (. . . Î“ğ‘ƒ (Î“ğ‘ƒ (ğœ‡, ğœ‹ 0 ), ğœ‹1 ) . . . , ğœ‹â„âˆ’1 )
|
{z
}

(â™¥)

(3)

â„ times

We upper bound the three terms separately. For (â–³), it holds that

= ğœ‡â„ğœ‹ = Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ )â„ ,
so Î“ğ‘ƒ0 (ğœ‡, ğœ‹ ) = ğœ‡ 0 . By repeated applications of Lemma 2.2, we obtain
the Lipschitz condition:
ğ‘› â€²
â€² ğ‘›âˆ’1
âˆ¥Î“ğ‘›ğ‘ƒ (ğœ‡, {ğœ‹ğ‘– }ğ‘›âˆ’1
ğ‘–=0 ) âˆ’ Î“ğ‘ƒ (ğœ‡ , {ğœ‹ğ‘– }ğ‘–=0 )âˆ¥ 1

ğ‘ âˆˆS

ğ‘›âˆ’2
ğ‘›âˆ’1 â€²
â€² ğ‘›âˆ’2
â‰¤ ğ¿ğ‘ğ‘œğ‘,ğœ‡ âˆ¥Î“ğ‘›âˆ’1
ğ‘ƒ (ğœ‡, {ğœ‹ğ‘– }ğ‘–=0 ) âˆ’ Î“ğ‘ƒ (ğœ‡ , {ğœ‹ğ‘– }ğ‘–=0 )âˆ¥ 1

ğ¾ğ‘
â€²
âˆ¥ğœ‹ğ‘›âˆ’1 âˆ’ ğœ‹ğ‘›âˆ’1
âˆ¥1
+
2
ğ‘›âˆ’1
ğ‘›
â‰¤ ğ¿ğ‘ğ‘œğ‘,ğœ‡
âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1 +

ğ¾ğ‘ âˆ‘ï¸ ğ‘›âˆ’1âˆ’ğ‘–
ğ¿
âˆ¥ğœ‹ğ‘– âˆ’ ğœ‹ğ‘–â€² âˆ¥ 1,
2 ğ‘–=0 ğ‘ğ‘œğ‘,ğœ‡

(â–³) =E [âˆ¥b
ğœ‡â„+1 âˆ’ E [b
ğœ‡â„+1 |Fâ„ ] âˆ¥ 1 |Fâ„ ]
âˆšï¸‚
âˆ‘ï¸
ğœ‹
=
E [|b
ğœ‡â„+1 (ğ‘ ) âˆ’ E [b
ğœ‡â„+1 (ğ‘ ) |Fâ„ ] | |Fâ„ ] â‰¤ |S|
,
2ğ‘

(2)

where ğ¿ğ‘ğ‘œğ‘,ğœ‡ = (ğ¾ğœ‡ + ğ¾2ğ‘  + ğ¾2ğ‘ ).
The proof will proceed in three steps:
â€¢ Step 1. Bounding the expected deviation of the empirical
population
distribution
from the mean-field distribution
i
h
E âˆ¥b
ğœ‡â„ âˆ’ ğœ‡â„ğœ‹ âˆ¥ 1 for any given policy ğœ‹ .
â€¢ Step 2. Bounding difference of ğ‘ agent value function
ğ»,ğ‘ ,(ğ‘– )
ğ» .
ğ½ğ‘ƒ,ğ‘…
and the infinite player value function ğ‘‰ğ‘ƒ,ğ‘…
â€¢ Step 3. Bounding the exploitability of an agent when each
of ğ‘ agents are playing the FH-MFG-NE policy.
Step 1: Empirical distribution bound. Due to its relevance
for a general connection between the FH-MFG and the ğ‘ -player
game, we state this result in the form of an explicit bound.
Lemma A.7. Suppose for the ğ‘ -FH-MFG (ğ‘ , S, A, ğ‘ , ğ‘ƒ, ğ‘…, ğ›¾), agents
ğ‘– = 1, . . . , ğ‘ follow policies ğœ‹ ğ‘– = {ğœ‹â„ğ‘– }â„ . Let ğœ‹ = {ğœ‹ â„ }â„ âˆˆ Î ğ»

since each b
ğœ‡â„+1 (ğ‘ ) is an average of independent subgaussian random variables given Fâ„ . Specifically, each indicator is bounded
1 {ğ‘ ğ‘– =ğ‘  } âˆˆ [0, 1] a.s. and therefore is sub-Gaussian with 1 {ğ‘ ğ‘– =ğ‘  } âˆˆ
â„+1
â„+1
ğ‘†ğº (1/4). Thus we get b
ğœ‡â„+1 (ğ‘ ) âˆˆ ğ‘†ğº (1/(4ğ‘ )) and apply bound on
expected value discussed in Appendix A.1.
Next, for (â–¡) = âˆ¥E [b
ğœ‡â„+1 |Fâ„ ] âˆ’ Î“ğ‘ƒ (b
ğœ‡â„ , ğœ‹ â„ )âˆ¥ 1 , we note that
"
#
ğ‘
ğ‘
1 âˆ‘ï¸
1 âˆ‘ï¸
E [b
ğœ‡â„+1 (ğ‘ ) |Fâ„ ] = E
ğ‘ƒ (ğ‘  |ğ‘ â„ğ‘– , ğœ‹â„ğ‘– (ğ‘ â„ğ‘– ), b
ğœ‡â„ ),
1 {ğ‘ ğ‘– =ğ‘  } |Fâ„ =
â„+1
ğ‘ ğ‘–=1
ğ‘ ğ‘–=1
therefore
ğ‘

(â–¡) =

âˆ‘ï¸
1 âˆ‘ï¸
ğœ‡â„ ) âˆ’
b
ğœ‡â„ (ğ‘  â€² )ğ‘ƒ (Â·|ğ‘  â€², ğœ‹â„ (Â·|ğ‘  â€² ), b
ğ‘ƒ (Â·|ğ‘ â„ğ‘– , ğœ‹â„ğ‘– (Â·|ğ‘ â„ğ‘– ), b
ğœ‡â„ )
ğ‘ ğ‘–=1
â€²
ğ‘ 

=

ğ‘
1 âˆ‘ï¸ 

ğ‘ ğ‘–=1
â‰¤

ğ‘
1 âˆ‘ï¸

ğ‘ ğ‘–=1

1

âˆ¥ğ‘ƒ (Â·|ğ‘ â„ğ‘– , ğœ‹â„ğ‘– (Â·|ğ‘ â„ğ‘– ), b
ğœ‡â„ ) âˆ’ ğ‘ƒ (Â·|ğ‘ â„ğ‘– , ğœ‹â„ (Â·|ğ‘ â„ğ‘– ), b
ğœ‡â„ ) âˆ¥ 1

ğ‘
(ğ¼ ) ğ¾ğ‘ âˆ‘ï¸

â‰¤

ğœ‡â„ ) âˆ’ ğ‘ƒ (Â·|ğ‘ â„ğ‘– , ğœ‹â„ (Â·|ğ‘ â„ğ‘– ), b
ğ‘ƒ (Â·|ğ‘ â„ğ‘– , ğœ‹â„ğ‘– (Â·|ğ‘ â„ğ‘– ), b
ğœ‡â„ )



2ğ‘ ğ‘–=1

âˆ¥ğœ‹â„ğ‘– (Â·|ğ‘ â„ğ‘– ) âˆ’ ğœ‹â„ (Â·|ğ‘ â„ğ‘– )âˆ¥ 1 â‰¤

ğ¾ğ‘
Î” ,
2 â„

1

where (I) follows from the Lipschitz property (1). Finally, the last
term (â™¥) can be bounded using:
h
i
(â™¥) =E âˆ¥Î“ğ‘ƒ (b
ğœ‡â„ , ğœ‹ â„ ) âˆ’ Î“ğ‘ƒ (ğœ‡â„ğœ‹ , ğœ‹ â„ )âˆ¥ 1 |Fâ„ â‰¤ ğ¿ğ‘ğ‘œğ‘,ğœ‡ âˆ¥b
ğœ‡â„ âˆ’ ğœ‡â„ğœ‹ âˆ¥ 1 .

so by induction Pâˆ (ğ‘ â„ = Â·) = ğœ‡â„ . Then we can conclude that
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ ) = Eâˆ

"ğ» âˆ’1
âˆ‘ï¸

#
ğ‘…(ğ‘ â„ , ğœ‹â„ (ğ‘ â„ ), ğœ‡â„ )

â„=0

To conclude, merging the bounds on the three terms in Inequality (3)
and taking the expectations we obtain:
âˆšï¸‚
h
i
h
i
ğœ‹
ğ¾ğ‘ Î”â„
ğœ‹
E âˆ¥b
ğœ‡â„+1 âˆ’ ğœ‡â„+1
+
.
âˆ¥ 1 â‰¤ ğ¿ğ‘ğ‘œğ‘,ğœ‡ E âˆ¥b
ğœ‡â„ âˆ’ ğœ‡â„ğœ‹ âˆ¥ 1 + |S|
2ğ‘
2

=

ğ»
âˆ’1 âˆ‘ï¸
âˆ‘ï¸

ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), ğœ‡â„ ).

â„=0 ğ‘  âˆˆ S

Merging the two equalities for ğ½, ğ‘‰ , we have the bound:
ğ»,ğ‘ ,(1)

â–¡
Step 2: Bounding difference of ğ‘ agent value function. Next,
we bound the difference between the ğ‘ -player expected reward
ğ»,ğ‘ ,(1)
function ğ½ğ‘ƒ,ğ‘…
and the infinite player expected reward function
ğ» . For ease of reading, expectations, probabilities, and laws of
ğ‘‰ğ‘ƒ,ğ‘…
random variables will be denoted Eâˆ, Pâˆ, Lâˆ respectively over the
infinite player finite horizon game and Eğ‘ , Pğ‘ , Lğ‘ respectively
over the ğ‘ -player game. We use the regular notation E[Â·], P[Â·], L (Â·)
without subscripts if the underlying randomness is clearly defined.
We state the main result of this step in the following lemma.

Lemma A.8. Suppose ğ‘ -FH-MFG agents follow the same sequence
ğ» âˆ’1 . Then
of policies ğœ‹ = {ğœ‹â„ }â„=0
ğ»,ğ‘ ,(1)

ğ½ğ‘ƒ,ğ‘…

ğ»
(ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ )|
"ğ» âˆ’1
# ğ» âˆ’1
âˆ‘ï¸ âˆ‘ï¸
âˆ‘ï¸ âˆ‘ï¸
= Eğ‘
b
ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), b
ğœ‡â„ ) âˆ’
ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), ğœ‡â„ )

|ğ½ğ‘ƒ,ğ‘…

Induction on â„ yields the statement of the lemma.

ğ»
(ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ )
âˆšï¸‚
â„+1
ğ» âˆ’1
ğ¿ğ‘ 
ğœ‹ âˆ‘ï¸ 1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
â‰¤ (ğ¿ğœ‡ + )|S|
.
2
2ğ‘
1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
â„=0

â„=0 ğ‘  âˆˆ S

â‰¤Eğ‘

â‰¤Eğ‘

â„=0 ğ‘  âˆˆ S

"ğ» âˆ’1
âˆ‘ï¸ âˆ‘ï¸ 

b
ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), b
ğœ‡â„ ) âˆ’ ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), ğœ‡â„ )

â„=0 ğ‘  âˆˆ S
"ğ» âˆ’1 
âˆ‘ï¸ ğ¿ğ‘ 

2

â„=0



#

#

âˆ¥ğœ‡â„ âˆ’ b
ğœ‡â„ âˆ¥ 1 + ğ¿ğœ‡ âˆ¥ğœ‡â„ âˆ’ b
ğœ‡â„ âˆ¥ 1

.

The statement of the lemma follows by an application of Lemma A.7.
â–¡
Step 3: Bounding difference in policy deviation. Finally, to
conclude the proof of the main theorem of this section, we will
prove that the improvement in expectation

 due to single-sided
policy changes are at most of order O âˆš1

.

ğ‘
ğ» âˆ’1 âˆˆ Î ğ» and ğœ‹ â€² = {ğœ‹ â€² }ğ» âˆ’1 âˆˆ
Lemma A.9. Suppose ğœ‹ = {ğœ‹â„ }â„=0
â„ â„=0
ğœ‹
ğ»
Î  arbitrary policies, and ğ := Î›ğ»
(ğœ‡
,
ğœ‹
)
is
the
population
distri0
ğ‘ƒ

bution induced by ğœ‹ . Then
Proof. Due to symmetry in the ğ‘ agent game, any permutation
ğœ : [ğ‘ ] â†’ [ğ‘ ] of agents does not change their distribution, that
ğœ (1)
ğœ (ğ‘ )
is Lğ‘ (ğ‘ â„1 , . . . , ğ‘ â„ğ‘ ) = Lğ‘ (ğ‘ â„ , . . . , ğ‘ â„
). We can then conclude
that:
ğ‘




1 âˆ‘ï¸
Eğ‘ ğ‘…(ğ‘ â„1 , ğ‘â„1 , b
ğœ‡â„ ) =
Eğ‘ ğ‘…(ğ‘ â„ğ‘– , ğ‘â„ğ‘– , b
ğœ‡â„ )
ğ‘ ğ‘–=1
"
#
âˆ‘ï¸
= Eğ‘
b
ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), b
ğœ‡â„ ).

Therefore, we by definition:
"ğ» âˆ’1
âˆ‘ï¸ âˆ‘ï¸

#
b
ğœ‡â„ (ğ‘ )ğ‘…(ğ‘ , ğœ‹â„ (ğ‘ ), b
ğœ‡â„ ) .

â„=0 ğ‘  âˆˆ S
ğ» âˆ’1 =
Next, in the FH-MFG, under the population distribution {ğœ‡â„ }â„=0
ğ»
Î›ğ‘ƒ (ğœ‡0, ğœ‹ ) we have that for all â„ âˆˆ 0, . . . , ğ» âˆ’ 1,

Pâˆ (ğ‘  0 = Â·) = ğœ‡ 0,
âˆ‘ï¸
Pâˆ (ğ‘ â„+1 = Â·) =
Pâˆ (ğ‘ â„ = ğ‘ ) Pâˆ (ğ‘ â„ = Â·|ğ‘ â„ = ğ‘ )
ğ‘ âˆˆS

= Î“ğ‘ƒ (Pâˆ (ğ‘ â„ = Â·), ğœ‹â„ ),

â‰¤

ğ»
â€²
(ğœ‹ â€², ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ), ğœ‹ )

ğ»
âˆ’1 
âˆ‘ï¸
â„=0

ğ‘ âˆˆS

ğ»,ğ‘ ,(1)
ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , . . . , ğœ‹ ) = Eğ‘

ğ»,ğ‘ ,(1)

ğ½ğ‘ƒ,ğ‘…


â„âˆ’1
âˆ‘ï¸ 


ğ¿ğœ‡ 
E âˆ¥b
ğœ‡â„ âˆ’ ğœ‡â„ğœ‹ âˆ¥ 1 + ğ¾ğœ‡
E âˆ¥b
ğœ‡â„ â€² âˆ’ ğœ‡â„ğœ‹â€² âˆ¥ 1 .
2
â€²
â„ =0

ğœ‡â„ }â„ as in the
Proof. Define the random variables {ğ‘ â„ğ‘– , ğ‘â„ğ‘– }ğ‘–,â„ , {b
definition of ğ‘ -FH-SAG (Definition 3.1). In addition, define the
random variables {ğ‘ â„ , ğ‘â„ }â„ evolving according to the FH-MFG with
population ğğœ‹ := {ğœ‡â„ğœ‹ }â„ := Î›ğ»
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ) and representative policy
â€²
ğœ‹ , independent from the random variables {ğ‘ â„ğ‘– , ğ‘â„ğ‘– }ğ‘–,â„ . Hence ğ‘  0 âˆ¼
ğœ‡ 0, ğ‘â„ âˆ¼ ğœ‹ â€² (Â·|ğ‘ â„ ), ğ‘ â„+1 âˆ¼ ğ‘ƒ (Â·|ğ‘ â„ , ğ‘â„ , ğœ‡â„ğœ‹ ). Define also for simplicity
ğ»,ğ‘ ,(1)

ğ¸ ğ‘ := ğ½ğ‘ƒ,ğ‘…

ğ»
â€²
(ğœ‹ â€², ğœ‹ , . . . , ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‡ 0ğœ‹ ), ğœ‹ ) .

With these definitions, we have
#
"ğ» âˆ’1
ğ»
âˆ’1
âˆ‘ï¸
âˆ‘ï¸
ğœ‹
1 1
ğ¸ğ‘ = E
ğ‘…(ğ‘ â„ , ğ‘â„ , ğœ‡â„ ) âˆ’
ğ‘…(ğ‘ â„ , ğ‘â„ , b
ğœ‡â„ )
â‰¤

â„=0
ğ»
âˆ’1
âˆ‘ï¸

â„=0


E ğ‘…(ğ‘ â„ , ğ‘â„ , ğœ‡â„ğœ‹ ) âˆ’ ğ‘…(ğ‘ â„1 , ğ‘â„1 , b
ğœ‡â„ ) .

â„=0



(4)

ğ»,ğ‘ ,(1)

ğ» and ğ½
The upper bounds on the deviation between ğ‘‰ğ‘ƒ,ğ‘…
from
ğ‘ƒ,ğ‘…
the previous steps directly yields the statement of the theorem. We
state it below for completeness.

Furthermore, for any â„ âˆˆ {0, . . . , ğ» âˆ’ 1},


| E ğ‘…(ğ‘ â„ , ğ‘â„ , ğœ‡â„ğœ‹ ) âˆ’ ğ‘…(ğ‘ â„1 , ğ‘â„1 , b
ğœ‡â„ ) |


â‰¤ E ğ‘…(ğ‘ â„ , ğ‘â„ , ğœ‡â„ğœ‹ ) âˆ’ ğ‘…(ğ‘ â„1 , ğ‘â„1 , ğœ‡â„ğœ‹ )


+ E ğ‘…(ğ‘ â„1 , ğ‘â„1 , ğœ‡â„ğœ‹ ) âˆ’ ğ‘…(ğ‘ â„1 , ğ‘â„1 , b
ğœ‡â„ )


â‰¤ E ğ‘…(ğ‘ â„ , ğœ‹â„â€² (ğ‘ â„ ), ğœ‡â„ğœ‹ ) âˆ’ ğ‘…(ğ‘ â„1 , ğœ‹â„â€² (ğ‘ â„1 ), ğœ‡â„ğœ‹ )


+ ğ¿ğœ‡ E âˆ¥ğœ‡â„ğœ‹ âˆ’ b
ğœ‡â„ âˆ¥ 1


1
ğœ‡â„ âˆ¥ 1 ,
â‰¤ âˆ¥ P[ğ‘ â„ = Â·] âˆ’ P[ğ‘ â„1 = Â·] âˆ¥ 1 + ğ¿ğœ‡ E âˆ¥ğœ‡â„ğœ‹ âˆ’ b
2
where the last line follows since ğ‘… is bounded in [0, 1]. Replacing
this in Equation (4),
âˆ‘ï¸ 

1 âˆ‘ï¸
âˆ¥ P[ğ‘ â„ = Â·] âˆ’ P[ğ‘ â„1 = Â·] âˆ¥ 1 + ğ¿ğœ‡
E âˆ¥ğœ‡â„ğœ‹ âˆ’ b
ğ¸ğ‘ â‰¤
ğœ‡â„ âˆ¥ 1 .
2
â„
â„
(5)

Theorem A.10. It holds that


ğ¶2
1
+
=ğ‘‚ ğ›¿ + âˆš
ğ‘
ğ‘
ğ‘

ğ¶1
ğ»,ğ‘ ,(1)
Eğ‘ƒ,ğ‘…
(ğœ‹ ğ›¿ , . . . , ğœ‹ ğ›¿ ) â‰¤ 2ğ›¿ + âˆš

where ğœ‹ ğ›¿ is a ğ›¿-FH-MFG Nash equilibrium and
!
âˆšï¸‚
â„+1
ğ‘–+1
ğ» âˆ’1
ğ»
âˆ’1 â„âˆ’1
âˆ‘ï¸
âˆ‘ï¸ 1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
ğœ‹
ğ¿ğ‘  âˆ‘ï¸ 1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
ğ¶ 1 = |S|
(2ğ¿ğœ‡ + )
+ ğ¾ğœ‡
2
2
1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡
ğ‘–=0
â„=0

ğ¶ 2 = ğ¿ğœ‡ ğ¾ğ‘

â„=0

â‰¤

âˆ‘ï¸

ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡) P[ğ‘ â„1 = ğ‘ , b
ğœ‡â„ = ğœ‡] âˆ’
ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡â„ğœ‹ ) P[ğ‘ â„1 = ğ‘ ] âˆ’

âˆ‘ï¸

ğ‘ 

1

ğ‘ 

âˆ‘ï¸ 



ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡) âˆ’ ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡â„ğœ‹ ) P[ğ‘ â„1 = ğ‘ , b
ğœ‡â„ = ğœ‡]

ğ‘ ,ğœ‡

1

â‰¤ P[ğ‘ â„1 = Â·] âˆ’ P[ğ‘ â„ = Â·] 1 +

âˆ‘ï¸

ğ¾ğœ‡ ğœ‡ âˆ’ ğœ‡â„ğœ‹ 1 P[ğ‘ â„1 = ğ‘ , b
ğœ‡â„ = ğœ‡]

ğ‘ ,ğœ‡

â‰¤

P[ğ‘ â„1 = Â·] âˆ’ P[ğ‘ â„ = Â·] 1 + ğ¾ğœ‡ E

h

b
ğœ‡â„ğœ‹ âˆ’ ğœ‡â„ğœ‹ 1

i

where the last two lines follow from the fact that ğ‘ƒ is ğ¾ğœ‡ Lipschitz in
ğœ‡ and stochastic matrices are non-expansive in the total-variation
norm over probability distributions. By induction, we conclude that
for all â„ â‰¥ 0, it holds that:
â„
h
i
âˆ‘ï¸
âˆ¥ P[ğ‘ â„ = Â·] âˆ’ P[ğ‘ â„1 = Â·] âˆ¥ 1 â‰¤ ğ¾ğœ‡
E b
ğœ‡â„ğœ‹â€² âˆ’ ğœ‡â„ğœ‹â€² 1 .
â„ â€² =0

Placing this result into Equation (5), we obtain the statement of
the lemma.
â–¡
h
i
ğœ‹
Since E âˆ¥b
ğœ‡â„ â€² âˆ’ ğœ‡â„ â€² âˆ¥ 1 above in the theorem is of the order of
âˆš
1
O ( / ğ‘ ) by the result in step 1, the result above allows us to bound
exploitability in the ğ‘ -FH-SAG.
Conclusion and Statement of Result. Finally, we can merge
the results up until this stage to upper bound the exploitability. By
definition of the FH-MFG-NE, we have:
ğ»
â€²
ğ»
ğ»
(Î›ğ»
ğ›¿ â‰¥ max ğ‘‰ğ‘ƒ,ğ‘…
ğ‘ƒ (ğœ‡ 0 , ğœ‹ ğ›¿ ), ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (Î›ğ‘ƒ (ğœ‡ 0 , ğœ‹ ğ›¿ ), ğœ‹ ğ›¿ )
ğœ‹ â€² âˆˆÎ ğ»

1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡

,

1âˆ’ğ¿ğ» +1

and with ğ» 2 1âˆ’ğ¿ğ‘ğ‘œğ‘,ğœ‡
if ğ¿ğ‘ğ‘œğ‘,ğœ‡ > 1.
ğ‘ğ‘œğ‘,ğœ‡

A.3

1

ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡â„ğœ‹ ) P[ğ‘ â„ = ğ‘ ]

â„=0 ğ‘–=0

A note on constants. Note that constants ğ¶ 1, ğ¶ 2 in Theorem A.10
2
depend on horizon with 1âˆ’ğ¿ğ»ğ‘ğ‘œğ‘,ğœ‡ if ğ¿ğ‘ğ‘œğ‘,ğœ‡ < 1, with ğ» 3 if ğ¿ğ‘ğ‘œğ‘,ğœ‡ = 1

ğ‘ƒ (ğ‘ , ğœ‹â„â€² (ğ‘ ), ğœ‡â„ğœ‹ ) P[ğ‘ â„ = ğ‘ ]

ğ‘ 

ğ‘ ,ğœ‡

+

âˆ‘ï¸

ğ‘–
ğ»
âˆ’1 â„âˆ’1
âˆ‘ï¸
âˆ‘ï¸ 1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡

1âˆ’ğ¿ğ‘˜

Lower Bound for FH-MFG: Extended Proof
of Theorem 3.3

1
âˆ¥ P[ğ‘ â„+1 = Â·] âˆ’ P[ğ‘ â„+1
= Â·] âˆ¥ 1

âˆ‘ï¸

1 âˆ’ ğ¿ğ‘ğ‘œğ‘,ğœ‡

+ ğ¾ğ‘ ğ¾ğœ‡

where we use shorthand notation 1âˆ’ğ¿ğ‘ğ‘œğ‘,ğœ‡
:= ğ‘˜ âˆ’ 1 when ğ¿ğ‘ğ‘œğ‘,ğœ‡ = 1.
ğ‘ğ‘œğ‘,ğœ‡

The first sum above we upper bound in the rest of the proof inductively.
Firstly, by definitions of ğ‘ -FH-SAG and FH-MFG, both ğ‘  01 and ğ‘  0
have distribution ğœ‡ 0 , hence âˆ¥ P[ğ‘  0 = Â·] âˆ’ P[ğ‘  01 = Â·] âˆ¥ 1 = 0. Assume
that â„ â‰¥ 1. We note that ğ‘ƒ takes values in Î” S and the random vector
Ã
b
ğœ‡â„ takes values in the discrete set { ğ‘1 ğ‘¢ : ğ‘¢ âˆˆ {0, . . . , ğ‘ } S , ğ‘  ğ‘¢ (ğ‘ ) =
ğ‘ } âŠ‚ Î” S , hence we have the bounds:

â‰¤

ğ»
âˆ’1 1 âˆ’ ğ¿â„
âˆ‘ï¸
ğ‘ğ‘œğ‘,ğœ‡

â„=0

The proof will be by construction: we will explicitly define an
FH-MFG where the optimal policy for the ğ‘ -agent game diverges
quickly from the FH-MFG-NE policy.
Preliminaries. We first define a few utility functions. Define
2
g : Î”2 â†’ ğµ âˆ,+
:= {x âˆˆ R2 : âˆ¥xâˆ¥ âˆ = 1, ğ‘¥ 1, ğ‘¥ 2 â‰¥ 0} and h : Î”2 â†’
2
[0, 1] as follows:
!


ğ‘¥1
g (ğ‘¥ , ğ‘¥ )
1 ,ğ‘¥ 2 } ,
g(ğ‘¥ 1, ğ‘¥ 2 ) := 1 1 2 := max{ğ‘¥
ğ‘¥2
g2 (ğ‘¥ 1, ğ‘¥ 2 )
max{ğ‘¥ 1 ,ğ‘¥ 2 }




h (ğ‘¥ , ğ‘¥ )
max{4ğ‘¥ 2, 1}
h(ğ‘¥ 1, ğ‘¥ 2 ) := 1 1 2 :=
.
h2 (ğ‘¥ 1, ğ‘¥ 2 )
max{4ğ‘¥ 1, 1}
Furthermore, for any ğœ– > 0 we define ğœ”ğœ– : [0, 1] â†’ [0, 1] as:
ï£±
ï£´
1, ğ‘¥ > 1/2 + ğœ–
ï£´
ï£²
ï£´
ğœ”ğœ– (ğ‘¥) = 0, ğ‘¥ < 1/2 âˆ’ ğœ–
.
ï£´
ï£´
ï£´ 1 + ğ‘¥ âˆ’ 1/2 , ğ‘¥ âˆˆ [1/2 âˆ’ ğœ–, 1/2 + ğœ–]
ï£³2
2ğœ–
1
ğœ– âˆˆ (0, /2) will be specified later.
It is straightforward to verify that g has an inverse in its domain
given by


ğ‘¥1
ğ‘¥2
2
.
g âˆ’1 (ğ‘¥ 1, ğ‘¥ 2 ) =
,
, âˆ€(ğ‘¥ 1, ğ‘¥ 2 ) âˆˆ ğµ âˆ,+
ğ‘¥1 + ğ‘¥2 ğ‘¥1 + ğ‘¥2
2 , y = (ğ‘¦ , ğ‘¦ ) âˆˆ ğµ 2
Furthermore, it holds for x = (ğ‘¥ 1, ğ‘¥ 2 ) âˆˆ ğµ âˆ,+
1 2
âˆ,+

âˆ¥g âˆ’1 (x) âˆ’ g âˆ’1 (y)âˆ¥ 1
ğ‘¦1
ğ‘¦2
ğ‘¥1
ğ‘¥2
âˆ’
+
âˆ’
ğ‘¥ 1 + ğ‘¥ 2 ğ‘¦1 + ğ‘¦2
ğ‘¥ 1 + ğ‘¥ 2 ğ‘¦1 + ğ‘¦2
ğ‘¥ 1 (ğ‘¦2 âˆ’ ğ‘¥ 2 ) + ğ‘¥ 2 (ğ‘¥ 1 âˆ’ ğ‘¦1 )
ğ‘¥ 2 (ğ‘¦1 âˆ’ ğ‘¥ 1 ) + ğ‘¥ 1 (ğ‘¥ 2 âˆ’ ğ‘¦2 )
=
+
(ğ‘¥ 1 + ğ‘¥ 2 )(ğ‘¦1 + ğ‘¦2 )
(ğ‘¥ 1 + ğ‘¥ 2 )(ğ‘¦1 + ğ‘¦2 )
â‰¤2âˆ¥x âˆ’ yâˆ¥ 1,
=

and likewise for u, v âˆˆ Î”2 , letting ğ‘¢ + := max{ğ‘¢ 1, ğ‘¢ 2 }, ğ‘£ + := max{ğ‘£ 1, ğ‘£ 2 },

ğ‘¢1 ğ‘£ 1
ğ‘¢2 ğ‘£ 2
âˆ’
+
âˆ’
ğ‘¢+ ğ‘£ +
ğ‘¢+ ğ‘£ +
ğ‘¢2ğ‘£ + âˆ’ ğ‘¢+ğ‘£ 2
ğ‘¢ 1 ğ‘£ + âˆ’ ğ‘£ 1ğ‘¢ +
+
â‰¤ 2âˆ¥u âˆ’ vâˆ¥ 1 .
=
ğ‘¢+ğ‘£ +
ğ‘¢+ğ‘£ +

âˆ¥g(u) âˆ’ g(v) âˆ¥ 1 =

This follows from considering cases and observation that ğ‘¢ + â‰¥ 1/2,
ğ‘£ + â‰¥ 1/2. Then for all u, v âˆˆ Î”2 , g, h have the bi-Lipschitz and
Lipschitz properties:
1
âˆ¥u âˆ’ vâˆ¥ 1 â‰¤ âˆ¥g(u) âˆ’ g(v)âˆ¥ 1 â‰¤ 2âˆ¥u âˆ’ vâˆ¥ 1,
2
âˆ¥h(u) âˆ’ h(v)âˆ¥ 1 â‰¤ 4âˆ¥u âˆ’ vâˆ¥ 1 .

(6)
(7)

Likewise, ğœ”ğœ– , being piecewise linear, also satisfies the Lipschitz
1 |ğ‘¥ âˆ’ ğ‘¦|, âˆ€ğ‘¥, ğ‘¦ âˆˆ [0, 1].
condition: |ğœ”ğœ– (ğ‘¥) âˆ’ ğœ”ğœ– (ğ‘¦)| â‰¤ 2ğœ–
Defining the FH-MFG. We take a particular FH-MFG with 6
states, 2 actions. Define the state-actions sets:
S = {ğ‘  Left, ğ‘  Right, ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB },

A = {ğ‘ A, ğ‘ B }.

Intuitively, the â€œmainâ€ states of the game are ğ‘  Left, ğ‘  Right and the 4
states ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB are dummy states that keep track of which
actions were taken by which percentage of players used to introduce
a dependency of the rewards on the distribution of agents over
actions as well as states. Define the initial probabilities ğœ‡ 0 by:
ğ 0 (ğ‘  Left ) = ğ 0 (ğ‘  Right ) = 1/2,
ğ 0 (ğ‘  LA ) = ğ 0 (ğ‘  RA ) = ğ 0 (ğ‘  RA ) = ğ 0 (ğ‘  RB ) = 0.

Finally, let ğ›¼, ğ›½ > 0 such that ğ›¼ + ğ›½ < 1 (to be also defined later).
The reward functions are defined for all ğœ‡ âˆˆ Î” S as follows:
ğ‘…(ğ‘  Left, ğ‘ A, ğœ‡) =ğ‘…(ğ‘  Left, ğ‘ B, ğœ‡) = 0,
ğ‘…(ğ‘  Right, ğ‘ A, ğœ‡) =ğ‘…(ğ‘  Right, ğ‘ B, ğœ‡) = 0,



ğ‘…(ğ‘  LA, ğ‘ A, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ), ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB )
ğ‘…(ğ‘  LB, ğ‘ A, ğœ‡)
+ ğ›¼h(ğœ‡ (ğ‘  LA ), ğœ‡ (ğ‘  LB ))



ğ‘…(ğ‘  LA, ğ‘ B, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ), ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB )
ğ‘…(ğ‘  LB, ğ‘ B, ğœ‡)
+ ğ›¼h(ğœ‡ (ğ‘  LA ), ğœ‡ (ğ‘  LB )) + ğ›½1



ğ‘…(ğ‘  RA, ğ‘ A, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ), ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB )
ğ‘…(ğ‘  RB, ğ‘ A, ğœ‡)
+ ğ›¼h(ğœ‡ (ğ‘  RA ), ğœ‡ (ğ‘  RB ))



ğ‘…(ğ‘  RA, ğ‘ B, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ), ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB )
ğ‘…(ğ‘  RB, ğ‘ B, ğœ‡)
+ ğ›¼h(ğœ‡ (ğ‘  RA ), ğœ‡ (ğ‘  RB )) + ğ›½1
Note that only at odd steps do the agents get a reward, and at
this step, it does not matter which action the agent plays, only the
state among {ğ‘  LA, ğ‘  LA, ğ‘  RA, ğ‘  RB } and the population distribution.
The parameters ğœ–, ğ›¼, ğ›½ of the above FH-MFG are â€œfreeâ€ parameters
to be specified later.
A minor remark. The arguments of g above will be with probability one in the set Î”2 at odd-numbered time steps, but to formally
satisfy the Lipschitz condition ğ‘… âˆˆ R 2 one can for instance replace g ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ), ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ) with g ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ) +
ğœ‡ (ğ‘  Left ), ğœ‡ (ğ‘  LA )+ğœ‡ (ğ‘  LB )+ğœ‡ (ğ‘  Right ) in the definitions, which will not
impact the analysis since at odd timesteps ğœ‡ (ğ‘  Right ) = ğœ‡ (ğ‘  Left ) = 0
for both the FH-MFG and ğ‘ -FH-SAG.
Note that with these definitions, ğ‘ƒ âˆˆ P1/2ğœ– , ğ‘… âˆˆ R 2 since only
âˆ€ ğ‘ , ğ‘  â€² âˆˆ S, ğ‘, ğ‘ â€² âˆˆ A, ğœ‡, ğœ‡ â€² âˆˆ Î” S , we have by the definitions:
1
âˆ¥ğ‘ƒ (Â·|ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘ƒ (Â·|ğ‘  â€², ğ‘ â€², ğœ‡ â€² )âˆ¥ 1 â‰¤ 2ğ‘‘ (ğ‘ , ğ‘  â€² ) + 2ğ‘‘ (ğ‘, ğ‘ â€² ) + âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1,
2ğœ–
(8)
|ğ‘…(ğ‘ , ğ‘, ğœ‡) âˆ’ ğ‘…(ğ‘  â€², ğ‘ â€², ğœ‡ â€² )| â‰¤ ğ‘‘ (ğ‘ , ğ‘  â€² ) + ğ‘‘ (ğ‘, ğ‘ â€² ) + 2âˆ¥ğœ‡ âˆ’ ğœ‡ â€² âˆ¥ 1,

When at the states ğ‘  Left, ğ‘  Right , the transition probabilities are defined for all ğœ‡ âˆˆ Î” S by:
ğ‘ƒ (ğ‘  LA |ğ‘  Left, ğ‘ A, ğœ‡) = 1,

ğ‘ƒ (ğ‘  LB |ğ‘  Left, ğ‘ B, ğœ‡) = 1,

ğ‘ƒ (ğ‘  RA |ğ‘  Right, ğ‘ A, ğœ‡) = 1,

ğ‘ƒ (ğ‘  RB |ğ‘  Right, ğ‘ B, ğœ‡) = 1.

(9)
for any ğ›¼, ğ›½ > 0 with ğ›¼ + ğ›½ < 1 and ğ›¼ < 41 , using the Lipschitz
conditions in (6), (7).
Step 1: Solution of the FH-MFG. Next, we solve the infinite
âˆ— := {ğœ‹ âˆ— }ğ» âˆ’1 given by:
player FH-MFG and show that the policy ğœ‹ ğ»
â„ â„=0
ï£±
ï£´
1, if â„ odd and ğ‘ = ğ‘ B
ï£´
ï£²
ï£´

That is, the agent transitions to one of {ğ‘  LA, ğ‘  RA, ğ‘  RB, ğ‘  LB } to remember its last action and left-right state. When at states {ğ‘  LA, ğ‘  RA, ğ‘  RB, ğ‘  LB },
the transition probabilities are:

ğœ‹â„âˆ— (ğ‘|ğ‘ ) := 21 , if â„ even
ï£´

ï£´
ï£´ 0, if â„ odd and ğ‘ = ğ‘ B
ï£³
It is easy to verify in this case that, if ğ âˆ— := {ğœ‡â„âˆ— }â„ is induced by ğœ‹ âˆ— :
ğœ‡â„âˆ— (ğ‘  LA ) = ğœ‡â„âˆ— (ğ‘  LB ) = ğœ‡â„âˆ— (ğ‘  RA ) = ğœ‡â„âˆ— (ğ‘  RB ) = 1/4, if â„ odd,

If ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB } :
(
ğœ”ğœ– (ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB )), if ğ‘  â€² = ğ‘  Left
â€²
ğ‘ƒ (ğ‘  |ğ‘ , ğ‘, ğœ‡) =
ğœ”ğœ– (ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB )), if ğ‘  â€² = ğ‘  Right

ğœ‡â„âˆ— (ğ‘  Left ) = ğœ‡â„âˆ— (ğ‘  Right ) = 1/2, if â„ even.
, âˆ€ğœ‡, ğ‘.

The other non-defined transition probabilities are of course 0.

In this case, the induced rewards in odd steps are state-independent
(it is the same for all states ğ‘  RA, ğ‘  RB, ğ‘  LA, ğ‘  LB ), therefore the policy
ğœ‹ âˆ— is the optimal best response to the population and a FH-MFG.
In fact, ğœ‹ âˆ— is unique up to modifications in zero-probability sets
(e.g., modifying ğœ‹â„âˆ— (ğ‘  Left ) for odd â„, for which P[ğ‘ â„ = ğ‘  Left ] = 0). To

see this, for any policy ğœ‹ âˆˆ Î ğ» , it holds that

Otherwise, if ğœ”ğœ– (ğ‘) âˆˆ (0, 1),

ğœ‡â„ğœ‹ (ğ‘  Left ) = ğœ‡â„ğœ‹ (ğ‘  Right ) = 1/2, if â„ even,
ğœ‹
ğœ‡â„ (ğ‘  LA ) + ğœ‡â„ğœ‹ (ğ‘  LB ) = ğœ‡â„ğœ‹ (ğ‘  RA ) + ğœ‡â„ğœ‹ (ğ‘  RB ) = 1/2, if â„ odd,
as the action of the agent does not affect transition probabilities
between ğ‘  Left, ğ‘  Right in even rounds. Moreover, as odd stages, the action rewards terms only depend on the state apart from the positive
additional term ğ›½1, so the only optimal action will be ğ‘ B . Finally,
for ğ›¼ > 0, the actions ğ‘ A, ğ‘ B must be played with equal probability
as otherwise the term ğ›¼h(ğœ‡ (ğ‘  RA ), ğœ‡ (ğ‘  RB )) will lead to the action
with lower probability assigned by being optimal.
Step 2: Population divergence in ğ‘ -FH-MFG. We will analyze the empirical population distribution deviation from ğ âˆ— , namely,
we will lower bound E[âˆ¥ğœ‡â„âˆ— âˆ’ b
ğœ‡â„ âˆ¥ 1 ]. The results in this step will be
1
valid for any policy profile (ğœ‹ , . . . , ğœ‹ ğ‘ ) âˆˆ Î : we emphasize that at
even â„, b
ğœ‡â„ is independent of agent policies in the ğ‘ player game.
In this step, we also fix 1/2ğœ€ = 8.
We will analyze b
ğœ‡â„ at all even steps â„ = 2ğ‘š where ğ‘š âˆˆ N â‰¥0 .
Define the sequence of random variables for all ğ‘š âˆˆ N â‰¥0 as ğ‘‹ğ‘š :=
b
ğœ‡ 2ğ‘š (ğ‘  Left ). Define G := { ğ‘ğ‘˜ : ğ‘˜ = 0, . . . , ğ‘ }. Note that for all even
â„ = 2ğ‘š, it holds almost surely that b
ğœ‡â„ (ğ‘  Left ), b
ğœ‡â„ (ğ‘  Right ) âˆˆ G. By the
definition of the MFG, it holds for any ğ‘š â‰¥ 0, ğ‘˜ âˆˆ [ğ‘ ] that
 
ğ‘ âˆ’ğ‘
P[ğ‘ ğ‘‹ 0 = ğ‘˜] =
2 ,
ğ‘˜
 
ğ‘
(ğœ”ğœ€ (ğ‘‹ğ‘š ))ğ‘˜ (1 âˆ’ ğœ”ğœ€ (ğ‘‹ğ‘š ))ğ‘˜ ,
P[ğ‘ ğ‘‹ğ‘š+1 = ğ‘˜ |ğ‘‹ğ‘š ] =
ğ‘˜
that is, given ğ‘‹ğ‘š , ğ‘ ğ‘‹ğ‘š+1 is binomially distributed with ğ‘ ğ‘‹ğ‘š+1 âˆ¼
Binom(ğ‘ , ğœ”ğœ– (ğ‘‹ğ‘š )) without any dependence on the actions played
by agents. Therefore
E [ğ‘‹ğ‘š+1 |ğ‘‹ğ‘š ] = ğœ”ğœ– (ğ‘‹ğ‘š ),

Var[ğ‘‹ğ‘š+1 |ğ‘‹ğ‘š ] â‰¤

1
.
4ğ‘

We define the following set Gâˆ— := {0, 1} âŠ‚ G. By the definition of
the mechanics, if ğ‘¥ âˆˆ Gâˆ—, ğ‘š âˆˆ N â‰¥0 , it holds for all ğ‘š â€² > ğ‘š that
P[ğ‘‹ğ‘š â€² = ğ‘‹ğ‘š |ğ‘‹ğ‘š = ğ‘¥] = 1, that is once the Markovian random
process
âˆš ğ‘‹ğ‘š hits Gâˆ— , it will remain in Gâˆ— . Furthermore, for ğ¾ :=
âŒŠlog5 ğ‘ âŒ‹, and for ğ‘˜ = 0, . . . , ğ¾ define the level sets:
(
)
1
5ğ‘˜
Gâˆ’1 := G, Gğ‘˜ := ğ‘¥ âˆˆ G : ğ‘¥ âˆ’ â‰¥ âˆš
.
2
2 ğ‘
For all ğ‘˜ â‰¥ ğ¾, define Gğ‘˜ := Gâˆ— .
Firstly, we have that
"
#
1 âˆ‘ï¸
1
1
P[ğ‘‹ 0 âˆˆ G0 ] = P
1 ğ‘–
âˆ’ â‰¥ âˆš
ğ‘ ğ‘– {ğ‘ 0 =ğ‘ Left } 2
2 ğ‘
"
âˆš #
âˆ‘ï¸
ğ‘
ğ‘
1
=P
1 {ğ‘ 0ğ‘– =ğ‘ Left } âˆ’ â‰¥
â‰¥ ,
2
2
10
ğ‘–
where in the last line we applied the anti-concentration result of
Lemma A.4 on the sum of independent Bernoulli random variables
1 {ğ‘ 0ğ‘– =ğ‘ Left } for ğ‘– âˆˆ [ğ‘ ].
Next, assume that for some ğ‘š âˆˆ 1, . . . , ğ¾ âˆ’ 1 we have ğ‘ âˆˆ Gğ‘š . If
ğœ”ğœ– (ğ‘) âˆˆ {0, 1}, it holds trivially that P[ğ‘‹ğ‘š+1 âˆˆ Gğ‘š+1 |ğ‘‹ğ‘š = ğ‘] = 1.

P[ğ‘‹ğ‘š+1 âˆˆ Gğ‘š+1 |ğ‘‹ğ‘š = ğ‘]


5ğ‘š+1
1
ğ‘‹ğ‘š = ğ‘
= P |ğ‘‹ğ‘š+1 âˆ’ | â‰¥ âˆš
2
2 ğ‘


1
5ğ‘š+1
â‰¥ P |ğœ”ğœ– (ğ‘) âˆ’ | âˆ’ |ğ‘‹ğ‘š+1 âˆ’ ğœ”ğœ– (ğ‘)| â‰¥ âˆš ğ‘‹ğ‘š = ğ‘ .
2
2 ğ‘
Since in this case |ğœ”ğœ– (ğ‘‹ğ‘š ) âˆ’ 12 | = |ğœ”ğœ– (ğ‘‹ğ‘š ) âˆ’ ğœ”ğœ– ( 12 )| â‰¥ 1/2ğœ– |ğ‘‹ğ‘š âˆ’
ğœ”ğœ– ( 21 )|, we have
P[ğ‘‹ğ‘š+1 âˆˆ Gğ‘š+1 |ğ‘‹ğ‘š = ğ‘]


5ğ‘š+1
1
â‰¥ P |ğœ”ğœ– (ğ‘) âˆ’ | âˆ’ |ğ‘‹ğ‘š+1 âˆ’ ğœ”ğœ– (ğ‘)| â‰¥ âˆš ğ‘‹ğ‘š = ğ‘
2
2 ğ‘


5ğ‘š+1
1
= P ğ‘‹ğ‘š+1 âˆ’ ğœ”ğœ– (ğ‘) â‰¤ ğœ”ğœ– (ğ‘) âˆ’ âˆ’ âˆš ğ‘‹ğ‘š = ğ‘
2
2 ğ‘


5ğ‘š
5ğ‘š+1
â‰¥ P ğ‘‹ğ‘š+1 âˆ’ ğœ”ğœ– (ğ‘) â‰¤ 8 âˆš âˆ’ âˆš ğ‘‹ğ‘š = ğ‘
2 ğ‘
2 ğ‘


5ğ‘š
= P ğ‘‹ğ‘š+1 âˆ’ ğœ”ğœ– (ğ‘) â‰¤ 3 âˆš ğ‘‹ğ‘š = ğ‘
2 ğ‘


9 ğ‘š+1
â‰¥1 âˆ’ 2 exp âˆ’ 25
50
where in the last line we invoked the Hoeffding concentration
bound (Lemma A.2).
Using the above result inductively for ğ‘š âˆˆ 0, . . . , ğ¾ it holds that
P[ğ‘‹ğ‘š âˆˆ Gğ‘š |ğ‘‹ 0 âˆˆ G0 ] â‰¥

ğ‘š
Ã–

P[ğ‘‹ğ‘š â€² âˆˆ Gğ‘š â€² |ğ‘‹ğ‘š â€² âˆ’1 âˆˆ Gğ‘š â€² âˆ’1 ]

ğ‘š â€² =1
ğ‘š 
Ã–



â€²
9
1 âˆ’ 2 exp âˆ’ 25ğ‘š
50
ğ‘š â€² =1
!

âˆ
âˆ‘ï¸
9 ğ‘š â€² +1
â‰¥ 1âˆ’2
exp âˆ’ 25
50
ğ‘š â€² =0

!
âˆ
âˆ‘ï¸
9 â€² 9
â‰¥ 1âˆ’2
exp âˆ’ ğ‘š âˆ’
2
2
ğ‘š â€² =0
!
9
2ğ‘’ âˆ’ /2
9
â‰¥ 1âˆ’
â‰¥
.
10
1 âˆ’ ğ‘’ âˆ’ 9/2
â‰¥

Since for ğ‘˜ > ğ¾, P[ğ‘‹ğ‘˜+1 âˆˆ Gâˆ— |ğ‘‹ğ‘˜ âˆˆ Gâˆ— ] = 1 and P[ğ‘‹ 0 âˆˆ G0 ] â‰¥ 1/10,
it also holds that
P[ğ‘‹ğ‘š âˆˆ Gğ‘š , âˆ€ğ‘š â‰¥ 0] â‰¥

9
.
100

Finally, we use the above lower bound on the probability to lower
bound the expectation:


E [âˆ¥b
ğœ‡2ğ‘š âˆ’ ğœ‡ 2ğ‘š âˆ¥ 1 ] â‰¥ P[ğ‘‹ğ‘š âˆˆ Gğ‘š ] E âˆ¥b
ğœ‡ 2ğ‘š âˆ’ ğœ‡ 2ğ‘š âˆ¥ 1 |ğ‘‹ğ‘š âˆˆ Gğ‘š
â‰¥ P[ğ‘‹ğ‘š âˆˆ Gğ‘š ] E [2|ğ‘‹ğ‘š âˆ’ 1/2 | |ğ‘‹ğ‘š âˆˆ Gğ‘š ]
 ğ‘š 
9
5
â‰¥
min âˆš , 1 .
100
ğ‘

For odd â„ = 2ğ‘š + 1, we also have the inequality

(Lemma A.4) it holds that:

E [âˆ¥b
ğœ‡ 2ğ‘š+1 âˆ’ ğœ‡2ğ‘š+1 âˆ¥ 1 ] â‰¥ E [âˆ¥b
ğœ‡ 2ğ‘š âˆ’ ğœ‡ 2ğ‘š âˆ¥ 1 ]
 ğ‘š 
5
9
min âˆš , 1 .
â‰¥
100
ğ‘

ğ‘‡âˆ’1 â‰¤ sup E[ğœ |ğ‘‹ 0 = ğ‘¥]
ğ‘¥ âˆˆ Gâˆ’1

â‰¤ E[ğœ |ğ‘¥ âˆˆ G0, ğ‘‹ 0 = ğ‘¥] P[ğ‘¥ âˆˆ G0 |ğ‘‹ 0 = ğ‘¥]
+ E[ğœ |ğ‘¥ âˆ‰ G0, ğ‘‹ 0 = ğ‘¥] P[ğ‘¥ âˆ‰ G0 |ğ‘‹ 0 = ğ‘¥]
19
1
â‰¤ (ğ‘‡0 + 1) + (ğ‘‡âˆ’1 + 1),
20
20

which completes the first statement of the theorem (as 5ğ» /2 =
Î©(2ğ» )).
Step 3: Hitting time for Gâˆ— . We will show that the empirical
distribution of agent states almost always concentrates on one of
ğ‘  Left, ğ‘  Right during the even rounds in the ğ‘ -player game, and bound
the expected waiting time for this to happen. The distributions of
agents over states ğ‘  Left, ğ‘  Right in the even rounds are policy independent (they are not affected by which actions are played): hence
the results from Step 2 still hold for the population distribution and
the expected time computed in this step will be valid for any policy.
For simplicity, we define the FH-MFG for the non-terminating
infinite horizon chain, and we will compute value functions up to
horizon ğ» . Define the (random) hitting time ğœ as follows:

the last line following since ğ‘‡âˆ’1 > ğ‘‡0 by definition. Solving the two
inequalities, we obtain
ğ‘‡ğœ â‰¤ ğ‘‡âˆ’1 â‰¤

200 10ğ¾
5
+
â‰¤ 23 + log5 ğ‘ .
9
9
9

Step 4: Ergodic optimal response to ğ‘ -players. Next, we
ğ» âˆ’1 âˆˆ Î ğ» that is ergodically optimal
formulate a policy ğœ‹ br = {ğœ‹â„br }â„=0
for the ğ‘ -player game and can exploit a population that deploys
the unique FH-MFG-NE. For all â„, the optimal policy will be defined
by:

ğœ := inf {ğ‘š â‰¥ 0 : b
ğœ‡ 2ğ‘š (ğ‘  Left ) âˆˆ Gâˆ— } = inf {ğ‘š â‰¥ 0 : ğ‘‹ğ‘š âˆˆ Gâˆ— }.
Note that for any ğ‘ âˆˆ G, it holds that P[ğ‘‹ğ‘š+1 âˆˆ Gâˆ— |ğ‘‹ğ‘š = ğ‘] =
b
ğœ‡ 2ğ‘š (ğ‘  Left ) ğ‘ + b
ğœ‡ 2ğ‘š (ğ‘  Right ) ğ‘ = ğ‘ ğ‘ + (1 âˆ’ ğ‘) ğ‘ â‰¥ 2 âˆ’ğ‘ . Therefore

ğ‘šâˆ’1
for all ğ‘š it holds that P[b
ğœ‡ 2ğ‘š âˆ‰ Gâˆ— ] â‰¤ 1 âˆ’ 2 âˆ’ğ‘
. By the BorelCantelli lemma, we can conclude that ğœ < âˆ almost surely, and in
particular ğ‘‡ğœ := E[ğœ |ğ‘‹ 0 = ğ‘¥] < âˆ for any ğ‘¥ âˆˆ G.
Next, we compute the expected value ğ‘‡ğœ . Define the following
two quantities:
ğ‘‡âˆ’1 := sup {E[ğœ |ğ‘‹ 0 = ğ‘¥]}
ğ‘¥ âˆˆ Gâˆ’1

ğ‘‡0 := sup {E[ğœ |ğ‘‹ 0 = ğ‘¥]}.

ğœ‹â„br (ğ‘|ğ‘ ) =

ğ»,ğ‘ ,(1)

ğ½ğ‘ƒ,ğ‘…

ğ¸ 0 :=

Ã™

{ğ‘‹ğ‘š â€² âˆˆ Gğ‘š â€² }.

ğ‘š â€² âˆˆ [ğ¾ ]

Then, ğ‘‡0 is upper bounded by:

Right

B

ï£´
1, if ğ‘  âˆ‰ {ğ‘  Left, ğ‘  Right }, ğ‘ = ğ‘ B
ï£´
ï£´
ï£´
ï£´ 0, otherwise
ï£³

Intuitively, ğœ‹â„br becomes optimal once all the agents are concentrated in the same states during the even rounds, which happens
very quickly as shown in Step 3. Assume that agents ğ‘– = 2, . . . ğ‘ deploy the unique FH-MFG-NE ğœ‹ ğ‘– = ğœ‹ âˆ— , and for agent ğ‘– = 1, ğœ‹ 1 = ğœ‹ br .
We decompose the three components of the rewards for the first
agent, as defined in the construction of the MFG (Step 1):

ğ‘¥ âˆˆ G0

First, we compute an upper bound for ğ‘‡0 . Define the event:

ï£±
1, if ğ‘  = ğ‘  Left , ğ‘ = ğ‘ A
ï£´
ï£´
ï£´
ï£´
ï£² 1, if ğ‘  = ğ‘ 
ï£´
,ğ‘ =ğ‘

(ğœ‹ br, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— )
ï£®
ï£¹
ï£¯ âˆ‘ï¸
ï£º
ï£¯
ï£º
1,g
1,h
= E ï£¯ï£¯
(1 âˆ’ ğ›¼ âˆ’ ğ›½)ğ‘…â„ + ğ›¼ğ‘…â„ + ğ›½ 1 {ğ‘1 =ğ‘ğµ } ï£ºï£º
â„
ï£¯ â„ odd
ï£º
ï£¯0â‰¤â„â‰¤ğ»
ï£º
ï£°
ï£»
" ğ» âˆ’1
#


âˆ‘ï¸ 1,g
ğ»
â‰¥ (1 âˆ’ ğ›¼ âˆ’ ğ›½)E
ğ‘…â„ + ğ›½
2
odd â„=0

ğ‘‡0 = sup E[ğœ |ğ‘‹ 0 = ğ‘¥]
ğ‘¥ âˆˆ G0

= sup E[ğœ |ğ¸ 0, ğ‘‹ 0 = ğ‘¥] P[ğ¸ 0 |ğ‘‹ 0 = ğ‘¥]
ğ‘¥ âˆˆ G0

+ E[ğœ |ğ¸ğ‘0 , ğ‘‹ 0 = ğ‘¥] P[ğ¸ğ‘0 |ğ‘‹ 0 = ğ‘¥]
â‰¤ sup E[ğœ |ğ¸ 0, ğ‘‹ 0 = ğ‘¥] P[ğ¸ 0 |ğ‘‹ 0 = ğ‘¥]

h
i
as by definition clearly E 1 {ğ‘1 =ğ‘ğµ } = 1 for all odd â„ and ğ‘…â„h â‰¥ 0
â„
almost surely.
1,g
We analyze the terms ğ‘…â„ when the first agent follows ğœ‹ br . By
the definition of the dynamics and ğœ‹ br , it holds that

ğ‘¥ âˆˆ G0

+ E[ğœ |ğ¸ğ‘0 , ğ‘‹ 0 = ğ‘¥] P[ğ¸ğ‘0 |ğ‘‹ 0 = ğ‘¥]
9
1
ğ‘‡âˆ’1
+ (ğ¾ + ğ‘‡âˆ’1 )
=ğ¾+
â‰¤ğ¾
10
10
10
where in the last step we used the lower bound on P[ğ¸ 0 ] from Step
2. Similarly for ğ‘‡âˆ’1 , from the one-sided anti-concentration bound

1,g

1
1
ğ‘…â„ = ğ‘”1 (b
ğœ‡â„âˆ’1 (ğ‘ â„âˆ’1
), b
ğœ‡â„âˆ’1 (ğ‘  â„âˆ’1
))
1
1
1
1
where ğ‘  â„âˆ’1
:= ğ‘  Left if ğ‘ â„âˆ’1
= ğ‘  Right and ğ‘  â„âˆ’1
:= ğ‘  Right if ğ‘ â„âˆ’1
= ğ‘  Left .
1
ğ‘
As P[ğ‘ â„âˆ’1 = Â·, . . . , ğ‘ â„âˆ’1 = Â·] at even step â„ âˆ’ 1 is permutation
1
invariant, it holds that P[ğ‘ â„âˆ’1
= Â·|b
ğœ‡â„âˆ’1 = ğœ‡] = ğœ‡ (Â·) for any ğœ‡ âˆˆ G.

g

Therefore,

g

yielding ğ‘…â„ = 1 and ğ‘…â„ = 0 respectively almost surely. As before,

1,g

E[ğ‘…â„ ] =

âˆ‘ï¸

1
P[b
ğœ‡â„âˆ’1 = ğœ‡] P[ğ‘ â„âˆ’1
= ğ‘  |b
ğœ‡â„âˆ’1 = ğœ‡]

ğœ‡âˆˆG
ğ‘  âˆˆ {ğ‘  Left ,ğ‘  Right }
1,g

1
E[ğ‘…â„ |ğ‘ â„âˆ’1
= ğ‘ , b
ğœ‡â„âˆ’1 = ğœ‡]
âˆ‘ï¸
=
P[b
ğœ‡â„âˆ’1 = ğœ‡]ğœ‡ (ğ‘ )ğ‘”1 (ğœ‡ (ğ‘ ), ğœ‡ (ğ‘ )) â‰¥ 1/2,
ğœ‡âˆˆG
ğ‘  âˆˆ {ğ‘  Left ,ğ‘  Right }

as for any ğœ‡, if ğ‘  is such that ğœ‡ (ğ‘ ) â‰¥ ğœ‡ (ğ‘ ) then ğ‘”1 (ğœ‡ (ğ‘ ), ğœ‡ (ğ‘ )) = 1.
Furthermore,
by thei definition
of the hitting time
h
h
i ğœ, for any odd
g

g

â„ â‰¥ 1, E ğ‘…â„ |2ğœ < â„ = E ğ‘…â„ |b
ğœ‡â„âˆ’1 (ğ‘  Left ) âˆˆ Gâˆ— = 1, as after time

g
2ğœ the action ğ‘ A will be optimal with reward ğ‘…â„ = 1 almost surely,
as ğœ‹ ğ‘ğ‘Ÿ chooses action ğ‘ A at even steps.
g
Finally, using the lower bound of 1/2 for ğ‘…â„ when â„ < 2ğœ and that
g
ğ‘…â„ = 1 when â„ > 2ğœ, we obtain:

ï£®
ï£¹
ï£®
ï£¹
ï£¯ âˆ‘ï¸
ï£º
ï£¯
ï£º
âˆ‘ï¸
âˆ‘ï¸
ï£¯
ï£º
ï£¯
g
1,g
1,g ï£º
E ï£¯ï£¯
ğ‘…â„ ï£ºï£º =E ï£¯ï£¯
ğ‘…â„ +
ğ‘…â„ ï£ºï£º
ï£¯ â„ odd
ï£º
ï£¯
ï£º
â„ odd
â„ odd
ï£¯0â‰¤â„â‰¤ğ» ï£º
ï£¯0â‰¤â„â‰¤min{2ğœ,ğ»
ï£º
}
min{2ğœ,ğ» }+1â‰¤â„<ğ»
ï£°
ï£»
ï£°
ï£»

    
   
ğ»
ğ»
ğ»
1
â‰¥E
min ğœ,
+
âˆ’ min ğœ,
2
2
2
2
 

   
ğ»
1
ğ»
â‰¥
âˆ’ E min ğœ,
2
2
2
 
 
ğ»
1
ğ»
ğ‘‡ğœ
â‰¥
âˆ’ E [ğœ] =
âˆ’
2
2
2
2

ï£®
ï£¹
ï£®
ï£¹
ï£¯ âˆ‘ï¸
ï£º
ï£¯
ï£º
âˆ‘ï¸
âˆ‘ï¸
ï£¯
ï£º
ï£¯
g
1,g
1,g ï£º
E ï£¯ï£¯
ğ‘…â„ ï£ºï£º =E ï£¯ï£¯
ğ‘…â„ +
ğ‘…â„ ï£ºï£º
ï£¯ â„ odd
ï£º
ï£¯
ï£º
â„ odd
â„ odd
ï£¯0â‰¤â„â‰¤ğ» ï£º
ï£¯0â‰¤â„â‰¤min{2ğœ,ğ»
ï£º
}
min{2ğœ,ğ» }+1â‰¤â„<ğ»
ï£°
ï£»
ï£°
ï£»
 
    

  
1 ğ»
ğ»
ğ»
+
âˆ’ min ğœ,
â‰¤E min ğœ,
2
2
2
2
 
   
1
ğ»
ğ»
= E
+ min ğœ,
2
2
2
 
 
1 ğ»
1
1 ğ»
1
â‰¤
+ E[ğœ] =
+ ğ‘‡ğœ .
2 2
2
2 2
2
The statement of the theorem then follows by lower bounding
the exploitability as follows:
ğ»,ğ‘ ,(1)

Eğ‘ƒ,ğ‘…

(ğœ‹ âˆ—, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— )

ğ»,ğ‘ ,(1)
ğ»,ğ‘ ,(1) âˆ— âˆ—
(ğœ‹ , ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— )
ğœ‹
ğ»,ğ‘ ,(1) br âˆ—
ğ»,ğ‘ ,(1) âˆ— âˆ—
â‰¥ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— )

= max ğ½ğ‘ƒ,ğ‘…




 

 
ğ»
ğ‘‡ğœ 1 ğ»
ğ‘‡ğœ
ğ»
âˆ’
âˆ’
âˆ’
âˆ’ğ›¼
2
2
2 2
2
2

 

5
ğ»
ğ»
âˆ’ 24 âˆ’ log5 ğ‘ âˆ’ ğ›¼
â‰¥(1 âˆ’ ğ›¼ âˆ’ ğ›½)
4
9
2
â‰¥(1 âˆ’ ğ›¼ âˆ’ ğ›½)

The above inequality implies that if ğ» â‰¥ log2 ğ‘ , then
(ğœ‹ âˆ—, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— )


1
5
ğ»
â‰¥(1 âˆ’ ğ›¼ âˆ’ ğ›½)
âˆ’
ğ» âˆ’ ğ›¼ âˆ’ 24,
4 9 log2 5
2

ğ»,ğ‘ ,(1)

Eğ‘ƒ,ğ‘…

ğ»,ğ‘ ,(1)

which implies Eğ‘ƒ,ğ‘…

Merging the inequalities above, we obtain
ğ»,ğ‘ ,(1)

ğ½ğ‘ƒ,ğ‘…

(ğœ‹ br, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) â‰¥ (1 âˆ’ ğ›¼ âˆ’ ğ›½)





 
ğ»
ğ‘‡ğœ
ğ»
âˆ’
+ğ›½
.
2
2
2

Step 5: Bounding exploitability. Finally, we will upper bound
also the expected reward of the FH-MFG-NE policy ğœ‹ âˆ— and hence
lower bound the exploitability. Our conclusion will be that ğœ‹ âˆ— suffers from a non-vanishing exploitability for large ğ» , as ğœ‹ br becomes
the best response policy after ğ» â‰³ log ğ‘ . In this step, we assume the
probability space induced by all ğ‘ agents following FH-MFG-NE
policy ğœ‹ br .
We have the definition
ğ»,ğ‘ ,(1) âˆ— âˆ—
ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— ) = E

"ğ» âˆ’1
âˆ‘ï¸

#
ğ‘…(ğ‘ â„1 , ğ‘â„1 , b
ğœ‡â„ )

â„=0

â‰¤(1 âˆ’ ğ›¼ âˆ’ ğ›½) E

" ğ» âˆ’1
âˆ‘ï¸

1,g

ğ‘…â„

(ğœ‹ âˆ—, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) â‰¥ Î©(ğ» ) by choosing ğ›¼, ğ›½

5
small constants as 41 âˆ’ 9 log
> 0.
25

#


+ (ğ›¼ + ğ›½)

odd â„=0
g

ğ»
2



This time, when â„ odd and â„ > 2ğœ, it holds that E[ğ‘…â„ |â„ > 2ğœ] = 1/2
since ğœ‹ âˆ— takes actions ğ‘ A, ğ‘ B with equal probability in even steps,

A.4

Upper Bound for Stat-MFG: Extended Proof
of Theorem 3.5

Let ğœ‡ âˆ—, ğœ‹ âˆ— be a ğ›¿-Stat-MFG-NE. As before, the proof will proceed in
three steps:
â€¢ Step 1. Bounding the expected deviation of the empirical
population distribution from the mean-field distribution
E [âˆ¥b
ğœ‡â„ âˆ’ ğœ‡ âˆ— âˆ¥ 1 ] for any given policy ğœ‹ .
â€¢ Step 2. Bounding difference of ğ‘ agent value function
ğ›¾,ğ‘ ,(ğ‘– )
ğ›¾
ğ½ğ‘ƒ,ğ‘…
and the infinite player value function ğ‘‰ğ‘ƒ,ğ‘… in the
stationary mean-field game setting.
â€¢ Step 3. Bounding the exploitability of an agent when each
of ğ‘ agents are playing the Stat-MFG-NE policy.
Step 1: Empirical distribution bound. We first analyze the
deviation of the empirical population distribution b
ğœ‡ğ‘¡ over time from
the stable distribution ğœ‡ âˆ— . For this, we state the following lemma
and prove it using techniques similar to Corollary D.4 of [36].
Lemma A.11. Assume that the conditions of Theorem 3.5 hold,
and that (ğœ‡ âˆ—, ğœ‹ âˆ— ) âˆˆ Î” S is a Stat-MFG-NE. Furthermore, assume
ğ‘ in the ğ‘ -Stat-MFG, define
that the ğ‘ agents follow policies {ğœ‹ ğ‘– }ğ‘–=1

Î”ğœ‹ := ğ‘1

Ã

ğ‘–
ğ‘– âˆ¥ğœ‹ âˆ’ ğœ‹ âˆ¥ 1 . Then, or any ğ‘¡ â‰¥ 0, we have

Hence, by the law of total expectation, we can conclude
âˆšï¸



 ğ¾ğ‘ Î”ğœ‹ 2 |S|
E âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ +1 âˆ¥ 1 â‰¤ E âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1 +
+ âˆš
2
ğ‘

âˆšï¸


 ğ‘¡ğ¾ğ‘ Î”ğœ‹ 2(ğ‘¡ + 1) |S|
.
E âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1 â‰¤
+
âˆš
2
ğ‘
Proof. Fğ‘¡ as the ğœ-algebra generated by the states of agents
{ğ‘ ğ‘¡ğ‘– } at time ğ‘¡. For ğœ‡b0 , we have by definitions that
"

#
1 âˆ‘ï¸
eğ‘  ğ‘– = ğœ‡ âˆ—
ğ‘¡
ğ‘ ğ‘–

"

1 âˆ‘ï¸ 

E [ ğœ‡b0 ] = E

E âˆ¥ ğœ‡b0 âˆ’ ğœ‡ âˆ— âˆ¥ 22 = E


ğ‘2 ğ‘–

eğ‘  ğ‘– âˆ’ ğœ‡ âˆ—
ğ‘¡

#

2

â‰¤

4
ğ‘

2 |S|

eğ‘  â€²

ğ‘
ğ‘–=1

ğ‘ â€² âˆˆ S

ğ›¾,ğ‘ ,(ğ‘– )

ğ‘ƒ (ğ‘  â€² |ğ‘ ğ‘¡ğ‘– , ğœ‹ ğ‘– (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ ),

(10)

E[âˆ¥b
ğœ‡ğ‘¡ +1 âˆ’ E[b
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] âˆ¥ 22 |Fğ‘¡ ]
ğ‘

=

1 âˆ‘ï¸
4
E[âˆ¥eğ‘  ğ‘– âˆ’ E[eğ‘  ğ‘– |Fğ‘¡ ] âˆ¥ 22 |Fğ‘¡ ] â‰¤ .
ğ‘¡ +1
ğ‘¡ +1
ğ‘
ğ‘ 2 ğ‘–=1

(11)

We bound the â„“1 distance to the stable distribution as


E âˆ¥b
ğœ‡ğ‘¡ +1 âˆ’ ğœ‡ âˆ— âˆ¥ 1 |Fğ‘¡
â‰¤ E [âˆ¥ E [b
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] |Fğ‘¡ ] âˆ’ ğœ‡ âˆ— âˆ¥ 1 + E [âˆ¥ E [b
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] âˆ’ b
ğœ‡ğ‘¡ +1 âˆ¥ 1 Fğ‘¡ ] .
|
{z
} |
{z
}
(â–¡)

(â–³)

The two terms can be bounded separately using Inequalities (10)
and (11).
âˆšï¸
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] âˆ’ b
ğœ‡ğ‘¡ +1 âˆ¥ 2 Fğ‘¡ ]
(â–³) â‰¤ |S| E [âˆ¥ E [b
âˆšï¸
âˆšï¸ âˆšï¸ƒ 
 2 |S|
â‰¤ |S| E âˆ¥ E [b
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] âˆ’ b
ğœ‡ğ‘¡ +1 âˆ¥ 22 Fğ‘¡ â‰¤ âˆš
ğ‘
ğ‘
âˆ‘ï¸
âˆ‘ï¸
1
(â–¡) =
eğ‘  â€²
ğ‘ƒ (ğ‘  â€² |ğ‘ ğ‘¡ğ‘– , ğœ‹ ğ‘– (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ ) âˆ’ ğœ‡ âˆ—
ğ‘
â€²
ğ‘–=1
ğ‘  âˆˆS

âˆ‘ï¸

=

1

eğ‘  â€²

ğ‘

ğ‘ â€² âˆˆ S

â‰¤

ğ‘
âˆ‘ï¸
1

ğ‘
ğ‘–=1
+

ğ‘
âˆ‘ï¸
1

ğ‘ƒ (ğ‘  â€² |ğ‘ ğ‘¡ğ‘– , ğœ‹ ğ‘– (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ ) âˆ’ Î“ğ‘ğ‘œğ‘ (ğœ‹ âˆ—, ğœ‡ âˆ— )

ğ‘–=1

ğ‘ƒ (Â·|ğ‘ ğ‘¡ğ‘– , ğœ‹ ğ‘– (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ ) âˆ’

âˆ‘ï¸
ğ‘ â€² âˆˆ S

ğ‘
ğ‘–=1

ğ›¾

Proof. For ease of reading, in this proof expectations, probabilities, and laws of random variables will be denoted Eâˆ, Pâˆ, Lâˆ respectively over the infinite player finite horizon game and Eğ‘ , Pğ‘ , Lğ‘
respectively over the ğ‘ -player game. Due to symmetry in the ğ‘
agent game, any permutation ğœ : [ğ‘ ] â†’ [ğ‘ ] of agents does not
ğœ (1)
ğœ (ğ‘ )
change their distribution, that is Lğ‘ (ğ‘ ğ‘¡1, . . . , ğ‘ ğ‘¡ğ‘ ) = Lğ‘ (ğ‘ ğ‘¡ , . . . , ğ‘ ğ‘¡
).
We can then conclude that:
ğ‘




1 âˆ‘ï¸
Eğ‘ ğ‘…(ğ‘ ğ‘¡ğ‘– , ğ‘ğ‘–ğ‘¡ , b
Eğ‘ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡â„ ) =
ğœ‡ğ‘¡ )
ğ‘ ğ‘–=1
#
"
âˆ‘ï¸
ğœ‡ğ‘¡ ).
= Eğ‘
b
ğœ‡ğ‘¡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ğ‘¡ (ğ‘ ), b
ğ‘ âˆˆS

Therefore, we by definition:
ğ›¾,ğ‘ ,(1)
(ğœ‹ , . . . , ğœ‹ ) = Eğ‘
ğ½ğ‘ƒ,ğ‘…

"âˆ
âˆ‘ï¸ âˆ‘ï¸

#
âˆ—

b
ğœ‡ğ‘¡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ (ğ‘ ), b
ğœ‡ğ‘¡ ) .

ğ‘¡ =0 ğ‘  âˆˆ S

Next, in the Stat-MFG, we have that for all ğ‘¡ â‰¥ 0,
Pâˆ (ğ‘ ğ‘¡ = Â·) = ğœ‡ âˆ—,
âˆ‘ï¸
Pâˆ (ğ‘ ğ‘¡ +1 = Â·) =
Pâˆ (ğ‘ ğ‘¡ = ğ‘ ) Pâˆ (ğ‘ ğ‘¡ = Â·|ğ‘ ğ‘¡ = ğ‘ )
ğ‘ âˆˆS

ğ‘ƒ (Â·|ğ‘ ğ‘¡ğ‘– , ğœ‹ âˆ— (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ )
1

b
ğœ‡ğ‘¡ (ğ‘  â€² )ğ‘ƒ (ğ‘  â€² |ğ‘ ğ‘¡ğ‘– , ğœ‹ ğ‘– (ğ‘ ğ‘¡ğ‘– ), b
ğœ‡ğ‘¡ ) âˆ’ Î“ğ‘ğ‘œğ‘ (ğœ‹ âˆ—, ğœ‡ âˆ— )

so by induction Pâˆ (ğ‘ ğ‘¡ = Â·) = ğœ‡ âˆ— . Then we can conclude that
"âˆ
#
âˆ‘ï¸
ğ›¾
âˆ— âˆ—
ğ‘¡
âˆ—
ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ , ğœ‹ ) = Eâˆ
ğ›¾ ğ‘…(ğ‘ ğ‘¡ , ğœ‹ (ğ‘ ğ‘¡ ), ğœ‡ğ‘¡ )
ğ‘¡ =0

1

ğ¾ğ‘ âˆ‘ï¸ âˆ—
â‰¤
âˆ¥ğœ‹ âˆ’ ğœ‹ ğ‘– âˆ¥ 1 + Î“ğ‘ğ‘œğ‘ (ğœ‹ âˆ—, b
ğœ‡ğ‘¡ ) âˆ’ Î“ğ‘ğ‘œğ‘ (ğœ‹ âˆ—, ğœ‡ âˆ— ) 1
2ğ‘ ğ‘–
ğ¾ğ‘ Î”ğœ‹
â‰¤
+ âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1
2

(ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹ âˆ— )|

 âˆšï¸
ğ›¾
ğ¿ğ‘  2 |S|
ğ¿ğœ‡ +
â‰¤
âˆš
1 âˆ’ğ›¾
2
ğ‘

= Î“ğ‘ƒ (Pâˆ (ğ‘ ğ‘¡ = ğ‘ ), ğœ‹ âˆ— ) = ğœ‡ âˆ—,

1
ğ‘
âˆ‘ï¸
1



Lemma A.12. Suppose ğ‘ -Stat-MFG agents follow the same sequence of policy ğœ‹ âˆ— . Then for all ğ‘–,
|ğ½ğ‘ƒ,ğ‘…

ğ‘  âˆˆS

=

âˆ—

Step 2: Bounding difference in value functions. Next, we
bound the differences in the infinite-horizon

E [âˆ¥ ğœ‡b0 âˆ’ ğœ‡ âˆ— âˆ¥ 1 ] â‰¤ âˆš .
ğ‘
Next, we inductively calculate:
"
#
ğ‘
1 âˆ‘ï¸ âˆ‘ï¸
ğ‘–
â€²
E [b
ğœ‡ğ‘¡ +1 |Fğ‘¡ ] = E
1 (ğ‘ ğ‘¡ +1 = ğ‘  )eğ‘  â€² Fğ‘¡
ğ‘ â€² ğ‘–=1
ğ‘
âˆ‘ï¸
1

âˆšï¸
ğ‘¡ğ¾ğ‘ Î”ğœ‹ 2(ğ‘¡ + 1) |S|
E âˆ¥ğœ‡ âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1 â‰¤
+
.
âˆš
2
ğ‘


â–¡
 2

where the last line follows
by independence. The two above imply
âˆš

âˆ‘ï¸

or inductively,

=

âˆ
âˆ‘ï¸
ğ‘¡ =0

ğ›¾ğ‘¡

âˆ‘ï¸

ğœ‡ âˆ— (ğ‘ )ğ‘…(ğ‘ , ğœ‹ âˆ— (ğ‘ ), ğœ‡ âˆ— ),

ğ‘ âˆˆS

by a simple application of the dominated convergence theorem. We
next bound the differences in truncated expect reward until some

Then using the two bounds from Steps 2,3 and the fact that ğœ‹ âˆ—
ğ›¿-optimal with respect to ğœ‡ âˆ— :

time ğ‘‡ > 0:
#
"ğ‘‡
âˆ‘ï¸ âˆ‘ï¸
âˆ—
ğ‘¡
ğœ‡ğ‘¡ )
Eğ‘
ğ›¾
b
ğœ‡ğ‘¡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ (ğ‘ ), b
ğ‘¡ =0

âˆ’

ğ‘‡
âˆ‘ï¸

ğ›¾ğ‘¡

âˆ‘ï¸

ğ‘¡ =0

ğ‘ âˆˆS

"ğ‘‡
âˆ‘ï¸

ğ‘¡

â‰¤Eğ‘

(ğœ‹ â€², ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ âˆ—, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— )
âˆšï¸ !
 âˆšï¸ 
ğ¾ğ‘ 2 |S| ğ¿ğœ‡/2 + ğ¾ğœ‡ ğ¿ğœ‡ + ğ¿ğ‘  /2 2 |S|
+ âˆš
+
â‰¤ 2ğ›¿ +
âˆš
2ğ‘
(1 âˆ’ ğ›¾) 3 (1 âˆ’ ğ›¾) 2
ğ‘
ğ‘

max ğ½

ğ›¾

ğœ‡ğ‘¡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ âˆ— (ğ‘ ), ğœ‡ğ‘¡ )

âˆ‘ï¸ 

ğ‘¡ =0

ğ‘ âˆˆS

"ğ‘‡
âˆ‘ï¸



ğ»,ğ‘ ,(1)

âˆ—

âˆ—

âˆ—

âˆ—

ğœ‡ğ‘¡ ) âˆ’ ğœ‡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ (ğ‘ ), ğœ‡ )
b
ğœ‡ğ‘¡ (ğ‘ )ğ‘…(ğ‘ , ğœ‹ (ğ‘ ), b

ğ¿ğ‘  âˆ—
â‰¤Eğ‘
ğ›¾
âˆ¥ğœ‡ âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1 + ğ¿ğœ‡ âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1
2
ğ‘¡ =0


ğ‘‡
âˆ‘ï¸


ğ¿ğ‘ 
â‰¤
ğ›¾ ğ‘¡ ğ¿ğœ‡ +
Eğ‘ âˆ¥ğœ‡ âˆ— âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1
2
ğ‘¡ =0

 âˆšï¸
ğ¿ğ‘  2 |S|
1
ğ¿ğœ‡ +
â‰¤
âˆš
2
(1 âˆ’ ğ›¾) 2
ğ‘



#

#

ğ‘¡

Takingğ‘‡ â†’ âˆ and applying once again the dominated convergence
theorem the result is obtained.
â–¡
Step 3: Bounding difference in policy deviation. Finally, to
conclude the proof of the main theorem of this section, we will
prove that the improvement in expectation
due

 to single-sided
policy changes are at most of order O ğ›¿ + âˆš1

.

A.5

Lower Bound for Stat-MFG: Extended Proof
of Theorem 3.6

Similar to the finite horizon case, we define constructively the
counter-example: the idea and the nature of the counter-example
remain the same. However, minor details of the construction are
modified, as it will not hold immediately that all agents are on states
{ğ‘  Left, ğ‘  Right } on even times ğ‘¡, and that the Stat-MFG-NE is unique
as before.
Defining the Stat-MFG. We use the same definitions for S, A, g, h, ğœ”ğœ–
as in the FH-MFG case. Define the convenience functions ğ‘„ ğ¿ , ğ‘„ ğ‘…
as
ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB )
,
ğ‘„ ğ¿ (ğœ‡) :=
max{ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ) + ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ), 4/9}
ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB )
ğ‘„ ğ‘… (ğœ‡) :=
.
max{ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ) + ğœ‡ (ğ‘  RA ) + ğœ‡ (ğ‘  RB ), 4/9}
We define the transition probabilities:
If ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB }, âˆ€ğœ‡, ğ‘ :

ğ‘

Lemma A.13. Suppose we have two policy sequences ğœ‹ âˆ—, ğœ‹ âˆˆ Î  and
âˆ—
ğœ‡ âˆˆ Î” S such that Î“ğ‘ƒ (ğœ‡ âˆ—, ğœ‹ âˆ— ) = ğœ‡ âˆ— and Î“ğ‘ƒ (Â·, ğœ‹ âˆ— ) is non-expansive.
Then,
(ğœ‹ â€², ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹ â€² )


âˆ
ğ‘¡âˆ‘ï¸
âˆ’1
âˆ‘ï¸




E âˆ¥b
ğœ‡ğ‘¡ â€² âˆ’ ğœ‡ğ‘¡ğœ‹â€² âˆ¥ 1
â‰¤
ğ›¾ ğ‘¡ ğ¿ğœ‡ E âˆ¥b
ğœ‡ğ‘¡ âˆ’ ğœ‡ğ‘¡ğœ‹ âˆ¥ 1 + ğ¾ğœ‡
ğ›¾,ğ‘ ,(1)

ğ›¾

ğ½ğ‘ƒ,ğ‘…

ğ‘¡ â€² =0

ğ‘¡ =0

Proof. For the truncated game ğ‘‡ , it still holds by the derivation
in the FH-MFG that:




|Eğ‘ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ ) âˆ’ Eâˆ ğ‘…(ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğœ‡ğ‘¡ğœ‹ ) |




ğ‘¡âˆ‘ï¸
âˆ’1
ğ¿ğœ‡
â‰¤
Eğ‘ âˆ¥ğœ‡ğ‘¡ğœ‹ âˆ’ b
ğœ‡ğ‘¡ âˆ¥ 1 + ğ¾ ğœ‡
ğœ‡ğ‘¡ â€² âˆ¥ 1 .
Eğ‘ âˆ¥ğœ‡ğ‘¡ğœ‹â€² âˆ’ b
2
â€²
ğ‘¡ =0

We take the limit ğ‘‡ â†’ âˆ and apply the dominated convergence
Ã
theorem to obtain the state bound, also noting that 1/2 Â· ğ‘¡ (ğ‘¡ + 1)(ğ‘¡ +
1
â–¡
2)ğ›¾ ğ‘¡ â‰¤ (1âˆ’ğ›¾ ) 3 .
Conclusion and Statement of the Result. Finally, if ğœ‡ âˆ—, ğœ‹ âˆ— is
a ğ›¿-Stat-MFG-NE, by definition we have that: By definition of the
Stat-MFG-NE, we have:
ğ»
ğ›¿ â‰¥ Eğ‘ƒ,ğ‘…
(ğœ‹ ğ›¿ ) = max
ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹ â€² ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (ğœ‡ âˆ—, ğœ‹ âˆ— )
â€²
ğ›¾

ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘, ğœ‡) =

ï£±
ğœ”ğœ– (ğ‘„ ğ¿ (ğœ‡)), if ğ‘  â€² = ğ‘  Right, ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB }
ï£´
ï£´
ï£´
ï£´
ï£²ğœ” (ğ‘„ (ğœ‡)), if ğ‘  â€² = ğ‘  , ğ‘  âˆˆ {ğ‘  , ğ‘  }
ï£´
ğœ–

ğ‘…

Left

LA LB

ï£´
ğœ”ğœ– (ğ‘„ ğ¿ (ğœ‡)), if ğ‘  â€² = ğ‘  Right, ğ‘  âˆˆ {ğ‘  RA, ğ‘  RB }
ï£´
ï£´
ï£´
ï£´ğœ” (ğ‘„ (ğœ‡)), if ğ‘  â€² = ğ‘  , ğ‘  âˆˆ {ğ‘  , ğ‘  }
RA RB
Left
ï£³ ğœ– ğ‘…

,

and define ğ‘ƒ (ğ‘  Left, ğ‘, ğœ‡), ğ‘ƒ (ğ‘  Right, ğ‘, ğœ‡) as before. With previous Lipschitz continuity results, it follows that ğ‘ƒ âˆˆ P9/8ğœ€ .
Similarly, we modify the reward function ğ‘… as follows:
ğ‘…(ğ‘  Left, ğ‘ A, ğœ‡) =ğ‘…(ğ‘  Left, ğ‘ B, ğœ‡) = 0,

âˆšï¸ !
ğ¾ğ‘ 2 |S| ğ¿ğœ‡/2 + ğ¾ğœ‡
+ âˆš
â‰¤
2ğ‘
(1 âˆ’ ğ›¾) 3
ğ‘

ğœ‹ âˆˆÎ 

ğ»,ğ‘ ,(1)

ğœ‹ â€² âˆˆÎ  ğ‘ƒ,ğ‘…

ğ‘ âˆˆS

ğ›¾

ğ‘…(ğ‘  Right, ğ‘ A, ğœ‡) =ğ‘…(ğ‘  Right, ğ‘ B, ğœ‡) = 0,



ğ‘…(ğ‘  LA, ğ‘ A, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğ‘„ ğ¿ (ğœ‡), ğ‘„ ğ‘… (ğœ‡) + ğ›¼h(ğœ‡ (ğ‘  LA ), ğœ‡ (ğ‘  LB ))
ğ‘…(ğ‘  LB, ğ‘ A, ğœ‡)



ğ‘…(ğ‘  LA, ğ‘ B, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğ‘„ ğ¿ (ğœ‡), ğ‘„ ğ‘… (ğœ‡) + h(ğœ‡ (ğ‘  LA ), ğœ‡ (ğ‘  LB ))
ğ‘…(ğ‘  LB, ğ‘ B, ğœ‡)
+ ğ›½1



ğ‘…(ğ‘  RA, ğ‘ A, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğ‘„ ğ‘… (ğœ‡), ğ‘„ ğ¿ (ğœ‡) + ğ›¼h(ğœ‡ (ğ‘  RA ), ğœ‡ (ğ‘  RB ))
ğ‘…(ğ‘  RB, ğ‘ A, ğœ‡)



ğ‘…(ğ‘  RA, ğ‘ B, ğœ‡)
=(1 âˆ’ ğ›¼ âˆ’ ğ›½)g ğ‘„ ğ‘… (ğœ‡), ğ‘„ ğ¿ (ğœ‡) + ğ›¼h(ğœ‡ (ğ‘  RA ), ğœ‡ (ğ‘  RB ))
ğ‘…(ğ‘  RB, ğ‘ B, ğœ‡)
+ ğ›½1,
simple computation shows that ğ‘… âˆˆ R 3 . In this proof, unlike the
ğ‘ -FH-SAG case, ğ›¼ will be chosen as a function of ğ‘ , namely ğ›¼ =
O (ğ‘’ âˆ’ğ‘ ).
Step 1: Solution of the Stat-MFG. We solve the infinite agent
game: let ğœ‡ âˆ—, ğœ‹ âˆ— be an Stat-MFG-NE. By simple computation, one
can see that for any stationary distribution ğœ‡ âˆ— of the game, probability must be distributed equally between groups of states {ğ‘  Left, ğ‘  Right }

and {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB }, that is,
ğœ‡ âˆ— (ğ‘  Left ) + ğœ‡ âˆ— (ğ‘  Right ) = 1/2,
âˆ—

âˆ—

ğœ‡ (ğ‘  LA ) + ğœ‡ (ğ‘  LB ) + ğœ‡ âˆ— (ğ‘  RA ) + ğœ‡ âˆ— (ğ‘  RB ) = 1/2.
It holds by the stationarity equation Î“ğ‘ƒ (ğœ‡ âˆ—, ğœ‹ âˆ— ) = ğœ‹ âˆ— that
ğœ‡ âˆ— (ğ‘  Left ) =ğœ‡ âˆ— (ğ‘  LA ) + ğœ‡ âˆ— (ğ‘  LB ),
ğœ‡ âˆ— (ğ‘  Right ) =ğœ‡ âˆ— (ğ‘  RA ) + ğœ‡ âˆ— (ğ‘  RB ),
âˆ‘ï¸
ğœ‡ âˆ— (ğ‘  Left ) =
ğœ‡ âˆ— (ğ‘ )ğœ‹ âˆ— (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  Left |ğ‘ , ğ‘, ğœ‡ âˆ— )
ğ‘ âˆˆS
âˆ—

=ğ‘ƒ (ğ‘  Left |ğ‘  LA, ğ‘ A, ğœ‡ ),
âˆ‘ï¸
ğœ‡ (ğ‘  Right ) =
ğœ‡ âˆ— (ğ‘ )ğœ‹ âˆ— (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  Right |ğ‘ , ğ‘, ğœ‡ âˆ— )
âˆ—

As before, using the Markov property, Hoeffding, and the fact that
|ğœ”ğœ– (ğ‘¥) âˆ’ 1/2 | â‰¥ 1/2ğœ– |ğ‘¥ âˆ’ 1/2 | we obtain âˆ€ğ‘˜ âˆˆ 0, . . . , ğ¾ âˆ’ 1, âˆ€ğ‘š that
P[ğ‘‹ğ‘š+1 âˆˆ G0 |ğ‘‹ğ‘š âˆˆ Gâˆ’1, ğ¸ 0 ] â‰¥ 1/20


1
P[ğ‘‹ğ‘š+1 âˆˆ Gğ‘˜+1 |ğ‘‹ğ‘š âˆˆ Gğ‘˜ , ğ¸ 0 ] â‰¥ ğ›¼ğ‘˜ := 1 âˆ’ 2 exp âˆ’ 4ğ‘˜+1 ,
8
hence from the analysis before we have the lower bound
 ğ‘š 
2
E[|ğ‘‹ğ‘š âˆ’ 1/2 | |ğ¸ 0 ] â‰¥ ğ¶ 1 min âˆš , 1 ,
ğ‘0
for some absolute constant ğ¶ 2 > 0.
Step 3. Exploitability lower bound. As in the case of FH-MFG,
the ergodic optimal policy is given by

ğ‘ âˆˆS

=ğ‘ƒ (ğ‘  Right |ğ‘  LA, ğ‘ A, ğœ‡ âˆ— ),
as ğ‘ƒ (ğ‘  Right |ğ‘ , ğ‘, ğœ‡ âˆ— ) = ğ‘ƒ (ğ‘  Right |ğ‘ , ğ‘, ğœ‡ âˆ— ) and similarly ğ‘ƒ (ğ‘  Left |ğ‘ , ğ‘, ğœ‡ âˆ— ) =
ğ‘ƒ (ğ‘  Left |ğ‘ , ğ‘, ğœ‡ âˆ— ) for any ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB }, ğ‘ âˆˆ A. If ğœ‡ âˆ— (ğ‘  Left ) >
âˆ—
1
1/4, then by definition ğ‘ƒ (ğ‘ 
Left |ğ‘  LA , ğ‘ A , ğœ‡ ) < /4, and similarly if
ğœ‡ âˆ— (ğ‘  Left ) < 1/4, then by definition ğ‘ƒ (ğ‘  Left |ğ‘  LA, ğ‘ A, ğœ‡ âˆ— ) > 1/4. So it
must be the case that ğœ‡ âˆ— (ğ‘  Left ) = ğœ‡ âˆ— (ğ‘  Right ) = 1/4. Then the unique
Stat-MFG-NE must be
ï£±
ï£´
1, if ğ‘ = ğ‘ B, ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB }
ï£´
ï£²
ï£´
ğœ‹ âˆ— (ğ‘|ğ‘ ) := 21 , if ğ‘  âˆˆ {ğ‘  Left, ğ‘  Right }
ï£´
ï£´
ï£´ 0, if ğ‘ = ğ‘ A, ğ‘  âˆˆ {ğ‘  LA, ğ‘  LB, ğ‘  RA, ğ‘  RB },
ï£³

ğœ‡ âˆ— (ğ‘  RA ) = ğœ‡ âˆ— (ğ‘  LA ) = ğœ‡ âˆ— (ğ‘  RB ) = ğœ‡ âˆ— (ğ‘  LB ) = 1/8,

as otherwise the action arg minğ‘âˆˆ A ğœ‹ âˆ— (ğ‘|ğ‘  Right ) will be a better
response in state ğ‘  Right and the action arg minğ‘âˆˆ A ğœ‹ âˆ— (ğ‘|ğ‘  Left ) will
be optimal in state ğ‘  Right .
Step 2: Expected population deviation in ğ‘ -Stat-SAG. We fix
1/2ğœ€ = 3, define the random variable ğ‘ := ğ‘ (b
ğœ‡0 (ğ‘  Right ) + b
ğœ‡ 0 (ğ‘  Left )).
We will analyze the population under the event ğ‘ := {| ğ‘ /ğ‘ âˆ’ 1/2 | â‰¤
2
1/18 }, which holds with probability Î©(1 âˆ’ ğ‘’ âˆ’ğ‘ ) by the Hoeffding
inequality. Under the event ğ¸, it holds that b
ğœ‡ğ‘¡ (ğ‘  LA ) + b
ğœ‡ğ‘¡ (ğ‘  LA ) +
b
ğœ‡ğ‘¡ (ğ‘  LA ) + b
ğœ‡ğ‘¡ (ğ‘  LA ) > 4/9 almost surely at all ğ‘¡.
Fix ğ‘ 0 âˆˆ N>0 such that | ğ‘0/ğ‘ âˆ’ 1/2 | â‰¤ 1/18, in this step we will
condition on ğ¸ 0 := {ğ‘ := ğ‘ 0 }. Once again define the random
process ğ‘‹ğ‘š for ğ‘š âˆˆ N â‰¥0 such that
ğ‘‹ğ‘š :=

b
ğœ‡ 2ğ‘š (ğ‘  Left )
ï£±
ï£´
, if ğ‘š odd
ï£²
ï£´b
ğœ‡ 2ğ‘š (ğ‘  Left )+b
ğœ‡ 2ğ‘š (ğ‘  Right )

b
ğœ‡ 2ğ‘š (ğ‘  Right )
ï£´
, if ğ‘š even
ï£´b
ğœ‡ 2ğ‘š (ğ‘  Right )
ï£³ ğœ‡2ğ‘š (ğ‘ Left )+b

with the modification at odd ğ‘š necessary because of the difference
in dynamics ğ‘ƒ (oscillating between ğ‘  Left, ğ‘  Right ) from the FH-SAG
case. It still holds that ğ‘‹ğ‘š is Markovian, and given ğ‘‹ğ‘š we have
ğ‘ 0ğ‘‹ğ‘š+1 âˆ¼ Binom(ğ‘ 0, ğœ”ğœ– (ğ‘‹ğ‘š )). As before, ğ‘‹ğ‘š is independent from
the policies of agents. âˆš
Define ğ¾ := âŒŠlog2 ğ‘ 0 âŒ‹, G := {ğ‘˜/ğ‘0 : ğ‘˜ = 0, . . . , ğ‘ 0 }, Gâˆ— :=
{0, 1} âŠ‚ G and the level sets once again as
(
)
1
2ğ‘˜
Gâˆ’1 := G, Gğ‘˜ := ğ‘¥ âˆˆ G : ğ‘¥ âˆ’ â‰¥ âˆš
when ğ‘˜ â‰¤ ğ¾,
2
2 ğ‘0
Gğ¾+1 := Gâˆ— .

ğœ‹ (ğ‘|ğ‘ ) =

ï£±
1, if ğ‘  = ğ‘  Left , ğ‘ = ğ‘ A
ï£´
ï£´
ï£´
ï£´
ï£² 1, if ğ‘  = ğ‘ 
ï£´
,ğ‘ =ğ‘
Right

A

ï£´
1, if ğ‘  âˆ‰ {ğ‘  Left, ğ‘  Right }, ğ‘ = ğ‘ B
ï£´
ï£´
ï£´
ï£´ 0, otherwise
ï£³

We define the shorthand functions
S âˆ— := {ğ‘  Left, ğ‘  Right }, ğ‘„ (ğœ‡) := (ğ‘„ ğ¿ (ğœ‡), ğ‘„ ğ‘… (ğœ‡)),
ğ‘„ min (ğœ‡) := min{ğ‘„ ğ¿ (ğœ‡), ğ‘„ ğ‘… (ğœ‡)}, ğ‘„ max := max{ğ‘„ ğ¿ (ğœ‡), ğ‘„ ğ‘… (ğœ‡)}.
We condition on ğ¸ S âˆ— := {ğ‘  01 âˆˆ S âˆ— }, that is the first agent starts
from states {ğ‘  Left, ğ‘  Right }, the analysis will be similar under event
ğ¸ğ‘S âˆ— . As in the case of FH-MFG, due to permutation invariance, it
holds for any odd ğ‘¡ and ğœ‡ âˆˆ {ğœ‡ â€² âˆˆ Î” S âˆ— : ğ‘ 0 ğœ‡ â€² âˆˆ N2>0 } that
P[ğ‘ ğ‘¡1 âˆˆ {ğ‘  LA, ğ‘  LB }|ğ¸ 0, ğ¸ S âˆ— , ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡] = ğ‘„ ğ¿ (ğœ‡)
P[ğ‘ ğ‘¡1 âˆˆ {ğ‘  RA, ğ‘  RB }|ğ¸ 0, ğ¸ S âˆ— , ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡] = ğ‘„ ğ‘… (ğœ‡),
1,g

therefore expressing the error component due to g as ğ‘…ğ‘¡ and
expressing some repeating conditionals as â€¢:
h
i
ğ‘–
âˆ— ğ‘–
1,g
ğœ‡
ğº ğ‘¡ := E ğ‘…ğ‘¡ ğ¸ 0, ğ¸ S âˆ— , ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡, ğ‘ğ‘¡1 âˆ¼ ğœ‹ (ğ‘ ğ‘¡1 ), ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
when ğ‘– â‰  1
âˆ‘ï¸
1,g
=
P[ğ‘ ğ‘¡1 = ğ‘  |ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡, â€¢] E[ğ‘…ğ‘¡ |ğ‘ ğ‘¡1 = ğ‘ , ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡, â€¢]
ğ‘  âˆˆ Sâˆ—

=

ğ‘„ min (ğœ‡)
ğ‘„ max (ğœ‡)
ğ‘„ max (ğœ‡) +
ğ‘„ min (ğœ‡).
ğ‘„ max (ğœ‡)
ğ‘„ max (ğœ‡)

Similarly, since ğœ‹ âˆ— (ğ‘|ğ‘ ) = 1/2 for any ğ‘  âˆˆ S âˆ— , it holds that
h
i
ğ‘–
âˆ— ğ‘–
1,g
ğœ‡
ğºğ‘¡ := E ğ‘…ğ‘¡ ğ¸ 0, ğ¸ S âˆ— , ğ‘„ (b
ğœ‡ğ‘¡ ) = ğœ‡, ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
âˆ€ğ‘–

1 ğ‘„ min (ğœ‡) 1 ğ‘„ max (ğœ‡)
=
+
.
2 ğ‘„ max (ğœ‡) 2 ğ‘„ max (ğœ‡)
Therefore, given the population distribution between ğ‘  LA, ğ‘  LB and
ğ‘  RA, ğ‘  RB , the expected difference in rewards for the two policies is
 


1
1 ğ‘„ min (ğœ‡)
ğœ‡
ğœ‡
ğº ğ‘¡ âˆ’ ğºğ‘¡ = ğ‘„ max (ğœ‡) âˆ’
+ ğ‘„ min (ğœ‡) âˆ’
2
2 ğ‘„ max (ğœ‡)

 

ğ‘„ min (ğœ‡)
1
1
= ğ‘„ max (ğœ‡) âˆ’
+
âˆ’ ğ‘„ max (ğœ‡)
2
2
ğ‘„ max (ğœ‡)



ğ‘„ min (ğœ‡)
1
= ğ‘„ max (ğœ‡) âˆ’
1âˆ’
2
ğ‘„ max (ğœ‡)

2
1
â‰¥2 ğ‘„ max (ğœ‡) âˆ’
.
2

Therefore from above, we conclude that
 ğ‘¡ 
2
b
ğœ‡
b
ğœ‡
E[ğº ğ‘¡ ğ‘¡ âˆ’ ğºğ‘¡ ğ‘¡ |ğ¸ 0 ] â‰¥ E[2|ğ‘‹ ğ‘¡ âˆ’1 âˆ’ 1/2 | 2 |ğ¸ 0, ğ¸ S âˆ— ] â‰¥ 2ğ¶ 12 min
,1 .
2
2ğ‘ 0
Using the lower bound above, the conditional expected difference
in discounted total reward is
E

âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ 0, ğ¸ S âˆ— , ğ‘ğ‘¡1 âˆ¼ ğœ‹ (ğ‘ ğ‘¡1 ), ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
when ğ‘– â‰  1

ğ‘¡ =0

âˆ’E

âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ 0, ğ¸ S âˆ— , ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),

â‰¥ (1 âˆ’ ğ›¼ âˆ’ ğ›½)

â‰¥

âˆ
âˆ‘ï¸

(
2ğ¶ 12 ğ›¾ 2ğ‘˜+1 min

ğ‘˜=0
âŒŠlog
4 ğ‘0 âŒ‹
âˆ‘ï¸

ğ¶2
ğ‘0

(4ğ›¾ 2 )ğ‘˜ +

ğ‘˜=0

ğ¶3
ğ‘0

ğ‘› times

âˆ
âˆ‘ï¸

ğ›¾ 2ğ‘˜ âˆ’

ğ‘˜=âŒŠlog4 ğ‘ 0 âŒ‹
âˆ’1

(ğ›¾ ) 4 0 ğ‘ 0
ğ¶ 4 ((4ğ›¾ 2 ) log4 ğ‘0 âˆ’ 1)
+ ğ¶5
ğ‘0
1 âˆ’ ğ›¾2

âˆ’

2ğ›¼
1 âˆ’ğ›¾

Taking expectation over ğ‘ 0 (using E[ğ‘ |ğ¸ âˆ— ] = ğ‘ /2 and Jensenâ€™s):
âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ âˆ—, ğ¸ S âˆ— , ğ‘ğ‘¡1 âˆ¼ ğœ‹ (ğ‘ ğ‘¡1 ), ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
when ğ‘– â‰  1

ğ‘¡ =0

âˆ’E

âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ âˆ—, ğ¸ S âˆ— , ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
âˆ€ğ‘–

ğ‘¡ =0
log2 ğ›¾

â‰¥ğ¶ 6 ğ‘ 0

+ ğ¶7

log ğ›¾ âˆ’2
ğ‘0 2

1 âˆ’ğ›¾

âˆ’

2ğ›¼
1 âˆ’ğ›¾

While the analysis above assumes event ğ¸ S âˆ— , the same analysis
lower bound follows with a shift between even and odd steps when
ğ‘  01 âˆ‰ S âˆ— , hence
E

âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ âˆ—, ğ‘ğ‘¡1 âˆ¼ ğœ‹ (ğ‘ ğ‘¡1 ), ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
when ğ‘– â‰  1

ğ‘¡ =0

âˆ’E

âˆ
 âˆ‘ï¸
ğ‘–
âˆ— ğ‘– 
ğ›¾ ğ‘¡ ğ‘…(ğ‘ ğ‘¡1, ğ‘ğ‘¡1, b
ğœ‡ğ‘¡ )|ğ¸ âˆ—, ğ‘ğ‘¡ âˆ¼ğœ‹ (ğ‘ ğ‘¡ ),
âˆ€ğ‘–

ğ‘¡ =0
log ğ›¾ âˆ’2

log2 ğ›¾

â‰¥ğ¶ 6 ğ‘ 0

ğ‘ 2
+ ğ¶7 0
1 âˆ’ğ›¾

âˆ’

ğ‘¢ğ›¼ :R â†’ [0, ğ›¼]
ï£±
ï£´
ğ›¼, if ğ‘¥ â‰¥ ğ›¼,
ï£´
ï£²
ï£´
ğ‘¢ğ›¼ (ğ‘¥) := max{0, min{ğ›¼, ğ‘¥ }} = ğ‘¥, if 0 â‰¤ ğ‘¥ â‰¤ ğ›¼,
ï£´
ï£´
ï£´ 0, if ğ‘¥ â‰¤ 0.
ï£³

2ğ›¼
1 âˆ’ğ›¾

log ğ›¾ âˆ’1
ğ‘0 2
2ğ›¼
log2 ğ›¾
+ ğ¶7
â‰¥ ğ¶6 ğ‘0
âˆ’
.
1 âˆ’ğ›¾
1 âˆ’ğ›¾

E

Notations. For a finite set Î£, we denote by Î£ğ‘› the set of tuples ğ‘›
Ã
elements from Î£, and by Î£âˆ— = ğ‘›â‰¥0 Î£ğ‘› the set of finite sequences
of elements of Î£. For any ğ›¼ âˆˆ Î£, let ğ›¼ ğ‘› âˆˆ Î£ğ‘› denote the ğ‘›-tuple
(ğ›¼, . . . , ğ›¼ ). For ğ‘¥ âˆˆ Î£âˆ— , by |ğ‘¥ | we denote the length of the sequence
| {z }

)
2ğ›¼
22ğ‘˜
,1 âˆ’
ğ‘0
1 âˆ’ğ›¾

2 log ğ‘

â‰¥

We first introduce standard definitions and tools, mostly taken from
[7, 11, 24].

ğ‘¥. Finally, the following function will be useful, defined for any
ğ›¼ > 0:

âˆ€ğ‘–

ğ‘¡ =0

B INTRACTABILITY RESULTS
B.1 Fundamentals of PPAD

2ğ›¼
1 âˆ’ğ›¾

We define a search problem S on alphabet Î£ as a relation from
a set IS âŠ‚ Î£âˆ— to Î£âˆ— such that for all ğ‘¥ âˆˆ IS , the image of ğ‘¥ under
ğ‘˜
ğ‘˜
S satisfies Sğ‘¥ âŠ‚ Î£ |ğ‘¥ | for some ğ‘˜ âˆˆ N>0 , and given ğ‘¦ âˆˆ Î£ |ğ‘¥ | m
whether ğ‘¦ âˆˆ Sğ‘¥ is decidable in polynomial time.
Intuitively speaking, PPAD is the complexity class of search problems that can be shown to always have a solution using a â€œparity
argumentâ€ on a directed graph. The simplest complete example (the
example that defines the problem class) of PPAD problems is the
computational problem End-of-The-Line. The problem, formally
defined below, can be summarized as such: given a directed graph
where each node has in-degree and out-degree at most one and
given a node that is a source in this graph (i.e., no incoming edge
but one outgoing edge), find another node that is a sink or a source.
Such a node can be always shown to exist using a simple parity
argument.
Definition B.1 (End-of-The-Line [7]). The computational problem End-of-The-Line is defined as follows: given two binary
circuits ğ‘†, ğ‘ƒ each with ğ‘› input bits and ğ‘› output bits such that
ğ‘ƒ (0ğ‘› ) = 0ğ‘› â‰  ğ‘† (ğ‘  ğ‘› ), find an input ğ‘¥ âˆˆ {0, 1}ğ‘› such that ğ‘ƒ (ğ‘† (ğ‘¥)) â‰ 
ğ‘¥ or ğ‘† (ğ‘ƒ (ğ‘¥)) â‰  ğ‘¥ â‰  0ğ‘› .
The obvious solution to the above is to follow the graph node by
node using the given circuits until we reach a sink: however, this
can take exponential time as the graph size can be exponential in
the bit descriptions of the circuits. It is believed that End-of-TheLine is difficult [11], that there is no efficient way to use the bit
descriptions of the circuits ğ‘†, ğ‘ƒ to find another node with degree 1.

B.2
Finally, we conclude the proof with the observation
ğ›¾,ğ‘ ,(1)
ğ»,ğ‘ ,(1) âˆ— âˆ—
(ğœ‹, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— )
ğœ‹
ğ›¾,ğ‘ ,(1)
ğ»,ğ‘ ,(1) âˆ— âˆ—
â‰¥ğ½ğ‘ƒ,ğ‘…
(ğœ‹, ğœ‹ âˆ—, . . . , ğœ‹ âˆ— ) âˆ’ ğ½ğ‘ƒ,ğ‘…
(ğœ‹ , ğœ‹ , . . . , ğœ‹ âˆ— )
log ğ›¾ âˆ’2
ğ‘0 2
2ğ›¼
ğ‘
log2 ğ›¾
âˆ’
âˆ’ (1 âˆ’ ğ›¾) âˆ’1 P[ğ¸ ],
â‰¥ğ¶ 6 ğ‘ 0
+ ğ¶7

max ğ½ğ‘ƒ,ğ‘…

1 âˆ’ğ›¾

ğ‘

2

1 âˆ’ğ›¾

where P[ğ¸ ] = ğ‘‚ (ğ‘’ âˆ’ğ‘ ) and we pick ğ›¼ = O (ğ‘’ âˆ’ğ‘ ).

Proof of Intractability of Stat-MFG

We reduce any ğœ€-GCircuit problem to the problem ğœ€-StatDist for
some simple transition function ğ‘ƒ âˆˆ P Sim .
Let (V, G) be a generalized circuit to be reduced to a stable
distribution computation problem. Let ğ‘‰ = |V | â‰¥ 1. We will define
a game that has at most ğ‘‰ +1 states and |A| = 1 actions, that is, agent
policy will not have significance, and it will suffice to determine
simple transition probabilities ğ‘ƒ (ğ‘  â€² |ğ‘ , ğœ‡) for all ğ‘ , ğ‘  â€² âˆˆ S, ğœ‡ âˆˆ Î” S .
The proposed system will have a base state ğ‘  base âˆˆ S and 1
additional state ğ‘  ğ‘£ associated with the gate whose output is ğ‘£ âˆˆ V.

Our construction will be sparse: only transition probabilities in
between states associated with a gate and ğ‘  base will take positive
1 , ğµ := 1 .
values. We define the useful constants ğœƒ := 8ğ‘‰
4
Given an (approximately) stable distribution ğœ‡ âˆ— of ğ‘ƒ, for each
vertex ğ‘£ we will read the satisfying assignment for the ğœ€-GCircuit
problem by the value ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘  ğ‘£ )). For each possible gate, we
define the following gadgets.
Binary assignment gadget. For a gate of the form ğº â† (ğœ ||ğ‘£), we
will add one state ğ‘  ğ‘£ such that
ï£±
ğ‘ƒ (ğ‘ 
|ğ‘  , ğœ‡) = 1,
ï£´
ï£´
ï£² base ğ‘£
ï£´
ğ‘ƒ
(ğ‘ 
|ğ‘ 
,
If ğœ = 1 :
ğ‘£ ğ‘£ ğœ‡) = 0,
ï£´
ï£´
ğœƒ
ï£´ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) =
max{ğµ, ğœ‡ (ğ‘  ) }
ï£³
base

ï£±
ï£´
ğ‘ƒ (ğ‘ 
|ğ‘  , ğœ‡) = 1,
ï£´
ï£² base ğ‘£
ï£´
If ğœ = 0 : ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  ğ‘£ , ğœ‡) = 0,
ï£´
ï£´
ï£´ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) = 0
ï£³
Weighted addition gadget. Next, we implement the addition gadget ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£) for ğ›¼, ğ›½ âˆˆ [âˆ’1, 1]. In this case, we also add
one state ğ‘  ğ‘£ to the game, and define the transition probabilities:
ğ‘ƒ (ğ‘  base |ğ‘  ğ‘£ , ğœ‡) = 1,
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  ğ‘£ , ğœ‡) = 0,
ğ‘¢ (ğ›¼ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 1 )) + ğ›½ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 2 )))
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) = ğœƒ
max{ğµ, ğœ‡ (ğ‘  base )}
Brittle comparison gadget. For the comparison gate ğº < (|ğ‘£ 1, ğ‘£ 1 |ğ‘£),
we also add one state ğ‘  ğ‘£ to the game. Define the function ğ‘ğ›¿ :
[âˆ’1, 1] â†’ [0, 1]


1
âˆ’1
+ ğ›¿ (ğ‘¥ âˆ’ ğ‘¦) ,
ğ‘ğ›¿ (ğ‘¥, ğ‘¦) := ğ‘¢ 1
2
for any ğ›¿ > 0. In particular, if ğ‘¥ â‰¥ ğ‘¦ + ğ›¿, then ğ‘ğ›¿ (ğ‘¥, ğ‘¦) = 1, and if
ğ‘¥ â‰¤ ğ‘¦ âˆ’ ğ›¿, then ğ‘ğ›¿ (ğ‘¥, ğ‘¦) = 0. We define the probability transitions
to and from ğ‘  ğ‘£ as
ğœƒğ‘ 8ğœ€ (ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ (ğ‘  1 )), ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ (ğ‘  2 )))
,
max{ğµ, ğœ‡ (ğ‘  base )}
ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  ğ‘£ , ğœ‡) = 0,

ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡) =

ğ‘ƒ (ğ‘  base |ğ‘  ğ‘£ , ğœ‡) = 1.
Finally, after all ğ‘  ğ‘£ have been added, we complete the definition
of ğ‘ƒ by setting
âˆ‘ï¸
ğ‘ƒ (ğ‘  base |ğ‘  base, ğœ‡) = 1 âˆ’
ğ‘ƒ (ğ‘  â€² |ğ‘  base, ğœ‡).
ğ‘ â€² âˆˆ S

We first verify that the above assignment is a valid transition
probability matrix for any ğœ‡ âˆˆ Î” S . It is clear from definitions that
for any ğœ‡, ğ‘  â‰  ğ‘  base , ğ‘ƒ (Â·|ğ‘ , ğœ‡) is a valid probability distribution as
long as 8ğœ€ < 1. Moreover, for any ğ‘  â‰  ğ‘  base , it holds that 0 â‰¤
ğ‘ƒ (ğ‘  |ğ‘  base, ğœ‡) â‰¤ ğµğœƒ < 1, and it also holds that
âˆ‘ï¸
ğ‘‰ğœƒ
ğ‘ƒ (ğ‘  base |ğ‘  base, ğœ‡) = 1 âˆ’
ğ‘ƒ (ğ‘  â€² |ğ‘  base, ğœ‡) â‰¥ 1 âˆ’
â‰¥0
ğµ
â€²

that ğ‘‰ + 1 = |S|, since for each gate in the generalized circuit we
defined one additional state.
Error propagation. We finally analyze the error propagation of
the stationary distribution problem in terms of the generalized
circuit. Without loss of generality we assume ğœ€ < 18 . First, for any
solution of the ğœ€-StatDist problem ğœ‡ âˆ— , whenever ğœ€ < 18 , it must
hold that:
âˆ‘ï¸
1
ğœ‡ âˆ— (ğ‘  base ) âˆ’
ğœ‡ âˆ— (ğ‘ )ğ‘ƒ (ğ‘  base |ğ‘ , ğœ‡ âˆ— ) â‰¤
,
8|S|
â€²
ğ‘  âˆˆS

hence (using ğ‘‰ < |S|) we have the lower bound on ğœ‡ âˆ— (ğ‘  base ) given
by:
âˆ‘ï¸
1
ğœ‡ âˆ— (ğ‘  base ) â‰¥
ğœ‡ âˆ— (ğ‘ )ğ‘ƒ (ğ‘  base |ğ‘ , ğœ‡ âˆ— ) âˆ’
8ğ‘‰
ğ‘ âˆˆS
âˆ‘ï¸
1
â‰¥ğœ‡ âˆ— (ğ‘  base )ğ‘ƒ (ğ‘  base |ğ‘  base, ğœ‡ âˆ— ) +
ğœ‡ âˆ— (ğ‘ )ğ‘ƒ (ğ‘  base |ğ‘ , ğœ‡ âˆ— ) âˆ’
8ğ‘‰
ğ‘ â‰ ğ‘  base


âˆ‘ï¸
ğ‘‰
ğœƒ
1
â‰¥ğœ‡ âˆ— (ğ‘  base ) 1 âˆ’
+
ğœ‡ âˆ— (ğ‘ ) âˆ’
ğµ
8ğ‘‰
ğ‘ â‰ ğ‘  base


ğ‘‰ğœƒ
1
â‰¥ğœ‡ âˆ— (ğ‘  base ) 1 âˆ’
+ (1 âˆ’ ğœ‡ âˆ— (ğ‘  base )) âˆ’
ğµ
8ğ‘‰
=â‡’ ğœ‡ âˆ— (ğ‘  base ) â‰¥

1
1 âˆ’ 8ğ‘‰

1 + ğ‘‰ğµğœƒ

1
.
4

We will show that a solution of the ğœ€-StatDist can be converted
into a ğœ€ â€² -satisfying assignment
 âˆ—

ğœ‡ (ğ‘  ğ‘£ )
ğ‘£ â†’ ğ‘¢1
,
ğœƒ
for some appropriate ğœ€ â€² to be defined later.
Case 1: Binary assignment error. First, assume ğº â† (ğœ ||ğ‘£) âˆˆ G
If ğœ = 1, since ğœ‡ âˆ— is a ğœ€ stable distribution we have
ğœ€
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœ‡ âˆ— (ğ‘  base )ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡ âˆ— )| â‰¤
|S|
ğœƒ
ğœ€
ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœ‡ âˆ— (ğ‘  base )
â‰¤
max{ğµ, ğœ‡ âˆ— (ğ‘  base )}
|S|
ğœ€
ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœƒ â‰¤
|S|
ğœ‡ âˆ— (ğ‘  ğ‘£ )
ğœ€
ğœ€
âˆ’1 â‰¤
â‰¤
â‰¤ 8ğœ€,
ğœƒ
ğœƒ |S|
ğœƒğ‘‰
where we used the fact that max{ğµ,ğœ‡ğœƒâˆ— (ğ‘  ) } = ğœ‡ âˆ— (ğ‘  base ). and it
 âˆ—
 base
ğœ‡ (ğ‘  ğ‘£ )
âˆ’ 1| â‰¤ 8ğœ€, since the map ğ‘¢ 1
follows by definition that |ğ‘¢ 1
ğœƒ
is 1-Lipschitz and therefore can only decrease the absolute value
on the left. Likewise, if ğœ = 0,
âˆ‘ï¸
ğœ€
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’
ğœ‡ âˆ— (ğ‘ )ğ‘ƒ (ğ‘  ğ‘£ |ğ‘ , ğœ‡ âˆ— )| â‰¤
|S|
ğ‘ âˆˆS

ğœ€
|S|
ğœ‡ âˆ— (ğ‘  ğ‘£ )
ğœ€
â‰¤
â‰¤ 8ğœ€
ğœƒ
ğœƒ |S|

|ğœ‡ âˆ— (ğ‘  ğ‘£ )| â‰¤

ğ‘  âˆˆS

so ğ‘ƒ (Â·|ğ‘  base, ğœ‡) is a valid probability transition matrix. Finally, the
defined transition probability function ğ‘ƒ is Lipschitz in the components of ğœ‡, and ğ‘ƒ can be defined as a composition of simple
functions, hence ğ‘ƒ âˆˆ P Sim . Finally, in this defined MFG, it holds

â‰¥ğµ=

and once again ğ‘¢ 1

 âˆ—

ğœ‡ (ğ‘  ğ‘£ )
ğœƒ



â‰¤ 8ğœ€.

Case 2: Weighted addition error. Assume that ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£) âˆˆ
G, and set â–¡ := ğ‘¢ğœƒ (ğ›¼ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 1 )) + ğ›½ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 2 ))). Using the fact that
âˆ¥ğœ‡ âˆ— âˆ’ Î“ğ‘ƒ (ğœ‡ âˆ— ) âˆ¥ â‰¤ | Sğœ€ | ,
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’

âˆ‘ï¸

ğœ‡ âˆ— (ğ‘ )ğ‘ƒ (ğ‘  ğ‘£ |ğ‘ , ğœ‡ âˆ— )| â‰¤

ğ‘ âˆˆS

and we define two actions for each state: A = {ğ‘ 1, ğ‘ 0 }. The state
transition probability matrix is given by
ï£±
ï£´
1, if ğ‘ = ğ‘ 1, ğ‘  = ğ‘  ğ‘£,1,
ï£´
ï£²
ï£´
ğ‘ƒ (ğ‘  |ğ‘  ğ‘£,base, ğ‘) = 1, if ğ‘ = ğ‘ 0, ğ‘  = ğ‘  ğ‘£,0,
ï£´
ï£´
ï£´ 0, otherwise.
ï£³

ğœ€
,
|S|

ğ‘¢ (ğ›¼ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 1 )) + ğ›½ğ‘¢ğœƒ (ğœ‡ (ğ‘£ 2 )))
ğœ€
â‰¤
ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœ‡ âˆ— (ğ‘  base ) ğœƒ
,
max{ğµ, ğœ‡ (ğ‘  base )}
|S|
ğœ‡ âˆ— (ğ‘  ğ‘£ ) â–¡
ğœ€
â‰¤
âˆ’
,
ğœƒ
ğœƒ
|S|ğœƒ
which implies
 âˆ—

 âˆ—
 âˆ—



ğœ‡ (ğ‘  ğ‘£ )
ğœ‡ (ğ‘£ 1 )
ğœ‡ (ğ‘£ 2 )
ğ‘¢1
âˆ’ ğ‘¢ 1 ğ›¼ğ‘¢ 1
+ ğ›½ğ‘¢ 1
â‰¤ 8ğœ€.
ğœƒ
ğœƒ
ğœƒ
Case 3: Brittle comparison gadget. Finally, we analyze the
more involved case of the comparison gadget. Assume ğº < (|ğ‘£ 1, ğ‘£ 2 |ğ‘£) âˆˆ
G. The stability conditions for ğ‘  ğ‘£ yield:
ğœ€
|S|
ğœ€
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœƒğ‘ 8ğœ€ (ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ âˆ— (ğ‘£ 1 )), ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ âˆ— (ğ‘£ 2 )))| â‰¤
|S|
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœ‡ âˆ— (ğ‘  base )ğ‘ƒ (ğ‘  ğ‘£ |ğ‘  base, ğœ‡ âˆ— )| â‰¤

ğ‘ƒ (ğ‘  ğ‘£,base |ğ‘ , ğ‘) = 0, âˆ€ğ‘£ âˆˆ V, ğ‘  âˆˆ S, ğ‘ âˆˆ A,
and an ğœ€ satisfying assignment ğ‘ : V â†’ [0, 1] will be read by
1 . We will
ğ‘ (ğ‘£) = ğœ‹ 1âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base ) for the optimal policy ğœ‹ âˆ— = {ğœ‹â„ }â„=0
Simple
specify population-dependent rewards ğ‘… âˆˆ R
, since ğ‘… will not
depend on the particular action but only the state and population
distribution, we will concisely denote ğ‘…(ğ‘ , ğ‘, ğœ‡) = ğ‘…(ğ‘ , ğœ‡). It will be
the case that
ğ‘…(ğ‘  ğ‘£,base, ğœ‡) = 0, âˆ€ğ‘£ âˆˆ V, ğœ‡ âˆˆ Î” S .
We assign ğ‘…(ğ‘  ğ‘£,1, ğœ‡) = ğ‘…(ğ‘  ğ‘£,0, ğœ‡) = 0, âˆ€ğœ‡ for any vertex ğ‘£ of the
generalized circuit that is not the output of any gate in G.
Binary assignment gadget. For any binary assignment gate ğº â† (ğœ ||ğ‘£),
we assign
ğ‘…(ğ‘  ğ‘£,1, ğœ‡) = ğœ ,
ğ‘…(ğ‘  ğ‘£,0, ğœ‡) = 1 âˆ’ ğœ , âˆ€ğœ‡ âˆˆ Î” S .

We analyze two cases: ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 1 )) â‰¥ ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 2 )) + 8ğœ€ and
ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 1 )) â‰¤ ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 2 )) âˆ’ 8ğœ€. In the first case, we obtain

Weighted addition gadget. For any gate ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£),

ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ âˆ— (ğ‘£ 1 )) â‰¥ ğœƒ âˆ’1ğ‘¢ğœƒ (ğœ‡ âˆ— (ğ‘£ 2 )) + 8ğœ€,

ğ‘…(ğ‘  ğ‘£,0, ğœ‡) = ğ‘¢ 1 (ğ‘‰ ğœ‡ (ğ‘  ğ‘£,1 ) âˆ’ ğ‘¢ 1 (ğ›¼ğ‘‰ ğœ‡ (ğ‘  ğ‘£1 ,1 ) + ğ›½ğ‘‰ ğœ‡ (ğ‘  ğ‘£2 ,1 ))),

which implies by the definition of ğ‘ 8ğœ€
ğœ€
|S|
ğœ€
|ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘  ğ‘£ )) âˆ’ 1| â‰¤
|S|ğœƒ
|ğœ‡ âˆ— (ğ‘  ğ‘£ ) âˆ’ ğœƒ | â‰¤

ğœ€
â‰¥ 1 âˆ’ 8ğœ€.
ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘  ğ‘£ )) â‰¥1 âˆ’
|S|ğœƒ
In the second case ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 1 )) â‰¤ ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘£ 2 )) âˆ’ 8ğœ€, it follows
by a similar analysis that
ğ‘¢ 1 (ğœƒ âˆ’1 ğœ‡ âˆ— (ğ‘  ğ‘£ )) â‰¤

for all ğœ‡ âˆˆ Î” S .
Brittle comparison gadget. For any gate ğº < (|ğ‘£ 1, ğ‘£ 2 |ğ‘£), we define
the rewards for states ğ‘  ğ‘£,1, ğ‘  ğ‘£,0 as
ğ‘…(ğ‘  ğ‘£,1, ğœ‡) = ğ‘¢ 1 (ğ‘‰ ğœ‡ (ğ‘  ğ‘£2 ,1 ) âˆ’ ğ‘‰ ğœ‡ (ğ‘  ğ‘£1 ,1 )),
ğ‘…(ğ‘  ğ‘£,0, ğœ‡) = ğ‘¢ 1 (ğ‘‰ ğœ‡ (ğ‘  ğ‘£1 ,1 ) âˆ’ ğ‘‰ ğœ‡ (ğ‘  ğ‘£2 ,1 )), âˆ€ğœ‡ âˆˆ Î” S .
1
Now assume that ğœ‹ âˆ— = {ğœ‹â„âˆ— }â„=0
is a solution to the (ğœ€ 2, 2)-FHâˆ—
2
âˆ—
Nash problem and ğ = Î›ğ‘ƒ,ğœ‡ (ğœ‹ ), that is, assume that for all
0

ğœ‹ âˆˆ Î 2 ,
ğœ€
â‰¤ 8ğœ€.
|S|ğœƒ

Hence, in the above, we reduced the 8ğœ€-GCircuit problem to
the ğœ€-StatDist problem, completing the proof that ğœ€-StatDist
is PPAD-hard. The fact that ğœ€-StatDist is in PPAD on the other
hand easily follows from the fact that ğœ€-StatDist is the fixed point
problem for the (simple) operator Î“ğ‘ƒ , reducing it to the End-ofthe-Line problem by a standard construction [7].

B.3

ğ‘…(ğ‘  ğ‘£,1, ğœ‡) = ğ‘¢ 1 (ğ‘¢ 1 (ğ›¼ğ‘‰ ğœ‡ (ğ‘  ğ‘£1 ,1 ) + ğ›½ğ‘‰ ğœ‡ (ğ‘  ğ‘£2 ,1 )) âˆ’ ğ‘‰ ğœ‡ (ğ‘  ğ‘£,1 )),

Proof of Intractability of FH-MFG

As in the previous section, we reduce any ğœ€-GCircuit problem
(G, V) to the problem (ğœ€ 2, 2)-FH-Nash for some simple reward
ğ‘… âˆˆ R Sim . Once again let ğ‘‰ = |V |.
Associated with each ğ‘£ âˆˆ V we define ğ‘  ğ‘£,1, ğ‘  ğ‘£,0, ğ‘  ğ‘£,base âˆˆ S. The
initial distribution is defined as
1
ğœ‡ 0 (ğ‘  ğ‘£,base ) = , âˆ€ğ‘£ âˆˆ V,
ğ‘‰

ğœ€2
.
ğ‘‰
Firstly, if ğœ‡ 1âˆ— is induced by ğœ‹ âˆ— , it holds that âˆ€ğ‘£ âˆˆ V,
ğ»
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ âˆ— ) â‰¤

ğœ‡ 1âˆ— (ğ‘  ğ‘£,base ) = 0,

1 âˆ—
ğœ‹ (ğ‘  ğ‘£,1 |ğ‘  ğ‘£,base ),
ğ‘‰ 0
1 âˆ’ ğœ‹0âˆ— (ğ‘  ğ‘£,1 |ğ‘  ğ‘£,base )
ğœ‡ 1âˆ— (ğ‘  ğ‘£,0 ) =
.
ğ‘‰
ğœ‡1âˆ— (ğ‘  ğ‘£,1 ) =

Furthermore, a policy ğœ‹ br âˆˆ Î 2 that is the best response to
âˆ—
ğ := {ğœ‡ 0âˆ—, ğœ‡1âˆ— } can be always formulated as:
(
ğœ‹0br (ğ‘ 1 |ğ‘  ğ‘£,base ) =

1, if ğ‘…(ğ‘  ğ‘£,1, ğœ‡1âˆ— ) > ğ‘…(ğ‘  ğ‘£,1, ğœ‡1âˆ— ),
0, otherwise

ğœ‹ 0br (ğ‘ 0 |ğ‘  ğ‘£,base ) = 1 âˆ’ ğœ‹ 0br (ğ‘ 1 |ğ‘  ğ‘£,base ),
ğœ‹ 1br (ğ‘ 1 |ğ‘  ğ‘£,base ) = 1,
ğœ‹ 1br (ğ‘ 0 |ğ‘  ğ‘£,base ) = 0.

By the optimality conditions, we will have
ğ»
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ br ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ âˆ— ) â‰¤

ğœ€2
.
ğ‘‰

Furthermore, for any ğ‘£ âˆˆ V it holds that
ğ»
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ br ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ âˆ— )
âˆ‘ï¸
=
ğœ‡ 0 (ğ‘  ğ‘£,base ) [ max ğ‘…(ğ‘ , ğœ‡ 1âˆ— )
ğ‘£âˆˆV

ğ‘  âˆˆ {ğ‘  ğ‘£,1 ,ğ‘  ğ‘£,0 }

Case 3. Finally, for any ğ‘£ âˆˆ V such that ğº < (|ğ‘£ 1, ğ‘£ 2 |ğ‘£) âˆˆ G,
n
o
1
max ğ‘¢ 1 (ğœ‡ (ğ‘  ğ‘£2 ,1 ) âˆ’ ğœ‡ (ğ‘  ğ‘£1 ,1 )), ğ‘¢ 1 (ğœ‡ (ğ‘  ğ‘£1 ,1 ) âˆ’ ğœ‡ (ğ‘  ğ‘£2 ,1 ))
ğ‘‰
1
âˆ’ ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğ‘¢ 1 (ğœ‡ (ğ‘  ğ‘£1 ,1 ) âˆ’ ğœ‡ (ğ‘  ğ‘£2 ,1 ))
ğ‘‰
1
âˆ’ ğœ‹ 0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )ğ‘¢ 1 (ğœ‡ (ğ‘  ğ‘£2 ,1 ) âˆ’ ğœ‡ (ğ‘  ğ‘£1 ,1 )) â‰¤ ğœ€
ğ‘‰
hence once again using the shorthand notation:

âˆ’ ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğ‘…(ğ‘  ğ‘£,1, ğœ‡1âˆ— ) âˆ’ ğœ‹ 0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )ğ‘…(ğ‘  ğ‘£,0, ğœ‡1âˆ— )]
1
max ğ‘…(ğ‘ , ğœ‡ 1âˆ— )
â‰¥
ğ‘‰ ğ‘  âˆˆ {ğ‘  ğ‘£,1 ,ğ‘  ğ‘£,0 }
1
1
âˆ’ ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğ‘…(ğ‘  ğ‘£,1, ğœ‡1âˆ— ) âˆ’ ğœ‹ 0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )ğ‘…(ğ‘  ğ‘£,0, ğœ‡1âˆ— )
ğ‘‰
ğ‘‰
as the summands are all positive. We prove that all gate conditions
are satisfied case by base. Without loss of generality, we assume
ğœ€ < 1 below.
Case 1. It follows that for any ğ‘£ âˆˆ V such that ğº â† (ğœ ||ğ‘£) âˆˆ G,
we have
1
1
1
ğœ€2
âˆ’ ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğœ âˆ’ ğœ‹0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )(1 âˆ’ ğœ ) â‰¤
ğ‘‰ ğ‘‰
ğ‘‰
ğ‘‰
1 âˆ’ ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğœ âˆ’ (1 âˆ’ ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base ))(1 âˆ’ ğœ ) â‰¤ ğœ€ 2
ğœ (1 âˆ’ 2ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )) + ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base ) â‰¤ ğœ€ 2 â‰¤ ğœ€.
The above implies ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base ) â‰¥ 1 âˆ’ ğœ€ if ğœ = 1, and if ğœ = 0, it
implies ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base ) â‰¤ ğœ€.
Case 2. For any ğ‘£ âˆˆ V such that ğº Ã—,+ (ğ›¼, ğ›½ |ğ‘£ 1, ğ‘£ 2 |ğ‘£) âˆˆ G, denoting in short
â–¡ := ğ‘¢ 1 (ğ›¼ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£1 ,1 ) + ğ›½ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£2 ,1 ))
= ğ‘¢ 1 (ğ›¼ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£1 ,1 ) + ğ›½ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£2 ,1 )),

â–³ := ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£2 ,1 ) âˆ’ ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£1 ,1 ) = ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£2 ,1 ) âˆ’ ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£1 ,1 )
ğ‘ 1 := ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )
ğ‘ 0 := ğœ‹0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )
we have the inequality:
ğ‘¢ 1 (|â–³|) âˆ’ ğ‘ 1ğ‘¢ 1 (â–³) âˆ’ ğ‘ 0ğ‘¢ 1 (âˆ’â–³) â‰¤ ğœ€ 2
ğ‘¢ 1 (|â–³|) âˆ’ ğ‘ 1ğ‘¢ 1 (â–³) âˆ’ (1 âˆ’ ğ‘ 1 )ğ‘¢ 1 (âˆ’â–³) â‰¤ ğœ€ 2 .
First assume â–³ â‰¥ ğœ€, then
ğ‘¢ 1 (â–³)(1 âˆ’ ğ‘ 1 ) â‰¤ ğœ€ 2 =â‡’ 1 âˆ’ ğœ€ â‰¤ ğ‘ 1,
and conversely if â–³ â‰¤ âˆ’ğœ€,
ğ‘¢ 1 (âˆ’â–³)ğ‘ 1 â‰¤ ğœ€ 2 =â‡’ ğ‘ 1 â‰¤ ğœ€,
concluding that the comparison gate conditions are ğœ€ satisfied for
the assignment ğ‘£ â†’ ğœ‹0br (ğ‘ 1 |ğ‘  ğ‘£,base ).
The three cases above conclude that ğ‘£ â†’ ğœ‹0br (ğ‘ 1 |ğ‘  ğ‘£,base ) is an ğœ€satisfying assignment for the generalized circuit (V, G), concluding
the proof that (ğœ€ 0, 2)-FH-Nash is PPAD-hard for some ğœ€ 0 > 0. The
fact that (ğœ€ 0, 2)-FH-Nash is in PPAD follows from the fact that the
NE is a fixed point of a simple map on space Î 2 , see for instance
[15].

ğ‘ 1 := ğœ‹0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )

B.4

ğ‘ 0 := ğœ‹0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )
we have

1
max ğ‘¢ 1 (ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£,1 ) âˆ’ â–¡), ğ‘¢ 1 (â–¡ âˆ’ ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£,1 ))
ğ‘‰
1
âˆ’ ğœ‹ 0âˆ— (ğ‘ 1 |ğ‘  ğ‘£,base )ğ‘¢ 1 (â–¡ âˆ’ ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£,1 ))
ğ‘‰
1
âˆ’ ğœ‹ 0âˆ— (ğ‘ 0 |ğ‘  ğ‘£,base )ğ‘¢ 1 (ğ‘‰ ğœ‡1âˆ— (ğ‘  ğ‘£,1 ) âˆ’ â–¡) â‰¤ ğœ€ 2,
ğ‘‰
or equivalently

max ğ‘¢ 1 (ğ‘ 1 âˆ’ â–¡),ğ‘¢ 1 (â–¡ âˆ’ ğ‘ 1 ) âˆ’ ğ‘ 1ğ‘¢ 1 (â–¡ âˆ’ ğ‘ 1 ) âˆ’ ğ‘ 0ğ‘¢ 1 (ğ‘ 1 âˆ’ â–¡) â‰¤ ğœ€ 2 .
First, assume it holds that ğ‘ 1 â‰¤ â–¡, then:
ğ‘¢ 1 (â–¡ âˆ’ ğ‘ 1 ) âˆ’ ğ‘ 1ğ‘¢ 1 (â–¡ âˆ’ ğ‘ 1 ) â‰¤ğœ€ 2

Proof of Intractability of 2-FH-Linear

Our reduction will be similar to the previous section, however,
instead of reducing a ğœ€-GCircuit to an MFG, we will reduce a 2
player general sum normal form game, 2-Nash, to a finite horizon
mean field game with linear rewards with horizon ğ» = 2 (2-FHLinear). Let ğœ€ > 0, ğ¾1, ğ¾2 âˆˆ N>0, ğ´, ğµ âˆˆ Rğ¾1 ,ğ¾2 be given for a
2-Nash problem. We assume without loss of generality that ğ¾1 > 1,
as otherwise, the solution of 2-Nash is trivial.
This time, we define finite horizon game with ğ¾1 + ğ¾2 + 2 states,
1 , ğ‘  2 , ğ‘  1 , . . . , ğ‘  1 , ğ‘  2 , . . . , ğ‘  2 }. Without loss of
denoted S := {ğ‘  base
ğ¾1 1
ğ¾2
base 1
generality, we can assume ğ¾1 â‰¤ ğ¾2 . The action set will be defined
by A = [ğ¾2 ] = {1, . . . , ğ¾2 }. The initial state distribution will be
1 ) = ğœ‡ (ğ‘  2 ) = 1/2, with ğœ‡ (ğ‘ ) = 0 for all other
given by ğœ‡ 0 (ğ‘  base
0 base
0
states. We define the transitions for any ğ‘  âˆˆ S, ğ‘, ğ‘ â€² âˆˆ A as:

(1 âˆ’ ğ‘ 1 )(â–¡ âˆ’ ğ‘ 1 ) â‰¤ğœ€ 2 .

ï£± 1, if ğ‘  = ğ‘  1 and ğ‘ â‰¤ ğ¾ ,
ï£´
1
ï£´
ğ‘
ï£²
ï£´
1
ğ‘ƒ (ğ‘  |ğ‘  base, ğ‘) = 1, if ğ‘  = ğ‘ ğ‘1 and ğ‘ > ğ¾1,
ï£´

The above implies that either ğ‘ 1 â‰¥ 1 âˆ’ ğœ€ or ğ‘¢ 1 (â–¡ âˆ’ ğ‘ 1 ) â‰¤ ğœ€, both
cases implying |â–¡ âˆ’ ğ‘ 1 | â‰¤ ğœ€ since we assume â–¡ â‰¥ ğ‘ 1 . To conclude
case 2, assume that â–¡ < ğ‘ 1 , then

ï£´
ï£´ 0, otherwise.
ï£³
(
1, if ğ‘  = ğ‘ ğ‘2 ,
2
ğ‘ƒ (ğ‘  |ğ‘  base, ğ‘) =
0, otherwise.

ğ‘¢ 1 (ğ‘ 1 âˆ’ â–¡) âˆ’ (1 âˆ’ ğ‘ 1 )ğ‘¢ 1 (ğ‘ 1 âˆ’ â–¡) â‰¤ ğœ€ 2,
ğ‘ 1 (ğ‘ 1 âˆ’ â–¡) â‰¤ ğœ€ 2,
then either ğ‘ 1 â‰¤ ğœ€ or ğ‘ 1 âˆ’ â–¡ â‰¤ ğœ€, either case implying once again
|â–¡ âˆ’ ğ‘ 1 | â‰¤ ğœ€.

(
ğ‘ƒ (ğ‘  |ğ‘ ğ‘1 , ğ‘ â€² ) =

1, if ğ‘  = ğ‘ ğ‘1 ,
0, otherwise.

(
ğ‘ƒ (ğ‘  |ğ‘ ğ‘2 , ğ‘ â€² ) =

1, if ğ‘  = ğ‘ ğ‘2 ,
0, otherwise.

Finally, we will define the linear reward function as for all ğ‘ âˆˆ [ğ¾2 ]:

ğ» (ğ âˆ— , ğœ‹ ) simplifies to
Case 1. Assume ğ¾1 = ğ¾2 . Then, ğ‘‰ğ‘ƒ,ğ‘…

1 1 âˆ‘ï¸
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ ) = +
2 8

1
ğ‘…(ğ‘  base
, ğ‘, ğœ‡) = 0,
2
ğ‘…(ğ‘  base
, ğ‘, ğœ‡) = 0,

(

+

0, if ğ‘ > ğ¾1,
1 + 1Ã â€²
2
â€²
2
2 ğ‘ âˆˆ [ğ¾2 ] ğœ‡ (ğ‘ ğ‘ â€² )ğ´ğ‘,ğ‘
âˆ‘ï¸
1
1
ğ‘…(ğ‘ ğ‘2 , ğ‘, ğœ‡) = +
ğœ‡ (ğ‘ ğ‘1â€² )ğµğ‘ â€² ,ğ‘ .
2 2 â€²

1 âˆ‘ï¸
8

âˆ‘ï¸

2
1
ğœ‹0 (ğ‘ â€² |ğ‘  base
)ğœ‹ 0âˆ— (ğ‘|ğ‘  base
)ğµğ‘,ğ‘ â€² .

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

(13)
Take an arbitrary mixed strategy ğœ1 âˆˆ Î” [ğ¾1 ] and define the policy
1
ğœ‹ ğ´ = {ğœ‹ğ´,â„ }â„=0
âˆˆ Î 2 so that

ğ‘ âˆˆ [ğ¾1 ]

1 , ğ‘ 2
In words, the states ğ‘  base
represent the two players of the
base
2-Nash, and an agent starting from one of the initial base states
1 , ğ‘ 2
ğ‘  base
of the FH-MFG at round â„ = 0 will be placed at â„ = 1 at a
base
state representing the (pure) strategies of each player respectively.
1
Given the game description above, assume ğœ‹ âˆ— = {ğœ‹â„âˆ— }â„=0
is
an ğœ€ solution of the 2-FH-Linear. Then, it holds for the induced
1
distribution ğ âˆ— := {ğœ‡â„âˆ— }â„=0
= Î›ğ»
ğ‘ƒ that:

2
2
ğœ‹ğ´,0 (ğ‘  base
) = ğœ‹ 0âˆ— (ğ‘  base
),

1
ğœ‹ğ´,0 (ğ‘  base
) = ğœ1,

ğœ‹ğ´,1 = ğœ‹ 1âˆ— .

Then, placing ğœ‹ ğ´ in equations (13) and (12), it follows that
âˆ‘ï¸
âˆ‘ï¸
2
ğœ1 (ğ‘)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²
ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

âˆ’

âˆ‘ï¸

âˆ‘ï¸

1
2
ğœ‹0âˆ— (ğ‘|ğ‘  base
)ğœ‹ 0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€² â‰¤ 8ğœ€.

(14)

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

ğœ‡ 0âˆ— = ğœ‡ 0,
âˆ‘ï¸

Similarly, for any ğœ2 âˆˆ Î” [ ğ¾2 ], replacing ğœ‹ in equations (13) and
(12) with a policy ğœ‹ ğµ such that

ğœ‡0 (ğ‘  â€² )ğœ‹ âˆ— (ğ‘ â€² |ğ‘  â€² )ğ‘ƒ (ğ‘  |ğ‘  â€², ğ‘ â€² )

ğ‘  â€² ,ğ‘ â€² âˆˆ SÃ— A

1
1
ğœ‹ğµ,0 (ğ‘  base
) = ğœ‹ 0âˆ— (ğ‘  base
),

1
1
1
ï£±
ï£´
2 ğœ‹ 0 (ğ‘– |ğ‘  base ), if ğ‘  = ğ‘ ğ‘– , for some ğ‘– âˆˆ [ğ¾1 ],
ï£´
ï£´
ï£´
ï£² 1 ğœ‹0 (ğ‘– |ğ‘  2 ), if ğ‘  = ğ‘  2 , for some ğ‘– âˆˆ [ğ¾2 ],
ï£´
ğ‘–
= 21 1 Ãbase
1 ), if ğ‘  = ğ‘  1 ,
ï£´
âˆ’
ğœ‹
(ğ‘–
|ğ‘ 
0
ï£´
ğ‘–
âˆˆ
[ğ¾
]
2
2
1
base
base
ï£´
ï£´
ï£´ 0, otherwise.
ï£³

we obtain
âˆ‘ï¸
âˆ‘ï¸

âˆ’

ğœ‹ â€² âˆˆÎ ğ»

in particular, it holds for any ğœ‹ âˆˆ Î 2 that
(12)

ğ» can be written
By direct computation, the value functions ğ‘‰ğ‘ƒ,ğ‘…
directly in this case for any ğœ‹:

1 âˆ‘ï¸
Â© 1 1 âˆ‘ï¸ âˆ— 2
Âª
1
ğœ‹0 (ğ‘|ğ‘  base
)Â­ +
ğœ‡ 1 (ğ‘ ğ‘ â€² )ğ´ğ‘,ğ‘ â€² Â®
2
2 2 â€²
ğ‘âˆˆ [ğ¾1 ]
ğ‘ âˆˆ [ğ¾2 ]
Â«
Â¬
âˆ‘ï¸
âˆ‘ï¸
1
Â©1 1
Âª
2
)Â­ +
+
ğœ‹ 0 (ğ‘ â€² |ğ‘  base
ğœ‡1âˆ— (ğ‘ ğ‘1 )ğµğ‘,ğ‘ â€² Â®
2 â€²
2 2
ğ‘ âˆˆ [ğ¾2 ]
ğ‘âˆˆ [ğ¾1 ]
Â«
Â¬
âˆ‘ï¸
1Â©
Âª
1
= Â­1 +
ğœ‹ 0 (ğ‘|ğ‘  base
)Â®
4
Â¬
Â« ğ‘âˆˆ [ğ¾1 ]
âˆ‘ï¸
1 âˆ‘ï¸
1
2
)ğœ‹ 0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²
+
ğœ‹0 (ğ‘|ğ‘  base
8
â€²

ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ ) =

ğ‘âˆˆ [ğ¾1 ] ğ‘ âˆˆ [ğ¾2 ]

1 âˆ‘ï¸
8

âˆ‘ï¸

1
ğœ2 (ğ‘)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğµğ‘,ğ‘ â€²

âˆ‘ï¸

âˆ‘ï¸

2
1
)ğµğ‘,ğ‘ â€² â‰¤ 8ğœ€.
ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğœ‹0âˆ— (ğ‘|ğ‘  base

(15)

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

ğ»
ğ»
âˆ—
â€²
ğ»
ğ» âˆ—
Eğ‘ƒ,ğ‘…
(ğœ‹ âˆ— ) := max ğ‘‰ğ‘ƒ,ğ‘…
(Î›ğ»
ğ‘ƒ (ğœ‹ ), ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘… (Î›ğ‘ƒ (ğœ‹ ), ğœ‹ ) â‰¤ ğœ€,

ğ»
ğ»
ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ ) âˆ’ ğ‘‰ğ‘ƒ,ğ‘…
(ğ âˆ—, ğœ‹ âˆ— ) â‰¤ ğœ€.

ğœ‹ğµ,1 = ğœ‹ 1âˆ—,

2
ğœ‹ğµ,0 (ğ‘  base
) = ğœ2,

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

By definition of the ğœ€ finite horizon Nash equilibrium,

+

1
2
ğœ‹0 (ğ‘|ğ‘  base
)ğœ‹ 0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

ğ‘…(ğ‘ ğ‘1 , ğ‘, ğœ‡) =

ğœ‡ 1âˆ— (ğ‘ ) =

âˆ‘ï¸

2
1
ğœ‹0 (ğ‘ â€² |ğ‘  base
)ğœ‹0âˆ— (ğ‘|ğ‘  base
)ğµğ‘,ğ‘ â€²

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

We analyze two different cases, accounting for a possible imbalance
between the strategy spaces of the two players, [ğ¾1 ] and [ğ¾2 ].

Hence, the resulting equations (14), (15) imply that in this case the
1 ), ğœ‹ âˆ— (ğ‘  2 )) is a 8ğœ€-Nash equilibrium for
strategy profile (ğœ‹0âˆ— (ğ‘  base
0 base
the normal form game defined by matrices ğ´, ğµ.
Case 2. Next, we analyze the case when 1 < ğ¾1 < ğ¾2 . If
Ã
âˆ— â€² 1
ğ‘ â€² âˆˆ [ğ¾1 ] ğœ‹ 0 (ğ‘ |ğ‘  base ) = 0, then the policy
1
ğœ‹0â€² (1|ğ‘  base
) = 1,

2
2
ğœ‹0â€² (ğ‘  base
) = ğœ‹ 0âˆ— (ğ‘  base
),

ğœ‹1â€² = ğœ‹1âˆ— .

yields an exploitability of at least 1/4, so by taking ğœ€ smaller than 1/4
we can discard this possibility.
1
Otherwise, we define a policy ğœ‹ ğ¶ = {ğœ‹ğ¶,â„ }â„=0
âˆˆ Î  2 such that
1 )
ğœ‹ 0âˆ— (ğ‘ |ğ‘  base
âˆ— (ğ‘ â€² |ğ‘  1 )
ğœ‹
â€²
ğ‘ âˆˆ [ğ¾1 ] 0
base

ï£±
ï£´
ï£²Ã
ï£´

1
ğœ‹ğ¶,0 (ğ‘|ğ‘  base
)=
ï£´

, if ğ‘ âˆˆ [ğ¾1 ],

ï£´ 0, otherwise.
ï£³
2
2
ğœ‹ğ¶,0 (ğ‘  base
) = ğœ‹0âˆ— (ğ‘  base
),

ğœ‹ğ¶,1 = ğœ‹1âˆ—,

and replace ğœ‹ in Equation (12) with ğœ‹ ğ¶ to obtain:
1 1
âˆ’ ğ‘†
4 4
 âˆ‘ï¸
1  âˆ’1
ğ‘† âˆ’1
+
8

âˆ‘ï¸

1
2
)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€² â‰¤ ğœ€
ğœ‹0âˆ— (ğ‘|ğ‘  base

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

where ğ‘† :=

âˆ— â€² 1
ğ‘ â€² âˆˆ [ğ¾1 ] ğœ‹ 0 (ğ‘ |ğ‘  base ) < 1, hence

Ã

1âˆ’ğ‘† =

âˆ‘ï¸
ğ‘ â€² âˆˆ [ğ¾2 ]âˆ’[ğ¾1 ]

1
ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
) â‰¤ 4ğœ€.

Now for some ğœ1 âˆˆ Î” [ğ¾1 ] , once again take the policy ğœ‹ ğ´ defined
in Case 1, and use Inequality (12) to obtain:
âˆ‘ï¸
1 âˆ‘ï¸
1
2
(1 âˆ’ ğ‘†)+
ğœ1 (ğ‘)ğœ‹ 0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²
4
8
â€²
ğ‘âˆˆ [ğ¾1 ] ğ‘ âˆˆ [ğ¾2 ]

âˆ‘ï¸
1 âˆ‘ï¸
1
2
âˆ’
ğœ‹0âˆ— (ğ‘|ğ‘  base
)ğœ‹ 0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€² â‰¤ ğœ€
8
ğ‘âˆˆ [ğ¾2 ] ğ‘ â€² âˆˆ [ğ¾2 ]
âˆ‘ï¸
âˆ‘ï¸
2
ğœ1 (ğ‘)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²
ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

âˆ’

âˆ‘ï¸

âˆ‘ï¸

1
2
ğœ‹ 0âˆ— (ğ‘|ğ‘  base
)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€² â‰¤ 8ğœ€.

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]
1 ) â‰¥ ğœ‹ âˆ— (ğ‘|ğ‘  1 ) for
Here, using the definition of ğœ‹ ğ¶ , as ğœ‹ğ¶,0 (ğ‘|ğ‘  base
0
base
ğ‘ âˆˆ [ğ¾1 ], we obtain:
âˆ‘ï¸
âˆ‘ï¸
2
ğœ1 (ğ‘)ğœ‹ğ¶,0 (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€²
â€²
ğ‘âˆˆ [ğ¾1 ] ğ‘ âˆˆ [ğ¾2 ]

âˆ’

âˆ‘ï¸

âˆ‘ï¸

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

1
2
ğœ‹ğ¶,0 (ğ‘|ğ‘  base
)ğœ‹ğ¶,0 (ğ‘ â€² |ğ‘  base
)ğ´ğ‘,ğ‘ â€² â‰¤ 8ğœ€.

Next take ğœ‹ ğµ as defined above in Case 1 for any arbitrary ğœ2 âˆˆ
Î” [ğ¾2 ] and use the Inequality 12:
âˆ‘ï¸
âˆ‘ï¸
1
ğœ2 (ğ‘ â€² )ğœ‹0âˆ— (ğ‘|ğ‘  base
)ğµğ‘,ğ‘ â€²
ğ‘ â€² âˆˆ [ğ¾2 ] ğ‘âˆˆ [ğ¾1 ]

âˆ’

âˆ‘ï¸

âˆ‘ï¸

1
2
ğœ‹ 0âˆ— (ğ‘|ğ‘  base
)ğœ‹0âˆ— (ğ‘ â€² |ğ‘  base
)ğµğ‘,ğ‘ â€² â‰¤ 8ğœ€

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

âˆ‘ï¸

âˆ‘ï¸

1
ğœ2 (ğ‘ â€² )ğœ‹ğ¶,0 (ğ‘|ğ‘  base
)ğµğ‘,ğ‘ â€²

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

âˆ’

âˆ‘ï¸

âˆ‘ï¸

ğ‘âˆˆ [ğ¾1 ] ğ‘ â€² âˆˆ [ğ¾2 ]

1
2
ğœ‹ğ¶,0 (ğ‘|ğ‘  base
)ğœ‹ğ¶,0 (ğ‘ â€² |ğ‘  base
)ğµğ‘,ğ‘ â€² â‰¤

8ğœ€
8ğœ€
â‰¤
.
ğ‘†
1 âˆ’ 4ğœ€

Assuming without loss of generality that ğœ€ < 18 , it follows that
1 ), ğœ‹
2
ğœ‹ğ¶,0 (ğ‘  base
ğ¶,0 (ğ‘  base ) is a 16ğœ€ solution to the 2-Nash.

