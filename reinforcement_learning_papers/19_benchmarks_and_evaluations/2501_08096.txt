1

Hybrid Action Based Reinforcement Learning for
Multi-Objective Compatible Autonomous Driving

arXiv:2501.08096v3 [cs.RO] 19 Aug 2025

Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, and Chen Sun

Abstract‚ÄîReinforcement Learning (RL) has shown excellent
performance in solving decision-making and control problems
of autonomous driving, which is increasingly applied in diverse
driving scenarios. However, driving is a multi-attribute problem,
leading to challenges in achieving multi-objective compatibility
for current RL methods, especially in both policy updating and
policy execution. On the one hand, a single value evaluation
network limits the policy updating in complex scenarios with
coupled driving objectives. On the other hand, the common
single-type action space structure limits driving flexibility or
results in large behavior fluctuations during policy execution.
To this end, we propose a Multi-objective Ensemble-Critic
reinforcement learning method with Hybrid Parametrized Action
for multi-objective compatible autonomous driving. Specifically,
an advanced MORL architecture is constructed, in which the
ensemble-critic focuses on different objectives through independent reward functions. The architecture integrates a hybrid
parameterized action space structure, and the generated driving
actions contain both abstract guidance that matches the hybrid
road modality and concrete control commands. Additionally, an
uncertainty-based exploration mechanism that supports hybrid
actions is developed to learn multi-objective compatible policies
more quickly. Experimental results demonstrate that, in both
simulator-based and HighD dataset-based multi-lane highway
scenarios, our method efficiently learns multi-objective compatible autonomous driving with respect to efficiency, action
consistency, and safety.
Index Terms‚ÄîReinforcement learning, autonomous driving,
motion planning, hybrid action.

I. I NTRODUCTION
Reinforcement learning (RL) has good potential in solving temporal decision-making problems [1], which can learn
viable and near-optimal policies for complex tasks [2]. The
RL agent explores policies through interactions with the environment, enabling self-improvement [3], [4]. Therefore, RL
is considered as an effective way to solve decision-making
and control problems for autonomous driving (AD) [5]. It has
led to widespread application in driving scenarios [6] and has
outperformed human drivers in certain tasks [7].
However, current RL methods still face several limitations
in achieving compatibility with key driving objectives such as
safety, efficiency, and action consistency [8], [9]. In particular,
Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han and Lu Xiong are
with the School of Automotive Studies, Tongji University, Shanghai
201804, China. (Email: jgz13573016892@163.com, 1911055@tongji.edu.cn,
lengbo@tongji.edu.cn, tjhanwei@foxmail.com, xiong lu@tongji.edu.cn).
Chen Sun is with the Department of Data and Systems Engineering,
University of Hong Kong, Hong Kong. (Email: c87sun@hku.hk).
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.

when addressing multi-attribute driving tasks, mainstream RLbased AD approaches exhibit shortcomings in both policy
update and execution: (i) For policy updates, most rely on
a single critic (value network) to evaluate and guide learning, making it difficult to efficiently explore multi-objectivecompatible policies within a large and complex traffic state
space; (ii) For policy execution, most approaches employ
a single-type action space structure to handle hybrid road
modality, which limits the policy‚Äôs ability to fully represent
real driving behaviors and forces a trade-off among certain
objectives.
In terms of policy update, employing a single critic (i.e.,
a single reward function) to evaluate policy performance
fails to capture the strong coupling and potential conflicts
among driving objectives. When multiple attributes of an
AD task are combined into a single reward function, the
agent may disproportionately focus on certain attributes during
training [8]. As a result, some objectives may be neglected
in specific states, leading to inaccurate value estimation and
suboptimal policy performance. This can cause the agent
to behave in ways that are misaligned with multi-objective
expectations, such as becoming overly aggressive to maximize
speed or excessively conservative to ensure safety. In contrast,
multi-objective reinforcement learning (MORL) addresses this
issue more effectively by constructing reward function vectors
[10], enabling better compatibility among multiple objectives.
Furthermore, complex traffic state spaces demand efficient
exploration during policy updates. Most existing RL-based AD
methods rely on random exploration, which prevents the agent
from actively seeking out unknown regions and discovering
potentially effective policies [11]. This random exploration
often results in the collection of redundant experiences that
contribute little to policy improvement, leading to inefficient
convergence or even entrapment in local optima.
For policy execution, using a single-type action space to
generate either abstract or concrete driving behaviors constrains RL agents to discrete actions that lack flexibility or continuous actions that lack consistency. A common approach is to
have the agent produce discrete, long-term driving goals, such
as semantic decisions [12] or target points for path planning
[13]. However, because the agent does not directly control the
vehicle‚Äôs motion, its ability to adapt driving behavior flexibly
is limited. While this long-term planning enhances action
consistency, it reduces responsiveness to dynamic changes.
Conversely, directly outputting short-term control commands
[14] allows for greater flexibility, but often results in less
consistent behavior, with frequent fluctuations and abrupt
reactions to environmental changes.

2

To alleviate the limitations of policy updating and execution in typical multi-objective AD tasks, this paper proposes a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action space (HPAMoEC) for multi-objective compatibility. The HPA-MoEC
adopts a MORL architecture focused on AD tasks. By defining multiple reward functions to decouple different driving
attributes, each reward function guides an ensemble-critic to
focus on specific driving objectives, thereby assisting the
actor (policy network) in learning multi-objective compatible
driving behaviors. The architecture further integrates a hybrid
parameterized action space structure containing a discrete
action set and its corresponding continuous parameters, which
together generate driving actions that combine abstract guidance and concrete control commands. Additionally, uncertainty
estimates from the ensemble-critic guide the agent to explore
promising driving policies, facilitating more efficient exploration in unknown environments. Evaluation results on both
simulation and HighD dataset-based multi-lane highway scenarios demonstrate that HPA-MoEC efficiently learns multiobjective compatible driving behaviors, significantly improving driving efficiency, action consistency, and safety. The main
contributions are summarized as follows:
1) A MORL architecture compatible with multiple AD
objectives is proposed, in which ensemble-critic focuses
on a distinct objective using separate reward functions.
Considering the safety-critical nature of AD, two driving
objectives are defined and evaluated with two ensemble
critics: one targeting overall performance, including interactivity, and the other dedicated to safety. By isolating
the safety objective, the effectiveness of our architecture
is demonstrated through improved safety performance in
experimental results.
2) A hybrid parameterized action space structure is designed to combine finer-grained guidance and control
commands to adapt to hybrid road modality. This hybrid action space consists of discrete actions and their
corresponding continuous parameters, which together
generate both abstract guidance and concrete control
outputs. Our design achieves greater driving flexibility
and reduced behavioral fluctuations, ensuring compatibility between driving efficiency and action consistency.
3) An epistemic uncertainty-based exploration mechanism
is developed to enhance learning efficiency and complement the hybrid action space structure. By dynamically
adjusting the direction and magnitude of exploration
according to uncertainty and its trends, the agent is
encouraged to more rapidly explore high-uncertainty regions for potentially effective policies. This exploration
mechanism significantly improves the learning efficiency
of multi-objective compatible policies.
The remainder of this paper is organized as follows: Section II reviews related work, while Section III outlines the
methodology. Specific implementation details are presented in
Section IV. Section V discusses the experimental results, and
the conclusions are provided in Section VI.

II. R ELATED W ORKS
The AD task involves making complex sequential decisions
in a dynamic environment and can therefore be modeled
as Markov Decision Processes (MDPs) [15]. The MDP is
commonly represented as a tuple < S, A, R, T , Œ≥ > , where
S is the state space, A is the action space, R is the reward
function, T is the transition function, and Œ≥ is the discount
factor. At time t, the RL agent selects action at ‚àà A based
on state st ‚àà S, then receives reward rt ‚àà R from the
environment and transitions to state st+1 according to T . The
goal of the agent is to find an optimal policy through trialand-error to maximize the expected reward.
A. Multi-Objective Policy Evaluation
For AD problems, multiple attitudes should be considered,
requiring the policy to be multi-objective compatible. The
objectives are sometimes conflicting, like safety and driving
efficiency [6]. The most common design is to linearly combine
all attributes into a single, additive reward function for policy
evaluation [16], typically based on mainstream RL algorithms such as Deep Q-Network (DQN) [17]and Soft ActorCritic (SAC) [18]. Specifically, the weights of this linearly
expressed reward function are typically determined through
manual design after multiple trial-and-error iterations [19],
or by applying Inverse-RL to human demonstrations [20].
However, policy evaluation under this linear assumption may
be inaccurate because the highest rewarding action may not be
the one that enables multi-objective compatible driving [10],
[21], leading to reduced policy performance [22]. Additionally,
a single critic representing multiple attitude rewards forces the
learning of value coherence, which may not accurately reflect
the true critic and degrade policy quality [23].
The MORL has recently attracted significant attention for
its ability to address complex decision-making problems involving multiple, often conflicting, objectives [21]. A typical
MORL approach employs an architecture with multiple critics
to enable multi-objective compatible policy updates [10], [24]‚Äì
[26]. Specifically, key attributes are separated from a single reward function by defining multiple reward functions, with each
attribute treated as an independent evaluation objective [10].
Several AD-related studies have demonstrated the advantages
of MORL in driving tasks by incorporating objectives such
as safety [24], efficiency [27], and comfort [25]. Additionally,
[26] introduces a pre-trained safe-critic to guide the policy
towards safer actions. Building on these ideas, we propose
HPA-MoEC, an advanced MORL architecture that integrates
a novel hybrid action space and introduces epistemic uncertainty via ensemble-critic to enhance policy exploration and
learning capability. Compared to previous MORL methods,
HPA-MoEC achieves better and faster multi-objective learning
in hybrid road modality.
B. Action Space Structure
Many current RL-based AD methods use a single action
type to control vehicle, which fail to be compatible with high
driving flexibility and small behavior fluctuations. On one

3

hand, some studies use a discrete action space to generate
abstract behavior decisions, offering long-term targets that indirectly guide vehicle control. Specifically, [28], [29] use DQN
and its improved versions to generate semantic lateral actions,
such as left or right lane changes. Additionally, [30], [31]
introduces longitudinal discrete acceleration and deceleration
actions. To provide clearer guidance, some studies select from
a discrete set of trajectories [32], [33] or directly generate the
positions and desired speeds of target points [34]. However,
these methods often reduce the alignment between agent
outputs and driving behavior, as they rely on integration with
a basic controller for vehicle control, which limits flexibility.
On the other hand, some studies [35], [36] directly generate
steering angles laterally and accelerations longitudinally from
a continuous action space, aiming to enhance flexibility. However, fluctuations in the network‚Äôs output can cause frequent
changes in steering angle and acceleration commands [37].
In scenarios with dedicated lanes, lateral fluctuations caused
by steering angle variations will result in unpredictable paths.
The experimental results in [14] provide further evidence for
the existence of driving behavior fluctuations. In contrast,
fluctuations in longitudinal acceleration are manageable and
enable more flexible speed trajectories [38].
For compatibility of flexibility and small behavior fluctuations, several studies design hybrid actions by discretizing
parts of a continuous action space [39], [40], or using a
parameterized action space [14], [38], [41], which generates
both lateral discrete abstract targets and longitudinal continuous concrete acceleration commands. Additionally, [42]
designs a dual-layer decision-making control model that combines parallel DQN and Deep Deterministic Policy Gradient
(DDPG) for hybrid output. [43] trains skill-agents for various
driving objectives to output acceleration, from which DQN
can flexibly select. However, the aforementioned studies fail
to sufficiently integrate discrete and continuous actions, nor
do they adequately account for the hybrid nature of road
structures in complex driving environments. In contrast, our
novel hybrid action space is specifically tailored to driving, providing abstract guidance compatible with hybrid road
modality alongside continuous concrete control commands.

Fig. 1. The overall framework of proposed HPA-MoEC. The actor of
RL Method firstly generates the continuous action parameters ao based
on states s, which are then input into the Multi-Objective Critics module
along with s for evaluating the value function. This module consists of N
ensemble-critics corresponding to the different attributes, and each of them
is an ensemble of M critics. The Exploration Strategy module then captures
epistemic uncertainty from the ensemble-critics and selects the final hybrid
action (o, ao ) that enhances training efficiency.

scenarios. Some other studies [50] attempt to use reward
shaping to encourage exploration, but the manually imposed
rewards heavily depend on the designer‚Äôs experience.
Some studies [51], [52] use model ensemble technique to
capture epistemic uncertainty and select actions that encourage
the agent to explore high-uncertainty areas, thus accelerating
policy training. Few studies leverage epistemic uncertainty in
AD tasks to improve driving policy training efficiency [53].
Therefore, this paper develops an epistemic uncertainty-based
exploration mechanism with multiple ensemble-critics for hybrid actions, enabling faster learning of multi-objective policy.
III. M ETHODOLOGY
In this section, we will present the overall framework
and specific formulation details related to the HPA-MoEC
methodology.

C. Policy Exploration Mechanism
Policy exploration helps agents discover potentially multiobjective compatible policies. A proper exploration mechanism can accelerate the learning process to converge to a viable
policy faster [44]. However, the most common exploration
strategy in RL is random exploration, such as œµ-greedy in
DQN [45], random noise in TD3 [46], and the maximum
entropy mechanism in SAC [18]. This randomized mechanism
makes policy exploration lack of orientation and leads to
repeated collection of experience samples, which reduces the
training efficiency [47]. Although some studies attempt to
introduce reward novel state [48] or error of reward [49] to
modify the exploration level, this does not change the nature
of random exploration. This inefficient exploration mechanism
limits the potential performance of RL policies, particularly
when pursuing multi-objective compatibility in complex traffic

A. Overall Framework
The method proposed in this paper is based on a hybrid
parameterized action space for policy evaluation and improvement, considering multiple objectives to achieve multiobjective compatibility. Thus, the MDP can be rewritten as a
new tuple < S, H, [R1 , ¬∑ ¬∑ ¬∑ , RN ] , T , Œ≥ >, where:
‚Ä¢ H represents the hybrid parameterized action space,
where H = {(o, ao ) |ao ‚àà AO , for ‚àÄo ‚àà O }. The o is
the discrete action option selected from the discrete action
option set O. The ao can be seen as the continuous action
parameter corresponding to o, drawn from the continuous
interval AO corresponding to O.
‚Ä¢ [R1 , ¬∑ ¬∑ ¬∑ , RN ] represents a set of N reward functions, where Ri denotes the i-th reward function for
i ‚àà [1, ¬∑ ¬∑ ¬∑ , N ].

4

To construct a fine-grained abstract guidance suitable for
hybrid road modality, the designed hybrid action space enables
the agent to simultaneously output discrete actions o and
continuous action parameters ao , ensuring optimality in both.
These outputs are then used to generate both abstract guidance
and concrete control commands. Specifically, lateral concrete
control commands are generated by combining abstract guidance with prior knowledge, while longitudinal commands are
directly derived from ao .
In addition, the agent should consider multiple attributes
of the AD task during policy evaluation and efficiently explore multi-objective compatible viable policies. Therefore, we
design the Multi-objective Ensemble-Critic framework, which
takes N attributes as evaluation objectives and helps agent
explore in high-certainty regions. Specifically, the framework
consists of N ensemble-critics, which work together for policy
evaluation based on the reward functions [R1 , ¬∑ ¬∑ ¬∑ , RN ], each
focusing on different attributes. Meanwhile, each ensemblecritic consists of M critics. The epistemic uncertainty œÉe
and its change trend can be captured through ensemble-critic,
which helps to orient exploration. The overall framework of
the proposed HPA-MoEC method is shown in Fig. 1 .
B. Policy and Value Function Representation
Under the hybrid parameterized action space, the stateaction value function of the optimal policy can be described
by the Bellman optimal equation as follows:



‚Ä≤
Q(st , ot , ao,t ) = E rt +Œ≥ max sup Q (st+1 , o, ao) . (1)
o‚ààO

ao ‚ààAO

HPA-MoEC consists of N ensemble-critics, each composed
of M critics, resulting in a total of N ‚àóM critics for value function evaluation. Specifically, each critic can estimate the value
of the action (o, ao ) in state s based on its focused attributes.
Let Qij represents the optimal value function evaluated by the
i-th critic corresponding to Ri :



Qij (st , ot , ao,t) = E ri,t +Œ≥ max sup Qij (st+1 , o, ao )
o‚ààO

ao ‚ààAO

(2)
where j ‚àà [1, ¬∑ ¬∑ ¬∑ , M ], ri,t = Ri (s, o, ao ). However, finding
the optimal continuous action ao is challenging in a hybrid
parameterized action space. To overcome this, we assume
that the value function is fixed, meaning that for any state
s and discrete action o, the ao depends on state s. At this
stage, the problem of optimizing in the continuous space
becomes determining the mapping from state s to action ao :
S ‚Üí AO . By using a deterministic policy network ¬µ(s; Œ∏¬µ )
to approximate this mapping, the continuous action ao can be
obtained, with network parameters Œ∏¬µ . This policy network is
known as the actor. Meanwhile, a value network is employed
Q
to approximate the value function Qij , with parameters Œ∏ij
.
Given the assumption that the value function is fixed, the MDP
in the parameterized action space can be viewed as the process
Q
of exploring Œ∏¬µ for a given Œ∏ij
:




Q
Q
Qij s, o, ¬µ (s; Œ∏¬µ ) ; Œ∏ij
‚âà sup Qij s, o, ao ; Œ∏ij
|‚àÄo‚ààO .
ao ‚ààAO

(3)

Specifically, this process can be approximated using a twotimescale update rule [54], where the training update step size
¬µ
Q
. Therefore, Qij can be
is much larger than that for Œ∏ij
for Œ∏ij
expressed as:


Q
=
Qij st , ot , ao,t ; Œ∏ij



(4)
Q
¬µ
E ri,t + Œ≥ max Qij st+1 , o, ¬µ (st+1 ; Œ∏ ) ; Œ∏ij .
o‚ààO

To pursue higher returns, referring to the value network update
target in DQN [17], the update target for a single critic is:




‚Ä≤
Q‚Ä≤
yij,t = ri,t + Œ≥ max Q‚Ä≤ij st+1 , o, ¬µ‚Ä≤ st+1 ; Œ∏¬µ ; Œ∏ij
, (5)
o‚ààO

‚Ä≤

where, Qij and ¬µ‚Ä≤ are the target networks used to assist in
Q‚Ä≤
¬µ‚Ä≤
updating the critic and actor, with parameters Œ∏ij
and Œ∏ij
,
respectively.
In our architecture, each critic is not updated independently.
For each ensemble-critic, every critic within shares the same
driving attribute. Then, all ensemble-critics collaborate to
guide the actor in learning a multi-objective compatible driving
policy. To ensure consistency among all critics in evaluating
driving behavior, the evaluation results‚Äîboth for specific
attributes and overall performance‚Äîshould be fed back to
each critic, for updating networks. Therefore, it is necessary
to construct the critic‚Äôs update target from both the ensemblecritic perspective and the multi-objective compatible overall
perspective. For the i-th ensemble-critic, its overall evaluation
of the policy‚Äôs performance under a given attribute is the
expectation of the value provided by the M critics:
h 
i
Q
QÃÑi (st , o, ao,t ) = Ej‚àà[1,¬∑¬∑¬∑ ,M ] Qij st , o, ¬µ (st+1 ; Œ∏¬µ ) ; Œ∏ij
M

=


1 X 
Q
Qij st , o, ¬µ (st+1 ; Œ∏¬µ ) ; Œ∏ij
|‚àÄo‚ààO .
M j=1
(6)

Correspondingly, the overall target of this ensemble-critic
during training can be expressed as:
yÃÑi,t = ri,t + Œ≥ max QÃÑ‚Ä≤i (st+1 , o, ao,t+1 ) ,
o‚ààO

(7)

where QÃÑ‚Ä≤i is the expectation of all Q‚Ä≤ij for j ‚àà [1, ¬∑ ¬∑ ¬∑ , M ],
similar to Eq.(6).
In addition, the actor‚Äôs outputs assign different attention to
the N ensemble-critics, according to the weights œâi . Thus,
the value function for evaluating the policy‚Äôs multi-objective
compatibility at the overall level can be represented as follows:
Qall (st , o, ao,t ) =

N
X

œâi QÃÑi (st , o, ao,t ) |‚àÄo‚ààO ,

(8)

i=1

PN
where i œâi = 1. Building on this, the overall target for all
critics in the HPA-MoEC can be written as:
yall,t = rtall + Œ≥ max Q‚Ä≤all (st+1 , o, ao,t+1 ) ,
o‚ààO

(9)

where rall combines the attribute rewards basedPon the attenN
tion level of each ensemble-critic, i.e., rall = i œâi ri . The
‚Ä≤
‚Ä≤
Qall is denoted by the weighted sum of QÃÑi , similar to Eq.(8).

5

C. Uncertainty Estimation and Exploration Strategy
Epistemic uncertainty reflects the agent‚Äôs lack of knowledge
due to incomplete learning and can be captured by ensemblecritic [55]. In the i-th ensemble-critic, a larger discrepancy
between the evaluation results of the critics indicates higher
epistemic uncertainty about the corresponding attribute. Such
discrepancies can be quantified by the variance, so the epis2
temic uncertainty œÉe,i
of the i-th attribute is:
2
œÉe,i
(s, o, ao ) = Varj‚àà[1,¬∑¬∑¬∑ ,M ] [Qij (s, o, ao )] |‚àÄo‚ààO .

(16)

Considering that different attention levels are assigned to
each ensemble-critic to achieve multi-objective compatibility,
the weights œâi are also used to compute the agent‚Äôs total
epistemic uncertainty:
Fig. 2. The network parameter update process for actor and any critic. The
target networks are soft-updated.

œÉe2 (s, o, ao ) =

N
X

2
œâi œÉe,i
(s, o, ao ) |‚àÄo‚ààO .

(17)

i=1
Q
Thus, the update of the Œ∏ij
considers not only the critic‚Äôs
own TD error but also the average TD error of all critics in the
ensemble for a given attribute, and the overall TD error of all
critics. For these three aspects, corresponding loss functions
are defined as follows:

i2
1h
Q
Q
(10)
Lij,t (Œ∏ij
) = yij,t ‚àí Qij st , ot , ao,t ; Œ∏ij
,
2
2
1
Q
(11)
Li,t (Œ∏ij
) = yi,t ‚àí QÃÑi (st , ot , ao,t ) ,
2

1
2
(12)
[yall,t ‚àí Qall (st , ot , ao,t )] .
2
To prevent any critic from significantly deviating due to
random factors and disrupting policy convergence, we have
added a guiding term to the loss function of the parameter
Q
Œ∏ij
. This helps ensure that all critics in the ensemble-critic are
updated in a similar direction:


i2
1h
Q
Q
Lconv,t (Œ∏ij
) = Qij st , ot , ao,t ; Œ∏ij
‚àí QÃÑi (st , ot , ao,t ) .
2
(13)
Q
Lall,t (Œ∏ij
)=

Q
In summary, when updating the parameters Œ∏ij
, the final loss
function account for the four aspects discussed earlier:
Q
Lt (Œ∏ij
) = Œªt ¬∑ LTt .

(14)

where Lt = [Lij,t , Li,t , Lall,t , Lconv,t ] represents the vector
of loss function and Œªt = [Œª1 , Œª2 , Œª3 , Œª4 ] is the corresponding
weight vector. By backpropagating the loss defined in Eq. 14,
the value network Qij can be updated iteratively.
The target for updating the actor is more straightforward,
i.e., finding a multi-objective compatible optimal policy by
maximizing the overall value function:
¬µ

Lt (Œ∏ ) = ‚àí

N
1 X

M i=1

œâi

M X
X

Qij (st , o, ¬µ (st+1 ; Œ∏

¬µ

Q
) ; Œ∏ij
).

j=1 o‚ààO

(15)
Overall, the updating process of the actor‚Äôs parameter Œ∏¬µ
Q
and any critic‚Äôs parameter Œ∏ij
is shown in Fig. 2.

In the parameterized action space, ao is treated as a parameter
of o. Thus, the change in epistemic uncertainty for any action
pair (o, ao ) can be captured by the gradient:
G = ‚àáao œÉe2 (s, o, ao ) ‚àÄao ‚àº¬µ(s) .

(18)

Additionally, it is necessary to clarify that œÉe2 (s, o, ao ) represents the epistemic uncertainty of the state-action pair for
‚àÄo ‚àà O, while the overall uncertainty of the environment at
state s is denoted as œÉe2 (s). Specifically, the two are related
as follows:


1 X 2
œÉe (s, o, ao ). (19)
œÉe2 (s) = E œÉe2 (s, o, ao ) |‚àÄo‚ààO =
|O|
o‚ààO

Oriented by the captured epistemic uncertainty, the agent
employs two different exploration strategies for the discrete
action o and its corresponding continuous action ao while
exploring potentially viable policies. For continuous action,
the agent‚Äôs final executed ao is determined by both the actor‚Äôs
output and the chosen o. Thus, the ideal continuous action exploration strategy is to solve a nonlinear continuous optimization problem: arg maxao ‚ààAO œÉe2 (s, o, ao ) |‚àÄo‚ààO , to maximize
exploration across all discrete actions o. However, solving
this problem is computationally expensive and impractical
for efficient policy training. Therefore, we choose a cheaper
alternative by constructing a finite set of actions A‚àí , where
A‚àí ‚äÇ AO , based on the actor‚Äôs origin output and epistemic
uncertainty gradient. This discretizes the problem of selecting
high-uncertainty actions in the continuous domain:




k¬∑œÇ
‚àí
A = ao ao = satAO ¬µ(s) +
G , k ‚àº U (1, K) ,
K
(20)
ao = arg max œÉe2 (s, o, ao ) |‚àÄo‚ààO ,
ao ‚ààA‚àí

(21)

where U (1, K) denotes a uniform distribution over integers
from 1 to K. The œÇ is a coefficient that decreases with training
steps, where œÇ ‚àà (0, 1), reflecting the agent‚Äôs focus on exploring actions. This simplified approach enhances continuous
action exploration with low computational cost.

6

Similarly, the most exploratory discrete action is the one that
maximizes epistemic uncertainty: arg maxo‚ààO œÉe2 (s, o, ao ).
However, when the epistemic uncertainty of all discrete actions
in the set O is low, relying on epistemic uncertainty to choose
actions contributes little to strategy exploration, since the
agent is already confident about all actions. Thus, we define
2
an uncertainty threshold œÉe,th
to ensure the agent adopts a
greedy strategy and maximizes reward when its uncertainty is
low. Additionally, since the parameters of the critic-networks
are randomly initialized and their outputs may fluctuate, the
estimation of epistemic uncertainty has fluctuations. We use a
probabilistic approach rather than directly selecting the action
with maximum uncertainty. Specifically, similar to the Softmax
function, the probability of selecting a discrete action is based
on its uncertainty value, with the total probability across all
actions summing to 1. Therefore, the selection of discrete
actions follows the function F, where o ‚àº F(s, o, ao ) |‚àÄo‚ààO :
(
2
o ‚àº Œµ (s, o, ao ) |‚àÄo‚ààO if œÇœÉe2 (s) > œÉe,th
F=
, (22)
arg max Qall (s, o, ao )
else
o‚ààO
2

eœÉe (s,o,ao )
|
,
œÉe2 (s,o,ao ) ‚àÄo‚ààO
o‚ààO e

Œµ (s, o, ao ) = P

(23)

where Œµ indicates the probability of choosing each action.
Based on the methods discussed above, we provide the
complete algorithmic training process for our HPA-MoEC in
Algorithm 1.
IV. I MPLEMENTATION
Multi-lane highway scenarios are both common and challenging, requiring driving policies that satisfy objectives such
as efficiency, action consistency, and safety. This section
presents the implementation details of HPA-MoEC in these
scenarios, including the MDP formulation, training setup, and
baseline models.

A. MDP Formulation
1) State Space: An appropriate state space representation
is essential for effective policy learning. Specifically, the state
space includes feature information about the Ego Vehicle (EV)
and six surrounding vehicles (SVs) in its current and adjacent
lanes:
(
)
EV
[IDlane , x, y, œÜ, vx , vy ] ,
‚àÜ
S=
, (24)
SVs
[pn , ‚àÜxn , ‚àÜyn , œÜn , ‚àÜvx,n , ‚àÜvy,n ]n‚àà[1¬∑¬∑¬∑6]
where the state of the EV in the road coordinate system consists of six variables: lane ID, longitudinal and lateral position,
heading angle, and longitudinal and lateral velocity. For the
n-th SV, the relevant information includes: a presence flag,
longitudinal and lateral position relative to the EV, heading
angle, and longitudinal and lateral velocity relative to the EV.
Notably, the EV only monitors SVs within the longitudinal
observation range ‚àÜx ‚àà [‚àí80m, 160m].

Algorithm 1 Training process of proposed HPA-MoEC
Require: Step sizes {Œ±, Œ≤} ,total training steps T , soft-update
parameter œÑ , number of critics in ensemble-critic M ,
attribute weight œâ, loss function weight Œª.
1: Initialize: networks {{Qij }, ¬µ, {Q‚Ä≤ij }, ¬µ‚Ä≤ } with random
‚Ä≤
Q
Q‚Ä≤
parameters {{Œ∏ij
}, Œ∏¬µ , {Œ∏ij
}, Œ∏¬µ } for i ‚àà [1, ¬∑ ¬∑ ¬∑ , N ]
and j ‚àà [1, ¬∑ ¬∑ ¬∑ , M ], replay buffer size D, exploration
parameter œÇ.
2: for t = 0 to T do
3:
Get state st from environment.
4:
Capture œÉe2 and its gradient according to Eq. (16) (18).
5:
Select ao,t for ‚àÄo ‚àà O, according to Eq. (21).
6:
Select ot according to Eq. (22).
7:
Generate abstract guidance by ot and ao,t .
8:
Generate concrete control commands for EV.
9:
Get st+1 and ri,t from environment, for i ‚àà [1,¬∑ ¬∑ ¬∑,N ].
10:
Store {st , (ot , ao,t ), [r1,t , ¬∑ ¬∑ ¬∑ , rN,t ] , st+1 )} into D.
11:
Sample transitions randomly from D.
Q
12:
Calculate Lt (Œ∏ij
) for each critic, according to Eq.(14).
13:
Update every critic network via gradient descent:
Q
Q
Q
14:
Œ∏ij,t+1
‚Üê Œ∏ij,t
‚àí Œ±t ‚àáLt (Œ∏ij
).
¬µ
15:
Calculate Lt (Œ∏ ) for actor, according to Eq.(15).
16:
Update actor network via gradient descent:
¬µ
¬µ
17:
Œ∏ij,t+1
‚Üê Œ∏ij,t
‚àí Œ≤t ‚àáLt (Œ∏¬µ ).
18:
Soft-update‚Ä≤ every target critic network:
Q
Q
Q‚Ä≤
19:
Œ∏ij,t+1
‚Üê œÑ Œ∏ij,t
+ (1 ‚àí œÑ )Œ∏ij,t
.
20:
Soft-update‚Ä≤ actor:
‚Ä≤
¬µ
21:
Œ∏t+1
‚Üê œÑ Œ∏t¬µ + (1 ‚àí œÑ )Œ∏t¬µ .
22:
update œÇ, st ‚Üê st+1 .
23:
if st is terminal then
24:
Reset environment.
25:
end if
26: end for
27: return

2) Hybrid Parameterized Action Space: For multi-lane scenarios with hybrid road modalities, we design explicit hybrid
parameterized actions as follows: i) discrete semantic decision
action b, ii) continuous parameter l for constructing a guiding
path, and iii) continuous acceleration command acc. The concrete correspondence is: b ‚Üê o, (l, acc) ‚Üê ao . Specifically, b is
selected from a discrete set {LLC : ‚àíwr , RLC : wr , LK : 0},
where wr represents the road width, with LLC and RLC
representing left and right lane-change, respectively, and LK
indicating lane-keeping. Considering the vehicle kinematic
model [16], the value range for l is defined as follows:


p
4R0 wr ‚àí wr2 ,
l ‚àà min

vx2
2acc‚àí
max




, e|vx |+wr ,

(25)

where R0 and acc‚àí
max represent the minimum turning radius
and maximum braking acceleration of the EV.
 In addition, the

range of the acceleration command acc is ‚àí3m/s2 , 3m/s2 .
At each time step t, with (bt , lt , acct ) output by the agent,
the positions of the guiding path points can be generated using

7

a polynomial curve-based formula:
y0,t+k =

5
X

Œ≥m xm
0,t+h , where h ‚àà [1, ¬∑ ¬∑ ¬∑ , Hp ] ,

(26)

m=0

where (xt+h , yt+h ) represents the position of the point at time
step t + h, and Hp is the planning horizon. The coefficients
Œ≥m of the polynomial curve can be obtained by solving a
system of linear equations. Specifically, the EV‚Äôs position and
heading at the starting point are known, while the heading at
the end point can be obtained from the road information [6].
Actually, the guiding path is determined by the selection of
its endpoint position (xt+Hp , yt+Hp ), which is derived from
the RL agent‚Äôs output, where xt+Hp = lt and yt+Hp = bt . As
the guiding path generated, the EV‚Äôs steering angle command
Œ¥ is output using prior knowledge, specifically the Stanley
algorithm in this paper. Finally, both the steering angle Œ¥ and
the acceleration acc are used together for EV driving control.
3) Reward Function for Multi-Objective: Since safety is
the fundamental requirement for driving, safety attribute is
treated as a distinct objective and a corresponding safety
reward function is designed for one ensemble-critic. Other
attributes are combined into a single general performance
reward function for another ensemble-critic. This enhances
the RL agent‚Äôs compatibility with safety and general driving
performance.
The safety reward function, Rsaf e , focuses on safety in two
aspects:


‚àÜt
,
(27)
Rsaf e = ‚àí10funsaf e + 0.5sat[0,1]
tmax
where, funsaf e is set to 1 when the EV goes off the road
or collides with SVs, and 0 otherwise. To further identify
potential safety risks, the safety reward function also includes
the TTC (Time to Collision) metric, where ‚àÜt is the estimated
time to collision between the EV and the vehicle ahead, and
tmax is the maximum time for TTC evaluation. The values 10
and 0.5 are the weights assigned to the two aspects mentioned
above, respectively.
The general performance reward function, Rgen , incorporates considerations of efficiency, comfort, and interaction:
Rgen = Ref f + Rcomf + Rint
vp ‚àí v
|v ‚àí vt |
‚àí max(0,
)
Ref f =
vt
vp
|Œ¥|
|acc|
Rcomf = ‚àí0.5
‚àí 0.5
|Œ¥max |
|accmax |
6
X
accSV
n
Rint = ‚àí0.1
,
|acc
|
max
n=1

(28)

where Ref f is efficiency reward, encouraging the EV to
maintain a speed close to the target vt . Meanwhile, a low-speed
penalty is applied to minimize the impact of the vehicle‚Äôs
deceleration on overall traffic flow, with the threshold set at vp .
The Rcomf is comfort reward, related to the action consistency
of steering angle and acceleration, where Œ¥max and accmax
denote the maximum values of the two control commands.
Moreover, Rint represents the interaction reward, penalizing

Fig. 3. Schematic diagram of multi-lane highway environments in highwayenv. Green vehicles represent EVs, while blue vehicles represent SVs.

EV‚Äôs interference with SV‚Äôs motion while interacting with
environment. The accSV
n denotes the observed acceleration of
the n-th SV. The number before each item is the weight of
the attention given to it.

B. Training Setup
We developed a three-lane structured road environment
using the AD simulation platform, highway-env [56], in which
the EV attempts to accomplish a multi-objective compatible
driving task. A schematic diagram of the study scenario is
shown in Fig. 3. Specifically, all vehicles, including the EV,
are randomly placed on the three-lane road with random initial
speeds. The IDM and MOBIL models are applied to control
the longitudinal and lateral movements of the SVs [16]. The
SVs may change lanes at appropriate times to get closer to the
target speed, potentially disrupting the EV. Additionally, we
use the vehicle capacity (V/C) to represent traffic congestion,
setting it to 0.5 to create moderate congestion. This ensures the
EV has enough space to change lanes without oversimplifying
the environment.
During training, the episode ends when funsaf e = 1, after
which the environment and all vehicles are reinitialized. Each
episode is capped at 200 seconds to avoid the EV operating
for long periods in low-variability scenarios. Details of the
hyperparameter settings used in the algorithm training are
provided in Table I.
TABLE I
H YPERPARAMETERS
Para.
M
œâ1 , œâ 2
Œ≥
Œ±
Œ≤
Œª
K
œÑ
T
œÇ
‚àí
‚àí
‚àí
‚àí
‚àí
‚àí

Item
Number of critics in an ensemble-critic
Weights of Rsaf e and Rgen
Discount factor
Training step size of critic
Training step size of actor
Weights of loss functions for critic
parameters for con-action exploration
Soft-update parameter
Number of steps for training
Exploration weight parameter
Number of hidden layers in critic/actor
Hidden layer size
Activation function
Replay buffer size
Sample batch size
Training optimizer

Value
6
0.4, 0.6
0.9
0.01
0.001
[0.5,0.2,0.2,0.1]
10
0.005
200000
1‚Üí0.001
3
256
Tanh
40000
256
Adam

Additionally, our method is tested on 200 episodes in
both the training environment and the HighD [57] real-world
dataset. For testing on the HighD dataset, the trained agent
controls randomly selected vehicles, while the SVs follow their
predefined trajectories.

8

C. Comparison Models
1) Comparison Baseline: To comprehensively evaluate the
proposed HPA-MoEC, we compare it with several widely
used RL methods for the AD task. All methods share the
same training and testing environments, as well as the state
space. The main difference is that, unlike HPA-MoEC, the
other methods couple the attributes into a single reward
function: Rbase = œâ1 Rsaf e + œâ2 Rgen . More importantly, the
action spaces structure and policy exploration strategies in the
following methods differ:
‚Ä¢ Deep Q-Network (DQN) [17]: It only generates discrete
semantic decisions and is paired with a PID controller to
control the EV. The exploration strategy used is œµ-greedy.
‚Ä¢ SAC with Continuous actions (SAC-C) [18]: It only
outputs continuous control commands, which are lateral
steering angle and longitudinal acceleration. Its exploration is enhanced through maximum entropy and the
addition of Gaussian noise to the actions.
‚Ä¢ SAC with Hybrid actions (SAC-H) [18]: SAC-H discretizes part of the continuous action space in SAC,
producing outputs similar to HPA-MoEC.
‚Ä¢ PPO with Hybrid actions (PPO-H) [58]: An on-policy
actor-critic algorithm with action space similar to SAC-H.
To ensure fair comparisons and reliable conclusions, all methods use the same network architecture, learning rate, and
other key hyperparameters. Additionally, for method-specific
parameters, we perform extensive tuning within reasonable
ranges and select the optimal configuration for each method.
2) Ablation Model: To further validate the effectiveness
of the three key techniques used in HPA-MoEC: i) Hybrid
parametric action space structure; ii) Multi-critic policy evaluation architecture; and iii) Epistemic uncertainty-based policy
exploration, we design the following ablation baselines:
‚Ä¢ HPA-MoEC: The method proposed in this paper includes
all three technical components.
‚Ä¢ HPA-Mo: By removing component iii from HPA-MoEC,
policy exploration is no longer oriented by uncertainty.
Policy evaluation for each objective is performed by a
single critic only, rather than by an ensemble-critic.
‚Ä¢ HPA: By removing component ii from HPA-Mo, only one
overall objective remains, with one corresponding critic
for evaluating policies that considers multiple attributes.
In fact, this baseline is similar to the algorithm in [16].
‚Ä¢ DA-Mo: By removing component i from HPA-Mo, this
baseline generates only coarse-grained discrete semantic
decisions as abstract guidance, which are combined with
the PID controller to output steering angles. In fact, this
baseline is similar to part of the work in [21].
D. Evaluation Metrics
To evaluate the driving performance of the proposed method
across multiple objectives, we used several metrics for each
episode:
‚Ä¢ Average Reward (AR): AR is the ratio of total reward to
episode length, offering a comprehensive evaluation of
the RL agent‚Äôs performance.

Collision Rate (CR, %): Collisions result from hazardous
driving behavior and can be used to evaluate the safety
of the agent‚Äôs driving policy.
‚Ä¢ Average Speed (AS, m/s): The EV‚Äôs speed indicates
the agent‚Äôs ability to intelligently execute lane changes
actions to enhance driving efficiency.
‚Ä¢ Number of Lane-change (NL): NL partially reflects the
EV‚Äôs flexibility and can be analyzed alongside AS to
explain the reasons for improved driving efficiency.
2
‚Ä¢ Variance of Steering angle (VS, rad ) and Acceleration
(VA, m2 /s4 ): VS and VA respectively indicate the vehicle‚Äôs fluctuations in lateral and longitudinal behavior,
reflecting the consistency of the driving policy‚Äôs actions.
‚Ä¢

V. R ESULTS AND D ISCUSSIONS
A. Training Performance
The learning curves for general performance and safety
during training are shown in Fig. 4, with each algorithm
trained six times using different seeds. The total reward curve
and corresponding variance distribution in Fig. 4(a) show that
the our HPA-MoEC achieves higher rewards with smaller
policy fluctuations. This indicates that, regardless of seed
variations, its policy consistently converges to the best general
performance. By comparison, the similar rewards achieved by
SAC-H and PPO-H indicate that both of them perform worse
than HPA-MoEC. Furthermore, without finer-grained guiding
paths, the reward during SAC-C convergence is much lower,
indicating poorer driving performance when both longitudinal
and lateral direct control commands are output together. Using
only semantic decision actions, the DQN receives the lowest
reward, indicating that discrete actions alone are insufficient
for complex driving tasks.
Additionally, once the minimum sample size required for
training is gathered in the experience replay pool, the policy
improvement speed of HPA-MoEC is significantly faster than
that of all the baselines. This increase in training efficiency
is attributed to the introduction of an epistemic uncertaintybased exploration strategy, which enables a oriented and
faster exploration of potentially viable policies. Notably, since
SAC-C directly controls the EV by outputting steering angle
commands, it often veers off the road and ends the episode
early, causing the reward curve to differ significantly from
other methods.
As shown in Fig. 4(b), the change in CR for each method
during training is illustrated, with the zoomed-in view of
the converged curves highlighting that HPA-MoEC ultimately
maintains a low CR. Thanks to the decoupling of the safety
objective from the general performance objective within the
multi-objective policy evaluation architecture, the agent places
greater emphasis on safety. In contrast, SAC-H and PPOH have slightly higher CRs, whereas DQN has the highest.
Notably, although SAC-C performs poorly in total reward, it
prioritizes the safety of the EV by maintaining a very low CR.
This results from its conservative following behavior, which
will be discussed in detail in Section V-B1.
B. Testing Performance

9

Fig. 4. The training process of our method with comparison methods
quantified by: a) Total Reward and b) Collision Rate.

Fig. 6. Metrics distribution of testing in HighD dataset.
TABLE III
T EST RESULTS IN H IGH D DATASET

Fig. 5. Metrics distribution of testing with Rule-Based SVs: (a) average
reward, (b) average speed, (c) variance of steering angle, (d) variance of
acceleration.
TABLE II
T EST RESULTS WITH RULE - BASED SV S
Method

AR

AS

NL

VS

VA

CR

DQN
SAC-C
PPO-H
SAC-H
HPA-MoEC

0.860
0.868
0.902
0.903
0.932

8.18
6.95
9.57
9.62
10.87

7.71
2.04
5.76
5.57
7.14

0.0055
0.0063
0.0027
0.0024
0.0019

0.381
0.452
0.279
0.256
0.181

0.38%
0.01%
0.13%
0.12%
0.04%

1) Testing with Rule-Based SVs: The boxplots in Fig. 5
illustrate the distribution of four metrics in testing: average
reward (Fig. 5(a)), average speed (Fig. 5(b)), and the variance
of steering angle and acceleration (Fig. 5(c) and Fig. 5(d)).
The quantitative statistics for all metrics are provided in
Table II. Specifically, the driving policy of the proposed HPAMoEC demonstrates advantages in driving efficiency, action
consistency, and safety.
In general, HPA-MoEC receives the highest AR, which
is consistent with the training results and indicates a more
effective driving policy. SAC-H and PPO-H also perform well,
with similar AR levels. In contrast, the driving policy of
DQN and SAC-C perform poorly and exhibit considerable
fluctuation, with lower and more dispersed AR values.
For driving efficiency, HPA-MoEC achieves the highest

Method

AR

AS

NL

VS

VA

CR

DQN
SAC-C
PPO-H
SAC-H
HPA-MoEC

0.909
0.913
0.938
0.945
0.976

12.74
10.12
13.32
13.58
15.27

8.84
1.25
5.67
5.69
7.03

0.0035
0.0054
0.0028
0.0029
0.0021

0.316
0.451
0.155
0.160
0.051

0.29%
0.00%
0.04%
0.05%
0.01%

AS through more flexible lane changes. In comparison, SACH and PPO-H have lower ASs due to reduced lane-changing
flexibility, leading to suboptimal efficiency. Specifically, compared to SAC-H, HPA-MoEC improves AS by 13% and
increases NL by 28%. Compared to the above methods with
hybrid actions, SAC-C‚Äôs direct control of the EV results in
the lowest AS and the fewest NL, indicating its inability to
effectively leverage lane-changing opportunities to increase
speed. Relying on discrete actions, DQN achieves higher AS
in some episodes by frequent lane changes, but its overall AS
ranks second to last. Overall, the hybrid actions provide greater
flexibility and thus improve driving efficiency, especially by
using a parameterized action space to generate outputs rather
than discretizing part of the continuous actions.
For action consistency, HPA-MoEC exhibits the smallest VS and VA, implying a significant reduction in lateral
and longitudinal driving behavior fluctuations. In comparison,
although PPO-H and SAC-H also generate hybrid actions,
their VS increases by 26% and 42%, respectively, while their
VA increases by 41% and 54%, respectively. This indicates
that the HPA-MoEC generates smoother guiding paths and
acceleration commands through its parameterized action space.
Notably, both SAC-C and DQN exhibit large VS and VA,
indicating large behavior fluctuations. For DQN, the discrete
decision set hampers smooth steering adjustments during lane
changes and restricts acceleration flexibility. For SAC-C, the
coupling between steering angle and acceleration commands
makes it extremely challenging to produce smooth and regular

10

Fig. 7. The training process of our framework with ablation baselines
quantified by: a) Total Reward and b) Collision Rate.

outputs when both exhibit fluctuations.
For safety performance, HPA-MoEC demonstrates the
second-lowest CR, trailing only to SAC-C, highlighting its
strong focus on safety. This is facilitated by a policy evaluation
design with safety attribute as a separate objective, achieving
a CR reduction of 67% and 69% for HPA compared to SACH and PPO-H, respectively. Notably, since SAC-C directly
outputs control commands through a single continuous action
space, it learns an over-conservative driving policy. Although
this extreme concern for short-term safety significantly reduces
CR, it greatly sacrifices efficiency and action consistency. In
contrast, the slight compromise in safety offered by HPAMoEC brings significant improvements in efficiency and
action consistency, which is more aligned with the multiobjective requirements of AD. Additionally, DQN has a CR
of 0.38%, much higher than other methods. With an average
of 7.73 NL per episode, this indicates that its more aggressive
driving policy increases the risk of putting the EV in danger.
2) Testing in HighD-Dataset: The testing results on the
HighD dataset, including the distribution of evaluation metrics
and quantitative statistics, are shown in Fig. 6 and Table III,
respectively. Compared to the constructed simulation scenario,
the traffic density in HighD is sparser, and all methods
demonstrate better driving performance. Clearly, HPA-MoEC
still achieves the highest AR, showing the good adaptability of
its driving policy. It also maintains excellent control over acceleration and flexible lane-changing abilities, resulting in the
highest AS and the most NL, except for DQN. Additionally,
the guiding path still plays a role in the reduction of vehicle
behavior fluctuations, keeping the VS and VA low. In terms of
safety, the emphasis on safety attributes in HPA-MoEC reduces
the CR to just 0.01%. Overall, HPA-MoEC outperforms all
other baselines in terms of compatibility with the objectives
of driving efficiency, action consistency, and safety, offering
greater potential for real-world traffic applications.
C. Ablation study
1) Training Performance: The changes in total reward and
collision rate for all ablation baselines during training are
shown in Fig. 7. It is clear that as key components of HPAMoEC are gradually removed, the performance decreases.
For HPA-Mo, policy convergence is greatly delayed. Compared to HPA-MoEC, HPA-Mo reaches similar final rewards
and slightly higher CR. However, its convergence is slower,

Fig. 8. Metrics distribution of ablation study with rule-based SVs.
TABLE IV
A BLATIVE STUDIES FOR HPA-M O EC WITH RULE -BASED SV S
Method

AR

AS

NL

VS

VA

CR

HPA-MoEC
HPA-Mo
HPA
DA-Mo

0.932
0.927
0.905
0.898

10.87
10.63
10.36
9.22

7.14
6.90
6.11
6.43

0.0019
0.0020
0.0016
0.0025

0.181
0.160
0.175
0.185

0.04%
0.03%
0.08%
0.06%

only reaching around the 1700th episode. In contrast, HPAMoEC, despite involving more networks, converges around the
1400th episode, suggesting that epistemic uncertainty-based
policy exploration improves training efficiency by about 18%.
For HPA, it shows lower rewards and higher CR at convergence compared to HPA-Mo. This suggests that the designed
multi-objective compatible policy evaluation architecture is effective. Utilizing critics that specifically target general driving
attributes and safety during policy evaluation can promote
driving that is compatible with general performance and safety.
For DA-Mo, the reward it can obtain when converging is
the lowest, and the CR is the highest. This shows that hybrid
action space structure plays an important role in improving
policy execution capabilities. A finer-grained guidance path
enhances the correlation between agent output and driving
behavior, further improving overall policy performance and
safety.
2) Testing with Rule-Based SVs: The results of the ablation
baseline tests, including data distributions and quantitative
statistics, are shown in Fig. 8 and Table IV. The HPAMoEC, with all technology components, demonstrates the
best driving performance. As components are progressively
removed, the driving performance of the ablation baselines
declines accordingly.
HPA-Mo, although slow in policy convergence during training, shows driving performance close to HPA-MoEC in the
final testing, with only a slight reduction in AR and AS.
HPA performs worse in both general driving performance
and safety, with lower AR and higher CR. Specifically, remov-

11

Fig. 10. Changes in epistemic uncertainty (EU) during training, including:
a) average epistemic uncertainty, (b) EU for left lane-change, (c) EU for lane
keeping, (d) EU for right lane-change.
Fig. 9. Metrics distribution of ablation study in HighD dataset.

D. Discussion
TABLE V
A BLATIVE STUDIES FOR HPA-M O EC IN H IGH D DATASET
Method

AR

AS

NL

VS

VA

CR

HPA-MoEC
HPA-Mo
HPA
DA-Mo

0.976
0.975
0.948
0.947

15.27
14.71
13.49
14.68

7.03
6.88
5.95
6.53

0.0021
0.0019
0.0024
0.0036

0.051
0.057
0.073
0.076

0.01%
0.01%
0.04%
0.04%

ing the multi-objective policy evaluation component leads to
a significant decrease in AR and, more importantly, nearly
a threefold increase in CR for HPA compared to HPAMo. This clearly demonstrates that our design maintains the
compatibility of the policy with both general performance and
safety during testing.
DA-Mo performs the worst across all metrics compared to
the other ablation baselines. Notably, removing the hybrid
action space results in approximately a 25% increase in
VS compared to HPA, highlighting the larger fluctuations in
lateral driving behavior. In addition, its AS decreases by 15%,
with a wider distribution, while the CR increases by 100%,
reflecting a decline in both driving efficiency and safety. Therefore, implementing a hybrid parameterized action space with
finer-grained guidance paths helps the agent promote multiobjective driving, particularly in terms of reducing fluctuations
in driving behavior.
3) Testing in HighD-Dataset: The testing results for all
ablation baselines in the HighD dataset are shown in Fig. 9
and Table V. HPA-Mo falls slightly below HPA-MoEC in
driving efficiency, but both have good driving performance.
In contrast, HPA lags clearly behind both previous methods in
AS and NL and has a higher CR. The DA-Mo is even worse,
accompanying a notable increase in VS. This suggests that the
multi-objective policy evaluation architecture and the hybrid
parameterized action space with guiding paths still promote
the compatibility of the objectives of driving efficiency, action
consistency and safety in the HighD dataset.

In summary, our HPA-MoEC method outperforms all the
RL comparison baselines, where all three key technology components play a significant role in facilitating the learning of a
multi-objective compatible policy. The hybrid parameterized
action enhances the connection between agent actions and
driving behavior by simultaneously outputting finer-grained
guiding paths as well as direct acceleration commands. This
action space structure promotes multi-objective compatibility,
particularly enhancing action consistency by reducing driving
behavior fluctuations while maintaining flexibility. The multiobjective policy evaluation architecture guides the agent in
improving policy learning by treating general and safety
attributes as distinct objectives and building the corresponding
reward function and critic. This policy evaluation architecture
improves both the driving general performance and safety,
demonstrating its ability to achieve multi-objective compatible
driving. In addition, the epistemic uncertainty-based policy
exploration mechanism accelerates the convergence of multiobjective compatible viable policies, improving the training efficiency. It is also noteworthy that SVs in the HighD dataset exhibit human driving behaviors, which differ significantly from
those in simulation traffic. Although HPA-MoEC is trained in
simulated traffic, it still achieves strong driving performance
when confronted with unfamiliar SVs. This demonstrates that
HPA-MoEC possesses strong generalization capabilities and
can effectively adapt to unfamiliar environments.
Additionally, to better observe the impact of our exploration
mechanism on epistemic uncertainty, we denote ‚Äôw/o EU-E‚Äô as
an attempt. In this attempt, ensemble-critics generate epistemic
uncertainty but do not use it for exploration, instead performing random exploration. The curves in Fig. 10 show how
epistemic uncertainty evolves throughout the policy improvement process. Our HPA-MoEC experiences higher average
uncertainty in the early training phases, and then makes the uncertainty lower more rapidly during exploration. This suggests
that HPA-MoEC explores more fully while converging the
policy faster than randomized exploration. Further, the changes

12

in epistemic uncertainty for the three lane-change decisions
follow a similar trend. Notably, changing lanes‚Äîwhether to
the left or right‚Äîresults in higher uncertainty compared to
lane keeping, suggesting that lane changes involve greater
unknowns and risks.
VI. C ONCLUSION AND F UTURE W ORK
This paper proposes a Multi-objective Ensemble-Critic
(HPA-MoEC) reinforcement learning method with Hybrid Parameterized Action space, capable of efficiently learning multiobjective compatible driving policies. HPA-MoEC adopts a
more advanced MORL architecture, in which multiple reward
functions guide ensemble-critics to focus on specific driving
objectives. Meanwhile, the architecture integrates a hybrid parameterized action space structure, which can simultaneously
generate abstract guidance and specific control commands that
fit the hybrid road modality. In addition, an uncertainty-based
exploration mechanism is developed to achieve faster learning
of multi-objective compatible policies. We conduct the training
and testing of the policy in both simulated traffic environments
and the HighD dataset. The results show that HPA-MoEC
effectively learns a multi-objective compatible autonomous
driving policy in terms of efficiency, action consistency, and
safety. The ablation study further demonstrated the role of
technology components in HPA-MoEC in promoting multiobjective compatibility.
One limitation of our study is that the driving scenarios
for training and testing are restricted to multi-lane highways.
Although this typical structured road environment differs from
other road types such as ramps and intersections, the driving
objectives of EV in these various scenarios are generally
similar: selecting appropriate behavioral goals and interacting
with other vehicles. The key difference lies in how the state
space is designed to enable the RL agent to comprehensively
perceive the environment. Therefore, in future work, we aim to
use higher-dimensional perception information (such as BEV
images) as the state space to extend the application of HPAMoEC to more complex traffic scenarios.
ACKNOWLEDGMENTS
This work is supported in part by the National Natural
Science Foundation of China under Grant No. 52325212 and
No.52372394, in part by the National Key R&D Program of
China under Grant No. 2022YFE0117100.
R EFERENCES
[1] A. Y. Majid, S. Saaybi, V. Francois-Lavet, R. V. Prasad, and C. Verhoeven, ‚ÄúDeep reinforcement learning versus evolution strategies: A
comparative survey,‚Äù IEEE Trans. Neural Netw. Learn. Sys., 2023.
[2] Y. Zhang, B. Gao, L. Guo, H. Guo, and H. Chen, ‚ÄúAdaptive decisionmaking for automated vehicles under roundabout scenarios using optimization embedded reinforcement learning,‚Äù IEEE Trans. Neural Netw.
Learn. Sys., vol. 32, no. 12, pp. 5526‚Äì5538, 2021.
[3] J. Hao, T. Yang, H. Tang, C. Bai, J. Liu, Z. Meng, P. Liu, and
Z. Wang, ‚ÄúExploration in deep reinforcement learning: From singleagent to multiagent domain,‚Äù IEEE Trans. Neural Netw. Learn. Sys.,
vol. 35, no. 7, pp. 8762‚Äì8782, 2024.
[4] Z. He, L. Dong, C. Song, and C. Sun, ‚ÄúMultiagent soft actor-critic based
hybrid motion planner for mobile robots,‚Äù IEEE Trans. Neural Netw.
Learn. Sys., vol. 34, no. 12, pp. 10980‚Äì10992, 2022.

[5] J. Xing, D. Wei, S. Zhou, T. Wang, Y. Huang, and H. Chen, ‚ÄúA
comprehensive study on self-learning methods and implications to
autonomous driving,‚Äù IEEE Trans. Neural Netw. Learn. Sys., pp. 1‚Äì20,
2024.
[6] Z. Li, G. Jin, R. Yu, B. Leng, and L. Xiong, ‚ÄúInteraction-aware deep
reinforcement learning approach based on hybrid parameterized action
space for autonomous driving,‚Äù in Proc. SAE Intell. Connected Veh.
Symposium (SAE ICVS), 2024.
[7] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs, et al.,
‚ÄúOutracing champion gran turismo drivers with deep reinforcement
learning,‚Äù Nature, vol. 602, no. 7896, pp. 223‚Äì228, 2022.
[8] W. B. Knox, A. Allievi, H. Banzhaf, F. Schmitt, and P. Stone, ‚ÄúReward
(mis) design for autonomous driving,‚Äù Artif. Intell., vol. 316, p. 103829,
2023.
[9] Z. Zhu and H. Zhao, ‚ÄúA survey of deep rl and il for autonomous
driving policy learning,‚Äù IEEE Trans. Intell. Transp. Syst., vol. 23, no. 9,
pp. 14043‚Äì14065, 2022.
[10] X.-Q. Cai, P. Zhang, L. Zhao, J. Bian, M. Sugiyama, and A. Llorens,
‚ÄúDistributional pareto-optimal multi-objective reinforcement learning,‚Äù
Advances in Neural Information Processing Systems, vol. 36, pp. 15593‚Äì
15613, 2023.
[11] P. Ladosz, L. Weng, M. Kim, and H. Oh, ‚ÄúExploration in deep reinforcement learning: A survey,‚Äù Inf. Fusion, vol. 85, pp. 1‚Äì22, 2022.
[12] G. Li, Y. Qiu, Y. Yang, Z. Li, S. Li, W. Chu, P. Green, and S. E. Li,
‚ÄúLane change strategies for autonomous vehicles: A deep reinforcement
learning approach based on transformer,‚Äù IEEE Trans. Intell. Veh., vol. 8,
no. 3, pp. 2197‚Äì2211, 2023.
[13] X. Lu, F. X. Fan, and T. Wang, ‚ÄúAction and trajectory planning for
urban autonomous driving with hierarchical reinforcement learning,‚Äù
arXiv preprint arXiv:2306.15968, 2023.
[14] L. Chen, Y. He, Q. Wang, W. Pan, and Z. Ming, ‚ÄúJoint optimization
of sensing, decision-making and motion-controlling for autonomous
vehicles: A deep reinforcement learning approach,‚Äù IEEE Trans. Veh.
Technol., vol. 71, no. 5, pp. 4642‚Äì4654, 2022.
[15] X. Wang, S. Wang, X. Liang, D. Zhao, J. Huang, X. Xu, B. Dai, and
Q. Miao, ‚ÄúDeep reinforcement learning: A survey,‚Äù IEEE Trans. Neural
Netw. Learn. Sys., vol. 35, no. 4, pp. 5064‚Äì5078, 2024.
[16] G. Jin, Z. Li, B. Leng, W. Han, and L. Xiong, ‚ÄúStability enhanced
hierarchical reinforcement learning for autonomous driving with parameterized trajectory action,‚Äù in Proc. IEEE Intell. Transp. Syst. Conf.
(ITSC), 2024.
[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù
nature, vol. 518, no. 7540, pp. 529‚Äì533, 2015.
[18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, ‚ÄúSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic
actor,‚Äù in Int. Conf. Mach. Learn. (ICML), pp. 1861‚Äì1870, PMLR, 2018.
[19] A. Abouelazm, J. Michel, and J. M. Zoellner, ‚ÄúA review of reward
functions for reinforcement learning in the context of autonomous
driving,‚Äù arXiv preprint arXiv:2405.01440, 2024.
[20] X. Wen, S. Jian, and D. He, ‚ÄúModeling the effects of autonomous
vehicles on human driver car-following behaviors using inverse reinforcement learning,‚Äù IEEE Trans. Intell.Transp. Syst., 2023.
[21] X. He, J. Hao, X. Chen, J. Wang, X. Ji, and C. Lv, ‚ÄúRobust multiobjective reinforcement learning considering environmental uncertainties,‚Äù
IEEE Transactions on Neural Networks and Learning Systems, vol. 36,
no. 4, pp. 6368‚Äì6382, 2024.
[22] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman,
and D. ManeÃÅ, ‚ÄúConcrete problems in ai safety,‚Äù arXiv preprint
arXiv:1606.06565, 2016.
[23] S. Mysore, G. Cheng, Y. Zhao, K. Saenko, and M. Wu, ‚ÄúMulti-critic
actor learning: Teaching rl policies to act with style,‚Äù in Proc. Int. Conf.
Learn. Representations (ICLR), 2022.
[24] Z. Wang, S. Zhang, X. Feng, and Y. Sui, ‚ÄúAutonomous underwater vehicle path planning based on actor-multi-critic reinforcement learning,‚Äù
Proceedings of the Institution of Mechanical Engineers, Part I: Journal
of Systems and Control Engineering, vol. 235, no. 10, pp. 1787‚Äì1796,
2021.
[25] X. He and C. Lv, ‚ÄúToward personalized decision making for autonomous
vehicles: a constrained multi-objective reinforcement learning technique,‚Äù Transp. Res. Part C Emerg. Technol., vol. 156, p. 104352, 2023.
[26] K. Srinivasan, B. Eysenbach, S. Ha, J. Tan, and C. Finn, ‚ÄúLearning to
be safe: Deep rl with a safety critic,‚Äù arXiv preprint arXiv:2010.14603,
2020.

13

[27] K. Yang, X. Tang, S. Qiu, S. Jin, Z. Wei, and H. Wang, ‚ÄúTowards robust
decision-making for autonomous driving on highway,‚Äù IEEE Trans. Veh.
Technol., vol. 72, no. 9, pp. 11251‚Äì11263, 2023.
[28] S. Nageshrao, H. E. Tseng, and D. Filev, ‚ÄúAutonomous highway driving
using deep reinforcement learning,‚Äù in Proc. IEEE Int. Conf. Syst. Man
Cybern. (SMC), pp. 2326‚Äì2331, 2019.
[29] S. Li, C. Wei, and Y. Wang, ‚ÄúCombining decision making and trajectory
planning for lane changing using deep reinforcement learning,‚Äù IEEE
Trans. Intell.Transp. Syst., vol. 23, no. 9, pp. 16110‚Äì16136, 2022.
[30] P. Wolf, K. Kurzer, T. Wingert, F. Kuhnt, and J. M. Zollner, ‚ÄúAdaptive
behavior generation for autonomous driving using deep reinforcement
learning with compact semantic states,‚Äù in Proc. IEEE Intell. Veh.
Symposium (IV), pp. 993‚Äì1000, 2018.
[31] G. Chen, Y. Zhang, and X. Li, ‚ÄúAttention-based highway safety planner
for autonomous driving via deep reinforcement learning,‚Äù IEEE Trans.
Veh. Technol., 2023.
[32] Y. Yu, C. Lu, L. Yang, Z. Li, F. Hu, and J. Gong, ‚ÄúHierarchical
reinforcement learning combined with motion primitives for automated
overtaking,‚Äù in Proc. IEEE Intell. Veh. Symposium (IV), pp. 1‚Äì6, 2020.
[33] G. Jin, Z. Li, B. Leng, and M. Shao, ‚ÄúDeep reinforcement learning
lane-change decision-making for autonomous vehicles based on motion
primitives library in hierarchical action space,‚Äù Artificial Intelligence and
Autonomous Systems, vol. 2, no. 0009, 2024.
[34] X. Lu, F. X. Fan, and T. Wang, ‚ÄúAction and trajectory planning for
urban autonomous driving with hierarchical reinforcement learning,‚Äù
arXiv:2306.15968, 2023.
[35] Z. Wang, H. Huang, J. Tang, and L. Hu, ‚ÄúA deep reinforcement learningbased approach for autonomous lane-changing velocity control in mixed
flow of vehicle group level,‚Äù Expert Syst. Appl., vol. 238, p. 122158,
2024.
[36] Z. Qi, T. Wang, J. Chen, D. Narang, Y. Wang, and H. Yang, ‚ÄúLearningbased path planning and predictive control for autonomous vehicles with
low-cost positioning,‚Äù IEEE Trans. Intell. Veh., vol. 8, no. 2, pp. 1093‚Äì
1104, 2023.
[37] Anonymous, ‚ÄúFlipnet: Fourier lipschitz smooth policy network for
reinforcement learning,‚Äù in Submitted to The 30th Proc. Int. Conf. Learn.
Representations (ICLR), 2024. under review.
[38] Q. Guo, O. Angah, Z. Liu, and X. J. Ban, ‚ÄúHybrid deep reinforcement
learning based eco-driving for low-level connected and automated vehicles along signalized corridors,‚Äù Transp. Res. Part C Emerg. Technol.,
vol. 124, p. 102980, 2021.
[39] Z. Wei, P. Hao, and M. J. Barth, ‚ÄúDeveloping an adaptive strategy for
connected eco-driving under uncertain traffic condition,‚Äù in Proc. IEEE
Intell. Veh. Symposium (IV), pp. 2066‚Äì2071, 2019.
[40] H. Liu, Z. Huang, X. Mo, and C. Lv, ‚ÄúAugmenting reinforcement learning with transformer-based scene representation learning for decisionmaking of autonomous driving,‚Äù IEEE Trans. Intell. Veh., vol. 9, no. 3,
pp. 4405‚Äì4421, 2024.
[41] Y. Lin, X. Liu, and Z. Zheng, ‚ÄúDiscretionary lane-change decision and
control via parameterized soft actor‚Äìcritic for hybrid action space,‚Äù
Machines, vol. 12, no. 4, p. 213, 2024.
[42] J. Peng, S. Zhang, Y. Zhou, and Z. Li, ‚ÄúAn integrated model for
autonomous speed and lane change decision-making based on deep
reinforcement learning,‚Äù IEEE Trans. Intell.Transp. Syst., vol. 23, no. 11,
pp. 21848‚Äì21860, 2022.
[43] Y. Gurses, K. Buyukdemirci, and Y. Yildiz, ‚ÄúDeveloping driving strategies efficiently: A skill-based hierarchical reinforcement learning approach,‚Äù IEEE Control Syst. Lett., 2024.
[44] Q. Liu, Y. Li, S. Chen, K. Lin, X. Shi, and Y. Lou, ‚ÄúDistributional reinforcement learning with epistemic and aleatoric uncertainty estimation,‚Äù
Inf. Sci., vol. 644, p. 119217, 2023.
[45] M. Tokic, ‚ÄúAdaptive Œµ-greedy exploration in reinforcement learning
based on value differences,‚Äù in KI 2010: Advances in Artif. Intell.,
pp. 203‚Äì210, Springer, 2010.
[46] S. Fujimoto, H. Hoof, and D. Meger, ‚ÄúAddressing function approximation error in actor-critic methods,‚Äù in International conference on
machine learning, pp. 1587‚Äì1596, PMLR, 2018.
[47] R. Chai, H. Niu, J. Carrasco, F. Arvin, H. Yin, and B. Lennox,
‚ÄúDesign and experimental validation of deep reinforcement learningbased fast trajectory planning and control for mobile robot in unknown
environment,‚Äù IEEE Trans. Neural Netw. Learn. Sys., vol. 35, no. 4,
pp. 5778‚Äì5792, 2022.
[48] M. C. Machado, M. G. Bellemare, and M. Bowling, ‚ÄúCount-based
exploration with the successor representation,‚Äù vol. 34, no. 04, pp. 5125‚Äì
5133, 2020.

[49] M. Usama and D. E. Chang, ‚ÄúLearning-driven exploration for reinforcement learning,‚Äù in Int. Conf. Control, Autom. Syst. (ICCAS), pp. 1146‚Äì
1151, IEEE, 2021.
[50] J. Wu, Z. Huang, W. Huang, and C. Lv, ‚ÄúPrioritized experience-based
reinforcement learning with human guidance for autonomous driving,‚Äù
IEEE Trans. Neural Netw. Learn. Sys., vol. 35, no. 1, pp. 855‚Äì869, 2024.
[51] J. Zhang, B. Cheung, C. Finn, S. Levine, and D. Jayaraman, ‚ÄúCautious
adaptation for reinforcement learning in safety-critical settings,‚Äù in Int.
Conf. Mach. Learn. (ICML), pp. 11055‚Äì11065, PMLR, 2020.
[52] D. Kim, J. Shin, P. Abbeel, and Y. Seo, ‚ÄúAccelerating reinforcement
learning with value-conditional state entropy exploration,‚Äù Adv. Neural
Inf. Process. Syst. (NeurIPS), vol. 36, 2024.
[53] Z. Zhang, Q. Liu, Y. Li, K. Lin, and L. Li, ‚ÄúSafe reinforcement learning
in autonomous driving with epistemic uncertainty estimation,‚Äù IEEE
Trans. Intell.Transp. Syst., 2024.
[54] V. S. Borkar, ‚ÄúStochastic approximation with two time scales,‚Äù Systems
& Control Letters, vol. 29, no. 5, pp. 291‚Äì294, 1997.
[55] C.-J. Hoel, K. Wolff, and L. Laine, ‚ÄúTactical decision-making in
autonomous driving by reinforcement learning with uncertainty estimation,‚Äù in Proc. IEEE Intell. Veh. Symposium (IV), pp. 1563‚Äì1569, 2020.
[56] E. Leurent, ‚ÄúAn environment for autonomous driving decision-making.‚Äù
https://github.com/eleurent/highway-env, 2018.
[57] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, ‚ÄúThe highd dataset:
A drone dataset of naturalistic vehicle trajectories on german highways
for validation of highly automated driving systems,‚Äù in Proc. IEEE Intell.
Transp. Syst. Conf. (ITSC), pp. 2118‚Äì2125, 2018.
[58] J. Schulman, ‚ÄúTrust region policy optimization,‚Äù arXiv preprint
arXiv:1502.05477, 2015.

