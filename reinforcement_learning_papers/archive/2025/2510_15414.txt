Preprint. Under review.

MARSHAL: I NCENTIVIZING M ULTI -AGENT R EA SONING VIA S ELF -P LAY WITH S TRATEGIC LLM S

arXiv:2510.15414v2 [cs.AI] 2 Dec 2025

Huining Yuan1∗ , Zelai Xu1∗ , Zheyue Tan2 , Xiangmin Yi1 ,
Mo Guang3 , Kaiwen Long3 , Haojia Hui3 , Boxun Li4 , Xinlei Chen1 , Bo Zhao2 ,
Xiao-Ping Zhang1† , Chao Yu1† , Yu Wang1†
1
Tsinghua University, 2 Aalto University, 3 Li Auto Inc., 4 Infinigence-AI
 Project Page § Code
Models

A BSTRACT
Developing large language models (LLMs) to cooperate and compete effectively
within multi-agent systems (MASs) is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing
reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios
remains underexplored due to the challenges of long-horizon credit assignment
and agent-specific advantage estimation. To address these challenges, we introduce MARSHAL, an end-to-end RL framework that incentivizes Multi-Agent
Reasoning through Self-play witH strAtegic LLMs in both cooperative and competitive games. MARSHAL features a turn-level advantage estimator that aligns
learning signals with each interaction for credit assignment, and an agent-specific
advantage normalization to stabilize multi-agent training. By learning with selfplay across cooperative and competitive games, MARSHAL agent trained from
Qwen3-4B develops strong strategic abilities that generalize to held-out games with
up to 28.7% performance improvements. More importantly, the capability acquired
through self-play generalizes beyond games, yielding consistent performance gains
of MASs in reasoning benchmarks. When integrated into leading MASs, our MARSHAL agent achieves significant performance gains of up to 10.0% on AIME, 6.6%
on GPQA-Diamond, and 3.5% on average across all benchmarks. These results
establish end-to-end RL training with self-play in strategic games as a powerful
approach for developing generalizable multi-agent reasoning capabilities in LLMs.

Strategic Ability

Generalization to Multi-Agent Systems

Generalize

MATH
Connect Four

Tic-Tac-Toe

60

Train

MATH
100

100

25
36

Test

15

GSM8K

GPQA

90

100

50

90

Kuhn
50
Poker

36

30

60

80

AQUA

75

Mini
Hanabi

30
50

Simple
Hanabi

90

MMLU AQUA

30

90

50

40

(Cooperative)

80

70

90

AMC

95

90
100

AIME

Qwen3-4B

85

MMLU

50

80

AIME

60

90

AutoGen

(Competitive)

80

GPQA

90

100

MAD

Leduc
Hold’em
90

45

GSM8K

30

AMC

SPIRAL

MARSHAL

Figure 1: Evaluation of MARSHAL and two baselines on strategic games and reasoning benchmarks.
MARSHAL incentivizes multi-agent reasoning ability via self-play in strategic games and generalizes
to improvements of multi-agent systems like MAD and AutoGen on math and QA benchmarks.

∗

Equal contribution: {yuanhuining0, zelai.eecs}@gmail.com
Corresponding authors: xiaoping.zhang@sz.tsinghua.edu.cn,
yuchao@tsinghua.edu.cn, yu-wang@tsinghua.edu.cn
†

1

Preprint. Under review.

1

I NTRODUCTION

The remarkable capabilities of large language models (LLMs) have revolutionized numerous domains, enabling strong performance on a wide range of tasks from question answering to code
generation (Achiam et al., 2023; Team et al., 2023). However, many real-world scenarios, such as
negotiation (Bianchi et al., 2024), strategic gameplay (Silver et al., 2016; FAIR et al., 2022), and
collaborative software development (Qian et al., 2023; Wu et al., 2024a), inherently involve multiple
agents interacting over long horizons. Enabling LLMs to cooperate and compete effectively within
multi-agent systems (MASs) is a critical frontier for advancing artificial intelligence.
While reinforcement learning (RL) has demonstrated remarkable success in enhancing the reasoning
capabilities of individual LLMs (Guo et al., 2025; Team et al., 2025), its extension to multi-turn,
multi-agent tasks faces two critical challenges. First, the problem of long-horizon credit assignment
arises in multi-turn interactions. As a sequence of actions can each result in a immediate reward while
collectively leading to the final sparse reward, accurately attributing the contribution of the action in
each turn is inherently more complex than standard single-turn settings. Second, multi-agent training
couples heterogeneous game roles with asymmetric information and different payoff scales, which
introduces variance in advantage estimation, destabilizing the training process.
In this work, we present MARSHAL (Multi-Agent Reasoning through Self-play witH strAtegic
LLMs), an end-to-end RL framework that incentivizes multi-agent reasoning skills via self-play
in strategic games. We introduce two novel techniques to address the challenges in multi-turn,
multi-agent self-play with Group-Relative Policy Optimization (GRPO) (Shao et al., 2024). First, we
propose a simple yet effective turn-level advantage estimator to enable fine-grained credit assignment.
This allows the model to accurately attribute long-term outcomes to individual actions and provide
learning signals across multiple turns and agents. Second, we propose an agent-specific advantage
normalization that stabilizes the training process by calibrating advantage estimates relative to the
performance of each agent. This normalization accounts for the heterogeneous roles in multi-agent
systems and ensures stable policy updates. By integrating these components, MARSHAL enables
robust self-play learning from game outcomes, empowering LLMs to develop strategic abilities and
multi-agent reasoning skills through cooperation and competition with themselves.
To evaluate the performance and generalization ability of MARSHAL agents, we conduct extensive experiments by training Qwen3-4B in a diverse range of cooperative and competitive games.
Specifically, MARSHAL agents exhibit strong strategic ability across all game environments, with
up to 28.7% performance improvements in three held-out games. More importantly, the capability
acquired through self-play in games further generalizes to improvements of multi-agent systems in
reasoning benchmarks. When integrated into both cooperative and competitive multi-agent systems,
including AutoGen (Wu et al., 2024a) and MAD (Liang et al., 2023), our MARSHAL agents achieve
performance improvements up to 10.0% on AIME, 6.6% on GPQA, and 3.5% on average across
all benchmarks. We further conduct ablation studies to validate the effectiveness of our algorithmic
design, complemented by a comprehensive analysis of reasoning patterns and failure modes to
understand the successful generalization. The evaluation results establish MARSHAL as a powerful
approach for developing generalizable multi-agent reasoning capabilities in LLMs.
In summary, our contributions are as follows:
• We propose MARSHAL, an end-to-end RL framework that enhances generalizable multi-agent
reasoning capability through self-play in a diverse range of strategic games.
• We introduce two novel techniques of turn-level advantage estimator and agent-specific normalization to address the credit assignment and advantage variance in multi-turn, multi-agent RL
training for LLMs.
• We perform extensive experiments and ablation studies to show that MARSHAL incentivize
strong strategic ability and multi-agent reasoning capability that are generalizable to held-out
games and multi-agent LLM systems.

2

P RELIMINARY

In standard RL fine-tuning for tasks like math solving, the environment is static. The model generates
a complete Chain-of-Thought response, and only then is a terminal reward assigned. This process is
2

Preprint. Under review.

Player 0 Loop

GRPO

Player 1 Loop

Env

Turn1
a

s
As player X.
I should..
My action is..

s
You are X.
Current state..
Legal actions..

MARSHAL

All Trajectories
Turn3

Player 1 Traj
Player 0 Traj Turn2 Turn4

Turn5

Turn1

a
You are O.
Current state..
Legal actions..

Now, as O.
I should..
My action is..

Traj-Level
Rewards

Turn3

Turn-Level
Rewards

R

Turn-Level
Returns

LLM

Turn1 (s,a,r)

Turn2

Player 0 trajectory
Turn1

Turn3

Turn5

...

Turn3

Turn4

Turn5

Turn2

Turn4

Turn6

Player 1
Player 0

R1 R3 R5

BatchNorm()

...

Player 1 trajectory

r1 r3 r5

CumSum()

BatchNorm()

Game episode

Turn6

Turn5

Traj-Level
Advantages

Turn-Level
A1 A3 A5
Advantages

A

Token-Level Advs

...

Token-Level Advs
A1

A

A3

A5

Figure 2: Overview of MARSHAL. Left column: generating player trajectories through self-play in
strategic games. Middle column: naive advantage estimation by GRPO. Right column: advantage
estimation by MARSHAL for accurate credit assignment in multi-turn, multi-agent setting.
typically modeled as a token-level Markov decision process (MDP). Here, an trajectory consists of
a single "turn"—the generation of one complete response with multiple tokens. The state at step t,
is the sequence of previously generated tokens (o1 , ..., ot−1 ), and the action is the selection of the
next token ot . Given a task question q, the goal is to optimize the token-level policy π(ot |q, o<t ) to
produce a sequence that maximizes the final, sparse reward.
In contrast, strategic games introduce a more complex, hierarchical decision-making structure. An
entire game, not a single response, constitutes an episode. This is best modeled as a turn-level
MDP, where decisions occur at two levels. At the high level, the state sk represents the state of
the game at the beginning of turn k (e.g., the board configuration, cards in hand). A high-level
action, ak , corresponds to the agent’s entire output for that turn (e.g., "I place my X in the top-left
corner and here is why..."). This action ak is itself a sequence of multiple tokens generated by the
LLM’s low-level, autoregressive policy. In this case, the goal is to optimize the turn-level policy
QT
PK
π(ak |sk ) = t=1 π(ok,t |sk , ok,<t ) to maximize the total return of all turns R = k=1 rk .

3

M ETHOD

In this section, we introduce MARSHAL, an end-to-end RL framework that enhances multi-agent
reasoning ability of LLM through self-play in a diverse range of cooperative and competitive games.
We begin by outlining the overall architecture of our training framework, building upon GroupRelative Policy Optimization (GRPO) (Shao et al., 2024). We then detail our primary technical
contributions for addressing the credit assignment and advantage estimation challenge in multi-turn,
multi-agent training. Finally, we describe the selection of our game environments and design of
reward structures. An overview of our proposed method is shown in Fig. 2.
3.1

S ELF - PLAY WITH GRPO

To eliminate the extensive computational cost introduce by the critic model in classic Proximal
Policy Optimization (PPO) (Schulman et al., 2017), GRPO rollout each query for multiple times,
i G
constructing a group of G responses {oi }G
i=1 and their corresponding outcome rewards r = {r }i=1 ,
then estimate the advantage of each response by their relative reward to their corresponding group,
which assigns equal advantage to the whole sequence (Fig.2 middle), yielding:
i

|o |
G
1 X 1 X
(1)
JGRPO (θ) = Eq∼P (Q),{oi }G
Jsurr (πθ ; πθold , Ait , ε)
i|
i=1 ∼πθold (O|q) G
|o
t=1
i=1
h

 i
π (oi |q,oi )
π (oi |q,oi )
where Jsurr (πθ ; πθold , Ait , ε) = min πθ θ (ot i |q,o<ti ) Ait , clip πθ θ (ot i |q,o<ti ) , 1 − ε, 1 + ε Ait is the
old

t

<t

old

t

<t

i

token-level PPO surrogate objective, and the advantages are estimated by Ait = r −mean(r)
.
std(r)
3

Preprint. Under review.

In multi-agent self-play, where all players in a strategic game is controlled by the same model, each
episode of game results in a multi-turn trajectory for each player role, respectively. As a naive
generalization of the original GRPO to our multi-turn scenario, we start by considering all trajectories
i
G
from a game environment as a group of response, i.e. {(sik , aik )K
k=1 }i=1 , with total return as the
i G
terminal reward for each trajectory r = {R }i=1 . Then, with an additional summation for multiple
turns, GRPO directly generalized to:
i

i

|ok |
G
K
X
1 X 1 X
Jsurr (πθ ; πθold , Aik,t , ε)
G i=1 K i
|oik | t=1

1
multi
JGRPO
(θ) = Esik ∼P (S),oik,t ∼πθ (O|sik ,oik,<t )
old

(2)

k=1

i

where the advantages are estimated by Aik,t = R −mean(r)
. This assigns equal advantage to all tokens
std(r)
in the multi-turn trajectory, similar to the original GRPO. For multi-game training, trajectories for
each game are naturally considered separate groups and normalized independently.
3.2

A DVANTAGE ESTIMATION FOR MULTI - TURN , MULTI - AGENT LEARNING

While Group-Relative Policy Optimization (GRPO) has demonstrated remarkable efficiency and
stability in single-turn settings, such direct application to the multi-turn, multi-agent structure of
self-play introduces significant challenges of long-horizon credit assignment and agent-specific
advantage estimation. To address these issues, we introduce two novel modifications to GRPO.
Turn-level advantage estimator. To enable fine-grained credit assignment across a long trajectory,
G
we incorporate the sequence of turn-level rewards r = {(rki )K
k=1 }i=1 . This setup is analogous to
the "Process Supervision" setting in the original GRPO paper (Shao et al., 2024). In that context,
the proposed advantage estimation involves first normalizing all rewards across the entire batch,
r̃ki = (rki − mean(r))/std(r), and then computing the cumulative sum of these normalized values,
PK
Aik = k̂=k r̃k̂i . While this procedure normalizes the immediate rewards, a critical flaw remains: the
advantages Aik are not, in general, zero-mean, which can reintroduce instability into training.
To rectify this, we propose a crucial inversion of these two steps: we first sum, then normalize (Fig.2
right). Specifically, we begin by calculating the standard Monte Carlo return (or cumulative reward)
PK
from each turn k onwards: Rki = k̂=k rk̂i . We then compute the advantage by normalizing these
returns to their mean: Aik,t = Rki − mean(R), where R is the collection of all cumulative rewards in
the group. This formulation is equivalent to Generalized Advantage Estimation (GAE) (Schulman
et al., 2015) with γ = 1 and λ = 1, where the state-value function V (sk ) is approximated by a
simple, yet effective, baseline—the empirical mean of the batch returns E[R]. By ensuring the
final advantages are properly centered, this method provides a much more stable learning signal for
multi-turn decision-making.
Agent-specific advantage normalization. In many games, the expected return can be highly
dependent on a player’s role (e.g., player 0 vs. player 1, or different roles in a cooperative game).
Normalizing advantages across a heterogeneous batch of roles would force all players toward a shared
baseline, which is statistically inappropriate and can wash out role-specific learning signals.
To address this, we refine our advantage calculation further. We partition the batch of trajectories
into sub-groups based on player role and apply the turn-level advantage estimator described above
independently within each sub-group Gp (Fig.2 right), where p denotes the player index. This ensures
that the advantage for a given action is calculated relative to the average outcome for that specific role,
providing a more accurate and stable credit assignment in multi-agent settings. This agent-specific,
sum-then-normalize advantage estimation forms the final objective for MARSHAL:
JMARSHAL (θ) = Esp,i ∼P (S),op,i ∼πθ
k

k,t

old

p,i
(O|sp,i
k ,ok,<t )
p,i

|ok |
Gp
P
K
1 X 1 X 1 X 1 X
Jsurr (πθ ; πθold , Ap,i
k,t , ε)
P p=1 Gp i=1 K i
|op,i
|
k
t=1
i

(3)

k=1

p,i
p
p
p
where Ap,i
k,t = Rk − mean(R ), R denotes the total set of cumulative rewards from subgroup G .

4

Preprint. Under review.

3.3

G AME ENVIRONMENTS

To cultivate complementary multi-agent skills for MARSHAL, we design a portfolio of six strategic,
two-player games. These games are partitioned into a training set and a more complex, held-out
testing set for out-of-distribution (OOD) evaluation:
• Perfect-information, competitive games: To build deterministic planning and role adaptation,
we train on Tic-Tac-Toe, requiring the agent to recognize its strategic position (e.g., first-mover
vs. second-mover) and plan accordingly. For evaluation, Connect Four serves as a more complex
out-of-distribution test due to its vastly larger state space.
• Imperfect-information, competitive games: To foster robust reasoning and decision-making
under imperfect-information and uncertainty, we train on Kuhn Poker, a simplified poker variant.
We evaluate generalization on the more sophisticated Leduc Hold’em.
• Imperfect-information, cooperative games: The inclusion of cooperative games is a key
differentiator of our work. We use Hanabi to develop social intelligence skills like intent
recognition and Theory of Mind. Specifically, we train on Mini Hanabi and evaluate on a more
challenging variant (Simple Hanabi) to test the generalization of cooperative strategies.
Collectively, this diverse portfolio of both competitive and cooperative challenges ensures the agent
develops a robust multi-agent skillset and avoids overfitting to the narrow, competitive-only mindset
of prior work (Liu et al., 2025).
3.4

R EWARD DESIGN

While our primary learning signal is derived from unsupervised game outcomes to minimize reward
engineering, we incorporate two auxiliary rewards to ensure stable training. The final reward signal
is composed of three components:
• Intrinsic game rewards: The primary signal is the intrinsic game outcome. This is a +/-1
reward for a win/loss/draw in Tic-Tac-Toe, chips won or lost in Kuhn Poker (max of 2), and a
shared +1 reward per successfully played card in Mini Hanabi (max of 4). To balance these
varying scales during multi-game training, we normalize the maximum reward across games to
4 by scaling the Tic-Tac-Toe reward with a factor of 2.
• Action format regularization: To ensure parsable outputs, we provide a small positive reward
(+0.05) for validly formatted actions and a large negative penalty (-10.0) that terminates the
game for invalid ones, similar to the approach in DeepSeek-R1 (Guo et al., 2025).
• Response length penalty: To encourage conciseness, we apply a turn-level penalty for verbosity,
inspired by Kimi k1.5 (Team et al., 2025), which scales linearly for responses longer than a set
threshold. The penalty is calculated as:


l − lmin
rlength (l) = α · max 0, 1 −
(4)
lmax − lmin
where we set lmin = 11, lmax = 2048, and the scaling coefficient α = 0.5.

4

E XPERIMENTS

In this section, we present a series of extensive experiments designed to validate MARSHAL’s ability
to obtain generalizable multi-agent skills. Additional results can be found in the Appendix.
4.1

E XPERIMENTAL SETUP

We use Qwen-4B (Yang et al., 2025), a state-of-the-art instruction-tuned model, as our foundation to
measure the incremental multi-agent capabilities unlocked by MARSHAL. We train two model types:
specialist agents on single games (Tic-Tac-Toe, Kuhn Poker, Mini Hanabi) and a generalist agent on
all three simultaneously. Our primary baseline is SPIRAL, a recent work focused on self-play in
purely competitive, zero-sum games (Liu et al., 2025). Our evaluation is structured in four stages:
5

Testing Games

Generalize

Training Games

Preprint. Under review.

Tic-Tac-Toe
80

75.3
70.2 73.5

77.2

30

60
20

40

Kuhn Poker
32.1

18.6

29.1

60

18.8

0

0

ﬁrst move

62.3

5
0.0

0

26.9

30.9 30.6
28.9

10

10

5

0

ﬁrst move

0

40

3.9

3.9
0.0

second move

14.8
11.0
8.3
4.4

second move

60

63.9
47.7

0

0

24.5

ﬁrst move

Qwen3-4B

SPIRAL

Simple Hanabi

60
40

40
20

61.7

40

55.1

28.7

47.4

27.3

33.9

36.8

30
20

20
0

30.0

37.4

20

Leduc Hold’em

15

20

0

ﬁrst move

55.5 54.3

20
10

20

second move

Mini Hanabi
60

22.5
15

Connect Four
30

76.2

40

10

20

79.0

80

13.9

13.9

10

second move

MARSHAL Specialist

0

MARSHAL Generalist

Figure 3: Average normalized game returns. Specialist agents not only master their training domains
but also generalize effectively to their more complex, held-out counterparts (e.g., from Tic-Tac-Toe to
Connect Four). The generalist model achieves consistently high performance across the entire suite
of games, establishing it as the most robust and versatile agent.
1. Strategic ability: We first evaluate strategic competence by benchmarking agents against
strong Monte Carlo Tree Search (MCTS) or Nash Equilibrium (NE) opponents. Performance is
measured over 1000 games on both the training games and a suite of more complex, held-out
variants. For the cooperative Hanabi games, we report the standard self-play score.
2. Generalization to multi-agent systems: This is the primary test of this work. We integrate
MARSHAL agents into two established systems: the competitive MAD framework (Liang et al.,
2023) and the cooperative AutoGen framework (Wu et al., 2024a), measuring performance on
downstream math and QA benchmarks.
3. Pattern analysis: We conduct a qualitative analysis of the agent’s reasoning process and a
quantitative analysis of failure modes to understand the successful generalization to MASs.
4. Ablation studies: Finally, we perform ablation studies to validate our key algorithmic designs,
particularly our novel advantage estimation technique.
S TRATEGIC ABILITY
Tic-Tac-Toe Specialist
on Tic-Tac-Toe
Game Return

Our analysis begins with strategic ability on games, with normalized
game return detailed in Fig 3. The MARSHAL framework proves
highly effective: specialist agents not only master their training
domains and outperform baselines, but also generalize effectively
to more complex, held-out counterparts (e.g., from Tic-Tac-Toe to
Connect Four). We also observe evidence of cross-category skill
transfer. For example, the Tic-Tac-Toe specialist model demonstrates
smooth improvement not only on Tic-Tac-Toe, but also the OOD
Mini Hanabi (Fig. 4), suggesting MARSHAL cultivates foundational
skills like turn-based planning that are broadly beneficial.

-0.4
-0.5
-0.6
-0.7

Tic-Tac-Toe Specialist
on Mini Hanabi
Game Return

4.2

1.6
1.2
0.8

Crucially, the generalist model, trained on all environments simulta0
40 80 120 160 200
neously, achieves high performance across the entire suite of games,
Step
achieving 28.7% improvement on Leduc Hold’em and 22.9% on Figure 4: Eval curves of the
Simple Hanabi. Its broad competence across competitive and coop- Tic-Tac-Toe specialist in Ticerative settings establishes it as our most robust agent overall.
Tac-Toe and Mini Hanabi.
4.3

G ENERALIZATION TO MULTI - AGENT SYSTEMS

The ultimate test of MARSHAL is whether skills honed in games transfer to practical, out-of-domain
challenges. We evaluate this on a suite of demanding mathematics and general QA benchmarks,
including MATH500 (Cobbe et al., 2021), GSM8K, AQUA-RAT (Ling et al., 2017), AIME24,
AMC23, MMLU-STEM (Hendrycks et al., 2020), and GPQA-Diamond (Rein et al., 2024). As a
preliminary step, we investigate how our game tasks enhances foundational reasoning in a standard
6

Preprint. Under review.

Table 1: Evaluation results on downstream reasoning benchmarks within multi-agent systems.
Competitive game-trained agents excel in the competitive MAD framework, while the cooperativetrained agent excels in the cooperative AutoGen framework. The generalist model performs robustly
across both. Bold and underlined indicate the best and second-best scores, respectively.
MATH

GSM8K

Math
AQUA

AIME

AMC

QA
MMLU GPQA

60.74
63.75

87.60
87.50

94.60
94.80

39.80
51.20

36.70
36.70

70.00
80.00

57.10
58.70

39.39
37.37

63.54
61.38
62.05
62.79

89.10
87.80
88.10
89.90

95.20
94.50
94.70
94.60

46.50
48.40
48.00
52.00

40.00
33.30
43.30
33.30

77.50
72.50
65.00
75.00

57.60
59.30
58.90
59.90

38.89
33.84
36.36
34.85

72.45
73.41

90.20
91.60

95.91
95.45

80.71
81.89

40.00
40.00

75.00
77.50

87.42
87.01

37.88
40.40

75.01
74.54
73.70
75.96

92.20
91.60
91.40
92.80

96.06
96.21
95.60
95.60

83.07
82.68
82.68
83.86

43.33
40.00
43.33
46.67

82.50
82.50
77.50
80.00

86.76
87.39
87.04
87.36

41.12
41.41
38.38
45.45

79.14
80.05

93.40
94.20

94.69
94.47

85.04
86.61

56.67
60.00

87.50
87.50

89.21
91.60

47.47
45.96

80.15
81.54
81.54
82.15

94.40
95.80
94.40
95.20

94.69
94.39
94.54
94.54

87.01
86.61
86.22
86.61

60.00
63.33
66.67
66.67

90.00
92.50
95.00
92.50

89.53
89.65
88.98
89.53

45.45
48.48
44.95
50.00

Setting

Model

Average

Single
Agent

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

MAD
(Competitive)

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

AutoGen
(Cooperative)

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

single-agent setting. Notably, MARSHAL models achieve notable improvements over both the
Qwen-4B baseline on a number of math benchmarks, on par with SPIRAL (Table 1).
We further embed our agents into established multi-agent systems to directly measure their cooperative and competitive capabilities. In the competitive MAD debate framework, agents trained on
competitive games show a clear advantage. Notably, the generalist agent is able to achieve an average
of 3.51% over the original Qwen3-4B across all benchmarks. Conversely, in the cooperative AutoGen
framework, the agents trained for cooperation—the Hanabi specialist and the generalist—excel
across numerous benchmarks. In particular, the generalist model achieves a striking gain of 6.57% on
GPQA-Diamond in the MAD framework, and 10.00% on AIME in the AutoGen framework.
These results provide compelling evidence that MARSHAL forges distinct, transferable skills for
both competition and collaboration. These capabilities directly translate to improved performance in
downstream multi-agent applications, with the generalist model proving to be the most robust.
4.4

R EASONING PATTERN AND FAILURE MODE ANALYSIS

To understand the mechanisms behind MARSHAL’s successful generalization, we analyze the agent’s
reasoning (<think>) traces. This qualitative analysis reveals the emergence of key multi-agent
skills cultivated in our mix of strategic games. We highlight two representative patterns corresponding
to the competitive and cooperative training environments, as showcased in Table 2.
First, reflecting the skills honed in competitive games, MARSHAL develops a role-aware strategy. In
Tic-Tac-Toe, the agent explicitly identifies itself as the "second-move player" and adopts a defensive
strategy. This skill transfers directly to the MAD debate framework, where the agent recognizes its
role as the "negative side assistant" and adopts a negative stance rather than just solving the problem.
More profoundly, MARSHAL cultivates intent recognition, a key component of Theory of Mind. In
the cooperative Hanabi, the agent learns to interpret a teammate’s actions as communications with
hidden intent (e.g., "Maybe they want me to play this card?"). This sophisticated skill transfers to the
AutoGen framework, where the agent infers a collaborator’s uncertainty from subtle cues, such as a
7

Preprint. Under review.

Table 2: Qualitative analysis of emergent reasoning patterns.
Skill

Manifestation in Game-Play

Generalize to Multi-Agent Systems

Role
The Tic-Tac-Toe specialist recognizes its role
Understanding as the second player (O) and adopts a defensive strategy.
<think>
Okay, so I am playing the game of Tic-TacToe as the mark O... As the second-move
player, I should prioritize blocking the X
marks from forming a horizontal, vertical,
or diagonal line...
</think>

The same agent, acting as the "negative" debater in MAD, adapts its strategy to refute
the opponent.
<think>
Okay, so I need to figure out the minimum
number of... The affirmative side just argued for 4. As the negative side assistant,
I should try to find an alternative solution
and convince the judge...
</think>

Intent
Recognition

The same agent, acting as a user proxy in
AutoGen, infers uncertainty from a collaborator’s missing ‘TERMINATE‘ token.
<think>
Okay, let’s see. The assistant gave the answer 17.5 m/s, but did not end the conversation with ’TERMINATE’. Maybe the assistant is not sure with this answer? ...
</think>

The Hanabi specialist infers the intent behind a teammate’s ambiguous hint.
<think>
Okay, so I need to figure out the next move.
Player 0 just revealed my only red card.
Wait, but the rank remains unknown. Maybe
they want me to play this card to the
stack? ...
</think>
Failure Categories

Failure Rate

80
55.2

60
40
20
0

Breakdown of Category 2: Inter-Agent Misalignment
70.7 67.4

43.7
29.5

12.2

13.4

13.2
9.9

9.3

10

13.9

1:
ory n
teg esig s
a
e
C
D
m Issu
ste
Sy

14.9

15

12.2 11.6

5

2:
ory t
teg Agen t
a
C eren
Int lignm
sa
Mi

3:
ory k
teg Tas n
a
C
tio
ca
riﬁ
Ve

0.5

0

1.1

0.1

Co

ion
at t
ers Rese
nv
for

sk
oA
il t ion
Fa ﬁcat
i
r
Cla

De

sk
Ta nt
e
lm
rai

0.7

r
n
ion
tio h
the t
at g
Ac
d O pu
g- atc
orm din
ore t’s In
Inf hhol
nin ism
n
o
g
n
t
I
M
e
as
Wi
Ag
Re

Qwen3-4B

MARSHAL Generalist

Figure 5: Percentage of different failure modes in GPQA-Dimond. Through MARSHAL training,
the generalist agent significantly reduces the occurrence of Inter-Agent Misalignment.

missing TERMINATE token, instead of treating it as a simple error. Together, these patterns provide
qualitative evidence that MARSHAL goes beyond improving benchmark scores; it equips the agent
with the cognitive toolkit essential for effective multi-agent interaction.
To further substantiate these qualitative observations with quantitative evidence, we performed a
failure mode analysis on the GPQA-Diamond benchmark within the MAD framework. Adopting the
taxonomy of Cemri et al. (2025), we categorized failures into System Design Issues (e.g., formatting
errors, loop repetition), Inter-Agent Misalignment (e.g., multi-agent reasoning failures like ignoring
peers or task derailment), and Task Verification issues.
As shown in Fig 5 (left), while MARSHAL improves basic instruction following (reducing System
Design Issues by ∼7%), the reduction in Inter-Agent Misalignment is significantly larger (11.5%).
To understand the drivers of this strategic improvement, we further decompose the Inter-Agent
Misalignment category into its sub-categories. The breakdown reveals that the performance gains are
primarily driven by reductions in Task Derailment and Ignored Other Agent’s Input. This indicates
that MARSHAL agent are actively listening to peers and maintaining focus on its objective, validating
our hypothesis that game-theoretic self-play cultivates transferable multi-agent reasoning skills.
4.5

A BLATION STUDIES

To validate our core algorithmic designs, we conduct two targeted ablation studies: (1) comparing
self-play to training against a fixed opponent, and (2) sequentially removing the two key algorithmic
components: our turn-level advantage estimator and agent-specific advantage normalization.
8

Preprint. Under review.

Table 3: Generalization comparison between MARSHAL (self-play) and its fixed-opponent variant.
The latter exhibits significant overfitting to static environments and opponents. Values denote average
normalized game returns; for competitive games, entries indicate first-move / second-move returns.
Underlined scores indicate performance degradation compared to the standard MARSHAL model.
Training Games

Testing Games
Leduc
Mini Connect Four
Hold’em
Hanabi

Simple
Hanabi

MARSHAL (Tic-Tac-Toe) 75.30 / 32.10 74.15 / 3.42
w/ fixed opponent
88.00 / 41.95 63.15 / 28.84

50.48
34.93

30.65 / 14.85 58.36 / 27.65
20.35 / 5.65 47.38 / 35.55

29.75
12.22

MARSHAL (Kuhn Poker) 69.85 / 25.50 79.04 / 22.49
w/ fixed opponent
0.00 / 0.00 76.19 / 15.64

44.98
0.00

27.60 / 12.70 63.94 / 62.10
0.00 / 0.00 0.00 / 0.00

29.35
0.00

Model

Tic-Tac-Toe

Kuhn Poker

Table 4: Ablation results for algorithmic design. Both our turn-level advantage estimator and agentspecific advantage normalization prove essential for performance. Notation follows Table 3.
Training Games

Testing Games
Leduc
Mini Connect Four
Hold’em
Hanabi

Simple
Hanabi

MARSHAL (Tic-Tac-Toe) 75.30 / 32.10 74.15 / 3.42
w/o Turn-Level.
74.60 / 24.15 80.26 / 28.35
w/o Agent-Specific.
82.70 / 31.20 70.89 / 11.24

50.48
34.80
44.10

30.65 / 14.85 58.36 / 27.65
26.75 / 12.30 48.34 / 41.34
25.40 / 10.50 51.04 / 49.88

29.75
19.05
21.72

MARSHAL (Kuhn Poker) 69.85 / 25.50 79.04 / 22.49
w/o Turn-Level.
63.35 / 19.65 92.49 / 21.02
w/o Agent-Specific.
69.55 / 24.55 75.37 / 19.55

44.98
41.65
40.18

27.60 / 12.70 63.94 / 62.10
29.60 / 10.85 32.26 / 31.23
27.00 / 10.50 35.73 / 21.50

29.35
22.98
22.42

MARSHAL (Hanabi)
w/o Turn-Level.
w/o Agent-Specific.

55.55
53.20
52.50

26.75 / 5.75
25.25 / 3.05
32.10 / 5.10

33.93
30.68
32.08

Model

Tic-Tac-Toe

Kuhn Poker

71.90 / 7.35 72.52 / 9.29
67.55 / 10.60 68.45 / 31.78
68.15 / 13.40 74.15 / 10.27

37.36 / 55.12
54.79 / 47.77
44.30 / 56.41

Self-play vs. fixed-opponent learning. Training agents against fixed, expert opponents reveals a
intuitive and critical flaw: the agents overfit. As shown in Table 3, these models fail to generalize,
exhibiting significant performance drop on games outside their direct training domain. This is
particularly evident with the Kuhn Poker specialist, which suffers a severe degradation on non-poker
games—a clear case of strategic mode collapse. These results confirm that the adaptive curriculum
provided by self-play is essential for the development of robust, generalizable policies.

Density

Density

Analysis of advantage estimation design.
Return Distribution of Tic-Tac-Toe
Player 0
player 0 mean=0.49
We then ablate our two-part advantage estimation technique. First, reverting from our
fine-grained turn-level advantage estimator to
player 1 mean=-0.28
Player 1
a coarse, trajectory-level reward signal causes
a significant performance drop, especially for
the Tic-Tac-Toe specialist, which is trained on
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
a long-horizon game. Second, removing agentReturn
Return Distribution of Mini Hanabi
specific advantage normalization also degrades
Player 0
player 0 mean=1.39
performance, particularly in competitive gamestrained model like the Tic-Tac-Toe specialist and
the Kuhn Poker specialist where player expePlayer 1
player 1 mean=1.11
riences and agent return are distinct (Fig. 6).
Conversely, the effect is mild in the Hanabi specialist, which is trained on a cooperative game
-1
0
1
2
3
4
Return
with similar return distribution between players
(Fig. 6). Together, these results confirm that both Figure 6: Return distribution of the generalist
components of our advantage estimation are crit- MARSHAL agent as different player roles on Ticical for effective learning in complex multi-turn, Tac-Toe and Mini Hanabi.
multi-agent environments.
9

Preprint. Under review.

5

R ELATED WORK

LLM-based multi-agent systems. To further extend the ability of LLMs with collaborative intelligence, LLM-based multi-agent systems have been proposed for various tasks. AutoGen (Wu et al.,
2024a) and CAMEL (Li et al., 2023) designs cooperative agents for general reasoning and question
answering. MetaGPT (Hong et al., 2024) and ChatDev (Qian et al., 2023) propose static multi-agent
workflows with specialized roles for software development tasks. Multi-Agent Debate (Liang et al.,
2023) takes a competitive approach by letting LLMs propose and criticize solutions over multiple
rounds for the final answer. These works mainly focus on designing workflows for multi-agent interactions with fixed LLMs. Other work have proposed to generate workflows automatically (Zhang et al.,
2024a). Our work takes a complementary approach by training LLMs to enhance their multi-agent
capabilities and build stronger foundation models that can be integrated with these systems.
Reinforcement learning for LLMs. Reinforcement learning (RL) has emerged as a prominent
approach to enhance LLMs’ ability from instruction following to reasoning. To align LLMs with
human values, RL from human feedback (RLHF) (Ouyang et al., 2022) and from AI feedback
(RLAIF) (Bai et al., 2022) have been widely used in post-training to steer models towards favorable
behaviors. The recent success of reasoning models (Jaech et al., 2024; Guo et al., 2025) further
popularize RL from verifiable rewards (RLVR) for incentivizing long Chain-of-Thought reasoning
capability of LLMs and have been applied in mathematical reasoning (Shao et al., 2024), coding (Wei
et al., 2025), and tool-using (Jin et al., 2025). However, these works primarily consider single-turn
or single-agent tasks, while our approach extends RL training to multi-turn, multi-agent scenarios.
Recently, MT-GRPO (Zeng et al., 2025) also address the turn-level credit assignment issue for multiturn RL. However, they propose to normalize the turn-level rewards turn-by-turn, which can introduce
variance in strategic games with varying number of turns. We propose a "sum-then-normalize"
approach that takes advantage of all turns in the normalization, increasing robustness.
Self-play training of LLMs. The great success of achieving superhuman performance in multiagent games (Silver et al., 2016; Vinyals et al., 2019; Berner et al., 2019) has spurred the use of
self-play to train LLMs by playing with themselves (Zhang et al., 2024b). Some work (Chen et al.,
2024; Wu et al., 2024b) combines game-theoretic property of self-play to overcome intransitivity
of human preference in LLM alignment. Another line of work (FAIR et al., 2022; Xu et al., 2023;
2025a) uses self-play to improve the strategic ability of LLM agents in specific games. The most
relevant work to ours is SPAG (Cheng et al., 2024) and SPIRAL (Liu et al., 2025), which also train
LLMs via self-play to show generalization to reasoning tasks. However, these work focuses on
zero-sum game and show generalization in single-agent evaluations. In contrast, our work consider
both cooperative and competitive games and demonstrate generalized improvement in multi-agent
system. As a concurrent work to ours, SPIRAL also address the heterogeneity of player role in
advantage estimation and introduces a Role-Conditioned Advantage Estimation (RAE), analogous to
our agent-specific normalization. We advance this finding by revealing that role separation is only
critical in games with distinct return distributions between players.

6

D ISCUSSION

In this work, we introduced MARSHAL, a framework utilizing self-play in a diverse range cooperative and competitive strategic games to cultivate multi-agent capabilities in LLMs. Our findings
demonstrate that skills honed in strategic games directly transfer to enhanced performance in general
multi-agent systems, establishing self-play as a scalable paradigm for training LLM agents.
Despite these promising results, limitations remain. First, our study utilizes two-player games as
efficient "prototypes" to cultivate foundational skills. While we demonstrate that these constrained
settings are sufficient for strong transfer to general tasks, scaling to larger N -player environments
introduces significantly greater challenges regarding non-stationarity, population diversity, and credit
assignment that warrant dedicated future investigation. Additionally, moving beyond classic games
to complex "social sandboxes" (e.g., simulated software engineering, collaborative researching)
represents a compelling direction for training agents in more practical domains.
Ultimately, our work provides strong evidence that the principles of self-play are a powerful engine
for progress, paving the way for the next generation of sophisticated LLM agents.
10

Preprint. Under review.

R EPRODUCIBILITY STATEMENT
All code, model checkpoints, and training scripts required to reproduce the findings of this paper are
publicly available at https://github.com/thu-nics/MARSHAL. The repository includes
all necessary configurations to replicate our key experiments.

E THICS STATEMENT
This work adheres to the ICLR Code of Ethics. The primary ethical consideration of our work is
the dual-use nature of enhancing LLM agent capabilities. While our goal is to foster beneficial
multi-agent skills like cooperation and competition, the resulting models could potentially be applied
to malicious ends. We release our code and models to the research community to encourage further
study into the safety and alignment of more capable agents.

R EFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774, 2023.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness
from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James
Zou. How well can llms negotiate? negotiationarena platform and analysis. arXiv preprint
arXiv:2402.05863, 2024.
Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt
Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm
systems fail? arXiv preprint arXiv:2503.13657, 2025.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning
converts weak language models to strong language models. arXiv preprint arXiv:2401.01335,
2024.
Pengyu Cheng, Yong Dai, Tianhao Hu, Han Xu, Zhisong Zhang, Lei Han, Nan Du, and Xiaolong Li.
Self-playing adversarial language game enhances llm reasoning. Advances in Neural Information
Processing Systems, 37:126515–126543, 2024.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve
math word problems. arXiv preprint arXiv:2110.14168, 2021.
FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,
Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by
combining language models with strategic reasoning. Science, 378(6624):1067–1074, 2022.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,
Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation
harness, 07 2024. URL https://zenodo.org/records/12608602.
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.
11

Preprint. Under review.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300, 2020.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin
Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for
a multi-agent collaborative framework. International Conference on Learning Representations,
ICLR, 2024.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint
arXiv:2412.16720, 2024.
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and
Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement
learning. arXiv preprint arXiv:2503.09516, 2025.
Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.
Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay,
Julien Pérolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel
Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, János Kramár, Bart De
Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian
Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah Ryan-Davis. OpenSpiel: A framework for reinforcement learning in games. CoRR, abs/1908.09453, 2019. URL
http://arxiv.org/abs/1908.09453.
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural
Information Processing Systems, 36:51991–52008, 2023.
Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu,
and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent
debate. arXiv preprint arXiv:2305.19118, 2023.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL, 2017.
Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston
Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via
multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in neural information processing systems, 35:27730–
27744, 2022.
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,
Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. arXiv
preprint arXiv:2307.07924, 2023.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,
Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In
First Conference on Language Modeling, 2024.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,
2015.
12

Preprint. Under review.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun
Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with
llms. arXiv preprint arXiv:2501.12599, 2025.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350–354, 2019.
Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng
Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning:
An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025.
Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried,
Gabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via
reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent
conversations. In First Conference on Language Modeling, 2024a.
Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play
preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024b.
Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for
strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.
Zelai Xu, Wanjun Gu, Chao Yu, Yi Wu, and Yu Wang. Learning strategic language agents in the
werewolf game with iterative latent space policy optimization. In Forty-second International
Conference on Machine Learning, 2025a.
Zelai Xu, Zhexuan Xu, Xiangmin Yi, Huining Yuan, Xinlei Chen, Yi Wu, Chao Yu, and Yu Wang. Vsbench: Evaluating vlms for strategic reasoning and decision-making in multi-agent environments.
arXiv preprint arXiv:2506.02387, 2025b.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint
arXiv:2407.10671, 2024a.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical
expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388,
2025.
13

Preprint. Under review.

Rui Ye, Keduan Huang, Qimin Wu, Yuzhu Cai, Tian Jin, Xianghe Pang, Xiangrui Liu, Jiaqi Su, Chen
Qian, Bohan Tang, et al. Maslab: A unified and comprehensive codebase for llm-based multi-agent
systems. arXiv preprint arXiv:2505.16988, 2025.
Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong.
Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. arXiv preprint
arXiv:2505.11821, 2025.
Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge,
Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv
preprint arXiv:2410.10762, 2024a.
Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng
Ye, Wenbo Ding, Yaodong Yang, et al. A survey on self-play methods in reinforcement learning.
arXiv preprint arXiv:2408.01072, 2024b.

14

Preprint. Under review.

A

U SE OF LLM S

In the preparation of this paper, LLMs were utilized as writing assistants. Their use were limited to
improving grammar, clarity, and overall readability. The core research, including the methodology,
experiments, and analysis, represents the original work of the authors.

B

I MPLEMENTATION DETAILS

Framework and software stack. Our implementation of MARSHAL is built upon ROLL (Wang
et al., 2025), a robust open-source codebase for post-training LLMs with reinforcement learning.
ROLL’s native support for agentic, multi-turn rollouts provided a strong foundation for our system.
To achieve high performance, ROLL leverages vLLM for efficient inference during the rollout
phase (Kwon et al., 2023) and is built on Megatron-LM for distributed training (Shoeybi et al., 2019).
All game environments used in this work were implemented using OpenSpiel (Lanctot et al., 2019)
and VS-Bench (Xu et al., 2025b), ensuring the correctness and standardization of game logic.
Training settings. We use Qwen-4B as our base model for all experiments reported in our main
results. The training is conducted in a fully online manner, where self-play trajectories are immediately
used for policy updates. Specialist agents are trained with a batch size of 128 trajectories, while the
generalist agent uses a batch size of 384 (128 from each game). All models were trained for a total of
200 optimization steps. For optimization, we employ the Adam optimizer (Kingma, 2014) with a
cosine annealing learning rate schedule, which warms up for 10 steps to a peak of 1 × 10−6 before
gradually decaying to 0.
Generation parameters. During the self-play rollout phase, text is generated via nucleus sampling.
We use a temperature of 0.6, a Top-P of 0.99, and a Top-K of 100. These parameters were selected to
encourage a diverse yet coherent exploration of strategies.
Hardware configuration.

C

Models were trained on a single server with 8 NVIDIA H100 GPUs.

E VALUATION DETAILS

Strategic ability. To rigorously assess game-playing proficiency, we evaluate our agents against a
suite of strong, fixed opponents. For competitive games, this includes:
• Tic-Tac-Toe and Connect Four: two MCTS agents with varying simulation counts
(100/1000 for Tic-Tac-Toe, 10/100 for Connect Four) to test against different strengths.
• Kuhn Poker: The exact Nash Equilibrium (NE) policy.
• Leduc Hold’em: A NE policy approximated by 5 × 108 iterations of Counterfactual Regret
Minimization (CFR).
In all competitive games, models are evaluated as both the first-move and second-move player,
measured by the average game return. For the cooperative Hanabi variants, performance is measured
by the standard self-play game return. In all game settings, agents are evaluated for 1000 games.
For the results on strategic ability reported in the main text, we normalize the scores with respect to
the worst-observed performance and the theoretical optimal performance.
For the fixed-opponent ablation studies, the MCTS agent with 100 simulations is used for Tic-Tac-Toe
as the opponent, and the NE policy is used for Kuhn Poker.
Generalization to multi-agent systems. For downstream evaluations, we assess all models on a
comprehensive suite of mathematics and general QA benchmarks. In the single-agent setting, we
utilize evaluation scripts from Qwen2.5-Math (Yang et al., 2024a;b) for MATH500, GSM8K, AQUARAT, AIME24, AMC23, and MMLU-STEM, while employing lm-evaluation-harness (Gao et al.,
2024) for GPQA-Diamond. For multi-agent evaluations, we adopt the MASLab framework (Ye et al.,
2025) across all benchmarks. To balance evaluation efficiency and accuracy, we set the maximum
output length to 8,192 tokens for all settings.
15

Preprint. Under review.

D

S CALING TO LARGER MODELS

To validate the scalability of the MARSHAL framework and ensure our findings are not specific
to the 4B parameter scale, we extended our training to the larger Qwen3-8B model. We trained an
8B-parameter MARSHAL generalist agent using the exact same set of strategic games (Tic-Tac-Toe,
Kuhn Poker, and Mini Hanabi) and hyperparameters as the 4B experiments.
Strategic ability. First, we evaluate the model’s proficiency on both training and held-out games.
As shown in Table 5, the MARSHAL-8B agent achieves significant improvements over the Qwen3-8B
base model across all environments. Notably, we observe the same strong generalization pattern seen
in the 4B model: the agent generalizes effectively to held-out, complex games, achieving a drastic
improvement in Leduc Hold’em (7.26% to 53.89%) and Simple Hanabi (4.55% to 37.27%).
Table 5: Strategic ability comparison on Qwen3-8B. The MARSHAL training yields consistent
improvements on both training games and held-out testing games (Connect Four, Leduc Hold’em,
Simple Hanabi).
Model

Tic-Tac-Toe Kuhn Poker Mini Hanabi Connect Four Leduc Hold’em Simple Hanabi

Qwen3-8B
MARSHAL (Generalist, 8B)

48.38
54.05

33.12
44.49

27.00
55.28

10.48
21.55

7.26
53.89

4.55
37.27

Generalization to multi-agent systems. We further evaluate whether these improved strategic
capabilities translate to general reasoning benchmarks within multi-agent systems. We integrated
the MARSHAL-8B agent into both the competitive MAD framework and the cooperative AutoGen
framework.
The results, detailed in Table 6, confirm the scalability of our approach. The MARSHAL-trained 8B
model consistently outperforms the base model:
• In the competitive MAD setting, we observe an average improvement of 2.60%, with substantial
gains on challenging math benchmarks like AIME (70.00% to 80.00%).
• In the cooperative AutoGen setting, the improvement is even more pronounced, with an average
gain of 3.90%, and a significant boost on AIME (60.00% to 70.00%).
These results demonstrate that the MARSHAL algorithm and game selection scale stably to larger
models, consistently unlocking cooperative and competitive reasoning capabilities.
Table 6: Generalization to multi-agent systems using Qwen3-8B. The MARSHAL generalist consistently improves performance on reasoning benchmarks in both competitive (MAD) and cooperative
(AutoGen) settings.
MAS

Model

Avg

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

MAD

Qwen3-8B
MARSHAL (Generalist, 8B)

82.49
85.09

95.00
96.40

96.36
96.59

83.46
83.46

70.00
80.00

90.00
95.00

89.59
90.70

53.03
53.54

AutoGen

Qwen3-8B
MARSHAL (Generalist, 8B)

79.68
83.58

88.80
94.40

95.91
95.00

83.07
85.04

60.00
70.00

89.19
95.00

89.30
90.04

51.52
55.56

E

C OMPARISON WITH SPIRAL: DECOUPLING ALGORITHM AND GAME
ENVIRONMENTS

To rigorously decouple the contributions of our proposed learning algorithm and our selection of
game environments, we conducted a set of controlled experiments comparing MARSHAL against
SPIRAL baseline methods. We analyze two specific variations:
1. Ablation on algorithm (RAE + our games): We train an agent using SPIRAL’s Roleconditioned Advantage Estimation (RAE) on our full selection of competitive and cooperative
game environments. This isolates the impact of the advantage estimation technique.
16

Preprint. Under review.

2. Ablation on games (MARSHAL algorithm + competitive games): We train an agent using the
MARSHAL algorithm but restricted to competitive-only games (Tic-Tac-Toe and Kuhn Poker),
similar to SPIRAL. This isolates the impact of the cooperative games.
E.1

S TRATEGIC ABILITY COMPARISON

Table 7 presents the performance on both training and held-out games. We analyze the results through
the lens of the two ablations:
Ablation on algorithm. When replacing the MARSHAL algorithm with RAE (row 2), we observe
a sharp performance decline in the cooperative games. Specifically, performance in Mini Hanabi
falls from 54.33% to 24.93%, and in Simple Hanabi from 36.75% to 5.53%. This confirms that RAE
struggles with the dense, multi-turn credit assignment required for cooperative settings, validating
the necessity of MARSHAL’s turn-level "sum-then-normalize" technique to unlock the potential of
self-play learning.
Ablation on games. When training only on competitive games (row 3), the agent naturally fails
to acquire cooperative skills (low scores in Hanabi). This serves as a crucial control baseline for
analyzing the downstream transfer results below.
Table 7: Strategic ability comparison decoupling algorithm and game environments. MARSHAL
(row 1) outperforms both the RAE-based agent (row 2) and the competitive-only agent (row 3),
particularly in cooperative environments.
Model

Tic-Tac-Toe Kuhn Poker Mini Hanabi Connect Four Leduc Hold’em Simple Hanabi

MARSHAL (Full Method)
SPIRAL (RAE) + Our Games
MARSHAL Alg. + Adv. Games

E.2

53.13
48.35
48.80

54.33
24.93
24.48

40.05
42.05
47.55

19.98
9.70
17.90

54.56
6.11
29.98

36.75
5.53
11.48

G ENERALIZATION TO MULTI - AGENT SYSTEMS

We further evaluate how these variations affect transfer to general reasoning benchmarks within
multi-agent systems.
Generalization to competitive systems (MAD). As shown in Table 8, the full MARSHAL method
achieves the highest average performance (75.96%). Notably, the RAE-based agent underperforms
(74.23%), confirming the algorithmic superiority of MARSHAL. The competitive-only agent performs comparably to the full agent, which is expected as MAD is a competitive framework.
Generalization to cooperative systems (AutoGen). Table 9 reveals the critical role of the game
environments. The agent trained on the competitive-only games suffers a significant performance drop
in the cooperative AutoGen framework (Average 80.34% vs. 82.15% for MARSHAL). This confirms
that cooperative games are not merely "more data"; they are a necessary component for transferring
skills to cooperative downstream tasks. Additionally, the RAE-based agent also underperforms, again
solidifying the novelty of the algorithmic design of MARSHAL.
Table 8: Generalization to the competitive MAD framework.
Model

Avg

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

MARSHAL (Full Method)
SPIRAL (RAE) + Our Games
MARSHAL Alg. + Adv. Games

75.96
74.23
75.24

92.80
92.40
92.20

95.60
96.06
95.60

83.86
84.25
83.46

46.67
36.67
46.67

80.00
77.50
80.00

87.36
86.79
86.88

45.45
45.96
41.92

Conclusion. These decoupling experiments confirms the superiority of MARSHAL’s algorithmic
design over SPIRAL’s RAE, and that MARSHAL’s successful generalization is not due to one factor
alone. It requires both our selection of competitive and cooperative games to provide necessary
learning signals, and our novel credit assignment algorithm to effectively learn from them.
17

Preprint. Under review.

Table 9: Generalization to the cooperative AutoGen framework. Note the drop in performance for the
competitive-only games (row 3), highlighting the necessity of cooperative training.
Model

Avg

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

MARSHAL (Full Method)
SPIRAL (RAE) + Our Games
MARSHAL Alg. + Adv. Games

82.15
80.39
80.34

95.20
94.80
94.20

94.54
94.77
94.24

86.61
86.61
86.61

66.67
60.00
60.00

92.50
92.50
90.00

89.53
88.57
89.37

50.00
45.45
47.98

F

C OMPARISON WITH MT-GRPO

A concurrent work MT-GRPO (Zeng et al. (2025)) also identifies the limitations of trajectory-level
estimation in the original GRPO and proposes a fine-grained turn-level strategy. While MT-GRPO
shares our motivation, and is also a critical first step towards fine-grain credit assignment in multi-turn
GRPO, our "sum-then-normalize" approach differs fundamentally from their strategy, offering distinct
advantages in stability and scalability in the game tasks that we consider in this work.
F.1

T HEORETICAL DIFFERENCES

MT-GRPO relies on normalizing the turn-level rewards turn-by-turn (i.e., computing the mean and
variance for the reward at step t across the batch). We identify two structural limitations with this
approach that MARSHAL addresses:
1. The variable length problem: In real-world reasoning and gaming tasks, trajectory lengths
vary significantly. With turn-by-turn normalization, the effective batch size shrinks as shorter
trajectories conclude. This causes the variance of the normalization statistics to increase for later
turns in the sequence, introducing instability for training. In contrast, MARSHAL normalizes
the cumulative return, which remains defined for the entire trajectory regardless of length.
2. Immediate reward vs. long-term return: MT-GRPO’s local normalization focuses on the
immediate step-wise reward relative to peers at that specific step. MARSHAL calculates the
full Monte Carlo return first. By normalizing the cumulative outcome, we capture the long-term
impact of actions relative to the global baseline, preserving critical long-term dependencies
often obscured by local normalization.
F.2

E MPIRICAL C OMPARISON

To validate this theoretical analysis, we implemented MT-GRPO and compared it directly against
MARSHAL. We trained a Tic-Tac-Toe specialist using both algorithms and evaluated their strategic
proficiency across our full game suite.
Strategic ability. As shown in Table 10, the MT-GRPO agent exhibits notable performance drop
across both the training game (Tic-Tac-Toe) and generalization to held-out games (e.g., Connect Four,
Hanabi). In particular, the drop is significant in Mini Hanabi (50.48% to 36.67%), a game requiring
consistent long-term coordination. This confirms that our "sum-then-normalize" formulation is more
robust for the variable-length, multi-turn interactions inherent in strategic games.
Generalization to MAS. We further extended this comparison to the integration within multi-agent
systems to assess if the stability issues in game training affect general reasoning transfer. Table 11
present the results in the competitive MAD and cooperative AutoGen frameworks, respectively.
MARSHAL consistently achieves higher average scores (75.01% vs. 73.77% in MAD; 80.15% vs.
79.10% in AutoGen) and outperforms MT-GRPO on key hard reasoning benchmarks like AIME and
AMC. This indicates that the training proficiency provided by MARSHAL translates directly to better
robust reasoning in general multi-agent tasks.
18

Preprint. Under review.

Table 10: Comparison between MARSHAL and MT-GRPO. The MARSHAL advantage estimation
yields superior performance and generalization compared to the concurrent MT-GRPO method,
validating the robustness of the "sum-then-normalize" approach.
Model (Tic-Tac-Toe Specialist) Tic-Tac-Toe Kuhn Poker Mini Hanabi Connect Four Leduc Hold’em Simple Hanabi
53.70
50.10

MARSHAL (Ours)
MT-GRPO

50.48
36.67

38.79
40.05

22.75
18.55

43.00
39.94

29.75
20.08

Table 11: Comparison of generalization to multi-agent systems between MARSHAL and MT-GRPO
(using Tic-Tac-Toe specialists). MARSHAL achieves higher average performance and demonstrates
greater stability in downstream tasks.
MAS

Model (Tic-Tac-Toe Specialist)

Avg

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

MAD

MARSHAL (Ours)
MT-GRPO

75.01
73.77

92.20
92.60

96.06
95.91

83.07
84.65

43.33
36.67

82.50
77.50

86.76
86.91

41.12
42.13

AutoGen

MARSHAL (Ours)
MT-GRPO

80.15
79.10

94.40
95.40

94.69
94.62

87.01
85.04

60.00
56.67

90.00
90.00

89.53
89.02

45.45
42.93

G

H YPERPARAMETER ANALYSIS ON RESPONSE LENGTH PENALTY

To ensure efficient and stable reasoning within the context window limits, our reward design includes
a response length penalty characterized by three hyperparameters: the minimum length lmin , the
maximum threshold lmax , and the penalty weight α.
Selection of length constraints. The length bounds were chosen based on practical engineering
constraints:
• lmin = 11: This is the hard lower bound determined by the minimum number of tokens
required to generate a syntactically valid response structure with the correct format (e.g.,
<think></think><action>...</action>).
• lmax = 2048: This soft upper bound is derived from the context window of the base model.
Given Qwen3-4B’s 32k context length, we allocated a budget of approximately 16 turns per
game history (32, 768/16 = 2048), which provides a sufficient horizon for the reasoning traces
in our chosen game portfolio while preventing context overflow.
Ablation on penalty weight α. To assess the sensitivity of the framework to the penalty weight α,
we conducted an ablation study training Tic-Tac-Toe specialists with α = 1.0 (stronger regularization),
α = 0.5 (default), and α = 0.0 (no regularization). The results are detailed in Table 12.
Table 12: Ablation study on the length penalty weight α. The default setting (α = 0.5) strikes
a balance between performance and conciseness. Removing the penalty (α = 0) leads to overly
verbose responses and performance degradation in cooperative games like Hanabi. "OL" denotes the
percentage of games lost due to overlong responses.
Strategic Performance

Model Setup

α = 0.5 (Default)
α = 1.0 (Strong)
α = 0.0 (None)

Analysis.

Tic-TacToe

Kuhn
Poker

Mini
Hanabi

Connect
Four

Leduc
Hold’em

Simple
Hanabi

53.70
54.90
54.65

38.79
42.82
40.95

50.48
47.53
38.18

22.75
18.25
23.35

43.00
50.46
28.61

29.75
26.05
20.10

Response Statistics
OL in
OL in
Mini
Simple
Avg Len
Hanabi Hanabi
1700
1657
1954

9.6%
7.7%
20.4%

14.3%
13.9%
27.3%

The results yield two key conclusions:

1. Robustness: The framework is robust to reasonable variations in α. Increasing the regularization
to α = 1.0 maintains comparable strategic performance to the default setting, indicating that the
method is not overly sensitive to the precise magnitude of the penalty.
19

Preprint. Under review.

2. Necessity of regularization: The length penalty is crucial for stability. When removed (α = 0),
the average response length increases significantly (from 1700 to 1954 tokens). This verbosity
leads to a sharp performance drop in cooperative, theory-of-mind-demanding games like Mini
Hanabi (50.48% to 38.18%). This degradation correlates directly with a dramatic increase in
the "Overlong Response" failure rate (9.6% to 20.4%), confirming that the penalty effectively
prevents the model from degenerating into computationally wasteful loops.

H

H YPERPARAMETERS

To ensure reproducibility and consistency, we maintain a unified hyperparameter configuration for all
trained models. The complete settings are provided in Table 13.
Table 13: Hyperparameters
Category
Model Configuration

Training Settings

RL Settings

I

Parameter

Value

Max Response Length
Max Sequence Length
Dtype

4096
32768
bf16

Max Steps
Train Batch Size
Optimizer
Adam parameters (β1 , β2 )
Learning Scheduler
Learning Rate
Weight Decay
Warm-up Step
Gradient Norm Clip

200
128
Adam
(0.9, 0.95)
Cosine Annealing
1 × 10−6
0.05
10
1.0

Sampling Temperature
PPO Epochs
(top P, top k)
KL Loss
KL Loss Coefficient
Entropy Coefficient
Dual Clip Loss
PPO Policy Clip
Lambd
Gamma

0.5
1
(0.99, 100)
True
0.2
0
ture
0.2
0.95
1

G AME OBSERVATION AND PROMPT

Tic-Tac-Toe For Tic-Tac-Toe, we provide the agent with a complete observation of the 3x3 game
board. The state of each cell—whether it is empty, occupied by ’X’, or occupied by ’O’—is explicitly
provided. The prompt clearly indicates which player’s turn it is (’X’ or ’O’) and presents the current
board state, asking the agent to select coordinates for its next move from the available empty cells.
For example, the game begins with a prompt that provides the empty 3x3 grid and asks the agent to
make the first move (Listing 1).
Listing 1: Prompt for Tic-Tac-Toe.
system_prompt:
You are an AI agent that makes optimal decisions to win in the game of
Tic-Tac-Toe.
user_prompt:
GAME RULES:
1. Tic-Tac-Toe is a two-player board game played on a three-by-three grid
. The grid is 0-indexed, where (0,0) is the top-left corner and (2,2) is
the bottom-right corner.

20

Preprint. Under review.

2. Two players take turns placing their marks X and O in empty cells of
the grid.
3. The player who first places three of their marks in a horizontal,
vertical, or diagonal line wins.
4. If all cells are filled and no player wins, the game ends in a draw.
PLAYER INFORMATION:
1. Your mark is X. You are competing with another player controlling the
mark O.
2. In each of your turns:
a. The game state demonstrates the current board with a three-line
text grid, where ’X’ and ’O’ are the marks of the two players, and ’_’
represents empty cells.
b. You need to chose an action to place your mark in an empty cell,
based on the given game state and the history of your decisions.
c. All legal actions for the current turn are provided in the format
of ‘<X({row},{column})>‘, where ‘X‘ is your mark, and {row} and {
column} are integers indicating the row and column of the cell to
place your mark.
RESPONSE INSTRUCTIONS:
Always choose only one action from the legal actions and output ‘<answer
>{your chosen action}</answer>‘ with no extra text after you finish the
thinking process. For example, ‘<answer><X(0,0)></answer>‘. Strictly
follow the above format and keep your thinking process concise. Responses
that do not follow the format will result in immediate loss of the game.
Information of Turn-1:
This is your turn. The game state and legal actions for this turn are
provided below. Please choose your action and strictly follow the given
output format in the response instructions.
GAME STATE:
___
___
___
LEGAL ACTIONS:
<X(0,0)>, <X(0,1)>, <X(0,2)>, <X(1,0)>, <X(1,1)>, <X(1,2)>, <X(2,0)>, <X
(2,1)>, <X(2,2)>.

Kuhn Poker For Kuhn Poker, we provide the agent with the observation consists of its single
private card (e.g., Jack, Queen, or King) and the complete history actions of two players. The prompt
is structured to provide this context clearly, asking the agent to decide on its next action (pass or
bet) based on its private information and the betting history. For example, the initial prompt at the
beginning of game provides the agent’s private card and the empty action history, asking for the first
move (Listing 2).
Listing 2: Prompt for Kuhn Poker.
system_prompt:
You are an AI agent that makes optimal decisions to win in the game of
Kuhn Poker.
user_prompt:
GAME RULES:
1. Kuhn Poker is a two-player card game. The deck includes only three
cards: King (K) > Queen (Q) > Jack (J).
2. At the start of each game, both player_0 and player_1 place 1 chip
into the pot as a blind ante.
3. Each player is dealt a private card, and the third card is set aside
unseen.
4. The two players take turns acting, starting with player_0. A player
can choose to:

21

Preprint. Under review.

a. <PASS>: place no additional chips into the pot.
b. <BET>: place 1 additional chip into the pot.
5. If a player chooses to <PASS> after the other player’s <BET>, the
betting player wins the pot.
6. If both players choose to <PASS> or both players choose to <BET>, the
player with the higher card wins the pot.
PLAYER INFORMATION:
1. You are player_0. You are competing with player_1.
2. In each of your turns:
a. The game state shows your private card and the betting history.
b. You need to choose an action based on your card and the current
game state.
c. All legal actions for the current turn are provided in the format
of ‘<PASS>‘ or ‘<BET>‘.
RESPONSE INSTRUCTIONS:
Always choose only one action from the legal actions and output ‘<answer
>{your chosen action}</answer>‘ with no extra text after you finish the
thinking process. For example, ‘<answer><PASS></answer>‘. Strictly follow
the above format and keep your thinking process concise. Responses that
do not follow the format will result in immediate loss of the game.
Information of Turn-1:
This is your turn. The game state and legal actions for this turn are
provided below. Please choose your action and strictly follow the given
output format in the response instructions.
GAME STATE:
1. Blind ante: both player_0 and player_1 place 1 chip into the pot.
2. Deal: your card is Jack (J).
LEGAL ACTIONS:
<PASS>, <BET>.

Hanabi For the Hanabi variants, a cooperative imperfect information game, we provide the agent
with a unique observation. An agent observes the cards held by all other players but remains unaware
of its own hand. The observation also includes the number of remaining information and life tokens,
the cards successfully played on the board (fireworks), and the contents of the discard pile. The
prompt is highly structured, detailing the game rules, player-specific information, and strict response
formatting. For training, we use Mini Hanabi, a simplified 2-player version with 2 colors, 2 ranks, 3card hands, and 3 information/life tokens. For out-of-distribution evaluation, we use a more complex
variant, Simple Hanabi, which features 3 colors, 2 ranks, 5-card hands, 8 information tokens and 3
life tokens. For example, an initial prompt details the starting setup of the chosen variant, including
all visible cards and token counts, before asking the first player to make the first move (Listing 3).
Listing 3: Prompt for Mini Hanabi.
system_prompt:
You are an AI agent that makes optimal decisions to achieve the highest
score in the game of Hanabi.
user_prompt:
GAME RULES:
1. Hanabi is a cooperative card game for 2 players, player 0 and player
1.
2. The deck consists of 2 colors: Red(denoted by R), Yellow(denoted by Y)
, with ranks ranging from 1 to 2. Each color contains 4 cards: three of
rank 1, and one of rank 2, for a total of 8 cards.
3. Each player holds 3 cards in hand. Players can observe the hand of the
other player, but not their own.
4. There are 3 information tokens and 3 life tokens shared by both
players.

22

Preprint. Under review.

5. The objective is to play cards in ascending order of rank, from 1 to
2, to their corresponding color stacks, hence achieving the ’Fireworks’.
6. The players take turns to take one of the following actions:
a. <Play ‘i‘>: play the i-th card from the player’s own hand (0indexed). If the card is sequential to the top card of its
corresponding color stack, the move is valid and the card is added to
the top of the stack, then both players receive 1 point. Otherwise,
a life token is lost.
b. <Discard ‘i‘>: discard the i-th card from the player’s own hand
and gain one information token.
c. <Reveal player +1 color ‘c‘>: spend one information token to
reveal all cards of color ‘c‘ in the other player’s hand.
d. <Reveal player +1 rank ‘r‘>: spend one information token to reveal
all cards of rank ‘r‘ in the other player’s hand.
7. After playing or discarding, the player receives a new card from the
deck (if remaining).
8. The game ends when:
a. If all color stacks are completed (i.e., all cards of rank 2 are
played to their corresponding color stacks), then both players finish
the game with the highest possible total score of 4.
b. If deck is depleted, both players finish the game with a total
score which equals the sum of the highest ranks of each color stack.
c. If all life tokens are lost before the above two conditions are
met, then both players lose all points they have earned so far, and
finish the game with a total score of 0.
PLAYER INFORMATION:
1. You will be playing as the player 0.
2. In each of your turns, you will be provided with the current game
state information, including the remaining life tokens and information
tokens, the current color stacks, the remaining deck size, the discard
pile, the hand of the other player, and the revealed information on your
own hand.
3. Known cards are denoted by their color and rank. For example, ’R2’
means a red card of rank 2. 4. The current color stacks are represented
by the top card of each color stack. In particular, rank 0 denotes an
empty stack. For example, ’Y0’ means the yellow stack is still empty.
RESPONSE INSTRUCTIONS:
Always choose only one action from the legal actions and output ‘<answer
>{your chosen action}</answer>‘ with no extra text after you finish the
thinking process. For example, ‘<answer><Discard Card 0></answer>‘.
Strictly follow the above format and keep your thinking process concise.
Responses that do not follow the format will result in immediate loss of
all life tokens and end of the game.
Information of Turn-1:
This is your turn. The game state and legal actions for this turn are
provided below. Please choose your action and strictly follow the given
output format in the response instructions.
GAME STATE:
1. There are 3 life tokens and 3 information tokens remaining.
2. The top of the color stacks are: R0 Y0.
3. 2 cards remain in the draw pile.
4. The discard pile currently contains: None.
5. The other player’s hand:
- Card 0 (Y1): the other player believes it is one of the colors [R,
Y] and one of the ranks [1, 2].
- Card 1 (R1): the other player believes it is one of the colors [R,
Y] and one of the ranks [1, 2].
- Card 2 (R1): the other player believes it is one of the colors [R,
Y] and one of the ranks [1, 2].
6. Your own hand, based on the revealed information:
- Card 0: one of the colors [R, Y] and one of the ranks [1, 2].

23

Preprint. Under review.

- Card 1: one of the colors [R, Y] and one of the ranks [1, 2].
- Card 2: one of the colors [R, Y] and one of the ranks [1, 2].
LEGAL ACTIONS:
<Play card 0>, <Play card 1>, <Play card 2>, <Reveal player +1 color R>,
<Reveal player +1 color Y>, <Reveal player +1 rank 1>.

Connect Four For Connect Four, we provide the agent with a complete observation of the 6x7 game
board, showing the positions of all ’X’ and ’O’ marks. The prompt is designed to be comprehensive,
initially presenting the rules of the game (connecting four pieces to win, draw conditions). It then
informs the agent of its assigned mark (’X’ or ’O’) and the opponent’s mark. For each turn, the
prompt presents the current board state and asks the agent to choose a column (0-6) to drop its piece.
For example, the game begins with a prompt that provides the empty 6x7 grid, explains the rules and
player marks, and asks the first player to choose a column (Listing 4).
Listing 4: Prompt for Connect Four.
system_prompt:
You are an AI agent that makes optimal decisions to win in the game of
Connect Four.
user_prompt:
GAME RULES:
1. Connect Four is a two-player board game played on a 6x7 grid. Players
take turns dropping their pieces into columns.
2. The goal is to connect four of your pieces horizontally, vertically,
or diagonally.
3. Pieces fall to the bottom of the column or stack on top of existing
pieces.
4. The first player to connect four pieces wins. If the board fills up
without a winner, it’s a draw.
PLAYER INFORMATION:
1. Your mark is X. You are competing with another player controlling the
mark O.
2. In each of your turns:
a. The game state shows the current board as a 6x7 grid.
b. You need to choose a column (0-6) to drop your piece, where 0
denotes the leftmost column, 6 denotes the rightmost column.
c. All legal actions are provided as ‘<X({column})>‘, where ‘X‘ is
your mark, and {column} is the column number.
RESPONSE INSTRUCTIONS:
Always choose only one action from the legal actions and output ‘<answer
>{your chosen column}</answer>‘ with no extra text after you finish the
thinking process. For example, ‘<answer><X(3)></answer>‘. Strictly follow
the above format and keep your thinking process concise. Responses that
do not follow the format will result in immediate loss of the game.
Information of Turn-1:
This is your turn. The game state and legal actions for this turn are
provided below. Please choose your action and strictly follow the given
output format in the response instructions.
GAME STATE:
_______
_______
_______
_______
_______
_______
LEGAL ACTIONS:

24

Preprint. Under review.

<X(0)>, <X(1)>, <X(2)>, <X(3)>, <X(4)>, <X(5)>, <X(6)>.

Leduc Hold’em For Leduc Hold’em, we provide the agent with the observation which contains
all necessary information for decision-making. The observation is more complex due to two betting
rounds and a public card. In the first round, the agent observes its private card and the betting history.
In the second round, after a public card is revealed, the observation is updated to include this board
card. The prompt provides the agent with its private card, the current betting history, and the public
card if revealed, requiring it to make an action in the context of the evolving game state. For example,
the initial prompt at the beginning of game provides the agent with its private card and asks for an
action in the first betting round before any actions have been taken (Listing 5).
Listing 5: Prompt for Leduc Hold’em.
system_prompt:
You are an AI agent that makes optimal decisions to win in the game of
Leduc Poker.
user_prompt:
GAME RULES:
1. Leduc poker is a two-player card game. The deck includes only six
cards: two pairs of King (K), Queen (Q), and Jack (J).
2. At the start of each game, both player_0 and player_1 place 1 chip
into the pot as a blind ante.
3. Each player is dealt one private card from the deck, and the remaining
cards are set aside unseen.
4. The game has two betting rounds. When the first round ends, one public
card from the remaining cards of the deck is revealed to both players.
5. The two players take turns acting in the betting rounds, both starting
with player_0. A player can choose to:
a. <FOLD>: stop betting and the other player wins the pot.
b. <CALL>: match the current bet. If no bet has been made in the
current round, this is equivalent to checking.
c. <RAISE>: first match the current bet and then add ‘n‘ chips to the
bet, where ‘n=2‘ in the first round and ‘n=4‘ in the second round.
If no bet has been made in the current round, this is equivalent to
betting ‘n‘ chips.
6. A maximum of two <RAISE>s are allowed in each round. Each round ends
when both players have acted and their bets are equal.
7. If a player chooses to <FOLD>, the other player wins the pot.
8. If neither player chooses to <FOLD>, the second round ends with a
showdown:
a. If a player has a pair (private card = public card), the player
wins the pot.
b. If neither player has a pair, the player with the higher card (K >
Q > J) wins the pot.
c. If two players have the same card, the players split the pot.
PLAYER INFORMATION:
1. You are player_0. You are competing with player_1.
2. In each of your turns:
a. The game state shows your private card, public card (if revealed),
and the betting history.
b. You need to choose an action based on your cards and the current
game state.
c. All legal actions for the current turn are provided in the format
of ‘<FOLD>‘, ‘<CALL>‘, or ‘<RAISE>‘.
RESPONSE INSTRUCTIONS:
Always choose only one action from the legal actions and output ‘<answer
>{your chosen action}</answer>‘ with no extra text after you finish the
thinking process. For example, ‘<answer><CALL></answer>‘. Strictly follow
the above format and keep your thinking process concise. Responses that
do not follow the format will result in immediate loss of the game.

25

Preprint. Under review.

Information of Turn-1:
This is your turn. The game state and legal actions for this turn are
provided below. Please choose your action and strictly follow the given
output format in the response instructions.
GAME STATE:
1. Blind ante: both player_0 and player_1 place 1 chip into the pot.
2. Deal: your card is J.
LEGAL ACTIONS:
<CALL>, <RAISE>.

J

F ULL RESULTS

In this section, we present the raw (unnormalized) results for all experiments in the main text.
Table 14: Full results of game-play return in training games
Model
Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

Tic-Tac-Toe
MCTS
MCTS
(100 Simulations)
(1000 Simulations)

Kuhn Poker

Mini Hanabi

0.403/-0.629
0.470/-0.624

-0.374/-0.687
-0.297/-0.717

-0.148/-0.141
-0.301/-0.149

1.200
1.494

0.506/-0.358
0.397/-0.490
0.438/-0.644
0.544/-0.419

-0.278/-0.379
-0.388/-0.538
-0.350/-0.702
-0.302/-0.450

-0.119/-0.142
-0.107/-0.103
-0.123/-0.130
-0.114/-0.141

2.019
1.799
2.222
2.173

MARSHAL (w/ Fixed Opponent)
Tic-Tac-Toe
0.760/-0.161
Kuhn Poker
-1.000/-1.000
Mini Hanabi
-

-0.378/-0.162
-1.000/-1.000
-

-0.146/-0.090
-0.114/-0.117
-

1.397
0.000
-

MARSHAL (w/o Turn-Level Advantage Estimator)
Tic-Tac-Toe
0.492/-0.517
-0.292/-0.567
Kuhn Poker
0.267/-0.607
-0.426/-0.625
Mini Hanabi
0.351/-0.788
-0.400/-0.811

-0.104/-0.091
-0.074/-0.106
-0.133/-0.084

1.392
1.666
2.128

MARSHAL (w/o Agent-Specific Advantage Normalization)
Tic-Tac-Toe
0.654/-0.376
-0.213/-0.366
Kuhn Poker
0.391/-0.509
-0.446/-0.567
Mini Hanabi
0.363/-0.732
-0.364/-0.764

-0.127/-0.126
-0.116/-0.109
-0.119/-0.128

1.764
1.607
2.100

26

Preprint. Under review.

Table 15: Full results on game-play return in testing games
Connect Four
MCTS
MCTS
(10 Simulations)
(100 Simulations)

Leduc Hold’em

Simple Hanabi

-0.462/-0.912
-0.383/-0.833

-0.902/-0.998
-0.868/-0.995

-0.629/-0.691
-0.870/-0.706

0.833
0.836

-0.387/-0.703
-0.448/-0.746
-0.465/-0.885
-0.421/-0.780

-0.858/-0.974
-0.874/-0.995
-0.880/-0.998
-0.871/-0.979

-0.518/-0.702
-0.460/-0.327
-0.736/-0.403
-0.483/-0.487

1.785
1.761
2.036
2.205

MARSHAL (w/ Fixed Opponent)
Tic-Tac-Toe
-0.593/-0.887
Kuhn Poker
-1.000/-1.000
Mini Hanabi
-

-0.939/-0.990
-1.000/-1.000
-

-0.632/-0.616
-1.124/-1.003
-

0.733
0.000
-

MARSHAL (w/o Turn-Level Advantage Estimator)
Tic-Tac-Toe
-0.465/-0.754
-0.843/-0.989
Kuhn Poker
-0.408/-0.783
-0.899/-0.998
Mini Hanabi
-0.495/-0.939
-0.886/-0.998

-0.622/-0.553
-0.789/-0.663
-0.555/-0.483

1.143
1.379
1.841

MARSHAL (w/o Agent-Specific Advantage Normalization)
Tic-Tac-Toe
-0.492/-0.790
-0.882/-0.983
Kuhn Poker
-0.460/-0.790
-0.908/-0.992
Mini Hanabi
-0.358/-0.898
-0.902/-0.998

-0.594/-0.460
-0.753/-0.769
-0.664/-0.389

1.303
1.345
1.925

Model
Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

Table 16: Full results on general benchmarks in standard single agent setting
Model

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

87.60
87.50

94.60
94.80

39.80
51.20

36.70
36.70

70.00
80.00

57.10
58.70

39.39
37.37

89.10
87.80
88.10
89.90

95.20
94.50
94.70
94.60

46.50
48.40
48.00
52.00

40.00
33.30
43.30
33.30

77.50
72.50
65.00
75.00

57.60
59.30
58.90
59.90

38.89
33.84
36.36
34.85

MARSHAL (w/ Fixed Opponent)
Tic-Tac-Toe
89.10
94.80
Kuhn Poker
86.20
94.00
Mini Hanabi
-

53.90
45.70
-

50.00
33.30
-

72.50
72.50
-

63.00
53.80
-

37.88
33.84
-

MARSHAL (w/o Turn-Level Advantage Estimator)
Tic-Tac-Toe
88.80
94.80
48.80
40.00
Kuhn Poker
88.60
94.00
48.40
40.00
Mini Hanabi
87.80
94.50
48.40
36.70

72.50
72.50
70.00

57.70
57.90
59.30

36.36
34.34
36.87

MARSHAL (w/o Agent-Specific Advantage Normalization)
Tic-Tac-Toe
88.20
94.50
53.10
36.70
Kuhn Poker
88.10
94.80
53.90
40.00
Mini Hanabi
88.60
93.90
46.10
40.00

77.50
75.00
75.00

58.20
58.70
57.00

39.39
35.86
38.89

27

Preprint. Under review.

Table 17: Full results on general benchmarks using the MAD framework
Model

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

90.20
91.60

95.91
95.45

80.71
81.89

40.00
40.00

75.00
77.50

87.42
87.01

37.88
40.40

92.20
91.60
91.40
92.80

96.06
96.21
95.60
95.60

83.07
82.68
82.68
83.86

43.33
40.00
43.33
46.67

82.50
82.50
77.50
80.00

86.76
87.39
87.04
87.36

41.12
41.41
38.38
45.45

MARSHAL (w/ Fixed Opponent)
Tic-Tac-Toe
94.80
94.54
Kuhn Poker
95.00
94.24
Mini Hanabi
-

85.04
86.22
-

53.33
53.33
-

90.00
85.00
-

88.57
88.54
-

42.93
45.45
-

MARSHAL (w/o Turn-Level Advantage Estimator)
Tic-Tac-Toe
93.80
94.77
85.04
60.00
Kuhn Poker
95.00
94.54
86.22
56.67
Mini Hanabi
94.20
94.62
85.43
53.33

85.00
90.00
90.00

88.98
89.05
88.51

47.98
50.00
46.97

MARSHAL (w/o Agent-Specific Advantage Normalization)
Tic-Tac-Toe
94.40
94.62
84.65
56.67
Kuhn Poker
95.00
94.69
84.25
56.67
Mini Hanabi
94.20
94.47
87.01
63.33

87.50
85.00
90.00

89.33
89.62
88.92

46.97
47.47
43.94

Table 18: Full results on general benchmarks using the AutoGen framework
Model

MATH

GSM8K

AQUA

AIME

AMC

MMLU

GPQA

Qwen3-4B
SPIRAL
MARSHAL
Tic-Tac-Toe
Kuhn Poker
Mini Hanabi
Generalist

93.40
94.20

94.69
94.47

85.04
86.61

56.67
60.00

87.50
87.50

89.21
91.60

47.47
45.96

94.40
95.80
94.40
95.20

94.69
94.39
94.54
94.54

87.01
86.61
86.22
86.61

60.00
63.33
66.67
66.67

90.00
92.50
95.00
92.50

89.53
89.65
88.98
89.53

45.45
48.48
44.95
50.00

MARSHAL (w/ Fixed Opponent)
Tic-Tac-Toe
92.40
95.38
Kuhn Poker
90.20
95.83
Mini Hanabi
-

83.07
83.46
-

43.33
33.33
-

80.00
72.50
-

87.11
86.69
-

43.94
42.42
-

MARSHAL (w/o Turn-Level Advantage Estimator)
Tic-Tac-Toe
91.00
95.75
82.28
40.00
Kuhn Poker
91.80
95.45
82.68
36.67
Mini Hanabi
91.20
95.91
83.86
40.00

72.50
82.50
75.00

86.34
86.79
87.33

42.93
41.92
41.92

MARSHAL (w/o Agent-Specific Advantage Normalization)
Tic-Tac-Toe
91.20
95.83
83.46
40.00
Kuhn Poker
91.20
95.53
84.65
36.67
Mini Hanabi
92.60
96.06
82.68
33.33

80.00
75.00
80.00

87.14
86.63
87.11

38.58
43.94
43.43

28

