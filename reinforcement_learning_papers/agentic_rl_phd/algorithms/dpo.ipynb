{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dpo.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/eric-mitchell/direct-preference-optimization\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paper: \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" (Rafailov et al., 2023)\n",
    "# Category: Alignment / RLHF Alternative\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             ref_chosen_logps, ref_rejected_logps, \n",
    "             beta=0.1):\n",
    "    \"\"\"\n",
    "    The DPO Loss Function.\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: Log prob of the 'chosen' response under the policy model.\n",
    "        policy_rejected_logps: Log prob of the 'rejected' response under the policy model.\n",
    "        ref_chosen_logps: Log prob of the 'chosen' response under the reference (frozen) model.\n",
    "        ref_rejected_logps: Log prob of the 'rejected' response under the reference (frozen) model.\n",
    "        beta: Temperature parameter (controls deviation from reference).\n",
    "        \n",
    "    Returns:\n",
    "        losses: The DPO loss for each example in the batch.\n",
    "        rewards: Implicit rewards (for logging).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Equation 4 from the paper:\n",
    "    # r(x,y) = beta * (log pi(y|x) - log ref(y|x))\n",
    "    \n",
    "    # Calculate log ratios\n",
    "    pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "    ref_logratios = ref_chosen_logps - ref_rejected_logps\n",
    "    \n",
    "    # Calculate logits for the sigmoid\n",
    "    # logits = log(pi_chosen/ref_chosen) - log(pi_rejected/ref_rejected)\n",
    "    logits = pi_logratios - ref_logratios\n",
    "    \n",
    "    # The DPO loss is -log(sigmoid(beta * logits))\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "    \n",
    "    # Implicit rewards (for tracking progress)\n",
    "    chosen_rewards = beta * (policy_chosen_logps - ref_chosen_logps).detach()\n",
    "    rejected_rewards = beta * (policy_rejected_logps - ref_rejected_logps).detach()\n",
    "    \n",
    "    return losses, chosen_rewards, rejected_rewards\n",
    "\n",
    "class DPOTrainer(nn.Module):\n",
    "    def __init__(self, model, ref_model, beta=0.1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model # Frozen copy\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, chosen_ids, rejected_ids):\n",
    "        # This is a simplified forward pass logic\n",
    "        # In practice, you need to gather log probs of specific tokens\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}