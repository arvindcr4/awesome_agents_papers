arXiv:2412.00661v4 [cs.LG] 24 Oct 2025

Mean-Field Sampling for Cooperative Multi-Agent
Reinforcement Learning

Emile Anand∗
Georgia Institute of Technology
Atlanta, GA 30308
emile@gatech.edu

Ishani Karmarkar
Stanford University
Palo Alto, CA, 94305
ishanik@stanford.edu

Guannan Qu
Carnegie Mellon University
Pittsburgh, PA 94035
gqu@andrew.cmu.edu

Abstract
Designing efficient algorithms for multi-agent reinforcement learning (MARL) is
fundamentally challenging because the size of the joint state and action spaces
grows exponentially in the number of agents. These difficulties are exacerbated
when balancing sequential global decision-making with local agent interactions. In
this work, we propose a new algorithm SUBSAMPLE-MFQ (Subsample-Mean-FieldQ-learning) and a decentralized randomized policy for a system with n agents. For
any k ≤ n, our algorithm learns a policy for the system in time polynomial in
k. We prove
√ that this learned policy converges to the optimal policy on the order
of Õ(1/ k) as the number of subsampled agents k increases. In particular, this
bound is independent of the number of agents n.

1

Introduction

Reinforcement Learning (RL) has become a popular framework to solve sequential decision making
problems in unknown environments and has achieved tremendous success in a wide array of domains
such as playing the game of Go [Silver et al., 2016], robotic control [Kober et al., 2013], and
autonomous driving [Kiran et al., 2022, Lin et al., 2023a]. A key feature of most real-world systems
is their uncertain nature, and thus, RL has emerged as a powerful tool for learning optimal policies
for multi-agent systems to operate in unknown environments [Kim and Giannakis, 2017, Zhang et al.,
2021, Lin et al., 2024, Anand and Qu, 2024]. While early RL works focused on the single-agent
setting, multi-agent RL (MARL) has recently achieved impressive success in many applications, such
as coordinating robotic swarms [Preiss et al., 2017, DeWeese and Qu, 2024], real-time bidding [Jin
et al., 2018], ride-sharing [Li et al., 2019], and stochastic games [Jin et al., 2020].
Despite growing interest in MARL, extending RL to multi-agent settings poses significant computational challenges due to the curse of dimensionality. MARL is fundamentally difficult as agents
in the real-world not only interact with the environment but also with each other [Shapley, 1953]:
if each of the n agents has a state space S and action space A, the global state-action space has
size (|S||A|)n , which is exponential in n. Thus, many RL algorithms (such as temporal difference
learning and tabular Q-learning) require computing and storing an (|S||A|)n -sized Q-table [Sutton
et al., 1999, Bertsekas and Tsitsiklis, 1996]. This scalability issue has been observed in a variety of
MARL settings [Blondel and Tsitsiklis, 2000, Papadimitriou and Tsitsiklis, 1999, Littman, 1994].
∗

Work partially done while a visiting student at Carnegie Mellon University and intern at Cognition AI.

39th Conference on Neural Information Processing Systems (NeurIPS 2025).

An exciting line of work that addresses this intractability is mean-field MARL [Lasry and Lions, 2007,
Yang et al., 2018, Gu et al., 2021, 2022, Hu et al., 2023]. The mean-field approach assumes agents
are homogeneous in their state-action spaces, enabling their interactions to be approximated by a
two-agent setting: here, each agent interacts with a representative “mean agent” which evolves as the
empirical distribution of states of all other agents. With these assumptions, mean-field MARL learns
an optimal policy with sample complexity O(n|S||A| |S||A|), which is polynomial in the number of
agents. However, if n is large, this remains prohibitive even for moderate values of |S| and |A|.
Motivated by this problem, in this paper we study the cooperative setting–where agents work
collaboratively to maximize a structured global reward–and ask: Can we design a scalable MARL
algorithm for learning an approximately optimal policy in a cooperative multi-agent system?
Contributions. We answer this question affirmatively. Our key contributions are outlined below.
Subsampling algorithm. We model the problem as a Markov Decision Process (MDP) with a global
agent and n local agents. We propose SUBSAMPLE-MFQ to address the challenge of MARL with a
large number of local agents. SUBSAMPLE-MFQ selects k ≤ n local agents to learn a deterministic
policy π̂kest by applying mean-field value iteration on the k-local-agent subsystem to learn Q̂est
k ,
which can be viewed as a smaller Q function. SUBSAMPLE-MFQ then deploys a stochastic policy πkest ,
where the global agent samples k local agents uniformly at each step and uses π̂kest to determine its
action, while each local agent samples k − 1 other local agents and uses π̂kest to determine its action.
Sample complexity and theoretical guarantee. As the number of local agents increases, the
size of Q̂k scales polynomially with k, rather than polynomially with n as in mean-field MARL.
Analogously, when the size of the local agent’s state space grows, the size of Q̂k scales exponentially
with k, rather than exponentially with n, as in traditional Q-learning. The key analytic technique
underlying our results is a novel MDP sampling result. This
√ result shows that the performance gap
between πkest and the optimal policy π ∗ is at most Õ(1/ k), with high probability. The choice of
k reveals a fundamental trade-off between the size of the Q-table and the optimality of πkest . For
example, if k is set to O(log n), SUBSAMPLE-MFQ is the first centralized MARL algorithm to achieve
a polylogarithmic run-time in n, representing an exponential speedup over the previously best-known
polytime mean-field MARL methods, while maintaining a decaying optimality gap as n gets large,
While our results are theoretical in nature, we hope SUBSAMPLE-MFQ will further exploration of
sampling in Markov games, and potentially inspire new practical multi-agent algorithms.
Related work. MARL has a rich history, starting with early works on Markov games [Littman, 1994,
Sutton et al., 1999], which are a multi-agent extension of MDPs. MARL has since been actively
studied [Zhang et al., 2021] in a broad range of settings. MARL is most similar to the category of
“succinctly described” MDPs [Blondel and Tsitsiklis, 2000], where the state/action space is a product
space formed by the individual state/action spaces of multiple agents, and where the agents interact
to maximize an objective. A recent line of research constrains the problem to sparse networked
instances to enforce local interactions between agents [Qu et al., 2020a, Lin et al., 2020, Mondal
et al., 2022]. In this formulation, the agents correspond to vertices on a graph who only interact with
nearby agents. By exploiting Gamarnik’s correlation decay property from combinatorial optimization
[Gamarnik et al., 2009], they overcome the curse of dimensionality by simplifying the problem to
only search over the policy space derived from the truncated graph to learn approximately optimal
solutions. However, as the underlying network structure becomes dense with many local interactions,
the neighborhood of each agent gets large, and these algorithms become intractable.
Mean-Field RL. Under assumptions of homogeneity in the state/action spaces of the agents, the
problem of densely networked multi-agent RL was studied by Yang et al. [2018], Gu et al. [2021], who
approximated the solution
in polynomial time with a mean-field approach where the approximation
√
error scales in O(1/ n). In contrast, our work achieves subpolynomial runtimes by directly sampling
from this mean-field distribution. Cui and Koeppl [2022] introduce heterogeneity to mean-field MARL
by modeling non-uniform interactions through graphons; however, these methods crucially assume
the existence of graphon sequences that converge in cut-norm to the finite graph. In the cooperative
setting, Subramanian et al. [2022], Cui et al. [2023] studies a mean-field setting with q types of
homogeneous agents; however, their learned policy does not provably converge to the optimum.
Other related works. Our work is related to factored MDPs, where there is a global action affecting
every agent; however, in our case, each agent has its own action [Min et al., 2023, Lauer and

2

Riedmiller, 2000]. Jin et al. [2020] reduces the dependence of the product action space to an additive
dependence with V-learning. Our work further reduces the complexity of the joint state space,
which has not been previously accomplished. We add to the growing literature on the Centralized
Training with Decentralized Execution regime [Zhou et al., 2023], as our algorithm learns a provably
approximately optimal policy using centralized information, but makes decisions using only local
information during execution. Finally, one can efficiently approximate the Q-table through function
approximation [Jin et al., 2021]. However, achieving theoretical bounds on the performance loss
due to function approximation is intractable without strong assumptions such as linear Bellman
completeness or low Bellman-Eluder dimension [Golowich and Moitra, 2024]. While our work
primarily studies the finite tabular setting, we extend it to non-tabular linear MDPs in Section J.

2

Preliminaries


Notation. For k, n ∈ N where k ≤ n, let [n]
k denote the set of k-sized subsets of [n] = {1, . . . , n}.
For any vector z ∈ Rd , let ∥z∥1 and ∥z∥∞ denote the standard ℓ1 and ℓ∞ norms of z respectively.
Let ∥A∥1 denote the matrix ℓ1 -norm of A ∈ Rn×m . Given variables s1 , . . . , sn , s∆ := {si : i ∈ ∆}
for ∆ ⊆ [n]. We use Õ(·) to suppress polylogarithmic factors in all problem parameters except n. For
a discrete measurable space (X
P, F), the total variation distance between probability measures ρ1 , ρ2
is given by TV(ρ1 , ρ2 ) = 12 x∈X |ρ1 (x) − ρ2 (x)|. Next, x ∼ D(·) denotes that x is a random
element sampled from a distribution D, and we denote that x is a random sample from the uniform
distribution over a finite set Ω by x ∼ U(Ω). We include a detailed notation table in Table 1.
2.1

Problem formulation

We consider a system of n + 1 agents, where agent g is a “global decision making agent” and
the remaining n agents, denoted by [n], are “local agents.” At time t, the agents are in state
s(t) = (sg (t), s1 (t), ..., sn (t)) ∈ S := Sg × Sln , where sg (t) ∈ Sg denotes the global agent’s state,
and for each i ∈ [n], si (t) ∈ Sl denotes the state of the i’th local agent. The agents cooperatively
select actions a(t) = (ag (t), a1 (t), ..., an (t)) ∈ A := Ag ×Anl , where ag (t) ∈ Ag denotes the global
agent’s action and ai (t) ∈ Al denotes the i’th local agent’s action. At time t + 1, the next state for all
the agents is independently generated by stochastic transition kernels Pg : Sg × Sg × Ag → ∆(Sg )
and Pl : Sl × Sl × Sg × Al → ∆(Sl ) as follows:
sg (t + 1) ∼ Pg (·|sg (t), ag (t)),

si (t + 1) ∼ Pl (·|si (t), sg (t), ai (t)), ∀i ∈ [n].

(1)

The system then collects a structured stage reward r(s(t), a(t)) where the reward r : S × A → R
depends on s(t) and a(t) through Equation (2), and where rg and rl is typically application specific.
1 X
rl (si , sg , ai )
r(s, a) = rg (sg , ag ) +
(2)
n
|
| {z }
{z
}
i∈[n] local component

global component

A policy π : S → P(A) maps from states to distributions of actions such that a ∼ π(·|s). Given
γ ∈ (0, 1), we seek to learn a policy π that maximizes the (γ-discounted) value for each s ∈ S:
V π (s) = Ea(t)∼π(·|s)

∞
hX

i
γ t r(s(t), a(t))|s(0) = s .

(3)

t=0

The cardinality of the search space simplex for the optimal policy is |Sg ||Sl |n |Ag ||Al |n , which is
exponential in the number of agents, underscoring the need for efficient approximation algorithms.
To efficiently learn policies that maximize the objective, we make the following standard assumptions:
Assumption 2.1 (Finite state/action spaces). We assume that the state and action spaces of all the
agents in the MARL game are finite: |Sl |, |Sg |, |Ag |, |Al | < ∞. Section J of the supplementary
material relaxes this assumption to the non-tabular setting with infinite continuous sets.
Assumption 2.2 (Bounded rewards). The components of the reward function are bounded. Specifically, ∥rg (·, ·)∥∞ ≤ r̃g , and ∥rl (·, ·, ·)∥∞ ≤ r̃l . This implies ∥r(·, ·)∥∞ ≤ r̃g + r̃l := r̃.
Definition 2.3 (ϵ-optimal policy). Given a policy simplex Π, π ∈ Π is an ϵ-optimal policy if
∗
V π (s) ≥ supπ∗ ∈Π V π (s) − ϵ.
3

Motivating examples. Below we give examples of two cooperative MARL settings which are
naturally modeled by our setting. Our experiments in Section B reveal a monotonic improvement in
the learned policies as k → n, while providing a substantial speedup over mean-field Q-learning2 .

Figure 1: Bounded exploration in warehouse accidents, and traffic congestions with Gaussian squeeze.
• Gaussian squeeze: In this task, n homogeneous agents determine individual actions ai to jointly
Pn
2
2
maximize the objective r(x) = xe−(x−µ) /σ , where x = i=1 ai , and µ and σ are the predefined mean and variance of the system. In scenarios of traffic congestion, each agent i ∈ [n] is a
controller trying to send ai vehicles into the main road, where controllers coordinate with each
other to avoid congestion, hence avoiding either over-use or under-use, thereby contributing to
the entire system. This GS problem is previously studied in Yang et al. [2018], and serves as an
ablation study on the impact of subsampling for MARL.
• Constrained exploration: Consider an M × M grid. Each agent’s state is a coordinate in
[M ] × [M ]. The state represents the center of a d × d box where the global agent constrains
the local agents’ movements. Initially, all agents are in the same location. At each time-step,
the local agents take actions ai (t) ∈ R2 (e.g., up, down, left, right) to transition between states
and collect rewards. The transition kernel ensures that local agents remain within the d × d box
dictated by the global agent, using knowledge of ai (t), sg (t), and si (t). In warehouse settings
where shelves have collapsed, creating hazardous or inaccessible areas, we want agents to clean
these areas. However, exploration in these regions may be challenging due to physical constraints
or safety concerns, causing exploration in these regions to be disincentivized from the local agents’
perspectives. Through an appropriate design of the reward and transition functions, the global
agent could guide the local agents to focus on specific d × d grids, allowing efficient cleanup while
minimizing risk.
Capturing heterogeneity. Following Mondal et al. [2022], Xu and Klabjan [2023], our model can
capture heterogeneity in the local agents by modeling agent types as part of the state: to do this, we
assign a type εi ∈ E to each local agent by letting Sl = E × Sl′ , where E is a set that enumerates
possible types that are treated as a fixed part of the agent’s state, and Sl′ is the latent state space of
any local agent. The transition and reward functions can vary depending on the agent’s type. The
global agent can provide unique signals to local agents of each type by letting sg ∈ Sg and ag ∈ Ag
denote a state/action vector where each element corresponds to a type.

3

Algorithmic Approach: Subsampled Value Iteration

Q-learning. Our starting point is the classic Q-learning framework [Watkins and Dayan, 1992] for
π
offline-RL,
P∞ twhich seeks to learn the Q-function Q : S 0× A → R. For any policy π, Q (s, a) =
π
E [ t=0 γ r(s(t), a(t))|s(0) = s, a(0) = a]. Initially, Q (s, a) = 0, for all (s, a) ∈ S ×A. Then, for
all t ∈ [T ], it is updated as Qt+1 (s, a) ← T Qt (s, a), where the Bellman operator T is
T Qt (s, a) = r(s, a) + γEs′g ∼Pg (·|sg ,a),s′i ∼Pl (·|si ,sg ,ai ),∀i∈[n] max
Qt (s′ , a′ ).
′
a ∈A

(4)

It is well known that T is γ-contractive, ensuring that the above updates converge to a unique
Q∗ such that T Q∗ = Q∗ . The optimal policy π ∗ : S → A can then be computed greedily as
2

We provide supporting code for the algorithm and experiments in https://github.com/emiletimothy/
Mean-Field-Subsample-Q-Learning

4

π ∗ (s) = arg maxa∈A Q∗ (s, a). However, the update complexity for the Q-function is O(|S||A|) =
O(|Sg ||Sl |n |Ag ||Al |n ), which is exponential in the number of local agents increases.
Mean-field transformation. To address this, mean-field MARL [Yang et al., 2018] (under homogeneity assumptions) studies the empirical distribution function Fz[n] : Zl → R, for Zl := Sl × Al :
n

Fz[n] (z) :=

1X
1{si = zs , ai = za },
n i=1

∀z := (zs , za ) ∈ Zl := Sl × Al .

(5)

Let µn (Zl ) = { nb |b ∈ {0, . . . , n}}|Zl | be the space of |Zl |-sized vectors (or |Sl | × |Al |-sized tables),
where each entry is in {0, 1/n, 2/n, . . . , 1}. Intuitively, µn (Zl ) is a discrete distribution over (Sl , Al )
where each probability assigned is a multiple of 1/n. Then, Fz[n] ∈ µn (Zl ) indicates the proportion
of agents in each state-action pair.
As in Q-learning, in mean-field Q-learning initially Q̂0 (sg , s1 , ag , a1 , Fz[n]\1 ) = 0. At each time
t ∈ [T ], we update Q̂ as Q̂t+1 = T̂ Q̂t , where T̂ is the Bellman operator in distribution space:
T̂ Q̂t (sg , s1 , ag , a1 , Fz[n]\1 ) = r(s, a) + γE s′g ∼Pg (·|sg ,ag )
s′i ∼Pl (·|si ,sg ,ai )
∀i∈[n]

max

(a′g ,a′1 ,a′[n]\1 )

′
Q̂t (s′g , s′1 , a′g , a′1 , Fz[n]\1
)

∈Ag ×Al ×An−1
l

Since the Q-function is permutation-invariant in the homogeneous local agents, one sees that for
each t ≥ 0, Qt (sg , s[n] , ag , a[n] ) = Q̂t (sg , s1 , ag , a1 , Fz[n]\1 ). In other words, mean-field Q-learning
implements the same updates as standard Q-learning. However, the update complexity of Q̂ is only
O(|Sg ||Ag ||Zl |n|Zl | ), which scales polynomially in n but exponentially in |Zl |.
Remark 3.1. Existing methods use sample complexity Õ(min{|Sg ||Ag ||Zl |n , |Sg ||Ag ||Zl |n|Zl | }):
one uses Q-learning if |Zl |n−1 < n|Zl | , and mean-field value iteration otherwise. In each regime, as
n scales, the update complexity becomes computationally infeasible.
To further reduce the update complexity, we propose SUBSAMPLE-MFQ to overcome the polynomial
(in n) sample complexity of mean-field Q-learning and the exponential (in n) sample complexity of
traditional Q-learning. We begin by motivating the intuition behind our algorithms.
3.1

Overview of approach.

Offline Planning: Algorithm 1. First, the global agent randomly samples a subset of local agents
∆ ⊆ [n] such that |∆| = k, for k ≤ n. It then ignores all other local agents [n] \ ∆, and performs
value iteration (using m samples to update the Q-function in each iterate) to approximately learn
est
the Q-function Q̂est
k,m and policy π̂k,m for this surrogate subsystem of k local agents. We denote the
surrogate reward gained by this subsystem at each time step by r∆ : S × A → R, where
r∆ (s, a) = rg (sg , ag ) +

1 X
rl (sg , si , ai ).
|∆|

(6)

i∈∆

∗
In Theorem E.3, we show that ∥Q̂est
k,m −Q ∥∞ is Lipschitz continuous with respect to the TV-distance
between the state/action pairs of the subsampled agents and the full set of n agents. Equipped with
this approximation guarantee, we show how to construct an approximately optimal policy on the
full system on n agents. In general, converting this policy on k local agents to a policy on the full
n-agent system without sacrificing error guarantees can be intractable, and there is a line on works
on centralized-training decentralized-execution (CTDE) [Xu and Klabjan, 2023, Zhou et al., 2023]
which shows that designing performant decentralized policies can be highly non-trivial.

Online Execution: Algorithm 2. In order to circumvent this obstacle and convert the optimality of
each agent’s action in the k local-agent subsystem to an approximate optimality guarantee on the full
est
n-agent system, we propose a randomized policy πk,m
, where the global agent samples ∆ ∈ [n]
k at
est
each time-step to derive an action ag ← π̂k,m
(sg , s∆ ), and where each local i agent samples k − 1
est
other local agents ∆i to derive an action ai ← π̂k,m
(sg , si , s∆i ). Finally, Theorem 4.4 shows that
est
the policy πk,m converges to the optimal policy π ∗ as k → n with rate Õ( √1k ).
5

3.2

Algorithm description.

We now formally describe the algorithms. Before describing Algorithm 1 (SUBSAMPLE-MFQ: Learning) and Algorithm 2 (SUBSAMPLE-MFQ: Execution) in detail, it will be helpful to first define the
empirical distribution function:
Definition 3.2 (Empirical Distribution Function). For any population (z1 , . . . , zn ) ∈ Zln , where
Zl := Sl × Al , define the empirical
distribution

P function Fz∆ : Zl → R+ for all z := (zs , za ) ∈
:= k1 i∈∆ 1{si = zs , ai = za }.
Sl × Al and for all ∆ ∈ [n]
k by Fz∆ (x)

|Zl |
Let µk (Zl ) := kb |b ∈ {0, . . . , k}
be the space of |Zl |-sized vectors (or |Sl | × |Al |-sized tables)
where each entry is in {0, 1/k, 2/k, . . . , 1}. Intuitively, µk (Zl ) is a discrete distribution over (Sl , Al )
where each probability assigned is a multiple of 1/k. Here, Fz∆ ∈ µk (Zl ) indicates the proportion
of agents (in the k-local-agent subsystem) at each state/action pair.
Algorithm 1 (Offline learning). Let m ∈ N denote the sample size for the learning algorithm with
sampling parameter k ≤ n. As in Theorem 3.1, when |Zl |k ≤ |Zl |k |Zl | , the algorithm uses traditional
value-iteration, and when |Zl |k > |Zl |k |Zl | , it uses mean-field value iteration. We formally describe
the procedure for each regime below:
Regime with Large State/Action Space: When |Zl |k ≤ |Zl |k |Zl | , we iteratively learn the optimal
Q-function for a subsystem with k-local agents denoted by Q̂tk,m : Sg × Slk × Ag × Akl → R, which
is initialized to 0. At time t, we update
t
Q̂t+1
k,m (sg , s∆ , ag , a∆ ) = T̃k,m Q̂k,m (sg , s∆ , ag , a∆ ),

(7)

where T̃k,m is the empirically adapted Bellman operator in Equation (8). Since the system is
unknown, T̃k,m cannot directly perform the Bellman update, so it instead uses m random samples
sjg ∼ Pg (·|sg , ag ) and sji ∼ Pl (·|si , sg , ai ) for each j ∈ [m], i ∈ ∆ to approximate the system:
γ X
max
T̃k,m Q̂tk,m (sg , s∆ , ag , a∆ ) = r∆ (s, a) +
Q̂tk,m (sjg , sj∆ , a′g , a′∆ ).
m
a′g ∈Ag ,a′∆ ∈Ak
l
j∈[m]

Since T̃k,m is γ-contractive, Algorithm 1 applies value iteration with T̃ until Q̂k,m converges to a
est
est
fixed point satisfying T̃k,m Q̂est
k,m = Q̂k,m , yielding a deterministic policy π̂k,m (sg , s∆ ) where
est
π̂k,m
(sg , s∆ ) = arg max

ag ∈Ag ,a∆ ∈Ak
l

Q̂est
k,m (sg , s∆ , ag , a∆ ).

(8)

Regime with Large Number of Agents: When |Zl |k > |Zl |k |Zl | , we learn the optimal mean-field
Q-function for a k local agent system, denoted by Q̂tk,m : Sg × Sl × µk−1 (Zl ) × Ag × Al → R,
which is initialized to 0. At time t, we update
t
Q̂t+1
˜ , a1 , ag ) = T̂k,m Q̂k,m (sg , s1 , Fz∆
˜ , a1 , ag ),
k,m (sg , s1 , Fz∆

(9)

where T̂k,m is the empirically adapted mean-field Bellman operator in Equation (10). Similarly, as
the system is unknown, T̂k,m cannot directly perform the Bellman update and instead uses m random
samples sjg ∼ Pg (·|sg , ag ) and sji ∼ Pl (·|si , sg , ai ), ∀j ∈ [m], i ∈ ∆ to approximate the system:
γ X
T̂k,m Q̂tk,m (sg , s1 , Fz∆˜ , a1 , ag ) = r∆ (s, a)+
max
Q̂tk,m (sjg , sj1 , Fsj ,a ˜ ′ , a′1 , a′g )
∆
˜
m
∆
a′g ∈Ag ,a′1 ∈Al ,
j∈[m]
Fa′ ∈µk−1 (Al )
˜
∆

(10)
Since Q̂tk,m depends on s∆ and a∆ through Fz∆ , T̂k,m is also γ-contractive. So, Algorithm 1 applies
est
value iteration with T̂ until Q̂k,m converges to a fixed point satisfying T̂k,m Q̂est
k,m = Q̂k,m , yielding
est
a deterministic policy π̂k,m (sg , s1 , Fs∆˜ ):
est
π̂k,m
(sg , s1 , Fs∆˜ ) =

arg max

ag ∈Ag ,a1 ∈Al ,Fa ˜ ∈µk−1 (Al )

Q̂est
k,m (sg , s1 , Fz∆
˜ , a1 , ag )

∆

est
est
est
For π̂k,m
(sg , si , s∆\i ) = a∗g , a∗i , a∗∆\i , let [π̂k,m
(sg , s∆ )]g = a∗g and [π̂k,m
(sg , si , s∆\i )]l = a∗i .

6

Algorithm 1 SUB-SAMPLE-MFQ: Learning
Require: A multi-agent system as in Section 2, number of iterations T , sampling parameters k ∈ [n],
m ∈ N, and discount factor γ ∈ (0, 1).
˜ = {2, . . . , k}, and µk−1 (Zl ) = { b : b ∈ {0, 1, . . . , k − 1}}|Sl |×|Al | .
1: Let ∆ = {1, . . . , k}, ∆
k−1
2: if |Zl |k ≤ |Zl |k |Zl | then
3:
4:
5:
6:
7:

// Sub-sampled version of standard Q-learning

Initialize Q̂0k,m (sg , s∆ , ag , a∆ ) = 0, ∀(sg , s∆ , ag , a∆ ).
for t = 1 to T do
for (sg , s∆ , a∆ , ag ) ∈ Sg × Slk × Ag × Akl do
t
Q̂t+1
k,m (sg , s∆ , ag , a∆ ) = T̃k,m Q̂k,m (sg , s∆ , ag , a∆ )
est
8:
Let the greedy policy be π̂k,m (sg , s∆ ) := arg maxag ∈Ag ,a∆ ∈Ak Q̂Tk,m (sg , s∆ , ag , a∆ ).
l
9: else
10:
11:
12:
13:
14:
15:

// Sub-sampled version of mean-field Q-learning

Initialize Q̂0k,m (sg , s1 , Fz∆˜ , a1 , ag ) = 0, ∀(sg , s∆ , a∆ , ag ).
for t = 1 to T do
for (sg , s1 , Fz∆˜ , a1 , ag ) ∈ Sg × Sl × µk−1 (Zl ) × Al × Ag do
t
Q̂t+1
˜ , a1 , ag ) = T̂k,m Q̂k,m (sg , s1 , Fz∆
˜ , a1 , ag )
k,m (sg , s1 , Fz∆
est
Let the greedy policy be π̂k,m (sg , si , Fs∆˜ ) := arg max ag ∈Ag ,ai ∈Al , Q̂Tk,m (sg , s1 , Fz∆˜ , a1 , ag ).
Fa ˜ ∈µk−1 (Al )
∆

Remark 3.3. Since mean-field Q-learning maintains the same updates as standard Q-learning (from
Theorem C.19 which follows by noting that the Q-function is permutation-invariant in the homogeest
est
neous local agents), the deterministic policies π̂k,m
(sg , s∆ ) and π̂k,m
(sg , s1 , Fs∆¯ ) are equivalent.
Algorithm 2 (Online implementation). In
the global
 Algorithm 2, (SUBSAMPLE-MFQ: Execution)
[n]
est
agent samples local agents ∆(t) ∼ U k at each step to derive action ag (t) = [π̂k,m (sg , s∆ (t))]g ,

and each local agent i samples other local agents ∆i (t) ∼ U [n]\i
k−1 to derive action ai (t) =
est
[π̂k,m
(sg , si , s∆ (t))]l . The system then incurs a reward r(s, a). This procedure of first sampling
est
est
est
agents and then applying π̂k,m
is denoted by a stochastic policy πk,m
(a|s), where πk,m
(ag |s) is the
est
global agent’s action distribution and πk,m (al |s) is the local agent’s action distribution:
1 X
est
est
πk,m
(ag |s) = n
1(π̂k,m
(sg , s∆ ) = a)
k ∆∈ [n]
(k)
X
1
est
est
1(πk,m
(sg , si , Fs∆˜ ) = ai )
πk,m
(ai |s) = n−1
[n]\i
k−1 ∆∈
˜ (
k−1 )

(11)

(12)

The agents then transition to their next states.

4

Theoretical Guarantees and Analysis Approach

est
We now show the value of the expected discounted cumulative reward produced by πk,m
is approximately optimal, where the optimality gap decays as k → n and m grows.

Bellman noise. We introduce the notion of Bellman noise, which is used in the main theorem. Note
that T̂k,m is an unbiased estimator of the adapted Bellman operator T̂k ,
T̂k Q̂k (sg , s∆ , ag , a∆ ) = r∆ (s, a)+γE

max

s′g ∼Pg (·|sg ,ag ),
a′ ∈Ag ,a′∆ ∈Ak
l
s′i ∼Pl (·|si ,sg ,ai ),∀i∈∆ g

Q̂k (s′g , s′∆ , a′g , a′∆ ). (13)

Let Q̂0k (sg , s∆ , ag , a∆ ) = 0. For t > 0, let Q̂t+1
= T̂k Q̂tk , where T̂k is defined for k ≤ n in Equak
tion (13). Then, T̂k is also a γ-contraction with fixed-point Q̂∗k . By the law of large numbers,
∗
est
∗
limm→∞ T̂k,m = T̂k and ∥Q̂est
k,m − Q̂k ∥∞ → 0 as m → ∞. For finite m, ϵk,m := ∥Q̂k,m − Q̂k ∥∞
7

Algorithm 2 SUBSAMPLE-MFQ: Execution
Require: Parameter T ′ for the number of iterations for the decision-making sequence. Sampling
est
parameter k ∈ [n], m ∈ N. Discount factor γ. Policy π̂k,m
(sg , Fs∆ ).
est
1: Learn π̂k,m from Algorithm 1.
2: Sample (sg (0), s[n] (0)) ∼ s0 , where s0 is a distribution on the initial global state (sg , s[n] )
3: Initialize the total reward R0 = 0.
est
4: Policy πk,m
(s) is defined as follows:
5: for t = 0 to T ′ do

est
6:
Choose ∆ uniformly at random from [n]
k and let ag (t) = [π̂k,m (sg (t), s∆ (t))]g .
7:
for i = 1 to n do

est
8:
Choose ∆i uniformly at random from [n]\i
k−1 and let ai (t) = [π̂k,m (sg (t), si (t), s∆i (t))]l .
9:
Let sg (t + 1) ∼ Pg (·|sg (t), ag (t)).
10:
Let si (t + 1) ∼ Pl (·|si (t), sg (t), ai (t)), ∀i ∈ [n].
11:
Rt+1 = Rt + γ t · r(s, a)
is the well-studied Bellman noise. To compare the performance between π ∗ and πkest , we define the
value function of a policy π:
Definition 4.1. For a given policy π, the value function V π : S → R for S := Sg × Sln is given by:
"∞
#
X
π
t
V (s) =
γ r(s(t), a(t)) s(0) = s .
(14)
E
a(t)∼π(·|s(t))

t=0

Intuitively, V π (s) is the expected discounted cumulative reward when starting from state s and
applying actions from the policy π across an infinite horizon.
With the above preparations, we are primed to present our main result: a high-probability
bound on
√
est
that decays with rate Õ(1/ k).
the optimality gap for our learned policy πk,m
est
Theorem 4.2. Let πk,m
denote the learned policy deployed in SUBSAMPLE-MFQ: Execution. Then,
for all s0 ∈ S := Sg × Sln , we have
s
r
1
∗
est
r̃
n
−
k
+
1
40r̃|Sl ||Al ||Ag |k |Al |+ 2
1
π
πk,m
V (s0 ) − V
(s0 ) ≤
ln
+ √ + 2ϵk,m
(1 − γ)2
2nk
(1 − γ)2
10 k

We prove Theorem 4.4 in Section G, and provide a proof sketch in Section D. We also generalize the
result to stochastic rewards in Section H.
∗
∗
To control the Bellman
√noise ϵk,m , we show that for sufficiently many samples m , ϵk,m also decays
on the order of Õ(1/ k) with high probability. For this, we introduce Theorem 4.3:
Lemma 4.3 (Controlling the Bellman Noise.). For k ∈ [n], let
log(|Sg ||Ag ||Al ||Sl |)
1
m∗ = 2|Sg ||Ag ||Sl ||Al |k 3.5+|Sl ||Al |
log
5
(1 − γ)
(1 − γ)2
√
2
be the number of samples in Equation (10). If the number of iterations T satisfies T ≥ 1−γ
log r̃1−γk
1
√1 ).
then with probability at least 1 − 100e
k , the Bellman error satisfies ϵk,m∗ ≤ Õ(
k
est
We defer the proof of Theorem 4.3 to Section G.1. To simplify notation, let πkest := πk,m
∗ . Then, by
combining Theorem 4.3 and Theorem 4.2, we obtain our main result in Theorem 4.4:
Theorem 4.4. Let πkest denote the learned policy from SUBSAMPLE-MFQ: Execution
where the
√
2
r̃ k
number of samples m is determined in Theorem 4.3. Suppose T ≥ 1−γ log 1−γ . Then, ∀s0 ∈ S :=
Sg × Sln , with probability at least 1 − 1/100ek , we have 3 ,
s
1
est
r̃
n − k + 1 40r̃|Sl ||Al ||Ag |k |Al |+ 2
4
π∗
πk
V (s0 ) − V
(s0 ) ≤
ln
+√ .
(1 − γ)2
2nk
(1 − γ)2
k
k
The 1/100e
√ term can be replaced to any arbitrary δ > 0 at the cost of attaching logarithmic dependencies
of δ on the 4/ k term in the error bound.
3

8

Remark 4.5. One could also derive an alternate bound that retains the T factor via a γ T dependence
in the final bound (which shows an exponentially decaying error with the time horizon T ). Moreover,
in Theorem G.10, we show that the query complexity is on the√order of O(mT |Sg ||Sl |k |Ag ||Al |k ),
and we bound T and set m = poly(1/(1 − γ)) to attain the 1/ k rate.
Remark 4.6. Additionally, our poly(1/(1 − γ))-dependence may be loose since we do not use more
complicated variance reduction techniques as in Sidford et al. [2018a,b], Wainwright [2019], Jin et al.
[2024] to optimize the number of samples m which is used to bound the Bellman error ϵk,m . Moreover,
incorporating variance reduction would significantly complicate the algorithm and intuition.
Our analysis hinges on two non-trivial technical steps. Firstly, for an intermediary step in Theorem E.3
we establish that TV distance, rather than the stronger KL-divergence, is the correct metric to use (as
the KL-divergence between Fz∆ and Fz[n] decays too slowly as k → n, as we show in Theorem F.6).
This requires exploiting a recent extension of the DKW inequality to sampling without replacement
[Anand and Qu, 2024], and showing that the transition dynamics we study saturates the dataprocessing inequality for TV-distance. Secondly, we adapt the celebrated performance-difference
lemma [Kakade and Langford, 2002] to our multi-agent setting, which entails a principled analysis
and careful probabilistic argument (to which we refer the reader to Section G).
Remark 4.7. We also extend the formulation of SUBSAMPLE-MFQ to off-policy Q-learning Chen
et al. [2021b], Chen and Maguluri [2022a], which replaces the generative oracle assumption with a
stochastic approximation scheme that learns an approximately optimal policy using historical data.
Section I provides theoretical guarantees with a similar decaying optimality gap as in Theorem 4.4.
Remark 4.8. The asymptotic sample complexity of Algorithm 1 for learning π̂kest for a fixed k is
min{Õ(|Zl |k |Sg ||Ag |), Õ(k |Zl | |Zl ||Sg ||Ag |)}, which is at least polynomially faster the standard
Q-learning or mean-field value iteration as discussed in Theorem 3.1. By Theorem 4.4, as k → n,
the optimality gap decays, revealing a fundamental trade-off in the choice of k: increasing k improves the performance of the policy, but increases the size of the Q-function. We explore this
trade-off further in our experiments. If we set k = O(log n), this leads to a sample complexity
of min{Õ(nlog |Zl | |Sg ||Ag |), Õ((log n)|Zl | |Sg ||Ag |)}. This is an exponential speedup on the complexity from mean-field value iteration (from poly(n) to poly(log n)), as well as over traditional
1
).
value-iteration (from exp(n) to poly(n)), where the optimality gap decays to 0 with rate O( √log
n
Remark 4.9. If k = O(log n), SUBSAMPLE-MFQ handles |E| ≤ O(log n/ log log n) types of local
agents, since the run-time of the learning algorithm becomes poly(n). This surpasses the previous
heterogeneity capacity from Mondal et al. [2022], which only handles constant |E| ≤ O(1). Increasing the agent heterogeneity using this type formulation does make the algorithm somewhat
more expensive since it factors into the query complexity via the state space of
√ the local agents;
however,
since
the
optimality
gap
of
the
learned
policy
is
on
the
order
of
Õ(1/
k) (modulo very
p
small log |Sl ||Al | factors), increasing the amount of agent heterogeneity does not degrade the
quality of the learned policy as the theoretical bound does not get worse. Moreover, recent methods
from the graphon mean-field MARL community might be able to enable stronger heterogeneity in
the system [Cui and Koeppl, 2022, Anand and Liaw, 2025, Hu et al., 2023].
In the non-tabular setting with infinite state/action spaces, one could replace the Q-learning algorithm
with any arbitrary value-based RL method that learns Q̂k with function approximation [Sutton et al.,
1999] such as deep Q-networks [Silver et al., 2016]. Doing so raises an additional error that factors
into Theorem 4.4. We formalize this below.
Assumption 4.10 (Linear MDP with infinite state spaces). Suppose Sg and Sl are infinite compact
sets. Furthermore, suppose there exists a feature map ϕ : S × A → Rd and d unknown (signed)
measures µ = (µ1 , . . . , µd ) over S and a vector θ ∈ Rd such that for any (s, a) ∈ S × A, we have
P(·|s, a) = ⟨ϕ(s, a), µ(·)⟩ and r(s, a) = ⟨ϕ(s, a), θ⟩.
The existence of ϕ : S × A → Rd implies one can estimate the Q-function of any policy as a linear
function. This assumption is commonly used in policy iteration algorithms Lattimore et al. [2020],
Wang et al. [2023], and allows one to obtain sample complexity bounds that are independent of |Sl |
and |Al |. Finally, as is standard in RL, we assume bounded feature-norms [Tkachuk et al., 2023]:
Assumption 4.11 (Bounded features). We assume that ∥ϕ(s, a)∥2 ≤ 1 for all (s, a) ∈ S × A.
9

Then, through a reduction from Zhang et al. [2024], Ren et al. [2024] that uses function approximation
to learn the spectral features ϕk for Q̂k , we derive a performance guarantee for the learned policy
πkest , where the optimality gap decays with k.
Theorem 4.12. When πkest is derived from the spectral features ϕk learned in Q̂k , and M is the
number of samples used in the function approximation, then



∗
est
1
201
∥ϕk ∥5 log 2k 2
2γ r̃ · ∥ϕk ∥
200
√
√
√
Pr V π (s) − V πk (s) ≤ Õ √ +
−
+
≥1+
k
M
k
(1 − γ) k
200 k
We defer the proof of Theorem 4.12 to Section J.

5

Conclusion and Future Works

This work develops subsampling for mean field MARL in a cooperative system with a global decisionmaking agent and n homogeneous local agents. We propose SUBSAMPLE-MFQ which learns each
agent’s best response to the mean effect from a sample of its neighbors, allowing an exponential
reduction on the sample complexity of approximating a solution to the MDP. We provide a theoretical
analysis on the optimality gap of the learned policy, showing that (with high probability) the learned
√
policy converges to the optimal policy with the number of agents k sampled at the rate Õ(1/ k),
and validate our theoretical results through numerical experiments. We show that the decay rate is
maintained, on expectation, when the reward functions are stochastic and when agents learn from a
single trajectory on historical data through off-policy Q-learning. Finally, we extend this result to the
non-tabular setting with infinite state and action spaces under assumptions of a linear MDP model.
Limitations and future work. Our current work assumes that the global and local agents cooperate
to optimize a structured reward under a specific dynamic model. While this model is more general
than the federated learning setting, one direction would be to extend our algorithms and analysis to
weaker network assumptions. We believe our framework, which can handle dense subgraphs, as well
as expander-graph decompositions [Reingold, 2008, Anand and Umans, 2023, Anand, 2025] may be
amenable for this. Secondly, our current work incorporates mild heterogeneity among agents and
assumes they are cooperative; thus, another avenue would be to consider settings with competitive
agents or more complex agent heterogeneity. Finally, it would be exciting to generalize this work to
the online no-regret setting.
Societal Impacts. This work is theoretical and foundational in nature. As such, while it enables
more scalable multi-agent algorithms, it is not tied to any specific applications or deployments.
Acknowledgements. This work was supported by NSF Grants 2154171, CAREER Award 2339112,
CMU CyLab Seed Funding, and CCF 2338816. We gratefully acknowledge insightful discussions
with Siva Theja Maguluri, Yiheng Lin, Jan van den Brand, Adam Wierman, Yunbum Kook, Yi Wu,
Sarah Liaw, and Rishi Veerapaneni. Finally, we thank the anonymous NeurIPS reviewers for their
helpful feedback.

10

References
Emile Anand. Towards the pseudorandomness of expander random walks for read-once acc0 circuits,
2025. URL https://arxiv.org/abs/2501.07752.
Emile Anand and Sarah Liaw. Feel-good thompson sampling for contextual bandits: a markov chain
monte carlo showdown, 2025. URL https://arxiv.org/abs/2507.15290.
Emile Anand and Guannan Qu. Efficient reinforcement learning for global decision making in the
presence of local agents at scale. arXiV, 2024. URL https://arxiv.org/abs/2403.00222.
Emile Anand and Chris Umans. Pseudorandomness of the Sticky Random Walk. Caltech Undergraduate Thesis, 2023. URL https://thesis.library.caltech.edu/16116/7/AnandCaltech_
Final_Thesis_Submission2.pdf.
Emile Anand, Jan van den Brand, Mehrdad Ghadiri, and Daniel J. Zhang. The Bit Complexity of Dynamic Algebraic Formulas and Their Determinants. In Karl Bringmann, Martin
Grohe, Gabriele Puppis, and Ola Svensson, editors, 51st International Colloquium on Automata,
Languages, and Programming (ICALP 2024), volume 297 of Leibniz International Proceedings in Informatics (LIPIcs), pages 10:1–10:20, Dagstuhl, Germany, 2024. Schloss Dagstuhl –
Leibniz-Zentrum für Informatik. ISBN 978-3-95977-322-5. doi: 10.4230/LIPIcs.ICALP.2024.
10. URL https://drops.dagstuhl.de/storage/00lipics/lipics-vol297-icalp2024/
LIPIcs.ICALP.2024.10/LIPIcs.ICALP.2024.10.pdf.
Emile Anand, Jan van den Brand, and Rose McCarty. The structural complexity of matrix-vector
multiplication. arXiv preprint arXiv:2502.21240, 2025. URL https://www.arxiv.org/pdf/
2502.21240.
Stefan Banach. Sur les opérations dans les ensembles abstraits et leur application aux équations
intégrales. Fundamenta Mathematicae, 3(1):133–181, 1922. URL http://eudml.org/doc/
213289.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1st edition, 1996. ISBN 1886529108. URL https://web.stanford.edu/~bvr/pubs/NDPoverview.
pdf.
Vincent D. Blondel and John N. Tsitsiklis. A Survey of Computational Complexity Results in Systems
and Control. Automatica, 36(9):1249–1274, 2000. ISSN 0005-1098. doi: https://doi.org/10.1016/
S0005-1098(00)00050-9. URL https://www.sciencedirect.com/science/article/abs/
pii/S0005109800000509.
Shreyas Chaudhari, Srinivasa Pranav, Emile Anand, and José M. F. Moura. Peer-to-peer learning
dynamics of wide neural networks. IEEE International Conference on Acoustics, Speech, and
Signal Processing 2025, 2024. URL https://arxiv.org/abs/2409.15267.
Zaiwei Chen and Siva Theja Maguluri. Sample complexity of policy-based methods under off-policy
sampling and linear function approximation. In International Conference on Artificial Intelligence
and Statistics, pages 11195–11214. PMLR, 2022a. URL https://proceedings.mlr.press/
v151/chen22i/chen22i.pdf.
Zaiwei Chen and Siva Theja Maguluri. Sample complexity of policy-based methods under offpolicy sampling and linear function approximation. In Gustau Camps-Valls, Francisco J. R.
Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial
Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages
11195–11214. PMLR, 28–30 Mar 2022b. URL https://proceedings.mlr.press/v151/
chen22i/chen22i.pdf.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample
analysis of off-policy td-learning via generalized bellman operators. Advances in Neural Information Processing Systems, 34:21440–21452, 2021a. URL https://proceedings.neurips.cc/
paper/2021/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf.
11

Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A lyapunov
theory for finite-sample guarantees of asynchronous q-learning and td-learning variants. arXiv
preprint arXiv:2102.01567, 2021b. URL https://pubsonline.informs.org/doi/10.1287/
opre.2022.0249.
Zaiwei Chen, Siva Theja Maguluri, and Martin Zubeldia. Concentration of contractive stochastic
approximation: Additive and multiplicative noise. The Annals of Applied Probability, 35(2):
1298–1352, 2025.
Kai Cui and Heinz Koeppl. Learning Graphon Mean Field Games and Approximate Nash Equilibria.
In International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=0sgntlpKDOz.
Kai Cui, Christian Fabian, and Heinz Koeppl. Multi-Agent Reinforcement Learning via Mean
Field Control: Common Noise, Major Agents and Approximation Properties, 2023. URL https:
//arxiv.org/abs/2303.10665.
Alex DeWeese and Guannan Qu. Locally interdependent multi-agent MDP: Theoretical framework
for decentralized agents with dynamic dependencies. In Forty-first International Conference on
Machine Learning, 2024. URL https://arxiv.org/pdf/2406.06823.
Thomas G. Dietterich, George Trimponias, and Zhitang Chen. Discovering and removing exogenous
state variables and rewards for reinforcement learning, 2018. URL https://arxiv.org/abs/
1806.01584.
A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic Minimax Character of the Sample Distribution
Function and of the Classical Multinomial Estimator. The Annals of Mathematical Statistics, 27
(3):642 – 669, 1956. doi: 10.1214/aoms/1177728174. URL https://doi.org/10.1214/aoms/
1177728174.
Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the Complexity of Adversarial Decision Making. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=pgBpQYss2ba.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International conference on machine learning, pages 2052–2062. PMLR, 2019.
URL https://arxiv.org/pdf/1812.02900.
David Gamarnik, David Goldberg, and Theophane Weber. Correlation Decay in Random Decision
Networks, 2009. URL https://www.jstor.org/stable/24540896.
Subhashis Ghosal and Aad van der Vaart. Fundamentals of Nonparametric Bayesian Inference:
Space of Probability Densities, page 516–527. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2017.
URL https://www.cambridge.
org/core/books/fundamentals-of-nonparametric-bayesian-inference/
C96325101025D308C9F31F4470DEA2E8.
Noah Golowich and Ankur Moitra. The role of inherent bellman error in offline reinforcement learning
with linear function approximation, 2024. URL https://arxiv.org/abs/2406.11686.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-Field Controls with Q-Learning for
Cooperative MARL: Convergence and Complexity Analysis. SIAM Journal on Mathematics of
Data Science, 3(4):1168–1196, 2021. doi: 10.1137/20M1360700. URL https://doi.org/10.
1137/20M1360700.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-Field Multi-Agent Reinforcement
Learning: A Decentralized Network Approach, 2022. URL https://arxiv.org/pdf/2108.
02731.pdf.
Yuanquan Hu, Xiaoli Wei, Junji Yan, and Hengxi Zhang. Graphon Mean-Field Control for Cooperative Multi-Agent Reinforcement Learning. Journal of the Franklin Institute, 360(18):14783–14805,
2023. ISSN 0016-0032. URL https://doi.org/10.1016/j.jfranklin.2023.09.002.
12

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Jacob Abernethy and Shivani Agarwal,
editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 2137–2143. PMLR, 09–12 Jul 2020. URL
https://proceedings.mlr.press/v125/jin20a.html.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms, 2021. URL https://arxiv.org/abs/2102.00815.
Junqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. Real-time bidding with
multi-agent reinforcement learning in display advertising. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM ’18, page 2193–2201,
New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360142. doi:
10.1145/3269206.3272021. URL https://dl.acm.org/doi/10.1145/3269206.3272021.
Yujia Jin, Ishani Karmarkar, Aaron Sidford, and Jiayi Wang.
Truncated variance reduced value iteration.
In Advances in Neural Information Processing Systems,
2024.
URL https://proceedings.neurips.cc/paper_files/paper/2024/file/
d529b943af3dba734f8a7d49efcb6d09-Paper-Conference.pdf. Also available as arXiv
preprint arXiv:2405.12952.
Sham Kakade and John Langford. Approximately Optimal Approximate Reinforcement Learning.
In Claude Sammut and Achim Hoffman, editors, Proceedings of the Nineteenth International
Conference on Machine Learning (ICML 2002), pages 267–274, San Francisco, CA, USA, 2002.
Morgan Kauffman. ISBN 1-55860-873-7. URL http://ttic.uchicago.edu/~sham/papers/
rl/aoarl.pdf.
Seung-Jun Kim and Geogios B. Giannakis. An Online Convex Optimization Approach to Realtime Energy Pricing for Demand Response. IEEE Transactions on Smart Grid, 8(6):2784–2793,
2017. doi: 10.1109/TSG.2016.2539948. URL https://ieeexplore.ieee.org/document/
7438918.
B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil
Yogamani, and Patrick Pérez. Deep Reinforcement Learning for Autonomous Driving: A Survey.
IEEE Transactions on Intelligent Transportation Systems, 23(6):4909–4926, 2022. doi: 10.1109/
TITS.2021.3054625. URL https://ieeexplore.ieee.org/document/9351818.
Robert Kleinberg. A multiple-choice secretary algorithm with applications to online auctions. In
Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05,
page 630–631, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.
URL https://dl.acm.org/doi/10.5555/1070432.1070519.
Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement Learning in Robotics: A
Survey. The International Journal of Robotics Research, 32(11):1238–1274, 2013. doi:
10.1177/0278364913495721. URL https://doi.org/10.1177/0278364913495721.
Kasper Green Larsen, Omar Montasser, and Nikita Zhivotovskiy. Derandomizing multi-distribution
learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems,
2024. URL https://openreview.net/forum?id=twYE75Mnkt.
Jean-Michel Lasry and Pierre-Louis Lions. Mean Field Games. Japanese Journal of Mathematics,
2(1):229–260, March 2007. ISSN 1861-3624. doi: 10.1007/s11537-007-0657-8. URL https:
//link.springer.com/article/10.1007/s11537-007-0657-8.
Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in
bandits and in RL with a generative model. In Hal Daumé III and Aarti Singh, editors, Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pages 5662–5670. PMLR, 13–18 Jul 2020. URL https://proceedings.
mlr.press/v119/lattimore20a.html.
Martin Lauer and Martin A. Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In Proceedings of the Seventeenth International Conference on Machine
Learning, ICML ’00, page 535–542, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers
Inc. ISBN 1558607072. URL https://dl.acm.org/doi/10.5555/645529.658113.
13

Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample Complexity of Asynchronous
Q-Learning: Sharper Analysis and Variance Reduction. IEEE Transactions on Information Theory,
68(1):448–473, 2022. doi: 10.1109/TIT.2021.3120096. URL https://arxiv.org/abs/2006.
03041.
Minne Li, Zhiwei Qin, Yan Jiao, Yaodong Yang, Jun Wang, Chenxi Wang, Guobin Wu, and Jieping
Ye. Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning.
In The World Wide Web Conference, WWW ’19, page 983–994, New York, NY, USA, 2019.
Association for Computing Machinery. ISBN 9781450366748. doi: 10.1145/3308558.3313433.
URL https://doi.org/10.1145/3308558.3313433.
Yiheng Lin, Guannan Qu, Longbo Huang, and Adam Wierman. Distributed Reinforcement Learning
in Multi-Agent Networked Systems. CoRR, abs/2006.06555, 2020. URL https://arxiv.org/
abs/2006.06555.
Yiheng Lin, James A Preiss, Emile Timothy Anand, Yingying Li, Yisong Yue, and Adam Wierman.
Online Adaptive Policy Selection in Time-Varying Systems: No-Regret via Contractive Perturbations. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL
https://openreview.net/forum?id=hDajsofjRM.
Yiheng Lin, James A Preiss, Emile Timothy Anand, Yingying Li, Yisong Yue, and Adam Wierman.
Learning-augmented Control via Online Adaptive Policy Selection: No Regret via contractive
Perturbations. In ACM SIGMETRICS, Workshop on Learning-augmented Algorithms: Theory and
Applications 2023, 2023b. URL https://learning-augmented-algorithms.github.io/
papers/sigmetrics23-lata-posters-paper5.pdf.
Yiheng Lin, James A. Preiss, Fengze Xie, Emile Anand, Soon-Jo Chung, Yisong Yue, and Adam
Wierman. Online policy optimization in unknown nonlinear systems. In Shipra Agrawal and
Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247
of Proceedings of Machine Learning Research, pages 3475–3522. PMLR, 30 Jun–03 Jul 2024.
URL https://proceedings.mlr.press/v247/lin24a.html.
Michael L. Littman. Markov Games as a Framework for Multi-Agent Reinforcement Learning. In Machine learning proceedings, Elsevier, page 157–163, 1994. URL https://www.sciencedirect.
com/science/article/abs/pii/B9781558603356500271.
P. Massart. The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality. The Annals of Probability, 18(3):1269 – 1283, 1990. doi: 10.1214/aop/1176990746. URL
https://projecteuclid.org/journals/annals-of-probability/volume-18/
issue-3/The-Tight-Constant-in-the-Dvoretzky-Kiefer-Wolfowitz-Inequality/
10.1214/aop/1176990746.full.
Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Cooperative Multi-Agent Reinforcement
Learning: Asynchronous Communication and Linear Function Approximation. In Andreas Krause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,
editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of
Proceedings of Machine Learning Research, pages 24785–24811. PMLR, 23–29 Jul 2023. URL
https://proceedings.mlr.press/v202/min23a.html.
Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V. Ukkusuri. On the Approximation of Cooperative Heterogeneous Multi-Agent Reinforcement Learning (MARL) Using Mean
Field Control (MFC). Journal of Machine Learning Research, 23(1), jan 2022. ISSN 1532-4435.
URL https://jmlr.org/papers/volume23/21-1312/21-1312.pdf.
Michael Naaman. On the Tight Constant in the Multivariate Dvoretzky–Kiefer–Wolfowitz Inequality.
Statistics & Probability Letters, 173:109088, 2021. ISSN 0167-7152. doi: https://doi.org/10.
1016/j.spl.2021.109088. URL https://www.sciencedirect.com/science/article/pii/
S016771522100050X.
Christos H. Papadimitriou and John N. Tsitsiklis. The Complexity of Optimal Queuing Network
Control. Mathematics of Operations Research, 24(2):293–305, 1999. ISSN 0364765X, 15265471.
URL https://www.jstor.org/stable/3690486.
14

James A. Preiss, Wolfgang Honig, Gaurav S. Sukhatme, and Nora Ayanian. Crazyswarm: A large
nano-quadcopter swarm. In 2017 IEEE International Conference on Robotics and Automation
(ICRA), pages 3299–3304, 2017. doi: 10.1109/ICRA.2017.7989376. URL https://ieeexplore.
ieee.org/document/7989376/.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable Multi-Agent Reinforcement Learning
for Networked Systems with Average Reward. In Proceedings of the 34th International Conference
on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA, 2020a. Curran Associates Inc. ISBN 9781713829546. URL https://proceedings.neurips.cc/paper/2020/
file/168efc366c449fab9c2843e9b54e2a18-Paper.pdf.
Guannan Qu, Adam Wierman, and Na Li. Scalable Reinforcement Learning of Localized Policies
for Multi-Agent Networked Systems. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas,
Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors, Proceedings of
the 2nd Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine
Learning Research, pages 256–266. PMLR, 10–11 Jun 2020b. URL https://proceedings.
mlr.press/v120/qu20a.html.
Omer Reingold. Undirected Connectivity in Log-Space. J. ACM, 55(4), sep 2008. ISSN 0004-5411.
doi: 10.1145/1391289.1391291. URL https://doi.org/10.1145/1391289.1391291.
Zhaolin Ren, Runyu, Zhang, Bo Dai, and Na Li. Scalable spectral representations for network
multiagent control, 2024. URL https://arxiv.org/abs/2410.17221.
Ross D. Shachter. Bayes-ball: The rational pastime (for determining irrelevance and requisite
information in belief networks and influence diagrams), 2013.
Lloyd S. Shapley. A value for n-person games. In Contributions to the Theory of Games, 1953. URL
https://www.rand.org/content/dam/rand/pubs/papers/2021/P295.pdf.
Aaron Sidford, Mengdi Wang, Xian Wu, Lin F. Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving discounted markov decision process with a generative model. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
Neural Information Processing Systems, volume 31, pages 7739–7750, 2018a.
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster
algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA), New Orleans, LA, USA, 2018b. Society
for Industrial and Applied Mathematics. doi: 10.1137/1.9781611975031.116.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game of Go with
Deep Neural Networks and Tree Search. Nature, 529(7587):484–489, January 2016. ISSN 14764687. doi: 10.1038/nature16961. URL https://www.nature.com/articles/nature16961.
Sriram Ganapathi Subramanian, Matthew E. Taylor, Mark Crowley, and Pascal Poupart. Decentralized
mean field games, 2022. URL https://arxiv.org/abs/2112.09099.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and
K. Müller, editors, Advances in Neural Information Processing Systems, volume 12. MIT
Press, 1999. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/
464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.
Volodymyr Tkachuk, Seyed Alireza Bakhtiari, Johannes Kirschner, Matej Jusup, Ilija Bogunovic,
and Csaba Szepesvári. Efficient planning in combinatorial action spaces with applications to
cooperative multi-agent reinforcement learning. arXiV, 2023. URL https://proceedings.mlr.
press/v206/tkachuk23a/tkachuk23a.pdf.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company,
Incorporated, 1st edition, 2008. ISBN 0387790519. URL https://link.springer.com/book/
10.1007/b13794.
15

Martin J Wainwright. Variance-reduced q-learning is minimax optimal.
arXiv:1906.04697, 2019.

arXiv preprint

Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized
execution: Multi-agent conditional policy factorization, 2023. URL https://arxiv.org/abs/
2209.12681.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3):279–292, May
1992. ISSN 1573-0565. doi: 10.1007/BF00992698. URL https://link.springer.com/
article/10.1007/BF00992698.
Mengfan Xu and Diego Klabjan. Decentralized randomly distributed multi-agent multi-armed bandit
with heterogeneous rewards. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023. URL https://openreview.net/forum?id=DqfdhM64LI.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean Field MultiAgent Reinforcement Learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 5571–5580. PMLR, 10–15 Jul 2018. URL https://proceedings.
mlr.press/v80/yang18d.html.
Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, and Bo Dai. Provable representation with efficient planning for partially observable reinforcement learning. In Proceedings
of the 41st International Conference on Machine Learning, ICML’24. JMLR.org, 2024. URL
https://dl.acm.org/doi/10.5555/3692070.3694540.
Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-Agent Reinforcement Learning: A Selective
Overview of Theories and Algorithms, 2021. URL https://arxiv.org/abs/1911.10635.
Yihe Zhou, Shunyu Liu, Yunpeng Qing, Kaixuan Chen, Tongya Zheng, Yanhao Huang, Jie Song, and
Mingli Song. Is centralized training with decentralized execution framework centralized enough
for marl?, 2023. URL https://arxiv.org/abs/2305.17352.

16

A

Mathematical Background and Additional Remarks

Outline of the Appendices.
• Section B presents numerical simulations on the performance of SUBSAMPLE-MFQ on the
Gaussian squeeze and constrained exploration tasks.
• Section C presents notation and basic lemmas involving the learned Q̂k -function, as well as
a stable and practical implementation of Algorithm 1.
• Section D presents a proof sketch of our main result in Theorem 4.4.
• Section E presents the proof of the Lipschitz continuity between Q̂k and Q∗
• Section F presents our bound on the TV-distance in Theorem D.2.
est
• Section G proves the bound on the optimality gap between the learned policy π̃k,m
and the
∗
optimal policy π .
• Section H presents an extension of the result to stochastic rewards.
• Section I presents an extension of the result to off-policy learning that follows by using
historical data.
• Section J presents an extension of the result to continuous state/action spaces under a linear
MDP assumption.
• Section K presents some practical derandomized variants with randomness-sharing.
Table 1: Important notations in this paper.
Notation
∥ · ∥1
∥ · ∥∞
Z+
Rd
[m]
[m]
k
ag

sg
a1 , . . . , an
s1 , . . . , sn
a
s
zi
µk (Zl )
µ(Zl )
s∆
′
σ(z∆ , z∆
)
∗
π
π̂k∗
π̃kest
Pg (·|sg , ag )
Pl (·|ai , si , sg )
rg (sg , ag )
rl (si , sg , ai )
r(s, a)
r∆ (s, a)
T
T̂k
ΠΘ (y)

Meaning
ℓ1 (Manhattan) norm;
ℓ∞ norm;
The set of strictly positive integers;
The set of d-dimensional reals;
The set {1, . . . , m}, where m ∈ Z+ ;
The set of k-sized subsets of {1, . . . , m};
ag ∈ Ag is the action of the global agent;
sg ∈ Sg is the state of the global agent;
a1 , . . . , an ∈ Anl are the actions of the local agents 1, . . . , n;
s1 , . . . , sn ∈ Sln are the states of the local agents 1, . . . , n;
a = (ag , a1 , . . . , an ) ∈ Ag × Anl is the tuple of actions of all agents;
s = (sg , s1 , . . . , sn ) ∈ Sg × Sln is the tuple of states of all agents;
zi = (si , ai ) ∈ Zl , for i ∈ [n];
µk (Zl ) = {0, 1/k, 2/k, . . . , 1}|Zl | ;
µ(Zl ) := µn (Zl ){0, 1/n, 2/n, . . . , 1}|Zl | ;
For ∆ ⊆ [n], and a collection of variables {s1 , . . . , sn }, s∆ := {si : i ∈ ∆};
′
Product sigma-algebra generated by sequences z∆ and z∆
;
∗
π is the optimal deterministic policy function such that a = π ∗ (s);
π̂k∗ is the optimal deterministic policy function on a constrained system of
|∆| = k local agents;
π̃kest is the stochastic policy map learned with parameter k such that a ∼ π̃kest (s);
Pg (·|sg , ag ) is the stochastic transition kernel for the state of the global agent;
Pl (·|ai , si , sg ) is the stochastic transition kernel for the state of local agent i ∈ [n];
rg is the global agent’s component of the reward;
rl is the component of theP
reward for local agent i ∈ [n];
r(s, a) = rg (sg , ag ) + n1 i∈[n] rl (si , sg , ai ) is the reward of the system;
P
1
r∆ (s, a) = rg (sg , ag ) + |∆|
i∈∆ rl (si , sg , ai ) is the constrained system’s reward
with |∆| = k local agents;
T is the centralized Bellman operator;
T̂k is the Bellman operator on a constrained system of |∆| = k local agents;
ℓ1 projection of y onto set Θ;

17

Definition A.1 (Lipschitz continuity). Given metric spaces (X , dX ) and (Y, dY ) and a constant L >
0, a map f : X → Y is L-Lipschitz continuous if for all x, y ∈ X , dY (f (x), f (y)) ≤ L · dX (x, y).
Theorem A.2 (Banach-Caccioppoli fixed point theorem [Banach, 1922]). Consider the metric space
(X , dX ), and T : X → X such that T is a γ-Lipschitz continuous mapping for γ ∈ (0, 1). Then,
by the Banach-Cacciopoli fixed-point theorem, there exists a unique fixed point x∗ ∈ X for which
T (x∗ ) = x∗ . Additionally, x∗ = lims→∞ T s (x0 ) for any x0 ∈ X .

B

Numerical Experiments

This section provides numerical simulations for the examples outlined in Section 2. All experiments
were run on a 2-core CPU server with 12GB RAM. We chose a parameter complexity for each
simulation that was sufficient to emphasize characteristics of the theory, such as the complexity
improvement and the decaying optimality gap.4

est
Figure 2: a) Reward optimality gap (log scale) with πk,m
running 300 iterations. b) Computation
est
time (in minutes) against sampling parameter k, for k ≤ n = 8, to learn policy π̂k,m
. c) Discounted
cumulative rewards for k ≤ n = 50.

B.1

Constrained Exploration
(i)

Let Sg = Sl = [6]2 where for sa ∈ {Sg , Sl }, sa denotes the projection of sa onto its
i’th coordinate.
let Al = Ag = {“up", “down", “right", “left"} given formally by
 0 0Further,

t+1 t
t
{ 10 , −1
,
,
}.
Let
Π
D (x) denote the ℓ1 -projection of x onto set D. Then, let sg (sg , ag ) =
0
1
−1
t+1 t t
t
t
t
t
t
t
t
ΠSg (sg + ag ), si (si , sg , ai ) = ΠSl (si + |si − sg | + ai ). We let the global agent’s reward be
 (1)
 (1)
P3
P6
rg (sg , ag ) = 2 i=1 1{ag = 10 , sg ≤ i} + 2 i=3 1{ag = −1
0 , sg > i}, and the local agent’s
(1)
(1)
rewards be rl (si , sg , ai ) = 1{ati ̸= 0} + 6 · 1{si = sg } + 2 · ∥sti − stg ∥1 .
Intuitively, the reward function is designed to force the global agent to oscillate vertically in the grid,
while forcing the local agents to oscillate horizontally around the global agent, thereby simulating a
constrained exploration. This model has been studied previously in Lin et al. [2023b].
For this task, we ran a simulation with n = 8 agents, with m = 20 samples in the empirically adapted
Bellman operator. We provide simulation results in Figure 2a. We observe monotonic improvements
in the cumulative discounted rewards as k → n. Since k = n recovers value-iteration and mean-field
MARL algorithms, the reward at k = n is the baseline we compare our algorithm to. When k < n,
we observe that the reward accrued by SUBSAMPLE-MFQ is only marginally less than the reward
gained by value-iteration.
B.2

Gaussian Squeeze (GS)

In this task, n homogeneous agents determine their individual action ai to jointly maximize the
Pn
2
2
objective r(x) = xe−(x−µ) /σ , where x = i=1 ai , ai ∈ {0, . . . , 9}, and µ and σ are the predefined mean and variance of the system. We consider a homogeneous system devoid of a global
4

We provide supporting code for the algorithm and experiments in https://github.com/emiletimothy/
Mean-Field-Subsample-Q-Learning

18

agent to match the mean-field setting in Yang et al. [2018], where the richness of our model setting
can still express GS. We use the global agent to model the state of the system, given by the number of
vehicles in each controller’s lane.

 Pn
We set Sl = [4], Al = {0, . . . , 4}, and Sg = Sl such that sg = n1 i=1 si , and Ag = {0}.
The transition
are given
 by si (t + 1) = si (t) − 1 · 1{si (t) > sg (t)} + Ber(p) and
 functions
Pn
sg (t + 1) = n1 i=1 si (t + 1) , where Ber(p) is a Bernoulli random variable with parameter p > 0.
Finally, the global agent’s reward function is given by rg (sg , ag ) = −sg and the local agent’s reward
2
function is given by rl (si , sg , ai ) = si · e−(si −sg ) /4 − max{si , ai }.
For this task, we ran a small-scale simulation with n = 8 agents, and a large-scale simulation with
n = 50 agents, and used m = 20 samples in the empirical Bellman operator. We provide simulation
results in Figure 2b and Figure 2c, where Figure 2b demonstrates the exponential improvement in
computational complexity of SUBSAMPLE-MFQ, and Figure 2c demonstrates a monotonic improvement
in the cumulative rewards, consistent with Theorem 4.4. Here, both metrics outperform the mean-field
value iteration benchmark.

C

Notation and Basic Lemmas

For convenience, we restate below the various Bellman operators under consideration.
Definition C.1 (Bellman Operator T ).
T Qt (s, a) := r(s, a) + γE

max
Qt (s′ , a′ )
s′g ∼Pg (·|sg ,ag ),
′ ∈A
a
s′i ∼Pl (·|si ,sg ,ai ),∀i∈[n]

(15)

Definition C.2 (Adapted Bellman Operator T̂k ). The adapted Bellman operator updates a smaller Q
function (which we denote by Q̂k ), for a surrogate system with the global agent and k ∈ [n] local
agents denoted by ∆, using mean-field value iteration and j ∈ ∆ such that:
′
max Q̂tk (s′g , s′j , Fz∆\j
, a′j , a′g )

T̂k Q̂tk (sg , sj , Fz∆\j , aj , ag ) := r∆ (s, a) + γE

s′g ∼Pg (·|sg ,ag ),
a′ ∈A
s′i ∼Pl (·|si ,sg ,ai ),∀i∈∆

(16)
Definition C.3 (Empirical Adapted Bellman Operator T̂k,m ). The empirical adapted Bellman operator
T̂k,m empirically estimates the adapted Bellman operator update using mean-field value iteration
by drawing m random samples of sg ∼ Pg (·|sg , ag ) and si ∼ Pl (·|si , sg , ai ) for i ∈ ∆, where for
ℓ ∈ [m], the ℓ’th random sample is given by sℓg and sℓ∆ , and j ∈ ∆:
γ X
max Q̂tk,m (sℓg , sℓj , Fzℓ , aℓj , aℓg ) (17)
T̂k,m Q̂tk,m (sg , sj , Fz∆\j , aj , ag ) := r∆ (s, a) +
∆\j
a′ ∈A
m
ℓ∈[m]

Lemma C.4. For any ∆ ⊆ [n] such that |∆| = k, suppose 0 ≤ r∆ (s, a) ≤ r̃. Then, for all t ∈ N,
r̃
Q̂tk ≤ 1−γ
.
Proof. The proof follows by induction on t. The base case follows from Q̂0k := 0. For the induction,
r̃
r̃
t
note that by the triangle inequality ∥Q̂t+1
k ∥∞ ≤ ∥r∆ ∥∞ + γ∥Q̂k ∥∞ ≤ r̃ + γ 1−γ = 1−γ .
√
Remark C.5. By the law of large numbers, limm→∞ T̂k,m = T̂k , where the error decays in O(1/ m)
by the Chernoff bound. Also, T̂n := T . Further, Theorem C.4 is independent of the choice of k.
Therefore, for k = n, this implies an identical bound on Qt . An identical argument implies the same
bound on Q̂tk,m .
T satisfies a γ-contractive property under the infinity norm [Watkins and Dayan, 1992]. We similarly
show that T̂k and T̂k,m satisfy a γ-contractive property under infinity norm in Theorems C.6 and C.7.
Lemma C.6. T̂k satisfies the γ-contractive property under infinity norm:
∥T̂k Q̂′k − T̂k Q̂k ∥∞ ≤ γ∥Q̂′k − Q̂k ∥∞
19

(18)

Proof. Suppose we apply T̂k to Q̂k (sg , Fz∆ , ag ) and Q̂′k (sg , Fz∆ , ag ) for |∆| = k. Then:
∥T̂k Q̂′k − T̂k Q̂k ∥∞
=γ

max

sg ∈Sg ,
ag ∈Ag ,
Fz∆ ∈µk (Zl )

≤γ ′

′
′
′ ,a ) − E ′
′ ,a )
E s′g ∼Pg (·|sg ,ag ), max
Q̂′k (s′g , Fz∆
Q̂k (s′g , Fz∆
sg ∼Pg (·|sg ,ag ), max
g
g
′
′
s′i ∼Pl (·|si ,sg ,ai ),
∀i∈∆,

max

sg ∈Sg ,Fz∆ ∈µk (Zl ),a′ ∈A

a ∈A

s′i ∼Pl (·|si ,sg ,ai ),
∀i∈∆′

a ∈A

′
′
′
′
′ , a ) − Q̂k (s , Fz ′ , a ) = γ∥Q̂ − Q̂k ∥∞
Q̂′k (s′g , Fz∆
g
g
g
k
∆

The equality cancels common r∆ (s, a) terms in each operator. The second line uses Jensen’s
inequality, maximizes over actions, and bounds expected values with the maximizers of the random
variables.
Lemma C.7. T̂k,m satisfies the γ-contractive property under infinity norm.
Proof. Similarly to Theorem C.6, suppose we apply T̂k,m to Q̂k,m (sg , Fz∆ , ag ) and
Q̂′k,m (sg , Fz∆ , ag ). Then:
∥T̂k,m Q̂k − T̂k,m Q̂′k ∥∞ =

γ
m

≤γ

X 
ℓ∈[m]


′
ℓ
′
ℓ
′
Q̂
(s
,
F
,
a
)
Q̂
(s
,
F
,
a
)
−
max
max
ℓ
ℓ
k g
z∆
z∆
k g
g
g
′
′
a ∈A

a ∈A

∞
′
′
′
′
′ , a ) − Q̂ (s , Fz ′ , a )|
|Q̂k (s′g , Fz∆
g
k g
g
∆
k

max

a′g ∈Ag ,s′g ∈Sg ,z∆ ∈Zl

= γ∥Q̂k − Q̂′k ∥∞
The first inequality uses the triangle inequality and the general property | maxa∈A f (a) −
maxb∈A f (b)| ≤ maxc∈A |f (a) − f (b)|. The last line recovers the definition of infinity norm.
Remark C.8. The γ-contractivity of T̂k and T̂k,m attracts the trajectory between two Q̂k and
Q̂k,m functions on the same state-action tuple by γ at each step. Repeatedly applying the Bellman
operators produces a unique fixed-point from the Banach fixed-point theorem which we introduce in
Theorems C.9 and C.10.
t
Definition C.9 (Q̂∗k -function). Suppose Q̂0k := 0 and let Q̂t+1
k (sg , Fz∆ , ag ) = T̂k Q̂k (sg , Fz∆ , ag )
∗
∗
∗
for t ∈ N. Denote the fixed-point of T̂k by Q̂k such that T̂k Q̂k (sg , Fz∆ , ag ) = Q̂k (sg , Fz∆ , ag ).
t+1
0
t
Definition C.10 (Q̂est
k,m -function). Let Q̂k,m := 0 and Q̂k,m (sg , Fz∆ , ag ) = T̂k,m Q̂k,m (sg , Fz∆ , ag )
for t ∈ N. Then, the Banach-Cacciopoli fixed-point of the adapted Bellman operator T̂k,m is Q̂est
k,m
est
such that T̂k,m Q̂est
k,m (sg , Fz∆ , ag ) = Q̂k,m (sg , Fz∆ , ag ).

Corollary C.11. Observe that by recursively using the γ-contractive property for T time steps:
∥Q̂∗k − Q̂Tk ∥∞ ≤ γ T · ∥Q̂∗k − Q̂0k ∥∞

(19)

T
T
est
0
∥Q̂est
k,m − Q̂k,m ∥∞ ≤ γ · ∥Q̂k,m − Q̂k,m ∥∞

(20)

r̃
r̃
Further, noting that Q̂0k = Q̂0k,m := 0, ∥Q̂∗k ∥∞ ≤ 1−γ
, and ∥Q̂est
k,m ∥∞ ≤ 1−γ from Theorem C.4:

∥Q̂∗k − Q̂Tk ∥∞ ≤ γ T

r̃
,
1−γ

(21)

r̃
1−γ

(22)

T
T
∥Q̂est
k,m − Q̂k,m ∥∞ ≤ γ

Remark C.12. Theorem C.11 characterizes the error decay between Q̂Tk and Q̂∗k and shows that it
decays exponentially in the number of Bellman iterations by a γ T multiplicative factor.
Furthermore, we characterize the maximal policies greedy policies obtained from Q∗ , Q̂∗k , and Q̂est
k,m .
20

Definition C.13 (Optimal policy π ∗ ). The greedy policy derived from Q∗ is
π ∗ (s) := arg max Q∗ (s, a).

(23)

a∈A

Definition C.14 (Optimal subsampled policy π̂k∗ ). The greedy policy from Q̂∗k is
π̂k∗ (sg , si , Fs∆\i ) :=

arg max
(ag ,ai ,Fa∆\i )∈Ag ×Al ×µk−1 (Al )

Q̂∗k (sg , si , Fz∆\i , ai , ag ).

(24)

est
Definition C.15 (Optimal empirically subsampled policy π̂k,m
). The greedy policy from Q̂est
k,m is
given by
est
π̂k,m
(sg , Fs∆ ) :=

arg max
(ag ,ai ,Fa∆\i )∈Ag ×Al ×µk−1 (Al )

Q̂est
k,m (sg , si , Fz∆\i , ai , ag ).

(25)

Figure 4 details the analytic flow on how we use the empirical adapted Bellman operator to perform
∗
value iteration on Q̂k,m to get Q̂est
k,m which approximates Q .
Algorithm 3 gives a stable implementation of Algorithm 1 with learning rates {ηt }t∈[T ] . Algorithm 3
is provably numerical stable under fixed-point arithmetic [Anand et al., 2024, 2025]. Q̂tk,m in
Algorithm 3 is γ-contractive as in Theorem C.6, given an appropriately conditioned sequence of
learning rates ηt :
Algorithm 3 Stable (Practical) Implementation of Algorithm 1: SUBSAMPLE-Q: Learning
Require: A multi-agent system as described in Section 2. Parameter T for the number of iterations
in the initial value iteration step. Hyperparameter k ∈ [n]. Discount parameter γ ∈ (0, 1). Oracle
O to sample s′g ∼ Pg (·|sg , ag ) and si ∼ Pl (·|si , sg , ai ) for all i ∈ [n]. Learning rate sequence
{ηt }t∈[T ] where ηt ∈ (0, 1].
1: Let ∆ = [k].
2: for (sg , s1 , Fz∆\1 ) ∈ Sg × Sl × µk−1 (Zl ) do
3:
for (ag , a1 ) ∈ Ag × Al do
4:
Set Q̂0k,m (sg , s1 , Fz∆\1 , a1 , ag ) = 0
5: for t = 1 to T do
6:
for (sg , s1 , Fz∆\1 ) ∈ Sg × Sl × µk−1 (Zl ) do
7:
for (ag , a1 ) ∈ Ag × Al do
8:
t
Q̂t+1
k,m (sg , s1 , Fz∆\1 , ag , a1 ) = (1 − ηt )Q̂k,m (sg , s1 , Fz∆\1 , ag , a1 )

+ηt T̂k,m Q̂tk,m (sg , s1 , Fz∆\1 , ag , a1 )
9: Let the approximate policy be
T
π̂k,m
(sg , s1 , Fs∆\1 ) =

arg max
(ag ,a1 ,a∆\1 )∈Ag ×Al ×µk−1 (Al )

Q̂Tk,m (sg , s1 , Fz∆\1 , ag , a1 ).

PT
PT
Theorem C.16. As T → ∞, if t=1 ηt = ∞, and t=1 ηt2 < ∞, then Q-learning converges to the
optimal Q function asymptotically with probability 1.
Furthermore, finite-time guarantees with the learning rate and sample complexity have been shown in
Chen and Maguluri [2022b], which when adapted to our Q̂k,m framework in Algorithm 3 yields:
Theorem C.17 (Chen and Maguluri [2022b]). For all t = {1, . . . , T } and for any ϵ > 0, if the
learning rate sequence ηt satisfies ηt = (1 − γ)4 ϵ2 and T =

|Sl ||Al |k|Sl ||Al | |Sg ||Ag | 2
ϵ , then
(1−γ)5

∥Q̂Tk,m − Q̂est
k,m ∥ ≤ ϵ.
Definition C.18 (Star Graph Sn ). For n ∈ N, the star graph Sn is the complete bipartite graph K1,n .
21

Sn captures the graph density notion by saturating the set of neighbors of the central node. Such
settings find applications beyond RL as well [Chaudhari et al., 2024, Anand and Qu, 2024, Li et al.,
2019]. The cardinality of the search space simplex for the optimal policy is exponential in n, so
it cannot be naively modeled by an MDP: we need to exploit the symmetry of the local agents.
This intuition allows our subsampling algorithm to run in polylogarithmic time (in n). Some works
leverage an exponential decaying property that truncates the search space for policies over immediate
neighborhoods of agents; however, this still relies on the assumption that the graph neighborhood for
the agent is sparse [Qu et al., 2020a,b]; however, Sn is not locally sparse; hence, previous methods
do not apply to this problem instance.

0
1

...

2

n

3
Figure 3: Star graph Sn
Finally, we argue why mean-field value iteration faithfully performs the same update as value iteration.
Lemma C.19 (Equivalence of Mean-Field Value Iteration and Value Iteration). Since the local agents
1, . . . , n are all homogeneous in their state/action spaces, the Q̂k -function only depends on them
through their empirical distribution Fz∆ , proving the lemma. Therefore, for the remainder of the paper,
we will use Q̂k (sg , Fz∆ , ag ) := Q̂k (sg , si , Fz∆\i , ag , ai ) := Q̂k (sg , s∆ , ag , a∆ ) interchangeably,
unless making a remark about the computational complexity of learning each function.

D

Proof Sketch

This section details an outline for the proof of Theorem 4.4, as well as some key ideas. At a high
level, our SUBSAMPLE-MFQ framework recovers exact mean-field Q learning and traditional value
iteration when k = n and as m → ∞. Further, as k → n, Q̂∗k should intuitively get closer to Q∗ from
which the optimal policy is derived. Thus, the proof is divided into three major steps: firstly, we
prove a Lipschitz continuity bound between Q̂∗k and Q̂∗n in terms of the total variation (TV) distance
between Fz∆ and Fz[n] . Next, we bound the TV distance between Fz∆ and Fz[n] . Finally, we bound
the value differences between πkest and π ∗ by bounding Q∗ (s, π ∗ (s)) − Q∗ (s, πkest (s)) and then using
the performance difference lemma from Kakade and Langford [2002].
Step 1: Lipschitz Continuity Bound. To compare Q̂∗k (sg , Fs∆ , ag ) with Q∗ (s, ag ), we prove a
Lipschitz continuity bound between Q̂∗k (sg , Fs∆ , ag ) and Q̂∗k′ (sg , Fs∆′ , ag ) with respect to the TV
distance measure between s∆ ∈ s[n]
and s∆′ ∈ sk[n]′ :
k


[n]
′
Theorem D.1 (Lipschitz continuity in Q̂∗k ). For all (s, a) ∈ S × A, ∆ ∈ [n]
k and ∆ ∈ k′ ,
|Q̂∗k (sg , Fz∆ , ag ) − Q̂∗k′ (sg , Fz∆′ , ag )| ≤


2
∥rl (·, ·)∥∞ · TV Fz∆ , Fz∆′
1−γ

We defer the proof of Theorem D.1 to Appendix E. See Figure 4 for a comparison between the Q̂∗k
learning and estimation process, and the exact Q-learning framework.
Step 2: Bounding Total Variation (TV) Distance. We bound the TV distance between Fz∆ and

Fz[n] , where ∆ ∈ U [n]
k . This task is equivalent to bounding the discrepancy between the empirical
distribution and the distribution of the underlying finite population. When each i ∈ ∆ is uniformly
sampled without replacement, we use Theorem F.3 from Anand and Qu [2024] which generalizes the
Dvoretzky-Kiefer-Wolfowitz (DKW) concentration inequality for empirical distribution functions.
Using this, we show:
22

Q∗ (sg , s[n] , ag , a[n] )

Q̂0k,m (sg , Fz∆ , ag )

(4)
=

(1)

Q̂est
k,m (sg , Fz∆ , ag )

(2)

Q̂∗k (sg , Fz∆ , ag )

(3)
≈

Q̂∗n (sg , Fz[n] , ag )

Figure 4: Flow of the algorithm and relevant analyses in learning Q∗ . Here, (1) follows by performing
Algorithm 1 (SUBSAMPLE-MFQ: Learning) on Q̂0k,m . (2) follows from Theorem 4.3. (3) follows from
the Lipschitz continuity and total variation distance bounds in Theorems D.1 and D.2. Finally, (4)
follows from noting that Q̂∗n = Q∗ .
Theorem D.2. Given a finite population Z = (z1 , . . . , zn ) for Z ∈ Zln , let ∆ ⊆ [n] be a uniformly
random sample from Z of size k chosen without replacement. Fix ϵ > 0. Then, for all x ∈ Zl :


2knϵ2
1 X
1 X
Pr sup
1{zi = x} −
1{zi = x} ≤ ϵ ≥ 1 − 2|Zl |e− n−k+1 .
n
x∈Zl |∆|
i∈∆

i∈[n]

Then, by Theorem D.2 and the definition of total variation distance from Section 2, we have that for
δ ∈ (0, 1], with probability at least 1 − δ,
r
n − k + 1 2|Zl |
ln
(26)
TV(Fs∆ , Fs[n] ) ≤
8nk
δ
We then apply this result to our MARL setting by studying the rate of decay of the objective function
between the learned policy πkest and the optimal policy π ∗ (Theorem 4.4).
Step 3: Performance Difference Lemma to Complete the Proof. As a consequence of the prior two
′
steps and Lemma 4.3, Q∗ (s, a′ ) and Q̂est
k,m (sg , Fz∆ , ag ) become similar as k → n. We further prove
∗
est
that the value generated by the policies π and πk must also be very close (where the residue shrinks
as k → n). We then use the well-known performance difference lemma [Kakade and Langford, 2002]
which we restate in Appendix G.1. A crucial theorem needed to use the performance difference
lemma is a bound on Q∗ (s′ , π ∗ (s′ )) − Q∗ (s′ , π̂kest (s′g , Fs′∆ )).
Therefore, we formulate and prove Theorem D.3 which
 yields a probabilistic bound on this difference,
[n]
where the randomness is over the choice of ∆ ∈ k :
Theorem D.3. For a fixed s′ ∈ S := Sg × Sln and for δ ∈ (0, 1], with probability at least
1 − 2|Ag |k |Al | δ:
s


2|Zl |
2∥r
(·,
·)∥
n−k+1
l
∞
∗ ′
∗ ′
∗ ′
est
′
Q (s , π (s )) − Q (s , π̂k,m (sg , Fs′∆ )) ≤
ln
+ 2ϵk,m .
1−γ
2nk
δ
We defer the proof of Theorem D.3 and finding optimal value of δ to Theorem G.8 in the Appendix.
Using Theorem D.3 and the performance difference lemma leads to Theorem 4.4.

E

Proof of Lipschitz-continuity Bound

This section proves a Lipschitz-continuity bound between Q̂∗k and Q∗ and includes a framework to
P
∗
∗
compare n1
[n] Q̂ (s , s , a ) and Q (s, ag ) in Theorem E.12.
(k ) ∆∈( k ) k g ∆ g
Let zi := (si , ai ) ∈ Zl := Sl × Al and z∆ = {zi : i ∈ ∆} ∈ Zlk . For zi = (si , ai ), let zi (s) = si ,
and zi (a) = ai . With abuse of notation, note that Q̂Tk (sg , ag , s∆ , a∆ ) is equal to Q̂Tk (sg , ag , z∆ ).
The following definitions will be relevant to the proof of Theorem E.3.
|∆|

Definition E.1 (Empirical Distribution Function). For all z ∈ Zl and (s′ , a′ ) ∈ Sl × Al , where
∆ ⊆ [n],
1 X
Fz∆ (s′ , a′ ) =
1{zi (s) = s′ , zi (a) = a′ }
|∆|
i∈∆

23

Definition E.2 (Total Variation Distance). Let P and Q be discrete probability distribution over some
domain Ω. Then,
1
TV(P, Q) = ∥P − Q∥1 = sup Pr(E) − Pr(E)
Q
2
E⊆Ω P
2
Theorem E.3 (Q̂Tk is 1−γ
∥rl (·, ·)∥∞ -Lipschitz continuous with respect to Fz∆ in total variation
distance). Suppose ∆, ∆′ ⊆ [n] such that |∆| = k and |∆′ | = k ′ . Then:
!
T
−1
X

Q̂Tk (sg , ag , Fz∆ ) − Q̂Tk′ (sg , ag , Fz∆′ ) ≤
2γ t ∥rl (·, ·, ·)∥∞ · TV Fz∆ , Fz∆′
t=0

Proof. We prove this inductively. First, note that Q̂0k (·, ·, ·) = Q̂0k′ (·, ·, ·) = 0 from the initialization
step, which proves the lemma for T = 0, since TV(·, ·) ≥ 0. At T = 1:
|Q̂1k (sg , ag , Fz∆ ) − Q̂1k′ (sg , ag , Fz∆′ )| = T̂k Q̂0k (sg , ag , Fz∆ ) − T̂k′ Q̂0k′ (sg , ag , Fz∆′ )
= r(sg , Fs∆ , ag ) + γEs′g ,s′∆ max
Q̂0k (s′g , Fs′∆ , a′g )
′
ag ∈Ag

− r(sg , Fs∆′ , ag ) − γEs′g ,s′∆′ max
Q̂0k′ (s′g , Fs′∆′ , a′g )
′
ag ∈Ag

= |r(sg , ag , Fz∆ ) − r(sg , ag , Fz∆′ )|
=

1 X
1X
rl (sg , zi ) − ′
rl (sg , zi )
k
k
′
i∈∆

i∈∆

= Ezl ∼Fz∆ rl (sg , zl ) − Ezl′ ∼Fz ′ rl (sg , zl′ )
∆

In the first and second equalities, we use the time evolution property of Q̂1k and Q̂1k′ by applying
the adapted Bellman operators T̂k and T̂k′ to Q̂0k and Q̂0k′ , respectively, and expanding. In the third
and fourth equalities, we note that Q̂0k (·, ·, ·) = Q̂0k′ (·, ·, ·) = 0, and subtract the common ‘global
component’ of the reward function.
Then, noting
P the general property that for any function f : X → Y for |X | < ∞ we can write
f (x) = y∈X f (y)1{y = x}, we have:
|Q̂1k (sg , ag , Fz∆ ) − Q̂1k′ (sg , ag , Fz∆′ )|
"
#
"
#
X
X
′
= Ezl ∼Fz∆
rl (sg , z)1{zl = z} − Ezl′ ∼Fz ′
rl (sg , z)1{zl = z}
∆

z∈Z

=

X

z∈Z

rl (sg , z) · (Ezl ∼Fz∆ 1{zl = z} − Ezl′ ∼Fz ′ 1{zl′ = z})
∆

z∈Z

=

X

rl (sg , z) · (Fz∆ (z) − Fz∆′ (z))

z∈Z

≤ max rl (sg , z)| ·
z∈Z

X

|Fz∆ (z) − Fz∆′ (z)

z∈Z

≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )
The second equality follows from the linearity of expectations, and the third equality follows by
noting that for any random variable X ∼ X , EX 1[X = x] = Pr[X = x]. The first inequality
follows from an application of the triangle inequality and the Cauchy-Schwarz inequality, and the
second inequality uses the definition of TV distance. Thus, at T = 1, Q̂ is (2∥rl (·, ·)∥∞ )-Lipschitz
continuous in TV distance, proving the base case.
24

Assume that for T ≤ t′ ∈ N:
Q̂Tk (sg , ag , Fz∆ ) − Q̂Tk′ (sg , ag , Fz∆′ )

T
−1
X

≤

!
2γ

t

∥rl (·, ·)∥∞ · TV Fz∆ , Fz∆′



t=0

Then, inductively:
|Q̂Tk +1 (sg , ag , Fz∆ ) − Q̂Tk′+1 (sg , ag , Fz∆′ )|
1 X
1 X
rl (sg , zi ) − ′
rl (sg , zi )
≤
|∆|
|∆ |
′
i∈∆

i∈∆

′ ) − Es′ ,s′
′ )
+ γ Es′g ,s′∆ max
Q̂Tk (s′g , a′g , Fz∆
max
Q̂Tk′ (s′g , a′g , Fz∆
′
g ∆′
′
′

ag ∈Ag ,
a′∆ ∈Ak
l

ag ∈Ag ,

a′∆′ ∈Ak
l

′

≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )
′ ) − E(s′ ,s′ )∼J
′ )
+ γ E(s′g ,s′∆ )∼Jk max
Q̂Tk (s′g , a′g , Fz∆
max
Q̂Tk′ (s′g , a′g , Fz∆
′
′
k′
g
′
′
∆

ag ∈Ag ,
a′∆ ∈Ak
l

ag ∈Ag ,

a′∆′ ∈Ak
l

≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ ) + γ

T
−1
X

′

!
2γ τ

∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )

τ =0

=

T
X

!
2γ

τ

∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )

τ =0

In the first inequality, we rewrite the expectations over the states as the expectation over the joint
transition probabilities. The second inequality then follows from Theorem E.14. To apply it to
Theorem E.14, we conflate the joint expectation over (sg , s∆∪∆′ ) and reduce it back to the original
form of its expectation. Finally, the third inequality follows from Theorem E.5.
By the inductive hypothesis, the claim is proven.
Definition E.4. [Joint Stochastic Kernels] The joint stochastic kernel on (sg , s∆ ) for ∆ ⊆ [n] where
|∆| = k is defined as Jk : Sg × Slk × Sg × Ag × Slk × Akl → [0, 1], where
Jk (s′g , s′∆ |sg , ag , s∆ , a∆ ) := Pr[(s′g , s′∆ )|sg , ag , s∆ , a∆ ]
(27)
Lemma E.5. For all T ∈ N, for any ag ∈ Ag , sg ∈ Sg , s∆ ∈ Slk , a∆ ∈ Akl , a′∆ ∈ Akl , and for all
joint stochastic kernels Jk as defined in Theorem E.4:
′ )
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
Q̂Tk (s′g , a′g , Fz∆
′
′

ag ,a∆

′ ) ≤
−E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂Tk′ (s′g , a′g , Fz∆
′
∆
a′g ,a∆′

T
−1
X

!
2γ

τ

∥rl (·, ·)∥∞ TV Fz∆ , Fz∆′



τ =0

Proof. We prove this inductively. At T = 0, the statement is true since Q̂0k (·, ·, ·) = Q̂0k′ (·, ·, ·) = 0
and TV(·, ·) ≥ 0.
At T = 1,
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
Q̂1k (s′g , a′g , s′∆ , a′∆ )
′
′
ag ,a∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂1k′ (s′g , a′g , s′∆′ , a′∆′ )
′
′
∆

ag ,a∆′



= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
rg (s′g , a′g ) +
′
′

′
′ ′ 
i∈∆ rl (si , ai , sg )

P

k
P

′
′ ′ 
i∈∆′ rl (si , ai , sg )
′
′
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
rg (sg , ag ) +
∆
a′g ,a′∆′
k′
ag ,a∆

25

′
′ ′
i∈∆ rl (si , ai , sg )

P
= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
′

k

a∆

′
′ ′
i∈∆′ rl (si , ai , sg )
k′

P
−E
= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ )

(s′g ,s′∆′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ )

max
′
a∆′

1X
1 X
r̃l (s′i , s′g )
r̃l (s′i , s′g ) − E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) ′
∆
k
k
′
i∈∆

i∈∆

In the last equality, we note that
X
X
X
X
max
rl (s′i , a′i , s′g ) =
max
rl (s′i , a′i , s′g ) =
max
rl (s′i , a′i , s′g ) =
r̃l (s′i , s′g ),
′
′
′
a∆

a
i∈∆
i∈∆ ∆
′
′
′ ′
where r̃l (si , sg ) := maxa′i rl (si , ai , s′g ).

i∈∆

ai

i∈∆

Then, we have:
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ )

1X
1 X
r̃l (s′i , s′g )
r̃l (s′i , s′g ) − E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) ′
∆
k
k
′
i∈∆

i∈∆

1 X
= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ )
r̃l (x, s′g )Fs′∆ (x)
k
x∈Sl

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ )
∆

1 X
r̃l (x, s′g )Fs′∆′ (x)
k′
x∈Sl

= Es′g ∼P ′
s

∆∪∆′

|∆∪∆′ | J|∆∪∆′ |(·,s′
|s ,a ,s
,a
)
∈S
∆∪∆′ g g ∆∪∆′ ∆∪∆′
l

X

r˜l (x, s′g )Es′

∆∪∆′

≤ ∥r˜l (·, ·)∥∞ · Es′g ∼

∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) [Fs′∆ (x) − Fs′∆′ (x)]

x∈Sl
P

|∆∪∆′ | J|∆∪∆′ |(·,s′
|s ,a ,s
,a
)
s′
∈S
∆∪∆′ g g ∆∪∆′ ∆∪∆′
l
∆∪∆′

X

|Es′

∆∪∆′

|s′g Fs′∆ (x) − Es′∆∪∆′ |s′g Fs′∆′ (x)|

x∈Sl

≤ ∥rl (·, ·)∥∞ · Es′g ∼P ′
s

|∆∪∆′ | J|∆∪∆′ |(·,s′

∆∪∆′

∈S
l

∆∪∆′

|sg ,ag ,s

X

∆∪∆′

|Es′

)
,a
∆∪∆′

∆∪∆′

|s′g Fs′∆ (x) − Es′∆∪∆′ |s′g Fs′∆′ (x)|

x∈Sl

≤ 2∥rl (·, ·)∥∞ · Es′g ∼P ′
s

∆∪∆′

|∆∪∆′ | J|∆∪∆′ |(·,s′
,a
)
|s ,a ,s
∈S
∆∪∆′ g g ∆∪∆′ ∆∪∆′
l

TV(Es′

∆∪∆′

|s′g Fs′∆ , Es′∆∪∆′ |s′g Fs′∆′ )

′ )
≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆

P
The first equality follows from noting that f (x) = x′ ∈X f (x′ )1{x = x′ } and from Fubini-Tonelli’s
inequality which allows us to swap the order of summations as the summand is finite. The second
equality uses the law of total expectation. The first inequality uses Jensen’s inequality and the
triangle inequality. The second inequality uses ∥r̃l (·, ·)∥∞ ≤ ∥rl (·, ·)∥∞ which holds as ∥rl ∥∞ is
the infinite-norm of the local reward functions and is therefore atleast as large any other element in
the image of rl . The third inequality follows from the definition of total variation distance, and the
final inequality follows from Theorem E.7. This proves the base case.
Then, assume that for T ≤ t′ ∈ N, for all joint stochastic kernels Jk and Jk′ , and for all a′g ∈
Ag , a′∆ ∈ Akl :
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
Q̂Tk (s′g , a′g , s′∆ , a′∆ )−
′
′
ag ,a∆

E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂Tk′ (s′g , a′g , s′∆′ , a′∆′ )
∆
a′g ,a′∆′
26

≤2

T
−1
X

γ

t=0

!
t

∥rl (·, ·)∥∞ TV(Fz∆ , Fz∆′ )

For the remainder of the proof, we adopt the shorthand E(s′g ,s′∆ )∼J to denote
E(s′g ,s′∆ )∼J|∆| (·,·|sg ,ag ,s∆ ,a∆ ) , and E(s′′g ,s′′∆ )∼J to denote E(s′′g ,s′′∆ )∼J|∆| (·,·|s′g ,a′g ,s′∆ ,a′∆ ) .
Then, inductively, we have:
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
Q̂Tk +1 (s′g , a′g , s′∆ , a′∆ )
′
′
ag ,a∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂Tk′+1 (s′g , a′g , s′∆′ , a′∆′ ) = (I) - (II),
′
′
∆

ag ,a∆′

where

(I) = E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
r∆ (s′g , a′g , s′∆ , a′∆ )
′
′
ag ,a∆


T ′′ ′′
′′ ′′
+ γE(s′′g ,s′′∆ )∼Jk (·,·|s′g ,a′g ,s′∆ ,a′∆ ) max
Q̂
(s
,
s
,
a
,
a
)
k
g ∆ g
∆
′′ ′′
ag ,a∆

and
(II) = E

(s′g ,s′∆′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ )


max
r∆′ (s′g , a′g , s′∆′ , a′∆′ )
′
′

ag ,a∆′



+ γE(s′′g ,s′′ ′ )∼Jk′ (·,·|s′g ,a′g ,s′ ′ ,a′ ′ ) max
Q̂Tk′ (s′′g , s′′∆′ , a′′g , a′′∆′ )
′′
∆
∆
∆
a′′
g ,a∆′
Let
ãg , ã∆
=

arg max
a′g ∈Ag ,a′∆ ∈Ak
l



T ′′ ′′
′′ ′′
Q̂
(s
,
s
,
a
,
a
)
r∆ (s′g , a′g , s′∆ , a′∆ ) + γE(s′′g ,s′′∆ )∼Jk (·,·|s′g ,a′g ,s′∆ ,a′∆ ) max
k
g ∆ g
∆ ,
′′ ′′
ag ,a∆

and
âg , â∆′
=

arg max
a′g ∈Ag ,a′∆′ ∈Ak
l


r∆′ (s′g , a′g , s′∆′ , a′∆′ )+γ
|∆∪∆′ |

Then, define ã∆∪∆′ ∈ Al

′′
′
′
′
a ,a∆′
(s′′
g ,s∆′ )∼Jk (·,·|sg ,ag ,s∆′ ,a∆′ ) g

by

ãi =

|∆∪∆′ |

Similarly, define â∆∪∆′ ∈ Al

max
Q̂Tk (s′′g , s′′∆′ , a′′g , a′′∆′ )
′′ ′′

E′

ãi ,
âi ,

if i ∈ ∆,
if i ∈ ∆′ \ ∆



i ∈ ∆′
i ∈ ∆ \ ∆′

by
âi =

âi ,
ãi ,

Suppose (I) ≥ (II). Then,
Q̂Tk +1 (s′g , a′g , s′∆ , a′∆ )
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) max
′
′
ag ,a∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂Tk′+1 (s′g , a′g , s′∆′ , a′∆′ )
′
′
∆

ag ,a∆′

27



= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) Q̂Tk +1 (s′g , ãg , s′∆ , ã∆ )
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) Q̂Tk′+1 (s′g , âg , s′∆′ , â∆′ )
∆

≤ E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) Q̂Tk +1 (s′g , ãg , s′∆ , ã∆ )
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) Q̂Tk′+1 (s′g , ãg , s′∆′ , ã∆′ )
∆


= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) r∆ (s′g , s′∆ , ãg , ã∆ )


Q̂Tk (s′′g , a′′g , s′′∆ , a′′∆ )
+ γE(s′′g ,s′′∆ )∼Jk (·,·|s′g ,ãg ,s′∆ ,ã∆ ) max
′′
a′′
g ,a∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) r∆′ (s′g , s′∆′ , ãg , ã∆′ )
∆


T
′′ ′′ ′′
′′
+ γE(s′′g ,s′′ ′ )∼Jk′ (·,·|s′g ,ãg ,s′ ′ ,ã∆′ ) max
Q̂
(s
,
a
,
s
,
sa
)
′
′
′
k
g
g ∆
∆
′′ ′′
∆

∆

ag ,a∆′

≤ E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) r∆ (s′g , s′∆ , ãg , ã∆ )
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) r∆′ (s′g , s′∆′ , ãg , ã∆′ )
∆

Q̂Tk (s′′g , a′′g , s′′∆ , a′′∆ )
+ γ E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) E(s′′g ,s′′∆ )∼Jk (·,·|s′g ,ãg ,s′∆ ,ã∆ ) max
′′ ′′
ag ,a∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) E(s′′g ,s′′ ′ )∼Jk′ (·,·|s′g ,ãg ,s′ ′ ,ã∆′ ) max
Q̂Tk′ (s′′g , a′′g , s′′∆′ , a′′∆′ )
′′ ′′
∆

∆

= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ )

1X
k

∆

ag ,a∆′

rl (s′i , s′g , ãi )

i∈∆

− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ )
∆

1 X
rl (s′i , s′g , ãi )
k′
′
i∈∆

Q̂Tk (s′′g , a′′g , s′′∆ , a′′∆ )
+ γ E(s′′ ,s′′ )∼J˜k (·,·|sg ,ag ,s∆ ,a∆ ) max
′′ ′′
g

∆

ag ,a∆

− E(s′′ ,s′ ′′ )∼J˜k′ (·,·|sg ,ag ,s∆′ ,a∆′ ) max
Q̂Tk′ (s′′g , a′′g , s′′∆′ , a′′∆′ )
′′ ′′
g

≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ ) + γ

ag ,a∆′

∆

T
X

!
2γ t

∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )

t=0

=

T
+1
X

!
2γ

t

∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )

t=0

The first equality rewrites the equations with their respective maximizing actions. The first
inequality upper-bounds this difference by allowing all terms to share the common action
ã. Using the Bellman equation, the second equality expands Q̂Tk +1 and Q̂Tk′+1 . The second inequality follows from the triangle inequality. The third equality follows by subtracting the common rg (s′g , a˜g ) terms from the reward and noting that the two expectation terms
Es′g ,s′∆ ∼ Jk (·, ·|sg , ag , s∆ , a∆ )Es′′g ,s∆′′ ∼Jk (·,·|sg ,ãg ,s∆ ,ã∆ ) can be combined into a single expectation Es′′ ,s′′ ∼J˜k (·,·|sg ,ag ,s∆ ,a∆ ) .
g

∆

In the special case where a∆ = ã∆ , we derive a closed form expression for J˜k in Theorem E.11. To
justify the third inequality, the second term follows from the induction hypothesis and the first term
28

follows from the following derivation:
P
P
′ ′
rl (s′i , s′g , ãi )
′ rl (si , sg , ãi )
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) i∈∆
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) i∈∆
′
∆
k
k
P
P
ã ′ ′
ã ′ ′
r (si , sg )
′ r (si , sg )
= E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) i∈∆ l
− E(s′g ,s′ ′ )∼Jk′ (·,·|sg ,ag ,s∆′ ,a∆′ ) i∈∆ l′
∆
k
k
≤ ∥rlã ∥∞ Es′g ∼P ′
s

∆∪∆′

′
|∆∪∆′ | J|∆∪∆′ | (·,s∆∪∆′ |sg ,ag ,s∆∪∆′ ,a∆∪∆′ )
∈S
l

TV(Es′

∆∪∆′

|s′g Fs∆ , Es′∆∪∆′ |s′g Fs∆′ )

≤ 2∥rl (·, ·)∥∞ · TV(Fz∆ , Fz∆′ )
The above derivation follows from the same argument as in the base case where rlã (s′i , s′g ) :=
rl (s′i , s′g , ãi ) for any i ∈ ∆ ∪ ∆′ . Similarly, if (I) < (II), an analogous argument that replaces ã∆
with â∆ yields the same result.
Therefore, by the induction hypothesis, the claim is proven.
Remark E.6. Given a joint transition probability function J|∆∪∆′ | as defined in Theorem E.4, we
can recover the transition function for a single agent i ∈ ∆ ∪ ∆′ given by J1 using the law of total
probability and the conditional independence between si and sg ∪ s[n]\i in Equation (28). This
characterization is crucial in Theorem E.7 and Theorem E.5:
X
J1 (·|s′g , sg , ag , si , ai ) =
J|∆∪∆′ | (s′∆∪∆′ \i ◦ s′i |s′g , sg , ag , s∆∪∆′ , a∆∪∆′ ) (28)
|∆∪∆′ |−1

s′∆∪∆′ \i ∼Sl

Here, we use a conditional independence property proven in Theorem E.10.
Lemma E.7. The TV-distance between the next-step expected empirical distribution functions is
bounded by the TV-distance between the existing empirical distribution functions.


TV Es′ ′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆ , Es′ ′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆′
∆∪∆

∆∪∆

≤ TV(Fz∆ , Fz∆′ )
Proof. From the definition of total variation distance, we have:


′
′
′
′
′
′
′
′
TV Es
Fs∆ , Es
Fs∆′
′ |sg ∼J|∆∪∆′ | (·|sg ,sg ,ag ,s∆∪∆′ ,a∆∪∆′
′ |sg ∼J|∆∪∆′ | (·|sg ,sg ,ag ,s∆∪∆′ ,a∆∪∆′
∆∪∆

=

1 X
2

∆∪∆

Es′

∆∪∆′

|s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆ (x)

x∈Sl

− Es′

∆∪∆′

=

|s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆′ (x)

1 X 1X
1 X
J1 (x|s′g , sg , ag , si , ai ) − ′
J1 (x|s′g , sg , ag , si , ai )
2
k
k
′
x∈Sl

i∈∆

i∈∆

1 X 1 X X X
l
=
J1 (x|s′g , sg , ag , si , ai )1 {asii =a
=sl }
2
k
x∈Sl

al ∈Al sl ∈Sl i∈∆

−

1 X X X
l
J1 (x|s′g , sg , ag , si , ai )1 {asii =a
=sl }
k′
′
al ∈Al sl ∈Sl i∈∆

1 X 1 X X X
=
J1 (x|s′g , sg , ag , ·, ·)⃗1{ai =al }
si =sl
2
k
x∈Sl

al ∈Al sl ∈Sl i∈∆

−

1 X X X
J1 (x|s′g , sg , ag , ·, ·)⃗1{ai =al }
si =sl
k′
′
al ∈Al sl ∈Sl i∈∆

29

The second equality uses Theorem E.9. The third equality uses the property that each local agent can
only have one state/action pair, and the fourth equality vectorizes the indicator variables. Then,


TV Es′ ′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ Fs′∆ , Es′ ′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ Fs′∆′
∆∪∆

≤

∆∪∆

1 X X X 1X
1 X
J1 (x|s′g , sg , ag , ·, ·)⃗1{ai =al }
J1 (x|s′g , sg , ag , ·, ·)⃗1{ai =al } − ′
si =sl
si =sl
2
k
k
′
x∈Sl al ∈Al sl ∈Sl

=

≤

=

1X

2

k

al ∈Al sl ∈Sl

1X

2

k

i∈∆

1 X X

1X

2

k

1
=
2
1
=
2

al ∈Al sl ∈Sl

X
(sl ,al )∈Sl ×Al

X

1 X

J1 (·|s′g , sg , ag , ·, ·)⃗1{ai =al } −

k′

si =sl

i∈∆

1 X X
al ∈Al sl ∈Sl

i∈∆

i∈∆

1 X X

1 X

⃗1 ai =al −
{ si =sl } k ′
=al
1{asii =s
l} −

i∈∆

i∈∆′

1 X
k′

⃗1 ai =al
{ si =sl }

J1 (·|s′g , sg , ag , ·, ·)⃗1{ai =al }

i∈∆′

si =sl

1

· ∥J1 (·|s′g , sg , ag , ·, ·)∥1
1

l
1{asii =a
=sl }

i∈∆′

1

1 X ai =al
1 X ai =al
1{ si =sl } − ′
1{ si =sl }
k
k
′
i∈∆

i∈∆

Fz∆ (zl ) − Fz∆′ (zl )

zl ∈Sl ×Al

:= TV(Fz∆ , Fz∆′ )
The first inequality uses the triangle inequality. The second inequality and fifth equality follow from
Hölder’s inequality and the sum of the probabilities from the stochastic transition function J1 being
equal to 1. The sixth equality uses Fubini-Tonelli’s theorem which applies as the total variation
distance measure is bounded from above by 1. The final equality recovers the total variation distance
for the variable z = (s, a) ∈ Sl × Al across agents ∆ and ∆′ , which proves the claim.
Next, recall the data processing inequality.
Lemma E.8 (Data Processing Inequality.). Let A and B be random variables over some domain S.
Let f be some function (not necessarily deterministically) mapping from S to any codomain T . Then,
every f -divergence χ satisfies
χ(f (A)∥f (B)) ≤ χ(A∥B)
Remark: We show an analog of the data processing inequality. Under this lens, Theorem E.7
saturates the data-processing relation for the TV distance in our multi-agent setting.
Lemma E.9.
|s′ ∼J|∆∪∆′ | (·|sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆ (x) =
∆∪∆′ g

Es′

1X
J1 (x|s′g , sg , ag , si , ai )
k
i∈∆

Proof. By expanding on the definition of the empirical distribution function Fs∆′ (x) =
P
1
i∈∆ 1{si = x}, we have:
|∆|
Es∆∪∆′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) Fs′∆ (x)
1X
=
Es∆∪∆′ |s′g ∼J|∆∪∆′ | (·|s′g ,sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) 1{s′i = x}
k
i∈∆
1X
=
Es∆∪∆′ |s′g ∼J1 (·|s′g ,sg ,ag ,si ,ai ) 1{s′i = x}
k
i∈∆
1X
=
J1 (x|s′g , sg , ag , si , ai )
k
i∈∆

30

The second equality follows from the conditional independence of s′i from s∆∪∆′ \i from Theorem E.10, and the final equality uses the fact that the expectation of an indicator random variable is
the probability distribution function of the random variable.
Lemma E.10. The distribution s′∆∪∆′ \i |s′g , sg , ag , s∆∪∆′ , a∆∪∆′ is conditionally independent to
the distribution s′i |s′g , sg , ag , si , ai for any i ∈ ∆ ∪ ∆′ .
Proof. We direct the interested reader to the Bayes-Ball theorem in Shachter [2013] for proving
conditional independence. For ease of exposition, we restate the two rules in Shachter [2013] that
introduce the notion of d-separations which implies conditional independence. Suppose we have a
causal graph G = (V, E) where the vertex set V = [p] for p ∈ N is a set of variables and the edge set
E ⊆ 2V denotes dependence through connectivity. Then, the two rules that establish d-separations
are as follows:
1. For x, y ∈ V , we say that x, y are d-connected if there exists a path (x, . . . , y) that can be
traced without traversing a pair of arrows that point at the same vertex.
2. We say that x, y ∈ V are d-connected, conditioned on a set of variables Z ⊆ V , if there is a
path (x, . . . , y) that does not contain an event z ∈ Z that can be traced without traversing a
pair of arrows that point at the same vertex.
If x, y ∈ V is not d-connected through any such path, then x, y is d-separated, which implies
conditional independence.
Let Z = {s′g , sg , ag , si , ai } be the set of variables we condition on. Then, the below figure demonstrates the causal graph for the events of interest.

ag

s∆∪∆′ \i

sg

si

a∆∪∆′ \i

ai

a′g

s′∆∪∆′ \i

s′g

s′i

a′∆∪∆′ \i

a′i

Figure 5: Causal graph to demonstrate the dependencies between variables.
1. Observe that all paths (through undirected edges) stemming from s′∆∪∆′ \i → s∆∪∆′ \i pass
through si ∈ Z which is blocked.
2. All other paths from s′∆∪∆′ \i to s′i pass through sg ∪ s′g ∈ Z.
Therefore, s′∆∪∆′ \i and s′i are d-separated by Z. Hence, by Shachter [2013], s′∆∪∆′ \i and s′i are
conditionally independent.
Lemma E.11. For any joint transition probability function Jk on sg ∈ Sg , s∆ ∈ Slk , a∆ ∈ Akl
|∆|
|∆|
|∆|
where |∆| = k, given by Jk : Sg × Sl × Sg × Ag × Sl × Al → [0, 1], we have:
"
#
E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) E(s′′g ,s′′∆ )∼Jk (·,·|s′g ,ag ,s′∆ ,a∆ )

max

′′
k
a′′
g ∈Ag ,a∆ ∈Al

= E(s′′g ,s′′∆ )∼Jk2 (·,·|sg ,ag ,s∆ ,a∆ ) max
Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )
′′
ag ∈Ag

31

Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )

Proof. By expanding the expectations:
"

#

E(s′g ,s′∆ )∼Jk (·,·|sg ,ag ,s∆ ,a∆ ) E(s′′g ,s′′∆ )∼Jk (·,·|s′g ,ag ,s′∆ ,a∆ )
X

X

=

max

Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )
k

′′
a′′
g ∈Ag ,a∆ ∈Al

Jk [s′g , s′∆ , sg , ag , s∆ , a∆ ]Jk [s′′g , s′′∆ , s′g , ag , s′∆ , a∆ ]

|∆|
|∆|
′′
(s′g ,s′∆ )∈Sg ×Sl (s′′
g ,s∆ )∈Sg ×Sl

max

′′
k
a′′
g ∈Ag ,a∆ ∈Al

X

=

Jk2 [s′′g , s′′∆ , sg , ag , s∆ , a∆ ]
|∆|

′′
(s′′
g ,s∆ )∈Sg ×Sl

= E(s′′g ,s′′∆ )∼Jk2 (·,·|sg ,ag ,s∆ ,a∆ )

max

k
′′
a′′
g ∈Ag ,a∆ ∈Al

max

′′
k
a′′
g ∈Ag ,a∆ ∈Al

Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )

Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )

Q̂Tk (s′′g , s′′∆ , a′′g , a′′∆ )

In the second equality, the right-stochasticity of Jk implies the right-stochasticity of Jk2 .
Further, observe that Jk [s′g , s′∆ , sg , ag , s∆ , a∆ ]Jk [s′′g , s′′∆ , s′g , ag , s′∆ , a∆ ] denotes the probability of
the transitions (sg , s∆ ) → (s′g , s′∆ ) → (s′′g , s′′∆ ) with actions ag , a∆ at each step, where the joint
state evolution is governed by Jk . Thus,
X
Jk [s′g , s′∆ , sg , ag , s∆ , a∆ ]Jk [s′′g , s′′∆ , s′g , ag , s′g , a∆ ] = Jk2 [s′′g , s′′∆ , sg , ag , s∆ , a∆ ]
|∆|

(s′g ,s′∆ )∈Sg ×Sl

since

P

|∆|

(s′g ,s′∆ )∈Sg ×Sl

Jk [s′g , s′∆ , sg , ag , s∆ , a∆ ]Jk [s′′g , s′′∆ , s′g , ag , s′g , a∆ ] is the stochastic proba-

bility function corresponding to the two-step evolution of the joint states from (sg , s∆ ) to (s′′g , s′′∆ )
under actions ag , a∆ . This can be thought of as an analogously to the fact that a 1 on the (i, j)’th
entry on the square of a 0/1 adjacency matrix of a graph represents the fact that there is a path of
distance 2 between vertices i and j.
Finally, the third equality recovers the definition of the expectation, with respect to the joint probability
function Jk2 .
We next show limT →∞ E∆∈([n]) Q̂Tk (sg , s∆ , ag , a∆ ) = E∆∈([n]) Q̂∗k (sg , s∆ , ag , a∆ ) = Q∗ (s, a).
k
k

∗
Lemma E.12. The Q function is the average value of each of the nk sub-sampled Q̂∗k -functions.
Q∗ (s, a) −

r̃
1 X

Q̂Tk (sg , s∆ , ag , a∆ ) ≤ γ T ·
n
1−γ
k ∆∈ [n]
(k)

Proof. We bound the differences between Q̂Tk at each Bellman iteration of our approximation to Q∗ .
Note that:
1 X

Q̂Tk (sg , s∆ , ag , a∆ )
n
k ∆∈ [n]
(k)
X
1
= T Q∗ (s, a) − n
T̂k Q̂Tk −1 (sg , s∆ , ag , a∆ )
k ∆∈ [n]
(k)

Q∗ (s, a) −

= r[n] (sg , s[n] , ag ) + γE

max

s′g ∼Pg (·|sg ,ag ),
a′ ∈Ag ,a′[n] ∈An
l
s′i ∼Pl (·|si ,ai ,sg ),∀i∈[n]) g

Q∗ (s′ , a′ )



1 X
T
′
′
′
′
r∆ (sg , s∆ , ag , a∆ ) + γE s′g ∼Pg (·|sg ,ag )
max
Qk (sg , s∆ , ag , a∆ )
− n
a′g ∈Ag ,a′∆ ∈Ak
′
l
k ∆∈ [n]
s
∼P
(·|s
,a
s
),∀i∈∆
i
i
g
l
i
(k)
P
Next, observe that r[n] (sg , s[n] , ag , a[n] ) = n1
(s , s , a , a ).
[n] r
(k ) ∆∈( k ) [∆] g ∆ g ∆
32

To prove this, we write:
1 X
1X
1 X

r[∆] (sg , s∆ , ag , a∆ ) = n
(rg (sg , ag ) +
rl (si , ai , sg ))
n
k
k ∆∈ [n]
k ∆∈ [n]
i∈∆
(k)
(k)

n−1 X
k−1

rl (si , ai , sg )
= rg (sg , ag ) +
k nk
i∈[n]

1 X
= rg (sg , ag ) +
rl (si , ai , sg ) := r[n] (sg , s[n] , ag , a[n] )
n
i∈[n]

In the second equality, we reparameterized the sum to count the number of times each rl (si , sg ) was
added for each i ∈ ∆. To do this, we count the number of (k − 1) other agents that could form a
k-tuple with agent i, and there are (n − 1)) candidates from which we chooses the (k − 1) agents. In
the last equality, we expanded and simplified the binomial coefficients.
So, we have that:


1 X
∗
T
sup
Q (s, a) − n
Q̂k (sg , s[n] , ag , a∆ )
(s,a)∈S×A
k ∆∈ [n]
(k)


X
1
=
sup
T Q∗ (s, a) − n
T̂k Q̂Tk −1 (sg , s∆ , ag , a∆ )
(s,a)∈S×A
k ∆∈ [n]
(k)

= γ sup
E
max
Q∗ (s′ , a′ )
s′g ∼P (·|sg ,ag ),
′
(s,a)∈S×A

s′i ∼Pl (·|si ,ai ,sg ),∀i∈[n]

−

=γ

sup
(s,a)∈S×A

E


1 X
T −1 ′
′
′
′

′
E
max
Q̂
(s
,
s
,
a
,
a
)
sg ∼Pg (·|sg ,ag ),
g ∆ g ∆
n
k
a′g ∈Ag ,
′
k ∆∈ [n]
( k ) si ∼Pl (·|si ,ai ,sg ),∀i∈∆ a′∆ ∈Akl

max
Q∗ (s′ , a′ )
′

s′g ∼Pg (·|sg ,ag ),
s′i ∼Pl (·|si ,ai ,sg ),∀i∈[n]

−
sup
(s,a)∈S×A

a ∈A

1 X

n
k

≤γ

a ∈A

max

∆∈([n]
k )

a′g ∈Ag ,a′∆ ∈Ak
l


Q̂Tk −1 (s′g , s′∆ , a′g , a′∆ )

E

s′g ∼Pg (·|sg ,ag ),
s′i ∼Pl (·|si ,ai ,sg ),∀i∈[n]



1 X
T −1 ′
∗ ′ ′
′
′
′

Q
(s
,
a
)
−
Q̂
(s
,
s
,
a
,
a
)
g ∆ g ∆
n
k
a′g ∈Ag ,a[n] ∈An
l
k ∆∈ [n]
(k)


1 X
Q̂Tk −1 (s′g , s′∆ , a′g , a′∆ )
≤γ
sup
Q∗ (s′ , a′ ) − n
(s′ ,a′ )∈S×A
k ∆∈ [n]
(k)
max
′

We justify the first inequality by noting the general property that for positive vectors v, v ′ for which
v ⪰ v ′ which follows from the triangle inequality:
v−

1 X ′

v
≥ ∥v∥∞ −
n
∞
k ∆∈ [n]
(k)

1 X ′

v
n
∞
k ∆∈ [n]
(k)
1 X ′
v
= ∥v∥∞ − n
∞
k ∆∈ [n]
(k)
1 X
≥ ∥v∥∞ − n
∥v ′ ∥∞
k ∆∈ [n]
(k)
33

Thus, applying this bound recursively, we get:
1 X
Q̂Tk (sg , s∆ , ag , a∆ )
Q∗ (s, a) − n
k ∆∈ [n]
(k)


1 X
T
∗ ′ ′
0
′
′
′
′
≤γ
sup
Q (s , a ) − n
Q̂k (sg , s∆ , ag , a∆ )
(s′ ,a′ )∈S×A
k ∆∈ [n]
(k)
= γT

Q∗ (s′ , a′ )

sup
(s′ ,a′ )∈S×A

= γT ·

r̃
1−γ

The first inequality follows from the γ-contraction property of the update procedure, and the ensuing
equality follows from our bound on the maximum possible value of Q from Theorem C.4 and noting
that Q̂0k := 0.
Therefore, as T → ∞,
Q∗ (s, ag ) −

1 X

Q̂Tk (sg , s∆ , ag ) → 0,
n
k ∆∈ [n]
(k)

which proves the lemma.
Corollary E.13. Since

1

(nk)

P

∆∈([n]
k )

Q̂Tk (sg , s∆ , ag , a∆ ) := E∆∈([n]) Q̂Tk (sg , s∆ , ag , a∆ ), we therek

fore get:
Q∗ (s, a) − lim E∆∈([n]) Q̂Tk (sg , s∆ , ag , a∆ ) ≤ lim γ T ·
T →∞

T →∞

k

r̃
1−γ

Consequently,
Q∗ (s, a) − E∆∈([n]) Q̂∗k (sg , s∆ , ag , a∆ ) ≤ 0
k

Further, we have that

Q∗ (s, a) − E∆∈([n]) Q̂∗k (sg , s∆ , ag , a∆ ) = 0,
k

since Q̂∗k (sg , s∆ , ag , a∆ ) ≤ Q∗ (s, a).
Lemma E.14. The absolute difference between the expected maximums between Q̂k and Q̂k′ is
atmost the maximum of the absolute difference between Q̂k and Q̂k′ , where the expectations are
taken over any joint distributions of states J , and the maximums are taken over the actions.

E(s′g ,s′ ′ )∼J|∆∪∆′ | (·,·|sg ,ag ,s∆∪∆′ ,a∆∪∆′ )
max
Q̂Tk (s′g , s′∆ , a′g , a′∆ )
a′g ∈Ag ,a′∆ ∈Ak
l

∆∪∆



−

max
a′g ∈Ag ,a′∆′ ∈Ak
l

≤

max

|∆∪∆′ |

a′g ∈Ag ,a′∆∪∆′ ∈Al

E(s′g ,s′

∆∪∆

Q̂Tk′ (s′g , s′∆′ , a′g , a′∆′ )
′


Q̂Tk (s′g , s′∆ , a′g , a′∆ )
′ )∼J|∆∪∆′ | (·,·|sg ,ag ,s∆∪∆′ ,a∆∪∆′ )
− Q̂Tk′ (s′g , s′∆′ , a′g , a′∆′ )

Proof. Denote:
a∗g , a∗∆ := arg max
Q̂Tk (s′g , Fs′∆ , a′g , a′∆ )
′
ag ∈Ag ,
a′∆ ∈Ak
l

ã∗g , ã∗∆′ := arg max
Q̂Tk′ (s′g , Fs′∆′ , a′g , a′∆′ )
′
ag ∈Ag ,

a′∆′ ∈Ak
l

′

34



We extend a∗∆ to a∗∆′ by letting a∗∆′ \∆ be the corresponding ∆ \ ∆′ variables from ã∗∆′
|∆′ \∆|

in Al

. For the remainder of this proof, we adopt the shorthand Es′g ,s′∆∪∆′ to refer to

′
′ ,a )−
Q̂Tk (s′g , Fz∆
)∼J|∆∪∆′ | (·,·|sg ,ag ,s∆∪∆′ ,a∆∪∆′ ) . Then, if Es′g ,s′∆∪∆′ maxa′g ∈Ag ,a′∆ ∈Ak
g
l
′
′ , a ) > 0, we have:
Es′g ,s′∆∪∆′ maxa′ ∈Ag ,a′ ′ ∈Ak′ Q̂Tk′ (s′g , Fz∆
g
′
g
l

E(s′g ,s′

∆∪∆′

∆

Es′g ,s′∆∪∆′

max

a′g ∈Ag ,a′∆ ∈Ak
l

′
′ , a ) − Es′ ,s′
Q̂Tk (s′g , Fz∆
g
g ∆∪∆′

max
a′g ∈Ag ,a′∆′ ∈Ak
l

′

′
′ ,a )
Q̂Tk′ (s′g , Fz∆
g
′

= Es′g ,s′∆∪∆′ Q̂Tk (s′g , s′∆ , a∗g , a∗∆ ) − Es′g ,s′∆∪∆′ Q̂Tk′ (s′g , s′∆′ , ã∗g , ã∗∆′ )
≤ Es′g ,s′∆∪∆′ Q̂Tk (s′g , s′∆ , a∗g , a∗∆ ) − Es′g ,s′∆∪∆′ Q̂Tk′ (s′g , s′∆′ , a∗g , a∗∆′ )
≤

max

a′g ∈Ag ,

Es′g ,s′∆∪∆′ Q̂Tk (s′g , s′∆ , a′g , a′∆ ) − Es′g ,s′∆∪∆′ Q̂Tk′ (s′g , s′∆′ , a′g , a′∆′ )

|∆∪∆′ |

a∆∪∆′ ∈Al

We observe that if the opposite inequality holds (i.e., Es′g ,s′∆∪∆′ maxa′g ∈Ag ,a′∆ ∈Akl Q̂Tk (s′g , s′∆ , a′g , a′∆ )−
Es′g ,s′∆∪∆′ maxa′ ∈Ag ,a′ ′ ∈Ak′ Q̂Tk′ (s′g , s′∆′ , a′g , a′∆′ ) < 0), an analogous argument by replacing a∗g
g
l
∆
with ã∗g and a∗∆ with ã∗∆ yields an identical bound.
Lemma E.15. Suppose z, z ′ ≥ 1. Consider functions Γ : Θ1 × Θ2 × · · · × Θz × Θ∗ → R and
Γ′ : Θ′1 × Θ′2 × · · · × Θ′z′ × Θ∗ → R, where Θ1 , . . . , Θz and Θ′1 , . . . , Θ′z′ are finite sets. Consider
a probability distribution function µΘi for i ∈ [z] and µ′Θi for i ∈ [z ′ ]. Then:
Eθ1 ∼µΘ1 max
Γ(θ1 , . . . , θz , θ∗ ) − E θ1 ∼µ′Θ
∗
∗
θ ∈Θ
...
θz ∼µΘz

max
Γ′ (θ1 , . . . , θz′ , θ∗ )
∗
∗

... 1 θ ∈Θ
θz′ ∼µ′Θ ′
z

≤ max
Eθ1 ∼µΘ1 Γ(θ1 , . . . , θz , θ∗ ) − E θ1 ∼µ′Θ Γ′ (θ1 , . . . , θz′ , θ∗ )
∗
∗
θ ∈Θ

...
θz ∼µΘz

... 1
θz′ ∼µ′Θ ′
z

Proof. Let θ̂∗ := arg maxθ∗ ∈Θ∗ Γ(θ1 , . . . , θz , θ∗ ) and θ̃∗ := arg maxθ∗ ∈Θ∗ Γ′ (θ1 , . . . , θz′ , θ∗ ).
If Eθ1 ∼µΘ1 ,...,θz ∼µΘz maxθ∗ ∈Θ∗ Γ(θ1 , . . . , θz , θ∗ )−Eθ1 ∼µ′ ′ ,...,θz′ ∼µ′ ′ maxθ∗ ∈Θ∗ Γ′ (θ1 , . . . , θz′ , θ∗ ) >
Θ1

Θ ′
z

0, then:

Eθ1 ∼µΘ1 max
Γ(θ1 , . . . , θz , θ∗ ) − E θ1 ∼µ′ ′ max
Γ′ (θ1 , . . . , θz′ , θ∗ )
∗
∗
∗
∗
θ ∈Θ
...
θz ∼µΘz

Θ
... 1 θ ∈Θ
θz′ ∼µ′Θ′
z′

= Eθ1 ∼µΘ1 Γ(θ1 , . . . , θz , θ̂∗ ) − E θ1 ∼µ′ ′ Γ′ (θ1 , . . . , θz′ , θ̃∗ )
Θ

...
θz ∼µΘz

... 1
θz′ ∼µ′Θ′

z′

≤ Eθ1 ∼µΘ1 Γ(θ1 , . . . , θz , θ̂∗ ) − E θ1 ∼µ′ ′ Γ′ (θ1 , . . . , θz′ , θ̂∗ )
Θ

...
θz ∼µΘz

... 1
θz′ ∼µ′Θ′

z′

≤ max
Eθ1 ∼µΘ1 Γ(θ1 , . . . , θz , θ∗ ) − E θ1 ∼µ′ ′ Γ′ (θ1 , . . . , θz′ , θ∗ )
∗
∗
θ ∈Θ

Θ

...
θz ∼µΘz

... 1
θz′ ∼µ′Θ′

z′

Here, we replace each θ∗ with the maximizers of their corresponding terms, and upper bound them
by the maximizer of the larger term. Next, we replace θˆ∗ in both expressions with the maximizer
choice θ∗ from Θ∗ , and further bound the expression by its absolute value.
If Eθ1 ∼µΘ1 ,...,θz ∼µΘz maxθ∗ ∈Θ∗ Γ(θ1 , . . . , θz , θ∗ )−Eθ1 ∼µ′ ′ ,...,θz′ ∼µ′ ′ maxθ∗ ∈Θ∗ Γ′ (θ1 , . . . , θz′ , θ∗ )
Θ1

∗

Θ ′
z

∗

is negative, then an analogous argument that replaces θ̂ with θ̃ yields the same result.

35

F

Bounding Total Variation Distance

As |∆| → n, we prove that the total variation (TV) distance between the empirical distribution of

z∆ and z[n] goes to 0. Here, recall that zi ∈ Z = Sl × Al , and z∆ = {zi : i ∈ ∆} for ∆ ∈ [n]
k .
Before bounding the total variation distance between Fz∆ and Fz∆′ , we first introduce Lemma C.5 of
Anand and Qu [2024] which can be viewed as a generalization of the Dvoretzky-Kiefer-Wolfowitz
concentration inequality, for sampling without replacement. We first make an important remark.
Remark
F.1. First, observe that if ∆ is an independent random variable uniformly supported on

[n]
random variables uniformly supported on the global
k , then s∆ and a∆ are also independent
a[n] 
state s[n]
and
the
global
action
.
To
see this, let ψ1 : [n] → Sl where ψ1 (i) = si and
k
k
ξ1 : [n] → Al where ξ1 (i) = ai . This naturally extends to ψk : [n]k → Slk , where ψk (i1 , . . . , ik ) =
(si1 , . . . , sik ) and ξk : [n]k → Akl , where ξk (i1 , . . . , ik ) = (ai1 , . . . , aik ) for all k ∈ [n]. Then, the
independence of ∆ implies the independence of the generated σ-algebra. Further, ψk and ξk (which
are a Lebesgue measurable function of a σ-algebra) are sub-algebras, implying that s∆ and a∆ must
also be independent random variables.
For reference, we present the multidimensional Dvoretzky-Kiefer-Wolfowitz (DKW) inequality
(Dvoretzky et al. [1956], Massart [1990], Naaman [2021]) which bounds the difference between an
empirical distribution function for a set B∆ and B[n] when each element of ∆ for |∆| = k is sampled
uniformly at random from [n] with replacement.
Theorem F.2 (Multi-dimensional Dvoretzky-Kiefer-Wolfowitz (DFW) inequality [Dvoretzky et al.,
1956, Naaman, 2021]). Suppose B ⊂ Rd and ϵ > 0. If ∆ ⊆ [n] is sampled uniformly with
replacement, then
"
#
n
2
1 X
1X
Pr sup
1{Bi = x} −
1{Bi = x} < ϵ ≥ 1 − d(n + 1)e−2|∆|ϵ ·
|∆|
n
x∈B
i=1
i∈∆

Lemma C.5 of Anand and Qu [2024] generalizes the DKW inequality for sampling without replacement:
Lemma F.3 (Sampling without replacement analogue of the DKW inequality, Lemma C.5 in Anand
and Qu [2024]). Consider a finite population X = (x1 , . . . , xn ) ∈ Bln where Bl is a finite set. Let
∆ ⊆ [n] be a random sample of size k chosen uniformly and without replacement. Then, for all
x ∈ Bl :


X
X
2|∆|nϵ2
1
1
Pr  sup
1{xi = x} < ϵ ≥ 1 − 2|Bl |e− n−|∆|+1
1{xi = x} −
n
x∈Bl |∆|
i∈∆

i∈[n]

8knϵ2

Lemma F.4. With probability atleast 1 − 2|Sl ||Al |e− n−k+1 ,
TV(Fz∆ , Fz[n] ) ≤ ϵ
Proof. Recall that z := (sl , al ) ∈ Sl × Al . From Theorem F.3, substituting Bl = Zl yields:


8knϵ2
Pr sup Fz∆ (zl ) − Fz[n] (zl ) ≤ 2ϵ ≥ 1 − 2|Zl |e− n−k+1
zl ∈Zl

8knϵ2

= 1 − 2|Sl ||Al |e− n−k+1 ,
which yields the proof.
We now present an alternate bound for the total variation distance, where the distance actually goes
to 0 as |∆| → n. For this, we use the fact that the total variation distance between two product
distributions is subadditive.
Lemma F.5 (Lemma B.8.1 of Ghosal and van der Vaart [2017]. Subadditivity of TV distance for
Product Distributions). Let P and Q be product distributions over some domain S. Let α1 , . . . , αd
be the marginal distributions of P and β1 , . . . , βq be the marginal distributions of Q. Then,
∥P − Q∥1 ≤

d
X
i=1

36

∥αi − βi ∥1

Lemma F.6 (KL-divergence decays too slowly).
r
TV(Fz∆ , Fz[n] ) ≤

1−

|∆|
n

Proof. By the symmetry of the total variation distance metric, we have that
TV(Fz[n] , Fz∆ ) = TV(Fz∆ , Fz[n] ).
From the Bretagnolle-Huber inequality Tsybakov [2008] we have that
p
TV(f, g) = 1 − e−DKL (f ∥g) .
Here, DKL (f ∥g) is the Kullback-Leibler (KL) divergence metric between probability distributions f
and g over the sample space, which we denote by X and is given by
X
f (x)
DKL (f ∥g) :=
f (x) ln
(29)
g(x)
x∈X

Thus, from Equation (29):
!
P
n i∈∆ 1{zi = z}
1 X
P
DKL (Fz∆ ∥Fz[n] ) =
1{zi = z} ln
|∆|
|∆| i∈[n] 1{zi = z}
z∈Zl
i∈∆
!
n
1 X X
1{zi = z} ln
=
|∆|
|∆|
z∈Zl i∈∆
! P
1{z = z}
1 X X
1{zi = z} ln P i∈∆ i
+
|∆|
i∈[n] 1{zi = z}
z∈Zl i∈∆
! P
1{z = z}
n
1 X X
= ln
+
1{zi = z} ln P i∈∆ i
|∆| |∆|
i∈[n] 1{zi = z}
z∈Zl i∈∆


n
≤ ln
|∆|
P
P
In the third line, we note that z∈Zl i∈∆ 1{zi = z} = |∆| since each local agent contained in ∆
P
must
P have some state/action pair contained in Zl . In the last line, we note that i∈∆ 1{zi = z} ≤
i∈[n] 1{zi = z}, For all z ∈ Zl , and thus the summation of logarithmic terms in the third line is
negative.
X

Finally, using this bound in the Bretagnolle-Huber inequality yields the lemma.
Corollary F.7. From Theorem F.6, setting ∆ = [n] also recovers TV(Fz∆ , Fz[n] ) = 0.
Theorem F.8. With probability atleast 1 − δ for δ ∈ (0, 1)2 :
Q̂∗k (sg , Fs∆ , ag , Fa∆ ) − Q̂∗n (sg , Fs[n] , ag , Fa[n] )

ln 2|Slδ||Al |
≤
1−γ

r

n−k+1
· ∥rl (·, ·)∥∞
8kn

Proof. From combining the total variation distance bound in Theorem F.4 and the Lipschitz continuity
PT
1
bound in Theorem E.3 with t=0 γ T ≤ 1−γ
for γ ∈ (0, 1), we have:


8knϵ2
2ϵ
∗
∗
Pr |Q̂k (sg , Fs∆ , ag , Fa∆ ) − Q̂n (sg , Fs[n] , ag , Fa[n] )| ≤
· ∥rl (·, ·)∥∞ ≥ 1−2|Sl ||Al |e− n−k+1
1−γ
r


8knϵ2
2|Sl ||Al |
Then, reparameterizing 1 − 2|Sl ||Al |e− n−k+1 into 1 − δ to get ϵ = n−k+1
ln
gives
8kn
δ
that with probability at least 1 − δ,
Q̂∗k (sg , Fs∆ , ag , Fa∆ ) − Q̂∗n (sg , Fs[n] , ag , Fa[n] )
proving the claim.
37

ln 2|Slδ||Al |
≤
1−γ

r

n−k+1
· ∥rl (·, ·)∥∞ ,
8kn

G

Using the Performance Difference Lemma

In general, convergence analysis requires the guarantee that a stationary optimal policy exists.
Fortunately, when working with the empirical distribution function, the existence of a stationary
optimal policy is guaranteed when the state/action spaces are finite or countably infinite. However,
lifting the knowledge of states onto the continuous empirical distribution function space, and designing
a policy on the lifted space is still analytically challenging. To circumvent this, Gu et al. [2021]
creates lifted ϵ-nets and does kernel regression to obtain convergence guarantees. Moreover, our
result has a similar flavor to MDPs with dynamic exogenous inputs from learning theory, [Dietterich
et al., 2018, Foster et al., 2022, Anand and Qu, 2024], wherein our subsampling algorithm treats each
sampled state as an endogenous state.
Here, our analytic approach bears a stark difference, wherein we analyze the sampled structure of
the mean-field empirical distribution function, rather than studying the structure of the lifted space.
For this, we leverage the classical Performance Difference Lemma, which we restate below for
completeness.
Lemma G.1 (Performance Difference Lemma, Kakade and Langford [2002]). Given policies π1 , π2 ,
with corresponding value functions V π1 , V π2 :
1
E ′ πs 1 [Aπ2 (s′ , a′ )]
1 − γ a′ s∼π∼d(·|s
′
)
1
P∞
π2 ′ ′
π2 ′ ′
π2 ′
π1 ′
Here, A (s , a ) := Q (s , a ) − V (s ) and ds (s ) = (1 − γ) h=0 γ h Prπh1 [s′ , s] where
Prπh1 [s′ , s] is the probability of π1 reaching state s′ at time step h starting from state s.
V π1 (s) − V π2 (s) =

est
We denote our learned policy πk,m
where:
g,est
1,est
n,est
est
πk,m
(sg , s[n] ) = (πk,m
(sg , s[n] ), πk,m
(sg , s[n] ), . . . , πk,m
(sg , s[n] )) ∈ P(Ag ) × P(Al )n ,
g,est
g,est
i,est
where πk,m
(sg , s[n] ) = π̂k,m
(sg , su , Fs∆\u ) is the global agent’s action and πk,m
(sg , s[n] ) :=
est
π̂k,m (sg , si , Fs∆i ) is the action of the i’th local agent. Here, ∆i is a random variable supported on

[n]\i
k−1 , u is a random variable uniformly distributed on [n], and ∆ is a random variable uniformly
distributed on [n] \ u. Then, denote the optimal policy π ∗ given by

π ∗ (s) = (πg∗ (sg , s[n] ), π1∗ (sg , s[n] ), . . . , πn∗ (sg , s[n] )) ∈ P(Ag ) × P(Al )n ,
where πg∗ (sg , s[n] ) is the global agent’s action, and πi∗ (sg , s[n] ) := π ∗ (sg , si , Fs∆i ) is the action
of the i’th local agent. Next, in order to compare the difference in the performance of π ∗ (s) and
est
πk,m
(sg , s[n] ), we define the value function of a policy π to be the infinite-horizon γ-discounted
rewards, (denoted by V π ) as follows:
Definition G.2. The value function V π : S → R of a given policy π, for S := Sg × Sln is:
#
"∞
X
π
t
V (s) = Ea(t)∼π(·|s(t))
γ r(s(t), a(t))|s(0) = s .
(30)
t=0
∗

est
Theorem G.3. For the optimal policy π and the learned policy πk,m
, for any state s0 ∈ S, we have:
r
r
˜
∗
est
r̃
n−k+1
2|Sl ||Al |
2r
V π (s0 ) − V πk,m (s0 ) ≤
ln
+ 2ϵk,m +
|Ag |k |Al | δ
(1 − γ)2
2nk
δ
(1 − γ)2

Proof. Applying the performance difference lemma to the policies gives us:
∗

est

∗
∗
1
est
E πest E
[V π (s) − Qπ (s, a)]
1 − γ s∼ds0k,m a∼πk,m (·|s)


1
π∗
′
π∗
est (·|s) Q
=
E πest Ea′ ∼π∗ (·|s) Q (s, a ) − Ea∼πk,m
(s, a)
1 − γ s∼ds0k,m


∗
1
π∗
est
est (·|s) Q
=
E π̃k,m
Qπ (s, π ∗ (·|s)) − Ea∼πk,m
(s, a)
1 − γ s∼ds0

V π (s0 ) − V π̃k,m (s0 ) =

38

Next, by the law of total expectation,
∗
est (·|s) [Q (s, a)]
Ea∼πk,m
X
X
1
1
est
est
est
 n−1n Q∗ (s, π̂k,m
(sg , Fs∆ )g , π̂k,m
(sg , s1 , Fs∆1 ), . . . , π̂k,m
=
(sg , sn , Fs∆n ))
n
k
[n] ∆1 ,...,∆n :
k−1
∆∈( k )
∆i ∈([n]\i
k−1 )

Therefore,
∗

∗
est (·|s) Q (s, a)
Qπ (s, π ∗ (·|s)) − Ea∼πk,m

X
X
X
X
1
1
 n−1n Q∗ (s, π ∗ (·|s))
···
=
n
k
[n]\1
[n]\2
k−1
1
2
∆n ∈([n]\n
∆∈([n]
k−1 )
k ) ∆ ∈( k−1 ) ∆ ∈( k−1 )

est
est
est
− Q∗ (s, π̂k,m
(sg , Fs∆ )g , π̂k,m
(sg , s1 , Fs∆1 ), . . . , π̂k,m
(sg , sn , Fs∆n ))

Therefore, by grouping the equations above, we have:
∗

est

V π (s0 ) − V π̃k,m (s0 )
≤

X
X
X
1
1
est Ea∼π̃ est (·|s)
Q∗ (s, π ∗ (s))
E π̃k,m


n n−1 n
k,m
1 − γ s∼ds0
n
[n]\i
i
k k−1
i∈[n]
∆∈([n]
k ) ∆ ∈( k−1 ),
∀i∈[n]
∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
1

+

 n 
n n−1 n

X

k

i∈[n]

k−1

est
est
Q̂est
k,m (sg , si , Fs∆i , π̂k,m (sg , Fs∆ )g , {π̂k,m (sg , sj,∆j )}j∈{i,∆i } )

est
est
− Q∗ (s, π̂k,m
(sg , s∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈[n]



Theorem G.4 shows a uniform bound on
est
est
est
Q∗ (s, π ∗ (·|s)) − Q∗ (s, π̂k,m
(sg , Fs∆ )g , π̂k,m
(sg , s1 , Fs∆1 ), . . . , π̂k,m
(sg , sn , Fs∆n ))

(independent of ∆i , ∀i ∈ [n]), allowing the sums and the counts in the denominator will cancel
∗
: S → A and π ∗ : S → A are deterministic functions. Therefore, denote
out. Observe that π̂k,m
∗
a = π (s). Then, from Theorem G.6,
X
X
X
X
1
1
est Ea∼π est (·|s)
E πk,m
Q∗ (s, π ∗ (s))
·
·
·


n
n n−1
k,m
1 − γ s∼ds0
n
[n]
[n]\1
[n]\n
k
k−1
i∈[n]
∆∈( k ) ∆1 ∈( k−1 )
∆n ∈( k−1 )
∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

1
E πest
=
1 − γ s∼ds0k,m

X

X

1
n( n
k)

X

n−1 n
k−1
i∈[n]



∆1 ,...,∆n :
∆∈([n]
k )
∆i ∈([n]\i
k−1 )

Q̂∗n (sg , Fs[n] , π̂n∗ (sg , Fs[n] )g , π̂n∗ (sg , Fs[n] )1:n )

∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

r̃
≤
(1 − γ)2

r

n−k+1
8nk

r
ln

r̃
2|Sl ||Al |
+ ϵk,m +
|Ag |k |Al | δ
δ
(1 − γ)2
39

Similarly, from Theorem G.7, we have that
1
E πest
1 − γ s∼ds0k,m

X

X

[n]\1
1
∆∈([n]
k ) ∆ ∈( k−1 )

···

X
∆n ∈([n]\n
k−1 )

1
n(n
k)

 Q̂est
k,m (sg , si , Fs∆i ,

n−1 n
k−1

est
est
est
est
π̂k,m
(sg , s∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈{i,∆i } ) − Q∗ (s, π̂k,m
(sg , s∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈[n]
r
r
r̃
n−k+1
2|Sl ||Al |
r̃
≤
ln
|Ag |k |Al | δ
+ ϵk,m +
(1 − γ)2
8nk
δ
(1 − γ)2

Hence, combining the above inequalities, we get:
r
r
˜
est
2r̃
n−k+1
2|Sl ||Al |
2r
π∗
πk,m
V (s0 ) − V
(s0 ) ≤
|Ag |k |Al | δ
ln
+ 2ϵk,m +
2
(1 − γ)
8nk
δ
(1 − γ)2
which yields the claim. We defer parameter optimization to Theorem G.8.
Lemma G.4 (Uniform Bound on Q∗ with different actions). For all s ∈ S, ∆ ∈

[n]\i
k−1 for i ∈ [n], we have:

[n]
k



and ∆i ∈

est
est
Q∗ (s, π ∗ (·|s)) − Q∗ (s, π̂k,m
(sg , Fs∆ )g , {π̂k,m
(sg , si , Fs∆i )}i∈[n] )
X
1
∗
∗
≤
Q∗ (s, π ∗ (s)) − Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
n
i∈[n]

est
est
+ Q̂est
k,m (sg , si , Fs∆i , π̂k,m (sg , Fs∆ )g , {π̂k,m (sg , sj,∆j )}j∈{i,∆i } )
est
est
− Q∗ (s, π̂k,m
(sg , s∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈[n]

Proof. Observe that
est
est
Q∗ (s, π ∗ (·|s)) − Q∗ (s, π̂k,m
(sg , Fs∆ )g , {π̂k,m
(sg , si , Fs∆i )}i∈[n] )
1 X est
est
est
est
Q̂k,m (sg , si , Fs∆i , π̂k,m
(sg , Fs∆ )g , π̂k,m
(sg , si , Fs∆i ), {π̂k,m
(sg , sj , Fs∆j )}j∈∆i )
≤
n
i∈[n]

−

1 X est
est
est
est
Q̂k,m (sg , si , Fs∆i , π̂k,m
(sg , Fs∆ )g , π̂k,m
(sg , si , Fs∆i ), {π̂k,m
(sg , sj , Fs∆j )}j∈∆i )
n
i∈[n]

+

1 X est
Q̂k,m (sg , si , Fs∆i , π ∗ (s)g , π ∗ (s)i , {π ∗ (s)j }j∈∆k )
n
i∈[n]

−

1 X est
Q̂k,m (sg , si , Fs∆i , π ∗ (s)g , π ∗ (s)i , {π ∗ (s)j }j∈∆k )
n
i∈[n]

≤ Q∗ (s, π ∗ (s)) −

1 X est
Q̂k,m (sg , si , Fs∆i , π ∗ (s)g , π ∗ (s)i , {π ∗ (s)j }j∈∆i )
n
i∈[n]

+

1 X
n

est
est
Q̂est
k,m (sg , si , Fs∆i , π̂k,m (sg , Fs∆ )g , {π̂k,m (sg , sj , Fs∆j )}j∈{i,∆i } )

i∈[n]
est
est
− Q∗ (s, π̂k,m
(sg , Fs∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈[n]

≤

1 X ∗
∗
∗
Q (s, π ∗ (s)) − Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
n
i∈[n]

40

+

1 X est
est
est
Q̂k,m (sg , si , Fs∆i , π̂k,m
(sg , Fs∆ )g , {π̂k,m
(sg , sj,∆j )}j∈{i,∆i } )
n
i∈[n]

est
est
− Q∗ (s, π̂k,m
(sg , s∆ )g , {π̂k,m
(sg , sj , Fs∆j )}j∈[n] ,

which proves the claim.
Lemma G.5. Fix s ∈ S := Sg × Sln . For each j ∈ [n], suppose we are given T -length sequences

of random variables {∆j1 , . . . , ∆jT }j∈[n] , distributed uniformly over the support [n]\j
k−1 . Further,
suppose we are given a fixed sequence δ1 , . . . , δT , where each δt ∈ (0, 1] for t ∈ [T ]. Let
s
n − k + 1 2|Sl ||Al |
1
Φk,t =
ln
.
1−γ
8kn
δt
Then, for each action a = (ag , a[n] ) = π ∗ (s), for t ∈ [T ] and j ∈ [n], define deviation events
ag ,a

Bt

j
j,∆t

ag ,a

Bt

j
j,∆t

such that:
o
n
:= Q∗ (sg , sj , Fz[n]\j , aj , ag )− Q̂est
(s
,
s
,
F
,
a
,
a
)
>
Φ
·
∥r
(·,
·)∥
+
ϵ
g
j
z
j
g
k,t
l
∞
k,m
j
k,m
∆t

(31)
For i ∈ [T ], we define bad-events Bt (which is a union over each deviation event):
[ [
[
ag ,a
j
j,∆t
Bt
Bt =
j∈[n] ag ∈Ag a

k
j ∈Al
j,∆t

Next, denote B =

ST

i=1 Bi . Then, the probability that no bad event Bt occurs is:
T
X
 
Pr B̄ := 1 − Pr[B] ≥ 1 − |Ag |k |Al |
δi
i=1

Proof.
|Q∗ (sg , sj , Fz[n]\j , ag , aj ) − Q̂est
k,m (sg , sj , Fz j , ag , aj )|
∆t

= Q∗ (sg , sj , Fz[n]\j , ag , aj ) − Q̂∗k (sg , sj , Fz j , ag , aj )
∆t

+ Q̂∗k (sg , sj , Fz j , ag , aj ) − Q̂est
k,m (sg , sj , Fz j , ag , aj )
∆t

∆t

≤ Q∗ (sg , sj , Fz[n]\j , ag , aj ) − Q̂∗k (sg , sj , Fz j , ag , aj )
∆t

+

Q̂∗k (sg , sj , Fz j , ag , aj ) − Q̂est
k,m (sg , sj , Fz∆j , ag , aj )
∆
t

≤

t

∗

Q (sg , sj , Fz[n]\j , ag , aj ) − Q̂∗k (sg , sj , Fz j , ag , aj )
∆

+ ϵk,m

t

The first inequality above follows from the triangle inequality, and the second inequality uses
Q̂∗k (sg , sj , Fz j , ag , aj ) − Q̂est
k,m (sg , sj , Fz j , ag , aj )
∆t

∆t

≤ Q̂∗k (sg , sj , Fz j , ag , aj ) − Q̂est
k,m (sg , sj , Fz j , ag , aj )
∆t

∆t

∞

≤ ϵk,m ,
where the ϵk,m follows from Theorem 4.3. From Theorem F.8, we have that with probability at least
1 − δt ,
Q∗ (sg , sj , Fz[n]\j , ag , aj ) − Q̂∗k (sg , sj , Fz j , ag , aj ) ≤ Φk,t · ∥rl (·, ·)∥∞
∆t
s
r
1
2|Sl ||Al | n − k + 1
=
ln
· ∥rl (·, ·)∥∞
1−γ
δt
8kn
41

ag ,a

So, event Bt

j
j,∆t

occurs with probability atmost δt .

Here, observe that if we union bound across all the events parameterized by the empirical distributions
of ag , aj,∆j ∈ Ag × Akl given by Fa j , this also forms a covering of the choice of variables Fs j by
∆t

t

∆t

agents j ∈ [m], and therefore across all choices of ∆1t , . . . , ∆nt (subject to the permutation invariance
of local agents) for a fixed t.
Thus, from the union bound, we get:
X
X
Pr[Bt ] ≤

ag ,a1,∆1

Pr[Bt

t

ag ,a1,∆1

] ≤ |Ag |k |Al | Pr[Bt

t

]

ag ∈Ag Fa∆ ∈µk (Al )
t

Applying the union bound again proves the lemma:
T
T
X
X
Pr[B̄] ≥ 1 −
Pr[Bt ] ≥ 1 − |Ag |k |Al |
δt ,
t=1

t=1

which proves the claim.
Lemma G.6. For any arbitrary distribution D of states S := Sg × Sln , for any ∆i ∈
i ∈ [n] and for δ ∈ (0, 1] we have:
 X
X 1
X
X
1
1
 n−1n
Q∗ (s, π ∗ (s))
Es∼D
···
n
n
k
[n]\1
k−1
1
i∈[n]
∆∈([n]
∆n ∈([n]\n
k ) ∆ ∈( k−1 )
k−1 )

[n]\i
k−1



for

∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

r̃
≤
1−γ

r

n−k+1
8nk

r
ln



2|Sl ||Al |
r̃
+ ϵk,m +
|Ag |k |Al | δ
δ
1−γ

Proof. By the linearity of expectations, observe that:
 X
X
X 1
X
1
1
 n−1n
Es∼D
Q∗ (s, π ∗ (s))
···
n
n
k
[n]\1
k−1
1
i∈[n]
∆∈([n]
∆n ∈([n]\n
k ) ∆ ∈( k−1 )
k−1 )
∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

=

X

X

[n]\1
1
∆∈([n]
k ) ∆ ∈( k−1 )

···

X
∆n ∈([n]\n
k−1 )



X 1
1
1
 n−1n
Es∼D Q∗ (s, π ∗ (s))
n
n
k
k−1
i∈[n]

∗
∗
− Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

Let

r
n−k+1
2|Sl ||Al |
Φk,δ =
ln
.
8nk
δ
Then, define the indicator function I : [n] × S × N × (0, 1] → {0, 1} by:
I(i, s, k, δ) :=


∥rl (·, ·)∥∞
∗
∗
est
∗
∗
1 Q (s, π (s)) − Q̂k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } ) ≤
Φk,δ + ϵk,m
1−γ
r

∗
∗
The expected difference between Q∗ (s′ , π ∗ (s′ )) and Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
is bounded as follows:
∗
∗
Es∼D Q∗ (s′ , π ∗ (s′ )) − Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
i
h
∗
∗
= Es∼D I(i, s, k, δ) Q∗ (s′ , π ∗ (s′ )) − Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
h
i
∗
∗
+ Es∼D (1 − I(i, s, k, δ)) Q∗ (s′ , π ∗ (s′ )) − Q̂est
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )

42

Here, we have used the general property for a random variable X and constant c that E[X] =
E[X 1{X ≤ c}] + E[(1 − 1{X ≤ c})X]. Then,
∗
∗
Es∼D Q∗ (s′ , π ∗ (s′ )) − Q̂est
k,m (sg , si,∆i , π (s)g , π (s)i,∆i )
r
r
r̃
2|Sl ||Al |
r̃
n−k+1
+ ϵk,m +
(1 − Es′ ∼D I(s′ , k, δ)))
≤
ln
1−γ
8nk
δ
1−γ
r
r
r̃
n−k+1
2|Sl ||Al |
r̃
≤
ln
+ ϵk,m +
|Ag |k |Al | δ
1−γ
8nk
δ
1−γ
r
r
r̃
n−k+1
2|Sl ||Al |
r̃
=
ln
+ ϵk,m +
|Ag |k |Al | δ
1−γ
8nk
δ
1−γ

For the first term in the first inequality, we use E[X 1{X ≤ c}] ≤ c. For the second term, we trivially
bound Q∗ (s′ , π ∗ (s′ )) − Q̂∗k (sg , si,∆i , π ∗ (s)g , π ∗ (s)i,∆i ) by the maximum value Q∗ can take, which
r̃
is 1−γ
by Theorem C.4. In the second inequality, we use the fact that the expectation of an indicator
function is the conditional probability of the underlying event. The second inequality follows from
Theorem G.5.
Since this is a uniform bound that is independent of ∆j for j ∈ [n] and i ∈ [n], we have:
X

X

X
1/n
∗
∗
Es∼D Q∗ (s, π ∗ (s))− Q̂est


n
k,m (sg , si , Fs∆i , π (s)g , {π (s)j }j∈{i,∆i } )
n n−1

∆1 ,...,∆n : k
∆∈([n]
k )
∆i ∈([n]\i
k−1 )

k−1

i∈[n]

r̃
≤
1−γ

r

n−k+1
8nk

r
ln

2|Sl ||Al |
r̃
+ ϵk,m +
|Ag |k |Al | δ
δ
1−γ

This proves the claim.
est
Corollary G.7. Changing the notation of the policy to πk,m
in the proofs of Theorems G.5 and G.6,
est
such that a ∼ π̃k,m then yields that:


X
X 1
1
1
 X

 n−1n
Es∼D 
Q∗ (s, a) − Q̂∗k,m (sg , si , Fs∆i , ag , ai,∆i } ) 
n
n
k
[n]\i
1
n
i
k−1
i∈[n]
∆∈([n]
k ) ∆ ,...,∆ :∆ ∈∈( k−1 )
r
r
r̃
n−k+1
2|Sl ||Al |
r̃
+ ϵk,m +
|Ag |k |Al | δ
ln
≤
1−γ
8nk
δ
1−γ
1
Lemma G.8 (Optimizing Parameters). With probability at least 1 − 100e
k,


∗
1
V π (s0 ) − V πk,m (s0 ) ≤ Õ √
k
2

(1−γ)
Proof. Setting δ = 2r̃ε|A
in the bound for the optimality gap in Theorem G.3 gives:
|k|Al |
g

V

π∗

(s0 ) − V

est
π̃k,m

r̃
(s0 ) ≤
(1 − γ)2

r

r̃
=
(1 − γ)2

r

n−k+1
2nk

r

n−k+1
2nk

s

ln

˜
2|Sl ||Al |
2r
+ 2ϵk,m +
|Ag |k |Al | δ
δ
(1 − γ)2

ln

4r̃ε|Sl ||Al ||Ag |k |Al |
1
+ + 2ϵk,m
(1 − γ)2
ε

√
Setting ε = 10 k for any c > 0 recovers a decaying optimality gap of the order
s
r
1
∗
est
r̃
n
−
k
+
1
40r̃|Sl ||Al ||Ag |k |Al |+ 2
1
V π (s0 ) − V π̃k,m (s0 ) ≤
ln
+ √ + 2ϵk,m
2
2
(1 − γ)
2nk
(1 − γ)
10 k
43

Finally, using the probabilistic bound of ϵk,m ≤ Õ( √1k ) from Theorem G.10 yields that with
1
probability at least 1 − 100e
k,


∗
1
,
V π (s0 ) − V π̃k,m (s0 ) ≤ Õ √
k
which proves the claim.

G.1

Bounding the Bellman Error

This section is devoted to the proof of Theorem 4.3.
Theorem G.9 (Theorem 2 of Li et al. [2022]). If m ∈ N is the number of samples in the Bellman
1
update, there exists a universal constant 0 < c0 < 2 and a Bellman noise 0 < ϵk,m ≤ 1−γ
such that
∗
est
∗
∥T̂k,m Q̂est
k,m − T̂k Q̂k ∥∞ = ∥Q̂k,m − Q̂k ∥∞ ≤ ϵk,m ,

where ϵk,m satisfies
c0 · r̃ · tcover
m=
log2
(1 − γ)5 ϵ2k,m



|S||A|
ρ




log

1
(1 − γ)2


,

(32)

with probability at least 1 − ρ, for any ρ ∈ (0, 1). Here, tcover stands for the cover time, which is the
time taken for the trajectory to visit all state-action pairs at least once. Formally,


1
,
(33)
tcover := min t
min
P(Bt |s0 , a0 ) ≥
2
(s0 ,a0 )∈S×A
where Bt denotes the event that all (s, a) ∈ S × A have been visited at least once between time 0
and time t, and P(Bt |s0 , a0 ) denotes the probability of Bt conditioned on the initial state (s0 , a0 ).
Lemma G.10. If T
2

2

2

√
2
r̃ k
log
1−γ
1−γ ,
2 3.5+2|Sl ||Al |

=

Õ(T |Sg | |Ag | |Al | |Sl | k
1
with probability at least 1 − 100e
k, .

SUB-SAMPLE-MFQ: Learning runs in time
√
r̃), while accruing a Bellman noise ϵk,m ≤ Õ(1/ k)

Proof. We first prove that ∥Q̂Tk − Q̂∗k ∥∞ ≤ √1k .
r̃
√ . Then, using γ = 1 − (1 − γ) ≤ e−(1−γ) ,
For this, it suffices to show γ T 1−γ
≤ √1k =⇒ γ T ≤ r̃1−γ
k
√ . Taking logarithms, we have
it again suffices to show e−(1−γ)T ≤ r̃1−γ
k

1−γ
√
r̃ k
1−γ
−T (1 − γ) ≤ log √
r̃ k
√
1
r̃ k
T ≥
log
1−γ
1−γ

exp(−T (1 − γ)) ≤

√

√

2
1
Since T = 1−γ
log r̃1−γk > 1−γ
log r̃1−γk , the condition holds and ∥Q̂Tk − Q̂∗k ∥∞ ≤ √1k . Then,

rearranging Equation (32) and incorporating the convergence error of the Q̂k -function, one has that
with probability at least 1 − ρ,
√




1
k 2r̃tcover
|Sg ||Ag ||Al ||Sl |
1
√
ϵk,m ≤ √ +
log
log
(34)
ρ
(1 − γ)2
k (1 − γ)2.5 m
Then, using the naïve bound tcover (since we are doing offline learning), we have
tcover ≤ |Sg ||Ag ||Sl ||Al |k |Sl ||Al | .
44

Substituting this in Equation (34) yields that with probability 1 − ρ,
p




k 2r̃|Sg ||Ag ||Sl ||Al |k |Sl ||Al |
1
|Sg ||Ag ||Al ||Sl |
1
√
ϵk,m ≤ √ +
log
log
ρ
(1 − γ)2
(1 − γ)2.5 m
k

(35)

1
Therefore, letting ρ = 100e
k , setting



2r̃|Sg ||Ag ||Sl ||Al |k 3.5+|Sl ||Al |
1
m=
log (100|Sg ||Ag ||Al ||Sl |) log
(36)
(1 − γ)5
(1 − γ)2
√
1
attains a Bellman error of ϵk,m ≤ Õ(1/ k) with probability at least 1 − 100e
k . Finally, the runtime
of our learning algorithm is
O(mT |Sg ||Sl ||Ag ||Al |k |Sl ||Al | ) = Õ(|Sg |2 |Ag |2 |Al |2 |Sl |2 k 3.5+2|Sl ||Al | r̃),
which is still polynomial in k, proving the claim.

H

Generalization to Stochastic Rewards

Suppose we are given two families of distributions, {Gsg ,ag }sg ,ag ∈Sg ×Ag
{Lsi ,sg ,ai }si ,sg ,ai ∈Sl ×Sg ×Al . Let R(s, a) denote a stochastic reward of the form
X
R(s, a) = rg (sg , ag ) +
rl (si , sg , ai ),

and

(37)

i∈[n]

where the rewards of the global agent rg emerge from a distribution rg (sg , ag ) ∼ Gsg ,ag , and the
rewards of the local agents rl emerge from a distribution rl (si , sg , ai ) ∼ Lsi ,sg ,ai . For ∆ ⊆ [n],
let R∆ (s, a) be defined as:
X
R∆ (s, a) = rg (sg , ag ) +
rl (si , sg , ai )
(38)
i∈∆

We make some standard assumptions of boundedness on Gsg ,ag and Lsi ,sg ,ai .
Assumption H.1. Define
Ḡ =

[

supp Gsg ,ag



(sg ,ag )∈Sg ×Ag

L̄ =

[

supp Lsi ,sg ,ai



(si ,sg ,ai )∈Sl ×Sg ×Al

where for any distribution D, supp(D) is the support (set of all random variables D thatbcan be sam


pled bwith probability strictly larger than 0) of D. Then,
b let Ĝ = sup
b Ḡ , L̂ = sup L̄ , Gb=binf Ḡ ,

and L = inf L̄ . We assume that Ĝ < ∞, L̂ < ∞, G > −∞, L > −∞, and that Ĝ, L̂, G, L are all
known in advance.
random
Definition H.2. Let the randomized empirical adapted Bellman operator be T̂k,m
such that:
random t
T̂k,m
Q̂k,m (sg , s1 , Fz∆ , ag , a1 ) = R∆ (s, a) +

γ X
max Q̂tk,m (sℓg , sℓj , Fzℓ , aℓj , aℓg ), (39)
∆\j
a′ ∈A
m
ℓ∈[m]

SUBSAMPLE-MFQ: Learning with Stochastic Rewards. The proposed algorithm averages Ξ
random
samples of the adapted randomized empirical adapted Bellman operator, Tk,m
and updates the
random
Q̂k,m function using with the average. One can show that Tk,m is a contraction operator with
random
module γ. By Banach’s fixed point theorem, Tk,m
admits a unique fixed point Q̂random
k,m .
45

Algorithm 4 SUBSAMPLE-MFQ: Learning with Stochastic Rewards
Require: A multi-agent system as described in Section 2. Parameter T for the number of iterations
in the initial value iteration step. Sampling parameters k ∈ [n] and m ∈ N. Discount parameter
γ ∈ (0, 1). Oracle O to sample s′g ∼ Pg (·|sg , ag ) and si ∼ Pl (·|si , sg , ai ) for all i ∈ [n].
˜ = {2, . . . , k}.
1: Set ∆
1
2
2: Set µk−1 (Zl ) = {0, k−1
, k−1
, . . . , 1}|Sl |×|Al | .
3: Set Q̂0k,m (sg , s1 , Fz∆˜ , ag , a1 ) = 0, for (sg , s1 , Fz∆˜ , a1 , ag ) ∈ Sg × Sl × µk−1 (Zl ) × Ag × Al .
4: for t = 1 to T do
5:
for (sg , s1 , Fz∆˜ , ag , a1 ) ∈ Sg × Sl × µk−1 (Zl ) × Ag × Al do
6:
ρ=0
7:
for ξ ∈ {1, . . . , Ξ} do
random t
8:
ρ = ρ + T̂k,m
Q̂k,m (sg , s1 , Fz∆˜ , ag , a1 )
t+1
9:
Q̂k,m (sg , s1 , Fz∆˜ , ag , a1 ) = ρ/Ξ
10: For all (sg , si , Fs∆\i ) ∈ Sg × Sl × µk−1 (Sl ), let
est
π̂k,m
(sg , si , Fs∆\i ) =

arg max
(ag ,ai ,Fa∆\i )∈Ag ×Al ×µk−1 (Zl )

Q̂Tk,m (sg , si , Fz∆\i , ag , ai ).

Theorem H.3 (Hoeffding’s Theorem [Tsybakov, 2008]). Let X1 , . . . , Xn be independent random
variables such that ai ≤ Xi ≤ bi almost surely. Then, let Sn = X1 + · · · + Xn . Then, for all ϵ > 0,


2ϵ2
P[|Sn − E[Sn ]| ≥ ϵ] ≤ 2 exp − Pn
(40)
2
i=1 (bi − ai )
est
Lemma H.4. When πk,m
is derived from the randomized empirical value iteration operation and
applying our online subsampling execution in Algorithm 2, we get



1
1
π∗
π̃k,m
√ .
Pr V (s0 ) − V
(s0 ) ≤ Õ √
≥1−
(41)
k
100 k

Proof.
!
ϵi
2ϵ2 b b
Pr
− E[R∆ (s, a)] ≥
≤ 2 exp − PΞ
2
Ξ
Ξ
i=1 |Ĝ + L̂ − G − L|
!
2ϵ2 b b
= 2 exp −
Ξ|Ĝ + L̂ − G − L|2
Rearranging this, we get:
b b 

s
 
ρ
2 |Ĝ + L̂ − G − L|2 
Pr  − E[R∆ (s, a)] ≤ ln
≥1−δ
(42)
Ξ
δ
2Ξ2
b b
q
√
1√
Then, setting δ = 100 k , and setting Ξ = 10|Ĝ + L̂ − G − L|k 1/4 ln(200 k) gives:


ρ
1
1
√
Pr
− E[R∆ (s, a)] ≤ √
≥1−
Ξ
200k
100 k
Then, applying the triangle inequality to ϵk,m allows us to derive a probabilistic bound on the
∗
√ 1
optimality gap between Q̂est
k,m and Q , where the gap is increased by 200k , and where the randomness
h ρ

est

is over the stochasticity of the rewards. Then, the optimality gap between V π∗ and V πk,m , for
est
when the policy π̂k,m
is learned using Algorithm 4 in the presence of stochastic rewards obeying
Theorem H.1 follows



1
1
π∗
π̃k,m
√
√ ,
Pr V (s0 ) − V
≥1−
(43)
(s0 ) ≤ Õ
k
100 k
which proves the lemma.

46

Remark H.5. First, note that that through the naïve averaging argument, the optimality gap above
still decays to 0 with probability decaying to 1, as k increases.
b b
Remark H.6. This method of analysis could be strengthened by obtaining estimates of Ĝ, L̂, G, L
and using methods from order statistics to bound the errors of the estimates and use the deviations
between estimates to determine an optimal online stopping time [Kleinberg, 2005] as is done in the
online secretary problem. This, more sophisticated, argument would become an essential step in
converting this to a truly online learning, with a stochastic approximation scheme. Furthermore,
one could incorporate variance-based analysis in the algorithm, where we use information derived
from higher-order moments to do a weighted update of the Bellman update [Jin et al., 2024], thereby
taking advantage of the skewedness of the distribution, and allowing us the freedom to assign an
optimism/pessimism score to our estimate of the reward.

I

Partially Relaxing the Offline Learning assumption: Off-Policy Learning

A limitation of the planning Algorithm 1 is that it learns Q̂∗k in an offline manner by assuming a
generative oracle access to the transition functions Pg , Pl , and reward function r(·, ·). In certain
realistic RL applications, such a generative oracle might not exist, and it is more desirable to perform
off-policy learning where the agent continues to learn in an offline manner but from historical data
Fujimoto et al. [2019]. In this setting, the agents learn the target policy π̂k∗ using data generated by a
different behavior policy (the strategy it uses to explore the environment). There is a significant body
of work on the theoretical guarantees in off-policy learning Chen et al. [2021b], Chen and Maguluri
[2022b], Chen et al. [2021a, 2025].
In fact, these previous results are amenable to transforming guarantees about offline Q-learning to
off-policy Q-learning, at the cost of log |Sg ||Sl |k |Ag ||Al |k factors in the runtime. Therefore, this
section is devoted to showing that our previous result satisfy the assumptions of transforming offline
Q-learning to off-policy Q-learning for the subsampled Q̂k -function,
and we further show that, in
√
expectation, can maintain the decaying optimality gap of Õ(1/ k) of the learned policy πk , where
the randomness is over the heuristic exploration policy πb .
The off-policy Q̂k -learning algorithm is an iterative algorithm to estimate the optimal Q̂k -function
as follows: first, a sample trajectory {(sg , s∆ , ag , a∆ )} is collected using a suitable behavior policy
πk,b . For simplicity of notation, let S∆ = (sg , s∆ ) and A∆ = (ag , a∆ ). Then, initialize Q̂0k :
|Sg ||Sl |k |Ag ||Al |k → R and let α > 0 be determined later. For each t ≥ 0 and state-action pair
′
(S∆ , A∆ ) that is updated to S∆
, the iterate Q̂tk (S∆ , A∆ ) is updated by
!
t
Q̂t+1
k (S∆ , A∆ ) = (1 − α)Q̂k (S∆ , A∆ ) + αt

r(S∆ , A∆ ) + γ

max

A′∆ ∈Ag ×Ak
l

′
Q̂tk (S∆
, A′∆ ) . (44)

Note that the update in Equation (44) does not include an expectation and can be computed in a
single trajectory via historical data. We make the following ergodicity assumption:
Assumption I.1. The behavior policy πb satisfies πb (A∆ |S∆ ) > 0 for all (S∆ , A∆ ) ∈ Sg × Slk ×
(t)
Ag × Akl and the Markov chain MS∆ = {S∆ }t≥0 induced by πb is irreducible and aperiodic
with stationary distribution µ and mixing time tδ (M) = min{t ≥ 0 : maxS∆ ∈Sg ×Slk ∥P t (S∆ , ·) −
µ(·)∥T V ≤ δ}. There are many heuristics of such behavior policies [Fujimoto et al., 2019].
Theorem I.2. Let πk be the policy learned through off-policy Q̂k -learning. Then, under Theorem I.1,
1
we have that that with probability at least 1 − 100e
k,
s
r
1
∗
r̃
n−k+1
40r̃|Sl ||Al ||Ag |k |Al |+ 2
1
E[V π (s0 ) − V πk (s0 )] ≤
ln
+ √ + 2ϵk,m
(1 − γ)2
2nk
(1 − γ)2
10 k
√
= Õ(1/ k),
where the randomness in the expectation is over the stochasticity of the exploration policy πb .

47

Proposition I.3. Recall that the following are true:
1
′
′
′ , FA )∥ ≤
1. ∥Q̂k (FS∆ , FA∆ ) − Q̂k (FS∆
∆
1−γ ∥rl (·, ·)∥∞ · ∥FS∆ − FS∆ ∥1 , for any S∆ , S∆ ∈
Sg × Akl and A∆ ∈ Ag × Akl by Theorem E.3,
r̃
2. ∥Q̂k ∥ ≤ 1−γ
by Theorem C.4,
t+1
t
t
3. ∥Q̂t+1
k (S∆ , A∆ ) − Q̃k (S∆ , A∆ )∥∞ ≤ γ∥Q̂k − Q̃k (S∆ , A∆ )∥∞ by Theorem C.6,

4. The Markov chain MS enjoys a rapid mixing property from Theorem I.1,
Then, by treating the single trajectory update of the Q̂k -function as a noisy addition to the expected
update from the ideal Bellman operator, Chen et al. [2021b] uses Markovian stochastic approximation
to bound E[∥Q̂Tk − Q̂∗k ∥2∞ ]. We restate their result:
Theorem I.4 (Theorem 3.1 in Chen et al. [2021b] adapted to our setting). Suppose αt = α for all
2
1)
t ≥ 0, where α is chosen such that αtα (MS∆ ) ≤ cQ,0 log |Sg(1−β
, where cQ,0 is a numerical
||Slk ||Ag |Ak
l|
constant. Then, under Theorem I.3, for all t ≥ tα (MS∆ ), we have

t−tα (MS∆ )
(1 − γ)α
log |Sg ||Sl |k |Ag ||Al |k
E[∥Q̂tk − Q̂∗k ∥2∞ ] ≤ cQ,1 1 −
αtα (MS∆ ),
+ cQ,2
2
(1 − γ)2
r̃
3r̃
where cQ,1 = 3( 1−γ
+ 1)2 , cQ,2 = 912e( 1−γ
+ 1)2 , and where the randomness in the expectation
is over the randomness of the stochasticity of the behavior/exploration policy πb .

Corollary I.5 (Corollary 3.2 in Chen et al. [2021b] adapted to our setting). To make E[∥Q̂tk −Q̂∗k ∥∞ ≤
1√
] for ϵ > 0, we need
100 k
√
10000k log2 (100 k)|Sg ||Sl |k |Ag ||Al |k log |Sg ||Ag ||Sl |k |Al |k
t > Õ(
).
(1 − γ)5
With this sample complexity, by the triangle inequality we also recover an expected-value analog of
Theorem F.8.
Corollary I.6. For δ ∈ (0, 1)2 , with probability at least 1 − δ, we have
r
ln 2|Slδ||Al | n − k + 1
∗
∗
E[Q̂k (sg , Fs∆ , ag , Fa∆ ) − Qn (sg , Fs[n] ,ag ,Fa[n] )] ≤
∥rl ∥∞ ,
1−γ
8kn
where the randomness in the expectation is over the stochasticity of the exploration policy πb .
In turn, following the argument in the proof of Theorem G.3, it is straightforward to verify that this
leads to a result on the expected performance difference using off-policy learning:
1
Corollary I.7. With probability at least 1 − 100e
k,
s
r
1
∗
n
−
k
+
1
40r̃|Sl ||Al ||Ag |k |Al |+ 2
1
r̃
π
πk
ln
+ √ + 2ϵk,m
E[V (s0 ) − V (s0 )] ≤
(1 − γ)2
2nk
(1 − γ)2
10 k
√
= Õ(1/ k),
where the randomness in the expectation is over the stochasticity of the exploration policy πb .

J

Extension to Continuous State/Action Spaces

Multi-agent settings where each agent handles a continuous state and action space find many applications in optimization, control, and synchronization.
Example J.1 (Quadcopter Swarm Drone [Preiss et al., 2017]). Consider a system of drones with
a global controller, where each drone has to chase the controller, and the controller is designed to
follow a bounded trajectory. Here, the state of each local agent i ∈ [n] is its position and velocity in
the bounded region, and the state of the global agent g is a signal on its position and direction. The
action of each local agent is a velocity vector al ∈ Al ⊂ R3 which is a bounded subset of R3 , and
the action of the global agent is a vector ag ∈ Ag ⊂ R2 which is a bounded subset of R2 .
48

Hence, this section is devoted to extending our algorithm and theoretical results to the case where the
state and action spaces can be continuous (and therefore have infinite cardinality).
Preliminaries. For a measurable space (S, B), where B is a σ-algebra on S, let RS denote the set
of all real-valued B-measurable functions on S. Let (X, d) be a metric space, where X is a set and d
is a metric on X. A set S ⊆ X is dense in X if every element of X is either in S or a limit point of
S. A set is nowhere dense in X if the interior of its closure in X is empty. A set S ⊆ X is of Baire
first category if S is a union of countably many nowhere dense sets. A set S ⊆ X is of Baire second
category if X \ S is of first category.
Theorem J.2 (Baire Category Theorem [Jin et al., 2020]). Let (X, d) be a complete metric space.
Then any countable intersection of dense open subsets of X is dense.
Definition J.3 (Linear MDP). MDP(S, A, P, r) is a linear MDP with feature map ϕ : S × A → Rd
if there exist d unknown (signed) measures µ = (µ1 , . . . , µd ) over S and a vector θ ∈ Rd such that
for any (s, a) ∈ S × A, we have
P(·|s, a) = ⟨ϕ(s, a), µ(·)⟩,
r(s, a) = ⟨ϕ(s, a), θ⟩
Assumption J.4 (Bounded features). Without loss of generality, we assume
√ boundedness of the
features: ∥ϕ(s, a)∥ ≤ 1, for all (s, a) ∈ S × A and max{∥µ(S)∥, ∥θ∥} ≤ d.
We motivate our analysis by reviewing representation learning in RL via spectral decompositions. For
instance, if P (s′ |s, a) admits a linear decomposition in terms of some spectral features ϕ(s, a) and
µ(s′ ), then the Q(s, a)-function can be linearly represented in terms of the spectral features ϕ(s, a).
Then, through a reduction from Ren et al. [2024] that uses function approximation to learn the
spectral features ϕk for Q̂k , we derive a performance guarantee for the learned policy πkest , where the
optimality gap decays with k:
Theorem J.5. When πkest is derived from the spectral features ϕk learned in Q̂k , and M is the
number of samples used in the function approximation, then



est
1
∥ϕk ∥5 log 2k 2
2γ r̃
1
201
π∗
πk
√
√ ∥ϕk ∥
√
Pr V (s) − V
(s) ≤ Õ √ +
+
≥1+
−
50k 100 k
M
k
(1 − γ) k
We assume the system is a linear MDP, where Sg and Sl are infinite compact sets. By a reduction
from Ren et al. [2024] and using function approximation to learn the spectral features ϕk for Q̂k , we
derive a performance guarantee for the learned policy πkest , where the optimality gap decays with k.
Assumption J.6. Suppose Sg ⊂ Rσg , Sl ⊂ Rσl , Ag ⊂ Rαg , Al ⊂ Rαl are bounded compact sets.
From the Baire category theorem, the underlying field R can be replaced to any set of Baire first
category, which satisfies the property that there exists a dense open subset. In particular, replace
Theorem 2.1 with a boundedness assumption.
Multi-agent settings where each agent handles a continuous state/action space find many applications
in optimization, control, and synchronization.
Example J.7 (Federated Learning). Consider a peer-to-peer learning setting [Chaudhari et al.,
2024] with n agents, where each agent possesses a common neural network architecture f (x; θ) :
RN → RM , parameterized by θ ∈ RP . Here, each local agent has a common local objective function
∆
Lq (θ) : RP → R defined using a local dataset Dq = {xqi , yq,i }D
i=1 . For instance, Lq (θ) can be the
local empirical risk function that evaluates the performance of f (x; θ) on a task involving the local
dataset Dq , where the agents cooperate to learn the optimal θ∗ that minimizes the global objective,
which is a weighted average of all local objectives. Formally, the agents collaboratively aim to find
θ∗ satisfying:
θ∗ = arg min L(θ)
θ∈RP

L(θ) =

Q
X

Q
D
X
1
1 X
∆ 1
Lq (θ) =
ℓ(f (xq,i ; θ), yq,i )
Q q=1
Q q=1 D i=1

49

Here, the global agent may act to balance the loss among agents, by ensuring that the variance on
the losses across the agents is small. If the variance on the losses of the agents is large, it could
assign a large cost to the system, and signal that the policies of the local agents must favor the mean
state, promoting convergence.
Lemma J.8 (Proposition 2.3 in Jin et al. [2020]). For any linear MDP, for any policy π, there exist
weights {wπ } such that for any (s, a) ∈ S × A, we have Qπ (s, a) = ⟨ϕ(s, a), wπ ⟩.
Lemma J.9 (Proposition A.1 in Jin et al. [2020]). For any linear MDP, for any (s, a) ∈ S × A, and
for any measurable subset B ⊆ S, we have that
ϕ(s, a)⊤ µ(S) = 1,
ϕ(s, a)⊤ µ(B) ≥ 0.
Property J.10. Suppose that there exist spectral representation µg (s′g ) ∈ Rd and µl (s′i ) ∈ Rd such
that the probability transitions Pg (s′g , |sg , ag ) and Pl (s′i |si , sg , ai ) can be linearly decomposed by
Pg (s′g |sg , ag ) = ϕg (sg , ag )⊤ µg (s′g )
Pl (s′i |si , sg , ai ) = ϕl (si , sg , ai )⊤ µl (s′i )
for some features ϕg (sg , ag ) ∈ Rd and ϕl (si , sg , ai ) ∈ Rd . Then, the dynamics are amenable to the
following spectral factorization, as in Ren et al. [2024].
Lemma J.11. Q̂k admits a linear representation.
Proof. Given the factorization of the dynamics of the multi-agent system, we have:
P(s′ |s, a) = Pg (s′g |sg , ag ) ·

n
Y

Pl (s′i |si , sg , ai )

i=1

= ⟨ϕg (sg , ag ), µg (s′g )⟩ ·

n
Y

⟨ϕl (si , sg , ai ), µl (s′i )⟩

i=1

= ϕg (sg , ag ), µg (s′g )⟩ · ⟨⊗ni=1 ϕl (si , sg , ai ), ⊗ni=1 µl (s′i )⟩
:= ⟨ϕ̄n (s, a), µ̄n (s′ )⟩
Similarly, for any ∆ ⊆ [n] where |∆| = k, the subsystem consisting of k local agents ∆ has a
subsystem dynamics given by
P(s′∆ , sg |s∆ , sg , ag , a∆ ) = ⟨ϕ̄k (sg , s∆ , ag , a∆ ), µ̄k (s′∆ )⟩.
Therefore, Q̂k admits the linear representation:
1X
′
′ ,a )
Q̂πk k (sg , Fz∆ , ag ) = rg (sg , ag ) +
rl (si , sg , ai ) + γEs′g ,s∆′ max
Q̂πk k (s′g , Fz∆
g
a′g ,a′∆
k
i∈∆
1X
rl (si , sg , ai ) + (PV πk (sg , Fs∆ , ag ))
= rg (sg , ag ) +
k
i∈∆

h
i
R
r∆ (s, a)
1 γ s′ µk (s′∆ )V πk (s′∆ )ds′∆ ,
=
∆
ϕ̄k (sg , s∆ , ag , a∆ )
proving the claim.
Therefore, ϕ̄k serves as a good representation for the Q̂k -function making the problem amenable to
the classic linear functional approximation algorithms, consisting of feature generation and policy
gradients.
In feature generation [Ren et al., 2024], we generate the appropriate features ϕ, comprising of the
local reward functions and the spectral features coming from the factorization of the dynamics. In
50

applying the policy gradient, we perform a gradient step to update the local policy weights θi and
update the new policy.
For this, we update the weight w via the TD(0) target semi-gradient method (with step size α), to get
⊤
t ⊤
t
wt+1 = wt + α(r∆ (st , at ) + γ ϕ̄k (st+1
∆ ) wt − ϕ̄k (s∆ ) wt )ϕ̄k (s∆ )

(45)

Definition J.12. Let the complete feature map be denoted by Φ : S × d → R, and let the subsampled
feature map be denoted by Φ̂k : Sg × Slk × d ∈ R, where we let
Φ̄k (1)
Φ̄k (2)
..
.



Φ̂k = 




 ∈ R|Sg |×|µk−1 (Sl )|×d


(46)

Φ̄k (|Sg | × |µk−1 (Sl )|)
Here, the system’s stage rewards are given by
r = [r(1)

r(S)] ∈ RS

...

and
rk = [rk (1), . . . , rk (|Sg | × |µk−1 (Sl )|] ∈ R|Sg |×|µk−1 (Sl )| ,
where d is a low-dimensional embedding we wish to learn.
Agnostically, the goal is to approximate the value function through the approximation


Φ̂k (1)


..
Vw ≈ Φ̂k w = 
 w ∈ span(Φ̂k )
.
Φ̂k (|Sg | × |µk−1 (Sl ))
This manner of updating the weights can be considered via the projected Bellman equation Φ̂k w =
Πµ T (Φ̂k w), where Πµ (v) = arg minz∈Φ̂k w ∥z − v∥2µ . Notably, the fixed point of the the projected
Bellman equation satisfies
−1 ⊤
w = (Φ̂⊤
Φ̂k Dµ (r + γP π Φ̂k w),
k Dµ Φ̂k )

where Dµ is a diagonal matrix comprising of µ’s. In other words, Dµ = diag(µ1 , . . . , µn ). Then,
⊤
π
(Φ̂⊤
k Dµ Φ̂k )w = Φ̂k Dµ (r + γP Φ̂k w)

In turn, this implies
π
⊤
Φ̂⊤
k Dµ (I − γP )Φw = Φ̂k Dµ r.

Therefore, the problem is amenable to Algorithm 1 in Ren et al. [2024]. To bound the error of using
linear function approximation to learn Q̂k , we use a result from Ren et al. [2024].
Lemma J.13 (Policy Evaluation Error, Theorem 6 of Ren et al. [2024]). Suppose the sample size
M ≥ log 4n
δ , where n is the number of agents and δ ∈ (0, 1) is an error parameter. Then, with
probability at least 1 − 2δ, the ground truth Q̂πk (s, a) function and the approximated Q̂k -function
(s, a) satisfies, for any (s, a) ∈ S × A,
Q̂LFA
k


h
E

Q̂πk (s, a) − Q̂LFA
(s, a)
k

i

  2k  ∥ϕ̄ ∥5

r̃


√k + 2ϵP γ ·
≤ O log
· ∥ϕ̄k ∥ ,
δ
1
−
γ


M
{z
}
|
{z
} |
statistical error

where ϵP is the error in approximating the spectral features ϕg , ϕl .

51

approximation error

Algorithm 5 SUB-SAMPLE-MFQ: Execution with weakly shared randomness
Require: A multi-agent system as described in Section 2. A distribution s0 on the initial global state
s0 = (sg , s[n] ). Parameter T ′ for the number of iterations for the decision-making sequence.
est
Hyperparameter k ∈ [n]. Discount parameter γ. Policy π̂k,m
(sg , s∆ ).
1: Sample (sg (0), s[n] (0)) ∼ s0 .
 
2: Define groups h1 , . . . , hx of agents where x := n
k and |h1 | = |h2 | = · · · = |hx−1 | = k and
|hx | = n mod k.
3: for t = 0 to T ′ − 1 do
4:
for i ∈ [x] do

est
i
5:
Let ∆i be a uniform sample of [n]\g
, and let agi (t) = [π̂k,m
(sg (t), s∆i (t))]1:k .
k−1
n

o
est
6:
Let ag (t) = majority
[π̂k,m
(sg (t), s∆i (t))]g
.
i∈[x]

Let sg (t + 1) ∼ Pg (·|sg (t), ag (t))
Let si (t + 1) ∼ Pl (·|si (t)
sg (t), ai (t)), for all i ∈ [n].
P
Get reward r(s, a) = rg (sg , ag ) + n1 i∈[n] rl (si , ai , sg )

7:
8:
9:
10:

Corollary J.14. Therefore, when πk,m is derived from the spectral features learned in Q̂LFA
, applying
k
the triangle inequality on the Bellman noise and setting δ = ϵP = √1k yields5



5

2
γ r̃
1
1
201
π∗
πk,m
2 ∥ϕ̄k ∥
√
√
Pr V (s0 )−V
+√ ·
(s0 ) ≤ Õ √ + log 2k
∥ϕ̄k ∥ ≥ 1+
−
50k 100 k
M
k
k 1−γ
Using ∥ϕ̃k ∥ ≤ 1 and simplifying gives
"
Pr V

π∗

(s0 ) − V

πk,m

(s0 ) ≤ Õ

!#

log 2k 2
1
2γ r̃
201
√ + √
√
+√
≥1−
M
k
k(1 − γ)
100 k

Remark J.15. Hence, in the continuous state/action space setting, as k → n and M → ∞,
∗
V π̃k (s0 ) → V π̃n (s0 ) = V π (s0 ). Intuitively, as k → n, the optimality gap diminishes following the
arguments in Theorem G.7. As M → ∞, the number of samples obtained allows for more effective
learning of the spectral features.

K

Towards Deterministic Algorithms: Sharing Randomness

In large distributed systems, the random sampling in our communication network may be a bottleneck
for “decentralized execution”. In light of this, we have provided a practical derandomized heuristic
where the agents can share some randomness by only sampling within pre-defined fixed blocks of size.
In this section, we propose algorithms RANDOM-SUB-SAMPLE-MFQ and RANDOM-SUB-SAMPLE-MFQ+,
which shares randomness between agents through various careful groupings and using shared randomness within each groups. By implementing these algorithms, we derive simulation results, and
establish progress towards derandomizing inherently stochastic approximately-optimal policies.
Algorithm 5 (Execution with weakly shared randomness). The local agents are heuristically divided
into arbitrary groups of size k. For each group hi , the k − 1 local agents sampled are the same at
each iteration. The global agent’s action is then the majority

global agent action determined by each
group of local agents. At each round, this requires O( n2 /k (n − k)) random numbers.
Algorithm 6 (Execution with strongly shared randomness). The local agents are randomly divided
in to groups of size k. For each group, each agent uses the other agents in the group as the k − 1
other sampled states. Similarly, the global agent’s action is then the majority global
 agent
 action
determined by each group of local agents. Here, at each round, this requires O( n2 /k ) random
numbers.
5

Following Ren et al. [2024], the result easily generalizes to any positive-definite transition Kernel noise (e.g.
Laplacian, Cauchy, Matérn, etc.

52

Algorithm 6 SUB-SAMPLE-MFQ: Execution with strongly shared randomness
Require: A multi-agent system as described in Section 2. A distribution s0 on the initial global state
s0 = (sg , s[n] ). Parameter T ′ for the number of iterations for the decision-making sequence.
est
Hyperparameter k ∈ [n]. Discount parameter γ. Policy π̂k,m
(sg , s∆ ).
1: Sample (sg (0), s[n] (0)) ∼ s0 .
 
2: Define groups h1 , . . . , hx of agents where x := n
k and |h1 | = |h2 | = · · · = |hx−1 | = k and
|hx | = n mod k.
3: for t = 0 to T ′ − 1 do
4:
for i ∈ [x − 1] do
est
5:
Let ahi (t) = [π̂k,m
(sg (t), shi (t))]1:k .
g
est
6:
Let ahi (t) = [π̂k,m (sg (t), shi (t))]g .

[n]\hx
7:
Let ∆residual := k−(n−⌈n/k⌉k)
.
est
8:
Let ahx (t) = [π̂k,m
(sg (t), shx ∪∆residual (t))]1:n−⌈n/k⌉k
est
9:
Let aghx (t) = [π̂k,m
(sg (t), shx ∪∆residual (t))]g

10:
Let ag (t) = majority {aghi (t) : i ∈ [x]} .
11:
Let sg (t + 1) ∼ Pg (·|sg (t), ag (t)), siP
(t + 1) ∼ Pl (·|si (t), sg (t), ai (t)), for all i ∈ [n].
12:
Get reward r(s, a) = rg (sg , ag ) + n1 i∈[n] rl (si , ai , sg )

The probability of a bad event (the policy causing the Q-function to exceed the Lipschitz bound)
scales with the O(nk ) independent random variables, which is polynomially large. Agnostically,
some randomness is always needed to avoid periodic dynamics that may occur when the underlying
Markov chain is not irreducible. In this case, an adversarial reward function could be designed such
that the system does not minimize the optimality gap, thereby penalizing excessive determinism.
This ties into an open problem of interest, which has been recently explored [Larsen et al., 2024].
What is the minimum amount of randomness required for SUBSAMPLE-MFQ (or general mean-field or
multi-agent RL algorithms) to perform well? Can we derive a theoretical result that demonstrates and
balances the tradeoff between the amount of random bits and the performance of the subsampled
policy when using the greedy action from the k-agent subsystem derived from Q̂∗k ? We leave this
problem as an exciting direction for future research.

53

