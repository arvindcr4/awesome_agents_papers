Paper: RLHF_Principled.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons Banghua Zhu† †  Michael I. Jordan†, ‡  Jiantao Jiao†, ‡  Department of Electrical Engineering and Computer Sciences, UC Berkeley ‡ Department of Statistics, UC Berkeley  arXiv:2301.11270v5 [cs.LG] 8 Feb 2024  Abstract We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) mod...

Specific Evidence for Classification:
- "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons Banghua Zhu† †  Michael I."
- "Jordan†, ‡  Jiantao Jiao†, ‡  Department of Electrical Engineering and Computer Sciences, UC Berkeley ‡ Department of Statistics, UC Berkeley  arXiv:2301.11270v5 [cs.LG] 8 Feb 2024  Abstract We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF)."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
