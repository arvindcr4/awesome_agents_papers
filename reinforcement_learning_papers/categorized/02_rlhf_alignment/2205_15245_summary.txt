Here's the extracted information:

**Paper:** Residual Q-Networks for Value Function Factorizing in Multi-Agent Reinforcement Learning

**Title:** Residual Q-Networks for Value Function Factorizing in Multi-Agent Reinforcement Learning

**ARXIV_ID:** 2205.15245v1 [cs.LG]

**RESEARCH_METHOD:** 02_rlhf_alignment

**METHOD_DESCRIPTION:** This paper proposes a novel method for value function factorization in multi-agent reinforcement learning (MARL) using Residual Q-Networks (RQNs). RQNs learn to transform individual Q-value trajectories to preserve the Individual-Global-Max (IGM) property, enabling robust factorization. The RQN acts as an auxiliary network that accelerates convergence and becomes obsolete as agents reach training objectives.

**KEY_CONTRIBUTIONS:**

* Proposes a novel concept of Residual Q-Networks (RQNs) for MARL
* Demonstrates the effectiveness of RQNs in factorizing value functions for cooperative MARL tasks
* Evaluates the performance of RQNs against state-of-the-art methods (QMIX, VDN, QTRAN, QPLEX, and WQMIX) in various matrix environments and Starcraft II environments
* Shows that RQNs can learn a wider family of environments than previous methods, with improved performance stability over time.
