The challenge of redundancy on multi-agent value factorisation
Extended Abstract
Siddarth Singh

Benjamin Rosman

Instadeep Ltd
Johannesburg, South Africa
siddarthsingh2@gmail.com

The University of the Witwatersrand
Johannesburg, South Africa
benjamin.rosman1@wits.ac.za

arXiv:2304.00009v1 [cs.AI] 28 Mar 2023

ABSTRACT
In the field of cooperative multi-agent reinforcement learning (MARL),
the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of
the cooperative agents based on a central state. It has been shown,
that in cases with large numbers of redundant agents these methods
become less effective. In a more general case, there is likely to be a
larger number of agents in an environment than is required to solve
the task. These redundant agents reduce performance by enlarging
the dimensionality of both the state space and and increasing the
size of the joint policy used to solve the environment. We propose
leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of
local reward signals and create a new MARL algorithm: relevance
decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number
of redundant agents, RDN is unaffected.

KEYWORDS
Machine Learning, Reinforcement Learning, Multi-Agent Reinforcement Learning, Multi-Agent Systems
ACM Reference Format:
Siddarth Singh and Benjamin Rosman. 2023. The challenge of redundancy
on multi-agent value factorisation: Extended Abstract. In Proc. of the 22nd
International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2023), London, United Kingdom, May 29 â€“ June 2, 2023, IFAAMAS,
3 pages.

1

INTRODUCTION

For most multi-agent tasks in a practical setting, we would not
know the precise number of agents required to optimally solve the
problem. In general, there is likely to be a larger number of agents
in an environment than is required to solve the task. As the number
of independent agents increases so would the size of the state space
that most problems require.
In complex environments constructing an accurate ground truth
representation becomes difficult and in practical applications, the
state representation data collected is often noisy or incomplete.
Given the limitations of this space, it is not reliable to assume
that the state space is reliable. When there is a large number of
agents, many of them may be redundant for achieving an optimal
policy. These redundant agents exacerbate the issue of the state
space growth in the multi-agent setting. Therefore it is important
to develop algorithms that can effectively separate agents that are
Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2023), A. Ricci, W. Yeoh, N. Agmon, B. An (eds.), May 29 â€“ June 2, 2023,
London, United Kingdom. Â© 2023 International Foundation for Autonomous Agents
and Multiagent Systems (www.ifaamas.org). All rights reserved.

essential to solving a task from the agents that are redundant to
allow MARL algorithms to be deployed into more realistic and
eventually real-world challenges.
We consider the idea of a collaborative task with a small margin
of error like the traditional Piano Movers problem [1]. However, we
consider the case where only ğ‘› of ğ‘š total agents in the environment
are required to effectively complete the task. In the Piano Movers
problem we therefore consider the case where an arbitrarily large
number of agents are required to re-position the piano with each
agent occupying a fixed amount of space in the environment along
with the piano. Realistically with a large number of agents only
a small subgroup ğ‘› of ğ‘š will be required to solve the task under
the joint optimal policy. Ultimately during training, the policy will
update to minimise the reliance on certain agents on the overall
outcome as the joint optimal policy only require that they do not act
in a manner that is destructive to the actions of the smaller group of
required agents. In the Qatten [2] paper the idea is put forward that
both VDN and Qmix exhibit poor performance in environments
with a high number of redundant agents as it is difficult to assign
credit for task completion accurately when a disproportionately
high credit is assigned to only a small number of the agents.
In this paper, we propose a method to resolve this problem, relevance decomposition network (RDN) which makes use of layerwise
relevance propagation (LRP) [3] as an alternative to learned value
decomposition only using local agent observations. By not using
learned decomposition we can separate the learning of the joint
value function from the training of the independent agents and
make full use of the relationships between the local observations
of the agents, their identities and their actions at each timestep to
decompose the relationship between the global and local rewards.

2

RELEVANCE DECOMPOSITION NETWORK
(RDN)

We propose RDN to perform credit assignment between ad-hoc
agents in the cooperative multi-agent setting under the assumption
of a linear relationship between the individual local rewards and
the shared global reward. However, unlike most central training
decentralised execution (CTDE) methods which use learned decomposition as an end-to-end system, RDN separates the learning of
the global reward function from the local Q-values of the agents
[4].
The main motivation for RDN is that in difficult settings, where
agents are not all similarly responsible for the total reward, Qmix
and VDN see decreased performance [5][6]. In these cases to achieve
accurate reward assignment, some agents must be weighted much
higher than others when performing value decomposition [2]. In the

case of many redundant agents, Qmix and VDN have difficulties decomposing the relationship between the high and low-value agents
[7]. Due to this, the decomposed rewards have high variance during
training which makes discovering stable policies difficult. LRP is
more effective at determining this relationship, which stabilises the
training process.
For RDN our independent agents are modelled as Deep Q Networks (DQNs) [8]. These DQNs gather data where ğ‘„ ğ‘– (ğ‘œğ‘¡,ğ‘–,. ) is
the ğ‘„ value of independent agent ğ‘– at timestep ğ‘¡, â„ğ‘–ğ‘¡ is the hidden state of agent ğ‘– at timestep ğ‘¡, ğ‘œğ‘¡,ğ‘–,. is the local observation of
agent ğ‘– at timestep ğ‘¡,ğ‘ğ‘¡,ğ‘– is the action taken by agent ğ‘– at timestep
ğ‘¡ and ğœ‹ğ‘– (ğ‘„ ğ‘– (ğ‘œğ‘¡,ğ‘–,. ), ğœ– (ğ‘’)) is the policy ğœ‹ of agent ğ‘– dictated by a
Q value function and an ğœ– âˆ’ ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ exploration strategy. We use
the critic network to calculate the expected total Q value at each
timestep parameterised by ğœƒ ğ‘ and a target Q value using a target critic parameterised by ğœƒËœğ‘ whose parameters are copied over
from the critic at fixed intervals. The critic network is updated usğœƒğ‘
2
ing loss ğ¿(ğœƒ ğ‘ ) = ğ¸ #Â»
ğ‘œ , #Â»
ğ‘ ,ğ‘Ÿ, #Â»
ğ‘œ â€² [(ğ‘„ ğ‘¡ğ‘œğ‘¡ (ğ‘œ 1 , ..., ğ‘œğ‘› , ğ‘ 1 , ..., ğ‘ğ‘› ) âˆ’ğ‘¦) ] where
â€²ğ‘
â€²
â€²
â€²
â€²
ğœƒ
ğ‘
ğ‘¦ = ğ‘Ÿ + ğ›¾ (ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‘œ 1, ..., ğ‘œğ‘› , ğ‘ 1, ..., ğ‘ğ‘› )) and ğœƒ is the criticâ€™s parameâ€²
ters and ğœƒ ğ‘ is the target critic parameters, which are reset every
ğ¶ training epochs. To train the agents, we calculate the Q value at
the current timestep for each independent agent. This is done in
the same manner as standard independent Q-learning where we
calculate ğ‘„ (ğ‘œ, ğ‘ğ‘– ) for each agent based on their local observations
and actions. We calculate the total target Q-value for the current
timestep ğ‘¡ using a separate critic network. This critic takes in the
concatenation of local agent observations and actions to predict
the target total Q-value of the global state at the current timestep
then we use LRP to decompose the global reward calculated into
local rewards for each agent. Essentially we assign relevance scores
to all data points in the concatenated observation space and then
separate the scores for each index based on the agent they were
collected or generated from. The concatenated observation space is
a concatenation of the local agent observations and the actions of
each agent at each timestep. The relevance scores for each agent
are then summed per agent and are used as the target Q-values
to train the independent learners as if performing standard independent Q learning. Finally, the total expected Q value for each
timestep is decomposed into independent target Q-values ğœƒËœğ‘– and the
DRQNs [9] which act as the agent networks are trained using the
ğ‘–,ğœƒ ğ‘– (ğ‘œ , ğ‘ ) âˆ’ ğ‘„Ëœ ğ‘– ) 2 ] [10] and ğ‘„Ëœ ğ‘– = Ã ğ‘…
loss ğ¿(ğœƒ ğ‘– ) = ğ¸ #Â»
ğ‘– ğ‘–
ğ‘œ , #Â»
ğ‘ ,ğ‘Ÿ, #Â»
ğ‘œ â€² [(ğ‘„
ğ‘– ğ‘–ğ‘›
Ã
where ğ‘„Ëœ ğ‘– is the ğ‘„ target for agent ğ‘– and ğ‘– ğ‘…ğ‘–ğ‘› is the sum of all
relevance values associated with agent ğ‘–.
A characteristic of LRP is it maintains a conservative calculation
between layers of the neural network [3]. As such the relevance
values from later layers are included completely in the calculation
of the relevance values of the layers closer to the input. Essentially
we can assume that the total sum of the relevance values is approximately the same as the output of the NN when LRP is used.
Therefore we can equate ğ‘„ğ‘¡ğ‘œğ‘¡ to the sum of relevance scores as
ğ‘„ğ‘¡ğ‘œğ‘¡ â‰ˆ ğ‘…ğ‘–ğ‘› .

We show how increasing numbers of redundant agents makes reaching stable convergence to an optimal joint policy difficult for both
monotonic factorisation methods like QMIX and linear factorisation methods like VDN where only ğ‘› of ğ‘š total agents are required
for an environment to be solved. We then propose RDN as a method
that uses LRP to perform more optimal credit assignments in environments with high numbers of redundant agents using only local
agent observation. RDN can reach near-optimal convergence on all
environments used with similar overall winrates without the use of
the ground truth state information. With VDN and QMIX we see a
gradual decay in performance and, an increase in variance as the
number of redundant agents increases.

3

REFERENCES

RESULTS

The pre-existing map from the Starcraft Multi-Agent Challenge
(SMAC) we make use of is the bane_vs_bane map. In this map,
each side has 20 zerglings and 4 banelings. The most optimal policy

for this environment has the zerglings move out of the way to not
obstruct the banelingsâ€™ movement. We make use of 3 additional
variants of the original map. bane_med which only has 15 zerglings,
bane_small with has 10 zerglings and bane_no_z with has no zerglings. We vary the number of redundant agents (the zerglings) to
show the effect of redundancy on performance.
From figure 1 we can see that across all maps RDN outperforms
all baselines although the performance of VDN improves as the
number of redundant agents is reduced. Interestingly QMIX is
outperformed by QMIX_NS which does not use the global state
in bane_med and in bane _no_z indicating that in cases where
there are an intermediate number of redundant agents the central
state already begins to become uninformative making accurate
multi-agent credit assignment difficult.
In the case where all redundant agents are removed VDN performs similarly to RDN however, we also find that when no redundant agents are present at all in the bane _no_z map performance
decreased across all algorithms when compared to bane_small. We
suspect that having a small number of extra agents may aid in
exploration early in the training regime.

Figure 1: Percentage winrates from highest number of redundant agents (left) to least redundant agents (right) for
all tested algorithms

4

CONCLUSION

[1] J. T. Schwartz and M. Sharir, â€œOn the â€œpiano moversâ€™â€ problem i. the case of
a two-dimensional rigid polygonal body moving amidst polygonal barriers,â€
Communications on pure and applied mathematics, vol. 36, no. 3, pp. 345â€“398,
1983.

[2] Y. Yang, J. Hao, B. Liao, K. Shao, G. Chen, W. Liu, and H. Tang, â€œQatten: A general
framework for cooperative multiagent reinforcement learning,â€ arXiv preprint
arXiv:2002.03939, 2020.
[3] G. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. MÃ¼ller, â€œLayer-wise
relevance propagation: an overview,â€ Explainable AI: interpreting, explaining and
visualizing deep learning, pp. 193â€“209, 2019.
[4] K. Zhang, Z. Yang, and T. BaÅŸar, â€œMulti-agent reinforcement learning: A selective
overview of theories and algorithms,â€ Handbook of Reinforcement Learning and
Control, pp. 321â€“384, 2021.
[5] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster, and S. Whiteson,
â€œQmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning,â€ in ICML, 2018.
[6] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel, â€œValuedecomposition networks for cooperative multi-agent learning based on team

reward,â€ in Proceedings of the 17th International Conference on Autonomous Agents
and MultiAgent Systems, ser. AAMAS â€™18. Richland, SC: International Foundation
for Autonomous Agents and Multiagent Systems, 2018, p. 2085â€“2087.
[7] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J. Rudner,
C.-M. Hung, P. H. S. Torr, J. Foerster, and S. Whiteson, â€œThe StarCraft Multi-Agent
Challenge,â€ CoRR, vol. abs/1902.04043, 2019.
[8] S. M. Schulze C., â€œVizdoom: Drqn with prioritized experience replay, doubleq learning and snapshot ensembling,â€ in Intelligent Systems Conference 2018
(IntelliSys 2018). London: Springer, Cham, 6-7 Sept 2018.
[9] M. Hausknecht and P. Stone, â€œDeep recurrent q-learning for partially observable
mdps,â€ arXiv preprint arXiv:1507.06527, 2015.
[10] T. W. Sandholm and R. H. Crites, â€œOn multiagent q-learning in a semi-competitive
domain,â€ in International Joint Conference on Artificial Intelligence. Springer,
1995, pp. 191â€“205.

