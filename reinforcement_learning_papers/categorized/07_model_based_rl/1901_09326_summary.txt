Here is the extracted information in the requested format:

**PAPER:** Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning
**TITLE:** Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning
**ARXIV_ID:** 1901.09326v4
**RESEARCH_METHOD:** 07_model_based_rl
**METHOD_DESCRIPTION:** The authors propose a decentralized multi-agent reinforcement learning algorithm called Value Propagation, which uses a primal-dual decentralized optimization method to learn the value function and policy of each agent in a networked setting. The algorithm is based on the concept of softmax temporal consistency and uses a two-step update rule to update the policy and value function of each agent.
**KEY_CONTRIBUTIONS:**
* The authors propose a decentralized multi-agent reinforcement learning algorithm that can learn the value function and policy of each agent in a networked setting.
* The algorithm uses a primal-dual decentralized optimization method to update the policy and value function of each agent.
* The authors provide a convergence analysis of the algorithm and show that it converges to a stationary solution with a rate of O(1/T).
* The algorithm is evaluated on a cooperative navigation task and shows better performance than other decentralized multi-agent reinforcement learning algorithms.
