1

MASH: Cooperative-Heterogeneous Multi-Agent
Reinforcement Learning for Single Humanoid
Robot Locomotion

arXiv:2508.10423v1 [cs.RO] 14 Aug 2025

Qi Liu1† , Xiaopeng Zhang2† , Mingshan Tan2 , Shuaikang Ma2 , Jinliang Ding1 , Yanjie Li2∗

Abstract—This paper proposes a novel method to enhance
locomotion for a single humanoid robot through cooperativeheterogeneous multi-agent deep reinforcement learning (MARL).
While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or
MARL algorithms for multi-robot system tasks, we propose a
distinct paradigm: applying cooperative-heterogeneous MARL to
optimize locomotion for a single humanoid robot. The proposed
method, multi-agent reinforcement learning for single humanoid
locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot’s action space while
sharing a global critic for cooperative learning. Experiments
demonstrate that MASH accelerates training convergence and
improves whole-body cooperation ability, outperforming conventional single-agent reinforcement learning methods. This work
advances the integration of MARL into single-humanoid-robot
control, offering new insights into efficient locomotion strategies.
Index Terms—Robot control, humanoid robot locomotion,
reinforcement learning (RL), multi-agent RL.

I. I NTRODUCTION

D

EEP reinforcement learning (RL) has achieved remarkable success in various domains of robotic control [1],
[2], such as quadruped locomotion [3], [4], bipedal walking
[5], [6], and autonomous aerial vehicle navigation [7]. This
work focuses on advancing deep RL for humanoid robot
locomotion, a complex yet critical challenge in robotics.
The locomotion methodologies for humanoid robots can
be categorized into two principal paradigms: model-based
locomotion methods and model-free learning-based methods.
The former, exemplified by whole-body dynamics locomotion
methods incorporating model predictive control and trajectory
optimization [8]–[10], demonstrates robust performance in
structured environments but relies on precise dynamic modeling. The latter encompasses deep reinforcement learning
(RL) [11], [12] and imitation learning [13]. Recent advances
[14]–[16] have revealed the superior policy generalization
capabilities of learning-based methods over model-based locomotion methods. Learning-based locomotion methods can
This work was supported by Shenzhen Basic Research Program (Grant
No. JCYJ20220818102415033, KJZD2023092311422045). (Corresponding
author: Yanjie Li, autolyj@hit.edu.cn)
1 Faculty of Robot Science and Engineering, Northeastern University,
Shenyang, 110819, China.
2 Guangdong Key Laboratory of Intelligent Morphing Mechanisms and
Adaptive Robotics and School of Intelligence Science and Engineering, the
Harbin Institute of Technology Shenzhen, Shenzhen, 518055, China.
† These authors contributed equally to this work.

be summarized into two classes: (1) phased policy training
and system integration for the upper and lower body, and (2)
whole-body motion via imitation learning or single-agent deep
RL.
Phased policy training and system integration methods
employ a modular decomposition and progressive integration
framework, mitigating locomotion complexity in high-degreeof-freedom (DoF) systems. Despite significant advancements,
this class method faces persistent challenges: Phased training
may lead to insufficient coordination between upper-body
and lower-body policies, limiting whole-body synergy. And
its adaptability remains constrained for complex tasks and
scenarios [15], [17].
Whole-body imitation learning and single-agent deep RL
methods [18] begin by collecting or generating whole-body
motion data for humanoid robots, then use the acquired data to
train locomotion policies through imitation learning or singleagent deep RL. However, obtaining high-quality humanoid
motion data faces significant challenges, including high collection costs, low efficiency, and complex post-processing
requirements. Although imitation learning-based methods are
straightforward, the resulting policies exhibit limited generalization capabilities. Compared to imitation learning, deep
RL-based methods show improved generalization but fail to
leverage deep RL’s trial-and-error learning mechanism fully.
Because these methods rely on human motion data as reference
trajectories, they struggle to explore action and skill spaces
beyond the demonstrated data, ultimately constraining their
performance ceiling in complex environments.
When applying deep RL to single-humanoid robot locomotion, the predominant method employs single-agent deep RL
algorithms [19], [20]. However, these methods may exhibit
limitations in addressing coordination challenges inherent in
complex robotic systems. Existing solutions typically utilize
either single-agent RL for individual robots or multi-agent
deep reinforcement learning (MARL) for cooperative multirobot tasks [21]. However, there remains an unexplored potential in leveraging MARL principles for enhanced coordination
within a single humanoid robotic entity. Thus, developing
more efficient whole-body cooperative locomotion methods to
enhance humanoid robots’ mobility-manipulation coordination
in unstructured environments remains challenging.
Cooperative MARL algorithms have demonstrated remarkable success in multi-agent cooperation in game artificial
intelligence domains [22], [23]. Liu et al. [24] propose a novel
method to improve locomotion learning for a single quadruped

2

robot using MARL. Departing from conventional robot locomotion learning approaches, this paper proposes a novel
method that models locomotion learning as a cooperativeheterogeneous MARL problem and uses the cooperativeheterogeneous MARL algorithm to enhance a single humanoid
robot’s locomotion. Our method enables superior coordination
in complex tasks by treating each limb (two arms and two
legs) as an independent agent within a cooperative MARL
framework. The proposed cooperative-heterogeneous multiagent reinforcement learning for single humanoid locomotion
(MASH) leverages a shared learning structure, where agents
(limbs) collectively optimize locomotion through experience
sharing and cooperative policy learning.
The main contributions of this paper are as follows:
• We propose MASH, a novel framework that reformulates
humanoid locomotion as a cooperative-heterogeneous
MARL problem. MASH enables more efficient coordination learning than conventional single-agent RL by
treating each limb (arms and legs) as an independent
agent with distinct action spaces.
• Experimental results show that the proposed method
achieves superior gait execution and final performance,
improves training efficiency and sample complexity, and
enhances robustness in dynamic environments. These
results validate the effectiveness of applying MARL
principles to single humanoid robot control.
The remainder of this paper is structured as follows.
Section II reviews related works. Section III introduces the
preliminaries of the Markov decision process (MDP) and RL.
Section IV presents the proposed MASH method in detail.
Section V reports the experimental results that demonstrate
the effectiveness of the proposed approach. Finally, Section VI
concludes the paper and outlines directions for future work.
II. R ELATED WORK
A. Deep RL for Humanoid Robot Locomotion
The locomotion methodologies for humanoid robots can
be categorized into two principal paradigms: model-based
locomotion methods and model-free learning-based methods.
Model-based locomotion methods, exemplified by whole-body
dynamics locomotion methods incorporating model predictive
control and trajectory optimization [8]–[10], demonstrate robust performance in structured environments but rely on precise dynamic modeling. Model-free learning-based methods
encompass deep reinforcement learning (RL) [11], [12] and
imitation learning [13]. Recent advances [14]–[16] have shown
the superior policy generalization capabilities of learningbased methods over model-based methods. Learning-based
locomotion methods can be summarized into two classes: (1)
phased policy training and system integration for the upper and
lower body, and (2) whole-body locomotion learning based on
imitation learning or single-agent deep RL.
Phased policy training and system integration methods
employ a modular decomposition and progressive integration
framework, mitigating locomotion complexity in high degrees
of freedom (DoFs) systems. This class method consists of
three stages: (1) Decoupled policy training: The whole-body

control problem is partitioned into independent policy training
for the lower body (focused on locomotion stability) and
the upper body (emphasizing manipulation dexterity) [25].
(2) System integration: Upper-body manipulation tasks are
gradually incorporated after the lower-body policy converges
[26]. (3) Task-oriented optimization: Task constraints (e.g.,
end-effector trajectory tracking) are superimposed onto foundational mobility, with whole-body control achieved through
coupled dynamic optimization [1], [27]. Despite significant
advancements, this class method faces persistent challenges:
Decoupled policy training may lead to insufficient coordination between upper-body and lower-body policies, limiting
whole-body synergy, and its adaptability remains constrained
for complex tasks and scenarios [15], [17]. Thus, developing
more efficient whole-body cooperative locomotion methods to
enhance humanoid robots’ mobility-manipulation coordination
remains challenging.
Whole-body locomotion learning based on imitation
learning or single-agent deep RL methods begin by collecting or generating whole-body motion data for humanoid
robots [18], then use the acquired data to train locomotion
policies through imitation learning or single-agent deep RL.
Based on data sources, this class method can be categorized
into four types: (1) Robot teleoperation data collection [18],
[28]: Involves direct operation of physical robots to acquire
kinematically accurate motion data. Although this yields highquality, platform-specific data, it suffers from high acquisition
costs and hardware dependence. (2) Human motion capture
data: Methods like HumanPlus [29] record human motions
using mocap systems and map them to the robot joint space.
Although intuitive, this approach requires specialized equipment, incurs scalability challenges, and must address humanrobot kinematic discrepancies. (3) Internet-sourced human
video data: OKAMI [30] extracts human motions from online
videos, performs 3D reconstruction, and transfers them to
robots. Although data is abundant, this method struggles
with noisy inputs and kinematic mismatches. (4) Synthetic
animation data: Works like OmniH2O [31] and PMCP [32]
generate motions through animation tools and remap them to
robots. Although flexible, such data may lack physical realism.
Furthermore, obtaining high-quality humanoid motion data
faces significant challenges, including high collection costs,
low efficiency, and complex post-processing requirements.
Although imitation learning-based control methods are relatively straightforward, the resulting policies exhibit limited
generalization capabilities. Compared to imitation learning,
deep RL-based methods show improved generalization but fail
to leverage deep RL’s trial-and-error learning mechanism fully.
Because these methods rely on human motion data as reference
trajectories, they struggle to explore action and skill spaces
beyond the demonstrated data, ultimately constraining their
performance ceiling in complex environments.
Other methods: [33] employs traditional trajectory optimization to generate reference trajectories for robots, combined with single-agent deep RL for trajectory tracking control. Although this approach has shown success in quadruped
robotics, its application to humanoid robots remains constrained due to their higher DoF and more complex dynamic

3

constraints. Some methods employ extensive reward shaping
to guide policy learning, including hand-tuned motion tracking
[34], periodic reward composition [35], adversarial motion
priors [36], and periodic sinusoidal trajectories [37]. However,
designing reward functions manually is often labor-intensive
and time-consuming, requiring extensive domain expertise and
iterative tuning to achieve desired behaviors.

where ψ denotes the parameters of value function (Vψ ) network, ϵ denotes a coefficient. The policy parameters θ are
updated as follows:
θ ← θ + α∇θ LCLIP (θ)

(5)

PPO’s constrained updates stabilize training and improve
performance, making it practical for complex single-agent RL
tasks.

B. MARL for Multi-robot Control
MARL has demonstrated remarkable success across diverse
multi-robot systems, including cooperation robot swarms [38],
autonomous driving [39], unmanned aerial vehicles coordination [40], and intelligent warehouse [41]. These applications
showcase MARL’s capacity to coordinate complex behaviors
in physically embodied systems, where decentralized decisionmaking must reconcile environmental constraints with interagent coordination. In contrast to these multi-robot applications, this paper presents a novel paradigm by formulating
single-humanoid locomotion as a cooperative MARL problem. Our method treats each leg and arm as an independent
agent, departing from conventional methods that either model
the robot as a single unified agent or focus on multi-robot
cooperation.

IV. M ULTI -AGENT R EINFORCEMENT L EARNING FOR
S INGLE H UMANOID ROBOT L OCOMOTION
This paper proposes a novel MARL framework to enhance
single-robot locomotion by leveraging inter-limb coordination.
Our approach treats each limb (arms and legs) of the humanoid robot as an independent agent within a cooperativeheterogeneous multi-agent system, where agents share a global
critic while maintaining individual observations and policies.
This framework bridges the gap between single humanoid
robot locomotion learning and multi-agent coordination learning, demonstrating that cooperative multi-agent learning strategies can improve a single humanoid robot’s locomotion.

III. BACKGROUND
This section summarizes the MDP [42] and RL. The
MDP considered in this paper is modeled as a tuple
(S, A, P, R, γ, T ), where S is the state space, A is the action
space, P : S × A × S → [0, 1] represents the state transition
probability. R : S × A → R is the reward function, γ ∈ [0, 1)
is the discount factor, and T denotes the time horizon. At
each timestep, t, an action at ∈ A is selected according to a
policy. The agent then transitions to the next state by sampling
from p (st+1 | st , at ), where p ∈ P, and receives a scalar
reward r (st , at ) ∈ R. The agent continues interacting with
the environment until it reaches a terminal state. The goal of
RL is to learn a policy π : S × A → [0, 1] that maximizes the
expected discounted cumulative rewards. For any policy π, the
state-action value function (Q function) is defined as follows:
" T
#
X
π
π
t
Q (s, a) = E
γ r (st , at ) | s0 = s, a0 = a
(1)
t=0

Proximal Policy Optimization (PPO) [43] addresses key
stability challenges in policy gradient methods through a
constrained optimization approach. The algorithm’s core innovation lies in its clipped surrogate objective function:
h

i
LCLIP (θ) = Et min rt (θ)Ât , clip(rt (θ), 1 − ϵ, 1 + ϵ)Ât
(2)
where
πθ (at |st )
(3)
rt (θ) =
πθold (at |st )
and
Ât = Qπ (st , at ) − Vψ (st )

(4)

Fig. 1. MARL model for a single humanoid robot’s locomotion

A. MASH
This paper models a single humanoid robot locomotion as
a cooperative multi-agent problem, which is described as a
partially observable decentralized Markov decision process
(decPOMDP) [44]. The decPOMDP is defined by the tuple
G = (S, A, P, r, Z, O, N, γ, T ). S is the state space, A is
the action space, P is the state transition distribution, r is
the reward function, Z is the observation space, O is the
observation function, N is the number of agents, γ is the
discount factor, and T is the time horizon. At each time step t,
each agent n ∈ {1, . . . , N } selects an action ant ∈ A, resulting
in a joint action at = {a1t , a2t , . . . , aN
t }. The environment
transitions to a new state st+1 according to P(st+1 |st , at )
and provides a shared reward r(st , at ). Each agent receives
an observation ztn from O(st , n) and maintains an observationaction history τtn . MARL aims to learn policies {π n }N
n=1 that
maximize expected cumulative rewards:
" T
#
X
t
J(π) = E
γ r(st , at )
(6)
t=0

4

Fig. 2. The framework of MASH

This paper proposes MASH, which applies the MARL
algorithm to treat different parts of a single humanoid robot
as independent agents, trained collaboratively using shared
rewards. Specifically, this paper uses multi-agent PPO [45]
(MAPPO) to solve the modeled multi-agent problem. MAPPO
optimizes the following objective function in a multi-agent
system:
LCLIP
MAPPO (θ) =

n
X

h

Et min rti (θi )Ât ,
(7)

i=1

clip(rti (θi ), 1 − ϵ, 1 + ϵ)Ât
where
rti (θi ) =

πθi (ait |oit )
πθi,old (ait |oit )

i

(8)

where i denotes the i-th agent in MARL. Each agent updates
its policy parameters as follows:
θi ← θi + α∇θi LCLIP
(θi )
i

(9)

This work adopts the centralized training with decentralized
execution (CTDE) paradigm [46] to address key challenges in
MARL while ensuring training stability in dynamic environments. The implementation employs dual neural networks that
separately learn a policy (πθ ) and an enhanced value function
(Vψ (s)), where the latter incorporates global state information
to improve training stability. By leveraging this architecture,
the proposed approach can achieve superior final performance,

accelerated convergence rates, and enhanced deployment robustness compared to conventional methods, demonstrating
particular efficacy for complex robotic control tasks requiring
both coordinated learning and operational flexibility.
State Space and Observation: A shared-parameter actor network is constructed for both the bipedal and dualarm systems, respectively, receiving concatenated observations
from the two agents. For the bipedal system, each agent’s
observation includes: motor positions qt ∈ R6 (representing
the angles of the 6 leg motors), motor velocities q˙t ∈ R6
(representing the rotational speeds of the motors), the action
from the previous time step at ∈ R6 , trigonometric timing
guidance ϕt ∈ R2 for gait sequencing (which regulates the
timing of steps), the Euler angles θt ∈ R3 and angular
velocities ωt ∈ R3 of the torso, control commands ct ∈ R4
provided by the remote controller (including a binary standing
label, velocities in the x and y directions, and the yaw angular
velocity), a one-hot encoding et ∈ R2 for each agent. Thus,
each agent has a 32-dimensional observation, resulting in a
total input dimension of oactor
∈ R64 (32 × 2) for the actor
t
network. For the dual-arm system, there are only 4 degrees of
freedom for the motors. As a result, each arm agent receives a
26-dimensional observation, leading to a total input dimension
of oactor
∈ R52 (26 × 2).
t
The Critic network takes the global observations of the entire robot body as input, including: motor positions qt ∈ R20 ,
motor velocities q˙t ∈ R20 , the action from the previous time
step at ∈ R20 , motor position deviation dt ∈ R20 , timing

5

guidance ϕt ∈ R2 for the limbs, control commands ct ∈ R4
provided by the remote controller, as well as the linear velocity
vt ∈ R3 , Euler angles θt ∈ R3 and angular velocities ωt ∈ R3
of the torso. In addition, the observation includes the external
disturbance forces ft ∈ R2 and torques τt ∈ R3 , environment
friction coefficients µt ∈ R1 , body mass mt ∈ R1 , and two
binary masks representing the stance state and contact state of
the limbs, each ∈ R2 . Therefore, the total input dimension for
the Critic network is ocritic
∈ R106 .
t
Action Space: The output of the bipedal actor network is
a continuous action at ∈ R12 , representing the torques for the
12 motors. For the dual-arm system, the actor network outputs
at ∈ R8 .
The Critic network outputs the value function Vt ∈ R4 ,
which is used to compute the advantage function.
Reward Function: The reward functions in Table I are
designed to optimize the robot’s performance by encouraging
desired behaviors and penalizing undesired ones. Key rewards
include: joint position to follow reference postures; velocity
tracking terms such as tracking linear velocity and tracking
angular velocity to follow commanded motions; energy-related
penalties including DOF torques, DOF velocity, and DOF
acceleration to encourage smooth and efficient movement;
gait-specific rewards like feet air time, feet clearance, and feet
contact number promote coordinated stepping, while stability
and safety are addressed by orientation, collision, feet slip,
and base height. Additional terms such as action smoothness
and torque rate help ensure motion consistency and reduce actuation spikes. Together, these terms shape the robot’s actions
toward achieving robust and physically plausible behavior in
complex tasks.
TABLE I
R EWARD FUNCTION
REWARD SETTINGS, CORRESPONDING EQUATIONS, AND THEIR SCALES
Reward Term

Equation

Scale

Joint Position

3.5

DOF Torques

exp (−∥q − qdefault − qref ∥)


∥v
−v ∥2
exp − cmdσ b
t


−ω
(ω
)2
exp − cmd,zσ b,z
yaw
2
P

DOF Velocity

P

−5 × 10−4

DOF Acceleration

2
i ∥vd,i ∥
P  vd,i,t −vd,i,t−1 2
i
∆t

Feet Air Time

P2

i=1 tair,i · exp (−α · ∥∆xi ∥)

2.0

Feet Clearance

P

Feet Contact Number

(|zf eet − htarget feet | < δ)
(
+1,
if contacti = stancei
−0.3, otherwise

2.0

P

Orientation

exp(|θpitch, roll |) + exp(∥gproj ∥)
P
(1.0 · (∥fc ∥ > 0.1))
P
(cf · ∥vf ∥2 )

1.0

exp (−|hbase − htargetb ase |)
P
(at − at−1 )2
P
(at − 2at−1 + at−2 )2
2
P

0.2

Tracking Linear Velocity
Tracking Angular Velocity

Collision
Feet Slip
Base Height
Action Smoothness 1
Action Smoothness 2
Torque Rate

τ
τmax

τt −τt−1
τmax ·∆t

B. Multi-agent Actor and Global Critic Networks
In the Isaac Gym [47] simulation environment, the robot
receives observations and rewards to facilitate learning. Observations are divided into shared and private features, with
both actor and critic networks using rewards to optimize the
policy.
The actor-network employs an MLP/RNN architecture with
an activation layer that generates actions and their log probabilities, while the critic network utilizes a similar MLP/RNN
structure to estimate state values. During training, the actornetwork selects actions based on environmental observations,
with the critic providing value estimates to guide policy
updates. Action outputs correspond to desired joint positions.
A proportional-derivative (PD) controller computes the corresponding joint torques based on these targets, which are then
applied to the robot model in the physics simulator. Upon
completion of training, the optimized actor-network is directly
implemented on the robotic platform for real-world operation.
The complete learning pipeline is depicted in Fig. 2.
We model each leg and arm as an independent agent for the
humanoid robot system while employing a shared-parameter
actor-network across two limbs and two arms, respectively
(that is, two arms share one shared-parameter actor-network,
while two legs share another). This architectural choice offers
two key advantages: (1) it significantly reduces computational
overhead compared to maintaining separate networks for each
limb, and (2) it properly captures the inherent symmetry and
coordinated nature between the left and right limbs of a
humanoid robot during locomotion. Unlike conventional multiagent systems like StarCraft [22], where agents operate independently, our humanoid’s legs and arms form an intrinsically
coupled system, symmetrically arranged around the body’s
center and mechanically constrained to move in coordination. The shared-parameter network naturally encodes these
physical symmetries and coordination requirements, making it
particularly well-suited for robotic control while maintaining
the benefits of a MARL framework.
We express the policy for each leg and arm as follows:

1.5
1.4
−2.0 × 10−3

−1.0 × 10−7

1.2

−1.0
−5 × 10−2

−0.1
−0.1
−2 × 10−4

πθleg
(ai,t | si,t ) = πθleg (ai,t | si,t )
i

(10)

πθarm
(ai,t | si,t ) = πθarm (ai,t | si,t )
i

(11)

where i = 1, 2 indexes the left and right limbs. The leg policy
πθleg
is shared across both legs, and the arm policy πθarm
is
i
i
shared across both arms.
To enhance the coordination among the agents, we augment each independent observation with shared observations,
including Euler angle and angular velocity calculated from
inertial measurement unit data, the temporal director, and the
agent identifier (ID). The temporal director Ti (t) guides the
gait sequence of each leg and coordinates the motion patterns
of each arm under different movement postures, while the
agent ID is necessary for the shared-parameter network. This
setup ensures the independence of each agent while improving
their cooperative capabilities. The temporal director helps to
synchronize the movements of different legs and coordinate

6

the motions of the arms, ensuring smooth and balanced gait
patterns. It can be defined as follows:
Ti (t) = sin (2π(kt + ∆i ))

(12)

where
•
•

k is the scaling factor of gait cycle.
∆i is the phase offset for the i-th limb (leg or arm),
which determines its relative timing within the gait cycle
to ensure coordinated movement.

The global critic employs a centralized value function
within the CTDE framework, utilizing global observations that
combine individual limb observations and shared system-wide
information. This architecture enables effective coordination
among the two-legged and two-armed agents while maintaining the benefits of single-agent optimization approaches
like PPO. The critic network processes this comprehensive
state representation to learn value functions that guide policy
updates. This ensures synchronized learning across all agents
while accounting for the robot’s global state and inter-limb
dependencies.
C. Sim-to-Real
To enhance sim-to-real transfer, we employ domain randomization. To enhance sim-to-real transfer, we employ domain
randomization. Based on the timing of the application, all
domain randomization parameters can be categorized into two
types. The first type includes physical parameters randomized
at environment initialization, such as link mass and inertia,
joint damping and friction, ground friction, and gravity. The
second type involves parameters randomized at each simulation step, including action delay, torque noise, and external
perturbations. These randomizations enhance the policy’s robustness and adaptability to varying physical conditions.

A. Experiment Setup
1) Task Description: In the simulation environment, we
train a single humanoid robot to walk forward on flat terrain
in a fast and stable manner. This task serves as a standard
benchmark in humanoid robot control, assessing the robot’s
capabilities in terms of speed and balance. Our training
pipeline begins with upright locomotion training in Isaac
Gym, which includes: (i) bipedal walking (with both legs
forming a single group of multiple agents) and (ii) arm-swing
walking (with both legs and both arms forming two groups
of multiple agents). Compared to bipedal walking, arm-swing
walking imposes higher requirements on limb coordination.
We then perform Sim-to-Sim transfer to the MuJoCo simulator
to rigorously assess the robustness of the learned policy under
different simulation environments. Ultimately, the policy is
deployed on a physical robot to demonstrate its effectiveness
in real-world scenarios.
2) Evaluation Metrics: To comprehensively demonstrate
the superiority of MASH, we establish the following evaluation metrics:
• Convergence time TConv : we define the convergence
time as the number of training iterations required for
the average episodic reward to stably reach 95% of the
asymptotic reward value during training.
• Action smoothness Saction : we measure the continuity
and stability of control actions (e.g., joint torques or target
joint positions), aiming to avoid abrupt changes in the
action sequence. Quantified by the squared L2-norm of
the second-order difference of the action sequence:
T −1 N

Saction =
•

V. E XPERIMENTS
We conducted MASH experiments using the BanXing humanoid robot with various experiments. Section V-A proposes
the experimental setup. Section V-B shows the experimental
results of the simulation. Section V-C shows the real-world
experiments and comparisons.

1 XX
(ai,t+1 − ai,t )2
T t=1 i=1

Torso stability Storso : The stability of the torso during
locomotion is evaluated by measuring the fluctuations in
torso height and orientation, including pitch, roll, and yaw
angles:
Storso = wh · Var(ht ) + wθ · Var(θt )

•

(13)

(14)

Limb coordination Climb : By analyzing the motion
trajectories of key joints in the legs, such as the hip or
knee, and computing their relative phase difference over
the gait cycle, limb coordination can be quantitatively
evaluated:
T

Climb =

1X
|ϕleft (t) − ϕright (t) − ϕtarget |
T t=1

(15)

3) Baselines: To better demonstrate the strong potential of
MASH, we compare it with the conventional single-agent PPO
algorithm, training both for 3000 episodes in the simulation
environment with an episode length of 48 steps.
B. Simulation Experiments

Fig. 3. Simulation experiments of MASH

The experimental evaluation compares MASH against the
other baselines under identical simulation conditions. Fig.
4 illustrates the smoothed reward growth trends during the
training process for (a) legs and (b) the whole body, comparing

7

Reward growth trend for training bipedal walking
6

Smoothed Reward

Smoothed Reward

Converged at ~1017
Converged at ~1238

8

Converged at ~1611

5

6

4
3

4

2

2

1
0

Reward growth trend for training whole body

Converged at ~1306

MASH (Ours)
Single Agent PPO

0

500

1000 1500 2000
Training Iterations

2500

3000

0

MASH (Ours)
Single Agent PPO

0

500

1000 1500 2000
Training Iterations

(a)

2500

3000

(b)

Fig. 4. Reward growth trends for (a) leg training and (b) whole-body training, comparing MASH with the Single Agent PPO baseline. The smoothed reward
is plotted against the number of training iterations.
TABLE II
E VALUATION METRICS RESULTS
TConv

Method
Bipedal

Saction

Arm-swing

Bipedal

Storso

Climb

Arm-swing

Bipedal

Arm-swing

Bipedal

Arm-swing

2.720 × 10−3 (↓)

0.612 (↓)

0.421 (↓)

2.802 × 10−2

0.974

0.951

MASH (Ours)

∼1306 (↓)

∼1017 (↓)

0.107 (↓)

0.124 (↓)

8.240 × 10−4 (↓)

Single-agent PPO

∼1661

∼1238

0.547

0.546

3.398 × 10−3

the proposed MASH framework with the Single Agent PPO
baseline. In both scenarios, MASH achieves a noticeably
faster increase in reward during the early training phase,
indicating accelerated learning efficiency. Moreover, MASH
converges to a higher asymptotic reward than the baseline,
particularly in leg training, where the improvement margin is
more pronounced. These results demonstrate the advantage of
the multi-agent hierarchical structure in capturing inter-limb
coordination patterns and optimizing control performance.
After training, we evaluated the deployment performance of
all methods. All evaluation metrics are summarized in Table
II.
As shown in Table II, we quantitatively evaluate our proposed method, MASH, against a Single-agent PPO baseline across both bipedal and full-body (arm-swing) locomotion tasks. The evaluation metrics—convergence time,
action smoothness, torso stability, and limb coordination—demonstrate the clear superiority of our method in
nearly all aspects.
Fig. 5 compares the performance of our MASH controller
against a baseline Single-agent PPO on a hip pitch trajectory
tracking task. As shown, the PPO controller (orange line)
struggles to follow the reference trajectory (green dashed line),
exhibiting significant overshooting and phase lag. This results
in an unstable and imprecise gait. In contrast, our MASH
controller (blue line) demonstrates excellent tracking accuracy,
closely matching the reference trajectory in both phase and
amplitude. This highlights the superiority of our method in
generating stable, smooth, and precise robotic motion.

C. Real-world Experiments
We deploy MASH on a humanoid robot. To facilitate simto-real transfer, we incorporate domain randomization during
training, with the parameters listed in Table III.
TABLE III
D OMAIN RANDOMIZATION PARAMETERS USED IN TRAINING .
Parameter
Friction coefficient
Link mass scale factor
Center of mass offset (m)
Motor delay (ms)
External push force (N )
Gravity (m/s2 )
Joint damping
Joint friction
Joint armature
Kp scale factor
Kd scale factor

Distribution Type

Range / Std. Dev.

Uniform
Uniform
Uniform
Uniform
Uniform
Uniform
Uniform
Uniform
Uniform
Uniform
Uniform

[0.1, 1.2]
[0.9, 1.13]
[−0.03, 0.03]
[0.0, 3.0]
[−20, 20]
[9.78, 9.83]
[0, 0.05]
[0., 0.05]
[0.005, 0.015]
[0.95, 1.05]
[0.95, 1.05]

Fig. 6 demonstrates the real-world hardware validation of
a walking gait for a humanoid robot trained using MASH.
From a kinematic perspective, the stroboscopic images clearly
capture a coherent and fluid gait cycle, showcasing proficient
dynamic balance. The corresponding plot of the knee joint
angle (in radians) over time (in seconds) quantifies this stable
performance. The curve exhibits a high degree of periodicity
and smoothness, with the knee angle oscillating regularly
within a range of approximately 1.2 to 1.7 radians, and each
gait cycle lasting about 0.6 seconds. The smooth, non-abrupt
nature of the curve reflects the precision of the motor control
and the soundness of the generated policy, effectively avoiding

8

Comparison of limb motion trajectories between MASH (Ours) and PPO
Left hip pitch Position
0.05

rad

0.00
0.05
0.10

MASH (Ours)
Single-agent PPO
Referece position

0.15
0.20

Right hip pitch position

MASH (Ours)
Single-agent PPO
Referece position

0.10
0.05

rad

0.00
0.05
0.10
0.15
0.20
200

300

400

Step

500

600

Knee position (rad)

Fig. 5. Comparison of hip pitch joint trajectories. The plots show the tracking performance of our MASH controller versus a Single-agent PPO baseline
for the left and right hip joints. Our method (blue) closely follows the reference trajectory (green, dashed), while the baseline (orange) exhibits significant
overshoot and phase error.

Timestamp (s)
Fig. 6. Stable walking gait on the physical humanoid robot using a policy trained with MASH. The stroboscopic images (top) and corresponding knee joint
trajectory (bottom) show a smooth, periodic motion, confirming successful sim-to-real transfer.

9

impacts and instability. This strong correlation between motion
and data confirms that MASH can successfully generate a
complex motor control policy that is not only effective in simulation but also directly transferable to a real-world physical
system. This result indicates that the trained policy possesses
excellent robustness, enabling it to handle real-world physical
constraints.
VI. C ONCLUSION AND F UTURE W ORK
In this work, we presented MASH, a novel cooperativeheterogeneous multi-agent reinforcement learning framework
designed to enhance the locomotion of a single humanoid
robot. By reformulating the control problem and treating
each of the robot’s limbs as an independent agent within a
cooperative system, our approach effectively leverages MARL
principles to foster superior inter-limb coordination. Implemented in Isaac Gym, the proposed method demonstrated three
key advantages: (1) accelerated training convergence through
coordinated limb learning, (2) improving limb movement
coordination through multi-agent design, and (3) enhanced
robustness via domain randomization. Our experimental results demonstrate that MASH significantly outperforms the
conventional single-agent PPO baseline, achieving faster training convergence and a higher asymptotic reward. The learned
policies exhibit quantitatively superior performance in terms
of action smoothness, torso stability, and limb coordination.
Crucially, the efficacy and robustness of our method were
validated through the successful sim-to-real transfer of the
learned policy to a physical humanoid robot, which executed a
stable and smooth walking gait. This study not only introduces
a potent and efficient strategy for complex single-robot control
but also offers new insights into applying multi-agent learning
paradigms to advance humanoid locomotion. For future work,
we will apply MASH to robots with other configurations.
R EFERENCES
[1] T. He, W. Xiao, T. Lin, Z. Luo, Z. Xu, Z. Jiang, J. Kautz, C. Liu, G. Shi,
X. Wang et al., “HOVER: Versatile neural whole-body controller for
humanoid robots,” in 2025 IEEE International Conference on Robotics
and Automation (ICRA), 2025.
[2] Q. Liu, Y. Li, X. Shi, K. Lin, Y. Liu, and Y. Lou, “Distributional
policy gradient with distributional value function,” IEEE Transactions on
Neural Networks and Learning Systems, vol. 36, no. 4, pp. 6556–6568,
2025.
[3] G. B. Margolis and P. Agrawal, “Walk these ways: Tuning robot control
for generalization with multiplicity of behavior,” in Conference on Robot
Learning. PMLR, 2023, pp. 22–31.
[4] I. M. Aswin Nahrendra, B. Yu, and H. Myung, “Dreamwaq: Learning
robust quadrupedal locomotion with implicit terrain imagination via
deep reinforcement learning,” in 2023 IEEE International Conference
on Robotics and Automation (ICRA), 2023, pp. 5078–5084.
[5] H. Benbrahim and J. A. Franklin, “Biped dynamic walking using
reinforcement learning,” Robotics and Autonomous Systems, vol. 22, no.
3-4, pp. 283–302, 1997.
[6] H. Duan, B. Pandit, M. S. Gadde, B. Van Marum, J. Dao, C. Kim,
and A. Fern, “Learning vision-based bipedal locomotion for challenging terrain,” in 2024 IEEE International Conference on Robotics and
Automation (ICRA), 2024, pp. 56–62.
[7] H. Lu, Y. Li, S. Mu, D. Wang, H. Kim, and S. Serikawa, “Motor anomaly
detection for unmanned aerial vehicles using reinforcement learning,”
IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2315–2322, 2018.
[8] P. M. Wensing, M. Posa, Y. Hu, A. Escande, N. Mansard, and A. D.
Prete, “Optimization-based control for dynamic legged robots,” IEEE
Transactions on Robotics, vol. 40, pp. 43–63, 2024.

[9] J.-P. Sleiman, F. Farshidian, and M. Hutter, “Versatile multicontact
planning and control for legged loco-manipulation,” Science Robotics,
vol. 8, no. 81, p. eadg5014, 2023.
[10] X. Da and J. Grizzle, “Combining trajectory optimization, supervised
machine learning, and model structure for mitigating the curse of
dimensionality in the control of bipedal robots,” The International
Journal of Robotics Research, vol. 38, no. 9, pp. 1063–1097, 2019.
[11] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath,
“Real-world humanoid locomotion with reinforcement learning,” Science Robotics, vol. 9, no. 89, p. eadi9579, 2024.
[12] Y. Kim, H. Oh, J. Lee, J. Choi, G. Ji, M. Jung, D. Youm, and J. Hwangbo,
“Not only rewards but also constraints: Applications on legged robot
locomotion,” IEEE Transactions on Robotics, 2024.
[13] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake,
and S. Song, “Diffusion policy: Visuomotor policy learning via action diffusion,” The International Journal of Robotics Research, p.
02783649241273668, 2023.
[14] S. Ha, J. Lee, M. van de Panne, Z. Xie, W. Yu, and M. Khadiv,
“Learning-based legged locomotion: State of the art and future
perspectives,” The International Journal of Robotics Research, p.
02783649241312698, 2024.
[15] K. Jiang, Z. Fu, J. Guo, W. Zhang, and H. Chen, “Learning whole-body
loco-manipulation for omni-directional task space pose tracking with
a wheeled-quadrupedal-manipulator,” IEEE Robotics and Automation
Letters, 2024.
[16] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik,
M. Wulfmeier, S. Tunyasuvunakool, N. Y. Siegel, R. Hafner et al.,
“Learning agile soccer skills for a bipedal robot with deep reinforcement
learning,” Science Robotics, vol. 9, no. 89, p. eadi8022, 2024.
[17] M. Liu, Z. Chen, X. Cheng, Y. Ji, R. Qiu, R. Yang, and X. Wang, “Visual
whole-body control for legged loco-manipulation,” in Conference on
Robot Learning, 2024.
[18] K. Darvish, L. Penco, J. Ramos, R. Cisneros, J. Pratt, E. Yoshida,
S. Ivaldi, and D. Pucci, “Teleoperation of humanoid robots: A survey,”
IEEE Transactions on Robotics, vol. 39, no. 3, pp. 1706–1727, 2023.
[19] B. Jia and D. Manocha, “Sim-to-real robotic sketching using behavior
cloning and reinforcement learning,” in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024, pp. 18 272–18 278.
[20] S. Lyu, H. Zhao, and D. Wang, “A composite control strategy for
quadruped robot by integrating reinforcement learning and model-based
control,” in 2023 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2023, pp. 751–758.
[21] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer,
H. Dong, S.-C. Zhu, and Y. Yang, “Towards human-level bimanual
dexterous manipulation with reinforcement learning,” in Advances in
Neural Information Processing Systems, vol. 35, 2022, pp. 5150–5163.
[22] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
S. Whiteson, “Monotonic value function factorisation for deep multiagent reinforcement learning,” Journal of Machine Learning Research,
vol. 21, no. 178, pp. 1–51, 2020.
[23] Q. Liu, Y. Li, Y. Liu, K. Lin, J. Gao, and Y. Lou, “Data efficient deep
reinforcement learning with action-ranked temporal difference learning,”
IEEE Transactions on Emerging Topics in Computational Intelligence,
vol. 8, no. 4, pp. 2949–2961, 2024.
[24] Q. Liu, J. Guo, S. Lin, S. Ma, J. Zhu, and Y. Li, “Masq: Multi-agent
reinforcement learning for single quadruped robot locomotion,” arXiv
preprint arXiv:2408.13759, 2024.
[25] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, “Expressive
whole-body control for humanoid robots,” in Robotics Science and
Systems, 2024.
[26] C. Lu, X. Cheng, J. Li, S. Yang, M. Ji, C. Yuan, G. Yang, S. Yi,
and X. Wang, “Mobile-television: Predictive motion priors for humanoid whole-body control,” in 2025 IEEE International Conference
on Robotics and Automation (ICRA), 2025.
[27] C. Zhang, W. Xiao, T. He, and G. Shi, “Wococo: Learning whole-body
humanoid control with sequential contacts,” in Conference on Robot
Learning, 2024.
[28] T. He, Z. Luo, W. Xiao, C. Zhang, K. Kitani, C. Liu, and G. Shi,
“Learning human-to-humanoid real-time whole-body teleoperation,” in
2024 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), 2024, pp. 8944–8951.
[29] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, “Humanplus:
Humanoid shadowing and imitation from humans,” in Conference on
Robot Learning, 2024.
[30] J. Li, Y. Zhu, Y. Xie, Z. Jiang, M. Seo, G. Pavlakos, and Y. Zhu,
“Okami: Teaching humanoid robots manipulation skills through single
video imitation,” in Conference on Robot Learning, 2024.

10

[31] T. He, Z. Luo, X. He, W. Xiao, C. Zhang, W. Zhang, K. M. Kitani,
C. Liu, and G. Shi, “OmniH2O: Universal and dexterous human-tohumanoid whole-body teleoperation and learning,” in Conference on
Robot Learning, 2024.
[32] Z. Luo, J. Cao, K. Kitani, W. Xu et al., “Perpetual humanoid control
for real-time simulated avatars,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2023, pp. 10 895–10 904.
[33] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
“Robust and versatile bipedal jumping control through reinforcement
learning,” in Robotics Science and Systems, 2023.
[34] Z. Xie, G. Berseth, P. Clary, J. Hurst, and M. Van de Panne, “Feedback
control for cassie with deep reinforcement learning,” in 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 1241–1246.
[35] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, “Sim-to-real learning of all
common bipedal gaits via periodic reward composition,” in 2021 IEEE
International Conference on Robotics and Automation (ICRA). IEEE,
2021, pp. 7309–7315.
[36] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa, “Amp:
Adversarial motion priors for stylized physics-based character control,”
ACM Transactions on Graphics (ToG), vol. 40, no. 4, pp. 1–20, 2021.
[37] X. Gu, Y.-J. Wang, X. Zhu, C. Shi, Y. Guo, Y. Liu, and J. Chen,
“Advancing humanoid locomotion: Mastering challenging terrains with
denoising world model learning,” arXiv e-prints, pp. arXiv–2408, 2024.
[38] M.-A. Blais and M. A. Akhloufi, “Reinforcement learning for swarm
robotics: An overview of applications, algorithms and simulators,”
Cognitive Robotics, vol. 3, pp. 226–256, 2023.
[39] M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao, W. Zhang,
M. Alban, I. Fadakar, Z. Chen et al., “Smarts: An open-source scalable

multi-agent rl training school for autonomous driving,” in Conference
on robot learning. PMLR, 2021, pp. 264–285.
[40] S. Lim, H. Yu, and H. Lee, “Optimal tethered-uav deployment in
a2g communication networks: Multi-agent q-learning approach,” IEEE
Internet of Things Journal, vol. 9, no. 19, pp. 18 539–18 549, 2022.
[41] Q. Liu, J. Gao, D. Zhu, Z. Qiao, P. Chen, J. Guo, and Y. Li, “Multiagent target assignment and path finding for intelligent warehouse: A
cooperative multi-agent deep reinforcement learning perspective,” arXiv
preprint arXiv:2408.13750, 2024.
[42] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
MIT press, 2018.
[43] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.
[44] S. C. Ong, S. W. Png, D. Hsu, and W. S. Lee, “Pomdps for robotic tasks
with mixed observability.” in Robotics: Science and Systems, vol. 5,
2009, p. 4.
[45] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu,
“The surprising effectiveness of ppo in cooperative multi-agent games,”
in Advances in Neural Information Processing Systems, vol. 35, 2022,
pp. 24 611–24 624.
[46] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and
S. Whiteson, “Stabilising experience replay for deep multi-agent reinforcement learning,” in International Conference on Machine Learning.
PMLR, 2017, pp. 1146–1155.
[47] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin,
D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym: High
performance gpu based physics simulation for robot learning,” in Thirtyfifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 2), 2021.

