arXiv:2203.08975v2 [cs.MA] 18 Oct 2024

A S URVEY OF M ULTI -AGENT D EEP R EINFORCEMENT
L EARNING WITH C OMMUNICATION

Changxi Zhu
Department of Information and Computing Sciences
Utrecht University
Utrecht
c.zhu@uu.nl

Mehdi Dastani
Department of Information and Computing Sciences
Utrecht University
Utrecht
m.m.dastani@uu.nl

Shihan Wang
Department of Information and Computing Sciences
Utrecht University
Utrecht
s.wang2@uu.nl

A BSTRACT
Communication is an effective mechanism for coordinating the behaviors of multiple agents, broadening their views of the environment, and to support their collaborations. In the field of multi-agent
deep reinforcement learning (MADRL), agents can improve the overall learning performance and
achieve their objectives by communication. Agents can communicate various types of messages,
either to all agents or to specific agent groups, or conditioned on specific constraints. With the growing body of research work in MADRL with communication (Comm-MADRL), there is a lack of a
systematic and structural approach to distinguish and classify existing Comm-MADRL approaches.
In this paper, we survey recent works in the Comm-MADRL field and consider various aspects of
communication that can play a role in designing and developing multi-agent reinforcement learning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL
approaches can be analyzed, developed, and compared. By projecting existing works into the multidimensional space, we discover interesting trends. We also propose some novel directions for designing future Comm-MADRL systems through exploring possible combinations of the dimensions.
Keywords Multi-Agent Reinforcement Learning · Deep Reinforcement Learning · Communication · Survey

1

Introduction

Many real-world scenarios, such as autonomous driving [1], sensor networks [2], robotics [3] and game-playing [4, 5],
can be modeled as multi-agent systems. Such multi-agent systems can be designed and developed using multi-agent
reinforcement learning (MARL) techniques to learn the behavior of individual agents, which can be cooperative,
competitive, or a mixture of them. As agents are often distributed in the environment where they only have access to
their local observations rather than the complete state of the environment, partial observability becomes an essential
assumption in MARL [6, 7, 8]. Moreover, MARL suffers from the non-stationary issue [9], since each agent faces a
dynamic environment that can be influenced by the changing and adapting policies of other agents. Communication
has been viewed as a vital means to tackle the problems of partial observability and non-stationary in MARL. Agents
can communicate individual information, e.g., observations, intentions, experiences, or derived features, to have a
broader view of the environment, which in turn allows them to make well-informed decisions [9, 10].
Due to the recent success of deep learning [11] and its application to reinforcement learning [12], multi-agent deep
reinforcement learning (MADRL) has witnessed great achievements in recent years, where agents can process highdimensional data and have generalization ability in large state and action spaces [7, 8]. We notice that a large number

of research works focus on learning tasks with communication, which aim at learning to solve domain-specific tasks,
such as navigation, traffic, and video games, by communicating and sharing information. To the best of our knowledge,
there is a lack of survey literature that can cover recent works on learning tasks with communication in multi-agent
deep reinforcement learning (Comm-MADRL). Early surveys consider the role of communication in MARL but assume it to be predefined rather than a subject of learning [13, 14, 15]. Most Comm-MADRL surveys cover only a
small number of research works without proposing a fine-grained classification system to compare and analyze them.1
In cooperative scenarios, Hernandez-Leal et al. [16] use learning communication to denote the area of learning communication protocols to promote the cooperation of agents.2 The only survey that we found classifying some early
works in Comm-MADRL is from Gronauer and Diepold [17], which is based on distinguishing whether messages are
received by all agents, a set of agents, or a network of agents. However, other aspects of Comm-MADRL, such as the
type of messages and training paradigms, which are essential for communication and can help characterize existing
communication protocols, are ignored. As a result, the reviewed papers in recent surveys regarding learning tasks
with communication are rather limited and the proposed categorizations are too narrow to distinguish existing works
in Comm-MADRL. On the other hand, there is a closely related research area, emergent language/communication,
which also considers learning communication through various reinforcement learning techniques [18]. Different from
Comm-MADRL, the primary goal of emergent language studies is to learn a symbolic language.3 However, a subset
of emergent language research works pursues an additional goal to leverage learnable symbolic language to enhance
task-level performance. Notably, these research works have not been encompassed within existing Comm-MADRL
surveys but included in our survey, referred to learning tasks with emergent language. In summary, our survey overlaps
in scope with surveys of emergent language (i.e., in learning tasks with emergent language), but our survey focuses
on different primary goals (i.e., achieving domain-specific tasks rather than learning a symbolic language). We further
clarify the differences between learning tasks with communication and emergent language in Section 2.2.
In our survey paper, we review the Comm-MADRL literature by focusing on how communication can be utilized
to improve the performance of multi-agent deep reinforcement learning techniques. Specifically, we focus on learnable communication protocols, which are aligned with recent works that emphasize the development of dynamic and
adaptive communication, including learning when, how, and what to communicate with deep reinforcement learning
techniques. Through a comprehensive review of recent Comm-MADRL literature, we propose a systematic and structured classification methodology designed to differentiate and categorize various Comm-MADRL approaches. Such
a methodology will also provide guidance for the design and advancement of new Comm-MADRL systems. Suppose
we plan to develop a Comm-MADRL system for a domain task at hand. Starting with the questions of when, how,
and what to communicate, the system can be characterized from various aspects. Agents need to learn when to communicate, with whom to communicate, what information to convey, how to integrate received information, and, lastly,
what learning objectives can be achieved through communication. We propose 9 dimensions that correspond to unique
aspects of Comm-MADRL systems: Controlled Goals, Communication Constraints, Communicatee Type, Communication Policy, Communicated Messages, Message Combination, Inner Integration, Learning Methods, and Training
Schemes. These dimensions, which form the skeleton of a Comm-MADRL system, can be used to analyze and gain
insights into designed Comm-MADRL approaches thoroughly. By mapping recent Comm-MADRL approaches into
this multi-dimensional structure, we not only provide insight into the current state of the art in this field but also
determine some important directions for designing future Comm-MADRL systems.
The remaining sections of this paper are organized as follows. In Section 2 the preliminaries of multi-agent RL are
discussed, together with existing extensions regarding communication and a detailed comparison of recent surveys. In
Section 3, we present our proposed dimensions, explaining how we group the recent works in the categories of each
dimension. In Section 4, we discuss the trends that we found in the literature, and, driven by the proposed dimensions,
we propose possible research directions in this research area. We finalize the paper with some conclusions in Section
5.

2

Background

In this section, we first provide the necessary background on multi-agent reinforcement learning. Then, we show how
multi-agent reinforcement learning can be extended to consider communication between agents. Finally, we present
1

We provide a detailed comparison of recent surveys on MADRL which involves communication in Section 2.3.
In our survey, we extend the concept of learning communication to general multi-agent tasks and use the term learning tasks
with communication to emphasize that the primary goal of recent research, which is centered on solving specific domain tasks
through the use of communication.
3
In the literature, emergent language and emergent communication are used interchangeably. In our survey, we use emergent
language for referring to both terms.
2

2

and compare recent surveys involving communication, from which we can directly see our motivations to fill the gaps
among existing surveys.
2.1

Multi-agent Reinforcement Learning

Real-world applications often contain more than one agent that operate in the environment. Agents are generally
assumed to be autonomous and required to learn their strategies for achieving their goals. A multi-agent environment
can be formalized in several ways [19], depending on whether the environment is fully observable, how agents’ goals
are correlated, etc. Among them, the Partially Observable Stochastic Game (POSG) [20, 21] is one of the most flexible
formalizations. A POSG is defined by a tuple I, S, ρ0 , {Ai } , P, {Oi } , O, {Ri } , where I is a (finite) set of agents
indexed as {1, ..., n}, S is a set of environment states, ρ0 is the initial state distribution over state space S, Ai is a set of
actions available to agent i, and Oi is a set of observations of agent i. We denote a joint action space as A = ×i∈I Ai
and a joint observation space of agents as O = ×i∈I Oi . Therefore, P : S × A → ∆(S) denotes the transition
probability from a state s ∈ S to a new state s′ ∈ S given agents’ joint action ⃗a = ⟨a1 , ..., an ⟩, where ⃗a ∈ A. With the
environment transitioning to the new state s′ , the probability of observing a joint observation ⃗o = ⟨o1 , ..., on ⟩ (where
⃗o ∈ O) given the joint action ⃗a is determined according to the observation probability function O : S × A → ∆(O).
Each agent then receives an immediate reward according to their own reward functions Ri : S × A × S → R.
Similar to the joint action and observation, we could denote ⃗r = ⟨r1 , ..., rn ⟩ as a joint reward. If agents’ reward
functions happen to be the same, i.e., they have identical goals, then r1 = r2 = ... = rn holds for every time step.
In this setting, the POSG is reduced to a Dec-POMDP [19]. If at every time step the state is uniquely determined
from the current set of observations of agents, i.e., s ≡ ⃗o, the Dec-POMDP is reduced to a Dec-MDP. If each agent
knows what the true environment state is, the Dec-MDP is reduced to a Multi-agent MDP. If there is only one single
agent in the set of agents, i.e., I = {1}, then the Multi-agent MDP is reduced to an MDP and the Dec-POMDP
is reduced to a POMDP. Due to the partial observability, MARL methods often use the observation-action history
τi,t = {oi,0 , ai,0 , oi,1 , ..., oi,t } up to time step t for each agent to approximate the environment state. Note that time
step t is often omitted for the sake of simplification.
In the multi-agent reinforcement learning setting, agents can learn their policies in either a decentralized or a centralized fashion. In decentralized learning (e.g., decentralized Q-learning [22, 23]), an n-agent MARL problem is
decomposed into n decentralized single-agent problems where each agent learns its own policy by considering all
other agents as a part of the environment [24, 25]. In such a decentralized setting, the learned policy of each agent
is conditioned on its local observation and history. A major problem with decentralized learning is the so-called
non-stationarity of the environment, i.e., the fact that each agent learns in an environment where other agents are
simultaneously exploring and learning. Centralized learning enables the training of either a single joint policy for all
agents or a centralized value function to facilitate the learning of n decentralized policies. While centralized (joint)
learning removes or mitigates issues of partial observability and non-stationarity, it faces the challenge of joint action
(and observation) spaces that expand exponentially with the number of agents and their actions. For a deeper dive into
various training schemes used in MARL, we recommend the comprehensive survey by [17], which offers valuable
insights into the training and execution of policies. Based on whether policies are derived from value functions or
directly learned, multi-agent reinforcement learning methods can be categorized into value-based and policy-based
methods. Both methods have been largely utilized in Comm-MADRL.
Value-based Value-based methods in the multi-agent case borrow considerable ideas from the single-agent case. As
one of the most popular value-based algorithms, the decentralized Q-learning learns a local Q-function for each agent.
In the cooperative setting where agents share a common reward, the update rule for agent i is as follows:
Qi (s, ai ) ← Qi (s, ai ) + α(r + γ max
Qi (s′ , a′i ) − Qi (s, ai ) )
a′i
| {z }
|
{z
}
new estimate

(1)

current estimate

where r is the shared reward, and a′i is the action with the highest Q-value in the next state s′ . In partially observable
environments, the environment state is not fully observable and is usually replaced by the individual observation
or history of each agent. The Q-values for each state-action pair are incrementally updated according to the TD
error. This error, i.e., r + γ maxa′i Qi (s′ , a′i ) − Qi (s, ai ), represents the difference between a new estimate (i.e.,
r + γ maxa′i Qi (s′ , a′i )) and the current estimate (i.e., Qi (s, ai )) based on the Bellman equation [26]. As the state
and action space could be too large to be encountered frequently for accurate estimation, function approximation
methods, like deep neural networks, have become popular for endowing value or policy models with generalization
abilities across both discrete and continuous states and actions [12]. For example, the Deep Q-network (DQN) [12]
minimizes the difference between the new estimate calculated from sampled rewards and the current estimate of a
parameterized Q-function. In DQN-based methods, the Q-function in Equation 1 is notated as Qi (s, ai ; θi ), which
3

depends on learnable parameters θi . On the other hand, centralized learning in value-based methods learns a joint Qfunction Q(s, ⃗a; θ) with parameters θ. However, this approach can be challenging to scale with an increasing number
of agents. Value decomposition methods [27, 28, 29, 30] are popular MARL methods that decompose a joint Qfunction to enable efficient training. These methods are also widely employed in research works in Comm-MADRL
[31, 32, 33]. In partially observable environments, linear value decomposition methods decompose history-based joint
Q-functions as follows:
n
X
Qjoint (⃗τ , ⃗a) =
wi Qi (τi , ai )
(2)
i

where the joint Q-function is based on the joint history of all agents and is decomposed into local Q-functions based
on individual histories. The weight wi can either be a fixed value [27, 29] or a learnable parameter subject to certain
constraints [30]. Advantage functions can also replace the Q-function in the above equation to reduce variance [34].
Policy-based Policy-based methods directly search over the policy space instead of obtaining the policy through
value functions implicitly. The policy gradient theorem [26] provides an analytical expression of the gradients for a
stochastic policy with learnable parameters in single-agent cases. In the multi-agent case with centralized learning,
the policy gradient theorem is expressed as follows:
∇θ J(θ) = E⃗a∼π(·|s),s∼ρπ [∇θ log π(⃗a | s; θ)Qπ (s, ⃗a)]

(3)

where J(θ) represents the learning objective, and π(⃗a | s; θ) denotes a stochastic policy parameterized by θ (abbreviated as π). Additionally, ρπ signifies the state distribution under the policy π, and ∇θ J(θ) represents the expected
gradient with respect to all possible actions and states. Due to the computational intractability of the expected gradient,
stochastic gradient ascent can be applied to update the parameters θ at every learning step l as follows:
θl+1 = θl + α∇\
θ J(θ)
where α is the learning rate, and ∇\
θ J(θ) is an estimate of the expected gradient based on sampled actions and states.
Moreover, the Q-function in Equation 3 can be replaced by average returns over episodes to form REINFORCE
algorithms [26], or by an estimated value function to form actor-critic algorithms [35, 36]. In actor-critic methods, the
policy and value function are termed the actor and the critic, respectively. The critic will, therefore, guide the learning
of the actor.
Actor-critic methods have undergone various adaptations for multi-agent environments [7, 8, 37, 38]. A typical extension is the multi-agent deep deterministic policy gradient (MADDPG) [7]. In MADDPG, the critic is a centralized
Q-function designed to capture global information and coordinate learning signals. Meanwhile, the actors are local
policies, ensuring decentralized execution. MADDPG assumes deterministic actors with continuous actions, allowing
for the backpropagation of gradients from the value function to the policies. The gradient of each parameterized actor
µθi (ai | oi ) with learnable parameters θi , abbreviated as µi , is defined as follows:


∇θi J (θi ) = E⃗o,⃗a∼D ∇θi µi (ai | oi ) ∇ai Qµi (⃗o, a1 , . . . , aN ) |ai =µi (oi )
where D is the experience buffer that contains joint observation-action tuples ⟨⃗o, ⃗a, ⃗r, o⃗′ ⟩. Each agent’s Q-function,
denoted as Qµi (⃗o, a1 , . . . , aN ), takes joint observations and actions as inputs, while decentralized actors use local
observations as inputs. Contrary to Equation 3, gradients with respect to the current action of agent i (specifically,
µi (oi )) are utilized to guide the update of the policy parameter θi . Both MADDPG and its single-agent counterpart,
DDPG, have seen widespread application in Comm-MADRL [39, 40, 41, 42, 43].
2.2

Extensions with Communication

In the MADRL literature where communication is used, we notice two closely related research areas, which we will refer to with the terms emergent language and learning tasks with communication. The emergent language research area
[18, 44, 45, 46, 47] aims at learning a language grounded on symbols in communities of interacting/communicating
agents. This line of research tries to understand the evolution of the language in agents equipped with neural networks.
On the other hand, learning tasks with communication [16, 48, 49, 50] focuses primarily on solving multi-agent reinforcement learning tasks with the aid of communication. Communication is often regarded as information exchange
rather than learning a (human-like) language. Despite the distinction, when using MADRL techniques on specific
domain tasks, languages might emerge, which can potentially enhance the learning system’s explainability in accomplishing those tasks. We illustrate the research areas, emergent language and learning tasks with communication,
along with their intersection learning tasks with emergent language in Figure 1. Notably, our survey focuses on learning tasks with communication in multi-agent deep reinforcement learning, including the intersection with emergent
4

Multi-agent Reinforcement Learning Tasks

Emergent
Language

Learning Tasks with Learning Tasks with
Emergent Language
Communication

Figure 1: An illustration depicting the scope of this survey. The focus of our survey is represented by the blue part.

language.4 Within this focus, multiple agents often operate in partially observable environments and learn to share
information encoded through neural networks. Furthermore, communication protocols, determining when and with
whom to communicate, often leverage deep learning models to find the optimal choices that minimize communication overhead and yield more targeted communication. A multitude of works have been proposed to handle these
subproblems inherent in Comm-MADRL. Most research works model only one or a few aspects of Comm-MADRL
while selecting a default approach for other aspects. Given that the common goal of Comm-MADRL approaches is to
design an effective and efficient communication protocol to improve agents’ learning performance in the environment,
the proposed Comm-MADRL approaches inevitably share similarities to some extent. Consequently, establishing
a classification system for Comm-MADRL becomes crucial. Such a system would aid in categorizing critical elements like contributions, targeted problems, and learning objectives, from which we can compare and analyse existing
Comm-MADRL approaches.
In the emergent language literature, numerous works employ various forms of the Lewis game, often referred to as referential games and operate under a cheap-talk setting [51], as highlighted in several surveys [10, 18].5 In these games,
a goal, often represented as a target location, an image, or a semantic concept, is given to a sender agent but remains
unrevealed from a receiver agent. The receiver agent must then either identify the correct goal based on the sender’s
signaling [52, 53, 54, 47, 55, 56, 57, 58] or accomplish its single-agent task using the received signals (messages)
[59, 60]. Research works in learning tasks with emergent language are grounded in a multi-agent environment where
the joint actions of both sender and receiver agents impact environment transitions. Consequently, the learning tasks
with emergent language literature considers multi-agent domain tasks [61, 62, 63, 64, 65], building on foundational
concepts from MARL such as Dec-POMDPs or POSGs.
We further distinguish explicit versus non-explicit communication [19] in the literature of MADRL with communication. Explicit communication refers to communication through a set of messages separate from domain-level actions.
Here, agents’ action policies are influenced by both their observations and the messages they receive. Such messages,
crucial for supporting agents’ decision-making, are essential in both the training and execution phases. MADRL
frameworks without explicit communication can still allow for communication through domain-level actions, such as
the act of influencing the observations of one agent through the actions of another. Furthermore, without explicit communication, agents can transmit gradient signals, which facilitate centralized training (and decentralized execution) but
are not utilized during execution phases. Specifically, in our survey, we focus on explicit and learnable communication.
Dec-POMDPs and POSGs are often extended to accommodate explicit communication. The communication can
be integrated into the action set, adding a collection of communication acts alongside domain-level actions. Alternatively, a Dec-POMDP or a POSG can be extended to explicitly include a set of messages [19]. For instance, the POSG can be expanded with a (shared) message space M, resulting in a POSG-Comm, defined as
I, S, ρ0 , {Ai } , P, {Oi } , O, {Ri } , M , where all components remain unchanged except for the added message
space M. A Dec-POMDP-Comm can be defined as similar to the POSG-Comm with shared rewards. In both POSGComm and Dec-POMDP-Comm, action policies take into account both environmental observations and inter-agent
messages. Research works in Comm-MADRL that expand upon a POSG or a Dec-POMDP can be seen in references
such as [61, 63, 66, 65, 67].
4

Throughout the remainder of our survey, Comm-MADRL will be used to specifically refer to the areas of our focus.
In the emergent language research area, research works that do not adopt the cheap-talk setting but communicate through
observable (domain-level) actions, are not included in our survey. Our survey focuses on explicit message transfer between agents.
5

5

2.3

Communication in Recent Surveys

Communication has attracted much attention in the field of multi-agent reinforcement learning (MARL). Previous
surveys mentioning communication in MARL primarily focus on providing an overview of MARL’s development.
These surveys view communication as a subfield in MARL, and no extensive and substantial progress is reported. In
an early survey, Stone and Veloso [13] classify MARL based on whether agents communicate and whether agents
are homogeneous or not.6 They view learnable communication as a future research opportunity. Busoniu et al. [15]
consider communication as a means to negotiate action choices and select equilibrium in the research direction of
explicit coordination, without further classifying communication. With the advancement of deep learning, MARL
has gradually incorporated deep neural networks such that recent developments are dominated by multi-agent deep
reinforcement learning (MADRL). In the MADRL context, Hernandez-Leal et al. [16], Nguyen et al. [68], and Papoudakis et al. [9] briefly review early Comm-MADRL methods, which have now become baselines in many recent
works. Specifically, Hernandez-Leal et al. [16] use learning communication to denote a new branch in MADRL.
Papoudakis et al. [9] consider communication as an approach to handle the non-stationary problem in MADRL, as
agents can exchange information to stabilize their training. Compared to the aforementioned surveys, OroojlooyJadid
and Hajinezhad [37] provide a more detailed review of Comm-MADRL, covering a significant number of existing
works. They view communication as a way to solve cooperative MADRL problems but did not propose a categorization model for Comm-MADRL. Zhang et al. [69] and Yang et al. [21] review communication from a theoretical
perspective. Their primary focus is on communication within networked multi-agent systems. In these systems, agents
share information through a time-varying network, aiming to reach consensus on learned value functions or policies.
Despite this, no further classification of communication is made.
Two more recent surveys in MADRL, proposed by Gronauer and Diepold [17] and Wong et al. [70], focus on classifying existing works on communication. Gronauer and Diepold classify early research works in Comm-MADRL into
Broadcasting, Targeted, and Networked communication, based on whether messages are received from all agents, a
subset of agents, or a network of agents. Wong et al., similar to the survey of Papoudakis et al. [9], view communication as a method to address the issues of non-stationarity and partial observability. In the survey of Wong et al.,
research works on communication are categorized into three groups from a high-level perspective: communication as
the primary learning goal, communication as an instrument to learn a specific task, and peer-to-peer teaching. However, they do not delve into how agents utilize communication to enhance learning. These surveys focus on limited
aspects of communication, making their categorizations too narrow to distinguish recent works effectively, given the
fact that many existing works share similar assumptions and conditions. To the best of our knowledge, only one survey
[71] exclusively focuses on communication issues in MADRL. They review algorithms for communication and cooperation, including efforts to interpret languages developed through communication. Despite this, their survey mainly
covers early models without proposing a categorization framework.
The literature has investigated communication from other perspectives. Shoham and Leyton-Brown [72] investigate
communication from a game-theoretic perspective. They introduce several theories of communication in multi-agent
systems, with the particular concern that agents can be self-motivated to convey information, driven by underlying
incentives (e.g., the knowledge of game structure), or communicate in a pragmatic way analogous to human communication. Deep neural networks and deep reinforcement learning techniques have greatly widened the scope of
language development in multi-agent systems. Lazaridou and Baroni [18] provide an extensive survey focused on
emergent language, aiming to establish effective human-machine communication. As highlighted in section 2.2, the
primary goal of emergent language research is to learn a human-like language from scratch. The goal of our survey
is, however, to classify the literature on learning tasks with communication that aims at exploiting communication to
accomplish multi-agent tasks.
In summary, existing surveys in Comm-MADRL lack coverage of the latest developments. These surveys also do
not elaborate on the fact that communication itself is a combinatorial problem. Importantly, communication models
engage with MADRL algorithms across various processes, including learning and decision-making. To effectively
distinguish between existing Comm-MADRL approaches, it is crucial to analyze and classify them from a wider range
of perspectives. In the following section, we delve into the field of Comm-MADRL through multiple dimensions, each
linked to a unique research question pertinent to system design. These dimensions allow us to provide a fine-grained
classification, highlighting the differences between Comm-MADRL approaches even within similar domains.

6

Homogeneous agents have the same internal structure including goals, domain knowledge, and possible actions.

6

Key Components
Problem Settings

Communication
Processes

Training Processes

3

Table 1: Proposed dimensions and associated research questions.
Target Questions
Dimensions
What kind of behaviors are desired to emerge Controlled Goals
with communication?
How to fulfill realistic requirements?
Communication Constraints
Which type of agents to communicate with?
Communicatee Type
When and how to build communication links Communication Policy
among agents?
Which piece of information to share?
Communicated Messages
How to combine received messages?
Message Combination
How to integrate combined messages into learn- Inner Integration
ing models?
How to train and improve communication?
Learning Methods
How to utilize collected experience from agents? Training Schemes

Index
1
⃝
2
⃝
3
⃝
4
⃝
5
⃝
6
⃝
7
⃝
8
⃝
9
⃝

Learning Tasks with Communication in MADRL

In our survey, we consider explicit communication where action policies of agents are conditioned on communication that is learnable and dynamic, rather than static and predefined. Therefore, both the content of the messages
and the chances of communication occurrences are subject to learning. As agents engage in multi-agent tasks, they
learn domain-specific action policies and their communication protocols concurrently. As a result, learning tasks with
communication becomes a joint learning challenge, where agents employ reinforcement learning to maximize environmental rewards and simultaneously utilize various machine learning techniques to develop efficient and effective
communication protocols.
Learning tasks with communication in multi-agent deep reinforcement learning (Comm-MADRL) is a significant
research problem, particularly as communication can lead to higher rewards. Numerous studies have emerged, developing effective and efficient Comm-MADRL systems, often sharing similarities. Our review begins with the seminal
works such as DIAL[73], RIAL[73], and CommNet[48], and then expands to include the most relevant research works
presented at major AI conferences and journals like AAMAS, AAAI, NeurIPS, and ICML, totaling 41 models in
Comm-MADRL. To better distinguish among these models, we propose classifying them based on several dimensions
in Comm-MADRL system design. These dimensions aim to comprehensively cover the current literature, allowing
us to project the research works into a space where their similarities and differences become clear. We start by focusing on three key components of Comm-MADRL systems: problem settings, communication processes, and training
processes. Problem settings encompass both communication-specific settings (e.g., communication constraints) and
non-communication-specified settings (e.g., reward structures). Communication processes include common communication procedures, such as deciding whether to communicate and what messages to communicate. Training processes
cover the learning of both agents and communication within MADRL. Based on the three key components, we identify and summarize 9 research questions that commonly arise in Comm-MADRL system design, corresponding to 9
dimensions as detailed in Table 1. These research questions and dimensions are designed to capture various aspects
of Comm-MADRL, covering the learning objectives of agents and communication, the processes by which messages
are generated, transmitted, integrated, and learned within the MADRL framework. We outline a systematic procedure
for providing a guideline to effectively navigate through these dimensions when developing Comm-MADRL systems.
The procedure allows us to organize the dimensions, demonstrate their relevance in system design, and guide the
creation of customized Comm-MADRL systems in a step-by-step manner.
As outlined in Procedure 1, N reinforcement learning agents employ communication throughout their learning and
decision-making. Initially, the learning objective for the N agents is set, defining rewards that induce cooperative,
competitive, or mixed behaviors, as captured by dimension 1. We then consider potential communication-specified
settings like limited resources, addressing the need for realistic scenarios as described in dimension 2. Dimension 3
identifies potential communicatees, determining the agents for messages to be received, which varies across domains.
At each time step, agents decide when and with whom to communicate, as highlighted in dimension 4. The patterns of
communication occurrences are structured like a graph, where links, either undirected or directed, aid information exchange. Subsequently, messages that encapsulate agents’ understanding of the environment are generated and shared,
relating to dimension 5. Given that agents often receive multiple messages, they must decide on how to combine these
messages effectively. This process, crucial for integrating messages into their policies or value functions, is captured
7

in dimensions 6 and 7. In cases of Comm-MADRL studies focusing on emergent language (i.e., learning tasks with
emergent language), where messages are modeled as communicative acts emitted alongside domain-level actions, a
specific rearrangement of the procedure is required. Here, messages are not observed by other agents until the next
time step. Therefore, the processes outlined in dimensions 6 and 7 (lines 8 and 9) are moved to the front of those in
dimension 4 (line 6). This rearrangement allows agents to combine and integrate messages from the previous time
step before initiating new communication. As a result, agents make decisions and perform actions in the environment
based not only on their environmental observations but also on information obtained from other agents (lines 10 and
11). During the training phase, experiences from both environmental interactions and inter-agent communication are
utilized to train how agents will behave and communicate, i.e., agents’ policies, value functions, and communication
processes, as characterized in dimensions 8 and 9 (line 14).
In the following sections, we make an extensive survey on Comm-MADRL based on each dimension and classify the
literature when we focus on a specific dimension. We finally provide a comprehensive table to frame recent works
with the aid of the 9 dimensions.
Procedure 1 A guideline of Comm-MADRL systems
Require: N reinforcement learning agents
1: Set goals for reinforcement learning agents
2: Set possible communication constraints
3: Set the type of communicatees
4: for episode = 1, 2, ... do
5:
for every environment step do
6:
Decide with whom and whether to communicate
7:
Decide which piece of information to share
8:
Combine received information shared from others
9:
Integrate messages into agents’ internal models
10:
Select actions based on communication
11:
Perform in the environment (and store experiences)
12:
end for
13:
if training is enablled then
14:
Update agents’ policies, value function, and communication processes
15:
end if
16: end for

1
▷ Dimension ⃝
2
▷ Dimension ⃝
3
▷ Dimension ⃝
4
▷ Dimension ⃝
5
▷ Dimension ⃝
6
▷ Dimension ⃝
7
▷ Dimension ⃝

8 &⃝
9
▷ Dimensions ⃝

Procedure 1: A guideline of Comm-MADRL systems. The guideline positions dimensions where communication
influences interaction with the environment and training phases.
3.1

Controlled Goal

With a given reward configuration, reinforcement learning agents are guided to achieve their designated goals and
interests. As agents communicate in order to obtain higher rewards, the goal of communication and the goal of
achieving domain-specific tasks are inherently aligned. The emergent behaviors of agents can be summarized into
three types: cooperative, competitive, and mixed [96, 23], each corresponding to different reward configurations
and goals. Notably, some Comm-MADRL methods have been tested in more than one benchmark environment to
show their flexibility and scalability, where the reward configurations may vary [49, 81, 89, 77, 92]. Furthermore, a
multi-agent environment may consist of both fixed opponents and teammates, which typically do not participate in
communication. Therefore, we exclude fixed agents when identifying reward configurations. Consequently, we focus
on (learnable) agents involved in communication and classify their behaviors that are desired to emerge, aligning them
with associated reward configurations (summarized in Table 2).
Cooperative In cooperative scenarios, agents have the incentive to communicate to achieve better team performance.
Cooperative settings can be characterized by either a global reward that all agents share or a sum of local rewards that
could be different among agents. Communication is usually used to promote cooperation as a team. Thus, in the
literature, a team of agents can receive a global reward [73, 48, 61, 88, 39, 89, 40, 41, 43, 74, 31, 75, 66, 76, 63,
77, 78, 79, 90, 32, 91, 92, 93, 80, 64, 33, 94], which does not account for the contribution of each agent. The
agents can also receive local rewards, with designs to make the reward depend on teammates’ collective performance
[50, 88, 81, 49, 42, 82, 83, 87, 91], to penalize collisions [39, 81, 82, 83, 90, 86, 92, 93], or to share the reward with
other agents for encouraging mutual cooperation [84, 85, 65].
There are a variety of cooperative environments where communication has shown performance improvements, from
small-scale games to complex video games. In early works, Foerster et al. [73] developed two simple games, named
8

Table 2: The category of controlled goals.
Types

Configurations

Methods

Cooperative

Global Rewards

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; MAGNetSA-GS-MG [40]; MADDPG-M [41]; SchedNet [43]; AgentEntity Graph [74]; VBC [31]; NDQ [75]; IMAC [66]; GatedACML [76]; Bias [63]; LSC [77]; Diff Discrete[78]; I2C
[79]; TMC [32]; GAXNet [80]; DCSS [64]; MAIC [33];

Local Rewards

BiCNet [50]; DGN [81]; IC3Net [49]; MD-MADDPG [42];
DCC-MD [82]; GA-Comm [83]; NeurComm [84]; IP [85];
ETCNet [86]; Variable-length Coding [87]; AE-Comm [65];

Global or Local Rewards

MS-MARL-GCM [88]; ATOC [39]; TarMAC [89]; IS [90];
HAMMER [91]; MAGIC [92]; FlowComm [93]; FCMNet
[94];

Competitive

Conflict Rewards

IC3Net [49]; R-MACRL [95];

Mixed

Self-interested Rewards

IC [62]; DGN [81]; TarMAC [89]; IC3Net [49]; NDQ [75];
LSC [77]; MAGIC [92];

Switch Riddle and MNIST Games, for their proposed models, DIAL and RIAL. Sukhbaatar et al. [48] used Traffic
Junction for evaluating CommNet, which has become a popular testbed in recent works [88, 89, 49, 83, 79, 90, 92].
Among them, MAGIC [92] achieved higher performance on Traffic Junction with local rewards compared to two
early works, CommNet [48], IC3Net [49], and one recent work, GA-Comm [83]. StarCraft [97, 98, 99] is another
benchmark environment in cooperative MARL with relatively flexible settings. BiCNet [50] and MS-MARL-GCM
[88] are evaluated on an early version of StarCraft [97]. Then, a new version of StarCraft, SMAC, has become
popular in recent works [31, 75, 66, 32, 33, 94]. By controlling a team of agents, the cooperative goal in SMAC is
to defeat enemies on easy, hard, and super hard maps. FCMNet [94] and MAIC are two recent works that surpass
multiple communication methods and value decomposition methods (e.g., QMIX) on different maps. Google research
football [100] is an even more challenging game with a physics-based 3D soccer simulator. Only MAGIC has reported
performance on this platform with communication, and more investigations on this environment are needed. Compared
to the above approaches in Comm-MADRL, ATOC [39] has been examined using a significantly larger number of
learning agents in the predator-prey domain. Predator-prey is a grid world game with a long history in MARL. It
has been developed with several versions [101, 102, 7], while still viewed as a standard test environment due to its
flexibility and customizability. ATOC reports performance on this platform with continuous state and action spaces. In
the subfield learning tasks with emergent language, cooperative scenarios are popularly used. They are mostly based
on grid world or particle environments and have explicit role assignments, e.g., senders and receivers [61, 63, 65, 64].
Competitive In case agents need to compete with each other to occupy limited resources, they are assigned competitive learning objectives. In some competitive games, such as zero-sum games, one player wins and the others lose
and therefore rational agents do not have the incentive to communicate. Nevertheless, in other competitive scenarios
where agents compete for long-term goals, communication can allow for low-level cooperation among agents before
the (long-term) goals are achieved. Based on our observations, only one work, IC3Net [49], tests competitive settings and enables agents to compete for rewards.7 IC3Net shows that competitive agents communicate only when it
is profitable, e.g., before catching prey in the predator-prey domain. R-MACRL [95] considers communication from
malicious agents to improve the worst-case performance. In R-MACRL, the whole environment is cooperative while
agents learn to defend against malicious messages. Although the environment is cooperative, we classify this work
under the competitive category as the learning goal between malicious agents and other agents is competitive.
Mixed For a MAS where we care about self-interest agents, individual rewards can be designed and distributed to
each agent [81, 89, 49, 75, 77, 92, 94]. Therefore, cooperative and competitive behaviors coexist during learning,
which may show more complex communication patterns. Specifically, DGN [81] considers a game where each agent
gets positive rewards by eating food but gets higher rewards by attacking other agents. However, being attacked will
get a high punishment. With communication, agents can learn to share resources collaboratively rather than attacking
each other. IC3Net [49], TarMAC [89] and MAGIC [92] are evaluated on a mixed version of Predator-prey, and agents
learn to communicate only when necessary. NDQ [75] is examined in an independent search scenario, where two
7

IC3Net has been tested in several settings, including cooperative, competitive, and mixed scenarios, with different reward
configurations.

9

Table 3: The category of communication constraints.
Types

Subtypes

CommNet [48]; BiCNet [50]; MS-MARL-GCM [88]; ATOC
[39]; DGN [81]; TarMAC [89]; MAGNet-SA-GS-MG [40];
MADDPG-M [41]; IC3Net [49]; MD-MADDPG [42]; DCC-MD
[82]; Agent-Entity Graph [74]; GA-Comm [83]; LSC [77]; NeurComm [84]; IP [85]; I2C [79]; IS [90]; HAMMER [91]; MAGIC
[92]; FlowComm [93]; GAXNet [80]; FCMNet [94];

Unconstrained Communication

Constrained Communication

Methods

Limited Bandwidth

RIAL [73]; DIAL [73]; GCL [61]; IC [62]; SchedNet [43]; VBC
[31]; NDQ [75]; IMAC [66]; Gated-ACML [76]; Bias [63]; ETCNet [86]; Variable-length Coding [87]; TMC [32]; AE-Comm
[65]; MAIC [33];

Corrupted
Messages

DIAL [73]; Diff Discrete[78]; DCSS [64]; R-MACRL [95];

agents are rewarded according to their own goals, and shows that agents learn to not communicate in independent
scenarios. IC [62] considers a scenario in which sender and receiver agents have different abilities to complete the
goal. The sender agents have more vision but cannot clean obstacles, while receiver agents have limited vision but are
able to clear obstacles. With communication, agents show collaborative behaviors to get higher rewards.
3.2 Communication Constraints
Practical concerns such as communication cost and noisy environment impair Comm-MADRL systems from embracing realistic applications more than simulations. This dimension, Communication Constraints, determines which type
of communication concerns are handled in a Comm-MADRL system. We categorize recent works on this dimension
into the following categories (summarized in Table 3).
Unconstrained Communication In this category, communication processes, including communication channels,
the content and transmission of messages, and the decisions of whether to communicate or not, are not explicitly
restricted. In principle, agents can communicate as much as information they can without any decision to disallow
communication in order to prevent communication overhead [48, 50, 88, 81, 89, 40, 42, 90, 91, 80, 94]. Specifically,
several works consider blocking communication through predefined or learnable decisions of whether to communicate
or not, while aiming to differentiate useful communicated information [39, 41, 49, 82, 74, 83, 77, 84, 85, 79, 92, 93].
We also put those works under this category as they do not explicitly assume that communication is limited by cost.
Constrained Communication In this category, communication processes are explicitly constrained by cost or noise.
Thus, agents need to utilize communication resources efficiently to promote learning. We further identify two practical
concerns that have been considered in the literature.
• Limited Bandwidth. In this category, communication bandwidth is limited by channel capacity. Thus, communication needs to be used more efficiently, both in the number of times that agents can communicate and the size of
communicated information. Early works focus on transmitting succinct messages to avoid communication overhead.
RIAL and DIAL [73] are proposed to communicate very little information (i.e., a binary value or a real number)
at every time step to reduce the bandwidth needed. MD-MADDPG [42] considers a fixed-size memory, which
is shared by all agents. Agents communicate through the shared memory instead of ad hoc channels. VBC [31]
and TMC [32] reduce communication costs by using predefined thresholds to filter unnecessary communication,
and both show lower communication overhead. NDQ [75] cuts 80% of messages by ordering the distributions of
messages according to their means and drops accordingly to prevent meaningless messages. MAIC [33] also cuts
messages by examining several message pruning rates. In MAIC, messages are encoded to consider their respective
importance. Sent messages are ordered and then pruned with a given pruning rate. IMAC [66] explicitly models
bandwidth limitation as a constraint to optimization. An upper bound of the mutual information between messages
and observations is derived according to bandwidth constraint, which turns out to minimize the entropy of messages.
Then agents learn not only to maximize cumulative rewards but also to generate low-entropy messages. The number
of agents to communicate can also be restricted to reduce the total amount of communication. SchedNet [43] considers a scenario of a shared channel together with limited bandwidth. Only a subset of agents are chosen to convey
their messages according to their importance. Gated-ACML [76] learns a probabilistic gate unit to block messages
transmitting between each agent and a centralized message coordinator, with the extra cost of learning optimal gates.
Inspired by Gated-ACML and IMAC, ETCNet [86] puts constraints on the behaviors of deciding whether to send
10

messages or not. A penalty term is added to the environment rewards, and an additional reinforcement learning
algorithm is used to optimize the sending behaviors. Variable-length Coding [87] also utilizes a penalty term while
encouraging short messages. When learning tasks with emergent language, symbolic languages are acquired for
communication through a limited number of tokens. Therefore, we classify those works under limited bandwidth
[61, 62, 63, 65].
• Corrupted Messages. In this category, messages transmitted among agents can be corrupted due to environmental
noise or malicious intentions. DIAL [73] shows that during training, adding Gaussian noise to the communication
channel can push the distribution of messages into two modes to convey different types of information. Diff Discrete
[78] considers how to backpropagate gradients through a discrete communication channel (between 2 agents) with
unknown noise. An encoder/channel/decoder system is modeled, where the encoder is used to discretize a realvalued signal into a discrete message to pass through the discrete communication channel, and the decoder is used
to compute an approximation of the original signal. Later they show that the encoder/channel/decoder system is
equivalent to an analog communication channel with additive noise. With the additional assumption that training
is centralized, the gradient of the receiver with respect to real-value messages from the sender can be computed
to allow backpropagation. DCSS [64] also considers a noisy setting. They prove that representing messages as
one-hot vectors may not be optimal when the environment becomes noisy. Inspired by word embedding in the
NLP field, they propose to generate a semantic representation of discrete tokens that are communicated among
agents. The results show that such representation is robust in noisy environments and benefits human understanding
of communication. Different from noisy environments, R-MACRL [95] assumes that an agent holds a malicious
messaging policy, producing adversarial messages that can mislead other agents’ action selections. Therefore, other
agents need to prevent being exploited by learning a defense policy in order to filter the messages.
3.3 Communicatee Type
Communicatee Type determines which type of agents are assumed to receive messages in a Comm-MADRL system.
We found that in the literature, communicatee type can be classified into the following categories based on whether
agents in the environment communicate with each other directly or not.
Agents in the MAS In this category, the set of communicatees consists of agents in the environment, and they directly communicate with each other. Nevertheless, due to partial observability, agents may not be able to communicate
with every agent in the MAS, and thus we further distinguish the types of communicatees as follows:
• Nearby Agents. In many Comm-MADRL systems, communication is only allowed between neighbors. Nearby
agents can be defined as observable agents [80], agents within a certain distance [81, 74, 77] or neighboring agents
on a graph [84]. GAXNet [80] labels observable agents and enables communication between them. DGN [81] limits
communication within 3 closest neighbors while using a distance metric to find them. Agent-Entity Graph [74] also
uses distance to measure whether agents are nearby or not. As long as two agents are close to each other, they will
be allowed to communicate. LSC [77] enables agents within a cluster radius to decide whether to become a leader
agent. Then all non-leader agents in the same cluster will only communicate with the leader agent. NeurComm [84]
and IP [85] preset a graph structure among agents built upon networked multi-agent systems. In both NeurComm
and IP, communicatees are restricted to neighbors on the graph. MAGNet-SA-GS-MG [40] uses a pre-trained graph
to limit communication and restricts communication on neighboring agents. Neighboring agents can also emerge
during learning instead of being predetermined, as proposed in GA-Comm [83], MAGIC [92] and FlowComm [93],
which explicitly learn a graph structure among agents. Specifically, in GA-Comm [83] and MAGIC [92], a central
unit (e.g., GNN) learns a graph inside and coordinates messages based on the (complete) graph simultaneously. In
this case, agents do not communicate with each other directly; instead, they communicate through a virtual agent
who does not affect the environment. Therefore, we categorize these two works into the proxy category.
• Other (Learning) Agents. If nearby agents are not identified, the set of communicatees typically consists of other
(learning) agents. Specifically, IC3Net [49] enables communication between learning agents and their opponents.
Experiments indicate that these opponents eventually learn to not communicate to avoid being exploited. Some
works assume explicit role assignments, i.e., senders and receivers. The role of the receiver can be taken by a
disjoint set of agents separate from the senders [62, 63, 64] or by all other agents in the environment [61, 65]. In
both cases, agents communicate with each other directly.
Proxy A proxy is a virtual agent that plays an essential role (e.g., as a medium) in facilitating communication but
does not directly affect the environment. Using a proxy as the communicatee means that agents will not directly
communicate with each other, instead viewing the proxy as a medium, coordinating and transforming messages for
specific purposes. MS-MARL-GCM [88] utilizes a master agent that collects local observations and hidden states
from agents in the environment and sends a common message back to each of them. Similarly, HAMMER [91]
employs a central proxy that gathers local observations from agents and sends a private message to each agent.
11

Table 4: The category of communicatee type.
Types

Subtypes

Methods

Agents in the MAS

Nearby Agents

DGN [81]; MAGNet-SA-GS-MG [40]; Agent-Entity Graph [74];
LSC [77]; NeurComm [84]; IP [85]; FlowComm [93]; GAXNet
[80];

Other Agents

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet [50];
IC [62]; TarMAC [89]; MADDPG-M [41]; IC3Net [49]; SchedNet [43]; DCC-MD [82]; VBC [31]; NDQ [75]; Bias [63]; Diff
Discrete[78]; I2C [79]; IS [90]; ETCNet [86]; Variable-length
Coding [87]; TMC [32]; AE-Comm [65]; DCSS [64]; R-MACRL
[95]; MAIC [33]; FCMNet [94];
MS-MARL-GCM [88]; ATOC [39]; MD-MADDPG [42]; IMAC
[66]; GA-Comm [83]; Gated-ACML [76]; HAMMER [91];
MAGIC [92];

Proxy

Agent 4

Proxy
Agent 2
Agent 3

Agent 1
s

ob

le
ab
erv

ld

fie

Agent 5

le
ab
erv

ld
fie

s
ob

Figure 2: Three communicatee types in the same system.

MD-MADDPG [42] maintains a shared memory among agents, learning to selectively store and retrieve local
observations from the memory. IMAC [66] defines a scheduler that aggregates encoded information from all agents
and sends individual messages to each agent. These works primarily focus on how to encode messages through
the proxy without determining whether to send or receive messages. By contrast, ATOC [39], Gated-ACML [103],
GA-Comm [83] and MAGIC [92] are all designed for agents to decide whether to communicate with a message
coordinator. In ATOC and Gated-ACML, each agent’s decisions are made locally based on individual observations,
with messages aggregated from nearby agents and from the entire MAS, respectively. Both GA-Comm and MAGIC
develop a global communication graph, coupled with a graph neural network (GNN) to aggregate messages by
weights and send new messages back to each agent, informing action selection in the environment.
Table 4 summarizes recent works on communication types in MAS. To illustrate these categories, we present an
example of different communication methods used in a Comm-MADRL system in Figure 2. The system consists of
five agents and one proxy. Agent 3 is the nearby agent of Agent 1, while Agent 4 is the nearby agent of Agent 2. Agent
5 is out of the view range of Agents 1 and 2. If communication is limited to nearby agents, Agent 1 will communicate
only with Agent 3, and Agent 2 will communicate only with Agent 4. However, if communication involves a proxy,
all agents can send their messages to the proxy and receive coordinated messages.
3.4

Communication Policy

Communication Policy determines when and with which agents (i.e., communicatees) to communicate in order to
enable message transmission. A Communication Policy defines a set of communication actions, which can be modeled
in different ways. For example, a communication action can be represented as a vector of binary values, where each
value indicates whether communication with one of the other agents is allowed at a certain time step. These actions
form communication links between pairs of agents, which can be represented as a communication graph among agents.
In the literature, communication policies can be either predefined or learned, allowing communication with all other
agents or only a subset of agents. Furthermore, communication policies can be centralized, controlling communication
among all agents, or decentralized, enabling individual agents to control whether to communicate. Therefore, we first
categorize the literature based on whether communication policies are predefined or learned. We find that in predefined
communication policies, the literature often uses either full communication among agents, where the communication
12

Table 5: The category of communication policy
Types

Subtypes

Methods

Predefined

Full Communication

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet
[50]; MS-MARL-GCM [88]; TarMAC [89]; MD-MADDPG
[42]; DCC-MD [82]; IMAC [66]; Diff Discrete[78]; IS [90];
Variable-length Coding [87]; HAMMER [91]; AE-Comm [65];
R-MACRL [95]; FCMNet [94];

Partial Structure

IC [62]; DGN [81]; MAGNet-SA-GS-MG [40]; Agent-Entity
Graph [74]; VBC [31]; NDQ [75]; Bias [63]; NeurComm [84];
IP [85]; TMC [32]; GAXNet [80]; DCSS [64]; MAIC [33];

Individual Control

ATOC [39]; MADDPG-M [41]; IC3Net [49]; Gated-ACML
[76]; LSC [77]; I2C [79]; ETCNet [86];

Global Control

SchedNet [43]; GA-Comm [83]; MAGIC [92]; FlowComm
[93];

Learnable

Comm
Policy

A1

A1

A1

A1

Comm
A1 Policy
1

A1

A1

A2
...

A2
...

A2
...

A2
...

Comm
A2 Policy
2
...

A2
...

A2
...

A2
...

An

An

An

An

Comm
An Policy
n

An

An

An

P

P

P

P

P

P

P

P

A1

Figure 3: Four types of communication policy with agents (shown as A) in the environment and a possible proxy
(shown as P).

graph becomes complete, or a partial graph structure to incorporate constraints on communication policies. On the
other hand, in learnable communication policies, we identify two distinct categories: individual control and global
control. In individual control, communication policies are learned by each agent independently, whereas in global
control, these policies are learned and implemented centrally, applying to all agents in Comm-MADRL systems. As
a result, we have identified four subcategories within the dimension of communication policy: Full Communication,
(Predefined) Partial Structure, Individual Control, and Global Control. These categorizations are summarized in Table
5.
We present examples of how agents form communication links in the four categories of communication policy, as
illustrated in Figure 3. Both Full Communication and Partial Structure rely on a predefined communication policy to
determine communication actions. In contrast, Individual Control and Global Control involve the learning of a local
communication policy and a global communication policy, respectively, to establish communication links between
agents or a potential proxy. If a proxy is involved, it coordinates messages from agents choosing to communicate
through this proxy. The categories and their associated research works are introduced as follows:
Full Communication In this category, every pair of agents is connected so that messages are transmitted in a broadcast manner. Full communication can be regarded as a fully connected graph, often used in early works on CommMADRL. DIAL [73], RIAL [73], CommNet [48], and BiCNet [50] learn a communication protocol which connect
all agents together. Inspired by BiCNet, FCMNet [94] uses multiple RNNs to link all agents together with different
sequences, allowing agents to benefit from communication flow from various directions. In contrast, Diff Discrete
[78] and Variable-length Coding [87] focus on two-agent cases but do not learn to block messages from each other.
TarMAC [89] and IS [90] learn meaningful messages while using a broadcast way to share messages, thus still adhering to full communication. DCC-MD [82] and R-MACRL [95] introduce a strategy to drop out received messages
without specifying whether to send messages. Specifically, DCC-MD drops out messages with a fixed probability to
reduce input dimensions, and R-MACRL learns to drop out adversary messages through a defense policy. In CommMADRL methods like IMAC [66], MS-MARL-GCM [88] and HAMMER [91], a central proxy that receives local
observations or encoded messages is always connected with agents in the MAS. In addition, GCL [61] and AE-Comm
13

[65] learn a language grounded on discrete tokens among agents, where all agents have the capability to send and
receive messages.
(Predefined) Partial Structure In this category, the communication between agents is captured by a predetermined
partial graph to reduce overall communication. Then, each agent communicates with a limited number of agents within
the MAS, rather than with every agent. NeurComm [84] and IP [85] operate in a networked multi-agent environment,
randomly generating a communication network while maintaining a fixed average number of connections per agent
during the learning process. DGN [81], MAGNet-SA-GS-MG [40], and GAXNet [80] restrict communication to a
certain proximity of agents. The Agent-Entity Graph [74] employs a pre-trained graph to capture agent relationships.
Comm-MADRL approaches like VBC [31], NDQ [75], TMC [32], and MAIC [33] utilize handcrafted thresholds or
pruning rates to limit communication opportunities. In IC [62], Bias [63], and DCSS [64], disjoint sets of agents are
designated as either senders or receivers, facilitating unidirectional communication from senders to receivers only.
Individual Control In this category, each agent actively and individually determines whether to communicate with
other agents, implicitly forming a graph structure. A common method employed in Comm-MADRL studies within
this category is a learnable gate mechanism, which aids agents in making the decision to communicate. For instance,
IC3Net [49] and ATOC [39] use a gate mechanism that enables agents to decide whether to broadcast their messages,
in a deterministic and probabilistic manner, respectively. ETCNet [86] also implements a gate unit but limits the
overall probability of message-sending behaviors. If a proxy, such as a message coordinator, is present, Gated-ACML
[76] introduces a learning mechanism for each agent to decide whether to communicate with the proxy, as opposed
to direct communication with other agents. Diverging from the gate function approach, I2C [79] allows each agent
to unilaterally decide on communication with other agents, based on evaluating the impact of those agents on its
own policy. LSC [77] allows each group of agents, defined by a specific radius, to compare their weights in order
to elect a leader. This system then facilitates communication from each group to their respective leaders and from
leader to leader. Notably, the leader agent in this model is not considered a proxy, as it still directly interacts with the
environment.
Global Control In this category, a globally shared communication policy is learned, providing more complete control over the communication links between agents. SchedNet [43] employs a global scheduler that limits the number
of agents allowed to broadcast their messages, thereby reducing overall communication. FlowComm [93] learns a directed graph among agents, enabling unilateral or bilateral communication between them. Similarly, GA-Comm [83]
and MAGIC [92] develop an undirected and a directed graph for communication, respectively. These Comm-MADRL
systems incorporate an additional message coordinator to coordinate and transform messages sent by the agents.
3.5 Communicated Messages
After establishing communication links among agents through a communication policy, agents should determine which
specific information to communicate. This information can derive from historical experiences, intended actions, or
future plans, enriching the messages with valuable insights. Consequently, the communicated information can expand
the agents’ understanding of the environment and enhance the coordination of their behaviors. In the dimension of
communicated messages, an important consideration is whether the communication includes future information, such
as intentions and plans. This kind of information, being inherently private, often requires an (estimated) model of the
environment to effectively simulate and generate conjectured intentions and plans. Accordingly, we categorize recent
studies in this dimension into two categories, as summarized in Table 6.
Existing Knowledge In this category, agents share their knowledge of the environment (e.g., past observations),
previous movements, or policies to assist other agents in selecting actions. As historical information accumulates,
agents use a low-dimensional encoding of their knowledge as messages to reduce communication overhead. Notably,
the RNN family (e.g., LSTM and GRU) is commonly used as an encoding function, capable of selectively retaining and
forgetting historical observations [48, 50, 88, 89, 49, 42, 83, 79, 92, 93, 33, 94], action-observation histories [73, 50],
or action-observation-message histories [62, 63]. When a proxy is present, messages are generated and transformed
from agents to the proxy, and then from the proxy to agents. Thus, local observations can either be encoded [42,
66, 83, 76, 92] or directly sent [88, 91] to the proxy. The proxy, after gathering these local (encoded) observations,
can generate a unified message for all agents [88], or individualized messages for each agent [42, 66, 83, 76, 91, 92].
Both methods provide a message containing global information, relieving agents from the task of combining multiple
received messages. In Comm-MADRL systems without a proxy, messages are sent directly to each agent. Specifically,
in MADDPG-M [41], agents communicate local observations without an encoding of them. On the other hand,
DIAL and RIAL [73] encode past observations, actions, and current observations as messages. BiCNet [50] encodes
both local observations of each agent and a global view of the environment. Other research works employ various
methods such as simple feed-forward networks [43, 78, 86, 87], MLP [40, 31, 32, 95], autoencoders [82], CNNs [81],
RNNs [48, 89, 49, 79, 93, 33, 94], or GNNs [74, 77] to encode local observations as messages. Furthermore, agents
can communicate more specific information, such as in GAXNet [80], where agents coordinate their local attention
14

Table 6: The category of communicated messages.
Types

Methods

Existing Knowledge

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet [50]; MS-MARLGCM [88]; IC [62]; DGN [81]; TarMAC [89]; MAGNet-SA-GS-MG [40];
MADDPG-M [41]; IC3Net [49]; MD-MADDPG [42]; SchedNet [43]; DCC-MD
[82]; Agent-Entity Graph [74]; VBC [31]; NDQ [75]; IMAC [66]; GA-Comm
[83]; Gated-ACML [76]; Bias [63]; LSC [77]; Diff Discrete[78]; I2C [79]; ETCNet [86]; Variable-length Coding [87]; TMC [32]; HAMMER [91]; MAGIC [92];
FlowComm [93]; AE-Comm [65]; GAXNet [80]; DCSS [64]; R-MACRL [95];
MAIC [33]; FCMNet [94];

Imagined Future Knowledge

ATOC [39]; NeurComm [84]; IP [85]; IS [90];
Table 7: The category of message combination.

Types

Methods

Equally Valued

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; IC [62]; MADDPG-M [41];
IC3Net [49]; SchedNet [43]; VBC [31]; NDQ [75]; Bias [63]; Diff Discrete[78];
IS [90]; ETCNet [86]; Variable-length Coding [87]; FlowComm [93]; AE-Comm
[65]; DCSS [64];

Unequally Valued

BiCNet [50]; MS-MARL-GCM [88]; ATOC [39]; DGN [81]; TarMAC [89];
MAGNet-SA-GS-MG [40]; MD-MADDPG [42]; DCC-MD [82]; Agent-Entity
Graph [74]; IMAC [66]; GA-Comm [83]; Gated-ACML [76]; LSC [77]; NeurComm [84]; IP [85]; I2C [79]; TMC [32]; HAMMER [91]; MAGIC [92]; GAXNet
[80]; R-MACRL [95]; MAIC [33]; FCMNet [94];

weights, integrating hidden states from neighboring agents. Messages can also be modeled as random variables, as
seen in NDQ [75], where messages are drawn from a multivariate Gaussian distribution to maximize expressiveness
by maximizing mutual information between messages and receivers’ action selection. In learning tasks with emergent
language, agents often communicate goal-related information, such as the goal’s location [61, 62, 63, 65, 64].
Imagined Future Knowledge In this context, Imagined Future Knowledge refers to aspects such as intended actions
[39], policy fingerprints (i.e., action probabilities in a given state) [85, 84], or future plans [90]. Since intentions
are related to the current environment state, recent works often combine intended actions with local observations to
produce more relevant messages. The concept of future plans extends this idea further by utilizing an approximated
model of the environment and the behavior models of other agents. This approach enables the generation of a sequence
of possible future observations and actions [90]. Such knowledge is shared among agents, allowing the receivers to
consider the potential future outcomes of the senders’ actions.
3.6 Message Combination
When agents receive more than one message, current works often aggregate all received messages to reduce the
input for the action policy. Message Combination determines how to integrate multiple messages before they are
processed by an agent’s internal model. If a proxy is involved, each agent receives already coordinated and combined
messages from the proxy, eliminating the need for further message combination. If no proxy is presented, each agent
independently determines how to combine multiple messages. Since communicated messages encode the senders’
understanding of the learning process or the environment, some messages can be more valuable than others. As shown
in Table 7, recent works in the dimension of message combination are categorized based on how agents prioritize
received messages.
Equally Valued In this category, messages received by agents are treated without preference, meaning they are
assigned equal weights or simply no weights at all. Without having preferences, agents can concatenate all messages,
ensuring no loss of information, though it may significantly expand the input space for the action policy [73, 61, 62,
41, 43, 75, 78, 90, 86, 87]. Recent research involving concatenated messages typically represent the sent messages
either as single values [73, 62, 87, 86] or as short vectors [61, 41, 43, 75, 78, 90]. Alternatively, messages can be
combined by averaging [48, 49, 31] or summing [93], under the assumption that messages from different agents
have the same dimension. In some cases, particularly in two-agent scenarios, no explicit preferences are assigned to
messages [63, 65, 64].
15

Table 8: The category of inner integration.
Types

Methods

Policy-level

CommNet [48]; GCL [61]; MS-MARL-GCM [88]; ATOC [39]; MAGNet-SAGS-MG [40]; IC3Net [49]; MD-MADDPG [42]; SchedNet [43]; IMAC [66]; GAComm [83]; Gated-ACML [76]; Diff Discrete[78]; IP [85]; I2C [79]; IS [90];
ETCNet [86]; Variable-length Coding [87]; HAMMER [91]; FlowComm [93];
GAXNet [80]; R-MACRL [95];

Value-level

DIAL [73]; RIAL [73]; DGN [81]; DCC-MD [82]; VBC [31]; NDQ [75]; LSC
[77]; TMC [32]; MAIC [33];

Policy- and Value-level

BiCNet [50]; IC [62]; TarMAC [89]; MADDPG-M [41]; Agent-Entity Graph [74];
Bias [63]; NeurComm [84]; MAGIC [92]; AE-Comm [65]; DCSS [64]; FCMNet
[94];

Unequally Valued In this category, messages are assigned distinct preferences, which potentially impose differences
on sender agents. DCC-MD [82] and TMC [32] use handcrafted rules to prune received messages. In DCC-MD, each
received message can be dropped out with a certain probability. TMC stores the received messages and checks whether
they are expired or not within a preset time window. Only valid messages are integrated into an agent’s model. Instead
of using fixed rules, R-MACRL [95] learns a gate unit to decide whether to use a received message. An attention
mechanism can also be learned to assign weights to received messages and then combine them, rather than filtering
messages out, as seen in research works [89, 40, 74, 33]. Moreover, a neural network can aggregate received messages
into a single message or a low-dimensional vector, which implicitly imposes preferences on messages during the
mapping. Feedforward neural networks [66, 76, 91], CNNs [81], LSTMs (or RNNs) [50, 88, 39, 42, 84, 79, 80, 94],
and GNNs [83, 77, 85, 92] have been used as aggregators. Among them, GNNs utilize a learned graph structure of
agents and assign different weights to neighboring agents.
3.7 Inner Integration
Inner Integration determines how to integrate (combined) messages into an agent’s learning model, such as a policy
or a value function. In most existing literature, messages are viewed as additional observations. Agents take messages
as extra input to a policy function, a value function, or both. Thus, in the dimension of inner integration, we classify
recent works into categories based on the learning model that is used to integrate messages. These categories are
summarized in Table 8.
Policy-level By exploiting information from other agents, each agent will no longer act independently. Policies can
be learned through policy gradient methods like REINFORCE, as seen in studies [48, 61, 88, 49, 83], which collect
rewards during episodes and train the policy models at the end of episodes. Moreover, the Comm-MADRL approaches
that utilize actor-critic methods [39, 40, 42, 43, 66, 76, 78, 85, 79, 90, 86, 87, 91, 93, 80, 95] assume that a critic model
(i.e., a Q-function) guides the learning of an actor model (i.e., a policy network).
Value-level In this category, a value function incorporates messages as input, and a policy is derived by selecting the
action with the highest Q-value. Most works in this category employ DQN-like methods to train their value functions
[73, 81, 82, 31, 75, 77, 32, 33]. Specifically, Comm-MADRL approaches like VBC [31], NDQ [75], TMC [32], and
MAIC [33] are based on value decomposition methods in cooperative scenarios (with global rewards). These methods
involve learning to decompose a joint Q-function.
Policy- and Value-level Integrating messages using both a policy function and a value function typically relies
on actor-critic methods. In Comm-MADRL approaches within this category, received messages can be treated as
extra inputs for both the actor and critic models [50, 74, 64]. Alternatively, messages can be combined with local
observations to generate new internal states, which are then shared with both the actor and critic models [62, 89, 41,
63, 84, 92, 65, 94].
3.8 Learning Methods
Learning methods determine which type of machine learning techniques is used to learn a communication protocol.
The learning of communication is at the center of modern Comm-MADRL and can benefit from the advancements
in the machine learning field. If proper assumptions about communication are made, such as being able to calculate
the derivatives with respect to the message generator function and the communication policy, then the training of
communication can be integrated into the overall learning process of agents. This integration allows for the use of fully
differentiable methods for backpropagation. Other machine learning techniques, including reinforcement learning,
16

Table 9: The assumptions behind different learning methods.
Types

Assumptions

Fully differentiable

The messages or the communication actions are generated by a differentiable function and thus backpropagation is used everywhere.

Supervised learning

True labels (or the ground truth) are assumed to be given or defined to
guide the learning of communication policy or messages.

Reinforcement learning

Environment rewards or self-defined rewards are used to update communication policy or messages incrementally.

Regularizers

Regularizations such as entropy inspired from information theory are
added to agents’ optimization objectives to regularize the learning of
communication.
Table 10: The category of learning methods.

Types

Methods

Differentiable

GCL [61]; DIAL [73]; CommNet [48]; BiCNet [50]; MS-MARL-GCM [88]; DGN
[81]; TarMAC [89]; MAGNet-SA-GS-MG [40]; MD-MADDPG [42]; DCC-MD
[82]; Agent-Entity Graph [74]; VBC [31]; GA-Comm [83]; Diff Discrete[78];
NeurComm [84]; IP [85]; IS [90]; Variable-length Coding [87]; TMC [32];
MAGIC [92]; FlowComm [93]; GAXNet [80]; DCSS [64]; FCMNet [94];

Supervised

DCSS [64]; ATOC [39]; Gated-ACML [76]; I2C [79]; R-MACRL [95];

Reinforced

GCL [61]; RIAL [73]; IC [62]; MADDPG-M [41]; IC3Net [49]; SchedNet [43];
LSC [77]; ETCNet [86]; HAMMER [91];

Regularized

NDQ [75]; IMAC [66]; Bias [63]; AE-Comm [65]; MAIC [33];

supervised learning, and regularizations, can also be utilized to incorporate our requirements and available ground
truth into the learning of communication, each carrying respective assumptions. The assumptions used in the literature
are summarized in Table 9. For instance, supervised methods require defining true labels for communication (e.g., the
correct information to share or the right agents to communicate with). In contrast, reinforced methods use rewards
as learning signals. Regularized methods, which use neither true labels nor rewards, employ an additional learning
objective by using regularizers, such as minimizing the entropy of messages to reduce stochasticity. Therefore, we
classify recent works based on how they differ in the learning of communication (summarized in Table 10).
Differentiable In this category, communication is learned and improved by backpropagating gradients from agent to
agent. When the communication policy is predefined, such as in full communication [73, 48, 61, 50, 88, 89, 42, 82, 78,
90, 87, 94] or by communicating with a subset of agents [81, 40, 74, 31, 84, 85, 32, 80, 64], agents learn the content of
messages through backpropagation. Several recent studies [61, 83, 92, 93, 64] address the issue of non-differentiable
communication actions by utilizing gradient estimators like Gumbel-softmax [104], which replaces non-differentiable
samples with a differentiable approximation during training, albeit requiring additional parameter tuning. Specifically,
both GCL [61] and DCSS [64] employ a differential message function in their approaches. Additionally, GCL integrates auxiliary rewards, and DCSS utilizes labeled messages for training communication policies. Thus, they are categorized under the Differentiable category, each additionally aligning with the Reinforced and Supervised categories
respectively. Freed et al. [78] propose an alternative method, Diff Discrete, to address the challenge of continuous
messages versus discrete channels. This method models message transmitting as an encoder/channel/decoder system,
where the receiver decodes the messages and reconstructs the original signals. These reconstructed signals enable the
calculation of derivatives with respect to the sender, allowing gradients to be sent back to the sender.
Supervised In this category, additional efforts need to be made to define the true label for when and what information to communicate. ATOC [39] and Gated-ACML [76] use the difference in Q-values between actions chosen with
and without a message to define a label of communication actions. If the difference exceeds a threshold, the message is
17

deemed valuable, indicating a high probability of sending it; otherwise, the probability is 0. This process sets up a classification task to decide whether to communicate. Similarly, I2C [79] trains a classifier to determine communication
but relies on the causal effect between two agents, using a threshold to tag effective communication. R-MACRL [95]
learns a classifier to identify malicious messages, using the status of a message (malicious or not) as a label. DCSS
[64] learns message content by using a small dataset that maps observations to desired communication symbols. In
DCSS, the gradient from the supervised loss is added to the policy loss, leading agents to use communication that
aligns with the grounding data and enables high task performance.
Reinforced In this category, reinforcement learning is utilized to train communication in addition to the learning
of action policies. RIAL [73] and HAMMER [91] focus on learning the content of messages through reinforcement
learning, without addressing the decision of whether to communicate. In GCL [61], auxiliary rewards are used for
predicting goals and consolidating symbols, facilitating the development of a compositional language for communication. IC [62] employs the difference in outcomes from using and not using communication on action policies as
rewards. Maximizing the rewards can enhance the influence of communication on the receivers’ action policies. Other
studies [41, 49, 43, 77, 86] consider both the learning of communication content and the decision to communicate. Notably, MADDPG-M [41] suggests using intrinsic rewards to train the communication policy instead of relying solely
on environmental rewards. ETCNet [86] shapes environmental rewards by introducing a penalty term to discourage
unnecessary communication.
Regularized Regularized methods are used to reduce redundant information in communication [75, 66, 33]. NDQ
[75] calculates a lower bound of the mutual information between received messages and the receivers’ action selection.
This approach suggests that messages can be optimized to decrease the uncertainty in the action-value functions of the
receivers. IMAC [66] establishes an upper bound on the mutual information between messages and the senders’ observations, and minimizing this upper bound helps agents send messages with lower uncertainty. MAIC [33] employs an
estimated model of teammates and aims to maximize the mutual information between teammates’ actions and hidden
variables from this model. This model then guides the encoding of messages, resulting in tailored communications
for different agents. Bias [63] focuses on the long-term impact of messages on agents’ decision-making to enhance
signaling and listening effectiveness. AE-Comm [65] adopts an autoencoder to learn a low-dimensional encoding of
observations.
3.9

Training Schemes

This dimension focuses on how to utilize the collected experiences (such as observations, actions, rewards, and messages) of agents to train their action policies and communication architectures in a Comm-MADRL system. Agents
can train their models in a fully decentralized manner using only their local experience. Alternatively, when global
information is accessible, the experiences of all agents can be collected to centrally train a single (centralized) model
that controls all agents. However, each approach has inherent challenges. Fully decentralized learning must cope with
a non-stationary environment due to the changing and adapting behaviors of agents, while fully centralized learning
faces the complexities of joint observation and policy spaces. As a balanced solution, Centralized Training and Decentralized Execution (CTDE) [105, 73] has emerged as a popular training schemes in MADRL. CTDE approaches allow
agents to learn their local policies using guidance from central information. Therefore, in the dimension of training
schemes, we categorize recent works based on how agents’ experiences are collected and utilized, as detailed in Table
11.
Centralized Learning As shown in Figure 4a, experiences are gathered into a central unit, then learning to control
all agents. Based on our observations, recent works on Comm-MADRL usually do not assume a central controller.
Fully Decentralized Learning As illustrated in Figure 4b, in fully decentralized learning, experiences are collected
individually by each agent, and they undergo independent training processes. Recent works in this category often
employ actor-critic based methods for each agent [40, 41, 82, 74, 84, 85, 95]. Specifically, decentralized learning has
gained much attention in learning tasks with emergent language, as it most closely resembles language learning in
nature [62, 63, 65].
Centralized Training and Decentralized Execution In CTDE approaches, the experiences of all agents are collectively used for optimization. Gradients derived from the joint experiences of agents guide the learning of local
policies. However, once training is complete, only the policies are needed and gradients can be discarded, facilitating
decentralized execution. When agents are assumed to be homogeneous, meaning they have identical sensory inputs,
actuators, and model structures, they can share parameters. Parameters sharing reduces the overall number of parameters, potentially enhancing learning efficiency compared to training in separate processes. Despite sharing parameters,
agents can still exhibit distinct behaviors because they are likely to receive different observations at the same time step.
Based on these considerations, recent works in this field can be further divided into the following subcategories.
18

Environment

<a1, a2, ..., an>
Policy 1

Environment

<o1, o2, ..., on>

Policy 2

...

a1

Policy n

Policy 1

Update with <r1, r2, ..., rn>

a2

o2

Policy 1

Policy 2

Update

Update

Central Unit

...

Policy 2

on

Policy n

Update with rn

Environment

an
...

an

(b) Fully Decentralized Learning

Environment
o1

o2

Update with r1 Update with r2

(a) Fully Centralized Learning

a1

a2

o1

on

a1

o1

Policy n

Policy 1

Update

Update

Global Information
e.g., rewards

a2

Policy 2

Central Unit

(c) Individual Parameters

an
...

o1

a2

o2

an
...

on

Policy 1

Policy 2

Policy n

Update

Update

Update

Central Unit 1

Central Unit 2

Central Unit n

Global Information (e.g., rewards)

(e) Concurrent

Figure 4: Five types of Training Schemes.

19

on

Policy n

Global Information
e.g., rewards

(d) Parameter Sharing
Environment

a1

o2

Table 11: The category of training schemes.
Types

Subtypes

IC [62]; MAGNet-SA-GS-MG [40]; MADDPG-M [41]; DCCMD [82]; Agent-Entity Graph [74]; Bias [63]; NeurComm [84];
IP [85]; AE-Comm [65]; R-MACRL [95];

Fully
Decentralized
Learning
Centralized Training
and
Decentralized
Execution

Methods

Individual Parameters

MS-MARL-GCM [88]; SchedNet [43]; IMAC [66]; GatedACML [76]; GAXNet [80]; DCSS [64];

Parameter Sharing

DIAL [73]; RIAL [73]; CommNet [48]; GCL [61]; BiCNet
[50]; ATOC [39]; DGN [81]; TarMAC [89]; IC3Net [49]; VBC
[31]; NDQ [75]; GA-Comm [83]; LSC [77]; Diff Discrete[78];
I2C [79]; ETCNet [86]; Variable-length Coding [87]; TMC
[32]; HAMMER [91]; MAGIC [92]; FlowComm [93]; MAIC
[33]; FCMNet [94];

Concurrent

MD-MADDPG [42]; IS [90];

• Independent Policies. In this category, each local policy is trained with its own set of learning parameters. A central
unit collects experiences from all agents to provide global information and guidance, such as gradients, as depicted
in Figure 4c. The training of the entire system can employ policy gradient algorithms (e.g., using REINFORCE)
[88], or actor-critic methods [43, 66, 76, 80, 64].
• Parameter Sharing. In this category, all local policies (or local value functions) utilize a shared set of parameters,
as illustrated in Figure 4d. Commonly used algorithms in this scenario include DQN-like algorithms, actor-critic
methods, and policy gradient algorithms with REINFORCE. When employing a DQN-like algorithm, a shared
local Q-function, which processes each agent’s individual experience, is learned collectively across agents [73, 81,
77]. Additionally, DQN-based methods can be integrated with value decomposition models (e.g., QMIX [28])
in cooperative environments, which enable learning from factorized rewards (value functions) [31, 75, 32, 33].
In the case of actor-critic methods, a shared actor (i.e., policy model) is trained using all individual experiences,
supported by gradient guidance from a central critic [50, 39, 89, 78, 79, 86, 87, 91, 92, 93, 94]. Policy gradient with
REINFORCE can alternatively be used, requiring the collection of sampled rewards over episodes [48, 61, 49, 83].
• Concurrent. In scenarios where storing all experiences in a central unit is not feasible, agents can alternatively
create backups of all experiences, with the assumption that they are able to observe other agents’ actions and observations. The concurrent approaches differ inherently from fully decentralized learning. In CTDE with concurrent
approaches, each agent maintains an individual set of policy parameters and receives the guidance from a local unit
that collects global information (with additional assumptions on observability), as depicted in Figure 4e. Concurrent
CTDE often employs actor-critic methods, where each agent has its own central critic to guide its local actor (policy)
[42, 90].
3.10

Possible Relations of Dimensions

We have introduced 9 dimensions for Comm-MADRL and identified a range of categories within each dimension. It
is crucial to consider the potential interdependencies among these dimensions. We realize that the dimensions do not
inherently depend on one another based on the criteria used for classifying the literature. However, specific implementations of Comm-MADRL systems may create dependencies between dimensions. For instance, limited bandwidth
constraints (defined in the communication constraints dimension) can be realized by setting a limited number of times
to communicate, rendering the full communication category (within the communication policy dimension) infeasible.
This scenario illustrates how the dimensions of communication constraints (Section 3.2) and communication policy
(Section 3.4) become interdependent due to specific implementations. Another example about communicated messages shows that the classification criteria we used do not depend on each other. During implementation, a proxy (in
the communicatee type dimension) or corrupted message constraints (in the communication constraints dimension)
may change the value of message content. However, we categorize communicated messages as Existing Knowledge
or Imagined Future Knowledge, based on whether future knowledge is simulated and utilized. This classification criterion is not inherently linked to a specific type of communicatee or communication constraint. Thus, the dimensions
of communicatee type (Section 3.3) and communication constraints (Section 3.2) are independent from the viewpoint
of classification criteria. Consequently, the proposed categories and dimensions effectively encapsulate the literature
from their unique perspectives.
20

4

Dimensions

Table 12: The notations of all categories.
Notations

Controlled Goals (CG)

Coo : Cooperative; Com : Competitive; M: Mixed

Communication Constraints (CC)

U: Unconstrained Communication;Lb : Limited Bandwidth; Cm : Corrupted Messages;

Communicatee Type (CT)

Na : Nearby Agents; A: Other (Learning) Agents; P: Proxy

Communication Policy (CP)

Fc : Full Communication; Ps : Predefined (Partial) Structure; Ic : Individual Control; Gc : Global Control

Communicated Messages (CM)

E: Existing Knowledge; I: Imagined Future Knowledge

Message Combination (MC)

Ve : Equally Valued; Vu : Unequally Valued

Inner Integration (II)

Pl : Policy-level; Vl : Value-level; PV: Policy-level & Value-level

Learning Methods (LM)

D: Differentiable; Sp : Supervised; Re : Reinforced; Rg : Regularized

Training Schemes (TS)

CL: Centralized Learning; DL: Decentralized Learning; CT DE ip :
CTDE with Individual (Policy) Parameters; CT DE ps : CTDE with Parameter Sharing; CT DE c : Concurrent CTDE

Findings, Discussions, and Research Directions

In this section, we discuss the trend of the current literature and provide our observations and findings based on the
proposed dimensions and categorizations. We also dive into the dimensions and suggest possible future research
directions.
4.1 Findings and Discussions
To provide a more comprehensive overview of the literature, we have utilized the proposed 9 dimensions to categorize
existing works, thereby creating an extensive table. For ease of reference, we introduce notations for these dimensions
and their associated categories in Table 12. These notations are subsequently employed to categorize research works
in Table 13. In Table 13, research works are sorted based on their publication or archival dates (e.g., on arXiv). Our
proposed 9 dimensions offer different perspectives for analyzing and comparing recent works in the field of CommMADRL. Through these dimensions and categories, we have observed several intriguing findings.
• In the dimension of Controlled Goals, recent research has focused on various cooperative settings, together with
a few mixed scenarios. Communication in non-cooperative multi-agent tasks, however, has not been extensively
explored. In such (non-cooperative) environments, the goals of different agents may conflict. In the emergent language literature, Noukhovitch et al. [47] have investigated how communication emerges between sender and receiver
agents when they exhibit different levels of competitiveness, ranging from full cooperation to full competition. The
results reveal that both sender and receiver agents can obtain higher rewards through communication when the level
of competition is not high. However, their research primarily focuses on a simplified game without considering
state transitions. The effectiveness of communication in MARL tasks with large state spaces, particularly in partial
competitive settings where agents can still gain mutual benefits through low-level cooperation, remains an area for
further exploration. Moreover, in non-cooperative settings, agents may be motivated to deceive or manipulate the
communication channel to mislead others. The notion of trust in multi-agent systems introduces the possibility of
establishing a truthful communication protocol [106, 107]. Agents could assess the reliability of opponents and
defend against malicious messages. Additionally, agents might evaluate interactions of opponents with other agents
to determine their reputations, which could influence the priorities of communicating with other agents.
• In the dimension of Communication Constraints, many existing works do not account for communication constraints, which may limit their applicability in realistic scenarios that have such limitations. For instance, transmitting messages in a large multi-agent system across long distances can result in delays, losses, or even be infeasible.
Communication might be asynchronous, requiring several time steps for information exchange. These factors introduce new challenges to Comm-MADRL systems, such as validating previously sent messages and integrating
messages from different time steps. Moreover, if communication resources are limited due to budget or capacity
constraints, agents must decide how to allocate these resources effectively, especially when their goals vary. Conveying too much information might benefit others while sacrificing the agent’s own learning opportunities. The
concept of fairness, which has been extensively studied in multi-agent systems, focuses on developing fair solutions
for resource allocation. The ideas of maximizing the utility of worse-off agents and decreasing the difference in
21

utilities between agents in the fairness study can be utilized to distribute communication resources equally. For
instance, agents with lower utilities could be allotted more resources to facilitate their communication with others.
• In the dimension of Communicatee Type, the concept of a proxy is utilized to facilitate message coordination. When
global observability is available, a proxy often considers all agents within the environment. This proxy can be
particularly effective and targeted by utilizing the independence among agents, coordinating messages among only
a subset of agents as necessary.
• In the dimension of Communication Policy, current works often assume a binary communication action regarding
whether or not to communicate with other agents (or a specific agent). However, communication actions can be
more fine-grained and descriptive. For instance, agents might opt to send only a portion of their messages due
to uncertainty or lack of confidence. Additionally, a communication action could be defined more specifically,
such as communicate with others if the budget exceeds a predetermined threshold. Thus, a communication policy
can encompass a variety of communication actions, tailored to align with human heuristics and specific system
requirements.
• In the dimension of Communicated Messages, various methods have been proposed to utilize the existing knowledge of agents for message generation. Some existing works consider incorporating agents’ intentions or future
plans. However, intentions or future plans may lead to catastrophic errors due to insufficient understanding of the
underlying (transition) dynamics. Model-based Reinforcement Learning (RL) could assist agents in making more
accurate predictions about future situations, thereby enabling the agents to communicate information with more
certainty regarding upcoming changes. Additionally, current literature often assumes that messages are conveyed as
single values or vectors. In contrast, modern devices allow for more complex formats, such as graphs and logical
expressions. These formats can convey a substantial amount of knowledge or facts concisely, facilitating fast coordination. However, the challenge lies in effectively encoding and decoding complex information structures, which
requires more sophisticated learning signals.
• In the dimension of Message Combination, as messages often contain information related to each agent’s individual
experiences, goals, etc., many recent works consider the varying importance of these messages. These research
works mostly rely on attention mechanisms to impose weights on received messages. Furthermore, agents can
incorporate their prior knowledge or preferences about other agents’ capabilities into these weights, enhancing the
relevance and effectiveness of message combination.
• In the dimension of Inner Integration, many recent works have focused on integrating messages into the policy
model. This trend is likely due to the growing interest in policy-based methods, particularly actor-critic algorithms,
within the field of MADRL, where significant advancements have been achieved. Given that neural networks typically feature a hierarchical structure, there is potential for agents to effectively integrate messages into different
layers. This approach would allow for considering varying levels of abstraction, potentially enhancing the decisionmaking process.
• In the dimension of Learning Methods, the learning process for communication typically requires instantaneous
feedback from agents who receive and act upon messages. This feedback could be in the form of gradient information or changes in the policies or rewards of the receiving agents. However, obtaining instantaneous feedback from
other agents might not always be feasible in real-time decision-making systems. Despite this challenge, agents can
still observe changes in the environment and their rewards to self-evaluate the effectiveness of their communication.
This self-evaluation process enables agents to update and learn their communication protocols over time.
• In the dimension of Training Schemes, parameter sharing combined with centralized training and decentralized
execution is widely adopted in Comm-MADRL to reduce the number of learning parameters. However, accessing
other agents’ memories and parameters might raise privacy concerns. On the other hand, fully decentralized learning
presents significant challenges and remains a key research area in MARL. In fully decentralized learning, agents
have limited knowledge about the environment and must deal with non-stationarity, a problem that intensifies with
an increasing number of agents. Nonetheless, Comm-MADRL can benefit from advancements in MARL, potentially
leading to the development of novel training paradigms that better balance knowledge sharing, privacy, and learning
efficiency.
Based on the proposed dimensions, we have identified a range of findings and potential issues in the field of CommMADRL. Among these issues, achieving fully decentralized learning and self-evaluated communication protocols
remains a significant challenge. This difficulty arises because each agent has access only to their own data collected
from the environment, adding complexity to message evaluation without the help of other agents. Decentralized action policies and self-evaluated communication protocols, however, could be advantageous in areas like Electronic
Commerce [108], Networks [109], and Blockchain [110], where synchronizing knowledge and information among
users or agents can be computationally demanding. Another open question involves how to effectively communicate
using more complex message formats and implement efficient training methods, potentially leading to more sophisti22

cated communication architectures. Importantly, advancements in multi-agent systems and multi-agent reinforcement
learning can significantly contribute to the progress of Comm-MADRL.
Table 13: An overview of recent works in Comm-MADRL. a + b denotes that the research work considers categories a
and b simultaneously in the environment. a/b denotes that the research work has been examined in multiple categories
but in separate environments or settings.
Methods

CG

CC

CT

CP

CM

MC

II

LM

TS

DIAL [73]

Coo

Lb +Cm

A

Fc

E

Ve

Vl

D

CT DE ps

RIAL [73]

Coo

Lb

A

Fc

E

Ve

Vl

Re

CT DE ps

CommNet [48]

Coo

U

A

Fc

E

Ve

Pl

D

CT DE ps

GCL [61]

Coo

Lb

A

Fc

E

Ve

Pl

D+Re

CT DE ps

BiCNet [50]

Coo

U

A

Fc

E

Vu

PV

D

CT DE ps

MS-MARLGCM [88]

Coo

U

P

Fc

E

Vu

Pl

D

CT DE ip

ATOC [39]

Coo

U

P

Ic

I

Vu

Pl

Sp

CT DE ps

IC [62]

Coo

Lb

A

Fc

E

Ve

PV

Re

DL

DGN [81]

Coo /M

U

Na

Ps

E

Vu

Vl

D

CT DE ps

TarMAC [89]

Coo /M

U

A

Fc

E

Vu

PV

D

CT DE ps

MAGNet-SAGS-MG [40]

Coo

U

Na

Ps

E

Vu

Pl

D

DL

MADDPG-M
[41]

Coo

U

A

Ic

E

Ve

PV

Re

DL

IC3Net [49]

Coo /Com /M
U

A

Ic

E

Ve

Pl

Re

CT DE ps

MD-MADDPG
[42]

Coo

U

P

Fc

E

Vu

Pl

D

CT DE c

SchedNet [43]

Coo

Lb

A

Gc

E

Ve

Pl

Re

CT DE ip

DCC-MD [82]

Coo

U

A

Fc

E

Vu

Vl

D

DL

Agent-Entity
Graph [74]

Coo

U

Na

Ps

E

Vu

PV

D

DL

VBC [31]

Coo

Lb

A

Ps

E

Ve

Vl

D

CT DE ps

NDQ [75]

Coo /M

Lb

A

Ps

E

Ve

Vl

Rg

CT DE ps

IMAC [66]

Coo

Lb

P

Fc

E

Vu

Pl

Rg

CT DE ip

GA-Comm [83]

Coo

U

P

Gc

E

Vu

Pl

D

CT DE ps

Gated-ACML
[76]

Coo

Lb

P

Ic

E

Vu

Pl

Sp

CT DE ip

Bias [63]

Coo

Lb

A

Fc

E

Ve

PV

Rg

DL

LSC [77]

Coo /M

U

Na

Ic

E

Vu

Vl

Re

CT DE ps

Diff Discrete[78]

Coo

Cm

A

Fc

E

Ve

Pl

D

CT DE ps

NeurComm [84]

Coo

U

Na

Ps

I

Vu

PV

D

DL

IP [85]

Coo

U

Na

Ps

I

Vu

Pl

D

DL

I2C [79]

Coo

U

A

Ic

E

Vu

Pl

Sp

CT DE ps

IS [90]

Coo

U

A

Fc

I

Ve

Pl

D

CT DE c

ETCNet [86]

Coo

Lb

A

Ic

E

Ve

Pl

Re

CT DE ps

Continued on next page
23

Variable-length
Coding [87]

Coo

Lb

A

Fc

E

Ve

Pl

D

CT DE ps

TMC [32]

Coo

Lb

A

Ps

E

Vu

Vl

D

CT DE ps

HAMMER [91]

Coo

U

P

Fc

E

Vu

Pl

Re

CT DE ps

MAGIC [92]

Coo /M

U

P

Gc

E

Vu

PV

D

CT DE ps

FlowComm [93]

Coo

U

Na

Gc

E

Ve

Pl

D

CT DE ps

AE-Comm [65]

Coo

Lb

A

Fc

E

Ve

PV

Rg

DL

GAXNet [80]

Coo

U

Na

Ps

E

Vu

Pl

D

CT DE ip

DCSS [64]

Coo

Cm

P

Fc

E

Ve

PV

D+Sp

CT DE ip

R-MACRL [95]

Com

Cm

A

Fc

E

Vu

Pl

Sp

DL

MAIC [33]

Coo

Lb

A

Ps

E

Vu

Vl

Rg

CT DE ps

FCMNet [94]

Coo

U

A

Fc

E

Vu

PV

D

CT DE ps

In addition to these findings, the evaluation metrics used in Comm-MADRL research are of significant interest. It is
noteworthy that existing works have been evaluated across various platforms and games, employing different metrics
to assess performance. Crucially, Comm-MADRL studies often use varying settings of experiments, such as the
number of agents or the use of parameter sharing. These settings can make it challenging to fairly compare the relative
strengths and limitations of algorithms based on their performance outcomes [111]. We have identified four evaluation
metrics commonly used in Comm-MADRL studies as follows:
• Reward-based: This metric employs the converged return or average rewards per episode or time step to demonstrate the profit gained by agents.
• Win or Fail Rate: This metric calculates the percentage that agents achieve their goal or fail the game during
learning. It is often used in episodic tasks.
• Steps Taken: This metric evaluates the number of time steps learned to reach the goal. It is often used in episodic
tasks and essential in scenarios where time efficiency is key.
• Communication Efficiency: This metric evaluates how much communication resource has been used, such as the
frequency of communication between agents.
• Emergence Degree: Originating from the field of emergent language, this metric evaluates and detects the emergence of language [112, 45]. It is often used in learning tasks with emergent language. Positive signaling and
positive listening are two common approaches. Positive signaling measures the correlation between a message and
the sender’s observation or intended action. Positive listening assesses the impact of an observed message on the
receiver’s beliefs or behavior.
We have analyzed the number of times that the above performance metrics are used in existing Comm-MADRL
studies, as illustrated in Figure 5. It is shown that the metric of communication efficiency has not been extensively
used in the literature, requiring further investigation into the use of communication resources in Comm-MADRL
approaches. The Emergence Degree metric, intended to measure whether a language is emergent, is primarily utilized
in emergent language studies. Nonetheless, this metric can also yield significant insights for other Comm-MADRL
systems. By analyzing the correlation between communication and the observations and behaviors of both senders
and receivers, we could obtain a deeper understanding and explanation of communication for Comm-MADRL.
In the next section, inspired by the proposed dimensions, we demonstrate the potential for discovering new ideas
through our survey. We identify several possible research directions that jointly explore multiple dimensions, aiming
to bridge the gaps in current works.
4.2

Research Directions

Comm-MADRL is a young but rapidly enlarging field. There are still lots of possibilities to develop new CommMADRL systems. Our proposed dimensions encapsulate several aspects of Comm-MADRL, from which we can
identify new research directions. Therefore, we showcase four research directions motivated by leveraging the possible
combinations of dimensions and the extensions of corresponding categories. We also point out further challenges for
Comm-MADRL.
24

  
  

 & R X Q W

  
  
  
 
 

 5 H Z D U G  E D V H G

 : L Q  R U  ) D L O  5 D W H

 6 W H S V  7 D N H Q

 & R P P X Q L F D W L R Q  ( I I L F L H Q F \

 ( P H U J H Q F H  ' H J U H H

Figure 5: The Statistics of Evaluation Metrics in existing Comm-MADRL systems.
4.2.1 Multimodal Communication
A versatile robot can hear by sound sensors, read text or talk with human partners. Intelligent agents may be surrounded
by different data sources and act based on multimodal input. By jointly considering the dimensions of communicatee
type and communicated messages, we can imagine a fertile scenario where communication is not limited to images
or handcrafted features but encompasses multimodal data, such as speech, videos, and text from humans or domestic
robots, to prosper applications like smart home. To the best of our knowledge, existing works in Comm-MADRL do
not consider communicating multimodal data or encoding them. Recent works often use encoded images as messages,
which only cover visually-based applications. Therefore, we believe exploring multimodal communication represents
a promising research direction and introduces several challenges that need to be addressed. In multimodal communication, agents have to coordinate heterogeneous modalities and encode various types of information into messages. A
possible solution is to use separate channels to communicate specific modalities, while agents must decide on the right
channel to communicate and merge data from different channels. A more efficient way is to learn a joint representation
of multimodal observations and communicate on one channel. Due to the progress of Multimodal machine learning
[113], we can bring ideas from this area to equip agents with the ability to create a single representation of multimodal
data. Nevertheless, it is unclear how the solutions from Multimodal machine learning can be extended to multi-agent
reinforcement learning. Poklukar et al. [114] propose learning an aligned representation from multiple modalities,
although their tests are conducted in a single-agent reinforcement learning task. The multi-agent scenarios, however,
may need to consider the individual abilities and preferences of different agents. For example, a voice-activated agent
may favor voice data for interaction, while a monitoring agent may only access video data. Therefore, in multi-agent
settings, agents need to align their individual preferences regarding multimodality when learning a joint representation
of the multimodal data. Another crucial technical issue is how to represent multimodal messages in low-dimensional
vectors without losing essential information from each modality, as Comm-MADRL systems often consider reducing
communication costs. Eventually, we expect the progress of multimodal communication will benefit human-agent
interaction and diverse communicating agents.
The emergence of new research works would introduce new categories under each dimension. For example, with
developments in multimodal communication, we can extend the categories of communicated messages with speech,
image, text, and video data. Nevertheless, our proposed dimensions can be adaptive and robust to cover new CommMADRL research in this direction.
4.2.2 Structural Communication
Through the Internet, electronic devices like routers can process and transmit information. On social media, chatbots
can be community members [115, 116], to engage in conversations with users and share information/opinions. In those
large-scale multi-agent systems [117, 118], agents may belong to different groups, where their relationships can be
complicated. For example, local area networks create boundaries of communication and interaction between devices.
Chatbots may not be able to reach some users because of limited permission or the lack of friendship relations. These
restricted connectivities among agents require more efficient usage of communication structure. Therefore, we think
that the research direction focusing on structural communication opens up possibilities for enabling communication
among a larger number of agents. In the current literature in Comm-MADRL, ATOC [39] and LSC [77] have investigated communication with multiple groups, where agents can only communicate with other agents who belong to
the same group. In both approaches, different groups may share common member agents, i.e., bridge agents, which
are used to enable information to flow from group to group. However, communication through bridge agents is not
25

targeted and each agent unconsciously shares their information with other groups. In terms of the dimension of controlled goals, agents may have individualized goals and require collaboration with a specific set of agents. Therefore,
an important future direction of structural communication is to send critical information and opinions to target agents.
For example, agent 1 may observe the goal location of agent 2 while they belong to different groups. If agent 3 happens to be a common friend of agent 1 and 2, agent 1 can actively send the goal information to agent 2 with the help of
agent 3. If communication is costly and information is private, agents need to make thoughtful decisions about which
bridge agents to be used to find the shortest and safe path to reach targeted agents. At the same time, bridge agents
need to agree on the communication path to transmit information successfully. If a complex and hierarchical friendship network is identified, another important question is how to prioritize and schedule different communication paths
to make communication fluent. Regarding communicated messages, agents need to build a common protocol with
targeted agents so that information can be encoded and decoded successfully. As a result, agents can more actively
utilize the communication structure among agents to achieve better collaboration and agreements.
4.2.3 Robust Centralized Unit
Robustness has been widely considered in the field of reinforcement learning [119, 120], where an agent needs to cope
with disturbances in learning in order to achieve a robust policy that can generalize under changes in training/test data.
In MARL, agents’ policies can be sensitive to environmental noise or malicious intentions of opponents, and thus
robust policies are required [121, 122]. With communication, opponents may produce malicious messages, implying adversary intentions. Preventing malicious messages is important in non-cooperative settings as adversary agents
may manipulate communication to achieve their own goals at the expense of other agents’ benefits. Existing works on
Comm-MADRL, such as R-MACRL [95], have investigated how to detect adversary information and reconstruct original messages. However, as we discussed in the dimensions of communicatee type and training paradigm, proxy and
critics are often centralized and gather information from all agents. Robustness becomes essential for these centralized units as all agents involved in communication can be misled by polluted feedback, for example, incorrect gradient
signals from critics or malicious messages from a proxy. Moreover, malicious messages can easily spread through the
(centralized) proxy. Therefore, we think building a robust centralized unit is a promising and underdeveloped direction for safe communication in MADRL, where proxies and critics need to avoid communication being exploited by
adversaries or affected by harmful environmental changes. By considering the dimension of communication policy,
sender agents can learn a versatile communication policy. For example, the communication policy can be defined to
select different encoding protocols for different groups of agents, in case malicious agents may easily find a solution to
cheat on a specific encoding protocol. Besides, as malicious or noisy messages can be hidden in the centralized proxy,
it is important to figure out which messages are malicious and how to reconstruct the original messages. Nonetheless,
developing robust centralized units is vital for reliable and protected Comm-MADRL systems.
4.2.4 Learning Tasks with Emergent Language
In this survey, we have identified the intersection between learning tasks with communication and emergent language
in the field of MADRL, which we have called learning tasks with emergent language. We also observed that there
is only a limited number of research works concerning this sub-area learning tasks with emergent language, which
learns a language while achieving a MADRL task. We believe this area can be further expanded and investigated, by
considering several dimensions proposed by our survey. First, the communicated messages, as we discussed earlier,
can be encoded into more complex symbolic formats, such as graphs or logical expressions. Existing works in the
field only learn how to communicate through atomic symbols or a combination [61, 63, 64]. However, it is important
to learn the relation between symbols. For example, symbol A is on the left of symbol B. Those messages can express
facts about what agents know or conjecture. Therefore, receivers can quickly adapt their behaviors by successfully
decoding the messages. The important question is how to learn both encoding and decoding with complex expressions
of messages, which can have a significant number of possibilities. The senders should also properly encapsulate their
knowledge and the receivers should reason on the messages correctly. In addition, how complex symbolic formats
can emerge in non-cooperative settings is an interesting but unexplored research area. What’s more, the combination
of complex messages will not be as easy as handling single values or vectors. Therefore, learning together with
complicated communication is still challenging.
4.2.5 Further Challenges
In the field of Comm-MADRL, there are further challenges. For instance, the design of neural network architectures
plays a critical role in performance and communication. A deeper neural network may be effective in some domains
while failing in other domains. For example, LSTM is effective in capturing history information while may require
much time to train the parameters [123, 124], which could greatly slow down the learning in tasks with high complexity. The choice of architectures and fine-tuning hyperparameters are significant problems of Comm-MADRL. With
communication, another crucial issue is the explainability of communicated messages. Emergent language has made
a step towards human-like language. However, whether machines communicate in a human-like way and can learn a
human-interpretable language is still unclear. A great number of existing works regarding learning tasks with commu26

nication seek hidden, deep, and obscure codes for messages [48, 49, 79, 92], which still need to be further interpreted
and understood.

5

Conclusions

Our survey proposes to classify the literature based on 9 dimensions. These dimensions constitute the basis of designing Comm-MADRL systems. We further categorize existing works under each dimension, where readers can easily
compare research works from a unique perspective. Based on those dimensions, we also observe findings through
the trend of the literature and identify new research directions by filling the gap among recent works. Our survey
concludes that while the number of works in Comm-MADRL is notable and represents significant achievements,
communication can be more fruitful and versatile to incorporate non-cooperative settings, heterogeneous players, and
many more agents. Agents can communicate information not only from raw image inputs or handcrafted features but
also from diverse data sources such as voice and text. Furthermore, we can explore novel metrics to better understand
the contribution of communication to the overall learning process. Ultimately, Comm-MADRL can benefit from the
MARL community and take advantage of good solutions from MARL.

References
[1] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. CoRR, abs/1610.03295, 2016.
[2] Meritxell Vinyals, Juan A. Rodrı́guez-Aguilar, and Jesús Cerquides. A survey on sensor networks from a
multiagent perspective. Comput. J., 54(3):455–470, 2011.
[3] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. Int. J. Robotics
Res., 32(11):1238–1274, 2013.
[4] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre,
George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human
knowledge. Nature, 550(7676):354–359, 2017.
[5] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885–890,
2019.
[6] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer Briefs
in Intelligent Systems. 2016.
[7] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pages 6379–6390, 2017.
[8] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications
of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 2974–2982, 2018.
[9] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V. Albrecht. Dealing with nonstationarity in multi-agent deep reinforcement learning. CoRR, abs/1906.04737, 2019.
[10] Mohamed Salah Zaı̈em and Etienne Bennequin. Learning to communicate in multi-agent reinforcement learning
: A review. CoRR, abs/1911.05438, 2019.
[11] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
[13] Peter Stone and Manuela M. Veloso. Multiagent systems: A survey from a machine learning perspective. Auton.
Robots, 8(3):345–383, 2000.
[14] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Auton. Agents Multi Agent
Syst., 11(3):387–434, 2005.
27

[15] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156–
172, 2008.
[16] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A survey and critique of multiagent deep reinforcement learning. Auton. Agents Multi Agent Syst., 33(6):750–797, 2019.
[17] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence
Review, pages 1–49, 2021.
[18] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning era. CoRR,
abs/2006.02419, 2020.
[19] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs. Springer Briefs
in Intelligent Systems. 2016.
[20] Eric A. Hansen, Daniel S. Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable
stochastic games. In Deborah L. McGuinness and George Ferguson, editors, Proceedings of the Nineteenth
National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial
Intelligence, July 25-29, 2004, San Jose, California, USA, pages 709–715, 2004.
[21] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. CoRR, abs/2011.00583, 2020.
[22] Ming Tan. Multi-agent reinforcement learning: Independent versus cooperative agents. In Paul E. Utgoff,
editor, Machine Learning, Proceedings of the Tenth International Conference, University of Massachusetts,
Amherst, MA, USA, June 27-29, 1993, pages 330–337, 1993.
[23] Laëtitia Matignon, Guillaume J. Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners in
cooperative markov games: a survey regarding coordination problems. Knowledge Engineering Revie, 27(1):1–
31, 2012.
[24] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems.
In Jack Mostow and Chuck Rich, editors, Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98, IAAI 98, July 26-30,
1998, Madison, Wisconsin, USA, pages 746–752, 1998.
[25] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul
Vicente. Multiagent cooperation and competition with deep reinforcement learning. CoRR, abs/1511.08779,
2015.
[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, 2nd Edition. The MIT Press.
2018.
[27] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinı́cius Flores Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Elisabeth André, Sven Koenig, Mehdi
Dastani, and Gita Sukthankar, editors, Proceedings of the 17th International Conference on Autonomous Agents
and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pages 2085–2087, 2018.
[28] Tabish Rashid, Mikayel Samvelyan, Christian Schröder de Witt, Gregory Farquhar, Jakob N. Foerster, and
Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning.
In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of
Machine Learning Research, pages 4292–4301, 2018.
[29] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize
with transformation for cooperative multi-agent reinforcement learning. In Kamalika Chaudhuri and Ruslan
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages
5887–5896, 2019.
[30] Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. DOP: off-policy multi-agent
decomposed policy gradients. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021, 2021.
[31] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement learning via
variance based control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32 (NeurIPS),
pages 3230–3239, 2019.
28

[32] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Succinct and robust multi-agent communication with temporal
message control. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin, editors, Advances in Neural Information Processing Systems 33 (NIPS), 2020.
[33] Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang Zhang, Yang Yu, and Chongjie Zhang.
Multi-agent incentive communication via decentralized teammate modeling. In Thirty-Sixth AAAI Conference
on Artificial Intelligence (AAAI-22), 2022.
[34] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: duplex dueling multi-agent
q-learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021, 2021.
[35] Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Sara A. Solla, Todd K. Leen, and KlausRobert Müller, editors, Advances in Neural Information Processing Systems 12, NIPS Conference, pages 1008–
1014, 1999.
[36] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
Track Proceedings, 2016.
[37] Afshin Oroojlooyjadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning. CoRR, abs/1908.03963, 2019.
[38] Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Comparative evaluation of
cooperative multi-agent deep reinforcement learning algorithms. arXiv: 2006.07869, 2020.
[39] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. In Advances
in Neural Information Processing Systems 31 (NIPS), pages 7265–7275, 2018.
[40] Aleksandra Malysheva, Tegg Tae Kyong Sung, Chae-Bong Sohn, Daniel Kudenko, and Aleksei Shpilman. Deep
multi-agent reinforcement learning with relevance graphs. CoRR, abs/1811.12557, 2018.
[41] Ozsel Kilinc and Giovanni Montana. Multi-agent deep reinforcement learning with extremely noisy observations. CoRR, abs/1812.00922, 2018.
[42] Emanuele Pesce and Giovanni Montana. Improving coordination in small-scale multi-agent deep reinforcement
learning through memory-driven communication. Machine Learning, 109(9-10):1727–1747, 2020.
[43] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi.
Learning to schedule communication in multi-agent reinforcement learning. In 7th International Conference
on Learning Representations (ICLR), 2019.
[44] Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z. Leibo, Karl Tuyls, and Stephen Clark. Emergent communication through negotiation. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.
[45] Ryan Lowe, Jakob N. Foerster, Y-Lan Boureau, Joelle Pineau, and Yann N. Dauphin. On the pitfalls of measuring emergent communication. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors,
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19,
Montreal, QC, Canada, May 13-17, 2019, pages 693–701, 2019.
[46] Kalesha Bullard, Douwe Kiela, Joelle Pineau, and Jakob N. Foerster. Quasi-equivalence discovery for zero-shot
emergent communication. CoRR, abs/2103.08067, 2021.
[47] Michael Noukhovitch, Travis LaCroix, Angeliki Lazaridou, and Aaron C. Courville. Emergent communication
under competition. In Frank Dignum, Alessio Lomuscio, Ulle Endriss, and Ann Nowé, editors, AAMAS ’21:
20th International Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom,
May 3-7, 2021, pages 974–982, 2021.
[48] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems 29 (NIPS), pages 2244–2252, 2016.
[49] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale in multiagent
cooperative and competitive tasks. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.
[50] Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent
bidirectionally-coordinated nets for learning to play starcraft combat games. CoRR, abs/1703.10069, 2017.
[51] Joseph Farrell and Matthew Rabin. Cheap talk. Journal of Economic perspectives, 10(3):103–118, 1996.
29

[52] Hyowoon Seo, Jihong Park, Mehdi Bennis, and Mérouane Debbah. Semantics-native communication with
contextual reasoning. CoRR, abs/2108.05681, 2021.
[53] Tadahiro Taniguchi, Yuto Yoshida, Akira Taniguchi, and Yoshinobu Hagiwara. Emergent communication
through metropolis-hastings naming game with deep generative models. CoRR, abs/2205.12392, 2022.
[54] Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. In
The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022,
2022.
[55] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compositionality and generalization in emergent languages. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020, pages 4427–4442, 2020.
[56] Cinjon Resnick, Abhinav Gupta, Jakob N. Foerster, Andrew M. Dai, and Kyunghyun Cho. Capacity, bandwidth,
and compositionality in emergent language learning. In Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An,
and Neil Yorke-Smith, editors, Proceedings of the 19th International Conference on Autonomous Agents and
Multiagent Systems, AAMAS ’20, Auckland, New Zealand, May 9-13, 2020, pages 1125–1133, 2020.
[57] Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, and Marco Baroni. Anti-efficient encoding in
emergent communication. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pages 6290–6300, 2019.
[58] Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to communicate with
sequences of symbols. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,
S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, pages 2149–2159, 2017.
[59] Alexander Imani Cowen-Rivers and Jason Naradowsky. Emergent communication with world models. CoRR,
abs/2002.09604, 2020.
[60] Ivana Kajic, Eser Aygün, and Doina Precup. Learning to cooperate: Emergent communication in multi-agent
navigation. In Stephanie Denison, Michael Mack, Yang Xu, and Blair C. Armstrong, editors, Proceedings of
the 42th Annual Meeting of the Cognitive Science Society - Developing a Mind: Learning in Humans, Animals,
and Machines, CogSci 2020, virtual, July 29 - August 1, 2020, 2020.
[61] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations.
In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on
Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the
8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018, pages 1495–1502, 2018.
[62] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Çaglar Gülçehre, Pedro A. Ortega, DJ Strouse, Joel Z.
Leibo, and Nando de Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement
learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pages 3040–3049, 2019.
[63] Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, and Thore Graepel. Biases for emergent communication in multi-agent reinforcement learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, pages 13111–13121, 2019.
[64] Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia P. Sycara, Michael Lewis, and Julie A. Shah.
Emergent discrete communication in semantic spaces. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual, pages 10574–10586, 2021.
[65] Toru Lin, Jacob Huh, Christopher Stauffer, Ser-Nam Lim, and Phillip Isola. Learning to ground multi-agent
communication with autoencoders. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy
Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual
30

Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,
pages 15230–15242, 2021.
[66] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning efficient multi-agent
communication: An information bottleneck approach. In Proceedings of the 37th International Conference
on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 9908–9918,
2020.
[67] Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, and Chai Kiat Yeo. Mis-spoke or
mis-lead: Achieving robustness in multi-agent communicative reinforcement learning. CoRR, abs/2108.03803,
2021.
[68] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multi-agent
systems: A review of challenges, solutions and applications. CoRR, abs/1812.11794, 2018.
[69] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of
theories and algorithms. CoRR, abs/1911.10635, 2019.
[70] Annie Wong, Thomas Bäck, Anna V. Kononova, and Aske Plaat. Multiagent deep reinforcement learning:
Challenges and directions towards human-like approaches. CoRR, abs/2106.15691, 2021.
[71] Mohamed Salah Zaı̈em and Etienne Bennequin. Learning to communicate in multi-agent reinforcement learning
: A review. CoRR, abs/1911.05438, 2019.
[72] Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems - Algorithmic, Game-Theoretic, and Logical
Foundations. Cambridge University Press. 2009.
[73] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with
deep multi-agent reinforcement learning. In Advances in Neural Information Processing Systems 29 (NIPS),
pages 2137–2145, 2016.
[74] Akshat Agarwal, Sumit Kumar, Katia P. Sycara, and Michael Lewis. Learning transferable cooperative behavior in multi-agent teams. In Proceedings of the 19th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS), pages 1741–1743, 2020.
[75] Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value
functions via communication minimization. In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
[76] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under
limited bandwidth by message pruning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages
5142–5149, 2020.
[77] Junjie Sheng, Xiangfeng Wang, Bo Jin, Junchi Yan, Wenhao Li, Tsung-Hui Chang, Jun Wang, and Hongyuan
Zha. Learning structured communication for multi-agent reinforcement learning. CoRR, abs/2002.04235, 2020.
[78] Benjamin Freed, Guillaume Sartoretti, Jiaheng Hu, and Howie Choset. Communication learning via backpropagation in discrete channels with unknown noise. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, pages 7160–7168, 2020.
[79] Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent
cooperation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin, editors, Advances in Neural Information Processing Systems 33 (NeurIPS), 2020.
[80] Won Joon Yun, Byungju Lim, Soyi Jung, Young-Chai Ko, Jihong Park, Joongheon Kim, and Mehdi Bennis.
Attention-based reinforcement learning for real-time UAV semantic communication. CoRR, abs/2105.10716,
2021.
[81] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In
8th International Conference on Learning Representations (ICLR), 2020.
[82] Woojun Kim, Myungsik Cho, and Youngchul Sung. Message-dropout: An efficient training method for multiagent deep reinforcement learning. In The Thirty-Third AAAI Conference on Artificial Intelligence, pages 6079–
6086, 2019.
[83] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent game abstraction
via graph attention neural network. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI),
pages 7211–7218, 2020.
[84] Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for networked system
control. In 8th International Conference on Learning Representations (ICLR), 2020.
31

[85] Chao Qu, Hui Li, Chang Liu, Junwu Xiong, James Zhang, Wei Chu, Yuan Qi, and Le Song. Intention propagation for multi-agent reinforcement learning. CoRR, abs/2004.08883, 2020.
[86] Guangzheng Hu, Yuanheng Zhu, Dongbin Zhao, Mengchen Zhao, and Jianye Hao. Event-triggered multi-agent
reinforcement learning with communication under limited-bandwidth constraint. CoRR, abs/2010.04978, 2020.
[87] Benjamin Freed, Rohan James, Guillaume Sartoretti, and Howie Choset. Sparse discrete communication learning for multi-agent cooperation through backpropagation. In IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 7993–7998, 2020.
[88] Xiangyu Kong, Bo Xin, Fangchen Liu, and Yizhou Wang. Revisiting the master-slave architecture in multiagent deep reinforcement learning. CoRR, abs/1712.07305, 2017.
[89] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau.
Tarmac: Targeted multi-agent communication. In Proceedings of the 36th International Conference on Machine
Learning (ICML), pages 1538–1546, 2019.
[90] Woojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning:
Intention sharing. In 9th International Conference on Learning Representations (ICLR), 2021.
[91] Nikunj Gupta, G. Srinivasaraghavan, Swarup Kumar Mohalik, and Matthew E. Taylor. HAMMER: multi-level
coordination of reinforcement learning agents via learned messaging. CoRR, abs/2102.00824, 2021.
[92] Yaru Niu, Rohan R. Paleja, and Matthew C. Gombolay. Multi-agent graph-attention communication and teaming. In 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 964–
973, 2021.
[93] Yali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and Haifeng Zhang. Learning
correlated communication topology in multi-agent reinforcement learning. In 20th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS), pages 456–464, 2021.
[94] Yutong Wang and Guillaume Sartoretti. Fcmnet: Full communication memory net for team-level cooperation
in multi-agent systems. CoRR, abs/2201.11994, 2022.
[95] Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, and Chai Kiat Yeo. Mis-spoke or
mis-lead: Achieving robustness in multi-agent communicative reinforcement learning. CoRR, abs/2108.03803,
2021.
[96] Lucian Busoniu, Robert Babuska, and Bart De Schutter. Multi-agent reinforcement learning: A survey. In Ninth
International Conference on Control, Automation, Robotics and Vision (ICARCV), pages 1–6, 2006.
[97] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian
Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on real-time strategy games.
CoRR, abs/1611.00625, 2016.
[98] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo,
Alireza Makhzani, Heinrich Küttler, John P. Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig
Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone,
Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II:
A new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017.
[99] Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J.
Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors, Proceedings of the
18th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC,
Canada, May 13-17, 2019, pages 2186–2188, 2019.
[100] Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos
Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football:
A novel reinforcement learning environment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,
pages 4501–4510, 2020.
[101] Laëtitia Matignon, Guillaume J. Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners in
cooperative markov games: a survey regarding coordination problems. Knowl. Eng. Rev., 27(1):1–31, 2012.
[102] Tim Brys, Ann Nowé, Daniel Kudenko, and Matthew E. Taylor. Combining multiple correlated reward and
shaping signals by measuring confidence. In Carla E. Brodley and Peter Stone, editors, Proceedings of the
Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada,
pages 1687–1693, 2014.
32

[103] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under
limited bandwidth by message pruning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth
AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February
7-12, 2020, pages 5142–5149, 2020.
[104] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations (ICLR), 2017.
[105] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing, 190:82–94, 2016.
[106] Jones Granatyr, Vanderson Botelho, Otto Robert Lessing, Edson Emı́lio Scalabrin, Jean-Paul A. Barthès, and
Fabrı́cio Enembreck. Trust and reputation models for multiagent systems. ACM Comput. Surv., 48(2):27:1–
27:42, 2015.
[107] Dogan Gunes, Taha. Strategic and Adaptive Behaviours in Trust Systems. PhD thesis, University of Southampton, 2021.
[108] Jörg P. Müller and Klaus Fischer. Application impact of multi-agent systems and technologies: A survey. In
Onn Shehory and Arnon Sturm, editors, Agent-Oriented Software Engineering - Reflections on Architectures,
Methodologies, Languages, and Frameworks, pages 27–53. 2014.
[109] Manuel Herrera, Marco Pérez-Hernández, Ajith Kumar Parlikad, and Joaquı́n Izquierdo. Multi-agent systems
and complex networks: Review and applications in systems engineering. Processes, 8(3), 2020.
[110] Davide Calvaresi, Alevtina Dubovitskaya, Jean-Paul Calbimonte, Kuldar Taveter, and Michael Schumacher.
Multi-agent systems and blockchain: Results from a systematic literature review. In Yves Demazeau, Bo An,
Javier Bajo, and Antonio Fernández-Caballero, editors, Advances in Practical Applications of Agents, MultiAgent Systems, and Complexity: The PAAMS Collection - 16th International Conference, PAAMS 2018, Toledo,
Spain, June 20-22, 2018, Proceedings, volume 10978 of Lecture Notes in Computer Science, pages 110–126,
2018.
[111] Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V. Albrecht. Benchmarking multi-agent
deep reinforcement learning algorithms in cooperative tasks. In Joaquin Vanschoren and Sai-Kit Yeung, editors,
Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS
Datasets and Benchmarks 2021, December 2021, virtual, 2021.
[112] Ben Bogin, Mor Geva, and Jonathan Berant. Emergence of communication in an interactive world with consistent speakers. CoRR, abs/1809.00549, 2018.
[113] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and
taxonomy. IEEE Trans. Pattern Anal. Mach. Intell., 41(2):423–443, 2019.
[114] Petra Poklukar, Miguel Vasco, Hang Yin, Francisco S. Melo, Ana Paiva, and Danica Kragic. Geometric
multimodal contrastive representation learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022,
17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages
17782–17800, 2022.
[115] Joseph Seering, Michal Luria, Geoff Kaufman, and Jessica Hammer. Beyond dyadic interactions: Considering
chatbots as community members. In Stephen A. Brewster, Geraldine Fitzpatrick, Anna L. Cox, and Vassilis
Kostakos, editors, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI
2019, Glasgow, Scotland, UK, May 04-09, 2019, page 450, 2019.
[116] Joseph Seering, Michal Luria, Connie Ye, Geoff Kaufman, and Jessica Hammer. It takes a village: Integrating
an adaptive chatbot into an online gaming community. In Regina Bernhaupt, Florian ’Floyd’ Mueller, David
Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, Alix Goguey, Pernille Bjøn,
Shengdong Zhao, Briane Paul Samson, and Rafal Kocielnik, editors, CHI ’20: CHI Conference on Human
Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pages 1–13, 2020.
[117] Asbjørn Følstad and Petter Bae Brandtzæg. Chatbots and the new world of HCI. Interactions, 24(4):38–42,
2017.
[118] Romit Roy Choudhury, Krishna Paul, and Somprakash Bandyopadhyay. Marp: a multi-agent routing protocol
for mobile wireless ad hoc networks. Autonomous Agents and Multi-Agent Systems, 8(1):47–68, 2004.
[119] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine
33

Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2817–2826, 2017.
[120] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. In Elisabeth André, Sven Koenig, Mehdi Dastani, and Gita
Sukthankar, editors, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent
Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pages 2040–2042, 2018.
[121] Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In The Thirty-Third AAAI Conference on Artificial
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI
2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
Hawaii, USA, January 27 - February 1, 2019, pages 4213–4220, 2019.
[122] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-agent
reinforcement learning with model uncertainty. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
[123] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997.
[124] Jakob N. Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip H. S. Torr, Pushmeet Kohli,
and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research,
pages 1146–1155, 2017.

34

