QTRAN: Learning to Factorize with Transformation for
Cooperative Multi-Agent Reinforcement learning

arXiv:1905.05408v1 [cs.LG] 14 May 2019

Kyunghwan Son 1 Daewoo Kim 1 Wan Ju Kang 1 David Hostallero 1 Yung Yi 1

Abstract
We explore value-based solutions for multi-agent
reinforcement learning (MARL) tasks in the centralized training with decentralized execution
(CTDE) regime popularized recently. However,
VDN and QMIX are representative examples that
use the idea of factorization of the joint actionvalue function into individual ones for decentralized execution. VDN and QMIX address only
a fraction of factorizable MARL tasks due to
their structural constraint in factorization such
as additivity and monotonicity. In this paper, we
propose a new factorization method for MARL,
QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function
into an easily factorizable one, with the same
optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than
does previous methods. Our experiments for the
tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRANâ€™s superior performance with especially larger margins
in games whose payoffs penalize non-cooperative
behavior more aggressively.

1. Introduction
Reinforcement learning aims to instill in agents a good
policy that maximizes the cumulative reward in a given
environment. Recent progress has witnessed success in
various tasks, such as Atari games (Mnih et al., 2015), Go
(Silver et al., 2016; 2017), and robot control (Lillicrap et al.,
2015), just to name a few, with the development of deep
learning techniques. Such advances largely consist of deep
1

School of Electrical Enginerring, KAIST, Daejeon, South Korea. Correspondence to: Yung Yi <yiyung@kaist.edu>, Kyunghwan Son <kevinson9473@kaist.ac.kr>.
Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

neural networks, which can represent action-value functions
and policy functions in reinforcement learning problems
as a high-capacity function approximator. However, more
complex tasks such as robot swarm control and autonomous
driving, often modeled as cooperative multi-agent learning
problems, still remain unconquered due to their high scales
and operational constraints such as distributed execution.
The use of deep learning techniques carries through to cooperative multi-agent reinforcement learning (MARL). MADDPG (Lowe et al., 2017) learns distributed policy in continuous action spaces, and COMA (Foerster et al., 2018)
utilizes a counterfactual baseline to address the credit assignment problem. Among value-based methods, value function
factorization (Koller & Parr, 1999; Guestrin et al., 2002a;
Sunehag et al., 2018; Rashid et al., 2018) methods have been
proposed to efficiently handle a joint action-value function
whose complexity grows exponentially with the number of
agents.
Two representative examples of value function factorization
include VDN (Sunehag et al., 2018) and QMIX (Rashid
et al., 2018). VDN factorizes the joint action-value function
into a sum of individual action-value functions. QMIX extends this additive value factorization to represent the joint
action-value function as a monotonic function â€” rather
than just as a sum â€” of individual action-value functions,
thereby covering a richer class of multi-agent reinforcement
learning problems than does VDN. However, these value
factorization techniques still suffer structural constraints,
namely, additive decomposability in VDN and monotonicity in QMIX, often failing to factorize a factorizable task.
A task is factorizable if the optimal actions of the joint
action-value function are the same as the optimal ones of
the individual action-value functions, where additive decomposability and monotonicity are only sufficient â€” somewhat
excessively restrictive â€” for factorizability.
Contribution In this paper, we aim at successfully
factorizing any factorizable task, free from additivity/monotonicity concerns. We transform the original joint
action-value function into a new, easily factorizable one
with the same optimal actions in both functions. This is
done by learning a state-value function, which corrects for
the severity of the partial observability issue in the agents.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

We incorporate the said idea in a novel architecture, called
QTRAN, consisting of the following inter-connected deep
neural networks: (i) joint action-value network, (ii) individual action-value networks, and (iii) state-value network.
To train this architecture, we define loss functions appropriate for each neural network. We develop two variants of
QTRAN: QTRAN-base and QTRAN-alt, whose distinction
is twofold: how to construct the transformed action-value
functions for non-optimal actions; and the degree of stability and convergence speed. We assess the performance
of QTRAN by comparing it against VDN and QMIX in
three environments. First, we consider a simple, singlestate matrix game that does not satisfy additivity or monotonicity, where QTRAN successfully finds the joint optimal action, whereas neither VDN nor QMIX does. We
then observe a similarly desirable cooperation-inducing tendency of QTRAN in more complex environments: modified
predator-prey games and multi-domain Gaussian squeeze
tasks. In particular, we show that the performance gap
between QTRAN and VDN/QMIX increases with environments having more pronounced non-monotonic characteristics.
Related work Extent of centralization varies across the
spectrum of cooperative MARL research. While more decentralized methods benefit from scalability, they often suffer non-stationarity problems arising from a trivialized superposition of individually learned behavior. Conversely,
more centralized methods alleviate the non-stationarity issue at the cost of complexity that grows exponentially with
the number of agents.
Prior work tending more towards the decentralized end of
the spectrum include Tan (1993), whose independent Qlearning algorithm exhibits the greatest degree of decentralization. Tampuu et al. (2017) combines this algorithm with
deep learning techniques presented in DQN (Mnih et al.,
2015). These studies, while relatively simpler to implement,
are subject to the threats of training instability, as multiple
agents attempt to improve their policy in the midst of other
agents, whose policies also change over time during training.
This simultaneous alteration of policies essentially makes
the environment non-stationary.
The other end of the spectrum involves some centralized
entity to resolve the non-stationarity problem. Guestrin et al.
(2002b) and Kok & Vlassis (2006) are some of the earlier representative works. Guestrin et al. (2002b) proposes
a graphical model approach in presenting an alternative
characterization of a global reward function as a sum of
conditionally independent agent-local terms. Kok & Vlassis
(2006) exploits the sparsity of the states requiring coordination compared to the whole state space and then tabularize
those states to carry out tabular Q-learning methods as in
Watkins (1989).

The line of research positioned mid-spectrum aims to put
together the best of both worlds. More recent studies, such
as COMA (Foerster et al., 2018), take advantage of CTDE
(Oliehoek et al., 2008); actors are trained by a joint critic to
estimate a counterfactual baseline designed to gauge each
agentâ€™s contribution to the shared task. Gupta et al. (2017)
implements per-agent critics to opt for better scalability at
the cost of diluted benefits of centralization. MADDPG
(Lowe et al., 2017) extends DDPG (Lillicrap et al., 2015) to
the multi-agent setting by similar means of having a joint
critic train the actors. Wei et al. (2018) proposes MultiAgent Soft Q-learning in continuous action spaces to tackle
the relative overgeneralization problem (Wei & Luke, 2016)
and achieves better coordination. Other related work includes CommNet (Sukhbaatar et al., 2016), DIAL (Foerster
et al., 2016), ATOC (Jiang & Lu, 2018), and SCHEDNET
(Kim et al., 2019), which exploit inter-agent communication
in execution.
On a different note, two representative examples of valuebased methods have recently been shown to be somewhat
effective in analyzing a class of games. Namely, VDN
(Sunehag et al., 2018) and QMIX (Rashid et al., 2018) represent the body of literature most closely related to this
paper. While both are value-based methods and follow the
CTDE approach, the additivity and monotonicity assumptions naturally limit the class of games that VDN or QMIX
can solve.

2. Background
2.1. Model and CTDE
DEC-POMDP We take DEC-POMDP (Oliehoek et al.,
2016) as the de facto standard for modelling cooperative multi-agent tasks, as do many previous works: as
a tuple G =< S, U, P, r, Z, O, N, Î³ >, where s âˆˆ S
denotes the true state of the environment. Each agent
i âˆˆ N := {1, ..., N } chooses an action ui âˆˆ U at each time
N
step, giving rise to a joint action vector, u := [ui ]N
i=1 âˆˆ U .
0
N
Function P (s |s, u) : S Ã— U Ã— S 7â†’ [0, 1] governs all
state transition dynamics. Every agent shares the same joint
reward function r(s, u) : S Ã— U N 7â†’ R, and Î³ âˆˆ [0, 1)
is the discount factor. Each agent has individual, partial
observation z âˆˆ Z, according to some observation function
O(s, i) : S Ã— N 7â†’ Z. Each agent also has an actionobservation history Ï„i âˆˆ T := (Z Ã— U)âˆ— , on which it
conditions its stochastic policy Ï€i (ui |Ï„i ) : T Ã— U 7â†’ [0, 1].
Training and execution: CTDE Arguably the most
naÃ¯ve training method for MARL tasks is to learn the individual agentsâ€™ action-value functions independently, i.e.,
independent Q-learning. This method would be simple and
scalable, but it cannot guarantee convergence even in the
limit of infinite greedy exploration. As an alternative solution, recent works including VDN (Sunehag et al., 2018)

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

and QMIX (Rashid et al., 2018) employ centralized training
with decentralized execution (CTDE) (Oliehoek et al., 2008)
to train multiple agents. CTDE allows agents to learn and
construct individual action-value functions, such that optimization at the individual level leads to optimization of the
joint action-value function. This in turn, enables agents at
execution time to select an optimal action simply by looking
up the individual action-value functions, without having to
refer to the joint one. Even with only partial observability
and restricted inter-agent communication, information can
be made accessible to all agents at training time.
2.2. IGM Condition and Factorizable Task
Consider a class of sequential decision-making tasks that are
amenable to factorization in the centralized training phase.
We first define IGM (Individual-Global-Max):
Definition 1 (IGM). For a joint action-value function
Qjt : T N Ã— U N 7â†’ R, where Ï„ âˆˆ T N is a joint actionobservation histories, if there exist individual action-value
functions [Qi : T Ã— U 7â†’ R]N
i=1 , such that the following
holds
ï£«
ï£¶
arg maxu1 Q1 (Ï„1 , u1 )
ï£·
ï£¬
..
arg max Qjt (Ï„ , u) = ï£­
ï£¸ , (1)
.

3. QTRAN: Learning to Factorize with
Transformation
In this section, we propose a new method called QTRAN,
aiming at factorizing any factorizable task. The key idea is
to transform the original joint action-value function Qjt into
a new one Q0jt that shares the optimal joint action with Qjt .
3.1. Conditions for the Factor Functions [Qi ]
For a given joint observation Ï„ , consider an arbitrary factorizable Qjt (Ï„ , u). Then, by Definition 1 of IGM, we can
find individual action-value functions [Qi (Ï„i , ui )] that factorize Qjt (Ï„ , u). Theorem 1 states the sufficient condition
for [Qi ] that satisfy IGM. Let uÌ„i denote the optimal local
action arg maxui Qi (Ï„i , ui ) and uÌ„ = [uÌ„i ]N
i=1 ,. Also, let
Q = [Qi ] âˆˆ RN , i.e., a column vector of Qi , i = 1, . . . , N.
Theorem 1. A factorizable joint action-value function
Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], if
N
X

Qi (Ï„i , ui )âˆ’Qjt (Ï„ , u)+Vjt (Ï„ ) =

i=1

Vjt (Ï„ ) = max Qjt (Ï„ , u) âˆ’
u

then, we say that [Qi ] satisfy IGM for Qjt under Ï„ . In this
case, we also say that Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )],
or that [Qi ] are factors of Qjt .
Simply put, the optimal joint actions across agents are equivalent to the collection of individual optimal actions of each
agent. If Qjt (Ï„ , u) in a given task is factorizable under all
Ï„ âˆˆ T N , we say that the task itself is factorizable.
2.3. VDN and QMIX
Given Qjt , one can consider the following two sufficient
conditions for IGM:
(Additivity)

Qjt (Ï„ , u) =

N
X

Qi (Ï„i , ui ),

(2)

i=1

(Monotonicity)

âˆ‚Qjt (Ï„ , u)
â‰¥ 0,
âˆ‚Qi (Ï„i , ui )

âˆ€i âˆˆ N .

(3)

u = uÌ„, (4a)

â‰¥ 0 u 6= uÌ„, (4b)

where

u

arg maxuN QN (Ï„n , uN )


0

N
X

Qi (Ï„i , uÌ„i ).

i=1

The proof is provided in the Supplementary. We note that
conditions in (4) are also necessary under an affine transformation. That is, there exists an affine transformation
Ã—N
Ï†(Q) = A Â· Q + B, where A = [aii ] âˆˆ RN
is a symmet+
ric diagonal matrix with aii > 0, âˆ€i and B = [bi ] âˆˆ RN ,
such that if Qjt is factorized by [Qi ], then (4) holds by replacing Qi with aii Qi + bi . This is because for all i, bi
cancels out,
PN and aii just plays the role of re-scaling the
value of i=1 Qi in multiplicative (with a positive scaling
constant) and additive manners, since IGM is invariant to Ï†
of [Qi ].
Factorization via transformation We first define a new
function Q0jt as the linear sum of individual factor functions
[Qi ]:
N
X
Q0jt (Ï„ , u) :=
Qi (Ï„i , ui ).
(5)
i=1

VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018)
are methods that attempt to factorize Qjt assuming additivity and monotonicity, respectively. Thus, joint actionvalue functions satisfying those conditions would be wellfactorized by VDN and QMIX. However, there exist tasks
whose joint action-value functions do not meet the said conditions. We illustrate this limitation of VDN and QMIX
using a simple matrix game in the next section.

We call Q0jt (Ï„ , u) the transformed joint-action value function throughout this paper. Our idea of factorization is
as follows: from the additive construction of Q0jt based
on [Qi ], [Qi ] satisfy IGM for the new joint action-value
function Q0jt , implying that [Qi ] are also the factorized individual action-value functions of Q0jt . From the fact that
arg maxu Qjt (Ï„ , u) = arg maxu Q0jt (Ï„ , u), finding [Qi ]
satisfying (4) is precisely the factorization of Q0jt (Ï„ , u).

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

+

Training
Execution
(ðœ1 , ð‘¢1 )

â€²
ð‘„ð‘—ð‘¡
(ð‰, ð’–)

ð‘„1 ðœ1 ,â‹…
ð‘„1 (ðœ1 , ð‘¢1 )

ð‘³ð’ð’‘ð’• , ð‘³ð’ð’ð’‘ð’•

ð‘‰ð‘—ð‘¡ ð‰
â„Ž1 (ðœ1 , ð‘¢1 )

ð‘„ð‘—ð‘¡ ð‰, ð’–

ð‘³ð’•ð’…

â€¦

â€²
ð‘„ð‘—ð‘¡
ð‰,â‹…, ð’–âˆ’1

ð‘„ð‘—ð‘¡ ð‰,â‹…, ð’–âˆ’1

â„Žð‘ (ðœð‘ , ð‘¢ð‘ )

ð‘„ð‘—ð‘¡ ð‰,â‹…, ð’–âˆ’ð‘

ð‘³ð’ð’‘ð’• , ð‘³ð’ð’ð’‘ð’•âˆ’ð’Žð’Šð’
ð‘³ð’•ð’…
ð‘³ð’ð’‘ð’• , ð‘³ð’ð’ð’‘ð’•âˆ’ð’Žð’Šð’

(ðœð‘ , ð‘¢ð‘ )
ð‘„ð‘ (ðœð‘ , ð‘¢ð‘ )
ð‘„ð‘ ðœð‘ ,â‹…

+

ð‘‰ð‘—ð‘¡ ð‰

ð‘³ð’•ð’…
â€²
ð‘„ð‘—ð‘¡
ð‰,â‹…, ð’–âˆ’ð‘

QTRAN-base
or
QTRAN-alt

Figure 1. QTRAN-base and QTRAN-alt Architecture

One interpretation of this process is that rather than directly
factorizing Qjt , we consider an alternative joint action-value
function (i.e., Q0jt ) that is factorized by additive decomposition. The function Vjt (Ï„ ) corrects for the discrepancy
between the centralized joint action-value function Qjt and
the sum of individual joint action-value functions [Qi ]. This
discrepancy arises from the partial observability of agents.
If bestowed with full observability, Vjt can be re-defined
as zero, and the definitions would still stand. Refer to the
Supplementary for more detail.
3.2. Method
Overall framework In this section, we propose a new
deep RL framework with value function factorization, called
QTRAN, whose architectural sketch is given in Figure 1.
QTRAN consists of three separate estimators: (i) each agent
iâ€™s individual action-value network for Qi , (ii) a joint actionvalue network for Qjt to be factorized into individual actionvalue functions Qi , and (iii) a state-value network for Vjt ,
i.e.,
(Individual action-value network) fq : (Ï„i , ui ) 7â†’ Qi ,
(Joint action-value network) fr : (Ï„ , u) 7â†’ Qjt ,
(State-value network) fv : Ï„ 7â†’ Vjt .
Three neural networks are trained in a centralized manner,
and each agent uses its own factorized individual actionvalue function Qi to take action during decentralized execution. Each network is elaborated next.
Individual action-value networks For each agent, an
action-value network takes its own action-observation history Ï„i as input, and produces action-values Qi (Ï„i , Â·) as
output. This action-value network is used for each agent to

determine its own action by calculating the action-value for
a given Ï„i . As defined in (5), Q0jt is just the summation of
the outputs of all agents.
Joint action-value network The joint action-value network approximates Qjt . It takes as input the selected action
and produces the Q-value of the chosen action as output. For
scalability and sample efficiency, we design this network
as follows. First, we use the action vector sampled by all
individual action-value networks to update the joint actionvalue network. Since the joint action space is U N , finding
an optimal action requires high complexity as the number
of agents N grows, whereas obtaining an optimal action
in each individual network is done by decentralized policies with linear-time individual arg max operations. Second, the joint action-value network shares the parameters
at the lower layers of individual networks, where the joint
action-value
network combines hidden features with summaP
tion i hQ,i (Ï„i , ui ) of hi (Ï„i , ui ) = [hQ,i (Ï„i , ui ), hV,i (Ï„i )]
from individual networks. This parameter sharing is used to
enable scalable training with good sample efficiency at the
expense of expressive power.
State-value network The state-value network is responsible for computing a scalar state-value, similar to V (s) in
the dueling network (Wang et al., 2016). Vjt is required
to provide the flexibility to match Qjt and Q0jt + Vjt at
arg max. Without state-value network, partial observability
would limit the representational complexity of Q0jt . The
state-value is independent of the selected action for a given
Ï„ . Thus, this value network does not contribute to choosing
an action, and is instead used to calculate the loss of (4).
Like the joint action-value
network, we use the combined
P
hidden features i hV,i (Ï„i ) from the individual networks
as input to the value network for scalability.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

3.3. Loss Functions
There are two major goals in centralized training. One is
that we should train the joint action-value function Qjt to
estimate the true action-value; the other is that the transformed action-value function Q0jt should â€œtrackâ€ the joint
action-value function in the sense that their optimal actions
are equivalent. We use the algorithm introduced in DQN
(Mnih et al., 2015) to update networks, where we maintain
a target network and a replay buffer. To this end, we devise
the following global loss function in QTRAN, combining
three loss functions in a weighted manner:
L(Ï„ , u, r, Ï„ 0 ; Î¸) = Ltd + Î»opt Lopt + Î»nopt Lnopt ,

(6)

where r is the reward for action u at observation histories Ï„
with transition to Ï„ 0 . Ltd is the loss function for estimating
the true action-value, by minimizing the TD-error as Qjt
is learned. Lopt and Lnopt are losses for factorizing Qjt by
[Qi ] satisfying condition (4). The role of Lnopt is to check at
each step if the action selected in the samples satisfied (4b),
and Lopt confirms that the optimal local action obtained
satisfies (4a). One could implement (4) by defining a loss
depending on how well the networks satisfy (4a) or (4b)
with actions taken in the samples. However, in this way,
verifying whether (4a) is indeed satisfied would take too
many samples since optimal actions are seldom taken at
training. Since we aim to learn Q0jt and Vjt to factorize
for a given Qjt , we stabilize the learning by fixing Qjt
when learning with Lopt and Lnopt . We let QÌ‚jt denote this
fixed Qjt . Î»opt and Î»nopt are the weight constants for two
losses. The detailed forms of Ltd , Lopt , and Lnopt are given
as follows, where we omit their common function arguments
(Ï„ , u, r, Ï„ 0 ) in loss functions for presentational simplicity:
2
Ltd (; Î¸) = Qjt (Ï„ , u) âˆ’ y dqn (r, Ï„ 0 ; Î¸ âˆ’ ) ,
2
Lopt (; Î¸) = Q0jt (Ï„ , uÌ„) âˆ’ QÌ‚jt (Ï„ , uÌ„) + Vjt (Ï„ ) ,


2
Lnopt (; Î¸) = min Q0jt (Ï„ , u) âˆ’ QÌ‚jt (Ï„ , u) + Vjt (Ï„ ), 0
,
where y dqn (r, Ï„ 0 ; Î¸ âˆ’ ) = r + Î³Qjt (Ï„ 0 , uÌ„0 ; Î¸ âˆ’ ), uÌ„0 =
âˆ’
[arg maxui Qi (Ï„i0 , ui ; Î¸ âˆ’ )]N
i=1 , and Î¸ are the periodically
copied parameters from Î¸, as in DQN (Mnih et al., 2015).
3.4. Tracking the Joint Action-value Function
Differently
We name the method previously discussed QTRAN-base,
to reflect the basic nature of how it keeps track of the joint
action-value function. Here on, we consider a variant of
QTRAN, which utilizes a counterfactual measure. As mentioned earlier, Theorem 1 is used to enforce IGM by (4a)
and determine how the individual action-value functions
[Qi ] and the State-value function Vjt jointly â€œtrackâ€ Qjt by
(4b), which governs the stability of constructing the correct factorizing Qi â€™s. We found that condition (4b) is often

too loose, leading the neural networks to fail their mission
of constructing the correct factors of Qjt . That is, condition (4b) imposes undesirable influence on the non-optimal
actions, which in turn compromises the stability and/or convergence speed of the training process. This motivates us to
study conditions stronger than (4b) that would still be sufficient for factorizability, but at the same time would also be
necessary under the aforementioned affine transformation
Ï†, as in Theorem 1.
Theorem 2. The statement presented in Theorem 1 and the
necessary condition of Theorem 1 holds by replacing (4b)
with the following (7): if u 6= uÌ„,
h
min Q0jt (Ï„ , ui , uâˆ’i ) âˆ’ Qjt (Ï„ , ui , uâˆ’i )
ui âˆˆU
i
+ Vjt (Ï„ ) = 0, âˆ€i = 1, . . . , N,
(7)
where uâˆ’i = (u1 , . . . , uiâˆ’1 , ui+1 , . . . , uN ), i.e., the action
vector except for iâ€™s action.
The proof is presented in the Supplementary. The key idea
behind (7) lies in what conditions to enforce on non-optimal
actions. It stipulates that Q0jt (Ï„ , u) âˆ’ Qjt (Ï„ , u) + Vjt (Ï„ )
be set to zero for some actions. Now, it is not possible to
zero this value for every action u, but it is available for at
least one action whilst still abiding by Theorem 1. It is
clear that condition (7) is stronger than condition (4b), as
desired. For non-optimal actions u 6= uÌ„, the conditions
of Theorem 1 are satisfied when Qjt (Ï„ , u) âˆ’ Vjt (Ï„ ) â‰¤
Q0jt (Ï„ , u) â‰¤ Q0jt (Ï„ , uÌ„) for any given Ï„ . Under this condition, however, there can exist a non-optimal action u 6= uÌ„
whose Q0jt (Ï„ , u) is comparable to Q0jt (Ï„ , uÌ„) but Qjt (Ï„ , u)
is much smaller than Qjt (Ï„ , uÌ„). This may cause instability in the practical learning process. However, the newly
devised condition (7) compels Q0jt (Ï„ , u) to track Qjt (Ï„ , u)
even for the problematic non-optimal actions mentioned
above. This helps in widening the gap between Q0jt (Ï„ , u)
and Q0jt (Ï„ , uÌ„), and this gap makes the algorithm more stable. Henceforth, we call the deep MARL method outlined
by Theorem 2 QTRAN-alt, to distinguish it from the one
due to condition (4b).
Counterfactual joint networks To reflect our idea of (7),
we now propose a counterfactual joint network, which replaces the joint action-value network of QTRAN-base, to
efficiently calculate Qjt (Ï„ , Â·, uâˆ’i ) and Q0jt (Ï„ , Â·, uâˆ’i ) for
all i âˆˆ N with only one forward pass. To this end, in
the QTRAN-alt module, each agent has a counterfactual
joint network with the output of Qjt (Ï„ , Â·, uâˆ’i ) for each
possible action, given other agentsâ€™ actions. As a joint
action-value network,
we use hV,i (Ï„i ) and the combined hidP
den features j6=i hQ,j (Ï„j , uj ) from otherPagents. Finally,
Q0jt (Ï„ , Â·, uâˆ’i ) is calculated as Qi (Ï„i , Â·) + j6=i Qj (Ï„j , uj )
for all agents. This architectural choice is realized by
choosing the loss function to be Lnopt-min , replacing Lnopt in

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning
u2
u1

A
8
-12
-12

A
B
C

B
-12
0
0

C
-12
0
0

Q2
Q1
3.84(A)
-2.06(B)
-2.25(C)

4.16(A) 2.29(B) 2.29(C)
8.00
2.10
1.92

6.13
0.23
0.04

6.12
0.23
0.04

(a) Payoff of matrix game

(b) QTRAN: Q1 , Q2 , Q0jt

u2

u2

u1
A
B
C

A

B

C

8.00
-12.00
-12.00

-12.02
0.00
0.00

-12.02
0.00
-0.01

(c) QTRAN: Qjt
Q2
Q1
-2.29(A)
-1.22(B)
-0.73(C)

-3.14(A) -2.29(B) -2.41(C)
-5.42
-4.35
-3.87

-4.57
-3.51
-3.02

-4.70
-3.63
-3.14

(e) VDN: Q1 , Q2 , Qjt

u1

A

B

C

0.00
14.11
13.93

18.14
0.23
0.05

18.14
0.23
0.05

20

20

15

15

15

10

10

10

10

5

5

5

0

0
0

5

10

15

20

5

0
0

5

10

15

20

0
0

5

10

15

20

0

20

20

20

15

15

15

10

10

10

10

(d) QTRAN: Q0jt âˆ’ Qjt

5

5

5

5

Q2

0

0

0

Q1
-1.02(A)
0.11(B)
0.10(C)

-0.92(A) 0.00(B) 0.01(C)
-8.08
-8.08
-8.08

-8.08
0.01
0.01

-8.08
0.03
0.02

(f) QMIX: Q1 , Q2 , Qjt

N

1 X
( min D(Ï„ , ui , uâˆ’i ))2 ,
N i=1 ui âˆˆU

where
D(Ï„ , ui , uâˆ’i ) = Q0jt (Ï„ , ui , uâˆ’i ) âˆ’ QÌ‚jt (Ï„ , ui , uâˆ’i )+Vjt (Ï„ ).
In QTRAN-alt, Ltd , Lopt are also used, but they are also
computed for all agents.
3.5. Example: Single-state Matrix Game
In this subsection, we present how QTRAN performs compared to existing value factorization ideas such as VDN
and QMIX, and how the two variants QTRAN-base and
QTRAN-alt behave. The matrix game and learning results
are shown in Table 11 . This symmetric matrix game has
the optimal joint action (A, A), and captures a very simple
cooperative multi-agent task, where we have two users with
three actions each. Evaluation with more complicated tasks
are provided in the next subsection. We show the results
of VDN, QMIX, and QTRAN through a full exploration
(i.e.,  = 1 in -greedy) conducted over 20,000 steps. Full
exploration guarantees to explore all available game states.
Therefore, we can compare only the expressive power of the
methods. Other details are included in the Supplementary.
Comparison with VDN and QMIX Tables 1b-1f show
the learning results of QTRAN, VDN, and QMIX. Table 1b
shows that QTRAN enables each agent to jointly take the
optimal action only by using its own locally optimal action,
We present only Qjt and Q0jt , because in fully observable
cases (i.e., observation function is bijective for all i) Theorem 1
holds for Vjt (Ï„ ) = 0. We discuss further in Supplementary.

5

10

15

20

(a) Qjt : 1000 step (b) Qjt : 2000 step (c) Qjt : 3000 step (d) Qjt : 4000 step
15

QTRAN-base as follows:

1

20

15

20

A
B
C

Table 1. Payoff matrix of the one-step game and reconstructed Qjt
results on the game. Boldface means optimal/greedy actions from
the state-action value

Lnopt-min (Ï„ , u, r, Ï„ 0 ; Î¸) =

20

0

5

10

15

20

0

5

10

15

20

0
0

5

10

15

20

0

5

10

15

20

(e) base: 1000 step (f) base: 2000 step (g) base: 3000 step (h) base: 4000 step
20

20

20

20

15

15

15

15

10

10

10

10

5

5

5

5

0

0

0

0

5

10

15

20

(i) alt: 1000 step

0

5

10

15

20

(j) alt: 2000 step

0
0

5

10

15

20

(k) alt: 3000 step

0

5

10

15

20

(l) alt: 4000 step

Figure 2. base: QTRAN-base, alt: QTRAN-alt. x-axis and y-axis:
agents 1 and 2â€™s actions, respectively. Colored values represent the
values of Qjt ((a)-(d)) and Q0jt ((e)-(l)) for selected actions.

meaning successful factorization. Note that Tables 1c and
1d demonstrate the difference between Qjt and Q0jt , stemming from our transformation, where their optimal actions
are the nonetheless same. Table 1d shows that QTRAN
also satisfies (4), thereby validating our design principle as
described in Theorem 1. However, in VDN, agents 1â€™s and
2â€™s individual optimal actions are C and B, respectively.
VDN fails to factorize
Pbecause the structural constraint of
additivity Qjt (u) = i=1,2 Qi (ui ) is enforced, leading to
deviations from IGM,Pwhose sufficient condition is additivity, i.e., Qjt (u) = i=1,2 Qi (ui ) for all u = (u1 , u2 ).
QMIX also fails in factorization in a similar manner due to
the structural constraint of monotonicity.
Impact of QTRAN-alt In order to see the impact of
QTRAN-alt, we train the agents in a matrix game where
two agents each have 21 actions. Figure 2 illustrates the
joint action-value function and its transformations of both
QTRAN-base and QTRAN-alt. The result shows that both
algorithms successfully learn the optimal action by correctly
estimating the Qjt for u = uÌ„ for any given state. Q0jt values
for non-optimal actions are different from Qjt , but it has a
different tendency in each algorithm as follows. As shown
in Figures 2e-2h, all Q0jt values in QTRAN-base have only
a small difference from the maximum value of Qjt , whereas
Figures 2i-2l show that QTRAN-alt has the ability to more
accurately distinguish optimal actions from non-optimal actions. Thus, in QTRAN-alt, the agent can smartly explore
and have better sample efficiency to train the networks. This
feature of QTRAN-alt also prevents learning unsatisfactory
policies in complex environments. Full details on the experiment are included in the Supplementary.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

4

0

QTRAN-alt
QTRAN-base
QMIX
VDN

0

5Ã—105
Training step

(a) Gaussian Squeeze

106

8

4

0

QTRAN-alt
QTRAN-base
QMIX
VDN

0

5Ã—105
Training step

106

Score

8

Score

Score

8

4

0

QTRAN-alt
QTRAN-base
QMIX
VDN

0

5Ã—105
Training step

106

(b) Gaussian Squeeze without epsilon decay (c) Multi-domain Gaussian Squeeze K = 2

Figure 3. Average reward on the GS and GMS tasks with 95% confidence intervals for VDN, QMIX, and QTRAN

4. Experiment
4.1. Environments
To demonstrate the performance of QTRAN, we consider
two environments: (i) Multi-domain Gaussian Squeeze
and (ii) modified predator-prey. Details on our implementation of QTRAN, the source code of our implementation in TensorFlow, and other experimental scripts are
available in the Supplementary and a public repository:
https://github.com/Sonkyunghwan/QTRAN.
Multi-domain Gaussian Squeeze (MGS) Gaussian
Squeeze (GS) (HolmesParker et al., 2014) is a simple nonmonotonic multi-agent resource allocation problem, used
in other papers, e.g., Yang et al. (2018) and Chen et al.
(2018). In GS, multiple homogeneous agents need to work
together for efficient resource allocation whilst avoiding
congestion. We use GS in conjunction with Multi-domain
Gaussian Squeeze (MGS) as follows: we have ten agents;
each agent i takes action ui , which controls the resource
usage level, ranging over {0, 1, ..., 9}. Each agent has its
own amount of unit-level resource si âˆˆ [0, 0.2], given by
the environment a priori. Then, a joint
P action u determines
the overall resource usage x(u) = i si Ã— ui . We assume
that there exist K domains, where the above resource allocation takes place. Then, the goal is to maximize the joint
PK
2
2
reward defined as G(u) = k=1 xeâˆ’(xâˆ’Âµk ) /Ïƒk , where
Âµk and Ïƒk are the parameters of each domain. Depending
on the number of domains, GS has only one local maximum,
whereas MGS has multiple local maxima. In our MGS setting, compared to GS, the optimal policy is similar to that in
GS, and through this policy, the reward similar to that in GS
can be obtained. Additionally, in MGS, a new sub-optimal
â€œpitfallâ€ policy that is easier to achieve but is only half as
rewarding as the optimal policy. The case when K > 1 is
usefully utilized to test the algorithms that are required to
avoid sub-optimal points in the joint space of actions. The
full details on the environment setup and hyperparameters
are described in the Supplementary.
Modified predator-prey (MPP) We adopt a more complicated environment by modifying the well-known predator-

prey (Stone & Veloso, 2000) in the grid world, used in many
other MARL research. State and action spaces are constructed similarly to those of the classic predator-prey game.
â€œCatchingâ€ a prey is equivalent to having the prey within an
agentâ€™s observation horizon. We extend it to the scenario
that positive reward is given only if multiple predators catch
a prey simultaneously, requiring a higher degree of cooperation. The predators get a team reward of 1, if two or more
catch a prey at the same time, but they are given negative
reward âˆ’P , when only one predator catches the prey.
Note that the value of penalty P also determines the degree
of monotonicity, i.e., the higher P is, the less monotonic the
task is. The prey that has been caught regenerated at random
positions whenever caught by more than one predator. In
our evaluation, we tested up to N = 4 predators and up
to two prey, and the game proceeds over fixed 100 steps.
We experimented with six different settings with varying
P values and numbers of agents, where N = 2, 4 and
P = 0.5, 1.0, 1.5. For the N = 4 case, we placed two prey;
otherwise, just one. The detailed settings are available in
the Supplementary.
4.2. Results
Multi-domain Gaussian Squeeze Figure 3a shows the
result of GS, where it is not surprising to observe that all algorithms converge to the optimal point. However, QTRAN
noticeably differs in its convergence speed; it is capable
of handling the non-monotonic nature of the environment
more accurately and of finding an optimal policy from wellexpressed action-value functions. VDN and QMIX have
some structural constraints, which hinder the accurate learning of the action-value functions for non-monotonic structures. These algorithms converge to the locally optimal
point â€” which is the globally optimal point in GS, where
K = 1 â€” near the biased samples by a wrong policy with
epsilon-decay exploration. To support our claim, we experiment with the full exploration without epsilon-decay for the
same environment, as shown in Figure 3b. We observe that
QTRAN learns more or less the same policy as in Figure 3a,
whereas VDN and QMIX significantly deteriorate. Fig-

30

30

15

15

15

QTRAN-alt
QTRAN-base
QMIX
VDN

0
âˆ’15

0

1.5Ã—106
Training step

âˆ’15

3.0Ã—106

QTRAN-alt
QTRAN-base
QMIX
VDN

0

0

âˆ’15

3.0Ã—106

30

0

0

0

QTRAN-alt
QTRAN-base
QMIX
VDN

0

3Ã—106
Training step

(d) N = 4, P = 0.5

6Ã—106

QTRAN-alt
QTRAN-base
QMIX
VDN

âˆ’30
âˆ’60

0

5Ã—106
Training step

107

(e) N = 4, P = 1.0

Score

30

âˆ’60

0

1.5Ã—106
Training step

3.0Ã—106

(c) N = 2, P = 1.5

30

âˆ’30

QTRAN-alt
QTRAN-base
QMIX
VDN

0

(b) N = 2, P = 1.0

Score

Score

(a) N = 2, P = 0.5

1.5Ã—106
Training step

Score

30

Score

Score

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

QTRAN-alt
QTRAN-base
QMIX
VDN

âˆ’30
âˆ’60

0

5Ã—106
Training step

107

(f) N = 4, P = 1.5

Figure 4. Average reward per episode on the MPP tasks with 95% confidence intervals for VDN, QMIX, and QTRAN

ure 3c shows the result for a more challenging scenario of
MGS, with each of [si ] being the same, where agents can always achieve higher performance than GS. VDN and QMIX
are shown to learn only a sub-optimal policy in MGS, whose
rewards are even smaller than those in GS. QTRAN-base
and QTRAN-alt achieve significantly higher rewards, where
QTRAN-alt is more stable, as expected. This is because
QTRAN-altâ€™s alternative loss increases the gap between Q0jt
for non-optimal and optimal actions, and prevents it from
being updated to an undesirable policy.
Modified predator-prey Figure 4 shows the performance
of the three algorithms for six settings with different N and
P values, where all results demonstrate the superiority of
QTRAN to VDN and QMIX. In Figures 4a and 4d with
low penalties P , all three algorithms learn the policy that
catches the prey well and thus obtain high rewards. However, again, the speed in finding the policy in VDN and
QMIX is slower than that in QTRAN. As the penalty P
grows and exacerbates the non-monotonic characteristic of
the environment, we observe a larger performance gap between QTRAN and the two other algorithms. As shown in
Figures 4b and 4c, even for a large penalty, QTRAN agents
still cooperate well to catch the prey. However, in VDN
and QMIX, agents learn to work together with somewhat
limited coordination, so that they do not actively try to catch
the prey and instead attempt only to minimize the penaltyreceiving risk. In Figures 4e and 4f, when the number of
agents N grows, VDN and QMIX do not cooperate at all
and learn only a sub-optimal policy: running away from the
prey to minimize the risks of being penalized.

For the tested values of N and P , we note that the performance gap between QTRAN-base and QTRAN-alt is influenced more strongly by N . When N = 2, the gap in the
convergence speed between QTRAN-base and QTRAN-alt
increases as the penalty grows. Nevertheless, both algorithms ultimately turn out to train the agents to cooperate well for every penalty value tested. On the other
hand, with N = 4, the gap of convergence speed between
QTRAN-base and QTRAN-alt becomes larger. With more
agents, as shown in Figures 4d and 4e, QTRAN-alt achieves
higher scores than QTRAN-base does, and QTRAN-base
requires longer times to reach positive scores. Figure 4f
shows that QTRAN-base does not achieve a positive reward
until the end, implying that QTRAN-base fails to converge,
whereas QTRAN-altâ€™s score increases, with training steps
up to 107 .

5. Conclusion
This paper presented QTRAN, a learning method to factorize the joint action-value functions of a wide variety
of MARL tasks. QTRAN takes advantage of centralized
training and fully decentralized execution of the learned
policies by appropriately transforming and factorizing the
joint action-value function into individual action-value functions. Our theoretical analysis demonstrates that QTRAN
handles a richer class of tasks than its predecessors, and our
simulation results indicate that QTRAN outperforms VDN
and QMIX by a substantial margin, especially so when the
game exhibits more severe non-monotonic characteristics.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

Acknowledgements
This work was supported by Institute for Information &
communications Technology Planning & Evaluation(IITP)
grant funded by the Korea government(MSIT) (No.20180-00170,Virtual Presence in Moving Objects through 5G
(PriMo-5G))
This research was supported by Basic Science Research
Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science and ICT(No.
2016R1A2A2A05921755)

References
Chen, Y., Zhou, M., Wen, Y., Yang, Y., Su, Y., Zhang,
W., Zhang, D., Wang, J., and Liu, H. Factorized qlearning for large-scale multi-agent systems. arXiv
preprint arXiv:1809.03738, 2018.
Foerster, J., Assael, I. A., de Freitas, N., and Whiteson,
S. Learning to communicate with deep multi-agent reinforcement learning. In Proceedings of the Advances in
Neural Information Processing Systems, pp. 2137â€“2145,
2016.
Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and
Whiteson, S. Counterfactual multi-agent policy gradients.
In Proceedings of AAAI, 2018.
Guestrin, C., Koller, D., and Parr, R. Multiagent planning
with factored mdps. In Proceedings of the Advances in
Neural Information Processing Systems, pp. 1523â€“1530,
2002a.
Guestrin, C., Lagoudakis, M., and Parr, R. Coordinated
reinforcement learning. In Proceedings of ICML, pp.
227â€“234, 2002b.
Gupta, J. K., Egorov, M., and Kochenderfer, M. Cooperative
multi-agent control using deep reinforcement learning. In
Proceedings of International Conference on Autonomous
Agents and Multiagent Systems, 2017.
HolmesParker, C., Taylor, M., Zhan, Y., and Tumer, K.
Exploiting structure and agent-centric rewards to promote
coordination in large multiagent systems. In Proceedings
of Adaptive and Learning Agents Workshop, 2014.
Jiang, J. and Lu, Z. Learning attentional communication
for multi-agent cooperation. In Proceedings of NIPS, pp.
7265â€“7275, 2018.
Kim, D., Moon, S., Hostallero, D., Kang, W. J., Lee, T.,
Son, K., and Yi, Y. Learning to schedule communication
in multi-agent reinforcement learning. In Proceedings of
ICLR, 2019.

Kok, J. R. and Vlassis, N. Collaborative multiagent reinforcement learning by payoff propagation. Journal of
Machine Learning Research, 7(Sep):1789â€“1828, 2006.
Koller, D. and Parr, R. Computing factored value functions
for policies in structured mdps. In Proceedings of IJCAI,
pp. 1332â€“1339, 1999.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arxiv preprint
arXiv:1509.02971, 2015.
Lowe, R., WU, Y., Tamar, A., Harb, J., Pieter Abbeel,
O., and Mordatch, I. Multi-agent actor-critic for mixed
cooperative-competitive environments. In Proceedings of
NIPS, pp. 6379â€“6390, 2017.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529, 2015.
Oliehoek, F. A., Spaan, M. T., and Vlassis, N. Optimal and
approximate q-value functions for decentralized pomdps.
Journal of Artificial Intelligence Research, 32:289â€“353,
2008.
Oliehoek, F. A., Amato, C., et al. A concise introduction to
decentralized POMDPs, volume 1. Springer, 2016.
Omidshafiei, S., Pazis, J., Amato, C., How, J. P., and Vian,
J. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. arxiv preprint
arXiv:1703.06182, 2017.
Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
Foerster, J., and Whiteson, S. QMIX: Monotonic value
function factorisation for deep multi-agent reinforcement
learning. In Proceedings of ICML, 2018.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
Nature, 529(7587):484, 2016.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Stone, P. and Veloso, M. Multiagent systems: A survey from
a machine learning perspective. Autonomous Robots, 8
(3):345â€“383, 2000.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

Sukhbaatar, S., Fergus, R., et al. Learning multiagent communication with backpropagation. In Proceedings of the
Advances in Neural Information Processing Systems, pp.
2244â€“2252, 2016.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M.,
Zambaldi, V. F., Jaderberg, M., Lanctot, M., Sonnerat,
N., Leibo, J. Z., Tuyls, K., and Graepel, T. Valuedecomposition networks for cooperative multi-agent
learning based on team reward. In Proceedings of AAMAS, pp. 2085â€“2087, 2018.
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus,
K., Aru, J., Aru, J., and Vicente, R. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017.
Tan, M. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In Proceedings of ICML, pp. 330â€“
337, 1993.
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,
and Freitas, N. Dueling network architectures for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1995â€“2003,
2016.
Watkins, C. J. C. H. Learning from delayed rewards. PhD
thesis, Kingâ€™s College, Cambridge, 1989.
Wei, E. and Luke, S. Lenient learning in independent-learner
stochastic cooperative games. The Journal of Machine
Learning Research, 17(1):2914â€“2955, 2016.
Wei, E., Wicke, D., Freelan, D., and Luke, S. Multiagent
soft q-learning. arXiv preprint arXiv:1804.09817, 2018.
Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang,
J. Mean field multi-agent reinforcement learning. In
Proceedings of ICML, pp. 5567â€“5576, 2018.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

Supplementary
A. QTRAN Training Algorithm
The training algorithms for QTRAN-base and QTRAN-alt are provided in Algorithm 1.
Algorithm 1 QTRAN-base and QTRAN-alt
1: Initialize replay memory D
2: Initialize [Qi ], Qjt , and Vjt with random parameters Î¸
3: Initialize target parameters Î¸ âˆ’ = Î¸
4: for episode = 1 to M do
5:
Observe initial state s0 and observation o0 = [O(s0 , i)]N
i=1 for each agent i
6:
for t = 1 to T do
7:
With probability  select a random action uti
8:
Otherwise uti = arg maxuti Qi (Ï„it , uti ) for each agent i
9:
Take action ut , and retrieve next observation and reward (ot+1 , rt )
10:
Store transition (Ï„ t , ut , rt , Ï„ t+1 ) in D
11:
Sample a random minibatch of transitions (Ï„ , u, r, Ï„ 0 ) from D
12:
Set y dqn (r, Ï„ 0 ; Î¸ âˆ’ ) = r + Î³Qjt (Ï„ 0 , uÌ„0 ; Î¸ âˆ’ ), uÌ„0 = [arg maxui Qi (Ï„i0 , ui ; Î¸ âˆ’ )]N
i=1 ,
13:
If QTRAN-base, update Î¸ by minimizing the loss:

L(Ï„ , u, r, Ï„ 0 ; Î¸) = Ltd + Î»opt Lopt + Î»nopt Lnopt ,
2
Ltd (Ï„ , u, r, Ï„ 0 ; Î¸) = Qjt (Ï„ , u) âˆ’ y dqn (r, Ï„ 0 ; Î¸ âˆ’ ) ,
2
Lopt (Ï„ , u, r, Ï„ 0 ; Î¸) = Q0jt (Ï„ , uÌ„) âˆ’ QÌ‚jt (Ï„ , uÌ„) + Vjt (Ï„ ) ,


2
Lnopt (Ï„ , u, r, Ï„ 0 ; Î¸) = min Q0jt (Ï„ , u) âˆ’ QÌ‚jt (Ï„ , u) + Vjt (Ï„ ), 0
.
14:

If QTRAN-alt, update Î¸ by minimizing the loss:
L(Ï„ , u, r, Ï„ 0 ; Î¸) = Ltd + Î»opt Lopt + Î»nopt-min Lnopt-min ,
2
Ltd (Ï„ , u, r, Ï„ 0 ; Î¸) = Qjt (Ï„ , u) âˆ’ y dqn (r, Ï„ 0 ; Î¸ âˆ’ ) ,
2
Lopt (Ï„ , u, r, Ï„ 0 ; Î¸) = Q0jt (Ï„ , uÌ„) âˆ’ QÌ‚jt (Ï„ , uÌ„) + Vjt (Ï„ ) ,
N 

 2
1 X
0
0
min Qjt (Ï„ , ui , uâˆ’i ) âˆ’ QÌ‚jt (Ï„ , ui , uâˆ’i ) + Vjt (Ï„ )
.
Lnopt-min (Ï„ , u, r, Ï„ ; Î¸) =
N i=1 ui âˆˆU

15:
Update target network parameters Î¸âˆ’ = Î¸ with period I
16:
end for
17: end for

B. Proofs
In this section, we provide the proofs of theorems and propositions coming from theorems.
B.1. Proof of Theorem 1
Theorem 1. A factorizable joint action-value function Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], if

PN

i=1 Qi (Ï„i , ui ) âˆ’ Qjt (Ï„ , u) + Vjt (Ï„ ) =



0

u = uÌ„,

(4a)

â‰¥0

u 6= uÌ„,

(4b)

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

where
Vjt (Ï„ ) = max Qjt (Ï„ , u) âˆ’
u

N
X

Qi (Ï„i , uÌ„i ).

i=1

Proof. Theorem 1 shows that if condition (4) holds, then Qi satisfies IGM. Thus, for some given Qi that satisfies (4), we
will show that arg maxu Qjt (Ï„ , u) = uÌ„. Recall that uÌ„i = arg maxui Qi (Ï„i , ui ) and uÌ„ = [uÌ„i ]N
i=1 ,.
Qjt (Ï„ , uÌ„) =

N
X

Qi (Ï„i , uÌ„i ) + Vjt (Ï„ )

(From (4a))

i=1

â‰¥

N
X

Qi (Ï„i , ui ) + Vjt (Ï„ )

i=1

â‰¥ Qjt (Ï„ , u)

(From (4b)).

It means that the set of optimal local actions uÌ„ maximizes Qjt , showing that Qi satisfies IGM. This completes the proof.
B.2. Necessity in Theorem 1 Under Affine-transformation
As mentioned in Section 3.1, the conditions (4) in Theroem 1 are necessary under an affine transformation. The necessary
condition shows that for some given factorizable Qjt , there exists Qi that satisfies (4), which guides us to design the QTRAN
Ã—N
neural network. Note that the affine transformation Ï† is Ï†(Q) = A Â· Q + B, where A = [aii ] âˆˆ RN
is a symmetric
+
diagonal matrix with aii > 0, âˆ€i and B = [bi ] âˆˆ RN . To abuse notation, let Ï†(Qi (Ï„i , ui )) = aii Qi (Ï„i , ui ) + bi .
Proposition 1. If Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], then there exists an affine transformation Ï†(Q) such that Qjt (Ï„ , u)
is factorized by [Ï†(Qi (Ï„i , ui ))] and the condition (4) holds by replacing [Qi (Ï„i , ui )] with [Ï†(Qi (Ï„i , ui ))].
Proof. To prove, we will show that, for the factors [Qi ] of Qjt , there exists an affine transformation of Qi that also satisfies
conditions (4).
By definition, if Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], then the followings hold: (i) Qjt (Ï„ , uÌ„) âˆ’ maxu Qjt (Ï„ , u) = 0,
PN
(ii) Qjt (Ï„ , u) âˆ’ Qjt (Ï„ , uÌ„) < 0, and (iii) i=1 (Qi (Ï„i , ui ) âˆ’ Qi (Ï„i , uÌ„i )) < 0 if u 6= uÌ„. Now, we consider an affine
transformation, in which aii = Î± and bi = 0 âˆ€i, where Î± > 0, and Ï†(Qi ) = Î±Qi with this transformation. Since this is
a linearly scaled transformation, it satisfies the IGM condition, and thus (4a) holds. We also prove that Ï†(Qi ) satisfies
condition (4a) by showing that there exists a constant Î± small enough such that
N
X

Î±Qi (Ï„i , ui ) âˆ’ Qjt (Ï„ , u) + Vjt (Ï„ , u) =

i=1

N
X

Î±(Qi (Ï„i , ui ) âˆ’ Qi (Ï„i , uÌ„i )) âˆ’ (Qjt (Ï„ , u) âˆ’ Qjt (Ï„ , uÌ„)) â‰¥ 0,

i=1

where Vjt (Ï„ ) is redefined for linearly scaled Î±Qi as maxu Qjt (Ï„ , u) âˆ’

PN

i=1 Î±Qi (Ï„i , uÌ„i ). This completes the proof.

B.3. Special Case: Theorem 1 in Fully Observable Environments
If the task is a fully observable case (observation function is bijective for all i), the state-value network is not required
and all Vjt (Ï„ ) values can be set to zero. We show that Theorem 1 holds equally for the case where Vjt (Ï„ ) = 0 for a fully
observable case. This fully observable case is applied to our example of the simple matrix game. The similar necessity
under an affine-transformation holds in this case.
Theorem 1a. (Fully observable case) A factorizable joint action-value function Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], if
N
X

Qi (Ï„i , ui )âˆ’Qjt (Ï„ , u) =


0

i=1

u = uÌ„,

(9a)

â‰¥ 0 u 6= uÌ„,

(9b)

Proof. We will show arg maxu Qjt (Ï„ , u) = uÌ„ (i.e., IGM), if the (9) holds.
Qjt (Ï„ , uÌ„) =

N
X
i=1

Qi (Ï„i , uÌ„i ) â‰¥

N
X
i=1

Qi (Ï„i , ui ) â‰¥ Qjt (Ï„ , u).

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

The first equality comes from (9a), and the last inequality comes from (9b). We note that arg maxu Qjt (Ï„ , u) =
[arg maxui Qi (Ï„i , ui )], so this completes the proof.
Proposition 1a. If Qjt (Ï„ , u) is factorized by [Qi (Ï„i , ui )], then there exists an affine transformation Ï†(Q), such that
Qjt (Ï„ , u) is factorized by [Ï†(Qi (Ï„i , ui ))] and condition (9) holds by replacing [Qi (Ï„i , ui )] with [Ï†(Qi (Ï„i , ui ))].
Proof. From Theorem 1a, if Qjt (Ï„ , u) is factorizable, then there exist [Qi ] satisfying both IGM and (9). Now, we define
an additive transformation Ï†0i (Qi (Ï„i , ui )) = Qi (Ï„i , ui ) + N1 maxu Qjt (Ï„ , u) âˆ’ Qi (Ï„i , uÌ„i ) for a given Ï„i , which is uniquely
defined for fully observable cases. [Ï†0i (Qi (Ï„i , ui ))] also satisfy IGM, and the left-hand side of (9) can be rewritten as:
N
X

Qi (Ï„i , ui ) âˆ’ Qjt (Ï„ , u) âˆ’

i=1

N
X
i=1

Qi (Ï„i , uÌ„i ) + max Qjt (Ï„ , u) =
u

N
X

Ï†0i (Qi (Ï„i , ui )) âˆ’ Qjt (Ï„ , u)

i=1

So there exist individual action-value functions [Ï†0i (Q0i (Ï„i , ui ))] that satisfy both IGM and (9), where Vjt (Ï„ ) is redefined as
0. This completes the proof of the necessity.

B.4. Proof of Theorem 2
Theorem 2. The statement presented in Theorem 1 and the necessary condition of Theorem 1 holds by replacing (4b) with
the following (7): if u 6= uÌ„,
h
i
min Q0jt (Ï„ , ui , uâˆ’i ) âˆ’ Qjt (Ï„ , ui , uâˆ’i ) + Vjt (Ï„ ) = 0,

ui âˆˆU

âˆ€i = 1, . . . , N,

(7)

where uâˆ’i = (u1 , . . . , uiâˆ’1 , ui+1 , . . . , uN ), i.e., the action vector except for iâ€™s action.
Proof. (â‡’) Recall that condition (7) is stronger than (4b), which is itself sufficient for Theorem 1. Therefore, by transitivity,
condition (7) is sufficient for Theorem 2. Following paragraphs focus on the other direction, i.e., how condition (7) is
necessary for Theorem 2.
(â‡) We prove that, if there exist individual action-value functions satisfying condition (4), then there exists an individual
action-value function Q0i that satisfies (7). In order to show the existence of such Q0i , we propose a way to construct Q0i .
We first consider the case with N = 2 and then generalize the result for any N . The condition (7) for N = 2 is denoted as:
h
i
min Q1 (Ï„1 , u1 ) + Q2 (Ï„2 , u2 ) âˆ’ Qjt (Ï„ , u1 , u2 ) + Vjt (Ï„ ) = 0.

ui âˆˆU

Since this way of constructing Q0i is symmetric for
h all i, we present its construction only for u1 withouti loss of generality.
For Q1 and Q2 satisfying (4), if Î² := minu1 âˆˆU Q1 (Ï„1 , u1 ) + Q2 (Ï„2 , u2 ) âˆ’ Qjt (Ï„ , u1 , u2 ) + Vjt (Ï„ ) > 0 for given Ï„

and u2 , then u2 6= uÌ„2 . This is because Q1 (Ï„1 , uÌ„1 ) + Q2 (Ï„2 , uÌ„2 ) âˆ’ Qjt (Ï„ , uÌ„1 , uÌ„2 ) + Vjt (Ï„ ) = 0 by condition (4a). Now,
we replace Q2 (Ï„2 , u2 ) with Q02 (Ï„2 , u2 ) = Q2 (Ï„2 , u2 ) âˆ’ Î². Since Q2 (Ï„2 , uÌ„2 ) > Q2 (Ï„2 , u2 ) > Q2 (Ï„2 , u2 ) âˆ’ Î², it does not
change the optimal action and other conditions. Then, (7) is satisfied for given Ï„ and u2 . By repeating this replacement
process, we can construct Q0i that satisfies condition (7).
h
i
More generally, when N 6= 2, if minui âˆˆU Q0jt (Ï„ , ui , uâˆ’i ) âˆ’ Qjt (Ï„ , ui , uâˆ’i ) + Vjt (Ï„ ) = Î² > 0 for given Ï„ and uâˆ’i ,
then there exists some j 6= i that satisfies uj 6= uÌ„j . Therefore, by repeating the same process as when N = 2 through j,
we can construct Q0i for all i, and this confirms that individual action-value functions satisfying condition (7) exist. This
completes the proof.

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

B.4.1. P ROCESS OF CONSTRUCTING Q0i IN THE MATRIX GAME USING T HEOREM 2
We now present the process of how we have demonstrated through the example in Section 3.5. In the original matrix shown
in Tables 2a and 2d, the second row does not satisfy the condition (7), and Î² = 0.23 for u1 = B. Then, we replace Q1 (B)
as shown in Table 2e. Table 2b shows that its third row does not satisfy the condition (7). Finally, we replace Q1 (C) as
shown in Table 2f. Then, the resulting Table 2c satisfies the condition (7).
u2
u1

A

B

C

u2
u1

A

B

C

u2
u1

A

B

C

A
B
C

0.00
14.11
13.93

18.14
0.23
0.05

18.14
0.23
0.05

A
B
C

0.00
13.88
13.93

18.14
0.00
0.05

18.14
0.00
0.05

A
B
C

0.00
13.88
13.88

18.14
0.00
0.00

18.14
0.00
0.00

(a) Q0jt âˆ’ Qjt

(b) Q0jt âˆ’ Qjt after one replacement

(c) Q0jt âˆ’ Qjt after two replacements

Q2
Q1

4.16(A)

2.29(B)

2.29(C)

Q2
Q1

4.16(A)

2.29(B)

2.29(C)

Q2
Q1

4.16(A)

2.29(B)

2.29(C)

3.84(A)
-2.06(B)
-2.25(C)

8.00
2.10
1.92

6.13
0.23
0.04

6.12
0.23
0.04

3.84(A)
-2.29(B)
-2.25(C)

8.00
1.87
1.92

6.13
0.00
0.04

6.12
0.00
0.04

3.84(A)
-2.29(B)
-2.30(C)

8.00
1.87
1.87

6.13
0.00
-0.01

6.12
0.00
-0.01

(d) Q1 , Q2 , Q0jt

(e) Q1 , Q2 , Q0jt after one replacement

(f) Q1 , Q2 , Q0jt after two replacements

Table 2. The process of replacing [Qi ] satisfying the condition (9b) with [Qi ] satisfying the condition (7)

C. Details of environments and implementation
C.1. Environment
Matrix game In order to see the impact of QTRAN-alt, we train the agents in a single state matrix game where two agents
each have 21 actions. Each agent i takes action ui , ranging over âˆˆ {0, ..., 20}. The reward value R for a joint action is given
as follows:
2 
2
15 âˆ’ u1
5 âˆ’ u2
âˆ’
3
3
2 
2

15 âˆ’ u2
5 âˆ’ u1
âˆ’
f2 (u1 , u2 ) = 10 âˆ’
1
1


f1 (u1 , u2 ) = 5 âˆ’

R(u1 , u2 ) = max(f1 (u1 , u2 ), f2 (u1 , u2 ))
10.0

20

7.5

15

5.0
2.5

10

0.0
2.5

5

5.0
7.5

0

0

5

10

15

20

10.0

Figure 5. x-axis and y-axis: agents 1 and 2â€™s actions, respectively. Colored values represent the reward for selected actions.

Figure 5 shows reward for selected action. Colored values represent the values. In the simple matrix game, the reward
function has a global maximum point at (u1 , u2 ) = (5, 15) and a local maximum point at (u1 , u2 ) = (15, 5).

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

Multi-domain Gaussian Squeeze We adopt and modify Gaussian Squeeze, where agent numbers (N = 10) and action
spaces (ui âˆˆ {0, 1, ..., 9}) are much larger than a simple matrix game. In MGS, each of the ten agents has its own amount
of unit-level resource si âˆˆ [0, 0.2] given by the environment a priori. This task covers a fully observable case where all
agents can see the entire state. We assume that there exist K domains,
where the above resource allocation takes place. The
P
joint action u determines the overall resource usage x(u) = i si Ã— ui . Reward is given as a function of resource usage
PK
2
2
as follows: G(u) = k=1 xeâˆ’(xâˆ’Âµk ) /Ïƒk . We test with two different settings: (i) K = 1, Âµ1 = 8, Ïƒ1 = 1 as shown in
Figure 6a, and (ii) K = 2, Âµ1 = 8, Ïƒ1 = 0.5, Âµ2 = 5, Ïƒ2 = 1 as shown in Figure 6b. In the second setting, there are two
local maxima for resource x. The maximum on the left is relatively easy to find through exploration â€“ as manifested in the
greater variance of the Gaussian distribution, but the maximum reward â€” as represented by the lower peak â€” is relatively
low. On the other hand, the maximum on the right is more difficult to find through exploration, but it offers higher reward.

9
8
7
6
5
4
3
2
1
00

Score

Score

9
8
7
6
5
4
3
2
1
00

2

4

6

8

Resource

10

(a) Gaussian Squeeze

2

4

6

Resource

8

10

(b) Multi-domain Gaussian Squeeze

Figure 6. Gaussian Squeeze and Multi-domain Gaussian Squeeze

Modified predator-prey Predator-prey involves a grid world, in which multiple predators attempt to capture randomly
moving prey. Agents have a 5 Ã— 5 view and select one of five actions âˆˆ {Left, Right, Up, Down, Stop} at each time step.
Prey move according to selecting a uniformly random action at each time step. We define the â€œcatchingâ€ of a prey as when
the prey is within the cardinal direction of at least one predator. Each agentâ€™s observation includes its own coordinates, agent
ID, and the coordinates of the prey relative to itself, if observed. The agents can separate roles even if the parameters of the
neural networks are shared by agent ID. We test with two different grid worlds: (i) a 5 Ã— 5 grid world with two predators
and one prey, and (ii) a 7 Ã— 7 grid world with four predators and two prey. We modify the general predator-prey, such that a
positive reward is given only if multiple predators catch a prey simultaneously, requiring a higher degree of cooperation. The
predators get a team reward of 1 if two or more catch a prey at the same time, but they are given negative reward âˆ’P , if only
one predator catches the prey as shown in Figure 7. We experimented with three varying P vales, where P = 0.5, 1.0, 1.5.
The terminating condition of this task is when a prey is caught with more than one predator. The prey that has been caught is
regenerated at random positions whenever the task terminates, and the game proceeds over fixed 100 steps.

P

A

P

A

A

P : Prey
A

Reward = +1

A : Agent (Predator)

Reward = -P
Figure 7. Predator-prey environment

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

C.2. Experiment details
Matrix game Table 3 shows the values of the hyperparameters for the training in the matrix game environment. Individual
action-value networks, which are common in all VDN, QMIX, and QTRAN, each consist of two hidden layers. In addition
to the individual Q-networks, QMIX incorporates a monotone network with one hidden layer, and the weights and biases
of this network are produced by separate hypernetworks. QTRAN has an additional joint action-value network with two
hidden layers. All hidden layer widths are 32, and all activation functions are ReLU. All neural networks are trained using
the Adam optimizer. We use -greedy action selection with  = 1 independently for exploration.
Hyperparameter

Value

Description

training step
learning rate
replay buffer size
minibatch size
Î»opt , Î»nopt

20000
0.0005
20000
32
1,1

Maximum time steps until the end of training
Learning rate used by Adam optimizer
Maximum number of samples to store in memory
Number of samples to use for each update
Weight constants for loss functions Lopt , Lnopt and Lnoptâˆ’min

Table 3. Hyperparameters for matrix game training

Multi-domain Gaussian Squeeze Table 4 shows the values of the hyperparameters for the training in the MGS environment. Each individual action-value network consists of three hidden layers. In addition to the individual Q-networks, QMIX
incorporates a monotone network with one hidden layer, and the weights and biases of this network are produced by separate
hypernetworks. QTRAN has an additional joint action-value network with two hidden layers. All hidden layer widths are
64, and all activation functions are ReLU. All neural networks are trained using the Adam optimizer. We use -greedy action
selection with decreasing  from 1 to 0.1 for exploration.
Hyperparameter

Value

Description

training step
learning rate
replay buffer size
final exploration step
minibatch size
Î»opt , Î»nopt

1000000
0.0005
200000
500000
32
1,1

Maximum time steps until the end of training
Learning rate used by Adam optimizer
Maximum number of samples to store in memory
Number of steps over which  is annealed to the final value
Number of samples to use for each update
Weight constants for loss functions Lopt , Lnopt and Lnoptâˆ’min

Table 4. Hyperparameters for Multi-domain Gaussian Squeeze

Modified predator-prey Table 5 shows the values of the hyperparameters for the training in the modified predator-prey
environment. Each individual action-value network consists of three hidden layers. In addition to the individual Q-networks,
QMIX incorporates a monotone network with one hidden layer, and the weights and biases of this network are produced
by separate hypernetworks. QTRAN has additional joint action-value network with two hidden layers. All hidden layer
widths are 64, and all activation functions are ReLU. All neural networks are trained using the Adam optimizer. We use
-greedy action selection with decreasing  from 1 to 0.1 for exploration. Since history is not very important in this task, our
experiments use feed-forward policies, but our method is also applicable with recurrent policies.
Hyperparameter

Value

Description

training step
discount factor
learning rate
target update period
replay buffer size
final exploration step
minibatch size
Î»opt , Î»nopt

10000000
0.99
0.0005
10000
1000000
3000000
32
1,1

Maximum time steps until the end of training
Importance of future rewards
Learning rate used by Adam optimizer
Target network update period to track learned network
Maximum number of samples to store in memory
Number of steps over which  is annealed to the final value
Number of samples to use for each update
Weight constants for loss functions Lopt , Lnopt and Lnoptâˆ’min

Table 5. Hyperparameters for predator-prey training

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

D. Additional results for matrix games
Table 6 shows the comparison between the final performance levels of VDN, QMIX, and QTRAN-base for 310 3 Ã— 3
random matrix games, where each value of the payoff matrix is given as a random parameter from 0 to 1. Experimental
settings are the same as in the previous matrix game. Since matrix games always satisfy IGM conditions, QTRAN-base
always trains the optimal action for all 310 cases. On the other hand, VDN and QMIX were shown to be unable to learn an
optimal policy for more than half of non-monotonic matrix games.
We briefly analyze the nature of the structural constraints assumed by VDN and QMIX, namely, additivity and monotonicity
of the joint action-value functions. There have been only 19 cases in which the results of VDN and QMIX differ from each
other. Interestingly, for a total of five cases, the performance of VDN is better than QMIX. QMIX was shown to outperform
VDN in more cases (14) than the converse case (5). This supports the idea that the additivity assumption imposed by VDN
on the joint action-value functions is indeed stronger than the monotonicity assumption imposed by QMIX.
QTRAN=VDN=QMIX
114

QTRAN>VDN=QMIX
177

VDN>QMIX
5

QMIX>VDN
14

Table 6. Final performance comparison with 310 random matrices

Figures 8-9 show the joint action-value function of VDN and QMIX, and Figures 10-11 show the transformed joint
action-value function of QTRAN-base and QTRAN-alt for a matrix game where two agents each have 20 actions. In the
result, VDN and QMIX can not recover joint action-value, and these algorithms learn sub-optimal policy u1 , u2 = (15, 5).
In the other hand, the result shows that QTRAN-base and QTRAN-alt successfully learn the optimal action, but QTRAN-alt
has the ability to more accurately distinguish action from non-optimal action as shown in Figure 11.
20

20

20

20

20

20

20

20

15

15

15

15

15

15

15

15

10

10

10

10

10

10

10

10

5

5

5

5

5

5

5

0

0
0

5

10

15

20

0
0

(a) 1000 step

5

10

15

20

0
0

(b) 2000 step

5

10

15

20

0
0

(c) 3000 step

5

10

15

20

0
0

(d) 4000 step

5

10

15

20

5

0
0

(e) 5000 step

5

10

15

20

0
0

(f) 6000 step

5

10

15

20

0

(g) 7000 step

5

10

15

20

(h) 8000 step

Figure 8. x-axis and y-axis: agents 1 and 2â€™s actions. Colored values represent the values of Qjt for VDN
20

20

20

20

20

20

20

20

15

15

15

15

15

15

15

15

10

10

10

10

10

10

10

10

5

5

5

5

5

5

5

0

0
0

5

10

15

20

0
0

(a) 1000 step

5

10

15

20

0
0

(b) 2000 step

5

10

15

20

0
0

(c) 3000 step

5

10

15

20

0
0

(d) 4000 step

5

10

15

20

5

0
0

(e) 5000 step

5

10

15

20

0
0

(f) 6000 step

5

10

15

20

0

(g) 7000 step

5

10

15

20

(h) 8000 step

Figure 9. x-axis and y-axis: agents 1 and 2â€™s actions. Colored values represent the values of Qjt for QMIX
20

20

20

20

20

20

20

20

15

15

15

15

15

15

15

15

10

10

10

10

10

10

10

10

5

5

5

5

5

5

5

0

0
0

5

10

15

20

0
0

(a) 1000 step

5

10

15

20

0
0

(b) 2000 step

5

10

15

20

0
0

(c) 3000 step

5

10

15

20

0
0

(d) 4000 step

5

10

15

20

5

0
0

(e) 5000 step

5

10

15

20

0
0

(f) 6000 step

5

10

15

20

0

(g) 7000 step

5

10

15

20

(h) 8000 step

Figure 10. x-axis and y-axis: agents 1 and 2â€™s actions. Colored values represent the values of Q0jt for QTRAN-base
20

20

20

20

20

20

20

20

15

15

15

15

15

15

15

15

10

10

10

10

10

10

10

10

5

5

5

5

5

5

5

0

0
0

5

10

15

(a) 1000 step

20

0
0

5

10

15

(b) 2000 step

20

0
0

5

10

15

(c) 3000 step

20

0
0

5

10

15

(d) 4000 step

20

0
0

5

10

15

(e) 5000 step

20

5

0
0

5

10

15

(f) 6000 step

20

0
0

5

10

15

(g) 7000 step

20

0

5

10

15

(h) 8000 step

Figure 11. x-axis and y-axis: agents 1 and 2â€™s actions. Colored values represent the values of Q0jt for QTRAN-alt

20

30

30

15

15

15

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
VDN

0
âˆ’15

0

1.5Ã—106
Training step

0
âˆ’15

3.0Ã—106

0

1.5Ã—10
Training step

0
âˆ’15

3.0Ã—106

30

0

0

0

âˆ’60

0

3Ã—106
Training step

(d) N = 4, P = 0.5

6Ã—106

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
VDN

âˆ’30
âˆ’60

0

5Ã—106
Training step

(e) N = 4, P = 1.0

107

Score

30

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
VDN

1.5Ã—106
Training step

0

3.0Ã—106

(c) N = 2, P = 1.5

30

âˆ’30

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
VDN

(b) N = 2, P = 1.0

Score

Score

(a) N = 2, P = 0.5

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
6 VDN

Score

30

Score

Score

QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning

QTRAN-alt
QTRAN-base
Dec-HDRQN
QMIX
VDN

âˆ’30
âˆ’60

5Ã—106
Training step

0

107

(f) N = 4, P = 1.5

Figure 12. Average reward per episode on the MPP tasks with 95% confidence intervals for VDN, QMIX, Dec-HDRQN and QTRAN

E. Comparison with other value-based methods for modified predator-prey
Additionally, we have conducted experiments with Dec-HDRQN (Omidshafiei et al., 2017). Dec-HDRQN can indeed solve
problems similar to ours by changing the learning rate according to TD-error without factorization. However, Dec-HDRQN
does not take advantage of centralized training. We implemented Dec-HDRQN (Î± = 0.001, Î² = 0.0002) with modified
predator-prey experiments and Figure 12 shows the performance of algorithms for six settings with different N and P
values.
First, when the complexity of the task is relatively low, Dec-HDRQN shows better performance than VDN and QMIX as
shown in the Figure 12b. However, QTRAN performs better than Dec-HDRQN in the case. When the penalty and the
number of agents are larger, Dec-HDRQN underperforms VDN and QMIX. Figure 12c shows Dec-HDRQN scores an
average of nearly 0 in the case of N = 2, P = 1.5. There is a limit of Dec-HDRQN since the method is heuristic and does
not perform centralized training. Finally, Dec-HDRQN showed slower convergence speed overall.

