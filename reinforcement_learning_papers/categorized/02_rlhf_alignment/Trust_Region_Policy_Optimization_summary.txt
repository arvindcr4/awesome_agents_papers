Here is the extracted information:

**PAPER:** Trust Region Policy Optimization
**TITLE:** Trust Region Policy Optimization
**RESEARCH_METHOD:** 02_rlhf_alignment
**METHOD_DESCRIPTION:** Trust Region Policy Optimization (TRPO) is a model-free, on-policy reinforcement learning algorithm that optimizes the policy using a trust region method. The algorithm iteratively updates the policy by maximizing a surrogate objective function, which is a lower bound on the expected return, subject to a constraint on the KL divergence between the old and new policies.
**KEY_CONTRIBUTIONS:**
* TRPO is a scalable and efficient algorithm for optimizing large, nonlinear policies, such as neural networks.
* The algorithm has a strong theoretical foundation, with guaranteed monotonic improvement in the policy's performance.
* TRPO can be applied to a wide range of tasks, including robotic locomotion and playing Atari games from images.
* The algorithm outperforms prior methods, such as natural policy gradient and cross-entropy method, on several tasks.
* TRPO can be used to learn complex behaviors, such as swimming, walking, and hopping, in a physics simulator.
* The algorithm can be applied to partially observed tasks, such as playing Atari games from images, and achieves competitive results.
