arXiv:2303.08909v2 [cs.LG] 5 Oct 2024

Latent-Conditioned Policy Gradient for Multi-Objective Deep
Reinforcement Learning
Takuya Kanazawa and Chetan Gupta
Industrial AI Lab, Hitachi America, Ltd. R&D, Santa Clara, CA 95054, USA.

*Corresponding author(s). E-mail(s): takuya.kanazawa.cz@hitachi.com;
Abstract
Sequential decision making in the real world often requires finding a good balance of conflicting objectives.
In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises
between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks.
In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single
neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete
action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.
Keywords: Deep reinforcement learning, multi-objective optimization, Pareto frontier, policy gradient theorem, Markov
decision processes, implicit generative network

1 Introduction

Learning (MORL) [5â€“7]. Such an extension is motivated by the fact that many real-world control problems require balancing multiple conflicting objectives;
an example is a control of a walking robot where
one may wish to maximize the walking speed while
minimizing the electricity consumption. The existence of a trade-off between objectives naturally leads
to an ensemble of best solutions (known as Paretooptimal policies), each of which is characterized by
an inequivalent compromise of objectives. The goal of
MORL is to obtain a set of policies that approximates
such an ensemble as closely as possible.
In MORL problems, the set of true Pareto-optimal
policies often consists of many (even infinite) diverse
and disparate solutions, and poses computational as
well as conceptual challenges that hamper a naÃ¯ve
application of conventional RL techniques. One of
the popular approaches to tackling this difficulty is to
convert the original MORL problem into a series of

In recent years Reinforcement Learning (RL) has
grown into a major part of the machine learning
research. RL offers a powerful and principled framework to solve sequential decision making problems
under uncertainty [1]. In RL, an agent takes actions
sequentially in an environment and receives rewards.
The goal of RL is to find an optimal policy that maximizes cumulative rewards over multiple time steps.
Rapid progress in the field of deep neural networks
(NN) has enabled deep RL to solve a number of complex nonlinear control tasks in the field of games,
robotics, autonomous driving, traffic signal control,
recommender systems, finance, etc. at a level comparable to or even beyond human experts [2â€“4].
While standard RL assumes scalar rewards, RL
can be extended to incorporate vector-valued rewards,
which is known as Multi-Objective Reinforcement

1

Collect trajectories and returns
by running in the environment

Score trajectories

Env

Policy
network
State

Latent
variable

Env
Objective 2

Action

Env

Objective 1

Env

ð’„âˆ¼ð‘ƒ ð’„

Update policy parameter
Fig. 1 Illustration of the proposed algorithm (LC-MOPG) for a bi-objective problem. There is an alternative that uses value networks in
addition to the policy network (LC-MOPG-V).

single-objective RL problems and solve them by training an ensemble of agents [8]. This method is simple
but suffers from a high computational cost. Another
popular method is to train a single agent that receives
a preference (linear weight vector) over objectives as
additional input to the value function [9, 10]. This
approach is computationally efficient but linear scalarization hampers finding the concave part of the Pareto
frontier (PF) [11, 12].
In this work, we propose a novel model-free onpolicy MORL algorithm that obtains infinitely many
inequivalent policies by training just a single policy
network. In this approach, as illustrated in Fig. 1,
a stochastic policy receives a random latent variable
sampled from a fixed external probability distribution
as additional input. We train this latent-conditioned
NN in a policy-gradient manner, so that the quality
of the set of returns generated by policies conditioned
on different random latent variables gets improved
successively. We introduce a novel exploration bonus
that helps to substantially enhance diversity of the
policy ensemble. The proposed method, coined as
Latent-Conditioned Multi-Objective Policy Gradient
(LC-MOPG), is applicable to both continuous and
discrete action spaces, and can in principle discover
the whole PF without any convexity assumptions, as
LC-MOPG does not rely on linear scalarization of
objectives. We confirm the effectiveness of LC-MOPG
in various benchmark tests through comparison with
exact PF and standard baselines for MORL.
The rest of this paper is organized as follows. In
Sec. 2 related work in MORL is reviewed. In Sec. 3

the preliminaries are summarized and the problem to
be solved is formally stated. In Sec. 4 our new method
is introduced and explained in great detail. In Sec. 5
the features of each environment used for benchmarking are described. In Sec. 6 the results of numerical
experiments are discussed and the proposed method is
compared with baselines. In Sec. 7 we conclude.

2 Related work
2.1 Scalarization-based MORL
There are a number of MORL methods that employ
some parametric scalarization function to combine
multiple objectives into a single objective, and then
optimize it while varying the parameters of the scalarization function so as to cover the whole PF as widely
as possible. For example, Van Moffaert et al. [13] proposed the use of the Chebyshev scalarization function.
This method has the advantage that it can access the
entire PF without prior assumptions on its shape, but
the nonlinearity of the Chebyshev function makes it
impossible to derive the Bellman optimality equation.
Linear scalarization (i.e., taking a weighted sum of
objectives as the target of optimization) is an alternative approach, which is more popular due mainly to
the fact that the Bellman equation for a weighted sum
of multi-objective returns can be derived straightforwardly and readily integrated with common methodologies developed for single-objective RL. For example, Castelletti et al. [9, 10] adapted fitted Q iteration
to the multi-objective domain by postulating a generalized scalar Q function which receives not only a state
2

and action pair but also a linear weight (preference)
vector as input. While Castelletti et al. [9, 10] used
decision tree-based models to parametrize the Q function, most of more recent studies (reviewed below)
use deep NN as a universal function approximator.
Mossalam et al. [8] introduced a NN architecture with
vector output that parameterizes the multi-objective
return. Training of multiple Q networks associated
with different weights is done sequentially, reusing
the parameters of the already trained NN to accelerate
training of a new NN. Abels et al. [14] investigated the
MORL setup in which a linear weight in the scalarized objective changes dynamically during an episode.
Abels et al. combined the methods of Castelletti et
al. [9, 10] and Mossalam et al. [8] by proposing to
train a single vector-valued NN that can learn the
multi-objective Q function for the entire weight space
efficiently. Yang et al. [15] further improved this by
introducing a new Bellman update scheme, called
envelope Q-learning, which optimizes the Q value
not only over actions but also over weights, leading
to more optimistic Q-updates. Recently, Basaklar et
al. [16] introduced a Q-learning scheme that contains
a cosine similarity term between projected weight
vectors and Q values. This method requires solving
a series of single-objective RL problems for fixed
key weights separately, before training the agent for
general weights.
While the methods outlined above are valuebased, there are also policy-based approaches for
MORL that combine policy gradient [17] with linear scalarization. For example, Chen et al. [18] proposed to train a meta-policy that can approximately
solve an ensemble of tasks characterized by different
preferences of objectives. The meta-policy is trained
with policy gradient, in which the advantage function is computed as a weighted sum of advantage
functions for individual objectives. The learnt metapolicy is fine-tuned for each task separately. Xu et
al. [19] proposed an evolutionary algorithm in which
a population of policies are trained in parallel using
various weighted sums of objectives. At each generation, the combination of a policy and a weight that
best improves the PF is selected via a greedy optimization algorithm. Each policy is then trained via
multi-objective policy gradient, analogously to Chen
et al. [18]. Siddique et al. [20] also introduced a similar
policy gradient approach.
These policy-based approaches [18â€“20] require
training an ensemble of policy networks to encompass the PF, so they are prone to relatively high

computational costs and implementational complexity. In this regard, we believe that our work is the
first policy-based approach for MORL that efficiently
learns multiple policies by training a single NN.

2.2 MORL without explicit scalarization
Miscellaneous MORL approaches that do not rest
on scalarization techniques have been developed.
Van Moffaert and NowÃ© [21] developed Pareto Qlearning, which searches for Pareto-optimal policies
using set-based temporal-different learning. However
its application so far has been limited to small-scale
problems [22]. Lexicographic RL [23â€“25] uses lexicographic ordering of objectives, which is useful when
the priority over objectives is clearly known. Reymond et al. [26] extended the framework of rewardconditioned policies [27â€“29] to MORL, in which the
PF is learnt in a supervised learning style. Abdolmaleki et al. [30] proposed to learn an action distribution for each objective based on a scale-invariant
encoding of preferences across objectives, extending
the prior work on maximum a posteriori policy optimization [31] to the multi-objective domain. Parisi et
al. [32] developed two policy gradient methods, called
Radial Algorithm (RA) and Pareto-Following Algorithm (PFA). RA assigns a set of weights to policies
and optimizes each policy separately via policy gradient. PFA first performs a single-objective optimization
and then updates the resulting policy incrementally to
sweep the PF. PFA is not applicable to problems with
more than two objectives.
Pirotta et al. [33, 34] proposed a manifold-based
policy search algorithm in which a policy is sampled
from a continuous manifold in the policy parameter
space. This manifold is specified by a parametric map
from a predefined base space. The parameters of this
map are updated in a gradient-descent fashion, based
on a differentiable loss (or indicator) function that
measures the quality of the PF. To define the loss,
we have to specify several hyperparameters including
coordinates of the utopia and anti-utopia points in the
objective space, which requires some domain knowledge of the problem. The resulting performance was
observed to be highly sensitive to the choice of these
hyperparameters and calls for careful manual tuning,
which limits the utility of this approach.
Parisi et al. proposed a generalization of natural evolution strategies (NES) [35] to MORL called
MO-NES [36]. Analogously to [33, 34], policies
3

{ci }N
i=1 to IGN, hence the name â€œgenerativeâ€. IGN
has been employed for uncertainty quantification of
deep learning models and has shown competitive performance [40â€“42]. In RL, IGN is often used as an
architecture for value networks that can capture the
probabilistic distribution of expected future returns
[43]. Tessler et al. [44] proposed a generalized policy gradient algorithm that adopts IGN to express a
stochastic policy over a continuous action space in a
nonparametric manner. Our work is similar to [44]
in that both employ IGN for the policy network. The
main difference, however, is that [44] uses IGN to represent a single stochastic policy that samples a new
latent variable c at each time step, whereas our work
adopts IGN to represent a population of (deterministic) policies each of which samples c at the start of an
episode and uses it until the episode ends.
Among the challenges faced by RL are the issues
of sample complexity and brittleness against task variations. A promising approach to overcoming them is
unsupervised skill discovery. Eysenbach et al. [45]
showed that diverse behaviors (e..g, walking, jumping,
and hopping of a bipedal robot) can be learnt without
external rewards. Specifically, they used IGN for the
policy and introduced an intrinsic reward that incentivises to maximize the mutual information between a
probabilistic latent variable of IGN and states induced
by a latent-conditioned policy. A large body of work
has been done to improve this approach [46â€“51]. It is
noteworthy that the adoption of IGN has made training of a diverse set of policies significantly faster and
more efficient than the preceding RL-based approach
to skill discovery by Kume et al. [52], which required
training hundreds to thousands of NN independently.
Neuroevolution is a competitive alternative to RL
for diverse skill discovery [53, 54]. Unlike deep
RL, neuroevolution applies evolutionary algorithms
to update policies. Recent works [55, 56] combined
neuroevolution (specifically, Quality-Diversity methods) with policy-gradient updates for better sample
efficiency.

are sampled from a continuous parametric distribution in the policy parameter space. The parameters
of this distribution are updated via NES by using
the Fisher Information Matrix (FIM). MO-NES can
employ a non-differentiable indicator function such
as the hypervolume metric [37]. However, a downside of MO-NES is that the number of parameters to
be optimized increases rapidly with the complexity
of the policy. For instance, in order to learn a distribution over policies each having 30 parameters, one
has to optimize 7448 parameters in the distribution
[36]. Another downside is that, to make the computation of FIM feasible, the form of the distribution must
be simple enough (e.g., Gaussian), which limits the
flexibility of this approach.
Our algorithm, LC-MOPG, is similar to [33, 34,
36] in that a continuous PF (i.e., infinitely many policies) can be learnt in a single run of training. There are,
however, some important advantages for LC-MOPG.
First, unlike [33, 34, 36], LC-MOPG is a nonparametric approach, implying that we need not manually
design the parametric form of the distribution over
policies. Secondly, LC-MOPG does not require specifying reference points (utopia and anti-utopia) in the
objective space, obviating the need for domain knowledge of the desired solutions. Thirdly, LC-MOPG
makes it possible to train deep policies with thousands
of parameters efficiently over a high-dimensional continuous state space.

2.3 Implicit generative networks
The concept of universal value function approximators (UVFA) was introduced by Schaul et al. [38].
UVFA postulate a single NN for a value function that
generalizes not only over states but also over goals.
As UVFA enable efficient transfer of knowledge and
skills from an already acquired goal to a new goal,
UVFA serve as a powerful scheme to tackle generic
multi-goal RL problems [39].
In a broader context, NN that receive a latent
variable as additional input are generally called
latent-conditioned NN or implicit generative networks
(IGN). In the context of UVFA, the latent variable represents a goal. Unlike plain NN of the form y = fÎ¸ (x)
with x the input features and y the label, IGN is of the
form y = fÎ¸ (x, c) where the latent variable c is typically sampled from a fixed (discrete or continuous)
probability distribution such as N (0, 1). For a given
x, one can generate arbitrarily many distinct output
{yi }N
i=1 by feeding randomly sampled latent variables

3 Problem formulation
3.1 Markov decision processes
We consider sequential decision making problems that
are formally described as a Markov Decision Process
(MDP): âŸ¨S, A, P, r, Î³, Ï0 âŸ© where S and A are the state
and action spaces, P : S Ã— A Ã— S â†’ [0, âˆž) is the
transition probability (density), r : S Ã— A Ã— S â†’ R is
4

S Ã—AÃ—S â†’ Rm becomes vector-valued, with m â‰¥ 2
the number of objectives. The discount factor Î³ can
also be generalized to m components, but for the sake
of simplicity, we will limit ourselves to a common Î³
for all objectives throughout this paper.
In MOMDP, a policy Ï€ induces a vector of
expected returns

the reward function, Î³ âˆˆ [0, 1] is the discount factor,
and Ï0 is the initial state distribution.
A policy Ï€ specifies which action is taken in a
given state. A deterministic policy Ï€ : S â†’ A yields
action as a function of state, while a stochastic policy
Ï€ : S Ã— A â†’ [0, âˆž) gives a probabilistic distribution over A as a function of state. The value function
vÏ€ : S â†’ R of a policy Ï€ is formally defined as
vÏ€ (s) = EÏ€

"âˆž
X

#
k

Î³ rt+k st = s .

(1)

GÏ€ â‰¡ (GÏ€1 , GÏ€2 , Â· Â· Â· , GÏ€m )
(4)
"âˆž
#
X
=E
Î³ t r(st , at , st+1 ) at âˆ¼ Ï€(st ) âˆˆ Rm .

k=0

The
cumulative sum of rewards Gt =
Pâˆž (discounted)
k
Î³
r
is
called
return. The goal of RL metht+k
k=0
ods is to (approximately) find the optimal policy Ï€âˆ—
that maximizes vÏ€ (s) for all s âˆˆ S [1].

t=0

(5)

3.2 Policy gradient

A policy Ï€ is said to dominate another policy Ï€ â€² if
â€²
â€²
GÏ€i â‰¥ GÏ€i for all i and GÏ€i > GÏ€i for at least
one i. (With a slight abuse of terminology, we also
â€²
say that GÏ€ dominates GÏ€ , and express this relation
â€²
as GÏ€ â‰» GÏ€ .) If there exists no policy that dominates Ï€ , then Ï€ is said to be Pareto-optimal. The set
of all such policies is called the Pareto set, and the
set of expected returns of all Pareto-optimal policies is
called the Pareto frontier (PF), which is a submanifold
of Rm . The goal of MORL is to find a set of policies
that best approximates the true Pareto set.
In MOMDP, there is a possibility that the Pareto
set is composed of stochastic policies [5]. Nevertheless, in real-world deployment of trained RL agents,
deterministic policies are often preferred to stochastic
policies because the latter are intrinsically unpredictable and quality assurance becomes nontrivial.
In this paper, we will use stochastic policies in the
training phase to facilitate exploration, and switch to
deterministic policies in the evaluation phase.
There are several performance metrics proposed
in the literature to assess the quality of an approximate PF [62, 63]. Some metrics require knowing the
true PF, which is often difficult in real-world applications. One of the most widely used metrics that do
not require knowing the true PF is the hypervolume
indicator [37, 64], which is essentially the area (volume) of regions in Rm that are dominated by a given
(approximate) PF. To make the hypervolume finite and
well-defined, we need to bound Rm by introducing
a reference point, often referred to as the anti-utopia
point GAU , which is worse than the PF in all objectives. Then the hypervolume indicator HV is defined

Policy-based methods iteratively update the parameters Î¸ of a policy Ï€Î¸ to improve the expected
return. The classical REINFORCE algorithm [57] for
episodic environments runs a stochastic policy, samples trajectories {(st , at )}t , and optimizes the parameters as
X
Î¸ â†Î¸+Î·
Gt âˆ‡Î¸ log Ï€Î¸ (at |st )
(2)
t

where Î· > 0 is the learning rate. Intuitively this
update rule implies that actions that lead to higher
return are reinforced more strongly than those that
lead to lower return. This method, based on the Monte
Carlo estimate of return, is prone to high variance.
To remedy this, we may utilize an advantage function
AÏ• : S Ã— A â†’ R, which has its own parameters and is
trained concurrently with the policy via recursive Bellman updates. The policy is then optimized iteratively
as
X
Î¸ â†Î¸+Î·
AÏ• (st , at )âˆ‡Î¸ log Ï€Î¸ (at |st ) .
(3)
t

This class of RL algorithms is called (on-policy)
Actor-Critic methods [58]. Widely used variants are
A3C [59], TRPO [60], and PPO [61].

3.3 Multi-objective MDP
MDP may be generalized to multi-objective MDP
(MOMDP) for multi-objective sequential decision
making problems, in which the reward function r :
5

Algorithm 1 Latent-Conditioned Multi-Objective
Policy Gradient (LC-MOPG)
Input: Ï€Î¸ : policy network, Î³ âˆˆ (0, 1]: discount rate,
dlat âˆˆ N: dimension of latent variables, Î² â‰¥ 0:
bonus coefficient, Nlat âˆˆ N: number of latent
variables sampled per iteration
1: Initialize Ï€Î¸
2: while Ï€Î¸ has not converged do
3:
for i in {1, 2, Â· Â· Â· , Nlat } do

4:
Sample ci âˆ¼ Uniform [0, 1]dlat
5:
Obtain trajectory Ï„i = {(st , at )}t and
return Gi by running policy Ï€Î¸ (ci ) in
the environment
6:
end for

b i Nlat
7:
Compute normalized returns G
i=1
lat
from {Gi }N
i=1

lat
b Nlat
8:
Compute scores {fi }N
i=1 from Gi i=1
â–· Algorithm 2

Nlat
b i Nlat
9:
Compute bonuses {bi }i=1 from G
i=1
â–· Algorithm 3
10:
for i in {1, 2, Â· Â· Â· , Nlat } do
11:
Fi â† max(fi + Î²bi , 0)
12:
end for
13:
Update Î¸ via gradient descent of loss

N
lat 
X
X
log Ï€Î¸ (a|s, ci )
Fi
LÏ€ (Î¸) = âˆ’
i=1

Parameters of action distribution
MLP
âŠ™

(s,a)âˆˆÏ„i

!
Gâˆ— âˆˆPF
m


â„¦(Gâˆ— ) = g âˆˆ R

â„¦(G ) ,

(6)

GAU â‰º g â‰º Gâˆ—

(7)

Embedding layer
(* = Optional)

ðœâˆ¼ð‘ƒ ðœ

The training is done on-policy, i.e., we start with a randomly initialized policy and use actions sampled from
the stochastic policy for exploration. Over the course
of training the policyâ€™s stochasticity decreases, signaling convergence. Since the exploration ability of this
algorithm entirely hinges on the randomness of the
policy in the early phase of training, we propose to
initialize the NN parameters so that the resulting initial policy is (nearly) uniformly random in the action
space.
An important point to note is that the latent variable c that is fed to the policy network carries no such
intuitive meaning as â€œpreference over objectives.â€ In
the proposed algorithm c is simply a mathematical steering wheel to switch policies. We
 will use a
uniform distribution Uniform [0, 1]dlat as a latent
space distribution P (c), in accordance with [47]. The
dimension dlat âˆˆ N is an important hyperparameter
of the algorithm. Higher dlat implies larger capacity (or representation power) of the policy NN, but
too high dlat makes training slow. If the dimension
of the PF, dim(PF), is known, then dlat had better be
at least as large as dim(PF). We recommend using
dlat = dim(PF) + 1 or dim(PF) + 2, which are found
to work well empirically.
The policy NN used in this work has the structure shown in Fig. 2. It provides a mapping from a
state+latent space to action distribution. The outputs
of policy are parameters of any parametric probability distribution that is suitable for MOMDP under
consideration; e.g., the mean and variance if a Gaussian distribution is chosen. For a continuous bounded
action space, we choose to use the Beta distribution
B(Î±, Î²) with Î±, Î² > 0 for each action dimension,
as recommended by Chou et al. [66]. For a discrete
action space, the outputs of policy are logits of action
probabilities.
The feature embedding layer for latent input,
which has no trainable parameters, is basically the

as
HV(PF) = Âµ

Embedding layer*

Fig. 2 Architecture of the policy network.

end while
15: return Ï€Î¸

âˆ—

Linear layer + Activation

ð¬: state

14:

[

Linear layer + Activation

where Âµ is the Lebesgue measure. HV is the only
unary indicator that is known to be Pareto compliant
[65].

4 Methodology
In this section, we introduce a novel policy-gradient
approach to MORL coined as LC-MOPG. The pseudocode is presented in Algorithm 1. The main idea
is quite simple: train a latent-conditioned NN as a
parameter-efficient representation of a diverse collection of policies in the Pareto set of given MOMDP.
6

Algorithm 2 Score computation

b i âˆˆ Rm Nlat
Input: Normalized returns G
i=1
1: Obtain the set of undominated points pf =


bi
PF G
i
2: for i âˆˆ {1, 2, Â· Â· Â· , Nlat } do


b i âˆ¥2
3:
D â† min âˆ¥z âˆ’ G

same as in [43] and explicitly given as

cos(Ï€c1 ), cos(2Ï€c1 ), Â· Â· Â· , cos(KÏ€c1 ) âŠ• Â· Â· Â·
Â· Â· Â·âŠ• cos(Ï€cdlat ), cos(2Ï€cdlat ), Â· Â· Â· , cos(KÏ€cdlat )


(8)

which embeds c âˆˆ [0, 1]dlat into [âˆ’1, 1]Kdlat . The
inflation factor K âˆˆ N is a free parameter. In principle
one can also apply the same embedding to a (properly
shifted and scaled) state vector; this is not mandatory
and its benefit should be examined for each problem
on an individual basis. Finally, the outputs of state layers and latent layers are mixed through a dot product.
For image inputs, convolutional neural networks may
be used as the first layer for states.
We note that in the evaluation phase (i.e., after
training is done) we make the policy deterministic.
For discrete actions, this is done by simply taking
the action with the highest probability. For continuous
actions, the mean of the Beta distribution is selected
for each dimension.
The crux of LC-MOPG is the parameter update
scheme of policy. Let us recall that, in policy gradient
methods for single-objective RL such as REINFORCE
[57], trajectories {Ï„i }i can be easily classified into
â€˜goodâ€™ ones and â€˜badâ€™ ones, according to their return
values {Gi âˆˆ R}i ; subsequently, actions in good trajectories are reinforced while those in bad ones are
weakened. In MORL, however, returns {Gi âˆˆ Rm }i
are vector-valued and there is more than one way to
assess the quality of trajectories. We now describe
LC-MOPGâ€™s scheme in four steps.

zâˆˆpf

4:
5:



zâˆˆpf

end for
7:
fi â† âˆ’ min D
8: end for
9: for i âˆˆ {1, 2, Â· Â· Â· , Nlat } do
10:
fi â† fi âˆ’ avg({fi }i )
â–· avg âˆˆ {mean, median}
11: end for
Nlat
12: return Scores {fi }i=1

where iqr denotes the interquartile range.
â€¢ Max-min normalization:
b =
G


G âˆ’ median({Gi }i )
.
max({Gi }i ) âˆ’ min({Gi }i )

(11)

We empirically found robust normalization most useful because it is stable in the presence of outliers due
to a large negative reward given to a fatal failure, such
as the falling down of a robot.
In some environments, there may exist a locally
optimal solution such that most of rollout trajectories
are clustered around this solution, resulting in vanishingly small denominators in (9) and (10). In such a
case, the max-min normalization is numerically more
stable.
Scoring of returns
The second step is score computation for each normalb i , as summarized in Algorithm 2. The
ized return G
main idea is to first determine the current PF (= the
b i }i ); Next,
set of undominated points) pf = PF({G
b i ; Finally,
compute the distance between pf and each G
use the negative of this distance as a score (higher is
better). This simple protocol is slightly modified in
Algorithm 2 (lines 4â€“6) due to the following reason.
Let us imagine that biobjective returns are uniformly
distributed inside the square [0, 100]2 . If we add a
point P = (100, 100) to this set, P obviously becomes

â€¢ Standard normalization:
(9)

where mean and std are computed for each dimension separately. The division of a vector by a vector
is performed elementwise.
â€¢ Robust normalization:

b = G âˆ’ median({Gi }i ) /iqr({Gi }i )
G


bi
D â† D âˆª max zj âˆ’ G
j

6:

Normalization of returns
In the first step, raw returns {Gi âˆˆ Rm }i obtained
by running latent-conditioned policies Ï€Î¸ (ci ), ci âˆ¼
P (c), must be normalized to make the algorithm
insensitive to the scale of rewards. We shall adopt one
of the normalization conventions below.


b = G âˆ’ mean({Gi }i ) /std({Gi }i )
G

for j âˆˆ {1, 2, Â·
Â· Â· , m} do

(10)
7

â€¢ Trajectories whose returns form a dense cluster in
Rm receive low bonuses.

Algorithm 3 Bonus computation
b i Nlat ,
G
i=1



Input: Normalized returns
scores
lat
{fi }N
,
and
parameter
k
âˆˆ
N
i=1
1: for i = {1, 2, Â· Â· Â· , Nlat } do
2:
maski â† 1 if fi > 0 else 0
b i to its k th nearest
3:
di â† distance from G

Nlat
b
neighbor in Gi i=1
4:
bi â† maski âˆ— di
5: end for
Nlat
6: return {bi }i=1

This distinction is quantified by the local density of

b i Nlat in Rm . We shall measure it via
returns G
i=1
the k -nearest-neighbor distance. The index k âˆˆ N
is a hyperparameter that must be carefully tuned in
conjunction with the population size Nlat . It is recommended to keep k/Nlat roughly constant when
Nlat is varied. Unfortunately, this naÃ¯ve definition of
bonus may cause a rapid degradation of policies when
bi
there are highly inferior policies whose returns G
are located far below those of other policies. To prevent this from happening, we multiply each bonus by
a binary mask âˆˆ {0, 1} so that only trajectories in the
better half of the population get bonuses (lines 2 and
4 of Algorithm 3).

the sole PF since all points inside the square are dominated by P . Now, let us look at a point Q = (0, 100).
Since Q is far away from P , the score of Q must
be very low, according to the above simple protocol.
However, if Q is slightly shifted upward (e.g., Q =
(0, 100 + 10âˆ’4 )), Q suddenly becomes a new PF and
acquires the highest score. Such a discontinuous jump
of scores would make the policy optimization bumpy
and should be avoided. This discontinuity is removed
by lines 4â€“6 of Algorithm 2.
It is a common practice in evolutionary algorithms
to use robust scores such as rank-based metrics instead
of raw objective values to assess the fitness of a population [35, 36, 67, 68]. We do not employ this method
because ranks of returns change discretely as we vary
the policy parameters continuously and make the optimization landscape less smooth. Note also that we do
not use HV-based metrics used e.g., by MO-NES [36],
because HV is generally sensitive to the choice of a
reference point.

Clipping
The fourth and last step of our policy assessment is to
cut off the sum of score and bonus at zero (line 11 of
Algorithm 1). This step yields the final score of the ith
trajectory as Fi = max(fi + Î²bi , 0), which is used in
the subsequent policy gradient update. This clipping
means that, unlike standard policy gradient methods,
we do not penalize (or weaken) actions in inferior
trajectories. We found this expedient to stabilize the
entire learning process well.
This completes the description of the overall
design and technical details of the proposed algorithm
LC-MOPG.
In single-objective RL it is well known that onpolicy trajectory-based methods (REINFORCE) are
generally outperformed by methods with advantage
functions such as A2C [59] and PPO [61]. The advantage function A(st , at ) enables to estimate a proper
update weight for each transition (state-action pair),
which therefore provides a more fine-grained policy
update than a trajectory-based update. We can consider a similar generalization of LC-MOPG, named
LC-MOPG-V. The pseudocode is presented in Algorithm 4. In this method we train two additional NN,
{QÏ• , VÏˆ }, which we call generalized value functions
despite that they have nothing to do with the standard value functions in single-objective RL. The role
of these NN is to estimate the contribution of individual states and actions within each trajectory to the full
score {Fi }i . Unlike in LC-MOPG, score thresholding
above zero is not required in LC-MOPG-V: we simply
have Fi = fi + Î²bi . Instability of learning is avoided

Bonus computation
The third step of policy assessment in LC-MOPG is
bonus computation. The bonus, denoted bi , is to be
added to the score fi for the sake of better exploration
and higher diversity of policies. The bonus is especially useful when the scoring scheme described above
alone is not enough to prevent policies from collapsing
to local optima. Although our bonus bears similarity
to intrinsic rewards used widely in RL [69], a notable
difference is that intrinsic rewards are usually independent of external rewards rt while our bonus is
directly calculated from the return distribution {Gi }i
of trajectories.
The procedure of bonus computation is outlined in
Algorithm 3. The essential features are:
â€¢ A high bonus is given to a trajectory i whose return
b i is dissimilar to other returns.
G
8

Algorithm 4 LC-MOPG with Generalized Value Networks (LC-MOPG-V)
Input: Ï€Î¸ : policy network, {QÏ• , VÏˆ }: generalized
value networks, Î³ âˆˆ (0, 1]: discount rate, dlat âˆˆ
N: dimension of latent variables, Î² â‰¥ 0: bonus
coefficient, Nlat âˆˆ N: number of latent variables
sampled per iteration, D: rollout buffer
1: Initialize Ï€Î¸ , QÏ• , VÏˆ
2: while Ï€Î¸ has not converged do
3:
for i in {1, 2, Â· Â· Â· , Nlat } do

4:
Sample ci âˆ¼ Uniform [0, 1]dlat
5:
Obtain trajectory Ï„i = {(st , at )}t and
return Gi by running policy Ï€Î¸ (ci ) in
the environment
6:
Store Ï„i in D
7:
end for

b i Nlat
8:
Compute normalized returns G
i=1
lat
from {Gi }N
i=1

lat
b Nlat
9:
Compute scores {fi }N
i=1 from Gi i=1

lat
b Nlat
10:
Compute bonuses {bi }N
i=1 from Gi i=1
11:
for i in {1, 2, Â· Â· Â· , Nlat } do
12:
Fi â† fi + Î²bi
13:
Append the information of Fi to Ï„i
in D
14:
end for
15:
Update Ï• and Ïˆ via stochastic gradient
descent of loss
X
2
LQ (Ï•) =
(QÏ• (s, a) âˆ’ Fi )

T1
T2
T3
T4 T5 T6

T7 T8
T9
T10

Fig. 3 The DST environment. Orange cells are treasures and blue
cells are the ocean.

5 Environments
In this section, we describe environments that are
used to numerically evaluate the proposed algorithm
in Sec. 6.

5.1 Deep Sea Treasure
Deep Sea Treasure (DST) is a simple grid world proposed by Vamplew et al. [37]. Many prior works
employed DST to evaluate MORL methods [14â€“16,
21, 22, 26, 30, 32]. As shown in Fig. 3, a submarine
controlled by the agent starts from the top left cell and
moves over the 11 Ã— 11 grid by taking one of the four
actions {up, down, left, right}. There are two objectives in DST: time cost and treasure reward. At each
time step, the agent incurs a time cost of âˆ’1. The
treasure reward is 0 if the agent is in blue cells and
is equal to the treasure value âˆˆ {T1, T2, Â· Â· Â· , T10}
if it reaches one of the treasures (orange cells). The
episode ends if either the agent reaches a treasure or
moves into a cliff (gray cells). Fortunately the exact
PF of DST is known; it is given by trajectories of minimum Manhattan distance from the start cell to each
treasure.
There are some variations in the literature regarding the treasure values. In this work, we consider two
settings.

(s,a,Fi )âˆˆB

LV (Ïˆ) =

X

2

(VÏˆ (s) âˆ’ Fi )

(s,Fi )âˆˆB

16:

17:

where a minibatch B is drawn from D
Compute corrected scores
Fbi (s, a) = QÏ• (s, a) âˆ’ VÏˆ (s)
Update Î¸ via gradient descent of loss
N
lat
X
X
LÏ€ (Î¸) = âˆ’
Fbi (s, a) log Ï€Î¸ (a|s, ci )
i=1 (s,a)âˆˆÏ„i

end while
19: return Ï€Î¸

18:

â€¢ Original version [21, 22, 26, 32, 37]: {Tn}10
n=1 =
(1, 2, 3, 5, 8, 16, 24, 50, 74, 124).
â€¢ Convex version [15, 16, 30]: {Tn}10
n=1 =
(0.7, 8.2, 11.5, 14.0, 15.1, 16.1, 19.6, 20.3, 22.4, 23.7).

thanks to the transition-wise update capability of LCMOPG-V. An important point to note is that QÏ• and
VÏˆ do not depend on the latent variables {ci }i .
Both LC-MOPG and LC-MOPG-V require collecting Nlat trajectories at every iteration. This step
can be accelerated efficiently by means of parallelization over multiple CPU cores.

The convex version is easier to solve because all solutions on the PF can in principle be discovered with
linear scalarization methods. Note that upper treasures
such as T1 and T2 are easy to find while farther
treasures such as T10 are substantially more difficult to find. Thus, despite its deceptive simplicity,
9

DST poses a hard challenge of solving the exploration
vs. exploitation dilemma in RL.

5.2 Fruit Tree Navigation
Fruit Tree Navigation (FTN) [15] is a binary tree
of depth d. The state space is discrete and twodimensional: s = (i, j) âˆˆ N2 with 0 â‰¤ i â‰¤ d and
0 â‰¤ j â‰¤ 2i âˆ’ 1. At every non-terminal node, the
agent selects between left and right. At a terminal node
the agent receives a reward r âˆˆ R6 and finishes the
episode. Thus the length of every episode is equal to d.
The set of reward values is configured such that every
reward is on the PF. Now the challenge for MORL
methods is to discover and maintain all the 2d Paretooptimal policies, which gets harder for higher d. In this
paper we consider d âˆˆ {5, 6, 7} following [15, 16].
We use the code available at GitHub [70].

Fig. 4 Minecart environment. The cart departs from the top-left
corner, goes for mining at any of the 5 mines (blue circles), and
returns home (red quarter circle) to sell ores.

R are m Ã— m positive-definite matrices. Then the optimal control
Pâˆžthat maximizes the discounted cumulative
rewards t=0 Î³ t rt is given by
a = âˆ’Î³(R + Î³S)âˆ’1 Ss .

5.3 Linear Quadratic Gaussian Control

The positive-definite square matrix S is obtained by
numerically solving the discrete-time Riccati difference equation

Linear Quadratic Gaussian Control (LQG) is a wellknown classic problem in control theory [71] with
multi-dimensional continuous state and action spaces.
LQG has been considered as a test problem in a number of RL and MORL studies, e.g., [32, 34, 36, 72,
73]. Specifically, in this paper we consider the multiobjective version presented in [32, 34, 36]. Let m
denote the dimension of state and action spaces. The
state transition dynamics is defined by
st+1 = st + at + ÏƒÎµ ,

S = Q + Î³S âˆ’ Î³ 2 S(R + Î³S)âˆ’1 S .

(i = 1, ..., m)

(15)

Note that (14) and (15) do not depend on Ïƒ . Thus,
under the assumption that the PF for the multiobjective problem (13) is convex, the entire Pareto set
can be obtained by iteratively solving (14)
and (15)
Pm
using the linearly
weighted
sums
Q
=
i=1 wi Qi
Pm
and R =
w
R
for
a
sufficiently
dense
set of
i=1 i i
Pm
weights w âˆˆ Rm , wi â‰¥ 0 âˆ€i and i=1 wi = 1. The
convexity of the PF has been verified in [32, 34, 36].

(12)

where s âˆˆ Rm and a âˆˆ Rm are state and action, Îµ is a
random noise drawn from the m-dimensional normal
distribution, and Ïƒ â‰¥ 0 is the noise strength parameter.
The reward r âˆˆ Rm is defined by
ri = âˆ’sT Qi s âˆ’ aT Ri a

(14)

5.4 Minecart
Minecart [14] is a multi-objective environment with
a continuous state space, designed for testing MORL
algorithms. The map of Minecart is shown in Fig. 4.
The cart is initially at the top-left corner (0, 0) and can
go anywhere inside the unit square [0, 1]2 . There are
five mines, to which the cart must travel to perform
mining. Inside a mine (blue circle) the cart can mine
and get two kinds of ores. The amount of ores available via one-time mining is different for each mine.
The cartâ€™s capacity is 1.5 so that the sum of mined
ores on the cart cannot exceed this capacity. Once the
cart returns to the home port (red circle), it sells all
the ores, acquires rewards, and terminates the episode.
The return in Minecart is three-dimensional: the first

(13)

where Qi and Ri are m Ã— m diagonal matrices. All
diagonal entries of Qi are equal to Î¾ except (Qi )ii =
1 âˆ’ Î¾ , whereas all diagonal entries of Ri are equal to
1 âˆ’ Î¾ except (Ri )ii = Î¾ . Following [32, 34, 36], we
set Î¾ = 0.1 and the initial state to s0 = (10, ..., 10)T .
As for the noise we consider both Ïƒ = 0 and 1.
Part of the motivation to consider LQG for benchmarking stems from the fact that the optimal policy
for LQG is known [71, 74]. For simplicity, let us consider a single-objective problem with scalar reward
r = âˆ’sT Qs âˆ’ aT Ra and dynamics (12), where Q and
10

two components are the amount of mined ores, and
the last component is the total fuel cost. The cart can
take six discrete actions, [Mine, Turn Left, Turn Right,
Accelerate, Brake, Do Nothing]. The underlying state
space is R6 consisting of the cartâ€™s position, speed,
angle, the mined amounts of ore1 and ore2.
To get ores while minimizing the fuel cost, the
cart must go straight to the desired mine(s), do mining, rotate the direction, and return straight to the
home port. This requires a highly nonlinear control and offers an attractive testbed for deep MORL
approaches. We use the source code of Minecart available at [75].

Fig. 5 Exact PF of the DST environment. The vertical axis is the
cumulative time cost, and the horizontal axis is the treasure reward.
Left: For convex treasure values and Î³ = 0.99. Right: For original
treasure values and Î³ = 1.0. These discount rates were chosen to
ensure a fair comparison with related work.

6 Numerical experiments

Table 1 Comparison of methods in the DST (convex PF)
environment. Results for baselines are taken from [15, 16]. Best
values are marked with an asterisk.

In this section we conduct several experiments to
evaluate our method.

6.1 Implementation details

Envelope [15]
CN+DER [14]
PD-MORL [16]
LC-MOPG (Ours)

We use PyTorch to implement all NN models. Initial parameters of NN are randomly drawn from
N (0, 0.22 ). NN are optimized with ADAM stochastic
optimizer [76] with learning rate 1eâˆ’3. Tanh activation is used for the first activation of the latent input
to policy NN. SELU activation [77] is used for all
the other activation layers in policy NN, QÏ• , and VÏˆ .
The computation of HV is done with external multiobjective optimization library pymoo [78]. We use
an Intel Xeon Gold 5218 CPU with 2.30GHz and an
NVIDIA Tesla V100 GPU for our experiments.
Unless stated otherwise, numerical plots in this
section show the mean Â± one standard deviation over
5 independent runs with different random seeds. In
learning curve plots, the abscissa iteration refers to
the number of gradient updates performed on the policy. During training, quality of the PF produced by the
policy (conditioned on Nlat latent variables {ci } randomly drawn from P (c)) is evaluated on test episodes
at every iteration round. The number of test episodes
per iteration is 1 for deterministic environments and
> 1 for stochastic environments, for which the average of scores over multiple test episodes is recorded.
The mean and the standard deviation of the best test
scores from 5 training runs are reported in tables for
comparison with baselines.

CRF1 (â†‘)

HV (â†‘)

0.994
0.989
1.0âˆ—
1.0âˆ—

227.89
â€”
241.73âˆ—
241.73 Â±0âˆ—

Table 2 Comparison of methods in the DST (original)
environment. Results for baselines are taken from [26]. Best value
is marked with an asterisk.
HV (â†‘)
RA [32]
MO-NES [36]
PCN [26]
LC-MOPG (Ours)

22437.40 Â± 49.20
17384.83 Â± 6521.10
22845.40 Â± 19.20
22855.0 Â± 0âˆ—

in Fig. 5. Hyperparameters used for the experiment are
summarized in Appendix A.1.
In the setting with convex PF, our LC-MOPG
yielded the result in Table 1. To compute HV we
have used (0, âˆ’19) as a reference point. The score
CRF1 (higher is better, the maximum is 1), which was
defined in [15], is also shown for reference. We find
that LC-MOPG found the true PF for all runs. The
bonus factor plays an essential role in obtaining this
result. In Fig. 6 (top) we plot the learning curves of
LC-MOPG with and without bonus. We observe that,
without bonus, the policy is trapped in local optima
(i.e., the treasures T1 and T2 located near the initial
position) and never discovers the true PF.
The results for DST with the original treasure values are presented in Table 2. To compute HV we
have used (0, âˆ’200) as a reference point. Unlike other

6.2 Results for DST
As noted previously, there are two settings for DST
environment. The true PF for both cases are displayed
11

d=5

d=6

d=7

Fig. 7 Training process of LC-MOPG for the FTN environment
with depth d = 5, 6, 7.
Fig. 6 Training process of LC-MOPG for the DST environment.
Top: For convex treasure values. Bottom: For original treasure values.

The learning curves of LC-MOPG are displayed
in Fig. 7. As the computation of HV becomes exponentially hard in high dimensions, we monitored
the performance at every iteration using Nlat =
(300, 400, 400) for d = 5, 6, 7, respectively, and measured the scores of the best policies using Nlat = 1500
in the evaluation phase. This explains why the scores
in Table 3 are higher than those in Fig. 7. It must
be noted that, although our method can parametrize
infinitely many policies with a single NN, their performance evaluation can be computationally challenging
especially when the number of objectives is high (>
3). In our experiment, the evaluation of 1500 policies
took about half an hour.

methods, LC-MOPG has successfully found the true
PF for all runs. This result may not be obtained when
the bonus is zero, as shown in the learning curves in
Fig. 6 (bottom).

6.3 Results for FTN
Next, we report numerical results obtained with LCMOPG in the FTN environment with the depth parameter d = 5, 6, 7. The true PF comprises 2d points
and the problem becomes harder for higher d. Hyperparameters used in LC-MOPG are summarized in
Appendices A.2, A.3 and A.4. We have utilized state
embedding in this environment. Namely, a state s =
(i, j) âˆˆ N2 is first normalized as (i/d, j/2i ) and then
embedded into Re1 Ã— Re2 via cosine embedding (8).
The values (e1, e2) âˆˆ N2 are listed in the appendices.
The performance of our method is compared with
baselines in Table 3. The origin (0, Â· Â· Â· , 0) âˆˆ R6 was
used as a reference point for HV. For d = 5 and 6, our
method was able to discover the true PF in all runs. For
d = 7 our method discovered the true PF in 4 out of 5
runs. The mean score is hence very close to the highest
possible HV for d = 7, which is 12302.34. We conclude that LC-MOPG solved all cases almost exactly.
For d = 7 our score is 7.6% higher than the best
baseline score (PD-MORL). However, we found that
very careful tuning of the bonus parameters {k, Î²},
the latent dimension dlat , and the hidden layer width
of policy NN was necessary to attain this result.

6.4 Results for LQG
Next, we proceed to the evaluation of our method in
the LQG environment. In this section we aim to: (i)
Compare the PF of LC-MOPG with the ground-truth
PF. (ii) Visualize the evolution of the PF of LC-MOPG
during the training phase. (iii) Check dependence on
the latent dimension dlat . (iv) Compare the performance of LC-MOPG and LC-MOPG-V. (v) Check if
LC-MOPG and LC-MOPG-V work when the transition dynamics is stochastic. Hyperparameters used in
the experiment are tabulated in Appendices A.5 and
A.6.
Two dimensions
The results for deterministic LQG in two dimensions
(Ïƒ = 0) are summarized in Table 4. HV is computed
with a reference point (âˆ’310, âˆ’310) and divided by
1602 . The optimal HV value is obtained with the
12

Table 3 Comparison of methods in the FTN environment. Results for baselines are taken from [16]. Best values are marked with an asterisk.

HV (â†‘)
d=5

Envelope [15]
PD-MORL [16]
LC-MOPG (Ours)

âˆ—

6920.58
6920.58âˆ—
6920.58 Â± 0âˆ—

d=6

d=7

8427.51
9299.15
9302.38 Â± 0âˆ—

6395.27
11419.58
12290.93 Â± 22.82âˆ—

Table 4 Comparison of methods in the two-dimensional
deterministic LQG environment (Ïƒ = 0).
HV (â†‘)
Optimal value
LC-MOPG (dlat = 1)
LC-MOPG (dlat = 2)
LC-MOPG (dlat = 3)
LC-MOPG-V (dlat = 2)

1.1646
1.1408 Â± 0.0061
1.1457 Â± 0.0040
1.1408 Â± 0.0076
1.1031 Â± 0.0090

Fig. 9 Comparison of the PF obtained with LC-MOPG (left) and
LC-MOPG-V (right) in the two-dimensional LQG environment,
overlayed with the optimal PF (red solid line) obtained by solving
the algebraic Riccati difference equation.

However, LC-MOPG-Vâ€™s performance saturates early
and is eventually surpassed by LC-MOPG after 200
iterations.
To visually inspect their difference in the objective
space, we randomly sampled 1000 latent variables, fed
them into the trained policy networks of LC-MOPG
and LC-MOPG-V, obtained their returns, and identified the PF. The result is displayed in Fig. 9. For
reference, the optimal PF obtained by solving the Riccati equation is also shown. The PF of both methods
lie in the close vicinity of the true PF. However, the
PF from LC-MOPG-V is narrower than that of LCMOPG, implying that the issue with LC-MOPG-V is
coverage, rather than accuracy, of the PF.
The convergence of the return set {Gi }i towards
the optimal PF during training is displayed in Fig. 10.
One can see that the return set at iteration 25 is of low
quality, but as the training progresses, the returns gradually move to the top-right area of the return space. At
iteration 500 most returns cluster near the true PF.

Fig. 8 Comparison of the training process of our methods for twodimensional deterministic LQG environment. The blue dashed line
is the reference value obtained by solving the algebraic Riccati
equation.

method of Sec. 5.3 using a mesh of weights (w, 1âˆ’w)
with w âˆˆ {0.01, 0.02, Â· Â· Â· , 0.99}. It is observed that
LC-MOPG with dlat = 2 attained the best score equal
to 98.0% of the true HV. We have also tested LCMOPG-V that uses the generalized value functions Q
and V , and obtained the final score equal to 94.7% of
the true HV. It is concluded that LC-MOPG was able
to solve the problem accurately and that LC-MOPG
outperformed LC-MOPG-V on this problem.
We have also investigated LC-MOPG without
clipping of scores above zero (line 11 of Algorithm 1).
It led to diverging gradient updates and policy parameters soon became NaN at an early stage of training.
As a result, no sensible policy was obtained.
The learning curves of LC-MOPG and LCMOPG-V (both for dlat = 2) are compared in Fig. 8.
It is observed that the initial learning of LC-MOPGV is roughly twice faster than that of LC-MOPG,
which may be attributed to the accurate score estimation of state-action pairs thanks to Q and V .

Three dimensions
Next, to see if our method works for more than two
objectives, we evaluated the method in LQG in three
dimensions without noise (Ïƒ = 0). The results are
summarized in Table 5. HV is computed with a
reference point (âˆ’500, âˆ’500, âˆ’500) and divided by
3503 . The optimal HV value is obtained with the
13

Fig. 11 Same as Fig. 8 but in three dimensions.

Table 6 Comparison of methods in two-dimensional stochastic
LQG environment with Ïƒ = 1.0.
Fig. 10 Progression of the return set of LC-MOPG during training
in the two-dimensional LQG environment. At each iteration, 200
latent variables are randomly sampled, and the better half of returns
(i.e., the set of Gi for which fi > 0 with fi computed as in Algorithm 2) is shown in the figure. Some low-lying returns are out of
the scope of the figure.

HV (â†‘)
Optimal value
LC-MOPG (dlat = 2)
LC-MOPG-V (dlat = 2)

Table 5 Comparison of methods in the three-dimensional
deterministic LQG environment.

zero. However none of this produced qualitatively
different results. How to cure this problem is left for
future research. One direction could be to employ
the generalized value functions in the early phase of
training and later switch them off. Another direction
could be to add an entropy term to the loss function so
that the policyâ€™s randomness does not decay too fast.

HV (â†‘)
Optimal value
LC-MOPG (dlat = 1)
LC-MOPG (dlat = 2)
LC-MOPG (dlat = 3)
LC-MOPG-V (dlat = 3)

0.9967
0.9616 Â± 0.0038
0.9144 Â± 0.0245

0.8476
0.8124 Â± 0.0036
0.8153 Â± 0.0106
0.8208 Â± 0.0037
0.7544 Â± 0.0152

method of Sec. 5.3 using a mesh of 4851 weights
{(0.01, 0.01, 0.98), (0.01, 0.02, 0.97), Â· Â· Â· , (0.98, 0.01, 0.01)}.

We observe that LC-MOPG (dlat = 3) performs best,
attaining HV equal to 96.8% of the optimal value. It
is surprising that LC-MOPG (dlat = 1) has attained a
high score despite the low latent dimension. We speculate that the latent space [0, 1] is windingly embedded
in R3 to densely approximate the two-dimensional
PF. As for the impact of using generalized value
functions, we once again find that LC-MOPG-Vâ€™s
performance is slightly inferior to that of LC-MOPG,
attaining HV equal to only 91.9% of LC-MOPG.
Their learning curves are compared in Fig. 11 (both
for dlat = 3). The qualitative behavior is the same as
in two dimensions. Namely, LC-MOPG-V achieves
substantial speed-up of training compared to LCMOPG but quickly saturates at a suboptimal solution.
It seems that the agent does not have enough time to
randomly explore the environment, hence failing to
find the global optimum and attracted to local optima.
We have tried several tricks to fix this, including the
use of different learning rates for the policy and value
networks, and the clipping of trajectory scores above
14

Nonzero stochasticity
We have so far discussed only deterministic LQG
environments (Ïƒ = 0). As most of industrial use cases
of RL assume the presence of stochasticity in environments, it is important to benchmark the proposed
methods at finite noise. With this in mind we have
run LC-MOPG and LC-MOPG-V in two-dimensional
stochastic LQG with Ïƒ = 1.0. At each iteration during training, we monitored the performance of the
trained policy by running them for 10 test episodes.
After training, we sampled 1500 latent variables and
fed them into the best policy. Each of the resultant
1500 policies was evaluated on 200 test episodes.
This process was repeated for 5 independent runs. The
reference value for optimal HV was obtained by solving the Riccati equation numerically for a series of
weights; the optimal control policy thus obtained for
each weight was tested on 2000 episodes, to reduce
statistical errors.
The scores are summarized in Table 6. We note
that the optimal HV value has dropped from 1.1646
(Table 4) to 0.9967 by 14% reflecting the effect of
disturbance (Ïƒ ). It is satisfactory to see that LCMOPG attained 96.5% of the optimal HV, implying

Fig. 13 Training process of LC-MOPG in the Minecart environment. To smooth the plot, moving averages over 7 steps were taken.

Fig. 12 Same as Fig. 8 but with nonzero stochasticity (Ïƒ = 1.0).

Table 7 Comparison of methods in the Minecart environment.
Results for baselines are taken from [26]. Best value is marked
with an asterisk.
HV (â†‘)
RA [32]
MO-NES [36]
PCN [26]
LC-MOPG (Ours)

123.92 Â± 0.25
123.81 Â± 23.03
197.56 Â± 0.70
198.17 Â± 0.32âˆ—

that a wide and accurate PF was obtained with this
method. In contrast, LC-MOPG-V attained 91.7% of
the optimal HV. Their learning curves are juxtaposed
in Fig. 12. We observe that the same trend as in Figs. 8
and 11 persists here.

Fig. 14 The PF obtained by LC-MOPG projected onto the first two
dimensions. The color of each point indicates the third component
of the return.

salient stability of the proposed approach. The learning curve is shown in Fig. 13. The training with 5
random seeds took approximately 29 hours. The fact
that the number of required iterations (3000) is substantially larger than that for simpler environments
such as LQG (500âˆ¼800) highlights the complexity of control required in the Minecart environment.
Due to the limitation on computational resources,
we could not fully explore optimal hyperparameter
values. Although we speculate that the training of
optimally tuned LC-MOPG would converge faster, an
in-depth examination of this point is beyond the scope
of this paper.
To gain more insight into the obtained policies,
we have plotted the PF of LC-MOPG in Fig. 14. We
observe that LC-MOPG was able to discover all 17
points on the diagonal line that are associated with trajectories in which the cart mines ores until it gets full.
These points are in blue, because their trajectories are
generally long and the fuel cost is high (i.e., the third
component of return is low). The PF point at the origin
(0, 0) is associated with an exceptionally short trajectory in which the cart departs home and then returns
immediately, without ever reaching mines. The three
isolated points below the diagonal represent trajectories in which the cart returns home before it gets full.

6.5 Results for Minecart
In this section we discuss numerical results in the
Minecart environment. For numerical evaluation of
policies we will use the point (0, 0, âˆ’200) as a reference point of HV. As shown in [26], the majority
of points in the PF lie on the straight line between
(0, 1.5) and (1.5, 0) on the plane of two kinds of mined
ores due to the fact that the cartâ€™s capacity is 1.5.
Hence, neglecting the fuel cost, the upper bound on
the possible HV would be (1.52 /2) Ã— 200 = 225. An
elementary calculation using the profiles of 5 mines
reveals that only 17 points on the diagonal line are feasible, which further reduces the upper bound on HV
to 199.78. The actual HV of the true PF must be even
lower, because the total fuel cost cannot be zero.
We have trained the agent of LC-MOPG using
hyperparameter values in Appendix A.7. After training, we fed 2000 random latent variables to the policy
network and evaluated the HV. The result is summarized in Table 7. We observe that LC-MOPG outperformed all baselines in terms of HV. The attained
score of LC-MOPG is close to that of PCN [26], but
it is noteworthy that the standard deviation of LCMOPG is less than half of that of PCN, indicating
15

Fig. 15 Example trajectories in the Pareto set. All these trajectories were generated by the same policy network conditioned on different latent
values. The value of the return is (0.65, 0.85, âˆ’1.30) in the left panel, (0.40, 0.60, âˆ’0.84) in the middle panel, and (0.28, 1.22, âˆ’1.38) in
the right panel, respectively. The symbol â€˜Mâ€™ indicates that the agent mined ores at that point.

maximum episode length limit (100) enforced during
training does not interfere with the learning of optimal
policies.

7 Conclusions and outlook
Optimal control problems in real life often involve
multiple objectives that are hard to maximize simultaneously, such as the speed and safety in car driving.
Single-objective RL methods may apply if there is
an agreed way to aggregate multiple objectives into
a single target by means of, e.g., linear scalarization that uses a userâ€™s preference over objectives.
However, preference may vary from one occasion to
another, and obtaining separate policies through separate training could be quite inefficient. In this work,
we introduced LC-MOPG, an MORL algorithm that
obtains infinitely many policies in a single run of training. It uses a latent-conditioned NN to learn diverse
behaviors in a parameter-efficient way. It does not
rely on linear scalarization and hence is capable of
finding the concave part of the PF. In numerical experiments we confirmed that LC-MOPG performed on
per with or even better than standard baselines from
the MORL literature. While recent work on MORL
predominantly focuses on value-based methods (some
variants of Q learning) [8â€“10, 14â€“16, 22], our work
demonstrates that policy-based methods still stand as
a competitive alternative.
In future research, we wish to employ the proposed
methods in solving complex real-life problems in
industry. The challenge for us is then to improve sample complexity. The proposed methods (LC-MOPG
and LC-MOPG-V) collect a large number of rollout
trajectories on-the-fly, and make use of them only once
to update the policy parameters. It would be valuable if
we could introduce some kind of off-policiness so that

Fig. 16 Distribution of the episode lengths of 247 trajectories that
correspond to the PF depicted in Fig. 14 in the Minecart environment.

Although ore1 and ore2 are symmetric in this environment, these three PF points are not symmetric, which
implies that a hidden fourth point at (0.6, 0.4) has been
missed by our agent.
In Fig. 15 we present three example trajectories
generated by the trained policy network. It is satisfactory to see that a single policy network can actually
produce markedly different behaviors if conditioned
on different values of the latent input.
Finally we comment on the episode length distribution. In the Minecart environment, an episode ends
if the cart once departs and later returns home, otherwise the episode will last forever. For numerical
efficiency, we set the maximum episode length to 100
during training, and elongated it to 1000 in the evaluation phase. (We count 4 frames as 1 time step.) We
collected 10000 random trajectories by running the
trained policies from 5 independent runs with random
latent input, and found that 8236 trajectories were of
length 1000. This means that in more than 80% of the
acquired behaviors, the cart either stays home forever,
or departs but never returns home. Out of the remaining 1764 cases, we found 247 cases to belong to the
PF. The episode lengths in these cases obey the distribution in Fig. 16. Most episodes in the PF seem
to end with less than 50 steps. This verifies that the
16

A.3

stored rollout trajectories may be utilized more than
once. Another challenge is to evaluate the proposed
methods on problems with high-dimensional continuous states such as images, and problems with partially
observable states (i.e., POMDP). How to utilize specialized architectures such as convolutional NN and
recurrent NN as a component of the policy network
for LC-MOPG is an interesting question that calls for
further investigation.

Appendix A
A.1

Hyperparameter
Settings

Hyperparameter

Value

dlat
Nlat (train)
Nlat (test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations
(e1, e2)

7
400
1500
140
3
â€”
10
10.0
0.99
Max-min
20
(10, 10)

Hyperparameters for DST
Hyperparameter

Value

dlat
Nlat (train and test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²

3
400
36
3
50
10
4.0

0.99 (convex case)
1.0 (original case)
Max-min
30

Î³ (train and test)

normalization of returns
number of iterations

A.2

Hyperparameters for FTN (d = 6)

A.4

Hyperparameters for FTN (d = 5)

A.5

Hyperparameters for FTN (d = 7)
Hyperparameter

Value

dlat
Nlat (train)
Nlat (test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations
(e1, e2)

7
400
1500
210
3
â€”
10
10
0.99
Max-min
20
(10, 10)

Hyperparameters for LQG (2D)

Hyperparameter

Value

Hyperparameter

Value

dlat
Nlat (train and test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations
(e1, e2)

5
300
100
3
â€”
3
5.0
0.99
Max-min
20
(10, 20)

dlat
Nlat (train)
Nlat (test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations

1, 2, 3
200
1500
24
3
30
3
10.0
0.9
Robust
500

17

Hyperparameter for Q and V

Value

epochs per iteration
batch size
hidden layer width
hidden layer depth

1
64
24
3

Declarations
Conflict of interest
The authors declare that they have no conflict of
interest.

Ethical approval
A.6

A.7

This article does not contain any studies with animals
performed by any of the authors.

Hyperparameters for LQG (3D)
Hyperparameter

Value

dlat
Nlat (train)
Nlat (test)
width of policyâ€™s MLP
depth of policyâ€™s MLP
max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations

1, 2, 3
300
1500
30
3
30
3
10.0
0.9
Robust
800

Hyperparameter for Q and V

Value

epochs per iteration
batch size
hidden layer width
hidden layer depth

1
100
30
3

References
[1] Sutton, R. S. & Barto, A. G. Reinforcement
Learning: An Introduction Second edn (MIT
Press, 2018).
[2] Mnih, V. et al. Human-level control through
deep reinforcement learning. Nature 518 (7540),
529â€“533 (2015) .
[3] Silver, D. et al. Mastering the game of Go with
deep neural networks and tree search. Nature
529 (7587), 484â€“489 (2016) .
[4] Li, Y. Deep Reinforcement Learning. CoRR
abs/1810.06339 (2018). 1810.06339 .
[5] Roijers, D. M., Vamplew, P., Whiteson, S. &
Dazeley, R.
A Survey of Multi-Objective
Sequential Decision-Making. J. Artif. Intell. Res.
48, 67â€“113 (2013). https://doi.org/10.1613/jair.
3987 .

Hyperparameters for Minecart
Hyperparameter

Value

dlat
Nlat (train)
Nlat (test)
width of policyâ€™s MLP
depth of policyâ€™s MLP

3
400
2000
36
3

100 (train)
1000 (test)
3
6.0
1.0
Max-min
3000
all 10

max episode length
k
Î²
Î³ (train and test)
normalization of returns
number of iterations
(e1, e2, e3, e4, e5, e6)

[6] Liu, C., Xu, X. & Hu, D.
Multiobjective Reinforcement Learning: A Comprehensive
Overview. IEEE Trans. Syst. Man Cybern. Syst.
45 (3), 385â€“398 (2015). https://doi.org/10.1109/
TSMC.2014.2358639 .
[7] Hayes, C. F. et al.
A practical guide
to multi-objective reinforcement learning and
planning.
Auton. Agents Multi Agent Syst.
36 (1), 26 (2022).
https://doi.org/10.1007/
s10458-022-09552-y .
[8] Mossalam, H., Assael, Y. M., Roijers, D. M. &
Whiteson, S. Multi-Objective Deep Reinforcement Learning. CoRR abs/1610.02707 (2016).
In: NIPS 2016 Workshop on Deep Reinforcement Learning, 1610.02707 .
18

[9] Castelletti, A., Pianosi, F. & Restelli, M. Editors (ed.) Tree-based Fitted Q-iteration for
Multi-Objective Markov Decision problems.
(ed.Editors) The 2012 International Joint Conference on Neural Networks (IJCNN), Brisbane,
Australia, June 10-15, 2012, 1â€“8 (IEEE, 2012).

December 8-14, 2019, Vancouver, BC, Canada,
14610â€“14621 (2019).
[16] Basaklar, T., Gumussoy, S. & Ogras, Ãœ. Y.
PD-MORL: Preference-Driven Multi-Objective
Reinforcement Learning Algorithm.
CoRR
abs/2208.07914 (2022). 2208.07914 .

[10] Castelletti, A., Pianosi, F. & Restelli, M. A multiobjective reinforcement learning approach to
water resourcessystems operation: Pareto frontier approximation in a single run. Water Resour.
Res. 49, 3476â€“3486 (2013) .

[17] Sutton, R. S., McAllester, D., Singh, S. &
Mansour, Y. Solla, S., Leen, T. & MÃ¼ller,
K. (eds) Policy Gradient Methods for Reinforcement Learning with Function Approximation. (eds Solla, S., Leen, T. & MÃ¼ller, K.)
Advances in Neural Information Processing Systems, Vol. 12 (MIT Press, 1999).

[11] Das, I. & Dennis, J. E. A closer look at drawbacks of minimizing weighted sums of objectives for Pareto set generation in multicriteria
optimization problems. Structural Optimization
14, 63â€“69 (1997) .

[18] Chen, X., Ghadirzadeh, A., BjÃ¶rkman, M.
& Jensfelt, P. Editors (ed.) Meta-Learning
for Multi-objective Reinforcement Learning.
(ed.Editors) 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 977â€“983 (2019).

[12] Vamplew, P., Yearwood, J., Dazeley, R. & Berry,
A. Wobcke, W. & Zhang, M. (eds) On the
Limitations of Scalarisation for Multi-objective
Reinforcement Learning of Pareto Fronts. (eds
Wobcke, W. & Zhang, M.) AI 2008: Advances in
Artificial Intelligence, 372â€“378 (Springer Berlin
Heidelberg, Berlin, Heidelberg, 2008).

[19] Xu, J. et al. Editors (ed.) Prediction-Guided
Multi-Objective Reinforcement Learning for
Continuous Robot Control. (ed.Editors) Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event, Vol. 119 of Proceedings
of Machine Learning Research, 10607â€“10616
(PMLR, 2020).

[13] Van Moffaert, K., Drugan, M. M. & NowÃ©,
A. Editors (ed.) Scalarized multi-objective reinforcement learning: Novel design techniques.
(ed.Editors) 2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL), 191â€“199 (2013).

[20] Siddique, U., Weng, P. & Zimmer, M. Learning Fair Policies in Multiobjective (Deep) Reinforcement Learning with Average and Discounted Rewards.
CoRR abs/2008.07773
(2020). 2008.07773 .

[14] Abels, A., Roijers, D. M., Lenaerts, T., NowÃ©,
A. & Steckelmacher, D. Chaudhuri, K. &
Salakhutdinov, R. (eds) Dynamic Weights in
Multi-Objective Deep Reinforcement Learning.
(eds Chaudhuri, K. & Salakhutdinov, R.) Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning Research, 11â€“20
(PMLR, 2019).

[21] Moffaert, K. V. & NowÃ©, A. Multi-objective
reinforcement learning using sets of pareto dominating policies. J. Mach. Learn. Res. 15 (1),
3483â€“3512 (2014).
https://doi.org/10.5555/
2627435.2750356 .
[22] Reymond, M. & Nowe, A.
Editors (ed.)
Pareto-DQN: Approximating the Pareto front
in complex multi-objective decision problems.
(ed.Editors) Proceedings of the Adaptive and
Learning Agents Workshop 2019 (ALA-19) at
AAMAS (2019).

[15] Yang, R., Sun, X. & Narasimhan, K. Wallach, H. M. et al. (eds) A Generalized Algorithm
for Multi-Objective Reinforcement Learning and
Policy Adaptation. (eds Wallach, H. M. et al.)
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
19

[23] GÃ¡bor, Z., KalmÃ¡r, Z. & SzepesvÃ¡ri, C. Shavlik,
J. W. (ed.) Multi-criteria Reinforcement Learning. (ed.Shavlik, J. W.) Proceedings of the
Fifteenth International Conference on Machine
Learning (ICML 1998), Madison, Wisconsin,
USA, July 24-27, 1998, 197â€“205 (Morgan Kaufmann, 1998).

[30] Abdolmaleki, A. et al. Editors (ed.) A distributional view on multi-objective policy optimization. (ed.Editors) Proceedings of the 37th
International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event,
Vol. 119 of Proceedings of Machine Learning
Research, 11â€“22 (PMLR, 2020).

[24] Wray, K. H. & Zilberstein, S.
Yang, Q.
& Wooldridge, M. J. (eds) Multi-Objective
POMDPs with Lexicographic Reward Preferences. (eds Yang, Q. & Wooldridge, M. J.)
Proceedings of the Twenty-Fourth International
Joint Conference on Artificial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 2531, 2015, 1719â€“1725 (AAAI Press, 2015).

[31] Abdolmaleki, A. et al. Editors (ed.) Maximum a
Posteriori Policy Optimisation. (ed.Editors) 6th
International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings (2018).
[32] Parisi, S., Pirotta, M., Smacchia, N., Bascetta,
L. & Restelli, M. Editors (ed.) Policy gradient
approaches for multi-objective sequential decision making. (ed.Editors) 2014 International
Joint Conference on Neural Networks, IJCNN
2014, Beijing, China, July 6-11, 2014, 2323â€“
2330 (IEEE, 2014).

[25] Skalse, J., Hammond, L., Griffin, C. & Abate,
A. Raedt, L. D. (ed.) Lexicographic MultiObjective Reinforcement Learning. (ed.Raedt,
L. D.) Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,
IJCAI 2022, Vienna, Austria, 23-29 July 2022,
3430â€“3436 (ijcai.org, 2022).

[33] Pirotta, M., Parisi, S. & Restelli, M. Bonet, B.
& Koenig, S. (eds) Multi-Objective Reinforcement Learning with Continuous Pareto Frontier
Approximation. (eds Bonet, B. & Koenig, S.)
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,
2015, Austin, Texas, USA, 2928â€“2934 (AAAI
Press, 2015).

[26] Reymond, M., Bargiacchi, E. & NowÃ©, A. Faliszewski, P., Mascardi, V., Pelachaud, C. & Taylor, M. E. (eds) Pareto Conditioned Networks.
(eds Faliszewski, P., Mascardi, V., Pelachaud, C.
& Taylor, M. E.) 21st International Conference
on Autonomous Agents and Multiagent Systems,
AAMAS 2022, Auckland, New Zealand, May 913, 2022, 1110â€“1118 (International Foundation
for Autonomous Agents and Multiagent Systems
(IFAAMAS), 2022).

[34] Parisi, S., Pirotta, M. & Restelli, M. Multiobjective Reinforcement Learning through Continuous Pareto Manifold Approximation. J. Artif.
Intell. Res. 57, 187â€“227 (2016). https://doi.org/
10.1613/jair.4961 .

[27] Schmidhuber, J. Reinforcement Learning Upside
Down: Donâ€™t Predict Rewards - Just Map Them
to Actions.
CoRR abs/1912.02875 (2019).
1912.02875 .

[35] Wierstra, D. et al. Natural evolution strategies. J.
Mach. Learn. Res. 15 (1), 949â€“980 (2014). https:
//doi.org/10.5555/2627435.2638566 .

[28] Srivastava, R. K., Shyam, P., Mutz, F.,
Jaskowski, W. & Schmidhuber, J. Training
Agents using Upside-Down Reinforcement
Learning.
CoRR abs/1912.02877 (2019).
1912.02877 .

[36] Parisi, S., Pirotta, M. & Peters, J. Manifoldbased multi-objective policy search with sample
reuse. Neurocomputing 263, 3â€“14 (2017). https:
//doi.org/10.1016/j.neucom.2016.11.094 .
[37] Vamplew, P., Dazeley, R., Berry, A., Issabekov,
R. & Dekker, E. Empirical evaluation methods
for multiobjective reinforcement learning algorithms. Mach. Learn. 84 (1-2), 51â€“80 (2011).
https://doi.org/10.1007/s10994-010-5232-5 .

[29] Kumar, A., Peng, X. B. & Levine, S. RewardConditioned Policies. CoRR abs/1912.13465
(2019). 1912.13465 .

20

[38] Schaul, T., Horgan, D., Gregor, K. & Silver, D.
Bach, F. R. & Blei, D. M. (eds) Universal Value
Function Approximators. (eds Bach, F. R. &
Blei, D. M.) Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, Vol. 37 of
JMLR Workshop and Conference Proceedings,
1312â€“1320 (JMLR.org, 2015).

Optimization: An Alternative Approach for Continuous Control. (eds Wallach, H. M. et al.)
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada,
1350â€“1360 (2019).
[45] Eysenbach, B., Gupta, A., Ibarz, J. & Levine, S.
Editors (ed.) Diversity is All You Need: Learning
Skills without a Reward Function. (ed.Editors)
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 (OpenReview.net, 2019).

[39] Colas, C., Karch, T., Sigaud, O. & Oudeyer,
P. Autotelic Agents with Intrinsically Motivated
Goal-Conditioned Reinforcement Learning: A
Short Survey. J. Artif. Intell. Res. 74, 1159â€“1199
(2022). https://doi.org/10.1613/jair.1.13554 .
[40] Bouchacourt, D., Mudigonda, P. K. & Nowozin,
S. Lee, D. D., Sugiyama, M., von Luxburg,
U., Guyon, I. & Garnett, R. (eds) DISCO Nets :
DISsimilarity COefficients Networks. (eds Lee,
D. D., Sugiyama, M., von Luxburg, U., Guyon,
I. & Garnett, R.) Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, 352â€“
360 (2016).

[46] Achiam, J., Edwards, H., Amodei, D. & Abbeel,
P. Variational Option Discovery Algorithms.
CoRR abs/1807.10299 (2018). 1807.10299 .

[41] Kanazawa, T. & Gupta, C. BÃ¤ck, T. et al. (eds)
Sample-based Uncertainty Quantification with a
Single Deterministic Neural Network. (eds BÃ¤ck,
T. et al.) Proceedings of the 14th International
Joint Conference on Computational Intelligence,
IJCCI 2022, Valletta, Malta, October 24-26,
2022, 292â€“304 (SCITEPRESS, 2022).

[48] Campos, V. et al. Editors (ed.) Explore, Discover and Learn: Unsupervised Discovery of
State-Covering Skills. (ed.Editors) Proceedings
of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine
Learning Research, 1317â€“1327 (PMLR, 2020).

[42] Gouttes, A., Rasul, K., Koren, M., Stephan, J.
& Naghibi, T. Probabilistic Time Series Forecasting with Implicit Quantile Networks. CoRR
abs/2107.03743 (2021). 2107.03743 .

[49] Kumar, S., Kumar, A., Levine, S. & Finn, C.
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. & Lin, H. (eds) One Solution is Not
All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. (eds Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M. & Lin, H.) Advances
in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual (2020).

[47] Sharma, A., Gu, S., Levine, S., Kumar, V. &
Hausman, K. Editors (ed.) Dynamics-Aware
Unsupervised Discovery of Skills. (ed.Editors)
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 (OpenReview.net, 2020).

[43] Dabney, W., Ostrovski, G., Silver, D. & Munos,
R. Dy, J. G. & Krause, A. (eds) Implicit
Quantile Networks for Distributional Reinforcement Learning. (eds Dy, J. G. & Krause, A.)
Proceedings of the 35th International Conference on Machine Learning, ICML 2018, StockholmsmÃ¤ssan, Stockholm, Sweden, July 10-15,
2018, Vol. 80 of Proceedings of Machine Learning Research, 1104â€“1113 (PMLR, 2018).

[50] Osa, T., Tangkaratt, V. & Sugiyama, M. Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based
mutual information. Neural Networks 152, 90â€“
104 (2022). https://doi.org/10.1016/j.neunet.
2022.04.009 .

[44] Tessler, C., Tennenholtz, G. & Mannor, S. Wallach, H. M. et al. (eds) Distributional Policy

21

1999], 1008â€“1014 (The MIT Press, 1999).

[51] DJ Strouse, Baumli, K., Warde-Farley, D., Mnih,
V. & Hansen, S. S.
Editors (ed.) Learning more skills through optimistic exploration.
(ed.Editors) The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 (OpenReview.net,
2022).

[59] Mnih, V. et al. Balcan, M. & Weinberger, K. Q.
(eds) Asynchronous Methods for Deep Reinforcement Learning. (eds Balcan, M. & Weinberger, K. Q.) Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016,
Vol. 48 of JMLR Workshop and Conference Proceedings, 1928â€“1937 (JMLR.org, 2016).

[52] Kume, A., Matsumoto, E., Takahashi, K., Ko,
W. & Tan, J. Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of
Robots by Deep Reinforcement Learning. CoRR
abs/1710.06117 (2017). 1710.06117 .

[60] Schulman, J., Levine, S., Abbeel, P., Jordan,
M. I. & Moritz, P. Bach, F. R. & Blei, D. M. (eds)
Trust Region Policy Optimization. (eds Bach,
F. R. & Blei, D. M.) Proceedings of the 32nd
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,
Vol. 37 of JMLR Workshop and Conference Proceedings, 1889â€“1897 (JMLR.org, 2015).

[53] Colas, C., Madhavan, V., Huizinga, J. & Clune,
J. Coello, C. A. C. (ed.) Scaling MAP-Elites
to deep neuroevolution. (ed.Coello, C. A. C.)
GECCO â€™20: Genetic and Evolutionary Computation Conference, CancÃºn Mexico, July 8-12,
2020, 67â€“75 (ACM, 2020).

[61] Schulman, J., Wolski, F., Dhariwal, P., Radford,
A. & Klimov, O. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017).
1707.06347 .

[54] Chalumeau, F. et al. Neuroevolution is a Competitive Alternative to Reinforcement Learning
for Skill Discovery. CoRR abs/2210.03516
(2022). 2210.03516 .

[62] Wang, S., Ali, S., Yue, T., Li, Y. & Liaaen, M.
Dillon, L. K., Visser, W. & Williams, L. A. (eds)
A practical guide to select quality indicators
for assessing pareto-based search algorithms in
search-based software engineering. (eds Dillon,
L. K., Visser, W. & Williams, L. A.) Proceedings
of the 38th International Conference on Software
Engineering, ICSE 2016, Austin, TX, USA, May
14-22, 2016, 631â€“642 (ACM, 2016).

[55] Nilsson, O. & Cully, A. Chicano, F. & Krawiec,
K. (eds) Policy gradient assisted MAP-Elites.
(eds Chicano, F. & Krawiec, K.) GECCO â€™21:
Genetic and Evolutionary Computation Conference, Lille, France, July 10-14, 2021, 866â€“875
(ACM, 2021).
[56] Pierrot, T. et al. Fieldsend, J. E. & Wagner, M. (eds) Diversity policy gradient for sample efficient quality-diversity optimization. (eds
Fieldsend, J. E. & Wagner, M.) GECCO â€™22:
Genetic and Evolutionary Computation Conference, Boston, Massachusetts, USA, July 9 - 13,
2022, 1075â€“1083 (ACM, 2022).

[63] Li, M., Chen, T. & Yao, X. How to Evaluate Solutions in Pareto-Based Search-Based
Software Engineering: A Critical Review and
Methodological Guidance. IEEE Trans. Software Eng. 48 (5), 1771â€“1799 (2022). https:
//doi.org/10.1109/TSE.2020.3036108 .

[57] Williams, R. J. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine Learning 8, 229â€“256
(1992) .

[64] Zitzler, E. & Thiele, L. Multiobjective evolutionary algorithms: a comparative case study and
the strength Pareto approach. IEEE Trans. Evol.
Comput. 3 (4), 257â€“271 (1999). https://doi.org/
10.1109/4235.797969 .

[58] Konda, V. R. & Tsitsiklis, J. N. Solla, S. A.,
Leen, T. K. & MÃ¼ller, K. (eds) Actor-Critic
Algorithms. (eds Solla, S. A., Leen, T. K. &
MÃ¼ller, K.) Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver,
Colorado, USA, November 29 - December 4,

[65] Zitzler, E., Thiele, L., Laumanns, M., Fonseca,
C. M. & da Fonseca, V. G. Performance assessment of multiobjective optimizers: an analysis
and review. IEEE Trans. Evol. Comput. 7 (2),
22

117â€“132 (2003). https://doi.org/10.1109/TEVC.
2003.810758 .

[74] Tedrake, R. Linear Quadratic Regulators (2023).
URL http://underactuated.mit.edu/lqr.html .

[66] Chou, P., Maturana, D. & Scherer, S. A. Precup,
D. & Teh, Y. W. (eds) Improving Stochastic Policy Gradients in Continuous Control with Deep
Reinforcement Learning using the Beta Distribution. (eds Precup, D. & Teh, Y. W.) Proceedings
of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017, Vol. 70 of Proceedings of
Machine Learning Research, 834â€“843 (PMLR,
2017).

[75] Reymond, M. (2022). URL https://github.com/
mathieu-reymond/pareto-conditioned-networks/
tree/main/envs/minecart.

[67] Hansen, N. & Ostermeier, A. Completely Derandomized Self-Adaptation in Evolution Strategies. Evol. Comput. 9 (2), 159â€“195 (2001). https:
//doi.org/10.1162/106365601750190398 .

[77] Klambauer, G., Unterthiner, T., Mayr, A.
& Hochreiter, S.
Self-Normalizing Neural Networks. CoRR abs/1706.02515 (2017).
1706.02515 .

[68] Salimans, T., Ho, J., Chen, X. & Sutskever, I.
Evolution Strategies as a Scalable Alternative to
Reinforcement Learning. CoRR abs/1703.03864
(2017). 1703.03864 .

[78] Blank, J. & Deb, K. pymoo: Multi-Objective
Optimization in Python. IEEE Access 8, 89497â€“
89509 (2020) .

[76] Kingma, D. P. & Ba, J. Bengio, Y. & LeCun, Y.
(eds) Adam: A Method for Stochastic Optimization. (eds Bengio, Y. & LeCun, Y.) 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings (2015).

[69] Hao, J. et al. Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain. IEEE Transactions on Neural
Networks and Learning Systems 1â€“21 (2023).
https://doi.org/10.1109/TNNLS.2023.3236361 .
[70] Yang, R. (2019).
URL https://github.com/
RunzheYang/MORL/blob/master/synthetic/
envs/fruit_tree.py.
[71] Dorato, P., Cerone, V. & Abdallah, C. LinearQuadratic Control: An Introduction (Simon &
Schuster, Inc., USA, 1994).
[72] Peters, J. & Schaal, S. Reinforcement learning of
motor skills with policy gradients. Neural Networks 21 (4), 682â€“697 (2008). https://doi.org/
10.1016/j.neunet.2008.02.003 .
[73] Fazel, M., Ge, R., Kakade, S. M. & Mesbahi,
M. Dy, J. G. & Krause, A. (eds) Global Convergence of Policy Gradient Methods for the Linear
Quadratic Regulator. (eds Dy, J. G. & Krause,
A.) Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
StockholmsmÃ¤ssan, Stockholm, Sweden, July 1015, 2018, Vol. 80 of Proceedings of Machine
Learning Research, 1466â€“1475 (PMLR, 2018).

23

