1

Multi-Agent Reinforcement Learning for Distributed
Resource Allocation in Cell-Free Massive
MIMO-enabled Mobile Edge Computing Network

arXiv:2201.09057v3 [cs.NI] 1 Jul 2023

Fitsum Debebe Tilahun, Ameha Tsegaye Abebe, and Chung G. Kang, Senior Member, IEEE

Abstract—To support the newly introduced multimedia services with ultra-low latency and extensive computation requirements, resource-constrained end-user devices should utilize the
ubiquitous computing resources available at network edge for
augmenting on-board (local) processing with edge computing.
In this regard, the capability of cell-free massive MIMO to
provide reliable access links by guaranteeing uniform quality
of service without cell edge can be exploited for a seamless
parallel computing. Taking this into account, we formulate a joint
communication and computing resource allocation (JCCRA)
problem for a cell-free massive MIMO-enabled mobile edge
computing (MEC) network with the objective of minimizing the
total energy consumption of the users while meeting the ultralow delay constraints. To derive efficient and adaptive JCCRA
scheme robust to network dynamics, we present a distributed solution approach based on cooperative multi-agent reinforcement
learning. The simulation results demonstrate that the proposed
distributed approach can achieve comparable performance to
a centralized deep deterministic policy gradient (DDPG)-based
target benchmark, without incurring additional overhead and
time cost. It is also shown that our approach significantly outperforms heuristic baselines in terms of energy efficiency, roughly
up to 5 times less total energy consumption. Furthermore, we
demonstrate substantial performance improvement compared to
cellular MEC systems.
Index Terms—Joint communication and computing resource
allocation (JCCRA), mobile edge computing, cell-free massive
MIMO, multi-agent reinforcement learning.

I. I NTRODUCTION

T

HE past few years have seen a rapid increase in computationally intensive applications such as face recognition, real time gaming, autonomous driving and so on. One
key aspect of the services is their tight delay requirement
which poses a serious challenge to user equipment (UE)
with limited battery power, computational and storage capabilities. To handle these bottlenecks at the UEs, computation offloading to powerful cloud computing platforms had
been implemented for various services [2], [3]. Recently, a
paradigm that has brought computing resources to the edge
of the network, dubbed as mobile edge computing (MEC),
has been introduced to further reduce the experienced latency.
This work was supported by the National Research Foundation
of Korea (NRF) grant funded by the Korean government (MSIT)
(No.2020R1A2C100998413). (Corresponding author: Chung G. Kang)
Fitsum Debebe Tilahun and Chung G. Kang are with the School
of Electrical Engineering, Korea University, Seoul, South Korea (email: fitsum debebe@korea.ac.kr, ccgkang@korea.ac.kr). Ameha Tsegaye
Abebe is with Samsung Research, Seoul, South Korea (e-mail: amehat.abebe@samsung.com). A part of this work was presented at the IEEE
Global Communications Conference (GLOBECOM) 2022 [1].

As we move towards the next decade, however, the current
cellular MEC systems can hardly keep up with the diverse
and yet, even more stringent requirements of the envisaged
next generation advanced services, and the evolution of the
existing ones [4], [5]. For instance, expected use cases of
the sixth generation (6G) communication network [6], such as
multi-sensory extended reality (augmented, virtual, and mixed
reality), and holographic displays, require extremely high data
rate, improved coverage and spectral efficiency, deterministic
ultra-low latency, and extensive computational capability [7].
To attain the requirements of the envisioned applications,
however, a functionality of reliable virtual bus might be
required for parallel & cooperative processing, i.e., to connect
the users with ubiquitous computing resources at the edge,
such as nearby mobile users (with idle or more powerful processors), and edge/cloud servers (with full-blown computing
resources). In this regard, we consider a cell-free massive
MIMO system [8], one of the potential network infrastructures
envisioned for beyond-5G and 6G networks, for enabling
a seamless computation offloading. A cell-free network can
provide sufficiently fast and reliable access throughout the
coverage of the network, virtually eliminating cell-edge users.
This is accomplished by serving a relatively small number of
users simultaneously from several geographically distributed
access points (APs) that are connected to a central processing
unit (CPU) [9]. In the mobile edge computing network under
consideration, the CPU is equipped with an edge server of
finite capacity to render computing services for resourceconstrained users with time-critical and computationally intensive tasks. As opposed to the unreliable wireless links
in cellular MEC systems, the cell-free access links in our
framework can play the key functionality of realizing virtual
buses for parallel computing, i.e., allowing for instant access
to the edge server in the CPU, irrespective of users’ location,
thus providing energy-efficient and consistently low-latency
computational task offloading.
In this paper, we formulate a joint communication and
computing resource allocation (JCCRA) problem for the cellfree massive MIMO-enabled mobile edge computing network.
Specifically, with the objective of minimizing the total energy
consumption of the users subject to the ultra-low delay requirements, we intend to design a joint allocation of local processor
clock speed and uplink transmission power for each user under
the constantly changing computation task load and wireless
channel conditions. While solving the problem centrally at
the CPU can allow for efficient joint resource allocation,

2

global knowledge of the entire network state is required for
decision making. For instance, information originating from
users’ side such as computing demands, maximum tolerable
task execution deadlines, and channel conditions should be
collected, processed and then the allocation decisions need
to be communicated back to the users within the tight delay
tolerance, incurring prohibitively large overhead and additional
delay for the two-way information exchange. On the other
hand, distributed approaches based on traditional optimization
methods may lead to suboptimal performance due to lack
of a central coordinator. Particularly, in a multi-user MEC
system, where users compete for a limited communication
and computing resources to accomplish computational task
execution under stringent delay constraints, conceiving a globally optimal distributed conventional algorithm, that is also
robust to time-varying computation task loads of the users,
solely based on local observations is a challenging task. In
general, applying conventional optimization methods or other
heuristic algorithms to solve the JCCRA problem in a dynamic
mobile edge network necessitates frequent re-evaluation of
the optimal allocation strategy, following the non-deterministic
task arrivals with time-varying task size and stochastic wireless
channel conditions. It is also difficult to ensure a steady longterm performance with conventional algorithms relying on a
one-shot optimization [11]. Furthermore, the algorithms must
converge within the ultra-low delay tolerance. Consequently,
the application of these methods to support time-critical and
computation-intensive services is critically limited in practice.
The JCCRA problem, therefore, calls for adaptive and robust
solution approaches with reasonable complexity and signaling
overheads.
Recently, integrating the capabilities of artificial intelligence
(AI) in the mobile edge network, referred to as edge intelligence, is considered as one of the key enablers to realize
the highly delay-intolerant advanced multimedia applications,
e.g., extended reality (XR) [12]. In particular, owing to their
adaptability in dynamic systems, reinforcement learning algorithms can be applied at different entities in the mobile
edge network to derive efficient resource allocation strategies. Noting the above-mentioned difficulties associated with
conventional optimization methods, as well as the additional
delay and overhead for a two-way information exchange
incurred in centralized JCCRA approach, we propose a distributed solution approach based on cooperative multi-agent
reinforcement learning (MARL) for the formulated JCCRA
problem. More specifically, each user is implemented as a
learning agent that makes joint resource allocation relying
on local observations only. The agents are trained with the
state-of-the-art multi-agent deep deterministic policy gradient
(MADDPG) algorithm under the framework of centralized
training and decentralized execution. As each agent interacts
continuously with the mobile edge network, it tries to capture
the underlying dynamics of the environment and makes use
of the acquired knowledge for an adaptive and robust joint
resource allocation during the online execution phase. Our
simulation results show that the performance of the proposed
distributed approach outperforms the conventional baselines,
while converging to that of a centralized deep deterministic

Table I: Summary of important notations
Notation

Description

M = {1, 2, ..., M }
K = {1, 2, ..., K}
Ck
Nk

Set of access point (AP) indices
Set of user indices
Cluster of APs to serve user k
Number of APs in cluster for user k

(k)

APn
∆t
tdk
αk
ηk

pmax
/ pk
k
W
Rk
Ncpb
Tk / Tklocal / Tkoffload
fkmax / fklocal
f CPU
fkCPU
comp
ttr
k / tk

tlocal
/ toffload
/ tk
k
k
Eklocal /Ekoffload / Ek

n-th AP in Ck serving user k, n = 1, 2, ..., Nk
Duration of a time step
Application deadline for user k
Proportion of local processor clock speed allocated to user k
Uplink transmit power control coefficient of
user k
Max / allocated uplink transmit power by user
k
System bandwidth
Uplink rate of user k
Number of CPU cycles required to process
one-bit task
Incoming / locally computed / offloaded task
size (in bits) by user k
Max / allocated local processor clock speed (in
Hz) by user k
Computing clock speed of the edge server in
the CPU (in Hz)
Allocated computational resource at the CPU
for user k
Transmission / computing delay experienced
by user k
Local execution / offloading / total delay experienced by user k
Local / offloading / total energy consumption
incurred by user k

policy gradient (DDPG)-based target benchmark, without the
need for central processing and resorting to additional overhead and time cost. Furthermore, a significant performance
improvement in terms of providing energy-efficient and consistently low latency task offloading has been demonstrated as
compared to cellular MEC systems, namely small-cell network
and co-located massive MIMO system. To the best of our
knowledge, this is the very first attempt to solve JCCRA
problem in a distributed fashion for cell-free network enabled
mobile edge computing network. The intelligent distributed
JCCRA scheme coupled with the reliable performance of the
cell-free massive MIMO architecture in our framework can
be a promising means of handling the stringent requirements
of the envisaged multimedia applications, such as holographic
displays, XR and others.
The rest of the paper is organized as follows: In Section
II, we review related works on JCCRA allocation in different
MEC system models. In Section III, we discuss the system
model for the considered mobile edge network and then,
present problem formulation for JCCRA. Section IV discusses
the proposed distributed cooperative multi-agent reinforcement
learning-based solution approach for the JCCRA problem.
Simulation results are presented in Section V, followed by
some concluding remarks in Section VI. For convenience, we
summarized the key notations to be used throughout the paper
in Table I.

3

II. R ELATED W ORK
Recently, several centralized computation offloading and
resource allocation schemes have been proposed for different
MEC system models, targeting latency minimization, energy
consumption reduction, or delay-energy tradeoff balancing.
For instance, [13], and [14] analyze a joint computation
offloading and resource allocation problem to minimize latency in vehicular networks, and in energy harvesting devices,
respectively. Minimization of energy consumption with partial
offloading in a single-user MEC system is investigated in [15],
which was later extended to a multi-user MEC system by
jointly optimizing computation and communication resources
[16]. Therein, dynamic voltage scaling (DVS) technique was
adopted to adjust the frequency of the processor. Additionally,
[17] studies energy-efficient resource allocation in latency
constrained three-node MEC system, while the cost of cooling
edge serve is included for wirelessly powered devices [18].
Meanwhile, energy-delay tradeoff is examined in [19] for a
binary offloading MEC system, in which the users decide
to offload or not. Similarly, power-delay tradeoff for partial
offloading MEC system based on Lyapunov optimization is
studied in [20]. We note that the above centralized schemes are
based on conventional optimization methods that are typically
limited to quasi-static systems, e.g., deterministic task models
and time-invariant channel conditions, or require accurate
knowledge of network wide information [21], such as task
size, application requirements, energy level of battery, and
channel conditions of the users, which is difficult to obtain
in practice under uncertain wireless network within the tight
delay constraint. Therefore, their application to support timecritical and computation-intensive services might be hindered
due to the associated communication and signaling overheads
and additional time cost. Moreover, the centralized algorithms
are not scalable in the sense that their computational complexity increases with system size. It is even more challenging from
a cell-free massive MIMO point of view as we have to deal
with large number of access points (APs) in a joint manner, in
contrast to the above cell-centric MEC models, where a user
is connected to only single AP in each cell.
To relieve the signaling overhead and address scalability issues of the centralized resource allocation schemes, a
plethora of distributed approaches have been proposed. A
design of distributed offloading decision is formulated as a
multi-user offloading game among the users for mobile cloud
computing [22], wireless-powered MEC networks [23], and
MEC-empowered small-cell networks [24]. In [25], offloading
decision and resource allocation problem is solved sequentially
through decomposition technique within proximate clouds in
a distributed fashion. However, the focus of the majority
distributed JCCRA approaches is confined to the design of
offloading decisions. In addition, due to a lack of a central
coordinator, the algorithms may converge to suboptimal solutions. It is worth mentioning that all these schemes are based
on conventional optimization methods that are limited to quasistatic systems with deterministic task arrival and invariant
channel conditions, failing to capture the randomness in practical systems. Consequently, in response to the dynamics in the

environment, the JCCRA problem should be solved frequently,
and the algorithms are expected to converge swiftly within the
task execution deadline. Such approaches are, therefore, not
suited for handling time-sensitive applications.
JCCRA in a dynamic mobile edge computing network
with time-varying channel conditions and stochastic task arrivals require robust solutions that can not only adapt to
the diversities in the network conditions and applicationspecific requirements (e.g., delay constraint), but also perform
well with constrained radio and computational resources. In
this regard, owing to their adaptability in dynamic settings,
reinforcement learning-based frameworks have recently been
proposed to solve joint resource allocation problems in various
MEC systems. For instance, a Q-learning based design of
offloading decisions has been proposed for a single-cell MEC
system [28], while a deep Q-network (DQN) algorithm has
been proposed to solve binary offloading decisions in different
MEC system models in [27], and [28]. Furthermore, [29],
and [30] utilize a deep deterministic policy gradient (DDPG)
algorithm for joint resource allocation schemes in multi-server
MEC systems. In all of these works, however, the decisions
are made by central entities which need to gather systemwide information; thus, the schemes are prone to the shortcomings of centralized resource allocations discussed above.
Moreover, training a centralized super-agent can potentially be
ineffective as the convergence of the training is not guaranteed.
[33] investigates a distributed JCCRA problem in a singlecell MEC system in which each user tries to minimize the
individual cost, defined as a weighted sum of power and delay,
in a non-cooperative fashion, possibly resulting in suboptimal
performance. Moreover, the adopted computational model is
not applicable to handle computation-intensive tasks, requiring
extreme reliability and ultra-low hard deadline.
Recently, there are few works on the cell-free massive
MIMO-based MEC systems. In a MEC system with both
CPU and APs equipped with computing servers, several performance analyses are presented in [32], while considering
coverage radius of the APs and probabilities of offloading to
the servers. From a massive access perspective, the issues of
active user detection and channel estimations are discussed
for a cell-free massive MIMO-based computing framework in
[33]. In contrast to our work, both [34] and [35] do not deal
with the JCCRA problem. A centralized resource allocation
scheme based on an iterative conventional optimization algorithm is proposed in [34]. In a dynamic MEC system, where
optimal solutions are recalculated frequently, the algorithm
may become computationally expensive. From the perspective
of improving energy efficiency, [35] highlights the potential
of integrating reconfigurable intelligent surface (RIS)-aided
cell-free massive MIMO system with wireless energy transfer
(WET) to extend the life-span of devices with limited battery
power. Meanwhile, [36] explores the optimization of transmit
power for access points (APs), and temporary deactivation of
a subset of APs in cell-free network to maximize energy efficiency, while meeting users’ spectral efficiency requirements.

4

sequences ψ1 , ψ2 , ....., ψK , i.e., τp = K. Then, the received
p
pilot vector at the m-th AP, ym
∈ Cτp × 1 , can be represented
as follows:
K q
√ X
p
p
ppk gmk ψk + ωm
,
(1)
ym = τp

Central Processing Unit (CPU)

Computing
Resource

Server

f CPU
Fronthaul links

AP m

UE k
Cluster k

f kmax

k=1

Access
links
UE

UE
Cluster

Cluster

Fig. 1. User-centric cell-free massive MIMO-enabled mobile
edge network: Illustrative system model
III. S YSTEM M ODEL AND P ROBLEM F ORMULATION
A. User-centric Cell-free Massive MIMO: Overview
A cell-free massive MIMO system is considered as a
potential beyond-5G and 6G network infrastructure as it
provides a uniform user experience by eliminating the cell
edge. It exploits joint signal processing from a large number of
distributed access points (APs), which simultaneously serve a
much smaller number of users using the same radio resources.
To attain the tight requirements of the envisaged advanced
services, the capability of cell-free access links to provide
reliable performance without cell edge is essential to play the
key functionality of a virtual bus while offloading an intensive
task for parallel computation at the edge server.
We consider a user-centric mobile edge network with M
single-antenna access points (APs) and K single-antenna
users, where M ≫ K , entailing to the widely known model
for cell-free massive MIMO system [9]. The APs are all
connected to a central processing unit (CPU) via error-free
backhaul links. Let M = {1, 2, ..., M } and K = {1, 2, ..., K}
denote sets of AP and user indices, respectively. We assume
that all APs and UEs are distributed uniformly throughout the
network. Fig. 1 illustrates a system model for cell-free massive
MIMO-enabled mobile edge network. Therein, the CPU is
equipped with a computing server of finite computational
capability to provide computing resources for users with computationally intensive and latency stringent applications. Each
user is also equipped with limited local processing capability.
Let the channel between m-th AP and k-th user is given
1
/
as gmk = βmk2 hmk where βmk is a large-scale channel
gain coefficient and hmk ∼ CN (0, 1) represents small-scale
channel fading. Let τc denote a channel coherence period in
which hmk remains the same, while βmk remains constant for
a time interval which lapses multiple channel coherence times.
The coherence period τc is divided into pilot transmission time
of τp samples and uplink data transmission time of (τc − τp )
samples.
All users simultaneously transmit pilot sequences to the APs
for uplink channel estimation. Let ψk ∈ Cτp × 1 denote a pilot
2
sequence of user k where ∥ψk ∥ = 1. We assume that pilot
contamination can be ignored by picking pairwise orthogonal

p
where ppk denotes the pilot transmit power, and ωm
denotes
a τp -dimensional additive noise vector with independent
and

p
2
identically distributed entries of ωm
∼ CN 0, σm
. Based on
p
the received vector ym
, the least-square (LS) channel estimate
ĝmk can be expressed as [10]
1
p
.
(2)
ĝmk = p p ψkH ym
τp pk

The channel estimates are used to decode the uplink transmitted data of the users. After pilot transmission, the users
transmit offloaded data to the APs. Let xk denote the uplink
u
transmission data of user k. Then, the received signal ym
at
the m-th AP is given as
u
ym
=

K
X
√

pk gmk xk + ωm ,

(3)

k=1

where pk is the uplink data transmit power. Let pk = ηk pmax
,
k
where ηk and pmax
represent
the
power
control
coefficient
and
k
maximum uplink transmit power of UE k, respectively.
In order to ensure scalability of cell-free massive MIMO
system in terms of complexity for pilot detection and data
processing, each UE is served by a limited number of APs,
which forms a cluster. To this end, it is essential to form a
cluster of APs serving each user in a user-centric manner.
Limiting the total number of APs serving the k-th user to Nk ,
such that
n Nk ≤ M , we then form ao user-centric cluster of APs
(k)
(k)
(k)
(k)
Ck = AP1 , AP2 , · · · , APNk , where APn denotes the
n-th AP in the cluster. For simplicity of exposition in the
current discussion, assume that all users have the same size
of cluster, given by C max . In order to construct a cluster Ck ,
we employ a greedy approach which orders the large-scale
channel coefficients βmk of user k with the respective APs in
a descending order and then, includes all APs with the largest
βmk until Nk = C max , k = 1, 2, · · ·, K. Thus, the transmitted
data by user k is only decoded by the APs in Ck . Then, each
∗
u
AP m ∈ Ck transmits the quantity ĝmk
ym
to the CPU via a
fronthaul link. The received soft estimates are combined at the
CPU to decode the data transmitted by user k as follows:
x̂k =

Nk
X

∗
u
ĝmk
ym
.

(4)

m=1

Then, the uplink SINR γk for user k can be expressed as
2

pk

P
m∈Ck

γk =

∗
ĝmk
gmk
2

P
k′ ̸=k

pk′

P
m∈Ck

∗ g
ĝmk
mk′

2
2
+ σm

P

.

(5)

ĝmk

m∈Ck

The uplink rate of user k is then given as Rk =
W ((τc − τp )/τc ) log2 (1 + γk ), where W is the system bandwidth.

5

B. Parallel Computation Model
Without loss of generality, we assume each user k has a
computationally intensive task with Tk (t) bits at the beginning
of every discrete time step t = 1, 2, ..., whose duration is
set to a coherence period, i.e., ∆t = τc . The task sizes in
different time steps are independent and uniformly distributed
over [Tmin , Tmax ], for every user k ∈ K. Let tdk denote a strict
deadline of the k-th user to complete execution of the timesensitive application, e.g., tactile internet [6]. Furthermore, we
assume the tasks are independent and fine grained, i.e., can be
broken into arbitrary portions so that they can be computed
at the user device locally and the edge server in parallel,
similar to [15] and [16]. At time step t, considering the delay
constraint and energy consumption, the k-th user processes
Tklocal (t) bits locally and offloads the remaining Tkoffload (t)
bits to the edge server at the CPU. In the sequel, we describe
the models adopted for local computation and computation
offloading.
1) Local Computation: Let us denote the maximum local
computing clock speed of user k by fkmax (in cycle per
second). Furthermore, let Ncpb denote the number of CPU
cycles to process a one-bit task. Taking energy consumption
and delay requirement of the application into account, the
user decides the proportion αk ∈ [0, 1] for achieving the
local clock speed of fklocal (t) = αk (t) fkmax , which in turn
determines the size of locally computed task. The entire task
Tk (t) is offloaded to the edge server if αk (t) = 0. Meanwhile,
αk (t) = 1 if the whole local processing capability of fkmax is
fully utilized while offloading the remaining task bits to the
edge server. Note that conservative local processing may lead
to high energy consumption, while aggressive computation
offloading by all users may subject some users to service
outage, since offloading all tasks cannot be supported with
finite radio and computation resources. In general, αk is one of
the decision variables for efficient JCCRA, which is governed
by the channel and computational constraints in the local and
edge systems, subject to the energy consumption and delay
constraints.
Given the application deadline tdk to process Tk (t) task
bits subject to the local processing clock speed of fklocal (t),
then the size of locally
computed task
 can be expressed as

td f local (t)
(in bits). Let tlocal
(t)
Tklocal (t) = min Tk (t) , k Nk cpb
k
denote the time taken for local execution at time step t, which
is given as
 local

Tk
(t) Ncpb d
local
tk (t) = min
, tk .
(6)
fklocal (t)
Then, the energy consumed for the local execution is expressed
as
2
Eklocal (t) = ς Tklocal (t) Ncpb fklocal (t) ,
(7)
where ς corresponds to the effective switched capacitance
depending on the chip architecture.
2) Computation Offloading: The k-th user offloads the
remaining task bits Tkoffload (t) = max 0, Tk (t) − Tklocal (t)
to the edge server at the CPU for parallel computation. While
offloading the computational task to the edge server, the
experienced latency can be broken down into transmission

delay for offloading data, processing delay in edge server, and
transmission delay for retrieving the result. Since the retrieved
data size after computation in the server is much smaller as
compared to the offloaded data size, we ignore the retrieving
comp
delay in our formulation. Let ttr
(t) denote the
k (t) and tk
transmission delay and computing delay, respectively, for the
k-th user. To offload Tkoffload (t) data bits to the edge server,
the transmission delay ttr
k (t) is given as
ttr
k (t) =

Tkoffload (t)
,
Rk (ηk , t)

(8)

where Rk (t) is the uplink rate of user k at time step t,
which is expressed according to the discussion in the previous
subsection.
Let f CPU (in cycle per second) denote the computing clock
speed of the edge server in the CPU which is shared among the
users in proportion to the offloaded task size so that each user
can experience uniform computation delay. In other words,
the allocated computational resource at the CPU for user k,
denoted as fk CPU (t), can be expressed as
fkCPU (t) =

Tkoffload (t)
f CPU .
K
P
offload
Tk
(t)

(9)

k=1

Then the computing time tcomp
(t) required to execute bits
k
Tkoffload (t) is given as
tcomp
(t) =
k

Tkoffload (t) Ncpb
.
fkCPU (t)

(10)

Therefore, the total edge-computing delay for executing
Tkoffload (t) bits is given as the sum of the delays for uplink
transmission and computation at the CPU, i.e., toffload
(t) =
k
tr
tcomp
(t)
+
t
(t).
The
corresponding
energy
consumption
is
k
k
given as
Ekoffload (t) = pk (t) ttr
(11)
k (t) ,
where the transmission power at time step t, pk (t), is expressed as pk (t) = ηk (t) pmax
, where pmax
and ηk (t)
k
k
correspond to the maximum uplink transmission power and
power control factor of UE k, respectively. It should be noted
that power control plays a critical role as it governs co-channel
interference among the different clusters during computation
offloading, in addition to determining the energy consumption
for offloading. To that end, ηk is another variable to consider for efficient utilization of limited radio and computing
resources.
According to the communication and parallel computation
models discussed above, the overall experienced latency tk (t)
by user k, to execute Tk (t) bits locally and at the edge is
given by
tk (t) = max(tlocal
(t) , toffload
(t)).
k
k

(12)

The corresponding energy consumption Ek (t) of user k, can
be expressed as
Ek (t) = Eklocal (t) + Ekoffload (t) .

(13)

6

d

min

{αk (t),ηk (t)|∀k}

subject to

max

t α (t)fk


max 0, Tk (t)−min Tk (t), k k
K
K
Ncpb
2
P
P
td α (t)f max
Ncpb αk (t)fkmax + ηk (t) pmax
Ek (t) =
ς min Tk (t), k kN k
k
Rk (t)
cpb
k=1
k=1
!




max
td α (t)fk


min Tk (t), k k
Ncpb
Ncpb
N




cpb
max min 
, tdk  , Tkoffload (t) CPU
+ R 1(t)  ≤ tdk , ∀k
α (t)f max
k

fk

k

(t)

!!

k

0 ≤ αk (t) ≤ 1, ∀k
0 ≤ ηk (t) ≤ 1, ∀k
(14)

C. JCCRA Problem Formulation
o1  t 

a1  t 

Environment
Central Processing Unit (CPU)
Computing
Server
Resource

Agent 1

r1  t 

…

ok  t 

ak  t 
Agent k

rk  t 

Joint Action

a1  t  ,..., ak  t  ,..., aK  t 

…

We now present JCCRA problem formulation in our framework. Specifically, our objective is to minimize the total energy
consumption of all users while meeting the respective userspecific delay requirements by jointly optimizing the local processor speed fklocal (t) = αk (t) fkmax , and uplink transmission
power pk (t) = ηk (t) pmax
, for every user k ∈ K at each time
k
step t, without any prior assumptions on the realizations of the
incoming task size and channel conditions of the users. The
JCCRA problem to jointly determine (αk (t) , ηk (t)) , ∀k ∈ K
can mathematically be formulated by (14), as shown at the
top of this page. Therein, the first constraint ensures that the
task execution delay tk (t) should not exceed the user-specific
delay requirement tdk .
The JCCRA problem in (14) is a stochastic optimization
problem in which the objective function and the first condition
involve constantly changing random variables, following the
dynamics caused by the random incoming task size at each
user, and wireless channel conditions. Hence, the time-varying
optimization variables (αk (t) , ηk (t)) , ∀k ∈ K, should be determined frequently, i.e., at every step t, and rapidly within the
ultra-low deadline. It is, therefore, challenging to efficiently
solve the problem using conventional optimization algorithms
in an adaptive manner with a reasonable computational complexity. Moreover, the distributed nature of the cell-free massive MIMO systems introduces unique challenges for JCCRA.
Unlike the traditional cellular systems, where base stations
have fixed coverage areas with limited number of users, cellfree systems have multiple access points (APs) serving users,
with typically no fixed number of users per AP, leading to
more complex and dynamic interference patterns, which must
be managed effectively for optimal joint resource allocation. It
is also important to mention that solving the JCCRA problem
centrally at the CPU might allow for efficient management of
the available radio and computing resources due to globally
processing network-wide information, however, the associated
overheads and additional delay, i.e., time cost, for the two-way
information exchange are forbiddingly significant, especially
given the ultra-low delay constraints. Thus, to cope up with
the dynamics in the mobile edge network and derive a flexible
and efficient joint resource allocation for every user, we
propose a distributed JCCRA based on cooperative multiagent reinforcement learning. We note that this is the very
first attempt to solve JCCRA problem in a distributed manner
for a cell-free enabled mobile edge computing network. While
alleviating the overheads and time cost associated with the
centralized implementation, the proposed distributed approach

Local Observations

o1  t  ,..., ok  t  ,..., oK  t 

aK  t 

oK  t 
Agent K

rK  t 

Rewards
r1  t  ,..., rk  t  ,..., rK  t 

Fig. 2. Multi-agent reinforcement learning framework: Illustration

enables users to entertain energy-efficient and consistently low
end-to-end delay computational task offloading.
IV. T HE P ROPOSED M ULTI -AGENT R EINFORCEMENT
L EARNING - BASED D ISTRIBUTED JCCRA
In this section, we present a novel distributed JCCRA solution approach based on cooperative multi-agent reinforcement
learning framework, in which each user device is implemented
as an agent for joint resource allocation relying on local
observation only. In particular, the agents learn to map their
local observation into JCCRA actions that minimize the total
energy consumption while meeting the respective application
deadlines.
A. Distributed DRL Formulation for JCCRA
The DRL agents sequentially interact with the mobile edge
network in discrete-time steps to learn optimal joint resource
allocation policies. Let Ok , and S denote the local observation
space of agent k ∈ K, and the complete environment state
space, respectively. As shown in Fig. 2, at time step t, each
agent relies on local observation of the environment state
ok (t) : S 7→ Ok to determine an action ak (t) ∈ Ak ,
from its action space Ak according to the current JCCRA
policy µk . The shared environment collects the joint action
of the agents a (t) = (a1 (t) , ..., aK (t)), and emits the next
observations ok (t + 1) ∈ Ok and real-valued scalar rewards
rk (t) : S × Ak 7→ R for all k ∈ K. The goal of the agents is,

7

therefore, to constantly improve their respective policy until it
converges to the optimal JCCRA policy µ∗k that maximizes the
expected long-term
discounted
T
 cumulative reward, defined as
P t−1
Jk (µk ) = E
ε rk (t) , where ε ∈ [0, 1] is the discount
t=1

Q-value

Q-value


Q
1

Q-value


Q
2

 KQ
….....

Critic

Critic

(o1 , a1 )

(o2 , a2 )

1

 2

Critic

factor and T is the total number of total time steps (horizon).
The optimal JCCRA policy µ∗k is then given as
µ∗k = arg max Jk (µk ) .

(15)

µk

In the sequel, we define the local observation, action, and
reward of each agent k ∈ K for JCRRA at a given time t.
1) Local observation: As discussed in the previous section,
the compute-intensive tasks are subjected to user-specific
application deadline. Specifically, at time step t, the maximum
delay tolerance of the k-th agent to execute incoming task of
Tk (t) bits is given by tdk . Meanwhile, we assume the agent has
a rate assignment result from the previous time step, Rk (t−1).
At the beginning of each time step t, the local observation of
agent k is defined as

∆ 
ok (t) = Tk (t), tdk , Rk (t − 1) .
(16)
Here, Rk (t − 1), which is a function of the channel state
information (CSI), serves as a proxy for the CSI in ok (t)
to abstract the changes in wireless channel conditions. Please
note that directly incorporating the channel estimates between
the APs and the k-th user, i.e., ĝmk , ∀m ∈ M, in the local
observation may not be practical due to the required high
dimensional feedback, simply because the number of APs is
typically much larger than the number of users, i.e. M ≫ K,
in cell-free massive MIMO system.
2) Action: Based on the local observation of the environment, each agent k jointly determines how much local computing resources and uplink transmit power must be allocated
to execute Tk (t) within tdk . Accordingly, the action of the k-th
agent at time step t can be expressed as
∆

ak (t) = [αk (t) , ηk (t)] ,

(17)

where αk (t) ∈ [0, 1], and ηk (t) ∈ [0, 1] are continuous
values that govern the local processor clock speed fklocal (t) =
αk (t) fkmax , and uplink transmit power pk = ηk pmax
, respeck
tively.
3) Reward: The agents must learn cooperative JCCRA
policies that minimize the total energy consumption of all
users while meeting the respective delay constraints. The
immediate reward, therefore, should encapsulate both aspects
of the design objective. Accordingly, to encourage the agents
to maximize the common goal and enforce cooperation among
them, we define the joint reward as
rk (t) = −

K
X

ξk Ek (t) , ∀k ∈ K,

(18)

k=1

where ξk = 1 if tk (t) ≤ tdk , otherwise ξk = 10 to punish
potentially selfish behavior of the agents that lead to failure
in meeting the delay constraint.
We note that the decision variables, i.e., αk and ηk , for
the JCCRA, are continuous-valued. One possible approach

….....

( oK , a K )
 K

….....
Actor

Actor

Agent 1

Agent 2

Decentralized
Execution

Actor
Agent K

Centralized Training

Fig. 3. Centralized training and decentralized execution framework in MADDPG algorithm
is to quantize the continuous action space into a finite set
of actions and employ DRL algorithms designed for discrete
action spaces, such as DQN [37]. Nevertheless, discretization
into high-dimensional action space can cause the issue of
the curse of dimensionality. Limited quantization levels, on
the other hand, may lead to suboptimal policies due to loss
of information from the action space (loss of granularity).
Instead, reinforcement learning methods for continuous action
spaces, such as deep deterministic policy gradient (DDPG)
[38], can efficiently handle the curse of dimensionality in
a continuous domain by employing actor-critic architecture,
deep neural network-based function approximators for policy
and value estimation, and learn compact representations of
the state and action space, without explicitly enumerating
them. However, a direct extension of single-agent DDPG
algorithm, i.e., training each agent independently, faces several
problems when applied to learn coordinated policies. Learning
in a multi-agent setup is more challenging and complex
than in single-agent case, since the environment is no longer
stationary from the agent’s perspective as the other agents
update their policies concurrently. In other words, the agents
face a moving target problem, which may lead to learning
instability. Moreover, the non-stationarity of the environment
compounded on agents’ partial observations of the dynamic
environment can cause shadowed equilibria, a phenomenon
in which agents’ local optimal actions result in globally suboptimal joint action [39]. In the next subsection, we discuss a
multi-agent version of DDPG algorithm, referred to as multiagent deep deterministic policy gradient (MADDPG) [40],
based on the centralized training and decentralized execution
framework, to train all agents for JCCRA decision making.
B. MADDPG Algorithm for Distributed JCCRA
Similar to its single agent counterpart, MADDPG is an
actor-critic policy gradient algorithm. However, in the course
of the offline training phase, the multi-agents are trained at a
central unit which presents an opportunity for sharing extra
information so as to ease the training process. As depicted in
Fig. 3, in addition to the local observations of the environment,

8

Main Networks

a

Actor
 k

Critic
 kQ

sk

Qk ( sk , a )

ok
ak   k  ok 

 

( sk , a, rk , sk )
( sk , a, rk , sk )
( sk , a, rk , sk )
( sk , a, rk , sk )

K

Qk sk , ak k 1

Update  k with
Policy Gradient

Update  k with
Critic Loss
2


K
 rk   Qk sk , ak k 1  Qk ( sk , a ) 


Q

 



    o   Q s , a K 
k
k k 1
 k k k ak


Mini batch





 





Target Networks
Actor target
 k 

Critic target
 kQ

sk

ok

ak   k  ok 

 k    k  1    k 
Soft update

 

Qk sk , ak k 1
K



 kQ   kQ  1    kQ
Soft update

Fig. 4. Illustration of the training procedure in MADDPG algorithm for the k-th agent

the agents are provided with additional information (shown
as dotted lines), such as observations and actions of the
other agents. Specifically, agent k has now access to the
joint action a (t) = (a1 (t) , ..., aK (t)), and full observation
of the environment state sk (t) = (ok (t) , o−k (t)), where
o−k (t) is the local observations of other agents at time step
t. The additional information, in particular the actions taken
by other agents, can enable the multi-agents to overcome the
challenges posed by the non-stationarity of the environment,
thereby successfully capturing the dynamics of the mobile
edge network. The extra information endowed to the agents
during the centralized training phase, however, is discarded
during the execution phase, meaning that the agents fully rely
on their local observations to make JCCRA decisions in a fully
distributed manner.
As shown in Fig. 4, the MADDPG agent k employs two
main deep neural networks: actor network with parameters θkµ
to approximate a joint resource allocation policy µk (ok |θkµ )
Q
and a critic network, parametrized

 by θk , to approximate a
state-value function Qk sk , a|θkQ , along with their respective
′

′

time-delayed copies, θkµ and θkQ which serve as targets. At
time step t, the actor deterministically maps local observation ok (t) to a specific continuous action µk (ok (t) |θkµ ) and
then, a random noise process Nk is added to generate an
exploratory policy such that ak (t) = µk (ok (t) |θkµ ) + Nk (t).
The environment, which is shared among the agents, collects
the joint action a (t) = {ak (t) , ∀k ∈ K} and returns the
immediate reward rk (t) and the next observation ok (t + 1) to
the respective agents. To make use of the experience in later
decision-making steps, i.e., to improve sample efficiency, and
stability of the training, the agent’s transition along with the
extra information ek (t) = (sk (t) , a (t) , rk (t) , sk (t + 1)) is

saved in the replay buffer Dk of the agent k.
To train the main networks, mini batch of B samples
B
, is randomly drawn from the replay
sik , ai , rki , si+1
k
i=1
buffer Dk , denoting a sample index by i. The critic network
is updated to minimize the following loss function:
 

2
1 X  i
Lk θkQ =
yk − Qk sik , ai |θkQ
,
(19)
i
B
=
where yki is the target value expressed as yki
Q′
i+1
i
′
, with a
rk + ε Q k sk , ai+1 |θk
ai+1 ={µ′ k (oi+1
),∀k∈K}
k
discount factor ε . More specifically, the parameters of the
critic network, θkQ , are adjusted by following the gradient
of (19) such that θkQ ← θkQ − βQ ∇θQ Lk θkQ , with a
k
learning rate of βQ for the critic network. On the other hand,
the policy network updates its parameters to maximize the
expected long-term discounted reward J (µk |θkµ ) according to
θkµ ← θkµ − βµ ∇θkµ J (µk |θkµ ), where ∇θQ J (µk |θkµ ) is the
k
multi-agent deterministic policy gradient expressed as
"


1 X
µ
∇θkµ J (µk |θk ) ≈
∇ak Qk sik , ai |θkQ ×
i
B
#
µ
i
∇θkµ µk ok |θk
, (20)
i
ai ={µk (ok ), ∀k∈K}
and βµ is the learning rate of the actor network. The target
parameters in both actor and critic networks are updated with
soft updates as follows:
′

′

θkµ ← τ θkµ + (1 − τ ) θkµ
′

′

θkQ ← τ θkQ + (1 − τ ) θkQ ,
where τ is a constant close to zero.

(21)

9

Algorithm 1: MADDPG Algorithm for JCCRA

C. Complexity Analysis

for each agent k ∈ K do
2
Initialize replay buffer Dk .
3
Initialize theactor network
µk (ok |θkµ ) and critic


In this subsection, we analyze the computational complexity
of the proposed algorithm during training and inference (execution) phases. The computational complexity during training
is primarily determined by a structure of the two primary
networks, namely the actor and critic.
More specifically, for a fully connected actor and critic
networks with L layers, denoting the number of neurons in
the l-th layer of the corresponding networks by Nla and Nlc ,
respectively,
 training step is given
P athea complexity
P c atc each
as O
N
N
+
N
N
l+1
l+1 , ∀l ∈ [0, L − 1], where
l l
l l
l = 0 denotes the respective input layers. Accordingly, the
total computational complexity for training K agents can be
expressed as

X

X
a
c
O KNs B
,
(22)
Nla Nl+1
+
Nlc Nl+1

1

network Qk sk , a|θkQ with weights θkµ and θkQ ,
respectively.
4
Initialize the target networks, µ′k and Q′k with
′
′
weights θkµ and θkQ , respectively.
5 end
6 for each episode e = 1, 2, ... do
7
for each agent k ∈ K do
8
Initialize random process Nk for exploration.
9
Generate initial local observation from the
environment simulator.
10
end
11
for each step t = 1, 2, ... do
12
for each agent k ∈ K do
13
Select action
ak (t) = µk (ok (t) |θkµ ) + Nk (t)
14
end
15
Execute joint action a (t) = (a1 (t) , ...., aK (t))
for each agent k ∈ K do
16
Collect reward rk (t) and observe
sk (t + 1).
17
Store the transition
(sk (t) , a (t) , rk (t) , sk (t + 1)) into Dk .
18
Sample randomminibatch of B transitions
sik , ai , rki , si+1
from Dk .
k
19
Update the critic network by minimizing
the loss given by (19).
20
Update the actor policy using the sampled
policy gradient given by (20).
21
Update the target networks according to
(21).
22
end
23
end
24 end

It is important to note that the agents are trained offline
using the MADDPG algorithm, as summarized in Algorithm
1. The individual pre-trained actor models are employed for
inference during the execution phase. In dynamic environments, the tracking capability of DRL algorithms empowers
the agents to adapt to gradual shifts over time, such as
minor variations in the user-configuration and task profiles
by adjusting their policies based on their observations and
perceived rewards. However, for more significant deviations
from the training environment, such as sudden large influxes of
users or increased shifts in task profiles, the pre-trained actor
models may serve as a valuable starting point for performing on-the-fly fine-tuning. This involves training new models
at the CPU using newly collected data from the changing
environment to mitigate performance degradation caused by
environmental drift, eliminating the necessity for retraining
from the beginning.

l

l

where Ns represents the total number of training steps, and
B is the mini-batch size. On the other hand, the inference
complexity of the agents solely depends
on the individual
actor
L−1

P a a
networks, which can be given as O
Nl Nl+1 .
l=0

V. P ERFORMANCE A NALYSIS
A. Simulation Setup
We start by uniformly distributing M = 100 APs and
K = 10 active users over an area of 1km2 . The APs are
connected to the CPU via ideal fronthaul links. Unless stated
otherwise, 30% of the entire APs are clustered to serve each
user, i.e., Nk = 0.3M , k = 1, 2, · · · , 10. The system bandwidth is set to 5 MHz, which is shared among all users without
channelization. A channel coefficient of the small-scale fading
between the k-th user and m-th AP is set as hmk ∼ CN (0, 1),
which is independent across all APs and users. Furthermore,
a channel gain of large-scale fading between the k-th user and
m-th AP is given as
βmk = 10

P Lmk
10

10

σsh zmk
10

,

(23)

where σsh represents the standard deviation of the shadow
fading, and zmk ∼ N (0, 1). According to the three-slope
model [41], with fixed distance of d0 , and d1 , the path loss
P Lmk between the k-th user and m-th AP with a distance of
dmk apart at the carrier frequency of f is given as


−L − 35log10 (dmk )  if dmk > d1
P Lmk = −L − 10log10 d2mk d1.5
if d0 < dmk ≤ d1 ,
1



−L − 10log10 d20 d1.5
if dmk ≤ d0
1
(24)
where
L = 46.3 + 33.9 log10 (f ) − 13.82 log10 (hAP )
− (1.1 log10 (f ) − 0.7) hu + 1.56 log10 (f ) − 0.8 , (25)
with hAP and hu to denote the antenna height (in meters)
of AP and user, respectively. Further, there is no shadowing
unless dmk > d1 . The values of d0 , and d1 are set to 10 m,
and 50m, respectively, similar to [9]. The edge server at the
CPU has computation capacity of f CPU = 100 GHz, while

10

Table II: Simulation parameters

used in our simulations. The agents are trained with MADDPG
algorithm for 1000 episodes, each consisting of 100 steps. In
the next subsection, we present simulation results averaged
from multiple experiments.

Notation Parameter

Value

f CPU

Computation capacity of the server in the
CPU

100 GHz

fkmax

Maximum local computation capacity

1 GHz

B. Performance Evaluation

Ncpb

Number of cycles to process one bit task

500 cycles/bit

ς

Effective switched capacitance of the
devices

10−27

tdk

Application deadline for user k

1 msec

W

System bandwidth

5 MHz

pmax
k

Maximum uplink transmit power

0.1 W

f

Carrier frequency

1.9 GHz

hAP

AP antenna height

15 m

hu

User antenna height

1.65 m

σsh

Standard deviation of the shadow fading

10 dB

To

Noise temperature

290 K

ηf

Noise figure

9 dB

KB

Boltzmann constant

1.381
×
10−23 J/K

In this section, we present the performance of the proposed
distributed MADDPG-based JCCRA. For performance comparison, we consider the following three benchmarks:
•
Centralized DDPG-based JCCRA scheme: This approach refers to DDPG-based centralized resource allocation scheme at the CPU. We adopt the same neural network structure and other hyperparameters as the MADDPG scheme to train the actor and critic in the single
DDPG agent. However, the agent has full observation
of the environment state, given in dimension of K × 3,
during training and execution. The action has dimension
of K × 2, with each row corresponding to joint resource
allocation for a particular user. Since global information
of the entire network is processed centrally at the CPU to
make JCCRA decisions for all users, we can potentially
obtain the most efficient resource allocation. As a result,
we intend to demonstrate a competitive performance of
the distributed scheme against this baseline, which serves
as a target performance benchmark. However, as discussed in the previous sections, due to the additional time
cost and associated overheads for two-way information
exchange, this scheme may be infeasible to support timesensitive applications.
• Offloading-first with an uplink fractional power control
(FPC) scheme: This approach preferably offloads the
computation to the edge server with the aim of aggressively exploiting the reliable access link provided by cellfree massive MIMO while saving the local processing
energy consumption. The uplink transmit power for the
k-th user is given by the standard fractional power control
(FPC) [42] as follows:

pk = min pmax
, p0 λ−ν
,
(26)
k
k

Table III: Simulation hyperparameters for MADDPG training
Hyperparameter
Discount factor ε
Soft update rate τ
Critic learning rate
Actor learning rate
Mini batch size
Replay buffer size

Value
0.99
0.005
0.001
0.0001
128
10000

all users are equipped with the same computation capacity
of fkmax = 1 GHz. Furthermore, each bit requires 500 clock
cycles to be processed, i.e., Ncpb = 500. A time step ∆t is
set to 1ms, and each user generates a task with a random
size that is uniformly distributed in the range of 2.5 to 7.5
kbits, corresponding to a rate of 2.5 Mbps to 7.5 Mbps, at
the beginning of every time step t. Without loss of generality,
each user demands the task to be processed within the same
interval, i.e., tdk ≤ ∆t = 1 ms, for every time step. Meanwhile,
we assume that a channel does not change over each time step,
i.e., τc = 1 ms. The simulation parameters are summarized in
Table II.
For the proposed MADDPG-based JCCRA, both actor and
critic networks of each agent are implemented with fully
connected neural networks, which consist of three hidden
layers, with 128, 64, and 64 nodes, respectively. All the
hidden layers are activated by ReLu function, while the outputs
of the actors are activated by sigmoid function. The target
networks at each agent are copies of the respective actor and
critic networks. The parameters of critic and actor networks
are updated with adaptive moment (Adam) optimizer with
learning rate 0.001 and 0.0001. Further, the discount factor
and target networks update parameter are set to ε = 0.99
and τ = 0.005, respectively, while the size of a mini batch
is set to 128. Table III summarizes the hyperparameter values

where p0 = −35 dBm, λk =

N
Pk
m=1

βmk , and ν = 0.5.

Local execution-first with an uplink fractional power
control (FPC) scheme: The entire local processing capability is fully utilized, i.e., fklocal = fkmax , and the
remaining task bits are offloaded to the edge server with
uplink transmit power given according to (26).
We note that there is a lack of conclusive proof in the
literature for the convergence of model-free actor-critic methods, or any other reinforcement learning algorithms using
neural networks for function approximation. Consequently,
we have empirically demonstrated the convergence of the
proposed scheme in Fig. 5 by evaluating the total reward
achieved over the training episodes, while comparing it against
other benchmark schemes. In contrast to the two heuristic
approaches, it can clearly be observed that the learning-based
schemes, i.e., centralized DDPG and the proposed MADDPGbased JCCRA, entertain higher reward values, which reflects
•

11

   
    

 

    

 
  

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
 & H Q W U D O L ] H G  ' ' 3 *  - & & 5 $
 / R F D O  ( [ H F X W L R Q  ) L U V W  Z L W K  ) 3 &
 2 I I O R D G L Q J  ) L U V W  Z L W K  ) 3 &

  
  
   

   

   

 ( S L V R G H  , Q G H [

   

    
    

    

Fig. 5. Total reward with training process

lower total energy consumption while meeting the respective
application deadlines of the users. The rewards per episode
have smoothly increased in both learning schemes until they
finally converge. A closer look at the convergence rate, however, reveals that the centralized scheme has converged faster
than the distributed variant. This is attributed to the advantage
of globally processing system wide information at the CPU to
derive JCCRA policy for all users. Nevertheless, as the training
episodes increase, the proposed algorithm has closed the gap
and converged to the target benchmark, i.e., the centralized
counterpart. Moreover, as the agents are exposed to a larger
number of scenarios in the stochastic MEC environment, they
are able to learn and adapt with respect to the dynamics
in the mobile edge network, leading to an efficient joint
resource allocation policy with steady long-term performance.
It is also evident from Fig. 5 that conservative use of local
computation resources leads to high energy consumption,
while aggressively offloading results in ineffective utilization
of the limited communication and computing resources.
In Fig. 6, we compare the average success rate of the
users in meeting the delay constraint of the time-sensitive
applications. For the proposed MADDPG-based and centralized single-agent DDPG-based JCCRA schemes, the success
rate under the simulation setup has gradually increased with
episode indices to eventually reach more than 99%, showing
only a marginal gap from local execution-first approach. The
100% success rate in the latter scheme is, however, at the cost
of unreasonably high energy consumption, which is 5 times
more compared to the learning-based schemes, as observed
in Fig 5. Moreover, the fact that the learning-based schemes
outperform the conventional offloading-first approach implies
that even if cell-free massive MIMO can provide reliable
access links to the users, aggressive computation offloading
can result in performance degradation due to the ineffective
use of the finite communication and computing resources,
thereby subjecting users to service outage due to the failure

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
 & H Q W U D O L ] H G  ' ' 3 *  - & & 5 $
 / R F D O  ( [ H F X W L R Q  ) L U V W  Z L W K  ) 3 &
 2 I I O R D G L Q J  ) L U V W  Z L W K  ) 3 &

   
   

   

   

 ( S L V R G H  , Q G H [

   

    

Fig. 6. Average success rate with training process

 7 R W D O  ( Q H U J \  & R Q V X P S W L R Q   - 

 5 H Z D U G

 

 $ Y H U D J H  6 X F F H V V  5 D W H

 

    
   

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
 0 $ ' ' 3 *  & 5 $    ) 3 & 
 0 $ ' ' 3 *  & 5 $    0 $ ; 

   

    
    
    
    
    
    

   
   
   
   

   

   

   

 ( S L V R G H  , Q G H [

   

    

Fig. 7. Comparison of the proposed JCCRA approach against
variants with computing resource allocation optimization only

to attain the tight delay constraints. It is, therefore, evident
that an adaptive joint resource optimization is very critical to
fully unleash the potential of the available communication and
computing resources in the mobile edge network.
To further investigate a gain obtained by the joint resource
allocation, we then compare the proposed JCCRA scheme with
two variants that adopt MADDPG algorithm for optimizing
computing resource allocation (CRA) only, i.e. without taking
power control into account. Additionally, we consider the
heuristic fractional power control mechanism given in (25)
for the first variant, referred to as MADDPG-CRA (FPC),
while in the other variant, users offload computation with the
maximum transmit power of pk = pmax
, ∀k ∈ K, referred
k
to as MADDPG-CRA (MAX). From Fig. 7, the performance
degradation is readily observed in the variant schemes, which
is reflected by 33% and 42% rise in total energy consumption

12

 $ Y H U D J H  ( Q H U J \  & R Q V X P S W L R Q   - 

   
   
   

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
 / R F D O  ( [ H F X W L R Q  ) L U V W  Z L W K  ) 3 &
 2 I I O R D G L Q J  ) L U V W  Z L W K  ) 3 &

   
   
   
   

   

   

 ( S L V R G H  , Q G H [

   

    

Fig. 8. Per user average energy consumption at the testing
(execution) stage

 7 R W D O  ( Q H U J \  & R Q V X P S W L R Q   - 

  

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
Nk = 10
Nk = 20
Nk = 30
Nk = 50
Nk = 100

 
 
 
 
   

   

   

 ( S L V R G H  , Q G H [

   

    

Fig. 9. Effect of cluster size

of the user devices in MADDPG-CRA (FPC), and MADDPGCRA (MAX), respectively. This can be attributed to two
factors. The first one is due to the critical role of power control
in handling co-channel interference among different clusters
during computation offloading. Thus, the adaptive MADDPGbased power control gives the proposed approach an edge over
the variant schemes. The second factor is that computing and
communication resource management in multi-user mobile
edge network are inherently inter-dependent. Consequently,
the proposed joint communication and computing resource
allocation (JCCRA) optimization approach in the proposed
scheme yields additional gain as compared with the separate
optimization of computing resources in the other two variants.
After demonstrating the learning convergence and performance matching to the target centralized DDPG-based JCCRA
scheme during the offline training stage, we then test the
performance of the proposed JCCRA scheme in the considered

MEC environment. Fig. 8 presents the per-user average energy
consumption of the schemes during the execution stage. As it
can be seen, the proposed approach achieves the lowest energy
consumption, which is roughly 2.5 and 5 times lower than the
offloading-first with FPC and local execution-first with FPC
baseline schemes, respectively. We can deduce that the offline
centralized training has successfully addressed the learning
challenges of the multi-agent setting, enabling the agents
effectively capture the complicated dynamics of the mobile
edge network and perform well during the testing stage. Note
that the execution phase is fully distributed in the sense that
the agents perform inference based on their local observations
only, alleviating the burden associated with signaling overhead
and additional delay in the centralized scheme. Specifically,
in accordance with the learned policy, each MADDPG agent
relies on its own actor network to map their local observation
of the environment into efficient joint resource allocation in
real time.
Next, we investigate the effect of cluster size on the performance of the proposed framework. As discussed in Section
III.A, we form a user-centric cluster of APs Ck to serve a given
user by including Nk APs with the largest βmk (large-scale
channel fading). In Fig. 9, we evaluate the average energy
consumption by the different sizes of cluster, i.e., setting
Nk = 0.1M, 0.2M, 0.3M, 0.5M, M for M = 100. It can
be observed that the case of Nk = 0.1M, ∀k ∈ K has the
highest energy consumption since the cell-free access rate
is highly constrained to support computation offloading. The
agents, therefore, have to either predominantly depend on local
computation or increase the transmission power to meet the
requirements of the multimedia applications, thus incurring
high energy cost. On the other hand, as Nk increases to 0.2M
and 0.3M , the performance improves notably, leading to a
marginal gap compared to the canonical case where all APs
serve every user, i.e., Nk = 100. In fact, the case of Nk = 50
has negligible performance gap in contrast to Nk = 100. This
indicates that with a sufficient number of APs in a cluster, a
user-centric cell-free massive MIMO can effectively establish
a reliable link for computation offloading with negligible
performance loss. It should be noted that the user-centric
approach requires a limited computational complexity for pilot
detection and data processing, in contrast to the canonical
counterpart. In practice, therefore, a legitimate cluster size
must be set to deal with complexity and deployment cost while
solving the JCCRA problem.
The effect of the number of users on the performance of
the proposed JCCRA scheme is investigated in Fig. 10, fixing
M = 100 and Nk = 0.3M . One can observe that the proposed
algorithm has converged in all cases. Since there are abundant
communication and computing resources for a small number
of users in the system, e.g., K = 5, the agents entertain almost
100% success rate in accomplishing the intensive tasks within
the deadline, while collecting the highest average reward,
implying the lowest energy consumption compared to the other
two cases. However, as the number of users increases, the
competition among the users for limited communication and
computing resources becomes more severe. In other words,
the per-user resource reduces with an increasing number of

   
    

   

    

   

 $ Y H U D J H  6 X F F H V V  5 D W H

 $ Y H U D J H  6 X F F H V V  5 D W H

13

   
    

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
K=5
K = 10
K = 15

   
    

   

   

   

 ( S L V R G H  , Q G H [

   

   
   

   

    

 & H O O  I U H H  0 D V V L Y H  0 , 0 2  V \ V W H P
 6 P D O O  F H O O  V \ V W H P
 & R  O R F D W H G  P D V V L Y H  0 , 0 2  V \ V W H P

   

   

   

   

    

(a)

   

 

   

  
  

   

 5 H Z D U G

 3 H U  8 V H U  $ Y H U D J H  5 H Z D U G

(a)

   

 3 U R S R V H G  0 $ ' ' 3 *  - & & 5 $
K=5
K = 10
K = 15

   
   

   

 ( S L V R G H  , Q G H [

   

   

   

 ( S L V R G H  , Q G H [

   

  
  

 & H O O  I U H H  0 D V V L Y H  0 , 0 2  V \ V W H P
 6 P D O O  F H O O  V \ V W H P
 & R  O R F D W H G  P D V V L Y H  0 , 0 2  V \ V W H P

  
    

   

   

   

 ( S L V R G H  , Q G H [

   

    

(b)

(b)

Fig. 10. Effect of the number of users in the mobile edge network.
(a) Average success rate, (b) Per user average reward

Fig. 11. Performance comparison of the proposed algorithm:
Cell-free vs Cellular MEC systems. (a) Average success rate, (b)
Total reward

users, e.g., the link rate decreases as K increases. In particular,
the case K = 15 represents a very constrained scenario
that suffers from a significant outage. Herein, the available
resources are not sufficient enough or the performance requirements are too strict to support all the devices within the
stringent delay constraints, subjecting some users to service
outage. This implies for critically constrained scenarios, either
relaxing the performance requirements, for instance with soft
delay constraints, or injection of more resources are required
to support more users without outage.
Finally, we compare the performance of the proposed
distributed approach in cell-free Massive MIMO with two
cellular-based MEC systems, namely a small-cell system and
a single-cell system with co-located massive MIMO. For a
fair comparison, a base station (BS) in the single-cell system
is equipped with Nk antennas, matching the number of APs
in the cell-free system to simultaneously serve K active
users. Moreover, each user connects to a single AP with
the largest large-scale fading coefficient, i.e., one with index
m = arg max βmk , in the small-cell system, following the
m∈{1,2,···M }

methodology in [9]. Furthermore, to render edge computing
services, both types of cellular MEC systems employ an edge
server with a computation capacity of f E = f CPU , where
f CPU corresponds to the edge server capacity in the cell-free
MEC setup. Moreover, within the context of single-cell mobile
edge computing (MEC), the edge server is located in close
proximity to the base station (BS). Conversely, in the smallcell MEC system, to ensure a fair comparison with the cell-free
MEC model, it is presumed that the edge server is connected
to each access point (AP) through error-free links. In Fig.
11, we evaluate the performance of the proposed distributed
approach in both cell-free and cellular MEC systems, for
K = 10, Nk = 30, and f E = f CPU = 100 GHz. Furthermore,
consistent computational and radio parameters are employed
across all types of MEC systems in our simulations, as outlined
in Table II. Demonstrated in Fig.’s 11(a) and (b), the proposed
algorithm performs significantly better on a cell-free system,
in terms of average success rate and total reward accumulated,
which also reflects the least total energy consumption accord-

14

ing to (18), compared to the cellular MEC systems. The significant interference incurred from the co-located antennas under
maximum ratio combining (MRC) in the single-cell co-located
massive MIMO system, and the spatial co-channel interference
from adjacent cells in the small-cell system has adversely
affected computational task offloading by limiting the uplink
rate. However, benefited from the distributed reliable links,
the algorithm in cell-free MEC is shown to provide the
most energy-efficient and consistently-low computational task
offloading in contrast to the cellular MEC systems, shedding a
light on the potential of our framework to handle the stringent
requirements of advanced multimedia applications.
VI. C ONCLUSION AND F UTURE W ORKS
In this paper, motivated by its capability of realizing a
reliable access link without cell edge, we presented a cellfree massive MIMO-enabled mobile edge network to address
the stringent requirements of the next generation advanced
services. We formulated a user-centric joint communication
and computing resource allocation (JCCRA) problem to minimize the total energy consumption of the users while satisfying the corresponding delay constraints. We then proposed
a distributed solution approach based on cooperative multiagent reinforcement learning framework, wherein each user
is implemented as a learning agent to make joint resource
allocation relying only on local information during execution. The simulation results demonstrate that our distributed
approach has outperformed the heuristic baselines, while
matching the performance of the target centralized DDPGbased benchmark, without resorting to additional overhead
and time cost. Furthermore, the distributed JCCRA in cellfree massive MIMO system is shown to provide a substantial
performance improvement in terms of energy-efficiency and
consistently-low latency task execution, compared to cellular
MEC systems. Such a fully distributed and adaptive framework, enabled by cell-free massive MIMO system, can be
a promising tool to realize edge intelligence for supporting
the envisaged applications in dynamic mobile edge network.
We note that the current framework does not fully capture
certain network dynamics present in practical MEC systems,
including non-stationary scenarios characterized by significant
variations in a user-configuration (e.g., distribution or mobility
patterns) and task profiles (such as priority levels or task size
distribution). We believe that such limitations can be addressed
by incorporating techniques that can enhance generalization
capability and fast adaptability, such as attention mechanism
[43], meta learning [44], and transfer learning [45], which
will be investigated in our future work. It would also be
interesting to extend the current framework by incorporating
mechanisms for dynamically determining a set of access points
(APs) that can adapt to user mobility pattern. One possible
approach is to dynamically configuring a cluster size of users
in response to their mobility while taking into account the
computing requirements [46]. Furthermore, for applications
with a relatively relaxed delay tolerance, spanning multiple
time steps, long-term performance optimization under queue
dynamics can be another possible future direction.

R EFERENCES
[1] F. D. Tilahun, A. T. Abebe and C. G. Kang, “DRL-based Distributed
Resource Allocation for Edge Computing in Cell-Free Massive MIMO
Network,” GLOBECOM 2022 - 2022 IEEE Global Communications
Conference, Rio de Janeiro, Brazil, 2022, pp. 3845-3850.
[2] K. Kumar and Y. Lu, “Cloud Computing for Mobile Users: Can
Offloading Computation Save Energy?,” in Computer, vol. 43, no. 4,
pp. 51-56, April 2010.
[3] S. Barbarossa, S. Sardellitti and P. Di Lorenzo, “Communicating While
Computing: Distributed mobile cloud computing over 5G heterogeneous
networks,” in IEEE Signal Processing Magazine, vol. 31, no. 6, pp. 4555, Nov. 2014.
[4] Samsung Research, “6G: The Next Hyper–Connected Experience for
All”, White Paper, July, 2020.
[5] F. Tang, X. Chen, M. Zhao and N. Kato, “The Roadmap of Communication and Networking in 6G for the Metaverse”, in IEEE Wireless
Communications , June 2022. .
[6] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjoland and F.
Tufvesson, “6G Wireless Systems: Vision, Requirements, challenges,
Insights, and Opportunities“, in Proceedings of the IEEE, vol. 109, no.
7. Pp. 1166-1199, July 2021. .
[7] N. Rajatheva et al., “White paper on broadband connectivity in 6G” ,
arXiv preprint arXiv:2004.14247, 2020..
[8] Özlem Tugfe Demir; Emil Björnson; Luca Sanguinetti, “Foundations of
User-Centric Cell-Free Massive MIMO” , Foundations and Trends® in
Signal Processing, vol. 14, no. 3-4, pp. 162–472, 2021
[9] H. Q. Ngo, A. Ashikhmin, H. Yang, E. G. Larsson and T. L. Marzetta,
”Cell-Free Massive MIMO Versus Small Cells,“ in IEEE Transactions
on Wireless Communications, vol. 16, no. 3, pp. 1834-1850, March
2017.
[10] W. Fan, J. Zhang, E. Bjornson, S. Chen and Z. Zhong, ”Performance
Analysis of Cell-Free Massive MIMO Over Spatially Correlated Fading
Channels,“ ICC 2019 - 2019 International Conference on Communications (ICC), Shanghai, China, 2019, pp. 1-6.
[11] P. Wei, K. Guo, Y. Li, J. Wang, W. Feng, S. Jin, N. Ge, and Y. Liang,
”Reinforcement Learning-Empowered Mobile Edge Computing for 6G
Edge Intelligence, “, in IEEE Access, vol. 10, pp.65156-65192, 2022.
[12] E. Peltonen, M. Bennis, M. Capobianco, M. Debbah et al., “6G White
Paper on Edge Intelligence” , arXiv preprint arXiv:2004.14850, Apri
2020.
[13] J. Zhao, Q. Li, Y. Gong and K. Zhang, “Computation Offloading and
Resource Allocation For Cloud Assisted Mobile Edge Computing in
Vehicular Networks,” in Transactions on Vehicular Technology, vol.
68, no. 8, pp. 7944-7956, Aug. 2019.
[14] Y. Mao, J. Zhang and K. B. Letaief, “Dynamic Computation Offloading
for Mobile-Edge Computing With Energy Harvesting Devices,” in
Journal on Selected Areas in Communications, vol. 34, no. 12, pp. 35903605, Dec. 2016.
[15] Y. Wang, M. Sheng, X. Wang, L. Wang and J. Li, “Mobile-Edge
Computing: Partial Computation Offloading Using Dynamic Voltage
Scaling,” in IEEE Transactions on Communications, vol. 64, no. 10,
pp. 4268-4282, Oct. 2016.
[16] M. Sheng, Y. Wang, X. Wang and J. Li, “Energy-Efficient Multiuser
Partial Computation Offloading With Collaboration of Terminals, Radio
Access Network, and Edge Server,” in IEEE Transactions on Communications, vol. 68, no. 3, pp. 1524-1537, March 2020.
[17] X. Cao, F. Wang, J. Xu, R. Zhang, and S. Cui, “Joint Computation
and Communication Cooperation for Energy-Efficient Mobile Edge
Computing,” in IEEE Internet of Things Journal, vol. 6, no. 3, pp. 41884200, June 2019.
[18] X. Chen et al., “Cooling-Aware Optimization of Edge Server Configuration and Edge Computation Offloading for Wirelessly Powered Devices,”
in IEEE Transactions on Vehicular Technology, vol. 70, no. 5, pp. 50435056, May 2021.
[19] J. Zhang et al., “Energy-Latency Tradeoff for Energy-Aware Offloading
in Mobile Edge Computing Networks,” in IEEE Internet of Things
Journal, vol. 5, no. 4, pp. 2633-2645, Aug. 2018.
[20] Y. Mao, J. Zhang, S. H. Song and K. B. Letaief, “Power-Delay Tradeoff
in Multi-User Mobile-Edge Computing Systems,” 2016 IEEE Global
Communications Conference (GLOBECOM), Washington, DC, 2016.
[21] Y. Mao, C. You, J. Zhang, K. Huang and K. B. Letaief, “A Survey
on Mobile Edge Computing: The Communication Perspective,” in IEEE
Communications Surveys & Tutorials, vol. 19, no. 4, pp. 2322-2358,
Fourthquarter 2017.

15

[22] X. Chen, L. Jiao, W. Li and X. Fu, “Efficient Multi-User Computation
Offloading for Mobile-Edge Cloud Computing,” in IEEE/ACM Transactions on Networking, vol. 24, no. 5, pp. 2795-2808, October 2016.
[23] Y. Zhang, X. Dong and Y. Zhao, “Decentralized Computation Offloading
over Wireless-Powered Mobile-Edge Computing Networks,” 2020 IEEE
International Conference on Artificial Intelligence and Information
Systems (ICAIIS), Dalian, China, 2020.
[24] J. Guo, H. Zhang, L. Yang, H. Ji and X. Li, “Decentralized Computation Offloading in Mobile Edge Computing Empowered Small-Cell
Networks,” 2017 IEEE Globecom Workshops (GC Wkshps), Singapore,
2017, pp. 1-6.
[25] X. Lyu, H. Tian, C. Sengul and P. Zhang, “Multiuser Joint Task
Offloading and Resource Optimization in Proximate Clouds,” in IEEE
Transactions on Vehicular Technology, vol. 66, no. 4, pp. 3435-3447,
April 2017.
[26] K. Jiang, H. Zhou, D. Li, X. Liu and S. Xu, “A Q-learning based Method
for Energy-Efficient Computation Offloading in Mobile Edge Computing,” 2020 29th International Conference on Computer Communications
and Networks (ICCCN), Honolulu, HI, USA, 2020.
[27] C. Li et al., “Dynamic Offloading for Multiuser Muti-CAP MEC
Networks: A Deep Reinforcement Learning Approach,” in IEEE Transactions on Vehicular Technology, vol. 70, no. 3, pp. 2922-2927, March
2021.
[28] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “LearningBased Computation Offloading for IoT Devices With Energy Harvesting,” in IEEE Transactions on Vehicular Technology, vol. 68, no. 2, pp.
1930-1941, Feb. 2019.
[29] Y. Dai, K. Zhang, S. Maharjan and Y. Zhang, “Edge Intelligence for
Energy-Efficient Computation Offloading and Resource Allocation in
5G Beyond,” in IEEE Transactions on Vehicular Technology, vol. 69,
no. 10, pp. 12175-12186, Oct. 2020.
[30] H. Zhang, Y. Yang, X. Huang, C. Fang and P. Zhang, “Ultra-Low
Latency Multi-Task Offloading in Mobile Edge Computing,” in IEEE
Access, vol. 9, pp. 32569-32581, 2021.
[31] Chen Z., Wang, X. “Decentralized computation offloading for multiuser mobile edge computing: a deep reinforcement learning approach”.
J Wireless Com Network 2020, 188 (2020).
[32] S. Mukherjee and J. Lee, “Edge Computing-Enabled Cell-Free Massive
MIMO Systems,” in IEEE Transactions on Wireless Communications,
vol. 19, no. 4, pp. 2884-2899, April 2020.
[33] M. Ke, Z. Gao, Y. Wu, X. Gao and K. -K. Wong, “Massive Access in
Cell-Free Massive MIMO-Based Internet of Things: Cloud Computing
and Edge Computing Paradigms,” in IEEE Journal on Selected Areas
in Communications, vol. 39, no. 3, pp. 756-772, March 2021.
[34] G. Interdonato and S. Buzzi, “The Promising Marriage of Mobile
Edge Computing and Cell-Free Massive MIMO,” ICC 2022 - IEEE
International Conference on Communications, 2022, pp. 243-248.
[35] E. Shi, J. Zhang, S. Chen, J. Zheng, Y. Zhang, D. W. Kwan Ng, and B.
Ai, “Wireless Energy Transfer in RIS-Aided Cell-Free Massive MIMO
Systems: Opportunities and challenges,” IEEE Commun. Mag.,, vol. 60,
no. 3, pp. 26–32, Mar. 2022.
[36] T. Van Chien, E. Björnson and E. G. Larsson, “Joint Power Allocation and Load Balancing Optimization for Energy-Efficient CellFree Massive MIMO Networks,” in IEEE Transactions on Wireless
Communications,, vol. 19, no. 10, pp. 6798-6812, Oct. 2020.
[37] V. Mnih, et al., “Human-level control through deep reinforcement
learning,” Nature, vol. 518, no. 7540, p. 529, 2015.
[38] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D.
Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” in Proc. International Conference on Learning Representations (ICLR), 2016.
[39] Laëtitia Matignon, Guillaume J. Laurent, Nadine Le Fort-Piat., Independent reinforcement learners in cooperative Markov games: a survey
regarding coordination problems. Knowledge Engineering Review, Cambridge University Press (CUP), 2012, 27 (1), pp.1-31.
[40] R. Lowe, Y.Wu, A. Tamar, J. Harb, P. Abbeel, and I.Mordatch, “Multiagent actor-critic for mixed cooperative-competitive environments,” in
Proc. 31st Conf. Neural Inf. Process. Syst., Long Beach, CA, 2017, pp.
1–12.
[41] Ao Tang, JiXian Sun and Ke Gong, “Mobile propagation loss with a low
base station antenna for NLOS street microcells in urban area,” IEEE
VTS 53rd Vehicular Technology Conference, Spring 2001. Proceedings
(Cat. No.01CH37202), Rhodes, Greece, 2001, pp. 333-336 vol.1.
[42] R. Nikbakht, R. Mosayebi and A. Lozano, “Uplink Fractional Power
Control and Downlink Power Allocation for Cell-Free Networks,” IEEE
Wireless Communications Letters, vol. 9, no. 6, pp. 774-777, June 2020.

[43] A. Vaswani et al., “Attention is all you need,” in Neural Information
Processing Systems, pp. 5998-6008, 2017.
[44] Z. Xu, H. van Hasselt, and D. Silver, “Meta-gradient reinforcement
learning,” in Proceedings of the 32nd International Conference on
Neural Information Processing Systems, 2018..
[45] F. L. Da Silva and A. H. R. Costa, “A survey on transfer learning
for multiagent reinforcement learning systems,” Journal of Artificial
Intelligence Research, vol. 64, March 2019.
[46] F. D. Tilahun, A. T. Abebe and C. G. Kang, “Delay-aware Joint Resource
Allocation in Cell-Free Mobile Edge Computing,” 2022 27th Asia Pacific
Conference on Communications (APCC), Jeju Island, Korea, Republic
of, 2022, pp. 81-82.

