Cooperative Reward Shaping for Multi-Agent
Pathfinding

arXiv:2407.10403v1 [cs.AI] 15 Jul 2024

Zhenyu Song, Ronghao Zheng, Senlin Zhang, Meiqin Liu

Abstract—The primary objective of Multi-Agent Pathfinding
(MAPF) is to plan efficient and conflict-free paths for all agents.
Traditional multi-agent path planning algorithms struggle to
achieve efficient distributed path planning for multiple agents. In
contrast, Multi-Agent Reinforcement Learning (MARL) has been
demonstrated as an effective approach to achieve this objective.
By modeling the MAPF problem as a MARL problem, agents can
achieve efficient path planning and collision avoidance through
distributed strategies under partial observation. However, MARL
strategies often lack cooperation among agents due to the absence
of global information, which subsequently leads to reduced
MAPF efficiency. To address this challenge, this letter introduces
a unique reward shaping technique based on Independent QLearning (IQL). The aim of this method is to evaluate the
influence of one agent on its neighbors and integrate such an
interaction into the reward function, leading to active cooperation
among agents. This reward shaping method facilitates cooperation among agents while operating in a distributed manner.
The proposed approach has been evaluated through experiments
across various scenarios with different scales and agent counts.
The results are compared with those from other state-of-theart (SOTA) planners. The evidence suggests that the approach
proposed in this letter parallels other planners in numerous
aspects, and outperforms them in scenarios featuring a large
number of agents.
Index Terms—Multi-agent pathfinding, reinforcement learning,
motion and pathfinding.

I. I NTRODUCTION

M

APF is a fundamental research area in the field of
multi-agent systems, aiming to find conflict-free routes
for each agent. This has notable implications in various
environments such as ports, airports [1], [2], and warehouses
[3], [4], [5]. In these scenarios, there are typically a large
number of mobile agents. These scenarios can generally be
abstracted into grid maps, as illustrated in Fig. 1. The MAPF
methodology is divided mainly into two classes: centralized
and decentralized algorithms. Centralized algorithms [6], [7],
[8] offer efficient paths by leveraging global information but
fall short in scalability when handling a significant number of
agents because of increased computational needs and extended
planning time. In contrast, decentralized algorithms [9], [10]
show better scalability in large-scale environments but struggle
to ensure sufficient cooperation among agents, thereby affecting the success rate in pathfinding and overall efficiency.
MARL techniques, particularly those utilizing distributed
execution, provide effective solutions to MAPF problems. By
modeling MAPF as a partially observable Markov decision
process (POMDP), MARL algorithms can develop policies
that make decisions based on agents’ local observations and
inter-agent communication. Given that MARL-trained policy

Fig. 1. Multi-agent scenarios in real-world and grid map with numerous
mobile robots. In the grid map, blue cells denote agents, yellow cells indicate
their goals, and green cells signify agents that have reached their goals.

networks do not rely on global observations, these methods
exhibit excellent scalability and flexibility in dynamic environments. MARL enhances the success rate and robustness
of path planning, making it particularly suitable for largescale multi-agent scenarios. Algorithms such as [11], [12],
[13] utilize a centralized training distributed execution (CTDE)
framework and foster cooperation between agents using global
information during training. However, they struggle to scale
for larger numbers of agents due to increasing training costs.
In contrast, algorithms based on distributed training distributed
execution (DTDE) frameworks [14], [15] perform well in
large-scale systems. However, due to the lack of global information, individual agents tend to solely focus on maximizing
their own rewards, resulting in limited cooperation among
the whole system. To address this issue, recent work [16],
[17], [18] emerges that improves the performance of RL
algorithms in distributed training frameworks through reward
shaping. However, some of these reward shaping methods are
too computationally complex, while others lack stability. This
instability arises because an agent’s rewards are influenced by
the actions of other agents, which are typically unknown to
this agent.
In this letter, a reward shaping method named Cooperative
Reward Shaping (CoRS) is devised to enhance MAPF efficiency within a DTDE framework. The approach is straightforward and tailored for a limited action space. The cooperative
trend of action ai is represented by the maximum rewards
that neighboring agents can achieve after agent Ai performs
ai . Specifically, when agent Ai takes action ai , its neighbor Aj
traverses its action space, determining the maximum reward
that Aj can achieve given the condition of Ai taking action ai .
The shaped reward is then generated by weighting this index
with the reward earned by Ai itself.
The principal contributions of our work are as follows:

1) This letter introduces a novel reward-shaping method
CoRS, designed to promote cooperation among agents
in MAPF tasks within the IQL framework. This rewardshaping method is unaffected by actions from other
agents, ensures easy convergence during training, and
is notable for its computational simplicity.
2) It can be demonstrated that the CoRS method can alter
the agent’s behavior, making it more inclined to cooperate with other agents, thereby improving the overall
efficiency of the system.
3) The CoRS method is challenged against present SOTA
algorithms, showcasing equivalent or superior performance, eliminating the necessity for complex network
structures.
The rest of this letter is organized as follows: Section II
reviews the related works on MAPF. Preliminary concepts are
presented in Section III. The proposed algorithm is detailed
in Section IV. Section V discusses the experimental results.
Finally, Section VI concludes this letter.
II. R ELATED W ORKS
A. MAPF Based on Reinforcement Learning
RL-based planners such as [19], [20], [21], [22], typically
cast MAPF as a MARL problem to learn distributed policies for agents from partial observations. This method is
particularly effective in environments populated by a large
number of agents. Techniques like Imitation Learning (IL)
often enhance policy learning during this process. Notably, the
PRIMAL algorithm [20] utilizes the Asynchronous Advantage
Actor Critic (A3C) algorithm and applies behavior cloning for
supervised RL training using experiences from the centralized
planner ODrM* [23]. However, the use of a centralized planner
limits its efficiency, as solving the MAPF problem can be timeintensive, particularly in complex environments with a large
number of agents.
In contrast, Distributed Heuristic Coordination (DHC) [24]
and Decision Causal Communication (DCC) [25] algorithms
do not require a centralized planner. Although guided by
an individual agent’s shortest path, DHC innovatively incorporates all potential shortest path choices into the model’s
heuristic input rather than obligating an agent to a specific
path. Additionally, DHC collects data from neighboring agents
to inform its decisions and employs multi-head attention as
a convolution kernel to calculate interactions among agents.
DCC is an efficient algorithm that enhances the performance
of agents by enabling selective communication with neighbors
during both training and execution. Specifically, a neighboring
agent is deemed significant only if its presence instigates a
change in the decision of the central agent. The central agent
only needs to communicate with its significant neighbors.

effect of modifying the reward function in Markov Decision
Processes on optimal strategies, indicating that the addition of
a transition reward function can boost the learning efficiency.
In multi-agent systems, the aim is to encourage cooperation
among agents through appropriate reward shaping methods,
thereby improving overall system performance. [27] probes the
enhancement of cooperative agent behavior within the context
of a two-player Stag Hunt game, achieved through the design
of reward functions. Introducing a prosocial coefficient, the
study validates through experimentation that prosocial reward
shaping methods elevate performance in multi-agent systems
with static network structures. Moreover, [28] promotes cooperation among agents through the reward of an agent whose
actions causally influence the behavior of other agents. The
evaluation of causal impacts is achieved through counterfactual
reasoning, with each agent simulating alternative actions at
each time step and calculating their effect on other agents’
behaviors. Actions that lead to significant changes in the behavior of other agents are deemed influential and are rewarded
accordingly.
Several studies, including [29], [17], [30], utilize the Shapley value decomposition method to calculate or redistribute
each agent’s cooperative benefits. [30] confirms that if a
transferable utility game is a convex game, the MARL reward redistribution, based on Shapley values, falls within
the core, thereby securing stable and effective cooperation.
Consequently, agents should maintain their partnerships or
collaborative groups. This concept is the basis for a proposed
cooperative strategy learning algorithm rooted in Shapley
value reward redistribution. The effectiveness of this reward
shaping method in promoting cooperation among agents,
specifically within a basic autonomous driving scenario, is
demonstrated in the paper. However, the process of calculating
the Shapley value can be intricate and laborious. Ref. [29] aims
to alleviate these computational challenges by introducing
approximation of marginal contributions and employing Monte
Carlo sampling to estimate Shapley values. Coordinated Policy
Optimization (CoPO)[18] puts forth the concept of “cooperation coefficient”, which shapes the reward by taking a weighted
average of an agent’s individual rewards and the average
rewards of its neighboring agents, based on the cooperation
coefficient. This approach proves that rewards shaped in this
manner fulfill the Individual Global Max (IGM) condition.
Findings from traffic simulation experiments further suggest
that this method of reward shaping can significantly enhance
the overall performance and safety of the system. However,
this approach ties an agent’s rewards not merely to its personal
actions but also to those of its neighbors. Such dependencies
might compromise the stability during the training process and
the efficiency of the converged strategy.
III. P RELIMINARY

B. Reward Shaping

A. Cooperative Multi-Agent Reinforcement Learning

The reward function significantly impacts the performance
of RL algorithms. Researchers persistently focus on designing
reward functions to optimize algorithm learning efficiency
and agent performance. A previous study [26] analyzes the

Consider a Markov process involving n agents
{A1 , . . . , An }
:=
A, represented by the tuple
(S, A, O, R, P, γ). Ai represents agent i. At each time
step t, Ai chooses an action ait from its action space

Ai based on its state sit ∈ S i and observation oi ∈ O
according to its policy π i . All ait form a joint action
āt = {a1t , . . . , ant } ∈ (A1 × · · · × An ) := A, and all sit form
a joint state s̄t = {s1t , . . . , snt } ∈ (S 1 × · · · × S n ) := S. For
convenience in further discussions, agent’s local observations
oit are treated as a part of the agent’s state sit . Whenever
a joint action āt is taken, the agents acquire a reward
r̄ = {rt1 , rt2 , ..., rtn } ∈ R, which is determined by the local
reward function rti (s̄t , āt ) : S × A → R with respect to
the joint state and action. The state transition function
P(s̄t+1 |s̄t , āt ) : S × S × A → [0, 1] characterizes the
probability of transition from the current state s̄t to s̄t+1
under action āt . The policy π i (ait |sit ) provides the probability
of Ai taking action ait in the state sit . π̄ represents the
joint policy for all agents. The action-value
function is
PT
given by Qiπ (sit , ait ) = Eτ i ∼πi [ t=0 γ t rti ], where the
trajectory τ i = [sit+1 , ait+1 , sit+2 , ...] represents the path
taken by the agent
Ai . The state-value function is given
P
i
i
i
by Vπi (s) =
a∈Ai π(a|st )Qπ (st , a) and the discounted
cumulative reward is J i = Es0 ∼ρ0 Vπi (s0 ), where ρ0 represents
the initial state distribution. For cooperative MARL tasks,
the objective
Pn is to maximize the total cumulative reward
J tot = i=1 J i for all agents.
B. Multi-agent Pathfinding Environment Setup
This letter adopts the same definition of the multi-agent
path-finding problem as presented in [24], [25]. Consider n
agents in an w × h undirected grid graph G(V, E) with m
obstacles {B 1 , . . . , B m }, where V = {v(i, j)|1 ≤ i ≤ w, 1 ≤
j ≤ h} is the set of vertices in the graph, and all agents
and obstacles located within V . All vertices v(i, j) ∈ V
follow the 4-neighborhood rule, that is, [v(i, j), v(p, q)] ∈ E
for all v(p, q) ∈ {v(i, j ± 1), v(i ± 1, j)} ∩ V . Each agent
Ai has its unique starting vertex si and goal vertex g i ,
and its position at time t is xit ∈ V . The position of the
obstacle B k is represented as bk ∈ V . At each time step,
each agent can execute an action chosen from its action
space Ai = {“Up”, “Down”, “Left”, “Right”, “Stop”}. During the execution process, two types of conflict can arise:
vertex conflict (xit = xjt or xit = bk ) and edge conflict
([xit−1 , xit ] = [xjt , xjt−1 ]). If two agents conflict with each
other, their positions remain unchanged. The subscript t for
all the aforementioned variables can be omitted as long as it
does not cause ambiguity. The goal of the MAPF problem is to
find a set of non-conflicting paths P̄ = {P 1 , P 2 , ..., P n } for
all agents, where the agent’s path P i = [si , xi1 , . . . , xit , . . . , g i ]
is an ordered list of Ai ’s position. Incorporating the setup
of multi-agent reinforcement learning, we design a reward
function for the MAPF task, as detailed in Table I. The
design of the reward function basically follows [24], [25], with
slight adjustments to increase the reward for the agent moving
towards the target to better align with our reward shaping
method.
IV. C OOPERATIVE R EWARD S HAPING
Many algorithms employing MARL techniques to address
the MAPF problem utilize IQL or other decentralized training

TABLE I
R EWARD F UNCTION .
Action
Move (towards goal, away from goal)
Stay (on goal, off goal)
Collision (obstacle/agents)
Finish

Reward
-0.070, -0.075
0, -0.075
-0.5
3

and execution frameworks to ensure good scalability. For
example, [24], [25] are developed based on IQL. Although
IQL can be applied to scenarios with a large number of agents,
it often performs poorly in tasks that require a high degree of
cooperation among agents, such as the MAPF task. This poor
performance arises because, within the IQL framework, each
agent greedily maximizes its own cumulative reward, leading
agents to behave in an egocentric and aggressive manner, thus
reducing the overall efficiency of the system. To counteract
this, this letter introduces a reward shaping method named
Cooperative Reward Shaping (CoRS), and combines CoRS
with the DHC algorithm. The framework combining CoRS
with DHC is shown in Fig. 2. The aim of CoRS is to enhance
performance within MAPF problem scenarios. The employment of reward shaping intends to stimulate collaboration
among agents, effectively curtailing the performance decline
in the multi-agent system caused by selfish behaviors within
a distributed framework.

A. Design of the Reward Shaping Method
In the MAPF task, the policies trained using IQL often result
in scenarios where one agent blocks the path of other agents
or collides with them. To enhance cooperation among agents
within the IQL framework, a feasible approach is reward
shaping. Reward shaping involves meticulously designing the
agents’ reward functions to influence their behavior. For example, when a certain type of behavior needs to be encouraged,
a higher reward function is typically assigned to that behavior. Thus, to foster cooperation among agents in the MAPF
problem, it is necessary to design a metric that accurately
evaluates the collaboration of agents’ behavior and incorporate
this metric into the agents’ rewards. Consequently, as each
agent maximizes its own reward, it will consider the impact
of its action on other agents, thereby promoting cooperation
among agents and improving overall system efficiency.
Let Ici (s̄t , āt ) be a metric that measures the cooperativeness
of the action ait of agent Ai . To better regulate the behavior
of the agent, [27] introduces a cooperation coefficient α and
shapes the agent’s reward function in the following form:
r̃ti = (1 − α)rti + αIci (s̄t , āt ),

(1)

where α describes the cooperativeness of the agent. When
α = 0, the agent completely disregards the impact of its
actions on other agents, acting entirely selfishly and when
α = 1.0, the agent behaves with complete altruism. For agent
Ai , an intuitive approach to measure the cooperativeness of

Fig. 2. The combined framework of CoRS and DHC algorithms. The CoRS component predominantly shapes rewards. DHC component comprises of
communication blocks and dueling Q networks. Notably, the communication block employs a multi-head attention mechanism. The framework utilizes parallel
training as an efficient solution. This process involves the simultaneous generation of multiple actors to produce experiential data and upload it to the global
buffer. The learner then retrieves this data from the global buffer for training purposes, thereby allowing for frequent updates to the Actor’s network.

Ai ’s behavior is to use the average reward of all agents except
Ai :
X
1
Ici (s̄t , āt ) = −i
rj (s̄t , āt ),
(2)
|A |
−i
j∈A

where A−i denotes the set of all agents except Ai , and
|A−i | represents the number of agents in A−i . This reward
shaping method is equivalent to the neighborhood reward
proposed in [18] when dn , the neighborhood radius of the
agent, approaches infinity. The physical significance of Eq. (2)
is as follows: If the average reward of the agents other than
Ai is relatively high, it indicates that Ai ’s actions have not
harmed the interests of other agents. Hence, Ai ’s behavior
can be considered as cooperative. Conversely, if the average
reward of the other agents is low, it suggests that Ai ’s behavior
exhibits poor cooperation.
However, Ici (s̄t , āt ) in Eq. (2) is unstable, which is not only
related to ait but is also strongly correlated with the actions
a−i = {aj |Aj ∈ A, j ̸= i} of other agents. Appendix B-A
provides specific examples to illustrate this instability. Within
the IQL framework, this instability in the reward function
makes learning of Q-values challenging and can even prevent
convergence. To address this issue, this letter proposes a new
metric to assess the cooperativeness of agent behavior. The
specific form of this metric is as follows:
Ici (s̄t , āt ) = max
−i
a

X rj (s̄t , {ai , a−i })
t
.
−i |
|A
−i

(3)

j∈A

Here āt = {ait , a−i
t }. The use of the max operator in Eq. (3) eliminates the influence of a−i
on Ici
t
i
while reflecting
P the impact of at on other agents. The
term maxa−i j∈A−i rj (s̄t , {ait , a−i }) represents the maximum reward that all the agents except Ai can achieve
under the condition
of s̄t and ait , whereas the acP
j
i −i
tual value of
j∈A−i r (s̄t , {at , at }) is determined by
−i
i
at when at is given. Accordingly,
it holds true unP
j
der any circumstances that
r
(s̄t , {ait , a−i
≤
−i
t })
j∈A

Fig. 3. An example of the reward shaping method.

P
maxa−i j∈A−i rj (s̄t , {ait , a−i }). The complete reward shaping method is then as follows:
r̃ti (s̄t , ait ) = (1 − α)rti + α max
a−i

X rj (s̄t , {ai , a−i })
t
. (4)
|A−i |
−i

j∈A

Fig. 3 gives an one step example to illustrate how this rewardshaping approach Eq. (4) is calculated and promotes interagent cooperation when α = 12 . There are two agents, A1
and A2 , along with some obstacles. Without considering A2 ,
A1 has two optimal actions: “Up” and “Right”. Fig. 3 also
illustrates the potential rewards that A2 may receive when A1
takes different actions. Table II presents the specific reward
values for each agent in this example. According to the reward
shaping method proposed in Eq. (4), choosing the “Right”
action could yield a higher one-step reward, and in such case,
A2 should also take action“Right”. Appendix B-B provides
another analysis of this example from the vantage point of the
cumulative rewards of the agent, further elucidating how the
reward shaping method Eq. (4) facilitates cooperation among
agents.
B. Analysis of Reward Shaping
This section provides a detailed analysis of how reward
shaping method Eq. (4) influences agent behavior. For ease

Assumption 2. During the interaction process between Ai
and Ã−i , neither Ai nor Ã−i has ever reached its respective
endpoint.

TABLE II
R EWARDS FOR AGENTS U NDER D IFFERENT ACTIONS .
a1
→
↑

↑

↓

a2
←

-0.3
-0.3

-0.3
-0.3

-0.075
-0.075

→

wait

-0.070
-0.3

-0.075
-0.075

max r2

r̃1

-0.07
-0.075

-0.140
-0.145

Here, Ã−i not reaching destination means ∀j ∈ A−i , Aj
has not reached its destination. Based on the assumptions, it
can be proven that:
Theorem 1. Assume Assumps. 1 and 2 hold. Then when α =
−i
1
i
i
−i
tot
i −i
2 , Qπ∗i (s̄t , a ), Qπ∗−i (s̄t , a ) and Qπ̄∗ (s̄t , {a , a }) satisfy
the IGM condition.

of discussion, the subsequent analysis will be conducted from
the perspective of Ai .
In the MAPF problem, the actions of agents are tightly Proof. The proof is provided in Appendix A.
coupled. The action of Ai may impact Aj , and subsequently,
Theorem 1 demonstrates that the optimal policy, trained
the action of Aj may also affect Ak . Thus, the action of Ai
indirectly affect Ak . This coupling makes it challenging to using the reward function given by Eq. (4), maximizes the
analyze the interactions among multiple agents. To mitigate overall rewards of Ai and the virtual agents Ã−i , rather
this coupling, we consider all agents in A−i as a single virtual than selfishly maximizing its own cumulative reward. This
−i
−i
−i
implies that Ai will actively cooperate with other agents, thus
agent Ã−i . The action of
PÃ is aj and its reward r is their
1
average reward |A−i | j∈A−i r . The use of average here improving the overall efficiency of the system. It should be
ensures that Ai and Ã−i are placed on an equal footing. The noted that the impact 1of α on the agent’s behavior is complex.
virtual agent Ã−i must satisfy the condition that no collisions The choice of α = 2 here is a specific result derived from
−i
occur between the agents constituting Ã−i . It is important to using the virtual agent Ã . Although this illustrates how the
i
−i
note that the interaction between A and Ã is not entirely reward shaping method in Eq. (4) induces cooperative behavior
1
equivalent to the interaction between Ai and agents in A−i . in agents, it does not imply that 2 is the optimal value of α
−i
−i
This is because when considering the agents in A as Ã , in all scenarios.
all robots within Ã−i fully cooperate. In contrast, treating
these agents as independent individuals makes it difficult to C. Approximation
ensure full cooperation. Nonetheless, Ã−i can still represent
Theorem 1 illustrates that the reward shaping method
the ideal behavior of agents in A−i . Therefore, analyzing
Eq.
(4) can promote cooperation among agents in MAPF
i
−i
the interaction between A and Ã can still illustrate the
tasks.
However, calculating Eq. (3) requires traversing the
impact of reward shaping on agent interactions. Consider the
joint
action
space of A−i . For the MAPF problem where the
i
−i
interaction between
 t A and Ã in the time period [ts , te ].
action space size for each agent is 5, the joint action space
e
P
γ t−ts r̃i (s̄t , ait ) s̄ts = s̄, aits = ai and for n agents contains up to 5n states, significantly reducing
Qiπ (s̄, ai ) = Eτ
t=tt s
 the computational efficiency of the reward shaping method.
e
P
−i
−i
−i
−i
t−ts −i
−i
Qπ (s̄, a ) = Eτ
γ
r̃ (s̄t , at ) s̄ts = s̄, ats = a
Therefore, we must simplify the calcut=ts
lation of Eq. (3). Before approximating
are the cumulative reward of Ai and Ã−i . Qtot (s̄, ā) =
Eq. (3), we first define the neighbors
te
P
Eτ
γ t−ts (ri + r−i ) s̄ts = s̄, āts = ā is the cumulative of an agent in the grid map as follows:
t=ts
Aj is considered a neighbor of Ai if
reward for both Ai and Ã−i . The optimal policies the Manhattan distance between them
−i
π∗i = argmaxπ Qiπ (s̄, ai ), π∗−i = argmaxπ Q−i
π (s̄, a ),
is no more than dn , where dn is the
tot
i
−i
and π̄∗ = argmaxπ Qπ (s̄, ā). A and Ã
will select neighborhood radius of the agent. If
their actions according to ait = argmaxai Qiπ (s̄t , ai ) and the Manhattan distance between two Fig. 4. Neighbors of Ai
when dn = 2.
−i
−i
i
a−i
= argmaxa−i Q−i
t
π (s̄t , a ). We hope that at and at
agents is no more than 2, a conflict may arise between them
tot
tot
i −i
will collectively maximize Q (s̄, ā) = Q (s̄, {at , at }). in a single step. Therefore, we choose d = 2, which maxin
Specifically,
mally
simplifies
interactions
between
agents
while adequately
(
)
i
i
considering
potential
collisions.
Let
N
denote
the set of
argmax
Q
(s̄,
a
),
i
i
a
π∗i
i
argmaxā Qtot
.
−i
−i
π̄∗ (s̄, ā) =
neighboring
agents
of
A
,
and
|N
|
represent
the
number of
i
argmaxa−i Qπ−i (s̄, a )
∗
neighbors of Ai . Fig. 4 illustrates the neighbors of Ai when
dn = 2.
That is, Qiπi , Q−i
, and Qtot
π̄∗ (s̄, ā) satisfy the Individualπ∗−i
∗
Considering that in MAPF tasks, the direct interactions
Global-Max (IGM) condition. The definition of the IGM
among
agents are constrained by the distances between them,
condition can be found in [31]. We introduce the following
the
interactions
between any given agent and all other agents
two assumptions to facilitate the analysis.
can be simplified to the interactions between the agent and its
Assumption 1. The reward for the agents staying at the target neighbors. That is:
point is 0, while the rewards for both movement and staying
1 X j
at non-target points are rm < 0, and the collision reward
Ici (s̄t , āt ) ≈ max
r (s̄t , {ait , aNi }),
N
a i |Ni |
rc < rm .
j∈Ni

adjusting α. Some work employs zero-order optimization
of stochastic gradient estimation to handle non-differentiable
optimization problems, that is, estimating the gradient by finite
differences of function values. We also use differences in the
cumulative reward of the agent to estimate the gradient of the
cooperation coefficient α:

Fig. 5. Training losses of two different reward shaping methods in 10 × 10
map with 10 agents. Eq. (5) significantly reduces the training loss compared
to the method uses Eq. (2).

where aNi represents the joint actionsPof all neighbors of Ai .
Next, we approximate maxaNi |N1i | j∈Ni rj (s̄t , {ait , aNi })
P
using |N1i | j∈Ni maxaj rj (s̄t , {ait , aj }). This approximation
implies that we consider the interaction between agent Ai
and one of its neighbors Aj at a time. The computational
complexity of the approximate Ic is much lower than the
original complexity.
To approximate s̄t , we posit that when Ai can observe its
two-hop neighbors, its observation is considered sufficient,
and its state sit can approximate s̄t to a certain extent, i.e.,
r̃i (s̄t , ait ) = r̃i (sit , ait ). When dn = 2, the minimum observation range includes all grids within a Manhattan distance of
no more than 2 · dn = 4 from the agent. For convenience, a
9 × 9 square is chosen as the agent’s field of view (FOV),
consistent with the settings in [24] and covering the agent’s
minimum observation range. Consequently, the final reward
shaping method is given by:
1 X
r̃i (si , ai ) = (1−α)rti +α
max rj (sit , {ait , aj }). (5)
|Ni |
aj ∈A
j∈Ni

This reward shaping method possesses the following characteristics:
1) r̃i for Ai is explicitly dependent on ai and is explicitly
independent of aj for j ̸= i. This property improves the
stability of the agent’s reward function and facilitates
the convergence of the network, as shown in Fig. 5.
2) This method of reward shaping is applicable to MAPF
scenarios where the relationship among agents’ neighbors is time-varying.
3) This method, which considers the interaction between
just two agents at a time, dramatically reduces the
computational difficulty of the reward.
D. Adjustment of the cooperation coefficient
For Eq. (5), it is necessary to adjust α so that the agent can
balance its own interests with the interests of other agents. To
find an appropriate cooperation coefficient α, it is advisable
to examine how the policies trained under different α perform
in practice and then employ a gradient descent algorithm
based on the performance of the policy to optimize α. The
cumulative reward J of the agent often serves as a performance
metric in RL. However, in environments with discrete action
and state spaces, the cumulative reward J is non-differentiable
with respect to the α, presenting a significant challenge for

ˆ
J(α + u) − J(α)
∂J(α)
=
, u ∈ [−ϵ, ϵ],
∂α
u
where ϵ represents the maximum step size of the differential.
If the update step length is too small, the updates are halted.
To expedite the training process, a method of fine-tuning the
network is employed. That is, after each adjustment of α, the
network is not trained from the initial state. Instead, fine-tuning
training is performed based on the optimal policy network
obtained previously. This training method improves the speed
of the training process.
V. E XPERIMENTS
We conducted our experiments in the standard MAPF
environment, where each agent has a 9 × 9 FOV and can
communicate with up to two nearest neighbors. Following the
curriculum learning method [32] used by DHC, we gradually
introduced more challenging tasks to the agents. Training
began with a simple task that involved a single agent in a
10 × 10 environment. Upon achieving a success rate above
0.9, we either added an agent or increased the environment
size by 5 to establish two more complex tasks. The model
was ultimately trained to handle 10 agents in a 40 × 40
environment. The maximum number of steps per episode was
set to 256. Training was carried out with a batch size of 192,
a sequence length of 20, and a dynamic learning rate starting
at 10−4 , which was halved at 100,000 and 300,000 steps, with
a maximum of 500,000 training steps. During fine-tuning, the
learning rate was maintained at 10−5 . Distributed training was
used to improve efficiency, with 16 independent environments
running in parallel to generate agent experiences, which were
uploaded to a global buffer. The learner then retrieved these
data from the buffer and trained the agent’s strategy on a GPU.
CoRS-DHC adopted the same network structure as DHC. All
training and testing were performed on an Intel® i5-13600KF
and Nvidia® RTX2060 6G.
A. Impact of Reward Shaping
Following several rounds of network fine-tuning and updates
to the cooperation coefficient α, a value of α = 0.1675
and strategy π are obtained. Upon obtaining the final α
and π, the CoRS-DHC-trained policy is compared with the
original DHC algorithm-trained policy to assess the effect of
the reward shaping method on performance improvement. For
a fair comparison, the DHC and CoRS-DHC algorithms are
tested on maps of different scales (40 × 40 and 80 × 80) with
varying agent counts {4, 8, 16, 32, 64}. Recognizing the larger
environmental spatial capacity of the 80 × 80 map, a scenario
with 128 agents is also introduced for additional insights.
Each experimental scenario includes 200 individual test cases,
maintaining a consistent obstacle density of 0.3. The maximum

Fig. 6. The Enhancement of DHC through CoRS. This figure demonstrates
the success rate of CoRS-DHC and DHC under different testing scenarios.
The table provides the average steps required to complete tasks in different
testing environments.
TABLE III
AVERAGE S TEPS WITH AND WITHOUT C O RS
Steps
Agents
4
8
16
32
64
128

Map Size 40 × 40
DHC
CoRS-DHC
64.15
50.36
77.67
64.77
86.87
68.48
115.72
95.42
179.69
151.02

Map Size 80 × 80
DHC
CoRS-DHC
114.69
92.14
133.39
109.15
147.55
121.25
158.58
137.06
183.44
153.06
213.75
193.50

time steps for the 40 × 40 and 80 × 80 maps are 256 and 386,
respectively.
From the experimental results illustrated in Fig. 6, it is
evident that our proposed CoRS-DHC significantly outperforms the existing DHC algorithm in all test sets. In high
agent density scenarios, such as 40 × 40 with 64 agents
and 80 × 80 with 128 agents, our CoRS-DHC improves the
pathfinding success rate by more than 20% compared to DHC.
Furthermore, as shown in Table III, CoRS effectively reduces
the number of steps required for all agents to reach their target
spot in various test scenarios. Although the DHC algorithm
incorporates heuristic functions and inter-agent communication, it still struggles with cooperative issues among agents. In
contrast, our CoRS-DHC promotes inter-agent collaboration
through reward shaping, thereby substantially enhancing the
performance of the DHC algorithm in high-density scenarios.
Notably, this significant improvement was achieved without
any changes to the algorithm’s structure or network scale,
underscoring the effectiveness of our approach.
B. Success Rate and Average Step
Additionally, the policy trained through CoRS-DHC is
compared with other advanced MAPF algorithms. The current
SOTA algorithm, DCC[25], which is also based on reinforcement learning, is selected as the main comparison object, with
the centralized MAPF algorithm ODrM*[6] and PRIMAL[20]
(based on RL and IL) serving as references. DCC is an efficient
model designed to enhance agent performance by training
agents to selectively communicate with their neighbors during
both training and execution stages. It introduces a complex
decision causal unit to each agent, which determines the appropriate neighbors for communication during these stages. Conversely, the PRIMAL algorithm achieves distributed MAPF
by imitating ODrM* and incorporating reinforcement learning

Fig. 7. Success rate and average steps across different testing scenarios.

algorithms. ODrM* is a centralized algorithm designed to
generate optimal paths for multiple agents. It is one of the
best centralized MAPF algorithms currently available. We use
it as a comparative baseline to show the differences between
distributed and centralized algorithms. The experimental results are demonstrated in Fig. 7:
The experimental results indicate that our CoRS-DHC algorithm consistently exceeds the success rate of the DCC
algorithm in the majority of scenarios. Additionally, aside
from the 40 × 40 grid with 64 agents, the makespan of the
policies trained by the CoRS-DHC algorithm is comparable to
or even shorter than that of the DCC algorithm across other
scenarios. These results clearly demonstrate that our CoRSDHC algorithm achieves a performance comparable to that
of DCC. However, it should be noted that DCC employs a
significantly more complex communication mechanism during
both training and execution, while our CoRS algorithm only
utilizes simple reward shaping during the training phase.
Compared to PRIMAL and DHC, CoRS-DHC exhibits a
remarkably superior performance.

VI. C ONCLUSION
This letter proposes a reward shaping method termed CoRS,
applicable to the standard MAPF tasks. By promoting cooperative behavior among multiple agents, CoRS significantly
improves the efficiency of MAPF. The experimental results indicate that CoRS significantly enhances the performance of the
MARL algorithms in solving the MAPF problem. CoRS also
has implications for other multi-agent reinforcement learning
tasks. We plan to further extend the application of this rewardshaping strategy to a wider range of MARL environments in
future exploration.
R EFERENCES
[1] Fu Chen. Aircraft taxiing route planning based on multi-agent system.
In 2016 IEEE Advanced Information Management, Communicates,
Electronic and Automation Control Conference (IMCEC), pages 1421–
1425, 2016.
[2] Jiankun Wang and Max Q-H Meng. Real-time decision making and
path planning for robotic autonomous luggage trolley collection at
airports. IEEE Transactions on Systems, Man, and Cybernetics: Systems,
52(4):2174–2183, 2021.
[3] Oren Salzman and Roni Stern. Research challenges and opportunities in
multi-agent path finding and multi-agent pickup and delivery problems.
In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems, pages 1711–1715, 2020.
[4] Altan Yalcin. Multi-Agent Route Planning in Grid-Based Storage
Systems. doctoralthesis, Europa-Universität Viadrina Frankfurt, 2018.
[5] Kevin Nagorny, Armando Walter Colombo, and Uwe Schmidtmann. A
service-and multi-agent-oriented manufacturing automation architecture:
An iec 62264 level 2 compliant implementation. Computers in Industry,
63(8):813–823, 2012.
[6] Cornelia Ferner, Glenn Wagner, and Howie Choset. Odrm* optimal
multirobot path planning in low dimensional search spaces. In 2013
IEEE international conference on robotics and automation, pages 3854–
3859. IEEE, 2013.
[7] Teng Guo and Jingjin Yu. Sub-1.5 time-optimal multi-robot path
planning on grids in polynomial time. arXiv preprint arXiv:2201.08976,
2022.
[8] Han Zhang, Jiaoyang Li, Pavel Surynek, TK Satish Kumar, and Sven
Koenig. Multi-agent path finding with mutex propagation. Artificial
Intelligence, 311:103766, 2022.
[9] Glenn Wagner and Howie Choset. Subdimensional expansion for
multirobot path planning. Artificial intelligence, 219:1–24, 2015.
[10] Na Fan, Nan Bao, Jiakuo Zuo, and Xixia Sun. Decentralized multirobot collision avoidance algorithm based on rssi. In 2021 13th International Conference on Wireless Communications and Signal Processing
(WCSP), pages 1–5. IEEE, 2021.
[11] Chunyi Peng, Minkyong Kim, Zhe Zhang, and Hui Lei. Vdn: Virtual
machine image distribution network for cloud data centers. In 2012
Proceedings IEEE INFOCOM, pages 181–189. IEEE, 2012.
[12] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic
value function factorisation for deep multi-agent reinforcement learning.
Journal of Machine Learning Research, 21(178):1–51, 2020.
[13] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas
Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018.
[14] Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang, Tamer
Başar, and Ji Liu. A multi-agent off-policy actor-critic algorithm for
distributed reinforcement learning. IFAC-PapersOnLine, 53(2):1549–
1554, 2020. 21st IFAC World Congress.
[15] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan
Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. PloS one,
12(4):e0172395, 2017.
[16] Sirui Chen, Zhaowei Zhang, Yaodong Yang, and Yali Du. Stas: Spatialtemporal return decomposition for multi-agent reinforcement learning.
arXiv preprint arXiv:2304.07520, 2023.

[17] Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Fei
Wu, and Jun Xiao. Shapley counterfactual credits for multi-agent
reinforcement learning. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining, pages 934–942,
2021.
[18] Zhenghao Peng, Quanyi Li, Ka Ming Hui, Chunxiao Liu, and Bolei
Zhou. Learning to simulate self-driven particles system with coordinated policy optimization. Advances in Neural Information Processing
Systems, 34:10784–10797, 2021.
[19] Binyu Wang, Zhe Liu, Qingbiao Li, and Amanda Prorok. Mobile robot
path planning in dynamic environments through globally guided reinforcement learning. IEEE Robotics and Automation Letters, 5(4):6932–
6939, 2020.
[20] Guillaume Sartoretti, Justin Kerr, Yunfei Shi, Glenn Wagner, TK Satish
Kumar, Sven Koenig, and Howie Choset. Primal: Pathfinding via
reinforcement and imitation multi-agent learning. IEEE Robotics and
Automation Letters, 4(3):2378–2385, 2019.
[21] Zuxin Liu, Baiming Chen, Hongyi Zhou, Guru Koushik, Martial Hebert,
and Ding Zhao. Mapper: Multi-agent path planning with evolutionary
reinforcement learning in mixed dynamic environments. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 11748–11754. IEEE, 2020.
[22] Qingbiao Li, Fernando Gama, Alejandro Ribeiro, and Amanda Prorok.
Graph neural networks for decentralized multi-robot path planning.
In 2020 IEEE/RSJ international conference on intelligent robots and
systems (IROS), pages 11785–11792. IEEE, 2020.
[23] Cornelia Ferner, Glenn Wagner, and Howie Choset. Odrm* optimal
multirobot path planning in low dimensional search spaces. In 2013
IEEE International Conference on Robotics and Automation, pages
3854–3859, 2013.
[24] Ziyuan Ma, Yudong Luo, and Hang Ma. Distributed heuristic multiagent path finding with communication. In 2021 IEEE International
Conference on Robotics and Automation (ICRA), pages 8699–8705.
IEEE, 2021.
[25] Ziyuan Ma, Yudong Luo, and Jia Pan. Learning selective communication
for multi-agent path finding. IEEE Robotics and Automation Letters,
7(2):1455–1462, 2022.
[26] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance
under reward transformations: Theory and application to reward shaping.
In Icml, volume 99, pages 278–287, 1999.
[27] Alexander Peysakhovich and Adam Lerer. Prosocial learning agents
solve generalized stag hunts better than selfish ones. arXiv preprint
arXiv:1709.02865, 2017.
[28] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre,
Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social
influence as intrinsic motivation for multi-agent deep reinforcement
learning. In International conference on machine learning, pages 3040–
3049. PMLR, 2019.
[29] Sirui Chen, Zhaowei Zhang, Yaodong Yang, and Yali Du. Stas: Spatialtemporal return decomposition for solving sparse rewards problems
in multi-agent reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 38, pages 17337–17345,
2024.
[30] Songyang Han, He Wang, Sanbao Su, Yuanyuan Shi, and Fei Miao.
Stable and efficient shapley value-based reward reallocation for multiagent reinforcement learning of autonomous vehicles. In 2022 International Conference on Robotics and Automation (ICRA), pages 8765–
8771. IEEE, 2022.
[31] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero,
and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International conference
on machine learning, pages 5887–5896. PMLR, 2019.
[32] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.
Curriculum learning. In Proceedings of the 26th annual international
conference on machine learning, pages 41–48, 2009.

A PPENDIX A
P ROOFS
In this section, proofs of Theorem 1 is provided. For clarity,
we hereby reiterate the assumptions involved in the proof
processes.
Assumption 1. The reward for agents staying at the target
point is 0, while the rewards for both movement and staying at
non-target points are rm < 0. The collision reward rc < rm .
Assumption 2. During the interaction process between Ai
and Ã−i , neither Ai nor Ã−i has ever reached its respective
endpoint.
Theorem 1. Assume Assumps. 1 and 2 hold. Then when α =
−i
1
i
i
−i
tot
i −i
2 , Qπ∗i (s̄t , a ), Qπ∗−i (s̄t , a ) and Qπ̄∗ (s̄t , {a , a }) satisfy
the IGM condition.
Proof. Theorem 1 is equivalent to the following statement: for ai∗ = argmaxai Qiπi (s̄t , ai ), there exists a−i
=
∗
∗
−i
−i
i
−i
tot
argmaxa−i Qπ−i (s̄t , a ), such that Qπ̄∗ (s̄t , {a∗ , a∗ }) =
∗
argmaxā Qtot
π̄∗ (s̄t , ā).
For the sake of simplicity in the discussion, P
we denote
N
−i
−i
i
t j
ri (s̄P
,
ā
)
and
r
(s̄
,
ā
)
as
r
and
r
,
and
E
[
t t
t t
τ
t
t
i=0 γ rt ]
1
1
1
−i
t j
i
i
as
and
τ γ r . When α = 2 , r̃ = 2 r + 2 maxa−i r
r̃−i = 12 r−i + 21 maxai ri .
First, under Assumps. 1 and Assump. 2, if no collisions
occur along the trajectory τ , then for all s̄t and āt ∈ τ , we
have ri = maxai ri = rm and r−i = maxa−i r−i = rm . This
holds because if Ai and Ã−i do not reach their goals, then
ri < 0 and r−i < 0. Meanwhile, if no collisions occur along τ ,
then ri > rc and r−i > rc . Thus, it follows that for all s̄t and
āt ∈ τ , ri = maxai ri = rm and r−i = maxa−i r−i = rm .
Second, ∀ai ∈ Ai and ∀{ai , a−i } ∈ A, Qiπi (s̄t , ai ) ≥
∗
1 tot
i −i
2 Qπ̄∗ (s̄t , {a , a }). This is because:
1
Qiπ∗i (s̄, ai ) − Qtot
(s̄, ā)
2 π̄∗
 X 

X 1
1 i 1 −i
1
t
−i
t
i
=
γ
−
γ
r + max r
r + r
2
2 a−i
2
2
τ̄
τi




X
(a) X
1 i 1
1 i 1 −i
−i
t
t
≥
r + max
r
−
γ
r + r
γ
2
2 a−i
2
2
τ̄
τ̄


X
1
1
=
γt
max
r−i − r−i ≥ 0,
−i
2 a
2
τ̄
P
i
where (a) holds because the trajectory τP
maximizesP γ t r̃i ,
when the trajectory τ i is replaced by τ̄ , τ i γ tP
r̃i ≥ τ̄ γ t r̃i .
i
i
i
t 1 i
Then consider a∗ and Qπi (s̄, a∗ ) =
τi γ ( 2 r +
∗
1
−i
−i r
). There must be no collisions along τ i , oth2 maxa
i
erwise a∗ cannot be optimal. Therefore, maxai Qiπi (s̄t , ai ) =
∗
P
P
1
1 −i
−i
t 1 i
t 1 i
τ i γ ( 2 r + 2 r ). Furτ i γ ( 2 r + 2 maxa−i r ) =
i
−i
thermore, since Qiπi (s̄t , ai∗ ) ≥ 21 Qtot
π̄∗ (s̄t , {a∗ , a }), we
∗
P
1 −i
t 1 i
i
i
find that
τ i γ ( 2 r + 2 r ) = maxai Qπ∗i (s̄t , a ) =
1
1 tot
i
tot
2 maxā Qπ̄∗ (s̄t , ā) = 2 Qπ̄∗ (s̄t , āt ), where āt ∈ τ . We select
−i
i
−i
a∗ such that {a∗ , a∗ } = āt .
Based on the previous discussion, it can be deduced that ai∗ and a−i
can maximize both Qiπi (s̄t , ai )
∗
∗
tot
i −i
and Qπ̄i (s̄t , {a , a }). Next, we demonstrate that a−i
∗
∗

can also maximize Q−i
(s̄ , a−i ). Let Qiπi (s̄t , ai∗ ) =
π∗−i t
∗
tot
i
−i
Qπ̄i (s̄t , {a∗ , a∗ }) = U . Given that there is no conflict in
∗
τ i and U represents the maximum P
accumulated reward of
1 −i
1 −i
1 i
t 1 i
r
+
r
in
state
s̄t,
we
have
U
=
=
τi γ
2
2
2r + 2r

P
−i
1 −i
i
−i
t 1
i
= Qπ−i (s̄, a∗ ). Assume there
τi γ
2 maxa r + 2 r
∗
exists â−i ̸= a−i
such that maxa−i Q−i
(s̄ , a−i ) =
∗
π∗−i t
Q−i
(s̄ , â−i ) = V > U . Therefore:
π −i t
∗

U <V =

X
1
1
1
1
i
γ t ( r−i + max
γ t ( r−i + ri )
r
)
=
i
2
2 a
2
2
−i
−i

X
τ

τ

(b) X

≤

τ

1
1
γ t ( r−i + ri ) = U,
2
2
i

which is clearly
a contradiction.
 The validity of (b) follows
P
from that τ i γ t 21 ri + 12 r−i = 12 maxā Qtot
π̄∗ (s̄t , ā). There−i
−i
−i
fore, Q−i
).
−i (s̄t , a∗ ) = maxa−i Q −i (s̄t , a
π
π
∗

∗

A PPENDIX B
E XAMPLES
In this section, some examples will be provided to facilitate
the understanding of the CoRS algorithm.

A. The instability of Eq. (2)
In this section, we illustrate the instability of Eq. (2) through
an example. Fig. 8 presents two agents, A1 and A2 , with the
blue and red flags representing their respective endpoints. The
optimal trajectory in this scenario is already depicted in the
figure. Based on the optimal trajectory, it is evident that the
optimal action pair in the current state is Action Pair 1. We
anticipate that when A1 takes the action from Action Pair 1,
Ic1 (s̄t , āt ) will be higher than for other actions.

Fig. 8. An example illustrating the instability of Eq. (2).

According to the calculation method provided in Eq. (2) and
the reward function given in Table I, we compute the values
of Ic1 when A1 takes the action from Action Pair 1 under
2
different conditions.
the action in Action Pair
P Whenj A takes
1
1
1, Ic = |A−1 | j∈A−1 r = r2 = −0.070. When A2 takes
the action in Action Pair P
2, a collision occurs between A1
1
2
1
and A , and Ic = |A−1 | j∈A−1 rj = r2 = −0.5. These
calculations show that in the same state, even though A1 takes
the optimal action, the value of Ic1 changes depending on a2 .
This indicates that the calculation method provided in Eq. (2)
is unstable.

Fig. 9. An example illustrating how reward shaping method enhances inter-agent cooperation.

B. How Reward Shaping Promotes Inter-Agent Cooperation
In this section, we illustrate with an example how the
reward-shaping method Eq. (4) promotes inter-agent cooperation, thereby enhancing the overall system efficiency. Table I
presents the reward function used in this example. As shown in
Fig. 9, there are two agents, A1 and A2 , engaged in interaction.
Throughout their interaction, A1 and A2 remain neighbors.
Each agent aims to maximize its cumulative reward. For this
two-agent scenario, we set α = 12 . Two trajectories, τ1 and
τ2 , are illustrated in Fig. 9. We will calculate the cumulative
rewards that A1 can obtain on the two trajectories using
different reward functions. For the sake of simplicity, in this
example, γ = 1.
If A1 uses r1 as the reward function, the cumulative reward
on the trajectory P 1 is (−0.070) + (−0.070) + (−0.070) =
−0.21, and on the trajectory P 2 it is (−0.070) + (−0.070) +
(−0.070) = −0.21. Consequently, A1 may choose trajectory
τ2 . In τ2 , A2 will stop and wait due to being blocked by A1 .
This indicates that using the original reward function may not
promote collaboration between agents, resulting in a decrease
in overall system efficiency.
However, if A1 adopts r̃1 = 21 r1 + 21 maxa2 r2 as its reward,
the cumulative rewards for A1 on trajectories τ1 is (− 12 0.070−
1
1
1
1
1
2 ·0.070)+(− 2 ·0.070− 2 ·0.070)+(− 2 ·0.070− 2 ·0.070) =
1
−0.21. In contrast, on trajectory τ2 , it is (− 2 · 0.070 − 21 ·
0.075) + (− 21 · 0.070 − 12 · 0.070) + (− 12 · 0.070 − 12 · 0.070) +
( 12 · 0 − 12 · 0.070) = −0.2475. Therefore, A1 will choose the
trajectory τ1 instead of τ2 , allowing A2 to reach its goal as
quickly as possible. This demonstrates that using the reward
r̃1 shaped by the reward-shaping method Eq. (4) enables A1 to
adequately consider the impact of its actions on other agents,
thereby promoting cooperation among agents and enhancing
the overall efficiency of the system.

among other algorithms that use reinforcement learning techniques are marked in red.
TABLE IV
S UCCESS R ATE IN 40 × 40 E NVIRONMENT.
Success Rate%
Agents
4
8
16
32
64

DHC
96
95
94
92
64

DCC
100
98
98
94
91

Map Size 40 × 40
CoRS-DHC
PRIMAL
100
84
98
88
100
94
96
86
89
17

ODrM*
100
100
100
100
92

TABLE V
S UCCESS R ATE IN 80 × 80 E NVIRONMENT.
Success Rate%
Agents
4
8
16
32
64
128

DHC
93
90
90
88
87
40

DCC
99
98
97
94
92
81

Map Size 80 × 80
CoRS-DHC
PRIMAL
100
67
100
53
97
33
94
17
93
6
83
5

ODrM*
100
100
100
100
100
100

TABLE VI
AVERAGE S TEPS IN 40 × 40 E NVIRONMENT.
Average Steps
Agents
4
8
16
32
64

DHC
64.15
77.67
86.87
115.72
179.69

DCC
48.58
59.60
71.34
93.45
135.55

Map Size 40 × 40
CoRS-DHC
PRIMAL
50.36
79.08
64.77
76.53
68.48
107.14
95.42
155.21
151.02
170.48

ODrM*
50
52.17
59.78
67.39
82.60

TABLE VII
AVERAGE S TEPS IN 80 × 80 E NVIRONMENT.

A PPENDIX C
D ETAILS OF E XPERIMENTS
In this section, we provide detailed data in Section V.
Table IV and Table V presents the success rates of different
algorithms across various test scenarios, while Table VI and
Table VII shows the makespan of different algorithms in these
scenarios. The results for the ODrM* algorithm, which are
considered optimal, are highlighted in bold. The best metrics

Average Steps
Agents
4
8
16
32
64
128

DHC
114.69
133.39
147.55
158.58
183.44
213.75

DCC
93.89
109.89
122.24
132.99
159.67
192.90

Map Size 80 × 80
CoRS-DHC
PRIMAL
92.14
134.86
109.15
153.20
121.25
180.74
137.06
250.07
153.06
321.63
193.50
350.76

ODrM*
93.40
104.92
114.75
121.31
134.42
143.84

