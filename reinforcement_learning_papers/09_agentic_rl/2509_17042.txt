1

Orchestrate, Generate, Reflect: A VLM-Based
Multi-Agent Collaboration Framework for
Automated Driving Policy Learning

arXiv:2509.17042v1 [cs.RO] 21 Sep 2025

Zengqi Peng, Yusen Xie, Yubin Wang, Rui Yang, Qifeng Chen, and Jun Ma, Senior Member, IEEE

Abstract‚ÄîThe advancement of foundation models fosters new
initiatives for policy learning in achieving safe and efficient
autonomous driving. However, a critical bottleneck lies in the
manual engineering of reward functions and training curricula
for complex and dynamic driving tasks, which is a labor-intensive
and time-consuming process. To address this problem, we propose
OGR (Orchestrate, Generate, Reflect), a novel automated driving
policy learning framework that leverages vision-language model
(VLM)-based multi-agent collaboration. Our framework capitalizes on advanced reasoning and multimodal understanding capabilities of VLMs to construct a hierarchical agent system. Specifically, a centralized orchestrator plans high-level training objectives, while a generation module employs a two-step analyze-thengenerate process for efficient generation of reward-curriculum
pairs. A reflection module then facilitates iterative optimization
based on the online evaluation. Furthermore, a dedicated memory
module endows the VLM agents with the capabilities of long-term
memory. To enhance robustness and diversity of the generation
process, we introduce a parallel generation scheme and a humanin-the-loop technique for augmentation of the reward observation
space. Through efficient multi-agent cooperation and leveraging
rich multimodal information, OGR enables the online evolution
of reinforcement learning policies to acquire interaction-aware
driving skills. Extensive experiments in the CARLA simulator
demonstrate the superior performance, robust generalizability
across distinct urban scenarios, and strong compatibility with
various RL algorithms. Further real-world experiments highlight
the practical viability and effectiveness of our framework. The
source code will be available upon acceptance of the paper.
Index Terms‚ÄîAutonomous driving, vision-language model,
reward-curriculum generation, VLM-based multi-agent system.

I. I NTRODUCTION
In recent years, the rapid advancement of foundation models
and multimodal learning has substantially propelled progress
in robotics and autonomous systems across both academia and
industry [1], [2], [3]. These developments provide promising
solutions for achieving safe and efficient autonomous driving.
Autonomous vehicles (AVs), as a human-centric system with a
Zengqi Peng, Yusen Xie, Yubin Wang, and Rui Yang are with the Robotics
and Autonomous Systems Thrust, The Hong Kong University of Science
and Technology (Guangzhou), China (e-mail: zpeng940@connect.hkustgz.edu.cn; yxie827@connect.hkust-gz.edu.cn; ywang575@connect.hkustgz.edu.cn; ryang253@connect.hkust-gz.edu.cn).
Qifeng Chen is with the Department of Computer Science and Engineering,
The Hong Kong University of Science and Technology, Hong Kong SAR,
China (e-mail: cqf@ust.hk).
Jun Ma is with the Robotics and Autonomous Systems Thrust, The
Hong Kong University of Science and Technology (Guangzhou), Guangzhou
511453, China, and also with the Cheng Kar-Shun Robotics Institute, The
Hong Kong University of Science and Technology, Hong Kong SAR, China
(e-mail: jun.ma@ust.hk).

high safety demand, are required to make appropriate decisions
in real time under dynamic conditions [4], [5]. However,
compared to highway scenarios, urban driving environments
are characterized by the diversity of driving scenarios and
frequent interactions among traffic participants [6], [7]. Various road topologies and distributions of surrounding vehicles (SVs) pose significant challenges to both driving safety
and task efficiency. These factors collectively contribute to
complicated driving situations. In this sense, enabling AVs to
avoid collisions while maintaining high task efficiency in such
complex and highly dynamic scenarios has become a critical
and pressing research focus.
Learning-based approaches for autonomous driving mainly
contain imitation learning (IL) and reinforcement learning
(RL) [8], [9], [10]. Although IL exhibits high sample efficiency
in specific contexts by fitting expert-demonstrated data [11],
its performance is fundamentally capped by the quality of the
expert data. Furthermore, it often struggles in highly dynamic
and interactive urban driving tasks, even with enhancements
like DAgger [12] and data augmentation techniques [13]. In
contrast, RL enables agents to explore and learn through
direct trial-and-error interaction with the environment [14]. It
optimizes policies by maximizing cumulative rewards. This
interaction-driven process allows RL to adapt to dynamic
urban environments and to discover policies that surpass
expert performance. However, applying RL to complex urban
scenarios introduces two critical challenges [15]. The first
challenge is to design a reward function that elicits safe
and efficient behaviors [16], [17]. The second one involves
ensuring training efficiency, particularly when exploring the
vast state-action space [18], [19].
Traditionally, researchers have employed techniques like
reward shaping [20], [21] and curriculum learning (CL) [22],
[23] to mitigate these challenges. However, these methods rely
heavily on manual engineering, and their effectiveness is ultimately constrained by the prior knowledge and intuition of human experts. The recent rise of large language models (LLMs)
and vision-language models (VLMs) presents a transformative
opportunity to address this gap [24], [25]. These foundation
models exhibit remarkable emergent abilities that approach
the level of comprehensive human expertise, such as powerful
multimodal understanding [26], common-sense reasoning [27],
and even code generation [28], [29]. Crucially, this capacity
for sophisticated reasoning and action allows them to serve
as powerful autonomous agents, making collaborative multi-

2

Fig. 1. The overview of our VLM-in-the-training-loop paradigm. (1) A VLM-based multi-agent cooperative system interacts with the environment to facilitate
the optimization of the RL policy. Specifically, VLM agents collaborate to provide a sequence of reward-curriculum pairs, which are used to parameterize the
environment. (2) The RL agent then explores and learns within these configured environments, and the resulting experiences and responses of VLM agents
are stored in a memory module. (3) In the next training stage, this memory module provides historical question-answer data and training data to the VLM
agents to assist in generating updated reward-curriculum pairs.

agent systems newly feasible for complex real-world tasks
[30], [31], [32]. The above advancements position LLMs and
VLMs as ideal candidates for automating key components of
RL training pipelines, particularly in safety-critical domains
like urban autonomous driving.
Based on the above background, our central insight is that
the integrated capabilities of advanced VLMs can empower a
multi-agent collaborative framework for the automated joint
design of reward functions and training curricula. We propose
a framework named OGR (Orchestrate, Generate, Reflect) for
urban driving tasks, while retaining a compatible interface
for human expert cooperation. To the best of our knowledge,
this work is the first to introduce a collaborative framework
leveraging multiple VLM-empowered agents for the automated
generation of reward-curriculum pairs in urban autonomous
driving scenarios. Fig. 1 depicts the overall architecture of the
proposed training paradigm. Our contributions are summarized
as follows:
‚Ä¢ An innovative automated RL policy learning framework
is developed for interaction-aware urban autonomous
driving. This framework significantly improves the sample efficiency of the RL policy and enhances driving
safety in diverse traffic conditions.
‚Ä¢ A novel VLM-in-the-training-loop paradigm is developed to generate highly consistent and efficient rewardcurriculum pairs online. It is composed of multiple VLM
agents, including an orchestrator, generators, and a policy
reflector. These VLM agents cooperate by engaging in
role-playing and coordinated task division, thereby boosting the training efficiency and productivity.
‚Ä¢ A unique generation module is devised for robust and
diverse synthesis of reward-curriculum pairs. It integrates a parallel generation scheme to mitigate VLM
hallucinations and a human-in-the-loop mechanism to
progressively expand the reward observation space.
‚Ä¢ Extensive experiments are conducted across both the
high-fidelity simulator and hardware platforms. Simulation results demonstrate the superior performance of our
framework in terms of task success rate, generalizability,
and compatibility across different driving scenarios and
RL algorithms. Real-world experiment results further

verify the effectiveness of our framework.
The rest of the paper is structured as follows. Section II
presents the related work. Section III illustrates important
preliminaries. Section IV provides a detailed introduction to
the proposed framework. Section V includes the experimental
validation and corresponding discussion. Finally, Section VI
presents the conclusion of this work and outlines potential
directions for future work.
II. R ELATED W ORK
A. Deep Reinforcement Learning for Autonomous Driving
Deep RL has emerged as a mainstream paradigm for
learning complex driving policies. However, its successful
application in autonomous driving is often hindered by key
challenges of reward design and training efficiency [33].
Regarding reward design, traditional approaches typically rely
on manually engineering the reward function through a process of trial and error [34], [35]. Although various reward
embedding techniques have been proposed to enhance policy
performance [36], [37], the multi-objective nature of the
driving goals makes this process exceptionally challenging
[38]. For instance, an effective driving policy should simultaneously balance safety, efficiency, and comfort [39]. These
partly conflicting objectives with varying priorities could make
designs based purely on expert intuition laborious and timeconsuming, even yielding suboptimal results. As an alternative,
Inverse RL is used to extract an intrinsic reward model from
expert demonstrations [40], [41]. But these approaches are
constrained by the availability of large-scale, high-quality
datasets, and the learned reward models often exhibit poor
generalization. In terms of training efficiency, Model-based RL
aims to reduce interaction costs by learning an environment
model to aid in decision-making or to generate synthetic data
[42]. However, the core drawback of this method is its susceptibility to compounding model errors, with performance being
limited by the distribution of its training data. On the other
hand, CL has proven effective at guiding policy exploration in
vast state-action spaces [43], [44], [45]. Nevertheless, for complex tasks, predefined curricula that rely on expert knowledge
often have limited effectiveness. While adaptive curriculum

3

methods have been proposed [46], [14], they typically rely
on assumptions about the structured properties of the tasks
and require extensive parameter tuning when applied to new
scenarios.
B. Foundation Models for Autonomous Driving
In recent years, powerful foundation models have been
widely applied to numerous frontier domains, including code
generation, robotics, and autonomous driving. A direct integration paradigm in autonomous driving is to employ these
models as a functional component within the onboard system,
such as for perception, prediction, or decision-making [47],
[48]. LMDrive [49] directly generates control signals for AVs
by processing multimodal sensor data. Dilu [50] leverages
LLMs to enable the system to perform decision-making by
customized reasoning and reflection modules. However, the
high inference latency inherent to LLMs makes it difficult to
satisfy the stringent real-time requirements of driving tasks.
To address this problem, DriveVLM-Dual [26] proposed a
fast-slow system that pairs a VLM with a traditional planner
in parallel. Although this design mitigates the latency issue,
the overall performance bottleneck of the system shifts to the
traditional planner acting as the fast component.
On the other hand, foundation models are used to assist
the optimization process of RL policies during the training
phase. In the area of reward design, existing works like LORD
[51] and VLM-RL [52] utilize LLMs and VLMs to compute
reward signals in real time. However, the dense invocation
of LLMs and VLMs can lead to slow training speed and
high costs. Recent works, such as Eureka [53], AutoReward
[54], and SDS [55], have explored using LLMs and VLMs to
design entire reward functions iteratively, even deriving reward
functions from demonstration videos. For training environment
and curriculum design, studies such as Eurekaverse [56],
CurricuLLM [57], and CurricuVLM [58] have demonstrated
the ability of these models to generate increasingly challenging
environments, decompose complex tasks, and create safetycritical scenarios. However, the majority of these methods
focus on automating only a single aspect of the pipeline, either
the reward or the curriculum. While the recent LearningFlow
framework [59] explores the generation of both aspects, its
design suffers from two key limitations. First, it relies solely on
numerical and textual feedback, neglecting rich and intuitive
information from multimodal data such as images and videos.
Second, its reward and curriculum generation processes are
independent. This lack of coordination can potentially lead to
a mismatch between the reward function and the curriculum,
which could compromise the stability of policy training and
degrade the performance of the trained policy.

vehicle (EV) and SVs are initialized at arbitrary, yet trafficrule-compliant, positions with varying traffic densities. The
behavior of EV is determined by an RL policy, while the
driving styles of the SVs are randomly initialized to represent
a heterogeneous mix of driver behaviors. Under such settings,
the RL policy is required to guide the EV to safely and
efficiently complete the driving task in environments characterized by diverse configurations and traffic conditions. This
task formulation introduces significant stochasticity and high
dynamic complexity into the task scenarios, making the task
scenarios closer to those in the real world and thus presenting
greater challenges.
B. Learning Environment
We formulate the target tasks as a Markov Decision Process
(MDP), which can be represented as a tuple ‚ü®S, A, P, R, Œ≥‚ü©.
The definitions of all components are introduced as follows:
State space S: S contains kinematic features of vehicles.
The state matrix at k-th time step is defined as follows:
h
iT
N max
(1)
Sk = s0k s1k ... sk sv
,
max
is the maximum number of observed SVs; s0k and
where Nsv
i
max
) are the states of the EV and the i-th
sk (i = 1, 2, ..., Nsv

T
SV, respectively. Specifically, sik = xik yki vki œàki
,
where xik and yki represent the X-axis and Y-axis coordinates,
respectively; vki and œàki denote the speed and the heading
angle, respectively.
Action space A: A is a multi-discrete action space consisting of three sub-action spaces:

A = {A1 , A2 , A3 } ,

(2)

where A1 , A2 , and A3 represent three sub-action spaces. The
definitions of the sub-action spaces will be detailed in Section
IV-F.
State transition dynamics P(Sk+1 |Sk , ak ): P determines
the probabilities regarding the state transitions which satisfy
the Markov property. The environment implicitly defines the
probabilities, which are not directly accessible to the RL agent.
Reward function R: R encourages appropriate and desirable behaviors of the RL agent, while penalizing incorrect
or suboptimal ones. It plays a critical role in guiding the
exploration and learning of the RL agent. In this work,
we harness the multimodal reasoning capabilities of VLMs
to automatically generate and recursively optimize reward
functions for the RL agent in complex tasks.
Discount factor Œ≥: Œ≥ ‚àà (0, 1) is utilized to discount future
accumulated rewards.
C. Reward Function Generation

III. P RELIMINARIES
A. Task Definition
This study aims to address the challenge of training an
RL policy to achieve safe, efficient, and interaction-aware
performance across various complex urban driving tasks, such
as multi-lane overtaking, on-ramp merging, and unsignalized
intersections. Within our defined task formulation, the ego

In this work, the goal of reward function design is to construct a shaped reward function for complex autonomous driving tasks in which the ground-truth reward is difficult to obtain
directly. Generally, the ground-truth reward is sparse, making it
challenging to learn effective RL policies in high-dimensional,
continuous state spaces. Furthermore, the ground-truth reward
is usually not directly observable and can only be accessed

4

indirectly through queries. The definition of the formal reward
design problem is shown as follows.
Problem 1 [53], [60] (Reward Function Design Problem). The
reward function design problem can be represented as a tuple
P = ‚ü®E, RM , M, F ‚ü©, where E denotes an environmental
distribution which is a set of MDPs; RM represents the
reward function space; M is an RL agent which uses function
rM ‚àà RM to optimize its policy; F is a fitness function
which provides a scalar evaluation for a rollout trajectory.
‚àó
The goal is to design an optimal reward function rM
whose
optimized policy can achieve the maximum fitness score over
the environmental distribution E.
Based on Problem 1, this study aims to generate the reward
terms of a reward function by introducing the code agent
[61], [28], which is defined as the reward function generation
problem. The objective of this problem is to generate a reward
function code that can optimize the RL policy to maximize
the fitness function based on a given context. Considering the
requirements of our target task, the fitness function can be
defined as the task success rate.
D. Online Task-Level Curriculum Generation
Given the complex target task, particularly in the highly
dynamic and interactive environments, we model the RL
policy training as a curriculum learning problem. In this work,
the goal of curriculum design is to develop a sequential multitask sequence for an RL agent. Compared to naively training
the RL policy in the most complex environment, a suitable
curriculum can help to improve the sample efficiency [62],
[63], thereby enhancing driving safety and efficiency. The
curriculum design problem is defined as follows.
Problem 2 [22] (Task-Level Curriculum Design Problem). A
curriculum is a directed acyclic graph. It can be represented by
a tuple C = ‚ü®V, E, g, T ‚ü©, where V denotes the set of vertices,
E represents the directed edges, g is a function that associates
vertices to subsets of all transition samples, and T denotes a
set of tasks. The directed acyclic graph terminates on a single
sink node. The goal is to design an optimal curriculum C ‚àó =
[T1‚àó , T2‚àó , ¬∑ ¬∑ ¬∑ , TN‚àó ] that can guide the RL policy to achieve the
best performance metrics over the target tasks.
Based on Problem 2, our objective is to leverage a generative AI agent that generates a training curriculum online,
conditioned on contextual information, in order to optimize
task-specific performance metrics. The above challenge is
referred to as the online task-level curriculum generation
problem.
Specifically, the challenge of different urban scenarios can
typically be attributed to two key dimensions. For instance, the
complexity of overtaking and merging tasks primarily arises
from traffic density and the velocity patterns of SVs. For
intersection scenarios, it stems from traffic density and the
task type, such as turning left, going straight, or turning right.
Accordingly, we define the task set for CL as a two-level set
as follows:
max
max
T = {tij | i = 0, 1, . . . , Nl1
, j = 0, 1, . . . , Nl2
},

(3)

max
max
where Nl1
and Nl2
denote the number of discrete values
associated with two orthogonal dimensions of task difficulty,
respectively. The goal of the online task-level curriculum
generation problem can be formulated as follows:
X
C ‚àó = {Tn‚àó } = arg max
R,
(4)

where C ‚àó denotes the optimal curriculum; Tn‚àó is the optimal
training task at the n-th training interval; R represents the
reward obtained by the RL agent from the environment.
E. Joint Reward-Curriculum Pair Generation Problem
The target tasks involve a wide range of environmental
configurations, making it challenging for the driving policy
to learn appropriate behaviors across diverse situations. To
address this challenge, we divide the policy training process
into multiple stages and generate a tailored reward-curriculum
pair for each stage. Based on Section III-C and Section III-D,
we define the following problem:
Problem 3 (Joint Reward-Curriculum Pair Generation Problem). For target tasks and task set T = {tij }, the
driving policy is represented as œÄŒ∏ . The objective is to
generate an optimal reward-curriculum sequence Q‚àóRC =
‚àó
‚àó
)] to optimize the RL pol, CN
[(R1‚àó , C1‚àó ), (R2‚àó , C2‚àó ), ¬∑ ¬∑ ¬∑ , (RN
icy, which can maximize the reward over target tasks œÄŒ∏‚àó =
arg maxQ‚àóRC R.
To improve the performance of the trained policy, it is
important to enhance the relationship between the generated
reward function and the curriculum. To address this challenge,
we will introduce the proposed closed-loop OGR in the next
section.
IV. M ETHODOLOGY
A. Overview of Framework
The overall framework is depicted in Fig. 2. During the
training phase, context is first provided to an orchestrator
agent, which formulates a stage-wise training plan. It then
issues the training objective of the current stage to the generation module to ensure consistency between the subsequently
generated reward function and curriculum. This training objective, combined with the task-specific context, serves as
the input for the generation module. Given the complexity
of the task scenarios and the generation task, we design this
generation process as a two-step procedure, which is shown is
Fig. 3. In the first stage, analysis agents receive this combined
information and perform a preliminary analysis of the reward
function and curriculum generation. They subsequently output
their analytical findings and concrete recommendations. In
the second stage, the responses from the analysis agents are
delivered to the generation agents along with the generation
context. This guides the final generation of specific reward
terms and training curricula. Subsequently, the generated reward function and curriculum are passed to the environment
module for initialization. They are then used for interaction
with the RL agent to collect experience and update its policy.
Finally, all responses generated by the VLM agents and the

5

Fig. 2. The diagram of the automated RL policy learning framework via VLM-based multi-agent collaboration for autonomous driving tasks. This framework
consists of an orchestration module, a generation module, a reflection module, a memory module, a prompt generator, and a context library.

interaction data of the RL agent are recorded in the memory
module.
To enhance training stability, we generate multiple rewardcurriculum pairs simultaneously in each training round. After a
predefined number of episodes, the current intermediate policy
is evaluated in an in-training test. The resulting statistical
metrics and video recordings are stored in the memory module
and fed to a reflection agent. This agent then selects the
optimal reward-curriculum pair to be used in the next training
stage. At the end of an orchestration or generation cycle, the
results and test data from the previous round are sampled
and integrated with the latest contextual information. This
consolidated input is then provided back to the corresponding
VLM agents for their subsequent reasoning tasks. Notably,
during the initial stage of training, only textual context is
provided to the VLM agents. Once the training enters the first
feedback loop, sampled videos will be integrated as visual
input into the contextual information for the VLM agents. The
following subsections provide a detailed description.

proposed framework requires multiple VLM invocations, including orchestration, generation, and reflection, across various stages of a complete RL policy training process. This
design necessitates that the reward-curriculum pairs generated
by the VLM agents across different training stages maintain a
high degree of consistency and temporal coherence.
Therefore, we introduce a memory module specifically for
storing multimodal experiences generated throughout the training process. This data primarily includes the interaction history
of VLM agents, historical data from interactions between the
RL agent and the environment, and video recordings from intraining evaluations. The VLM interaction history further encompasses customized prompts, reasoning process, extracted
reward-curriculum pairs, and other valuable information. The
stored content is selectively retrieved and embedded into the
subsequent prompt for the next reasoning round in the form of
vectors, text, and image sequences. By equipping the framework with this memory module, we endow the VLM agents
with long-term memory, thereby enabling the generation of
reward-curriculum pairs that exhibit both temporal coherence
and logical continuity throughout the training process.

B. Multimodal Memory Module for Storage and Retrieval
Mainstream LLMs and VLMs generally lack a built-in
long-term memory mechanism. Their operational memory is
typically confined to the current context window, preventing
them from retaining historical interaction data. However, our

C. High-Level Orchestration for Reward-Curriculum Alignment
Given the complex target task and the diversity of agent
roles, a key challenge in generating reward-curriculum pairs

6

Fig. 3. Details of the two-step reward-curriculum generation module.

using multiple VLM agents is to achieve effective coordination and orchestration. Prior works usually provide historical
query-response data and other context directly and separately
to each generation agent, which can lead to a mismatch
between the generated reward function and curriculum. This
potentially results in inefficient training and suboptimal performance of the final policy.
Inspired by centralized coordination strategies in multiagent systems, we introduce a centralized orchestrator agent
into our framework. This agent is responsible for the high-level
planning of training objectives at different stages of the RL
policy training process, dynamically adjusting them based on
historical training information. Subsequently, the orchestrator
agent provides a clear training objective for the current stage as
part of the context for the generation module. This centralized
design enables the generation module to produce rewardcurriculum pairs tailored to the objectives of each specific
training stage. This goal-oriented process can result in a
significant improvement in training efficiency. Ultimately, this
enables the RL policy to acquire driving skills of increasing
difficulty more effectively.
Building on the above description, the orchestration module
can be modelled as a two-step process, which can be expressed
as follows:
(
text
Œ¶O
)
if nO
s =1
V LM (lO , I
(5)
GnO
=
O
text image
s
Œ¶V LM (lO , I
,I
, HnO
) if nO
s ‚â•2
s ‚àí1


(r) (c)
gnO , gnO = ParseO (GnO
)
(6)
s
s

s

nO
s

where
is the index for the training stage; Œ¶O
V LM (¬∑)
and ParseO (¬∑) represent the VLM-powered orchestrator agent
and parse function, respectively; lO is the language goal
of orchestration; I text and I image denote textual context
and visual context, respectively; GnO
and HnO
are the
s
s ‚àí1
generated comprehensive plan text and multimodal dialogue
(r)
(c)
history, respectively; gnO and gnO represent reward generation
s
s
objective and curriculum generation objective extracted from
GnO
, respectively.
s
D. Low-Level Two-Step Reward-Curriculum Generation
Following the high-level training objective provided by the
orchestrator agent, this section details the low-level process of
generating specific reward-curriculum pairs. Directly translating a high-level goal into a fine-grained reward function and

a complex curriculum sequence is a significant challenge due
to the inherent complexity of the target tasks. To address this,
we draw inspiration from hierarchical task decomposition in
multi-agent systems and reformulate the generation process
into two distinct steps: first analysis and then generation. This
decoupled approach allows dedicated agents to first interpret
and analyze the current training goal and contextual signals
before synthesizing the corresponding reward functions and
curriculum instances accordingly.
1) Pre-Generation Task Analysis for Reward and Curriculum: Given the heterogeneous nature of the information sources within the multimodal context and the inherent
complexity of the generation task, relying on a single agent
to perform long-chain reasoning often leads to a significant
degradation in performance. Therefore, we introduce two distinct multimodal analysis agents, a reward analysis agent and a
curriculum analysis agent. They are responsible for conducting
an in-depth analysis of the current RL policy training progress
and behavioral patterns based on the multimodal context.
Subsequently, they provide separate and specialized guiding
blueprints for the reward function and curriculum generation
of the next training stage. Specifically, these analysis agents
leverage the superior capabilities of the underlying VLM
for comprehensive multimodal understanding and detailed
description generation.
To facilitate their specialized roles, each analysis agent is
provided with a distinct stream of customized information.
For instance, the reward analysis agent receives the stage-wise
objective for reward generation from the orchestrator agent,
relevant high-level guidelines, and historical data detailing the
numerical trajectories of the total reward and its individual
components. Its primary function is to leverage these inputs
to devise a plan for the reward function, outputting its overall
design objectives in a structured format, including specific
reward terms, meanings, types, and value ranges. In parallel,
the curriculum analysis agent processes its own set of inputs,
including the objective for curriculum selection, guidelines
for curriculum switching, and historical statistics such as the
success, collision, and timeout rates of previously executed
curricula. Its role is then to evaluate the efficacy of the prior
curriculum and analyze its suitability for the upcoming training
stage. Therefore, the analysis process of reward and curriculum
can be expressed as follows:
Ô£±


(r,A)
(r) text
Ô£¥
Œ¶
l
,
g
,
I
Ô£¥
(r,A) 1
V LM
(r,A) ,
Ô£¥
Ô£¥
Ô£≤
if nR
(r)
s =1,

An R =
(r,A)
(r)
(r,A)
image
text
s
Ô£¥
Œ¶V LM l(r,A) , gnR , I(r,A) , I(r,A) , HnR ‚àí1 ,
Ô£¥
Ô£¥
s
s
Ô£¥
Ô£≥
if nR
s ‚â• 2,
(7)
Ô£±


(c,A)
(c) text
Ô£¥
,
I
,
Œ¶
l
,
g
Ô£¥
(c,A) 1
V LM
(c,A)
Ô£¥
Ô£¥
Ô£≤
if nC
(c)
s =1,

An C =
(c,A)
(c)
(c,A)
image
text
s
Ô£¥
Œ¶V LM l(c,A) , gnC , I(c,A) , I(c,A) , HnC ‚àí1 ,
Ô£¥
Ô£¥
s
s
Ô£¥
Ô£≥
if nC
s ‚â• 2,
(8)
(r,A)
(c,A)
where Œ¶V LM and Œ¶V LM denote VLM-powered analysis
C
agent of reward and curriculum, respectively; nR
s and ns
are the indices for the generation stage of reward and cur-

7

riculum, respectively; l(r,A) and l(c,A) represent the language goals of reward and curriculum analysis, respectively;
image text
image
text
I(r,A)
, I(c,A) , and I(c,A)
denote textual context and
, I(r,A)
visual context of reward and curriculum analysis, respectively;
(r,A)
(c,A)
HnR ‚àí1 and HnC ‚àí1 are multimodal dialogue history of reward
s

s

(r)

(c)

and curriculum analysis, respectively; AnR and AnC denote
s
s
in-depth structured analysis of reward and curriculum.
2) Parallel Reward-Curriculum Generation and Human-inthe-Loop Reward Observation Space Augmentation: Building
upon the structured analysis provided by the analysis agents,
the generation agents proceed to generate the reward functions
and training curricula. To ensure this process is systematic,
consistent, and produces high-quality outputs, we employ
a comprehensive prompt template. The construction of this
template begins with role assignment, which establishes the
identity of agents by positioning them as a specialized generator for a given task. Next, during context injection, we provide
key context information, including the available environment
observation variable code, the definitions and contents of
the predefined curriculum set, and the fine-grained guidelines
produced by the analysis agents. Finally, in the memory
integration step starting from the second round onward, the
complete dialogue history from the previous round is supplied
as a long-term memory context. The VLM-based agents then
reference this context to inform their subsequent inference.
Within this prompting framework, two generation agents
execute their respective tasks. The core responsibility of the
reward generation agent is to translate the analytical blueprint
from the preceding step into a concrete code implementation.
Its workflow begins with a deliberate evaluation of each reward
term proposed in the analysis. During this evaluation, the agent
comprehensively reviews relevant environment observation
variables and historical reward functions. It then decides on
a component-by-component basis whether to include each
term and designs its precise mathematical expression. Finally,
all selected components are aggregated into an executable
reward function code. Meanwhile, the curriculum generation
agent focuses on the dynamic adjustment of the curriculum
sequence. It leverages the analysis conclusion and training
progress context to develop a comprehensive understanding
of the current learning state of the policy. Subsequently, it initiates a situational assessment procedure, comparing historical
training data against the task settings of the previous stage
to judge if the curriculum difficulty is well-matched to the
capabilities of the current policy. Based on this assessment,
the agent then selects the most suitable set of tasks from the
predefined curriculum set for the upcoming training stage.
Therefore, the generation process of reward and curriculum
can be expressed as follows:
Ô£±


(r,G)
(r) text
Ô£¥
Œ¶V LM l(r,G) , A1 , I(r,G)
,
Ô£¥
Ô£¥
Ô£¥
Ô£≤
if nR
(G)
s =1,

RnR =
(r,G)
(r)
(r,G)
image
text
s
Ô£¥
Œ¶V LM l(r,G) , AnR , I(r,G) , I(r,G) , HnR ‚àí1 ,
Ô£¥
Ô£¥
s
s
Ô£¥
Ô£≥
if nR
s ‚â• 2,
(9)


(G)
R nR
= ParseR RnR ,
(10)
s
s

Ô£±


(c,G)
(c) text
Ô£¥
,
Œ¶V LM l(c,G) , A1 , I(c,G)
Ô£¥
Ô£¥
Ô£¥
Ô£≤
if nC
(G)
s =1,

CnC =
(c,G)
(c)
(c,G)
image
text
s
Ô£¥
Œ¶V LM l(c,G) , AnC , I(c,G) , I(c,G) , HnC ‚àí1 ,
Ô£¥
Ô£¥
s
s
Ô£¥
Ô£≥
if nC
s ‚â• 2,
(11)


(G)
CnCs = ParseC CnC ,
(12)
s

(r,G)

(c,G)

where Œ¶V LM and Œ¶V LM denote VLM-powered generation
agent of reward and curriculum, respectively; l(r,G) and l(c,G)
represent the language goals of reward and curriculum genimage text
image
text
eration, respectively; I(r,G)
, I(c,G) , and I(c,G)
denote
, I(r,G)
textual context and visual context of reward and curriculum
(r,G)
(c,G)
generation, respectively; HnR ‚àí1 and HnC ‚àí1 are multimodal
s
s
dialogue history of reward and curriculum generation, respec(G)
(G)
tively; RnR and CnC denote generated content of reward
s
s
and curriculum; ParseR (¬∑) and ParseR (¬∑) represent the parse
function of reward function and curriculum, respectively; RnR
s
and CnCs are extracted reward function code and curriculum
index, respectively.
Once the executable reward function code and selected curriculum identifiers are parsed from the raw output of the VLM,
these components are passed to the RL training environment
via a standardized interface. Their primary role is to configure
and parameterize the environment for the interaction of the
RL agent and experience collection in the upcoming training
stage.
To mitigate the potential impact of VLM hallucinations on
inferred content quality, our framework generates a set of
reward-curriculum pairs in parallel within each training stage.
These pairs are subsequently used to parameterize multiple
environment instances for the exploration of the RL agent,
and the interaction history from each instance is independently
logged to the memory module along with its corresponding
generated pair. This parallel generation can improve the safety
and reliability of the generations through diversity, and ultimately enhance the performance of the final trained policy.
3) Human-in-the-Loop Reward Observation Space Augmentation: Notably, the environment observation code within
our framework is designed to be extensible as training progresses. Initially, it is constructed based on the prior knowledge
of human experts in reward design. However, this initial observation library is only a subset of the full range of potential
observations. To dynamically expand this library, we introduce
a dedicated human-in-the-loop mechanism for reward observation space augmentation. Under this mechanism, if the expression of a reward component needs an observation variable
absent from the current observation library, the proposal will
not be dismissed or result in a programming error. Instead, it
will be output in a structured format and flagged for human
expert review. Before the next training round, the human expert
assesses the feasibility and potential utility of the proposed
variable. If it is accessible and beneficial, the expert will
expose this new variable within the environment observation
code. This augmentation process allows the observation code
to co-evolve with the reward function and RL policy, thereby
potentially enhancing the quality of the generated reward
functions.

8

E. Automated Policy Evolution via In-Training Reflection
Upon completion of a training stage, where the RL agent has
finished a set of episodes across multiple environments parameterized by NG parallel-generated
n reward-curriculum pairs,
o the
(i)
resultant intermediate policy œÄnG | i = 1, 2, ¬∑ ¬∑ ¬∑ , NG from
s
each branch is extracted for an in-training test. Specifically, the
test requires each intermediate policy to perform repeatedly
in the most challenging task setting from the curriculum it
has just experienced. Key performance metrics and the full
video recordings from this testing process are archived in the
memory module.
To conduct a comprehensive evaluation of the actual performance of the policy that goes beyond traditional metrics,
we additionally
n introduce a scoring agent.
o This agent receives
(i)
NS key clips Ij | j = 1, 2, ¬∑ ¬∑ ¬∑ , NS sampled from the test
videos and, based on a meticulously designed and structured
scoring prompt, outputs a quantitative behavioral score. Subsequently, the average behavioral score, along with the aforementioned statistical metrics, is delivered to a reflection agent.
The core task of the Reflection Agent is to assess the training effectiveness of each reward-curriculum pair within the
past stage. Ultimately, it selects the best-performing rewardcurriculum pair and its corresponding intermediate policy from
all parallel branches. This selected combination then serves as
the foundation for the next training stage, informing the new
round of reward-curriculum generation and the subsequent
update of the RL policy. Therefore, the complete reflection
process can be expressed as follows:



(i)
(i)
bj = ParseS Œ¶SV LM lS , Ij
,
(13)
(i)

BÃÑnO =
s

= Œ¶R
E nO
V LM
s



NS
1 X
(i)
b ,
NS j=1 j

(14)

oNG 
n
(i)
(i)
,
lR , MnO , BÃÑnO
s

s

i=1


O ,
i‚àónO
=
Parse
E
R
n
s
s

(15)
(16)

where Œ¶SV LM and Œ¶R
V LM are VLM-powered scoring agent
and reflection agent, respectively; ParseS (¬∑) and ParseR (¬∑)
represent the parse function of scoring and reflection, respectively; lS and lR denote the prompt instruction of scoring
(i)
and reflection, respectively; bj represent the visual behavioral
(i)
(i)
score for j-th clip of RL policy œÄN G ; BÃÑnO is the average
(i)

s

s

behavioral score; MnO represents statistical metrics; EnO
and
s
s
i‚àónO denote the response of the reflection agent and parsed
s
index of the optimal policy in training stage nO
s , respectively.
F. Downstream RL Executor
After the training environments are configured by the
multiple reward-curriculum pairs, a hierarchical RL-model
predictive control (RL-MPC) architecture [14] is used as
the downstream executor for interaction. Specifically, the RL
policy is implemented as a neural network œÄ with parameters
Œ∏. This policy processes observations to determine a set of
decision variables, which are then used to direct the model

predictive controller (MPC) to generate low-level control signals. Therefore, given the time step k, the action of the RL
agent is derived from the current observation Ok as follows:
aRL
= œÄŒ∏ (Ok )
k

(17)

where the observations are defined as follows:
iT
h
N max
Ok = o0k o1k ... ok obs
Ô£Æ
‚àÜx0k
‚àÜyk0
vk0
‚àÜœàk0
Ô£Ø ‚àÜx1k
‚àÜyk1
‚àÜvk1
‚àÜœàk1
Ô£Ø
=Ô£Ø
..
..
..
..
Ô£∞
.
.
.
.
N max

‚àÜxk obs

N max

‚àÜyk obs

N max

‚àÜvk obs

N max

Ô£π
Ô£∫ (18)
Ô£∫
Ô£∫,
Ô£ª

‚àÜœàk obs

max
where Nobs
is the maximum number of the observable SVs;
i
‚àÜxk , ‚àÜyki , ‚àÜvki , and ‚àÜœàki represent the respective deviations
of the EV from the target pose (i = 0) and from the i-th SV
max
(i = 1, 2, . . . , Nobs
).
The multi-discrete action space (2) consists of three subaction spaces, including waypoint, reference velocity, and
lane change sub-action spaces. The mathematical definition
is expressed as follows:

A1 = {WP0 , WP1 , ..., WP4 } ,


vlimit vlimit 3vlimit
,
,
, vlimit ,
A2 = 0,
4
2
4
A3 = {‚àí1, 0, 1} ,

(19)
(20)
(21)

where WPi = [xWP
yiWP œàiWP ]T includes the related refi
erence information of the i-th waypoint; vlimit is speed
limitation of the current road; ‚àí1, 0, and 1 denote the left
lane-change decision, lane-keeping decision, and right lanechange decision, respectively. Waypoints are predefined by the
road map. Before the task commences, a reference waypoint
set is generated by the A‚àó search algorithm. At each time
step, the five waypoints nearest to the EV are selected to
construct the A1 . The synergy of these three sub-action spaces
endows the EV with a rich repertoire of motion patterns,
which is critical for navigating complex interactions with
SVs exhibiting heterogeneous behaviors. After the RL policy
outputs decision variables, they are first decoded into a set of
concrete parameters and then passed to the MPC. Then the
MPC performs real-time trajectory planning and computes the
control commands applied to the EV.
Relevant training data during the interaction process of the
RL agent is stored in the replay buffer. After completing a
predetermined count of episodes, a gradient update is applied
to the RL policy. This update aims to optimize the objective
function J(Œ∏) that corresponds to the reward-curriculum pairs,
which can be expressed as follows:
Œ∏ ‚àó = arg

max
Œ∏,{(R1 ,C1 ),(R2 ,C2 ),¬∑¬∑¬∑ ,(RN ,CN )}

J(Œ∏).

(22)

V. E XPERIMENTS
A. Experimental Setup
To verify the effectiveness of the proposed method, we conduct comprehensive experiments and summarize the results.
We first compare the proposed method against both classical

9

baselines and recent state-of-the-art approaches on a multilane overtaking task. This scenario also serves as a qualitative
demonstration of the effective collaboration among the VLM
agents within our framework. We then further validate its
effectiveness in more complex autonomous driving scenarios,
including on-ramp merging and unsignalized intersections.
We also assess the compatibility of our framework with
different classic RL algorithms. We conduct all simulations
on a computer system equipped with an Intel(R) Core(TM)
i9-14900K CPU and an NVIDIA GeForce RTX 3090 GPU,
operating on the Ubuntu 18.04 system. All driving tasks are
constructed on the CARLA simulator [64], which is an opensource high-fidelity simulator. It features a suite of maps with
diverse driving scenarios and configurable driving styles for
SVs, which are capable of simulating a wide spectrum of
driving conditions. All SVs are controlled by the built-in
autopilot mode of the CARLA simulator, and each is assigned
a random driving style.
The proposed framework involves cooperation among multiple VLM agents and prompts with multimodal context. This
requires a VLM with exceptional capabilities for multimodal
information understanding and reasoning to build our multiagent system. Furthermore, our framework invokes the VLM
for inference only during the discrete stages of rewardcurriculum pair generation. Consequently, the invocation frequency is significantly lower than that of VLM-in-the-taskloop frameworks. Based on these considerations, we select the
GPT-4o model as the foundational VLM for constructing the
agents within our system. The number of parallel generations
is empirically set to 5. Proximal policy optimization (PPO)
[65] is adopted as the default algorithm to update the policy.
This choice is primarily motivated by its effective balance
between training stability and sample efficiency. Furthermore,
its inherent support for the multi-dimensional discrete action
space is critical for our task. The policy update is governed
by a clipped objective function as follows:
h

i
J(Œ∏) = E min œÅ(Œ∏)AÃÇ, clip (œÅ(Œ∏), 1 ‚àí œµ, 1 + œµ) AÃÇ , (23)
where œÅ(Œ∏) denotes the probability ratio between the updated
policy and the previous policy; AÃÇ represents the estimated
advantage; œµ is the clipping threshold.
For the following experiments, we implement the PPO
policy for the RL executor using an actor-critic architecture.
Both the actor and critic networks are constructed as multilayer perceptrons in PyTorch. The architecture of each network
comprises two hidden layers, the first with 256 neurons and
the second with 128. We utilize the Adam optimizer [66] for
training these networks. For the MPC module, the parameterized optimization problem is solved by employing the IPOPT
solver within the CasADi toolbox [67] and adopting a singleshooting approach. We train all policies for a maximum of
5000 episodes using the proposed OGR framework. Key hyperparameters include a discount factor Œ≥ of 0.99, a PPO clip
parameter œµ of 0.2. The learning rates for the actor and critic
networks are set to 5 √ó 10‚àí4 and 1 √ó 10‚àí3 , respectively, with
50 epochs per update. The update frequency parameters for
orchestration, reward generation, and curriculum generation

are set to 1000, 1000, and 100, respectively. Therefore, there
are a total of 5 training stages. When applying our method to
different experimental scenarios, the only modifications made
are to the textual descriptions of the driving task characteristics.
Here, we compare the proposed framework against the
following baseline and state-of-the-art (SOTA) methods:
‚Ä¢ Vanilla PPO [65]: the vanilla PPO baseline is trained
directly in the task scenario using a reward function
designed by a domain expert.
‚Ä¢ Curriculum PPO [45]: we train the PPO policy with an
expert-designed curriculum that distributes episodes over
a two-level curriculum set, which includes four traffic
densities (1:2:2:5 ratio) and three SV speed modes (1:1:3
ratio).
‚Ä¢ AutoReward [54]: a state-of-the-art method where the
policy is trained directly in the task scenario. In this approach, an LLM-generated reward function is iteratively
refined after the whole training process.
‚Ä¢ LearningFlow [59]: a SOTA approach that iteratively
generates the reward function and curriculum by invoking
LLMs during the training process.
‚Ä¢ OGR Without Orchestrator Agent (w/o OA): the proposed
method without the orchestrator agent, which serves an
ablation study for the orchestration module.
To ensure a fair comparison, the parameters for the downstream RL executors are held constant across all evaluated
methods. We use success rate (SR), collision rate (CR), and
time-out rate (TOR) as the main evaluation metrics, where a
successful episode is defined as the EV reaching the destination without collision and within the predefined maximum
time steps.

Fig. 4. A representative sample of the orchestrator.

B. Demonstration of Multi-VLM-Agent Collaboration
To qualitatively demonstrate the effectiveness of our framework, we provide a complete visual example that deconstructs

10

TABLE I
P ERFORMANCE COMPARISON AMONG DIFFERENT APPROACHES IN THE MULTI - LANE OVERTAKING TASK .
Empty

Methods
Vanilla PPO [65]
Curriculum PPO [45]
AutoReward (iter=0) [54]
AutoReward (iter=5) [54]
LearningFlow [59]
OGR (w/o OA)
OGR

Low Density

Medium Density

High Density

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

99
100
99
100
100
100
100

1
0
0
0
0
0
0

0
0
1
0
0
0
0

80
92
73
85
96
97
99

20
8
27
15
4
3
1

0
0
0
0
0
0
0

69
83
60
76
90
92
95

31
17
40
24
10
8
5

0
0
0
0
0
0
0

62
77
48
70
85
89
93

38
23
52
30
15
11
7

0
0
0
0
0
0
0

Fig. 6. A representative sample of the curriculum analyst.

Fig. 5. A representative sample of the reward analyst.

the collaborative workflow among the multiple agents within
our framework during a typical training stage. The process begins as shown in Fig. 4, which illustrates how the orchestrator
establishes a high-level training objective for the current stage
and generates initial suggestions for reward and curriculum
generation. Subsequently, the analysis phase is depicted in
Fig. 5 and Fig. 6, where the reward analyst and curriculum
analyst receive corresponding guidance from the orchestrator
and perform in-depth analyses by combining it with their
unique contextual information. The outputs of this analysis
phase are passed to the reward generator and curriculum
generator, respectively, to guide the final generation of the

Fig. 7. A representative sample of the reward generator.

11

reward-curriculum pairs, which are shown in Fig. 7 and Fig.
8.
Within the environments parameterized by these pairs, the
RL agent explores, collects experience, and updates the RL
policy. After a predetermined number of updates, the multiple
intermediate policies from all parallel branches undergo an
in-training test. Finally, as shown in Fig. 9, the multimodal
results of these tests and corresponding contexts are provided
to the policy reflector. Through a comprehensive evaluation,
this agent selects the optimal intermediate RL policy, which
then serves as the initial policy for the subsequent training
stage, thus completing the entire closed loop.

superior performance in every tested configuration. Notably,
even as traffic density increases, the success rate of our method
degrades only slightly, which demonstrates that our framework
extends the performance boundary of the RL policy.
The ablation study on the orchestrator agent further highlights its core value. We find that without the coordination of
this module, the task performance of the policy degraded comprehensively, a phenomenon that is particularly pronounced
in high-density traffic. We hypothesize that this is due to an
accumulated divergence between the objectives of the reward
function and the training curriculum as training progresses.
Therefore, we conclude that the dynamic coordination for the
training objectives of the orchestrator agent is a key factor in
effectively improving policy performance.
Furthermore, statistical test results reveal performance differences among various RL policy learning paradigms. First,
compared to baselines that rely solely on human expert
knowledge, most methods that leverage LLM and VLM assistance for training generally exhibit superior performance.
The performance of the AutoReward, after five iterations of
reward generation, significantly outperforms that of the vanilla
RL approach. This finding demonstrates the capability of
LLMs to generate high-quality reward functions. However,
as it only automates the reward generation component, its
final performance does not surpass that of Curriculum RL,
which is trained with a well-designed manual curriculum. A
comparison among methods employing CL techniques further
reveals that policies trained with automated design of rewards
and curricula comprehensively outperform those using only
a manually designed curriculum. Finally, when compared to
SOTA methods such as LearningFlow and AutoReward, our
framework achieves superior task performance by leveraging
multimodal information during the multi-agent collaboration
process. This advantage is particularly evident in the multilane overtaking tasks with the high-density setting.
We also evaluate the performance evolution of our proposed
method at different training stages, specifically within the
multi-lane overtaking task under various traffic densities. The
statistical results are summarized in Table II, which reveals
a clear evolution pattern. In the early stages of training, the
RL policy first achieves rapid performance gains in simple tasks with lower density. As training progresses, guided
by the adaptive reward functions and curricula dynamically
generated by our framework, the performance of policies in
more complex tasks also steadily improves. This progressive
learning progression indicates that our framework successfully provides appropriate reward-curriculum pairs throughout
the entire training process, enabling efficient policy learning
through an easy-to-hard progression.

C. Comparative Results and Analysis

D. Generalization Performance in Different Driving Scenarios

For a quantitative comparison among different methods, we
tested the policies trained by our proposed method against
several baselines. Specifically, each policy trained on the
multi-lane overtaking task is subjected to 100 repeated evaluation trials under various traffic densities. Table I presents
the statistical results, which reveal that our approach exhibits

To further validate the generalization capability of our
proposed framework, we applied it to two additional and
distinct urban driving scenarios for both training and testing,
including on-ramp merging and unsignalized intersections. It
is worth emphasizing that transferring the framework to these
new tasks is highly efficient, as we only need to modify a

Fig. 8. A representative sample of the curriculum generator.

Fig. 9. A representative sample of the policy reflector.

12

Fig. 10. Key snapshots of our method in the multi-lane overtaking scenario in the CARLA simulator. The top and bottom of each sub-figure provide thirdperson views and bird-eye views, respectively. In the third-person view, the green rectangles denote the selected waypoints of the RL policy. In the bird-eye
view, the red rectangle represents the EV, while the green rectangles indicate the SVs.

Fig. 11. Key snapshots of our method in the on-ramp merging scenario in the CARLA simulator. The EV is shown as a red rectangle and the SVs as green
rectangles.

TABLE II
P ERFORMANCE EVOLUTION OF THE PROPOSED METHOD IN THE MULTI -L ANE OVERTAKING TASK ACROSS DIFFERENT TRAINING STAGES .
Empty

Traning Stage

Low Density

Medium Density

High Density

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

100
100
100
100
100

0
0
0
0
0

0
0
0
0
0

92
98
99
98
99

8
2
1
2
1

0
0
0
0
0

63
72
83
91
95

25
18
17
9
5

12
10
0
0
0

45
57
71
85
93

31
30
29
15
7

24
13
0
0
0

1
2
3
4
5

TABLE III
G ENERALIZATION PERFORMANCE AMONG DIFFERENT URBAN DRIVING TASKS .
Empty

Task Types
Merging
Unsignalized
Intersection

Left Turn
Go Straight
Right Turn

Low Density

Medium Density

High Density

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

100

0

0

99

1

0

96

4

0

94

6

TOR (%)
0

100
100
100

0
0
0

0
0
0

97
100
100

3
0
0

0
0
0

93
95
100

7
5
0

0
0
0

90
91
99

10
9
1

0
0
0

13

(a) Left-turn task.

(b) Go-straight task.

(c) Right-turn task.
Fig. 12. Key snapshots of our method in unsignalized intersection scenarios in the CARLA simulator. The EV is shown as a red rectangle and the SVs as
green rectangles.

small amount of scene-specific contextual prompts. All other
core training and testing hyperparameters are kept identical to
those in the prior multi-lane overtaking scenario.
The statistical results, presented in Table III, demonstrate
that our method achieves exceptionally high success rates
across these disparate task types and under different traffic
densities. This outcome indicates that the proposed framework
can effectively handle diverse motion patterns and interaction
paradigms of SVs, thereby confirming its superior generalization ability across different types of driving tasks. Crucially,
this level of performance is achieved with minimal human ef-

fort and time costs for adaptation, highlighting the practicality
and scalability of our approach.

E. Demonstration of Maneuvering Abilities in Driving Tasks
To visualize the driving skills acquired through our framework, we present representative test cases for each driving
task, illustrating the ability of policies to handle complex
interactions. Figs. 10-12 depict the key snapshots of these
demonstrations. A detailed description of each demonstration
is provided below.

14

TABLE IV
C OMPATIBILITY PERFORMANCE WITH DIFFERENT RL ALGORITHMS IN THE MULTI - LANE OVERTAKING SCENARIO .
Empty

Methods

Vanilla DQN
OGR DQN
Vanilla SAC
OGR SAC
Vanilla PPO
OGR PPO

Low Density

Medium Density

High Density

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

SR (%)

CR (%)

TOR (%)

100
100
100
100
99
100

0
0
0
0
1
0

0
0
0
0
0
0

78
95
82
95
80
99

22
5
18
5
20
1

0
0
0
0
0
0

70
91
72
89
69
95

30
9
28
11
31
5

0
0
0
0
0
0

59
86
63
85
62
93

41
14
37
15
38
7

0
0
0
0
0
0

1) Multi-Lane Overtaking Task: In Fig. 10, the initial
position of the EV is set to the middle lane, with two SVs
ahead in the same lane and another SV in the right lane.
At 0.0 s, after perceiving the traffic in the middle and right
lanes, the RL policy infers that the left lane is suitable for
performing an overtaking maneuver. It consequently selects a
waypoint to the front-left as a high-level reference and begins
to accelerate, initiating a left lane change maneuver. However,
the traffic situation evolves dynamically at 1.4 s as the SV in
the middle lane closer to the EV suddenly changes lanes to the
left. Our policy demonstrates excellent reactive capabilities. It
recognizes that sufficient space will open up in the middle
lane, and then immediately aborts its initial intent to overtake
on the left. Instead, it re-evaluates and decides to back into the
middle lane. By 3.2 s, the EV successfully catches up with the
two SVs ahead by maintaining a high speed in the middle lane.
At 5.9 s, the policy once again assesses the traffic situation.
After determining that the left SV is slower than the right
one, it again selects a waypoint to the front-left, initiating an
attempt at a left lane change to overtake. Finally, at 7.2 s, the
EV safely and efficiently completes the multi-lane overtaking
task under the guidance of the RL policy.
2) On-Ramp Merging Task: In Fig. 11, the initial position
of the EV is set to a random location on the merging lane. A
key constraint is introduced by placing a stationary obstacle
vehicle at the end of the ramp, requiring the EV to merge
into the main carriageway before colliding with the obstacle
vehicle. Concurrently, multiple SVs are populated on the main
carriageway, traveling at various speeds. During the initial
phase, from 0.0 s to 2.8 s, the RL policy selects a nearby
waypoint ahead as a reference, guiding the EV to advance
along the ramp at a moderate speed while actively seeking
a safe gap in the traffic to merge into the main flow. The
critical decision-making moment occurs at 5.5 s. At this point,
the policy accurately detects that an SV approaching from its
rear-right is decelerating and correctly infers this behavior as a
yielding intention. Based on this crucial judgment, the policy
decisively selects a waypoint to the front-right to initiate the
merging maneuver. From 6.1 s to 7.0 s, the EV smoothly
completes the maneuver, ultimately integrating safely and
efficiently into the traffic on the main carriageway under the
continuous guidance of the RL policy.
3) Left-Turn Task: In Fig. 12(a), the EV is initialized in the
left lane. At 0.0 s, due to three SVs approaching the center
area from three different directions simultaneously, the RL
policy guides the EV to drive at a low speed. At 3.0 s, as
the SV from the left area maintains its speed while passing

through the center area, the RL policy selects a closer waypoint
to guide the EV to decelerate and yield. Between 4.2 s and
4.8 s, the SV from the left has already cleared the potential
collision area, and the oncoming SV decelerates, exhibiting
a yielding intention. Consequently, the RL policy chooses a
farther waypoint and a high reference speed to guide the EV
rapidly through the intersection. Finally, at 8.7 s, the EV safely
and efficiently completes the left-turn task under the guidance
of the RL policy.
4) Go-Straight Task: In Fig. 12(b), the EV is initialized in
the right lane. Starting from 0.0 s, the RL policy guides the EV
to approach the intersection at a low speed. At 3.0 s, as the SV
from the left area maintains a high speed to proceed straight,
the RL policy chooses a closer waypoint and a low reference
speed to guide the EV to decelerate and yield. Between 4.4
s and 5.8 s, since the SV from the left has already cleared
the intersection, the RL policy chooses a farther waypoint
and a high reference speed to guide the EV to traverse the
intersection. Finally, at 7.3 s, the EV safely and efficiently
completes the go-straight task.
5) Right-Turn Task: In Fig. 12(c), the EV is positioned in
the left lane. From 0.0 s to 1.7 s, the RL policy first executes a
preparatory right lane change and guides the EV to approach
the intersection at a low speed, positioning it for the upcoming
right turn. At 2.8 s, after assessing the traffic situation, the
policy infers that the trajectory of the SV approaching from the
left will not conflict with the intended path of the EV. Based
on this safety assessment, it begins to guide the EV into the
intersection to execute a small-radius right turn. Subsequently,
between 3.5 s and 5.1 s, the EV smoothly finishes the rightturn task.
TABLE V
AVERAGE , MAXIMUM , AND MINIMUM COMPUTATION TIME ( S ) OF THE
PROPOSED METHOD IN VARIOUS URBAN SCENARIOS .

Overtaking
Merging
Left-Turn
Go-Straight
Right-Turn

Average

Maximum

Minimum

0.025
0.026
0.027
0.028
0.028

0.116
0.114
0.116
0.118
0.118

0.009
0.009
0.010
0.009
0.010

To evaluate the real-time performance of our framework,
we conduct a statistical analysis of the inference and computation times for the RL executor across different driving
tasks. The results are summarized in Table V. It indicates
that the RL executor exhibits stable real-time performance
in all driving tasks. Specifically, the maximum computation

15

Fig. 13. Key snapshots of a hardware experiment demonstration in the multi-lane overtaking scenario. In this case, the EV (circled in red) overtakes the
low-speed SV (circled in green) ahead by changing lanes to the right. The top sub-figures show the third-person views, while the bottom sub-figures show
the corresponding top views. The red and green arrows represent the trajectories of the EV and SV, respectively.

Fig. 14. Key snapshots of a hardware experiment demonstration in the on-ramp merging scenario. In this case, the EV (circled in red) detects the SV (circled
in green) decelerating to yield, and therefore proceeds first to complete the merging task.

time for any task is approximately 0.1 seconds, even in the
worst case. Furthermore, the results of average computation
time demonstrate that the executor consistently operates at a
frequency above 30 Hz across all task types. Therefore, the
computational efficiency of the RL executor can fully satisfy
the real-time requirements for autonomous driving tasks.

multi-discrete action spaces of this task. Specifically, PPO
is generally considered to be more adept in such action
spaces. Therefore, this result demonstrates that our method
exhibits excellent compatibility with a range of mainstream
RL algorithms.
G. Real-World Experiments

F. Compatibility with Different RL Algorithms
In all preceding experiments, the downstream RL executor is trained using the PPO algorithm. To comprehensively
evaluate the compatibility of our framework with different RL
algorithms, we select two additional representative algorithms
from the RL family, including DQN and SAC. These are
used to train policies for the multi-lane overtaking task. To
ensure a fair comparison, the relevant hyperparameters for
all algorithms are carefully tuned to achieve their respective
optimal performance. We then directly compare the policies
trained using our framework in conjunction with each RL
algorithm against their corresponding vanilla baselines.
The statistical results of all three RL algorithms are summarized in Table IV. The quantitative result shows that, regardless
of the underlying RL algorithm, policies trained with our
integrated framework significantly outperform their vanilla RL
counterparts across all test settings. This result indicates that
our framework can universally enhance the training process
for different types of RL algorithms, thereby substantially
improving both sample efficiency and final task performance.
Although the performance with DQN and SAC under the
proposed framework is slightly inferior to that with PPO, we
attribute this discrepancy primarily to the intrinsic adaptability
of each algorithm to the high-dimensional continuous and

To further validate the practical effectiveness of our approach, we carry out a series of real-world experiments. In
these experiments, the Tianbot T100, which is equipped with
an NVIDIA Jetson Xavier NX, serves as the EV. Meanwhile,
the Agilex LIMO platform is employed as the human-operated
SV. The experimental setup is implemented using the Robot
Operating System (ROS) [68], running on a laptop with an
11th Gen Intel(R) Core(TM) i7-1165G7 CPU operating at 2.80
GHz. Vehicle pose information is captured in real time by the
OptiTrack motion tracking system at 200 frames per second.
For low-level control, a pure pursuit algorithm is applied to
follow the high-level decisions generated by RL policies.
Similar to the simulation experiments, we consider three
driving scenarios and five driving tasks, including the multilane overtaking scenario, on-ramp merging scenario, and
unsignalized intersection scenario. Snapshots of these experiments are shown in Figs. 13-15. By deploying our trained RL
policies in various interactive driving tasks, we find that the
EV can make appropriate decisions in response to the diverse
intentions exhibited by the SV, thereby completing all driving
tasks successfully. Several representative cases illustrate this
capability. In the multi-lane overtaking scenario shown in Fig.
13, when faced with a lead vehicle maintaining a low speed,
the RL policy decisively executes a right lane change and

16

(a) Left-turn task. In this case, the EV detects the SV decelerating to yield and directly performs a left turn to pass through the center area of the
intersection.

(b) Go-straight task. In this case, the EV detects that the SV approaching from the left area maintains its speed and goes straight. Therefore, the EV
slows down to yield and passes through the center area of the intersection after the SV to complete the go-straight task.

(c) Right-turn task. In this case, the EV detects the SV from the left area decelerating to yield and directly performs a right turn to pass through the
center area of the intersection.
Fig. 15. Key snapshots of three hardware experiment demonstrations in the unsignalized intersection scenario. The EV is highlighted by a red circle and the
SV by a green one.

accelerates, completing the overtaking maneuver safely and
efficiently. In Fig. 14, at a critical merging interaction point,
the policy infers a yielding intention from a deceleration of
the SV. It immediately selects a further waypoint and a high
reference speed to seize the opportunity, quickly completing
the merge and thus improving traffic efficiency. The three
subfigures in Fig. 15 provide a detailed illustration of how our
RL policy flexibly adapts its driving strategy to safely and efficiently complete different tasks based on the various intentions
of SVs in the unsignalized intersection scenario. In the leftturn task shown in Fig. 15(a), the RL policy infers a yielding
intention from the SV that is decelerating as it approaches the
intersection. Therefore, the policy decisively proceeds before
the SV, safely passing through the intersection to efficiently
complete the left-turn maneuver. This demonstration shows

the ability to seize opportunities of the policy. In contrast,
the go-straight task in Fig. 15(b) showcases the necessary
defensive driving behavior of the policy. Recognizing that the
SV is maintaining the current velocity, the policy proactively
decelerates to yield before entering the conflict zone. After
confirming the SV has safely passed, the policy guides the EV
to accelerate and cross the intersection. Finally, in the rightturn task depicted in Fig. 15(c), the policy again correctly
interprets the yielding intention of the SV and maintains
its heading and speed to smoothly complete the right-turn
maneuver.
In conclusion, the above experiments validate the effectiveness of our framework on hardware platforms. The results demonstrate a successful deployment and highlight the
superior performance of the proposed method in safe and

17

flexible interaction with SVs of diverse driving intentions
across various tasks.
VI. C ONCLUSION
In this work, we proposed OGR, an advanced automated
policy learning framework based on VLM-based multi-agent
collaboration. The framework innovatively integrates multiple
VLM agents with an RL agent, thereby achieving efficient RL
policy training. Specifically, our framework is composed of an
orchestration module for training planning, a two-step analyzethen-generate generation module, and an in-training reflection
module for online evaluation and selection. To further enhance
the robustness of our framework, we integrated parallel generation techniques and a human-in-the-loop reward observation
space augmentation mechanism. This multi-agent collaborative
system enables the fully automated iterative optimization of
reward functions, training curricula, and RL policies. The deep
leveraging of multimodal information significantly improves
the quality of the generated reward-curriculum pairs, which
ultimately translates into superior sample efficiency and task
performance of the RL policy. Comprehensive simulation
results showcase the effective collaboration among the various
agents and demonstrate the superior performance and scalability of our framework on various urban driving tasks and
different RL algorithms.Furthermore, real-world experiments
validate the potential of our method to be effectively deployed
on physical platforms. A promising direction for future work
involves leveraging advanced diffusion models to enhance the
representational capacity of RL policies.
R EFERENCES
[1] P. Xu, X. Zhu, and D. A. Clifton, ‚ÄúMultimodal learning with transformers: A survey,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 45, no. 10, pp. 12 113‚Äì12 132, 2023.
[2] D. Hanover, A. Loquercio, L. Bauersfeld, A. Romero, R. Penicka,
Y. Song, G. Cioffi, E. Kaufmann, and D. Scaramuzza, ‚ÄúAutonomous
drone racing: A survey,‚Äù IEEE Transactions on Robotics, vol. 40, pp.
3044‚Äì3067, 2024.
[3] X. Han, S. Chen, Z. Fu, Z. Feng, L. Fan, D. An, C. Wang, L. Guo,
W. Meng, X. Zhang et al., ‚ÄúMultimodal fusion and vision-language
models: A survey for robot vision,‚Äù arXiv preprint arXiv:2504.02477,
2025.
[4] L. Zheng, R. Yang, M. Zheng, M. Y. Wang, and J. Ma, ‚ÄúSafe and realtime consistent planning for autonomous vehicles in partially observed
environments via parallel consensus optimization,‚Äù IEEE Transactions
on Intelligent Transportation Systems, pp. 1‚Äì17, 2025.
[5] Z. Huang, Y. Xie, B. Ma, S. Shen, and J. Ma, ‚ÄúFast and scalable
game-theoretic trajectory planning with intentional uncertainties,‚Äù arXiv
preprint arXiv:2507.12174, 2025.
[6] P. Cai and D. Hsu, ‚ÄúClosing the planning‚Äìlearning loop with application
to autonomous driving,‚Äù IEEE Transactions on Robotics, vol. 39, no. 2,
pp. 998‚Äì1011, 2023.
[7] J. Li, D. Isele, K. Lee, J. Park, K. Fujimura, and M. J. Kochenderfer,
‚ÄúInteractive autonomous navigation with internal state inference and
interactivity estimation,‚Äù IEEE Transactions on Robotics, vol. 40, pp.
2932‚Äì2949, 2024.
[8] Z. Zhu and H. Zhao, ‚ÄúA survey of deep RL and IL for autonomous
driving policy learning,‚Äù IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 9, pp. 14 043‚Äì14 065, 2021.
[9] P. S. Chib and P. Singh, ‚ÄúRecent advancements in end-to-end autonomous driving using deep learning: A survey,‚Äù IEEE Transactions
on Intelligent Vehicles, vol. 9, no. 1, pp. 103‚Äì118, 2023.
[10] J. Cheng, Y. Chen, and Q. Chen, ‚ÄúPluto: Pushing the limit of imitation learning-based planning for autonomous driving,‚Äù arXiv preprint
arXiv:2404.14327, 2024.

[11] H. Liao, Z. Li, K. Zhu, K. Li, and C. Xu, ‚ÄúSA-TP2 : A safety-aware
trajectory prediction and planning model for autonomous driving,‚Äù IEEE
Transactions on Robotics, pp. 1‚Äì20, 2025.
[12] S. Ross, G. Gordon, and D. Bagnell, ‚ÄúA reduction of imitation learning
and structured prediction to no-regret online learning,‚Äù in Proceedings of
International Conference on Artificial Intelligence and Statistics, 2011,
pp. 627‚Äì635.
[13] M. Zare, P. M. Kebria, A. Khosravi, and S. Nahavandi, ‚ÄúA survey of
imitation learning: Algorithms, recent developments, and challenges,‚Äù
IEEE Transactions on Cybernetics, vol. 54, no. 12, pp. 7173‚Äì7186, 2024.
[14] Z. Peng, Y. Wang, L. Zheng, and J. Ma, ‚ÄúBilevel multi-armed banditbased hierarchical reinforcement learning for interaction-aware selfdriving at unsignalized intersections,‚Äù IEEE Transactions on Vehicular
Technology, vol. 74, no. 6, pp. 8824‚Äì8838, 2025.
[15] X. Wang, S. Wang, X. Liang, D. Zhao, J. Huang, X. Xu, B. Dai, and
Q. Miao, ‚ÄúDeep reinforcement learning: A survey,‚Äù IEEE Transactions
on Neural Networks and Learning Systems, vol. 35, no. 4, pp. 5064‚Äì
5078, 2024.
[16] E. Bƒ±yƒ±k, D. P. Losey, M. Palan, N. C. Landolfi, G. Shevchuk, and
D. Sadigh, ‚ÄúLearning reward functions from diverse sources of human
feedback: Optimally integrating demonstrations and preferences,‚Äù The
International Journal of Robotics Research, vol. 41, no. 1, pp. 45‚Äì67,
2022.
[17] R. Yu, S. Wan, Y. Wang, C.-X. Gao, L. Gan, Z. Zhang, and D.-C.
Zhan, ‚ÄúReward models in deep reinforcement learning: A survey,‚Äù arXiv
preprint arXiv:2506.15421, 2025.
[18] Y. Yu, ‚ÄúTowards sample efficient reinforcement learning,‚Äù in Proceedings
of the International Joint Conference on Artificial Intelligence. IJCAI,
2018, pp. 5739‚Äì5743.
[19] Z. Huang, J. Wu, and C. Lv, ‚ÄúEfficient deep reinforcement learning with
imitative expert priors for autonomous driving,‚Äù IEEE Transactions on
Neural Networks and Learning Systems, vol. 34, no. 10, pp. 7391‚Äì7403,
2023.
[20] Y. Hu, W. Wang, H. Jia, Y. Wang, Y. Chen, J. Hao, F. Wu, and
C. Fan, ‚ÄúLearning to utilize shaping rewards: A new approach of reward
shaping,‚Äù Advances in Neural Information Processing Systems, vol. 33,
pp. 15 931‚Äì15 941, 2020.
[21] R. Devidze, P. Kamalaruban, and A. Singla, ‚ÄúExploration-guided reward
shaping for reinforcement learning under sparse rewards,‚Äù Advances in
Neural Information Processing Systems, vol. 35, pp. 5829‚Äì5842, 2022.
[22] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone,
‚ÄúCurriculum learning for reinforcement learning domains: A framework
and survey,‚Äù Journal of Machine Learning Research, vol. 21, no. 181,
pp. 1‚Äì50, 2020.
[23] X. Wang, Y. Chen, and W. Zhu, ‚ÄúA survey on curriculum learning,‚Äù IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 44,
no. 9, pp. 4555‚Äì4576, 2022.
[24] J. Zhang, J. Huang, S. Jin, and S. Lu, ‚ÄúVision-language models for vision
tasks: A survey,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 46, no. 8, pp. 5625‚Äì5644, 2024.
[25] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, ‚ÄúA survey on
multimodal large language models,‚Äù National Science Review, vol. 11,
no. 12, pp. 1‚Äì20, 2024.
[26] X. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan, P. Jia, X. Lang,
and H. Zhao, ‚ÄúDrivevlm: The convergence of autonomous driving and
large vision-language models,‚Äù in Proceedings of the Conference on
Robot Learning, 2024.
[27] C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, J. Bei√üwenger,
P. Luo, A. Geiger, and H. Li, ‚ÄúDriveLM: Driving with graph visual
question answering,‚Äù in European Conference on Computer Vision.
Springer, 2024, pp. 256‚Äì274.
[28] C. Guo, X. Liu, C. Xie, A. Zhou, Y. Zeng, Z. Lin, D. Song, and B. Li,
‚ÄúRedcode: Risky code execution and generation benchmark for code
agents,‚Äù Advances in Neural Information Processing Systems, vol. 37,
pp. 106 190‚Äì106 236, 2024.
[29] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan,
and A. Anandkumar, ‚ÄúVoyager: An open-ended embodied agent with
large language models,‚Äù Transactions on Machine Learning Research,
pp. 1‚Äì21, 2024.
[30] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, ‚ÄúCamel:
Communicative agents for ‚Äúmind‚Äù exploration of large language model
society,‚Äù Advances in Neural Information Processing Systems, vol. 36,
pp. 51 991‚Äì52 008, 2023.
[31] M. Hu, Y. Zhou, W. Fan, Y. Nie, B. Xia, T. Sun, Z. Ye, Z. Jin,
Y. Li, Q. Chen et al., ‚ÄúOwl: Optimized workforce learning for general
multi-agent assistance in real-world task automation,‚Äù arXiv preprint
arXiv:2505.23885, 2025.

18

[32] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang,
Z. Wang, S. K. S. Yau, Z. Lin et al., ‚ÄúMetaGPT: Meta Programming for
A Multi-Agent Collaborative Framework,‚Äù in International Conference
on Learning Representations, 2024.
[33] R. Zhao, Y. Li, Y. Fan, F. Gao, M. Tsukada, and Z. Gao, ‚ÄúA survey on
recent advancements in autonomous driving using deep reinforcement
learning: Applications, challenges, and solutions,‚Äù IEEE Transactions on
Intelligent Transportation Systems, vol. 25, no. 12, pp. 19 365‚Äì19 398,
2024.
[34] M. Al-Sharman, R. Dempster, M. A. Daoud, M. Nasr, D. Rayside, and
W. Melek, ‚ÄúSelf-learned autonomous driving at unsignalized intersections: A hierarchical reinforced learning approach for feasible decisionmaking,‚Äù IEEE Transactions on Intelligent Transportation Systems,
vol. 24, no. 11, pp. 12 345‚Äì12 356, 2023.
[35] Y. Wang, Z. Peng, Y. Xie, Y. Li, H. Ghazzai, and J. Ma, ‚ÄúLearning the
references of online model predictive control for urban self-driving,‚Äù
arXiv preprint arXiv:2308.15808, 2023.
[36] J. Chen, S. E. Li, and M. Tomizuka, ‚ÄúInterpretable end-to-end urban
autonomous driving with latent deep reinforcement learning,‚Äù IEEE
Transactions on Intelligent Transportation Systems, vol. 23, no. 6, pp.
5068‚Äì5078, 2022.
[37] D. Chen, H. Li, Z. Jin, H. Tu, and M. Zhu, ‚ÄúRisk-anticipatory autonomous driving strategies considering vehicles‚Äô weights based on hierarchical deep reinforcement learning,‚Äù IEEE Transactions on Intelligent
Transportation Systems, vol. 25, no. 12, pp. 19 605‚Äì19 618, 2024.
[38] A. Abouelazm, J. Michel, and J. M. ZoÃàllner, ‚ÄúA review of reward
functions for reinforcement learning in the context of autonomous
driving,‚Äù in IEEE Intelligent Vehicles Symposium, 2024, pp. 156‚Äì163.
[39] M. Golchoubian, M. Ghafurian, K. Dautenhahn, and N. L. Azad,
‚ÄúUncertainty-aware drl for autonomous vehicle crowd navigation in
shared space,‚Äù IEEE Transactions on Intelligent Vehicles, vol. 9, no. 12,
pp. 7931‚Äì7944, 2024.
[40] Z. Huang, H. Liu, J. Wu, and C. Lv, ‚ÄúConditional predictive behavior
planning with inverse reinforcement learning for human-like autonomous
driving,‚Äù IEEE Transactions on Intelligent Transportation Systems,
vol. 24, no. 7, pp. 7244‚Äì7258, 2023.
[41] W. Li, F. Qiu, L. Li, Y. Zhang, and K. Wang, ‚ÄúSimulation of vehicle
interaction behavior in merging scenarios: A deep maximum entropyinverse reinforcement learning method combined with game theory,‚Äù
IEEE Transactions on Intelligent Vehicles, vol. 9, no. 1, pp. 1079‚Äì1093,
2024.
[42] Y. Guan, Y. Ren, S. E. Li, Q. Sun, L. Luo, and K. Li, ‚ÄúCentralized
cooperation for connected and automated vehicles at intersections by
proximal policy optimization,‚Äù IEEE Transactions on Vehicular Technology, vol. 69, no. 11, pp. 12 597‚Äì12 608, 2020.
[43] Y. Song, H. Lin, E. Kaufmann, P. DuÃàrr, and D. Scaramuzza, ‚ÄúAutonomous overtaking in Gran Turismo sport using curriculum reinforcement learning,‚Äù in IEEE International Conference on Robotics and
Automation, 2021, pp. 9403‚Äì9409.
[44] S. Khaitan and J. M. Dolan, ‚ÄúState dropout-based curriculum reinforcement learning for self-driving at unsignalized intersections,‚Äù in IEEE/RSJ
International Conference on Intelligent Robots and Systems, 2022, pp.
12 219‚Äì12 224.
[45] Z. Peng, X. Zhou, Y. Wang, L. Zheng, M. Liu, and J. Ma, ‚ÄúCurriculum
proximal policy optimization with stage-decaying clipping for selfdriving at unsignalized intersections,‚Äù in Proceedings of the International
Intelligent Transportation Systems Conference, 2023, pp. 5027‚Äì5033.
[46] Z. Peng, X. Zhou, L. Zheng, Y. Wang, and J. Ma, ‚ÄúReward-driven
automated curriculum learning for interaction-aware self-driving at
unsignalized intersections,‚Äù in IEEE/RSJ International Conference on
Intelligent Robots and Systems, 2024.
[47] J. Mei, Y. Ma, X. Yang, L. Wen, X. Cai, X. Li, D. Fu, B. Zhang,
P. Cai, M. Dou et al., ‚ÄúContinuously learning, adapting, and improving:
A dual-process approach to autonomous driving,‚Äù Advances in Neural
Information Processing Systems, vol. 37, pp. 123 261‚Äì123 290, 2024.
[48] Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K.-Y. K. Wong, Z. Li, and
H. Zhao, ‚ÄúDriveGPT4: Interpretable end-to-end autonomous driving via
large language model,‚Äù IEEE Robotics and Automation Letters, vol. 9,
no. 10, pp. 8186‚Äì8193, 2024.
[49] H. Shao, Y. Hu, L. Wang, G. Song, S. L. Waslander, Y. Liu, and H. Li,
‚ÄúLMDrive: Closed-loop end-to-end driving with large language models,‚Äù
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2024, pp. 15 120‚Äì15 130.
[50] L. Wen, D. Fu, X. Li, X. Cai, T. Ma, P. Cai, M. Dou, B. Shi, L. He, and
Y. Qiao, ‚ÄúDiLu: A knowledge-driven approach to autonomous driving
with large language models,‚Äù in Proceedings of International Conference
on Learning Representations, 2024, pp. 1‚Äì20.

[51] X. Ye, F. Tao, A. Mallik, B. Yaman, and L. Ren, ‚ÄúLORD: Large models
based opposite reward design for autonomous driving,‚Äù in IEEE/CVF
Winter Conference on Applications of Computer Vision, 2025, pp. 5072‚Äì
5081.
[52] Z. Huang, Z. Sheng, Y. Qu, J. You, and S. Chen, ‚ÄúVLM-RL: A unified
vision language models and reinforcement learning framework for safe
autonomous driving,‚Äù arXiv preprint arXiv:2412.15544, 2024.
[53] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,
Y. Zhu, L. Fan, and A. Anandkumar, ‚ÄúEureka: Human-level reward design via coding large language models,‚Äù in Proceedings of International
Conference on Learning Representations, 2024, pp. 1‚Äì14.
[54] X. Han, Q. Yang, X. Chen, Z. Cai, X. Chu, and M. Zhu, ‚ÄúAutoreward:
Closed-loop reward design with large language models for autonomous
driving,‚Äù IEEE Transactions on Intelligent Vehicles, pp. 1‚Äì13, 2024.
[55] J. Li, M. Stamatopoulou, and D. Kanoulas, ‚ÄúSDS‚Äìsee it, do it, sorted:
Quadruped skill synthesis from single video demonstration,‚Äù arXiv
preprint arXiv:2410.11571, 2024.
[56] W. Liang, S. Wang, H.-J. Wang, O. Bastani, D. Jayaraman, and Y. J.
Ma, ‚ÄúEnvironment curriculum generation via large language models,‚Äù in
Proceedings of the Conference on Robot Learning, 2024, pp. 1‚Äì14.
[57] K. Ryu, Q. Liao, Z. Li, K. Sreenath, and N. Mehr, ‚ÄúCurricullm:
Automatic task curricula design for learning complex robot skills using
large language models,‚Äù arXiv preprint arXiv:2409.18382, 2024.
[58] Z. Sheng, Z. Huang, Y. Qu, Y. Leng, S. Bhavanam, and S. Chen,
‚ÄúCurricuvlm: Towards safe autonomous driving via personalized safetycritical curriculum learning with vision-language models,‚Äù arXiv preprint
arXiv:2502.15119, 2025.
[59] Z. Peng, Y. Wang, X. Han, L. Zheng, and J. Ma, ‚ÄúLearningFlow: Automated policy learning workflow for urban driving with large language
models,‚Äù arXiv preprint arXiv:2501.05057, 2025.
[60] S. Singh, R. L. Lewis, and A. G. Barto, ‚ÄúWhere do rewards come
from,‚Äù in Proceedings of the Annual Conference of the Cognitive Science
Society. Cognitive Science Society, 2009, pp. 2601‚Äì2606.
[61] D. Huang, J. M. Zhang, M. Luck, Q. Bu, Y. Qing, and H. Cui,
‚ÄúAgentcoder: Multi-agent-based code generation with iterative testing
and optimisation,‚Äù arXiv preprint arXiv:2312.13010, 2023.
[62] Y. Zhang, P. Abbeel, and L. Pinto, ‚ÄúAutomatic curriculum learning
through value disagreement,‚Äù Advances in Neural Information Processing Systems, vol. 33, pp. 7648‚Äì7659, 2020.
[63] S. Tao, A. Shukla, T.-k. Chan, and H. Su, ‚ÄúReverse forward curriculum
learning for extreme sample and demonstration efficiency in reinforcement learning,‚Äù in Proceedings of International Conference on Learning
Representations, 2024, pp. 1‚Äì13.
[64] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
‚ÄúCARLA: An open urban driving simulator,‚Äù in Proceedings of the
Conference on Robot Learning, 2017, pp. 1‚Äì16.
[65] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, ‚ÄúProximal policy optimization algorithms,‚Äù arXiv preprint arXiv:1707.06347,
2017.
[66] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980, 2014.
[67] J. A. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and M. Diehl,
‚ÄúCasadi: a software framework for nonlinear optimization and optimal
control,‚Äù Mathematical Programming Computation, vol. 11, pp. 1‚Äì36,
2019.
[68] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, A. Y. Ng et al., ‚ÄúROS: an open-source robot operating
system,‚Äù in ICRA Workshop on Open Source Software, vol. 3, no. 3.2.
Kobe, 2009, p. 5.

