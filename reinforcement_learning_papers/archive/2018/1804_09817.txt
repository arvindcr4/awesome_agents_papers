Multiagent Soft Q-Learning
Ermo Wei and Drew Wicke and David Freelan and Sean Luke

arXiv:1804.09817v1 [cs.AI] 25 Apr 2018

Department of Computer Science, George Mason University, Fairfax, VA USA
ewei@cs.gmu.edu, dwicke@gmu.edu, dfreelan@gmu.edu, sean@cs.gmu.edu

Abstract
Policy gradient methods are often applied to reinforcement
learning in continuous multiagent games. These methods perform local search in the joint-action space, and as we show,
they are susceptable to a game-theoretic pathology known
as relative overgeneralization. To resolve this issue, we propose Multiagent Soft Q-learning, which can be seen as the
analogue of applying Q-learning to continuous controls. We
compare our method to MADDPG, a state-of-the-art approach, and show that our method achieves better coordination in multiagent cooperative tasks, converging to better local optima in the joint action space.

Introduction
Multiagent reinforcement learning (or MARL) is a type of
Reinforcement Learning (RL) involving two or more agents.
The mechanism is similar to traditional reinforcement learning: the environment is some current state (which the agent
can only sense through its observation), the agents each perform some action while in that state, the agents each receive
some reward, the state transitions to some new state, and
than the process repeats. However in MARL, both transitions from state to state and the rewards allotted are functions of the joint action of the agents while in that state.
Each agent ultimately tries to learn a policy that maps its
observation to the optimal action in that state: but these are
individual actions, not joint actions, as ultimately an agent
cannot dictate the other agentsâ€™ actions.
Multiagent Learning has been investigated comprehensively in discrete action domains. Many methods have been
proposed for equilibrium learning (Littman 1994; 2001;
Hu and Wellman 2003; Greenwald, Hall, and Serrano 2003),
where the agents are trying to learn policies that satisfy some
equilibrium concept from Game Theory. Almost all the equilibrium learning methods that have been proposed are based
on off-policy Q-learning. This is not surprising, as multiagent equilibrium learning is naturally off-policy, that is, the
agents are trying to learn an equilibrium policy while exploring the environment by following another policy. However,
this situation does not apply to continuous games, that is,
games with continuous actions. When RL must be applied to
Copyright c 2018, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

continuous control, policy gradient methods are often taken
into consideration. However, in the past, it does not combine
with off-policy samples as easily as the tabular Q-Learning.
For this reason, RL has not been able to achieve as good performance in continuous games as it has in discrete domains.
In this paper, we consider cooperative games, where
the agents all have the same reward function. Cooperative
MARL problems can be categorized based on how much information each agent knows. If we have a central controller
to control the learning process of each agent, then we have
centralized training with decentralized execution (Oliehoek,
Spaan, and Vlassis 2008). If the agents are learning concurrently, and each agent is told what the other agent or
agents did, then the problem is known as a joint action
learner problem. If the agents are learning concurrently but
are not told what the others did, then we have an independent
learner problem.
When the information is limited for learners in cooperative games, as is the case with independent learners, a
pathology called relative overgeneralization can arise (Wei
and Luke 2016). Relative overgeneralization occurs when
a suboptimal Nash Equilibrium in the joint space of actions is preferred over an optimal Nash Equilibrium because
each agentâ€™s action in the suboptimal equilibrium is a better
choice when matched with arbitrary actions from the collaborating agents. For instance, consider a continuous game in
Figure 1. The axes i and j are the various actions that agents
Ai and Aj may perform (we assume the agents are performing deterministic actions), and the axis rewards (i, j) is the
joint reward received by the agents from a given joint action
hi, ji. Joint action M has a higher reward than joint action
N . However, the average of all possible rewards for action
iM , of agent Ai is lower than the average of all possible rewards for action iN . Thus, the agents tend to converge to
N.
In this paper, we first analytically show how relative
overgeneralization prevents policy gradient methods from
achieving better coordination in cooperative continuous
games. This is even true in centralized training if we are
not using the information wisely. Then we tackle the relative overgeneralization problem in these games by introducing Multiagent Soft Q-Learning, a novel method based on
Soft Q-Learning and deep energy-based policies (Haarnoja
et al. 2017). Our method is similar to MADDPG (Lowe et al.

reward(i,j)
200

N

M

0

iN

!200

!400

iM

!600
0

1

0.2
0.8
0.4

0.6
0.6

j

0.4
0.8

0.2
1

i

0

Figure 1: The relative overgeneralization pathology in continuous games.

R : S Ã— A â†’ R is the reward function for each agent i,
and T (s, ~a, s0 ) = P (s0 |s, ~a) is the transition function, where
~a = ha1 Â· Â· Â· an i âˆˆ A is the joint action of all agents. Thus
the reward the agents receive and the state to which they
transition depends on the current state and agentsâ€™ joint action. Each agent i determines its action using a policy Ï€ i .
We will also use âˆ’i to denote all agents except for agent
i. A POCSG can be thought as taking CSG into a partially
observable setting.
In the multiagent setting, a rational agent will play
its best response to the other agentsâ€™ strategy. If all
agents are following a policy that implements this
strategy, they will arrive at a Nash equilibrium defined as a solution where âˆ€i Ri (s, Ï€1âˆ— , . . . Ï€iâˆ— . . . Ï€nâˆ— ) â‰¥
âˆ—
âˆ—
Ri (s, Ï€1âˆ— , . . . , Ï€iâˆ’1
, Ï€i , Ï€i+1
, . . . , Ï€nâˆ— ) for all of the strategies Ï€i available to agent i. Ï€iâˆ— denotes the best response
policy of agent i.

Policy Gradient Methods
2017), a recently proposed centralized learning algorithm.
Thus, it belongs to the centralized training with decentralized execution paradigm. In this setting, since the training
is centralized and we use the information wisely, it avoids
the co-adaptation problem in Multiagent RL and greatly reduces the sample complexity, as the environment for agents
is stationary.

Background
In this section, we first give an introduction to Markov Decision Processes (MDP) and various generalizations. We then
introduce policy gradient methods.

Markov Decision Processes and Stochastic Games
A Markov Decision Process (or MDP) can be used to model
the interaction an agent has with its environment. An MDP
is a tuple {S, A, T, R, Î³, H} where S is the set of states; A
is the set of actions available to the agent; T is the transition
function T (s, a, s0 ) = P (s0 |s, a) defining the probability of
transitioning to state s0 âˆˆ S when in state s âˆˆ S and taking
action a âˆˆ A; R is the reward function R : S Ã— A 7â†’ R;
0 < Î³ < 1 is a discount factor; and H is the horizon time
of the MDP, that is, the number of steps the MDP runs.1 An
agent selects its actions based on the policy Ï€Î¸ (a|s), which
is a distribution over all possible actions a in state s parameterized by Î¸ âˆˆ Rn .
The concept of an MDP can be extended to partially observable (POMDP) settings, where agents do not directly
sense the state s. Rather, they receive some observation o
sampled from a distribution conditioned on s.
MDPs can also be generalized to a cooperative multiagent settings, called a Cooperative Stochastic Game or
CSG. This is a game with n agents (or players), defined
by the tuple {S, A, R, T, Î³, H}, where S is the state space,
A = A1 Ã— ... Ã— An is the joint action space of n agents,
1

Any infinite horizon MDP with discounted rewards can be
-approximated by a finite horizon MDP using a horizon H =
logÎ³ ((1âˆ’Î³))
(Jie and Abbeel 2010)
maxs,a |R(s,a)|

In single agent continuous control tasks, it is common to
apply a policy gradient method to determine an optimal policy. We describe that process here. To start, we define the
expected return J(Î¸) of a policy Ï€Î¸ as

 X
PÎ¸ (Ï„ )R(Ï„ ),
(1)
J(Î¸) = EPÎ¸ (Ï„ ) R(Ï„ ) =
Ï„

where PÎ¸ (Ï„ ) is the probability distribution over all possible
state-action trajectories Ï„ = hs0 , a0 , s1 , a1 , . . . , sH , aH i
induced by following policy Ï€Î¸ , and R(Ï„ )
=
PH t
Î³
R(s
,
a
)
is
the
discounted
accumulated
ret
t
t=0
ward along trajectory Ï„ . We want to compute the gradient
âˆ‡Î¸ J(Î¸), so that we can follow the gradient to a local
optimum in the space of policy parameters. To do this we
use the likelihood-ratio trick (Williams 1992), where we
write the gradient as
âˆ‡Î¸ J(Î¸) =

X
Ï„

âˆ‡Î¸ PÎ¸ (Ï„ )R(Ï„ ) =

X

PÎ¸ (Ï„ )

Ï„

âˆ‡Î¸ PÎ¸ (Ï„ )
R(Ï„ )
PÎ¸ (Ï„ )



=EPÎ¸ (Ï„ ) âˆ‡Î¸ ln PÎ¸ (Ï„ )R(Ï„ )
(2)
and estimate it by performing m sample trajectories hÏ„ (1) , . . . , Ï„ (m) i, calculating the corresponding terms, and Pthen taking the average, that is,
m
1
(j)
âˆ‡Î¸ J(Î¸) â‰ˆ m
)R(Ï„ (j) ). This polj=1 âˆ‡Î¸ ln PÎ¸ (Ï„
icy gradient method can also use off-policy samples by
introducing importance sampling, where we scale each term
Î¸ (Ï„ )
in the empirical expectation by PQ(Ï„
) , where Q is another
distribution from which our off-policy samples come. The
intuition behind Equation 2 is that the reward term R(Ï„ )
scales the gradient proportionally to the reward along the
trajectories.
One problem with using this likelihood-ratio estimator in
practice is that it suffers from a large variance, and thus
requires a great many samples to give an accurate estimation. There are various methods proposed to deal with this.
A first approach is to replace the Monte Carlo estimation

of the reward along trajectories R(Ï„ ) with a value function. This leads to the Stochastic (SPG) and Deterministic Policy Gradient (DPG) Theorems (Sutton et al. 1999;
Silver et al. 2014), shown below respectively:
Z
Z
âˆ‡Î¸ J(Î¸) = ÏÏ€Î¸ (s) âˆ‡Î¸ Ï€Î¸ (a|s)QÏ€Î¸ (s, a) da ds
S
A


Ï€
Î¸
= Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸ âˆ‡Î¸ ln Ï€Î¸ (a|s)Q (s, a)
Z
âˆ‡Î¸ J(Î¸) = ÏÏ€Î¸ (s)âˆ‡Î¸ Ï€Î¸ (s)âˆ‡a QÏ€Î¸ (s, a)|a=Ï€Î¸ (s) ds
S


Ï€Î¸
Ï€
= Esâˆ¼Ï Î¸ âˆ‡Î¸ Ï€Î¸ (s)âˆ‡a Q (s, a)|a=Ï€Î¸ (s) ,
R Pâˆž
where ÏÏ€Î¸ (s0 ) = S t=1 Î³ tâˆ’1 P (s)P (s â†’ s0 , t, Ï€Î¸ ) ds
is the discounted distribution over states induced by policy Ï€Î¸ and starting from some state s âˆˆ S. Specifically,
P (s â†’ s0 , t, Ï€) is the probability of going t steps under policy Ï€ from state s and ending up in state s0 . The theorems
introduced a class of algorithms (Peters and Schaal 2008;
Degris, White, and Sutton 2012) under the name actor-critic
methods, where the actor is the policy Ï€ and the critic is the
Q-function.
The actor-critic algorithms have also been used in an offpolicy setting through importance sampling (Degris, White,
and Sutton 2012). Recently, another method called a replay
buffer (Mnih et al. 2013) has drawn peopleâ€™s attention for
being able to do off-policy learning with actor-critic algorithms (Lillicrap et al. 2015). In this method, we store all the
samples in a buffer and at every step of learning we sample a
mini-batch from this buffer to estimate the gradient of either
Q-Function or policy.

we can ease the training of multiple agents, and that by keeping a separate policy, the agent can execute with only its
local information, which makes it possible to learn in POCSGs. Among these two algorithms, MADDPG (Lowe et al.
2017) is most relevant to us. It uses the learning rule from
DDPG (Lillicrap et al. 2015) to learn a central off-policy
critic based on Q-learning, and uses the following gradient
estimator to learn the policies for each agent i:


i
i
âˆ‡Î¸i J(Î¸ ) = Es,aâˆ’i âˆ¼D âˆ‡Î¸i Ï€Î¸i (ai |oi )âˆ‡ai Q(s, ~a)|ai =Ï€Î¸i (oi ) ,
i

i

where Î¸ is the agent iâ€™s policy parameters, D is the replay
buffer, and oi is the local observation of agent i. During the
centralized training process, the critic has access to the true
state s = [o1 , . . . , on ]. But at execution time, each agent
only has access to oi .

Multiagent Actor-Critic Algorithms
As we described earlier, if we have limited information for
our agent, we can suffer from the relative overgeneralization
problem. In this section, we demonstrate how this affects the
actor critic algorithm. We will first derive the policy gradient estimator for the cooperative multiagent case and then
discuss several problems that occurs if we naively use this
estimator, which will shed light on the reasons why Multiagent Soft Q-Learning may be useful.
Proposition 1. For any episodic cooperative stochastic
game with n agents, we have the following multiagent
stochastic policy gradient theorem:
Z
Z
Ï€ 1 ,Â·Â·Â· ,Ï€ n
~
âˆ‡Î¸i J(Î¸) = Ï
(s)
âˆ‡Î¸ Ï€(ai |s)
s
Ai
Z
1
n
Ï€ âˆ’i (aâˆ’i |s)QÏ€ ,Â·Â·Â· ,Ï€ (s, ~a) daâˆ’i dai ds
Aâˆ’i

Related Work
The idea of learning to cooperate through policy gradient
methods has been around for a long time, but mainly for
discrete action domains (Banerjee and Peng 2003). Peshkin
et al. have applied the REINFORCE policy gradient to both
CSG and POCSG tasks. However, as we will show later, a
naive use of this gradient estimator is dangerous in the multiagent case. Nair et al. proposed Joint Equilibrium-Based
Search for Policies (JESP), applied to POCSGs. The main
idea here is to perform policy search in one agent while
fixing the policies of other agents. Although this method is
guaranteed to converge to a local Nash Equilibrium, it is essentially a round-robin single agent algorithm.
Recently, with the boom of Deep Reinforcement Learning (DRL), deep MARL algorithms have been proposed to
tackle large scale problems. One of the main streams is
the centralized training with decentralized execution. Foerster et al. proposed a method to learn communication
protocols between the agents. They use inter-agent backpropagation and parameter sharing. Foerster et al. studied
how to stablize the training of multiagent deep reinforcment learning using importance sampling. Two actor-critic
algorithms have been proposed in (Foerster et al. 2017a;
Lowe et al. 2017). They argue that by using a central critic

The proof of this proposition is provided in Proof A at
the end of this paper. From Proposition 1, we can see that
the policy gradient for agent i at each state is scaled by
R
i
1
n
QÏ€ (s, ai ) = Aâˆ’i Ï€ âˆ’i (aâˆ’i |s)QÏ€ Â·Â·Â·Ï€ (s, ~a)daâˆ’i , which
are the joint-action Q-values averaged by the other agentsâ€™
policies. Their are several problems with this estimator.
First, for any agent i, the joint-action Q-function is an onpolicy Q-function. That is, it is learned under policy Ï€ i , and
Ï€ âˆ’i which is not the best response policy of other agents.
Thus, the joint-action Q-function may not scale the gradient
in right magnitude. Second, if we are an independent learner
and play as Ai in game shown in Figure 1, we only have
i
access to QÏ€ (s, ai ), since this value is averaged by otherâ€™s
policies, the value of the action hiN i would be higher than
the value of action hiM i even under the optimal Q-function,
thus mistakenly scaling the gradient to towards hiN i.
MADDPG solves the previous two issues by using the
following methods. First, it uses the replay buffer (Lillicrap et al. 2015) to learn an off-policy optimal Q-function
very much like what we learn for Q-Learning (Silver et al.
2014). This is not doable with traditional importance sampling based off-policy learning. Second, itâ€™s using the centralized training method which gives it direct access to the
joint-action Q-function, but not the policies.

However, MADDPG fails to use the optimal action for
gradient scaling, making it still vulnerable to the relative
overgeneralization problem. To see that, consider its gradient estimator,


âˆ‡Î¸i J(Î¸i ) = Es,aâˆ’i âˆ¼D âˆ‡Î¸i Ï€Î¸i i (ai |oi )âˆ‡ai Qâˆ— (s, ~a)|ai =Ï€Î¸i (oi )
i

We see that for agent i, it tries to ascend the policy gradient
based on Qâˆ— (s, ~a), where aâˆ’i is from the replay buffer D
rather than the optimal policy, which is another way of averaging the Q-values based on othersâ€™ policies. As we showed
in Figure 1, this average-based estimation can lead to relative overgeneralization.

Multiagent Soft Q-Learning
In this paper, we propose MARL method for cooperative
continuous games. We show that on the one hand, our
method is an actor-critic method, which thus can benefit
from the centralized training method, with one central critic
and multiple distributed policies. And on the other hand, our
method resembles Q-Learning, and thus, it efficiently avoids
the relative overgeneralization problem. We first introduce
Soft Q-Learning and then describe how we use it for multiagent training.

Soft Q-Learning
Although Q-Learning has been widely used to deal with control tasks, it has many drawbacks. One of the problems is
that at the early stage of learning, the max operator can
bring bias into the Q-value estimation (Fox, Pakman, and
Tishby 2016). To remedy this, Maximum Entropy Reinforcment Learning (MERL) was introduced, in which tries to
find the following policy:
X
âˆ—
Ï€MaxEnt
= argmaxÏ€
E(st ,at )âˆ¼ÏÏ€ [r(st , at ) + Î±H(Ï€(Â·|s)]
t

where H(Ï€(Â·|s)) is the entropy of the policy. The parameter Î± controls the relative importance of the reward and entropy: when it goes to 0, we recover ordinary RL. From this
objective, a learning method similar to Q-Learning can be
derived, called Soft Q-Learning (Haarnoja et al. 2017). Its
learning algorithm is
Qsoft (st , at ) â† rt + Î³Est+1 [Vsoft (st+1 )] âˆ€st , at ,
Z
1
Vsoft (st ) â† Î± log
exp( Qsoft (st , a0 ))da0 .
Î±
A
Haarnoja et al. have shown that by using this update rule,
âˆ—
respectively,
Qsoft and Vsoft can converge to Qâˆ—soft and Vsoft
and by driving Î± â†’ 0, Q-learning with a hard max operator
can be recovered. For this reason, Haarnoja et al. named this
Soft Q-learning.
Once we have the learned Q-function above, we can get
the optimal max entropy policy as
1
âˆ—
âˆ—
Ï€MaxEnt
(at |st ) = exp( Qâˆ—soft (st , at )âˆ’Vsoft
(st )) âˆ Qâˆ—soft (st , at ).
Î±
A nice property of this policy is that it spreads widely over
the entire action space in continuous control tasks. Thus, if

we have such a policy, and if there are multiple modes in
the action space, we can find them much more effectively
than with more deterministic policies (e.g. Gaussian policy)
which are typically used in actor-critic algorithms. However,
since the form of this policy is so general, sampling from it
is very hard. Soft Q-Learning solves this issue by using Stein
Variational Gradient Descent (SVGD) (Liu and Wang 2016)
to approximate the optimal policy through minimizing the
KL-divergence:



1 âˆ—
âˆ—
DKL = Ï€Î¸ (Â·|st ) exp
Qsoft (st , at ) âˆ’ Vsoft
(st ) , (3)
Î±
where policy Ï€Î¸ (Â·|s) is our approximate policy. Since
âˆ’ Î±1 Qâˆ—soft (st , at ) can be viewed as an energy function, and
the authors are using a deep neural network to approximate
the Q-function, they call this a deep energy-based policy. It
has been demonstrated that using the Soft Q-Learning with
deep energy based policies can learn multimodal objectives.
In Soft Q-Learning we need to learn both the Q-function
and the energy-based policy Ï€(Â·|s). Thus, Soft Q-Learning
can be thought as an actor-critic algorithm. Now consider
the multiagent case. To make it clear, we first recall how we
can achieve coordination in a discrete domain. In discrete
domains, when we have the Qâˆ— (s, a) function, we simply
apply the argmax operator to it and then let each agent do
its own part of the optimal action. This is possible since we
can do global search in the joint-action space for a given
state. Now, with Soft Q-Learning and a deep energy-based
model, we can mimic what we did in the discrete case. In this
situation, we start with a high Î± to do global search in the
joint-action space, then quickly anneal the Î± to lock on some
optimal action, like the argmax operator. It has been shown
that by annealing the Î±, we can get a deterministic policy
from deep energy-based policies (Haarnoja et al. 2017).
Algorithm 1: Multiagent Soft Q-Learning
input: A central critic Q, N policies for all N agents, Î±,
and the epoch start to annealing t
for episode = 0 to M do
Update central critic Q using the method from Soft
Q-Learning.
for agent = 1 to N do
Update the joint policy for agent i using
equation (3)
end
if episode â‰¥ t then
anneal Î±
end
As we described before, Soft Q-Learning is also an actorcritic method. Thus, we can borrow the idea of learning
a centralized joint action critic with Soft Q-Learning from
MADDPG. Then for each of the agents, instead of learning a mapping for its own observation to its own action, we
learn a mapping from its own observation to the joint-action.
When the agent interacts with the environment, it just performs its own part of the joint action. We start the learning
with high Î± value and let it explore the joint action space,

we then quickly anneal the Î± to let each agent find a better
local optima in joint-action space. Our algorithm is given at
Algorithm 1.

Figure 3: The average reward for both algorithms. Multiagent Soft Q-Learning finds the better local optima quickly
after we anneal Î±.

Figure 2: The Max of Two Quadratic game. The dots mark
the two local optima in the joint action space while the star
marks the joint action of the two agents. The contour shows
the reward level.

Experiments
To show that our Multaigent Soft Q-Learning method can
achieve better coordination, we consider the Max of Two
Quadractics game from previous literature (Panait, Luke,
and Wiegand 2006). This is a simple single state continuous game for 2 agents, one action dimension per agent. Each
agent has a bounded action space. The reward for a joint action is given by following equation

a2 âˆ’ y1 2 
a1 âˆ’ x1 2
âˆ’
f1 = h1 Ã— âˆ’
s1
s1

a1 âˆ’ x2 2
a2 âˆ’ y2 2 
f2 = h2 Ã— âˆ’
âˆ’
) +c
s2
s2
r(a1 , a2 ) = max(f1 , f2 )
where a1 , a2 are the actions from agent 1 and agent 2 respectively. In the equation above, h1 = 0.8, h2 = 1, s1 =
3, s2 = 1, x1 = 5, x2 = 5, y1 = âˆ’5, y2 = âˆ’5, c = 10
are the coefficients to determine the reward surface (see Figure 2). Although the formulation of the game is rather simple, it poses a great difficulty to gradient-based algorithms
as, over almost all the joint-action space, the gradient points
to the sub-optimal solution located at (-5, -5).
We trained the MADDPG agent along with our Multagent
Soft Q-Learning agent in this domain. As this was a simple
domain, we used two-hidden-layer networks with size {100,
100}, and we trained the agents for 150 epochs for 100 steps
per epoch. The training was not started until we had 1000
samples in the replay buffer. Both agents scaled their reward
by 0.1. For our Multiagent Soft Q-Learning agent, we started
the annealing at epoch 100, and finished the annealing in 15
epochs. We started with Î± = 1, and annealed it to 0.001.
For the rest of the parameters, we used the default setting

from the original DDPG (Lillicrap et al. 2015) and Soft QLearning (Haarnoja et al. 2017) papers. In addition, to mimic
the local observation setting where centralized learning was
suitable, we gave the two agents in both algorithms different observation signals, where the first agent would always
sense the state as h0i, and the second agent would always
sense it as h1i. Then the state for the central critic was h0, 1i.
The result is in Figure 3. However, the plot is an average over all 50 experiment runs, and hence, may hide some
critical information. On closer investigation, we found that
Multiagent Soft Q-Learning converged to the better equilibrium 72% of the time, while MADDPG never converged
to it.

Conclusion and Future Work
In this paper, we investigated how to achieve better controls
in continuous games. We showed why the traditional policy
gradient methods is not suitable for these tasks, and why the
gradient-based method can fail to find better local optima
in the joint-action space. We then proposed Multiagent Soft
Q-Learning based on the centralized training and decentralized execution paradigm, and showed that, we can achieve
much better coordination with higher probability. And since
we are using centralized training, the co-adaption problem
can be avoided, thus, making our method sample-efficient
compared to independent learners. We argue that Multiagent
Soft Q-Learning is a competitive RL learner for hard coordination problems.
There are some issues that we havenâ€™t been able to investigate thoroughly in this work. First, so far we have only
applied our learner in the single state games. To better understand the algorithm, we would like to try our algorithm on
sequential continuous games with hard coordination problems. Second, as we show in the experiment, with Soft QLearning we are not able to converge the better equilibrium
for 100% of the time. In the future, we would like to investigate different annealing methods to improve the convergence rate. Last, we notice that Multiagent Soft Q-Learning
models the joint action of all the agents, and thus the dimension of the action can explode with more agents. To solve
this issue, we will investigate how to apply Soft Q-Learning
in the independent learner case, where the algorithm scales
well.

Acknowledgments
The research in this paper was conducted with the support of
research infrastructure developed under NSF grant 1727303.
We thank Tuomas Haarnoja and Haoran Tang for their helpful comments on implementing Soft Q-Learning.

P (s â†’ s0 , t, Ï€). Now we iterate the formula,
Z
Z 
âˆ’i âˆ’i
= Ï€ (a |s)
âˆ‡Î¸i Ï€ i (ai |s)Q~Ï€ (s, ~a) + Ï€ i (ai |s)
Aâˆ’i
Ai
Z
Z 
hZ
Î³P (s0 |s, ~a)
âˆ‡Î¸i Ï€ i (ai |s0 )Q~Ï€ (s0 , ~a)
Ï€ âˆ’i (aâˆ’i |s0 )
âˆ’i
i
S
A

Z A
i

i i 0
00 0
+ Ï€ (a |s ) Î³P (s |s , ~a)âˆ‡Î¸i V ~Ï€ (s00 ) ds00 dai daâˆ’i ds0
S

Proof A
We first denote ~Ï€ as the joint-policy. This proof requires
that P (s), P (s0 |s, ~a), Ï€ i (ai |s), âˆ‡Î¸i Ï€ i (ai |s), and Q~Ï€ (s, ~a)
be continuous in all parameters and variables s, s0 , ~a. This
regularity condition implies that V ~Ï€ (s) and âˆ‡Î¸i V ~Ï€ (s) are
continuous functions of Î¸ and s. S is also required to be
compact, and so for any Î¸, ||âˆ‡Î¸i V ~Ï€ (s)|| is a bounded function of s. The proof mainly follows along the standard
Stochastic Policy Gradient Theorem. We assume agent i follows the policy Ï€ i (ai |s) parameterized by Î¸i . We denote Ï€ âˆ’i
as the joint policy of all agents but agent i, and aâˆ’i as the
joint action of all agents except agent i. For notation simplicity, we denote:

dai daâˆ’i
Z
Z
= Ï€ âˆ’i (aâˆ’i |s) âˆ‡Î¸i Ï€ i (ai |s)Q~Ï€ (s, ~a)dai daâˆ’i
Aâˆ’i
Ai
Z
Z
~
Ï€ 0
+ Î³P (s |s, 1) Ï€ âˆ’i (aâˆ’i |s0 )
Aâˆ’i
Z S
i i 0
~
Ï€ 0
âˆ‡Î¸i Ï€ (a |s )Q (s , ~a)dai daâˆ’i ds0
Ai
Z
Z
+ Î³P ~Ï€ (s0 |s, 1) Î³P ~Ï€ (s00 |s0 , 1)âˆ‡Î¸i V ~Ï€ (s00 ) ds00 ds0
S

=

Ï€ âˆ’i (aâˆ’i |s)f (aâˆ’i ) daâˆ’i
Z
Z
= Ï€ 1 (a1 |s) Â· Â· Â·
Ï€ iâˆ’1 (aiâˆ’1 |s)
1
iâˆ’1
A
ZA
Z
i+1 i+1
Ï€ (a |s) Â· Â· Â· Ï€ n (an |s)f (aâˆ’i )
Ai+1
n

An

da Â· Â· Â· da

i+1

iâˆ’1

da

Â· Â· Â· da

1

Using this new notation the proof follows:

S

Î³ t P ~Ï€ (s0 |s, t)

S t=0

Z

Z

Aâˆ’i

Z X
âˆž

=

Ï€ âˆ’i (aâˆ’i |s0 )

Aâˆ’i
Z X
âˆž

Z

âˆ‡Î¸i Ï€ i (ai |s0 )Q~Ï€ (s0 , ~a) dai daâˆ’i ds0

Ai

Î³ t P ~Ï€ (s0 |s, t)

S t=0

Z

âˆ‡Î¸i Ï€ i (ai |s0 )

Z

Ï€ âˆ’i (aâˆ’i |s0 )Q~Ï€ (s0 , ~a) daâˆ’i dai ds0

Aâˆ’i

Ai

In the final line we use Fubiniâ€™s theorem and exchange the
order of integration using the regularity condition so that
||âˆ‡Î¸i V ~Ï€ (s)|| is bounded. We then take the expectation over
the possible start states s:
Z
Z
âˆ‡Î¸i J(Î¸) =âˆ‡Î¸i P (s)V ~Ï€ (s) ds = P (s)âˆ‡Î¸i V ~Ï€ (s) ds
S

âˆ‡Î¸i V ~Ï€ (s)
Z
Z
=âˆ‡Î¸i Ï€ 1 (a1 |s) Â· Â· Â· Ï€ n (an |s) Q~Ï€ (s, ~a) d~a
n
A1
Z
ZA
= Ï€ âˆ’i (aâˆ’i |s)âˆ‡Î¸i Ï€ i (ai |s)Q~Ï€ (s, ~a) dai daâˆ’i
âˆ’i
i
ZA
Z A
= Ï€ âˆ’i (aâˆ’i |s)
âˆ‡Î¸i Ï€ i (ai |s)Q~Ï€ (s, ~a)
Aâˆ’i
Ai

i i
~
Ï€
+ Ï€ (a |s)âˆ‡Î¸i Q (s, ~a) dai daâˆ’i
Z
Z 
âˆ’i âˆ’i
= Ï€ (a |s)
âˆ‡Î¸i Ï€ i (ai |s)Q~Ï€ (s, ~a)
Aâˆ’i
Ai

Z
i i
0
~
Ï€ 0
0
+ Ï€ (a |s) Î³P (s |s, ~a)âˆ‡Î¸i V (s ) ds dai daâˆ’i
S

We used Leibniz integral rule to exchange order of derivative and integration using the regularity condition and expanding Q~Ï€ (s, ~a) above. We use P Ï€ (s0 |s, t) as short for

=

Z Z X
âˆž
S

S

Î³ t P (s)P ~Ï€ (s0 |s, t) ds

Z

âˆ‡Î¸i Ï€ i (ai |s0 )

Ai

S t=0

Z

Ï€ âˆ’i (aâˆ’i |s0 )Q~Ï€ (s0 , ~a) daâˆ’i dai ds0
Z
Z
= Ï~Ï€ (s0 ) âˆ‡Î¸i Ï€ i (ai |s0 )
Ai
ZS
Ï€ âˆ’i (aâˆ’i |s0 )Q~Ï€ (s0 , ~a) daâˆ’i dai ds0
Aâˆ’i

Aâˆ’i

References
Banerjee, B., and Peng, J. 2003. Adaptive policy gradient in
multiagent learning. In Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS â€™03, 686â€“692. New York, NY, USA:
ACM.
Degris, T.; White, M.; and Sutton, R. S. 2012. Linear offpolicy actor-critic. In In International Conference on Machine Learning. Citeseer.

Foerster, J.; Assael, Y. M.; de Freitas, N.; and Whiteson, S.
2016. Learning to communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, 2137â€“2145.
Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and
Whiteson, S. 2017a. Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926.
Foerster, J.; Nardelli, N.; Farquhar, G.; Afouras, T.; Torr, P.
H. S.; Kohli, P.; and Whiteson, S. 2017b. Stabilising experience replay for deep multi-agent reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning, volume 70, 1146â€“1155.
Fox, R.; Pakman, A.; and Tishby, N. 2016. Taming the noise
in reinforcement learning via soft updates. In Proceedings
of the Thirty-Second Conference on Uncertainty in Artificial
Intelligence, UAIâ€™16, 202â€“211.
Greenwald, A.; Hall, K.; and Serrano, R. 2003. Correlated
Q-learning. In AAAI Spring Symposium, volume 3, 242â€“249.
Haarnoja, T.; Tang, H.; Abbeel, P.; and Levine, S. 2017.
Reinforcement learning with deep energy-based policies.
ICML.
Hu, J., and Wellman, M. P. 2003. Nash Q-learning for
general-sum stochastic games. Journal of Machine Learning
Research 4:1039â€“1069.
Jie, T., and Abbeel, P. 2010. On a connection between
importance sampling and the likelihood ratio policy gradient. In Advances in Neural Information Processing Systems,
1000â€“1008.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;
Tassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971.
Littman, M. L. 1994. Markov games as a framework for
multi-agent reinforcement learning. In ICML, volume 94,
157â€“163.
Littman, M. L. 2001. Friend-or-foe Q-learning in generalsum games. In ICML, volume 1, 322â€“328.
Liu, Q., and Wang, D. 2016. Stein variational gradient descent: A general purpose bayesian inference algorithm. In
Advances In Neural Information Processing Systems, 2378â€“
2386.
Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and
Mordatch, I. 2017. Multi-agent actor-critic for mixed
cooperative-competitive environments. Neural Information
Processing Systems (NIPS).
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.
Nair, R.; Tambe, M.; Yokoo, M.; Pynadath, D.; and
Marsella, S. 2003. Taming decentralized pomdps: Towards
efficient policy computation for multiagent settings. In IJCAI, 705â€“711.
Oliehoek, F. A.; Spaan, M. T.; and Vlassis, N. 2008. Optimal and approximate q-value functions for decentralized

pomdps. Journal of Artificial Intelligence Research 32:289â€“
353.
Panait, L.; Luke, S.; and Wiegand, R. P. 2006. Biasing coevolutionary search for optimal multiagent behaviors. IEEE
Transactions on Evolutionary Computation 10(6):629â€“645.
Peshkin, L.; Kim, K.-E.; Meuleau, N.; and Kaelbling, L. P.
2000. Learning to cooperate via policy search. In Proceedings of the Sixteenth conference on Uncertainty in artificial
intelligence, 489â€“496. Morgan Kaufmann Publishers Inc.
Peters, J., and Schaal, S. 2008. Natural actor-critic. Neurocomputing 71(7):1180â€“1190.
Silver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.;
and Riedmiller, M. 2014. Deterministic policy gradient algorithms. In ICML.
Sutton, R. S.; McAllester, D. A.; Singh, S. P.; Mansour, Y.;
et al. 1999. Policy gradient methods for reinforcement learning with function approximation. In NIPS.
Wei, E., and Luke, S. 2016. Lenient learning in independentlearner stochastic cooperative games. Journal of Machine
Learning Research 17(84):1â€“42.
Williams, R. J. 1992. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Machine learning 8(3-4):229â€“256.

