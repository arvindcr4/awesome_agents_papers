Bi-level Mean Field: Dynamic
Grouping for Large-Scale MARL
Yuxuan Zhenga , Yihe Zhoua , Feiyang Xua , Miingli Songa and Shunyu Liub,*

arXiv:2505.06706v2 [cs.AI] 20 May 2025

a Zhejiang University
b Nanyang Technological University

Abstract.
Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as
the exponential growth in agent interactions significantly increases
computational complexity and impedes learning efficiency. To
mitigate this, existing efforts that rely on Mean Field (MF) simplify
the interaction landscape by approximating neighboring agents as
a single mean agent, thus reducing overall complexity to pairwise
interactions. However, these MF methods inevitably fail to account
for individual differences, leading to aggregation noise caused
by inaccurate iterative updates during MF learning. In this paper,
we propose a Bi-level Mean Field (BMF) method to capture agent
diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF
introduces a dynamic group assignment module, which employs
a Variational AutoEncoder (VAE) to learn the representations of
agents, facilitating their dynamic grouping over time. Furthermore,
we propose a bi-level interaction module to model both interand intra-group interactions for effective neighboring aggregation.
Experiments across various tasks demonstrate that the proposed
BMF yields results superior to the state-of-the-art methods.

1

Introduction

Multi-Agent Reinforcement Learning (MARL) involves multiple
autonomous agents operating within a shared environment, which
has been widely applied in domains such as robotics [21, 13], autonomous driving [24, 27] and UAV trajectory design [30, 3]. In
practical real-world scenarios, many tasks involve a large number
of agents, which necessitates the use of large-scale MARL for efficient cooperation. However, unlike traditional MARL, large-scale
settings, especially in complex scenarios, face the curse of dimensionality. This is due to the massive volume of interactions among
agents [32, 18], which increases computational costs and hampers
the learning process.
To remedy this problem, the mean field (MF) method offers a solution by approximating the interactions between agents using an averaging mechanism [26]. Specifically, it treats the collective influence of all other agents as a single virtual mean agent, simplifying
each agent interaction into one between itself and this virtual entity. This significantly reduces the dimensionality of the interaction
space. However, this averaging mechanism disregards individual differences between all agents, and the noise introduced by iterative
approximations can degrade the cooperative performance [33, 23].
Recent advancements in MF methods can be categorized into

two primary ways: (1) The first aims to enhance the accuracy of
the virtual mean agent model, as demonstrated by GAT-MF [7],
GAMFQ [25], and MFRAD [22]. These methods transition from a
simple averaging scheme to a weighted version by leveraging graphbased techniques, which help reduce noise introduced by naive averaging. However, they introduce a significant computational burden,
as they require pairwise computations between all agent pairs. This
challenges the scalability of these methods in large-scale MARL.
(2) The second seeks to represent agent heterogeneity by grouping
agents into multiple types and assigning a virtual mean agent to each
group, as seen in MTMF [6] and NPG [11]. Compared to orignal
MF, they improve accuracy by constructing multiple virtual mean
agents. But these methods rely heavily on prior knowledge to define
agent groups and do not consider the impact of actions from other
groups. This lack of communication between groups leads to information loss during long-term aggregations, limiting adaptability to
dynamic environments and different tasks.
In this paper, we introduce a novel Bi-level Mean Field (BMF)
method for large-scale MARL, which can alleviate interaction aggregation noise while maintaining low computational overhead, making
it adaptable to diverse and dynamic agent-based tasks. The proposed
BMF comprises two key components, namely, dynamic group assignment module and bi-level interaction module. Specifically, the
dynamic group assignment module utilizes a VAE-based extractor to
derive agent representations based on their individual observations
and features. Agents are then dynamically assigned to groups based
on their representations using k-means clustering. Furthermore, the
bi-level interaction module improves the traditional MF method by
incorporating both inter- and intra-group interactions. Within groups,
the classic MF can effectively model agent interactions, given their
similarity. However, across groups, the distinct characteristics of
agents require the introduction of a group attention mechanism to
model inter-group dynamics. The intra-group features are captured
by MF, while the inter-group interactions are modeled using the attention mechanism. By combining these two modules, BMF reduces
the noise caused by iterative aggregation processes and lowers computational overhead. Figure 1 demonstrates the two-phase implementation of BMF, and highlights the differences between normal and
MF methods for large-scale MARL, where the normal method has
a very high feature dimensionality and the MF method suffers from
aggregation noise brought by MF approximation. Our main contributions can be summarized as follows:
(1) We develop BMF, a method that involves a bi-level aggregation
process to Improve classic MF under large-scale MARL. Besides,

we theoretically analyze the validity of BMF.
(2) We propose a dynamic group assignment module using learnable
VAE-based agent representations, allowing for adaptive group assignments over time.
(3) We incorporate a bi-level interaction module, combining intragroup mean-field modeling with inter-group attention to reduce
aggregation noise in MF.
(4) Experimental results across various large-scale MARL tasks show
that BMF offers superior performance while maintaining a moderate computational cost.

Agent

Action
Vector

Mean
Vector

Target
Agent

...

Critic

Large-Scale MARL scenario

...
...
Normal Large-Scale MARL

Mean Field Large-Scale MARL
Critic

Dynamic Group Assignment

...

...
...
Bi-level Mean Field Large-Scale MARL

Critic

Figure 1: Normal, mean field and bi-level mean field methods apply to the large-scale MARL scenario. Limitation of normal largescale MARL is the excessively high dimension of the concatenated
critic input. Limitation of mean field is ignoring agents’ difference
and solely using average aggregation, resulting in a loss of precision. Our proposed Bi-level Mean Field (BMF) effectively reduces
the critic input dimension while ensuring precision.

2

Related work

Multi-agent reinforcement learning. Multi-agent reinforcement
learning (MARL) has gained significant attention, particularly
through the centralized training with decentralized execution
(CTDE) framework [10, 15, 31]. Several methods exemplify the
CTDE framework, including value decomposition (VD) methods
and policy-based methods. VD factorizes the joint Q-function into
components tied to each agent’s local Q-function. VDN [20] adopts
a straightforward additive method for this decomposition, while
QMIX [15] uses a monotonic function to optimize joint action values. Building on these concepts, QTRAN [19] and WQMIX [14]
offer more flexible functional representations, further extending the
capabilities of value decomposition techniques. Policy-based methods, such as MAPPO [28] extends Proximal Policy Optimization
(PPO) to multi-agent settings, achieving a balance between sample
efficiency and stability. MADDPG [10] integrates the Actor-Critic
architecture with centralized training, enabling agents to share information effectively and handle continuous action spaces. Similarly,
COMA [4] introduces a counterfactual baseline to address the credit
assignment problem [15] in cooperative scenarios, improving both
policy learning and optimization. Despite these advances, the scalability of MARL methods remains a significant challenge, particularly in environments with numerous agents. As the number of agents
increases, the complexity of interactions can lead to computational
bottlenecks. In response to this challenge, our proposed method is
specifically designed for large-scale multi-agent systems, ensuring
robust performance even in complex and dynamic environments.
Large-scale MARL. Mean Field (MF) [26] is the most classic
method to reduce the unaffordable high dimension of the centralized training (CT) features and implement MARL on more agents.
The MF models multi-agent interactions as a pairwise interaction
between two agents, where one is the specific agent and the other
is constructed as a mean field virtual agent. Specifically, the virtual agent corresponds to the mean effect of all the neighboring
agents. MF techniques have proven effective in large-scale MARL
applications, such as edge computing [1], game playing [12] and
robotic controlling [16]. However, this approximation overlooks the
differing strengths of interactions among agents, resulting in precision loss when modeling their complex relationships. To address
this limitation, enhancements like MTMF [6] and GAT-MF [7] have
been introduced. MTMF categorizes agents into several types and
perform mean field approximation for each type, but it still suffers
from precision issues and requires prior knowledge for categorization. GAT-MF utilizes parameters to represent agent correlations and
performs a weighted mean field approximation, but the virtual mean
field agent can only be approximated from fixed neighbors, failing
to adapt to dynamic scenarios, especially systems with fluctuating
agent numbers. Our method, on the other hand, designs an innovative
dynamic grouping attention mechanism for effective approximation
while adapting to dynamic scenarios.
Agent grouping in large scale MARL. Agent grouping effectively simplifies large-scale multi-agent problems into smaller, more
manageable ones. There are some direct agent grouping methods that
conform to human intuition, which are widely used in multi-agent
systems to simplify complex problems. MTMF [6] use K-means
to group agents based on their observation and state information.
SOG [17] selects group leaders and clusters agents within their observation range, but the effectiveness of grouping largely hinges on
the reasonable selection of leaders. DHCG [9] models agent relationships as a cooperation graph and groups agents with a threshold.

HCGL [5] and GACG [2] uses deep neural networks to automatically classify agents into predefined groups. These methods only use
agents’ original partial information for grouping, which causes noise
and is incomplete in complex multi-agent environments. Our method
presents an improved classification scheme that leverages agent observations, states, and actions to derive their representations. By encoding agents’ randomness into latent variables, BMF effectively reduces noise and enables a reliable agents’ classification.

4

Method

4.1

Overview

As shown in Figure 1, original MF uses an unweighted mechanism
to collect the average global information, the approximation of the
value function for agent j is considered as:
e j (s, aj , ãj ),
Qj (s, a) ∼ Q

3

ãj =

X
1
ak ,
|N (j)|

Preliminary

3.1

Multi-agent Markov Decision Process

We discuss the large-scale MARL setting as Multi-agent Markov Decision Process (MMDP), defined as a tuple ⟨N , S, p0 , A, P, r, γ⟩,
where N = {i}n
i=1 is the set consists of n agents and s =
(s1 , · · · , sn ) ∈ S is the global state of the environment. The initial state distribution is given by p0 = ∆(S), which is a probability
distribution collection obtained from state space S. The joint action
space a = (a1 , · · · , an ) ∈ A = A1 ×· · ·×AN and each agent’s action ai ∈ Ai is produced by policy πi (ai |s), forming the joint action
a at each time step. The state transition from s to s′ for one step is defined by the state transition function P (s′ |s, a) : S ×A×S → [0, 1]
and will receive a reward via reward function r(s, a) : S × A → R.
The long-term reward from t0 to t is defined as:
R=

T
X

γ

t−t0

rt ,

where N (j) is the neighboring agents of agent j and the action set a
in value function Qj is approximate by a two actions unit (aj , ak ) .
This method reduces the dimension of a by averaging the neighbor
information. However, this unweighted approximation fails to consider that interactions between agents differ across various pairs and
can change over time. Therefore, we propose a bi-level mean field
with a dynamic group mechanism to refine the types of agents and
effectively model agent groups’ interactions. The value function is as
follow when considering agent j in group m ∈ G, where G is the set
of groups and G(m) means the collection of groups in G excluding
m:
b j (s, aj , ãj , ãm ),
Qj (s, a) ∼ Q
ãm =

(1)

t=t0

where γ ∈ [0, 1] is the discount factor and T is the maximum count
of steps in one episode.

3.2

1
Wm

X

ãj =

wmn ãn ,

1
|Nm (j)|

Wm =

n∈G(m)

X

ak ,

k∈Nm (j)

X

(6)

wmn .

n∈G(m)

Here Nm (j) is the neighboring agent of agent j in group m, and ãj
is calculated as intra-group mean field action, while ãm is a weighted
aggregation based on the inter-group mean field results.

Q Learning

One of the basic methods is Deep Q Learning, which attempts to
find an efficient policy by maximizing the value function Q(s, a).
Q(s, a) = E[rt+1 + γrt+2 + γ 2 rt+3 + · · · |st = s, at = a]. For
one step transition from state-action unit (s, a) to (s′ , a′ ), This value
function is modeled by minimizing the following loss function:
L = E(s,a,r,s′ ) [(Q(s, a) − y)2 ],
y = r + γ max
Q′(s′ ,a′ ) ,
′

(2)

a

where Q′ is synchronized from Q with a time step delay.

3.3

(5)

k∈N (j)

Policy Gradient

Policy Gradient addresses continuous action space problems, with
Actor-Critic being a classic method. It involves two stages: the policy
network outputs an action, and the value network evaluates a Q-value
for the state-action pair (s, a). The value network is trained similarly
to Q Learning:
L = E(s,a,r,s′ ) [(Q(s, a) − y)2 ],

y = r + γQ′ (s′ , a′ ),

(3)

and the policy network is optimized to maximize the value, following
the gradient:
∇J = Ea∼π [∇ log π(a|s)Q(s, a)],

(4)

where π is the policy network. The actions are decided by the policy
network, which is well modeled through the Actor-Critic algorithm.

4.2

Dynamic Group Assignment

The dynamic group assignment mechanism effectively aggregates
similar types of agents, establishing the foundational framework
for our bi-level mean field method and reducing the interaction
costs among agents. Considering the high randomness in large-scale
MARL, the observations and actions of each agent exhibit differences over time, leading to frequent changes in agent types and corresponding adjustments in agent group assignment. Therefore, our
method dynamically evaluates agent types and performs grouping
at certain time intervals, allowing the group assignment mechanism
could adapt to complex large-scale MARL scenarios without requiring prior knowledge to specify agent types.
Our dynamic group assignment extracts representations of agents
by inputting their states, observations, and actions information. In
order to make sure agents of the same type are focused on actions
that own similar effects, we design a mechanism for extracting preference representations that emphasize actions, expecting these representations to be used to derive the reward with the given current state.
Additionally, since agents executing the same action are distributed
sparsely in large-scale MARL scenarios, we use a variational autoencoder (VAE) to extract the agents’ representations. The VAE encodes
the randomness of the agent distribution into the representation, serving to filter out noise.
As shown in Figure 2, we employed a mixed model of prediction
and regression to learn the agent encoder. For a given agent j, we
can obtain the representation zj and design a reconstruct loss. We
also combine zj and sj to predict the value of the next state V (s′j ),

...
...

...
Reconstructed
Distribution

State

Action

Intra-group
Mean Field

Inter-group
Mean Field

Group
Assignment

Representation

Decode

...

Attention
Head

...

sample

Dot

...

...

Bi-level
Mean Field

Encode

intra-group infomation

Inter-group
Mean Field

...

Probability
Distribution

Agent 1

Intra-group
Mean Field

Replay
Buffer

Agent 2

...

...

...

Intra-group
Mean Field

Agent

...

Group
assignment

Agent n

...

Agent

Figure 2: The Bi-level Mean Field (BMF) framework. The group assignment forward model for learning agent representations is based on

a VAE architecture. The intra-group MF considers the influence of same-type agents and approximates the average effect for a group. The
inter-group MF focuses on the differences between different groups and calculates the total Q value.
designing a predict loss. Combining prediction and regression, the
loss function is designed as:

Lq (θj ) = (Qθj (sj , aj , ãj , ãm ) − yj )2 ,

n
h X
(d(z i ) − (si , ai ))2
Le (θe , θd , θp ) = E(s,a,r,s′ )∼D λp
i=1
n
i
X
+ λe
(p(z i , si ) + ri − γs′ )2 ,

(7)

i=1

In this paper, the encoder is trained as a forward model, and we simply use k-means clustering based on Euclidean distances between
agents’ representations, but it can be easily extended to other clustering methods.

4.3

address different types of reinforcement learning tasks. For agent j
in group m, BMF-Q is trained by minimizing the loss function:

Bi-level Mean Field

The bilevel mean field method, based on dynamic group assignment, has designed an interaction framework for agents that incorporates both inter-group and intra-group interactions. This framework
not only enhances overall performance but also effectively reduces
computational overhead in large-scale MARL. By optimizing collaboration and competition mechanisms among agents, this method
achieves more efficient resource utilization and decision-making,
demonstrating excellent performance in complex environments.
Intra-group interaction employs an unweighted mean field
method, treating agents within the same group as homogeneous (assuming similar behaviors among group members). This retains the
advantage of low computational overhead associated with mean field
calculations. On the other hand, inter-group interaction utilizes a
separate unweighted mean field method, treating agents from different groups as heterogeneous. It incorporates an attention mechanism
to account for the differences in interactions among heterogeneous
agents, effectively modeling their interactions.
Our method includes two forms tailored for discrete and continuous actions, specifically BMF-Q and BMF-AC. BMF-Q follows
the optimization method of Q-learning, suitable for decision-making
problems in discrete action spaces, while BMF-AC is based on the
Deep Deterministic Policy Gradient (DDPG), suitable for optimizing
continuous action spaces. This design allows our method to flexibly

(8)

where yj = γ · Vθj (s′j ) + rj , γ ∈ (0, 1] is the target value of state
s based on Vθj (s′j ). And BMF-AC consists of a value network and a
policy network. The value network follows the same setting of BMFQ and the policy network is trained by the policy gradient:
∇ϕj J (ϕj ) = Eaj ∼πϕj (s) [∇ϕj log πϕj (s)Qθj (s, aj , ãj , ãm )],
(9)
where πϕj is the policy explicitly modeled by neural networks with
the weights ϕ. BMF pseudocode is provided in Algorithm 1.

4.4

Theoretical Analysis

We prove the validity of the BMF method under mean field conditions and discuss the error bounds brought by the BMF method.
We find that the error of BMF is bounded by a interval [−2K, 2K],
under the condition that the Q-function is K-smooth. Due to space
limitation, we provide the proof validating the BMF here, while the
detailed proof of the error bound is provided in the supplementary
material.
Assumption 1. The global Q-function is equivalent to the sum of
local Q-functions:
X
Q(s, a) =
Qj (s, a),
(10)
j

Assumption 2. For agent j in group m, the local Q-function can be
factorized by a set of Q-functions that capture pairwise interactions:
X
1
e j (s, aj , ak )
Qj (s, a) =
Q
|Nm (j)|
k∈Nm (j)

1
+
Wm

X

e j (s, aj , ak′ )
wmn Q

(11)

n∈G(m)
k′ ∈Nn (j)

where Nm (j) is the set of neighbors of agent j and n is a group
different from m.

Algorithm 1 Large scale MARL with Bi-level MF
Require: Number of agents na , max episode length ne , MARL
model update interval Iu , group assignment interval Ig , forward
VAE model Mθ , discount factor γ.
Ensure: The trained MARL model.
θ, ϕ ∼ initial parameters for evaluation network.
D ← empty replay buffer.
for each episode do
for t = 1 to ne do
if reach group assignment interval Ig then
agent representations Ra ← Mθ (st−1 , at−1 ).
G ← Kmeans(Ra ).
end if
for each group ∈ G do
Calculate
P each group’s representation action: ãn =
1
k∈Gn ak .
|Gn |
end for
for agent j = 1 to n do
For agent j in group m: get intra-group neighboring agents
collection Nm (j) and inter-group neighboring agents collection Nn (j).
CalculateP the intra-group mean field action: ãj =
1
k∈Nm (j) ak .
|Nm (j)|
Calculate
the inter-group mean field action: ãm =
P
1
n∈G(m) wmn ãn .
Wm
end for
Organize intra-group mean field actions as ãintra and intergroup mean field actions as ãinter .
Calculate and execute the joint action at = (a1 , ..., an ) ∈ A
by fϕ : at ← fϕ (st ), get the next state s′t = (s′1 , ..., s′n ) ∈ S
and the reward r t = (r1 , ..., rn ) ∈ Rn .
Store (s, a, r, s′ , ãintra , ãinter ) into D.
if reach model update interval Iu then
Sample an experience (s, a, r, s′ , ãintra , ãinter ) from
D.
Update θ according to critic loss: Lq (θj )
=
(Qθj (sj , aj , ãj , ãm ) − yj )2 .
Update ϕ according to actor loss: ∇ϕj J (ϕj ) =
Eaj ∼πϕj (s) [∇ϕj log πϕj (s)Qθj (s, aj , ãj , ãm )].
end if
end for
end for
Theorem 1 (BMF approximation). When considering agent j in
group m, the global Q-function can be represented as:
X
b j (s, aj , ãj , ãm ),
Qj (s, a) ∼
Q
(12)
m

and inter-group. Considering the intra-group mean field action ãj ,
since
ãj =

X

ak ,

(13)

k∈Nm (j)

in group m, we can regard each agent ak , k ∈ Nm (j) has a action
represented by ãj with a fluctuation:
ak = ãj + δajk ,

(14)

and we can get a formula of δajk :
1
|Nm (j)|

X

1
|Nm (j)|

δajk =

k∈Nm (j)

1
|Nm (j)|

=

X

(ak − ãj )

k∈Nm (j)

X

ak − ãj

(15)

k∈Nm (j)

= ãj − ãj = 0,
Similarly, considering the inter-group mean field action ãm , we have
1
Wm

ãn = ãm + δamn ,

X

wmn δamn = 0,

(16)

n∈G(m)

and the isomorphic properties of agents within the same group determine that the agent’s action differs from the mean field action of its
group by a tiny deviation. We have
aj ∼ ãm ,

agent j ∈ group m,

∼ ãn ,

agent k′ ∈ group n,

a

k′

(17)

(m, n ∈ G, m ̸= n)
when considering two agents j, k, which belong to different groups
and have a noticeable difference. The total value function of agent j
can be expressed as

Qj (s, a) =

where ãj is intra-group MF action and ãm is the inter-group MF
action.
Proof. Here we prove the feasibility of using BMF approximation
for the global Q-function.
As shown in Assumption 1, the global Q-function is equivalent
to the sum P
of local Q-functions under MF setting, and we have
Q(s, a) =
j Qj (s, a). Therefore, to demonstrate that Theorem
1 is valid, we need to prove the local Q-function Qj (s, a) can be
e j (s, aj , ãj , ãm ).
approximated as Q
When considering agent j in group m, the value function can be
e j (s, aj , ãj , ãm ). Here we have two
approximated as Qj (s, a) ∼ Q
types of mean field actions, ãj and ãm , respectively for intra-group

1
|Nm (j)|

1
|Nm (j)|
+

=

1
Wm

e j (s, aj , ak )
Q

k∈Nm (j)

X

e j (s, aj , ak′ )
wmn Q

n∈G(m)
k′ ∈Nn (j)

1
|Nm (j)|
1
+
Wm

X

(18)

X

e j (s, aj , ãj + δajk )
Q

k∈Nm (j)

X

e j (s, aj , ãm + δamn ).
wmn Q

n∈G(m)
k′ ∈Nn (j)

e j (s, aj , ãj ) as Q0 , Q
e j (s, aj , ãm ) as Q1 , we can obtain
Denoting Q

BMF(
Our
s
)

MFQ

GATMF

(a) Firefighter

AC

(b) Adversarial Pursuit

MFAC

Q

(c) Battle

Figure 3: Compare BMF with other methods in various large-scale MARL environments. All experimental results are illustrated with the mean

and the standard deviation of the metrics over 4 random seeds for a fair comparison.
a derivation using Taylor’s formula:
i
X h
1
(18) =
Q0 + ∇ãj Q0 · δajk + ojk
|Nm (j)|
k∈Nm (j)
i
h
X
1
+
wmn Q1 + ∇ãm Q1 · δamn + omn
Wm

5.2

n∈G(m)

= Q0 + ∇ãj Q0 ·

1
|Nm (j)|

+ Q1 + ∇ãm Q1 ·
+

1
|Nm (j)|

1
Wm

X

X

X

k∈Nm (j)

wmn δamn

(19)

n∈G(m)

ojk +

k∈Nm (j)

= Q0 + 0 + Q1 + 0
X
1
+
|Nm (j)|

δajk

k∈Nm (j)

ojk +

1
Wm

1
Wm

X

wmn omn

n∈G(m)

X

wmn omn

n∈G(m)

e j (s, aj , ãj ) + Q
e j (s, aj , ãm ),
≈ Q0 + Q1 = Q
where oj k and om n denote the Taylor polynomial’s remainder. Since
the agents within and between groups do not overlap, the intra-group
and inter-group mean field actions can be combined for considerab j (s, aj , ãj , ãm ) = Q
e j (s, aj , ãj ) + Q
e j (s, aj , ãm ).
tion, forming Q
b j (s, aj , ãj , ãm ) accroding to forTherefore, we prove Qj (s, a) ∼ Q
mula (18) and (19).

5

Experiments

5.1

Settings

Our BMF method is tested in three multi-agent experimental environments: Firefighter, Adversarial Pursuit, and Battle, following
previous works [8, 29]. Specifically, Firefighter focuses on a fully
cooperative task, while the other two focus on mixed cooperativecompetitive tasks. The detailed introduction and visualization of the
three environments are provided in the supplementary material. We
select the four algorithms tested in the original mean field methods
(MFAC, MFQ, AC, Q) as the baseline, and our BMF is also compared with the state-of-the-art method GAT-MF. Unlike these methods, BMF sets the number of clusters as a hyperparameter and conducts robustness experiments on this hyperparameter. To further explain the learned collaboration and competition strategies from BMF,
we implement a qualitative visualization analysis in Battle environment. The detailed results and analysis is provided in the supplementary material.

Performance

We first compare the performance of our BMF and other baselines
under three environments in cooperative and competitive tasks. Figure 3 presents the return curves of comparison methods.
The Firefighter environment is set up with 50 firefighter agents
and 50 burning houses, where each agent has a fixed position. Each
house initially has a value of −3 representing the current fire intensity and decreases by −1 at each time step. The total sum of the fire
conditions of all houses serves as the final return value. Our BMF
method significantly outperforms the MF method and slightly surpasses GAT-MF, which performs excellently in convergence speed
and the value obtained.
The Adversarial Pursuit environment is set up with 25 pursuit
agents and 50 targets. Whenever a pursuit agent successfully tags a
target, it receives a reward of 1. Regardless of whether the tagging
is successful or not, there is a penalty of −0.2 for each tagging attempt. The total value obtained from successfully tagging all targets
is considered the final value. Our BMF method is adapted to this dynamically changing environment compared to GAT-MF, and it has
achieved better final values compared to other methods.
The Battle environment is set up as a combat environment with
128 agents. The agents are evenly divided into two teams that compete against each other. Agents receive small rewards for specific
actions, and killing an enemy grants the team a value of 5. The overall value obtained by the team is considered as the final value. Our
BMF method performs better than other methods in this large-scale
cooperative competition environment with dynamic changes.
The results show that BMF can effectively adapt to dynamic and
variable large-scale MARL environments, yielding favorable outcomes. In addition to evaluating the return value, we also tested the
success rate of BMF and the effectiveness of killing enemies in the
Battle environment, where agents on the same team must collaborate
to compete against opposing agents. The results are provided in the
supplementary material.

5.3

Computational Efficiency

We compared the time and space costs of BMF with the current
state-of-the-art method, GAT-MF. This computational efficiency experiment is conducted in the Firefighter environment and used an
NVIDIA RTX A6000 GPU. The results are presented in Table 1,
which show that BMF reduces the time cost by 31.3% and the space
cost by 15.9%.

k=1
k=2

Table 1: The computational efficiency results with 3 random seeds.
method

Time(s)

Space(MiB)

GAT-MF
BMF(ours)

27162±1421.8
18655±715.3

2116±0.0
1780±0.0

400

k=4
k=8

k = 16
k = 64

350
300

Zero-Shot Generalization

return

5.4

In this section, we conducted experiments on the battle task with
variable agent scales, where the number of agents starts from 128
and increases. The rewards are provided in the table below, which
demonstrates that BMF has zero-shot generalization ability across
different scales.

250
200
150
100
50
0

Battle

(a) Number of clusters ablation

Table 2: Zero-shot generalization results with 3 random seeds.
128
288
512

5.5

Q

MFQ

AC

MFAC

BMFVAE

BMF(ours)

186.98 ± 66.44 174.21 ± 71.42 306.19 ± 23.09
318.20 ± 29.01
396.76 ± 14.57
137.54 ± 306.07 91.91 ± 108.44 565.83 ± 181.58 550.96 ± 68.01
613.19 ± 169.70
428.37 ± 542.01 176.35 ± 218.24 963.75 ± 373.27 1018.89 ± 144.66 1188.86 ± 348.37

return

Num

Ablation study

To analyze the impact of different frameworks and different number
of clusters in group assignment, we design ablation experiments to
quantitatively validate these effectiveness under the Battle environment. The comparison results of the ablation experiments are presented in Figure 4.
The “delays in agent interactions" issue. The original MFRL
uses the action from the previous step to calculate the mean field action, resulting in a time delay. BMF follows the MFRL framework
and uses historical mean actions to compute current actions. We implement a no-delay version BMF (follows MTMF’s delay handling)
and conduct experiments on the battle task with three random seeds.
The final rewards are 375.03±25.44 and 370.81±35.96, which perform a difference of 1.1%. Therefore, one-step delay causes slight
impact.
The robustness of sub-groups. We have additionally conducted
experiments to prove that the impact brought by the number of clusters has a certain threshold. When the value of k(the number of subgroups) rises above this threshold, the variations in k have little impact on the results, demonstrating robustness. We have conducted
ablation, and Figure 4a shows the performance in the Battle environment under different values of k. Appropriate predefined number
of sub-groups is selected for each task.
BMF w/o group assignment module. To verify the effectiveness
of the dynamic group assignment module, we designed BMF-RC,
which represents a version of BMF without the dynamic group assignment module. Results in Figure 4b show that BMF-RC is inferior to our VAE-based BMF in terms of convergence speed and final
value.
BMF with AE-based representation. To validate the effectiveness of VAE as the framework for the dynamic group assignment
module, we designed BMF-AE, which represents BMF with the AE
framework replacing the VAE framework. Results in Figure 4b indicate that AE does not have a positive impact on BMF, due to the AE
framework’s inability to handle the noise introduced by the agents’
randomness. Consequently, AE-based BMF is inferior to our VAEbased BMF in terms of convergence speed and final value.
BMF with VAE-based representation. Our BMF method employs a VAE-based dynamic group assignment module. BMF-VAE
represents our method, which demonstrated better performance according to ablation results in Figure 4b.

400
200
0
200
400
600
800
1000

0

BMFAE

BMFRC

250 500 750 1000 1250 1500 1750 2000

episode

(b) Group assignment ablation
Figure 4: Ablations about group assignment and the number of clus-

ters under the Battle environment.

5.6

Visualization.
k=1
k=2

k=4
k=8

k = 16
k = 64

To further explain the learned collaboration and competition strategies from BMF, we implement a qualitative visualization analysis in
Battle environment as shown in Figure 5. From the behavior slices, it
can be observed that the agents exhibit cooperative behaviors such as
coordinated attacks and collaborative pursuits. Additionally, agents
from different teams engage in competitive behaviors against each
other. Our BMF method enables effective cooperation among multiple agents in the same group to chase, while agents in different
groups will engage in intense operations such as kite and attack operations with opponents.

Step 24/400

Attacking

Step 56/400

...
Chasing

Figure 5: Visualization of agents collaboration and competition under

the Battle scenario.

6

Conclusion

In this paper, we explore the large-scale MARL problems and propose the Bi-level Mean Field (BMF) method. Unlike traditional

methods, BMF dynamically groups agents based on their extracted
hidden features, allowing for a deeper understanding of the relationships between agents. In BMF, we introduce intra-group and intergroup MF. Experiments demonstrate that BMF exhibits strong adaptability across various dynamic large-scale multi-agent environments,
outperforming existing methods. In future work, we will explore the
combination of adaptive grouping mechanisms with BMF and extend
BMF to more practical applications.

References
[1] A. Abouaomar, S. Cherkaoui, Z. Mlika, and A. Kobbane. Mean-field
game and reinforcement learning mec resource provisioning for sfc. In
2021 IEEE Global Communications Conference (GLOBECOM), pages
1–6, 2021.
[2] W. Duan, J. Lu, and J. Xuan. Group-aware coordination graph for multiagent reinforcement learning. arXiv preprint arXiv:2404.10976, 2024.
[3] C. Fan, H. Xu, and Q. Wang. Multi-agent deep reinforcement learning for trajectory planning in uavs-assisted mobile edge computing with
heterogeneous requirements. Computer Networks, 248:110469, 2024.
[4] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI
Conference on Artificial Intelligence, 2018.
[5] Q. Fu, T. Qiu, J. Yi, Z. Pu, and X. Ai. Self-clustering hierarchical multiagent reinforcement learning with extensible cooperation graph. arXiv
preprint arXiv:2403.18056, 2024.
[6] S. Ganapathi Subramanian, P. Poupart, M. E. Taylor, and N. Hegde.
Multi type mean field reinforcement learning. In Proceedings of the
International Joint Conference on Autonomous Agents and Multiagent
Systems, pages 411–419, 2020.
[7] Q. Hao, W. Huang, T. Feng, J. Yuan, and Y. Li. Gat-mf: Graph attention
mean field for very large scale multi-agent reinforcement learning. In
Proceedings of the ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 685–697, 2023.
[8] S. Jiang and C. Amato. Multi-agent reinforcement learning with directed exploration and selective memory reuse. In Proceedings of the
36th Annual ACM Symposium on Applied Computing, pages 777–784,
2021.
[9] Z. Liu, L. Wan, X. Sui, Z. Chen, K. Sun, and X. Lan. Deep hierarchical communication graph in multi-agent reinforcement learning. In
Proceedings of the International Joint Conference on Artificial Intelligence, pages 208–216, 2023.
[10] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing Systems, 2017.
[11] W. U. Mondal, M. Agarwal, V. Aggarwal, and S. V. Ukkusuri. On the
approximation of cooperative heterogeneous multi-agent reinforcement
learning (marl) using mean field control (mfc). Journal of Machine
Learning Research, pages 1–46, 2022.
[12] S. Perrin, M. Laurière, J. Pérolat, M. Geist, R. Élie, and O. Pietquin.
Mean field games flock! the reinforcement learning way. arXiv preprint
arXiv:2105.07933, 2021.
[13] L. Piardi, V. C. Kalempa, M. Limeira, A. S. de Oliveira, and P. Leitão.
Arena—augmented reality to enhanced experimentation in smart warehouses. Sensors, 19(19):4308, 2019.
[14] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson. Weighted QMIX:
expanding monotonic value function factorisation for deep multi-agent
reinforcement learning. In Annual Conference on Neural Information
Processing Systems, 2020.
[15] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
S. Whiteson. Monotonic value function factorisation for deep multiagent reinforcement learning. Journal of Machine Learning Research,
pages 1–51, 2020.
[16] T. Said, J. Wolbert, S. Khodadadeh, A. Dutta, O. P. Kreidl, L. Bölöni,
and S. Roy. Multi-robot information sampling using deep mean field
reinforcement learning. In 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 1215–1220, 2021.
[17] J. Shao, Z. Lou, H. Zhang, Y. Jiang, S. He, and X. Ji. Self-organized
group for cooperative multi-agent reinforcement learning. Advances in
Neural Information Processing Systems, pages 5711–5723, 2022.
[18] Y. Shike, L. Jingchen, and S. Haobin. Mix-attention approximation for
homogeneous large-scale multi-agent reinforcement learning. Neural
Computing and Applications, pages 3143–3154, 2023.
[19] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent rein-

forcement learning. In Proceedings of the International Conference on
Machine Learning, pages 5887–5896, 2019.
[20] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al.
Value-decomposition networks for cooperative multi-agent learning.
arXiv preprint arXiv:1706.05296, 2017.
[21] I. Viksnin, S. Chuprov, M. Usova, and D. Zakoldaev. Police office
model for multi-agent robotic systems. In IOP Conference Series: Materials Science and Engineering, page 012036, 2019.
[22] T. Wu, W. Li, B. Jin, W. Zhang, and X. Wang. Weighted mean-field
multi-agent reinforcement learning via reward attribution decomposition. In International Conference on Database Systems for Advanced
Applications, pages 301–316, 2022.
[23] J. Xu, J. Chen, S. You, Z. Xiao, Y. Yang, and J. Lu. Robustness of deep
learning models on graphs: A survey. AI Open, pages 69–78, 2021.
[24] Z. Yan, H. Zheng, and C. Wu. Multi-agent path finding for cooperative
autonomous driving. In Proceedings of the IEEE International Conference on Robotics and Automation, pages 12361–12367, 2024.
[25] M. Yang, G. Liu, Z. Zhou, and J. Wang. Partially observable mean field
multi-agent reinforcement learning based on graph attention network
for uav swarms. Drones, page 476, 2023.
[26] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang. Mean field
multi-agent reinforcement learning. In Proceedings of the International
Conference on Machine Learning, pages 5571–5580, 2018.
[27] J.-C. Yeh and V.-W. Soo. Toward socially friendly autonomous driving
using multi-agent deep reinforcement learning. In Proceedings of the
International Joint Conference on Autonomous Agents and Multiagent
Systems, pages 2573–2575, 2024.
[28] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The
surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, pages 24611–24624,
2022.
[29] L. Zheng, J. Yang, H. Cai, M. Zhou, W. Zhang, J. Wang, and Y. Yu.
Magent: A many-agent reinforcement learning platform for artificial
collective intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.
[30] X. Zhou, J. Xiong, H. Zhao, X. Liu, B. Ren, X. Zhang, J. Wei, and
H. Yin. Joint uav trajectory and communication design with heterogeneous multi-agent reinforcement learning. Science China Information
Sciences, 67(3):132302, 2024.
[31] Y. Zhou, S. Liu, Y. Qing, K. Chen, T. Zheng, Y. Huang, J. Song, and
M. Song. Is centralized training with decentralized execution framework centralized enough for marl? arXiv preprint arXiv:2305.17352,
2023.
[32] Z. Zhou, L. Qian, and H. Xu. Decentralized multi-agent reinforcement
learning for large-scale mobile wireless sensor network control using
mean field games. In International Conference on Computer Communications and Networks, pages 1–6, 2024.
[33] Y. Zhu, W. Xu, J. Zhang, Y. Du, J. Zhang, Q. Liu, C. Yang, and S. Wu.
A survey on graph structure learning: Progress and opportunities. arXiv
preprint arXiv:2103.03036, 2021.

