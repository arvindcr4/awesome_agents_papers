Highlights
A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges
and Perspectives

arXiv:2503.13415v1 [cs.MA] 17 Mar 2025

Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang

• Provides a comprehensive survey of multi-agent
decision-making methods.

• Analyzes key simulation environments for multiagent reinforcement learning.

• Investigate decision-making approaches, including
MARL and large language models.

• Identifies challenges and future research directions
in multi-agent collaboration.

• Reviews real-world applications in transportation,
aerial systems, and automation.

A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:
Scenarios, Approaches, Challenges and Perspectives
Weiqiang Jina , Hongyang Dub , Biao Zhaoa,∗, Xingwu Tiana , Bohang Shia , Guang Yangc,d,e,f,∗
a School of Information and Communications Engineering, Xi‘an Jiaotong University, Innovation Harbour, Xi‘an, 710049, Shaanxi, China
b Department of Electrical and Electronic Engineering, The University of Hong Kong (HKU), Hong Kong, Hong Kong, China
c Bioengineering Department and Imperial-X, Imperial College London, London, W12 7SL, UK
d National Heart and Lung Institute, Imperial College London, London, SW7 2AZ, UK
e Cardiovascular Research Centre, Royal Brompton Hospital, London, SW3 6NP, UK
f School of Biomedical Engineering & Imaging Sciences, King’s College London, London, WC2R 2LS, UK

Abstract
With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed
human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios.
Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks
and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous
driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making.
Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including
task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent
systems (MAS). These approaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game
theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models (LLMs) reasoning-based. Given the significant advantages of MARL and LLMs-based decision-making
methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent
methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in
the future and potential challenges of multi-agent cooperative decision-making are also detailed.
Keywords:
Intelligent decision-making, Multi-agent systems, Multi-agent cooperative environments, Multi-agent reinforcement
learning, Large language models.

1. Introduction
1.1. Research Backgrounds of Multi-Agent DecisionMaking
With the continuous advancement of science and
technology, intelligent decision-making technology has

∗ Corresponding authors: Biao Zhao and Guang Yang.

Email addresses: weiqiangjin@stu.xjtu.edu.cn (Weiqiang
Jin), duhy@hku.hk (Hongyang Du), biaozhao@xjtu.edu.cn
(Biao Zhao), txw_xjtu@163.com (Xingwu Tian),
Bh_567@stu.xjtu.edu.cn (Bohang Shi),
g.yang@imperial.ac.uk (Guang Yang)
Preprint submitted to Information fusion

made rapid progress. These technologies have gradually surpassed human capabilities in various humanmachine game competitions, even exceeding the top human levels. Over the past few decades, especially following the successful application of Deep Q-Networks
(DQN) [1, 2] in the Arita game and the victories of AlphaGo and AlphaZero [3, 4] over top human opponents,
these landmark achievements have significantly propelled the advancement of intelligent decision-making
research.
To meet the growing complexity of real-world applications and the increasing demand for more sophisticated, reliable, and efficient intelligent systems, multiMarch 18, 2025

DeepMind AlphaGo Zero

DQN on Atari games

SMAC - StarCraft MultiAgent Challenge

Agar-like CooperativeCompetitive Game

Single-Agent → Multi-Agents

Multi-Agent Particle Environment

Since 2013 (DQN)

2015 (AlphaGo Zero)

, … … MPE (2017)

SMAC (2019)

,……,

Until Now (2024)

Figure 1: An overview of the evolution of scenarios and methods in decision-making from single-agent to multi-agent systems.

been a marked increase in systematic literature reviews
in this domain [24, 6, 8, 25]. These reviews have covered a wide range of topics, from theoretical innovations to practical applications, providing a comprehensive overview of the state-of-the-art.
Ning et al. [25] provided a comprehensive overview
of the evolution, challenges, and applications of multiagent reinforcement learning (MARL)-based intelligent
agents, including its practical implementation aspects.
Gronauer et al. [6] provided an overview of recent
developments in multi-agent deep reinforcement learning, focusing on training schemes, emergent agent behaviors, and the unique challenges of the multi-agent
domain, while also discussing future research directions. Yang et al. [26] explored the utility theory application in AI robotics, focusing on how utility AI
models can guide decision-making and cooperation in
multi-agent/robot systems. Orr et al. [8] reviewed
recent advancements in MARL, particularly its applications in multi-robot systems, while discussing current challenges and potential future applications. Du
et al. [24] provided a systematic overview of multiagent deep reinforcement learning for MAS, focusing
on its challenges, methodologies, and applications. Pamul et al. [7] provided a comprehensive analysis of
the application of MARL in connected and automated
vehicles (CAVs), identifying current developments, existing research directions, and challenges. HernandezLeal et al. [27] provided a comprehensive overview
of approaches to addressing opponent-induced non-

agent cooperative decision-making has rapidly evolved
from simple single-agent scenarios [5, 6, 7, 8]. Multiagent cooperative decision-making is a crucial subfield
within machine learning (ML) [9] and artificial intelligence (AI) [10]. It involves multiple interacting agents
working together to complete established tasks across
diverse well-designed dynamic simulated environments
and various complex real-world systems.
As depicted in Figure 1, the evolution research
progress from single-agent to multi-agent decisionmaking systems, along with methodological comparisons, highlights that this rapidly advancing field is
a crucial step toward achieving human-level AI and
the Artificial General Intelligence (AGI) age. Multiagent cooperative decision-making has a wide range of
practical applications and many fundamental theoretical
works across various domains. The service scenarios
are extensive, encompassing smart agriculture management [11, 12], intelligent collaborative robots [13, 14,
15, 16], self-driving collaborative obstacle avoidance
[17, 18, 19], autonomous navigation [20, 21, 22] as well
as joint rescue tasks [12, 23]. Correspondingly, considering the rapid pace of technological advancement and
the multifaceted needs of the real world, in this work,
we focus on the comprehensive study of multi-agent cooperative decision-making.
1.2. Overview of Previous Multi-Agent Surveys
Concurrent with the fast-paced advancements in
multi-agent cooperative decision-making, there has
2

stationarity in multi-agent learning, categorizing algorithms into a new framework and offering insights into
their effectiveness across different environments. The
survey by Zhu et al. [28] provided a systematic classification and analysis of MARL systems that incorporate
communication, encompassing recent advanced CommMARL research and identifying key dimensions that influence the design and development of these multi-agent
systems.

However, simulation environments are not merely auxiliary tools but are an integral part of the MAS development and evaluation process. The agents’ learning
and decision-making processes are influenced and constrained by these environments, making it equally important to understand and develop these environments
as it is to focus on the algorithms themselves. Finally,
the lack of attention to practical implementation details in current reviews has resulted in a disconnect between theory and practice. This survey will delve into
the specifics of project implementation, including code
structures, system architecture, and the challenges encountered during development, to enhance research reproducibility and facilitate the effective translation of
theoretical research into practical applications.
Building on the motivations outlined earlier, this survey extends beyond the scope of previous reviews,
which were often limited to specific areas of discussion.
We treat multi-agent environments as equally important
components, alongside the methods and techniques, and
provide a thorough introduction to the most advanced
algorithms and simulation environments. Moreover,
we categorize various multi-agent cooperative decisionmaking methods from a more fundamental implementation perspective. In summary, this survey seeks to provide a more comprehensive and practical framework for
research in multi-agent cooperative decision-making,
thereby advancing the continuous development of this
critical field.

1.3. Motivations of the Current Survey
However, despite the growing body of work in this
field, existing surveys often have noticeable limitations
[24, 6, 25, 28]. Specifically, our thorough investigation
reveals that most current reviews and surveys share several common and significant significant drawbacks and
limitations:
• Limited Research Scope: Previous literature reviews [27, 28] predominantly remain within the
primary framework of reinforcement learning and
have not broken through theoretical limitations, resulting in a lack of comprehensive coverage.
• Neglect of Environments: Previous literature
reviews [29, 6, 30] have largely concentrated
on methodological and algorithmic advancements,
frequently overlooking the essential role of simulation environments and platforms in multi-agent
intelligent decision-making.

1.4. The Survey Overview / Contents Organization
As depicted in Figure 2, we have structured the survey to reflect our research approach, with each main
and sub-branch corresponding to a specific part: First,
in Section 1, we introduce the research background of
multi-agent cooperative decision-making, discuss the
drawbacks of previous surveys, and outline the organizational structure of this survey. Given that MARL
and LLMs-based intelligent decision-making methods
demonstrate significant advantages and future potential,
our primary attentions are placed on Deep MARL-based
and LLMs-based methods due to their superior ability to manage dynamic and uncertain environments. In
Section 2, we then delve into mainstream intelligent
decision-making approaches, algorithms, and models.
We categorize these approaches, with a continued focus
on MARL-based and LLMs-based methods, discussing
their methodologies, advantages, and limitations. Following this, in Section 3, we provide an in-depth
analysis of the leading simulation environments and
platforms for multi-agent cooperative decision-making,
again focusing on Deep MARL-based and LLMs-based

• Under-emphasis of Project Implementation:
Prior surveys [25, 28, 30] often focus on theoretical models and overlook detailed implementation
aspects, including code-bases and project architectures. This gap limits readers’ ability to fully understand and apply the findings.
To address the aforementioned limitations and challenges, we recognize the need for more systematic and
comprehensive reviews in the multi-agent intelligent
decision-making field.
Firstly, current reviews overly emphasize deep reinforcement learning and fail to adequately consider other
potentially effective intelligent decision-making methods [24, 5, 7, 28]. Secondly, with the rapid development of large language models (LLMs), their potential
in natural language processing, knowledge representation, and complex decision-making has become increasingly apparent. However, current surveys have largely
overlooked their integration. Additionally, existing reviews often neglect the critical role of simulation environments in the development of multi-agent systems.
3

Research
Backgrounds

This Review of Multi-Agent Cooperative Decision-Making

Multi-Agent Scenarios

Two key-factors:
agents and environments

(simulation environments
and open-source platforms)

Environments
For
Multi-Agent RL

LLMs-based
Multi-agent
Environments

MARL-based Method Taxonomies

&

MARL-based

LLMs-based

Practice
Applications

Previous Surveys’
Limitations

Our Work
Motivations

Multi-Agent Decision-Making Methods
(models and algorithm)

Game
Theorybased

Rule-based

Evolutionary
Algorithms
-based

LLM-based Method Taxonomies

Multi-Agent
System (MAS)

Challenges in MARL

Future Directions of MARL

Challenges in LLM Reasoning
Outstanding
Works

Future Directions of MARL

Figure 2: Illustration of our systematic review of multi-agent intelligent decision-making research. Compared to previous reviews, we have
incorporated comprehensive introduction and analysis, with each segment corresponding to a specific chapter in the survey.

methods. Furthermore, in Section 4, we discuss the
practical applications of multi-agent decision-making
systems, such as autonomous driving, UAV navigation,
and collaborative robotics. Finally, in Sections 5 and 6,
we explore the potential challenges and future research
directions of multi-agent cooperative decision-making.

section provides a comprehensive analysis of the
deep MARL-based methods in multi-agent systems, detailing their corresponding technological
taxonomies, advantages, and limitations.
• If you are focused on decision-making based on
LLMs, Section 2.4 will offer you an in-depth exploration, with the corresponding technological
taxonomies, advantages, and limitations. This part
discusses the unique capabilities of LLMs in multiagent environments and their potential applications, especially in reasoning and decision-making.

1.5. How to read this survey?
This survey caters to a diverse readership, each with
varying levels of expertise and interest in different aspects of multi-agent decision-making systems. To help
readers efficiently find the content that interests them,
we offer the following guide, providing direction based
on different topics:

• For readers focused on the well-known simulation
environments of MAS, we suggest reading Section 3, which primarily covers an introduction to
MARL-based Simulation Environments (Section
3.1) and LLMs Reasoning-based Simulation Environments (Section 3.2).

• For those interested in rule (fuzzy logic)-based,
game theory-based, and evolutionary algorithmbased decision-making research, please refer to
Section 2.2.1, 2.2.2, and 2.2.3. This section
provides a comprehensive analysis of the rule
and game-based methods in multi-agent systems,
detailing their corresponding technological taxonomies, features, and limitations.

• If your interest lies in the practical applications
of multi-agent decision-making systems, Section
4 will be of particular relevance. This section
offers a detailed discussion of how these systems are applied across various fields, such as autonomous driving, UAV navigation, and collaborative robotics.

• For those interested in MARL-based decisionmaking research, please refer to Section 2.3. This
4

• If you are interested in the challenges and problems
faced by existing multi-agent decision-making
methods, Section 5 provides an in-depth discussion, exploring the limitations of current approaches and unresolved issues in the field, offering insights into these challenges.

quently, Sections 2.2, 2.3, and 2.4 introduce mainstream
paradigms of cooperative decision-making, MARLbased decision-making methods, and LLMs-based
multi-agent systems, respectively.
2.1. Agent Interaction Dynamics for Multi-Agent Systems
In multi-agent systems, the nature of interaction
among agents can be categorized into distinct relational
dynamics, i.e., agent interaction dynamics, each influencing the overall system behavior and outcomes.
These dynamics are critical to understanding and designing intelligent systems where agents operate within
shared environments. Below, we detail the primary
types of agent relationships:

• Finally, if you wish to learn about future research directions and the prospects for multi-agent
decision-making technique, we recommend reading Section 6. This section looks ahead to future
research trends and potential breakthroughs, exploring key directions that could drive the field forward.
2. Multi-Agent Decision-Making Taxonomies

1. Fully Cooperative: In this scenario setting, all
agents have aligned objectives, meaning they share
identical reward structures and work towards a
common goal. The agents operate with complete
cooperation, aiming to maximize collective benefits. This relationship is typical in systems where
synergy is essential, and the success of one agent
directly contributes to the success of others, ensuring mutual reinforcement of strategies and actions.
2. Fully Competitive: This relationship is characterized by a zero-sum game dynamic, where the gain
of one agent directly translates into the loss of another. Agents are in direct opposition, with their
objectives fundamentally misaligned. This is commonly observed in competitive environments such
as robotic competitions, where agents are designed
to outperform each other, and success is measured
relative to the failure or underperformance of others.
3. Mixed Cooperative and Competitive: In most realworld scenarios, agents may engage in both cooperation and competition simultaneously. This type
of relationship is evident in team-based environments like robotic soccer, where agents within the
same team cooperate to achieve a shared objective
(e.g., scoring goals), but simultaneously compete
against agents from opposing teams. The complexity of such systems lies in balancing internal cooperation with external competition, often requiring sophisticated strategies to optimize outcomes
at both individual and collective levels.
4. Self-Interested: In self-interested dynamics, each
agent acts primarily to maximize its own utility,
with little regard for the impact on others. An
agent’s actions might incidentally benefit or harm
other agents, but these effects are not a concern for

This section discusses the taxonomies of decisionmaking in multi-agent systems and their related techniques. The multi-agent cooperative decision-making
methods can be broadly classified into five categories:
rule-based (primarily fuzzy logic), game theory-based,
evolutionary algorithms-based methods, MARL-based
approaches, and LLMs-based methods [31]. Although
these rule-based, game theory-based, and evolutionary
algorithms-based solutions demonstrate a degree of effectiveness, they typically rely heavily on pre-designed
strategies and assumptions. This dependence limits
their adaptability to changing and complex environments and ill-suited for handling highly dynamic and
uncertain scenarios. In contrast, DRL-based and LLMs
reasoning-based solutions offer more dynamic and flexible approaches, capable of learning and adapting to new
strategies on the fly. Therefore, these methods have
significant advantages in dealing with dynamic and uncertain environments. Thus, special research attentions
are placed on DRL-based and LLMs-based methods due
to their significant advantages in handling dynamic and
uncertain environments.
The analysis is conducted from multiple perspectives, including agent interaction dynamics, mainstream
paradigms of cooperative decision-making, MARL
(multi-agent reinforcement learning), and LLM (large
language model)-driven multi-agent systems, aiming to
provide a systematic framework and technical foundation for the design and optimization of multi-agent
decision-making.
Specifically, Section 2.1 analyzes agent interaction
dynamics in MAS, categorizing them into four typical types: fully cooperative, fully competitive, mixed
cooperative-competitive, and self-interested, while discussing their impact on overall system behavior. Subse5

the self-interested agent. This relationship is pertinent in scenarios where agents are designed to prioritize personal gain over collective welfare, often
leading to outcomes where the overall system efficiency is not necessarily optimized, as individual
agents do not account for the potential externalities
of their actions on the environment or other agents.

Miki et al. [32] presented a rule-based multi-agent
control algorithm that utilizes local information instead
of absolute coordinates, making it more practical for
real-world applications. Charaf et al. [120] introduced a rule-based multi-agent system to address coordination challenges, such as controllability and observability, in distributed testing environments. Yarahmadi et al. [33] reviewed the applications of multiagent systems in Cyber-Physical Systems (CPS) and the
Internet of Things (IoT), proposing a combination of
learning and rule-based reasoning to improve decisionmaking in MAS. Marti et al. [117] presented an expert
rule-based system using multi-agent technology to support traffic management during weather-related issues.
Daeichian et al. [121] used fuzzy logic in combination with Q-learning and game theory to control traffic lights autonomously. Wu et al. [34] introduced a
fuzzy-theoretic game framework that integrates fuzzy
logic with game theory to handle uncertainty in utility
values during multi-agent decision making. Nekhai et
al. [118] devised a cybersecurity management model
for agricultural enterprises using a multi-agent system
(MAS) based on fuzzy logical reasoning. Ramezani et
al. [119] applied fuzzy logic to multi-agent decisionmaking in soccer robot teams, combining cooperative
and non-cooperative game strategies. Zhang et al. [35]
introduced a new online method for optimal coordination control in multi-agent differential games, combining fuzzy logic, and adaptive dynamic programming.
Ren et al. [36] presented a fuzzy logic-based approach
for partner selection in multi-agent systems, emphasizing flexibility and adaptability in dynamic environments. Gu et al. [37] introduced a cooperative reinforcement learning algorithm for multi-agent systems
using a leader-follower framework, modeled as a Stackelberg game. Schwartz et al. [60] introduced a multiagent fuzzy actor-critic learning algorithm for differential games. Harmati et al. [61] proposed a gametheoretic model for coordinating multiple robots in target tracking, using a semi-cooperative Stackelberg equilibrium and a fuzzy inference system for high-level cost
tuning. Khuen et al. [62] introduced an Adaptive Fuzzy
Logic (AFL) approach for multi-agent systems with negotiation capabilities, focusing on resource allocation.
Yan et al. [63] proposed a graphical game-based adaptive fuzzy optimal bipartite containment control scheme
for high-order nonlinear multi-agent systems (MASs).
Vicerra et al. [64] proposed a multi-agent robot system using a pure fuzzy logic-based artificial intelligence
model. Gu et al. [65] presented a fuzzy logic-based
policy gradient multi-agent reinforcement learning algorithm for leader-follower systems, where fuzzy logic

Overall, these agent interaction dynamics is crucial
for the design and analysis of multi-agent systems, as
they directly impact the strategies employed by agents
and the overall system performance. The complexity of
agent interactions in mixed or self-interested relationships often requires advanced coordination mechanisms
and incentive structures to manage potential conflicts
and ensure desired outcomes.
While the overarching concept of agent interaction
dynamics holds some value, this survey focuses more
on analyzing the characteristics of these methods from
a technical and scientific perspective. Therefore, distinguishing relationships is not the primary emphasis of
this study. Instead, we will proceed with a more comprehensive taxonomy of Multi-Agent Decision-making
Systems.
2.2. Mainstream Paradigms of Multi-Agent Cooperative Decision-Making
In multi-agent cooperative decision-making, several
mainstream paradigms exist, each leveraging different
techniques to tackle challenges associated with coordination, learning, adaptability, and optimization among
autonomous agents. These paradigms utilize diverse approaches, including rule-based (primarily fuzzy logic)
systems [32, 33, 34, 35, 36, 37], game theory-based
[38, 39, 40, 41, 42, 43], evolutionary algorithms-based
[44, 45, 46, 47, 48, 49], MARL-based [50, 51, 52, 53,
54, 55, 56], and LLMs-based [57, 30, 58, 59] multiagent decision-making systems. Each of these methods
has distinct strengths and applications, depending on the
problem context and the complexity of interactions between agents. For a comprehensive overview, please refer to Table 1, which provides a detailed classification
of these paradigms.
2.2.1. Rule-Based (Primarily Fuzzy Logic)
Rule-based decision-making, particularly fuzzy
logic, has been widely adopted in multi-agent systems
(MAS) due to its ability to handle uncertainty, imprecise
data, and dynamic environments [117, 118, 36, 119].
Fuzzy logic enables agents to make adaptive, humanlike decisions by mapping inputs to linguistic rules
rather than strict mathematical models.
6

Table 1: Representative Methods in Mainstream Paradigms of Multi-Agent Cooperative Decision-Making.

Paradigm

Representative Methods and Key References

Rule-Based (Primarily Fuzzy Logic)

Miki et al. [32], Yarahmadi et al. [33], Wu et al. [34], Zhang et al. [35], Ren et al.
[36], Gu et al. [37], Schwartz et al. [60], Harmati et al. [61], Khuen et al. [62], Yan
et al. [63], Vicerra et al. [64], Gu et al. [65], Maruyama et al. [66], Peng et al. [67],
Yang et al. [68]

Game Theory-based

Wang et al. [38], Guo et al. [39], Schwung et al. [40], Wang et al. [41], Lin et al.
[42], Wang et al. [43], Wang et al. [69], Lanctot et al. [70], Guo et al. [71], Zhang
et al. [72], Kong et al. [73], Wang et al. [74], Dong et al. [75], Nguyen et al. [76],
Schwung et al. [40], Khan et al. [77]

Evolutionary Algorithms-based

Liu et al. [45], Xu et al. [78], Daan et al. [46], Franciszek et al. [79], Larry et al. [44],
Daan et al. [47], Liu et al. [80], Yuan et al. [48], Dong et al. [81], Chen et al. [82],
Zhang et al. [49]

MARL-based

Wai et al. [83], Hu et al. [84], Son et al. [53], Yu et al. [52], Rashid et al. [50], Rashid
et al. [54], Sunehag et al. [85], Huang et al. [15], Xu et al. [58], Yun et al. [86], Mao
et al. [87], Kraemer et al. [51], Kouzeghar et al. [88], Gao et al. [89], Liu et al. [19],
Qi et al. [90], Vinyals et al. [91], Lu et al. [18], Chu et al. [92], et al. [20], Kurach et
al. [55], Lv et al. [93], Radac et al. [94], Wang et al. [56], Liu et al. [95]

LLMs-based

Mordatch et al. [59], Zhang et al. [96], Xu et al. [58], Li et al. [57], Wang et al. [30],
Zhao et al. [10], Hou et al. [97], Puig et al. [98, 99], Gao et al. [100], Xiao et al.
[101], Wang et al. [102], Wu et al. [103], Wen et al. [104], Chen et al. [105], Liu et
al. [106], Chen et al. [107], Hong et al. [108, 109], XAgent Team [110], Wang et al.
[111, 112], Zheng et al. [113], Zhang et al. [114, 115], Cao et al. [116]

controllers act as policies. Maruyama et al. [66] extended the classical framework for reasoning about distributed knowledge, incorporating fuzzy logic to handle
uncertainty and degrees of certainty within multi-agent
systems. Peng et al. [67] proposed a two-layer coordination model for multi-agent systems using fuzzy reinforcement learning. Yang et al. [68] presented a multiagent reinforcement learning algorithm with fuzzy policy to address control challenges in cooperative multiagent systems, particularly for autonomous robotic formations.
Overall, fuzzy logic remains a foundational approach
for rule-based decision-making in MAS, offering interpretability and robustness in uncertain environments. In
the future, fuzzy logic will be further integrated with
LLMs, hierarchical decision architectures, and multiagent planning, enabling more precise and adaptive
decision-making in complex real-world scenarios.

integrate reinforcement learning and Bayesian inference
to enhance adaptability in dynamic environments.
Wang et al. [38] provided a broad discussion on
game-theoretic approaches in multi-agent systems, covering cooperative and non-cooperative scenarios. Guo
et al. [39] applied game theory to multi-agent path planning, leveraging Nash equilibrium to optimize navigation and obstacle avoidance. Zhang et al. [72] developed a distributed control algorithm that ensures optimal coverage while maintaining network connectivity.
Beyond fundamental decision-making, game theory
has been applied in communication networks and energy systems. Wang et al. [41] utilized game-theoretic
learning to enhance resource allocation in wireless networks while countering adversarial actions like jamming. Lin et al. [42] introduced potential game theory to optimize distributed energy management in microgrids, where agents autonomously coordinate power
distribution. Dong et al. [75] further extended this approach using a hierarchical Stackelberg model for energy trading, balancing incentives between microgrids
and individual agents.
Incorporating machine learning with game theory
has also led to advances in multi-agent optimization.
Schwung et al. [40] combined potential game theory with reinforcement learning for adaptive production scheduling, while Wang et al. [74] designed a

2.2.2. Game theory-based
Game theory provides a structured framework for analyzing strategic interactions in multi-agent systems.
It enables agents to make rational decisions in cooperative, competitive, or mixed scenarios through
equilibrium-based optimization [5, 29]. Traditional
methods such as Nash equilibrium and Stackelberg
games form the foundation, while modern approaches
7

Nash equilibrium-based fault-tolerant control strategy
for multi-agent systems. Additionally, game-theoretic
methods have been explored for distributed computing,
as shown by Khan et al. [77], who developed a replica
placement strategy to minimize data access delays in
distributed systems.
Overall, game theory remains a cornerstone of multiagent decision-making, offering well-defined theoretical guarantees while enabling dynamic adaptation
through hybrid approaches. Future research will likely
focus on integrating game theory with deep learning and
large language models to enhance strategic reasoning in
high-dimensional, uncertain environments.

tonomous agents into multi-agent systems using evolutionary techniques like mutation and selection.
Evolutionary game theory has also been explored to
improve cooperative behavior. Dong et al. [81] designed a three-strategy decision model, where agents
adopt conservative or adaptive strategies based on their
interactions with neighbors, fostering long-term cooperation. Chen et al. [82] proposed a kinetic decisionmaking model grounded in rarefied gas dynamics, offering a new perspective on agent evolution using the
Boltzmann equation. Zhang et al. [49] applied evolutionary game theory to policy optimization, analyzing
cooperation strategies among governments, enterprises,
and farmers in agricultural water conservation projects.
Overall, evolutionary algorithms provide a robust
framework for decentralized decision-making, allowing agents to self-improve and adapt in uncertain environments. In the future, evolutionary algorithms will
be further integrated with deep learning, hierarchical
evolution, and large-scale multi-agent coordination, enabling more adaptive, autonomous MAS.

2.2.3. Evolutionary Algorithms-based
Evolutionary algorithms (EAs) provide a bio-inspired
approach to optimization in multi-agent systems by
leveraging principles such as natural selection, mutation, and recombination [79, 47, 81]. By allowing
agents to evolve their strategies iteratively, EAs are
particularly effective for problems requiring continuous
learning, large-scale coordination, and self-organized
behavior.
Liu et al. [45] introduced the Multi-Agent Genetic
Algorithm (MAGA), where agents interact through
competition and cooperation to optimize global solutions. Xu et al. [78] extended this idea to hardwarebased multi-agent systems, using nanoclusters as physical agents to achieve large-scale parallel computation.
Daan et al. [46] explored the role of evolutionary strategies in dynamic environments such as financial markets,
smart grids, and robotics, demonstrating how adaptive
algorithms can handle real-world uncertainties.
Franciszek et al. [79] proposed a self-optimization
model integrating cellular automata and game theory,
simulating competitive evolutionary interactions among
agents. Larry et al. [44] analyzed the trade-offs between mutation and recombination, showing that mutation can sometimes outperform traditional recombination strategies in evolutionary computing. To further enhance adaptability, Daan et al. [47] introduced
Deep Neuroevolution (DNE), applying coevolutionary
techniques to complex multi-agent scenarios, including
Atari games.
Recent studies have focused on scaling evolutionary learning to larger agent populations. Liu et al.
[80] developed Evolutionary Reinforcement Learning
(ERL), a scalable approach that partitions learning into
multiple stages, ensuring better adaptability in multiagent environments. Yuan et al. [48] introduced
EvoAgent, a framework that extends LLMs-based au-

2.2.4. MARL-based Multi-Agent Systems
Before introducing the MARL-based multi-agent
systems (MAS), we provide a detailed discussion in Appendix A on the key technological comparisons and
methodological principles of both DRL-based singleagent systems and MARL-based MAS. This helps readers build the necessary background knowledge for better
understanding the following discussions.
Multi-Agent Reinforcement Learning offers a structured framework to tackle decision-making in MAS,
where autonomous agents interact with each other
and a shared environment. The MAS research in
MARL is broadly divided into three paradigms: Centralized Training with Centralized Execution (CTCE)
[122, 123], Decentralized Training with Decentralized Execution (DTDE) [122], and Centralized Training with Decentralized Execution (CTDE) [51, 124].
Each paradigm is designed to address specific challenges such as coordination, scalability, and policy optimization, providing tailored solutions for diverse scenarios.
Centralized Training with Centralized Execution
The CTCE paradigm [125, 123] relies on a central controller that governs all agents by aggregating their observations, actions, and rewards to make joint decisions.
While this paradigm enables high levels of coordination, its scalability is limited in large-scale systems.
Multi-Agent DQN (MADQN) [126, 1, 2] is a representative method, employing parameter-sharing mechanisms to handle cooperative tasks effectively. However,
8

2.2.5. LLMs-based Multi-Agent Systems

its reliance on centralized control restricts its applicability in dynamic environments with numerous agents.

Although LLMs like GPT [140, 141, 142], Llama
[143, 144], and Gemini [145] support very long input
contexts, their ability to understand complex inputs still
varies. In this context, multi-agent collaboration optimizes task execution through role assignment, enabling
better performance through collaboration among multiple agents compared to a single agent. Each agent has
an independent workflow, memory, and can seek help
from other agents when necessary. LLMs-based MultiAgent Systems represent a relatively new multi-agent
decision-making model that leverages the powerful capabilities of language models, to enhance communication and collaboration between autonomous agents. In
an LLMs-based multi-agent system, agents communicate via natural language or symbolic representations,
breaking down complex tasks into smaller, more manageable subtasks. One important feature of LLMsbased systems is the hierarchical organization of agents,
typically consisting of two levels [30, 116]:
1) Global planning agents, responsible for high-level
decisions such as task decomposition, resource allocation, and overall strategy management.
2) Local execution agents, which are responsible for
executing specific subtasks and providing feedback to
the global planning agent. These agents are generally
more focused on local tasks but communicate progress
and challenges with the global level for adjustments.
This decomposition makes distributed problem solving
possible, with agents sharing information, strategies,
and goals through language, thus advancing task execution together.
For example, frameworks like AutoGen [103, 146,
107], Crew AI [114, 115], and LangGraph [111, 112]
provide rich tools for building multi-agent solutions,
supporting efficient cooperation and interaction between agents. Through these frameworks, developers
can build virtual teams that leverage the strengths of different agents in distributed tasks. Additionally, LLMsbased multi-agent systems possess adaptive re-planning
capabilities, enabling them to adjust in dynamic environments. When agents encounter changes or new information, they can quickly update strategies or re-plan
tasks using language models, ensuring the system remains aligned with changing goals.
Firstly, LLMs-based multi-agent environments have
emerged as pivotal platforms for advancing research
in multi-agent collaboration, reasoning, and task execution. For instance, environments such as ThreeDWorld Multi-Agent Transport (TDW-MAT) [147,
148], Communicative Watch-And-Help (C-WAH) [99],

Decentralized Training with Decentralized Execution The DTDE paradigm [122] emphasizes independent learning and execution, where each agent interacts
with the environment individually and updates its policy based solely on local observations and rewards. This
paradigm excels in scalability and robustness, especially
in scenarios with limited communication. Notable
methods include Independent Q-Learning (IQL) [127,
128] and Decentralized REINFORCE [129], which allow agents to learn autonomously. Despite its advantages, DTDE faces challenges such as learning nonstationarity, where the environment changes as other
agents adapt, and difficulty in addressing the credit assignment problem in cooperative settings.
Centralized Training with Decentralized Execution The CTDE paradigm [51, 130, 131] combines the
strengths of centralized training and decentralized execution, making it the most prominent paradigm in
MARL research. During training, a central controller
aggregates information from all agents to optimize their
policies, but during execution, each agent operates independently based on its own observations. CTDE addresses key challenges like non-stationarity and scalability, with methods such as Value Decomposition
Networks (VDN) [85] and QMIX [50, 52] for valuebased learning, Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [132] for actor-critic frameworks, and Multi-Agent Proximal Policy Optimization
(MAPPO) [133] for policy gradient optimization. These
approaches are widely applied in complex environments
like StarCraft II [134, 135] and the Multi-Agent Particle
Environment (MPE) [136, 137].
Communication-based MARL Algorithms Additionally, communication-based MARL algorithms have
emerged to enhance coordination by enabling agents
to share critical information during training and execution. Examples include Attentional Communication
(ATOC) [138] and Targeted Multi-Agent Communication (TarMAC) [139], which use advanced mechanisms
to improve the efficiency and effectiveness of interagent communication in cooperative tasks.
By structuring MARL methods within these
paradigms, researchers provide a clear framework
for addressing the diverse challenges of multi-agent
decision-making. From autonomous driving fleets to
resource allocation systems, MARL continues to push
the boundaries of what distributed intelligent systems
can achieve [124].
9

Cuisineworld [149], and AgentScope [100] offer diverse settings for evaluating and enhancing multi-agent
systems in various contexts, from household chores to
gaming interactions and beyond. For instance, MindAgent [149] is a novel infrastructure for evaluating planning and coordination capabilities in gaming interactions, leveraging large foundation models (LFMs) to coordinate multi-agent systems, collaborate with human
players. Communicative Watch-And-Help (C-WAH)
[98, 99] is a realistic multi-agent simulation environment and an extension of the Watch-And-Help Challenge platform, VirtualHome-Social. AgentScope [100]
is a robust and flexible multi-agent platform designed to
empower developers in building advanced multi-agent
systems by leveraging the potential of LLMs.
Meanwhile, LLMs-based multi-agent systems have
broad applications and great prospects [30, 116]. They
can collaborate in robotic teams to perform complex
tasks, such as product assembly or joint research, ensuring seamless interaction and cooperation [58, 59, 57].
In autonomous driving, LLMs help vehicles communicate, sharing real-time data and navigation strategies
to achieve coordinated actions. Moreover, LLMs can
support agents (such as drones) in disaster response,
transmitting critical information to help systems efficiently respond to crises. Wu et al. [103] proposed
AutoGen, an open-source framework for developing
next-generation LLM applications through multi-agent
conversations, allowing customizable agent interactions
and integration of LLMs, human inputs, and tools.
Xiao et al. [101] introduced Chain-of-Experts (CoE), a
multi-agent framework that enhances reasoning in complex operations research (OR) problems using LLMs,
with domain-specific roles and a conductor for guidance. Chen et al. [105] presented AgentVerse, a multiagent framework inspired by human group dynamics,
dynamically adjusting agent roles and composition to
enhance complex task-solving across various domains.
Chen et al. [107] developed AutoAgents, a framework that adaptively generates and coordinates multiple specialized agents for efficient task completion. Liu
et al. [106] proposed Dynamic LLM-Agent Network
(DyLAN), a framework that enhances LLM-agent collaboration through dynamic interactions based on task
requirements. Zhang et al. [96] introduced CoELA,
a Cooperative Embodied Language Agent framework
that leverages LLMs to enhance multi-agent cooperation in complex, decentralized environments. Hong et
al. [108, 109] proposed MetaGPT, a meta-programming
framework that enhances LLMs-based multi-agent system collaboration using Standard Operating Procedures
(SOPs). XAgent Team [110] developed XAgent, an

open-source, LLM-driven autonomous agent framework for solving complex tasks using a dual-loop architecture for task planning and execution. Zheng et
al. [113] introduced PlanAgent, a closed-loop motion planning framework for autonomous driving using multi-modal LLMs to generate hierarchical driving commands. Wang et al. [111, 112] developed
LangGraph, a library for building stateful, multi-actor
applications with LLMs, offering fine-grained control
over workflows and state management. Zhang et al.
[114, 115] introduced CrewAI, an open-source framework for coordinating AI agents in role-playing and autonomous operations, with a modular design for efficient collaboration. Hou et al. [97] proposed CoAct ,
a hierarchical multi-agent system leveraging LLMs for
collaborative task execution. It features a global planning agent for task decomposition and strategy management, and a local execution agent for subtask implementation, feedback collection, and adaptive replanning, ensuring alignment with overarching goals.
Despite the strong capabilities of LLMs in small to
medium-sized multi-agent systems, scalability remains
an open issue, particularly in maintaining coherent communication between large numbers of agents in large
environments. As the number of agents increases, the
complexity of coordinating their behaviors through language also intensifies. Finding a balance between agent
autonomy and effective collaboration is a significant
challenge. Additionally, LLMs are often seen as blackbox models, meaning understanding the reasoning process behind an agent’s decision-making can be difficult.
The lack of transparency poses challenges for trust and
debugging.
In summary, LLMs-based multi-agent systems hold
great potential in a variety of applications, offering an
advanced way to model and solve complex decisionmaking problems that require high levels of coordination, adaptability, and communication between agents.
By optimizing task decomposition, collaboration, and
feedback mechanisms, LLMs bring unprecedented efficiency and flexibility to multi-agent systems.
2.3. MARL-based Multi-Agent Decision-Making Taxonomies
In multi-agent systems, where multiple autonomous
agents interact with a shared environment and often
with each other, the complexity of decision-making increases significantly. To achieve optimal performance,
agents need to learn not only how to act individually but also how to coordinate with others. One of
the central challenges in MARL-based multi-agent systems is determining how much information should be
10

shared among agents during different phases of learning and deployment. The MARL research is typically structured into three primary paradigms: CTCE
[150, 151, 152, 126, 123], DTDE [122, 153], and CTDE
[124, 130, 51]. As illustrated in Figure 3, we will next
explain the principles and differences of the three methods in conjunction with this conceptual framework diagram.

Under CTDE, MARL algorithms can primarily be
categorized into three types based on their technical implementations: value function decomposition-based algorithms, actor-critic-based algorithms, and algorithms
based on policy gradient methods, such as proximal policy optimization (PPO).
1. Value Decomposition-based Algorithms Value
decomposition-based algorithms mainly address the
challenge of estimating the joint state-action value function (Q-value) in multi-agent systems, which is difficult due to the high dimensionality of the joint action space. Instead of directly estimating this joint
value function, these algorithms decompose it into more
manageable individual state-action value functions (Qvalue) for each agent. During execution, each agent selects its action based on its own value function. In training, the joint value function is computed from individual value functions, and the temporal difference error
of the joint value guides the learning of the individual
functions. A key principle these algorithms must satisfy
is the Individual-Global-Max (IGM) principle, ensuring
that the actions maximizing the joint value are consistent with those maximizing individual values. Different
algorithms use various methods to approximate or satisfy the IGM principle.
Value Decomposition Networks (VDN) [85] is one
of the earliest value decomposition-based algorithms in
CTDE-based MARL models. VDN simplifies the estimation of the joint state-action value function by assuming that it can be represented as the sum of the individual
state-action value functions of all agents. It means that
the joint value function is obtained by simply adding
up the individual value functions, which does not take
into account the varying contributions of each agent’s
Q-value. However, the assumption made by VDN is a
sufficient but not necessary condition for satisfying the
IGM principle, which can limit its applicability. Additionally, VDN does not utilize global state information during training, further restricting its effectiveness
in more complex environments.
To address this issue, Rashid et al. [50] proposed the
QMIX algorithm within the CTDE paradigm. QMIX
assumes a monotonic nonlinear relationship between
the joint state-action value function and the individual
state-action value functions of agents. To implement
this, QMIX introduces a mixing network that computes
the joint state-action value function based on the individual Q-values of all agents. This mixing network is
designed with non-negative parameters to ensure that
the monotonicity assumption is met. QMIX has been
successfully applied in various scenarios and is considered one of the most successful value decomposition al-

2.3.1. Centralized Training with Decentralized Execution (CTDE)
As shown in the left of Figure 3, CTDE is a hybrid
MARL approach that combines the strengths of both
centralized and decentralized systems [124]. In CTDE,
each agent possesses its own policy network, which is
trained under the guidance of a central controller. This
approach is characterized by a two-phase process: centralized training followed by decentralized execution.
1. Centralized Training (Phase 1): During the training phase, the central controller collects data from
all agents, including their observations, actions,
and rewards. This centralized data aggregation allows the controller to oversee the learning process
and facilitate the training of each agent’s policy
network.
2. Decentralized Execution (Phase 2): Once the
training is complete, the central controller’s involvement ceases, and each agent operates independently. At execution, agents make decisions
based on their own observations using their trained
policy networks.
In some communication-constrained scenarios,
agents often cannot share or fully share their observations of the environment. Instead, they must make
decisions independently based on their own local observations and policies, which limits the applicability of
fully centralized methods. To overcome this challenge,
Kraemer et al. [124, 51] proposed the CTDE learning
paradigm. The CTDE agents have access to global
environmental state information and the observations of
other agents during the training phase, allowing them
to learn a joint policy together. However, during the
execution phase, each agent relies solely on its own
observations and the trained policy to make independent decisions. It combines the advantages of fully
decentralized and fully centralized methods, effectively
mitigating issues such as learning non-stationarity and
the curse of dimensionality, making it the dominant
paradigm in current MARL solutions.
11

Agent 2

𝑎1𝑜1 , 𝑟1

…

Agent N

𝑎2𝑜 2 , 𝑟 2

𝑎𝑁 𝑜 𝑁 , 𝑟 𝑁

Training

Agent 1

2

…𝑁

𝑜𝑁 , 𝑟 𝑁

𝑎 ~ 𝜋(a𝑁 |𝑜 𝑁 ; 𝜃 𝑁 )

Environment 𝑺golab

Execution

𝑜1 , 𝑟 1

2

Agent N

… 𝑎 𝑜 ,𝑟
𝑁

𝑁

Agent 1

𝑁

Agent 2

𝑎1 𝑺1∗ , 𝑟1 𝑎2 𝑺∗2 , 𝑟 2

…

Agent N

… 𝑎

𝑁

∗
𝑺𝑁
, 𝑟𝑁

Central Controller
𝑓control (·)

Environment 𝑺golab
(2) DTDE

(1) CTDE
𝑃𝑜𝑙𝑖𝑐𝑦 N𝑒𝑡𝑤𝑜𝑟𝑘 of Agent 𝑖: 𝜋(a𝑖 |𝑠; 𝜃 i )

…

Agent 2

𝑎 1 𝑜1 , 𝑟 1 𝑎 2 𝑜 , 𝑟

Central Controller
𝑎1 ~ 𝜋(a1 |𝑜1 ; 𝜃 1 )

Agent 1

𝑺𝑔𝑜𝑙𝑎𝑏

= [𝑜1 , 𝑜2 , … , 𝑜𝑖 , … , 𝑜𝑁 ]

Environment 𝑺golab
(3) CTCE
∗
𝑺1∗ , 𝑺2∗ , … , 𝑺𝑁
: = 𝑓control (𝑺𝒈𝒐𝒍𝒂𝒃 )

Figure 3: The paradigms visualization of CTDE (left), DTDE (centre), and CTCE (right), consisting of three crucial elements: agent (i.e., algorithm
or model), environment, central controller (Optional).

network structure. Fast-QMIX [52] enhances the original QMIX by dynamically assigning virtual weighted
Q-values with an additional network, improving convergence speed, stability, and overall performance in cooperative multi-agent scenarios. QTRAN [53] introduces
a more flexible factorization method that overcomes the
structural limitations of QMIX, where the joint Q-value
is constrained to be a monotonic function of the individual Q-values, thereby imposing a specific structural
form on the factorization. Specifically, QTRAN introduces a necessary and sufficient condition for the IGM
principle and incorporates two additional loss terms into
the loss function to constrain the training of individual
Q-value functions, ensuring they satisfy this IGM principle.
2. Actor-Critic-based Algorithms: Actor-Criticbased algorithms [132, 60, 155] represent a foundational class of methods within the CTDE paradigm,
offering a flexible and effective approach for tackling
the challenges of multi-agent environments. These algorithms combine the strengths of policy optimization
(actor) with value estimation (critic), allowing agents
to learn robust and adaptive strategies in both cooperative and competitive settings. By leveraging a centralized critic during training, Actor-Critic-based methods
[136, 137, 92] address key issues such as environmental
non-stationarity and credit assignment, enabling effective policy optimization in dynamic and complex multiagent scenarios. Below, we discuss several prominent
Actor-Critic-based approaches and their contributions
to advancing MARL.
MADDPG [132] is a typical Actor-Critic-based
CTDE approach specifically designed to address the
challenges of multi-agent environments, where agents
engage in both cooperative and competitive interactions.
Traditional reinforcement learning algorithms, such as

gorithms to date. By enforcing a monotonic relationship between the joint action Q-values and individual Qvalues, QMIX simplifies the policy decomposition process, facilitating decentralized decision-making. However, the monotonicity assumption, while sufficient for
ensuring the Individual-Global-Max (IGM) principle, is
not a necessary condition. This limitation restricts the
algorithm’s applicability in situations where an agent’s
optimal action depends on the actions of other agents.
Weighted QMIX [54] builds upon QMIX and addresses this limitation by introducing a novel weighting mechanism during the projection of Q-values,
which is widely used for cooperative MARL scenarios. In QMIX, the projection of Q-learning targets
into the representable space is done with equal weighting across all joint actions, which can lead to suboptimal policy representations, even if the true optimal Qvalues (Q*) are known. To overcome this, Weighted
QMIX introduces two weighting schemes-CentrallyWeighted (CW) QMIX and Optimistically-Weighted
(OW) QMIX-that place greater emphasis on the better
joint actions during this projection process. The weighting schemes ensure that the correct maximal action is
recovered for any set of joint action Q-values, effectively improving the algorithm’s ability to learn optimal
policies. These schemes in Weighted QMIX enhances
the representational capacity of QMIX, demonstrating
improved results on both predator-prey scenarios of
Multi-Agent Particle Environment (MPE) [137] and the
challenging StarCraft II benchmarks [154, 91, 134].
Since then, numerous methods building on value
function decomposition have been developed. QPLEX
[56] introduces a novel duplex dueling network architecture for multi-agent Q-learning, designed to nonlinearly decompose the joint state-action value function
while embedding the IGM principle directly into the
12

Q-learning and policy gradient methods, struggle in
multi-agent settings due to issues like non-stationaritywhere the environment constantly changes as other
agents learn-and increased variance with the growing
number of agents. MADDPG adapts the actor-critic
framework by incorporating a centralized critic during
training, which has access to the actions and observations of all agents. This centralized critic helps mitigate the non-stationarity problem by learning a more
stable value function that considers the joint action
space. During execution, however, each agent independently follows its policy (actor) based on local observations, enabling decentralized decision-making. It
allows each agent to successfully learn and execute
complex coordination strategies, outperforming existing methods in both cooperative and competitive multiagent environments. To address the computational challenges of continuous action spaces, Li et al. [156] extend the MADDPG algorithm to Multi-Agent Mutual
Information Maximization Deep Deterministic Policy
Gradient (M3DDPG) by incorporating a minimax approach to enhance robustness in multi-agent environments. M3DDPG introduce Multi-Agent Adversarial
Learning (MAAL), which efficiently solves the minimax formulation, ensuring agents can generalize even
when opponents’ policies change and leading to significant improvements over existing baselines in mixed
cooperative-competitive scenarios.
Counterfactual Multi-Agent Policy Gradient
(COMA) [157] is a cooperative algorithm based
on the Actor-Critic framework that uses centralized
learning to address the credit assignment problem
in multi-agent settings. COMA employs a centralized critic to compute advantage functions for each
agent, using counterfactual baselines to reduce policy
dependencies among agents and improve learning
efficiency. Each agent has its own policy network, but
the shared centralized critic evaluates joint Q-values by
considering the collective state and action information
of all agents. This approach minimizes the negative
impacts of policy dependencies and allows for a more
comprehensive assessment of each agent’s behavior,
enhancing overall policy optimization.
3. Proximal Policy Optimization-based Algorithms: Proximal Policy Optimization (PPO) [158] is
a widely used CTDE reinforcement learning algorithm
that has been adapted and extended to address challenges in MARL. Within the CTDE paradigm, PPO and
its multi-agent variants have shown remarkable effectiveness in balancing policy optimization efficiency and
stability. PPO was introduced by Schulman et al. [158]
as an efficient policy gradient algorithm designed to im-

prove upon the trust region policy optimization (TRPO)
framework [159]. PPO employs a clipped surrogate objective function that simplifies the trust region constraint
in TRPO, allowing for stable updates without overly restrictive computational overhead. The key innovation of
PPO lies in its ability to control the magnitude of policy
updates through the clipping mechanism, which ensures
that policies do not deviate excessively from their previous versions.
In MARL, Multi-Agent PPO (MAPPO) [133] extends PPO to the centralized critic paradigm. MAPPO
uses a centralized value function (critic) that evaluates
joint states and actions during training, while agents execute independently using their decentralized policies.
MAPPO has demonstrated superior performance in various cooperative and competitive multi-agent environments, such as the StarCraft II [134, 135] and MultiAgent Particle Environment (MPE) [59, 136, 137]
benchmarks. The centralized critic allows for improved
credit assignment and non-stationarity handling during training, while the decentralized execution ensures
scalability. While MAPPO leverages parameter sharing among agents, this assumption may not hold in
heterogeneous-agent systems where agents differ in capabilities, objectives, or action spaces.
To address this, Kuba et al.
[160] proposed
Heterogeneous-Agent Trust Region Policy Optimization (HATRPO) and Heterogeneous-Agent Proximal
Policy Optimization (HAPPO). These algorithms remove the parameter-sharing assumption, allowing for
individualized policy networks for each agent. HATRPO builds upon TRPO by introducing a sequential
update scheme, where only one agent updates its policy at a time while the policies of other agents remain
fixed. This approach ensures monotonic improvement
in joint policies, as it approximates the Nash equilibrium under certain conditions, such as full observability
and deterministic environments. HAPPO extends PPO
in a similar vein, replacing parameter sharing with individualized policies. Like HATRPO, HAPPO employs
a sequential update mechanism, but it retains the computational efficiency and practical simplicity of PPO’s
clipped objective function.
Both HATRPO and HAPPO utilize a sequential update process where one agent updates its policy while
others remain fixed. This prevents conflicts during policy optimization and ensures theoretical convergence to
a stable joint policy. Moreover, HATRPO and HAPPO
provide monotonic improvement guarantees under specific conditions. By removing the parameter-sharing
constraint, these algorithms enable agents to learn tailored policies that account for their unique roles and
13

capabilities. Both algorithms perform competitively in
benchmark tasks, demonstrating their ability to scale to
high-dimensional state-action spaces while maintaining
robust coordination among agents.
PPO-based algorithms, including MAPPO [133],
HATRPO [160], and HAPPO [160], have revolutionized multi-agent reinforcement learning by combining
the stability of PPO with the coordination benefits of
centralized critics. These algorithms have proven effective across a wide array of cooperative and competitive
MARL tasks, offering strong performance and scalability.
3. Other Categories of Algorithms within the
CTDE Paradigm: In addition to the well-established
categories of Value Decomposition-based, Actor-Criticbased, and Proximal Policy Optimization (PPO)-based
algorithms, the MARL research has seen significant advancements through innovative optimizations and enhancements within CTDE paradigm that are not confined to these traditional classifications. These approaches aim to address the inherent challenges of
multi-agent environments, such as non-stationarity and
limited communication, to improve overall cooperation
and policy learning efficiency.
For example, Centralized Advising and Decentralized Pruning (CADP) is a novel framework introduced
by Zhou et al. [131] to address limitations in the
CTDE paradigm. CADP enhances the training process
by allowing agents to explicitly communicate and exchange advice during centralized training, thus improving joint-policy exploration. To maintain decentralized
execution, CADP incorporates a smooth model pruning mechanism that gradually restricts agent communication without compromising their cooperative capabilities, demonstrating its superior performance on multiagent StarCraft II SMAC and Google Research Football benchmarks. CommNet [161] introduces a neural model where multiple agents learn to communicate continuously and collaboratively through a shared
communication channel, optimizing their performance
on fully cooperative tasks. The method allows agents
to develop their own communication protocols during
training, leading to improved coordination and tasksolving capabilities. Mao et al. [87] introduced a
novel Meta-MARL framework by integrating gametheoretical meta-learning with MARL algorithms using
the CTDE’s framework, such as the Actor-Critic-based
COMA [157]. This framework offers initializationdependent convergence guarantees and significantly improves convergence rates by addressing related tasks
collectively. Yun et al. [86] proposed a novel approach called Quantum Meta Multi-Agent Reinforce-

ment Learning (QM2ARL), achieving high rewards,
fast convergence, and quick adaptation in dynamic
environments. QM2ARL leverages the unique dualdimensional trainability of Quantum Neural Networks
(QNNs) to enhance MARL. Liu et al. [95] proposed
the Learning before Interaction (LBI) framework, which
integrates a language-guided simulator into the multiagent reinforcement learning pipeline to address complex decision-making problems. By leveraging a generative world model with dynamics and reward components, LBI generates trial-and-error experiences to improve policy learning, demonstrating superior performance and generalization on the StarCraft Multi-Agent
Challenge benchmark [134, 135].
2.3.2. Decentralized Training with Decentralized Execution (DTDE)
As shown in the centre of Figure 3, DTDE represents
a fully decentralized mechanism where each agent interacts independently with the environment and updates its
own policy based on its own observations and rewards
[122]. In this framework, each agent trains and operates
completely independently, relying only on its own observations and rewards to update its strategy. DTDE is
particularly suited for environments with limited communication or no global coordination, offering strong
scalability and robustness [123].
The core idea behind DTDE is the independence of
agents [122]. Each agent interacts with its environment
and learns without requiring information from others.
This makes DTDE scalable, but it also introduces challenges such as non-stationarity, where the environment
appears to change as other agents adapt their strategies.
This characteristic makes DTDE a valuable and challenging area of research. The theoretical foundation of
DTDE is often based on Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs). As
described by Amato et al. [122, 124], a Dec-POMDP
models a decentralized decision-making environment
where agents operate independently with limited observations while aiming to maximize a shared reward. The
decentralized nature of DTDE requires each agent to
learn optimal strategies based on local information only.
First and foremost, one of the earliest DTDE approaches is Independent Q-Learning (IQL) by et al.
[127]. Here, each agent applies Q-learning independently, maintaining its own Q-function and updating it
based on local observations and rewards. However, IQL
faces several challenges, such as the non-stationary nature of the environment caused by other agents learning
simultaneously. It also struggles with credit assignment,
where it is hard to determine how an individual agent
14

contributes to the team’s success. To address these issues, several extensions of IQL have been proposed:

and update more frequently, improving sample efficiency.

• Distributed Q-Learning [128] optimistically assumes other agents always take optimal actions, focusing on learning from high-reward interactions.
While effective in deterministic settings, it can be
overly optimistic in environments with randomness.

• Independent Proximal Policy Optimization
(IPPO) [158, 153, 133] Extending Proximal Policy Optimization (PPO) to decentralized settings,
IPPO improves policy stability by limiting how
much policies can change between updates.
Despite its advantages, DTDE still faces significant
challenges [122, 123]: 1. Non-Stationarity: As other
agents learn and adapt, the environment appears dynamic and unstable to each agent, making convergence
difficult; 2. Credit Assignment: It is hard to determine
how each agent’s actions contribute to the team’s overall reward in cooperative tasks; 3. Trade-Offs Between
Scalability and Performance: While DTDE scales well,
its performance may be limited in tasks requiring high
levels of coordination. To overcome these challenges,
future research could focus on improving communication strategies during training and more robust strategies
for dynamic environments.
In conclusion, the DTDE paradigm provides a powerful framework for solving distributed decision-making
problems, balancing scalability, independence, and efficiency. It has been successfully applied in areas such
as autonomous driving, distributed energy management,
and swarm robotics. As research continues, DTDE is
expected to play a larger role in real-world multi-agent
systems, especially in scenarios requiring robust, independent learning.

• Hysteretic Q-Learning [162] By introducing two
learning rates—one for positive updates and another, smaller rate for negative updates—hysteretic
Q-learning balances optimism with robustness in
stochastic environments.
• Lenient Q-Learning [163] dynamically adjusts
how lenient the agent is in updating its values,
depending on how frequently specific state-action
pairs are encountered. It allows for more exploration in the early stages of learning while focusing
on optimization later.
As MARL problems become more complex, DTDE
methods have been extended to deep learning. Deep
Q-Networks [150, 151, 152, 126] and Deep Recurrent
Q-Networks [164] have been adapted for decentralized
settings, enabling agents to handle high-dimensional
state and action spaces. Independent DRQN (IDRQN)
[165], for example, combines DRQN with independent
learning, but its asynchronous experience replay can
cause instability. To solve this, Concurrent Experience
Replay Trajectories (CERTs) [166] synchronize experience replay among agents, reducing non-stationarity
and improving learning efficiency. Other advancements
include Deep Hysteretic DRQN (Dec-HDRQN) [166],
which combines hysteretic updates with deep neural
networks and uses concurrent buffers to handle decentralized data. These methods have shown robust performance in partially observable environments.
In the DTDE paradigm, policy gradient methods offer
an alternative to value-based approaches, particularly
for scenarios involving continuous action spaces [122].
Several policy gradient DTDE methods have been proposed:

2.3.3. Centralized Training with Centralized Execution
(CTCE)
As shown in the right of Figure 3, Centralized Training with Centralized Execution (CTCE) stands out
as a fully centralized mechanism to MARL decisionmaking, where all agents transmit their information to
a central controller [150, 151, 152, 126]. This central
controller has access to the observations, actions, and
rewards of all agents. The agents themselves do not possess policy networks and are not responsible for making
decisions. Instead, they simply execute the directives
issued by the central controller [125, 123].
Multi-Agent DQN [126] is a classic example of
the CTCE paradigm, where DQN is combined with a
parameter-sharing mechanism to address tasks in multiagent environments. Gupta et al. [126] firstly extends three single-agent DRL algorithms (DQN [1, 2],
TRPO, and A3C) to multi-agent systems, resulting
in Multi-Agent-DQN, Multi-Agent-TRPO, and MultiAgent-A3C. These approaches were designed to learn

• Decentralized REINFORCE [129] independently optimizes each agent’s policy using gradient
ascent based on rewards observed during episodes.
While simple, it is less sample-efficient.
• Independent Actor-Critic (IAC) [157] Combining value estimation (critic) and policy optimization (actor), IAC enables agents to learn faster
15

paradigms has its strengths and limitations, yet all face
inherent difficulties in handling communication among
agents, which is critical for effective collaboration and
decision-making.
Specifically, the CTCE paradigm, while providing a
fully integrated framework for learning and execution,
struggles with scalability as the system size grows. The
DTDE paradigm, on the other hand, allows for independent agent training and execution, but often lacks
the necessary coordination required for global task optimization. The CTDE paradigm has emerged as a widely
adopted approach due to its ability to leverage centralized information during training to learn effective policies, while enabling decentralized execution to operate
efficiently in distributed environments. However, even
in CTDE, the communication between agents during execution is a bottleneck, prompting researchers to focus
on improving communication strategies to enhance system performance.
Communication-based MARL algorithms have made
significant progress in overcoming these challenges.
From the perspective of communication protocols and
languages, communication-based MARL methods can
be categorized into three types: broadcasting communication, targeted communication, and networked communication, as shown in Figure 4. From the technical
angle, we provide an overview of these communicationbased MARL advancements, categorizing the algorithms into three main groups based on their focus: (1)
value function-based Communication-based MARL,
(2) policy search-based Communication-based MARL,
and (3) Communication-based MARL algorithms designed to improve communication efficiency. These approaches represent the forefront of research in enabling
agents to effectively share information, coordinate actions, and optimize performance in complex environments. Here, we provide a detailed introduction to these
approaches.
Value Function-Based Communication-based
MARL: For Value Function-Based Communicationbased MARL Algorithms, several notable works
include Differentiable Inter-Agent Learning (DIAL)
[168] and Deep Distributed Recurrent Q-Networks
(DDRQN) [169, 170]. Among them, DIAL facilitates
effective collaboration and optimization of joint action
policies by enabling the exchange of gradients of Qfunctions between agents. On the other hand, DDRQN
leverages recurrent neural networks to address partially
observable environments, allowing agents to share
critical Q-values or hidden states and make adaptive
decisions in dynamic settings.
Policy
Search-Based
Communication-based

cooperative policies in complex, partially observable
environments without requiring explicit communication
between agents. The DQN algorithm based on multiagent settings, also known as PS-DQN (Parameter Sharing DQN), effectively utilizes curriculum learning to
handle increasing task complexity. By starting with
fewer agents and gradually increasing the number, the
model scales well to more complex scenarios. Further,
this foundational work has led to numerous enhancements and variants based on Multi-Agent DQN, each
designed to address specific challenges in multi-agent
systems, such as CoRe [150], MARL-DQN [151], and
[152]. CoRe [150] introduces a counterfactual reward
mechanism into MARL to address the credit assignment problem in cooperative settings. By computing the
difference in global rewards when an agent hypothetically changes its action while others keep theirs fixed,
CoRe enhances the standard DQN framework, significantly improving learning efficiency and performance
in cooperative tasks. MARL-DQN [151] optimizes energy efficiency and resource allocation in NOMA wireless systems by using MARL framework combined with
Deep Q-Networks. By combining MARL with DQN, it
dynamically adjusts power and time allocation to minimize energy consumption while ensuring quality of service, outperforming traditional methods in terms of efficiency and performance. Hafiz et al. [152] proposed a
simplified and efficient multi-agent DQN-based multiagent system (MAS) that addresses the challenges of
complexity, resource demands, and training difficulties
inherent in more advanced MARL frameworks. The
work introduced a shared state and reward system while
maintaining agent-specific actions, which streamlines
the experience replay process. The significant improvements in tasks such as Cartpole-v11 , LunarLander-v22 ,
and Maze Traversal3 from OpenAI Gym [167] demonstrates the model’s effectiveness and superiority.
2.3.4. Addition
Taxonomies
and
Efforts
of
Communication-based MARL Algorithms
As outlined above, three primary paradigms—CTCE,
DTDE, and CTDE—have emerged in the MARL domain to tackle the challenges associated with training
and execution in multi-agent systems. Each of these
1 Cartpole-v1
game:
https://www.gymlibrary.dev/
environments/classic_control/cart_pole/.
2 LunarLander-v2
game:
https://www.gymlibrary.
dev/environments/box2d/lunar_lander/
and
https:
//github.com/topics/lunarlander-v2.
3 Maze Traversal game: https://github.com/vision-mini/
MazeSolverLLM.

16

（1）Broadcasting communication

（2）Targeted communication

（3）Networked communication

Figure 4: A schematic representation of three distinct communication methods among agents, with arrows indicating the direction of message
transmission. (a) Broadcasting communication: The activated agent transmits messages to all other agents within the communication network. (b)
Targeted communication: Agents selectively communicate with specific target agents based on a supervisory mechanism that regulates the timing,
content, and recipients of the messages. (c) Networked communication: Agents engage in localized interactions with their neighboring agents
within the network.

MARL: For Policy Search-Based Communicationbased MARL Algorithms, significant progress has
been made with approaches such as Communication
Networks (CommNet) [171], Bidirectional Coordinated
Network (BiCNet) [172, 173], Multi-Agent Distributed
MADDPG (MD-MADDPG) [174, 132], Intrinsic A3C
[175, 155], and Multi-Agent Communication and Coordination (MACC) [176, 177]. Among them, CommNet
[171] proposes a centralized but differentiable communication framework where agents share encoded
signals to form a global context, improving policy
decisions. BiCNet [172, 173] enhances coordination
among agents by employing bidirectional recurrent
layers, making it suitable for complex tasks. MDMADDPG [174, 132] combines centralized training
and decentralized execution, enabling agents to exchange critical state-action information during training
for robust policy learning. Intrinsic A3C [175, 155]
introduces intrinsic motivation to encourage effective
exploration in sparse-reward scenarios, with agents
sharing intrinsic rewards through communication to
boost performance. Finally, Multi-Agent Communication and Coordination (MACC) [176, 177] focuses
on adaptive communication mechanisms, providing
stable and secure coordination to enhance training and
execution in dynamic multi-agent environments.

(TarMAC) [139], Inter-Agent Centralized Communication (IC3Net) [178]. Attentional Communication
(ATOC) [138] employs an attention mechanism to dynamically determine when communication is necessary, achieving a balance between efficiency and coordination. Targeted Multi-Agent Communication (TarMAC) [139] introduces targeted attention mechanisms
to direct messages to relevant teammates, minimizing redundant communication, and improving overall
performance. Inter-Agent Centralized Communication
(IC3Net) [178] incorporates a gating mechanism that allows agents to learn when and how to communicate, optimizing both the frequency and quality of interactions.
These research advances in Communication-based
MARL methods demonstrate significant strides in enabling agents to share information and achieve coordinated decision-making in MAS. These advancements
will pave the way for deploying MARL in real-world
scenarios where efficient and effective communication
is essential.
2.4. LLMs-based Multi-Agent System Taxonomies
The field of LLMs-based multi-agent systems has
seen significant advancements, with researchers exploring various aspects of these systems to enhance their capabilities and applications [116, 30]. A comprehensive
taxonomy can help categorize and understand the different dimensions of LLMs-based multi-agent systems, including architectural design, application domains, evaluation methods, and future research directions.

Communication-based MARL Enhancing Communication Efficiency: For algorithms aimed at enhancing communication efficiency, several outstanding approaches include Attentional Communication
(ATOC) [138], Targeted Multi-Agent Communication
17

2.4.1. Architectural Design
The design of architectures for LLMs-based multiagent systems is a critical component in harnessing the
full potential of LLMs to enhance the capabilities of autonomous agents. Architectural design encompasses the
framework and mechanisms that enable agents to interact, adapt, and make decisions in complex and dynamic
environments. This section explores two primary levels
of autonomy within these systems: Adaptive Autonomy
and Self-Organizing Autonomy.

Experiments have shown that LLMs can produce
results qualitatively similar to those of traditional
economic models, making them a promising tool
for exploring new social science insights. For example, in market simulation, LLMs can predict
market trends and the impact of economic policies.
In behavioral economics, LLMs can model individual and group decision-making processes, providing a more nuanced understanding of economic
behavior.

• Adaptive Autonomy: [103, 146, 113, 110] Adaptive autonomy refers to systems where agents can
adjust their behavior within a predefined framework. These agents are designed to operate within
the constraints set by the system architects but can
adapt their actions based on the specific requirements of the task at hand. For example, in a taskspecific adaptation scenario, an agent might adjust
its search strategy in an information retrieval task
based on the relevance of the results. In a contextaware adaptation scenario, an agent might change
its communication style based on the social context
of the interaction. This level of autonomy is crucial
for agents that need to operate in dynamic environments where the task requirements can change over
time.

• 2) Social Network Simulation: [147, 148, 99, 180]
The Social-network Simulation System (S3) uses
LLMs-based agents to simulate social networks,
accurately replicating individual attitudes, emotions, and behaviors. This system can model the
propagation of information, attitudes, and emotions at the population level, providing valuable insights into social dynamics. For example, it can
simulate how information spreads through social
networks and identify influential nodes, or model
the evolution of social norms and behaviors over
time.
• 3) User Behavior Analysis: [97, 111, 112] LLMs
are employed for user simulation in recommender
systems, demonstrating superiority over baseline
simulation systems. They can generate reliable
user behaviors, improving the accuracy of recommendations. For example, in personalized recommendations, LLMs can generate user profiles and
behaviors to optimize recommendation algorithms.
In user engagement, LLMs can simulate user interactions to optimize user retention and engagement.

• Self-Organizing Autonomy: [101, 103, 105, 102,
104, 154] Self-organizing autonomy represents a
higher level of autonomy where agents can dynamically adapt their behavior without predefined
structures. This allows for more flexible and
context-aware interactions among agents. For instance, in dynamic task allocation, agents can
assign tasks to each other based on the current
state of the environment and their individual skills.
Emergent behavior is another key feature at this
level, where agents can form coalitions or develop
new strategies to solve complex problems. This
level of autonomy is essential for multi-agent systems that need to operate in highly dynamic and
unpredictable environments.

In the natural sciences [100, 149, 181], LLMs-based
agents have been used to simulate complex systems and
processes, providing insights into natural phenomena
and scientific theories.
• 1) Macroeconomic Simulation: LLMs-based
agents are used for macroeconomic simulation,
making realistic decisions and reproducing classic macroeconomic phenomena. These agents can
simulate the impact of economic policies on the
macroeconomy, providing a more accurate and dynamic model of economic behavior. For example,
they can simulate the interactions between different economic sectors and their impact on the overall economy, helping policymakers make more informed decisions.

2.4.2. Applications
In the social sciences [148, 99, 112], LLMs-based
agents have been used to simulate various social phenomena, providing insights into human behavior and social dynamics.
• 1) Economic Agents: [179, 101] LLMs can be
used to model economic agents, similar to how
economists use the concept of homo economicus.

• 2) Generative Agent-Based Modeling: This approach couples mechanistic models with genera18

tive artificial intelligence to unveil social system
dynamics, such as norm diffusion and social dynamics. By combining the strengths of both approaches, researchers can model complex social
systems and predict their behavior over time. For
example, they can model the spread of diseases in
a population, the impact of environmental changes
on ecosystems, or the evolution of social norms in
a community.

us better understand the behaviors and interactions of
agents in dynamic settings. By simulating these interactions, researchers can gain insights into how agents
coordinate and adapt to changing conditions, thereby
improving the robustness and efficiency of multi-agent
systems in real-world applications. Consequently, the
importance of these simulation environments cannot be
overstated. They serve as a testing ground for theoretical models, allowing researchers to observe the practical implications of their intelligent algorithms. Additionally, these platforms help in identifying potential issues and refining strategies before deployment in actual
scenarios, ensuring that the agents are well-prepared
to handle the complexities of real-world environments.
In Table 2, a wide range of simulated environments is
listed. Next, we will delve into these environments one
by one, emphasizing their significance and features for
future development.

In engineering [103, 146, 114, 115], LLMs-based
agents have been used to develop and optimize complex
systems, improving efficiency and performance.
• 1) Software Development: LLMs-based agents are
used for software development, facilitating sophisticated interactions and decision-making in a wide
range of contexts. These agents can assist in code
generation, bug detection, and system optimization, improving the productivity and quality of
software development. For example, they can generate code snippets based on natural language descriptions, detect bugs in code, and suggest optimizations to improve performance.

3.1. MARL-based Simulation Environments
This section provides an overview of several widelyused simulation environments designed for MARL.
These platforms, such as Multi-Agent Particle Environment [59, 136, 137], and PettingZoo [182], offer diverse
scenarios and functionalities for exploring cooperative
and competitive agent interactions in both simple and
complex tasks.

• 2) Multi-Robot Systems: LLMs-based multiagent systems are used to simulate complex realworld environments effectively, enabling interactions among diverse agents to solve various tasks.
These systems can coordinate the actions of multiple robots, optimizing their behavior to achieve
common goals. For example, they can be used
in search and rescue operations, where multiple
robots need to coordinate their actions to locate and
rescue victims.

Agent 0 Listener

of

Agent 0

Agent 1

“·· Red ··”

“·· Red ··”

Speaker
Agent 1
Agent 1

Listener

Communication

3. Simulation Environments
Decision-Making

Agent 0

“·· Green ··”

Agent 2

Spread

Reference

Multi-Agent
Figure 5: Typical Scenarios in Multi-Agent Particle Environment
(MPE).

First and foremost, the designs and implementations of multi-agent cooperative simulation environments are crucial in the historical research of multiagent decision-making, which are widely utilized in
practical applications and production. These simulation environments form the foundation for conducting
efficient and effective studies in multi-agent cooperative decision-making. Specifically, a dynamic multiagent cooperative decision-making environment refers
to predetermined scenarios and platforms where multiple agents collaborate to solve problems, complete
tasks, and achieve goals. Such environments provide
not only a platform for testing and validating various intelligent decision-making algorithms but also help

3.1.1. Several Widely-used Environments on MARL
Multi-Agent Particle Environment (MPE) [59,
136, 137] is a versatile and widely-used MARL platform designed for research in both cooperative and competitive settings. Developed by OpenAI, it is primarily
known for being the testing environment of the MADDPG algorithm [136]. MPE is a time-discrete, spacecontinuous 2D platform designed for evaluating MARL
algorithms.
Figure 5, initially derived from Malloy et al.
[137], illustrates various scenarios within the MultiAgent Particle Environment (MPE), including tasks
19

Table 2: Diverse MARL-based and LLMs-based Simulated Environments for Multi-Agent Systems.

Categories

Multi-Agent System Environments

MARL-based Multi-Agent Particle Environment (MPE) [59, 136, 137], PettingZoo [182],
SMAC [134], SMAC-v2 [135], GFootball [55], Gym-Microrts [183], MAgent
[184], Dexterous Hands [185, 186], OpenAI Gym [167], Gym-MiniGrid [187]4 ,
Melting Pot [188]5 , Capture The Flag6 [189], VillagerAgent [190], Minecraft
[191, 192, 193], Unity ML-Agents [194], SUMO7 [195], Hanabi Learning
[196, 197], Predator-Prey [198, 199]
LLMs-based TDW-MAT [147, 148], C-WAH [99], Cuisineworld [149], AgentScope [100], RoCoBench [200], Generative Agents [181, 201], SocialAI school [202, 203], Welfare Diplomacy [204]

such as adversarial interactions, cooperative crypto,
object pushing, and team-based world navigation.
Compatible with the widely-used Gym interface, it
supports a variety of tasks ranging from fully cooperative to mixed cooperative-competitive scenarios,
such
as
simple adversary, simple crypto,
simple spread, simple speaker listener,
and simple world comm8 . Each scenario highlights
distinct cooperative and competitive dynamics among
agents. MPE allows agents to interact and strategize
within a visually simplistic UI where particles represent
different entities. MPE is a open-source platform that
widely adopted in the multi-agent system research,
enabling extensive customization and contributing to its
role as a standard tool for studying complex multi-agent
dynamics.
Overall, MPE is a pivotal resource in the MARL
community, offering a well-rounded platform for experimentation and algorithm comparison. Its design and
functionality have made it an indispensable tool for researchers seeking to push the boundaries of what is possible in multi-agent systems.
StarCraft Multi-Agent Challenge (SMAC)9 [134]
is a widely-used benchmark for MARL that focuses
on decentralized micromanagement tasks in the popular real-time strategy game StarCraft II10 . In SMAC,
multiple agents control individual units and must learn
to cooperate and coordinate actions based on local,
partial observations. The agents face complex challenges, including coordinating combat techniques like
focus fire, kiting, and positioning, while the opponent

is controlled by the built-in StarCraft II AI. SMAC emphasizes problems such as partial observability, decentralized decision-making, and multi-agent credit assignment. The environment is structured to simulate realworld scenarios where agents must learn to collaborate
without full knowledge of the global state. Agents’
observations are restricted to a limited field of view,
forcing them to rely on local information for decisionmaking. As shown in Figure 6, these multi-agent cooperative decision-making environments are respectively
2s vs 3z, 5m vs 6m, 6h vs 8z, MMM2, where the inside numbers represent the number of units and the letters represent the unit types in general. In recent years,
SMAC has become a standard benchmark for evaluating
MARL algorithms, offering a rigorous and challenging
environment for advancing the field.

Figure 6: Several Typical Scenarios in StarCraft Multi-Agent Challenge (SMAC).

8 Multi-Agent
Particle Environment:
https://github.
com/openai/multiagent-particle-envs/tree/master/
multiagent/scenarios.
9 StarCraft Multi-Agent Challenge (SMAC): https://github.
com/oxwhirl/smac.
10 StarCraft II: https://starcraft2.blizzard.com/.

StarCraft Multi-Agent Challenge 2 (SMACv2)11
[134, 154, 91] However, SMAC [134] has limitations,
11 StarCraft

20

Multi-Agent Challenge 2 (SMACv2):

https://

experiments and multiplayer scenarios, enabling the exploration of more complex interactions and strategies.
GFootball supports various scenarios, including fullgame simulations with varying difficulty levels, as well
as simpler tasks in the Football Academy that focus on
specific skills like passing or scoring.

including insufficient stochasticity and partial observability, which allows agents to perform well with simple open-loop policies. To address these shortcomings, SMACv2 introduces procedural content generation (PCG), randomizing team compositions and agent
positions, ensuring agents face novel, diverse scenarios. Several multi-agent decision-making scenarios are
depicted in Figure 7, which are from Benjamin et al.
[135]. This requires more sophisticated, closed-loop
policies that condition on both ally and enemy information. Additionally, SMACv2 includes the Extended
Partial Observability Challenge (EPO), where enemy
observations are masked stochastically, forcing agents
to adapt to incomplete information and communicate
more effectively. SMACv2 thus represents a major evolution of the original benchmark, addressing key gaps
such as the lack of stochasticity and meaningful partial observability. These changes make SMACv2 a more
challenging environment, requiring agents to generalize
across varied settings and improve coordination, communication, and decentralized decision-making. Overall, SMACv2 provides a more rigorous testbed for advancing the field of cooperative MARL.

Midfield build-up

Attacking approach

Counterplay setup

Goal attempt

Figure 8:
Academy.

Typical examples of Training Scenarios in Football

Moreover, training agents for the ”Football Benchmark” can be quite challenging. To help researchers efficiently test and iterate on new ideas, researchers provide a toolset called ”Football Academy”, as illustrated
in Figure 8, which includes a series of scenarios with
varying levels of difficulty. These scenarios range from
simple setups, such as a single player scoring against
an open goal (e.g., approaching an open goal, scoring
in an open goal, or scoring while running), to more
complex team-based setups, where a controlled team
must break through specific defensive formations (e.g.,
scoring while running against a goalkeeper, passing and
shooting against a goalkeeper, and 3v1 against a goalkeeper). Additionally, the toolset covers common situations in football matches, such as corner kicks, simple counterattacks, and complex counterattacks. Lastly,
as an famous open-source GitHub project12 , it offers
a unique opportunity for researchers and pushes the
boundaries of AI research in a reproducible and scalable manner.
Unity Machine Learning-Agents Toolkit13 [194] is
an open-source platform designed to enable games and
simulations to serve as environments for training intelligent agents. Built on Unity’s powerful game engine, it supports a wide range of AI and machine learn-

Figure 7: Several scenarios from SMACv2 showing agents battling
the built-in AI.

Google Research Football Environment (GFootball) [55] is a state-of-the-art multi-agent simulation
environment developed by the Google Research Brain
Team. It is specifically designed for reinforcement
learning research and is built on top of the open-source
football game, GamePlay Football. GFootball is compatible with the OpenAI Gym API, making it a versatile tool not only for training intelligent agents but
also for allowing players to interact with the built-in AI
or trained agents using a keyboard or game controller.
GFootball features an advanced, physics-based 3D football simulator where agents can be trained to play football, offering a challenging yet highly customizable
platform for testing novel reinforcement learning algorithms and ideas. GFootball is tailored for multi-agent

12 Google
Research Football:
google-research/football.
13 Unity
ML-Agents Toolkit:
Unity-Technologies/ml-agents.

github.com/oxwhirl/smacv2.

21

https://github.com/
https://github.com/

Gym-Microrts14 [183] (pronounced ”Gym-microRTS”) is a fast and affordable reinforcement learning
(RL) platform designed to facilitate research in fullgame Real-Time Strategy (RTS) games. Unlike traditional RTS research that demands extensive computational resources, Gym-µRTS allows training advanced
agents using limited hardware, such as a single GPU
and CPU setup, within reasonable timeframes. Figure
10 showcases a match between our best-trained agent
(top-left) and CoacAI (bottom-right), the 2020 µRTS AI
competition champion. The agent employs an efficient
strategy, starting with resource harvesting and a worker
rush to damage the enemy base, transitioning into midgame combat unit production to secure victory.

ing methods, including reinforcement learning, imitation learning, and neuroevolution, through an intuitive
Python API. The platform includes state-of-the-art algorithm implementations (based on PyTorch), allowing
researchers and developers to train agents for 2D, 3D,
and VR/AR applications.

Figure 9: Typical Training Scenarios in Unity Machine LearningAgents Toolkit (released version: v0.11). From Left-to-right, up-todown: (a) Basic, (b) 3DBall, (c) Crawler, (d) Push Block, (e) Tennis,
(f) Worm, (g) Bouncer, (h) Grid World, (i) Walker, (j) Reacher, (k)
Food Collector, (l) Pyramids, (m) Wall Jump, (n) Hallway, (o) Soccer
Twos [194].
Figure 10: Screenshot of our best-trained agent (top-left) playing
against CoacAI (bottom-right), the 2020 µRTS AI competition champion [183].

ML-Agents is particularly useful for training NPC
behaviors in diverse scenarios, automated testing of
game builds, and evaluating game design decisions. It
features a highly flexible simulation environment with
realistic visuals, physics-driven interactions, and rich
task complexity. By integrating tools for creating custom environments and supporting multi-agent and adversarial settings, the toolkit bridges the gap between
AI research and practical applications in game development.
As seen from Figure 9, it depicts several typical
multi-agent environments from the previous work of
Juliani et al. [194]. The platform also provides key
components such as a Python API, Unity SDK, and
pre-built environments, enabling users to customize and
evaluate their algorithms in Unity’s interactive and visually rich settings. With its versatility and accessibility, Unity ML-Agents Toolkit has become an indispensable resource for both AI researchers and game developers, driving innovation in artificial intelligence and
simulation-based learning.

The platform offers a simplified RTS environment
that captures the core challenges of RTS games, including combinatorial action spaces, real-time decisionmaking, and partial observability. Gym-µRTS employs
a low-level action space, enabling fine-grained control
over individual units without AI assistance, which poses
unique challenges and opportunities for RL algorithms.
It supports Proximal Policy Optimization (PPO) and incorporates techniques like invalid action masking, action composition, and diverse training opponents to enhance training efficiency and agent performance.
Gym-µRTS has demonstrated its effectiveness by
producing state-of-the-art DRL agents capable of defeating top competition bots, such as CoacAI. The platform is open-source and provides all necessary tools for
14 Gym-Microrts:
gym-microrts.

22

https://github.com/kered9/

information. Through a flexible Python interface, researchers can easily customize the state space, action
space, and reward mechanisms, enabling the rapid creation of complex cooperative or competitive environments. MAgent comes with several built-in scenarios, such as pursuit, resource gathering, and team-based
battles, which highlight emergent social behaviors like
cooperative strategies, competitive dynamics, and resource monopolization.
MAgent is highly scalable, leveraging GPU-based
parallelism to simulate large-scale interactions efficiently. It also provides intuitive visualization tools
for real-time observation of agent behaviors, facilitating
analysis and debugging. Its flexibility and scalability
make MAgent a powerful tool for MARL research, enabling the study of large-scale agent interactions, emergent behaviors, and the dynamics of artificial societies.

researchers to explore and advance RL techniques in
RTS games, making it a valuable resource for both AI
researchers and gaming enthusiasts.
MAgent15 [184] is an open-source platform specifically designed to support large-scale MARL research,
with a focus on exploring Artificial Collective Intelligence (ACI). Unlike traditional MARL platforms, MAgent excels in handling scenarios involving hundreds to
millions of agents, making it ideal for studying complex interactions and emergent behaviors in large populations.
For instance, as shown in Figure 11, the ”Pursuit”
scenario is a classic example designed to showcase the
emergent cooperative behaviors of agents in a predatorprey environment. In this setup, predators work together
to capture preys while the preys attempt to evade capture. Each predator receives rewards for successfully
attacking a prey, while preys are penalized if caught.
Over time, predators learn to form collaborative strategies, such as surrounding and trapping preys, highlighting the emergence of local cooperation.

Adversarial Pursuit

Combined Arms

Gather

Tiger Deer

3.2. LLMs Reasoning-based Simulation Environments
LLMs-based multi-agent systems have become an essential tool for enhancing the collaboration, reasoning,
and decision-making capabilities of autonomous agents
[116]. By integrating LLMs with simulation platforms,
researchers can create complex test environments to explore the interactions of multi-agent systems in various
tasks and scenarios. These simulation environments not
only provide rich dynamic testing scenarios but also
promote the widespread application of LLMs in task
planning, coordination, and execution. The following
will introduce several widely used simulation platforms
for LLM multi-agent systems.
ThreeDWorld Multi-Agent Transport (TDWMAT)16 [147, 148] is a simulation environment for
multi-agent embodied task, which is extended from
the ThreeDWorld Transport Challenge [147] and is
designed for visually-guided task-and-motion planning
in physically realistic settings. It operates within
the ThreeDWorld (TDW) platform, which offers
high-fidelity sensory data, real-time physics-driven
interactions, and near-photorealistic rendering. In
TDW-MAT, embodied agents are tasked with transporting objects scattered throughout a simulated home
environment using containers, emphasizing the need for
coordination, physics awareness, and efficient planning.
For instance, in the common scenario shown in Figure
12, the agent must transport objects scattered across
multiple rooms and place them on the bed (marked with
a green bounding box) in the bedroom.

Figure 11: Illustrations of one of the typical running example in MAgent, called ”Pursuit” [184].

The platform is based on a grid-world model where
agents can perform actions such as moving, turning,
or attacking, while perceiving both local and global

16 ThreeDWorld Multi-Agent Transport: https://github.com/
threedworld-mit/tdw.

15 MAgent: https://github.com/geek-ai/MAgent.

23

and complete dish orders within a limited time frame.
The tasks range from simple preparations, like chopping
ingredients, to complex cooking processes that involve
multiple tools and steps. CuisineWorld is equipped with
a textual interface, and it supports various levels of difficulty, making it a flexible and robust testbed for assessing the planning and scheduling capabilities of Large
Foundation Models (LFMs). The environment also introduces a ”Collaboration Score” (CoS) metric to measure the efficiency of agent coordination as task demands increase, providing a comprehensive benchmark
for multi-agent system performance in dynamic and cooperative settings.

Figure 12: An overview of one of the example task in ThreeDWorld
Transport Challenge [147, 148].

Communicative Watch-And-Help (C-WAH)17 [99]
is a realistic multi-agent simulation environment and an
extension of the Watch-And-Help Challenge platform,
VirtualHome-Social [98]. C-WAH places a greater emphasis on cooperation and enhances communication between agents compared to VirtualHome-Social. Built
on the VirtualHome-Social, C-WAH simulates common
household activities where agents must collaborate to
complete tasks such as preparing meals, washing dishes,
and setting up a dinner table. As shown in Figure 13,
C-WAH supports both symbolic and visual observation
modes, allowing agents to perceive their surroundings
either through detailed object information or egocentric
RGB and depth images.

Figure 14: An typical multi-agent cooperative scenario in the
CuisineWorld platform [149].

AgentScope19 [100] is a innovative, robust and flexible multi-agent platform designed to empower developers in building advanced multi-agent systems by leveraging the potential of LLMs. At its core, the platform employs a process-based message exchange mechanism, simplifying the complexities of agent communication and collaboration. This approach ensures smooth
and efficient agent interaction, enabling developers to
focus on designing workflows rather than low-level
details. The platform stands out for its comprehensive fault-tolerance infrastructure, which includes retry
mechanisms, rule-based corrections, and customizable
error-handling configurations. AgentScope also excels
in multi-modal support, seamlessly integrating text, images, audio, and video into its workflows. By decoupling data storage and transfer, it optimizes memory usage and enhances scalability, making it ideal for applications requiring rich multi-modal interactions. Additionally, its actor-based distributed framework enables
efficient parallel execution and supports hybrid deployments, bridging the gap between local and distributed

Figure 13: An typical object-moving task leveraging LLMs-based embodied agents within the Communicative Watch-And-Help [99].

Cuisineworld18 [149] is a virtual kitchen environment designed to evaluate and enhance multi-agent
collaboration and coordination (i.e., the working efficiency) in a gaming context. As shown in Figure 14, in
this scenario, multiple agents work together to prepare
17 Communicative Watch-And-Help:
https://github.com/
xavierpuigf/watch_and_help.
18 Cuisineworld: https://mindagent.github.io/.

19 AgentScope:
agentscope.

24

https://github.com/modelscope/

of inter-robot communication. RoCoBench serves as a
comprehensive platform for evaluating the potential of
LLMs in driving dialectic multi-robot collaboration, offering a scalable and flexible environment for developers and researchers alike

environments with ease.

Figure 15: The official multi-modal interaction Web UI page between
agents in the AgentScope platform [100].
Figure 16: An overview of RoCoBench, a collection of six multirobot collaboration tasks set in a tabletop manipulation environment.
The scenarios encompass a diverse range of collaborative challenges,
each demanding distinct communication and coordination strategies
between robots, incorporating familiar, intuitive objects designed to
align with the semantic understanding capabilities of LLMs [200].

Moreover, to improve user interaction with multiple
agents, AgentScope assigns distinct colors and icons
to each agent, as shown in Figure 15, providing clear
visual differentiation in both the terminal and web interface. Designed with user accessibility in mind,
AgentScope provides intuitive programming tools, including pipelines and message centers, which streamline the development process. Its interactive user interfaces, both terminal- and web-based, allow developers
to monitor performance, track costs, and engage with
agents effectively. These features position AgentScope
as a state-of-the-art platform for advancing multi-agent
systems, combining ease of use with cutting-edge technology.
RoCoBench20 RoCoBench is a benchmark platform,
proposed by Mandi et al. [200], designed to evaluate and enhance the collaborative capabilities of multirobot systems powered by LLMs. Built as an extension to the RoCo project21 , RoCoBench provides a realistic simulation environment where robotic agents interact and collaborate to complete complex tasks, as
shown in Figure 16, such as sorting packages, assembling components, or preparing a workspace. RoCoBench places a strong emphasis on communicationdriven collaboration, integrating both symbolic and visual interaction modes to enable robots to perceive and
respond to their environment effectively. Each robot
is equipped with LLMs-powered reasoning, facilitating
real-time dialogue and coordination. Correspondingly,
the benchmark introduces a ”Collaboration Efficiency
Metric” (CEM) to evaluate the effectiveness of multirobot teamwork, taking into account factors like task
completion time, resource allocation, and the quality

Generative Agents22 Park et al. [181, 201] introduces Generative Agents, a groundbreaking framework
for simulating human behavior in interactive virtual
worlds. These agents exhibit realistic individual and
collective behaviors by incorporating dynamic memory, self-reflection, and action planning capabilities.
The system leverages LLMs to store, retrieve, and synthesize memories into higher-level reasoning, enabling
agents to adapt their actions based on personal experiences and evolving environmental changes. As illustrated in Figure 16, they present an interactive sandbox environment called Smallville, akin to ”The Sims,”
where 25 distinct virtual agents live, interact, and carry
out daily activities. Each agent has a detailed initial profile, including personal traits, relationships, and goals,
stored as ”seed memories.” Agents engage in natural
language-based dialogues and demonstrate social behaviors such as hosting events, making new acquaintances, and responding to user interventions. Generative Agents enable interactive applications in fields such
as simulating realistic social dynamics for games and
training simulations; designing dynamic, non-scripted
virtual worlds for interactive systems; and exploring
theories and behaviors in a controlled yet realistic virtual setting. The evaluations revealed the critical role of
memory retrieval, self-reflection, and action planning
in achieving coherent agent behaviors. Common issues,

20 RoCoBench: https://project-roco.github.io/.

22 Generative Agents:
https://youmingyeh.github.io/
cs-book/papers/generative-agents.

21 RoCo Project: https://project-roco.github.io/.

25

such as exaggerated responses and overly formal communication, were identified as areas for improvement.
Generative Agents push the boundaries of human behavior simulation, offering a robust framework for creating autonomous, memory-driven virtual agents.

Linguistic Action:
move forward.

Object; Color; State; ...
0132037300
Object; Color; State; ...
00103 53000

Saying: I Need Help !!!

Language
Modality

Warm!

SocialAI environment

Figure 18: A clear workflow of an agent acting in the SocialAI school
environment [202, 203].

Figure 17: Generative agents serve as realistic simulations of human
behavior, designed for interactive applications. In a sandbox environment inspired by The Sims, twenty-five agents engage in activities such as planning their routines, sharing updates, building relationships, and collaborating on group events, while allowing users to
observe and interact with them. [181, 201].

quest with investments in domestic welfare. Players accumulate Welfare Points (WPs) throughout the game by
prioritizing welfare over military expansion, and their
total utility at the end of the game is determined by these
points, removing the notion of a single ”winner”. Welfare Diplomacy enables clearer assessments of cooperation and provides stronger incentives for training cooperative AI agents. Players take on the roles of European powers, negotiating, forming alliances, and strategizing to compete for key supply centers. Orders are
submitted and resolved simultaneously, with the goal of
controlling a specified number of SCs to achieve victory, emphasizing a balance between cooperation and
betrayal. Based on these rules, Welfare Diplomacy implements themselves via an open-source platform, and
develops zero-shot baseline agents using advanced language models like GPT-4 [141, 140]. Experiments reveal that while these agents achieve high social welfare
through mutual demilitarization, they remain vulnerable
to exploitation, highlighting room for future improvement.

SocialAI school23 Kovač et al. [202, 203] introduces
The SocialAI School, a novel framework designed to
explore and develop socio-cognitive abilities in artificial
agents. The study emphasizes the importance of sociocognitive skills as foundational to human intelligence
and cultural evolution. As shown in Figure 18, the SocialAI School provides a customizable suite of procedurally generated environments that enable systematic research into the socio-cognitive abilities required for artificial agents to interact with and contribute to complex
cultures. Built on MiniGrid, it provides procedural environments for RL and LLMs-based agents to study social skills like joint attention, imitation, and scaffolding.
Open-source and versatile, it enables diverse research,
including generalizing social inferences, role reversal
studies, and scaffolded learning. The SocialAI School
represents a significant step toward enriching AI systems with socio-cognitive abilities inspired by human
development.
Welfare Diplomacy24 [204] is an innovative variant of the zero-sum game Diplomacy, designed to evaluate the cooperative capabilities of multi-agent systems. Unlike the original game, which focuses on a
single winner, Welfare Diplomacy introduces a generalsum framework where players balance military con-

In summary, these cutting-edge LLMs-powered simulation environments—ranging from task-specific platforms like TDW-MAT [148, 147] and CuisineWorld
[149] to socially-driven frameworks such as Generative
Agents [201] and the SocialAI School [203]—highlight
the transformative potential of integrating advanced AI
reasoning and multi-agent systems. By fostering research on collaboration, social cognition, and cooperative decision-making, these tools not only advance our
understanding of AI’s capabilities but also pave the way
for practical applications in dynamic, real-world scenarios.

23 SocialAI school project: https://sites.google.com/view/
socialai-school.
24 Welfare
Diplomacy:
https://github.com/mukobi/
welfare-diplomacy.

26

challenging terrain. Samad et al. [23] presents a
cloud-based multi-agent framework for efficiently managing aerial robots in disaster response scenarios, aiming to optimize rescue efforts by autonomously processing real-time sensory data to locate and assist injured
individuals.
In military confrontations, Qi et al. [90] designed a
distributed MARL framework based on the actor-worklearner architecture, addressing the issues of slow sample collection and low training efficiency in MARL
within the MaCA [89] and SMAC 3D realtime gaming [134, 135] military simulation environments. Benke
et al. [208] proposed a computational model for agent
decision-making that incorporates strategic deception,
enhancing the representation of deceptive behaviors
in multi-agent simulations for military operations research. Sutagundar et al. [209] proposed a Context
Aware Agent based Military Sensor Network (CAMSN)
to enhance multi-sensor image fusion, using node and
sink-driven contexts, forming an improved infrastructure for multi-sensor image fusion.
In efficient limited-bandwidth communication field,
Wang et al. [242] proposed a method called IMAC
(Informative Multi-Agent Communication) to address
the problem of limited-bandwidth communication in
MARL.
In the research of UAV swarm communications
against jamming, Lv et al. [93] proposed a MARLbased scheme to optimize relay selection and power
allocation. This strategy leverages network topology,
channel states, and shared experiences to improve policy exploration and stability, ultimately enhancing antijamming performance.
In UAV pursuit-evasion [20, 21, 22], Kouzeghar [88]
proposed a decentralized heterogeneous UAV swarm
approach for multi-target pursuit using MARL technique and introduced a variant of the MADDPG [132] to
address pursuit-evasion scenarios in non-stationary environments with random obstacles. Alexopoulos et al.
[212] addressed the challenge of pursuit-evasion games
involving two pursuing and one evading unmanned
aerial vehicle (UAV) by introducing a hierarchical decomposition of the game. Luo et al. [213] proposed
a cooperative maneuver decision-making method for
multi-UAV pursuit-evasion scenarios using an improved
MARL approach, which incorporates an enhanced
CommNet network with a communication mechanism
to address multi-agent coordination.
In large-scale traffic signal/flow control, Wang [214]
proposed a curiosity-inspired algorithm to optimize safe
and smooth traffic flow in autonomous vehicle on-ramp
merging; Chu et al. [92] proposed a fully scalable

Figure 19: The Balkans in the Diplomacy map in Welfare Diplomacy
[204].

4. Practice Applications of Multi-Agent DecisionMaking
Multi-agent cooperative decision-making has a wide
range of practical applications across various domains.
In this section, we delve into the practical applications
of multi-agent decision-making, focusing on how advanced methods, particularly multi-agent MARL, are
employed to address complex challenges in dynamic
and evolving environments. We explore the contributions of advanced multi-agent systems across domains such as agriculture, disaster rescue, military simulations, traffic management, autonomous driving, and
multi-robot collaboration. A broad array of applications
applications is presented in Table 3. In the following,
we will provide a detailed introduction to these applications, highlighting their impact and potential for future
advancements.
4.1. MARL-based Intelligent Applications
Recently, a variety of MARL methods have been
developed, fostering efficient collaboration, strategic
learning, and adaptive problem-solving in multi-agent
systems [83, 24, 25, 28]. Below, we highlight notable contributions that demonstrate the application of
MARL in enhancing multi-agent collaboration and performance
In smart precious agriculture and continuous pest
disease detection, Seewald et al. [11] addressed the
challenge of continuous exploration for multi-agent systems with battery constraints by integrating ergodic
search methods with energy-aware coverage. In disaster rescue, Qazzaz et al. [12] proposed a novel technique using a reinforcement learning multi Q-learning
algorithm to optimize UAV connectivity operations in
27

Table 3: Categorized Applications of MARL and LLMsin Diverse Domains.
Category

Application Area

Works / References

Smart Precious Agriculture

Seewald et al. [11], Qazzaz et al. [12], Samad et al. [23], Boubin et al. [205],

& Disaster Rescue

Li et al. [206], Mahajan et al. [207]

Military Confrontations

Qi et al. [90], Benke et al. [208], Sutagundar et al. [209], Vangaru et al. [210],
Wang et al. [211], MaCA [89], SMAC [134], SMAC-v2 [135]

MARLbased MAS

UAV Pursuit-Evasion
Kouzeghar [88], Alexopoulos et al. [212], Luo et al. [213], Lv et al. [93],
& Swarm Communications
Xue et al. [20], Rezwan et al. [21], Baroomi et al. [22]
& Navigation
Wang [214], Chu et al. [92], Aboueleneen et al. [215], Yu et al. [216],
Traffic Signal/Flow Control

Sun et al. [217], Azfar et al. [218], Bokade et al. [219], Kwesiga et al. [220],
Zhang et al. [221]
Xue et al. [222], Liu et al. [19], Wen et al. [18], Jayawardana et al. [17],

Autonomous Driving

Liu et al. [223], Formanek et al. [224], Zhang et al. [225], Kotoku et al. [226],
Hua et al. [227]

Multiple Robots Collaborative

Georgios et al. [13], Silva et al. [14], Huang et al. [15], Cena et al. (SMART)
[16], Kevin (SCRIMMAGE) [228], Liu et al. [95]

Multi-Agent Collaboration

Wu et al. (AutoGen) [103], Xiao et al. (CoE) [101], Chen et al. (AgentVerse) [105], Liu et al. (DyLAN) [106], Zhang et al. (CoELA) [96]

LLMsbased MAS

Xu et al. (LLM-Werewolf) [58], Gong et al. (MindAgent) [149], Xie et al.
Gaming Interaction

[229], Lin et al. [230], Jia et al. (GameFi) [231], Yin et al. (MIRAGE)
[232], Zhang et al. (DVM) [233], Bonorino et al. [234]
Zheng et al. (PlanAgent) [113], Luo et al. (SenseRAG) [235], Mahmud et al. [236],

Autonomous Driving

Peng et al. (LearningFlow) [237], Karagounis et al. [238], Luo et al. [239],
Gao et al. [240], Hegde et al. [241]

Multi-Modal Application

Wang et al. (LangGraph) [111, 112], Zhang et al. (CrewAI) [114, 115],
Zheng et al. (PlanAgent) [113], Wang et al. (MLLM-Tool) [102]

and decentralized multi-agent deep reinforcement learning algorithm based on the advantage actor-critic (A2C)
method.

Constrained Policy Optimization (PCPO) based on the
actor-critic architecture to address the issues of unexplainable behaviors and lack of safety guarantees in autonomous driving. Jayawardana et al. [17] proposed enabling socially compatible driving by leveraging human
driving data to learn a social preference model, integrating it with reinforcement learning-based AV policy synthesis using Social Value Orientation theory.

In autonomous driving area, a large number of superior multi-agent decision-making algorithms and models are continuously being explored and devised. Xue
et al. [222] developed a two-stage system framework
for improving Multi-Agent Autonomous Driving Systems (MADS) by enabling agents to recognize and
understand the Social Value Orientations (SVOs) of
other agents. Liu et al. [19] proposed the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to
model diverse interactions in highly interactive scenarios, addressing the issue of diverse driving styles
in autonomous driving. Wen et al. [18] proposed a
safe reinforcement learning algorithm called Parallel

In multiple robots collaborative fields, Georgios et al.
[13] introduces a novel cognitive architecture for largescale multi-agent Learning from Demonstration (LfD),
leveraging Federated Learning (FL) to enable scalable,
collaborative, and AI-driven robotic systems in complex environments. Silva et al. [14] address the challenges and limitations in evaluating intelligent collaborative robots for Industry 4.0. The review emphasizes
the urgent need for improved evaluation methods and
28

guided simulator into the MARL pipeline, enabling
agents to learn grounded reasoning through simulated
experiences. LBI consists of a world model composed
of a dynamics model and a reward model. The dynamics model incorporates a vector quantized variational
autoencoder (VQ-VAE) [243] for discrete image representation and a causal transformer to autoregressively
generate interaction transitions. Meanwhile, the reward model employs a bidirectional transformer trained
on expert demonstrations to provide task-specific rewards based on natural language descriptions. LBI further distinguishes itself by generating explainable interaction sequences and reward functions, providing interpretable solutions for multi-agent decision-making
problems. By addressing challenges such as the compositional complexity of MARL environments and the
scarcity of paired text-image datasets, LBI represents a
significant advancement in the field.
Ye et al. [244] proposed an adaptive genetic algorithm (AGA) that dynamically adjusts crossover and
mutation populations, leveraging the Dubins car model
and state-transition strategies to optimize the allocation
of heterogeneous UAVs in suppression of enemy air
defense missions. Radac et al. combine two modelfree controller tuning techniques linear virtual reference
feedback tuning (VRFT) and nonlinear state-feedback
Q-learning as a novel mixed VRFT-Q learning method
[94]. VRFT is initially employed to determine a stabilizing feedback controller using input-output experimental data within a model reference tracking framework. Subsequently, reinforcement Q-learning is applied in the same framework, utilizing input-state experimental data gathered under perturbed VRFT to ensure
effective exploration. Extensive simulations on position
control of a two-degrees-of-motion open-loop stable
multi input-multi output (MIMO) aerodynamic system
(AS) demonstrates the mixed VRFT-Q’s significant performance improvement over the Q-learning controllers
and the VRFT controllers.
To address the lack of a general metric for quantifying
policy differences in MARL problems, Hu et al. [84]
proposed the Multi-Agent Policy Distance (MAPD),
a tool designed to measure policy differences among
agents. Additionally, they developed a Multi-Agent
Dynamic Parameter Sharing (MADPS) algorithm based
on MAPD, demonstrating its effectiveness in enhancing
policy diversity and overall performance through extensive experiments. To addresses the challenge of cooperative MARL in scenarios with dynamic team compositions, Wang et al. [245] propose using mutual information as an augmented reward to encourage robustness
in agent policies across different team configurations.

standards to account for the complexities posed by human variability, AI integration, and advanced control
systems in industrial environments. Huang et al. [15]
presents a multi-agent reinforcement learning approach
using the MADDPG algorithm, enhanced with an experience sample optimizer, to train swarm robots for
autonomous, collaborative exploration on Mars. This
approach outperforms traditional DRL algorithms in efficiency as the number of robots and targets increases.
The SMART multi-agent robotic system [16] is a comprehensive and advanced platform designed for executing coordinated robotic tasks. It integrates both hardware components, such as robots and IP-Cameras, and
software agents responsible for image processing, path
planning, communication, and decision-making. By
utilizing Work-Flow Petri Nets for modeling and control, the system effectively ensures coordination and
successful task execution even in unstructured environments.
Furthermore, the well-known project, Simulating
Collaborative Robots in a Massive Multi-agent Game
Environment (SCRIMMAGE)25 [228], tackles the high
costs of field testing robotic systems by offering a
flexible and efficient simulation environment specifically designed for mobile robotics research. Unlike
many existing simulators that are primarily designed
for ground-based systems with high-fidelity multi-body
physics models, SCRIMMAGE focuses on simulating
large numbers of aerial vehicles, where precise collision
detection and complex physics are often unnecessary.
SCRIMMAGE is designed to be highly adaptable, with
a plugin-based architecture that supports various levels
of sensor fidelity, motion models, and network configurations. This flexibility allows the simulation of hundreds of aircraft with low-fidelity models or a smaller
number with high-fidelity models on standard consumer
hardware. Overall, SCRIMMAGE26 provides a robust
and scalable solution for testing and refining robotic
algorithms in a controlled virtual environment, significantly reducing the risks and costs associated with physical testing.
Liu et al. [95] proposed the Learning before Interaction (LBI) framework, a novel approach designed to
enhance multi-agent decision-making through generative world models. Traditional generative models struggle with trial-and-error reasoning, often failing to produce reliable solutions for complex multi-agent tasks.
To address this limitation, LBI integrates a language25 SCRIMMAGE Web: http://www.scrimmagesim.org/.
26 SCRIMMAGE
scrimmage.

project:

https://github.com/gtri/

29

demonstrating its superior performance and scalability
in large-scale multi-agent scenarios, such as decentralized collective assault simulations. This research represents a significant advancement in the field, providing a
scalable solution for effective decision-making in largescale multi-agent environments.
In conclusion, MARL-based intelligent applications
have shown exceptional adaptability across diverse domains such as autonomous driving, UAV systems, disaster response, and collaborative robotics [242, 88, 20,
19, 13, 16]. Key innovations, including communicationenhanced learning [168, 161, 174, 139], adaptive policy optimization, and mutual information [246] frameworks, have significantly advanced the field. While
challenges like sparse rewards and scalability remain,
these advancements highlight MARL’s potential to address dynamic and complex multi-agent environments
effectively, paving the way for further impactful developments.

They develop a multi-agent policy iteration algorithm
with a fixed marginal distribution and demonstrate its
strong zero-shot generalization to dynamic team compositions in complex cooperative tasks. Progressive
Mutual Information Collaboration (PMIC)27 is a novel
framework that leverages mutual information (MI) to
guide collaboration among agents, thereby enhancing
performance in mult-agent cooperative tasks [246]. The
key innovation of is its dual MI objectives: maximizing
MI associated with superior collaborative behaviors and
minimizing MI linked to inferior ones, ensuring more
effective learning and avoiding sub-optimal collaborations. Wai et al. [83] proposes a novel double averaging
primal-dual optimization algorithm for MARL, specifically targeting decentralized applications like sensor
networks and swarm robotics. The MARL algorithm
enables agents to collaboratively evaluate policies by
incorporating neighboring gradient and local reward
information, achieving fast finite-time convergence to
the optimal solution in decentralized convex-concave
saddle-point problems. To address the challenge of
sparse rewards in MARL, Kang et al. [247] introduce
the Dual Preferences-based Multi-Agent Reinforcement
Learning (DPM) framework. DPM extends preferencebased reinforcement learning (PbRL) by incorporating
dual preference types-comparing both trajectories and
individual agent contributions-thereby optimizing individual reward functions more effectively. DPM also
leverages LLMs to gather preferences, mitigating issues associated with human-based preference collection. Experimental results in the StarCraft Multi-Agent
Challenge (SMAC) [135] demonstrate that DPM significantly outperforms existing baselines, particularly in
sparse reward settings.
Traditional methods like soft attention struggle with
scalability and efficiency in LMAS due to the overwhelming number of agent interactions and large observation spaces. To address these challenges of largescale multi-agent systems (LMAS) involving hundreds
of agents, University of Chinese Academy of Sciences
[248] introduces the Concentration Network (ConcNet),
a novel reinforcement learning framework. ConcNet
mimics human cognitive processes of concentration by
prioritizing and aggregating observations based on motivational indices, such as expected survival time and
state value. It allows the system to focus on the most relevant entities, enhancing decision-making efficiency in
complex environments. In ConcNet, a novel concentration policy gradient architecture was further proposed,

4.2. LLMs reasoning-based Intelligent Applications
To address diverse and complex challenges, a variety
of frameworks leveraging LLMs have been developed,
enabling advanced reasoning, collaboration, and task
execution in multi-agent systems [116, 30, 57]. Below,
we highlight notable contributions that demonstrate the
application of LLMs in enhancing multi-agent decisionmaking and coordination.
Wu et al. [103] introduced AutoGen, an open-source
framework designed to enable the development of nextgeneration LLM applications through multi-agent conversations. AutoGen allows for customizable agent interactions and the integration of LLMs, human inputs,
and tools to collaboratively solve complex tasks. Xiao
et al. [101] proposed Chain-of-Experts (CoE), a novel
multi-agent framework designed to enhance reasoning
in complex operations research (OR) problems using
LLMs. Chen et al. [105] presented AgentVerse, a multiagent framework designed to facilitate collaboration
among autonomous agents, inspired by human group
dynamics. AgentVerse dynamically adjusts the composition and roles of agents throughout the problemsolving process, enhancing their ability to tackle complex tasks across various domains, including text understanding, reasoning, coding, and embodied AI. The
framework consists of four stages: Expert Recruitment,
Collaborative Decision-Making, Action Execution, and
Evaluation. Chen et al. [107] introduced AutoAgents,
a framework capable of adaptively generating and coordinating multiple specialized agents based on different
tasks, thereby constructing efficient multi-agent teams

27 PMIC code: https://github.com/yeshenpy/PMIC.

30

”Cuisineworld”30 , a new gaming scenario and benchmark for assessing multi-agent collaboration efficiency.
Despite LLMs’ success in various collaborative tasks,
they struggle with spatial and decentralized decisionmaking required for flocking. Li et al. [57] explored
the challenges faced by LLMs in solving multi-agent
flocking tasks, where agents strive to stay close, avoid
collisions, and maintain a formation. Sun et al. [180]
proposed Corex, a novel framework that enhances complex reasoning by leveraging multi-model collaboration. Inspired by human cognitive processes, Corex employs three collaborative paradigms-Discuss, Review,
and Retrieve-where different LLMs act as autonomous
agents to collectively solve complex tasks. Corex empowers LLM agents to ”think outside the box” by facilitating collaborative group discussions, effectively mitigating the cognitive biases inherent in individual LLMs.
This approach not only enhances performance but also
improves cost-effectiveness and annotation efficiency,
offering a significant advantage in complex tasks.
Next, we will provide a detailed introduction to some
outstanding achievements in the application of LLMs
for multi-agent collaborative task execution.
MetaGPT: Existing LLMs-based multi-agent systems often struggle with complex tasks due to logical
inconsistencies and cumulative hallucinations, leading
to biased results. Hong et al. [108, 109] from DeepWisdom31 proposed MetaGPT32 , an innovative metaprogramming framework designed to enhance the collaboration capabilities of LLMs-based multi-agent systems. MetaGPT integrates Standard Operating Procedures (SOPs) commonly used in human workflows,
thereby constructing a more efficient and coherent
multi-agent collaboration system. MetaGPT employs
an assembly-line approach, breaking down complex
tasks into multiple subtasks and assigning them to
agents with specific domain expertise. These agents
collaborate during task execution through clearly defined roles and structured communication interfaces, reducing the risk of information distortion and misunderstanding. In summary, MetaGPT offers a flexible and
powerful platform for developing LLMs-based multiagent systems. Its unique meta-programming framework and rigorous workflow design enable it to excel
in handling complex tasks, greatly advancing the field
of multi-agent collaboration research.
CoAct: Hou et al. [97] proposed CoAct33 , a multi-

to accomplish complex tasks. Liu et al. [106] proposed the Dynamic LLM-Agent Network (DyLAN), a
framework designed to enhance LLM-agent collaboration by enabling agents to interact dynamically based on
task requirements, rather than within a static architecture. Xu et al. [58] proposed a novel multi-agent framework that combines LLMs with reinforcement learning to enhance strategic decision-making and communication in the Werewolf game28 , effectively overcoming intrinsic biases and achieving human-level performance. Wen et al. [104] introduce the Multi-Agent
Transformer (MAT), a novel architecture that frames cooperative MARL as a sequence modeling problem. Experiments on StarCraftII [154, 91, 134], Multi-Agent
MuJoCo (MAMuJoCo) [249], Dexterous Hands Manipulation [185, 186], and Google Research Football
[55] benchmarks demonstrate that it achieves superior
performance and data efficiency by leveraging modern sequence models in an on-policy learning framework. Wang et al. [102] introduced MLLM-Tool29 ,
a multimodal tool agent system that integrates opensource LLMs with multimodal encoders, enabling it to
process visual and auditory inputs for selecting appropriate tools based on ambiguous multimodal instructions. Moreover, they introduced ToolMMBench, a
novel benchmark with multi-modal inputs and multioption solutions, demonstrating its effectiveness in addressing real-world multimodal multi-agent scenarios.
Zhang et al. [96] introduce CoELA, a Cooperative
Embodied Language Agent framework that leverages
LLMs to enhance multi-agent cooperation in complex,
decentralized environments. CoELA integrates LLMs
with cognitive-inspired modules for perception, memory and execution, allowing agents to plan, communicate, and collaborate effectively on long-horizon tasks,
outperforming traditional planning-based methods such
as Multi-Agent Transformer(MAT) [104], and showing promising results in human-agent interaction simulation environments, Communicative Watch-And-Help
(C-WAH) [99] and ThreeDWorld Multi-Agent Transport (TDW-MAT) [147, 148]. Gong et al. [149] from
Team of Li.FeiFei. introduce MindAgent, a novel infrastructure for evaluating planning and coordination
capabilities in gaming interactions, leveraging large
foundation models (LFMs) to coordinate multi-agent
system (MAS), collaborate with human players, and
enable in-context learning. Their team also present

30 Cuisineworld: https://mindagent.github.io/.

28 Werewolf
game:
https://sites.google.com/view/
strategic-language-agents/.
29 MLLM-Tool:
https://github.com/MLLM-Tool/
MLLM-Tool.

31 DeepWisdom: https://www.deepwisdom.ai/.
32 MetaGPT: https://github.com/geekan/MetaGPT.
33 CoAct: https://github.com/dxhou/CoAct.

31

agent system based on LLMs designed for hierarchical collaboration tasks. The framework consists of six
stages: task decomposition, subtask assignment and
communication, subtask analysis and execution, feedback collection, progress evaluation, and replanning
when necessary. The global planning agent plays a critical role in managing complex tasks. The local execution agent is responsible for executing specific subtasks.
This hierarchical framework demonstrates strong adaptability and performance, particularly in complex realworld tasks requiring dynamic replanning and collaborative execution.
AutoGen: Microsoft [103, 146] introduced AutoGen34 , a flexible framework for creating and managing multiple autonomous agents to collaboratively complete complex tasks, particularly in programming, planning, and creative writing domains. AutoGen allows
users to define distinct agent roles, including specialists, general assistants, and decision-makers, ensuring
clear task division and effective coordination. Agents
interact in a structured conversational environment, exchanging messages to resolve tasks iteratively. AutoGen introduces feedback loops where agents analyze
outputs, refine strategies, and optimize task completion autonomously. Notably, it supports integration with
various LLMs, offering developers the flexibility to replace APIs without altering code significantly. In summary, AutoGen facilitates scalable, efficient, and robust
multi-agent collaboration, demonstrating potential for
applications ranging from enhanced ChatGPT systems
to real-world industrial workflows.
XAgent: XAgent Team35 [110] developed XAgent36
is an open-source, LLMs-driven autonomous agent
framework designed for solving complex tasks automatically and efficiently. As shown in Figure 20, it employs
a dual-loop architecture: the outer loop for high-level
task planning and coordination, and the inner loop for
executing subtasks. The PlanAgent in the outer loop
generates an initial plan by breaking a complex task
into manageable subtasks, organizing them into a task
queue. It iteratively monitors progress, optimizes plans
based on feedback from the inner loop, and continues
until all subtasks are completed. The inner loop utilizes ToolAgents, which employ various tools like file
editors, Python notebooks, web browsers, and shell interfaces within a secure docker environment to execute
subtasks. XAgent emphasizes autonomy, safety, and extensibility, allowing users to add new agents or tools

Figure 20: An structure overview of the XAgents framework, highlighting the Task Node as the starting point, the sequence of Sub-Task
Nodes forming the Task Execution Graph (TEG), and the Fusion Node
integrating outputs for the final result [110].

to enhance functionality. Its GUI facilitates user interaction while supporting human collaboration, enabling
real-time guidance and assistance for challenging tasks.
PlanAgent: PlanAgent37 , developed by Chinese
Academy of Sciences and Li Auto [113], introduces
a closed-loop motion planning framework for autonomous driving by leveraging multi-modal large language models (MLLMs). The system utilizes MLLM’s
multi-modal reasoning and commonsense understanding capabilities to generate hierarchical driving commands based on scene information. PlanAgent addresses key limitations of traditional rule-based and
learning-based methods, including overfitting in longtail scenarios and inefficiencies in scene representation.
Its novel integration of MLLM-driven reasoning into
autonomous driving planning establishes a new benchmark for safety and robustness in real-world applications.
LangGraph: LangChain Inc38 [111, 112] introduced
LangGraph39 , a library designed for building stateful,
multi-actor applications with LLMs, enabling the creation of complex agent and multi-agent workflows. Inspired by frameworks like Pregel and Apache Beam,
LangGraph provides fine-grained control over workflows and state management while offering advanced
features like persistence and human-in-the-loop capabilities. LangGraph stands out for its support of iterative workflows with cycles and branching, which
differentiates it from DAG-based frameworks. Each

34 AutoGen: https://github.com/microsoft/autogen.

37 PlanAgent: http://www.chinasem.cn/planagent.

35 XAgent Team: https://blog.x-agent.net/.

38 LangChain Inc: https://langchain.ac.cn/langgraph.

36 XAgent: https://github.com/OpenBMB/XAgent.

39 LangGraph: https://www.langchain.com/langgraph

32

graph execution generates a state, dynamically updated
by node outputs, enabling reliable and adaptive agent
behavior. Its built-in persistence allows workflows to
pause and resume at any point, facilitating error recovery and advanced human-agent interactions, including
”time travel” to modify past actions. LangGraph integrates seamlessly with LangChain [250, 179] but functions independently, offering flexibility for various applications, from dialogue agents and recommendation
systems to natural language processing and game development. With streaming support, it processes outputs in
real-time, making it suitable for tasks requiring immediate feedback. Its low-level architecture and customizable workflows make LangGraph a powerful tool for
creating robust, scalable, and interactive LLMs-based
systems.
Question

Task 0
Task 1
Agents can naturally
communicate with
each other to delegate
tasks or seek assistance.

Processes determine how
agents collaborate, assign
tasks, interact, and execute
their work.

Process
Tools

AI Agent 0

Tasks can specify which tools
should be used instead of
default ones and assign specific
agents to handle them.

AI Agent 1

Tools

AI Agents 2

Figure 22: An overview of the processing workflow for the roleplaying multi-agent framework, CrewAI [114, 115].

capabilities, such as OpenAI and Google Gemini. The
framework supports real-time decision-making and task
adaptation, with future versions planned to include more
advanced collaboration processes, such as consensusdriven workflows and autonomous decision-making. Its
innovative features, such as role-based design, dynamic
rule generation, and modular task workflows, position CrewAI as a robust and scalable framework for
multi-agent collaboration across creative and industrial
domains. Overall, CrewAI 41 offers a cutting-edge
approach to multi-agent systems by integrating rolespecific autonomy, flexible workflows, and advanced AI
toolsets, making it a versatile framework for collaborative AI applications.
In summary, these frameworks and applications [108,
97, 103, 110] highlight the rapid advancements in leveraging LLMs for multi-agent collaboration, reasoning,
and task execution. Each system introduces unique innovations—ranging from dynamic agent coordination
to enhanced reasoning and human-in-the-loop workflows—demonstrating their potential to tackle complex,
real-world challenges across various domains [113, 111,
114, 115]. These developments pave the way for more
flexible, scalable, and efficient AI-driven solutions.

{Question; Document;
Generation}

Any document
is not relevant?

Decide To Generate
(Conditional Edge)

Figure 21: A LangGraph workflow representation demonstrating conditional branching and iterative loops for document retrieval, grading,
query transformation, and web search before generating a final output
[111, 112].

CrewAI: CrewAI40 [114, 115] is an open-source
framework designed to coordinate AI agents that specialize in role-playing and autonomous operations, enabling efficient collaboration to achieve complex goals.
The framework’s modular design allows users to create
AI teams that operate like real-world teams, with agents
assigned specific roles and tasks to ensure clear division
of labor and shared objectives. As seen from 22, this
framework operates in three primary stages: Agent Creation, where developers define roles with specific goals
and tools; Task Management, enabling flexible task assignment and multi-view knowledge enhancement; and
Execution and Collaboration, where agents interact in
workflows to resolve tasks, with outputs parsed into
reusable formats. CrewAI integrates seamlessly with
the LangChain ecosystem, leveraging its tools and LLM

5. Challenges in MARL-based and LLMs-based approaches
The extension of single-agent decision-making into
multi-agent cooperative contexts introduces several
challenges, including developing effective training
41 CrewAI
Multi-Agent
System
platform:
//www.deeplearning.ai/short-courses/
multi-ai-agent-systems-with-crewai/.

40 CrewAI: https://github.com/crewAIInc/crewAI.

33

https:

2. Strategy Learning Difficulty: [252, 86, 116]
Strategy learning in MARL-based decisionmaking systems involves multidimensional
challenges. Firstly, agents must consider the behaviors of other agents, and this interdependence
increases the difficulty of strategy learning. Each
agent not only has to optimize its own strategy but
also predict and adapt to the strategy changes of
others. Additionally, the vast joint action space
of multiple agents makes it challenging for any
single agent to learn effective joint strategies.
The vast joint action space means that each agent
needs to explore and learn within a larger decision
space, which significantly increases the demands
on computational resources and time;
3. Complexity of Reward Functions: [251] In
MARL-based decision-making systems, reward
functions become more complex [85, 150]. The
rewards received from the environment in multiagent cooperative techniques are influenced not
only by an individual agent’s actions and the environment but also by the actions of other agents,
which makes the stable policy learning and modeling process more difficult. In other words, an
agent’s reward depends not only on its own actions but also on the actions of other agents, making it challenging for the reinforcement learningbased multi-agent decision-making policies to converge. This intricate reward mechanism complicates the design and optimization of reward functions. Agents need to evaluate their behaviors’ impact on the overall system through complex interactions to learn effective strategies;
4. Coordination and Cooperation: [253, 90, 254,
251] Furthermore, in MARL-based decisionmaking systems, agents need to coordinate and cooperate to achieve common goals. This requires
establishing effective communication and coordination mechanisms among agents to ensure that
their actions are globally consistent and complementary [49]. For example, in disaster rescue scenarios [57, 12], multiple drones need to coordinate
their actions to cover the maximum area and utilize
resources most efficiently;
5. Non-Stationary: [27, 25] The environment of
MARL-based decision-making systems is nonstationary because each agent’s behavior dynamically changes the state of the environment. This
non-stationarity increases the difficulty of strategy learning, as agents must continually adapt to
changes in the environment and the behaviors of
other agents.

schemes for multiple agents learning and adapting simultaneously, managing increased computational complexity due to the more sophisticated and stochastic environments compared to single-agent settings, and addressing the foundational role of strategic interaction
among agents. Additionally, ensuring the scalability
of algorithms to handle larger observation and action
spaces, facilitating coordination and cooperation among
agents to achieve consistent goals, and dealing with
non-stationary environments where agents’ behaviors
and strategies continuously evolve are also inevitable
and critical challenges.
Applying multi-agent decision-making techniques to
real-world problems, which often involve complex and
dynamic interactions, further underscores the need for
sophisticated and advanced approaches to effectively
adapt these ever-increasing complexities. Multi-agent
cooperative decision making significantly surpasses
single-agent decision-making in terms of environmental stochasticity, complexity, the difficulty of strategy
optimization, and so on. As shown in Figure 23, we
present a tree diagram summarizing the existing challenges in MARL-based and LLMs-based multi-agent
decision-making approaches.
5.1. Challenges in MARL-based multi-agent systems
The advancement of MARL remains in its formative
stages, with its potential for enabling effective multiagent coordination and achieving scalability in dynamic
environments yet to be fully unlocked [7, 6, 8, 25, 28].
Challenges such as environmental stochasticity, strategy learning difficulty, non-stationarity, scalability, and
reward complexity have emerged as major bottlenecks.
This section provides an in-depth analysis of these challenges, exploring the current state, technical limitations, and potential solutions in MARL-based multiagent systems to enable more robust, efficient, and scalable decision-making frameworks.
1. Environmental Stochasticity and Complexity:
[251, 25] In MARL-based decision-making systems, environmental dynamics are influenced not
only by external factors but also by the behaviors and decisions of individual agents. This complex interaction results in high levels of stochasticity and complexity in the environment, making
prediction and modeling significantly more difficult. For example, in autonomous driving scenarios [19, 214, 18, 222], the behavior of each vehicle affects the decisions of surrounding vehicles,
thereby increasing the overall complexity of the
system;
34

Figure 23: A tree diagram of the challenges in MARL and LLMs-based multi-agent decision-making approaches.

6. Scalability: [87, 252, 25] Addressing scalability
in MARL demands innovative approaches to tackle
the exponential growth in complexity as the number of agents increases. Techniques that leverage
knowledge reuse [172, 138, 168], such as parameter sharing and transfer learning [255, 256], have
proven indispensable. When agents share structural similarities, information sharing can streamline the training process, enabling systems to scale
more effectively. Transfer learning, in particular,
allows agents to adapt knowledge from previous
tasks to new, related ones, significantly accelerating learning in dynamic environments. Moreover, curriculum learning [257, 258] plays a pivotal role in tackling the increased complexity of
training multiple agents. It enables gradual learning by exposing agents to progressively more challenging tasks, thereby improving policy generalization and accelerating convergence. Robustness
remains critical for scalability, as learned policies
must withstand environmental disturbances. Techniques like policy ensembles and adversarial training [259, 260] enhance resilience by fostering diversity and adaptability in policies. The DTDE
paradigm addresses these issues but introduces
new complexities [124, 131], such as environmental instability. One promising solution is employing Independent Deep Q-networks (IDQNs)
[126, 261, 127], which adapt traditional singleagent approaches to multi-agent contexts.

ability in MARL is a key area for future exploration.
While existing techniques provide strong foundations,
integrating methods like meta-learning could offer a
way for agents to rapidly adapt to new tasks and environments. Additionally, leveraging recent advances in
graph neural networks might enhance the scalability of
MARL by modeling agent interactions more efficiently.
These directions hold promise for tackling the dynamic
and large-scale nature of multi-agent environments.
5.2. Challenges in LLMs reasoning-based multi-agent
systems
The development of LLMs-based multi-agent systems is still in its early stages, and its advantages in
multi-agent collaboration in cross-domain applications
have not been fully realized. In this process, technical bottlenecks, design limitations, and imperfect evaluation methods have revealed numerous challenges. This
section provides a comprehensive analysis of these challenges, exploring the current status, bottlenecks, and potential breakthrough directions of LLMs-based multiagent systems in key areas such as multi-modal interaction, system scalability, hallucination control, evaluation, and privacy protection.
1. Expansion of Multi-Modal Environments: [102,
113, 148] Current LLMs-based multi-agent systems primarily focus on text processing and generation, particularly excelling in language-based interactions. However, applications in multi-modal
environments remain insufficient. Multi-modal environments require agents to handle various in-

Overall, the interplay between robustness and scal35

puts from images, audio, video, and physical sensors, while also generating multi-modal outputs,
such as descriptions of visual scenes or simulations of physical actions. This cross-modal interaction not only demands stronger model processing capabilities but also requires efficient information fusion between agents. For example, in
practical applications, one agent may need to extract visual features from an image and collaborate
with other agents through language to accomplish
complex tasks, posing new technical challenges.
Future research should focus on building unified
multi-modal frameworks that enable agents to efficiently understand and collaboratively process various types of data.
2. Hallucination Problem: [262, 30, 108, 263] The
hallucination in LLMs, which involves generating false or inaccurate information, becomes more
complex in multi-agent environments. This issue
may be triggered within a single agent and further
propagated through multi-agent interactions, ultimately negatively impacting the decision-making
of the entire system. Because the information flow
in multi-agent systems is interconnected, any misjudgment at one node can trigger a chain reaction.
This characteristic makes the hallucination problem not only confined to the behavior of individual
agents but also poses challenges to the stability of
the entire system. Therefore, addressing this issue
requires a dual approach: on one hand, improving model training methods to reduce the probability of hallucinations in individual agents; on
the other hand, designing information verification
mechanisms and propagation management strategies to minimize the spread of hallucinated information within the agent network.
3. Acquisition of Collective Intelligence: [34, 176,
253] Current LLMs-based multi-agent systems
rely more on real-time feedback for learning rather
than offline data, unlike traditional multi-agent systems [264, 265]. This real-time learning approach
imposes higher requirements on the interactive environment [262, 266]. Since designing and maintaining a reliable real-time interactive environment
is not easy, it limits the scalability of the system. Additionally, existing research mostly focuses on optimizing individual agents, neglecting
the potential overall efficiency improvements that
could arise from agent collaboration. For example, knowledge sharing and behavioral coordination among agents may create advantages of collective intelligence in certain complex tasks. Fu-

ture research needs to explore how to fully leverage
the potential of collective intelligence by optimizing multi-agent interaction strategies and collaboration mechanisms.
4. System Scalability: [262, 267, 253] As the number of agents in LLMs-based multi-agent systems increases, the demand for computational resources grows exponentially, posing challenges in
resource-constrained environments. A single LLM
agent already requires substantial computational
power, and when the system scales to hundreds or
thousands of agents, existing hardware and software architectures may not be able to support it.
Furthermore, scaling the system introduces new
complexities, such as how to efficiently allocate
tasks, coordinate, and communicate among numerous agents. Studies have shown that the more
agents there are, the more difficult it becomes to
coordinate their operations, especially in reducing
redundancy and conflicts. Therefore, future work
needs to optimize resource utilization through the
development of lightweight models and efficient
communication protocols, while also exploring the
scaling laws for agent expansion.
5. Evaluation and Benchmarking: [262, 253] Current evaluation methods and benchmark tests for
LLMs-based multi-agent systems are still incomplete. Most research focuses solely on the performance of individual agents in specific tasks, neglecting the overall system performance in complex scenarios. Evaluating group behavior is more
challenging because the dynamics and complexity of multi-agent systems are difficult to measure
with a single metric. Additionally, the lack of
a unified testing framework and benchmark data
is a major obstacle when comparing the capabilities of different LLMs-based multi-agent systems
across domains. Future research needs to develop
comprehensive evaluation standards and universal
benchmark tests, especially in key fields such as
scientific experiments, economic analysis, and urban planning, to provide a basis for system performance comparison and improvement.
6. Interaction Efficiency and Cumulative Effects:
[262, 268, 105, 254] The complexity of multiagent systems leads to prominent issues of low interaction efficiency and cumulative effects. Low
interaction efficiency is mainly reflected in the
need for generative agents to frequently query
models, making the system inefficient in largescale applications. On the other hand, because the
system state highly depends on the results of the
36

previous round, when an error occurs in one round,
it may accumulate and propagate to subsequent operations, ultimately degrading the system’s overall performance. Future efforts should focus on
designing more efficient communication protocols
and intermediate result correction mechanisms to
reduce interaction costs and the impact of cumulative errors.
7. Security and Privacy Issues: [5, 118, 269, 270,
271] Context sharing within multi-agent systems
poses risks of introducing noise and privacy leaks.
For example, sensitive information shared between
agents (such as identities or locations) may be
misused by untrusted nodes, thereby threatening
the system’s security. Addressing this issue requires a two-pronged approach: first, establishing
clear organizational structures to restrict information access permissions; second, introducing more
advanced trust management mechanisms, such as
distributed trust systems based on consensus algorithms, to enhance the system’s security and reliability.
In summary, LLMs-based multi-agent systems face
challenges across multiple domains, including multimodal adaptation, scalability, evaluation methods, collective intelligence development, and privacy protection. These challenges not only reveal the current technological limitations but also provide ample space for
future research. With advancements in technology and
the deepening of interdisciplinary studies, LLMs-based
multi-agent systems are expected to achieve significant
breakthroughs both theoretically and in applications.

understanding and reasoning [14, 253]. Traditional
MARL requires agents to learn control strategies in dynamic environments with limited data [8, 5, 27, 24].
However, this approach often faces challenges like low
sample efficiency, difficult reward design, and poor generalization. LLMs, with their strong reasoning and
knowledge representation capabilities, offer solutions
[272, 30]. For example, they can process multi-modal
information such as natural language and vision [113,
273, 102, 260], helping agents understand tasks and
environments more effectively. This improves learning speed and generalization. Furthermore, LLMs can
act as reasoning tools, providing additional context and
knowledge to optimize long-term planning.
The LLMs-enhanced MARL framework is a groundbreaking integration of LLMs and MARL techniques,
which includes roles such as information processor, reward designer, decision-maker, and generator [116].
Figure 24 presents a flowchart illustrating the structure
of the LLMs-enhanced MARL framework, highlighting its four key roles. These roles work together to
streamline task complexity and improve learning. For
instance, LLMs can translate unstructured task descriptions into formal task semantics, reducing learning difficulty. They can also design advanced reward functions to accelerate learning in sparse-reward environments. These roles collectively address the challenges
of task complexity, data efficiency, and generalization
in MARL [29, 24, 251], while streamlining processes
like reward design and policy generation. As shown in
Table 4, we summarize recent advancements in LLMsenhanced MARL methods across these four roles into a
comprehensive table for clarity and comparison.

6. Future Research Prospects
and Emerging Trends

6.2. Technical Integration: From Multi-Modal to MultiTask Optimization

Multi-Agent Decision-Making Systems are entering a
new era where LLMs are combined with MARL [116].
This combination can improve learning efficiency in
complex dynamic environments. It also enables better
multi-modal information processing, multi-task collaboration, and long-term planning [7, 25, 30, 28, 266]. In
this section, we discuss future prospects and challenges
of multi-agent decision-making system (MAS) research
from theoretical, technical, application, and ethical perspectives.

Combining LLMs and MARL significantly improves
the ability to handle multi-modal information, multitask learning, and long-term task planning [273, 102,
272, 30]. Traditional MARL often requires separate
modules to process visual, textual, or other forms of
data. In contrast, LLMs can unify this processing, enabling comprehensive environment understanding. For
example, in a robot task involving voice commands and
visual inputs, LLMs can process both types of data simultaneously and generate actions directly. Additionally, LLMs provide a distinct advantage in multi-task
learning due to their pre-trained knowledge [30, 57].
Through knowledge transfer, they help agents share experiences across different tasks, improving adaptability
[255, 66]. For long-term planning, LLMs can break

6.1. Theoretical Development: From Traditional RL to
LLMs-Enhanced MARL Framework
LLMs-enhanced MARL redefines collaboration in
multi-agent systems by introducing natural language
37

(i) Feature Representation Extractor
Actor-Critic Network

Contrastive
Learning Finetuning

��

Frozen LLM
/ VLM

(ii) Language Translator

(i) Direct Decision-maker

(i) Implicit Reward Model
Prompting
LLM

Similarity
Alignment

Next Action ��+1

Task Specific Model

Language

Pre-trained LM
as General Scoffold

Directly
Alignment Visual
Reward
Prompting Designer Scoring

Goal /
Instruction

(ii) Explicit Reward Model

010110
101110

Task Language Translation

a) LLM as information processor.

Pre-trained LM

�������

LLM
LLM Function
Generation

LLM

argmax(Q
+v.logPllm)

Evaluation

Replay Buffer

Real
World

DataBase
Data

Simulat
-ed Env

Traj Buffer

Selection

LLM

��−1��−1��−1 ��

Try ...

LLM
Actions Set

Reconstruction

(ii) Policy Interpreter

State; Guide
Action;
LLM
History.

Further
Prompts

Critic
Network

c) LLM as decision-maker

b) LLM as reward designer.

Collect

History of
Env & Traj

Env

Directly exposed to NL

��−1

(ii) Indirect Decision-maker

Help me check this!

Large Language Model

Trajectory

(i) World Model Simulator

d) LLM as generator

Figure 24: Schematic diagram of the LLMs-enhanced MARL framework based on Cao et al. [116], showcasing its core roles: information
processor (a), reward designer (b), decision-maker (c), and generator (d).

improve the overall efficiency of the system. In dynamic and complex environments such as disaster relief
[12], LLMs can dynamically allocate roles and responsibilities according to task requirements, helping multiagent systems quickly adapt to changing environments
and highly complex task divisions [296, 295, 297]. This
capability provides a solid technical support for a wide
range of applications.

down complex tasks into subtasks, addressing challenges like the credit assignment problem. This capability is particularly useful in tasks requiring extended
reasoning, such as construction tasks in Minecraft. In
optimizing reinforcement learning’s sample efficiency
[308, 281], the generative capabilities of LLMs can
provide agents with additional virtual samples through
high-fidelity environment simulations [116, 308]. This
not only reduces the cost of real-world learning but
also offers high-quality trajectories that serve as valuable references for policy optimization. Furthermore, in
sparse reward environments, LLMs can accelerate policy learning by automatically designing reward signals.

6.4. Human Society Coordination: Balancing Technology and Ethics
The integration of LLMs into MARL opens new avenues for advancing multi-agent systems, while also
highlighting exciting research directions in improving
technical efficiency and addressing ethical considerations. For instance, enhancing the robustness of LLMs
in unfamiliar environments offers the opportunity to develop strategies for minimizing biases and hallucinations, thereby improving decision accuracy. Furthermore, the computational complexity and resource demands of LLMs present a chance to innovate in optimizing inference efficiency and scalability. This is especially relevant in dynamic multi-agent environments
where real-time responsiveness is critical.
From an ethical perspective, the incorporation of
LLMs calls for advancements in ensuring data privacy,
safeguarding against adversarial attacks, and establishing clear accountability frameworks for AI-driven decisions. Sensitive domains such as healthcare and disaster response could particularly benefit from focused
research on protecting sensitive information and enhancing system resilience. Additionally, improving the
transparency and explainability of LLMs-driven decisions is another promising area for exploration, as it

6.3. Application Expansion: Driving Intelligent Collaboration in Complex Scenarios
The potential of LLMs-enhanced MARL in practical
applications is enormous, especially in scenarios that
require complex collaboration and real-time decisionmaking [116, 300, 183, 171]. For example, in the field
of autonomous driving [19, 18, 17], the integration of
LLMs with MARL can simultaneously process sensor
data and natural language information (such as traffic
regulations, passenger instructions, etc. [92]), thereby
enhancing the safety and accuracy of decision-making
[214, 74]. In the field of collaborative robots, LLMs can
help multiple robots form a more intuitive communication mechanism, achieving highly complex task division
and dynamic adjustment. In addition, in multi-objective
optimization tasks such as smart grids and intelligent
healthcare, LLMs can provide domain knowledge and
optimization suggestions to assist reinforcement learning, design more practical reward functions, and thus
38

7. Conclusion

Table 4: Summary of recent studies categorized by the four key
roles of LLMs in MARL: Information Processor, Reward Designer,
Decision-Maker, and Generator, highlighting their respective contributions and applications.
Method Types
LLM as ...

Researchers. / Works. / Refs.
Poudel et al. (ReCoRe) [274], Choi et al. (ConPE) [275],

Information

Paischer et al. (HELM) [276] and (Semantic HELM) [277],

Processor

Radford et al. (CLIP) [278], Oord et al. (CPC) [279],
Michael et al. (CURL) [280], Schwarzer et al. (SPR) [281]
Kwon et al. (LLMrewardRL) [282], Song et al. (SelfRefined LLM) [283], Wu et al. (Read & Reward) [284],

Reward

Carta et al. (GLAM) [285], Chu et al. (Lafite-RL) [286],

Designer

Kim et al. (ARP) [287], Yu et al. [288], Adeniji
et al. (LAMP) [289], Madaan et al. (Self-Refine) [290],
Ma et al. (Eureka) [291], Xie et al. (Text2Reward) [292]
Janner et al. (TT-Offline RL) [293], Shi et al. (LaMo) [294],
Li et al. (LLM scaffold) [295], Mezghani et al.

Decision
(text BabyAI) [296], Grigsby et al. (AMAGO) [297],
-Maker
Zitkovich et al. (RT-2) [298], Yao et al. (CALM) [299],
Hu et al. (instructRL) [300], Zhou et al. (LLM4Teach) [301]
Chen et al. (TransDreamer) [302], Das et al. (S2E) [303],
Generator

Lin et al. (Dynalang) [304] , Robine et al. (TWM) [305],
Poudel et al. (LanGWM) [306], Lin et al. (HomeGrid) [307]

Multi-agent cooperative decision-making has demonstrated remarkable potential in addressing complex
tasks through intelligent collaboration and adaptability.
In this survey, we systematically review the evolution
of multi-agent systems, highlighting the shift from traditional methods, such as rule-based and game-theory
approaches, to advanced paradigms like MARL and
LLMs. We differentiate these methods by examining
their unique capabilities, challenges, and applications
in diverse environments, paying particular attention to
dynamic and uncertain settings. In addition, we explore the critical role of simulation environments as
a bridge between theoretical advancements and realworld implementation, emphasizing their influence on
agent interaction, learning, and decision-making. Practical applications of multi-agent systems in domains
such as autonomous driving, disaster response, and
robotics further underscore their transformative potential. By summarizing advanced multi-agent decisionmaking methodologies, datasets, benchmarks, and future research directions, this survey aims to provide a
comprehensive resource for researchers and practitioners. We hope it inspires future studies to address existing challenges, such as improving inter-agent communication and adaptability, while leveraging the innovative potential of DRL and LLMs-based approaches to
advance multi-agent cooperative decision-making.
Acknowledgment

would increase trust and user confidence in multi-agent
systems.

The corresponding authors of this survey are B. Zhao
and G.Yang from Xi‘an Jiaotong University and Imperial College London. Guang Yang was supported
in part by the ERC IMI (101005122), the H2020
(952172), the MRC (MC/PC/21013), the Royal Society (IEC\NSFC\211235), the NVIDIA Academic
Hardware Grant Program, the SABER project supported by Boehringer Ingelheim Ltd, NIHR Imperial Biomedical Research Centre (RDA01), Wellcome
Leap Dynamic Resilience, UKRI guarantee funding
for Horizon Europe MSCA Postdoctoral Fellowships
(EP/Z002206/1), and the UKRI Future Leaders Fellowship (MR/V023799/1). The authors would like to thank
the editors and anonymous reviewers, who significantly
enhanced the quality of the survey.

By addressing these areas, future research can maximize the potential of LLMs-enhanced MARL systems,
ensuring they are both technically effective and ethically
sound in diverse, real-world applications.
Overall, the combination of LLMs and MARL brings
new momentum to research and applications in multiagent systems. By enhancing collaboration through
natural language understanding and leveraging largescale knowledge, these systems can achieve greater efficiency and robustness in complex scenarios. However, fully unlocking their potential requires further exploration in theoretical methods, technological development, and ethical practices. With systematic advancements in these areas, LLMs-enhanced MARL can
become the foundation for next-generation intelligent
decision-making systems, transforming fields like autonomous driving, collaborative robotics, and healthcare, while shaping the future of AI research.

Declaration of Generative AI and AI-assisted Technologies in the Writing Process
During the preparation of this work, the authors utilized generative AI and AI-assisted technologies for
39

partial observation oi rather than the full state S . The
agents take individual actions ai , forming a joint action
at , which influences state transitions and results in individual rewards ri . The observations are generated according to the observation function Z(o | s′ , a), requiring each agent to infer the missing state information and
maintain a belief state for effective decision-making.
In summary, MDPs are well-suited for single-agent
systems, where the environment is static and fully observable, allowing the agent to make optimal decisions
based on complete knowledge of the state. On the other
hand, POMDPs are crucial for multi-agent reinforcement learning scenarios, where multiple agents interact
dynamically in an uncertain environment with limited
information. This setting introduces challenges such as
coordination, competition, and reward interdependencies, making decision-making significantly more complex.

proofreading and enhancing readability and language
clarity in certain sections. The authors have carefully reviewed these contents to ensure accuracy and completeness, acknowledging that AI can generate authoritativesounding output that may be incorrect, incomplete, or
biased.
Appendix A. Technological Comparisons between
Single-Agent and Multi-Agent (Under
Reinforcement Learning)
Here, we discuss a series of technological comparisons of both DRL-based single-agent and MARLbased multi-agent research.
In solving these single-agent sequential decisionmaking problems, Markov Decision Processes (MDP)
is a powerful mathematical modeling framework, especially in uncertain environments. Since the decisionmaking process of an agent can inherently be modeled
as a sequence of decisions, the single-agent decisionmaking process can be formulated as an typical MDP,
similar to a Markov chain.
In contrast to single-agent DRL systems, multi-agent
systems under the MARL techniques involve multiple agents interacting within a shared environment.
POMDP is a powerful mathematical modeling framework. It is an extension of the MDP framework that is
particularly well-suited for modeling decision-making
in environments where the agent does not have full visibility of the entire state space. POMDPs extend MDPs
to environments where the agent cannot fully observe
the underlying state. Instead, the agent maintains a belief state, which is a probability distribution over the
possible states.
Figure A.25 provides a comparative illustration of
Markov Decision Processes (MDP) and Partially Observable Markov Decision Processes (POMDP), which
correspond to single-agent and multi-agent reinforcement learning paradigms, respectively.
The left side of Figure A.25 depicts an MDP, which
models single-agent decision-making in a fully observable environment. The agent selects an action a from
the action space A based on the current state s from
the state space S . The environment transitions to a
new state s′ following the transition probability function
P(s′ | s, a), and the agent receives a reward r. The objective is to optimize a policy π∗ that maximizes the cumulative reward. Since the entire state is observable, the
decision-making process is relatively straightforward.
On the right side, the POMDP framework extends
MDPs to multi-agent settings where agents operate under partial observability. Each agent i receives only a

References
[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, M. Riedmiller, Playing atari with deep reinforcement learning (2013). arXiv:1312.5602.
URL https://arxiv.org/abs/1312.5602
[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik,
I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
D. Hassabis, Human-level control through deep reinforcement
learning, Nature 518 (7540) (2015) 529–533. doi:10.1038/
nature14236.
URL https://doi.org/10.1038/nature14236
[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,
G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham,
N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach,
K. Kavukcuoglu, T. Graepel, D. Hassabis, Mastering the game
of go with deep neural networks and tree search, Nature
529 (7587) (2016) 484–489. doi:10.1038/nature16961.
URL https://doi.org/10.1038/nature16961
[4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton,
Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, D. Hassabis, Mastering the game of go without human knowledge, Nature 550 (7676) (2017) 354–359.
doi:10.1038/nature24270.
URL https://doi.org/10.1038/nature24270
[5] T. Li, K. Zhu, N. C. Luong, D. Niyato, Q. Wu, Y. Zhang,
B. Chen, Applications of multi-agent reinforcement learning
in future internet: A comprehensive survey, IEEE Communications Surveys & Tutorials 24 (2) (2022) 1240–1279. doi:
10.1109/COMST.2022.3160697.
[6] S. Gronauer, K. Diepold, Multi-agent deep reinforcement
learning: a survey, Artificial Intelligence Review 55 (2) (2022)
895–943. doi:10.1007/s10462-021-09996-w.
URL https://doi.org/10.1007/s10462-021-09996-w
[7] P. Yadav, A. Mishra, S. Kim, A comprehensive survey on
multi-agent reinforcement learning for connected and automated vehicles, Sensors 23 (10) (2023). doi:10.3390/

40

Task xx
Time t

Agent

��

S�+1

Environment

Action Space

�

a� ∈ �

�� ∈ �(�, �)

S�+1

Policy
Network

~ �(�′ |�, �)

State Space

S

Agent 1 �1� Agent 2

�1�

r1�
S

�2�

�2�

�2�

… ���

�� → [r1� , r2� , …, r�
�]

Environment

Agent i’s Observation

Agent N

���

��
�

��
�

joint action a�

� ~ �(�∣� ′, �)

Partially Observable Markov Decision
Processes (POMDP) ⟨�, �, �, �, �, �, �⟩

Markov Decision Processes (MDP) ⟨�, �, �, �, �⟩

Figure A.25: The Markov Decision Process modeling for the single-agent reinforcement learning paradigm (left) and the Partially Observable
Markov Decision Process modeling for the multi-agent reinforcement learning paradigm (right).

agent reinforcement learning method for swarm robots in space
collaborative exploration, in: 2020 6th International Conference on Control, Automation and Robotics (ICCAR), 2020,
pp. 139–144. doi:10.1109/ICCAR49639.2020.9107997.
[16] C. G. Cena, P. F. Cardenas, R. S. Pazmino, L. Puglisi,
R. A. Santonja, A cooperative multi-agent robotics
system: Design and modelling, Expert Systems with
Applications 40 (12) (2013) 4737–4748.
doi:https:
//doi.org/10.1016/j.eswa.2013.01.048.
URL
https://www.sciencedirect.com/science/
article/pii/S0957417413000791
[17] S. Jayawardana, V. Jayawardana, K. Vidanage, C. Wu, Multibehavior learning for socially compatible autonomous driving, in: 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), 2023, pp. 4422–4427.
doi:10.1109/ITSC57777.2023.10422120.
[18] L. Wen, J. Duan, S. E. Li, S. Xu, H. Peng, Safe reinforcement
learning for autonomous vehicles through parallel constrained
policy optimization, in: 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), 2020, pp.
1–7. doi:10.1109/ITSC45102.2020.9294262.
[19] L. Weiwei, H. Wenxuan, J. Wei, L. Lanxin, G. Lingping,
L. Yong, Learning to model diverse driving behaviors in highly
interactive autonomous driving scenarios with multi-agent reinforcement learning (2024). arXiv:2402.13481.
URL https://arxiv.org/abs/2402.13481
[20] Y. Xue, W. Chen, Multi-agent deep reinforcement learning
for uavs navigation in unknown complex environment, IEEE
Transactions on Intelligent Vehicles 9 (1) (2024) 2290–2303.
doi:10.1109/TIV.2023.3298292.
[21] S. Rezwan, W. Choi, Artificial intelligence approaches for uav
navigation: Recent advances and future challenges, IEEE Access 10 (2022) 26320–26339. doi:10.1109/ACCESS.2022.
3157626.
[22] B. Al Baroomi, T. Myo, M. R. Ahmed, A. Al Shibli, M. H.
Marhaban, M. S. Kaiser, Ant colony optimization-based path
planning for uav navigation in dynamic environments, in:
2023 7th International Conference on Automation, Control
and Robots (ICACR), 2023, pp. 168–173. doi:10.1109/
ICACR59381.2023.10314603.
[23] T. Samad, S. Iqbal, A. W. Malik, O. Arif, P. Bloodsworth, A

s23104710.
URL https://www.mdpi.com/1424-8220/23/10/4710
[8] J. Orr, A. Dutta, Multi-agent deep reinforcement learning for
multi-robot applications: A survey, Sensors 23 (7) (2023).
doi:10.3390/s23073625.
URL https://www.mdpi.com/1424-8220/23/7/3625
[9] W. Jin, B. Zhao, Y. Zhang, J. Huang, H. Yu, Wordtransabsa: Enhancing aspect-based sentiment analysis with
masked language modeling for affective token prediction,
Expert Systems with Applications 238 (2024) 122289.
doi:https://doi.org/10.1016/j.eswa.2023.122289.
URL
https://www.sciencedirect.com/science/
article/pii/S0957417423027914
[10] B. Zhao, W. Jin, Y. Zhang, S. Huang, G. Yang, Prompt
learning for metonymy resolution: Enhancing performance
with internal prior knowledge of pre-trained language models,
Knowledge-Based Systems 279 (2023) 110928. doi:https:
//doi.org/10.1016/j.knosys.2023.110928.
URL
https://www.sciencedirect.com/science/
article/pii/S0950705123006780
[11] A. Seewald, C. J. Lerch, M. Chancán, A. M. Dollar, I. Abraham, Energy-aware ergodic search: Continuous exploration for
multi-agent systems with battery constraints (2024). arXiv:
2310.09470.
URL https://arxiv.org/abs/2310.09470
[12] M. M. H. Qazzaz, S. A. R. Zaidi, D. C. McLernon, A. Salama,
A. A. Al-Hameed, Optimizing search and rescue uav connectivity in challenging terrain through multi q-learning (2024).
arXiv:2405.10042.
URL https://arxiv.org/abs/2405.10042
[13] G. T. Papadopoulos, M. Antona, C. Stephanidis, Towards
open and expandable cognitive ai architectures for large-scale
multi-agent human-robot collaborative learning, IEEE Access 9 (2021) 73890–73909. doi:10.1109/ACCESS.2021.
3080517.
[14] M. D. Silva, R. Regnier, M. Makarov, G. Avrin, D. Dumur,
Evaluation of intelligent collaborative robots: a review, in:
2023 IEEE/SICE International Symposium on System Integration (SII), 2023, pp. 1–7. doi:10.1109/SII55687.2023.
10039365.
[15] Y. Huang, S. Wu, Z. Mu, X. Long, S. Chu, G. Zhao, A multi-

41

multi-agent framework for cloud-based management of collaborative robots, International Journal of Advanced Robotic Systems 15 (4) (2018). doi:10.1177/1729881418785073.
[24] W. Du, S. Ding, A survey on multi-agent deep reinforcement
learning: from the perspective of challenges and applications,
Artificial Intelligence Review 54 (5) (2021) 3215–3238. doi:
10.1007/s10462-020-09938-y.
URL https://doi.org/10.1007/s10462-020-09938-y
[25] Z. Ning, L. Xie, A survey on multi-agent reinforcement learning and its application, Journal of
Automation and Intelligence 3 (2) (2024) 73–91.
doi:https://doi.org/10.1016/j.jai.2024.02.003.
URL
https://www.sciencedirect.com/science/
article/pii/S2949855424000042
[26] Q. Yang, R. Liu, Understanding the application of utility theory
in robotics and artificial intelligence: A survey (2023). arXiv:
2306.09445.
URL https://arxiv.org/abs/2306.09445
[27] P. Hernandez-Leal, M. Kaisers, T. Baarslag, E. M. de Cote, A
survey of learning in multiagent environments: Dealing with
non-stationarity (2019). arXiv:1707.09183.
URL https://arxiv.org/abs/1707.09183
[28] C. Zhu, M. Dastani, S. Wang, A survey of multi-agent deep
reinforcement learning with communication, Autonomous
Agents and Multi-Agent Systems 38 (1) (2024) 4. doi:
10.1007/s10458-023-09633-6.
URL https://doi.org/10.1007/s10458-023-09633-6
[29] T. T. Nguyen, N. D. Nguyen, S. Nahavandi, Deep reinforcement learning for multiagent systems: A review of challenges,
solutions, and applications, IEEE Transactions on Cybernetics 50 (9) (2020) 3826–3839. doi:10.1109/TCYB.2020.
2977374.
[30] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang,
Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei,
J. Wen, A survey on large language model based autonomous
agents, Frontiers of Computer Science 18 (6) (2024) 186345.
doi:10.1007/s11704-024-40231-1.
URL https://doi.org/10.1007/s11704-024-40231-1
[31] B. Zhao, W. Jin, J. Del Ser, G. Yang, Chatagri: Exploring
potentials of chatgpt on cross-linguistic agricultural text
classification, Neurocomputing 557 (2023) 126708. doi:
https://doi.org/10.1016/j.neucom.2023.126708.
URL
https://www.sciencedirect.com/science/
article/pii/S0925231223008317
[32] T. Miki, M. Nagao, H. Kobayashi, T. Nakamura, A simple rule
based multi-agent control algorithm and its implementation using autonomous mobile robots, in: 2010 World Automation
Congress, 2010, pp. 1–6.
[33] H. Yarahmadi, H. Navidi, M. Challenger, Improving the resource allocation in iot systems based on the integration of
reinforcement learning and rule-based approaches in multiagent systems, in: 2024 8th International Conference on Smart
Cities, Internet of Things and Applications (SCIoT), 2024, pp.
135–141. doi:10.1109/SCIoT62588.2024.10570102.
[34] S.-H. Wu, V.-W. Soo, A fuzzy game theoretic approach to
multi-agent coordination, in: T. Ishida (Ed.), Multiagent Platforms, Springer Berlin Heidelberg, Berlin, Heidelberg, 1999,
pp. 76–87. doi:10.1007/3-540-48826-X\_6.
[35] H. Zhang, J. Zhang, G.-H. Yang, Y. Luo, Leader-based optimal
coordination control for the consensus problem of multiagent
differential games via fuzzy adaptive dynamic programming,
IEEE Transactions on Fuzzy Systems 23 (1) (2015) 152–163.
doi:10.1109/TFUZZ.2014.2310238.
[36] F. Ren, M. Zhang, Q. Bai, A fuzzy-based approach for partner
selection in multi-agent systems, in: 6th IEEE/ACIS Interna-

tional Conference on Computer and Information Science (ICIS
2007), 2007, pp. 457–462. doi:10.1109/ICIS.2007.21.
[37] D. Gu, H. Hu, Fuzzy multi-agent cooperative q-learning, in:
2005 IEEE International Conference on Information Acquisition, 2005, p. 5 pp. doi:10.1109/ICIA.2005.1635080.
[38] J. Wang, Y. Hong, J. Wang, J. Xu, Y. Tang, Q.-L. Han,
J. Kurths, Cooperative and competitive multi-agent systems:
From optimization to games, IEEE/CAA Journal of Automatica Sinica 9 (5) (2022) 763–783. doi:10.1109/JAS.2022.
105506.
[39] Y. Guo, Q. Pan, Q. Sun, C. Zhao, D. Wang, M. Feng, Cooperative game-based multi-agent path planning with obstacle avoidance, in: 2019 IEEE 28th International Symposium
on Industrial Electronics (ISIE), 2019, pp. 1385–1390. doi:
10.1109/ISIE.2019.8781205.
[40] D. Schwung, A. Schwung, S. X. Ding, Distributed selfoptimization of modular production units: A state-based potential game approach, IEEE Transactions on Cybernetics 52 (4)
(2022) 2174–2185. doi:10.1109/TCYB.2020.3006620.
[41] X. Wang, J. Wang, J. Chen, Y. Yang, L. Kong, X. Liu, L. Jia,
Y. Xu, A game-theoretic learning framework for multi-agent
intelligent wireless networks (2019). arXiv:1812.01267.
URL https://arxiv.org/abs/1812.01267
[42] W. Lin, Y. Chen, Q. Q. Wang, J. Zeng, J. Liu, Multi-agents
based distributed-energy-resource management for intelligent
microgrid with potential game algorithm, in: IECON 2017
- 43rd Annual Conference of the IEEE Industrial Electronics
Society, 2017, pp. 7795–7800. doi:10.1109/IECON.2017.
8217366.
[43] H. Wang, Z. Ning, H. Luo, Y. Jiang, M. Huo, Game-based
adaptive optimization approach for multi-agent systems, in:
2023 IEEE International Conference on Industrial Technology (ICIT), 2023, pp. 1–5. doi:10.1109/ICIT58465.2023.
10143172.
[44] L. Bull, Evolutionary computing in multi-agent environments:
Operators, in: V. W. Porto, N. Saravanan, D. Waagen, A. E.
Eiben (Eds.), Evolutionary Programming VII, Springer Berlin
Heidelberg, Berlin, Heidelberg, 1998, pp. 43–52.
[45] J. Liu, W. Zhong, L. Jiao, Multi-Agent Evolutionary Model
for Global Numerical Optimization, Springer Berlin Heidelberg, Berlin, Heidelberg, 2010, pp. 13–48. doi:10.1007/
978-3-642-13425-8\_2.
URL https://doi.org/10.1007/978-3-642-13425-8_
2
[46] D. Bloembergen, K. Tuyls, D. Hennes, M. Kaisers, Evolutionary dynamics of multi-agent learning: a survey, J. Artif. Int.
Res. 53 (1) (2015) 659–697.
[47] D. Klijn, A. E. Eiben, A coevolutionary approach to deep
multi-agent reinforcement learning, in: Proceedings of the
Genetic and Evolutionary Computation Conference Companion, GECCO ’21, Association for Computing Machinery, New
York, NY, USA, 2021, p. 283–284. doi:10.1145/3449726.
3459576.
URL https://doi.org/10.1145/3449726.3459576
[48] S. Yuan, K. Song, J. Chen, X. Tan, D. Li, D. Yang, Evoagent:
Towards automatic multi-agent generation via evolutionary algorithms (2024). arXiv:2406.14228.
URL https://arxiv.org/abs/2406.14228
[49] W. Zhang, H. Liu, Evolutionary game analysis of multi-agent
cooperation strategy analysis in agricultural water conservancy
ppp project under digitization background, Scientific Reports
14 (1) (2024) 22915. doi:10.1038/s41598-024-74065-5.
URL https://doi.org/10.1038/s41598-024-74065-5
[50] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar,
J. Foerster, S. Whiteson, QMIX: Monotonic value function

42

factorisation for deep multi-agent reinforcement learning, in:
J. Dy, A. Krause (Eds.), Proceedings of the 35th International
Conference on Machine Learning, Vol. 80 of Proceedings of
Machine Learning Research, PMLR, 2018, pp. 4295–4304.
URL
https://proceedings.mlr.press/v80/
rashid18a.html
[51] L. Kraemer, B. Banerjee, Multi-agent reinforcement
learning as a rehearsal for decentralized planning,
Neurocomputing 190 (2016) 82–94.
doi:https:
//doi.org/10.1016/j.neucom.2016.01.031.
URL
https://www.sciencedirect.com/science/
article/pii/S0925231216000783
[52] B. Yu, Z. Cai, J. He, Fast-qmix: Accelerating deep multiagent reinforcement learning with virtual weighted q-values,
in: 2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT), 2021, pp.
594–599. doi:10.1109/CECIT53797.2021.00110.
[53] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, Y. Yi, QTRAN:
Learning to factorize with transformation for cooperative
multi-agent reinforcement learning, in: K. Chaudhuri,
R. Salakhutdinov (Eds.), Proceedings of the 36th International
Conference on Machine Learning, Vol. 97 of Proceedings of
Machine Learning Research, PMLR, 2019, pp. 5887–5896.
URL https://proceedings.mlr.press/v97/son19a.
html
[54] T. Rashid, G. Farquhar, B. Peng, S. Whiteson, Weighted qmix:
expanding monotonic value function factorisation for deep
multi-agent reinforcement learning, in: Proceedings of the
34th International Conference on Neural Information Processing Systems, NIPS ’20, Curran Associates Inc., Red Hook, NY,
USA, 2020.
[55] K. Kurach, A. Raichuk, P. Stanczyk, M. Zajac, O. Bachem,
L. Espeholt, C. Riquelme, D. Vincent, M. Michalski, O. Bousquet, S. Gelly, Google research football: A novel reinforcement learning environment, Proceedings of the AAAI Conference on Artificial Intelligence 34 (04) (2020) 4501–4510.
doi:10.1609/aaai.v34i04.5878.
URL https://doi.org/10.1609/aaai.v34i04.5878
[56] J. Wang, Z. Ren, T. Liu, Y. Yu, C. Zhang, QPLEX: duplex dueling multi-agent q-learning, in: 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, OpenReview.net, 2021.
URL https://openreview.net/forum?id=Rcmk0xxIQV
[57] P. Li, V. Menon, B. Gudiguntla, D. Ting, L. Zhou, Challenges
faced by large language models in solving multi-agent flocking
(2024). arXiv:2404.04752.
URL https://arxiv.org/abs/2404.04752
[58] Z. Xu, C. Yu, F. Fang, Y. Wang, Y. Wu, Language agents
with reinforcement learning for strategic play in the werewolf
game, in: R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller,
N. Oliver, J. Scarlett, F. Berkenkamp (Eds.), Proceedings of the
41st International Conference on Machine Learning, Vol. 235
of Proceedings of Machine Learning Research, PMLR, 2024,
pp. 55434–55464.
URL https://proceedings.mlr.press/v235/xu24ad.
html
[59] I. Mordatch, P. Abbeel, Emergence of grounded compositional language in multi-agent populations, arXiv preprint
arXiv:1703.04908 (2017).
[60] H. Schwartz, An object oriented approach to fuzzy actor-critic
learning for multi-agent differential games, in: 2019 IEEE
Symposium Series on Computational Intelligence (SSCI),
2019, pp. 183–190.
doi:10.1109/SSCI44817.2019.
9002707.
[61] I. Harmati, Multi-agent coordination for target tracking using

fuzzy inference system in game theoretic framework, in: 2006
IEEE Conference on Computer Aided Control System Design,
2006 IEEE International Conference on Control Applications,
2006 IEEE International Symposium on Intelligent Control,
2006, pp. 2390–2395. doi:10.1109/CACSD-CCA-ISIC.
2006.4777014.
[62] C. W. Khuen, C. H. Yong, F. Haron, Multi-agent negotiation
system using adaptive fuzzy logic in resource allocation, in:
The 2nd International Conference on Distributed Frameworks
for Multimedia Applications, 2006, pp. 1–7. doi:10.1109/
DFMA.2006.296888.
[63] L. Yan, J. Liu, C. P. Chen, Y. Zhang, Z. Wu, Z. Liu, Gamebased adaptive fuzzy optimal bipartite containment of nonlinear multiagent systems, IEEE Transactions on Fuzzy Systems 32 (3) (2024) 1455–1465. doi:10.1109/TFUZZ.2023.
3327699.
[64] R. R. P. Vicerra, K. K. A. David, A. R. dela Cruz, E. A. Roxas,
K. B. C. Simbulan, A. A. Bandala, E. P. Dadios, A multiple
level mimo fuzzy logic based intelligence for multiple agent
cooperative robot system, in: TENCON 2015 - 2015 IEEE Region 10 Conference, 2015, pp. 1–7. doi:10.1109/TENCON.
2015.7372985.
[65] D. Gu, E. Yang, Fuzzy policy gradient reinforcement learning
for leader-follower systems, in: IEEE International Conference
Mechatronics and Automation, 2005, Vol. 3, 2005, pp. 1557–
1561 Vol. 3. doi:10.1109/ICMA.2005.1626787.
[66] Y. Maruyama, A reasoning system for fuzzy distributed knowledge representation in multi-agent systems, in: 2021 IEEE
International Conference on Fuzzy Systems (FUZZ-IEEE),
2021, pp. 1–6. doi:10.1109/FUZZ45933.2021.9494454.
[67] J. Peng, M. Liu, J. Liu, K.-C. Lin, M. Wu, A coordination
model using fuzzy reinforcement learning for multi-agent system, in: 2007 International Symposium on Collaborative Technologies and Systems, 2007, pp. 129–136. doi:10.1109/
CTS.2007.4621748.
[68] E. Yang, D. Gu, A multiagent fuzzy policy reinforcement
learning algorithm with application to leader-follower robotic
systems, in: 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2006, pp. 3197–3202. doi:
10.1109/IROS.2006.282421.
[69] H. Wang, H. Luo, Y. Jiang, A game-based distributed faulttolerant control method for multi-agent systems, in: 2022 IEEE
1st Industrial Electronics Society Annual On-Line Conference (ONCON), 2022, pp. 1–5. doi:10.1109/ONCON56984.
2022.10126545.
[70] M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls,
A unified game-theoretic approach to multiagent reinforcement learning, in: Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17,
Curran Associates Inc., Red Hook, NY, USA, 2017, p.
4193–4206.
[71] C. Guo, L. Zhong, T. Liu, A study on game theory in decision
interaction for multi intelligent agents based on information
fusion, in: Y. Wu (Ed.), Computing and Intelligent Systems,
Springer Berlin Heidelberg, Berlin, Heidelberg, 2011, pp. 442–
452.
[72] J. Zhang, J. Zhang, The coverage control solutions based on
evolutionary game theory in the multi-agent systems, in: 2019
12th Asian Control Conference (ASCC), 2019, pp. 1–6.
[73] R. Kong, N. Zhang, X. Bao, X. Lu, Multi-agent distributed
optimization based on the game theory and its application in
automobile chassis, in: 2018 37th Chinese Control Conference
(CCC), 2018, pp. 1107–1111. doi:10.23919/ChiCC.2018.
8484212.
[74] H. Wang, X. Zhang, H. Luo, X. Qiao, M. Huo, Y. Jiang, Dis-

43

tributed fault tolerant control for multi-agent systems with sensor faults in non-cooperative games, in: 2023 CAA Symposium on Fault Detection, Supervision and Safety for Technical
Processes (SAFEPROCESS), 2023, pp. 1–6. doi:10.1109/
SAFEPROCESS58597.2023.10295775.
[75] X. Dong, X. Li, S. Cheng, Energy management optimization of
microgrid cluster based on multi-agent-system and hierarchical stackelberg game theory, IEEE Access 8 (2020) 206183–
206197. doi:10.1109/ACCESS.2020.3037676.
[76] P. H. Nguyen, W. L. Kling, P. F. Ribeiro, A game theory
strategy to integrate distributed agent-based functions in smart
grids, IEEE Transactions on Smart Grid 4 (1) (2013) 568–576.
doi:10.1109/TSG.2012.2236657.
[77] S. U. Khan, I. Ahmad, A semi-distributed axiomatic game theoretical mechanism for replicating data objects in large distributed computing systems, in: 2007 IEEE International Parallel and Distributed Processing Symposium, 2007, pp. 1–10.
doi:10.1109/IPDPS.2007.370279.
[78] L. Xu, J. Zhu, B. Chen, Z. Yang, K. Liu, B. Dang, T. Zhang,
Y. Yang, R. Huang, A distributed nanocluster based multiagent evolutionary network, Nature Communications 13 (1)
(2022) 4698. doi:10.1038/s41467-022-32497-5.
URL https://doi.org/10.1038/s41467-022-32497-5
[79] F. Seredyński, T. Kulpa, R. Hoffmann, Towards evolutionary self-optimization of large multi-agent systems, in: Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO ’22, Association for Computing
Machinery, New York, NY, USA, 2022, p. 200–203. doi:
10.1145/3520304.3529042.
URL https://doi.org/10.1145/3520304.3529042
[80] H. Liu, Z. Li, K. Huang, R. Wang, G. Cheng, T. Li, Evolutionary reinforcement learning algorithm for large-scale multiagent cooperation and confrontation applications, The Journal
of Supercomputing 80 (2) (2024) 2319–2346. doi:10.1007/
s11227-023-05551-2.
URL https://doi.org/10.1007/s11227-023-05551-2
[81] Y. Dong, X. Liu, T. Li, C. L. P. Chen, Evolutionary game dynamics of multi-agent systems using local information considering hide right, Complex & Intelligent Systems 10 (1) (2024)
917–925. doi:10.1007/s40747-023-01172-7.
URL https://doi.org/10.1007/s40747-023-01172-7
[82] H. Chen, C. Hu, Z. Huang, Optimal control of multiagent
decision-making based on competence evolution, Discrete Dynamics in Nature and Society 2023 (1) (2023) 2179376. doi:
https://doi.org/10.1155/2023/2179376.
[83] H.-T. Wai, Z. Yang, Z. Wang, M. Hong, Multi-agent reinforcement learning via double averaging primal-dual optimization,
in: Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, Curran Associates Inc., Red Hook, NY, USA, 2018, pp. 9672–9683.
[84] T. Hu, Z. Pu, X. Ai, T. Qiu, J. Yi, Measuring policy distance
for multi-agent reinforcement learning (2024). arXiv:2401.
11257.
URL https://arxiv.org/abs/2401.11257
[85] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo,
K. Tuyls, T. Graepel, Value-decomposition networks for cooperative multi-agent learning based on team reward, in: Proceedings of the 17th International Conference on Autonomous
Agents and MultiAgent Systems, AAMAS ’18, International
Foundation for Autonomous Agents and Multiagent Systems,
Richland, SC, 2018, pp. 2085–2087.
[86] W. J. Yun, J. Park, J. Kim, Quantum multi-agent meta reinforcement learning, in: Proceedings of the Thirty-Seventh
AAAI Conference on Artificial Intelligence and Thirty-Fifth

Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances
in Artificial Intelligence, AAAI’23/IAAI’23/EAAI’23, AAAI
Press, 2023. doi:10.1609/aaai.v37i9.26313.
URL https://doi.org/10.1609/aaai.v37i9.26313
[87] W. Mao, H. Qiu, C. Wang, H. Franke, Z. Kalbarczyk,
R. K. Iyer, T. Başar, Multi-agent meta-reinforcement learning:
sharper convergence rates with task similarity, Curran Associates Inc., Red Hook, NY, USA, 2024.
[88] M. Kouzeghar, Y. Song, M. Meghjani, R. Bouffanais, Multitarget pursuit by a decentralized heterogeneous uav swarm using deep multi-agent reinforcement learning (2023). arXiv:
2303.01799.
URL https://arxiv.org/abs/2303.01799
[89] F. Gao, S. Chen, M. Li, B. Huang, Maca: a multi-agent reinforcement learning platform for collective intelligence, in:
2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS), 2019, pp. 108–111.
doi:10.1109/ICSESS47205.2019.9040781.
[90] S. Qi, S. Zhang, X. Hou, J. Zhang, X. Wang, J. Xiao, Efficient
distributed framework for collaborative multi-agent reinforcement learning (2022). arXiv:2205.05248.
URL https://arxiv.org/abs/2205.05248
[91] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu,
A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds,
P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka,
A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg, A. S.
Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden,
Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang,
T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. Wünsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, D. Silver, Grandmaster level in starcraft ii using
multi-agent reinforcement learning, Nature 575 (7782) (2019)
350–354. doi:10.1038/s41586-019-1724-z.
URL https://doi.org/10.1038/s41586-019-1724-z
[92] T. Chu, J. Wang, L. Codecà, Z. Li, Multi-agent deep reinforcement learning for large-scale traffic signal control, IEEE Transactions on Intelligent Transportation Systems 21 (3) (2020)
1086–1095. doi:10.1109/TITS.2019.2901791.
[93] Z. Lv, L. Xiao, Y. Du, G. Niu, C. Xing, W. Xu, Multiagent reinforcement learning based uav swarm communications against jamming, Trans. Wireless. Comm. 22 (12) (2023)
9063–9075. doi:10.1109/TWC.2023.3268082.
URL https://doi.org/10.1109/TWC.2023.3268082
[94] R.-E. P. Mircea-Bogdan Radac, R.-C. Roman, Model-free control performance improvement using virtual reference feedback tuning and reinforcement q-learning, International Journal of Systems Science 48 (5) (2017) 1071–1083. doi:10.
1080/00207721.2016.1236423.
[95] Z. Liu, X. Yang, S. Sun, L. Qian, L. Wan, X. Chen, X. Lan,
Grounded answers for multi-agent decision-making problem
through generative world model, in: Advances in Neural Information Processing Systems (NeurIPS), 2024.
[96] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum,
T. Shu, C. Gan, Building cooperative embodied agents modularly with large language models, in: The Twelfth International
Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=EnXJfQqy0K
[97] X. Hou, M. Yang, W. Jiao, X. Wang, Z. Tu, W. X. Zhao, Coact:
A global-local hierarchy for autonomous agent collaboration
(2024). arXiv:2406.13381.
URL https://arxiv.org/abs/2406.13381
[98] X. Puig, K. K. Ra, M. Boben, J. Li, T. Wang, S. Fidler,
A. Torralba, Virtualhome: Simulating household activities via
programs, 2018 IEEE/CVF Conference on Computer Vision

44

and Pattern Recognition (2018) 8494–8502.
URL https://api.semanticscholar.org/CorpusID:
49317780
[99] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum,
S. Fidler, A. Torralba, Watch-and-help: A challenge for social
perception and human-{ai} collaboration, in: International
Conference on Learning Representations, 2021.
URL
https://openreview.net/forum?id=w_
7JMpGZRh0
[100] D. Gao, Z. Li, X. Pan, W. Kuang, Z. Ma, B. Qian, F. Wei,
W. Zhang, Y. Xie, D. Chen, L. Yao, H. Peng, Z. Zhang, L. Zhu,
C. Cheng, H. Shi, Y. Li, B. Ding, J. Zhou, Agentscope: A
flexible yet robust multi-agent platform (2024). arXiv:2402.
14034.
URL https://arxiv.org/abs/2402.14034
[101] Z. Xiao, D. Zhang, Y. Wu, L. Xu, Y. J. Wang, X. Han,
X. Fu, T. Zhong, J. Zeng, M. Song, G. Chen, Chain-of-experts:
When LLMs meet complex operations research problems, in:
The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=HobyL1B9CZ
[102] C. Wang, W. Luo, Q. Chen, H. Mai, J. Guo, S. Dong, X. M.
Xuan, Z. Li, L. Ma, S. Gao, Mllm-tool: A multimodal
large language model for tool agent learning, arXiv preprint
arXiv:2401.10727 (2024).
[103] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. E. Zhu, B. Li,
L. Jiang, X. Zhang, C. Wang, Autogen: Enabling next-gen llm
applications via multi-agent conversation, Tech. Rep. MSRTR-2023-33, Microsoft (August 2023).
[104] M. Wen, J. Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, Y. Yang,
Multi-agent reinforcement learning is a sequence modeling
problem, in: S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh (Eds.), Advances in Neural Information
Processing Systems, Vol. 35, Curran Associates, Inc., 2022,
pp. 16509–16521.
URL
https://proceedings.neurips.
cc/paper_files/paper/2022/file/
69413f87e5a34897cd010ca698097d0a-Paper-Conference.
pdf
[105] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu,
Y. Lu, Y.-H. Hung, C. Qian, Y. Qin, X. Cong, R. Xie, Z. Liu,
M. Sun, J. Zhou, Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors, in: The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=EHg5GDnyq1
[106] Z. Liu, Y. Zhang, P. Li, Y. Liu, D. Yang, Dynamic LLM-agent
network: An LLM-agent collaboration framework with agent
team optimization (2024).
URL https://openreview.net/forum?id=i43XCU54Br
[107] G. Chen, S. Dong, Y. Shu, G. Zhang, J. Sesay, B. Karlsson, J. Fu, Y. Shi, Autoagents: A framework for automatic
agent generation, in: K. Larson (Ed.), Proceedings of the
Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, International Joint Conferences on Artificial Intelligence Organization, 2024, pp. 22–30, main Track.
doi:10.24963/ijcai.2024/3.
URL https://doi.org/10.24963/ijcai.2024/3
[108] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang,
C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran,
L. Xiao, C. Wu, J. Schmidhuber, MetaGPT: Meta programming for a multi-agent collaborative framework, in: The
Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=VtmBAGCN7o
[109] S. Hong, Y. Lin, B. Liu, B. Liu, B. Wu, D. Li, J. Chen,

45

J. Zhang, J. Wang, L. Zhang, L. Zhang, M. Yang, M. Zhuge,
T. Guo, T. Zhou, W. Tao, W. Wang, X. Tang, X. Lu, X. Zheng,
X. Liang, Y. Fei, Y. Cheng, Z. Xu, C. Wu, Data interpreter: An
llm agent for data science (2024). arXiv:2402.18679.
[110] X. Team, Xagent: An autonomous agent for complex task solving (2023).
[111] J. Wang, Z. Duan, Intelligent spark agents: A modular langgraph framework for scalable, visualized, and enhanced big
data machine learning workflows (2024). arXiv:2412.
01490.
URL https://arxiv.org/abs/2412.01490
[112] J. Wang, Z. Duan, Agent ai with langgraph: A modular framework for enhancing machine translation using large language
models (2024). arXiv:2412.03801.
URL https://arxiv.org/abs/2412.03801
[113] Y. Zheng, Z. Xing, Q. Zhang, B. Jin, P. Li, Y. Zheng, Z. Xia,
K. Zhan, X. Lang, Y. Chen, D. Zhao, Planagent: A multimodal large language agent for closed-loop vehicle motion
planning (2024). arXiv:2406.01587.
URL https://arxiv.org/abs/2406.01587
[114] L. Zhang, Z. Ji, B. Chen, Crew: Facilitating human-ai teaming
research (2025). arXiv:2408.00170.
URL https://arxiv.org/abs/2408.00170
[115] Z. Duan, J. Wang, Exploration of llm multi-agent application
implementation based on langgraph+crewai (2024). arXiv:
2411.18241.
URL https://arxiv.org/abs/2411.18241
[116] Y. Cao, H. Zhao, Y. Cheng, T. Shu, Y. Chen, G. Liu, G. Liang,
J. Zhao, J. Yan, Y. Li, Survey on large language modelenhanced reinforcement learning: Concept, taxonomy, and
methods, IEEE Transactions on Neural Networks and Learning
Systems (2024) 1–21doi:10.1109/TNNLS.2024.3497992.
[117] I. Marti, V. R. Tomas, A. Saez, J. J. Martinez, A rule-based
multi-agent system for road traffic management, in: 2009
IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, Vol. 3, 2009, pp.
595–598. doi:10.1109/WI-IAT.2009.358.
[118] V. V. Nekhai, E. Trunova, I. Bilous, I. Bohdan, M. Voitsekhovska, Fuzzy game-theoretic modeling of a multi-agent
cybersecurity management system for an agricultural enterprise, in: S. Shkarlet, A. Morozov, A. Palagin, D. Vinnikov,
N. Stoianov, M. Zhelezniak, V. Kazymyr (Eds.), Mathematical
Modeling and Simulation of Systems, Springer International
Publishing, Cham, 2022, pp. 423–434.
[119] A. Ramezani, M. R. Andalibizadeh, S. Bahrampour,
H. Ramezani, B. Moshiri, Select reliable strategy in multiagent systems using fuzzy logic-based fusion, in: 2008 Second Asia International Conference on Modelling & Simulation
(AMS), 2008, pp. 13–17. doi:10.1109/AMS.2008.63.
[120] M. E. H. Charaf, M. Benattou, S. Azzouzi, A rule-based
multi-agent system for testing distributed applications, in:
2012 International Conference on Multimedia Computing and
Systems, 2012, pp. 967–972. doi:10.1109/ICMCS.2012.
6320205.
[121] A. Daeichian, A. Haghani, Fuzzy q-learning-based multi-agent
system for intelligent traffic control by a game theory approach,
Arabian Journal for Science and Engineering 43 (6) (2018)
3241–3247. doi:10.1007/s13369-017-3018-9.
URL https://doi.org/10.1007/s13369-017-3018-9
[122] C. Amato, A first introduction to cooperative multi-agent reinforcement learning (2024). arXiv:2405.06161.
URL https://arxiv.org/abs/2405.06161
[123] Y. Zhou, S. Liu, Y. Qing, K. Chen, T. Zheng, Y. Huang,
J. Song, M. Song, Is centralized training with decentralized
execution framework centralized enough for marl? (2023).

benchmark for cooperative multi-agent reinforcement learning, in: A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, S. Levine (Eds.), Advances in Neural Information
Processing Systems, Vol. 36, Curran Associates, Inc., 2023,
pp. 37567–37593.
URL
https://proceedings.neurips.
cc/paper_files/paper/2023/file/
764c18ad230f9e7bf6a77ffc2312c55e-Paper-Datasets_
and_Benchmarks.pdf
[136] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch,
Multi-agent actor-critic for mixed cooperative-competitive environments, Neural Information Processing Systems (NIPS)
(2017).
[137] T. Malloy, C. R. Sims, T. Klinger, M. Liu, M. Riemer,
G. Tesauro, Capacity-limited decentralized actor-critic for
multi-agent games, in: 2021 IEEE Conference on Games
(CoG), 2021, pp. 1–8. doi:10.1109/CoG52621.2021.
9619081.
[138] J. Jiang, Z. Lu, Learning attentional communication for
multi-agent cooperation, in: Advances in Neural Information
Processing Systems, Vol. 31, 2018, pp. 7254–7264.
URL https://proceedings.neurips.cc/paper/2018/
file/6a8018b3a00b69c008601b8becae392b-Paper.
pdf
[139] C. Sun, Z. Zang, J. Li, J. Li, X. Xu, R. Wang, C. Zheng, T2mac:
targeted and trusted multi-agent communication through selective engagement and evidence-driven integration, in: Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative
Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence,
AAAI’24/IAAI’24/EAAI’24, AAAI Press, 2025. doi:10.
1609/aaai.v38i13.29438.
URL https://doi.org/10.1609/aaai.v38i13.29438
[140] OpenAI, Gpt-4 technical report,
arXiv preprint
arXiv:2303.08774 (2023).
URL https://arxiv.org/abs/2303.08774
[141] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,
P. Welinder, P. Christiano, J. Leike, R. Lowe, Training language models to follow instructions with human feedback, in:
Proceedings of the 36th International Conference on Neural
Information Processing Systems, NIPS ’22, Curran Associates
Inc., Red Hook, NY, USA, 2024.
[142] R. Mao, G. Chen, X. Zhang, F. Guerin, E. Cambria, GPTEval: A survey on assessments of ChatGPT and GPT-4, in:
N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, N. Xue
(Eds.), Proceedings of the 2024 Joint International Conference
on Computational Linguistics, Language Resources and
Evaluation (LREC-COLING 2024), ELRA and ICCL, Torino,
Italia, 2024, pp. 7844–7866.
URL
https://aclanthology.org/2024.lrec-main.
693
[143] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian,
A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan,
A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, et al., The
llama 3 herd of models (2024). arXiv:2407.21783.
URL https://arxiv.org/abs/2407.21783
[144] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, G. Lample, Llama: Open and
efficient foundation language models (2023). arXiv:2302.
13971.
URL https://arxiv.org/abs/2302.13971

arXiv:2305.17352.
URL https://arxiv.org/abs/2305.17352
[124] C. Amato, An introduction to centralized training for decentralized execution in cooperative multi-agent reinforcement
learning (2024). arXiv:2409.03052.
URL https://arxiv.org/abs/2409.03052
[125] P. K. Sharma, E. G. Zaroukian, R. Fernandez, A. Basak,
D. E. Asher, Survey of recent multi-agent reinforcement learning algorithms utilizing centralized training, in: T. Pham,
L. Solomon, M. E. Hohil (Eds.), Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III,
SPIE, 2021, p. 84. doi:10.1117/12.2585808.
URL http://dx.doi.org/10.1117/12.2585808
[126] J. K. Gupta, M. Egorov, M. Kochenderfer, Cooperative multiagent control using deep reinforcement learning, in: G. Sukthankar, J. A. Rodriguez-Aguilar (Eds.), Autonomous Agents
and Multiagent Systems, Springer International Publishing,
Cham, 2017, pp. 66–83.
[127] C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative multiagent systems, in: Proceedings of
the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, AAAI
Press, 1998, pp. 746–752.
[128] M. Lauer, M. A. Riedmiller, An algorithm for distributed reinforcement learning in cooperative multi-agent systems, in:
Proceedings of the Seventeenth International Conference on
Machine Learning, ICML ’00, Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 2000, p. 535–542.
[129] M. Bowling, M. Veloso, Multiagent learning using a variable
learning rate, Artificial Intelligence 136 (2) (2002) 215–
250. doi:https://doi.org/10.1016/S0004-3702(02)
00121-2.
URL
https://www.sciencedirect.com/science/
article/pii/S0004370202001212
[130] T. Ikeda, T. Shibuya, Centralized training with decentralized
execution reinforcement learning for cooperative multi-agent
systems with communication delay, in: 2022 61st Annual Conference of the Society of Instrument and Control Engineers
(SICE), 2022, pp. 135–140. doi:10.23919/SICE56594.
2022.9905866.
[131] Y. Zhou, S. Liu, Y. Qing, K. Chen, T. Zheng, Y. Huang,
J. Song, M. Song, Is centralized training with decentralized
execution framework centralized enough for marl? (2023).
arXiv:2305.17352.
URL https://arxiv.org/abs/2305.17352
[132] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, I. Mordatch,
Multi-agent actor-critic for mixed cooperative-competitive environments, in: Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17,
Curran Associates Inc., Red Hook, NY, USA, 2017, pp. 6382–
6393.
[133] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, Y. Wu,
The surprising effectiveness of ppo in cooperative multi-agent
games, in: Advances in Neural Information Processing
Systems, Vol. 35, 2022, pp. 29914–29928.
URL
https://proceedings.neurips.
cc/paper_files/paper/2022/hash/
9c1535a02f0ce079433344e14d910597-Paper-Datasets_
and_Benchmarks.pdf
[134] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar,
N. Nardelli, T. G. J. Rudner, C.-M. Hung, P. H. S. Torr, J. Foerster, S. Whiteson, The StarCraft Multi-Agent Challenge, CoRR
abs/1902.04043 (2019).
[135] B. Ellis, J. Cook, S. Moalla, M. Samvelyan, M. Sun, A. Mahajan, J. Foerster, S. Whiteson, Smacv2: An improved

46

[145] G. Team, Gemini: A family of highly capable multimodal
models (2024). arXiv:2312.11805.
URL https://arxiv.org/abs/2312.11805
[146] V. Dibia, J. Chen, G. Bansal, S. Syed, A. Fourney, E. Zhu,
C. Wang, S. Amershi, Autogen studio: A no-code developer
tool for building and debugging multi-agent systems (2024).
arXiv:2408.15247.
URL https://arxiv.org/abs/2408.15247
[147] C. Gan, S. Zhou, J. Schwartz, S. Alter, A. Bhandwaldar,
D. Gutfreund, D. L. Yamins, J. J. DiCarlo, J. McDermott,
A. Torralba, J. B. Tenenbaum, The threedworld transport challenge: A visually guided task-and-motion planning benchmark
towards physically realistic embodied ai, in: 2022 International
Conference on Robotics and Automation (ICRA), 2022, pp.
8847–8854. doi:10.1109/ICRA46639.2022.9812329.
[148] C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf,
J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber,
M. Sano, K. Kim, E. Wang, M. Lingelbach, A. Curtis,
K. Feigelis, D. Bear, D. Gutfreund, D. Cox, A. Torralba,
J. J. DiCarlo, J. Tenenbaum, J. McDermott, D. Yamins,
Threedworld: A platform for interactive multi-modal physical
simulation, in: J. Vanschoren, S. Yeung (Eds.), Proceedings of
the Neural Information Processing Systems Track on Datasets
and Benchmarks, Vol. 1, 2021.
URL
https://datasets-benchmarks-proceedings.
neurips.cc/paper_files/paper/2021/file/
735b90b4568125ed6c3f678819b6e058-Paper-round1.
pdf
[149] R. Gong, Q. Huang, X. Ma, Y. Noda, Z. Durante, Z. Zheng,
D. Terzopoulos, L. Fei-Fei, J. Gao, H. Vo, MindAgent: Emergent gaming interaction, in: K. Duh, H. Gomez, S. Bethard
(Eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Association for Computational
Linguistics, Mexico City, Mexico, 2024, pp. 3154–3183.
doi:10.18653/v1/2024.findings-naacl.200.
URL
https://aclanthology.org/2024.
findings-naacl.200
[150] K. Shao, Y. Zhu, Z. Tang, D. Zhao, Cooperative multiagent deep reinforcement learning with counterfactual reward,
in: 2020 International Joint Conference on Neural Networks
(IJCNN), 2020, pp. 1–8. doi:10.1109/IJCNN48605.2020.
9207169.
[151] K. R. Chandra, S. Borugadda, Multi agent deep reinforcement
learning with deep q-network based energy efficiency and resource allocation in noma wireless systems, in: 2023 Second
International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT), 2023, pp.
1–8. doi:10.1109/ICEEICT56924.2023.10157052.
[152] A. M. Hafiz, G. M. Bhat, Deep q-network based multi-agent reinforcement learning with binary action agents (2020). arXiv:
2008.04109.
URL https://arxiv.org/abs/2008.04109
[153] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk,
P. H. S. Torr, M. Sun, S. Whiteson, Is independent learning
all you need in the starcraft multi-agent challenge? (2020).
arXiv:2011.09533.
URL https://arxiv.org/abs/2011.09533
[154] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, J. Quan, S. Gaffney, S. Petersen, K. Simonyan,
T. Schaul, H. van Hasselt, D. Silver, T. Lillicrap, K. Calderone,
P. Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, R. Tsing, Starcraft ii: A new challenge for reinforcement learning
(2017). arXiv:1708.04782.
URL https://arxiv.org/abs/1708.04782

[155] H. Shen, K. Zhang, M. Hong, T. Chen, Towards understanding
asynchronous advantage actor-critic: Convergence and linear
speedup, IEEE Transactions on Signal Processing 71 (2023)
2579–2594. doi:10.1109/tsp.2023.3268475.
URL http://dx.doi.org/10.1109/TSP.2023.3268475
[156] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, S. Russell, Robust
multi-agent reinforcement learning via minimax deep deterministic policy gradient, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, AAAI Press, 2019,
pp. 4213–4220. doi:10.1609/aaai.v33i01.33014213.
[157] J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, S. Whiteson, Counterfactual multi-agent policy gradients, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of
Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence,
AAAI’18/IAAI’18/EAAI’18, AAAI Press, 2018.
[158] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov,
Proximal policy optimization algorithms (2017). arXiv:
1707.06347.
URL https://arxiv.org/abs/1707.06347
[159] J. Schulman, S. Levine, P. Moritz, M. Jordan, P. Abbeel, Trust
region policy optimization, in: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, JMLR.org, 2015, p.
1889–1897.
[160] J. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang,
Y. Yang, Trust region policy optimisation in multi-agent reinforcement learning, in: International Conference on Learning
Representations, 2022.
URL https://arxiv.org/abs/2109.11251
[161] S. Sukhbaatar, A. Szlam, R. Fergus, Learning multiagent communication with backpropagation, in: Proceedings of the 30th
International Conference on Neural Information Processing
Systems, NIPS’16, Curran Associates Inc., Red Hook, NY,
USA, 2016, pp. 2252–2260.
[162] L. Matignon, G. J. Laurent, N. Le Fort-Piat, Hysteretic qlearning : an algorithm for decentralized reinforcement learning in cooperative multi-agent teams, in: 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2007,
pp. 64–69. doi:10.1109/IROS.2007.4399095.
[163] E. Wei, S. Luke, Lenient learning in independent-learner
stochastic cooperative games, J. Mach. Learn. Res. 17 (1)
(2016) 2914–2955.
[164] M. Hausknecht, P. Stone, Deep recurrent q-learning for partially observable mdps (2017). arXiv:1507.06527.
URL https://arxiv.org/abs/1507.06527
[165] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus,
J. Aru, J. Aru, R. Vicente, Multiagent cooperation and competition with deep reinforcement learning, PLOS ONE 12 (4)
(2017) 1–15. doi:10.1371/journal.pone.0172395.
URL
https://doi.org/10.1371/journal.pone.
0172395
[166] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, J. Vian, Deep
decentralized multi-task multi-agent reinforcement learning
under partial observability, in: Proceedings of the 34th International Conference on Machine Learning - Volume 70,
ICML’17, JMLR.org, 2017, p. 2681–2690.
[167] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym (2016). arXiv:
1606.01540.
URL https://arxiv.org/abs/1606.01540
[168] J. N. Foerster, Y. M. Assael, N. de Freitas, S. Whiteson,
Learning to communicate with deep multi-agent reinforcement
learning (2016). arXiv:1605.06676.

47

URL https://arxiv.org/abs/1605.06676
[169] J. N. Foerster, Y. M. Assael, N. de Freitas, S. Whiteson, Learning to communicate to solve riddles with deep distributed recurrent q-networks (2016). arXiv:1602.02672.
URL https://arxiv.org/abs/1602.02672
[170] L. Fan, Y.-y. Liu, S. Zhang, Partially observable multi-agent rl
with enhanced deep distributed recurrent q-network, in: 2018
5th International Conference on Information Science and Control Engineering (ICISCE), 2018, pp. 375–379. doi:10.
1109/ICISCE.2018.00085.
[171] S. Sukhbaatar, A. Szlam, R. Fergus, Learning multiagent
communication with backpropagation, in: Advances in Neural
Information Processing Systems, Vol. 29, 2016, pp. 2244–
2252.
URL https://proceedings.neurips.cc/paper/2016/
file/55b1927fdafef39c48e5b73b5d61ea60-Paper.
pdf
[172] P. Peng, Y. Wen, Y. Yang, Q. Yuan, Z. Tang, H. Long, J. Wang,
Multiagent bidirectionally-coordinated nets: Emergence of
human-level coordination in learning to play starcraft combat
games (2017). arXiv:1703.10069.
URL https://arxiv.org/abs/1703.10069
[173] G. Wang, D. Shi, C. Xue, H. Jiang, Y. Wang, Bic-ddpg:
Bidirectionally-coordinated nets for deep multi-agent reinforcement learning, in: H. Gao, X. Wang, M. Iqbal, Y. Yin,
J. Yin, N. Gu (Eds.), Collaborative Computing: Networking,
Applications and Worksharing, Springer International Publishing, Cham, 2021, pp. 337–354.
[174] D. Fan, H. Shen, L. Dong, Multi-agent distributed deep deterministic policy gradient for partially observable tracking, Actuators 10 (10) (2021). doi:10.3390/act10100268.
URL https://www.mdpi.com/2076-0825/10/10/268
[175] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,
T. Harley, D. Silver, K. Kavukcuoglu, Asynchronous methods
for deep reinforcement learning, in: Proceedings of The 33rd
International Conference on Machine Learning, PMLR, 2016,
pp. 1928–1937.
URL https://proceedings.mlr.press/v48/mniha16.
html
[176] L. Yuan, C. Wang, J. Wang, F. Zhang, F. Chen, C. Guan,
Z. Zhang, C. Zhang, Y. Yu, Multi-agent concentrative coordination with decentralized task representation, in: L. D. Raedt
(Ed.), Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, International Joint
Conferences on Artificial Intelligence Organization, 2022, pp.
599–605, main Track. doi:10.24963/ijcai.2022/85.
URL https://doi.org/10.24963/ijcai.2022/85
[177] S. Vanneste, A. Vanneste, K. Mets, T. D. Schepper, A. Anwar, S. Mercelis, P. Hellinckx, Learning to communicate using
a communication critic and counterfactual reasoning, Neural
Computing and Applications N/A (N/A) (2025) N/A. doi:
10.1007/s00521-024-10598-0.
URL https://doi.org/10.1007/s00521-024-10598-0
[178] A. Singh, T. Jain, S. Sukhbaatar, Learning when to communicate at scale in multiagent cooperative and competitive
tasks, in: International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=BklWt24tvH
[179] D. Das, R. L. Rath, T. Singh, S. Mishra, V. Malik, R. Sobti,
B. Brahma, Llm-based custom chatbot using langchain, in:
A. E. Hassanien, S. Anand, A. Jaiswal, P. Kumar (Eds.), Innovative Computing and Communications, Springer Nature Singapore, Singapore, 2024, pp. 257–267.
[180] Q. Sun, Z. Yin, X. Li, Z. Wu, X. Qiu, L. Kong, Corex: Pushing the boundaries of complex reasoning through multi-model

collaboration (2023). arXiv:2310.00280.
[181] J. S. Park, L. Popowski, C. Cai, M. R. Morris, P. Liang, M. S.
Bernstein, Social simulacra: Creating populated prototypes for
social computing systems, in: Proceedings of the 35th Annual
ACM Symposium on User Interface Software and Technology,
UIST ’22, Association for Computing Machinery, New York,
NY, USA, 2022. doi:10.1145/3526113.3545616.
URL https://doi.org/10.1145/3526113.3545616
[182] J. Terry, B. Black, N. Grammel, M. Jayakumar, A. Hari,
R. Sullivan, L. Santos, R. Perez, C. Horsch, C. Dieffendahl,
et al., Pettingzoo: A standard api for multi-agent reinforcement
learning, in: Proceedings of the 35th Conference on Neural Information Processing Systems, in: Advances in Neural Information Processing Systems, Vol. 34, 2021, pp. 15032–15043.
[183] S. Huang, S. Ontañón, C. Bamford, L. Grela, Gymµrts:
Toward affordable full game real-time strategy
games research with deep reinforcement learning, in:
2021 IEEE Conference on Games (CoG), Copenhagen,
Denmark, August 17-20, 2021, IEEE, 2021, pp. 1–8.
doi:10.1109/CoG52621.2021.9619076.
URL
https://doi.org/10.1109/CoG52621.2021.
9619076
[184] L. Zheng, J. Yang, H. Cai, M. Zhou, W. Zhang, J. Wang,
Y. Yu, Magent: A many-agent reinforcement learning platform for artificial collective intelligence, Proceedings of the
AAAI Conference on Artificial Intelligence 32 (1) (2018).
doi:10.1609/aaai.v32i1.11371.
URL
https://ojs.aaai.org/index.php/AAAI/
article/view/11371
[185] C. Yu, P. Wang, Dexterous manipulation for multifingered robotic hands with reinforcement learning: A review, Frontiers in Neurorobotics 16 (2022).
doi:10.3389/fnbot.2022.861825.
URL
https://www.frontiersin.org/journals/
neurorobotics/articles/10.3389/fnbot.2022.
861825
[186] O. M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz,
B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, W. Zaremba, Learning dexterous in-hand manipulation, The International Journal of Robotics Research
39 (1) (2020) 3–20. arXiv:https://doi.org/10.1177/
0278364919887447, doi:10.1177/0278364919887447.
URL https://doi.org/10.1177/0278364919887447
[187] M. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano,
L. Willems, S. Lahlou, S. Pal, P. S. Castro, J. Terry, Minigrid & miniworld: Modular & customizable reinforcement
learning environments for goal-oriented tasks (2023). arXiv:
2306.13831.
URL https://arxiv.org/abs/2306.13831
[188] J. Z. Leibo, E. Duéñez-Guzmán, A. S. Vezhnevets, J. P. Agapiou, P. Sunehag, R. Koster, J. Matyas, C. Beattie, I. Mordatch,
T. Graepel, Scalable evaluation of multi-agent reinforcement
learning with melting pot (2021). arXiv:2107.06857.
URL https://arxiv.org/abs/2107.06857
[189] C. Mehlman, J. Abramov, G. Falco, Cat-and-mouse satellite dynamics: Divergent adversarial reinforcement learning
for contested multi-agent space operations (2024). arXiv:
2409.17443.
URL https://arxiv.org/abs/2409.17443
[190] Y. Dong, X. Zhu, Z. Pan, L. Zhu, Y. Yang, VillagerAgent: A
graph-based multi-agent framework for coordinating complex
task dependencies in Minecraft, in: L.-W. Ku, A. Martins,
V. Srikumar (Eds.), Findings of the Association for Computational Linguistics: ACL 2024, Association for Computational

48

Linguistics, Bangkok, Thailand, 2024, pp. 16290–16314.
doi:10.18653/v1/2024.findings-acl.964.
URL
https://aclanthology.org/2024.
findings-acl.964/
[191] H. Li, X. Yang, Z. Wang, X. Zhu, J. Zhou, Y. Qiao, X. Wang,
H. Li, L. Lu, J. Dai, Auto mc-reward: Automated dense reward design with large language models for minecraft (2024).
arXiv:2312.09238.
URL https://arxiv.org/abs/2312.09238
[192] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang,
B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang, J. Dai, Ghost in the
minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge
and memory (2023). arXiv:2305.17144.
URL https://arxiv.org/abs/2305.17144
[193] S. C. Duncan, Minecraft, beyond construction and survival,
Well Played 1 (1) (2011) 1–22.
[194] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion,
C. Goy, Y. Gao, H. Henry, M. Mattar, D. Lange, Unity: A
general platform for intelligent agents (2020). arXiv:1809.
02627.
URL https://arxiv.org/abs/1809.02627
[195] L. Hu, J. Tang, G. Zou, Z. Li, J. Zeng, M. Li, Simulation optimization of highway hard shoulder running based
on multi-agent deep deterministic policy gradient algorithm, Alexandria Engineering Journal 117 (2025) 99–115.
doi:https://doi.org/10.1016/j.aej.2024.12.110.
URL
https://www.sciencedirect.com/science/
article/pii/S1110016824017095
[196] Anonymous, A generalist hanabi agent, in: Submitted to The
Thirteenth International Conference on Learning Representations, 2024, under review.
URL https://openreview.net/forum?id=pCj2sLNoJq
[197] F. Bredell, H. A. Engelbrecht, J. C. Schoeman, Augmenting
the action space with conventions to improve multi-agent cooperation in hanabi (2024). arXiv:2412.06333.
URL https://arxiv.org/abs/2412.06333
[198] M. Kölle, Y. Erpelding, F. Ritz, T. Phan, S. Illium, C. LinnhoffPopien, Aquarium: A comprehensive framework for exploring predator-prey dynamics through multi-agent reinforcement
learning algorithms (2024). arXiv:2401.07056.
URL https://arxiv.org/abs/2401.07056
[199] A. Chatterjee, M. A. Abbasi, E. Venturino, J. Zhen,
M. Haque, A predator–prey model with prey refuge: under a stochastic and deterministic environment, Nonlinear
Dynamics 112 (15) (2024) 13667–13693. doi:10.1007/
s11071-024-09756-9.
URL https://doi.org/10.1007/s11071-024-09756-9
[200] Z. Mandi, S. Jain, S. Song, Roco: Dialectic multi-robot collaboration with large language models, in: 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024,
pp. 286–299. doi:10.1109/ICRA57147.2024.10610855.
[201] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang,
M. S. Bernstein, Generative agents: Interactive simulacra of
human behavior, in: Proceedings of the 36th Annual ACM
Symposium on User Interface Software and Technology (UIST
’23), ACM, ACM, 2023, p. October 2023. doi:10.1145/
3586183.3606763.
[202] G. Kovac, R. Portelas, P. F. Dominey, P.-Y. Oudeyer, The socialAI school: Insights from developmental psychology towards artificial socio-cultural agents, in: First Workshop on
Theory of Mind in Communicating Agents, 2023.
URL https://openreview.net/forum?id=Y5r8Wa67Ob
[203] G. Kovač, R. Portelas, P. F. Dominey, P.-Y. Oudeyer, The socialai school: a framework leveraging developmental psychol-

ogy toward artificial socio-cultural agents, Frontiers in Neurorobotics 18, part of the Research Topic: Theory of Mind
in Robots and Intelligent Systems (2024). doi:10.3389/
fnbot.2024.1396359.
[204] G. Mukobi, H. Erlebach, N. Lauffer, L. Hammond, A. Chan,
J. Clifton, Welfare diplomacy: Benchmarking language model
cooperation (2024).
URL https://openreview.net/forum?id=AKJLnDgzkm
[205] J. Boubin, C. Burley, P. Han, B. Li, B. Porter, C. Stewart, Programming and deployment of autonomous swarms using multi-agent reinforcement learning (2021). arXiv:2105.
10605.
URL https://arxiv.org/abs/2105.10605
[206] T. Li, F. Xie, Y. Xiong, Q. Feng, Multi-arm robot task planning
for fruit harvesting using multi-agent reinforcement learning
(2023). arXiv:2303.00460.
URL https://arxiv.org/abs/2303.00460
[207] A. Mahajan, S. Hegde, E. Shay, D. Wu, A. Prins, Comparative
analysis of multi-agent reinforcement learning policies for crop
planning decision support (2024). arXiv:2412.02057.
URL https://arxiv.org/abs/2412.02057
[208] L. Benke, M. Papasimeon, T. Miller, Modelling strategic
deceptive planning in adversarial multi-agent systems, in:
S. Sarkadi, B. Wright, P. Masters, P. McBurney (Eds.), Deceptive AI, Springer International Publishing, Cham, 2021, pp.
76–83.
[209] A. V. Sutagundar, S. Manvi, Context aware multisensor image
fusion for military sensor networks using multi-agent system,
International Journal of Ad hoc, Sensor Ubiquitous Computing
2 (1) (2011) 147–167. doi:10.5121/ijasuc.2011.2113.
URL http://dx.doi.org/10.5121/ijasuc.2011.2113
[210] S. Vangaru, D. Rosen, D. Green, R. Rodriguez, M. Wiecek,
A. Johnson, A. M. Jones, W. C. Headley, A multi-agent reinforcement learning testbed for cognitive radio applications
(2024). arXiv:2410.21521.
URL https://arxiv.org/abs/2410.21521
[211] Z. Wang, L. Wang, Q. Yi, Y. Liu, A marl based multitarget tracking algorithm under jamming against radar (2024).
arXiv:2412.12547.
URL https://arxiv.org/abs/2412.12547
[212] A. Alexopoulos, T. Schmidt, E. Badreddin, Cooperative pursue
in pursuit-evasion games with unmanned aerial vehicles, in:
2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), 2015, pp. 4538–4543. doi:10.1109/
IROS.2015.7354022.
[213] D. Luo, Z. Fan, Z. Yang, Y. Xu, Multi-uav cooperative
maneuver decision-making for pursuit-evasion using improved madrl, Defence Technology 35 (2024) 187–197.
doi:https://doi.org/10.1016/j.dt.2023.11.013.
URL
https://www.sciencedirect.com/science/
article/pii/S221491472300301X
[214] D. Wang, Multi-agent reinforcement learning for safe driving
in on-ramp merging of autonomous vehicles, in: 2024 14th
International Conference on Cloud Computing, Data Science
& Engineering (Confluence), 2024, pp. 644–651. doi:10.
1109/Confluence60223.2024.10463500.
[215] N. Aboueleneen, Y. Bello, A. Albaseer, A. R. Hussein, M. Abdallah, E. Hossain, Distributed traffic control in complex dynamic roadblocks: A multi-agent deep rl approach (2024).
arXiv:2501.00211.
URL https://arxiv.org/abs/2501.00211
[216] J. Yu, L. Liang, C. Guo, Z. Guo, S. Jin, G. Y. Li, Heterogeneous
multi-agent reinforcement learning for distributed channel access in wlans (2024). arXiv:2412.14218.
URL https://arxiv.org/abs/2412.14218

49

[217] K. Sun, H. Yu, Reinforcement learning for freeway lanechange regulation via connected vehicles (2024). arXiv:
2412.04341.
URL https://arxiv.org/abs/2412.04341
[218] T. Azfar, R. Ke, Traffic co-simulation framework empowered
by infrastructure camera sensing and reinforcement learning
(2024). arXiv:2412.03925.
URL https://arxiv.org/abs/2412.03925
[219] R. Bokade, X. Jin, Offlight: An offline multi-agent reinforcement learning framework for traffic signal control (2024).
arXiv:2411.06601.
URL https://arxiv.org/abs/2411.06601
[220] D. K. Kwesiga, S. C. Vishnoi, A. Guin, M. Hunter, Integrating
transit signal priority into multi-agent reinforcement learning
based traffic signal control (2024). arXiv:2411.19359.
URL https://arxiv.org/abs/2411.19359
[221] Y. Zhang, G. Zheng, Z. Liu, Q. Li, H. Zeng, Marlens:
Understanding multi-agent reinforcement learning for
traffic signal control via visual analytics, IEEE Transactions on Visualization and Computer Graphics (2024)
1–16doi:10.1109/tvcg.2024.3392587.
URL
http://dx.doi.org/10.1109/TVCG.2024.
3392587
[222] J. Xue, D. Zhang, R. Xiong, Y. Wang, E. Liu, A two-stage
based social preference recognition in multi-agent autonomous
driving system (2023). arXiv:2310.03303.
URL https://arxiv.org/abs/2310.03303
[223] J. Liu, P. Hang, X. Na, C. Huang, J. Sun, Cooperative decisionmaking for cavs at unsignalized intersections: A marl approach
with attention and hierarchical game priors (2024). arXiv:
2409.05712.
URL https://arxiv.org/abs/2409.05712
[224] C. Formanek, L. Beyers, C. R. Tilbury, J. P. Shock, A. Pretorius, Putting data at the centre of offline multi-agent reinforcement learning (2024). arXiv:2409.12001.
URL https://arxiv.org/abs/2409.12001
[225] R. Zhang, J. Hou, F. Walter, S. Gu, J. Guan, F. Röhrbein,
Y. Du, P. Cai, G. Chen, A. Knoll, Multi-agent reinforcement
learning for autonomous driving: A survey (2024). arXiv:
2408.09675.
URL https://arxiv.org/abs/2408.09675
[226] S. Kotoku, T. Mihana, A. Röhm, R. Horisaki, Decentralized
multi-agent reinforcement learning algorithm using a clustersynchronized laser network (2024). arXiv:2407.09124.
URL https://arxiv.org/abs/2407.09124
[227] M. Hua, D. Chen, K. Jiang, F. Zhang, J. Wang, B. Wang,
Q. Zhou, H. Xu, Communication-efficient marl for platoon
stability and energy-efficiency co-optimization in cooperative
adaptive cruise control of cavs (2024). arXiv:2406.11653.
URL https://arxiv.org/abs/2406.11653
[228] K. DeMarco, E. Squires, M. Day, C. Pippin, Simulating collaborative robots in a massive multi-agent game environment
(scrimmage), in: N. Correll, M. Schwager, M. Otte (Eds.), Distributed Autonomous Robotic Systems, Springer International
Publishing, Cham, 2019, pp. 283–297.
[229] Y. Xie, Y. Liu, Z. Ma, L. Shi, X. Wang, W. Yuan, M. O. Jackson, Q. Mei, How different ai chatbots behave? benchmarking
large language models in behavioral economics games (2024).
arXiv:2412.12362.
URL https://arxiv.org/abs/2412.12362
[230] W. Lin, J. Roberts, Y. Yang, S. Albanie, Z. Lu, K. Han, Beyond
outcomes: Transparent assessment of llm reasoning in games
(2024). arXiv:2412.13602.
URL https://arxiv.org/abs/2412.13602
[231] F. Jia, J. Zheng, F. Li, Decentralized intelligence in gamefi:

Embodied ai agents and the convergence of defi and virtual
ecosystems (2024). arXiv:2412.18601.
URL https://arxiv.org/abs/2412.18601
[232] C. Yin, G. Zhouhong, D. Zhaohan, Y. Zheyu, C. Shaosheng,
X. Yiqian, F. Hongwei, C. Ping, Mirage: Exploring how large
language models perform in complex social interactive environments (2025). arXiv:2501.01652.
URL https://arxiv.org/abs/2501.01652
[233] Z. Zhang, Y. Lan, Y. Chen, L. Wang, X. Wang, H. Wang, Dvm:
Towards controllable llm agents in social deduction games
(2025). arXiv:2501.06695.
URL https://arxiv.org/abs/2501.06695
[234] A. Gonzalez-Bonorino, M. Capra, E. Pantoja, Llms model nonweird populations: Experiments with synthetic cultural agents
(2025). arXiv:2501.06834.
URL https://arxiv.org/abs/2501.06834
[235] X. Luo, F. Ding, F. Yang, Y. Zhou, J. Loo, H. H. Tew, C. Liu,
Senserag: Constructing environmental knowledge bases with
proactive querying for llm-based autonomous driving (2025).
arXiv:2501.03535.
URL https://arxiv.org/abs/2501.03535
[236] D. Mahmud, H. Hajmohamed, S. Almentheri, S. Alqaydi,
L. Aldhaheri, R. A. Khalil, N. Saeed, Integrating llms with its:
Recent advances, potentials, challenges, and future directions
(2025). arXiv:2501.04437.
URL https://arxiv.org/abs/2501.04437
[237] Z. Peng, Y. Wang, X. Han, L. Zheng, J. Ma, Learningflow: Automated policy learning workflow for urban driving with large
language models (2025). arXiv:2501.05057.
URL https://arxiv.org/abs/2501.05057
[238] A. Karagounis, Leveraging large language models for enhancing autonomous vehicle perception (2024). arXiv:2412.
20230.
URL https://arxiv.org/abs/2412.20230
[239] X. Luo, F. Ding, R. Chen, R. Panda, J. Loo, S. Zhang, ”what’s
happening”- a human-centered multimodal interpreter explaining the actions of autonomous vehicles (2025). arXiv:2501.
05322.
URL https://arxiv.org/abs/2501.05322
[240] H. Gao, Y. Zhao, Application of vision-language model to
pedestrians behavior and scene understanding in autonomous
driving (2025). arXiv:2501.06680.
URL https://arxiv.org/abs/2501.06680
[241] D. Hegde, R. Yasarla, H. Cai, S. Han, A. Bhattacharyya, S. Mahajan, L. Liu, R. Garrepalli, V. M. Patel, F. Porikli, Distilling multi-modal large language models for autonomous driving (2025). arXiv:2501.09757.
URL https://arxiv.org/abs/2501.09757
[242] R. Wang, X. He, R. Yu, W. Qiu, B. An, Z. Rabinovich,
Learning efficient multi-agent communication: An information bottleneck approach, in: H. D. III, A. Singh (Eds.),
Proceedings of the 37th International Conference on Machine
Learning, Vol. 119 of Proceedings of Machine Learning
Research, PMLR, 2020, pp. 9908–9918.
URL
https://proceedings.mlr.press/v119/
wang20i.html
[243] A. van den Oord, O. Vinyals, K. Kavukcuoglu, Neural discrete
representation learning, in: Proceedings of the 31st International Conference on Neural Information Processing Systems,
NIPS’17, Curran Associates Inc., Red Hook, NY, USA, 2017,
p. 6309–6318.
[244] F. Ye, J. Chen, Y. Tian, T. Jiang, Cooperative task assignment of a heterogeneous multi-uav system using an adaptive
genetic algorithm, Electronics 9 (4) (2020). doi:10.3390/
electronics9040687.

50

URL https://www.mdpi.com/2079-9292/9/4/687
[245] J. Wang, D. Ye, Z. Lu, Mutual-information regularized multiagent policy iteration, in: A. Oh, T. Naumann, A. Globerson,
K. Saenko, M. Hardt, S. Levine (Eds.), Advances in Neural
Information Processing Systems, Vol. 36, Curran Associates,
Inc., 2023, pp. 2617–2635.
URL
https://proceedings.neurips.
cc/paper_files/paper/2023/file/
0799492e7be38b66d10ead5e8809616d-Paper-Conference.
pdf
[246] P. Li, H. Tang, T. Yang, X. Hao, T. Sang, Y. Zheng, J. Hao,
M. E. Taylor, Z. Wang, Pmic: Improving multi-agent reinforcement learning with progressive mutual information collaboration, ICML 2022 (2022).
[247] S. Kang, Y. Lee, S.-Y. Yun, DPM: Dual preferences-based
multi-agent reinforcement learning, in: ICML 2024 Workshop
on Models of Human Feedback for AI Alignment, 2024.
URL https://openreview.net/forum?id=TW3DIP2h5p
[248] Q. Fu, T. Qiu, J. Yi, Z. Pu, S. Wu, Concentration network
for reinforcement learning of large-scale multi-agent systems,
in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36, AAAI Press, Beijing, China, 2022, p. 21165.
doi:10.1609/aaai.v36i9.21165.
[249] B. Peng, T. Rashid, C. A. S. de Witt, P.-A. Kamienny, P. H. S.
Torr, W. Böhmer, S. Whiteson, Facmac: Factored multi-agent
centralised policy gradients (2021). arXiv:2003.06709.
URL https://arxiv.org/abs/2003.06709
[250] K. Pandya, M. Holia, Automating customer service using
langchain: Building custom open-source gpt chatbot for organizations (2023). arXiv:2310.05421.
URL https://arxiv.org/abs/2310.05421
[251] H. Du, S. Thudumu, R. Vasa, K. Mouzakis, A survey on
context-aware multi-agent systems: Techniques, challenges
and future directions (2024). arXiv:2402.01968.
URL https://arxiv.org/abs/2402.01968
[252] K. Hu, M. Li, Z. Song, K. Xu, Q. Xia, N. Sun, P. Zhou, M. Xia,
A review of research on reinforcement learning algorithms
for multi-agents, Neurocomputing 599 (2024) 128068. doi:
https://doi.org/10.1016/j.neucom.2024.128068.
URL
https://www.sciencedirect.com/science/
article/pii/S0925231224008397
[253] K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O’Sullivan,
H. D. Nguyen, Multi-agent collaboration mechanisms: A survey of llms (2025). arXiv:2501.06322.
URL https://arxiv.org/abs/2501.06322
[254] Y. Talebirad, A. Nadiri, Multi-agent collaboration: Harnessing the power of intelligent llm agents (2023). arXiv:2306.
03314.
URL https://arxiv.org/abs/2306.03314
[255] F. L. D. Silva, M. E. Taylor, A. H. R. Costa, Autonomously
reusing knowledge in multiagent reinforcement learning, in:
Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, International Joint Conferences
on Artificial Intelligence Organization, 2018, pp. 5487–5493.
[256] F. L. Da Silva, A. H. R. Costa, A survey on transfer learning for
multiagent reinforcement learning systems, J. Artif. Int. Res.
64 (1) (2019) 645–703. doi:10.1613/jair.1.11396.
URL https://doi.org/10.1613/jair.1.11396
[257] Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning, in: Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, Association for
Computing Machinery, New York, NY, USA, 2009, p. 41–48.
doi:10.1145/1553374.1553380.
URL https://doi.org/10.1145/1553374.1553380
[258] X. Wang, Y. Chen, W. Zhu, A survey on curriculum learn-

51

ing, IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (9) (2022) 4555–4576. doi:10.1109/TPAMI.
2021.3069908.
[259] L. Pinto, J. Davidson, R. Sukthankar, A. Gupta, Robust adversarial reinforcement learning, in: Proceedings of the 34th
International Conference on Machine Learning, PMLR, 2017,
pp. 2817–2826.
[260] W. Jin, N. Wang, T. Tao, B. Shi, H. Bi, B. Zhao, H. Wu,
H. Duan, G. Yang, A veracity dissemination consistencybased few-shot fake news detection framework by synergizing adversarial and contrastive self-supervised learning,
Scientific Reports 14 (1) (2024) 19470. doi:10.1038/
s41598-024-70039-9.
URL https://doi.org/10.1038/s41598-024-70039-9
[261] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr,
P. Kohli, S. Whiteson, Stabilising experience replay for deep
multi-agent reinforcement learning, in: Proceedings of the 34th
International Conference on Machine Learning - Volume 70,
ICML’17, JMLR.org, 2017, p. 1146–1155.
[262] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla,
O. Wiest, X. Zhang, Large language model based multi-agents:
A survey of progress and challenges, in: Proceedings of the
Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI ’24), IJCAI, 2024, pp. 8048–8057. doi:
10.24963/ijcai.2024/890.
URL https://doi.org/10.24963/ijcai.2024/890
[263] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,
Q. Chen, W. Peng, X. Feng, B. Qin, T. Liu, A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ACM Transactions on Information SystemsAccepted on 24 September 2024 (2024). doi:
10.1145/3703155.
URL https://doi.org/10.1145/3703155
[264] E. Alonso, D. Kudenko, D. Kazakov (Eds.), Adaptive agents
and multi-agent systems: adaptation and multi-agent learning,
Springer-Verlag, Berlin, Heidelberg, 2003.
[265] K. Tuyls, P. J. Hoen, K. Verbeeck, S. Sen (Eds.), Learning and
Adaption in Multi-Agent Systems, Vol. 3898 of Lecture Notes
in Computer Science, Springer, Berlin, Heidelberg, 2006.
URL
https://link.springer.com/book/10.1007/
978-3-540-33316-2
[266] S. Chen, Y. Liu, W. Han, W. Zhang, T. Liu, A survey on llmbased multi-agent system: Recent advances and new frontiers
in application (2025). arXiv:2412.17481.
URL https://arxiv.org/abs/2412.17481
[267] V. Dibia, Multi-agent llm applications — a review of current
research, tools, and challenges, accessed from Victor Dibia’s
newsletter (2023).
URL
https://newsletter.victordibia.com/p/
multi-agent-llm-applications-a-review
[268] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar,
D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll,
M. Lin, A. Mayhew, K. Collins, M. Molamohammadi, J. Burden, W. Zhao, S. Rismani, K. Voudouris, U. Bhatt, A. Weller,
D. Krueger, T. Maharaj, Harms from increasingly agentic algorithmic systems, in: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, FAccT
’23, Association for Computing Machinery, New York, NY,
USA, 2023, p. 651–666. doi:10.1145/3593013.3594033.
URL https://doi.org/10.1145/3593013.3594033
[269] A. Amirkhani, A. H. Barshooi, Consensus in multi-agent
systems: A review, Artificial Intelligence Review 55 (5)
(2022) 3897–3935, accessed: 2022-06-01. doi:10.1007/
s10462-021-10097-x.
URL https://doi.org/10.1007/s10462-021-10097-x

[270] G. Beydoun, G. Low, H. Mouratidis, B. Henderson-Sellers,
A security-aware metamodel for multi-agent systems
(mas), Information and Software Technology 51 (5)
(2009) 832–845, sPECIAL ISSUE: Model-Driven Development for Secure Information Systems.
doi:https:
//doi.org/10.1016/j.infsof.2008.05.003.
URL
https://www.sciencedirect.com/science/
article/pii/S0950584908000724
[271] X. Xie, C. Zhang, Y. Zhu, Y. N. Wu, S.-C. Zhu, Congestionaware multi-agent trajectory prediction for collision avoidance,
in: 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 13693–13700. doi:10.1109/
ICRA48506.2021.9560994.
[272] A. Stooke, K. Lee, P. Abbeel, M. Laskin, Decoupling representation learning from reinforcement learning, in: Proceedings
of the 38th International Conference on Machine Learning,
PMLR, 2021, pp. 9870–9879.
URL
https://proceedings.mlr.press/v139/
stooke21a.html
[273] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas,
E. Grefenstette, S. Whiteson, T. Rocktäschel, A survey of reinforcement learning informed by natural language, in: Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI-19, 2019, pp. 6309–6317.
doi:10.24963/ijcai.2019/880.
URL https://doi.org/10.24963/ijcai.2019/880
[274] R. P. Poudel, H. Pandya, S. Liwicki, R. Cipolla, Recore: Regularized contrastive representation learning of world model,
in: 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 22904–22913. doi:
10.1109/CVPR52733.2024.02161.
[275] W. Choi, W. K. Kim, S. Kim, H. Woo, Efficient policy adaptation with contrastive prompt ensemble for embodied agents,
in: Proceedings of the 37th International Conference on Neural
Information Processing Systems, NIPS ’23, Curran Associates
Inc., Red Hook, NY, USA, 2024.
[276] F. Paischer, T. Adler, V. Patil, A. Bitto-Nemling, M. Holzleitner, S. Lehner, H. Eghbal-Zadeh, S. Hochreiter, History
compression via language models in reinforcement learning,
in: K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu,
S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine Learning, Vol. 162 of Proceedings of
Machine Learning Research, PMLR, 2022, pp. 17156–17185.
URL
https://proceedings.mlr.press/v162/
paischer22a.html
[277] F. Paischer, T. Adler, M. Hofmarcher, S. Hochreiter, Semantic
HELM: A human-readable memory for reinforcement learning, in: Thirty-seventh Conference on Neural Information Processing Systems, 2023.
URL https://openreview.net/forum?id=ebMPmx5mr7
[278] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,
G. Krueger, I. Sutskever, Learning transferable visual models
from natural language supervision, in: M. Meila, T. Zhang
(Eds.), Proceedings of the 38th International Conference
on Machine Learning, Vol. 139 of Proceedings of Machine
Learning Research, PMLR, 2021, pp. 8748–8763.
URL
https://proceedings.mlr.press/v139/
radford21a.html
[279] A. van den Oord, Y. Li, O. Vinyals, Representation learning with contrastive predictive coding (2019). arXiv:1807.
03748.
URL https://arxiv.org/abs/1807.03748
[280] M. Laskin, A. Srinivas, P. Abbeel, Curl: contrastive unsupervised representations for reinforcement learning, in: Proceed-

ings of the 37th International Conference on Machine Learning, ICML’20, JMLR.org, 2020.
[281] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. Courville,
P. Bachman, Data-efficient reinforcement learning with selfpredictive representations, in: International Conference on
Learning Representations, 2021.
URL
https://openreview.net/forum?id=
uCQfPZwRaUu
[282] M. Kwon, S. M. Xie, K. Bullard, D. Sadigh, Reward design
with language models, in: The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=10uNUgI5Kl
[283] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, L. Ma, Self-refined
large language model as automated reward function designer
for deep reinforcement learning in robotics (2023). arXiv:
2309.06687.
URL https://arxiv.org/abs/2309.06687
[284] Y. Wu, Y. Fan, P. P. Liang, A. Azaria, Y. Li, T. M. Mitchell,
Read and reap the rewards: Learning to play atari with the help
of instruction manuals, in: A. Oh, T. Naumann, A. Globerson,
K. Saenko, M. Hardt, S. Levine (Eds.), Advances in Neural
Information Processing Systems, Vol. 36, Curran Associates,
Inc., 2023, pp. 1009–1023.
URL
https://proceedings.neurips.
cc/paper_files/paper/2023/file/
034d7bfeace2a9a258648b16fc626298-Paper-Conference.
pdf
[285] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, P.-Y.
Oudeyer, Grounding large language models in interactive environments with online reinforcement learning, in: Proceedings of the 40th International Conference on Machine Learning, ICML’23, JMLR.org, 2023.
[286] K. Chu, X. Zhao, C. Weber, M. Li, S. Wermter, Accelerating
reinforcement learning of robotic manipulations via feedback
from large language models, in: Bridging the Gap between
Cognitive Science and Robot Learning in the Real World: Progresses and New Directions, 2024.
URL https://openreview.net/forum?id=MBeeqmD8Zk
[287] C. Kim, Y. Seo, H. Liu, L. Lee, J. Shin, H. Lee, K. Lee, Guide
your agent with adaptive multimodal rewards, in: Proceedings
of the 37th International Conference on Neural Information
Processing Systems, NIPS ’23, Curran Associates Inc., Red
Hook, NY, USA, 2024.
[288] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas,
H.-T. L. Chiang, T. Erez, L. Hasenclever, J. Humplik, B. Ichter,
T. Xiao, P. Xu, A. Zeng, T. Zhang, N. Heess, D. Sadigh, J. Tan,
Y. Tassa, F. Xia, Language to rewards for robotic skill synthesis
(2023). arXiv:2306.08647.
URL https://arxiv.org/abs/2306.08647
[289] A. Adeniji, A. Xie, C. Sferrazza, Y. Seo, S. James, P. Abbeel,
Language reward modulation for pretraining reinforcement
learning (2024).
URL https://openreview.net/forum?id=SWRFC2EupO
[290] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,
S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,
S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, P. Clark, Self-refine: Iterative refinement with selffeedback (2023). arXiv:2303.17651.
URL https://arxiv.org/abs/2303.17651
[291] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, A. Anandkumar, Eureka: Humanlevel reward design via coding large language models, in:
The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=IEduRUO55F

52

[292] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong, Y. Yang,
T. Yu, Text2reward: Reward shaping with language models for
reinforcement learning, in: The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=tUM39YTRxH
[293] M. Janner, Q. Li, S. Levine, Offline reinforcement learning as
one big sequence modeling problem, in: Proceedings of the
35th International Conference on Neural Information Processing Systems, NIPS ’21, Curran Associates Inc., Red Hook, NY,
USA, 2024.
[294] R. Shi, Y. Liu, Y. Ze, S. S. Du, H. Xu, Unleashing the power of
pre-trained language models for offline reinforcement learning,
in: The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=AY6aM13gGF
[295] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen,
D.-A. Huang, E. Akyürek, A. Anandkumar, J. Andreas, I. Mordatch, A. Torralba, Y. Zhu, Pre-trained language models for interactive decision-making, in: Proceedings of the 36th International Conference on Neural Information Processing Systems,
NIPS ’22, Curran Associates Inc., Red Hook, NY, USA, 2024.
[296] L. Mezghani, P. Bojanowski, K. Alahari, S. Sukhbaatar, Think
before you act: Unified policy for interleaving language reasoning with actions, in: Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023.
URL https://openreview.net/forum?id=HQmhQIi6mN
[297] J. Grigsby, L. Fan, Y. Zhu, Amago: Scalable in-context reinforcement learning for adaptive agents (2024). arXiv:2310.
09971.
URL https://arxiv.org/abs/2310.09971
[298] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu,
P. Wohlhart, S. Welker, A. Wahid, Q. Vuong, Vanhoucke, Rt2: Vision-language-action models transfer web knowledge to
robotic control, in: J. Tan, M. Toussaint, K. Darvish (Eds.),
Proceedings of The 7th Conference on Robot Learning, Vol.
229 of Proceedings of Machine Learning Research, PMLR,
2023, pp. 2165–2183.
[299] S. Yao, R. Rao, M. Hausknecht, K. Narasimhan, Keep CALM
and explore: Language models for action generation in textbased games, in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.),
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Association for
Computational Linguistics, Online, 2020, pp. 8736–8754.
doi:10.18653/v1/2020.emnlp-main.704.
URL https://aclanthology.org/2020.emnlp-main.
704/
[300] H. Hu, D. Sadigh, Language instructed reinforcement learning
for human-ai coordination, in: Proceedings of the 40th International Conference on Machine Learning, ICML’23, JMLR.org,
2023.
[301] Z. Zhou, B. Hu, C. Zhao, P. Zhang, B. Liu, Large language
model as a policy teacher for training reinforcement learning
agents, in: Proceedings of the Thirty-Third International Joint
Conference on Artificial Intelligence, IJCAI ’24, 2025. doi:
10.24963/ijcai.2024/627.
URL https://doi.org/10.24963/ijcai.2024/627
[302] C. Chen, J. Yoon, Y.-F. Wu, S. Ahn, Transdreamer: Reinforcement learning with transformer world models, in: Deep RL
Workshop NeurIPS 2021, 2021.
URL
https://openreview.net/forum?id=
sVrzVAL90sA
[303] D. Das, S. Chernova, B. Kim, State2explanation: Conceptbased explanations to benefit agent learning and user
understanding, in: A. Oh, T. Naumann, A. Globerson,
K. Saenko, M. Hardt, S. Levine (Eds.), Advances in Neural

Information Processing Systems, Vol. 36, Curran Associates,
Inc., 2023, pp. 67156–67182.
URL
https://proceedings.neurips.
cc/paper_files/paper/2023/file/
d4387c37b3b06e55f86eccdb8cd1f829-Paper-Conference.
pdf
[304] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein,
A. Dragan, Learning to model the world with language (2024).
arXiv:2308.01399.
URL https://arxiv.org/abs/2308.01399
[305] J. Robine, M. Höftmann, T. Uelwer, S. Harmeling,
Transformer-based world models are happy with 100k
interactions, in: ICLR, 2023.
URL
https://openreview.net/forum?id=
TdBaDGCpjly
[306] R. P. K. Poudel, H. Pandya, C. Zhang, R. Cipolla, Langwm:
Language grounded world model (2023). arXiv:2311.
17593.
URL https://arxiv.org/abs/2311.17593
[307] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein,
A. Dragan, Learning to model the world with language (2024).
URL https://openreview.net/forum?id=eWLOoaShEH
[308] Z. Yang, K. Ren, X. Luo, M. Liu, W. Liu, J. Bian, W. Zhang,
D. Li, Towards applicable reinforcement learning: Improving the generalization and sample efficiency with policy ensemble, in: L. D. Raedt (Ed.), Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence,
IJCAI-22, International Joint Conferences on Artificial Intelligence Organization, 2022, pp. 3659–3665, main Track. doi:
10.24963/ijcai.2022/508.
URL https://doi.org/10.24963/ijcai.2022/508

53

