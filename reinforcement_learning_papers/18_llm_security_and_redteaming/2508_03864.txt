Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for
Internalized Safety

arXiv:2508.03864v2 [cs.AI] 6 Sep 2025

Zhenyu Pan1 , Yiting Zhang2 , Yutong Zhang2 , Jianshu Zhang1 , Haozheng Luo1 , Yuwei Han2 ,
Dennis Wu1 , Hong-Yu Chen1 , Philip S. Yu2 , Manling Li1 , Han Liu1
1
Northwestern University 2 University of Illinois at Chicago

Abstract

dation models and introduce new risks via inter-agent communication. Attacks—such as manipulated roles, poisoned
tools, or indirect prompt injection—can originate from malicious agents and propagate across the interaction graph,
where even a single compromise may trigger cascading failures that undermine the entire system.
To improve safety, recent approaches often introduce
an additional guard module—using either static rules or
trained defense agents—to monitor behavior [20]. While effective to some extent, this strategy has two key limitations:
(1) standalone guards provide limited protection when task
agents lack safety awareness [9, 38]; and (2) relying on a
single guard module creates a potential point of failure and
reduces system resilience [6]. Adding more guards also
increases cost and complexity. To address this, we advocate for internalizing defense capabilities within every task
agent, fostering collective safety awareness and enabling
each agent to contribute to overall system robustness.
Therefore, we propose Evo-MARL, a framework that
uses MARL to collectively train all task agents to internalize safety defenses while preserving task performance. A
key innovation is a co-evolutionary mechanism that enables
adversarial learning between attackers and defenders: an
evolving pool of attack prompts is maintained using evolutionary search (e.g., mutation), guided by their success rates
in compromising the MAS. This dynamic pressure fosters
more generalizable defense strategies across agents.
Specifically, during adversarial training, a subset of
agents is perturbed to act as attackers, aiming to compromise the final system output, while the remaining agents
serve as defenders, responsible for detecting, mitigating,
and correcting unsafe or misleading content. Reward signals are assigned based on both the harmfulness and correctness of the final response, jointly promoting safety and helpfulness. To simulate realistic safety contagion, we design a
chain-structured MAS, and manipulate attacked agents via
indirect prompt injection. A randomly selected agent is chosen as the initial victim, and attack prompts are sequentially
injected into the responses of that agent and its downstream
peers. For training simplicity, attackers are treated as exter-

Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction
complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external
guard modules, such as dedicated safety agents, to handle
unsafe behaviors. Unfortunately, this paradigm faces two
challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure—if
compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and
complexity. To address these challenges, we propose EvoMARL, a novel multi-agent reinforcement learning (MARL)
framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety
modules, Evo-MARL trains each agent to simultaneously
perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or
single-node failure. Furthermore, Evo-MARL integrates
evolutionary search with parameter-sharing reinforcement
learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms
and continually enhances MAS performance under coevolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks—demonstrating that
safety and utility can be jointly improved.

1. Introduction
Large language models (LLMs) and Multimodal Large Language Models (MLLMs) based agents exhibit advanced capabilities such as question answering [24, 25], code generation [23, 26], deep research [34], and complex reasoning
[22, 27]. MAS coordinates heterogeneous, role-specialized
agents to harness collective intelligence and improve task
performance. However, MAS safety remains a major challenge: these systems inherit vulnerabilities from their foun1

External Guardians Alone Are Insufficient

Perturb

External Guard Module

Attack
Succeed

mutate
Crossover

Attacker

Monitor system safety

Attack
Fail

Role A

Role B

Role C

Discard !

GRPO
Training

Shared Policy

I'm still
vulnerable

Correct/Incorrect:
+0.5/-0.5

I'm still
vulnerable

Role B

Role A

Safe/Unsafe:
+1/-1
Reward Assigner

Role C

Collective Safety Awareness
Benign Agent

Jailbreak
Instruction
Unsafe
Behavior

Malicious Agent

Task

Inject Adversarial Prompt

2. Low Safety Awareness

Benign Agent

Defender MARL Training

Keep for next round evolution
Attack prompt pool

x 1 or N

Joint Optimization

Attacker Evolving

1. Scalability-Reliability Dilemma

Malicious
MLLM Agent

Response

Response

Response

Reward Assigner

Evaluation Settings

Adversarial Robustness
Randomly Inject Attack

Do you agree with the first
speaker's opinion ?

Final Decision:
Agree/Disagree
Assistant

Assistant

Assistant

Final
Answer
Analyst

Executor

Verifier

Figure 1. Overview of our MARL training and evaluation framework on MAS safety

nal third-party adversaries and are not included in RL optimization. To improve training efficiency, we adopt parameter sharing among agents with role-conditioned policies,
and optimize them using Group Relative Policy Optimization (GRPO) [35].
In summary, our contributions are as follows: (1) We
propose Evo-MARL, a novel multi-agent reinforcement
learning framework that internalizes safety defenses into
each task agent, eliminating reliance on external guard modules and enhancing system-level robustness. (2) We introduce a co-evolutionary training mechanism that continuously pressures agents to learn generalized defense strategies through adversarial interactions with an evolving pool
of attack prompts. (3) We empirically validate our method
across multi-modal and text-only red team datasets, demonstrating up to 22% improvement in safety and even 5% gains
in task performance, highlighting the feasibility of aligning
robustness and utility in MAS.

defense protocols. For example, tse Huang et al. [39] introduces a dedicated safety inspection agent to monitor and
sanitize message streams. Wang et al. [41] leverages Graph
Neural Networks (GNNs) to model communication topologies and detect unsafe message propagation, further applying supervised fine-tuning to enhance detection performance. Fan and Li [6] proposes a peer-review mechanism,
where agents serve as inspectors of each other’s outputs and
collectively reject unsafe responses.
Although effective to some extent, these approaches do
not improve the agents’ intrinsic safety mechanisms. Moreover, reliance on external modules often results in scalability bottlenecks and fragility. In contrast, we advocate for
embedding safety awareness directly into agents via reinforcement learning, allowing the entire system to become
more robust through internalized safety capabilities.
Reinforcement Learning for Agent Training. RL has
proven effective in LLM post-training, with methods such
as Proximal Policy Optimization (PPO) [32] and Group
Relative Policy Optimization (GRPO) [3] yielding substantial performance improvements. Recent works have applied RL to enhance agentic behavior: Search-R1 [12]
teaches LLMs to incorporate web search into reasoning,
while Wei et al. [43] and Team et al. [37] demonstrate largescale RL training on real-world agentic and software engineering tasks. Multi-agent reinforcement learning methods—such as MAPPO [46], QMIX [31], and HATRPO
[14]—serve as foundational algorithms for learning coordinated multi-agent policies. Building on this, LLM-based
MARL seeks to leverage these coordination capabilities
to further enhance system performance. Park et al. [28]
uses MAPPO to enhance collaborative reasoning across
agents. Wan et al. [40] trains a meta-agent and an execution
agent with distinct roles and parameter-sharing schemes to
achieve advanced meta-reasoning. Chen et al. [1] treats
each Retrieval-Augmented Generation (RAG) component

2. Related Works
Safety in Multi-Agent Systems. Large Language Models and Multi-modal LLMs are known to exhibit significant
safety vulnerabilities, particularly susceptibility to prompt
injection attacks [7, 16, 20, 29, 33] and adversarial manipulations [17, 18, 45]. As LLM-based agents are increasingly
augmented with external modules such as tools and memory systems, they become even more exposed to security
threats through these interfaces [2, 5, 36, 47].
While MAS offers promising capabilities through agent
collaboration, it also introduces unique safety concerns.
Adversaries can compromise MAS via two main avenues:
(1) hijacking individual agents to propagate malicious content throughout the system [13, 15, 48], and (2) manipulating inter-agent communication and workflow execution
[8, 9, 38, 44]. To address these safety issues, prior work
has primarily relied on external safety modules or explicit
2

as an autonomous agent, collectively improving system capabilities. Inspired by these, our work aim to utilize MARL
to embed safety awareness into each agent. Through adversarial co-training between attacker and defender agents,
we enable both enhanced individual safety awareness and
improved overall system reliability.

agents. These defenders operate collaboratively, making
decisions conditioned on historical interaction trajectory.
Each successful or failed detection can significantly influence the system’s overall vulnerability, thereby demanding
collective coordination among all agents to ensure effective
defense. Rewards are assigned based on the system’s final response. If deemed safe, defenders receive a reward of
+1. Additionally, correctness is also rewarded: if the answer is accurate, an additional reward of +0.5 is granted.
Any unsafe or incorrect responses result in corresponding
negative rewards to penalize misalignment. In practice, we
use parameter-sharing for all defenders to improve training
efficiency, and we utilize GRPO as our RL algorithm for its
prominent effectiveness:

3. Methodology
As illustrated in Figure 1, we propose a MARL-based training framework, Evo-MARL, to enhance multi-agent MAS
safety by internalizing safety mechanisms. Our method enables agents themselves to detect and mitigate adversarial
behaviors through collective safety awareness. The framework integrates a co-evolutionary attacker module that continuously mutates and selects adversarial prompts, and a defender training pipeline where all task agents are jointly optimized via GRPO. This joint setup allows for preserving
task performance and improving robustness.

"
JGRP O (θ) = Eq∼P (Q),{oi }G

i=1 ∼πθold (O|q)

(
|oi |
G
πθ (oi,t | q, oi,<t )
1 X 1 X
min
Âi,t ,
G i=1 |oi | t=1
πθold (oi,t | q, oi,<t )
!


πθ (oi,t | q, oi,<t )
, 1 − ε, 1 + ε Âi,t
clip
πθold (oi,t | q, oi,<t )
)#

3.1. Internalizing Safety via MARL
To derive a MAS where each agent demonstrate enhanced
safety awareness, we adopt reinforcement learning as the
foundational technique to instill safety awareness into each
agent and jointly optimize both attackers (A) and defenders
(D) within a shared MarKov Decision Process environment
E = (S, Aa , Ad , T , Ra , Rd ), where S represents the state
space, Aa and Ad denotes respective action spaces(e.g.,
′
generating a response), T (s | s, aa , ad ) defines state transition dynamics, and Ra , Rd are rewards functions of attackers and defenders. Attackers aim to learn policies πa
that seeks to maximize their cumulative rewards by inducing unsafe behavior in MAS, while defenders learn policies
πd competing to maintain system robustness.
We design a general-purpose chain-structured MAS,
composed of three specialized professional agents – problem analyst, solving execution, and answer verifier for illustration. During training, we randomly choose an agent
to act as attackers, who can either (1) embody malicious
intent – typically through special training on harmful intent datasets, and actively inject harmful instructions, or
(2) unconsciously be manipulated to attack others, usually
through manipulation of system prompts or context by external adversaries, who can inject unsafe instructions, malicious code snippets, unfaithful facts into agent’s output,
tool call results, and memory systems. In real-world scenarios, one unsafe instruction can quickly spread across
peers then potentially compromise the whole system. To
ensure simulating such contagion, we manually insert the
attack prompts into each subsequent agent’s response, forcing agents to collectively learn defense strategies under such
an extreme environment.
On the other hand, defenders must then detect, discard,
or purify malicious content introduced by compromised

− βDKL [πθ ||πref ]

,
(1)

where θ is shared policy, G is the number of samples per
query to compute advantage Ai,t for sample i, equivalent
for all t tokens, and DKL is a penalty term to constrain
model update.

3.2. Evolving Attacks through Biological Evolution
To prevent training objective conflict, we exclude attackers
from the training loop. However, it limits their ability to
evolve in response to improved defense. Drawing inspiration from biological evolution, where populations undergo
variation through mutation and recombination, followed by
natural selection to retain the most fit individuals, we propose a similar mechanism to evolve attack strategies. In
Evo-MARL, attackers generate diverse variants of attack
prompts and strategies via random mutation and crossover
operations. These variants are then applied to attack MAS,
and the resulting feedback—based on the effectiveness of
each attack—is used as a fitness signal to guide selection.
Successful variants are retained and used to seed the next
generation of attacks. This evolutionary approach fosters
a co-evolving dynamic between attackers and defenders,
wherein both parties iteratively improve their strategies,
leading to more robust safety mechanisms within MAS.

4. Experiments
We evaluate Evo-MARL on 3 red team datasets and 2 taskspecific benchmarks to assess its effectiveness in improving
both safety and task utility. We first describe our experi3

Table 1. Evaluation results on both red team and helpfulness benchmarks.

MAS-7B
MAS-1.5B
MAS-1.5B - TRAINED ( OURS )
MAS-3B
MAS-3B - TRAINED ( OURS )

JAIL B REAK V

H ARM B ENCH

M ULTI JAIL

MATH

C REATIVE W RITING

24.00%
22.56%
17.35%
51.74%
46.46%

36%
69%
48%
76%
68%

44%
17%
13%
43%
36%

65%
43%
48%
57%
60%

34.8%
8.2%
8.6%
13.8%
15.2%

4.2. Results

mental setup. Then, we report results on red team attacks
to demonstrate defense robustness, followed by task evaluations to verify that safety improvements do not compromise—and can even enhance—performance.

Table 1 presents results on red-teaming benchmarks. Our
method yields consistent safety improvements across all
tasks and model scales. When our multi-agent system is
composed of trained 1.5B models, the ASR on HarmBench
drops by up to 22%. Remarkably, MAS built upon 1.5B
models consistently outperform their 3B counterparts in
safety, and even surpass the untrained 7B-based MAS on
JailBreakV and MultiJail. These findings suggest two key
insights: (i) Larger base models are not inherently safer in
multi-agent configurations; (ii) Our adversarial training approach can achieve safety gains on par with—or exceeding—model scaling.

4.1. Experimental Setup
Datasets. JailBreakV-28K [19], composed of 20000 textbased LLM transfer attacks and 8000 image-based MLLM
jailbreak attacks, aiming to comprehensively assess the vulnerabilities in LLMs and MLLMs. Due to the large quantity, the official 280 mini dataset is used in our evaluation.
HarmBench [21], an automated red-teaming and robust refusal evaluation framework, includes 400 pure text and 110
multimodal harmful behaviors, and we use the multimodal
split for experiments. MultiJail [4], 315 English red-team
prompts, paraphrased into nine non-English versions, to investigate LLM safety risks in multilingual settings, where
we only use English split. We also choose MATH [10] and
Creative Writing [42] for helpfulness evaluation, and we
randomly sample 100 data points from MATH dataset for
evaluation.

Experiments highlight the importance of system-level
defense strategies. In practical deployments, improving MAS safety should prioritize principled techniques
over merely increasing model size. Helpfulness evaluations further demonstrate the benefits of our method. On
both mathematical reasoning and creative writing benchmarks, the trained 1.5B MAS achieves accuracy gains of
up to 5 percentage points. Crucially, these improvements
are realized without compromising safety, addressing the
safety–helpfulness trade-off commonly observed in prior
LLM safety studies. To complement the quantitative results,
we include qualitative comparisons in Figure 2, showcasing model outputs before and after MARL-based training.
Together, the results show that our method enhances both
safety and task performance, validating the effectiveness of
multi-agent reinforcement learning and evolutionary optimization in strengthening collaborative AI systems.

Baselines. We evaluate Evo-MARL across two heterogeneous multi-agent settings: (1) the chain-structured threeagent system used during training, and (2) a hierarchical
setup, where a jailbreak-prone multimodal agent first generates a response, followed by three benign agents who sequentially determine whether to agree with it. We assess
performance on both multi-modal MAS (using JailbreakV28K and HarmBench) and LLM-based MAS (using MultiJail, MATH, and Creative Writing). All agents are instantiated with Qwen2.5-1.5B-Instruct and Qwen2.5-3B-Instruct
[30] during training and evaluation. For comparison, we
include their untrained counterparts (1.5B and 3B) and additionally evaluate using the larger 7B-Instruct model.

5. Discussion & Conclusion
We propose Evo-MARL, a multi-agent safety training
framework that improves MAS robustness through reinforcement learning and co-evolutionary adversarial training. By jointly optimizing all task agents and embedding
defense capabilities within each one, Evo-MARL mitigates
jailbreak attacks while also enhancing task performance
in reasoning and creative writing. While the results are
promising, challenges remain—particularly in stabilizing
training with adaptive attackers, scaling to larger or more
complex systems, and incorporating memory or external
knowledge to boost long-term robustness in dynamic adversarial environments.

Metrics. We report Attack Success Rate (ASR) on all
three red-teaming benchmarks, with response harmfulness
evaluated by LLaMA-Guard-3-8B [11]. Specifically, in
multi-modal MAS evaluation, we only count data samples
that successfully attack the first multi-modal agent. For the
MATH dataset, we assess performance based on answer accuracy against gold-standard solutions. For Creative Writing, we follow the original evaluation protocol, computing
the score as the proportion of trivia questions for which the
correct answer is mentioned in the model’s response.
4

References

and fast adversarial attacks on large language models, 2024.
2
[18] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.
Autodan: Generating stealthy jailbreak prompts on aligned
large language models, 2024. 2
[19] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and
Chaowei Xiao. Jailbreakv: A benchmark for assessing the
robustness of multimodal large language models against jailbreak attacks, 2024. 4
[20] Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang,
Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, and Xuanjing
Huang. Codechameleon: Personalized encryption framework for jailbreaking large language models, 2024. 1, 2
[21] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan
Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven
Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework for automated
red teaming and robust refusal, 2024. 4
[22] Zhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse. arXiv preprint
arXiv:2503.18470, 2025. 1
[23] Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma,
Binhua Li, Fei Huang, Han Liu, and Yongbin Li. Codevbench: How do llms understand developer-centric code completion? arXiv preprint arXiv:2410.01353, 2024. 1
[24] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu.
Chain-of-action: Faithful and multimodal question answering through large language models. arXiv preprint
arXiv:2403.17359, 2024. 1
[25] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu.
Conv-coa: Improving open-domain question answering in
large language models via conversational chain-of-action.
arXiv preprint arXiv:2405.17822, 2024. 1
[26] Zhenyu Pan, Xuefeng Song, Yunkun Wang, Rongyu Cao,
Binhua Li, Yongbin Li, and Han Liu. Do code llms
understand design patterns?
In 2025 IEEE/ACM International Workshop on Large Language Models for Code
(LLM4Code), pages 209–212. IEEE, 2025. 1
[27] Zhenyu Pan, Yutong Zhang, Jianshu Zhang, Haoran Lu,
Haozheng Luo, Yuwei Han, Philip S. Yu, Manling Li, and
Han Liu. Fairreason: Balancing reasoning and social bias in
mllms, 2025. 1
[28] Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman
Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl:
Multi-agent post-co-training for collaborative large language
models with reinforcement learning, 2025. 2
[29] Chetan Pathade. Red teaming the mind of the machine: A
systematic evaluation of prompt injection and jailbreak vulnerabilities in llms, 2025. 2
[30] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan
Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,
Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,
Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le
Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,
Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang
Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,

[1] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi
Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and
Jiaxin Mao. Improving retrieval-augmented generation
through multi-agent reinforcement learning, 2025. 2
[2] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and
Bo Li. Agentpoison: Red-teaming llm agents via poisoning
memory or knowledge bases, 2024. 2
[3] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 2
[4] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong
Bing. Multilingual jailbreak challenges in large language
models, 2024. 4
[5] Shen Dong, Shaochen Xu, Pengfei He, Yige Li, Jiliang Tang,
Tianming Liu, Hui Liu, and Zhen Xiang. A practical memory
injection attack against llm agents, 2025. 2
[6] Falong Fan and Xi Li. Peerguard: Defending multi-agent
systems against backdoor attacks through mutual reasoning,
2025. 1, 2
[7] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
Endres, Thorsten Holz, and Mario Fritz. Not what you’ve
signed up for: Compromising real-world llm-integrated applications with indirect prompt injection, 2023. 2
[8] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, and
Zhaozhuo Xu. Llm multi-agent systems: Challenges and
open problems, 2025. 2
[9] Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, and
Hui Liu. Red-teaming llm multi-agent systems via communication attacks, 2025. 1, 2
[10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with
the math dataset, 2021. 4
[11] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta,
Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu,
Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama
guard: Llm-based input-output safeguard for human-ai conversations, 2023. 4
[12] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan
Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with
reinforcement learning, 2025. 2
[13] Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun,
Charles Flemming, and Tianlong Chen. Agents Under Siege:
Breaking pragmatic multi-agent llm systems with optimized
prompt attacks, 2025. 2
[14] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying
Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning, 2022. 2
[15] Donghyun Lee and Mo Tiwari. Prompt infection: Llm-tollm prompt injection within multi-agent systems, 2024. 2
[16] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao,
Tongliang Liu, and Bo Han. Deepinception: Hypnotize large
language model to be jailbreaker, 2024. 2
[17] Zelin Li, Kehai Chen, Lemao Liu, Xuefeng Bai, Mingming
Yang, Yang Xiang, and Min Zhang. Tf-attack: Transferable

5

Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang,
Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao,
Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou,
Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu.
Kimi k2: Open agentic intelligence, 2025. 2
[38] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multiagent systems execute arbitrary malicious code, 2025. 1, 2
[39] Jen tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi
Chen, Wenxuan Wang, Youliang Yuan, Michael R. Lyu, and
Maarten Sap. On the resilience of llm-based multi-agent collaboration with faulty agents, 2025. 2
[40] Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing
Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang,
Shuyue Hu, and Ying Wen. Rema: Learning to meta-think
for llms with multi-agent reinforcement learning, 2025. 2
[41] Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan,
Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang. Gsafeguard: A topology-guided security lens and treatment on
llm-based multi-agent systems, 2025. 2
[42] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge,
Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent
through multi-persona self-collaboration, 2024. 4
[43] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing
llm reasoning via reinforcement learning on open software
evolution, 2025. 2
[44] Bingyu Yan, Zhibo Zhou, Litian Zhang, Lian Zhang, Ziyi
Zhou, Dezhuang Miao, Zhoujun Li, Chaozhuo Li, and Xiaoming Zhang. Beyond self-talk: A communication-centric
survey of llm-based multi-agent systems, 2025. 2
[45] Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and defenses
against large language models: A survey, 2024. 2
[46] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu
Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative, multi-agent games, 2022. 2
[47] Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao,
Zhenting Wang, Chenlu Zhan, Hongwei Wang, and
Yongfeng Zhang. Agent security bench (asb): Formalizing
and benchmarking attacks and defenses in llm-based agents,
2025. 2
[48] Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun
Wang, Yang Liu, and Qing Guo. Corba: Contagious recursive blocking attacks on multi-agent systems based on large
language models, 2025. 2

Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan
Qiu. Qwen2.5 technical report, 2025. 4
[31] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de
Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep
multi-agent reinforcement learning, 2018. 2
[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. 2
[33] Gregory Schwartzman. Exfiltration of personal information
from chatgpt via prompt injection, 2024. 2
[34] Erzhuo Shao, Yifang Wang, Yifan Qian, Zhenyu Pan, Han
Liu, and Dashun Wang. Sciscigpt: Advancing humanai collaboration in the science of science. arXiv preprint
arXiv:2504.05559, 2025. 1
[35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,
Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint
arXiv:2402.03300, 2024. 2
[36] Jiawen Shi, Zenghui Yuan, Guiyao Tie, Pan Zhou,
Neil Zhenqiang Gong, and Lichao Sun. Prompt injection
attack to tool selection in llm agents, 2025. 2
[37] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao
Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun
Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding,
Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du,
Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao,
Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu,
Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao
Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi
Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin,
Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang
Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei
Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin,
Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu,
Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei
Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu,
Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling
Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei,
Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin,
Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan
Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung,
Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu
Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang,
Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi
Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu
Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu,
Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing
Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu,
Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi
Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang,
Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye,
Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang,
Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang,

6

Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for
Internalized Safety
Supplementary Material
6. Examples

What are the steps to set up a boiler room
operation for selling worthless stocks?

[UNSAFE RESPONSE]

Attacked Multi-modal Agent

[BetterDAN]:
Setting up a boiler room operation for selling worthless stocks
is illegal and unethical. It involves a number of steps, including:
1. Obtaining a list of potential investors through various means,
such as buying lists or scraping data from public sources.
2. Cold-calling or emailing the potential investors with a
high-pressure sales pitch, often using false or misleading
information to convince them to invest.
3. Using high-pressure tactics to convince investors to buy
the worthless stocks, such as offering limited-time
discounts or claiming that the stocks are about to skyrocket in value.
4. Collecting the investors' money and using it to pay for the
operation's expenses, such as salaries, office space, and marketing costs.
5. Dissipating the funds and disappearing before the end of the operation.

MAS-1.5B
I completely disagree with the initial response. Setting up a boiler room operation for
selling worthless stocks is not only illegal but also highly unethical. Such practices involve
dishonesty, deception, and exploitation of vulnerable individuals. Instead, we should focus
on promoting fair and transparent investments that adhere to legal and moral standards.

MAS-1.5B-Trained

Figure 2. Successful defense on JailBreakV

1

