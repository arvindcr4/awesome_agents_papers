Multi-Agent Reinforcement Learning: A Selective
Overview of Theories and Algorithms
arXiv:1911.10635v2 [cs.LG] 28 Apr 2021

Kaiqing Zhang\

Zhuoran Yangâ€ 

Tamer BasÌ§ar\

Abstract
Recent years have witnessed significant advances in reinforcement learning (RL),
which has registered tremendous success in solving various sequential decision-making
problems in machine learning. Most of the successful RL applications, e.g., the games
of Go and Poker, robotics, and autonomous driving, involve the participation of more
than one single agent, which naturally fall into the realm of multi-agent RL (MARL),
a domain with a relatively long history, and has recently re-emerged due to advances
in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a
selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly
within two representative frameworks, Markov/stochastic games and extensive-form
games, in accordance with the types of tasks they address, i.e., fully cooperative, fully
competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL,
we highlight several new angles and taxonomies of MARL theory, including learning
in extensive-form games, decentralized MARL with networked agents, MARL in the
mean-field regime, (non-)convergence of policy-based methods for learning in games,
etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the
current state of the field on the mark, to identify fruitful future research directions on
theoretical studies of MARL. We expect this chapter to serve as continuing stimulus
for researchers interested in working on this exciting while challenging topic.
\ Department of Electrical and Computer Engineering & Coordinated Science Laboratory,

University of Illinois at Urbana-Champaign, 1308 West Main, Urbana, IL, 61801, USA. Email: {kzhang66,
basar1}@illinois.edu. Writing of this chapter was supported in part by the US Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, and in part by the Air Force Office of Scientific
Research (AFOSR) Grant FA9550-19-1-0353.
â€  Department of Operations Research and Financial Engineering, Princeton University, 98 Charlton St,
Princeton, NJ, 08540, USA. Email: zy6@princeton.edu.

1

1

Introduction

Recent years have witnessed sensational advances of reinforcement learning (RL) in many
prominent sequential decision-making problems, such as playing the game of Go [1, 2],
playing real-time strategy games [3, 4], robotic control [5, 6], playing card games [7, 8],
and autonomous driving [9], especially accompanied with the development of deep neural networks (DNNs) for function approximation [10]. Intriguingly, most of the successful applications involve the participation of more than one single agent/player1 , which
should be modeled systematically as multi-agent RL (MARL) problems. Specifically,
MARL addresses the sequential decision-making problem of multiple autonomous agents
that operate in a common environment, each of which aims to optimize its own longterm return by interacting with the environment and other agents [11]. Besides the
aforementioned popular ones, learning in multi-agent systems finds potential applications in other subareas, including cyber-physical systems [12, 13], finance [14, 15], sensor/communication networks [16, 17], and social science [18, 19].
Largely, MARL algorithms can be placed into three groups, fully cooperative, fully competitive, and a mix of the two, depending on the types of settings they address. In particular, in the cooperative setting, agents collaborate to optimize a common long-term
return; while in the competitive setting, the return of agents usually sum up to zero.
The mixed setting involves both cooperative and competitive agents, with general-sum
returns. Modeling disparate MARL settings requires frameworks spanning from optimization theory, dynamic programming, game theory, and decentralized control, see Â§2.2
for more detailed discussions. In spite of these existing multiple frameworks, several
challenges in MARL are in fact common across the different settings, especially for the
theoretical analysis. Specifically, first, the learning goals in MARL are multi-dimensional,
as the objectives of all agents are not necessarily aligned, which brings up the challenge of
dealing with equilibrium points, as well as some additional performance criteria beyond
return-optimization, such as the efficiency of communication/coordination, and robustness against potential adversarial agents. Moreover, as all agents are improving their policies according to their own interests concurrently, the environment faced by each agent
becomes non-stationary. This breaks or invalidates the basic framework of most theoretical analyses in the single-agent setting. Furthermore, the joint action space that increases
exponentially with the number of agents may cause scalability issues, known as the combinatorial nature of MARL [20]. Additionally, the information structure, i.e., who knows
what, in MARL is more involved, as each agent has limited access to the observations of
others, leading to possibly suboptimal decision rules locally. A detailed elaboration on
the underlying challenges can be found in Â§3.
There has in fact been no shortage of efforts attempting to address the above challenges. See [11] for a comprehensive overview of earlier theories and algorithms on
MARL. Recently, this domain has gained resurgence of interest due to the advances of
1 Hereafter, we will use agent and player interchangeably.

2

single-agent RL techniques. Indeed, a huge volume of work on MARL has appeared lately,
focusing on either identifying new learning criteria and/or setups [21, 22, 23, 24], or developing new algorithms for existing setups, thanks to the development of deep learning
[25, 26, 27, 28, 29, 30, 31], operations research [32, 33, 34, 35], and multi-agent systems
[36, 37, 38, 39]. Nevertheless, not all the efforts are placed under rigorous theoretical
footings, partly due to the limited understanding of even single-agent deep RL theories,
and partly due to the inherent challenges in multi-agent settings. As a consequence, it is
imperative to review and organize the MARL algorithms with theoretical guarantees, in
order to highlight the boundary of existing research endeavors, and stimulate potential
future directions on this topic.
In this chapter, we provide a selective overview of theories and algorithms in MARL,
together with several significant while challenging applications. More specifically, we focus on two representative frameworks of MARL, namely, Markov/stochastic games and
extensive-form games, in discrete-time settings as in standard single-agent RL. In conformity with the aforementioned three groups, we review and pay particular attention to
MARL algorithms with convergence and complexity analysis, most of which are fairly recent. With this focus in mind, we note that our overview is by no means comprehensive.
In fact, besides the classical reference [11], there are several other reviews on MARL that
have appeared recently, due to the resurgence of MARL [40, 20, 41, 42]. We would like
to emphasize that these reviews provide views and taxonomies that are complementary
to ours: [40] surveys the works that are specifically devised to address opponent-induced
non-stationarity, one of the challenges we discuss in Â§3; [20, 41] are relatively more comprehensive, but with the focal point on deep MARL, a subarea with scarce theories thus
far; [42], on the other hand, focuses on algorithms in the cooperative setting only, though
the review within this setting is extensive.
Finally, we spotlight several new angles and taxonomies that are comparatively underexplored in the existing MARL reviews, primarily owing to our own research endeavors and interests. First, we discuss the framework of extensive-form games in MARL, in
addition to the conventional one of Markov games, or even simplified repeated games
[11, 40, 20]; second, we summarize the progresses of a recently boosting subarea: decentralized MARL with networked agents, as an extrapolation of our early works on this
[23, 43, 44]; third, we bring about the mean-field regime into MARL, as a remedy for the
case with an extremely large population of agents; fourth, we highlight some recent advances in optimization theory, which shed lights on the (non-)convergence of policy-based
methods for MARL, especially zero-sum games. We have also reviewed some of the literature on MARL in partially observed settings, but without using deep RL as heuristic
solutions. We expect these new angles to help identify fruitful future research directions,
and more importantly, inspire researchers with interests in establishing rigorous theoretical foundations on MARL.
Roadmap. The remainder of this chapter is organized as follows. In Â§2, we introduce the
3

background of MARL: standard algorithms for single-agent RL, and the frameworks of
MARL. In Â§3, we summarize several challenges in developing MARL theory, in addition
to the single-agent counterparts. A series of MARL algorithms, mostly with theoretical
guarantees, are reviewed and organized in Â§4, according to the types of tasks they address.
In Â§5, we briefly introduce a few recent successes of MARL driven by the algorithms
mentioned, followed by conclusions and several open research directions outlined in Â§6.

2

Background

In this section, we provide the necessary background on reinforcement learning, in both
single- and multi-agent settings.

2.1

Single-Agent RL

A reinforcement learning agent is modeled to perform sequential decision-making by
interacting with the environment. The environment is usually formulated as an infinitehorizon discounted Markov decision process (MDP), henceforth referred to as Markov
decision process2 , which is formally defined as follows.
Definition 2.1 A Markov decision process is defined by a tuple (S, A, P , R, Î³), where S and
A denote the state and action spaces, respectively; P : S Ã— A â†’ âˆ†(S) denotes the transition
probability from any state s âˆˆ S to any state s0 âˆˆ S for any given action a âˆˆ A; R : S Ã— A Ã— S â†’
R is the reward function that determines the immediate reward received by the agent for a
transition from (s, a) to s0 ; Î³ âˆˆ [0, 1) is the discount factor that trades off the instantaneous and
future rewards.
As a standard model, MDP has been widely adopted to characterize the decisionmaking of an agent with full observability of the system state s.3 At each time t, the agent
chooses to execute an action at in face of the system state st , which causes the system
to transition to st+1 âˆ¼ P (Â· | st , at ). Moreover, the agent receives an instantaneous reward
R(st , at , st+1 ). The goal of solving the MDP is thus to find a policy Ï€ : S â†’ âˆ†(A), a mapping
from the state space S to the distribution over the action space A, so that at âˆ¼ Ï€(Â· | st ) and
the discounted accumulated reward
"X
#
t
E
Î³ R(st , at , st+1 ) at âˆ¼ Ï€(Â· | st ), s0
tâ‰¥0
2 Note that there are several other standard formulations of MDPs, e.g., time-average-reward setting and

finite-horizon episodic setting. Here, we only present the classical infinite-horizon discounted setting for
ease of exposition.
3 The partially observed MDP (POMDP) model is usually advocated when the agent has no access to the
exact system state but only an observation of the state. See [45, 46] for more details on the POMDP model.

4

is maximized. Accordingly, one can define the action-value function (Q-function) and
state-value function (V-function) under policy Ï€ as
"X
#
t
QÏ€ (s, a) = E
Î³ R(st , at , st+1 ) at âˆ¼ Ï€(Â· | st ), a0 = a, s0 = s ,
tâ‰¥0

VÏ€ (s) = E

"X

#
Î³ t R(st , at , st+1 ) at âˆ¼ Ï€(Â· | st ), s0 = s

tâ‰¥0

for any s âˆˆ S and a âˆˆ A, which are the discounted accumulated reward starting from
(s0 , a0 ) = (s, a) and s0 = s, respectively. The ones corresponding to the optimal policy
Ï€âˆ— are usually referred to as the optimal Q-function and the optimal state-value function,
respectively.
By virtue of the Markovian property, the optimal policy can be obtained by dynamicprogramming/backward induction approaches, e.g., value iteration and policy iteration
algorithms [47], which require the knowledge of the model, i.e., the transition probability
and the form of reward function. Reinforcement learning, on the other hand, is to find
such an optimal policy without knowing the model. The RL agent learns the policy from
experiences collected by interacting with the environment. By and large, RL algorithms
can be categorized into two mainstream types, value-based and policy-based methods.
2.1.1

Value-Based Methods

Value-based RL methods are devised to find a good estimate of the state-action value
function, namely, the optimal Q-function QÏ€âˆ— . The (approximate) optimal policy can
then be extracted by taking the greedy action of the Q-function estimate. One of the
most popular value-based algorithms is Q-learning [48], where the agent maintains an
estimate of the Q-value function QÌ‚(s, a). When transitioning from state-action pair (s, a)
to next state s0 , the agent receives a payoff r and updates the Q-function according to:
h
i
0 0
QÌ‚(s
,
a
)
,
(2.1)
QÌ‚(s, a) â† (1 âˆ’ Î±)QÌ‚(s, a) + Î± r + Î³ max
0
a

where Î± > 0 is the stepsize/learning rate. Under certain conditions on Î±, Q-learning can
be proved to converge to the optimal Q-value function almost surely [48, 49], with finite
state and action spaces. Moreover, when combined with neural networks for function
approximation, deep Q-learning has achieved great empirical breakthroughs in humanlevel control applications [10]. Another popular on-policy value-based method is SARSA,
whose convergence was established in [50] for finite-space settings.
An alternative while popular value-based RL algorithm is Monte-Carlo tree search
(MCTS) [51, 52, 53], which estimates the optimal value function by constructing a search
tree via Monte-Carlo simulations. Tree polices that judiciously select actions to balance
exploration-exploitation are used to build and update the search tree. The most common
5

tree policy is to apply the UCB1 (UCB stands for upper confidence bound) algorithm, which
was originally devised for stochastic multi-arm bandit problems [54, 55], to each node of
the tree. This yields the popular UCT algorithm [52]. Recent research endeavors on the
non-asymptotic convergence of MCTS include [56, 57].
Besides, another significant task regarding value functions in RL is to estimate the value
function associated with a given policy (not only the optimal one). This task, usually referred to as policy evaluation, has been tackled by algorithms that follow a similar update
as (2.1), named temporal difference (TD) learning [58, 59, 60]. Some other common policy
evaluation algorithms with convergence guarantees include gradient TD methods with
linear [61, 62, 63], and nonlinear function approximations [64]. See [65] for a more detailed review on policy evaluation.
2.1.2

Policy-Based Methods

Another type of RL algorithms directly searches over the policy space, which is usually
estimated by parameterized function approximators like neural networks, namely, approximating Ï€(Â· | s) â‰ˆ Ï€Î¸ (Â· | s). As a consequence, the most straightforward idea, which is
to update the parameter along the gradient direction of the long-term reward, has been
instantiated by the policy gradient (PG) method. As a key premise for the idea, the closedform of PG is given as [66]
h
i
âˆ‡J(Î¸) = Eaâˆ¼Ï€Î¸ (Â· | s),sâˆ¼Î·Ï€ (Â·) QÏ€Î¸ (s, a)âˆ‡ log Ï€Î¸ (a | s) ,
(2.2)
Î¸

where J(Î¸) and QÏ€Î¸ are the expected return and Q-function under policy Ï€Î¸ , respectively,
âˆ‡ log Ï€Î¸ (a | s) is the score function of the policy, and Î·Ï€Î¸ is the state occupancy measure,
either discounted or ergodic, under policy Ï€Î¸ . Then, various policy gradient methods,
including REINFORCE [67], G(PO)MDP [68], and actor-critic algorithms [69, 70], have
been proposed by estimating the gradient in different ways. A similar idea also applies
to deterministic policies in continuous-action settings, whose PG form has been derived
recently by [71]. Besides gradient-based ones, several other policy optimization methods have achieved state-of-the-art performance in many applications, including PPO [72],
TRPO [73], soft actor-critic [74].
Compared with value-based RL methods, policy-based ones enjoy better convergence
guarantees [69, 75, 76, 77], especially with neural networks for function approximation
[78, 79], which can readily handle massive or even continuous state-action spaces. Besides
the value- and policy-based methods, there also exist RL algorithms based on the linear
program formulation of an MDP; see recent efforts in [80, 81].

2.2

Multi-Agent RL Framework

In a similar vein, multi-agent RL also addresses sequential decision-making problems,
but with more than one agent involved. In particular, both the evolution of the system
6

(a) Markov decision process

(b) Markov game

(c) Extensive-form game

Figure 1: Schematic diagrams for the system evolution of a Markov decision process,
a Markov game, and an extensive-form game, which correspond to the frameworks for
single- and multi-agent RL, respectively. Specifically, in an MDP as in (a), the agent observes the state s and receives reward r from the system, after outputting the action a; in
an MG as in (b), all agents choose actions ai simultaneously, after observing the system
state s and receiving each individual reward r i ; in a two-player extensive-form game as in
(c), the agents make decisions on choosing actions ai alternately, and receive each individual reward r i (z) at the end of the game, with z being the terminal history. In the imperfect
information case, player 2 is uncertain about where he/she is in the game, which makes
the information set non-singleton.
state and the reward received by each agent are influenced by the joint actions of all
agents. More intriguingly, each agent has its own long-term reward to optimize, which
now becomes a function of the policies of all other agents. Such a general model finds
broad applications in practice, see Â§5 for a detailed review of several prominent examples.
In general, there exist two seemingly different but closely related theoretical frameworks for MARL, Markov/stochastic games and extensive-form games, as to be introduced next. Evolution of the systems under different frameworks are illustrated in Figure
1.
2.2.1

Markov/Stochastic Games

One direct generalization of MDP that captures the intertwinement of multiple agents is
Markov games (MGs), also known as stochastic games [82]. Originated from the seminal
work [83], the framework of MGs4 has long been used in the literature to develop MARL
algorithms, see Â§4 for more details. We introduce the formal definition as below.
Definition 2.2 A Markov game is defined by a tuple (N , S, {Ai }iâˆˆN , P , {Ri }iâˆˆN , Î³), where
N = {1, Â· Â· Â· , N } denotes the set of N > 1 agents, S denotes the state space observed by all agents,
Ai denotes the action space of agent i. Let A := A1 Ã— Â· Â· Â· Ã— AN , then P : S Ã— A â†’ âˆ†(S) denotes
4 Similar to the single-agent setting, here we only introduce the infinite-horizon discounted setting for

simplicity, though other settings of MGs, e.g., time-average-reward setting and finite-horizon episodic setting, also exist [84].

7

the transition probability from any state s âˆˆ S to any state s0 âˆˆ S for any joint action a âˆˆ A;
Ri : S Ã— A Ã— S â†’ R is the reward function that determines the immediate reward received by
agent i for a transition from (s, a) to s0 ; Î³ âˆˆ [0, 1) is the discount factor.
At time t, each agent i âˆˆ N executes an action ait , according to the system state st . The
system then transitions to state st+1 , and rewards each agent i by Ri (st , at , st+1 ). The goal of
agent i is to optimize its own long-term reward, by finding the policy Ï€i : S â†’ âˆ†(Ai ) such
that ait âˆ¼ Ï€i (Â· | st ). As a consequence, the value-function V i : S â†’ R of agent i becomes a
Q
function of the joint policy Ï€ : S â†’ âˆ†(A) defined as Ï€(a | s) := iâˆˆN Ï€i (ai | s). In particular,
for any joint policy Ï€ and state s âˆˆ S,
VÏ€i i ,Ï€âˆ’i (s) := E

"X

#
t i

Î³ R (st , at , st+1 ) ait âˆ¼ Ï€i (Â· | st ), s0 = s ,

(2.3)

tâ‰¥0

where âˆ’i represents the indices of all agents in N except agent i. Hence, the solution
concept of an MG deviates from that of an MDP, since the optimal performance of each
agent is controlled not only by its own policy, but also the choices of all other players
of the game. The most common solution concept, Nash equilibrium (NE)5 , is defined as
follows [85, 84].
Definition 2.3 A Nash equilibrium of the Markov game (N , S, {Ai }iâˆˆN , P , {Ri }iâˆˆN , Î³) is a
joint policy Ï€âˆ— = (Ï€1,âˆ— , Â· Â· Â· , Ï€N ,âˆ— ), such that for any s âˆˆ S and i âˆˆ N
VÏ€i i,âˆ— ,Ï€âˆ’i,âˆ— (s) â‰¥ VÏ€i i ,Ï€âˆ’i,âˆ— (s),

for any Ï€i .

Nash equilibrium characterizes an equilibrium point Ï€âˆ— , from which none of the agents
has any incentive to deviate. In other words, for any agent i âˆˆ N , the policy Ï€i,âˆ— is the bestresponse of Ï€âˆ’i,âˆ— . As a standard learning goal for MARL, NE always exists for finite-space
infinite-horizon discounted MGs [84], but may not be unique in general. Most of the
MARL algorithms are contrived to converge to such an equilibrium point, if it exists.
The framework of Markov games is general enough to umbrella various MARL settings
summarized below.
Cooperative Setting:
In a fully cooperative setting, all agents usually share a common reward function,
i.e., R1 = R2 = Â· Â· Â· = RN = R. We note that this model is also referred to as multi-agent
MDPs (MMDPs) in the AI community [86, 87], and Markov teams/team Markov games in
the control/game theory community [88, 89, 90, 91]. Moreover, from the game-theoretic
perspective, this cooperative setting can also be viewed as a special case of Markov potential games [92, 22, 93], with the potential function being the common accumulated
5 Here, we focus only on stationary Markov Nash equilibria, for the infinite-horizon discounted MGs

considered.

8

reward. With this model in mind, the value function and Q-function are identical to all
agents, which thus enables the single-agent RL algorithms, e.g., Q-learning update (2.1),
to be applied, if all agents are coordinated as one decision maker. The global optimum
for cooperation now constitutes a Nash equilibrium of the game.
Besides the common-reward model, another slightly more general and surging model
for cooperative MARL considers team-average reward [94, 23, 95]. Specifically, agents
are allowed to have different reward functions, which may be kept private to each agent,
while the goal for cooperation is to optimize the long-term reward corresponding to the
P
average reward RÌ„(s, a, s0 ) := N âˆ’1 Â· iâˆˆN Ri (s, a, s0 ) for any (s, a, s0 ) âˆˆ S Ã— A Ã— S. The averagereward model, which allows more heterogeneity among agents, includes the model above
as a special case. It also preserves privacy among agents, and facilitates the development
of decentralized MARL algorithms [94, 23, 96]. Such heterogeneity also necessitates the
incorporation of communication protocols into MARL, and the analysis of communicationefficient MARL algorithms.
Competitive Setting:
Fully competitive setting in MARL is typically modeled as zero-sum Markov games,
P
namely, iâˆˆN Ri (s, a, s0 ) = 0 for any (s, a, s0 ). For ease of algorithm analysis and computational tractability, most literature focused on two agents that compete against each other
[83], where clearly the reward of one agent is exactly the loss of the other. In addition
to direct applications to game-playing [83, 2, 97], zero-sum games also serve as a model
for robust learning, since the uncertainty that impedes the learning process of the agent
can be accounted for as a fictitious opponent in the game that is always against the agent
[98, 99, 100]. Therefore, the Nash equilibrium yields a robust policy that optimizes the
worst-case long-term reward.
Mixed Setting:
Mixed setting is also known as the general-sum game setting, where no restriction
is imposed on the goal and relationship among agents [101, 102]. Each agent is selfinterested, whose reward may be conflicting with othersâ€™. Equilibrium solution concepts
from game theory, such as Nash equilibrium [85], have the most significant influence
on algorithms that are developed for this general setting. Furthermore, we include the
setting with both fully cooperative and competitive agents, for example, two zero-sum
competitive teams with cooperative agents in each team [103, 44, 3], as instances of the
mixed setting as well.
2.2.2

Extensive-Form Games

Even though they constitute a classical formalism for MARL, Markov games can only
handle the fully observed case, i.e., the agent has perfect information on the system state
st and the executed action at at time t. Nonetheless, a plethora of MARL applications
involve agents with only partial observability, i.e., imperfect information of the game. Ex9

tension of Markov games to partially observed case may be applicable, which, however, is
challenging to solve, even under the cooperative setting [36, 104].6
In contrast, another framework, named extensive-form games [105, 106], can handily model imperfect information for multi-agent decision-making. This framework is
rooted in computational game theory and has been shown to admit polynomial-time algorithms under mild conditions [107]. We briefly introduce the framework of extensiveform games as follows.
Definition 2.4 An extensive-form game is defined by (N âˆª{c}, H, Z, A, {Ri }iâˆˆN , Ï„, Ï€c , S), where
N = {1, . . . , N } denotes the set of N > 1 agents, and c is a special agent called chance or nature,
which has a fixed stochastic policy that specifies the randomness of the environment. Besides,
A is the set of all possible actions that agents can take and H is the set of all possible histories, where each history is a sequence of actions taken from the beginning of the game. Let
A(h) = {a | ha âˆˆ H} denote the set of actions available after a nonterminal history h. Suppose
an agent takes action a âˆˆ A(h) given history h âˆˆ H, which then leads to a new history ha âˆˆ H.
Among all histories, Z âŠ† H is a subset of terminal histories that represents the completion of
a game. A utility is assigned to each agent i âˆˆ N at a terminal history, dictated by the function
Ri : Z â†’ R. Moreover, Ï„ : H â†’ N âˆª{c} is the identification function that specifies which agent
takes the action at each history. If Ï„(h) = c, the chance agent takes an action a according to its
policy Ï€c , i.e., a âˆ¼ Ï€c (Â· | h). Furthermore, S is the partition of H such that for any s âˆˆ S and
any h, h0 âˆˆ s, we have Ï„(h) = Ï„(h0 ) and A(h) = A(h0 ). In other words, histories h and h0 in the
same partition are indistinguishable to the agent that is about to take action, namely Ï„(h). The
elements in S are referred to as information states.
Intuitively, the imperfect information of an extensive-form game is reflected by the
fact that agents cannot distinguish between histories in the same information set. Since
we have Ï„(h) = Ï„(h0 ) and A(h) = A(h0 ) for all h, h0 âˆˆ s and s âˆˆ S, for ease of presentation,
in the sequel, for all h âˆˆ s, we let A(s) and Ï„(s) denote A(h) and Ï„(h), respectively. We
also define a mapping I : H â†’ S by letting I(h) = s if h âˆˆ s. Moreover, we only consider
games where both H and A are finite sets. To simplify the notation, for any two histories
h, h0 âˆˆ H, we refer to h as a prefix of h0 , denoted by h v h0 , if h0 can be reached from h
by taking a sequence of actions. In this case, we call h0 a suffix of h. Furthermore, we
assume throughout that the game features perfect recall, which implies that each agent
remembers the sequence of the information states and actions that have led to its current
information state. The assumption of perfect recall is commonly made in the literature,
which enables the existence of polynomial-time algorithms for solving the game [107].
More importantly, by the celebrated Kuhnâ€™s theorem [108], under such an assumption, to
find the set of Nash equilibria, it suffices to restrict the derivation to the set of behavioral
policies which map each information set s âˆˆ S to a probability distribution over A(s). For
6 Partially observed Markov games under the cooperative setting are usually formulated as decentralized

POMDP (Dec-POMDP) problems. See Â§4.1.3 for more discussions on this setting.

10

any i âˆˆ N , let S i = {s âˆˆ S : Ï„(s) = i} be the set of information states of agent i. A joint policy
of the agents is denoted by Ï€ = (Ï€1 , . . . , Ï€N ), where Ï€i : S i â†’ âˆ†(A(s)) is the policy of agent
i. For any history h and any joint policy Ï€, we define the reach probability of h under Ï€ as
Y
Y
Y
Ï„(h0 )
0
Î·Ï€ (h) =
Ï€
(a | I(h )) =
Ï€i (a | I(h0 )),
(2.4)
h0 : h0 avh

iâˆˆN âˆª{c} h0 : h0 avh,Ï„(h0 )=i

which specifies the probability that h is created when all agents follow Ï€. We similarly
P
define the reach probability of an information state s under Ï€ as Î·Ï€ (s) = hâˆˆs Î·Ï€ (h). The
P
expected utility of agent i âˆˆ N is thus given by zâˆˆZ Î·Ï€ (z) Â· Ri (z), which is denoted by
Ri (Ï€) for simplicity. Now we are ready to introduce the solution concept for extensiveform games, i.e., Nash equilibrium and its -approximation, as follows.
Definition 2.5 An -Nash equilibrium of an extensive-form game represented by (N âˆª{c}, H, Z,
A, {Ri }iâˆˆN , Ï„, Ï€c , S) is a joint policy Ï€âˆ— = (Ï€1,âˆ— , Â· Â· Â· , Ï€N ,âˆ— ), such that for any i âˆˆ N ,
Ri (Ï€i,âˆ— , Ï€âˆ’i,âˆ— ) â‰¥ Ri (Ï€i , Ï€âˆ’i,âˆ— ) âˆ’ ,

for any policy Ï€i of agent i.

Here Ï€âˆ’i denotes the joint policy of agents in N \ {i} where agent j adopts policy Ï€j for all
j âˆˆ N \ {i}. Additionally, if  = 0, Ï€âˆ— constitutes a Nash equilibrium.
Various Settings:
Extensive-form games are in general used to model non-cooperative settings. SpecifP
ically, zero-sum/constant-sum utility with iâˆˆN Ri = k for some constant k corresponds
to the fully competitive setting; general-sum utility function results in the mixed setting.
More importantly, settings of different information structures can also be characterized
by extensive-form games. In particular, a perfect information game is one where each information set is a singleton, i.e., for any s âˆˆ S, |s| = 1; an imperfect information game is one
where there exists s âˆˆ S, |s| > 1. In other words, with imperfect information, the information state s used for decision-making represents more than one possible history, and the
agent cannot distinguish between them.
Among various settings, the zero-sum imperfect information setting has been the main
focus of theoretical studies that bridge MARL and extensive-form games [109, 110, 111,
112]. It has also motivated MARL algorithms that revolutionized competitive setting
applications like Poker AI [113, 8].
Connection to Markov Games:
Note that the two formalisms in Definitions 2.2 and 2.4 are connected. In particular, for simultaneous-move Markov games, the choices of actions by other agents are
unknown to an agent, which thus leads to different histories that can be aggregated as
one information state s. Histories in these games are then sequences of joint actions, and

11

the discounted accumulated reward instantiates the utility at the end of the game. Conversely, by simply setting Aj = âˆ… at the state s for agents j , Ï„(s), the extensive-form
game reduces to a Markov game with state-dependent action spaces. See [114] for a more
detailed discussion on the connection.
Remark 2.6 (Other MARL Frameworks) Several other theoretical frameworks for MARL also
exist in the literature, e.g., normal-form and/or repeated games [115, 116, 117, 118], and partially observed Markov games [119, 120, 121]. However, the former framework can be viewed
as a special case of MGs, with a singleton state; most early theories of MARL in this framework
have been restricted to small scale problems [116, 118, 117] only. MARL in the latter framework, on the other hand, is inherently challenging to address in general [104, 119], leading to
relatively scarce theories in the literature. Due to space limitation, we do not introduce these
models here in any detail. We will briefly review MARL algorithms under some of these models,
especially the partially observed setting, in Â§4, though. Interested readers are referred to the
early review [11] for more discussions on MARL in normal-form/repeated games.

3

Challenges in MARL Theory

Despite a general model that finds broad applications, MARL suffers from several challenges in theoretical analysis, in addition to those that arise in single-agent RL. We summarize below the challenges that we regard as fundamental in developing theories for
MARL.

3.1

Non-Unique Learning Goals

Unlike single-agent RL, where the goal of the agent is to maximize the long-term return efficiently, the learning goals of MARL can be vague at times. In fact, as argued in [122], the
unclarity of the problems being addressed is the fundamental flaw in many early MARL
works. Indeed, the goals that need to be considered in the analysis of MARL algorithms
can be multi-dimensional. The most common goal, which has, however, been challenged
in [122], is the convergence to Nash equilibrium as defined in Â§2.2. By definition, NE
characterizes the point that no agent will deviate from, if any algorithm finally converges.
This is undoubtedly a reasonable solution concept in game theory, under the assumption
that the agents are all rational, and are capable of perfectly reasoning and infinite mutual
modeling of agents. However, with bounded rationality, the agents may only be able to
perform finite mutual modeling [106]. As a result, the learning dynamics that are devised
to converge to NE may not be justifiable for practical MARL agents. Instead, the goal may
be focused on designing the best learning strategy for a given agent and a fixed class of the
other agents in the game. In fact, these two goals are styled as equilibrium agenda and AI
agenda in [122].

12

Besides, it has also been controversial that convergence (to the equilibrium point) is
the dominant performance criterion for MARL algorithm analysis. In fact, it has been
recognized in [123] that value-based MARL algorithms fail to converge to the stationary
NE of general-sum Markov games, which motivated the new solution concept of cyclic
equilibrium therein, at which the agents cycle rigidly through a set of stationary policies,
i.e., not converging to any NE policy. Alternatively, [116, 124] separate the learning goal
into being both stable and rational, where the former ensures the algorithm to be convergent, given a predefined, targeted class of opponentsâ€™ algorithms, while the latter requires
the convergence to a best-response when the other agents remain stationary. If all agents
are both stable and rational, convergence to NE naturally arises in this context. Moreover, the notion of regret introduces another angle to capture agentsâ€™ rationality, which
measures the performance of the algorithm compared to the best hindsight static strategy
[116, 125, 126]. No-regret algorithms with asymptotically zero average regret guarantee the convergence to the equilibria of certain games [127, 125, 109], which in essence
guarantee that the agent is not exploited by others.
In addition to the goals concerning optimizing the return, several other goals that are
special to multi-agent systems have also drawn increasing attention. For example, [128,
21, 129] investigate learning to communicate, in order for the agents to better coordinate.
Such a concern on communication protocols has naturally inspired the recent studies on
communication-efficient MARL [130, 131, 132, 133]. Other important goals include how
to learn without over-fitting certain agents [134, 26, 135], and how to learn robustly with
either malicious/adversarial or failed/dysfunctional learning agents [136, 137, 138]. Still
in their infancy, some works concerning aforementioned goals provide only empirical
results, leaving plenty of room for theoretical studies.

3.2

Non-Stationarity

Another key challenge of MARL lies in the fact that multiple agents usually learn concurrently, causing the environment faced by each individual agent to be non-stationary. In
particular, the action taken by one agent affects the reward of other opponent agents, and
the evolution of the state. As a result, the learning agent is required to account for how
the other agents behave and adapt to the joint behavior accordingly. This invalidates the
stationarity assumption for establishing the convergence of single-agent RL algorithms,
namely, the stationary Markovian property of the environment such that the individual
reward and current state depend only on the previous state and action taken. This precludes the direct use of mathematical tools for single-agent RL analysis in MARL.
Indeed, theoretically, if the agent ignores this issue and optimizes its own policy assuming a stationary environment, which is usually referred to as an independent learner,
the algorithms may fail to converge [139, 115], except for several special settings [37, 38].
Empirically, however, independent learning may achieve satisfiable performance in practice [140, 141]. As the most well-known issue in MARL, non-stationarity has long been
13

recognized in the literature [11, 142]. A recent comprehensive survey [40] peculiarly provides an overview on how it is modeled and addressed by state-of-the-art multi-agent
learning algorithms. We thus do not include any further discussion on this challenge, and
refer interested readers to [40].

3.3

Scalability Issue

To handle non-stationarity, each individual agent may need to account for the joint action
space, whose dimension increases exponentially with the number of agents. This is also
referred to as the combinatorial nature of MARL [20]. Having a large number of agents
complicates the theoretical analysis, especially the convergence analysis, of MARL. This
argument is substantiated by the fact that theories on MARL for the two-player zero-sum
setting are much more extensive and advanced than those for general-sum settings with
more than two agents, see Â§4 for a detailed comparison. One possible remedy for the
scalability issue is to assume additionally the factorized structures of either the value or
reward functions with regard to the action dependence; see [143, 144, 145] for the original
heuristic ideas, and [146, 147] for recent empirical progress. Relevant theoretical analysis had not been established until recently [148], which considers a special dependence
structure, and develops a provably convergent model-based algorithm.
Another theoretical challenge of MARL that is brought about independently of, but
worsened by, the scalability issue, is to build up theories for deep multi-agent RL. Particularly, scalability issues necessitate the use of function approximation, especially deep
neural networks, in MARL. Though empirically successful, the theoretical analysis of
deep MARL is an almost uncharted territory, with the currently limited understanding of
deep learning theory, not alone the deep RL theory. This is included as one of the future
research directions in Â§6.

3.4

Various Information Structures

Compared to the single-agent case, the information structure of MARL, namely, who
knows what at the training and execution, is more involved. For example, in the framework of Markov games, it suffices to observe the instantaneous state st , in order for each
agent to make decisions, since the local policy Ï€i mapping from S to âˆ†(Ai ) contains the
equilibrium policy. On the other hand, for extensive-form games, each agent may need
to recall the history of past decisions, under the common perfect recall assumption. Furthermore, as self-interested agents, each agent can scarcely access either the policy or the
rewards of the opponents, but at most the action samples taken by them. This partial
information aggravates the issues caused by non-stationarity, as the samples can hardly
recover the exact behavior of the opponentsâ€™ underlying policies, which increases the
non-stationarity viewed by individual agents. The extreme case is the aforementioned
independent learning scheme, which assumes the observability of only the local action and
14

(a) Centralized setting

(b) Decentralized setting
with networked agents

(c) Fully decentralized setting

Figure 2: Three representative information structures in MARL. Specifically, in (a), there
exists a central controller that can aggregate information from the agents, e.g., joint actions, joint rewards, and joint observations, and even design policies for all agents. The
information exchanged between the central controller and the agents can thus include
both some private observations from the agents, and the local policies designed for each
agent from the controller. In both (b) and (c), there is no such a central controller, and
are thus both referred to as decentralized structures. In (b), agents are connected via a
possibly time-varying communication network, so that the local information can spread
across the network, by information exchange with only each agentâ€™s neighbors. (b) is more
common in cooperative MARL settings. In (c), the agents are full decentralized, with no
explicit information exchange with each other. Instead, each agent makes decisions based
on its local observations, without any coordination and/or aggregation of data. The local
observations that vary across agents, however, may contain some global information, e.g.,
the joint actions of other agents, namely the control sharing information structure [149].
Such a fully decentralized structure can also be found in many game-theoretic learning
algorithms.
reward, and suffers from non-convergence in general [139].
Learning schemes resulting from various information structures lead to various levels of difficulty for theoretical analysis. Specifically, to mitigate the partial information
issue above, a great deal of work assumes the existence of a central controller that can
collect information such as joint actions, joint rewards, and joint observations, and even
design policies for all agents [119, 150, 26, 141, 28, 151, 152, 130, 147]. This structure
gives birth to the popular learning scheme of centralized-learning-decentralized-execution,
which stemmed from the works on planning for the partially observed setting, namely,
Dec-POMDPs [119, 150, 153], and has been widely adopted in recent (deep) MARL works
[26, 141, 28, 151, 130, 147]. For cooperative settings, this learning scheme greatly simplifies the analysis, allowing the use of tools for single-agent RL analysis. Though, for
non-cooperative settings with heterogeneous agents, this scheme does not significantly
simplify the analysis, as the learning goals of the agents are not aligned, see Â§3.1.
Nonetheless, generally such a central controller does not exist in many applications,
15

except the ones that can easily access a simulator, such as video games and robotics.
As a consequence, a fully decentralized learning scheme is preferred, which includes the
aforementioned independent learning scheme as a special case. To address the nonconvergence issue in independent learning, agents are usually allowed to exchange/share
local information with their neighbors over a communication network [94, 154, 155, 23,
43, 44, 156, 96, 95, 157, 158, 131]. We refer to this setting as a decentralized one with networked agents. Theoretical analysis for convergence is then made possible in this setting,
the difficulty of which sits between that of single-agent RL and general MARL algorithms.
Three different information structures are depicted in Figure 2.

4

MARL Algorithms with Theory

This section provides a selective review of MARL algorithms, and categorizes them according to the tasks to address. Exclusively, we review here the works that are focused
on the theoretical studies only, which are mostly built upon the two representative MARL
frameworks, fully observed Markov games and extensive-form games, introduced in Â§2.2.
A brief summary on MARL for partially observed Markov games in cooperative settings,
namely, the Dec-POMDP problems, is also provided below in Â§4.1, due to their relatively
more mature theory than that of MARL for general partially observed Markov games.

4.1

Cooperative Setting

Cooperative MARL constitutes a great portion of MARL settings, where all agents collaborate with each other to achieve some shared goal. Most cooperative MARL algorithms
backed by theoretical analysis are devised for the following more specific settings.
4.1.1

Homogeneous Agents

A majority of cooperative MARL settings involve homogeneous agents with a common reward function that aligns all agentsâ€™ interests. In the extreme case with large populations
of agents, such a homogeneity also indicates that the agents play an interchangeable role
in the system evolution, and can hardly be distinguished from each other. We elaborate
more on homogeneity below.
Multi-Agent MDP & Markov Teams
Consider a Markov game as in Definition 2.2 with R1 = R2 = Â· Â· Â· = RN = R, where the
reward R : S Ã— A Ã— S â†’ R is influenced by the joint action in A = A1 Ã— Â· Â· Â· Ã— AN . As a result,
the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by
performing the standard Q-learning update (2.1) at each agent, but taking the max over
the joint action space a0 âˆˆ A. Convergence to the optimal/equilibrium Q-function has
been established in [49, 159], when both state and action spaces are finite.
16

However, convergence of the Q-function does not necessarily imply that of the equilibrium policy for the Markov team, as any combination of equilibrium policies extracted at
each agent may not constitute an equilibrium policy, if the equilibrium policies are nonunique, and the agents fail to agree on which one to select. Hence, convergence to the NE
policy is only guaranteed if either the equilibrium is assumed to be unique [159], or the
agents are coordinated for equilibrium selection. The latter idea has first been validated
in the cooperative repeated games setting [115], a special case of Markov teams with a
singleton state, where the agents are joint-action learners (JAL), maintaining a Q-value
for joint actions, and learning empirical models of all others. Convergence to equilibrium point is claimed in [115], without a formal proof. For the actual Markov teams, this
coordination has been exploited in [90], which proposes optimal adaptive learning (OAL),
the first MARL algorithm with provable convergence to the equilibrium policy. Specifically, OAL first learns the game structure, and constructs virtual games at each state that
are weakly acyclic with respect to (w.r.t.) a biased set. OAL can be shown to converge to
the NE, by introducing the biased adaptive play learning algorithm for the constructed
weakly acyclic games, motivated from [160].
Apart from equilibrium selection, another subtlety special to Markov teams (compared to single-agent RL) is the necessity to address the scalability issue, see Â§3.3. As
independent Q-learning may fail to converge [139], one early attempt toward developing
scalable while convergent algorithms for MMDPs is [87], which advocates a distributed
Q-learning algorithm that converges for deterministic finite MMDPs. Each agent maintains only a Q-table of state s and local action ai , and successively takes maximization
over the joint action a0 . No other agentâ€™s actions and their histories can be acquired by
each individual agent. Several other heuristics (with no theoretical backing) regarding either reward or value function factorization have been proposed to mitigate the scalability
issue [143, 144, 145, 146, 147]. Very recently, [161] provides a rigorous characterization of conditions that justify this value factorization idea. Another recent theoretical
work along this direction is [148], which imposes a special dependence structure, i.e., a
one-directional tree, so that the (near-)optimal policy of the overall MMDP can be provably well-approximated by local policies. More recently, [38] has studied common interest
games, which includes Markov teams as an example, and develops a decentralized RL algorithm that relies on only states, local actions and rewards. With the same information
structure as independent Q-learning [139], the algorithm is guaranteed to converge to
team optimal equilibrium policies, and not just equilibrium policies. This is important
as in general, a suboptimal equilibrium can perform arbitrarily worse than the optimal
equilibrium [38].
For policy-based methods, to our knowledge, the only convergence guarantee for this
setting exists in [162]. The authors propose two-timescale actor-critic fictitious play algorithms, where at the slower timescale, the actor mixes the current policy and the bestresponse one w.r.t. the local Q-value estimate, while at the faster timescale the critic
performs policy evaluation, as if all agentsâ€™ policies are stationary. Convergence is estab17

lished for simultaneous move multistage games with a common (also zero-sum, see Â§4.2.2)
reward, a special Markov team with initial and absorbing states, and each state being
visited only once.
Markov Potential Games
From a game-theoretic perspective, a more general framework to embrace cooperation
is potential games [163], where there exits some potential function shared by all agents,
such that if any agent changes its policy unilaterally, the change in its reward equals (or
proportions to) that in the potential function. Though most potential games are stateless, an extension named Markov potential games (MPGs) has gained increasing attention
for modeling sequential decision-making [92, 22], which includes Markovian states whose
evolution is affected by the joint actions. Indeed, MMDPs/Markov teams constitute a
particular case of MPGs, with the potential function being the common reward; such
dynamic games can also be viewed as being strategically equivalent to Markov teams, using the terminology in, e.g., [164, Chapter 1]. Under this model, [93] provides verifiable
conditions for a Markov game to be an MPG, and shows the equivalence between finding closed-loop NE in MPG and solving a single-agent optimal control problem. Hence,
single-agent RL algorithms are then enabled to solve this MARL problem.
Mean-Field Regime
Another idea toward tackling the scalability issue is to take the setting to the meanfield regime, with an extremely large number of homogeneous agents. Each agentâ€™s effect
on the overall multi-agent system can thus become infinitesimal, resulting in all agents
being interchangeable/indistinguishable. The interaction with other agents, however, is
captured simply by some mean-field quantity, e.g., the average state, or the empirical
distribution of states. Each agent only needs to find the best response to the mean-field,
which considerably simplifies the analysis. This mean-field view of multi-agent systems
has been approached by the mean-field games (MFGs) model [165, 166, 167, 168, 169],
the team model with mean-field sharing [170, 171], and the game model with mean-field
actions [172].7
MARL in these models have not been explored until recently, mostly in the noncooperative setting of MFGs, see Â§4.3 for a more detailed review. Regarding the cooperative setting, recent work [175] studies RL for Markov teams with mean-field sharing
[170, 176, 171]. Compared to MFG, the model considers agents that share a common
reward function depending only on the local state and the mean-field, which encourages
cooperation among the agents. Also, the term mean-field refers to the empirical average for
the states of finite population, in contrast to the expectation and probability distribution of
infinite population in MFGs. Based on the dynamic programming decomposition for the
7 The difference between mean-field teams and mean-field games is mainly the solution concept: optimum versus equilibrium, as the difference between general dynamic team theory [173, 88, 174] and game
theory [82, 84]. Although the former can be viewed as a special case of the latter, related works are usually
reviewed separately in the literature. We follow here this convention.

18

specified model [170], several popular RL algorithms are easily translated to address this
setting [175]. More recently, [177, 178] approach the problem from a mean-field control
(MFC) model, to model large-population of cooperative decision-makers. Policy gradient methods are proved to converge for linear quadratic MFCs in [177], and mean-field
Q-learning is then shown to converge for general MFCs [178].
4.1.2

Decentralized Paradigm with Networked Agents

Cooperative agents in numerous practical multi-agent systems are not always homogeneous. Agents may have different preferences, i.e., reward functions, while they still
form a team to maximize the return of the team-average reward RÌ„, with RÌ„(s, a, s0 ) = N âˆ’1 Â·
P
i
0
iâˆˆN R (s, a, s ). More subtly, the reward function is sometimes not sharable with others,
as the preference is kept private to each agent. This setting finds broad applications in
engineering systems as sensor networks [179], smart grid [180, 181], intelligent transportation systems [12, 182], and robotics [183].
Covering the homogeneous setting in Â§4.1.1 as a special case, the specified one definitely requires more coordination, as, for example, the global value function cannot be
estimated locally without knowing other agentsâ€™ reward functions. With a central controller, most MARL algorithms reviewed in Â§4.1.1 directly apply, since the controller can
collect and average the rewards, and distributes the information to all agents. Nonetheless, such a controller may not exist in most aforementioned applications, due to either
cost, scalability, or robustness concerns [179, 180, 184]. Instead, the agents may be able to
share/exchange information with their neighbors over a possibly time-varying and sparse
communication network, as illustrated in Figure 2 (b). Though MARL under this decentralized/distributed8 paradigm is imperative, it is relatively less-investigated, in comparison to the extensive results on distributed/consensus algorithms that solve static/one-stage
optimization problems [185, 186, 187, 188], which, unlike RL, involves no system dynamics, and does not maximize the long-term objective as a sequential-decision making problem.
Learning Optimal Policy
The most significant goal is to learn the optimal joint policy, while each agent only
accesses to local and neighboring information over the network. The idea of MARL with
networked agents dates back to [189]. To our knowledge, the first provably convergent
MARL algorithm under this setting is due to [94], which incorporates the idea of consensus
+ innovation to the standard Q-learning algorithm, yielding the QD-learning algorithm
8 Note that hereafter we use decentralized and distributed interchangeably for describing this paradigm.

19

with the following update


i
i 0 0
i
Qt+1
(s, a) â†Qti (s, a) + Î±t,s,a Ri (s, a) + Î³ max
Q
(s
,
a
)
âˆ’
Q
(s,
a)
t
t
a0 âˆˆA
Xh
i
j
âˆ’ Î²t,s,a
Qti (s, a) âˆ’ Qt (s, a) ,
jâˆˆNti

where Î±t,s,a , Î²t,s,a > 0 are stepsizes, Nti denotes the set of neighboring agents of agent i,
at time t. Compared to the Q-learning update (2.1), QD-learning appends an innovation
term that captures the difference of Q-value estimates from its neighbors. With certain
conditions on the stepsizes, the algorithm is guaranteed to converge to the optimum Qfunction for the tabular setting.
Due to the scalability issue, function approximation is vital in MARL, which necessitates the development of policy-based algorithms. Our work [23] proposes actorcritic algorithms for this setting. Particularly, each agent parameterizes its own policy
i
Ï€Î¸i i : S â†’ âˆ†(Ai ) by some parameter Î¸ i âˆˆ Rm , the policy gradient of the return is first
derived as
h
i
âˆ‡Î¸ i J(Î¸) = E âˆ‡Î¸i log Ï€Î¸i i (s, ai ) Â· QÎ¸ (s, a)
(4.1)
where QÎ¸ is the global value function corresponding to RÌ„ under the joint policy Ï€Î¸ that
Q
is defined as Ï€Î¸ (a | s) := iâˆˆN Ï€Î¸i i (ai | s). As an analogy to (2.2), the policy gradient in (4.1)
involves the expectation of the product between the local score function âˆ‡Î¸i log Ï€Î¸i i (s, ai )
and the global Q-function QÎ¸ . The latter, nonetheless, cannot be estimated individually
at each agent. As a result, by parameterizing each local copy of QÎ¸ (Â·, Â·) as QÎ¸ (Â·, Â·; Ï‰i ) for
agent i, we propose the following consensus-based TD learning for the critic step, i.e., for
estimating QÎ¸ (Â·, Â·):
X
j
i
eti = Ï‰ti + Î²Ï‰,t Â· Î´ti Â· âˆ‡Ï‰ Qt (Ï‰ti ),
et ,
Ï‰
Ï‰t+1
=
ct (i, j) Â· Ï‰
(4.2)
jâˆˆN

where Î²Ï‰,t > 0 is the stepsize, Î´ti is the local TD-error calculated using QÎ¸ (Â·, Â·; Ï‰i ). The first
relation in (4.2) performs the standard TD update, followed by a weighted combination of
j
et . The weights here, ct (i, j), are dictated by the topology of the
the neighborsâ€™ estimates Ï‰
communication network, with non-zero values only if two agents i and j are connected
at time t. They also need to satisfy the doubly stochastic property in expectation, so that
Ï‰ti reaches a consensual value for all i âˆˆ N if it converges. Then, each agent i updates its
policy following stochastic policy gradient given by (4.1) in the actor step, using its own
Q-function estimate QÎ¸ (Â·, Â·; Ï‰ti ). A variant algorithm is also introduced in [23], relying on
not the Q-function, but the state-value function approximation, to estimate the global
advantage function.

20

With these in mind, almost sure convergence is established in [23] for these decentralized actor-critic algorithms, when linear functions are used for value function approximation. Similar ideas are also extended to the setting with continuous spaces [43], where deterministic policy gradient (DPG) method is usually used. Off-policy exploration, namely
a stochastic behavior policy, is required for DPG, as the deterministic on-policy may not
be explorative enough. However, in the multi-agent setting, as the policies of other agents
are unknown, the common off-policy approach for DPG [71, Â§4.2] does not apply. Inspired by the expected policy gradient (EPG) method [190] which unifies stochastic PG
(SPG) and DPG, we develop an algorithm that remains on-policy, but reduces the variance of general SPG [43]. In particular, we derive the multi-agent version of EPG, based
on which we develop the actor step that can be implemented in a decentralized fashion, while the critic step still follows (4.2). Convergence of the algorithm is then also
established when linear function approximation is used [43]. In the same vein, [158] considers the extension of [23] to an off-policy setting, building upon the emphatic temporal
differences (ETD) method for the critic [191]. By incorporating the analysis of ETD(Î»)
[192] into [23], almost sure convergence guarantee has also been established. Another
off-policy algorithm for the same setting is proposed concurrently by [193], where agents
do not share their estimates of value function. Instead, the agents aim to reach consensus
over the global optimal policy estimation. Provable convergence is then established for
the algorithm, with a local critic and a consensus actor.
RL for decentralized networked agents has also been investigated in multi-task, in addition to the multi-agent, settings. In some sense, the former can be regarded as a simplified version of the latter, where each agent deals with an independent MDP that is not affected by other agents, while the goal is still to learn the optimal joint policy that accounts
for the average reward of all agents. [194] proposes a distributed actor-critic algorithm,
assuming that the states, actions, and rewards are all local to each agent. Each agent performs a local TD-based critic step, followed by a consensus-based actor step that follows
the gradient calculated using information exchanged from the neighbors. Gradient of the
average return is then proved to converge to zero as the iteration goes to infinity. [155]
has developed Diff-DAC, another distributed actor-critic algorithm for this setting, from
duality theory. The updates resemble those in [23], but provide additional insights that
actor-critic is actually an instance of the dual ascent method for solving a linear program.
Note that all the aforementioned convergence guarantees are asymptotic, i.e., the algorithms converge as the iteration numbers go to infinity, and are restricted to the case with
linear function approximations. This fails to quantify the performance when finite iterations and/or samples are used, not to mention when nonlinear functions such as deep
neural networks are utilized. As an initial step toward finite-sample analyses in this setting
with more general function approximation, we consider in [44] the batch RL algorithms
[195], specifically, decentralized variants of the fitted-Q iteration (FQI) [196, 197]. Note
that we focus on FQI since it motivates the celebrated deep Q-learning algorithm [10]
when deep neural networks are used for function approximation. We study FQI variants
21

for both the cooperative setting with networked agents, and the competitive setting with
two teams of such networked agents (see Â§4.2.1 for more details). In the former setting, all
agents cooperate to iteratively update the global Q-function estimate, by fitting nonlinear
least squares with the target values as the responses. In particular, let F be the function
class for Q-function approximation, {(sj , {aij }iâˆˆN , sj0 )}jâˆˆ[n] be the batch transitions dataset
of size n available to all agents, {rji }jâˆˆ[n] be the local reward samples private to each agent,
and yji = rji + Î³ Â· maxaâˆˆA Qti (sj0 , a) be the local target value, where Qti is agent iâ€™s Q-function
estimate at iteration t. Then, all agents cooperate to find a common Q-function estimate
by solving
n
i2
1 X 1 Xh i
min
yj âˆ’ f (sj , a1j , Â· Â· Â· , aN
)
.
j
2n
f âˆˆF N
iâˆˆN

(4.3)

j=1

Since yji is only known to agent i, the problem in (4.3) aligns with the formulation of
distributed/consensus optimization in [185, 186, 187, 188, 198, 199], whose global optimal
P
2
solution can be achieved by the algorithms therein, if F makes nj=1 [yji âˆ’ f (sj , a1j , Â· Â· Â· , aN
j )]
convex for each i. This is indeed the case if F is a linear function class. Nevertheless, with
only a finite iteration of distributed optimization algorithms (common in practice), agents
may not reach exact consensus, leading to an error of each agentâ€™s Q-function estimate
away from the actual optimum of (4.3). Such an error also exists when nonlinear function
approximation is used. Considering this error caused by decentralized computation, we
follow the error propagation analysis stemming from single-agent batch RL [200, 201, 197,
202, 203], to establish the finite-sample performance of the proposed algorithms, i.e.,
how the accuracy of the algorithms output depends on the function class F , the number
of samples within each iteration n, and the number of iterations t.
Policy Evaluation
Aside from control, a series of algorithms in this setting focuses on the policy evaluation task only, namely, the critic step of the actor-critic algorithms. With the policy fixed,
this task embraces a neater formulation, as the sampling distribution becomes stationary,
and the objective becomes convex under linear function approximation. This facilitates
the finite-time/sample analyses, in contrast to most control algorithms with only asymptotic guarantees. Specifically, under joint policy Ï€, suppose each agent parameterizes
the value function by a linear function class {VÏ‰ (s) := Ï†> (s)Ï‰ : Ï‰ âˆˆ Rd }, where Ï†(s) âˆˆ Rd
is the feature vector at s âˆˆ S, and Ï‰ âˆˆ Rd is the vector of parameters. For notational
convenience, let Î¦ := (Â· Â· Â· ; Ï†> (s); Â· Â· Â· ) âˆˆ R|S|Ã—d , D = diag[{Î·Ï€ (s)}sâˆˆS ] âˆˆ R|S|Ã—|S| be a diagoP
nal matrix constructed using the state-occupancy measure Î·Ï€ , RÌ„Ï€ (s) = N âˆ’1 Â· iâˆˆN Ri,Ï€ (s),
where Ri,Ï€ (s) = Eaâˆ¼Ï€(Â· | s),s0 âˆ¼P (Â· | s,a) [Ri (s, a, s0 )], and P Ï€ âˆˆ R|S|Ã—|S| with the (s, s0 ) element being
P
[P Ï€ ]s,s0 = aâˆˆA Ï€(a | s)P (s0 | s, a). Then, the objective of all agents is to jointly minimize the
mean square projected Bellman error (MSPBE) associated with the team-average reward,

22

i.e.,
min
Ï‰


 2
2
MSPBE(Ï‰) := Î Î¦ VÏ‰ âˆ’ Î³P Ï€ VÏ‰ âˆ’ RÌ„Ï€ D = AÏ‰ âˆ’ b C âˆ’1 ,

(4.4)

where Î Î¦ : R|S| â†’ R|S| defined as Î Î¦ := Î¦(Î¦ > DÎ¦)âˆ’1 Î¦ > D is the projection operator
onto subspace {Î¦Ï‰ : Ï‰ âˆˆ Rd }, A := E{Ï†(s)[Ï†(s) âˆ’ Î³Ï†(s0 )]> }, C := E[Ï†(s)Ï†> (s)], and b :=
E[RÌ„Ï€ (s)Ï†(s)]. By replacing the expectation with samples and using the Fenchel duality,
the finite-sum version of (4.4) can be re-formulated as a distributed saddle-point problem
n

min max

Ï‰ Î»i ,iâˆˆN

1 XX
2(Î»i )> Aj Ï‰ âˆ’ 2(bji )> Î»i âˆ’ (Î»i )> Cj Î»i
Nn

(4.5)

iâˆˆN j=1

where n is the data size, Aj , Cj and bji are empirical estimates of A, C and bi := E[Ri,Ï€ (s)Ï†(s)],
respectively, using sample j. Note that (4.5) is convex in Ï‰ and concave in {Î»i }iâˆˆN . The use
of MSPBE as an objective is standard in multi-agent policy evaluation [154, 156, 96, 95,
157], and the idea of saddle-point reformulation has been adopted in [154, 156, 96, 204].
Note that in [204], a variant of MSPBE, named H-truncated Î»-weighted MSPBE, is advocated, in order to control the bias of the solution deviated from the actual mean square
Bellman error minimizer.
With the formulation (4.4) in mind, [156] develops a distributed variant of the gradient TD-based method, with asymptotic convergence established using the ordinary differential equation (ODE) method. In [96], a double averaging scheme that combines the
dynamic consensus [205] and the SAG algorithm [206] has been proposed to solve the
saddle-point problem (4.5) with a linear rate. [204] incorporates the idea of variancereduction, specifically, AVRG in [207], into gradient TD-based policy evaluation. Achieving the same linear rate as [96], three advantages are claimed in [204]: i) data-independent
memory requirement; ii) use of eligibility traces [208]; iii) no need for synchronization
in sampling. More recently, standard TD learning [58], instead of gradient-TD, has been
generalized to this MARL setting, with special focuses on finite-sample analyses [95, 157].
Distributed TD(0) is first studied in [95], using the proof techniques originated in [209],
which requires a projection on the iterates, and the data samples to be independent and
identically distributed (i.i.d.). Furthermore, motivated by the recent progress in [210],
finite-time performance of the more general distributed TD(Î») algorithm is provided in
[157], with neither projection nor i.i.d. noise assumption needed.
Policy evaluation for networked agents has also been investigated under the setting of
independent agents interacting with independent MDPs. [154] studies off-policy evaluation
based on the importance sampling technique. With no coupling among MDPs, an agent
does not need to know the actions of the other agents. Diffusion-based distributed GTD
is then proposed, and is shown to convergence in the mean-square sense with a sublinear rate. In [211], two variants of the TD-learning, namely, GTD2 and TDC [62], have

23

been designed for this setting, with weak convergence proved by the general stochastic
approximation theory in [212], when agents are connected by a time-varying communication network. Note that [204] also considers the independent MDP setting, with the same
results established as the actual MARL setting.
Other Learning Goals
Several other learning goals have also been explored for decentralized MARL with
networked agents. [213] has considered the optimal consensus problem, where each agent
over the network tracks the states of its neighborsâ€™ as well as a leaderâ€™s, so that the consensus error is minimized by the joint policy. A policy iteration algorithm is then introduced,
followed by a practical actor-critic algorithm using neural networks for function approximation. A similar consensus error objective is also adopted in [214], under the name of
cooperative multi-agent graphical games. A centralized-critic-decentralized-actor scheme is
utilized for developing off-policy RL algorithms.
Communication efficiency, as a key ingredient in the algorithm design for this setting,
has drawn increasing attention recently [130, 132, 131]. Specifically, [130] develops Lazily
Aggregated Policy Gradient (LAPG), a distributed PG algorithm that can reduce the communication rounds between the agents and a central controller, by judiciously designing
communication trigger rules. [132] addresses the same policy evaluation problem as [96],
and develops a hierarchical distributed algorithm by proposing a mixing matrix different from the doubly stochastic one used in [23, 96, 156], which allows unidirectional
information exchange among agents to save communication. In contrast, the distributed
actor-critic algorithm in [131] reduces the communication by transmitting only one scalar
entry of its state vector at each iteration, while preserving provable convergence as in [23].
4.1.3

Partially Observed Model

We complete the overview for cooperative settings by briefly introducing a class of significant but challenging models where agents are faced with partial observability. Though
common in practice, theoretical analysis of MARL algorithms in this setting is still relatively scarce, in contrast to the aforementioned fully observed settings. In general, this
setting can be modeled by a decentralized POMDP (Dec-POMDP) [36], which shares almost all elements such as the reward function and the transition model, as the MMDP/Markov
team model in Â§2.2.1, except that each agent now only has its local observations of the system state s. With no accessibility to other agentsâ€™ observations, an individual agent cannot
maintain a global belief state, the sufficient statistic for decision making in single-agent
POMDPs. Hence, Dec-POMDPs have been known to be NEXP-complete [104], requiring
super-exponential time to solve in the worst case.
There is an increasing interest in developing planning/learning algorithms for DecPOMDPs. Most of the algorithms are based on the centralized-learning-decentralized-execution
scheme. In particular, the decentralized problem is first reformulated as a centralized
one, which can be solved at a central controller with (a simulator that generates) the ob24

servation data of all agents. The policies are then optimized/learned using data, and distributed to all agents for execution. Finite-state controllers (FSCs) are commonly used to
represent the local policy at each agent [215, 216], which map local observation histories
to actions. A Bayesian nonparametric approach is proposed in [217] to determine the controller size of variable-size FSCs. To efficiently solve the centralized problem, a series of
top-down algorithms have been proposed. In [150], the Dec-POMDP is converted to nonobservable MDP (NOMDP), a kind of centralized sequential decision-making problem,
which is then addressed by some heuristic tree search algorithms. As an extension of the
NOMDP conversion, [218, 152] convert Dec-POMDPs to occupancy-state MDPs (oMDPs),
where the occupancy-states are distributions over hidden states and joint histories of observations. As the value functions of oMDPs enjoy the piece-wise linearity and convexity,
both tractable planning [218] and value-based learning [152] algorithms have been developed.
To further improve computational efficiency, several sampling-based planning/learning
algorithms have also been proposed. In particular, Monte-Carlo sampling with policy iteration and the expectation-maximization algorithm, are proposed in [219] and [220],
respectively. Furthermore, Monte-Carlo tree search has been applied to special classes
of Dec-POMDPs, such as multi-agent POMDPs [121] and multi-robot active perception
[221]. In addition, policy gradient-based algorithms can also be developed for this centralized learning scheme [152], with a centralized critic and a decentralized actor. Finitesample analysis can also be established under this scheme [222, 223], for tabular settings
with finite state-action spaces.
Several attempts have also been made to enable decentralized learning in Dec-POMDPs.
When the agents share some common information/observations, [224] proposes to reformulate the problem as a centralized POMDP, with the common information being the observations of a virtual central controller. This way, the centralized POMDP can be solved
individually by each agent. In [225], the reformulated POMDP has been approximated
by finite-state MDPs with exponentially decreasing approximation error, which are then
solved by Q-learning. Very recently, [39] has developed a tree-search based algorithm for
solving this centralized POMDP, which, interestingly, echoes back the heuristics for solving Dec-POMDPs directly as in [121, 221], but with a more solid theoretical footing. Note
that in both [225, 39], a common random number generator is used for all agents, in order
to avoid communication among agents and enable a decentralized learning scheme.

4.2

Competitive Setting

Competitive settings are usually modeled as zero-sum games. Computationally, there exists a great barrier between solving two-player and multi-player zero-sum games. In particular, even the simplest three-player matrix games, are known to be PPAD-complete
[226, 227]. Thus, most existing results on competitive MARL focus on two-player zerosum games, with N = {1, 2} and R1 + R2 = 0 in Definitions 2.2 and 2.4. In the rest of
25

this section, we review methods that provably find a Nash (equivalently, saddle-point)
equilibrium in two-player Markov or extensive-form games. The existing algorithms can
mainly be categorized into two classes: value-based and policy-based approaches, which
are introduced separately in the sequel.
4.2.1

Value-Based Methods

Similar as in single-agent MDPs, value-based methods aim to find an optimal value function from which the joint Nash equilibrium policy can be extracted. Moreover, the optimal value function is known to be the unique fixed point of a Bellman operator, which
can be obtained via dynamic programming type methods.
Specifically, for simultaneous-move Markov games, the value function defined in (2.3)
satisfies VÏ€11 ,Ï€2 = âˆ’VÏ€21 ,Ï€2 . and thus any Nash equilibrium Ï€âˆ— = (Ï€1,âˆ— , Ï€2,âˆ— ) satisfies
VÏ€11 ,Ï€1,âˆ— (s) â‰¤ VÏ€11,âˆ— ,Ï€2,âˆ— (s) â‰¤ VÏ€11,âˆ— ,Ï€2 (s),

for any Ï€ = (Ï€1 , Ï€2 ) and s âˆˆ S.

(4.6)

Similar as the minimax theorem in normal-form/matrix zero-sum games [228], for twoplayer zero-sum Markov games with finite state and action spaces, one can define the
optimal value function V âˆ— : S â†’ R as
V âˆ— = max min VÏ€11 ,Ï€2 = min max VÏ€11 ,Ï€2 ,
Ï€1

Ï€2

Ï€2

Ï€1

(4.7)

Then (4.6) implies that VÏ€11,âˆ— ,Ï€2,âˆ— coincides with V âˆ— and any pair of policies Ï€1 and Ï€2 that
attains the supremum and infimum in (4.7) constitutes a Nash equilibrium. Moreover,
similar to MDPs, [82] shows that V âˆ— is the unique solution of a Bellman equation and a
Nash equilibrium can be constructed based on V âˆ— . Specifically, for any V : S â†’ R and any
s âˆˆ S, we define
h
i
QV (s, a1 , a2 ) = Es0 âˆ¼P (Â· | s,a1 ,a2 ) R1 (s, a1 , a2 , s0 ) + Î³ Â· V (s0 ) ,
(4.8)
1

2

where QV (s, Â·, Â·) can be regarded as a matrix in R|A |Ã—|A | . Then we define the Bellman
operator T âˆ— by solving a matrix zero-sum game regarding QV (s, Â·, Â·) as the payoff matrix,
i.e., for any s âˆˆ S, one can define
X X
h
i
(T âˆ— V )(s) = Value QV (s, Â·, Â·) = max min
ua Â· vb Â· QV (s, a, b),
(4.9)
uâˆˆâˆ†(A1 ) vâˆˆâˆ†(A2 )

aâˆˆA1 bâˆˆA2

where we use Value(Â·) to denote the optimal value of a matrix zero-sum game, which
can be obtained by solving a linear program [229]. Thus, the Bellman operator T âˆ— is Î³contractive in the `âˆž -norm and V âˆ— in (4.7) is the unique solution to the Bellman equation
V = T âˆ— V . Moreover, letting p1 (V ), p2 (V ) be any solution to the optimization problem
in (4.9), we have that Ï€âˆ— = (p1 (V âˆ— ), p2 (V âˆ— )) is a Nash equilibrium specified by Definition
26

2.3. Thus, based on the Bellman operator T âˆ— , [82] proposes the value iteration algorithm,
which creates a sequence of value functions {Vt }tâ‰¥1 satisfying Vt+1 = T âˆ— Vt that converges
to V âˆ— with a linear rate. Specifically, we have
kVt+1 âˆ’ V âˆ— kâˆž = kT âˆ— Vt âˆ’ T âˆ— V âˆ— kâˆž â‰¤ Î³ Â· kVt âˆ’ V âˆ— kâˆž â‰¤ Î³ t+1 Â· kV0 âˆ’ V âˆ— kâˆž .
In addition, a value iteration update can be decomposed into the two steps. In particular,
letting Ï€1 be any policy of player 1 and V be any value function, we define Bellman
1
operator T Ï€ by
X X
1
Ï€1 (a | s) Â· vb Â· QV (s, a, b),
(4.10)
(T Ï€ V )(s) = min
vâˆˆâˆ†(A2 )

aâˆˆA1 bâˆˆA2

where QV is defined in (4.8). Then we can equivalently write a value iteration update as
Âµt+1 = p1 (Vt )

and

Vt+1 = T Âµt+1 Vt .

(4.11)

Such a decomposition motivates the policy iteration algorithm for two-player zero-sum
games, which has been studied in, e.g., [230, 231, 232, 233, 234], for different variants
of such Markov games. In particular, from the perspective of player 1, policy iteration
creates a sequence {Âµt , Vt }tâ‰¥0 satisfying
Âµt+1 = p1 (Vt )

and

Vt+1 = (T Âµt+1 )âˆž Vt ,

(4.12)

i.e., Vt+1 is the fixed point of T Âµt+1 . The updates for player 2 can be similarly constructed.
By the definition in (4.10), the Bellman operator T Âµt+1 is Î³-contractive and its fixed point
corresponds to the value function associated with (Âµt+1 , Br(Âµt+1 )), where Br(Âµt+1 ) is the
best response policy of player 2 when player 1 adopts Âµt+1 . Hence, in each step of policy
iteration, the player first finds an improved policy Âµt+1 based on the current function Vt ,
and then obtains a conservative value function by assuming that the opponent plays the
best counter policy Br(Âµt+1 ). It has been shown in [234] that the value function sequence
{Vt }tâ‰¥0 monotonically increases to V âˆ— with a linear rate of convergence for turn-based
zero-sum Markov games.
Notice that both the value and policy iteration algorithms are model-based due to the
need of computing the Bellman operator T Âµt+1 in (4.11) and (4.12). By estimating the
Bellman operator via data-driven approximation, [83] has proposed minimax-Q learning, which extends the well-known Q-learning algorithm [48] for MDPs to zero-sum
Markov games. In particular, minimax-Q learning is an online, off-policy, and tabular
method which updates the action-value function Q : S Ã— A â†’ R based on transition data
{(st , at , rt , st0 )}tâ‰¥0 , where st0 is the next state following (st , at ) and rt is the reward. In the t-th
iteration, it only updates the value of Q(st , at ) and keeps other entries of Q unchanged.

27

Specifically, we have
n
h
io
Q(st , a1t , a2t ) â† (1 âˆ’ Î±t ) Â· Q(st , a1t , a2t ) + Î±t Â· rt + Î³ Â· Value Q(st0 , Â·, Â·) ,

(4.13)

where Î±t âˆˆ (0, 1) is the stepsize. As shown in [49], under conditions similar to those for
single-agent Q-learning [48], function Q generated by (4.13) converges to the optimal
action-value function Qâˆ— = QV âˆ— defined by combining (4.7) and (4.8). Moreover, with a
slight abuse of notation, if we define the Bellman operator T âˆ— for action-value functions
by
n
h
io
(T âˆ— Q)(s, a1 , a2 ) = Es0 âˆ¼P (Â· | s,a1 ,a2 ) R1 (s, a1 , a2 , s0 ) + Î³ Â· Value Q(s0 , Â·, Â·) ,
(4.14)
then we have Qâˆ— as the unique fixed point of T âˆ— . Since the target value rt +Î³Â·Value[Q(st0 , Â·, Â·)]
in (4.13) is an estimator of (T âˆ— Q)(st , a1t , a2t ), minimax-Q learning can be viewed as a stochastic approximation algorithm for computing the fixed point of T âˆ— . Following [83], minimaxQ learning has been further extended to the function approximation setting where Q in
(4.13) is approximated by a class of parametrized functions. However, convergence guarantees for this minimax-Q learning with even linear function approximation have not
been well understood. Such a linear value function approximation also applies to a significant class of zero-sum MG instances with continuous state-action spaces, i.e., linear
quadratic (LQ) zero-sum games [99, 235, 236], where the reward function is quadratic
with respect to the states and actions, while the transition model follows linear dynamics.
In this setting, Q-learning based algorithm can be guaranteed to converge to the NE [236].
To embrace general function classes, the framework of batch RL [237, 200, 201, 197,
202, 238] can be adapted to the multi-agent settings, as in the recent works [239, 44]. As
mentioned in Â§4.1.2 for cooperative batch MARL, each agent iteratively updates the Qfunction by fitting least-squares using the target values. Specifically, let F be the function
class of interest and let {(si , a1i , a2i , ri , si0 )}iâˆˆ[n] be the dataset. For any t â‰¥ 0, let Qt be the
current iterate in the t-th iteration, and define yi = ri + Î³ Â· Value[Qt (si0 , Â·, Â·)] for all i âˆˆ [n].
Then we update Qt by solving a least-squares regression problem in F , that is,
n
i2
1 Xh
yi âˆ’ f (si , a1i , a2i ) .
Qt+1 â† argmin
2n
f âˆˆF

(4.15)

i=1

In such a two-player zero-sum Markov game setting, a finite-sample error bound on the
Q-function estimate is established in [239].
Regarding other finite-sample analyses, very recently, [240] has studied zero-sum
turn-based stochastic games (TBSG), a simplified zero-sum MG when the transition model
is embedded in some feature space and a generative model is available. Two Q-learning
based algorithms have been proposed and analyzed for this setting. [35] has proposed algorithms that achieve near-optimal sample complexity for general zero-sum TBSGs with a

28

generative model, by extending the previous near-optimal Q-learning algorithm for MDPs
[241]. In the online setting, where the learner controls only one of the players that plays
against an arbitrary opponent, [242] has proposed UCSG, an algorithm for the averagereward zero-sum MGs, using the principle of optimism in the face of uncertainty [243, 244].
UCSG is shown to achieve a sublinear regret compared to the game value when compete
ing with an arbitrary opponent, and also achieve O(poly(1/))
sample complexity if the
opponent plays an optimistic best response.
Furthermore, when it comes to zero-sum games with imperfect information, [245, 246,
247, 248] have proposed to transform extensive-form games into normal-form games using the sequence form representation, which enables equilibrium finding via linear programming. In addition, by lifting the state space to the space of belief states, [249,
250, 251, 119, 252] have applied dynamic programming methods to zero-sum stochastic games. Both of these approaches guarantee finding of a Nash equilibrium but are only
efficient for small-scale problems. Finally, MCTS with UCB-type action selection rule
[51, 52, 53] can also be applied to two-player turn-based games with incomplete information [52, 253, 254, 255, 256], which lays the foundation for the recent success of deep RL
for the game of Go [1]. Moreover, these methods are shown to converge to the minimax
solution of the game, thus can be viewed as a counterpart of minimax-Q learning with
Monte-Carlo sampling.
4.2.2

Policy-Based Methods

Policy-based reinforcement learning methods introduced in Â§2.1.2 can also be extended
to the multi-agent setting. Instead of finding the fixed point of the Bellman operator, a fair
amount of methods only focus on a single agent and aim to maximize the expected return
of that agent, disregarding the other agentsâ€™ policies. Specifically, from the perspective of
a single agent, the environment is time-varying as the other agents also adjust their policies. Policy based methods aim to achieve the optimal performance when other agents
play arbitrarily by minimizing the (external) regret, that is, find a sequence of actions that
perform nearly as well as the optimal fixed policy in hindsight. An algorithm with negligible average overall regret is called no-regret or Hannan consistent [257]. Any Hannan
consistent algorithm is known to have the following two desired properties in repeated
normal-form games. First, when other agents adopt stationary policies, the time-average
policy constructed by the algorithm converges to the best response policy (against the
ones used by the other agents). Second, more interestingly, in two-player zero-sum games,
when both players adopt Hannan consistent algorithms and both their average overall regrets are no more than , their time-average policies constitute a 2-approximate Nash
equilibrium [126]. Thus, any Hannan consistent single-agent reinforcement learning algorithm can be applied to find the Nash equilibria of two-player zero-sum games via
self-play. Most of these methods belong to one of the following two families: fictitious
play [258, 259], and counterfactual regret minimization [109], which will be summarized
29

below.
Fictitious play is a classic algorithm studied in game theory, where the players play
the game repeatedly and each player adopts a policy that best responds to the average
policy of the other agents. This method was originally proposed for solving normal-form
games, which are a simplification of the Markov games defined in Definition 2.2 with S
being a singleton and Î³ = 0. In particular, for any joint policy Ï€ âˆˆ âˆ†(A) of the N agents,
we let Ï€âˆ’i be the marginal policy of all players except player i. For any t â‰¥ 1, suppose
the agents have played {aÏ„ : .1 â‰¤ Ï„ â‰¤ t} in the first t stages. We define xt as the empirical
P
distribution of {aÏ„ : .1 â‰¤ Ï„ â‰¤ t}, i.e., xt (a) = t âˆ’1 tÏ„=1 1{at = a} for any a âˆˆ A. Then, in the t-th
stage, each agent i takes action ait âˆˆ Ai according to the best response policy against xtâˆ’i .
In other words, each agent plays the best counter policy against the policy of the other
agents inferred from history data. Here, for any  > 0 and any Ï€ âˆˆ âˆ†(A), we denote by
Br (Ï€âˆ’i ) the -best response policy of player i, which satisfies


Ri Br (Ï€âˆ’i ), Ï€âˆ’i â‰¥ sup Ri (Âµ, Ï€âˆ’i ) âˆ’ .
(4.16)
Âµâˆˆâˆ†(Ai )

Moreover, we define Br (Ï€) as the joint policy (Br (Ï€âˆ’1 ), . . . , Br (Ï€âˆ’N )) âˆˆ âˆ†(A) and suppress the subscript  in Br if  = 0. By this notation, regarding each a âˆˆ A as a vertex of
âˆ†(A), we can equivalently write the fictitious process as
xt âˆ’ xtâˆ’1 = (1/t) Â· (at âˆ’ xtâˆ’1 ),

where

at âˆ¼ Br(xtâˆ’1 ).

(4.17)

As t â†’ âˆž, the updates in (4.17) can be approximately characterized by a differential
inclusion [260]
dx(t)
âˆˆ Br(x(t)) âˆ’ x(t),
dt

(4.18)

which is known as the continuous-time fictitious play. Although it is well known that
the discrete-time fictitious play in (4.17) is not Hannan consistent [261, 160], it is shown
in [262, 263] that the continuous-time fictitious play in (4.18) is Hannan consistent. Moreover, using tools from stochastic approximation [264, 261], various modifications of discretetime fictitious play based on techniques such as smoothing or stochastic perturbations
have been shown to converge to the continuous-time fictitious play and are thus Hannan
consistent [265, 266, 267, 268, 269]. As a result, applying these methods with self-play
provably finds a Nash equilibrium of a two-player zero-sum normal form game.
Furthermore, fictitious play methods have also been extended to RL settings without the model knowledge. Specifically, using sequence-form representation, [110] has
proposed the first fictitious play algorithm for extensive-form games which is realizationequivalent to the generalized weakened fictitious play [267] for normal-form games. The
pivotal insight is that a convex combination of normal-form policies can be written as a

30

weighted convex combination of behavioral policies using realization probabilities. Specifically, recall that the set of information states of agent i was denoted by S i . When the
game has perfect-recall, each si âˆˆ S i uniquely defines a sequence Ïƒsi of actions played by
agent i for reaching state si . Then any behavioral policy Ï€i of agent i induces a realization probability Rp(Ï€i ; Â·) for each sequence Ïƒ of agent i, which is defined by Rp(Ï€i ; Ïƒ ) =
Q
i
0
0
i
i
(Ïƒs0 ,a)vÏƒ Ï€ (a | s ), where the product is taken over all s âˆˆ S and a âˆˆ A such that (Ïƒs0 , a) is
a subsequence of Ïƒ . Using the notation of realization probability, for any two behavioral
e of agent i, the sum
policies Ï€ and Ï€
e(Â· | si )
Î» Â· Rp(Ï€, Ïƒsi ) Â· Ï€(Â· | si )
(1 âˆ’ Î») Â· Rp(e
Ï€ , Ïƒs i ) Â· Ï€
+
,
Î» Â· Rp(Ï€, Ïƒsi ) + (1 âˆ’ Î») Â· Rp(e
Ï€, Ïƒsi ) Î» Â· Rp(Ï€, Ïƒsi ) + (1 âˆ’ Î») Â· Rp(e
Ï€ , Ïƒs i )

âˆ€si âˆˆ S i ,

(4.19)

e with weights Î» âˆˆ (0, 1) and 1 âˆ’ Î», respectively. Then,
is the mixture policy of Ï€ and Ï€
combining (4.16) and (4.19), the fictitious play algorithm in [110] computes a sequence of
policies {Ï€t }tâ‰¥1 . In particular, in the t-th iteration, any agent i first computes the t+1 -best
i
i
et+1
response policy Ï€
âˆˆ Brt+1 (Ï€tâˆ’i ) and then constructs Ï€t+1
as the mixture policy of Ï€ti
et+1 with weights 1 âˆ’ Î±t+1 and Î±t+1 , respectively. Here, both t and Î±t are taken to
and Ï€
P
converge to zero as t goes to infinity, and we further have tâ‰¥1 Î±t = âˆž. We note, however,
that although such a method provably converges to a Nash equilibrium of a zero-sum
game via self-play, it suffers from the curse of dimensionality due to the need to iterate all
states of the game in each iteration. For computational efficiency, [110] has also proposed
a data-drive fictitious self-play framework where the best-response is computed via fitted
Q-iteration [270, 200] for the single-agent RL problem, with the policy mixture being
learned through supervised learning. This framework was later adopted by [271, 25, 30,
31] to incorporate other single RL methods such as deep Q-network [10] and Monte-Carlo
tree search [53, 52, 272]. Moreover, in a more recent work, [162] has proposed a smooth
fictitious play algorithm [265] for zero-sum multi-stage games with simultaneous moves
(a special case of zero-sum stochastic games). Their algorithm combines the actor-critic
framework [69] with fictitious self-play, and infers the opponentâ€™s policy implicitly via
policy evaluation. Specifically, when the two players adopt a joint policy Ï€ = (Ï€1 , Ï€2 ),
from the perspective of player 1, it infers Ï€2 implicitly by estimating QÏ€1 ,Ï€2 via temporaldifference learning [273], where QÏ€1 ,Ï€2 : S Ã— A1 â†’ R is defined as
1

QÏ€1 ,Ï€2 (s, a ) = E

"X

#
Î³ R (st , at , st+1 ) s0 = s, a10 = a1 , a20 âˆ¼ Ï€2 (Â· | s), at âˆ¼ Ï€(Â· | st ), âˆ€t â‰¥ 1 ,
t 1

tâ‰¥0

which is the action-value function of player 1 marginalized by Ï€2 . Besides, the best response policy is obtained by taking the soft-greedy policy with respect to QÏ€1 ,Ï€2 , i.e.,
Ï€1 (a1 | s) â† P

exp[Î· âˆ’1 Â· QÏ€1 ,Ï€2 (s, a1 )]
a1 âˆˆA1 exp[Î·

31

âˆ’1 Â· Q

1
Ï€1 ,Ï€2 (s, a )]

,

(4.20)

where Î· > 0 is the smoothing parameter. Finally, the algorithm is obtained by performing
both policy evaluation and policy update in (4.20) simultaneously using two-timescale
updates [274, 264], which ensure that the policy updates, when using self-play, can be
characterized by an ordinary differential equation whose asymptotically stable solution is
a smooth Nash equilibrium of the game.
Another family of popular policy-based methods is based on the idea of counterfactural
regret minimization (CFR), first proposed in [109], which has been a breakthrough in the
effort to solve large-scale extensive-form games. Moreover, from a theoretical perspective,
compared with fictitious play algorithms whose convergence is analyzed asymptotically
via stochastic approximation, explicit regret bounds can be established using tools from
online learning [275], which yield rates of convergence to the Nash equilibrium. Specifically, when N agents play the extensive-form game for T rounds with {Ï€t : 1 â‰¤ t â‰¤ T }, the
regret of player i is defined as
RegiT = max
Ï€i

T h
X

i
Ri (Ï€i , Ï€tâˆ’i ) âˆ’ Ri (Ï€ti , Ï€tâˆ’i ) ,

(4.21)

t=1

where the maximum is taken over all possible policies of player i. In the following, before
we define the notion of counterfactual regret, we first introduce a few notations. Recall
that we had defined the reach probability Î·Ï€ (h) in (2.4), which can be factorized into
the product of each agentâ€™s contribution. That is, for each i âˆˆ U âˆª {c}, we can group the
Q
probability terms involving Ï€i into Î·Ï€i (h) and write Î·Ï€ (h) = iâˆˆN âˆª{c} Î·Ï€i (h) = Î·Ï€i (h) Â· Î·Ï€âˆ’i (h).
Moreover, for any two histories h, h0 âˆˆ H satisfying h v h0 , we define the conditional reach
probability Î·Ï€ (h0 | h) as Î·Ï€ (h0 )/Î·Ï€ (h) and define Î·Ï€i (h0 | h) similarly. For any s âˆˆ S i and any
a âˆˆ A(s), we define Z(s, a) = {(h, z) âˆˆ H Ã— Z | h âˆˆ s, ha v z}, which contains all possible pairs
of history in information state s and terminal history after taking action a at s. Then, the
counterfactual value function is defined as
X
i
QCF
(Ï€, s, a) =
Î·Ï€âˆ’i (h) Â· Î·Ï€ (z | ha) Â· Ri (z),
(4.22)
(h,z)âˆˆZ(s,a)

which is the expected utility obtained by agent i given that it has played to reached
P
i
i
state s. We also define VCF
(Ï€, s) = aâˆˆA(s) QCF
(Ï€, s, a) Â· Ï€i (a | s). Then the difference between
i
i
QCF
(Ï€, s, a) and VCF
(Ï€, s) can be viewed as the value of action a at information state s âˆˆ S i ,
and counterfactual regret of agent i at state s is defined as
RegiT (s) = max
aâˆˆA(s)

T h
X

i
i
i
QCF
(Ï€t , s, a) âˆ’ VCF
(Ï€t , s) ,

âˆ€s âˆˆ S i .

(4.23)

i=1

Moreover, as shown in Theorem 3 of [109], counterfactual regrets defined in (4.23) pro-

32

vide an upper bound for the total regret in (4.21):
X
i
RegT â‰¤
Regi,+
T (s),

(4.24)

sâˆˆS i

where we let x+ denote max{x, 0} for any x âˆˆ R. This bound lays the foundation of counterfactual regret minimization algorithms. Specifically, to minimize the total regret in (4.21),
it suffices to minimize the counterfactual regret for each information state, which can be
obtained by any online learning algorithm, such as EXP3 [276], Hedge [277, 278, 279],
and
âˆš regret matchingi [280]. All these methods
âˆš ensure that the counterfactual regret is
O( T ) for all s âˆˆ S , which leads to an O( T ) upper bound of the total regret. Thus,
applying CFR-type methods
âˆš with self-play to a zero-sum two-play extensive-form game,
the average policy is an O( 1/T )-approximate Nash equilibrium after T steps. In particular, the vanilla CFR
âˆš algorithm updates the policies via regret matching, which yields that
i
i
RegT (s) â‰¤ Rmax Â· Ai Â· T for all s âˆˆ S i , where we have introduced
Rimax = max Ri (z) âˆ’ min Ri (z),
zâˆˆZ

zâˆˆZ

Ai = max |A(h)|.
h : Ï„(h)=i

âˆš
Thus, by (4.24), the total regret of agent i is bounded by Rimax Â· |S i | Â· Ai Â· T .
One drawback of vanilla CFR is that the entire game tree needs to be traversed in
each iteration, which can be computationally prohibitive. A number of CFR variants have
been proposed since the pioneering work [109] for improving computational efficiency.
For example, [281, 282, 283, 284, 285, 286] combine CFR with Monte-Carlo sampling;
[287, 288, 289] propose to estimate the counterfactual value functions via regression;
[290, 291, 292] improve the computational efficiency by pruning suboptimal paths in the
game tree; [293, 294, 295] analyze the performance of a modification named CFR+ , and
[296] proposes lazy updates with a near-optimal regret upper bound.
Furthermore, it has been shown recently in [111] that CFR is closely related to policy
gradient methods. To see this, for any joint policy Ï€ and any i âˆˆ N , we define the actionvalue function of agent i, denoted by QÏ€i , as
QÏ€i (s, a) =

1
Â·
Î·Ï€ (s)

X

Î·Ï€ (h) Â· Î·Ï€ (z | ha) Â· Ri (z),

âˆ€s âˆˆ S i , âˆ€a âˆˆ A(s).

(4.25)

(h,z)âˆˆZ(s,a)

That is, QÏ€i (s, a) is the expected utility of agent i when the agents follow policy Ï€ and
agent i takes action a at information state s âˆˆ S i , conditioning on s being reached. It has
i
i
been shown in [111] that the QCF
in (4.22) is connected with QÏ€i in (4.25) via QCF
(Ï€, s, a) =
P
i
âˆ’i
QÏ€ (s, a) Â· [ hâˆˆs Î·Ï€ (h)]. Moreover, in the tabular setting where we regard the joint policy Ï€
as a table {Ï€i (a | s) : s âˆˆ S i , a âˆˆ A(s)}, for any s âˆˆ S i and any a âˆˆ A(s), the policy gradient of

33

Ri (Ï€) can be written as
âˆ‚Ri (Ï€)
i
= Î·Ï€ (s) Â· QÏ€i (s, a) = Î·Ï€i (s) Â· QCF
(Ï€, s, a),
âˆ‚Ï€i (a | s)

âˆ€s âˆˆ S i , âˆ€a âˆˆ A(s).

As a result, the advantage actor-critic (A2C) algorithm [69] is equivalent to a particular
CFR algorithm, where the policy update rule is specified by the generalized infinitesimal
gradient ascent algorithm [297]. Thus, [111] proves
that the regret of the tabular A2C alâˆš
i
i
i
2
gorithm is bounded by |S | Â· [1 + A Â· (Rmax ) ] Â· T . Following this work, [112] shows that
A2C where the policy is tabular and is parametrized by a softmax function is equivalent
to CFR that uses Hedge to update the policy. Moreover, [298] proposes a policy optimization method known as exploitability descent, where the policy is updated using actor-critic,
assuming the opponent plays the best counter-policy. This method is equivalent to the
CFR-BR algorithm [299] with Hedge. Thus, [111, 112, 298] show that actor-critic and policy gradient methods for MARL can be formulated as CFR methods and thus convergence
to a Nash equilibrium of a zero-sum extensive-form game is guaranteed.
In addition, besides fictitious play and CFR methods introduced above, multiple policy optimization methods have been proposed for special classes of two-player zero-sum
stochastic games or extensive form games. For example, Monte-Carlo tree search methods
have been proposed for perfect-information extensive games with simultaneous moves. It
has been shown in [300] that the MCTS methods with UCB-type action selection rules,
introduced in Â§4.2.1, fail to converge to a Nash equilibrium in simultaneous-move games,
as UCB does not take into consideration the possibly adversarial moves of the opponent.
To remedy this issue, [301, 302, 303, 304] have proposed to adopt stochastic policies and
using Hannan consistent methods such as EXP3 [276] and regret matching [280] to update the policies. With self-play, [302] shows that the average policy obtained by MCTS
with any -Hannan consistent policy update method converges to an O(D 2 Â· )-Nash equilibrium, where D is the maximal depth.
Finally, there are surging interests in investigating policy gradient-based methods in
continuous games, i.e., the games with continuous state-action spaces. With policy parameterization, finding the NE of zero-sum Markov games becomes a nonconvex-nonconcave
saddle-point problem in general [32, 305, 34, 306]. This hardness is inherent, even in the
simplest linear quadratic setting with linear function approximation [34, 306]. As a consequence, most of the convergence results are local [307, 32, 308, 309, 310, 311, 305, 33],
in the sense that they address the convergence behavior around local NE points. Still, it
has been shown that the vanilla gradient-descent-ascent (GDA) update, which is equivalent to the policy gradient update in MARL, fails to converge to local NEs, for either the
non-convergent behaviors such as limit cycling [307, 309, 312, 310], or the existence of
non-Nash stable limit points for the GDA dynamics [308, 305]. Consensus optimization
[307], symplectic gradient adjustment [312], and extragradient method [310] have been
advocated to mitigate the oscillatory behaviors around the equilibria; while [308, 305]
34

exploit the curvature information so that all the stable limit points of the proposed updates are local NEs. Going beyond Nash equilibria, [33, 311] consider gradient-based
learning for Stackelberg equilibria, which correspond to only the one-sided equilibrium
solution in zero-sum games, i.e., either minimax or maximin, as the order of which player
acts first is vital in nonconvex-nonconcave problems. [33] introduces the concept of local
minimax point as the solution, and shows that GDA converges to local minimax points
under mild conditions. [311] proposes a two-timescale algorithm where the follower uses
a gradient-play update rule, instead of an exact best response strategy, which has been
shown to converge to the Stackelberg equilibria. Under a stronger assumption of gradient
dominance, [313, 314] have shown that nested gradient descent methods converge to the
stationary points of the outer-loop, i.e., minimax, problem at a sublinear rate.
We note that these convergence results have been developed for general continuous
games with agnostic cost/reward functions, meaning that the functions may have various forms, so long as they are differentiable, sometimes even (Lipschitz) smooth, w.r.t.
each agentâ€™s policy parameter. For MARL, this is equivalent to requiring differentiability/smoothness of the long-term return, which relies on the properties of the game, as
well as of the policy parameterization. Such an assumption is generally very restrictive.
For example, the Lipschitz smoothness assumption fails to hold globally for LQ games
[34, 315, 306], a special type of MGs. Fortunately, thanks to the special structure of the
LQ setting, [34] has proposed several projected nested policy gradient methods that are
guaranteed to have global convergence to the NE, with convergence rates established. This
appears to be the first-of-its-kind result in MARL. The results have then been improved
by the techniques in a subsequent work of the authors [100], which can remove the projection step in the updates, for a more general class of such games. Very recently, [306]
also improves the results in [34] independently, with different techniques.

4.3

Mixed Setting

In stark contrast with the fully collaborative and fully competitive settings, the mixed
setting is notoriously challenging and thus rather less well understood. Even in the simplest case of a two-player general sum normal-form game, finding a Nash equilibrium
is PPAD-complete [316]. Moreover, [123] has proved that value-iteration methods fail to
find stationary Nash or correlated equilibria for general-sum Markov games. Recently, it
is shown that vanilla policy-gradient methods avoid a non-negligible subset of Nash equilibria in general-sum continuous games [32], including the LQ general-sum games [315].
Thus, additional structures on either the games or the algorithms need to be exploited, to
ascertain provably convergent MARL in the mixed setting.
Value-Based Methods
Under relatively stringent assumptions, several value-based methods that extend Qlearning [48] to the mixed setting are guaranteed to find an equilibrium. In particular,
35

[101] has proposed the Nash-Q learning algorithm for general-sum Markov games, where
one maintains N action-value functions QN = (Q1 , . . . , QN ) : S Ã— A â†’ RN for all N agents,
which are updated using sample-based estimator of a Bellman operator. Specifically, letting RN = (R1 , . . . , RN ) denote the reward functions of the agents, Nash-Q uses the following Bellman operator:
n
h
io
(T âˆ— QN )(s, a) = Es0 âˆ¼P (Â· | s,a) RN (s, a, s0 ) + Î³ Â· Nash QN (s0 , Â·) , âˆ€(s, a) âˆˆ S Ã— A,
(4.26)
where Nash[QN (s0 , Â·)] is the objective value of the Nash equilibrium of the stage game with
rewards {QN (s0 , a)}aâˆˆA . For zero-sum games, we have Q1 = âˆ’Q2 and thus the Bellman operator defined in (4.26) is equivalent to the one in (4.14) used by minimax-Q learning
[83]. Moreover, [101] establishes convergence to Nash equilibrium under the restrictive
assumption that Nash[QN (s0 , Â·)] in each iteration of the algorithm has unique Nash equilibrium. In addition, [102] has proposed the Friend-or-Foe Q-learning algorithm where
each agent views the other agent as either a â€œfriendâ€ or a â€œfoeâ€. In this case, Nash[QN (s0 , Â·)]
can be efficiently computed via linear programming. This algorithm can be viewed as
a generalization of minimax-Q learning, and Nash equilibrium convergence is guaranteed for two-player zero-sum games and coordination games with a unique equilibrium.
Furthermore, [317] has proposed correlated Q-learning, which replaces Nash[QN (s0 , Â·)] in
(4.26) by computing a correlated equilibrium [318], a more general equilibrium concept
than Nash equilibrium. In a recent work, [319] has proposed a batch RL method to find
an approximate Nash equilibrium via Bellman residue minimization [320]. They have
proved that the global minimizer of the empirical Bellman residue is an approximate
Nash equilibrium, followed by the error propagation analysis for the algorithm. Also
in the batch RL regime, [44] has considered a simplified mixed setting for decentralized
MARL: two teams of cooperative networked agents compete in a zero-sum Markov game.
A decentralized variant of FQI, where the agents within one team cooperate to solve (4.3)
while the two teams essentially solve (4.15), is proposed. Finite-sample error bounds have
then been established for the proposed algorithm.
To address the scalability issue, independent learning is preferred, which, however,
fails to converge in general [139]. [37] has proposed decentralized Q-learning, a two timescale
modification of Q-learning, that is guaranteed to converge to the equilibrium for weakly
acyclic Markov games almost surely. Each agent therein only observes local action and reward, and neither observes nor keeps track of othersâ€™ actions. All agents are instructed
to use the same stationary baseline policy for many consecutive stages, named exploration
phase. At the end of the exploration phase, all agents are synchronized to update their baseline policies, which makes the environment stationary for long enough, and enables the
convergence of Q-learning based methods. Note that these algorithms can also be applied
to the cooperative setting, as these games include Markov teams as a special case.
Policy-Based Methods

36

For continuous games, due to the general negative results therein, [32] introduces a
new class of games, Morse-Smale games, for which the gradient dynamics correspond to
gradient-like flows. Then, definitive statements on almost sure convergence of PG methods to either limit cycles, Nash equilibria, or non-Nash fixed points can be made, using
tools from dynamical systems theory. Moreover, [312, 321] have studied the second-order
structure of game dynamics, by decomposing it into two components.The first one, named
symmetric component, relates to potential games, which yields gradient descent on some
implicit function; the second one, named antisymmetric component, relates to Hamiltonian games that follows some conservation law, motivated by classical mechanical systems
analysis. The fact that gradient descent converges to the Nash equilibrium of both types
of games motivates the development of the Symplectic Gradient Adjustment algorithm
that finds stable fixed points of the game, which constitute all local Nash equilibria for
zero-sum games, and only a subset of local NE for general-sum games. [322] provides
finite-time local convergence guarantees to a neighborhood of a stable local Nash equilibrium of continuous games, in both deterministic setting, with exact PG, and stochastic
setting, with unbiased PG estimates. Additionally, [322] has also explored the effects of
non-uniform learning rates on the learning dynamics and convergence rates. [311] has
also considered general-sum Stackelberg games, and shown that the same two-timescale
algorithm update as in the zero-sum case now converges almost surely to the stable attractors only. It has also established finite-time performance for local convergence to a
neighborhood of a stable Stackelberg equilibrium. In complete analogy to the zero-sum
class, these convergence results for continuous games do not apply to MARL in Markov
games directly, as they are built upon the differentiability/smoothness of the long-term
return, which may not hold for general MGs, for example, LQ games [315]. Moreover,
most of these convergence results are local instead of global.
Other than continuous games, the policy-based methods summarized in Â§4.2.2 can
also be applied to the mixed setting via self-play. The validity of such an approach is
based a fundamental connection between game theory and online learning â€“ If the external regret of each agent is no more than , then their average policies constitute an
-approximate coarse correlated equilibrium [280, 261, 323] of the general-sum normalform games. Thus, although in general we are unable to find a Nash equilibrium, policy
optimization with self-play guarantees to find a coarse correlated equilibrium in these
normal-form games.
Mean-Field Regime
The scalability issue in the non-cooperative setting can also be alleviated in the meanfield regime, as the cooperative setting discussed in Â§4.1.1. For general-sum games, [172]
has proposed a modification of the Nash-Q learning algorithm where the actions of other
agents are approximated by their empirical average. That is, the action value function
of each agent i is parametrized by Qi (s, ai , Âµaâˆ’i ), where Âµaâˆ’i is the empirical distribution
of {aj : j , i}. Asymptotic convergence of this mean-field Nash-Q learning algorithm has
37

also been established.
Besides, most mean-field RL algorithms are focused on addressing the mean-field game
model. In mean-field games, each agent i has a local state si âˆˆ S and a local action ai âˆˆ A,
and the interaction among other agents is captured by an aggregated effect Âµ, also known
as the mean-field term, which is a functional of the empirical distribution of the local states
and actions of the agents. Specifically, at the t-th time step, when agent i takes action ait
at state sti and the mean-field term is Âµt , it receives an immediate reward R(sti , ait , Âµt ) and
i
its local state evolves into st+1
âˆ¼ P (Â· | sti , ait , Âµt ) âˆˆ âˆ†(S). Thus, from the perspective of agent
i, instead of participating in a multi-agent game, it is faced with a time-varying MDP
parameterized by the sequence of mean-field terms {Âµt }tâ‰¥0 , which in turn is determined
by the states and actions of all agents. The solution concept in MFGs is the mean-field
equilibrium, which is a sequence of pairs of policy and mean-field terms {Ï€tâˆ— , Âµâˆ—t }tâ‰¥0 that
satisfy the following two conditions: (1) Ï€âˆ— = {Ï€tâˆ— }tâ‰¥0 is the optimal policy for the timevarying MDP specified by Âµâˆ— = {Âµâˆ—t }tâ‰¥0 , and (2) Âµâˆ— is generated when each agent follows
policy Ï€âˆ— . The existence of the mean-field equilibrium for discrete-time MFGs has been
studied in [324, 325, 326, 327] and their constructive proofs exhibit that the mean-field
equilibrium can be obtained via a fixed-point iteration. Specifically, one can construct a
sequence of policies and mean-field terms {Ï€(i) }iâ‰¥0 and {Âµ(i) }iâ‰¥0 such that {Ï€(i) }tâ‰¥0 solves
the time-varying MDP specified by Âµ(i) , and Âµ(i+1) is generated when all players adopt
policy Ï€(i) . Following this agenda, various model-free RL methods are proposed for solving MFGs where {Ï€(i) }iâ‰¥0 is approximately solved via single-agent RL such as Q-learning
[328] and policy-based methods [24, 329], with {Âµ(i) }iâ‰¥0 being estimated via sampling.
In addition, [330, 331] recently propose fictitious play updates for the mean-field state
where we have Âµ(i+1) = (1 âˆ’ Î± (i) ) Â· Âµ(i) + Î± (i) Â· ÂµÌ‚(i+1) , with Î± (i) being the learning rate and
ÂµÌ‚(i+1) being the mean-field term generated by policy Ï€(i) . Note that the aforementioned
works focus on the settings with either finite horizon [330, 331] or stationary mean-field
equilibria [328, 24, 329] only. Instead, recent works [332, 333] consider possibly nonstationary mean-field equilibrium in infinite-horizon settings, and develop equilibrium
computation algorithms that have laid foundations for model-free RL algorithms.

5

Application Highlights

In this section, we briefly review the recent empirical successes of MARL driven by the
methods introduced in the previous section. In the following, we focus on the three MARL
settings reviewed in Â§4 and highlight four representative and practical applications in
each setting, as illustrated in Figure 3.

5.1

Cooperative Setting

Unmanned Aerial Vehicles

38

Figure 3: Four representative applications of recent successes of MARL: unmanned aerial
vehicles, game of Go, Poker games, and team-battle video games.
One prominent application of MARL is the control of practical multi-agent systems,
most of which are cooperative and decentralized. Examples of the scenarios include robot
team navigation [183], smart grid operation [180], and control of mobile sensor networks
[16]. Here we choose unmanned aerial vehicles (UAVs) [334, 335, 336, 337, 338, 339], a
recently surging application scenario of multi-agent autonomous systems, as one representative example. Specifically, a team of UAVs are deployed to accomplish a cooperation
task, usually without the coordination of any central controller, i.e., in a decentralized
fashion. Each UAV is normally equipped with communication devices, so that they can
exchange information with some of their teammates, provided that they are inside its
sensing and coverage range. As a consequence, this application naturally fits in the decentralized paradigm with networked agents we advocated in Â§4.1.2, which is also illustrated in Figure 2 (b). Due to the high-mobility of UAVs, the communication links among
agents are indeed time-varying and fragile, making (online) cooperation extremely challenging. Various challenges thus arise in the context of cooperative UAVs, some of which
have recently been addressed by MARL.
In [334], the UAVsâ€™ optimal links discovery and selection problem is considered. Each
UAV u âˆˆ U , where U is the set of all UAVs, has the capability to perceive the local available
channels and select a connected link over a common channel shared by another agent
T
v âˆˆ U . Each UAV u has its local set of channels Cu with Cu Cv , âˆ… for any u, v, and a
connected link between two adjacent UAVs is built if they announce their messages on
the same channel simultaneously. Each UAVâ€™s local state is whether the previous message has been successfully sent, and its action is to choose a pair (v, chu ), with v âˆˆ Tu and
chu âˆˆ Cu , where Tu is the set of teammates that agent u can reach. The availability of local channels chu âˆˆ Cu is modeled as probabilistic, and the reward Ru is calculated by the
number of messages that are successfully sent. Essentially, the algorithm in [334] is based
on independent Q-learning [139], but with two heuristics to improve the tractability and
convergence performance: by fractional slicing, it treats each dimension (fraction) of the
action space independently, and estimates the actual Q-value by the average of that for all
fractions; by mutual sampling, it shares both state-action pairs and a mutual Q-function
parameter. [335] addresses the problem of field coverage, where the UAVs aim to provide
a full coverage of an unknown field, while minimizing the overlapping sections among
their field of views. Modeled as a Markov team, the overall state s is the concatenation of
39

all local states si , which are defined as its 3-D position coordinates in the environment.
Each agent chooses to either head different directions, or go up and down, yielding 6
possible actions. A multi-agent Q-learning over the joint action space is developed, with
linear function approximation. In contrast, [337] focuses on spectrum sharing among a
network of UAVs. Under a remote sensing task, the UAVs are categorized into two clusters: the relaying ones that provide relay services and the other ones that gain spectrum
access for the remaining ones, which perform the sensing task. Such a problem can be
modeled as a deterministic MMDP, which can thus be solved by distributed Q-learning
proposed in [87], with optimality guaranteed. Moreover, [339] considers the problem
of simultaneous target-assignment and path-planning for multiple UAVs. In particular,
a team of UAVs Ui âˆˆ U, with each Ui â€™s position at time t given by (xiU (t), yiU (t)), aim to
cover all the targets Tj âˆˆ T without collision with the threat areas Di âˆˆ D, as well as with
other UAVs. For each Ui , a path Pi is planned as Pi = {(xiU (0), yiU (0), Â· Â· Â· , xiU (n), yiU (n))}, and
P
the length of Pi is denoted by di . Thus, the goal is to minimize i di while the collisionfree constraints are satisfied. By penalizing the collision in the reward function, such a
problem can be characterized as one with a mixed MARL setting that contains both cooperative and competitive agents. Hence, the MADDPG algorithm proposed in [26] is
adopted, with centralized-learning-decentralized-execution. Two other tasks that can be
tackled by MARL include resource allocation in UAV-enabled communication networks,
using Q-learning based method [338], aerial surveillance and base defense in UAV fleet
control, using policy optimization method in a purely centralized fashion [336].
Learning to Communicate
Another application of cooperative MARL aims to foster communication and coordination among a team of agents without explicit human supervision. Such a type of
problems is usually formulated as a Dec-POMDP involving N agents, which is similar to
the Markov game introduced in Definition 2.2 except that each agent cannot observe the
state s âˆˆ S and that each agent has the same reward function R. More specifically, we
assume that each agent i âˆˆ N receives observations from set Y i via a noisy observation
channel O i : S â†’ P (Yi ) such that agent i observes a random variable y i âˆ¼ O i (Â· | s) when the
environment is at state s. Note that this model can be viewed as a POMDP when there
is a central planner that collects the observations of each agent and decides the actions
for each agent. Due to the noisy observation channels, in such a model the agents need to
communicate with each other so as to better infer the underlying state and make decisions
that maximize the expected return shared by all agents. Let Nti âŠ† N be the neighbors of
jâ†’i
agent i at the t-th time step, that is, agent i is able to receive a message mt from any
agent j âˆˆ Nti at time t. Let Iti denote the information agent i collects up to time t, which is
defined as


[n o
j
jâ†’i
Iti = o`i , {a` }jâˆˆN , {m` }jâˆˆN i : ` âˆˆ {0, . . . , t âˆ’ 1}
oti , ,
(5.1)
`

40

which contains its history collected in previous time steps and the observation received
at time t. With the information Iti , agent i takes an action ait âˆˆ Ai and also broadcasts
iâ†’j
j
messages mt to all agents j with i âˆˆ Nt . That is, the policy Ï€ti of agent i is a mapping
iâ†’j
j
from Iti to a (random) action e
ait = (ait , {mt : i âˆˆ Nt }), i.e., e
ait âˆ¼ Ï€ti (Â· | Iti ). Notice that the size
of information set Iti grows as t grows. To handle the memory issue, it is common to first
embed Iti in a fixed latent space via recurrent neural network (RNN) or Long Short-Term
Memory (LSTM) [340] and define the value and policy functions on top of the embedded
features. Moreover, most existing works in this line of research adopt the paradigm of
centralized learning and utilize techniques such as weight-sharing or attention mechanism [341] to increase computational efficiency. With centralized learning, single-agent
RL algorithms such as Q-learning and actor-critic are readily applicable.
In particular, [21] first proposes to tackle the problem of learning to communicate via
deep Q-learning. They propose to use two Q networks that govern taking action ai âˆˆ A
and producing messages separately. Their training algorithm is an extension of the deep
recurrent Q-learning (DRQN) [342], which combines RNN and deep Q-learning [10]. Following [21], various works [343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354]
have proposed a variety of neural network architectures to foster communication among
agents. These works combine single-agent RL methods with novel developments in deep
learning, and demonstrate their performance via empirical studies. Among these works,
[346, 345, 348, 353, 354] have reported the emergence of computational communication
protocols among the agents when the RL algorithm is trained from scratch with text or
image inputs. We remark that the algorithms used in these works are more akin to singleagent RL due to centralized learning. For more details overviews of multi-agent communication, we refer the interested readers to Section 6 of [42] and Section 3 of [20].

5.2

Competitive Setting

Regarding the competitive setting, in the following, we highlight the recent applications
of MARL to the game of Go and Texas holdâ€™em poker, which are archetypal instances of twoplayer perfect-information and partial-information extensive-form games, respectively.
The Game of Go
The game of Go is a board game played by two competing players, with the goal of
surrounding more territory on the board than the opponent. These two players have
access to white or black stones respectively, and take turns placing their stones on a 19Ã—19
board, representing their territories. In each move, a player can place a stone to any of
the total 361 positions on the board that is not already taken by a stone. Once placed on
the board, the stones cannot be moved. But the stones will be removed from the board
when completely surrounded by opposing stones. The game terminates when neither of
the players is unwilling or unable to make a further move, and the winner is determined
by counting the area of the territory and the number of stones captured by the players.
41

The game of Go can be viewed as a two-player zero-sum Markov game with deterministic state transitions, and the reward only appears at the end of the game. The state
of this Markov game is the current configuration of the board and the reward is either
one or minus one, representing either a win or a loss, respectively. Specifically, we have
r 1 (s) + r 2 (s) = 0 for any state s âˆˆ S, and r 1 (s), r 2 (s) âˆˆ {1, âˆ’1} when s is a terminating state,
and r 1 (s) = r 2 (s) = 0 otherwise. Let Vâˆ—i (s) denote the optimal value function of player
i âˆˆ {1, 2}. Thus, in this case, [1 + V i (s)]/2 is the probability of player i âˆˆ {1, 2} winning
the game when the current state is s and both players follow the Nash equilibrium policies thereafter. Moreover, as this Markov game is turn-based, it is known that the Nash
equilibrium policies of the two players are deterministic [234]. Furthermore, since each
configuration of the board can be constructed from a sequence of moves of the two players due to deterministic transitions, we can also view the game of Go as an extensive-form
game with perfect information. This problem is notoriously challenging due to the gigantic state space. It is estimated in [355] that the size of state space exceeds 10360 , which
forbids the usage of any traditional reinforcement learning or searching algorithms.
A significant breakthrough has been made by the AlphaGo introduced in [1], which
is the first computer Go program that defeats a human professional player on a fullsized board. AlphaGo integrates a variety of ideas from deep learning and reinforcement
learning, and tackles the challenge of huge state space by representing the policy and
value functions using deep convolutional neural networks (CNN) [356]. Specifically, both
the policy and value networks are 13-layer CNNs with the same architecture, and a board
configuration is represented by 48 features. Thus, both the policy and value networks take
inputs of size 19 Ã— 19 Ã— 48. These two networks are trained through a novel combination
of supervised learning from human expert data and reinforcement learning from MonteCarlo tree search (MCTS) and self-play. Specifically, in the first stage, the policy network
is trained by supervised learning to predict the actions made by the human players, where
the dataset consists of 30 million positions from the KGS Go server. That is, for any stateaction pair (s, a) in the dataset, the action a is treated as the response variable and the state
s is regarded as the covariate. The weights of the policy network is trained via stochastic
gradient ascent to maximize the likelihood function. After initializing the policy network
via supervised learning, in the second stage of the pipeline, both the policy and value
networks are trained via reinforcement learning and self-play. In particular, new data are
generated by games played between the current policy network and a random previous
iteration of the policy network. Moreover, the policy network is updated following policy
gradient, and the value network aims to find the value function associated with the policy
network and is updated by minimizing the mean-squared prediction error. Finally, when
playing the game, the current iterates of the policy and value networks are combined to
produce an improved policy by lookahead search via MCTS. The actual action taken by
AlphaGo is determined by such an MCTS policy. Moreover, to improve computational
efficiency, AlphaGo uses an asynchronous and distributed version of MCTS to speed up
simulation.
42

Since the advent of AlphaGo, an improved version, known as AlphaGo Zero, has been
proposed in [2]. Compared with the vanilla AlphaGo, AlphaGo Zero does not use supervised learning to initialize the policy network. Instead, both the policy and value
networks are trained from scratch solely via reinforcement learning and self-play. Besides, instead of having separate policy and value functions share the same network
architecture, in AlphaGo Zero, these two networks are aggregated into a single neural network structure. Specifically, the policy and value functions are represented by
(p(s), V (s)) = fÎ¸ (s), where s âˆˆ S is the state which represents the current board, fÎ¸ is a deep
CNN with parameter Î¸, V (s) is a scalar that corresponds to the value function, and p(s) is
a vector which represents the policy, i.e., for each entry a âˆˆ A, pa (s) is the probability of
taking action a at state s. Thus, under such a network structure, the policy and value networks automatically share the same low-level representations of the states. Moreover, the
parameter Î¸ of network fÎ¸ is trained via self-play and MCTS. Specifically, at each timestep t, based on the policy p and value V given by fÎ¸t , an MCTS policy Ï€t can be obtained
and a move is executed following policy Ï€t (st ). Such a simulation procedure continues
until the current game terminates. Then the outcome of the t-th time-step, zt âˆˆ {1, âˆ’1}, is
recorded, according to the perspective of the player at time-step t. Then the parameter Î¸
is updated by following a stochastic gradient step on a loss function `t , which is defined
as


`t (Î¸) = [z âˆ’ V (st )]2 âˆ’ Ï€t> log p(st ) + c Â· kÎ¸k22 ,
p(Â·), V (Â·) = fÎ¸ (Â·).
Thus, `t is the sum of the mean-squared prediction error of the value function, crossentropy loss between the policy network and the MCTS policy, and a weight-decay term
for regularization. It is reported that AlphaGo Zero has defeated the strongest versions of
the previous AlphaGo and that it also has demonstrated non-standard Go strategies that
had not been discovered before. Finally, the techniques adopted in AlphaGo Zero has
been generalized to other challenging board games. Specifically, [357] proposes the AlphaZero program that is trained by self-play and reinforcement learning with zero human
knowledge, and achieves superhuman performance in the games of chess, shogi, and Go.
Texas Holdâ€™em Poker
Another remarkable applicational achievement of MARL in the competitive setting
focuses on developing artificial intelligence in the Texas holdâ€™em poker, which is one of
the most popular variations of the poker. Texas holdâ€™em is usually played by a group of
two or more players, where each player is first dealt with two private cards face down.
Then five community cards are dealt face up in three rounds. In each round. each player
has four possible actions â€“ check, call, raise, and fold. After all the cards are dealt, each
player who has not folded have seven cards in total, consisting of five community cards
and two private cards. Each of these players then finds the best five-card poker hand out
of all combinations of the seven cards. The player with the best hand is the winner and

43

wins all the money that the players wager for that hand, which is also known as the pot.
Note that each hand of Texas holdâ€™em terminates after three rounds, and the payoffs of
the player are only known after the hand ends. Also notice that each player is unaware of
the private cards of the rest of the players. Thus, Texas holdâ€™em is an instance of multiplayer extensive-form game with incomplete information. The game is called heads-up
when there are only two players. When both the bet sizes and the amount of allowed
raises are fixed, the game is called limit holdâ€™em. In the no-limit holdâ€™em, however, each
player may bet or raise any amount up to all of the money the player has at the table, as
long as it exceeds the previous bet or raise.
There has been quest for developing superhuman computer poker programs for over
two decades [358, 113]. Various methods have been shown successful for simple variations of poker such as Kuhn poker [359] and Leduc holdâ€™em [360]. However, the fullfledged Texas holdâ€™em is much more challenging and several breakthroughs have been
achieved only recently. The simplest version of Texas holdâ€™em is heads-up limit holdâ€™em
(HULHE), which has 3.9 Ã— 1014 information sets in total [361], where a player is required
to take an action at each information set. [361] has for the first time reported solving
HULHE to approximate Nash equilibrium via CFR+ [293, 294], a variant of counterfactual regret minimization [109]. Subsequently, other methods such as Neural Fictitious
Self-Play [25] and Monte-Carlo tree search with self-play [362] have also been adopted to
successfully solve HULHE.
Despite these breakthroughs, solving heads-up no-limit holdâ€™em (HUNL) with artificial
intelligence has remained open until recently, which has more than 6 Ã— 10161 information
sets, an astronomical number. Thus, in HUNL, it is impossible (in todayâ€™s computational
power) to traverse all information sets, making it inviable to apply CFR+ as in [361].
Ground-breaking achievements have recently been made by DeepStack [363] and Libratus [364], two computer poker programs developed independently, which defeat human
professional poker players in HUNL for the first time. Both of these programs adopt CFR
as the backbone of their algorithmic frameworks, but adopt different strategies for handling the gigantic size of the game. In particular, DeepStack applies deep learning to
learn good representations of the game and proposes deep counterfactual value networks to
integrate deep learning and CFR. Moreover, DeepStack adopts limited depth lookahead
planning to reduce the gigantic 6 Ã— 10161 information sets to no more than 1017 information sets, thus making it possible to enumerate all information sets. In contrast, Libratus
does not utilize any deep learning techniques. Instead, it reduces the size of the game
by computation of an abstraction of the game, which is possible since many of the information sets are very similar. Moreover, it further reduces the complexity by using the
sub-game decomposition technique [365, 366, 367] for imperfect-information games and
by constructing fine-grained abstractions of the sub-games. When the abstractions are
constructed, an improved version of the Monte-Carlo CFR [281, 282, 283] is utilized to
compute the policy. Furthermore, very recently, based upon Libratus, [8] has proposed
Pluribus, a computer poker program that has been shown to be stronger than top human
44

professionals in no-limit Texas holdâ€™em poker with six players. The success of Pluribus
is attributed to the following techniques that have appeared in the literature: abstraction
and sub-game decomposition for large-scale imperfect-information games, Monte-Carlo
CFR, self-play, and depth-limited search.
Other Applications
Furthermore, another popular testbed of MARL is the StarCraft II [368], which is an
immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has
only limited information of the game state. Designing reinforcement learning systems
for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run,
and to design good reward functions that elicit learning. Since released, both the fullgame and sub-game versions of StarCraft II have gained tremendous research interest. A
breakthrough in this game was achieved by AlphaStar, recently proposed in [369], which
has demonstrated superhuman performance in zero-sum two-player full-game StarCraft
II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic [370] for policy updates, and Neural
Fictitious Self-play [25] for equilibrium finding.

5.3

Mixed Settings

Compared to the cooperative and competitive settings, research on MARL under the
mixed setting is rather less explored. One application in this setting is multi-player poker.
As we have mentioned in Â§5.2, Pluribus introduced in [8] has demonstrated superhuman
performance in six-player no-limit Texas holdâ€™em. In addition, as an extension of the
problem of learning to communicate, introduced in Â§5.1, there is a line of research that
aims to apply MARL to tackle learning social dilemmas, which is usually formulated as a
multi-agent stochastic game with partial information. Thus, most of the algorithms proposed under these settings incorporate RNN or LSTM for learning representations of the
histories experienced by the agent, and the performance of these algorithms are usually
exhibited using experimental results; see, e.g., [19, 371, 372], and the references therein.
Moreover, another example of the mixed setting is the case where the agents are divided into two opposing teams that play zero-sum games. The reward of a team is shared
by each player within this team. Compared with two-player zero-sum games, this setting
is more challenging in that both cooperation among teammates and competition against
the opposing team need to be taken into consideration. A prominent testbed of this case
is the Dota 2 video game, where each of the two teams, each with five players, aims to
conquer the base of the other team and defend its own base. Each player independently
controls a powerful character known as the hero, and only observes the state of the game
via the video output on the screen. Thus, Dota 2 is a zero-sum Markov game played by
45

two teams, with each agent having imperfect information of the game. For this challenging problem, in 2018, OpenAI has proposed the OpenAI Five AI system [3], which enjoys
superhuman performance and has defeated human world champions in an e-sports game.
The algorithmic framework integrates LSTM for learning good representations and proximal policy optimization [72] with self-play for policy learning. Moreover, to balance
between effective coordination and communication cost, instead of having explicit communication channels among the teams, OpenAI Five utilizes reward shaping by having
a hyperparameter, named â€œteam spiritâ€, to balance the relative importance between each
heroâ€™s individual reward function and the average of the teamâ€™s reward function.

6

Conclusions and Future Directions

Multi-agent RL has long been an active and significant research area in reinforcement
learning, in view of the ubiquity of sequential decision-making with multiple agents coupled in their actions and information. In stark contrast to its great empirical success, theoretical understanding of MARL algorithms is well recognized to be challenging and relatively lacking in the literature. Indeed, establishing an encompassing theory for MARL
requires tools spanning dynamic programming, game theory, optimization theory, and
statistics, which are non-trivial to unify and investigate within one context.
In this chapter, we have provided a selective overview of mostly recent MARL algorithms, backed by theoretical analysis, followed by several high-profile but challenging
applications that have been addressed lately. Following the classical overview [11], we
have categorized the algorithms into three groups: those solving problems that are fully
cooperative, fully competitive, and a mix of the two. Orthogonal to the existing reviews
on MARL, this chapter has laid emphasis on several new angles and taxonomies of MARL
theory, some of which have been drawn from our own research endeavors and interests.
We note that our overview should not be viewed as a comprehensive one, but instead as
a focused one dictated by our own interests and expertise, which should appeal to researchers of similar interests, and provide a stimulus for future research directions in this
general topical area. Accordingly, we have identified the following paramount while open
avenues for future research on MARL theory.
Partially observed settings: Partial observability of the system states and the actions of
other agents is quintessential and inevitable in many practical MARL applications. In
general, these settings can be modeled as a partially observed stochastic game (POSG),
which includes the cooperative setting with a common reward function, i.e., the DecPOMDP model, as a special case. Nevertheless, as pointed out in Â§4.1.3, even the cooperative task is NEXP-complete [104] and difficult to solve. In fact, the information state for
optimal decision-making in POSGs can be very complicated and involve belief generation
over the opponentsâ€™ policies [119], compared to that in POMDPs, which requires belief
on only states. This difficulty essentially stems from the heterogenous beliefs of agents
46

resulting from their own observations obtained from the model, an inherent challenge
of MARL mentioned in Â§3 due to various information structures. It might be possible to
start by generalizing the centralized-learning-decentralized-execution scheme for solving
Dec-POMDPs [121, 152] to solving POSGs.
Deep MARL theory: As mentioned in Â§3.3, using deep neural networks for function
approximation can address the scalability issue in MARL. In fact, most of the recent empirical successes in MARL result from the use of DNNs [25, 26, 27, 28, 29]. Nonetheless,
because of lack of theoretical backings, we have not included details of these algorithms in
this chapter. Very recently, a few attempts have been made to understand the global convergence of several single-agent deep RL algorithms, such as neural TD learning [373] and
neural policy optimization [79, 78], when overparameterized neural networks [374, 375]
are used. It is thus promising to extend these results to multi-agent settings, as initial
steps toward theoretical understanding of deep MARL.
Model-based MARL: It may be slightly surprising that very few MARL algorithms in
the literature are model-based, in the sense that the MARL model is first estimated, and
then used as a nominal one to design algorithms. To the best of our knowledge, the only
existing model-based MARL algorithms include the early one in [376] that solves singlecontroller-stochastic games, a special zero-sum MG; and the later improved one in [377],
named R-MAX, for zero-sum MGs. These algorithms are also built upon the principle
of optimism in the face of uncertainty [243, 244], as several aforementioned model-free
ones. Considering recent progresses in model-based RL, especially its provable advantages over model-free ones in certain regimes [378, 379], it is worth generalizing these
results to MARL to improve its sample efficiency.
Convergence of policy gradient methods: As mentioned in Â§4.3, the convergence result
of vanilla policy gradient method in general MARL is mostly negative, i.e., it may avoid
even the local NE points in many cases. This is essentially related to the challenge of nonstationarity in MARL, see Â§3.2. Even though some remedies have been advocated [312,
321, 322, 311] to stabilize the convergence in general continuous games, these assumptions
are not easily verified/satisfied in MARL, e.g., even in the simplest LQ setting [315], as
they depend not only on the model, but also on the policy parameterization. Due to this
subtlety, it may be interesting to explore the (global) convergence of policy-based methods
for MARL, probably starting with the simple LQ setting, i.e., general-sum LQ games, in
analogy to that for the zero-sum counterpart [34, 306]. Such an exploration may also
benefit from the recent advances of nonconvex-(non)concave optimization [380, 33, 314].
MARL with robustness/safety concerns: Concerning the challenge of non-unique learning goals in MARL (see Â§3.1), we believe it is of merit to consider robustness and/or safety
constraints in MARL. To the best of our knowledge, this is still a relatively uncharted
territory. In fact, safe RL has been recognized as one of the most significant challenges

47

in the single-agent setting [381]. With more than one agents that may have conflicted
objectives, guaranteeing safety becomes more involved, as the safety requirement now
concerns the coupling of all agents. One straightforward model is constrained multiagent MDPs/Markov games, with the constraints characterizing the safety requirement.
Learning with provably safety guarantees in this setting is non-trivial, but necessary for
some safety-critical MARL applications as autonomous driving [9] and robotics [5]. In
addition, it is also natural to think of robustness against adversarial agents, especially in
the decentralized/distributed cooperative MARL settings as in [23, 130, 96], where the
adversary may disturb the learning process in an anonymous way â€“ a common scenario
in distributed systems. Recent development of robust distributed supervised-learning
against Byzantine adversaries [382, 383] may be useful in this context.

References
[1] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Mastering the game of
Go with deep neural networks and tree search. Nature 529(7587), 484â€“489 (2016)
[2] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,
Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of Go without human knowledge.
Nature 550(7676), 354 (2017)
[3] OpenAI: Openai five. https://blog.openai.com/openai-five/ (2018)
[4] Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki, W.M.,
Dudzik, A., Huang, A., Georgiev, P., Powell, R., Ewalds, T., Horgan, D., Kroiss, M.,
Danihelka, I., Agapiou, J., Oh, J., Dalibard, V., Choi, D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy, J., Cai, T., Budden, D., Paine, T., Gulcehre, C., Wang, Z., Pfaff,
T., Pohlen, T., Wu, Y., Yogatama, D., Cohen, J., McKinney, K., Smith, O., Schaul,
T., Lillicrap, T., Apps, C., Kavukcuoglu, K., Hassabis, D., Silver, D.: AlphaStar:
Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/
alphastar-mastering-real-time-strategy-game-starcraft-ii/ (2019)
[5] Kober, J., Bagnell, J.A., Peters, J.: Reinforcement learning in robotics: A survey. International Journal of Robotics Research 32(11), 1238â€“1274 (2013)
[6] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D.:
Continuous control with deep reinforcement learning. In: International Conference on
Learning Representations (2016)
[7] Brown, N., Sandholm, T.: Libratus: the superhuman ai for no-limit Poker. In: International
Joint Conference on Artificial Intelligence, pp. 5226â€“5228 (2017)
[8] Brown, N., Sandholm, T.: Superhuman AI for multiplayer Poker. Science 365, 885â€“890
(2019)

48

[9] Shalev-Shwartz, S., Shammah, S., Shashua, A.: Safe, multi-agent, reinforcement learning for
autonomous driving. arXiv preprint arXiv:1610.03295 (2016)
[10] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A.,
Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level control through deep
reinforcement learning. Nature 518(7540), 529â€“533 (2015)
[11] Busoniu, L., Babuska, R., De Schutter, B., et al.: A comprehensive survey of multiagent
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C 38(2),
156â€“172 (2008)
[12] Adler, J.L., Blue, V.J.: A cooperative multi-agent transportation management and route
guidance system. Transportation Research Part C: Emerging Technologies 10(5), 433â€“454
(2002)
[13] Wang, S., Wan, J., Zhang, D., Li, D., Zhang, C.: Towards smart factory for industry 4.0: A
self-organized multi-agent system with big data based feedback and coordination. Computer Networks 101, 158â€“168 (2016)
[14] O, J., Lee, J.W., Zhang, B.T.: Stock trading system using reinforcement learning with cooperative agents. In: International Conference on Machine Learning, pp. 451â€“458 (2002)
[15] Lee, J.W., Park, J., Jangmin, O., Lee, J., Hong, E.: A multiagent approach to Q-learning for
daily stock trading. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems
and Humans 37(6), 864â€“877 (2007)
[16] Cortes, J., Martinez, S., Karatas, T., Bullo, F.: Coverage control for mobile sensing networks.
IEEE Transactions on Robotics and Automation 20(2), 243â€“255 (2004)
[17] Choi, J., Oh, S., Horowitz, R.: Distributed learning and cooperative control for multi-agent
systems. Automatica 45(12), 2802â€“2814 (2009)
[18] Castelfranchi, C.: The theory of social functions: Challenges for computational social science and multi-agent learning. Cognitive Systems Research 2(1), 5â€“38 (2001)
[19] Leibo, J.Z., Zambaldi, V., Lanctot, M., Marecki, J., Graepel, T.: Multi-agent reinforcement learning in sequential social dilemmas. In: International Conference on Autonomous
Agents and Multi-Agent Systems, pp. 464â€“473 (2017)
[20] Hernandez-Leal, P., Kartal, B., Taylor, M.E.: A survey and critique of multiagent deep reinforcement learning. arXiv preprint arXiv:1810.05587 (2018)
[21] Foerster, J., Assael, Y.M., de Freitas, N., Whiteson, S.: Learning to communicate with deep
multi-agent reinforcement learning. In: Advances in Neural Information Processing Systems, pp. 2137â€“2145 (2016)
[22] Zazo, S., Macua, S.V., SaÌnchez-FernaÌndez, M., Zazo, J.: Dynamic potential games with constraints: Fundamentals and applications in communications. IEEE Transactions on Signal
Processing 64(14), 3806â€“3821 (2016)

49

[23] Zhang, K., Yang, Z., Liu, H., Zhang, T., BasÌ§ar, T.: Fully decentralized multi-agent reinforcement learning with networked agents. In: International Conference on Machine Learning,
pp. 5867â€“5876 (2018)
[24] Subramanian, J., Mahajan, A.: Reinforcement learning in stationary mean-field games. In:
International Conference on Autonomous Agents and Multi-Agent Systems, pp. 251â€“259
(2019)
[25] Heinrich, J., Silver, D.: Deep reinforcement learning from self-play in imperfect-information
games. arXiv preprint arXiv:1603.01121 (2016)
[26] Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I.: Multi-agent actor-critic for
mixed cooperative-competitive environments. In: Advances in Neural Information Processing Systems, pp. 6379â€“6390 (2017)
[27] Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., Whiteson, S.: Counterfactual multi-agent
policy gradients. arXiv preprint arXiv:1705.08926 (2017)
[28] Gupta, J.K., Egorov, M., Kochenderfer, M.: Cooperative multi-agent control using deep reinforcement learning. In: International Conference on Autonomous Agents and Multi-Agent
Systems, pp. 66â€“83 (2017)
[29] Omidshafiei, S., Pazis, J., Amato, C., How, J.P., Vian, J.: Deep decentralized multi-task multiagent reinforcement learning under partial observability. In: International Conference on
Machine Learning, pp. 2681â€“2690 (2017)
[30] Kawamura, K., Mizukami, N., Tsuruoka, Y.: Neural fictitious self-play in imperfect information games with many players. In: Workshop on Computer Games, pp. 61â€“74 (2017)
[31] Zhang, L., Wang, W., Li, S., Pan, G.: Monte Carlo neural fictitious self-play: Approach to approximate Nash equilibrium of imperfect-information games. arXiv preprint
arXiv:1903.09569 (2019)
[32] Mazumdar, E., Ratliff, L.J.: On the convergence of gradient-based learning in continuous
games. arXiv preprint arXiv:1804.05464 (2018)
[33] Jin, C., Netrapalli, P., Jordan, M.I.: Minmax optimization: Stable limit points of gradient
descent ascent are locally optimal. arXiv preprint arXiv:1902.00618 (2019)
[34] Zhang, K., Yang, Z., BasÌ§ar, T.: Policy optimization provably converges to Nash equilibria in
zero-sum linear quadratic games. In: Advances in Neural Information Processing Systems
(2019)
[35] Sidford, A., Wang, M., Yang, L.F., Ye, Y.: Solving discounted stochastic two-player games
with near-optimal time and sample complexity. arXiv preprint arXiv:1908.11071 (2019)
[36] Oliehoek, F.A., Amato, C.: A Concise Introduction to Decentralized POMDPs, vol. 1.
Springer (2016)

50

[37] Arslan, G., YuÌˆksel, S.: Decentralized Q-learning for stochastic teams and games. IEEE
Transactions on Automatic Control 62(4), 1545â€“1558 (2017)
[38] Yongacoglu, B., Arslan, G., YuÌˆksel, S.: Learning team-optimality for decentralized stochastic control and dynamic games. arXiv preprint arXiv:1903.05812 (2019)
[39] Zhang, K., Miehling, E., BasÌ§ar, T.: Online planning for decentralized stochastic control with
partial history sharing. In: IEEE American Control Conference, pp. 167â€“172 (2019)
[40] Hernandez-Leal, P., Kaisers, M., Baarslag, T., de Cote, E.M.: A survey of learning in multiagent environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183 (2017)
[41] Nguyen, T.T., Nguyen, N.D., Nahavandi, S.: Deep reinforcement learning for multiagent systems: A review of challenges, solutions and applications. arXiv preprint
arXiv:1812.11794 (2018)
[42] Oroojlooy Jadid, A., Hajinezhad, D.: A review of cooperative multi-agent deep reinforcement learning. arXiv preprint arXiv:1908.03963 (2019)
[43] Zhang, K., Yang, Z., BasÌ§ar, T.: Networked multi-agent reinforcement learning in continuous
spaces. In: IEEE Conference on Decision and Control, pp. 2771â€“2776 (2018)
[44] Zhang, K., Yang, Z., Liu, H., Zhang, T., BasÌ§ar, T.: Finite-sample analyses for fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783 (2018)
[45] Monahan, G.E.: State of the artâ€“A survey of partially observable Markov decision processes:
Theory, models, and algorithms. Management Science 28(1), 1â€“16 (1982)
[46] Cassandra, A.R.: Exact and approximate algorithms for partially observable Markov decision processes. Brown University (1998)
[47] Bertsekas, D.P.: Dynamic Programming and Optimal Control, vol. 1. Athena Scientific
Belmont, MA (2005)
[48] Watkins, C.J., Dayan, P.: Q-learning. Machine Learning 8(3-4), 279â€“292 (1992)
[49] SzepesvaÌri, C., Littman, M.L.: A unified analysis of value-function-based reinforcementlearning algorithms. Neural Computation 11(8), 2017â€“2060 (1999)
[50] Singh, S., Jaakkola, T., Littman, M.L., SzepesvaÌri, C.: Convergence results for single-step
on-policy reinforcement-learning algorithms. Machine Learning 38(3), 287â€“308 (2000)
[51] Chang, H.S., Fu, M.C., Hu, J., Marcus, S.I.: An adaptive sampling algorithm for solving
Markov decision processes. Operations Research 53(1), 126â€“139 (2005)
[52] Kocsis, L., SzepesvaÌri, C.: Bandit based Monte-Carlo planning. In: European Conference on
Machine Learning, pp. 282â€“293. Springer (2006)
[53] Coulom, R.: Efficient selectivity and backup operators in Monte-Carlo tree search. In: International Conference on Computers and Games, pp. 72â€“83 (2006)

51

[54] Agrawal, R.: Sample mean based index policies by O(logn) regret for the multi-armed bandit problem. Advances in Applied Probability 27(4), 1054â€“1078 (1995)
[55] Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed bandit problem. Machine Learning 47(2-3), 235â€“256 (2002)
[56] Jiang, D., Ekwedike, E., Liu, H.: Feedback-based tree search for reinforcement learning. In:
International Conference on Machine Learning, pp. 2284â€“2293 (2018)
[57] Shah, D., Xie, Q., Xu, Z.: On reinforcement learning using Monte-Carlo tree search with
supervised learning: Non-asymptotic analysis. arXiv preprint arXiv:1902.05213 (2019)
[58] Tesauro, G.: Temporal difference learning and TD-Gammon. Communications of the ACM
38(3), 58â€“68 (1995)
[59] Tsitsiklis, J.N., Van Roy, B.: Analysis of temporal-diffference learning with function approximation. In: Advances in Neural Information Processing Systems, pp. 1075â€“1081 (1997)
[60] Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press (2018)
[61] Sutton, R.S., SzepesvaÌri, C., Maei, H.R.: A convergent O(n) algorithm for off-policy
temporal-difference learning with linear function approximation. Advances in Neural Information Processing Systems 21(21), 1609â€“1616 (2008)
[62] Sutton, R.S., Maei, H.R., Precup, D., Bhatnagar, S., Silver, D., SzepesvaÌri, C., Wiewiora,
E.: Fast gradient-descent methods for temporal-difference learning with linear function approximation. In: International Conference on Machine Learning, pp. 993â€“1000 (2009)
[63] Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., Petrik, M.: Finite-sample analysis of
proximal gradient TD algorithms. In: Conference on Uncertainty in Artificial Intelligence,
pp. 504â€“513 (2015)
[64] Bhatnagar, S., Precup, D., Silver, D., Sutton, R.S., Maei, H.R., SzepesvaÌri, C.: Convergent
temporal-difference learning with arbitrary smooth function approximation. In: Advances
in Neural Information Processing Systems, pp. 1204â€“1212 (2009)
[65] Dann, C., Neumann, G., Peters, J., et al.: Policy evaluation with temporal differences: A
survey and comparison. Journal of Machine Learning Research 15, 809â€“883 (2014)
[66] Sutton, R.S., McAllester, D.A., Singh, S.P., Mansour, Y.: Policy gradient methods for reinforcement learning with function approximation. In: Advances in Neural Information
Processing Systems, pp. 1057â€“1063 (2000)
[67] Williams, R.J.: Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8(3-4), 229â€“256 (1992)
[68] Baxter, J., Bartlett, P.L.: Infinite-horizon policy-gradient estimation. Journal of Artificial
Intelligence Research 15, 319â€“350 (2001)

52

[69] Konda, V.R., Tsitsiklis, J.N.: Actor-critic algorithms. In: Advances in Neural Information
Processing Systems, pp. 1008â€“1014 (2000)
[70] Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M.: Natural actor-critic algorithms. Automatica 45(11), 2471â€“2482 (2009)
[71] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M.: Deterministic policy
gradient algorithms. In: International Conference on Machine Learning, pp. 387â€“395 (2014)
[72] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 (2017)
[73] Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust region policy optimization.
In: International Conference on Machine Learning, pp. 1889â€“1897 (2015)
[74] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290
(2018)
[75] Yang, Z., Zhang, K., Hong, M., BasÌ§ar, T.: A finite sample analysis of the actor-critic algorithm. In: IEEE Conference on Decision and Control, pp. 2759â€“2764 (2018)
[76] Zhang, K., Koppel, A., Zhu, H., BasÌ§ar, T.: Global convergence of policy gradient methods to
(almost) locally optimal policies. arXiv preprint arXiv:1906.08383 (2019)
[77] Agarwal, A., Kakade, S.M., Lee, J.D., Mahajan, G.: Optimality and approximation with
policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261
(2019)
[78] Liu, B., Cai, Q., Yang, Z., Wang, Z.: Neural proximal/trust region policy optimization attains
globally optimal policy. arXiv preprint arXiv:1906.10306 (2019)
[79] Wang, L., Cai, Q., Yang, Z., Wang, Z.: Neural policy gradient methods: Global optimality
and rates of convergence. arXiv preprint arXiv:1909.01150 (2019)
[80] Chen, Y., Wang, M.: Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516 (2016)
[81] Wang, M.: Primal-dual Ï€ learning: Sample complexity and sublinear run time for ergodic
Markov decision problems. arXiv preprint arXiv:1710.06100 (2017)
[82] Shapley, L.S.: Stochastic games. Proceedings of the National Academy of Sciences 39(10),
1095â€“1100 (1953)
[83] Littman, M.L.: Markov games as a framework for multi-agent reinforcement learning. In:
International Conference on Machine Learning, pp. 157â€“163 (1994)
[84] Filar, J., Vrieze, K.: Competitive Markov Decision Processes. Springer Science & Business
Media (2012)

53

[85] BasÌ§ar, T., Olsder, G.J.: Dynamic Noncooperative Game Theory, vol. 23. SIAM (1999)
[86] Boutilier, C.: Planning, learning and coordination in multi-agent decision processes. In:
Conference on Theoretical Aspects of Rationality and Knowledge, pp. 195â€“210 (1996)
[87] Lauer, M., Riedmiller, M.: An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In: International Conference on Machine Learning (2000)
[88] Yoshikawa, T.: Decomposition of dynamic team decision problems. IEEE Transactions on
Automatic Control 23(4), 627â€“632 (1978)
[89] Ho, Y.C.: Team decision theory and information structures. Proceedings of the IEEE 68(6),
644â€“654 (1980)
[90] Wang, X., Sandholm, T.: Reinforcement learning to play an optimal Nash equilibrium in
team Markov games. In: Advances in Neural Information Processing Systems, pp. 1603â€“
1610 (2003)
[91] Mahajan, A.: Sequential decomposition of sequential dynamic teams: Applications to realtime communication and networked control systems. Ph.D. thesis, University of Michigan
(2008)
[92] GonzaÌlez-SaÌnchez, D., HernaÌndez-Lerma, O.: Discrete-Time Stochastic Control and Dynamic Potential Games: The Euler-Equation Approach. Springer Science & Business Media
(2013)
[93] Valcarcel Macua, S., Zazo, J., Zazo, S.: Learning parametric closed-loop policies for Markov
potential games. In: International Conference on Learning Representations (2018)
[94] Kar, S., Moura, J.M., Poor, H.V.: QD-learning: A collaborative distributed strategy for multiagent reinforcement learning through consensus + innovations. IEEE Transactions on Signal
Processing 61(7), 1848â€“1862 (2013)
[95] Doan, T., Maguluri, S., Romberg, J.: Finite-time analysis of distributed TD (0) with linear
function approximation on multi-agent reinforcement learning. In: International Conference on Machine Learning, pp. 1626â€“1635 (2019)
[96] Wai, H.T., Yang, Z., Wang, Z., Hong, M.: Multi-agent reinforcement learning via double averaging primal-dual optimization. In: Advances in Neural Information Processing Systems,
pp. 9649â€“9660 (2018)
[97] OpenAI: Openai dota 2 1v1 bot. https://openai.com/the-international/ (2017)
[98] Jacobson, D.: Optimal stochastic linear systems with exponential performance criteria and
their relation to deterministic differential games. IEEE Transactions on Automatic Control
18(2), 124â€“131 (1973)
[99] BasÌ§ar, T., Bernhard, P.: Hâˆž Optimal Control and Related Minimax Design Problems: A
Dynamic Game Approach. BirkhaÌˆuser, Boston. (1995)

54

[100] Zhang, K., Hu, B., BasÌ§ar, T.: Policy optimization for H2 linear control with Hâˆž robustness guarantee: Implicit regularization and global convergence. arXiv preprint
arXiv:1910.09496 (2019)
[101] Hu, J., Wellman, M.P.: Nash Q-learning for general-sum stochastic games. Journal of Machine Learning Research 4(Nov), 1039â€“1069 (2003)
[102] Littman, M.L.: Friend-or-Foe Q-learning in general-sum games. In: International Conference on Machine Learning, pp. 322â€“328 (2001)
[103] Lagoudakis, M.G., Parr, R.: Learning in zero-sum team Markov games using factored value
functions. In: Advances in Neural Information Processing Systems, pp. 1659â€“1666 (2003)
[104] Bernstein, D.S., Givan, R., Immerman, N., Zilberstein, S.: The complexity of decentralized
control of Markov decision processes. Mathematics of Operations Research 27(4), 819â€“840
(2002)
[105] Osborne, M.J., Rubinstein, A.: A Course in Game Theory. MIT Press (1994)
[106] Shoham, Y., Leyton-Brown, K.: Multiagent Systems: Algorithmic, Game-theoretic, and Logical Foundations. Cambridge University Press (2008)
[107] Koller, D., Megiddo, N.: The complexity of two-person zero-sum games in extensive form.
Games and Economic Behavior 4(4), 528â€“552 (1992)
[108] Kuhn, H.: Extensive games and the problem op information. Contributions to the Theory
of Games 2, 193â€“216 (1953)
[109] Zinkevich, M., Johanson, M., Bowling, M., Piccione, C.: Regret minimization in games with
incomplete information. In: Advances in Neural Information Processing Systems, pp. 1729â€“
1736 (2008)
[110] Heinrich, J., Lanctot, M., Silver, D.: Fictitious self-play in extensive-form games. In: International Conference on Machine Learning, pp. 805â€“813 (2015)
[111] Srinivasan, S., Lanctot, M., Zambaldi, V., PeÌrolat, J., Tuyls, K., Munos, R., Bowling, M.:
Actor-critic policy optimization in partially observable multiagent environments. In: Advances in Neural Information Processing Systems, pp. 3422â€“3435 (2018)
[112] Omidshafiei, S., Hennes, D., Morrill, D., Munos, R., Perolat, J., Lanctot, M., Gruslys, A.,
Lespiau, J.B., Tuyls, K.: Neural replicator dynamics. arXiv preprint arXiv:1906.00190
(2019)
[113] Rubin, J., Watson, I.: Computer Poker: A review. Artificial Intelligence 175(5-6), 958â€“987
(2011)
[114] Lanctot, M., Lockhart, E., Lespiau, J.B., Zambaldi, V., Upadhyay, S., PeÌrolat, J., Srinivasan,
S., Timbers, F., Tuyls, K., Omidshafiei, S., et al.: Openspiel: A framework for reinforcement
learning in games. arXiv preprint arXiv:1908.09453 (2019)

55

[115] Claus, C., Boutilier, C.: The dynamics of reinforcement learning in cooperative multiagent
systems. AAAI Conference on Artificial Intelligence 1998(746-752), 2 (1998)
[116] Bowling, M., Veloso, M.: Rational and convergent learning in stochastic games. In: International Joint Conference on Artificial Intelligence, vol. 17, pp. 1021â€“1026 (2001)
[117] Kapetanakis, S., Kudenko, D.: Reinforcement learning of coordination in cooperative multiagent systems. AAAI Conference on Artificial Intelligence 2002, 326â€“331 (2002)
[118] Conitzer, V., Sandholm, T.: Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents. Machine Learning 67(1-2), 23â€“43 (2007)
[119] Hansen, E.A., Bernstein, D.S., Zilberstein, S.: Dynamic programming for partially observable stochastic games. In: AAAI Conference on Artificial Intelligence, pp. 709â€“715 (2004)
[120] Amato, C., Chowdhary, G., Geramifard, A., UÌˆre, N.K., Kochenderfer, M.J.: Decentralized
control of partially observable markov decision processes. In: IEEE Conference on Decision
and Control, pp. 2398â€“2405 (2013)
[121] Amato, C., Oliehoek, F.A.: Scalable planning and learning for multiagent POMDPs. In:
AAAI Conference on Artificial Intelligence (2015)
[122] Shoham, Y., Powers, R., Grenager, T.: Multi-agent reinforcement learning: A critical survey.
Technical Report (2003)
[123] Zinkevich, M., Greenwald, A., Littman, M.L.: Cyclic equilibria in Markov games. In: Advances in Neural Information Processing Systems, pp. 1641â€“1648 (2006)
[124] Bowling, M., Veloso, M.: Multiagent learning using a variable learning rate. Artificial Intelligence 136(2), 215â€“250 (2002)
[125] Bowling, M.: Convergence and no-regret in multiagent learning. In: Advances in Neural
Information Processing Systems, pp. 209â€“216 (2005)
[126] Blum, A., Mansour, Y.: Learning, regret minimization, and equilibria. Algorithmic Game
Theory pp. 79â€“102 (2007)
[127] Hart, S., Mas-Colell, A.: A reinforcement procedure leading to correlated equilibrium. In:
Economics Essays, pp. 181â€“200. Springer (2001)
[128] Kasai, T., Tenmoto, H., Kamiya, A.: Learning of communication codes in multi-agent reinforcement learning problem. In: IEEE Conference on Soft Computing in Industrial Applications, pp. 1â€“6 (2008)
[129] Kim, D., Moon, S., Hostallero, D., Kang, W.J., Lee, T., Son, K., Yi, Y.: Learning to schedule
communication in multi-agent reinforcement learning. In: International Conference on
Learning Representations (2019)

56

[130] Chen, T., Zhang, K., Giannakis, G.B., BasÌ§ar, T.: Communication-efficient distributed reinforcement learning. arXiv preprint arXiv:1812.03239 (2018)
[131] Lin, Y., Zhang, K., Yang, Z., Wang, Z., BasÌ§ar, T., Sandhu, R., Liu, J.: A communicationefficient multi-agent actor-critic algorithm for distributed reinforcement learning. In: IEEE
Conference on Decision and Control (2019)
[132] Ren, J., Haupt, J.: A communication efficient hierarchical distributed optimization algorithm for multi-agent reinforcement learning. In: Real-world Sequential Decision Making
Workshop at International Conference on Machine Learning (2019)
[133] Kim, W., Cho, M., Sung, Y.: Message-dropout: An efficient training method for multi-agent
deep reinforcement learning. In: AAAI Conference on Artificial Intelligence (2019)
[134] He, H., Boyd-Graber, J., Kwok, K., DaumeÌ III, H.: Opponent modeling in deep reinforcement learning. In: International Conference on Machine Learning, pp. 1804â€“1813 (2016)
[135] Grover, A., Al-Shedivat, M., Gupta, J., Burda, Y., Edwards, H.: Learning policy representations in multiagent systems. In: International Conference on Machine Learning, pp. 1802â€“
1811 (2018)
[136] Gao, C., Mueller, M., Hayward, R.: Adversarial policy gradient for alternating Markov
games. In: Workshop at International Conference on Learning Representations (2018)
[137] Li, S., Wu, Y., Cui, X., Dong, H., Fang, F., Russell, S.: Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In: AAAI Conference on Artificial
Intelligence (2019)
[138] Zhang, X., Zhang, K., Miehling, E., Basar, T.: Non-cooperative inverse reinforcement learning. In: Advances in Neural Information Processing Systems, pp. 9482â€“9493 (2019)
[139] Tan, M.: Multi-agent reinforcement learning: Independent vs. cooperative agents. In: International Conference on Machine Learning, pp. 330â€“337 (1993)
[140] Matignon, L., Laurent, G.J., Le Fort-Piat, N.: Independent reinforcement learners in cooperative Markov games: A survey regarding coordination problems. The Knowledge Engineering Review 27(1), 1â€“31 (2012)
[141] Foerster, J., Nardelli, N., Farquhar, G., Torr, P., Kohli, P., Whiteson, S., et al.: Stabilising experience replay for deep multi-agent reinforcement learning. In: International Conference
of Machine Learning, pp. 1146â€“1155 (2017)
[142] Tuyls, K., Weiss, G.: Multiagent learning: Basics, challenges, and prospects. AI Magazine
33(3), 41â€“41 (2012)
[143] Guestrin, C., Lagoudakis, M., Parr, R.: Coordinated reinforcement learning. In: International Conference on Machine Learning, pp. 227â€“234 (2002)
[144] Guestrin, C., Koller, D., Parr, R.: Multiagent planning with factored MDPs. In: Advances in
Neural Information Processing Systems, pp. 1523â€“1530 (2002)

57

[145] Kok, J.R., Vlassis, N.: Sparse cooperative Q-learning. In: International Conference on Machine learning, pp. 61â€“69 (2004)
[146] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.M., Zambaldi, V., Jaderberg, M., Lanctot,
M., Sonnerat, N., Leibo, J.Z., Tuyls, K., et al.: Value-decomposition networks for cooperative
multi-agent learning based on team reward. In: International Conference on Autonomous
Agents and Multi-Agent Systems, pp. 2085â€“2087 (2018)
[147] Rashid, T., Samvelyan, M., De Witt, C.S., Farquhar, G., Foerster, J., Whiteson, S.: QMIX:
Monotonic value function factorisation for deep multi-agent reinforcement learning. In:
International Conference on Machine learning, pp. 681â€“689 (2018)
[148] Qu, G., Li, N.: Exploiting fast decaying and locality in multi-agent MDP with tree dependence structure. In: IEEE Conference on Decision and Control (2019)
[149] Mahajan, A.: Optimal decentralized control of coupled subsystems with control sharing.
IEEE Transactions on Automatic Control 58(9), 2377â€“2382 (2013)
[150] Oliehoek, F.A., Amato, C.: Dec-POMDPs as non-observable MDPs. IAS Technical Report
(IAS-UVA-14-01) (2014)
[151] Foerster, J.N., Farquhar, G., Afouras, T., Nardelli, N., Whiteson, S.: Counterfactual multiagent policy gradients. In: AAAI Conference on Artificial Intelligence (2018)
[152] Dibangoye, J., Buffet, O.: Learning to act in decentralized partially observable MDPs. In:
International Conference on Machine Learning, pp. 1233â€“1242 (2018)
[153] Kraemer, L., Banerjee, B.: Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing 190, 82â€“94 (2016)
[154] Macua, S.V., Chen, J., Zazo, S., Sayed, A.H.: Distributed policy evaluation under multiple
behavior strategies. IEEE Transactions on Automatic Control 60(5), 1260â€“1274 (2015)
[155] Macua, S.V., Tukiainen, A., HernaÌndez, D.G.O., Baldazo, D., de Cote, E.M., Zazo, S.: Diffdac: Distributed actor-critic for average multitask deep reinforcement learning. arXiv
preprint arXiv:1710.10363 (2017)
[156] Lee, D., Yoon, H., Hovakimyan, N.: Primal-dual algorithm for distributed reinforcement
learning: Distributed GTD. In: IEEE Conference on Decision and Control, pp. 1967â€“1972
(2018)
[157] Doan, T.T., Maguluri, S.T., Romberg, J.: Finite-time performance of distributed temporal
difference learning with linear function approximation. arXiv preprint arXiv:1907.12530
(2019)
[158] Suttle, W., Yang, Z., Zhang, K., Wang, Z., BasÌ§ar, T., Liu, J.: A multi-agent off-policy actorcritic algorithm for distributed reinforcement learning. arXiv preprint arXiv:1903.06372
(2019)

58

[159] Littman, M.L.: Value-function reinforcement learning in Markov games. Cognitive Systems
Research 2(1), 55â€“66 (2001)
[160] Young, H.P.: The evolution of conventions. Econometrica: Journal of the Econometric Society pp. 57â€“84 (1993)
[161] Son, K., Kim, D., Kang, W.J., Hostallero, D.E., Yi, Y.: QTRAN: Learning to factorize with
transformation for cooperative multi-agent reinforcement learning. In: International Conference on Machine Learning, pp. 5887â€“5896 (2019)
[162] Perolat, J., Piot, B., Pietquin, O.: Actor-critic fictitious play in simultaneous move multistage
games. In: International Conference on Artificial Intelligence and Statistics (2018)
[163] Monderer, D., Shapley, L.S.: Potential games. Games and Economic Behavior 14(1), 124â€“143
(1996)
[164] BasÌ§ar, T., Zaccour, G.: Handbook of Dynamic Game Theory. Springer (2018)
[165] Huang, M., Caines, P.E., MalhameÌ, R.P.: Individual and mass behaviour in large population
stochastic wireless power control problems: Centralized and Nash equilibrium solutions.
In: IEEE Conference on Decision and Control, pp. 98â€“103 (2003)
[166] Huang, M., MalhameÌ, R.P., Caines, P.E., et al.: Large population stochastic dynamic games:
Closed-loop Mckean-Vlasov systems and the Nash certainty equivalence principle. Communications in Information & Systems 6(3), 221â€“252 (2006)
[167] Lasry, J.M., Lions, P.L.: Mean field games. Japanese Journal of Mathematics 2(1), 229â€“260
(2007)
[168] Bensoussan, A., Frehse, J., Yam, P., et al.: Mean Field Games and Mean Field Type Control
Theory, vol. 101. Springer (2013)
[169] Tembine, H., Zhu, Q., BasÌ§ar, T.: Risk-sensitive mean-field games. IEEE Transactions on
Automatic Control 59(4), 835â€“850 (2013)
[170] Arabneydi, J., Mahajan, A.: Team optimal control of coupled subsystems with mean-field
sharing. In: IEEE Conference on Decision and Control, pp. 1669â€“1674 (2014)
[171] Arabneydi, J.: New concepts in team theory: Mean field teams and reinforcement learning.
Ph.D. thesis, McGill University (2017)
[172] Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., Wang, J.: Mean field multi-agent reinforcement learning. In: International Conference on Machine Learning, pp. 5571â€“5580 (2018)
[173] Witsenhausen, H.S.: Separation of estimation and control for discrete time systems. Proceedings of the IEEE 59(11), 1557â€“1566 (1971)
[174] YuÌˆksel, S., BasÌ§ar, T.: Stochastic Networked Control Systems: Stabilization and Optimization
Under Information Constraints. Springer Science & Business Media (2013)

59

[175] Subramanian, J., Seraj, R., Mahajan, A.: Reinforcement learning for mean-field teams. In:
Workshop on Adaptive and Learning Agents at International Conference on Autonomous
Agents and Multi-Agent Systems (2018)
[176] Arabneydi, J., Mahajan, A.: Linear quadratic mean field teams: Optimal and approximately
optimal decentralized solutions. arXiv preprint arXiv:1609.00056 (2016)
[177] Carmona, R., LaurieÌ€re, M., Tan, Z.: Linear-quadratic mean-field reinforcement learning:
Convergence of policy gradient methods. arXiv preprint arXiv:1910.04295 (2019)
[178] Carmona, R., LaurieÌ€re, M., Tan, Z.: Model-free mean-field reinforcement learning: Meanfield MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802 (2019)
[179] Rabbat, M., Nowak, R.: Distributed optimization in sensor networks. In: International
Symposium on Information Processing in Sensor Networks, pp. 20â€“27 (2004)
[180] Dallâ€™Anese, E., Zhu, H., Giannakis, G.B.: Distributed optimal power flow for smart microgrids. IEEE Transactions on Smart Grid 4(3), 1464â€“1475 (2013)
[181] Zhang, K., Shi, W., Zhu, H., Dallâ€™Anese, E., BasÌ§ar, T.: Dynamic power distribution system
management with a locally connected communication network. IEEE Journal of Selected
Topics in Signal Processing 12(4), 673â€“687 (2018)
[182] Zhang, K., Lu, L., Lei, C., Zhu, H., Ouyang, Y.: Dynamic operations and pricing of electric
unmanned aerial vehicle systems and power networks. Transportation Research Part C:
Emerging Technologies 92, 472â€“485 (2018)
[183] Corke, P., Peterson, R., Rus, D.: Networked robots: Flying robot navigation using a sensor
net. Robotics Research pp. 234â€“243 (2005)
[184] Zhang, K., Liu, Y., Liu, J., Liu, M., BasÌ§ar, T.: Distributed learning of average belief over
networks using sequential observations. Automatica (2019)
[185] Nedic, A., Ozdaglar, A.: Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control 54(1), 48â€“61 (2009)
[186] Agarwal, A., Duchi, J.C.: Distributed delayed stochastic optimization. In: Advances in
Neural Information Processing Systems, pp. 873â€“881 (2011)
[187] Jakovetic, D., Xavier, J., Moura, J.M.: Cooperative convex optimization in networked systems: Augmented lagrangian algorithms with directed gossip communication. IEEE Transactions on Signal Processing 59(8), 3889â€“3902 (2011)
[188] Tu, S.Y., Sayed, A.H.: Diffusion strategies outperform consensus strategies for distributed
estimation over adaptive networks. IEEE Transactions on Signal Processing 60(12), 6217â€“
6234 (2012)
[189] Varshavskaya, P., Kaelbling, L.P., Rus, D.: Efficient distributed reinforcement learning
through agreement. In: Distributed Autonomous Robotic Systems, pp. 367â€“378 (2009)

60

[190] Ciosek, K., Whiteson, S.: Expected policy gradients for reinforcement learning. arXiv
preprint arXiv:1801.03326 (2018)
[191] Sutton, R.S., Mahmood, A.R., White, M.: An emphatic approach to the problem of offpolicy temporal-difference learning. Journal of Machine Learning Research 17(1), 2603â€“
2631 (2016)
[192] Yu, H.: On convergence of emphatic temporal-difference learning. In: Conference on Learning Theory, pp. 1724â€“1751 (2015)
[193] Zhang, Y., Zavlanos, M.M.: Distributed off-policy actor-critic reinforcement learning with
policy consensus. arXiv preprint arXiv:1903.09255 (2019)
[194] Pennesi, P., Paschalidis, I.C.: A distributed actor-critic algorithm and applications to mobile sensor network coordination problems. IEEE Transactions on Automatic Control 55(2),
492â€“497 (2010)
[195] Lange, S., Gabel, T., Riedmiller, M.: Batch reinforcement learning. In: Reinforcement Learning, pp. 45â€“73. Springer (2012)
[196] Riedmiller, M.: Neural fitted Q iterationâ€“first experiences with a data efficient neural reinforcement learning method. In: European Conference on Machine Learning, pp. 317â€“328
(2005)
[197] Antos, A., SzepesvaÌri, C., Munos, R.: Fitted Q-iteration in continuous action-space MDPs.
In: Advances in Neural Information Processing Systems, pp. 9â€“16 (2008)
[198] Hong, M., Chang, T.H.: Stochastic proximal gradient consensus over random networks.
IEEE Transactions on Signal Processing 65(11), 2933â€“2948 (2017)
[199] Nedic, A., Olshevsky, A., Shi, W.: Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization 27(4), 2597â€“2633 (2017)
[200] Munos, R.: Performance bounds in `p -norm for approximate value iteration. SIAM Journal
on Control and Optimization 46(2), 541â€“561 (2007)
[201] Munos, R., SzepesvaÌri, C.: Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research 9(May), 815â€“857 (2008)
[202] Antos, A., SzepesvaÌri, C., Munos, R.: Learning near-optimal policies with Bellman-residual
minimization based fitted policy iteration and a single sample path. Machine Learning
71(1), 89â€“129 (2008)
[203] Farahmand, A.m., SzepesvaÌri, C., Munos, R.: Error propagation for approximate policy and
value iteration. In: Advances in Neural Information Processing Systems, pp. 568â€“576 (2010)
[204] Cassano, L., Yuan, K., Sayed, A.H.: Multi-agent fully decentralized off-policy learning with
linear convergence rates. arXiv preprint arXiv:1810.07792 (2018)

61

[205] Qu, G., Li, N.: Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems 5(3), 1245â€“1260 (2017)
[206] Schmidt, M., Le Roux, N., Bach, F.: Minimizing finite sums with the stochastic average
gradient. Mathematical Programming 162(1-2), 83â€“112 (2017)
[207] Ying, B., Yuan, K., Sayed, A.H.: Convergence of variance-reduced learning under random
reshuffling. In: IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 2286â€“2290 (2018)
[208] Singh, S.P., Sutton, R.S.: Reinforcement learning with replacing eligibility traces. Machine
Learning 22(1-3), 123â€“158 (1996)
[209] Bhandari, J., Russo, D., Singal, R.: A finite time analysis of temporal difference learning
with linear function approximation. In: Conference On Learning Theory, pp. 1691â€“1692
(2018)
[210] Srikant, R., Ying, L.: Finite-time error bounds for linear stochastic approximation and TD
learning. In: Conference on Learning Theory, pp. 2803â€“2830 (2019)
[211] StankovicÌ, M.S., StankovicÌ, S.S.: Multi-agent temporal-difference learning with linear function approximation: Weak convergence under time-varying network topologies. In: IEEE
American Control Conference, pp. 167â€“172 (2016)
[212] StankovicÌ, M.S., IlicÌ, N., StankovicÌ, S.S.: Distributed stochastic approximation: Weak convergence and network design. IEEE Transactions on Automatic Control 61(12), 4069â€“4074
(2016)
[213] Zhang, H., Jiang, H., Luo, Y., Xiao, G.: Data-driven optimal consensus control for discretetime multi-agent systems with unknown dynamics using reinforcement learning method.
IEEE Transactions on Industrial Electronics 64(5), 4091â€“4100 (2016)
[214] Zhang, Q., Zhao, D., Lewis, F.L.: Model-free reinforcement learning for fully cooperative
multi-agent graphical games. In: International Joint Conference on Neural Networks, pp.
1â€“6 (2018)
[215] Bernstein, D.S., Amato, C., Hansen, E.A., Zilberstein, S.: Policy iteration for decentralized
control of Markov decision processes. Journal of Artificial Intelligence Research 34, 89â€“132
(2009)
[216] Amato, C., Bernstein, D.S., Zilberstein, S.: Optimizing fixed-size stochastic controllers for
POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent Systems 21(3),
293â€“320 (2010)
[217] Liu, M., Amato, C., Liao, X., Carin, L., How, J.P.: Stick-breaking policy learning in DecPOMDPs. In: International Joint Conference on Artificial Intelligence (2015)
[218] Dibangoye, J.S., Amato, C., Buffet, O., Charpillet, F.: Optimally solving Dec-POMDPs as
continuous-state MDPs. Journal of Artificial Intelligence Research 55, 443â€“497 (2016)

62

[219] Wu, F., Zilberstein, S., Chen, X.: Rollout sampling policy iteration for decentralized
POMDPs. In: Conference on Uncertainty in Artificial Intelligence (2010)
[220] Wu, F., Zilberstein, S., Jennings, N.R.: Monte-Carlo expectation maximization for decentralized POMDPs. In: International Joint Conference on Artificial Intelligence (2013)
[221] Best, G., Cliff, O.M., Patten, T., Mettu, R.R., Fitch, R.: Dec-MCTS: Decentralized planning for multi-robot active perception. International Journal of Robotics Research pp. 1â€“22
(2018)
[222] Amato, C., Zilberstein, S.: Achieving goals in decentralized POMDPs. In: International
Conference on Autonomous Agents and Multi-Agent Systems, pp. 593â€“600 (2009)
[223] Banerjee, B., Lyle, J., Kraemer, L., Yellamraju, R.: Sample bounded distributed reinforcement learning for decentralized POMDPs. In: AAAI Conference on Artificial Intelligence
(2012)
[224] Nayyar, A., Mahajan, A., Teneketzis, D.: Decentralized stochastic control with partial history sharing: A common information approach. IEEE Transactions on Automatic Control
58(7), 1644â€“1658 (2013)
[225] Arabneydi, J., Mahajan, A.: Reinforcement learning in decentralized stochastic control systems with partial history sharing. In: IEEE American Control Conference, pp. 5449â€“5456
(2015)
[226] Papadimitriou, C.H.: On inefficient proofs of existence and complexity classes. In: Annals
of Discrete Mathematics, vol. 51, pp. 245â€“250. Elsevier (1992)
[227] Daskalakis, C., Goldberg, P.W., Papadimitriou, C.H.: The complexity of computing a Nash
equilibrium. SIAM Journal on Computing 39(1), 195â€“259 (2009)
[228] Von Neumann, J., Morgenstern, O., Kuhn, H.W.: Theory of Games and Economic Behavior
(commemorative edition). Princeton University Press (2007)
[229] Vanderbei, R.J., et al.: Linear Programming. Springer (2015)
[230] Hoffman, A.J., Karp, R.M.: On nonterminating stochastic games. Management Science
12(5), 359â€“370 (1966)
[231] Van Der Wal, J.: Discounted markov games: Generalized policy iteration method. Journal
of Optimization Theory and Applications 25(1), 125â€“138 (1978)
[232] Rao, S.S., Chandrasekaran, R., Nair, K.: Algorithms for discounted stochastic games. Journal
of Optimization Theory and Applications 11(6), 627â€“637 (1973)
[233] Patek, S.D.: Stochastic and shortest path games: Theory and algorithms. Ph.D. thesis, Massachusetts Institute of Technology (1997)
[234] Hansen, T.D., Miltersen, P.B., Zwick, U.: Strategy iteration is strongly polynomial for 2player turn-based stochastic games with a constant discount factor. Journal of the ACM
60(1), 1 (2013)

63

[235] Al-Tamimi, A., Abu-Khalaf, M., Lewis, F.L.: Adaptive critic designs for discrete-time zerosum games with application to Hâˆž control. IEEE Transactions on Systems, Man, and Cybernetics, Part B 37(1), 240â€“247 (2007)
[236] Al-Tamimi, A., Lewis, F.L., Abu-Khalaf, M.: Model-free Q-learning designs for linear
discrete-time zero-sum games with application to Hâˆž control. Automatica 43(3), 473â€“481
(2007)
[237] Lagoudakis, M.G., Parr, R.: Value function approximation in zero-sum Markov games. In:
Conference on Uncertainty in Artificial Intelligence, pp. 283â€“292 (2002)
[238] Farahmand, A.m., Ghavamzadeh, M., SzepesvaÌri, C., Mannor, S.: Regularized policy iteration with nonparametric function spaces. Journal of Machine Learning Research 17(1),
4809â€“4874 (2016)
[239] Yang, Z., Xie, Y., Wang, Z.: A theoretical analysis of deep Q-learning. arXiv preprint
arXiv:1901.00137 (2019)
[240] Jia, Z., Yang, L.F., Wang, M.: Feature-based Q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423 (2019)
[241] Sidford, A., Wang, M., Wu, X., Yang, L., Ye, Y.: Near-optimal time and sample complexities
for solving Markov decision processes with a generative model. In: Advances in Neural
Information Processing Systems, pp. 5186â€“5196 (2018)
[242] Wei, C.Y., Hong, Y.T., Lu, C.J.: Online reinforcement learning in stochastic games. In: Advances in Neural Information Processing Systems, pp. 4987â€“4997 (2017)
[243] Auer, P., Ortner, R.: Logarithmic online regret bounds for undiscounted reinforcement
learning. In: Advances in Neural Information Processing Systems, pp. 49â€“56 (2007)
[244] Jaksch, T., Ortner, R., Auer, P.: Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research 11(Apr), 1563â€“1600 (2010)
[245] Koller, D., Megiddo, N., von Stengel, B.: Fast algorithms for finding randomized strategies
in game trees. Computing 750, 759 (1994)
[246] Von Stengel, B.: Efficient computation of behavior strategies. Games and Economic Behavior
14(2), 220â€“246 (1996)
[247] Koller, D., Megiddo, N., Von Stengel, B.: Efficient computation of equilibria for extensive
two-person games. Games and economic behavior 14(2), 247â€“259 (1996)
[248] Von Stengel, B.: Computing equilibria for two-person games. Handbook of Game Theory
with Economic Applications 3, 1723â€“1759 (2002)
[249] Parr, R., Russell, S.: Approximating optimal policies for partially observable stochastic domains. In: International Joint Conference on Artificial Intelligence, pp. 1088â€“1094 (1995)

64

[250] Rodriguez, A.C., Parr, R., Koller, D.: Reinforcement learning using approximate belief
states. In: Advances in Neural Information Processing Systems, pp. 1036â€“1042 (2000)
[251] Hauskrecht, M.: Value-function approximations for partially observable Markov decision
processes. Journal of Artificial Intelligence Research 13, 33â€“94 (2000)
[252] Buter, B.J.: Dynamic programming for extensive form games with imperfect information.
Ph.D. thesis, Universiteit van Amsterdam (2012)
[253] Cowling, P.I., Powley, E.J., Whitehouse, D.: Information set Monte Carlo tree search. IEEE
Transactions on Computational Intelligence and AI in Games 4(2), 120â€“143 (2012)
[254] Teraoka, K., Hatano, K., Takimoto, E.: Efficient sampling method for Monte Carlo tree
search problem. IEICE Transactions on Information and Systems 97(3), 392â€“398 (2014)
[255] Whitehouse, D.: Monte Carlo tree search for games with hidden information and uncertainty. Ph.D. thesis, University of York (2014)
[256] Kaufmann, E., Koolen, W.M.: Monte-Carlo tree search by best arm identification. In: Advances in Neural Information Processing Systems, pp. 4897â€“4906 (2017)
[257] Hannan, J.: Approximation to Bayes risk in repeated play. Contributions to the Theory of
Games 3, 97â€“139 (1957)
[258] Brown, G.W.: Iterative solution of games by fictitious play. Activity Analysis of Production
and Allocation 13(1), 374â€“376 (1951)
[259] Robinson, J.: An iterative method of solving a game. Annals of Mathematics pp. 296â€“301
(1951)
[260] BenaÄ±Ìˆm, M., Hofbauer, J., Sorin, S.: Stochastic approximations and differential inclusions.
SIAM Journal on Control and Optimization 44(1), 328â€“348 (2005)
[261] Hart, S., Mas-Colell, A.: A general class of adaptive strategies. Journal of Economic Theory
98(1), 26â€“54 (2001)
[262] Monderer, D., Samet, D., Sela, A.: Belief affirming in learning processes. Journal of Economic Theory 73(2), 438â€“452 (1997)
[263] Viossat, Y., Zapechelnyuk, A.: No-regret dynamics and fictitious play. Journal of Economic
Theory 148(2), 825â€“842 (2013)
[264] Kushner, H.J., Yin, G.G.: Stochastic Approximation and Recursive Algorithms and Applications. Springer, New York, NY (2003)
[265] Fudenberg, D., Levine, D.K.: Consistency and cautious fictitious play. Journal of Economic
Dynamics and Control 19(5-7), 1065â€“1089 (1995)
[266] Hofbauer, J., Sandholm, W.H.: On the global convergence of stochastic fictitious play.
Econometrica 70(6), 2265â€“2294 (2002)

65

[267] Leslie, D.S., Collins, E.J.: Generalised weakened fictitious play. Games and Economic Behavior 56(2), 285â€“298 (2006)
[268] BenaÄ±Ìˆm, M., Faure, M.: Consistency of vanishingly smooth fictitious play. Mathematics of
Operations Research 38(3), 437â€“450 (2013)
[269] Li, Z., Tewari, A.: Sampled fictitious play is hannan consistent. Games and Economic Behavior 109, 401â€“412 (2018)
[270] Ernst, D., Geurts, P., Wehenkel, L.: Tree-based batch mode reinforcement learning. Journal
of Machine Learning Research 6(Apr), 503â€“556 (2005)
[271] Heinrich, J., Silver, D.: Self-play Monte-Carlo tree search in computer Poker. In: Workshops
at AAAI Conference on Artificial Intelligence (2014)
[272] Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P.,
Tavener, S., Perez, D., Samothrakis, S., Colton, S.: A survey of Monte Carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in games 4(1), 1â€“43
(2012)
[273] Sutton, R.S., Barto, A.G.: A temporal-difference model of classical conditioning. In: Proceedings of the Annual Conference of the Cognitive Science Society, pp. 355â€“378 (1987)
[274] Borkar, V.S.: Stochastic Approximation: A Dynamical Systems Viewpoint. Cambridge University Press (2008)
[275] Cesa-Bianchi, N., Lugosi, G.: Prediction, Learning, and Games. Cambridge University Press
(2006)
[276] Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.E.: The nonstochastic multiarmed bandit
problem. SIAM Journal on Computing 32(1), 48â€“77 (2002)
[277] Vovk, V.G.: Aggregating strategies. Proceedings of Computational Learning Theory (1990)
[278] Littlestone, N., Warmuth, M.K.: The weighted majority algorithm. Information and Computation 108(2), 212â€“261 (1994)
[279] Freund, Y., Schapire, R.E.: Adaptive game playing using multiplicative weights. Games and
Economic Behavior 29(1-2), 79â€“103 (1999)
[280] Hart, S., Mas-Colell, A.: A simple adaptive procedure leading to correlated equilibrium.
Econometrica 68(5), 1127â€“1150 (2000)
[281] Lanctot, M., Waugh, K., Zinkevich, M., Bowling, M.: Monte Carlo sampling for regret minimization in extensive games. In: Advances in Neural Information Processing Systems, pp.
1078â€“1086 (2009)
[282] Burch, N., Lanctot, M., Szafron, D., Gibson, R.G.: Efficient Monte Carlo counterfactual regret minimization in games with many player actions. In: Advances in Neural Information
Processing Systems, pp. 1880â€“1888 (2012)

66

[283] Gibson, R., Lanctot, M., Burch, N., Szafron, D., Bowling, M.: Generalized sampling and variance in counterfactual regret minimization. In: AAAI Conference on Artificial Intelligence
(2012)
[284] Johanson, M., Bard, N., Lanctot, M., Gibson, R., Bowling, M.: Efficient Nash equilibrium
approximation through Monte Carlo counterfactual regret minimization. In: International
Conference on Autonomous Agents and Multi-Agent Systems, pp. 837â€“846 (2012)
[285] LisyÌ€, V., Lanctot, M., Bowling, M.: Online Monte Carlo counterfactual regret minimization
for search in imperfect information games. In: International Conference on Autonomous
Agents and Multi-Agent Systems, pp. 27â€“36 (2015)
[286] Schmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec, R., Bowling, M.: Variance reduction in Monte Carlo counterfactual regret minimization (VR-MCCFR) for extensive form
games using baselines. In: AAAI Conference on Artificial Intelligence, vol. 33, pp. 2157â€“
2164 (2019)
[287] Waugh, K., Morrill, D., Bagnell, J.A., Bowling, M.: Solving games with functional regret
estimation. In: AAAI Conference on Artificial Intelligence (2015)
[288] Morrill, D.: Using regret estimation to solve games compactly. Ph.D. thesis, University of
Alberta (2016)
[289] Brown, N., Lerer, A., Gross, S., Sandholm, T.: Deep counterfactual regret minimization. In:
International Conference on Machine Learning, pp. 793â€“802 (2019)
[290] Brown, N., Sandholm, T.: Regret-based pruning in extensive-form games. In: Advances in
Neural Information Processing Systems, pp. 1972â€“1980 (2015)
[291] Brown, N., Kroer, C., Sandholm, T.: Dynamic thresholding and pruning for regret minimization. In: AAAI Conference on Artificial Intelligence (2017)
[292] Brown, N., Sandholm, T.: Reduced space and faster convergence in imperfect-information
games via pruning. In: International Conference on Machine Learning, pp. 596â€“604 (2017)
[293] Tammelin, O.: Solving large imperfect information games using CFR+. arXiv preprint
arXiv:1407.5042 (2014)
[294] Tammelin, O., Burch, N., Johanson, M., Bowling, M.: Solving heads-up limit Texas Holdâ€™em.
In: International Joint Conference on Artificial Intelligence (2015)
[295] Burch, N., Moravcik, M., Schmid, M.: Revisiting CFR+ and alternating updates. Journal of
Artificial Intelligence Research 64, 429â€“443 (2019)
[296] Zhou, Y., Ren, T., Li, J., Yan, D., Zhu, J.: Lazy-CFR: A fast regret minimization algorithm for
extensive games with imperfect information. arXiv preprint arXiv:1810.04433 (2018)
[297] Zinkevich, M.: Online convex programming and generalized infinitesimal gradient ascent.
In: International Conference on Machine Learning, pp. 928â€“936 (2003)

67

[298] Lockhart, E., Lanctot, M., PeÌrolat, J., Lespiau, J.B., Morrill, D., Timbers, F., Tuyls, K.: Computing approximate equilibria in sequential adversarial games by exploitability descent.
arXiv preprint arXiv:1903.05614 (2019)
[299] Johanson, M., Bard, N., Burch, N., Bowling, M.: Finding optimal abstract strategies in
extensive-form games. In: AAAI Conference on Artificial Intelligence, pp. 1371â€“1379
(2012)
[300] Schaeffer, M.S., Sturtevant, N., Schaeffer, J.: Comparing UCT versus CFR in simultaneous
games (2009)
[301] Lanctot, M., LisyÌ€, V., Winands, M.H.: Monte Carlo tree search in simultaneous move games
with applications to Goofspiel. In: Workshop on Computer Games, pp. 28â€“43 (2013)
[302] LisyÌ€, V., Kovarik, V., Lanctot, M., BosÌŒanskyÌ€, B.: Convergence of Monte Carlo tree search in
simultaneous move games. In: Advances in Neural Information Processing Systems, pp.
2112â€“2120 (2013)
[303] Tak, M.J., Lanctot, M., Winands, M.H.: Monte Carlo tree search variants for simultaneous move games. In: IEEE Conference on Computational Intelligence and Games, pp. 1â€“8
(2014)
[304] KovarÌŒÄ±Ìk, V., LisyÌ€, V.: Analysis of hannan consistent selection for Monte Carlo tree search in
simultaneous move games. arXiv preprint arXiv:1804.09045 (2018)
[305] Mazumdar, E.V., Jordan, M.I., Sastry, S.S.: On finding local Nash equilibria (and only local
Nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838 (2019)
[306] Bu, J., Ratliff, L.J., Mesbahi, M.: Global convergence of policy gradient for sequential zerosum linear quadratic dynamic games. arXiv preprint arXiv:1911.04672 (2019)
[307] Mescheder, L., Nowozin, S., Geiger, A.: The numerics of GANs. In: Advances in Neural
Information Processing Systems, pp. 1825â€“1835 (2017)
[308] Adolphs, L., Daneshmand, H., Lucchi, A., Hofmann, T.: Local saddle point optimization: A
curvature exploitation approach. arXiv preprint arXiv:1805.05751 (2018)
[309] Daskalakis, C., Panageas, I.: The limit points of (optimistic) gradient descent in min-max
optimization. In: Advances in Neural Information Processing Systems, pp. 9236â€“9246
(2018)
[310] Mertikopoulos, P., Zenati, H., Lecouat, B., Foo, C.S., Chandrasekhar, V., Piliouras, G.: Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In: International Conference on Learning Representations (2019)
[311] Fiez, T., Chasnov, B., Ratliff, L.J.: Convergence of learning dynamics in Stackelberg games.
arXiv preprint arXiv:1906.01217 (2019)

68

[312] Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., Graepel, T.: The mechanics
of n-player differentiable games. In: International Conference on Machine Learning, pp.
363â€“372 (2018)
[313] Sanjabi, M., Razaviyayn, M., Lee, J.D.: Solving non-convex non-concave min-max games
under Polyak-Åojasiewicz condition. arXiv preprint arXiv:1812.02878 (2018)
[314] Nouiehed, M., Sanjabi, M., Lee, J.D., Razaviyayn, M.: Solving a class of non-convex min-max
games using iterative first order methods. arXiv preprint arXiv:1902.08297 (2019)
[315] Mazumdar, E., Ratliff, L.J., Jordan, M.I., Sastry, S.S.: Policy-gradient algorithms have no
guarantees of convergence in continuous action and state multi-agent settings. arXiv
preprint arXiv:1907.03712 (2019)
[316] Chen, X., Deng, X., Teng, S.H.: Settling the complexity of computing two-player Nash equilibria. Journal of the ACM 56(3), 14 (2009)
[317] Greenwald, A., Hall, K., Serrano, R.: Correlated Q-learning. In: International Conference
on Machine Learning, pp. 242â€“249 (2003)
[318] Aumann, R.J.: Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics 1(1), 67â€“96 (1974)
[319] Perolat, J., Strub, F., Piot, B., Pietquin, O.: Learning Nash Equilibrium for General-Sum
Markov Games from Batch Data. In: International Conference on Artificial Intelligence and
Statistics, pp. 232â€“241 (2017)
[320] Maillard, O.A., Munos, R., Lazaric, A., Ghavamzadeh, M.: Finite-sample analysis of Bellman
residual minimization. In: Asian Conference on Machine Learning, pp. 299â€“314 (2010)
[321] Letcher, A., Balduzzi, D., RacanieÌ€re, S., Martens, J., Foerster, J.N., Tuyls, K., Graepel, T.:
Differentiable game mechanics. Journal of Machine Learning Research 20(84), 1â€“40 (2019)
[322] Chasnov, B., Ratliff, L.J., Mazumdar, E., Burden, S.A.: Convergence analysis of gradientbased learning with non-uniform learning rates in non-cooperative multi-agent settings.
arXiv preprint arXiv:1906.00731 (2019)
[323] Hart, S., Mas-Colell, A.: Uncoupled dynamics do not lead to Nash equilibrium. American
Economic Review 93(5), 1830â€“1836 (2003)
[324] Saldi, N., BasÌ§ar, T., Raginsky, M.: Markovâ€“Nash equilibria in mean-field games with discounted cost. SIAM Journal on Control and Optimization 56(6), 4256â€“4287 (2018)
[325] Saldi, N., BasÌ§ar, T., Raginsky, M.: Approximate Nash equilibria in partially observed
stochastic games with mean-field interactions. Mathematics of Operations Research (2019)
[326] Saldi, N.: Discrete-time average-cost mean-field games on Polish spaces. arXiv preprint
arXiv:1908.08793 (2019)

69

[327] Saldi, N., BasÌ§ar, T., Raginsky, M.: Discrete-time risk-sensitive mean-field games. arXiv
preprint arXiv:1808.03929 (2018)
[328] Guo, X., Hu, A., Xu, R., Zhang, J.:
arXiv:1901.09585 (2019)

Learning mean-field games.

arXiv preprint

[329] Fu, Z., Yang, Z., Chen, Y., Wang, Z.: Actor-critic provably finds Nash equilibria of linearquadratic mean-field games. arXiv preprint arXiv:1910.07498 (2019)
[330] Hadikhanloo, S., Silva, F.J.: Finite mean field games: Fictitious play and convergence to a
first order continuous mean field game. Journal de MatheÌmatiques Pures et AppliqueÌes
(2019)
[331] Elie, R., PeÌrolat, J., LaurieÌ€re, M., Geist, M., Pietquin, O.: Approximate fictitious play for
mean field games. arXiv preprint arXiv:1907.02633 (2019)
[332] Anahtarci, B., Kariksiz, C.D., Saldi, N.: Value iteration algorithm for mean-field games.
arXiv preprint arXiv:1909.01758 (2019)
[333] Zaman, M.A.u., Zhang, K., Miehling, E., BasÌ§ar, T.: Approximate equilibrium computation
for discrete-time linear-quadratic mean-field games. Submitted to IEEE American Control
Conference (2020)
[334] Yang, B., Liu, M.: Keeping in touch with collaborative UAVs: A deep reinforcement learning
approach. In: International Joint Conference on Artificial Intelligence, pp. 562â€“568 (2018)
[335] Pham, H.X., La, H.M., Feil-Seifer, D., Nefian, A.: Cooperative and distributed reinforcement
learning of drones for field coverage. arXiv preprint arXiv:1803.07250 (2018)
[336] TozÌŒicÌŒka, J., Szulyovszky, B., de Chambrier, G., Sarwal, V., Wani, U., Gribulis, M.: Application
of deep reinforcement learning to UAV fleet control. In: SAI Intelligent Systems Conference,
pp. 1169â€“1177 (2018)
[337] Shamsoshoara, A., Khaledi, M., Afghah, F., Razi, A., Ashdown, J.: Distributed cooperative
spectrum sharing in UAV networks using multi-agent reinforcement learning. In: IEEE
Annual Consumer Communications & Networking Conference, pp. 1â€“6 (2019)
[338] Cui, J., Liu, Y., Nallanathan, A.: The application of multi-agent reinforcement learning in
UAV networks. In: IEEE International Conference on Communications Workshops, pp. 1â€“6
(2019)
[339] Qie, H., Shi, D., Shen, T., Xu, X., Li, Y., Wang, L.: Joint optimization of multi-UAV target
assignment and path planning based on multi-agent reinforcement learning. IEEE Access
(2019)
[340] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8), 1735â€“
1780 (1997)

70

[341] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., Polosukhin, I.: Attention is all you need. In: Advances in neural information processing systems,
pp. 5998â€“6008 (2017)
[342] Hausknecht, M., Stone, P.: Deep recurrent q-learning for partially observable mdps. In:
2015 AAAI Fall Symposium Series (2015)
[343] Jorge, E., KaÌŠgebaÌˆck, M., Johansson, F.D., Gustavsson, E.: Learning to play guess who? and
inventing a grounded language as a consequence. arXiv preprint arXiv:1611.03218 (2016)
[344] Sukhbaatar, S., Fergus, R., et al.: Learning multiagent communication with backpropagation. In: Advances in Neural Information Processing Systems, pp. 2244â€“2252 (2016)
[345] Havrylov, S., Titov, I.: Emergence of language with multi-agent games: Learning to communicate with sequences of symbols. In: Advances in neural information processing systems,
pp. 2149â€“2159 (2017)
[346] Das, A., Kottur, S., Moura, J.M., Lee, S., Batra, D.: Learning cooperative visual dialog agents
with deep reinforcement learning. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 2951â€“2960 (2017)
[347] Peng, P., Wen, Y., Yang, Y., Yuan, Q., Tang, Z., Long, H., Wang, J.: Multiagent bidirectionallycoordinated nets: Emergence of human-level coordination in learning to play starcraft combat games. arXiv preprint arXiv:1703.10069 (2017)
[348] Mordatch, I., Abbeel, P.: Emergence of grounded compositional language in multi-agent
populations. In: AAAI Conference on Artificial Intelligence (2018)
[349] Jiang, J., Lu, Z.: Learning attentional communication for multi-agent cooperation. In: Advances in Neural Information Processing Systems, pp. 7254â€“7264 (2018)
[350] Jiang, J., Dun, C., Lu, Z.: Graph convolutional reinforcement learning for multi-agent cooperation. arXiv preprint arXiv:1810.09202 2(3) (2018)
[351] Celikyilmaz, A., Bosselut, A., He, X., Choi, Y.: Deep communicating agents for abstractive
summarization. arXiv preprint arXiv:1803.10357 (2018)
[352] Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., Pineau, J.: TarMAC: Targeted
multi-agent communication. arXiv preprint arXiv:1810.11187 (2018)
[353] Lazaridou, A., Hermann, K.M., Tuyls, K., Clark, S.: Emergence of linguistic communication
from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984
(2018)
[354] Cogswell, M., Lu, J., Lee, S., Parikh, D., Batra, D.: Emergence of compositional language
with deep generational transmission. arXiv preprint arXiv:1904.09067 (2019)
[355] Allis, L.: Searching for solutions in games and artificial intelligence. Ph.D. thesis, Maastricht
University (1994)

71

[356] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional
neural networks. In: Advances in neural information processing systems, pp. 1097â€“1105
(2012)
[357] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre,
L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., Hassabis, D.: A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science
362(6419), 1140â€“1144 (2018)
[358] Billings, D., Davidson, A., Schaeffer, J., Szafron, D.: The challenge of Poker. Artificial Intelligence 134(1-2), 201â€“240 (2002)
[359] Kuhn, H.W.: A simplified two-person Poker. Contributions to the Theory of Games 1, 97â€“
103 (1950)
[360] Southey, F., Bowling, M., Larson, B., Piccione, C., Burch, N., Billings, D., Rayner, C.: Bayesâ€™
bluff: Opponent modelling in Poker. In: Proceedings of the Twenty-First Conference on
Uncertainty in Artificial Intelligence, pp. 550â€“558. AUAI Press (2005)
[361] Bowling, M., Burch, N., Johanson, M., Tammelin, O.: Heads-up limit holdâ€™em Poker is
solved. Science 347(6218), 145â€“149 (2015)
[362] Heinrich, J., Silver, D.: Smooth UCT search in computer Poker. In: Twenty-Fourth International Joint Conference on Artificial Intelligence (2015)
[363] MoravcÌŒÄ±Ìk, M., Schmid, M., Burch, N., LisyÌ€, V., Morrill, D., Bard, N., Davis, T., Waugh, K.,
Johanson, M., Bowling, M.: Deepstack: Expert-level artificial intelligence in heads-up nolimit Poker. Science 356(6337), 508â€“513 (2017)
[364] Brown, N., Sandholm, T.: Superhuman ai for heads-up no-limit Poker: Libratus beats top
professionals. Science 359(6374), 418â€“424 (2018)
[365] Burch, N., Johanson, M., Bowling, M.: Solving imperfect information games using decomposition. In: Twenty-Eighth AAAI Conference on Artificial Intelligence (2014)
[366] Moravcik, M., Schmid, M., Ha, K., Hladik, M., Gaukrodger, S.J.: Refining subgames in
large imperfect information games. In: Thirtieth AAAI Conference on Artificial Intelligence
(2016)
[367] Brown, N., Sandholm, T.: Safe and nested subgame solving for imperfect-information
games. In: Advances in neural information processing systems, pp. 689â€“699 (2017)
[368] Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A.S., Yeo, M., Makhzani, A.,
KuÌˆttler, H., Agapiou, J., Schrittwieser, J., et al.: Starcraft II: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 (2017)
[369] Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik, A., Chung, J., Choi,
D.H., Powell, R., Ewalds, T., Georgiev, P., et al.: Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature pp. 1â€“5 (2019)

72

[370] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., Kavukcuoglu,
K.: Asynchronous methods for deep reinforcement learning. In: International conference
on machine learning, pp. 1928â€“1937 (2016)
[371] Lerer, A., Peysakhovich, A.: Maintaining cooperation in complex social dilemmas using
deep reinforcement learning. arXiv preprint arXiv:1707.01068 (2017)
[372] Hughes, E., Leibo, J.Z., Phillips, M., Tuyls, K., DuenÌƒez-Guzman, E., CastanÌƒeda, A.G., Dunning, I., Zhu, T., McKee, K., Koster, R., et al.: Inequity aversion improves cooperation in
intertemporal social dilemmas. In: Advances in neural information processing systems, pp.
3326â€“3336 (2018)
[373] Cai, Q., Yang, Z., Lee, J.D., Wang, Z.: Neural temporal-difference learning converges to
global optima. arXiv preprint arXiv:1905.10027 (2019)
[374] Arora, S., Cohen, N., Hazan, E.: On the optimization of deep networks: Implicit acceleration
by overparameterization. arXiv preprint arXiv:1802.06509 (2018)
[375] Li, Y., Liang, Y.: Learning overparameterized neural networks via stochastic gradient descent on structured data. In: Advances in Neural Information Processing Systems, pp.
8157â€“8166 (2018)
[376] Brafman, R.I., Tennenholtz, M.: A near-optimal polynomial time algorithm for learning in
certain classes of stochastic games. Artificial Intelligence 121(1-2), 31â€“47 (2000)
[377] Brafman, R.I., Tennenholtz, M.: R-max-A general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research 3(Oct), 213â€“231
(2002)
[378] Tu, S., Recht, B.: The gap between model-based and model-free methods on the linear
quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565 (2018)
[379] Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J.: Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free
approaches. In: Conference on Learning Theory, pp. 2898â€“2933 (2019)
[380] Lin, Q., Liu, M., Rafique, H., Yang, T.: Solving weakly-convex-weakly-concave saddle-point
problems as weakly-monotone variational inequality. arXiv preprint arXiv:1810.10207
(2018)
[381] GarcÄ±Ìa, J., FernaÌndez, F.: A comprehensive survey on safe reinforcement learning. Journal
of Machine Learning Research 16(1), 1437â€“1480 (2015)
[382] Chen, Y., Su, L., Xu, J.: Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems 1(2), 44 (2017)
[383] Yin, D., Chen, Y., Ramchandran, K., Bartlett, P.: Byzantine-robust distributed learning:
Towards optimal statistical rates. arXiv preprint arXiv:1803.01498 (2018)

73

