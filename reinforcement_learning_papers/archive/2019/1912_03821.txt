Zhang et al. / Front Inform Technol Electron Eng

in press

1

Frontiers of Information Technology & Electronic Engineering
www.jzus.zju.edu.cn; engineering.cae.cn; www.springerlink.com
ISSN 2095-9184 (print); ISSN 2095-9230 (online)
E-mail: jzus@zju.edu.cn

Review

Decentralized Multi-Agent Reinforcement Learning

arXiv:1912.03821v1 [cs.LG] 9 Dec 2019

with Networked Agents: Recent Advances∗
Kaiqing Zhang‡1 , Zhuoran Yang2 , Tamer Başar1
1Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, IL, USA
2Department of Operations Research and Financial Engineering, Princeton University, NJ, USA

E-mail: kzhang66@illinois.edu; zy6@princeton.edu; basar1@illinois.edu
Received Nov. 30, 2019

Abstract: Multi-agent reinforcement learning (MARL) has long been a signiﬁcant and everlasting research topic in
both machine learning and control. With the recent development of (single-agent) deep RL, there is a resurgence
of interests in developing new MARL algorithms, especially those that are backed by theoretical analysis. In
this paper, we review some recent advances a sub-area of this topic: decentralized MARL with networked agents.
Speciﬁcally, multiple agents perform sequential decision-making in a common environment, without the coordination
of any central controller. Instead, the agents are allowed to exchange information with their neighbors over a
communication network. Such a setting ﬁnds broad applications in the control and operation of robots, unmanned
vehicles, mobile sensor networks, and smart grid. This review is built upon several our research endeavors in this
direction, together with some progresses made by other researchers along the line. We hope this review to inspire
the devotion of more research eﬀorts to this exciting yet challenging area.
Key words: Reinforcement Learning; Multi-Agent/Networked Systems; Consensus/Distributed Optimization;
Game Theory
https://doi.org/
CLC number: TP391.4

1 Introduction
Reinforcement learning (RL) has achieved
tremendous successes recently in many sequential
decision-making problems, especially associated with
the development of deep neural networks for function approximation (Mnih et al., 2015). Preeminent examples include playing the game of Go (Silver
et al., 2016, 2017), robotics (Kober et al., 2013; Lillicrap et al., 2016), and autonomous driving (ShalevShwartz et al., 2016), etc. Most of the applications, interestingly, involve more than one single
‡ Corresponding author
*

Project supported in part by the US Army Research Laboratory
(ARL) Cooperative Agreement W911NF-17-2-0196, and in part
by the Air Force Office of Scientific Research (AFOSR) Grant
FA9550-19-1-0353.
ORCID: Kaiqing Zhang, http://orcid.org/0000-0002-7446-7581
c Zhejiang University and Springer-Verlag GmbH Germany, part
of Springer Nature 2019

agent/player1, which naturally fall into the realm
of multi-agent RL (MARL). In particular, MARL
models the sequential decision-making of multiple
autonomous agents in a common environment, while
each agent’s objective and the system evolution are
both aﬀected by the joint decision made by all agents.
MARL algorithms can be generally categorized into
three groups, according to the settings they address:
fully cooperative, fully competitive, and a mix of the
two (Busoniu et al., 2008; Zhang et al., 2019). Specifically, fully cooperative MARL agents aim to optimize a long-term return that is common to all;
while fully competitive MARL agents usually have
completely misaligned returns that sum up to zero.
Agents in the mixed MARL setting, on the other
hand, can be both fully cooperative and competitive.
1 Hereafter, we will interchangeably use agent and player.

2

Zhang et al. / Front Inform Technol Electron Eng

In the present review, for simplicity, we refer to the
ﬁrst ones as cooperative MARL, and the second and
third ones as non-cooperative MARL, respectively.
There exist several long-standing challenges in
both cooperative and non-cooperative MARL, especially in the theoretical analysis for it. First, since
the agents’ objectives may be misaligned with each
other, the learning goals in MARL are not just singledimensional, introducing the challenge of handling
equilibrium points, and several performance criteria other than return-optimization, e.g., the communication/coordination eﬃciency, and the robustness against potential adversaries. Second, it is wellknown that the environment faced by each agent is
non-stationary in MARL, as it is aﬀected not only by
the underlying system evolution, but also by the decisions made by other agents, who are concurrently
improving their policies. This non-stationarity invalidates the framework of most theoretical analyses
in single-agent RL, which are stationary and Markovian. Third, since the joint action space increases
exponentially with the number of agents, MARL algorithms may suﬀer from the scalability issues by
nature. Fourth, the information structure, which
dictates the information availability to each agent,
becomes more complicated in multi-agent settings,
as some of the observations may not be sharable to
each other, and sometimes kept in a decentralized
fashion. Therefore, the theoretical analysis of MARL
algorithms is still relatively lacking in the literature.
Besides the earlier works on MARL as summarized in Busoniu et al. (2008), there has been a
resurgent interest in this area, especially with the
advances of single-agent RL recently (Foerster et al.,
2016; Zazo et al., 2016; Gupta et al., 2017; Lowe
et al., 2017; Omidshaﬁei et al., 2017; Zhang et al.,
2018d). Most of these works, with deep neural networks for function approximation, are not placed under rigorous theoretical footings, due to the limited
understanding of even single-agent deep RL theories.
On the other hand, a relatively new paradigm for
MARL, decentralized MARL with networked agents,
has gained increasing research attention (Kar et al.,
2013; Zhang et al., 2018d; Wai et al., 2018; Doan
et al., 2019a). This is partly due to the fact that
the algorithms under this paradigm require no existence of any central controller, i.e., can be implemented in a decentralized fashion. This can partially
address the scalability issues, one of the aforemen-

in press

tioned challenges, and more amenable to a decentralized information structure that is common in practical multi-agent systems (Rabbat and Nowak, 2004;
Corke et al., 2005; Dall’Anese et al., 2013). The second reason for its popularity is that most algorithms
under this paradigm are accompanied with theoretical analysis for convergence/sample complexity, as
they are closely related to, and inspired by the recent development of distributed/consensus optimization with networked agents, across the areas of control (Nedic and Ozdaglar, 2009), operations research
(Nedic et al., 2017), signal processing (Sayed et al.,
2014; Shi et al., 2015), and statistical learning (Boyd
et al., 2011; Fan et al., 2015).
Speciﬁcally, we focus on the MARL setting
where the agents, mostly cooperative, are connected
by a communication network for the information exchange with each other. The setting is decentralized
in the sense that each agent makes their own decisions, based on only local observations and information transmitted from its neighbors, without the
coordination of any central controller. Such a setting
ﬁnds broad applications in practice, such as robotics
(Corke et al., 2005), unmanned vehicles (Qie et al.,
2019), mobile sensor networks (Rabbat and Nowak,
2004), intelligent transportation systems (Adler and
Blue, 2002; Zhang et al., 2018a), and smart grid
(Dall’Anese et al., 2013; Zhang et al., 2018a), which
enjoys several advantages over a centralized setting,
in terms of either cost, scalability, or robustness. For
example, it might be costly to even establish a central
controller for coordination for some systems (Adler
and Blue, 2002; Dall’Anese et al., 2013), which also
easily suﬀers from malicious attacks and high communication traﬃc, as the malfunctioning of the central controller will take down the overall system as
a whole, and the communication is concentrated at
one place, between the controller and the agents. As
a result, it is imperative to summarize the theories
and algorithms on this topic, for the purpose of both
highlighting the boundary of existing research endeavors, and stimulating future research directions.
In this paper, we provide such a review of recent advances on decentralized MARL with networked agents, based on our recent review Zhang
et al. (2019) on general MARL algorithms . Indeed,
Zhang et al. (2019) has provided a comparatively
complete overview of general MARL algorithms that
are backed by theoretical analysis, serving as the big

Zhang et al. / Front Inform Technol Electron Eng

picture and basis of the present review. Interested
readers are referred to Zhang et al. (2019) for a more
detailed review. The present review summarizes several our earlier works on this decentralized MARL
setting (Zhang et al., 2018d,b,c), together with some
recent progresses by other researchers along the line.
We expect our review to provide continuing stimulus
for researchers with similar interests in working on
this exciting yet challenging area.

2 Background
In this section, we provide the necessary background on MARL, especially the decentralized setting with networked agents.
2.1 Single-Agent RL
A general RL agent is modeled to perform sequential decision-making in a Markov decision process (MDP), as formally deﬁned below.
Definition 1 A Markov decision process is deﬁned by a tuple (S, A, P, R, γ), where S and A
denote the state and action spaces, respectively;
P : S × A → ∆(S) denotes the transition probability from any state s ∈ S to any state s′ ∈ S for
any given action a ∈ A; R : S × A × S → R is the reward function that determines the immediate reward
received by the agent for a transition from (s, a) to
s′ ; γ ∈ [0, 1] is the discount factor that trades oﬀ the
instantaneous and future rewards.
At each time t, the agent chooses to execute
an action at in face of the system state st , which
causes the system to transition to st+1 ∼ P(· | st , at ).
Moreover, the agent receives an instantaneous reward R(st , at , st+1 ). The goal of the agent is to ﬁnd
a policy π : S → ∆(A) so that at ∼ π(· | st ) maximizes the discounted accumulated reward
X

t
E
γ R(st , at , st+1 ) at ∼ π(· | st ), s0 .
t≥0

Due to the Markovian property, the optimal policy can be calculated by dynamicprogramming/backward induction, such as value iteration and policy iteration (Bertsekas, 2005), which
require the full knowledge of the model. Reinforcement learning, on the other hand, is devised to ﬁnd
the optimal policy without knowing the model, but
by learning from experiences collected by interacting with either the environment or the simulator. In

in press

3

general, RL algorithms can be categorized into two
types, value-based and policy-based methods.
Value-Based Methods: Value-based methods aim
to ﬁnd an estimate of the state-action value/Q- function, which leads to the optimal policy by taking the
greedy action with respect to the estimate. Classical value-based RL algorithms include Q-learning
(Watkins and Dayan, 1992) and SARSA (Singh
et al., 2000). Another important task in RL that
is related to value functions is to estimate the value
function of a fixed policy (not necessarily the optimal
one). This task is referred to as policy evaluation,
and can be addressed by standard algorithms such
as temporal difference (TD) learning (Tesauro, 1995;
Tsitsiklis and Van Roy, 1997) and gradient TD methods (Sutton et al., 2008; Bhatnagar et al., 2009a;
Sutton et al., 2009; Liu et al., 2015).
Policy-Based Methods: Policy-based methods
propose to directly searches for the optimal one over
the policy space, while the space is generally parameterized by function approximators like neural networks, i.e., parameterizing π(· | s) ≈ πθ (· | s). Hence,
it is straightforward to improve the policy following
the gradient direction of the long-term return, known
as the policy gradient (PG) method. (Sutton et al.,
2000) has derived the closed-form of PG as


∇J(θ) = Ea∼πθ (· | s),s∼ηπθ (·) Qπθ (s, a)∇ log πθ (a | s) ,

where J(θ) and Qπθ are the return and Q-function
under policy πθ , respectively, ∇ log πθ (a | s) is the
score function of the policy, and ηπθ is the state occupancy measure, either discounted or ergodic, under
policy πθ . Other standard PG methods include REINFORCE (Williams, 1992), G(PO)MDP (Baxter
and Bartlett, 2001), actor-critic (Konda and Tsitsiklis, 2000), and deterministic PGs (Silver et al., 2014).
2.2 Multi-Agent RL Framework
Multi-agent RL also addresses the sequential
decision-making problems, but with more than one
agent involved. Speciﬁcally, both the system state
evolution and the reward received by each agent are
inﬂuenced by the joint actions of all agents. Moreover, each agent has its own long-term reward to optimize, which now becomes a function of the policies
of all other agents. Though various MARL frameworks exist in the literature (Busoniu et al., 2008;
Zhang et al., 2019), we here focus on two examples

4

Zhang et al. / Front Inform Technol Electron Eng

in press

that are either representative or pertinent to our decentralized MARL setting.

to such an equilibrium point, making MGs the most
standard framework in MARL.

Markov/Stochastic Games: As a direct generalization of MDPs to the multi-agent setting, Markov
games (MGs), also known as stochastic games (Shapley, 1953) has long been treated as a classical framework of MARL (Littman, 1994). A formal deﬁnition
of MGs is introduced as follows.

Indeed, this framework of MGs is general enough to cover both cooperative and noncooperative MARL settings. For the formal one,
all agents share a common reward function, i.e.,
R1 = R2 = · · · = RN = R. Such a model is also
known as multi-agent MDPs (MMDPs) (Boutilier,
1996; Lauer and Riedmiller, 2000) and Markov teams
(Wang and Sandholm, 2003; Mahajan, 2008). In this
setting, the value functions are identical to all agents,
enabling the use of single-agent RL algorithms, provided that all agents are coordinated as one decision maker. The latter setting with non-cooperative
agents correspond to the MGs with either zero-sum
or general-sum reward functions. Such misaligned
objectives of self-interested agents necessitate the use
of Nash equilibrium as the solution concept.

Definition 2 A Markov game is deﬁned by a
tuple (N , S, {Ai }i∈N , P, {Ri }i∈N , γ), where N =
{1, · · · , N } denotes the set of N > 1 agents, S
denotes the state space observed by all agents, Ai
denotes the action space of agent i. Let A :=
A1 × · · · × AN , then P : S × A → ∆(S) denotes
the transition probability from any state s ∈ S
to any state s′ ∈ S for any joint action a ∈ A;
Ri : S × A × S → R is the reward function that
determines the immediate reward received by agent
i for a transition from (s, a) to s′ ; γ ∈ [0, 1] is the
discount factor.
At time t, each agent i ∈ N chooses an action
ait , according to the system state st . The joint chosen action at = (a1t , · · · , aN
t ) then makes the system
transition to state st+1 , and assigns to each agent
i a reward rti = Ri (st , at , st+1 ). Agent i’s goal is
to ﬁnding the policy π i : S → ∆(Ai ) such that
its own long-term return is optimized. Accordingly,
the agent i’s value-function V i : S → R becomes
a function of the joint policy π : S → ∆(A) with
Q
π(a | s) := i∈N π i (ai | s), which is deﬁned as
Vπii ,π−i (s) := Eait ∼πi (· | st )

X

γ t rti

t≥0



s0 = s , (2.1)

where −i represents the indices of all agents in N except agent i. Owing to this coupling of polices, the
solution concept of MGs is not simply an optimum,
but an equilibrium among all agents. The most common one, named Nash equilibrium (NE) in MGs, is
deﬁned as below (Başar and Olsder, 1999).
Definition 3 A Nash equilibrium of the MG
(N , S, {Ai }i∈N , P, {Ri }i∈N , γ) is a joint policy π ∗ =
(π 1,∗ , · · · , π N,∗ ), such that for any s ∈ S and i ∈ N
Vπii,∗ ,π−i,∗ (s) ≥ Vπii ,π−i,∗ (s),

for any π i .

Nash equilibrium describes an point π ∗ , from
which no agent has any incentive to deviate. Most
of the MARL algorithms are contrived to converge

Networked MMDPs: As a generalization of the
above common-reward cooperative model, the following one of networked MMDPs plays an essential
role in decentralized MARL with networked agents.
Definition 4 A networked MMDP is deﬁned
by a tuple (N , S, {Ai }i∈N , P, {Ri }i∈N , γ, {Gt }t≥0 ),
where the ﬁrst six elements are identical to those in
Deﬁnition 2 for MGs, and Gt = (N , Et ) denotes the
time-varying communication network that connects
all agents, with Et being the set of communication
links at time t, i.e., an edge (i, j) for agents i, j ∈ N
belongs to Et if agent i and j can communicate with
each other at time t.
The system evolution of networked MMDPs
is identical to MGs, but with one diﬀerence in
terms of the objective: all agents aim to cooperatively optimize the long-term return corresponding to the team-average reward R̄(s, a, s′ ) := N −1 ·
P
i
′
′
i∈N R (s, a, s ) for any (s, a, s ) ∈ S × A × S.
Moreover, each agent makes decisions using only the
local information, including the information transmitted from its neighbors over the network. The
networked MMDP model allows agents to cooperate, but with diﬀerent reward functions/preferences.
This model is able to not only capture more heterogeneity and privacy among agents (compared to
conventional MMDPs), but also facilitate the development of decentralized MARL algorithms with only
neighbor-to-neighbor communications (Kar et al.,
2013; Zhang et al., 2018d; Wai et al., 2018). In ad-

Zhang et al. / Front Inform Technol Electron Eng

dition, such heterogeneity also necessitates the incorporation of more efficient communication protocols into MARL, an important while relatively open
problem in MARL that naturally arises in networked
MMDPs (Chen et al., 2018; Ren and Haupt, 2019;
Lin et al., 2019).

3 Algorithms
This section provides a review of MARL algorithms under the frameworks introduced in §2.2.
Speciﬁcally, we categorize the algorithms, which
are amenable to the decentralized setting with networked agents, according to the tasks they address,
such as learning the optimal/equilibrium policies,
and policy evaluation. Besides, we will also mention
several algorithms aiming to achieve other learning
goals in this setting.
3.1 Learning Policies
We ﬁrst review the algorithms for the task
of control in RL, namely, learning the optimal/equilibrium polices for the agents. Algorithms
for both cooperative and non-cooperative settings
exist in the literature.
Cooperative Setting:
Consider a team of agents cooperating under the framework of networked MMDPs introduced in Deﬁnition 4. Including the framework
of MMDPs/Markov teams as a special case, this
one generally requires more coordination, since the
global value function cannot be estimated locally
without knowing the other agents’ reward functions.
This challenge becomes more severe when no central
controller, but only neighbor-to-neighbor communication over a network, is available for coordination.
Such an information structure has appeared
frequently in the proliferate studies on decentralized/distributed 2 algorithms, such as average consensus (Xiao et al., 2007) and distributed/consensus
optimization (Nedic and Ozdaglar, 2009; Shi et al.,
2015). Nevertheless, relatively fewer eﬀorts have devoted to address this structure in MARL. In fact,
most existing results in distributed/consensus optimization can be viewed as solving static/one-stage
2 Note that hereafter we use decentralized and distributed

interchangeably to describe this structure, to respect some
conventions from distributed optimization literature.

in press

5

decision-making problems (Nedic and Ozdaglar,
2009; Agarwal and Duchi, 2011; Jakovetic et al.,
2011; Tu and Sayed, 2012), which is easier to analyze compared to RL, a sequential decision-making
setting where the decisions made at current time will
have a long-term eﬀect.
The idea of decentralized MARL over networked
agents dates back to Varshavskaya et al. (2009), for
the control of distributed robotic systems. The algorithm therein uses the idea of average consensus,
and is policy-based. However, no theoretical analysis is provided in the work. To the best of our
knowledge, under this setting, the ﬁrst MARL algorithm with provable convergence guarantees is Kar
et al. (2013), which combines the idea of consensus +
innovation (Kar and Moura, 2013) to the standard
Q-learning algorithm, leading to the QD-learning algorithm that is updated as follows:
h
Qit+1 (s, a) ← Qit (s, a) + αt,s,a Ri (s, a) + γ min
Qit (s′ , a′ )
a′ ∈A
i
X  i

Qt (s, a) − Qjt (s, a) ,
− Qit (s, a) − βt,s,a
j∈Nti

where αt,s,a , βt,s,a > 0 denote the stepsizes, Nti denotes agent i’s set of neighboring agents, at time
t. Compared to the Q-learning update (Watkins and
Dayan, 1992), QD-learning adds an innovation term,
which is the diﬀerence between the agent’s Q-value
estimate and its neighbors’. Under some standard
conditions on the stepsizes, QD-learning is proven
to converge to the optimum Q-function, for the tabular setting with ﬁnite state-action spaces.
As the joint action space increases exponentially with the number of agents, function approximation becomes especially pivotal to the scalability of MARL algorithms. To establish convergence
analysis in the function approximation regime, we
have resorted to policy-based algorithms, speciﬁcally, actor-critic algorithms, for this setting (Zhang
et al., 2018d). Speciﬁcally, each agent i’s policy is parameterized as πθi i : S → Ai by some
i
θi ∈ Rm , and the joint policy is thus deﬁned as
Q
πθ (a | s) := i∈N πθi i (ai | s). Let Qθ be the global
value function corresponding to the team-average reward R̄ under the joint policy πθ . Then, we ﬁrst establish the policy gradient of the return w.r.t. each
agent i’s parameter θi as


∇θi J(θ) = E ∇θi log πθi i (s, ai ) · Qθ (s, a) . (3.1)

Analogous to the single-agent PG given in §2.1, the
PG in (3.1) involves the expectation of the prod-

6

Zhang et al. / Front Inform Technol Electron Eng

uct between the global Q-function Qθ , and the local
score function ∇θi log πθi i (s, ai ). The former quantity, however, cannot be estimated locally at each
agent. Therefore, by parameterizing each local copy
of Qθ (·, ·) as Qθ (·, ·; ω i ), a consensus-based TD learning update is proposed for the critic step, i.e., for
estimating Qθ (·, ·) given πθ :
ω
eti = ωti + βω,t · δti · ∇ω Qt (ωti ),
X
i
ωt+1
=
ct (i, j) · ω
etj ,

(3.2)
(3.3)

j∈N

where βω,t > 0 denotes the stepsize, and δti is the local TD-error calculated using Qθ (·, ·; ω i ). (3.2) is the
standard TD learning update at agent i, while (3.3)
is a weighted combination step of the neighbors’ estimates ω
etj . The weights ct (i, j) are determined by
the topology of the communication network, namely,
it only has non-zero values if the two agents i and j
are connected at time t, i.e., (i, j) ∈ Et . The weights
also need to satisfy the doubly stochastic property in
expectation, so that ωti reaches a consensual value
for all i ∈ N as t → ∞. Then, in the actor step,
each agent i updates its policy following stochastic
policy gradient (3.1), using its own Q-function estimate Qθ (·, ·; ωti ). In addition, motivated by the fact
that the temporal diﬀerence can also be used in policy gradient to replace the Q-function (Bhatnagar
et al., 2009b), we also propose a variant algorithm
that relies on not the Q-function, but the state-value
function approximation (Zhang et al., 2018d), in order to reduce the variance in the PG update.
When linear functions are used for value function approximation, we can establish the almost sure
convergence of the decentralized actor-critic updates
(Zhang et al., 2018d). The proof techniques therein
are based on the two-timescale stochastic approximation approach in Borkar (2008). Later in Zhang
et al. (2018b), we extend the similar ideas to the setting speciﬁcally with continuous spaces, where deterministic policy gradient (DPG) method is usually
used. For DPG methods, oﬀ-policy exploration using
a stochastic behavior policy is required in general, as
the deterministic on-policy may not be explorative
enough. Nonetheless, as the policies of other agents
are unknown in the multi-agent setting, the standard oﬀ-policy approach (Silver et al., 2014, §4.2)
is not applicable. As a result, we develop an actorcritic algorithm (Zhang et al., 2018b), which is still
on-policy, using the recent development of the ex-

in press

pected policy gradient (EPG) method (Ciosek and
Whiteson, 2018). EPG uniﬁes stochastic PG (SPG)
and DPG, but reduces the variance of general SPGs.
Speciﬁcally, the critic step remains identical to (3.2)(3.3), while the actor step is replaced by the multiagent version of EPG we newly derived. When linear
function approximation is used, we can also establish
the almost sure convergence of the algorithm. In the
same vein, the extension of Zhang et al. (2018d) to
an oﬀ-policy setting has been investigated in Suttle et al. (2019), which is built upon the emphatic
temporal diﬀerences (ETD) method for the critic
(Sutton et al., 2016). Convergence can also be established using stochastic approximation approach,
by incorporating the analysis of ETD(λ) (Yu, 2015)
into Zhang et al. (2018d). In addition, another oﬀpolicy algorithm for the same setting is proposed in a
concurrent work in Zhang and Zavlanos (2019). Deviated from the line of works above, agents do not
share/exchange their estimates of value function. In
contrast, the agents’ goal is to reach consensus over
the global optimal policy estimation. This yields a
local critic and a consensus actor update, which also
enjoys provably asymptotic convergence.
We note that aforementioned convergence guarantees are asymptotic, namely, the algorithms are
guaranteed to converge only as the iteration numbers
go to inﬁnity. More importantly, these convergence
results are restricted to the case with linear function approximations. These two drawbacks make it
imperative, while challenging, to quantify the performance when ﬁnite iterations and/or samples are
used, and when nonlinear functions such as deep neural networks are used in practice. Serving as an initial step towards the finite-sample analyses in this
setting with more general function approximation,
we study in Zhang et al. (2018c) the batch RL algorithms (Lange et al., 2012) in the multi-agent setting.
In particular, we propose decentralized variants of
the ﬁtted-Q iteration (FQI) algorithm (Riedmiller,
2005; Antos et al., 2008a). We focus on FQI as it
motivates the celebrated deep Q-learning algorithm
(Mnih et al., 2015) that has achieved great empirical
success. All agents collaborate to update the global
Q-function estimate iteratively, by ﬁtting nonlinear
least squares with the target values as the responses.
Let F denote the function class for Q-function approximation, {(sj , {aij }i∈N , s′j )}j∈[n] be the batch
transitions dataset of size n available to all agents,

Zhang et al. / Front Inform Technol Electron Eng

and {rji }j∈[n] be the local reward samples private
to each agent. Then, the local target value at each
agent i is calculated as yji = rji +γ ·maxa∈A Qit (s′j , a),
where Qit is agent i’s Q-function estimate at iteration
t. As a consequence, all agents aim to collaboratively
ﬁnd a common Q-function estimate by solving
n
2
1 X 1 X i
yj − f (sj , a1j , · · · , aN
min
j ) . (3.4)
f ∈F N
2n j=1
i∈N

As rji , and thus yji , is only available to agent i,
the problem in (3.4) ﬁts in the standard formulation of distributed/consensus optimization (Nedic
and Ozdaglar, 2009; Agarwal and Duchi, 2011;
Jakovetic et al., 2011; Tu and Sayed, 2012; Hong
and Chang, 2017; Nedic et al., 2017). If F makes
Pn
i
1
N 2
j=1 [yj − f (sj , aj , · · · , aj )] convex for each i, then
the global optimum can be achieved by the algorithms in these references. For the special case when
F is a linear function class, this is indeed the case.
Unfortunately, with only a ﬁnite iteration of distributed optimization algorithms performed at each
agent, the agents may not reach exact consensus.
This results in an error in each agent’s Q-function estimate, compared with the actual optimum of (3.4).
When nonlinear function approximation is used, this
error is even more obvious, as the actual global optimum can hardly be obtained in general. By accounting for this error due to decentralized computation, we derive the error propagation results following those for the single-agent batch RL (Munos,
2007; Munos and Szepesvári, 2008; Antos et al.,
2008a,b; Farahmand et al., 2010), in order to establish the ﬁnite-sample performance of the proposed
algorithms. Speciﬁcally, we establish the dependence
of the accuracy of the algorithms output, on the function class F , the number of samples within each iteration n, and the number of iterations for t.
Non-Cooperative Setting:
The networked MMDP model can also be considered in a non-cooperative setting, which though
has not been extensively studied in the literature.
In Zhang et al. (2018c), we also consider one type
of non-cooperative setting, where two teams of networked agents, Teams 1 and 2, form a zero-sum
Markov game as introduced in Deﬁnition 2. Such a
setting can be viewed as a mixed one with both cooperative (within each team), and competitive (against
the opponent team) agents. We then establish ﬁnite-

in press

7

sample analysis for a decentralized variant of FQI for
this setting.
In particular, by instantiating the deﬁnition of
Nash equilibrium in a two-player zero-sum case, for
a given Q-value Q(s, ·, ·) : A × B → R, one can deﬁne
a Value operator at any state s ∈ S as


Value Q(s, ·, ·) = max



min Ea∼u,b∼v Q(s, a, b) ,

u∈∆(A) v∈∆(B)

where A and B are the joint action spaces, u and
v are the one-stage strategy, of agents in Teams 1
and 2, respectively. Additionally, if some function
Q satisﬁes the following ﬁxed-point equation for any
s∈S


Q(s, a, b) = R̄(s, a, b) + γ · Value Q(s, ·, ·) ,

(3.5)

where R̄ is the team-average reward of Team 1 (thus


−R̄ is that of Team 2), then such a Value Q(s, ·, ·)
deﬁnes the value of the game at any state s ∈ S.
In comparison to the single-agent case, the max min
operator, instead of the max one is used to deﬁne the
optimal/equilibrium value function.
Therefore, in order to solve the MARL problem in this setting, it suﬃces to ﬁnd a good estimate of the Q-function satisfying (3.5). Hence,
similarly as the single-team cooperative setting, all
agents within one team now collaboratively solve
for a common Q-function estimate by solving (3.4),
but replace the local target value at each agent i


by yji = rji + γ · Value Qit (s′j , a, b) , and the ﬁtting
function f (sj , aj ) by f (sj , aj , bj ), a function over the
joint action spaces of both teams. Then, such an optimization problem is solved in a distributed fashion
as (3.4). Similar error-propagation analysis can be
performed in this setting, leading to the ﬁnite-sample
error bounds of the decentralized FQI algorithm. To
the best of our knowledge, this appears to be the ﬁrst
ﬁnite-sample analysis for decentralized batch MARL
in non-cooperative settings.
3.2 Policy Evaluation
Besides control, a great number of algorithms
have been developed to address the policy evaluation
task in this decentralized MARL setting. In particular, policy evaluation corresponds to the critic step
of the aforementioned actor-critic algorithms only.
With a ﬁxed policy, this task enjoys a neater formulation, because the sampling distribution now becomes
stationary. Moreover, as linear function approximation is commonly used for this task, the objective is

8

Zhang et al. / Front Inform Technol Electron Eng

mostly convex. This makes the ﬁnite-time/sample
analyses easier, in comparison to many control algorithms with only asymptotic convergence guarantees.
Speciﬁcally, under joint policy π, suppose
each agent parameterizes the value function by
{Vω (s) := φ⊤ (s)ω : ω ∈ Rd }, where φ(s) ∈ Rd
is the feature vector at s ∈ S, and ω ∈ Rd
is the parameter vector. For notational convenience, let Φ := (· · · ; φ⊤ (s); · · · ) ∈ R|S|×d , D =
diag[{ηπ (s)}s∈S ] ∈ R|S|×|S| be a diagonal matrix constructed using the state-occupancy meaP
i,π
(s), where
sure ηπ , R̄π (s) = N −1 ·
i∈N R
Ri,π (s) = Ea∼π(· | s),s′ ∼P (· | s,a) [Ri (s, a, s′ )], and
P π ∈ R|S|×|S| with the (s, s′ ) element being
P
′
[P π ]s,s′ =
a∈A π(a | s)P (s | s, a). The objective
of all agents is to jointly minimize the mean square
projected Bellman error (MSPBE) associated with
the team-average reward, i.e.,
 2
min MSPBE(ω) : = ΠΦ Vω − γP π Vω − R̄π D
ω

2

= Aω − b C −1 ,

(3.6)

where ΠΦ := Φ(Φ⊤ DΦ)−1 Φ⊤ D is the projection
operator onto subspace {Φω : ω ∈ Rd }, A :=
E{φ(s)[φ(s) − γφ(s′ )]⊤ }, C := E[φ(s)φ⊤ (s)], and
b := E[R̄π (s)φ(s)]. Using Fenchel duality, and replacing the expectation with samples, the ﬁnite-sum
version of (3.6) can be re-formulated as a distributed
saddle-point problem
n

min max
ω

λi

1 XX
2(λi )⊤ Aj ω − 2(bij )⊤ λi − (λi )⊤ Cj λi ,
N n i∈N j=1

where n is the data size, Aj , Cj and bij are empirical
estimates of A, C and bi := E[Ri,π (s)φ(s)] using sample
j, respectively. The objective above is convex in ω and
concave in {λi }i∈N . The use of MSPBE as an objective is
standard in multi-agent policy evaluation (Macua et al.,
2015; Lee et al., 2018; Wai et al., 2018; Doan et al.,
2019a), and the idea of saddle-point reformulation has
been adopted in Macua et al. (2015); Lee et al. (2018);
Wai et al. (2018); Cassano et al. (2018).
With the formulation (3.6), Lee et al. (2018) develops a distributed variant of the gradient TD-based
method (Sutton et al., 2009), and establishes the asymptotic convergence using the ordinary diﬀerential equation
(ODE) method. Wai et al. (2018) proposes a double averaging scheme that combines the dynamic consensus
(Qu and Li, 2017) and the SAG algorithm (Schmidt
et al., 2017), in order to solve the saddle-point problem
with a linear rate. In Cassano et al. (2018), the idea
of variance-reduction, speciﬁcally, AVRG in (Ying et al.,

in press

2018), has been incorporated into gradient TD-based
policy evaluation. Achieving the same linear rate as Wai
et al. (2018), three advantages are claimed in Cassano
et al. (2018): i) data-independent memory requirement;
ii) use of eligibility traces (Singh and Sutton, 1996);
iii) no need for synchronization in sampling. More recently, standard TD learning (Tesauro, 1995), instead of
gradient-TD, has been generalized to this MARL setting,
with special focuses on ﬁnite-sample analyses, see Doan
et al. (2019a,b). By the proof techniques in Bhandari
et al. (2018), Doan et al. (2019a) studies the distributed
TD(0) algorithm. A projection operation is required on
the iterates, and the data samples are assumed to be
independent and identically distributed (i.i.d.). Then,
following the recent advance in Srikant and Ying (2019),
Doan et al. (2019b) provides ﬁnite-time performance of
the more general distributed TD(λ) algorithm, without
the need of any projection or i.i.d. noise assumption.
3.3 Other Learning Goals
Several other learning goals have also been investigated in this setting. Zhang et al. (2016) considers the
optimal consensus problem, where each agent tracks its
neighbors’ as well as a leader’s states, so that the consensus error is minimized by the joint policy. Then, a policy
iteration algorithm is devised, and made practical by introducing an actor-critic algorithm with neural networks
for function approximation. Zhang et al. (2018) also
uses a similar consensus error objective, with the name
of cooperative multi-agent graphical games. Oﬀ-policy
RL algorithms are developed, using a centralized-criticdecentralized-actor scheme.
As an essential ingredient in the algorithm design
for the decentralized MARL settings, communication efﬁciency in MARL has drawn increasing attention recently (Chen et al., 2018; Ren and Haupt, 2019; Lin
et al., 2019). In Chen et al. (2018), Lazily Aggregated
Policy Gradient (LAPG), a distributed PG algorithm is
developed, which can reduce the communication rounds
between the agents and a central controller. This is
achieved by judiciously designing communication trigger
rules. In Ren and Haupt (2019), the same policy evaluation problem as Wai et al. (2018) is addressed, and
develops a hierarchical distributed algorithm by proposing a mixing matrix diﬀerent from the doubly stochastic
one used in Zhang et al. (2018d); Wai et al. (2018);
Lee et al. (2018), which saves communication by allowing unidirectional information exchange among agents.
In comparison, Lin et al. (2019) proposes a distributed
actor-critic algorithm, which reduces the communication
by transmitting only one scalar entry of its state vector
at each iteration. The same convergence guarantee as
Zhang et al. (2018d) can be established.

Zhang et al. / Front Inform Technol Electron Eng

We note that RL under this decentralized setting
with networked agents has been studied beyond the
multi-agent setting. Indeed, several works have modeled
the setting for multi-task RL, where multiple cooperative
agents are also connected by a communication network,
without any coordination from a central controller. However, each agents is in face of an independent MDP, which
is not inﬂuenced by other agents. Diﬀerent agents may
still have diﬀerent reward functions, while the goal is to
learn the optimal joint policy that optimizes the longterm return corresponding to the team-average reward.
In some sense, this setting can be deemed as a simpliﬁed
version of the our MARL setting, for the less coupling
among agents. Under this setting, Pennesi and Paschalidis (2010) develops a distributed actor-critic algorithm,
where each agent ﬁrst conducts a local TD-based critic
step, followed by a consensus-based actor step that calculates the gradient based on the neighbors’ information
exchanged. The gradient of the average return is then
shown to converge to zero. In Macua et al. (2017),
Diff-DAC, another distributed actor-critic algorithm is
developed from duality theory. The updates, which look
similar to those of Zhang et al. (2018d), are essentially
an example of the dual ascent method to solve some linear program. This provides additional insights into the
actor-critic update for this setting.
Policy evaluation has also been considered under
this setting of networked agents interacting with independent MDPs. The early work Macua et al. (2015) studies oﬀ-policy evaluation using the importance sampling
technique. Without coupling among agents, there is no
need for each agent to know the actions of the others.
Then, a diﬀusion-based distributed gradient-TD method
is proposed, which is proven to converge with a sublinear
rate in the mean-square sense. Stanković and Stanković
(2016) then proposes two other variants of the gradientTD updates, i.e., GTD2 and TDC (Sutton et al., 2009),
and proves weak convergence using the general stochastic approximation theory developed in Stanković et al.
(2016). Stanković and Stanković (2016) speciﬁcally considers the case where agents are connected by a timevarying communication network. The aforementioned
work Cassano et al. (2018) also considers the independent MDP setting, with the same results established as
the actual MARL one.

4 Concluding Remarks
Owing to the ubiquity of sequential decision-making
in presence of more than one agents, multi-agent RL has
long been a signiﬁcant while challenging research area. In
this review, we have summarized the recent advances in a
sub-area of MARL: decentralized MARL with networked

in press

9

agents. Particularly, we have focused on the MARL
algorithms that concern this setting, and are backed by
theoretical analysis. We hope our review is appealing
to the researchers of similar interests, and has provided
stimulus for them to continue pursuing this direction.
Interesting while open future directions may concern the
setting with partial observability, with adversarial agents
in the system. It is also interesting to develop theoretical
results for MARL algorithms under this setting with
deep neural networks as function approximators, which
have already achieved tremendous empirical success. See
Zhang et al. (2019) for more discussions on intriguing
future directions.
References
Adler JL, Blue VJ, 2002.
A cooperative multi-agent
transportation management and route guidance system.
Transportation Research Part C: Emerging Technologies, 10(5):433-454.
Agarwal A, Duchi JC, 2011. Distributed delayed stochastic
optimization. Advances in Neural Information Processing Systems, p.873-881.
Antos A, Szepesvári C, Munos R, 2008a. Fitted Q-iteration
in continuous action-space MDPs. Advances in Neural
Information Processing Systems, p.9-16.
Antos A, Szepesvári C, Munos R, 2008b. Learning nearoptimal policies with Bellman-residual minimization
based ﬁtted policy iteration and a single sample path.
Machine Learning, 71(1):89-129.
Başar T, Olsder GJ, 1999. Dynamic Noncooperative Game
Theory. SIAM.
Baxter J, Bartlett PL, 2001. Inﬁnite-horizon policy-gradient
estimation. Journal of Artificial Intelligence Research,
15:319-350.
Bertsekas DP, 2005. Dynamic Programming and Optimal
Control. Athena Scientiﬁc Belmont, MA.
Bhandari J, Russo D, Singal R, 2018. A ﬁnite time analysis of
temporal diﬀerence learning with linear function approximation. Conference On Learning Theory, p.1691-1692.
Bhatnagar S, Precup D, Silver D, et al., 2009a. Convergent temporal-diﬀerence learning with arbitrary smooth
function approximation. Advances in Neural Information Processing Systems, p.1204-1212.
Bhatnagar S, Sutton R, Ghavamzadeh M, et al., 2009b. Natural actor-critic algorithms. Automatica, 45(11):24712482.
Borkar VS, 2008. Stochastic Approximation: A Dynamical
Systems Viewpoint. Cambridge University Press.
Boutilier C, 1996. Planning, learning and coordination in
multi-agent decision processes. Conference on Theoretical Aspects of Rationality and Knowledge, p.195-210.
Boyd S, Parikh N, Chu E, et al., 2011. Distributed optimization and statistical learning via the alternating direction
method of multipliers. Foundations and Trends R in
Machine Learning, 3(1):1-122.
Busoniu L, Babuska R, De Schutter B, et al., 2008. A comprehensive survey of multiagent reinforcement learning.
IEEE Transactions on Systems, Man, and Cybernetics,
Part C, 38(2):156-172.
Cassano L, Yuan K, Sayed AH, 2018. Multi-agent fully
decentralized oﬀ-policy learning with linear convergence
rates. arXiv preprint arXiv:181007792, .

10

Zhang et al. / Front Inform Technol Electron Eng

Chen T, Zhang K, Giannakis GB, et al., 2018.
Communication-eﬃcient
distributed
reinforcement
learning. arXiv preprint arXiv:181203239, .
Ciosek K, Whiteson S, 2018. Expected policy gradients for reinforcement learning. arXiv preprint arXiv:180103326,
.
Corke P, Peterson R, Rus D, 2005. Networked robots: Flying
robot navigation using a sensor net. Robotics Research,
:234-243.
Dall’Anese E, Zhu H, Giannakis GB, 2013.
Distributed
optimal power ﬂow for smart microgrids. IEEE Transactions on Smart Grid, 4(3):1464-1475.
Doan T, Maguluri S, Romberg J, 2019a. Finite-time analysis
of distributed TD (0) with linear function approximation
on multi-agent reinforcement learning. International
Conference on Machine Learning, p.1626-1635.
Doan TT, Maguluri ST, Romberg J, 2019b. Finite-time
performance of distributed temporal diﬀerence learning
with linear function approximation.
arXiv preprint
arXiv:190712530, .
Fan J, Tong X, Zeng Y, 2015.
Multi-agent inference
in social networks: A ﬁnite population learning approach. Journal of the American Statistical Association,
110(509):149-158.
Farahmand Am, Szepesvári C, Munos R, 2010. Error propagation for approximate policy and value iteration. Advances in Neural Information Processing Systems, p.568576.
Foerster J, Assael YM, de Freitas N, et al., 2016. Learning
to communicate with deep multi-agent reinforcement
learning. Advances in Neural Information Processing
Systems, p.2137-2145.
Gupta JK, Egorov M, Kochenderfer M, 2017. Cooperative
multi-agent control using deep reinforcement learning.
International Conference on Autonomous Agents and
Multi-Agent Systems, p.66-83.
Hong M, Chang TH, 2017. Stochastic proximal gradient
consensus over random networks. IEEE Transactions
on Signal Processing, 65(11):2933-2948.
Jakovetic D, Xavier J, Moura JM, 2011. Cooperative convex optimization in networked systems: Augmented
lagrangian algorithms with directed gossip communication.
IEEE Transactions on Signal Processing,
59(8):3889-3902.
Kar S, Moura JM, 2013. Consensus+innovations distributed
inference over networks: cooperation and sensing in networked systems. IEEE Signal Processing Magazine,
30(3):99-109.
Kar S, Moura JM, Poor HV, 2013. QD-learning: A collaborative distributed strategy for multi-agent reinforcement
learning through consensus + innovations. IEEE Transactions on Signal Processing, 61(7):1848-1862.
Kober J, Bagnell JA, Peters J, 2013. Reinforcement learning
in robotics: A survey. International Journal of Robotics
Research, 32(11):1238-1274.
Konda VR, Tsitsiklis JN, 2000.
Actor-critic algorithms.
Advances in Neural Information Processing Systems,
p.1008-1014.
, 2012. Batch reinforcement learning.
Lauer M, Riedmiller M, 2000. An algorithm for distributed
reinforcement learning in cooperative multi-agent systems. International Conference on Machine Learning.
Lee D, Yoon H, Hovakimyan N, 2018. Primal-dual algorithm
for distributed reinforcement learning: distributed GTD.
IEEE Conference on Decision and Control, p.1967-1972.

in press

Lillicrap TP, Hunt JJ, Pritzel A, et al., 2016. Continuous
control with deep reinforcement learning. International
Conference on Learning Representations.
Lin Y, Zhang K, Yang Z, et al., 2019. A communicationeﬃcient multi-agent actor-critic algorithm for distributed reinforcement learning. IEEE Conference on
Decision and Control.
Littman ML, 1994. Markov games as a framework for multiagent reinforcement learning. International Conference
on Machine Learning, p.157-163.
Liu B, Liu J, Ghavamzadeh M, et al., 2015. Finite-sample
analysis of proximal gradient TD algorithms. Conference on Uncertainty in Artiﬁcial Intelligence, p.504-513.
Lowe R, Wu Y, Tamar A, et al., 2017. Multi-agent actorcritic for mixed cooperative-competitive environments.
Advances in Neural Information Processing Systems,
p.6379-6390.
Macua SV, Chen J, Zazo S, et al., 2015. Distributed policy
evaluation under multiple behavior strategies. IEEE
Transactions on Automatic Control, 60(5):1260-1274.
Macua SV, Tukiainen A, Hernández DGO, et al., 2017.
Diﬀ-dac: Distributed actor-critic for average multitask deep reinforcement learning.
arXiv preprint
arXiv:171010363, .
, 2008. Sequential Decomposition of Sequential Dynamic
Teams: Applications to Real-Time Communication and
Networked Control Systems.
Mnih V, Kavukcuoglu K, Silver D, et al., 2015. Human-level
control through deep reinforcement learning. Nature,
518(7540):529-533.
Munos R, Szepesvári C, 2008. Finite-time bounds for ﬁtted
value iteration. Journal of Machine Learning Research,
9(May):815-857.
Munos R, 2007. Performance bounds in ℓp -norm for approximate value iteration. SIAM Journal on Control and
Optimization, 46(2):541-561.
Nedic A, Ozdaglar A, 2009. Distributed subgradient methods
for multi-agent optimization. IEEE Transactions on
Automatic Control, 54(1):48-61.
Nedic A, Olshevsky A, Shi W, 2017.
Achieving geometric convergence for distributed optimization over
time-varying graphs. SIAM Journal on Optimization,
27(4):2597-2633.
Omidshaﬁei S, Pazis J, Amato C, et al., 2017. Deep decentralized multi-task multi-agent reinforcement learning
under partial observability. International Conference on
Machine Learning, p.2681-2690.
Pennesi P, Paschalidis IC, 2010. A distributed actor-critic
algorithm and applications to mobile sensor network coordination problems. IEEE Transactions on Automatic
Control, 55(2):492-497.
Qie H, Shi D, Shen T, et al., 2019. Joint optimization of
multi-UAV target assignment and path planning based
on multi-agent reinforcement learning. IEEE Access, .
Qu G, Li N, 2017. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control
of Network Systems, 5(3):1245-1260.
Rabbat M, Nowak R, 2004. Distributed optimization in sensor networks. International Symposium on Information
Processing in Sensor Networks, p.20-27.
Ren J, Haupt J, 2019. A communication eﬃcient hierarchical distributed optimization algorithm for multi-agent
reinforcement learning. Real-world Sequential Decision
Making Workshop at International Conference on Machine Learning.

Zhang et al. / Front Inform Technol Electron Eng

Riedmiller M, 2005. Neural ﬁtted Q iteration–ﬁrst experiences with a data eﬃcient neural reinforcement learning
method. European Conference on Machine Learning,
p.317-328.
Sayed AH, et al., 2014. Adaptation, learning, and optimization over networks. Foundations and Trends R in
Machine Learning, 7(4-5):311-801.
Schmidt M, Le Roux N, Bach F, 2017. Minimizing ﬁnite sums
with the stochastic average gradient.
Mathematical
Programming, 162(1-2):83-112.
Shalev-Shwartz S, Shammah S, Shashua A, 2016. Safe, multiagent, reinforcement learning for autonomous driving.
arXiv preprint arXiv:161003295, .
Shapley LS, 1953. Stochastic games. Proceedings of the
National Academy of Sciences, 39(10):1095-1100.
Shi W, Ling Q, Wu G, et al., 2015. Extra: An exact ﬁrstorder algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944-966.
Silver D, Lever G, Heess N, et al., 2014.
Deterministic
policy gradient algorithms. International Conference on
Machine Learning, p.387-395.
Silver D, Huang A, Maddison CJ, et al., 2016. Mastering the
game of Go with deep neural networks and tree search.
Nature, 529(7587):484-489.
Silver D, Schrittwieser J, Simonyan K, et al., 2017. Mastering
the game of Go without human knowledge. Nature,
550(7676):354.
Singh S, Jaakkola T, Littman ML, et al., 2000. Convergence
results for single-step on-policy reinforcement-learning
algorithms. Machine Learning, 38(3):287-308.
Singh SP, Sutton RS, 1996. Reinforcement learning with
replacing eligibility traces. Machine Learning, 22(13):123-158.
Srikant R, Ying L, 2019. Finite-time error bounds for linear
stochastic approximation and TD learning. Conference
on Learning Theory, p.2803-2830.
Stanković MS, Stanković SS, 2016. Multi-agent temporaldiﬀerence learning with linear function approximation:
Weak convergence under time-varying network topologies. IEEE American Control Conference, p.167-172.
Stanković MS, Ilić N, Stanković SS, 2016.
Distributed
stochastic approximation: Weak convergence and network design. IEEE Transactions on Automatic Control,
61(12):4069-4074.
Suttle W, Yang Z, Zhang K, et al., 2019. A multi-agent
oﬀ-policy actor-critic algorithm for distributed reinforcement learning. arXiv preprint arXiv:190306372, .
Sutton RS, McAllester DA, Singh SP, et al., 2000. Policy
gradient methods for reinforcement learning with function approximation. Advances in Neural Information
Processing Systems, p.1057-1063.
Sutton RS, Szepesvári C, Maei HR, 2008. A convergent
O(n) algorithm for oﬀ-policy temporal-diﬀerence learning with linear function approximation. Advances in
Neural Information Processing Systems, 21(21):16091616.
Sutton RS, Maei HR, Precup D, et al., 2009. Fast gradientdescent methods for temporal-diﬀerence learning with
linear function approximation. International Conference
on Machine Learning, p.993-1000.
Sutton RS, Mahmood AR, White M, 2016. An emphatic approach to the problem of oﬀ-policy temporal-diﬀerence
learning.
Journal of Machine Learning Research,
17(1):2603-2631.
Tesauro G, 1995. Temporal diﬀerence learning and TDGammon. Communications of the ACM, 38(3):58-68.

in press

11

Tsitsiklis JN, Van Roy B, 1997.
Analysis of temporaldiﬀference learning with function approximation. Advances in Neural Information Processing Systems,
p.1075-1081.
Tu SY, Sayed AH, 2012. Diﬀusion strategies outperform consensus strategies for distributed estimation over adaptive networks. IEEE Transactions on Signal Processing,
60(12):6217-6234.
Varshavskaya P, Kaelbling LP, Rus D, 2009. Eﬃcient distributed reinforcement learning through agreement. Distributed Autonomous Robotic Systems, p.367-378.
Wai HT, Yang Z, Wang Z, et al., 2018. Multi-agent reinforcement learning via double averaging primal-dual
optimization. Advances in Neural Information Processing Systems, p.9649-9660.
Wang X, Sandholm T, 2003. Reinforcement learning to play
an optimal Nash equilibrium in team Markov games.
Advances in Neural Information Processing Systems,
p.1603-1610.
Watkins CJ, Dayan P, 1992. Q-learning. Machine Learning,
8(3-4):279-292.
Williams RJ, 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256.
Xiao L, Boyd S, Kim SJ, 2007. Distributed average consensus
with least-mean-square deviation. Journal of Parallel
and Distributed Computing, 67(1):33-46.
Ying B, Yuan K, Sayed AH, 2018. Convergence of variancereduced learning under random reshuﬄing. IEEE International Conference on Acoustics, Speech and Signal
Processing, p.2286-2290.
Yu H, 2015. On convergence of emphatic temporal-diﬀerence
learning. Conference on Learning Theory, p.1724-1751.
Zazo S, Macua SV, Sánchez-Fernández M, et al., 2016. Dynamic potential games with constraints: Fundamentals
and applications in communications. IEEE Transactions on Signal Processing, 64(14):3806-3821.
Zhang H, Jiang H, Luo Y, et al., 2016. Data-driven optimal
consensus control for discrete-time multi-agent systems
with unknown dynamics using reinforcement learning
method. IEEE Transactions on Industrial Electronics,
64(5):4091-4100.
Zhang K, Lu L, Lei C, et al., 2018a. Dynamic operations
and pricing of electric unmanned aerial vehicle systems
and power networks. Transportation Research Part C:
Emerging Technologies, 92:472-485.
Zhang K, Yang Z, Başar T, 2018b. Networked multi-agent
reinforcement learning in continuous spaces. IEEE Conference on Decision and Control, p.2771-2776.
Zhang K, Yang Z, Liu H, et al., 2018c. Finite-sample analyses for fully decentralized multi-agent reinforcement
learning. arXiv preprint arXiv:181202783, .
Zhang K, Yang Z, Liu H, et al., 2018d. Fully decentralized multi-agent reinforcement learning with networked
agents. International Conference on Machine Learning,
p.5867-5876.
Zhang K, Yang Z, Başar T, 2019. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:191110635, .
Zhang Q, Zhao D, Lewis FL, 2018. Model-free reinforcement learning for fully cooperative multi-agent graphical games. International Joint Conference on Neural
Networks, p.1-6.
Zhang Y, Zavlanos MM, 2019.
Distributed oﬀ-policy
actor-critic reinforcement learning with policy consensus. arXiv preprint arXiv:190309255, .

