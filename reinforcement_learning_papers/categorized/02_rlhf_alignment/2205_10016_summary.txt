Here is the extracted information in the requested format:

**PAPER:** Learning Progress Driven Multi-Agent Curriculum
**TITLE:** Learning Progress Driven Multi-Agent Curriculum
**ARXIV_ID:** 2205.10016v3
**RESEARCH_METHOD:** 02_rlhf_alignment

**METHOD_DESCRIPTION:** The paper proposes a new method for multi-agent curriculum learning, called Self-Paced Multi-Agent Reinforcement Learning (SPMARL). SPMARL extends single-agent SPRL to multi-agent settings and addresses two potential flaws in general reward-based curriculum methods for MARL: unstable estimation based on sparse episode returns and increased credit assignment difficulty in tasks where more agents tend to yield higher returns. SPMARL prioritizes tasks based on learning progress instead of episode returns and optimizes value loss over the context distribution.

**KEY_CONTRIBUTIONS:**
* Proposes a new method for multi-agent curriculum learning, SPMARL, which extends single-agent SPRL to multi-agent settings.
* Addresses two potential flaws in general reward-based curriculum methods for MARL: unstable estimation and increased credit assignment difficulty.
* SPMARL prioritizes tasks based on learning progress instead of episode returns and optimizes value loss over the context distribution.
* Evaluates SPMARL on three challenging benchmarks, including MPE Simple-Spread, XOR matrix game, and SMAC-v2 Protoss tasks, and shows that it outperforms baseline methods.
* Provides an ablation study on the hyperparameter VLB and shows that SPMARL performs robustly across a broad range of VLB values.
