arXiv:2203.10603v1 [cs.MA] 20 Mar 2022

Model-based Multi-agent Reinforcement Learning:
Recent Progress and Prospects
Xihuai Wang , Zhicheng Zhang , Weinan Zhang
Shanghai Jiao Tong University
{leoxhwang, zhangzhicheng1, wnzhang}@sjtu.edu.cn

Abstract
Significant advances have recently been achieved
in Multi-Agent Reinforcement Learning (MARL)
which tackles sequential decision-making problems involving multiple participants. However,
MARL requires a tremendous number of samples
for effective training. On the other hand, modelbased methods have been shown to achieve provable advantages of sample efficiency. However,
the attempts of model-based methods to MARL
have just started very recently. This paper presents
a review of the existing research on model-based
MARL, including theoretical analyses, algorithms,
and applications, and analyzes the advantages and
potential of model-based MARL. Specifically, we
provide a detailed taxonomy of the algorithms and
point out the pros and cons for each algorithm according to the challenges inherent to multi-agent
scenarios. We also outline promising directions for
future development of this field.

1 Introduction
Recent years have witnessed the tremendous success of
Multi-Agent Reinforcement Learning (MARL) in various
complex tasks involving multiple participants, including
playing real-time strategy games [Arulkumaran et al., 2019;
[Bard et al., 2020],
Ye et al., 2020],
card
games
sports games [Kurach et al., 2020], autonomous driv[Zhou et al., 2020],
ing
and multi-robot navigation [Long et al., 2018].
MARL solves the sequential
decision-making problem in which multiple agents interact
in a common environment, receive reward signals, and
improve their policies to maximize cumulative reward.
Despite the success stories of MARL algorithms, several
challenges of multi-agent scenarios give rise to the sample inefficiency of MARL. Specifically, the major challenge is the
non-stationarity, which means that all agents improve their
policies according to their observations and rewards, leading
to a non-stationary environment from the perspective of an
individual agent. The guarantees of the effectiveness of most
reinforcement learning algorithms may not hold in multiagent scenarios since the Markov property to which the guarantees are tied could be invalidated in non-stationary multi-

agent scenarios. Furthermore, the dimensions of the variable spaces could grow exponentially w.r.t. the number of
agents, leading to the dimension blowup in the policy search
space [Hernandez-Leal et al., 2020]. Due to two significant
challenges, together with the associated problems such as partial observability, coordination, credit assignment, and scalability, MARL requires a tremendous number of samples for
effective training [Gronauer and Diepold, 2021].
On the other hand, model-based methods in single-agent
RL scenarios have shown their advantages in sample efficiency both practically [Wang et al., 2019] and theoretically [Sun et al., 2019]. However, model-based MARL, in
the sense that agents learn environment models1 and leverage the learned model for policy improvement, has just
started to attract attention and to be applied in real-world
tasks [Chockalingam et al., 2018]. Moreover, simply applying off-the-shelf model-based RL methods in single-agent
scenarios to multi-agent scenarios would hardly work due to
the previously mentioned challenges.
This paper provides an overview of existing model-based
MARL methods and shows the potential of model-based
MARL methods in improving sample efficiency and resolving the inherent challenges. First, we present the theoretical analyses on the sample efficiency of model-based MARL
in limited settings. Second, we give a taxonomy of the algorithms, organized by the training schemes, the opponent
awareness, and the environment model usage. Each algorithm
is then discussed in detail on its pros and cons according to
the environment usage and whether the model-based methods
resolve or sharpen the challenges. Finally, we spotlight several areas worth exploring and outline future directions that
have the potential to the development of this field based on
our knowledge.

2 Background
2.1

Problem Formulation

The sequential decision making problem in multiagent scenarios is generally formulated as a stochastic
game [Shapley, 1953], which is also known as Markov game.
An n-agent stochastic game can be formalized as a tuple
(S, {Ai }i∈N , {Ri }i∈N , T , γ), where N = {1, . . . , n} is the
1
In this paper, an environment model refers to a dynamics model
and a reward function model.

set of agents, S is the state space of the stochastic game, Ai
is the action space of agent i. Denote the joint action space as
A = A1 × · · · × An , Ri : S × A 7→ R is the reward function
of agent i, T : S × A 7→ ∆(S) is the dynamics function
denoting the transition probability to each state. γ ∈ [0, 1) is
the discount factor for future rewards.
At each timestep t, each agent i takes action ait simultaneously according to the state st . Then the state transits to st+1
and each agent i receives reward rti . Denoting the joint acj
tion of other agents as a−i
t = {at }j6=i , we can formulate the
joint policy of other agents from the perspective of agent i as
j j
j
i
π −i (a−i
t |st ) = Πj∈{−i} π (at |st ), where π : S 7→ ∆(A ) is
the policy of agent j. Each agent i aims at finding its optimal
policy to maximize the expected return (cumulative reward),
defined as
πi∗ = argmaxπi ηi [π i , π −i ]
∞
hX
i
= argmaxπi Eτ ∼(T ,πi ,π−i )
)
,
γ t Ri (st , ait , a−i
t
t=0

−i
i
where τ = {(s0 , ai0 , a−i
0 ), (s1 , a1 , a1 ), . . .} denotes the
sampled trajectory.
From the objective, we note that the optimal performance
and the optimal policy of agent i depend not only on its own
policy but also on the behaviors of other agents.
An important motivation for model-based methods is to
reduce the sample complexity, i.e., how many samples are
needed to achieve a certain performance in a multi-agent environment. Following Zhang et al. [2021], from the perspective of an arbitrary agent in this stochastic game, we define
the sample complexity in terms of two parts: (i) dynamics
sample complexity, i.e., the number of state samples from the
dynamics environment in which the group of agents interact,
and (ii) opponent sample complexity, i.e., the total number
of opponent action samples, including the samples from real
interactions with the environment and the number of actions
requested through communication in decentralized settings.

2.2

Challenges in MARL

To reveal the role of model-based methods in MARL,
we depict several challenges [Du and Ding, 2021;
Zhang et al., 2019; Nguyen et al., 2018] that MARL suffers
from and analyze whether current model-based methods
alleviate or sharpen these challenges in Section 4.
Non-stationarity. One of the key challenges of MARL origins from the fact that multiple agents interact with the environment to improve their policies simultaneously and individually. From the perspective of a single agent, the environment becomes non-stationary due to the co-adaption of the
opponents, i.e., the perceived transition function and reward
function change over time. The policy learned in the nonstationary environment has mismatched expectation about the
opponents’ policies, and the optimal behavior of the agent depends not only on the perceived information but also on the
behavior of its opponents. Thus, the Markov assumption is
violated, and the theoretical guarantees for single-agent scenarios may be lost in multi-agent systems. We refer the readers to Papoudakis et al. [2019] for further discussions on the
non-stationarity.

Partial Observability. Unlike the stochastic game setting,
agents may neither observe the global state of the environment nor have full knowledge about opponents in real-world
applications. The Markov property may not be satisfied under partially observable scenarios. Agents need to approximate the global environment state based on limited knowledge to alleviate the non-Markov problem. The decisionmaking problem in partially observable scenarios has a similar definition as a stochastic game, but with observation
spaces {Ωi }i∈N for each agent and the observation function
O : S × A 7→ ∆(Ω) that denotes the conditional probability
over the joint observation space Ω = Ω1 × · · · × Ωn given
state s′ ∈ S and joint action in the previous timestep a ∈ A.
Coordination. Accomplishing a shared goal in cooperative
settings requires the agents to reach a consensus about how
the joint action would improve the performance of all agents.
However, since the behavior of one agent influences the reward obtained by other agents, the exploration or sub-optimal
action of one agent could overshadow other agents’ actions
selected at the same time, leading to incorrect reward and
value estimation. The misled value estimation could influence the agents’ search spaces and result in sub-optimal policies.
Credit Assignment. Given a shared reward signal in the
fully-cooperative setting, the agents could estimate the impact of the joint action on the common reward but lack
the ability to estimate the impact of a single agent’s action. Even in the centralized training scheme in which agents
observe the global state and access to the joint action, the
credit assignment problem is difficult to alleviate without
strong assumptions on the reward structure [Hu et al., 2021;
Wang et al., 2021]. Further, the partially observable setting
and the decentralized training scheme bring in additional
challenges to the credit assignment problem.
Scalability. In multi-agent scenarios, the agents may take
the joint action space A, the joint observation space Ω and the
global state space S into consideration. The dimension of A
and Ω grows exponentially w.r.t. the number of agents and the
dimension of S also grows w.r.t. the number of agents. Furthermore, every agent in the environment adds extra complexity for improving the policies. Such properties of multi-agent
problems complicate finding and analyzing optimal policies,
and the required computation effort grows exponentially w.r.t.
the number of agents, leading to the scalability problem of
MARL methods.

3

Efficiency of Model-based MARL

This section presents the theoretical analyses on the efficiency
of model-based MARL. We claim that the efficiency benefits
both from leveraging the knowledge of the learned environment model in planning or policy improvement, and alleviating the challenges as mentioned in Section 2.2.
Model-based methods in single-agent scenarios are
known to be efficient, i.e., yield lower sample complexity, both practically [Wang et al., 2019] and theoretically [Sun et al., 2019]. We note that the enhanced data
efficiency of these model-based methods comes from both

Algorithm

Training Scheme

MAMBPO [Willemsen et al., 2021]
CPS [Bargiacchi et al., 2021]
MATO [Park et al., 2019]
[Brafman and Tennenholtz, 2000]
R-MAX [Brafman and Tennenholtz, 2001]
M3 -UCRL [Yang et al., 2018]
Tesseract [Mahajan et al., 2021]
[Vaart et al., 2021]
AORPO [Zhang et al., 2021]
HPP [Wang et al., 2020]
MBOM [Yu et al., 2021]
MACI [Pretorius et al., 2020]
IS [Kim et al., 2021]

Centralized
Centralized
Centralized
Centralized
Centralized
Centralized
Centralized
Centralized
Decentralized
Decentralized
Decentralized
Decentralized
Decentralized

Opponent Awareness
×
×
X
×
×
×
×
×
X
X
X
×
X

Environment Model Usage
Dyna-style
Dyna-style
MPC
Direct Method
Direct Method
Direct Method
Dynamic Programming
Dynamic Programming
Dyna-style
MPC
MPC
Communication
Communication

Table 1: Classification of model-based MARL algorithms. This classification is first organized according to their training schemes, determined
by the settings of possible applications. The next criteria opponent awareness refers to whether the agents are aware of and utilize the
knowledge of other agents in modeling the dynamics transition or making decisions. We report how the algorithms leverage the learned
models in the Dynamics Model Usage column.

leveraging the knowledge of learned environment models and other benefits from learning the models such
as the improved stability and the encouraged exploration
[Moerland et al., 2020]. Similarly, model-based methods
in multi-agent scenarios are proved to have lower sample
complexity theoretically, due to the benefits from leveraging the knowledge of the learned environment model.
Zhang et al. [2020] investigated the sample complexity in the
two-player discounted zero-sum Markov games. The analyzed model-based MARL method decouples the learning
and planning phases: first estimate an empirical model using the collected data and then find the optimal policies in
the learned model. Given a generative model, the modelbased method that only updates the policy based on the generated data achieves a sample complexity for finding the ǫNE policies as O(|S||A1 ||A2 |(1 − γ)−3 ǫ−2 ) in the rewardagnostic setting and O(|S|(|A1 | + |A2 |)(1 − γ)−3 ǫ−2 ) in
the reward-aware setting, respectively, where ǫ measures the
difference from the accurate Nash Equilibriums. The sample complexities in both cases indicate that the model-based
method in multi-agent scenarios is sample-efficient, compared to the sample complexities derived in model-free methods [Bai and Jin, 2020].
Subramanian et al. [2021] analyzed the model-based
MARL methods in general-sum games from the perspective
that the learned dynamics model and learned reward function
lead to an approximate game. The Markov perfect equilibrium (MPE) of an approximated game is always an approximate MPE of the original game, denoted as α-approximate
MPE, where α describes the difference between the Q functions obtained in the approximated game and that in the
original game. The model-based MARL methods achieve
a sample complexity for finding an α-approximate MPE as
O(|S||A|(1 − γ)−2 α−2 ), which also shows the superior sample efficiency of model-based MARL methods. Additionally,
the robustness analysis establishes an explicit bound for α on

the approximation errors of the dynamics model and the reward function.
Although these theoretical analyses prove the advantages
of model-based MARL methods in sample efficiency under
limited settings, the study under more general settings is still
not investigated. Moreover, the other benefits of model-based
MARL methods are not identified.

4 Algorithms
This section provides a review of recent progress in modelbased MARL. We first categorize the methods according to
their training schemes, for example, centralized training or
decentralized training. The classification is illustrated in Table 1. Then we review each method in detail and analyze
them on two aspects:
i) How the learned models are used? We focus on the
scheme of model usage and the consideration about the properties of multi-agent scenarios, e.g., the interactions among
agents when utilizing the learned models.
ii) Whether and how the model-based techniques alleviate or sharpen the inherent challenges in MARL? Although
the foremost motivation of model-based methods is to improve the sample efficiency by leveraging the knowledge of
learned environment models in planning or policy improvement, a new possibility for resolving the challenges mentioned in Section 2.2 may arise with the learned models. The
alleviation of the inherent challenges in multi-agent scenarios
could also improve the sample efficiency.

4.1

Centralized Training

The direct extension of model-based RL methods in singleagent scenarios to multi-agent scenarios adopts the centralized training scheme. Most of the methods do not consider
the properties of multi-agent scenarios but only consider improving the sample efficiency by leveraging the knowledge

of the learned environment model in planning or policy improvement, as in single-agent scenarios.
Dyna-style. Many recent works have focused on the Dynastyle model-based methods in single-agent scenarios, where
the data collected in the real environment is used both to
learn environment model and to improve policies. Dyna-style
methods in single-agent scenarios show their sample efficiency both practically and theoretically [Janner et al., 2019].
In terms of which part of the generated data to use
when improving the policy, the principles include trusting data from learned models that are estimated to be accurate enough and trusting data that is itself close to the
real data. MAMBPO [Willemsen et al., 2021] investigates
model-based methods in Centralized Training Decentralized
Execution (CTDE) paradigm where agents only observe local observations when making decisions, and global information is accessible when agents improving their policies.
A centralized environment model P (s′ , o′ , r|s, a) is learned
to predict the next state, the next joint observation and the
current reward, given the current state and current joint action. MAMBPO follows the principle that only the generated
data which is close to the real data can be leveraged for policy improvement. Similar to MBPO [Janner et al., 2019], the
model rollouts begin from the experienced states, and the data
generated from the model within k steps is used to improve
the policies. MAMBPO can indeed improve the sample efficiency empirically, and alleviate the partial observability and
non-stationarity problems by adopting the CTDE paradigm.
Additionally, with learned dynamics and reward models,
extensions of other techniques to multi-agent scenarios could
be feasible and efficient. In CPS [Bargiacchi et al., 2021], the
agent learns a dynamics model and a reward model to determine which state-action pairs have the higher priority to be
updated, and the generated data is also used for policy improvement.
Model Predictive Control. One typical usage of learned
models is planning in the predicted states to select actions.
Model Predictive Control (MPC) is a cardinal example, which
selects the action with the highest reward in the planning
rollouts. MATO [Krupnik et al., 2019] considers the twoagent scenario and aims at reducing the accumulating errors
through predicting trajectory segments rather than a single
next state, instead of deciding which part of the generated
data to use as in the Dyna-style methods. As the fundamental problem of modeling the dynamics in multi-agent scenarios, the interaction between the agents are captured by dis−i
i −i
entangling the joint model: P̂ (sit+1 , ait , st+1
, a−i
t |st , st ) =
−i −i i
i
−i −i
i i
i i
P̂ (st+1 , at |st ) · P̂ (st+1 , at |st , st+1 , at ), which means
that agent i infers the incoming action and the next state of
agent −i based on agent −i’s current state, its own current
action, and its own predicted next state. After approximating P̂ i and P̂ −i by variational autoencoders, MATO selects
among the latent variables which are the candidates using
in the MPC procedure, and determines the best segments.
MATO indicates the effectiveness of modeling the interaction
among agents in two-agent scenarios. The use of latent variables instead of state-action pairs as candidates in the MPC
procedure makes similar methods more feasible to apply in

scenarios with more agents, alleviating the scalability challenge.
Direct Method. The direct method means that the dynamics model is learned based on the data collected from the
multi-agent environment and the policy is improved based
only on the data generated by the learned models. To
the best of our knowledge, Brafman and Tennenholtz [2000]
and R-MAX [Brafman and Tennenholtz, 2001] are the earliest model-based algorithms using the direct method in
multi-agent scenarios, solving single-controller stochastic
games and zero-sum stochastic games respectively. Both
methods focus on building a dynamics model and a reward model to resolve the exploration vs. exploitation
dilemma. The dynamics model is initialized towards encouraging exploration and transition probability of a stateaction pair (s, a) is updated only when (s, a) is encountered enough times, thus balancing exploration and exploitation. M3 -UCRL [Pásztor et al., 2021] incorporates modelbased techniques in the multi-agent mean-field reinforcement
learning method [Yang et al., 2018]. An agent using M3 UCRL updates its policy using only the data generated from
the dynamics model and the mean-field trajectory model.
Yang et al. [2018] proved a cumulative regret bound that
measures the discrepancy between the cumulative expected
reward of the optimal policy and that of the policy generated
from the simulated data. These direct model-based methods
verify that the learned model could help resolve the exploration vs. exploitation dilemma. In contrast, the direct usage
of the learned model may neglect the negative influence of
the inaccurate models.
Dynamic Programming. Among recent progress in
MARL, very few algorithms in the literature use Dynamic
Programming (DP), in the sense that the policy and value
function are improved considering the state transition
probabilities, e.g., the value iteration
P algorithm updates
the policy as π(s) = argmaxa s′ ,r p(s′ , r|s, a)[r +
γV (s′ )] [Sutton and Barto, 1998]. DP may not be practical for large-scale problems due to its high computation
complexity, especially in multi-agent scenarios where
the variable spaces grow exponentially with the number
of agents. To resolve the scalability challenge, Tesseract [Mahajan et al., 2021] shows that the exponential blowup
problem in learning policies and value functions can be
addressed by accurately representing states and actions using
low-rank tensors. This indicates that appropriate classes
of models could be found to balance the learnability and
expressiveness of policies and value functions. Model-based
Tesseract builds an empirical environment model as the sum
of low-rank tensors since states and actions are decomposed
into low-rank tensors, then evaluates the Q function using
DP with this approximate environment model. Based on
Tesseract, Vaart et al. [2021] investigated the generalization
of the low-rank dynamics and reward model and shows that
low-rank models lead to higher sample efficiency in modelbased MARL methods. These methods use dimensionality
reduction techniques to describe high dimensional observations from multi-agent tasks as low dimensional feature
vectors, reducing the learning complexity in the centralized

training scheme. Furthermore, the lower learning complexity
and higher sample efficiency provide new possibilities for
efficient MARL algorithms, including algorithms in both the
centralized training scheme and the decentralized training
scheme.
Summary. The non-stationarity, partial observability, and
coordination challenges may not significantly influence the
learning problem when methods utilize a centralized training
scheme. In contrast, the scalability of the methods becomes
increasingly important since the joint action space and the
joint observation space are widely used in model-based methods. To resolve the scalability problem, MATO and Tesseract
methods perform the planning and improvement in the latent
variable spaces or the original variable spaces with reduced
dimensions, making it feasible to apply centralized modelbased methods into scenarios with multiple agents.

4.2

Decentralized Training

Another approach of model-based multi-agent reinforcement
learning is to adopt the decentralized training scheme, where
the partial observability problem prevails. Additionally, the
non-stationarity and coordination problems become more difficult without access to global information. A natural way
to deal with these exacerbated problems is through information exchange between the agents, such as the communication
methods detailed as below.
Dyna-style. Compared to centralized Dyna-style methods,
an agent would need to predict other agents’ actions to generate model rollouts in decentralized multi-agent training. The
actions could be predicted by the learned opponent models or
requested from the correspondent agents through communication. AORPO [Zhang et al., 2021] investigates improving
the sample efficiency in stochastic games, under the setting
that agents improve their policies individually but with the
ability to communicate with each other. In AORPO, an agent
is trained using short rollouts using the decentralized modelbased MARL method. From the perspective of a single agent,
a return discrepancy upper bound between the expected cumulative reward obtained using AORPO and that obtained
with the model-free method is derived, proving the effectiveness of such model-based MARL methods. The theoretical
result also indicates the necessity of reducing the opponent
models’ generalization error. Since the communication cost
is one kind of opponent sample complexity that may increase
when reducing the opponent models’ generalization error,
AORPO balances between reducing the generalization error
and reducing the opponent sample complexity through the
adaptive opponent-wise rollout scheme, each opponent model
is used to generate model rollouts for certain number of steps,
where the number of steps is determined based on the opponent model’s validation error. Actions are requested from the
corresponding opponent through communication for the steps
afterward. Agents in AORPO learn joint Q functions and the
opponent models, alleviating the non-stationarity and coordination problems. Besides, improving policies using the data
generated from environment models that predicted joint actions of the agents helps resolve the non-stationarities. However, the opponent models and the joint Q functions require

full observability, which rarely holds for real-world applications.
Model Predictive Control. With decentralized training
scheme, MPC requires a learned environment model and possibly opponent models for an agent to perform decision-time
planning. In HPP [Wang et al., 2020], two agents are presented with a rendezvous task, where they must align their
goals without explicit communication. Each agent learns
goal-conditioned prediction models to predict the future observations of the agent itself and the other agent given only
and a goal
previous history of its own observation ot−h:t
i
g, essentially combining the dynamics model and the opponent models. The models are pretrained with supervised
learning on datasets collected with existing controllers. The
higher-level HPP planner then uses the cross-entropy method
[De Boer et al., 2005] to select the best goal for every Th
timesteps during execution after receiving the history of observations of all agents for these timesteps. During execution,
the high-level HPP planner updates its beliefs over potential
rendezvous points and adjusts the goal for the low-level controller accordingly, which leads to agents reaching a common
rendezvous. The fact that HPP plans over the goal space instead of the action space allows more effective planning due
to the lowered dimensionality of the goal space. It also helps
accelerate training because goals can be instructive throughout multiple timesteps.
To account for unseen or adapting agents,
MBOM [Yu et al., 2021] investigates the learning process of opponents by modeling it as recursively fine-tuning
pretrained opponent models. Each stage of fine-tuning is
done by calculating the best response of the fine-tuned model
of the previous stage. Specifically, before each interaction,
a MBOM agent generates M fine-tuned models {φ}M−1
i=0
PM−1
and executes a mixed policy πmix (·|s) = i=0 αi π(·|s; φi )
with Bayesian mixing. This decision-time planning, with
opponent models of varying reasoning levels, allows MBOM
to relax previous restrictions on opponent modeling and
potentially compete or coordinate with changing or unseen
opponents. Decision-time planning by recursive reasoning
also increases sample efficiency by generating high-quality
responses. However, MBOM has no guarantee about the
convergence of the recursive reasoning of opponents’ actions
and requires full observability.
Communication Methods. Communication methods benefit from environment models since the messages are thus
able to convey agents’ predictions about the long-term future
rather than the immediate partial observations, which facilitates agents to reach a common consensus. Communication
protocols can reduce dynamics sample complexity since the
agent can leverage the knowledge of the messages from other
agents, at the cost of increasing opponent complexity, i.e.,
the communication cost. MACI [Pretorius et al., 2020] leverages a communication protocol that allows agents to communicate with their neighbors for multiple rounds before deciding on a joint action. The message for each round encodes
each agent’s imagined rollouts generated using a recurrent
dynamics model which takes as input agent’s local observation and the incoming message from the neighbors. The pro-

posed communication protocol is differentiable and thus the
encoder, and the dynamics model can be learned with end-toend training. The dynamics model enables communication
for decision-time planning, which acts to alleviate the nonstationarity and partial observability problem since the communication allows agents to share their observations and policies implicitly. In terms of sample complexity, while multiple
communication rounds increase opponent sample complexity
to some degree, dynamics sample complexity is reduced because the multiple rounds of exchanging information accelerate the consensus-reaching process.
Similarly, Intention Sharing (IS) [Kim et al., 2021] proposes a communication protocol that also encodes into the
message an agent’s imagined trajectory of multiple steps.
From the perspective of agent i, the imagined trajectory is
generated by performing rollouts with the opponent model
that predicts other agents’ actions a−i based on its local ob′
servation oi , and a dynamics model P (oi |oi , ai , â−i ) which
−i
takes the predicted â as input. IS further incorporates an
attention module to focus on encoding the more important
parts of the imagined trajectory. Different from MACI, IS
utilizes both a dynamics model and opponent models to generate imagined rollouts. With opponent models, the dynamics
model takes as input the joint action instead of the action of
a single agent, further addressing the non-stationarity and coordination problems since interactions among agents are now
considered. The sample complexity is greatly affected by the
usefulness of the encoded message, which is why IS adopts
the attention module to focus on the more critical parts of the
trajectory.
Summary. In the decentralized training scheme, the nonstationarity, partial observability, and coordination problems
become more critical. In contrast, scalability is less concerned than that in the centralized training scheme. Opponent modeling is often introduced in decentralized modelbased methods to alleviate the non-stationarity and coordination problems to explicitly model the decision-making
process of other agents. In terms of model-based methods, opponent modeling makes Dyna-style and MPC methods possible in decentralized settings, which require predicting the state transition and other agents’ actions when
performing model rollouts. However, modeling opponents
in partially observable environments remains an open problem [Albrecht and Stone, 2018]. Additionally, under communication protocols that utilize the environment model, agents
can share their prediction of the long-term future instead of
only the intention at the current step, which assists agents
in effectively reaching a consensus. Effective communication protocols also help with increasing dynamics sample efficiency because a consensus is quicker to form under the effective exchange of long-term information.

5 Conclusions and Future Directions
This paper takes a review of the recent progress of theoretical analyses, algorithms, and applications in model-based
MARL. The classification of the algorithms is organized on
three dimensions, i.e., the training scheme, the opponent
awareness, and the environment usage. We discuss how

the learned environment models, possibly together with opponent models, are leveraged in detail and the influence of
these model-based methods on the inherent challenges of
multi-agent scenarios. Model-based MARL algorithms are
still under-researched: some fundamental problems of modelbased methods, including the model inaccuracy, the mismatch
between the environment model, the value function, and so
on, as well as multi-agent properties, for instance, the interaction among agents and the credit assignment problem in
fully cooperative settings, have not been extensively investigated. We summarize the pros and cons of the algorithms in
both centralized training and decentralized training schemes.
It is worth generalizing the techniques and principles in both
model-based RL and model-free MARL for the advantages
in sample efficiency and the possibility to resolve the inherent challenges in multi-agent scenarios. More specifically,
since model-based MARL is a new and promising branch of
MARL, there are many future directions in both the centralized training scheme and the decentralized training scheme.
We now provide several research directions that have the potential to push the future development of this field based on
our knowledge.
Scalability in Centralized Model-based MARL. The
scalability becomes a major challenge in centralized training model-based algorithms, which requires large amounts
of computational resources and increases the learning complexity in both learning and leveraging environment models.
The scalability problem limits the incorporation of effective
methods such as Dynamic Programming and Monte Carlo
Tree Search [Browne et al., 2012] into model-based MARL.
Reducing the joint variables’ dimensions shows its promise
for effective model learning and usage [Mahajan et al., 2021;
Krupnik et al., 2019].
Decentralized Model-based MARL with Opponent Modeling. Scenarios where agents improve their policies individually, are often characterized by partial observability,
which makes resolving non-stationarity and coordination
problems more difficult. Although suffering from the partial observability problem, opponent modeling is a natural
approach for alleviating these challenges and is necessary for
capturing the interaction among agents when modeling the
environment. In addition, the environment model’s learnability increases with opponent modeling since the learning objective could be decoupled into learning a dynamics
model with stabilized opponents and learning accurate opponent models.
Communication with Learned Models. Messages encoded with future trajectories predicted from environment
models outperform those encoded with only current-step information regarding capturing agents’ intentions and leading
agents to common consensuses. Extensions of techniques developed in communication-based MARL into model-based
MARL are worth investigating, such as automatic learning of
communication protocols and influence measurement among
agents.
We expect that this paper presents the advantages and potential of model-based MARL and reveals its opportunities
and limitations. In the foreseeable future, we hope that the

community could be inspired by this paper and encouraged
for further developments in this attractive, necessary, and
young field of research.

References
[Albrecht and Stone, 2018] Stefano V. Albrecht and Peter
Stone. Autonomous agents modelling other agents: A
comprehensive survey and open problems. Artif. Intell.,
2018.
[Arulkumaran et al., 2019] Kai Arulkumaran,
Antoine
Cully, and Julian Togelius. Alphastar: an evolutionary
computation perspective. In Proc. of GECCO, 2019.
[Bai and Jin, 2020] Yu Bai and Chi Jin. Provable self-play
algorithms for competitive reinforcement learning. In
Proc. of ICML, 2020.
[Bard et al., 2020] Nolan Bard, Jakob N. Foerster, Sarath
Chandar, Neil Burch, Marc Lanctot, H. Francis Song,
Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra,
Edward Hughes, Iain Dunning, Shibl Mourad, Hugo
Larochelle, Marc G. Bellemare, and Michael Bowling.
The hanabi challenge: A new frontier for AI research. Artif. Intell., 2020.
[Bargiacchi et al., 2021] Eugenio Bargiacchi, Timothy Verstraeten, and Diederik M. Roijers. Cooperative prioritized
sweeping. In AAMAS ’21: 20th International Conference
on Autonomous Agents and Multiagent Systems, Virtual
Event, United Kingdom, May 3-7, 2021, 2021.
[Brafman and Tennenholtz, 2000] Ronen I. Brafman and
Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning in certain classes of stochastic games.
Artif. Intell., 2000.
[Brafman and Tennenholtz, 2001] Ronen I. Brafman and
Moshe Tennenholtz. R-MAX - A general polynomial
time algorithm for near-optimal reinforcement learning. In
Proc. of IJCAI, 2001.
[Browne et al., 2012] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez,
Spyridon Samothrakis, and Simon Colton. A survey of
monte carlo tree search methods. IEEE Transactions on
Computational Intelligence and AI in games, 4(1):1–43,
2012.
[Chockalingam et al., 2018] Valliappa Chockalingam, Tegg
Tae Kyong Sung, Feryal Behbahani, Rishab Gargeya, Amlesh Sivanantham, and Aleksandra Malysheva. Extending world models for multi-agent reinforcement learning
in malmö. In Proc. of AAAI, 2018.
[De Boer et al., 2005] Pieter-Tjerk De Boer, Dirk P Kroese,
Shie Mannor, and Reuven Y Rubinstein. A tutorial on
the cross-entropy method. Annals of operations research,
2005.
[Du and Ding, 2021] Wei Du and Shifei Ding. A survey on
multi-agent deep reinforcement learning: from the perspective of challenges and applications. Artif. Intell. Rev.,
2021.

[Gronauer and Diepold, 2021] Sven Gronauer and Klaus
Diepold. Multi-agent deep reinforcement learning: a survey. AI, pages 1–49, 2021.
[Hernandez-Leal et al., 2020] Pablo Hernandez-Leal, Bilal
Kartal, and Matthew E. Taylor. A very condensed survey
and critique of multiagent deep reinforcement learning.
In Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An,
and Neil Yorke-Smith, editors, AAMAS 2020, pages 2146–
2148. International Foundation for Autonomous Agents
and Multiagent Systems, 2020.
[Hu et al., 2021] Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, and SW Liao. Rethinking the implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2102.03479, 2021.
[Janner et al., 2019] Michael Janner, Justin Fu, Marvin
Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In Proc. of NeurIPS,
2019.
[Kim et al., 2021] Woojun Kim, Jongeui Park, and
Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In Proc. of ICLR,
2021.
[Krupnik et al., 2019] Orr Krupnik, Igor Mordatch, and Aviv
Tamar. Multi-agent reinforcement learning with multistep generative models. In 3rd Annual Conference on
Robot Learning, CoRL 2019, Osaka, Japan, October 30
- November 1, 2019, Proceedings, 2019.
[Kurach et al., 2020] Karol Kurach, Anton Raichuk, Piotr
Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research
football: A novel reinforcement learning environment. In
Proc. of AAAI, 2020.
[Long et al., 2018] Pinxin Long, Tingxiang Fan, Xinyi Liao,
Wenxi Liu, Hao Zhang, and Jia Pan. Towards optimally
decentralized multi-robot collision avoidance via deep reinforcement learning. In Proc. of ICRA, 2018.
[Mahajan et al., 2021] Anuj Mahajan, Mikayel Samvelyan,
Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean
Kossaifi, Shimon Whiteson, Yuke Zhu, and Animashree
Anandkumar. Tesseract: Tensorised actors for multi-agent
reinforcement learning. In Proc. of ICML, 2021.
[Moerland et al., 2020] Thomas M. Moerland, Joost
Broekens, and Catholijn M. Jonker.
Model-based
reinforcement learning: A survey. CoRR, 2020.
[Nguyen et al., 2018] Thanh Thi Nguyen, Ngoc Duy
Nguyen, and Saeid Nahavandi. Deep reinforcement
learning for multi-agent systems: A review of challenges,
solutions and applications. CoRR, abs/1812.11794, 2018.
[Papoudakis et al., 2019] Georgios Papoudakis, Filippos
Christianos, Arrasy Rahman, and Stefano V. Albrecht.
Dealing with non-stationarity in multi-agent deep
reinforcement learning. CoRR, 2019.

[Park et al., 2019] Young Joon Park, Yoon Sang Cho, and
Seoung Bum Kim. Multi-agent reinforcement learning
with approximate model learning for competitive games.
PLOS One, 2019.
[Pásztor et al., 2021] Barna Pásztor, Ilija Bogunovic, and
Andreas Krause. Efficient model-based multi-agent meanfield reinforcement learning. CoRR, abs/2107.04050,
2021.
[Pretorius et al., 2020] Arnu Pretorius, Scott Cameron, Andries Petrus Smit, Elan van Biljon, Lawrence Francis,
Femi Azeez, Alexandre Laterre, and Karim Beguir. Learning to communicate through imagination with modelbased deep multi-agent reinforcement learning. 2020.
[Shapley, 1953] Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 1953.
[Subramanian et al., 2021] Jayakumar Subramanian, Amit
Sinha, and Aditya Mahajan. Robustness and sample complexity of model-based MARL for general-sum markov
games. CoRR, abs/2110.02355, 2021.
[Sun et al., 2019] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In
Conference on Learning Theory, COLT 2019, 25-28 June
2019, Phoenix, AZ, USA, 2019.
[Sutton and Barto, 1998] Richard S. Sutton and Andrew G.
Barto. Reinforcement learning - an introduction. MIT
Press, 1998.
[Vaart et al., 2021] Pascal Van Der Vaart, Anuj Mahajan, and
Shimon Whiteson. Model based multi-agent reinforcement learning with tensor decompositions. CoRR, 2021.
[Wang et al., 2019] Tingwu Wang, Xuchan Bao, Ignasi
Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois,
Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and
Jimmy Ba. Benchmarking model-based reinforcement
learning. CoRR, abs/1907.02057, 2019.
[Wang et al., 2020] Rose E. Wang, J. Chase Kew, Dennis
Lee, Tsang-Wei Edward Lee, Tingnan Zhang, Brian Ichter,
Jie Tan, and Aleksandra Faust. Model-based reinforcement learning for decentralized multiagent rendezvous. In
4th Conference on Robot Learning, CoRL 2020, 16-18
November 2020, Virtual Event / Cambridge, MA, USA,
2020.
[Wang et al., 2021] Jianhao Wang, Zhizhou Ren, Terry Liu,
Yang Yu, and Chongjie Zhang. QPLEX: duplex dueling
multi-agent q-learning. In Proc. of ICLR, 2021.
[Willemsen et al., 2021] Daniël Willemsen, Mario Coppola,
and Guido C. H. E. de Croon. MAMBPO: sample-efficient
multi-robot reinforcement learning using learned world
models. In Proc. of IROS, 2021.
[Yang et al., 2018] Yaodong Yang, Rui Luo, Minne Li, Ming
Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In Proc. of ICML, 2018.
[Ye et al., 2020] Deheng Ye, Zhao Liu, Mingfei Sun, Bei
Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,

Xipeng Wu, Qingwei Guo, Qiaobo Chen, Yinyuting Yin,
Hao Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei
Yang, and Lanxiao Huang. Mastering complex control in
MOBA games with deep reinforcement learning. In Proc.
of AAAI, 2020.
[Yu et al., 2021] Xiaopeng Yu, Jiechuan Jiang, Haobin
Jiang, and Zongqing Lu. Model-based opponent modeling. CoRR, abs/2108.01843, 2021.
[Zhang et al., 2019] Kaiqing Zhang, Zhuoran Yang, and
Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. CoRR, 2019.
[Zhang et al., 2020] Kaiqing Zhang, Sham M. Kakade,
Tamer Basar, and Lin F. Yang. Model-based multi-agent
RL in zero-sum markov games with near-optimal sample
complexity. In Proc. of NeurIPS, 2020.
[Zhang et al., 2021] Weinan Zhang, Xihuai Wang, Jian
Shen, and Ming Zhou. Model-based multi-agent policy optimization with adaptive opponent-wise rollouts. In
Proc. of IJCAI, 2021.
[Zhou et al., 2020] Ming Zhou, Jun Luo, Julian Villela,
Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang,
Montgomery Alban, Iman Fadakar, Zheng Chen, Aurora Chongxi Huang, Ying Wen, Kimia Hassanzadeh,
Daniel Graves, Dong Chen, Zhengbang Zhu, Nhat M.
Nguyen, Mohamed Elsayed, Kun Shao, Sanjeevan Ahilan, Baokuan Zhang, Jiannan Wu, Zhengang Fu, Kasra
Rezaee, Peyman Yadmellat, Mohsen Rohani, Nicolas Perez Nieves, Yihan Ni, Seyedershad Banijamali,
Alexander Imani Cowen-Rivers, Zheng Tian, Daniel
Palenicek, Haitham Bou-Ammar, Hongbo Zhang, Wulong Liu, Jianye Hao, and Jun Wang. SMARTS: scalable
multi-agent reinforcement learning training school for autonomous driving. CoRL, 2020.

