Here is the extracted information:

**PAPER:** Concept Learning for Cooperative Multi-Agent Reinforcement Learning
**TITLE:** Concept Learning for Cooperative Multi-Agent Reinforcement Learning
**ARXIV_ID:** 2507.20143v1
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** The paper proposes a novel value decomposition method called Concept-based Multi-agent Q-learning (CMQ), which integrates concept bottleneck learning into value decomposition to enhance interpretability and coordination in multi-agent reinforcement learning (MARL). CMQ represents each cooperation concept as a supervised vector and uses a concept predictor to generate a mixture of two global state semantics for each cooperation concept. The method also introduces explicit cooperation semantics, enabling test-time concept interventions to simulate how specific modes of coordination impact performance.
**KEY_CONTRIBUTIONS:**
* Proposes a novel value decomposition method, CMQ, which integrates concept bottleneck learning into value decomposition to enhance interpretability and coordination in MARL.
* Provides a test-time concept intervention to diagnose which cooperation concepts are incorrect or do not align with human experts.
* Achieves superior performance compared to state-of-the-art baselines on challenging MARL benchmarks, such as the StarCraft II micromanagement challenge and level-based foraging (LBF).
* Allows for an easy-to-understand interpretation of credit assignment and supports test-time concept interventions for detecting potential biases of cooperation mode and identifying spurious artifacts that impact cooperation.
