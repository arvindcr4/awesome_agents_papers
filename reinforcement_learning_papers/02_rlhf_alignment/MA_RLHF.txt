Published as a conference paper at ICLR 2025

MA-RLHF: R EINFORCEMENT L EARNING FROM H U MAN F EEDBACK WITH M ACRO ACTIONS

arXiv:2410.02743v2 [cs.CL] 14 Feb 2025

Yekun Chai‚àó Haoran Sun‚àó Huang Fang Shuohuan Wang Yu Sun Hua Wu
Baidu Inc.
{chaiyekun,fanghuang,wangshuohuan}@baidu.com
sunhaoran0402@gmail.com

A BSTRACT
Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long
sequences, where delayed rewards make it challenging for the model to discern
which actions contributed to preferred outcomes. This hinders learning efficiency
and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions ‚Äî sequences of tokens
or higher-level language constructs ‚Äî into the learning process. By operating
at higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment.
This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments
across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains
of up to 30% in text summarization and code generation, 18% in dialogue, and 8%
in question answering tasks. Notably, our approach reaches parity with vanilla
RLHF 1.7 ‚àº 2 times faster in terms of training time and continues to outperform it with further training. We make our code and data publicly available at
https://github.com/ernie-research/MA-RLHF.

1

I NTRODUCTION

Recent advancements in large language models (LLMs) have revolutionized natural language processing tasks, demonstrating impressive capabilities across a wide range of applications such as
code generation (Roziere et al., 2023; Chai et al., 2023; Lozhkov et al., 2024), mathematical reasoning (Lewkowycz et al., 2022; Anil et al., 2023), and dialogue assistance (OpenAI, 2023; Team
et al., 2023; Anthropic). Despite these successes, aligning LLMs with human values and preferences
remains a critical challenge. Reinforcement learning from human feedback (RLHF) has emerged as
a promising approach to address this alignment issue by incorporating human evaluations into the
training process (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020).
Existing RLHF (Ouyang et al., 2022; Bai et al., 2022; Askell et al., 2021) methods mainly optimize decisions at the level of individual tokens, and require to process a vast number of minute
adjustments. However, this fine-grained training paradigm can lead to the credit assignment problem (Kaelbling et al., 1996; Pang et al., 2019; Machado et al., 2023b; Pignatelli et al., 2023), particularly when dealing with long-distance dependencies. As LLM agents attempt to optimize decisions
across extensive sequences, the difficulty in attributing the credits of actions to specific tokens complicates the reinforcement learning (RL) process (Pignatelli et al., 2024). Moreover, the use of
subword tokenization, such as Byte-Pair Encoding (Sennrich et al., 2016), often splits words into
‚àó

Equal contribution. Correspondence to: YC.
Work done during HS‚Äôs internship at Baidu.

1

Published as a conference paper at ICLR 2025

smaller pieces. For instance, OpenAI‚Äôs ChatGPT1 treats each token as three quarters of a word on
average, resulting in sequences that are 33% longer than word counts (OpenAI, 2024) and further
exacerbates the credit assignment problem.
Additionally, standard RLHF methods may overlook essential local co-occurrence patterns or inherent structures between adjacent tokens in natural language. For example, consider the phrase
Big Apple2 , treating Big and Apple as isolated decisions misses the cohesive meaning of the
term, which actually refers to the ‚ÄúNew York City‚Äù. The token-level granularity of natural language
can hinder the agent‚Äôs ability to capture high-level language constructs in RL optimization, as some
sequences are better understood when evaluated holistically.
To address these challenges, we propose a new framework called macro-action RLHF (MA-RLHF)
that incorporate macro action ‚Äî sequences of tokens or high-level language constructs ‚Äî into the
RLHF framework. The concept of macro actions, has been explored in the literature of planning
(Iba, 1989; Korf, 1985; Sacerdoti, 1974) and reinforcement learning (Thrun & Schwartz, 1994;
Precup et al., 1997; Hauskrecht et al., 2013), simplifies decision-making by operating at high levels
of temporal abstraction under the framework of semi-Markov Decision Processes (SMDPs) (Sutton
et al., 1999b). Macro actions leverage temporal abstraction by chunking the sequences and reducing
the decision resolution, enabling the agent to learn from ‚Äúlong-sighted‚Äù macro-level actions instead
of ‚Äúshort-sighted‚Äù token-level actions. This can potentially lead to improved learning efficiency
and scalability. Alternatively, MA-RLHF can also be interpreted from the perspective of reversing
tokenization; MA-RLHF serves as a de-tokenization process to reconstruct high-level language units
from subword pieces. By merging tokens into macro actions, we reduce the number of decision
points and shorten decision trajectories, alleviating the credit assignment problem caused by long
temporal distances.
To conclude, our main contributions are as follows:
‚Ä¢ We propose MA-RLHF, a simple yet effective RLHF framework that integrates the macro actions into RLHF to align LLMs with human preference. We demonstrate the effectiveness of
our approach through extensive experiments across various datasets and tasks, including text
summarization, dialogue generation, question answering, and code generation.
‚Ä¢ We show that MA-RLHF achieves 1.7√ó to 2√ó faster learning efficiency in reward scores during
training compared to the standard token-level RLHF, without introducing additional computational costs during training or inference. MA-RLHF also exhibits strong scalability across
model sizes ranging from 2B to 27B parameters.
‚Ä¢ Our analysis reveals that MA-RLHF exhibits robust generalization capabilities under varying
experimental settings, such as temperature values and rejection sampling, consistently outperforms the standard RLHF approaches.

2

P RELIMINARIES

We introduce some basic concepts and notations used in RL and RLHF.
2.1

R EINFORCEMENT L EARNING AND P OLICY O PTIMIZATION

Problem Definition RL addresses the problem of finding a policy to make optimal sequential decisions in environments modeled as a Markov Decision Process (MDP) (Sutton & Barto, 1999).
An MDP is defined by the tuple (S, A, P, r, œÅ0 , Œ≥), where S denotes a finite set of states, A is a
finite set of actions, P : S √ó A √ó S ‚Üí [0, 1] represents the state transition probability distribution,
r : S √ó A ‚Üí R is the reward function, œÅ0 : S ‚Üí [0, 1] defines the initial state distribution, and
Œ≥ ‚àà (0, 1) is the discount factor that determines the importance of future rewards.
Given a trajectory (s0 , a0 , s1 , a1 , ¬∑ ¬∑ ¬∑ ), a reward rt =P
r(st , at ) is received at each time t. The
‚àû
l
state-action value function QœÄ (st , at ) = Est+1 ,at+1 ,...
l=0 Œ≥ rt+l measures the expected return
of taking action
P‚àû at l at state
 st and following policy œÄ thereafter. The value function VœÄ (st ) =
Eat ,st+1 ,...
Œ≥
r
estimates the expected return from state st under the policy œÄ. The adt+l
l=0
vantage function AœÄ (st , at ) = QœÄ (st , at ) ‚àí VœÄ (st ) reflects the relative value of taking action at at
state st compared to the average value of the state.
1
2

https://platform.openai.com/tokenizer
https://en.wikipedia.org/wiki/Big_Apple

2

Published as a conference paper at ICLR 2025

The goal of RL is to find an optimal policy œÄŒ∏ (a | s), parameterized
by Œ∏, that maximizes the exP‚àû
pected cumulative discounted reward: J(Œ∏) = Es0 ,a0 ,... [ t=0 Œ≥ t rt ], where s0 ‚àº œÅ0 (s0 ) represents
the initial state distribution, at ‚àº œÄŒ∏ (at | st ) denotes the action selection based on the policy, and
st+1 ‚àº P (st+1 | st , at ) specifies the state transition dynamics.
Proximal Policy Optimization Policy gradient methods are a common approach for optimizing
policies by estimating the gradient of a performance
P‚àûobjective with respect to the policy parameters
Œ∏. The policy gradient is given by: ‚àáŒ∏ J(Œ∏) = E [ t=0 At ‚àáŒ∏ log œÄŒ∏ (at | st )], where the expectation
E[¬∑] is taken over the randomness of the initial state, policy, and state-transition. The policy gradient
guides us how to adjust the policy parameters to improve the expected return. Among the family
of policy gradient methods, Proximal Policy Optimization (Schulman et al., 2017, PPO) is perhaps
the most widely-used one due to its simplicity and empirical effectiveness. PPO simplifies TRPO
(Schulman et al., 2015) by using a clipped surrogate objective function to penalize large deviations
from the old policy, thereby ensuring more stable updates. Specifically, PPO introduces a clipped
objective function:



œÄŒ∏ (at | st )
œÄŒ∏ (at | st )
ppo-clip
J
(Œ∏) = Et min
At , clip(
, 1 ‚àí œµ, 1 + œµ)At ,
(1)
œÄŒ∏old (at | st )
œÄŒ∏old (at | st )
where œµ is a hyperparameter that defines the range for clipping. The expectation Et [. . . ] indicates the
empirical average over a finite batch of samples. Nowadays, PPO usually comes as the first choice
for RL practitioners.
2.2

RLHF FOR H UMAN A LIGNMENT

The post-training of LLMs (Stiennon et al., 2020; Ouyang et al., 2022) is a multi-stage training
paradigm to align LLMs with human preferences. Post-training typically involves three stages:
(1) Supervised Fine-Tuning (SFT) stage: A pre-trained language model (LM) is fine-tuned on a
dataset of human demonstrations, learning to generate responses that align with human instructions
and preferences.
(2) Reward Modeling (RM) stage: A reward model is trained on a labeled preference dataset D =
N
(xi , yi+ , yi‚àí )i=1 , consisting of prompts xi and pairs of responses (yi+ , yi‚àí ), where yi+ is preferred
‚àí
over yi by human annotators. The reward model rœï (x, y), parameterized by œï, is trained using the
ranking loss: LRM = ‚àí log œÉ(log(rœï (x, y+ ) ‚àí rœï (x, y‚àí ))), where œÉ denotes the sigmoid function.
(3) RLHF stage: The RL fine-tuning utilizes the RM to provide feedback on the generated outputs,
optimizing the policy using RL methods such as PPO. The reward signal is modified by incorporating a Kullback-Leibler (KL) divergence penalty to balance the exploration of new policies with
adherence to the SFT model. The reshaped reward is defined as:
R(x, y) = rœï (x, y) ‚àí Œ≤DKL (œÄŒ∏ (¬∑ | x) ‚à• œÄsft (¬∑ | x)),
where œÄŒ∏ represents the policy learned through RL, œÄsft is the policy produced from the SFT stage,
and Œ≤ > 0 is a hyperparameter that controls the strength of the KL penalty.
In the RLHF stage, the PPO algorithm, as detailed in Equation (1), is employed to optimize the RL
policy. In the context of RLHF, we denote the state st = {s0 , a0 , a1 , . . . , at‚àí1 } as the sequence
of tokens generated up to time step t, while s0 represents the initial states, i.e., the prompt, and at
represents the token selected at the t-th position.

3

M ARCO -ACTION RLHF

3.1

R EVISITING M ACRO ACTIONS (O PTIONS )

Macro actions, also referred to as options (Sutton et al., 1999b), are high-level constructs that
encapsulate a sequence of primitive actions (i.e., subword tokens); by its definition, macro actions
allows an agent to operate at a coarser temporal scale.
Formally, a macro action is characterized by three components: (1) a policy œÄ : S √ó A ‚Üí [0, 1]
which guides the action selection among actions; (2) a termination condition Œ∂ : S + ‚Üí [0, 1], which
determines where the macro action should end; (3) a initiation set I ‚äÜ S, which is a subset of states
3

Published as a conference paper at ICLR 2025

Vanilla RLHF
Time

MA-RLHF

ùëé$%

‚Ä¶

‚Ä¶

MDP

Macro actions
over MDP

ùúî$!
Optimize

PPO

ùë°!

MA-PPO

ùë°!"#

ùëÖ(‚ãÖ)
reward

ùëé%

|ùúî! |
action

Reward
Model

ùúî!

RM

macro action

END

end of the trajectory

episode

‚Ä¶

‚Ä¶

Figure 1: Illustration of the MA-RLHF optimization framework. Standard RLHF makes decisions
and evaluates value scores at the token level, while MA-RLHF makes decisions over sequences of
tokens at a coarser temporal scale.
that macro actions can begin with. Once initiated with a state s0 ‚àà I, the macro action follows policy
œÄ until it reaches the termination condition according to Œ∂. Intuitively, the use of carefully designed
macro actions can extend decision-making temporally, it allows the agent to avoid ‚Äúshort-sighted‚Äù
token-level decisions and encourage ‚Äúlong-sighted‚Äù macro-level decisions, thereby simplifies the
decision-making process and potentially enhances learning efficiency.
3.2

RLHF WITH M ACRO ACTIONS

We describe how we integrate macro-actions into the existing RLHF framework, the resulting framework is named as macro-action RLHF (MA-RLHF).
3.2.1

F ORMALIZATION OF M ACRO ACTIONS

We denote macro actions as œâ1 , œâ2 , . . . , œâœÑ . In the context of LLMs, a macro action œâœÑ consists of a
sequence of consecutive tokens, i.e., œâœÑ = {atœÑ , atœÑ +1 , . . . , atœÑ +1 ‚àí1 }, where tœÑ is the starting index
of the œÑ -th macro action. We let |œâœÑ | denotes the number of primitive actions that œâœÑ contains.
Unless otherwise specified, we use œÑ to index macro actions/states and use t to index primitive
actions/states.
As mentioned in ¬ß3.1, macro actions are defined by the policy model, the termination condition
and the initiation set. In MA-RLHF, we set the policy model the same as the standard token-level
RLHF and let the initiation set to be any possible sequence of tokens. Therefore, the macro action
used in MA-RLHF is decided solely by the termination condition, which plays a crucial rule in the
MA-RLHF framework. We explore three termination conditions in this work:
‚Ä¢ n-gram based termination: Following Vezhnevets et al. (2016), we find that n-grams serve as a
simple yet effective termination condition for macro actions, i.e., |œâœÑ | = n, where n represents the
length of the n-gram. We consider two variants of the n-gram termination condition: (a) Fixed ngram: We group tokens into fixed-length n-grams, simplifying the action space while maintaining
common linguistic patterns. We empirically find fixed n-gram macro action perform best and use
it as the default setup. (b) Randomized n-gram: We randomly select the length of a n-gram from
a predefined list of lengths n ‚àà {2, 3, 5, 10} to introduce variability, allowing the policy to adapt
to different sequence lengths.
‚Ä¢ Parsing-based termination: œâœÑ is derived from syntactic or semantic parsing of the input text,
aligning macro actions with grammatical structures like phrases or clauses. Concretely, we traverse the constituent tree of the entire sequence using depth-first search (DFS), expanding nonterminal nodes until current non-terminal state contains no more than a specified threshold of leaf
tokens, set at C = 5.
‚Ä¢ Perplexity-based (PPL) termination: Perplexity measures the likelihood of a sequence of tokens. Here, the perplexity of a macro
P action is proportional to the averaged entropy of the token
within it, i.e., ppl(œâœÑ ) ‚àù ‚àí |œâ1œÑ | a‚ààœâœÑ log pa . A macro action terminates until it reaches a
token that has negative impact on the perplexity of the macro action. Mathematically, we construct œâœÑ = {atœÑ , . . . , atœÑ +1 ‚àí1 } such that ppl(œâœÑ ‚à™ atœÑ +1 ) > ppl(œâœÑ ) and ppl({atœÑ , . . . , ai }) ‚â•
ppl({atœÑ , . . . , ai+1 }) for all tœÑ ‚â§ i ‚â§ tœÑ +1 ‚àí 2.
4

Published as a conference paper at ICLR 2025

After determining the macro action based on the termination condition, we apply the state value
function and importance sampling at the macro level Equation (1). We provide the details of implementation in Appendix D.1.
3.2.2

P OLICY O PTIMIZATION WITH M ACRO ACTIONS

In MA-RLHF, we adapt the PPO algorithm for optimization, referred to as MA-PPO. In the context
of LLMs, expanding the action space with additional macro actions/tokens results in re-architecting
the LLM‚Äôs vocabulary and retraining the model, which is computationally prohibitive. Thus, we
maintain the original action space as pretrained LLMs, which can be treated as ‚Äúsingle-step‚Äù primitive options as noted in (Sutton et al., 1999b). The policy œÄŒ∏ still outputs probabilities over
individual tokens,
for optimization, we consider the joint probability of the macro action:
QtœÑbut
+1
œÄ
œÄŒ∏ (œâœÑ | sœÑ ) = t=t
Œ∏ (at | a<t ). The macro reward for executing the macro action œâœÑ at the
œÑ

 P|œâœÑ |‚àí1 i
œÅ rtœÑ +i sœÑ , where rt is the reward received at
macro time step œÑ is defined as: RœÑ = E
i=0
time step t, and we set the discount factor œÅ = 1 in our experiments.
Each macro action represents a contiguous sequence of tokens, and is treated as an option in the
SMDP framework. The option-level value function with macro action is then estimated as:


V œÄ (sœÑ , œâœÑ ) = E RœÑ + Œ≥V œÄ (stœÑ +1 ) sœÑ , œâœÑ ,
where Œ≥ is the discount factor for future rewards beyond the macro action.
The advantage function AœÄ (sœÑ , œâœÑ ) in MA-PPO determines how much the chosen macro action
outperforms the average, which is defined as AœÄ (sœÑ , œâœÑ ) = QœÄ (sœÑ , œâœÑ ) ‚àí V œÄ (sœÑ ). Similar to
the definition stated in ¬ß2, QœÄ (sœÑ , œâœÑ ) is the expected return conditioned on executing œâœÑ at state
sœÑ , which is calculated by summing the immediate macro rewards from the macro action with the
discounted value of the subsequent state.
In MA-PPO, the objective function is adapted for MA-level evaluation. The policy gradient is
computed based on the advantage of the MA sequences:
"
! !#


œÄŒ∏ œâœÑ | sœÑ
œÄŒ∏ œâœÑ | sœÑ
MA-PPO
 AÃÇœÑ , clip
 , 1 ‚àí œµ, 1 + œµ AÃÇœÑ
L
(Œ∏) = EœÑ min
,
œÄŒ∏old œâœÑ | sœÑ
œÄŒ∏old œâœÑ | sœÑ
where AÃÇœÑ is the estimated advantage at macro time step œÑ , œµ is a constant that defines the range for
clipping, and œÄŒ∏old is the policy before the update.
3.2.3

C ONNECTION TO P REVIOUS M ETHODS

MA-RLHF builds on and generalizes prior work in the RLHF literature by varying the length of
macro actions. When the macro action length is set to 1, MA-RLHF reduces to the standard tokenlevel RLHF (Stiennon et al., 2020; Ouyang et al., 2022), operating as an MDP. Conversely, if we
allow |œâœÑ | ‚Üí ‚àû, then MA-RLHF converges toward methods like RLOO (Ahmadian et al., 2024),
REINFORCE (Williams, 1992; Sutton et al., 1999a), and GRPO (Shao et al., 2024), approximating
a contextual bandit problem where decisions are made based on the entire sequence context. By
varying the length of macro actions |œâœÑ |, MA-RLHF provides a flexible framework that balances
the granularity of action decisions. We provide further analysis on the impact of |œâœÑ | in ¬ß4.3.

4

E XPERIMENTS

4.1

E XPERIMENTAL S ETTINGS

Tasks and Datasets We evaluate MA-RLHF on three different datasets for open-ended generation
tasks: TL;DR (Stiennon et al., 2020) dataset for text summarization, Anthropic Helpful and Harmless (HH-RLHF) (Bai et al., 2022) for dialogue generation3 , and WebGPT Comparison (Nakano
et al., 2021) for question answering. Additionally, we evaluate MA-RLHF on code generation using
the APPS (Hendrycks et al., 2021) dataset. More details can be found in Appendix B.1.
3

https://huggingface.co/datasets/Dahoas/full-hh-rlhf

5

Published as a conference paper at ICLR 2025

Vanilla PPO
MA-PPO

10.0%

30%

1.9√ó

68%

Percentage

2.2√ó

7.5%
5.0%
2.5%
0.0%

3

2

1

0

RM Score

1

2

Figure 2: Test RM scores of Gemma-2B and Gemma-7B models Figure 3: RM score distribution
on the TL;DR dataset. The shaded regions represent the standard for PPO and MA-PPO (2B) at fideviation on test RM scores across training runs.
nal steps (4.6k) on TL;DR.

0

20

Tie

Loss
22%

86%

14%

74%

10%16%

69%

10% 21%

40

60

% Win Rate

80

100

Win

GPT-4 Evaluation
Gemma-2B
GPT-4 Evaluation
Gemma-7B
Human Evaluation
Gemma-2B
Human Evaluation
Gemma-7B
0

58%

Tie
Loss
4% 38%

72%

2% 26%

52%

20% 28%

56%

24% 20%

20

40

60

% Win Rate

80

100

MA-PPO v.s. Vanilla PPO 2B

Win Rate %

78%

60
40

MA-PPO v.s. Vanilla PPO 7B

Win
Tie
Loss

20
0

F
Accacuta
ura l
c
Coh y
ere
nce
UseOver
rfuln all
e
Hu ss
Evaman
l.
F
Accacuta
ura l
c
Coh y
ere
nce
UseOver
rfuln all
e
Hu ss
Evaman
l.

Win

GPT-4 Evaluation
Gemma-2B
GPT-4 Evaluation
Gemma-7B
Human Evaluation
Gemma-2B
Human Evaluation
Gemma-7B

Figure 4: Win rates of MA-PPO against vanilla PPO on TL;DR (left), HH-RLHF (middle) and
WebGPT Comparisons (right), estimated by GPT-4 and Human.
Base Models and Training Details For open-ended generation tasks, we use pre-trained Gemma2B (Team et al., 2024) as our base model; we further adopt Gemma-7B and Gemma-2-27B to test
the scaling trend. For the program synthesis task, we use CodeGemma-1.1-2B and CodeGemma1.1-7B-it as our base models. The data split for SFT / RM / PPO and the hyperparameters used in
SFT / RM / PPO stages are detailed in Appendix B.2. The implementation details of MA-PPO can
be found in Appendix E.
Evaluation For open-ended generation tasks, our evaluation metrics includes RM scores, GPT-4
pairwise evaluation, and human pairwise evaluation. To compute the RM score, we randomly sample 2k validation instances for the TL;DR and HH-RLHF datasets and use the default validation set
of the WebGPT dataset. For GPT-4 and human evaluations, we simulate the win-rate on 50 instances
that are drawn from the instances used in the RM evaluation. The GPT-4 and human evaluations are
based on task-specific criterion: relevance, coherence, consistency, and fluency for TL;DR; helpfulness for HH-RLHF; factual accuracy, coherence, and usefulness for WebGPT. We followed prior
studies (Askell et al., 2021; Zheng et al., 2024) by randomizing the order of responses during evaluation to mitigating potential evaluation biases. The prompts used by the GPT-4 evaluation are placed
in Appendix F.1, and the annotation rules used for human evaluation are given in Appendix F.2. For
the program synthesis task, we utilize pass@1 and pass@5 metrics to assess the performance of the
model, evaluated on the provided 5k test set.
4.2

M AIN R ESULTS

In this section, we present the main results of applying MA-PPO across three key tasks: summarization, dialogue, and question answering. The main takeaway is that MA-PPO consistently outperforms vanilla PPO in terms of both training efficiency and generation quality; MA-PPO obtains
a significant improvement in testing reward model scores and human/GPT-4 evaluation win rates.
TL;DR Summarization For the TL;DR summarization task, MA-PPO shows a marked improvement over vanilla PPO. As shown in Figure 2, MA-PPO achieves parity with vanilla PPO approximately 1.7 ‚Äì 2 times faster during training. Specifically, Gemma-2B trained with 1.7k MA-PPO
updates reaches similar testing RM scores obtained by vanilla PPO trained with 3.7k steps. We
also find similar trends when scaling up the parameter sizes to 7B, demonstrating the generalized
capability of MA-PPO on model sizes.
Moreover, Figure 3 highlights the distribution of RM scores, where MA-PPO consistently shifts
towards higher RM sores compared to vanilla PPO. Further evaluation using GPT-4, given in the left
figure of Figure 4, shows that MA-PPO achieves 78% and 86% win rate over vanilla PPO for the 2B
and 7B models, respectively. Human evaluation gives similar results, where MA-PPO obtains win
rates of 74% and 69%, further demonstrating the effectiveness of macro actions. The final testing
RM scores of MA-PPO and vanilla PPO are given in Table 2.
6

Published as a conference paper at ICLR 2025

Table 1: Agreement among RM, GPT4, and human evaluations on TL;DR.
RM

GPT-4

Human

2B

100%
78%
76%

100%
58%

100%

RM
GPT-4
Human

7B

100%
78%
74%

100%
64%

100%

1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.25
0.50

Model

TL;DR

HH-RLHF

WebGPT

Vanilla PPO (2B)
MA-PPO (2B)

0.84
1.41+68%

1.31
1.55+18%

-0.62
-0.60+3%

Vanilla PPO (7B)
MA-PPO (7B)

1.90
2.47+30%

1.05
1.24+18%

-0.61
-0.56+8%

7

Vanilla PPO
MA-PPO (Fixed)
MA-PPO (PPL)

MA-PPO (Randomized)
MA-PPO (Parsing)

6

GPT-4 Score

RM score

#Param
RM
GPT-4
Human

Table 2: Test RM scores of vanilla PPO and MA-PPO
on TL;DR, HH-RLHF, and WebGPT datasets.

Vanilla PPO
MA-PPO (Fixed)
MA-PPO (PPL)
MA-PPO (Randomized)
MA-PPO (Parsing)
reference
0

1000

2000

3000

Training step

5
4

4000

3

Relevance Coherence Consistency Fluency
Figure 5: Performance of MA-PPO with various macro action termination strategies on the TL;DR
dataset using Gemma-2B. Left: Test RM scores for different termination strategies. Right: GPT4 evaluation across four dimensions ‚Äì relevance, coherence, consistency, and fluency ‚Äì comparing
different MA termination methods.
HH-RLHF Dialogue We use the HH-RLHF dataset to evaluate the helpfulness and harmlessness
of single-turn dialogues. MA-PPO shows clear advantages over vanilla PPO, as depicted in the
middle figure of Figure 4. GPT-4 evaluations show that MA-PPO yields a 72% win rate for the
Gemma-7B model, compared to 58% for the Gemma-2B model. Human evaluation results align
with these findings, with the win rate increasing from 52% to 56% as model size scales from 2B
to 7B. The testing RM score of MA-PPO and vanilla PPO are presented in Table 2. These results
highlight the scalability and effectiveness of MA-PPO in dialogue tasks. We refer to Appendix C.1
for detailed experimental results.
WebGPT Comparisons We evaluate MA-PPO on the WebGPT Comparison dataset for questionanswering tasks. As shown in Figure 4 (Right), MA-PPO consistently outperforms vanilla PPO, with
GPT-4 evaluations yielding a win rate of 64% for the Gemma-7B model. This result demonstrate
the robustness of MA-PPO across different tasks, including more structured tasks like question
answering. More experimental details refer to Appendix C.2.
Validating Model-based Judgments with Human Evaluation We evaluate the reliability of our
evaluation methods by calculating the agreement between the reward model, GPT-4, and human
evaluators. Since GPT-4 and human evaluations are conducted pairwise, we determine the reward
model‚Äôs win rate by selecting the summary with the higher RM score. The results, shown in Table 1,
demonstrate that the reward model aligns more closely with both GPT-4 and human evaluations.
Furthermore, the agreement between GPT-4 and human evaluators averaged 62% across models,
reinforcing the consistency and validity of our evaluation framework.
4.3

A NALYZING THE U SE OF M ACRO ACTIONS

We study the performance of various termination strategies. Unless otherwise specified, we conduct
our analysis on the TL;DR dataset.
4.3.1 E XPLORING D IFFERENT S TRATEGIES FOR MA T ERMINATION (Œ∂)
In MA-RLHF, the termination condition (Œ∂) for macro actions is critical as it determines when a
macro action should conclude. We compare the performance of various termination strategies, particularly on reward maximization and linguistic coherence. The termination strategies studied in
this section including fixed / randomized n-gram-based, parsing-based, and perplexity-based termination, as aforementioned in ¬ß3.2.1; please see Figure 12 for detailed illustration.
Figure 5 illustrates the overall test-set performance on RM scores (Left) and GPT-4 evaluation scores
(Right) with different MA termination strategies. All macro action termination strategies outperform
the vanilla PPO approach, underscoring the importance of temporal abstraction in decision-making.
Figure 5 (Left) shows that n-gram based approach, both fixed and randomized, achieves the opti7

0

1000

2000

3000

Training step

7

Vanilla PPO
MA-PPO (n = 3)
MA-PPO (n = 5)
MA-PPO (n = 10)
MA-PPO (n = )
reference
0

4000

1000

2000

3000

4000

Training step

0.2

T=0.2
T=0.4
T=0.6

0.0
4

8

16

T=0.8
T=1.0
T=1.2
32

1.45
1.40
1.35
1.30
1.25
1.20
1.15
1.10

RM Score

RM Score

0.4

MA-PPO (n = 10)
MA-PPO (n = )

5
4

5000

3

Figure 6: Test RM scores of different n values in MA-PPO
evaluated by corresponding RM on the TL;DR (left) and HHRLHF (right) dataset.
SFT
PPO
0.8
0.6

Vanilla PPO
MA-PPO (n = 3)
MA-PPO (n = 5)

6

GPT-4 Score

Vanilla PPO
MA-PPO (n = 3)
MA-PPO (n = 5)
MA-PPO (n = 10)
MA-PPO (n = )
reference

1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

T=0.2
T=0.4
T=0.6
4

8

16

T=0.8
T=1.0
T=1.2
32

Relevance Coherence Consistency Fluency

Figure 7: GPT-4 scores of
vanilla PPO and MA-PPO with
different n values on TL;DR.
1.85
1.80
1.75
1.70
1.65
1.60
1.55
1.50

MA-PPO

RM Score

1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.25
0.50

RM score

RM score

Published as a conference paper at ICLR 2025

T=0.2
T=0.4
T=0.6
4

8

16

T=0.8
T=1.0
T=1.2
32

Best-of-N
Best-of-N
Best-of-N
Figure 8: The effect of temperature on RM scores for varying sample sizes (Best-of-N ) across
models. (Left): RM score of the SFT model under different temperatures and sample sizes. (Mid):
RM score of vanilla PPO under the same settings. (Right): RM score of MA-PPO.

mal results among others. Notably, randomized n-gram-based termination performs the best across
multiple dimensions, including relevance, coherence, and consistency, as shown in Figure 5 (Right).
As expected, the perplexity-based termination enhances fluency, and is most suited for tasks that
prioritize smooth and natural language generation. Furthermore, parsing-based termination shows
promising ability to handle complex grammar, as it is designed to better capture linguistic structures.
4.3.2

A BLATION S TUDY: VARYING n IN MA-RLHF

The n-gram based macro action strategy in MA-RLHF uses a hyper-parameter n to control the
length of macro actions. Notably, when n = 1, MA-PPO is equivalent to vanilla PPO, and treats
the problem as a traditional Markov Decision Process (MDP), making decisions token by token. In
contrast, setting n ‚Üí ‚àû corresponds to the REINFORCE algorithm (McGovern & Sutton, 1998),
where the entire sequence is treated as a single macro action, akin to a contextual bandit problem, as
discussed in ¬ß 3.2.3. For intermediate values of n (i.e., n ‚àà (1, ‚àû)), MA-PPO falls under the SMDP
framework, which allows for temporally extended actions; see ¬ß3. This continuum between MDPs
and contextual bandits highlights the flexibility of the MA-RLHF approach in handling varying
levels of temporal abstraction.
RM Scores We conducted experiments with varying values of n (n ‚àà {3, 5, 10, ‚àû}) on the TL;DR
and HH-RLHF datasets. Figure 6 shows that all values of n lead to performance improvements
over the vanilla PPO (n = 1), indicating the advantage of modeling sequences of tokens as macro
actions. Notably, for the TL;DR dataset, n = ‚àû yields the highest RM score, suggesting that
treating the entire sequence as a macro action is particularly effective for the summarization task.
For the HH-RLHF dataset, setting n = 10 gives the best performance, likely because this task
benefits from moderate-length macro actions that can capture essential linguistic structures while
maintaining sufficient granularity.
GPT-4 Evaluation Analysis As shown in Figure 7, setting n = 5 strikes a good balance between
relevance, coherence, consistency; it outperforms both smaller and larger values of n. These findings
align with the semi-MDP framework: increasing n allows for better credit assignment and context
retention, but excessive abstraction (e.g., n = ‚àû) sacrifices fine-grained control. Overall, moderate
values of n = 5 and n = 10 provide the best trade-offs, highlighting the adaptability across tasks.
4.4 G ENERALIZATION P ROBING IN M ACRO ACTIONS
Robustness on Rejection Sampling vs. Temperature Best-of-N (a.k.a, rejection sampling) (Touvron et al., 2023) enhances response quality by selecting the highest-reward response from N
samples generated by the policy model. We compare MA-PPO, SFT, and vanilla PPO using the
best-of-N sampling across various temperatures T ‚àà {0.2, 0.4, 0.6, 0.8, 1.0, 1.2} and sample sizes
N ‚àà {4, 8, 16, 32}. As shown in Figure 8, best-of-N sampling improves RM scores for all methods,
8

Published as a conference paper at ICLR 2025

Vanilla PPO
MA-PPO

Percentage

13%

1.9√ó

6.2%
5.0%
3.8%
2.5%
1.2%
0.0%

2

1

0

RM Score

5

Reward Score

7.5%

1

2

RM Score

3

4

Vanilla PPO
MA-PPO

4

50
40

3

30

2

20

1

10

0

5

GPT-4 Eval. Human Eval.

Win Rate

8.8%

2B 7B 27B 2B 7B 27B 2B 7B 27B

0

Figure 9: Evaluation results for vanilla PPO and MA-PPO on Gemma-2-27B using the TL;DR
dataset. Left: RM scores on validation set. Mid: Distribution of RM scores for vanilla PPO and
MA-PPO (27B) at final steps (4.6k). Right: Scaling trending on TL;DR dataset across 2B, 7B, and
27B model size, showing RM scores, GPT-4 evaluation, human evaluation results.

0.8

BoN RM Score

0.6

0.6

0.4

0.2
0.0

0.8

0.6

0.4

0.4

0.2

0.8

1.0

0.0

0.2

0.4

0.6

SFT RM Score

0.8

1.0

0.0

0.2

0.4

0.6

SFT RM Score

0.8

1.0

0.8
0.6
0.4
0.2
0.0

0.0

0.2

0.4

0.6

SFT RM Score

0.8

1.0

0
200
10
0

0.6

0.0

0
200
10
0

0.4

SFT RM Score

0.2

0
200
10
0

0.2

0
200
10
0

0.0

0.0

200
100
0
1.0

MA-PPO (n=‚àû) RM Score

0.8

200
100
0
1.0

MA-PPO (n=5) RM Score

200
100
0
1.0

Vanilla PPO RM Score

200
100
0
1.0

Figure 10: RM score shifting pattern after RLHF training; Left: RM scores of best-of-N (N = 8)
sampling compared to the SFT model. Mid Left: RM scores of vanilla PPO compared to the SFT
model. Mid Right: RM scores of MA-PPO (n = 5) compared to the SFT model. Right: RM scores
of MA-PPO (n = ‚àû) compared to the SFT model.
with performance increasing as N grows. We observe that SFT and vanilla PPO are sensitive to
temperature variations, requiring specific adjustments to achieve optimal results. In contrast, MAPPO demonstrates robustness in sampling temperature, it consistently delivers the best performance
at T = 1.2 and shows consistent improvement across all tested temperatures. Moreover, MA-PPO
maintains stable performance across varying temperature settings, as detailed in Appendix D.4,
highlighting its robustness and generalization capabilities under different sampling temperatures.
Scaling Trends up to 27B Models We evaluate the performance of MA-PPO across different model
sizes, specifically Gemma-2B, 7B, and 27B. As demonstrated in Figure 9 (Left and Mid), MAPPO consistently surpasses vanilla PPO, exhibiting higher RM scores throughout training. Figure 9
(Right) presents the scaling trend of MA-PPO across the 2B, 7B, and 27B models in terms of testing
RM scores, GPT-4, and human evaluations. The experimental results underscore the scalability and
robust performance of MA-PPO across varying model sizes.
Analyzing the Impact on RM Score Distribution We evaluate the RM score distribution shift after
applying RLHF using vanilla PPO and MA-PPO on the TL;DR dataset, with the SFT model serving
as the baseline. To further contextualize the impact of RLHF, we include the Best-of-N sampling
(N = 8) on the SFT model. As illustrated in Figure 10, Best-of-N enhances overall response quality
but falls short compared to RLHF. While vanilla PPO shifts the distribution towards higher RM
scores, it leaves a significant number of low-quality, long-tailed instances. In contrast, MA-PPO
demonstrates a more pronounced positive impact, effectively reduces the number of low-quality
outliers and improves overall score distribution compared with the vanilla PPO. This highlights the
robustness of MA-PPO in enhancing response quality through RLHF.
4.5

A DDITIONAL A NALYSIS

Impact on L2 -Norm of Advantage and Q Values We present the L2 -norm of both the advantage
and Q-values for MA-PPO and vanilla PPO during training in Figure 11. The advantage function,
which reflects the difference between the expected return (Q-value) and the baseline, is critical in
guiding policy optimization. A lower L2 -norm of both the advantage and Q-values suggests more
stable and less noisy policy updates, likely contributing to faster learning speed observed in ¬ß4.2.
 PT

The policy gradient for a sequence of length T is given by: ‚àáŒ∏ J = E
t=1 ‚àáŒ∏ log œÄŒ∏ (a|s) ¬∑ R ,
where R is the sequence reward provided by the RM. In the case of using n-gram based macro
actions, the sequence length is reduced by a factor of n, shortening the decision horizon: T ‚Üí T /n.
This reduction in the number of actions, T /n, where n > 1, implies that the temporal distance
between actions and corresponding rewards is decreased, thus reducing the variance in the gradient
9

Published as a conference paper at ICLR 2025

MA-PPO
Vanilla PPO

4

14

L2 Norm of Q Values

L2 Norm of Advantages

5

MA-PPO
Vanilla PPO

12
10

3
2
1
0

1000

2000

Step

3000

4000

Table 3: Pass@k (k = {1, 5}) metric
evaluated on the APPS test set.

8
6

CodeGemma-2B

CodeGemma-7B

PPO

MA-PPO

PPO

MA-PPO

pass@1

Inter.
Intro.
Comp.
All

2.82
15.26
0.92
4.92

3.25+15%
16.56+8%
0.94+2%
5.45+11%

4.26
20.90
1.21
6.98

6.22+46%
26.74+28%
2.00+65%
9.48+35%

pass@5

Inter.
Intro.
Comp.
All

4.10
17.30
1.70
6.26

4.37+7%
18.30+6%
1.60-6%
6.60+5%

6.57
23.30
2.30
9.06

8.37+27%
30.30+30%
3.30+43%
11.74+30%

Method

4
2
0

1000

2000

Step

3000

4000

Figure 11: L2 Norm of advantages and Q-values during
training for MA-PPO and vanilla PPO. Left: L2 norm of advantages over training steps; Right: L2 norm of Q-values.

estimate and improving credit assignment. We refer readers to Mann & Mannor (2014) for the
theoretical foundations of variance reduction through macro actions and their benefits in RL.
Case Study We show some qualitative examples in Appendix G.1, demonstrating that MA-PPO can
produce more coherent and contextually appropriate responses compared to vanilla PPO, capturing
both short/long-term dependencies effectively.
Extended Experiments: Code Generation We further assess the effectiveness of MA-PPO on the
code generation task. Following Shojaee et al. (2023); Liu et al. (2023), we utilize the compiler signal as the final reward; see Appendix B.5 for implementation details. We compare the performance
of MA-PPO and vanilla PPO using the pass@k (k=1, 5) metric (Chen et al., 2021) on the 5k test
set of the APPS dataset (Hendrycks et al., 2021). As shown in Table 3, MA-PPO significantly outperforms vanilla PPO in both pass @ 1 and pass @ 5 metrics, with more pronounced improvements
as model size scales. Notably, for the 7B model, MA-PPO achieves an improvement of +35% in
pass@1 and +30% in pass@5 over vanilla PPO, demonstrating the effectiveness of our approach in
code generation tasks.

5

R ELATED W ORK

LLM Alignment RLHF have shown impressive success in aligning LLMs with human preferences
through multi-stage training, including SFT, RM, and RL fine-tuning (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Sun et al., 2025). Recent research has explored optimization
methods for RL in LLMs, employing both online (Ahmadian et al., 2024; Farebrother et al., 2024;
Shen et al., 2024; Chakraborty et al., 2024; Shao et al., 2024) and offline RL algorithms (Snell et al.,
2023; Hu et al., 2023; Yu et al., 2024) to address training instability, improve efficiency (Tang et al.,
2024) and diversity (Sun et al., 2025). Improvements to RM learning have been proposed, such as
parameter scaling (Gao et al., 2023), fine-grained reward (Wu et al., 2023), tool use (Li et al., 2024),
and model merging (RameÃÅ et al., 2024; Rame et al., 2024). Alternatively, direct policy optimization (Rafailov et al., 2024; Ethayarajh et al., 2024; Gheshlaghi Azar et al., 2023; Rosset et al., 2024)
has emerged as a promising approach, bypassing the instability of RL while directly aligning models to human preferences. In this paper, we enhance the RLHF action space by integrating macro
actions, a well-established concept in RL (Sutton et al., 1999b; Mann & Mannor, 2014).
Macro Action in RL Macro actions introduce temporal abstraction in RL by grouping sequences of
primitive actions, reducing decision complexity and improving long-horizon credit assignment (Precup et al., 1997; Hauskrecht et al., 2013; Sutton et al., 1999b; Pignatelli et al., 2024; Machado et al.,
2023a). This method has demonstrated its utility in speeding up convergence and stabilizing policy
updates in various domains (Mann & Mannor, 2014; Solway et al., 2014). Our work applies macro
actions to RLHF in LLM training, leveraging this structure to enhance scalability and optimize credit
assignment over extended sequences.

6

C ONCLUSION AND F UTURE W ORK

In this paper, we introduced MA-RLHF, a novel framework that incorporates macro actions into
RLHF to enhance the alignment of LLMs with human preferences. Our approach demonstrates consistent improvements across multiple tasks, including summarization, dialogue generation, question
answering, and code generation. Notably, MA-RLHF achieves parity with vanilla RLHF 1.7x to 2x
faster in reward scores without incurring additional computational overhead, showing robust scalability across model sizes ranging from 2B to 27B parameters. It is promising to explore MA-RLHF
in complex step-by-step reasoning tasks for future research.
10

Published as a conference paper at ICLR 2025

R EPRODUCIBILITY S TATEMENT
We are committed to ensuring the reproducibility of the experiments presented in Section 4. To this
end, we make the source code and model checkpoints publicly available at https://github.
com/ernie-research/MA-RLHF. The detailed source code for training and evaluating both
the conventional RLHF and our proposed MA-RLHF approach is included in the supplementary
materials. We believe that these efforts will enable researchers to rigorously verify our findings and
build upon our work.

ACKNOWLEDGMENTS
We would like to express our gratitude to the anonymous reviewers for their insightful and constructive feedback.

R EFERENCES
Arash Ahmadian, Chris Cremer, Matthias GalleÃÅ, Marzieh Fadaee, Julia Kreutzer, Ahmet UÃàstuÃàn, and
Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human
feedback in llms. arXiv preprint arXiv:2402.14740, 2024.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H.
Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu,
Yujing Zhang, Gustavo HernaÃÅndez AÃÅbrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A.
Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, CleÃÅment Crepy, Shachi Dave,
Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Dƒ±ÃÅaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann,
Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023. doi:
10.48550/arXiv.2305.10403. URL https://doi.org/10.48550/arXiv.2305.10403.
Anthropic. Introducing the next generation of Claude ‚Äî anthropic.com. https://www.
anthropic.com/news/claude-3-family. [Accessed 22-07-2024].
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny
Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown,
Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a
laboratory for alignment. CoRR, abs/2112.00861, 2021. URL https://arxiv.org/abs/
2112.00861.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine
Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin
Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning
from human feedback. CoRR, abs/2204.05862, 2022. doi: 10.48550/arXiv.2204.05862. URL
https://doi.org/10.48550/arXiv.2204.05862.
Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu. ERNIE-code: Beyond
English-centric cross-lingual pretraining for programming languages. In Anna Rogers, Jordan
Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 10628‚Äì10650, Toronto, Canada, July 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.findings-acl.676. URL https://aclanthology.org/
2023.findings-acl.676.
11

Published as a conference paper at ICLR 2025

Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. Maxmin-rlhf: Towards equitable alignment of large language
models with diverse human preferences. arXiv preprint arXiv:2402.08925, 2024.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
4299‚Äì4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model
alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 3558‚Äì3567, 2019.
Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taƒ±Ãàga, Yevgen Chebotar, Ted Xiao, Alex
Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al. Stop regressing: Training
value functions via classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024.
Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023,
Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 10835‚Äì
10866. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23h.html.
Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal
Valko, and ReÃÅmi Munos. A general theoretical paradigm to understand learning from human
preferences. arXiv e-prints, pp. arXiv‚Äì2310, 2023.
Milos Hauskrecht, Nicolas Meuleau, Leslie Pack Kaelbling, Thomas L Dean, and Craig Boutilier.
Hierarchical solution of markov decision processes using macro-actions. arXiv preprint
arXiv:1301.7381, 2013.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring
coding challenge competence with APPS. In Joaquin Vanschoren and Sai-Kit Yeung (eds.),
Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL
https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/
hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html.
Jian Hu, Li Tao, June Yang, and Chandler Zhou. Aligning language models with offline reinforcement learning from human feedback. CoRR, abs/2308.12050, 2023. doi: 10.48550/ARXIV.2308.
12050. URL https://doi.org/10.48550/arXiv.2308.12050.
Glenn A Iba. A heuristic approach to the discovery of macro-operators. Machine Learning, 3:
285‚Äì317, 1989.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601‚Äì1611,
2017.
12

Published as a conference paper at ICLR 2025

Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: a
survey. J. Artif. Int. Res., 4(1):237‚Äì285, May 1996. ISSN 1076-9757.
Richard E Korf. Learning to solve problems by searching for macro-operators. 1985.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo GutmanSolo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
Solving quantitative reasoning problems with language models.
In NeurIPS, 2022.
URL
http://papers.nips.cc/paper_files/paper/2022/hash/
18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html.
Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Toolaugmented reward modeling. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=d94x0gWTUX.
Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. RLTF: reinforcement learning from unit test feedback. Trans. Mach. Learn. Res., 2023, 2023. URL
https://openreview.net/forum?id=hjYmsV6nXZ.
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane
Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The
next generation. arXiv preprint arXiv:2402.19173, 2024.
Marlos C. Machado, Andre Barreto, Doina Precup, and Michael Bowling. Temporal abstraction in
reinforcement learning with the successor representation. Journal of Machine Learning Research,
24(80):1‚Äì69, 2023a. URL http://jmlr.org/papers/v24/21-1213.html.
Marlos C Machado, Andre Barreto, Doina Precup, and Michael Bowling. Temporal abstraction in
reinforcement learning with the successor representation. Journal of Machine Learning Research,
24(80):1‚Äì69, 2023b.
Timothy Mann and Shie Mannor. Scaling up approximate value iteration with options: Better policies with fewer iterations. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the
31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
Learning Research, pp. 127‚Äì135, Bejing, China, 22‚Äì24 Jun 2014. PMLR. URL https:
//proceedings.mlr.press/v32/mann14.html.
Amy McGovern and Richard S Sutton. Macro-actions in reinforcement learning: An empirical
analysis. Computer Science Department Faculty Publication Series, pp. 15, 1998.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.
URL https://doi.org/10.48550/arXiv.2303.08774.
OpenAI. What are tokens and how to count them? https://help.openai.com/en/
articles/4936856-what-are-tokens-and-how-to-count-them, 2024. [Accessed 30-09-2024].
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser
Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan
Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In
NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
b1efde53be364a73914f58805a001731-Abstract-Conference.html.
Zhen-Jia Pang, Ruo-Ze Liu, Zhou-Yu Meng, Yi Zhang, Yang Yu, and Tong Lu. On reinforcement
learning for full-length game of starcraft. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 4691‚Äì4698, 2019.
13

Published as a conference paper at ICLR 2025

Eduardo Pignatelli, Johan Ferret, Matthieu Geist, Thomas Mesnard, Hado van Hasselt, Olivier
Pietquin, and Laura Toni. A survey of temporal credit assignment in deep reinforcement learning.
arXiv preprint arXiv:2312.01072, 2023.
Eduardo Pignatelli, Johan Ferret, Matthieu Geist, Thomas Mesnard, Hado van Hasselt, and Laura
Toni. A survey of temporal credit assignment in deep reinforcement learning. Trans. Mach. Learn.
Res., 2024, 2024. URL https://openreview.net/forum?id=bNtr6SLgZf.
Doina Precup, Richard S Sutton, and Satinder P Singh. Planning with closed-loop macro actions.
In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, pp.
70‚Äì76. Citeseer, 1997.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems, 36, 2024.
Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor,
Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing
Systems, 36, 2024.
Alexandre RameÃÅ, Nino Vieillard, LeÃÅonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier
Bachem, and Johan Ferret. WARM: on the benefits of weight averaged reward models.
In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria,
July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=
s7RDnNUJy6.
Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and
Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general
preferences. arXiv preprint arXiv:2404.03715, 2024.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, JeÃÅreÃÅmy Rapin, et al. Code llama: Open foundation models for code.
arXiv preprint arXiv:2308.12950, 2023.
Earl D Sacerdoti. Planning in a hierarchy of abstraction spaces. Artificial intelligence, 5(2):115‚Äì135,
1974.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889‚Äì1897. PMLR,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715‚Äì1725, Berlin,
Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162.
URL https://aclanthology.org/P16-1162.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li,
Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open
language models. arXiv preprint arXiv:2402.03300, 2024.
Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, and Yang Liu. Improving reinforcement learning from human feedback using contrastive rewards. arXiv preprint
arXiv:2403.07708, 2024.
Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. Execution-based code
generation using deep reinforcement learning. Trans. Mach. Learn. Res., 2023, 2023. URL
https://openreview.net/forum?id=0XBuaxqEcG.
14

Published as a conference paper at ICLR 2025

Charlie Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline RL for natural language generation with implicit language Q learning. In The Eleventh International Conference on
Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
URL https://openreview.net/forum?id=aBH_DydEvoH.
Alec Solway, Carlos Diuk, Natalia CoÃÅrdova, Debbie Yee, Andrew G Barto, Yael Niv, and Matthew M
Botvinick. Optimal behavioral hierarchy. PLoS computational biology, 10(8):e1003779, 2014.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR,
abs/2009.01325, 2020. URL https://arxiv.org/abs/2009.01325.
Haoran Sun, Yekun Chai, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Curiosity-driven
reinforcement learning from human feedback. arXiv preprint arXiv:2501.11463, 2025.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Robotica, 17(2):
229‚Äì235, 1999.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information
processing systems, 12, 1999a.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181‚Äì
211, 1999b.
Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov,
ReÃÅmi Munos, Bernardo AÃÅvila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding
the performance gap between online and offline alignment algorithms. CoRR, abs/2405.08448,
2024. doi: 10.48550/ARXIV.2405.08448. URL https://doi.org/10.48550/arXiv.
2405.08448.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane RivieÃÄre, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
Sebastian Thrun and Anton Schwartz. Finding structure in reinforcement learning. Advances in
neural information processing systems, 7, 1994.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya
Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar
Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AureÃÅlien Rodriguez,
Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/arXiv.2307.09288. URL
https://doi.org/10.48550/arXiv.2307.09288.
Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, et al. Strategic attentive writer for learning macro-actions. Advances in neural information
processing systems, 29, 2016.
15

Published as a conference paper at ICLR 2025

Michael VoÃàlske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn
automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization,
pp. 59‚Äì63, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8:229‚Äì256, 1992.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith,
Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards
for language model training. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023. URL https://openreview.net/forum?id=CSbGXyCswu.
Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, et al. Deepspeedchat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales. arXiv preprint
arXiv:2308.01320, 2023.
Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, and Hongxia Yang. $\mathcal{B}$-coder: Valuebased deep reinforcement learning for program synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
fLf589bx1f.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593, 2019.

A

L IMITATIONS

While our work demonstrates the effectiveness of MA-RLHF across multiple tasks, there are several limitations that leave room for future improvements. In our implementation, we apply the
identical action / vocabulary space as pretrained LLMs, considering the fact that defining macro
actions as one options (e.g., one macro action per n-gram) would require re-architecting the LLM‚Äôs
vocabulary and retraining the model, which is computationally infeasible. Meanwhile, our macro
action termination methods are rule-based, including linguistics- or perplexity-driven approaches;
future research could explore more complex or learnable termination strategies to further enhance
performance. Furthermore, regarding the generalization of MA-RLHF, our experiments are conducted using models with up to 27B parameters; exploring more advanced models, such as LLaMA
3.1 405B (Dubey et al., 2024) or other state-of-the-art architectures and tasks (e.g., mathematical
and complex reasoning), may provide additional insights into the scalability of MA-RLHF. Lastly,
although we observe significant improvements in training efficiency, further investigation into the
trade-offs between training stability and performance under diverse real-world conditions is necessary. Addressing these limitations will pave the way for more robust applications of MA-RLHF.

B

E XPERIMENTAL D ETAILS

B.1

DATASETS AND TASKS

TL;DR Summarization In this task, the policy is asked to generate summarizations for Reddit
posts. This dataset consists of 93k human-annotated preference pairs and 86k pairs for validation.
The trainable pairs are derived from the Reddit TL;DR (VoÃàlske et al., 2017) dataset. Additionally, a
portion of the validation pairs is sourced from the CNN Daily Mails, which serves as the test set for
out-of-distribution generalization.
HH-RLHF With the Anthropic HH-RLHF dataset, the policy is asked to generate a helpful and
harmless response given a single-turn dialogue or multi-turn dialogue. This dataset provides 112k
preference-labeled instances for training, and 12.5k for validation.
16

Published as a conference paper at ICLR 2025

WebGPT Comparisons The WebGPT Comparisons dataset contains QA pairs from the ELI5 (Fan
et al., 2019) and the TriviaQA (Joshi et al., 2017). The policy is responsible for information retrieval
and response generation. In our experimental setup, we focus exclusively on the generation task.
The policy must generate a response that balances factual accuracy and coherence. This dataset
contains 19.6k instances for training. We split 5% instances for validation, as no separate validation
set is provided.
Code Generation For this task, we leverage the APPS dataset, which contains 5k training and 5k
validation instances. The policy must write executable code based on a natural language described
in the question, using Python as the target programming language.
We present the data statistics in Table 4.
Table 4: Statistics of datasets involved in experiments. The number of tokens are calculated with
Gemma-2B tokenizer.
Dataset
Anthropic HH-RLHF
OpenAI Summarization
OpenAI WebGPT
APPS

B.2

Num. of
Comparisons

Num. of
Train Samples

Num. of
Test Samples

Avg. Tokens
in Prompt

Avg. Tokens
in Chosen

Avg. Tokens
in Rejected

127.5k
179k
19.6k
10k

112k
92.9k
18.5k
5k

12.5k
86.1k
979
5k

160
325
49
453

83
35
149
203

75
33
137
-

T RAINING D ETAILS

Following the procedure used by InstructGPT (Ouyang et al., 2022), we fine-tune both the SFT
model and the reward model on the same dataset to avoid a distribution gap. We implement our
training code with the Deepspeed-Chat package (Yao et al., 2023).
SFT Training We split the dataset into three parts, allocating 20% of the data in the supervised finetuning stage. We use the prompts and the chosen sentences as the instruction data. For the TL;DR
Summarize dataset, we concatenate the post and summarization following the approach of Stiennon
et al. (2020). For the single-turn dialogue and the question answering dataset, we apply a humanassistant chat template to format the instructions. For the program synthesis dataset, we format the
instruction data in line with Hendrycks et al. (2021).
Reward Modeling In this stage, we use 40% of the data to train the reward model for each dataset,
formatting the preference data the same way as in the SFT training stage. We initialize the reward
model using the fine-tuned SFT model. Due to the lack of preference pairs in the program synthesis
dataset, this stage is omitted for this task.
PPO Training Similar to previous stages, the remaining 40% of the data is used to optimize the
policy model. The SFT model initializes the policy model, and the reward model initializes the critic
model. For the program synthesis dataset, 80% of the data is used in this stage, with both the policy
and critic models initialized using the SFT model. The pass@1 metric serves as the reward signal
for program synthesis, compensating for the absence of a reward model. While training 7B model
on TL;DR dataset using MA-PPO, we encountered unstable training with a KL coefficient of 0.05.
Reducing the coefficient to 0.01 for the 7B model led to more stable optimization.
Table 5 lists the hyperparameters used across all training stages for each task.
B.3

N OTATIONS

In Table 6, we present the notations used in our paper.
B.4

D ETAILS OF M ACRO ACTION T ERMINATION

The general form of the segmentation rule is thus tœÑ +1 = tœÑ + |œâœÑ |, where |œâœÑ | is determined by the
chosen criterion, such as n-grams, random, parsing, or perplexity-based segmentation.
1. Fixed n-gram length: For all macro actions, we set |œâœÑ | = n, where n is a constant value.
17

Published as a conference paper at ICLR 2025

Table 5: Hyper-parameters for training Gemma series of models in MA-PPO and vanilla PPO.
Gemma

Hyper-Parameter
2B
Batch size
SFT

7B

27B

2B

7B

64 for WebGPT
512 for others

128

128

16

32

3

5 for WebGPT
1 for others

3

1

1

Epochs

1e-4 for WebGPT
5e-5 for others
cosine
0.1

Learning rate
LR scheduler
Warmup ratio

Epochs

RM

1
2e-5 for WebGPT
1e-5 for others
cosine
0.1

Learning rate
LR scheduler
Warmup ratio
Batch size
Policy learning rate
Critic learning rate

PPO epochs
Rollout
Clip ratio
Œª in GAE
Œ≥ in GAE

256
1.5e-5
1.5e-5
4 for WebGPT
1 for others
1
1
0.2
0.95
1

KL coefficient

0.05

Max prompt length
Max response length
Warmup steps
Temperature
Top-p
Top-k

512
512
200
0.8
1.0
50

Epochs
PPO

Fixed ùëõ-gram Based Termination

5
|ùúî!| = 5

5

5

5

5

5

N
crew

NP

V
repair

NP
P
from

N
Mars

1

1

2

128

-

-

1

-

-

-

-

cosine
0.1

cosine
0.1

-

-

256
1e-6
1e-6
4 for WebGPT
1 for others
1
1
0.2
0.95
1
0.1 for WebGPT
0.05 for others
512
512
200
0.8
1.0
50

256
7e-7
1e-6

16
5e-7
5e-5

16
5e-7
5e-5

1

1

1

1
1
0.2
0.95
1

1
1
0.2
0.95
1

1
1
0.2
0.95
1

0.1

0.05

0.05

512
512
0
0.8
1.0
50

600
512
20
1.0
1.0
5

600
512
20
1.0
1.0
5

Actions

2

3

5

10

2

10

ùëé!

10

ùëé"

. . . ùëé#$"

ùëé#

Random Select
5

3

NP

D
N
the spaceship

2e-6
cosine
0

8e-6

S

D
the

5e-6
cosine
0

Randomized ùëõ-gram Based Termination

5

VP

5e-6
cosine
0.1

1e-6

Parsing Based Termination

NP

2e-5
cosine
0.1
128 for TL;DR
64 for HH-RLHF
32 for WebGPT
1

32 for WebGPT
64 for others

Batch size

CodeGemma

3

2

5

1.74

1.69

Macro Action
Termination

Perplexity Based Termination
PPL: 1.75

1.71

1.82

1.78

Macro Actions
2

3

4

ùëé! ùëé"
ùúî!

...

ùëé#$% ùëé#$"
ùúî&$"

ùëé#
ùúî&

Figure 12: Illustration of four termination rules for macro actions in the MA-RLHF framework.
Each termination rule outputs a list of |œâœÑ |. In the parsing based termination, the macro action is
determined when the token number of the current node is less than C = 4, which is represented as
a number in the tree node.

2. Randomized n-gram length: We define a list of {|œâœÑ |} = {2, 3, 5, 10} to model macro actions.
This list is repeated multiple times to cover the length of the sample, in practice, we repeat this list
3 times. If the total length of macro actions can not match the number of tokens, a large number
will be considered as an additional |œâœÑ | to mitigate this gap, which is similar to the |œâœÑ | = ‚àû.
We shuffle the list and take this as a random-based length.
3. Parsing-based length: We parse the response into a constituent tree and perform a depth-first
search (DFS) to identify macro action length. Two rules guide the termination of |œâœÑ |: (1) nodes
18

Published as a conference paper at ICLR 2025

Table 6: List of notation used in this paper.
Sym.

Meaning
RL

S
A
P
r
œÅ0
Œ≥
œÄŒ∏ (a | s)
Œ∑(œÄ)
at
QœÄ (st , at )
VœÄ (st )
AœÄ (st , at )
Gt

A finite set of states.
A finite set of actions.
The state transition probability distribution.
The reward function.
The initial state distribution.
The discount factor related with future rewards.
Policy parameterized by Œ∏.
The expected cumulative discount reward.
The actions selected by the policy.
The state-action value function.
The state value function.
The advantage function.
The expected return.
RLHF

rœï (x, y)
x
y+
y‚àí
Œ≤
Œ∑
t

The reward model parameterized by œï.
Prompt.
Chosen response.
Rejected response.
KL coefficient.
The range for clipping in PPO.
Time step of tokens.
Macro Action

Œ∂
I
œÑ
œâœÑ
tœÑ
œÉœÑ

Termination condition.
Initiation set.
The index of macro action/state/reward.
Macro action at time step œÑ .
Time step of macro actions.
The weight used to measure the value of macro action.

with fewer than C tokens mark the end of a macro action; (2) nodes with single token are included
in the last macro action, avoiding single-token termination conditions like punctuation. Due to
differences between the training and parsing tokenizers, we revert to the standard PPO method
when discrepancies occur. We set the cut-off threshold C = 5, providing optimal granularity in
practice.
4. Perplexity-based length: Given a response y generated by policy model, we calculate the perplexity pt at any time step t by treating y‚â§t as the ground truth response. This process leverages the logits from the reference model, avoiding additional forward passes. Intuitively, selecting the macro actions based on perplexity P = {p0 , p1 , . . . , p|y| } can be defined as selecting tokens which consistently attribute to the decrease of the perplexity given partial sentence. Mathematically, it can be represented as œâœÑ = {atœÑ , atœÑ +1 , . . . , atœÑ +|œâœÑ |‚àí1 } where
PtœÑ = {ptœÑ , ptœÑ +1 , . . . , ptœÑ +|œâœÑ |‚àí1 } exhibits a monotonic decreasing pattern.
B.5

T RAINING S ETTINGS OF P ROGRAM S YNTHESIS

Defining the reward score solely based on the state ‚ÄúAccept‚Äù or ‚ÄúWrong Answer‚Äù is somewhat restrictive, as some generated code may pass certain unit tests while failing others. These actions
should also receive positive signals to encourage the policy to maximize the number of passed unit
tests. To address this, we incorporate an adaptive compiler signal into the reward feedback as previ19

Published as a conference paper at ICLR 2025

1.0
0.5
0.0

Vanilla PPO
MA-PPO

0.5
1000

2000

3000

4000

Training step

1.5
1.0
0.5
0.0
0.5
1.0

5000

Vanilla PPO
MA-PPO
0

1000

2000

3000

4000

2.0%

Vanilla PPO
MA-PPO

0.75
500

1000

1500

2000

Training step

2500

3000

Percentage

2.5%

0.50

RM score (7B)

0.45

0.55

0.70

2

1

0.55

0

1

RM Score

2

3

Figure 14: Distribution of test
RM scores for vanilla PPO
and MA-PPO (2B) at final
steps (5.6k) on the HH-RLHF
dataset.

0.50

0.65

3

5000

Training step

Figure 13: Test RM scores evaluated by corresponding reward model of Gemma-2B and Gemma-7B model on HHRLHF dataset.

0.60

Vanilla PPO
MA-PPO

Percentage

1.5

0

RM score (2B)

7.0%
6.0%
5.0%
4.0%
3.0%
2.0%
1.0%
0.0%

2.0

2.0

RM score (7B)

RM score (2B)

2.5

Vanilla PPO
MA-PPO

1.5%

0.60

1.0%

0.65

Vanilla PPO
MA-PPO

0.70
500

1000

1500

2000

2500

Training step

3000

0.5%
0.0%

0.9

0.8

0.7

0.6

RM Score

0.5

0.4

Figure 15: Test RM scores evaluated by corresponding reward Figure 16: Distribution of test
model of Gemma-2B and Gemma-7B model on the WebGPT RM scores for vanilla PPO and
MA-PPO (2B) at final steps
Comparisons dataset.
(3.2k) on WebGPT dataset.

ous work (Shojaee et al., 2023; Liu et al., 2023):
Ô£±
Npass
Ô£¥
Ô£¥
Ô£≤ ‚àí 0.3 + 1.3 ¬∑ Npass + Nfail , if y successfully compiled.
R(x, y) =
Ô£¥
‚àí 0.6,
if y received runtime error.
Ô£¥
Ô£≥
‚àí 1.0,
if y received compile rrror.
where x represents the prompt, and y represents the code snippet generated by the policy model.

C

A DDITIONAL E XPERIMENTS R ESULTS

C.1

R ESULTS OF D IALOGUE G ENERATION

In Figure 13, we demonstrate the RM scores on the validation set of vanilla PPO and MA-PPO. It
shows that MA-PPO surpasses vanilla PPO under RM evaluation, MA-PPO achieves parity performance at 3100 step and 2600 step for 2B and 7B models, respectively, while vanilla PPO at 5100
step and 5400 step. Generally, MA-PPO is 1.6-2x faster than vanilla PPO. Figure 14 compares the
RM score distribution of both methods.
C.2

R ESULTS OF Q UESTION A NSWERING

We assess the performance of MA-PPO on the OpenAI WebGPT Comparison dataset, which focuses
on the question answering task.
Figure 15 presents the evaluation results based on the reward model. We observe that the policy
model is challenging to optimize in this task, likely due to the suboptimal performance of the reward
model. We applied early stopping during PPO training since the policy model exhibited reward
hacking behavior which generated repetition tokens to inflate higher reward scores towards the end
of training. Despite this, evaluations on the saved checkpoints show that MA-PPO still outperforms
vanilla PPO across both tested model sizes. The reward score distribution in Figure 16 further
confirms that MA-PPO achieves superior reward scores.
20

Published as a conference paper at ICLR 2025

Win

Table 7: Test RM scores of SFT model,
vanilla PPO, MA-PPO, and baselines: DPO
and RLOO on TL;DR and HH-RLHF
datasets.
Method
SFT
DPO
RLOO
PPO
MA-PPO (n=5)

RM Score
(TL;DR)

RM Score
(HH-RLHF)

-0.64
0.03
0.81
0.83
1.40

0.13
0.64
1.31
1.55

Tie

Loss

RLOO v.s. MA-PPO 24% 4%
72%
TL;DR
RLOO v.s. PPO
50%
2%
48%
TL;DR
DPO v.s. MA-PPO 8%12%
80%
TL;DR
DPO v.s. PPO 34% 10%
56%
TL;DR
DPO v.s. MA-PPO
42%
8%
50%
HH-RLHF
DPO v.s. PPO
52%
4% 44%
HH-RLHF
0

20

40

60

% Win Rate

80

100

Figure 17: Win rates of DPO and RLOO
against PPO and MA-PPO on TL;DR and
HH-RLHF estimated by GPT-4.

When using GPT-4 as the judge, we consider three different metrics to evaluate the answers
generated by the policy: factual accuracy, coherence, and usefulness overall, following previous
work (Nakano et al., 2021). The win rates depicted in Figure 4 (Right) show that MA-PPO consistently outperforms the policy trained with vanilla PPO across all criteria. Notably, MA-PPO achieves
higher win rates in coherence and usefulness compared to factual accuracy. Human evaluation was
conducted to select the preferred answer between those generated by the two policy models. Results in Figure 4 (Right) show that answers produced by MA-PPO were predominantly preferred by
human annotators.

C.3

C OMPARING WITH A DDITIONAL BASELINES

In this section, we compare MA-PPO with two additional baselines: DPO (Rafailov et al., 2024)
and RLOO (Ahmadian et al., 2024) on Gemma-2B model. Both of the methods are implemented
with Deepspeed-Chat. Specifically, DPO models are trained on TL;DR and HH-RLHF datasets,
with the same data split as we used when training PPO. RLOO model is trained on TL;DR dataset
only, with the same policy and reward model initialization as PPO. For the training details of DPO,
the learning rate is set to 2e-7, with Œ≤ = 0.1 for TL;DR and Œ≤ = 0.01 for HH-RLHF. The policy
and reference models are initialized using the same SFT model as in PPO. For RLOO, the learning
rate for the policy model is set to 1.5e-5, and the number of online samples is K = 4. All other
hyperparameters are kept consistent with PPO.
We demonstrate the results evaluated by reward model score in Table 7, and win rates estimated
by GPT-4 in Figure 17. On TL;DR dataset, DPO fails to gain improvement compared to PPO and
MA-PPO, while RLOO achieves similar performance compared to PPO, but outperformed by MAPPO. On HH-RLHF dataset, DPO exhibits superior performance than PPO but still underperforms
the MA-PPO.

C.4

E XPERIMENTS ON L LAMA -3.2-3B

We conduct experiments on Llama-3.2-3B
model to validate the generalizability of our Table 8: Test RM scores of Llama-3.2-3B models
method across different model families. The on TL;DR dataset.
experiments are conducted on TL;DR dataset,
following the same data split as Gemma-2B.
Method
RM Score (TL;DR)
We set the learning rates of actor and critic to
SFT
2.38
5e-6 and 1e-5, and the KL coefficient is set to
PPO
3.33
0.1. Table 8 demonstrate the results evaluated
MA-PPO (n=5)
3.96
by RM score, we show MA-PPO still remarkably outperforms vanilla PPO. Using GPT-4 to assess the win rate, MA-PPO obtains 61% win, 4%
tie and 34% loss rate compared against PPO. These results prove the generalizability of our method.
21

Published as a conference paper at ICLR 2025

Actions ùëé"

ùëé#

ùëé%

Equal Assignment

ùëé$

‚àö

Critic Model

0.3

ùëâ(ùë†& )

0.9

1.5

0.5 +

ùúé!

Value Function

ùëâ(ùë†! )

0.3

Position Decay Assignment

0.5

0.5 +

0.6

0.5

0.9

Unit Assignment

ùúé!

1/3 + 2/3

1/3 + 2/3

ùúé!

ùëâ(ùë†! )

0.7

0.7

ùëâ(ùë†! )

0

+

1

0.9

0

+

1

0.3

Figure 18: Illustration of value function of macro actions in MA-RLHF framework. It takes the
outputs from the value function of tokens as input, and returns the value of macro actions with
different œÉœÑ assignment.
Table 9: Pass@1 metric evaluated when applying different termination conditions on APPS
dataset.
Dataset

Termination

RM Score

GPT-4 Win Rate
(v.s. PPO)

TL;DR

Fixed 5-gram
Parsing
PPL

1.40
1.37
1.27

78%
78%
72%

Fixed 5-gram
Parsing

1.55
1.64

58%
62%

HH-RLHF

Table 10: Test RM scores and GPT-4
win rates when applying different termination conditions on TL;DR and HH-RLHF
datasets.
Termination
pass@1

D

F URTHER A NALYSIS

D.1

VALUE F UNCTION E STIMATION OF M ACRO ACTION

Inter.
Intro.
Comp.
All

Fixed 10-gram

Parsing

PPL

3.25
16.56
0.94
5.45

3.17
17.05
1.24
5.56

3.04
16.36
0.80
5.26

When implementing the macro actions, the value function of macro actions is estimated
through the value function of tokens. This process can be formulated as: V œÄ (sœÑ , œâœÑ ) =
P|œâœÑ |
œÄ
i=0 œÉtœÑ +i V (stœÑ +i , atœÑ +i ), where œÉœÑ = {œÉtœÑ , ¬∑ ¬∑ ¬∑ , œÉtœÑ +|œâœÑ | } control the contribution of each
value function of tokens.
In this section, we explore several assignments of œÉœÑ and their effectiveness on MA-PPO. Figure 18
illustrates macro action value function with different œÉœÑ assignments:
1. Equal assignment: We treats the contributions of each value function of tokens equally when
considering the value function of macro actions, i.e., œÉœÑ = { |œâ1œÑ | }œÑi=1 . This is the naive assignment in MA-PPO used in all our experiments.
2. Unit assignment Since a macro action is a higher-level construct of a sequence of actions, we
can use the value function of the last action as the macro action‚Äôs value function, where œÉœÑ =
{0, 0, ¬∑ ¬∑ ¬∑ , 0, 1}.
3. Position decayed assignment The contributions of each value function of tokens are determined
by taking the position into consideration. We define œÉœÑ based on the position of the token, i.e.,
P|œâœÑ |‚àí1
P
|œâœÑ |‚àí1
1
1
œÉœÑ = { (|œâœÑ |‚àíi)¬∑H
}i=0
, where H = i=0
œÉ‚ààœÉœÑ œÉ =
(|œâœÑ |‚àíi) , this construction ensures
1.
We tested these approaches with fixed n-gram based termination on TL;DR dataset, with n = 5.
We report the RM score and GPT-4 score as previous. Results in Figure 19 show that the equal
assignment yields higher RM scores. However, the unit assignment achieves the best consistency
and fluency according to GPT-4 evaluations.
22

Published as a conference paper at ICLR 2025

7

Vanilla PPO
MA-PPO (Equal)
MA-PPO (Unit)
MA-PPO (Position)

6

GPT-4 Score

RM score

1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.25
0.50

Vanilla PPO
MA-PPO (Equal)
MA-PPO (Unit)
MA-PPO (Position)
reference
0

1000

2000

3000

5
4

4000

Training step

3

Relevance Coherence Consistency Fluency

Figure 19: Performance of MA-PPO with different value function estimations in MA-PPO on
TL;DR dataset for Gemma-2B model. Left test RM scores. Right GPT-4 scores on 4 dimensions.

D.2

T ERMINATION C ONDITIONS ON D IFFERENT TASKS

In this section, we analysis the effectiveness of termination conditions on TL;DR, HH-RLHF, and
APPS datasets. When implementing parsing-based termination condition on APPS dataset, we use
a programming-language-based parser.4 The results of TL;DR and HH-RLHF datasets are shown
in Table 9 and Table 10. We can notice that parsing-based termination condition performs well
on the HH-RLHF tasks, with higher RM score and win rate than fixed 5-gram based termination
condition. While on the TL;DR dataset, parsing-based termination condition also achieves excellent
performance compared to fixed 5-gram termination condition. On APPS dataset, parsing-based
termination condition achieves the best results, except for the interview level task. These results
demonstrate that construct macro action with linguistic information indeed brings performance gain
to MA-PPO.
I MPACT OF RLHF ON R EWARD S CORE D ISTRIBUTION

MA-PPO BoN RM Score

0.8

PPO BoN RM Score

0.8

0.6

0.4

0.4

0.4

0.2

0.2

0.8

1.0

0.0

0.2

0.4

0.6

0.8

MA-PPO RM Score

1.0

0.0

0.2

0.4

0.6

PPO RM Score

0.8

1.0

0.8
0.6
0.4
0.2
0.0

0.0

0.2

0.4

0.6

PPO RM Score

0.8

1.0

0
200
10
0

0.6

0.0

0
200
10
0

0.4

PPO RM Score

0.2

0
2000
01

0.2

0
200
10
0

0.0

0.0

200
100
0
1.0

0.8

0.6

0.6

0.0

200
100
0
1.0

200
100
0
1.0

MA-PPO (n=‚àû) RM Score

200
100
0
1.0

MA-PPO (n=5) RM Score

D.3

Figure 20: RM score shifting pattern after RLHF training. Left presents the RM score of best of 8
sampling on vanilla PPO compared to the vanilla PPO. Mid Left presents the RM score of best of
8 sampling on MA-PPO compared to the MA-PPO. Mid Right presents the RM score of MA-PPO
(n = 5) compared to the vanilla PPO model. Right presents the RM scores of MA-PPO (n = ‚àû)
compared to the vanilla PPO model.
We apply Best-of-N sampling on both vanilla PPO and MA-PPO. The RM score shifting patterns
for these methods are illustrated in Figure 20 (Left and Mid Left). From the results, we can conclude
that Best-of-N sampling continues to enhance the performance of RLHF models effectively.
In Figure 20 (Mid Right and Right), we compare the MA-PPO with vanilla PPO using settings of
n = 5 and n = ‚àû, both of which demonstrate positive effects on the RM score distribution.
D.4

I MPACT OF S AMPLING T EMPERATURE

In the previous experiments, the results were sampled with a temperature temp = 0.8 to align
with the sampling strategy used during training. In this section, we examine the effect of sampling
4

RedBaronhttps://github.com/PyCQA/redbaron

23

Published as a conference paper at ICLR 2025

2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

0.0

0.2

2.75
2.50
2.25
2.00
1.75
1.50
1.25
1.00

Vanilla PPO
MA-PPO

Reward score

Reward score

Vanilla PPO
MA-PPO

0.4

0.6

0.8

Temperature (2B)

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Temperature (7B)

Figure 21: Test reward scores evaluated by the corresponding reward model for summarizations
generated with different sampling temperature on the TL;DR dataset.
Token-Level RLHF

Macro Action RLHF

RL Optimization
MA Rewards (KL)

Actions

?
Policy Model

Macro Action
Termination

ùëé!

ùëé"

...

...

ùëÖ&%

1

ùëÖ&$

Rewards

MA Values

MA Rewards

ùëâ(ùë†# )

ùëÖ#

ùëâ(ùë†$ , ùúî$ )

ùëÖ$

ùëÖ$
Macro Actions

‚àö

ùëÖ#
Critic Model

2

ùëé#

‚àö

Query

2

|ùúî$ |

Values

?
Reward Model

Values
ùëâ(ùë†! ) ùëâ(ùë†" ) . . . ùëâ(ùë†# )

|ùúî"| = 2

ùëé!

ùëé"

ùëé%

ùëé#

...

ùëÖ#
Reward Model

Policy Model

ùúî!
Query

ùúî"

...

GAE Function

‚àö

|ùúî#| = 1

ùúî$

‚àö

Critic Model

Advantages

MA Values

ùëâ(ùë†! ) ùëâ(ùë†" )
ùëâ(ùë†$ , ùúî$ )

ùê¥#
‚Ñí

State-Action Value

ùê¥$

&&'

ùëÑ#

ùëÑ$

ùëâ(ùë†# )

ùëâ(ùë†$ , ùúî$ )

‚Ñí ()*+,

ùëé#

ùúî$

Figure 22: Illustration of the macro action-RLHF (MA-RLHF) framework.
temperature on response quality. We vary the temperature temp ‚àà {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}, and
report the results in Figure 21. The performance of both methods remains stable when temp < 0.8.
However, the performance of vanilla PPO begins to decline after temp = 0.8, whereas MA-PPO
continues to demonstrate stable performance, even at temp = 1.0.
Algorithm 1: Framework of Macro Action RLHF.
Input: Prompts: X = {x0 , x1 , . . . , xn }; Policy model: œÄpolicy ;Reference model:
œÄref ; Critic model: œÄcritic ; Reward model: œÄrm ; Termination rule Œ∂(¬∑) in
Section 3.2.1; Value function estimation œÉtœÑ in Section D.1.
Output: Policy loss Lppo , Critic loss Lvalue .
foreach prompt xi in X do
Make experience using policy model y := œÄpolicy (x);
Get value V (st ) := œÄcritic (x, st ) at every time step t ‚àà [0, |y|);
Get reward score at current experience r := œÄrm (x, y);
m
Compute macro actions {œâœÑ }m
œÑ =1 based on the termination rule {œâœÑ }œÑ =1 := Œ∂(y);
m
foreach macro action œâœÑ in {œâœÑ }œÑ =1 do
Compute macro action value function
P|œâœÑ |
V œÄ (sœÑ , œâœÑ ) = i=0
œÉtœÑ +i V œÄ (stœÑ +i , atœÑ +i );
œÄ
Obtain AÃÇœÑ and QÃÇœÑ with
(sœÑ , œâœÑ ), r);
h GAE(V


œÑ)
œÑ)
Optimize Lppo = EÃÇ min œÄŒ∏œÄŒ∏ (œâ(œâœÑ |s
AÃÇœÑ , clip( œÄŒ∏œÄŒ∏ (œâ(œâœÑ |s
, 1 ‚àí œµ, 1 + œµ)AÃÇœÑ
œÑ |sœÑ )
œÑ |sœÑ )
old
old
h
i
value
œÄ
2
Optimize L
= EÃÇ ‚à•V (sœÑ , œâœÑ ) ‚àí QÃÇœÑ ‚à•

E

i

MA-RLHF A LGORITHMS

Figure 22 illustrates the framework of MA-RLHF. In practice, to implement MA-RLHF, once the
macro actions are obtained via the termination function, we compute their value (as estimated by
the critic model) and rewards (based on a per-token KL penalty) using the value function estimation.
With these values and rewards, we apply Generalized Advantage Estimation (GAE) without modification to derive advantage estimates and state-action value functions. These advantage estimates
and state-action value functions are then used to all tokens within the macro action during the opti24

Published as a conference paper at ICLR 2025

mization of both the policy and critic models. The macro action RLHF algorithm, utilizing PPO, is
detailed in Algorithm 1.
In this implementation, the introduced additional time complexity is in the option termination. While
fixed n-gram based, randomized n-gram based, and perplexity based terminations achieves same
time complexity, the time complexity of parsing based termination is related to the constituent tree
which we applied DFS to obtain |œâœÑ |. During the inference stage, our MA-PPO will not introduce
additional complexity since it only works at the training stage.
We provide the Pytorch code for implementation of the macro action in PPO below:

25

Published as a conference paper at ICLR 2025

Obtain Macro Action Positions
def get_macro_action_positions(self, start, mask, termination=‚Äôngram‚Äô,
n_gram: int=None, ppl: List[torch.float16]=None, repeat_times: int=
None, cutoff: int=None):
sequence = [start]
if termination == ‚Äôngram‚Äô:
assert n_gram is not None
current_count = 0
for i in range(mask[:, start:].size(1) - 1):
current_count += mask[0, start + i].item()
if current_count == n_gram:
sequence.append(start + i + 1)
current_count = 0
elif termination == ‚Äôrandomized_ngram‚Äô:
k_list = torch.tensor([2, 3, 5, 10], dtype=int)
k_list = torch.repeat_interleave(k_list, 3)
k_list = k_list[torch.randperm(k_list.size()[-1])]
indexed_k_list = torch.cumsum(k_list, dim=-1)
sequence = [n for n in range(start, mask[:, start:].size(1) 1)]
indexed_k_list = [x.item() for x in indexed_k_list if x.item()
< len(sequence)]
sequence = [start] + [sequence[i] for i in indexed_k_list]
elif termination == ‚Äôppl‚Äô:
assert ppl is not None
for i in range(1, len(ppl)):
if ppl[i] > ppl[i - 1]:
sequence.append(start + i)
elif termination == ‚Äôparser‚Äô:
if len(node.leaves()) < 1:
return False, ma_length + 1
if len(node.leaves()) < cutoff:
sequence.append(ma_length + node.leaves())
return True, ma_length + node.leaves()
for nxt_node in node.childs():
state, ma_length_ = dfs(nxt_node, ma_length)
if !state:
sequence[-1] = ma_length_
ma_length = ma_length_
return True, ma_length
sequence.append(int(mask.size(1) - 1))
return sequence

Calculate Values / Rewards of Macro Action
def get_macro_action_values(self, values, mask, start, sequence):
split_list = torch.diff(torch.tensor(sequence)).tolist()
splited_values = torch.split(values[:, start:], split_list, dim=-1)
splited_mask = torch.split(mask[:, start:], split_list, dim=-1)
inplace_values = torch.zeros(1, len(split_list), dtype=values.dtype
).to(values.device)
for idx, (value_i, mask_i) in enumerate(zip(splited_values,
splited_mask)):
masked_values = value_i[mask_i != 0]
inplace_values[0, idx] = torch.mean(masked_values) if
masked_values.numel() > 0 else 0.0
return inplace_values

26

Published as a conference paper at ICLR 2025

Calculate Policy Model Loss
def policy_loss_macro_action(self, logprobs, old_logprobs, advantages,
mask, sequence):
log_ratio = (logprobs - old_logprobs) * mask
ratio = torch.exp(log_ratio)
# calculate loss with macro action
split_list = torch.diff(torch.tensor(sequence)).tolist()
split_ratio = torch.split(ratio, split_list, dim=-1)
split_mask = torch.split(mask, split_list, dim=-1)
pg_loss = 0.0
total_mask_sum = 0.0
for i in range(len(split_list)):
ratio_i = split_ratio[i]
mask_i = split_mask[i]
advantages_i = advantages[:, i]
pg_loss1 = -advantages_i * ratio_i
pg_loss2 = -advantages_i * torch.clamp(ratio_i, 1.0 - self.
cliprange, 1.0 + self.cliprange)
pg_loss += torch.sum(torch.max(pg_loss1, pg_loss2) * mask_i)
total_mask_sum += mask_i.sum()
pg_loss = pg_loss / total_mask_sum
return pg_loss

Calculate Critic Model Loss
def critic_loss_macro_action(self, values, old_values, returns, mask,
sequence):
values_clipped = torch.clamp(
values,
old_values - self.cliprange_value,
old_values + self.cliprange_value,
)
if self.compute_fp32_loss:
values = values.float()
values_clipped = values_clipped.float()
# calculate loss with macro action
split_list = torch.diff(torch.tensor(sequence)).tolist()
splited_values = torch.split(values, split_list, dim=-1)
splited_values_clipped = torch.split(values_clipped, split_list,
dim=-1)
splited_mask = torch.split(mask, split_list, dim=-1)
total_vf_loss = 0.0
total_mask_sum = 0.0
for i in range(len(splited_values)):
vf_loss1 = (splited_values[i] - returns[:, i])**2
vf_loss2 = (splited_values_clipped[i] - returns[:, i])**2
vf_loss = 0.5 * torch.sum(
torch.max(vf_loss1, vf_loss2) * splited_mask[i])
total_vf_loss += vf_loss
total_mask_sum += splited_mask[i].sum()
total_vf_loss = total_vf_loss / total_mask_sum
return total_vf_loss

27

Published as a conference paper at ICLR 2025

PPO
# In PPO algorithm
start = prompts.size()[-1] - 1
action_mask = attention_mask[:, 1:]
...
sequence = get_macro_action_positions(start, action_mask, termination=‚Äô
ngram‚Äô, n_gram=n_gram)
macro_action_old_values = get_macro_action_values(old_values,
action_mask, start, sequence)
macro_action_old_rewards = get_macro_action_values(old_rewards,
action_mask, start, sequence)
advantages, returns = get_advantages_and_returns(sumed_old_values,
sumed_old_rewards)
policy_loss = policy_loss_macro_action(policy_log_prob[:, start:],
log_probs[:, start:], advantages, action_mask[:, start:], sequence)
critic_loss = critic_loss_macro_action(value[:, start:], old_values[:,
start:], returns, action_mask[:, start:], sequence)

F

E VALUATION D ETAILS

F.1

GPT-4 E VALUATION P ROMPTS

In our experiments, we take GPT-4 as a main judgment of the quality of policy models. The prompts
used to generate win rates using GPT-4 are listed below. We utilize the gpt-4o-05-13 for all of
our experiments. The order of the responses generated by policy models is randomly chosen for all
experiments.

TL;DR GPT-4 Evaluation Prompt
You will be given two summaries written for an article. Your task is to pick the better one between
them, based on the four criteria. Please make sure you read and understand these instructions carefully.
Relevance - selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which
contained redundancies and excess information.
Coherence - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby ‚Äúthe summary should be well-structured and well-organized.
The summary should not just be a heap of related information, but should build from sentence to a
coherent body of information about a topic.‚Äù
Consistency - the factual alignment between the summary and the summarized source. A factually
consistent summary contains only statements that are entailed by the source document. Annotators
were also asked to penalize summaries that contained hallucinated facts.
Fluency - the quality of the summary in terms of grammar, spelling, punctuation, word choice, and
sentence structure.
You should output single character to indicate which summary you think is better. ‚ÄòA‚Äô stands for
Summary A and ‚ÄòB‚Äô stands for Summary B. If you think both summaries are equally good, output ‚ÄòE‚Äô.
Article / Post:{article / post}
Summary A:{summary a}
Summary B:{summary b}
Your Choice (only a single character):

28

Published as a conference paper at ICLR 2025

HH-RLHF GPT-4 Evaluation Prompt
For the following query to a chatbot assistant, which response is more helpful?
First provide a one-sentence comparison of the two responses and explain which you feel is more
helpful. Second, on a new line, state only ‚ÄòA‚Äô or ‚ÄòB‚Äô to indicate which response is more helpful. If they
are equally good or bad, state ‚ÄòE‚Äô. Your response should use the json format, with ‚Äúcomparison‚Äù and
‚Äúchoice‚Äù as keys.
Query: {query}
Response A: {response a}
Response B: {response b}
Your Judgment:

WebGPT Comparisons GPT-4 Evaluation Prompt
You will be given two response written for an question. Your task is to pick the better one between
them, based on these criteria.
Factual accuracy - which answer is more factually accurate?
Coherence - which answer is easier to follow?
Usefulness overall - all things considered, which answer would be more helpful to the person who
asked this question?
You should output with a json format where the key is the criteria and the value is the choice you made,
using ‚ÄòA‚Äô stands for Response A and ‚ÄòB‚Äô stands for Response B. If you think both responses are equally
good, output ‚ÄòE‚Äô.
Question: {question}
Answer A: {answer a}
Answer B: {answer b}
Your Judgment (you should also output the reason, note that you are allowed to think both responses
are equally good, then output with ‚ÄòE‚Äô):

F.2

H UMAN E VALUATION

To estimate the quality from a human perspective, we collect human preference data on the TL;DR,
HH-RLHF, and WebGPT datasets. Human annotators select the preferred response based on taskspecific criteria. For TL;DR, the evaluation criteria focus on three main perspectives:
1. Hallucination: this considers whether the generated summary includes any additional information not present in the original post or article.
2. Verbosity: this assesses if the summary includes unnecessary context that could be removed
without negatively impacting its quality.
3. Overall Quality: this measures the general coherence, informativeness, and readability of the
generated summary.
For evaluation on TL;DR dataset, the annotators should first compare the overall quality of two
responses. If overall qualities are equally good for responses, then they should choose the winner
based on hallucination and verbosity.
In the context of HH-RLHF, annotators focus on the helpfulness of the responses:
1. Instruction Following: whether the generated response follows the requirements in the instruction
2. Usefulness: whether the advices in the response are applicable, and does the response ideally
guide the user on what to do next.
Annotators are instructed to choose the response based on these aspects, while excluding superficial replies such as ‚ÄùYou‚Äôre welcome.‚Äù For the WebGPT dataset, the primary evaluation factor is
factual accuracy. Annotators are provided with retrieval information relevant to the question from
the dataset to aid in their judgment. They are tasked with selecting the answer that most accurately
matches the retrieved information.
During the evaluation process, annotators are presented with a prompt and two responses, each
generated by either vanilla PPO or MA-PPO. To ensure impartiality and prevent annotators from
29

Published as a conference paper at ICLR 2025

guessing which model produced which response, we shuffle the positions of the responses. Annotators are given three choices: response A wins, response B wins, or a tie. The results are then
collected to calculate the win rates for each model.
For evaluations on the TL;DR and HH-RLHF datasets using 7B models, we conduct the human
evaluation with 3 different annotators and collect their preference data to report the win rates. For
all other human evaluations, we conduct them with a single annotator. The inter-rater agreement
achieves an average of 68% on total 100 samples. On the TL;DR dataset the agreement is 64%, and
on the HH-RLHF dataset the agreement is 72% across 50 samples per task.

G

G ENERATED E XAMPLES

G.1

C ASE S TUDY

When evaluating the responses of MA-RLHF with human annotators, we observe that the MARLHF exhibits coherence and contextual appropriate abilities. We illustrate this phenomenon with
an example by comparing MA-RLHF with the baseline in Table 11. We found that the MA-PPO
method tends to generate responses with phrases more than the baseline method. Specifically, for
nouns, it commonly includes adjectives for modification to make the generated summaries more
accurate, such as ‚Äúfeeding indoor cat food‚Äù.
G.2

E XAMPLE R ESPONSES

In this section, we demonstrate some examples of validation sets to highlight the superiority of
MA-PPO. In Table 12, we feature examples from the TL;DR dataset. Compared to the responses
generated by vanilla PPO, the responses from MA-PPO offer more concise and relevant details
about the situation. Table 13 showcases a dialogue focusing on the differences between Spanish
and Italian. MA-PPO provides a more helpful response, addressing both cultural and grammatical
perspectives. In Table 14, we compare answers from the WebGPT dataset. The responses provided
by MA-PPO are more factual and accurate compared to those generated by vanilla PPO.

30

Published as a conference paper at ICLR 2025

Table 11: Example response from TL;DR dataset to illustrate the responses generated by MA-RLHF
is more coherence and contextual appropriate compared to baseline. We highlight the context that
we observed with green .
Prompt

POST Subreddit: r/cats
Hello everyone! One of my cats is about 10 years old now, she is pretty much strictly
indoors save for some time she spends on our screened in porch each day. (She likes
to watch the birds in the yard while she suns herself by the pool, quite the princess).
Anyway, when she was younger she was very active and quite small, however with
age she has put on a pretty hefty amount of weight. I feed her indoor cat food
for weight control, I‚Äôve switched brands a few times trying to find something that
works, I‚Äôve cut back on feeding her by a lot (she gets very angry and demanding
when she wants food but I don‚Äôt give in) however, nothing really seems to work.
I‚Äôve tried cat toys, and bought a harness thinking I could try to walk her but she just
lays down and looks at me like I‚Äôm stupid. Basically I just want to know if you all
have any suggestions for exercise or food. I care about her and don‚Äôt want this to
get any worse. I also have another cat that eats the same amount and type of food
as her and is a completely normal weight and only a year younger, however he is a
male, not sure if that makes a difference in predisposition for weight gain. They are
also both fixed.

Vanilla PPO

10 year old cat has gained a noticeable amount of weight and is becoming increasingly inactive. Looking for suggestions for exercise or food to help her get healthier.
Male and female cats are both affected.

Fixed 3-gram

10 year old cat put on hefty amount of weight despite feeding indoor cat food for
weight control. Any suggestions for exercise or food to help with weight control and
prevent further weight gain? Specifically wondering if there are any suggestions
for cats that are both female and fixed.

Fixed 5-gram

10 year old indoor cat has put on significant amount of weight despite being fed
low calorie/weight control food. Need suggestions for exercise or food to help with
weight management. Also have another cat that eats same type/amount of food as
her and is a completely normal weight and only a year younger, however he is a
male. Any insights would be appreciated.

Fixed 10-gram

10
year
old
cat
is
putting
on
significant
weight
despite feeding indoor cat food for weight control .
Any suggestions for exercise or food to help her lose weight and keep her healthy? Also, is there a
difference in predisposition for weight gain between male and female cats when
fed same amount/type of food?

Fixed ‚àû-gram

One 10 year old cat has put on hefty amount of weight with age , feed
indoor cat food for weight control, tried various brands and cut back
feeding by lot, nothing really seems to work in terms of exercise or
food, also have another cat same age and weight as her and completely normal weight and no issues currently however male and
not sure if that makes difference predisposition for weight gain , both fixed.

Randomized n-gram

10 year old cat has put on a lot of weight, feed her indoor cat food for weight control
yet nothing seems to work. Any suggestions for exercise or food to address this?
Other cat that eats same amount and type of food as her is a normal weight and a year
younger, however is male, not sure if that makes a difference in predisposition for
weight gain.

Perplexity based

10
yo
cat
has
put
on
significant
amount
of
weight
despite diet change and other attempts at exercise/activity.
Any suggestions
for exercise or food to help her lose weight? Also, any differences between males
and females regarding predisposition for weight gain?

Parsing based

10 year old indoor cat has gained significant amount of weight
despite proper diet and exercise , potentially due to age related health issues .
Need suggestions for exercise or food changes to help reduce weight and
maintain proper body weight . Is weight gain common in older cats, and does
gender play a factor in predisposition ?

31

Published as a conference paper at ICLR 2025

Table 12: Example responses from TL;DR dataset. The summarization of MA-PPO exactly capture
the details of the post.
Prompt

Vanilla PPO 2B
RM Score: 0.75
MA-PPO 2B
RM Score: 1.55
Vanilla PPO 7B
RM Score: 1.71
MA-PPO 7B
RM Score: 2.61

Vanilla PPO 27B
RM Score: 3.45

MA-PPO 27B
RM Score: 3.95

POST Subreddit: r/running
Last night I experienced my first ever absolutely miserable run where instead of feeling energized and accomplished when it was over, I lay on my
bathroom floor shivering and hugging the toilet.
I have had frustrating runs before and I have had runs when my knee/ankle/toe hurt and made me want to quit but last night was far worse than any
of those.
Earlier in the day, I ate a sandwich and fell asleep on the couch for a two
hour surprise nap. When I woke up my stomach was slightly unsettled like
I was about to have heartburn. I decided to do out for my long run anyways
because I always have heartburn and it doesn‚Äôt usually stop me. This run
was scheduled to be my longest yet at 1:30:00 (I am training for a half).
At about 45 minutes I started to get stomach cramps so I slowed how much
water I was taking in. The entire run I had to walk several times which is
unusual for me but my times were better than expected so I thought I was
just pushing too hard and tried to take it easy on the way back.
My post long run ritual is usually to drink several long drinks once I get
back to the car since now I won‚Äôt be jostling my innards around, I can do
what I want. When I got home, I felt like I probably shouldn‚Äôt try eating
right away so I had about 1/3 bottle of Gatorade.
Well...Gatorade was the wrong answer. Within 20 minutes I was violently
heaving into my kitchen garbage and I lost all of the Gatorade and likely
most of the water I had while running.
My question is, do I blame my sandwich from earlier or my run? I am
worried that 1:30:00 is my limit. I haven‚Äôt considered quitting my half
marathon training until now because everything up to this point has been
fine.
1:30:00 run was extremely stressful and painful. Do I blame sandwich from
earlier or run itself? I am worried that 1:30:00 is my limit.
1:30:00 long run resulted in severe stomach cramps and forced heavy heaving into kitchen garbage. Do I blame my sandwich from earlier or my run?
Concerned that 1:30:00 is my limit.
1:30:00 run caused serious stomach pains and I puked up all of the water I
drank while running. Do I blame the sandwich for my stomach pain or the
run itself?
1:30:00 long run, had stomach cramps and had to walk often, drank
Gatorade post run, vomited violently within 20 minutes, now worried that
1:30 is my limit, do I blame sandwich or run? Worried about quitting half
marathon training. Help?
45 minutes into my run my stomach started to cramp and I was vomiting
profusely within 20 minutes of finishing my 1:30:00 long run. Do I blame
my sandwich from earlier or my run? Does this mean my limit is 1:30:00
or is it something else?
I went out for a 1:30:00 long run yesterday and came home with stomach
cramps and nausea. I drank 1/3 bottle of Gatorade when I got home and got
violently sick. I lost that and most of the water I drank while running. Do
I blame my sandwich from earlier or my run? I am worried that 1:30:00 is
my limit and that I won‚Äôt be able to finish my half marathon training.

32

Published as a conference paper at ICLR 2025

Table 13: Example responses from HH-RLHF dataset, MA-PPO method presents more helpful
response compared to vanilla PPO, and is prefered by GPT-4 and human.
Prompt

Vanilla PPO 2B
RM Score: 1.07

MA-PPO 2B
RM Score: 1.24

Vanilla PPO 7B
RM Score: 0.80

MA-PPO 7B
RM Score: 1.22

Human: Would you be able to explain the differences between the Spanish
and Italian language? Assistant: Of course. Can you tell me more about
the specific areas where you‚Äôre interested in knowing more? Human: I‚Äôm
thinking between the Spanish spoken in Mexico and Italian spoken in Italy.
Assistant:
The main difference between Spanish and Italian is that Spanish is an IndoEuropean language with a very rich historical background, while Italian is
an Italian Romance language derived from Latin. Spanish has influences
from many different languages, while Italian has fewer and more limited
influences from other languages.
Here are some of the main differences between Spanish spoken in Mexico
and Italian spoken in Italy: 1. Spanish spoken in Mexico tends to be more
relaxed and casual, while Italian spoken in Italy tends to be more formal
and courteous. 2. Spanish in Mexico is heavily influenced by indigenous
languages and has a rich vocabulary, while Italian is mostly derived from
Latin and has a more simple and elegant sound. 3. Spanish is spoken
throughout Latin America, while Italian is confined to the Italian peninsula.
Overall, Spanish and Italian have many subtle differences that contribute to
their unique cultural identities.
The main differences between the Spanish spoken in Mexico and the Italian
spoken in Italy are in the pronunciation and grammar of the two languages.
In Mexico, the Spanish spoken is much more similar to the Spanish spoken
in Spain. In Italy, the Italian spoken is similar to the Italian spoken in other
parts of Europe, but with its own unique style of pronunciation. In both
languages, the grammar is very similar, though the Italian language does
have some unique grammar rules.
Sure, here are some of the main differences between the Spanish spoken in
Mexico and the Italian spoken in Italy: - Spanish is a latin language, which
is a language that was derived from the latin language of ancient Rome.
Italian is based on the ancient language of the romans of ancient Rome. Spanish is spoken in a number of different countries throughout the world,
whereas Italian is the native language of Italy, and is only spoken in some
parts of northern Italy. - Spanish is spoken primarily in Latin America, and
has taken on many local variations and differences due to the influence of
local indigenous languages (i.e. Mexican Spanish is quite different from
Spanish spoken in Argentina). Italian is primarily spoken in Italy and has
more localized variations. - There are many differences in grammar, vocabulary, and pronunciation between Spanish and Italian that are reflected
in the differences between Mexican Spanish and Italian. Some of the most
important differences in grammar include the use of masculine and feminine gender to refer to objects, and the different use of the tenses. There are
many differences in vocabulary that also exist between the two languages,
and the pronunciation of each language also varies.

33

Published as a conference paper at ICLR 2025

Table 14: Example responses from WebGPT dataset.
Prompt
Vanilla PPO 2B
RM Score: -0.68

MA-PPO 2B
RM Score: -0.41

Vanilla PPO 7B
RM Score: -0.41

MA-PPO 7B
RM Score: -0.39

What causes that sudden sinking, heavy, hot flush feeling when you become anxious
and how are the effects felt so rapidly?
Anxiety has been linked to several physical symptoms, including a rapid heartbeat,
trembling, dizziness, and a sense of impending doom. While some of these symptoms may be due to the anxiety-provoking situation causing the symptoms, some
may be the result of an anxiety disorder itself. The symptoms that result from anxiety can be divided into physical symptoms, emotional symptoms, and behavioral
symptoms. Physical symptoms of anxiety include nausea, dizziness, and a queasy
or empty feeling in the stomach. Emotional symptoms include feeling helpless,
hopeless, afraid, irritable, and worried. Behavioral symptoms include avoiding social interactions, remaining in bed all day, and not performing daily activities.
The ‚Äúhot flush‚Äù sensation that some people experience when they become anxious
is the result of the nervous system‚Äôs fight-or-flight response. When the body is
faced with a stressful or dangerous situation, the nervous system‚Äôs adrenaline release causes blood to rush through the cardiovascular system to the muscles. This
causes the body to become more efficient at using oxygen and energy, and it can
also cause the skin to turn red and hot. This response is designed to help the body
deal with stress and danger, and it can include a range of symptoms that impact
different parts of the body. For example, elevated blood pressure can cause the kidneys to release more urine, and adrenaline can speed up the heart rate. It can also
cause the mouth to dry out as the nervous system reacts to the perceived threat. In
addition, anxiety can cause the chest to tighten and breathing to become quicker and
shallower, which can cause the heart rate to increase. It can also cause the digestive
system to slow down, and the muscles of the gastrointestinal tract to tighten, which
can cause nausea and abdominal cramping.
There are several possible explanations as to why we feel hot and uncomfortable
when anxious. One explanation is that anxiety causes the body to release hormones
and chemicals into the bloodstream, which then circulate throughout the body and
affect organs, tissues, and nerve cells. A different explanation is that a reaction
to stress causes the body to release hormones and chemicals into the bloodstream,
which then circulate throughout the body and affect organs, tissues, and nerve cells.
Another explanation is that the body‚Äôs stress response causes the release of hormones and chemicals into the bloodstream, which then circulate throughout the
body and affect organs, tissues, and nerve cells. A fourth explanation is that anxiety
triggers the release of cortisol, which increases blood sugar levels and causes the
body to burn fat and muscle tissue for energy.
When we become anxious, it is because our brain has encountered something that
it sees as a threat. The brain then sends out a warning signal, which takes the form
of a fight-or-flight response. This response causes a number of physical effects,
including increased heart rate and breathing rate, increased blood flow to muscles,
and elevated blood sugar. This is because when we are threatened our body needs
to be able to run or fight, and to do this we need increased oxygen and energy. Our
body also needs to be able to protect itself, so we have increased blood flow to our
muscles. When we are in a high-stress situation, our body produces serotonin and
dopamine, which help us to feel good and focus on the task at hand. At the same
time, our body produces norepinephrine and cortisol. Norepinephrine and cortisol
work together to increase our heart rate, breath rate, and blood pressure, which in
turn increases the flow of oxygen and nutrients to our muscles. Thus, when we
are anxious, our body‚Äô way of protecting us is by increasing our awareness and
increasing the flow of nutrients and oxygen to our muscles, so we can run or fight
if needed. All of these processes occur so quickly because our body‚Äô systems are
already set up to respond instantly to threats.

34

