arXiv:1602.02672v1 [cs.AI] 8 Feb 2016

Learning to Communicate to Solve Riddles
with Deep Distributed Recurrent Q-Networks

Jakob N. Foerster1,‚Ä†
Yannis M. Assael1,‚Ä†
Nando de Freitas1,2,3
Shimon Whiteson1
1
University of Oxford, United Kingdom
2
Canadian Institute for Advanced Research, CIFAR NCAP Program
3
Google DeepMind

Abstract
We propose deep distributed recurrent Qnetworks (DDRQN), which enable teams of
agents to learn to solve communication-based coordination tasks. In these tasks, the agents are
not given any pre-designed communication protocol. Therefore, in order to successfully communicate, they must first automatically develop
and agree upon their own communication protocol. We present empirical results on two multiagent learning problems based on well-known
riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so. To our knowledge, this is the first time deep reinforcement
learning has succeeded in learning communication protocols. In addition, we present ablation
experiments that confirm that each of the main
components of the DDRQN architecture are critical to its success.

1. Introduction
In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional
robot control (Levine et al., 2015; Assael et al., 2015; Watter et al., 2015), visual attention (Ba et al., 2015), and the
Atari learning environment (ALE) (Guo et al., 2014; Mnih
et al., 2015; Stadie et al., 2015; Wang et al., 2015; Schaul
et al., 2016; van Hasselt et al., 2016; Oh et al., 2015; Bellemare et al., 2016; Nair et al., 2015).
The above-mentioned problems all involve only a single
learning agent. However, recent work has begun to address
multi-agent deep RL. In competitive settings, deep learn‚Ä†

These authors contributed equally to this work.

JAKOB . FOERSTER @ CS . OX . AC . UK
YANNIS . ASSAEL @ CS . OX . AC . UK
NANDODEFREITAS @ GOOGLE . COM
SHIMON . WHITESON @ CS . OX . AC . UK

ing for Go (Maddison et al., 2015; Silver et al., 2016) has
recently shown success. In cooperative settings, Tampuu
et al. (2015) have adapted deep Q-networks (Mnih et al.,
2015) to allow two agents to tackle a multi-agent extension to ALE. Their approach is based on independent Qlearning (Shoham et al., 2007; Shoham & Leyton-Brown,
2009; Zawadzki et al., 2014), in which all agents learn their
own Q-functions independently in parallel.
However, these approaches all assume that each agent can
fully observe the state of the environment. While DQN
has also been extended to address partial observability
(Hausknecht & Stone, 2015), only single-agent settings
have been considered. To our knowledge, no work on deep
reinforcement learning has yet considered settings that are
both partially observable and multi-agent.
Such problems are both challenging and important. In the
cooperative case, multiple agents must coordinate their behaviour so as to maximise their common payoff while facing uncertainty, not only about the hidden state of the environment but about what their teammates have observed and
thus how they will act. Such problems arise naturally in a
variety of settings, such as multi-robot systems and sensor
networks (Matari, 1997; Fox et al., 2000; Gerkey & Matari,
2004; Olfati-Saber et al., 2007; Cao et al., 2013).
In this paper, we propose deep distributed recurrent Qnetworks (DDRQN) to enable teams of agents to learn effectively coordinated policies on such challenging problems. We show that a naive approach to simply training independent DQN agents with long short-term memory
(LSTM) networks (Hochreiter & Schmidhuber, 1997) is inadequate for multi-agent partially observable problems.
Therefore, we introduce three modifications that are key to
DDRQN‚Äôs success: a) last-action inputs: supplying each
agent with its previous action as input on the next time step
so that agents can approximate their action-observation histories; b) inter-agent weight sharing: a single network‚Äôs

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

weights are used by all agents but that network conditions
on the agent‚Äôs unique ID, to enable fast learning while also
allowing for diverse behaviour; and c) disabling experience
replay, which is poorly suited to the non-stationarity arising from multiple agents learning simultaneously.

The optimal action-value function Q‚àó (s, a)
=
maxœÄ QœÄ (s, a) obeys the Bellman optimality equation:
h
i
‚àó 0 0
Q‚àó (s, a) = Es0 r + Œ≥ max
Q
(s
,
a
)
|
s,
a
. (3)
0

To evaluate DDRQN, we propose two multi-agent reinforcement learning problems that are based on well-known
riddles: the hats riddle, where n prisoners in a line must
determine their own hat colours; and the switch riddle, in
which n prisoners must determine when they have all visited a room containing a single switch. Both riddles have
been used as interview questions at companies like Google
and Goldman Sachs.

Deep Q-networks (Mnih et al., 2015) (DQNs) use neural
networks parameterised by Œ∏ to represent Q(s, a; Œ∏). DQNs
are optimised by minimising the following loss function at
each iteration i:

2 
Li (Œ∏i ) = Es,a,r,s0 yiDQN ‚àí Q(s, a; Œ∏i )
, (4)

While these environments do not require convolutional networks for perception, the presence of partial observability means that they do require recurrent networks to deal
with complex sequences, as in some single-agent works
(Hausknecht & Stone, 2015; Ba et al., 2015) and languagebased (Narasimhan et al., 2015) tasks. In addition, because
partial observability is coupled with multiple agents, optimal policies critically rely on communication between
agents. Since no communication protocol is given a priori, reinforcement learning must automatically develop a
coordinated communication protocol.
Our results demonstrate that DDRQN can successfully
solve these tasks, outperforming baseline methods, and discovering elegant communication protocols along the way.
To our knowledge, this is the first time deep reinforcement
learning has succeeded in learning communication protocols. In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.

2. Background
In this section, we briefly introduce DQN and its multiagent and recurrent extensions.
2.1. Deep Q-Networks
In a single-agent, fully-observable, reinforcement learning
setting (Sutton & Barto, 1998), an agent observes its current state st ‚àà S at each discrete time step t, chooses an
action at ‚àà A according to a potentially stochastic policy
œÄ, observes a reward signal rt , and transitions to a new state
st+1 . Its objective is to maximize an expectation over the
discounted return, Rt
Rt = rt + Œ≥rt+1 + Œ≥ 2 rt+2 + ¬∑ ¬∑ ¬∑ ,

(1)

where Œ≥ ‚àà [0, 1) is a discount factor. The Q-function of a
policy œÄ is:
QœÄ (s, a) = E [Rt |st = s, at = a] .

(2)

a

with target
Q(s0 , a0 ; Œ∏i‚àí ).
yiDQN = r + Œ≥ max
0
a

(5)

Here, Œ∏i‚àí are the weights of a target network that is frozen
for a number of iterations while updating the online network Q(s, a; Œ∏i ) by gradient descent. DQN uses experience
replay (Lin, 1993; Mnih et al., 2015): during learning, the
agent builds a dataset Dt = {e1 , e2 , . . . , et } of experiences
et = (st , at , rt , st+1 ) across episodes. The Q-network is
then trained by sampling mini-batches of experiences from
D uniformly at random. Experience replay helps prevent
divergence by breaking correlations among the samples. It
also enables reuse of past experiences for learning, thereby
reducing sample costs.
2.2. Independent DQN
DQN has been extended to cooperative multi-agent settings, in which each agent m observes the global st , selects an individual action am
t , and receives a team reward,
rt , shared among all agents. Tampuu et al. (2015) address this setting with a framework that combines DQN
with independent Q-learning, applied to two-player pong,
in which all agents independently and simultaneously learn
their own Q-functions Qm (s, am ; Œ∏im ). While independent Q-learning can in principle lead to convergence problems (since one agent‚Äôs learning makes the environment appear non-stationary to other agents), it has a strong empirical track record (Shoham et al., 2007; Shoham & LeytonBrown, 2009; Zawadzki et al., 2014).
2.3. Deep Recurrent Q-Networks
Both DQN and independent DQN assume full observability, i.e., the agent receives st as input. By contrast, in partially observable environments, st is hidden and instead the
agent receives only an observation ot that is correlated with
st but in general does not disambiguate it.
Hausknecht & Stone (2015) propose the deep recurrent Qnetwork (DRQN) architecture to address single-agent, partially observable settings. Instead of approximating Q(s, a)

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

with a feed-forward network, they approximate Q(o, a)
with a recurrent neural network that can maintain an internal state and aggregate observations over time. This can
be modelled by adding an extra input ht‚àí1 that represents
the hidden state of the network, yielding Q(ot , ht‚àí1 , a; Œ∏i ).
Thus, DRQN outputs both Qt , and ht , at each time step.
DRQN was tested on a partially observable version of ALE
in which a portion of the input screens were blanked out.
2.4. Partially Observable Multi-Agent RL
In this work, we consider settings where there are both multiple agents and partial observability: each agent receives
its own private om
t at each time step and maintains an internal state hm
t . However, we assume that learning can occur
in a centralised fashion, i.e., agents can share parameters,
etc., during learning so long as the policies they learn condition only on their private histories. In other words, we
consider centralised learning of decentralised policies.
We are interested in such settings because it is only when
multiple agents and partial observability coexist that agents
have the incentive to communicate. Because no communication protocol is given a priori, the agents must first automatically develop and agree upon such a protocol. To
our knowledge, no work on deep RL has considered such
settings and no work has demonstrated that deep RL can
successfully learn communication protocols.

3. DDRQN
The most straightforward approach to deep RL in partially observable multi-agent settings is to simply combine
DRQN with independent Q-learning, in which case each
m
m m
agent‚Äôs Q-network represents Qm (om
t , ht‚àí1 , a ; Œ∏i ),
which conditions on that agent‚Äôs individual hidden state as
well as observation. This approach, which we call the naive
method, performs poorly, as we show in Section 5.
Instead, we propose deep distributed recurrent Q-networks
(DDRQN), which makes three key modifications to the
naive method. The first, last-action input, involves providing each agent with its previous action as input to the
next time step. Since the agents employ stochastic policies
for the sake of exploration, they should in general condition their actions on their action-observation histories, not
just their observation histories. Feeding the last action as
input allows the RNN to approximate action-observation
histories.
The second, inter-agent weight sharing, involves tying the
weights of all agents networks. In effect, only one network
is learned and used by all agents. However, the agents can
still behave differently because they receive different observations and thus evolve different hidden states. In addition, each agent receives its own index m as input, mak-

Algorithm 1 DDRQN
Initialise Œ∏1 and Œ∏1‚àí
for each episode e do
hm
1 = 0 for each agent m
s1 = initial state, t = 1
while st 6= terminal and t < T do
for each agent m do
With probability  pick random am
t
m
m
m
else am
t = arg maxa Q(ot , ht‚àí1 , m, at‚àí1 , a; Œ∏i )
Get reward rt and next state st+1 , t = t + 1
‚àáŒ∏ = 0 . reset gradient
for j = t ‚àí 1 to 1, ‚àí1 do
for each agent m do
n r , if s terminal, else
j
yjm = j
‚àí
m
m
rj + Œ≥ maxa Q(om
j+1 , hj , m, aj , a; Œ∏i )
Accumulate gradients for:
m
m
m
2
(yjm ‚àí Q(om
j , hj‚àí1 , m, aj‚àí1 , aj ; Œ∏i ))
Œ∏i+1 = Œ∏i + Œ±‚àáŒ∏ . update parameters
‚àí
Œ∏i+1
= Œ∏i‚àí + Œ±‚àí (Œ∏i+1 ‚àí Œ∏i‚àí ) . update target network

ing it easier for agents to specialise. Weight sharing dramatically reduces the number of parameters that must be
learned, greatly speeding learning.
The third, disabling experience replay, simply involves
turning off this feature of DQN. Although experience replay is helpful in single-agent settings, when multiple
agents learn independently the environment appears nonstationary to each agent, rendering its own experience obsolete and possibly misleading.
Given these modifications, DDRQN learns a Q-function of
m
m
m
the form Q(om
t , ht‚àí1 , m, at‚àí1 , at ; Œ∏i ). Note that Œ∏i does
not condition on m, due to weight sharing, and that am
t‚àí1 is
a portion of the history while am
is
the
action
whose
value
t
the Q-network estimates.
Algorithm 1 describes DDRQN. First, we initialise the target and Q-networks. For each episode, we also initialise
m
the state, s1 , the internal state of the agents, hm
1 , and a0 .
Next, for each time step we pick an action for each agent
-greedily w.r.t. the Q-function. We feed in the previous
action, am
t‚àí1 , the agent index, m, along with the observam
tion om
t and the previous internal state, ht‚àí1 . After all
agents have taken their action, we query the environment
for a state update and reward information.
When we reach the final time step or a terminal state, we
proceed to the Bellman updates. Here, for each agent, m,
and time step, j, we calculate a target Q-value, yjm , using
the observed reward, rj , and the discounted target network.
We also accumulate the gradients, ‚àáŒ∏, by regressing the
m
m
Q-value estimate, Q(om
j , hj‚àí1 , m, aj‚àí1 , a; Œ∏i ), against the
m
target Q-value, yj , for the action chosen, am
j .

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

Lastly, we conduct two weight updates, first Œ∏i in the direction of the accumulated gradients, ‚àáŒ∏, and then the target
network, Œ∏i‚àí , in the direction of Œ∏i .

Although this riddle is a single action and observation problem it is still partially observable, given that none of the
agents can observe the colour of their own hat.

4. Multi-Agent Riddles

4.2. Switch Riddle

In this section, we describe the riddles on which we evaluate DDRQN.

The switch riddle can be described as follows: ‚ÄúOne hundred prisoners have been newly ushered into prison. The
warden tells them that starting tomorrow, each of them
will be placed in an isolated cell, unable to communicate
amongst each other. Each day, the warden will choose one
of the prisoners uniformly at random with replacement, and
place him in a central interrogation room containing only
a light bulb with a toggle switch. The prisoner will be able
to observe the current state of the light bulb. If he wishes,
he can toggle the light bulb. He also has the option of announcing that he believes all prisoners have visited the interrogation room at some point in time. If this announcement is true, then all prisoners are set free, but if it is false,
all prisoners are executed. The warden leaves and the prisoners huddle together to discuss their fate. Can they agree
on a protocol that will guarantee their freedom?‚Äù (Wu,
2002).

4.1. Hats Riddle
The hats riddle can be described as follows: ‚ÄúAn executioner lines up 100 prisoners single file and puts a red or a
blue hat on each prisoner‚Äôs head. Every prisoner can see
the hats of the people in front of him in the line - but not
his own hat, nor those of anyone behind him. The executioner starts at the end (back) and asks the last prisoner
the colour of his hat. He must answer ‚Äúred‚Äù or ‚Äúblue.‚Äù If
he answers correctly, he is allowed to live. If he gives the
wrong answer, he is killed instantly and silently. (While everyone hears the answer, no one knows whether an answer
was right.) On the night before the line-up, the prisoners
confer on a strategy to help them. What should they do?‚Äù
(Poundstone, 2012). Figure 1 illustrates this setup.
Answers:

‚ÄúRed‚Äù

‚ÄúRed‚Äù

?

Hats:
1

2

Action:

On

None

None

Tell

Prisoner :
in IR

3

2

3

1

Observed hats

3

Switch:
4

5

On

On

On

On

Off

Off

Off

Off

6

Day 1

Day 2

Day 3

Day 4

Prisoners:

Figure 1. Hats: Each prisoner can hear the answers from all preceding prisoners (to the left) and see the colour of the hats in front
of him (to the right) but must guess his own hat colour.

An optimal strategy is for all prisoners to agree on a communication protocol in which the first prisoner says ‚Äúblue‚Äù
if the number of blue hats is even and ‚Äúred‚Äù otherwise (or
vice-versa). All remaining prisoners can then deduce their
hat colour given the hats they see in front of them and the
responses they have heard behind them. Thus, everyone
except the first prisoner will definitely answer correctly.
To formalise the hats riddle as a multi-agent RL task, we
define a state space s = (s1 , . . . , sn , a1 , . . . , an ), where n
is the total number of agents, sm ‚àà {blue, red} is the mth agent‚Äôs hat colour and am ‚àà {blue, red} is the action it
took on the m-th step. At all other time steps, agent m can
only take a null action. On the m-th time step, agent m‚Äôs
observation is om = (a1 , . . . , am‚àí1 , sm+1 , . . . , sn ). Reward is zero except at the end of the episode, when it is
the total number of agents with the correct action: rn =
P
m
= sm ). We label only the relevant observation
m I(a
m
o and action am of agent m, omitting the time index.

Figure 2. Switch: Every day one prisoner gets sent to the interrogation room where he can see the switch and choose between
actions ‚ÄúOn‚Äù, ‚ÄúOff‚Äù, ‚ÄúTell‚Äù and ‚ÄúNone‚Äù.

A number of strategies (Song, 2012; Wu, 2002) have been
analysed for the infinite time-horizon version of this problem in which the goal is to guarantee survival. One wellknown strategy is for one prisoner to be designated the
counter. Only he is allowed to turn the switch off while
each other prisoner turns it on only once. Thus, when the
counter has turned the switch off n‚àí1 times, he can ‚ÄúTell‚Äù.
To formalise the switch riddle, we define a state space
s = (SWt , IRt , s1 , . . . , sn ), where SWt ‚àà {on, off} is
the position of the switch, IR ‚àà {1 . . . n} is the current
visitor in the interrogation room and s1 , . . . , sn ‚àà {0, 1}
tracks which agents have already been to the interrogation
room. At time step t, agent m observes om
t = (irt , swt ),
where irt = I(IRt = m), and swt = SWt if the agent
is in the interrogation room and null otherwise. If agent
m is in the interrogation room then its actions are am
t ‚àà
{‚ÄúOn‚Äù, ‚ÄúOff‚Äù, ‚ÄúTell‚Äù, ‚ÄúNone‚Äù}; otherwise the only action
is ‚ÄùNone‚Äù. The episode ends when an agent chooses ‚ÄúTell‚Äù
or when the maximum time step is reached. The reward rt

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks
LSTM unrolled for n-m steps
s m+1

m, n

sk

m, n

‚Ä¶

‚Ä¶
+

+

h sm+1

z ks

z ns

h ks

h 1a
z 1a

y ns

Qm

h ka

y am-1

z ka
+

z m-1
a
+

+

‚Ä¶

m, n

m, n

+

z m+1
s

a1

sn

‚Ä¶

ak

m, n

a m-1

m, n

LSTM unrolled for m-1 steps

Figure 3. Hats: Each agent m observes the answers, ak , k < m,
from all preceding agents and hat colour, sk in front of him,
k > m. Both variable length sequences are processed through
RNNs. First, the answers heard are passed through two singlelayer MLPs, zak = MLP(ak ) ‚äï MLP(m, n), and their outputs
are added element-wise. zak is passed through an LSTM network yak , hka = LSTMa (zak , hk‚àí1
). Similarly for the observed
a
k
k‚àí1
hats we define ysk‚àí1 , hk‚àí1
=
LSTM
). The last vals (zs , hs
s
m‚àí1
n
ues of the two LSTMs ya
and ys are used to approximate
Qm = MLP(yam‚àí1 ||ysn ) from which the action am is chosen.

64](m, n), and their outputs are added element-wise.
Subsequently, zak is passed through an LSTM network
yak , hka = LSTMa [64](zak , hk‚àí1
). We follow a similar proa
cedure for the n ‚àí m hats observed defining ysk , hks =
LSTMs [64](zsk , hk‚àí1
). Finally, the last values of the two
s
LSTM networks yam‚àí1 and ysn are used to approximate
the Q-Values for each action Qm = MLP[128 √ó 64, 64 √ó
64, 64 √ó 1](yam‚àí1 ||ysn ). The network is trained with minibatches of 20 episodes.
Furthermore, we use an adaptive variant of curriculum
learning (Bengio et al., 2009) to pave the way for scalable
strategies and better training performance. We sample examples from a multinomial distribution of curricula, each
corresponding to a different n, where the current bound is
raised every time performance becomes near optimal. The
probability of sampling a given n is inversely proportional
to the performance gap compared to the normalised maximum reward. The performance is depicted in Figure 5.
We first evaluate DDRQN for n = 10 and compare it with
tabular Q-learning. Tabular Q-learning is feasible only with
few agents, since the state space grows exponentially with
n. In addition, separate tables for each agent precludes generalising across agents.

is 0 except unless an agent chooses ‚ÄúTell‚Äù, in which case
it is 1 if all agents have been to the interrogation room and
‚àí1 otherwise.

Figure 4 shows the results, in which DDRQN substantially outperforms tabular Q-learning. In addition, DDRQN
also comes near in performance to the optimal strategy described in Section 4.1. This figure also shows the results of
an ablation experiment in which inter-agent weight sharing
has been removed from DDRQN. The results confirm that
inter-agent weight sharing is key to performance.

5. Experiments

Since each agent takes only one action in the hats riddle, it
is essentially a single step problem. Therefore, last-action

DDRQN w Tied Weights
DDRQN w/o Tied Weights
0.9
0.8
0.7
0.6
0.5

5.1. Hats Riddle
Figure 3 shows the architecture we use to apply DDRQN
to the hats riddle. To select am , the network is fed as
input om = (a1 , . . . , am‚àí1 , sm+1 , . . . , sn ), as well as
m and n. The answers heard are passed through two
single-layer MLPs, zak = MLP[1 √ó 64](ak ) ‚äï MLP[2 √ó

Q-Table
Optimal

1.0

Norm. R (Optimal)

In this section, we evaluate DDRQN on both multi-agent
riddles. In our experiments, prisoners select actions using
1
an -greedy policy with  = 1‚àí0.5 n for the hats riddle and
 = 0.05 for the switch riddle. For the latter, the discount
factor was set to Œ≥ = 0.95, and the target networks, as
described in Section 3, update with Œ±‚àí = 0.01, while in
both cases weights were optimised using Adam (Kingma &
Ba, 2014) with a learning rate of 1 √ó 10‚àí3 . The proposed
architectures make use of rectified linear units, and LSTM
cells. Further details of the network implementations are
described in the Supplementary Material and source code
will be published online.

20k

40k

60k

# Epochs

80k

100k

Figure 4. Results on the hats riddle with n = 10 agents, comparing DDRQN with and without inter-agent weight sharing to
a tabular Q-table and a hand-coded optimal strategy. The lines
depict the average of 10 runs and 95% confidence intervals.

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks
Table 1. Percent agreement on the hats riddle between DDRQN
and the optimal parity-encoding strategy.

Qm
1

Qm
2

% AGREEMENT

3
5
8
12
16
20

100.0%
100.0%
79.6%
52.6%
50.8%
52.5%

ym
2
hm
1

zm
1

Figure 6 illustrates the model architecture used in the
switch riddle. Each agent m is modelled as a recurrent
neural network with LSTM cells that is unrolled for Dmax
time-steps, where d denotes the number of days of the
episode. In our experiments, we limit d to Dmax = 4n ‚àí 6
in order to keep the experiments computationally tractable.
m
The inputs, om
t , at‚àí1 , m and n, are processed through
a 2-layer MLP ztm = MLP[(7 + n) √ó 128, 128 √ó

20k

60k

n=16 C.L.
n=20 C.L.

n=20
Optimal

Norm. R (Optimal)

1.0
0.9
0.8
0.7
0.6
40k

max

‚Ä¶

ym
t
hm
t

zm
3

m
yD

max

m
hD
-1
max

zm
t

m
zD

max

‚Ä¶

max

5.2. Switch Riddle

0.5

hm
2
zm
2

m
QD

m m
m
m
m
(sw1, ir 1 , a 0m, m, n) ‚Ä¶ (sw3, ir 3 , a 2 , m, n) ‚Ä¶ ( swD , ir D , a D -1, m, n)

We compare the strategies DDRQN learns to the optimal
strategy by computing the percentage of trials in which the
first agent correctly encodes the parity of the observed hats
in its answer. Table 1 shows that the encoding is almost
perfect for n ‚àà {3, 5, 8}. For n ‚àà {12, 16, 20}, the agents
do not encode parity but learn a different distributed solution that is nonetheless close to optimal. We believe that
qualitatively this solution corresponds to more the agents
communicating information about other hats through their
answers, instead of only the first agent.

n=8 C.L.
n=12 C.L.

ym
3

‚Ä¶

inputs and disabling experience replay do not play a role
and do not need to be ablated. We consider these components in the switch riddle in Section 5.2.

n=3 C.L.
n=5 C.L.

Qm
t
‚Ä¶

ym
1

N

Qm
3

80k 100k 120k 140k 160k

# Epochs

Figure 5. Hats: Using Curriculum Learning DDRQN achieves
good performance for n = 3...20 agents, compared to the optimal strategy.

max

max

Figure 6. Switch: Agent m receives as input: the switch
state swt , the room he is in, irtm , his last action, am
t‚àí1 ,
his ID, m, and the # of agents, n.
At each step,
the inputs are processed through a 2-layer MLP ztm =
MLP(swt , irtm , OneHot(am
t‚àí1 ), OneHot(m), n). Their embedding ztm is then passed to an LSTM network, ytm , hm
=
t
LSTM(ztm , hm
t‚àí1 ), which is used to approximate the agent‚Äôs
action-observation history. Finally, the output ytm of the LSTM
m
is used at each step to compute Qm
t = MLP(yt ).
m
128](om
Their embedt , OneHot(at‚àí1 ), OneHot(m), n).
m
=
ding zt is then passed an LSTM network, ytm , hm
t
),
which
is
used
to
approximate
the
LSTM[128](ztm , hm
t‚àí1
agent‚Äôs action-observation history. Finally, the output ytm
of the LSTM is used at each step to approximate the
Q-values of each action using a 2-layer MLP Qm
=
t
MLP[128 √ó 128, 128 √ó 128, 128 √ó 4](ytm ). As in the hats
riddle, curriculum learning was used for training.

Figure 7, which shows results for n = 3, shows that
DDRQN learns an optimal policy, beating the naive method
and the hand coded strategy, ‚Äútell on last day‚Äù. This verifies that the three modifications of DDRQN substantially
improve performance on this task. In following paragraphs
we analyse the importance of the individual modifications.
We analysed the strategy that DDRQN discovered for n =
3 by looking at 1000 sampled episodes. Figure 8 shows a
decision tree, constructed from those samples, that corresponds to an optimal strategy allowing the agents to collectively track the number of visitors to the interrogation
room. When a prisoner visits the interrogation room after
day two, there are only two options: either one or two prisoners may have visited the room before. If three prisoners
had been, the third prisoner would have already finished
the game. The two remaining options can be encoded via
the ‚Äúon‚Äù and ‚Äúoff‚Äù position respectively. In order to carry
out this strategy each prisoner has to learn to keep track
of whether he has visited the cell before and what day it
currently is.
Figure 9 compares the performance of DDRQN to a variant
in which the switch has been disabled. After around 3,500
episodes, the two diverge in performance. Hence, there is a
clearly identifiable point during learning when the prison-

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

Hand-coded

Oracle

DDRQN
w Disabled Switch

1.0

1.0

0.5

0.8

Norm. R (Oracle)

Norm. R (Oracle)

DDRQN
Naive Method

0.0
0.5

Hand-coded
Oracle

Split

0.6
0.4
0.2
0.0

1.0

10k

20k

30k

40k

# Epochs

50k

Figure 7. Switch: For n = 3 DDRQN outperforms the ‚ÄúNaive
Method‚Äù and a simple hand coded strategy, ‚Äútell on last day‚Äù,
achieving ‚ÄúOracle‚Äù level performance. The lines depict the average of 10 runs and the 95% confidence interval.

ers start learning to communicate via the switch. Note that
only when the switch is enabled can DDRQN outperform
the hand-coded ‚Äútell on last day‚Äù strategy. Thus, communication via the switch is required for good performance.
Figure 10 shows performance for n = 4. On most runs,
DDRQN clearly beats the hand-coded ‚Äútell on last day‚Äù
strategy and final performance approaches 90% of the oracle. However, on some of the remaining runs DDRQN
fails to significantly outperform the hand-coded strategy.
Analysing the learned strategies suggests that prisoners
typically encode whether 2 or 3 prisoners have been to the
room via the ‚Äúon‚Äù and ‚Äúoff‚Äù positions of the switch, respectively. This strategy generates no false negatives, i.e., when
the 4th prisoner enters the room, he always ‚ÄúTells‚Äù, but
generates false positives around 5% of the time. Example
strategies are included in the Supplementary Material.

10k

2

Yes

Off

No

On

Yes

None

No

Switch?

Has Been?

3+
Has Been?

On

Tell

Off

On

Figure 8. Switch: For n = 3, DDRQN manages to discover a perfect strategy, which we visualise as a decision tree in this Figure.
After day 2, the ‚Äúon‚Äù position of the switch encodes that 2 prisoners have visited the interrogation room, while ‚Äúoff‚Äù encodes that
one prisoner has.

40k

50k

modifications contribute substantially to DDRQN‚Äôs performance. Inter-agent weight sharing is by far the most important, without which the agents are essentially unable to
learn the task, even for n = 3. Last-action inputs also
play a significant role, without which performance does
not substantially exceed that of the ‚Äútell on last day‚Äù strategy. Disabling experience replay also makes a difference,
as performance with replay never reaches optimal, even after 50,000 episodes. This result is not surprising given the
non-stationarity induced by multiple agents learning in parallel. Such non-stationarity arises even though agents can
track their action-observation histories via RNNs within
a given episode. Since their memories are reset between
episodes, learning performed by other agents appears as
non-stationary from their perspective.

n=4 C.L

Hand-coded

Oracle

1.0

Norm. R (Oracle)

Day

On

30k

# Epochs

Figure 9. Switch: At 3.5k episodes the DDRQN line clearly separates from the performance line for the ‚Äúno switch‚Äù test and start
exceeding ‚Äútell on last day‚Äù. At this point the agents start to discover strategies that evolve communication using the switch. The
lines depict the mean of 10 runs and the 95% confidence interval.

Furthermore, Figure 11 shows the results of ablation experiments in which each of the modifications in DDRQN
is removed one by one. The results show that all three

1

20k

0.9
0.8
0.7
0.6
100k

200k

300k

# Epochs

400k

500k

Figure 10. Switch: 10 runs using curriculum learning for n =
3, 4. In most cases was able DDRQN to find strategies that outperform ‚Äútell on last day‚Äù for n = 4.

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

Norm. R (Oracle)

DDRQN
w/o Last Action
w/o Tied Weights

w Experience Replay
Hand-coded
Oracle

1.0
0.8
0.6
0.4
0.2
0.0
0.2
0.4
10k

20k

30k

# Epochs

40k

50k

Figure 11. Switch: Tied weights and last action input are key for
the performance of DDRQN. Experience replay prevents agents
from reaching Oracle level. The experiment was executed for
n = 3 and the lines depict the average of 10 runs and the 95%
confidence interval.

However, it is particularly important in communicationbased tasks like these riddles, since the value function for
communication actions depends heavily on the interpretation of these messages by the other agents, which is in turn
set by their Q-functions.

6. Related Work
There has been a plethora of work on multi-agent reinforcement learning with communication, e.g., (Tan, 1993;
Melo et al., 2011; Panait & Luke, 2005; Zhang & Lesser,
2013; Maravall et al., 2013). However, most of this work
assumes a pre-defined communication protocol. One exception is the work of Kasai et al. (2008), in which the
tabular Q-learning agents have to learn the content of a
message to solve a predator-prey task. Their approach is
similar to the Q-table benchmark used in Section 5.1. By
contrast, DDRQN uses recurrent neural networks that allow for memory-based communication and generalisation
across agents.
Another example of open-ended communication learning
in a multi-agent task is given in (Giles & Jim, 2002). However, here evolutionary methods are used for learning communication protocols, rather than RL. By using deep RL
with shared weights we enable our agents to develop distributed communication strategies and to allow for faster
learning via gradient based optimisation.
Furthermore, planning-based RL methods have been employed to include messages as an integral part of the
multi-agent reinforcement learning challenge (Spaan et al.,
2006). However, so far this work has not been extended to

deal with high dimensional complex problems.
In ALE, partial observability has been artificially introduced by blanking out a fraction of the input
screen (Hausknecht & Stone, 2015). Deep recurrent
reinforcement learning has also been applied to textbased games, which are naturally partially observable
(Narasimhan et al., 2015). Recurrent DQN was also successful in the email campaign challenge (Li et al., 2015).
However, all these examples apply recurrent DQN in
single-agent domains. Without the combination of multiple
agents and partial observability, there is no need to learn a
communication protocol, an essential feature of our work.

7. Conclusions & Future Work
This paper proposed deep distributed recurrent Q-networks
(DDRQN), which enable teams of agents to learn to solve
communication-based coordination tasks. In order to successfully communicate, agents in these tasks must first automatically develop and agree upon their own communication protocol. We presented empirical results on two
multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve
such tasks and discover elegant communication protocols
to do so. In addition, we presented ablation experiments
that confirm that each of the main components of the
DDRQN architecture are critical to its success.
Future work is needed to fully understand and improve
the scalability of the DDRQN architecture for large numbers of agents, e.g., for n > 4 in the switch riddle. We
also hope to further explore the ‚Äúlocal minima‚Äù structure
of the coordination and strategy space that underlies these
riddles. Another avenue for improvement is to extend
DDRQN to make use of various multi-agent adaptations of
Q-learning (Tan, 1993; Littman, 1994; Lauer & Riedmiller,
2000; Panait & Luke, 2005).
A benefit of using deep models is that they can efficiently
cope with high dimensional perceptual signals as inputs. In
the future this can be tested by replacing the binary representation of the colour with real images of hats or applying
DDRQN to other scenarios that involve real world data as
input.
While we have advanced a new proposal for using riddles
as a test field for multi-agent partially observable reinforcement learning with communication, we also hope that this
research will spur the development of further interesting
and challenging domains in the area.

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

8. Acknowledgements
This work was supported by the Oxford-Google DeepMind
Graduate Scholarship and the EPSRC.

Lauer, M. and Riedmiller, M. An algorithm for distributed
reinforcement learning in cooperative multi-agent systems. In ICML, 2000.

References

Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-toend training of deep visuomotor policies. arXiv preprint
arXiv:1504.00702, 2015.

Assael, J.-A. M, WahlstroÃàm, N., SchoÃàn, T. B., and Deisenroth, M. p. Data-efficient learning of feedback policies
from image pixels using deep dynamical models. arXiv
preprint arXiv:1510.02173, 2015.

Li, X., Li, L., Gao, J., He, X., Chen, J., Deng, L., and He,
J. Recurrent reinforcement learning: A hybrid approach.
arXiv preprint 1509.03044, 2015.

Ba, J., Mnih, V., and Kavukcuoglu, K. Multiple object
recognition with visual attention. In ICLR, 2015.
Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S.,
and Munos, R. Increasing the action gap: New operators
for reinforcement learning. In AAAI, 2016.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
Curriculum learning. In ICML, pp. 41‚Äì48, 2009.
Cao, Y., Yu, W., Ren, W., and Chen, G. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial Informatics,
9(1):427‚Äì438, 2013.
Fox, D., Burgard, W., Kruppa, H., and Thrun, S. Probabilistic approach to collaborative multi-robot localization. Autonomous Robots, 8(3):325‚Äì344, 2000.
Gerkey, B.P. and Matari, M.J. A formal analysis and taxonomy of task allocation in multi-robot systems. International Journal of Robotics Research, 23(9):939‚Äì954,
2004.
Giles, C. L. and Jim, K. C. Learning communication for
multi-agent systems. In Innovative Concepts for AgentBased Systems, pp. 377‚Äì390. Springer, 2002.
Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X.
Deep learning for real-time Atari game play using offline
Monte-Carlo tree search planning. In NIPS, pp. 3338‚Äì
3346. 2014.
Hausknecht, M. and Stone, P.
Deep recurrent Qlearning for partially observable MDPs. arXiv preprint
arXiv:1507.06527, 2015.
Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997.
Kasai, T., Tenmoto, H., and Kamiya, A. Learning of communication codes in multi-agent reinforcement learning
problem. In IEEE Conference on Soft Computing in Industrial Applications, pp. 1‚Äì6, 2008.
Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Lin, L.J. Reinforcement learning for robots using neural networks. PhD thesis, School of Computer Science,
Carnegie Mellon University, 1993.
Littman, M. L. Markov games as a framework for multiagent reinforcement learning. In International Conference on Machine Learning (ICML), pp. 157‚Äì163, 1994.
Maddison, C. J., Huang, A., Sutskever, I., and Silver, D.
Move Evaluation in Go Using Deep Convolutional Neural Networks. In ICLR, 2015.
Maravall, D., De Lope, J., and Domnguez, R. Coordination of communication in robot teams by reinforcement
learning. Robotics and Autonomous Systems, 61(7):661‚Äì
666, 2013.
Matari, M.J. Reinforcement learning in the multi-robot domain. Autonomous Robots, 4(1):73‚Äì83, 1997.
Melo, F. S., Spaan, M., and Witwicki, S. J. QueryPOMDP:
POMDP-based communication in multiagent systems.
In Multi-Agent Systems, pp. 189‚Äì204. 2011.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,
Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518
(7540):529‚Äì533, 2015.
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C.,
Fearon, R., Maria, A. De, Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih,
V., Kavukcuoglu, K., and Silver, D. Massively parallel methods for deep reinforcement learning. In Deep
Learning Workshop, ICML, 2015.
Narasimhan, K., Kulkarni, T., and Barzilay, R. Language understanding for text-based games using deep reinforcement learning. In EMNLP, 2015.
Oh, J., Guo, X., Lee, H., Lewis, R. L., and Singh,
S. Action-conditional video prediction using deep networks in Atari games. In NIPS, pp. 2845‚Äì2853, 2015.

Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks

Olfati-Saber, R., Fax, J.A., and Murray, R.M. Consensus
and cooperation in networked multi-agent systems. Proceedings of the IEEE, 95(1):215‚Äì233, 2007.

Wang, Z., de Freitas, N., and Lanctot, M. Dueling network
architectures for deep reinforcement learning. arXiv
preprint 1511.06581, 2015.

Panait, L. and Luke, S. Cooperative multi-agent learning:
The state of the art. Autonomous Agents and Multi-Agent
Systems, 11(3):387‚Äì434, 2005.

Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. A. Embed to control: A locally linear latent
dynamics model for control from raw images. In NIPS,
2015.

Poundstone, W. Are You Smart Enough to Work at
Google?: Fiendish Puzzles and Impossible Interview
Questions from the World‚Äôs Top Companies. Oneworld
Publications, 2012.

Wu, W. 100 prisoners and a lightbulb. Technical report,
OCF, UC Berkeley, 2002.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In ICLR, 2016.

Zawadzki, E., Lipson, A., and Leyton-Brown, K. Empirically evaluating multiagent learning algorithms. arXiv
preprint 1401.8074, 2014.

Shoham, Y. and Leyton-Brown, K. Multiagent Systems:
Algorithmic, Game-Theoretic, and Logical Foundations.
Cambridge University Press, New York, 2009.

Zhang, C. and Lesser, V. Coordinating multi-agent reinforcement learning with limited communication. volume 2, pp. 1101‚Äì1108, 2013.

Shoham, Y., Powers, R., and Grenager, T. If multi-agent
learning is the answer, what is the question? Artificial
Intelligence, 171(7):365‚Äì377, 2007.
Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L.,
van den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,
D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of Go with deep neural
networks and tree search. Nature, 529(7587):484‚Äì489,
2016.
Song, Y. 100 prisoners and a light bulb. Technical report,
University of Washington, 2012.
Spaan, M., Gordon, G. J., and Vlassis, N. Decentralized
planning under uncertainty for teams of communicating
agents. In International joint conference on Autonomous
agents and multiagent systems, pp. 249‚Äì256, 2006.
Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing exploration in reinforcement learning with deep predictive
models. arXiv preprint arXiv:1507.00814, 2015.
Sutton, R. S. and Barto, A. G. Introduction to reinforcement learning. MIT Press, 1998.
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente, R. Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1511.08779, 2015.
Tan, M. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In ICML, 1993.
van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double Q-learning. In AAAI, 2016.

