An Outlook on the Opportunities and Challenges of
Multi-Agent AI Systems
Fangqiao Tian1 , An Luo1 , Jin Du1 , Xun Xian2 , Robert Specht1 , Ganghua Wang3 ,
Xuan Bi4 , Jiawei Zhou5 , Ashish Kundu6 , Jayanth Srinivasa6 , Charles Fleming6 ,
Rui Zhang7 , Zirui Liu8 , Mingyi Hong2 , Jie Ding1

arXiv:2505.18397v3 [cs.MA] 24 Aug 2025

1

2

School of Statistics, University of Minnesota
Department of Electrical & Computer Engineering, University of Minnesota
3
Data Science Institute, University of Chicago
4
Carlson School of Management, University of Minnesota
5
Department of Applied Mathematics & Statistics, Stony Brook University
6
Cisco Research
7
Department of Surgery, University of Minnesota
8
Department of Computer Science & Engineering, University of Minnesota

Abstract
A multi-agent AI system (MAS) is composed of multiple autonomous agents that
interact, exchange information, and make decisions based on internal generative
models. Recent advances in large language models and tool-using agents have
made MAS increasingly practical in areas like scientific discovery and collaborative automation. However, key questions remain: When are MAS more effective
than single-agent systems? What new safety risks arise from agent interactions?
And how should we evaluate their reliability and structure? This paper outlines a
formal framework for analyzing MAS, focusing on two core aspects: effectiveness
and safety. We explore whether MAS truly improve robustness, adaptability, and
performance, or merely repackage known techniques like ensemble learning. We
also study how inter-agent dynamics may amplify or suppress system vulnerabilities. While MAS are relatively new to the signal processing community, we
envision them as a powerful abstraction that extends classical tools like distributed
estimation and sensor fusion to higher-level, policy-driven inference. Through
experiments on data science automation, we highlight the potential of MAS to
reshape how signal processing systems are designed and trusted.

1

Introduction

Multi-agent AI systems (MAS) have emerged as a leading paradigm for addressing complex tasks
beyond the capabilities of single-agent solutions. Reflecting the growing industrial adoption of MAS,
the global AI agents market is expected to surge from roughly $3.66 billion in 2023 to $139.12 billion
by 2033, with a compound annual growth rate (CAGR) of about 43.88% [1]. Industry analyses further
indicate MAS could significantly impact the global economy, potentially increasing global GDP
by over 25% within the next decades and establishing MAS as the basis of the next trillion-dollar
AI market. Recent open-source frameworks such as AutoGen [2], AutoGPT [3], and AGNTCY [4]
showcased diverse and successful implementations of agent communication and task assignment.
The concept of multi-agent systems has its roots in decades of interdisciplinary research. Game
theory [5] provided formal frameworks to analyze strategic interactions among rational agents. Information theory, driven by seminal works from Shannon [6] and Kolmogorov [7], provide mechanisms
crucial for encoding, transmission, and signal interpretation among agents. Cybernetics [8] highlighted the critical role of feedback loops for adaptive control. Early robotics experiments, such as
those conducted by W. Grey Walter [9], demonstrated the potential for autonomous interactions within
Preprint.

Human Interactions

Physical Environment

Action

Feedback

Application Layer

Knowledge Base
(Memory)

Orchestrator
(Manager)

Agent 1 (Planner)

Modeling Layer

Execution

Optimization

Agent 2 (Executor)
Agent 3 (Monitor)

Multi-agent Framework

Computation Layer
GPU/CPU/XPU

Figure 1: Architecture of a multi-agent AI system with three interconnected layers: (1) Application
Layer, interacting dynamically with human users and physical environments through feedback
and actions; (2) Modeling Layer, featuring specialized agents (Planner, Executor, and Monitor)
coordinated by an orchestrator and a knowledge base; and (3) Computation Layer, comprising
hardware infrastructures (GPU/CPU/XPU) for executing and optimizing agent-driven computations.

dynamic environments. Later, machine learning, particularly deep reinforcement learning [10–13],
further advanced these concepts.
Despite the rapid development and deployment of MAS, a formal definition of MAS remains
elusive, especially as new agent modalities and interactive structures continually emerge. Prior
surveys have extensively covered traditional multi-agent learning and coordination theory [14–16].
However, they typically focus on the classical MAS perspective, insufficient for modern large-scale
needs. Such theories often predate or loosely address today’s generative, LLM-based multi-agent AI
paradigms. Although some recent overviews have reviewed multi-agent AI systems [17, 18], they
mostly summarize existing frameworks and benchmarks without explicitly defining what multi-agent
actually means, nor contrasting multi-agent and single-agent solutions in a principled manner.
The resulting conceptual ambiguity around MAS leaves crucial questions regarding their effectiveness
and safety unanswered. To bridge this gap, our paper advances the following position: Designing
better MAS fundamentally requires understanding under which precise conditions MAS significantly
outperform single-agent systems, and how multi-agent setups influence or exacerbate existing safety
concerns. We therefore examine these two pivotal dimensions, effectiveness and safety. Our goal is
to stimulate principled research towards robust and trustworthy multi-agent AI systems.

2

Definition and Formulation of Multi-Agent AI Systems

We present a formal framework to describe how autonomous agents operate and learn within a
shared system. Specifically, Section 2.1 introduces notations and key concepts of multi-agent AI
systems (MAS). Section 2.2 discusses how feedback can be incorporated to iteratively update MAS.
Section 2.3 further extends this framework to model the internet of agentic systems in open-world,
networked environments, inspired by the recently proposed AGNTCY architecture [19].
2.1

Core Elements

An MAS consists of a set of autonomous agents {i ∈ I} that interact through a dynamic communication graph and collectively process inputs over time. Each agent operates via a probabilistic
2

mapping from prior internal state and current input to new state and output. We start by defining the
fundamental building block of the MAS: the individual AI agent.
Definition 1 (AI Agent). An AI agent i ∈ I is defined by a tuple (Si , Xi , Yi , pi ), where:
• Si is the agent’s internal state space representing its memory or context.
• Xi is the input space defined as a tuple combining outputs from other agents and
exogenous feedback:
n
o

(t)
(t−1)
(t)
xi =
yj
| j ∈ V (t−1) , (j → i) ∈ E (t−1) , Fi
(t−1)

where yj

denotes outputs from agent j at the previous time step.

(t)

• Fi represents exogenous feedback (e.g., signals or human inputs from the physical
environment) received by the agent at time t.
• Yi is the output space representing all actions or messages the agent may produce.
• pi is a transition kernel updating the agent’s internal state and output:


(t) (t)
(t−1)
(t)
(si , yi ) ∼ pi · | si
, xi ,
(t)

(t)

where si ∈ Si and yi ∈ Yi are the internal state and output at time t, respectively,
(t−1)
(t)
given the previous state si
and the current input xi ∈ Xi .
Remark 1. We track discrete time t so that feedback events can trigger updates to connections and
agent states in a systematic manner. In simpler cases, one might omit time dependence to study only
static or batch-style multi-agent interactions.
(t)

Remark 2. Each agent’s input xi can incorporate outputs from its connected agents in the last
(t)
round and exogenous feedback, namely yjt−1 for j ∈ I and Fi .
(t)

(t−1)

Remark 3 (On flexibility of input timing). We define that xi uses yj
as input to ensure that all
agents update at time t are causally well-posed and can be computed in parallel.
However, in systems where a partial or total update order is required (for example, an agent topology
(t)
structured as DAG), inputs may include some outputs yj from agent j that are updated strictly
before agent i at time t. In this case, we define
n ′
o

(t)
(t )
(t)
yj | t′ ∈ {t − 1, t}, j ≺ i if t′ = t , Fi
xi =
where the update precedence relation j ≺ i is any valid topological order of agent updates.
Given these individual agents, we next formalize how they interact by defining the concept of dynamic
multi-agent topology.
Definition 2 (Multi-Agent Topology). An multi-agent topology at time t is a directed graph
G(t) = (V (t) , E (t) ), where:
• V (t) ⊆ V is the subset of agents active at time t.
• E (t) ⊆ V (t) ×V (t) is the set of directed edges at time t. A directed edge (j → i) ∈ E (t)
indicates that agent i observes the output of agent j at step t.
A graph update function ϕ maps the previous graph, internal states, and inputs to the current
graph, thereby evolving the topology over time:


G(t) = ϕ G(t−1) , {(sk , xk )}k∈V (t−1) .
Combining these agents and their evolving interactions, we now define the complete structure of a
Multi-Agent AI System.
3

Agent

Agent

Internal State
Update via

Agent

Input

Internal State

Multi-agent Topology at time
Agent

at time

AI Agent

Multi-agent Topology at time

Agent

Agent

Output

Figure 2: Core update and topology dynamics in an MAS. Left: Illustration of multi-agent
(t−1)
(t−1)
topologies at time t−1 and t, showing how agent k receives messages yi
and yj
from
connected agents and how the topology evolves via a graph update function ϕ. Right: Update
mechanism for agent k at time t−1, where it updates its internal state based on received inputs and
(t−1)
produces output yk
.
Definition 3 (Multi-Agent AI System). An multi-agent AI system (MAS) is defined by a tuple:


I, {(Si , Xi , Yi , pi )}i∈I , {(G(0) , ϕ)} ,
where:
• I is an index set of agents in the system.
• Each agent i ∈ I is defined by (Si , Xi , Yi , pi ) as per Definition 1.
• G(0) is the initial topology, and ϕ is the graph update function introduced in Definition 2.
Based on the above definitions, and intuitively speaking, the MAS evolves over time by running the
following sequence of operations at each time step t:
1. Each agent i ∈ V (t−1) receives input
n
o

(t)
(t−1)
(t)
xi =
yj
| j ∈ V (t−1) , (j → i) ∈ E (t−1) , Fi
.
2. Each agent updates its internal state and generates an output according to its transition kernel
pi :
(t)

(t)

(t−1)

(si , yi ) ∼ pi (· | si

(t)

, xi ).

3. The topology evolves according to the graph update function ϕ:


G(t) = ϕ G(t−1) , {(sk , xk )}k∈V (t−1) .
The iteration continues until a predefined stopping criterion is satisfied. The outputs produced by the
MAS can be evaluated based on the specified evaluation metrics.
This proposed concise framework accommodates both symbolic and neural agents, and serves as the
foundational structure for all subsequent modeling and experiments presented in this paper. Figure 2
illustrates the core update and topology dynamics in an MAS.
Below, we exemplify the definitions of core elements in our framework with a concrete use case on
flight booking.

4

Definitions in Practice: A Flight Booking Use Case
Consider MAS designed for flight booking, consisting of the following agents:
• Agent 1 (User Interface) has an internal state S1 (e.g., cached user queries), takes
(t)
user-provided input x1 containing origin, destination, travel dates, and preferences,
(t)
and outputs structured query data y1 .
• Agent 2 (Flight Search) maintains an internal state S2 (e.g., cached flight availabil(t)
ity), receives structured query input x2 from Agent 1, and outputs specific flight
(t)
options y2 , including airline, flight number, departure time, and price.
• Agent 3 (Payment Processing) holds internal state S3 (e.g., previous transaction
(t)
data), receives input x3 consisting of selected flight details from Agent 2 and
(t)
payment information (exogenous feedback F3 ), and outputs payment confirmations
(t)
or transaction failures y3 .
• Agent 4 (Promotion), dynamically activated by the graph update function ϕ, main(t)
tains an internal state S4 (e.g., user loyalty information), takes input x4 consisting
of confirmed booking details, determines promotion availability, and outputs adver(t)
tisement y4 .
(t)

Each agent i operates by updating its state Si and generating output yi according to the
(t)
probabilistic transition kernel pi based on inputs xi defined above. The MAS topology
G(t) may form a directed chain, where Agent 1 forwards structured requests to Agent 2,
and Agent 2 passes selected flight options to Agent 3 for payment processing. The graph
update function ϕ dynamically adjusts this topology, for instance, activating Agent 4 to handle
user-requested upgrades.
In the next section, we show how feedback mechanisms can further enrich this fundamental formulation by enabling agents to adjust their transition kernels pi or the topology G(t) in response to
changing performance needs or adversarial threats.
2.2

Feedback-Driven Updates

Building on our formulation in Section 2.1, we highlight the central role of feedback in enabling the
dynamic adaptability that distinguishes multi-agent AI systems from their single-agent counterparts.
Rather than treating each agent’s function pi as static, feedback can be incorporated to adjust the
(t)
input-output relationship, effectively transforming pi into a time-dependent function pi .
(t)

How Feedback Updates Agents’ Generation. At each time step t, an agent i updates its state si
(t)
(t)
and output yi based on its input xi , which consists of past agent outputs and exogenous feedback:


(t) (t)
(t−1)
(t)
pi si , yi | si
, xi .
(t)

(t)

Since xi includes exogenous feedback Fi , receiving new feedback effectively conditions the
transition kernel on additional information. This conditioning refines the agent’s belief about how to
update its state and generate outputs, relative to when no feedback is incorporated. A natural way
to represent this refinement is through Bayesian updating. Consider the transition kernel without
feedback as a prior:
(t) (t)
(t−1)
(t)
(t) 
pprior
si , yi | si
, xi \ Fi ,
i
(t)

(t)

(t−1)

| j ∈ V (t−1) , (j → i) ∈ E (t−1) } contains only the output from agents
(t)
(t)
(t)
(t) 
connected to agent i at time t − 1, and xi = xi \ Fi , Fi . This transition kernel updates
(t)
the state and output when there is no exogenous feedback. When the feedback Fi arrives, it is

where xi \ Fi

= { yj

5

incorporated through the Bayes rule:






(t) (t)
(t−1)
(t)
(t)
(t) (t)
(t−1)
(t)
(t)
(t)
(t)
(t)
si , yi | si
, xi \ Fi
,
pi si , yi | si
, xi
∝ p Fi | yi , xi \ Fi
· pprior
i
(t)

(t)

(t)

(t)

(t)

(t)

(t)

(t−1)

(t)

(t)

where we assume that p(Fi | yi , xi \ Fi ) = p(Fi | si , yi , si
, xi \ Fi ), i.e., the
(t)
feedback depends only on the agent’s output and input and not on its internal states. p(Fi |
(t)
(t)
(t)
(t)
yi , xi \ Fi ) measures how plausible the observed feedback is given output yi and input
(t)
(t)
(t−1)
(t)
xi \ Fi . The resulting posterior pi (· | si
, xi ) exactly updates with feedback.
How Feedback Impacts Multi-Agent Topology. In a dynamic multi-agent AI system, feedback
plays a crucial role in modifying not only individual agent functions but also the structure of interagent communication. When certain agents fail to function properly, produce low-quality outputs, or
even generate adversarial responses, feedback mechanisms help redirect interactions to more reliable
agents. This adaptability ensures that the system remains functional even in the presence of unreliable
agents.
As previously defined, the MAS topology evolves based on received feedback:


(t)
G(t+1) = ϕ G(t) , {(sk , yk )}k∈V (t) , {Fi }i∈I ,
where the function ϕ determines how agent interactions evolve in response to feedback. If feedback
indicates improved cooperation between certain agents, new edges may be added to G(t) , enhancing
connectivity. Conversely, if feedback identifies unreliable agents, connections may be pruned,
preventing their influence from degrading overall system performance. By dynamically adjusting the
topology, feedback enables the MAS to maintain robustness and efficiency in evolving environments.
In the example below, we illustrate the feedback-driven updates in a multi-robot warehouse scenario.
Practical Scenario: Multi-Robot Warehouse
Consider a warehouse managed by a multi-agent AI system, where autonomous robots
(t)
(agents) collaborate to pick and place items efficiently. Each robot i updates its state si ,
(t)
(t)
processes input xi , and generates output yi according to:


(t) (t)
(t−1)
(t)
pi si , yi | si
, xi .
(t)

(t)

Since xi includes exogenous feedback Fi , the transition kernel pi is conditioned on
additional information, refining the robot’s decisions and task coordination.
(t)

• Robot 1 (Inventory Scanner) maintains an internal state s1 tracking inventory
(t)
data. It receives feedback F1 (e.g., new item arrivals or restock alerts). Its updated
(t)
output y1 provides an item availability list.
(t)

(t)

(t)

(t)

• Robot 2 (Item Picker) takes input x2 = (y1 , F2 ), where y1 encodes inventory
(t)
status and F2 includes human operator modifications or system reassignments. It
(t)
updates its belief over available items and produces y2 listing the items picked.
(t)

(t)

(t)

(t)

• Robot 3 (Quality Checker) receives x3 = (y2 , F3 ), where y2 contains picked
(t)
items and F3 consists of sensor-based quality assessments. It adjusts its quality
(t)
classification, outputting y3 as either "approve" or "reject."
(t)

(t)

(t)

(t)

• Robot 4 (Order Dispatcher) processes x4 = (y3 , F4 ), where y3 provides
(t)
inspected items and F4 contains external constraints (e.g., urgent orders). It
(t)
dynamically updates shipment scheduling and outputs y4 determining whether an
order is shipped or held.
Impact on Topology. Initially, the MAS topology G(t) follows a directed sequence, where
tasks flow from Robot 1 to Robot 4. However, upon receiving real-time feedback (e.g.,

6

damage alerts, shipment delays), the graph update function ϕ dynamically modifies agent
connections. For instance:
• If Robot 3 identifies defective items, ϕ reroutes them to Robot 2 for re-picking,
altering G(t) .
(t)

• If urgent orders are received (F4 ), Robot 4 may directly request prioritization from
Robot 2, bypassing non-essential steps.
By iteratively conditioning the transition kernel on feedback and adjusting topology via ϕ,
the MAS adapts to operational changes, improving efficiency and coordination.

Public Cloud

Private Cloud
Tools/APIs

Data

Agent
Gateway

MCP Server

ACP

ACP

AC
P

MCP

Agent Directory

MCP

MCP

MCP Server

MCP Server

Private Cloud

Private Cloud

Tools/APIs

Data

Tools/APIs

Data

Figure 3: Extension of MAS to Internet of MAS. A networked environment where agents can be
registered, discovered, and composed much like modular web services with standardized interfaces
and protocols. Specifically, agents register with an Agent Directory, analogous to the Domain Name
System (DNS), and connect using the Agent Connect Protocol (ACP) [19], similar to service-level
routing. Calls to external tools are structured via the Model Context Protocol (MCP) [20] and
executed through an MCP Server, like HTTP requests via an API gateway. All communication flows
through the Agent Gateway, which functions as a secure proxy analogous to HTTPS gateways or
service meshes.
2.3

Extending MAS for Open and Networked Environments: Internet of MAS

The introduced MAS in Section 2.1 and Section 2.2 describe how agents interact and evolve over
time within a closed environment, such as a government agency, company, or research lab. However,
in many real-world settings, agents are deployed across different systems and need to interact with
others outside their original environment [21–23]. This raises new challenges such as how MAS can
find each other, connect securely, and collaborate across system boundaries.
To address these challenges, we aim to extend the MAS framework to support open, networked
environments where MAS from different systems can communicate and work together. This forms the
basis of what we call the Internet of MAS. To support this vision, the AGNTCY architecture [19] was
recently proposed, offering standardized protocols and components for MAS registration, discovery,
7

coordination, and external tool access. Inspired by this design, we incorporate key mechanisms into
our framework, as illustrated in Figure 3.
To understand this extension, consider a simple analogy: think of each agent as a computer on the
Internet, and each MAS as a local area network (LAN) or autonomous system (AS). For agents within
and across MAS to interoperate, they must follow steps similar to how Internet-connected devices
discover, communicate, and collaborate. First, agents must be made visible to other MAS. This is
enabled by the Agent Directory, which functions like the Domain Name System (DNS) or a search
engine, allowing MAS agents to be registered, discovered, and queried across boundaries. Next, when
agents attempt to connect, the Agent Connect Protocol (ACP) defines their communication structure
and network topology, much like DNS-based routing supports Internet-level service discovery. When
MAS agents need to invoke external tools or services, this is handled via the Model Context Protocol
(MCP), which defines standard message formats (analogous to HTTP or gRPC). These messages are
routed through an MCP Server, which selects and executes the correct tool. Finally, all inter-MAS
communication flows through an Agent Gateway, which serves as a secure proxy that enforces access
policies, routes connections, and enables cloud-level function access. Each of these components
supports MAS-level communication and collaboration over open, decentralized networks.
Below, we provide a mathematical formalization of the key components in the Internet of MAS. We
first define Agent Directory, where agents can be registered, queried, and discovered based on their
functional description.
Definition 4 (Agent Directory). Let I be a set of agent identifiers. An Agent Directory is a map
D : I → M,
where each D(i) ∈ M contains structured metadata describing the input/output interface,
declared capabilities, and version of agent i.
Once agents are available on the Agent Directory, ACP determines which agents can communicate
with others based on whether the output message formats of one agent are accepted as input by
another.
Definition 5 (Agent Connect Protocol (ACP)). Let V ⊆ I be a set of agents, where each agent
i ∈ V declares:
• an output type set Yi (the types of messages it can emit),
• an input type set Xi (the types of messages it can consume).
The Agent Connect Protocol (ACP) defines a directed graph G = (V, E), where
(j → i) ∈ E

iff Yj ∩ Xi ̸= ∅.

While ACP governs agent-to-agent communication, MCP addresses how agents interact with external
tools and services. Each MCP message includes a header that specifies which tool to call, and a body
that provides the required parameters. The MCP Server reads the header to select the correct tool and
passes the body to it as input.
Definition 6 (Model Context Protocol and Server). Let M = H × B be the MCP message
space, where:
• H is the space of headers (e.g., task type, auth token),
• B is the space of message bodies (e.g., structured parameters).
An MCP Server is a function:
Smcp : M → O,

defined as Smcp (h, b) := fh (b),

where fh ∈ F is the function specified by header h.
Finally, the Agent Gateway serves as the central checkpoint for inter-agent communication across
systems. It enforces security and access control by determining whether messages between agents
are allowed to pass, based on predefined system policies.
8

Definition 7 (Agent Gateway). An Agent Gateway controls whether a message between two
agents is allowed to pass. It uses a simple rule:

1 allowed, if the message is permitted by system policies
Π(src, dst, m) =
0 otherwise
If Π returns 1, the gateway applies a processing function
G(src, dst, m) = m′ ,
where:
• src ∈ I is the sending agent,
• dst ∈ I is the receiving agent,
• m ∈ M is the original message (e.g., an MCP message),
• m′ ∈ M is the processed message after optional formatting, filtering, or routing.
If Π returns 0, the message is blocked and not delivered.

3

Is Multi-Agent More Effective Than Single-Agent?

The adoption of MAS has been widely explored in various fields, with proposed benefits including
improved computational efficiency [24], enhanced adaptability [25], and greater robustness [26].
However, whether MAS is fundamentally more effective than single-agent systems remains an open
question. Regarding the underlying mechanisms that may contribute to MAS advantages, different
perspectives exist. Some studies argue that decomposing tasks among multiple agents enables parallel
processing, thereby reducing computational complexity [27]. Others suggest that distributed decisionmaking improves robustness against agent failures [28], while additional work highlights the role
of self-organization in dynamic task allocation [29]. Despite these theoretical advantages, MAS
does not universally outperform single-agent approaches. Coordination overhead [15], conflicting
agent objectives [30], and suboptimal communication structures [13] can introduce inefficiencies.
Identifying the fundamental principles that govern when MAS is effective and when it fails is critical
for designing robust and scalable multi-agent architectures.
This motivates the research question: Under what conditions does a multi-agent AI system outperform
a single-agent one, and what are the principles that govern its success or failure? To evaluate this
question, we frame our analysis around three progressively connected perspectives: task allocation,
robustness, and feedback integration. Task allocation considers how MAS can dynamically divide
and coordinate tasks more effectively than single agents, drawing from divide-and-conquer strategies.
Robustness examines whether these benefits hold under uncertainty and failure, relating closely to
ensemble learning principles. Feedback integration explores how MAS adapt over time through
internal and external signals, leveraging Bayesian approaches. These perspectives form a layered
framework for assessing when and why MAS can outperform single-agent systems. See Figure 4 for
an illustration of the three perspectives.
3.1

Perspective One: The Potential Benefit of Multi-Agent AI Systems in Task Allocation

Our analysis of task allocation is a reminiscence of the classical “divide-and-conquer” paradigm.
This paradigm addresses complex computational problems by decomposing them into smaller
subproblems, solving each independently, and then combining the results [31]. Traditionally, subtasks
are determined in advance and assigned to computational agents in a static manner [15, 30]. While this
strategy works effectively in stable and predictable environments, it often struggles under dynamic
conditions such as workload fluctuations, agent performance variations, or external disruptions [29].
Multi-agent AI systems (MAS) address the limitations in “divide-and-conquer” by allowing agents
to dynamically adjust task allocations based on real-time feedback and environmental changes [26].
Instead of relying on fixed task partitions, MAS adaptively refine the distribution of workloads
among agents in response to current conditions. This capability naturally leads to a central question:
9

Task Allocation

Robustness

Orchestrator

Aggregator

Feedback
Human

Successful

Evolves

Human
Agent 1

Agent 2

Fails

Agent 3

Figure 4: Overview of our position on effectiveness of MAS. Nodes represent agents. Left: MAS
can dynamically allocate tasks across agents to adapt to varying requirements. Middle: MAS may
or may not exhibit robustness. In the upper example, even if some agents fail, the system can still
succeed through a vote aggregator. In contrast, the lower example shows a case where the failure of
any single agent causes the entire system to fail. Right: MAS benefits from both external feedback
(e.g., from humans) and internal feedback via inter-agent communication. Feedback can even drive
the system to evolve its topology.
Can MAS autonomously identify optimal task distributions in real-time without predefined problem
decompositions?
Answering this question shifts the focus from static problem divisions to dynamic, iterative decisionmaking. Agents continuously revise their task assignments using ongoing feedback on their own
performance and environmental state [12, 13]. Thus, the main challenge for MAS becomes effectively implementing dynamic task reallocation strategies that balance responsiveness, flexibility, and
computational efficiency [26].
A New Opportunity: Self-Organized Task Allocation in MAS Beyond Human-Specified Decompositions. In traditional systems, humans have to decide in advance how to divide a complex
task into smaller subtasks and assign them to different agents. This approach works well in stable
environments. But in dynamic settings, where workloads shift, agents perform differently over time,
or external feedback changes, fixed task assignment often becomes inefficient or even fails.
In this work, we propose a new perspective: can MAS learn how to divide and assign tasks on its
own, without relying on any human-specified decomposition? This capability, if realized, would allow
MAS to handle environments that are too complex or unpredictable for human pre-programming.
We describe the structure of an MAS as a dynamic interaction graph G(t) = (V (t) , E (t) ), where nodes
are agents and edges describe communication links. Each agent receives input from its neighbors and
the environment, updates its internal state, and outputs a response:
(t)

(t)

(t)

(t)

(si , yi ) ∼ pi (si , yi

(t−1)

| si

(t)

, xi ).

Unlike static systems, the interaction structure G(t) can change over time. The system uses a graph
update rule:
(t−1)
(t−1)
G(t) = ϕ(G(t−1) , {(sk
, xk
)}k∈V (t−1) ),
so that agents can rewire their communication links based on performance, feedback, or needs.
The dynamic adaptability allows MAS to go beyond fixed task splits. Instead of following a hardcoded divide-and-conquer scheme, agents self-organize based on real-time feedback. The system
iteratively explores and reshapes its internal structure to better match task demands — something that
would be infeasible through human pre-specification or static programming.
An Underexplored Challenge: Error Amplification in Self-Organizing Task Allocation. The
ability of MAS to adaptively organize and rewire their task assignments could be powerful, but it
also comes with risks. In traditional systems with fixed task assignment, errors tend to stay local.
However, the dynamic structure of MAS means that a single mistake can quickly spread through the
10

entire system. To illustrate this, consider the previous setting where agents continuously update their
communication links based on performance and feedback. While this helps the system adapt to new
conditions, it can also lead to fragile coordination if not handled carefully. For example, suppose
agent h becomes a central connector, or “hub”, that many other agents depend on for information.
If h makes a mistake, its output may mislead every agent connected to it. Formally, the input to a
dependent agent j can be written as:
(t−1)

(t)

xj = {yh

(t−1)

} ∪ {yk

(t−1)

| k ∈ Nj

(t)

\ {h}} ∪ Fj .

Even if only h fails, its error may be passed down to many others in the next step. Over time, these
small mistakes can snowball into system-wide failures.
This raises a critical challenge: how can we design MAS that not only adapt quickly but also remain
robust to individual agent failures? Unlike static architectures, where failures remain localized, in
MAS, poor connectivity design can amplify errors system-wide. Thus, a future research direction is
to develop mechanisms that optimize ϕ not only for efficiency but also for stability, ensuring that the
benefits of adaptive task allocation do not come at the cost of increased fragility.
3.2

Perspective Two: The Limits of Multi-Agent AI Systems in Robustness

Beyond the benefit from dynamic topology design in MAS, another widely held belief is that MAS are
inherently more reliable than single-agent systems. A common intuition is that redundancy improves
reliability: if one agent fails, others can take over its role. By having multiple agents capable of
handling similar tasks, the system is expected to tolerate isolated errors more effectively. This idea
draws inspiration from ensemble learning in machine learning [32, 33], where multiple models (often
termed “weak learners”) are combined to improve overall decision accuracy. Classical ensemble
methods such as Bagging [34], Boosting [35], and Random Forests [36] leverage diverse predictions
to mitigate individual errors, theoretically leading to more stable performance.
A Common Belief: Redundancy Makes MAS Robust by Majority Voting. To analyze the
robustness from redundancy, consider an MAS composed of k agents, where each agent gi independently makes a binary decision yi ∈ {0, 1}. Suppose each agent correctly predicts the true label with
probability 1 − ei , where ei represents the error rate. The system aggregates individual decisions via
a majority vote function:
(
F (y1 , y2 , . . . , yk ) =

Pk
1, if i=1 Xi ≥ k2 ,
0, otherwise,

where Xi ∼ Bernoulli(1 − ei ) indicates whether agent gi predicts correctly.
By Hoeffding’s inequality [37], the probability of an erroneous final decision is bounded as:
k

P

1
1X
Xi <
k i=1
2

!

(



1
≤ exp −2k (1 − ē) −
2

2 )

Pk
where ē = k1 i=1 ei is the average agent error rate. This suggests that as k increases, the system’s
robustness improves exponentially, but only under the assumption that agent errors are independent.
The Fundamental Problem: Assumptions Underlying the Robustness Could Fail. The theoretical
result above suggests that adding more agents improves robustness. However, this relies on the
assumption that the training data behind agents have a certain level of independence and the training
objective is somewhat aligned. This leads to a central question: When does redundancy actually help,
and when does it fail? In the following, we highlight two common failure modes, high dependency
among agents and misalignment of goals, that can undermine the expected benefits of redundancy.
Failure Mode 1: High Dependency Among Agents.
The majority vote strategy assumes that agents make independent errors. But in practice, agents may
share training data, interact closely, or operate in similar environments. These factors can lead to
highly correlated decisions.
11

Suppose agents gi and gj make decisions with correlation λ. Then their agreement is:
q
p
E[Xi Xj ] = (1 − ei )(1 − ej ) + λ (1 − ei )ei · (1 − ej )ej
When λ → 1, their predictions become nearly identical, and the group behaves like a single agent. In
this case, increasing the number of agents does not improve robustness. Instead, it may just reinforce
the same bias. This problem is common in large-scale MAS trained on similar data sources.
Failure Mode 2: Misalignment of Goals.
Even if agent failures are independent, robustness may still fail if a subset of agents actively work
against the system’s objective. Consider a MAS where agents are divided into two groups:
{g1 , . . . , gk0 }
|
{z
}

∪

Aligned Group (correct w.p. 1−e0 )

{gk +1 , . . . , gk }
| 0 {z
}

.

Misaligned Group (correct w.p. 1−e1 )

If k ≫ k0 or e1 ≫ e0 , then the misaligned group dominates the final decision, nullifying the benefits
of redundancy. Specifically, the probability of an incorrect majority vote follows:

P

k0
X
i=1

Xi +

k
X
i=k0 +1

k
Xi <
2

!
,

which increases significantly as k1 grows larger or as misaligned agents deliberately introduce
errors. Unlike independent errors, misalignment leads to systematic failure, which redundancy cannot
mitigate.
The Challenges in Real-World: Overlapping Training Data. In practice, the above two key
failure modes: high dependency and misalignment of goals, often stem from a common root cause:
overlapping training data. Unlike traditional ensemble learning, which promotes diversity through
data resampling, MAS agents are frequently trained on large-scale datasets with substantial content
similarity. For instance, recent studies [38, 39] indicate that LLMs often share significant portions of
training data, commonly drawn from open-source corpora such as The Pile, Common Crawl, and
StarCoder [40]. This overlap can induce systematic correlations in agents’ outputs. In the extreme
case where all agents are trained on the same data, MAS effectively collapses to a single-agent system.
These observations challenge the prevailing belief that the diversity of LLMs inherently enhances
robustness in MAS.
3.2.1

Empirical Study: Redundancy by Ensemble Voting on Overlapping Training Data

To evaluate whether redundancy in MAS genuinely enhances robustness, we conduct a controlled
experiment using ensemble-based majority voting over independently trained agents. Specifically,
we use the Covtype dataset from scikit-learn, a commonly used classification dataset. Each
agent is trained on a small subset of data, and we vary two key parameters: Overlap Ratio ρ ∈ [0, 1]:
The proportion of each agent’s training data that overlaps with others. Higher ρ implies stronger
similarity between agents due to common data sources. Number of Agents k: The ensemble size,
i.e., the number of individually trained agents that will participate in majority voting to construct
the final decision. We simulate k agents, each trained on 100 samples (with shared and unique
portions controlled by ρ), and evaluate the ensemble performance via majority voting with random
tie-breaking.
Figure 5 summarizes our experimental results. The left panel plots ensemble accuracy against the
overlap ratio ρ, under varying ensemble sizes k ∈ {1, 5, 7, 11, 15}. We observe a clear trend: for all
k > 1, ensemble accuracy consistently declines as ρ increases. This demonstrates that excessive data
overlap across agents undermines the benefits of ensembling. In particular, when ρ = 1, each agent
is trained on exactly the same data, and the ensemble behaves like a single model repeated k times.
As in our theoretical discussion, the ensemble in this setting fails to gain robustness, since agents
make highly correlated decisions. This is an empirical instance of the failure mode where ρ → 1
leads to collapsed diversity. The k = 1 curve serves as a baseline that reflects the performance of
12

1
5
7
11
15

0
0.1
0.8

Figure 5: Ensemble accuracy under varying overlap and ensemble size. Left: Accuracy
decreases with overlap ratio ρ for k > 1, showing that high overlap reduces diversity and harms
ensemble performance. When ρ = 1, all agents behave similarly, matching the k = 1 baseline. Right:
When ρ = 0, accuracy improves clearly with larger k, illustrating the benefit of redundancy under
independent errors. With increasing ρ, this improvement diminishes due to reduced diversity.

a single-agent system. This comparison illustrates that increased redundancy does not translate to
improved robustness when diversity among agents is compromised by overlapping training data.
The right panel of Figure 5 plots ensemble accuracy against the number of agents k, under three
different overlap ratios ρ ∈ {0, 0.1, 0.8}. When ρ = 0, agents are trained on fully disjoint datasets,
and the resulting predictions are approximately independent. In this setting, we observe a clear and
steady increase in ensemble accuracy as k grows, accompanied by a noticeable reduction in the
variance (error bars) of the prediction accuracy. This behavior reflects improved robustness due to
aggregation of independent decisions and matches the guarantee provided by Hoeffding’s inequality.
Hence, under minimal data overlap, redundancy in MAS exhibits its strongest benefits: increasing
both accuracy and stability of the system. In comparison, when ρ = 0.1 or ρ = 0.8, agents share a
non-negligible portion of their training data, and their predictions become partially correlated. As a
result, especially when ρ = 0.8, the accuracy improvements with increasing k are reduced or even
saturated. While overlap ratio is not a direct measure of prediction correlation, it serves as a practical
proxy. These results highlight that the robustness gains from redundancy are most effective when
agent decisions are close to independent, which in practice requires careful control over training data
overlap.
These results show that adding more agents only improves robustness when their predictions are not
too similar. In real-world MAS, if agents are trained on highly overlapping data, they tend to make
similar mistakes, and voting does not help. To truly benefit from redundancy, it is important to ensure
enough diversity in how agents are trained.
3.3

Perspective Three: Challenge of Incorporating Feedback in Multi-Agent AI Systems

MAS offer a special advantage over single-agent models by leveraging structured feedback loops to
refine decision-making dynamically. Unlike single-agent systems, where feedback primarily influences a single trajectory, MAS enables agents to condition their updates based on both endogenous
and exogenous feedback, leading to improved adaptability and collaboration. This conditioning
mechanism allows for continuous refinement of agent decisions, making MAS inherently more
flexible than single-agent systems.
Feedback not only refines agent behavior but also reshapes the MAS topology. If feedback signals
indicate improved performance, edges can be reinforced, improving cooperation among agents.
Conversely, poor feedback can prompt topology pruning, preventing unreliable agents from negatively
influencing the system. This adaptability ensures robust coordination in MAS, setting it apart from
static single-agent systems.
13

The Fundamental Challenge: Effective Feedback Integration. A central question in MAS is
how to effectively integrate feedback. To analyze this, we classify feedback into the following two
types: endogenous feedback and exogenous feedback. Endogenous feedback is internal agent-toagent signals, such as an agent detecting a failure in its task (e.g., a code-writing agent producing
an error that affects subsequent agents). Exogenous feedback is human interventions or external
environmental signals, which remain largely underexplored in MAS but have been studied extensively
in reinforcement learning with human feedback (RLHF) [41–44].
For endogenous feedback, the simplest approach for integration is direct propagation: the agent
(t)
(t+1)
appends its output yi to the next agent’s input xi+1 , allowing downstream agents to adjust
accordingly. This mechanism aligns well with the Bayesian conditioning perspective discussed
earlier.
For exogenous feedback, integration is less straightforward. MAS feedback mechanisms can be
viewed as generalizing RLHF in MAS with our formulation. Let θ denote the policy parameters,
and D denote the observed human feedback data, RLHF typically updates policies using human
preference data by applying an update rule of the form:
p(θ|D) ∝ p(D|θ) · p(θ),
where the likelihood function p(D|θ) is often modeled as an exponential function of a reward model:
p(D|θ) ∝ e

P

i λi ri (θ)

.

This form mirrors the Bayesian conditioning update in MAS, where e−loss in RLHF plays a role
(t)
(t)
(t)
(t)
similar to the likelihood function p(Fi | yi , xi \ Fi ) in MAS feedback updates. While
existing RLHF methods focus on single-agent learning, our formulation suggests that MAS feedback
mechanisms can be viewed as generalizing these approaches.
Despite its potential, the effective application of RLHF in MAS remains underexplored. Existing
literature on multi-agent reinforcement learning highlights structured inter-agent feedback mechanisms [45], but integrating human feedback in MAS settings requires novel approaches. Notably, the
challenge of real-time feedback propagation across multiple agents introduces additional complexities
compared to standard RLHF frameworks.
The Pitfall of Poor Feedback Integration: Reducing to a Single-Agent System. If feedback
is not effectively injected into MAS, the system risks degenerating into a single-agent paradigm.
Formally, consider an MAS where agent interactions follow a directed graph G(t) = (V (t) , E (t) ). If
feedback updates do not sufficiently alter agent transitions, then for all agents i, ϕ(G(t+1) ) ≈ G(t) ,
which effectively collapses the system into a static single-agent model. Thus, ensuring that feedback
dynamically influences ϕ is crucial for preserving the advantages of MAS.
To fully leverage feedback in MAS, future research should address the following aspects. First, it
is essential to develop advanced techniques for aggregating multi-source, multi-modality feedback,
dynamically reconciling both exogenous human input and endogenous agent signals. Second, the
extension of RLHF frameworks from single-agent to multi-agent contexts will be critical for ensuring
that MAS align with multi-dimensional human values or preferences [46]. Finally, future MAS
architectures could be designed to enable interpretable agent-to-agent feedback propagation.
Effectiveness Comparison of Two-Agent and Single-Agent Systems: A Data Science
Case
Consider a collaborative data science assistant implemented in two different ways:
• Single-Agent System: A single agent attempts to design and execute the entire data
science pipeline, including preprocessing, modeling, and evaluation.
• Two-Agent System: The task is decomposed into two stages, handled by distinct
agents that exchange feedback.
Two-Agent Decomposition:
• Agent 1 (Workflow Proposal): Proposes a data science workflow y1 based on task
description and data metadata (such as suggesting imputation + random forest +
cross-validation).

14

• Agent 2 (Execution and Refinement): Executes the proposed pipeline, evaluates
performance, and refines the workflow y2 by tuning hyperparameters or modifying
components.
(t)

(t)

Each agent i maintains an internal state si , receives input xi , and outputs yi . The MAS
topology G(t) is sequential: Agent 1 proposes a workflow passed to Agent 2 for refinement
and finalization.
Cost Comparison:
• Single-Agent Cost: The agent must explore a large joint space of pipelines and
parameters:
Costsingle = csingle · E[Stepsmulti-dim ],
where csingle is the cost per step (e.g., running a full pipeline), and Stepsmulti-dim is
the expected number of steps needed for end-to-end optimization.
• Two-Agent Cost: Task decomposition reduces the search space:
Costtwo-agent = c1 · E[Stepsy1 ] + (c1 · E[Stepsy1 ] + c2 ) · E[Stepsy2 |y1 ] + δcomm ,
where c1 and c2 are the per-step costs for proposing and executing pipelines respectively, and δcomm captures the overhead of exchanging and interpreting workflow
proposals.
Conditions for Cost Reduction: The two-agent system is advantageous when:
• Step Reduction:
E[Stepsy1 ] + E[Stepsy2 |y1 ] ≪ E[Stepsmulti-dim ].
• Low Communication Overhead:
δcomm ≪ c2 · E[Stepsy2 |y1 ].
Conclusion: The two-agent strategy improves efficiency when decomposition reduces the
pipeline search dimensionality, and the communication cost between agents remains small.
The cost advantage is determined by:
∆Cost = Costsingle − Costtwo-agent > 0.
This holds when:
c1 + c2 + δcomm /E[Stepsy2 |y1 ]
E[Stepsmulti-dim ]
≫
.
E[Stepsy1 ] + E[Stepsy2 |y1 ]
csingle
This example illustrates how dividing a data science task into proposal and refinement stages
enables more efficient search and decision-making under constrained computational budgets.

4

Safety in Multi-Agent AI Systems

As AI systems become more complex and widely deployed, ensuring their safety has emerged
as a critical challenge. In single-agent settings, safety issues such as adversarial attacks [47–49],
jailbreaking attacks [50, 51], data or model privacy [52–55], and especially backdoor attacks [56–58]
have been extensively studied and partially mitigated. As these foundational safety efforts mature, an
essential extension involves moving beyond single-agent settings to scenarios where multiple agents
operate within a shared environment. This shift inevitably heightens complexity, as interactions
between agents may introduce new failure modes or exacerbate existing vulnerabilities.
Unlike single-agent architectures, where failures tend to remain localized, MAS exhibit complex
interdependencies that can propagate or even amplify existing vulnerabilities [59]. These interdependencies significantly complicate efforts to ensure robustness, reliability, and security across the entire
system. This raises a critical question:
15

Do multi-agent AI systems amplify or mitigate safety risks compared to single-agent
systems, and under what circumstances?
Next, we introduce a framework to study these questions, present toy experiments, and use backdoor
attacks to illustrate how different MAS can impact amplification or attenuation of vulnerabilities.
4.1

Formalizing Vulnerability Propagation in MAS

General Notion of Vulnerability. A single-agent system F : X → Y is said to exhibit a vulnerability
if there is a small perturbation δ, either in the input or model configuration, such that
V(F ; P, δ) := M(F ; P) − M((F ; P)δ )
is substantially large. Here, M(F ; P) = EX∼P [M (F (X), X)] is a task-specific performance metric,
the larger the better. The perturbation δ may reflect changes in the input space (e.g., adversarial
examples, trigger injections) or model structure (e.g., poisoned weights, latent neuron triggers). The
notation M((F ; P)δ ) denotes the performance of the system after perturbation δ is applied.
Extension to Multi-Agent Pipelines. In MAS, multiple agents {g1 , g2 , . . . , gn } may process data
sequentially or in parallel, where each gi : Xi → Xi+1 . The full system forms a pipeline: FMAS =
gn ◦ · · · ◦ g1 . The propagation of vulnerabilities across such a pipeline becomes a key concern:
earlier agents may amplify or mitigate vulnerabilities based on how they transform or filter data. Let
Fsingle = B be a baseline single-agent model and Fmulti = B ◦ A be a two-agent pipeline. Then,
under the same perturbation scenario (e.g., input or model-based), if
M(Fmulti ; P) < M(Fsingle ; P),
we say that agent A amplifies the vulnerability. If the inequality is reversed, A attenuates the
vulnerability.
To explain this further, we introduce the concept of directional alignment in feature space. Let ∆A
be the direction in which agent A transforms or maps data, and ∆B be the direction to which agent
B is most sensitive (e.g., vulnerable). The alignment score ⟨∆A , ∆B ⟩ indicates how likely A’s
transformation pushes data toward B’s vulnerable zone. Positive alignment indicates amplification,
while orthogonality or negative alignment suggests attenuation. The work by Wang et al. [58] provides
supporting evidence for this insight into the single-agent backdoor problem. This insight generalizes
to deeper pipelines: when all ∆i directions reinforce each other, vulnerability compounds; when
directions interfere destructively, malicious signals may be filtered out.
Empirical Simulation: Propagation of Attribution Errors in Two-Agent Pipelines. We design
a hypothetical simulation inspired by multi-modal single-cell experiments [60]. In such biological
settings, some cells are measured with both RNA expression and electrophysiological signals (Agent
1), while others only have RNA and cell-type labels (Agent 2). Due to biological relevance, genes
important for predicting signal strength in Agent 1 are often also informative for predicting cell type
in Agent 2. This motivates a multi-agent collaboration strategy in which Agent 1 performs feature
selection on large-scale multi-modal data, and Agent 2 reuses the selected features for downstream
classification. Figure 6 illustrates this setup.
To test the robustness of this feature transfer, we compare three approaches across 10 independent
simulations: Clean RF: Agent 1 selects features via Random Forest on clean data (X1 , ysignal ),
which are then used by Agent 2 for cell-type classification. Corrupted RF: Agent 1’s input features
are corrupted by weakening the signal-bearing genes (via scaling) and injecting Gaussian noise.
Agent 2 uses the features selected from this corrupted dataset. All Features: Agent 2 uses all 1000
features without selection. Agent 1 is trained on 1000 samples and Agent 2 on only 200, simulating a
common high-dimensional, low-sample regime in single-cell analysis. The downstream performance
(Macro-F1) of Agent 2 is summarized in table 1.
Although the regression output y remains unchanged, corruption of X1 severely degrades Agent 1’s
feature attribution. This leads to substantial overlap reduction and downstream performance drop.
In this example, we illustrate that multi-agent pipelines can silently propagate upstream errors,
resulting in worse performance than a single-agent baseline.
This case highlights a key risk in MAS pipelines: errors made by upstream agents can covertly bias
downstream agents, even without adversarial intent. Unlike traditional backdoor attacks, these errors
emerge from structural misalignment rather than explicit poisoning.
16

Figure 6: Illustration of our experiment on propagation of attribution errors in a two-agent system for
cell-type classification. Top: Clean RNA input is passed to Agent 1 for feature selection. Selected
features produced by Agent 1 are passed to Agent 2 for classification. Bottom: Corrupted RNA input
is passed to Agent 1 for feature selection. Corrupted selected features produced by Agent 1 magnify
this corruption, and are passed to Agent 2 for classification. The corruption is further magnified in
the final performance.
Setting
Mean Macro-F1

Clean RF

Corrupted RF

All Features

0.461

0.330

0.380

Table 1: Mean Macro-F1 performance of Agent 2 under different feature sources.

4.2

Impact of Topology on Backdoor Amplification and Attenuation

The previous sections formalized how vulnerabilities in MAS, such as backdoor attacks, can propagate
and either amplify or attenuate depending on directional alignment between agents. However, a key
underexplored factor in this propagation is the topology of the multi-agent AI system itself. That is,
beyond the behavior between any two agents, the system’s overall graph structure can fundamentally
shape how perturbations travel and interact.
Understanding the role of topology is crucial for two reasons. First, it governs the extent to which a
single malicious signal can spread or get suppressed. Second, many real-world MAS operate under
fixed or constrained topologies (e.g., star or cascade), making topological robustness a critical design
constraint. Empirical studies such as [58, 61, 62] have shown that identical agent vulnerabilities
can lead to vastly different outcomes depending on whether agents operate in centralized (star)
or sequential (cascade) architectures. However, formal characterizations of how such topological
differences result in varied outcomes are still limited. We illustrate these effects in the following
using two canonical MAS topologies, star and cascade, and formalize their backdoor vulnerability
propagation using performance metrics.
Star Topology. In a star topology, each agent gi : Xi → Yi processes its input independently, and
their outputs are aggregated by a central function h. The full system is:

Fstar (X1 , . . . , Xm ) = h g1 (X1 ), g2 (X2 ), . . . , gm (Xm ) .
If multiple agents share similar backdoor vulnerabilities—such as reacting to a common trigger
νX —then their outputs may correlate, causing the aggregated output to reinforce the malicious effect.
Formally, amplification occurs if:
M(Fstar ; P) < max M(gi ; P).
1≤i≤m

This condition indicates that the MAS performs worse than its weakest agent under attack, due to
compounded vulnerabilities.
Conversely, attenuation occurs if the agents are diverse in their vulnerabilities or the aggregator is
robust (e.g., via voting or median):
M(Fstar ; P) > max M(gi ; P),
1≤i≤m

meaning the system as a whole mitigates backdoor effects better than any individual agent.
17

Cascade Topology.
pipeline:

In a cascade topology, each agent’s output feeds into the next, forming a

Xi+1 = gi (Xi ), for i = 1, . . . , m − 1,
with X1 ∼ (1 − α)µX + ανX . The full pipeline becomes:
Fcascade (X1 ) = gm ◦ gm−1 ◦ · · · ◦ g1 (X1 ).
Cascade structures can amplify vulnerabilities when early-stage transformations align with later
agents’ trigger directions (i.e., when ⟨∆i , ∆i+1 ⟩ > 0 repeatedly). Amplification arises when:
M(Fcascade ; P) ≪ M(g1 ; P),
signifying severe degradation relative to the first agent’s performance.
On the other hand, attenuation emerges when intermediate agents disrupt the signal path, scrambling
or filtering malicious directions. Formally:
M(Fcascade ; P) > M(g1 ; P),
implying that sequential transformations dilute the backdoor effect.

5

Conclusion

This paper presents a formal and systematic framework for analyzing multi-agent AI systems (MAS),
with a focus on effectiveness and safety. We establish mathematical definitions for agent interactions,
dynamic topologies, and feedback mechanisms, and extend the classical MAS formulation to opennetwork settings via the Internet of MAS.
On the effectiveness side, we identify three key factors: task allocation, robustness, and feedback.
First, we show that MAS can outperform single-agent systems when tasks can be flexibly divided and
reallocated among agents in response to real-time feedback. Second, redundancy among agents can
also enhance robustness, but only when their training data is sufficiently diverse. Our experiments
demonstrate that high overlap in training data undermines the benefit of ensembling by making agent
decisions highly correlated. Third, we show that structured feedback, both from humans and between
agents, enables MAS to adapt not only their own behaviors but also their topologies over time.
On the safety side, we formalize how vulnerabilities, such as backdoor attacks, can propagate or
amplify within MAS due to agent interdependence. Our analysis highlights that both the directional
alignment between agents and the topology of the multi-agent system influence how vulnerabilities
propagate, either amplifying or attenuating errors across the system. Empirical simulations reveal that
upstream errors or misaligned objectives can silently degrade downstream performance, sometimes
making multi-agent pipelines more fragile than their single-agent counterparts.
Overall, this work provides a structured foundation for understanding and evaluating MAS from
the perspectives of effectiveness and safety. We hope this framework can inform and inspire future
research toward building more capable and reliable multi-agent AI systems. Moving forward, several
important directions remain to be explored. These include developing formal metrics for interagent influence and failure propagation, designing coordination protocols that are robust to agent
misalignment, and ensuring the safe deployment of MAS in open, heterogeneous environments.
In particular, maintaining trust and cooperation across organizational boundaries continues to be a
significant challenge for real-world applications.

References
[1] Market.US. Global AI agents market size, share, upcoming investments report agent type, by
agent system, region and companies - industry segment outlook, market assessment, competition
scenario, trends and forecast 2024-2033. Online; accessed 8 February 2025, 2024. https:
//market.us/report/ai-agents-market/.
[2] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun
Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger,
and Chi Wang. AutoGen: Enabling next-gen LLM applications via multi-agent conversations.
In First Conference on Language Modeling, 2024.
18

[3] Significant Gravitas. AutoGPT: Build, Deploy, and Run AI Agents. https://github.com/
Significant-Gravitas/AutoGPT, 2023. Accessed: 2025-04-14.
[4] AGNTCY – Building Infrastructure for the Internet of Agents. https://agntcy.org/, 2025.
Accessed: 2025-08-21.
[5] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, Princeton, NJ, USA, 1944.
[6] Claude E. Shannon. A mathematical theory of communication. The Bell System Technical
Journal, 27:379–423, 623–656, 1948.
[7] A. N. Kolmogorov. Three approaches to the quantitative definition of information. Problems of
Information Transmission, 1(1):1–7, 1965.
[8] Norbert Wiener. Cybernetics: or Control and Communication in the Animal and the Machine.
MIT Press, Cambridge, MA, USA, 1948.
[9] W. G. Walter. An imitation of life. Scientific American, 182(5):42–45, 1950.
[10] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the Eleventh International Conference on Machine Learning, pages 157–163,
1994.
[11] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare,
Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529–533, 02 2015. doi: 10.1038/nature14236.
[12] Jakob Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, 2016.
[13] Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In Advances in Neural
Information Processing Systems, 2017.
[14] Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic,
and Logical Foundations. Cambridge University Press, Cambridge, UK, 2008.
[15] Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning
perspective. In Autonomous Robots, pages 345–383, 2000.
[16] Michael Wooldridge. An Introduction to MultiAgent Systems. John Wiley & Sons, Chichester,
UK, 2nd edition, 2009.
[17] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf
Wiest, and Xiangliang Zhang. Large Language Model Based Multi-agents: A Survey of Progress
and Challenges. In Proceedings of the Thirty-Third International Joint Conference on Artificial
Intelligence, pages 8048–8057, 2024.
[18] Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. LLM
Multi-Agent Systems: Challenges and Open Problems. ArXiv, abs/2402.03578, 2024.
[19] Vijay Pandey.
The Internet of Agents: an open, interoperable internet for agentagent and agent-human quantum-safe communication. Technical report, Cisco Outshift,
2025. URL https://outshift-headless-cms-s3.s3.us-east-2.amazonaws.com/
Internet_of_Agents_Whitepaper.pdf. Technical report (white paper), accessed 202508-22.
[20] Anthropic. Model Context Protocol (MCP). https://modelcontextprotocol.io/, 2024.
accessed 2025-08-22.
19

[21] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and
Chaozhuo Li. Beyond self-talk: A communication-centric survey of llm-based multi-agent
systems. arXiv preprint arXiv:2502.14321, 2025.
[22] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and
Yong Li. Large language models empowered agent-based modeling and simulation: a survey
and perspectives. Humanities and Social Sciences Communications, 11(1):1259, 2024.
[23] Harrison Chase. Langchain: A framework for developing applications with llms. https:
//github.com/langchain-ai/langchain, 2022.
[24] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous
Agents and Multi-Agent Systems, 11(3):387–434, 2005.
[25] Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, pages 330–337, 1993.
[26] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik,
Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh,
Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou,
Max Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard,
David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang,
Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney,
Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris
Apps, and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
learning. Nature, 575:350 – 354, 2019.
[27] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systemszhe, pages 2137–2145, 2016.
[28] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A survey and critique of multiagent
deep reinforcement learning. Autonomous Agents and Multiagent Systems, 33(6):750–797,
2019.
[29] Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multiagent reinforcement learning in sequential social dilemmas. Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems, pages 464–473, 2017.
[30] Yoav Shoham and Kevin Leyton-Brown. Multiagent Systems: Algorithmic, Game-Theoretic,
and Logical Foundations. Cambridge University Press, New York, NY, 2009.
[31] Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. The design and analysis of computer
algorithms. 1974.
[32] Thomas G. Dietterich. Ensemble methods in machine learning. In Proceedings of the First
International Workshop on Multiple Classifier Systems (MCS 2000), volume 1857 of Lecture
Notes in Computer Science, pages 1–15. Springer-Verlag, 2000.
[33] Robi Polikar. Ensemble based systems in decision making. IEEE Circuits and Systems Magazine,
6(3):21–45, 2006.
[34] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
[35] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139,
1997.
[36] Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
[37] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of
the American Statistical Association, 58(301):13–30, 1963.
20

[38] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Skyler Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027,
2020.
[39] Together Computer. Redpajama: An open dataset for training large language models. https:
//github.com/togethercomputer/RedPajama-Data, 2023.
[40] Hugging Face. The pile, common crawl, and starcoder dataset descriptions, 2024. Available at
https://huggingface.co/datasets.
[41] Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz.
Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural
information processing systems, 26, 2013.
[42] Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. Deep reinforcement
learning from policy-dependent human feedback. arXiv preprint arXiv:1902.04257, 2019.
[43] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, and Tom Henighan. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,
2022.
[44] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and
Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth
International Conference on Learning Representations, 2024.
[45] Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A
selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019. URL
https://arxiv.org/abs/1911.10635. Revised version (v2) last updated 28 April 2021.
[46] Xinran Wang, Qi Le, Ammar Ahmed, Enmao Diao, Yi Zhou, Nathalie Baracaldo, Jie Ding,
and Ali Anwar. MAP: Multi-human-value alignment palette. In The Thirteenth International
Conference on Learning Representations, 2025. URL https://openreview.net/forum?
id=NN6QHwgRrQ.
[47] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.
[48] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations, 2015.
[49] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses
for deep learning. IEEE Transactions on Neural Networks and Learning Systems, 30(9):
2805–2824, 2019.
[50] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety
Training Fail? In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[51] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang,
Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against LLMintegrated applications. arXiv preprint arXiv:2306.05499v2, 2024. URL https://arxiv.
org/abs/2306.05499v2. Revised version updated 2 March 2024.
[52] Alex Evfimievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in
privacy-preserving data mining. In Proceedings of the twenty-second ACM SIGMOD-SIGACTSIGART symposium on Principles of database systems, pages 211–222. ACM, 2003.
[53] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of cryptography conference, pages 265–284.
Springer, 2006.
21

[54] Jie Ding and Bangjun Ding. Interval privacy: A framework for privacy-preserving data collection. IEEE Transactions on Signal Processing, 2021.
[55] Ganghua Wang, Yuhong Yang, and Jie Ding. Model privacy: A unified framework to understand
model stealing attacks and defenses. arXiv preprint arXiv:2402.01852, 2024.
[56] Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, and
Jie Ding. A Unified Detection Framework for Inference-Stage Backdoor Defenses. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.
[57] Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, and Jie
Ding. Understanding Backdoor Attacks through the Adaptability Hypothesis. In Proceedings
of the 40th International Conference on Machine Learning, 2023.
[58] Ganghua Wang, Xun Xian, Ashish Kundu, Jayanth Srinivasa, Xuan Bi, Mingyi Hong, and Jie
Ding. Demystifying poisoning backdoor attacks from a statistical perspective. In The Twelfth
International Conference on Learning Representations, 2024.
[59] Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z.
Leibo, K. Larson, and Thore Graepel. Open problems in cooperative ai. ArXiv, abs/2012.08630,
2020.
[60] Bosiljka Tasic, Zizhen Yao, Lucas T. Graybuck, et al. Shared and distinct transcriptomic cell
types across neocortical areas. Nature, 563:72–78, 2018.
[61] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor
attack with sample-specific triggers. In Proceedings of the IEEE International Conference on
Computer Vision, pages 16443–16452, 2021. doi: 10.1109/ICCV48922.2021.01615.
[62] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on
deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.

22

