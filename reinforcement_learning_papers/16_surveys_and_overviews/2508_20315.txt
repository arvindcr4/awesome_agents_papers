1

Multi-Agent Reinforcement Learning in Intelligent Transportation
Systems: A Comprehensive Survey

arXiv:2508.20315v1 [cs.LG] 27 Aug 2025

RexCharles Donatus, Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe

Abstract—The growing complexity of urban mobility and the
demand for efficient, sustainable, and adaptive solutions have positioned Intelligent Transportation Systems (ITS) at the forefront
of modern infrastructure innovation. At the core of ITS lies the
challenge of autonomous decision-making across dynamic, largescale, and uncertain environments where multiple agents—traffic
signals, autonomous vehicles, or fleet units—must coordinate
effectively. Multi-Agent Reinforcement Learning (MARL) offers
a promising paradigm for addressing these challenges by enabling distributed agents to jointly learn optimal strategies that
balance individual objectives with system-wide efficiency. This
paper presents a comprehensive survey of MARL applications
in ITS. We introduce a structured taxonomy that categorizes
MARL approaches according to coordination models and learning algorithms, spanning value-based, policy-based, actor–critic,
and communication-enhanced frameworks. Applications are reviewed across key ITS domains, including traffic signal control,
connected and autonomous vehicle coordination, logistics optimization, and mobility-on-demand systems. Furthermore, we
highlight widely used simulation platforms—such as SUMO,
CARLA, and CityFlow—that support MARL experimentation,
along with emerging benchmarks. The survey also identifies core
challenges, including scalability, non-stationarity, credit assignment, communication constraints, and the sim-to-real transfer
gap, which continue to hinder real-world deployment. Finally,
we outline future research opportunities emphasizing federated
learning, safety-aware policy design, robust communication protocols, and integration with edge computing. By consolidating
fragmented efforts across disciplines, this survey provides both
the transportation and AI research communities with a roadmap
for advancing MARL toward practical, scalable, and equitable
ITS solutions.
Index Terms—Multi-Agent Reinforcement Learning; Intelligent Transportation Systems; Traffic Signal Control;

I. INTRODUCTION
The global evolution of urban mobility is marked by
growing transportation demands, increasing urban congestion,
and the pressing need for sustainable and efficient mobility
solutions [1], [2]. To meet these challenges, Intelligent Transportation Systems (ITS) have emerged as a cornerstone of
modern infrastructure, aiming to integrate advanced sensing,
control, and communication technologies to enhance traffic
efficiency, safety, and environmental performance [3].
At the heart of ITS lies a critical need for autonomous
decision-making in complex, dynamic, and often uncertain
Ore-Ofe Ajayi is with the Department of Computer Engineering, Ahmadu Bello University, Kaduna State, Nigeria (email: aoreofe@abu.edu.ng)
while Rexcharles Donatus and Kumater Ter are with the Department of
Aerospace Engineering, Air Force Institute of Technology, Nigeria (emails:
rdonatus@afit.edu, kumater.ter@afit.edu) and Daniel Udekwe is with the
Department of Civil and Environmental Engineering, FAMU-FSU College
of Engineering, Tallahassee, Florida, 32310, USA (email: dau24@fsu.edu)
Manuscript received xxx, 2025; revised xxx, 2025.

environments [4]. Traditional rule-based and optimizationbased methods often fall short when faced with large-scale,
stochastic, and multi-agent traffic scenarios [4]. In this context, Reinforcement Learning (RL) has gained traction as a
powerful data-driven control paradigm capable of learning
optimal or near-optimal policies through interaction with the
environment [5]. However, real-world transportation systems
are rarely single-agent systems [6]. Instead, they involve
numerous distributed and interacting agents—such as traffic
lights, autonomous vehicles, or fleet units—making MultiAgent Reinforcement Learning (MARL) particularly relevant
[7].
While standard reinforcement learning (RL) has shown
success in isolated tasks, multi-agent reinforcement learning
(MARL) uniquely enables agents to learn both individual
policies and coordination strategies, facilitating cooperative
traffic signal optimization across large networks, vehicle-tovehicle (V2V) and vehicle-to-infrastructure (V2I) coordination in connected autonomous driving, and scalable solutions
for complex logistics, ride-sharing, and mobility-on-demand
systems. However, despite its growing adoption, the MARL
landscape in transportation remains fragmented, characterized
by diverse algorithmic designs, inconsistent evaluation standards, and varied assumptions regarding agent interactions and
reward structures. A consolidated understanding is urgently
needed to clarify which methods are effective, under what
conditions, and for what types of transportation problems.
A. Scope and Contributions
This paper provides a comprehensive survey of MultiAgent Reinforcement Learning (MARL) approaches applied
to Intelligent Transportation Systems (ITS), targeting both the
transportation and artificial intelligence communities engaged
in multi-agent challenges. A structured taxonomy is introduced to classify MARL architectures based on coordination
models and learning algorithms. The survey offers a detailed
analysis of MARL applications across various ITS domains,
including traffic signal control and autonomous driving. In
addition, commonly used simulation platforms and opensource benchmarks for MARL evaluation in ITS are reviewed.
Key challenges—such as scalability, safety, non-stationarity,
and the sim-to-real transfer problem—are identified as major
barriers to practical deployment. The paper concludes by
outlining future research directions, emphasizing opportunities
in federate
B. Organization of the Paper
The remainder of the paper is organized as follows: Section
II introduces the fundamentals of reinforcement learning and

2

Fig. 2. Hierarchy of Reinforcement Learning Methods: Categorizing Approaches into Model-Free and Model-Based

Fig. 1. Illustration of the reinforcement learning loop: the agent interacts
with the environment by taking actions at , and in return receives the next
state st+1 and reward rt , forming a continuous feedback cycle for learning
optimal behavior.

its extension to multi-agent systems. Section III classifies
MARL architectures and learning paradigms relevant to ITS.
Section IV reviews real-world applications of MARL across
various ITS domains. Section V presents core challenges and
limitations of MARL in transportation. Section VI outlines
future research directions, followed by conclusions in Section
VII.
II. REINFORCEMENT LEARNING
A. Single Agent Reinforcement Learning (RL)
Reinforcement Learning (RL) is a framework in which an
agent learns to make decisions by interacting with an environment [8]–[10]. In the case of single agent reinforcement
learning, there exists only one agent that perceives the environment’s state, takes actions, and learns from the feedback it
receives [11], [12]. This setup forms the foundational structure
of many RL algorithms and is depicted in Figure 1.
The agent-environment interaction is typically modeled
as a Markov Decision Process (MDP), defined by a tuple
(S, A, P, R, γ) where [13]–[15]:
• S is the set of possible states,
• A is the set of possible actions,
′
• P (s |s, a) defines the transition probabilities,
• R(s, a) is the reward function,
• γ ∈ [0, 1) is the discount factor.
The agent is a computational unit responsible for selecting
actions based on its current policy π(a|s) [16]. At each time
step t, the agent receives a state st from the environment
and selects an action at [17]. This action is executed in the

environment, which responds by providing a scalar reward rt
and the next state st+1 [18].
The cycle continues as the agent updates its policy or value
estimates using this feedback [19]. The aim is to maximize the
cumulative reward over time, often formalized as the return
[20]:
∞
X
Gt =
γ k Rt+k+1
(1)
k=0

The environment in Figure 1 is represented with various
real-world elements such as traffic signals, parking, and sensors—illustrating that reinforcement learning can be applied
to complex domains like autonomous driving, smart traffic
control, or robotic systems.
Single agent reinforcement learning assumes that the environment is stationary and non-adversarial. The learning
process involves trial and error, where the agent explores different strategies and improves its behavior based on observed
outcomes [10]. Over time, the agent converges to an optimal
or near-optimal policy that maximizes its expected return [21].
This simple yet powerful interaction loop serves as the basis
for more advanced scenarios in multi-agent systems, partially
observable environments, and continuous control tasks.
Reinforcement Learning (RL) algorithms are commonly
classified into three main categories based on their learning
strategies: value-based, policy-based, and actor-critic methods,
as illustrated in Figure 2. This categorization highlights the
different ways in which each approach models the agent’s
decision-making process during interaction with the environment. The following sections provide an overview of these
learning strategies.
1) Value-Based Methods: Value-based reinforcement learning (RL) methods aim to learn a policy that maximizes the
expected cumulative reward by estimating the value of states
or state-action pairs [22]. In such methods, the agent interacts
with the environment and updates its internal estimates of the
desirability of being in particular states or performing specific

3

actions [23], [24]. These methods do not require an explicit
model of the environment and are widely used due to their
simplicity and general applicability [25].
The central idea in value-based approaches is the notion of
a value function, which quantifies the expected return from a
state or from taking an action in a state under a specific policy
[26]. The state-value function under a policy π, denoted as
V π (s), represents the expected total discounted reward when
starting in state s and following the policy π thereafter [27].
It is formally defined as:
"∞
#
X
π
k
V (s) = Eπ
γ Rt+k | St = s
(2)
where γ ∈ [0, 1) is the discount factor, which determines
the present value of future rewards, and Rt+k is the reward
received k steps into the future [28].
Similarly, the action-value function Qπ (s, a) gives the expected return when the agent starts from state s, takes action
a, and then follows the policy π:

Q (s, a) = Eπ

"∞
X

γ Rt+k | St = s, At = a

(3)

The goal in reinforcement learning is to find an optimal
policy π ∗ that yields the highest expected return from each
state [29]. The corresponding optimal state-value function is
defined as:
V ∗ (s) = max V π (s)
π

(4)

and the optimal action-value function as:
Q∗ (s, a) = max Qπ (s, a)
π

(5)

From the optimal action-value function, an optimal policy
can be derived by selecting the action that maximizes the
expected return:
π ∗ (s) = arg max Q∗ (s, a)
a

(6)

To compute these value functions, recursive relationships
known as Bellman equations are employed [29]. The Bellman
expectation equation for the state-value function under policy
π is given by:
V π (s) =

a

π(a|s)

X

P (s′ , r|s, a) [r + γV π (s′ )]

(7)

s′ ,r

where P (s′ , r|s, a) is the probability of transitioning to state
s′ and receiving reward r after taking action a in state s [30].
Similarly, the Bellman expectation equation for the actionvalue function is:
"
Qπ (s, a) =

X
s′ ,r

a

P (s′ , r|s, a) r + γ

#
X

X

P (s′ , r|s, a) [r + γV ∗ (s′ )]

π(a′ |s′ )Qπ (s′ , a′ )

a′

(8)

(9)

s′ ,r

and the optimal action-value function is defined recursively
as:
Q∗ (s, a) =

X

h
i
∗ ′ ′
P (s′ , r|s, a) r + γ max
Q
(s
,
a
)
′
a

(10)

In practice, these value functions are typically estimated
through interaction with the environment. One popular approach is temporal-difference (TD) learning, which updates
value estimates using bootstrapping [32]. For instance, the
TD(0) update rule for the state-value function is:
V (st ) ← V (st ) + α [Rt+1 + γV (st+1 ) − V (st )]

#
k

k=0

X

V ∗ (s) = max

s′ ,r

k=0

π

When seeking the optimal value functions, the Bellman
optimality equations replace the expectations over the policy
with maximizations [31]. The optimal state-value function
satisfies:

(11)

where α is the learning rate.
For estimating the action-value function, two widely used
algorithms are Q-learning and SARSA. Q-learning is an offpolicy method that learns the optimal value function regardless
of the agent’s current policy. Its update rule is:
h
i
′
Q(st , at ) ← Q(st , at )+α Rt+1 + γ max
Q(s
,
a
)
−
Q(s
,
a
)
t+1
t t
a′
(12)
SARSA (State-Action-Reward-State-Action), on the other
hand, is an on-policy method that updates the value function
based on the agent’s actual behavior:
Q(st , at ) ← Q(st , at )+α [Rt+1 + γQ(st+1 , at+1 ) − Q(st , at )]
(13)
These value-based reinforcement learning methods are
foundational in the field and serve as the basis for more
advanced techniques, including deep reinforcement learning.
By learning to accurately estimate value functions, agents can
make increasingly effective decisions in complex, uncertain
environments.
2) Policy-Based Methods: Policy-based reinforcement
learning methods take a different approach from value-based
methods by directly parameterizing and optimizing the policy
itself, rather than deriving it indirectly from value functions [33]. These methods are particularly well-suited to
environments with large or continuous action spaces, where
maintaining and maximizing action-value functions becomes
computationally expensive or unstable [34].
In policy-based methods, the agent’s behavior is described
by a policy π(a|s; θ), which defines the probability of selecting
action a in state s, given a set of parameters θ [35]. The
objective is to find the optimal policy parameters θ∗ that
maximize the expected return from each state [36].

4

The performance of a policy is typically quantified using
the objective function J(θ), which measures the expected
cumulative reward when following the policy πθ [35]:
"∞
#
X
t
J(θ) = Eπθ
γ Rt
(14)
t=0

To optimize this objective, gradient ascent is applied. The
core idea is to update the parameters θ in the direction of the
gradient of J(θ) with respect to θ [37]. The update rule is:
θ ← θ + α∇θ J(θ)

(15)

The fundamental result enabling this update is the policy
gradient theorem, which provides a way to compute the
gradient of the expected return without needing to differentiate
through the state transition probabilities [38]. The policy
gradient is given by:
∇θ J(θ) = Eπθ [∇θ log πθ (a|s) · Qπθ (s, a)]

(16)

In practice, since the true action-value function Qπθ (s, a)
is usually unknown, various estimators are used [39]. One
common approach is to use the return Gt observed from a
trajectory:
∇θ J(θ) ≈ Eπθ [∇θ log πθ (at |st ) · Gt ]

(17)

This forms the basis of the REINFORCE algorithm, a
Monte Carlo policy gradient method. While simple and unbiased, REINFORCE suffers from high variance [40]. To
reduce this variance, a baseline function b(st )—often chosen
as the state-value function V π (st )—can be subtracted without
introducing bias:
∇θ J(θ) ≈ Eπθ [∇θ log πθ (at |st ) · (Gt − b(st ))]

(18)

Using the state-value function as a baseline leads to the
advantage function:
Aπ (s, a) = Qπ (s, a) − V π (s)

(19)

In this case, the policy gradient becomes:
∇θ J(θ) = Eπθ [∇θ log πθ (a|s) · Aπ (s, a)]

(20)

Policy-based methods naturally support stochastic policies,
which are essential in partially observable or multi-agent
environments [41]. Another important property of policybased methods is their ability to represent deterministic or
continuous action distributions, which is difficult for valuebased approaches [42]. This makes policy gradient methods
suitable for high-dimensional control tasks such as robotics
and continuous control benchmarks [43], [44].
Despite their advantages, policy-based methods can suffer
from problems such as slow convergence and sensitivity to
hyperparameters [45]. As a result, much research has been
devoted to improving their stability and efficiency, including
algorithms such as Trust Region Policy Optimization (TRPO),
Proximal Policy Optimization (PPO), and Soft Actor-Critic
(SAC) [41].

Fig. 3. Illustration of the Actor-Critic Reinforcement Learning Framework

3) Actor Critic Methods: Actor-critic methods combine the
key principles of value-based and policy-based reinforcement
learning approaches [46]. In these methods visualized in
Figure 3, the agent maintains two separate models: an actor,
which is responsible for selecting actions according to a policy,
and a critic, which evaluates the chosen actions by estimating
value functions [47]. This architecture enables the agent to
improve its decision-making process through a blend of policy
optimization and value estimation [48].
The actor corresponds to the policy function πθ (a|s), parameterized by θ, and determines the agent’s behavior by
specifying the probability distribution over actions given the
current state [49]. The critic, on the other hand, estimates either the state-value function V π (s) or the action-value function
Qπ (s, a), using a separate set of parameters, often denoted by
ϕ [49].
The core idea of actor-critic methods is to use the critic to
provide a low-variance estimate of the policy gradient, which
guides the actor’s updates [50]. Instead of relying on highvariance returns from entire trajectories, the actor adjusts its
policy parameters in the direction of the estimated advantage:
∇θ J(θ) ≈ Eπθ [∇θ log πθ (at |st ) · Aπ (st , at )]

(21)

Here, Aπ (s, a) is the advantage function, defined as:
Aπ (s, a) = Qπ (s, a) − V π (s)

(22)

The advantage quantifies how much better (or worse) an
action is compared to the average action in that state under
the current policy [51]. A positive advantage indicates that the
action yields a higher return than expected, encouraging the
actor to increase the probability of selecting it.
In practice, Aπ (s, a) is often approximated using bootstrapped estimates. A common estimator is the one-step temporal difference advantage [52]:
Ât = Rt+1 + γV (st+1 ) − V (st )

(23)

5

This can be extended to multi-step or generalized advantage
estimation (GAE) for improved stability and reduced variance.
The critic itself is updated using standard temporal-difference
learning rules. When estimating the state-value function, the
critic’s update is typically [28], [52]–[54]:
V (st ) ← V (st ) + α [Rt+1 + γV (st+1 ) − V (st )]

(24)

When the action-value function is used instead, the update
resembles that of Q-learning or SARSA, depending on whether
the method is off-policy or on-policy.
Actor-critic methods can be further categorized based on
how the policy is represented and updated [51]. In discrete
action spaces, the policy is usually stochastic, and the actor
samples from πθ (a|s). In continuous action spaces, deterministic policies are often used, and the deterministic policy gradient
theorem provides the corresponding update [55]:
h
i
∇θ J(θ) = Es∼D ∇θ πθ (s) · ∇a Qπ (s, a) a=π (s)
θ

(25)

Several popular reinforcement learning algorithms are based
on the actor-critic framework. These include Advantage ActorCritic (A2C), Asynchronous Advantage Actor-Critic (A3C),
Deep Deterministic Policy Gradient (DDPG), Twin Delayed
Deep Deterministic Policy Gradient (TD3), Soft Actor-Critic
(SAC), and Proximal Policy Optimization (PPO) [55], [56].
Each of these algorithms introduces modifications to improve
training stability, sample efficiency, or exploration.
Overall, actor-critic methods offer a powerful and flexible
class of algorithms capable of handling high-dimensional and
continuous control problems. By combining the strengths of
policy gradients and value estimation, they enable effective
learning in environments where purely value-based or policybased methods may struggle.
B. Multi-Agent Reinforcement Learning (MARL)
Multi-Agent Reinforcement Learning (MARL) extends traditional reinforcement learning to environments involving multiple decision-making agents [57], [58]. Each agent interacts
with a shared environment, learning to optimize its behavior
based on received rewards and observed states . Unlike singleagent scenarios, MARL introduces additional complexity due
to the presence of other learning agents, leading to nonstationary dynamics and coordination challenges [59].
III. MARL ARCHITECTURES AND TAXONOMIES
Multi-Agent Reinforcement Learning (MARL) involves
multiple agents learning simultaneously within an environment, interacting with each other and the environment to
achieve individual or shared objectives [60]. A fundamental
challenge in MARL is coordination—how agents align their
policies or behaviors, especially under partial observability,
non-stationarity, and sparse rewards [61].
To handle these challenges, MARL research has introduced
a variety of architectures and design taxonomies. One of the
most crucial dimensions in this classification is the coordination model, which determines how agents share information,

learn policies, and make decisions [61]. This section delves
into three prominent coordination models which are shown in
Figure 4:
A. Coordination Models
In Multi-Agent Reinforcement Learning (MARL), coordination models determine how agents are trained and how they act
during execution. A widely accepted way to categorize these
models is based on the nature of training (centralized or decentralized) and execution (centralized or decentralized) [41].
This framework gives rise to three principal coordination models: centralized training with centralized execution (CTCE),
centralized training with decentralized execution (CTDE), and
decentralized training with decentralized execution (DTDE)
[62].
1) Centralized Training with Centralized Execution
(CTCE): In centralized training with centralized execution
(CTCE), both the learning and deployment phases rely on a
centralized controller that possesses access to the complete
global state, actions, and rewards of all agents [63]. The
agents are treated as components of a single joint system, with
policies often optimized together [64]. At execution time, this
central authority continues to dictate the actions of all agents
using the combined environmental information [65]. CTCE
enables optimal coordination and strategic joint behaviors,
often achieving superior performance in fully observable
and stable environments [66]. However, it is impractical
in many real-world settings due to its high demands on
communication, synchronization, and computational overhead
[67]. Its reliance on continuous centralized control also
renders it vulnerable to single points of failure, and it
is largely restricted to simulation environments or highly
controlled applications where latency and scalability are not
critical concerns [68].
2) Centralized Training with Decentralized Execution
(CTDE): Centralized training with decentralized execution
(CTDE) is the most dominant coordination model in contemporary MARL research and applications [68]. In this
framework, agents are trained in a centralized manner where
they may share access to the global state, other agents’ actions,
or centralized critics that help stabilize learning and improve
credit assignment [67]. However, once trained, the agents
operate independently based solely on their local observations during execution [63]. This model achieves a balance
between the advantages of centralized learning—such as improved coordination, faster convergence, and better sample
efficiency—and the flexibility and scalability of decentralized
action. CTDE is particularly useful in environments with partial observability and dynamic multi-agent interactions, such
as in swarm robotics, autonomous vehicles, or multi-drone
systems [63]. Despite its advantages, CTDE still depends
on centralized infrastructure during training, which might be
infeasible in fully distributed settings or environments where
privacy and limited observability are critical [64].
3) Decentralized Training with Decentralized Execution
(DTDE): Decentralized training with decentralized execution
(DTDE) represents the most decentralized coordination ap-

6

(a)

(b)

(c)

Fig. 4. Multi-agent reinforcement learning architectures: (a) Decentralized Training and Decentralized Execution (DTDE) — agents learn independently with
local observations and rewards; (b) Centralized Training with Centralized Execution (CTCE) — agents are trained and executed using shared global information;
(c) Centralized Training with Decentralized Execution (CTDE) — agents are trained with global information but execute using only local observations.

proach in MARL [66]. Here, each agent learns and acts independently using only its own local observations and rewards.
There is no shared training infrastructure or global state, and
each agent treats others as part of an evolving environment
[64]. This model is simple, scalable, and naturally suited
to highly distributed or communication-constrained systems,
such as sensor networks or large-scale mobile ad hoc networks
[66]. However, it suffers significantly from the non-stationarity
of the environment since each agent’s policy is changing
in parallel with others [64]. This leads to unstable learning
dynamics and often results in suboptimal policies, especially in
cooperative tasks that require coordination. Moreover, the lack
of shared information and explicit coordination mechanisms
means that DTDE agents may converge to selfish or conflicting
behaviors unless strong environmental incentives guide them
toward cooperation [66].
B. MARL Algorithms
The complexity of multi-agent environments—marked
by non-stationarity, partial observability, and inter-agent
dependencies—has driven the development of specialized
algorithms that go beyond simple independent learning. These
algorithms enhance stability, improve coordination, and allow
for scalable learning across multiple agents. This section
introduces several landmark algorithms in MARL used in
intelligent transportation systems
1) Value Decomposition Network (VDN): Decomposes the
joint action-value function into an additive sum of individual
agent utilities [69]. Suitable for cooperative settings under the
CTDE paradigm [69].
Figure 5 illustrates the core architecture of the Value Decomposition Network (VDN), a foundational algorithm in cooperative Multi-Agent Reinforcement Learning (MARL) [70].

Fig. 5. Illustration of the Value Decomposition Network (VDN) framework
for multi-agent reinforcement learning. Each agent receives its own local
observation (o1t , o2t ) and uses an individual Q-network (Q-Net) to estimate its
action-value function. Based on these estimates, actions (a1t , a2t ) are selected.
The corresponding Q-values are then aggregated (summed) to compute the
joint action-value function qt , which is used for centralized training.

VDN is designed for environments where multiple agents work
together to maximize a shared reward [69]. It leverages the
concept of centralized training with decentralized execution
(CTDE) by decomposing the global action-value function into
individual agent components [71].
At each time step t, every agent i ∈ {1, 2, . . . , N } receives a
local observation oit from the environment [72]. These observations are processed independently by each agent’s Q-network,
producing an estimated action-value function Qi (oit , ait ) for
each possible action ait [69]:
Qit = Qi (oit , ait )
Each agent selects its action ait using an action selection
strategy such as ϵ-greedy, based on its Q-values. These actions
are executed simultaneously in the environment, resulting in a
joint action vector [72]:
⃗at = (a1t , a2t , . . . , aN
t )

7

The environment then transitions to the next state and
provides a shared reward rt to all agents [72].
The core innovation of VDN lies in how it estimates the
joint action-value function Qtot for the entire agent team.
Instead of modeling the full joint Q-function directly—which
is computationally infeasible due to its exponential complexity—VDN approximates it as a sum of individual Q-values
[69]:
Qtot (⃗ot , ⃗at ) =

N
X

Qi (oit , ait )

i=1

This decomposition assumes independent contributions
from each agent and allows the overall Q-learning update to
be driven by the collective behavior [70]:
θ ← θ − α∇θ

N
X

!2
Q (oit , ait ) − yt

Each agent i ∈ {1, 2, ..., N } receives a partial observation
and encodes it using an agent-specific recurrent neural network
(RNN). These RNNs output individual Q-values Qi (oit , ait ) for
local action-observation histories [75]:
Qit = Qi (oi1:t , ait )
The Q-values are combined via a centralized mixing network, which takes as input both [76]:
1
2
N
• the individual agent Q-values Qt , Qt , ..., Qt , and
• the global state st available during training.
This mixing network computes the joint action-value function Qtot [77]:
Qtot (st , ⃗at ) = MixingNet(Q1t , Q2t , ..., QN
t ; st )
The key constraint in QMIX is monotonicity [78]:

i

i=1

where the target value is defined as [70]:
yt = rt + γ max
Qtot (⃗ot+1 , ⃗a′ )
′
⃗
a

and γ is the discount factor.
During execution, agents rely only on their local Qnetworks and observations, ensuring decentralized policies
while benefiting from centralized training using the global
reward signal [70].
2) QMIX: Extends VDN by allowing a monotonic mixing
of individual Q-values using a mixing network. Enables more
flexible coordination in cooperative tasks [73].

∂Qtot
≥ 0,
∂Qi

∀i

This ensures that selecting actions to maximize each Qi
also maximizes Qtot , preserving decentralized execution while
enabling a richer representational capacity during centralized
training [78].
The global reward is used to update the joint Q-function,
and through backpropagation, each agent’s RNN is updated
accordingly [76].
• Execution: Fully decentralized — each agent acts based
only on its own observation.
• Training: Centralized — full global state and joint Qvalue are used.
This makes QMIX suitable for partially observable,
cooperative tasks such as traffic signal control, robotic team
coordination, and multi-drone exploration [76].
3) Multi-Agent Deep Deterministic Policy Gradient
(MADDPG): Uses decentralized actors with centralized critics, enabling agents to handle mixed cooperative-competitive
settings using continuous actions [79].

Fig. 6. Illustration of the QMIX framework for value-based multi-agent
reinforcement learning. Each agent receives a partial observation and uses
a recurrent neural network (RNN) to estimate its individual action-value
function. The selected Q-values are aggregated through a mixing network,
which is conditioned on the global state and outputs a joint action-value
function Qtotal

Figure 6 illustrates the architecture of QMIX, a value-based
cooperative Multi-Agent Reinforcement Learning (MARL)
algorithm designed to overcome the limitations of linear
factorisation in VDN [74]. While VDN assumes the total team
Q-value is the sum of individual agent Q-values, QMIX uses
a more expressive nonlinear mixing network that allows for
flexible but still consistent value decomposition [73].

Fig. 7. Illustration of the MADDPG (Multi-Agent Deep Deterministic Policy
Gradient) architecture. Each agent i has its own actor and critic networks.
The actor receives the agent’s local observation oi and outputs an action ai ,
while the critic is trained with access to the joint observations and actions of
all agents.

Figure 7 illustrates the architecture of MADDPG (MultiAgent Deep Deterministic Policy Gradient), a prominent algorithm for learning policies in both cooperative and competitive

8

multi-agent environments [80]. MADDPG extends the Deep
Deterministic Policy Gradient (DDPG) framework into the
multi-agent setting by adopting a centralized training with
decentralized execution (CTDE) paradigm [80].
Each agent i ∈ {1, 2, ..., N } is modeled as an actor–critic
pair. The actor network π i (oit ) outputs a deterministic action
ait given the agent’s partial observation oit [81]. During training, each agent’s critic network Qi takes as input the joint
observations and actions of all agents [81]:
1
N
Qi (o1t , ..., oN
t , at , ..., at )

This centralized critic allows each agent to account for the
influence of other agents’ actions during policy updates, which
is critical in non-stationary multi-agent environments [82].
The actor is updated by maximizing the critic’s Q-value
[83]:


1
N
∇θi J(θi ) = Eo,a ∇θi π i (oit )∇ai Qi (o1t , ..., oN
t , at , ..., at )
The critic is updated by minimizing the temporal-difference
(TD) error [82]:
h
2 i
L(θi ) = Eo,a,r,o′ Qi (ot , at ) − yt
′

yt = rti + γQi (ot+1 , a′t+1 )
′

′

where Qi and π i are the target networks for stability [82].
Execution Phase: After training, each agent uses only its
local actor π i (oit ) to choose actions without requiring access
to other agents’ observations or actions — enabling fully
decentralized execution [82].
MADDPG has been effectively applied to:
• Cooperative communication tasks
• Competitive games and navigation
• Autonomous vehicle interactions and adversarial driving
4) Multi-Agent Proximal Policy Optimization (MAPPO):
An extension of PPO to multi-agent domains with centralized
critics and stable, scalable performance [84].

with decentralized execution (CTDE), enabling scalable and
stable learning in cooperative multi-agent environments.
Each agent i ∈ {1, 2, . . . , n} receives a local observation
oit from the environment and outputs an action ait via its
decentralized actor policy πθi (ait |oit ) [85].
The centralized critic uses the global state st and all agents’
actions to evaluate the joint policy and compute the shared or
individual value function V (st ) or Q(st , ⃗at ) [85]. The critic
is used to compute the surrogate objective and advantage
estimates required for PPO updates [84].
Each actor is updated using the PPO clipped surrogate
objective [84]:
h

i
L(θi ) = Et min rt (θi )Ât , clip(rt (θi ), 1 − ϵ, 1 + ϵ)Ât
(ai |oi )

π

where rt (θi ) = π θoldi (ati |oti ) is the probability ratio, and Ât is
θ

i

t

t

the advantage function derived from the critic [84].
Key properties of MAPPO:
• Centralized critic during training enables credit assignment using global information.
• Decentralized actors ensure agents act based only on local
observations.
• Clipped updates stabilize policy improvement, inherited
from single-agent PPO.
MAPPO has been widely applied in:
• Multi-robot coordination
• Multi-agent games (e.g., StarCraft II micromanagement)
• Urban traffic control and air traffic deconfliction
.
5) Hysteretic Q-Learning: Hysteretic Q-learning is a
value-based reinforcement learning algorithm that introduces
asymmetry in learning rates to enhance stability in multiagent settings [86]. It is particularly effective in cooperative
environments where agents face non-stationarity and partial
observability due to the presence of other learning agents [86].
Unlike standard Q-learning, which applies a single learning
rate for all updates, hysteretic Q-learning uses two distinct
learning rates depending on whether the temporal-difference
(TD) error is positive or negative [86]. The standard Q-learning
update is given by [86]:


′
Q(st , at ) ← Q(st , at )+α rt + γ max
Q(s
,
a
)
−
Q(s
,
a
)
t+1
t
t
′
a

In hysteretic Q-learning, the update is modified as follows
[86]:
Q(st , at ) ← Q(st , at ) + α+ δt
−

Q(st , at ) ← Q(st , at ) + α δt
Fig. 8. Illustration of the MAPPO (Multi-Agent Proximal Policy Optimization) architecture. Each agent receives a local observation (o1t , . . . , on
t ) and
uses its own actor to produce an action (a1t , . . . , an
t ). A centralized critic
receives the global state st and reward rt to compute the policy loss L(θi )
for updating each actor i.

Figure 8 depicts the architecture of MAPPO (Multi-Agent
Proximal Policy Optimization), an actor–critic MARL algorithm adapted from PPO [85]. It leverages centralized training

if δt ≥ 0
if δt < 0

where δt is the TD-error, and α+ > α− , ensuring that the
agent learns more readily from positive feedback while being
cautious about penalizing actions that may appear suboptimal
due to the exploratory or noisy behavior of other agents [87].
This hysteresis mechanism helps reduce instability caused
by teammates’ fluctuating policies, especially in environments
with shared team rewards or sparse feedback [87]. It is often

9

used in decentralized training settings, where each agent learns
independently using only its local observations and rewards
[87].
Applications of hysteretic Q-learning include:
• cooperative robotic exploration
• decentralized traffic signal control
• coordinated navigation tasks
Hysteretic Q-learning serves as a foundational method
in multi-agent reinforcement learning and has inspired
further developments such as lenient Q-learning, which adds
stochastic forgiveness to early mistakes, and Dec-HDRQN,
which combines hysteresis with deep recurrent networks [87].
6) Lenient Q-Learning: Lenient Q-learning is an extension of standard Q-learning tailored for cooperative multiagent environments, particularly under stochastic dynamics
and delayed rewards [88]. It introduces the concept of leniency,
allowing agents to be forgiving of early mistakes made by
themselves or by teammates during exploration [88].
The main idea is to maintain a leniency value for each
state–action pair that gradually decays over time [88]. Initially, agents are optimistic about joint actions and ignore
low rewards or penalties, enabling them to explore without
prematurely discarding potentially beneficial actions due to
the noisy influence of other agents [88].
The update rule modifies the Q-learning formula by incorporating a temperature-based leniency factor [88]:
(
Q(st , at ) ←

Q(st , at ) + α δt , if δt > 0 or pl
Q(st , at ),
else

δt = rt + γ max
Q(st+1 , a′ ) − Q(st , at )
′
a

Each state–action pair (s, a) is associated with a temperature
T (s, a) that decreases over time as the pair is visited more
frequently [88]. A Boltzmann-like function is used to compute
the leniency L(s, a) [88]:
L(s, a) = 1 − exp (−κ · T (s, a))
where κ is a scaling factor. The agent uses L(s, a) to
probabilistically ignore negative updates during early exploration [88]. This mechanism encourages optimism and helps
the system converge to coordinated joint policies [88].
Key aspects of lenient Q-learning include:
• tolerance of early suboptimal actions, improving learning
in stochastic cooperative tasks
• decaying leniency to allow gradual enforcement of accurate value estimates
• decentralized execution, with each agent maintaining and
updating its own Q-table and temperature values
Lenient Q-learning has been successfully applied in
domains such as cooperative navigation, coordination games,
and traffic signal control [88]. It performs particularly well
when optimal joint actions require synchronized behavior
among agents, and where premature penalization can lead to
policy divergence.

7) Parameter Sharing Trust Region Policy Optimization
(PS-TRPO): PS-TRPO (Parameter Sharing Trust Region Policy Optimization) is a policy gradient-based multi-agent reinforcement learning approach that extends Trust Region Policy
Optimization (TRPO) to cooperative settings by enforcing
parameter sharing among agents [89]. It is particularly useful
in environments where agents are homogeneous or perform
similar roles [89].
In PS-TRPO, a single policy network is shared among all
agents. Each agent receives its own observation oit and acts
independently according to a shared policy πθ (ait |oit ) [89].
Despite this decentralized execution, training is centralized
using the aggregated experience of all agents [90].
The TRPO update is based on maximizing the expected
advantage while constraining the KL divergence between the
old and new policies [90] [90]:


πθ (a|o) πθ
Â old (o, a)
max E(o,a)∼πθold
θ
πθold (a|o)
subject to Eo [DKL (πθold (·|o)∥πθ (·|o))] ≤ δ
Here, Âπθold is the advantage estimate, and δ is a small
trust region threshold that limits policy updates to stay within
a reliable improvement region [90].
Key characteristics of PS-TRPO include:
• parameter sharing reduces model complexity and improves sample efficiency
• centralized training benefits from the collective experiences of all agents
• decentralized execution allows each agent to act independently in real time
PS-TRPO has been shown to perform well in tasks such as
cooperative navigation, predator–prey games, and formation
control, especially where symmetry among agents makes
parameter sharing natural [90]. It also serves as a basis for
more complex multi-agent policy gradient methods, including
MAPPO and HAPPO.
8) CommNet: Communication Network: CommNet (Communication Network) is a neural network architecture proposed for deep multi-agent reinforcement learning, in which
agents are trained end-to-end with differentiable inter-agent
communication [91]. Unlike traditional decentralized methods,
CommNet enables agents to share information via continuous
vectors during training and execution, allowing for learned
coordination in cooperative tasks [91], [92].
In CommNet, each agent i receives a local observation oit
and processes it through an encoder to produce a hidden state
hit [92]. These hidden states are then averaged to produce a
shared communication vector cit [92]:
cit =

1 X j
ht
N −1
j̸=i

Each agent then updates its own hidden state using both its
local information and the communication vector [92]:
hit+1 = f (hit , cit )

10

TABLE I
S UMMARY OF MARL A LGORITHMS IN I NTELLIGENT T RANSPORTATION S YSTEMS
Algorithm
Hysteretic Q-Learning [87]

Agent Type
Value-based

Structure
DTDE

Features
Uses different learning rates for increasing and decreasing Q-values.
Requires no communication between agents.

Lenient Q-Learning [93]

Value-based

DTDE

Adds leniency to Q-updates by storing temperature values in experience replay, useful in stochastic environments.

MAPPO [94]

Policy Optimization

CTDE

Extends PPO to multi-agent settings with decentralized actors and a
centralized critic. Supports stable learning through clipped surrogate
objectives and trust region constraints.

MADQN [95]

Value-based

DTDE

Uses importance sampling and low-dimensional encoding to handle
multi-agent experience replay efficiently.

PS-TRPO [96]

Policy Optimization

CTCE

Shares policy parameters during centralized training and updates
them using trust-region constraints with curriculum learning.

VDN [97]

Value-based

CTDE

Decomposes the joint Q-function into a sum of individual agent Qvalues, enabling decentralized execution.

QMIX [77]

Value-based

CTDE

Extends VDN with a monotonic mixing network for better representational power while retaining decentralized execution.

CommNet [98]

Policy Optimization

CTDE

Learns communication and action policies jointly. Agents exchange
continuous messages to select coordinated actions.

MADDPG [99]

Actor-Critic

CTDE

Employs decentralized actors with centralized critics that observe
all agent states and actions. Supports both cooperative and mixed
settings with continuous action spaces.

The updated hidden state is then used to select an action ait
via a policy network [92]. The entire system is differentiable,
and the agents are trained jointly using backpropagation and
policy gradient methods such as REINFORCE or actor–critic
approaches [92].
CommNet is especially effective in settings where:
• agents require tight coordination (e.g., formation flying,
cooperative navigation)
• partial observability limits individual performance
• communication can improve joint value estimation
Because the communication mechanism is fully differentiable and learned jointly with the policy, CommNet provides
a natural and scalable way to integrate communication into
MARL [91]. It is typically trained with centralized learning
and executed in a decentralized manner, assuming agents can
communicate their internal states in real time.
CommNet laid the foundation for subsequent
communication-aware MARL methods, such as IC3Net,
DIAL, and RIAL, which further explore gated, discrete, and
attention-based communication mechanisms.

1) SUMO 1 : An open-source, microscopic traffic simulator
that supports large-scale transportation networks. It
is widely used for tasks such as intersection control,
lane merging, and vehicle routing. MARL agents can
interface with SUMO via the TraCI API.

C. MARL Simulation Platforms

5) Highway-env 5 : A lightweight simulator for highway
scenarios, including lane keeping, merging, and
platooning. It is widely used for prototyping and
benchmarking MARL methods in constrained driving
environments.

Effective evaluation and development of Multi-Agent Reinforcement Learning (MARL) algorithms require simulation
platforms that can model complex, dynamic environments with
multiple interacting agents. In the context of autonomous vehicle coordination and intelligent transportation systems (ITS),
several simulation environments have become prominent for
testing MARL algorithms. These platforms support integration
with RL libraries, allow for custom scenario creation, and
provide real-time traffic dynamics.

2) CARLA 2 : A high-fidelity 3D simulator for autonomous
driving research. It provides detailed vehicle dynamics,
sensor models (e.g., LIDAR, cameras), and supports
integration with MARL for decision-making in urban
environments such as intersections and lane changes.
3) CityFlow 3 : A high-performance traffic simulator
tailored for large-scale signal control environments. It
is particularly suited for graph-based MARL research
on coordinated intersection management.
4) SMARTS 4 : A recent platform focused on realistic
multi-agent AV interactions. SMARTS offers modular
scenario design and supports MARL algorithms like
MAPPO and QMIX in complex environments.

1 https://eclipse.dev/sumo/
2 https://carla.org/
3 https://cityflow.readthedocs.io/en/latest/introduction.html
4 https://github.com/huawei-noah/SMARTS
5 https://github.com/Farama-Foundation/HighwayEnv

11

6) AIMSUN, VISSIM, and Paramics 6 : Commercialgrade traffic simulators that support high-fidelity,
city-scale modeling. These platforms are used for
validating MARL-based strategies in more realistic
traffic networks and are often applied in industry or
urban policy research.
7) PRESCAN 7 : A high-fidelity simulation platform
designed for autonomous driving research, featuring
photorealistic sensor modeling, traffic scenarios, and
advanced vehicle dynamics. PRESCAN is widely used
in industry and academia for testing MARL-based
coordination strategies in safety-critical driving tasks
such as intersection negotiation, obstacle avoidance,
and multi-agent highway maneuvers.
8) MATLAB/Simulink 8 : A widely used engineering simulation environment offering powerful tools for modeling vehicle dynamics, control systems, and signal
processing. It supports integration with Stateflow and
Reinforcement Learning Toolbox, enabling the implementation and testing of MARL algorithms for tasks
such as adaptive cruise control, platooning, and cooperative lane changing in a highly customizable and modular
framework.
These simulation platforms shown in Figure 9 form the
backbone of experimental validation in MARL research. Selecting an appropriate environment depends on the specific
task (e.g., lane merging vs. platooning), scale (e.g., intersection
vs. city-wide), and required fidelity (e.g., sensor-level vs.
abstracted traffic dynamics).
IV. APPLICATION OF MARL IN INTELLIGENT
TRANSPORTATION SYSTEMS
Multi-Agent Reinforcement Learning (MARL) has emerged
as a powerful paradigm for solving complex, dynamic
problems in intelligent transportation systems (ITS), where
multiple decision-makers—vehicles, signals, fleets, or aerial
agents—must coordinate under uncertainty [61]. MARL’s ability to handle distributed decision-making, adapt to real-time
data, and scale to large environments makes it especially suitable for managing the growing complexity of modern mobility
networks [100]. This section explores its key applications
across various ITS domains.

localized improvements, this approach is limited in its capacity
to optimize flows at a network level [102].
To address this, network-wide control approaches model
each intersection as a cooperative agent, where coordination is key [101]. Techniques such as Graph-based MARL
(e.g., CoLight, PressLight) allow intersections to exchange
information and adjust their strategies based on downstream and upstream traffic dynamics [102]. Coordination
strategies—like hierarchical learning, value decomposition, or
message-passing—enable emergent traffic patterns such as
green waves and adaptive lane prioritization [102].
These methods support real-time control by adapting to
fluctuating traffic conditions. MARL-based systems can reduce
average delays, improve throughput, and dynamically respond
to incidents or surges, outperforming traditional rule-based or
fixed-timing approaches. A summary of recent MARL papers
in TSC is given in Table II
B. Autonomous Vehicle Coordination
In the context of connected and autonomous vehicles
(CAVs), MARL provides a decentralized framework for vehicle coordination [112]. Applications include lane merging
on highways, intersection crossing without traffic lights, and
roundabout negotiation, where each vehicle acts as an agent
that must infer the intentions of others and optimize for safety
and efficiency [113].
In platooning and convoy control, groups of autonomous
vehicles travel closely together to improve aerodynamics and
traffic flow [114]. MARL enables these vehicles to jointly learn
policies that minimize fuel consumption while maintaining
safety and communication constraints [115]. Adaptive gap
control and coordinated acceleration/deceleration patterns are
examples of emergent behaviors learned through MARL [116].
Multi-agent highway driving simulations, such as those
used in CARLA, SUMO, or Flow, help train and validate
MARL policies in realistic traffic scenarios [114], [117].
These simulations test cooperative and competitive interactions
between human-driven and autonomous vehicles, ensuring safe
deployment of MARL in real-world autonomous traffic [115].
A summary of recent MARL papers in autonomous vehicle
coordination and control is given in Table III
Multi-agent reinforcement learning has also been employed
in various domains within intelligent transportation systems,
including freight and logistics, UAV control and coordination,
and transportation safety, among others. A summary of recent
studies in these areas is presented in Table IV.

A. Traffic Signal Control
One of the earliest and most studied applications of MARL
in ITS is traffic signal control, where intersections are modeled
as agents that learn to minimize congestion and delay [101].
In single intersection control, MARL agents learn optimal
phase timings based on local traffic states, such as queue
lengths or vehicle waiting times [101]. While effective for
6 https://www.aimsun.com/
7 https://plm.sw.siemens.com/en-US/simcenter/autonomous-vehiclesolutions/prescan/
8 https://www.mathworks.com/products/simulink.html

V. CHALLENGES IN MARL FOR ITS
Multi-Agent Reinforcement Learning (MARL) offers substantial promise for improving the performance and adaptability of Intelligent Transportation Systems (ITS). From managing traffic flows and coordinating fleets to optimizing logistics
networks and autonomous vehicle behavior, MARL can enable intelligent decision-making across distributed, dynamic
environments. However, transitioning these capabilities from
theory to real-world application presents a host of complex
and interrelated challenges.

Multi-agent A2C (MA2C)

Q-learning

DQN with coordination

Game-theoretic MARL

Distributed MADRL
Transfer RL

Delay-Aware MARL with
CTDE

MA-DRLS
DRL)

adv.RAIM
MADRL)

Decentralized A2C with spa[104] tial discounting and neighbor
awareness

Multi-agent Q-learning with
[105] simple cost-based coordination

DQN with coordination via
[106] max-plus; focus on reward
shaping and stability

Introduced GAMEPLAN, a
[107] game-theoretic auction system
prioritizing agents based on
observable driving behavior to
ensure collision-free planning.

Developed modular and dis[108] tributed MADRL policies for
large-scale traffic networks with
congestion reduction.

Proposed Delay-Aware Markov
[109] Games with centralized training, decentralized execution to
mitigate delay effects.

Proposed MA-DRLS for co[110] operative control at nonsignalized intersections using FCTP
scheduling and DRL.

Presented adv.RAIM using end[111] to-end MADRL with curriculum self-play and LSTM-based
control.

(End-to-End

(Multi-agent

+

MARL Algorithm
Cooperative Group-Based
Multi-Agent
Q-Learning
(CGB-MAQL)

PaperMain Contribution
Introduced a scalable multi[103] agent framework for adaptive traffic signal control using
group-based coordination.

Eliminate collisions, reduce
waiting and travel time at
intersections

Throughput maximization
and wait time reduction

Improve stability and performance under delay in
cooperative/competitive settings

Maximize vehicle outflow
and reduce congestion

Turn-based
ordering
for unsignalized traffic
scenarios

Improve multi-agent policy
coordination in TSC

Reduce average vehicle delay at intersections

Stabilize learning and improve policy in large scale
TSC

Objective
Improve coordination and
scalability in large-scale
TSC

Reduced travel time 59%, congestion delay 95%, waiting
time 88%

Significantly better throughput
and lower wait time than traffic
light methods

Outperformed standard MARL
in delayed environments

Improved over human traffic;
scalable to hundreds of AVs

10–20% fewer collisions than
DRL; 3.5% better than state-ofthe-art auctions

Coordination improved travel
time; noted instability in singleagent cases

Outperformed fixed and adaptive baselines

Outperformed
Independent
A2C (IA2C) and independent
Q-learning (IQL) in control
efficiency

Outcome
Better congestion mitigation
and energy efficiency

TABLE II
S UMMARY OF M ULTI -AGENT RL A PPROACHES FOR T RAFFIC S IGNAL C ONTROL (TSC)

Custom
AIM
simulator
(3-lane,
1200 veh/h/lane)

Custom
intersection
simulation with V2X

Multi-Agent Particle Environment

SUMO + RLlib

Real-world
+
synthetic
(merging/intersections/roundabouts)

SUMO

VISSIM (real road networks)

SUMO

Simulator
SUMO

12

TD3

Hierarchical RL with planning and
control layers
IRL with deep policies

Applies TD3 for energy-efficient CACC
in heavy-duty BEVs

Presents Multi-agent Advantage ActorCritic (MA2C) for cooperative lanechanging in mixed AV/HDV traffic

Explores robust and efficient behavioral
planning with risk-sensitive RL

Graph convolutional Q-network for
multi-agent CAV cooperative control

Decentralized collision avoidance without communication via value network

Safe hierarchical RL via decomposition
of Desires and trajectory planning

Lane-free CAV coordination using coordination graphs and max-plus

Hierarchical control for head-to-head
autonomous racing

Inverse RL-based multi-agent lane
merging behaviors

Energy-aware platoon control using
DRL

[121]

[122]

[123]

[124]

[125]

[126]

[127]

[128]

[129]

[130]

Actor-Critic

Max-plus

Policy Gradient

Value network

Graph Q-Network (GQN)

Budgeted RL, Tree-based Planning

MA2C

Policy Gradient with LSTM for
MARL-CACC)

Proposes an cooperative adaptive cruise
control (CACC) MARL framework using LSTM with learned communication
protocol

[120]

RPE-MAAC

Proposes Relative Position Encoding
- Multi-Actor Attention Critic (RPEMAAC) for stabilizing mixed platoons
with CAVs and HDVs

MARL Algorithm
Adaptive Monte Carlo Tree Search
(AMCTS)

[119]

Paper Main Contribution
Introduced a method for cooperative
[118] planning at roundabouts

Reduce energy loss during traffic
oscillations

Imitate human-like merging decisions in CAVs

Competitive racing behavior with
safety

Coordination without lanes to increase traffic flow and safety

Learning driving policy with guarantees on safety and comfort

Collision-free multi-agent path
planning under partial observability

Efficient lane-changing in highdensity traffic

Balancing safety and efficiency

Safe, comfortable, fuel-efficient
lane-changing

Energy savings with safety and
comfort

String stability and communication
robustness

Mixed platoon stability and safety

Objective
Time-optimal, collision-free trajectory planning

Smoother velocity profiles and less energy use

Learned policies outperform rule-based
merging

More robust and aggressive racing
strategies

Higher speeds and flow rates with efficient lateral usage

Low-variance RL strategy for autonomous driving under uncertainty

26% improvement over ORCA in navigation efficiency

Improved safety and mobility compared
to rule-based and LSTM fusion methods

Continuum of behaviors from conservative to aggressive

Superior to benchmarks in mixed traffic

Up to 19.8% energy reduction with
comfort preserved

Improved trajectory and convergence

Enhanced comfort and reduced disruptions

Outcome
Efficient and safe roundabout traversal

TABLE III
S UMMARY OF MARL-BASED P UBLICATIONS FOR AUTONOMOUS V EHICLE C OORDINATION

Custom simulator

CARLA

F1TENTH simulator

SUMO

Not specified

2D simulation environment

Custom simulator

Highway-env (open-source environment)

Custom highway simulation environment

HHDDT driving cycle simulation

Custom Simulator

Custom numerical simulator

Simulator
Custom Simulator

13

Introduced contextual MARL with scalable coordination for fleet management.

[136]

Policy Gradient

Multi-Agent A2C

Uses DDMAC-CTDE for lifecycle management of transport infrastructure.

MARL for decentralized bidding in freight
transport markets.

Shared MARL for dynamic logistics service collaboration.

[138]

Freight and Lo[139] gistics

[140]

DDMAC-CTDE

Custom Cooperative MARL

Proposes cooperative MARL for resource
balancing in logistics with cooperative reward shaping.

[137]

[126]

Non-Markovian Policy Gradient

CA2C, CDQN

Decomposed policy into safe planning and
learnable desires using Option Graph.

Automotive

Energy-aware UAV charging via coordinated deep MARL using CommNet.

[135]

CommNet-based MADRL

Decentralized Q-Learning

Cooperative spectrum sharing with task
allocation in constrained communication
settings.

[133]

[134]

MADDPG

MARL Algorithm
Independent Q-Learning

Game-theoretic MARL with
function approximation

UAV

Joint target assignment and path planning
using MADDPG.

Main Contribution
Resource allocation for multi-UAV downlink networks without information exchange between agents.

Field coverage by a UAV team while
minimizing overlap using correlated equilibrium.

[132]

[131]

PaperCategory

TABLE IV
K EY PAPERS ON MARL FOR ITS

Freight collaboration

Freight bidding strategy

Transportation
infrastructure I&M

Ocean container repositioning

Autonomous driving strategy

Ride-hailing fleet repositioning

Charging resource allocation

Task division: relaying vs.
sensing

Coverage optimization

Target allocation & path
planning

Problem Domain
Power control, user & subchannel selection

and

simulated

Custom simulator

Custom simulator

Custom simulator

Simulated ocean freight network

Custom double-merge scenario

Custom imulator with Didi
Chuxing data

Simulated smart grid environment

MATLAB-based simulator

Physical
testbeds

Custom 2D dynamic simulator

Simulation Environment
Custom simulator

14

TD3

QMIX

Multiple RL architectures
(reviewed)

Policy gradient

RL-MPC integration for safe intersection
crossing

Joint trajectory prediction using egocentric
and allocentric views to enable symmetrical multi-agent modeling with GNN

Introduces a conditional generative memory for continual multi-agent prediction
avoiding catastrophic forgetting

Latent strategy learning and influence
modeling for co-adaptive multi-agent interaction

Survey and experimental framework for
modeling cooperation in mixed humanmachine environments

Introduces framework for cooperative control in mixed traffic with attention-based
feature integration

[144]

[145]

[146]

Trajectory
[147] Prediction

[148]

[149]

MADDPG

MADDPG

Policy-Gradient RL (hybrid
model)

Hybrid RL model for AV motion planning
at unsignalized mid-block crosswalks.

[142]

[143]

MADDPG

Introduced MADRL framework that effectively addresses dynamic flexible assembly job shop scheduling (FAJSS) problems
under uncertainty in processing and transport times
Independent PPO, MADDPG, QMix

Safety

MARL Algorithm

Main Contribution

Proposes BARK, a benchmark for safetyoriented evaluation of MARL policies.

[141]

PaperCategory

multi-agent

Mixed traffic cooperative
control

Cooperative
learning

Non-stationary multi-agent
interaction

Continual multi-agent interaction behavior prediction

Multi-agent trajectory prediction (vehicles & pedestrians)

Urban intersections

Pedestrian-aware AV driving

Safety benchmarking across
environments

Flexible assembly job shop
scheduling

Problem Domain

Safety-

Discrete-Event

SUMO-based traffic simulation

Custom simulator

Custom simulator

SUMO

Custom simulator

Semantic map-based simulator

Real-world pedestrian speed
profiles

MiniGrid,
Gymnasium

Custom
Simulators

Simulation Environment

15

16

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Fig. 9. Common Simulators (a) SUMO (b) CARLA (c) CITYFLOW (d) SMARTS (e) HIGHWAY ENV (f) VISSIM (g) PRESCAN (h) MATLAB

A fundamental challenge lies in the issue of scalability.
ITS environments often consist of a large number of interacting agents—such as vehicles, traffic signals, or delivery
drones—operating concurrently. As the number of agents
increases, the joint state-action space grows exponentially,
making centralized control or joint-policy learning computationally infeasible. This combinatorial explosion necessitates
the use of decentralized learning architectures or factorized
representations to maintain tractability in large-scale scenarios
like network-wide traffic signal coordination or city-level
mobility management.
Another major hurdle is credit assignment in cooperative

MARL settings. When agents collectively contribute to a
global objective, such as minimizing congestion or maximizing throughput, it becomes difficult to determine the contribution of each individual agent to the overall outcome. Inaccurate
credit assignment can lead to inefficient or misguided learning,
particularly in systems with heterogeneous agents that play
different roles. Approaches like value function factorization
(e.g., QMIX) and counterfactual baselines (e.g., COMA) attempt to tackle this issue by better estimating individual agent
contributions.
Many ITS tasks also involve continuous control variables,
such as vehicle acceleration, steering angles, or lane-changing

17

maneuvers. Learning effective policies in continuous action
spaces is significantly more challenging than in discrete settings. Algorithms like MADDPG and MAPPO have been
developed to address this, but they often require careful parameter tuning, are sensitive to stochastic noise, and may suffer
from instability—especially when scaled to high-dimensional,
multi-agent contexts.
Communication between agents is another vital yet difficult
aspect of MARL in ITS. Effective coordination frequently
relies on the timely exchange of information between agents,
such as vehicle-to-vehicle or vehicle-to-infrastructure messages. However, real-world communication is constrained by
limited bandwidth, transmission delays, packet loss, and unreliable connectivity. Designing learning-based communication
protocols that are robust, efficient, and scalable remains an
active area of research, especially for applications like swarm
coordination, cooperative merging, and platooning.
Beyond these algorithmic complexities, the learning process
itself presents formidable challenges. ITS scenarios typically
involve high-dimensional sensor inputs, rapidly changing environments, and multiple competing goals. Developing MARL
agents that can learn effectively in such conditions requires
substantial computational resources, meticulous reward design,
and extensive tuning of learning parameters. Moreover, the
policies must not only succeed in training but also generalize
across diverse real-world situations, such as varying traffic
densities, unexpected detours, or weather-related disruptions.
Lastly, real-world deployment of MARL in transportation
systems must address non-technical constraints such as safety
assurance, policy explainability, and operational robustness. A
significant gap often exists between performance in simulation
and deployment in physical systems, known as the ”sim-toreal” gap. This arises due to discrepancies in sensing accuracy,
environment dynamics, and agent behaviors. Bridging this
gap requires methods like domain randomization, real-world
fine-tuning, or adaptive online learning to ensure that trained
policies remain effective and safe under real-world conditions.
Collectively, these challenges highlight the complexity of
deploying MARL in ITS and underscore the need for continued interdisciplinary research that integrates advances in
machine learning, systems engineering, and transportation
science.
VI. FUTURE RESEARCH DIRECTIONS
As Multi-Agent Reinforcement Learning (MARL) becomes
increasingly integrated into Intelligent Transportation Systems
(ITS), numerous promising research directions have emerged
to overcome current limitations and unlock greater potential.
These opportunities span theoretical frameworks, algorithmic
innovations, and practical deployments, reflecting the complex
and evolving nature of ITS environments.
One pressing area for future research is the development
of safe and explainable MARL systems. Safety is a critical
concern in transportation, and there is an urgent need for
MARL algorithms that can offer formal guarantees under
uncertainty while adhering to safety constraints. Incorporating methods from safe reinforcement learning, constrained

Markov Decision Processes (MDPs), and formal shielding
techniques can enhance the reliability of MARL in high-stakes
environments. At the same time, explainability is essential for
trust and adoption. Mechanisms such as interpretable policy
models, causal reasoning, and human-in-the-loop learning can
improve transparency and help stakeholders better understand
and validate agent decisions.
Another key challenge lies in sim-to-real transfer and domain adaptation. Policies trained in simulated environments
often degrade in performance when deployed in the real world
due to discrepancies in dynamics, noise, or context—known as
the ”sim-to-real” gap. Bridging this divide requires techniques
like domain randomization, curriculum learning, and adaptive
fine-tuning. Moreover, multi-fidelity simulations that integrate
both high- and low-resolution models can help expose agents
to a broader range of scenarios, improving generalization and
robustness in real-world applications.
Future research must also address the need for multiobjective and human-centric learning approaches. Transportation systems are inherently multi-faceted, requiring agents to
manage trade-offs among efficiency, equity, environmental sustainability, and passenger comfort. Developing MARL frameworks capable of optimizing across multiple, often conflicting
objectives will be essential. Additionally, integrating models of
human decision-making—such as bounded rationality, social
norms, and user preferences—can produce more realistic and
socially aligned agent behaviors that better reflect how people
interact with transportation systems.
As ITS increasingly functions in distributed settings,
communication-efficient and decentralized MARL methods
are becoming more critical. In many scenarios, agents operate in bandwidth-constrained or intermittent communication
environments. Research must focus on enabling agents to
learn what, when, and with whom to communicate effectively. Innovations such as emergent communication protocols,
attention-based messaging systems, and decentralized policy
architectures that leverage latent state representations offer
promising paths forward.
Lastly, generalization and lifelong learning remain foundational challenges for MARL in ITS. Given the dynamic nature
of transportation networks, agents must continuously adapt
to new cities, evolving infrastructure, changing traffic patterns, and unforeseen events without retraining from scratch.
Strategies such as continual learning, meta-learning, few-shot
adaptation, and transfer-based pretraining can equip agents
with the flexibility to respond to novel tasks and maintain longterm performance across diverse and shifting environments.
Together, these research directions aim to make MARL a
more powerful, reliable, and practical tool for next-generation
transportation systems—capable of delivering safe, adaptive,
and intelligent decision-making at scale.
VII. CONCLUSION
Multi-Agent Reinforcement Learning (MARL) has emerged
as a powerful tool for enhancing the adaptability, scalability,
and autonomy of Intelligent Transportation Systems (ITS).
Through coordinated decision-making and learning in decentralized environments, MARL enables various transportation

18

agents—such as vehicles, traffic lights, and delivery systems—to collaboratively address the complex and dynamic
challenges of modern mobility networks.
This paper has provided a comprehensive overview of
recent developments in applying MARL to ITS, highlighting
its potential across multiple application domains including
traffic management, freight logistics, UAV coordination, and
autonomous vehicle interactions. In doing so, it has also
identified the fundamental challenges that hinder real-world
deployment, such as scalability issues, credit assignment complexity, communication constraints, and the sim-to-real gap.
To bridge these gaps, future research must focus on designing safe, explainable, and human-centric MARL frameworks
that can generalize across environments and support realtime operation under uncertainty. Emphasis on communication efficiency, domain adaptation, and lifelong learning will
further ensure the robustness and practicality of MARL in
large-scale ITS deployments. By addressing these challenges
through interdisciplinary collaboration and advanced algorithmic innovations, MARL can play a pivotal role in shaping the
future of intelligent, resilient, and sustainable transportation
systems.
R EFERENCES
[1] L. E. Karjalainen and S. Juhola, “Urban transportation sustainability
assessments: a systematic review of literature,” Transport reviews,
vol. 41, no. 5, pp. 659–684, 2021.
[2] M. Z. Serdar, M. Koç, and S. G. Al-Ghamdi, “Urban transportation
networks resilience: indicators, disturbances, and assessment methods,”
Sustainable Cities and Society, vol. 76, p. 103452, 2022.
[3] C. Creß, Z. Bing, and A. C. Knoll, “Intelligent transportation systems
using roadside infrastructure: A literature survey,” IEEE Transactions
on Intelligent Transportation Systems, vol. 25, no. 7, pp. 6309–6327,
2023.
[4] M. N. Azadani and A. Boukerche, “Driving behavior analysis guidelines for intelligent transportation systems,” IEEE transactions on
intelligent transportation systems, vol. 23, no. 7, pp. 6027–6045, 2021.
[5] M. Nama, A. Nath, N. Bechra, J. Bhatia, S. Tanwar, M. Chaturvedi,
and B. Sadoun, “Machine learning-based traffic scheduling techniques
for intelligent transportation system: Opportunities and challenges,”
International Journal of Communication Systems, vol. 34, no. 9, p.
e4814, 2021.
[6] H. Wei, G. Zheng, V. Gayah, and Z. Li, “Recent advances in reinforcement learning for traffic signal control: A survey of models and
evaluation,” ACM SIGKDD explorations newsletter, vol. 22, no. 2, pp.
12–18, 2021.
[7] B. Lin, B. Ghaddar, and J. Nathwani, “Deep reinforcement learning
for the electric vehicle routing problem with time windows,” IEEE
Transactions on Intelligent Transportation Systems, vol. 23, no. 8, pp.
11 528–11 538, 2021.
[8] A. K. Shakya, G. Pillai, and S. Chakrabarty, “Reinforcement learning
algorithms: A brief survey,” Expert Systems with Applications, vol. 231,
p. 120495, 2023.
[9] Y. Matsuo, Y. LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama,
E. Uchibe, and J. Morimoto, “Deep learning, reinforcement learning,
and world models,” Neural Networks, vol. 152, pp. 267–275, 2022.
[10] D. Abel, A. Barreto, B. Van Roy, D. Precup, H. P. van Hasselt, and
S. Singh, “A definition of continual reinforcement learning,” Advances
in Neural Information Processing Systems, vol. 36, pp. 50 377–50 407,
2023.
[11] E. Okafor, D. Udekwe, Y. Ibrahim, M. Bashir Mu’azu, and E. G.
Okafor, “Heuristic and deep reinforcement learning-based pid control
of trajectory tracking in a ball-and-plate system,” Journal of Information and Telecommunication, vol. 5, no. 2, pp. 179–196, 2021.
[12] A. Adetifa, P. Okonkwo, B. B. Muhammed, and D. Udekwe, “Deep
reinforcement learning for aircraft longitudinal control augmentation
system,” Nigerian Journal of Technology, vol. 42, no. 1, pp. 144–151,
2023.

[13] A. Bennett and N. Kallus, “Proximal reinforcement learning: Efficient
off-policy evaluation in partially observed markov decision processes,”
Operations Research, vol. 72, no. 3, pp. 1071–1086, 2024.
[14] M. E. Ororbia and G. P. Warn, “Design synthesis through a markov
decision process and reinforcement learning framework,” Journal of
Computing and Information Science in Engineering, vol. 22, no. 2, p.
021002, 2022.
[15] L. Ren, X. Ning, and Z. Wang, “A competitive markov decision process
model and a recursive reinforcement-learning algorithm for fairness
scheduling of agile satellites,” Computers & Industrial Engineering,
vol. 169, p. 108242, 2022.
[16] J. He, H. Zhao, D. Zhou, and Q. Gu, “Nearly minimax optimal
reinforcement learning for linear markov decision processes,” in International Conference on Machine Learning. PMLR, 2023, pp. 12 790–
12 822.
[17] O. Chala, V. Yevsieiev, S. Maksymova, and A. Abu-Jassar, “Mathematical model based on multi-agent reinforcement learning (marl) and
partially observable markov decision process (pomdp) for modeling
cargo movement for a mobile robots group,” Multidisciplinary Journal
of Science and Technology, vol. 5, no. 4, pp. 480–489, 2025.
[18] A. Ronca, G. P. Licks, and G. De Giacomo, “Markov abstractions for
pac reinforcement learning in non-markov decision processes,” arXiv
preprint arXiv:2205.01053, 2022.
[19] H. Kurniawati, “Partially observable markov decision processes and
robotics,” Annual Review of Control, Robotics, and Autonomous Systems, vol. 5, no. 1, pp. 253–277, 2022.
[20] N. Kumar, E. Derman, M. Geist, K. Y. Levy, and S. Mannor, “Policy
gradient for rectangular robust markov decision processes,” Advances
in Neural Information Processing Systems, vol. 36, pp. 59 477–59 501,
2023.
[21] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu,
L. Liu, X. Liu et al., “Dapo: An open-source llm reinforcement learning
system at scale,” arXiv preprint arXiv:2503.14476, 2025.
[22] M. C. McKenzie and M. D. McDonnell, “Modern value based reinforcement learning: a chronological review,” IEEE Access, vol. 10, pp.
134 704–134 725, 2022.
[23] H. Byeon, “Advances in value-based, policy-based, and deep learningbased reinforcement learning,” International Journal of Advanced
Computer Science and Applications, vol. 14, no. 8, 2023.
[24] R. N. Boute, J. Gijsbrechts, W. Van Jaarsveld, and N. Vanvuchelen,
“Deep reinforcement learning for inventory control: A roadmap,”
European Journal of Operational Research, vol. 298, no. 2, pp. 401–
412, 2022.
[25] A. Esteso, D. Peidro, J. Mula, and M. Dı́az-Madroñero, “Reinforcement
learning applied to production planning and control,” International
Journal of Production Research, vol. 61, no. 16, pp. 5772–5789, 2023.
[26] Q. Liu, T. Yu, Y. Bai, and C. Jin, “A sharp analysis of model-based
reinforcement learning with self-play,” in International Conference on
Machine Learning. PMLR, 2021, pp. 7001–7010.
[27] H. Moradimaryamnegari, M. Frego, and A. Peer, “Model predictive
control-based reinforcement learning using expected sarsa,” IEEE Access, vol. 10, pp. 81 177–81 191, 2022.
[28] E. Okafor, D. Udekwe, O. Ubadike, E. Okafor, P. Jemitola, and
M. Abba, “Photovoltaic system mppt evaluation using classical, metaheuristics, and reinforcement learning-based controllers: A comparative
study,” Journal of Southwest Jiaotong University, vol. 56, no. 3, 2021.
[29] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine, “Efficient online
reinforcement learning with offline data,” in International Conference
on Machine Learning. PMLR, 2023, pp. 1577–1594.
[30] F.-M. Luo, T. Xu, H. Lai, X.-H. Chen, W. Zhang, and Y. Yu, “A survey
on model-based reinforcement learning,” Science China Information
Sciences, vol. 67, no. 2, p. 121101, 2024.
[31] T. Kaufmann, P. Weng, V. Bengs, and E. Hüllermeier, “A survey
of reinforcement learning from human feedback,” arXiv preprint
arXiv:2312.14925, vol. 10, 2023.
[32] M. Paniri, M. B. Dowlatshahi, and H. Nezamabadi-pour, “Ant-td: Ant
colony optimization plus temporal difference reinforcement learning for
multi-label feature selection,” Swarm and Evolutionary Computation,
vol. 64, p. 100892, 2021.
[33] L. Yang, X. Li, M. Sun, and C. Sun, “Hybrid policy-based reinforcement learning of adaptive energy management for the energy
transmission-constrained island group,” IEEE Transactions on Industrial Informatics, vol. 19, no. 11, pp. 10 751–10 762, 2023.
[34] J. Nousiainen, C. Rajani, M. Kasper, T. Helin, S. Y. Haffert,
C. Vérinaud, J. R. Males, K. Van Gorkom, L. M. Close, J. D. Long
et al., “Toward on-sky adaptive optics control using reinforcement

19

learning-model-based policy optimization for adaptive optics,” Astronomy & Astrophysics, vol. 664, p. A71, 2022.
[35] Z. Wang, J. J. Hunt, and M. Zhou, “Diffusion policies as an expressive policy class for offline reinforcement learning,” arXiv preprint
arXiv:2208.06193, 2022.
[36] V. G. Lopez, M. Alsalti, and M. A. Müller, “Efficient off-policy qlearning for data-based discrete-time lqr problems,” IEEE Transactions
on Automatic Control, vol. 68, no. 5, pp. 2922–2933, 2023.
[37] B. Hambly, R. Xu, and H. Yang, “Recent advances in reinforcement
learning in finance,” Mathematical Finance, vol. 33, no. 3, pp. 437–
503, 2023.
[38] C. Guo, X. Wang, Y. Zheng, and F. Zhang, “Real-time optimal
energy management of microgrid with uncertainties based on deep
reinforcement learning,” Energy, vol. 238, p. 121873, 2022.
[39] L. Wang, Z. Pan, and J. Wang, “A review of reinforcement learning
based intelligent optimization for manufacturing scheduling,” Complex
System Modeling and Simulation, vol. 1, no. 4, pp. 257–270, 2021.
[40] I. A. Zamfirache, R.-E. Precup, R.-C. Roman, and E. M. Petriu,
“Policy iteration reinforcement learning-based control using a grey
wolf optimizer algorithm,” Information Sciences, vol. 585, pp. 162–
175, 2022.
[41] J. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and
Y. Yang, “Trust region policy optimisation in multi-agent reinforcement
learning,” arXiv preprint arXiv:2109.11251, 2021.
[42] J. Zhou, S. Xue, Y. Xue, Y. Liao, J. Liu, and W. Zhao, “A novel energy
management strategy of hybrid electric vehicle via an improved td3
deep reinforcement learning,” Energy, vol. 224, p. 120118, 2021.
[43] R. Liu, F. Nageotte, P. Zanne, M. de Mathelin, and B. Dresp-Langley,
“Deep reinforcement learning for the control of robotic manipulation:
a focussed mini-review,” Robotics, vol. 10, no. 1, p. 22, 2021.
[44] K. Zhu and T. Zhang, “Deep reinforcement learning based mobile robot
navigation: A review,” Tsinghua Science and Technology, vol. 26, no. 5,
pp. 674–691, 2021.
[45] J. Hua, L. Zeng, G. Li, and Z. Ju, “Learning for a robot: Deep
reinforcement learning, imitation learning, transfer learning,” Sensors,
vol. 21, no. 4, p. 1278, 2021.
[46] W. Sun, Y. Zou, X. Zhang, N. Guo, B. Zhang, and G. Du, “High
robustness energy management strategy of hybrid electric vehicle based
on improved soft actor-critic deep reinforcement learning,” Energy, vol.
258, p. 124806, 2022.
[47] I. A. Zamfirache, R.-E. Precup, R.-C. Roman, and E. M. Petriu, “Neural
network-based control using actor-critic reinforcement learning and
grey wolf optimizer with experimental servo system validation,” Expert
Systems with Applications, vol. 225, p. 120112, 2023.
[48] L. Chen, S.-L. Dai, and C. Dong, “Adaptive optimal tracking control of
an underactuated surface vessel using actor–critic reinforcement learning,” IEEE Transactions on Neural Networks and Learning Systems,
2022.
[49] C.-A. Cheng, T. Xie, N. Jiang, and A. Agarwal, “Adversarially trained
actor critic for offline reinforcement learning,” in International Conference on Machine Learning. PMLR, 2022, pp. 3852–3878.
[50] I. A. Zamfirache, R.-E. Precup, and E. M. Petriu, “Q-learning, policy iteration and actor-critic reinforcement learning combined with
metaheuristic algorithms in servo system control,” Facta Universitatis,
Series: Mechanical Engineering, vol. 21, no. 4, pp. 615–630, 2023.
[51] J. Chen, Y. Wang, and T. Lan, “Bringing fairness to actor-critic
reinforcement learning for network utility optimization,” in IEEE
INFOCOM 2021-IEEE Conference on Computer Communications.
IEEE, 2021, pp. 1–10.
[52] D. Udekwe, O.-o. Ajayi, O. Ubadike, K. Ter, and E. Okafor, “Comparing actor-critic deep reinforcement learning controllers for enhanced
performance on a ball-and-plate system,” Expert systems with applications, vol. 245, p. 123055, 2024.
[53] E. Okafor, D. Udekwe, M. Muhammad, O. Ubadike, and E. Okafor,
“Solar system maximum power point tracking evaluation using reinforcement learning,” in Proceedings of 2021 Sustainable Engineering
and Industrial Technology Conference, 2021.
[54] D. Udekwe, “Evaluating a ddpg reinforcement learning agent on
a ball-and-plate system: A comparative study of intelligent control
approaches,” Nigerian Journal of Technology, vol. 44, no. 2, pp. 338–
346, 2025.
[55] A. Zanette, M. J. Wainwright, and E. Brunskill, “Provable benefits
of actor-critic methods for offline reinforcement learning,” Advances
in neural information processing systems, vol. 34, pp. 13 626–13 640,
2021.

[56] Z. Yu and X. Zhang, “Actor-critic alignment for offline-to-online reinforcement learning,” in International Conference on Machine Learning.
PMLR, 2023, pp. 40 452–40 474.
[57] S. V. Albrecht, F. Christianos, and L. Schäfer, Multi-agent reinforcement learning: Foundations and modern approaches. MIT Press,
2024.
[58] S. Gu, J. G. Kuba, Y. Chen, Y. Du, L. Yang, A. Knoll, and Y. Yang,
“Safe multi-agent reinforcement learning for multi-robot control,”
Artificial Intelligence, vol. 319, p. 103905, 2023.
[59] C. Li, T. Wang, C. Wu, Q. Zhao, J. Yang, and C. Zhang, “Celebrating
diversity in shared multi-agent reinforcement learning,” Advances in
Neural Information Processing Systems, vol. 34, pp. 3991–4002, 2021.
[60] M. Wen, J. Kuba, R. Lin, W. Zhang, Y. Wen, J. Wang, and Y. Yang,
“Multi-agent reinforcement learning is a sequence modeling problem,”
Advances in Neural Information Processing Systems, vol. 35, pp.
16 509–16 521, 2022.
[61] Y. Huang, C. Zhou, K. Cui, and X. Lu, “A multi-agent reinforcement
learning framework for optimizing financial trading strategies based
on timesnet,” Expert Systems with Applications, vol. 237, p. 121502,
2024.
[62] M. Bettini, A. Prorok, and V. Moens, “Benchmarl: Benchmarking
multi-agent reinforcement learning,” Journal of Machine Learning
Research, vol. 25, no. 217, pp. 1–10, 2024.
[63] Z. Ning and L. Xie, “A survey on multi-agent reinforcement learning
and its application,” Journal of Automation and Intelligence, vol. 3,
no. 2, pp. 73–91, 2024.
[64] F. Christianos, G. Papoudakis, M. A. Rahman, and S. V. Albrecht,
“Scaling multi-agent reinforcement learning with selective parameter
sharing,” in International Conference on Machine Learning. PMLR,
2021, pp. 1989–1998.
[65] H. J. Bae and P. Koumoutsakos, “Scientific multi-agent reinforcement
learning for wall-models of turbulent flows,” Nature Communications,
vol. 13, no. 1, p. 1443, 2022.
[66] C. Sun, S. Huang, and D. Pompili, “Llm-based multi-agent reinforcement learning: Current and future directions,” arXiv preprint
arXiv:2405.11106, 2024.
[67] D. Huh and P. Mohapatra, “Multi-agent reinforcement learning: A
comprehensive survey,” arXiv preprint arXiv:2312.10256, 2023.
[68] Z. Xia, J. Du, J. Wang, C. Jiang, Y. Ren, G. Li, and Z. Han, “Multiagent reinforcement learning aided intelligent uav swarm for target
tracking,” IEEE Transactions on Vehicular Technology, vol. 71, no. 1,
pp. 931–945, 2021.
[69] Q. Wei, Y. Li, J. Zhang, and F.-Y. Wang, “Vgn: Value decomposition
with graph attention networks for multiagent reinforcement learning,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 35,
no. 1, pp. 182–195, 2022.
[70] W. Du and S. Ding, “A survey on multi-agent deep reinforcement
learning: from the perspective of challenges and applications,” Artificial
Intelligence Review, vol. 54, no. 5, pp. 3215–3238, 2021.
[71] J. Su, S. Adams, and P. Beling, “Value-decomposition multi-agent
actor-critics,” in Proceedings of the AAAI conference on artificial
intelligence, vol. 35, no. 13, 2021, pp. 11 352–11 360.
[72] W. Fu, C. Yu, Z. Xu, J. Yang, and Y. Wu, “Revisiting some common
practices in cooperative multi-agent reinforcement learning,” arXiv
preprint arXiv:2206.07505, 2022.
[73] T. Zhao, T. Chen, and B. Zhang, “Qmix-gnn: A graph neural networkbased heterogeneous multi-agent reinforcement learning model for improved collaboration and decision-making,” Applied Sciences, vol. 15,
no. 7, p. 3794, 2025.
[74] M. Zhang, W. Tong, G. Zhu, X. Xu, and E. Q. Wu, “Sqix: Qmix
algorithm activated by general softmax operator for cooperative multiagent reinforcement learning,” IEEE Transactions on Systems, Man,
and Cybernetics: Systems, 2024.
[75] J. van Selm, “Applying qmix to active wake control,” Ph.D. dissertation,
Delft University of Technology, 2023.
[76] L. Wang, S. Liu, P. Wang, L. Xu, L. Hou, and A. Fei, “Qmix-based
multi-agent reinforcement learning for electric vehicle-facilitated peak
shaving,” in GLOBECOM 2023-2023 IEEE Global Communications
Conference. IEEE, 2023, pp. 1693–1698.
[77] D. Heik, A. Bohm, F. Bahrpeyma, and D. Reichelt, “Application
of inhomogeneous qmix in various architectures to solve dynamic
scheduling in manufacturing environments,” in 2024 IEEE 22nd International Conference on Industrial Informatics (INDIN). IEEE, 2024,
pp. 1–8.
[78] M. Kim, “Cooperative multi-agent reinforcement learning on sparse
reward battlefield environment using qmix and rnd in ray rllib,” Journal

20

of The Korea Society of Computer and Information, vol. 29, no. 1, pp.
11–19, 2024.
[79] B. Li, J. Wang, C. Song, Z. Yang, K. Wan, and Q. Zhang, “Multiuav roundup strategy method based on deep reinforcement learning
cel-maddpg algorithm,” Expert Systems with Applications, vol. 245, p.
123018, 2024.
[80] H. Zhang, Y. Du, S. Zhao, Y. Yuan, and Q. Gao, “Vn-maddpg:
A variable-noise-based multi-agent reinforcement learning algorithm
for autonomous vehicles at unsignalized intersections,” Electronics,
vol. 13, no. 16, p. 3180, 2024.
[81] J. Yang, X. Yang, and T. Yu, “Multi-unmanned aerial vehicle confrontation in intelligent air combat: A multi-agent deep reinforcement
learning approach,” Drones, vol. 8, no. 8, p. 382, 2024.
[82] S. A. Zakaryia, M. Meaad, T. Nabil, and M. K. Hussein, “Task
offloading and resource allocation for multi-uav asset edge computing
with multi-agent deep reinforcement learning,” Computing, vol. 107,
no. 5, pp. 1–31, 2025.
[83] A. Gao, Q. Wang, W. Liang, and Z. Ding, “Game combined multi-agent
reinforcement learning approach for uav assisted offloading,” IEEE
Transactions on Vehicular Technology, vol. 70, no. 12, pp. 12 888–
12 901, 2021.
[84] L. Chen, B. Hu, Z.-H. Guan, L. Zhao, and X. Shen, “Multiagent metareinforcement learning for adaptive multipath routing optimization,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 33,
no. 10, pp. 5374–5386, 2021.
[85] W. Cai, X. Huang, Y. Chen, and Q. Guan, “Joint optimization of
spectrum and power for vehicular networks: A mappo based deep reinforcement learning approach,” in 2024 IEEE Wireless Communications
and Networking Conference (WCNC). IEEE, 2024, pp. 1–6.
[86] N. K. Brown, A. Deshpande, A. Garland, S. A. Pradeep, G. Fadel,
S. Pilla, and G. Li, “Deep reinforcement learning for the design
of mechanical metamaterials with tunable deformation and hysteretic
characteristics,” Materials & Design, vol. 235, p. 112428, 2023.
[87] B. Chalaki and A. A. Malikopoulos, “A hysteretic q-learning coordination framework for emerging mobility systems in smart cities,” in
2021 European Control Conference (ECC). IEEE, 2021, pp. 17–22.
[88] E. Amhraoui and T. Masrour, “Expected lenient q-learning: a fast
variant of the lenient q-learning algorithm for cooperative stochastic
markov games,” International Journal of Machine Learning and Cybernetics, vol. 15, no. 7, pp. 2781–2797, 2024.
[89] M. Wang, J. Cui, Y. W. Wong, Y. Chang, L. Wu, and J. Jin, “Urban
vehicle trajectory generation based on generative adversarial imitation
learning,” IEEE Transactions on Vehicular Technology, 2024.
[90] J. Yu, F. Wu, and J. Zhao, “Trust region method using k-fac in
multi-agent reinforcement learning,” in Proceedings of the 2022 5th
International Conference on Algorithms, Computing and Artificial
Intelligence, 2022, pp. 1–7.
[91] R. Khan, N. Khan, and T. Ahmad, “Communication in multi-agent
reinforcement learning: A survey,” The Nucleus, vol. 60, no. 2, pp.
174–184, 2023.
[92] Y. Du, B. Liu, V. Moens, Z. Liu, Z. Ren, J. Wang, X. Chen, and
H. Zhang, “Learning correlated communication topology in multiagent reinforcement learning,” in Proceedings of the 20th International
Conference on Autonomous Agents and MultiAgent Systems, 2021, pp.
456–464.
[93] Q. Mao, “Multi-agent lenient reinforcement learning based algorithm
for balanced train operation of single-track railway considering dos
attacks,” in International Conference on Intelligent Transportation
Engineering. Springer, 2021, pp. 351–361.
[94] J. Tian, H. Jia, G. Wang, Q. Huang, R. Wu, H. Gao, and C. Liu, “Optimal scheduling of shared autonomous electric vehicles with multi-agent
reinforcement learning: A mappo-based approach,” Neurocomputing,
vol. 622, p. 129343, 2025.
[95] J. Wang, Y. Li, Q. Sun, and Y. Tang, “Demand-responsive transport
dynamic scheduling optimization based on multi-agent reinforcement
learning under mixed demand,” in International Conference on Artificial Neural Networks. Springer, 2024, pp. 356–368.
[96] K. Menda, Y.-C. Chen, J. Grana, J. W. Bono, B. D. Tracey, M. J.
Kochenderfer, and D. Wolpert, “Deep reinforcement learning for eventdriven multi-agent decision processes,” IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 4, pp. 1259–1268, 2018.
[97] Q. Wang, C. Guo, H.-N. Dai, and M. Xia, “Variant-depth neural
networks for deblurring traffic images in intelligent transportation
systems,” IEEE Transactions on Intelligent Transportation Systems,
vol. 24, no. 6, pp. 5792–5802, 2023.
[98] C. Park, G. S. Kim, S. Park, S. Jung, and J. Kim, “Multi-agent reinforcement learning for cooperative air transportation services in city-

wide autonomous urban air mobility,” IEEE Transactions on Intelligent
Vehicles, vol. 8, no. 8, pp. 4016–4030, 2023.
[99] T. Wu, M. Jiang, and L. Zhang, “Cooperative multiagent deep deterministic policy gradient (comaddpg) for intelligent connected transportation with unsignalized intersection,” Mathematical Problems in
Engineering, vol. 2020, no. 1, p. 1820527, 2020.
[100] W. Qin, Y.-N. Sun, Z.-L. Zhuang, Z.-Y. Lu, and Y.-M. Zhou, “Multiagent reinforcement learning-based dynamic task assignment for vehicles in urban transportation system,” International Journal of Production Economics, vol. 240, p. 108251, 2021.
[101] M. Kolat, B. Kővári, T. Bécsi, and S. Aradi, “Multi-agent reinforcement
learning for traffic signal control: A cooperative approach,” Sustainability, vol. 15, no. 4, p. 3479, 2023.
[102] S. Yang, B. Yang, Z. Zeng, and Z. Kang, “Causal inference multi-agent
reinforcement learning for traffic signal control,” Information Fusion,
vol. 94, pp. 243–256, 2023.
[103] T. Wang, J. Cao, and A. Hussain, “Adaptive traffic signal control
for large-scale scenario with cooperative group-based multi-agent
reinforcement learning,” Transportation research part C: emerging
technologies, vol. 125, p. 103046, 2021.
[104] T. Chu, J. Wang, L. Codecà, and Z. Li, “Multi-agent deep reinforcement
learning for large-scale traffic signal control,” IEEE transactions on
intelligent transportation systems, vol. 21, no. 3, pp. 1086–1095, 2019.
[105] K. Prabuchandran, H. K. AN, and S. Bhatnagar, “Multi-agent reinforcement learning for traffic signal control,” in 17th International IEEE
Conference on Intelligent Transportation Systems (ITSC). IEEE, 2014,
pp. 2529–2534.
[106] E. Van der Pol and F. A. Oliehoek, “Coordinated deep reinforcement
learners for traffic light control,” Proceedings of learning, inference
and control of multi-agent systems (at NIPS 2016), vol. 8, pp. 21–38,
2016.
[107] R. Chandra and D. Manocha, “Gameplan: Game-theoretic multi-agent
planning with human drivers at intersections, roundabouts, and merging,” IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2676–
2683, 2022.
[108] J. Cui, W. Macke, H. Yedidsion, D. Urieli, and P. Stone, “Scalable
multiagent driving policies for reducing traffic congestion,” arXiv
preprint arXiv:2103.00058, 2021.
[109] B. Chen, M. Xu, Z. Liu, L. Li, and D. Zhao, “Delay-aware multi-agent
reinforcement learning for cooperative and competitive environments,”
arXiv preprint arXiv:2005.05441, 2020.
[110] Y. Xu, H. Zhou, T. Ma, J. Zhao, B. Qian, and X. Shen, “Leveraging
multiagent learning for automated vehicles scheduling at nonsignalized
intersections,” IEEE Internet of Things Journal, vol. 8, no. 14, pp.
11 427–11 439, 2021.
[111] G.-P. Antonio and C. Maria-Dolores, “Multi-agent deep reinforcement
learning to manage connected autonomous vehicles at tomorrow’s
intersections,” IEEE Transactions on Vehicular Technology, vol. 71,
no. 7, pp. 7033–7043, 2022.
[112] M. Hua, X. Qi, D. Chen, K. Jiang, Z. E. Liu, H. Sun, Q. Zhou, and
H. Xu, “Multi-agent reinforcement learning for connected and automated vehicles control: Recent advancements and future prospects,”
IEEE Transactions on Automation Science and Engineering, 2025.
[113] S. K. S. Nakka, B. Chalaki, and A. A. Malikopoulos, “A multi-agent
deep reinforcement learning coordination framework for connected and
automated vehicles at merging roadways,” in 2022 American control
conference (ACC). IEEE, 2022, pp. 3297–3302.
[114] P. Yadav, A. Mishra, and S. Kim, “A comprehensive survey on multiagent reinforcement learning for connected and automated vehicles,”
Sensors, vol. 23, no. 10, p. 4710, 2023.
[115] E. Vinitsky, N. Lichtlé, K. Parvate, and A. Bayen, “Optimizing mixed
autonomy traffic flow with decentralized autonomous vehicles and
multi-agent reinforcement learning,” ACM Transactions on CyberPhysical Systems, vol. 7, no. 2, pp. 1–22, 2023.
[116] S. Han, S. Zhou, J. Wang, L. Pepin, C. Ding, J. Fu, and F. Miao, “A
multi-agent reinforcement learning approach for safe and efficient behavior planning of connected autonomous vehicles,” IEEE Transactions
on Intelligent Transportation Systems, vol. 25, no. 5, pp. 3654–3670,
2023.
[117] E. Okafor, E. Okafor, O. Ubadike, M. Abba, P. Jemitola, A. Shinkafi,
G. Sule, M. Bonnet, and D. Udekwe, “Electric vehicle integrated with
pmsm and regenerative braking system speed evaluation based on
diverse control strategies,” Journal of Southwest Jiaotong University,
vol. 56, no. 6, 2021.
[118] X. Gong, P. Lyu, and B. Wang, “Cooperative motion planning and
decision-making for cavs at roundabouts: A data-efficient learning-

21

based iterative optimization method,” IEEE Internet of Things Journal,
2024.
[119] Y. Xu, Y. Shi, X. Tong, S. Chen, and Y. Ge, “A multi-agent reinforcement learning based control method for connected and autonomous
vehicles in a mixed platoon,” IEEE Transactions on Vehicular Technology, 2024.
[120] A. Peake, J. McCalmon, B. Raiford, T. Liu, and S. Alqahtani, “Multiagent reinforcement learning for cooperative adaptive cruise control,”
in 2020 IEEE 32nd International Conference on Tools with Artificial
Intelligence (ICTAI). IEEE, 2020, pp. 15–22.
[121] M. Acquarone, F. Miretti, D. Misul, and L. Sassara, “Cooperative
adaptive cruise control based on reinforcement learning for heavy-duty
bevs,” IEEE Access, vol. 11, pp. 127 145–127 156, 2023.
[122] W. Zhou, D. Chen, J. Yan, Z. Li, H. Yin, and W. Ge, “Multi-agent
reinforcement learning for cooperative lane changing of connected and
autonomous vehicles in mixed traffic,” Autonomous Intelligent Systems,
vol. 2, no. 1, p. 5, 2022.
[123] E. Leurent, “Safe and efficient reinforcement learning for behavioural
planning in autonomous driving,” Ph.D. dissertation, Université de
Lille, 2020.
[124] S. Chen, J. Dong, P. Ha, Y. Li, and S. Labi, “Graph neural network
and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,” Computer-Aided Civil and Infrastructure
Engineering, vol. 36, no. 7, pp. 838–857, 2021.
[125] Y. F. Chen, M. Liu, M. Everett, and J. P. How, “Decentralized noncommunicating multiagent collision avoidance with deep reinforcement
learning,” in 2017 IEEE international conference on robotics and
automation (ICRA). IEEE, 2017, pp. 285–292.
[126] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multiagent, reinforcement learning for autonomous driving,” arXiv preprint
arXiv:1610.03295, 2016.
[127] D. Troullinos, G. Chalkiadakis, I. Papamichail, and M. Papageorgiou,
“Collaborative multiagent decision making for lane-free autonomous
driving,” in Proceedings of the 20th international conference on autonomous agents and multiagent systems, 2021, pp. 1335–1343.
[128] R. S. Thakkar, A. S. Samyal, D. Fridovich-Keil, Z. Xu, and U. Topcu,
“Hierarchical control for head-to-head autonomous racing,” Field
Robotics, vol. 4, pp. 46–69, 2024.
[129] B. Toghi, R. Valiente, D. Sadigh, R. Pedarsani, and Y. P. Fallah, “Altruistic maneuver planning for cooperative autonomous vehicles using
multi-agent advantage actor-critic,” arXiv preprint arXiv:2107.05664,
2021.
[130] M. Li, Z. Cao, and Z. Li, “A reinforcement learning-based vehicle
platoon control strategy for reducing energy consumption in traffic
oscillations,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 32, no. 12, pp. 5309–5322, 2021.
[131] J. Cui, Y. Liu, and A. Nallanathan, “Multi-agent reinforcement
learning-based resource allocation for uav networks,” IEEE Transactions on Wireless Communications, vol. 19, no. 2, pp. 729–743, 2019.
[132] H. Qie, D. Shi, T. Shen, X. Xu, Y. Li, and L. Wang, “Joint optimization
of multi-uav target assignment and path planning based on multi-agent
reinforcement learning,” IEEE access, vol. 7, pp. 146 264–146 272,
2019.
[133] H. X. Pham, H. M. La, D. Feil-Seifer, and A. Nefian, “Cooperative and
distributed reinforcement learning of drones for field coverage,” arXiv
preprint arXiv:1803.07250, 2018.
[134] A. Shamsoshoara, M. Khaledi, F. Afghah, A. Razi, and J. Ashdown,
“Distributed cooperative spectrum sharing in uav networks using multiagent reinforcement learning,” in 2019 16th IEEE Annual Consumer
Communications & Networking Conference (CCNC). IEEE, 2019, pp.
1–6.
[135] S. Jung, W. J. Yun, J. Kim, and J.-H. Kim, “Coordinated multiagent deep reinforcement learning for energy-aware uav-based big-data
platforms,” Electronics, vol. 10, no. 5, p. 543, 2021.
[136] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-scale fleet management via multi-agent deep reinforcement learning,” in Proceedings
of the 24th ACM SIGKDD international conference on knowledge
discovery & data mining, 2018, pp. 1774–1783.
[137] X. Li, J. Zhang, J. Bian, Y. Tong, and T.-Y. Liu, “A cooperative
multi-agent reinforcement learning framework for resource balancing
in complex logistics network,” arXiv preprint arXiv:1903.00714, 2019.
[138] M. Saifullah, K. Papakonstantinou, C. Andriotis, and S. Stoffels,
“Multi-agent deep reinforcement learning with centralized training and
decentralized execution for transportation infrastructure management,”
arXiv preprint arXiv:2401.12455, 2024.

[139] W. van Heeswijk, “Strategic bidding in freight transport using deep
reinforcement learning,” Annals of Operations Research, pp. 1–38,
2022.
[140] H. Wang, W. Lin, T. Peng, Q. Xiao, and R. Tang, “Multi-agent deep
reinforcement learning-based approach for dynamic flexible assembly
job shop scheduling with uncertain processing and transport times,”
Expert Systems with Applications, vol. 270, p. 126441, 2025.
[141] I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and
L. Feng, “Safe multi-agent reinforcement learning via shielding,” arXiv
preprint arXiv:2101.11196, 2021.
[142] J. Bernhard, K. Esterle, P. Hart, and T. Kessler, “Bark: Open behavior
benchmarking in multi-agent environments,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,
2020, pp. 6201–6208.
[143] H. Zhu, T. Han, W. K. Alhajyaseen, M. Iryo-Asano, and H. Nakamura,
“Can automated driving prevent crashes with distracted pedestrians? an
exploration of motion planning at unsignalized mid-block crosswalks,”
Accident Analysis & Prevention, vol. 173, p. 106711, 2022.
[144] R. Bautista-Montesano, R. Galluzzi, K. Ruan, Y. Fu, and X. Di, “Autonomous navigation at unsignalized intersections: A coupled reinforcement learning and model predictive control approach,” Transportation
research part C: emerging technologies, vol. 139, p. 103662, 2022.
[145] X. Jia, L. Sun, H. Zhao, M. Tomizuka, and W. Zhan, “Multi-agent
trajectory prediction by combining egocentric and allocentric views,”
in Conference on Robot Learning. PMLR, 2022, pp. 1434–1443.
[146] H. Ma, Y. Sun, J. Li, M. Tomizuka, and C. Choi, “Continual multi-agent
interaction behavior prediction with conditional generative memory,”
IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 8410–8417,
2021.
[147] A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh, “Learning latent
representations to influence multi-agent interaction,” in Conference on
robot learning. PMLR, 2021, pp. 575–588.
[148] J. Wiederer, A. Bouazizi, M. Troina, U. Kressel, and V. Belagiannis,
“Anomaly detection in multi-agent trajectories for automated driving,”
in Conference on Robot Learning. PMLR, 2022, pp. 1223–1233.
[149] X. Mo, Z. Huang, Y. Xing, and C. Lv, “Multi-agent trajectory prediction with heterogeneous edge-enhanced graph attention network,” IEEE
Transactions on Intelligent Transportation Systems, vol. 23, no. 7, pp.
9554–9567, 2022.

