{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# offline_rl.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/aviralkumar2907/CQL (CQL), https://github.com/kzl/decision-transformer (DT)\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Papers:\n",
    "# 1. \"Conservative Q-Learning for Offline Reinforcement Learning\" (CQL)\n",
    "# 2. \"Decision Transformer: Reinforcement Learning via Sequence Modeling\"\n",
    "\n",
    "def cql_loss(q_values, current_action_q, q_function):\n",
    "    \"\"\"\n",
    "    Paper: Conservative Q-Learning (Kumar et al., 2020)\n",
    "    Innovation: Penalize Q-values for out-of-distribution actions.\n",
    "    Loss = MSE(Bellman) + alpha * (logsumexp(Q(s, all_a)) - Q(s, pi(a)))\n",
    "    \"\"\"\n",
    "    # 1. Push down Q-values of random/unseen actions (logsumexp)\n",
    "    # 2. Push up Q-values of actions actually in the dataset (current_action_q)\n",
    "    cql_diff = torch.logsumexp(q_values, dim=1) - current_action_q\n",
    "    return cql_diff.mean()\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Decision Transformer (Chen et al., 2021)\n",
    "    Innovation: RL as Sequence Modeling (RTG, State, Action) -> Action\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, act_dim, hidden_size, max_length=20):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_t = nn.Embedding(1000, hidden_size) # Timesteps\n",
    "        self.embed_s = nn.Linear(state_dim, hidden_size)\n",
    "        self.embed_a = nn.Linear(act_dim, hidden_size)\n",
    "        self.embed_R = nn.Linear(1, hidden_size) # Returns-to-go\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=4), num_layers=3\n",
    "        )\n",
    "        self.predict_action = nn.Linear(hidden_size, act_dim)\n",
    "\n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        # Input: (Batch, Seq_Len, Dim)\n",
    "        # We interleave: R_t, s_t, a_t\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "        \n",
    "        embed_s = self.embed_s(states)\n",
    "        embed_a = self.embed_a(actions)\n",
    "        embed_R = self.embed_R(returns_to_go)\n",
    "        embed_t = self.embed_t(timesteps)\n",
    "\n",
    "        # Add time embeddings\n",
    "        embed_s = embed_s + embed_t\n",
    "        embed_a = embed_a + embed_t\n",
    "        embed_R = embed_R + embed_t\n",
    "\n",
    "        # Stack inputs: R1, s1, a1, R2, s2, a2...\n",
    "        stacked_inputs = torch.stack((embed_R, embed_s, embed_a), dim=2)\n",
    "        stacked_inputs = stacked_inputs.reshape(batch_size, 3 * seq_len, self.hidden_size)\n",
    "        \n",
    "        # Causal Mask (Autoregressive)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(3 * seq_len)\n",
    "        \n",
    "        # Process\n",
    "        output = self.transformer(stacked_inputs, mask=mask)\n",
    "        \n",
    "        # Predict action (based on R and s, so we take the middle embedding of the triplet)\n",
    "        # Reshape back to (Batch, Seq, 3, Hidden)\n",
    "        output = output.reshape(batch_size, seq_len, 3, self.hidden_size)\n",
    "        \n",
    "        # We want to predict action given R and s. \n",
    "        # In the stack (R, s, a), 's' is at index 1.\n",
    "        action_preds = self.predict_action(output[:, :, 1]) \n",
    "        \n",
    "        return action_preds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}