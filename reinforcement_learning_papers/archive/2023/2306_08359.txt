Hierarchical Task Network Planning for Facilitating
Cooperative Multi-Agent Reinforcement Learning
Xuechen Mua , Hankz Hankui Zhuob; * , Chen Chenc , Kai Zhanga , Chao Yub and Jianye Haoc

arXiv:2306.08359v1 [cs.AI] 14 Jun 2023

a Jilin University
b Sun Yat-sen University
c Huawei Noah’s Ark Lab

Abstract.
Exploring sparse reward multi-agent reinforcement learning
(MARL) environments with traps in a collaborative manner is a complex task. Agents typically fail to reach the goal state and fall into
traps, which affects the overall performance of the system. To overcome this issue, we present SOMARL, a framework that uses prior
knowledge to reduce the exploration space and assist learning. In
SOMARL, agents are treated as part of the MARL environment, and
symbolic knowledge is embedded using a tree structure to build a
knowledge hierarchy. The framework has a two-layer hierarchical
structure, comprising a hybrid module with a Hierarchical Task Network (HTN) planning and meta-controller at the higher level, and a
MARL-based interactive module at the lower level. The HTN module and meta-controller use Hierarchical Domain Definition Language (HDDL) and the option framework to formalize symbolic
knowledge and obtain domain knowledge and a symbolic option set,
respectively. Moreover, the HTN module leverages domain knowledge to guide low-level agent exploration by assisting the metacontroller in selecting symbolic options. The meta-controller further
computes intrinsic rewards of symbolic options to limit exploration
behavior and adjust HTN planning solutions as needed. We evaluate
SOMARL on two benchmarks, FindTreasure and MoveBox, and report superior performance over state-of-the-art MARL and subgoalbased baselines for MARL environments significantly.

1

Introduction

Deep multi-agent reinforcement learning (MARL) achieves satisfactory results on multiple challenging problems, such as games [20],
autonomous driving [14], etc., which are modeled as multi-agent systems [1]. Since the development of the simplest method, independent Q-learning (IQL) [31], which views each agent as an individual,
there have been many advanced MARL approaches, such as VDN
[29], QMIX [25], CommNet [28], DyMA [33], COMA [5], G2ANet
[15], MADDPG [16], MAPPO [36], MAVEN [18]. Although those
approaches have effectively addressed the difficulties of effective exploration as well as agent communication due to the growth of states
and action spaces [25], they are all developed based on the assumption of dense rewards. There are, however, many real-world scenarios
with sparse rewards, where the environment does not provide immediate rewards to agents. For example, in a multi-agent soccer game,
∗ Corresponding Author. Email: zhuohank@mail.sysu.edu.cn.

most of the actions, such as passing and dribbling, cannot be rewarded, and the environment only gives non-zero rewards when one
side scores a goal [8]. Current MARL approaches often fail to to
learn policies effectively in this multi-agent setting due to the joint
actions of agents affecting the multi-agent system and the lack of
non-zero reward drive.
To address this issue, one way is to abstract the sparse reward environment and reduce the exploration space of agents, such as the
subgoal-based approach, which has been widely used in single-agent
environments [11, 37, 34, 17]. In MARL, a subgoal-based approach
for sparse reward was proposed by Tang et al. [32], which is a twolayer hierarchical model (h-MARL) using temporal abstraction to
model MARL and incorporating neural networks, where higher layer
agents learn the assignment of subgoal and lower layer agents explore the environment based on selected subgoals. Yang et al. [35] introduced the hidden space into h-MARL such that high-level agents
can learn how to select complementary latent skill variables. However, the higher and lower layers of both methods are designed by
neural networks, leading to low data efficiency and the lack of interpretability in multi-agent systems. In addition to this class of domain knowledge-based hierarchical methods, Jeon et al. [8] also proposed a replay buffer-based subgoal automatic generation technique
(MASER), which avoids the manual design of subgoals. However, in
some scenarios, agents may trigger traps in the environment and die
before exploring the goal state, leading to a lack of information about
the target state in the replay buffer. As shown in Figure 1, when the
box (green block) is moved to the target area (orange area) or the trap
(gray area) by two agents (red and blue), the environment will terminate and the two agents will receive corresponding rewards. As the
box is closer to the trap, the agent may give up exploring the target
area in order to get a higher reward in a short period of time, which
ultimately results in the replay buffer not collecting goal states and
not generating valid subgoals.
To address the above issue, we propose a novel architecture that
combines MARL with symbolic planning in MAS, called Symbolic
Selection for Multi-Agent Reinforcement Learning (SOMARL).
Specifically, we assume there exists a bijective relationship between
knowledge and MARL environment, and based on this assumption,
we propose a scheme for the construction of symbolic knowledge.
Then, we formalize the symbolic knowledge using a hierarchical
domain definition language (HDDL) [7] and option framework to
obtain the domain knowledge and the symbolic option set, respectively. Further, the planning module searches for solutions in domain

and delayed reward signals (e.g. Montezuma’s Revenge), it lacks interpretability and data efficiency because of higher and lower level
neural network architectures. To alleviate these issues, a class of hierarchical frameworks that integrate knowledge-based symbolic planning with RL was proposed [4, 6, 12, 34, 17]. In this paradigm, highlevel is required to collect prior knowledge, which is formulated as
a formal, logic-based language such as Planning Domain Definition
Language (PDDL [19]), and guide low-level agent to explore effectively.
Considering the existence of multi-agent scenarios in the real
world, Tang et al. [32] extended h-DRL on MARL, in which the hierarchical structure based on Qmix network (h-Qmix) is considered
to be the most effective. On this basis, HSD [35] introduces skill
and extends h-Qmix. In addition, Andrychowicz et al. [2] proposed
to use replay buffer to define subgoal. MASER [8] extended it to
multi-agents and proposed to consider both local Q-value and global
Q-value of the agents, making it possible to select a more efficient
subgoal from the replay buffer. However, since this method generates subgoals from the replay buffer, in some multi-agent scenarios
with traps, the agents may fail to explore the goal state so that the
replay buffer cannot generate effective subgoals. On the other hand,
the subgoal based on symbolic knowledge makes SOMARL more
interpretable on MASs compared to existing methods.

Figure 1. An illustration of agents falling into a trap and causing the replay
buffer to collect no goal state information. The goal of the two agents (red
and blue) is to move the box (green block) from the initial state to the target
area (orange region).

knowledge, and the meta-controller selects appropriate symbolic options from the set of symbolic options to assign to low-level agents
according to the planning solution and guides them to explore the environment. Notably, in order for the planning module to take into account low-level environment exploration in its solutions and to constrain the behavior of agents in low-level exploration, we define a
new intrinsic reward calculation method in the meta-controller.
We summarize the contributions of this paper as follows:
• To the best of our knowledge, this is the first work to integrate
HTN planner and MARL in subgoal-based multi-agent reinforcement learning, i.e., applying the HTN to guide MARL and in turn
being assisted by MARL in planning.
• We propose a unified knowledge construction scheme to reduce
the exploration space of agents and improve the goal success rate
of agents in different collaborative sparse reward MARL environments with traps. Moreover, hierarchies with symbolic options
have better interpretability than h-MARL.
• We design a novel method of intrinsic reward calculation in the
meta-controller. On the one hand, it is used to constrain the exploration behavior of low-level agents. On the other hand, it is used
to control the planning solution of the HTN planner.

2
2.1

3

In this section, we explain the relevant notation and briefly introduce
the key aspects of symbolic knowledge and MARL.

3.1

Hierarchical Task Network (HTN) Planning with
HDDL

In the paper, we adopt the definition of HTN Planning proposed by
Holler et al. [7]. in the form of an extension language to PDDL,
called Hierarchical Domain Definition Language (HDDL). Specifically, HDDL is based on a quantifier-free first-order predicate logic
L to describe properties of the world, and its set of instance proposition P is used to represent state S. To facilitate the distinction
from state in MARL, we call it symbolic state. In contrast to classical planning that focuses on symbolic states, HDDL includes two
different types of tasks: primitive tasks (also known as actions)
and abstract tasks (also known as compound tasks). It is worth
noting that the definition of actions is consistent with PDDL, i.e.,
a = (name, pre + , pre − , eff + , eff − ), where name is the name of
the action model, and (pre + , pre − ) and (eff + , eff − ) denote the preconditions and effects of the action model, respectively.
In particular, an abstract task t is just a task name X, i.e., an
atom. Its declaration contains two elements: the name X and argument types. For example, (:task deliver :parameters (? r1 - robot ?r2
- robot ?p - package)) is a declaration of an abstract task, where "deliver" is the name of the abstract task, and "robot" and "package" are
two argument types of the abstract task. Its purpose is not to induce
state transition but to reference a pre-defined mapping M to one or
more task networks tn by which that abstract task can be refined. A
task network tn over a set of task names X is a tuple (I, ≺, α, V C),
where I is a set of task identifiers (using I because an abstract task
may appear multiple times in the same task network), ≺ is a strict
partial order over I, and α and V C are mappings from I to X (linking task networks to abstract tasks) and a set of variable constraints,
respectively. It is worth noting that I, ≺, and V C in tn can all be
empty. A method m ∈ M is a triple (X, tn, V C), which is used

Related Work
MARL with centralized training and decentralized
execution

Centralized training and decentralized execution (CTDE) [23, 25]
is a key architecture in MARL. The architecture uses the central
controller to assist the agents in learning policies during training,
each agent makes decisions based on local observations and no
longer needs a central controller during execution. VDN [29], QMIX
[25], QTRAN [27], etc. are value-based CTDE methods., and the
other is policy-based methods, including MADDPG [16], COMA
[5], MAPPO [36], etc. MAPPO is an extension of the PPO [26] on
multi-agent scenarios, using an actor-critic architecture, where critic
learns a central value function. In this paper, we use MAPPO as the
low-level learning algorithm.

2.2

Preliminaries

Subgoal Assignment

In the single-agent case, Kulkarni et al. [11] proposed a Hierarchical Deep Reinforcement Learning (h-DRL) method, which abstractly
models the original MDP problem to obtain a subgoal set, and divides
the solution process into high-level and low-level. Among them, the
high level selects the appropriate subgoal from the subgoal set by
meta policy and assigns it to the low-level, and then the low-level
learns to explore the environment to reach the subgoal. Although
this method successfully solves a class of Atari tasks with sparse
2

to transform a task network tn to another. Here, X and V C are the
name of the abstract task and the set of variable constraints, respectively. It should be noted that the subtasks of a decomposition method
m have two keywords: ordered-subtasks (supporting totally ordered)
and subtasks (allowing partially ordered tasks), and all abstract tasks
and actions in the domain can be used as subtasks.
The HTN planning domain D = (L , TP , TC , M ) contains the
predicate logic L , the primitive task set TP , the abstract task set TC ,
and the set of decomposition methods M used to decompose TC ,
which implicitly defines all symbolic states S. Based on the domain,
we further define a planning problem (D, sI , tn I ), where sI and tn I
represent the initial symbolic state and initial task network, respectively. Finally, solving the HTN planning problem yields a solution
Π consisting of primitive tasks. Note that if Π contains abstract tasks,
the subtasks it contains must be executed in order.

3.2

next section. It is worth noting that since the planning problem and
options come from the same TREE , it naturally connects HTN planning with the option framework, and TREE acts as a bridge between
the two.

Figure 2. The SOMARL framework consists of four sub-modules: knowledge, meta-controller, HTN planner, and agents.

Multi-Agent Reinforcement Learning (MARL)

A fully cooperative environment can be described as Decentralized
Partially Observable Markov Decision
D Processes (DEC-POMDP)
E
e A,
e O,
e Pe, R,
e n, γ , where
[22], which is represented by a tuple S,
e= A
e1 × A
e2 × · · · × A
en is the joint action
Se is the state space, A
e s; i) represpace composed of the action space of all agents, oei = O(e
sents the local observation of the i-th agent in the state se. P (e
s′ |e
s, e
a)
is the state transition probability of n agents from state se to se′ given
e s, e
e
a. R(e
a) represents the shared reward function and γ is the discount factor. In a fully cooperative environment, each agent optimizes the common discounted accumulated reward J(θ1 , · · · , θn ) =
P
e st , e
at )] by its own policy function πθi (e
ai |e
oi ).
Eset ,eat [ t γ t R(e

3.3

4

We formalize the MARL environment as
e A,
e O,
e R,
e Pe, n, γ),
(TREE , L , TP , TC , M, sI , tn I , S,
can be divided into three parts:

a

tuple
which

• Firstly, we design the knowledge on the MARL environment in
advance (denoted as TREE ), which is a bijection between HTN
planning and option framework.
• Secondly, (L , TP , TC , M, sI , tn I ) are symbolically described
using HDDL on TREE .
• Finally, we define a decision problem using a DEC-POMDP tuple
e A,
e O,
e R,
e Pe, n, γ).
(S,

Option Framework

Hierarchical Reinforcement Learning (HRL) [11, 21, 24] is a class
of methods that models high-level behavior as temporal abstraction
actions. We consider a two-layer hierarchical model, and use the option framework to model temporal abstract actions as options [30].
Specifically, an option o = (Io (e
s), πo (e
s), βo (e
s)), where Io (e
s) is
used as the initial condition of option o to determine whether o is executable at state se, the termination condition βo (e
s) is used to determine whether o is terminable at state se, and πo (e
s) is a policy function
that maps state se to low-level action. In the option framework, one
needs to define the set of options in advance, and agents at higher
levels need to learn the policy of choosing the optimal option and
assigning it to agents at lower levels, which we call the controller
level.

3.4

The SOMARL Framework

We consider solving the above problem using SOMARL. Specifically, SOMARL is designed as a closed-loop framework, in which
the HTN planner and meta-controller improve the exploration efficiency, and in turn, the low-level policy improves the quality of
planning. The SOMARL framework is shown in Figure 2, which
consists of four modules: (1) Knowledge: constructs the planning
problem and symbolic option set required by HTN planner and metacontroller, respectively, (2) Meta-controller: selects symbolic options
based on the plan and calculate intrinsic reward, (3) HTN Planner:
generates plans based on domain knowledge and intrinsic rewards of
methods, and (4) Multi-agent: interacts with the environment based
on the chosen symbolic options. We will address each module in detail below.

Formalization of Multi-Agent Reinforcement
Learning (MARL) Environments

4.1

In this section, we define the formal rules for MARL environments
based on a tree structure, which serves as the foundation of SOMARL. Specifically, a tree is a tuple TREE = (V, E), where V
is the set of nodes and E ⊆ V × V is a set of edges. It is worth
noting that TREE does not contain cycles and has a special node
called the root node. For example, consider a tree TREE = (V, E),
where V = a, b, c, d, e, f, g, h, i and E contains the following edges:
(a, b), (b, c), (c, d), (c, e), (b, f ), (f, g), (f, h), (h, i). Here, a is the
root node.
Furthermore, we can use HDDL and the option framework to construct planning problems and options on TREE , as described in the

Construction of Knowledge

To obtain domain knowledge and the symbolic option set, we first
design a formal description TREE for the MARL environment.
We define each node v in TREE to correspond to a state in the
MARL environment, and each edge describes a change in the state.
Specifically, the goal state sgoal and the initial state sinitial of the
MARL environment are defined as the root node and the leaf node of
TREE , respectively. In addition, we assume that there are enumerable and necessary intermediate states, i.e., subgoals, in the MARL
environment from sinitial to sgoal . These subgoals constitute the
child nodes of sgoal , and we can determine the parent-child relationship in TREE based on their temporal order.
3

As shown in Figure 3(a), in the FindTreasure environment, there
are two agents (red and blue), a lever (yellow), a trap (pink), and a
treasure (green). Only when one agent is at the lever, the other agent
can enter the room above (as shown in Figure 3(b)). We set the goal
state sgoal as when one of the agents reaches the green area, and
define two subgoals (as shown in Figure 3(c)): one is that one agent
is inside the channel and the other agent is at the lever (subgoal-1),
and the other is that both agents are in the room below (subgoal2). Obviously, subgoal-2 must occur before subgoal-1, so the node
representing subgoal-2 is the child of the node representing subgoal1 (since sgoal is the root node and sinitial is the leaf node, as shown
in Figure 3(d)).
By defining subgoals, we expand TREE vertically. Furthermore,
by instantiating each subgoal, we obtain the horizontal expansion of
TREE , which allows different instances of agents of the same subgoal to correspond to different nodes of the same layer in TREE .
For example, if the state in which one agent is at the lever and the
other agent is at the front of the channel is considered as a subgoal,
then such subgoal instantiation corresponds to different tree nodes of
the same layer in TREE , as shown in Figure 3(e).
In this way, we can obtain a formal rule TREE that define the path
from the initial state to the goal state in the MARL environment.

Figure 4. An example of building domain knowledge on TREE . The
nodes and edges in TREE correspond to the abstract tasks and decomposition methods in HDDL.

HTN planning, we define that the task network tn of each decomposition method m is a single abstract task, and the variable constraint
V C and order ≺ in m are empty. This definition enables the subtask
in m to be determined by the parent-child relationship in TREE . As
shown in Figure 4(c), the abstract task corresponding to subgoal-2
is a subtask of the abstract task corresponding to subgoal-1 (since
subgoal-2 is a child node of subgoal-1). In particular, the goal state
(the root node of TREE ) corresponds to the initial abstract task tn I .
This approach of constructing the corresponding HTN planning
domain from TREE ensures that each path from the leaf nodes (the
symbolic initial state sI ) to the root node (the initial abstract task
tn I ) of TREE is one of the solutions in HTN planning, which includes both primary tasks and abstract tasks.

4.1.2

In this paper, we extend the symbolic option [10] to multi-agent scenarios and use it to associate HTN planning with MARL. A symbolic
option is defined as (s, π, s′ ), where s and s′ represent different subgoals, which correspond to two nodes of the same edge in TREE
(e.g., subgoal-1 and subgoal-2 in Figure 3c). π is a low-level policy
that interacts with the environment and is trained with intrinsic reward. Considering that a symbolic option represents the transition of
subgoals and HDDL decomposition methods M can correspond to
each edge e ∈ E in TREE , we use TREE as a bridge to establish
a one-to-one correspondence between symbolic options and decomposition methods M . Obviously, the number of edges in TREE is
equal to the number of decomposition methods m in HDDL, which
is also equal to the number of symbolic options. We use equations
(1) and (2) to determine whether the initial condition Iso (e
s) and the
termination condition βso (e
s) hold at state se.

Figure 3. A figure of building T REE based on the states of a MARL environment. (a) and (b) show the description of the FindTreasure environment,
(c) and (d) show instances of vertically expanding T REE based on subgoals,
and (e) shows how to instantiate subgoals and horizontally expand T REE.

4.1.1

Symbolic Option Set

Domain Knowledge

After obtaining the symbolic knowledge TREE of the MARL
environment, we use the HDDL language to formulate it as
(L , TP , TC , M, sI , tn I ). Specifically, we define that the symbolic
initial state sI corresponds to a leaf node in TREE , and therefore, a
primitive task TP corresponds to an edge connecting the leaf node.
Additionally, in HTN planning, while an abstract task TC may not
involve specific action details (as it describes high-level task), it can
be further decomposed into primitive tasks that describe state transitions, so that the ultimate goal of the abstract task TC is to achieve a
certain state. Considering that each child node in TREE corresponds
to a subgoal, we can define that the abstract task TC corresponds to a
set of nodes V in TREE (one-to-one correspondence). Furthermore,
the instance objects contained in the subgoals of TREE correspond
to the argument types of the abstract task TC . As shown in Figure
4(b), these two abstract tasks correspond to subgoal-1 and subgoal-2
in TREE , and their parameter types are the instance objects contained in subgoal-1 and subgoal-2.
In addition, to establish a one-to-one correspondence between the
edges e ∈ E in TREE and the decomposition methods m ∈ M in

(
Iso (e
s) =

T rue,

s = F (e
s),

F alse,

otherwise.

(
βso (e
s) =

T rue,

s′ = F (e
s),

F alse,

otherwise.

(1)

(2)

It is worth noting that the symbolic option makes SOMARL more
interpretable than the black-box neural network-based algorithms.

4.2

Meta-controller

In SOMARL, the meta-controller is responsible for selecting the
symbolic option to assign to the agents according to the planning
solution, and calculating the intrinsic reward of the symbolic option
after the agents have finished interacting with the environment.
4

4.2.1

Symbolic Option and Intrinsic Reward
k
hmadd (n) = hf (n) + max′n∈Sub(n) hmax
add (′n) − Rm

According to the construction of knowledge, the elements of the
method set M in HTN planning have a one-to-one correspondence
with the elements in the symbolic option set O (since they both correspond to edges e ∈ E in TREE ). Furthermore, we can use the
mapping between the method set M and the symbolic option set O
to select symbolic options from the planning solution. Specifically,
the planning solution consists of both primitive tasks and abstract
tasks, and we first use the mapping alpha in the task networks tn I
(all of which are single abstract tasks) to correspond these abstract
tasks to decomposition tasks. Then, we construct a new sequence of
tasks by using the parent-child relationships in TREE as the order
between decomposition tasks and primitive tasks. Finally, the metacontroller selects the corresponding option from the symbolic option
set in the order of the new sequence of decomposition tasks and primitive tasks.
In order to better train the policy of symbolic option, we design
an intrinsic reward to constrain the transition of s to s′ in symbolic
option, see equation (3):

(4)

P
k
Where Rm
= kj=1 ri indicates the cumulative intrinsic reward
of the symbolic option corresponding to method m up to the k-th
episode. At the initial moment, the intrinsic reward of each symbolic
option are initialized to 0.
The improvement enables hmadd (n) in pyHIPOP to not only consider the cost of primitive tasks in the method and the contextual
information related to the method but also take into account the success of the symbolic option’s policy π in guiding the low-level agent
to reach s′ . The better the policy π of the symbolic option, the higher
its intrinsic reward, and the smaller hmax
add (n) will be. We name the
HTN planner that solves planning solutions using this novel heuristic
function pyHIPOP+.

4.4

Planning and Learning

Considering that the purpose of an HTN planner is to assist low-level
agents in exploration, the planning solution obtained by the planner
should not only have a small cost function value but also take into
account the completion of symbolic options by the low-level agents
during actual execution. The completion of symbolic options can be
evaluated using the intrinsic reward of symbolic options. Therefore,
we improve the existing HTN planner to consider the intrinsic rewards of low-level agents when searching for solutions.

We need to pre-design the knowledge of MARL environment.
Specifically, we define the subgoal set on the MARL environment,
and instantiate their agents to obtain a symbolic knowledge T REE
with the goal state of environment as the root node. Then we use
HDDL to formulate T REE to get domain knowledge D and use the
correspondence between symbolic option and the edges in T REE
to construct symbolic option set O. Finally, we define the cumulative
intrinsic reward for each symbolic option and their dictionary is denoted as R. Note that the initial value of each symbolic option in R
is 0.
As shown in Algorithm 1, SOMARL takes domain knowledge D,
symbolic option set O and cumulative intrinsic reward dictionary R
of symbolic options as input. When an episode k starts, we first get
the initial state se0 of the environment. After that pyHIPOP+ receives
the updated cumulative intrinsic reward dictionary R from the previous episode to solve the domain knowledge D to get planning solution Πk . Next, the meta-controller selects elements from symbolic
option set O to assign to low-level in order of method in Πk , combining the correspondence between symbolic option and method,
until the environment terminates or the selected symbolic option is
reached. Note that under each selected symbolic option som , lowlevel multi-agents will select actions according to the policy of som ,
and get external reward re and the reduced count of external reward
n after interacting with the environment. Then meta-controller calculates the intrinsic reward ri according to the formula (3), and trains
the policy of som through ri and interaction data. Finally, the cumulative intrinsic reward of som is updated by ri and used for the
planning solution at the next episode.

4.3.1

5

(
ri =

re + ϕ − n · c,

βso (e
s) = T rue,

0,

otherwise.

(3)

Where, ri is a non-zero value if and only if the termination condition of so is true. ϕ and c are hyperparameters of SOMARL, which
generally need to be tuned on different MARL environments. Specifically, ϕ represents the extra reward received by low-level agents
when βso (e
s) = T rue, so it is a positive number. c is the penalty
term coefficient. In the paper, we take ϕ = 5, c = 0.01 on two environments. re and n represent the external reward and the reduced
count of external reward after the agents perform an action respectively. We specify that when re < 0, n = 1, otherwise n = 0.
In addition, in order to improve the interaction efficiency of lowlevel agents, we use the intrinsic reward of the symbolic option to
correct the planning solution of the HTN planner.

4.3

HTN Planner

Improved Heuristic Function

Experiments

In this section, we evaluate SOMARL on two collaborative multiagent sparse reward environments with traps, FindTreasure and
MoveBox. In addition to comparing with baseline multi-agent algorithms (VDN [29], QMIX [25], DyMA [33], MAVEN [18], CommNet [28], COMA [5], G2ANet [15], MADDPG [16], MAPPO [36]),
we also compared with two subgoal-based algorithms designed for
multi-agent sparse reward. One of the algorithms is MASER that defines the subgoal by the replay buffer [8], and the other is the hierarchical multi-agent reinforcement learning algorithm (h-MAPPO)
designed on the same subgoal set and low-level training procedure
as our SOMARL.

pyHIPOP is an HTN planner that uses heuristic functions to drive
the search for solutions in plan space [13]. Specifically, in pyHIPOP,
the additive heuristic function hmadd (n) for a method m calculates
the cost of the method. It is composed of the maximum additive
heuristic function hmax
add (n) [3] of the primitive tasks in the method
and the heuristic function hf (n) associated with the method, i.e.,
′
hmadd (n) = hf (n) + maxn′ ∈Sub(n) hmax
add (n ). Here, Sub(n) denotes the set of child nodes of n, and hf (n) can be any heuristic
function associated with the method, such as the number of primitive
tasks in Oi or the length of the critical path. We extend hmadd (n) as
follows:
5

Algorithm 1: SOMARL
Input : domain knowledge D, symbolic option set O,
intrinsic reward dictionary
R = {so0 : 0, so0 : 0, ..., soN : 0}
1 for t = 1, 2, ..., Max _episodes do
2
env.reset()
3
Πk ← pyHIPOP +.solve(D, R)
4
while (env isn’t terminal) and (steps < Max_Steps) do
5
so m = meta-controller (Πk , O)
6
while (env isn’t terminal) and (steps < Max_Steps)
s) = F alse) do
and (βsom (e
7
oe, re , oenext , n ← Multi-agents(som )
8
ri = IntrinsicReward (re , n)
9
Training som with oe, oenext and ri
10
R[som ]k = R[som ]k−1 + ri
11
end
12
end
13 end

Figure 6. Design examples of the symbolic knowledge TREE for tasks in
FindTreasure and MoveBox environments, where the nodes of TREE represent subgoals and edges represent symbolic options. Specifically, the three
tasks in the MoveBox environment (task1/2/3) share the same TREE .

curve among all methods and achieves the highest rewards rewards at the final moments.
• Interpretability As shown in Figure 6(a), the T REE consisting of subgoals and symbolic options describes the decomposition
of the target task, and its combination with the planning solution
can enhance the interpretability of SOMARL. For example, the
symbolic option sequence corresponding to the planning solution
at the 29th episode is [so0 , so4 , so8 , so10 , so12 ] (the blue path in
Figure 6(a)). When r1 is not at the trap and r2 is at the lever, the
termination condition of so0 is successfully reached by low-level,
then the meta-controller assigns so4 to low-level until r1 is at the
channel and r2 is at the lever, which triggers the termination condition of so4 .
• Success Rate Figure 7(b) shows the success rates of different
methods to reach the goal state. From the figure, it can be seen
that SOMARL eventually achieves a higher success rate and the
exploration is overall smooth and less volatile. This suggests that
the symbolic knowledge-based approach helps MARL reduce the
exploration space of agents.

Figure 5. Task design on two environments, FindTreasure and MoveBox.
(a) is a task on the FindTreasure environment, while (c), (d1), (e), and (f) are
four tasks on the MoveBox environment.

5.1

Experiments in the FindTreasure Environment

FindTreasure [9] contains two rooms. At the initial moment both
agents (red and blue) are in the lower room and one treasure (green)
is in the upper room, their positions are shown in Figure 5(a). In this
environment, if and only if one of the agents reaches the lever (yellow), the other agent can reach the upper room through the channel
(as shown in Figure 5(b)). If an agent reaches treasure, then each
agent’s reward is +100, and the environment terminates. It is worth
noting that there is a trap (pink) in FindTreasure, i.e. if one agent is
at the lever and the other agent is at the trap, the environment will
also be terminated and each agent’s reward will be increased by 3. In
addition, if agents hit an obstacle (black) once during the movement,
their reward is −0.1. The action space of FindTreasure contains five
elements: up/down/left/right/wait, and the observation space consists
of the position coordinates of two agents.

5.1.1

Figure 7. SOMARL and the performance of various baselines on the FindTreasure environment are shown in the following figures. (a) and (b) show the
cumulative average reward and success rate in reaching the goal state during
the interaction process of each algorithm, respectively. (c) shows the ablation
experiment of intrinsic reward in SOMARL, where the y-axis represents the
cumulative average reward value.

Results

5.1.2

Ablation Study

We conduct ablation study on intrinsic reward to verify its effect in
SOMARL. From Figure 7(c), we can see that the exploration ability
of SOMARL_2 decreases significantly after removing the intrinsic
reward of symbolic option compared to the original version. This
indicates that the intrinsic reward of the symbolic option enhances
the learning ability of the model, and the penalty term in the intrinsic
reward constrains behavior of low-level agents.

As shown in Figure 6(a), we design the symbolic knowledge of FindTreasure and evaluate SOMARL on this basis.
• Data-efficiency As shown in Figure 7(a), SOMARL can get
higher rewards faster at the same step. It is noted that although
G2ANet achieves higher rewards over time, this method is the
least stable. In contrast, SOMARL has the smoothest learning
6

5.2

Experiments in the MoveBox Environment

The second environment we use is called MoveBox [9]. As shown in
Figure 5(c), there is a black base in the MoveBox with a box (green)
on it, and two agents (red and blue) are located on both sides of the
base. The goal of both agents is to move the box to the top area, at
which point each agent’s reward is +100 and the environment terminates. Note that there is a trap in the middle area of MoveBox, when
two agents move the box to that area, their respective rewards are
+10, and the environment is terminated.
In addition, in order to increase the difficulty of reaching the target
area for the agents and the box, we add key entities at three locations
near the trap (noted as task1, task2, and task3, as in Figures 5(d1),
(e), (f)). When two agents carry the box to get the key, each agent’s
reward +5 and an additional channel to the target area appears (as in
Figure 5(d2)).

Figure 10. SOMARL and baselines’ results on task-2 in MoveBox environment are shown below. (a) and (b) are the accumulated average reward and
success rate of reaching the goal state during the interaction process for each
algorithm.

Figure 11. SOMARL and baselines’ experimental results on task-3 in
MoveBox environment are shown in (a) and (b), respectively.
Figure 8. SOMARL and various baselines’ results on task-0 in the MoveBox are shown in this figure. Panels (a) and (b) display the cumulative average
reward and success rate in reaching the goal state during the interaction process, respectively.

5.2.1

Further, because the key is placed in front of the trap in MoveBoxtask3(see Figure 5(f)), this cause all baselines to forgo exploration
and simply choose to take the key and enter the trap to end the interaction. Although this short-sighted behavior makes some baselines
gain higher rewards in the short term, it is clear that such exploration
by agents is not acceptable. In addition, symbolic knowledge makes
SOMARL better interpretable in different scenarios. For example,
the symbolic option sequence corresponding to the planning solution
in the 6th episode is [so1 , so4 , so7 , so10 ] (the blue path in Figure
6(c)). When r1 and r2 take the box together, so4 terminates. Then
the meta-controller continues to assign so7 to low-level, instructing
agents to explore the environment until agents get the key and enter
the left channel.

Results

As in Figures 6(b), (c), we design the symbolic knowledge of MoveBox. Notice that although the keys have different placements, they
all have the same symbolic knowledge. This shows that symbolic
knowledge is only related to the number of subgoals, not to the trajectory of low-level agents, and thus has some generalization. The
performance of different methods on different versions of MoveBox
is shown in Figure 8-11.

6

Conclusion

In summary, we propose a novel multi-agent framework that combines HTN planning and MARL for cooperative multi-agent sparse
reward environments with traps. The framework leverages domain
knowledge and symbolic options to construct a method for generating symbolic knowledge on the MARL environment. The HTN planner solves the domain knowledge, and the meta-controller selects the
symbolic option from the symbolic option set to guide the exploration of the agents. During exploration, the meta-controller computes intrinsic reward to constrain agent behavior and guide the HTN
planner. Our experiments demonstrated that SOMARL outperforms
existing methods in terms of performance, interpretability, and success rate stability.
To further enhance the universality of SOMARL, it would be interesting to investigate automatically learning (HTN) domain knowledge [41, 40] and planning with the learnt knoweldge [39] for facilitating multi-agent reinforcement learning. It would also be interesting to investigate the integration between plan recognition [38, 42]
and multi-agent reinforcement learning.

Figure 9. SOMARL and various baselines’ experimental results on task-1
in the MoveBox. The cumulative average reward obtained during the interaction process and the success rate of reaching the target state are shown in (a)
and (b), respectively, for each algorithm.

On the whole, SOMARL achieves higher rewards faster in each
scenario than other methods, far ahead of those baselines (see Figures 8(a), 9(a), 10(a), 11(a)). Combining their success rate, only SOMARL is able to reach the goal state consistently (see Figures 8(b),
9(b), 10(b), 11(b)). This is because MoveBox has a stronger trap design compared to FindTreasure, which causes all baselines to terminate exploration early.
7

References
[21]
[1] CP Andriotis and KG Papakonstantinou, ‘Managing engineering systems with large state and action spaces through deep reinforcement
learning’, Reliability Engineering & System Safety, 191, 106483,
(2019).
[2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider,
Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI
Pieter Abbeel, and Wojciech Zaremba, ‘Hindsight experience replay’,
Advances in neural information processing systems, 30, (2017).
[3] Pascal Bercher, Gregor Behnke, Daniel Höller, and Susanne Biundo,
‘An admissible htn planning heuristic.’, in IJCAI, pp. 480–488, (2017).
[4] Alessandro Cimatti, Marco Pistore, and Paolo Traverso, ‘Automated
planning’, Foundations of Artificial Intelligence, 3, 841–867, (2008).
[5] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas
Nardelli, and Shimon Whiteson, ‘Counterfactual multi-agent policy
gradients’, in Proceedings of the AAAI conference on artificial intelligence, volume 32, (2018).
[6] Chad Hogg, Ugur Kuter, and Hector Munoz-Avila, ‘Learning methods to generate good plans: Integrating htn learning and reinforcement
learning’, in Twenty-Fourth AAAI Conference on Artificial Intelligence,
(2010).
[7] Daniel Höller, Gregor Behnke, Pascal Bercher, Susanne Biundo, Humbert Fiorino, Damien Pellier, and Ron Alford, ‘Hddl: An extension to
pddl for expressing hierarchical planning problems’, in Proceedings of
the AAAI Conference on Artificial Intelligence, volume 34, pp. 9883–
9891, (2020).
[8] Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung,
‘Maser: Multi-agent reinforcement learning with subgoals generated
from experience replay buffer’, in International Conference on Machine Learning, pp. 10041–10052. PMLR, (2022).
[9] Shuo Jiang and Christopher Amato, ‘Multi-agent reinforcement learning with directed exploration and selective memory reuse’, in Proceedings of the 36th Annual ACM Symposium on Applied Computing, pp.
777–784, (2021).
[10] Mu Jin, Zhihao Ma, Kebing Jin, Hankz Hankui Zhuo, Chen Chen,
and Chao Yu, ‘Creativity of ai: Automatic symbolic option discovery for facilitating deep reinforcement learning’, in Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pp. 7042–7050,
(2022).
[11] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh
Tenenbaum, ‘Hierarchical deep reinforcement learning: Integrating
temporal abstraction and intrinsic motivation’, Advances in neural information processing systems, 29, (2016).
[12] Matteo Leonetti, Luca Iocchi, and Peter Stone, ‘A synthesis of automated planning and reinforcement learning for efficient, robust
decision-making’, Artificial Intelligence, 241, 103–130, (2016).
[13] Charles Lesire and Alexandre Albore, ‘pyhipop–hierarchical partialorder planner’, IPC 2020, 13, (2020).
[14] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas
Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra,
‘Continuous control with deep reinforcement learning’, arXiv preprint
arXiv:1509.02971, (2015).
[15] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and
Yang Gao, ‘Multi-agent game abstraction via graph attention neural
network’, in Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 7211–7218, (2020).
[16] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel,
and Igor Mordatch, ‘Multi-agent actor-critic for mixed cooperativecompetitive environments’, Advances in neural information processing
systems, 30, (2017).
[17] Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson, ‘Sdrl: interpretable and data-efficient deep reinforcement learning leveraging
symbolic planning’, in Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 2970–2977, (2019).
[18] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson, ‘Maven: Multi-agent variational exploration’, Advances in Neural
Information Processing Systems, 32, (2019).
[19] Drew McDermott, Malik Ghallab, Adele E. Howe, Craig A. Knoblock,
Ashwin Ram, Manuela M. Veloso, Daniel S. Weld, and David E.
Wilkins, ‘Pddl-the planning domain definition language’, (1998).
[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,
Andreas K Fidjeland, Georg Ostrovski, et al., ‘Human-level control

[22]
[23]
[24]
[25]

[26]
[27]

[28]
[29]

[30]
[31]
[32]

[33]

[34]
[35]
[36]
[37]

[38]
[39]
[40]
[41]

8

through deep reinforcement learning’, nature, 518(7540), 529–533,
(2015).
Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine,
‘Data-efficient hierarchical reinforcement learning’, Advances in neural information processing systems, 31, (2018).
Frans A Oliehoek and Christopher Amato, A concise introduction to
decentralized POMDPs, Springer, 2016.
Afshin Oroojlooy and Davood Hajinezhad, ‘A review of cooperative
multi-agent deep reinforcement learning’, Applied Intelligence, 1–46,
(2022).
Ronald Parr and Stuart Russell, ‘Reinforcement learning with hierarchies of machines’, Advances in neural information processing systems,
10, (1997).
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson, ‘Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement learning’, in
International conference on machine learning, pp. 4295–4304. PMLR,
(2018).
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,
and Oleg Klimov, ‘Proximal policy optimization algorithms’, arXiv
preprint arXiv:1707.06347, (2017).
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero,
and Yung Yi, ‘Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning’, in International conference on machine learning, pp. 5887–5896. PMLR, (2019).
Sainbayar Sukhbaatar, Rob Fergus, et al., ‘Learning multiagent communication with backpropagation’, Advances in neural information
processing systems, 29, (2016).
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al., ‘Valuedecomposition networks for cooperative multi-agent learning’, arXiv
preprint arXiv:1706.05296, (2017).
Richard S Sutton, Doina Precup, and Satinder Singh, ‘Between mdps
and semi-mdps: A framework for temporal abstraction in reinforcement
learning’, Artificial intelligence, 112(1-2), 181–211, (1999).
Ming Tan, ‘Multi-agent reinforcement learning: Independent versus cooperative agents’, in ICML, (1993).
Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang
Zhang, Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng,
Changjie Fan, et al., ‘Hierarchical deep multiagent reinforcement
learning with temporal abstraction’, arXiv preprint arXiv:1809.09332,
(2018).
Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, and Yang Gao, ‘From few to
more: Large-scale dynamic multiagent curriculum learning’, in Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 7293–7300, (2020).
Fangkai Yang, Daoming Lyu, Bo Liu, and Steven Gustafson, ‘Peorl:
Integrating symbolic planning and hierarchical reinforcement learning
for robust decision-making’, arXiv preprint arXiv:1804.07779, (2018).
Jiachen Yang, Igor Borovikov, and Hongyuan Zha, ‘Hierarchical cooperative multi-agent reinforcement learning with skill discovery’, arXiv
preprint arXiv:1912.03558, (2019).
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen,
and Yi Wu, ‘The surprising effectiveness of ppo in cooperative, multiagent games’, arXiv preprint arXiv:2103.01955, (2021).
Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen,
‘Generating adjacency-constrained subgoals in hierarchical reinforcement learning’, Advances in Neural Information Processing Systems,
33, 21579–21590, (2020).
Hankz Hankui Zhuo, ‘Recognizing multi-agent plans when action models and team plans are both incomplete’, ACM Trans. Intell. Syst. Technol., 10(3), 30:1–30:24, (2019).
Hankz Hankui Zhuo and Subbarao Kambhampati, ‘Model-lite planning: Case-based vs. model-based approaches’, Artif. Intell., 246, 1–21,
(2017).
Hankz Hankui Zhuo, Héctor Muñoz-Avila, and Qiang Yang, ‘Learning
hierarchical task network domains from partially observed plan traces’,
Artif. Intell., 212, 134–157, (2014).
Hankz Hankui Zhuo, Qiang Yang, Rong Pan, and Lei Li, ‘Crossdomain action-model acquisition for planning via web search’, in Proceedings of the 21st International Conference on Automated Planning
and Scheduling, ICAPS 2011, Freiburg, Germany June 11-16, 2011,

eds., Fahiem Bacchus, Carmel Domshlak, Stefan Edelkamp, and Malte
Helmert. AAAI, (2011).
[42] Hankz Hankui Zhuo, Yantian Zha, Subbarao Kambhampati, and Xin
Tian, ‘Discovering underlying plans based on shallow models’, ACM
Trans. Intell. Syst. Technol., 11(2), 18:1–18:30, (2020).

9

