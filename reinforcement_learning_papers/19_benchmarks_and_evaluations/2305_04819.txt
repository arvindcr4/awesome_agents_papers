Local Optimization Achieves Global Optimality in
Multi-Agent Reinforcement Learning
Yulai Zhao‚àó

Zhuoran Yang‚Ä†

Zhaoran Wang‚Ä°

Jason D. Lee¬ß

arXiv:2305.04819v1 [cs.LG] 8 May 2023

Abstract
Policy optimization methods with function approximation are widely used in multiagent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference
lemma that characterizes the landscape of multi-agent policy optimization, we find
that the localized action value function serves as an ideal descent direction for each
local policy. Motivated by the observation, we present a multi-agent PPO algorithm
in which the local policy of each agent is updated similarly to vanilla PPO. We prove
that with standard regularity conditions on the Markov game and problem-dependent
quantities, our algorithm converges to the globally optimal policy at a sublinear rate.
We extend our algorithm to the off-policy setting and introduce pessimism to policy
evaluation, which aligns with experiments. To our knowledge, this is the first provably
convergent multi-agent PPO algorithm in cooperative Markov games.

1

Introduction

Recently, multi-agent reinforcement learning (MARL) has demonstrated many empirical
successes, e.g., popular strategy games such as Go [Silver et al., 2016], StarCraft II [Vinyals
et al., 2019], and poker [Brown and Sandholm, 2018]. In contrast to vanilla reinforcement
learning (RL), which is only concerned with a single agent seeking to maximize the total
reward, MARL studies how multiple agents interact with the shared environment and
other agents.
Policy optimization methods are widely used in MARL. These algorithms often parameterize policies with a function class and compute the gradients of the cumulative reward
using the policy gradient theorem [Sutton et al., 1999] or its variants (e.g., NPG Kakade
[2001] and PPO [Schulman et al., 2017]) to update the policy parameters.
Despite the empirical successes, theoretical studies of policy optimization in MARL
are very limited. Even for the cooperative setting where the agents share a common goal:
maximizing the total reward function, numerous challenges arise [Zhang et al., 2021]. (1)
non-stationarity: each action taken by one agent affects the total reward and the transition of state. Consequently, each learning agent must learn to adapt to the changing
environment caused by other agents. From the optimization perspective, the geometry
of the multi-agent policy optimization problem becomes unclear. Direct application of
traditional single-agent analysis becomes vague due to the lack of stationary Markovian
property, which states that evolution in the future only depends on the previous state
and individual action. (2) scalability: taking other agents into consideration, each individual agent would face the joint action space, whose dimension increases exponentially
‚àó

Princeton University; yulaiz@princeton.edu
Yale University; zhuoran.yang@yale.edu
‚Ä°
Northwestern University; zhaoranwang@gmail.com
¬ß
Princeton University; jasonlee@princeton.edu
‚Ä†

1

with the number of agents. Thus, having numerous agents in the environment problematizes the theoretical analysis of MARL. (3) function approximation: closely related to the
scalability issue, the state space and joint action space are often immense in MARL, promoting function approximation to become a necessary component in MARL at the ease
of computation and statistical analysis.
In this paper, we aim to answer the following fundamental question:
Can we design a provably convergent multi-agent policy optimization algorithm in the
cooperative setting with function approximation?
We answer the above question affirmatively. We propose a multi-agent PPO algorithm
in which the local policy of each agent is updated sequantially in a similar fashion as
vanilla PPO algorithm [Schulman et al., 2017]. In particular, we leverage a multi-agent
performance difference lemma (cf. Lemma 4.1), assuming the joint policy is decomposed
into conditional dependent policies. Such a lemma characterizes the landscape of policy
optimization, showing the superiority of using localized action value functions as the decent
direction for each local policy. Such factorized structure essentially bypasses the nonstationarity and scalability concerns. To address large state spaces, we parameterize each
local policy using log-linear parametrization and propose to update the policy parameters
via KL divergence-regularized mirror descent, where the descent direction is estimated
separately. Combining these results, we obtain our multi-agent PPO algorithm. We prove
that the multi-agent PPO algorithm converges to globally optimal policy at a sublinear
rate. Furthermore, we extend multi-agent PPO to the off-policy setting in which policy
is evaluated using samples collected according to data distribution ¬µ. We prove similar
theoretical guarantees under a coverage assumption of the sampling distribution.
We summarize our contributions below.
Our contributions. First, by focusing on the factorized policies, we prove a multi-agent
version of the performance difference lemma showing that the action value functions are
ideal descent directions for local policies. Such a geometric characterization functions as
a remedy for the non-stationarity concern, motivating our multi-agent PPO algorithm.
Second, we adopt the log-linear function approximation
for

 the policies. We prove that
q
multi-agent PPO converges at a sublinear O

N
1‚àíŒ≥

log |A|
K

rate up to some statistical

errors incurred in evaluating/improving policies, where K is the number of iterations, N
is the number of agents and |A| is the action space of each individual agent. The sample
complexity depends polynomially on N , thus breaking the curse of scalability.
Third, we propose an off-policy variant of the multi-agent PPO algorithm and introduce
pessimism into policy evaluation. The algorithm also converges sublinearly to the globally
e ‚àí 13 ). Here, n is the number of samples used to
optimal policy up to the statistical error O(n
estimate the critics.1 A key feature of the sample complexity bound is that it only requires
single-policy concentrability.
To our knowledge, this is the first provably convergent multi-agent PPO algorithm in
cooperative Markov games with function approximation.
Organization. This paper is organized as follows. In Section 2, we review related literature. In Section 3, we formally describe the problem setup and introduce the necessary
definitions. In Section 4, we state the main multi-agent PPO algorithm in detail. We
further extend our results to the off-policy setting in Section 5. We conclude in Section 6
and defer the proofs to the Appendix.
1 e

O (¬∑) hides logarithmic factors.

2

2

Related Work

Policy optimization Many empirical works have proven the validity and efficiency of
policy optimization methods in games and other applications [Silver et al., 2016, 2017,
Guo et al., 2016, Tian et al., 2019]. These works usually update the policy parameter in
its parametric space using the pioneering policy gradient (PG) theorem by Sutton et al.
[1999], or many PG variants invented to improve the empirical performances of vanilla
PG methods. In particular, Kakade [2001] introduced the natural policy gradient (NPG)
algorithm which searched for the steepest descent direction within the parameter space
based on the idea of KL divergence-regularization. Trust region learning-based algorithms
are often regarded as advanced policy optimization methods in practice [Lillicrap et al.,
2015, Duan et al., 2016], showing superior performances with stable updates. Specifically, TRPO [Schulman et al., 2015] and PPO [Schulman et al., 2017] could be seen as
KL divergence-constrained variants of NPG. A benign feature of these algorithms is the
monotonic improvement guarantees of the expected return.
Despite prosperous empirical findings, the lack of convexity often impedes the development of theories for policy optimization methods. Denote K and T as the number of
1
iterations and samples. Agarwal et al. [2020] showed an iteration complexity of O(K ‚àí 2 )
1
and a sample complexity of O(T ‚àí 4 ) for online NPG with function approximation. Shani
1
et al. [2020] considered a sample-based TRPO and proved a OÃÉ(T ‚àí 2 ) rate converging to
the global optimum, which could be improved to OÃÉ(1/T ) when regularized. Making minor
modifications to the vanilla PPO algorithm, Liu et al. [2019] presented a convergence
1
rate of O(K ‚àí 2 ) to global optima when parameterizing both policy and Q functions with
neural networks. The key to their analysis is the desirable one-point monotonicity in
infinite-dimensional mirror descent that assists in characterizing the policy updates without convexity. We also make use of similar one-point properties in our multi-agent PPO
algorithm analysis.
MARL Markov Game (MG) is a commonly used model to characterize the multi-agent
decision-making process [Shapley, 1953, Littman, 1994], which can be regarded as a multiagent extension to the Markov Decision Process (MDP). Policy-based algorithms could
generalize to large states through function approximation. There has been growing interest
in developing provably efficient algorithms for Markov games [Daskalakis et al., 2020, Cen
et al., 2021, Zhao et al., 2022, Ding et al., 2022, Cen et al., 2022]. These works often
studied competitive RL settings, e.g., zero-sum games. Their convergence rates usually
depended on various notions of concentrability coefficient and may not scale tightly under
the worst scenario.
Policy optimization for MARL Applying policy optimization methods in the MARL
setting is more complicated than in the single-agent setting because of the non-stationary
environment faced by each agent [Zhang et al., 2021]. A learning paradigm called centralized training with decentralized execution (CTDE) is often used in practice [Kraemer
and Banerjee, 2016, Lowe et al., 2017, Foerster et al., 2018, Yang et al., 2018, Wen et al.,
2019, Zhang et al., 2020]. In CTDE, a joint centralized value function helps to address the
non-stationarity issue caused by other agents. Each agent has access to the global state
and actions of other agents during training, thus allowing them to adjust their policy
parameters individually. For instance, Lowe et al. [2017] proposed a multi-agent policy
gradient algorithm in which agents learned a centralized critic based on the observations
and actions of all agents.
Trust region learning [Schulman et al., 2015] has recently been combined with the
CTDE paradigm to ensure monotonic improvements. In particular, IPPO [de Witt et al.,
3

2020] and MAPPO [Yu et al., 2021] showed strong performances of PPO-based methods
in the cooperative setting. The practical efficacy of these methods is usually restricted by
the homogeneity assumption, where the agents share a common action space and policy
parameter. Theoretically, providing statistical guarantees for policy optimization algorithms in MARL is more complicated than single-agent scenario [Zhang et al., 2021]. In
Markov games, the non-stationary environment faced by each agent precludes direct application of the single-agent convergence analysis. A recent attempt by Kuba et al. [2022]
proposed the first set of trust region learning algorithms in MARL that enjoyed monotonic improvement guarantees assuming neither homogeneity of agents nor value function
decomposition rule. The critical observation leading to their results is the multi-agent
advantage function decomposition rule that builds the sequential policy update structure.
However, they did not show rates of convergence. In this work, we design a new, provably
convergent PPO algorithm for fully cooperative Markov games that converges to globally
optimal at policy at sublinear rates by taking advantage of this conditional dependency
structure.
Pessimism-based RL methods Though being able to account for large state/action
spaces, function approximation also has its own drawbacks. A significant issue arising
in using function approximators is the usual occurrence of a positive bias in value function Thrun and Schwartz [1993]. The learner may not receive an accurate assessment.
Numerous empirical works leverage the principle of pessimism to correct such overestimation [Fujimoto et al., 2018, Laskin et al., 2020, Lee et al., 2020, Moskovitz et al., 2021].
For example, to reduce the evaluation bias brought by function approximation, Fujimoto
et al. [2018] constructed the Bellman target by choosing the minimum of two value estimates as an intuitive estimate lower bound. Their approach took a pessimistic view of the
value function.
On the theoretical side, a growing body of literature in offline reinforcement learning
has also focused pessimism to account for datasets lacking data coverage [Liu et al., 2020,
Jin et al., 2021, Uehara and Sun, 2021, Rashidinejad et al., 2021, Zhan et al., 2022].
Technically, these works aimed at maximizing the worst-case rewards that a trained agent
could obtain. Instead of relying on coverage assumptions on dataset [Munos, 2003, Munos
and SzepesvaÃÅri, 2008, Chen and Jiang, 2019], these methods provided dataset-dependent
performance bounds, thus providing robust results for datasets lacking exploration for
which traditional methods do not apply. We focus on the off-policy setting in Section 5
where we leverage the Bellman-consistent pessimism [Xie et al., 2021]. We show concrete
bounds under linear function approximation by assuming a sampling oracle that provides
rewards and transition estimates that are used in approximating action value functions.

3

Preliminaries

In this section, we introduce necessary notations, problem setup, and some useful quantities that will be frequently used in this work.

3.1

Setup and Notations

Setup We consider a fully-cooperative Markov game [Shapley, 1953, Littman, 1994],
which is defined by a tuple (N , S, A, P, r, Œ≥). Here, N = {1, . . . , N } denotes the set
of agents, S is the finite state space, A = AN is the product of finite action spaces
of all agents(i.e., joint action space), P : S √ó A √ó S ‚Üí [0, 1] decides the transition

4

scheme, a reward function r : S √ó A ‚Üí [0, 1], and Œ≥ ‚àà [0, 1) is the discount factor.2
The agents interact with the environment according to the following protocol: at time
step t, the agents are at state st ‚àà S; every agent i takes action ait ‚àà A, drawn from
its policy œÄ i (¬∑|st ), which together with actions of other agents
a joint action at =
QN gives
i (¬∑|s ); the agents receive
(a1t , . . . , aN
)
‚àà
A,
drawn
from
the
joint
policy
œÄ(¬∑|s
)
=
œÄ
t
t
t
i=1
a joint reward rt = r(st , at ) ‚àà R, and move to st+1 ‚àº P(¬∑|st , at ). Given the joint policy
œÄ, the transition probability function P, and the initial state distribution œÅ, we define the
discounted occupancy state-action distribution as
dœÄ (s, a) = (1 ‚àí Œ≥)E

‚àû
X

PrœÄ (st = s, at = a|s0 ‚àº œÅ).

t=0

The standard value function and action value function are defined as
"‚àû
#
X
VœÄ (s) ,
Œ≥ t rt s0 = s ,
E
a0:‚àû ‚àºœÄ,s1:‚àû ‚àºP

QœÄ (s, a) ,

E

s1:‚àû ‚àºP,a1:‚àû ‚àºœÄ

t=0

"‚àû
X

#
Œ≥ t rt s0 = s, a0 = a .

t=0

The standard advantage function considering all agents is written as AœÄ (s, a) ,
QœÄ (s, a) ‚àí VœÄ (s). Later, we shall introduce the agents-specific advantage functions.
Let ŒΩœÄ (s) and œÉœÄ (s, a) = œÄ(a|s)¬∑ŒΩœÄ (s) denote the stationary state distribution and the
stationary state-action distribution associated with a joint policy œÄ, respectively. Define
the underlying optimal policy as œÄ‚àó . We use ŒΩ‚àó and œÉ‚àó in this paper to indicate ŒΩœÄ‚àó and
œÉœÄ‚àó for simplicity.
Throughout this paper, we pay close attention to the contribution of different subsets
of agents to the performance of the whole team. We introduce the following multi-agent
notations before proceeding to multi-agent definitions.
Notations In this work, we index the N agents with integers from 1 to N and use set
N = {i|i = 1, ¬∑ ¬∑ ¬∑ , N } to represent all agents. We use m ‚àà N to indicate the specific m-th
agent. In particular, the set notation on the superscript of a term represents the quantities
associated with agents in that set. For example, a{1,2,3} represents the joint action of agents
1, 2 and 3. We may write index k on superscript when we refer to the specific k-th agent.
When bold symbols are used without any superscript (e.g., a), they consider all agents.
For simplicity, let (m : m0 ) be shorthand for set: {i|m ‚â§ i ‚â§ m0 , i ‚àà N }. An example is
œÄ 1:m (¬∑|s) which represents the joint policy considering agents 1, 2 ¬∑ ¬∑ ¬∑ , m.
We now introduce the multi-agent action value functions and advantage functions that
characterize contributions from specific sub-agents.
Definition 3.1. Let P be a subset in N . The multi-agent action value function associated
with agents in P is



QPœÄ s, aP , EaÃÉ‚àºœÄÃÉ QœÄ s, aP , aÃÉ ,
here we use a tilde over symbols to refer to the complement agents, namely aÃÉ = {ai |i 6‚àà
P, i ‚àà N }.
Let P, P 0 ‚äÜ N be two disjoint subsets of agents. The multi-agent advantage function
0
is defined below. Essentially, it accounts for the improvements of setting agents aP upon
setting agents aP , while all other agents follow œÄ.





0
0
0
0
APœÄ s, aP , aP , QPœÄ ‚à™P s, aP , aP ‚àí QPœÄ s, aP .
2

For clarity, we assume N agents share the same set of actions. It is straightforward to generalize our
results to the setting where action sets are different. See Section 4.

5

The multi-agent Bellman operators are defined by generalizing the classic versions.
Definition 3.2. For m ‚àà N and any function f : S √ó Am ‚Üí
‚àí R we define multi-agent
m
m
1:m
S√óA
S√óA
Bellman operator TœÄ : R
7‚Üí R
as
TœÄ1:m f (s, a1:m ) := E r(s, a1:m , aÃÉ) + Œ≥
aÃÉ‚àºœÄÃÉ

E

f (s0 , œÄ 1:m )

aÃÉ‚àºœÄÃÉ
s0 ‚àºP(¬∑|s,a1:m ,aÃÉ)

where f (s0 , œÄ 1:m ) is shorthand for Ea0 ‚àºœÄ1:m (¬∑|s0 ) f (s0 , a0 ).
It is straightforward to see that Q1:m
is the unique fixed point for TœÄ1:m , which correœÄ
sponds to the classic single-agent Bellman operator.

3.2

KL divergence-regularized mirror descent

We review the mirror-decent formulation in provable single-agent PPO algorithm [Liu
et al., 2019]. At the k-th iteration, the policy parameter Œ∏ is updated via


Œ∏k+1 ‚Üê
‚àí arg max EÃÇ hAk (s, ¬∑), œÄŒ∏ (¬∑|s)i ‚àí Œ≤k KL (œÄŒ∏ (¬∑ks)kœÄŒ∏k (¬∑|s)) .
(1)
Œ∏

Hereafter we shall use h¬∑, ¬∑i to represent the inner product over A. The expectation is
taken over EÃÇ, which is an empirical estimate of stationary state-action distribution ŒΩœÄŒ∏k ,
and Ak is estimate of advantage function AœÄŒ∏k .
Adopting the KL-divergence, (1) is closely related to the NPG [Kakade, 2001] update.
As a variant, this formulation is slightly different from the vanilla PPO [Schulman et al.,
2017]: here KL(œÄŒ∏ (¬∑ks)kœÄŒ∏k (¬∑|s)) is used instead of KL(œÄŒ∏k (¬∑ks)kœÄŒ∏ (¬∑|s)). Such variation
is essential for presenting provable guarantees, which will be shown in the next section.

4

Multi-Agent PPO

Recall that ŒΩ‚àó is the stationary state distribution for œÄ‚àó . In this section, we desire to
maximize the expected value function under distribution ŒΩ‚àó : J(œÄ) , Es‚àºŒΩ‚àó VœÄ (s).
This paper aims to present a trust region learning multi-agent algorithm that enjoys a
rigorous convergence theory. As we have mentioned, policy optimization for cooperative
MARL is challenging because the policy optimization problem becomes a joint optimization involving all the agents. It remains unclear: (a) what the landscape of the total
rewards as a multivariate function of the joint policy is and (b) what would be proper
policy descent directions for each agent. We come up with a solution to characterize the
landscape by taking advantage of a serial decomposition of the performance difference
lemma in MARL described below.
Lemma 4.1. For any joint policy œÄ we have
N

J(œÄ‚àó ) ‚àí J(œÄ) =

1 X
1‚àíŒ≥

E

s‚àºŒΩ‚àó
m=1 a1:m‚àí1 ‚àºœÄ 1:m‚àí1
‚àó

1:m‚àí1
Q1:m
, ¬∑), œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí œÄ m (¬∑|s, a1:m‚àí1 )
œÄ (s, a

where the inner product is over am ‚àà A.
1:m‚àí1 , am ) as
With this geometric characterization, we can justify that using Qm
œÄ (s, a
the descent direction and running KL divergence-regularized mirror descent for each agent
m ‚àà N can lead to a better total reward, which enables a serial optimization procedure.
Below we describe the algorithm in detail.
To represent conditional policies, we adopt log-linear parametrization.

6

Parametrization For the m-th agent (m ‚àà N ), its conditional policy depends on all
prior ordered agents a1:m‚àí1 . Given a coefficient vector Œ∏m ‚àà Œò, where Œò = {kŒ∏k ‚â§ R|Œ∏ ‚àà
Rd } is a convex, norm-constrained set. The probability of choosing action am under state
s is
exp (œÜ> (s, a1:m‚àí1 , am )Œ∏m )
œÄŒ∏m (am |s, a1:m‚àí1 ) = P
(2)
exp (œÜ> (s, a1:m‚àí1 , am )Œ∏m )
am ‚ààA

where œÜ is a set of feature vector representations. Without loss of generality, we impose
a regularity condition such that every kœÜk2 ‚â§ 1. This parametrization has been widely
used in RL literature [Branavan et al., 2009, Gimpel and Smith, 2010, Heess et al., 2013,
Agarwal et al., 2020, Zhao et al., 2022].3

4.1

Policy Improvement and Evaluation

At the k-th iteration, we have the current policy œÄŒ∏k , and we need to: (1) perform policy
evaluation to obtain the action value function estimates QÃÇœÄŒ∏k for determining the quality
of œÄŒ∏k . (2) perform policy improvement to update policy to œÄŒ∏k+1 .
For notational simplicity, we use ŒΩk and œÉk to represent stationary state distribution
ŒΩœÄk and the stationary state-action distribution œÉœÄk , which are induced by œÄŒ∏k .
Œ∏

Œ∏

m as the ideal update based
Policy Improvement At the k-th iteration, we define œÄÃÇk+1
1:m
on QÃÇ1:m
œÄŒ∏k (for agent m ‚àà N ), which is an estimator of QœÄŒ∏k . The ideal update is obtained
via the following update
m
‚Üê
‚àí arg max FÃÇ (œÄ m )
œÄÃÇk+1

(3)

œÄm

h

i
1:m‚àí1
, ¬∑), œÄ m (¬∑|s, a1:m‚àí1 )i ‚àí Œ≤k KL œÄ m (¬∑|s, a1:m‚àí1 )kœÄŒ∏km (¬∑|s, a1:m‚àí1 )
FÃÇ (œÄ m ) = E hQÃÇ1:m
œÄŒ∏ (s, a
œÉk

k

where Œ∏km is the parameter of the current conditional policy of the m-th agent. In above
equation, the distribution is taken over (s, a1:m‚àí1 ) ‚àº ŒΩk œÄŒ∏1:m‚àí1
, we write œÉk for simplicity.
k
>
m
Under log-linear parametrization: œÄŒ∏km ‚àù exp{œÜ Œ∏k }, we have the following closed-form
ideal policy update.
Proposition 4.2. Given an estimator QÃÇ1:m
œÄŒ∏k , the KL divergence-regularized update (3) has
the following explicit solution
o
n
1:m‚àí1
>
1:m‚àí1
m
m
.
œÄÃÇk+1
(¬∑|s, a1:m‚àí1 ) ‚àù exp Œ≤k‚àí1 QÃÇ1:m
(s,
a
,
¬∑)
+
œÜ
(s,
a
,
¬∑)Œ∏
œÄŒ∏
k
k

P
m
The proof is straightforward by adding the constraint:
am ‚ààA œÄ (¬∑) = 1 as a Lam
grangian multiplier to FÃÇ (œÄ ). See details in Appendix B.
m using a parameterized œÄ m ‚àù exp{œÜ> Œ∏ m }, we miniTo approximate the ideal œÄÃÇk+1
Œ∏k+1
k+1
mize the following mean-squared error (MSE) as a sub-problem
m
Œ∏k+1
‚Üê
‚àí arg min L(Œ∏m )

(4)

Œ∏m ‚ààŒò

where L(Œ∏m ) is defined as


m

L(Œ∏ ) = E (Œ∏
œÉk

m

‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , am ) ‚àí

1:m‚àí1 , am ) 
QÃÇ1:m
2
œÄŒ∏ (s, a
k

Œ≤k

3
We assume that all players share the same parameter set only for clarity. We only need minor modifications in the analysis to extend our results to the setting where N agents have different capabilities.
Specifically, we only need to treat norm bounds of updates (R), regularity conditions on features, and Œ≤
separately for each agent.

7

m . Moreover,
Intuitively, a small L(Œ∏) indicates that œÄŒ∏m is close to the ideal update œÄÃÇk+1
m
if œÄÃÇk+1 exactly lies in the log-linear function class, i.e., there exists a œë ‚àà Œò such that
m ‚àù exp {œÜ> œë}. Then we have L(œë) = 0.
œÄÃÇk+1
To solve the MSE minimization problem (4), we use the classic SGD updates. Let
stepsize be Œ∑, at each step t = 0, 1, ¬∑ ¬∑ ¬∑ , T ‚àí 1, parameter Œ∏ is updated via



1
)
Œ∏(t + ) ‚Üê
‚àí Œ∏(t) ‚àí 2Œ∑œÜ (Œ∏(t) ‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇ1:m
œÄŒ∏k
2
1
Œ∏(t + 1) ‚Üê
‚àí Œ†Œò Œ∏(t + )
2
where we omit (s, a1:m‚àí1 , am ) for simplicity, which is sampled from œÉk . See Algorithm 3
for the detailed solver.
Policy Evaluation In this step, we aim to examine the quality of the attained policy.
Thereby, a Q-function estimator is required. We make the following assumption.
Assumption 4.3. Assume we can access an estimator of Q function that returns QÃÇ. The
returned QÃÇ satisfies the following condition for all m ‚àà N at the k-th iteration



E

œÉk

1:m‚àí1 m
1:m‚àí1 m
QÃÇ1:m
, a ) ‚àí Q1:m
,a )
œÄŒ∏k (s, a
œÄŒ∏k (s, a

2 1/2

‚â§ Œækm .

We also have a regularity condition for the estimator: there exists a positive constant B,
such that for any m ‚àà N and (s, a1:m‚àí1 , am ) ‚àà S √ó Am‚àí1 √ó A,
1:m‚àí1 m
QÃÇ1:m
, a ) ‚â§ B.
œÄŒ∏ (s, a
k

In RL practice, such an estimator is often instantiated with deep neural networks
(DNNs) [Mnih et al., 2015]. While there has been recent interest in studying the theoretical
guarantees for DNNs as function approximators [Fan et al., 2020], we assume we have
access to such an estimator to ensure the generality of our algorithm. We note that policy
estimators like episodic sampling oracle that rolls out trajectories [Agarwal et al., 2020] or
neural networks [Mnih et al., 2015, Liu et al., 2019] could all be possible options here. As
a generalization, we introduce a specific value function approximation setting in Section 5,
in which we assume all Q-functions lie in linear class F. We further adopt the principle
of pessimism for better exploration.
Algorithm Equipped with the sub-problem solver for policy improvement and the Qfunction estimator, we are prepared to present the provable multi-agent PPO algorithm.
The pseudo-code is listed in Algorithm 1. The algorithm runs for K iterations. At
the k-th iteration, we estimate Q-function for each agent m ‚àà N via the estimator (cf.
Assumption 4.3) to measure the quality of œÄŒ∏k . The estimates would also serve as the
ideal descent direction for policy improvement. Since we use a constrained parametric
policy class, the ideal update is approximated with the best policy parameter Œ∏ ‚àà Œò by
minimizing the MSE problem (4), which runs SGD for T iterations (cf. Algorithm 3).
Thanks to the geometric characterization (cf. Lemma 4.1), we are guaranteed to reach a
globally improved total reward by updating each agent consecutively.

4.2

Theoretical Analysis

Our analysis relies on problem-dependent quantities. We denote weighted Lp -norm of
1
P
p p
function f on state-space X as kf kp,œÅ =
x‚ààX œÅ(x)|f (x)|
8

Algorithm 1 Multi-Agent PPO
Input: Markov game (N , S, A, P, r, Œ≥), penalty parameter Œ≤, stepsize Œ∑ for sub-problem,
number of SGD iterations T , number of iterations K.
Output: Uniformly sample k from 0, 1, ¬∑ ¬∑ ¬∑ K ‚àí 1, return œÄÃÑ = œÄŒ∏k .
1: Initialize Œ∏0m = 0 for every m ‚àà N .
2: for k = 0, 1, . . . , K ‚àí 1 do
‚àö
3:
Set parameter Œ≤k ‚Üê
‚àíŒ≤ K
4:
for m = 1, ¬∑ ¬∑ ¬∑ , N do
T ‚àí1
5:
Sample {st , a1:m‚àí1
, am
t }t=0 from œÉk = ŒΩk œÄŒ∏k .
t
1:m‚àí1
1:m
, am ) for each sample .
6:
Obtain QÃÇœÄŒ∏ (s, a
k
m .
7:
Feed samples into Algorithm 3, obtain Œ∏k+1
8:
end for
9: end for
Definition 4.4. At the k-th iteration, for m ‚àà N we define the following problemdependent quantity using Radon-Nikodym derivatives
œÜm
k =

d(ŒΩ‚àó œÄ‚àó1:m )
d(ŒΩk œÄŒ∏1:m
)
k

2,œÉk

These conditions are the well-known concentrability coefficients [Munos, 2003, Farahmand
et al., 2010, Chen and Jiang, 2019] for the factorized policy. Still, our conditions are
structurally simpler and weaker because they are only density ratios between stationary
state-action distributions, not requiring trajectories to roll out.
Now we are prepared to present the main theorem that characterizes the global convergence rate.
Theorem 4.5. Under Assumption 4.3, for the output policy œÄÃÑ attained by Algorithm 1
R
in the fully cooperative Markov game, set Œ∑ = G‚àö
and
T
s
Œ≤=

N log |A| +

N B 2 /2
PN PK‚àí1
m=1

m
m
k=0 (‚àÜk + Œ¥k )

.

After K iterations, we have J(œÄ‚àó ) ‚àí J(œÄÃÑ) upper bounded by
Ô£´ ‚àö s
Ô£∂
PN PK‚àí1 m
m
B N N log |A| + m=1 k=0 (‚àÜk + Œ¥k ) Ô£∏
OÔ£≠
1‚àíŒ≥
K


‚àö m
m
m‚àí1
m + Œæk
m
where ‚àÜm
=
2(œÜ
+
œÜ
)
¬∑

and Œ¥km = 2œÜm‚àí1
m
k
k
k
k . Here k is the statistical
k
k
Œ≤k
error of a PPO iteration: for agent m ‚àà N ,

2
m
2
EœÉk (Œ∏k+1
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇ1:m
‚â§ (m
œÄŒ∏
k )
k

where we omit (s, a1:m‚àí1 , am ) for simplicity.
Let approx be the approximation capability of the log-linear policy class we adopt, then
1
m
k = approx + O(T ‚àí 4 ).
Theorem 4.5 explicitly characterizes the performance of the output œÄÃÑ in terms of
the number of iterations and the iteration errors. When PPO updates are ideal, namely,
9

viewing Œ¥k0 , ‚àÜm
k to be 0 for any m ‚àà N , and k < K , the rate simplifies to O



NB
1‚àíŒ≥

q

log |A|
K


.

1

The dependency on iteration K is O(K ‚àí 2 ), matching the same rate as the sample-based
single-agent NPG analysis [Agarwal et al., 2020, Liu et al., 2019].
The proof of Theorem 4.5 further requires the following parts: mirror-descent update analysis used in [Liu et al., 2019] and Lemma 4.1 that builds sequential dependency
structure among the agents. The full proof is deferred to Appendix B.

4.3

Compare with Independent Learning

In MARL, independent learning refers to a class of algorithms that train multiple agents
independently. In these methods, each agent has its own policy function that maps the
agent‚Äôs observations to its actions. The policies are optimized using policy gradient methods in a decentralized manner without explicit communication or coordination, and without explicitly modeling the behavior of the other agents. Independent learning methods
are widely used in MARL due to its strong performance and efficiency.
In this subsection, we provide detailed comparisons between our algorithm and previous
results on independent learning (both experiments and theories). We also performed a
simulation study to showcase the superiority of our sequential policy update structure
over naive independent policy gradient updates.
Experiments Some empirical attempts showed independent policy gradient learning
could achieve surprisingly strong performance in MARL, such as MAPPO [Yu et al.,
2021], IPPO [de Witt et al., 2020], and [Papoudakis et al., 2021].
Despite the empirical success, these methods have several drawbacks. IPPO and
MAPPO assume homogeneity (agents share the same action space and policy parameters). Thus, parameter sharing is required. Even though the parameter sharing can be
turned off, they still suffer from no monotonic improvement guarantees, though being
called PPO-based algorithms. Recall that the main virtue of vanilla TRPO [Schulman
et al., 2015] is monotonicity. Also, these methods do not come with any convergence guarantees. The converging problem becomes more severe when parameter-sharing is switched
off. A counterexample in [Kuba et al., 2022, Proposition 1] shows parameter sharing could
lead to an exponentially-worse sub-optimal outcome.
Thanks to the sequential agents‚Äô structure and novel multi-agent mirror-decent analyses, we present the first MARL algorithm that converges at a sub-linear rate. Note that
our results neither rely on the homogeneity of agents nor the value function decomposition
rule.
Theories Several theoretical works have studied convergence guarantees of independent
policy optimization algorithms to a Nash equilibrium (NE) policy in MARL mathematically [Daskalakis et al., 2020, Leonardos et al., 2022, Fox et al., 2022, Ding et al., 2022].
Specifically, Daskalakis et al. [2020] studied competitive RL. And others studied convergence to the NE policy in Markov potential games (an extension of fully-cooperative
games). However, we argue that a NE policy is not necessarily optimal in terms of the
value function.
In contrast to their work, we present the first provable multi-agent policy optimization
algorithm that finds a policy with a near globally optimal value function equipped with a
sub-linear convergence rate.
Simulation To further validate the theoretical and experimental benefits of our algorithm, we conducted a numerical simulation to showcase the superiority of our algorithm

10

with sequential updates structure over naive independent policy gradient updates. We
consider von Neumann‚Äôs ratio game, a simple stochastic game also used by Daskalakis
et al. [2020]. Simulation results show that, unlike our algorithm, the independent learning
method has significant difficulty escaping the stationary point. Moreover, our algorithm
consistently outperforms independent learning in maximizing value function. See Section E
for detailed settings and results.

5

Pessimistic MA-PPO with Linear Function Approximation

In this section, we study the off-policy setting, using samples from a data distribution
¬µ to evaluate QœÄ . Experimentally, since function approximators often cause a positive
bias in value function Thrun and Schwartz [1993], many deep off-policy actor-critic algorithms introduce pessimism to reduce such overestimation [Fujimoto et al., 2018, Laskin
et al., 2020]. We also adopt pessimistic policy evaluation in this setting, aligning with
experimental works.
We focus on the setting where value functions and policies are linearly parameterized. Our results can extend to the general function approximation setting, presented in
Appendix D.
Definition 5.1 (Linear Function Approximation). Let œÜ be a set of feature mappings built
conditionally, the same definition as Section 4. Define the action value function class as
F m = {œÜ> œâ : œâ ‚àà Rd , kœâk2 ‚â§ L, œÜ> œâ ‚àà [0, 1/1‚àíŒ≥ ]}. The policy class is still parameterized
by log-linear: Œ†m = {œÄ ‚àù exp(œÜ> Œ∏) : Œ∏ ‚àà Rd , kŒ∏k2 ‚â§ R} (cf. Section 4).
Remark 5.2. Under the definition, for any m ‚àà N and policy œÄ, there must exist a
parameter œâ ‚àà Rd that satisfies
1:m
Q1:m
) = œÜ(s, a1:m )> œâ
œÄ (s, a

In this section, we fix the initial state at a certain s0 . Thus the expected reward we
aim to maximize is defined as
J(œÄ) , VœÄ (s0 ).
Note that, in single-agent offline RL, only one policy affects the action at a particular
state so that we can gauge the quality of value function estimates using an offline dataset
D consisting of states, actions, rewards, and transitions. Intuitively, when the following
L0 approaches 0, we can say f is a nice approximator for the Q-function [Xie et al., 2021].
L0 =

1
n

X

2
f (s, a) ‚àí r ‚àí Œ≥f (s0 , œÄ)

(s,a,r,s0 )‚àºD

P
where f (s0 , œÄ) is a shorthand for a0 f (s0 , a0 )œÄ(a0 |s0 ) which will be frequently used in this
section.
However, in the multi-agent environment, the complex dependent structure precludes
the application of such an offline dataset. Specifically, for the m-th agent and policy œÄ,
estimating the multi-agent value function Q1:m
demands that all agents not in {1 : m}
œÄ
must follow œÄ (cf. Definition 3.1), which could not be guaranteed by an offline dataset.
Therefore, online interactions are unavoidable in the multi-agent setting we study.
Below we make clarifications for the sample-generating protocol.
We will collect state-action samples from a fixed data distribution ¬µ = ¬µs ¬µa ‚àà ‚àÜ(S √ó
A). In the benign case, a well-covered ¬µ guarantees adequate exploration over the whole
state and action spaces. Assume we have access to a standard RL oracle
11

Definition 5.3 (Sampling Oracle). The oracle can start from s ‚àº ¬µs , take any action
a ‚àà A, and obtain the next state s0 ‚àº P(¬∑|s, a), and reward r(s, a).
Our query oracle aligns with the classic online sampling oracle for MDP [Kakade
and Langford, 2002, Du et al., 2019, Agarwal et al., 2020]. The difference is that we transit
for one step, while the classic online model usually terminates at the end of each episode.
We also note that our oracle is weaker than the generative model [Kearns and Singh,
2002, Kakade, 2003, Sidford et al., 2018, Li et al., 2020] which assumes that agent can
transit to any state, thus greatly weakening the need for explicit exploration. Whereas
our oracle starts from a fixed ¬µs .4
We take advantage of the sampler in the following steps to obtain action value functions
that preserve a small error under the multi-agent Bellman operator (cf. Definition 5.1).
For agent m ‚àà N and œÄ, (1) obtain s ‚àº ¬µs ; (2) obtain a ‚àº ¬µa and a0 ‚àº œÄ m+1:N (¬∑|s); (3)
take (a1:m , a0 ) as the joint action to query the oracle where a1:m represents the {1 : m}
subset of a. The oracle returns (r, s0 ), which are guaranteed to satisfy:
r‚àº

E

R(s, a1:m , aÃÉ),

s0 ‚àº

aÃÉ‚àºœÄ m+1:N

E

P(¬∑|s, a1:m , aÃÉ).

aÃÉ‚àºœÄ m+1:N

0
Repeat these steps for n times. Together this gives dataset Dm = {(si , a1:m
i , ri , si )|i =
1, 2, ¬∑ ¬∑ ¬∑ n}. Define

L1:m (f 0 , f, œÄ) :=

2
1X 0
f (s, a1:m ) ‚àí r ‚àí Œ≥f (s0 , œÄ 1:m )
n m
D

where f ‚àà F m (cf. Definition 5.1) and the summation is taken over n quadruples of
(s, a1:m , r, s0 ).
We will need the following Bellman error to evaluate the quality of f .
E 1:m (f, œÄ) = L1:m (f, f, œÄ) ‚àí min
L1:m (f 0 , f, œÄ).
0
m
f ‚ààF

(5)

1:m ) when the quantity
Intuitively, we consider f as a nice approximation of Q1:m
œÄ (s, a
is small. This formulation also works for general function approximation. See Appendix D
for details.
We shall need a concentrability measure accounting for the distributional mismatch.

Definition 5.4 (Concentrability). The following condition characterizes the distribution
shift from the dœÄ‚àó to the sampling distribution.
C¬µdœÄ‚àó =

f ‚àí TœÄ1:m f 2,dœÄ

‚àó

sup
m‚ààN ,f ‚ààF m ,œÄ‚ààŒ†m

kf ‚àí TœÄ1:m f k2,Dm

.

Recall that k ¬∑ k2,œÅ is the weighted L2 -norm. In the nominator, the sum is taken over
(s, a1:m ) ‚àº dœÄ‚àó . Whereas in the denominator, the sum is taken over (s, a1:m ) from Dm as
an empirical version of ¬µ. The notion serves a similar role as concentrability coefficients in
the literature [Munos, 2003, Agarwal et al., 2020]: it measures the distributional mismatch
between the underlying optimal distribution and the distribution of samples we employ.
Policy Evaluation At the k-th iteration, we have the current policy œÄŒ∏k . We perform
pessimistic policy evaluation via regularization to reduce value bias in evaluating Q1:m
œÄŒ∏ .
k


œâkm ‚Üê
‚àí arg min f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄŒ∏k ) .
œâ

4

In MDPs, such oracle is called ¬µ-reset model [Kakade and Langford, 2002].

12

Here E is the Bellman error defined in (5). We obtain fkm = œÜ> œâkm as the pessimistic estimate for Q1:m
œÄŒ∏k . This update has a closed-form solution under linear function approximation (cf. Definition 5.1). Moreover, under linear function approximation, the minimization
on the right-hand side can be solved computationally efficiently because of its quadratic
dependency on œâ. See details in Appendix C
Policy Improvement When both value functions and policies are linear parameterized
(cf. Definition 5.1), the mirror descent policy update for any (s, a1:m ) ‚àà S √ó Am
m
œÄk+1
(am |s, a1:m‚àí1 ) ‚àù œÄkm (am |s, a1:m‚àí1 ) ¬∑ exp(Œ∑fkm (s, a1:m ))

(6)

could be further simplified to parameter updates in Rd
m
Œ∏k+1
= Œ∏km + Œ∑œâkm .

This observation makes policy improvements in this setting significantly more superficial
than in Section 4. For the k-th iteration and agent m ‚àà N , we only need to add Œ∑œâkm to
the policy parameter Œ∏km to improve policy.
Algorithm With the pessimistic policy evaluation and intuitive policy improvement,
our pessimistic variant of the multi-agent PPO algorithm is presented in Algorithm 2.
Algorithm 2 Pessimistic Multi-Agent PPO with Linear Function Approximation
Input: Regularization coefficient Œª.
Output: Uniformly sample k from 0, 1 ¬∑ ¬∑ ¬∑ K ‚àí 1, return œÄÃÑ = œÄŒ∏k .
1: Initialize Œ∏0m = 0 for every m ‚àà N .
2: for k = 0, 1, . . . , K ‚àí 1 do
3:
for m = 1, 2, ¬∑ ¬∑ ¬∑ , N do
4:
Pessimistic policy
 evaluation:

1:m (f, œÄ ) .
m
)
+
ŒªE
‚àí arg min f (s0 , œÄŒ∏1:m
œâk ‚Üê
Œ∏
k
k
œâ

m = Œ∏ m + Œ∑œâ m .
Policy improvement: Œ∏k+1
k
k
6:
end for
7: end for

5:

Now we are prepared to present the main theorem for this section.
Theorem 5.5. For the output policy œÄÃÑ attained by Algorithm 2 in a fully cooperative

‚àí2/3
q
d log nLR
log |A|
‚àí1
Œ¥
Markov game, set Œ∑ = (1‚àíŒ≥)
. After K iterations,
2K and Œª = (1‚àíŒ≥)
n
w.p. at least 1 ‚àí Œ¥ we have J(œÄ‚àó ) ‚àí J(œÄÃÑ) upper bounded by
s
Ô£∂
Ô£´
r
dœÄ‚àó
nLR
3 d log
N
log |A|
C¬µ
Œ¥ Ô£∏
OÔ£≠
+
2
2
(1 ‚àí Œ≥)
K
(1 ‚àí Œ≥)
n
To interpret this bound, the first term accounts for the optimization error accumulating from mirror descent updates (6). The first term has an (1 ‚àí Œ≥)‚àí2 dependency on
the discount factor, which may not be tight, and we leave it as a future work to improve.
The second term represents the estimation errors accumulated during training. We use
state-action pairs from ¬µ and the sampling oracle for minimizing E 1:m (f, œÄ), thereby ind
troducing distribution mismatch which is expressed by C¬µœÄ‚àó . Note that this single-policy
concentrability is already weaker than traditional concentrability coefficients [Munos, 2003,
Farahmand et al., 2010, Perolat et al., 2015]. Intuitively, a small value of concentrability
13

requires the data distribution ¬µ close to dœÄ‚àó , which is the unknown occupancy distrid
bution of optimal policy. On the other hand, if C¬µœÄ‚àó is large, then the bound becomes
loose. We provide a similar result for general function approximation in the appendix (cf.
Theorem D.7).
There is no explicit dependence on state-space S in the theorem. Hence the online
algorithm proves nice guarantees for function approximation even in the infinite-state
setting.
To prove Theorem 5.5, the quantitative analysis for Bellman-consistent pessimism [Xie
et al., 2021] is useful. We obtain statistical and convergence guarantees by taking advantage of the conditional dependency structure of the cooperative Markov games. See
Appendix C for details.

6

Conclusion

In this work, we present a new multi-agent PPO algorithm that converges to the globally
optimal policy at a sublinear rate. The key to the algorithm is a multi-agent performance
difference lemma which enables sequential local policy updates. As a generalization, we
extend the algorithm to the off-policy setting and present similar convergence guarantees.
To our knowledge, this is the first multi-agent PPO algorithm in cooperative Markov
games that enjoys provable guarantees.

Acknowledgements
JDL acknowledges support of the ARO under MURI Award W911NF-11-1-0304, the Sloan
Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young
Investigator Award, and NSF CAREER Award 2144994.

References
A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and approximation
with policy gradient methods in Markov decision processes. In Conference on Learning
Theory, pages 64‚Äì66. PMLR, 2020.
A. Antos, C. SzepesvaÃÅri, and R. Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. Machine
Learning, 71(1):89‚Äì129, 2008.
S. R. K. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. Reinforcement learning
for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ‚Äô09, page 82‚Äì90, USA,
2009. Association for Computational Linguistics.
N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418‚Äì424, 2018.
S. Cen, Y. Wei, and Y. Chi. Fast policy extragradient methods for competitive games with
entropy regularization. In Advances in Neural Information Processing Systems, pages
27952‚Äì27964. Curran Associates, Inc., 2021.
S. Cen, Y. Chi, S. S. Du, and L. Xiao. Faster last-iterate convergence of policy optimization
in zero-sum Markov games. arXiv preprint arXiv:2210.01050, 2022.
14

J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042‚Äì1051. PMLR, 2019.
C. Daskalakis, D. J. Foster, and N. Golowich. Independent policy gradient methods for
competitive reinforcement learning. Advances in neural information processing systems,
33:5527‚Äì5540, 2020.
C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. Torr, M. Sun, and
S. Whiteson. Is independent learning all you need in the StarCraft multi-agent challenge?
arXiv preprint arXiv:2011.09533, 2020.
D. Ding, C.-Y. Wei, K. Zhang, and M. Jovanovic. Independent policy gradient for
large-scale Markov potential games: Sharper rates, function approximation, and gameagnostic convergence. In International Conference on Machine Learning, pages 5166‚Äì
5220. PMLR, 2022.
S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is a good representation sufficient for
sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International conference on machine
learning, pages 1329‚Äì1338. PMLR, 2016.
J. Fan, Z. Wang, Y. Xie, and Z. Yang. A theoretical analysis of deep Q-learning. In
Learning for Dynamics and Control, pages 486‚Äì489. PMLR, 2020.
A.-m. Farahmand, C. SzepesvaÃÅri, and R. Munos. Error propagation for approximate policy
and value iteration. Advances in Neural Information Processing Systems, 23, 2010.
J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multiagent policy gradients. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018.
R. Fox, S. M. Mcaleer, W. Overman, and I. Panageas. Independent natural policy gradient
always converges in markov potential games. In International Conference on Artificial
Intelligence and Statistics, pages 4414‚Äì4425. PMLR, 2022.
S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pages 1587‚Äì1596.
PMLR, 2018.
K. Gimpel and N. A. Smith. Softmax-margin CRFs: Training log-linear models with
cost functions. In Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages
733‚Äì736, 2010.
X. Guo, S. Singh, R. Lewis, and H. Lee. Deep learning for reward design to improve Monte
Carlo tree search in Atari games. arXiv preprint arXiv:1604.07095, 2016.
N. Heess, D. Silver, and Y. W. Teh. Actor-critic reinforcement learning with energy-based
policies. In European Workshop on Reinforcement Learning, pages 45‚Äì58. PMLR, 2013.
C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points
efficiently. In International conference on machine learning, pages 1724‚Äì1732. PMLR,
2017.

15

Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? In International Conference on Machine Learning, pages 5084‚Äì5096. PMLR, 2021.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning.
In In Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
S. M. Kakade. A natural policy gradient. Advances in neural information processing
systems, 14, 2001.
S. M. Kakade. On the sample complexity of reinforcement learning. University of London,
University College London (United Kingdom), 2003.
M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine
learning, 49(2):209‚Äì232, 2002.
L. Kraemer and B. Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing, 190:82‚Äì94, 2016.
J. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and Y. Yang. Trust region
policy optimisation in multi-agent reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
EcGGFkNTxdJ.
M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning
with augmented data. Advances in neural information processing systems, 33:19884‚Äì
19895, 2020.
K.-H. Lee, I. Fischer, A. Liu, Y. Guo, H. Lee, J. Canny, and S. Guadarrama. Predictive
information accelerates learning in RL. Advances in Neural Information Processing
Systems, 33:11890‚Äì11901, 2020.
S. Leonardos, W. Overman, I. Panageas, and G. Piliouras. Global convergence of multiagent policy gradient in markov potential games. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gfwON7rAm4.
G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Breaking the sample size barrier in modelbased reinforcement learning with a generative model. Advances in neural information
processing systems, 33:12861‚Äì12872, 2020.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and
D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.
M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pages 157‚Äì163. Elsevier, 1994.
B. Liu, Q. Cai, Z. Yang, and Z. Wang. Neural trust region/proximal policy optimization
attains globally optimal policy. Advances in neural information processing systems, 32,
2019.
Y. Liu, A. Swaminathan, A. Agarwal, and E. Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. Advances in neural information processing systems, 33:1264‚Äì1274, 2020.
R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. Advances in neural
information processing systems, 30, 2017.
16

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep
reinforcement learning. nature, 518(7540):529‚Äì533, 2015.
T. Moskovitz, J. Parker-Holder, A. Pacchiano, M. Arbel, and M. Jordan. Tactical optimism and pessimism for deep reinforcement learning. Advances in Neural Information
Processing Systems, 34:12849‚Äì12863, 2021.
R. Munos. Error bounds for approximate policy iteration. In International Conference on
Machine Learning, page 560‚Äì567, 2003.
R. Munos and C. SzepesvaÃÅri. Finite-time bounds for fitted value iteration. Journal of
Machine Learning Research, 9(5), 2008.
Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2003.
G. Papoudakis, F. Christianos, L. SchaÃàfer, and S. V. Albrecht. Benchmarking multi-agent
deep reinforcement learning algorithms in cooperative tasks. In Thirty-fifth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1),
2021.
J. Perolat, B. Scherrer, B. Piot, and O. Pietquin. Approximate dynamic programming for
two-player zero-sum Markov games. In International Conference on Machine Learning,
pages 1321‚Äì1329, 2015.
P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement
learning and imitation learning: A tale of pessimism. Advances in Neural Information
Processing Systems, 34:11702‚Äì11716, 2021.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889‚Äì1897. PMLR,
2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
L. Shani, Y. Efroni, and S. Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized MDPs. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5668‚Äì5675, 2020.
L. S. Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095‚Äì1100, 1953.
A. Sidford, M. Wang, X. Wu, L. F. Yang, and Y. Ye. Near-optimal time and sample
complexities for solving discounted markov decision process with a generative model.
arXiv preprint arXiv:1806.01492, 2018.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of
Go with deep neural networks and tree search. nature, 529(7587):484‚Äì489, 2016.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge.
nature, 550(7676):354‚Äì359, 2017.
17

R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for
reinforcement learning with function approximation. Advances in neural information
processing systems, 12, 1999.
S. Thrun and A. Schwartz. Issues in using function approximation for reinforcement
learning. In Proceedings of the Fourth Connectionist Models Summer School, volume
255, page 263. Hillsdale, NJ, 1993.
Y. Tian, J. Ma, Q. Gong, S. Sengupta, Z. Chen, J. Pinkerton, and L. Zitnick. Elf opengo:
An analysis and open reimplementation of AlphaZero. In International Conference on
Machine Learning, pages 6244‚Äì6253. PMLR, 2019.
M. Uehara and W. Sun. Pessimistic model-based offline RL: Pac bounds and posterior
sampling under partial coverage. arXiv e-prints, pages arXiv‚Äì2107, 2021.
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H.
Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350‚Äì354, 2019.
Y. Wen, Y. Yang, R. Luo, J. Wang, and W. Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. arXiv preprint arXiv:1901.09207, 2019.
T. Xie and N. Jiang. Q* approximation schemes for batch reinforcement learning: A
theoretical comparison. In Conference on Uncertainty in Artificial Intelligence, pages
550‚Äì559. PMLR, 2020.
T. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism
for offline reinforcement learning. Advances in neural information processing systems,
34:6683‚Äì6694, 2021.
Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang. Mean field multi-agent
reinforcement learning. In International conference on machine learning, pages 5571‚Äì
5580. PMLR, 2018.
C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness
of PPO in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
W. Zhan, B. Huang, A. Huang, N. Jiang, and J. Lee. Offline reinforcement learning with
realizability and single-policy concentrability. In Conference on Learning Theory, pages
2730‚Äì2775. PMLR, 2022.
H. Zhang, W. Chen, Z. Huang, M. Li, Y. Yang, W. Zhang, and J. Wang. Bi-level actorcritic for multi-agent coordination. In Proceedings of the AAAI Conference on Artificial
Intelligence, pages 7325‚Äì7332, 2020.
K. Zhang, Z. Yang, and T. BasÃßar. Multi-agent reinforcement learning: A selective overview
of theories and algorithms. Handbook of Reinforcement Learning and Control, pages
321‚Äì384, 2021.
Y. Zhao, Y. Tian, J. Lee, and S. Du. Provably efficient policy optimization for twoplayer zero-sum Markov games. In International Conference on Artificial Intelligence
and Statistics, pages 2736‚Äì2761. PMLR, 2022.

18

A

Sub-problem Solver for Section 4

Algorithm 3 Policy Improvement Solver for MA-PPO
T ‚àí1
Input: MG (N , S, A, P, r, Œ≥), iterations T , stepsize Œ∑, samples {st , a1:m‚àí1
, am
t }t=0 .
t
Output: Policy update Œ∏.
1: Initialize Œ∏0 = 0.
2: for t = 0, 1, . . . , T ‚àí 1 do
m ).
3:
Let (s, a1:m‚àí1 , a) ‚Üê
‚àí (st , a1:m‚àí1
, a
t
t

4:


1:m‚àí1 , am ) .
Œ∏(t+ 12 ) ‚Üê
‚àí Œ∏(t)‚àí2Œ∑œÜ(s, a1:m‚àí1 , a) (Œ∏(t) ‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , am ) ‚àí Œ≤k‚àí1 QÃÇ1:m
(s,
a
œÄk

Œ∏(t + 1) ‚Üê
‚àí Œ†Œò Œ∏(t + 12 )
6: end for
P
7: Calculate average: Œ∏ÃÑ ‚Üê
‚àí T1 Tt=1 Œ∏t .

5:

B

Proofs for Section 4

First, we note that using SGD updates to solve the MSE problem has the following guarantee.
Lemma B.1 (Average policy). For a convex objective function F (Œ∏), suppose the gradient
is bounded by G, and the output Œ∏ÃÑ converges to the best function in the class at
GR
F (Œ∏ÃÑ) ‚àí min F (Œ∏) ‚â§ ‚àö
kŒ∏k‚â§R
T
R
where we set Œ∑ = G‚àö
.
T

Proof. Please refer to Theorem 14.8 [Shalev-Shwartz and Ben-David, 2014].
T ‚àí1
Now we turn to Algorithm 3, in which, we feed samples {st , a1:m‚àí1
, am
t }t=0 from
t
œÉk = ŒΩk œÄŒ∏k into the algorithm (for m ‚àà N ), in order to minimize


2
1:m‚àí1 m
, a ) + (Œ∏km )> œÜ(s, a1:m‚àí1 , am )) .
L(Œ∏m ) = EœÉk (Œ∏m )> œÜ(s, a1:m‚àí1 , am ) ‚àí (Œ≤k‚àí1 QÃÇ1:m
œÄŒ∏ (s, a
k

We have the following theoretical guarantee for the algorithm.
Lemma B.2 (Policy Improvement error). At the k-th outer loop, the output policy Œ∏k+1
from Algorithm 3 satisfies
q
m ) ‚â§ m
L(Œ∏k+1
k
1

‚àí4
where m
).
k = approx + O(T

Proof. For Algorithm 3, we have kœÜk2 ‚â§ 1 and Œò = {kŒ∏k ‚â§ R|Œ∏ ‚àà Rd }.Thereby the
gradient of L(Œ∏) is bounded by


1
.
G=2 R+
(1 ‚àí Œ≥)Œ≤k
From Lemma B.1, we have the following guarantee holds for any outer iteration k < K
‚àö
m
L(Œ∏k+1
) ‚â§= min L(Œ∏) + O(1/ T ),
Œ∏

19

R
. Thus
when we set Œ∑ = G‚àö
T

m
k =

q
1
1
min L(Œ∏) + O(T ‚àí 4 ) = approx + O(T ‚àí 4 ).
Œ∏

The proof is completed.
Lemma B.3 (Multi-Agent Advantage Decomposition). In cooperative Markov games, the
following decomposition holds for any joint policy œÄ, state s, and agents 1 : m,
1:m
A1:m
)=
œÄ (s, a

m
X

AiœÄ (s, a1:i‚àí1 , ai )

i=1

Proof. Please refer to Lemma 1 [Kuba et al., 2022].
Proof for Lemma 4.1
Proof. From the classical performance difference lemma [Kakade and Langford, 2002,
Lemma 6.1] we have
J(œÄ‚àó ) ‚àí J(œÄ) =

1
1:N
1:N
E A (s, a )
1 ‚àí Œ≥ œÉ‚àó œÄ

Decomposing the all-agents advantage function into individual contributions via the multiagent advantage decomposition lemma (cf. Lemma B.3), we have
1
1:N
1:N
E A (s, a )
1 ‚àí Œ≥ œÉ‚àó œÄ
N
1 X
1:m‚àí1 m
=
Es‚àºŒΩ‚àó Ea1:m‚àí1 ‚àºœÄ‚àó1:m‚àí1 Am
, a ), œÄ‚àóm (¬∑|s, a1:m‚àí1 ) .
œÄ (s, a
1‚àíŒ≥

J(œÄ‚àó ) ‚àí J(œÄ) =

m=1

Note that we have

P

aœÄ

m (a|s, a1:m‚àí1 )Am (s, a1:m‚àí1 , a) = 0, then
œÄ

N

1 X
1:m‚àí1
Es‚àºŒΩ‚àó Ea1:m‚àí1 ‚àºœÄ‚àó1:m‚àí1 Am
, ¬∑), œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí œÄ m (¬∑|s, a1:m‚àí1 )
œÄ (s, a
1‚àíŒ≥
=

m=1
N
X

1
1‚àíŒ≥

1:m‚àí1
Es‚àºŒΩ‚àó Ea1:m‚àí1 ‚àºœÄ‚àó1:m‚àí1 Q1:m
, ¬∑), œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí œÄ m (¬∑|s, a1:m‚àí1 )
œÄ (s, a

m=1

1:m‚àí1 , am ) = Q1:m (s, a1:m‚àí1 , am ) ‚àí Q1:m‚àí1 (s, a1:m‚àí1 )
where the last line is because Am
œÄ (s, a
œÄ
œÄ
1:m‚àí1
1:m‚àí1
and QœÄ
(s, a
) can be omitted because it does not change with am .

Proof for Proposition 4.2.
m (¬∑|s, a1:m‚àí1 ) is obtained via
Proof. For any (s, a1:m‚àí1 ) ‚àà S √ó Am‚àí1 , policy œÄÃÇk+1
h
i
1:m
1:m‚àí1
m
1:m‚àí1
m
1:m‚àí1
1:m‚àí1
max
E
h
QÃÇ
(s,
a
,
¬∑),
œÄ
(¬∑|s,
a
)i
‚àí
Œ≤
KL
œÄ
(¬∑|s,
a
)kœÄ
(¬∑|s,
a
)
ŒΩ
k
Œ∏
œÄ
k
k
k
œÄm
X
s.t.
œÄ m (am |s, a1:m‚àí1 ) = 1
am ‚ààA

Z

Adding constraint as a Lagrangian multiplier, we have
h

i
1:m‚àí1
m
1:m‚àí1
m
1:m‚àí1
1:m‚àí1
m
hQÃÇ1:m
(s,
a
,
¬∑),
œÄ
(¬∑|s,
a
)i
‚àí
Œ≤
KL
œÄ
(¬∑|s,
a
)kœÄ
(¬∑|s,
a
)
œÉk dsda1:m‚àí1
k
Œ∏k
œÄk

S√óAm‚àí1

20

!

Z

X

+
S√óAm‚àí1

m

m

œÄ (a |s, a

1:m‚àí1

) ‚àí 1 dsda1:m‚àí1

am ‚ààA

Note that œÄŒ∏km ‚àù exp{Œ∏km > œÜ}, he optimality condition gives
m
1:m‚àí1
œÄÃÇk+1
(¬∑|s, a1:m‚àí1 ) ‚àù exp{Œ≤k‚àí1 QÃÇ1:m
, ¬∑) + Œ∏km > œÜ(s, a1:m‚àí1 , a)}.
œÄk (s, a

Lemma B.4. Suppose for any agent m ‚àà N , policy improvement error and policy evaluation errors satisfy
2

1:m‚àí1 m
2
1:m‚àí1 m
m>
m >
œÜ(s,
a
,
a
))
‚â§ (m
(s,
a
,
a
)
+
Œ∏
œÜ(s, a1:m‚àí1 , am ) ‚àí (Œ≤k‚àí1 QÃÇ1:m
EœÉk Œ∏k+1
œÄŒ∏
k ) ,
k
k

(7)


1:m‚àí1 m
1:m‚àí1 m
,a )
, a ) ‚àí Q1:m
EœÉk QÃÇ1:m
œÄŒ∏ (s, a
œÄŒ∏ (s, a
k

k

2

‚â§ (Œækm )2 .

(8)

m > œÜ ‚àí Œ∏ m > œÜ we have
Considering the L‚àû -norm of Œ∏k+1
k
1:m‚àí1
m
, ¬∑)
‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , ¬∑) ‚àí Œ≤k‚àí1 QÃÇ1:m
Es‚àºŒΩ‚àó ,a1:m‚àí1 ‚àºœÄ‚àó (Œ∏k+1
œÄŒ∏ (s, a
k

‚àû

‚â§

Œ¥km
2

m
where Œ¥km = 2œÜm‚àí1
k .
k
Proof. The proof is straightforward,
1:m‚àí1
m
, ¬∑)
Es‚àºŒΩ‚àó ,a1:m‚àí1 ‚àºœÄ‚àó1:m‚àí1 (Œ∏k+1
‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , ¬∑) ‚àí Œ≤k‚àí1 QÃÇ1:m
œÄŒ∏ (s, a
k

‚â§

1:m‚àí1 m
m
,a )
‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , am ) ‚àí Œ≤k‚àí1 QÃÇ1:m
(Œ∏k+1
œÄŒ∏k (s, a

E

s‚àºŒΩ‚àó
a1:m‚àí1 ‚àºœÄ‚àó

‚àû

.

We shift from œÉ‚àó to œÉk and introduce concentrability coefficients to measure distributional
shift
E
s‚àºŒΩ

k

m
1:m‚àí1 m
(Œ∏k+1
‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , am ) ‚àí Œ≤k‚àí1 QÃÇ1:m
,a ) ¬∑
œÄŒ∏ (s, a
k

a1:m‚àí1 ‚àºœÄŒ∏k





m
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇœÄŒ∏k
‚â§ EœÉk (Œ∏k+1

2 1/2

Ô£Æ
¬∑ Ô£∞EœÉk

ŒΩ‚àó œÄ‚àó1:m‚àí1
ŒΩk œÄŒ∏1:m‚àí1
k

Ô£π
2 1/2
d(ŒΩ‚àó œÄ‚àó1:m‚àí1 ) Ô£ª
d(ŒΩk œÄŒ∏1:m‚àí1
)
k

m‚àí1
= m
k œÜk

where we use Cauchy-Schwartz inequality in the second line.
The proof is completed.
m as the ideal update policy based on QÃÇ1:m . Correspondingly,
Recall that we define œÄÃÇk+1
œÄŒ∏k
1:m
we define the ideal update based on the exact value function QœÄŒ∏ as
k

m
œÄk+1
‚Üê
‚àí arg max F (œÄ m )
œÄm
h

i
1:m‚àí1
F (œÄ m ) = EœÉk hQ1:m
, ¬∑), œÄ m (¬∑|s, a1:m‚àí1 )i ‚àí Œ≤k KL œÄ m (¬∑|s, a1:m‚àí1 )kœÄŒ∏km (¬∑|s, a1:m‚àí1 ) .
œÄŒ∏ (s, a
k

Under log-linear parametrization: œÄŒ∏km ‚àù exp{œÜ> Œ∏km }, analogously we have
n
o
m
1:m‚àí1
>
1:m‚àí1
m
œÄk+1
(¬∑|s, a1:m‚àí1 ) ‚àù exp Œ≤k‚àí1 Q1:m
(s,
a
,
¬∑)
+
œÜ
(s,
a
,
¬∑)Œ∏
.
œÄk
k
21

Lemma B.5 (Error Propagation). Suppose for any agent m ‚àà N and (s, a1:m‚àí1 ) ‚àà
S √ó Am‚àí1 , policy improvement and policy evaluation errors satisfy,

2
‚àí1 1:m
2
m
m >
‚â§ (m
EœÉk (Œ∏k+1 ‚àí Œ∏k ) œÜ ‚àí Œ≤k QÃÇœÄŒ∏
k ) ,
k
2

1:m
‚àí
Q
‚â§ (Œækm )2
EœÉk QÃÇ1:m
œÄŒ∏
œÄŒ∏
k

k

where we omit (s, a1:m‚àí1 , am ) for simplicity.
Compare the statistical error, we have
*
+
1:m‚àí1 )
m (¬∑|s, a
œÄŒ∏k+1
m
1:m‚àí1
1:m‚àí1
Es‚àºŒΩ‚àó ,a‚àºœÄ‚àó log m
, œÄ (¬∑|s, a
) ‚àí œÄŒ∏km (¬∑|s, a
) ‚â§ ‚àÜm
k .
œÄk+1 (¬∑|s, a1:m‚àí1 ) ‚àó
where ‚àÜm
k =

‚àö



Œækm
m‚àí1
2(œÜm
) ¬∑ m
k + œÜk
k + Œ≤k

Lemma B.5 presents the quantitative differences between the actual parameterized
m based on the exact value function Q1:m .
œÄ
based on QÃÇ1:m and the ideal policy œÄk+1
m
Œ∏k+1

Proof. First, from definition for any m ‚àà N and s, a1:m‚àí1 ‚àà S √ó Am‚àí1 we have
o
n
m
(¬∑|s, a1:m‚àí1 ) = exp Œ≤k ‚àí1 QœÄŒ∏k + Œ∏km > œÜ(s, a1:m‚àí1 , ¬∑) /W (s, a1:m‚àí1 ),
œÄk+1
n
o
1:m‚àí1
m >
m (¬∑|s, a
) = exp Œ∏k+1
œÜ(s, a1:m‚àí1 , ¬∑) /M (s, a1:m‚àí1 ).
œÄŒ∏k+1

(9)
(10)

Substituting this into the expression, we have
E
D
m
m
‚àí log œÄk+1
, œÄ‚àóm ‚àí œÄŒ∏km
LHS = log œÄŒ∏k+1
E
D
m >
œÜ ‚àí (Œ≤k‚àí1 QœÄŒ∏k + Œ∏km > œÜ), œÄ‚àóm ‚àí œÄŒ∏km
= Œ∏k+1
E D
D
E
m >
œÜ ‚àí (Œ≤k‚àí1 QÃÇœÄŒ∏k + Œ∏km > œÜ), œÄ‚àóm ‚àí œÄŒ∏km + Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k , œÄ‚àóm ‚àí œÄŒ∏km
= Œ∏k+1
|
{z
} |
{z
}
(a)

(b)

D
E

P  m
m ‚àí œÄ m = log W
m (¬∑)
In the second line, we use the fact that log W
,
œÄ
œÄ
(¬∑)
‚àí
œÄ
=
m
Œ∏
Œ∏
‚àó
‚àó
a
M
M
k
k
0.
Bounding the two terms separately, we have
‚Ä¢ For (a), taking the expectation over S √ó Am‚àí1 we have
D
E
m >
Es‚àºŒΩ‚àó Ea1:m‚àí1 ‚àºœÄ‚àó Œ∏k+1
œÜ ‚àí (Œ≤k‚àí1 QÃÇœÄŒ∏k + Œ∏km > œÜ), œÄ‚àóm ‚àí œÄŒ∏km
Z

h
i 

m
(Œ∏k+1
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇœÄŒ∏k ¬∑ œÄ‚àóm ‚àí œÄŒ∏km dam ¬∑ œÄ‚àó1:m‚àí1 d(a1:m‚àí1 ) ¬∑ ŒΩ‚àó (s)ds

=
S√óAm‚àí1 √óA

Here the expectation is taken w.r.t. œÉ‚àó, we change it to be expectation over œÉk by
introducing concentrability coefficients
Z

h
i
m
(Œ∏k+1
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇœÄŒ∏k ¬∑

S√óAm‚àí1 √óA

Z
=

œÄ‚àó1:m œÄ‚àó1:m‚àí1 œÄŒ∏km
‚àí
œÄŒ∏1:m
œÄŒ∏1:m
k
k

h
i ŒΩ (s)
‚àó
m
(Œ∏k+1
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇœÄŒ∏k ¬∑
ŒΩk (s)

S√óAm‚àí1 √óA

22

!
œÄŒ∏1:m
(a1:m |s)d(a1:m ) ¬∑ ŒΩ‚àó (s)ds
k

œÄ‚àó1:m œÄ‚àó1:m‚àí1 œÄŒ∏km
‚àí
œÄŒ∏1:m
œÄŒ∏1:m
k
k

!
dœÉk

(i)





m
‚â§ EœÉk (Œ∏k+1
‚àí Œ∏km )> œÜ ‚àí Œ≤k‚àí1 QÃÇœÄŒ∏k

2 1/2

Ô£Æ
¬∑ Ô£∞EœÉk

Ô£π
2 1/2
d(ŒΩ‚àó œÄ‚àó1:m ) d(ŒΩ‚àó œÄ‚àó1:m‚àí1 ) Ô£ª
‚àí
) d(ŒΩk œÄŒ∏1:m‚àí1 )
d(ŒΩk œÄŒ∏1:m
k
k

(ii) ‚àö

‚â§

m‚àí1
m
2m
)
k (œÜk + œÜk

(i) : This is because of the
qRCauchy-Schwartzqinequality.
‚àö
R
R
(ii) : This is because:
|f ‚àí g|2 dœÉ ‚â§
2|f |2 dœÉ + 2|g|2 dœÉ ‚â§ 2(kf k2,œÉ +
kgk2,œÉ ).
‚Ä¢ For (b), taking the expectation over S √ó Am‚àí1 we have
D
E
Es‚àºŒΩ‚àó Ea1:m‚àí1 ‚àºœÄ‚àó Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k , œÄ‚àóm ‚àí œÄŒ∏km
Z

i 

h
Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k ¬∑ œÄ‚àóm ‚àí œÄŒ∏km dam ¬∑ œÄ‚àó1:m‚àí1 d(a1:m‚àí1 ) ¬∑ ŒΩ‚àó (s)ds

=
S√óAm‚àí1 √óA

Analogously we replace the expectation over œÉ‚àó with expectation over œÉk
Z

i ŒΩ (s)
h
‚àó
Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k ¬∑
ŒΩk (s)

œÄ‚àó1:m œÄ‚àó1:m‚àí1 œÄŒ∏km
‚àí
œÄŒ∏1:m
œÄŒ∏1:m
k
k

h
i ŒΩ (s)
‚àó
Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k ¬∑
ŒΩk (s)

œÄ‚àó1:m œÄ‚àó1:m‚àí1
‚àí 1:m‚àí1
œÄŒ∏1:m
œÄŒ∏
k

S√óAm‚àí1 √óA

Z
=
S√óAm‚àí1 √óA
(i)





‚â§ EœÉk Œ≤k‚àí1 QÃÇœÄŒ∏k ‚àí Œ≤k‚àí1 QœÄŒ∏k

(ii)

‚â§

‚àö

2 1/2

Ô£Æ
¬∑ Ô£∞EœÉk

!
(a1:m |s)d(a1:m ) ¬∑ ŒΩk (s)ds
œÄŒ∏1:m
k

!
dœÉk

k

Ô£π
2 1/2
d(ŒΩ‚àó œÄ‚àó1:m ) d(ŒΩ‚àó œÄ‚àó1:m‚àí1 ) Ô£ª
‚àí
d(ŒΩk œÄŒ∏1:m
) d(ŒΩk œÄŒ∏1:m‚àí1 )
k
k

2 m m
Œæk (œÜk + œÜm‚àí1
)
k

Œ≤k

(i) : This is because of the
qRCauchy-Schwartzqinequality.
‚àö
R
R
(ii) : This is because:
|f ‚àí g|2 dœÉ ‚â§
2|f |2 dœÉ + 2|g|2 dœÉ ‚â§ 2(kf k2,œÉ +
kgk2,œÉ ).
Combining the bounds, we conclude the proof for Lemma B.5


‚àö m
Œækm
m‚àí1
m
m
.
‚àÜk = 2(œÜk + œÜk ) ¬∑ k +
Œ≤k

Below we introduce a lemma that is crucial in our multi-agent PPO analysis. The original version is widely found and proven to be useful for mirror descent analysis [Nesterov,
2003].
m , the real updated
Lemma B.6 (One-Step Descent). For the ideal updated policy œÄk+1
1:m‚àí1
m
policy œÄŒ∏k+1
and current policy œÄŒ∏km , we have that for any (s, a
) ‚àà S √ó Am‚àí1 ,





1:m‚àí1
m (¬∑|s, a
KL œÄ‚àóm (¬∑|s, a1:m‚àí1 )kœÄŒ∏k+1
) ‚àí KL œÄ‚àóm (¬∑|s, a1:m‚àí1 )kœÄŒ∏km (¬∑|s, a1:m‚àí1 )

23

*
‚â§

log

1:m‚àí1 )
m (¬∑|s, a
œÄŒ∏k+1
m (¬∑|s, a1:m‚àí1 )
œÄk+1

+
1:m‚àí1

, œÄŒ∏km (¬∑|s, a

) ‚àí œÄ‚àóm (¬∑|s, a1:m‚àí1 )

E 1
2
1 D m
1:m‚àí1
m (¬∑|s, a
œÄŒ∏k+1
) ‚àí œÄŒ∏km (¬∑|s, a1:m‚àí1 )
AœÄŒ∏ (s, a1:m‚àí1 , ¬∑), œÄŒ∏km (¬∑|s, a1:m‚àí1 ) ‚àí œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí
k
Œ≤
2
1
Dk
E
m
1:m‚àí1
m (¬∑|s, a
‚àí (Œ∏k+1
‚àí Œ∏km )> œÜ(s, a1:m‚àí1 , ¬∑), œÄŒ∏km (¬∑|s, a1:m‚àí1 ) ‚àí œÄŒ∏k+1
)
+

Proof. In the proof, we simply omit (s, a1:m‚àí1 ) when making no abuse of notation.
Using the definition, we have




m
KL œÄ‚àóm kœÄŒ∏km ‚àí KL œÄ‚àóm kœÄŒ∏k+1
+
*
m
œÄŒ∏k+1
, œÄ‚àóm
= log
œÄŒ∏km
*
+


m
œÄŒ∏k+1
m
m kœÄŒ∏ m
= log
, œÄ‚àóm ‚àí œÄŒ∏k+1
+ KL œÄŒ∏k+1
(11)
k
œÄŒ∏km
m (9) and œÄ m (10), we have the following two equations
Recall the definitions of œÄk+1
Œ∏k+1

*
log
D

m
œÄŒ∏k+1

œÄŒ∏km

+
m
, œÄŒ∏km ‚àí œÄŒ∏k+1

Œ≤k‚àí1 QœÄŒ∏k , œÄ‚àóm ‚àí œÄŒ∏km

E

E
D
m
m
,
‚àí Œ∏km )> œÜ, œÄŒ∏km ‚àí œÄŒ∏k+1
= (Œ∏k+1
*
=

œÄm
log k+1 , œÄ‚àóm ‚àí œÄŒ∏km
œÄŒ∏km

(12)

+
.

(13)

Plugging these results (12), (13) into the RHS of (11) we have




m
KL œÄ‚àóm kœÄŒ∏km ‚àí KL œÄ‚àóm kœÄŒ∏k+1
+
*
E


E D
D
m
œÄŒ∏k+1
m >
m
m
m
m
m
m
m
m
)
œÜ,
œÄ
‚àí
œÄ
+
KL
œÄ
kœÄ
‚àí
Œ∏
,
œÄ
‚àí
œÄ
+
(Œ∏
= log m , œÄ‚àó ‚àí œÄŒ∏km + Œ≤k‚àí1 Q1:m
Œ∏k
Œ∏k+1
Œ∏k+1
Œ∏k
Œ∏k
œÄŒ∏k
‚àó
k
k+1
œÄk+1
*
+
E 1
E D
D
m
œÄŒ∏k+1
2
m
m
m >
m
m
m
‚â• log m , œÄ‚àó ‚àí œÄŒ∏km + Œ≤k‚àí1 Am
+
œÄŒ∏k+1
‚àí œÄŒ∏km
œÄŒ∏k , œÄ‚àó ‚àí œÄŒ∏km + (Œ∏k+1 ‚àí Œ∏k ) œÜ, œÄŒ∏km ‚àí œÄŒ∏k+1
œÄk+1
2
1
In the last line: (1) From the Definition 3.1 of multi-agent advantage functions, we have
m
m
hQ1:m
œÄŒ∏ ‚àí AœÄŒ∏ , œÄ‚àó ‚àí œÄŒ∏km i = 0,
k

k

and (2) Pinsker‚Äôs inequality in information theory gives a lower bound of the KL-divergence.

 1
2
m
m kœÄŒ∏ m
œÄŒ∏k+1
‚àí œÄŒ∏km ,
KL œÄŒ∏k+1
‚â•
k
2
1
plugging these into the expression, which concludes the proof.
With these results, we are ready to present the proofs for the main theorem.
Proofs for Theorem 4.5.
Proof. With Lemma B.6, take expectation with respect to s ‚àº ŒΩ‚àó and a ‚àº œÄ‚àó , we have
D
E
1
m
1:m‚àí1
, ¬∑), œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí œÄŒ∏km (¬∑|s, a1:m‚àí1 )
E AœÄŒ∏k (s, a
Œ≤ k œÉ‚àó
24

h



i
1:m‚àí1
m (¬∑|s, a
‚â§ E KL œÄ‚àóm (¬∑|s, a1:m‚àí1 )kœÄŒ∏km (¬∑|s, a1:m‚àí1 ) ‚àí KL œÄ‚àóm (¬∑|s, a1:m‚àí1 )kœÄŒ∏k+1
)
œÉ‚àó
*
+
1:m‚àí1 )
m (¬∑|s, a
œÄŒ∏k+1
‚àí E log m
, œÄ m (¬∑|s, a1:m‚àí1 ) ‚àí œÄŒ∏km (¬∑|s, a1:m‚àí1 )
œÄk+1 (¬∑|s, a1:m‚àí1 ) ‚àó
œÉ‚àó
2
1
1:m‚àí1
m (¬∑|s, a
) ‚àí œÄŒ∏km (¬∑|s, a1:m‚àí1 )
E œÄŒ∏k+1
2 œÉ‚àó
1
E
D
>
>
1:m‚àí1
m
m (¬∑|s, a
) .
œÜ(s, a1:m‚àí1 , ¬∑) ‚àí Œ∏km œÜ(s, a1:m‚àí1 , ¬∑), œÄŒ∏km (¬∑|s, a1:m‚àí1 ) ‚àí œÄŒ∏k+1
‚àí E Œ∏k+1

‚àí

œÉ‚àó

Analogous to the previous section, we omit (s, a1:m‚àí1 ) below for simplicity when making no abuse of notation. We arrange the above inequality by plugging in Lemma B.5
D
E
1
m
m
E AœÄŒ∏k , œÄ‚àó ‚àí œÄŒ∏km
Œ≤k œÉ‚àó
h



i
m
‚â§ E KL œÄ‚àóm kœÄŒ∏km ‚àí KL œÄ‚àóm kœÄŒ∏k+1
œÉ‚àó
"*
#
+
D
E
m
œÄŒ∏k+1
2
1
m
m
>
m
m
m
‚àíE
log m , œÄ‚àó ‚àí œÄŒ∏km +
œÄŒ∏k+1
‚àí œÄŒ∏km + (Œ∏k+1 ‚àí Œ∏k ) œÜ, œÄŒ∏km ‚àí œÄŒ∏k+1
œÄk+1
2
œÉ‚àó
1

E
h



i
D
2
1
m
m
m >
m
m
m
m
m
m
m
m
œÄŒ∏k+1 ‚àí œÄŒ∏k
‚â§ E KL œÄ‚àó kœÄŒ∏k ‚àí KL œÄ‚àó kœÄŒ∏k+1 +‚àÜk ‚àí E
+ (Œ∏k+1 ‚àí Œ∏k ) œÜ, œÄŒ∏k ‚àí œÄŒ∏k+1
œÉ‚àó 2
œÉ‚àó
1
|
{z
}
|
{z
}
(i)

(ii)

(14)
Take summation over m = 1, ¬∑ ¬∑ ¬∑ , N and k = 0, ¬∑ ¬∑ ¬∑ , K ‚àí 1 for both sides, note that
LHS This equals (1 ‚àí Œ≥)
Lemma 4.1).

PK‚àí1 1

k=0 Œ≤k (J(œÄ‚àó ) ‚àí J(œÄŒ∏k )) by performance difference lemma (cf.

RHS (i) : After taking summation over k and m,
 this term is upper bounded by N log A
because for any m ‚àà N , we have E KL œÄ‚àóm kœÄŒ∏m0 ‚â§ log |A| since the initial policy œÄŒ∏0 is
œÉ‚àó

uniformly distributed over action spaces.
(ii): Using HoÃàlder inequality, we have
E
D
m
m
m
‚â§ E (Œ∏k+1
‚àí Œ∏km )> œÜ
‚àí Œ∏km )> œÜ, œÄŒ∏km ‚àí œÄŒ∏k+1
‚àí E (Œ∏k+1
œÉ‚àó

œÉ‚àó

‚àû

m
¬∑ œÄŒ∏km ‚àí œÄŒ∏k+1

1

Using the triangle inequality, we can upper bound it by
‚àí1
m
m >
E (Œ∏k+1 ‚àí Œ∏k ) œÜ ‚àí Œ≤k QÃÇ

œÉ‚àó

‚â§ Œ¥km + E Œ≤k‚àí1 QÃÇ
œÉ‚àó

‚àû

‚àû

m
¬∑ œÄŒ∏km ‚àí œÄŒ∏k+1

m
¬∑ œÄŒ∏km ‚àí œÄŒ∏k+1

1

1

+ E Œ≤k‚àí1 QÃÇ
œÉ‚àó

‚àû

m
¬∑ œÄŒ∏km ‚àí œÄŒ∏k+1

,

m
m
= 2.
where we plug in Lemma B.4 and: œÄŒ∏km ‚àí œÄŒ∏k+1
‚â§ œÄŒ∏km + œÄŒ∏k+1
1
1
1
We have


2
1
m
m
‚àíE
œÄŒ∏k+1
‚àí œÄŒ∏km
+ E Œ≤k‚àí1 QÃÇ
¬∑ œÄŒ∏km ‚àí œÄŒ∏k+1
œÉ‚àó
œÉ‚àó 2
1
‚àû
1
2
1 ‚àí1
1
1
‚â§E
Œ≤k QÃÇ
, because ‚àÄx, y it holds: ‚àí x2 + yx ‚â§ y 2 .
2
2
œÉ‚àó 2
‚àû
2
B
‚â§
2Œ≤k2

25

1

Finally, by combining these results, we rearrange (14) and obtain
(1 ‚àí Œ≥)

K‚àí1
X
k=0

N

K‚àí1

N

K‚àí1

X X
X X B2
1
m
.
(J(œÄ‚àó ) ‚àí J(œÄŒ∏k )) ‚â§ N log |A| +
(‚àÜm
+
Œ¥
)
+
k
k
Œ≤k
2Œ≤k2
m=1 k=0

m=1 k=0

‚àö

Setting the penalty parameter Œ≤k = Œ≤ K and noting that œÄÃÑ is uniformly sampled from
œÄŒ∏k , k = 1, 2 ¬∑ ¬∑ ¬∑ K ‚àí 1, we have
PK‚àí1 m
P
m
N Œ≤ 2 log |A| + N B 2 /2 + Œ≤ 2 N
k=0 (‚àÜk + Œ¥k )
m=1
‚àö
J(œÄ‚àó ) ‚àí J(œÄÃÑ) ‚â§
.
(1 ‚àí Œ≥)Œ≤ K
Considering policy improvement/evaluation errors, the optimal choice for Œ≤ is
s
N B 2 /2
Œ≤=
PN PK‚àí1 m
N log |A| + m=1 k=0 (‚àÜk + Œ¥km )
then

Ô£´ ‚àö s
Ô£∂
PN PK‚àí1 m
m
B N N log |A| + m=1 k=0 (‚àÜk + Œ¥k ) Ô£∏
J(œÄ‚àó ) ‚àí J(œÄÃÑ) ‚â§ O Ô£≠
.
1‚àíŒ≥
K

The proof is completed.

C

Proofs for Section 5

C.1

Computational efficiency

Observe the pessimistic evaluation

œâkm ‚Üê
‚àí arg min f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄŒ∏k ) .
œâ

Under linear function approximation, f (s0 , œÄ 1:m ) is instantiated as œÜ(s0 , œÄ 1:m )> œâ, then
2
1 X
1:m 0
1:m > 0
0
1:m >
œÜ(s, a ) œâ ‚àí r ‚àí Œ≥œÜ(s , œÄ ) œâ ,
L (f , f, œÄ) =
n m
D

thus we have E 1:m (f, œÄ) defined as
X
X
(œÜ(s, a1:m )> œâ ‚àí r ‚àí Œ≥œÜ(s0 , œÄ 1:m )> œâ)2 ‚àí min
(œÜ(s, a1:m )> œâ 0 ‚àí r ‚àí Œ≥œÜ(s0 , œÄ 1:m )> œâ)2 ,
0
œâ

Dm

Dm

where summation is taken over samples (s, a1:m , r, s0 ) from Dm .
Therefore, the Bellman error has a quadratic-form dependency on value function parameter œâ, allowing the application of many efficient numerical solvers.

C.2

Proofs

The linear function approximation directly implies the Realizability and Completeness
conditions: For any m ‚àà N , œÄ ‚àà Œ†m ,
inf

sup

f ‚ààF m admissable ŒΩ

kf ‚àí TœÄ1:m f k22,ŒΩ = 0,

and
sup

inf kf 0 ‚àí TœÄ1:m f k22,¬µ = 0.

m
f ‚ààF m f ‚ààF

26

These conditions hold because we assume a linear structure for state-action value functions:
m (cf. Definition 5.1).
Q1:m
œÄ ‚ààF
First we examine concentration analysis for linear function approximation [Xie et al.,
2021].
Lemma C.1. For any m ‚àà N , œÄ ‚àà Œ†m , with probability at least 1 ‚àí Œ¥ it holds
!
d log nLR
1:m
œÄ
Œ¥
E (Q , œÄ) ‚â§ O
= Œµr .
(1 ‚àí Œ≥)2 n
Lemma C.2. For any m ‚àà N , œÄ ‚àà Œ†m , f ‚àà F m (cf. Definition 5.1) , if E 1:m (f, œÄ) ‚â§ Œµ,
with probability at least 1 ‚àí Œ¥ it holds
Ô£´s
Ô£∂
nLR
‚àö
d log Œ¥
Ô£∏ + Œµ.
f ‚àí TœÄ1:m f 2,Dm ‚â§ O Ô£≠
(1 ‚àí Œ≥)2 n
For simplicity, below, we shall define
R(s, a1:m ) = EaÃÉ‚àºœÄÃÉk r(s, a1:m , aÃÉ).
In the following lemma, we show that at every iteration k of Algorithm 2, there exists
a Markov game Mk whose multi-agent value function is exactly fkm , m ‚àà N . Moreover,
the transition dynamics of Mk are the same as those of the original M. We have the
following theoretical guarantees to control the differences between the reward of M and
rewards of Mk .
Lemma C.3. At each iteration k, there exists a Markov game Mk that has the same
dynamics as original M. Let the reward function of Mk be Rk , then
2

Rk1:m (s, a1:m ) ‚àí R(s, a1:m ) 2,Dm ‚â§ Œµr .
Proof. Set Rk = (I ‚àí Œ≥P)fk . It directly implies that
(s.a1:m )
fkm = TœÄ1:m
k ,Mk
= Rk1:m (s, a1:m ) + Œ≥Efkm (s0 , a0 ).
Therefore
2

Rk1:m (s, a1:m ) ‚àí R(s, a1:m ) 2,Dm = kfkm ‚àí TœÄ1:m
fkm k22,Dm ‚â§ Œµr
k

Lemma C.4. For any conditional policy œÄ : S √ó Am‚àí1 ‚Üí
‚àí ‚àÜ(A), and s ‚àà S, a1:m‚àí1 ‚àà
m‚àí1
A
,
K
X

‚â•

k=1
K
X

œÄk+1 (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑) ‚àí `sa (œÄ1 (¬∑|s, a1:m‚àí1 ))
œÄ(¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑) ‚àí `sa (œÄ)

k=1

where we assume `sa (œÄ) = Œ∑1

P

a‚ààA œÄ(a|s, a

1:m‚àí1 ) ¬∑ log œÄ(a|s, a1:m‚àí1 )

m = Œ∏ m + Œ∑w m .
Proof. Proofs are straightforward by noticing the fact that Œ∏k+1
k
k

27

Lemma C.5. For any conditional policy œÄ : S √ó Am‚àí1 ‚Üí
‚àí ‚àÜ(A), and s ‚àà S, a1:m‚àí1 ‚àà
m‚àí1
A
,
K
X

‚â§

œÄ(¬∑|s, a1:m‚àí1 ) ‚àí œÄk (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑)

k=1
K
X

œÄk+1 (¬∑|s, a1:m‚àí1 ) ‚àí œÄk (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑) ‚àí `sa (œÄ1 (¬∑|s, a1:m‚àí1 ))

k=1

Proof. The results could be obtained by applying Lemma C.4.
Lemma C.6. For any conditional
policy œÄ : S √ó Am‚àí1 ‚Üí
‚àí ‚àÜ(A), and s ‚àà S, a1:m‚àí1 ‚àà
q
Am‚àí1 , if set stepsize Œ∑ =
K
X

log |A|
2K

œÄ(¬∑|s, a1:m‚àí1 ) ‚àí œÄk (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑) ‚â§ 2

p
2 log |A|K

k=1

Pk
1:m‚àí1 ), f 0 (s, a1:m‚àí1 , ¬∑)i ‚àí ` (œÄ). Let B m (¬∑k¬∑) and
Proof. Define Lm
sa
Lsa,k
k
k0 =1 hœÄ(¬∑|s, a
sa,k =
and
`
.
Using
the
property of
B`sa (¬∑k¬∑) be the Bregman divergences w.r.t. losses Lm
sa
sa,k
divergence we have
m
m
m
m
Lm
(œÄkm (¬∑|s, a1:m‚àí1 )kœÄk+1
(¬∑|s, a1:m‚àí1 ))
sa,k (œÄk ) ‚â§ Lsa,k (œÄk+1 ) + BLm
sa,k
1:m‚àí1
m
m
m
(¬∑|s, a1:m‚àí1 )).
)kœÄk+1
= Lm
sa,k (œÄk+1 ) ‚àí B`sa (œÄk (¬∑|s, a

Reordering it we have
m
m
m
m
(¬∑|s, a1:m‚àí1 )) ‚â§ Lm
B`sa (œÄkm (¬∑|s, a1:m‚àí1 )kœÄk+1
sa,k (œÄk+1 ) ‚àí Lsa,k (œÄk ).

RHS of the expression above is not greater than
m
(¬∑|s, a1:m‚àí1 ) ‚àí œÄkm (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑) .
œÄk+1

Then
m
œÄk+1
(¬∑|s, a1:m‚àí1 ) ‚àí œÄkm (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑)
q
m (¬∑|s, a1:m‚àí1 )) ¬∑ f m (s, a1:m‚àí1 , ¬∑)
‚â§ 2Œ∑B`sa (œÄkm (¬∑|s, a1:m‚àí1 )kœÄk+1
k
‚àû
q
p
1
m (¬∑|s, a1:m‚àí1 ) ‚àí œÄ m (¬∑|s, a1:m‚àí1 ), f m (s, a1:m‚àí1 , ¬∑)i
‚â§ 2Œ∑ hœÄk+1
.
k
k
1‚àíŒ≥
2Œ∑
m (¬∑|s, a1:m‚àí1 ) ‚àí œÄ m (¬∑|s, a1:m‚àí1 ), f m (s, a1:m‚àí1 , ¬∑) ‚â§
Thus we have œÄk+1
. Substituting
k
k
(1‚àíŒ≥)2
it into Lemma C.5 we have
K
X

œÄ(¬∑|s, a1:m‚àí1 ) ‚àí œÄk (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑)

k=1

2Œ∑K
log |A|
+
2
(1 ‚àí Œ≥)
Œ∑
p
2 2 log |A|K
‚â§
1‚àíŒ≥
‚â§

q
where the last line is by setting Œ∑ = (1 ‚àí Œ≥) log2K|A| .
28

Lemma C.7. For any conditional policy œÄ : S √ó Am‚àí1 ‚Üí
‚àí ‚àÜ(A),

1:m
Q1:m
) ‚â• minm f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄk ) ‚àí ŒªŒµr .
œÄ (s0 , œÄ
f ‚ààF

Proof. For any conditional policy œÄ 1:m , and ‚àÄm ‚àà N . With realizability assumption we
1:m
2
have Q1:m
œÄ = arg minf supŒΩ kf ‚àí TœÄ f k2,ŒΩ for any admissible ŒΩ
1:m
J(œÄ) = Q1:m
)
œÄ (s0 , œÄ



1:m
1:m
1:m
1:m
= Q1:m
) ‚àí Q1:m
) + ŒªE 1:m (Q1:m
) + ŒªE 1:m (Q1:m
œÄ (s0 , œÄ
œÄ (s0 , œÄ
œÄ , œÄ) + QœÄ (s0 , œÄ
œÄ , œÄ)

‚â• minm f (s0 , œÄ 1:m ) + ŒªE 1:m (f, œÄ) ‚àí ŒªŒµr ,
f ‚ààF

where in the last line we use Lemma C.1.
Proofs for Theorem 2.
Proof. Use Lemma C.7, at the k-th iteration we have

J(œÄk ) ‚â• minm f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄk ) ‚àí ŒªŒµr
f ‚ààF
‚â• fkm (s0 , œÄk1:m ) ‚àí ŒªŒµr

= JMk (œÄk ) ‚àí ŒªŒµr
where fkm is the multi-agent value function of Markov game Mk (cf. Lemma C.3).
Therefore we have,
K

1 X
(J(œÄ‚àó ) ‚àí J(œÄk ))
J(œÄ‚àó ) ‚àí J(œÄÃÑ) =
K
‚â§

‚â§

1
K

k=1
K
X

(J(œÄ‚àó ) ‚àí JMk (œÄk )) + ŒªŒµr

k=1
K
X

K
1 X
1
(JMk (œÄ‚àó ) ‚àí JMk (œÄk )) +
(J(œÄ‚àó ) ‚àí JMk (œÄ‚àó )) +ŒªŒµr
K
K
k=1
k=1
|
{z
} |
{z
}
I

II

Term I. Apply performance difference lemma we have
K

1 X
(JMk (œÄ‚àó ) ‚àí JMk (œÄk ))
K
k=1

N
K 

X
1 1 X
œÄk
1:m‚àí1
m
1:m‚àí1
m
k
)
Es‚àºdœÄ‚àó Ea1:m‚àí1
=
QœÄ
(s,
a
,
œÄ
)
‚àí
Q
(s,
a
,
œÄ
‚àó
k
Mk
Mk
1‚àíŒ≥K
m=1

=

1 1
1‚àíŒ≥K

N
X

k=1
K
X

Es‚àºdœÄ‚àó ,a1:m‚àí1 ‚àºœÄ‚àó

m=1

œÄ‚àóm (¬∑|s, a1:m‚àí1 ) ‚àí œÄkm (¬∑|s, a1:m‚àí1 ), fkm (s, a1:m‚àí1 , ¬∑)

k=1

m
where the last line is
qbecause fk is the multi-agent state-action value function for Mk .

Then, if Œ∑ = (1 ‚àí Œ≥)

log |A|
2K , Lemma C.6 gives

2N
Term I ‚â§
(1 ‚àí Œ≥)2
29

r

2 log |A|
.
K

Term II. The following analysis holds for any m ‚àà N so we omit m for clarity. For
this term, again, we use Lemma 1 [Xie and Jiang, 2020] to transform it into norm over
state-action distributions

J(œÄ‚àó ) ‚àí JMk (œÄ‚àó ) = QœÄ‚àó (s, œÄ‚àó ) ‚àí JMk (œÄ‚àó ) ‚â§

1
kQœÄ‚àó ‚àí TœÄ‚àó ,Mk QœÄ‚àó k2,dœÄ
‚àó
1‚àíŒ≥

Note the definition of the auxiliary Markov game for which f k is its value function(cf.
Lemma C.3) we have
1
kQœÄ‚àó ‚àí Rk ‚àí Œ≥PœÄk QœÄ‚àó k2,dœÄ
‚àó
1‚àíŒ≥
1
=
kfk ‚àí TœÄk fk k2,dœÄ
‚àó
1‚àíŒ≥

Rk = (I ‚àí Œ≥P)fk

d

C¬µœÄ‚àó
‚â§
kfk ‚àí TœÄk fk k2,D
1‚àíŒ≥
d

C œÄ‚àó

¬µ
which is no greater than 1‚àíŒ≥

‚àö

Œµr +

q

1
Œª(1‚àíŒ≥)



1
, because E(fk , œÄk ) ‚â§ Œµr + (1‚àíŒ≥)Œª
:

fk (s0 , œÄk ) + ŒªE(fk , œÄk ) = min (f (s0 , œÄk ) + ŒªE(f, œÄk ))
f

‚â§ QœÄk (s0 , œÄk ) + ŒªE(QœÄk , œÄk ),
1
+ ŒªŒµr .
‚â§
1‚àíŒ≥

Lemma C.2

The proof is completed by substituting Œª to the original expression.

D

Pessimistic MA-PPO with General Function Approximation

In this section we extend the results from linear function approximation to general function
approximation (cf. Section 5).
Algorithm 4 Pessimistic Multi-Agent PPO with General Function Approximation
Input: Regularization coefficient Œª.
Output: Uniformly sample k from 0, 1 ¬∑ ¬∑ ¬∑ K ‚àí 1, return œÄÃÑ = œÄk .
1: Initialize uniformly: Œ∏0m = 0 for every m ‚àà N .
2: for k = 0, 1, . . . , K ‚àí 1 do
3:
for m = 1, 2, ¬∑ ¬∑ ¬∑ , N do
4:
Obtain the pessimistic estimate:

fkm ‚Üê
‚àí arg minf ‚ààF m f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄk ) .
5:
Policy improvement: for any (s, a1:m ) ‚àà S √ó Am ,
m
m
œÄk+1
(am |s, a1:m‚àí1 ) ‚àù œÄk+1
(am |s, a1:m‚àí1 ) ¬∑ exp (Œ∑fkm (s, a1:m‚àí1 , am )).

end for
7: end for
6:

In this setting, the value function is searched over a finite set F m . We impose the
following regularity conditions on the general function class

30

Assumption D.1. For any m ‚àà N , f ‚àà F m and (s, a1:m ) ‚àà S √ó Am , it holds
|f (s, a1:m‚àí1 , am )| ‚â§

1
,
1‚àíŒ≥

|F m | ‚â§ |F |

(15)
(16)

where |F| is a certain positive number.
Instead of a pre-defined fixed policy class Œ†, now policy improvement is made upon
F m , formally, for m ‚àà N and (s, a1:m‚àí1 ) ‚àà S √ó Am‚àí1
Ô£±
Ô£º
Ô£´
Ô£∂
k
Ô£Ω
Ô£≤
X
Œ†m = œÄ m (¬∑|s, a1:m‚àí1 ) ‚àù exp Ô£≠Œ∑
fj (s, a1:m‚àí1 , ¬∑)Ô£∏ : fj ‚àà F m , 0 ‚â§ k ‚â§ K ‚àí 1 . (17)
Ô£≥
Ô£æ
j=0

Also note that under general function approximation, policy improvement has to be
specific for each (s, a1:m ), which might become troublesome when the state space is enormous.
For general function approximation, two common expressivity assumptions on F are
required [Antos et al., 2008, Xie et al., 2021].
Assumption D.2 (Realizability). For any m ‚àà N , œÄ ‚àà Œ†m ,
inf

sup

f ‚ààF m admissable ŒΩ

kf ‚àí TœÄ1:m f k22,ŒΩ = Œ∂F .

where ŒΩ can be any admissible distribution over S √ó A1:m , and
Assumption D.3 (Completeness). For any m ‚àà N , œÄ ‚àà Œ†m ,
sup

infm kf 0 ‚àí TœÄ1:m f k22,¬µ = Œ∂F0 .

f ‚ààF m f ‚ààF

We have the following concentration guarantees for general function approximation [Xie
et al., 2021].
Lemma D.4. For any m ‚àà N , œÄ ‚àà Œ†m , let
fœÄ1:m = arg min
f ‚ààF m

sup
admissable ŒΩ

kf ‚àí TœÄ1:m f k22,ŒΩ ,

with probability at least 1 ‚àí Œ¥, it holds
E

1:m

(fœÄ1:m , œÄ) ‚â§ O
=O

!
m
m|
log |F ||Œ†
Œ¥
+ Œ∂F
n(1 ‚àí Œ≥)2
!
m
K log |FŒ¥ |
+ Œ∂F = Œµr ,
n(1 ‚àí Œ≥)2

(18)

where we note that |Œ†m | ‚â§ |F m |K from (17).
Lemma D.5. For any m ‚àà N , œÄ ‚àà Œ†m , f ‚àà F m , if E 1:m (f, œÄ) ‚â§ Œµ, with probability at
least 1 ‚àí Œ¥ it holds
s
Ô£∂
Ô£´
|F m |
q
q
K log Œ¥
1
1:m
0
Ô£≠
Ô£∏
f ‚àí TœÄ f 2,Dm ‚â§ O
+ Œ∂F + Œ∂F0 + Œµ.
1‚àíŒ≥
n

31

Lemma D.6. For any conditional policy œÄ : S √ó Am‚àí1 ‚Üí
‚àí ‚àÜ(A),
‚àö

1:m
)‚â•
Q1:m
œÄ (s0 , œÄ

minm

f ‚ààF

f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄk )



‚àí

Œ∂F
‚àí ŒªŒµr .
1‚àíŒ≥

Proof. For any conditional policy œÄ 1:m , and ‚àÄm ‚àà N . Let fœÄ1:m = arg minf supŒΩ kf ‚àí
TœÄ1:m f k22,ŒΩ for any admissible ŒΩ
1:m
J(œÄ) = Q1:m
)
œÄ (s0 , œÄ



1:m
1:m
1:m
= Q1:m
) ‚àí fœÄ1:m (s0 , œÄ 1:m ) + ŒªE 1:m (Q1:m
) + ŒªE 1:m (fœÄ1:m , œÄ)
œÄ (s0 , œÄ
œÄ , œÄ) + fœÄ (s0 , œÄ
‚àö

Œ∂F
1:m
1:m
‚â• minm f (s0 , œÄ ) + ŒªE (f, œÄ) ‚àí
‚àí ŒªŒµr ,
f ‚ààF
1‚àíŒ≥
where in the last line we use Lemma C.1 and the realizability assumption
‚àö
kfœÄ1:m ‚àí TœÄ1:m fœÄ1:m k2,dœÄ
Œ∂F
1:m
1:m
fœÄ ‚àí QœÄ ‚â§
‚â§
.
1‚àíŒ≥
1‚àíŒ≥

Equipped with these useful lemmas, we are prepared to proceed to the main theorem
for the general function approximation setting
Theorem D.7. Recall the definition of Œµr (18), for the output
q policy œÄÃÑ attained by Algo‚àí2

rithm 4 in a fully cooperative Markov game, set Œ∑ = (1 ‚àí Œ≥) log2K|A| and Œª = (1 ‚àí Œ≥)‚àí1 Œµr 3 .
After K iterations, w.p. at least 1 ‚àí Œ¥ we have
s
Ô£´
Ô£´
Ô£∂Ô£∂
r
|F |
dœÄ‚àó
q
p
log |A|
N
C¬µ
1 3 K log n
J(œÄ‚àó ) ‚àí J(œÄÃÑ) ‚â§ O Ô£≠
+
¬∑Ô£≠
+ Œ∂F + Œ∂F0 + 3 Œ∂F Ô£∏Ô£∏
2
(1 ‚àí Œ≥)
K
1‚àíŒ≥
1‚àíŒ≥
n
Proof. Use Lemma D.6, at the k-th iteration we have
‚àö

J(œÄk ) ‚â• minm
f ‚ààF

f (s0 , œÄk1:m ) + ŒªE 1:m (f, œÄk )



‚àö

‚â• fkm (s0 , œÄk1:m ) ‚àí
‚àö

= JMk (œÄk ) ‚àí

‚àí

Œ∂F
‚àí ŒªŒµr
1‚àíŒ≥

Œ∂F
‚àí ŒªŒµr
1‚àíŒ≥

Œ∂F
‚àí ŒªŒµr
1‚àíŒ≥

Analogous to Appendix C, we have
K

1 X
J(œÄ‚àó ) ‚àí J(œÄÃÑ) =
(J(œÄ‚àó ) ‚àí J(œÄk ))
K
1
‚â§
K

k=1
K
X

‚àö

(J(œÄ‚àó ) ‚àí JMk (œÄk )) +

k=1

Œ∂F
+ ŒªŒµr
1‚àíŒ≥

‚àö
K
1 X
Œ∂F
‚â§
(JMk (œÄ‚àó ) ‚àí JMk (œÄk )) +
(J(œÄ‚àó ) ‚àí JMk (œÄ‚àó )) +
+ ŒªŒµr
K
K
1‚àíŒ≥
k=1
k=1
|
{z
} |
{z
}
K
1 X

I

II

Term I. The
q analysis for the optimization term is the same as that in Appendix C. If
Œ∑ = (1 ‚àí Œ≥) log2K|A| , Lemma C.6 gives
r
2N
2 log |A|
Term I ‚â§
.
(1 ‚àí Œ≥)2
K
32

Term II.

Similar with Appendix C

J(œÄ‚àó ) ‚àí JMk (œÄ‚àó ) = QœÄ‚àó (s, œÄ‚àó ) ‚àí JMk (œÄ‚àó )
d

C¬µœÄ‚àó
‚â§
kfk ‚àí TœÄk fk k2,D
1‚àíŒ≥
which is no greater than
s
Ô£´ Ô£´
Ô£∂
Ô£∂
s
|F m |
dœÄ‚àó
q
q
K
log
C¬µ Ô£≠ Ô£≠ 1
1
Œ¥ Ô£∏
Ô£∏,
O
+ Œ∂F0 + Œ∂F0 + Œµr +
1‚àíŒ≥
1‚àíŒ≥
n
(1 ‚àí Œ≥)Œª
1
because we have E(fk , œÄk ) ‚â§ Œµr + (1‚àíŒ≥)Œª
:

fk (s0 , œÄk ) + ŒªE(fk , œÄk ) = min (f (s0 , œÄk ) + ŒªE(f, œÄk ))
f

‚â§ QœÄk (s0 , œÄk ) + ŒªE(QœÄk , œÄk ),
1
‚â§
+ ŒªŒµr .
1‚àíŒ≥

Lemma C.2

The proof is completed by substituting Œª to the original expression.

E

Simulation

In this section, we perform a toy example to showcase the superiority of our sequential
update structure over naive independent policy gradient updates. We consider von Neumann‚Äôs ratio game, a simple stochastic game also used by [Daskalakis et al., 2020].
In the game, there are only two agents, and each has an action space of 2. There is
only one state, i.e., no state transition exists. The immediate reward for selecting actions
(a, b) is R(a, b) the probability of stopping in each round is S(a, b) . The value function
V (œÄx , œÄy ) for this game is given by
V =

œÄx> RœÄy
.
œÄx> SœÄy

The two agents cooperate with each other to maximize the value function. From now
on, we shall use (x, 1 ‚àí x) and (y, 1 ‚àí y) to represent both policies. We set parameters as




1
0.5
1
1
R=
, and R =
.
‚àí0.5 1
0.1 0.1
Consider the value function as a function of variables x and y , then the stationary
point is near (x, y) = (0.5, 0) , at which the value function is V ‚âà 0.46 , which is smaller
than the global maximum V = 1.
To solve the problem, we adopt two algorithms: (1) our algorithm with sequential
updates and (2) the independent (policy gradient) learning method. In both algorithms,
we use softmax parametrization for policies. In particular, our log-linear parameterization
(2) becomes softmax parametrization by setting œÜ as one-hot representations, i.e., for
action a , œÜ(a)> Œ∏ = Œ∏a where Œ∏a represents the specific entry of Œ∏ that corresponds to a .

33

Results We test our algorithm with sequential gradient updates and the independent
learning method in different settings. The results are shown in Figure 1. 5 Below we
discuss the empirical findings from this simulation study.
First, we find that the independent policy optimization method often struggles around
the stationary point (see (a)-(c)) that is not necessarily globally optimal. In this example,
a big stepsize would help alleviate the issue (e.g., in (c), independent PG escapes the
stationary point after 3000 iterations). However, the convergence to global optima is still
slower than our method. We note that noise might help to escape the stationary points [Jin
et al., 2017]. Our findings align with the theoretical comparisons we made aforementioned.
Even if the independent PG method is not trapped by a stationary point, from (d)-(f),
our algorithm consistently outperforms in terms of maximizing the value function.
In this toy example, the optimization landscape is quite simple: only two agents participate, each with only two possible actions. No state transition is allowed, which is the
main difficulty in performing sequential decision-making. We point out that, globally,
there is only one stationary point. In such an effortless case, our algorithm consistently
outperforms independent PG in mainly two folds. First, our algorithm does not struggle
like independent PG when the current policy is near the stationary point where gradient information is few. Second, our algorithm demonstrates a fast convergence rate to
the global maximum value function. Therefore, when the complexity of the environment
increases significantly, for instance: (1) multiple heterogeneous agents interact with each
other and the unknown environment, (2) complex function approximators are adopted
(e.g., deep neural networks), utilizing independent PG would be more problematic in
terms of locating the global optimum because there will be more stationary points in the
landscape.
Our findings showcase the necessity and usefulness of the conditional dependency structure, which helps us find a policy that enjoys a globally sub-optimal value function.

a

b

c

d

e

f

Figure 1: Performances of our algorithm and the independent learning method. In (a)-(c):
policies are initialized close to the stationary point. In (d)-(f): both policies are uniformly
initialized.

5

Implementation can be found at https://github.com/zhaoyl18/ratio_game.

34

