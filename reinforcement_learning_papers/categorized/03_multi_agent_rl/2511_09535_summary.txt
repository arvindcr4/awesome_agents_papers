PAPER: 2511_09535.pdf
TITLE: Robust and Diverse Multi-Agent Learning via Rational Policy Gradient
ARXIV_ID: 2511.09535v1
RESEARCH_METHOD: 03_multi_agent_rl
METHOD_DESCRIPTION: This paper introduces Rational Policy Gradient (RPG), a novel approach to multi-agent reinforcement learning that addresses the issue of self-sabotage in adversarial optimization algorithms. RPG ensures that agents' policies are rational and optimal with respect to at least one possible partner policy, preventing self-sabotage and promoting robust and diverse learning. The method is based on the Rationality-Preserving Policy Optimization (RPO) formalism, which modifies the adversarial optimization objective to include a rationality constraint.
KEY_CONTRIBUTIONS:
* Introduces Rational Policy Gradient (RPG), a novel approach to multi-agent reinforcement learning that addresses self-sabotage in adversarial optimization algorithms.
* Develops the Rationality-Preserving Policy Optimization (RPO) formalism, which modifies the adversarial optimization objective to include a rationality constraint.
* Applies RPG to various multi-agent learning algorithms, including Adversarial Policy, Adversarial Training, and Adversarial Diversity, to prevent self-sabotage and promote robust and diverse learning.
* Empirically evaluates the effectiveness of RPG in several cooperative and general-sum environments, demonstrating its ability to prevent self-sabotage and improve robustness and diversity.
