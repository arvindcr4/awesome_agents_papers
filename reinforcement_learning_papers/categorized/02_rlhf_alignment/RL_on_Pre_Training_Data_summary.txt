Here's the extracted information:

**PAPER**: Reinforcement Learning on Pre-Training Data
**TITLE**: Reinforcement Learning on Pre-Training Data
**RESEARCH_METHOD**: 02_rlhf_alignment
**METHOD_DESCRIPTION**: This paper introduces Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing large language models (LLMs). RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). The method uses a next-segment reasoning objective, which rewards the policy for accurately predicting the subsequent segment of text conditioned on the preceding context.
**KEY_CONTRIBUTIONS**:
* The paper proposes RLPT, a method that scales RL on pre-training data, removing the reliance on human annotation.
* Extensive experiments on general-domain and mathematical reasoning tasks across multiple models show that RLPT substantially improves performance and exhibits a favorable scaling trend.
* Results demonstrate that RLPT provides a strong foundation for subsequent RLVR, extending the reasoning boundaries of LLMs and boosting performance on mathematical reasoning benchmarks.
* The paper analyzes the thinking patterns of RLPT, showing that it approaches the next-segment reasoning task through a structured sequence, and introduces a relaxed prefix reward to address the issue of uneven information distribution across sentence-based segmentation.
