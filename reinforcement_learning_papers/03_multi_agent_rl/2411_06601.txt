OffLight: An Offline Multi-Agent Reinforcement Learning Framework
for Traffic Signal Control

arXiv:2411.06601v3 [cs.AI] 18 Mar 2025

Rohit Bokade and Xiaoning Jin
Abstract— Efficient traffic signal control (TSC) is crucial
for reducing congestion and improving urban mobility. While
multi-agent reinforcement learning (MARL) has shown promise
in adaptive traffic management, its reliance on real-time
interactions makes online training costly and impractical. Offline
MARL, which learns from historical data, offers a safer and
more scalable alternative but struggles with heterogeneous behavior policies in real-world datasets. We introduce OffLight, a novel
offline MARL framework designed to handle policy heterogeneity
in traffic signal control datasets. OffLight integrates importance
sampling (IS) to correct for distributional shifts and returnbased prioritized sampling (RBPS) to emphasize high-quality
experiences. To model diverse behavior policies, it leverages a
Gaussian mixture model variational graph autoencoder (GMMVGAE), capturing spatial and temporal traffic dynamics while
improving policy estimation. We evaluate OffLight across realworld urban traffic scenarios varying from small to large traffic
signal networks. Results show that OffLight outperforms existing
offline RL methods, reducing average travel time by up to
7.8% and queue length by 11.2% compared to state-of-the-art
approaches. Unlike prior offline RL methods, OffLight effectively
adapts to complex and mixed-policy datasets, making it more
reliable for real-world deployment without risky online training.
These results highlight OffLight’s potential to improve urban
traffic management at scale. Our implementation is available
at https://github.com/rbokade/offlight.

I. INTRODUCTION
Efficient traffic signal control (TSC) is critical for urban
mobility, directly affecting congestion, travel times, and city
livability. Multi-agent reinforcement learning (MARL) has
emerged as a promising solution by enabling decentralized,
adaptive, and intelligent control of traffic signals [1], [2].
While online MARL methods rely on real-time interaction,
offline MARL uses pre-collected data, offering safer, more
cost-efficient, and scalable alternatives by eliminating the
need for risky real-time experimentation [3]–[6].
Several offline RL methods have shown promise in traffic
signal control by using imitation learning to bootstrap initial
training. Methods such as DemoLight [7], Cooperative
Control [8], and CrossLight [9] have leveraged expert demonstrations to quickly initialize and refine traffic control policies.
More recently, DataLight [4] improved policy performance
through attention mechanisms that capture spatial traffic
distributions. However, these approaches do not fully address
a significant issue in offline MARL datasets: heterogeneous
behavior policies arising from varied control strategies,
temporal variability, and the mixture of different operational
contexts [10], [11].
Both authors are with the Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA 02115, USA. {bokade.r,

xi.jin}@northeastern.edu

The heterogeneity in real-world traffic datasets poses
significant challenges for offline MARL, primarily due to
distributional shifts and suboptimal behavior biases that
degrade policy learning [12], [13]. Recent studies, including
Hong et al. [14] and Yue et al. [15], have proposed prioritized
resampling techniques to emphasize high-quality experiences,
thus improving policy robustness in offline settings. Nevertheless, effectively modeling diverse behavior policies in complex
multi-agent environments remains a substantial challenge for
offline MARL methods.
To address these challenges, we propose OffLight, an
offline MARL framework specifically designed for handling
heterogeneous behavior policies in TSC datasets. OffLight
integrates importance sampling (IS) to correct distributional
shifts and return-based prioritized sampling (RBPS) to
highlight high-quality experiences. Additionally, OffLight
uses a Gaussian mixture model variational graph autoencoder
(GMM-VGAE) to represent diverse behavior policies accurately. Experiments across real-world urban traffic scenarios
show that OffLight outperforms existing offline RL methods,
demonstrating reduced average travel times and queue lengths.
Our results highlight OffLight’s capability to reliably manage
urban traffic networks without the risks associated with online
training, making it a practical and scalable solution for realworld deployment.

II. BACKGROUND

A. Multi-Agent Reinforcement Learning
We model the MARL problem as Decentralized
Partially Observable Markov Decision Processes
(Dec-POMDPs) [16], formally defined by the tuple
i N
⟨N, S, {Ai }N
is the
i=1 , {O }i=1 , P, R, H, γ⟩. Here, N
number of agents, S is the state space, Ai and Oi are the
action and observation spaces for agent i, respectively, P is
the state transition probability, R is the shared reward, H is
the finite horizon, and γ ∈ [0, 1) is the discount factor.
Each agent i selects actions based on its local actionobservation history hit = {oi0 , ai0 , . . . , oit }, resulting in joint
action at = (a1t , . . . , aN
t ), transitioning the environment to
state st+1 according to P , and yielding a shared reward R.
The objective is to determine decentralized policies {π i }N
i=1 ,
with each policy π i : Hti → Ai mapping histories to actions,

Fig. 1: General Offline MARL Framework for Traffic Signal Control
to maximize expected cumulative reward1 :
"H
#
X
max
E
γ t R(st , at ) .
i
{π }

(1)

t=0

To optimize decentralized policies, Deep Q-Learning
(DQNs) [17] is commonly employed, where each agent independently updates its Q-values based on local observations
and rewards as follows:

Qi (oit , ait ) ← Qi (oit , ait ) + α rti + γ max
Qi (oit+1 , a′ )
a′
 (2)
i i i
− Q (ot , at ) .
However, independent learning may lead to suboptimal
coordination. Recent frameworks, such as CoLight [11],
PressLight [18], and AttendLight [19], address this by
incorporating Graph Attention Networks (GATs) [20] for
inter-agent communication. We leverage these successful
frameworks as the basis for our approach.
B. Offline Multi-Agent Reinforcement Learning
Offline Multi-Agent Reinforcement Learning (Offline
MARL) learns policies from fixed datasets without additional
environment interaction during training [3], [4].
Offline MARL presents several challenges: (1) Distributional Shift: The dataset may differ significantly from the
policy’s actual environment interactions, leading to inaccurate value estimations and reduced performance [21]. (2)
Extrapolation Error: Without interactive feedback, agents
risk generalizing poorly to situations not covered by the
dataset. (3) Limited Coverage: Historical data might not
include optimal coordination strategies necessary for effective
multi-agent policies.
Recent offline RL algorithms address these issues. Conservative Q-Learning (CQL) [12] introduces a penalty for
overestimating Q-values of actions not in the dataset. Its
objective is:
min α (Eoi ∼ D, ai ∼ π(ai |oi )[Q(oi , ai )]
Qi

(3)

− E(oi , ai ) ∼ D[Q(oi , ai )]) + LBellman (Qi ).
1 In practice, we employ local rewards for each agent and adopt Centralized
Training with Decentralized Execution (CTDE), as we found this approach
converges faster than using a scalar global reward.

TD3+BC [22] combines TD3 with a behavior cloning term
to reduce policy deviation from the dataset, thus managing
distributional shift:
min −Eoi ∼ D[Qϕ(oi , πθi (oi ))]
π θi

+ λE(oi , ai )

(4)

∼ D[|πθi (oi ) − ai |22 ].

Both methods can be adapted to multi-agent systems using
Centralized Training with Decentralized Execution (CTDE)
[23], [24], enabling centralized training and decentralized
agent execution for better scalability and coordination.
C. Motivation for the OffLight Framework
Offline MARL faces unique challenges when datasets
contain heterogeneous behavior policies, especially in domains like Traffic Signal Control (TSC). Different operational
contexts or sources create diverse behaviors, increasing the
likelihood of distributional mismatch. This diversity leads to
unreliable Q-value estimates and degraded performance, as
distributional correction methods become less effective [21].
Additionally, accurately estimating mixed behavior policies
is difficult, complicating policy constraints and importance
sampling [13].
To address heterogeneous behavior policies in offline
MARL, we propose the OffLight framework. OffLight
utilizes a behavior policy estimation technique (GMM-VGAE)
to accurately model policy diversity, employs importance
sampling to mitigate distributional shift, and integrates
Reward-Based Prioritized Sampling (RBPS) for improved
sample efficiency. RBPS prioritizes high-reward episodes,
enhancing the focus on valuable training data. These strategies
aim to improve policy robustness and performance specifically
in Traffic Signal Control (TSC) scenarios. The OffLight
framework details are introduced in the next section.
III. O FF L IGHT F RAMEWORK
OffLight is designed to tackle the challenges of offline
MARL in the domain of traffic signal control (TSC). It combines a self-supervised learning approach with IS to account
for heterogeneous policies and improve learning from static,
pre-collected datasets. IS and RBPS weights are collected
before the offline RL training and stored (see Figure 2).
During the offline RL training phase, these weights are used to
improve learning. Below, we outline the architecture and key

components of OffLight, highlighting how it handles mixedpolicy data to enhance traffic management performance.

subsequent observations after taking actions At , Rt is the
reward received at time t, and Tk is the length of trajectory τk .
This structure reflects real-world data collected from sensorequipped intersections and enables learning traffic control
policies from historical data.
C. Self-Supervised Learning of Behavior Policies

Fig. 2: OffLight Architecture: Integrating GMM-VGAE with
IS and RBPS for offline MARL in traffic signal control
A. OffLight Agent Design
Observation Space: Each traffic signal agent has a limited
range of observation. This range reflects the realistic coverage
of common sensors such as inductive loop detectors or
cameras. For each incoming lane, the agent observes: (1)
i
number of vehicles {nl }L
l=1 , where Li is the number of
i
lanes for traffic signal i, average speed of vehicles {sl }L
l=1 ,
normalized by the speed limit, number of halted vehicles,
i
representing the queue length {ql }L
l=1 , and the current phase
ID, indicating the traffic signal’s current phase. These local
observations provide the sensory input required for each agent
to make real-time decisions about the traffic signal phases.
Action Space: Each traffic signal agent is responsible for
managing the signal phases at its respective intersection.
The agent’s available actions involve selecting one of the
predefined green phases applicable to that intersection. Once
a green phase is chosen, the system enforces a mandatory
transition to a yellow phase. The action selection occurs at
fixed intervals of 5 simulation seconds.
Reward Function: We use minimizing the queue length
as objective. The rewardPfor each agent i at each time step
rti is defined as rti = − l∈Li ql (t), where ql (t) represents
the queue length on lane l at time t, and Li is the set of
incoming lanes for traffic signal i. This reward structure
encourages agents to minimize congestion and improve traffic
flow throughout the system.
B. Dataset Structure
We consider a dataset D collected from a network of N
intersections over M episodic trajectories. Each trajectory
τk comprises a sequence of interactions between the agents
(traffic signals) and the environment. Formally, the dataset is
represented as:
M
D = {τk }k=1 ,

Tk
τi = {(Ot , At , Ot′ , Rt )}t=0
,

where
(5)
 1 2
where Ot = ot , ot , . . . , oN
denotes the observations
t
at
time
step
t
for
each
of
the
N intersections, At =
 1 2
at , at , . . . , aN
represents
the
joint
actions taken by the
t

agents at time t, Ot′ = o1t+1 , o2t+1 , . . . , oN
are the
t+1

OffLight employs GMM-VGAE to model heterogeneous
policies, using: (1) GMM-VGAE for Policy Diversity:
A Gaussian mixture graph variational autoencoder model
(GMM) captures distinct traffic control policies. (2) GATs
and LSTMs for Spatial-Temporal Dynamics: Graph attention networks (GATs) encode local interactions, while LSTMs
track temporal trends.
These components construct a structured latent space
for estimating policies at each intersection. The encoder
processes agent observations and past actions to capture
spatial-temporal dependencies. The latent space represents
diverse traffic control policies using a GMM-VGAE model.
Finally, the decoder reconstructs policy distributions based on
the latent features and current observations. We also enable
parameter sharing among agent, reducing model complexity
while improving efficiency. This design allows OffLight to
effectively model heterogeneous policies across intersections.
D. Importance sampling Integration
To address the challenge of distributional shift between
the behavior policy and the target policy, OffLight integrates
IS. This mechanism adjusts the influence of each transition
based on its alignment with the target policy, ensuring that
the learning algorithm emphasizes relevant and high-quality
data.
N
1 X πθi (ait | hit )
k
(6)
wIS,t
=
N n=1 π̂bi (ait | hit , zt )
k
where wIS,t
is the importance sampling weight for the
transition at time step t in episode k, πθi (ait | hit ) 2 denotes
the probability of agent n taking action ait under the learned
target policy πθ , conditioned on the neighborhood aggregated
information hit , and π̂bi (ait | hit , zt ) represents the estimated
behavior policy probability for agent n, obtained from the
GMM-VGAE, conditioned on it’s hidden state hit and the
latent embedding of behavior policy zt .
IS corrects for the discrepancy between the behavior policy
and the target policy by re-weighting transitions 3 . Transitions
where the actions are more probable under the target policy
relative to the behavior policy receive higher importance
sampling weights.

E. Return-Based Prioritized Sampling
To enhance sample efficiency and accelerate the learning
process, OffLight employs RBPS, similar to [15]. This
2 hi is computed for each agent using Graph Neural Networks (GNNs),
t
which aggregate information from neighboring agents.
3 The target policy is the policy that the offline RL algorithm seeks to
optimize, while the behavior policy refers to the policy used to generate
the offline dataset.

strategy prioritizes episodes based on their cumulative rewards, ensuring that the learning algorithm focuses on more
successful traffic control experiences.


Gk − Gmin
k
+ pbase
(7)
wRBPS
=C
Gmax − Gmin
k
where wRBPS
is the Return-Based Prioritized Sampling weight
PTk
for episode k, Gk = t=0
rt is the total reward for episode k,
Gmin and Gmax are the minimum and maximum total rewards
across all episodes in the dataset, respectively. pbase is a small
positive constant added to ensure that all episodes have a nonzero probability of being sampled, and C is a normalization
constant to maintain numerical stability or ensure the weights
sum to a desired value.
RBPS ensures that episodes with higher returns, indicative
of more effective traffic management, are sampled more
frequently. This focus accelerates learning by emphasizing
experiences that contribute significantly to improved traffic
flow and reduced congestion. This especially proves beneficial
when the distribution of the data is skewed or multimodal as
can be seen in Figure 3.

G. Integration with Offline RL Algorithms
OffLight can easily be integrated with existing offline RL
algorithms. CQL and TD3+BC are chosen as baseline offline
RL algorithms. The combined weighting scheme—comprising
IS weights and RBPS weights—is incorporated into both
the loss functions and the sampling mechanisms of these
algorithms to ensure effective bias correction and sample
prioritization.
Loss Function Adjustment: The combined weights
w̃combined,t are used to scale the loss components, ensuring that
transitions from high-reward episodes aligned with the target
policy have a greater impact on policy and value updates.
This scaling is formalized in the loss functions of CQL and
TD3+BC as follows:
L = E(O,A,O′ ,R)∼D [w̃combined,t · ℓ(O, A, O′ , R)] ,

(9)

′

where ℓ(O, A, O , R) represents the specific loss term associated with the chosen RL algorithm (e.g., Bellman error for
CQL or policy loss for TD3+BC).
Sampling Mechanism: Episodes are sampled based on their
total rewards, prioritizing those with higher cumulative returns.
This is implemented by adjusting the sampling probabilities
pi according to wRBPS , ensuring that more successful episodes
are sampled more frequently. This prioritization enhances the
efficiency of the learning process by focusing on the most
informative and effective experiences.
IV. E XPERIMENTAL S ETUP

(a) Jinan

(b) Hangzhou

(c) Manhattan

Fig. 3: Distribution of Episodic Returns in mixed-policy
datasets. It highlights the variability and heterogeneity in the
offline dataset, with some episodes achieving high returns
while others are suboptimal. RBPS targets this imbalance
by prioritizing episodes with higher returns, ensuring that
learning is focused on successful traffic control strategies.
F. Combining importance sampling and Return-Based Prioritized Sampling
OffLight integrates IS and RBPS to improve sample
efficiency and mitigate distributional shift in offline MARL for
TSC. IS ensures transitions align with the target policy, while
RBPS prioritizes high-reward episodes, refining learning by
emphasizing both relevance and quality4 .
k
k
k
wcombined,t
= wIS,t
× wRBPS

(8)

To maintain stability, the combined weights are normalized
and clipped within each minibatch. These adjusted weights
are then incorporated into the Temporal Difference (TD) error
and Conservative Q-Learning (CQL) loss functions, ensuring
more effective policy learning.
k
4 Multiplying w k
IS,t and wRBPS ensures that only transitions that are both

relevant to the target policy and come from successful episodes are given
significant weight. This dual prioritization enhances the effectiveness and
robustness of policy learning.

We evaluate OffLight in three real-world urban traffic
scenarios 5 : Jinan (12 intersections), Hangzhou (16 intersections), and Manhattan (196 intersections). These scenarios
vary in size and traffic demand, allowing for a comprehensive
assessment of OffLight’s scalability and robustness. Traffic
demand is classified as low, medium (real-world data), or
high by adjusting the number of vehicles entering the network
proportionally.
The experiments are designed to address the following key
questions:
1) How effectively does OffLight model heterogeneous
behavior policies in offline datasets?
2) What is the impact of incorporating importance sampling on learning from mixed-policy datasets?
3) How does the proportion of suboptimal policy data
affect overall performance?
Average Network
Demand (veh/hr)

Low

Medium

High

Intersections

Jinan
Hangzhou
Manhattan

450
350
150

550
420
250

650
500
350

12
16
196

TABLE I: Traffic demand levels and network sizes for
Jinan, Hangzhou, and Manhattan. Demand represents vehicles
generated per incoming lane per hour.
5 https://github.com/traffic-signal-control/
sample-code/

(a) Jinan

(b) Hangzhou

(c) Manhattan

Fig. 4: Traffic networks used in the experiments: (a) Jinan, (b) Hangzhou, and (c) Manhattan, illustrating the varying scales
and complexities of the test scenarios.
Traffic Scenarios: The selected networks represent different
levels of complexity: Jinan (moderate, 12 intersections),
Hangzhou (higher traffic, 16 intersections), and Manhattan
(large-scale, 196 intersections). Each scenario simulates
low, medium, and high demand conditions (Table I).
Data Collection: Training data includes traffic simulations
using multiple controllers: rule-based methods (Fixed
Time, Greedy, Max Pressure, SOTL), expert RL controllers
(MAPPO), and random policies for suboptimal behavior.
Each simulation runs for 1 hour, divided into 10 episodes
of 6 minutes each, across three demand levels, totaling
100 simulation hours per controller per demand level.
Baselines: OffLight is compared against Behavior
Cloning (BC), Conservative Q-Learning (CQL), and
TD3+BC. Each algorithm is trained for 20k timesteps.
Evaluation Metrics: Performance is measured using
Average Travel Time (ATT) and Queue Length (QL),
assessing traffic flow and congestion reduction across
different demand levels.

Fig. 5: Performance Comparison of OffLight (CQL) and OffLight (TD3+BC) on Mixed Data showing the improvements
in average travel time (ATT) across different traffic demand
levels
networks. Performance gains are smaller in low-traffic scenarios, where traffic flow is already manageable. These results
indicate that OffLight effectively prioritizes high-quality data,
improving traffic efficiency in congested environments.

V. R ESULTS AND D ISCUSSION
This section presents the evaluation of OffLight across
various traffic scenarios. We measure performance in terms
of Average Travel Time (ATT) and Queue Length (QL),
highlighting the effectiveness of OffLight in learning from
mixed-policy datasets.
A. Performance on Mixed-Quality Data
We assess OffLight’s performance on datasets with varying
proportions of expert and random policy data. The results
are shown for three traffic networks: Jinan, Hangzhou, and
Manhattan, under low, medium, and high traffic demand.
Results show that OffLight consistently outperforms baselines,
particularly in high-demand conditions, demonstrating its
ability to manage congestion in large-scale networks.
1) Average Travel Time (ATT): Figure 5 compares OffLight
to offline RL baselines. OffLight achieves lower ATT across
all scenarios, with the greatest improvements in high-demand
conditions. In Manhattan, OffLight reduces ATT by up to
7.8% (TD3+BC variant) and 6.9% (CQL variant) compared
to their respective baselines, proving its scalability in complex

Fig. 6: Learning curves for the Jinan scenario under medium
traffic demand, showing the convergence of queue length for
OffLight and baseline methods.
2) Queue Length (QL): Table III and Figure 6 show that
OffLight significantly reduces queue lengths, especially in
medium and high traffic conditions. In Hangzhou, OffLight
(TD3+BC) decreases queue lengths by 11.2% compared to
TD3+BC alone. In Manhattan, OffLight (CQL) achieves a
6% reduction under high traffic demand, demonstrating its
capability to optimize traffic flow at intersections. Improvements are smaller in low-traffic scenarios where congestion
is naturally less severe.
Figure 6 highlights the learning curves for the Jinan sce-

Jinan
Category

Algorithm

Hangzhou

Manhattan

Low

Medium

High

Low

Medium

High

Low

Medium

High

Rule-based

Random
Fixed Time
Greedy
Max Pressure
SOTL

411.97
354.95
287.10
386.43
399.30

450.61
380.26
310.08
409.47
418.34

470.82
407.25
337.60
434.91
443.35

418.89
370.11
303.02
405.95
447.22

430.01
392.69
307.41
427.04
470.14

451.15
407.33
314.72
437.44
476.91

774.33
539.07
484.01
640.96
540.55

784.56
574.17
527.01
658.31
583.13

790.14
612.77
575.03
684.59
615.19

Online

MAPPO

263.87

282.86

301.49

272.57

280.82

292.72

322.99

367.83

402.99

Offline

BC
TD3+BC
CQL
OffLight (TD3+BC)
OffLight (CQL)

364.50
321.87
326.06
292.90
303.24

388.18
356.51
337.74
317.29
307.34

409.36
364.26
372.41
316.91
324.00

382.59
324.18
369.63
288.52
325.27

393.38
338.52
350.06
299.59
304.55

408.46
364.97
367.33
313.87
308.56

561.07
489.78
495.29
440.80
439.82

595.22
495.13
524.25
423.34
456.10

617.59
545.11
582.87
441.54
483.78

TABLE II: Average Travel Time (ATT): Mixed Policy Dataset
Jinan
Category

Algorithm

Hangzhou

Manhattan

Low

Medium

High

Low

Medium

High

Low

Medium

High

Rule-based

Random
Fixed Time
Greedy
Max Pressure
SOTL

590.14
411.88
216.56
500.51
518.57

809.45
553.15
356.59
639.20
645.27

944.45
716.47
514.37
803.91
790.08

260.09
193.45
170.81
254.08
320.91

295.46
231.09
192.55
309.32
377.54

357.30
270.04
219.85
343.49
418.35

2899.23
615.94
512.27
758.94
607.80

3291.49
757.01
676.02
881.23
767.82

3590.03
884.03
830.96
991.15
886.84

Online

MAPPO

170.98

213.19

228.95

152.81

162.34

171.91

374.44

452.74

512.33

Offline

BC
TD3+BC
CQL
OffLight (TD3+BC)
OffLight (CQL)

417.35
372.31
369.72
312.88
314.86

554.59
509.34
477.32
423.88
402.17

689.67
586.35
566.41
493.12
553.09

233.26
193.78
227.52
169.07
189.50

269.77
240.24
240.24
222.17
219.46

308.28
273.07
275.83
222.61
240.54

971.47
874.53
864.52
806.83
798.09

1173.25
956.15
1023.94
944.64
979.13

1303.26
1129.03
1218.43
962.81
999.89

TABLE III: Queue Length (QL): Mixed Policy Dataset

nario under medium traffic demand. OffLight (TD3+BC) and
OffLight (CQL) achieve faster convergence and lower queue
lengths compared to baselines, with OffLight (TD3+BC)
performing best. TD3+BC and CQL improve over time but
maintain higher queue lengths than OffLight, while BC and
other baselines show greater variance and slower convergence.
OffLight’s IS mechanism ensures that transitions are
weighted based on their relevance to the learned policy,
reducing reliance on low-quality data. This re-weighting helps
OffLight make better traffic control decisions, resulting in
smoother traffic flow, quicker convergence, and significantly
lower queue lengths across all demand levels.
3) Behavior Representation: Figure 7 visualizes t-SNE
embeddings of learned latent space in the Hangzhou scenario,
illustrating OffLight’s ability to differentiate between expert,
random, and rule-based policies. The distinct clusters confirm
that GMM-VGAE effectively captures structural differences
in control policies, enabling more robust policy learning.
OffLight remains effective even as the proportion of expert
data decreases, consistently outperforming baselines. This
robustness is crucial in real-world settings where dataset
quality varies across intersections and traffic conditions,

Fig. 7: t-SNE Representation of Actual Data and Learned
Trajectories for Hangzhou scenario
ensuring reliable performance despite heterogeneous data
sources.
B. Ablation Studies
1) Performance Comparison with Different Levels of Mixing: In this study, we analyze the impact of mixing varying
proportions of data from expert and random policies on
the performance of the OffLight framework compared to

scenarios. RBPS, while useful, has a smaller impact when
suboptimal policies dominate certain episodes. The combined
approach enhances OffLight’s ability to learn from offline
datasets efficiently, particularly in large-scale networks.
C. Discussion

Fig. 8: Performance Comparison of OffLight with Expert and
Random Data Mix
conventional offline RL algorithms. Specifically, we explore
how OffLight performs under different dataset compositions:
33%, 50%, and 67% expert policy data, with the remainder
coming from random policies.
Figure 8 presents the performance comparison across
different datasets. As the proportion of expert data decreases
and random policy data increases, the performance of all
algorithms degrades, which is expected. However, OffLight
exhibits a more gradual performance decline compared to
TD3+BC and CQL. This indicates that OffLight’s IS and
RBPS mechanisms are effective in mitigating the negative
effects of suboptimal data.

This study introduced OffLight, an offline MARL framework for traffic signal control (TSC) using heterogeneous
datasets. OffLight consistently outperforms baselines, demonstrating robustness across different traffic conditions and
dataset compositions. By leveraging GMM-VGAE for behavior modeling and incorporating IS and RBPS, OffLight
effectively handles mixed-quality data, prioritizing highreward transitions and mitigating distributional shifts.
OffLight scales well across diverse network sizes, from
small to large-scale urban environments. However, its reliance
on GMM-VGAE introduces computational overhead, which
may limit real-time deployment. Additionally, its performance
depends on dataset quality, requiring a balanced mix of expert
and suboptimal policies for optimal learning.
Beyond TSC, OffLight’s framework is applicable to smart
grids [25] and supply chain management [26], where offline
MARL can optimize decision-making in networked systems.
Its IS-based policy correction and graph-based modeling
make it well-suited for improving coordination in distributed
environments.
VI. C ONCLUSION

Fig. 9: Ablation Study on CQL (top) and TD3+BC (bottom)
with IS and RBPS
2) Effectiveness of Improved Sampling Strategies: Figure
9 analyzes the impact of IS and RBPS on OffLight’s performance. IS improves performance more than RBPS across
all scenarios and traffic demand levels, with the performance
gap widening as congestion increases. In Manhattan’s highdemand setting, IS improves ATT by up to 10% for CQL and
9% for TD3+BC, demonstrating its effectiveness in mitigating
distributional shift. RBPS provides smaller, incremental
gains (4-6% ATT reduction), especially in medium and
high-demand settings. While RBPS prioritizes high-reward
episodes, it is more sensitive to noisy data and less effective
than IS in complex networks like Manhattan.
By re-weighting transitions based on their relevance to
the target policy, IS ensures that OffLight learns from highquality data, improving traffic flow decisions in congested

OffLight demonstrates the potential to significantly enhance
traffic signal control (TSC) in urban networks through offline
multi-agent reinforcement learning (MARL). By addressing
the key challenges of heterogeneous behavior policies and
distributional shifts, OffLight offers a robust solution capable
of learning effective policies from mixed-quality datasets.
The framework’s incorporation of GMM-VGAE for behavior
policy modeling, alongside IS and RBPS, leads to substantial
improvements in traffic efficiency. These advancements reflect
the ability of OffLight to scale effectively across different
network sizes and traffic conditions, making it a practical
tool for offline reinforcement learning for real-world traffic
management.
APPENDIX
A. Importance Sampling Weight Consistency
In OffLight, IS is integrated to correct for the distributional
shift between the behavior policy πb and the target policy
πθ . We aim to show that the importance sampling weights
computed using the estimated behavior policy π̂b provide
consistent and unbiased estimates of expectations under the
target policy. For any function f (τ ) of a trajectory τ , the
expectation under πθ can be estimated using samples from
πb as:
Eτ ∼πθ [f (τ )] = Eτ ∼πb [w(τ )f (τ )] ,
(10)
where the importance weight w(τ ) is given by:
w(τ ) =

T Y
N
Y
π i (ai | si )
θ

t

t

π̂ i (ai | sit )
t=0 n=1 b t

.

(11)

Assumptions:
1) 1. The estimated behavior policy π̂b satisfies π̂bi (ait |
sit ) = πbi (ait | sit ) for all n, t.
2) The support of πθi (a | s) is contained within the support
of πbi (a | s) for all n, i.e., πbi (a | s) = 0 =⇒ πθi (a |
s) = 0.
Proof: Under these assumptions, the importance weights
provide an unbiased estimate:
Z
Eτ ∼πb [w(τ )f (τ )] = πb (τ )w(τ )f (τ ) dτ
(12)
Zτ
πθ (τ )
= πb (τ )
f (τ ) dτ
(13)
πb (τ )
τ
Z
= πθ (τ )f (τ ) dτ
(14)
τ

= Eτ ∼πθ [f (τ )].

(15)

This demonstrates that OffLight’s IS weights yield consistent
estimates when the behavior policy is accurately estimated.
B. Variance Reduction via Accurate Behavior Policy Estimation
While IS provides unbiased estimates, the variance of
the estimator can be high, particularly when the target and
behavior policies differ significantly. By accurately estimating
πb using the GMM-VGAE, OffLight reduces the variance of
the importance weights. The variance of the IS estimator is
given by:
2

Varπb [w(τ )f (τ )] = Eπb [w2 (τ )f 2 (τ )] − (Eπθ [f (τ )]) . (16)
An accurate estimation of πb ensures that w(τ ) does not have
extremely large values, which would otherwise inflate the
variance. Specifically, when πθ is close to πb , w(τ ) is close
to 1, minimizing the variance.
Theorem: If π̂b is a consistent estimator of πb , then as the
amount of data increases, the variance of the IS estimator
decreases.
Proof: As π̂b converges to πb , the distribution of w(τ )
becomes tighter around 1, reducing the variance. This is
formalized by the Law of Large Numbers and convergence
properties of consistent estimators.
R EFERENCES
[1] A. Haydari and Y. Yılmaz, “Deep reinforcement learning for intelligent
transportation systems: A survey,” IEEE Transactions on Intelligent
Transportation Systems, vol. 23, no. 1, pp. 11–32, 2020.
[2] M. Noaeen, A. Naik, L. Goodman, J. Crebo, T. Abrar, Z. S. H. Abad,
A. L. Bazzan, and B. Far, “Reinforcement learning in urban network
traffic signal control: A systematic literature review,” Expert Systems
with Applications, vol. 199, p. 116830, 2022.
[3] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Offline reinforcement
learning: Tutorial, review, and perspectives on open problems,” arXiv
preprint arXiv:2005.01643, 2020.
[4] W. Zhan, B. Huang, A. Huang, N. Jiang, and J. Lee, “Offline reinforcement learning with realizability and single-policy concentrability,”
in Conference on Learning Theory, pp. 2730–2775, PMLR, 2022.
[5] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:
Datasets for deep data-driven reinforcement learning,” arXiv preprint
arXiv:2004.07219, 2020.
[6] S. Lange, T. Gabel, and M. Riedmiller, “Batch reinforcement learning,”
in Reinforcement learning: State-of-the-art, pp. 45–73, Springer, 2012.

[7] Y. Xiong, G. Zheng, K. Xu, and Z. Li, “Learning traffic signal control
from demonstrations,” in Proceedings of the 28th ACM international
conference on information and knowledge management, pp. 2289–2292,
2019.
[8] Y. Huo, Q. Tao, and J. Hu, “Cooperative control for multi-intersection
traffic signal based on deep reinforcement learning and imitation
learning,” Ieee Access, vol. 8, pp. 199573–199585, 2020.
[9] Q. Sun, R. Zha, L. Zhang, J. Zhou, Y. Mei, Z. Li, and H. Xiong,
“Crosslight: Offline-to-online reinforcement learning for cross-city
traffic signal control,” in Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pp. 2765–2774,
2024.
[10] R. Chen, F. Fang, and N. Sadeh, “The real deal: A review of challenges
and opportunities in moving reinforcement learning-based traffic signal
control systems towards reality,” arXiv preprint arXiv:2206.11996,
2022.
[11] H. Wei, N. Xu, H. Zhang, G. Zheng, X. Zang, C. Chen, W. Zhang,
Y. Zhu, K. Xu, and Z. Li, “Colight: Learning network-level cooperation
for traffic signal control,” in Proceedings of the 28th ACM international
conference on information and knowledge management, pp. 1913–1922,
2019.
[12] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q-learning
for offline reinforcement learning,” Advances in Neural Information
Processing Systems, vol. 33, pp. 1179–1191, 2020.
[13] Y. Wu, G. Tucker, and O. Nachum, “Behavior regularized offline
reinforcement learning,” arXiv preprint arXiv:1911.11361, 2019.
[14] Z.-W. Hong, A. Kumar, S. Karnik, A. Bhandwaldar, A. Srivastava,
J. Pajarinen, R. Laroche, A. Gupta, and P. Agrawal, “Beyond uniform
sampling: Offline reinforcement learning with imbalanced datasets,”
Advances in Neural Information Processing Systems, vol. 36, pp. 4985–
5009, 2023.
[15] Y. Yue, B. Kang, X. Ma, Q. Yang, G. Huang, S. Song, and S. Yan,
“Decoupled prioritized resampling for offline rl,” arXiv preprint
arXiv:2306.05412, 2023.
[16] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
POMDPs, vol. 1. Springer, 2016.
[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
nature, vol. 518, no. 7540, pp. 529–533, 2015.
[18] H. Wei, C. Chen, G. Zheng, K. Wu, V. Gayah, K. Xu, and Z. Li,
“Presslight: Learning max pressure control to coordinate traffic signals
in arterial network,” in Proceedings of the 25th ACM SIGKDD
international conference on knowledge discovery & data mining,
pp. 1290–1298, 2019.
[19] A. Oroojlooy, M. Nazari, D. Hajinezhad, and J. Silva, “Attendlight:
Universal attention-based reinforcement learning model for traffic signal
control,” Advances in Neural Information Processing Systems, vol. 33,
pp. 4079–4090, 2020.
[20] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” arXiv preprint arXiv:1710.10903,
2017.
[21] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine, “Stabilizing
off-policy q-learning via bootstrapping error reduction,” Advances in
neural information processing systems, vol. 32, 2019.
[22] S. Fujimoto and S. S. Gu, “A minimalist approach to offline reinforcement learning,” Advances in neural information processing systems,
vol. 34, pp. 20132–20145, 2021.
[23] S. V. Albrecht, F. Christianos, and L. Schäfer, Multi-Agent Reinforcement Learning: Foundations and Modern Approaches. MIT Press,
2024.
[24] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
“Counterfactual mlti-agent policy gradients,” in Proceedings of the
AAAI conference on artificial intelligence, vol. 32, 2018.
[25] D. Vamvakas, P. Michailidis, C. Korkas, and E. Kosmatopoulos,
“Review and evaluation of reinforcement learning frameworks on smart
grid applications,” Energies, vol. 16, no. 14, p. 5326, 2023.
[26] M. Mousa, D. van de Berg, N. Kotecha, E. A. del Rio Chanona,
and M. Mowbray, “An analysis of multi-agent reinforcement learning
for decentralized inventory control systems,” Computers & Chemical
Engineering, vol. 188, p. 108783, 2024.

