1

3DPG: Distributed Deep Deterministic Policy
Gradient Algorithms for Networked
Multi-Agent Systems
arXiv:2201.00570v2 [cs.LG] 2 Nov 2022

Adrian Redder, Arunselvan Ramaswamy, and Holger Karl

Abstract
We present Distributed Deep Deterministic Policy Gradient (3DPG), a multi-agent actor-critic
(MAAC) algorithm for Markov games. Unlike previous MAAC algorithms, 3DPG is fully distributed
during both training and deployment. 3DPG agents calculate local policy gradients based on the most
recently available local data (states, actions) and local policies of other agents. During training, this
information is exchanged using a potentially lossy and delaying communication network. The network
therefore induces Age of Information (AoI) for data and policies. We prove the asymptotic convergence
of 3DPG even in the presence of potentially unbounded Age of Information (AoI). This provides
an important step towards practical online and distributed multi-agent learning since 3DPG does not
assume information to be available deterministically. We analyze 3DPG in the presence of policy and data
transfer under mild practical assumptions. Our analysis shows that 3DPG agents converge to a local Nash
equilibrium of Markov games in terms of utility functions expressed as the expected value of the agents
local approximate action-value functions (Q-functions). The expectations of the local Q-functions are
with respect to limiting distributions over the global state-action space shaped by the agents’ accumulated
local experiences. Our results also shed light on the policies obtained by general MAAC algorithms.
We show through a heuristic argument and numerical experiments that 3DPG improves convergence
over previous MAAC algorithms that use old actions instead of old policies during training. Further,
we show that 3DPG is robust to AoI; it learns competitive policies even with large AoI and low data
availability.

A. Redder was supported by the German Research Foundation (DFG) - 315248657 and SFB 901.
A. Redder is with the Computer Science Department, Paderborn University (e-mail: aredder@mail.upb.de).
A. Ramaswamy is with the Computer Science Department, Karlstad University (e-mail: arunselvan.ramaswamy@kau.se).
H. Karl is with the Hasso-Plattner-Institute, University of Potsdam (e-mail: holger.karl@hpi.de).

2

Index Terms
Actor-Critic Algorithms, Age of Information, Asymptotic Convergence, Deep Multi-Agent Reinforcement Learning, Distributed Online Learning, Networked Systems

I. I NTRODUCTION
Multi-Agent Actor-Critic (MAAC) algorithms are an important and popular class of Deep
Reinforcement Learning (DeepRL) algorithms for intelligent decision making in multi-agent
systems (MAS) [1]. MAAC algorithms, like the popular Multi-Agent Deep Deterministic Policy
Gradient (MADDPG) algorithm [2], typically assume instant access to global data in order to
train coordinated decentralized policies: to train MADDPG, agents need instant access to all
agent polices, their action sequences, and global state information. This training paradigm is
called centralized training with decentralized execution.
This central perspective can be justified only when the training involves an accurate simulator of the environment or when the agents are connected by communication network without
transmission errors and delays. These assumption are often impractical. For example, in edge
computational task offloading [3], observations and decisions are inherently local and not available globally to all edge nodes. Algorithms such as MADDPG are therefore unsuitable for such
truly distributed online multi-agent learning problems. Other works, such as [4], propose an
algorithm for fully decentralized multi-agent learning. Here, it is assumed that the state and
action spaces of the agents are finite, that the global state space can be observed by all agents,
and that the resulting Markov chain is ergodic for each potential policy. These assumptions will
be hard to verify in practice. Instead, we propose an algorithm that only uses local information
and we show explicitly that the limiting policies found by our algorithm result in an ergodic
Markov chain.
The assumptions in the literature regarding centralized training and global data access stem
from the need for coordinated learning among multiple agents. To provide a distributed online
learning algorithm based on MADDPG, all local states, actions, and policies must be made
available to all agents in a synchronized and lossless manner, across an imperfect network.
That is, in most cases, prohibitively expensive. Our work addresses this issue and presents the
Distributed Deep Deterministic Policy Gradient (3DPG) algorithm, a natural distributed extension
of the single agent DDPG algorithm [5] that enables coordinated but fully distributed online

3

multi-agent learning. 3DPG learns distributed but coordinated policies by using global data
during training that has been collected by agents using communication over an available network.
Notably, the agents only use local data to take decisions. In other words, individual actions are
not globally coordinated, but the agents joint average behavior is coordinated.
3DPG only requires that the agents can exchange information (local states, actions and policies)
in an imperfect manner: it might be lost, or experience significant delay, causing data to have
random age once it arrives – this is commonly described by so-called Age of Information
(AoI) random variables [6]. We analyze 3DPG under practical sufficient conditions, in particular
very weak communication assumptions for the MAS. Our modest communication assumptions
even allow for potentially unbounded asymptotic growth of the AoI and make no deterministic
requirements regarding data availability.
To guarantee convergence of 3DPG, we address another problem. Despite their popularity and
usefulness in many practical scenarios, the conditions under which AC and MAAC algorithms
converge are not well studied – we address this gap and present practically verifiable and
sufficient conditions for DDPG, 3DPG and MADDPG to converge asymptotically. Our result
is based on recent progress in understanding the convergence behavior of Deep Q-Learning
[7]. Such convergence guarantees and analyses are in general difficult, even for traditional
single-agent DeepRL algorithms. In the single-agent case, RL algorithms with linear function
approximations are well studied, but algorithms that use non-linear function approximators
like Deep Neural Networks (DNNs) are not well understood. At best, convergence is only
characterized under strict assumptions that are difficult to verify in practice, e.g., [8] assumes that
the state transition kernel of the Markov decision process is regular; this questions the practical
usefulness of such algorithms. The behavior of multi-agent DeepRL algorithms is even more
challenging since the various agents’ training processes are intertwined. It is thus pertinent, both
from a theoretical and from a practical standpoint, to analyze, under practical assumptions, the
asymptotic properties of multi-agent DeepRL algorithms.
A. The 3DPG Algorithm
To get a first idea (Section III provides details), let us view 3DPG as a multi-agent version
of DDPG [5], the most popular AC DeepRL algorithm. DDPG involves two DNNs, an actor
(policy) network and a critic (Q-function) network. The actor network is trained to approximate

4

the optimal policy, and the critic network is trained to approximate an objective function. More
specifically, the critic network is trained to minimize a variant of the squared Bellman error,
while the actor network is trained to pick actions that maximize the approximation of the
optimal Q-function, as found by the critic. Notably, both the critic and actor networks are trained
simultaneously.
In 3DPG, each agent only has access to a locally observable state (a part of the global state),
can exchange information with other agents in an imperfect manner, and takes actions that affect
both its local state and states observable by other agents. To take actions, each agent uses a local
actor/policy. To train its local policy, each agent uses a local critic/Q-function approximation.
The local policies are functions of the local agent states, which in turn constitute the global
(multi-agent) system state. At every discrete time step, after all agents take actions, they obtain
local rewards. Each agent then uses a local copy of DDPG (as explained above) to train a local
actor, while simultaneously training a local critic with respect to (w.r.t.) the global decision
making problem associated with its local reward structure.
To perform the actor training step, the agents use local policies from other agents transferred
via a potentially imperfect communication network that, e.g., causes delays. At each agent,
the training step may be viewed as calculating a local policy gradient using the best available
approximation of the current global policy. The 3DPG architecture at agent 1 of a D agent system
is illustrated in Figure 1. In addition to the old policies of other agents, all local actor and critic
training steps use data (states, actions) of other agents transferred via the communication network.
This gives a quasi-centralized view but based on information with potentially large age; this view
facilitates online training and execution of 3DPG.
The aforementioned actor training step is in stark contrast to the corresponding one in MADDPG from [2]. Here the actor training step is based on past actions of other agents. Since
these actions are sampled from old transitions, they sometimes include random actions due
to exploration. Theses actions, however, do not represent actual agent behavior and can thus
negatively impact the policies found by the algorithm (see Sections III-A and VIII for details).
Using old policies of other agents, as in 3DPG, may initially increase the training variance.
However, we show that for 3DPG the effect of unrepresentative behavior from old policies as
well as the effect of potentially random actions does not effect the policies found by 3DPG. For
MADDPG, we show that the presence of randomly explored actions can have a negative impact

5
Global state sample
s

s1

s2

s

Fig. 1.

D

π

1

· ; φ1n

Backpropagation
Gradient Flow

 Local
Actor



π 2 · ; φ2n−τ21 (n)


π D · ; φD
n−τD1 (n)

Q1 · , · ; θn1



Q1 s, a; θn1

Local Critic



Approximate Global
Policy at Agent 1

Illustration of the 3DPG architecture at agent 1. The local critic is evaluated for action a = π(s; φτ 1 (n) ) of the local

approximation of the global policy. See Section III for details.

on the training result of MADDPG. This result is supported by numerical examples that show
that 3DPG obtains better policies compared to MADDPG for problems that require coordinated
decision making.
3DPG converges even when the AoI associated with the used information of other agents
during training merely has finite mean. In other words, the AoI may vary with infinite variance
and may grow unbounded. The convergence in the presence of such potentially significantly
outdated information (the local states, actions and policies from other agents) is achieved by
agents using diminishing step-size sequences, ensuring that after some time the change of their
local policies does not grow significantly larger than the step size.
The asymptotic properties of 3DPG are the following:
1) All local critics converge to a set of DNN weights such that the local Bellman-error
gradients are zero on average w.r.t. to limiting distributions over the global state-action
space.
2) All local actors converge to a set of DNN weights such that the local policy gradients are
also zero w.r.t. the same limiting distributions.
The aforementioned limiting distributions are shaped the agents’ accumulated local experiences.
More specifically, the global data tuples available to agents through communication, which are
then used in the training steps for critics and actors, form local limiting distributions such that
all critics and actors converge jointly in expectation to stationary points with respect to these
limiting distributions over the global state action space. In that regard, no agent can improve
its performance locally w.r.t. its limiting distribution of the training process and the limiting

6

policies of the other agents. Specifically, we show that 3DPG agents converge to a local Nash
equilibrium of Markov games. Notably, this is achieved although every agent may have a different
local reward structure, i.e. irrespective of whether they cooperate or compete.

B. Main contributions and Paper Summary
•

We present 3DPG, an online, fully distributed MAAC learning algorithm for networked
MAS with continuous decision spaces.

•

We present an asymptotic convergence analysis for 3DPG under practical conditions. These
include novel communication assumptions that formalize how old information used by
3DPG agents are allowed to be.

•

We show that 3DPG converges to a local Nash equilibrium of a Markov Game.

•

Our convergence analysis, provides the first characterization of the policies found by Deep
AC and Deep MAAC algorithms under assumptions that represent how these DeepRL
algorithms are used in practice.

•

Our analysis reveals the difference between using policies of other agents (3DPG) compared
to using actions of other agents (MADDPG) in MAAC algorithms: in the presence of
exploration MAAC algorithms that learn from actions take into account that other agents
may act randomly even though they do not actually do so; 3DPG does not have this negative
property.

To study the convergence of 3DPG, we use recent asymptotic analyses of Deep Q-Learning
under practical and mild assumptions [7]. Since DDPG may be as a viewed special case of
3DPG involving a single agent, our analysis can be directly used to understand DDPG.
Paper organization. In Section II, we define the Markov game setting considered in this
work. In Section III, we describe the 3DPG Algorithm and the required assumptions to prove
its convergence. Sections V and VII present our main convergence analysis. Afterwards, we
analyze the difference between the 3DPG and MADDPG algorithm in Section VIII, which we
support with numerical experiments section IX. We close with conclusions and future work in
Section X.

7

II. M ARKOV G AMES
In multi-agent systems, the global state of the environment is typically the concatenation of the
agents’ local states. However, the global state is usually unobservable by any agent. The global
state transitions to a new state after each agent has taken its local action. After the global state
transition, the agents receive local feedback/reward signals. The structure of the local reward
signals depend on the nature of interaction between the agents: do they cooperate or compete?
In the following, we always use superscripts i for the agent index in the multi-agent system,
and we use subscripts n as discrete time steps. We assume that the multi-agent system under
consideration can be modeled as a D-agent Markov game [9], which is formally defined as the
4-tuple (S, A, p, {ri | 1 ≤ i ≤ D}) , where:
Q
i
i
ki
the local state space of agent i with
S = D
i=1 S is the global state space, with S := R
k i > 0.
Q
i
1
D
A = D
i=1 A is the global action space, where an action a = (a , . . . , a ) ∈ A denotes the
i

joint action as a concatenation of local actions ai ∈ Ai ⊆ Rd , di > 0.

p is the Markov transition kernel, i.e. p(· | s, a) is the distribution of the successor state of
state s after action a is executed.
ri is the local scalar reward function associated with agent i. Specifically, ri (s, a) is the local
reward that agent i observes when the system is in state s and the global action a is taken.
In many cooperative Markov games, the local reward functions coincide, i.e., ri ≡ r for 1 ≤
i ≤ D. Such models are called factored decentralized MDPs [10].
Consider a D-agent Markov game as defined in above. The D agents interact with the
environment at discrete time steps n ∈ N. At every time step n, agent i observes a local
i

state sin ∈ S i , based upon which it must take a local (continuous) action ain ∈ Ai ⊆ Rd , for
which it receives a reward rni .

Suppose that the local behavior of agent i is defined by a local DNN policy π i (si ; φi ),
parameterized by a vector φi . Define the associated global policy as
π := (π 1 , . . . , π D ).

(1)

For each local reward function ri , the return starting from time step 1 is defined by Ri :=
P∞ n−1 i
r (sn , an ) with discount factor 0 < γ < 1. Given a global policy π, the associated
n=1 γ
action-value function Qi of agent i is given by Qi (s, a) := Eπ [Ri | s1 = s, a1 = a]. For each local

8

reward function ri , the associated optimal policy is characterized by the optimal action-value
function Qi∗ (s, a), which is defined as a solution to Bellman’s equation [11]:


i
i 0 0
i
Q∗ (s , a ) | s, a
Q∗ (s, a) = Es0 ,ri r (s, a) + γ max
0
a ∈A

(2)

The problem is to find local policy parametrizations φi∗ for each local policy π i (si ; φi ), such that
for every agent i:
π i (si ; φi∗ ) ≈ argmax Qi∗ (s, a) πj6=i
ai ∈A

∀s ∈ S.

(3)

In other words, all local policies should act optimally conditioned on the local policies obtained
by all other agents.
III. D ISTRIBUTED D EEP D ETERMINISTIC P OLICY G RADIENT A LGORITHM (3DPG)
In this section, we define the 3DPG algorithm.
A. Multi-Agent Actor-Critic Gradients
Suppose each agent i uses a local DNN approximator Qi (s, a; θi ) for its local critic; θi
represents the associated vector of network weights. The local critic is trained using the Deep
Q-Learning algorithm [12] to find θ∗i such that Qi (s, a; θ∗i ) ≈ Qi∗ (s, a) for all state-action pairs

(s, a). As mentioned before, each local actor/policy π i (si ; φi ) is parameterized by φi . The goal

is to train the local critics and local actors jointly such that (3) holds. This is challenging due
to the potential conflicting rewards of the agents.
Consider that at some time step n, the local critic and actor parametrizations are θni and φin .
Further, suppose agent i gets access to a global data tuple
tim := (sm , am , ri (sm , am ), sm+1 )

(4)

from some transition from time m to m + 1 with m  n. The availability of at least some
global tuples must be ensured by coordinated communication between the agents. This will be
discussed further in the following subsections. Now, if agent i had access to the parametrizations
of the other agents, then it could calculate a local critic gradient

∇θi li (θni , φn , tim ) := ∇θi Qi (sm , am ; θni ) ri (sm , am )+

γQi (sm+1 , π(sm+1 ; φn ); θni ) − Qi (sm , am ; θni ) .

(5)

9

Equation (5) is a sample gradient of the local squared Bellman error of Qi for the observed
global tuple tm , which follows from the associated error in Bellmans equation (2).
Now, there two possible ways to formulate a “natural” distributed version of the policy gradient
in the DDPG algorithm [5]. The first one is the local policy gradient
i
∇φi gMADDPG
(θni , φin , sm , aj6m=i ) := ∇φi π i (sim ; φin )

(6)

i

i
∇ai Q (sm , a1m , . . . , π i (sim ; φin ), . . . , aD
m ; θn ),

which is used in the MADDPG algorithm of [2]. The second one is the local policy gradient
i
∇φi g3DPG
(θni , φn , sm ) := ∇φi π i (sim ; φin )

(7)

∇ai Qi (sm , π(sm ; φn )); θni ),
which will be used in our 3DPG algorithm. The MADDPG local policy gradient uses the actions
aj6m=i from the other agents from the global tuple tim , while the local policy gradients in 3DPG
use the policies φnj6=i from the other agents. In the following, we use ∇φi g i for (7) to simplify
the notation.
Remark 1. The subtle difference between (6) and (7) will be analyzed extensively in Section VIII.
We discuss a heuristic argument based on our asymptotic convergence analysis of MADDPG and
3DPG that shows that a multi-agent actor-critic algorithm based on (7) has a higher probability
of obtaining a better policy faster compared to a multi-agent actor-critic algorithm based on
(6). This is because (7) takes precisely into account how other agents would behave in certain
sampled states. Equation (6), on the other hand, also considers the sampled actions that may
arise from randomly explored actions. Our numerical experiment in Section IX supports this
theoretical prediction.
Remark 2. The local policy gradient (7) seems to have a more direct motivation from the
deterministic policy gradient theorem (DPGT) [13] in comparison to (6). The DDPG alogrithm
was inspired by taking samples from the DPGT1 . Similarly, the gradient (7) can be motivated
by inserting the global product policy (1) into the DPGT. Thus Equation (7) is in essence the
policy gradient from the DDPG algorithm [5], where the policy is defined in product form (1).
1

Be aware that the sample gradients used in the DDPG algorithm are not true sample gradients from the deterministic policy

gradient [14]

10

The idea behind 3DPG is to approximate (5) and (7) using old information from other agents
to train Qi and π i locally. To implement this, the agents require:
1) Local access to global data tuples tim ,
2) Local access to the global policy π(s; φm ),
for m  n “frequently” (the precise network assumptions are presented in section III-D). Recall
that in MADDPG the above information are required for all agents at every time step. These are
reasonable assumptions for simulated environments or under the paradigm of centralized training
with decentralized execution, but not for online fully distributed learning.
B. Approximate Global Policy induced by local AoI
We now decentralize the implementation of (5) and (7) using communication. Most notably,
we use communicated but potentially aged local policies as an approximation for the true global
policy φn .
Suppose that the D-agent Markov game is networked, such that the agents can exchange data
by communication. We suggest a communication paradigm where agents cooperatively forward
local data to other agents, such that local policies and local data (states and actions) can flow via
the network to all other agents. To guarantee this, the agents must use some forwarding protocol
[15] to forward old policies φin between the agents as well as coordinated communication protocol
to ensure that at least some global tuples (4) reach each agent “frequently”. The coordinated
communication protocol may be some broadcast protocol coupled with a central coordinator,
or it could also be a distributed snapshot protocol [16], which, however, would cost more
communication resources. For now, we suppose that the agents run suitable protocols of this
kind. Specifically, protocols that guarantee that our network assumptions (A1), as to be defined
in the next section, are satisfied.
Let us now suppose that each agent runs a local algorithm to train its policy and thereby
generates a sequence φin of associated policy parametrizations. Equipped with the ability to
transfer data via the available network, the agents exchange the local parametrization φin as
well as local tuples tin := (sin , ain , sin+1 ) using the communication network that possibly delays
or looses data for extended periods of time. Hence, agent i has only access to φjn−τij (n) for
every agent j 6= i at every time step n. Here, φjn−τij (n) denotes the latest available policy
parametrization from agent j at agent i at time n and we refer to τij (n) as the associated Age of

11

Information (AoI) random variable as a consequence of the potentially uncertain and delaying
communication.2 For every agent i, we can then define a global policy parametrization associated
with the aged information at time n by
φτ i (n) := (φ1n−τi1 (n) , . . . , φD
n−τD1 (n) ).

(8)

This global policy will serve as an approximation to the true global policy φn .
C. The 3DPG Algorithm
As discussed in the previous section, we suppose that the D-agent multi-agent system uses
an available network to exchange their local policy parametrizations φin and their local tuples
(sin , ain , sin+1 ). We can now state the 3DPG iteration.
Suppose that every agent i maintains a local replay memory Rin . At every time step n, the

memory can contain up to N old global transitions tim . At time step n, agent i samples a random
minibatch of M < N transitions from its replay memory. Agent i then updates its actor and
critic using step-size sequences α(n) and β(n) as follows:
1 X
i
θn+1
= θni + α(n)
∇θi li (θni , φτ i (n) , tim ),
M m
1 X
i
φin+1 = φin + β(n)
∇φi g i (θm
, φτ i (n) , sm )
M m

(9)

Notice that for a single sample tim , the gradients used in (9) are the gradients (5) and (7)
where the global policy φn has been replaced by the local approximation of the global policy
φτ i (n) induced by the aged parametrization (8). The resulting training architecture is presented
in Figure 1. Pseudocode for the algorithm is presented in Section B. We will now present our
assumptions to prove the convergence of (9). We begin with the required network assumptions.
D. Network Assumptions
The communication network needs to ensure two things. First, it needs to ensure that every
agent i receives the policy parametrizations φjn for all j 6= i “sufficiently” often. Second, it needs
to ensure that the available samples in the replay memories Rin are not too old. To capture the
2

For background on AoI we refer the reader to [17], where the effect of AoI was consider in an offline distributed optimization

setting.

12

age of the samples in the replay memories, define another AoI random variable ∆i (n) as the
age of the oldest sample in the replay memory Rin of agent i at time n ≥ 0.
Definition 1. A non-negative integer-valued random variable X is said to be stochastically

dominated by a random variable X if P (X > m) ≤ P X > m for all m ≥ 0.

(A1) (a) Policy communication assumptions:

There exists a non-negative integer-valued random variable τ that stochastically dominates
all τij (n) for all n ≥ 0 with E [τ q1 ] < ∞ for some q1 ≥ 1.
(A1) (b) Data communication assumptions:
There exists a non-negative integer-valued random variable ∆ that stochastically dominates
 q2 
all ∆i (n) for all n ≥ 0 with E ∆ < ∞ for some q2 ≥ 1.

(A1)(a) and (A2)(b) require that the tail distributions of the AoI variables τij (n) and ∆i (n)
decay uniformly, such that at least a dominating random variable with finite mean exists. This

ensures that the growth of each AoI variable cannot exceed any fraction of n after some
potentially large time step. We prove this in Lemma 2 and show in Lemma 3 that the use
of the approximate global policies φτ i (n) in (9) does not cause gradient errors asymptotically.
Finally, (A2)(b) is used in Lemma 7 to show that the agents experiences converge to a stationary
distribution. Here (A2)(b) ensures that the agents receive enough global tuples asymptotically to
“track” the Markov game state distribution.
Remark 3. (A1)(b) does not specify when exactly the global samples become available to each
agent. Further, the received data tuples do not have to be from the same time steps m for every
agent.

E. Algorithm and Markov Game Assumptions
In addition to the network assumptions, we require:
(A2) (a) The critic step size sequence α(n) is positive, monotonically decreasing and satisfies:
X
n≥0

α(n) = ∞ and

X
n≥0

α2 (n) < ∞.

13

(b) The actor step size sequence β(n) satisfies:
1

β(n) ∈ O(n− q ) and

b(n)
= 1,
n→∞ a(n)
lim

for q ∈ [1, 2) with q ≤ min(q1 , q2 ) for q1 , q2 from (A1).
(A3) (a) supn≥0 kθn k< ∞ a.s. and supn≥0 kφn k< ∞ a.s.
(b) supn≥0 ksn k< ∞ a.s. and the action space A is compact.
(A4) The state transition kernel p(· | s, a) is continuous.

(A5) The actor policies π i and the critics Qi are fully connected feedforward neural networks
with twice continuously differentiable activation functions such as Gaussian Error Linear
Units (GELUs).
(A6) The reward functions ri : S × A → R are continuous.
Assumption (A3)(a) is the strongest assumption as it requires almost sure stability of the
algorithm. We devote the next subsection to its discussion. The compactness of the action space
in (A3)(b) will usually be satisfied in many applications, e.g. in robotics.
β(n)
→ 1.
In (A2)(b) we require that the critic and actor step-size sequences are chosen such the α(n)

This is not a traditional assumption for actor-critc algorithms [18]. We will present a proof based
on a single-timescale analysis of (9) w.r.t. the timescale of the critic iterations. In practice, we
want the critic to converge faster so we would initially choose α(n) larger than β(n). (A2)(b)
requires that afterwards the iterations asymptotically take steps of the same size. The more
complex analysis using a two-timescale step-size schedule will be presented in an upcoming
paper.
In (A5), we require twice continuously differentiability of the activations used by the policy
and actor networks. GELUs are well-known examples that satisfy this property [19]. Additionally, GELUs are one of the well-known neural network activation functions with similar
high performance across different tasks compared to other well-known activations like ELUs or
LeakyReLUs [20].
F. Discussion of Assumption (A3)(a) and Related Work
Ideally, when one is dealing with a specific algorithm, it should be guaranteed or proven –
rather than assumed up-front – that the algorithm iterations are stable. Assuming stability is,
nonetheless, a typical first step towards understanding the convergence behavior of optimization

14

algorithms. Especially in deep RL, stability of algorithms like Deep Q-Learning or DDPG are
not well understood. Most notably, there is a significant gap between the assumptions made
in theory compared to assumptions verifiable in practice. Lets review some results on MAAC
learning.
In [4] the authors use linear function approximation and assume that the MA learning problem
can be described by finite state ergodic Markov process. They further assume assume the
existence of projection operator with knowledge of a compact set that includes a local minima of
the objective. [21] provides very interesting rate of convergence results for AC methods. However,
they assume that samples (sn , an , rn , sn+1 ) are drawn from a known stationary distribution of the
state Markov process. We instead show that our AC iterates converge such that the experience of
the agents give rise to stationary distributions of the state Markov process. In addition, knowledge
of the bias of the policy gradient and the bias of the critic estimates is required in [21], while
the critic should again be a linear combination of features. That work also assumes that the
policy gradient is Lipschitz continuous, which would require (A3)(a), since most DNNs are only
locally Lipschitz.
The assumptions made in the above works will be very hard to verify for most data-driven
applications in practice. Even worse, we fear that guaranteeing stability for practical data-driven
RL problems may always require assumptions that are not easily verifiable in practice. However,
a practitioner may not even be highly interested in stability. Usually, practitioners will design
their DNN parameterizations and their hyperparameter configurations using their experience,
such that they roughly observe stable behavior. Afterwards, practitioners would like to know
what limit they can expect from their algorithm. This is where our work comes into play.
In contrast to the assumptions made in the literature, our assumptions, except (A3)(a), are
very week, easily verifiable in practice and represent well how users apply DQN, DDPG and
its variants in practice. For this setting, our work answers to where one can expect the 3DPG
iterations (9) to converge asymptotically. Specifically, our analysis gives a comprehensive characterization of the found limit using a limiting distributions of the state-action process. These
limiting distributions are shown to be stationary distributions of state Markov process and are
shaped by the experience of the agents. We now present our convergence result.

15

IV. M AIN R ESULTS
i
Theorem 1. Under (A1)-(A6), the 3DPG iterations (9) converge to θ∞
and φi∞ , such that

∇θ i
∇ φi

Z

i
, φ∞ , s, a)µi∞ (ds, da)
l (θ∞
i

S×A

Z

S

i
, φ∞ , s)µi∞ (ds, A)
g (θ∞
i





= 0,
(10)

= 0,

where µi∞ is a limiting distribution of a continuous time measure process (defined in Section VII)
that captures the experience of agent i sampled from its local experience replay Rin during
training. Further, all µi∞ are stationary distributions of the state Markov process:
µi∞ (dy × A) =

Z

S

p(dy | x, π(x; φ∞ ))µi∞ (dx × A).

(11)

Theorem 1 shows that the critic iterations of 3DPG converge to stationary points of the
average local squared Bellmann errors. Further, the actor iterations converge to stationary points
of the average local deterministic policy gradients. For both limits the averaging is w.r.t. to the
stationary distributions of the state Markov process that capture the experienced samples of the
agents.
Since stochastic gradient descent schemes tend to avoid unstable equlibria [22], [23], [24],
we can expect that the aforementioned stationary points are local minima with high probability.
This can be made more precise using an avoidance of traps analysis [25, Ch. 4]. This shows
that 3DPG converges to local solution of the objective (3) with high probability. In other words,
given their local reward structure the agents converge to an equilibrium where they have locally
no desire to change their policies given their local experience and the final policies of the other
agents. More precisely, it follows that the agents converge to a local Nash equilibrium w.r.t. to
their locally approximated action-value functions.
i
Abbreviate the final local policies as π∞
(si ) := π i (si ; φi∞ ). For any open set U with 0 ∈ U in

i
the parameter space of π∞
(si ), define

Πi∞ (U) := {π i ( · ; φi ) : φi ∈ φi∞ + U}.
i
In other words, Πi∞ (U) is the set of policies in the U neighborhood of π∞
(s).

(12)

16

Corollary 1. Suppose the stationary points from Theorem 1 are local minima, then there are
open sets U i with 0 ∈ U i , such that
Z
1
i
D
i
Qi (s, π∞
(s), . . . , π∞
(s), . . . , π∞
(s); θ∞
)µi∞ (s, A)
S
Z
1
D
i
Qi (s, π∞
(s), . . . , π i (s), . . . , π∞
(s); θ∞
)µi∞ (s, A)
≥

(13)

S

for all π i ∈ Πi∞ (U i ).
Corollary 1 shows that the local policies converge to a local approximate Nash equilibrium
w.r.t. the experience gathered by each agent locally. The experience is again represented by the
limiting distributions µi∞ . Specifically, the local policies converge to a local Nash equilibrium
R
i
)µi∞ (s, A). In game theoretic terms,
w.r.t. the local expected action-value functions S Qi (s, · ; θ∞

these are the payoff (or utility) functions w.r.t. which the agents converge to a local Nash
equilibrium.
Remark 4. Corollary 1 holds when the local policies π i are linear functions of pretrained nonlinear features. This is common in the literature as e.g. used in the discussed references [4] and
[21]. The significance of Corollary 1 is that the agents converge to a local approximate Nash
equilibrium without assuming that the samples used in training are from a known stationary
distribution of the state Markov process. We instead show that the experience of the agents
give rise to stationary distributions of the state Markov process. This is important, as deep RL
algorithms are typically employed in complex environments with multiple stationary distributions

V. P RELIMINARIES AND AGE OF I NFORMATION A NALYSIS
In the following two sections, we prove Theorem 1. The proof builds on the analysis of single
agent deep Q-learning presented in [7]. To simplify the presentation, we will assume from
now on that the state space S is compact. All results can be generalized to d-dimensional real
spaces under the almost sure boundedness condition in (A3)(b), for which we refer to techniques
presented in [7, Section IV.A.2].
At its core, we will now present a convergence proof for the DDPG algorithm [5], using
a single timescale analysis. We begin with preliminary reductions and the analysis of the AoI
processes τij (n).

17

A. Reduction to mini batches of size 1
First, we make a simplifying reduction. We consider that the agents have ready access to
the global tuples (sn , an , ri (sn , an ), sn+1 ) during runtime and that merely the local policies φin
are communicated via the communication network. Further, we merely consider that the agents
use the global tuple from time n to update its critic and actor network. We therefore simplify
iteration (9) to:
i
θn+1
= θni + α(n)∇θi l(θni , φτ i (n) , tin ),

(14)

φin+1 = φn + β(n)∇φi g(θni , φτ i (n) , sn ).
In Section VII, we will extend our analysis to the setting presented in Section III.

B. Reduction to zero AoI
As the second step, we define the gradient errors that occur since we use the aged global
policies φτ i (n) instead of the true global policy:
i

eθn := ∇θi l(θni , φn , tn ) − ∇θi l(θni , φτ i (n) , tn )
i
eφn := ∇φi g(θni , φn , sn ) − ∇φi g(θni , φτ i (n) , sn )

(15)

Hence, (14) can be written as
i
θn+1
= θni + α(n)



i
∇θi l(θni , φn , tn ) + eθn



,


i
φin+1 = φn + β(n) ∇φi g(θni , φn , sn ) + eφn .

(16)

C. Reduction to marginalized critic gradient
As the third step, we rewrite the critic iterations in (16) further by integrating over the the
successor state sn+1 in tn given state sn . The resulting new loss gradient is ∇θi ˆli (θi , φ, s, a) :=
Z

i
r (s, a) + γ Qi (s0 , π(s0 ; φ); θi )p(ds0 | s, φ)
(17)

i
i
i
i
− Q (s, a; θ ) ∇θi Q (s, a; θ ),
With a slight abuse of notation, we use p(ds | sn , φn ) instead of p(ds | sn , φn (sn )) to highlight

the dependency of the action an on the policy φn and potential additional random noise for
exploration.

18

Define the induced error from using ∇θi ˆli instead of ∇θi li as ψni := ∇θi ˆli − ∇θi li , we can
then rewrite (16) as

i



i
i
θn+1
= θni + α(n) ∇θi ˆli (θni , φn , sn , an ) + ψni + eθn ,


i
φin+1 = φn + β(n) ∇φi g(θni , φn , sn ) + eφn .

(18)

i

D. eθn , eφn and ψni vanish asymptotically
In summary, we have rewritten (14) using:
i

i

1) The errors eθn and eφn induced by not considering the AoI random variables τij (n),
2) The errors ψni induced by marginalizing out the successor states sn+1 .
We will now show that these errors vanish asymptotically. For this, we first present properties
of the loss gradients ∇φi g i , ∇θi li and ∇θi ˆli .
Lemma 1. (i) ∇θi li (θni , φn , tn ) and ∇θi ˆli (θni , φn , sn , an ) are continuous and locally Lipschitz
continuous in the θi and φ-coordinate.

(ii) ∇φi g(θni , φn , sn , an ) is locally Lipschitz continuous in every coordinate.
Proof: See Appendix A.
i

i

We can now show that the errors eθn and eφn due to AoI vanish asymptotically. For this we
need a technical lemma.
Lemma 2. Under (A1)(a) it follows that for every ε ∈ (0, 1) and all agent pairs (i, j),
∞


X
1
1
P τij (n) > εn q1 ≤ q1 E [τ q1 ] < ∞.
ε
n=0

(19)

with τ from (A1)(a).

Proof: See Appendix A.
1

The lemma shows that the AoI processes τij (n) do not exceed any fraction of n q1 asymptoti

1
cally. More precisely, it now follows from the Borel-Cantelli Lemma that P τij (n) > εn q1 i.o. =
0. Hence, there is sample path dependent N (ε) ∈ N, such that
1

τij (n) ≤ εn q1

∀ n ≥ N (ε).

The following Lemma shows that the gradient errors vanish asymptotically.

(20)

19
i

i

Lemma 3. lim keθn k= 0 and lim keφn k= 0.
n→∞

n→∞

Proof: From Lemma 1 we have that ∇ˆli is locally Lipschitz. It follows from (A3)(a) that

∇ˆli is Lipschitz continuous with constant L when restricted to a sample path dependent compact
set. Using the triangular inequality, the established Lipschitz continuity of ∇ˆli and (A3)(a), it
follows that
i
keθn k ≤ L

≤C

X

n−1
X

kφjm+1 − φjm k

j6=i m=n−τij (n)

X

n−1
X

(21)
β(m),

j6=i m=n−τij (n)

for a sample path dependent constant C > 0. We will now show that


n−1
X
lim 
β(m) = 0,

(22)

n→0

m=n−τij (n)

i

which thus implies that lim keθn k= 0.
n→∞

1

By (A2)(b), we assume that β(n) ∈ O(n− q ) with q ≤ q1 Hence, there are constants c > 0

and N ∈ N, such that

−1

β(n) ≤ cn q1 for all n ≥ N.

(23)

Fix ε ∈ (0, 1). Equations (20) and (23) show that
n−1
X

m=n−τij (n)

β(m) ≤ c

n−1
X

−1

m q1

(24)

1
m=n−εn q1

1

−1

for all n with n ≥ N (ε) and n − εn q1 ≥ N . Using the monotonicity of n q1 , it follows that
n−1
X

1
m=n−εn q1

−1

1

Taking the lim sup on both sides above yields that


n−1
X
lim sup 
β(m) ≤
n→∞

1

−1

m q1 ≤ εn q1 (n − εn q1 ) q1

m=n−τij (n)

(25)

cε
.
1−ε

(26)
i

The statement follows since the choice of ε was arbitrary. The proof for eφn follows analogously.

20

The next lemma shows that the accumulated errors due to the marginalization of the successor
states in (18) is convergent almost surely. It therefore follows that ψni vanishes.
Lemma 4. Ψin :=

Pn−1

i
m=0 α(m)ψn

is a zero-mean square integrable martingale. Hence, Ψin

converges almost surely.

Proof: See Appendix A.
It now follows from Lemma 3 and Lemma 4 that we can study the convergence of (18)
i

i

without the additional error terms eθn , eφn and ψni . This is because the error terms will contribute
additional asymptotically negligible errors in the following proof of Lemma 5. With a slide
abuse of notation we now redefine the critic loss gradients ∇θi li as the marginalized critic loss
gradient ∇θi ˆli .
VI. C ONVERGENCE A NALYSIS
To analyze the asymptotic behavior of (18), we follow the ODE approach from SA [25], i.e.
we construct a continuous-time trajectory with the same limiting behavior as (18). First, we
divide the time axis using α(n) as follows:
t0 := 0 and tn :=

n−1
X

m=0

Now define
i

α(m) for n ≥ 1.

(27)

i

θ (tn ) := θni , n ≥ 0 and φ (tn ) := φin , n ≥ 0.
i

(28)

i

Let Rpθ and Rpφ be the parameter spaces of the θni ’s and φin ’s, respectively. Then define
i

i

i

i

i

i

θ ∈ C([0, ∞), Rpθ ) and φ ∈ C([0, ∞), Rpφ ) by linear interpolation of all θ (tn ) and φ (tn ),
respectively.
To analyze the training process, we formulate a measure process that captures the encountered
state-action pairs when using the global policy π(sn ; φn ). Therefore, define
µ(t) = δ(sn ,an ) , t ∈ [tn , tn+1 ]

(29)

where δ(x,a) denotes the Dirac measure. This defines a process of probability measures on S ×A.
For every probability measure ν on S × A, define
Z
i
i
˜ (θ , φ, ν) := ∇θi li (θi , φ, s, a)ν(ds, da),
∇l
Z
i
i
˜ (θ , φ, ν) := ∇φi g i (θi , φ, s)ν(ds, A).
∇g

(30)

21

Note that in ∇φi g i we used ν(ds, A), since the actor update in (18) is only state-dependent.
˜ i and ∇g
˜ i are continuous in all coordinates and locally
It follows from Lemma 1 that all ∇l
Lipschitz in both the θi - and φ-coordinate.

i

i

We can now define the associated continuous time trajectories in C([0, ∞), Rpθ ) and C([0, ∞), Rpφ )
that capture the training process starting from time tn for n ≥ 0:
Z t
i
i
˜ i (θni (x), φn (x), µn (x))dx,
∇l
θn (t) := θ (tn ) +
0
Z t
i
˜ i (θni (x), φn (x), µn (x))dx.
∇g
φin (t) := φ (tn ) +

(31)

0

where µn (t) := µ(tn + t). The combination of the continuous-time trajectories in (31) results in
the aforementioned single trajectory with the same limiting behavior as (18).
Per definition, the trajectories define solutions to the following families of non-autonomous
ordinary differential equations (ODEs):
˜ i (θi (t), φn (t), µn (t))}n≥0 ,
{θ̇ni (t) = ∇l
n

(32)

˜ i (θi (t), φn (t), µn (t))}n≥0 .
{φ̇in (t) = ∇g
n
By construction, we obtain that the limiting behavior of (18) is captured by the limits of the
i

i

sequences {θ ([tn , ∞))}n≥0 and {φ ([tn , ∞))}n≥0 defined by (28). Further, the sequences defined
in (31) can be analyzed as solutions to the ODEs in (32). If (28) and (31) behave asymptotically
identical, then the limiting behavior of (18) is thus captured by the solutions to the ODEs in
(32). This is formalized by the following important technical Lemma 5. This lemma is the key
component to enable a single-timescale analysis of DDPG style actor-critic algorithms along
the line of argument presented in [7] for Deep Q-Learning. To prove Lemma 5, we use that
β(n)
→ 1 from (A2)(b). This is essential since we just
the step size sequences are related by α(n)

constructed the continuous trajectories w.r.t. the timescale induced by α(n). The assumption in
essence requires that the critic and actor updates asymptotically run on the same time scale.
Lemma 5. For every T > 0, we have
i

lim sup kθ (tn + t) − θni (t)k = 0,

n→∞ t∈[0,T ]

i

lim sup kφ (tn + t) − φin (t)k = 0.

n→∞ t∈[0,T ]

(33)
(34)

22

Proof: Fix T > 0. We define [t] for t ≥ 0 as [t] := tsup{n|tn ≤t} . Fix t ∈ [0, T ], then
i

[tn + t] = tn+k for some k ≥ 0. Recall, that φ (t) is defined by linear interpolation w.r.t. α(n),
i

i

see (28). Hence, φ (tn + t) − φ (tn+k ) is equal to

tn + t − tn+k  i
i
φ (tn+k+1 ) − φ (tn+k ) .
α(n + k)

(35)

The stability of the algorithm and the compactness of the state-action space, i.e. (A3), show that
i

i

∇φi g i is bounded and hence kφ (tn+k+1 ) − φ (tn+k )k∈ O(β(n + k)). It follows that
i

i

sup kφ (tn + t) − φ ([tn + t])k∈ O(α(n)),

(36)

t∈[0,T ]

β(n)
since α(n) is monotonic and α(n)
→ 1. Similarly, we can show that

sup kφin (t) − φin ([tn + t] − tn )k∈ O(α(n))

(37)

t∈[0,T ]

To show (34), we now need to show that
i

sup kφ ([tn + t]) − φin ([tn + t] − tn )k→ 0.

(38)

t∈[0,T ]

From (31) it follows that
i

i

i

kφ (tn+k ) − φin (tn+k − tn )k≤ kφ (tn+k ) − φ (tn )
Z tn+k −tn
˜ i (θi (x), φn (x), µn (x))dxk.
∇g
−
n

(39)

0

i

Using a telescoping series, φ (tn+k ) − φin (tn+k − tn ) equals
n+k−1
X

i
β(m)∇φi g i (θm
, φm , sm )

m=n

=

n+k−1
X Z tm+1
m=n

tm

(40)
β(m) ˜ i i
∇g (θ ([x]), φ([x]), µn (x − tn ))dx
α(m)
i

i

The last step follows from α(m) = tm+1 − tm and using that φim = φ (tm ) = φ ([t]) for all
t ∈ [tm , tm+1 ). Now rewrite the second term in (39):
Z tn+k −tn
˜ i (θni (x), φn (x), µn (x))dx =
∇g
0
n+k−1
X Z tm+1
m=n

tm

˜ i (θni (x − tn ), φn (x − tn ), µn (x − tn ))dx
∇g

(41)

23

We now evaluate the difference of the terms under the integrals in (40) and (41).

k

β(m) ˜ i i
∇g (θ ([x]), φ([x]), µn (x − tn ))
α(m)
˜ i (θni (x − tn ), φn (x − tn ), µn (x − tn ))k
− ∇g

≤ C|

β(m)
˜ i (θi ([x]), φ([x]), µn (x − tn ))
− 1|+k∇g
α(m)
(42)

˜ i (θi (x − tn ), φn (x − tn ), µn (x − tn ))k
− ∇g
n
 i
β(m)
i
− 1|+L kθ ([x]) − θn ([x] − tn )k
≤ C|
α(m)

+ kφ([x]) − φn ([x] − tn )k+kθni (x − tn ) − θni ([x] − tn )k

+ kφn (x − tn ) − φn ([x] − tn )k
for some sample path dependent constant C < ∞ using the stability from (A3). The last step
˜ i . The combination of (39), (40), (41) and
adds zeros and uses the Lippschitz continuity of ∇g
(42) thus gives:

i

kφ (tn+k ) − φin (tn+k − tn )k


n+k−1
n+k−1
X
X

β(m)
α(m)O
O a(m)2
≤
−1 +L
α(m)
m=n
m=n
+L

n+k−1
X
m=n

(43)

 i
i
a(m) kθ (tm ) − θn (tm − tn )k

i

X
i
i
+
kφ (tm ) − φn (tm − tn )k

The first term in the above expression converges to zero as n → ∞, since

Pn+k−1
m=n

α(m) ≤ T

b(n)
by construction and since lim a(n)
= 1 from (A2)(b). The second term converges to zero, since
n→∞

α(n) is square summable (A2)(a).
i

Inequality (43) can now be derived analogously for kθ (tn+k ) − θni (tn+k − tn )k. We can now

24

sum up all L.H.S. and R.H.S. for all i in (43), and for all θi :
D
X
i
xn :=
kφ (tn+k ) − φin (tn+k − tn )k
i=1

+

D
X
i=1

i

kθ (tn+k ) − θni (tn+k − tn )k

≤ o(1) + 2L
+ 2LD

n+k−1
X
m=n

n+k−1
X

X i
i
a(m)
kθ (tm ) − θn (tm − tn )k

(44)

i

a(m)

m=n

X i
i
kφ (tm ) − φn (tm − tn )k,
i

where D is the number of agents. We now apply the discrete version of Gronwall inequality
Pn+k−1
P
[25] to xn . It follows that xn ≤ o(1)e2LD m=n a(m) . By construction n+k−1
m=n a(m) ≤ T for all

n ≥ 0, thus xn → 0, which proves the lemma.

Lemma 5 shows that we can analyze the limits of (18) as the limits of the continuous-time
trajectories defined in (28) in conjunction with the measure process (29). By construction, the
trajectories θni (t) and φin (t) are equicontinuous. Moreover, they are point-wise bounded from
(A3)(a). It now follows from the Arzela-Ascoli theorem, [26], that the families of trajectories
{θni ([0, ∞))}∞
n=0 ,
i

{φin ([0, ∞))}∞
n=0

(45)

i

are sequentially compact in C([0, ∞), Rpθ ) and C([0, ∞), Rpφ ), respectively. Further, it can be
shown that the space of measurable functions from [0, ∞) to the space of probability measures
on S × A is compact metrizable [27]. It now follows that the product space of all trajectories

θni (t) and φin (t) together with the aforementioned space of measurable functions is sequentially
compact. Hence, there is a common subsequence such that all considered sequences converge
i

i
simultaneously, i.e. we obtain (with a slight abuse of notation) that θni → θ∞
in C([0, ∞), Rpθ ),
i

φin → φi∞ in C([0, ∞), Rpφ ) and µn → µ∞ in the space of measurable functions. Analogously
to [7, Lemma 4], we can show that µn (t) also converges in distribution to µ∞ (t) in P(S × A),
t almost everywhere.
i
The following lemma now shows that the limits θ∞
, φi∞ and µ∞ are solutions to the limits

of the families of non-autonomous ordinary differential equations (32).

25
i
˜ i (θi (t), φ1 (t), . . . , φD (t), µ∞ (t))
Lemma 6. a) θ∞
is a solution to θ̇i (t) = ∇l
∞
∞
i
i
i i
1
i
˜
b) φ is a solution to φ̇ (t) = ∇g (θ (t), φ (t), . . . , φ (t), . . . , φD (t), µ∞ (t))
∞

∞

∞

∞

Proof: See Appendix A.
We can now study the limit of 3DPG as a solution to the aforementioned non-autonomous
ODE’s. Specifically, append the ODEs to form a new ODE in the appended parameter space.
The rest of the analysis follows the line of argument in [7, Thm. 1], so we only state the main
conclusion. Let (θ1 , . . . , θD , φ1 , . . . , φD ) be a solution to the appended ODE, then the solution
˜ i (θi , φ, µi∞ ) = 0 and ∇g
˜ i (θi , φ, µi∞ ) = 0
converges to an equilibrium of the appended ODE, i.e. ∇l
d

for all i, where lim µi∞ (t) = µi∞ . Lemma 5 and Lemma 6 show that the joint of the sequences
i

i

t→∞

θ (tn(k) ) and θ (tn(k) ) are solutions to the appended ODE for {n(k)}k≥0 ⊂ {n}n≥0 . The last two
i

i

i

i

statements thus show that the limits of θ (tn(k) ) and θ (tn(k) ), let us call them θ∞ and φ∞ , are
equilibrium points of the ODE’s in Lemma 6. These limits determine the long-term behavior of
3DPG.
Finally, the first part of Theorem 1 now follows (for the particular case of experience replay
with size 1 and global information access without communication) using (A3) to swap the order
˜ i and ∇g
˜ i . It left to show Theorem 1 for 3DPG with
of differentiation and integration in ∇l
experience replays and only local information access, and to show that the limiting distributions
µi∞ (s, A) are stationary w.r.t. the state Markov process. Both are the subject of the next section.
VII. E XTENSION TO E XPERIENCE R EPLAYS
Experience replay buffers play an important role in stabilization of RL algorithms [12]. The
fundamental idea is to learn from past experiences to reduce the bias of an RL algorithm towards
the interactions of an agent with its environment. For 3DPG this means that at time n an agent
does not use the transition tin to calculate the loss gradients, but it uses a random minibatch
of past transitions tim from old time steps m ≤ n. As a consequence, the training algorithm is
not overtly biased agents interaction with the environment, reducing learning variance, thereby
improving stability.
In the previous section, we analyzed 3DPG for centralized training where the global transitions
tin are locally available for every agent i. Additionally, we only used an experience replay of
size one. To accommodate the use of experience replays in Section V, the probability measure

26

µ(t) needs to be redefined. In (9), each agent i samples M < N global tuples independently
from its (local) random experience replay Rin at every iteration n. The sampling processes of
the agents will in general be different. Further, we will experience Rin 6= Rjn , since the global
tuples are communicated by the agents in a potentially delaying manner.
We now define a new measure process µi (t) for each agent. For t ∈ [tn , tn+1 ), define
1 X
δ(s
,a
).
M j=1 m(n,j,i) m(n,j,i)
M

µi (t) :=

(46)

Hence, µi (t) is the probability measure on S × A that places a mass of 1/M on each pair
(sm(n,j,i) , am(n,j,i) ) for 1 ≤ j ≤ M , where each m(n, j, i) denotes one of the time indices sampled
by agent i at time n from its memory Rin . Notice that in the presence of communication and

AoI, each experience replay is a random sequence of sets. If we use the redefined measures in
(30) we get for every t = tn that
˜ i (θi (t), φ(t), µi (t))
∇l
1 X
∇θi li (θni , φn , sm(n,j,i) , am(n,j,i) ),
=
M j=1
M

(47)

M
1 X
i i
i
˜
∇φi g i (θni , φn , sm(n,j,i) ).
∇g (θ (t), φ(t), µ (t)) =
M j=1

The analysis presented in Section V is also true for then new measure processes, where now
every agent has its own measure process. This shows the first part of Theorem 1. It is left to
characterize the properties of the limiting measure processes µi∞ , which are the limits of the
convergent subsequence extracted from µin (t) := µi (tn + t).
Lemma 7. For all t ∈ [0, ∞) and for all agents i
Z
i
µ∞ (t, dy × A) =
p(dy | x, φ∞ (t))µi∞ (t, dx × A).

(48)

S

In other words, the limiting marginals constitute stationary distributions over the state Markov
process.
Proof: Without loss of generality, assume a batch-size M = 1. The cases M > 1 will
only require additional bookkeeping. Recall that the samples used in the 3DPG iterations (9) are

27

potentially old and from random time-steps, such that (A1)(b) holds.3 Fix some agent i. In the
following, we will drop the agent index i. Since M = 1, the agent uses a global transition tkn
with random time index kn for its 3DPG training step at time n.
Pick f ∈ Cb (S), the convergence determining class for distributions on S. We analyze

Z
Z tn+1 
f (s) − f (y)p (dy | s, φn ) µ(z, ds, A)dz
tn
S


Z
= α(n) f (skn ) − f (y)p (dy | skn , φn )
(49)
S

as n → ∞. The error terms (49) consider the deviation between states sampled from µ(t) and
the associated expected transition under the policy that uses the sample during training. This
is the perspective of an experimenter that observes the Markov game and the 3DPG algorithm
during runtime.

Now accumulate the aforementioned deviations for all time steps where a sample tn would
be used during training and evaluate the deviation under the policy at time n. This information
is of course not required for the algorithm and solely a quantity for the analysis. In summary,
consider



γ(n) f (sn+1 ) −
at time n, where
γ(n) :=

Z

S


f (y)p (dy | sn , φn )

(50)

X

(51)

α(i).

i∈{i≥0|ki =n}

for every n ≥ 0 with

P

n
X
k=0

γ(k) ≤ O

2n
X
k=0

!

α(k)

− 1q

) with q ∈ [1, 2), then
Z 2n

− 1q
≤O
x dx

i∈∅ = 0. Recall that α(n) ∈ O(n

(52)

1

since at time n a sample from the replay memory can be at most n time steps old. It then follows
 1
 1
P
that n1 nk=0 γ(k) ∈ O n− q , which in turn implies that γ(k) ∈ O n− q and thus that γ(m)
is square summable.

To analyze (50), we now consider the sequence


Z
n−1
X
ξn :=
γ(m) f (sm+1 ) − f (y)p (dy | sm , φm )
m=0

3

(53)

S

Notably, (A1)(b) in conjunction with the Borel-Cantelli lemma guarantee that infinitely many global transitions reach each

agent.

28

and the filtration
Fn−1 := σhsm , am , φm , γ(m − 1) | m ≤ n − 1i.

(54)

Then ξn is a Martingale w.r.t Fn−1 . Since f is bounded and γ(m) is square summable, the
quadratic variation process associated with the Martingale ξn is convergent. It then follows from
the Martingale Convergence Theorem [28] that ξn converges almost surely.
Recall that we denote by ∆(n) the age of the oldest sample in the replay memory. Lemma 2
now also holds for ∆(n) using (A1)(b), i.e. fix ε ∈ (0, 1) then there is a sample path dependent
1

constant N (ε) ∈ N, such that ∆(n) ≤ εn q2 for all n ≥ N (ε) with q2 from (A1)(b). In other
words,
1

kn ∈ [n − εn q2 ], for all n ≥ N (ε).

(55)

Since ξn converges, it follows that for every t > 0
δ(n,t)

X

1
m=n−εn q2



Z
γ(m) f (sm+1 ) − f (y)p (dy | sm , φm )

(56)

S

converges to zero a.s., where
δ(n, t) := min{m ≥ n | tm ≥ tn + t}.
Next, spread out and rearrange the aggregated samples in (56). Specifically, use (55) and separate
the samples as the following three terms, whose sum converges to zero a.s.:
δ(n,t)

X

m=n



α(m) f (skm +1 ) −



+O

n
X



1

m=n−εn q2


f (y)p (dy | skm , φkm )

Z

S





α(m) + O 

1

δ(n,t)+εδ(n,t) q2

X

m=δ(n,t)



(57)


α(m)

Notably, the rearrangement is mathematically valid since (56) is a finite sum for each n. Since
q2 ≥ q, it follows that the second and third term converge to zero. The proofs use the same line
of argument as to show (22) in Lemma 3. We can therefore conclude that
δ(n,t)

X

m=n

converges to zero a.s.



α(m) f (skm +1 ) −

Z

S

f (y)p (dy | skm , φkm )



(58)

29

Equation (58) now also holds, if we replace φkm by φm since the resulting error terms when taking the difference between (58) and the version with φm converges to zero. To see this, note that
Pδ(n,t)−1
α(m) ∈ O(t) by construction. Further, all individual error terms in the aforementioned
m=n

difference converge to zero using weak convergence by continuity of (s, φ) 7→ p(· | s, φ) and
P
since kφkm −φm k→ 0 a.s. Finally, α(n) is eventually decreasing, hence δ(n,t)
m=n [α(m) − α(m + 1)] f (skm +1 ) →
0 a.s.. In summary, we have thus shown that


Z
δ(n,t)
X
α(m) f (skm ) − f (y)p (dy | skm , φm )
m=n

converges to zero a.s. With (49) and (59) it then follows that
Z tn +t Z h
i
f (s) − h(z, s) µ(z, ds, A)dz → 0 a.s.
where h(z, s) :=

R

tn

(59)

S

(60)

S


φ(z)
. The lemma now follows from (60). We refer to [7,
f
(y)p
dy
|
s,
S

Lemma 6] for details on this final step.

VIII. C OOPERATIVE T RAINING OF MAS BASED ON O LD ACTIONS VS . O LD P OLICIES
(MADDPG VS . 3DPG)
In this section, we discuss the difference between 3DPG and MADDPG for the centralized
training scenario with global information access.
A. The MADDPG policy gradient iteration
An MADDPG agent i updates its policy using the gradient
1 X
i
∇φi gMADDPG
(θni , φin , sm , aj6m=i )
M m

(61)

i
for 1 ≤ m ≤ M sampled transitions, with ∇φi gMADDPG
as defined in (6). Please again observe

the difference compared to the second iteration in our 3DPG algorithm (9). In MADDPG old
actions ajm for all j 6= i are used from the samples of the experience replay. In contrast the
true current policy aj = πφjn (sjm ) is used in the 3DPG policy gradient iteration (assuming the
centralized setting). MADDPG still uses the policies of other agents in the critic iteration. Hence,
availability of the policies from other agents is anyway required.
Lets try to understand the subtle difference between 3DPG and MADDPG intuitively. From the
perspective of some agent i the MADDPG policy gradient iteration appends the product action

30

space of other agents

Q

j6=i Aj

to the global state space S. For illustration, suppose agent i

i
i
samples transitions {tm }M
m=1 , and lets suppose all local states are equal, i.e. sm = s . Then (61)

gives ∇φi πi (si ; φin ) times

1 X
∇ai Qi (si , a1m , . . . , πi (si ; φi ), . . . , aD
m)
M m

!

(62)

as the sample policy gradient. The expression averages over sampled actions of the other agents.
These actions will sometimes include random actions due to exploration.4 This indicates that
agents would learn polices that also act well for random behavior of other agents. This seems
to be undesirable for cooperative learning. We will now make the above heuristic precise using
Theorem 1 and its variant for MADDPG.

B. The MADDPG limit vs. the 3DPG limit
First, we discuss the analog of Theorem 1 for MADDPG Specifically, consider (9) with
i
i
∇φi gMADDPG
as defined in (6) instead of ∇φi g3DPG
as defined in (7).

To analyze MADDPG with experience replays, we use the same measure processes (46) as
in the Section VII. However, we need to redefine the average policy gradient in (30) to
Z
i
i
i
i
˜
∇gMADDPG (θ , φ , ν) := ∇φi gMADDPG
(θi , φin , s, aj6=i )
1

i

(63)

D

ν(ds, da , . . . , A , . . . , da ).
The analysis from Sections V and VII can now be emulated for this gradient with the measure
processes µi (t). Due to the new average policy gradient the conclusion of the convergence
i
theorem are now fundamentally different. MADDPG converges to limits θMA
and φiMA , such that

˜ i (θi , φi , µi ) = 0,
∇g
MA MA
MA
MA

(64)

Here, µiMA are the local limiting distribution of the sampled experience at agent i under MADDPG. Recall that the 3DPG limit satisfies:
˜ i (θi , φ∞ , µi ) = 0
∇g
∞
3DPG ∞
4

(65)

In most DeepRL algorithms the probability to select a random action is decayed to a small value over time. However, it is

usually kept positive to also allow some asymptotic exploration.

31

In (64), the behavior of the other agents is solely present in the limiting measures µiMA . When
exploration is stopped after some time, then the asymptotic properties of MADDPG and 3DPG
are the same. However, when the exploration probability is not decayed to zero asymptotically,
the the limiting measures µiM A are also shaped by random actions. This formalizes the heuristic
from the previous subsection: the presence of exploration can deteriorate the policies found
by MADDPG agents. The agents would adapt their policies to random actions that are not
representative of the other agents. This is clearly an undesirable property. 3DPG fares better
in this regard as it does not have this negative property! 3DPG allows a high exploration
probability asymptotically without a negative effect on the found policies. Next, we discuss
the moving target problem.
Remark 5. In practice, the aforementioned negative property of MADDPG due random actions
is exacerbated due to the fact that training is stopped in finite time, possibly in a premature
manner. During training the other agents may have initially behaved in a certain way, which
was then well represented in the replay memory an agent. During later stages of learning,
the other agents may “quickly” converge to a different policy. Since the agent uses outdated
samples to calculate local gradients, the policy evolution is significantly biased towards the old
behavior of the other agents. This is particularly undesirable in cooperative problems. Again,
3DPG circumvents such scenarios by the using the latest available agent policies. MADDPG can
overcome these issues by stopping exploration after some time and by using decaying learning
rates.

C. The moving target problem in MARL
Multi-agent RL algorithms are effected by non-stationarity due to the change of other agents
behavior from the perspective of one agent (the so called moving target problem). Here, 3DPG
i
is no exception. The authors of [2], discuss that using the local policy gradients ∇φi gMADDPG

based on old actions of other agents removes the non-stationarity from the perspective of each
agent. We believe that this not wholly true. MADDPG smooths out the non-stationarity, as the
behavior of other agents is observed via the sampled actions from the experience replays. But
this does not remove the non-stationarity. Intuitively, it takes longer for the behavior of an agent
to manifest in the replay memory (in form of samples) compared to directly using the behavior

32

of an agent using its policy as in 3DPG.
Since 3DPG does use the policies of other agents, we expect that 3DPG has more variance due
to the changing behavior of other agents. However, since the 3DPG agents use more accurate
information from the other agents’ policies and are not affected by exploration as described in the
previous section, we expect a faster convergence rate compared to MADDPG. Both predictions,
are validated in our experiments in the following Section IX. Finally, notice that the moving target
problem has no impact on Corollary 1. The 3DPG agents converge to a local Nash equilibrium.
IX. N UMERICAL E XPERIMENTS
For our numerical experiments, we first compare 3DPG and MADDPG in a centralized training
setting with global information access. Afterwards, we evaluate 3DPG with communication,
where local states, actions and policies have to be communicated.
A. Experiment 1
For the comparison of 3DPG and MADDPG, we consider a multi-agent coordination problem. We add an additional coordination layer to a version of the simple spread multi particle
environment (SSMPE) presented in [2]. In the SSMPE, particles (the agents) move in the plane
to cover a number of landmarks and the landmarks are episodically reset to new positions. The
agents get a global reward
exp (−average closest distance to the landmarks) ∈ (0, 1).
In this particular scenario, we did not observe any significant difference between 3DPG and
MADDPG. This is because in SSMPE, the agents’ actions do not require any form of coordination and the global reward at every time-step is only a function of the global state.
We now add another coordination layer to the environment. We now consider two agents
that move around in the plane with the objective to minimize their average distance to three
landmarks as before. However, the agents only get rewarded if they move/watch in the same
direction. Specifically, the previous global reward gets weighted by
exp (−angle between the orientation of the agents).

(66)

The new reward structure therefore requires that the agents coordinate their decisions. More
details and algorithm hyperparameters used for training are presented in Appendix B.

33

0.5

3DPG
MADDPG

average reward per epoch

0.4
0.3
0.2
0.1
0.0

Fig. 2.

0

200

400

600

800
epoch

1000

1200

1400

Comparison of 3DPG and MADDPG with centralized training.

We trained both 3DPG and MADDPG with global data access and global access to all agents’
policies, i.e. for the centralized training setting without communication and AoI. We trained the
agents for 10 seeds over 1500 epochs, where each epoch had a horizon of 25 steps. Figure 2 shows
the the resulting average reward per epoch. Our simulations support precisely our theoretical
predictions from the previous section: For problems that require coordinated actions, 3DPG
obtains better policies faster than MADDPG at the cost of higher training variance.
B. Experiment 2
In our second experiment, we show that 3DPG with communication is robust to large AoI
and that 3DPG may even benefit from using older policies of other agents similar to how target
networks improve training in single agent RL [5].
We again consider the two agent, three landmark problem from Experiment 1. In addition, we consider that each of the two agents uses an independent communication channel for
communication. Specifically, each agent has a fixed communication budget of 15000 bits/slot
to communicate with the other agent whenever their channel access is success. We emulate
lossy communication by varying the channel access probability λ ∈ {e−1 , e−2 , e−3 , e−4 } ≈
{0.3679, 0.1353, 0.0498, 0.0183}. This simple communication model satisfies our network assumptions (A1). For 3DPG, we use the same hyper-parameter configurations as in Experiment 1
(Appendix B). For this setting, the agents therefore require at least 3 successive successful communication events to exchange a parameter vector φin , while at least 33 local tuples (sin , ain , sin+1 )

34

3DPG: Centralized
3DPG: = e 1
3DPG: = e 2
3DPG: = e 3
3DPG: = e 4

1000

0.3
0.2
3DPG: centralized
3DPG: = e 1
3DPG: = e 2
3DPG: = e 3
3DPG: = e 4

0.1

0

250

500

750

1000 1250 1500 1750 2000
epoch

(a) Average reward per epoch of 3DPG with variance over

Age of Information 12(n)

average reward per epoch

0.4

800
600
400
200
0

5500 5700 6000 6250 6500 6750 7000 7250 7500
Iteration step n

(b) Snapshot of the experienced AoI at agent 1

seeds.
Fig. 3.

Comparison of 3DPG with communication; λ is the communication success probability.

could be exchanged during the same time. We let each agent cycle between communicating a
policy update followed by communicating 33 local tuples. In that sense we give “equal weight”
to policy and data communication.
In Figure 3(a), we show the average reward per epoch. We see that 3DPG with even λ = e−4
is able to learn decent policies compared to centralized 3DPG, albeit at a slower convergence
rate. Notably, the λ = e−4 run achieves this with AoIs frequently over 500 time steps (20 epochs)
as shown in Figure 3(b). In addition, 3DPG with λ = e−4 has only access to 1/3 of the global
data tuples that are used by 3DPG with centralized training. This shows that 3DPG is highly
robust to AoI and low data availability. Finally, an interesting observation is that the 3DPG runs
with λ = e−1 , e−2 or e−3 consistently performed similar or even better than 3DPG centralized.
This indicates that 3DPG may even benefit from using older policies of other agents.
X. C ONCLUSIONS AND F UTURE W ORK
In this paper, we presented and analyzed 3DPG, a multi-agent reinforcement learning algorithm
for decentralized, online learning in networked systems. We showed that our analysis can
be modified to understand the popular MADDPG algorithm [2]. Our analysis and numerical
examples show that 3DPG should be preferred when a multi-agent decision making problem
requires coordinated decisions or a high degree of exploration.

35

In addition, we presented the first set of data availability assumptions (A1) for distributed
online actor-critic learning in network systems. The assumptions describe how old locally available global data tuples are allowed to be so that 3DPG converges. Our numerical experiments
show that 3DPG is highly robust to the use of this old information, which makes it attractive
for distributed online multi-agent learning.
For future work, we plan to analyze the trade off between using network resources for policy
communication vs. using network resources for data communication. In addition, we are working
on an analysis of 3DPG an its variants under a traditional two-timescale step-size schedule.
R EFERENCES
[1] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep reinforcement learning: A brief survey,” IEEE
Signal Processing Magazine, vol. 34, no. 6, pp. 26–38, 2017.
[2] R. Lowe, Y. WU, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperativecompetitive environments,” Advances in Neural Information Processing Systems, vol. 30, pp. 6379–6390, 2017.
[3] M. S. Sofla, M. H. Kashani, E. Mahdipour, and R. F. Mirzaee, “Towards effective offloading mechanisms in fog computing,”
Multimedia Tools and Applications, p. 1, 2022.
[4] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked
agents,” in International Conference on Machine Learning.

PMLR, 2018, pp. 5872–5881.

[5] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with
deep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015.
[6] A. Redder, A. Ramaswamy, and H. Karl, “Age of information process under strongly mixing communication – moment
bound, mixing rate and strong law,” in Proc. 58th Allerton Conference on Communication, Control, and Computing, 2022.
[7] A. Ramaswamy and E. Hullermeier, “Deep q-learning: Theoretical insights from an asymptotic analysis,” IEEE Transactions
on Artificial Intelligence, 2021.
[8] L. Wang, Q. Cai, Z. Yang, and Z. Wang, “Neural policy gradient methods: Global optimality and rates of convergence,”
arXiv preprint arXiv:1909.01150, 2019.
[9] M. L. Littman, “Markov games as a framework for multi-agent reinforcement learning,” in Machine learning proceedings
1994.

Elsevier, 1994, pp. 157–163.

[10] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The complexity of decentralized control of markov decision
processes,” Mathematics of operations research, vol. 27, no. 4, pp. 819–840, 2002.
[11] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming.

Athena Scientific Belmont, MA, 1996, vol. 5.

[12] V. Mnih, K. Kavukcuoglu, D. Silver et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518,
no. 7540, 2015.
[13] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy gradient algorithms,” in
International conference on machine learning.

PMLR, 2014, pp. 387–395.

[14] C. Nota and P. S. Thomas, “Is the policy gradient a gradient?” arXiv preprint arXiv:1906.07073, 2019.
[15] H. Lim and C. Kim, “Flooding in wireless ad hoc networks,” Computer Communications, vol. 24, no. 3-4, pp. 353–363,
2001.

36

[16] K. M. Chandy and L. Lamport, “Distributed snapshots: Determining global states of distributed systems,” ACM Transactions
on Computer Systems (TOCS), vol. 3, no. 1, pp. 63–75, 1985.
[17] A. Redder, A. Ramaswamy, and H. Karl, “Practical network conditions for the convergence of distributed optimization,”
IFAC-PapersOnLine, vol. 55, no. 13, pp. 133–138, 2022.
[18] V. S. Borkar and V. R. Konda, “The actor-critic algorithm as multi-time-scale stochastic approximation,” Sadhana, vol. 22,
no. 4, pp. 525–543, 1997.
[19] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv preprint arXiv:1606.08415, 2016.
[20] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,” arXiv preprint arXiv:1710.05941, 2017.
[21] H. Kumar, A. Koppel, and A. Ribeiro, “On the sample complexity of actor-critic method for reinforcement learning with
function approximation,” arXiv preprint arXiv:1910.08412, 2019.
[22] P. Mertikopoulos, N. Hallak, A. Kavis, and V. Cevher, “On the almost sure convergence of stochastic gradient descent in
non-convex problems,” Advances in Neural Information Processing Systems, vol. 33, pp. 1117–1128, 2020.
[23] S. Vlaski and A. H. Sayed, “Second-order guarantees of stochastic gradient descent in non-convex optimization,” IEEE
Transactions on Automatic Control, 2021.
[24] R. Ge, F. Huang, C. Jin, and Y. Yuan, “Escaping from saddle points—online stochastic gradient for tensor decomposition,”
in Conference on learning theory.

PMLR, 2015, pp. 797–842.

[25] V. S. Borkar, Stochastic approximation: a dynamical systems viewpoint.
[26] P. Billingsley, Convergence of probability measures.

Springer, 2009, vol. 48.

John Wiley & Sons, 2013.

[27] V. S. Borkar, “Stochastic approximation with ‘controlled Markov’noise,” Systems & control letters, vol. 55, no. 2, pp.
139–145, 2006.
[28] R. Durrett, Probability: Theory and Examples.

Cambridge university press, 2019, vol. 49.

[29] J. B. Conway, A course in functional analysis.

Springer, 2019, vol. 96.

A PPENDIX A
M ISSING PROOFS
Proof of Lemma 1: By (A5) the neural network activations are twice-continuous differentiability (C 2 ), hence π i (s; φi ) and Qi (s, a; θi ) are C 2 in their input coordinates. Additionally, it
follows from [7, Lemma 9] that π i (si ; φi ) and Qi (s, a; θi ) are C 2 in their parameter coordinates
φi and θi , respectively, for every fixed s ∈ S and a ∈ A. Note that composition, product and

sums of C 2 functions are C 2 . Moreover C 2 functions have local Lipschitz gradients. This is
because the gradient is C 1 , and C 1 functions are locally Lipschitz [29]. This immediately shows
that ∇θi li (θni , φn , sn , an , sn+1 ) and g(θni , φn , sn ) have the required properties.
For ∇θi ˆli (θni , φn , sn , an ), fix parameter vectors φ and θi as well as s ∈ S and a ∈ A. Since,

π(s; φ) and Qi (s, a; θi ) are C 2 in every coordinate, there is some R > 0 and continuous functions

37

LQi (y, θi , φ) and Lπ (y, φ), such that ∀ φ1 , φ2 ∈ B R (φ), we have
Z
Qi (y, π(y; φ1 ); θi )p(dy | s, a)
Z
− Qi (y, π(y; φ2 ); θi )p(dy | s, a)
Z
≤ LQi (y, θi )kπ(y; φ1 ) − π(y; φ2 )k2 p(dy | s, a)
Z
≤ kφ1 − φ2 k2 LQi (y, θi , φ)Lπ (y, φ)p(dy | s, a)

(67)

≤ L(φ)kφ1 − φ2 k2
for some L(φ) > 0. The last inequality follows from the stability of the critic iteration (A3)(a)
and the compactness of the state space (A3)(b). Hence, ∇θi ˆli (θi , φ, s, a) is locally Lipschitz
as a product and sum of locally Lipschitz functions. It is left to show that ∇θi ˆli (θi , φ, s, a) is

continuous in the s and a coordinate. This directly follows from the convergence in distribution
by continuity of p(dy | s, a) and ri (s, a), (A4) and (A6) respectively, and since Qi (s, π(s; φ); θi )
is a bounded continuous function using (A3).
Proof of Lemma 2: Fix ε ∈ (0, 1). By (A1)(a) there is a non-negative integer-valued random
variable τ , such that





1
1
P τij (n) > εn q1 ≤ P τ > εn q1

(68)

∞
∞
 X



X
X
1
1
q1
q1
P τij (n) > εn
P τ > εn
≤

(69)

for all n ∈ N0 and E [τ q1 ] < ∞. Hence,

m=0 n∈N (m)

n=0

≤
=

∞
X
X

P (τ > m)

(70)

|N (m)|P (τ > m) ,

(71)

m=0 n∈N (m)
∞
X

m=0

where the sets N (m) are defined as
1

N (m) := {n ∈ N0 : m ≤ εn q1 < m + 1}

(72)

for every m ∈ N0 . The second inequality then follows from the monotonicity of the cumulative

distribution function (CDF) by definition of the sets N (m). Since |N (m)| ≤ ε1q1 ((m + 1)q1 − mq1 ),

38

we have therefore shown that
∞
∞


X
1
1 X
q1
≤ q1
P τij (n) > εn
|N (m)|P (τ > n)
ε n=0
n=0

(73)
1
q1
≤ q1 E [τ ] < ∞.
ε
The last inequality follows since τ is a non-negative integer-valued random variable, using the
following proposition:
Proposition 1. Suppose X is a non-negative integer-valued random variable, then for every
q > 0:
E [X q ] =

∞
X

((m + 1)q − mq )P (X > m) .

(74)

m=0

Proof: of Lemma 4:



We have

ψni = γ Qi (sn+1 , π(sn+1 ; φn ); θni )−
Z

Qi (s, π(s; φn ); θni )p(ds | sn , an , φn ) ∇θi Qi (sn , an ; θni ).

(75)

Define the filtration Fn−1 := σ(sm , am , θm , φm | m ≤ n) for n ≥ 1. It then follows that {Ψn } is

a zero-mean martingale. It follows from (A3) and the C 2 condition in (A5) that supn≥0 kψni k≤
K < ∞ for a sample path dependent constant K. It then follows from the martingale convergence
P
i 2
k < ∞ almost surely by (A2)(a).
theorem [28] that Ψin converges, since nm=0 α2 (m)kψm

Consider the sequence θni . The proof for the other parameter

Proof: of Lemma 6:

sequences are identical. Fix T > 0. We need to show that
Z t
i
i
˜ i (θi (x), φ∞ (x), µ∞ (x))dxk
∇l
sup kθn (t) − θ∞ (0) −
n
t∈[0,T ]

(76)

0

converges to zero. The norm in (76) is bounded by
Z t
i
i
˜ i (θi (x), φn (x), µn (x))
kθn (0) − θ∞ (0)k + k ∇l
n
0

(77)

˜ (θi (x), φ∞ (x), µ∞ (x))dxk.
− ∇l
∞
i

We can now expand the second term, by successively adding zeros for each policy of each agent
j 6= i. We can then use Lemma 1 to bound the resulting expanison by a term
!
Z t
X
i
O
kθni (x) − θ∞
(x)k+
kφjn (x) − φj∞ (x)kdx ,
0

j6=i

(78)

39

Additionally, we are left with one term of the form
Z t
˜ i (θi (x), φ∞ (x), µn (x))
k ∇l
∞
0

(79)

i
˜ (θ∞
(x), φ∞ (x), µ∞ (x))dxk.
− ∇l
i

Due to the compact convergence of every parameter sequences (Arzela-Ascoli theorem) every
parameter sequence will converge uniformly over [0, T ]. This shows that (78) converges to zero.
i
(t), φ∞ (t) are bounded
Finally, (79) converges to zero as µn → µ∞ in distribution and since θ∞

almost surely by (A3)(a) and Lemma 5.
A PPENDIX B
S IMULATION D ETAILS
Algorithm 1: 3DPG Algorithm at agent i
Randomly initialize critic and actor weights θ0i , φi0 ;
Randomly initialize actor weights φj0 for all j 6= i ;

Initialize replay memory R0i and noise process N i . ;

for the entire duration do
Receive current state sin ;
Execute action ain = µi (sin ; φin ) + Nni ;

i
Observe rn+1
and sin+1 ;

Allocate local data (sin , ain , sin+1 ) and current local policy φni for transmission to
other agents ;
Run communication protocols ;
Store completely received global tuples tim in Rni ;
Sample M transitions from Rni ;
Apply iteration (9) using the sampled transitions ;
end

For our experiments, we consider a simplified version of the simple spread multi particle
coordination problem in [2]. Agents and landmarks are represented by points in [−1, 1]2 . Moreover, agents can move around by choosing a displacement from the set [−0.1, 0.1]2 . Agents

40

can observe their relative distance to the landmarks and other agents. The actual simple spread
environment considers that agents and landmarks take room in space, and the agents are penalized
for collisions.
For our experiments we use target networks for the local policy and critic as well as an
Ornstein–Uhlenbeck processes for exploration, both are described in [5]. Both MADDPG and
3DPG use the following algorithm configurations, chosen based on a rough hyperparameter
sweep for both algorithms.
•

Discount factor α = 0.9; Replay memory size 20000; Minibatch size 128; Two layer GELU
neural networks for each local policy with 64 and 8 neurons and tanh output layer; Two
layer GELU neural networks for each local critic with 1024 and 64 neurons.

•

α(n) =

e−6
n
+1
1000

, β(n) =

e−6
n
+1
1000

−6

+ ( ne +1)2
1000

