INDEPENDENT RL FOR COOPERATIVE-COMPETITIVE AGENTS:
A MEAN-FIELD PERSPECTIVE

arXiv:2403.11345v2 [cs.LG] 8 Feb 2025

MUHAMMAD ANEEQ UZ ZAMANâˆ— , ALEC KOPPELâ€  , MATHIEU LAURIEÌ€REâ€¡ , AND
TAMER BASÌ§ARÂ§
Abstract. We address in this paper Reinforcement Learning (RL) among agents that are
grouped into teams such that there is cooperation within each team but general-sum (non-zero
sum) competition across different teams. To develop an RL method that provably achieves a Nash
equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced
by multi-agent interactions in the finite population setting, we consider the case where the number of
agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ
Mean-Field Type Game (GS-MFTG). We characterize the Nash equilibrium (NE) of the GS-MFTG,
under a standard invertibility condition. This MFTG NE is then shown to be O(1/M )-NE for the
finite population game where M is a lower bound on the number of agents in each team. These
structural results motivate an algorithm called Multi-player Receding-horizon Natural Policy Gradient
(MRNPG), where each team minimizes its cumulative cost independently in a receding-horizon manner.
Despite the non-convexity of the problem, we establish that the resulting algorithm converges to
a global NE through a novel problem decomposition into sub-problems using backward recursive
discrete-time Hamilton-Jacobi-Isaacs (HJI) equations, in which independent natural policy gradient is
shown to exhibit linear convergence under time-independent diagonal dominance. Numerical studies
included corroborate the theoretical results.

1. Introduction. Multi-agent reinforcement learning (MARL) has gained popularity in recent years for its ability to address sequential decision-making problems
among agents [1, 2]. While a substantial effort has gone into developing algorithms and
performance guarantees when agents interact in a purely cooperative setting, relatively
less effort has gone into settings where agentsâ€™ objectives may be in opposition [3],
such as congestion [4], financial markets [5], and negotiations in markets [6]. It is
known that finding equilibrium policies, i.e., the Nash equilibrium (NE), for each
agent in such a general-sum stochastic game is in general NP-hard [7]. Furthermore,
in many real-world scenarios, agents behave in groups, with cooperation inside the
group and competition between groups. Therefore, in this work, we study mixed
Cooperative-Competitive (CC) team settings, and seek to understand conditions for
which a NE is achievable.
To enable a tractable formulation, we make two structural specifications: (i) agentsâ€™
dynamics are linear and their costs1 are quadratic, i.e., the linear-quadratic (LQ)
setting [8]; and (ii) the number of agents within a team approaches infinity such that
it may be approximated by its mean-field (MF) limit [9, 10]. This setting results in a
General Sum LQ Mean-Field Type Game (GS-MFTG). We provide more background
on these two specifications.
The LQ specification is motivated by a recent study of RL methods in the
LQ Regulator (LQR) setting, which has gained traction for its role as a benchmark
problem in which one can establish rigorous performance guarantees [11, 12], as well
as solve a variety of practical problems without the opacity of neural networks [13].
The LQ setting has several real-world applications as in finance (linear quadratic
âˆ— Analog Devices Incorporated
â€  Artificial Intelligence Research, JP Morgan Chase & Co
â€¡ School of Mathematics and Data Science, NYU Shanghai
Â§ Coordinated Science Lab, University of Illinois, Urbana-Champaign. Funding: Research of
T. BasÌ§ar was supported in part by the Air Force Office of Scientific Research (AFOSR) Grant
FA9550-24-1-0152
1 Costs are negative payoffs/rewards.

1

permanent income theory [14], portfolio management [15]) and engineering (Wireless
Power Control [16]), etc. Aside from these direct use cases, LQ system theory has been
essential in obtaining non-asymptotic sample bounds for RL algorithms like Policy
Gradient [11] and Actor-Critic [17], hence paving the way for later works to obtain
similar guarantees in more general settings [18, 19]. Our goal is to understand to what
extent we can broaden the scope of the LQ setting to provide a discernible problem
class in CC multi-agent settings.
The mean-field approximation is motivated by the fact that the complexity of
equilibria in finite population CC settings grows with the size of the teams [20, 21], and
thus the transient effect of competitive agentsâ€™ policies on stochastic state transitions
appears in any gradient estimate of the cost, which cannot be annihilated unless one
holds other agentsâ€™ policies fixed.
The aforementioned structures yield a GS-MFTG, which upon first glance may
seem a pristine setting, but in actuality, even this simplified setting exhibits fundamental technical challenges. Similar to the RL for LQR setting, the objective is non-convex,
which in principle should preclude finding a NE. This was already observed for the
simpler zero-sum (purely competitive setting) in [22]. Therefore, in this work, we pose
the following question:
Is it possible to construct a data driven method to achieve the Nash Equilibrium
in CC Games?
We answer this question affirmatively and our main contributions are as follows:
â€¢ We formalize the CC game in a finite population LQ framework and derive its
mean-field approximation as a MFTG. This approximation introduces a bias
that is O(1/M ) (Theorem 2.2), where M is the minimum number of agents
in any team. Inspired by adapted open-loop control analysis (Appendix D),
we decompose the GS-MFTG into two general-sum LQ games and establish
existence, uniqueness and characterization of the Nash equilibrium (NE) of
these games (Theorem 2.3), under standard invertibility conditions.
â€¢ To learn the NE of the GS-MFTG we develop a Multi-player Receding-horizon
Natural Policy Gradient (MRNPG) algorithm, in which the players independently update their policies using natural policy gradients in a receding-horizon
manner [23]. The receding-horizon approach decomposes the harder problem
of learning NE policies for all time-steps, into simpler sub-problems of learning
NE policies for each time-step, in a retrograde manner. This approach is
inspired by the Hamilton-Jacobi-Isaacs (HJI) equations.
â€¢ We establish convergence of MRNPG in two steps. First, we establish that
for each time-step the MRNPG algorithm converges to the NE policy at a
linear rate (Theorem 4.5) under a time and system noise-independent diagonal
dominance condition2 This new condition generalizes (while being much easier
to verify) the System Noise condition of [24] (Lemma 4.4). Furthermore, we
also obviate the need for covariance matrix estimation as in [24]. Finally, when
the policy gradient approximation error per time-step is O(Ïµ), the resulting
error in NE computation is shown to be O(Ïµ) (Theorem 4.6).
â€¢ Finally we corroborate the convergence of MRNPG within the context of a
numerical example, and provide a comparison with several benchmarks.
For ease of exposition, proofs of some of the auxiliary results are included in Supplementary Materials after References.
2We further relax the diagonal dominance condition using a cost-augmentation technique in

Appendix I.
2

Fig. 1: MRNPG Algorithm employs Natural Policy Gradient (NPG) for each agent at
timestep t, starting from t = T âˆ’ 1 and moving in a receding horizon manner (backwards-intime), to approximate the NE of the game.

MRNPG Illustration. Figure 1 shows the flow of the MRNPG algorithm. The
algorithm utilizes Natural Policy Gradient (NPG) to converge to the NE Ï€tiâˆ— , âˆ€i first
for t = T âˆ’ 1, then t = T âˆ’ 2 and continues in a receding horizon manner (backwardsin-time). MRNPG algorithm solves the HJI equations [8] in an approximate manner.
According to the HJI equations the NE can be computed backwards in time using,
âˆ—
Ï€tiâˆ— = argmin Cti (Ï€, Ï€tâˆ’iâˆ— |Ï€[t+1,T
âˆ’1] ), t âˆˆ {0, . . . , T âˆ’ 1},
Ï€

for all i, where Cti is the partial cost of agent i and Ï€[t,tâ€² ] is the set of policies for
all agents from time t to tâ€² . MRNPG utilizes NPG to perform the above shown
minimization (for each timestep t) in an independent data-driven stochastic manner
to obtain Ï€Ìƒtiâˆ— which is shown to be
âˆ—
Ï€Ìƒtiâˆ— â‰ˆ argmin Cti (Ï€, Ï€Ìƒtâˆ’iâˆ— |Ï€Ìƒ[t+1,T
âˆ’1] ), t âˆˆ {0, . . . , T âˆ’ 1}
Ï€

In the paper we show that due to the LQ framework and under the diagonal dominance
condition, MRNPG converges linearly to the exact NE of the game i.e. Ï€Ìƒtiâˆ— â‰ˆ Ï€tiâˆ— âˆ€i, t.
Related Work In the purely non-cooperative setting, considerable research
activity has taken place, in Reinforcement Learning (RL) both in the finite population
case [1, 24, 25] and in the mean-field limit (Mean-Field Games or MFGs for short)
[26, 27, 28, 29, 30, 31, 32, 33]; see [34] for a recent survey. Within the literature in
competitive multi-agent RL, two-player Zero-Sum games have proven to be particularly
amenable to analysis with works such as [35, 36, 37]. Conversely there have also been
results in data driven techniques for solving equilibria where the utilities of the agents
satisfy a potential function condition [38]. Apart from this work, RL for (General-Sum)
CC agents remains predominantly an uncharted territory from a theoretical analytical
standpoint besides a few empirical studies [39]. On the other hand, there has been
some recent works in RL for the purely cooperative setting (also called Mean-Field
Control) [27, 40, 41, 42] or for the Zero-Sum game setting with finitely many [43] or
infinitely many agents [37].
The work of [24] is particularly salient due to a lack of structure (Zero-Sum
or Potential) in the utilities of the agents. It proves that natural policy gradient
converges to the General-Sum Nash Equilibrium given knowledge of model parameters
and a system noise inequality (Assumption 4 [24]). This inequality is hard to verify as
the LHS increases with increasing system noise but the RHS may not decrease due
to the dependence of cost on system noise. We generalize this convergence result to
the data-driven (with unknown model parameters) CC setting under a more general,
verifible, time and system noise-independent, diagonal dominance condition and obviate
the need for covariance matrix estimation. This is made possible by employing the
3

receding-horizon approach introduced for the LQR problem under perfect [23] and
imperfect [44] information.
Notation: We use [N ] := {1, . . . , N } for any N âˆˆ N. We have âˆ¥xâˆ¥A representing
(xâŠ¤ Ax)1/2 for a nonnegative-definite matrix A, and with A = I, âˆ¥Â·âˆ¥ := âˆ¥Â·âˆ¥2 . As in
game theoretic notation, aâˆ’i := (aj )jÌ¸=i represents the set of values a for players other
than i. Gaussian distribution with mean Âµ and covariance Î£ is denoted by N(Âµ, Î£).
2. Setup & Equilibrium Characterization. We consider a general-sum game
among multiple teams, where agents within a team are cooperative, and distinct teams
compete. Initially, we pose this problem in the finite-agent setting for the case where
agentsâ€™ dynamics are linear stochastic and costs (negative rewards) are quadratic, i.e.,
the LQ framework. Subsequently, to enable the characterization of Nash equilibria of
the game, we consider the mean-field approximation within each team, i.e., the number
of agents within each team tends to infinity, which alleviates the transient effect of
other agentsâ€™ decisions on the system dynamics [20]. The result is a LQ mean-field
type game (MFTG). In this section we delineate the Nash equilibria of the GS-MFTG
and provide Ïµ-Nash guarantees for the finite agent Cooperative-Competitive game.
Linear Quadratic Cooperative-Competitive Games. We thus begin by
considering the finite agent Cooperative-Competitive (CC) game problem. In a CC
game, the agents are grouped into N teams, with each team i âˆˆ [N ] having Mi agents
and agent j in team i having linear dynamics, in that it is driven by a linear function
PMi i,j
i,j,i
xt /Mi
of the agentâ€™s state xi,j
, the average state xÌ„it = j=1
t , the agentâ€™s action ut
P
Mi
i,j,k
i,k
of population i, and the average actions uÌ„t = j=1 ut /Mi of population i. The
control action ui,j,i
refers to that of agent j in team i, whereas ui,j,k
for k âˆˆ [N ] \ i
t
t
refers to the adversarial control input of player k into the dynamics of agent j of
team i. In addition, the dynamics are affected by Gaussian noise with a team-specific
covariance, Ï‰ti,j âˆ¼ N(0, Î£i ), which are independent for every (i, j), as well as common
noise Ï‰t0,i âˆ¼ N(0, Î£0 ). Altogether, these lead to the linear dynamical system:
(2.1)

i i,j
i i
xi,j
t+1 =At xt + AÌ„t xÌ„t +

N
X


0,i
i,j
Bti,k ui,j,k
+ BÌ„ti,k uÌ„i,k
+ Ï‰t+1
+ Ï‰t+1
,
t
t

k=1

for t âˆˆ {0, . . . , T âˆ’ 1}, where Ait , AÌ„it âˆˆ RmÃ—m and Bti,k , BÌ„ti,k âˆˆ RmÃ—p . For simplicity of
analysis, we also assume that agentsâ€™ initial states are null except for the exogenous
i,j
0
input noise: xi,j
0 = Ï‰0 + Ï‰0 . All agents in team i aim to optimize a single cost
i
i
function JM over a finite horizon, that depends on both the teamâ€™s policy ui âˆˆ UM
âˆ’i
i
i
and those of other teams u âˆˆ UM , where the set of policies UM is adapted to the
state processes of all agents (xi,j
t )iâˆˆ[N ],jâˆˆ[Mi ],âˆ€t in a causal manner.
(2.2)

i
JM
(ui , uâˆ’i ) =

âˆ’1
X TX
1
i 2
i 2
E
âˆ¥xi,j
t âˆ’ xÌ„t âˆ¥Qit + âˆ¥xÌ„t âˆ¥QÌ„it
Mi
t=0
jâˆˆ[Mi ]

+

N
X

2
k,i 2
i,j
i 2
i 2
âˆ¥uk,j,i
âˆ’ uÌ„k,i
t
t âˆ¥Rk,i +âˆ¥uÌ„t âˆ¥RÌ„k,i +âˆ¥xT âˆ’ xÌ„T âˆ¥Qi +âˆ¥xÌ„T âˆ¥QÌ„i

k=1

t

t

T

T

where Rtik , RÌ„tik â‰» 0 and Qit , QÌ„it âª° 0 are symmetric matrices of suitable dimensions.
The subscript M of J refers to the fact that it is for the finite-agent setting. The cost
contains a consensus term that penalizes deviation from the average and regulation of
the average states and control actions.
For each agent to minimize (2.2) with respect to policies ui , the appropriate
solution concept is Nash Equilibrium, which we subsequently define. A set of policies
4

i
i
(uiâˆ— )iâˆˆ[N ] is a Nash equilibrium (NE) if JM
(uiâˆ— , uâˆ’iâˆ— ) â‰¤ JM
(ui , uâˆ’iâˆ— ) holds for all
i
i
alternative policy selections u âˆˆ UM .
Mean Field Approximation (GS-MFTG). In general, equilibrium policies
in finite-player dynamic games are functions of every playerâ€™s state. This causes
challenges in the computability and learnability of the NE with a large number of
agents. Due to this difficulty, we shift focus to the mean-field setting where the number
of agents in each team Mi â†’ âˆž. The NE policies in the mean-field setting are shown
to depend on the state of the generic agent and the average state (mean-field), thus
resolving the scalability problem. This limiting game is termed the GS-MFTG.
The state dynamics in the MFTG can be formulated by concatenating the state
dynamics of the j th agent (for any j âˆˆ N) from each team and discarding the superscript
j. Using (2.1), the dynamics of this joint state will be as follows

(2.3)

xt+1 = At xt + AÌ„t xÌ„t +

N
X


0
Bti uit + BÌ„ti uÌ„it + Ï‰t+1
+ Ï‰t+1

i=1

for t = {0, . . . , T âˆ’ 1}, where At = diag((Ait )iâˆˆ[N ] ), Bti = diag((Bti,k )kâˆˆ[N ] ) and
BÌ„ti = diag((BÌ„ti,k )kâˆˆ[N ] ), xÌ„t = E[xt | (Ï‰s0 )sâ‰¤t ] and uÌ„it = E[uit | (Ï‰s0 )sâ‰¤t ] for i âˆˆ [N ]. The
policies of the players belong to the feasible sets ui âˆˆ Ui , where Ui is the set of all
policies causally adapted to the state and mean-field process {x0 , xÌ„0 , . . . , xt , xÌ„t }. The
cost of player i âˆˆ [N ] is
(2.4)

J i (ui , uâˆ’i ) =E

âˆ’1
h TX

âˆ¥xt âˆ’ xÌ„t âˆ¥2Qi + âˆ¥xÌ„t âˆ¥2QÌ„i + âˆ¥uit âˆ’ uÌ„it âˆ¥2Ri + âˆ¥uÌ„it âˆ¥2RÌ„i
t

t

t

t

t=0

+ âˆ¥xT âˆ’ xÌ„T âˆ¥2Qi + âˆ¥xÌ„T âˆ¥2QÌ„i
T

i

T



where Rti = diag (Rti,k )kâˆˆ[N ] and RÌ„ti = diag (RÌ„ti,k )kâˆˆ[N ] â‰» 0 are symmetric. The
matrices Qit , QÌ„it âª° 0 are block matrices such that (Qit )ii = Qit and (QÌ„it )ii = QÌ„it and 0
otherwise. Each player i âˆˆ [N ] aims to minimize its cost function J i using its policy
ui âˆˆ Ui where now we formally introduce the solution concept of Nash equilibrium
for the MFTG.
Definition 2.1. The set of policies (uiâˆ— )iâˆˆ[N ] are in Nash equilibrium (NE) for
the MFTG if for each i âˆˆ [N ], J i (uiâˆ— , uâˆ’iâˆ— ) â‰¤ J i (ui , uâˆ’iâˆ— ) for all ui âˆˆ Ui .
By the definition of the class of policies Ui this NE is symmetric, i.e., cooperating
agents will have symmetric policies.
Approximate Nash Equilibria. The NE of the GS-MFTG is shown to be a
O(1/M )-Nash equilibrium of the CC game (2.1)-(2.2) where M := miniâˆˆ[N ] Mi is the
minimum number of agents across all teams i. This result guarantees that the NE
found using RL techniques (in Section 3) will be arbitrarily close to the NE of the
finite population CC game given that M is large enough.
Theorem 2.2. The NE of the MFTG is Ïµ-Nash for the finite agent CC game
(2.1)-(2.2) where Ïµ = O(1/ miniâˆˆ[N ] Mi ), i.e.
i
JM
(uiâˆ— , uâˆ’iâˆ— ) âˆ’ inf

i
ui âˆˆUM

i
JM
(ui , uâˆ’iâˆ— ) = O



TÏƒ
mini Mi



The guarantee is obtained by analyzing the difference between finite and infinite
population cost functions and by bounding Ïµ with a function of this difference. âˆš
The
complete proof is provided in Appendix F. Although a similar (albeit slower O(1/ N )
5

bound has been shown in the case of purely competitive MFGs [32], to our knowledge,
this work is the first to quantify the equilibrium gap between finite population CC
games and GS-MFTGs.
Characterization of Nash Equilibria. Next, we study a decomposition of
the GS-MFTG into two sub-problems, the first one pertaining to the mean-field and
the second one to the deviation from the mean-field. This decomposition is inspired
by the analysis of the open-loop Nash equilibrium of the MFTG, but since it is not
central to this exposition it is deferred to Appendix D. Then we use the discrete-time
Hamilton-Jacobi-Isaacs (HJI) equations to characterize the NE and present certain
invertibility conditions to guarantee its existence and uniqueness. These conditions
are a mainstay of scenarios with finitely many competing players [8]; it might also
be noted that the MFG framework with infinitely many competing players typically
requires a different set of conditions [9, 10].
To further elaborate on this, let us define yt = xt âˆ’ xÌ„t and vti = uit âˆ’ uÌ„it . The
dynamics of yt and xÌ„t can be written in a decoupled manner using (2.3),
(2.5)

yt+1 = At yt +

N
X

Bti vti + Ï‰t+1 , xÌ„t+1 = AÌƒt xÌ„t +

i=1

N
X

0
BÌƒti uÌ„it + Ï‰t+1
,

i=1

where AÌƒit = Ait + AÌ„it and BÌƒti = Bti + BÌ„ti . Since the dynamics of processes yt and xÌ„t are
decoupled, we can decompose the cost of ith player, J i (2.4), into two decoupled parts
as well:
J i (ui , uâˆ’i ) =Jyi (vi , vâˆ’i ) + JxÌƒi (uÌ„i , uÌ„âˆ’i ),
Jyi (vi , vâˆ’i ) =E

âˆ’1
h TX


i

âˆ¥yt âˆ¥2Qi + âˆ¥vti âˆ¥2Ri + âˆ¥yT âˆ¥2Qi ,
t

t

T

t=0

JxÌƒi (uÌ„i , uÌ„âˆ’i ) =E

(2.6)

âˆ’1
h TX


i

âˆ¥xÌ„t âˆ¥2QÌ„i + âˆ¥uÌ„it âˆ¥2RÌ„i + âˆ¥xÌ„T âˆ¥2QÌ„i .
t

t

T

t=0

This results in two decoupled N -player LQ game problems, and hence we use the
discrete-time HJI equations to characterize the NE of the GS-MFTG in the following
theorem. Before stating the theorem we introduce the coupled Riccati equations for
N -player LQ games [8, 24]. Consider control matrices
iâˆ—
iâˆ—
Ktiâˆ— = (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Lit , âˆ€i âˆˆ [N ]

where Ptiâˆ— are determined using Coupled Riccati equations,
iâˆ—
iâˆ— âŠ¤ i
iâˆ—
i
iâˆ—
i
Ptiâˆ— = LâŠ¤
t Pt+1 Lt + (Kt ) Rt Kt + Qt , PT = QT ,

(2.7)

PN
P
s.t. Lt = At âˆ’ i=1 Bti Ktiâˆ— and Lit = At âˆ’ jÌ¸=i Btj Ktjâˆ— . The expressions for KÌ„tiâˆ— and
PÌ„tiâˆ— can be obtained by replacing At , Bti , Qit and Rti matrices by AÌƒt , BÌƒti , QÌ„it and RÌ„ti .
Theorem 2.3. The set of policies (uiâˆ— )iâˆˆ[N ] constitutes a NE if, and only if,
(2.8)

iâˆ—
iâˆ—
uiâˆ—
t (xt ) = âˆ’Kt (xt âˆ’ xÌ„t ) âˆ’ KÌ„t xÌ„t ,

i âˆˆ [N ], t âˆˆ {0, . . . .T âˆ’ 1} where the control parameters Ktiâˆ— are guaranteed to exist
and be unique if the matrices Î¦t and Î¦Ì„t are invertible.
The proof can be found in Appendix E. The matrices Î¦t are block diagonal with the
iâˆ—
iâˆ—
ith diagonal entry Rti + (Bti )âŠ¤ Pt+1
Bti and ijth non-diagonal entries (Bti )âŠ¤ Pt+1
Btj . The
emergence of the invertibility condition is a consequence of the HJI equations and it
naturally arises in similar games with a finite number of competing entities, e.g. LQ
Games [8].
6

3. Multi-player Receding-horizon NPG (MRNPG). In this section, we
discuss the challenges that arise when solving the NE through a data-driven approach,
having established its linear form as shown in (2.8). In our CC setting, finding the NE
is elusive as the cost function even for a single agent LQ control problem is non-convex
[11, 45]. Additionally, vanilla policy gradient is known to diverge, in purely competitive
N -player LQ games [46]. [24] has proven the convergence of natural policy gradient for
purely competitive agents albeit under complete knowledge of the model parameters
and a system noise condition. In Section 4, show the linear rate of convergence of
the natural policy gradient to the CC NE even when model parameters are unknown
under a diagonal dominance condition which is shown to generalize the system noise
condition (Assumption 4 in [24]) in Lemma 4.4. We also eliminate a potential source
of error in the analysis by obviating the need for covariance matrix estimation.
We now provide some details of our algorithmic construction to achieve this result.
The key idea is the use of receding-horizon approach (inspired by the discrete-time
Hamilton-Jacobi-Isaacs (HJI) equations) whereby finding policies for all the agents
at a fixed time t (and moving backwards-in-time) reveals a quadratic cost structure,
allowing linear convergence guarantee. The net result is that teams comprised of
finitely many agents (M â‰¥ 2) following the MRNPG algorithm approach the NE
(Definition 2.1) of the GS-MFTG (2.3)-(2.4).
Disentangling the Mean Field and deviation from Mean-Field. Let
us define a joint state xjt for j âˆˆ [M ] by concatenating the states of j th agents in
N,j âŠ¤ âŠ¤
âŠ¤
all teams at time t such that xjt = [(x1,j
t ) , . . . , (xt ) ] . Due to the linear form
of NE (2.8) we restrict the controllers to be linear in state and empirical meanfield, xjt and xÌƒt , respectively (without loss of generality Theorem 2.3), such that
i j
i
ui,j
uÌƒi = âˆ’KÌ„ti xÌƒt , where the average state
t = âˆ’K
t (xt âˆ’ xÌƒt ) âˆ’ KÌ„t xÌƒt , which results in
PM
PMt
j
1
1
i
xÌƒt = M j=1 xt and average control uÌƒt = M j=1 ui,j
t . Under these control laws, the
dynamics of agent j âˆˆ [M ] and the dynamics of the empirical mean-field xÌƒt are linear.
Specifically the dynamics of the average state xÌƒt is
(3.1)

0
xÌƒt+1 = LÌ„t xÌƒt + Ï‰Ìƒt+1
,

PM j
PN
PN
0
0
/M .
where Lt = At âˆ’ i=1 Bti Kti , LÌ„t = AÌƒt âˆ’ i=1 BÌƒti KÌ„ti and Ï‰Ìƒt+1
= Ï‰t+1
+ j=1 Ï‰t+1
j
j
Now as in the Section 2, we introduce the deviation yt = xt âˆ’ xÌƒt with dynamics
(3.2)

j
j
yt+1
= Lt ytj + Ï‰Ìƒt+1
,

P
j
j
k
where Ï‰Ìƒt+1
= (M âˆ’ 1)Ï‰t+1
/M âˆ’ kÌ¸=j Ï‰t+1
/M . The cost of any agent j âˆˆ [M ] and
i
player/team i âˆˆ [N ] under control laws (K , Kâˆ’i ) where Ki = (Kti , KÌ„ti )tâˆˆ[T ] for any
i âˆˆ [N ] can be decomposed in a similar manner;
JËœi,j (Ki , Kâˆ’i ) = JËœyi,j (K i , K âˆ’i ) + JËœxÌƒi (KÌ„ i , KÌ„ âˆ’i )
âˆ’1
h TX
i
JËœyi,j (K i , K âˆ’i ) = E
âˆ¥ytj âˆ¥2Qi +(K i )âŠ¤ Ri K i + âˆ¥yTj âˆ¥2Qi
t

t

t

t

T

t=0
âˆ’1
h TX
i
i
i
âˆ’i
Ëœ
JxÌƒ (KÌ„ , KÌ„ ) = E
âˆ¥xÌƒt âˆ¥2QÌ„i +(KÌ„ i )âŠ¤ RÌ„i KÌ„ i + âˆ¥xÌƒT âˆ¥2QÌ„i ,
t

t

t

t

T

t=0

where K i = (Kti )tâˆˆ[T ] and KÌ„ i = (KÌ„ti )tâˆˆ[T ] . This problem re-parameterization allows
us to decouple the cost function into terms of consensus error y and mean field xÌƒ.
7

Receding Horizon Mechanism. We solve each problem using the recedinghorizon approach. This approach is inspired by the HJI equations [8] which obtain the
NE by solving for the NE policy at time T âˆ’1 and then moving in retrograde-time. The
receding-horizon approach is a data-driven version of the discrete-time HJI equations.
Next, we provide details of the receding-horizon approach for the process y. At each
time-step t we solve for the set of controllers at time t, (Kti )iâˆˆ[N ] , which minimize the
i,1
cost JËœy,t
(K i , K âˆ’i ), while keeping the controllers (Ks )t<s<T fixed
(3.3)

T
h
i
X
1 2
Ëœi (K i , K âˆ’i ) = E âˆ¥y 1 âˆ¥2 i
min
J
+
âˆ¥y
âˆ¥
i
i
i
i
i
âŠ¤
i
i
âŠ¤
y,t
t Q +(K ) R K
s Qs +(Ks ) Rs Ks ,
i
t

Kt

t

t

t

s=t+1

where yt âˆ¼ N(0, Î£y ), Î£y â‰» 0 and KTi = 0 for each player i âˆˆ [N ]. Notice that the
choice of agent 1 is arbitrary. The minimization problem at time t, (3.3) (due to
forward-in-time controllers (Ks )t<s<T being fixed) is quadratic in the control parameter
Kti which allows it to satisfy a specific PL condition (Lemma 4.1) which ensures the
linear rate of convergence of natural policy gradient to the NE. Similarly for the process
xÌƒ at each time-step t âˆˆ {T, . . . , 0} we fix the controllers forward-in-time (KÌ„s )t<s<T
and solve for the set of controllers (KÌ„ti )iâˆˆ[N ] which minimize the cost
(3.4)

T
h
i
X
i
min JËœxÌƒ,t
(KÌ„ i , KÌ„ âˆ’i ) := E âˆ¥xÌƒt âˆ¥2QÌ„i +(KÌ„ i )âŠ¤ RÌ„i KÌ„ i +
âˆ¥xÌƒs âˆ¥2QÌ„is +(KÌ„si )âŠ¤ RÌ„si KÌ„si
t

KÌ„ti

t

t

t

s=t+1

where xÌƒt âˆ¼ N(0, Î£xÌƒ ), Î£xÌƒ â‰» 0 and KÌ„Ti

= 0, âˆ€i âˆˆ [N ]. For given time t, the set of
controllers which satisfy equations (3.3) and (3.4) for a fixed set of forward-in-time
controller (Ks )t<s<T are called local-Nash controllers and defined as
i
Ëœ iâˆ— = argmin JËœi (KÌ„ i , KÌ„ âˆ’i ).
KÌƒtiâˆ— = argmin JËœy,t
(K i , K âˆ’i ), KÌ„
t
xÌƒ,t
Kti

KÌ„ti

MRNPG Algorithm Construction. Before introducing the MRNPG algorithm,
we first characterize what constitutes a valid search direction in policy space i.e. policy
gradient in the receding-horizon setting.
Lemma 3.1. In a receding-horizon setting for a fixed t âˆˆ {T âˆ’ 1, . . . , 0} the policy
i
i
gradient of cost JËœy,t
(K i , K âˆ’i ) with respect to Kti is âˆ‡iy,t (K i , K âˆ’i ) := Î´ JËœy,t
(K i , K âˆ’i )/Î´Kti
(3.5)

i
i
âˆ‡iy,t (K i , K âˆ’i ) =2 (Rti + (Bti )âŠ¤ Py,t+1
Bti )Kti âˆ’ (Bti )âŠ¤ Py,t+1
At âˆ’

X

Btj Ktj


Î£y ,

jÌ¸=i

i
with Pt+1
defined in terms of controllers (Ksi )iâˆˆ[N ],t<s<T

(3.6)

i
i
i
i
Py,t
= Qit + (Kti )T Rti Kti + LâŠ¤
t Py,t+1 Lt , Py,T = QT .

i
Proof can be found in Appendix G. The policy gradient âˆ‡ixÌƒ,t (K i , K âˆ’i ) := Î´ JËœxÌƒ,t
(KÌ„ i , KÌ„ âˆ’i )/Î´ KÌ„ti
has a similar expression. Comparing (3.5) with the policy gradient in [24] we note
i
that due to the receding-horizon approach the matrix PÌƒy,t
(3.6) is fixed and the policy
i
gradient âˆ‡y,t is a function of Î£y which can explicitly be chosen in the receding-horizon
approach obviating the need for estimating Î£y . We use mini-batched zero-order
Ëœ iy,t (K i , K âˆ’i ) and
techniques as in [11, 12] to approximate the policy gradients with âˆ‡
i
i
âˆ’i
i
Ëœ (KÌ„ , KÌ„ ), which are the stochastic gradients of the costs JËœ and JËœi
âˆ‡
y,t

xÌƒ,t

Nb

(3.7)

Ëœ iy,t (K i , K âˆ’i ) =
âˆ‡

m X Ëœi
Jy,t (KÌ‚ i (ej , t), K âˆ’i )ej
Nb r2 j=1
8

xÌƒ,t

respectively, where ej âˆ¼ SpN Ã—mN (r) is the perturbation and KÌ‚ i (e, t) := (Kti +
e, . . . , KTi âˆ’1 ) is the perturbed controller set at time-step t. Nb denotes the miniËœ i (KÌ„ i , KÌ„ âˆ’i ) is
batch size and r the smoothing radius of the stochastic gradient. âˆ‡
xÌƒ,t
computed in a similar manner. In Appendix J we generalize to the sample-path cost
oracle from the expected cost oracle of (3.4).
Now we state the MRNPG algorithm. The algorithm is quite simple; starting
at time t = T âˆ’ 1, each team/player i âˆˆ [N ] updates its control parameters (Kti , KÌ„ti )
using natural policy gradient and then moves one step backwards-in-time. Notice that
Algorithm 3.1 MRNPG for GS-MFTG
1: Initialize Kti = 0, KÌ„ti = 0 for all i âˆˆ [N ], t âˆˆ {0, . . . , T âˆ’ 1}
2: for t = T âˆ’ 1, . . . , 1, 0, do
3:
for k = 1, . . . , K do
4:
Natural Policy Gradient for i âˆˆ [N ]


 i
 i
 i
Ëœ (K i , K âˆ’i )Î£âˆ’1
Kt
Kt
âˆ‡
â†
âˆ’ Î·ki Ëœ iy,t i âˆ’i yâˆ’1
i
i
KÌ„t
KÌ„t
âˆ‡xÌƒ,t (KÌ„ , KÌ„ )Î£xÌƒ

(3.8)

5:

end for

6: end for

(in Algorithm 3.1) to compute the natural policy gradients, only the perturbed costs
are required, as shown in (3.7), and estimating the covariance matrix is not required
(as in [24]). This is an independent learning algorithm as all teams independently
compute their natural policy gradients and the learning rates Î·ki are also independent.
4. Achieving Nash Equilibrium. In this section we analyze the MRNPG
algorithm and show linear rate of convergence to the NE. To establish our main
result, several key steps are needed. Some are standard, such as unbiasedness and
smoothness properties of gradient estimators (Lemma 4.2). More unique to this work
is the establishment of a Polyak-Lojasiewicz (PL, also known as gradient dominance)
inequality that relates the difference of costs with the update direction as Lemma
4.1. While such a result is expected if an objective function is strongly convex, in a
non-convex setting it generally does not hold. That it does in the single-agent LQR
setting is the central contribution of [11]. Here, we generalize it to the LQ game
i
theoretic setting. In particular, we establish a PL condition of cost JËœy,t
which is much
simpler than the prior work [24]. We proceed then with the following technical lemma.
i
Lemma 4.1. (Polyak-Lojasiewicz inequality) The cost function JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) satisfies the following growth condition with respect to gradient âˆ¥âˆ‡iy,t (KÌƒ i , K âˆ’iâˆ— )âˆ¥2F
âˆ¥Î£K âˆ— âˆ¥
i
i
JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) âˆ’ JËœy,t
(K iâˆ— , K âˆ’iâˆ— ) â‰¤
âˆ¥âˆ‡iy,t (KÌƒ i , K âˆ’iâˆ— )âˆ¥2F
ÏƒR Ïƒy2
iâˆ—
, . . . , KTiâˆ—âˆ’1 ) and Ïƒy is the minimum eigenvalue of Î£y .
where KÌƒ i = (Kti , Kt+1
i
The proof can be found in Appendix H. The cost JËœxÌƒ,t
also satisfies a similar
PL condition. Next we analyze the MRNPG algorithm and show the linear rate of
convergence to the NE. Let us first introduce the smoothed gradients [11, 12, 45] of
i
i
Â¨ i (K i , K âˆ’i ) and âˆ‡
Â¨ i (KÌ„ i , KÌ„ âˆ’i ) which are the
costs JËœy,t
and JËœxÌƒ,t
, respectively as, âˆ‡
y,t
xÌƒ,t
Â¨ iy,t (K i , K âˆ’i )
expectations of the stochastic gradients (3.7). The smoothed gradient âˆ‡

9

is given by

 i
Â¨ iy,t (K i , K âˆ’i ) = m Ee JËœy,t
âˆ‡
(KÌ‚ i (e, t), Ktâˆ’i )e ,
r

Â¨ i (KÌ„ i , KÌ„ âˆ’i ) has a similar expression. Now we state some
where e âˆ¼ Smâˆ’1 (r). âˆ‡
xÌƒ,t
results from the literature which shows that the stochastic gradient is an unbiased
estimator of the smoothed gradient and quantifies the bias between the smoothed,
stochastic and policy gradients.
Â¨ iy,t (K i , K âˆ’i ) in (3.7) for
Lemma 4.2 ([12]). Consider the smoothed gradient âˆ‡
the per-team consensus error ytj for team j at time t, as well as the stochastic gradient
Ëœ i (K i , K âˆ’i ) of the receding horizon cost in Lemma 3.1, then
âˆ‡
y,t
Ëœ iy,t (K i , K âˆ’i )] = âˆ‡
Â¨ iy,t (K i , K âˆ’i ),
E[âˆ‡
Â¨ iy,t (K i , K âˆ’i ) âˆ’ âˆ‡iy,t (K i , K âˆ’i )âˆ¥2 = O(r),
âˆ¥âˆ‡
p

log Î´ âˆ’1
i
âˆ’i
i
i
âˆ’i
i
Ëœ
Â¨
âˆ¥âˆ‡y,t (K , K ) âˆ’ âˆ‡y,t (K , K )âˆ¥2 = O
Nb r

where the last equality follows with probability 1 âˆ’ Î´, Nb is a mini-batch size, and r is
the smoothing parameter.
Similar bounds can also be obtained for the gradients pertaining to process xÌƒ,
Ëœ i (KÌ„ i , KÌ„ âˆ’i ) and âˆ‡
Â¨ i (KÌ„ i , KÌ„ âˆ’i ). These bounds show that the approxâˆ‡ixÌƒ,t (K i , K âˆ’i ), âˆ‡
xÌƒ,t
xÌƒ,t
Ëœ iy,t (K i , K âˆ’i ) âˆ’ âˆ‡iy,t (K i , K âˆ’i )âˆ¥,
imation error in the stochastic gradient Î´ti := max(âˆ¥âˆ‡
i
i
i
âˆ’i
i
âˆ’i
Ëœ
âˆ¥âˆ‡xÌƒ,t (KÌ„ , KÌ„ ) âˆ’ âˆ‡xÌƒ,t (K , K )âˆ¥) = O(Ïµ) with probability 1 âˆ’ Î´ if the smoothing
radius, r = O(Ïµ) and mini-batch size, Nb = Î˜(log(1/Î´)/Ïµ2 ). Now we introduce the
diagonal dominance condition.
Assumption 4.3 (Diagonal Dominance).
entails that
Ïƒ(Rti ) â‰¥

p

The diagonal dominance condition

2
i
2m(N âˆ’ 1)Î³B,t
Î³P,t+1

i
where Î³B,t := maxiâˆˆ[N ] âˆ¥Bti âˆ¥ and Î³P,t
:= âˆ¥Ptiâˆ— âˆ¥ where matrices Ptiâˆ— are defined in
(2.7).

This condition is similar to conditions in the static games literature [47] and ensures
that the matrices Î¦t and Î¦Ì„t (Theorem 2.3) are diagonally dominant and hence
invertible. In Section I we propose a cost augmentation mechanism which ensures
the diagonal dominance condition. The System Noise (SN) condition (Assumption
4) in [24] is hard to verify as the LHS increases with increased system noise but
the RHS may not decrease due to the dependence of the cost on system noise. In
contrast, Assumption 4.3 is dependent on model parameters and can be verified by
direct computation. Moreover Assumption 4.3 is independent of the time-horizon T
whereas the SN condition gets harder to satisfy with increasing T . In the following
lemma we establish that for any T âˆˆ N, Assumption 4.3 generalizes the SN assumption.
Lemma 4.4. For any T âˆˆ N, Assumption 4.3 generalizes the System Noise (SN)
assumption in (Assumption 4 in [24]).
Now under Assumption 4.3 we show the linear rate of convergence of recedinghorizon update (3.8) to the local Nash controllers (3).
Theorem 4.5. Under Assumption 4.3
 for each t âˆˆ {T âˆ’ 1, . . . , 0}, if conditions in
Theorem 2.3 are satisfied, k = Î˜ log 1Ïµ , Î·ki is upper bounded by model parameters,
10

Fig. 2: Numerical Analysis of MRNPG algorithm. (left) comparison with Vanilla Natural
Policy Gradient (NPG) and Exact-MRNPG, (center) performance with respect to different
values of learning rate Î·ki , and (right) mini-batch size Nb .

the approximation error in the stochastic gradient is Î´t = O(Ïµ), then the optimality gap
Ëœ iâˆ— âˆ¥ = O(Ïµ) for all i âˆˆ [N ].
âˆ¥Kti,k âˆ’ KÌƒtiâˆ— âˆ¥ = O(Ïµ) and âˆ¥KÌ„ti,k âˆ’ KÌ„
t
Closed-form expressions of the bounds can be found in the proof (Section B). These
results are more general than those in [2, 11, 45, 12] due to the presence of competing
players learning in an independent fashion. Due to the receding-horizon approach, the
cost functions (3.3)-(3.4) to be minimized have a quadratic structure, which satisfies a
PL condition (Lemma 4.1) and allows for the linear rate of convergence. Moreover,
the receding-horizon approach bounds the difference between the target controller
i
(one which solves (3.4)) and the NE Ktiâˆ— , by controlling the error in the PÌƒy,t
matrix
(3.6). Finally the receding-horizon approach obviates the system noise condition in
[24] by explicitly designing the covariance matrix Î£y to be positive definite. All these
factors combine to ensure that if K = O(log(1/Ïµ)) the error in control parameters are
O(Ïµ). Using this result now we state the main result of the paper, presenting the finite
sample convergence bounds of Algorithm 3.1.
j
Theorem 4.6. If all conditions in Theorem 4.5 are satisfied, then eK
t := maxjâˆˆ[N ] âˆ¥Kt âˆ’

Ktjâˆ— âˆ¥ = O(Ïµ) for sufficiently small Ïµ > 0 and t âˆˆ {T âˆ’ 1, . . . , 0}.
The error eK
t is due to a combination of the approximation error in the stochastic
gradient Î´t at time t and the accumulated error in the forward-in-time controllers
K
eK
t+1 , . . . , eT âˆ’1 . We first characterize these two quantities and then show that if the
approximation error Î´t = O(Ïµ) and the number of inner-loop iterations K = O(log(1/Ïµ)),
then the accumulated error at any time t âˆˆ {0, . . . , T âˆ’ 1} never exceeds Ïµ scaled by
a constant multiplier. One important point to note is that Ïµ = O(1/N ) to keep the
approximation error Î´t small, which avoids instability in the algorithm.
5. Numerical Analysis. We first simulate the MRNPG algorithm for time
horizon T = 2, number of teams N = 2, number of agents per team M = 1000 and
agents having scalar dynamics. We note that in contrast to Zero-Sum Game studies in
the literature [7], we consider a General-Sum setting where N = 2, which essentially
carries the same kind of difficulty as N > 2 albeit with a different sample complexity.
For each time-step the number of inner-loop iterations K = 1000, the mini-batch
size Nb = 5000 and learning rate Î·ki = 0.001. In Figure 2(left) we first compare the
11

Fig. 3: Error convergence for exact versions of MRNPG, MF-MARL and MADPG for
N = {3, 6, 9}, T = 3 and m = p = 2.

error convergence in the MRNPG algorithm with Vanilla Natural Policy Gradient
(NPG) which does not utilize the receding-horizon approach and the Exact-MRNPG
which uses exact natural policy gradients for update. As expected, the Exact-MRNPG
converges very well for each time-step and MRNPG also converges albeit with variance
due to noise in the policy gradients. The NPG algorithm, on the other hand, is seen to
be slow at convergence with high variance. The superior convergence is due to the fact
that MRNPG decomposes the problem and solves each problem in retrograde-time.
In Figure 2 (center) we evaluate MRNPG performance for N = 1, T = 1 and different
values of learning rate Î·ki . Î·li = 0.001 is shown to provide the best convergence
properties with fast decrease in error (unlike slow convergence with Î·ki = 0.001) and
reliable performance (unlike the high variance with Î·ki = 0.01). Figure 2 (right) shows
that the variance of the natural policy gradient update decreases with increasing
mini-batch size Nb but at the cost of higher sample complexity.
In Figure 3 we provide a comparison of MRNPG with MADPG [39] and MF-MARL
[48] in CC setting. Notice that since CC setting is quite novel, direct comparison is
only possible with a limited number of works. We use exact versions of all algorithms
allowing faster convergence (compare the number of iterations in Figures 2 and 3),
and extension to data-driven stochastic versions appear to be straightforward. The
MF-MARL algorithm is also a receding-horizon algorithm (albeit for finite state and
action spaces) but assume a large number of competing players (notice that MRNPG
can deal with any number of competing players). The MADPG algorithm [39] is a
12

variant of the actor-critic method where each agent has an actor-critic and the critic
is learned in a centralized manner. Figure 3 compares the algorithms for increasing
number of players/teams N = {3, 6, 9}, T = 3, m = p = 2. For smaller N (N = 3, 6)
MF-MARL shows an offset in error convergence compared to MRNPG. This is due
to the fact that MF-MARL computes a mean-field equilibrium which will be Ïµ-Nash
with Ïµ â†’ 0 as N gets large. On the other hand, increasing N is shown to cause an
overshoot in error convergence of MADPG compared to the steady decrease in error
for MRNPG. This is due to the fact that the error in earlier time-steps is significantly
affected by error in the later time-steps (HJI). Due to the receding-horizon nature
of MRNPG, it learns backwards-in-time, thus allowing MRNPG to control the error
in later time-steps, and consequently avoiding the overshoot displayed by MADPG.
Hence MRNPG shows good performance for a wide range of N .
6. Conclusion. This paper has addressed the problem of achieving a Nash equilibrium in a General-Sum Mean-Field Type Game (GS-MFTG) within a CooperativeCompetitive (CC) multi-agent setting. The paper has developed the Multi-player
Receding-horizon Natural Policy Gradient (MRNPG) algorithm. We have then shown
linear convergence to the Nash equilibrium (NE) of the MFTG, relaxing the need of
system noise conditions and covariance matrix estimation, with a diagonal dominance
condition. The theoretical results have been corroborated through numerical analysis
and a comparison with benchmark algorithms (MADPG and MF-MARL), showing
good convergence of MRNPG for a large range of N . The main limitation of the
present work is that in order to have a full analysis, we worked within a linear-quadratic
structure. In future work, we plan to study more complex CC settings, based on the
multi-player receding-horizon algorithm we have developed here.

Appendix
Appendix A. Proof of Lemma 4.4. [Generality of the Diagonal Dominance Condition] Let us start by introducing some of the variables used in the SN
condition [24].
s
ÏÌ„ := Ïâˆ— + N Î³B,0
Ïâˆ— := max

TÏˆ
1
+
Ïƒ X ÏƒR
20T 2


max

0â‰¤tâ‰¤T âˆ’1

At +

N
X


Bti Ktiâˆ— , 1 + Î´ , Î´ > 0

t=1

 i
i
Ïˆ := max JËœy,0
(K i,(0) , K âˆ’iâˆ— ) âˆ’ JËœy,0
(K iâˆ— , K âˆ’iâˆ— )
iâˆˆ[N ]
n

o
âŠ¤
Ïƒ X := min Ïƒmin E[x0 xâŠ¤
0 ] , Ïƒmin E[Ï‰t Ï‰t ]
Furthermore Î£K âˆ— :=
written down as,

PT âˆ’1
t=0

 âˆ— K âˆ— âŠ¤ 
âˆ—
Kâˆ—
Î£K
= E xK
xt
. The SN condition can be
t
t , Î£t

5


i
Î³ 4 (maxi (JËœy,0
(K iâˆ— , K âˆ’iâˆ— ) + Ïˆ)4 ÏÌ„2T âˆ’ 1 2
ÏƒX
2 2 B,0
> 20m(N âˆ’ 1) T
2 Ïƒ2
ÏƒQ
ÏÌ„2 âˆ’ 1
Î£K âˆ—
R

2T
âˆ’1 2
> 1 because
The first thing to notice is that the term in the SN condition ÏÌ„ÏÌ„2 âˆ’1
ÏÌ„ > 1 (by definition) hence can be discarded. Furthermore Ïˆ > 0 and using Courant13

Fischer theorem we can deduce âˆ¥Î£K âˆ— âˆ¥ â‰¥ Ïƒ X , hence the SN condition can be simplified
Ïƒy2 â‰¥ Ïƒ X

2

>

âˆš

20m(N âˆ’ 1)

2
i
Î³B,0
(maxi (JËœy,0
(K iâˆ— , K âˆ’iâˆ— ))2
ÏƒQ ÏƒR

This can equivalently be written as
ÏƒR >
â‰¥

âˆš
âˆš

20m(N âˆ’ 1)

2
i
2
i
âˆš
Î³B,0
(maxi (JËœy,0
(K iâˆ— , K âˆ’iâˆ— ))2
Î³B,0
Ïƒy2 (Î³P,1
)2
â‰¥
20m(N
âˆ’
1)
ÏƒQ Ïƒy2
ÏƒQ Ïƒy2

2
i
20m(N âˆ’ 1)Î³B,0
Î³P,1

where the second inequality is obtained using Lemma 13 in [24] and the
âˆš last one
i
i
is obtained using
the
fact
that
P
=
Q
for
any
T
.
By
observation
20m(N âˆ’
T
T
p
2
i
2
i
1)Î³B,0
Î³P,1
> 2m(N âˆ’ 1)Î³B,0
Î³P,1
hence if the SN condition (Assumption 4 in [24]) is
true then Assumption 4.3 is also true and hence is more general than the SN condition.
Appendix B. Proof of Theorem 4.5. [Linear convergence of NPG] We
prove for a given t âˆˆ {0, . . . , T âˆ’ 1} the linear rate of convergence of natural policy
gradient (3.8) for the control policies (Kti )iâˆˆ[N ] with the set of future controllers
(Ksi )s>t,iâˆˆ[N ] fixed. The techniques for showing the same result for (KÌ„ti )iâˆˆ[N ] is quite
similar and hence is omitted. The linear convergence result follows due to the PL
condition (Lemma 4.1) for a given t âˆˆ {0, . . . , T âˆ’ 1}. We first introduce the set of
local -NE policies (KÌƒtiâˆ— )iâˆˆ[N ] which satisfy the equation (3.3) for all i âˆˆ [N ]. These are
essentially the targets that we want (Kti )iâˆˆ[N ] to achieve.
First we define the natural policy gradient for agent i if all agents are following
policies ((Ks1 )tâ‰¤sâ‰¤T âˆ’1 , . . . , (KsN )tâ‰¤sâ‰¤T âˆ’1 )
i i
i âŠ¤ i
Eti := âˆ‡iy,t (K i , K âˆ’i )Î£âˆ’1
y = Rt Kt âˆ’ (Bt ) Pt+1 (At âˆ’

N
X

Btj Ktj ).

j=1

Ëœ iy,t (K i , K âˆ’i )Î£âˆ’1
The policy update (3.8) in Algorithm 3.1 uses âˆ‡
y which is a stochastic
i
approximation of Et . Now we define another set of controls for a given i âˆˆ [N ],
(K i , KÌƒ âˆ’iâˆ— ) where the player i âˆˆ [N ] has control policy (Ksi )tâ‰¤sâ‰¤T âˆ’1 but the other
j
agents j Ì¸= i follow the local-NE only at time t, (KÌƒtjâˆ— , Kt+1
, . . . , KTj âˆ’1 ). This set of
policies (K i , KÌƒ âˆ’iâˆ— ) is if all players j Ì¸= i somehow achieved their respective local-NE
policies but player i is following policy Kti . This set of policies will be useful in the
analysis of the algorithm. The natural policy gradient under this set of policies is
given as
X j jâˆ—
i
Etiâˆ— = âˆ‡iy,t (K i , KÌƒ âˆ’iâˆ— )Î£yâˆ’1 = Rti Kti âˆ’ (Bti )âŠ¤ Pt+1
(At âˆ’ Bti Kti âˆ’
Bt KÌƒt ).
jÌ¸=i

Notice that the policy update (3.8) in Algorithm 3.1 is not an estimate of Etiâˆ— . Now
i
we introduce some properties of the cost JËœy,t
for the set of controllers (K i , KÌƒ âˆ’iâˆ— )
including the PL condition
i
i
Lemma B.1. Given that K iâ€² = (Ktiâ€² , Kt+1
, . . .), the cost function JËœy,t
satisfies the
following smoothness property:
i
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) = T r[((Ktiâ€² âˆ’ Kti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )

(B.1)

(Ktiâ€² âˆ’ Kti ) + 2(Ktiâ€² âˆ’ Kti )âŠ¤ Etiâˆ— )Î£y ].
14

i
Furthermore, the cost function JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) also satisfies the PL growth condition
with respect to policy gradient âˆ‡iy,t (KÌƒ i , K âˆ’iâˆ— ) and natural policy gradient Etiâˆ— :
Ïƒy
i
i
JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) â‰¤
âˆ¥E iâˆ— âˆ¥2
ÏƒR t F
1
â‰¤
âˆ¥âˆ‡iy,t (K i , KÌƒ âˆ’iâˆ— )âˆ¥2F
ÏƒR Ïƒy
i
where the value function matrix Py,t
is defined recursively by

(B.2)
i
Py,t
= Qit + (Kti )T Rti Kti + (At âˆ’

N
X

i
Btj Ktj )âŠ¤ Py,t+1
(At âˆ’

j=1

N
X

i
Btj Ktj ), Py,T
= QiT .

j=1

and Ïƒy is the minimum eigenvalue of Î£y .
The proof of this Lemma is similar to that of Lemma 4.1 and thus omitted. Before
starting the proof of Theorem 4.5, we state the bound on learning rates Î· i and smallest
singular value Ïƒy of matrix Î£y

1
i
(B.3)
Î· â‰¤ min
2
i Bi
i
i
i
i
âŠ¤
4 Rt + (Bt ) Pt+1 Bt + 3 Rti + (Bti )âŠ¤ Pt+1
t

1
,
i Bi 2 + 1
2 Rti + (Bti )âŠ¤ Pt+1
t
where ÏƒR is the LSV of matrices (Rti )tâˆˆ{0,...,T âˆ’1} . For simplicity of analysis we
will set Î£y = Ïƒy I. The bound on learning rate Î· i is standard in Policy Gradient
literature [11, 12]. The update (3.8) in Algorithm 3.1 employs stochastic natural
i
âˆ’i
Ëœ iy,t (K i , K âˆ’i )Î£âˆ’1
Ëœi
policy gradient EÌƒti := âˆ‡
y where âˆ‡y,t (K , K ) is the stochastic policy
gradient (3.7). Using Lemma 4.2, if the smoothing radius r = O(Ïµ) and mini-batch size
Nb = Î˜(log(1/Î´)/Ïµ2 ), we can obtain the approximation error in stochastic gradient
Ëœ i (K i , K âˆ’i ) âˆ’ âˆ‡i (K i , K âˆ’i )âˆ¥ = O(Ïµ) with probability 1 âˆ’ Î´.
Î´ti := âˆ¥âˆ‡
y,t
y,t
We denote the updated controller after one step of stochastic natural policy
gradient as K iâ€² , which is defined as follows:
Ktiâ€² = Kti âˆ’ Î· i EÌƒti ,
= Kti âˆ’ Î· i (Eti + âˆ†Eti ), where âˆ†Eti = EÌƒti âˆ’ Eti ,
(B.4)

= Kti âˆ’ Î· i (Etiâˆ— + âˆ†Eti + âˆ†Etiâˆ— ), where âˆ†Etiâˆ— = Eti âˆ’ Etiâˆ—

Now we characterize the difference in the costs produced by the update of the controller
for agent i from Kti to Ktiâ€² (given that the other players follow the NE controllers
(Ktjâˆ— )jÌ¸=i ) at the given time t. The future controllers (Ksi )s>t,iâˆˆ[N ] are assumed to
i
be fixed and the set of value matrices (Pt+1
)iâˆˆ[N ] is a function of the set of future
controllers (B.2). To conserve space in the following analysis we use the notation
2
2
|||A||| := AâŠ¤ A and |||A|||B := AâŠ¤ BA for matrices A and B of appropriate dimensions.
Using Lemma B.1 we get
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— )
i
= Ïƒy T r[(Ktiâ€² âˆ’ Kti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )(Ktiâ€² âˆ’ Kti ) + 2(Ktiâ€² âˆ’ Kti )âŠ¤ Etiâˆ— )],
i
= Ïƒy T r[(Î· i )2 (Etiâˆ— + âˆ†Eti + âˆ†Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )(Etiâˆ— + âˆ†Eti + âˆ†Etiâˆ— )]

âˆ’ 2Î· i T r[(Etiâˆ— + âˆ†Eti + âˆ†Etiâˆ— )âŠ¤ Etiâˆ— Î£y ]
15

Simplifying the expression
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— )

2
= Ïƒy T r (Î· i )2 Etiâˆ— (Ri +(B i )âŠ¤ P i

i
t+1 Bt )

t

t

i
+ 2(âˆ†Etiâˆ— + âˆ†Eti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )Etiâˆ—

+

âˆ†Etiâˆ—

2
i
+ 2(âˆ†Eti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ†Etiâˆ—
i
Bti )
(Rti +(Bti )âŠ¤ Pt+1

+

âˆ†Eti



2
âˆ’ 2Î· i Ïƒy T r
i
Bti )
(Rti +(Bti )âŠ¤ Pt+1


â‰¤ Ïƒy (Î· i )2 T r

Etiâˆ—

Etiâˆ—

2



âˆ’ 2Î· i T r Î£y (âˆ†Eti + âˆ†Etiâˆ— )âŠ¤ Etiâˆ—

 (Î· i )2
2
+
Tr
i
Bti )
(Rti +(Bti )âŠ¤ Pt+1
2

+ 2(Î· i )2 T r
3
+ (Î· i )2 T r
2

i
(âˆ†Etiâˆ— + âˆ†Eti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti )

2
âˆ†Etiâˆ— (Ri +(B i )âŠ¤ P i B i ) + 3(Î· i )2 T r

âˆ’ 2Î· i Ïƒy T r

Etiâˆ—

t

2

t

t+1

+ Î· i Ïƒy T r

2

âˆ†Eti

t

Etiâˆ—

2

+

2

Etiâˆ—

Î·i
Tr
Ïƒy


2
i
Bti )
(Rti +(Bti )âŠ¤ Pt+1

Î£y (âˆ†Eti + âˆ†Etiâˆ— )

2

where we have used the fact that 2T r(AâŠ¤ B) â‰¤ T r(AâŠ¤ A) + T r(B âŠ¤ B) to obtain the
inequality. We further have the following inequalities (bounds):
(B.5)
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— )

2
â‰¤ Ïƒy (Î· i )2 T r( Etiâˆ— (Ri +(B i )âŠ¤ P i
t

t

i
t+1 Bt )

)

(Î· i )2
2
2
i
T r( Etiâˆ— ) + 2(Î· i )2 T r( (âˆ†Etiâˆ— + âˆ†Eti )âŠ¤ (Rti + (Bti )âŠ¤ Pt+1
Bti ) )
2

3
2
2
+ (Î· i )2 T r( âˆ†Etiâˆ— (Ri +(B i )âŠ¤ P i B i ) ) + 3(Î· i )2 T r( âˆ†Eti (Ri +(B i )âŠ¤ P i B i ) )
t
t
t
t
t+1 t
t+1 t
2

+

2

2

2

âˆ’ Î· i Ïƒy T r( Etiâˆ— ) + Î· i Ïƒy T r( âˆ†Eti
+ âˆ†Etiâˆ— )



(Î· i )2
2
i
âˆ’ Î· i T r (Etiâˆ— )âŠ¤ Etiâˆ—
â‰¤ Ïƒy (Î· i )2 Rti + (Bti )âŠ¤ Pt+1
Bti +
2


2
i
i
Bti + Î· i
+ Ïƒy 4(Î· i )2 Rti + (Bti )âŠ¤ Pt+1
Bti + 3(Î· i )2 Rti + (Bti )âŠ¤ Pt+1

âˆ¥âˆ†Eti âˆ¥2F + âˆ¥âˆ†Etiâˆ— âˆ¥2F
By definition we know that Î· i â‰¤

min

1
i Bi
4 Rti + (Bti )âŠ¤ Pt+1
t

2

i Bi
+ 3 Rti + (Bti )âŠ¤ Pt+1
t

,

i Bi
2 Rti + (Bti )âŠ¤ Pt+1
t

2

Thus using this bound (B.5) can be written as
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— )

(B.6)



â‰¤ âˆ’Î· i Ïƒy T r (Etiâˆ— )âŠ¤ Etiâˆ— + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F + âˆ¥âˆ†Etiâˆ— âˆ¥2F
16


.

1
+1

Using the definition of âˆ†Etiâˆ— we can deduce that
i
âˆ¥âˆ†Etiâˆ— âˆ¥F = (Bti )âŠ¤ Pt+1

X

Btj (Ktj âˆ’ Ktjâˆ— )
F

jÌ¸=i

X j
X j
i
2
â‰¤ âˆ¥Bti âˆ¥F âˆ¥Pt+1
âˆ¥F
âˆ¥Bt âˆ¥F âˆ¥Ktj âˆ’ Ktjâˆ— âˆ¥F â‰¤ Î³B
Î³P,t+1
âˆ¥Kt âˆ’ Ktjâˆ— âˆ¥F
jÌ¸=i

jÌ¸=i

where Î³B := maxiâˆˆ[N ],t>0 âˆ¥Bti âˆ¥F and Î³P,t := maxiâˆˆ[N ] âˆ¥Pti âˆ¥F . Using these bound we
can re-write (B.6) as
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— )

X
2

4 2
â‰¤ âˆ’Î· i Ïƒy T r (Etiâˆ— )âŠ¤ Etiâˆ— + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F + 2Î· i Î³B
Î³P,t+1 Ïƒy
âˆ¥Ktj âˆ’ Ktjâˆ— âˆ¥2F
jÌ¸=i
i
i
â‰¤ âˆ’Î· ÏƒR (JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )) + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F
i

4 2
+ 2Î· i Î³B
Î³P,t+1 Ïƒy

X
2
âˆ¥Ktj âˆ’ Ktjâˆ— âˆ¥F
jÌ¸=i

where the second inequality is due to the gradient domination condition in Lemma
i
i
B.1. Next we characterize the cost difference JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ):
i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )
i
i
i
i
= JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) + JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )



i
i
â‰¤ 1 âˆ’ Î· i ÏƒR JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F
4 2
+ 2Î· i Î³B
Î³P,t+1 Ïƒy

X
2
j
jâˆ—
âˆ¥Kt âˆ’ KÌƒt âˆ¥F
jÌ¸=i



i
i
â‰¤ 1 âˆ’ Î· i ÏƒR (JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )) + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F
+ 2Î· i

4 2
Î³B
Î³P,t+1 (N âˆ’ 1) X j
j
JËœy,t (K j , KÌƒ âˆ’jâˆ— ) âˆ’ JËœy,t
(KÌƒ jâˆ— , KÌƒ âˆ’jâˆ— )
ÏƒR
jÌ¸=i

where the last inequality follows by utilizing the smoothness condition (B.1) in Lemma
B.1 as shown below:
(B.7)

X

j
j
JËœy,t
(K j , KÌƒ âˆ’jâˆ— ) âˆ’ JËœy,t
(KÌƒ jâˆ— , KÌƒ âˆ’jâˆ— )

jÌ¸=i

=

X

j
T r[((Ktj âˆ’ KÌƒtjâˆ— )âŠ¤ (Rtj + (Btj )âŠ¤ Pt+1
Btj )(Ktj âˆ’ KÌƒtjâˆ— ))Î£y ]

jÌ¸=i

â‰¥ Ïƒy ÏƒR

X
jÌ¸=i

âˆ¥Ktj âˆ’ KÌƒtjâˆ— âˆ¥2F â‰¥

Ïƒy ÏƒR
N âˆ’1

X
2
âˆ¥Ktj âˆ’ KÌƒtjâˆ— âˆ¥F
jÌ¸=i

Assume that âˆ¥âˆ†Eti âˆ¥2F â‰¤ Î´t and define Î·Ì„ := maxiâˆˆ[N ] Î· i . Now if we characterize the
17

sum of difference, of costs
N
X

PN

i=1


i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) we get


i
i
JËœy,t
(K iâ€² , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )

i=1



N 
X
m(N âˆ’ 1) 4 2
i
Î³B Î³P,t+1
Â·
â‰¤
1 âˆ’ Î· ÏƒR âˆ’ 2
ÏƒR
i=1

(B.8)


i
i
JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) + 2Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F
 N

N
X
ÏƒR X Ëœi
i
â‰¤ 1âˆ’Î·
(Jy,t (K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )) + 2
Î· i Ïƒy âˆ¥âˆ†Eti âˆ¥2F
2 i=1
i=1
X

N
ÏƒR
i
(JËœi (K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )) + 2N Î·Ì„ 2 Ïƒy Î´t
â‰¤ 1âˆ’Î·
2 i=1 y,t

where the second inequality is obtained using the diagonal dominance condition
2
4 2
ÏƒR
â‰¥ 2m(N âˆ’ 1)Î³B
Î³P,t+1 and Î· := miniâˆˆ[N ] Î· i . Now let us define a sequence of
i,k
controllers (Kt )iâˆˆ[N ] for k = {0, 1, 2, . . .} such that Kti,k+1 = Kti,k âˆ’ Î· i EÌƒti,k where
EÌƒti,k is the stochastic policy gradient at iteration k. Using (B.8) we can write
(B.9)

N
X

i
i
JËœy,t
(K i,k , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )

i=1

â‰¤ (1 âˆ’ Î·ÏƒR /2)k

N
X

i
i
(JËœy,t
(K i,0 , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ))

i=1

+ 2N Î·Ì„ 2 Ïƒy Î´t

k
X
(1 âˆ’ Î·ÏƒR /(2Ïƒy ))j
j=0

â‰¤ (1 âˆ’ Î·ÏƒR /2)k

N
X

i
i
(JËœy,t
(K i,0 , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )) +

i=1

4N Î·Ì„ 2 Ïƒy2
Î´t
Î·ÏƒR

We can use (H.3) and (B.7) to write down the following inequalities:
i
i
JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) â‰¥ Ïƒy ÏƒR âˆ¥Kti âˆ’ KÌƒtiâˆ— âˆ¥2F ,
JËœi (K i , KÌƒ âˆ’iâˆ— ) âˆ’ JËœi (KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— )
y,t

(B.10)

y,t

2
â‰¤ Ïƒy (Î³R + Î³B
Î³P,t+1 )âˆ¥Kti âˆ’ Ktiâˆ— âˆ¥2F + eÌ„âˆ¥Kti âˆ’ Ktiâˆ— âˆ¥F



These inequalities can be utilized to upper bound the difference between output of the
algorithm and the Nash equilibrium controllers. Using (B.10) and (B.9) we get
N
X
âˆ¥Kti,k âˆ’ KÌƒtiâˆ— âˆ¥2F â‰¤
i=1


k X
N
2
4N Î·Ì„ 2 Ïƒy
ÏƒR
(Î³R + Î³B
Î³P,t+1 )âˆ¥Kti,0 âˆ’ Ktiâˆ— âˆ¥2F + eÌ„âˆ¥Kti,0 âˆ’ Ktiâˆ— âˆ¥F
Î´
+
1
âˆ’
Î·
t
2
Î·ÏƒR
2
ÏƒR
i=1

PN
i
i
Hence if k = Î˜ log 1Ïµ and Î´t = O(Ïµ), then i=1 JËœy,t
(K i,k , KÌƒ âˆ’iâˆ— )âˆ’JËœy,t
(KÌƒ iâˆ— , KÌƒ âˆ’iâˆ— ) =
i,k
O(Ïµ) and using (B.1) we can deduce that âˆ¥Kt âˆ’ KÌƒtiâˆ— âˆ¥2 = O(Ïµ) for i âˆˆ [N ]. This linear
18

i
rate of convergence can also be proved for the cost JËœxÌ„,t
using similar techniques, and
hence we do not include it here.

Appendix C. Proof of Theorem 4.6. [Linear convergence of Alg. 3.1]
Definitions: Throughout the proof we refer to the output of the inner loop of
Algorithm 3.1 as the set of output controllers (Kti )iâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} . In the proof
we use two other sets of controllers as well. The first set (Ktiâˆ— )iâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} is
the NE as characterized in Theorem 2.3. The second set is called the local -NE
(as in proof of Theorem 4.5) and is denoted by (KÌƒtiâˆ— )iâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} . The proof
quantifies the error between the output controllers (Kti )iâˆˆ[N ] and the corresponding
NE controllers (Ktiâˆ— )iâˆˆ[N ] by utilizing the intermediate local-NE controllers (KÌƒtiâˆ— )iâˆˆ[N ]
for each time t âˆˆ {T âˆ’ 1, . . . , 0}. For each t the error is shown to depend on error
in future controllers (Ksi )sâ‰¥t,iâˆˆ[N ] and the approximation error âˆ†t introduced by the
NPG update. If âˆ†t = O(Ïµ), then the error between the output and NE controllers is
shown to be O(Ïµ).
Let us start by denoting the NE value function matrices for agent i âˆˆ [N ] at time
t âˆˆ {0, 1, . . . , T }, under the NE control matrices (Ksiâˆ— )iâˆˆ[N ],sâˆˆ{t+1,...,T âˆ’1} by Ptiâˆ— . The
NE control matrices can be characterized as:
i
iâˆ—
Ktiâˆ— := argmin JËœy,t
((Kti , Kt+1
, . . . , KTiâˆ—âˆ’1 ), K âˆ’iâˆ— )
Kti

iâˆ—
iâˆ—
= âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t

(C.1)

P
j jâˆ—
where Aiâˆ—
t := At +
jÌ¸=i Bt Kt . The NE value function matrices are defined recursively
using the NE controllers as
(C.2)

âŠ¤ iâˆ—
iâˆ—
Ptiâˆ— = Qit + (Aiâˆ—
t ) Pt+1 At
âŠ¤ iâˆ—
i
i
i âŠ¤ iâˆ—
i âˆ’1
iâˆ—
âˆ’ (Aiâˆ—
(Bti )âŠ¤ Pt+1
Aiâˆ—
t ) Pt+1 Bt (Rt + (Bt ) Pt+1 Bt )
t
âŠ¤ iâˆ—
iâˆ—
i iâˆ—
iâˆ—
i
= Qit + (Aiâˆ—
t ) Pt+1 (At + Bt Kt ), PT = QT

The sufficient condition for existence and uniqueness of the set of matrices Ktiâˆ— and
Ptiâˆ— is shown in Theorem 2.3. Let us now define the perturbed value matrices Pti
(resulting from the control matrices (Ksi )iâˆˆ[N ],sâˆˆ{t+1,...,T âˆ’1} ). At time t, let us assume
the existence and uniqueness of the target control matrices (KÌƒtiâˆ— )iâˆˆ[N ] such that
(C.3)

i
i
i
KÌƒtiâˆ— := argmin JËœy,t
(K i , KÌƒ âˆ’iâˆ— ) = âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
AÌƒit
Kti

P
j
where KÌƒ jâˆ— = (KÌƒtjâˆ— , Kt+1
, . . . , KTj âˆ’1 ) and AÌƒit := At + jÌ¸=i Btj KÌƒtjâˆ— . We will show the
existence and uniqueness of the target control matrices given that the inner-loop
convergence in Algorithm 3.1 (Theorem 4.5) is close enough. Using these matrices we
define the value function matrices (PÌƒti )iâˆˆ[N ] as follows:

âŠ¤


N
N
X
X
i
Bti KÌƒtjâˆ—
PÌƒti = Qit + (KÌƒtiâˆ— )âŠ¤ Rti KÌƒtiâˆ— + At +
Btj KÌƒtiâˆ— Pt+1
At +
j=1

(C.4)

j=1

i
i
i
i
= Qit + (AÌƒit )âŠ¤ Pt+1
AÌƒit âˆ’ (AÌƒit )âŠ¤ Pt+1
Bti (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
AÌƒit
i
= Qit + (AÌƒit )âŠ¤ Pt+1
(AÌƒit + Bti KÌƒtiâˆ— ), PÌƒTi = QiT

Finally we define the perturbed value function matrices Pti which result from the
perturbed matrices (Ksi )iâˆˆ[N ],sâˆˆ{t+1,...,T âˆ’1} obtained using the NPG (3.8) in Algorithm
19

3.1:

(C.5)


âŠ¤


N
N
X
X
i
Pti = Qit + (Kti )âŠ¤ Rti Kti + At +
Btj Ktj
Pt+1
At +
Btj Ktj
j=1

j=1

Throughout this proof we assume that the output of the inner loop in Algorithm 3.1,
also called the output matrices Kti , are âˆ†t away from the target matrices KÌƒtiâˆ— , such
that âˆ¥Kti âˆ’ KÌƒtiâˆ— âˆ¥ â‰¤ âˆ†t . We know that
âˆ¥Kti âˆ’ Ktiâˆ— âˆ¥ â‰¤ âˆ¥Kti âˆ’ KÌƒtiâˆ— âˆ¥ + âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥ â‰¤ âˆ†t + âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥.
Motivation for proof: Figure 4a shows the flow of the MRNPG algorithm. The
algorithm utilizes NPG to converge to the NE Ï€tiâˆ— , âˆ€i, for t = T âˆ’ 1, then t =
T âˆ’ 2 and continues in a receding horizon manner (backwards-in-time). Theorem 4.5 proves that the NPG converges to the local NE under the diagonal dominance condition 4.3. Figure 4b shows two distinct cost functions of a given agent
i âˆˆ [N ] in a game with time horizon T = 2. Let the cost CÌƒ i (Â·) be the cost under, MRNPG algorithm at timestep t = 0, after the NE has been approximated
âˆ’i
i
i
and fixed at timestep t = 1 i.e. KÌƒ1iâˆ— â‰ˆ K1iâˆ— , âˆ€i. Hence this
0 , K0 ) =
 costiâˆ—CÌƒ (K
âˆ’i
âˆ’iâˆ—
âˆ’i
âˆ’iâˆ—
âˆ’i
i
iâˆ—
i
i
iâˆ—
i
i
Ëœ
Ëœ
Jy,0 (K0 , KÌƒ1 ), (K0 , KÌƒ1 ) â‰ˆ Jy,0 (K0 , K1 ), (K0 , K1 ) =: C (K0 , K0 ). On
the other hand, let cost C i (Â·) be the cost of player i under Vanilla PG method for
timestep t = 0, where the NE has not been
for timestep t = 1, hence
 approximated
i
C i (K0i , K0âˆ’i ) = JËœy,0
(K0i , K1i ), (K0âˆ’i , K1âˆ’i ) =
Ì¸ C iâˆ— (K0i , K0âˆ’i ) since K1i Ì¸= K1iâˆ— , âˆ€i. For
each cost CÌƒ i or C i NPG ((3.8) Algorithm 3.1) converges to the target/minimum of
the cost (Theorem 4.5). In Figure 4b we denote the minimum of C i by K0iâ€² and the
minimum of CÌƒ i by KÌƒ0iâˆ— . The Hamilton-Jacobi-Isaacs equations [8] in the context of
this two-shot LQ game states that K0iâˆ— = argminK C iâˆ— (K, K0âˆ’iâˆ— ), âˆ€i which coupled
with CÌƒ i â‰ˆ C iâˆ— =â‡’ KÌƒ0i â‰ˆ K0iâˆ— . Hence MRNPG leads to a closer approximation of the
NE than Vanilla PG algorithms.

(a)

(b)

Fig. 4: (a) MRNPG Algorithm converges to the NE using Natural Policy Gradient (NPG)
starting from t = T âˆ’ 1 and moving in a receding horizon manner (backwards-in-time). (b)
Cost functions (C i and CÌƒ i ) of the ith agent in a two-shot game (T = 2). The arrows denote
how NPG in the previous timestep t = 1 converges to the true NE.
20

Now we obtain an upper bound on the term âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥ using (C.1) and (C.3):
KÌƒtiâˆ— âˆ’ Ktiâˆ—
i
i
iâˆ—
iâˆ—
= âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
AÌƒit + (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t
i
i
= âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
(AÌƒit âˆ’ Aiâˆ—
t )
i
i
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ (Pt+1
âˆ’ Pt+1
)Aiâˆ—
t


i
iâˆ—
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t
X j jâˆ—
jâˆ—
i
i âŠ¤ i
i âˆ’1
i âŠ¤ i
= âˆ’(Rt + (Bt ) Pt+1 Bt ) (Bt ) Pt+1
Bt (KÌƒt âˆ’ Kt )
jÌ¸=i
i
i
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ (Pt+1
âˆ’ Pt+1
)Aiâˆ—
t

i
i âŠ¤ i
i âˆ’1
i
i âŠ¤ iâˆ—
iâˆ—
âˆ’ (Rt + (Bt ) Pt+1 Bt ) âˆ’ (Rt + (Bt ) Pt+1 Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t
i
i
= âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1

X

Btj (KÌƒtjâˆ— âˆ’ Ktjâˆ— )

jÌ¸=i
i
i
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ (Pt+1
âˆ’ Pt+1
)Aiâˆ—
t

i
i
iâˆ—
+ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
âˆ’ Pt+1
iâˆ—
iâˆ—
Bti (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t

where the last equality is due to
i
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 + (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1
i
iâˆ—
iâˆ—
= âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Rti + (Bti )âŠ¤ Pt+1
Bti )(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1
i
i
iâˆ—
+ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Rti + (Bti )âŠ¤ Pt+1
Bti )(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1

i
i
iâˆ—
iâˆ—
= (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
âˆ’ Pt+1
Bti (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âˆ’1

Further analyzing (C) we get
i
i
(Rti + (Bti )âŠ¤ Pt+1
Bti )(KÌƒtiâˆ— âˆ’ Ktiâˆ— ) + (Bti )âŠ¤ Pt+1

X

Btj (KÌƒtjâˆ— âˆ’ Ktjâˆ— )

jÌ¸=i
i
iâˆ—
i iâˆ—
= âˆ’(Bti )âŠ¤ (Pt+1
âˆ’ Pt+1
)(Aiâˆ—
t + Bt K t )
ï£« 1
1
1
Bt1
(Bt1 )âŠ¤ Pt+1
Bt2
Rt + (Bt1 )âŠ¤ Pt+1
2 âŠ¤ 2
2
1
2
ï£¬ (Bt2 )âŠ¤ Pt+1
Bt
Rt + (Bt ) Pt+1 Bt2
ï£­
..
..
.
.
{z
|
Î¦Ìƒt+1

ï£«
(C.6)

ï£¬
=ï£­

1âˆ—
1
1 1âˆ—
(Bt1 )âŠ¤ (Pt+1
âˆ’ Pt+1
)(A1âˆ—
t + Bt Kt )

ï£¶
ï£¶ ï£« 1âˆ—
...
KÌƒt âˆ’ Kt1âˆ—
ï£¬
ï£·
..
. . .ï£·
ï£¸
ï£¸ï£­
.
..
.
KÌƒtN âˆ— âˆ’ KtN âˆ—
}

ï£¶

..
.

ï£·
ï£¸
Nâˆ—
N
âˆ—
N Nâˆ—
(BtN )âŠ¤ (Pt+1
âˆ’ Pt+1
)(AN
+
B
K
)
t
t
t

where Î¦Ìƒâˆ’1
t+1 is guaranteed to exist as shown below. Using (C.6) we now obtain an
upper bound on maxiâˆˆ[N ] âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥:
(C.7)

j
jâˆ—
j
âˆ—
max âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥ â‰¤ âˆ¥Î¦Ìƒâˆ’1
t+1 âˆ¥âˆ¥At âˆ¥ max âˆ¥Bt âˆ¥âˆž âˆ¥Pt+1 âˆ’ Pt+1 âˆ¥âˆž

jâˆˆ[N ]

jâˆˆ[N ]

21

We also define
ï£¶
1
1âˆ—
(Bt1 )âŠ¤ (Pt+1
âˆ’ Pt+1
)
ï£¬
ï£· 1
..
Î¦Ì‚t+1 := Î¦Ìƒt+1 âˆ’ Î¦âˆ—t+1 = ï£­
ï£¸ Bt
.
ï£«

...

BtN



N
Nâˆ—
(BtN )âŠ¤ (Pt+1
âˆ’ Pt+1
)

Next we characterize âˆ¥Î¦Ìƒâˆ’1
t+1 âˆ¥:
âˆ—
âˆ’1
âˆ¥Î¦Ìƒâˆ’1
Î¦Ì‚t+1 )âˆ’1 (Î¦âˆ—t+1 )âˆ’1 âˆ¥
t+1 âˆ¥ = âˆ¥(I + (Î¦t+1 )
âˆž
X
jâˆ— k
i
â‰¤ âˆ¥(Î¦âˆ—t+1 )âˆ’1 âˆ¥
âˆ¥âˆ¥(Î¦âˆ—t+1 )âˆ’1 âˆ¥k max âˆ¥Btj âˆ¥2 âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥
jâˆˆ[N ]

k=0
(âˆ’1)

â‰¤ 2âˆ¥(Î¦âˆ—t+1 )âˆ’1 âˆ¥ â‰¤ 2cÎ¦

(C.8)

j
jâˆ—
where the last inequality is possible due to the fact maxjâˆˆ[N ] âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥ â‰¤
(âˆ’1)

(âˆ’1)

1/(2cÎ¦ c2B ) where cÎ¦ = maxtâˆˆ{0,...,T âˆ’1} âˆ¥(Î¦âˆ—t+1 )âˆ’1 âˆ¥ and cB = maxiâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} âˆ¥Bti âˆ¥.
Hence combining (C.7)-(C.8),
jâˆ—
j
max âˆ¥KÌƒtiâˆ— âˆ’ Ktiâˆ— âˆ¥ â‰¤ cÌ„1 max âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥

(C.9)

jâˆˆ[N ]

jâˆˆ[N ]

(âˆ’1)
cÌ„1 := 2cÎ¦ cB câˆ—A and câˆ—A = âˆ¥Aâˆ—t âˆ¥.

where
Now we characterize the difference Pti âˆ’ Ptiâˆ— = Pti âˆ’ PÌƒti + PÌƒti âˆ’ Ptiâˆ— . First we can
characterize PÌƒti âˆ’ Ptiâˆ— using (C.2) and (C.4):
i
âŠ¤ iâˆ—
iâˆ—
i iâˆ—
PÌƒti âˆ’ Ptiâˆ— = (AÌƒit )âŠ¤ Pt+1
(AÌƒit + Bti KÌƒtiâˆ— ) âˆ’ (Aiâˆ—
t ) Pt+1 (At + Bt Kt )
âŠ¤ i
i
i iâˆ—
iâˆ— âŠ¤
i
iâˆ—
i
i iâˆ—
= (AÌƒit âˆ’ Aiâˆ—
t ) Pt+1 (AÌƒt + Bt KÌƒt ) + (At ) (Pt+1 âˆ’ Pt+1 )(AÌƒt + Bt KÌƒt )

=

X

âŠ¤ iâˆ—
i
i iâˆ—
iâˆ—
i iâˆ—
+ (Aiâˆ—
t ) Pt+1 (AÌƒt + Bt KÌƒt âˆ’ At âˆ’ Bt Kt )
âŠ¤
i
âŠ¤
i
iâˆ—
i
i iâˆ—
Btj (KÌƒtjâˆ— âˆ’ Ktjâˆ— ) Pt+1
(AÌƒit + Bti KÌƒtiâˆ— ) + (Aiâˆ—
t ) (Pt+1 âˆ’ Pt+1 )(AÌƒt + Bt KÌƒt )

jÌ¸=i
âŠ¤ iâˆ—
+ (Aiâˆ—
t ) Pt+1

N
X

Btj (KÌƒtjâˆ— âˆ’ Ktjâˆ— )

j=1

Using this characterization we bound âˆ¥PÌƒti âˆ’ Ptiâˆ— âˆ¥ using the AM-GM, âˆ¥PÌƒti âˆ’ Ptiâˆ— âˆ¥
4
 N
N
X
c4B X jâˆ—
jâˆ—
jâˆ—
jâˆ—
âˆ—
iâˆ—
(C.10)
âˆ¥KÌƒ âˆ’ Kt âˆ¥
â‰¤ 2âˆ¥At âˆ¥âˆ¥Pt+1 âˆ¥cB
âˆ¥KÌƒt âˆ’ Kt âˆ¥ +
2 j=1 t
j=1
X
2
N
jâˆ—
jâˆ—
âˆ¥KÌƒt âˆ’ Kt âˆ¥

+

iâˆ—
âˆ¥Aâˆ—t âˆ¥/2 + âˆ¥Pt+1
âˆ¥cB + âˆ¥Aiâˆ—
t âˆ¥/2 cB

+

i
iâˆ— 2
iâˆ—
âˆ—
i
iâˆ—
âˆ¥Aâˆ—t âˆ¥/2 + 1/2 + âˆ¥Aiâˆ—
t âˆ¥/2 âˆ¥Pt+1 âˆ’ Pt+1 âˆ¥ + âˆ¥At âˆ¥âˆ¥At âˆ¥âˆ¥Pt+1 âˆ’ Pt+1 âˆ¥



j=1



N
X
4
â‰¤ 2câˆ—A câˆ—P cB + (câˆ—A /2 + câˆ—P cB + ciâˆ—
/2)c
+
c
/2
âˆ¥KÌƒtjâˆ— âˆ’ Ktjâˆ— âˆ¥
B
A
B
|
{z
} j=1
cÌ„2

âˆ—
i
iâˆ—
+ câˆ—A /2 + 1/2 + ciâˆ—
/2 + ciâˆ—
A cA âˆ¥Pt+1 âˆ’ Pt+1 âˆ¥
|
{z A
}
cÌ„3

i
iâˆ—
â‰¤ cÌ„2 N max âˆ¥KÌƒtjâˆ— âˆ’ Ktjâˆ— âˆ¥ + cÌ„3 âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥
jâˆˆ[N ]

22

iâˆ—
âˆ—
iâˆ—
where ciâˆ—
A := maxtâˆˆ{0,...,T âˆ’1} âˆ¥At âˆ¥, cP := maxiâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} âˆ¥Pt+1 âˆ¥, and the last
jâˆ—
jâˆ—
i
iâˆ—
inequality follows from the fact that âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥, âˆ¥KÌƒt âˆ’ Kt âˆ¥ â‰¤ 1/N . Similarly
i
i
Pt âˆ’ PÌƒt can be decomposed using (C.4) and (C.5):


âŠ¤


N
N
X
X
i
Pti âˆ’ PÌƒti = (Kti )âŠ¤ Rti Kti + At +
Btj Ktj
Pt+1
At +
Btj Ktj
j=1

j=1



âŠ¤


N
N
X
X
i
âˆ’ (KÌƒtiâˆ— )âŠ¤ Rti KÌƒtiâˆ— + At +
Btj KÌƒtiâˆ— Pt+1
At +
Bti KÌƒtjâˆ— .
j=1

j=1

We start by analyzing the quadratic form xâŠ¤ Pti x:
"
x

âŠ¤

Pti x = xâŠ¤

Qit + (Kti )âŠ¤ Rti Kti +


At +

N
X

Btj Ktj

âŠ¤

i
Pt+1


At +

j=1

N
X

Btj Ktj


x

j=1



X j j
 i
i âŠ¤
i
i âŠ¤ i
i
i i âŠ¤ i
= x (Kt ) Rt + (Bt ) Pt+1 Bt Kt + 2(Bt Kt ) Pt+1 At +
Bt Kt + Qit
âŠ¤

jÌ¸=i


+ At +

X

Btj Ktj

âŠ¤


X j j 
i
Pt+1 At +
Bt Kt x

jÌ¸=i

jÌ¸=i



X j jâˆ— 
 i
âŠ¤
i âŠ¤
i
i âŠ¤ i
i
i i âŠ¤ i
= x (Kt ) Rt + (Bt ) Pt+1 Bt Kt + 2(Bt Kt ) Pt+1 At +
+ Qit
Bt KÌƒt
jÌ¸=i
i
+ 2(Bti Kti )âŠ¤ Pt+1

X



X j jâˆ— âŠ¤
X j jâˆ— 
j
j
jâˆ—
i
Pt+1 At +
Bt (Kt âˆ’ KÌƒt ) + At +
Bt KÌƒt
Bt KÌƒt

jÌ¸=i

+

X

Btj (Ktj âˆ’ KÌƒtjâˆ— )

jÌ¸=i

âŠ¤

i
Pt+1

X

jÌ¸=i


+ 2 At +

jÌ¸=i


Btj (Ktj âˆ’ KÌƒtjâˆ— )

jÌ¸=i

X


X

j jâˆ—
j
j
jâˆ—
i
Bt KÌƒt Pt+1
Bt (Kt âˆ’ KÌƒt ) x

jÌ¸=i

jÌ¸=i

Completing squares, we get
"
(C.11)


i
xâŠ¤ Pti x = xâŠ¤ (Kti âˆ’ KÌƒtiâˆ— )âŠ¤ Rti + (Bti )âŠ¤ Pt+1
Bti (Kti âˆ’ KÌƒtiâˆ— )


X j jâˆ— âŠ¤
X j jâˆ— 
i
+ At +
Bt KÌƒt
Pt+1 At +
Bt KÌƒt
+ Qit
jÌ¸=i

jÌ¸=i


X j jâˆ— âŠ¤
âˆ’1
i
i
âˆ’ At +
Bt KÌƒt
Pt+1
Bti Rti + (Bti )âŠ¤ Pt+1
Bti
jÌ¸=i
i
(Bti )âŠ¤ Pt+1


At +

X
jÌ¸=i

23

Btj KÌƒtjâˆ—



 
 X
âŠ¤
N
N
X
+ 2 At +
Btj KÌƒtjâˆ— +
Btj (Ktj âˆ’ KÌƒtjâˆ— )
j=1

j=1
i
Pt+1

X

Btj (Ktj âˆ’ KÌƒtjâˆ—


x.

jÌ¸=i

Now we take a look at the quadratic xâŠ¤ PÌƒti x:
xâŠ¤ PÌƒti x


âŠ¤


N
N
X
X
i
= xâŠ¤ Qit + (KÌƒtiâˆ— )âŠ¤ Rti KÌƒtiâˆ— + At +
Btj KÌƒtjâˆ— Pt+1
At +
Btj KÌƒtjâˆ— x
j=1

j=1



X j jâˆ— 

i
i
= xâŠ¤ (KÌƒtiâˆ— )âŠ¤ Rti + (Bti )âŠ¤ Pt+1
Bti KÌƒtiâˆ— + 2(Bti KÌƒtiâˆ— )âŠ¤ Pt+1
At +
Bt KÌƒt
jÌ¸=i



+ Qit + At +

X

Btj KÌƒtjâˆ—

âŠ¤



i
Pt+1
At +

jÌ¸=i

(C.12) = x

âŠ¤


At +

X

Btj KÌƒtjâˆ—

N
X

Btj KÌƒtjâˆ—


x

jÌ¸=i

âŠ¤

i
Pt+1


At +

jÌ¸=i

N
X

Btj KÌƒtjâˆ—



+ Qit

jÌ¸=i


X j jâˆ— âŠ¤
âˆ’1
i
i
Pt+1
Bti Rti + (Bti )âŠ¤ Pt+1
âˆ’ At +
Bt KÌƒt
Bti
jÌ¸=i
i
(Bti )âŠ¤ Pt+1


X j jâˆ— 
At +
Bt KÌƒt
x.
jÌ¸=i

Using (C.11) and (C.12), we get
"
x

âŠ¤

(Pti âˆ’ PÌƒti )x = xâŠ¤


i
(Kti âˆ’ KÌƒtiâˆ— )âŠ¤ Rti + (Bti )âŠ¤ Pt+1
Bti (Kti âˆ’ KÌƒtiâˆ— )

âŠ¤
X

 X
 
N
N
X
i
Btj KÌƒtjâˆ— +
Btj (Ktj âˆ’ KÌƒtjâˆ— ) Pt+1
Btj (Ktj âˆ’ KÌƒtjâˆ— x
+ 2 At +
j=1

j=1

jÌ¸=i

"

i
= xâŠ¤ (Kti âˆ’ KÌƒtiâˆ— )âŠ¤ Rti + (Bti )âŠ¤ Pt+1
Bti (Kti âˆ’ KÌƒtiâˆ— )
 

âŠ¤
N
N
N
X
X
X
+ 2 At +
Btj Ktjâˆ— + 2
Btj (KÌƒtjâˆ— âˆ’ Ktjâˆ— ) +
Btj (Ktj âˆ’ KÌƒtjâˆ— )
j=1

j=1

j=1
i
Pt+1

X

Btj (Ktj âˆ’ KÌƒtjâˆ—


x

jÌ¸=i

Using this characterization, we bound âˆ¥Pti âˆ’ PÌƒti âˆ¥:
 i  i
iâˆ—
iâˆ—
iâˆ—
âˆ¥Pti âˆ’ PÌƒti âˆ¥ â‰¤ âˆ¥Rti + (Bti )âŠ¤ Pt+1
Bti âˆ¥ + âˆ¥(Bti )âŠ¤ Pt+1
âˆ’ Pt+1
Bt âˆ¥ âˆ¥Kt âˆ’ KÌƒtiâˆ— âˆ¥
24

+ (2ciâˆ—
A + 2cB

N
X



iâˆ—
i
iâˆ—
âˆ¥KÌƒtjâˆ— âˆ’ Ktâˆ— âˆ¥ + âˆ¥Ktj âˆ’ KÌƒtjâˆ— âˆ¥ âˆ¥Pt+1
âˆ¥ + âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥

j=1

cB

N
X

âˆ¥Ktj âˆ’ KÌƒtjâˆ— âˆ¥

j=1
i
iâˆ—
As before, assuming âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥, âˆ¥KÌƒtjâˆ— âˆ’ Ktjâˆ— âˆ¥ â‰¤ 1/N ,
N
X
âˆ¥Ktj âˆ’ KÌƒtjâˆ— âˆ¥
âˆ¥Pti âˆ’ PÌƒti âˆ¥ â‰¤ cÌ„âˆ— + c2B /2 + 2c2B (câˆ—P + 1) + 2ciâˆ—
A
{z
} j=1
|
cÌ„4

N
X
 i
iâˆ—
âˆ—
âˆ¥P
âˆ¥KÌƒtjâˆ— âˆ’ Ktjâˆ— âˆ¥
+ c2B /2 + ciâˆ—
+
c
âˆ’
P
âˆ¥
+
c
(c
+
1)
B
B P
A
t+1
t+1
}
| {z
{z
}
|
j=1
cÌ„6

cÌ„5

(C.13)

i
iâˆ—
â‰¤ cÌ„4 N âˆ†t + cÌ„5 âˆ¥Pt+1
âˆ’ Pt+1
âˆ¥ + cÌ„6

N
X

âˆ¥KÌƒtjâˆ— âˆ’ Ktjâˆ— âˆ¥

j=1
j
iâˆ—
where cÌ„âˆ— := maxiâˆˆ[N ],tâˆˆ{0,...,T âˆ’1} âˆ¥Rti +(Bti )âŠ¤ Pt+1
Bti âˆ¥. Let us define eK
t := maxjâˆˆ[N ] âˆ¥Kt âˆ’
j
jâˆ—
Ktjâˆ— âˆ¥, eP
t := maxjâˆˆ[N ] âˆ¥Pt âˆ’ Pt âˆ¥. Using (C.9), (C.10) and (C.13) we get
P
eK
t â‰¤ cÌ„1 et+1 + âˆ†t
K
P
eP
t â‰¤ (cÌ„2 + cÌ„6 )N et + (cÌ„3 + cÌ„5 )et+1 + cÌ„4 N âˆ†

+ (cÌ„2 + cÌ„4 + cÌ„6 )N âˆ†t
â‰¤ (cÌ„1 (cÌ„2 + cÌ„6 )N + cÌ„3 + cÌ„5 ) eP
|
{z
} t+1 |
{z
}
cÌ„7

cÌ„8

Using this recursive definition, we deduce
T âˆ’t P
eP
eT + cÌ„8
t â‰¤ cÌ„7

T
âˆ’1
X

cÌ„s7 âˆ†t+s = cÌ„8

s=0

T
âˆ’1
X

cÌ„s7 âˆ†t+s

s=0

K
Hence if âˆ† = O(Ïµ), in particular âˆ†t â‰¤ Ïµ/(2cÌ„1 cÌ„t7 cÌ„8 T ), then eP
t â‰¤ Ïµ/2cÌ„1 and et â‰¤ Ïµ for
t âˆˆ {0, . . . , T âˆ’ 1}.

REFERENCES
[1] Kaiqing Zhang, Zhuoran Yang, and Tamer BasÌ§ar. Multi-agent reinforcement learning: A
selective overview of theories and algorithms. Handbook of Reinforcement Learning and
Control, pages 321â€“384, 2021.
[2] Yingying Li, Yujie Tang, Runyu Zhang, and Na Li. Distributed reinforcement learning for
decentralized linear quadratic control: A derivative-free policy optimization approach. IEEE
Transactions on Automatic Control, 67(12):6429â€“6444, 2021.
[3] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pages 157â€“163. Elsevier, 1994.
[4] Noureddine Toumi, Roland MalhameÌ, and Jerome Le Ny. A tractable mean field game model
for the analysis of crowd evacuation dynamics. In 2020 59th IEEE Conference on Decision
and Control (CDC), pages 1020â€“1025. IEEE, 2020.
[5] Johann Lussange, Ivan Lazarevich, Sacha Bourgeois-Gironde, Stefano Palminteri, and Boris
Gutkin. Modelling stock markets by multi-agent reinforcement learning. Computational
Economics, 57:113â€“147, 2021.
[6] V Krishna and VC Ramesh. Intelligent agents for negotiations in market games. i. model. IEEE
Transactions on Power Systems, 13(3):1103â€“1108, 1998.
25

[7] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learningâ€“a simple, efficient,
decentralized algorithm for multiagent RL. arXiv preprint arXiv:2110.14555, 2021.
[8] Tamer BasÌ§ar and Geert Jan Olsder. Dynamic Noncooperative Game Theory. SIAM, 1998.
[9] Minyi Huang, Roland P MalhameÌ, and Peter E Caines. Large population stochastic dynamic
games: Closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle.
Communications in Information & Systems, 6(3):221â€“252, 2006.
[10] Jean-Michel Lasry and Pierre-Louis Lions. Jeux aÌ€ champ moyen. iâ€“le cas stationnaire. Comptes
Rendus MatheÌmatique, 343(9):619â€“625, 2006.
[11] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning, pages 1467â€“1476. PMLR, 2018.
[12] Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and Martin
Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. In The 22nd International Conference on Artificial Intelligence and Statistics,
pages 2916â€“2925. PMLR, 2019.
[13] Ivan Ivanov and Boyan Lomev. Numerical properties of stochastic linear quadratic model with
applications in finance. TOJSAT, 2(3):41â€“46, 2012.
[14] Thomas J Sargent and Lars Ljungqvist. Recursive Macroeconomic Theory. Massachusetss
Institute of Technology, 2000.
[15] Pierre Cardaliaguet and Charles-Albert Lehalle. Mean field game of controls and an application
to trade crowding. Mathematics and Financial Economics, 12(3):335â€“363, 2018.
[16] Minyi Huang, Peter E Caines, and Roland P MalhameÌ. Individual and mass behaviour in large
population stochastic wireless power control problems: Centralized and Nash equilibrium
solutions. In IEEE Conference on Decision and Control, volume 1, pages 98â€“103. IEEE,
2003.
[17] Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence
of actor-critic: A case for linear quadratic regulator with ergodic cost. In Advances in
Neural Information Processing Systems, pages 8351â€“8363, 2019.
[18] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. The Journal of
Machine Learning Research, 22(1):4431â€“4506, 2021.
[19] Guannan Qu, Chenkai Yu, Steven Low, and Adam Wierman. Exploiting linear models for
model-free nonlinear control: A provably convergent policy gradient approach. In 2021 60th
IEEE Conference on Decision and Control (CDC), pages 6539â€“6546. IEEE, 2021.
[20] ReneÌ Carmona, Jean-Pierre Fouque, and Li-Hsien Sun. Mean field games and systemic risk.
Communications in Mathematical Sciences, 13(4):911â€“933, 2015.
[21] Sina Sanjari, Naci Saldi, and Serdar YuÌˆksel. Nash equilibria for exchangeable team against
team games and their mean field limit. arXiv preprint arXiv:2210.07339, 2022.
[22] ReneÌ Carmona, Kenza Hamidouche, Mathieu LaurieÌ€re, and Zongjun Tan. Policy optimization
for linear-quadratic zero-sum mean-field type games. In 2020 59th IEEE Conference on
Decision and Control (CDC), pages 1038â€“1043. IEEE, 2020.
[23] Xiangyuan Zhang and Tamer BasÌ§ar. Revisiting LQR control from the perspective of recedinghorizon policy gradient. IEEE Control Systems Letters, 2023.
[24] Ben Hambly, Renyuan Xu, and Huining Yang. Policy gradient methods find the Nash equilibrium
in N-player general-sum linear-quadratic games. Journal of Machine Learning Research,
24(139), 2023.
[25] Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer BasÌ§ar. On improving model-free algorithms
for decentralized multi-agent reinforcement learning. In International Conference on
Machine Learning, pages 15007â€“15049. PMLR, 2022.
[26] Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. Advances in
Neural Information Processing Systems, 32, 2019.
[27] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field
games. In International Conference on Autonomous Agents and Multiagent Systems, pages
251â€“259, 2019.
[28] Romuald Elie, Julien Perolat, Mathieu LaurieÌ€re, Matthieu Geist, and Olivier Pietquin. On
the convergence of model free learning in mean field games. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 34, pages 7143â€“7150, 2020.
[29] Muhammad Aneeq Uz Zaman, Sujay Bhatt, and Tamer BasÌ§ar. Adversarial linear-quadratic
mean-field games over multigraphs. In 2021 60th IEEE Conference on Decision and Control
(CDC), pages 209â€“214. IEEE, 2021.
[30] Andrea Angiuli, Jean-Pierre Fouque, and Mathieu LaurieÌ€re. Unified reinforcement Q-learning
for mean field game and control problems. Mathematics of Control, Signals, and Systems,
26

34(2):217â€“271, 2022.
[31] Mathieu Lauriere, Sarah Perrin, Sertan Girgin, Paul Muller, Ayush Jain, Theophile Cabannes,
Georgios Piliouras, Julien PeÌrolat, Romuald Elie, Olivier Pietquin, et al. Scalable deep
reinforcement learning algorithms for mean field games. In International Conference on
Machine Learning, pages 12078â€“12095. PMLR, 2022.
[32] Muhammad Aneeq Uz Zaman, Erik Miehling, and Tamer BasÌ§ar. Reinforcement learning for
non-stationary discrete-time linearâ€“quadratic mean-field games in multiple populations.
Dynamic Games and Applications, pages 118â€“164, March 2023.
[33] Muhammad Aneeq Uz Zaman, Alec Koppel, Sujay Bhatt, and Tamer BasÌ§ar. Oracle-free
reinforcement learning in mean-field games along a single sample path. In International
Conference on Artificial Intelligence and Statistics, pages 10178â€“10206. PMLR, 2023.
[34] Mathieu LaurieÌ€re, Sarah Perrin, Matthieu Geist, and Olivier Pietquin. Learning mean field
games: A survey. arXiv preprint arXiv:2205.12944, 2022.
[35] Kaiqing Zhang, Zhuoran Yang, and Tamer BasÌ§ar. Policy optimization provably converges
to Nash equilibria in zero-sum linear quadratic games. Advances in Neural Information
Processing Systems, 32, 2019.
[36] Jingjing Bu, Lillian J Ratliff, and Mehran Mesbahi. Global convergence of policy gradient for
sequential zero-sum linear quadratic dynamic games. arXiv preprint arXiv:1911.04672,
2019.
[37] ReneÌ Carmona, Kenza Hamidouche, Mathieu LaurieÌ€re, and Zongjun Tan. Linear-quadratic
zero-sum mean-field type games: Optimality conditions and policy optimization. Journal
of Dynamics & Games, 8(4), 2021.
[38] Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, and Mihailo Jovanovic. Independent policy
gradient for large-scale markov potential games: Sharper rates, function approximation,
and game-agnostic convergence. In International Conference on Machine Learning, pages
5166â€“5220. PMLR, 2022.
[39] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in
Neural Information Processing Systems, 30, 2017.
[40] ReneÌ Carmona, Mathieu LaurieÌ€re, and Zongjun Tan. Linear-quadratic mean-field reinforcement
learning: convergence of policy gradient methods. arXiv preprint arXiv:1910.04295, 2019.
[41] Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-field controls with Q-learning for
cooperative MARL: convergence and complexity analysis. SIAM Journal on Mathematics
of Data Science, 3(4):1168â€“1196, 2021.
[42] ReneÌ Carmona, Mathieu LaurieÌ€re, and Zongjun Tan. Model-free mean-field reinforcement
learning: mean-field MDP and mean-field Q-learning. The Annals of Applied Probability,
33(6B):5334â€“5381, 2023.
[43] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer BasÌ§ar. Robust
multi-agent reinforcement learning with model uncertainty. Advances in Neural Information
Processing Systems, 33:10571â€“10583, 2020.
[44] Xiangyuan Zhang, Bin Hu, and Tamer BasÌ§ar. Learning the Kalman filter with fine-grained
sample complexity. arXiv preprint arXiv:2301.12624, 2023.
[45] Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer BasÌ§ar. Toward a
theoretical foundation of policy optimization for learning control policies. Annual Review
of Control, Robotics, and Autonomous Systems, 6:123â€“158, 2023.
[46] Eric Mazumdar, Lillian J Ratliff, Michael I Jordan, and S Shankar Sastry. Policy-gradient
algorithms have no guarantees of convergence in continuous action and state multi-agent
settings. arXiv preprint arXiv:1907.03712, 2019.
[47] Paul Frihauf, Miroslav Krstic, and Tamer BasÌ§ar. Nash equilibrium seeking in noncooperative
games. IEEE Transactions on Automatic Control, 57(5):1192â€“1207, 2011.
[48] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field
multi-agent reinforcement learning. arXiv preprint arXiv:1802.05438, 2018.
[49] Jun Moon, Tyrone E Duncan, and Tamer BasÌ§ar. Risk-sensitive zero-sum differential games.
IEEE Transactions on Automatic Control, 64(4):1503â€“1518, 2018.
[50] George AF Seber and Alan J Lee. Linear regression analysis. John Wiley & Sons, 2012.

27

Supplementary Materials
Appendix D. Adapted Open-Loop Analysis of GS-MFTG. In this section
we characterize the Adapted Open-Loop Nash equilibrium (in short OLNE [49]), which
is shown to satisfy the necessary conditions for Nash Equilibrium (Theorem D.1). The
structure of the OLNE inspires a decomposition which simplifies the computation of
the Nash equilibrium (NE) and enables us to establish its existence and uniqueness
(Theorem 2.3). The required conditions for this result are similar to those of LQ
Games. We consider a set of controls referred to as adapted open-loop Uoi [49] where
control action at time t, uit , is adapted to the noise process {Ï‰00 , Ï‰0 , . . . , Ï‰t0 , Ï‰t }. If
Ui = Uoi then the corresponding NE is called Adapted Open-Loop Nash Equilibrium
(in short OLNE). Below we characterize the OLNE and then prove that the OLNE is
a broader class of NE than the NE. This allows a decomposition of the GS-MFTG
which simplifies the characterization of the NE. This result is distinct from other
results in literature such as LQ-MFGs and Zero-Sum MFTGs, due to the general-sum
competitive-cooperative nature of the problem.
Theorem D.1. (1) All OLNE policies (uiâˆ— )iâˆˆ[N ] are linear in the adjoint processes
(D.16) and (D.17),

(pit )iâˆˆ[N ],0â‰¤t<T

i
i i
iâˆ—
i i
i i
uÌ„iâˆ—
t = âˆ’(Lt + LÌ„t )pÌ„t , ut = âˆ’Lt pt + LÌ„t pÌ„t ,

where âˆ€i âˆˆ [N ], t âˆˆ {0, . . . .T âˆ’ 1} the adjoint processes depend on solutions of Riccati
equations (D.20) and (D.23).
(2) Furthermore, the OLNEâ€™s feedback representation is linear in (xt âˆ’ xÌ„t ) and xÌ„t ,
(D.1)

uit = âˆ’Lit Pti (xt âˆ’ xÌ„t ) âˆ’ (Lit + LÌ„it )PÌ„ti xÌ„t ,

âˆ€i âˆˆ [N ], t âˆˆ {0, . . . .T âˆ’ 1}.
(3) Finally, every OLNE satisfies the necessary conditions for NE.
In the proof we first develop necessary conditions for OLNE using the Stochastic
Maximum Principle and these conditions are shown to be sufficient due to the quadratic
structure of the cost. The OLNE is then shown to satisfy the necessary conditions
for NE. This fact coupled with the feedback structure of OLNE (D.1) suggests that
the NE will be a function of the state deviation from mean-field, (xt âˆ’ xÌ„t ), and the
mean-field xÌ„t .
Proof. We will use the Stochastic Minimum Principle to first characterize the
necessary conditions for the OLNE in terms of an adjoint process. We then study the
adjoint process and provide sufficient conditions for its existence and uniqueness in
terms of solutions to a set of Riccati equations. We then prove that the necessary
conditions for the OLNE are also sufficient due to the quadratic nature of the cost.
This analysis is a generalization of the Zero-Sum MFTG adapted open-loop analysis
in [37] to the General-Sum setting. Additionally we show that OLNE satisfies the
necessary conditions of the NE, by first characterizing the necessary conditions for NE
and then showing that every OLNE satisfies these conditions.
We first characterize the necessary conditions for OLNE. Let us write
down the expression for Adapted Open-Loop Nash Equilibrium (OLNE). An OLNE is
a tuple of policies (uiâˆ— )iâˆˆ[N ] (where each uiâˆ— âˆˆ Uoi is measurable with respect to the
noise process) such that,
J i (uiâˆ— , uâˆ’iâˆ— ) â‰¤ J i (ui , uâˆ’iâˆ— ), âˆ€ui âˆˆ Uoi .
28

First we find the equilibrium condition of the OLNE. Let us first define Î¶t =
(xt , xÌ„t , (uit , uÌ„it )iâˆˆ[N ] ) and
bt (Î¶t ) = (At âˆ’ I)xt + AÌ„t xÌ„t +

N
X
(Bti uit + BÌ„ti uÌ„it ) = xt+1 âˆ’ xt âˆ’ Ï‰t0 âˆ’ Ï‰t
j=1

for t âˆˆ {0, . . . , T âˆ’ 1}. Now we introduce the adjoint process pi = (pit )0â‰¤t<T which is
an Ft -adapted process and definehit (Î¶t , pit ) = bt (Î¶t )âŠ¤ pit + cit (Î¶t )
i
i
i âŠ¤ i i
i
i âŠ¤ i i
= bt (Î¶t )âŠ¤ pit + (xt âˆ’ xÌ„t )âŠ¤ Qit (xt âˆ’ xÌ„t ) + xÌ„âŠ¤
t QÌ„t xÌ„t + (ut âˆ’ uÌ„t ) Rt (ut âˆ’ uÌ„t ) + (uÌ„t ) RÌ„t uÌ„t

Now we evaluate the form of derivatives of hit and convexity of hit with respect to
(ui , uÌ„i ).
Lemma D.2. If Rti â‰» 0 and RÌ„ti â‰» 0, then hit is convex with respect to (ui , uÌ„i ) and
Î´xt hit (Î¶t , pit ) = (pit )âŠ¤ (At âˆ’ I) + 2(xt âˆ’ xÌ„t )âŠ¤ Qit ,
i
Î´xÌ„t hit (Î¶t , pit ) = (pit )âŠ¤ AÌ„t âˆ’ 2(xt âˆ’ xÌ„t )âŠ¤ Qit + 2xÌ„âŠ¤
t QÌ„t ,

Î´uit hit (Î¶t , pit ) = (pit )âŠ¤ Bti + 2(uit âˆ’ uÌ„it )âŠ¤ Rti ,
Î´uj hit (Î¶t , pit ) = (pit )âŠ¤ Btj ,
t

j Ì¸= i,

Î´uÌ„it hit (Î¶t , pit ) = (pit )âŠ¤ BÌ„ti âˆ’ 2(uit âˆ’ uÌ„it )âŠ¤ Rti + 2(uÌ„it )âŠ¤ RÌ„ti ,
Î´uÌ„j hit (Î¶t , pit ) = (pit )âŠ¤ BÌ„tj ,
t

j Ì¸= i.

Proof. The partial derivatives follow by direct computation and the convexity is
due to the fact that


2Rti
âˆ’2Rti
2
i
i
Î´(ui ,uÌ„i ),(ui ,uÌ„i ) ht (Î¶t , pt ) =
â‰» 0.
t
t
t
t
âˆ’2Rti 2(Rti + RÌ„ti )
This completes the proof.
The convexity property will be used later to characterize the sufficient conditions
for the OLNE. We hypothesize that the adjoint process for each player follows the
backwards difference equations


i
âŠ¤
i
i
i
(D.2)
pit = E AâŠ¤
t+1 pt+1 + AÌ„t+1 pÌ„t+1 + 2Qt+1 (xt+1 âˆ’ xÌ„t+1 ) + 2QÌ„t+1 xÌ„t+1 |Ft
where pÌ„it = E[pit |(Ï‰s0 )0â‰¤sâ‰¤t ]. This hypothesis will shown to be true while characterizing
the necessary conditions for OLNE. The first step towards obtaining the necessary
conditions for OLNE is to compute the Gateaux derivative of the cost with respect to
perturbation in just the control policy of the ith agent itself. The Gateux derivative is
shown to be a function of the adjoint process.
Lemma D.3. If the adjoint process is defined as in (D.2), then the Gateaux derivative of J i in the direction of Î² i âˆˆ Uoi is
DJ i (ui , uâˆ’i )(Î² i , 0) = E

T
âˆ’1
X

 i âŠ¤ i

((pt ) Bt + 2(uit âˆ’ uÌ„it )âŠ¤ Rti + (pÌ„it )âŠ¤ BÌ„ti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²ti

t=0

29

Proof. We compute the Gateaux derivative by first introducing a perturbation
in the control sequence of agent i, which results in the perturbed control uiâ€² . Then
the Gateaux derivative is computed by comparing the costs under the perturbed and
unperturbed control sequences.
Let us denote for each i âˆˆ [N ] the control sequence ui = (uit )tâ‰¥0 and the perturbed
â€²
control sequence uiâ€² = (uiâ€²
t )tâ‰¥0 , and the corresponding state processes xt and xt under
the same noise process and write
xt+1 âˆ’ xâ€²t+1 = At (xt âˆ’ xâ€²t ) + AÌ„t (xÌ„t âˆ’ xÌ„â€²t ) +

N
X


i i
iâ€²
Bti (uit âˆ’ uiâ€²
t ) + BÌ„t (uÌ„t âˆ’ uÌ„t ) .

i=1
iâ€²
0
i
i
where uÌ„it = E[uit |F 0 ] and uÌ„iâ€²
t = E[ut |F ]. Let us introduce Î¶t = (xt , xÌ„t , (ut , uÌ„t )iâˆˆ[N ] )
â€²
â€²
â€²
iâ€²
iâ€²
and Î¶t = (xt , xÌ„t , (ut , uÌ„t )iâˆˆ[N ] ). Next let us compute the difference between the costs,
T
âˆ’1
X

(cit (Î¶tâ€² ) âˆ’ cit (Î¶t )) =

t=0

T
âˆ’1
X


cit (Î¶tâ€² ) âˆ’ cit (Î¶t ) + (bt (Î¶tâ€² ) âˆ’ bt (Î¶t ))âŠ¤ pit âˆ’ (bt (Î¶tâ€² ) âˆ’ bt (Î¶t ))âŠ¤ pit ,

t=0

=

T
âˆ’1
X

hit (Î¶tâ€² , pit ) âˆ’ hit (Î¶t , pit ) âˆ’ (xâ€²t+1 âˆ’ xt+1 )âŠ¤ pit + (xâ€²t âˆ’ xt )âŠ¤ pit



t=0

=

T
âˆ’2
X


hit (Î¶tâ€² , pit ) âˆ’ hit (Î¶t , pit ) + (xâ€²t+1 âˆ’ xt+1 )âŠ¤ (pit+1 âˆ’ pit ) +

t=0

hiT âˆ’1 (Î¶Tâ€² âˆ’1 , piT âˆ’1 ) âˆ’ hiT âˆ’1 (Î¶T âˆ’1 , piT âˆ’1 ) + (xâ€²T âˆ’ xT )âŠ¤ piT âˆ’1

(D.3)

Let us denote the perturbation in uiâ€² as the Ft -adapted stochastic process Î²ti scaled by
a constant Ïµ such that uiâ€² = ui + ÏµÎ² i for all i âˆˆ [N ] (consequently uÌ„iâ€² = uÌ„i + ÏµE[Î² i |F 0 ]).
In order to compute the Gateaux derivative we define the infinitesimal change in state
process and the mean-field as
1
1
Vt = lim (xâ€²t âˆ’ xt ), VÌ„t = lim (xÌ„â€²t âˆ’ xÌ„t )
Ïµâ†’0 Ïµ
Ïµâ†’0 Ïµ

(D.4)

Now we compute the Gateaux derivative of J i in the direction of Î² i âˆˆ Uoi . By setting
piT = 0 we get the simplified expression
DJ i (ui , uâˆ’i )(Î² i , 0)
=E

T
âˆ’1
X



âŠ¤
Vt+1
(pit+1 âˆ’ pit ) + Î´xt hit (Î¶t , pit )âŠ¤ Vt + Î´xÌ„t hit (Î¶t , pit )âŠ¤ VÌ„t

t=0

+ Î´uit hit (Î¶t , pit )âŠ¤ Î²ti + Î´uÌ„it hit (Î¶t , pit )âŠ¤ Î²Ì„ti
=E

T
âˆ’1
X



 âŠ¤ i
Vt+1 (pt+1 âˆ’ pit ) + ((pit )âŠ¤ (At âˆ’ I) + 2(xt âˆ’ xÌ„t )âŠ¤ Qit )âŠ¤ Vt

t=0
i âŠ¤
i âŠ¤ i
i
i âŠ¤ i âŠ¤ i
+ ((pit )âŠ¤ AÌ„t âˆ’ 2(xt âˆ’ xÌ„t )âŠ¤ Qit + 2xÌ„âŠ¤
t QÌ„t ) VÌ„t + ((pt ) Bt + 2(ut âˆ’ uÌ„t ) Rt ) Î²t

(D.5)

+ ((pit )âŠ¤ BÌ„ti âˆ’ 2(uit âˆ’ uÌ„it )âŠ¤ Rti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²Ì„ti .
Next using techniques similar to [22] we deduce that
i âŠ¤
i âŠ¤
âŠ¤ i âŠ¤
((pit )âŠ¤ AÌ„t âˆ’ 2(xt âˆ’ xÌ„t )âŠ¤ Qit + 2xÌ„âŠ¤
t QÌ„t ) VÌ„t = ((pÌ„t ) AÌ„t + 2xÌ„t QÌ„t ) Vt .

30

Using the definition of the adjoint process (D.2) we can simplify the first three terms
in (D.5) to be
E

T
âˆ’1
X

 âŠ¤ i
Vt+1 (pt+1 âˆ’ pit ) + ((pit+1 )âŠ¤ (At+1 âˆ’ I) + (pÌ„it+1 )âŠ¤ AÌ„t+1

t=0


i
âŠ¤
+ 2(xt+1 âˆ’ xÌ„t+1 )âŠ¤ Qit+1 + 2xÌ„âŠ¤
t+1 QÌ„t+1 ) Vt+1 = 0.

(D.6)

Next using techniques similar to [22] we can also deduce that
(D.7)

((pit )âŠ¤ BÌ„ti âˆ’ 2(uit âˆ’ uÌ„it )âŠ¤ Rti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²Ì„ti = ((pÌ„it )âŠ¤ BÌ„ti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²ti .

Using (D.5)-(D.7) we obtain
DJ i (ui , uâˆ’i )(Î² i , 0) = E

T
âˆ’1
X

 i âŠ¤ i

((pt ) Bt + 2(uit âˆ’ uÌ„it )âŠ¤ Rti + (pÌ„it )âŠ¤ BÌ„ti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²ti

t=0

The necessary conditions for OLNE requires stationarity of the Gateaux derivative.
Now using this condition and the form of the Gateaux deriative we state the first order
necessary conditions for the OLNE the NE of the N -player LQ-MFTGs.
Proposition D.4 (OLNE Necessary Conditions). If the set of policies (uiâˆ— )iâˆˆ[N ]
constitutes OLNE, then for all i âˆˆ [N ] and t âˆˆ {0, . . . , T âˆ’ 1},
iâˆ— âŠ¤ i
i âŠ¤ i
iâˆ— âŠ¤ i
(pit )âŠ¤ Bti + 2(uiâˆ—
t âˆ’ uÌ„t ) Rt + (pÌ„t ) BÌ„t + 2(uÌ„t ) RÌ„t = 0



i
âŠ¤
i
i
i
where pit = E AâŠ¤
t+1 pt+1 + AÌ„t+1 pÌ„t+1 + 2Qt+1 (xt+1 âˆ’ xÌ„t+1 ) + 2QÌ„t+1 xÌ„t+1 |Ft .
Proof. Let us fix i âˆˆ [N ]. Then using the necessary conditions for OLNE and
Lemma D.3, we have
DJ i (ui , uâˆ’i )(Î² i , 0)
=E

T
âˆ’1
X

 i âŠ¤ i

((pt ) Bt + 2(uit âˆ’ uÌ„it )âŠ¤ Rti + (pÌ„it )âŠ¤ BÌ„ti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²ti = 0,

t=0

for any 0 Ì¸= Î² i âˆˆ Ui , as this statement has to be true for any perturbation Î² i each
summand has to be equal to 0 which results in the statement of the theorem.
Using the necessary conditions for OLNE we now identify the form of the OLNE
policies of the N -player LQ-MFTG.
Proposition D.5. The Adapted Open-Loop Nash Equilibrium (OLNE) policies
of the N -player LQ-MFTG has the form and the precise expression given by
i i
i i
iâˆ—
i
i i
uiâˆ—
t = âˆ’Lt pt âˆ’ LÌ„t pÌ„t , uÌ„t = âˆ’(Lt + LÌ„t )pÌ„t

(D.8)
where
(D.9)

Lit =


1 i âˆ’1 i âŠ¤
1
(Rt ) (Bt ) , LÌ„it = (Rti )âˆ’1 Rti (RÌ„ti )âˆ’1 (Bti + BÌ„ti )âŠ¤ âˆ’ (Bti )âŠ¤
2
2

Furthermore, the feedback representation of the OLNE is
uit = âˆ’Lit Pti (xt âˆ’ xÌ„t ) âˆ’ (Lit + LÌ„it )PÌ„ti xÌ„t , uÌ„it = âˆ’(Lit + LÌ„it )PÌ„ti xÌ„t
where the matrices Pti and PÌ„ti are computed recursively using (D.11)-(D.12).
31

Proof. In this proof the form of the OLNE control policies is computed by utilizing
the necessary conditions in Proposition D.4.Then by introducing deterministic processes
and reformulating the adjoint process the necessary conditions are transformed into
a set of forward-backwards equations (D.16). Moreover, these forward-backwards
equations are shown to be equivalent to a set of Riccati equations (D.11)-(D.12).
Finally utilizing these Riccati equations we formalize the feedback representation of
the OLNE.
Let us define a set of deterministic processes,
(D.10)

1,i
âŠ¤
i
i
âŠ¤ i
i
Zt0,i = (AâŠ¤
t + AÌ„t )PÌ„t + 2QÌ„t , Zt = At Pt + 2Qt ,

where the matrices PÌ„ti and Pti are defined as follows:
(D.11)

N
X


i
PÌ„ti = (At + AÌ„t )PÌ„t+1
+ 2QÌ„it At + AÌ„t âˆ’
(Btj + BÌ„tj )(Ljt + LÌ„jt )PÌ„tj ,
j=1

(D.12)

i
Pti = (AâŠ¤ Pt+1
+ 2Qit )(At âˆ’

N
X

Btj Ljt Ptj ).

j=1

Now we give the form of the adjoint process compatible with (D.2):
(D.13)
1,i
0,i 0
i
âŠ¤
i
i
i
pit = AâŠ¤
t+1 pt+1 + AÌ„t+1 pÌ„t+1 + 2Qt+1 (xt+1 âˆ’ xÌ„t+1 ) + 2QÌ„t+1 xÌ„t+1 âˆ’ Zt+1 Ï‰t+1 âˆ’ Zt+1 Ï‰t+1 .
Utilizing the necessary conditions for OLNE in Proposition D.4 and taking conditional
expectation E[Â·|F 0 ] we obtain,
(D.14)

1 i âˆ’1 i
(Bt + BÌ„ti )âŠ¤ pÌ„it = âˆ’(Lit + LÌ„it )pÌ„it
uÌ„iâˆ—
t = âˆ’ (RÌ„t )
2

Substituting this back into the OLNE in Proposition D.4, we get
(D.15)

1 i âˆ’1 i âŠ¤ i 1 i âˆ’1 i i âˆ’1 i
uiâˆ—
(Bt ) pt âˆ’ (Rt )
Rt (RÌ„t ) (Bt + BÌ„ti )âŠ¤ âˆ’ (Bti )âŠ¤ pÌ„it = âˆ’Lit pit + LÌ„it pÌ„it .
t = âˆ’ (Rt )
2
2
Substituting (D.14) and (D.15) into (2.3) and restating the adjoint process, we arrive
at the forward-backward necessary conditions for the OLNE:
xt+1 = At xt + AÌ„t xÌ„t âˆ’

N
X


0
Bti (Lit pit + LÌ„it pÌ„it ) + BÌ„ti (Lit + LÌ„it )pÌ„it + Ï‰t+1
+ Ï‰t+1 ,

i=1

(D.16)

i
âŠ¤
i
i
i
pit = AâŠ¤
t+1 pt+1 + AÌ„t+1 pÌ„t+1 + 2Qt+1 (xt+1 âˆ’ xÌ„t+1 ) + 2QÌ„t+1 xÌ„t+1
0,i 0
1,i
âˆ’ Zt+1
Ï‰t+1 âˆ’ Zt+1
Ï‰t+1 ,

and piT = 0. Taking conditional expectation E[Â·|F 0 ], we obtain
xÌ„t+1 = (At + AÌ„t )xÌ„t âˆ’
(D.17)

N
X
0
(Bti + BÌ„ti )(Lit + LÌ„it )pÌ„it + Ï‰t+1
,

i=1
0,i 0
i
âŠ¤
âŠ¤
Ï‰t+1 ,
pÌ„t = (At+1 + AÌ„t+1 )pÌ„it+1 + 2QÌ„it+1 xÌ„t+1 âˆ’ Zt+1

32

pÌ„iT = 0.

Let us introduce the ansatz
pÌ„it = PÌ„ti xÌ„t

(D.18)
and substitute into (D.17):
xÌ„t+1 = (At + AÌ„t ) âˆ’
(D.19)

PÌ„ti xÌ„t =

N
X

0
(Bti + BÌ„ti )(Lit + LÌ„it )PÌ„ti xÌ„t + Ï‰t+1
,

i=1

0,i 0
âŠ¤
âŠ¤
i
(At+1 + AÌ„t+1 )PÌ„t+1
+ 2QÌ„it+1 xÌ„t+1 âˆ’ Zt+1
Ï‰t+1 ,

pÌ„iT = 0.

Using (D.19) we arrive at the Riccati equation,
N
X


âŠ¤
i
i
(D.20) PÌ„ti = (AâŠ¤
(Bti + BÌ„ti )(Lit + LÌ„it )PÌ„ti
t+1 + AÌ„t+1 )PÌ„t+1 + 2QÌ„t+1 (At + AÌ„t ) âˆ’
i=1

Next we write the forward-backward equations (D.16)-(D.17) in terms of xt âˆ’ xÌ„t and
pit âˆ’ pÌ„it :
xt+1 âˆ’ xÌ„t+1 = At (xt âˆ’ xÌ„t ) âˆ’
(D.21)

N
X

Bti Lit (pit âˆ’ pÌ„it ) + Ï‰t+1 ,

i=1
1,i
i
i
âŠ¤
i
i
pt âˆ’ pÌ„t = At+1 (pt+1 âˆ’ pÌ„t+1 ) + 2Qit+1 (xt+1 âˆ’ xÌ„t+1 ) âˆ’ Zt+1
Ï‰t+1 , piT = 0.

As before, let us introduce another ansatz
(D.22)

pit âˆ’ pÌ„it = Pti (xt âˆ’ xÌ„t ).

We arrive at the second Riccati equation,
(D.23)

i
Pti = (AâŠ¤ Pt+1
+ 2Qit )(At âˆ’

N
X

Btj Ljt Ptj ).

j=1

Furthermore, using (D.18)-(D.22) we can deduce the feedback representation of the
OLNE:
uit = âˆ’Lit pit âˆ’ LÌ„it pÌ„it = âˆ’Lit Pti (xt âˆ’ xÌ„t ) âˆ’ (Lit + LÌ„it )PÌ„ti xÌ„t ,
uÌ„it = âˆ’(Lit + LÌ„it )pÌ„it = âˆ’(Lit + LÌ„it )PÌ„ti xÌ„t
This concludes the proof of the proposition.
Having provided the structure of the OLNE following from the necessary
conditions, and in terms of the Riccati equations (D.20)-(D.23) we now prove
that the necessary conditions are also sufficient. This follows form the convexity,
in particular, the quadratic nature of the cost function J i with respect to perturbations
in control policy.
Proposition D.6 (OLNE Sufficient Condition). If there exists a state process
(xt )tâ‰¥0 and an adjoint process (pi , Z 0,i , Z 1,i )iâˆˆ[N ] satisfying (D.16) and (D.10), then
the control laws given by (D.8) constitute the OLNE of the N -player LQ-MFTG.
33

Proof. The proof of this proposition starts by introducing a perturbation around
the candidate OLNE controls (i.e. the set of controls which satisfy the OLNE necessary
conditions). Then using a second-order expansion of the cost it is shown that the the
perturbation around the candidate OLNE controls will always lead to a strictly higher
cost. This is due to the quadratic nature of the cost. This proves that the necessary
conditions for the OLNE are also the sufficient conditions for the OLNE.
Let us first write a second-order expansion for cost J i at control policies (ui )iâˆˆ[N ] âˆˆ
i
U and (uiâ€² = ui + ÏµÎ² i )iâˆˆ[N ] , Î² i âˆˆ Ui . We introduce the deterministic process
corresponding to the control perturbations (Î² i )iâˆˆ[N ] , and let Vt = (xt âˆ’ xâ€²t )/Ïµ where
(xt )tâ‰¥0 is the state process under control policy (ui )iâˆˆ[N ] and (xâ€²t )tâ‰¥0 is the state
process under control policy (uiâ€² )iâˆˆ[N ] . Then,
Vt+1 = At Vt + AÌ„t VÌ„t +

N
X

Bti Î²ti + BÌ„ti Î²Ì„ti

i=1

The difference in costs due to the control perturbation is:
J i (uiâ€² , uâˆ’iâ€² ) âˆ’ J i (ui , uâˆ’i ) = J i (ui + ÏµÎ² i , uâˆ’i + ÏµÎ² âˆ’i ) âˆ’ J i (ui , uâˆ’i )
(D.24) = ÏµE

T
âˆ’1
X

T
âˆ’1
X
 âŠ¤ i
 1
 2

Vt (pt+1 âˆ’ pit ) + Î´Î¶t hit (Î¶t , pit )Î¶ÌŒt + Ïµ2 E
Î´Î¶t ,Î¶t hit (Î¶t , pit )Î¶ÌŒt Â· Î¶ÌŒt
2
t=0
t=0

iâ€²
â€²
where Î¶t = (xt , xÌ„t , (uit , uÌ„it )iâˆˆ[N ] ), Î¶tâ€² = (xâ€²t , xÌ„â€²t , (uiâ€²
t , uÌ„t )iâˆˆ[N ] ) and Î¶ÌŒt = (Î¶t âˆ’ Î¶t )/Ïµ =
i
i
i
iâˆ—
(Vt , VÌ„t , (Î²t , Î²Ì„t )iâˆˆ[N ] ). If we assume that (u )iâˆˆ[N ] = (u )iâˆˆ[N ] , meaning that they
satisfy the necessary conditions for the OLNE (Proposition D.4) and that there exists
an adjoint process such that (D.16) and (D.10) are satisfied, then the first order terms
PT âˆ’1
in the above given equation vanish, that is E t=0 [VtâŠ¤ (pit+1 âˆ’ pit ) + Î´Î¶t hit (Î¶t , pit )Î¶ÌŒt ] = 0.
To compute the last term we compute the second derivative of hit with respect to Î¶t :

Î´x2t ,xt hit (Î¶t , pit ) = 2Qit , Î´x2t ,xÌ„t hit (Î¶t , pit ) = Î´xÌ„2t ,xt hit (Î¶t , pit ) = âˆ’2Qit ,
Î´xÌ„2t ,xÌ„t hit (Î¶t , pit ) = 2(Qit + QÌ„it ),
Î´u2 t ,ut hit (Î¶t , pit ) = 2Rti , Î´u2 t ,uÌ„t hit (Î¶t , pit ) = Î´uÌ„2 t ,ut hit (Î¶t , pit ) = âˆ’2Rti ,
Î´uÌ„2 t ,uÌ„t hit (Î¶t , pit ) = 2(Rti + RÌ„ti )
Now we characterize the last term in (D.24):
J i (uiâ€² , uâˆ’iâ€² ) âˆ’ J i (ui , uâˆ’i ) =

=

T âˆ’1

1 2 X 2
Ïµ E
Î´Î¶t ,Î¶t hit (Î¶t , pit )Î¶ÌŒt Â· Î¶ÌŒt
2
t=0

T âˆ’1

1 2 X
Ïµ E
(Vt âˆ’ VÌ„t )âŠ¤ Qit (Vt âˆ’ VÌ„t ) + VÌ„tâŠ¤ QÌ„it VÌ„t + (Î²ti âˆ’ Î²Ì„ti )âŠ¤ Rti (Î²ti âˆ’ Î²Ì„ti ) + (Î²Ì„ti )âŠ¤ RÌ„ti Î²Ì„ti
2
t=0

>0
Therefore if there exists a state process (xt )tâ‰¥0 and an adjoint process (pi , Z 0,i , Z 1,i )iâˆˆ[N ]
satisfying (D.16) and (D.10) then the control laws given by (D.8) satisfy the necessary
and sufficient conditions for OLNE of the N -player LQ-MFTG.
Finally, we prove that every OLNE satisfies the necessary conditions
for Nash Equilibria (NE). We start by computing the Gateaux derivative of J i but
34

now with the control actions ujt , j =
Ì¸ i being functions of the state at time t, xt . This
will give us necessary conditions for the NE since in the NE (due to the backwards
nature of the discrete-time HJI equations) will have a feedback structure i.e. the NE
control actions at time t will depend on state xt . Recalling (D.3) and (D.4), we can
write down
DJ i (ui , uâˆ’i )(Î² i , 0)
=E

T
âˆ’1
X

 âŠ¤ i
Vt+1 (pt+1 âˆ’ pit ) + Î´xt hit (Î¶t , pit )âŠ¤ Vt

t=0

âŠ¤
Î´ujt
Î´ uÌ„jt
i
i
i
i
+
Î´ j h (Î¶t , pt ) +
Î´ j h (Î¶t , pt ) Vt
Î´xt ut t
Î´xt uÌ„t t
jÌ¸=i
X j
âŠ¤
Î´ut
Î´ uÌ„jt
i
i âŠ¤
i
i
i
i
Î´ j h (Î¶t , pt ) +
Î´ j h (Î¶t , pt ) VÌ„t + Î´uit hit (Î¶t , pit )âŠ¤ Î²ti
+ Î´xÌ„t ht (Î¶t , pt ) VÌ„t +
Î´ xÌ„t ut t
Î´ xÌ„t uÌ„t t
jÌ¸=i

+ Î´uÌ„it hit (Î¶t , pit )âŠ¤ Î²Ì„ti

T
âˆ’1
X
 âŠ¤ i
i
=E
Vt+1 (pt+1 âˆ’ pt ) + (pit )âŠ¤ (At âˆ’ I) + 2(xt âˆ’ xÌ„t )âŠ¤ Qit
X

t=0

+

X  Î´uj
t

jÌ¸=i

+

Î´xt

X  Î´uj
t

jÌ¸=i

Î´ xÌ„t

Btj +

Î´ uÌ„jt j
BÌ„
Î´xt t

âŠ¤ âŠ¤

i
pit Vt + (pit )âŠ¤ AÌ„t âˆ’ 2(xt âˆ’ xÌ„t )âŠ¤ Qit + 2xÌ„âŠ¤
t QÌ„t

Btj +

Î´ uÌ„jt j
BÌ„
Î´ xÌ„t t

âŠ¤ âŠ¤
pit VÌ„t + ((pit )âŠ¤ Bti + 2(uit âˆ’ uÌ„it )âŠ¤ Rti )âŠ¤ Î²ti


+ ((pit )âŠ¤ BÌ„ti âˆ’ 2(uit âˆ’ uÌ„it )âŠ¤ Rti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²Ì„ti .
Next using techniques similar to [22] we deduce that

X  Î´uj j Î´ uÌ„j j âŠ¤ âŠ¤
t
i âŠ¤
âŠ¤ i
âŠ¤ i
(pt ) AÌ„t âˆ’ 2(xt âˆ’ xÌ„t ) Qt + 2xÌ„t QÌ„t +
pit VÌ„t
B + t BÌ„t
Î´ xÌ„t t
Î´ xÌ„t
jÌ¸=i

X  Î´uj j Î´ uÌ„j j âŠ¤ âŠ¤
t
i âŠ¤
âŠ¤ i
pÌ„it Vt .
= (pÌ„t ) AÌ„t + 2xÌ„t QÌ„t +
B + t BÌ„t
Î´ xÌ„t t
Î´ xÌ„t
jÌ¸=i

Now let the adjoint process satisfy the following condition:
âŠ¤

X  Î´ujt+1 j
Î´ uÌ„jt+1 j
i
âŠ¤
i
i
pt =E At+1 pt+1 + 2Qt (xt+1 âˆ’ xÌ„t+1 ) +
B
+
BÌ„
pit+1
Î´xt+1 t+1 Î´xt+1 t+1
jÌ¸=i

i
+ AÌ„âŠ¤
t+1 pÌ„t+1

(D.25)
+ 2QÌ„it+1 xÌ„t+1 +

X  Î´ujt+1
jÌ¸=i

Î´ xÌ„t+1

j
+
Bt+1

Î´ uÌ„jt+1 j
BÌ„
Î´ xÌ„t+1 t+1

âŠ¤


pÌ„it+1 Ft .

Then the Gateaux derivative will have the form:
DJ i (ui , uâˆ’i )(Î² i , 0) = E

T
âˆ’1
X

 i âŠ¤ i

((pt ) Bt + 2(uit âˆ’ uÌ„it )âŠ¤ Rti + (pÌ„it )âŠ¤ BÌ„ti + 2(uÌ„it )âŠ¤ RÌ„ti )âŠ¤ Î²ti ,

t=0

35

which, using the same techniques as before, will result in
i i
i i
iâˆ—
i
i i
uiâˆ—
t = âˆ’Lt pt âˆ’ LÌ„t pÌ„t , uÌ„t = âˆ’(Lt + LÌ„t )pÌ„t

similar to Lemma D.3 hence we have obtained the necessary conditions for NE. Notice
that since every solution of (D.2) also satisfies the relation (D.25), every OLNE satisfies
the necessary conditions for NE. This analysis is similar to the analysis of [8] (Chapter
6.2) between Open-loop (OL) and Feedback (FB) NE for deterministic N -player LQ
games, but there it has been shown that OL (non-adapted) and FB NE are not related,
that is one cannot be derived from the other.
Appendix E. Proof of Theorem 2.3. [Closed-loop Analysis of GS-MFTG]
We will solve for the NE of the GS-MFTG using the discrete-time HamiltonJacobi-Isaacs (HJI) equations [8]. We will solve the problem of finding NE (viâˆ— )iâˆˆ[N ] .
The procedure for computing (uÌ„iâˆ— )iâˆˆ[N ] is similar, and hence is omitted. We first
introduce the following backwards recursive equations
X j

i
i
i
(E.1) Rti + (Bti )âŠ¤ Zt+1
Bti Ktiâˆ— + (Bti )âŠ¤ Zt+1
(Bt )âŠ¤ Ktjâˆ— = (Bti )âŠ¤ Zt+1
At ,
jÌ¸=i
i
RÌ„ti + (BÌƒti )âŠ¤ ZÌ„t+1
BÌƒti



i
KÌ„tiâˆ— + (BÌƒti )âŠ¤ ZÌ„t+1

X

i
(BÌƒtj )âŠ¤ KÌ„tjâˆ— = (BÌƒti )âŠ¤ ZÌ„t+1
AÌƒt , i âˆˆ [N ]

jÌ¸=i

and the matrices Zti are determined as follows,
i
Zti = FtâŠ¤ Zt+1
Ft + (Ktiâˆ— )âŠ¤ Rti Ktiâˆ— + Qit , ZTi = QiT ,

(E.2)

i
ZÌ„ti = FÌ„tâŠ¤ ZÌ„t+1
FÌ„t + (KÌ„tiâˆ— )âŠ¤ RÌ„ti KÌ„tiâˆ— + QÌ„it , ZÌ„Ti = QÌ„iT , i âˆˆ [N ]

PN
PN
where Ft = At âˆ’ i=1 Bti Ktiâˆ— and FÌ„t = AÌƒt âˆ’ i=1 BÌƒti KÌ„tiâˆ— . We start by writing down
the discrete-time Hamilton-Jacobi-Isaacs (HJI) equations [8] in order to find the set of
controls (viâˆ— )iâˆˆ[N ] :
i
Vti (y) = min
E[gti (fËœtiâˆ— (y, vti ), vtiâˆ— (y), . . . , vti (y), . . . , vtN âˆ— (y), y) + Vt+1
(fËœtiâˆ— (y, vti ))|v],
i
vt

i
= E[gti (fËœtiâˆ— (y, vtiâˆ— ), vtiâˆ— (y), . . . , vtiâˆ— , . . . , vtN âˆ— (y), y) + Vt+1
(fËœtiâˆ— (y, vtiâˆ— ))|y],

(E.3)
VTi (y) = 0
where
fËœti (y, vti ) = ft (y, vt1âˆ— (y), . . . , vti , . . . , vtN âˆ— (y)).
The dynamics of the deviation process yt and its corresponding instantaneous costs
are
ft (yt , vt1 , . . . , vtN ) = At yt +

N
X

Bti vti + Ï‰t+1 ,

i=1

1
gti (yt+1 , vt1 , . . . , vtN , yt ) =
2



âŠ¤
yt+1
Qit+1 yt+1 + (vti )âŠ¤ Rti vti



, gTi (yT ) = 0.

Notice that scaling the cost by 1/2 does not change the nature of the problem but
makes the analysis more compact. Hence starting at time T âˆ’ 1 and using the HJI
36

equations (E.3), we get
VTi âˆ’1 (yT âˆ’1 ) = min
E[yTâŠ¤ QiT yT + (vTi âˆ’1 )âŠ¤ RTi âˆ’1 vTi âˆ’1 |yT âˆ’1 ],
i
vT âˆ’1

[(vTi âˆ’1 )âŠ¤ (RTi âˆ’1 + (BTi âˆ’1 )âŠ¤ QiT BTi âˆ’1 )vTi âˆ’1
= min
i
vT âˆ’1

+ 2(BTi âˆ’1 vTi âˆ’1 )âŠ¤ QiT FÌƒTi âˆ’1 yT âˆ’1
+ (FÌƒTi âˆ’1 (yT âˆ’1 ))âŠ¤ QiT FÌƒTi âˆ’1 (yT âˆ’1 )|yT âˆ’1 ] + T r(QiT Î£)
P
where FÌƒti (yt ) = At yt + jÌ¸=i Btj vtjâˆ— (yt ). Now differentiating with respect to vti , the
necessary conditions for NE become
X jâˆ—
âˆ’ (RTi âˆ’1 + (BTi âˆ’1 )âŠ¤ QiT BTi âˆ’1 )vTiâˆ—âˆ’1 (yT âˆ’1 ) âˆ’ (BTi âˆ’1 )âŠ¤ QiT
vT âˆ’1 (yT âˆ’1 )

(E.4)

jÌ¸=i

= (BTi âˆ’1 )âŠ¤ QiT AT âˆ’1 yT âˆ’1
âˆ€i âˆˆ [N ]. Hence, vTiâˆ—âˆ’1 (xT âˆ’1 ) is linear in xT âˆ’1 , vTiâˆ—âˆ’1 (xT âˆ’1 ) = âˆ’KTiâˆ—âˆ’1 xT âˆ’1 . Hence,
we get
X j
(RTi âˆ’1 + (BTi âˆ’1 )âŠ¤ QiT BTi âˆ’1 )PTi âˆ’1 yT âˆ’1 + (BTi âˆ’1 )âŠ¤ QiT
PT âˆ’1 (yT âˆ’1 )
jÌ¸=i

= (BTi âˆ’1 )âŠ¤ QiT AT âˆ’1 yT âˆ’1
The value function at time T âˆ’ 1 can now be calculated as:
VTi âˆ’1 (xT âˆ’1 ) = yTâŠ¤âˆ’1 [FTâŠ¤âˆ’1 QiT FT âˆ’1 + (KTiâˆ—âˆ’1 )âŠ¤ RTi âˆ’1 KTiâˆ—âˆ’1 ]yT âˆ’1 + T r(QiT Î£),
= yTâŠ¤âˆ’1 [ZTi âˆ’1 âˆ’ QiT âˆ’1 ]yT âˆ’1 + T r(QiT Î£)
where ZTi âˆ’1 = FtâŠ¤ ZTi Ft + (KTiâˆ—âˆ’1 )âŠ¤ RTi âˆ’1 KTiâˆ—âˆ’1 + QiT âˆ’1 . Now let us take
i
i
Vt+1
(y) = y âŠ¤ (Zt+1
âˆ’ Qit+1 )x +

T
X

T r(Zsi Î£), i âˆˆ [N ].

s=t+2

Using the HJI equations (E.3),
i
Vti (yt ) = min
[(FÌƒti (yt ) + Bti vti )âŠ¤ Zt+1
(FÌƒti (yt ) + Bti vti ) + (vti )âŠ¤ Rti vti ] +
i
i
vt âˆˆUt

T
X

T r(Zsi Î£)

s=t+1

and differentiating with respect to vti and using the necessary conditions of NE we get
X j

i
i
âˆ’ Rti + (Bti )âŠ¤ Zt+1
Bti vtiâˆ— (xt ) âˆ’ (Bti )âŠ¤ Zt+1
(Bt )âŠ¤ vtjâˆ— (xt ) = (Bti )âŠ¤ Qit At .
jÌ¸=i

Again we can notice that vtiâˆ— is linear in yt , and thus we get vtiâˆ— (xt ) = âˆ’Ktiâˆ— yt and
recover (E.1). Using this expression of vtiâˆ— the value function for agent i becomes
Vti (yt ) = ytâŠ¤ [FtâŠ¤ Zt+1 Ft + (Ktiâˆ— )âŠ¤ Rti Ktiâˆ— ]yt +

T
X
s=t+1

= ytâŠ¤ (Zti âˆ’ Qit )yt +

T
X
s=t+1

37

T r(Zsi Î£)

T r(Zsi Î£),

where the second inequality is obtained using (E.2). Hence we have completed the
characterization of (viâˆ— )iâˆˆ[N ] , and the characterization of (uÌ„iâˆ— )iâˆˆ[N ] follows similar
techniques, and hence is omitted. As a result the NE has the following linear structure
(E.5)
iâˆ—
iâˆ—
iâˆ—
iâˆ—
uiâˆ—
t (xt ) = vt (yt ) + uÌ„t (xÌ„t ) = âˆ’Kt (xt âˆ’ xÌ„t ) âˆ’ KÌ„t xÌ„t , i âˆˆ [N ], t âˆˆ {0, . . . .T âˆ’ 1}
such that the matrices (Ktiâˆ— , KÌ„tiâˆ— )0â‰¤tâ‰¤T âˆ’1 satisfy (E.1)-(E.2). The sufficient condition
for existence and uniqueness of solution to (E.1) can be obtained by concatenating
(E.1) for all i âˆˆ [N ] and requiring that the matrices Î¦t and Î¦Ì„t be invertible, where
ï£« 1
ï£¶
1
1
Rt + (Bt1 )âŠ¤ Zt+1
Bt1
(Bt1 )âŠ¤ Zt+1
Bt2
Â·Â·Â·
2 âŠ¤ 2
1
2
ï£¬
Rt2 + (Bt2 )âŠ¤ Zt+1
Bt2 Â· Â· Â·ï£·
Î¦t = ï£­ (Bt ) Zt+1 Bt
ï£¸,
..
..
..
.
.
.
ï£« 1
ï£¶
1
1
RÌ„t + (BÌƒt1 )âŠ¤ ZÌ„t+1
BÌƒt1
(BÌƒt1 )âŠ¤ ZÌ„t+1
BÌƒt2
Â·Â·Â·
2 âŠ¤ 2
1
2
ï£¬
RÌ„t2 + (BÌƒt2 )âŠ¤ ZÌ„t+1
BÌ„t2 Â· Â· Â·ï£·
Î¦Ì„t = ï£­ (BÌƒt ) ZÌ„t+1 BÌ„t
ï£¸.
..
..
..
.
.
.
These sufficient conditions are similar to the sufficient conditions for the N -player LQ
games (Corollary 6.1 [8]). In case Î¦t and Î¦Ì„t are invertible, the control matrices Ktiâˆ—
and KÌ„tiâˆ— can be computed as
ï£«
ï£¶
ï£¶ ï£« 1âˆ— ï£¶
ï£« 1âˆ— ï£¶
ï£«
Kt
KÌ„t
(BÌƒt1 )âŠ¤ QÌ„1t At
(B 1 )âŠ¤ Q1 At
ï£¬ .. ï£·
ï£· ï£¬ .. ï£·
ï£·
..
..
âˆ’1 ï£¬
âˆ’1 ï£¬
ï£¸ , ï£­ . ï£¸ = Î¦Ì„t ï£­
ï£¸.
ï£­ . ï£¸ = Î¦t ï£­
.
.
KtN âˆ—

KÌ„tN âˆ—

(B N )âŠ¤ QN At

(BÌƒtN )âŠ¤ QN
t At

This completes the proof.
Appendix F. Proof of Theorem 2.2. [NE of the MFTG is O(1/M )-Nash
for the finite agent CC game]
Proof. For this proof we will analyze the Ïµ-Nash property for a fixed i âˆˆ [N ].
Central to this analysis is the quantification of the difference between the finite and
infinite population costs for a given set of control policies. First we express the
state and mean-field processes in terms of the noise processes, for the finite and
infinite population settings. This then allows us to write the costs (in both settings) as
quadratic functions of the noise process, which simplifies quantification of the difference
between these two costs.
Let us first concatenate the states of j th agents in all teams such that xjt =
1,j âŠ¤
âŠ¤ âŠ¤
[(xt ) , . . . , (xN,j
t ) ] . For simplicity of analysis we assume Mi = M . If for some
i âˆˆ [N ], Mi Ì¸= M then we can redefine M = mini Mi . Consider the dynamics of joint
state xj under the NE of the GS-MFTG (Theorem 2.3)
(F.1)

âˆ—
xjâˆ—,M
t+1 = (At âˆ’

N
X

Btj Ktjâˆ— ) xtjâˆ—,M + (AÌ„âˆ—t âˆ’

j=1

|

{z

Lâˆ—
t

N
X

j
0
âˆ—
BÌ„tj KÌ„tjâˆ— ) xÌ„M
+ Ï‰t+1
+ Ï‰t+1
t

j=1

}

|

{z

LÌ„âˆ—
t

}

where the superscript M denotes the dynamics in the finite population game (2.1)P
jâˆ—,M
1
(2.2) and xÌ„M âˆ— = M
is the empirical mean-field. We can also write the
jâˆˆ[M ] xt
38

dynamics of the empirical mean-field as
(F.2)

âˆ—
xÌ„M
t+1 =

N
X

1 X j
âˆ—
0
At + AÌ„t âˆ’
+
Ï‰t+1 + Ï‰t+1
(Btj Ktjâˆ— + BÌ„tj KÌ„tjâˆ— ) xÌ„M
.
t
M
j=1
jâˆˆ[M ]
|
{z
}
|
{z
}
LÌƒâˆ—
t

M
Ï‰Ì„t+1

âˆ—
For simplicity we assume that xjâˆ—,M
= Ï‰0j + Ï‰00 which also implies that xÌ„M
= Ï‰Ì„0M .
0
0
M
Using (F.2) we get the recursive definition of xÌ„t as
âˆ—
xÌ„M
=
t

t
X

LÌƒâˆ—[tâˆ’1,s] Ï‰Ì„sN , where LÌƒâˆ—[s,t] := LÌƒâˆ—s LÌƒâˆ—sâˆ’1 . . . LÌƒâˆ—t , if s â‰¥ t. and LÌƒâˆ—[s,t] = I otherwise.

s=0
âˆ—
Hence xÌ„M
can be characterized as a linear function of the noise process
t
ï£«
ï£¶
ï£« Mï£¶
I
0
0
...
Ï‰Ì„0
âˆ—
ï£¬LÌƒ
I
0
. . .ï£·
ï£¬Ï‰Ì„1M ï£·
ï£¬ [0,0]
ï£·
ï£¬ Mï£·
ï£¬LÌƒâˆ—

âˆ—
ï£·
âˆ—
I
. . .ï£·
ï£· and Ï‰Ì„ M = ï£¬
[1,0] LÌƒ[1,1]
xÌ„M
= Î¨Ì„âˆ— Ï‰Ì„ M t , where Î¨Ì„âˆ— = ï£¬
ï£¬Ï‰Ì„2 ï£·
t
ï£¬ âˆ—
ï£·
ï£¬
âˆ—
âˆ—
. ï£·
ï£¬LÌƒ[2,0] LÌƒ[2,1] LÌƒ[2,2] . . .ï£·
ï£­ .. ï£¸
ï£­
ï£¸
..
..
..
..
Ï‰Ì„TM
.
.
.
.

where (M )t denotes the tth block of matrix M and the covariance matrix of Ï‰Ì„ M is
E[Ï‰Ì„ M (Ï‰Ì„ M )âŠ¤ ] = diag((Î£/M + Î£0 )0â‰¤tâ‰¤T ). Similarly using (F.1) we can write
ï£«

xjâˆ—,M
= Î¨âˆ— Ï‰Ìƒ jâˆ—,M t ,
t

I

ï£¬Lâˆ—[0,0]
ï£¬ âˆ—
ï£¬
where Î¨âˆ— = ï£¬L[1,0]
ï£¬Lâˆ—
ï£­ [2,0]
..
.

0
I
Lâˆ—[1,1]
Lâˆ—[2,1]
..
.

0
0
I

ï£¶
...
. . .ï£·
ï£·
. . .ï£·
ï£·
. . .ï£·
ï£¸
..
.

Lâˆ—[2,2]
..
.

and
Ï‰Ìƒ jâˆ—,M =
ï£¶ ï£«
ï£« j
Ï‰0 + Ï‰00
0
ï£¬ Ï‰1j + Ï‰10 ï£· ï£¬LÌ„âˆ—0
ï£· ï£¬
ï£¬ j
ï£¬ Ï‰ + Ï‰0 ï£· ï£¬ 0
2ï£·+ï£¬
ï£¬ 2
ï£· ï£¬0
ï£¬
..
ï£¸ ï£­
ï£­
.
..
j
0
.
Ï‰T + Ï‰T
|

0
0
LÌ„âˆ—1
0
..
.

0
0
0
LÌ„âˆ—2
..
.

0
0
0
0
..
.

ï£¶ï£« I
...
ï£¬LÌƒâˆ—
. . .ï£·
[0,0]
ï£·ï£¬
ï£¬LÌƒâˆ—
. . .ï£·
ï£¬
ï£· ï£¬ [1,0]
. . .ï£·
LÌƒâˆ—[2,0]
ï£¸ï£¬
ï£­
..
..
.
.
{z
Lâˆ—

M â†’âˆž

0
I
LÌƒâˆ—[1,1]
LÌƒâˆ—[2,1]
..
.

0
0
I
LÌƒâˆ—[2,2]
..
.

ï£¶
. . . ï£«Ï‰Ì„ M ï£¶
0
Mï£·
. . .ï£·
Ï‰Ì„
ï£·ï£¬
ï£¬ 1M ï£·
ï£·
ï£¬
. . .ï£· ï£¬Ï‰Ì„2 ï£·
ï£·.
ï£·
.. ï£·
. . .ï£· ï£¬
ï£­
ï£¸
.
ï£¸
..
M
Ï‰Ì„
.
T
}

M â†’âˆž

Considering the infinite agent limit xÌ„M âˆ— âˆ’âˆ’âˆ’âˆ’â†’ xÌ„âˆ— and xjâˆ—,M âˆ’âˆ’âˆ’âˆ’â†’ xâˆ— , we have
ï£«
ï£¶
Ï‰0 + Ï‰00
ï£« 0ï£¶
ï£¬ Ï‰1 + Ï‰10 ï£·
Ï‰0
ï£¬
ï£·


0ï£·
ï£¬
ï£·
ï£¬
xÌ„âˆ—t = Î¨Ì„âˆ— Ï‰ 0 t , Ï‰ 0 = ï£­ ... ï£¸ and xâˆ—t = Î¨âˆ— Ï‰Ìƒ t , Ï‰Ìƒ = ï£¬ Ï‰2 + Ï‰2 ï£· + L âˆ— Ï‰ 0
ï£¬
ï£·
..
ï£­
ï£¸
Ï‰T0
.
0
Ï‰T + Ï‰T
39

where the covariance of Ï‰ 0 is E[Ï‰ 0 (Ï‰ 0 )âŠ¤ ] = diag((Î£0 )0â‰¤tâ‰¤T ). Similarly we characterize
âˆ—
the deviation process xjâˆ—,M
âˆ’ xÌ„M
t
t , using (F.1) and (F.2)
Mâˆ—
âˆ— jâˆ—,M
âˆ—
xjâˆ—,M
âˆ’ xÌ„M
t )+
t+1 âˆ’ xÌ„t+1 = Lt (xt

M âˆ’1 j
1 X k
Ï‰t+1 +
Ï‰t+1 .
M
M
kÌ¸=j
{z
}
|
j,M
Ï‰Ì„t+1

Hence
âˆ—
xjâˆ—,M
âˆ’ xÌ„M
=
t
t

t
X

Lâˆ—[tâˆ’1,s] Ï‰Ì„sj,M

s=0

ï£¶
ï£« P
k ï£¶
Ï‰0j
kÌ¸=j Ï‰0

1 ï£¬
M âˆ’1ï£¬ . ï£·
ï£·
..
= Î¨âˆ— Ï‰Ì„ j,M t , where Ï‰Ì„ j,M =
ï£­ .. ï£¸ +
ï£­
ï£¸
.
M
M
P
j
k
Ï‰
Ï‰T âˆ’1
kÌ¸=j T âˆ’1
ï£«

where the covariance matrix of Ï‰Ì„ j,M is E[Ï‰Ì„ j,M (Ï‰Ì„ j,M )âŠ¤ ] = diag(((M âˆ’1)/M Ã—Î£)0â‰¤tâ‰¤T ).
Similarly the infinite agent limit of this process is xâˆ—t âˆ’ xÌ„âˆ—t = (Î¨âˆ— Ï‰)t where Ï‰ =
(Ï‰0âŠ¤ , . . . , Ï‰TâŠ¤âˆ’1 )âŠ¤ whose covariance is E[Ï‰Ï‰ âŠ¤ ] = diag((Î£)0â‰¤tâ‰¤T ). Now we compute the
finite agent cost in terms of the noise processes,
i
JM
(uâˆ— )


T
1 X X jâˆ—,M
jâˆ—,M
Mâˆ— 2
Mâˆ— 2
Mâˆ— 2
Mâˆ— 2
=E
âˆ¥xt
âˆ’ xÌ„t âˆ¥Qt + âˆ¥xÌ„t âˆ¥QÌ„t + âˆ¥ut
âˆ’ uÌ„t âˆ¥Rt + âˆ¥uÌ„t âˆ¥RÌ„t
M
t=0
jâˆˆ[M ]



T
1 X X jâˆ—,M
Mâˆ— 2
Mâˆ— 2
=E
âˆ¥xt
âˆ’ xÌ„t âˆ¥Qt +(Ktâˆ— )âŠ¤ Rt Ktâˆ— + âˆ¥xÌ„t âˆ¥QÌ„t +(KÌ„ âˆ— )âŠ¤ RÌ„t KÌ„ âˆ—
t
t
M
jâˆˆ[M ] t=0


1 X
=E
(Î¨âˆ— Ï‰Ì„ j,M )âŠ¤ Q + (K âˆ— )âŠ¤ RK âˆ— Î¨âˆ— Ï‰Ì„ j,M
M
jâˆˆ[M ]

 âˆ— M
âˆ— âŠ¤
âˆ—
âˆ— M âŠ¤
+ (Î¨Ì„ Ï‰Ì„ ) QÌ„ + (KÌ„ ) RÌ„KÌ„ Î¨Ì„ Ï‰Ì„



= T r (Î¨âˆ— )âŠ¤ Q + (K âˆ— )âŠ¤ RK âˆ— Î¨âˆ— E Ï‰Ì„ j,M (Ï‰Ì„ j,M )âŠ¤



+ T r (Î¨Ì„âˆ— )âŠ¤ QÌ„ + (KÌ„ âˆ— )âŠ¤ RÌ„KÌ„ âˆ— Î¨Ì„âˆ— E Ï‰Ì„ M (Ï‰Ì„ M )âŠ¤
where Q = diag((Qt )0â‰¤T ), R = diag((Rt )0â‰¤T ) and K âˆ— = diag((Ktâˆ— )0â‰¤T ) with KTâˆ— = 0.
Using a similar technique we can compute the infinite agent cost:
X

T
J i (uâˆ— ) = E
âˆ¥xâˆ—t âˆ’ xÌ„âˆ—t âˆ¥2Qt + âˆ¥xÌ„âˆ—t âˆ¥2QÌ„t + âˆ¥uâˆ—t âˆ’ uÌ„âˆ—t âˆ¥2Rt + âˆ¥uÌ„âˆ—t âˆ¥2RÌ„t
t=0




= T r (Î¨âˆ— )âŠ¤ Q + (K âˆ— )âŠ¤ RK âˆ— Î¨âˆ— E Ï‰Ï‰ âŠ¤



+ T r (Î¨Ì„âˆ— )âŠ¤ QÌ„ + (KÌ„ âˆ— )âŠ¤ RÌ„KÌ„ âˆ— Î¨Ì„âˆ— E Ï‰Ì„ 0 (Ï‰Ì„ 0 )âŠ¤ .
40

Now evaluating the difference between the finite and infinite population costs:
i
JM
(uâˆ— ) âˆ’ J i (uâˆ— )





= T r (Î¨âˆ— )âŠ¤ Q + (K âˆ— )âŠ¤ RK âˆ— Î¨âˆ— E Ï‰Ì„ j,M (Ï‰Ì„ j,M )âŠ¤ âˆ’ E Ï‰Ï‰ âŠ¤





+ T r (Î¨Ì„âˆ— )âŠ¤ QÌ„ + (KÌ„ âˆ— )âŠ¤ RÌ„KÌ„ âˆ— Î¨Ì„âˆ— E Ï‰Ì„ M (Ï‰Ì„ M )âŠ¤ âˆ’ E Ï‰Ì„ 0 (Ï‰Ì„ 0 )âŠ¤




â‰¤ âˆ¥(Î¨âˆ— )âŠ¤ Q + (K âˆ— )âŠ¤ RK âˆ— Î¨âˆ— âˆ¥F + âˆ¥(Î¨Ì„âˆ— )âŠ¤ QÌ„ + (KÌ„ âˆ— )âŠ¤ RÌ„KÌ„ âˆ— Î¨Ì„âˆ— âˆ¥F
|
{z
}
C1


T r diag((Î£/M )0â‰¤tâ‰¤T )
(F.3)
â‰¤ C1

ÏƒT
M

where Ïƒ = âˆ¥Î£âˆ¥F . Now let us consider the same dynamics but under non-NE controls.
The finite and infinite population costs under these controls are
 

i
JM
(u) = T r Î¨âŠ¤ Q + K âŠ¤ RK Î¨E Ï‰Ì„ j,M (Ï‰Ì„ j,M )âŠ¤
 

+ T r Î¨Ì„âŠ¤ QÌ„ + KÌ„ âŠ¤ RÌ„KÌ„ Î¨Ì„E Ï‰Ì„ M (Ï‰Ì„ M )âŠ¤
 

 

J i (u) = T r Î¨âŠ¤ Q + K âŠ¤ RK Î¨E Ï‰Ï‰ âŠ¤ + T r Î¨Ì„âŠ¤ QÌ„ + KÌ„ âŠ¤ RÌ„KÌ„ Î¨Ì„E Ï‰Ì„ 0 (Ï‰Ì„ 0 )âŠ¤
with all matrices defined accordingly. Let us denote the control which infimizes the
i
i
M agent cost as uÌƒM , meaning inf u JM
(u) = JM
(uÌƒ). Using the same techniques as
before we get
i
JM
(uÌƒ) âˆ’ J i (uÌƒ)


Ëœ âŠ¤ RÌ„KÌ„
Ëœ Î¨Ì„
Ëœ + Î¨âŠ¤ Q + K âŠ¤ RK Î¨ diag((Î£/M )
Ëœ âŠ¤ QÌ„ + KÌ„
= T r Î¨Ì„
0â‰¤tâ‰¤T )

â‰¥ Î»min (Q0 )T r diag((Î£/M )0â‰¤tâ‰¤T )
ÏƒT
.
= Î»min (Q0 )
M

Using this we can further deduce
i
JM
(uÌƒ) â‰¥ J i (uÌƒ) + Î»min (Q0 )

(F.4)

ÏƒT
ÏƒT
â‰¥ J i (uâˆ— ) + Î»min (Q0 )
.
M
M

Hence we deduce
i
i
i
i
JM
(uâˆ— ) âˆ’ inf JM
(u) = JM
(uâˆ— ) âˆ’ J i (uâˆ— ) + J i (uâˆ— ) âˆ’ inf JM
(u) â‰¤ (C1 + Î»min (Q0 ))
u

u

ÏƒT
M

which completes the proof.
Appendix G. Proof of Lemma 3.1. [Policy Gradient Characterization]
We characterize the policy gradient of the cost by first proving the quadratic structure
of the cost. For a fixed t âˆˆ {0, . . . , T âˆ’ 1} and a given set of controllers (K i , K âˆ’i )
consider the partial cost
i
i
âˆ’i
JËœy,[t
)=
â€² ,T ] (K , K



T
X

E ysâŠ¤ Qis + (Ksi )âŠ¤ Rsi Ksi ys
s=tâ€²

41

i
Ëœi
for t â‰¤ tâ€² â‰¤ T and KT = 0. From direct calculation JËœy,[t,T
] = Jy,t . First will show
that JËœi â€² has a quadratic structure
y,[t ,T ]

(G.1)

i
i
âˆ’i
i
i
JËœy,[t
) = E[ytâŠ¤â€² Py,t
â€² ytâ€² ] + Ny,tâ€²
â€² ,T ] (K , K

i
where Py,t
is defined as in (3.6) and
i
i
i
i
Ny,t
= Ny,t+1
+ T r(Î£Py,t+1
), Ny,T
=0

where Î£ = diag((Î£i )iâˆˆ[N ] ). The hypothesis is true for the base case since cost
i
i
âˆ’i
âŠ¤ i
âŠ¤ i
i
JËœy,[T,T
] (K , K ) = E[yT QT yT ] = E[yT Py,T yT ] + Ny,T . Now assume for a given
tâ€² âˆˆ {t, . . . , T âˆ’ 1}, JËœi â€²
(K i , K âˆ’i ) = E[y âŠ¤â€² P i â€² ytâ€² +1 ] + N i â€² .
t +1 y,t +1

y,[t +1,T ]

y,t +1


 
i
i
âˆ’i
i
JËœy,[t
) = E ytâŠ¤â€² Qitâ€² + (Ktiâ€² )âŠ¤ Rtiâ€² Ktiâ€² ytâ€² + JËœy,[t
â€² ,T ] (K , K
â€² +1,T ] ,
 âŠ¤ i


i âŠ¤ i
i
âŠ¤
i
i
= E ytâ€² Qtâ€² + (Ktâ€² ) Rtâ€² Ktâ€² ytâ€² + E[ytâ€² +1 Py,tâ€² +1 ytâ€² +1 ] + Ny,t
â€² +1
 
 
N
N
X
X
j âŠ¤ i
j j
j
âŠ¤
i
i âŠ¤ i
i
= E ytâ€² Qtâ€² + (Ktâ€² ) Rtâ€² Ktâ€² + Atâ€² âˆ’
Btâ€² Ktâ€² Py,tâ€² +1 Atâ€² âˆ’
Bt Ktâ€² ytâ€²
j=1

j=1
i
i
+ T r(Î£Py,t
â€² +1 ) + Ny,tâ€² +1

i
i
= E[ytâŠ¤â€² Py,t
â€² ytâ€² ] + Ny,tâ€² .

Hence we have shown (G.1), which implies
i
i
i
JËœy,t
(K i , K âˆ’i ) = E[ytâŠ¤ Py,t
yt ] + Ny,t
.
i
Using (3.6) we can also write the cost JËœy,t
(K i , K âˆ’i ) in terms of the controller Kti as

"
i
i
JËœy,t
(K i , K âˆ’i ) = Ny,t
+E



i
ytâŠ¤ Qit + (Kti )âŠ¤ Rti + (Bti )âŠ¤ Py,t+1
Bti Kti

i
âˆ’ 2(Bti Kti )âŠ¤ Py,t+1
Ait âˆ’

X

Btj Ktj



jÌ¸=i

+ Ait âˆ’

X

j âŠ¤

Btj Kt

jÌ¸=i

i
Py,t+1
Ait âˆ’

X

Btj Ktj



 
yt .

jÌ¸=i

Taking the derivative with respect to Kti and using the fact that E[yt ytâŠ¤ ] = Î£y we can
conclude the proof:
i
X j j 
(K i , K âˆ’i )
Î´ JËœy,t
i
i
= 2 (Rti + (Bti )âŠ¤ Py,t+1
Bti )Kti âˆ’ (Bti )âŠ¤ Py,t+1
At âˆ’
Bt K t Î£ y .
i
Î´Kt
jÌ¸=i

Appendix H. Proof of Lemma 4.1. [Polyak-Lojasiewicz inequality] We
show that the gradient domination property (PL condition) is much simpler than those
in the literature, as the receding-horizon approach obviates the need for computing the
advantage function and the cost difference lemma as in [2, 11, 12]. Especially upon
comparison with with Lemma 3.9 in [24] one can see that there is no summation in
the PL condition and moreover the PL condition does not depend on the covariance
42

âˆ¥Î£K âˆ— âˆ¥ but instead it depends on âˆ¥Î£y âˆ¥ which is a parameter we can modify. Let us
iâˆ—
iâˆ—
start by defining the matrix sequences Py,t
and Ny,t
such that
(H.1)
iâˆ—
Py,t
= Qit + (Ktiâˆ— )T Rti Ktiâˆ— + (At âˆ’

N
X

iâˆ—
Btj Ktjâˆ— )âŠ¤ Py,t+1
(At âˆ’

j=1

N
X

iâˆ—
Btj Ktjâˆ— ), Py,T
= QiT .

j=1

and
iâˆ—
iâˆ—
iâˆ—
iâˆ—
Ny,t
= Ny,t+1
+ T r(Î£Py,t+1
), Ny,T
=0

From (G.1) in proof of Lemma 3.1 we know that
i
iâˆ—
iâˆ—
i
iâ€²
iâ€²
JËœy,t
(K iâˆ— , K âˆ’iâˆ— ) = E[ytâŠ¤ Py,t
yt ] + Ny,t
and JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) = E[ytâŠ¤ Py,t
yt ] + Ny,t
â€²

i
iâˆ—
iâˆ—
) = Ntiâˆ— and
where Ny,t
= Ny,t+1
+ T r(Î£Py,t+1

(H.2)
â€²

i
Py,t
=

Qit + (Kti )T Rti Kti + (At âˆ’

N
X

iâˆ—
Btj Ktjâˆ— âˆ’ Bti Kti )âŠ¤ Py,t+1
(At âˆ’

jÌ¸=i

N
X

Btj Ktjâˆ— âˆ’ Bti Kti )

jÌ¸=i

Using (H.1) and (H.2) we can deduce
i
i
i
i
JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) âˆ’ JËœy,t
(K iâˆ— , K âˆ’iâˆ— ) = âˆ’(JËœy,t
(K iâˆ— , K âˆ’iâˆ— ) âˆ’ JËœy,t
(KÌƒ i , K âˆ’iâˆ— ))
 âŠ¤ i
= âˆ’E yt Qt + (Ktiâˆ— )âŠ¤ Rti Ktiâˆ—

+ At âˆ’

N
X

Btj Ktjâˆ—

âŠ¤

iâˆ—
Py,t+1
At âˆ’

j=1

N
X


 
iâ€²
Btj Ktjâˆ— âˆ’ Py,t
yt

j=1


= âˆ’E ytâŠ¤ Qit + (Ktiâˆ— âˆ’ Kti + Kti )âŠ¤ Rti (Ktiâˆ— âˆ’ Kti + Kti )
X j jâˆ—
âŠ¤ iâˆ—
+ At âˆ’
Bt Kt âˆ’ Bti (Ktiâˆ— âˆ’ Kti + Kti ) Py,t+1
jÌ¸=i

At âˆ’

X


 
iâ€²
Btj Ktjâˆ— âˆ’ Bti (Ktiâˆ— âˆ’ Kti + Kti ) âˆ’ Py,t
yt

jÌ¸=i


iâˆ—
= âˆ’E ytâŠ¤ (Ktiâˆ— âˆ’ Kti )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )(Ktiâˆ— âˆ’ Kti )
X j jâˆ—
 
iâˆ—
+ 2(Ktiâˆ— âˆ’ Kti )âŠ¤ Rti Kti âˆ’ (Bti )âŠ¤ Py,t+1
(At âˆ’
Bt Kt âˆ’ Bti Kti ) yt
jÌ¸=i

Let us define the natural gradient [11, 12] as
i i
i âŠ¤ iâˆ—
Etiâˆ— = âˆ‡iy,t (KÌƒ i , K âˆ’iâˆ— )Î£âˆ’1
y = Rt Kt âˆ’ (Bt ) Py,t+1 (At âˆ’

X
jÌ¸=i

43

Btj Ktjâˆ— âˆ’ Bti Kti ).

Then, using completion of squares we get
i
i
JËœy,t
(KÌƒ i , K âˆ’iâˆ— ) âˆ’ JËœy,t
(K iâˆ— , K âˆ’iâˆ— )

(H.3)

 
iâˆ—
= âˆ’E ytâŠ¤ (Ktiâˆ— âˆ’ Kti )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )(Ktiâˆ— âˆ’ Kti ) + 2(Ktiâˆ— âˆ’ Kti )âŠ¤ Etiâˆ— yt

iâˆ—
= âˆ’E ytâŠ¤ (Ktiâˆ— âˆ’ Kti )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )(Ktiâˆ— âˆ’ Kti ) + 2(Ktiâˆ— âˆ’ Kti )âŠ¤ Etiâˆ—
 
iâˆ—
iâˆ—
+ (Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— âˆ’ (Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— yt

iâˆ—
iâˆ—
= âˆ’E ytâŠ¤ (Ktiâˆ— âˆ’ Kti + (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )
 
iâˆ—
(Ktiâˆ— âˆ’ Kti + (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— ) yt

 
iâˆ—
+ E ytâŠ¤ (Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— yt

 
iâˆ—
â‰¤ E ytâŠ¤ (Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ— yt


iâˆ—
= E T r yt ytâŠ¤ (Etiâˆ— )âŠ¤ (Rti + (Bti )âŠ¤ Py,t+1
Bti )âˆ’1 Etiâˆ—
(H.4)
âˆ¥Î£y âˆ¥ iâˆ— 2
âˆ¥Et âˆ¥F
â‰¤
ÏƒR
(H.5)
âˆ¥Î£y âˆ¥ i
âˆ¥âˆ‡y,t (KÌƒ i , K âˆ’iâˆ— )âˆ¥2F .
â‰¤
ÏƒR Ïƒy2
This concludes the proof.
Appendix I. Cost augmentation technique and O(Î³)-Nash equilibrium.
In this section we will prove how each agent can independently use a cost-augmentation
technique to ensure the satisfaction of the diagonal domination condition (Assumption
4.3) hence ensuring convergence of the MRPG algorithm. We also prove that this
cost augmentation results in an O(Î³)-Nash equilibrium where Î³ is the max over the
cost augmentation parameters. First we introduce the Î³ i -augmented cost functions as
follows.
T
hX
 i
i
JÂ¨y,t
(K i , K âˆ’i , Î³ i ) = E
(1 + Î³si )ysâŠ¤ Qis + (Ksi )âŠ¤ Rsi Ksi ys ,
s=t
T
hX
 i
i
i
i âŠ¤ i
i
JÂ¨xÌ„,t
(K i , K âˆ’i , Î³ i ) = E
(1 + Î³si )xÌ„âŠ¤
QÌ„
+
KÌ„
)
RÌ„
KÌ„
s
s
s
s s xÌ„s
s=t

The Î³ti â‰¥ 0 are chosen such that they satisfy the gradient domination condition which
can be written as follows.
p
(I.1)

Î³ti â‰¥

2
2m(N âˆ’ 1)Î³B,t
Î³PÌˆi ,t+1

Ïƒ(Rti )

âˆ’1

The stochastic gradient of the augmented cost function is,
Â¨ iy,t (K i , K âˆ’i ) =
âˆ‡

Nb
m X
JÂ¨i (KÌ‚ i (ej , t), K âˆ’i )ej
Nb r2 j=1 y,t

Â¨ ixÌƒ,t (KÌ„ i , KÌ„ âˆ’i ) =
âˆ‡

Nb
m X
Ë† i (e , t), KÌ„ âˆ’i )e
JÂ¨i (KÌ„
j
j
Nb r2 j=1 xÌƒ,t

44

respectively, where ej âˆ¼ SpN Ã—mN (r) is the perturbation and KÌ‚ i (e, t) := (Kti +
e, . . . , KTi âˆ’1 ) is the perturbed controller set at timestep t. Nb denotes the mini-batch
size and r the smoothing radius of the stochastic gradient. The MRPG algorithm for
this augmented cost is very similar to Algorithm 3.1 but it utilizes the augmented
Â¨ i and âˆ‡
Â¨ i and also a projection operator ProjD where D > 0
stochastic gradients âˆ‡
y,t
xÌƒ,t
is large enough.
Algorithm I.1 MRPG for augmented cost GS-MFTG
1: Initialize Kti = 0, KÌ„ti = 0 for all i âˆˆ [N ], t âˆˆ {0, . . . , T âˆ’ 1}
2: for t = T âˆ’ 1, . . . , 1, 0, do
3:
for k = 1, . . . , K do
4:
Natural Policy Gradient for i âˆˆ [N ]


(I.2)

Kti
KÌ„ti



  i
 i

Â¨ y,t (K i , K âˆ’i )Î£âˆ’1
âˆ‡
Kt
i
y
â† ProjD
âˆ’ Î·k Â¨ i
KÌ„ti
âˆ‡xÌƒ,t (KÌ„ i , KÌ„ âˆ’i )Î£âˆ’1
xÌƒ

5:
end for
6: end for

Since the diagonal dominance condition is satisfied due to (I.1), Algorithm I.1 will
converge linearly to the NE of the cost augmented GS-MFTG game. Now we prove
that the NE under the augmented cost structure is O(Î³Ì„) away from the NE of the
original game. In this section we only deal with the deviation process (yt )âˆ€t (2.5)-(2.6)
and similar guarantees can be obtained for the mean-field process (xÌ„t )âˆ€t .
Theorem I.1. The NE under the augmented cost structure denoted as set of
controllers (KÌˆtiâˆ— )âˆ€i,t such that
(I.3)

JÂ¨yi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) â‰¤ JÂ¨yi (K i , KÌˆ âˆ’iâˆ— , Î³ i ), âˆ€i âˆˆ [N ], K âˆˆ RpÃ—m

is an O(Î³)-Nash equilibrium for the GS-MFTG (2.3)-(2.4).
(I.4)

Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) â‰¤ Jyi (K, KÌˆ âˆ’iâˆ— ) + O(Î³), âˆ€i âˆˆ [N ], K âˆˆ RpÃ—m

where Î³ = max0â‰¤tâ‰¤T âˆ’1,iâˆˆ[N ] Î³ti . Also the difference between NE cost and augmented
NE cost are
|JÂ¨yi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) âˆ’ Jyi (K iâˆ— , K âˆ’iâˆ— )|= O(Î³).
Proof. Let us first define the augmented cost function
T
hX
 i
i
JÂ¨y,t
(K i , K âˆ’i , Î³ i ) = E
(1 + Î³ti )ysâŠ¤ Qis + (Ksi )âŠ¤ Rsi Ksi ys
s=t

with Î³Ti = 0 with i âˆˆ [N ]. Using a similar argument this augmented cost can be
written down as,
i
JÂ¨y,t
(K i , K âˆ’i , Î³ i ) = E[ytâŠ¤ PÌˆti yt ] + NÌˆti

45

where PÌˆti and NÌˆti can be written recursively as,
PÌˆti = (1 + Î³ti )(Qit + (Kti )âŠ¤ Rti Kti )
+ (At +

N
X

i
Btj Ktj )âŠ¤ PÌˆt+1
(At +

j=1

(I.5)

i
i
NÌˆti = NÌˆt+1
+ T r(Î£PÌˆt+1
),

N
X

Btj Ktj ), PÌˆTi = QiT

j=1

NÌˆTi = 0

Recalling the non-augmented cost can be written down as,
i
Jy,t
(K i , K âˆ’i ) = E[ytâŠ¤ Pti yt ] + Nti

where Pti and Nti can be written recursively as,
Pti = (Qit + (Kti )âŠ¤ Rti Kti ) + (At +

N
X

i
Btj Ktj )âŠ¤ Pt+1
(At +

j=1
i
i
(I.6) Nti = Nt+1
+ T r(Î£Pt+1
),

N
X

Btj Ktj ), PTi = QiT

j=1

NTi = 0

The difference between the augmented and non-augmented costs is as follows.
i
i
|JÂ¨y,t
(K i , K âˆ’i , Î³ i ) âˆ’ Jy,t
(K i , K âˆ’i )|

= E[ytâŠ¤ (PÌˆti âˆ’ Pti )yt ] + NÌˆti âˆ’ Nti = T r(Î£y (PÌˆti âˆ’ Pti )) + NÌˆti âˆ’ Nti

(I.7)

i
i
So to quantify the difference JÂ¨y,t
(K i , K âˆ’i , Î³ i ) âˆ’ Jy,t
(K i , K âˆ’i ) we need to upper bound
i
i
i
i
âˆ¥PÌˆt âˆ’ Pt âˆ¥ and |NÌˆt âˆ’ Nt |. Using (I.5) and (I.6) the quantity |NÌˆti âˆ’ Nti | is

|NÌˆti âˆ’ Nti | =

(I.8)

T
âˆ’1
X

T r(Î£(PÌˆsi âˆ’ Psi ))

s=t+1

The quantity âˆ¥PÌˆti âˆ’ Pti âˆ¥ can be bounded as follows.
âˆ¥PÌˆti âˆ’ Pti âˆ¥
= âˆ¥Î³ti (Qit + (Kti )âŠ¤ Rti Kti ) + (At +

N
X

i
i
Btj Ktj )âŠ¤ (PÌˆt+1
âˆ’ Pt+1
)(At +

j=1

â‰¤ Î³ti (Î³Q + Î³R âˆ¥Kti âˆ¥2 ) + (Î³A +

N
X

N
X

Btj Ktj )âˆ¥

j=1

i
i
Î³B âˆ¥Ktj âˆ¥)2 âˆ¥PÌˆt+1
âˆ’ Pt+1
âˆ¥

j=1

(I.9)

i
i
â‰¤ Î³ti (Î³Q + Î³R D2 ) + (Î³A + N Î³B D)2 âˆ¥PÌˆt+1
âˆ’ Pt+1
âˆ¥

|

{z
c1

}

|

{z
c2

}

Using (I.9) we can recursively say
(I.10)

âˆ¥PÌˆti âˆ’ Pti âˆ¥ â‰¤ c1

T
âˆ’1
X

T âˆ’t
i
csâˆ’t
âˆ¥PÌˆTi âˆ’ PTi âˆ¥ = O
2 Î³s + c2

s=t

Similarly
(I.11)

|NÌˆti âˆ’ Nti | = O(Î³)
46



max

tâ‰¤sâ‰¤T âˆ’1


Î³si = O(Î³)

using (I.8) and using (I.7) with t = 0
|JÂ¨yi (K i , K âˆ’i , Î³ i ) âˆ’ Jyi (K i , K âˆ’i )| = O(Î³)

(I.12)

Using (I.3) and (I.12) for any K i âˆˆ R(pÃ—m)Ã—T ,
(I.13)
Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ JÂ¨yi (K i , KÌˆ âˆ’iâˆ— , Î³ i ) â‰¤ Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ JÂ¨yi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) = O(Î³)
Now let us fix i âˆˆ [N ] and define a set of controllers (K i , KÌˆ âˆ’iâˆ— ) where K i âˆˆ R(pÃ—m)Ã—T .
Ëœ i as follows.
Let us define PÌƒ i , PÌˆËœ i , NÌƒ i and NÌˆ
t

t

t

t

PÌˆËœti = (1 + Î³ti )(Qit + (Kti )âŠ¤ Rti Kti )
+ (At + Bti Kti +

N
X

i
Btj KÌˆtjâˆ— )âŠ¤ PÌˆËœt+1
(At + Bti Kti +

jÌ¸=i

N
X

Btj KÌˆtjâˆ— ), PÌˆËœTi = QiT

jÌ¸=i

Ëœ i = NÌˆ
Ëœ i + T r(Î£PÌˆËœ i ), NÌˆ
Ëœi = 0
NÌˆ
t
t+1
t+1
T
PÌƒti = (Qit + (Kti )âŠ¤ Rti Kti )
+ (At + Bti Kti +

N
X

i
Btj KÌˆtjâˆ— )âŠ¤ PÌƒt+1
(At + Bti Kti +

jÌ¸=i
i
i
NÌƒti = NÌƒt+1
+ T r(Î£PÌƒt+1
),

N
X

Btj KÌˆtjâˆ— ), PÌƒTi = QiT

jÌ¸=i

NÌƒTi = 0

Using these expressions we can deduce
Ëœ i + NÌƒ i
Jyi (K i , KÌˆ âˆ’iâˆ— ) = JÂ¨yi (K i , KÌˆ âˆ’iâˆ— , Î³ i ) âˆ’ T r(Î£y (PÌˆËœ0i âˆ’ PÌƒ0i )) âˆ’ NÌˆ
0
0
Ëœ
Ëœ
i
iâˆ—
âˆ’iâˆ—
i
i
i
i
Â¨
â‰¥ J (KÌˆ , KÌˆ , Î³ ) âˆ’ T r(Î£ (PÌˆ âˆ’ PÌƒ )) âˆ’ NÌˆ + NÌƒ i

(I.14)

y

y

0

0

0

0

Using analysis similar to (I.5)-(I.11) we can deduce that âˆ¥PÌˆËœ0i âˆ’ PÌƒ0i âˆ¥ = O(Î³) and
Ëœ i âˆ’ NÌƒ i | = O(Î³). Using (I.13)-(I.14) we can write
|NÌˆ
0

0

Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ Jyi (K i , KÌˆ âˆ’iâˆ— )
= Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ JÂ¨yi (KÌˆ i , KÌˆ âˆ’iâˆ— , Î³ i ) + JÂ¨yi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) âˆ’ Jyi (K i , KÌˆ âˆ’iâˆ— )
â‰¤ Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ JÂ¨yi (KÌˆ i , KÌˆ âˆ’iâˆ— , Î³ i ) + Jyi (K i , KÌˆ âˆ’iâˆ— ) âˆ’ Jyi (K i , KÌˆ âˆ’iâˆ— )
Ëœ i âˆ’ NÌƒ i
+ T r(Î£ (PÌˆËœ i âˆ’ PÌƒ i )) + NÌˆ
y

â‰¤

Jyi (KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— ) âˆ’ JÂ¨yi (KÌˆ i , KÌˆ âˆ’iâˆ— , Î³ i )

0

0

0

0

Ëœ i âˆ’ NÌƒ i = O(Î³)
+ T r(Î£y (PÌˆËœ0i âˆ’ PÌƒ0i )) + NÌˆ
0
0

which results in (I.4).
i
i
Similarly we also bound the difference between JÂ¨y,t
(KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i )âˆ’Jy,t
(K iâˆ— , K âˆ’iâˆ— )
where (Ktiâˆ— )âˆ€i,t is the NE controllers and (KÌˆtiâˆ— )âˆ€i,t are the NE controllers under the
augmented cost. These matrices are defined as follows,
T
hX
 i
i
JÂ¨y,t
(KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) = E
(1 + Î³ti )ysâŠ¤ Qis + (KÌˆsiâˆ— )âŠ¤ Rsi KÌˆsiâˆ— ys
s=t

47

with Î³Ti = 0 with i âˆˆ [N ]. Using a similar argument this augmented cost can be
written down as,
i
JÂ¨y,t
(KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) = E[ytâŠ¤ PÌˆtiâˆ— yt ] + NÌˆtiâˆ—

where PÌˆti and NÌˆti can be written recursively as,
iâˆ—
PÌˆtiâˆ— = (1 + Î³ti )(Qit + (KÌˆtiâˆ— )âŠ¤ Rti KÌˆtiâˆ— ) + (AÌˆâˆ—t )âŠ¤ PÌˆt+1
AÌˆâˆ—t , PÌˆTiâˆ— = QiT
iâˆ—
iâˆ—
NÌˆtiâˆ— = NÌˆt+1
+ T r(Î£PÌˆt+1
), NÌˆTiâˆ— = 0
PN
where AÌˆâˆ—t = At + j=1 Btj KÌˆtjâˆ— and KÌˆtiâˆ— is the NE according to the augmented cost
defined as follows.

(I.15)

iâˆ—
iâˆ—
KÌˆtiâˆ— = âˆ’((1 + Î³ti )Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
t
P
j
jâˆ—
where AÌˆiâˆ—
t = At +
jÌ¸=i Bt KÌˆt . Recalling the non-augmented cost can be written
down as,
i
Jy,t
(K iâˆ— , K âˆ’iâˆ— ) = E[ytâŠ¤ Ptiâˆ— yt ] + Ntiâˆ—

where Ptiâˆ— and Ntiâˆ— can be written recursively as,
iâˆ—
Ptiâˆ— = (Qit + (Ktiâˆ— )âŠ¤ Rti Ktiâˆ— ) + (Aâˆ—t )âŠ¤ Pt+1
Aâˆ—t , PTiâˆ— = QiT
iâˆ—
iâˆ—
Ntiâˆ— = Nt+1
+ T r(Î£Pt+1
), NÌˆTiâˆ— = 0
PN
where Aâˆ—t = At + j=1 Btj Ktjâˆ— and Ktiâˆ— is the NE according to the augmented cost
defined as follows.

(I.16)

iâˆ—
iâˆ—
Ktiâˆ— = âˆ’(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t
P
j
jâˆ—
where Aiâˆ—
t = At +
jÌ¸=i Bt Kt . First we characterize the difference between the NE
controllers under the two cost functions.
iâˆ—
iâˆ—
KÌˆtiâˆ— âˆ’ Ktiâˆ— = âˆ’((1 + Î³ti )Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
t
iâˆ—
iâˆ—
+ (Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 (Bti )âŠ¤ Pt+1
Aiâˆ—
t
iâˆ—
iâˆ—
Â¨ t,1
Â¨ t,2
= âˆ†K
âˆ’ âˆ†K

where


iâˆ—
i
i
iâˆ—
Â¨ t,1
âˆ†K
= âˆ’ ((1 + Î³ti )Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 âˆ’ (Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
t
iâˆ—
Â¨
âˆ†Kt,2 =
i
iâˆ—
i
i âŠ¤ i
i âˆ’1
iâˆ—
âˆ’ (Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
(Bti )âŠ¤ Pt+1
Aiâˆ—
t + (Rt + (Bt ) Pt+1 Bt )
t

Using the equality Aâˆ’1 âˆ’ B âˆ’1 = Aâˆ’1 BB âˆ’1 âˆ’ Aâˆ’1 AB âˆ’1 = Aâˆ’1 (B âˆ’ A)B âˆ’1 for any
Â¨ iâˆ— as
set of matrices A and B whose inverses exist we can simplify the expression âˆ†K
t,1
follows.


iâˆ—
i
i
iâˆ—
Â¨ t,1
âˆ†K
= âˆ’ ((1 + Î³ti )Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Î³ti Rti )(Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 (Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
t
Hence the norm of this expression can be upper bounded by
iâˆ—
iâˆ—
iâˆ—
iâˆ—
Â¨ t,1
âˆ¥âˆ†K
âˆ¥ â‰¤ Î³ti âˆ¥Rti âˆ¥âˆ¥(Rti + (Bti )âŠ¤ PÌˆt+1
Bti )âˆ’1 âˆ¥âˆ¥(Rti + (Bti )âŠ¤ Pt+1
Bti )âˆ’1 âˆ¥âˆ¥(Bti )âŠ¤ PÌˆt+1
AÌˆiâˆ—
t âˆ¥
|
{z
}
cÌ„0

48

Â¨ iâˆ— can be bounded using techniques from proof
The norm of the second expression âˆ†K
t,2
of Theorem 4.6.
iâˆ—
iâˆ—
iâˆ—
âˆ—
Â¨ t,2
âˆ†K
â‰¤ âˆ¥Î¦Ìˆâˆ’1
t+1 âˆ¥âˆ¥At âˆ¥Î³B âˆ¥PÌˆt+1 âˆ’ Pt+1 âˆ¥
|
{z
}
cÌ„1

where
ï£«
1âˆ—
(1 + Î³t1 )Rt1 + (Bt1 )âŠ¤ PÌˆt+1
Bt1
2 âŠ¤ 2âˆ—
1
ï£¬
(Bt ) PÌˆt+1 Bt
ï£¬
Î¦Ìˆt+1 = ï£¬
..
ï£­
.
Nâˆ— 1
(BtN )âŠ¤ PÌˆt+1
Bt

...
...
..
.

1âˆ—
(Bt1 )âŠ¤ PÌˆt+1
BtN
2 âŠ¤ 2âˆ—
(Bt ) PÌˆt+1 BtN
..
.

...

Nâˆ— N
(1 + Î³tN )RtN + (BtN )âŠ¤ PÌˆt+1
Bt

ï£¶
ï£·
ï£·
ï£·
ï£¸

This matrix is invertible if Î³ti are chosen so as to satisfy the diagonal dominance
condition. The difference between the augmented and non-augmented costs at their
corresponding NE is as follows.
i
i
JÂ¨y,t
(KÌˆ iâˆ— , KÌˆ âˆ’iâˆ— , Î³ i ) âˆ’ Jy,t
(K iâˆ— , K âˆ’iâˆ— ) = E[ytâŠ¤ (PÌˆtiâˆ— âˆ’ Ptiâˆ— )yt ] + NÌˆtiâˆ— âˆ’ Ntiâˆ—

= T r(Î£y (PÌˆtiâˆ— âˆ’ Ptiâˆ— )) + NÌˆtiâˆ— âˆ’ Ntiâˆ—

(I.17)

So now we characterize the difference PÌˆtiâˆ— âˆ’ Ptiâˆ— using the alternate expressions for
both the matrices.
âŠ¤ iâˆ—
iâˆ—
i iâˆ—
iâˆ— âŠ¤ iâˆ—
iâˆ—
i iâˆ—
PÌˆtiâˆ— âˆ’ Ptiâˆ— = (AÌˆiâˆ—
t ) PÌˆt+1 (AÌˆt + Bt KÌˆt ) âˆ’ (At ) Pt+1 (At + Bt Kt )

Using this expression we bound the norm of PÌˆtiâˆ— âˆ’ Ptiâˆ— as follows.
âˆ— âˆ—
âˆ—
iâˆ—
4
âˆ¥PÌˆtiâˆ— âˆ’ Ptiâˆ— âˆ¥ â‰¤ (2Î³A
Î³P Î³B + (Î³A
/2 + Î³Pâˆ— Î³B + Î³A
/2)Î³B + Î³B
/2)
|
{z
}
cÌ„2

N
X
âˆ¥KÌˆtjâˆ— âˆ’ Ktjâˆ— âˆ¥
j=1

iâˆ—
âˆ—
iâˆ— 2 âˆ—
iâˆ—
+ (Î³A
/2 + 1/2 + (Î³A
) Î³A /2)âˆ¥PÌˆt+1
âˆ’ Pt+1
âˆ¥
|
{z
}
cÌ„3

(I.18)

iâˆ—
iâˆ—
âˆ’ Pt+1
âˆ¥ + cÌ„0 cÌ„2 N Î³ti
â‰¤ (cÌ„1 cÌ„2 N + cÌ„3 )âˆ¥PÌˆt+1
| {z }
{z
}
|
cÌ„5

cÌ„4

Using (I.18) recursively we get
âˆ¥PÌˆtiâˆ— âˆ’ Ptiâˆ— âˆ¥ â‰¤ cÌ„5

TX
âˆ’tâˆ’1

i
cÌ„s4 Î³t+s
+ cÌ„T4 âˆ’t âˆ¥PÌˆTiâˆ— âˆ’ PTiâˆ— âˆ¥

s=0

(I.19)

â‰¤ cÌ„5 cÌ„T4 âˆ’t (T âˆ’ t)

max

0â‰¤sâ‰¤T âˆ’tâˆ’1

i
Î³s+j
=O



Similarly (I.15) and (I.16)
(I.20)

|NÌˆtiâˆ— âˆ’ Ntiâˆ— | = O(Î³)

using (I.17) with t = 0, (I.19) and (I.20) we arrive at
|JÂ¨yi (K i , K âˆ’i , Î³ i ) âˆ’ Jyi (K i , K âˆ’i )| = O(Î³)
49

max

tâ‰¤sâ‰¤T âˆ’1

Î³si



Appendix J. MRPG algorithm: Analysis with sample-path gradients.
In this section we will show how to utilize the sample-paths of N teams comprising
M agents each to approximate the expected cost in stochastic gradient computation
(3.7). This empirical cost is referred to as the sample-path cost, and the sample-path
costs for the stochastic processes yt and xÌƒt ((3.2) and (3.1)) are defined as below.

(J.1)

M T
1 XX j âŠ¤ i
i
JË‡y,t
(K i , K âˆ’i ) =
(y ) (Qt + (Ksi )âŠ¤ Rti Ksi )ysj ,
M j=1 s=t s

(J.2)

i
JË‡xÌƒ,t
(KÌ„ i , KÌ„ âˆ’i ) =

T
X
(xÌƒs )âŠ¤ (QÌ„it + (KÌ„si )âŠ¤ RÌ„ti KÌ„si )xÌƒs ,
s=t

where KTi , KÌ„Ti = 0. Most literature [2, 11, 12] uses the sample-path cost as a stand-in
for the expected cost to be used in stochastic gradient computation as in (3.7). [12]
mentions that the sample-path cost is an unbiased estimator of the expected cost
(under a stabilizing control policy) and is Î´ close to the expected cost, if the length
of the sample path is O(log(1/Î´)). We show that this intuition although true for the
infinite-horizon ergodic-cost setting, does not carry over to the finite-horizon or even
infinite-horizon discounted-cost setting. The finite horizon setting prevents us from
obtaining sample paths of arbitrary lengths, which results in an approximation error.
We show in Lemma J.1 that the sample-path cost is an unbiased but high variance
estimator of the expected cost. And using the Markov inequality we show how to use
mini-batch size Nb to get a good estimate of the stochastic gradient.
Similar analysis can be carried out for the infinite-horizon discounted-cost setting
and we provide an outline of this argument below. In the infinite-horizon ergodic-cost
setting the expected cost essentially depends on the infinite tail of the Markov chain
which achieves a stationary distribution since all stabilizing controllers in stochastic
LQ settings ensure unique stationary distributions. As a result the sample path cost
is bound to approach the expected cost as the Markov chain distribution approaches
the stationary distribution. On the other hand, in the infinite-horizon discounted cost
setting, the expected cost depends on the head of the Markov chain (with an effective
time horizon of 1/(1 âˆ’ Î³))
In the infinite-horizon discounted-cost setting the cost cannot be written down
cleanly in terms of the stationary distribution. This is due to the fact that in the
discounted cost setting the cost depends essentially on the head of the sample path
and in the ergodic cost setting the cost depends on the tail of the sample path. As
the tail of the sample paths approaches the stationary distribution (for stabilizing
controllers), the ergodic cost approaches the expected cost.
Now we present the result proving that in the finite-horizon setting the sample-path
is an unbiased estimator of the expected cost and the second moment of the difference
is bounded.
Lemma J.1.
i
i
i
i
E[JË‡y,t
(K i , K âˆ’i )] = JËœy,t
(K i , K âˆ’i ), E[JË‡xÌƒ,t
(KÌ„ i , KÌ„ âˆ’i )] = JËœxÌƒ,t
(KÌ„ i , KÌ„ âˆ’i )
i
i
E[(JË‡y,t
(K i , K âˆ’i ) âˆ’ JËœy,t
(K i , K âˆ’i ))2 ] â‰¤ 2T r(Î¦)2

Proof. For ease of exposition in this proof we will consider an single agent stochastic
50

LQ system under set of controllers K = (Kt )0â‰¤tâ‰¤T âˆ’1
xt+1 = (At âˆ’ Bt Kt )xt + Ï‰t = AK (t)xt + Ï‰t ,

(dynamics)

JË‡t (K) =

(sample-path cost)

T
X

(xs )âŠ¤ (Qs + (Ks )âŠ¤ Rs Ks )xs ,

s=t
T
hX
i
JËœt (K) = E
(xs )âŠ¤ (Qs + (Ks )âŠ¤ Rs Ks )xs

(expected cost)

s=t

where x0 , Ï‰t âˆ¼ N(0, Î£) and are i.i.d. and KT = 0. The results can be generalized to
the systems (3.1)-(3.4) by concatenating the controllers for all N players into a joint
controller. Proof of the unbiasedness follows trivially.
T
X

E[JË‡t (K)] = E
(xÏ„ )âŠ¤ QK (Ï„ )xÏ„ = JËœt (K).
Ï„ =t

We first recall standard results from literate characterizing the expectation and variance
of the quadratic forms of Gaussian random variables.
Lemma J.2 ([50]). Let z âˆ¼ N(0, Ip ) and M âˆˆ RpÃ—p be a symmetric matrix then
E[z M z] = T r(M ) and V ar(z âŠ¤ M z) = 2âˆ¥M âˆ¥2F .
Now we show that JË‡t is an unbiased estimator for JËœt and also bound the second
moment of the difference between the two. From the above equation we can write
âŠ¤

xt =

t
X

s
Atâˆ’1
Ï„ Ï‰Ï„ âˆ’1 where At = AK (s)AK (s âˆ’ 1) . . . AK (t), âˆ€s â‰¥ t â‰¥ 0

Ï„ =1

and Ast = I, âˆ€s < t. Let us introduce
ï£«
1
0
0
Î£2
1
ï£¬ 1 21
Î£2
0
ï£¬ A1 Î£
1
ï£¬ 2 21
2 12
2
ï£¬
A
Î£
A
Î£
Î£
Î¨=ï£¬ 1
2
..
..
..
ï£¬
ï£­
.
.
.
1

2
Atâˆ’1
1 Î£

1

2
Atâˆ’1
2 Î£

1

2
Atâˆ’1
3 Î£

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

ï£¶
0
ï£·
0 ï£·
ï£·
0 ï£·,
.. ï£·
ï£·
. ï£¸

Â·Â·Â·

Î£2

1

ï£¶
âˆ’1
Î£Ï‰ 2 Ï‰0
ï£¬
ï£·
..
ï£·,
Ï–=ï£¬
.
ï£­
ï£¸
ï£«

âˆ’1

Î£Ï‰ 2 Ï‰tâˆ’1

We can deduce that (Î¨Ï–)t = xt where (Î¨Ï–)t is the t-th block of Î¨Ï–. We can use
these quantities to express the cost JË‡t as a quadratic function as follows
JË‡t (K) =

T
X
Ï„ =t

(xÏ„ )âŠ¤ QK (Ï„ )xÏ„ =

T
X
âŠ¤
(Î¨Ï–)âŠ¤
Ï„ QK (Ï„ )(Î¨Ï–)Ï„ = Ï– Î¦Ï–
Ï„ =t

where Î¦ = Î¨âŠ¤ diag((QK (Ï„ ))tÏ„ =1 )Î¨ and QK (Ï„ ) = QÏ„ + (KÏ„ )âŠ¤ RÏ„ KÏ„ . Proof of bounded
second moment follows from,
2
E[(JËœt (K) âˆ’ JËœt (K))2 ] â‰¤ E[JË‡t (K) âˆ’ JËœt (K)] + V ar(JË‡t (K))
= V ar(Ï–âŠ¤ Î¦Ï–) = 2âˆ¥Î¦âˆ¥2F â‰¤ 2T r(Î¦)2 ,
using Lemma J.2 and the fact that âˆ¥Â·âˆ¥F â‰¤ T r(Â·).
51

i
i
The lemma states that the sample path costs JË‡y,t
and JË‡xÌƒ,t
are unbiased estimates of
i
i
i
i
Ëœ
Ëœ
expected costs Jy,t and JxÌƒ,t , respectively and the second moments of JË‡y,t
and JË‡xÌƒ,t
are bounded. Notice that the bound on second moment can be converted into a
uniform bound by ensuring a uniform bound on the cost of controllers (K i , K âˆ’i ). The
sample-path cost can be used to obtain sample-path policy gradients as follows. We
Ëœ K JË‡i (T ) such
denote these sample-path gradients with respect to controller Ki,t as âˆ‡
i,t K
that

Ë‡ i (K i , K âˆ’i ) =
âˆ‡
y,t

Nb
m X
JË‡i (KÌ‚ i (ej , t), K âˆ’i )ej , ej âˆ¼ SpN Ã—mN (r),
Nb r2 j=1 y,t

Ë‡ ixÌƒ,t (KÌ„ i , KÌ„ âˆ’i ) =
âˆ‡

Nb
m X
Ë† i (e , t), KÌ„ âˆ’i )e , e âˆ¼ SpN Ã—mN (r),
JË‡i (KÌ„
j
j j
Nb r2 j=1 xÌƒ,t

Ë† i (e, t) := (KÌ„ i + e, . . . , KÌ„ i )
respectively, where KÌ‚ i (e, t) := (Kti + e, . . . , KTi âˆ’1 ) and KÌ„
t
T âˆ’1
are the controller sets with perturbations e at timesteps t.
Algorithm J.1 SP-MRPG for GS-MFTG
1: Initialize Kti = 0, KÌ„ti = 0 for all i âˆˆ [N ]
2: for t = T, T âˆ’ 1, . . . , 1, 0, do
3:
for k = 1, . . . , K do
4:

Gradient Descent

 i
 i
 i
Ë‡ y,t (K i , K âˆ’i )
âˆ‡
Kt
Kt
i
(J.3)
âˆ’
Î·
â†
k
Ë‡ i (KÌ„ i , KÌ„ âˆ’i )
KÌ„ti
KÌ„ti
âˆ‡
xÌƒ,t

5:
end for
6: end for

Now we introduce the MRPG algorithm which uses sample-path policy gradients
instead of stochastic policy gradients (3.7). This algorithm will be called SP-MRPG in
short. Using Lemma J.1 we can bound the difference between the stochastic gradient
Ëœ K JËœi (K) and the sample-path gradient âˆ‡
Ëœ K JË‡i (Kt ). The following lemma states
âˆ‡
i,t
i,t t
i
Ëœ
Ë‡
Ëœ K JËœi (K) and bounds the
the fact that âˆ‡Ki,t Jt (Kt ) is an unbiased estimator of âˆ‡
i,t
second moment of the difference between the two. The techniques used to prove this
Lemma are similar to [11, 12] hence are omitted.
Lemma J.3.
Ë‡ iy,t (K i , K âˆ’i )] = âˆ‡
Ëœ iy,t (K i , K âˆ’i ),
E[âˆ‡
2

Ë‡ i (K i , K âˆ’i ) âˆ’ âˆ‡
Ëœ i (K i , K âˆ’i )|] â‰¤ 2m T r(Î¦)2
E[|âˆ‡
y,t
y,t
Nb r 4
The proof follows from lemma J.1. Notice that although we do not have a high
confidence bound between the sample path policy gradient and stochastic gradient
as in Lemma 4.2, we instead have a bound on the second moment of their difference.
Now using the the Markov inequality we can convert the second moment bound into a
high confidence bound in the following lemma.
52

Lemma J.4. Using Markov inequality and Lemma J.3,
2
Ë‡ iy,t (K i , K âˆ’i ) âˆ’ âˆ‡
Ëœ iy,t (K i , K âˆ’i )| â‰¤ 2m T r(Î¦)2
|âˆ‡
Nb r4 Î´

with probability at least 1 âˆ’ Î´.
Hence for a given Ïµ > 0, if r = O(Ïµ) and Nb = Î˜Ìƒ(Ïµâˆ’4 Î´ âˆ’1 , then the approximation
Ë‡ iy,t (K i , K âˆ’i ) âˆ’
error between the sample-path gradient and the policy gradient âˆ¥âˆ‡
i
i
âˆ’i
âˆ‡y,t (K , K )âˆ¥ = O(Ïµ). Notice that the mini-batch size for the sample-path gradient
Î˜Ìƒ(Ïµâˆ’4 Î´ âˆ’1 is much higher compared to the mini-batch size needed for the stochastic
gradient Î˜Ìƒ(Ïµâˆ’2 ).
We now show this mini-batch size dependence using empirical results below. In
Figure 5 we compare the difference between the MRPG algorithm (Algorithm 3.1)
which utilizes stochastic gradients vs the SP-MRPG algorithm (Algorithm J.1) which
utilizes the sample-path gradients.

Fig. 5: Comparison between MRPG (Nb = 2, 000), SP-MRPG (Nb = 2, 000) and SP-MRPG
(Nb = 10, 000).

The figure shows convergence of norm of error over number of iterations with
number of teams N = 1, time horizon T = 1, number of agents per team M = 1000
and agents have scalar dynamics. The number of inner-loop iterations K = 1000
and learning rate Î·ki = 0.001. We simulate the MRPG algorithm with mini-batch
size Nb = 2000, SP-MRPG with mini-batch size Nb = 2000 and SP-MRPG with
mini-batch size Nb = 10, 000. It is shown that if mini-batch size is the same MRPG
has better convergence compared to SP-MRPG which is plagued with high variance.
If we increase the mini-batch size in SP-MRPG we can reduce the variance due to
lower variance of the sample-path gradient estimates.

53

