Preference-based Reinforcement Learning beyond
Pairwise Comparisons: Benefits of Multiple Options

arXiv:2510.18713v2 [cs.LG] 11 Nov 2025

Joongkyu Lee
Seoul National University
jklee0717@snu.ac.kr

Seouh-won Yi
Seoul National University
uniqueseouh@snu.ac.kr

Min-hwan Oh
Seoul National University
minoh@snu.ac.kr

Abstract
We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged—motivated by PbRL’s recent empirical success, particularly in aligning large language models (LLMs)—most existing studies focus only on pairwise
comparisons. A few recent works [92, 49, 76] have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve—and
can even deteriorate—as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett–Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that selects
multiple actions by maximizing the average uncertainty within
the offered
¯ subset.
´ bř
T
d
1
We prove that M-AUPO achieves a suboptimality gap of Õ T
t“1 |St | , where
T is the total number of rounds, d is the feature dimension, and |St | is the size of
the subset at round t. This result shows that larger subsets directly lead to improved
performance and, notably, the bound avoids the exponential dependence on the
unknown parameter’s norm, which was a fundamental limitation in´ most ¯previous
d
works. Moreover, we establish a near-matching lower bound of Ω K ?
, where
T
K is the maximum subset size. To the best of our knowledge, this is the first
theoretical result in PbRL with ranking feedback that explicitly shows improved
sample efficiency as a function of the subset size.

1

Introduction

The framework of Preference-based Reinforcement Learning (PbRL) [12, 79, 80, 70] was introduced
to address the difficulty of designing effective reward functions, which often demands substantial
and complex engineering effort [78, 80]. PbRL has been successfully applied in diverse domains,
including robot training, stock prediction, recommender systems, and clinical trials [30, 65, 18, 37, 52].
Notably, PbRL also serves as a foundational framework for Reinforcement Learning from Human
Feedback (RLHF) when feedback is provided in the form of preferences rather than explicit scalar
rewards. This preference-based approach has proven highly effective in aligning Large Language
Models (LLMs) with human values and preferences [18, 57, 62].
Given its practical success, the field has also seen significant theoretical advances [16, 47, 70, 92, 85,
90, 89, 82, 72, 51, 13, 64, 22, 19, 49, 74, 71, 76, 84, 14, 38]. However, despite this progress, most
existing models remain limited to handling only pairwise comparison feedback. A few works [92, 49,
76] explore the more general setting of multiple comparisons, offering a strict extension beyond the
pairwise case. Zhu et al. [92] study the offline setting, where a dataset of questions (or contexts) along
with corresponding ranking feedback over K answers (or actions), labeled by human annotators,
is available. Mukherjee et al. [49] investigate the online learning-to-rank problem [61], where a
dataset of questions with K candidate answers is provided, but no feedback is initially available.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).

Table 1: Comparisons of settings and theoretical guarantees in related works on PbRL with ranking
feedback. Here, T denotes the number of rounds (or the number of data points in the offline setting),
K is the (maximum) size of the offered action set (i.e., assortment), and d is the feature dimension.
ρ represents the unknown context distribution. Here, Õ hides logarithmic factors and polynomial
dependencies on B. “Pred. Error” refers to the prediction error.
Setting

Context

Assortment

Measure

Zhu et al. [92]

Offline

Accessible X

Given

Suboptimality

Mukherjee et al. [49]

Online

Accessible X

Given

Pred. Error

Thekumparampil et al. [76]

Online

No context

Select K

Pred. Error

This work (Theorem 1, 2)

Online

Sampled x „ ρ

Select ď K

Suboptimality

This work (Theorem 3)

Lower Bound

Sampled x „ ρ

Select ď K

Suboptimality

Result
b ¯
´
Õ eB K 2 Td
´
¯
Õ eB K 3 ?dT
´
¯
Õ eB K 3 ?dT
b
´
¯
řT
1
Õ Td
t“1 |St |
´
¯
d
Ω K?
T

Thekumparampil et al. [76] consider a context-free setting (i.e., a singleton context), and the goal is
to learn the ranking of N ě K answers based on ranking feedback obtained from subsets of size K.
However, all of their theoretical performance guarantees fail to show that using multiple comparisons
provides any advantage over the pairwise setting (see Table 1). This is counterintuitive, as ranking
feedback is inherently
` ˘ more informative than pairwise feedback. Specifically, since a ranking over K
actions provides K
2 pairwise comparisons, it should, in principle, enable faster learning and lead to
stronger performance guarantees. Thus, the following fundamental question remains open:
Can we design an algorithm that achieves a strictly better theoretical guarantee under
multiple-option feedback compared to the pairwise comparisons in the online PbRL setting?
In this paper, we assume that the ranking feedback follows the Plackett-Luce (PL) model [60, 45],
where, in each round, the learner receives ranking feedback over a subset of up to K actions (with
K ď N ) selected from a universe of N actions. This problem setup is closely related to that of
Thekumparampil et al. [76]; however, unlike their work, which focuses solely on a context-free
setting (or equivalently, a fixed singleton context), we study a more general setting where contexts
are diverse and drawn from an unknown distribution.
Under this problem setup, we provide an affirmative answer to the above question by introducing
a novel algorithm, Maximizing Average Uncertainty for Preference Optimization (M-AUPO), which
explicitly exploits the richer information available from ranking feedback under the Plackett–Luce
(PL) model. M-AUPO selects action subsets by maximizing average uncertainty and achieves a
suboptimality gap that strictly improves upon what is attainable with pairwise comparisons. In
particular, we show that its suboptimality gap decreases with longer ranking feedback.
Furthermore, our suboptimality gap eliminates the exponential dependence on the parameter norm
bound, OpeB q, in the leading term. This improvement stems from analytically dividing the total
rounds into warm-up and non–warm-up phases (see the proof sketch in Section 5.1). This represents a significant improvement over most prior works, where performance guarantees depend on
OpeB q [66, 70, 92, 85, 90, 19, 84, 76, 38]. Very recently, a few works [14, 20] have successfully
avoided the OpeB q dependency by relying on auxiliary techniques or additional information—such as
specialized sampling schemes [14] or prior knowledge of κ [20]—which, however, are often impractical. Moreover, their methods are limited to pairwise comparison settings. In contrast, our approach
eliminates the OpeB q dependency without using any auxiliary techniques and considers more general
ranking feedback beyond pairwise comparisons. Our main contributions are summarized as follows:
• Improved sample efficiency via larger subsets: We propose M-AUPO, a novel algorithm
for online
(or RLHF)
with PL ranking feedback, which achieves a suboptimality gap
´ bPbRL
¯
řT
1
of Õ Td
,
where
|St | is the size of the action subset offered at round t. This
t“1 |St |
result provides the first rigorous theoretical guarantee that larger subsets directly improve
sample efficiency. To the best of our knowledge, this is the first theoretical work in PbRL
that explicitly demonstrates performance improvements as a function of the subset size |St |.
2

• Improvement on OpeB q dependency: Our result eliminates the exponential dependence
on the parameter norm bound, OpeB q, in the leading term—without relying on any auxiliary
techniques—through a fully refined analysis. This shows that the OpeB q dependence widely
observed in PbRL (or RLHF) and dueling bandit analyses is not inherent but rather an artifact
of loose analysis. Moreover, our key technique for removing the OpeB q dependence—by
dividing the rounds into warm-up and non–warm-up phases only in the analysis—can be
seamlessly incorporated into existing PbRL or dueling bandit analyses, including regretminimization frameworks without altering the underlying algorithms (see Appendix G.1).
To the best of our knowledge, this is the first PbRL work with ranking feedback beyond
pairwise comparisons that avoids the OpeB q dependence.
• Efficiency of rank-breaking (RB): We present both naive PL loss–based and rank-breaking
(RB) loss–based learning approaches, and show that the RB formulation achieves superior
computational efficiency and empirical performance compared to the PL counterpart.
´
¯
d
under the PL
• Lower bound: We establish a near-matching lower bound of Ω K ?
T
model with ranking feedback, matching our upper bound up to a K factor. This shows that
leveraging richer ranking information (larger K) provably improves sample efficiency.
• Experiment: We empirically evaluate M-AUPO on both synthetic and real-world datasets,
showing its improved performance for larger K and its superiority over existing baselines.

2

Related Works

Fueled by the remarkable success of LLMs [18, 57, 62], the theoretical study of PbRL has rapidly
emerged as a central focus within the research community. Early work in this area traces back to the
dueling bandits literature [87, 94, 68, 8].
Dueling bandits. The dueling bandit framework, introduced by Yue et al. [87], departs from the
classical multi-armed bandit setting by requiring the learner to select two arms and observe only
their pairwise preference. For general preferences, a single best arm that is globally dominant may
not exist. To address this, various alternative winners have been proposed, including the Condorcet
winner [93, 35], Copeland winner [94, 81, 36], Borda winner [31, 25, 28, 69, 83], and von Neumann
winner [63, 24, 7], each with its own corresponding performance metric.
To address scalability and contextual information, Saha [66] proposed a structured contextual dueling
bandit setting in which preferences are modeled using a Bradley–Terry–Luce (BTL) model [11]
based on the unknown intrinsic rewards of each arm. In a similar setting, Bengs et al. [9] studied a
contextual linear stochastic transitivity model, and Di et al. [21] proposed a layered algorithm that
achieves variance-aware regret bounds. However, most prior dueling bandit works suffer from an
OpeB q dependence. Only a few recent studies [20, 14] have succeeded in removing this OpeB q term,
either by introducing additional complex subroutines [14] or by relying on prior knowledge of κ [20].
Preference-based reinforcement learning (PbRL). Building upon this line of work, subsequent
research has extended the dueling bandit framework to the RL, considering both online [86, 52, 16,
70, 82] and offline settings [92, 90, 44]. More recently, under the active learning framework—where
the full set of contexts X is accessible—many studies aim to improve sample efficiency by selecting
prompts either based on the differences in estimated rewards for their responses [50] or through
D-optimal design methods [47, 71, 19, 49, 76, 38]. However, most of these works focus exclusively
on pairwise preference feedback and cannot be extended to more general ranking feedback cases.
Mukherjee et al. [49] study the online learning-to-rank problem when prompts are given along with
K candidate answers, while Thekumparampil et al. [76] investigate learning to rank N ě K answers
from partial rankings over K answers, but under a context-free setting. In this paper, we consider
a stochastic contextual setting (more general than Thekumparampil et al. [76]), where contexts are
sampled from an unknown but fixed distribution, and aim to minimize the suboptimality gap using
ranking feedback of up to length K. For further related work, see Appendix A.

3

Problem Setting and Preliminaries

Notations. Given a set X , we use |X | to denote its cardinality. For a positive integer n, we denote
rns :“ t1, 2, . . . , nu. For a real-valued matrix A, we let }A}2 :“ supx:}x}2 “1 }Ax}2 which is the
3

maximum singular value of A. We write A ľ A1 if A ´ A1 is positive semidefinite. For a univariate
function f , we denote f9 as its derivative.
We have a set of contexts (or prompts), denoted by X , and a set of possible actions (or answers),
denoted by A :“ ta1 , . . . , aN u.1 . We consider preference feedback in the form of partial rankings
over subsets of A, and model this feedback using the Plackett-Luce (PL) distribution:
Definition 1 (PL model). Let S :“ tS Ď A | 2 ď |S| ď Ku be the collection of all action subsets
whose sizes range from 2 to K. For any S P S, let σ denote the labeler’s ranking feedback—that is,
a permutation of the elements in S. We write σj for the j-th most preferred action under σ. We model
the distribution of such rankings using the Plackett-Luce (PL) model [60, 45], defined as:
‹

Ppσ|x, S; θ q “

|S|
ź

exp prθ‹ px, σj qq
,
ř|S|
‹
j“1
k“j exp prθ px, σk qq

where px, Sq P X ˆ S.

(1)

Here, rθ‹ represents a reward model parameterized by the unknown parameter θ ‹ .
When K “ 2, this reduces to the pairwise comparison framework considered in the Bradley-TerryLuce (BTL) model [11]. The probability that a is preferred to a1 given x can be expressed as:
`
˘
exp prθ‹ px, aqq
Ppa ą a1 |x; θ ‹ q “
“ µ rθ‹ px, aq ´ rθ‹ px, a1 q ,
(2)
exp prθ‹ px, aqq ` exp prθ‹ px, a1 qq
where µpwq “ 1`e1´w is the sigmoid function. In this work, we assume a linear reward model:
Assumption 1. Let ϕ : X ˆ A Ñ Rd be a known feature map satisfying maxx,a }ϕpx, aq}2 ď 1,
and let θ ‹ P Rd denote the true but unknown parameter. The reward is assumed to follow a linear
structure given by rθ‹ px, aq “ ϕpx, aqJ θ ‹ . We further assume realizability, i.e., θ ‹ P Θ :“ tθ P
Rd | }θ}2 ď Bu. Without loss of generality, we assume B ě 1.
At each round t P rT s, a context xt P X is drawn from a fixed but unknown distribution ρ. Given
the context xt , the learning agent selects a subset of actions St P S—referred to as an assortment
throughout the paper—and receives a ranking over St as feedback, generated according to the PL
model. Let π ‹ pxq “ argmaxa rθ‹ px, aq be the optimal policy under the true reward rθ‹ . After T
pT : X Ñ A that minimizes the
rounds of interaction with the labeler, the goal is to output a policy π
suboptimality gap, defined as:
pT pxqqs .
SubOptpT q :“ Ex„ρ rrθ‹ px, π ‹ pxqq ´ rθ‹ px, π
3.1

Loss Functions and Rank-Breaking

In this paper, we consider two different losses for estimating the parameter: one directly induced by
the PL model, and the other obtained by splitting the ranking feedback into pairwise comparisons.
Plackett-Luce (PL) loss. The PL loss function for round t is defined as follows:
˜
¸
`
˘
|S
ÿt | pjq
exp ϕpxt , σtj qJ θ
pjq
.
ℓt pθq :“
ℓt pθq, where ℓt pθq :“ ´ log ř|S |
t
J
j“1
k“j exp pϕpxt , σtk q θqq

(3)

pjq

Here, ℓt pθq denotes the negative log-likelihood loss under the Multinomial Logit (MNL) model [46],
conditioned on the assortment being the remaining actions in St after removing the previously selected
actions σt1 , . . . , σtpj´1q —that is, over the set St ztσt1 , . . . , σtpj´1q u.
Rank-Breaking (RB) loss. In addition to this standard approach, one can replace the full |St |-action
` ˘
ranking with its |S2t | pairwise comparisons. This technique, referred to as rank-breaking (RB),
decomposes (partial) ranking data into individual pairwise comparisons, treating each comparison as
independent [6, 34, 32, 67]. Thus, the RB loss is defined as:
˜
¸
`
˘
|Sÿ
t |´1 |S
ÿt | pj,kq
exp ϕpxt , σtj qJ θ
pj,kq
ℓt pθq :“
ℓt pθq, where ℓt pθq :“ ´ log ř
. (4)
J
mPtj,ku exp pϕpxt , σtm q θq
j“1 k“j`1
This approach is applied in the current RLHF for LLM (e.g., Ouyang et al. [57]) and is also studied
in the theoretical RLHF paper [92] under the offline setting.
1

For simplicity, we assume a stationary action space A, though it may depend on the context x P X .

4

Procedure 1 OMD-PL, OMD for PL Loss

Procedure 2 OMD-RB, OMD for RB Loss

p1q
Input: θpt , St , Ht
for j “ 1 to |St | do
pjq
pj`1q
Update H̃t , θpt
via (5)

p1,2q

Input: θpt , St , Ht
for each pj, kq such that j ă k ď |St | do
pj,kq
pj,k`1q
Update H̃t , θpt
via (7)
end for
p|S |´1,|St |`1q
return θpt t

end for
p|S |`1q
return θpt t

3.2

Online Parameter Estimation

Motivated by recent advances in Multinomial Logit (MNL) bandits [91, 39, 41], we adopt an online
mirror descent (OMD) algorithm to estimate the underlying parameter θ ‹ , instead of relying on
maximum likelihood estimation (MLE). This enables a constant per-round computational cost, in
contrast to the MLE-based approach, whose cost grows linearly with the number of rounds t.
OMD update for PL loss. For the the PL loss (3), we estimate the true parameter θ ‹ as follows:
1
pj`1q
pjq pjq
pjq
θpt
“ argmin x∇ℓt pθpt q, θy ` }θ ´ θpt }2H̃ pjq ,
2η
t
θPΘ

j “ 1, . . . , |St |,

(5)

p|S |`1q
p1q
where we write θpt t
“ θpt`1 , and η is the step-size parameter to be specified later. The matrix
řj
pjq
pjq
pj 1 q pj 1 q
H̃ is given by H̃ :“ Ht ` η 1 ∇2 ℓ pθp q, where
t

t

t

j “1

Ht :“

t´1
s|
ÿ
ÿ |S

t

ppj`1q q ` λId ,
∇2 ℓpjq
s p θs

λ ą 0.

(6)

s“1 j“1

The optimization problem (5) can be solved using a single projected gradient step [55], which enjoys
a computational cost of only OpKd3 q—independent of t [48], unlike MLE—and requires only Opd2 q
pjq
storage, thanks to the incremental updates of H̃t and Ht .
OMD update for RB loss. Similarly, for the RB loss (4), we estimate the underlying parameter as:
1
pj,k`1q
pj,kq
pj,kq pj,kq
θpt
“ argmin x∇ℓt pθpt q, θy ` }θ ´ θpt }2H̃ pj,kq ,
2η
t
θPΘ

1 ď j ă k ď |St |,

(7)

pj,|S |`1q
pj`1,j`2q
p|S |´1,|St |`1q
where we set θpt t
“ θpt
for all j ă |St | ´ 1 and for the final pair, let θpt t
“
1 1
1 1
ř
p1,2q
pj,kq
pj,kq
2 pj ,k q ppj ,k q 2
p
θt`1 . The matrix H̃t
is defined as H̃t
:“ Ht ` η pj 1 ,k1 qďpj,kq ∇ ℓt
pθt
q , where

Ht :“

t´1
s |´1 |S
s|
ÿ |Sÿ
ÿ

∇2 ℓpj,kq
pθpspj,k`1q q ` λId ,
s

λ ą 0.

(8)

s“1 j“1 k“j`1

Remark 1 (Computational cost of OMD). The per-round computational cost of the PL parameter
update is OpK 2 d3 q, since the parameter is updated |St | ď K times per round. Similarly, the cost for
` ˘
the RB parameter update is OpK 3 d3 q, as the parameter is updated |S2t | times per round.

4

M-AUPO: Maximizing Average Uncertainty

In this section, we propose a new algorithm, M-AUPO, which selects an assortment that maximizes
the average uncertainty of St , thereby exploiting the potential benefits of a larger K. For clarity of
presentation, we first define the MNL probability [46] for a given assortment S at round t as follows:
`
˘
exp ϕpxt , aqJ θ
ř
Pt pa|S; θq :“
, @a P S.
(9)
1 J
a1 PS exp pϕpxt , a q θq
2

We write pj 1 , k1 q ď pj, kq to indicate lexicographic order, i.e., j 1 ă j or j 1 “ j and k1 ď k.

5

Algorithm 3 M-AUPO: Maximizing Average Uncertainty for Preference Optimization
1: Inputs: maximum assortment size K, regularization parameter λ, step size η
2: Initialize: H1 “ λId , θp1 P Θ
3: for round t “ 1 to T do
4:
Observe xt „ ρ and select St via (11)
5:
Observe ranking feedback σt for St
6:
θpt`1 Ð OMD-PLpθpt , St , Ht q (Proc. 1)
Ź or OMD-RBpθpt , St , Ht q (Proc. 2) if RB loss

ř|S |

pjq

pj`1q

t
7:
Update Ht`1 Ð Ht ` j“1
∇2 ℓt pθpt
q via (6)
8: end for
9: Return: π
pT pxq Ð argmaxaPA ϕpx, aqJ θpT `1

Ź or via (8) if RB loss

pjq

Given an assortment S and ranking feedback σ, let Sσ :“ tσj , . . . , σ|S| u denote the remaining
actions in S after removing the first j ´ 1 actions. Let Pt pσ|S; θq “ Ppxt , σ|S; θq, for simplicity.
Then, the PL model in Equation (1) can be expressed as follows:
Pt pσ|S; θq “ Pt pσ1 |S; θq ¨ Pt pσ2 |Sztσ1 u; θq ¨ . . . ¨ Pt pσ|S| |tσ|S| u; θq “

|S|
ź

Pt pσj |Sσpjq ; θq,

j“1

Greedy assortment selection for PL loss. Given Ht and θpt , we define the function ft pSq as follows:
„›
|S|
›2 ȷ
1 ÿ
›
1 ›
(10)
ft pSq :“
E
›ϕpxt , aq ´ Ea1 „Pt p¨|Sσpjq ;θpt q rϕpxt , a qs› ´1 .
θpt q
|S| j“1 σ„Pt p¨|S;
Ht
pjq p
a„Pt p¨|Sσ ;θt q

Intuitively, finding S that exactly maximizes ft pSq amounts to maximizing the average (meancentered) uncertainty—that is, the Mahalanobis dispersion of the feature vectors evaluated under
Ht´1 . However, computing the exact maximizer of Equation (10) is generally NP-hard. In our setting,
it is sufficient to add actions sequentially in a greedy manner. This is because our analysis centers on
the suboptimality gap between two actions—one optimal and one chosen by the policy. As a result,
the suboptimality gap is (approximately) upper bounded by a term involving ft pSq for pairwise action
sets (i.e., |S| “ 2). Therefore, greedily adding actions that increase ft pSq is sufficient.
We initialize S with a pair of actions (|S| “ 2) that maximizes the average information gain, as
defined in Equation (10). Then, iteratively add one action at a time by
a‹ P argmax ∆t pa | Sq,
where ∆t pa | Sq :“ ft pS Y tauq ´ ft pSq,
(11)
aPAzS

and accept a‹ if ∆t pa | Sq ě 0, until |S| “ K or no non-negative gain remains. This selection rule
is central to our algorithm, as it facilitates a rapid decrease in the reward estimation error by favoring
assortments that provide more informative feedback, especially when the assortment size |St | is large.
Notably, the greedy variant of the selection rule can be implemented efficiently with a computational
cost of OpN 2 d3 ` LN K 2 d2 q3 , where L denotes` the
˘ (approximate) expectation cost over the ranking
N
σ. This implementation avoids enumerating all K possible subsets.
Greedy assortment selection for RB loss. Analogous to Equation (10), we define the following
function to select an assortment that maximizes the average uncertainty:
„›
›2 ȷ
1 ÿ
›
›
ft pSq :“
Eā„Pt p¨|ta,a1 u;θpt q ›ϕpxt , āq ´ Eã„Pt p¨|ta,a1 u;θpt q rϕpxt , ãqs› ´1
|S| a,a1 PS
Ht
´
¯
›
›2
1 ÿ
“
µ9 pϕpxt , aq ´ ϕpxt , a1 qqJ θpt ›ϕpxt , aq ´ ϕpxt , a1 q›H ´1 .
(12)
t
2|S| a,a1 PS
We then perform greedy assortment selection according to Equation (11). Note that, unlike in the PL
loss case, assortment selection under the RB loss does not require taking expectations over rankings,
as all pairs within S are directly compared. Therefore, the RB-based assortment selection is both
exact and computationally efficient, without
any additional
`
˘ expectation-approximation cost L, and
incurs a total computational cost of O N 2 d3 ` N K 3 d2 4 .
3
4

OpN 2 d3 q to choose the first pair, followed by at most K ´2 additions, each of which requires OpLN Kd2 q.
OpN 2 d3 q to choose the first pair, followed by up to K ´ 2 additional steps, each costing OpN K 2 d3 q.

6

Once the assortment St is selected, the algorithm receives the ranking feedback σt from the labeler
(Line 5) and updates the parameter using Procedure 1 for the PL loss or Procedure 2 for the RB
pT , which selects actions by
loss (Line 6). After T rounds, the algorithm returns the final policy π
p
maximizing the estimated reward under the final parameter estimate θT `1 (Line 7).

5

Main Results

5.1

Suboptimality Gap of M-AUPO

We begin by presenting the online confidence bound for the PL loss, derived by extending the results
of Lee and Oh [41], who analyzed the MNL model [46]. Since the PL model constructs ranking
probabilities as a product of MNL probabilities, their confidence bound can
řt be directly applied to our
setting by replacing the round t with the cumulative number of updates s“1 |Ss |.
?
Corollary 1 (Online
confidence bound for PL loss). Let δ P p0, 1s. We set η “ p1 ` 3 2Bq{2 and
?
λ “ maxt12 2Bη, 144ηd, 2u. Then, under Assumption 1, with probability at least 1 ´ δ, we have
´ a
? ¯
pjq
}θpt ´ θ ‹ }H pjq ď βt pδq “ O B d logptK{δq ` B λ ,
t

pjq

where Ht

:“ Ht `

@t ě 1, j ď |St |,

1
1
2 pj q ppj `1q
q ` λId .
j 1 “1 ∇ ℓs pθs

řj´1

This confidence bound is free of any polynomial dependency on K, which is primarily made possible
by the improved self-concordant-like properties proposed by Lee and Oh [41]. Moreover, for the RB
loss, we can derive a confidence bound of the same order (see Corollary D.1). Based on this confidence
bound, we derive the suboptimality gap for M-AUPO, with the proof deferred to Appendix C.
`
˘
Theorem 1 (Suboptimality gap for PL loss). Let δ P p0, 1s. Set λ “ Ω d logpKT {δq ` ηpB ` dq
?
and η “ 12 p1 ` 3 2Bq. Define κ :“ e´6B . If Assumption 1 holds, then for any T ě eK{d , with
probability at least 1 ´ δ, M-AUPO (Algorithm 3) achieves the following suboptimality gap:
¨ g
˛
fT
2 4
fÿ 1
d
d
K
‚.
SubOptpT q “ Õ ˝ e
`
T t“1 |St |
κT
Discussion of Theorem 1. For sufficiently large T , the second (non-leading) term becomes negligible,
and Theorem 1 shows that the suboptimality gap of M-AUPO decreases as the assortment size |St |
increases. This establishes a strict advantage of receiving ranking feedback over larger assortments.
Moreover, our result does not involve any OpeB q dependency in the leading term, a harmful dependency that commonly appears in prior works [66, 70, 92, 85, 90, 19, 76, 38]. Although very recent
studies [20, 14] also achieve OpeB q-free performance in the leading term, they rely on auxiliary
techniques and are restricted to pairwise preference feedback. To the best of our knowledge, this
is the first theoretical study that simultaneously establishes (i) the performance benefits of utilizing
richer ranking feedback over larger assortments, and (ii) the elimination of the OpeB q dependence in
the leading term of the PbRL framework when accommodating multiple (i.e., more than two) options.
Proof sketch of Theorem 1. We provide a proof sketch for Theorem 1. For simplicity, we define
ψt,a,a1 :“ ϕpxt , aq ´ ϕpxt , a1 q. We begin by defining the set of warm-up rounds, denoted by T w ,
which consists of rounds with large (non-centered) uncertainty:
"
´ ?
¯*
1
´1
T w :“ t P rT s : max
}ψ
}
ě
1{
3
2Kβ
pδq
.
t,a,a H
T `1
1
a,a PA

t

1) Regret decomposition and assortment selection. The proof begins by decomposing the suboptimality gap into two components: the realized regrets and a martingale difference sequence (MDS).
Since the MDS term can be readily bounded using the Azuma–Hoeffding inequality, the analysis
focuses on bounding the realized regrets.
7

SubOptpT q “

T
T
˘J ‹ 1 ÿ
1 ÿ`
‹ px q,p
ψ
θ
`
MDSt
t,π
π
px
q
t
t
T
T t“1 loooooooooooomoooooooooooon
T t“1
loooooomoooooon
realized regret of π
p at round t
T

?
“Õp1{ T q

ˆ
˙
1 ÿ
1
d2 K 4
‹
p
}ψ ‹
} ´1 }θ ´ θT `1 }HT `Õ ? `
À
.
T tRT w t,π pxt q,pπT pxt q Ht loooooooomoooooooon
κT
T
?
ďβt pδq“Õp dq

˘J
In the inequality, we first use the fact that ´ ψt,π‹ pxt q,pπT pxt q θpT `1 ě 0, which follows from
pT . We then apply Hölder’s inequality together with the inequality HT `1 ľ Ht .
definition of π
Moreover, we invoke Lemma C.4, which states that the regret incurred during the warm-up rounds
?
T w is a non-leading term. Finally, by applying Corollary 1, we bound }θ ‹ ´ θpT `1 }HT by Õp dq.
`

2) Avoiding OpeB q for non-warm-up rounds. For t R T w , we have
`
˘J
βt pδq
ψt,π‹ pxt q,pπT pxt q θ ‹ ď }ψt,π‹ pxt q,pπT pxt q }H ´1 }θ ‹ ´ θpT `1 }HT ď ?
ď 1,
t
3 2KβT `1 pδq
``
˘J ˘
which implies that 1{µ9 ψt,π‹ pxt q,pπT pxt q θ ‹ ď p1 ` eq2 . Thus, we obtain
˘J ˘
p1 ` eq2 ÿ ``
1 ÿ
µ9 ψt,π‹ pxt q,pπT pxt q θ ‹ }ψt,π‹ pxt q,pπT pxt q }H ´1
}ψt,π‹ pxt q,pπT pxt q }H ´1 ď
t
t
T tRT w
T
tRT w
ď

˘J
˘
ep1 ` eq2 ÿ ``
µ9 ψt,π‹ pxt q,pπT pxt q θpt`1 }ψt,π‹ pxt q,pπT pxt q }H ´1 ,
t
T
tRT w

where the last inequality follows from Lemma E.2, together with the fact that for any a, a1 P A and
J
‹
‹
p
p
t R T w , |ψt,a,a
1 pθ ´ θt`1 q| ď }ψt,a,a1 }H ´1 }θ ´ θt`1 }HT ď βt pδq{βT `1 pδq “ 1. Then, by the
t
Cauchy–Schwarz inequality, and ignoring constants, we have
d
ÿ 1 d ÿ
˘J
˘
``
1
|St | ¨ µ9 ψt,π‹ pxt q,pπT pxt q θpt`1 }ψt,π‹ pxt q,pπT pxt q }2H ´1
t
T tRT w |St | tRT w
g
g
fT
fT
fÿ 1 d ÿ
ÿ 1 d ÿ
1
1f
e
‹
À
pT pxt quq ď e
|St | ¨ ft ptπ pxt q, π
|St | ¨ ft pSt q
T t“1 |St | tRT w
T t“1 |St | tRT w
(St selection)
g
g
fT
f
„›
›2 ȷ
f ÿ 1 f ÿ |S
ÿt |
›
›
e
Àe
E Pt p¨|St ;θ‹ q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a1 qs› ´1 , (Lemma C.2)
t
|S
|
Mt
t
pjq
t“1
tRT w j“1 Pt p¨|St ;θpt q
”
ı
ř
ř|Ss |
pjq
where Mt “ sPrt´1szT w j“1
Eσ„Ps p¨|Ss ,θ‹ q ∇2 ℓs,σ pθps q ` λId is the expected version of Ht .
Finally, applying the elliptical potential lemma (Lemma C.3), we concludes the proof.
Remark 2 (Generality of our technique for avoiding OpeB q). Our approach of dividing the total
rounds into warm-up and non–warm-up phases to improve the OpeB q dependency is broadly applicable. In particular, this technique can be readily incorporated into most existing PbRL and dueling
bandit algorithms without modifying their original algorithms. See Appendix G.1 for more details.
Furthermore, under the rank-breaking (RB) parameter update (7) and assortment selection rule (12),
we establish a comparable suboptimality gap for the RB loss. The proof is provided in Appendix D.
Theorem 2 (Online confidence bound for RB loss). Under the same setting as Theorem 1, if the
parameter is updated and the assortment is selected according to the RB loss setting, then for any
2
T ě eK {d , with probability at least 1 ´ δ, M-AUPO (Algorithm 3) satisfies
¨ g
˛
fT
2
fÿ 1
d
d
‚.
SubOptpT q “ Õ ˝ e
`
T t“1 |St | κT
8

Discussion of Theorem 2. For sufficiently large T , the suboptimality gap in Theorem 2 matches
the leading-order term of Theorem 1, while its second (non-leading) term is tighter by a factor of
OpK 4 q. In addition, the assortment selection cost for RB (12) is lower, since it avoids the expensive
expectation over rankings required in the PL loss case (Equation (10)). This leads to a useful
insight: under ranking feedback, breaking the ranking and updating pairs independently can be more
computationally efficient—and even more stable—than learning directly from the PL loss. Moreover,
this provides a theoretical explanation for the empirical success of RLHF in LLMs (e.g., Ouyang et al.
[57]), where ranking feedback is decomposed into pairwise comparisons for parameter estimation.
5.2

Lower Bound

Theorem 3 (Lower bound). Suppose T ě d2 {p8K 2 q. Define the feature spaceaas Φ :“ S d´1 , the
unit sphere in Rd , and let the parameter space be Θ “ t´µ, µud , where µ “ d{p8K 2 T q. Then,
pT P △Φ returned after collecting T samples (using any sampling policy), the
for any final policy π
expected suboptimality gap is lower bounded as:
ˆ
˙
d
?
SubOptpT q “ Ω
.
K T
Discussion of Theorem 3. The proof is deferred to Appendix F. Theorem 3 provides theoretical
support for our upper bounds, especially in their dependence on K; in particular, it matches the upper
bound in Theorem 2 when |St | “ K. To the best of our knowledge, this is the first lower bound on
the suboptimality gap that incorporates PL ranking feedback in PbRL and formally shows that the
suboptimality gap can diminish as K grows, highlighting the advantage of utilizing ranking feedback
over simple pairwise comparisons.

6

Numerical Experiments

We conduct two sets of experiments to empirically validate our theoretical findings: (i) one using
synthetic data (Subsection 6.1), and (ii) another using two real-world datasets (Subsection 6.2). We
compare our proposed algorithm, M-AUPO, against three baselines: (i) DopeWolfe [76], which selects
K actions in a non-contextual setting; (ii) Uniform, which uniformly samples assortments of size
K at random; and (iii) Best&Ref constructs an action pair (|St | “ 2) by combining the action that
maximizes the current reward estimate with another sampled from a reference policy (e.g., uniform
random or SFT), following the setup in Online GSHF [85] and XPO [84]. In our experiments, the
reference policy for Best&Ref is set to the uniform random policy.
6.1

Synthetic Data

In the synthetic data experiment, for each instance, we sample the underlying parameter θ ‹ „
N p0, Id q and normalize it to ensure that }θ ‹ }2 ď 1. At every round t, a context x P X is drawn
uniformly at random, and its feature vector ϕpx, ¨q lies within the unit ball. We set d “ 5, |A| “
N “ 100, and |X | “ 100. We measure the suboptimality gap every 25 rounds and report the mean
over 20 independent runs, together with one standard error.
The first two plots in Figure 1 show the suboptimality gap of M-AUPO under both the PL loss (3) and
RB loss (4) as the maximum assortment size K varies. The results clearly show that performance
improves as K increases, supporting our theoretical findings. In the third plot of Figure 1, we
compare the performance of M-AUPO with three baseline methods at the final round for K “ 5,
showing that our algorithm significantly outperforms all baselines. While DopeWolfe also considers
the selection of K actions from N actions, it treats each context x independently and is specifically
designed for the context-free setting (i.e., a singleton context). As a result, DopeWolfe cannot
leverage information sharing across varying contexts and performs poorly in our setting. Furthermore,
M-AUPO outperforms naive assortment selection strategies such as Uniform and Best&Ref, as it
explicitly chooses assortments that maximize the average uncertainty, thereby achieving more efficient
exploration. Finally, the results also show that the RB version of M-AUPO empirically outperforms the
PL-based approach, since RB does not incur approximation errors when computing expectations. See
Appendix H.1 for additional experimental results.
9

0.15
0.10
0.05
0.00

25

50

75

RB Loss, varying K

0.20

SubOpt. Gap

SubOpt. Gap

M-AUPO (K=2)
M-AUPO (K=3)
M-AUPO (K=5)
M-AUPO (K=7)
M-AUPO (K=10)

0.15
0.10
0.05
0.00

100 125 150 175 200

vs Other Methods, K=5

M-AUPO (K=2)
M-AUPO (K=3)
M-AUPO (K=5)
M-AUPO (K=7)
M-AUPO (K=10)

Round (t)

25

50

75

100 125 150 175 200

SubOpt. Gap (t=200)

PL Loss, varying K

0.20

Round (t)

0.2

DopeWolfe
Best&Ref

0.17

0.15

0.14

0.1

Uniform
M-AUPO (ours)

0.16
0.13
0.08

0.07

0.05

0.04

0.0

PL Loss

0.02

RB Loss

Figure 1: Synthetic data experiment: suboptimality gap of M-AUPO under varying K, evaluated with
PL loss (left) and RB loss (middle), along with comparison against other baselines (right).

SubOpt. Gap

0.3
0.2
0.1
5000

10000

15000

Round (t)

20000

NECTAR, varying K

PL Loss vs Baselines, K=3

M-AUPO (K=2)
M-AUPO (K=3)
M-AUPO (K=5)
M-AUPO (K=7)

0.4

SubOpt. Gap

M-AUPO (K=2)
M-AUPO (K=3)
M-AUPO (K=5)
M-AUPO (K=7)

0.3
0.2
0.1
5000

10000

15000

Round (t)

20000

SubOpt. Gap (t=20K)

TREC-DL, varying K
0.4

0.6
0.4
0.2
0.0

DopeWolfe
Best&Ref

0.58

0.33

0.29

0.22

TREC-DL

Uniform
M-AUPO (ours)

0.23 0.21 0.21

0.17

NECTAR

Figure 2: Real-world dataset experiment: suboptimality gap of M-AUPO under varying K on the
TREC-DL dataset (left) and the NECTAR dataset (middle), along with comparison against other
baselines (right). The results are rescaled to align the performances between the two datasets.
6.2

Real-World Dataset

We also conduct experiments using real-world datasets from TREC Deep Learning (TREC-DL)5
and NECTAR6 . The TREC-DL dataset provides 100 candidate answers for each question, while the
NECTAR dataset offers 7 candidate answers per question. We sample |X | “ 5000 prompts from
each dataset, with the corresponding set of actions (100 or 7 actions, respectively).
We use the gemma-2b7 [75] LLM to construct the feature ϕpx, aq. Specifically, ϕpx, aq is obtained by
extracting the embedding of the concatenated prompt and response from the last hidden layer of the
LLM, with size d “ 2048. Additionally, we use the Mistral-7B [33] reward model8 as the true reward
model rθ‹ to generate ranking feedback and compute the suboptimality gap accordingly. We measure
the suboptimality gap every 2,500 rounds and report the average over 10 independent runs, along
with the standard error. In these experiments, we report results only for the RB version of M-AUPO, as
it is computationally more efficient and empirically performs better, as shown in Figure 1.
The first two plots in Figure 2 show that the performance improves as K increases on two real-world
datasets In the third plot of Figure 2, we compare the performance of M-AUPO with other baselines
with K “ 3 at the final round, showing that M-AUPO outperforms baselines by a large margin. See
Appendix H.2 for additional experimental details and results.

7

Conclusion

To the best of our knowledge, this work provides the first theoretical result in online PbRL showing
that the suboptimality gap decreases as more options are revealed to the labeler for ranking feedback.
Our analysis also removes the OpeB q dependency in the leading term without modifying the algorithm,
implying that existing PbRL and dueling bandit methods can similarly avoid this dependence through
our refined analysis. Together, these results advance the theoretical understanding of PbRL, revealing
both the value of richer feedback and the opportunity for more refined and efficient analysis.
5

https://microsoft.github.io/msmarco/TREC-Deep-Learning
https://huggingface.co/datasets/berkeley-nest/Nectar
7
https://huggingface.co/google/gemma-2b-it
8
https://huggingface.co/Ray2333/reward-model-Mistral-7B-instruct-Unified-Feedback
6

10

Acknowledgements
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the
Korea government (MSIT) (No. RS-2022-NR071853, RS-2023-00222663, and RS-2025-25420849),
by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant
funded by the Korea government (MSIT) (No. RS-2025-02263754), by the AI-Bio Research Grant
through Seoul National University, and by the 2025 Global Google PhD Fellowship funded with
support from Google.org.

References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems, 24:2312–2320, 2011.
[2] Marc Abeille, Louis Faury, and Clément Calauzènes. Instance-wise minimax-optimal algorithms
for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages
3691–3699. PMLR, 2021.
[3] Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning
algorithm for the multinomial logit contextual bandit. European Journal of Operational
Research, 310(2):737–750, 2023.
[4] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for
the mnl-bandit. In Conference on learning theory, pages 76–78. PMLR, 2017.
[5] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic
learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.
[6] Hossein Azari Soufiani, William Chen, David C Parkes, and Lirong Xia. Generalized methodof-moments for rank aggregation. Advances in Neural Information Processing Systems, 26,
2013.
[7] Akshay Balsubramani, Zohar Karnin, Robert E Schapire, and Masrour Zoghi. Instancedependent regret bounds for dueling bandits. In Conference on Learning Theory, pages 336–360.
PMLR, 2016.
[8] Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier. Preferencebased online learning with dueling bandits: A survey. Journal of Machine Learning Research,
22(7):1–108, 2021.
[9] Viktor Bengs, Aadirupa Saha, and Eyke Hüllermeier. Stochastic contextual dueling bandits
under linear stochastic transitivity models. In International Conference on Machine Learning,
pages 1764–1786. PMLR, 2022.
[10] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual
bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth
International Conference on Artificial Intelligence and Statistics, pages 19–26. JMLR Workshop
and Conference Proceedings, 2011.
[11] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.
[12] Róbert Busa-Fekete, Balázs Szörényi, Paul Weng, Weiwei Cheng, and Eyke Hüllermeier.
Preference-based reinforcement learning: evolutionary direct policy search using a preferencebased racing algorithm. Machine learning, 97:327–351, 2014.
[13] Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale
Schuurmans, Yuejie Chi, and Bo Dai. Value-incentivized preference optimization: A unified
approach to online and offline RLHF. In The Thirteenth International Conference on Learning
Representations, 2025. URL https://openreview.net/forum?id=SQnitDuow6.
11

[14] Mingyu Chen, Yiding Chen, Wen Sun, and Xuezhou Zhang. Avoiding exp (r) scaling in
rlhf through preference-based exploration. In The Thirty-ninth Annual Conference on Neural
Information Processing Systems, 2025.
[15] Xi Chen, Yining Wang, and Yuan Zhou. Dynamic assortment optimization with changing
contextual information. The Journal of Machine Learning Research, 21(1):8918–8961, 2020.
[16] Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop:
Provably efficient preference-based reinforcement learning with general function approximation.
In International Conference on Machine Learning, pages 3773–3793. PMLR, 2022.
[17] Wooseong Cho, Taehyun Hwang, Joongkyu Lee, and Min-hwan Oh. Randomized exploration
for reinforcement learning with multinomial logistic function approximation. Advances in
Neural Information Processing Systems, 37:76643–76720, 2024.
[18] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems, 30, 2017.
[19] Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, and Sayak Ray Chowdhury. Active
preference optimization for sample efficient rlhf. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 96–112. Springer, 2025.
[20] Qiwei Di, Jiafan He, and Quanquan Gu. Nearly optimal algorithms for contextual dueling
bandits from adversarial feedback. In Forty-second International Conference on Machine
Learning, 2024.
[21] Qiwei Di, Tao Jin, Yue Wu, Heyang Zhao, Farzad Farnoud, and Quanquan Gu. Variance-aware
regret bounds for stochastic contextual dueling bandits. In The Twelfth International Conference
on Learning Representations, 2024.
[22] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen
Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf.
Transactions on Machine Learning Research, 2024.
[23] Shi Dong, Tengyu Ma, and Benjamin Van Roy. On the performance of thompson sampling on
logistic bandits. In Conference on Learning Theory, pages 1158–1160. PMLR, 2019.
[24] Miroslav Dudík, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi.
Contextual dueling bandits. In Conference on Learning Theory, pages 563–587. PMLR, 2015.
[25] Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, and Vaishakh Ravindrakumar. Maxing and ranking with few assumptions. Advances in Neural Information Processing
Systems, 30, 2017.
[26] Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic
algorithms for logistic bandits. In International Conference on Machine Learning, pages
3052–3060. PMLR, 2020.
[27] Louis Faury, Marc Abeille, Kwang-Sung Jun, and Clément Calauzènes. Jointly efficient and
optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence
and Statistics, pages 546–580. PMLR, 2022.
[28] Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking from pairwise comparisons. In International Conference on Artificial Intelligence
and Statistics, pages 1057–1066. PMLR, 2018.
[29] Taehyun Hwang and Min-hwan Oh. Model-based reinforcement learning with multinomial
logistic function approximation. In Proceedings of the AAAI conference on artificial intelligence,
volume 37, pages 7971–7979, 2023.
[30] Ashesh Jain, Brian Wojcik, Thorsten Joachims, and Ashutosh Saxena. Learning trajectory
preferences for manipulators via iterative improvement. Advances in neural information
processing systems, 26, 2013.
12

[31] Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse dueling bandits.
In Artificial Intelligence and Statistics, pages 416–424. PMLR, 2015.
[32] Minje Jang, Sunghyun Kim, Changho Suh, and Sewoong Oh. Optimal sample complexity of
m-wise data for top-k ranking. Advances in Neural Information Processing Systems, 30, 2017.
[33] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL
https://arxiv.org/abs/2310.06825.
[34] Ashish Khetan and Sewoong Oh. Data-driven rank breaking for efficient rank aggregation.
Journal of Machine Learning Research, 17(193):1–54, 2016.
[35] Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret lower bound
and optimal algorithm in dueling bandit problem. In Conference on learning theory, pages
1141–1154. PMLR, 2015.
[36] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Copeland dueling bandit problem:
Regret lower bound, optimal algorithm, and computationally efficient algorithm. In International
Conference on Machine Learning, pages 1235–1244. PMLR, 2016.
[37] Andras Kupcsik, David Hsu, and Wee Sun Lee. Learning dynamic robot-to-human object
handover from human feedback. In Robotics Research: Volume 1, pages 161–176. Springer,
2017.
[38] Branislav Kveton, Xintong Li, Julian McAuley, Ryan Rossi, Jingbo Shang, Junda Wu, and Tong
Yu. Active learning for direct preference optimization. arXiv preprint arXiv:2503.01076, 2025.
[39] Joongkyu Lee and Min-hwan Oh. Nearly minimax optimal regret for multinomial logistic
bandit. In The Thirty-eighth Annual Conference on Neural Information Processing Systems,
2024.
[40] Joongkyu Lee and Min-hwan Oh. Combinatorial reinforcement learning with preference
feedback. In Forty-second International Conference on Machine Learning, 2025.
[41] Joongkyu Lee and Min-hwan Oh. Improved online confidence bounds for multinomial logistic
bandits. In Forty-second International Conference on Machine Learning, 2025.
[42] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial)
logistic bandits via regret-to-confidence-set conversion. In International Conference on Artificial
Intelligence and Statistics, pages 4474–4482. PMLR, 2024.
[43] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. A unified confidence sequence for
generalized linear models, with applications to bandits. Advances in Neural Information
Processing Systems, 37:124640–124685, 2024.
[44] Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet,
and Zhaoran Wang. Provably mitigating overoptimization in RLHF: Your SFT loss is implicitly
an adversarial regularizer. In The Thirty-eighth Annual Conference on Neural Information
Processing Systems, 2024. URL https://openreview.net/forum?id=2cQ3lPhkeO.
[45] R Duncan Luce et al. Individual choice behavior, volume 4. Wiley New York, 1959.
[46] Daniel McFadden. Modelling the choice of residential location. 1977.
[47] Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and
Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active
exploration. arXiv preprint arXiv:2312.00267, 2023.
[48] Zakaria Mhammedi, Wouter M Koolen, and Tim Van Erven. Lipschitz adaptivity with multiple
learning rates in online learning. In Conference on Learning Theory, pages 2490–2511. PMLR,
2019.
13

[49] Subhojyoti Mukherjee, Anusha Lalitha, Kousha Kalantari, Aniket Anand Deshmukh, Ge Liu,
Yifei Ma, and Branislav Kveton. Optimal design for human preference elicitation. Advances in
Neural Information Processing Systems, 37:90132–90159, 2024.
[50] William Muldrew, Peter Hayes, Mingtian Zhang, and David Barber. Active preference learning
for large language models. In Forty-first International Conference on Machine Learning, 2024.
[51] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland,
Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Côme Fiegel, et al. Nash
learning from human feedback. In Forty-first International Conference on Machine Learning,
2023.
[52] Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior
sampling for preference-based reinforcement learning. In Conference on Uncertainty in Artificial
Intelligence, pages 1029–1038. PMLR, 2020.
[53] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits.
Advances in Neural Information Processing Systems, 32, 2019.
[54] Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality
and practicality. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 9205–9213, 2021.
[55] Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213,
2019.
[56] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. Multinomial logit bandit with linear
utility functions. In Proceedings of the Twenty-Seventh International Joint Conference on
Artificial Intelligence, IJCAI-18, pages 2602–2608. International Joint Conferences on Artificial
Intelligence Organization, 2018.
[57] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in neural information processing systems,
35:27730–27744, 2022.
[58] Jaehyun Park, Junyeop Kwon, and Dabeen Lee. Infinite-horizon reinforcement learning with
multinomial logit function approximation. In International Conference on Artificial Intelligence
and Statistics, pages 361–369. PMLR, 2025.
[59] Noemie Perivier and Vineet Goyal. Dynamic pricing and assortment under a contextual mnl
demand. Advances in Neural Information Processing Systems, 35:3461–3474, 2022.
[60] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series
C: Applied Statistics, 24(2):193–202, 1975.
[61] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with
multi-armed bandits. In Proceedings of the 25th international conference on Machine learning,
pages 784–791, 2008.
[62] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
Advances in Neural Information Processing Systems, 36:53728–53741, 2023.
[63] Siddartha Y Ramamohan, Arun Rajkumar, and Shivani Agarwal. Dueling bandits: Beyond
condorcet winners to general tournament solutions. Advances in Neural Information Processing
Systems, 29, 2016.
[64] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust DPO: Aligning
language models with noisy feedback. In Proceedings of the 41st International Conference
on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages
42258–42274. PMLR, 21–27 Jul 2024.
14

[65] Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active preference-based
learning of reward functions. 2017.
[66] Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in
Neural Information Processing Systems, 34:30050–30062, 2021.
[67] Aadirupa Saha and Pierre Gaillard. Finally rank-breaking conquers mnl bandits: Optimal and
efficient algorithms for mnl assortment. In The Thirteenth International Conference on Learning
Representations, 2025.
[68] Aadirupa Saha and Aditya Gopalan. Battle of bandits. In UAI, pages 805–814, 2018.
[69] Aadirupa Saha, Tomer Koren, and Yishay Mansour. Adversarial dueling bandits. In International
Conference on Machine Learning, pages 9235–9244. PMLR, 2021.
[70] Aadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with
trajectory preferences. In International conference on artificial intelligence and statistics, pages
6263–6289. PMLR, 2023.
[71] Antoine Scheid, Etienne Boursier, Alain Durmus, Michael I Jordan, Pierre Ménard, Eric
Moulines, and Michal Valko. Optimal design for reward modeling in rlhf. arXiv preprint
arXiv:2410.17055, 2024.
[72] Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. Contextual bandits and imitation
learning with preference-based active queries. Advances in Neural Information Processing
Systems, 36:11261–11295, 2023.
[73] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization.
In Conference on learning theory, pages 3–24. PMLR, 2013.
[74] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A
minimaximalist approach to reinforcement learning from human feedback. In Proceedings of
the 41st International Conference on Machine Learning, pages 47345–47377, 2024.
[75] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
[76] Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach,
and Branislav Kveton. Comparing few to rank many: Active human preference learning
using randomized frank-wolfe method. In Forty-second International Conference on Machine
Learning, 2024.
[77] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson.
Reward-free rl is no harder than reward-aware rl in linear markov decision processes. In
International Conference on Machine Learning, pages 22430–22456. PMLR, 2022.
[78] Christian Wirth and Johannes Fürnkranz. Preference-based reinforcement learning: A preliminary survey. In Proceedings of the ECML/PKDD-13 Workshop on Reinforcement Learning
from Generalized Feedback: Beyond Numeric Rewards, 2013.
[79] Christian Wirth, Johannes Fürnkranz, and Gerhard Neumann. Model-free preference-based
reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence,
volume 30, 2016.
[80] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz. A survey of
preference-based reinforcement learning methods. Journal of Machine Learning Research, 18
(136):1–46, 2017.
[81] Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. Advances in neural
information processing systems, 29, 2016.
[82] Runzhe Wu and Wen Sun. Making rl with preference-based feedback efficient via randomization.
In The Twelfth International Conference on Learning Representations, 2023.
15

[83] Yue Wu, Tao Jin, Qiwei Di, Hao Lou, Farzad Farnoud, and Quanquan Gu. Borda regret
minimization for generalized linear dueling bandits. In International Conference on Machine
Learning, pages 53571–53596. PMLR, 2024.
[84] Tengyang Xie, Dylan J Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Hassan Awadallah, and Alexander Rakhlin. Exploratory preference optimization: Harnessing implicit q*approximation for sample-efficient rlhf. In The Thirteenth International Conference on Learning
Representations, 2024.
[85] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong
Zhang. Iterative preference learning from human feedback: Bridging theory and practice for
rlhf under kl-constraint. In Forty-first International Conference on Machine Learning, 2024.
[86] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based
reinforcement learning with finite-time guarantees. Advances in Neural Information Processing
Systems, 33:18784–18794, 2020.
[87] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling
bandits problem. Journal of Computer and System Sciences, 78(5):1538–1556, 2012.
[88] Andrea Zanette, Ching-An Cheng, and Alekh Agarwal. Cautiously optimistic policy optimization and exploration with linear function approximation. In Conference on Learning Theory,
pages 4473–4525. PMLR, 2021.
[89] Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline
preference-based reinforcement learning. In The Twelfth International Conference on Learning
Representations, 2024.
[90] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. Provable reward-agnostic
preference-based reinforcement learning. In The Twelfth International Conference on Learning
Representations, 2024.
[91] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret
and constant computation cost. Advances in Neural Information Processing Systems, 36, 2024.
[92] Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. In International Conference on Machine
Learning, pages 43037–43067. PMLR, 2023.
[93] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence
bound for the k-armed dueling bandit problem. In International conference on machine learning,
pages 10–18. PMLR, 2014.
[94] Masrour Zoghi, Zohar S Karnin, Shimon Whiteson, and Maarten De Rijke. Copeland dueling
bandits. Advances in neural information processing systems, 28, 2015.

16

Appendix
Table of Contents

A

A Further Related Work

17

B Notation

18

C Proof of Theorem 1
C.1 Main Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Proofs of Lemmas for Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . .

19
19
24

D Proof of Theorem 2
D.1 Main Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Proofs of Lemmas for Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . .

29
29
31

E Technical Lemmas

33

F Proof of Theorem 3
F.1 Main Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
F.2 Proof of Lemmas for Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . .

35
35
38

G Additional Discussions
G.1 Avoiding eB Scaling in Regret Minimization . . . . . . . . . . . . . . . . . . .
G.2 Extension to Active Learning Setting . . . . . . . . . . . . . . . . . . . . . . .

39
39
41

H Experimental Details and Additional Results
H.1 Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
H.2 Real-World Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

42
42
46

Further Related Work

In this section, we provide additional related work that complements Section 2.
Logistic and MNL bandits. Our work is also closely related to logistic bandits and multinomial logit
(MNL) bandits. The logistic bandit problem [23, 26, 2, 27, 42, 43] is a special case of the MNL bandit
model in which the agent offers only a single item (i.e., K “ 1) at each round and receives binary
feedback indicating whether the item was selected (1) or not (0). Faury et al. [26] examined how the
regret in logistic bandits depends on the non-linearity parameter κ of the logistic link function and
proposed the first algorithm whose regret bound eliminates explicit dependence on 1{κ “ OpeB q.
Abeille et al. [2] further improved the theoretical dependency on 1{κ and established a matching,
problem-dependent lower bound. Building on this, Faury et al. [27] developed a computationally
efficient algorithm whose regret still matches the lower bound established by Abeille et al. [2].
Multinomial logit (MNL) bandits tackle a more sophisticated problem than logistic bandits. Instead of offering a single item and observing binary feedback, the learner chooses a subset of
items—underscoring the combinatorial nature of the task—and receives non-uniform rewards driven
by an MNL choice model [5, 4, 56, 15, 53, 54, 59, 3, 39, 41]. A recent breakthrough by Lee and
Oh [39] closed a long-standing gap by providing a computationally efficient algorithm that attains
the minimax-optimal regret for this setting. Building on this result, Lee and Oh [41] further reduced the regret bound by a factor polynomial in B and logarithmic in K, and established the first
variance-dependent regret bounds for MNL bandits.
Our work extends the online confidence bound analysis of Lee and Oh [41] to the Plackett–Luce (PL)
model. This extension is natural because the PL probability distribution decomposes into a sequence
17

of MNL probabilities over successive choices. Crucially, we leverage their key insight—that the
MNL loss exhibits an ℓ8 -self-concordant property—to eliminate the harmful OpeB q dependence.
This is one of the main contributions of our work (see Lemma C.2).
RL with MNL models. Recent work has extended the Multinomial Logit (MNL) framework beyond
bandit formulations to reinforcement learning. Lee and Oh [40] introduced combinatorial RL with
preference feedback, a framework in which an agent learns to select subsets of items so as to maximize
long-term cumulative rewards.
Another line of research incorporates MNL models directly into the transition dynamics. Hwang and
Oh [29] proposed MNL-MDPs, a class of Markov decision processes whose transition probabilities
follow an MNL parameterization. Building upon this formulation, Cho et al. [17] improved the regret
bounds by improving the exponential dependence on B, and Park et al. [58] extended the analysis to
the infinite-horizon setting.

B

Notation

Let T denote the total number of rounds, with t P rT s representing the current round. We use N for
the total number of items, K for the maximum assortment size, d for the feature vector dimension,
and B as an upper bound on the norm of the unknown parameter. For notational convenience, we
provide Table B.1.
For clarity, we derive the first- and second-order derivatives (i.e., gradients and Hessians) of the loss
pjq
pjq
functions. For the PL loss at round t for the j’th ranking, let yti “ 1 if i “ j, and yti “ 0 for
otherwise. Then, we have
˜
pjq
ℓt pθq “ ´ log

¸

`
˘
exp ϕpxt , σtj qJ θ
ř|St |

k“j exp pϕpxt , σtk q

|S
ÿt |

“´

J θqq

˜
pjq
yti log

i“j

`
˘
exp ϕpxt , σti qJ θ

¸

ř|St |
J
k“j exp pϕpxt , σtk q θqq
looooooooooooooomooooooooooooooon
pjq

“:Pt,θ pσti q
|S
ÿt |

“´

pjq

pjq

yti log Pt,θ pσti q,

i“j
pjq

∇ℓt pθq “

|S
ÿt | ´

pjq

pjq

Pt,θ pσti q ´ yti

¯
ϕpxt , σti q,

i“j
pjq

∇2 ℓt pθq “

|S
ÿt |

pjq

Pt,θ pσti qϕpxt , σti qϕpxt , σti qJ ´

i“j

|S
ÿt | |S
ÿt |

pjq

pjq

Pt,θ pσti qPt,θ pσtk qϕpxt , σti qϕpxt , σtk qJ

i“j k“j

|S
ÿt | |S
ÿt |

“

`
˘`
˘J
1
pjq
pjq
Pt,θ pσti qPt,θ pσtk q ϕpxt , σti q ´ ϕpxt , σtk q ϕpxt , σti q ´ ϕpxt , σtk q .
2 i“j k“j
pj,kq

For the RB loss at round t for the pairwise comparison between σtj and σtk , let yti
pj,kq
and yti “ 0 for otherwise (i.e., when i “ k). Then, we have
˜

“ 1 if i “ j,

¸
`
˘
exp ϕpxt , σtj qJ θ
exp pϕpxt , σtj qJ θq ` exp pϕpxt , σtk qJ θq
´`
˘J ¯
1
,
“ ´ log µ ϕpxt , σtj q ´ ϕpxt , σtk q θ , where µpwq “
1 ` e´w
´ ´`
¯
¯
˘J
`
˘
pj,kq
∇ℓt pθq “ µ ϕpxt , σtj q ´ ϕpxt , σtk q θ ´ 1 ϕpxt , σtj q ´ ϕpxt , σtk q ,
´`
˘J ¯ `
˘`
˘J
pj,kq
∇2 ℓt pθq “ µ9 ϕpxt , σtj q ´ ϕpxt , σtk q θ ϕpxt , σtj q ´ ϕpxt , σtk q ϕpxt , σtj q ´ ϕpxt , σtk q .
pj,kq
ℓt pθq “ ´ log

18

Table B.1: Symbols
X , A, S
ϕpx, aq P Rd
ztjk
St
pjq

ℓt pθq
pj,kq

ℓt

pθq

pjq
∇2 ℓt pθq
pj,kq

∇2 ℓt pθq
pj`1q
θp
t

pj,k`1q
θpt

η
λ
Ht
pjq

H̃t

pj,kq

H̃t

βt pδq
Tw
Mt

C

context (prompt) space, action (answer) space, assortment space
feature representation of context-action pair px, aq
:“ ϕpxt , σtj q ´ ϕpxt , σtk q, feature difference between σtj and σtk under context xt
assortment chosen by an algorithm at round t
ˆ
˙
exppϕpxt ,σtj qJ θ q
, PL loss at round t for j’th ranking
:“ ´ log ř|St |
J
ˆ k“j exppϕpxt ,σtk qJ θqq ˙
exppϕpxt ,σtj q θ q
:“ ´ log ř
exppϕpxt ,σtm qJ θq , RB loss at round t for comparison σtj vs σtk
mPtj,ku

“

ř|St | ř|St |
k1 “j

k“j

expppϕpxt ,σtk q`ϕpxt ,σtk1 qqJ θ q
´ř
¯2 ¨ ztkk1 z J 1
tkk
|S |
2 k1 t“j exppϕpxt ,σtk1 qJ θq

´
¯
J
J
“ µ9 ztjk
θ ztjk ztjk
, where µpwq “ 1`e1´w is sigmoid function
online parameter estimate using PL loss at round t, after j’th update
online parameter estimate using RB loss at round t, after pj, kq’th comparison update
?
:“ 12 p1 ` 3 2Bq, step-size parameter
`
˘
:“ Ω d logpKT {δq ` ηpB ` dq , regularization parameter
řt´1 ř|Ss |´1ř|Ss |
řt´1 ř|Ss | 2 pjq ppj`1q
2 pj,kq ppj,k`1q
q ` λId )
q ` λId (or s“1 j“1
p θs
:“ s“1 j“1
∇ ℓs pθs
k“j`1∇ ℓs
1
1
řj
2 pj q ppj q
:“ Ht ` η j 1 “1 ∇ ℓt pθt q (for PL loss)
ř
pj 1 ,k1 q ppj 1 ,k1 q
:“ Ht ` η pj 1 ,k1 qďpj,kq ∇2 ℓt
pθt
q (for RB loss)
´ a
? ¯
:“ O B d logptK{δq ` B λ , confidence radius for θt at round t
!
)
:“ t P rT s : maxa,a1 PA }ϕpxt , aq ´ ϕpxt , a1 q}H ´1 ě 3?2Kβ1 pδq , warm-up rounds
tı
T `1
”
ř
ř|Ss |
pjq
:“ sPrt´1szT w j“1
Eσ„Ps p¨|Ss ,θ‹ q ∇2 ℓs pθps q ` λId

Proof of Theorem 1

In this section, we present the proof of Theorem 1.
C.1

Main Proof of Theorem 1

PL loss and OMD. We begin by recalling the loss function and the parameter update rule. Specifically,
we use the PL loss defined in Equation (3) and update the parameter according to Equation (5).
˜
¸ |S |
`
˘
|S
ÿt |
ÿt pjq
exp ϕpxt , σtj qJ θ
“
ℓt pθq :“
´ log ř|S |
ℓt pθq,
t
J θqq
exp
pϕpx
,
σ
q
t
tk
j“1 loooooooooooooooooooooomoooooooooooooooooooooon
j“1
k“j
pjq

“:ℓt pθq

and
1
pj`1q
pjq pjq
pjq
θpt
“ argmin x∇ℓt pθpt q, θy ` }θ ´ θpt }2H̃ pjq , j “ 1, . . . , |St |,
2η
t
θPΘ
?
p|S
|`1q
p1q
pjq
where θpt t
“ θpt`1 , and η :“ 12 p1 ` 3 2Bq is the step-size parameter. The matrix H̃t is given
1
1
řj
pjq
pj q pj q
by H̃ :“ Ht ` η 1 ∇2 ℓ pθp q, where
t

j “1

t

Ht :“

t

t´1
s|
ÿ |S
ÿ

ppj`1q q ` λId ,
∇2 ℓpjq
s p θs

λ ą 0.

s“1 j“1

Online confidence bound for PL loss. Now, we present the confidence bound for online parameter
estimation in MNL models, as recently proposed by Lee and Oh [41].
Lemma C.1
? (Online confidence bound,
? Theorem 4.2 of Lee and Oh 41). Let δ P p0, 1s. We set
η “ p1 ` 3 2Bq{2 and λ “ maxt12 2Bη, 144ηd, 2u. Then, under Assumption 1, with probability
19

at least 1 ´ δ, we have
´ a
? ¯
}θpt ´ θ ‹ }Ht ď βt pδq “ O B d logpt{δq ` B λ ,

@t ě 1.

řt
We now extend this result to our setting. Since the total number of updates up to round t is s“1 |Ss |,
the corresponding confidence bound can be expressed as follows:
Corollary C.1 (Restatement
of Corollary 1, Online
confidence bound for PL loss). Let δ P p0, 1s.
?
?
We set η “ p1 ` 3 2Bq{2 and λ “ maxt12 2Bη, 144ηd, 2u. Then, under Assumption 1, with
probability at least 1 ´ δ, we have
´ a
? ¯
pjq
}θpt ´ θ ‹ }H pjq ď βt pδq “ O B d logptK{δq ` B λ , @t ě 1, j ď |St |,
t

řj´1
pjq
pj 1 q pj 1 `1q
p1q
where Ht :“ Ht ` j 1 “1 ∇2 ℓs pθps
q ` λId and θpt “ θpt .
Useful definitions. For clarity, we write ℓt,σ pθq to make explicit that the loss function is random with
respect to the ranking σ. When the ranking is realized as σt , we simply denote it by ℓt pθq. We define
the set of warm-up rounds, denoted by T w , which consists of rounds with large uncertainty, as:
"
*
1
1
?
´1
T w :“ t P rT s : max
}ϕpx
,
aq
´
ϕpx
,
a
q}
ě
,
(C.1)
t
t
Ht
a,a1 PA
3 2KβT `1 pδq
where βT `1 pδq denotes the confidence radius as defined in Corollary C.1.
Furthermore, we define the expected version of the Hessian matrix (taken with respect to the
randomness of the ranking feedback) as follows:
Mt “

ÿ

|S
s|
ÿ

”
ı
ps q ` λId
Eσ„Ps p¨|Ss ,θ‹ q ∇2 ℓpjq
p
θ
s,σ

(C.2)

sPrt´1szT w j“1
pjq
Note that ∇2 ℓs,σ pθps q is a random matrix conditional on the assortment Ss , where the randomness
arises from the ranking feedback σ.

Key lemmas. We now present key lemmas needed to prove Theorem 1. The following lemma, one
of our main contributions, is crucial for avoiding the 1{κ “ OpeB q dependency in the leading term.
Lemma C.2 (Empirical-to-expected Hessian lower bound). Let Mt be defined as in Equation (C.2).
Set λ “ Ωpd logpKT {δqq. Then, for all t P rT s, with probability at least 1 ´ δ, we have
1
Ht ľ 2 M t .
3e
The proof is deferred to Appendix C.2.1.
The following lemma is the elliptical potential lemma adapted to our setting.
Lemma C.3 (Elliptical potential for expected mean-centered uncertainty). Let Mt be defined as in
pjq
Equation (C.2). Let Pt pa | St ; θq denote the MNL probability for the subset of remaining actions in
St after removing the first j ´ 1 actions according to the ranking σ. Then, for any λ ą 0, we have
$
,
„›
& |S
›2 ȷ.
ÿ
ÿt |
›
›
‹
min 1,
E
›ϕpxt , aq ´ Ea„Pt p¨|S pjq ;θpt q rϕpxt , a1 qs› ´1
t ;θ q
t
% j“1 σ„Pt p¨|Spjq
Mt
w
p
tRT
a„Pt p¨|St ;θt q
ˆ
˙
KT
ď 2d log 1 `
.
dλ
The proof is deferred to Appendix C.2.2.
The size of the set T w is bounded as described in the following lemma.
␣
Lemma C.4. Let T w “ t P rT s : maxa,a1 PA }ϕpxt , aq ´ ϕpxt , a1 q}H ´1 ě 3?2Kβ1
t

κ :“ e´6B . Set λ ě 1. Then, the size of the set T w is bounded as follows:
ˆ
˙
288K 4
2KT
|T w | ď
βT `1 pδq2 d log 1 `
.
κ
dλ
20

T `1 pδq

(
. Define

The proof is deferred to Appendix C.2.3.
We are now ready to provide the proof of Theorem 1.
Proof of Theorem 1. To begin, we define a martingale difference sequence (MDS) ζt as follows:
”`
˘J ı `
˘J
pT pxqq θ ‹ ´ ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq θ ‹ ,
ζt :“Ex„ρ ϕ px, π ‹ pxqq ´ ϕ px, π
which satisfies |ζt | ď 2B. Then, by the definition of the suboptimality gap, we have
”`
˘J ı
pT pxqq θ ‹
SubOptpT q “ Ex„ρ ϕ px, π ‹ pxqq ´ ϕ px, π
“

T
T
˘J
1 ÿ
1 ÿ`
pT pxt qq θ ‹ `
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
ζt
T t“1
T t“1

ď

T
T
¯ 1 ÿ
˘J ´ ‹
1 ÿ`
pT pxt qq
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
θ ´ θpT `1 `
ζt
T t“1
T t“1

(Def. of ζt )

πT pxt q “ argmaxaPA ϕpxt , aqJ θpT `1 )
(p
ˆ
˙
T
¯
˘J ´ ‹
1
1 ÿ`
pT pxt qq
, (C.3)
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
θ ´ θpT `1 ` Õ ?
ď
T t“1
T
where the last inequality follows from the Azuma–Hoeffding inequality. Specifically, for any T ě 1,
with probability at least 1 ´ δ, we have
ˆ
˙
T
1 ÿ
1
1a 2
?
8B T logp1{δq “ Õ
.
ζt ď
T t“1
T
T
To complete the proof, it remains to bound the first term in Equation (C.3).
T
¯
˘J ´ ‹
1 ÿ`
pT pxt qq
θ ´ θpT `1
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
T t“1
¯
˘J ´ ‹
1 ÿ `
“
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq
θ ´ θpT `1
T tPT w
¯
˘J ´ ‹
1 ÿ `
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq
θ ´ θpT `1
`
T tRT w
¯
˘J ´ ‹
4B w
1 ÿ `
pT pxt qq
ď
|T | `
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
θ ´ θpT `1
T
T tRT w
ˆ
ˆ
˙˙
4
BK
T
2
ďO
βT `1 pδq d log 1 `
κT
dλ
¯
˘J ´ ‹
1 ÿ `
pT pxt qq
`
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
θ ´ θpT `1 .
T tRT w
To further bound the last term of Equation (C.4), we get
¯
˘J ´ ‹
1 ÿ `
θ ´ θpT `1
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq
T tRT w
›
›
1 ÿ
›
›
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1 ›θ ‹ ´ θpT `1 ›
ď
T `1
T tRT w
HT `1
›
›
1 ÿ
›
›
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1 ›θ ‹ ´ θpT `1 ›
ď
t
T tRT w
HT `1
ď

(Assumption 1)
(Lemma C.4)
(C.4)

(Hölder’s ineq.)
(HT `1 ľ Ht )

βT `1 pδq ÿ
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1 .
t
T
w
tRT
(Corollary C.1, with prob. 1 ´ δ)
21

Define the sigmoid function µpwq “ 1`e1´w . Then, we have
ÿ

}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t

tRT w

˘J ‹ ¯
θ
‹
´`
“
pT pxt qq}H ´1
˘J ¯ }ϕ pxt , π pxt qq ´ ϕ pxt , π
t
‹
‹
9 ϕ pxt , π pxt qq ´ ϕ pxt , π
pT pxt qq θ
tRT w µ
ÿ ´`
˘J ¯
“ p1 ` eq2
µ9 ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq θ ‹ }ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1 ,
ÿ µ9

´`

ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq

t

tRT w

where the last equality holds due to the fact that
1
µ9

´`

˘J ¯
pT pxt qq θ ‹
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
˘J ¯2
`
´
ϕpxt ,π ‹ pxt qq´ϕpxt ,p
πT pxt qq θ ‹
1`e
“
`
˘J
‹
‹
e ϕpxt ,π pxt qq´ϕpxt ,pπT pxt qq θ
`
˘J ¯2
´
`
˘J
‹
‹
pT pxt qq θ ‹ ě 0)
ď 1 ` e ϕpxt ,π pxt qq´ϕpxt ,pπT pxt qq θ
( ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
ˆ
˙2
`
˘J
πT pxt qq pθ ‹ ´θpT `1 q
ϕpxt ,π ‹ pxt qq´ϕpxt ,p
ď 1`e
(p
πT pxt q “ argmaxaPA ϕpxt , aqJ θpT `1 )
ˆ
1`e

ď
ˆ
ď

1`e

πT pxt qq}
}ϕpxt ,π ‹ pxt qq´ϕpxt ,p

´1 }θ
Ht

β
pδq
? T `1
3 2KβT `1 pδq

‹

´θpT `1 }HT `1

˙2
(Hölder’s ineq. and HT `1 ľ Ht )

˙2
(t R T w and Corollary C.1)

“ p1 ` e1{K q2 ď p1 ` eq2 .
pjq

Recall that Sσ :“ tσj , . . . , σ|S| u denotes the subset of remaining actions in S after removing the
first j ´ 1 actions, given S and σ. We denote the Multinomial Logit (MNL) model at round t by
Pt pa | S; θq (see Equation (9)). Then, by our assortment selection rule in Equation (11), we obtain

µ9

´`

˘J ¯
pT pxt qq θ ‹ }ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
t
`
˘ˇˇ
ˇ
´`
¯
ˇ
‹
J p
˘
J
πT pxt qq θt ´θ ‹ ˇ
ˇϕpx ,π pxt qq´ϕpxt ,p
pT pxt qq θpt e t
ď µ9 ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
¨ }ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
(Lemma E.2)
t
´`
¯
˘
J
ď e ¨ µ9 ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq θpt }ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t

(Eqn. C.5)
„›
ȷ
›
e
›
1 ›
“ ¨ Ea„Pt p¨|S ‹ ;θpt q ›ϕpxt , aq ´ Ea1 „Pt p¨|S ‹ ;θpt q rϕpxt , a qs› ´1
t
t
2
Ht
9
“ µpzqp1 ´ µpzqq and let St‹ :“ tπ ‹ pxt q, π
pT pxt qu)
(µpzq
‹
„›
ȷ
|St |
›
ÿ
e
›
›
Ea„Pt p¨|S ‹,pjq ;θpt q ›ϕpxt , aq ´ Ea1 „Pt p¨|S ‹,pjq ;θpt q rϕpxt , a1 qs› ´1 ,
“ ‹ Eσ„Pt p¨|S ‹ ;θpt q
t
t
t
|St |
Ht
j“1
(|St‹ | “ 2)
22

where the second inequality holds since, for any t R T w and a, a1 P A, the following property holds:
ˇ`
¯ˇ
˘J ´
ˇ
ˇ
θpt ´ θ ‹ ˇ
ˇ ϕpxt , aq ´ ϕpxt , a1 q
›
›
›
›
ď ›ϕpxt , aq ´ ϕpxt , a1 q›H ´1 ›θpt ´ θ ‹ ›Ht
t
›
›
1
p
›
ď ?
θt ´ θ ‹ ›Ht
3 2KβT `1 pδq
βt pδq
ď
KβT `1 pδq
1
ď .
K

(Hölder’s inequality)
(t ‰ T w )
(Corollary C.1)
(C.5)

Combining the above results, we get
ÿ

}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t

tRT w

„›
ȷ
|St‹ |
›
1 ÿ
›
1 ›
E
,
aq
´
E
rϕpx
,
a
qs
(C.6)
ď ep1 ` eq
‹,pjq
‹ p
›ϕpx
›
t
t
Pt p¨|St
;θpt q
t ;θt q
|St‹ | j“1 Pt p¨|S‹,pjq
Ht´1
tRT w
p
Pt p¨|St
;θt q
g
g
f
fT
„›
|St‹ |
›2 ȷ
fÿ 1 f
f ÿ |St | ÿ
›
2e
1 qs›
ď ep1 ` eq
E
,
aq
´
E
rϕpx
,
a
‹,pjq
e
‹ p
›ϕpx
› ´1
t
t
Pt p¨|St
;θpt q
t ;θt q
|St | tRT w |St‹ | j“1 Pt p¨|S‹,pjq
Ht
t“1
2

ÿ

Pt p¨|St

;θpt q

(Cauchy-Schwartz ineq., and T w Ď rT s)
g
g
f
fT
„›
|S
›2 ȷ
fÿ 1 f
ÿt |
|S
f ÿ 
›
›
t|
2e
1
ď ep1 ` eq
E Pt p¨|St ;θpt q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a qs› ´1
e
t

|S
|
Ht
|S |
t
pjq p
t“1
tRT w t j“1
Pt p¨|St ;θt q

(Assortment selection rule, Equation (11))
g
g
fT
f
„›
›2 ȷ
f ÿ 1 f ÿ |S
ÿt |
›
›
2
2e
e
1
‹
E Pt p¨|St ;θ q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a qs› ´1
ď e p1 ` eq
t
|S
|
Ht
t
pjq p
t“1
tRT w j“1
Pt p¨|St ;θt q

(Eqn. (C.7))
g
g
fT
f
„
›2 ȷ
›
f ÿ 1 f ÿ |S
ÿt |
? 3
›
›
2e
e
1
E Pt p¨|St ;θ‹ q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a qs› ´1 ,
ď 3e p1 ` eq
t
|St | tRT w j“1
Mt
pjq p
t“1
Pt p¨|St ;θt q

(Lemma C.2, with prob. 1 ´ δ)
where the second-to-last inequality holds because, for any t R T w , S P S, and a P S, we have
´
¯
exp ϕpxt , aqJ θpt
´
¯
Pt pa|S; θpt q “ ř
1 Jp
a1 PS exp ϕpxt , a q θt
´`
˘J ¯
exp ϕpxt , aq ´ ϕpxt , āq θpt
´`
“
˘J ¯
ř
1 ` a1 PSztāu exp ϕpxt , a1 q ´ ϕpxt , āq θpt
´`
¯
˘J
exp ϕpxt , aq ´ ϕpxt , āq θ ‹ ` 1{K
´`
¯
ď
˘J
ř
1 ` a1 PSztāu exp ϕpxt , a1 q ´ ϕpxt , āq θ ‹ ´ 1{K
“ e2{K Pt pa|S; θ ‹ q,
23

(any ā P S)

(Eqn. (C.5))

which implies that, for the ranking σ “ pσ1 , . . . , σ|S| q,
Pt pσ|S; θpt q “

|S|
ź

Pt pσj |Sσpjq ; θpt q ď e2|S|{K

j“1

|S|
ź

Pt pσj |Sσpjq ; θ ‹ q ď e2

j“1

2

|S|
ź

Pt pσj |Sσpjq ; θ ‹ q

j“1

‹

“ e Pt pσ|S; θ q.

(C.7)

Moreover, by setting λ “ d log T and T ě eK{d , for any t, we have
„›
|S
›2 ȷ 2K
ÿt |
›
›
1
ď 2.
E Pt p¨|St ;θ‹ q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a qs› ´1 ď
t
λ
Mt
pjq p
j“1

(C.8)

Pt p¨|St ;θt q

Hence, by the elliptical potential for expected mean-centered uncertainty (Lemma C.3), we obtain
¯
˘J ´ ‹
1 ÿ `
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq
θ ´ θpT `1
T tRT w
βT `1 pδq ÿ
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t
T
tRT w
g
g
˛
¨
fT
f
„›
ȷ
|S
t|
›
f
ÿ
ÿ
ÿ
f
2
βT `1 pδq e
1 e
›
›
E Pt p¨|St ;θ‹ q ›ϕpxt , aq ´ EPt p¨|S pjq ;θpt q rϕpxt , a1 qs› ´1 ‚
ď O˝
t
T
|S
|
Mt
t
pjq
t“1
tRT w j“1 Pt p¨|St ;θpt q
g
¨
˛
d
fT
ˆ
˙
fÿ 1
β
pδq
KT ‚
T
`1
e
˝
“O
d log 1 `
(Lemma C.3 with Eqn. (C.8))
T
|St |
dλ
t“1
g
¨
˛
fT
f
ÿ
a
β
pδq
1
T `1
e
¨ d log pKT q‚.
“ O˝
(C.9)
T
|S
|
t
t“1
ď

` a
By combining Equations (C.3), (C.4), and (C.9), and setting βT `1 pδq “ O B d logpKT q `
? ˘
B λ , we derive that, with probability at least 1 ´ 3δ (omitting logarithmic terms and polynomial
dependencies on B for brevity),
¨ g
˛
fT
2 4
fÿ 1
d
d
K
‚.
`
SubOptpT q “ Õ ˝ e
T t“1 |St |
κT
Substituting δ Ð 3δ , we conclude the proof of Theorem 1.

C.2
C.2.1

Proofs of Lemmas for Theorem 1
Proof of Lemma C.2

Proof of Lemma C.2. Recall the definition of Ht .
Ht “

t´1
s|
ÿ |S
ÿ

ÿ

ppj`1q q ` λId ľ
∇2 ℓpjq
s p θs

s“1 j“1

|S
s|
ÿ

ppj`1q q ` λId
∇2 ℓpjq
s pθs

sPrt´1szT w j“1
pjq

pj`1q

Here, we can equivalently express the MNL loss at step j and round s, denoted by ∇2 ℓs pθps
as follows:
´
¯
¨
˛
˜
¸
pj`1q
exp ϕpxs , σsj qJ θps
q
exp
pa
sj
pjq ppj`1q
´
¯ ‚ “ ´ log ř
ℓ s p θs
q “ ´ log ˝ ř
|Ss |
|Ss |
J ppj`1q q
k“j exp pask q
k“j exp ϕpxs , σsk q θs
pjq
“: ℓ̄pjq
s pas q,

q,

(C.10)
24

pj`1q
pjq
|Ss |
where asj “ ϕpxs , σsj qJ θps
, as “ pask qk“j
P R|Ss |´j`1 . Define the matrix

¨

˛
ϕpxs , σsj qJ
˚
‹
..
Φpjq
‚ P Rp|Ss |´j`1qˆd ,
s “˝
.
ϕpxs , σs|Ss | qJ
where each row corresponds to the feature vector of an action ranked from position j to |Ss | in the
pjq
|Ss |
ps “ pp
asj “ ϕpxs , σsj qJ θps and a
ask qk“j
ranking σs . Moreover, we define p
P R|Ss |´j`1
Then, using the ℓ8 -norm self-concordant property of the MNL loss [41], for any s P rt ´ 1szT w , we
obtain
´
¯J
ppj`1q q “ Φpjq ∇2 ℓ̄pjq papjq q Φpjq
∇2 ℓpjq
p
θ
(Eqn. (C.10))
s
s
s
a s
s
s
´
¯J
?
pjq
pjq
pjq
ľ e´3 2}as ´pas }8 Φpjq
∇2a ℓ̄pjq
apjq
(Lemma E.1)
s
s pp
s q Φs
¯J
1 ´
pjq
pjq
pjq
2
ps }8 ď 3?
)
ľ 2 Φpjq
∇2a ℓ̄pjq
apjq
(}as ´ a
s
s pp
s q Φs
2
e
1
p
“ 2 ∇2 ℓpjq
(Eqn. (C.10))
s pθs q,
e
where the last inequality holds because, for any s P rt ´ 1szT w and j ď |Ss |, the following holds:
ˇ
´
¯ˇ
ˇ
ˇ
ˇϕpxk , σsk qJ θpspk`1q ´ θps ˇ
k“j,...,|Ss |
ˆ›
›
›
›
ď max }ϕpxk , σsk q}Hs´1 ›θpspk`1q ´ θ ‹ ›

ppjq
}apjq
s ´a
s }8 ď

max

k“j,...,|Ss |

Hs

1
ď ?
max
3 2βT `1 pδq k“j,...,|Ss |

›
›
›
›
` ›θps ´ θ ‹ ›

˙

Hs

(Hölder’s inequality)
ˆ›
›
›
› ˙
› ppk`1q
›p
‹›
‹›
θ
´
θ
`
θ
´
θ
› s
› pk`1q › s
›
Hs

Hs

w

pk`1q

(s R T , Hs ĺ Hs
2βT `1 pδq
ď ?
3 2βT `1 pδq
2
“ ? .
3 2

)

(Corollary C.1, βt pδq is non-decreasing)

Therefore, we get

Ht ľ

ÿ

|S
s|
ÿ

ppj`1q q ` λId ľ
∇2 ℓpjq
s pθs

sPrt´1szT w j“1

1
e2

ÿ

|S
s|
ÿ

p
∇2 ℓpjq
s pθs q ` λId .

sPrt´1szT w j“1

Recall the definition of the MNL choice probability (9):
`
˘
exp ϕpxs , aqJ θ
,
Ps pa|S; θq :“ ř
1 J
a1 PS exp pϕpxs , a q θq

@a P S.

pjq

Given Ss “ tσs1 , . . . , σs|Ss | u and the ranking feedback σs , we define Sσs :“ tσsj , . . . , σs|Ss | u
as the subset of remaining actions in Ss after removing the first j ´ 1 actions. For simplicity, let
25

pjq
pjq
Pps “ Ps p¨|Sσs ; θpt q. Then, we can express the Hessian of the loss as follows:
´
¯
J
|S
|S
s | exp
s|
s | |S
s | |S
pϕpxs , σsk q ` ϕpxs , σsk1 qq θps
ÿ
ÿ
ÿ
ÿ
J
p
∇2 ℓpjq
´ř
´
¯¯2 ¨ zskk1 zskk1
s p θs q “
|Ss |
J
p
j“1
j“1 k“j k1 “j 2
1
k1 “j exp ϕpxs , σsk q θs
|Ss | |Ss | |Ss |

“

1 ÿ ÿ ÿ
J
Ps pσsk |Sspjq ; θps qPs pσsk1 |Sspjq ; θps qzskk1 zskk
1
2 j“1 k“j k1 “j

“

|Ss | |Ss | |Ss |
”`
˘`
˘J ı
1 ÿ ÿ ÿ
Epa,a1 q„Pp pjq ˆPp pjq ϕpxs , aq ´ ϕpxs , a1 q ϕpxs , aq ´ ϕpxs , a1 q
s
s
2 j“1 k“j k1 “j
|S
s|
ÿ

“

”`
EPp pjq
s

˘`
˘J ı
ϕpxs , aq ´ EPp pjq rϕpxs , a1 qs ϕpxs , aq ´ EPp pjq rϕpxs , a1 qs
ľ 0,
s

s

j“1

(C.11)
pjq
where zskk1 “ ϕpxs , σsk q ´ ϕpxs , σsk1 q. Therefore, j“1 ∇2 ℓs pθps q is a random PSD matrix conditional on the assortment Ss , where the randomness arises from the realized ranking
σs . Note that the true PL distribution Ps p¨|Ss ; θ ‹ q is measurable with respect to the filtration
Fs´1 ´“ σpx1 , S1 , σ1 , . ¯
. . , xs , Ss q. Moreover, the maximum eigenvalue of the Hessian satisfies
ř|Ss | 2 pjq p
λmax
j“1 ∇ ℓs pθs q ď K. Then, by the PSD matrices concentration lemma (Lemma E.3),
with probability 1 ´ δ, we have

ř|Ss |

Ht ľ

1
e2

|S
s|
ÿ

ÿ

p
∇2 ℓpjq
s pθs q ` λId

sPrt´1szT w j“1
|Ss |

1 ÿ 2 pjq p
∇ ℓs pθs q ` λId
K j“1
sPrt´1szT w
˛
fi
¨
»
|S
s|
ÿ
ÿ
1
K
p fl
‚
∇2 ℓpjq
ľ 2˝
Eσ„Ps p¨|Ss ,θ‹ q –
s,σ pθs q ` λId
3e
K
w
j“1
“

K
e2

ÿ

sPrt´1szT

¨
“

1 ˝
3e2

(Lemma E.3, with prob. 1 ´ δ)
˛
”
ı
ÿ
1
p
‚
Mt .
Eσ„Ps p¨|Ss ,θ‹ q ∇2 ℓpjq
s,σ pθs q ` λId “
2
3e
w j“1
|S
s|
ÿ

sPrt´1szT

This concludes the proof of Lemma C.2.
C.2.2

Proof of Lemma C.3

Proof of Lemma C.3. By the definition of Mt (C.2) and Equation (C.11), for t R T w , we have
¨
˛
|S
”
ı
ÿt |
pjq
det pMt`1 q “ det ˝Mt `
Eσ„Pt p¨|St ,θ‹ q ∇2 ℓt,σ pθpt q ‚
j“1

¨
“ det ˝Mt `

|S
ÿt |
j“1

¨
“ det pMt q ˝1 `

˛
”
ı
pjq
pjq
E σ„Pt p¨|St ,θ‹ q ϕ̄t paqϕ̄t paqJ ‚
|S
ÿt |

˛
”
ı
pjq
E σ„Pt p¨|St ,θ‹ q }ϕ̄t paq}2M ´1 ‚

j“1

ě det pλId q

sPrtszT

pjq
a„Pt p¨|St,σ ;θpt q

$
&

t

,˛
”
ı.
2
˝1 ` min 1,
‚,
‹
E
}ϕ̄pjq
s paq}Ms´1
s ,θ q
% j“1 σ„Ps p¨|Spjq
p
w
¨

ź

(Eqn. (C.11))

pjq
a„Pt p¨|St,σ ;θpt q

|S
s|
ÿ

a„Ps p¨|Ss,σ ;θs q

26

pjq

where, for simplicity, we denote ϕ̄t paq “ ϕpxt , aq ´ Ea1 „Pt p¨|S pjq ;θpt q rϕpxt , a1 qs.
t,σ

For t P T w , it is clear that det pMt`1 q “ det pMt q.
Hence, using the fact that a ď 2 logp1 ` aq for any a P r0, 1s, we get
,
”
ı.
pjq
min 1,
E σ„Pt p¨|St ,θ‹ q }ϕ̄t paq}2M ´1
t
%
pjq
j“1 a„Pt p¨|St,σ ;θpt q
tRT w
$
,˛
¨
& |S
”
ı.
ÿ
ÿt |
pjq
‹
}ϕ̄t paq}2M ´1 ‚
log ˝1 ` min 1,
ď2
E
t ,θ q
t
% j“1 σ„Pt p¨|Spjq
w
tRT
a„Pt p¨|St,σ ;θpt q
ˆ
˙
det pMT `1 q
ď 2 log
det pλId q
ˆ
˙
4KT
ď 2d log 1 `
,
dλ
$
&

ÿ

|S
ÿt |

where the last inequality holds because
ˆ
det pMT `1 q ď

λ1 ` ¨ ¨ ¨ ` λd
d

˙d
(λ1 , ¨ ¨ ¨ , λd are eigenvalues of MT `1 , AM-GM ineq.)

˙d
tracepMT `1 q
ď
d
˜
¸d ˆ
řT ř|St |
˙d
pjq
λd ` t“1 j“1 E}ϕ̄t paq}22
KT
“
ď λ`
.
d
d
ˆ

pjq

(E}ϕ̄t paq}22 ď 1)

This concludes the proof of Lemma C.3.

C.2.3

Proof of Lemma C.4

Proof of Lemma C.4. Let
pãt , āt q “ argmax µ9
a,a1 PA

´`

˘J ¯
ϕpxt , aq ´ ϕpxt , a1 q θpt }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1 ,
t

where µpwq “ 1`e1´w denote the sigmoid function. Note that, by our assortment selection rule in
´`
˘J ¯
Equation (11), the two actions a, a1 P A that maximize µ9 ϕpxt , aq ´ ϕpxt , a1 q θpt }ϕpxt , aq ´
ϕpxt , a1 q}2H ´1 are always included in St . It is because
t

µ9

´`

˘J ¯
ϕpxt , aq ´ ϕpxt , a1 q θpt }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
t
„›
›2 ȷ
›
›
“ 2Ea„Pt p¨|ta,a1 u;θpt q ›ϕpxt , aq ´ Ea1 „Pt p¨|ta,a1 u;θpt q rϕpxt , a1 qs› ´1
Ht
„›
›2 ȷ
›
›
“ 2Eσ„Pt p¨|ta,a1 u;θpt q ›ϕpxt , aq ´ Ea1 „Pt p¨|ta,a1 u;θpt q rϕpxt , a1 qs› ´1 “ 2ft pta, a1 uq,
Ht

a„Pt p¨|ta,a1 u;θpt q

which directly implies that ãt , āt P St .
27

For simplicity, we denote zskk1 “ ϕpxs , σsk q ´ ϕpxs , σsk1 q. Then, by the definition of Ht , we have
Ht “

t´1
s|
ÿ |S
ÿ

ppj`1q q ` λId
∇2 ℓpjq
s p θs

s“1 j“1

´
¯
J pj`1q
pϕpxs , σsk q ` ϕpxs , σsk1 qq θps
J
´ř
´
¯¯2 ¨ zskk1 zskk1 ` λId
|Ss |
pj`1q
J
p
1
2
k1 “j exp ϕpxs , σsk q θs

t´1
s | |S
s | |S
s | exp
ÿ |S
ÿ
ÿ
ÿ

“
s“1 j“1 k“j k1 “j

t´1 |Ss | |Ss | |Ss |

e´4B ÿ ÿ ÿ ÿ
J
zskk1 zskk
ľ
1 ` λId
2K 2 s“1 j“1 k“j k1 “j
t´1

˘`
˘J
e´4B ÿ `
ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q ` λId
( ãs , ās P Ss )
2
2K s“1
˜
ÿ ´`
˘J ¯ `
˘`
˘J
e´4B
µ
9
ϕpx
,
ã
q
´
ϕpx
,
ā
q
θps ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q
ľ
s
s
s
s
2K 2 sPT w
t
¸

ľ

` λId ,

(C.12)

´`
˘J ¯
where the last inequality holds since µ9 ϕpxs , aq ´ ϕpxs , a1 q θps ď 14 . In addition, we denote
Ttw “ T w ztt, . . . , T u in the last inequality. To better presentation, we define
ÿ ´`
˘J ¯ `
˘`
˘J
µ9 ϕpxs , ãs q ´ ϕpxs , ās q θps ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q
Λw
t :“
sPTtw

` λId .
On the other hand, by setting λ ě 1, for any t, we can ensure that
´`
˘J ¯
1 2
µ9 ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2` w ˘´1 ď ¨
4 λ
Λt

9 ď 41 )
(µp¨q

ď 1.

(C.13)

Then, we can derive that
ÿ
max
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
1
tPT w

a,a PA

t

˘J ¯
θpt
´`
¯ }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
“
max
˘
t
a,a1 PA
1q Jθ
p
w
µ
9
ϕpx
,
aq
´
ϕpx
,
a
tPT
t
t
t
´`
¯
ÿ
˘
1 Jp
µ
9
ď 4e2B
max
ϕpx
,
aq
´
ϕpx
,
a
q
θ
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
t
t
t
1
µ9

ÿ

tPT w

´`

ϕpxt , aq ´ ϕpxt , a1 q

a,a PA

t

(Assumption 1)
“ 4e2B

ÿ

µ9

´`

˘J ¯
ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2H ´1
t

tPT w

ď 8K 2 e6B

ÿ

µ9

´`

¯

˘J
ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2` w ˘´1
Λt

tPT w

“ 8K 2 e6B

ÿ

(Eqn. (C.12))
"
*
´`
˘J ¯
min 1, µ9 ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2` w ˘´1
Λt

tPT w

(Eqn. (C.13))
ˆ

2T
ď 16K 2 e6B d log 1 `
dλ

˙
.

(Lemma E.6)

28

On the other hand, for t P T w , we know that
ÿ
max
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1 ě
1
a,a PA

tPT w

t

|T w |
.
18K 2 βT `1 pδq2

By combining the two results above, and setting κ “ e´6 , we obtain
ˆ
˙
288K 4
2KT
|T w | ď
βT `1 pδq2 d log 1 `
,
κ
dλ
which concludes the proof.

D

Proof of Theorem 2

D.1

Main Proof of Theorem 2

In this section, we present the proof of Theorem 2, which is obtained by using the RB loss (4) instead
of the PL loss (3). Note that this approach is based on the concept of rank-breaking (RB), which
decomposes (partial) ranking data into individual pairwise comparisons, treats each comparison as
independent, and has been extensively studied in previous works [6, 34, 32, 67]. Moreover, this
RB approach is applied in the current RLHF for LLM (e.g., Ouyang et al. [57]) and is also studied
theoretically in Zhu et al. [92] under the offline setting.
RB loss and OMD. We begin by recalling the loss function and the parameter update rule. Specifically,
we use the PL loss defined in Equation (4) and update the parameter according to Equation (7).
˜
¸ |S |´1 |S |
`
˘
|Sÿ
t
t |´1 |S
ÿ
ÿt pj,kq
ÿt |
exp ϕpxt , σtj qJ θ
“
ℓt pθq.
ℓt pθq :“
´ log
exp pϕpxt , σtj qJ θq ` exp pϕpxt , σtk qJ θq
j“1 k“j`1
j“1 k“j`1 loooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon
pj,kq

“:ℓt

pθq

and
1
pj,kq pj,kq
pj,k`1q
pj,kq
“ argmin x∇ℓt pθpt q, θy ` }θ ´ θpt }2H̃ pj,kq ,
θpt
2η
t
θPΘ

1 ď j ă k ď |St |,

pj,k`1q
pj`1,j`2q
p|S |´1,|St |`1q
p1,2q
where if k “ |St |, we set θpt
“ θpt
, and for the final pair, let θpt t
“ θpt`1 .
1 1
1 1
ř
pj,kq
pj,kq
pj ,k q ppj ,k q 9
Also, the matrix H̃t
is defined as H̃t
:“ Ht ` η pj 1 ,k1 qďpj,kq ∇2 ℓt
p θt
q , where

Ht :“

t´1
s |´1 |S
s|
ÿ |Sÿ
ÿ

∇2 ℓpj,kq
pθpspj,k`1q q ` λId
s

s“1 j“1 k“j`1
t´1
s |´1 |S
s|
ÿ |Sÿ
ÿ

µ9

“

´`

¯
˘J
ϕpxt , σj q ´ ϕpxt , σk q θpspj,k`1q

s“1 j“1 k“j`1

`
˘`
˘J
¨ ϕpxt , σj q ´ ϕpxt , σk q ϕpxt , σj q ´ ϕpxt , σk q ` λId .
Online confidence bound for RB loss. Now, we introduce
˘ online confidence bound for RB loss.
řt ` the
Since the total number of updates up to round t is s“1 |S2s | , a modification of Lemma C.1 yields
the following result:
?
Corollary D.1?(Online confidence bound for RB loss). Let δ P p0, 1s. We set η “ p1 ` 3 2Bq{2 and
λ “ maxt12 2Bη, 144ηd, 2u. Then, under Assumption 1, with probability at least 1 ´ δ, we have
´ a
? ¯
pj,kq
}θpt
´ θ ‹ }H pj,kq ď βt pδq “ O B d logptK{δq ` B λ , @t ě 1, 1 ď j ă k ď |St |,
t

pj,kq

where Ht
9

:“ Ht `

1 1
1 1
p1,2q
2 pj ,k q ppj ,k `1q
pθt
q ` λId and θpt
“ θpt .
pj 1 ,k1 qăpj,kq ∇ ℓt

ř

We write pj 1 , k1 q ď pj, kq to indicate lexicographic order, i.e., j 1 ă j or j 1 “ j and k1 ď k.

29

Useful definitions. We define the set of warm-up rounds T w in a similar manner to the analysis of
Theorem 1: "
*
1
1
´1
T w :“ t P rT s : max
}ϕpx
,
aq
´
ϕpx
,
a
q}
ě
.
(warm-up rounds)
t
t
Ht
a,a1 PA
βT `1 pδq
Moreover, we define the matrix
´`
ÿ
ÿ
˘J ¯ `
˘`
˘J
µ9 ϕpxt , aq ´ ϕpxt , a1 q θps ϕpxt , aq ´ ϕpxt , a1 q ϕpxt , aq ´ ϕpxt , a1 q
Λt “
sPrt´1szT w a,a1 PSs

` λId
Key lemmas. The relationship between Ht and Λt is as follows:
Lemma D.1. Let Λt be defined as in Equation (D.1). Then, for all t P rT s, we have
1
Ht ľ 2 Λt .
2e

(D.1)

The proof is deferred to Appendix D.2.1.
The size of the set T w is bounded as described in the following lemma:
␣
(
1
Lemma D.2. Let T w “ t P rT s : maxa,a1 PA }ϕpxt , aq ´ ϕpxt , a1 q}H ´1 ě βT `1
pδq . Define
t
κ :“ e´6B . Set λ ě 1. Then, the size of the set T w is bounded as follows:
ˆ
˙
32
2T
|T w | ď
βT `1 pδq2 d log 1 `
.
κ
dλ
The proof is deferred to Appendix D.2.2. Note that the bound on the cardinality of T w is tighter than
in the PL loss case (Lemma C.4), by up to a factor of K 4 (and up to logarithmic factors in K).
We are now ready to provide the proof of Theorem 2.
Proof of Theorem 2. The overall proof structure is similar to that of Theorem 1. We start from
Equation (C.4), but apply Lemma D.2 instead of Lemma C.4. Then, with probability at least 1 ´ δ,
we have
”`
˘J ı
pT pxqq θ ‹
SubOptpT q “ Ex„ρ ϕ px, π ‹ pxqq ´ ϕ px, π
ˆ
˙˙
˙
ˆ
ˆ
T
B
1
2
βT `1 pδq d log 1 `
(Lemma D.2)
`O
ď Õ ?
κT
dλ
T
¯
˘J ´ ‹
1 ÿ `
pT pxt qq
`
θ ´ θpT `1 .
(D.2)
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
T tRT w
To further bound the last term of Equation (D.2), by following the similar reasoning from Equation (C.4) to Equation (C.6), with probability at least 1 ´ δ, we obtain
ÿ
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t

tRT w

ď ep1 ` eq2

ÿ
›
`
˘›
1
µ9 ψt pa, a1 qJ θpt ›ψt pa, a1 q›H ´1
‹
t
|St | a,a1 PS ‹
tRT w
ÿ

t

(St‹ :“ tπ ‹ pxt q, π
pT pxt qu, and let ψt pa, a1 q “ ϕpxt , aq ´ ϕpxt , a1 q)
g
g
fT
fÿ 1 f
`
˘
f ÿ |St | ÿ
2
2e
e
ď ep1 ` eq
µ9 ψt pa, a1 qJ θpt }ψt pa, a1 q}H ´1
‹
t
|S
|
|S
|
t
t a,a1 PS ‹
t“1
tRT w
t

(Cauchy-Schwartz ineq., and T w Ď rT s)
g
g
fT
fÿ 1 f
ÿ
`
˘
f ÿ 
|S
t|
2
2e
e
µ9 ψt pa, a1 qJ θpt }ψt pa, a1 q}H ´1
ď ep1 ` eq
t

|S
|
|S
|
t
t
1
w
t“1
tRT  a,a PSt
g
fT
fÿ 1 d ÿ ÿ
? 2
`
˘
2
2e
µ9 ψt pa, a1 qJ θpt }ψt pa, a1 q}Λ´1 . (Lemma D.1)
ď 2e p1 ` eq
t
|S
|
t
t“1
tRT w a,a1 PS
t

30

2

Moreover, by setting λ “ d log T and T ě eK {d , for any t, we have
›2
`
˘›
2K 2
µ9 ψt pa, a1 qJ θpt ›ψt pa, a1 q›Λ´1 ď
ď 2.
t
λ
a,a1 PS
ÿ

(D.3)

t

Hence, by the elliptical potential lemma (Lemma E.6), we derive that
¯
˘J ´ ‹
1 ÿ `
pT pxt qq
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
θ ´ θpT `1
T tRT w
βT `1 pδq ÿ
}ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
pT pxt qq}H ´1
t
T
w
tRT
g
˛
¨
fT
fÿ 1 a
β
pδq
T
`1
e
“ O˝
¨ d log pKT q‚.
T
|S
|
t
t“1

ď

(D.4)

? ˘
` a
By plugging Equation (D.4) into Equation (D.2) and setting βT `1 pδq “ O B d logpKT q ` B λ ,
then, with probability at least 1 ´ 2δ, we derive that
˛
¨ g
fT
2
f
ÿ
d ‚
d
1
`
.
SubOptpT q “ Õ ˝ e
T t“1 |St | κT
Substituting δ Ð 2δ , we conclude the proof of Theorem 2.
D.2

Proofs of Lemmas for Theorem 2

D.2.1

Proof of Lemma D.1

Proof of Lemma D.1. Recall that, under the Bradley–Terry-Luce (BTL) model defined in Equation (2), the probability that action a is preferred over action a1 is given by:
`
˘
´`
`
˘
˘ ¯
exp ϕpxt , aqJ θ
1 J
1
ϕpx
,
aq
´
ϕpx
,
a
q
θ .
P a ą a |xt , ; θ “
“
µ
t
t
exp pϕpxt , aqJ θq ` exp pϕpxt , a1 qJ θq
Then, we can derive a lower bound on the matrix Ht as follows:
Ht “

t´1
s |´1 |S
s|
ÿ |Sÿ
ÿ

∇2 ℓpj,kq
pθpspj,k`1q q ` λId
s

s“1 j“1 k“j`1
|Sÿ
s|
s |´1 |S
ÿ

ÿ
ľ

∇2 ℓpj,kq
pθpspj,k`1q q ` λId
s

sPrt´1szT w j“1 k“j`1
|Sÿ
s |´1 |S
s|
ÿ

ÿ
“

¯
´
J ppj,k`1q
J
µ9 zsjk
θs
zsjk zsjk
` λId

sPrt´1szT w j“1 k“j`1
|Sÿ
s |´1 |S
s|
ÿ

ÿ
ľ

´
¯ ˇˇ J ` ppj,k`1q p ˘ˇˇ
´ˇz
θ
´θs ˇ
J p
J
µ9 zsjk
θs e sjk s
zsjk zsjk
` λId

sPrt´1szT w j“1 k“j`1

ľ e´2

ÿ

|Sÿ
s |´1 |S
s|
ÿ

´
¯
J p
J
µ9 zsjk
θs zsjk zsjk
` λId

sPrt´1szT w j“1 k“j`1

ľ

e

´2

2

Λt ,

31

(Lemma E.2)

where the second-to-last inequality holds because, for any s R T w , the following property is satisfied:
ˇ
ˇ ˇ
¯ˇ
˘J ´ pj,k`1q
ˇ J ` ppj,k`1q p ˘ˇ ˇ`
ˇ
´ θs ˇ “ ˇ ϕpxs , σsj q ´ ϕpxs , σsk q
θps
´ θps ˇ
ˇzsjk θs
´›
›
›
›
›
› ¯
ď ›ϕpxs , σsj q ´ ϕpxs , σsk q›H ´1 ›θpspj,k`1q ´ θ ‹ ›Hs ` ›θps ´ θ ‹ ›Hs
s

ď

1
βT `1 pδq

(Hölder’s inequality)
¯
´›
›
›
›
›θpspj,k`1q ´ θ ‹ › pj,k`1q ` ›θps ´ θ ‹ ›
Hs
H
s

pj,k`1q

(s ‰ T w , Hs ĺ Hs
2βT `1 pδq
ď
βT `1 pδq
ď 2.

)

(Corollary D.1 and βt pδq is non-decreasing)

This concludes the proof of Lemma D.1.
D.2.2

Proof of Lemma D.2

Proof of Lemma D.2. The proof follows the same structure as the analysis of Lemma C.4, with the
main difference that we use the Hessian corresponding to the RB loss. Let
´`
˘J ¯
pãt , āt q “ argmax µ9 ϕpxt , aq ´ ϕpxt , a1 q θpt }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1 .
t

a,a1 PA

By the assortment selection rule in (11), the two actions ãt , āt are always included in St . It is because,
for any pair ta, a1 u, we have
´`
˘J ¯
µ9 ϕpxt , aq ´ ϕpxt , a1 q θpt }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1 “ 2ft pta, a1 uq,
t

which directly implies that ãt , āt P St .
For simplicity, we denote zskk1 “ ϕpxs , σsk q ´ ϕpxs , σsk1 q. Recall that by the definition of Ht ,
Ht “

t´1
s |´1 |S
s|
ÿ |Sÿ
ÿ

∇2 ℓpj,kq
pθpspj,k`1q q ` λId
s

s“1 j“1 k“j`1

ľ

t´1 |Ss |´1 |Ss |
e´4B ÿ ÿ ÿ
J
zsjk zsjk
` λId
4 s“1 j“1 k“j`1
t´1

˘`
˘J
e´4B ÿ `
ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q ` λId
( ãs , ās P Ss )
4 s“1
˜
ÿ ´`
˘`
˘J
˘J ¯ `
e´4B
µ9 ϕpxs , ãs q ´ ϕpxs , ās q θps ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q
ľ
4
sPTtw
¸

ľ

` λId ,

(D.5)

9 ď 14 and let Ttw “ T w ztt, . . . , T u. Define
where in the last inequality, we use the fact that µp¨q
ÿ ´`
˘J ¯ `
˘`
˘J
µ9 ϕpxs , ãs q ´ ϕpxs , ās q θps ϕpxs , ãs q ´ ϕpxs , ās q ϕpxs , ãs q ´ ϕpxs , ās q
Λw
t :“
sPTtw

` λId .
Furthermore, by setting λ ě 1, for any t, the following holds
´`
˘J ¯
1 2
µ9 ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2` w ˘´1 ď ¨
4 λ
Λt
ď 1.
32

9 ď 41 )
(µp¨q
(D.6)

Then, we obtain
ÿ
max
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
1
tPT w

a,a PA

t

˘J ¯
θpt
´`
¯ }ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
max
“
˘
Jp
t
a,a1 PA
1
w
µ9 ϕpxt , aq ´ ϕpxt , a q θt
tPT
´`
¯
ÿ
˘
1 Jp
µ
9
ď 4e2B
max
ϕpx
,
aq
´
ϕpx
,
a
q
θ
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1
t
t
t
1
µ9

ÿ

´`

ϕpxt , aq ´ ϕpxt , a1 q

a,a PA

tPT w

t

(Assumption 1)
ÿ

“ 4e2B

µ9

´`

˘J ¯
ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2H ´1
t

tPT w

ď 16e6B

ÿ

µ9

´`

¯

˘J
ϕpxt , ãt q ´ ϕpxt , āt q θpt }ϕpxt , ãt q ´ ϕpxt , āt q}2` w ˘´1 (Eqn. (D.5))
Λt

tPT w

“ 16e6B

ÿ

"
*
´`
˘J ¯
2
p
˘
`
min 1, µ9 ϕpxt , ãt q ´ ϕpxt , āt q θt }ϕpxt , ãt q ´ ϕpxt , āt q} w ´1
Λt

tPT w

(Eqn. (D.6))
ˆ

2T
dλ

ď 32e6B d log 1 `

˙
.

(Lemma E.6)

On the other hand, for t P T w , we know that
ÿ
max
}ϕpxt , aq ´ ϕpxt , a1 q}2H ´1 ě
1
tPT w

a,a PA

t

|T w |
.
βT `1 pδq2

By combining the two results above, and setting κ “ e´6 , we derive that
ˆ
˙
2T
32
2
w
βT `1 pδq d log 1 `
,
|T | ď
κ
dλ
which concludes the proof.

E

Technical Lemmas

Lemma E.1 (Proposition B.5 of Lee and Oh 41). The Hessian of the multinomial logistic loss
ℓ̄ : RM Ñ R satisfies that, for any a1 , a2 P RM , we have:
?

?

e´3 2}a1 ´a2 }8 ∇2 ℓ̄pa1 q ĺ ∇2 ℓ̄pa2 q ĺ e3 2}a1 ´a2 }8 ∇2 ℓ̄pa1 q.
Lemma E.2 (Lemma 9 of Abeille et al. 2). Let f be a strictly increasing function such that |f:| ď f9,
and let Z be any bounded interval of R. Then, for all z1 , z2 P Z, we have
f9pz2 q exp p´|z2 ´ z1 |q ď f9pz1 q ď f9pz2 q exp p|z2 ´ z1 |q .
We also provide a concentration lemma for positive semi-definite (PSD) random matrices.
Lemma E.3 (Concentration of PSD matrices). Let µi denote the conditional distribution of a
positive semi-definite
M P Rdˆd conditioned on the filtration Fi´1 . Assume λmax pM q ď 1. Define
řn
1
Ď
M :“ n i“1 EM „µi M . If λ “ Ωpd logpn{δqq, then with probability at least 1 ´ δ, for any n ě 1,
n
˘ ÿ
˘
1` Ď
5` Ď
nM ` λId ĺ
Mi ` λId ĺ
nM ` λId .
3
3
i“1

Proof of Lemma E.3. The overall structure of the proof closely follows that of Lemma 39 in Zanette
et al. [88]. For completeness, we provide the full proof below.
Ďi “ EM „µ M and M
Ď “ 1 řn M
Ďi . Then, we have
Fix x P Rd such that }x}2 “ 1. Let M
i

J

J

EM „µi x M x “ x

n

i“1

Ďi x.
EM „µi M x “ xJ M
33

Since M is a positive semi-definite matrix, the random variable xJ M x is non-negative, and it satisfies
Ďi x because
xJ M x ď λmax pM q}x}22 ď 1. Thus, the conditional variance is at most xJ M
J
J
2
J
Ďi x.
VarM „µ px M xq ď EM „µ px M xq ď EM „µ x M x “ xJ M
i

i

i

Applying Lemma E.4 with the filtration Fi , we obtain that, with probability at least 1 ´ δ, there exists
aˇ universal constant c such that
ˇ ˇ
ˇ
˜c
¸
n
n
ˇ
ˇ1 ÿ
Ďx
` J
˘ˇˇ ˇˇ 1 ÿ
2xJ M
logp2{δq
ˇ
JĎ
J
JĎ ˇ
.
x Mi x ´ x Mi x ˇ “ ˇ
x Mi x ´ x M x ˇ ď c
logp2{δq `
ˇ
ˇ ˇ n i“1
ˇ
ˇ n i“1
n
3n
Now, we will show that if λ “ Ωplogp1{δqq, we can derive
˜c
¸
ˆ
˙
Ďx
2xJ M
1
logp2{δq
λ
JĎ
c
ď
logp2{δq `
x Mx `
.
n
3n
2
n

(E.1)

Ďx ď λ .
Case 1. xJ M
n
In this case, it is sufficient to satisfy for some constants c1 , c2
c
c
2 logp2{δq
λ
1
ďc
ÐÑ Ωplogp1{δqq ď λ
n
n
ˆ ˙
logp2{δq
λ
ÐÑ Ωplogp1{δqq ď λ.
ď c2
3n
n
Ďx ą λ .
Case 2. xJ M
n
In this case, it is sufficient to satisfy for some constants c3 , c3
c
ˆ ˙
Ďx
2xJ M
λ
3
logp2{δq ď c
ÐÑ Ωplogp1{δqq ď λ
n
n
ˆ ˙
logp2{δq
λ
ď c4
ÐÑ Ωplogp1{δqq ď λ.
3n
n
Therefore, Equation (E.1) is satisfied. Since }x}2 ď 1, this implies
ˇ ˜
¸ ˇ
˙
n
ˇ
ˇ 1 ˆ
ˇ J 1 ÿ
ˇ
Ď
Ď ` λ Id x.
Mi ´ M x ˇ ď x J M
ˇx
ˇ
ˇ 2
n i“1
n

(E.2)

We denote the boundary of the unit ball by BB “ t}x}2 “ 1u. Then, for any x P BB, we know there
exists a x1 in the ϵ-covering such that }x ´ x1 }2 ď ϵ. Let Nϵ be the ϵ-covering number of BB. Then,
by the covering number of Euclidean ball lemma (Lemma E.5), we get
ˆ ˙d
3
Nϵ ď
.
(E.3)
ϵ
Taking a union bound over x1 and the number of samples n, with probability at least 1 ´ nNϵ δ, we
obtain
ˇ ˜
¸ ˇ ˇ
˜
¸ ˇ ˇ
˜
¸ ˇ
n
n
n
ˇ ˇ
ˇ ˇ
ˇ
ˇ
ÿ
ÿ
1
1
ˇ
ˇ
ˇ
ˇ
ˇ J 1 ÿ
1
J
1
1
J
Ď xˇ ď ˇpx q
Ď x ˇ ` ˇpx ´ x q
Ď x1 ˇˇ
Mi ´ M
Mi ´ M
Mi ´ M
ˇx
ˇ ˇ
ˇ ˇ
ˇ
ˇ
n i“1
n i“1
n i“1
ˇ
ˇ
˜
¸
n
ˇ
ˇ
1 ÿ
ˇ
Ď px ´ x1 qˇˇ
` ˇpx1 qJ
Mi ´ M
ˇ
ˇ
n i“1
ˇ
˜
¸ ˇ
n
ˇ
ˇ
1 ÿ
ˇ
Ď x1 ˇˇ ` 4ϵ.
ď ˇpx1 qJ
Mi ´ M
ˇ
ˇ
n i“1
Ď}2 ď 1)
(}x ´ x1 }2 ď ϵ and Mi , }M
ˆ
˙
1
Ď ` λ Id x1 ` 4ϵ
(Eqn. (E.2))
ď px1 qJ M
2
n
ˆ
˙
1
Ď ` λ Id x ` 9 ϵ
Ď}2 ď 1)
ď xJ M
(}x ´ x1 }2 ď ϵ and }M
2
n
2
ˆ
˙
2
Ď ` λ Id x,
ď xJ M
(set ϵ “ Op n1 q)
3
n
34

˘˘
` `
ϵ
where λ “ Ω log 2nN
. By substituting δ Ð δ{pnNϵ ` 1q and combining this with Equaδ
tion (E.3), we obtain:
ˆ
ˆ
˙
˙
n
1 Ď λ
λ
5 Ď λ
1 ÿ
M ` Id ĺ
M ` Id ,
Mi ` Id ĺ
3
n
n i“1
n
3
n
which concludes the proof.
Lemma E.4 (Bernstein for martingales, Theorem 1 of Beygelzimer et al. 10 and Lemma 45 of Zanette
et al. 88). Consider the stochastic process tXn u adapted to the filtration tFn u. Assume EXn “ 0
and cXn ď 1 for every n; then for every constant z ‰ 0 it holds that
˜
¸
N
N
ÿ
ÿ
1
1
2
Pr
Xn ď z
EpXn | Fn q ` log
ě 1 ´ δ.
z
δ
n“1
n“1
By optimizing the bound as a function of z, we also have
g
¨
˛
fN
N
fÿ
ÿ
1
1
Pr ˝
Xn ď ce
EpXn2 | Fn q log ` log ‚ ě 1 ´ δ.
δ
δ
n“1
n“1
Lemma E.5 (Covering number of Euclidean ball). For any ϵ ą 0, the ϵ-covering number of the
Euclidean ball in Rd with radius R ą 0 is upper bounded by p1 ` 2R{ϵq2 .
Lemma E.6 (Elliptical potential lemma Abbasi-Yadkori et al. 1). Let tzt utě1 be a bounded sequence
řt´1
in Rd satisfying maxtě1 }zt }2 ď X. For any t ě 1, we define Λt :“ s“1 zs zsJ ` λId with λ ą 0.
Then, we have
˙
ˆ
T
!
)
ÿ
X 2T
.
min 1, }zt }2Λ´1 ď 2d log 1 `
t
dλ
t“1

F

Proof of Theorem 3

F.1

Main Proof of Theorem 3

Throughout the proof, we consider the setting where the context space is a singleton, i.e., X “ txu.
As a result, the problem reduces to a context-free setting, and we focus solely on the action space A.
Note that this is equivalent to assuming that ρ is a Dirac distribution.
We first present the following theorem, which serves as the foundation for our analysis.
d´1
Theorem F.1 (Lower bound on adaptive PL model parameter
be the unit
? estimation). Let Φ “ S
d
d
sphere in R , and let Θ “ t´µ, µu for some µ P p0, 1{ ds. We consider a query model where, at
each round t “ 1, . . . , T , the learner selects a subset St Ď Φ of feature vectors, with cardinality
satisfying 2 ď |St | ď K, and then receives a ranking feedback σt drawn from the Plackett–Luce (PL)
model defined as:
´
¯
|St |
exp ϕJ
θ
ź
σtj
Ppσt |St ; θq “
`
˘,
ř|St |
J
j“1
k“j exp ϕσtk θ
where σt “ pσt1 , . . . , σt|St | q is a permutation of the actions in St , ϕa P Φ denotes the feature vector
associated with action a P A in the selected subset at round t, and θ P Θ. Then, we have
˜
¸
c
”›
2 T µ2
›2 ı dµ2
2K
inf max Eθ ›θ ´ θp›2 ě
1´
,
θPΘ
p
2
d
θ,π
where the infimum is over all measurable estimators θp and measurable (but possibly adaptive) query
rules π, and Eθ r¨s denotes the expectation over the randomness in the observations and decision
a
d2
rules if θ is the true instance. In particular, if T ě 8K
d{p8K 2 T q, we obtain
2 , by choosing µ “
”›
›2 ı
d2
inf max Eθ ›θ ´ θp›2 ě
.
θPΘ
p
32K 2 T
θ,π
35

Proof of Theorem F.1. The analysis of this result closely follows the proof of Theorem 3 in Shamir
[73]. The key distinction lies in the input structure: our setting involves a set of feature vectors, while
theirs is restricted to a single feature vector.
To begin with, since the worst-case expected regret with respect to θ can be lower bounded by the
average regret under the uniform prior over Θ, we have:
”›
”›
›2 ı
›2 ı
max Eθ ›θ ´ θp›2 ě Eθ„UnifpΘq Eθ ›θ ´ θp›2
θPΘ
«
ff
d ´
¯2
ÿ
“ Eθ„UnifpΘq Eθ
θ ´ θp
i“1

«
2

ě µ ¨ Eθ„UnifpΘq Eθ

ff
d
!
)
ÿ
p
I θi θi ă 0 .

(F.1)

i“1

As in Shamir [73], we assume that the query strategy is deterministic conditioned on the past: that is,
St is a deterministic function of the previous queries and observations, i.e., S1 , σ1 , . . . , St´1 , σt´1 .
This assumption is made without loss of generality, since any randomized querying strategy can be
viewed as a distribution over deterministic strategies. Therefore, a lower bound that holds uniformly
for all deterministic strategies also applies to any randomized strategy. Then, we use the following
lemma.
Lemma F.1 (Lemma 4 of Shamir 73). Let θ be a random vector, none of whose coordinates is
supported on 0, and let y1 , y2 , . . . , yT be a sequence of queries obtained by a deterministic strategy
returning a point θp (that is, ψt is a deterministic function of ψ1 , y1 , . . . , ψt´1 , yt´1 , and θp is a
deterministic function of y1 , . . . , yT ). Then, we have
g
˛
¨
ff
«
f d T
d
!
)
f1 ÿ ÿ
ÿ
d
Eθ„UnifpΘq Eθ
I θi θpi ă 0 ě ˝1 ´ e
Uti ‚,
2
d i“1 t“1
i“1
where
˘˘
˘
`
` `
t´1
.
Uti :“ sup DKL P yt |θi ą 0, tθj uj‰i , tys ut´1
s“1 } P yt |θi ă 0, tθj uj‰i , tys us“1
θj ,j‰i

In our setting, we interpret yt “ σt , and ψt “ tϕa uaPSt Ď Φ. Then, we can write Uti as follows:
Uti “ sup DKL pP pσt |St ; θi ą 0, tθj uj‰i , q } P pσt |St ; θi ă 0, tθj uj‰i , qq .
θj ,j‰i

For simplicity, let Pθ pσ|Sq “ P pσ|S; θq. Then, we can upper bound Uti using the following lemma.
Lemma F.2. For any θ, θ 1 P Rd , let Pθ p¨ | Sq denote the PL distribution over rankings induced by
the action set S and parameter vector θ. Then, we have
˘2
`
˘ K ÿ` J 1
DKL Pθ p¨|Sq}Pθ1 p¨|Sq ď
ϕa pθ ´ θq .
2 aPS
The proof is deferred to Appendix F.2.1.
By applying Lemma F.2, we have
d
ÿ
i“1

Uti ď

d
d
ÿ ÿ
K ÿ ÿ
2
p2µ ¨ rϕa si q “ 2Kµ2
prϕa si q2
2 i“1 aPS
aPSt i“1
t
looooomooooon
“1
2

“ 2Kµ ¨ |St |

(ϕa P S d´1 )

ď 2K 2 µ2 .

(|St | ď K)
36

Hence, by Lemma F.1, we get
g
¨
˛
ff
f d T
d
!
)
f
ÿ
ÿ
ÿ
1
d
Uti ‚
I θi θpi ă 0 ě ˝1 ´ e
2
d
i“1 t“1
i“1
˜
¸
c
d
2K 2 T µ2
1´
.
ě
2
d

«
Eθ„UnifpΘq Eθ

(F.2)

Combining Equation (F.1) and (F.2), we prove
a the first inequality of Theorem F.1. The second
inequality directly follows by choosing µ “ d{p8K 2 T q.
We are now ready to present the proof of Theorem 3.
Proof of Theorem 3. The structure of our proof is similar to that of Theorem 2 in Wagenmaker et al.
[77]. However, while they consider the linear bandit setting, we focus on the Plackett–Luce (PL)
bandit setting.
We adopt the same instance construction as in Theorem F.1, where Φ “ S d´1 and Θ “ t´µ, µud .
‹
Define ϕ‹ pθq “ argmaxaPA ϕJ
a θ. Then, since ϕ pθq P Φ and θ P Φ, it is clear that
?
?
(F.3)
ϕ‹ pθq “ θ{}θ}2 “ θ{p dµq, and ϕ‹ pθqJ θ “ dµ.
Fix the suboptimality gap ϵ ą 0. By definition, a policy π P △Φ is said to be ϵ-optimal if it satisfies
?
“
‰ `
˘J
Eϕ„π ϕJ θ “ E
θ ě ϕ‹ pθqJ θ ´ ϵ “ dµ ´ ϵ.
(F.4)
ϕ„π rϕs
looomooon
“:ϕπ

Moreover, by Jensen’s inequality, we have
“
‰
}ϕπ }22 ď Eϕ„π }ϕ}22 “ 1.
Let ∆ “ ϕπ ´ ϕ‹ pθq. Then, we get
1 ě }ϕπ }22 “ }ϕ‹ pθq ` ∆}22 “ 1 ` }∆}22 ` 2ϕ‹ pθqJ ∆
1
ðñ ϕ‹ pθqJ ∆ ď ´ }∆}22
? 2
dµ
}∆}22 .
ðñ θ ‹ ∆ ď ´
(Eqn. (F.3))
2
Hence, if a policy π is ϵ-optimal for a parameter θ, then the following bound holds:
?
dµ
}∆}22 .
(Eqn. (F.4))
´ϵď´
2
?
2ϵ
ðñ }∆}22 ď ? , where θ “ dµpϕπ ´ ∆q.
dµ
p. Define ϕp :“ ϕπp and the following estimator
We now assume that we are given an ϵ-optimal policy π
#
?
θ1
if Dθ 1 P Θ with θ 1 “ dµpϕp ´ ∆1 q for some ∆1 P Rd , }∆1 }22 ď ?2ϵdµ ;
θp “
1
any θ P Θ
otherwise.
p is indeed ϵ-optimal for some θ P Θ, then the first condition is satisfied, and we have:
If π
d
b ?
?
?
›
›
›?
›
›θp ´ θ › “ › dµpϕp ´ ∆1 q ´ dµpϕp ´ ∆q› ď 2 dµ ?2ϵ “ 8 dµϵ.
2
2
dµ

(F.5)

p is ϵ-optimal for θ P Θ. Then, we get
We denote E as the event that π
”›
”›
ı
›2 ı
›2
›
›2
Eθ ›θp ´ θ ›2 “ Eθ ›θp ´ θ ›2 ¨ ItEu ` ›θp ´ θ ›2 ¨ ItE c u
”›
ı
?
›2
ď 8 dµϵ ` Eθ ›θp ´ θ ›2 ¨ ItE c u
(Eqn. (F.5))
?
p 2 , }θ}2 u ď dµ2 )
ď 8 dµϵ ` 2dµ2 ¨ Pθ rE c s.
(maxt}θ}
2
2
37

On the other a
hand, by Theorem F.1, there exists a parameter θ P Θ such that, if we collect T samples
and set µ “ d{p8K 2 T q, then the following lower bound holds:
”›
›2 ı
d2
Eθ ›θp ´ θ ›2 ě
.
32K 2 T
To satisfy both inequalities, we require:
?
d2
2 2dϵ
d2
?
`
¨ Pθ rE c s ě
2
4K T
32K 2 T
K 2T
? ?
1 4 2K T ϵ
ðñ Pθ rE c s ě ´
.
8
d
It follows that if
? ?
1 4 2K T ϵ
0.0252
d2
´
ě 0.1 ðñ
¨ 2 2 ě T,
8
d
32
K ϵ
then we have that Pθ rE c s ě 0.1. In words, this means that with constant probability, any algorithm
2
must either collect more than c ¨ Kd2 ϵ2 samples, or output a policy that is not ϵ-optimal. This implies
2
that T “ Ωp Kd2 ϵ2 q samples are necessary to guarantee an ϵ-optimal policy. Equivalently, after T
rounds, the suboptimality gap ϵ is lower bounded as
ˆ
˙
d
?
SubOptpT q “ Ω
.
K T
This concludes the proof of Theorem 3.
F.2
F.2.1

Proof of Lemmas for Theorem 3
Proof of Lemma F.2

Proof of Lemma F.2. By the definition of KL divergence, we have
»
¨
ř|S| ϕJσ θ ˛fi
|S|
ÿ
k
`
˘
`
˘
k“j e
1
˝ϕJ
‚fl .
DKL Pθ p¨|Sq}Pθ1 p¨|Sq “ Eσ„Pθ p¨|Sq –
σj θ ´ θ ´ log ř|S|
J
ϕσ θ 1
k
e
j“1
k“j
Fix a stage j and a ranking σ. We define
¯
´
θ
exp ϕJ
σk 1
pk1 pθq :“ ř|S|
`
˘,
J
k“j exp ϕσk θ

(F.6)

where k 1 P tj, . . . , |S|u,

which corresponds to the Multinomial Logit (MNL) probability of selecting action σk1 at position j,
given the parameter θ and the choice set S. Moreover, we define
¨
˛
|S|
ÿ
J
f pθq :“ log ˝
eϕσk θ ‚.
k“j

Then, by applying the mean value form of Taylor’s theorem, there exists θ̄ “ p1 ´ cqθ ` cθ 1 for
some c P p0, 1q such that
ř|S| ϕJσ θ
k
k“j e
´ log ř|S| ϕJ θ1 “ f pθ 1 q ´ f pθq
σk
k“j e
`
˘ 1` 1
˘J
`
˘
θ ´ θ ∇2θ f pθ̄q θ 1 ´ θ (Taylor’s theorem)
“ ∇θ f pθqJ θ 1 ´ θ `
2
|S|
`
˘ 1 ÿ
`
˘2
1
ď ∇θ f pθqJ θ 1 ´ θ `
pk pθ̄q ϕJ
σk pθ ´ θq
2 k“j
|S|
ÿ

ď

` 1
˘ 1 ÿ` J 1
˘2
pk pθqϕJ
ϕa pθ ´ θq ,
σk θ ´ θ `
2 aPS
k“j
38

(F.7)

where the first inequality holds because
¨
˛¨
˛J
|S|
|S|
|S|
|S|
ÿ
ÿ
ÿ
ÿ
˝
∇2θ f pθ̄q “
pk pθ̄qϕσk ϕJ
pk pθ̄qϕσk ‚˝
pk pθ̄qϕσk ‚ ĺ
pk pθ̄qϕσk ϕJ
σk ´
σk .
k“j

k“j

k“j

k“j

Plugging Equation (F.7) into Equation (F.6), we get
`
˘
DKL Pθ p¨|Sq}Pθ1 p¨|Sq
« |S| ˆ
˙ff
|S|
ÿ`
ÿ
ÿ
˘
˘
`
`
˘
1
2
1
1
1
ϕJ
pk pθqϕJ
ď Eσ„Pθ p¨|Sq
ϕJ
a pθ ´ θq
σk θ ´ θ `
σj θ ´ θ ´
2
j“1
aPS
k“j
« |S|
ȷ ff
„
|S|
ÿ
˘ˇ
`
˘ ÿ
`
1 ˇ
J
1
J
pk pθqϕσk θ ´ θ ˇ σ1 , . . . , σj´1
Eσj ϕσj θ ´ θ ´
“ Eσ„Pθ p¨|Sq
j“1 loooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooon
k“j
“0

(Tower rule)
`
ď

|S| ÿ `
2 aPS
K ÿ`
2 aPS

1
ϕJ
a pθ ´ θq

1
ϕJ
a pθ ´ θq

˘2

˘2

,

(|S| ď K)

which concludes the proof.

G

Additional Discussions

This section provides further discussion of our approach. In Subsection G.1, we show how it extends
to the regret-minimization setting and improves the existing OpeB q dependence. In Subsection G.2,
we describe how the approach also applies to the active-learning setting considered in [19].
G.1

Avoiding eB Scaling in Regret Minimization

In this subsection, we show that our technique for eliminating the OpeB q dependence in the leading
term—primarily by dividing the total rounds into warm-up and non–warm-up phases—can also be
applied in the regret-minimization setting (e.g., Bengs et al. 9).
First, we formally define the cumulative regret as:
RegT :“

T ´
ÿ

¯
rθ‹ pxt , π ‹ pxt qq ´ max rθ‹ pxt , aq ,
aPSt

t“1

where the contexts xt can be given arbitrarily (in contrast to the main paper, where they are drawn
from a fixed distribution). We assume the linear rewards (Assumption 1) as in the main paper.
Moreover, we use the RB update rule (Procedure 2) in this subsection.
Next, we define
gt pSq :“

¯
1 ÿ ´
2
µ9 pϕpxt , aq ´ ϕpxt , p
at qqJ θpt }ϕpxt , aq ´ ϕpxt , p
at q}H ´1 ,
t
|S| aPS

at :“ argmaxaPA ϕpxt , aqJ θpt . The quantity measures the average uncertainty between the
where p
at .
feature vectors of actions in S and that of the current best action p
We then select the assortment St in a greedy manner, following the procedure in Equation (11). We
initialize S “ tp
at u, and subsequently add actions one by one. Specifically, at each step, we select
a‹ P argmax ∆t pa | Sq,

where ∆t pa | Sq :“ gt pS Y tauq ´ gt pSq,

(G.1)

aPAzS

and include a‹ in S if ∆t pa | Sq ě 0. The process continues until either |S| “ K or no action yields
a non-negative gain. Since gt ptp
at uq “ 0, this rule guarantees that the selected set satisfies |S| ě 2.
39

Theorem G.1 (Regret upper bound). In the same setting as Theorem 2, suppose T ě eK{d . Then,
with probability at least 1 ´ δ, the assortment selection rule in Equation (G.1), combined with the
RB loss update strategy (Procedure 2), guarantees
¨ g
˛
fT
2
fÿ 1
d
RegT “ Õ ˝de
` ‚.
|St |
κ
t“1

Proof of Theorem G.1. By the definition of the cumulative regret, we have

RegT :“

T ˆ
ÿ

˙J
θ‹
ϕpxt , π pxt qq ´ max ϕpxt , aq
‹

aPSt

t“1
T
ÿ

ď

J

pϕpxt , π ‹ pxt qq ´ ϕpxt , p
at qq θ ‹

at P St )
(p

t“1
T
ÿ

ď

pϕpxt , π ‹ pxt qq ´ ϕpxt , p
at qq

J

´
¯
θ ‹ ´ θpt .

(p
at :“ argmaxaPA ϕpxt , aqJ θpt )

t“1

␣
Recall the definition of the
set of warm-up rounds, i.e., T w “ t P rT s : maxa,a1 PA }ϕpxt , aq ´
(
ϕpxt , a1 q}H ´1 ě 1{βT pδq . Then, we get
t

RegT ď 4B|T w | `

ÿ

pϕpxt , π ‹ pxt qq ´ ϕpxt , p
at qq

J

´

θ ‹ ´ θpt

¯

tRT w

ˆ
ďO

ˆ
ďO

ˆ
˙˙
›
›
ÿ
T
B
›
›
2
}ϕpxt , π ‹ pxt qq ´ ϕpxt , p
at q}H ´1 ›θ ‹ ´ θpt ›
βT `1 pδq d log 1 `
`
t
κ
dλ
Ht
w
tRT
(Lemma D.2 and Cauchy-Schwartz ineq.)
ˆ
˙˙
ÿ
B
T
βT `1 pδq2 d log 1 `
` βT pδq
}ϕpxt , π ‹ pxt qq ´ ϕpxt , p
at q}H ´1 .
t
κ
dλ
w
tRT
(Corollary D.1)

For simplify the presentation, let ψt pa, a1 q “ ϕpxt , aq ´ ϕpxt , a1 q. Then, for t R T w , we have
´
1
“
µ9 pψt pπ ‹ pxt q, p
at qJ θ ‹ q

‹

J

1 ` eψt pπ pxt q,pat q θ

‹

¯2

eψt pπ‹ pxt q,pat qJ θ‹
¯
´
‹
J ‹ 2
ď 1 ` eψt pπ pxt q,pat q θ
(ψt pπ ‹ pxt q, p
at qJ θ ‹ ě 0)
´
¯2
‹
J
‹
p
ď 1 ` eψt pπ pxt q,pat q pθ ´θt q
(p
at :“ argmaxaPA ϕpxt , aqJ θpt )
ˆ
˙2
}ψt pπ ‹ pxt q,p
at qJ } ´1 }θ ‹ ´θpt }Ht
Ht
ď 1`e
ˆ

ď

βt pδq

1 ` e βT pδq

˙2
ď p1 ` eq2 .

40

(t R T w and Corollary D.1)

Therefore, the same line of reasoning used in the proof of Theorem 2 applies here as well.
ÿ
at q}H ´1
}ϕpxt , π ‹ pxt qq ´ ϕpxt , p
t

tRT w

˘
1 ÿ `
at q}H ´1
µ9 ψt pa, p
at qJ θpt }ψt pa, p
(St‹ :“ tπ ‹ pxt q, p
at u)
‹|
t
|S
‹
t
w
tRT
aPSt
g
g
fT
fÿ 1 f
˘
f ÿ |St | ÿ `
2
e
ď ep1 ` eq2 e
µ9 ψt pa, p
at qJ θpt }ψt pa, p
at q}H ´1
‹
t
|S
|
|S
|
t
‹
t
w
t“1
tRT
aPS
ď ep1 ` eq2

ÿ

t

(Cauchy-Schwartz ineq., and T w Ď rT s)
g
g
fT
fÿ 1 f
ÿ `
˘
f ÿ 
|S
t|
2
e
µ9 ψt pa, p
at qJ θpt }ψt pa, p
at q}H ´1
ď ep1 ` eq2 e
t

|S
|
|S |
t
t“1
tRT w t aPSt
g
fT
fÿ 1 d ÿ ÿ `
? 2
˘
2
2e
µ9 ψt pa, p
at qJ θpt }ψt pa, p
at q}Σ´1
ď 2e p1 ` eq
(Lemma D.1)
t
|S
|
t
t“1
tRT w aPSt
¨g
˛
fT
fÿ 1 a
“ O˝e
¨ d log pKT q‚,
(Lemma E.6)
|St |
t“1
´
¯
ř
ř
where, in the last inequality, we introduce Σt “ sPrt´1szT w aPSs µ9 ψt pa, p
at qJ θps ψt pa, p
at qψt pa, p
at qJ `
λId , and use the relation Ht ľ 2e12 Λt ľ 2e12 Σt (Λt is defined in Equation (D.1)). In the final
equality, we apply the fact that if λ “ d log T and T ě eK{d , then, for any t,
`
˘
2K
2
µ9 ψt pa, p
at qJ θpt }ψt pa, p
ď 2.
at q}Σ´1 ď
t
λ
a
?
`
˘
Therefore, using βT pδq “ O B d logpKT q ` B λ , we obtain, with probability at least 1 ´ δ,
¨ g
˛
fT
2
fÿ 1
d
RegT “ Õ ˝de
` ‚.
|S
|
κ
t
t“1
This completes the proof of Theorem G.1.
G.2

Extension to Active Learning Setting

In this subsection, we consider a different setting—referred to as the active learning setting—where
the learner has access to the entire context set X , and the objective is to minimize the following
worst-case suboptimality gap, defined as:
WorstSubOptpT q :“ max rrθ‹ px, π ‹ pxqq ´ rθ‹ px, π
ppxqqs .
xPX

This setting has received increasing attention in recent work [50, 47, 71, 19, 49, 76, 38]. However,
most existing approaches focus exclusively on pairwise preference feedback. Mukherjee et al. [49]
study an online learning-to-rank problem where, for each context, a fixed set of K actions is provided,
and the goal is to recover the true ranking based on feedback over these K actions. In contrast,
we consider a more general setting in which, for each context, a set of N actions is available. The
learner selects at most K actions from this set and receives ranking feedback over the selected subset.
Thekumparampil et al. [76] investigate the problem of ranking N ě K items using partial rankings
over K candidates, but under a context-free setting. In contrast, we study a stochastic contextual
setting, where contexts are drawn from an unknown (and fixed) distribution.
In the active learning setting, the algorithm jointly selects the context xt —which is no longer given
but actively chosen—and the assortment St by maximizing the average uncertainty objective. For
each candidate context x, it first constructs the assortment St pxq by solving Equation (11). It then
41

selects xt “ argmaxxPX ft pSt pxqq, and sets St “ St pxt q. The rest of the algorithm proceeds in the
same manner as Algorithm 3.
For simplicity, we focus on the RB loss and its corresponding update rule (Procedure 2). Then, we
can obtain the following bound on the worst-case suboptimality gap.
Theorem G.2. Under the same setting as Theorem 2, with probability at least 1 ´ δ, the selection
rule for pxt , St q described above achieves:
¨ g
˛
fT
2
fÿ 1
d
d
‚.
WorstSubOptpT q “ Õ ˝ e
`
T t“1 |St | κT
Proof of Theorem G.2. By the definition of the worst-case suboptimality gap, we have
”`
˘J ı
pT pxqq θ ‹
WorstSubOptpT q “ max ϕ px, π ‹ pxqq ´ ϕ px, π
xPX
”`
¯ı
˘J ´ ‹
ď max ϕ px, π ‹ pxqq ´ ϕ px, π
pT pxqq
θ ´ θpT `1
xPX

(p
πT pxq “ argmaxaPA ϕpx, aqJ θpT `1 )
T
”`
¯ı
˘J ´ ‹
1 ÿ
pT pxqq
max ϕ px, π ‹ pxqq ´ ϕ px, π
θ ´ θpT `1 .
“
T t“1 xPX
We adopt the same definitions for T w :“ tt P rT s : maxa,a1 PA }ϕpxt , aq ´ ϕpxt , a1 q}H ´1 ě
t
1{βT `1 pδqu. Thus, we get
T
¯ı
”`
˘J ´ ‹
1 ÿ
pT pxqq
θ ´ θpT `1
max ϕ px, π ‹ pxqq ´ ϕ px, π
T t“1 xPX
ˆ
ˆ
˙˙
B
T
2
ďO
βT `1 pδq d log 1 `
(Lemma D.2)
κT
dλ
´
¯ı
”
`
˘J ‹
1 ÿ
pT pxqq
θ ´ θpT `1 .
(G.2)
max ϕ px, π ‹ pxqq ´ ϕ px, π
`
T tRT w xPX

To further bound the last term of Equation (G.2), we get
”`
¯ı
˘J ´ ‹
1 ÿ
pT pxqq
max ϕ px, π ‹ pxqq ´ ϕ px, π
θ ´ θpT `1
T tRT w xPX
›
ı›
”
1 ÿ
›
›
max }ϕ px, π ‹ pxqq ´ ϕ px, π
pT pxqq}H ´1 ›θ ‹ ´ θpT `1 ›
ď
T
`1
xPX
T tRT w
HT `1
”
ı
ÿ
βT `1 pδq
pT pxqq}H ´1 .
ď
max }ϕ px, π ‹ pxqq ´ ϕ px, π
t
xPX
T
w
tRT

(Hölder’s ineq.)

(HT `1 ľ Ht and Corollary C.1, with prob. 1 ´ δ)
pT pxqq}H ´1 . It therefore suffices to bound
For simplicity, let x‹t “ argmaxx }ϕ px, π ‹ pxqq ´ ϕ px, π
t
}ϕ px‹t , π ‹ pxqq ´ ϕ px‹t , π
pT pxqq}H ´1 . From here, the same argument as in the proof of Theorem 2
t
applies, with the only modification that the procedure now selects both xt and St . This completes the
proof of Theorem G.2.

H

Experimental Details and Additional Results

H.1

Synthetic Data

Setup. In the synthetic data experiment, we sample the true but unknown parameter θ ‹ P Rd from
a d-dimensional standard normal distribution, i.e., θ ‹ „ N p0, Id q, and then normalize it to ensure
}θ ‹ }2 ď 1. We consider four different types of context sets X :
42

Instance 1, PL Loss, K=2

0.2
0.0

25

75

100 125 150 175 200

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

0.4
0.2
25

50

75

0.2

Round (t)

0.6

0.0

0.4

0.0

Instance 1, RB Loss, K=2

0.8

SubOpt. Gap

50

100 125 150 175 200

Round (t)

SubOpt. Gap

0.4

0.6

25

50

75

0.4
0.2
25

50

75

0.4
0.2
0.0

100 125 150 175 200

0.6

0.0

0.6

Round (t)

Instance 1, RB Loss, K=3

0.8

100 125 150 175 200

Round (t)

Instance 1, PL Loss, K=5

0.8

25

50

75

100 125 150 175 200

Round (t)

Instance 1, RB Loss, K=5

0.8

SubOpt. Gap

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

SubOpt. Gap

0.6

Instance 1, PL Loss, K=3

0.8

SubOpt. Gap

SubOpt. Gap

0.8

0.6
0.4
0.2
0.0

25

50

75

100 125 150 175 200

Round (t)

Figure G.1: Performance comparisons for Instance 1 (Stochastic contexts) with K “ 2, 3, and 5,
evaluated under the PL loss (first row) and RB loss (second row).
1. Instance 1 (Stochastic contexts): For each x P X , the feature vectors ϕpx, ¨q are sampled
from a standard normal distribution and then normalized to satisfy }ϕpx, ¨q}2 ď 1. Here,
|X | “ 100.
2. Instance 2 (Non-contextual): A single shared context is used for all rounds, i.e., X “ tx1 u
and |X | “ 1. The corresponding feature vectors ϕpx1 , ¨q are sampled from a standard
normal distribution and then normalized to satisfy }ϕpx1 , ¨q}2 ď 1.
3. Instance 3 (Hard-to-learn contexts): For each x P X , the feature vectors ϕpx, ¨q are
constructed such that most of them are approximately orthogonal to the true parameter θ ‹ .
Here, |X | “ 100.
4. Instance 4 (Skewed stochastic contexts): For each x P X , the feature vectors ϕpx, ¨q are
sampled in a skewed or biased manner and then normalized to satisfy }ϕpx, ¨q}2 ď 1. Here,
|X | “ 100. This is our main experimental setup in Section 6.1.
Additionally, we set the feature dimension to d “ 5 and the number of available actions to |A| “
N “ 100. The suboptimality gap is measured every 25 rounds. All results are averaged over 20
independent runs with different random seeds, and standard errors are reported to indicate variability.
The experiments are run on a Xeon(R) Gold 6226R CPU @ 2.90GHz (16 cores).
Baselines. We evaluate our proposed algorithm, M-AUPO, against three baselines: (i) DopeWolfe [76],
a method designed for non-contextual K-subset selection; (ii) Uniform, which selects assortments
of size K uniformly at random; and (iii) Best&Ref, which forms a pair of actions (|St | “ 2) by
combining one action from the current policy with another from a reference policy (e.g., uniform
random or SFT), following the setup in Online GSHF [85] and XPO [84].
When using the PL loss–based update in our algorithm, M-AUPO, the exact expectation over rankings
is computationally expensive. To mitigate this, we approximate the expectation via Monte Carlo
sampling with 5 samples. Consequently, the computational cost of the PL-based approach is higher
than that of the RB-based approach (see Table H.1).
Thekumparampil et al. [76] propose a D-optimal design approach for the Plackett-Luce objective
to efficiently select informative subsets of items for comparison. Recognizing the computational
complexity inherent in this method, they introduce a randomized Frank-Wolfe algorithm, named
DopeWolfe, which approximates the optimal design by solving linear maximization sub-problems
on randomly chosen variables. This approach reduces computational overhead while maintaining
effective learning performance. However, their approach is specifically tailored to the single-context
setting (e.g., Instance 2) and may not generalize well to the multiple-context scenarios (e.g., Instances
1, 3, and 4). While their original implementation updates the model parameters using a maximum
likelihood estimation (MLE) procedure, we instead adopt an online update strategy (as described in
43

Instance 2, PL Loss, K=2

0.2
0.0

25

75

100 125 150 175 200

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

0.4
0.2
25

50

75

0.2

Round (t)

0.6

0.0

0.4

0.0

Instance 2, RB Loss, K=2

0.8

SubOpt. Gap

50

100 125 150 175 200

Round (t)

SubOpt. Gap

0.4

0.6

25

50

75

0.4
0.2
25

50

75

0.4
0.2
0.0

100 125 150 175 200

0.6

0.0

0.6

Round (t)

Instance 2, RB Loss, K=3

0.8

100 125 150 175 200

Round (t)

Instance 2, PL Loss, K=5

0.8

25

50

75

100 125 150 175 200

Round (t)

Instance 2, RB Loss, K=5

0.8

SubOpt. Gap

0.6

Instance 2, PL Loss, K=3

0.8

SubOpt. Gap

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

SubOpt. Gap

SubOpt. Gap

0.8

0.6
0.4
0.2
0.0

25

50

75

100 125 150 175 200

Round (t)

Figure H.1: Performance comparisons for Instance 2 (Non-contextual) with K “ 2, 3, and 5,
evaluated under the PL loss (first row) and RB loss (second row).
Procedures 1 and `2) ˘to ensure a fair comparison across all methods. For sampling size parameter R,
N
we set R “ mint K
, 100, 000u.
The uniform random selection strategy, Uniform, selects K actions uniformly at random from the
available action set A at each round, without utilizing any uncertainty or reward-based information.
Best&Ref constructs an action pair (|St | “ 2) by combining two distinct sources of actions. The
first action is chosen to maximize the current reward estimate, while the second is sampled from a
reference policy—such as a uniform random policy or a supervised fine-tuned (SFT) model. This
pairing mechanism follows the framework introduced in Online GSHF [85] and XPO [84]. In our
experiments, we use the uniform random policy as the reference.
Performance measure. Since computing the exact suboptimality gap is challenging under a general
distribution ρ, we instead evaluate the realized regret, which serves as a slightly relaxed proxy for the
suboptimality gap.
˙
ˆ
T
˘J
1 ÿ`
1
pT pxt qq θ ‹ ` Õ ?
SubOptpT q À
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , π
T t“1
T
loooomoooon
incurred by MDS terms
T
ÿ

ď

`
˘J
1
ϕ pxt , π ‹ pxt qq ´ ϕ pxt , πt pxt qq θ ‹ `Õ
T t“1
looooooooooooooooooooooooooomooooooooooooooooooooooooooon

ˆ

1
?
T

˙
,

“:realized regret

pT denote the best policy among tπt uTt“1 ,
where we define πt pxq :“ argmaxa ϕpx, aqJ θpt , and let π
possibly selected using a validation set.
Results. We present performance comparisons in Figures G.1 through H.3, corresponding to Instances
1 through 4, respectively. Overall, our algorithm, M-AUPO, consistently outperforms other baseline
methods. The only exception is in Instance 2 (Figure H.1), a special case of the non-contextual
setting, where M-AUPO performs slightly worse than DopeWolfe. This is an expected outcome, as
DopeWolfe leverages a D-optimal design strategy, which is known to be highly effective in the
single-context setting. However, it is important to note that DopeWolfe completely fails in more
general contextual scenarios (Figures G.1, H.2, and H.3), and its computational cost is significantly
higher than that of our approach (see Table H.1).
The uniform random assortment selection strategy, Uniform, shows reasonable performance—though
consistently inferior to M-AUPO—in Instances 1, 2, and 4, as presented in Figures G.1, H.1, and H.3,
respectively. In contrast, for Instance 3 (Figure H.2), where most features are uninformative (being
44

Instance 3, PL Loss, K=2

0.0

25

SubOpt. Gap

0.2
0.0

25

SubOpt. Gap

0.2
25

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)
50 75 100 125 150 175 200

75

75

0.2
25

75

100 125 150 175 200

Round (t)

0.6
0.4
0.2
0.0

100 125 150 175 200

50

Instance 3, RB Loss, K=5

0.8

0.2
50

0.4

0.0

100 125 150 175 200

0.4

25

0.6

Round (t)

0.6

0.0

Round (t)

50

Instance 3, RB Loss, K=3

0.8

0.6
0.4

0.4

Round (t)

Instance 3, RB Loss, K=2

0.8

0.6

0.0

Instance 3, PL Loss, K=5

0.8

SubOpt. Gap

0.2

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)
50 75 100 125 150 175 200

SubOpt. Gap

0.6
0.4

Instance 3, PL Loss, K=3

0.8

SubOpt. Gap

SubOpt. Gap

0.8

Round (t)

25

50

75

100 125 150 175 200

Round (t)

Figure H.2: Performance comparisons for Instance 3 (Hard-to-learn contexts) with K “ 2, 3, and 5,
evaluated under the PL loss (first row) and RB loss (second row).

Instance 4, PL Loss, K=2

0.00

25

SubOpt. Gap

0.00

25

0.05

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)
50 75 100 125 150 175 200

Round (t)

25

50

75

0.10
0.05
50

75

0.10
0.05
25

100 125 150 175 200

50

75

100 125 150 175 200

Round (t)

Instance 4, RB Loss, K=5

0.25

0.15

25

0.15

0.00

100 125 150 175 200

0.20

0.00

0.20

Round (t)

Instance 4, RB Loss, K=3

0.25

0.15
0.05

0.10

Round (t)

0.20
0.10

0.15

0.00

Instance 4, RB Loss, K=2

0.25

0.20

SubOpt. Gap

0.05

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)
50 75 100 125 150 175 200

Instance 4, PL Loss, K=5

0.25

SubOpt. Gap

0.15

SubOpt. Gap

0.20
0.10

Instance 4, PL Loss, K=3

0.25

SubOpt. Gap

SubOpt. Gap

0.25

0.20
0.15
0.10
0.05
0.00

Round (t)

25

50

75

100 125 150 175 200

Round (t)

Figure H.3: Performance comparisons for Instance 4 (skewed stochastic contexts) with K “ 2, 3,
and 5, evaluated under the PL loss (first row) and RB loss (second row).
K

DopeWolfe

Uniform

Best&Ref

M-AUPO (PL)

M-AUPO (RB)

2
3
5
7
10

7.28 s
99.6 s
150.5 s
218.8 s
331.1 s

0.10 s
0.18 s
0.35 s
0.58 s
0.99 s

0.10s
0.10s
0.10s
0.10s
0.10s

1.09 s
2.35 s
2.71 s
4.10 s
6.89 s

1.04 s
1.06 s
1.10 s
1.13 s
1.31 s

Table H.1: Runtime comparison over 200 rounds (seconds)

nearly orthogonal to the true parameter) and thus careful action selection becomes crucial, Uniform
performs substantially worse than M-AUPO.
The Best&Ref algorithm performs consistently worse than our algorithm and does not benefit from
larger K, since it always selects only a pair of actions.
45

Moreover, the suboptimality gap consistently decreases with larger K across the three algorithms—M-AUPO, Uniform, and DopeWolfe—whereas Best&Ref shows no such improvement,
as it always selects only two actions regardless of K. For M-AUPO, this trend is consistent with
our theoretical results (Theorems 1 and 2). In contrast, the improvement observed for DopeWolfe
suggests that its current theoretical guarantees may be loose, as their bound actually deteriorates with
increasing K (recall that their theoretical guarantee worsens for larger K). This suggests that tighter
bounds could be obtained by incorporating techniques similar to those introduced in our work.
Table H.2 presents the average assortment size |St | of M-AUPO for various values of the maximum
assortment size K. In most cases, the algorithm selects the full K actions, i.e., |St | “ K. An
exception occurs when K is large (e.g., 30 or more), which may be impractical in real-world
applications due to the increased annotation burden on human labelers.
K

2

3

5

7

10

30

50

PL loss, |St |
RB loss, |St |

2.00
2.00

3.00
3.00

5.00
5.00

7.00
7.00

10.00
10.00

16.21
19.87

18.78
22.37

Table H.2: Assortment size |St | of M-AUPO with varying K in the synthetic experiment

H.2

Real-World Dataset

Setup. In our real-world dataset experiments, we evaluate performance on two widely used benchmark
datasets: TREC Deep Learning (TREC-DL) and NECTAR. The TREC-DL dataset provides 100
candidate answers for each query, offering a rich and diverse set of responses suitable for learning
from listwise feedback. In contrast, the NECTAR dataset presents a more concise setup, with only 7
candidate answers per question. From each dataset, we randomly sample |X | “ 5000 prompts, each
paired with its corresponding set of candidate actions—100 for TREC-DL and 7 for NECTAR.
We use the Gemma-2B language model [75] to construct the feature representation ϕpx, aq. To obtain
ϕpx, aq, we first concatenate the input prompt x and the candidate response a into a single sequence,
which is then fed into Gemma-2B. The resulting feature vector is extracted from the last hidden layer
of the model and has a dimensionality of d “ 2048. We then apply ℓ1 normalization to enhance
numerical stability and ensure consistent scaling. For each round t, we sample the context index from
an exponential distribution with rate λ “ 0.1, which assigns higher probability to smaller indices
and thus biases the selection toward earlier contexts. To generate ranking feedback and evaluate the
suboptimality gap, we use the Mistral-7B reward model [33] as the ground-truth reward function.
We measure the suboptimality gap every 2,500 rounds throughout the training process and report the
average performance over 10 independent runs, each with a different random seed. Along with the
average, we also include the standard error to indicate variability across runs. In these experiments,
we report results under the RB loss only, due to its superior performance, as demonstrated in the
synthetic data experiments. The experiments are conducted on a Xeon(R) Gold 6226R CPU @
2.90GHz (16 cores) and a single GeForce RTX 3090 GPU.
Baselines. We use the same set of baselines as in the`synthetic
data experiments. For DopeWolfe [76],
˘
N
we set the sampling size parameter R as R “ mint K
, 1000u. Although a small value of R ď 1000
may introduce
``N ˘˘ significant approximation error—since the theoretically minimal-error choice is
R“O K
—we adopt this smaller value in our experiment to reduce computational overhead.
Performance measure. We measure the realized regret as in the synthetic experiment.
Results. We present performance comparisons in Figure H.4. Our algorithm, M-AUPO, consistently
outperforms all baselines by a significant margin. As in the synthetic data experiments, the suboptimality gap for all methods decreases as K increases. Notably, DopeWolfe performs particularly
poorly on the TREC-DL dataset. This may be attributed to the
sampling size R,
`N ˘use of a small
which is insufficient compared to the full subset space of size K
“ OpN K q " 1000 ě R. This
result highlights an important practical limitation of DopeWolfe: despite its use of approximate
optimization to reduce runtime, the method still depends on combinatorial sampling to perform
well, which becomes computationally infeasible in large-scale settings. In contrast, our algorithm,
46

TREC-DL, K=2

4
3
2
5000

10000

15000

6

Simple Regret

5

5
4
3
2
5000

20000

10000

Round (t)

Nectar, K=2

3
2
5000

20000

10000

1.50
1.25

2.00

1.75
1.50
1.25
1.00

15000

20000

5000

20000

Nectar, K=5

2.00

1.00

15000

Round (t)

Simple Regret

1.75

10000

4

Nectar, K=3

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

Simple Regret

Simple Regret

15000

5

Round (t)

2.00

5000

TREC-DL, K=5

6

Simple Regret

Simple Regret

TREC-DL, K=3

DopeWolfe
Uniform
Best&Ref
M-AUPO (ours)

6

1.75
1.50
1.25
1.00

10000

Round (t)

15000

20000

5000

10000

Round (t)

15000

20000

Round (t)

Figure H.4: Performance comparisons on the TREC-DL dataset (top row) and the NECTAR dataset
(bottom row) for varying values of K “ 2, 3, and 5.
M-AUPO, achieves consistently strong performance while incurring only polynomial computational
cost, demonstrating superior scalability and practicality for real-world deployment.
Table H.3 reports the actual assortment size |St | selected by M-AUPO on both datasets. In the TRECDL experiment, |St | is nearly equal to K for all values of K, as the number of available actions is large
(N “ 100). In contrast, in the NECTAR experiment, where the number of available actions is much
smaller (N “ 7), the actual assortment size |St | is often smaller than K, especially when K “ N .
This reduction occurs because the limited action space constrains the potential informativeness of
larger assortments—for example, it becomes difficult to achieve high average uncertainty when there
are too few actions to choose from.
K

2

3

5

7

TREC-DL dataset, |St |
NECTAR dataset, |St |

2.00
2.00

3.00
2.99

5.00
4.45

6.98
4.76

Table H.3: Assortment size |St | of M-AUPO with varying K in the real-world dataset experiment

47

