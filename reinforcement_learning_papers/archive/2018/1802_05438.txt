Mean Field Multi-Agent Reinforcement Learning

Yaodong Yang 1 Rui Luo 1 Minne Li 1 Ming Zhou 2 Weinan Zhang 2 Jun Wang 1

arXiv:1802.05438v5 [cs.MA] 15 Dec 2020

Abstract
Existing multi-agent reinforcement learning methods are limited typically to a small number of
agents. When the agent number increases largely,
the learning becomes intractable due to the curse
of the dimensionality and the exponential growth
of agent interactions. In this paper, we present
Mean Field Reinforcement Learning where the
interactions within the population of agents are
approximated by those between a single agent
and the average effect from the overall population
or neighboring agents; the interplay between the
two entities is mutually reinforced: the learning
of the individual agentâ€™s optimal policy depends
on the dynamics of the population, while the dynamics of the population change according to the
collective patterns of the individual policies. We
develop practical mean field Q-learning and mean
field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and
battle games justify the learning effectiveness of
our mean field approaches. In addition, we report the first result to solve the Ising model via
model-free reinforcement learning methods.

1. Introduction
Multi-agent reinforcement learning (MARL) is concerned
with a set of autonomous agents that share a common environment (Busoniu et al., 2008). Learning in MARL is
fundamentally difficult since agents not only interact with
the environment but also with each other. Independent ğ‘„learning (Tan, 1993) that considers other agents as a part of
the environment often fails as the multi-agent setting breaks
the theoretical convergence guarantee and makes the learning unstable: changes in the policy of one agent will affect
that of the others, and vice versa (Matignon et al., 2012).
1

University College London, London, U.K. 2 Shanghai
Jiao Tong University, Shanghai, China.
Correspondence
to: Yaodong Yang <yaodong.yang@cs.ucl.ac.uk>, Jun Wang
<j.wang@cs.ucl.ac.uk>.
Proceedings of the 35 ğ‘¡ â„ International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Instead, accounting for the extra information from conjecturing the policies of other agents is beneficial to each single
learner (Foerster et al., 2017; Lowe et al., 2017a). Studies
show that an agent who learns the effect of joint actions has
better performance than those who do not in many scenarios,
including cooperative games (Panait & Luke, 2005), zerosum stochastic games (Littman, 1994), and general-sum
stochastic games (Littman, 2001; Hu & Wellman, 2003).
The existing equilibrium-solving approaches, although principled, are only capable of solving a handful of agents (Hu
& Wellman, 2003; Bowling & Veloso, 2002). The computational complexity of directly solving (Nash) equilibrium
would prevent them from applying to the situations with a
large group or even a population of agents. Yet, in practice,
many cases do require strategic interactions among a large
number of agents, such as the gaming bots in Massively
Multiplayer Online Role-Playing Game (Jeong et al., 2015),
the trading agents in stock markets (Troy, 1997), or the
online advertising bidding agents (Wang et al., 2017).
In this paper, we tackle MARL when a large number of
agents co-exist. We consider a setting where each agent is
directly interacting with a finite set of other agents; through
a chain of direct interactions, any pair of agents is interconnected globally (Blume, 1993). The scalability is solved by
employing Mean Field Theory (Stanley, 1971) â€“ the interactions within the population of agents are approximated by
that of a single agent played with the average effect from
the overall (local) population. The learning is mutually reinforced between two entities rather than many entities: the
learning of the individual agentâ€™s optimal policy is based
on the dynamics of the agent population, meanwhile, the
dynamics of the population is updated according to the individual policies. Based on such formulation, we develop
practical mean field ğ‘„-learning and mean field Actor-Critic
algorithms, and discuss the convergence of our solution
under certain assumptions. Our experiment on a simple
multi-agent resource allocation shows that our mean field
MARL is capable of learning over many-agent interactions
when others fail. We also demonstrate that with temporaldifference learning, mean field MARL manages to learn and
solve the Ising model without even explicitly knowing the
energy function. At last, in a mixed cooperative-competitive
battle game, we show that the mean field MARL achieves
high winning rates against other baselines previously reported for many agent systems.

Mean Field Multi-Agent Reinforcement Learning

2. Preliminary

2.2. Nash ğ‘„-learning

MARL intersects between reinforcement learning and game
theory. The marriage of the two gives rise to the general
framework of stochastic game (Shapley, 1953).

In MARL, the objective of each agent is to learn an optimal
policy to maximize its value function. Optimizing the ğ‘£ ğ…ğ‘—
for agent ğ‘— depends on the joint policy ğ… of all agents, the
concept of Nash equilibrium in stochastic games is therefore
of great importance (Hu & Wellman, 2003). It is represented
by a particular joint policy ğ…âˆ— , [ğœ‹âˆ—1 , . . . , ğœ‹âˆ—ğ‘ ] such that for
all ğ‘  âˆˆ S, ğ‘— âˆˆ {1, . . . , ğ‘} and all valid ğœ‹ ğ‘— , it satisfies

2.1. Stochastic Game
An ğ‘-agent (or, ğ‘-player) stochastic game ğ›¤ is formalized

by the tuple ğ›¤ , S, A1 , . . . , Ağ‘ , ğ‘Ÿ 1 , . . . , ğ‘Ÿ ğ‘ , ğ‘, ğ›¾ , where
Sdenotes the state space, and Ağ‘— is the action space of agent
ğ‘— âˆˆ {1, . . . , ğ‘}. The reward function for agent ğ‘— is defined
as ğ‘Ÿ ğ‘— : SÃ— A1 Ã— Â· Â· Â· Ã— Ağ‘ â†’ R, determining the immediate
reward. The transition probability ğ‘ : SÃ— A1 Ã—Â· Â· Â·Ã— Ağ‘ â†’
ğ›º(S) characterizes the stochastic evolution of states in time,
with ğ›º(S) being the collection of probability distributions
over the state space S. The constant ğ›¾ âˆˆ [0, 1) represents the
reward discount factor across time. At time step ğ‘¡, all agents
take actions simultaneously, each receives the immediate
reward ğ‘Ÿ ğ‘¡ğ‘— as a consequence of taking the previous actions.
The agents choose actions according to their policies, also
known as strategies. For agent ğ‘—, the corresponding policy is
defined as ğœ‹ ğ‘— : S â†’ ğ›º(Ağ‘— ), where ğ›º(Ağ‘— ) is the collection
of probability distributions over agent ğ‘—â€™s action space Ağ‘— .
Let ğ… , [ğœ‹ 1 , . . . , ğœ‹ ğ‘ ] denote the joint policy of all agents;
we assume, as one usually does, ğ… to be time-independent,
which is referred to be stationary. Provided an initial state
ğ‘ , the value function of agent ğ‘— under the joint policy ğ… is
written as the expected cumulative discounted future reward:
ğ‘£ ğ…ğ‘— (ğ‘ ) = ğ‘£ ğ‘— (ğ‘ ; ğ…) =

âˆ
X



ğ›¾ ğ‘¡ Eğ…, ğ‘ ğ‘Ÿ ğ‘¡ğ‘— |ğ‘ 0 = ğ‘ , ğ… .

(1)

ğ‘¡=0

The ğ‘„-function (or, the action-value function) can then be
defined within the framework of ğ‘-agent game based on the
Bellman equation given the value function in Eq. (1) such
that the ğ‘„-function ğ‘„ ğ…ğ‘— : SÃ— A1 Ã— Â· Â· Â· Ã— Ağ‘ â†’ R of agent
ğ‘— under the joint policy ğ… can be formulated as
ğ‘„ ğ…ğ‘— (ğ‘ , ğ’‚) = ğ‘Ÿ ğ‘— (ğ‘ , ğ’‚) + ğ›¾Eğ‘ 0 âˆ¼ ğ‘ [ğ‘£ ğ…ğ‘— (ğ‘ 0 )] ,

(2)

where ğ‘ 0 is the state at the next time step. The value function
ğ‘£ ğ…ğ‘— can be expressed in terms of the ğ‘„-function in Eq. (2) as


ğ‘£ ğ…ğ‘— (ğ‘ ) = Eğ’‚âˆ¼ğ… ğ‘„ ğ…ğ‘— (ğ‘ , ğ’‚) .
(3)
The ğ‘„-function for ğ‘-agent game in Eq. (2) extends the
formulation for single-agent game by considering the joint
action taken by all agents ğ’‚ , [ğ‘ 1 , . . . , ğ‘ ğ‘ ], and by taking
the expectation over the joint action in Eq. (3).
We formulate MARL by the stochastic game with a discretetime non-cooperative setting, i.e. no explicit coalitions are
considered. The game is assumed to be incomplete but to
have perfect information (Littman, 1994), i.e. each agent
knows neither the game dynamics nor the reward functions
of others, but it is able to observe and react to the previous
actions and the resulting immediate rewards of other agents.

ğ‘£ ğ‘— (ğ‘ ; ğ…âˆ— ) = ğ‘£ ğ‘— (ğ‘ ; ğœ‹âˆ—ğ‘— , ğ…âˆ—âˆ’ ğ‘— ) â‰¥ ğ‘£ ğ‘— (ğ‘ ; ğœ‹ ğ‘— , ğ…âˆ—âˆ’ ğ‘— ).
Here we adopt a compact notation for the joint policy of all
agents except ğ‘— as ğ…âˆ—âˆ’ ğ‘— , [ğœ‹âˆ—1 , . . . , ğœ‹âˆ—ğ‘—âˆ’1 , ğœ‹âˆ—ğ‘—+1 , . . . , ğœ‹âˆ—ğ‘ ].
In a Nash equilibrium, each agent acts with the best response
ğœ‹âˆ—ğ‘— to others, provided that all other agents follow the policy
ğ…âˆ—âˆ’ ğ‘— . It has been shown that, for a ğ‘-agent stochastic game,
there is at least one Nash equilibrium with stationary policies
(Fink et al., 1964). Given a Nash policy ğ…âˆ— , the Nash value
function ğ’— Nash (ğ‘ ) , [ğ‘£ ğ…1âˆ— (ğ‘ ), . . . , ğ‘£ ğ…ğ‘âˆ— (ğ‘ )] is calculated with
all agents following ğ…âˆ— from the initial state ğ‘  onward.
Nash ğ‘„-learning (Hu & Wellman, 2003) defines an iterative
procedure with two alternating steps for computing the Nash
policy: 1) solving the Nash equilibrium of the current stage
game defined by {ğ‘¸ ğ‘¡ } using the Lemke-Howson algorithm
(Lemke & Howson, 1964), 2) improving the estimation of
the ğ‘„-function with the new Nash equilibrium value. It can
be proved that under certain assumptions, the Nash operator
HNash defined by the following expression


HNash ğ‘¸(ğ‘ , ğ’‚) = Eğ‘ 0 âˆ¼ ğ‘ ğ’“(ğ‘ , ğ’‚) + ğ›¾ğ’— Nash (ğ‘ 0 )
(4)
forms a contraction mapping, where ğ‘¸ , [ğ‘„ 1 , . . . , ğ‘„ ğ‘ ],
and ğ’“(ğ‘ , ğ’‚) , [ğ‘Ÿ 1 (ğ‘ , ğ’‚), . . . , ğ‘Ÿ ğ‘ (ğ‘ , ğ’‚)]. The ğ‘„-function will
eventually converge to the value received in a Nash equilibrium of the game, referred to as the Nash ğ‘„-value.

3. Mean Field MARL
The dimension of joint action ğ’‚ grows proportionally w.r.t.
the number of agents ğ‘. As all agents act strategically and
evaluate simultaneously their value functions based on the
joint actions, it becomes infeasible to learn the standard
ğ‘„-function ğ‘„ ğ‘— (ğ‘ , ğ’‚). To address this issue, we factorize the
ğ‘„-function using only the pairwise local interactions:
1 X
ğ‘„ ğ‘— (ğ‘ , ğ’‚) = ğ‘—
ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ) ,
(5)
ğ‘
ğ‘˜âˆˆN( ğ‘—)

where N( ğ‘—) is the index set of the neighboring agents of
agent ğ‘— with the size ğ‘ ğ‘— = |N( ğ‘—)| determined by the settings of different applications. It is worth noting that the
pairwise approximation of the agent and its neighbors, while
significantly reducing the complexity of the interactions
among agents, still preserves global interactions between
any pair of agents implicitly (Blume, 1993). Similar approaches can be found in factorization machine (Rendle,
2012) and learning to rank (Cao et al., 2007).

Mean Field Multi-Agent Reinforcement Learning

Figure 1: Mean field approximation. Each agent is represented as a node in the grid,
which is only affected by the
mean effect from its neighbors (the blue area). Manyagent interactions are effectively converted into twoagent interactions.

3.1. Mean Field Approximation
The pairwise interaction ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ) as in Eq. (5) can be
approximated using the mean field theory (Stanley, 1971).
Here we consider discrete action spaces, where the action
ğ‘ ğ‘— of agent ğ‘— is a discrete categorical variable represented as
the one-hot encoding with each component indicating one of
the ğ· possible actions: ğ‘ ğ‘— , [ğ‘ 1ğ‘— , . . . , ğ‘ ğ·ğ‘— ]. We calculate the
mean action ğ‘Â¯ ğ‘— based on the neighborhood N( ğ‘—) of agent
ğ‘—, and express the one-hot action ğ‘ ğ‘˜ of each neighbor ğ‘˜ in
terms of the sum of ğ‘Â¯ ğ‘— and a small fluctuation ğ›¿ğ‘ ğ‘—,ğ‘˜ as
1 X ğ‘˜
ğ‘ ğ‘˜ = ğ‘Â¯ ğ‘— + ğ›¿ğ‘ ğ‘—,ğ‘˜ ,
where ğ‘Â¯ ğ‘— = ğ‘—
ğ‘ , (6)
ğ‘
ğ‘˜

where ğ‘Â¯ ğ‘— , [Â¯
ğ‘ 1ğ‘— , . . . , ğ‘Â¯ğ·ğ‘— ] can be interpreted as the empirical
distribution of the actions taken by agent ğ‘—â€™s neighbors. By
Taylorâ€™s theorem, the pairwise ğ‘„-function ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ), if
twice-differentiable w.r.t. the action ğ‘ ğ‘˜ taken by neighbor ğ‘˜,
can be expended and expressed as
1 X ğ‘—
ğ‘„ (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ )
ğ‘„ ğ‘— (ğ‘ , ğ’‚) = ğ‘—
ğ‘ ğ‘˜

1 X
= ğ‘—
ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) + âˆ‡ğ‘Â¯ ğ‘— ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) Â· ğ›¿ğ‘ ğ‘—,ğ‘˜
ğ‘ ğ‘˜

1
+
ğ›¿ğ‘ ğ‘—,ğ‘˜ Â· âˆ‡2ğ‘Ëœ ğ‘—,ğ‘˜ ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Ëœ ğ‘—,ğ‘˜ ) Â· ğ›¿ğ‘ ğ‘—,ğ‘˜
2


1 X ğ‘—,ğ‘˜
ğ‘—
ğ‘—
= ğ‘„ (ğ‘ , ğ‘ , ğ‘Â¯ ğ‘— ) + âˆ‡ğ‘Â¯ ğ‘— ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) Â·
ğ›¿ğ‘
ğ‘ğ‘— ğ‘˜


1 X
ğ‘—
ğ‘—
ğ‘—,ğ‘˜
ğ‘—,ğ‘˜
ğ‘—,ğ‘˜
2
+
(7)
ğ›¿ğ‘ Â· âˆ‡ğ‘Ëœ ğ‘—,ğ‘˜ ğ‘„ (ğ‘ , ğ‘ , ğ‘Ëœ ) Â· ğ›¿ğ‘
2ğ‘ ğ‘— ğ‘˜
1 X ğ‘—
= ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) +
ğ‘… ğ‘— (ğ‘ ğ‘˜ ) â‰ˆ ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) , (8)
2ğ‘ ğ‘— ğ‘˜ ğ‘ ,ğ‘
ğ‘—
ğ‘˜
ğ‘—,ğ‘˜
where ğ‘…ğ‘ ,ğ‘
Â· âˆ‡2ğ‘Ëœ ğ‘—,ğ‘˜ ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Ëœ ğ‘—,ğ‘˜ ) Â· ğ›¿ğ‘ ğ‘—,ğ‘˜
ğ‘— (ğ‘ ) , ğ›¿ğ‘
denotes the Taylor polynomialâ€™s remainder P
with ğ‘Ëœ ğ‘—,ğ‘˜ =
ğ‘Â¯ ğ‘— + ğœ– ğ‘—,ğ‘˜ ğ›¿ğ‘ ğ‘—,ğ‘˜ and ğœ– ğ‘—,ğ‘˜ âˆˆ [0, 1]. In Eq. (7), ğ‘˜ ğ›¿ğ‘ ğ‘˜ = 0
by Eq. (6) such that the first-order term is dropped. From
the perspective of agent ğ‘—, the action ğ‘ ğ‘˜ in the second-order
ğ‘—
ğ‘˜
remainders ğ‘…ğ‘ ,ğ‘
ğ‘— (ğ‘ ) is chosen based on the external acğ‘—
ğ‘˜
tion distribution of agent ğ‘˜, ğ‘…ğ‘ ,ğ‘
ğ‘— (ğ‘ ) is thus essentially
a random variable. In fact, one can further prove that the
ğ‘—
ğ‘˜
remainder ğ‘…ğ‘ ,ğ‘
ğ‘— (ğ‘ ) is bounded within a symmetric interval [âˆ’2ğ‘€, 2ğ‘€] under the mild condition of the ğ‘„-function
ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ) being ğ‘€-smooth (e.g. the linear function); as
ğ‘—
ğ‘˜
a result, ğ‘…ğ‘ ,ğ‘
ğ‘— (ğ‘ ) acts as a small fluctuation near zero. To
stay self-contained, the derivation of the bound is put in
the Appendix B. With the assumptions of homogeneity and
locality on all agents within the neighborhood, the remainders tend to cancel each other, leading to the left term of
ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) in Eq. (8).

As illustrated in Fig. 1, with the mean field approximation, the pairwise interactions ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ) between agent
ğ‘— and each neighboring agent ğ‘˜ are simplified as that between ğ‘—, the central agent, and the virtual mean agent, that
is abstracted by the mean effect of all neighbors within

ğ‘—â€™s neighborhood. The interaction is thus simplified and
expressed by the mean field ğ‘„-function ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) in
Eq. (8). During the learning
phase, given an experience

ğ‘’ = ğ‘ , {ğ‘ ğ‘˜ }, {ğ‘Ÿ ğ‘— }, ğ‘ 0 , the mean field ğ‘„-function is updated in a recurrent manner as
ğ‘—
ğ‘„ ğ‘¡+1
(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) = (1 âˆ’ ğ›¼)ğ‘„ ğ‘¡ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) + ğ›¼[ğ‘Ÿ ğ‘— + ğ›¾ğ‘£ ğ‘¡ğ‘— (ğ‘ 0 )] ,
(9)

where ğ›¼ğ‘¡ denotes the learning rate, and ğ‘Â¯ ğ‘— is the mean action
of all neighbors of agent ğ‘— as defined in Eq. (6). The mean
field value function ğ‘£ ğ‘¡ğ‘— (ğ‘ 0 ) for agent ğ‘— at time ğ‘¡ in Eq. (9) is
ğ‘£ ğ‘¡ğ‘— (ğ‘ 0 ) =

X

h

i
ğœ‹ğ‘¡ğ‘— ğ‘ ğ‘— |ğ‘ 0 , ğ‘Â¯ ğ‘— Eğ‘Â¯ ğ‘— (ğ’‚âˆ’ ğ‘— )âˆ¼ğ…âˆ’ ğ‘— ğ‘„ ğ‘¡ğ‘— ğ‘ 0 , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ,
ğ‘¡

ğ‘ğ‘—

(10)

As shown in Eqs. (9) and (10), with the mean field approximation, the MARL problem is converted into that of solving
for the central agent ğ‘—â€™s best response ğœ‹ğ‘¡ğ‘— w.r.t. the mean
action ğ‘Â¯ ğ‘— of all ğ‘—â€™s neighbors, which represents the action
distribution of all neighboring agents of the central agent ğ‘—.
We introduce an iterative procedure in computing the best
response ğœ‹ğ‘¡ğ‘— of each agent ğ‘—. In the stage game {ğ‘¸ ğ‘¡ }, the
mean action ğ‘Â¯ ğ‘— of all ğ‘—â€™s neighbors is first calculated by
averaging the actions ğ‘ ğ‘˜ taken by ğ‘—â€™s ğ‘ ğ‘— neighbors from
the policies ğœ‹ğ‘¡ğ‘˜ parametrized by their previous mean actions
ğ‘˜
ğ‘Â¯âˆ’
1 X ğ‘˜ ğ‘˜
ğ‘˜
ğ‘Â¯ ğ‘— = ğ‘—
ğ‘ , ğ‘ âˆ¼ ğœ‹ğ‘¡ğ‘˜ (Â·|ğ‘ , ğ‘Â¯âˆ’
),
(11)
ğ‘
ğ‘˜

With each ğ‘Â¯ ğ‘— calculated as in Eq. (11), the policy ğœ‹ğ‘¡ğ‘— changes
consequently due to the dependence on the current ğ‘Â¯ ğ‘— . The
new Boltzmann policy is then determined for each ğ‘— that

exp ğ›½ğ‘„ ğ‘¡ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )
ğ‘—
ğ‘—
ğ‘—
ğœ‹ğ‘¡ (ğ‘ |ğ‘ , ğ‘Â¯ ) = P
 . (12)
ğ‘—
ğ‘—0 Â¯ğ‘— )
ğ‘—0
ğ‘— exp ğ›½ğ‘„ ğ‘¡ (ğ‘ , ğ‘ , ğ‘
ğ‘ âˆˆA
By iterating Eqs. (11) and (12), the mean actions ğ‘Â¯ ğ‘— and the
corresponding policies ğœ‹ğ‘¡ğ‘— for all agents improves alternatively. In spite of lacking an intuitive impression of being
stationary, in the following subsections, we will show that
the mean action ğ‘Â¯ ğ‘— will be equilibrated at an unique point
after several iterations, and hence the policy ğœ‹ğ‘¡ğ‘— converges.
To distinguish from the Nash value function ğ’— Nash (ğ‘ ) in
Eq. (4), we denote the mean field value function in Eq. (10)
as ğ’— MF (ğ‘ ) , [ğ‘£ 1 (ğ‘ ), . . . , ğ‘£ ğ‘ (ğ‘ )]. With ğ’— MF assembled, we
now define the mean field operator HMF in the form of

Mean Field Multi-Agent Reinforcement Learning
j

4xÄ

HMF ğ‘¸(ğ‘ , ğ’‚) = Eğ‘ 0 âˆ¼ ğ‘




ğ’“(ğ‘ , ğ’‚) + ğ›¾ğ’— MF (ğ‘ 0 ) .

(13)

0
1
2
3
4

4xÄ

t

We can implement the mean field ğ‘„-function in Eq. (8) by
universal function approximators such as neural networks,
where the ğ‘„-function is parameterized with the weights ğœ™.
The update rule in Eq. (9) can be reformulated as weights adjustment. For off-policy learning, we exploit either standard
ğ‘„-learning (Watkins & Dayan, 1992) for discrete action
spaces or DPG (Silver et al., 2014) for continuous action
spaces. Here we focus on the former, which we call MF-ğ‘„.
In MF-ğ‘„, agent ğ‘— is trained by minimizing the loss function
2
L(ğœ™ ğ‘— ) = ğ‘¦ ğ‘— âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) ,
where ğ‘¦ ğ‘— = ğ‘Ÿ ğ‘— + ğ›¾ ğ‘£ MF
(ğ‘ 0 ) is the target mean field value
ğœ™ğ‘—
âˆ’

ğ‘—
calculated with the weights ğœ™âˆ’
. Differentiating L(ğœ™ ğ‘— ) gives



âˆ‡ ğœ™ ğ‘— L(ğœ™ ğ‘— ) = ğ‘¦ ğ‘— âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) âˆ‡ ğœ™ ğ‘— ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) , (14)

which enables the gradient-based optimizers for training.
Instead of setting up Boltzmann policy using the ğ‘„-function
as in MF-ğ‘„, we can explicitly model the policy by neural
networks with the weights ğœƒ, which leads to the on-policy
actor-critic method (Konda & Tsitsiklis, 2000) that we call
MF-AC. The policy network ğœ‹ ğœƒ ğ‘— , i.e. the actor, of MF-AC
is trained by the sampled policy gradient:
âˆ‡ ğœƒ ğ‘— J(ğœƒ ğ‘— ) â‰ˆ âˆ‡ ğœƒ ğ‘— log ğœ‹ ğœƒ ğ‘— (ğ‘ )ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )

.
ğ‘= ğœ‹ ğœƒ ğ‘— (ğ‘ )

The critic of MF-AC follows the same setting for MF-ğ‘„
with Eq. (14). During the training of MF-AC, one needs to
alternatively update ğœ™ and ğœƒ until convergence. We illustrate
the MF-ğ‘„ iterations in Fig. 2, and present the pesudocode
for both MF-ğ‘„ and MF-AC in Appendix A.

0.82
0.69
0.00
0.00
0.00

-0.29
-0.56 Q
t
0.00
0.00
0.00

0.93
0.69
0.00
0.00
0.00

-0.29 Q
t+1
-0.56
0.00
0.00
0.00

2.00
0.75
0.00
0.00
0.00

-0.29
-0.38 QEnd
0.00
0.00
0.00

j

4xÄ

In fact, we can prove that HMF forms a contraction mapping;
that is, one updates ğ‘¸ by iteratively applying the mean field
operator HMF , the mean field ğ‘„-function will eventually
converge to the Nash ğ‘„-value under certain assumptions.
3.2. Implementation

a

0
1
2
3
4

t+1

...

End

a

j

a

0
1
2
3
4

Figure 2: MF-ğ‘„ iterations on a 3 Ã— 3 stateless toy example.
The goal is to coordinate the agents to an agreed direction.
Each agent has two choices of actions: up â†‘ or down â†“.
The reward of each agentâ€™s staying in the same direction
as its [0, 1, 2, 3, 4] neighbors are [âˆ’2.0, âˆ’1.0, 0.0, 1.0, 2.0],
respectively. The neighbors are specified by the four directions on the grid with cyclic structure on all directions, e.g.
the first row and the third row are adjacent. The reward for
the highlighted agent ğ‘— on the bottom left at time ğ‘¡ + 1 is
2.0, as all neighboring agents stay down in the same time.
We listed the Q-tables for agent ğ‘— at three time steps where
ğ‘Â¯ ğ‘— is the percentage of neighboring ups. Following Eq. 9,
ğ‘—
we have ğ‘„ ğ‘¡+1
(â†‘, ğ‘Â¯ ğ‘— = 0) = ğ‘„ ğ‘¡ğ‘— (â†‘, ğ‘Â¯ ğ‘— = 0) + ğ›¼[ğ‘Ÿ ğ‘— âˆ’ ğ‘„ ğ‘¡ğ‘— (â†‘
ğ‘—
, ğ‘Â¯ = 0)] = 0.82 + 0.1 Ã— (2.0 âˆ’ 0.82) = 0.93. The rightmost plot shows the convergent scenario where the ğ‘„-value
of staying down is 2.0, which is the largest reward in the
environment.
Assumption 3. For each stage game [ğ‘„ 1ğ‘¡ (ğ‘ ), ..., ğ‘„ ğ‘¡ğ‘ (ğ‘ )] at
time ğ‘¡ and in state ğ‘  in training, for all ğ‘¡, ğ‘ , ğ‘— âˆˆ {1, . . . , ğ‘},
the Nash equilibrium ğ…âˆ— = [ğœ‹âˆ—1 , . . . , ğœ‹âˆ—ğ‘ ] is recognized either as 1) the global optimum or 2) a saddle point expressed
as:
Q ğ‘˜
1. Eğ…âˆ— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¥ Eğ… [ğ‘„ ğ‘¡ğ‘— (ğ‘ )], âˆ€ğ… âˆˆ ğ›º
ğ‘˜ A ;

2. Eğ…âˆ— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¥ E ğœ‹ ğ‘— Eğ…âˆ—âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )], âˆ€ğœ‹ ğ‘— âˆˆ ğ›º Ağ‘— and

Q
ğ‘˜
Eğ…âˆ— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¤ E ğœ‹âˆ—ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )], âˆ€ğ… âˆ’ ğ‘— âˆˆ ğ›º
ğ‘˜6= ğ‘— A .
Note that Assumption 3 imposes a strong constraint on every
single stage game encountered in training. In practice, however, we find this constraint appears not to be a necessary
condition for the learning algorithm to converge. This is in
line with the empirical findings in Hu & Wellman (2003).

3.3. Proof of Convergence

Our proof is also built upon the two lemmas as follows:

We now prove the convergence of ğ‘¸ ğ‘¡ , [ğ‘„ 1ğ‘¡ , . . . , ğ‘„ ğ‘¡ğ‘ ] to
the Nash ğ‘„-value ğ‘¸ âˆ— = [ğ‘„ 1âˆ— , . . . , ğ‘„ âˆ—ğ‘ ] as the iterations of
MF-ğ‘„ is applied. The proof is presented by showing that
the mean field operator HMF in Eq. (13) forms a contraction mapping with the fixed point at ğ‘¸ âˆ— under the main
assumptions. We start from introducing the assumptions:

Lemma 1. Under Assumption 3, the Nash operator HNash
in Eq. (4) forms a contraction mapping on the complete
metric space from Q to Q with the fixed point being the
Nash ğ‘„-value of the entire game, i.e. Hğ‘¡Nash ğ‘¸ âˆ— = ğ‘¸ âˆ— .

Assumption 1. Each action-value pair is visited infinitely
often, and the reward is bounded by some constant ğ¾.

Lemma 2. The random process {ğ›¥ğ‘¡ } defined in R as

Assumption 2. Agentâ€™s policy is Greedy in the Limit with
Infinite Exploration (GLIE). In the case with the Boltzmann
policy, the policy becomes greedy w.r.t. the ğ‘„-function in
the limit as the temperature decays asymptotically to zero.

Proof. See Theorem 17 in Hu & Wellman (2003).

ğ›¥ğ‘¡+1 (ğ‘¥) = (1 âˆ’ ğ›¼ğ‘¡ (ğ‘¥))ğ›¥ğ‘¡ (ğ‘¥) + ğ›¼ğ‘¡ (ğ‘¥)ğ¹ğ‘¡ (ğ‘¥)
converges to zero with probability 1 (w.p.1) when
P
P
1. 0 â‰¤ ğ›¼ğ‘¡ (ğ‘¥) â‰¤ 1, ğ‘¡ ğ›¼ğ‘¡ (ğ‘¥) = âˆ, ğ‘¡ ğ›¼ğ‘¡2 (ğ‘¥) < âˆ;

(15)

Mean Field Multi-Agent Reinforcement Learning

2. ğ‘¥ âˆˆ X, the set of possible states, and |X| < âˆ;
3. kE[ğ¹ğ‘¡ (ğ‘¥)|Fğ‘¡ ]kğ‘Š â‰¤ ğ›¾kğ›¥ğ‘¡ kğ‘Š + ğ‘ ğ‘¡ , where ğ›¾ âˆˆ [0, 1) and
ğ‘ ğ‘¡ converges to zero w.p.1;
4. var[ğ¹ğ‘¡ (ğ‘¥)|Fğ‘¡ ] â‰¤ ğ¾(1 + kğ›¥ğ‘¡ k2ğ‘Š ) with constant ğ¾ > 0.
Here Fğ‘¡ denotes the filtration of an increasing sequence of
ğœ-fields including the history of processes; ğ›¼ğ‘¡ , ğ›¥ğ‘¡ , ğ¹ğ‘¡ âˆˆ Fğ‘¡
and k Â· kğ‘Š is a weighted maximum norm (Bertsekas, 2012).
Proof. See Theorem 1 in Jaakkola et al. (1994) and Corollary 5 SzepesvÃ¡ri & Littman (1999) for detailed derivation.
We include it here to stay self-contained.
By subtracting ğ‘¸ âˆ— (ğ‘ , ğ’‚) on both sides of Eq. (9), we present
the relation from the comparison with Eq. (15) such that
ğœŸğ‘¡ (ğ‘¥) = ğ‘¸ ğ‘¡ (ğ‘ , ğ’‚) âˆ’ ğ‘¸ âˆ— (ğ‘ , ğ’‚),
ğ‘­ğ‘¡ (ğ‘¥) = ğ’“ ğ‘¡ + ğ›¾ğ’— MF
(16)
ğ‘¡ (ğ‘  ğ‘¡+1 ) âˆ’ ğ‘¸ âˆ— (ğ‘  ğ‘¡ , ğ’‚ ğ‘¡ ),
where ğ‘¥ , (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ) denotes the visited state-action pair
at time ğ‘¡. In Eq. (15), ğ›¼(ğ‘¡) is interpreted as the learning
rate with ğ›¼ğ‘¡ (ğ‘ 0 , ğ’‚ 0 ) = 0 for any (ğ‘ 0 , ğ’‚ 0 ) 6= (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ); this is
because that each agent only updates the ğ‘„-function with the
state ğ‘ ğ‘¡ and actions ğ’‚ ğ‘¡ visited at time ğ‘¡. Lemma 2 suggests
ğ›¥ğ‘¡ (ğ‘¥)â€™s convergence to zero, which means, if it holds, the
sequence of ğ‘„â€™s will asymptotically tend to the Nash ğ‘„value ğ‘¸ âˆ— .
One last piece to establish the main theorem is the below:
Proposition 1.PLet the metric space be R ğ‘ and the metric
ğ‘—
ğ‘—
ğ‘— ğ‘
ğ‘— ğ‘
be ğ‘‘(ğ’‚, ğ’ƒ) =
ğ‘— |ğ‘ âˆ’ ğ‘ |, for ğ’‚ = [ğ‘ ]1 , ğ’ƒ = [ğ‘ ]1 . If
ğ‘—
the ğ‘„-function is ğ¾-Lipschitz continuous w.r.t. ğ‘ , then
the operator B(ğ‘ ğ‘— ) , ğœ‹ ğ‘— (ğ‘ ğ‘— |ğ‘ , ğ‘Â¯ ğ‘— ) in Eq. (12) forms a
contraction mapping under sufficiently low temperature ğ›½.
Proof. See details in Appendix D due to the space limit.
Theorem 1. In a finite-state stochastic game, the ğ‘¸ ğ‘¡ values
computed by the update rule of MF-ğ‘„ in Eq. (9) converges
to the Nash ğ‘„-value ğ‘¸ âˆ— = [ğ‘„ 1âˆ— , . . . , ğ‘„ âˆ—ğ‘ ], if Assumptions 1,
2 & 3, and Lemma 2â€™s first and second conditions are met.
Proof. Let Fğ‘¡ denote the ğœ-field generated by all random
variables in the history of the stochastic game up to time
ğ‘¡: (ğ‘ ğ‘¡ , ğ›¼ğ‘¡ , ğ’‚ ğ‘¡ , ğ‘Ÿ ğ‘¡âˆ’1 , ..., ğ‘ 1 , ğ›¼1 , ğ’‚ 1 , ğ‘¸ 0 ). Note that ğ‘¸ ğ‘¡ is a random variable derived from the historical trajectory up to time
ğ‘¡. Given the fact that all ğ‘¸ ğœ with ğœ < ğ‘¡ are Fğ‘¡ -measurable,
both ğœŸğ‘¡ and ğ‘­ğ‘¡âˆ’1 are therefore also Fğ‘¡ -measurable, which
satisfies the measurability condition of Lemma 2.
To apply Lemma 2, we need to show that the mean field
operator HMF meets Lemma 2â€™s third and fourth conditions.
For Lemma 2â€™s third condition, we begin with Eq. (16) that
ğ‘­ğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ) = ğ’“ ğ‘¡ + ğ›¾ğ’— MF
ğ‘¡ (ğ‘  ğ‘¡+1 ) âˆ’ ğ‘¸ âˆ— (ğ‘  ğ‘¡ , ğ’‚ ğ‘¡ )
= ğ’“ ğ‘¡ + ğ›¾ğ’— Nash
(ğ‘ ğ‘¡+1 ) âˆ’ ğ‘¸ âˆ— (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )
ğ‘¡
Nash
+ ğ›¾[ğ’— MF
(ğ‘ ğ‘¡+1 )]
ğ‘¡ (ğ‘  ğ‘¡+1 ) âˆ’ ğ’— ğ‘¡


Nash
= ğ’“ ğ‘¡ + ğ›¾ğ’— ğ‘¡ (ğ‘ ğ‘¡+1 ) âˆ’ ğ‘¸ âˆ— (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ) + ğ¶ğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )

= ğ‘­ğ‘¡Nash (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ) + ğ‘ªğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ ).

(17)

Note the fact that ğ‘­ğ‘¡Nash in Eq. (17) is essentially the ğ‘­ğ‘¡ in
Lemma 2 in proving the convergence of the Nash ğ‘„-learning
algorithm. From Lemma 1, it is straightforward to show that
ğ‘­ğ‘¡Nash forms a contraction mapping with the norm k Â· kâˆ
being the maximum norm on ğ’‚. We thus have for all ğ‘¡ that
kE[ğ‘­ğ‘¡Nash (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ ]kâˆ â‰¤ ğ›¾kğ‘¸ ğ‘¡ âˆ’ ğ‘¸ âˆ— kâˆ = ğ›¾kğœŸğ‘¡ kâˆ .
In meeting the third condition, we obtain from Eq. (17) that
kE[ğ‘­ğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ ]kâˆ â‰¤ kğ‘­ğ‘¡Nash (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ kâˆ + kğ‘ªğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ kâˆ
â‰¤ ğ›¾kğœŸğ‘¡ kâˆ + kğ‘ªğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ kâˆ .
(18)

We are left to prove that ğ‘ ğ‘¡ = kğ‘ªğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ k converges to
zero w.p.1. With Assumption 3, for each stage game, all the
globally optimal equilibrium(s) share the same Nash value,
so does the saddle-point equilibrium(s). Each of the two
following results is essentially associated with one of the
two mutually exclusive scenarios in Assumption 3:
1. For globally optimal equilibriums, all players obtain the
joint maximum values that are unique and identical for
all equilibriums according to the definition;
2. Suppose that the stage game {ğ‘¸ ğ‘¡ } has two saddle-point
equilibriums, ğ… and ğ†. It holds for agent ğ‘— that
E ğœ‹ ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¥ EğœŒ ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )],
EğœŒ ğ‘— Eğ†âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¤ EğœŒ ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )].
By combing the above inequalities, we obtain
E ğœ‹ ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] â‰¥ EğœŒ ğ‘— Eğ†âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )].
By the definition of saddle points, the above inequality
still holds by reversing the order of ğ… and ğ†; hence, the
equilibriums for agent ğ‘– at both saddle points are the same
such that E ğœ‹ ğ‘— Eğ… âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )] = EğœŒ ğ‘— Eğ†âˆ’ ğ‘— [ğ‘„ ğ‘¡ğ‘— (ğ‘ )].
Given Proposition 1 that the policy based on the mean field
ğ‘„-function forms a contraction mapping, and that all optimal/saddle points share the same Nash value in each stage
game, with the homogeneity of agents, ğ’— MF will asymptotically converges to ğ’— Nash , the third condition is thus satisfied.
For the fourth condition, we exploit the conclusion that is
proved above that HMF forms a contraction mapping, i.e.
HMF ğ‘¸ âˆ— = ğ‘¸ âˆ— , and it follows that
2
var[ğ‘­ğ‘¡ (ğ‘ ğ‘¡ , ğ’‚ ğ‘¡ )|Fğ‘¡ ] = E[(ğ’“ ğ‘¡ + ğ›¾ğ’— MF
ğ‘¡ (ğ‘  ğ‘¡+1 ) âˆ’ ğ‘¸ âˆ— (ğ‘  ğ‘¡ , ğ’‚ ğ‘¡ )) ]
MF
2
= E[(ğ’“ ğ‘¡ + ğ›¾ğ’— MF
ğ‘¡ (ğ‘  ğ‘¡+1 ) âˆ’ H (ğ‘¸ âˆ— )) ]

= var[ğ’“ ğ‘¡ + ğ›¾ğ’— MF
ğ‘¡ (ğ‘  ğ‘¡+1 )|Fğ‘¡ ]
â‰¤ ğ¾(1 + kğœŸğ‘¡ k2ğ‘Š ).

(19)

In the last step of Eq. (19), we employ Assumption 1 that the
reward ğ’“ ğ‘¡ is always bounded by some constant. Finally, with
all conditions met, it follows Lemma 2 that ğœŸğ‘¡ converges to
zero w.p.1, i.e. ğ‘¸ ğ‘¡ converges to ğ‘¸ âˆ— w.p.1.
Apart from being convergent to the Nash ğ‘„-value, MF-ğ‘„ is
also Rational (Bowling & Veloso, 2001; 2002). We leave
the corresponding discussion in Appendix D for details.

1.0

0.8

0.8

0.6

IL
FMQ
Rec-FMQ
MF-Q
MAAC
MF-AC

0.4
0.2
0.0
0

200

400

600

Timestep

(a) ğ‘ = 100

800

1000

1.0

IL
FMQ
Rec-FMQ
MF-Q
MAAC
MF-AC

0.6
0.4

Performance

1.0

Performance

Performance

Mean Field Multi-Agent Reinforcement Learning

0.2
0.0

IL
FMQ
Rec-FMQ
MF-Q
MAAC
MF-AC

0.8
0.6
0.4
0.2
0.0

0

200

400

600

Timestep

800

1000

(b) ğ‘ = 500

0

200

400

600

Timestep

800

1000

(c) ğ‘ = 1000

Figure 3: Learning with ğ‘ agents in the GS environment with ğœ‡ = 400 and ğœ = 200.

4. Related Work
We continue our discussion on related work from Introduction and make comparisons with existing techniques in
a greater scope. Our work follows the same direction as
Littman (1994); Hu & Wellman (2003); Bowling & Veloso
(2002) on adapting a Stochastic Game (van der Wal et al.,
1981) into the MARL formulation. Specifically, Littman
(1994) addressed two-player zero-sum stochastic games by
introducing a â€œminimaxâ€ operator in ğ‘„-learning, whereas
Hu & Wellman (2003) extended it to the general-sum case
by learning a Nash equilibrium in each stage game and considering a mixed strategy. Nash-Q learning is guaranteed to
converge to Nash strategies under the (strong) assumption
that there exists an equilibrium for every stage game. In the
situation where agents can be identified as either "friends"
or "foes" (Littman, 2001), one can simply solve it by alternating between fully cooperative and zero-sum learning.
Considering the convergence speed, Littman & Stone (2005)
and de Cote & Littman (2008) draw on the folk theorem and
acquired a polynomial-time Nash equilibrium algorithm for
repeated stochastic games, while Bowling & Veloso (2002)
tried varying the learning rate to improve the convergence.
The recent treatment of MARL was using deep neural networks as the function approximator. In addressing the nonstationary issue in MARL, various solutions have been proposed including neural-based opponent modeling (He &
Boyd-Graber, 2016), policy parameters sharing (Gupta et al.,
2017), etc. Researchers have also adopted the paradigm of
centralized training with decentralized execution for multiagent policy-gradient learning: BICNET (Peng et al., 2017),
COMA (Foerster et al., 2018) and MADDPG (Lowe et al.,
2017a), which allows the centralized critic ğ‘„-function to
be trained with the actions of other agents, while the actor
needs only local observation to optimize agentâ€™s policy.
The above MARL approaches limit their studies mostly to
tens of agents. As the number of agents grows larger, not
only the input space of ğ‘„ grows exponentially, but most
critically, the accumulated noises by the exploratory actions
of other agents make the ğ‘„-function learning no longer feasible. Our work addresses the issue by employing the mean
field approximation (Stanley, 1971) over the joint action
space. The parameters of the ğ‘„-function is independent of
the number of agents as it transforms multiple agents interactions into two entities interactions (single agent v.s. the

distribution of the neighboring agents). This would effectively alleviate the problem of the exploratory noise (Colby
et al., 2015) caused by many other agents, and allow each
agent to determine which actions are beneficial to itself.
Our work is also closely related to the recent development
of mean field games (MFG) (Lasry & Lions, 2007; Huang
et al., 2006; Weintraub et al., 2006). MFG studies population behaviors resulting from the aggregations of decisions
taken from individuals. Mathematically, the dynamics are
governed by a set of two stochastic differential equations
that model the backward dynamics of individualâ€™s value
function, and the forward dynamics of the aggregate distribution of agent population. Despite that the backward
equation equivalently describes what Bellman equation indicates in the MDP, the primarily goal for MFG is rather for
a model-based planning and to infer the movements of the
individual density through time. The mean field approximation (Stanley, 1971) in also employed in physics, but our
work is different in that we focus on a model-free solution of
learning optimal actions when the dynamics of the system
and the reward function are unknown. Very recently, Yang
et al. (2017) built a connection between MFG and reinforcement learning. Their focus is, however, on the inverse RL
in order to learn both the reward function and the forward
dynamics of the MFG from the policy data, whereas our
goal is to form a computable ğ‘„-learning algorithm under
the framework of temporal difference learning.

5. Experiments
We analyze and evaluate our algorithms in three different
scenarios, including two stage games: the Gaussian Squeeze
and the Ising Model, and the mixed cooperative-competitive
battle game.
5.1. Gaussian Squeeze
Environment. In the Gaussian Squeeze (GS) task (HolmesParker et al., 2014), ğ‘ homogeneous agents determine their
individual action ğ‘ ğ‘— to jointly optimize the most appropriPğ‘
ğ‘—
ate summation ğ‘¥ =
ğ‘—=1 ğ‘ . Each agent has 10 action
choices â€“ integers 0 to 9. The system objective is defined
âˆ’(ğ‘¥âˆ’ğœ‡)2

as ğº(ğ‘¥) = ğ‘¥ğ‘’ ğœ2 , where ğœ‡ and ğœ are the pre-defined
mean and variance of the system. In the scenario of traffic
congestion, each agent is one traffic controller trying to send

Mean Field Multi-Agent Reinforcement Learning
Order Parameter

1.0
0.8
0.6

5

5

5

10

10

15

15

15

0

5

10

15

0

0.5

1.0

1.5

2.0

Temperature

MSE

0.8

OP

0.8

0.6

0.6

0.4

0.4

0.2

0.2
10000

Timestep

15000

(a) ğœ = 0.8

0.0
20000

1.0

1.0
MSE

0.8

OP

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0.0

0

5000

10000

Timestep

Ï„ < Ï„ C : Ï„ = 0 .9

0

2.5

Mean Squared Error

1.0

Order Parameter

1.0

5000

5

10

15

0

5

10

15

(a) MF-ğ‘„

Mean Squared Error

Order Parameter

Ï„ > Ï„ C : Ï„ = 2 .0

0

10

0.2

Figure 4: The order parameter at equilibrium v.s. temperature in the Ising model with 20 Ã— 20 grid.

0

Ï„ ~ Ï„ C : Ï„ = 1 .2

0

0.4

0.0
0.0

0.0

Ï„ < Ï„ C : Ï„ = 0 .9

0

MCMC
MF-Q

15000

0.0
20000

(b) ğœ = 1.2

Figure 5: Training performance of MF-ğ‘„ in the Ising model
with 20 Ã— 20 grid.
ğ‘ ğ‘— vehicles into the main road. Controllers are expected to
coordinate with each other to make the full use of the main
route while avoiding congestions. The goal of each agent
is to learn to allocate system resources efficiently, avoiding
either over-use or under-use. The GS problem here sits
ideally as an ablation study on the impact of multi-agent
exploratory noises toward the learning (Colby et al., 2015).
Model Settings. We implement MF-ğ‘„ and MF-AC following the framework of centralized training (shared critic) with
decentralized execution (independent actor). We compare
against 4 baseline models: (1) Independent Learner (IL),
a traditional ğ‘„-Learning algorithm that does not consider
the actions performed by other agents; (2) Frequency Maximum ğ‘„-value (FMQ) (Kapetanakis & Kudenko, 2002), a
modified IL which increases the ğ‘„-values of actions that
frequently produced good rewards in the past; (3) Recursive
Frequency Maximum ğ‘„-value (Rec-FMQ) (Matignon et al.,
2012), an improved version of FMQ that recursively computes the occurrence frequency to evaluate and then choose
actions; (4) Multi-agent Actor-Critic (MAAC), a variant of
MADDPG architecture for the discrete action space (see
Eq. (4) in Lowe et al. (2017b)). All models use the multilayer perception as the function approximator. The detailed
settings of the implementation are in the Appendix C.1.
Results. Figure. 3 illustrates the results for the GS environment of ğœ‡ = 400 and ğœ = 200 with three different
numbers of agents (ğ‘ = 100, 500, 1000) that stand for 3
levels of congestions. In the smallest GS setting of Fig. 3a,
all models show excellent performance. As the agent number increases, Figs. 3b and 3c show MF-ğ‘„ and MF-ACâ€™s
capabilities of learning the optimal allocation effectively
after a few iterations, whereas all four baselines fail to learn
at all. We believe this advantage is due to the awareness
of other agentsâ€™ actions under the mean field framework;

Ï„ ~ Ï„ C : Ï„ = 1 .2

0

5

5

5

10

10

10

15

15

15

0

5

10

15

0

5

10

Ï„ > Ï„ C : Ï„ = 2 .0

0

15

0

5

10

15

(b) MCMC

Figure 6: The spins of the Ising model at equilibrium under
different temperatures.
such mechanism keeps the interactions among agents manageable while reducing the noisy effect of the exploratory
behaviors from the other agents. Between MF-ğ‘„ and MFAC, MF-ğ‘„ converges faster. Both FMQ and Rec-FMQ fail
to reach pleasant performance, it might be because agents
are essentially unable to distinguish the rewards received for
the same actions, and are thus unable to update their own
ğ‘„-values w.r.t. the actual contributions. It is worth noting
that MAAC is surprisingly inefficient in learning when the
number of agents becomes large; it simply fails to handle
the non-aggregated noises due to the agentsâ€™ explorations.
5.2. Model-free MARL for Ising Model
Environment. In statistical mechanics, the Ising model
is a mathematical framework to describe ferromagnetism
(Ising, 1925). It also has wide applications in sociophysics
(Galam & Walliser, 2010). With the energy function explicitly defined, mean field approximation (Stanley, 1971) is a
typical way
P to solve the Ising model for every spin ğ‘—, i.e.
hğ‘ ğ‘— i = ğ‘ ğ‘ ğ‘— ğ‘ƒ(ğ‘). See the Appendix C.2 for more details.
To fit into the MARL setting, we transform the Ising model
into a stage game where the
Preward for each spin/agent is
defined by ğ‘Ÿ ğ‘— = â„ ğ‘— ğ‘ ğ‘— + ğœ†2 ğ‘˜âˆˆN( ğ‘—) ğ‘ ğ‘— ğ‘ ğ‘˜ ; here N( ğ‘—) is the
set of nearest neighbors of spin ğ‘—, â„ ğ‘— âˆˆ R is the external
field affecting the spin ğ‘—, and ğœ† âˆˆ R is an interaction coefficient that determines how much the spins are motivated
to stay aligned. Unlike the typical setting in physics, here
each spin does not know the energy function, but aims to
understand the environment, and to maximize its reward by
learning the optimal policy of choosing the spin state: up or
down.
In addition to the reward, the order parameter (OP) (Stanley,
1971) is a traditional measure of purity for the Ising model.
|ğ‘ âˆ’ğ‘ |
OP is defined as ğœ‰ = â†‘ ğ‘ â†“ , where ğ‘â†‘ represents the
number of up spins, and ğ‘â†“ for the down spins. The closer
the OP is to 1, the more orderly the system is.
Model Settings. To validate the correctness of the MFğ‘„ learning, we implement MCMC methods (Binder et al.,

Mean Field Multi-Agent Reinforcement Learning
1.4

âˆ’200

0.8
0.6
0.4

âˆ’600

0.2
MFAC
MFQ

âˆ’800
0

(a) Battle game scene.

1.0

âˆ’400

250 500 750 1000 1250 1500 1750 2000
Epoch

(b) Learning curve.

Figure 7: The battle game: 64 v.s. 64.
1993) to simulate the same Ising model and provide the
ground truth for comparison. The full settings of MCMC
and MF-ğ‘„ for Ising model are provided in the Appendix
C.2. One of the learning goals is to obtain the accurate
approximation of hğ‘ ğ‘— i. Notice that agents here do not know
exactly the energy function, but rather use the temporal
difference learning to approximate hğ‘ ğ‘— i during the learning
procedure. Once this is accurately approximated, the Ising
model as a whole should be able to converge to the same
simulation result suggested by MCMC.
Correctness of MF-ğ‘„. Figure. 4 illustrates the relationship
between the order parameter at equilibrium under different
system temperatures. MF-ğ‘„ converges nearly to the exact
same plot as MCMC, this justifies the correctness of our
algorithms. Critically, MF-ğ‘„ finds a similar Curie temperature (the phase change point) as MCMC that is ğœ = 1.2. As
far as we know, this is the first work that manages to solve
the Ising model via model-free reinforcement learning methods. Figure. 5 illustrates the mean squared error between the
learned ğ‘„-value and the reward target. MF-ğ‘„ is shown in
Fig. 5a to be able to learn the target well under low temperature settings. When it comes to the Curie temperature, the
environment enters into the phase change when the stochasticity dominates, resulting in a lower OP and higher MSE
observed in Fig. 5b. We visualize the equilibrium in Fig. 6.
The equilibrium points from MF-ğ‘„ in fact match MCMCâ€™s
results under three types of temperatures. The spins tend
to stay aligned under a low temperature (ğœ = 0.9). As the
temperature rises (ğœ = 1.2), some spins become volatile
and patches start to form as spontaneous magnetization.
This phenomenon is mostly observed around the Curie temperature. After passing the Curie temperature, the system
becomes unstable and disordered due to the large thermal
fluctuations, resulting in random spinning patterns.
5.3. Mixed Cooperative-Competitive Battle Game
Environment. The Battle game in the Open-source MAgent system (Zheng et al., 2018) is a Mixed CooperativeCompetitive scenario with two armies fighting against each
other in a grid world, each empowered by a different RL
algorithm. In the setting of Fig. 7a, each army consists of
64 homogeneous agents. The goal of each army is to get
more rewards by collaborating with teammates to destroy
all the opponents. Agent can takes actions to either move to
or attack nearby grids. Ideally, the agents army should learn

0.0

400

vs MF-AC
vs AC

vs IL
vs MF-Q

350
Total-Reward

Win-Rate

Reward

0

Start

vs IL
vs MF-Q

1.2

200

300

vs MF-AC
vs AC

250
200
150
100
50

AC

IL

MF-Q

MF-AC

(a) Average wining rate.

0

AC

IL

MF-Q

MF-AC

(b) Average total reward.

Figure 8: Performance comparisons in the battle game.
skills such as chasing to hunt after training. We adopt the
default reward setting: âˆ’0.005 for every move, 0.2 for attacking an enemy, 5 for killing an enemy, âˆ’0.1 for attacking
an empty grid, and âˆ’0.1 for being attacked or killed.
Model Settings. Our MF-ğ‘„ and MF-AC are compared
against the baselines that are proved successful on the MAgent platform. We focus on the battles between mean field
methods (MF-ğ‘„, MF-AC) and their non-mean field counterparts, independent ğ‘„-learning (IL) and advantageous actor
critic (AC). We exclude MADDPG/MAAC as baselines,
as the framework of centralized critic cannot deal with the
varying number of agents for the battle (simply because
agents could die in the battle). Also, as we demonstrated
in the previous experiment of Fig. 3, MAAC tends to scale
poorly and fail when the agent number is in hundreds.
Results and Discussion. We train all four models by 2000
rounds self-plays, and then use them for comparative battles.
During the training, agents can quickly pick up the skills
of chasing and cooperation to kill in Fig. 7a. The Fig. 8
shows the result of winning rate and the total reward over
2000 rounds cross-comparative experiments. It is evident
that on all the metrics mean field methods, MF-ğ‘„ largely
outperforms the corresponding baselines, i.e. IL and AC respectively, which shows the effectiveness of the mean field
MARL algorithms. Interestingly, IL performs far better
than AC and MF-AC (2nd block from the left in Fig. 8a),
although it is worse than the mean field counterpart MF-ğ‘„.
This might imply the effectiveness of off-policy learning
with shuffled buffer replay in many-agent RL towards a
more stable learning process. Also, the ğ‘„-learning family
tends to introduce a positive bias (Hasselt, 2010) by using
the maximum action value as an approximation for the maximum expected action value, and such overestimation can
be beneficial for each single agent to find the best response
to others even though the environment itself is still changing.
On the other hand, On-policy methods need to comply with
the GLIE assumption (Assumption 2 in Sec 3.3) so as to
converge properly to the optimal value (Singh et al., 2000),
which is in the end a greedy policy as off-policy methods.
Figure. 7b further shows the self-play learning curves of
MF-AC and MF-ğ‘„. MF-ğ‘„ presents a faster convergence
speed than MF-AC, which is consistent with the findings in
the Gaussian Squeeze task (see Fig. 3b & 3c). Apart from
64, we further test the scenarios when the agent size is 8,
144, 256, the comparative results keep the same relativity as
Fig. 8; we omit the presentations for clarity.

Mean Field Multi-Agent Reinforcement Learning

6. Conclusions
In this paper, we developed mean field reinforcement learning methods to model the dynamics of interactions in the
multi-agent systems. MF-ğ‘„ iteratively learns each agentâ€™s
best response to the mean effect from its neighbors; this effectively transform the many-body problem into a two-body
problem. Theoretical analysis on the convergence of the MFğ‘„ algorithm to Nash ğ‘„-value was provided. Three types of
tasks have justified the effectiveness of our approaches. In
particular, we report the first result to solve the Ising model
using model-free reinforcement learning methods.

Acknowledgement
We sincerely thank Ms. Yi Qu for her generous help on the
graphic design.

References
Bertsekas, D. P. Weighted sup-norm contractions in dynamic
programming: A review and some new applications. Dept.
Elect. Eng. Comput. Sci., Massachusetts Inst. Technol.,
Cambridge, MA, USA, Tech. Rep. LIDS-P-2884, 2012.
Binder, K., Heermann, D., Roelofs, L., Mallinckrodt, A. J.,
and McKay, S. Monte carlo simulation in statistical
physics. Computers in Physics, 7(2):156â€“157, 1993.
Blume, L. E. The statistical mechanics of strategic interaction. Games and economic behavior, 5(3):387â€“424,
1993.
Bowling, M. and Veloso, M. Rational and convergent learning in stochastic games. In International joint conference on artificial intelligence, volume 17, pp. 1021â€“1026.
Lawrence Erlbaum Associates Ltd, 2001.
Bowling, M. and Veloso, M. Multiagent learning using
a variable learning rate. Artificial Intelligence, 136(2):
215â€“250, 2002.
Busoniu, L., Babuska, R., and De Schutter, B. A comprehensive survey of multiagent reinforcement learning.
IEEE Trans. Systems, Man, and Cybernetics, Part C, 38
(2):156â€“172, 2008.
Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, H. Learning
to rank: from pairwise approach to listwise approach.
In Proceedings of the 24th international conference on
Machine learning, pp. 129â€“136. ACM, 2007.
Colby, M. K., Kharaghani, S., HolmesParker, C., and Tumer,
K. Counterfactual exploration for improving multiagent
learning. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
pp. 171â€“179. International Foundation for Autonomous
Agents and Multiagent Systems, 2015.

de Cote, E. M. and Littman, M. L. A polynomial-time nash
equilibrium algorithm for repeated stochastic games. In
McAllester, D. A. and MyllymÃ¤ki, P. (eds.), UAI 2008,
pp. 419â€“426. AUAI Press, 2008. ISBN 0-9749039-4-9.
Fink, A. M. et al. Equilibrium in a stochastic ğ‘›-person game.
Journal of science of the hiroshima university, series ai
(mathematics), 28(1):89â€“93, 1964.
Foerster, J. N., Chen, R. Y., Al-Shedivat, M., Whiteson, S.,
Abbeel, P., and Mordatch, I. Learning with opponentlearning awareness. CoRR, abs/1709.04326, 2017.
Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and
Whiteson, S. Counterfactual multi-agent policy gradients.
In McIlraith & Weinberger (2018).
Galam, S. and Walliser, B. Ising model versus normal
form game. Physica A: Statistical Mechanics and its
Applications, 389(3):481â€“489, 2010.
Gupta, J. K., Egorov, M., and Kochenderfer, M. Cooperative
multi-agent control using deep reinforcement learning. In
AAMAS, pp. 66â€“83. Springer, 2017.
Hasselt, H. V. Double q-learning. In NIPS, pp. 2613â€“2621,
2010.
He, H. and Boyd-Graber, J. L. Opponent modeling in
deep reinforcement learning. In Balcan, M. and Weinberger, K. Q. (eds.), ICML, volume 48, pp. 1804â€“1813.
JMLR.org, 2016.
HolmesParker, C., Taylor, M., Zhan, Y., and Tumer, K.
Exploiting structure and agent-centric rewards to promote
coordination in large multiagent systems. In Adaptive
and Learning Agents Workshop, 2014.
Hu, J. and Wellman, M. P. Nash q-learning for general-sum
stochastic games. Journal of machine learning research,
4(Nov):1039â€“1069, 2003.
Huang, M., MalhamÃ©, R. P., Caines, P. E., et al. Large population stochastic dynamic games: closed-loop mckeanvlasov systems and the nash certainty equivalence principle. Communications in Information & Systems, 6(3):
221â€“252, 2006.
Ising, E. Beitrag zur theorie des ferromagnetismus.
Zeitschrift fÃ¼r Physik, 31(1):253â€“258, 1925.
Jaakkola, T., Jordan, M. I., and Singh, S. P. Convergence of
stochastic iterative dynamic programming algorithms. In
NIPS, pp. 703â€“710, 1994.
Jeong, S. H., Kang, A. R., and Kim, H. K. Analysis of
game botâ€™s behavioral characteristics in social interaction
networks of mmorpg. In ACM SIGCOMM Computer
Communication Review, volume 45, pp. 99â€“100. ACM,
2015.

Mean Field Multi-Agent Reinforcement Learning

Kapetanakis, S. and Kudenko, D. Reinforcement learning
of coordination in cooperative multi-agent systems. In
NCAI, pp. 326â€“331, Menlo Park, CA, USA, 2002. ISBN
0-262-51129-0.

Peng, P., Yuan, Q., Wen, Y., Yang, Y., Tang, Z., Long,
H., and Wang, J. Multiagent bidirectionally-coordinated
nets for learning to play starcraft combat games. CoRR,
abs/1703.10069, 2017.

Konda, V. R. and Tsitsiklis, J. N. Actor-critic algorithms. In
Solla, S. A., Leen, T. K., and MÃ¼ller, K. (eds.), Advances
in Neural Information Processing Systems 12, pp. 1008â€“
1014. MIT Press, 2000.

Rendle, S. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST), 3
(3):57, 2012.

Kreyszig, E. Introductory functional analysis with applications, volume 1. wiley New York, 1978.
Lasry, J.-M. and Lions, P.-L. Mean field games. Japanese
journal of mathematics, 2(1):229â€“260, 2007.
Lemke, C. E. and Howson, Jr, J. T. Equilibrium points of
bimatrix games. Journal of the Society for Industrial and
Applied Mathematics, 12(2):413â€“423, 1964.
Littman, M. L. Markov games as a framework for multiagent reinforcement learning. In ICML, volume 157, pp.
157â€“163, 1994.
Littman, M. L. Friend-or-foe q-learning in general-sum
games. In ICML, volume 1, pp. 322â€“328, 2001.
Littman, M. L. and Stone, P. A polynomial-time nash equilibrium algorithm for repeated games. Decision Support
Systems, 39(1):55â€“66, 2005.
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P.,
and Mordatch, I. Multi-agent actor-critic for mixed
cooperative-competitive environments. In NIPS, pp. 6382â€“
6393, 2017a.
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I. Multi-agent actor-critic for mixed cooperativecompetitive environments. In Guyon, I., von Luxburg, U.,
Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan,
S. V. N., and Garnett, R. (eds.), NIPS, pp. 6382â€“6393,
2017b.
Matignon, L., Laurent, G. J., and Le Fort-Piat, N. Independent reinforcement learners in cooperative markov
games: a survey regarding coordination problems. The
Knowledge Engineering Review, 27(1):1â€“31, 2012.
McIlraith, S. A. and Weinberger, K. Q. (eds.). Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7,
2018, 2018. AAAI Press.
Melo, F. S., Meyn, S. P., and Ribeiro, M. I. An analysis
of reinforcement learning with function approximation.
In Proceedings of the 25th international conference on
Machine learning, pp. 664â€“671. ACM, 2008.
Panait, L. and Luke, S. Cooperative multi-agent learning:
The state of the art. AAMAS, 11(3):387â€“434, 2005.

Shapley, L. S. Stochastic games. Proceedings of the national
academy of sciences, 39(10):1095â€“1100, 1953.
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, pp. 387â€“395, 2014.
Singh, S., Jaakkola, T., Littman, M. L., and SzepesvÃ¡ri,
C.
Convergence results for single-step on-policy
reinforcement-learning algorithms. Machine learning,
38(3):287â€“308, 2000.
Stanley, H. E. Phase transitions and critical phenomena.
Clarendon, Oxford, 9, 1971.
SzepesvÃ¡ri, C. and Littman, M. L. A unified analysis of
value-function-based reinforcement-learning algorithms.
Neural computation, 11(8):2017â€“2060, 1999.
Tan, M. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pp. 330â€“337,
1993.
Troy, C. A. Envisioning stock trading where the brokers are
bots. New York Times, 16, 1997.
van der Wal, J., van der Wal, J., van der Wal, J., MathÃ©maticien, P.-B., van der Wal, J., and Mathematician,
N. Stochastic Dynamic Programming: successive approximations and nearly optimal strategies for Markov
decision processes and Markov games. Mathematisch
centrum, 1981.
Wang, J., Zhang, W., Yuan, S., et al. Display advertising
with real-time bidding (rtb) and behavioural targeting.
Foundations and TrendsÂ® in Information Retrieval, 11
(4-5):297â€“435, 2017.
Watkins, C. J. and Dayan, P. Q-learning. Machine learning,
8(3-4):279â€“292, 1992.
Weintraub, G. Y., Benkard, L., and Van Roy, B. Oblivious
equilibrium: A mean field approximation for large-scale
dynamic games. In Advances in neural information processing systems, pp. 1489â€“1496, 2006.
Yang, J., Ye, X., Trivedi, R., Xu, H., and Zha, H. Deep
mean field games for learning optimal behavior policy of
large populations. CoRR, abs/1711.03156, 2017.

Mean Field Multi-Agent Reinforcement Learning

Zheng, L., Yang, J., Cai, H., Zhou, M., Zhang, W., Wang, J.,
and Yu, Y. Magent: A many-agent reinforcement learning
platform for artificial collective intelligence. In McIlraith
& Weinberger (2018).

Mean Field Multi-Agent Reinforcement Learning

A. Detailed mean field reinforcement learning algorithms
We published the code at https://github.com/mlii/mfrl.
Algorithm 1 Mean Field ğ‘„-learning (MF-ğ‘„)
Initialise ğ‘„ ğœ™ ğ‘— , ğ‘„ ğœ™ ğ‘— , and ğ‘Â¯ ğ‘— for all ğ‘— âˆˆ {1, . . . , ğ‘}
âˆ’
while training not finished do
for ğ‘š = 1, ..., ğ‘€ do
For each agent ğ‘—, sample action ğ‘ ğ‘— from ğ‘„ ğœ™ ğ‘— by Eq. (12), with the current mean action ğ‘Â¯ ğ‘— and the exploration rate ğ›½
For each agent ğ‘—, compute the new mean action ğ‘Â¯ ğ‘— by Eq. (11)
Take the joint action ğ’‚ = [ğ‘ 1 , . . . , ğ‘ ğ‘ ] and observe the reward ğ’“ = [ğ‘Ÿ 1 , . . . , ğ‘Ÿ ğ‘ ] and the next state ğ‘ 0
Store hğ‘ , ğ’‚, ğ’“, ğ‘ 0 , ğ’‚Â¯i in replay buffer D, where ğ’‚Â¯ = [Â¯
ğ‘ 1 , . . . , ğ‘Â¯ğ‘ ]
for ğ‘— = 1 to ğ‘ do
Sample a minibatch of ğ¾ experiences hğ‘ , ğ’‚, ğ’“, ğ‘ 0 , ğ’‚Â¯i from D
ğ‘—
ğ‘—
Sample action ğ‘ âˆ’
from ğ‘„ ğœ™ ğ‘— with ğ‘Â¯âˆ’
â† ğ‘Â¯ ğ‘—
âˆ’

Set ğ‘¦ ğ‘— = ğ‘Ÿ ğ‘— + ğ›¾ ğ‘£ MF
(ğ‘ 0 ) by Eq. (10)
ğœ™ğ‘—

âˆ’
2
P ğ‘—
Update the ğ‘„-network by minimizing the loss L(ğœ™ ğ‘— ) = ğ¾1
ğ‘¦ âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘  ğ‘— , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )
Update the parameters of the target network for each agent ğ‘— with learning rate ğœ:

ğ‘—
ğ‘—
ğœ™âˆ’
â† ğœğœ™ ğ‘— + (1 âˆ’ ğœ)ğœ™âˆ’

Algorithm 2 Mean Field Actor-Critic (MF-AC)
Initialize ğ‘„ ğœ™ ğ‘— , ğ‘„ ğœ™ ğ‘— , ğœ‹ ğœƒ ğ‘— , ğœ‹ ğœƒ ğ‘— , and ğ‘Â¯ ğ‘— for all ğ‘— âˆˆ {1, . . . , ğ‘}
âˆ’
âˆ’
while training not finished do
For each agent ğ‘—, sample action ğ‘ ğ‘— = ğœ‹ ğœƒ ğ‘— (ğ‘ ); compute the new mean action ğ’‚Â¯ = [Â¯
ğ‘ 1 , . . . , ğ‘Â¯ğ‘ ]
1
ğ‘
1
ğ‘
Take the joint action ğ’‚ = [ğ‘ , . . . , ğ‘ ] and observe the reward ğ’“ = [ğ‘Ÿ , . . . , ğ‘Ÿ ] and the next state ğ‘ 0
Store hğ‘ , ğ’‚, ğ’“, ğ‘ 0 , ğ’‚Â¯i in replay buffer D
for ğ‘— = 1 to ğ‘ do
Sample a minibatch of ğ¾ experiences hğ‘ , ğ’‚, ğ’“, ğ‘ 0 , ğ’‚Â¯i from D
Set ğ‘¦ ğ‘— = ğ‘Ÿ ğ‘— + ğ›¾ ğ‘£ MF
(ğ‘ 0 ) by Eq. (10)
ğœ™ğ‘—

âˆ’
2
P ğ‘—
Update the critic by minimizing the loss L(ğœ™ ğ‘— ) = ğ¾1
ğ‘¦ âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )
Update the actor using the sampled policy gradient:
1 X
ğ‘—
ğ‘—
âˆ‡ ğœƒ ğ‘— J(ğœƒ ğ‘— ) â‰ˆ
âˆ‡ ğœƒ ğ‘— log ğœ‹ ğœƒ ğ‘— (ğ‘ 0 )ğ‘„ ğœ™ ğ‘— (ğ‘ 0 , ğ‘ âˆ’
, ğ‘Â¯âˆ’
) ğ‘—
âˆ’
ğ¾
ğ‘âˆ’ = ğœ‹ ğ‘— (ğ‘  0 )
ğœƒ
âˆ’

Update the parameters of the target networks for each agent ğ‘— with learning rates ğœğœ™ and ğœğœƒ :
ğ‘—
ğ‘—
ğœ™âˆ’
â† ğœğœ™ ğœ™ ğ‘— + (1 âˆ’ ğœğœ™ )ğœ™âˆ’
ğ‘—
ğ‘—
ğœƒâˆ’
â† ğœğœƒ ğœƒ ğ‘— + (1 âˆ’ ğœğœƒ )ğœƒ âˆ’

Mean Field Multi-Agent Reinforcement Learning

B. Proof of the bound for the remainder term in Eq. 7
Recall Eq. (8) that we approximate the action ğ‘ ğ‘˜ taken by the neighboring agent ğ‘˜ with the mean action ğ‘Â¯ calculated from
the neighborhood N( ğ‘—). The state ğ‘  and the action ğ‘ ğ‘— of the central agent ğ‘— can be considered as fixed parameters; the
indices ğ‘—, ğ‘˜ of agents are essentially irrelevant to the derivation. With those omitted for simplicity, We rewrite the expression
of the pairwise ğ‘„-function as ğ‘„(ğ‘) , ğ‘„ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘ ğ‘˜ ).
Suppose that ğ‘„ is ğ‘€-smooth, where its gradient âˆ‡ğ‘„ is Lipschitz-continuous with constant ğ‘€ such that for all ğ‘, ğ‘Â¯
kâˆ‡ğ‘„(ğ‘) âˆ’ âˆ‡ğ‘„(Â¯
ğ‘ )k2 â‰¤ ğ‘€kğ‘ âˆ’ ğ‘Â¯k2 ,

(20)

where k Â· k2 indicates the â„“2 -norm.
With the Lagrangeâ€™s mean value theorem, we have
âˆ‡ğ‘„(ğ‘) âˆ’ âˆ‡ğ‘„(Â¯
ğ‘ ) = âˆ‡ğ‘„(Â¯
ğ‘ + 1 Â· (ğ‘ âˆ’ ğ‘Â¯)) âˆ’ âˆ‡ğ‘„(Â¯
ğ‘ ) = âˆ‡2 ğ‘„(Â¯
ğ‘ + ğœ– Â· (ğ‘ âˆ’ ğ‘Â¯)) Â· (ğ‘ âˆ’ ğ‘Â¯),

where ğœ– âˆˆ [0, 1].

(21)

Take the â„“2 -norm on the both sides of the above equation, it follows from the smoothness condition that
kâˆ‡ğ‘„(ğ‘) âˆ’ âˆ‡ğ‘„(Â¯
ğ‘ )k2 = kâˆ‡2 ğ‘„(Â¯
ğ‘ + ğœ Â· (ğ‘ âˆ’ ğ‘Â¯)) Â· (ğ‘ âˆ’ ğ‘Â¯)k2 â‰¤ ğ‘€kğ‘ âˆ’ ğ‘Â¯k2 .

(22)

Define ğ›¿ğ‘ , ğ‘ âˆ’ ğ‘Â¯ and the normalized vector ğ›¿ ğ‘Ë† , ğ‘âˆ’ğ‘Â¯/kğ‘âˆ’ğ‘Â¯k2 with kğ›¿ ğ‘Ë†k2 = 1, it follows from the above inequality
kâˆ‡2 ğ‘„(ğ‘ + ğœ Â· ğ›¿ğ‘) Â· ğ›¿ ğ‘Ë†k2 â‰¤ ğ‘€.

(23)

By arbitrary choice of (the unnormalized vector) ğ›¿ğ‘ such that the magnitude kğ›¿ğ‘k2 â†’ 0, it follows from above that
kâˆ‡2 ğ‘„(ğ‘) Â· ğ›¿ ğ‘Ë†k2 â‰¤ ğ‘€.

(24)

By aligning (the normalized vector) ğ›¿ ğ‘Ë† in the direction of the eigenvectors of the Hessian matrix âˆ‡2 ğ‘„, we can obtain for
any eigenvalue ğœ† of âˆ‡2 ğ‘„ that
kâˆ‡2 ğ‘„(ğ‘) Â· ğ›¿ ğ‘Ë†k2 = kğœ† Â· ğ›¿ ğ‘Ë†k2 = |ğœ†| Â· kğ›¿ ğ‘Ë†k2 â‰¤ ğ‘€,

(25)

which indicates that all eigenvalues of âˆ‡2 ğ‘„ can be bounded in the symmetric interval [âˆ’ğ‘€, ğ‘€].
As the Hessian matrix âˆ‡2 ğ‘„ is real symmetric and hence diagonalizable, there exist an orthogonal matrix ğ‘ˆ such that
ğ‘ˆ > [âˆ‡2 ğ‘„]ğ‘ˆ = ğ›¬ , diag[ğœ†1 , . . . , ğœ† ğ· ]. It then follows that
2

>

ğ›¿ğ‘ Â· âˆ‡ ğ‘„ Â· ğ›¿ğ‘ = [ğ‘ˆğ›¿ğ‘] ğ›¬[ğ‘ˆğ›¿ğ‘] =

ğ·
X

ğœ†ğ‘– [ğ‘ˆğ›¿ğ‘]2ğ‘– ,

with âˆ’ ğ‘€kğ‘ˆğ›¿ğ‘k2 â‰¤

ğ‘–=1

ğ·
X

ğœ†ğ‘– [ğ‘ˆğ›¿ğ‘]2ğ‘– â‰¤ ğ‘€kğ‘ˆğ›¿ğ‘k2

(26)

ğ‘–=1

Recall the definition ğ›¿ğ‘ = ğ‘ âˆ’ ğ‘Â¯ in Eq. (6), where ğ‘ is the one-hot encoding for ğ· actions, and ğ‘Â¯ is a ğ·-dimensional
multinomial distribution. It can be shown that
kğ‘ˆğ›¿ğ‘k2 = kğ›¿ğ‘k2 = (ğ‘ âˆ’ ğ‘Â¯)> (ğ‘ âˆ’ ğ‘Â¯) = ğ‘ > ğ‘ + ğ‘Â¯> ğ‘Â¯ âˆ’ ğ‘Â¯> ğ‘ âˆ’ ğ‘ > ğ‘Â¯ = 2(1 âˆ’ ğ‘Â¯ğ‘– ) â‰¤ 2,

(27)

where ğ‘– represents the specific action ğ‘ has represented such that ğ‘ ğ‘–0 = 0 for ğ‘– 0 6= ğ‘–.
ğ‘—
ğ‘˜
With all elements assembled, we have proved that each single remainder term ğ‘…ğ‘ ,ğ‘
ğ‘— (ğ‘ ) in Eq. (8) is bounded in [âˆ’2ğ‘€, 2ğ‘€].

Mean Field Multi-Agent Reinforcement Learning

C. Experiment details
C.1. Gaussian Squeeze
IL, FMQ, Rec-FMQ and MF-ğ‘„ all use a three-layer MLP to approximate ğ‘„-value. All agents share the same ğ‘„-network
for each experiment. The shared ğ‘„-network takes an agent embedding as input and computes ğ‘„-value for each candidate
action. For MF-ğ‘„, we also feed in the action approximation ğ‘Â¯. We use the Adam optimizer with a learning rate of
0.00001 and ğœ–-greedy exploration unless otherwise specified. For FMQ, we set the exponential decay rate ğ‘  = 0.006, start
temperature max_temp=1000 and FMQ heuristic ğ‘ = 5. For Rec-FMQ, we set the frequency learning rate ğ›¼ ğ‘“ = 0.01.
MAAC and MF-AC use the Adam optimizer with a learning rate of 0.001 and 0.0001 for Critics and Actors respectively,
and ğœ = 0.01 for updating the target networks. We share the Critic among all agents in each experiment and feed in an agent
embedding as extra input. Actors are kept separate. The discounted factor ğ›¾ is set to be 0.95 and the mini-batch size is set to
be 200. The size of the replay buffer is 106 and we update the network parameters after every 500 samples added to the
replay buffer.
For all models, we use the performance of the joint-policy learned up to that point if learning and exploration were turned
off (i.e., take the greedy action w.r.t. the learned policy) to compare our method with the above baseline models.
C.2. Ising Model
An Ising model is defined as a stateless system with ğ‘ homogeneous sites on a finite square lattice. Each site determines
their individual spin ğ‘ ğ‘— to interact with each other and aims to minimize the system energy for a more stable environment.
The system energy is defined as
ğ¸(ğ‘, â„) = âˆ’

X
ğ‘—

(â„ ğ‘— ğ‘ ğ‘— +

ğœ† X ğ‘— ğ‘˜
ğ‘ ğ‘ )
2

(28)

ğ‘˜âˆˆN( ğ‘—)

where N( ğ‘—) is the set of nearest neighbors of site ğ‘—, â„ ğ‘— âˆˆ R is the external field affecting site ğ‘—, and ğœ† âˆˆ R is an interaction
term determines how much the sites tend to align in the same direction. The system is said to reach an equilibrium point
when the system energy is minimized, with the probability
exp (âˆ’ğ¸(ğ‘, â„)/ğœ)
ğ‘ƒ(ğ‘) = P
,
ğ‘ exp(âˆ’ğ¸(ğ‘, â„)/ğœ)

(29)

where ğœ is the system temperature. When the temperature rises beyond a certain point (the Curie temperature), the system
can no longer keep a stable form and a phase transition happens. As the ground-truth is known, we would be able to evaluate
the correctness of the ğ‘„-function learning when there is a large body of agents interacted.
P
The mean field theory provides an approximate solution to hğ‘ ğ‘— i = ğ‘ ğ‘ ğ‘— ğ‘ƒ(ğ‘) through a set of self-consistent mean field
equations


P
exp âˆ’[â„ ğ‘— ğ‘ ğ‘— + ğœ† ğ‘˜âˆˆN( ğ‘—) hğ‘ ğ‘˜ i]/ğœ

.
hğ‘ ğ‘— i =
(30)
P
1 + exp âˆ’[â„ ğ‘— ğ‘ ğ‘— + ğœ† ğ‘˜âˆˆN( ğ‘—) hğ‘ ğ‘˜ i]/ğœ
which can be solved iteratively by


P
exp âˆ’[â„ ğ‘— ğ‘ ğ‘— + ğœ† ğ‘˜âˆˆN( ğ‘—) hğ‘ ğ‘˜ i(ğ‘¡) ]/ğœ

,
hğ‘ ğ‘— i(ğ‘¡+1) =
P
1 + exp âˆ’[â„ ğ‘— ğ‘ ğ‘— + ğœ† ğ‘˜âˆˆN( ğ‘—) hğ‘ ğ‘˜ i(ğ‘¡) ]/ğœ

(31)

where ğ‘¡ represents the number of iterations.
To learn an optimal joint policy ğ… âˆ— for Ising model, we use the stateless ğ‘„-learning with mean field approximation (MF-ğ‘„),
defined as
ğ‘„ ğ‘— (ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) â† ğ‘„ ğ‘— (ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) + ğ›¼[ğ‘Ÿ ğ‘— âˆ’ ğ‘„ ğ‘— (ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )],

(32)

Mean Field Multi-Agent Reinforcement Learning

Algorithm 3 MCMC in Ising Model
initialize spin state ğ’‚ âˆˆ {âˆ’1, 1} ğ‘ for ğ‘ sites
while training not finished do
randomly choose site ğ‘— âˆˆ N( ğ‘—)
ğ‘—
flip the spin state for site ğ‘—: ğ‘ âˆ’
â† âˆ’ğ‘ ğ‘—
P
P
ğ‘—
compute neighbor energy ğ¸(ğ‘, â„) = âˆ’ ğ‘— (â„ ğ‘— ğ‘ ğ‘— + ğœ†2 ğ‘˜âˆˆN( ğ‘—) ğ‘ ğ‘— ğ‘ ğ‘˜ ) for ğ‘ ğ‘— and ğ‘ âˆ’
randomly choose ğœ– âˆ¼ ğ‘ˆ(0, 1)
ğ‘—
if exp((ğ¸(ğ‘ ğ‘— , â„) âˆ’ ğ¸(ğ‘ âˆ’
, â„))/ğœ) > ğœ– then
ğ‘—
ğ‘—
ğ‘ â† ğ‘âˆ’
where the mean ğ‘Â¯ ğ‘— is given as the mean hğ‘ ğ‘— i from the last time step, and the individual reward is
ğ‘Ÿ ğ‘— = â„ğ‘—ğ‘ğ‘— +

ğœ† X ğ‘— ğ‘˜
ğ‘ ğ‘ .
2

(33)

ğ‘˜âˆˆN( ğ‘—)

To balance the trade-off between exploration and exploitation under low temperature settings, we use a policy with Boltzmann
exploration and a decayed exploring temperature. The temperature for Boltzmann exploration of MF-ğ‘„ is multiplied by a
decay factor exponentially through out the training process.
Without lost of generality, we assume ğœ† > 0, thus neighboring sites with the same action result in lower energy (observe
higher reward) and are more stable. Each site should also align with the sign of external field â„ ğ‘— to reduce the system energy.
For simplification, we eliminate the effect of external fields and assume the model to be discrete, i.e., âˆ€ ğ‘— âˆˆ ğ‘, â„ ğ‘— = 0, ğ‘ ğ‘— âˆˆ
{âˆ’1, 1}.
We simulate the Ising model using Metropolis Monte Carlo methods (MCMC). After initialization, we randomly change a
siteâ€™s spin state and calculate the energy change, select a random number between 0 and 1, and accept the state change only
if the number is less than ğ‘’
more probable spin states.

ğ‘—
(ğ¸ ğ‘— âˆ’ğ¸ )
âˆ’
ğœ

. This is called the Metropolis technique, which saves computation time by selecting the

C.3. Battle Game
IL and MF-ğ‘„ have almost the same hyper-parameters settings. The learning rate is ğ›¼ = 10âˆ’4 , and with a dynamic
exploration rate linearly decays from ğ›¾ = 1.0 to ğ›¾ = 0.05 during the 2000 rounds training. The discounted factor ğ›¾ is set to
be 0.95 and the mini-batch size is 128. The size of replay buffer is 5 Ã— 105 .
AC and MF-AC also have almost the same hyper-parameters settings. The learning rate is ğ›¼ = 10âˆ’4 , the temperature of
soft-max layer in ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ is ğœ = 0.1. And the coefficient of entropy in the total loss is 0.08, the coefficient of value in the total
loss is 0.1.

Mean Field Multi-Agent Reinforcement Learning

D. Further details towards the theoretical guarantee of MF-ğ‘„
P
Proposition 1. Let the metric space be R ğ‘ and the metric be ğ‘‘(ğ’‚, ğ’ƒ) = ğ‘— |ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— |, for ğ’‚ = [ğ‘ ğ‘— ]1ğ‘ , ğ’ƒ = [ğ‘ ğ‘— ]1ğ‘ . If the
ğ‘„-function is ğ¾-Lipschitz continuous w.r.t. ğ‘ ğ‘— , then the operator B(ğ‘ ğ‘— ) , ğœ‹ ğ‘— (ğ‘ ğ‘— |ğ‘ , ğ‘Â¯ ğ‘— ) in Eq. (12) forms a contraction
mapping under sufficiently low temperature ğ›½.
Proof. Following the contraction mapping theorem (Kreyszig, 1978), in order to be a contraction, the operator has to satisfy:
ğ‘‘(B(ğ’‚), B(ğ’ƒ)) â‰¤ ğ›¼ğ‘‘(ğ’‚, ğ’ƒ), âˆ€ğ’‚, ğ’ƒ
where 0 â‰¤ ğ›¼ < 1 and B(ğ’‚) , [B(ğ‘ 1 ), . . . , B(ğ‘ ğ‘ )].
Here we start from binomial case and then adapt to the multinomial case in general. We first rewrite B(ğ‘ ğ‘— ) as

exp âˆ’ ğ›½ğ‘„ ğ‘¡ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )
ğ‘—
ğ‘—
ğ‘—
ğ‘—
B(ğ‘ ) = ğœ‹ (ğ‘ |ğ‘ , ğ‘Â¯ ) =


exp âˆ’ ğ›½ğ‘„ ğ‘¡ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) + exp âˆ’ ğ›½ğ‘„ ğ‘¡ğ‘— (ğ‘ , Â¬ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )
1
=
,
1 + exp (âˆ’ğ›½ Â· ğ›¥ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯))

(34)

where ğ›¥ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯) = ğ‘„(ğ‘ , ğ‘ Â¬ ğ‘— , ğ‘Â¯) âˆ’ ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯).
Then we have
1
1
âˆ’
1 + ğ‘’ âˆ’ğ›½Â·ğ›¥ğ‘„(ğ‘ ,ğ‘ ğ‘— , ğ‘Â¯)
1 + ğ‘’ âˆ’ğ›½Â·ğ›¥ğ‘„(ğ‘ ,ğ‘ ğ‘— , ğ‘Â¯)
ğ›½ğ‘’ âˆ’ğ›½ğ›¥ğ‘„0
=
ğ›¥ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯) âˆ’ ğ›¥ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯)
(1 + ğ‘’ âˆ’ğ›½ğ›¥ğ‘„0 )2
1
Â· ğ‘„(ğ‘ , ğ‘ Â¬ ğ‘— , ğ‘Â¯) âˆ’ ğ‘„(ğ‘ , ğ‘ Â¬ ğ‘— , ğ‘Â¯) + ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯) âˆ’ ğ‘„(ğ‘ , ğ‘ ğ‘— , ğ‘Â¯)
â‰¤
4ğ‘‡

1
â‰¤
Â· ğ¾ Â· 1 âˆ’ ğ‘ ğ‘— âˆ’ (1 âˆ’ ğ‘ ğ‘— ) + ğ¾ Â· ğ‘ ğ‘— âˆ’ ğ‘ ğ‘—
4ğ‘‡
X
1
â‰¤
Â· 2ğ¾ Â·
ğ‘ğ‘— âˆ’ ğ‘ğ‘— .
4ğ‘‡
ğ‘—

|B(ğ‘ ğ‘— ) âˆ’ B(ğ‘ ğ‘— )| =

(35)

In the second equation, we apply the mean value theorem in calculus: âˆƒğ‘¥ 0 âˆˆ [ğ‘¥1 , ğ‘¥2 ], s.t., ğ‘“ (ğ‘¥1 ) âˆ’ ğ‘“ (ğ‘¥2 ) = ğ‘“ 0 (ğ‘¥0 )(ğ‘¥ 1 âˆ’ ğ‘¥2 ).
In the third equation we use the maximum value for ğ‘’ âˆ’ğ›½ğ›¥ğ‘„0 /(1 + ğ‘’ âˆ’ğ›½ğ›¥ğ‘„0 )2 = 1/4 when ğ‘„ 0 = 0. In the last equation we
apply the Lipschitz constraint in the assumption where constant ğ¾ â‰¥ 0. Finally, we have:
ğ‘‘(B(ğ’‚), B(ğ’ƒ)) â‰¤
=

X
1
Â· 2ğ¾ Â·
ğ‘ğ‘— âˆ’ ğ‘ğ‘—
4ğ‘‡
ğ‘—
ğ¾
ğ‘‘(ğ’‚, ğ’ƒ)
2ğ‘‡

(36)

In order for the contraction to hold, ğ‘‡ > ğ¾2 . In other words, when the action space is binary for each agent, and the
temperature is sufficiently large, the mean field procedure converges.
This proposition can be easily extended to multinomial case by replacing binary variable ğ‘ ğ‘— by a multi-dimensional binary
indicator vector ğ’‚ ğ‘— , on each dimension, the rest of the derivations would remain essentially the same.
D.1. Discussion on Rationality
In line with (Bowling & Veloso, 2001; 2002), we argue that to better evaluate a multi-agent learning algorithm, on top of the
convergence guarantee, discussion on property of Rationality is also needed.
Property 1. (also see (Bowling & Veloso, 2001; 2002)) In an ğ‘-agent stochastic game defined in this paper, given all
agents converge to stationary policies, if the learning algorithm converges to a policy that is a best response to the other
agentsâ€™ policies, then the algorithm is Rationale.

Mean Field Multi-Agent Reinforcement Learning

Our mean field ğ‘„-learning is rational in that Eq. (5) converts many agents interactions into two-body interactions between a
single agent and the distribution of other agents actions. When all agents follow stationary policies, their policy distribution
would be stationary too. As such the two-body stochastic game becomes an MDP, and the agent would choose a policy
(based on Assumption 2) which is the best response to the distribution of other stationary policies. As agents are symmetric
in our case, they all show the best response to the distributions, and are therefore rational.

E. Proof of Mean Field Reinforcement Learning with Function Approximation
Previous convergence results in Theorem.1 has shown that the Mean Field Q-learning algorithm will converge when the Q
function is in a tabular case. We now move onto the proof that the MF-Q algorithm will converge when the Q function is
represented by some functional approximations.

An ğ‘-agent (or, ğ‘-player) stochastic game ğ›¤ is formalized by the tuple ğ›¤ , S, A1 , . . . , Ağ‘ , ğ‘Ÿ 1 , . . . , ğ‘Ÿ ğ‘ , ğ‘, ğ›¾ . The
state-space S is finite. Let (S, ğ‘ ğ… ) be the Markov chain induced by the joint policy ğ…, and we assume it to be uniformly
ergodic.
Let Q = {ğ‘¸ ğœƒ } be a family of real-valued functions defined on S Ã— A Ã— AÌ„, where AÌ„ is the action space for the mean
actions computed from the neighbors. Assuming that the function class is linearly parameterized, for each agent ğ‘—, Q can be
expressed as the linear span of a fixed set of ğ‘ƒ linearly independent functions ğœ” ğ‘ğ‘— : SÃ— AÃ— AÌ„ â†’ R. Given the parameter
vector ğœ™ ğ‘— âˆˆ R ğ‘ƒ , for each agent, the function ğ‘„ ğœ™ ğ‘— is thus defined as
ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) =

ğ‘ƒ
X

ğœ” ğ‘ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )ğœ™ ğ‘— (ğ‘) = ğœ” ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )> ğœ™ ğ‘—

(37)

ğ‘=1

In the functional approximation setting, we can apply the update rules:
ğ‘—
ğœ™ğ‘¡+1
= ğœ™ğ‘¡ğ‘— + ğ›¼ğ‘¡ ğ›¥ğ‘¡ âˆ‡ ğœ™ ğ‘— ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— )

= ğœ™ğ‘¡ğ‘— + ğ›¼ğ‘¡ ğ›¥ğ‘¡ ğœ” ğ‘— (ğ‘ , ğ‘ ğ‘¡ğ‘— , ğ‘Â¯ğ‘¡ğ‘— )

(38)

In the above, ğ›¥ğ‘¡ is the temporal difference at time ğ‘¡.
0
ğ‘—
Â¯ğ‘— )
ğ›¥ğ‘¡ = ğ‘Ÿ ğ‘— + ğ›¾ ğ‘£ MF
ğœ™ ğ‘— (ğ‘  ) âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ , ğ‘


= ğ‘Ÿ ğ‘— + ğ›¾ Eğ’‚âˆ¼ğ… MF ğ‘„ ğœ™ ğ‘— (ğ‘ 0 , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ) âˆ’ ğ‘„ ğœ™ ğ‘— (ğ‘ , ğ‘ ğ‘— , ğ‘Â¯ ğ‘— ).

(39)
(40)

And the goal is to derive the parameter vector ğ“ = {ğœ™ ğ‘— } such that ğ> ğ“ approximates the (local) Nash Q-values. At each
time step, the learning policy ğ… ğœ™ğ‘¡ is the Botlzmann policy with respect to ğ> ğ“. Give the Proposition 1, we know that the
ğ¾
policy ğ… ğœ™ğ‘¡ is 2ğ‘‡
Lipschitz continuous with respect to ğœ™ğ‘¡ .
Similar to the framework used in the convergence proof of Q-learning with function approximation (Melo et al., 2008), we
establish convergence of Eq. (38) by adopting an ordinary differentiable equation (ODE) with a globally asymptotically
stable equilibrium point where the trajectories closely follow.
ğ¾
Theorem 2. Given the MDP ğ›¤, ğ… ğœ™ğ‘¡ , {ğ ğ‘ , ğ‘ = 1, ..., ğ‘ƒ}, and the learning policy ğ… ğœ™ğ‘¡ that is 2ğ‘‡
Lipschitz continuous with
respect to ğœ™ğ‘¡ , if the Assumptions 1, 2 & 3, and Lemma 2â€™s first and second conditions are met, then there exists ğ¶0 such that
ğ¾
the algorithm in Eq. (38) converges w.p.1 if 2ğ‘‡
< ğ¶0 .

Proof. We first re-write the Eq. (38) as on ODE:


ğ‘‘ğ“
0
>
>
= Eğ“ ğ>
ğ‘  ğ’“(ğ‘ , ğ‘, ğ‘  ) + ğ›¾ğ ğ‘  0 ğ“ âˆ’ ğ ğ‘  ğ“
ğ‘‘ğ‘¡


 >

>
>
0
= Eğ“ ğ>
ğ‘  (ğ›¾ğ ğ‘  0 âˆ’ ğ ğ‘  ) ğ“ + Eğ“ ğ ğ‘  (ğ’“(ğ‘ , ğ‘, ğ‘  ))
= ğ‘¨ğœ™ ğ“ + ğ’ƒ ğœ™

(41)

Notice that we use a vector for considering the updating rule for the Q function of each agent. We can easily know that
necessity condition of the equilibrium is that it must follow ğ“âˆ— = ğ‘¨âˆ’1
ğœ™ âˆ— ğ’ƒ ğœ™ âˆ— . The existence of the such equilibrium has been
restricted in the scenario that meets Assumption 3. In the proof of Theorem 1 we have already pointed out that under the

Mean Field Multi-Agent Reinforcement Learning

Assumption 3, the existing equilibrium, either in the form of global equilibrium or in the form of saddle-point equilibrium,
is unique.
Let ğ“Ëœ = ğ“ğ‘¡ âˆ’ ğ“âˆ— , we have:
ğ‘‘ Ëœ
ğ‘‘ğ“
ğ‘‘ğ“
||ğ“||2 = 2ğ“ Â·
âˆ’ 2ğ“âˆ— Â·
ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
>
Ëœ
= 2ğ“ (ğ‘¨ ğœ™ ğ“ + ğ’ƒ ğœ™ âˆ’ ğ‘¨ ğœ™âˆ— ğ“âˆ— âˆ’ ğ’ƒ ğœ™âˆ— )
= 2ğ“Ëœ> (ğ‘¨ ğœ™âˆ— ğ“ âˆ’ ğ‘¨ ğœ™âˆ— ğ“ + ğ‘¨ ğœ™ ğ“ + ğ’ƒ ğœ™ âˆ’ ğ‘¨ ğœ™âˆ— ğ“âˆ— âˆ’ ğ’ƒ ğœ™âˆ— )
= 2ğ“Ëœ> ğ‘¨ ğœ™âˆ— ğ“Ëœ + 2ğ“Ëœ> (ğ‘¨ ğœ™ âˆ’ ğ‘¨ ğœ™âˆ— )ğ“ + 2ğ“Ëœ> (ğ’ƒ ğœ™ âˆ’ ğ’ƒ ğœ™âˆ— )


||ğ’ƒ ğœ™ âˆ’ ğ’ƒ âˆ—ğœ™ ||2
>
Ëœ
âˆ—
âˆ—
ğ“Ëœ
â‰¤ 2ğ“
ğ‘¨ ğœ™ + sup ||ğ‘¨ ğœ™ âˆ’ ğ‘¨ ğœ™ ||2 + sup
||ğ“ âˆ’ ğ“âˆ— ||2
ğ“
ğ“

(42)

As we know that the policy ğ… ğœ™ğ‘¡ is Lipschitz w.r.t ğœ™ğ‘¡ , this implies that ğ‘¨ ğœ™ and ğ’— ğœ™ are also Lipschitz continuğ¾
ous w.r.t to ğœ™. In other words, if 2ğ‘‡
â‰¤ ğ¶0 is sufficiently small and close to zero, then the norm term of


âˆ—
||ğ’ƒ ğœ™ âˆ’ ğ’ƒ ğœ™ ||2
supğ“ ||ğ‘¨ ğœ™ âˆ’ ğ‘¨ ğœ™âˆ— ||2 + supğ“
goes to zero. Considering near the equilibrium point ğœ™âˆ— , ğ‘¨ ğœ™âˆ— is a nega||ğ“ âˆ’ ğ“âˆ— ||2
tive definite matrix, the Eq. (42) tends to be negative definite as well, so the ODE in Eq.(41) is globally asymptotically
stable and the conclusion of the theorem follows.

