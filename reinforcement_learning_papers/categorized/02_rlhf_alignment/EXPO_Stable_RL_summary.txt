Here is the extracted information:

**PAPER:** Stable Reinforcement Learning with Expressive Policies
**TITLE:** EXPO: Expressive Policy Optimization
**RESEARCH_METHOD:** 02_rlhf_alignment (Reinforcement Learning with Expressive Policies)
**METHOD_DESCRIPTION:** EXPO is a sample-efficient online RL algorithm that utilizes an on-the-fly policy to maximize value with two parameterized policies â€“ a larger expressive base policy trained with a stable imitation learning objective and a light-weight Gaussian edit policy that edits the actions sampled from the base policy toward a higher value distribution.
**KEY_CONTRIBUTIONS:**
* EXPO achieves up to 2-3x improvement in sample efficiency on average over prior methods in both online RL and offline-to-online RL settings.
* EXPO can effectively leverage offline data for online RL and fine-tune pretrained policies without experiencing a large drop in performance.
* The on-the-fly policy extraction in the TD backup and action edits are crucial for EXPO's performance.
* EXPO can learn effectively even without retaining the offline dataset after pre-training, using the pre-trained policy as a strong prior for fine-tuning.
