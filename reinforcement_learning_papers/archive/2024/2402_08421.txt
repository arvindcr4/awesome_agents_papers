1

Conservative and Risk-Aware Offline Multi-Agent
Reinforcement Learning

arXiv:2402.08421v2 [cs.LG] 16 Nov 2024

Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, and Hirley Alves

Abstract‚ÄîReinforcement learning (RL) has been widely
adopted for controlling and optimizing complex engineering
systems such as next-generation wireless networks. An important
challenge in adopting RL is the need for direct access to the physical environment. This limitation is particularly severe in multiagent systems, for which conventional multi-agent reinforcement
learning (MARL) requires a large number of coordinated online
interactions with the environment during training. When only
offline data is available, a direct application of online MARL
schemes would generally fail due to the epistemic uncertainty
entailed by the lack of exploration during training. In this
work, we propose an offline MARL scheme that integrates
distributional RL and conservative Q-learning to address the
environment‚Äôs inherent aleatoric uncertainty and the epistemic
uncertainty arising from the use of offline data. We explore
both independent and joint learning strategies. The proposed
MARL scheme, referred to as multi-agent conservative quantile
regression, addresses general risk-sensitive design criteria and is
applied to the trajectory planning problem in drone networks,
showcasing its advantages.
Index Terms‚ÄîOffline multi-agent reinforcement learning, distributional reinforcement learning, conservative Q-learning, UAV
networks

I. I NTRODUCTION
A. Context and Motivation
Recent advances in machine learning (ML) and artificial
intelligence (AI), high-performance computing, cloudification,
and simulation intelligence [1] have supported the development of data-driven paradigms for the engineering of complex
systems, such as wireless networks [2], [3]. Reinforcement
Eslam Eldeeb and Hirley Alves are with Centre for Wireless Communications (CWC), University of Oulu, Finland. (e-mail: eslam.eldeeb@oulu.fi;
hirley.alves@oulu.fi). Houssem Sifaou and Osvaldo Simeone are with the
King‚Äôs Communications, Learning & Information Processing (KCLIP) lab
within the Centre for Intelligent Information Processing Systems (CIIPS),
Department of Engineering, King‚Äôs College London, WC2R 2LS London,
U.K. (e-mail: houssem.sifaou@kcl.ac.uk; osvaldo.simeone@kcl.ac.uk). Mohammad Shehab is with CEMSE Division, King Abdullah University of
Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia (email:
mohammad.shehab@kaust.edu.sa).
The work of E. Eldeeb and H. Alves was partially supported by the
Research Council of Finland (former Academy of Finland) 6G Flagship
Programme (Grant Number: 346208) and by the European Commission
through the Hexa-X-II (GA no. 101095759). The work of H. Sifaou and O.
Simeone was partially supported by the European Union‚Äôs Horizon Europe
project CENTRIC (101096379). O. Simeone was also supported by the
Open Fellowships of the EPSRC (EP/W024101/1) by the EPSRC project
(EP/X011852/1), and by Project REASON, a UK Government funded project
under the Future Open Networks Research Challenge (FONRC) sponsored by
the Department of Science Innovation and Technology (DSIT).
The second author has contributed to the problem definitions and the
experiments. The third author has had an active role in defining the problems,
as well as in writing the text, while the last two authors have had a supervisory
role and have revised the text.

Fig. 1: Consider access to data collected offline following some fixed and
unknown policies œÄŒ≤ = {œÄŒ≤i }Ii=1 in an environment consisting of I agents.
Based on this dataset, the goal is to optimize policies œÄ = {œÄ i }Ii=1 for the
agents while ensuring robustness to the uncertainty arising from the stochastic
environment, from the limited data, and from the lack of interactions with the
environment.

learning (RL) is a particularly appealing methodology for settings requiring dynamic decision-making, for which feedback
can be distributed automatically or via human judgement [4],
[5]. An important challenge in adopting RL solutions is
their reliance on online interaction with the environment.
This limitation is particularly severe in multi-agent systems,
for which conventional multi-agent reinforcement (MARL)
requires a large number of coordinated online interactions with
the environment [6].
When only data collected offline is available, a direct
application of online MARL schemes would generally fail due
to the epistemic uncertainty entailed by the limited availability
of data. In particular, even in the case of a single agent, offline
reinforcement learning, which relies only on offline data,
may over-estimate the quality of given actions that happened
to perform well during data collection due to the inherent
stochasticity and outliers of the environment [7]. This problem
can be addressed in online RL via exploration, trying actions,
and modifying return estimates based on environmental feedback. However, as mentioned, exploration is not feasible in
offline RL, as policy design is based solely on the offline
dataset. Furthermore, in multi-agent systems, this problem is
exacerbated by the inherent uncertainty caused by the nonstationary behavior of other agents during training [8, Chapter
11].
In this paper, we propose a novel offline MARL strategy,

2

multi-agent conservative quantile regression (MA-CQR), that
addresses the overall uncertainty caused by the use of offline data. The introduced approaches are termed multi-agent
conservative independent quantile regression (MA-CIQR) via
independent learning and multi-agent conservative centralized
quantile regression (MA-CCQR) via joint training. These
approaches integrate distributional RL [8] and offline RL [9]
to support a risk-sensitive multi-agent design that mitigates
impairments arising from access to limited data from the
environment. We showcase the performance of MA-CIQR
and MA-CCQR by focusing on the problem of designing
trajectories of unmanned aerial vehicles (UAVs) used to collect
data from sensors in an Internet-of-Things (IoT) scenario [10]‚Äì
[12] (see Fig. 1). It is noted that this work considers the worst
case in which agents can only use offline data without relying
on an internal model of the environment. Future work may
investigate settings in which the agents have prior information
about the environment that can be used to learn a world model
via methods such as model-based offline RL [13] (see Sec. I.B
for a review).
B. Related Work
Offline RL: Offline RL has gained increasing interest in recent
years due to its wide applicability to domains where online
interaction with the environment is impossible or presents high
costs and risks. Offline RL relies on a static offline transition
dataset collected from the environment using some behavioral
policy. The behavioral policy is generally suboptimal and
may be unknown to the designer [7]. The reliance on a
suboptimal policy for data collection distinguishes offline RL
from imitation learning, in which the goal is reproducing the
behavior of an expert policy [14]. The discrepancy between the
behavior and optimized policies creates a distributional shift
between training data and design objective. This shift could be
resolved by collecting more data, but this is not possible in an
offline setting. Therefore, the distributional shift contributes to
the epistemic uncertainty of the agent.
Several approaches have been proposed to address this
problem in offline RL. One class of methods constrains the
difference between the learned and behavior policies [15]. Another popular approach is to learn conservative estimates of the
action-value function or Q-function. Specifically, conservative
Q-learning (CQL), proposed in [9], penalizes the values of
the Q-function for out-of-distribution (OOD) actions. OOD
actions are those whose impact is not sufficiently covered by
the dataset.
Other works have leveraged offline RL with model-based
RL by exploiting information about the physical environment [13], [16], [17]. The work in [13] proposed a modelbased offline reinforcement learning framework that first learns
the transition dynamics of the environment from the offline
data and then optimizes the policy. To address the distributional shift arising in offline RL, reference [16] modified conventional model-based RL schemes by penalizing the rewards
by the amount of uncertainty regarding the environment dynamics. In [17], a model-based solution for offline MARL was
developed for coordination-insensitive settings. The approach

consists of learning a world model from the dataset and using
it to optimize the agents‚Äô policies.
Regarding applications of offline RL to wireless systems,
the recent work [18] investigated a radio resource management
problem by comparing the performance of several single-agent
offline RL algorithms.
Distributional RL: Apart from offline RL via CQL, the proposed scheme builds on distributional RL (DRL), which is
motivated by the inherent aleatoric uncertainty caused by
the stochasticity of the environment [19]‚Äì[21]. Rather than
targeting the average return as in conventional RL, DRL
maintains an estimate of the distribution of the return. This
supports the design of risk-sensitive policies that disregard
gains attained via risky behavior, favoring policies that ensure
satisfactory worst-case performance levels instead.
A popular risk measure for use in DRL is the conditional
value at risk (CVaR) [21]‚Äì[24], which evaluates the average
performance by focusing only on the lower tail of the return
distribution. Furthermore, a state-of-the-art DRL strategy is
quantile-regression deep Q-network (QR-DQN), which approximates the return distribution by estimating N uniformly
spaced quantiles [20].
Offline MARL: Recently, several works have been proposed
that adapt the idea of conservative offline learning to the
context of multi-agent systems (offline MARL) [25]‚Äì[29].
Specifically, conservative estimates of the value function in a
decentralized fashion are obtained in [25] via value deviation
and transition normalization. Several other works proposed
centralized learning approaches. The authors in [26] leveraged
first-order policy gradients to calculate conservative estimates
of the agents‚Äô value functions. The work [27] presented
a counterfactual conservative approach for offline MARL,
while [28] introduced a framework that converts global-level
value regularization into equivalent implicit local value regularization. The authors in [29] addressed the overestimation
problem using implicit constraints.
Overall, all of these works focused on risk-neutral objectives, hence not making any provisions to address risk-sensitive
criteria. In this regard, the paper [24] combined distributional
RL and conservative Q-learning to develop a risk-sensitive
algorithm, but only for single-agent settings.
Applications of MARL to wireless systems: Due to the multiobjective and multi-agent nature of many control and optimization problems in wireless networks, MARL has been adopted
as a promising solution in recent years. For instance, related to
our contribution, the work in [30] proposed an online MARL
algorithm to jointly minimize the age-of-information (AoI) and
the transmission power in IoT networks with traffic arrival
prediction, whereas the authors in [31] leveraged MARL for
AoI minimization in UAV-to-device communications. Moreover, MARL was used in [32] for resource allocation in UAV
networks. The work [33] developed a MARL-based solution
for optimizing power allocation dynamically in wireless systems. The authors in [34] used MARL for distributed resource
management and interference mitigation in wireless networks
and in [35], edge-end task division, transmit power, computing
resource type matching and allocation are jointly optimized
using a MARL algorithm.

3

Applications of Distributional RL to wireless systems: Distributional RL has been recently leveraged in [36] to carry
out the optimization for a downlink multi-user communication system with a base station assisted by a reconfigurable
intelligent reflector (IR). Meanwhile, reference [37] focused
on the case of mmWave communications with IRs on a UAV.
Distributional RL has also been used in [38] for resource
management in network slicing. The paper [24] combined
distributional RL and conservative Q-learning to develop a
risk-sensitive algorithm, but only for single-agent settings.
All in all, to the best of our knowledge, our work in this
paper is the first to integrate conservative offline RL and
distributional MARL, and it is also the first to investigate the
application of offline MARL to wireless systems.
C. Main Contributions
This work introduces MA-CQR, a novel offline MARL
scheme that supports optimizing risk-sensitive design criteria
such as CVaR. MA-CQR is evaluated on the relevant problem
of UAV trajectory design for IoT networks. The contributions
of this paper are summarized as follows.
‚Ä¢ We propose MA-CQR, a novel conservative and distributional offline MARL solution. MA-CQR leverages quantile regression (QR) to support the optimization of risksensitive design criteria and CQL to ensure robustness
to OOD actions. As a result, MA-CQR addresses both
the epistemic uncertainty arising from the presence of
limited data and the aleatoric uncertainty caused by the
randomness of the environment.
‚Ä¢ We present two versions of MA-CQR with different levels
of coordination among the agents. In the first version, referred to as MA-CIQR, the agents‚Äô policies are optimized
independently. In the second version, referred to as MACCQR, we leverage value decomposition techniques that
allow joint training [39], [40].
‚Ä¢ To showcase the proposed schemes, we consider a trajectory optimization problem in UAV networks [30]. As
illustrated in Fig. 1, the system comprises multiple UAVs
collecting information from IoT devices. The multiobjective design tackles the minimization of the AoI for
data collected from the devices and the overall transmit
power consumption. We specifically exploit MA-CQR to
design risk-sensitive policies that avoid excessively risky
trajectories in the pursuit of larger average returns.
‚Ä¢ Numerical results demonstrate that MA-CIQR and MACCQR versions yield faster convergence and higher returns than the baseline algorithms. Furthermore, both
schemes can avoid risky trajectories and provide the
best worst-case performance. Experiments also depict
that centralized training provides faster convergence and
requires less offline data.
The rest of the paper is organized as follows. Section II describes the MARL setting and the design objective. Section III
introduces distributional RL and conservative Q-Learning. In
section IV, we present the proposed MA-CIQR algorithm
using independent training, whereas section V presents the
proposed MA-CCQR algorithm using centralized training. In

TABLE I: Abbreviations
AoI
CDF
CQL
CVaR
DQN
DRL
MA-CCQL
MA-CCQR
MA-CIQL
MA-CIQR
MA-CQL
MA-CQR
MA-DCQN
MA-DIQN
MA-DQN
MA-QR-DCQN
MA-QR-DIQN
MA-QR-DQN
MARL
OOD
QR-DQN
UAV

Age-of-information
Cumulative distribution function
Conservative Q-learning
Conditional value at risk
Deep Q-network
Distributional reinforcement learning
Multi-agent conservative centralized Q-learning
Multi-agent conservative centralized quantile regression
Multi-agent conservative independent Q-learning
Multi-agent conservative independent quantile regression
Multi-agent conservative Q-learning
Multi-agent conservative quantile regression
Multi-agent deep centralized Q-network
Multi-agent deep independent Q-network
Multi-agent deep Q-network
Multi-agent quantile regression deep centralized Q-network
Multi-agent quantile regression deep independent Q-network
Multi-agent quantile regression deep Q-network
Multi-agent reinforcement learning
Out-of-distribution
Quantile-regression deep Q-network
Unmanned aerial vehicles

I
st
at
ait
rt
Œ≥
Q(s, a)
Z(s, a)
P (st+1 |st , at )
œÄ i (ait | st )
PZ(s,a)
R(rt | st , at )
Œæ
JŒæCVaR

FZ‚àí1
œÄ Œæ
D
Œ∏ji (s, a)
Œ∂œÑ (u)
i(k)
‚àÜjj ‚Ä≤

Number of agents
Overall state of the environment at time step t
Joint action of all agents at time step t
Action of agent i at time step t
Immediate reward at time step t
Discount factor
Q-function
Return starting from (s, a)
Transition probability
Policy of agent i
Distribution of the return
Stationary reward distribution
Risk tolerance level
CVaR risk measure
Inverse CDF of the return
Offline dataset collected
Quantile estimate of the distribution PZ i (s,a) (Œ∏ i )
Quantile regression Huber loss
TD errors evaluated with the quantile estimates
of agent i
CQL hyperparameter
Number of devices in the system
AoI of device m at time step t
Channel gain between agent i and device m
at time step t
Transmission power for device m to communicate with
agent i at time step t
Risk probability
Risk penalty

TABLE II: Notations

Œ±
M
Am
t
gti,m
Pti,m
prisk
Prisk

Section VI, we provide numerical experiments on trajectory
optimization in UAV networks. Section VII concludes the
paper.
II. P ROBLEM D EFINITION
In this section, we describe the multi-agent setting and formulate the problem. This discussion will be also instrumental
in introducing the necessary notation, which will be leveraged
in the following section to introduce important background
information. We consider the setting illustrated in Fig. 1,
where I agents act in a physical environment that evolves in
discrete time as a function of the agents‚Äô actions and random
dynamics. The design of the agents‚Äô policies œÄ = {œÄ i }Ii=1

is carried out at a central unit in Fig. 1 that has only access
to a fixed dataset D, while not being able to interact with the
physical system. The dataset D is collected offline by allowing
the agents to act in the environment according to arbitrary,
fixed, and generally unknown policies œÄŒ≤ = {œÄŒ≤i }Ii=1 . In this
section, we describe the multi-agent setting and formulate the
offline learning problem. Tables I and II summarize the list of
abbreviations and notations.

Return

4

CVaR [Z]

A. Multi-Agent Setting
Consider an environment characterized by a time-variant
state st , where t = 1, 2, ... is the discrete time index. At
time step t, each agent i takes action ait ‚àà Ai within some
discrete action space Ai . We denote by at = a1t , ¬∑ ¬∑ ¬∑ , aIt
the vector of actions of all agents at timestep t. The state st
evolves according to a transition probability P (st+1 |st , at ) as
a function of the current state st and of the action vector at .
The transition probability P (st+1 |st , at ) is stationary, i.e., it
does not vary with time index t.
We focus on a fully observable multi-agent reinforcement
learning setting, in which each agent i has access to the full
system state st and produces action ait by following a policy
œÄ i (ait | st ).
B. Design Goal
The desired goal is to find the optimal policies œÄ‚àó (a|s) =
I
maximize a risk measure œÅ(¬∑) of the return
{œÄ‚àói (a|s)}
P‚àûi=1 that
œÄ
Z = t=0 Œ≥ t rt , which we write as
JœÅ (œÄ) = œÅ [Z œÄ ] ,

(1)

where 0 < Œ≥ < 1 is a given discount factor. The distribution
of the return Z œÄ depends on the policies œÄ through the
distribution of the trajectory T = (s0 , a0 , r0 , s1 , a1 , r1 , ...),
which is given by
P (T ) = P (s0 )

‚àû
Y

œÄ(at | st )R(rt | st , at )P (st+1 | st , at ),

t=0

(2)
QI
with œÄ(at | st ) = i=1 œÄ i (ait | st ) being the joint conditional
distribution of the agents‚Äô actions; P (s0 ) being a fixed initial
distribution; and R(rt | st , at ) being the stationary reward
distribution.
The standard choice for the risk measure in (1) is the
expectation œÅ[¬∑] = E[¬∑], yielding the standard criterion
 
(3)
J avg (œÄ) = E Z œÄ .

The average criterion in (3) is considered to be risk neutral,
as it does not directly penalize worst-case situations, catering
only to the average performance.
In stochastic environments where the level of aleatoric
uncertainty caused by the transition probability and/or the
reward distribution is high, maximizing the expected return
may not be desirable since the return Z œÄ has high variance.
In such scenarios, designing risk-sensitive policies may be
preferable to enhance the worst-case outcomes while reducing
the average performance (3).

Fig. 2: Illustration of the conditional value-at-risk (CVaR). The quantile
function FZ‚àí1 (Œæ) is plotted as a function of the risk tolerance level Œæ. The
shaded area representing the lower tail of the distribution depicts the Œæ-level
CVaR.

A common risk-sensitive measure is the conditional valueat-risk (CVaR) [22], which is defined as the conditional mean


JŒæCVaR (œÄ) = E Z œÄ | Z œÄ ‚â§ FZ‚àí1
,
(4)
œÄ Œæ

where FZ‚àí1
is the inverse cumulative distribution function
œÄ Œæ
(CDF) of the return Z œÄ for some Œæ ‚àà [0, 1], i.e., the Œæ-th
quantile of the distribution of the return. The CVaR, illustrated
in Fig. 2, focuses on the lower tail of the return distribution
by neglecting values
 of the return that are larger than the Œæth quantile FZ‚àí1
œÄ Œæ . Accordingly, the probability Œæ represents
the risk tolerance level, with Œæ = 1 recovering the risk-neutral
objective (3). The CVaR can also be written as the integral of
the quantile function FZ‚àí1
œÄ (Œæ) as
Z
1 Œæ ‚àí1
JŒæCVaR (œÄ) =
F œÄ (u)du.
(5)
Œæ 0 Z
C. Offline Multi-Agent Reinforcement Learning
Conventional MARL [41] assumes that agents optimize
their policies œÄ = {œÄ i (a | s)}Ii=1 via an online interaction with
the environment, allowing for the exploration of new actions
at as a function the state st . In this paper, as illustrated in
Fig. 1, we assume that the design of policies is carried out
on the basis solely of the availability of an offline dataset
D = {(s, a, r, s‚Ä≤ )} of transitions (s, a, r, s‚Ä≤ ). Each transition
follows the stationary marginal distribution from (2), with
policy œÄ(a|s)
QIgiven by the fixed and unknown behavior policy
œÄŒ≤ (a|s) = i=1 œÄŒ≤i (ai |s).
III. BACKGROUND

In this section, we present a brief review of distributional
RL, as well as of offline RL via CQL for a single agent
model [9]. This material will be useful to introduce the
proposed multi-agent offline DRL solution in the next section.

5

The neural network is trained by minimizing the loss

A. Distributional Reinforcement Learning
Distributional RL aims at optimizing the agent‚Äôs policy,
œÄ, while accounting for the inherent aleatoric uncertainty
associated with the stochastic environment. To this end, it
tracks the return‚Äôs distribution, allowing the minimization of
an arbitrary risk measure, such as the CVaR.
To elaborate, let us denote the random variable representing
the return starting from a given state-action pair (s, a) as
Z œÄ (s, a). Taking the expectation of the return Z œÄ (s, a) over
distribution (2) yields the state-action value function, also
known as Q-function, as
QœÄ (s, a) = E [Z œÄ (s, a)] .

(6)

Classical Q-learning algorithms learn the optimal policy œÄ ‚àó
by finding the optimal Q-function Q(s, a) as the unique fixed
point of the Bellman optimality operator [42]


‚Ä≤ ‚Ä≤
Q(s, a) = E r + Œ≥ max
Q(s , a ) ,
(7)
‚Ä≤

N
N

1 XX
Œ∂œÑÃÇj ‚àÜjj ‚Ä≤ ,
2
N j=1 ‚Ä≤
j =1

where ‚àÜjj ‚Ä≤ are the temporal difference (TD) errors corresponding to the quantile estimates, i.e.,
‚àÜjj ‚Ä≤ = r + Œ≥Œ∏j ‚Ä≤ (s‚Ä≤ , a‚Ä≤ ) ‚àí Œ∏j (s, a),
(11)
PN
with a‚Ä≤ = arg maxa‚ààA N1 j ‚Ä≤ =1 Œ∏j ‚Ä≤ (s‚Ä≤ , a), and Œ∂œÑ is the
quantile regression Huber loss defined as
(
if |u| ‚â§ 1
‚àí 21 u2 |œÑ ‚àí 1{u < 0}| ,

Œ∂œÑ (u) =
(12)
1
|u| ‚àí 2 |œÑ ‚àí 1{u < 0}| , otherwise.

We refer the reader to [20] for more details about the
theoretical guarantees and practical implementation of QRDQN.

a ‚ààA

with average evaluated with respect to the random variables
(r, s‚Ä≤ ) ‚àº R(r|s, a)P (s‚Ä≤ |s, a). The optimal policy œÄ ‚àó for the
average criterion (3) is directly obtained from the optimal Qfunction as


(8)
œÄ ‚àó (a|s) = 1 a = arg max Q(s, a) ,
a‚ààA

with 1{¬∑} being the indicator function.
Similarly, for any risk measure œÅ[¬∑], one can define the
distributional Bellman optimality operator for the random
return Z (s, a) as [19], [20]


D
‚Ä≤
‚Ä≤ ‚Ä≤
Z(s, a) = r + Œ≥ Z s , arg max
œÅ[Z(s , a )] ,
(9)
‚Ä≤
a ‚ààA

where equality holds regarding the distribution of the random
variables on the left- and right-hand sides, and the random
variables (r, s‚Ä≤ ) are distributed as in (7). The optimal policy
for the general criterion (1) can be expressed directly as a
function of the optimal Z(s, a) in (9) as [19], [20]


œÄ(a|s) = 1 a = arg max œÅ[Z(s, a)] .
a‚ààA

Quantile regression DQN (QR-DQN) [20] estimates the
distribution PZ(s,a) of the optimal return Z(s, a) by approximating it via a uniform mixture of Dirac functions centered
at N values {Œ∏j (s, a)}N
j=1 , i.e.,
N

1 X
Œ¥Œ∏ (s,a) .
PÃÇZ(s,a) (Œ∏) =
N j=1 j

(10)

Each value Œ∏j (s, a) in (10) is an estimate of the quantile
‚àí1
FZ(s,a)
(œÑÃÇj ) of distribution PZ(s,a) corresponding to the quantile target œÑÃÇj = (œÑj‚àí1 + œÑj )/2, with œÑj = j/N for 1 ‚â§ j ‚â§ N .
Note that {Œ∏j (s, a)}N
j=1 are estimated via quantile regression,
which is achieved by modeling the function mapping (s, a) to
the N values {Œ∏j (s, a)}N
j=1 as a neural network [20], which
takes a state as input, and outputs the estimated Œ∏j (s, a) for
all actions a ‚àà A .

B. Conservative Q-Learning
Conservative Q-learning (CQL) is a Q-learning variant that
addresses epistemic uncertainty in offline RL. Specifically, it
tackles the uncertainty arising from the limited available data,
which may cause some actions to be OOD due to the lack of
exploration. This way, CQL is complementary to QR-DQN,
which, instead, targets the inherent aleatoric uncertainty in the
stochastic environment.
To introduce CQL, let us first review conventional DQN [7],
which approximates the solution of the Bellman optimality
condition (7) by iteratively minimizing the Bellman loss
"
2 #
L(Q, QÃÇ(k) ) = EÃÇ r + Œ≥ max
QÃÇ(k) (s‚Ä≤ , a‚Ä≤ ) ‚àí Q(s, a)
,
‚Ä≤
a ‚ààA

(13)

where EÃÇ[¬∑] is the empirical average over samples (s, a, r, s‚Ä≤ )
from the offline dataset D; QÃÇ(k) is the current estimate of the
optimal Q-function Q at iteration k; and the optimization is
over function Q(s, a), which is typically modeled as a neural
network. The term r+Œ≥ maxa‚Ä≤ ‚ààA QÃÇ(k) (s‚Ä≤ , a‚Ä≤ )‚àíQ(s, a) is also
known as the TD-error. The only difference between offline
DQN, defined in (13), and online DQNs lies in the way training data are gathered. For online DQN, the data are collected
by interacting with the environment, while learning from the
replay buffer, by using stochastic gradient descent (SGD). In
contrast, offline DQN has access to a static offline dataset of
transitions, or trajectories, that were previously generated by
some unknown behavioral policy, and it learns the optimal
Q-function by minimizing the Bellman error on the offline
dataset over multiple epochs. In this work, we use the term
‚Äùoffline DQN‚Äù to refer to the basic DQN scheme designed
for an offline setting. This scheme does not incorporate any
modifications intended to mitigate extrapolation errors and
overestimation bias that may affect an offline implementation.
Considering the basic offline DQN scheme will help illustrate
the failure of conventional DQN methods in offline settings in
the experiments in Section VI.

6

The maximization over the actions in the TD error in
(13) may yield over-optimistic return estimates when the Qfunction is estimated using offline data. In fact, a large value
of the estimated maximum return maxa‚Ä≤ ‚ààA Q(s, a) may be
obtained based purely on the randomness in the environment
during data collection. This uncertainty could be resolved by
collecting additional data. However, this is not possible in an
offline setting, and hence one should consider such actions as
OOD [7], [43], and count the resulting uncertainty as part of
the epistemic uncertainty.
To account for this issue, the CQL algorithm adds a
regularization term to the objective in (13) that penalizes
excessively large deviations between the maximum estimated
return maxa‚Ä≤P
with the differentiable
‚ààA Q(s, a), approximated

quantity log aÃÉ‚ààA exp Q(s, aÃÉ) , and the average value of
Q(s, a) in the data set D as
1
(14)
LCQL (Q, QÃÇ(k) ) = L(Q, QÃÇ(k) )
2


X

+ Œ±EÃÇ log
exp Q(s, aÃÉ) ‚àí Q(s, a) ,
aÃÉ‚ààA

where Œ± > 0 is a hyperparameter [9].
A combination of QR-DQN and CQL was proposed in [24]
for a single-agent setting to address risk-sensitive objectives
in offline learning. This approach applies a regularization term
as in (14) to the distributional Bellman operator (9). The next
section will introduce an extension of this approach for the
multi-agent scenario under study in this paper.
IV. O FFLINE C ONSERVATIVE D ISTRIBUTIONAL MARL
WITH I NDEPENDENT T RAINING
This section proposes a novel offline conservative distributional independent Q-learning approach for MARL problems.
The proposed method combines the benefits of distributional
RL and CQL to address the risk-sensitive objective (1) in
multi-agent systems based on offline optimization as in Fig. 1.
The approaches studied here apply an independent Q-learning
approach, whereby learning is done separately for each agent.
The next section will study more sophisticated methods based
on joint training.
A. Multi-Agent Conservative Independent Q-Learning
We first present a multi-agent version of CQL, referred to as
multi-agent conservative independent Q-learning (MA-CIQL),
for the offline MARL problem. As in its single-agent version
described in the previous section, MA-CIQL addresses the
average criterion (3), aiming to mitigate the effect of epistemic
uncertainty caused by OOD actions.
To this end, each agent i maintains a separable Q-function
Qi (s, ai ), which is updated at each iteration k by approximately minimizing the loss
1
(15)
LMA-CIQL (Qi , QÃÇi(k) ) = L(Qi , QÃÇi(k) )
2

 X


+ Œ±EÃÇ log
exp(Qi (s, aÃÉi )) ‚àí Qi (s, ai ) ,
aÃÉi ‚ààAi

Algorithm 1: Conservative Independent Q-learning for
Offline MARL (MA-CIQL)
Input: Discount factor Œ≥, learning rate Œ∑, conservative
penalty constant Œ±, number of agents I, number of
training iterations K, number of gradient steps G,
and offline dataset D
Output: Optimized Q-functions Qi (s, ai ) for
i = 1, ..., I
Initialize network parameters
for iteration k in {1,...,K} do
for gradient step g in {1,...,G} do
Sample a batch B from the dataset D
for agent i in {1,...,I} do
Estimate the MA-CIQL loss LMA-CIQL
in (15)
Perform a stochastic gradient step based on
the estimated loss
end
end
end
Return Qi (s, ai ) = QÃÇi(K) (s, ai ) for i = 1, ..., I

which is the multi-agent version of (14) over the Q-function
Qi , where L(Qi , QÃÇi(k) ) is the DQN loss in (13) and QÃÇi(k)
is the estimate of the Q-function of agent i at the k-th
iteration. Algorithm 1 summarizes the MA-CIQL algorithm
for offline MARL. Note that the algorithm applies separately
to each agent and is thus an example of independent per-agent
learning.
B. Multi-Agent
Regression

Conservative

Independent

Quantile-

MA-CIQL can only target the average criterion (3), thus
not accounting for risk-sensitive objectives that account for
the inherent stochasticity of the environment. This section
introduces a risk-sensitive Q-learning algorithm for offline
MARL to address the more general design objective (4) for
some risk tolerance level Œæ.
The proposed approach, which we refer to as multi-agent
conservative independent quantile regression (MA-CIQR),
maintains an estimate of the lower tail of the distribution of
the return Z i (s, a), up to the risk tolerance level Œæ, for each
agent i. This is done in a manner similar to (10) by using N
estimated quantiles, i.e.,
N

PÃÇZ i (s,a) (Œ∏i ) =

1 X
Œ¥ i
.
N j=1 Œ∏j (s,a)

(16)

Generalizing (10), however, the quantity Œ∏ji (s, a) is an estimate
œÑj‚àí1 +œÑj
of the quantile FZ‚àí1
and œÑj = Œæj/N
i (s,a) (œÑÃÇj ), with œÑÃÇj =
2
for 1 ‚â§ j ‚â§ N . This way, only the quantiles of interest cover
the return distribution up to the Œæ-th quantile.
At each iteration k, each agent, i, updates the distribution (16) by minimizing a loss function that combines the
quantile loss used by QR-DQN and the conservative penalty

7

introduced by CQL. Specifically, the loss function of MACIQR is given by
N X
N


X
1
i(k)
LMA-CIQR (Œ∏ , Œ∏ÃÇ ) =
(17)
‚àÜ
EÃÇ
Œ∂
‚Ä≤
œÑÃÇ
j
jj
2N 2 j=1 ‚Ä≤
j =1
"
"
##
N
X

1 X
i
i
i
i
+ Œ±EÃÇ
log
exp Œ∏j (s, aÃÉ ) ‚àí Œ∏j (s, a ) ,
N j=1
i
i
i

i(k)

aÃÉ ‚ààA

where Œ∂œÑ (u) is the quantile regression Huber loss defined in
i(k)
(12) and ‚àÜjj ‚Ä≤ are the TD errors evaluated with the quantile
estimates as
i(k)

i(k)

‚àÜjj ‚Ä≤ = r + Œ≥ Œ∏ÃÇj ‚Ä≤ (s‚Ä≤ , a‚Ä≤i ) ‚àí Œ∏ji (s, ai ),
(18)
PN
i(k) ‚Ä≤ i
where a‚Ä≤i = arg maxai ‚ààAi N1
j ‚Ä≤ =1 Œ∏ÃÇj ‚Ä≤ (s , a ). Note that
i(k)
the TD error ‚àÜjj ‚Ä≤ is obtained by using the j ‚Ä≤ -th quantile of the current k-th iteration to estimate the return as
i(k)
r +Œ≥ Œ∏ÃÇj ‚Ä≤ (s‚Ä≤ , a‚Ä≤i ), while considering the j-th quantile Œ∏ji (s, ai )
as the quantity to be optimized.
The corresponding optimized policy is finally obtained as
Ô£º
Ô£±
N
Ô£Ω
Ô£≤
X
1
i ‚Ä≤ i
Œ∏
(s
,
a
)
. (19)
œÄ i (ai |s) = 1 ai = arg max
j
Ô£æ
Ô£≥
ai ‚ààAi N
j=1

By (5), the objective in (19) is an estimate of the CVaR at
the risk tolerance level Œæ. The pseudocode of the MA-CIQR
algorithm is provided in Algorithm 2. As for MA-CIQL, MACIQR applies separately across all agents.
V. O FFLINE C ONSERVATIVE D ISTRIBUTIONAL MARL
WITH C ENTRALIZED T RAINING

The independent learning strategies studied in the previous
section may fail to yield coherent policies across different
agents. This section addresses this issue by introducing joint
/ centralized methods based on value decomposition [39].
A. Multi-Agent Conservative Centralized Q-Learning
With value decomposition, it is assumed that the global Qfunction can be written as [39]
Q(s, a) =

I
X

QÃÉi (s, ai ),

(20)

i=1

where the function QÃÉi (s, ai ) indicates the contribution of
the i-th agent to the overall Q-function. For conventional
DQN, the Bellman loss (13) is minimized over the functions
{QÃÉi (s, ai )}Ii=1 . This problem corresponds to the minimization
of the global loss
"
I
X
max
QÃÇi(k) (s‚Ä≤ , ai )
L({QÃÉi }Ii=1 , {QÃÇi(k) }Ii=1 ) =EÃÇ r + Œ≥
i
i
i=1

‚àí

I
X
i=1

Algorithm 2: Conservative Independent Quantile Regression for Offline MARL (MA-CIQR)
Input: Discount factor Œ≥, learning rate Œ∑, number of
quantiles N , conservative penalty constant Œ±, number
of agents I, number of training iterations K, number
of gradient steps G, offline dataset D, and CVaR
parameter Œæ
Output: Optimized quantile estimates {Œ∏ji (s, ai )}N
j=1
for all i = 1, ..., I
Define œÑi = Œæi/N, i = 1, ..., N
Initialize network parameters for each agent
for iteration k in {1,...,K} do
for gradient step g in {1,...,G} do
Sample a batch B from the dataset D
for agent i in {1,...,I} do
for j in {1,...,N } do
for j ‚Ä≤ in {1,...,N } do
i(k)
Calculate TD errors ‚àÜjj ‚Ä≤ using
(18)
end
end
Estimate the MA-CIQR loss LMA-CIQR in
(17)
Perform a stochastic gradient step based on
the estimated loss
end
end
end
i(K)
(s, ai )}N
Return {Œ∏ji (s, ai )}N
j=1 for all
j=1 = {Œ∏ÃÇj
i = 1, ..., I
Algorithm 3: Conservative Centralized Q-learning for
Offline MARL (MA-CCQL)
Input: Discount factor Œ≥, learning rate Œ∑, conservative
penalty constant Œ±, number of agents I, number of
training iterations K, number of gradient steps G,
and offline dataset D
Output: Optimized Q-functions Qi (s, ai ) for
i = 1, ..., I
Initialize network parameters for each agent
for iteration k in {1,...,K} do
for gradient step g in {1,...,G} do
Sample a batch B from the dataset D
Estimate the MA-CCQL loss LMA-CCQL in (23)
Perform a stochastic gradient step to update the
network parameters of each agent
end
end
Return QÃÉi (s, ai ) = QÃÇi(K) (s, ai ), for i = 1, ..., I

aÃÉ ‚ààA

!2 #

QÃÉi (s, ai )

,

(21)

where QÃÇi(k) is the current estimate of the contribution of agent
i. In practice, every function QÃÉi (s, ai ) is approximated using

a neural network. Furthermore, the policy of each agent is
obtained from the optimized function QÃÉi (s, ai ) as


i
i
i
i i
QÃÉ (s, a ) .
(22)
œÄ (a |s) = 1 a = arg max
i
i
a ‚ààA

The same approach can be adopted to enhance MA-CIQL

8

Algorithm 4: Conservative Centralized Quantile Regression for Offline MARL (MA-CCQR)
Input: Discount factor Œ≥, learning rate Œ∑, number of
quantiles N , conservative penalty constant Œ±, number
of agents I, number of training iterations K, number
of gradient steps G, offline dataset D, and CVaR
parameter Œæ
Output: Optimized functions {Œ∏ÃÉji (s, ai )}N
j=1 for all
i = 1, ..., I
Define œÑi = Œæi/N, i = 1, ..., N
Initialize network parameters for each agent
for iteration k in {1,...,K} do
for gradient step g in {1,...,G} do
Sample a batch B from the dataset D
for j in {1,...,N } do
for j ‚Ä≤ in {1,...,N } do
(k)
Calculate global TD error ‚àÜjj ‚Ä≤
using (26)
end
end
Estimate the MA-CCQR loss LMA-CCQR in (25)
Perform a stochastic gradient step to update the
network parameters of each agent
end
end
i(K)
Return {Œ∏ÃÇj (s, ai )}N
j=1 , for i = 1, ..., I

Fig. 3: Multiple UAVs serve limited-power sensors to minimize power
expenditure while also minimizing the age of information for data retrieval
from the sensors. The environment is characterized by a risk region for
navigation of the UAVs in the middle of the grid world (colored area).

loss obtained by plugging the decomposition (24) into (17) to
obtain
i(k)

}Ii=1 }N
LMA-CCQR ({{Œ∏ÃÉji }Ii=1 }N
j=1 ) =
j=1 , {{Œ∏ÃÇj
N
N


XX
1
(k)
EÃÇ
Œ∂œÑÃÇj ‚àÜjj ‚Ä≤
(25)
2
2N
j=1 j ‚Ä≤ =1
"
"
##
I
N


X
X
1 X
i
i
i
i
log
exp Œ∏ÃÉj (s, aÃÉ ) ‚àí Œ∏ÃÉj (s, a ) ,
+ Œ±EÃÇ
N j=1
i=1
aÃÉ‚ààA

i(k)

by using (20) in the loss (15). This yields the loss
1
LMA-CCQL ({QÃÉi }Ii=1 , {QÃÇi }Ii=1 ) = L({QÃÉi }Ii=1 , {QÃÇi }Ii=1 )
2


 X
I 
X
exp(QÃÉi (s, aÃÉi )) ‚àí QÃÉi (s, ai ) , (23)
+ Œ±EÃÇ
log
i=1

aÃÉi ‚ààAi

with L({QÃÉi }Ii=1 , {QÃÇi }Ii=1 ) defined in (21). The obtained
scheme, whose steps are detailed in Algorithm 3, is referred
to as multi-agent conservative centralized Q-learning (MACCQL). The optimized policy is given in (22).
B. Multi-Agent Conservative Centralized Quantile-Regression
The joint training approach based on the value decomposition (20) can also be applied to MA-CIQR to obtain a centralized training version referred to as multi-agent conservative
centralized quantile regression (MA-CCQR).
To this end, we first recall that the lower tail of the
distribution of Z(s, a) is approximated by MA-CIQR as in
‚àí1
(16) using the estimates of the quantiles FZ(s,a)
(œÑÃÇj ), with
œÑj‚àí1 +œÑj
and œÑj = Œæj/N for 1 ‚â§ j ‚â§ N . To jointly
œÑÃÇj =
2
optimize the agents‚Äô policies, we decompose each quantile
Œ∏j (s, a) as
I
X
Œ∏ÃÉji (s, ai ),
(24)
Œ∏j (s, a) =
i=1

where Œ∏ÃÉji (s, ai ) represents the contribution of agent i. The
functions {{Œ∏ÃÉji (s, ai )}Ii=1 }N
j=1 are jointly optimized using a

where {{Œ∏ÃÇj }Ii=1 }N
j=1 represents the current estimate of the
(k)
contribution of agent i and ‚àÜjj ‚Ä≤ is given by
(k)

‚àÜjj ‚Ä≤ = r + Œ≥

I
I
X
X
i(k)
Œ∏ÃÉji (s, ai ),
Œ∏ÃÇj ‚Ä≤ (s‚Ä≤ , a‚Ä≤i ) ‚àí
i=1

(26)

i=1

P
i(k) ‚Ä≤ i
with a‚Ä≤i = arg maxai ‚ààAi N1 N
j ‚Ä≤ =1 Œ∏ÃÇj ‚Ä≤ (s , a ). The individual policies of the agent are finally obtained as
Ô£º
Ô£±
N
Ô£Ω
Ô£≤
X
1
Œ∏ÃÉji (s, ai ) .
œÄ i (ai |s) = 1 ai = arg max
Ô£æ
Ô£≥
ai ‚ààAi N
j =1

For each agent, the function that maps (s, ai ) to the N values
{Œ∏ÃÉji (s, a)}N
j=1 is modeled as a neural network and the steps of
the MA-CCQR scheme are provided in Algorithm 4.
VI. A PPLICATION : T RAJECTORY L EARNING IN UAV
N ETWORKS

In this section, we consider the application of offline MARL
to the trajectory optimization problem in UAV networks.
Following [44], as illustrated in Fig. 3, we consider multiple
UAVs acting as BSs to receive uplink updates from limitedpower sensors.
A. Problem Definition and Performance Metrics
Consider a grid world, as shown in Fig. 3, where each cell
is a square of length Lc . The system comprises a set M of M
uplink IoT devices deployed uniformly in the grid world. The
devices report their observations to I fixed-velocity rotarywing UAVs flying at height h and starting from positions

9

selected randomly on the grid. The grid world contains normal
cells, represented as white squares, and a risk region of special
cells, colored in the figure. The risk region can be an area with
a high probability of UAV collision and/or locations with a
high chance of signal blockages. The current position at each
time t of each UAV i is projected on the plane as coordinates
xit , yti . The goal is to determine trajectories for the UAVs
on the grid that jointly minimize the AoI and the transmission
powers across all the IoT devices.
The AoI measures the freshness of the information collected
by the UAVs from the devices [44]. For each device m, the
AoI is defined as the time elapsed since the last time data from
the device was collected by a UAV [45], [46]. Accordingly,
the AoI of device m at time t is updated as follows
(
1,
if Vtm = 1,
m
At =
(27)
m
min{Amax , At‚àí1 + 1},
otherwise;
where Amax is the maximum AoI, and Vtm = 1 indicates that
device m is served by a UAV at time step t. The maximum
value Amax determines the maximum penalty assigned to the
UAVs for not collecting data from a device at any given time.
For the sake of demonstrating the idea, we assume line-ofsight (LoS) communication links and write the channel gain
between agent i and device m at time step t as
g0
,
(28)
gti,m =
2
2
h + (Li,m
t )
where g0 is the channel gain at a reference distance of 1 m
and Li,m
is the distance between UAV i and device m at time
t
t. Using the standard Shannon capacity formula, for device
m to communicate to UAV i at time step t, the transmission
power must be set to [47]
 E

2 B ‚àí 1 œÉ2
Pti,m =
,
(29)
gti,m
where E is the size of the transmitted packet, B is the
bandwidth, and œÉ 2 is the noise power.
If all the UAVs are outside the risk region, the reward
function is given deterministically as a weighted combination
of the sums of AoI and powers across all agents
rt = ‚àí

M
M
X
1 X m
Ptim ,m ,
At ‚àí Œª
M m=1
m=1

(30)

where Œª > 0 is a parameter that controls the desired trade-off
between AoI and power consumption. In contrast, if any of
the UAVs is within the risk region, with probability prisk , the
reward is given by (30) with the addition of a penalty value
Prisk > 0, while it is equal to (30) otherwise. For instance, if
the risk region corresponds to an area with a high probability
of signal blockages, the penalty Prisk may be chosen to be
proportional to the amount of power needed to resend the
packets lost due to blockages of the communication links
between the UAVs and the sensors. That said, it is emphasized
that the proposed model is general and that the specific
application scenario would practically dictate the choice of
the penalty Prisk .

TABLE III: Simulation parameters and hyperparameters
Parameter

Value

Parameter

Value

g0
30 dB
B
1 MHz
h
100 m
E
5 Mb
Œ≥
0.99
Batch size
128
Iterations K 150
Lc
100 m

Œ±
Œ≥
Œæ
œÉ2
Amax
Œª
prisk
Optimizer

1
0.99
0.15
‚àí100 dBm
100
500
0.1
Adam

To complete the setting description, we define state and
actions as follows. The global state of the system at
each time step t is the collection of the UAVs‚Äô posiand the individual AoI of the
 devices, i.e., st =
tions
x1t , yt1 , ¬∑ ¬∑ ¬∑ , xIt , ytI , A1t , A2t , ¬∑ ¬∑ ¬∑ , AM
t . At each time t, the
action ait = [wti , dit ] of each UAV i includes the direction
wti ‚àà {north, south, east, west, hover}, where ‚Äúhover‚Äù represents the decision of staying in the same cell, while the other
actions move the UAV by one cell in the given direction. It
also includes the identity dit ‚àà M ‚à™ {0} of the device served
at time t, with dit = 0 indicating that no device is served by
UAV i.
B. Implementation and Dataset Collection
We consider a 10 √ó 10 grid world with I = 2 UAVs serving
M = 10 limited-power sensors and a 5 √ó 4 risk region in the
middle of the grid world as illustrated in Fig. 3. We use a
fully connected neural network with two hidden layers of size
256 and ReLU activation functions to represent the Q-function
and the quantiles. The experiments are implemented using
Pytorch on a single NVIDIA Tesla V100 GPU. Table III shows
the UAV network parameters and the proposed schemes‚Äô
hyperparameters. We compare the proposed method MA-CQR
to baseline offline MARL schemes, namely multi-agent deep
Q-network (MA-DQN) [48], MA-CQL (see Sec.IV-A), and
multi-agent quantile regression DQN (MA-QR-DQN). MADQN corresponds to MA-CQL when no conservative penalty
for OOD is applied, i.e., Œ± = 0 in (15), whereas MA-QR-DQN
corresponds to MA-CQR when Œ± = 0 and Œæ = 1. Both the
independent and centralized training frameworks apply to MADQN, yielding multi-agent deep independent Q-network (MADIQN) and multi-agent deep centralized Q-network (MADCQN), and to MA-QR-DQN, yielding multi-agent quantile
regression deep independent Q-network (MA-QR-DIQN) and
multi-agent quantile regression deep centralized Q-network
(MA-QR-DCQN).
For the proposed MA-CQR, we consider two settings for
the risk tolerance level Œæ, namely Œæ = 1 and Œæ = 0.15, with
the former corresponding to a risk-neutral design. We refer to
the former as MA-CQR and the latter as MA-CQR-CVaR. For
all distributional RL schemes (MA-QR-DQN and MA-CQR in
all its variants), the learning rate is set to 10‚àí5 , while for all
other schemes, we use a learning rate of 10‚àí4 .
The offline dataset D is collected using online independent
DQN agents. In particular, we train the UAVs using an online
MA-DQN algorithm until convergence and use 6% and 16%

10

120

‚àí1
Average test ret rn

100

‚àí2

80

‚àí3

Sum-AoI

MA-CIQR
MA-CIQR-CVaR
MA-CIQL
MA-DIQN
MA-QR-DIQN

60
MA-QR-DIQN
MA-DIQN
MA-CIQL
MA-CIQR-CVaR
MA-CIQR
Idle

40

‚àí4

20

‚àí5
0

20

40

60
80
Epochs

100

120

0
0.0

140

Fig. 4: Average test return as a function of the number of training epochs
using Prisk = 100 and 16% offline dataset for a system of 2 UAVs serving
10 sensors. The return is averaged over 100 test episodes at the end of each
training epoch and shown upon division by 1000.

0.5

1.0

1.5
2.0
Sum-power

2.5

3.0

Fig. 5: Sum-AoI as a function of the sum-power using Prisk = Œª/4 and 16%
offline dataset for a system of 2 UAVs serving 10 sensors.

x

of the total number of transitions from the observed experience
as the offline datasets1 .

x
x

x

x

x
x

C. Numerical Results
First, we show the simulation results of the proposed model
via independent Q-learning compared to the baseline schemes.
Then, we investigate the benefits of the joint training approach
compared to independent training.
1) Independent Learning: Fig. 4 shows the average test
return, evaluated online using 100 test episodes, for the
policies obtained after a given number of training epochs. The
figure thus reports the actual return obtained by the system as
a function of the computational load, which increases with the
number of training epochs.
We first observe that both MA-DIQN and MA-QR-DIQN,
designed for online learning, fail to converge in the offline
setting at hand. This well-known problem arises from overestimating Q-values corresponding to OOD actions in the
offline dataset [7]. In contrast, conservative strategies designed
for offline learning, namely MA-CIQL, MA-CIQR, and MACIQR-CVaR, exhibit an increasing average return as a function
of the training epochs. In particular, the proposed MA-CIQR
and MA-CIQR-CVaR provide the fastest convergence, needing
around 30 training epochs to reach the maximum return. In
contrast, MA-CIQL shows slower convergence. This highlights
the benefits of distributional RL in handling the inherent uncertainties arising in multi-agent systems from the environment
and the actions of other agents [8].
In Fig 5, we report the optimal achievable trade-off between
sum-AoI and sum-power consumption across the devices.
This region is obtained by training the different schemes
while sweeping the hyperparameter values Œª. We recall that
1 The
code
and
datasets
are
available
https://github.com/Eslam211/Conservative-and-Distributional-MARL

at

x

x
x
Risk-neutral

Risk-sensitive

Fig. 6: A comparison between the trajectories of two UAVs using riskneutral and risk-sensitive policies obtained via MA-CIQR and MA-CIQRCVaR, respectively. Crosses represent the positions of the devices.

the hyperparameter Œª controls the weight of the power as
compared to the AoI in the reward function (30). In particular,
setting Œª ‚Üí 0 minimizes the AoI only, resulting in a roundrobin optimal policy. At the other extreme, setting a large value
of Œª causes the UAV never to probe the devices, achieving
the minimum power equal to zero, and the maximum AoI
Amax = 100. This point is denoted as ‚Äúidle point‚Äù the figure.
The other curves represent the minimum sum-AoI achievable
as a function of the sum-power.
From Fig. 5, we observe that the proposed MA-CIQR
always achieves the best age-power trade-off with the least
age and sum-power consumption within all the curves. As
in Fig. 4, MA-DIQN and MA-QR-DIQN provide the worst
performance due to their failure to handle the uncertainty
arising from OOD actions.
In the next experiment, we investigate the capacity of
the proposed risk-sensitive scheme MA-CIQR-CVaR to avoid
risky trajectories. As a first illustration of this aspect, Fig. 6
shows two examples of trajectories obtained via MA-CIQR

11

MA-CIQR-CVaR
MA-CCQR-CVaR
MA-CIQR
MA-CCQR
MA-CIQL

MA-CCQL
MA-DIQN
MA-DCQN
MA-QR-DIQN
MA-QR-DCQN

40

100

120

140

100

120

140

‚àí1
Average test ret rn

‚àí2
‚àí3
‚àí4
‚àí5
0

20

60
80
Epochs

(a) 16% dataset size

‚àí1
‚àí2

Average test return

and MA-CIQR-CVaR. It is observed that the risk-neutral
policies obtained by MA-CIQR take shortcuts through the risk
area, while the risk-sensitive trajectories obtained via MACIQR-CVaR avoid entering the risk area.
2) Centralized Learning: Here, we compare the centralized
training approach with independent learning. Fig. 7 shows the
average test return as a function of training epochs for an
environment of 2 agents and 10 sensors. We use two offline
datasets with different sizes, equal to 6% and 16% of the
total transitions from the observed experience of online DQN
agents. We increase the value of the risk region penalty to
Prisk = 300 compared to the previous subsection experiments.
Fig. 7a elucidates that the performance of the independent
learning schemes is affected by increasing the risk penalty
Prisk . Specifically, MA-CIQL fails to reach convergence, while
MA-CIQR and MA-CIQR-CVaR reach near-optimal performance but with a slower and less stable convergence than their
centralized variants. However, as in Fig. 7b, a significant performance gap is observed between the proposed independent
schemes and their centralized counterpart for reduced dataset
size. This joint training approach in coordinating between
agents during training, requiring less data to obtain effective
policies. Finally, we note the joint training approach did not
enhance the performance of MA-DCQN and MA-QR-DCQN
as these schemes are still heavily affected by the distributional
shift in the offline setting.
In a manner similar to Fig. 5, Fig. 8 shows the tradeoff between sum-AoI and sum-power consumption for both
independent and centralized training approaches for a system
of 3 agents serving 15 sensors. Increasing the parameter Œª
in (30) reduces the total power consumption at the expense
of AoI. Here again, we observe a significant gain in performance for MA-CCQR and MA-CCQR-CVaR compared to
their independent variants. In contrast, the non-distributional
schemes, MA-CIQL and MA-CCQL, show similar results,
as both perform poorly in this low data regime. We also
observe that the average return performance of MA-CIQRCVaR is worse than that of MA-CIQR, while MA-CCQRCVaR provides a comparable performance as its risk-neutral
counterpart MA-CCQR. This result suggests that, while producing low-risk trajectories, MA-CIQR-CVaR can yield lower
average returns as compared to the risky trajectories of its
risk-neutral counterpart, MA-CIQR. In contrast, thanks to
the higher level of coordination between the agents in the
joint training approach, MA-CCQR-CVaR can find low-risk
trajectories while maintaining a comparable average return as
MA-CCQR.
Finally, to gain further insights into the comparison between
MA-CCQR and MA-CCQR-CVaR, we leverage two metrics as
in [24], namely the percentage of violations and the CVaR0.15
return. The former is the percentage of timesteps at which
one of the UAVs enters the risk region with respect to the
total number of timesteps. In contrast, the CVaR0.15 metric is
the average return of the 15% worst episodes.
In Table IV, we report these two metrics, as well as the
average return, with all returns normalized by 1000. Thanks to
the ability of MA-CCQR-CVaR to learn how to avoid the risk
region, this scheme has the lowest percentage of violations

‚àí3
‚àí4
‚àí5
0

20

40

60
80
Epochs

(b) 6% dataset size
Fig. 7: Average test return as a function of the number of training epochs for
penalty Prisk = 300 in a system of 3 UAVs and 15 sensors with data set size
equal to (a) 16% and (b) 6%. The return is averaged over 100 test episodes
at the end of each training epoch, and shown upon division by 1000.
TABLE IV: Performance evaluation over 100 test episodes after 150 training
iterations for penalty Prisk = Œª/2.5 in a system of 3 UAVs and 15 sensors
(data set size equal to 6%).
Algorithm

Average return

CVaR0.15 return

Violations

MA-DQN (online)
MA-DCQN
MA-QR-DCQN
MA-CCQL
MA-CCQR
MA-CCQR-CVaR

‚àí1.5633
‚àí4.8993
‚àí4.2987
‚àí3.8518
‚àí1.4028
‚àí1.3641

‚àí1.9611
‚àí5.4930
‚àí4.5743
‚àí4.4695
‚àí2.1775
‚àí1.8513

11.83%
8.29%
6.85%
17.7%
8.56%
5.83%

among all the schemes. In addition, it achieves the largest
CVaR0.15 return, with a small gain in terms of average return
as compared to MA-CCQR. This demonstrates the advantages
of the risk-sensitive design of policies.

12

MA-CIQL
MA-CCQL
MA-CIQR-CVaR
MA-CCQR-CVaR

MA-CIQR
MA-CCQR
Idle

100

Sum-AoI

80
60
40
20
0

0

1

2
3
Sum-power

4

5

Fig. 8: Sum-AoI as a function of the sum-power for penalty Prisk = Œª/2.5
in a system of 3 UAVs and 15 sensors (data set size equal to 6%).

VII. C ONCLUSIONS
In this paper, we developed a distributional and conservative
offline MARL scheme for wireless systems. We considered
optimizing the CVaR of the cumulative return to obtain risksensitive policies. We introduced two variants of the proposed
scheme depending on the level of coordination between the
agents during training. The proposed algorithms were applied
to the trajectory optimization problem in UAV networks.
Numerical results illustrate that the learned policies avoid risky
trajectories more effectively and yield the best performance
compared to the baseline MARL schemes. The proposed
approach can be extended by considering online fine-tuning of
the policies in the environment to handle the possible changes
in the deployment environment compared to the one generating
the offline dataset. Finally, the analysis of model-based offline
MARL, which can leverage information about the physical
system to learn a model of the environment, is left for future
work.
R EFERENCES
[1] A. Lavin, D. Krakauer, H. Zenil, J. Gottschlich, T. Mattson, J. Brehmer,
A. Anandkumar, S. Choudry, K. Rocki, A. G. Baydin et al., ‚ÄúSimulation
intelligence: Towards a new generation of scientific methods,‚Äù arXiv
preprint arXiv:2112.03235, 2021.
[2] C.-X. Wang, M. Di Renzo, S. Stanczak, S. Wang, and E. G. Larsson,
‚ÄúArtificial intelligence enabled wireless networking for 5G and beyond:
Recent advances and future challenges,‚Äù IEEE Wireless Communications, vol. 27, no. 1, pp. 16‚Äì23, 2020.
[3] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, ‚ÄúArtificial neural
networks-based machine learning for wireless networks: A tutorial,‚Äù
IEEE Communications Surveys & Tutorials, vol. 21, no. 4, pp. 3039‚Äì
3071, 2019.
[4] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C.
Liang, and D. I. Kim, ‚ÄúApplications of deep reinforcement learning
in communications and networking: A survey,‚Äù IEEE Communications
Surveys & Tutorials, vol. 21, no. 4, pp. 3133‚Äì3174, 2019.

[5] W. Chen, X. Qiu, T. Cai, H.-N. Dai, Z. Zheng, and Y. Zhang, ‚ÄúDeep
reinforcement learning for internet of things: A comprehensive survey,‚Äù
IEEE Communications Surveys & Tutorials, vol. 23, no. 3, pp. 1659‚Äì
1692, 2021.
[6] S. V. Albrecht, F. Christianos, and L. SchaÃàfer, Multi-Agent Reinforcement
Learning: Foundations and Modern Approaches. MIT Press, 2024.
[Online]. Available: https://www.marl-book.com
[7] S. Levine, A. Kumar, G. Tucker, and J. Fu, ‚ÄúOffline reinforcement
learning: Tutorial, review, and perspectives on open problems,‚Äù arXiv
preprint arXiv:2005.01643, 2020.
[8] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Reinforcement Learning. MIT Press, 2023, http://www.distributional-rl.org.
[9] A. Kumar, A. Zhou, G. Tucker, and S. Levine, ‚ÄúConservative
Q-learning for offline reinforcement learning,‚Äù in Advances
in Neural Information Processing Systems, vol. 33.
Curran
Associates, Inc., 2020, pp. 1179‚Äì1191. [Online]. Available:
https://proceedings.neurips.cc/paper files/paper/2020/file/0d2b2061826a5df3221116a508
[10] M. A. Abd-Elmagid, A. Ferdowsi, H. S. Dhillon, and W. Saad, ‚ÄúDeep
reinforcement learning for minimizing age-of-information in UAVassisted networks,‚Äù in 2019 IEEE Global Communications Conference
(GLOBECOM). IEEE, 2019, pp. 1‚Äì6.
[11] M. Samir, C. Assi, S. Sharafeddine, D. Ebrahimi, and A. Ghrayeb,
‚ÄúAge of information aware trajectory planning of UAVs in intelligent
transportation systems: A deep learning approach,‚Äù IEEE Transactions
on Vehicular Technology, vol. 69, no. 11, pp. 12 382‚Äì12 395, 2020.
[12] E. Eldeeb, J. M. de Souza Sant‚ÄôAna, D. E. PeÃÅrez, M. Shehab, N. H.
Mahmood, and H. Alves, ‚ÄúMulti-UAV path learning for age and power
optimization in IoT with UAV battery recharge,‚Äù IEEE Transactions on
Vehicular Technology, vol. 72, no. 4, pp. 5356‚Äì5360, 2022.
[13] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims,
‚ÄúMorel: Model-based offline reinforcement learning,‚Äù in Advances
in Neural Information Processing Systems, vol. 33.
Curran
Associates, Inc., 2020, pp. 21 810‚Äì21 823. [Online]. Available:
https://proceedings.neurips.cc/paper files/paper/2020/file/f7efa4f864ae9b88d43527f4b14
[14] K. Ciosek, ‚ÄúImitation learning by reinforcement learning,‚Äù in
International Conference on Learning Representations, 2022. [Online].
Available: https://openreview.net/forum?id=1zwleytEpYx
[15] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, ‚ÄúTrust
region policy optimization,‚Äù in International conference on machine
learning. PMLR, 2015, pp. 1889‚Äì1897.
[16] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn,
and T. Ma, ‚ÄúMopo: Model-based offline policy optimization,‚Äù Advances
in Neural Information Processing Systems, vol. 33, pp. 14 129‚Äì14 142,
2020.
[17] P. Barde, J. Foerster, D. Nowrouzezahrai, and A. Zhang, ‚ÄúA model-based
solution to the offline multi-agent reinforcement learning coordination
problem,‚Äù in the 23rd International Conference on Autonomous Agents
and Multiagent Systems. International Foundation for Autonomous
Agents and Multiagent Systems, 2024.
[18] K. Yang, C. Shi, C. Shen, J. Yang, S.-p. Yeh, and J. J. Sydir, ‚ÄúOffline
reinforcement learning for wireless network optimization with mixture
datasets,‚Äù IEEE Transactions on Wireless Communications, pp. 1‚Äì1,
2024.
[19] M. G. Bellemare, W. Dabney, and R. Munos, ‚ÄúA distributional perspective on reinforcement learning,‚Äù in International conference on machine
learning. PMLR, 2017, pp. 449‚Äì458.
[20] W. Dabney, M. Rowland, M. Bellemare, and R. Munos, ‚ÄúDistributional
reinforcement learning with quantile regression,‚Äù in Proceedings of the
AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.
[21] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, ‚ÄúImplicit quantile
networks for distributional reinforcement learning,‚Äù in International
conference on machine learning. PMLR, 2018, pp. 1096‚Äì1105.
[22] R. T. Rockafellar, S. Uryasev et al., ‚ÄúOptimization of conditional valueat-risk,‚Äù Journal of risk, vol. 2, pp. 21‚Äì42, 2000.
[23] S. H. Lim and I. Malik, ‚ÄúDistributional reinforcement learning for risksensitive policies,‚Äù Advances in Neural Information Processing Systems,
vol. 35, pp. 30 977‚Äì30 989, 2022.
[24] Y. Ma, D. Jayaraman, and O. Bastani, ‚ÄúConservative
offline distributional reinforcement learning,‚Äù in Advances in
Neural Information Processing Systems, vol. 34.
Curran
Associates, Inc., 2021, pp. 19 235‚Äì19 247. [Online]. Available:
https://proceedings.neurips.cc/paper files/paper/2021/file/a05d886123a54de3ca4b0985b
[25] J. Jiang and Z. Lu, ‚ÄúOffline decentralized multi-agent reinforcement
learning.‚Äù in ECAI, 2023, pp. 1148‚Äì1155.
[26] L. Pan, L. Huang, T. Ma, and H. Xu, ‚ÄúPlan better amid conservatism:
Offline multi-agent reinforcement learning with actor rectification,‚Äù in

13

International Conference on Machine Learning.
PMLR, 2022, pp.
[46] A. Kosta, N. Pappas, and V. Angelakis, ‚ÄúAge of information: A new
17 221‚Äì17 237.
concept, metric, and tool,‚Äù Foundations and Trends in Networking, Now
[27] J. Shao, Y. Qu, C. Chen, H. Zhang, and X. Ji, ‚ÄúCounterfactual
Publishers, Inc., 2017.
conservative q learning for offline multi-agent reinforcement learning,‚Äù
[47] E. Eldeeb, M. Shehab, and H. Alves, ‚ÄúAge minimization in massive IoT
in Advances in Neural Information Processing Systems, vol. 36.
via UAV swarm: A multi-agent reinforcement learning approach,‚Äù in
Curran Associates, Inc., 2023, pp. 77 290‚Äì77 312. [Online]. Available:
2023 IEEE 34th Annual International Symposium on Personal, Indoor
https://proceedings.neurips.cc/paper files/paper/2023/file/f3f2ff9579ba6deeb89caa2fe1f0b99c-Paper-Conference.pdf
and Mobile Radio Communications (PIMRC), 2023, pp. 1‚Äì6.
[28] X. Wang, H. Xu, Y. Zheng, and X. Zhan, ‚ÄúOffline
[48] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
multi-agent
reinforcement
learning
with
implicit
globalJ. Aru, and R. Vicente, ‚ÄúMultiagent cooperation and competition with
to-local
value
regularization,‚Äù
in
Advances
in
Neural
deep reinforcement learning,‚Äù PloS one, vol. 12, no. 4, p. e0172395,
Information Processing Systems, vol. 36.
Curran Associates,
2017.
Inc.,
2023,
pp.
52 413‚Äì52 429.
[Online].
Available:
https://proceedings.neurips.cc/paper files/paper/2023/file/a46c84276e3a4249ab7dbf3e069baf7f-Paper-Conference.pdf
[29] Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang,
J. Yang, and Q. Zhao, ‚ÄúBelieve what you see: Implicit constraint
approach for offline multi-agent reinforcement learning,‚Äù in
Advances in Neural Information Processing Systems, vol. 34.
Curran Associates, Inc., 2021, pp. 10 299‚Äì10 312. [Online]. Available:
https://proceedings.neurips.cc/paper files/paper/2021/file/550a141f12de6341fba65b0ad0433500-Paper.pdf
[30] E. Eldeeb, M. Shehab, and H. Alves, ‚ÄúTraffic learning and proactive
UAV trajectory planning for data uplink in markovian IoT models,‚Äù IEEE
Internet of Things Journal, vol. 11, no. 8, pp. 13 496‚Äì13 508, 2024.
[31] F. Wu, H. Zhang, J. Wu, L. Song, Z. Han, and H. V. Poor, ‚ÄúAoI
minimization for UAV-to-device underlay communication by multiagent deep reinforcement learning,‚Äù in GLOBECOM 2020 - 2020 IEEE
Global Communications Conference, 2020, pp. 1‚Äì6.
[32] J. Cui, Y. Liu, and A. Nallanathan, ‚ÄúMulti-agent reinforcement learningbased resource allocation for UAV networks,‚Äù IEEE Transactions on
Wireless Communications, vol. 19, no. 2, pp. 729‚Äì743, 2020.
[33] Y. S. Nasir and D. Guo, ‚ÄúMulti-agent deep reinforcement learning
for dynamic power allocation in wireless networks,‚Äù IEEE Journal on
Selected Areas in Communications, vol. 37, no. 10, pp. 2239‚Äì2250,
2019.
[34] N. Naderializadeh, J. J. Sydir, M. Simsek, and H. Nikopour, ‚ÄúResource
management in wireless networks via multi-agent deep reinforcement
learning,‚Äù IEEE Transactions on Wireless Communications, vol. 20,
no. 6, pp. 3507‚Äì3523, 2021.
[35] C. Xu, Z. Tang, H. Yu, P. Zeng, and L. Kong, ‚ÄúDigital twin-driven
collaborative scheduling for heterogeneous task and edge-end resource
via multi-agent deep reinforcement learning,‚Äù IEEE Journal on Selected
Areas in Communications, vol. 41, no. 10, pp. 3056‚Äì3069, 2023.
[36] Q. Zhang, W. Saad, and M. Bennis, ‚ÄúMillimeter wave communications
with an intelligent reflector: Performance optimization and distributional
reinforcement learning,‚Äù IEEE Transactions on Wireless Communications, vol. 21, no. 3, pp. 1836‚Äì1850, 2021.
[37] ‚Äî‚Äî, ‚ÄúDistributional reinforcement learning for mmwave communications with intelligent reflectors on a uav,‚Äù in GLOBECOM 2020-2020
IEEE Global Communications Conference. IEEE, 2020, pp. 1‚Äì6.
[38] Y. Hua, R. Li, Z. Zhao, X. Chen, and H. Zhang, ‚ÄúGan-powered
deep distributional reinforcement learning for resource management in
network slicing,‚Äù IEEE Journal on Selected Areas in Communications,
vol. 38, no. 2, pp. 334‚Äì349, 2019.
[39] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
‚ÄúValue-decomposition networks for cooperative multi-agent learning,‚Äù
arXiv preprint arXiv:1706.05296, 2017.
[40] X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato, ‚ÄúOn centralized
critics in multi-agent reinforcement learning,‚Äù 2024. [Online]. Available:
https://arxiv.org/abs/2408.14597
[41] R. Lowe, Y. WU, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
‚ÄúMulti-agent
actor-critic
for
mixed
cooperative-competitive
environments,‚Äù in Advances in Neural Information Processing
Systems, vol. 30. Curran Associates, Inc., 2017. [Online]. Available:
https://proceedings.neurips.cc/paper files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf
[42] R. Bellman, ‚ÄúDynamic programming,‚Äù Science, vol. 153, no. 3731, pp.
34‚Äì37, 1966.
[43] H. Xu, L. Jiang, J. Li, Z. Yang, Z. Wang, V. W. K. Chan, and X. Zhan,
‚ÄúOffline rl with no ood actions: In-sample learning via implicit value
regularization,‚Äù arXiv preprint arXiv:2303.15810, 2023.
[44] E. Eldeeb, D. E. PeÃÅrez, J. Michel de Souza Sant‚ÄôAna, M. Shehab, N. H.
Mahmood, H. Alves, and M. Latva-Aho, ‚ÄúA learning-based trajectory
planning of multiple UAVs for AoI minimization in IoT networks,‚Äù in
2022 Joint European Conference on Networks and Communications &
6G Summit (EuCNC/6G Summit), 2022, pp. 172‚Äì177.
[45] E. Eldeeb, M. Shehab, A. E. Kal√∏ r, P. Popovski, and H. Alves, ‚ÄúTraffic
prediction and fast uplink for hidden markov IoT models,‚Äù IEEE Internet
of Things Journal, vol. 9, no. 18, pp. 17 172‚Äì17 184, 2022.

