M3 HF: Multi-agent Reinforcement Learning from
Multi-phase Human Feedback of Mixed Quality

Ziyan Wang 1 2 Zhicheng Zhang 2 Fei Fang 2 Yali Du 1

arXiv:2503.02077v3 [cs.MA] 4 Jun 2025

Abstract

larly in complex environments where the desired behaviors
are intricate or the rewards are sparse (Singh et al., 2009;
Ng et al., 2000). This difficulty is magnified in multi-agent
reinforcement learning (MARL) settings, where agents must
not only learn optimal individual behaviors but also coordinate with others, leading to an exponential increase in task
complexity (Zhang et al., 2021; Oroojlooy & Hajinezhad,
2023; Du et al., 2023). Sparse or hard-to-learn rewards can
severely hinder the learning process, causing agents to converge slowly or settle on suboptimal policies (Andrychowicz
et al., 2017; Pathak et al., 2017). In such scenarios, relying
solely on environmental rewards may be insufficient for
effective learning. Incorporating human feedback has thus
emerged as a promising approach (Christiano et al., 2017;
Knox & Stone, 2009; Ho & Ermon, 2016), since human
guidance can provide additional, informative signals that
help agents navigate complex tasks more efficiently when
intrinsic rewards are inadequate.

Designing effective reward functions in multiagent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or
misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3 HF), a novel framework that integrates multi-phase human feedback
of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, M3 HF
leverages both expert and non-expert feedback
to continuously refine agents’ policies. During
training, we strategically pause agent learning
for human evaluation, parse feedback using large
language models to assign it appropriately and
update reward functions through predefined templates and adaptive weights by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human
insights across various levels of quality, enhancing the interpretability and robustness of multiagent cooperation. Empirical results in challenging environments demonstrate that M3 HF significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward
design in MARL and enabling broader human
participation in the training process.

To leverage human expertise in accelerating the learning
process of MARL agents, we propose the Multi-phase Human Feedback Markov Game (MHF-MG), an extension of
the Markov Game that incorporates human feedback across
multiple generations of learning. At each generation, agents
gather experiences using their current policies but may still
struggle under the original reward function. Humans then
observe the agents’ behaviors, offering feedback that reflects the discrepancy between their own (potentially more
expert) policy and the agents’ policies. Building on the
MHF-MG, we develop Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
(M3 HF), which operationalizes the MHF-MG by directly
integrating feedback into the agents’ reward functions. This
framework utilizes large language models (LLMs) to parse
human feedback of various quality levels, employs predefined templates for structured reward shaping, and applies
adaptive weight adjustments to accommodate mixed-quality
signals.

1. Introduction
Designing effective reward functions for reinforcement
learning (RL) agents is a well-known challenge, particu1
Department of Informatics, King’s College London, London,
United Kingdom 2 Software and Societal Systems Department,
Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence
to: Ziyan Wang <ziyan.wang@kcl.ac.uk>.

In summary, we make the following contributions: (1) We
propose the MHF-MG to address reward sparsity and complexity in MARL through iterative human guidance; (2) We
develop the M3 HF framework, which leverages LLMs to
parse diverse human feedback and dynamically incorporates

Proceedings of the 42 nd International Conference on Machine
Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025
by the author(s).

1

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Mul t i - agent RL

Rol l out Vi deo

Rewar d Templ at es
Act i on- based

St at e- based

Pr ox- based

Success- based

Di st ance- based Ti me- based

New Rewar d Funct i ons
f or Each Agent
Di st ance- based
l ambda obs, act :
( - sqr t ( ( obs[ 19] - obs[ 0] ) * * 2
+ ( obs[ 20] - obs[ 1] ) * * 2) )

...
Act i on- based

Human Feedback

LLM

l ambda obs, act : ( 1 i f act
== 5 el se 0) + ( 1 i f obs[ 2]
== 1 el se 0)

" The r ose chef shoul d get t he t omat o
f i r st , t hen t he gr een chef can t ake i t
and cut i t as qui ckl y as possi bl e. The
bl ue chef can t ake t he pl at e and put
t he cut s on i t . "

...
St at e- based
l ambda obs, act : ( 1 i f act
== 5 el se 0) + ( 1 i f obs[ 9]
== obs[ 0] and obs[ 10] ==
obs[ 1] el se 0)

Rewar d Funct i ons Updat e
Func t i on
Wei ght s
Adj us t ment

Rewar d
Func t i on
Pool s

Figure 1. Workflow of the M3 HF method. Each generation k ∈ (0, .., K − 1) begins with Multi-agent RL training. Agents generate
rollout videos τk for human evaluation. Human feedback uk is parsed by a Large Language Model (LLM) into agent-specific instructions.
The LLM then selects appropriate reward function templates f ∈ F and parameterizes them based on the parsed feedback. New reward
functions Rki (s, a) are added to each agent’s reward function pool Pi , with weights wi,m adjusted using performance-based criteria. The
i
(s, a) guide the next generation of agents’ training, creating a loop of agents learning from feedback.
updated reward functions R̂k+1

it into agents’ reward functions; (3) We provide a theoretical
analysis justifying the use of rollout-based performance estimates and offering a weight decay mechanism that mitigates
the impact of low-quality feedback. Extensive experiments
in Overcooked demonstrate that M3 HF consistently outperforms strong baselines, ultimately providing a robust
and flexible method for enhancing multi-agent cooperation
under challenging reward structures.

Reinforcement Learning from Human Feedback (RLHF)
has emerged as a promising avenue to address the limitations
of handcrafted reward functions. Christiano et al. (2017)
introduced methods for training agents using human preferences to shape the reward function, demonstrating that
human feedback can significantly enhance policy learning.
Building on this, Lee et al. (2021) proposed PEBBLE, leveraging unsupervised pre-training and experience relabeling
to improve feedback efficiency in interactive RL settings.

2. Related Work

While RLHF has been successfully applied to train Large
Language Models (LLMs) (Ouyang et al., 2022; Shani et al.,
2024; Wang et al., 2025; Hu et al., 2025; Liu et al., 2024),
these approaches primarily focus on aligning LLM outputs
with human preferences through single-turn interactions
and scalar reward signals. Recently, PbMARL (Zhang et al.,
2024) applied RLHF to MARL by utilizing offline pairwise
preference comparisons extracted from pre-collected simulated policy data, but remained limited to an offline singlephase feedback scenario without dynamic, iterative human
interactions. In contrast, our work incorporates multi-phase,
mixed-quality human feedback directly into the reinforcement learning loop during online training in a multi-agent
environment, enabling more flexible and scalable reward
shaping.

Multi-Agent Reinforcement Learning (MARL) has been
extensively studied to enable agents to learn coordinated
behaviors in shared environments (Du et al., 2023; Yu et al.,
2022). Traditional MARL approaches often rely on predefined reward functions and suffer from scalability and
stability issues arising from the non-stationarity introduced
by multiple learning agents (Busoniu et al., 2008; Canese
et al., 2021). However, designing appropriate reward functions in MARL remains a significant challenge due to the
complexity of agent interactions and the potential for conflicting objectives. To address this, researchers have explored various techniques for reward design. These include
credit assignment methods (Nguyen et al., 2018; Zhou et al.,
2020), reward shaping (Mannion et al., 2018), and the use
of intrinsic rewards (Du et al., 2019). Furthermore, reward
decomposition approaches have been proposed to balance
individual and team objectives, such as separating rewards
into contributions from self and nearby agents (Zhang et al.,
2020), or combining dense individual rewards with sparse
team rewards (Wang et al., 2022).

Language Models in Reward Design and Policy Learning. Recent advancements in LLMs have created opportunities for incorporating natural language guidance into
reinforcement learning (RL) (Liu et al., 2022; Chen et al.,
2024). For instance, Ma et al. (2024) introduced EUREKA,
a method employing code-generating LLMs to craft sophis2

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

set of agents. The state space S encompasses all possible
configurations of the environment, while the action space
A denotes the set of actions available to each agent. At
each time step t, the environment is in a state st ∈ S. Each
agent i ∈ N selects an action ait ∈ A according to its
policy π i (ait |st ). The joint action at = (a1t , a2t , . . . , aN
t )
leads to a state transition to st+1 according to the transition
function P (st+1 |st , at ). The agents receive a shared reward
rt = R(st , at ), where R : S × AN → R is the reward
function, and γ ∈ [0, 1) is the discount factor. The objective
for each agent is to learn a policy π i that maximizes the
expected cumulative discounted reward:
"∞
#
X
J i (π i ) = E
γ t rt π i , π −i ,
(1)

ticated reward functions at a human-expert level. However,
extending methods such as EUREKA to multi-agent settings
remains non-trivial, as independently performing rollouts to
optimize rewards for each agent can become prohibitively
expensive and computationally intensive. Similarly, Liang
et al. (2023) proposed Code as Policies, where language
model programs are used for embodied control, allowing
agents to interpret and execute high-level instructions.
Other recent works have further leveraged LLMs to translate linguistic instructions into reward functions and policies.
Yu et al. (2023) utilized language instructions to shape rewards for robotic skills, effectively aligning robot behaviors
with human guidance. Kwon et al. (2023) highlighted the
capacity of language models to capture nuanced human
preferences during reward design. In human-AI collaboration (Zou et al., 2025), Hu & Sadigh (2023) demonstrated
how language-instructed RL could foster improved coordination between humans and agents, Wang et al. (2024) uses
LLMs to convert language constraints into cost signals for
agents. Additionally, Liang et al. (2024) proposed leveraging language model predictive control to iteratively refine
RL policy learning from human feedback more quickly.

t=0
−i

where π denotes the policies of all agents other than agent
i, and the expectation is over the trajectories induced by the
policies and the environment dynamics.
In our setting, although the reward function R is known
(denoted as original reward function Rori ), it is challenging
for agents to learn optimal policies due to its sparsity or
complexity. This difficulty can lead to slow convergence
or suboptimal performance for traditional reinforcement
learning algorithms.

Relatedly, Klissarov et al. (2023) presented MOTIF, adopting LLM-generated intrinsic rewards from textual task specifications to promote effective exploration in single-agent
environments. Although MOTIF effectively leverages language grounding, it is inherently limited to single-agent
contexts and does not incorporate iterative human interactions or complex multi-agent cooperation scenarios. In
contrast, our approach specifically employs LLMs to parse
multi-phase, mixed-quality human feedback dynamically to
iteratively refine reward functions in complex multi-agent
coordination problems.

4. Method
To address the challenges posed by sparse or complex reward functions in multi-agent environments, we introduce
the Multi-phase Human Feedback Markov Game (MHFMG) as a tuple, ⟨N , S, A, P, R, γ, U, π h ⟩. Compared to
a standard Markov Game, the added U denotes the set of
possible human utterances or feedback messages. π h represents the human’s policy. In this framework, the agents
interact with both the environment and a human over discrete generations indexed by k = 0, 1, . . . , K − 1. At each
generation k, agents collect experiences by interacting with
the environment using their current policies πki .

Multi-phase Human Feedback. Prior works have considered the role of iterative and multi-phase human feedback
in reinforcement learning. Yuan et al. (2022) and Sumers
et al. (2022) explored multi-phase bidirectional interactions
between humans and agents through predefined communication protocols, which, while structured, limit the flexibility
of feedback. Zhi-Xuan et al. (2024) examined the use of
human demonstrations via trajectories to convey intentions,
requiring humans to perform the task themselves, which
can be resource-intensive. Early attempts by Chen et al.
(2021) and Zhang et al. (2023) delved into language for task
generalization and policy explanation but were constrained
to single-agent domains.

Each generation k consists of hundreds of iterations; each
iteration comprises tens of episodes, and each episode spans
on the order of hundreds of environment time steps t, depending on the specific environment. This setup allows
agents to gain substantial experience within a generation
before receiving human feedback. The human possesses a
policy π h , which may be sub-optimal but is assumed to be
initially superior to the agents’ policies. This human policy
provides valuable guidance that can accelerate the agents’
learning. The human observes the agents’ behaviors and
generates utterances uk ∈ U at each generation k, offering
feedback based on the discrepancy between their own policy
and the agents’ current policies.

3. Preliminaries
We consider a Markov Game (Littman, 1994), defined by
the tuple ⟨N , S, A, P, R, γ⟩, in multi-agent reinforcement
learning (MARL). Here, N = {1, 2, . . . , N } represents the

We model the human’s utterances as a mapping f from the
3

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

(a) Overcooked-A

(b) Overcooked-B

(c) Overcooked-C

(d)
Lettuce-Tomato (e)
Lettuce-Onionsalad recipe
Tomato salad recipe

Figure 2. The Overcooked Environment. (a)-(c) The three different kitchen layouts with increasing difficulty: (a) Overcooked-A offers
ample movable space; (b) Overcooked-B forces agents divided on both sides to cooperate due to the partitioned kitchen; (c) Overcooked-C
has less movable space compared to A. (d)-(e) The two salad recipes: In both recipes, the corresponding chopped foods must be combined
on a single plate and delivered. To facilitate training, we use macro-actions based on (Xiao et al., 2022), where the agents’ actions are
simplified. More details refer to Section 5.

human’s policy and the agents’ policies to the set of possible
utterances:

uk = f π h , πk1 , πk2 , . . . , πkN ,
(2)

receiving human feedback, and updating their reward functions and policies accordingly. The inclusion of human
feedback helps agents navigate the challenges of sparse or
complex reward functions by providing additional signals
that highlight desirable behaviors and strategies. In this paper, to minimize human involvement, we limit agent-human
interactions to at most five times throughout the entire training process, enabling efficient learning with minimal guidance.

where f captures how the human generates feedback by
comparing their policy with those of the agents. The utterances may include specific action recommendations, strategic advice, or corrections aimed at guiding the agents toward
better performance. The agents parse the human’s utterance
uk to extract actionable information. This process may
involve natural language understanding techniques, potentially leveraging large language models (LLMs) to interpret
the feedback accurately. Based on the parsed feedback,
each agent adjusts its reward function to incorporate the
human’s guidance. For agent i, the updated reward function
at generation k + 1 becomes:
i
Rk+1
(s, a) = R(s, a) + Rhf ik (s, a, uk ),

4.1. Agent to Human Interaction
In the MHF-MG, agents periodically interact with the human to receive feedback that guides their learning process.
This interaction is initiated by the agents, who decide when
to seek human input based on specific criteria. The primary
mechanism for this interaction is through the generation of
rollout trajectories, which approximate the agents’ current
policy performance.

(3)

Rollouts Generation and Communication. During each
generation k, agents interact with the environment using
their current policies πki , collecting substantial experience
over multiple iterations. However, due to the complexity
or sparsity of the original reward function R, agents may
still face difficulties in identifying efficient strategies or
converging quickly to optimal policies.

where Rhf ik represents the reward adjustment derived from
the human’s feedback uk at generation k. This adjustment
modifies the reward signal to encourage behaviors aligned
with the human’s guidance, effectively reshaping the agents’
learning objectives.
The agents then update their policies for the next generation
by optimizing the expected cumulative reward under the
new reward function Ri,k+1 . Formally, the policy update
for agent i is given by:
"∞
#
X
−i
i
t i
i
πk+1 = arg max
E
γ Rk+1 (st , at ) π , πk , (4)
i
π

To address these challenges, agents periodically seek human
feedback based on predefined criteria. Specifically, after
every fixed number of training episodes, agents temporarily
pause their training and generate evaluation rollouts. These
rollouts consist of X independent trajectories collected under the fixed joint policy π k , defined formally as:

t=0

where πk−i denotes the policies of all other agents at generation k, and the expectation is taken over the distribution of
trajectories induced by the policies and the environment dynamics. This iterative process continues across generations,
with agents repeatedly interacting with the environment,

(x)

τk

=

n
oH−1
(x)
(x) (x)
st , at , rt
,
t=0
(x)

x = 1, . . . , X, (5)

where H is the time horizon, st denotes the state at time
(x)
t, at is the joint action chosen by all agents in trajectory
4

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
(x)

4.2. Human to Agents

x, and rt is the resulting environmental reward. These
rollouts allow human observers to assess current agent behaviors and provide structured, actionable feedback for subsequent policy improvements.

Feedback Parsing. Our method employs a Large Language
Model (LLM), denoted as M, to parse the human feedback
uk received at generation k and assign it either to specific
agents or to all agents collectively. This parsing process is
mathematically represented as uik , uall
k = M(uk , N ), where
N is the number of agents, uik is the feedback assigned to
agent i, and uall
k is the feedback applicable to all agents.
This approach ensures that each agent receives relevant
instructions or corrections based on the human input. The
detailed prompts used for guiding the LLM in this parsing
process are provided in the Appendix D.4.

Approximation of Policy Performance via Rollouts. Consider a stochastic game in which all agents follow stationary
joint policies. At generation k, the joint policy π k combined with the environment dynamics induces a distribution
over state-action-reward trajectories. Our goal is to formally
justify that empirical estimates obtained from multiple collected rollouts reliably approximate the true performance
(m)
J(π k ). Given the empirical trajectories τk defined previously, we define each rollout’s discounted return as:
(x)

GH = (1 − γ)

H−1
X

(x)

γ t rt ,

x = 1, . . . , X,

Generating New Reward Function The new reward function Rki for the current generation involves selecting and
parameterizing predefined function templates based on human feedback. For each agent i, the new reward function
for the current generation k is generated as follows:

(6)

t=0

and consider the empirical performance estimator:
Rki (s, a) = M(F, uik , uall
k , e),

X
X

1
(x)
G .
JˆX,H (π k ) =
X x=1 H

where F is a set of predefined function templates, uik is the
parsed feedback for agent i at generation k, and e are the
entities based on the environment states.

(7)

Under the Strong Law of Large Numbers (Kolmogorov,
1933; Durrett, 2010), as M → ∞, this estimator converges
almost surely to the finite-horizon expected discounted return:

Predefined Function Templates. In our framework, we
define a set of predefined reward function templates F that
can be parameterized based on human feedback and the
specific entities within the environment, as shown in Figure 1. These templates enable the system to systematically
generate reward functions aligned with human intentions,
facilitating efficient policy updates in response to feedback.
The templates capture common interaction patterns such as
distance-based rewards that encourage agents to minimize
their distance to target entities, action-based rewards that
incentivize specific actions, and status-based rewards that
reward agents for achieving certain states of the environment. For instance, given the human feedback “I think the
red chef needs to be responsible for getting the onion” and
the LLM will select the distance-based reward template and
parameterize it as:

a.s.
JˆX,H (π k ) −−−−→ JH (π k ),
X→∞

where JH (π k ) = (1 − γ)

H−1
X

γ t E[rt ].

(8)

t=0

Furthermore, when horizon length H grows towards infinity,
the finite-horizon discounted return JH (π k ) converges to
the true discounted return J(π k ). Thus, we have:
Proposition 4.1 (Performance Estimation). Under Assumption A.1 and Assumption A.4, assuming bounded rewards
and given X independent rollouts of length H collected
under policy π k , the empirical estimator converges almost
surely to the true discounted expected return:
JˆX,H (π k ) −−−−→ J(π k ) (a.s.)
X→∞
H→∞

(10)

Rki (s, a) = −∥s[Agent1.pos] − s[Onion.pos]∥2 .

(9)

(11)

Here, the s[Agent1.pos] and s[Onion.pos] are the relevant
entities of the observation vector, which encourages Agent
1 (The red chef in the rollout video) to minimize its distance
to the onion, thus aligning its behavior with the desired objective. This structured approach allows agents to interpret
and act upon multi-fidelity human feedback effectively. Detailed formulations of these reward templates and additional
examples are provided in the Appendix D.2.

Leveraging Proposition 4.1, the multiple rollout trajectories
serve as reliable empirical estimates of the policy performance J(π k ). Human evaluators use these rollouts to effectively assess agent behavior and provide targeted feedback
uk = f (τk ; π h ). This assumption that empirical rollout data
accurately reflects policy-induced distributions is analogous
to common practices in offline reinforcement learning and
policy evaluation contexts (Levine et al., 2020; Kumar et al.,
2019).

Reward Function for the next generation. At the end
of the processing of the human feedback in generation k,
we conclude the final reward function for each agent to a
5

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

0
50
0

Gen 0

200

Gen 1

400

600

800

Iteration

(a) Overcooked-A: Lettuce-Tomato Salad

Episode Return Mean

125
100
75

Gen 2
Gen 3
Gen 4

50
25
0
0

Gen 0

200

Gen 1

400

Gen 2

0

Gen 0

600

Iteration

Gen 3

800

Gen 4

100
75

Gen 1

400

Gen 2

50
0

50
600

800

Iteration

1000

0

25
0

200

Gen 1

400

Gen 2

600

Gen 3

Iteration

IPPO (Baseline)
MAPPO
Gen 0
Gen 1

80

50

Gen 0

800

1000

(c) Overcooked-C: Lettuce-Tomato Salad
100

Gen 2
Gen 3
Gen 4

Mac-Based Baseline

60
40

Gen 2
Gen 3
Gen 4

Mac-Based Baseline

20
0
20
40

25
1000

200

IPPO (Baseline)
MAPPO
Gen 0
Gen 1

125

Mac-Based Baseline

25

0

(b) Overcooked-B: Lettuce-Tomato Salad

Episode Return Mean

IPPO
MAPPO
Gen 0
Gen 1

150

50

50

1000

Mac-Based Baseline

Episode Return Mean

50

100

IPPO (Baseline)
MAPPO
Gen 0
Gen 1
100
Gen 2
Gen
3
Mac-Based
Baseline
150

Episode Return Mean

Episode Return Mean

100 Mac-Based Baseline

IPPO (Baseline)
MAPPO
Gen 0
Gen 1
Gen 2

150

Episode Return Mean

IPPO (Baseline)
MAPPO
Gen 0
Gen 1

150

0

Gen 0

200

Gen 1

400

Gen 2

600

Iteration

Gen 3

800

Gen 4

1000

60

0

Gen 0

200

Gen 1

400

Gen 2

600

Gen 3

Iteration

800

Gen 4

1000

(d) Overcooked-A : Lettuce-Onion-Tomato (e) Overcooked-B : Lettuce-Onion-Tomato (f) Overcooked-C : Lettuce-Onion-Tomato
Figure 3. Performance comparison of M3 HF against baseline methods across different Overcooked environments and recipes. The plots
show the mean episode return over 1000 training iterations (approximately 25k episodes) for (a-c) Lettuce-Tomato salad recipe and
(d-f) Lettuce-Onion-Tomato salad recipe in Overcooked layouts A, B, and C, respectively. M3 HF consistently outperforms the baseline
methods (Mac-based Baseline, IPPO, MAPPO) across all scenarios, with performance improvements becoming more pronounced in
more complex environments and recipes. Vertical lines indicate the start of each generation where human feedback is incorporated. All
experiments are run with three random seeds, and the shaded areas represent the standard deviation.

weighted combination of the base reward and the consistency weight:

functions:
wi,m = wi,m · αM −m , ∀m ∈ 1, . . . , M − 1,

i
R̂k+1
(s, a) =

X
j

wi,j · Rji (s, a) , ∀Rji ∈ Pi

(13)

(12)
where α ∈ (0, 1) is a constant decay factor. We then normalize all weights by using

i
Here, R̂k+1
(s, a) denotes the final reward function for agent
i after processing the k-th generation of human feedback,
and it will be used for the next generation k + 1 for the
policy training and await the subsequent rollout generation
and human interaction, as outlined in Algorithm 1.

wi,m
wi,m = P
, ∀m ∈ 1, . . . , M .
wi,m

(14)

Additionally, we introduce a performance-based adjustment
rule that compares the agent’s performance under the origii
nal reward function Rori
across consecutive generations. We
i
calculate rori k+1 −rori ik , where rori ik+1 is the performance of
i
the policy trained using the new reward function R̂k+1
(s, a)
(after processing human feedback in generation k) when
i
evaluated on Rori
, and rori ik is the performance of the policy
trained using the previous reward function Rki (s, a) (before
processing human feedback in generation k) when evaluated
i
on Rori
. If this difference is positive, it indicates that the
new reward function leads to improved performance on the
original task. Otherwise, it suggests that the new reward
function may be detrimental to the agent’s performance on
the original task. We then adjust the weight of the newest

One main challenge is how to set the weights wi,j which can
effectively balance different reward components and adapt
to changing human feedback. To address this challenge, we
employ weight decay and performance-based adjustment to
optimize the weights wi,j .
4.3. Weight Decay and Performance-based Adjustment
The straightforward way to adjust weights is based on a
simple weight decay mechanism and performance feedback.
When generating a new reward function, we add it to the
pool Pi , then set an initial weight, wi,m = |Pi1|+1 . Then,
we apply a decay to existing weights of the former reward
6

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

reward function component wi,m as follows:
(
wi,m + β,
if rori ik+1 − rori ik > 0,
wi,m =
max(0, wi,m − β), otherwise,
(15)
where β is a small adjustment factor. This approach allows
for the dynamic adjustment of the reward function pool,
emphasizing recent human feedback while maintaining a diverse set of reward components and adapting to performance
changes.

where δ is a bounded, positive constant independent of K,
and each ij denotes a generation index at which the received
human feedback reward is beneficial (i.e., yields a positive
increment ∆rij > 0).
Intuitively, Proposition 4.2 formalizes the notion that our
algorithm accumulates the positive contributions of helpful
feedback over multiple rounds of interaction. In contrast,
its performance can suffer at most a single bounded degradation, reflecting the limited and transient influence of the
most recent faulty feedback. Importantly, this robustness
property ensures stable long-term learning dynamics even
when human inputs are imperfect or mixed quality. We
provide a detailed proof of this proposition in Appendix B.

4.4. Analysis of the Low-Quality Feedback
We now examine the robustness of our proposed M3 HF
framework under the scenario where human feedback of
mixed quality—including noisy, irrelevant, or erroneous
instructions—is provided to the agents. Such situations
naturally arise due to confusion, limited human domain
expertise, misinterpretation of agent behaviors, or misunderstanding of the task objectives. Under these conditions,
it is crucial for a robust algorithm to clearly mitigate the
negative impact from low-quality feedback signals, while
consistently leveraging high-quality feedback to enhance
learning. Formally, at each generation k, we integrate rewards by forming a weighted combination of the original
task-based reward and the accumulated human-derived feedback rewards:
k
X
k
b
Rk =
wm
Rm ,
(16)

5. Experiment
In our experiment, we aim to address three key questions:
Q1. What is the overall performance of M3 HF compared
to current state-of-the-art methods? Q2. To what extent
does multi-quality human feedback impact the performance
of M3 HF within the same environment? Q3. Can VisionLanguage Models (VLMs) serve as a scalable and effective
alternative to human feedback in M3 HF? In all experiments
involving language-driven feedback parsing, we use the
LLM gpt-4o-2024-11-20 (OpenAI, 2024).
Environment - Macro-Action-Based Overcooked, as
shown in Figure 2. In our experiments, we utilize a challenging multi-agent environment based on the Overcooked
game (Wu et al., 2021; Xiao et al., 2022), where three agents
must learn to cooperatively prepare a correct salad and deliver it to a designated delivery cell. We followed the work
of Xiao et al. (2022), where agents operate using macroactions derived from primitive actions. These macro-actions
facilitate effective navigation and interaction within the environment but also introduce complexities in learning optimal
policies due to the increased action space. Each agent observes only the positions and statuses of entities within a
5 × 5 square centered on itself, introducing partial observability and heightening the coordination challenge. The
agents will only receive a significant reward for delivering
the correct salad (+200) and punishment if they deliver the
wrong salad or food (-50). During training, each generation
consists of 200 iterations; each iteration runs 25 episodes of
up to 200 timesteps. For more details about the environment
setting, please refer to the Appendix C.

m=0

where we define R0 = Rori as the original reward function and Rm>0 = Rhuman as human-generated feedback
k
rewards. Correspondingly, the weights wm
are dynamically
adjusted as described in Equation 15, explicitly accounting
for observable agent performance improvements or degradations.
Due to the design of our weighting mechanism, negative or
unhelpful feedback rapidly loses influence, as their corresponding weights decrease after any observed dip or stagnation of performance improvements. Conversely, helpful
feedback continuously guides performance upwards, maintaining substantial influence through increased weighting in
the combined reward function. We formalize this intuition
into the following robustness result:
Proposition 4.2 (Robustness to Low-Quality Human Feedback). Under Assumption A.3 (Performance Estimation Accuracy) and Assumption A.2 (Learning Algorithm Convergence), for any given integer K ≥ 1 and any arbitrary
sequence of human feedback rewards (Rk )k=1,2,...,K , the
performance improvement satisfies:

Baselines. We evaluate against three strong multi-agent
reinforcement learning approaches: The MAPPO (Yu et al.,
2022), IPPO (De Witt et al., 2020), and a Macro-ActionBased Baseline from Xiao et al. (2022). Our own framework adopts IPPO as the backbone algorithm, while the
macro-action baseline is the average performance of the two
best-performing methods in Xiao et al. (2022), namely Mac-

n(K)

Jori (πK ) − Jori (π0 ) ≥

X

∆rij − (δ − ϵ),

(17)

j=1

7

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
2. Mid-game: The red agent
successfully passes on the
onion and tomato.

Human Feedbac k
" The bl ue and gr een chef s s houl d not s er v e t he
t omat o and oni on sal ad, whi c h i s t he wr ong
var i et y . Af t er r ecei v i ng t he l et t uc e, t hey
shoul d cut i t and mi x i t bef or e s er vi ng i t .
The r ed agent shoul d not t ouc h t he veget abl es
af t er pl aci ng t hem on t he mi ddl e t abl e. "

Low Qual i t y Feedbac k
" The agent s s ucces sf ul l y compl et e t he t as k of
c r eat i ng t wo l et t uc e, oni on, and t omat o
sal ads. Not hi ng t o i mpr ov e"

VLM Feedbac k

3. Important: Blue Agent mixes
onion and tomato salad, but
also needs to mix sliced ?
lettuce.

4. Endgame: The blue agent
delivered the wrong salad, the
lettuce was not sliced ?
and
blended into the salad.

" Rose agent , you c oul d cons i der br i ngi ng t he
k ni f e t o t he i ngr edi ent s and choppi ng t hem
near t hei r sour ce t o s av e t i me. Bot h gr een
agent s , you c an i mpr ov e coor di nat i on. Whi l e
one agent c hops, t he ot her coul d al r eady be
r et r i evi ng a pl at e or pr epar i ng t he nex t
i ngr edi ent . "

(a) Example rollout in Generation 3 (b) Feedback example from different source

125

Episode Return Mean

1. Game Start: The red agent
needs to pass the vegetables
from right to left.

IPPO (Baseline)
MAPPO
M3HF (w/ Low Quality Feedback)
100
M3HF (w/ VLM Feedback)
M3HF (w/ Ideal Human Feedback)
75
Mac-Based Baseline
50
25
0
25
0

Gen 0

200

Gen 1

400

Gen 2

600

Iteration

Gen 3

800

Gen 4

1000

(c) Overcooked-B : Lettuce-Onion-Tomato Salad

Figure 4. Impact of Mixed-Quality Feedback on Agent Performance. (a) An example is the rollout in Generation 3, where agents exhibit
suboptimal behavior due to poor coordination and inefficient task execution. (b) Low-quality feedback provided to the agents, inaccurately
stating that they successfully completed the task and offering no constructive guidance for improvement. (c) Performance comparison on
Overcooked-B with the Lettuce-Onion-Tomato salad recipe under mixed quality feedback conditions.

Experiment Results for Question 2: Impact of MixedQuality Human Feedback We evaluated our method when
facing low-quality feedback by simulating such feedback
at each generation. For example, in the rollout shown in
Figure 4a, agents exhibited suboptimal behavior due to poor
coordination. Despite this, the low-quality feedback inaccurately stated, “The agents successfully complete the task of
creating two lettuce, onion, and tomato salads. Nothing to
improve,” as depicted in Figure 4b. When training with this
irrelevant or erroneous feedback, the agents’ performance,
illustrated in Figure 4c, remained only slightly below that
of the baseline IPPO algorithm and did not degrade significantly. This outcome supports Proposition 4.2, demonstrating that M3 HF effectively mitigates the impact of unhelpful
feedback through its weight adjustment mechanisms. Even
with mixed-quality human input, the framework maintains
performance close to the backbone algorithm, showcasing
its resilience to low-quality guidance.

IAICC and Mac-CAC, over 25,000 training episodes. Further details on these baselines can be found in Appendix D.
Experiment Results for Question 1: Overall Performance of M3 HF Figure 3 demonstrates the superior performance of M3 HF compared to SOTA baselines across
various Overcooked environments and recipes. Our method
consistently outperforms Mac-based Baseline, IPPO, and
MAPPO in all scenarios, maintaining a substantial performance advantage across different levels of task complexity.
The method exhibits accelerated learning, particularly in
early training stages, and achieves higher asymptotic performance levels. Notably, in the simpler recipe setting,
Figure 3a, 3b and 3c, M3 HF converges to the optimal performance less than five rounds of interaction, showcasing
the method’s exceptional efficiency in more straightforward
settings. The method’s robustness is evident as we move
to more complex environments. In the challenging Layout
C (Figure 3c and 3f), M3 HF maintains its effective performance advantage, particularly outperforming its backbone
algorithm IPPO. This consistent superiority across varying
complexity levels underscores M3 HF’s effectiveness and
adaptability in diverse multi-agent scenarios.

Experiment Results for Question 3: VLM-based Feedback Generation We explore the potential of VLMs as an
alternative to human feedback. The VLM is given the same
video rollouts that humans would observe, sampled at a rate
of 1 frame per second. Using all sampled frames and a
prompt asking for feedback (detailed in Appendix D.4), the
VLM generates feedback to the training agents. In our implementation, we leverage Gemini-1.5-Pro-002 (Reid
et al., 2024), which is chosen for its multimodal understanding capability across a long context. We showcase
an example of VLM feedback in Figure 4b alongside the
human feedback. Here, the feedback provided by the VLM
resembles human-like style but lacks specificity on critical
issues, which, in this case, are “wrong variety”, “cut it and

In addition, it is important to note that the baseline MAPPO
employs a shared policy among agents alongside a centralized value function, while IPPO utilizes independent
policies. We have observed that IPPO often achieves better
results in highly coordination-intensive scenarios such as
Overcooked, potentially due to reduced interference among
agents during policy training. Prior studies (De Witt et al.,
2020; Yu et al., 2022) similarly report that IPPO can match
or outperform MAPPO even without centralized critics.

8

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality
Table 1. Ablation results comparing feedback parsing and weight
adjustments. Overcooked-B: Lettuce-Onion- Tomato Salad scenario.
Method
Raw Feedback (w/o parsing)
LLM Parsing Only (no weight adj.)
Full M3 HF (LLM parsing + weight adj.)

Table 3. Comparison of intrinsic reward-based methods with our
M3 HF on Overcooked-B: Lettuce-Onion- Tomato Salad task. Evaluation at training iterations 400, 600, 800, and 1000 (corresponding
to Generations 1–4 in M3HF). Mean and standard deviation reported over three seeds.

Average Return (Mean ± Std)
45.3 ± 5.2
68.7 ± 4.1
102.7 ± 10.8

Table 2. Performance comparison between single-phase and multiphase feedback methods. Overcooked-B: Lettuce-Onion- Tomato
Salad scenario.
Method
Single-phase feedback (initial only)
Multi-phase feedback (M3 HF)

Algorithm

Iter. 400

Iter. 600

Iter. 800

Iter. 1000

IPPO (base)
IRAT-rw 1
IRAT-rw 2
IRAT-rw 3

19.2 ± 4.5
68.9 ± 10.1
1.1 ± 2.1
10.8 ± 9.1

23.1 ± 2.7
52.5 ± 11.3
9.3 ± 11.4
17.3 ± 10.6

23.2 ± 3.3
78.2 ± 14.5
16.0 ± 8.1
21.3 ± 8.7

27.4 ± 4.9
94.9 ± 10.7
34.5 ± 14.0
33.8 ± 9.9

M3 HF (Ours)

164.8 ± 1.2

—

—

—

Average Return (Mean ± Std)
43.1 ± 10.3
102.7 ± 10.8

as rw 1, rw 2, rw 3) reflecting progressively stronger coordination requirements:

mix it before serving it”. Instead it offers vague suggestions
like “improve coordination”, which is hard to translate into
reward design. This limitation is indicative of the VLM’s
current inability to perform complex reasoning across images. As a result, when plugged into our M3 HF framework,
the VLM feedback method does not yield much benefit,
as shown in Figure 4c. Nonetheless, we expect improved
performance with future advancements in VLM.

• rw 1: Rewards an agent whenever it picks up or chops
any ingredient.
• rw 2: Rewards the agent upon successfully reaching
the knife after picking up an ingredient.

Ablation 1: Impact of Feedback Parsing and Weight
Adjustment. We compare our complete proposed feedback
integration pipeline (“Full M3 HF”) against two variants:
Raw Feedback, which directly translates human instructions
into rewards without parsing or structured adjustment, and
LLM Parsing Only, which parses human feedback into structured reward functions without performance-based weight
adjustment. The results are summarized in Table 1. The
results indicate that structured parsing of feedback impacts
considerably performance, increasing average returns significantly compared to raw human feedback. Furthermore,
adding weight adjustment mechanisms based on agent performance improvements further amplifies policy learning
efficiency, underscoring the importance of combining structured parsing and dynamic reward weighting.

• rw 3: Rewards the agent only upon successfully chopping an ingredient after reaching the knife, approximating an optimal coordination strategy.
The results summarized in Table 3 show that while intrinsic
reward methods (IRAT variants) typically enhance performance over vanilla IPPO, they still remain significantly
inferior to M3 HF, especially in the early stages of training. This is largely due to IRAT’s reliance on predefined
reward structures generated prior to observing actual policy behaviors, resulting in suboptimal coordination patterns
(e.g., multiple agents crowding the same object). In contrast, M3 HF utilizes human feedback derived from observed
rollouts, precisely identifying and targeting coordination
failures, enabling agents to quickly improve policy behaviors and achieve superior cooperative results.

Ablation 2: Single-phase vs. Multi-phase Feedback. We
compared our proposed multi-phase feedback collection
methodology against a single-phase scenario (feedback provided only once at the start of training). Results are summarized in Table 2. Table 2 demonstrates superior performance
for our multi-phase framework over single-phase feedback,
highlighting the effectiveness of iterative, incremental feedback in enabling agents’ policy refinement across multiple
training stages.

6. Conclusion
In this paper, we introduced M3 HF, a novel framework for
MARL that incorporates multi-phase human feedback of
mixed quality to address the challenges of sparse or complex
reward signals. By extending the Markov Game to include
human input and leveraging LLMs to parse and integrate
human feedback, our approach enables agents to learn more
effectively. Empirically, M3 HF outperforms strong baselines, particularly in scenarios with increasing complexity.
Our findings highlight the potential of integrating diverse
human insights to enhance multi-agent policy learning in a
more accessible way.

Ablation 3: Comparison to Intrinsic Reward Methods.
We further evaluated the effectiveness of M3 HF relative
to intrinsic reward-based methods, specifically comparing
against the IRAT method (Wang et al., 2022). As Overcooked lacks built-in intrinsic rewards, we manually constructed three variants of intrinsic reward functions (denoted
9

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Impact Statement

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,
S., and Amodei, D. Deep reinforcement learning from
human preferences. In Advances in Neural Information
Processing Systems, volume 30, 2017.

This research introduces M3 HF, a framework that enables
multi-agent reinforcement learning systems to learn effectively from human feedback of varying quality. By allowing
non-expert humans to provide meaningful feedback to AI
systems, M3 HF democratizes the development of multiagent systems while making them more robust to real-world
situations where perfect expert guidance may not be available. This could accelerate the deployment of collaborative
AI systems in areas such as healthcare, manufacturing, and
emergency response, where multiple agents need to coordinate while incorporating human domain knowledge. While
this increased accessibility could lead to broader adoption,
we acknowledge the importance of appropriate oversight
and encourage future work to explore necessary safeguards
for such systems.

De Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk,
V., Torr, P. H., Sun, M., and Whiteson, S. Is independent learning all you need in the starcraft multi-agent
challenge? arXiv preprint arXiv:2011.09533, 2020.
Du, Y., Han, L., Fang, M., Liu, J., Dai, T., and Tao, D. Liir:
Learning individual intrinsic reward in multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, volume 32, 2019.
Du, Y., Leibo, J. Z., Islam, U., Willis, R., and Sunehag, P.
A review of cooperation in multi-agent learning. arXiv
preprint arXiv:2312.05162, 2023.

Acknowledgment

Durrett, R. Probability: Theory and Examples. Cambridge
University Press, 4th edition, 2010.

This work was supported by the Engineering and Physical
Sciences Research Council [grant number EP/Y003187/1,
UKRI849]. This work was also supported in part by NSF
grant IIS-2046640 (CAREER).

Ho, J. and Ermon, S. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, volume 29, 2016.
Hu, H. and Sadigh, D. Language instructed reinforcement
learning for human-ai coordination. In International Conference on Machine Learning, pp. 13584–13598. PMLR,
2023.

References
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,
R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel,
O., and Zaremba, W. Hindsight experience replay. In
Advances in Neural Information Processing Systems, volume 30, 2017.

Hu, Y., Goel, K., Killiakov, V., and Yang, Y. Eigenspectrum
analysis of neural networks without aspect ratio bias. In
International Conference on Machine Learning, 2025.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Programming. Athena Scientific, 1996.

Klissarov, M., D’Oro, P., Sodhani, S., Raileanu, R., Bacon,
P.-L., Vincent, P., Zhang, A., and Henaff, M. Motif:
Intrinsic motivation from artificial intelligence feedback.
arXiv preprint arXiv:2310.00166, 2023.

Busoniu, L., Babuska, R., and De Schutter, B. A comprehensive survey of multiagent reinforcement learning. IEEE
Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), 38(2):156–172, 2008.

Knox, W. B. and Stone, P. Interactively shaping agents via
human reinforcement: The tamer framework. In Proceedings of the fifth international conference on Knowledge
capture, pp. 9–16, 2009.

Canese, L., Cardarilli, G. C., Di Nunzio, L., Fazzolari, R.,
Giardino, D., Re, M., and Spanò, S. Multi-agent reinforcement learning: A review of challenges and applications.
In Applied Sciences, volume 11, 2021.

Kolmogorov, A. N. Sulla determinazione empirica di una
legge di distribuzione. Giornale dell’Istituto Italiano
degli Attuari, 4:83–91, 1933.

Chen, V., Gupta, A., and Marino, K. Ask your humans:
Using human instructions to improve generalization in
reinforcement learning. In International Conference on
Learning Representations, 2021.

Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
Stabilizing off-policy q-learning via bootstrapping error
reduction. In Advances in Neural Information Processing
Systems, volume 32, 2019.

Chen, X.-H., Wang, Z., Du, Y., Jiang, S., Fang, M., Yu,
Y., and Wang, J. Policy learning from tutorial books via
understanding, rehearsing and introspecting. In Advances
in Neural Information Processing Systems, volume 37,
2024.

Kurach, K., Raichuk, A., Stańczyk, P., Zajac, M., Bachem,
O., Espeholt, L., Riquelme, C., Vincent, D., Michalski,
M., Bousquet, O., et al. Google research football: A novel
reinforcement learning environment. In Proceedings of
10

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

the AAAI conference on artificial intelligence, volume 34,
pp. 4501–4510, 2020.

Nguyen, D. T., Kumar, A., and Lau, H. C. Credit assignment for collective multiagent rl with global rewards.
In Advances in Neural Information Processing Systems,
volume 31, 2018.

Kwon, M., Xie, S. M., Bullard, K., and Sadigh, D. Reward
design with language models. In The Eleventh International Conference on Learning Representations, 2023.

OpenAI.
GPT-4o (gpt-4o-2024-11-20).
https:
//platform.openai.com/models/
gpt-4o-2024-11-20, 2024.
Large language
model.

Lee, K., Smith, L., and Abbeel, P. Pebble: Feedbackefficient interactive reinforcement learning via relabeling
experience and unsupervised pre-training. In International Conference on Machine Learning, 2021.

Oroojlooy, A. and Hajinezhad, D. A review of cooperative multi-agent deep reinforcement learning. Applied
Intelligence, 53(11):13677–13722, 2023.

Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on
open problems. arXiv preprint arXiv:2005.01643, 2020.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,
Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,
et al. Training language models to follow instructions
with human feedback. In Advances in Neural Information
Processing Systems, volume 35, 2022.

Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B.,
Florence, P., and Zeng, A. Code as policies: Language
model programs for embodied control. In 2023 IEEE
International Conference on Robotics and Automation,
2023.

Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning,
2017.

Liang, J., Xia, F., Yu, W., Zeng, A., Arenas, M. G., Attarian,
M., Bauza, M., Bennice, M., Bewley, A., Dostmohamed,
A., et al. Learning to learn faster from human feedback
with language model predictive control. arXiv preprint
arXiv:2402.11450, 2024.

Puterman, M. L. Markov decision processes. Handbooks
in Operations Research and Management Science, 2:331–
434, 1990.

Littman, M. L. Markov games as a framework for multiagent reinforcement learning. In Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994.

Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,
O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
arXiv preprint arXiv:2403.05530, 2024.

Liu, R., Bai, F., Du, Y., and Yang, Y. Meta-reward-net:
Implicitly differentiable reward learning for preferencebased reinforcement learning. In Advances in Neural
Information Processing Systems, volume 35, 2022.

Shani, L., Rosenberg, A., Cassel, A., Lang, O., Calandriello,
D., Zipori, A., Noga, H., Keller, O., Piot, B., Szpektor,
I., Hassidim, A., Matias, Y., and Munos, R. Multi-turn
reinforcement learning from preference human feedback.
In Advances in Neural Information Processing Systems,
volume 37, 2024.

Liu, Z., Hu, Y., Pang, T., Zhou, Y., Ren, P., and Yang,
Y. Model balancing helps low-data training and finetuning. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N.
(eds.), Empirical Methods in Natural Language Processing, 2024.

Singh, S., Lewis, R. L., and Barto, A. G. Where do rewards come from. In Conference of the Cognitive Science
Society, 2009.

Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O.,
Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A.
Eureka: Human-level reward design via coding large language models. In The Twelfth International Conference
on Learning Representations, 2024.

Song, Y., Jiang, H., Tian, Z., Zhang, H., Zhang, Y., Zhu, J.,
Dai, Z., Zhang, W., and Wang, J. An empirical study on
google research football multi-agent scenarios. Machine
Intelligence Research, 21(3):549–570, 2024.

Mannion, P., Devlin, S., Duggan, J., and Howley, E. Reward
shaping for knowledge-based multi-objective multi-agent
reinforcement learning. In The Knowledge Engineering
Review, volume 33, 2018.

Sumers, T., Hawkins, R., Ho, M. K., Griffiths, T., and
Hadfield-Menell, D. How to talk so ai will learn: Instructions, descriptions, and autonomy. In Advances in Neural
Information Processing Systems, volume 35, 2022.

Ng, A. Y., Russell, S., et al. Algorithms for inverse reinforcement learning. In International Conference on Machine
Learning, 2000.

Sutton, R. S. Reinforcement learning: An introduction. A
Bradford Book, 2018.
11

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Wang, L., Zhang, Y., Hu, Y., Wang, W., Zhang, C., Gao, Y.,
Hao, J., Lv, T., and Fan, C. Individual reward assisted
multi-agent reinforcement learning. In International Conference on Machine Learning, 2022.

Zhi-Xuan, T., Ying, L., Mansinghka, V., and Tenenbaum,
J. B. Pragmatic instruction following and goal assistance via cooperative language-guided inverse planning.
In International Conference on Autonomous Agents and
Multiagent Systems, 2024.

Wang, Y., Krotov, D., Hu, Y., Gao, Y., Zhou, W., McAuley,
J., Gutfreund, D., Feris, R., and He, Z. M+: Extending memoryllm with scalable long-term memory. In The
Twelfth International Conference on Learning Representations, 2025.

Zhou, M., Liu, Z., Sui, P., Li, Y., and Chung, Y. Y. Learning
implicit credit assignment for cooperative multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, volume 33, 2020.

Wang, Z., Fang, M., Tomilin, T., Fang, F., and Du, Y. Safe
multi-agent reinforcement learning with natural language
constraints. arXiv preprint arXiv:2405.20018, 2024.

Zou, H. P., Huang, W.-C., Wu, Y., Chen, Y., Miao, C.,
Nguyen, H., Zhou, Y., Zhang, W., Fang, L., He, L., et al.
A survey on large language model based human-agent
systems. arXiv preprint arXiv:2505.00753, 2025.

Wu, S. A., Wang, R. E., Evans, J. A., Tenenbaum, J. B.,
Parkes, D. C., and Kleiman-Weiner, M. Too many cooks:
Bayesian inference for coordinating multi-agent collaboration. In Topics in Cognitive Science, volume 13, 2021.
Xiao, Y., Tan, W., and Amato, C. Asynchronous actorcritic for multi-agent reinforcement learning. In Advances
in Neural Information Processing Systems, volume 35,
2022.
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
A., and Wu, Y. The surprising effectiveness of ppo in
cooperative multi-agent games. In Advances in Neural
Information Processing Systems, volume 35, 2022.
Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Arenas, M. G., Chiang, H.-T. L., Erez, T., Hasenclever, L.,
Humplik, J., et al. Language to rewards for robotic skill
synthesis. In Conference on Robot Learning, 2023.
Yuan, L., Gao, X., Zheng, Z., Edmonds, M., Wu, Y. N.,
Rossano, F., Lu, H., Zhu, Y., and Zhu, S.-C. In situ
bidirectional human-robot value alignment. In Science
robotics, volume 7, 2022.
Zhang, K., Yang, Z., and Başar, T. Multi-agent reinforcement learning: A selective overview of theories and algorithms. In Handbook of Reinforcement Learning and
Control, pp. 321–384. Springer, 2021.
Zhang, N., Wang, X., Cui, Q., Zhou, R., Kakade, S. M., and
Du, S. S. Multi-agent reinforcement learning from human feedback: Data coverage and algorithmic techniques.
arXiv preprint arXiv:2409.00717, 2024.
Zhang, T., Xu, H., Wang, X., Wu, Y., Keutzer, K., Gonzalez, J. E., and Tian, Y. Multi-agent collaboration
via reward attribution decomposition. arXiv preprint
arXiv:2010.08531, 2020.
Zhang, X., Guo, Y., Stepputtis, S., Sycara, K., and Campbell, J. Understanding your agent: Leveraging large
language models for behavior explanation. arXiv preprint
arXiv:2311.18062, 2023.
12

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

A. Assumptions

and

We make the following assumptions to facilitate the analysis:

J(πk ) − J(π0 ) ≥

∀s, a.

The proof proceeds in three steps:

(18)
Step 1: Performance at positive improvement milestones
(k = ij−1 ). By definition of ij , we clearly have:

Assumption A.2 (Learning Algorithm Convergence).
Given a fixed reward function, the learning algorithm converges to a policy that is ϵ-optimal with respect to the expected cumulative reward under that reward function:
i
lim E[JR
(πti )] ≥ JiR (π∗i ) − ϵ,

for all ij−1 < k < ij ,

where δ is a small bounded positive constant defined explicitly later.

This assumption is standard in reinforcement learning to
ensure stability and convergence (Sutton, 2018).

t→∞

∆ril − δ,

l=1

Assumption A.1 (Bounded Rewards). All reward functions,
i
including the original reward Rori
, the true human feedback
i
reward Rtrue k , and the noisy human feedback reward Rhf ik ,
are uniformly bounded:
|R(s, a)| ≤ Rmax ,

j−1
X

J(πij−1 ) − J(π0 ) =

j−1
X

j−1
 X
J(πil ) − J(πil−1 ) =
∆ril .

l=1

l=1

Step 2: Performance between positive milestones (ij−1 <
k < ij ). For indices between ij−1 and ij , human feedback results in negative or zero performance improvement
(∆rk ≤ 0). According to our algorithm and Eq. (15), the
weight assigned to any unhelpful feedback reward function is adjusted (reduced), and eventually clipped to zero.
Specifically, whenever this negative improvement occurs,
the adaptive weighting mechanism significantly reduces or
eliminates the influence of this poor-quality feedback. Thus,
the previously established optimal policy at timestep ij−1
remains largely unchanged. The largest possible degradation from the previous high-performance point is therefore
bounded by a constant δ > 0, independent of the total
interaction length K:

(19)

i
where JR
(π) is the expected cumulative reward for agent i
under policy π and reward function R, and π∗i is the optimal
policy for agent i under R.

Assumption A.3 (Performance Estimation Accuracy). The
i
i
estimate of the performance difference ∆rki = Jori
(πk+1
)−
i
Jori
(πki ) accurately reflects the true change in expected cui
mulative reward under the original reward function Rori
between generations k and k + 1.
This assumption relies on having sufficient samples to estimate the performance difference accurately, which can be
ensured through appropriate exploration and sample size.

J(πk ) − J(πij−1 ) ≥ −δ,

Assumption A.4 (Independent Roll-outs). The X rollout
(x)
trajectories {τk }X
x=1 collected under the fixed joint policy
πk are independent and identically distributed (i.i.d.).

∀ij−1 < k < ij .

Step 3: Combining two results. Combining the two preceding steps directly, we establish the performance lower
bound at any time step k within the interval:

B. Proof of Proposition 4.2
In this section, we provide a complete and structured proof
for Proposition 4.2.

J(πk ) − J(π0 )
= (J(πk ) − J(πij−1 )) + (J(πij−1 ) − J(π0 ))

Let ij denote the generation index at which the incorporation of human feedback explicitly yields a positive policy
improvement for the j-th time, and let n(K) represent the
number of times positive improvements (helpful human
feedback) occur until timestep K. Formally, each ij satisfies ∆rij > 0, and indices between ij−1 and ij denote
interactions with low-quality or unhelpful feedback where
∆rk < 0.

≥ −δ +

j−1
X

∆ril ,

for

∆ril .

l=1

Since this holds for all intervals of indices, applying it specifically to the final interval (after the last helpful feedback
index n(K)), we obtain the desired inequality:
n(K)

The proposition explicitly states that for any generation k in
the interval ij−1 ≤ k < ij , the performance satisfies:
J(πk ) − J(π0 ) =

j−1
X

J(πK ) − J(π0 ) ≥

X

∆rij − δ.

j=1

k = ij−1 ,

Thus, Proposition 4.2 follows directly.

l=1

13

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

C. Environment Details

Explicit bound on the constant δ. We next explicitly
quantify the bounded constant δ appearing due to one step
of degraded performance. Consider the following general
lemma:
Lemma B.1. Refer to Puterman (1990) and Lemma 6.2
in Bertsekas & Tsitsiklis (1996), Let r1 , r2 , r3 be three
bounded reward functions. Define combined rewards:
(
R = (1 − p) r1 + p r2 ,

In this section, we will introduce the details of the environments we are using. We follow the setting from
Goal. Three agents need to learn cooperating with each
other to prepare a Tomato-Lettuce-Onion salad and deliver it
to the ‘star’ counter cell as soon as possible. The challenge
is that the recipe of making a tomato-lettuce-onion salad
is unknown to agents. Agents have to learn the correct
procedure in terms of picking up raw vegetables, chopping,
and merging in a plate before delivering.

R′ = (1 − p′ − q) r1 + p′ r2 + q r3 .
Let π and π ′ be the optimal policies corresponding respectively to R and R′ . Then the following performance bound
holds, for a discount factor γ ∈ (0, 1):

State Space. The environment is a 7×7 grid world involving
three agents, one tomato, one lettuce, one onion, two plates,
′
2
two cutting boards and one delivery cell. The global state
Vrπ1 − Vrπ1 ≤
∥ R − R′ ∥∞
information consists of the positions of each agent and
1−γ
 above items, and the status of each vegetable: chopped,
2  ′
|p + q − p| ∥ r1 − r3 ∥∞ + |p − p′ | ∥ r2 − r3 ∥∞ . unchopped, or the progress under chopping.
≤
1−γ

We now explicitly apply this lemma in our scenario. Consider the timestep k immediately after the last beneficial
feedback index ij , with:

Primitive-Action Space. Each agent has five primitiveactions: up, down, left, right and stay. Agents can
move around and achieve picking, placing, chopping and
delivering by standing next to the corresponding cell and
moving against it (e.g., in Figure 2a, the pink agent can
move right and then move up to pick up the tomato).

• r1 = Rori , the original environment reward.
• r2 = weighted combination of previously beneficial
feedback.

Macro-Action Space. Here, we first describe the main function of each macro-action and then list the corresponding
termination conditions.

• r3 = Rk , the current negative feedback reward.
Then, we have:

πi

j
k
VRπori
− VRori
≤

• Five one-step macro-actions that are the same as the
primitive ones;

2
[|p′ + q − p|∥Rori − Rk ∥∞ + |p − p′ |∥r2 − Rk ∥∞ ]
1−γ

(20)

• Chop, cuts a raw vegetable into pieces (taking three
time steps) when the agent stands next to a cutting
board and an unchopped vegetable is on the board,
otherwise it does nothing; and it terminates when:

By the current weight updating scheme (Eqs. 15), we explicitly have:
1/(2+j)
1
• q = 1+1/(2+j)
= 3+j
, and
′

j

– The vegetable on the cutting board has been
chopped into pieces;
– The agent is not next to a cutting board;
– There is no unchopped vegetable on the cutting
board;
– The agent holds something in hand.

j

• 0 ≤ p − p ≤ p − α p = p(1 − α ).
Noting that the feedback probability p is bounded (e.g. 0 ≤
p ≤ 1) and that all reward functions satisfy ∥r∥∞ ≤ Rmax ,
we obtain a uniform upper bound for δ. In particular,
δ ≤

i
2 h 1
− (1 − αj )p ∥Rori − Rk ∥∞ + p (1 − αj ) ∥r2 − Rk ∥∞ .
3+j
1−γ

• Get-Lettuce, Get-Tomato, and Get-Onion, navigate the
agent to the latest observed position of the vegetable,
and pick the vegetable up if it is there; otherwise, the
agent moves to check the initial position of the vegetable. The corresponding termination conditions are
listed below:

(21)
Since p ≤ 1 and ∥Rori − Rk ∥∞ , ∥r2 − Rk ∥∞ ≤ Rmax ,
the right-hand side is a finite constant independent of the
total iteration count K. This uniform bound on δ highlights
the robustness of our feedback-integration mechanism even
when occasional poor-quality feedback occurs.

– The agent successfully picks up a chopped or unchopped vegetable;

This completes the full proof.
14

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

– The agent observes the target vegetable is held by
another agent or itself;
– The agent is holding something else in hand;
– The agent’s path to the vegetable is blocked by
another agent;
– The agent does not find the vegetable either at the
latest observed location or the initial location;
– The agent attempts to enter the same cell with another agent, but has a lower priority than another
agent.

– The agent attempts to enter the same cell with another agent, but has a lower priority than another
agent.
Observation Space: The macro-observation space for each
agent is the same as the primitive observation space. Agents
are only allowed to observe the positions and status of the
entities within a 5 × 5 view centered on the agent.
The initial position of all the items are known to agents.

• Get-Plate-1/2, navigates the agent to the latest observed position of the plate, and picks the vegetable
up if it is there; otherwise, the agent moves to check
the initial position of the vegetable. The corresponding
termination conditions are listed below:

Dynamics: The transition in this task is deterministic. If an
agent delivers any wrong item, the item will be reset to its
initial position. From the low-level perspective, to chop a
vegetable into pieces on a cutting board, the agent needs to
stand next to the cutting board and executes left three times.
Only the chopped vegetable can be put on a plate.

– The agent successfully picks up a plate;
– The agent observes the target plate is held by another agent or itself;
– The agent is holding something else in hand;
– The agent’s path to the plate is blocked by another
agent;
– The agent does not find the plate either at the latest
observed location or at the initial location;
– The agent attempts to enter the same cell with
another agent but has a lower priority than another
agent.

Original Reward Function: +10 for chopping a vegetable,
+200 terminal reward for delivering a correct salad
(like tomato-lettuce-onion or tomato-lettuce salad), −5
for delivering any wrong entity, and −0.1 for every timestep.
Episode Termination: Each episode terminates either when
agents successfully deliver a tomato-lettuce-onion salad or
reaching the maximal time steps, 200.

• Go-Cut-Board-1/2, navigates the agent to the corresponding cutting board with the following termination
conditions:

D. Implementation Details
D.1. Algorithm
In here, we list the complete algorithm, as shown in Algorithm.1.

– The agent stops in front of the corresponding cutting board, and places an in-hand item on it if the
cutting board is not occupied;
– If any other agent is using the target cutting board,
the agent stops next to the teammate;
– The agent attempts to enter the same cell with
another agent but has a lower priority than another
agent.

D.2. Predefined Reward Function Templates
To effectively incorporate human feedback into the learning
process, we define a set of predefined reward function templates F that can be parameterized based on the feedback
and entities present in the environment. These templates
capture common interaction patterns between agents and
their environment, facilitating automatic reward function
generation aligned with human intentions.

• Go-Counter (only available in Overcook-B, Figure 2
b), navigates the agent to the center cell in the middle
of the map when the cell is not occupied, otherwise it
moves to an adjacent cell. If the agent is holding an
object the object will be placed. If an object is in the
cell, the object will be picked up.

Firstly, the distance-based reward function penalizes the
agent proportionally to the Euclidean distance between two
entities e1 and e2 within the environment:

• Deliver, navigates the agent to the ‘star’ cell for delivering with several possible termination conditions:
fdist (s, a, e1 , e2 ) = −∥s[e1 .pos] − s[e2 .pos]∥2 ,

– The agent places the in-hand item on the cell if it
is holding any item;
– If any other agent is standing in front of the ‘star’
cell, the agent stops next to the teammate;

(22)

where s[ei .pos] denotes the position vector of entity ei in
state s, and ∥ · ∥2 represents the Euclidean norm.
15

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Algorithm 1 M3 HF: Multi-agent Reinforcement Learning
from Multi-phase Human Feedback of Mixed Quality

Secondly, the action-based reward function provides a
reward when the agent performs a specific desired action
adesired :

Require: Number of agents N , Original Reward Functions
{Riori }N
i=1 , Predefined Reward Templates F , Environment E, Initial Policies {π i,0 }N
i=1 , Total Generations
K
Ensure: Trained Policies {π i,K }N
i=1
1: Initialize Reward Function Pools Pi = {Riori } for each
agent i
2: for generation k = 0 to K − 1 do
▷ Eq. 4
3: Multi-agent Training Phase

faction (s, a, adesired ) = I(a = adesired ),

where a is the action taken by the agent, and I(·) is the
indicator function, returning 1 if the condition is true and 0
otherwise.
Thirdly, the status-based reward function rewards the
agent when an entity e attains a particular desired status
statusdesired :

for each agent i do
Train policy π i,k using current reward function
R̂ik (s, a) (Eq. 12)
6:
end for
7: Rollout Generation
▷ Sec. 4.1
8:
if Periodic evaluation or performance stagnation
detected then
Generate rollout trajectories τk
=
9:
{(st , at , rt )}H−1
t=0
10:
end if
11: Human Feedback Phase
▷ Sec. 4.2
12:
Human observes τk and provides feedback uk
13:
Feedback Parsing:
14:
Use LLM M to parse uk and assign feedback to
agents:
uik , uall
k = M(uk , N )
4:
5:

15: Reward Function Update

fstatus (s, a, e, statusdesired ) = I(s[e.status] = statusdesired ),
(24)
where s[e.status] represents the current status of entity e in
state s.
Additionally, we define a composite reward function that
allows for more nuanced feedback by combining multiple
reward components:
fcomp (s, a) =

X

λi fi (s, a),

(25)

i

where fi (s, a) are individual reward components (e.g.,
fdist , fstatus ), and λi are weighting coefficients that determine the relative importance of each component.

▷ Sec. 4.3

for each agent i do
Generate new reward function from feedback
(Eq. 10):
Ri,new = M(F, uik , uall
k , e)
18:
Add Ri,new to reward function pool Pi
19:
end for
20:
Weight Update:
21:
for each agent i do
22:
Initialize weight for new reward function:
wi,M = |P1i |
23:
Apply weight decay to existing weights
(Eq. 15):
wi,m = wi,m · αM −m , ∀m ∈ {1, . . . , M − 1}
24:
Normalize weights:
w
wi,m = PM i,mw , ∀m ∈ {1, . . . , M }

16:
17:

j=1

(23)

For instance, given the human feedback “Agent 1 needs to
get the onion,” we might select the distance-based reward
template and parameterize it as:

Ri (s, a) = −∥s[Agent1.pos] − s[Onion.pos]∥2 .

(26)

This reward function encourages Agent 1 to minimize its
distance to the onion, thus aligning its behavior with the
desired objective.
Furthermore, other templates can be incorporated depending
on the environmental context and task requirements. For
example, a proximity-based reward function provides a
reward when an agent is within a certain distance d of a
target entity:

i,j

Compute performance difference:
ori
ori
∆ri = ri,k+1
− ri,k
26:
Adjust(weight of newest reward function:
wi,M + β,
if ∆ri > 0
wi,M =
max(0, wi,M − β), otherwise
27:
Update finalPreward function (Eq. 12):
M
R̂ik+1 (s, a) = m=1 wi,m · Ri,m (s, a)
28:
end for
29: end for
25:

(
fprox (s, a, e1 , e2 , d) =

rprox , if ∥s[e1 .pos] − s[e2 .pos]∥2 ≤ d,
0,
otherwise,
(27)

where rprox is the reward assigned for being within distance
d.
16

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

A time-based penalty can be introduced to encourage efficient task completion:
ftime (s, a, t) = −β · t,

Table 4. Hyperparameters used in Overcooked-A, B, and C.
Hyperparameter
Training Generation
Training Iterations
Training Episodes
Learning Rate
Training Batch Size
Minibatch Size
Epochs
Discount Factor (γ)
GAE Lambda (λ)
Clip Parameter
Value Function Clip Parameter
Entropy Coefficient
KL Coefficient
Gradient Clipping

(28)

where t is the current time step, and β is a penalty coefficient
reflecting the cost of time.
A success-based reward provides a reward upon achieving
a specific goal condition:

fsuccess (s, a) = I(goal condition met) · rsuccess ,

Baseline-IPPO

Baseline-MAPPO

5
1000
25k
0.0003
5120
1024
10
0.99
0.95
0.2
10.0
0.01
0.2
0.5

1000
25k
0.0003
5120
1024
10
0.99
0.95
0.2
10.0
0.01
0.2
0.5

1000
25k
0.0003
5120
1024
10
0.99
0.95
0.2
0.01
0.5

(29)
D.4. Prompts

where rsuccess is the reward value assigned when the goal
condition is met.

Prompt 1: FEEDBACK PARSING PROMPT

An energy-based penalty discourages unnecessary expenditure of resources:
fenergy (s, a) = −γ · energy(a),

M3 HF-IPPO

Given the following feedback for a multi-agent
system in an Overcooked environment, assign the feedback to appropriate agents or to
all agents. The system has {num agents}
agents.

(30)

where energy(a) represents the energy cost associated with
action a, and γ is a scaling factor.

Feedback: {Human Feedback}

By leveraging these templates, the system can systematically
generate reward functions that align with human feedback,
enabling agents to adapt their behavior effectively in response to diverse instructions. This approach allows for
the incorporation of mixed-quality human feedback into the
learning process, enhancing the agents’ ability to perform
complex tasks in multi-agent environments.

The agent 1 is the chef in Green, agent 2 is the
chef in Rose, agent 3 is the chef in Blue.
Return your response in the following JSON
format:
{{

D.3. Training Details
”agent 0”: ”feedback for agent 0”,
”agent 1”: ”feedback for agent 1”,
...
”all”: ”feedback for all agents”

Our experiments were conducted on a heterogeneous computing cluster running Ubuntu Linux. The hardware configuration included a variety of CPU models, such as Dual Intel
Xeon E5-2650, Dual Intel Xeon E5-2680 v2, and Dual Intel
Xeon E5-2690 v3. For accelerated computing, we utilized
3 NVIDIA A30 GPUs. The total computational resources
comprised 180 CPU cores and 500GB of system memory.

}}
Only include keys for agents that receive specific feedback and ’all’ if there’s general feedback.

We performed hyperparameter tuning for all baselines to
ensure fair comparison. Specifically, for IPPO and MAPPO,
we tuned learning rates, batch sizes, and gradient clipping values. For example, we systematically searched
over learning rates in {3e-4, 1e-4}, #sgd iters in {5, 10},
sgd batch size in {1024, 5120}, and entropy coefficient in
{0.01, 0.05}, ultimately selecting the configurations with
the best validation performance. For the macro-action based
baseline, we directly adopted the best-performing hyperparameters reported in Mac-based method (Xiao et al.,
2022). We will explicitly include these details in our revised manuscript.

Prompt 2:
PROMPT

REWARD FUNCTION BUILD

Given the parsed feedback for an agent in an
Overcooked environment, select and parameterize a reward function template.

17

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

{task name}.
The environment is a 7x7 grid with various
objects and {num agents} agents.

The observation space is a 32-length vector as
described in the task description.
Parsed Feedback: {feedback for this agent}

Observation Space (32-length vector for
each agent):
- Tomato: position (2), status (1)
- Lettuce: position (2), status (1)
- Onion: position (2), status (1)
- Plate 1: position (2)
- Plate 2: position (2)
- Knife 1: position (2)
- Knife 2: position (2)
- Delivery: position (2)
- Agent 1: position (2)
- Agent 2: position (2)
- Agent 3: position (2)
- Order: one-hot encoded (7)

Observation Space (32-length vector for each
agent):
- Tomato: position (2), status (1) (obs[0:2])
- Lettuce: position (2), status (1) (obs[3:5])
- Onion: position (2), status (1) (obs[6:8])
- Plate 1: position (2) (obs[9:10])
- Plate 2: position (2) (obs[11:12])
- Knife 1: position (2) (obs[13:14])
- Knife 2: position (2) (obs[15:16])
- Delivery: position (2) (obs[17:18])
- Agent 1: position (2) (obs[19:20])
- Agent 2: position (2) (obs[21:22])
- Agent 3: position (2) (obs[23:24])
- Order: one-hot encoded (7) (obs[25:32])

MA-V1 Actions (index indicates
action):
0: No operation
1: Move Up
2: Move Right
3: Move Down
4: Move Left
5: Interact (pick up, put down, chop)

Available function templates:
1. Distance-based: -sqrt((agent x - target x)**2
+ (agent y - target y)**2)
2. Action-based: reward for specific actions
(e.g., chopping, picking up)
3. State-based: reward for achieving specific
states (e.g., holding an item)
4. Time-based: penalty for time taken
5. Combination of the above

macro

You will be provided with a video of the
agents’ gameplay, which may be lengthy. Your
task is to:
1. Identify and summarize the key actions and
strategies employed by the agents throughout
the gameplay.
2. Provide constructive feedback based on
your observations in a single paragraph. Mark
this paragraph with [SUGGESTION].

Select a template and parameterize it based
on the feedback. Return your response as a
Python lambda function that takes the observation vector (obs) and action (act) as input.
For example, Distance between agent 1 and
tomato :
lambda obs, act: -sqrt((obs[19] - obs[0])**2 +
(obs[20] - obs[1])**2) # Distance between agent
1 and tomato

When generating feedback:
- Address specific agents by their color (e.g.,
Green agent, Rose agent) or position (e.g.,
agent on the left, agent near the cutting board).
- Focus on aspects of gameplay that could be
significantly improved for any or all agents.
- Offer specific, actionable suggestions that can
be immediately applied.
- Relate your feedback to the Overcooked
environment, tasks, and overall efficiency.
- Prioritize improvements in teamwork, task
allocation, or resource management.
- Consider how the suggestions could impact
the agents’ performance metrics.

Ensure that your function uses the correct indices from the observation vector as described
in the task description.

Prompt 3: VLM FEEDBACK PROMPT
You are an AI assistant helping to manage an
Overcooked environment with multiple agents.
The task is to prepare and deliver a

18

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Avoid: - Using overly technical jargon or
complex explanations.
- Giving vague or general advice not specific to
their gameplay.
- Mentioning anything outside the scope of the
Overcooked game.
- Using excessive praise or encouragement.
Provide a brief summary of the agents’
actions, followed by a single paragraph of feedback marked with [SUGGESTION], addressing
the agents directly about their gameplay in the
Overcooked environment. Focus on concrete
improvements for any or all agents rather than
motivational language.

E. Regarding the Scalability of Our Method
We extended our evaluation to Google Football 5v5 (Kurach et al., 2020; Song et al., 2024), a complex multi-agent
benchmark. M3HF continues to outperform standard MARL
baselines with the multi-phased human feedback. Full environment details are provided in the Figure 5,6 and their
captions.

19

Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality

Total Player Number
Training Iteration
Training Episodes
Episode Length
Training Timesteps
Deterministic
Offsides
End Episode on Score
end episode on out of play
end episode on possession change
Bot Level

(a) Rollout Screenshot

(b) Start Point

10
100
8000
3000
2.4e7
False
True
False
False
False
1.0

(c) 5-vs-5 full-game (5v5) configuration

Figure 5. Google Research Football Environment (5-vs-5 Full Game): We evaluate M3HF in the GRF 5-vs-5 HARD Built-in AI
scenario, a complex multi-agent benchmark widely used in prior work (Kurach et al., 2020; Song et al., 2024). Each team controls 5
players (10 agents total); we always control the yellow-shirt (left) team, which initiates the kickoff. Each episode lasts 3000 steps, with
the second half beginning at step 1501. The simulation is accelerated: 1 in-game minute equals 3 real-world seconds, so a full 90-minute
match takes 4.5 minutes. For human feedback collection, we typically present the first attacking phase (60s rollout), corresponding to
20 in-game minutes. (a) shows a rollout snapshot during the opening phase. (b) depicts the initial player formation around the center
circle. (c) presents the full environment configuration. Observations follow a dictionary structure with keys "obs" (state features) and
"controlled player index" (agent IDs: 0–4 for yellow team, 5–9 for blue). The action space is [Discrete(19)], where
each agent selects one of 19 discrete actions (e.g., pass, sprint, tackle) represented as a one-hot 19D vector.

Performance Metrics Comparison in Google Research Football
IPPO (Baseline)

MAPPO

Good Shot
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0

Gen 0

Gen 1

Gen 2

Score
0.8

8.0

Value

0.6

Value

Value

7.5
7.0

0.4

6.5

0

20

40

60

Iteration

80

0.2

6.0

100

Total Move

0

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4
20

40

60

Iteration

80

100

Total Possession

1000

0

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4
20

40

60

Iteration

80

100

40

60

Iteration

80

100

Win Rate

0.4

700
600

0.2

500

20

20

Value

Value

Value

40

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4

0.6

800

60

0
0.8

900

80

Gen 4

Interception

8.5

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4

Gen 3

400
0

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4
20

40

60

Iteration

80

100

0.0
0

Gen 0 Gen 1 Gen 2 Gen 3 Gen 4
20

40

60

Iteration

80

100

Figure 6. Performance Comparison of M3 HF vs. Baselines in GRF 5-vs-5 Full Game. We evaluate M³HF in the GRF 5v5-HARD
setting, a challenging multi-agent benchmark; Baselines include IPPO, MAPPO, and macro-action-based MACCS; Metrics include
Win Rate, Goal Difference, Total Move (indicating spatial coordination), Good Shot (quality shot attempts), and Interception (defensive
awareness); M³HF outperforms all baselines in most metrics, particularly in Win Rate and coordination-related metrics like Total Move
and Good Shot; this is due to multi-phase human feedback that encourages off-ball movement, proactive attacking, and cooperative
behavior (e.g., “agents should run into space”); these behaviors result in better scoring opportunities and team coordination; Conclusion:
M³HF scales effectively to GRF and enables efficient, high-quality policy learning with minimal human input.

20

