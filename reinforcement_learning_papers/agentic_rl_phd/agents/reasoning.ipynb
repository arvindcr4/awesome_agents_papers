{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reasoning.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/ysymyth/ReAct (ReAct), https://github.com/princeton-nlp/tree-of-thought-llm (ToT), https://github.com/PeterGriffinJin/Search-R1 (Search-R1)\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papers:\n",
    "# 1. \"ReAct: Synergizing Reasoning and Acting\" (Yao et al., 2022)\n",
    "# 2. \"Tree of Thoughts\" (Yao et al., 2023)\n",
    "# 3. \"Search-R1\" (Jin et al., 2025)\n",
    "\n",
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    The classic Thought-Action-Observation loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, tools):\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.history = \"\"\n",
    "\n",
    "    def step(self, question):\n",
    "        prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        History: {self.history}\n",
    "        Format: Thought -> Action -> Observation\n",
    "        Next step:\n",
    "        \"\"\"\n",
    "        response = self.llm.generate(prompt)\n",
    "        \n",
    "        if \"Action:\" in response:\n",
    "            action = self.parse_action(response)\n",
    "            obs = self.execute(action)\n",
    "            self.history += f\"\\n{response}\\nObservation: {obs}\"\n",
    "            return self.step(question) # Recursive loop\n",
    "        else:\n",
    "            return response # Final Answer\n",
    "\n",
    "class TreeOfThoughts:\n",
    "    \"\"\"\n",
    "    Paper: Tree of Thoughts (ToT)\n",
    "    Innovation: Search (BFS/DFS) over the \"thought space\".\n",
    "    \"\"\"\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def solve(self, problem, breadth=3, depth=3):\n",
    "        candidates = [problem] # Initial nodes\n",
    "        \n",
    "        for d in range(depth):\n",
    "            next_candidates = []\n",
    "            for node in candidates:\n",
    "                # 1. Generate: Propose `breadth` next steps\n",
    "                proposals = self.llm.generate(f\"Propose {breadth} next steps for: {node}\")\n",
    "                \n",
    "                # 2. Evaluate: Score each proposal\n",
    "                scored_proposals = []\n",
    "                for p in proposals:\n",
    "                    score = self.llm.evaluate(f\"Rate validness of {p} for {problem}\")\n",
    "                    if score > 0.5: # Pruning\n",
    "                        scored_proposals.append(p)\n",
    "                \n",
    "                next_candidates.extend(scored_proposals)\n",
    "            \n",
    "            candidates = next_candidates\n",
    "        \n",
    "        return candidates[0] if candidates else \"Failed\"\n",
    "\n",
    "class SearchR1:\n",
    "    \"\"\"\n",
    "    Paper: Search-R1 (2025)\n",
    "    Innovation: RL-trained Reasoning that autonomously calls search.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, search_tool):\n",
    "        self.llm = llm\n",
    "        self.search = search_tool\n",
    "\n",
    "    def reason(self, query):\n",
    "        # The model is trained to output special tokens like <search>\n",
    "        response = self.llm.generate(query)\n",
    "        \n",
    "        while \"<search>\" in response:\n",
    "            # 1. Extract Query\n",
    "            q = response.split(\"<search>\")[1].split(\"</search>\")[0]\n",
    "            \n",
    "            # 2. Execute\n",
    "            result = self.search(q)\n",
    "            \n",
    "            # 3. Append & Continue Reasoning\n",
    "            # This is where the RL training happens (PPO on this trajectory)\n",
    "            context = f\"{response}\\n<result>{result}</result>\"\n",
    "            response = self.llm.generate(context)\n",
    "            \n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}