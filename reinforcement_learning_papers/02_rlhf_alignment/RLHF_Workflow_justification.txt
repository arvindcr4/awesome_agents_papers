Paper: RLHF_Workflow.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Published in Transactions on Machine Learning Research (09/2024)  RLHF Workflow: From Reward Modeling to Online RLHF A Comprehensive Practical Alignment Recipe of Iterative Preference Learning Hanze Dong1∗ Han Zhao2  Wei Xiong2∗ Yingbo Zhou1  arXiv:2405.07863v3 [cs.LG] 12 Nov 2024  Caiming Xiong1† 1  Salesforce AI Research  Bo Pang1∗ Nan Jiang2  Haoxiang Wang2∗ Doyen Sahoo1  Tong Zhang2† 2  University of Illinois Urbana-Champaign  Reviewed on OpenReview: https://openreview.net/forum?id=a13aYUU9eU  Abstract We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (...

Specific Evidence for Classification:
- "Published in Transactions on Machine Learning Research (09/2024)  RLHF Workflow: From Reward Modeling to Online RLHF A Comprehensive Practical Alignment Recipe of Iterative Preference Learning Hanze Dong1∗ Han Zhao2  Wei Xiong2∗ Yingbo Zhou1  arXiv:2405.07863v3 [cs.LG] 12 Nov 2024  Caiming Xiong1† 1  Salesforce AI Research  Bo Pang1∗ Nan Jiang2  Haoxiang Wang2∗ Doyen Sahoo1  Tong Zhang2† 2  University of Illinois Urbana-Champaign  Reviewed on OpenReview: https://openreview.net/forum?id=a13aYUU9eU  Abstract We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature."
- "However, existing open-source RLHF projects are still largely confined to the offline learning setting."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
