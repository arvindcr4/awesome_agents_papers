Diverse and Adaptive Behavior Curriculum for Autonomous Driving:
A Student-Teacher Framework with Multi-Agent RL

arXiv:2507.19146v1 [cs.RO] 25 Jul 2025

Ahmed Abouelazm* 1 , Johannes Ratz* 2 , Philip Schoerner1 , and J. Marius ZoÌˆllner1,2
Abstractâ€” Autonomous driving faces challenges in navigating
complex real-world traffic, requiring safe handling of both
common and critical scenarios. Reinforcement learning (RL), a
prominent method in end-to-end driving, enables agents to learn
through trial and error in simulation. However, RL training
often relies on rule-based traffic scenarios, limiting generalization. Additionally, current scenario generation methods focus
heavily on critical scenarios, neglecting a balance with routine
driving behaviors. Curriculum learning, which progressively
trains agents on increasingly complex tasks, is a promising
approach to improving the robustness and coverage of RL
driving policies. However, existing research mainly emphasizes
manually designed curricula, focusing on scenery and actor
placement rather than traffic behavior dynamics. This work
introduces a novel student-teacher framework for automatic
curriculum learning. The teacher, a graph-based multi-agent
RL component, adaptively generates traffic behaviors across
diverse difficulty levels. An adaptive mechanism adjusts task
difficulty based on student performance, ensuring exposure
to behaviors ranging from common to critical. The student,
though exchangeable, is realized as a deep RL agent with partial observability, reflecting real-world perception constraints.
Results demonstrate the teacherâ€™s ability to generate diverse
traffic behaviors. The student, trained with automatic curricula,
outperformed agents trained on rule-based traffic, achieving
higher rewards and exhibiting balanced, assertive driving.

I. I NTRODUCTION
Car accidents cause 1.19 million deaths annually, with
road injuries the leading cause of death for ages 5â€“29 [1],
prompting research into safe and reliable autonomous vehicles. Traditional autonomous systems employ a modular
structure with distinct modules for tasks like perception,
localization, and planning. However, this approach often
leads to error propagation and requires significant engineering effort. Additionally, as each module is optimized
independently, the overall system may lack alignment toward
a unified goal, such as ensuring safe and efficient driving [2].
End-to-end (E2E) driving systems aim to address these
challenges by enabling joint learning across tasks, often
mapping raw sensor data to control actions or intermediate
representations. These systems enable joint optimization of
perception, prediction, and planning, making them wellsuited for complex scenarios and reacting to hazards [3].
Reinforcement learning (RL) has been explored as an E2E
approach [4], enabling systems to learn optimal driving
policies through trial and error. Agents receive rewards based
on task performance, such as maintaining lane position or
* These authors contributed equally to this work
1 Authors are with the FZI Research Center for Information Technology,
Germany abouelazm@fzi.de
2 Authors are with the Karlsruhe Institute of Technology, Germany

avoiding collisions, allowing continuous refinement of their
driving policy. RL agents are typically trained in simulations,
enabling safe exploration without real-world risks [3].
Research Gap. RL agents learn through interaction with
the environment, enabling them to handle traffic participant behaviors similar to those encountered during training.
Simulation environments must encompass a wide range of
diverse, common, and safety-critical behaviors to achieve
robustness and generalization. However, most simulations
rely on fixed non-player characters (NPCs) behaviors with
predefined rules, maintaining constant speed or distance [5].
This limits the agentâ€™s ability to generalize to unseen situations [6]. Similarly, prerecorded human driving behaviors [7]
offer realism but are non-adaptive and rarely include safetycritical situations, contributing to the long tail problem [8],
where critical events are underrepresented.
Recent work targets safety-critical behaviors by generating scenarios that provoke the self-driving vehicle (SDV)
collisions or off-road events [8], [9]. While effective for robustness against safety-critical situations, they neglect more
frequent traffic patterns, potentially limiting generalization
to everyday driving contexts. To overcome this limitation,
more generalizable approaches such as multi-agent reinforcement learning have been explored. Multi-agent RL (MARL)
methods [10] allow NPCs to make independent or cooperative decisions but primarily focus on traffic coordination
rather than training SDV agents. To develop driving skills
more comprehensively, some works adopt curriculum learning [11], a technique inspired by education that trains agents
on simpler tasks before progressively advancing to more
complex ones. This approach improves learning efficiency
and robustness. However, in autonomous driving, current
research relies on manually crafted task sequences [12],
which fail to encompass a diverse range of NPC behaviors.
Contribution. This work introduces a student-teacher framework for adaptively generating a behavior curriculum for
surrounding NPCs that dynamically adapts to the studentâ€™s
performance. By integrating MARL and curriculum learning,
the framework enhances the generalization and robustness of
the student, enabling it to handle a diverse spectrum of traffic
behaviors. The key contributions of this work are:
â€¢ Teacher Design: A novel MARL-based teacher capable
of generating traffic behaviors with varying difficulty
levels with a graph-based network and a novel reward.
â€¢ Automatic Curriculum Algorithm: A method for
automatically orchestrating the concurrent training of
student and teacher components, creating an adaptive
behavior curriculum.

Student Observation

Student Action

Student

Environment

Student Reward

NPCs Reward

Difficulty Level Î»

Teacher Reward

Teacher

Teacher Observation

NPCs Action

Fig. 1: Overview of the proposed behavior curriculum framework.
The student (blue) and teacher (red) interact concurrently in the
driving environment. The student learns a driving policy from
sensor data, guided by a standard reward. The teacher dynamically
orchestrates NPC behavior based on a fully observable state representation, the studentâ€™s performance, and an auxiliary input Î».

II. RELATED WORK
This section reviews prior research on traffic simulation, with an emphasis on behavior generation techniques.
Additionally, curriculum learning is discussed, highlighting
advancements in automatic curriculum generation.
A. Scenario Generation
Traditional Simulators like CARLA [5] generate rulebased NPC behavior, ensuring compliance with traffic laws,
lane-keeping, and collision avoidance. However, they lack
flexibility and require manual adjustments to parameters such
as speed limits. In addition, they fail to simulate unpredictable behavior or dynamically adapt to agentsâ€™ actions.
TrafficGen [13] addresses these limitations with a generative model trained on real-world scenarios to produce
realistic vehicle placements and trajectories. It generates scenarios autoregressively, predicting long-term motion. However, it relies heavily on large datasets, struggles with out-ofdistribution road topologies, and lacks adaptability to agent
actions. TrafficSim [14] enhances the learning objective with
a common-sense loss and predicts multi-modal trajectories to
capture the uncertainty in agent interactions. However, the
inclusion of a common-sense loss requires a differentiable
simulator, making the approach computationally intensive.
CoPO [10] eliminates the need for differentiable simulators by introducing a MARL algorithm that coordinates
traffic flow, allowing NPCs to exhibit cooperative and competitive behaviors. While the aforementioned approaches
effectively generate a wide range of traffic behaviors, they are
not tailored to capture safety-critical situations. In contrast,
AdvSim [6] generates safety-critical scenarios by modifying
real-world traffic through physically plausible perturbations

of agentsâ€™ trajectories. It employs black-box optimization
(BBO) [15] to identify failure-inducing perturbations. However, BBO is computationally intensive in high-dimensional
spaces [8]. KING [8] employs a gradient-based procedure
to optimize trajectory perturbations iteratively, eliminating
the need for BBO. However, this approach requires differentiable approximations of simulator aspects, such as vehicle
dynamics, and loss components, such as collision.
Liu et al. [9] use RL to edit scenarios by adding agents or
perturbing their trajectories. However, pre-setting trajectories
before scenarios unfold limits their ability to capture the
impact of agentsâ€™ behaviors and interactions at specific time
steps on the SDV. Conversely, FailMaker-AdvRL [16] leverages MARL to coordinate adversarial agents, introducing
techniques like Contributors Identification to rank agentsâ€™
contributions to SDV failures and Adversarial Reward Allocation to assign reward fractions based on their impact on
the failure. These approaches often rely on scene-specific
perturbations and fail to learn a generalizable policy.
B. Curriculum Learning
Instead of explicitly training SDV agents on specific
traffic behaviors, some methods adopt curriculum learning
to generate scenarios of increasing difficulty. This approach
aims to shorten training time and enhance robustness across
a range of tasks. Approaches proposed in [12], [17] manually
design multi-stage curricula by increasing the number of
agents, but overlook variations in NPCsâ€™ behavior. Khaitan
and Dolan [18] introduce a state dropout-based curriculum,
progressively removing privileged information about surrounding vehiclesâ€™ future states. However, these methods rely
heavily on manual effort and expert knowledge.
To address the challenges of manual curriculum design,
automated curriculum learning approaches have been introduced. A framework for Unsupervised Environment Design
(UED) [19] enables automatic modification of environments
(or tasks) to optimize specific objectives. PLR [20] extends
UED by combining curriculum learning with evolutionary
methods to generate increasingly complex tasks, using regretâ€”which measures the gap between the agentâ€™s performance and an optimal policyâ€”as a fitness objective. More
advanced methods like ACCEL [21] further improve sample
efficiency and refine promising tasks.
ARLPCG [22] employs a dynamic task generation framework with a solver-generator setup (Solver and Generator).
Unlike ACCEL, it adopts an adversarial approach where the
two RL agents operate simultaneously in the same environment. The Generator is conditioned on an auxiliary input to
control task difficulty, enhancing the Solverâ€™s generalization
through exposure to diverse scenarios.
A key challenge in automatic curriculum learning, and
a motivation for this paper, is the tendency to treat tasks
or environments as a whole, overlooking agent behavior
at individual simulation steps. Existing methods focus on
layout and actor placement, neglect NPC reactive behavior,
and are limited to benchmark games with no clear path to
autonomous driving applications.

III. M ETHODOLOGY
To address the limitations of traffic generation approaches
outlined in Section II, we present an automated curriculum
framework for behavior generation, as illustrated in Fig. 1.
The framework adopts a student-teacher paradigm, where
the student represents the ego vehicle, learning to navigate
safely toward a destination. On the other hand, the teacherâ€™s
primary role is to dynamically adapt NPC behaviors based
on a desired difficulty level, referred to as auxiliary input
Î» [22] and feedback on the studentâ€™s performance, based on
its driving reward. These components are combined into a
teacher reward signal that guides the teacher in generating
traffic behaviors suited to the current difficulty. This creates a symbiotic relationship between teacher and student,
where behaviors are continuously adapted to the studentâ€™s
capabilities, fostering structured and effective learning. The
following subsections provide a detailed discussion of the
framework components and the underlying algorithm.
A. Teacher
The teacherâ€™s role is to coordinate NPC behavior to
generate scenarios matching a desired difficulty level Î».
Unlike critical scenario generation frameworks, our approach
aims to train the teacher to learn a generalizable policy
applicable across diverse scenarios rather than being overfit
to an individual one. Imitation learning is inadequate for
our framework as it relies on expert trajectories, which are
unavailable for the diverse range of behaviors and difficulties
we aim to capture in our curriculum. Instead, we leverage a
MARL-based teacher, which is effective in training policies
for multi-agent systems [23], with each NPC treated as an
individual agent. In the following, we discuss key aspects
of the teacher, including its observation space, the reward
function that drives its training, and the algorithm employed.
1) Observation Space: To effectively coordinate traffic,
the teacherâ€™s observation space is designed to be fully
observable. This allows the teacher to access the complete
state of the environment, providing the necessary context to
manage complex scenarios. Unlike SDVs, which are limited
by sensor-based observations, the teacher leverages its full
observability to generate diverse and adaptive traffic behaviors in simulation. This paper introduces an observation space
designed to encompass the historical motion of all NPCs and
the student vehicle, along with a vectorized representation
of road topology. This design draws inspiration from wellestablished input spaces used in trajectory prediction [24].
The motion history of each agent is represented by a temporal sequence of poses. Each pose includes the agentâ€™s
position, heading, 2D velocity, and acceleration, capturing
all relevant information for modeling agent motion. Additionally, each agent is assigned a road option, which serves
as a high-level navigational command (e.g., straight, turn left,
or right) to guide NPCs toward their destinations [25].
The road topology is modeled as a vectorized lane
graph [24], where nodes represent lane segments and edges
capture spatial and topological relationships. Node attributes
include position, lane width, and curvature, as well as its

typeâ€”e.g., crosswalk or intersection. Nodes are connected
through various relations, such as predecessors, successors,
left neighbors, and right neighbors. To enrich the graphâ€™s
relational context, the predecessor and successor relationships are dilated with a dilation power D. For example, when
D = 1, the graph includes not only direct successors but also
the successor of a successor, providing additional contextual
information. Finally, the auxiliary input Î» is incorporated
into the observation space to represent the desired traffic
difficulty, ranging from easy (Î» = 1) to most difficult
(Î» = âˆ’1). It is sampled from a discrete distribution, i.e.,
a predefined set of values within [âˆ’1, 1]. Although Î» can be
sampled from a continuous distribution, this would lead to
longer training times and make it more challenging to design
a curriculum with consistently difficult levels [22].
2) Network Architecture: The teacher employs a custom network architecture, as shown in Fig. 2, inspired
by GoRela [26] to process high-dimensional observations
efficiently. Unlike previous MARL behavior generation approaches that handle state representations for each NPC
independently [10], our proposed network adopts a graphbased architecture that enables joint processing of a shared
state representation of agents and the map lane graph. By
leveraging a shared optimization process, the network effectively captures the relationships among a variable number
of NPCs while preserving the independence of agents in
decision-making, thereby enhancing behavioral diversity.
First, the map and agent representations are encoded using
pair-wise relative positional encoding, a viewpoint-invariant
encoding approach [26]. Viewpoint invariance enhances the
networkâ€™s ability to interpret the scene without being affected
by rotation or translation of the scene viewpoint, thereby
improving the teacherâ€™s robustness and eliminating the need
for training data augmentation.
The agent-history encoder extracts feature embeddings
from each agentâ€™s motion history using a 1D convolution
layer with residual connections, effectively capturing both
short- and long-term temporal dependencies. These extracted
features are subsequently aggregated by a Gated Recurrent
Unit (GRU) [27], where the final hidden state of the GRU
serves as a compact embedding of the agentâ€™s motion history.
To incorporate the agentâ€™s intended motion direction, we
encode its road option using a multi-layer perceptron (MLP)
and concatenate the resulting embedding with the history
embedding. The map topology, represented as a lane graph,
is processed through a lane graph encoder employing a stack
of heterogeneous message passing (HMP) [26] layers. HMP
is particularly advantageous as it enables each lane segmentâ€™s
embedding to be updated based on information from its
neighboring segments connected through different types of
spatial relationships. This allows the network to effectively
capture both topological connectivity and the diverse geometric relationships among lane segments. Moreover, since
the map topology remains fixed throughout a single driving
scenario, the embeddings of map nodes are computed once
per scenario and cached in memory, enhancing the networkâ€™s
computational efficiency.

Agents' History

Agent History
Encoder
Heterogeneous
Scene Encoder

Lane Graph

Auxiliary Input

Lane Graph
Encoder

Auxiliary Input
Encoder

Student
Embedding

Auxiliary Input
Fusion

NPC
Embeddings

Actor-Critic
Head

ðœ‹ , ð‘‰ (NPC 1)
â€¦
ðœ‹ , ð‘‰ (NPC N)

Legend
Auxiliary Input
Embedding

Fig. 2: The teacher network architecture first encodes the agentsâ€™ history and map lane graph separately, followed by hierarchical interaction
fusion, ensuring each NPC embedding incorporates information about the map topology, road layout, surrounding NPCs, and the student.
The NPC embeddings are concatenated and combined with a linear projection of the auxiliary input before being processed by the actorcritic MLP, which outputs the policies and value functions for each NPC.

Using the heterogeneous scene encoder, the network
captures interactions between agentsâ€™ motion histories and
map nodes embeddings. Unlike the heterogeneous scene
encoder in GoRela, which processes all interactions in a
single forward pass, our approach decomposes the interaction
encoding into four sequential HMP layers, each dedicated
to a specific type of relation: agent-to-map relations, which
associate each agent with its neighboring lane segments;
map-to-map relations, which update map node embeddings
based on neighboring nodes; map-to-agent relations, which
enable agents to receive spatial context from nearby map
nodes; and agent-to-agent relations, which allow agents in
proximity to exchange trajectory information and adjust their
movements accordingly.
The final agent embedding consists of two components:
the student embedding, representing the ego vehicle, and
the NPCs embedding, which encapsulates the surrounding
NPCs whose motion is coordinated by the teacher. This
sequential design ensures that the NPCsâ€™ embedding incorporates motion history, spatial context, and interactionbased information while remaining aware of the studentâ€™s
movement and position within the environment, resulting in
a holistic representation of the driving scenario.
The auxiliary input is incorporated into the network
through a linear layer and fused with the NPCsâ€™ embedding
via concatenation and a fusion MLP, ensuring that the
specified difficulty level is effectively embedded in NPC
behavior. The resulting representation is then concatenated
again with the auxiliary input embedding, reinforcing the
networkâ€™s ability to account for the desired difficulty. Finally,
an actor-critic MLP generates policies and value functions
for each NPC based on its respective embedding. Thus,
the proposed architecture enables the teacher to efficiently
coordinate the behavior of a varying number of NPCs across
different road topologies and difficulty levels.
3) Reward Function: In MARL, reward design plays a
crucial role in shaping agentsâ€™ interactions [28], ensuring that
cooperation, competition, and other dynamics align with the
frameworkâ€™s objectives. We propose a teacher reward that
balances two competing objectives for each NPC: intrinsic

and extrinsic rewards, as defined in Eq. 1. The intrinsic
reward, formulated in Eq. 2, encourages realistic traffic
behavior by promoting desirable driving traits such as goaldirected progress, collision avoidance, lane keeping, and
comfort [29]. Meanwhile, the extrinsic reward, defined in
Eq. 3, provides feedback based on the studentâ€™s driving
reward, enabling the teacher to adapt the NPC behavior to
the studentâ€™s learning progress.
RNPC = Rintrinsic
+ Rextrinsic
NPC
NPC
Rintrinsic
= (1 âˆ’ K(d)) Â· max(Îµ, 1 âˆ’ |Î»|) Â· Rdriving
NPC
NPC
(
Î»
, if |Î»| > Îµ
Rextrinsic
= K(d) Â· Rdriving
NPC
student Â·
sgn(Î») Â· Îµ , else

(1)
(2)
(3)

The auxiliary input, Î» âˆˆ [âˆ’1, 1], serves as the primary
balancing parameter, incentivizing NPCs to generate behaviors that align with the desired difficulty level. The intrinsic
reward is weighted by 1 âˆ’ |Î»|, while the extrinsic reward
is weighted by Î», with both weighting terms clipped at
Îµ âˆˆ (0, 0.1] to prevent either reward objective from being
entirely disregarded at any curriculum step. This weighting
scheme enables the teacher to regulate NPC behavior based
on the studentâ€™s driving reward. At the easiest difficulty
level (Î» = 1), NPCs are highly altruistic, prioritizing the
studentâ€™s success and actively assisting in their progress. As
Î» decreases, NPCs become progressively more self-centered,
prioritizing their intrinsic reward while reducing cooperation
with the student, thereby increasing the scenario difficulty.
When Î» = 0, NPCs exhibit predominantly egoistic behavior,
optimizing their own objectives with minimal regard for the
studentâ€™s performance. As the difficulty increases (Î» < 0),
NPCs adopt an increasingly adversarial role, coordinating to
hinder the studentâ€™s progress, reaching its peak at Î» = âˆ’1.
Additionally, we introduce a distance-based weighting
mechanism to balance intrinsic and extrinsic reward objectives, employing a Radial Basis Function (RBF) kernel K(d),
as defined in Eq. 4. Here, d denotes the distance between
the NPC and the student, while Ïƒ is a hyperparameter that
controls the scale of the kernel, determining how rapidly
the influence of NPCs diminishes as their distance from the

student increases. The intrinsic objective weight 1âˆ’K(d) increases with distance, whereas the extrinsic objective weight
K(d) correspondingly decreases. This weighting strategy
acts as a contribution assignment mechanism based on the
premise that NPCs farther from the student exert a weaker
influence on the studentâ€™s behavior. Consequently, distant
NPCs receive a lower contribution from the extrinsic reward
and are encouraged to prioritize their own driving behavior,
while closer NPCs rely more on the extrinsic objective,
adapting their behavior to align with the studentâ€™s actions.


d2
(4)
K(d) = exp âˆ’
2Â·Ïƒ2
4) Learning Algorithm: At first glance, Multi-Agent
Proximal Policy Optimization (MAPPO) [30] appears to be
a suitable algorithm for training the teacher, as it extends
PPO [31] to a collaborative multi-agent setting. However,
its reliance on a global shared reward, where all agents
contribute collectively, presents a key limitation. While this
approach can accelerate convergence toward a global objective, it complicates the training of individual NPC behaviors
and makes it difficult to isolate each agentâ€™s contribution to
the overall reward. Consequently, MAPPO is not well-suited
to our proposed methodology, where NPCs must balance
independent decision-making, optimizing their intrinsic goal
of reaching a destination, with coordinated behavior to
generate traffic dynamics at a specified difficulty level while
preserving distinct contribution assignments.
Thus, we adopt Independent PPO (IPPO) [32] as the
training algorithm. Unlike standard IPPO, where each agent
relies solely on its local observations, we propose a novel
shared global observation processed through a graph-based
architecture to compute embeddings for each agent, as discussed in section III-A. This approach ensures that agents
are well-informed about the scene context and interactions
with other agents in a differentiable manner, enabling the
teacher to develop a more comprehensive understanding of
the environment. Each NPC independently learns its policy
and value function using individual rewards, as defined in
Eq. 1. Additionally, we incorporate policy and value function
parameter sharing, allowing a single network to efficiently
compute outputs for all agents within a single forward pass.
B. Automatic Curriculum Algorithm
We propose a novel automatic curriculum algorithm to
jointly train the teacher and student RL components in a
shared environment (Fig. 3). The training process adopts an
alternating Markov game [33], where only one component
is actively trained at a time while the other is in inference
mode. This approach addresses instability issues arising from
concurrent updates in a non-stationary environment, where
dynamics change continuously during learning [22].
The teacher training phase updates the NPC behavior
generation policy over Nteacher iterations, adapting to the
current student performance. During this phase, the auxiliary
input Î» is uniformly sampled from a discrete set Î»set ,
allowing the teacher to learn behavior patterns and traffic

Teacher Training for ð‘ð‘¡ð‘’ð‘Žð‘â„Žð‘’ð‘Ÿ iterations
Sample

Update

Teacher

Inference

Recalibration: Evaluate Student performance for ð‘
Inference

Teacher

Student

iterations

Inference

Teacher

episodes

Inference

Environment

Student Training for ð‘

Student

Environment

Sample

Environment

Update

Student

Fig. 3: The proposed algorithm consists of three sequential steps.
Initially, the teacher undergoes training for Nteacher iterations to
refine its NPC behavior policy. Followed by a recalibration phase
to determine the initial difficulty level of the behavior curriculum.
Finally, the student driving policy is trained for Nstudent iterations
under a curriculum with progressively increasing difficulty.

coordination across the entire curriculum range. This process
aligns the generated behaviors with the desired Î», guided by
the reward function in Eq. 1, enabling the teacher to produce
traffic at varying difficulty levels.
The student training phase updates the student driving
policy over Nstudent iterations by navigating driving scenarios
with NPC behaviors inferred by the teacher. To ensure
sufficient exposure to each difficulty level, the auxiliary input
Î» remains fixed throughout each iteration. The curriculum
dynamically adjusts Î» based on the studentâ€™s success rate,
defined as the proportion of episodes in which the student
successfully reaches its goal in the previous iteration. When
the studentâ€™s success rate exceeds a predefined threshold
Tsuccess , the student is considered proficient at the current difficulty level, prompting an increase in challenge. Conversely,
a success rate below Tfail indicates that the difficulty is too
high, necessitating further training on easier levels before
retrying the more challenging ones. If the success rate falls
between these thresholds, the student training continues at
the current level to reinforce learning and master this level.
To mitigate catastrophic forgetting [34] of previously
mastered levels, the student has a probability Pold of sampling easier levels during training. The proposed self-paced
mechanism ensures that the curriculum adapts to the studentâ€™s performance, balancing difficulty progression while
incorporating past experiences to prevent regression.
An optional recalibration phase is incorporated after each
teacher training phase to adjust Î», accounting for changes
in traffic behavior induced by teacher policy updates. This
process evaluates the studentâ€™s performance across all difficulty levels using the updated teacher policy and determines
the initial Î» for the student training phase. By adapting
to the studentâ€™s performance, this approach ensures training
stability in non-stationary environments while fostering adaptive learning. Its structured design generates diverse traffic
behaviors, gradually increasing difficulty.

IV. EXPERIMENTAL S ETUP
This section outlines the experimental setup, including
the selected student model, the traffic scenarios, and the
evaluation metrics employed to analyze the performance of
the trained RL components.
A. Student Description
Our curriculum learning algorithm imposes no constraints
on the student, enabling interchangeable realization and a
flexible MDP setup with various observation and action
spaces, network architectures, and training algorithms. In
this work, the student employs a multi-modal observation
comprising a frontal RGB camera with a 256Ã—256 resolution
and a LiDAR point cloud discretized into a 256 Ã— 256
grid map with two height bins. Additionally, vehicle state
measurements, including longitudinal and angular velocities
as well as longitudinal and lateral accelerations, are incorporated into the observation space. This setup reflects the
partial observability nature of real-world SDVs. To encode
the input observations into a latent representation, we adopt
TransFuser [25], a transformer-based architecture designed
to effectively fuse RGB and LiDAR features through crossattention at multiple spatial scales, thereby enhancing spatial awareness. The encoded observations are subsequently
utilized for estimating both the student policy and value
function. The student policy is trained using PPO [31],
guided by a standard driving reward Rdriving
student .
B. Traffic Scenarios
For the evaluation of the proposed methodology, we focus
on unsignalized urban intersections, which are prevalent in
urban environments and often involve safety-critical situations requiring implicit coordination between vehicles. The
traffic scenarios are simulated using the CARLA simulator [5]. During training, NPCs are randomly positioned
across three T-intersections and four four-way intersections,
with assigned destinations that they navigate through the
intersection. NPCs are terminated upon reaching their destination, colliding, or veering off-road. Each episode concludes when the student reaches a terminal state or exceeds
a predefined time limit. To assess the capabilities of the
proposed algorithm, performance is evaluated on a holdout set consisting of one T-intersection and two four-way
intersections that were not encountered during training.
C. Baselines and Evaluation Metrics
To evaluate our approach, we conducted two training experiments: one without the recalibration step (Student CL ) and
one incorporating it (Student+CL ). Each experiment utilized
eight initial NPC agents to generate highly interactive scenarios and an auxiliary input set of nine equally spaced steps
Î»set = {âˆ’1, âˆ’0.75, âˆ’0.5, . . . , 1}, ensuring steady paced
curriculum progression. Additional algorithm parameters are
detailed in Table I. We compare students trained using the
proposed automatic curriculum with a student trained in
a baseline traffic environment that reflects rule-based NPC
behavior via CARLAâ€™s traffic manager (Student Rule ).

TABLE I: Parameters of the proposed automatic behavior curriculum learning algorithm.
Parameter

Value

Parameter

Value

Parameter

Value

D
Îµ
Ïƒ

2
0.1
5.0

Nteacher (Iterations)
Nstudent (Iterations)
Nrecalibrate (Episodes)

10
10
100

Tsuccess
Tfail
Pold

0.75
0.25
0.3

To maintain consistency in evaluation, all students are
trained for the same number of iterations with identical
architecture and PPO parameters. The evaluation metrics
include cumulative driving rewards per episode (R) and a
range of standard driving performance metrics. These metrics
cover terminal statistics such as the percentage of successful
episodes (SR), off-road deviations (OR), collisions (CR),
timeouts (T R), and behavioral metrics like route progress
(RP ) and average driving velocity (v) in m/s.
V. E VALUATION
This section presents the results of our proposed framework. We first examine the teacherâ€™s effectiveness in generating traffic behaviors of different difficulty levels. Then,
we compare the performance of students trained with our
framework to that of baseline students.
A. Traffic Behavior Generation
First, we evaluate the teacherâ€™s capability to generate
varying levels of traffic difficulty by analyzing the studentâ€™s
performance across five auxiliary inputs, comparing students
trained with and without the recalibration step. The results
presented in Table II confirm that the teacher effectively
establishes a clear relationship between the auxiliary input Î»
and the complexity of the generated traffic. As Î» decreases
from 1 (easiest) to -1 (hardest), the studentâ€™s success rate
(SR student ) and driving reward (R student ) decreases, while
the average NPC velocity (v NPCs ) increases. This trend
indicates that the teacher successfully generates progressively
more challenging traffic conditions. The recalibration step
(Student+CL ) enhances the smoothness and distinction of
difficulty progression, ensuring that each curriculum stage
accurately reflects the intended complexity.
Figure 4 illustrates qualitative scenarios at three different
auxiliary input values. At Î» = 1, traffic is sparse, with
only the NPC directly in front of the student moving to
clear the path, while others remain stationary, minimizing
interactions and difficulty. At Î» = 0, half of the NPCs
enter the intersection while the rest wait, creating a balanced
traffic flow and a moderate challenge for the student. At
Î» = âˆ’1, all NPCs move simultaneously into the intersection,
generating dense and dynamic traffic that demands complex
maneuvering. These results highlight the teacherâ€™s ability to
systematically generate increasingly complex scenarios.
B. Student Generalizability
We compare students trained using the proposed curriculum learning algorithm against baseline students trained
in rule-based traffic. The student performance is assessed
in scenarios where NPC behaviors are governed by the
rule-based CARLA traffic manager, as well as across three

TABLE II: Evaluation of students performance across different
difficulty levels Î» generated by the teacher.
Î»

Metric

Student CL

Student+
CL

1

SR student
R student
v NPCs

0.59
0.23 Â± 1.06
0.86 Â± 0.37

0.65
0.43 Â± 1.40
0.84 Â± 0.41

0.5

SR student
R student
v NPCs

0.53
0.23 Â± 1.40
0.86 Â± 0.32

0.57
0.23 Â± 1.32
0.81 Â± 0.39

0

SR student
R student
v NPCs

0.40
âˆ’0.43 Â± 2.02
1.96 Â± 0.76

0.45
âˆ’1.13 Â± 2.80
1.71 Â± 0.33

SR student
-0.5

R student
v NPCs

0.44
âˆ’0.72 Â± 2.21
2.50 Â± 0.71

0.39
âˆ’2.46 Â± 3.67
2.39 Â± 0.37

-1

SR student
R student
v NPCs

0.45
âˆ’1.02 Â± 2.54
2.74 Â± 0.69

0.40
âˆ’2.41 Â± 3.64
2.39 Â± 0.38

difficulty levels generated by a learned teacher within the
proposed framework. The results, demonstrated in Table III,
indicate that students trained with an automatic curriculum
generalize more effectively, achieving safer navigation in
rule-based traffic while exhibiting higher route progress
and average velocity. Additionally, students trained with
curriculum learning outperform the baseline across all three
teacher-generated difficulty levels, demonstrating significant
improvements in route progress, average driving velocity, and
overall driving rewards.
Further qualitative analysis of the learned student policies
exposes a critical flaw in the baseline student. While it
occasionally achieves higher success rates, this is a direct
consequence of an exploitative policy that merely waits for
all NPCs to clear the intersection before proceeding, as
investigated in Fig 5. In contrast to this unrealistic behavior,
students trained with curriculum learning exhibit a more
adaptive driving behavior. They proactively engage with
traffic, demonstrating realistic and intuitive decision-making
to safely navigate intersections. This enhanced adaptability
is further validated by their consistently higher average
velocity and route progress, underscoring the superiority of
curriculum learning in fostering interactive driving policies.
VI. C ONCLUSION
This work proposes a student-teacher framework to improve SDV robustness in dynamically generated traffic using
MARL automatic curriculum learning. The teacher adapts
NPCsâ€™ behavior, adjusting difficulty levels based on student
performance. Evaluations show that the teacher successfully
generates diverse traffic behaviors across difficulty levels,
enabling students trained with the dynamic curriculum to outperform those trained on rule-based traffic. Future research
will refine the teacherâ€™s reward to enhance traffic coordination by incorporating the surrounding NPCsâ€™ rewards and
expand the curriculum to include cyclists and pedestrians for
broader agent interaction.

(a) NPCs behavior at the easiest difficulty Î» = 1.

(b) NPCs behavior at the medium difficulty Î» = 0.

(c) NPCs behavior at the hardest difficulty Î» = âˆ’1.

Fig. 4: Exemplary scenarios of the teacherâ€™s behavior generation
across three difficulty levels, with the NPCs highlighted in red
and the student highlighted in blue. Screenshots are arranged in a
temporal sequence from left to right. Moving vehicles are marked
with a bounding box. The black and white flag indicates the goal
position of the student.

ACKNOWLEDGMENT
The research leading to these results is funded by the
German Federal Ministry for Economic Affairs and Energy
within the project â€œSafe AI Engineering â€“ Sicherheitsargumentation befaÌˆhigendes AI Engineering uÌˆber den gesamten
Lebenszyklus einer KI-Funktionâ€. The authors would like to
thank the consortium for the successful cooperation.
C OPYRIGHT N OTICE
Â© 2025 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in
any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of
this work in other works.
R EFERENCES
[1] World Health Organization, â€œRoad traffic injuries,â€ https://www.who.
int/news-room/fact-sheets/detail/road-traffic-injuries.
[2] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du,
T. Lin, W. Wang et al., â€œPlanning-oriented autonomous driving,â€ in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023.
[3] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, â€œEnd-to-end
autonomous driving: Challenges and frontiers,â€ IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2024.
[4] M. Al-Sharman, L. Edes, B. Sun, V. Jayakumar, M. A. Daoud,
D. Rayside, and W. Melek, â€œAutonomous driving at unsignalized intersections: A review of decision-making challenges and reinforcement
learning-based solutions,â€ arXiv preprint arXiv:2409.13144, 2024.
[5] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, â€œCarla:
An open urban driving simulator,â€ in Conference on robot learning,
2017.
[6] J. Wang, A. Pun, J. Tu, S. Manivasagam, A. Sadat, S. Casas, M. Ren,
and R. Urtasun, â€œAdvsim: Generating safety-critical scenarios for selfdriving vehicles,â€ in 2021 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2021.

TABLE III: Evaluation of student performance against baseline student trained in rule-based traffic on rule-based traffic and teachergenerated traffic at three different difficulty levels.
Traffic Generator

Student

SR

CR

OR

TR

RP

v

R

Carla Traffic Manager

CL
Rule

0.72
0.76

0.21
0.03

0.07
0.03

0.00
0.18

0.72 Â± 0.32
0.71 Â± 0.37

2.21 Â± 0.80
1.35 Â± 0.28

0.16 Â± 2.72
0.59 Â± 0.71

Teacher (Î» = 1)

CL
Rule

0.65
0.43

0.31
0.38

0.00
0.01

0.04
0.18

0.66 Â± 0.36
0.47 Â± 0.43

1.81 Â± 1.14
0.81 Â± 0.92

0.43 Â± 1.40
âˆ’0.15 Â± 1.24

Teacher (Î» = 0)

CL
Rule

0.45
0.47

0.53
0.53

0.02
0.00

0.00
0.00

0.52 Â± 0.40
0.45 Â± 0.45

1.73 Â± 1.11
0.86 Â± 0.92

âˆ’1.13 Â± 2.80
âˆ’1.72 Â± 2.81

Teacher (Î» = âˆ’1)

CL
Rule

0.40
0.41

0.58
0.59

0.02
0.00

0.00
0.00

0.45 Â± 0.41
0.40 Â± 0.45

1.63 Â± 1.06
0.82 Â± 0.94

âˆ’2.41 Â± 3.64
âˆ’2.62 Â± 3.17

Rule

Episode Start

Episode End

Rule

Episode Start

Î» = âˆ’1

Î»=0

StudentCL

Episode End

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

Velocity (m/s)

Velocity (m/s)

Velocity (m/s)

StudentCL

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

Velocity (m/s)

Î»=1

Carla Traffic Manager
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

StudentCL
Rule

Episode Start

Episode End

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

StudentCL
Rule

Episode Start

Episode End

Episode Progress

Episode Progress

Episode Progress

Episode Progress

(a) Velocity profiles at CARLA traffic manager (rule-based).

(b) Velocity profiles at the teacherâ€™s
easiest difficulty Î» = 1.

(c) Velocity profiles at the teacherâ€™s
medium difficulty Î» = 0.

(d) Velocity profiles at the teacherâ€™s
hardest difficulty Î» = âˆ’1.

Fig. 5: Comparison of student velocity profiles across different traffic conditions. Each plot shows two students: one trained with curriculum
learning (CL) and one with rule-based traffic. The rule-based student adopts an exploitative policy, often waiting passively for NPCs to
clear the intersection. Conversely, the CL student navigates traffic more assertively and maintains smoother, balanced velocity.
[7] H. Caesar, J. Kabzan, K. S. Tan, W. K. Fong, E. Wolff, A. Lang,
L. Fletcher, O. Beijbom, and S. Omari, â€œNuplan: A closed-loop mlbased planning benchmark for autonomous vehicles,â€ 2022.
[8] N. Hanselmann, K. Renz, K. Chitta, A. Bhattacharyya, and A. Geiger,
â€œKing: Generating safety-critical driving scenarios for robust imitation
via kinematics gradients,â€ in European Conference on Computer
Vision, 2022.
[9] H. Liu, L. Zhang, S. K. S. Hari, and J. Zhao, â€œSafety-critical scenario
generation via reinforcement learning based editing,â€ 2024 IEEE
International Conference on Robotics and Automation (ICRA), 2023.
[10] Z. Peng, Q. Li, K. M. Hui, C. Liu, and B. Zhou, â€œLearning to simulate
self-driven particles system with coordinated policy optimization,â€
Advances in Neural Information Processing Systems, 2021.
[11] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, â€œCurriculum
learning,â€ in Proceedings of the 26th annual international conference
on machine learning, 2009.
[12] L. Anzalone, P. Barra, S. Barra, A. Castiglione, and M. Nappi,
â€œAn end-to-end curriculum learning approach for autonomous driving
scenarios,â€ IEEE Transactions on Intelligent Transportation Systems,
2022.
[13] L. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou, â€œTrafficgen: Learning
to generate diverse and realistic traffic scenarios,â€ in 2023 IEEE
International Conference on Robotics and Automation (ICRA), 2023.
[14] S. Suo, S. Regalado, S. Casas, and R. Urtasun, â€œTrafficsim: Learning
to simulate realistic multi-agent behaviors,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021.
[15] M. Alzantot, Y. Sharma, S. Chakraborty, H. Zhang, C.-J. Hsieh,
and M. B. Srivastava, â€œGenattack: Practical black-box attacks with
gradient-free optimization,â€ in Proceedings of the genetic and evolutionary computation conference, 2019.
[16] A. Wachi, â€œFailure-scenario maker for rule-based agent using multiagent adversarial reinforcement learning and its application to autonomous driving,â€ in Twenty-Eighth International Joint Conference
on Artificial Intelligence IJCAI-19, 2019.
[17] Z. Qiao, K. Muelling, J. M. Dolan, P. Palanisamy, and P. Mudalige,
â€œAutomatically generated curriculum based reinforcement learning for
autonomous vehicles in urban environment,â€ in 2018 IEEE Intelligent
Vehicles Symposium (IV), 2018.
[18] S. Khaitan and J. M. Dolan, â€œState dropout-based curriculum reinforcement learning for self-driving at unsignalized intersections,â€ in
2022 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), 2022.
[19] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch,

and S. Levine, â€œEmergent complexity and zero-shot transfer via
unsupervised environment design,â€ Advances in neural information
processing systems, vol. 33, 2020.
[20] M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette,
and T. RocktaÌˆschel, â€œReplay-guided adversarial environment design,â€
Advances in Neural Information Processing Systems, 2021.
[21] J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster,
E. Grefenstette, and T. RocktaÌˆschel, â€œEvolving curricula with regretbased environment design,â€ in International Conference on Machine
Learning, 2022.
[22] L. GissleÌn, A. Eakins, C. Gordillo, J. Bergdahl, and K. Tollmar, â€œAdversarial reinforcement learning for procedural content generation,â€ in
2021 IEEE Conference on Games (CoG), 2021.
[23] A. Wong, T. BaÌˆck, A. V. Kononova, and A. Plaat, â€œDeep multiagent
reinforcement learning: Challenges and directions,â€ Artificial Intelligence Review, 2023.
[24] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun, â€œLearning lane graph representations for motion forecasting,â€ in
Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow,
UK, August 23â€“28, 2020, Proceedings, Part II 16, 2020.
[25] K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger,
â€œTransfuser: Imitation with transformer-based sensor fusion for autonomous driving,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.
[26] A. Cui, S. Casas, K. Wong, S. Suo, and R. Urtasun, â€œGorela: Go
relative for viewpoint-invariant motion forecasting,â€ in 2023 IEEE
International Conference on Robotics and Automation (ICRA), 2023.
[27] K. Cho, B. van Merrienboer, CÌ§aglar GuÌˆlcÌ§ehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, â€œLearning phrase representations using rnn encoderâ€“decoder for statistical machine translation,â€
in Conference on Empirical Methods in Natural Language Processing,
2014.
[28] Y. Du, L. Han, M. Fang, J. Liu, T. Dai, and D. Tao, â€œLiir: Learning
individual intrinsic reward in multi-agent reinforcement learning,â€
Advances in neural information processing systems, 2019.
[29] A. Abouelazm, J. Michel, and J. M. ZoÌˆllner, â€œA review of reward
functions for reinforcement learning in the context of autonomous
driving,â€ in 2024 IEEE Intelligent Vehicles Symposium (IV), 2024.
[30] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and
Y. Wu, â€œThe surprising effectiveness of ppo in cooperative multi-agent
games,â€ Advances in Neural Information Processing Systems, 2022.
[31] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
â€œProximal policy optimization algorithms,â€ arXiv preprint
arXiv:1707.06347, 2017.

[32] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. S.
Torr, M. Sun, and S. Whiteson, â€œIs independent learning all you need
in the starcraft multi-agent challenge?â€ 2020.
[33] M. L. Littman and C. SzepesvaÌri, â€œA generalized reinforcementlearning model: Convergence and applications,â€ in ICML, 1996.
[34] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al., â€œOvercoming catastrophic forgetting in neural networks,â€ Proceedings of the national academy of sciences, 2017.

