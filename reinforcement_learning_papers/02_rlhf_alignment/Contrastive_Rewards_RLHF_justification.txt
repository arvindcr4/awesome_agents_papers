Paper: Contrastive_Rewards_RLHF.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards  Wei Shen * 1 Xiaoying Zhang * 2 Yuanshun Yao 2 Rui Zheng 1 Hongyi Guo 3 Yang Liu 2  arXiv:2403.07708v2 [cs.CL] 14 Mar 2024  Abstract  2023a). Despite the successes, the effectiveness of RLHF relies heavily on the reward model (RM) used in the Proximal Policy Optimization (PPO) (Schulman et al., 2017) stage to guide the learning process.  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accur...

Specific Evidence for Classification:
- "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards  Wei Shen * 1 Xiaoying Zhang * 2 Yuanshun Yao 2 Rui Zheng 1 Hongyi Guo 3 Yang Liu 2  arXiv:2403.07708v2 [cs.CL] 14 Mar 2024  Abstract  2023a)."
- "Despite the successes, the effectiveness of RLHF relies heavily on the reward model (RM) used in the Proximal Policy Optimization (PPO) (Schulman et al., 2017) stage to guide the learning process."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
