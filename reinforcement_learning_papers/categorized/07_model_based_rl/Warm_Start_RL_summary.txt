Here is the extracted information:

**PAPER:** Efficient Online Reinforcement Learning: Fine-Tuning Need Not Retain Offline Data
**TITLE:** Efficient Online Reinforcement Learning: Fine-Tuning Need Not Retain Offline Data
**RESEARCH_METHOD:** 07_model_based_rl
**METHOD_DESCRIPTION:** The paper proposes a new approach to fine-tuning reinforcement learning (RL) agents online without retaining any offline datasets. The approach, called Warm-start RL (WSRL), uses a warmup phase to initialize the online replay buffer with a small number of rollouts from the pre-trained policy, and then runs a standard online RL algorithm for fine-tuning. WSRL mitigates the catastrophic forgetting of pre-trained initializations and prevents Q-value divergence due to distribution shift.
**KEY_CONTRIBUTIONS:**
* WSRL enables efficient fine-tuning without retaining offline data, outperforming existing algorithms in several environments.
* The warmup phase is crucial for preventing Q-value divergence and catastrophic forgetting.
* WSRL is agnostic to the offline RL pre-training algorithm and can work with different types of value initializations.
* The approach is simple and practical, making it a promising solution for scalable RL.
