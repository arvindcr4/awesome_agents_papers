Mean-Field Multi-Agent Reinforcement Learning: A
Decentralized Network Approach
Haotian Gu âˆ—

Xin Guo â€ 

Xiaoli Wei â€¡

Renyuan Xu Â§

arXiv:2108.02731v2 [cs.LG] 19 Feb 2022

February 22, 2022

Abstract
One of the challenges for multi-agent reinforcement learning (MARL) is designing efficient learning algorithms for a large system in which each agent has only limited or partial
information of the entire system. While exciting progress has been made to analyze decentralized MARL with the network of agents for social networks and team video games, little
is known theoretically for decentralized MARL with the network of states for modeling
self-driving vehicles, ride-sharing, and data and traffic routing.
This paper proposes a framework of localized training and decentralized execution to
study MARL with network of states. Localized training means that agents only need to
collect local information in their neighboring states during the training phase; decentralized
execution implies that agents can execute afterwards the learned decentralized policies,
which depend only on agentsâ€™ current states.
The theoretical analysis consists of three key components: the first is the reformulation
of the MARL system as a networked Markov decision process with teams of agents, enabling
updating the associated team Q-function in a localized fashion; the second is the Bellman
equation for the value function and the appropriate Q-function on the probability measure
space; and the third is the exponential decay property of the team Q-function, facilitating
its approximation with efficient sample efficiency and controllable error.
The theoretical analysis paves the way for a new algorithm LTDE-Neural-AC, where
the actor-critic approach with over-parameterized neural networks is proposed. The convergence and sample complexity is established and shown to be scalable with respect to the
sizes of both agents and states. To the best of our knowledge, this is the first neural network
based MARL algorithm with network structure and provably convergence guarantee.

Keywords: Multi-Agent Reinforcement Learning, Mean-field Cooperative Games, Neural
Network Approximation

1

Introduction

Multi-agent reinforcement learning (MARL) has achieved substantial successes in a broad range
of cooperative games and their applications, including coordination of robot swarms (HÃ¼ttenrauch et al. [28]), self-driving vehicles (Shalev-Shwartz et al. [48], Cabannes et al. [5]), real-time
bidding games (Jin et al. [31]), ride-sharing (Li et al. [35]), power management (Zhou et al. [66])
and traffic routing (El-Tantawy et al. [16]). One of the challenges for the development of MARL
is designing efficient learning algorithms for a large system, in which each individual agent has
only limited or partial information of the entire system. In such a system, it is necessary to
design algorithms to learn policies of the decentralized type, i.e., policies that depend only on
the local information of each agent.
âˆ— Department of Mathematics, University of California, Berkeley, USA. Email: haotian_gu@berkeley.edu
â€  Department of Industrial Engineering & Operations Research, University of California, Berkeley, USA.
Email: xinguo@berkeley.edu
â€¡ Tsinghua-Berkeley Shenzhen Institute, Shenzhen, China. Email: xiaoli_wei@sz.tsinghua.edu
Â§ Industrial & Systems Engineering, University of Southern California, Los Angeles, USA. Email:
renyuanx@usc.edu

1

In a simulated or laboratory setting, decentralized policies may be learned in a centralized
fashion. It is to train a central controller to dictate the actions of all agents. Such paradigm
of centralized training with decentralized execution has achieved significant empirical successes,
especially with the computational power of deep neural networks (Lowe et al. [40], Foerster et al.
[17], Chen et al. [14], Rashid et al. [47], Yang et al. [57], Vadori et al. [52]). Such a training
approach, however, suffers from the curse of dimensionality as the computational complexity
grows exponentially with the number of agents (Zhang et al. [62]); it also requires extensive
and costly communications between the central controller and all agents (Rabbat and Nowak
[45]). Moreover, policies derived from the centralized training stage may not be robust in the
execution phase (Zhang et al. [60]). Most importantly, this approach has not been supported
or analyzed theoretically.
An alternative and promising paradigm is to take into consideration the network structure
of the system to train decentralized policies. Compared with the centralized training approach,
exploiting network structures makes the training procedure more efficient as it allows the algorithm to be updated with parallel computing and reduces communication cost.
There are two distinct types of network structures. The first is the network of agents, often
found in social networks such as Facebook and Twitter, as well as team video games including
StarCraft II. This network describes interactions and relations among heterogeneous agents.
For MARL systems with such network of agents, Zhang et al. [63] establishes the asymptotic
convergence of decentralized-actor-critic algorithms which are scalable in agent actions. Similar
ideas are extended to the continuous space where deterministic policy gradient method (DPG)
is used (Zhang et al. [61]), with finite-sample analysis for such framework established in the
batch setting (Zhang et al. [64]). Qu et al. [44] studies a network of agents where state and
action interact in a local manner; by exploiting the network structure and the exponential decay
property of the Q-function, it proposes an actor-critic framework scalable in both actions and
states. Similar framework is considered for the linear quadratic case with local policy gradients
conducted with zero order optimization and parallel updating (Li et al. [36]).
The second type of network, the network of states, has been frequently used for modeling
self-driving vehicles, ride-sharing, and data and traffic routing. It focuses on the state of agents.
Compared with the network of agents which is static from agentâ€™s perspective (Sunehag et al.
[50]), the network of states is stochastic: neighboring agents of any given agent may change
dynamically. This type of network has been empirically studied in various applications, including
packet routing (You et al. [58]), traffic routing (Calderone and Sastry [7], GuÃ©riau and Dusparic
[25]), resource allocations (Cao et al. [8]) and social economic systems (Zheng et al. [65]).
However, there is no existing theoretical analysis for this type of decentralized MARL. Moreover,
the dynamic nature of agentsâ€™ relationship makes it difficult to adopt existing methodology from
the static network of agents. The goal of this paper is, therefore, to fill this void.
Our work. This paper proposes and studies multi-agent systems with network structure of
agent states. In this network, homogeneous agents can move from one state to any connecting
state, and observe (realistically) only partial information of the entire system in an aggregated
fashion. To study this system, we propose a framework of localized training and decentralized
execution (LTDE). Localized training means that agents only need to collect local information in
their neighboring states during the training phase; decentralized execution implies that, agents
can execute afterwards the learned decentralized policies which only require knowledge of agentsâ€™
current states.
The theoretical analysis consists of three key components. The first is to regroup these
homogeneous agents according to their states and reformulate the MARL system as a networked
Markov decision process with teams of agents. This reformulation leads to the decomposition
of the Q-function and the value function according to the states, enabling the update of the
consequent team Q-function in a localized fashion. The second is to establish the Bellman
equation for the value function and the appropriate Q-function on the probability measure
space, by utilizing the homogeneity of agents. These functions are invariant with respect to
the number of agents. The third is to explore the exponential decay property of the team Qfunction, enabling its approximation with a truncated version of a much smaller dimension and
2

yet with a controllable approximation error.
To design an efficient and scalable reinforcement learning algorithm for such framework, the
actor-critic approach with over-parameterized neural networks is adopted. The neural networks,
representing decentralized policies and localized Q-functions, are much smaller compared with
the global one. The convergence and the sample complexity of the proposed algorithm is
established and shown to be scalable with respect to the size of both agents and states. To the
best of our knowledge, this is the first neural network based MARL algorithm with network
structure and provably convergence guarantee.
Our contribution. Our work contributes to two lines of research.
The first one is for mean-field control with reinforcement learning, for which existing works
require that each agent have the full information of the population distribution (Gu et al.
[24], Carmona et al. [10, 11], Motte and Pham [42]) and yet in most applications agents only
have access to partial or limited information (Yang et al. [56]). We build a theoretical framework
that incorporates network structures in the MARL framework, and provide computationally
efficient algorithms where each agent only needs local information of neighborhood states to
learn and to execute the policy.
Secondly, our work builds the theoretical foundation for the practically popular scheme
of centralized-training-decentralized-execution (CTDE) (Lowe et al. [40], Rashid et al. [47],
Vadori et al. [52], Yang et al. [57]). The CTDE framework is first proposed in Lowe et al.
[40] to learn optimal policies in cooperative games with two steps: the first step is to train
a global policy for the central controller, and the second step is to decompose the central
policy (i.e., a large Q-table) into individual policies so that individual agent can apply the
decomposed/decentralized policy after training. Despite the popularity of CTDE, however,
there has been no theoretical study as to when the Q-table can be decomposed and when the
truncation error can be controlled, except for a heuristic argument by Lowe et al. [40] for large
N with local observations. Our paper analyzes for the first time with theoretical guarantee that
applying our algorithm to this CTDE paradigm yields a near-optimal sample complexity, when
there is a network structure among agent states. Moreover, our algorithm, which is easier to
scale-up, improves the centralized training step with a localized training. To differentiate our
approach from the CTDE scheme, we call it localized-training-decentralized-execution (LTDE).
Notation. For a set X , denote RX = {f : X â†’ R} as the set of all real-valued functions on
X . For each f âˆˆ RX , define kf kâˆž = supxâˆˆX |f (x)| as the sup norm of f . In addition, when
X is finite, denote |X | P
as the size of X , and P(X ) as the set of all probability measures on X :
P(X ) = {p : p(x) â‰¥ 0, xâˆˆX p(x) = 1}, which is equivalent to the probability simplex in R|X | .
[N ] := {1, 2, Â· Â· Â· , N }. For any Âµ âˆˆ P(X ) and a subset Y âŠ‚ X , let Âµ(Y) denote the restriction of
the vector Âµ on Y, and let P(Y) denote the set {Âµ(Y) : Âµ âˆˆ P(X )}. For x âˆˆ Rd , d âˆˆ N, denote
kxk2 as the L2 -norm of x and kxkâˆž as the Lâˆž -norm of x.

2

Mean-field MARL with Local Dependency

The focus of this paper is to study a cooperative multi-agent system with a network of agent
states, which consists of nodes representing states of the agents and edges by which states are
connected. In this system, every agent is only allowed to move from her present state to its connecting states. Moreover, she is assumed to only observe (realistically) partial information of the
system on an aggregated level. Mean-field theory provides efficient approximations when agents
only observe aggregated information, and has been applied in stochastic systems with large
homogeneous agents such as financial markets (Carmona et al. [9], Lacker and Zariphopoulou
[34], Hu and Zariphopoulou [27], Casgrain and Jaimungal [12]), energy markets (Germain et al.
[21], AÃ¯d et al. [2]), and auction systems (Iyer et al. [29], Guo et al. [26]).

3

2.1

Review of MARL

Let us first recall the cooperative MARL in an infinite time horizon, where there are N agents
whose policies are coordinated by a central controller. We assume that both the state space S
and the action space A are finite.
At each step t = 0, 1, Â· Â· Â· , the state of agent i (= 1, 2, Â· Â· Â· , N ) is sit âˆˆ S and she takes an
N
action ait âˆˆ A. Given the current state profile s t = (s1t , Â· Â· Â· , sN
and the current action
t ) âˆˆ S
1
N
N
profile a t = (at , Â· Â· Â· , at ) âˆˆ A of N agents, agent i will receive a reward ri (s t , a t ) and her
state will change to sit+1 according to a transition probability function P i (s t , a t ). A Markovian
game further restricts the admissible policy for agent i to be of the form ait âˆ¼ Ï€ti (s t ). That is,
Ï€ti : S N â†’ P(A) maps each state profile s âˆˆ S N to a randomized action, with P(A) the space
of all probability measures on space A.
In this cooperative MARL framework, the central controller is to maximize the expected
discounted accumulated reward averaged over all agents. That is to find
N

V (s ) = sup
Ï€

where

1 X i
v (s , Ï€ ),
N i=1


X
âˆž
t i
Î³ r (s t , a t ) s 0 = s
v (s , Ï€ ) = E
i

(2.1)

(2.2)

t=0

is the accumulated reward for agent i, given the initial state profile s 0 = s and policy Ï€ = {Ï€ t }âˆž
t=0
with Ï€ t = (Ï€t1 , . . . , Ï€tN ). Here Î³ âˆˆ (0, 1) is a discount factor, ait âˆ¼ Ï€ti (s t ), and sit+1 âˆ¼ P i (s t , a t ).
The corresponding Bellman equation for the value function (2.1) is
( "
#
)
N
1 X i
0
V (s ) = sup E
r (s , a ) + Î³Es 0 âˆ¼P (s ,a ) [V (s )] ,
(2.3)
N i=1
a âˆˆAN
with the population transition kernel P = (P 1 , Â· Â· Â· , P N ). The value function can be written as
V (s ) = sup Q(s , a ),
a âˆˆAN

in which the Q-function is defined as
"
#
N
1 X i
Q(s , a ) = E
r (s , a ) + Î³Es 0 âˆ¼P (s ,a ) [V (s 0 )],
N i=1

(2.4)

consisting of the expected reward from taking action a at state s and then following the optimal
policy thereafter. The Bellman equation for the Q-function, defined from S N Ã— AN to R, is
given by
#
"


N
1 X i
r (s , a ) + Î³Es0 âˆ¼PP (ss,aa) sup Q(s 0 , a 0 ) .
(2.5)
Q(s , a ) = E
N i=1
a 0 âˆˆAN
One can thus retrieve the optimal (stationary) control Ï€ âˆ— (s , a ) (if it exists) from Q(s , a ), with
Ï€ âˆ— (s ) âˆˆ arg maxa âˆˆAN Q(s , a ).

2.2

Mean-field MARL with Local Dependency

In this system, there are N agents who share a finite state space S and take actions from a
finite action space A. Moreover, there is a network on the state space S associated with an
underlying undirected graph (S, E), where E âŠ‚ S Ã— S is the set of edges. The distance between
two nodes is defined as the number of edges in a shortest path. For a given s âˆˆ S, Ns1 denotes
the nearest neighbor of s, which consists of all nodes connected to s by an edge and includes s
4

itself; and Nsk denotes the k-hop neighborhood of s, which consists of all nodes whose distance
to s is less than or equal to k, including s itself. For simplicity, we use Ns := Ns1 . From agent
iâ€™s perspective, agents in her neighborhood Nsit change stochastically over time.
To facilitate mean-field approximation to this system, assume throughout the paper that
the agents are homogeneous and indistinguishable. In particular, at each step t = 0, 1, Â· Â· Â· , if
agent i at state sit âˆˆ S takes an action ait âˆˆ A, then she will receive a stochastic reward which
is uniformly upper bounded by rmax such that


(2.6)
ri (s t , a t ) :=r sit , Âµt (Nsit ), ait â‰¤ rmax , i âˆˆ [N ];
and her state will change to a neighboring state sit+1 âˆˆ Nsit according to a transition probability
such that


sit+1 âˆ¼ P i (s t , a t ) :=P Â· sit , Âµt (Nsit ), ait , i âˆˆ [N ],
(2.7)
PN


1(si =Â·)
âˆˆ P N (S) := Âµ âˆˆ P(S) : Âµ(s) âˆˆ 0, N1 , N2 , Â· Â· Â· , NNâˆ’1 , 1 for all s âˆˆ S
where Âµt (Â·) = i=1 N t
is the empirical state distribution of N agents at time t, with N Â· Âµt (s) the number of agents in
state s at time t, and Âµt (Nsit ) the restriction of Âµt on the 1-hop neighbor of sit .
(2.6)-(2.7) indicate that the reward and the transition probability of agent i at time t depend on both her individual information (ait , sit ) and the mean-field of her 1-hop neighborhood
Âµt (Nsit ), in an aggregated yet localized format: aggregated or mean-field meaning that agent i
depends on other agents only through the empirical state distribution; localized meaning that
agent i depends on the mean-field information of her 1-hop neighborhood. Intuitive examples
of such a setting include traffic-routing, package delivery, data routing, resource allocations,
distributed control of autonomous vehicles and social economic systems.

Policies with partial information. To incorporate the element of partial or limited information into this mean-field MARL system, consider the following individual-decentralized
policies

ait âˆ¼ Ï€ i (s t ) := Ï€ sit , Âµt (sit ) âˆˆ P(A), i âˆˆ [N ],
(2.8)
and denote u as the admissible policy set of all such policies.
Note that for a given mean-field information Âµt , Ï€(Â·, Âµt (Â·)) : S â†’ P(A) maps the agent state
to a randomized action. That is, the policy of each agent is executed in a decentralized manner
and assumes that each agent only has access to the population information in her own state.
This is more realistic than centralized policies which assume full access to the state information
of all agents.
Value function and Q-function. The goal for this mean-field MARL is to maximize the
expected discounted accumulated reward averaged over all agents, i.e.,
"âˆž
#
N
X
X

1
V (Âµ) := sup V Ï€ (Âµ) = sup
E
Î³ t r sit , Âµt (Nsit ), ait
Âµ0 = Âµ ,
(MF-MARL)
Ï€âˆˆu
Ï€âˆˆu N
t=0
i=1
subject to (2.6)-(2.8) with a discount factor Î³ âˆˆ (0, 1).
The mean-field assumption leads to the following definition of the corresponding Q-function
for (MF-MARL) on the measure space:
N
i
hX
1
r(si0 , Âµ(Nsi0 ), ai0 ) s 0 , a 0
Q(Âµ, h) : =
E
N
i=1
|
{z
}
Expected reward of taking a 0 = (a10 , Â· Â· Â· , aN
0 )

"âˆž
X

+ Ei

s1 âˆ¼P Â· si0 , Âµ(Nsi ), ai0



0

|

t=1

Î³

t

N
X
1
i=1

{z

N

#
r(sit , Âµt (Nsit ), ait )

Expected reward of playing optimally thereafter ait âˆ¼ Ï€t?

5

ait âˆ¼ Ï€t?

, (2.9)
}

PN

1(si =Â·)

PN

1(si =s,ai =a)

PN 0 i 0
where Âµ(Â·) = i=1 N 0
is the initial empirical state distribution and h(s)(a) = i=1
i=1 1(s0 =s)
is a â€œdecentralizedâ€ policy representing the proportion of agents in state s that takes action a.
Specifically, given Âµ âˆˆ P N (S), s âˆˆ S, and the N Â· Âµ(s) agents in state s,


N Â· Âµ(s) âˆ’ 1
1
,Â·Â·Â· ,
} for all a âˆˆ A âŠ‚ P(A),
h(s) âˆˆ P N Â·Âµ(s) (A) := Ï‚ âˆˆ P(A) : Ï‚(a) âˆˆ {0,
N Â· Âµ(s)
N Â· Âµ(s)

where Ï‚ in P N Â·Âµ(s) (A) is an empirical action distribution of N Â· Âµ(s) agents in state s, and
Ï‚(a) is the proportion of agents taking action a âˆˆ A among all N Â· Âµ(s) agents in state s.
Furthermore, for a given s âˆˆ S, denote P N Â·Âµ(s) (A) the set of all admissible â€œdecentralizedâ€
policies h(s)(Â·); and for a given Âµ âˆˆ P N (S), denote the product of P N Â·Âµ(s) (A) over all states
by HN (Âµ) := {h : h(s) âˆˆ P N Â·Âµ(s) (A) âˆ€ s âˆˆ S}. Here HN (Âµ) depends on Âµ and is a subset of
H = {h : S â†’ P(A)}.
Note that Q(Âµ, h) defined in (2.9) is invariant with respect to the order of the elements in s 0
and a 0 . More critically, the input dimension of the Q-function defined in (2.9) is independent
from the number of agents in the system, hence is easier to scale up in a large population regime.
This differs from the the input dimension of the Q-function in (2.4), which grows exponentially
with respect to the number of agents, the main culprit of the curse of dimensionality for MARL
algorithms.

3

Analysis of MF-MARL with Local Dependency

The theoretical study of this mean-field MARL with local dependency (Section 2.2) consists
of three key components, which are crucial for subsequent algorithm design and convergence
analysis: the first is the reformulation of the MARL system as a networked Markov decision
process with teams of agents. This reformulation leads to the decomposition of the Q-function
and the value function according to states, facilitating updating the consequent team Q-function
in a localized fashion (Section 3.1); the second is the Bellman equation for the value function
and the Q-function on the probability measure space (Section 3.2); the third is the exponential
decay property of the team Q-function, enabling its approximation with a truncated version of
a much smaller dimension and yet with a controllable approximation error (Section 3.3).

3.1

Markov Decision Process (MDP) on Network of States

This section shows that the mean-field MARL (2.6)-(2.8) can be reformulated in an MDP
framework by exploiting the network structure of states. This reformulation leads to the decomposition of the Q-function, facilitating more computationally efficient updates.
The key idea is to utilize the homogeneity of the agents in the problem set-up and to regroup
these N agents according to their states. This regrouping translates (MF-MARL) with N agents
into a networked MDP with |S| agents teams, indexed by their states.

Figure 1: Left: MF-MARL problem (2.6)-(2.8). Right: Reformulation of team game (3.2)(3.6).

6

To see how the policy, the reward function, and the dynamics in this networked Markov
decision process are induced by the regrouping approach, recall that there are N Â· Âµ(s) agents
in state s, each agent i in state s will independently choose action ai âˆ¼ Ï€(s, Âµ(s)) according to
the individual-decentralized policy Ï€(s, Âµ(s)) âˆˆ P(A) in (2.8). Therefore the empirical action
distribution of {a1 , Â· Â· Â· , aN Â·Âµ(s) } is a random variable taking values from P N Â·Âµ(s) (A), the set of
empirical action distributions with N Â· Âµ(s) agents. Moreover, for any h(s) âˆˆ P N Â·Âµ(s) (A), we
have


i.i.d
P h(s) is the empirical action distribution of {a1 , Â· Â· Â· , aN Â·Âµ(s) }, ai âˆ¼ Ï€(s, Âµ(s))


i.i.d
= P for each a âˆˆ A, a appears N Â· Âµ(s)h(s)(a) times in {a1 , Â· Â· Â· , aN Â·Âµ(s) }, ai âˆ¼ Ï€(s, Âµ(s))
N Â·Âµ(s)h(s)(a)
Y
(N Â· Âµ(s))!
Ï€(s, Âµ(s))(a)
.
(3.1)
= Y
(N Â· Âµ(s)h(s)(a))! aâˆˆA
aâˆˆA

Here h(s)(a) denotes the proportion of agents taking action a among all agents in state s, with
last equality derived from the multinomial distribution with parameters N Â· Âµ(s) and Ï€(s, Âµ(s)).
Now, clearly each individual-decentralized policy Ï€(s, Âµ(s)) âˆˆ P(A) in (2.8) induces a teamdecentralized policy of the following form:
N Â·Âµ(s)h(s)(a)
Y
(N Â· Âµ(s))!
Ï€(s, Âµ(s))(a)
,
(3.2)
Î s (h(s) | Âµ(s)) = Y
(N Â· Âµ(s)h(s)(a))! aâˆˆA
aâˆˆA

where h(s) âˆˆ P N Â·Âµ(s) (A). Conversely, given a team-decentralized policy Î s ( Â· | Âµ(s)), one can
recover the individual-decentralized policy Ï€(s, Âµ(s)) by choosing appropriate h(s) âˆˆ P N Â·Âµ(s) (A)
and querying the value of Î s (h(s) | Âµ(s)): let hi (s) = Î´ai be the Dirac measure with ai âˆˆ A,
which is an action distribution such that all agents in state s take action ai . By (3.2), Î s (hi (s) |
1
N Â·Âµ(s)
Âµ(s)) = (Ï€(s, Âµ(s))(ai ))
, implying Ï€(s, Âµ(s))(ai ) = (Î (hi (s)|Âµ(s)) N Â·Âµ(s) .
Next, given Âµ âˆˆ P N (S) and h âˆˆ HN (Âµ) = {h : h(s) âˆˆ P N Â·Âµ(s) (A), âˆ€s âˆˆ S}, the set of
empirical action distributions on every state, if we define
Y
Î (h | Âµ) :=
Î s (h(s) | Âµ(s)),
(3.3)
sâˆˆS

then u, the admissible policy set of individual-decentralized policies in the form of (2.8), is now
replaced by U, the set of all team-decentralized policies Î  induced from Ï€ âˆˆ u through (3.2) and
(3.3). In addition, denote the set of all state-action distribution pairs as
Îž := âˆªÂµâˆˆP N (S) {Î¶ = (Âµ, h) : h âˆˆ HN (Âµ)},

(3.4)

Moreover, from the team perspective, the transition probability in (2.7) can be viewed as a
Markov process of Âµt and ht âˆˆ HN (Âµt ) with an induced transition probability PN from (2.7)
such that
Âµt+1 âˆ¼ PN (Â· | Âµt , ht ).
(3.5)
It is easy to verify that for a given state s âˆˆ S, Âµt+1 (s) only depends on Âµt (Ns2 ), the empirical
distribution in the 2-hop neighborhood of s, and ht (Ns ).
Finally, given Âµ(Ns ) âˆˆ P N (Ns ), an empirical distribution restricted to the 1-hop neighborhood of s, one can define a localized team reward function for team s from P N Â·Âµ(s) (A) to R as
X
rs (Âµ(Ns ), h(s)) =
r(s, Âµ(Ns ), a)h(s)(a),
(3.6)
aâˆˆA

which depends on the state s and its 1-hop neighborhood; and define the maximal expected
discounted accumulative localized team rewards over all teams as
X

âˆž X
Ve (Âµ) := sup Ve Î  (Âµ) = sup E
Î³ t rs (Âµt (Ns ), ht (s)) Âµ0 = Âµ .
(3.7)
Î âˆˆU

Î âˆˆU

t=0 sâˆˆS

7

With all these key elements, one can establish the equivalence between maximizing the
reward averaged over all agents in (MF-MARL) and maximizing the localized team reward
summed over all teams in (3.7), and can thus reformulate the (MF-MARL) problem as an
equivalent MDP of (3.2)-(3.7) with |S| teams, the latter denoted as (MF-DEC-MARL). (The
proof is detailed in Appendix A). That is,
Lemma 3.1 (Value function and Q-function decomposition)
X
V (Âµ) = Ve (Âµ) = sup
VesÎ  (Âµ),

(3.8)

where ht âˆ¼ Î (Â· | Âµt ), Âµt+1 âˆ¼ PN (Â· | Âµt , ht ), and
X

âˆž
Î 
t
e
Vs (Âµ) = E
Î³ rs (Âµt (Ns ), ht (s)) Âµ0 = Âµ

(3.9)

Î âˆˆU sâˆˆS

t=0

is called the value function under policy Î  for team s. Similarly,
X
 X
âˆž
X
QÎ  (Âµ, h) : = E
Î³t
rs (Âµt (Ns ), ht (s)) Âµ0 = Âµ, h0 = h =
QÎ 
s (Âµ, h),
t=0

where

sâˆˆS

(3.10)

sâˆˆS

X
âˆž

QÎ 
s (Âµ, h) = E


Î³ rs (Âµt (Ns ), ht (s)) Âµ0 = Âµ, h0 = h ,
t

(3.11)

t=0

is the Q-function under policy Î  for team s, called team-decentralized Q-function.
The decomposition for the Q-function in (3.10) is one of the key elements to allow for
approximation of QÎ 
s (Âµ, h) by a truncated Q-function defined on a smaller space and updated
in a localized fashion; it is useful for designing sample-efficient learning algorithms and for
parallel computing, as will be clear in the next Section 3.3.

3.2

Bellman equation for Q-function.

This section builds the second block for reinforcement learning algorithms, the Bellman equation
for Q-function. Indeed, the Bellman equation for Q(Âµ, h) can be derived following a similar argument in Gu et al. [23], after establishing the dynamic programming principle on an appropriate
probability measure space.
Lemma 3.2 (Bellman Equation for Q-function) The Q-function defined in (2.9) satisfies:
#
"
#
"N
X 1
i
i
0

r(s0 , Âµ(Nsi0 ), a0 ) s 0 , a 0 + Î³E i
sup Q (Âµ1 , h ) (3.12)
.
Q(Âµ, h) = E
s1 âˆ¼P Â· si0 , Âµ(Nsi ), ai0
N
h0 âˆˆHN (Âµ1 )
0
i=1
with Âµ1 (Â·) =

PN

i
i=1 1(s1 =Â·)

N

the empirical state distribution at time 1.

Note that the Bellman equation (3.12) is for the Q-function defined in (2.9) for general meanfield MARL. In order to enable the localized-training-decentralized-execution for computational
efficiency, one needs to consider the decomposition of Q-function (3.10) and the updating rule
based on the team-decentralized Q-function (3.11). The corresponding Bellman equation for
the team-decentralized Q-function (3.11) is:
Lemma 3.3 Given a policy Î  âˆˆ U, QÎ 
s defined in (3.11) is the unique solution to the Bellman
Î  Î 
Î 
equation QÎ 
s = Ts Qs , with Ts the Bellman operator taking the form of


Î  0
0
TsÎ  QÎ 
(3.13)
s (Âµ, h) = EÂµ0 âˆ¼PN (Â· | Âµ,h), h0 âˆ¼Î (Â· | Âµ) rs (Âµ, h) + Î³ Â· Qs (Âµ , h ) , âˆ€(Âµ, h) âˆˆ Îž.
These Bellman equations are the basis for general Q-function-based algorithms in mean-field
MARL.
8

3.3

Exponential Decay of Q-function

This section will show that the team-decentralized Q-function QÎ 
s (Âµ, h) has an exponential
decay property. This is another key element to enable an approximation to QÎ 
s by a localized Qk
k
bÎ 
(Âµ(N
),
h(N
)),
and
to
guarantee
the
scalability
and
sample
efficiency
of subsequent
function Q
s
s
s
algorithm design.
To establish the exponential decay property of the Q-function (3.11), first recall that Nsk
is the set of k-hop neighborhood of state s, and define Nsâˆ’k = S/Nsk as the set of states that
are outside of sâ€™th k-hop neighborhood.
Next, rewrite any given empirical state

 distribution
Âµ âˆˆ P N (S) as Âµ(Nsk ), Âµ(Nsâˆ’k ) , and similarly, h âˆˆ HN (Âµ) as h(Nsk ), h(Nsâˆ’k ) .
Definition 3.4 The QÎ  is said to have (c, Ï)-exponential decay property, if for any s âˆˆ S and
any Î  âˆˆ U, (Âµ, h), (Âµ0 , h0 ) âˆˆ Îž with Âµ(Nsk ) = Âµ0 (Nsk ) and h(Nsk ) = h0 (Nsk )


k
âˆ’k
k
âˆ’k
Î 
k
0
âˆ’k
k
0
âˆ’k
â‰¤ cÏk+1 .
QÎ 
s Âµ(Ns ), Âµ(Ns ), h(Ns ), h(Ns ) âˆ’ Qs Âµ(Ns ), Âµ (Ns ), h(Ns ), h (Ns )
Note that the exponential decay property is defined for the team-decentralized Q-function
,
instead of the centralized Q-function QÎ  . The following Lemma provides a sufficient
QÎ 
s
condition for the exponential decay property. Its proof is given in Appendix B.
Lemma 3.5 When thereward rs in (3.6) is uniformly upper bounded by rmax > 0, for any
rmax âˆš
s âˆˆ S, QÎ 
s satisfies the
1âˆ’Î³ , Î³ -exponential decay property.
The exponential decay property implies that for a given state s âˆˆ S, the dependence of
QÎ 
s on other states decays quickly with respect to its distance from state s. It motivates and
k
enables the approximation of QÎ 
s (Âµ, h) by a truncated function which only depends on Âµ(Ns )
k
and h(Ns ), especially when k is large and Ï is small. Specifically, consider the following class
of localized Q-functions,
"




X
k
k
bÎ 
Q
ws Âµ(Nsâˆ’k ), h(Nsâˆ’k ); Âµ(Nsk ), h(Nsk )
s Âµ(Ns ), h(Ns ) =
Âµ(Nsâˆ’k ),h(Nsâˆ’k )

Â· QÎ 
s





Âµ(Nsk ), Âµ(Nsâˆ’k ), h(Nsk ), h(Nsâˆ’k )

#
,

(Local Q-function)

where ws Âµ(Nsâˆ’k ), h(Nsâˆ’k ); Âµ(Nsk ), h(Nsâˆ’k ) are any non-negative weights of


X
ws Âµ(Nsâˆ’k ), h(Nsâˆ’k ); Âµ(Nsk ), h(Nsk ) = 1
Âµ(Nsâˆ’k ),h(Nsâˆ’k )

for any Âµ(Nsk ) and h(Nsk ).
Then, direct computation yields the following proposition.
bÎ 
Proposition 3.6 Let Q
s be any localized Q-function in the form of (Local Q-function). Assume the (c, Ï)-exponential decay property in Definition 3.4 holds, then for any Âµ âˆˆ P N (S) and
h âˆˆ HN (Âµ),


b Î  Âµ(N k ), h(N k ) âˆ’ QÎ  (Âµ, h) â‰¤ cÏk+1 .
Q
(3.14)
s
s
s
s
Moreover, (3.14) holds independent of the weights in (Local Q-function).
bÎ 
Note that given a team-decentralized
Q-function
QÎ 
s , its localized version Qs only takes


k
k
Î 
bÎ 
Âµ(Nsk ), h(Nsk ) as inputs, and Q
s Âµ(Ns ), h(Ns ) is defined as a weighted average of Qs over
9



all (Âµ, h)-pairs which agree with Âµ(Nsk ), h(Nsk ) in the k-hop neighborhood of s. Although
bÎ 
the localized Q-function Q
s may vary according to different choices of the weights, by the
b Î  approximates QÎ  with uniform error and requires a
exponential decay property, every Q
s
s
smaller dimension of input.
Remark 3.7 (Exponential Decay Property) In a discounted reward setting (2.1), the exponential decay property follows directly from the fact that the discount factor Î³ âˆˆ (0, 1) and the local
dependency structure in (3.2)-(3.7). For problems of finite-time or infinite horizons with ergodic
reward functions, this property can be established by imposing additional Lipschitz condition on
the transition kernel. (See Qu et al. [44], Theorem 1 for network of heterogeneous agents and
Î³ = 1).
It is also worth pointing out that the exponential decay property has been extensively explored
in random graphs (e.g., Gamarnik [19], Gamarnik et al. [20]) and for analysis of network of
agents in Qu et al. [44] and Lin et al. [37].

4

Algorithm Design

The three key analytical components for problem (MF-DEC-MARL) in previous sections pave
the way for designing efficient learning algorithms. In this section, we propose and analyze a
decentralized neural actor-critic algorithm, called LTDE-Neural-AC.
Î 
k
k
bÎ 
Our focus is the localized Q-function Q
s (Âµ(Ns ), h(Ns )), the approximation to Qs with a
Î 
b s and the team-decentralized policy
smaller input dimension. First, this localized Q-function Q
Î s will be parameterized by two-layer neural networks with parameters Ï‰s and Î¸s respectively
(Section 4.2). Next, these neural network parameters Î¸ = {Î¸s }sâˆˆS and Ï‰ = {Ï‰s }sâˆˆS are updated
via an actor-critic algorithm in a localized fashion (Section 4.3): the critic aims to find a proper
estimate for the localized Q-function under a fixed policy (parameterized by Î¸), while the actor
computes the policy gradient based on the localized Q-function, and updates Î¸ by a gradient
step.
These networks are updated locally requiring only information of the neighborhood states
during the training phase; afterwards agents in the system will execute these learned decentralized policies which requires only information of the agentâ€™s current state. This localized training
and decentralized execution enables efficient parallel computing especially for a large shared state
space.
Moreover, over-parameterization of neural networks avoids issues of nonconvexity and divergence associated with the neural network approach, and ensures the global convergence of our
proposed LTDE-Neural-AC algorithm.

4.1

Basic Set-up

Policy parameterization. To start, let us assume that at state s the team-decentralized
Q
policy Î Î¸ss is parameterized by Î¸s âˆˆ Î˜s . Further denote Î¸ := {Î¸s }sâˆˆS , Î˜ := sâˆˆS Î˜s , Î Î¸ :=
Q
Î¸s
Î¸
sâˆˆS Î s , and Î  := {Î  : Î¸ âˆˆ Î˜} as the class of admissible policies parameterized by the
parameter space {Î¸ : Î¸ âˆˆ Î˜}.
Initialization. Let us also assume that the initial state distribution Âµ0 of N agents is sampled
from a given distribution P0 over P N (S), i.e., Âµ0 âˆ¼ P0 ; and define the expected total reward
function J(Î¸) under policy Î Î¸ by
Î¸

J(Î¸) = EÂµ0 âˆ¼P0 [Ve Î  (Âµ0 )].

(4.1)

Visitation measure. Denote Î½Î¸ as the stationary distribution on Îž of the Markov process
(3.5) induced by Î Î¸ .

10

Similar to the single-agent RL problem (Agarwal et al. [1], Fu et al. [18]), each admissible
policy Î Î¸ induces a visitation measure ÏƒÎ¸ (Âµ, h) on Îž describing the frequency that policy Î Î¸
visits (Âµ, h), with
ÏƒÎ¸ (Âµ, h) := (1 âˆ’ Î³) Â·

âˆž
X


Î³ t Â· P Âµt = Âµ, ht = h | Î Î¸ ,

(4.2)

t=0

where Âµ0 âˆ¼ P0 , ht âˆ¼ Î Î¸ (Â· | Âµt ), and Âµt+1 âˆ¼ PN (Â· | Âµt , ht ).
Policy gradient theorem. In order to find the optimal parameterized policy Î Î¸ which maximizes the expected total reward function J(Î¸), the policy optimization step will search for Î¸ âˆˆ Î˜
along the gradient direction âˆ‡J(Î¸). Note that computing the gradient âˆ‡J(Î¸) depends on both
the action selection, which is directly determined by Î Î¸ , and the visitation measure ÏƒÎ¸ in (4.2),
which is indirectly determined by Î Î¸ .
A simple and elegant result called the policy gradient theorem (Lemma 4.1) proposed in
Sutton et al. [51], reformulates the gradient âˆ‡J(Î¸) in terms of QÎ Î¸ in (3.10) and âˆ‡ log Î Î¸ (h | Âµ)
under the visitation measure ÏƒÎ¸ . This result simplifies the gradient computation significantly,
and is fundamental for actor-critic algorithms.
h Î¸
i
1
EÏƒÎ¸ QÎ  (Âµ, h)âˆ‡ log Î Î¸ (h | Âµ) .
Lemma 4.1 (Sutton et al. [51]) âˆ‡J(Î¸) = 1âˆ’Î³
Now, direct implementation of the actor-critic algorithm with the centralized policy gradient
theorem in Lemma 4.1 suffers from high sample complexity due to the dimension of the Qfunction. Instead, we will show that the exponential decay property of Q-function allows efficient
approximation of the policy gradient via localization and hence a scalable algorithm to solve
(MF-MARL).

4.2

Neural Policy and Neural Q-function

b Î  (Âµ(N k ), h(N k )) (i.e., the approximation of QÎ  )
We now turn to the localized Q-function Q
s
s
s
s
and the team-decentralized policy Î s , and their parameterization by two-layer neural networks.
We emphasize that the parameterization framework in this section can be extended to any
neural-based single-agent algorithms with convergence guarantee.
Two-Layer Neural Network. For any input space X âŠ‚ Rdx with dimension dx âˆˆ N, a
two-layer neural network fe(x; W, b) with input x âˆˆ X and width M âˆˆ N takes the form of
M

1 X
fe(x; W, b) = âˆš
bm Â· ReLU (x Â· [W ]m ) .
M m=1

(4.3)

Here the scaling factor âˆš1M called the Xavier initialization (Glorot and Bengio [22]) ensures the
same input variance and the same gradient variance for all layers; the activation function ReLU :

> >
R â†’ R, defined as ReLU(u) = 1{u > 0} Â· u; b={bm }mâˆˆ[M ] and W = [W ]>
âˆˆ
1 , . . . , [W ]M
M Ã—dx
R
in (4.3) are parameters of the neural network.
Taking advantage of the homogeneity of ReLU (i.e., ReLU(c Â· u) = c Â· ReLU(u) for all c > 0
and u âˆˆ R), we adopt the usual trick (Cai et al. [6], Wang et al. [53], Allen-Zhu et al. [3])
to fix b throughout the training and only to update W in the sequel. Consequently, denote
fe(x; W, b) as f (x; W ) when bm = 1 is fixed. [W ]m is initialized according to a multivariate
normal distribution N (0, Idx /dx ), where Idx is the identity matrix of size dx .
Neural Policy. For each s âˆˆ S, denote the tuple Î¶s = (Âµ(s), h(s)) âˆˆ RdÎ¶s for notational
simplicity, where dÎ¶s := 1 + |A| is the dimension of Î¶s . Given the input Î¶s = (Âµ(s), h(s)) and

11

parameter W = Î¸s in the two-layer neural network f (Â·; Î¸s ) in (4.3), the team-decentralized
policy Î Î¸ss , called the actor, is parameterized in the form of an energy-based policy ,
exp[Ï„ Â· f ((Âµ(s), h(s)); Î¸s )]
,
exp [Ï„ Â· f ((Âµ(s), h0 (s)); Î¸s )]

Î Î¸ss (h(s) | Âµ(s)) = P

(4.4)

h0 (s)âˆˆP N Â·Âµ(s) (A)

where Ï„ is the temperature parameter and f is the energy function.
To study the policy gradient for (4.4), let us first define a class of feature mappings that is
consistent with the representation of two-layer neural networks. This connection between the
gradient of a two-layer ReLU neural network and the feature mapping defined in (4.6) is crucial
in the convergence analysis of Theorems 5.4 and 5.10. Specifically, rewrite the two-layer neural
network in (4.3) as
M
M

1 X
1 X  >
>
f (Î¶s ; Î¸s ) = âˆš
ReLU Î¶s [Î¸s ]m = âˆš
1 Î¶s [Î¸s ]m > 0 Â· Î¶s> [Î¸s ]m . := Ï†Î¸s (Î¶s )> Î¸s .
M m=1
M m=1
(4.5)

>
>
>
dÎ¶s
M Ã—dÎ¶s
Then the feature mapping Ï†Î¸s = [Ï†Î¸s ]1 , . . . , [Ï†Î¸s ]M
:R
â†’R
may take the following form:

1
(4.6)
[Ï†Î¸s ]m (Î¶s ) = âˆš Â· 1 Î¶s> [Î¸s ]m > 0 Â· Î¶s .
M

That is, the two-layer neural network f (Î¶s ; Î¸s ) may be viewed as the inner product between
the feature Ï†Î¸s (Î¶s ), and the neural network parameters Î¸s . Since f (Î¶s ; Î¸s ) is almost everywhere
differentiable with respect to Î¸s , we see âˆ‡Î¸s f (Î¶s ; Î¸s ) = Ï†Î¸s (Î¶s ).
Furthermore, define a â€œcenteredâ€ version of the feature Ï†Î¸s such that
Î¦(Î¸, s, Âµ, h) := Ï†Î¸s (Âµ(s), h(s)) âˆ’ Eh(s)0 âˆ¼Î Î¸ss (Â·|Âµ(s)) [Ï†Î¸s (Âµ(s), h0 (s))] .

(4.7)

Note that when policy Î Î¸ takes the energy-based form (4.4), Î¦ = Ï„1 âˆ‡Î¸ log Î Î¸ . Therefore,
Lemma 4.2 For any Î¸ âˆˆ Î˜, s âˆˆ S, Âµ âˆˆ P N (S) and h âˆˆ HN (Âµ), kÎ¦(Î¸, s, Âµ, h)k2 â‰¤ 2, and
h Î¸
i
Ï„
âˆ‡Î¸s J (Î¸) =
Â· EÏƒÎ¸ QÎ  (Âµ, h) Â· Î¦(Î¸, s, Âµ, h) .
(4.8)
1âˆ’Î³
Moreover, for each s âˆˆ S, define the following localized policy gradient
ï£®"
ï£¹
#
X
Î¸
Ï„
k
k
bÎ 
ï£»
EÏƒ ï£°
Q
gs (Î¸) =
y (Âµ(Ny ), h(Ny ) Â· Î¦(Î¸, s, Âµ, h) ,
1âˆ’Î³ Î¸
k

(4.9)

yâˆˆNs

Î¸

bÎ 
with Q
s in (Local Q-function) satisfying the (c, Ï)-exponential decay property, then there exists
a universal constant c0 > 0 such that
kgs (Î¸) âˆ’ âˆ‡Î¸s J(Î¸)k â‰¤

c0 Ï„ |S| k+1
Ï
.
1âˆ’Î³

(4.10)

Î¸
bÎ 
Neural Q-function. Note Q
in (Local Q-function) is unknown a priori. To obtain the
s
Î¸
bÎ 
localized policy gradient (4.9), the neural network (4.3) to parameterize Q
s is taken as:

Qs (Âµ(Nsk ), h(Nsk ); Ï‰s ) = f ((Âµ(Nsk ), h(Nsk )); Ï‰s ).
This Qs is called the critic. For simplicity, denote Î¶sk = (Âµ(Nsk ), h(Nsk )), with dÎ¶sk the dimension
of Î¶sk .

12

4.3

Actor-Critic

Î¸
bÎ 
Critic Update. For a fixed policy Î Î¸ , it is to estimate Q
s of (Local Q-function) by a twoÎ¸
bÎ 
layer neural network Qs ( Â· ; Ï‰s ), where Q
s serves as an approximation to the team-decentralized
Î¸
Q-function QÎ 
s .
Î¸
Î Î¸
bÎ 
To design the update rule for Q
ins , note that the Bellman equation (3.13) is for Qs
Î¸
Î¸
Î¸
Î 
Î 
Î 
b
b
stead of Qs . Indeed, Qs takes (Âµ, h) as the input while Qs takes the partial information
(Âµ(Nsk ), h(Nsk )) as the input.
In order to update parameter Ï‰s , we substitute (Âµ(Nsk ), h(Nsk )) for the state-action pair in
the Bellman equation (3.13). It is therefore necessary to study the error of using (Âµ(Nsk ), h(Nsk ))
as the input. Specifically, given a tuple (Âµt , ht , rs (Âµt (Ns ), ht (s)), Âµt+1 , ht+1 ) sampled from the
stationary distribution Î½Î¸ of adopting policy Î Î¸ , the parameter Ï‰s will be updated to minimize
the error:

2
(Î´s,t )2 = Qs (Âµt (Nsk ), ht (Nsk ); Ï‰s ) âˆ’ rs (Âµt (Ns ), ht (s)) âˆ’ Î³ Â· Qs (Âµt+1 (Nsk ), ht+1 (Nsk ); Ï‰s ) .

Estimating Î´s,t depends only on Âµt (Nsk ), ht (Nsk ) and can be collected locally. (See Theorem
5.4).
The neural critic update takes the iterative forms of
Ï‰s (t + 1/2) â† Ï‰s (t) âˆ’ Î·critic Â· Î´s,t Â· âˆ‡Ï‰s Qs (Âµt (Nsk ), ht (Nsk ); Ï‰s ),
Ï‰s (t + 1) â† arg min kÏ‰ âˆ’ Ï‰s (t + 1/2)k2 ,

(4.11)
(4.12)

Ï‰âˆˆBscritic

Ï‰Ì„s â† (t + 1)/(t + 2) Â· Ï‰Ì„s + 1/(t + 2) Â· Ï‰s (t + 1),

(4.13)

in which Î·critic is the learning rate. Here (4.11) is the stochastic semigradient step, (4.12) is
âˆš

M Ã—dÎ¶ k
s : kÏ‰ âˆ’ Ï‰ (0)k
for
a projection to the parameter space Bscritic := Ï‰s âˆˆ R
s
s
âˆž â‰¤ R/ M
some R > 0, and (4.13) is the averaging step. This critic update is summarized in Algorithm 1.
Algorithm 1 Localized-Training-Decentralized-Execution Neural Temporal Difference
1: Input: Width of the neural network M , radius of the constraint set R, number of iterations
Tcritic , policy Î Î¸ , learning rate Î·critic , localization parameter k.
2: Initialize:
For

 all m âˆˆ [M ] and s âˆˆ S, sample bm âˆ¼ Unif({âˆ’1, 1}), [Ï‰s (0)]m âˆ¼
N 0, IdÎ¶k /dÎ¶sk , Ï‰Ì„s = Ï‰s (0).
s
3: for t = 0 to Tcritic âˆ’ 2 do
4:
Sample (Âµt , ht , {rs (Âµt (Ns ), ht (s))}sâˆˆS , Âµt 0 , ht 0 ) from the stationary distribution Î½Î¸ of Î Î¸ .
5:
6:
7:
8:
9:
10:

for s âˆˆ S do
k
k 0
Denote Î¶s,t
= (Âµt (Nsk ), ht (Nsk )), Î¶s,t
= (Âµt 0 (Nsk ), ht 0 (Nsk )).
k
k 0
Residual calculation: Î´s,t â† Qs (Î¶s,t
; Ï‰s (t)) âˆ’ rs (Âµt (Ns ), ht (s)) âˆ’ Î³ Â· Qs (Î¶s,t
; Ï‰s (t)).
Temporal difference update:
k
Ï‰s (t + 1/2) â† Ï‰s (t) âˆ’ Î·critic Â· Î´s,t Â· âˆ‡Ï‰s Qs (Î¶s,t
; Ï‰s (t)).
Projection onto the parameter space: Ï‰s (t + 1) â† arg min kÏ‰ âˆ’ Ï‰s (t + 1/2)k2 .
Ï‰âˆˆBscritic
1
t+1
Averaging the output: Ï‰Ì„s â† t+2 Â· Ï‰Ì„s + t+2 Â· Ï‰s (t + 1).

11:
12:
end for
13: end for

14: Output: Qs ( Â· ; Ï‰Ì„s ), âˆ€s âˆˆ S.

Actor Update. At the iteration step t, a neural network estimation Qs ( Â· ; Ï‰Ì„s ) is given for
Î¸(t)
bÎ 
the localized Q-function Q
under the current policy Î Î¸(t) . Let {(Âµl , hl )}lâˆˆ[B] be samples
s
b s, Âµl , hl ) of
from the state-action visitation measure ÏƒÎ¸(t) of (4.2), and define an estimator Î¦(Î¸,
Î¦(Î¸, s, Âµl , hl ) in (4.7):
13

b s, Âµl , hl ) = Ï†Î¸ (Âµl (s), hl (s)) âˆ’ E Î¸s [Ï†Î¸ (Âµl (s), h0 (s))] .
Î¦(Î¸,
s
s
Î s
By Lemma 4.2, one can compute the following estimator of gs (Î¸(t)) defined in (4.9),
ï£®"
ï£¹
#
X
X

Ï„
b
ï£°
gbs (Î¸(t)) =
Qy Âµl (Nyk ), hl (Nyk ); Ï‰Ì„y
Â· Î¦(Î¸(t),
s, Âµl , hl )ï£» .
(1 âˆ’ Î³)B
k
lâˆˆ[B]

(4.14)

yâˆˆNs

b can be
This estimators gbs in (4.14) only depends locally on {(Âµl , hl )}lâˆˆ[B] . Hence gb and Î¦
computed in a localized fashion after the samples are collected. Similar to the critic update,
Î¸s (t) is updatedby performing a gradient step withâˆšgbs , and then projected onto the parameter
space Bsactor := Î¸s âˆˆ RM Ã—dÎ¶s : kÎ¸s âˆ’ Î¸s (0)kâˆž â‰¤ R/ M .
This actor update is summarized in Algorithm 2.
Sampling from Î½Î¸ and the Visitation Measure ÏƒÎ¸ . In Algorithms 1 and 2, it is assumed that one can sample independently from the stationary distribution Î½Î¸ and the visitation
measure ÏƒÎ¸ , respectively. Such an assumption of sampling from Î½Î¸ can be relaxed by either
sampling from a rapidly-mixing Markov chain mixing, with weakly-dependent sequence of samples (Bhandari et al. [4]), or by randomly picking samples from replay buffers consisting of long
trajectories, with reduced correlation between samples.
To sample from the visitation measure ÏƒÎ¸ and computing the unbiased policy gradient estimator, Konda and Tsitsiklis [33] suggests introducing a new MDP such that the next state
is sampled from the transition probability with probability Î³, and from the initial distribution
with probability 1 âˆ’ Î³. Then the stationary distribution of this new MDP is exactly the visitation measure. Alternatively, Liu et al. [39] proposes an importance-sampling-based algorithm
which enables off-policy evaluation with low variance.
Algorithm 2 Localized-Training-Decentralized-Execution Neural Actor-Critic
1: Input: Width of the neural network M , radius of the constraint set R, number of iterations

Tactor and Tcritic , learning rate Î·actor and Î·critic , temperature parameter Ï„ , batch size B,
localization parameter k.
2: Initialize: For
 all m âˆˆ [M ] and s âˆˆ S, sample bm âˆ¼ Unif({âˆ’1, 1}), [Î¸s (0)]m âˆ¼
N 0, IdÎ¶s /dÎ¶s .
3: for t = 1 to Tactor do
4:
Define the policy
Y
Y
exp[Ï„ Â· f ((Âµ(s), h(s)); Î¸s )]
P
Î Î¸ (h | Âµ) :=
Î Î¸ss (h(s) | Âµ(s)) =
.
0
h0 (s)âˆˆHN exp [Ï„ Â· f ((Âµ(s), h (s)); Î¸s )]
sâˆˆS

5:

6:
7:
8:
9:
10:

sâˆˆS

Output Qs ( Â· ; Ï‰Ì„s ) using Algorithm 1 with the inputs: policy Î Î¸ , width of the neural
network M , radius of the constraint set R, number of iterations Tcritic , learning rate
Î·critic and localization parameter k.
Sample {Âµl , hl }lâˆˆ[B] from the state-action visitation measure ÏƒÎ¸ (4.2) of Î Î¸ .
for s âˆˆ S do
Compute the local gradient estimator gbs (Î¸(t)) using (4.14).
Policy update: Î¸s (t + 1/2) â† Î¸s (t) + Î·actor Â· gbs (Î¸(t))
Projection onto the parameter space: Î¸s (t + 1) â† arg min kÎ¸ âˆ’ Î¸s (t + 1/2)k2 .
Î¸âˆˆBsactor

11:
end for
12: end for
13: Output: {Î Î¸(t) }tâˆˆ[Tactor ] .

5

Convergence of the Critic and Actor Updates

We now establish the global convergence for LTDE-Neural-AC proposed in Section 4.
14

Convergence of the Critic Update. The convergence of the decentralized neural critic
update in Algorithm 1 relies on the following assumptions.
Assumption 5.1 (Action-Value Function Class)For each s âˆˆ S, k âˆˆ N, define


Z

s,k
FR,âˆž
= f (Î¶sk ) = Qs (Î¶sk ; Ï‰s (0)) + 1 v > Î¶sk > 0 Â· (Î¶sk )> Î¹(v) dÂµ(v) : kÎ¹(v)kâˆž â‰¤ R ,
d

with Âµ : R Î¶sk â†’ R the density function of Gaussian distribution N (0, IdÎ¶k /dÎ¶sk ) and Qs (Î¶sk ; Ï‰s (0))
s
Î¸
s,k
bÎ 
the two-layer neural network under the initial parameter Ï‰s (0). We assume that Q
.
s âˆˆF
R,âˆž

Assumption 5.2 (Regularity of Î½Î¸ and ÏƒÎ¸ ) There exists a universal constant c0 > 0 such that
for any policy Î Î¸ , any Î± â‰¥ 0, and any v âˆˆ RdÎ¶ with kvk2 = 1, the stationary distribution Î½Î¸
and the state visitation measure ÏƒÎ¸ satisfy


PÎ¶âˆ¼Î½Î¸ v > Î¶ â‰¤ Î± â‰¤ c0 Â· Î±, PÎ¶âˆ¼ÏƒÎ¸ v > Î¶ â‰¤ Î± â‰¤ c0 Â· Î±.
Remark 5.3 Both Assumption 5.1 and Assumption 5.2 are similar to the standard assumptions
in the analysis of single-agent neural actor-critic algorithms (Cai et al. [6], Liu et al. [38], Wang
et al. [53], Cayci et al. [13]).
Î¸
bÎ 
In particular, Assumption 5.1 is a regularity condition for Q
s in (Local Q-function). Here
s,k
FR,âˆž is a subset of the reproducing kernel Hilbert space (RKHS) induced by the random feature

1 v > Î¶sk > 0 Â· (Î¶sk ) with v âˆ¼ N (0, IdÎ¶k /dÎ¶sk ) up to the shift of Qs (Î¶sk ; Ï‰s (0)) (Rahimi and Recht
s
[46]). This RKHS is dense in the space of continuous functions on any compact set (Micchelli
s,k
et al. [41], Ji et al. [30]). (See also Section D.1.1 for details of the connection between FR,âˆž
and the linearizations of two-layer neural networks (D.4)).
Assumption 5.2 holds when ÏƒÎ¸ and Î½Î¸ have uniformly upper bounded probability densities
(Cai et al. [6]).
Theorem 5.4 (Convergence of Critic Update) Assume Assumptions 5.1 and 5.2. Set Tcritic =
â„¦(M ) and Î·critic = min{(1 âˆ’ Î³)/8, (Tcritic )âˆ’1/2 } in Algorithm 1. Then Qs ( Â· ; Ï‰Ì„s ) generated by
Algorithm 1 satisfies
ï£«
ï£¶
3/2
5/4


k+1
2
R3 dÎ¶ k
R5/2 dÎ¶ k
2
Î¸
Î³
r
s
s
Einit Qs ( Â· ; Ï‰Ì„s ) âˆ’ QÎ 
â‰¤ Oï£­
(5.1)
+
+ max 2 ï£¸ ,
s (Â·)
(1 âˆ’ Î³)
L2 (Î½Î¸ )
M 1/2
M 1/4
where k f kL2 (Î½Î¸ ) := EÎ¶âˆ¼Î½Î¸ [f (Î¶)2 ]
random initialization.

1/2

, and the expectation (5.1) is taken with respect to the

Theorem 5.4 indicates the trade-off between the approximation-optimization error and the
localization error. The first two terms in (5.1) correspond to the neural network approximationoptimization error, similar to the single-agent case (Cai et al. [6], Cayci et al. [13]). This
approximation-optimization error decreases when the width of the hidden layer M increases.
Meanwhile, the last term in (5.1) represents the additional error from using the localized information in (4.11), unique for the mean-field MARL case. This localization error and Î³ k decrease
as the number of truncated neighborhood k increases, with more information from a larger
neighborhood used in the update. However, the input dimension dÎ¶sk and the approximationoptimization error will increase if the dimension of the problem increases.
In particular, for a relatively sparse network on S, one can choose k  |S| hence dÎ¶sk  dÎ¶ ,
and Theorem 5.4 indicates the superior performance of the localized training scheme in efficiency
over directly approximating the centralized Q-function.
Proof of Theorem 5.4 is presented in Section D.1.

15

Convergence of the Actor Update. This section establishes the global convergence of
the actor update. The convergence analysis consists of two steps. The first step proves the
e the second step controls the gap between the stationary
convergence to a stationary point Î¸;
point Î¸e and the optimality Î¸âˆ— in the overparametrization regime. The convergence is built under
the following assumptions and definition.
Assumption 5.5 (Variance Upper Bound) For every t âˆˆ [Tactor ] and s âˆˆ S, denote Î¾s (t) =
gbs(Î¸(t)) âˆ’ E [b
gs (Î¸(t))] with gbs (Î¸(t)) defined in (4.14). Assume there exists Î£ > 0 such that
E kÎ¾s (t)k22 â‰¤ Ï„ 2 Î£2 /B. Here the expectations are taken over ÏƒÎ¸(t) given {Ï‰Ì„s }sâˆˆS .
Assumption 5.6 (Regularity Condition on ÏƒÎ¸ and Î½Î¸ ) There exists an absolute constant D > 0
such that for every Î Î¸ , the stationary distribution Î½Î¸ and the state-action visitation measure ÏƒÎ¸
satisfy
n
h
io
2
EÎ½Î¸ (dÏƒÎ¸ /dÎ½Î¸ (Âµ, h))
â‰¤ D2 ,
where dÏƒÎ¸ /dÎ½Î¸ is the Radon-Nikodym derivative of ÏƒÎ¸ with respect to Î½Î¸ .
Assumption 5.7 (Lipschitz Continuous Policy Gradient) There exists an absolute constant
L > 0, such that âˆ‡Î¸ J(Î¸) is L-Lipschitz continuous with respect to Î¸, i.e., for all Î¸1 , Î¸2 ,
kâˆ‡Î¸ J(Î¸1 ) âˆ’ âˆ‡Î¸ J(Î¸2 )k2 â‰¤ L Â· kÎ¸1 âˆ’ Î¸2 k2 .
Definition 5.8 Î¸e âˆˆ B actor is called a stationary point of J(Î¸) if for all Î¸e âˆˆ B actor ,
e > (Î¸ âˆ’ Î¸)
e â‰¤ 0.
âˆ‡Î¸ J(Î¸)

(5.2)

Assumption 5.9 (Policy Function Class) Define a function class
(
"
#
)
Z
X

FR,âˆž = f (Î¶) =
Ï†Î¸s (0) (Î¶s )> Î¸s (0) + 1 v > Î¶s > 0 Â· (Î¶s )> Î¹(v) dÂµ(v) : kÎ¹(v)kâˆž â‰¤ R
sâˆˆS


where Âµ : RdÎ¶s â†’ R is the density function of the Gaussian distribution N 0, IdÎ¶s /dÎ¶s and Î¸(0)
e define the function
is the initial parameter. For any stationary point Î¸,
uÎ¸e(Âµ, h) :=

X
dÏƒÌ„Î¸âˆ—
dÏƒÎ¸âˆ—
(Î¶) âˆ’
(Âµ) +
Ï†Î¸es (Î¶s )> Î¸es ,
dÏƒÎ¸e
dÏƒÌ„Î¸e
sâˆˆS

Î¸ âˆ— dÏƒÌ„Î¸ âˆ—
with ÏƒÌ„Î¸ the state visitation measure under policy Î Î¸ , and dÏƒ
dÏƒÎ¸e , dÏƒÌ„Î¸e the Radon-Nikodym derivatives between corresponding measures. We assume that uÎ¸e âˆˆ FR,âˆž for any stationary point
e
Î¸.

A few remarks are in place for these Assumption 5.5 - Assumption 5.9.
Remark. All these assumptions are counterparts of standard assumption in the analysis of
single-agent policy gradient method (Pirotta et al. [43], Xu et al. [54], Xu et al. [55], Zhang
et al. [59], Wang et al. [53]).
In particular, Assumption 5.5 and Assumption 5.6 hold if the Markov chain (3.5) mixes
sufficiently fast, and the critic Qs ( Â· ; Ï‰s ) has an upper-bounded second moment under ÏƒÎ¸(t)
(Wang et al. [53]). Note that different from Assumption 5.2, where regularity conditions are
imposed separately on Î½Î¸ and ÏƒÎ¸ , Assumption 5.6 imposes the regularity condition directly on
the Radon-Nikodym derivative of ÏƒÎ¸ with respect to Î½Î¸ . This allows the change of measures in
the analysis of Theorem 5.10. In general, Assumption 5.2 does not necessarily imply Assumption
5.6.
Assumption 5.7 holds when the transition probability and the reward function are both
Lipschitz continuous with respect to their inputs (Pirotta et al. [43]), or when the reward is
16

uniformly bounded and the score function âˆ‡Î¸ Î Î¸ is uniformly bounded and Lipschitz continuous
with respect to Î¸ (Zhang et al. [59]).
As for Assumption 5.9, we first emphasize that uÎ¸e(Âµ, h) is a key element in the proof of
Theorem 5.10. More specifically, this assumption is motivated by the well-known Performance
Difference Lemma (Kakade and Langford [32]) in order to characterize the optimality gap of a
e In particular, it guarantees that u e can be decomposed into a sum of local
stationary point Î¸.
Î¸
functions depending on Î¶s , and that each local function lies in a rich RKHS (see the discussion
after Assumption 5.1).
With all these assumptions, we now establish the rate of convergence for Algorithm 2.
Theorem 5.10 Assume Assumptions 5.1 - 5.9 . Set Tcritic = â„¦(M ), Î·critic = min{(1
âˆ’

Î³)/8, (Tcritic )âˆ’1/2 }, Î·actor = (Tactor )âˆ’1/2 , R = Ï„ = 1, M = â„¦ (f (k)|A|)5 (Tactor )8 , Î³ â‰¤
(Tactor )âˆ’2/k , with f (k) := maxsâˆˆS |Nsk | the size of the largest k-neighborhood in the graph (S, E).
Then, the output {Î¸(t)}tâˆˆ[Tactor ] of Algorithm 2 satisfies



min E [J(Î¸âˆ— ) âˆ’ J(Î¸(t))] â‰¤ O |S|1/2 B âˆ’1/2 + |S||A|1/4 Î³ k/8 + (Tactor )âˆ’1/4 .
(5.3)
tâˆˆ[Tactor ]

The error O(Î³ k/8 |S||A|1/4 ) in Theorem 5.10, coming from the localized training, decays
exponentially fast as k increases and is negligible with a careful choice of k. According to
âˆ’1/4
Theorem 5.10, Algorithm 2 converges at rate Tactor with sufficiently large width M and batch
size B. Technically, {Î¸s (t)}sâˆˆS in Algorithm 2 are updated in parallel and our analysis extends
the single agent actor-critic in Cai et al. [6] to the multi-agent decentralized case.
Detailed proof is provided in Section D.2.

References
[1] Agarwal A, Kakade SM, Lee JD, Mahajan G (2021) On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. Journal of Machine Learning Research 22(98):1â€“
76.
[2] AÃ¯d R, Dumitrescu R, Tankov P (2021) The entry and exit game in the electricity markets: a
mean-field game approach. Journal of Dynamics & Games 8(4):331.
[3] Allen-Zhu Z, Li Y, Song Z (2019) A convergence theory for deep learning via over-parameterization.
International Conference on Machine Learning, 242â€“252 (PMLR).
[4] Bhandari J, Russo D, Singal R (2018) A finite time analysis of temporal difference learning with
linear function approximation. Conference on Learning Theory, 1691â€“1692 (PMLR).
[5] Cabannes T, Lauriere M, Perolat J, Marinier R, Girgin S, Perrin S, Pietquin O, Bayen AM,
Goubault E, Elie R (2021) Solving n-player dynamic routing games with congestion: a mean-field
approach. arXiv preprint arXiv:2110.11943 .
[6] Cai Q, Yang Z, Lee JD, Wang Z (2019) Neural temporal-difference learning converges to global
optima. Advances in Neural Information Processing Systems, volume 32, 11315â€“11326.
[7] Calderone D, Sastry SS (2017) Markov decision process routing games. International Conference
on Cyber-Physical Systems, 273â€“280 (IEEE).
[8] Cao Y, Yu W, Ren W, Chen G (2012) An overview of recent progress in the study of distributed
multi-agent coordination. IEEE Transactions on Industrial informatics 9(1):427â€“438.
[9] Carmona R, Fouque JP, Sun LH (2015) Mean-field games and systemic risk. Communications in
Mathematical Sciences 13(4):911â€“933.
[10] Carmona R, LauriÃ¨re M, Tan Z (2019) Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods. arXiv preprint arXiv:1910.04295 .
[11] Carmona R, LauriÃ¨re M, Tan Z (2019) Model-free mean-field reinforcement learning: mean-field
MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802 .
[12] Casgrain P, Jaimungal S (2020) Mean-field games with differing beliefs for algorithmic trading.
Mathematical Finance 30(3):995â€“1034.

17

[13] Cayci S, Satpathi S, He N, Srikant R (2021) Sample complexity and overparameterization bounds
for projection-free neural TD learning. arXiv preprint arXiv:2103.01391 .
[14] Chen T, Zhang K, Giannakis GB, Basar T (2021) Communication-efficient policy gradient methods
for distributed reinforcement learning. IEEE Transactions on Control of Network Systems .
[15] Dawson D (1993) Measure-valued Markov processes. Ã‰cole dâ€™Ã©tÃ© de probabilitÃ©s de Saint-Flour
XXI-1991, 1â€“260 (Springer).
[16] El-Tantawy S, Abdulhai B, Abdelgawad H (2013) Multi-agent reinforcement learning for integrated network of adaptive traffic signal controllers (MARLIN-ATSC): Methodology and largescale application on downtown Toronto. IEEE Transactions on Intelligent Transportation Systems
14(3):1140â€“1150.
[17] Foerster J, Farquhar G, Afouras T, Nardelli N, Whiteson S (2018) Counterfactual multi-agent
policy gradients. AAAI Conference on Artificial Intelligence, volume 32.
[18] Fu Z, Yang Z, Wang Z (2020) Single-timescale actor-critic provably finds globally optimal policy.
International Conference on Learning Representations.
[19] Gamarnik D (2013) Correlation decay method for decision, optimization, and inference in largescale networks. Theory Driven by Influential Applications, 108â€“121 (INFORMS).
[20] Gamarnik D, Goldberg DA, Weber T (2014) Correlation decay in random decision networks.
Mathematics of Operations Research 39(2):229â€“261.
[21] Germain M, Pham H, Warin X (2021) A level-set approach to the control of state-constrained
mckean-vlasov equations: application to renewable energy storage and portfolio selection. arXiv
preprint arXiv:2112.11059 .
[22] Glorot X, Bengio Y (2010) Understanding the difficulty of training deep feedforward neural networks. International Conference on Artificial Intelligence and Statistics, 249â€“256.
[23] Gu H, Guo X, Wei X, Xu R (2019) Dynamic programming principles for learning MFCs. arXiv
preprint arXiv:1911.07314 .
[24] Gu H, Guo X, Wei X, Xu R (2021) Mean-field controls with Q-learning for cooperative MARL:
convergence and complexity analysis. SIAM Journal on Mathematics of Data Science 3(4):1168â€“
1196.
[25] GuÃ©riau M, Dusparic I (2018) Samod: Shared autonomous mobility-on-demand using decentralized
reinforcement learning. International Conference on Intelligent Transportation Systems, 1558â€“1563
(IEEE).
[26] Guo X, Hu A, Xu R, Zhang J (2019) Learning mean-field games. Advances in Neural Information
Processing Systems, volume 32, 4966â€“4976.
[27] Hu R, Zariphopoulou T (2021) N-player and mean-field games in ItÃ´-diffusion markets with competitive or homophilous interaction. arXiv preprint arXiv:2106.00581 .
[28] HÃ¼ttenrauch M, Å oÅ¡iÄ‡ A, Neumann G (2017) Guided deep reinforcement learning for swarm systems. arXiv preprint arXiv:1709.06011 .
[29] Iyer K, Johari R, Sundararajan M (2014) Mean-field equilibria of dynamic auctions with learning.
Management Science 60(12):2949â€“2970.
[30] Ji Z, Telgarsky M, Xian R (2020) Neural tangent kernels, transportation mappings, and universal
approximation. International Conference on Learning Representations.
[31] Jin J, Song C, Li H, Gai K, Wang J, Zhang W (2018) Real-time bidding with multi-agent reinforcement learning in display advertising. ACM International Conference on Information and
Knowledge Management, 2193â€“2201.
[32] Kakade S, Langford J (2002) Approximately optimal approximate reinforcement learning.
International Conference on Machine Learning, 267â€“274 (PMLR).
[33] Konda VR, Tsitsiklis JN (2000) Actor-critic algorithms. Advances in Neural Information Processing
Systems, volume 12, 1008â€“1014.
[34] Lacker D, Zariphopoulou T (2019) Mean-field and n-agent games for optimal investment under
relative performance criteria. Mathematical Finance 29(4):1003â€“1038.
[35] Li M, Qin Z, Jiao Y, Yang Y, Wang J, Wang C, Wu G, Ye J (2019) Efficient ridesharing order
dispatching with mean-field multi-agent reinforcement learning. The World Wide Web Conference,
983â€“994.

18

[36] Li Y, Tang Y, Zhang R, Li N (2021) Distributed reinforcement learning for decentralized linear quadratic control: A derivative-free policy optimization approach. IEEE Transactions on
Automatic Control .
[37] Lin Y, Qu G, Huang L, Wierman A (2021) Multi-agent reinforcement learning in stochastic networked systems. Advances in Neural Information Processing Systems, volume 34.
[38] Liu B, Cai Q, Yang Z, Wang Z (2019) Neural trust region/proximal policy optimization attains
globally optimal policy. Advances in Neural Information Processing Systems, volume 32, 10565â€“
10576.
[39] Liu Y, Swaminathan A, Agarwal A, Brunskill E (2019) Off-policy policy gradient with stationary
distribution correction. Conference on Uncertainty in Artificial Intelligence, volume 115, 1180â€“1190
(PMLR).
[40] Lowe R, Wu YI, Tamar A, Harb J, Pieter Abbeel O, Mordatch I (2017) Multi-agent actor-critic for
mixed cooperative-competitive environments. Advances in Neural Information Processing Systems,
volume 30, 6382â€“6393.
[41] Micchelli CA, Xu Y, Zhang H (2006) Universal kernels. Journal of Machine Learning Research
7(12):2651â€“2667.
[42] Motte M, Pham H (2019) Mean-field markov decision processes with common noise and open-loop
controls. arXiv preprint arXiv:1912.07883 .
[43] Pirotta M, Restelli M, Bascetta L (2015) Policy gradient in lipschitz Markov decision processes.
Machine Learning 100(2):255â€“283.
[44] Qu G, Wierman A, Li N (2020) Scalable reinforcement learning of localized policies for multi-agent
networked systems. Learning for Dynamics and Control, 256â€“266 (PMLR).
[45] Rabbat M, Nowak R (2004) Distributed optimization in sensor networks. International Symposium
on Information Processing in Sensor Networks, 20â€“27.
[46] Rahimi A, Recht B (2008) Uniform approximation of functions with random bases. Annual Allerton
Conference on Communication, Control, and Computing, 555â€“561 (IEEE).
[47] Rashid T, Samvelyan M, Schroeder C, Farquhar G, Foerster J, Whiteson S (2018) QMIX:
Monotonic value function factorisation for deep multi-agent reinforcement learning. International
Conference on Machine Learning, 4295â€“4304 (PMLR).
[48] Shalev-Shwartz S, Shammah S, Shashua A (2016) Safe, multi-agent, reinforcement learning for
autonomous driving. arXiv preprint arXiv:1610.03295 .
[49] Sra S, Nowozin S, Wright SJ (2012) Optimization for Machine Learning (MIT Press).
[50] Sunehag P, Lever G, Gruslys A, Czarnecki WM, Zambaldi V, Jaderberg M, Lanctot M, Sonnerat
N, Leibo JZ, Tuyls K, Thore G (2018) Value-decomposition networks for cooperative multi-agent
learning based on team reward. International Conference on Autonomous Agents and Multi-agent
Systems, volume 3, 2085â€“2087.
[51] Sutton RS, McAllester DA, Singh SP, Mansour Y (2000) Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems,
volume 99, 1057â€“1063.
[52] Vadori N, Ganesh S, Reddy P, Veloso M (2020) Calibration of shared equilibria in general sum partially observable markov games. Advances in Neural Information Processing Systems, volume 33,
14118â€“14128.
[53] Wang L, Cai Q, Yang Z, Wang Z (2020) Neural policy gradient methods: Global optimality and
rates of convergence. International Conference on Learning Representations.
[54] Xu P, Gao F, Gu Q (2019) Sample efficient policy gradient methods with recursive variance reduction. International Conference on Learning Representations.
[55] Xu P, Gao F, Gu Q (2020) An improved convergence analysis of stochastic variance-reduced policy
gradient. Uncertainty in Artificial Intelligence, 541â€“551 (PMLR).
[56] Yang Y, Hao J, Chen G, Tang H, Chen Y, Hu Y, Fan C, Wei Z (2020) Q-value path decomposition
for deep multiagent reinforcement learning. International Conference on Machine Learning, 10706â€“
10715 (PMLR).
[57] Yang Y, Wen Y, Wang J, Chen L, Shao K, Mguni D, Zhang W (2020) Multi-agent determinantal
Q-learning. International Conference on Machine Learning, 10757â€“10766 (PMLR).

19

[58] You X, Li X, Xu Y, Feng H, Zhao J, Yan H (2020) Toward packet routing with fully distributed
multiagent deep reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics:
Systems .
[59] Zhang K, Koppel A, Zhu H, Basar T (2020) Global convergence of policy gradient methods to
(almost) locally optimal policies. SIAM Journal on Control and Optimization 58(6):3586â€“3612.
[60] Zhang K, Liu Y, Liu J, Liu M, BaÅŸar T (2020) Distributed learning of average belief over networks
using sequential observations. Automatica 115:108857.
[61] Zhang K, Yang Z, Basar T (2018) Networked multi-agent reinforcement learning in continuous
spaces. Conference on Decision and Control, 2771â€“2776 (IEEE).
[62] Zhang K, Yang Z, BaÅŸar T (2021) Multi-agent reinforcement learning: A selective overview of
theories and algorithms. Handbook of Reinforcement Learning and Control, 321â€“384 (Springer).
[63] Zhang K, Yang Z, Liu H, Zhang T, Basar T (2018) Fully decentralized multi-agent reinforcement learning with networked agents. International Conference on Machine Learning, 5872â€“5881
(PMLR).
[64] Zhang K, Yang Z, Liu H, Zhang T, Basar T (2021) Finite-sample analysis for decentralized
batch multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic
Control .
[65] Zheng S, Trott A, Srinivasa S, Naik N, Gruesbeck M, Parkes DC, Socher R (2020) The AI
economist: Improving equality and productivity with AI-driven tax policies. arXiv preprint
arXiv:2004.13332 .
[66] Zhou Z, Mertikopoulos P, Moustakas AL, Bambos N, Glynn P (2021) Robust power management
via learning and game design. Operations Research 69(1):331â€“345.

20

Appendix
A

Proof of Lemma 3.1

The goal is to show that V (Âµ) = Ve (Âµ), with the former the value function of (MF-MARL)
subject to the transition probability P defined in (2.7) under a given individual policy Ï€ âˆˆ u,
and the latter the value function of (3.7) subject to the joint transition probability PN defined
in (3.5) under the policy Î  âˆˆ U. The proof consists of two steps. Step 1 shows that V (Âµ)
can be reformulated as a measured-valued Markov decision problem. Step 2 shows that the
measured-valued Markov decision problem from Step 1 is equivalent to Ve (Âµ) in (3.7).
PN
Step 1:
Recall that Âµt+1 := N1 i=1 Î´sit+1 with sit+1 subject to (2.7). First, one can
show that Âµt is a measure-valued Markov decision process under Ï€. To see this, denote Fts =
1
N
Ïƒ(s1t , Â· Â· Â· , sN
t ) as the Ïƒ-algebra generated by st , Â· Â· Â· , st . Then it suffices to show
P(Âµt+1 | Ïƒ(Âµt ) âˆ¨ Fts ) = P(Âµt+1 | Ïƒ(Âµt )), P âˆ’ a.s..

(A.1)

Following similar arguments for Lemma 2.3.1 and Proposition 2.3.3 in Dawson [15], (A.1) holds
due to the exchangeability of the individual transition dynamics (2.7) under Ï€. (A.1) implies
e N such
that there exists a joint transition probability induced from (2.7) under Ï€, denoted as P
that
e N (Â· | Âµt , Ï€).
Âµt+1 âˆ¼ P
(A.2)
Meanwhile, rewrite V Ï€ (Âµ) in (MF-MARL) by regrouping the agents according to their states

X
âˆž
N
X
1
r(sit , Âµt (Nsit ), ait ) Âµ0 = Âµ ,
Î³t
:= E
N
t=0
i=1

X
âˆž
X
X
Î³t
= E
Âµt (s)
r(s, Âµt (Ns ), a)Ï€(s, Âµt (s))(a) Âµ0 = Âµ .

V Ï€ (Âµ)

t=0

sâˆˆS

(A.3)

aâˆˆA

We see (2.7)-(MF-MARL) is reformulated in an equivalent form of (A.2)-(A.3).
Step 2: It suffices to show that (A.2) under Ï€ is the same as (3.5) under Î  and that V Ï€
P
in (A.3) equals to Ve Î  in (3.7). To see this, denote hg, Âµi = sâˆˆS g(s)Âµ(s) for any measurable
bounded function g : S â†’ R, then


E hg, Âµt+1 i | Ïƒ(Âµt )
N
i
1 hX  j
= E
E g(st+1 ) | Ïƒ(Âµt ) âˆ¨ Fts
N
i=1
N

=

1 XXX
g(s0 )P (s0 | sit , Âµt (N (sit )), a)Ï€(sit , Âµt (sit ))(a)
N 0 i=1
s âˆˆS

aâˆˆA

N
XX
X
1 X
g(s0 )
1(sit = s)
P (s0 | sit , Âµt (N (sit )), a)Ï€(sit , Âµt (sit ))(a)
N 0
s âˆˆS
sâˆˆS i=1
aâˆˆA
X
X
X
0
0
=
g(s )
Âµt (s)
P (s | s, Âµt (N (s)), a)Ï€(s, Âµt (s))(a)

=

s0 âˆˆS

=

X
s0 âˆˆS

sâˆˆS
0

g(s )

X
sâˆˆS

aâˆˆA

Âµt (s)

X

Î (h | Âµt (s))

X

P (s0 | s, Âµt (N (s)), a)h(s)(a),

(A.4)

aâˆˆA

hâˆˆP N Â·Âµt (s) (A)

where in the last step, the expectation of random variable h(s)(a) with respect to distribution
Î (h | Âµ) is Ï€(s, Âµt (s)). And from the last equality, clearly Âµt+1 evolves according to transition
dynamics PN (Â·|Âµt , ht ) under Î (ht | Âµt ). This implies the equivalence of (A.2) and (3.5). As a
byproduct, when taking g(s0 ) = 1(s0 = so ) for any fixed so âˆˆ S, (A.4) becomes
21



E Âµt+1 (so )|Ïƒ(Âµt ) =

X

X

Âµt (s)

sâˆˆN (so )

Î (h | Âµt (s))

X

P (so | s, Âµt (N (s)), a)h(s)(a),

aâˆˆA

hâˆˆP N Â·Âµt (s) (A)

where the local structure (2.7) is used. This suggests that Âµt+1 (so ) only depends on Âµt (Ns2o )
and ht (Nso ) since N (s) = N 2 (so ) for s âˆˆ N (so ).
Now we show that V Ï€ (Âµ) in (A.3) and Ve Î  (Âµ) in (3.7) are equal. Take Ve Î  defined in (3.7),
X

âˆž X
Î 
t
e
V (Âµ) : = Eht âˆ¼Î (Â· | Âµt ), Âµt+1 âˆ¼PN (Â· | Âµt ,ht )
Î³ rs (Âµt (Ns ), ht ) Âµ0 = Âµ
t=0 sâˆˆS

= EÂµt+1 âˆ¼PN (Â· | Âµt ,ht )

X
âˆž

Î³t

t=0

= EÂµt+1 âˆ¼PN (Â· | Âµt ,ht )

X
âˆž

= EÂµt+1 âˆ¼PN (Â· | Âµt ,ht )

= EÂµt+1 âˆ¼P
e N (Â· | Âµt ,Ï€)




Eht âˆ¼Î (Â· | Âµt ) rs (Âµt (Ns ), ht )|Âµt Âµ0 = Âµ

sâˆˆS

Î³

t

t=0

X
âˆž

X

X


rs (Âµt (Ns ), ht (s))Î (h; Ï€) Âµ0 = Âµ

X

sâˆˆS ht âˆˆP N Â·Âµt (s) (A)

Î³

t

X

X

Âµt (s)

Î (ht | Âµt )

t=0

sâˆˆS

ht âˆˆP N Â·Âµt (s) (A)

X
âˆž

X

X

Î³

t

t=0

Âµt (s)

sâˆˆS

X


r(s, Âµt (Ns ), a)h(a) Âµ0 = Âµ

aâˆˆA


r(s, Âµt (Ns ), a)Ï€t (s, Âµt (s))(a) Âµ0 = Âµ

aâˆˆA

= V Ï€ (Âµ),
e N under Î , and the expectation of
where in the last second step, PN under Ï€ is equivalent to P
ht (s)(a) with distribution Î (ht | Âµt ) is Ï€(s, Âµt (s))(a) such that
hX
i
X
X
Î (ht | Âµt )
r(s, Âµt (Ns ), a)h(a) = Ehâˆ¼Î (Â·|Âµt )
r(s, Âµt (Ns , a)h(a)
hâˆˆP N Â·Âµt (s) (A)

aâˆˆA

aâˆˆA

=

X

r(s, Âµt (Ns ), a)Ï€t (s, Âµt (s))(a).

aâˆˆA
Î¸

Finally, the decomposition of Ve (Âµ) and QÎ  (Âµ, h) according to the states is straightforward.
Q.E.D.

B

Proof of Lemma 3.5

Let Pt,s and P0 t,s be, respectively, distribution of (Âµt (Ns ), ht (s)) and (Âµ0t (Ns ), h0t (s)) under
policy Î Î¸ . By localized transition kernel (2.7), it is easy to see that for any given s âˆˆ S, Âµt+1 (s)
only depends on Âµt (Ns2 ) and ht (Ns ). Then by the local dependency, (3.5) can be rewritten as
2
Âµt+1 (s) âˆ¼ PN
s (Â· | Âµt (Ns ), ht (Ns )).

(B.1)

Due to the local structure of dynamics (B.1) and local dependence of Î Î¸ , the distribution Pt,s ,
t â‰¤ b k2 c only depends on the initial value (Âµ(Nsk ), h(Nsk )). Therefore, Pt,s = P0 t,s , t â‰¤ b k2 c,
Î¸

QÎ 
s
=



Î¸
Âµ(Nsk ), Âµ(Nsâˆ’k ), h(Nsk ), h(Nsâˆ’k ) âˆ’ QÎ 
Âµ(Nsk ), Âµ0 (Nsâˆ’k ), h(Nsk ), h0 (Nsâˆ’k )
s

âˆž
X





E(Âµt (Ns ),ht (s))âˆ¼Pt,s rs (Âµt (Ns ), ht (s)) âˆ’ E(Âµ0t (Ns ),h0t (s))âˆ¼P0t,s rs (Âµ0t (Ns ), h0t (s))

t=b k
2 c+1

â‰¤

âˆž
X
t=b k
2 c+1

Î³ t rmax TV(Pt,s , P0 t,s ) â‰¤

rmax b k c+1
Î³ 2
,
1âˆ’Î³

where TV(Pt,s , P0 t,s ) is total variation between Pt,s and P0 t,s that is upper bounded by 1.
Q.E.D.
22

C

Proof of Lemma 4.2

For any Î¸ âˆˆ Î˜, s âˆˆ S, Âµ âˆˆ P N (S) and h âˆˆ HN (Âµ), it is easy to verify that kÎ¦(Î¸, s, Âµ, h)k2 â‰¤
kÎ¶s k2 â‰¤ 2, by the definitions of the feature mapping Ï† in (4.6) and the center feature mapping
Î¦ in (4.7).
To prove (4.8), note that by Lemma 4.1 & the definition of energy-based policy Î Î¸ss (4.4),
âˆ‡Î¸s log Î Î¸ss (h(s) | Âµ(s))

= Ï„ Â· âˆ‡Î¸s f ((Âµ(s), h(s)); Î¸s ) âˆ’ Ï„ Â· Eh(s)0 âˆ¼Î Î¸s (Â·|Âµ(s)) [âˆ‡Î¸s f (Âµ(s), h0 (s))]
= Ï„ Â· Ï†Î¸s (Âµ(s), h(s)) âˆ’ Ï„ Â· Eh(s)0 âˆ¼Î Î¸s (Â·|Âµ(s)) [Ï†Î¸s (Âµ(s), h(s))]
= Ï„ Â· Î¦(Î¸, s, Âµ, h).

The second equality follows from the fact that âˆ‡Î¸s f ((Âµ(s), h(s)); Î¸s ) = Ï†Î¸s (Âµ(s), h(s)). Therefore,
ï£®
ï£¹
h Î¸
i
X
Î¸
Ï„
Ï„
ï£»
âˆ‡Î¸s J(Î¸) =
EÏƒ QÎ  (Âµ, h) Â· Î¦(Î¸, s, Âµ, h) =
EÏƒ ï£°
QÎ 
y (Âµ, h) Â· Î¦(Î¸, s, Âµ, h) ,
1âˆ’Î³ Î¸
1âˆ’Î³ Î¸
yâˆˆS

where the second equality is by the decomposition of Q-function in Lemma 3.1.
The proof of (4.9) is based on the exponential decay property in Definition 3.4. Notice that
ï£¹
ï£®"
#
X
Î¸
1
k
k
Î¸s
bÎ 
ï£»
Q
EÏƒ ï£°
gs (Î¸) =
y (Âµ(Ny ), h(Ny ) âˆ‡Î¸s log Î  (h(s) | Âµ(s))
1âˆ’Î³ Î¸
yâˆˆNsk
ï£®"
ï£¹
#
X
Î¸
1
k
k
Î¸s
bÎ 
ï£»
=
EÏƒ ï£°
Q
(C.1)
y (Âµ(Ny ), h(Ny ) âˆ‡Î¸s log Î  (h(s) | Âµ(s)) .
1âˆ’Î³ Î¸
yâˆˆS

b Î Î¸ (Âµ(N k ), h(N k ) is independent of s. Consequently,
This is because for all y 6âˆˆ Nsk , Q
y
y
y
ï£®ï£®
ï£¹
ï£¹
X
b Î Î¸ (Âµ(N k ), h(N k )ï£» âˆ‡Î¸ log Î Î¸s (h(s) | Âµ(s))ï£» = 0.
EÏƒÎ¸ ï£°ï£°
Q
y
y
y
s
y6âˆˆNsk

Given Lemma 4.1 and (C.1), we have the following bound:

â‰¤

kgs (Î¸) âˆ’ âˆ‡Î¸s J(Î¸)k2
"
#


1 X
Î Î¸
k
k
Î Î¸
Î¸s
b
sup
Qy Âµ(Ny ), h(Ny ) âˆ’ Qy (Âµ, h) Â· kâˆ‡Î¸s log Î  (h(s) | Âµ(s))k2
1âˆ’Î³
ÂµâˆˆP N (S),
yâˆˆS

â‰¤

hâˆˆHN (Âµ)

c0 Ï„ |S| k+1
Ï
.
1âˆ’Î³

The last inequality follows from (3.14) and k log Î Î¸s (h(s) | Âµ(s))k2 = kÎ¦(Î¸, s, Âµ, h)k2 â‰¤ 2 for
any Âµ âˆˆ P N (S), h âˆˆ HN (Âµ).
Q.E.D.

D

Proof of Theorems 5.4 and 5.10

D.1

Proof of Theorem 5.4: Convergence of Critic Update

This section presents the proof of convergence of the decentralized neural critic update. It
consists of several steps. Section D.1.1 introduces necessary notations and definitions. Section
D.1.2 proves that the critic update minimizes the projected mean-square Bellman error given
a two-layer neural network. Section D.1.3 shows that the global minimizer of the projected
mean-square Bellman error converges to the true team-decentralized Q-function as the width of
hidden layer M â†’ âˆž.
23

D.1.1

Notations

Recall that the set of all state-action (distribution) pairs is denoted as Îž := âˆªÂµâˆˆP N (S) {Î¶ =
(Âµ, h) : h âˆˆ HN (Âµ)}. For any Î¶ = (Âµ, h) âˆˆ Îž, denote the localized state-action (distribution)
pair as Î¶sk = (Âµ(Nsk ), h(Nsk )). Meanwhile, denote Îžks = {Î¶sk : Î¶ âˆˆ Îž} as the set of all possible
localized state-action (distribution) pairs. Without loss of generality, assume kÎ¶sk k2 â‰¤ 1 for any
Î¶sk âˆˆ Îžks .
Let dÎ¶ denote the dimension of the space Îž. Since P N (S) has dimension (|S| âˆ’ 1) and
N
H (Âµ) has dimension |S|(|A| âˆ’ 1) for any Âµ âˆˆ P N (S), the product space Îž has dimension
dÎ¶ = |S||A| âˆ’ 1. Similarly, one can see that the dimension of the space Îžks , denoted by dÎ¶sk , is
at most f (k)|A|, where f (k) := maxsâˆˆX |Nsk | is the size of the largest k-neighborhood in the
graph (S, E).
k
Let RÎž and RÎžs be the sets of real-valued square-integrable functions (with respect to Î½Î¸ )
on Îž and Îžks , respectively. Define the norm k Â· kL2 (Î½Î¸ ) on RÎž by
k f kL2 (Î½Î¸ ) := EÎ¶âˆ¼Î½Î¸ [f (Î¶)2 ]

1/2

,

âˆ€f âˆˆ RÎž .

(D.1)

k
Note that for any function f âˆˆ RÎžs , a function fËœ âˆˆ RÎž is called a natural extension of f if
k
fËœ(Î¶) = f (Î¶sk ) for all Î¶ âˆˆ Îž. Since the natural extension is an injective mapping from RÎžs to
k
k
RÎž , one can view RÎžs as a subset of RÎž . In addition for a function f âˆˆ RÎžs , we use the same
Îž
notation f âˆˆ R to denote the natural extension of f .
For any closed and convex function class F âŠ‚ RÎž , define the project operator ProjF from
Îž
R onto F by
ProjF (g) := arg min kf âˆ’ gkL2 (Î½Î¸ ) .
(D.2)

f âˆˆF

This projection operator ProjF is non-expansive in the sense that
kProjF (f ) âˆ’ ProjF (g)kL2 (Î½Î¸ ) â‰¤ kf âˆ’ gkL2 (Î½Î¸ ) .

(D.3)

Recall that for each state s âˆˆ S, the critic parameter Ï‰s is updated in a localized fashion
using information from the k-hop neighborhood of s. Without loss of generality, let us omit the
subscript s of Ï‰s in the following presentation, and the result holds for all s âˆˆ S simultaneously.
M Ã—dÎ¶ k
s , define the following function class
Given an initialization Ï‰(0) âˆˆ R

FR,M =

1
Q0 (Î¶sk ; Ï‰) := âˆš

M
X

k
> k
1 [Ï‰(0)]>
m Î¶s > 0 Ï‰m Î¶s :

M m=1

Ï‰âˆˆR

M Ã—dÎ¶ k
s


âˆš
, kÏ‰ âˆ’ Ï‰(0)kâˆž â‰¤ R/ M .

(D.4)

Q0 ( Â· ; Ï‰) locally linearizes the neural network Q( Â· ; Ï‰) (with respect to Ï‰) at Ï‰(0). Any function
Q0 ( Â· ; Ï‰) âˆˆ FR,M can be viewed as an inner product between the feature mapping Ï†Ï‰(0) (Â·)
defined in (4.6) and the parameter Ï‰, i.e. Q0 ( Â· ; Ï‰) = Ï†Ï‰(0) (Â·)> Ï‰. In addition it holds that
âˆ‡Ï‰ Q0 ( Â· ; Ï‰) = Ï†Ï‰(0) (Â·). All functions in FR,M share the same feature mapping Ï†Ï‰(0) (Â·) which
only depends on the initialization Ï‰(0).
Recall the Bellman operator TsÎ¸ : RÎž â†’ RÎž defined in (3.13),
h
i
Î¸
Î Î¸
0
0
TsÎ¸ QÎ 
s (Âµ, h) = EÂµ0 âˆ¼PN (Â· | Âµ,h), h0 âˆ¼Î Î¸ (Â· | Âµ) rs (Âµ, h) + Î³ Â· Qs (Âµ , h ) , âˆ€(Âµ, h) âˆˆ Îž.
Î¸

Î¸

The team-decentralized Q-function QÎ 
in (3.10) is the unique fixed point of TsÎ¸ : QÎ 
=
s
s
Î¸ Î Î¸
Ts Qs . Now given a general parameterized function class F, we aim to learn a Qs ( Â· ; Ï‰) âˆˆ F to
Î¸
approximate QÎ 
s by minimizing the following projected mean-squared Bellman error (PMSBE):

2 
min PMSBE(Ï‰) = EÎ¶âˆ¼Î½Î¸ Qs ( Î¶sk ; Ï‰) âˆ’ ProjF TsÎ¸ Qs ( Î¶sk ; Ï‰)
.
(D.5)
Ï‰

24

In the first step of the convergence analysis, we take F = FR,M (the locally linearized two-layer
neural network defined in (D.4)) and consider the following PMSBE:

2 
min EÎ¶âˆ¼Î½Î¸ Q0 ( Î¶sk ; Ï‰) âˆ’ ProjFR,M TsÎ¸ Q0 ( Î¶sk ; Ï‰)
.
(D.6)
Ï‰

We will show in Section D.1.2 that the output of Algorithm 1 converges to the global minimizer
of (D.6).
D.1.2

Convergence to the Global Minimizer in FR,M

The following lemma guarantees the existence and the uniqueness of the global minimizer of
MSPBE that corresponds to the projection onto FR,M in (D.6).
Lemma D.1 (Existence and Uniqueness of the Global Minimizer in FR,M ) For any b âˆˆ RM
M Ã—dÎ¶ k
âˆ—
s , there exists an Ï‰
and Ï‰(0) âˆˆ R
such that Q0 ( Â· ; Ï‰ âˆ— ) âˆˆ FR,M is unique almost everywhere
in FR,M and is the global minimizer of MSPBE that corresponds to the projection onto FR,M
in (D.6).
Proof. Proof of Lemma D.1 We first show that the operator TsÎ¸ : RÎž â†’ RÎž (3.13) is a Î³contraction in the L2 (Î½Î¸ )-norm.
h
2 i
kTsÎ¸ Q1 âˆ’ TsÎ¸ Q2 k2L2 (Î½Î¸ ) = EÎ¶âˆ¼Î½Î¸ TsÎ¸ Q1 (Î¶) âˆ’ TsÎ¸ Q2 (Î¶)
h 
2 i
= Î³ 2 EÎ¶âˆ¼Î½Î¸ E Q1 (Î¶ 0 ) âˆ’ Q2 (Î¶ 0 ) Î¶ 0 = (Âµ0 , h0 ), Âµ0 âˆ¼ P N (Â· | Î¶), h0 âˆ¼ Î Î¸ (Â· | Âµ0 )
h h
ii
2
â‰¤ Î³ 2 EÎ¶âˆ¼Î½Î¸ E (Q1 (Î¶ 0 ) âˆ’ Q2 (Î¶ 0 )) Î¶ 0 = (Âµ0 , h0 ), Âµ0 âˆ¼ P N (Â· | Î¶), h0 âˆ¼ Î Î¸ (Â· | Âµ0 )
2

= Î³ 2 EÎ¶ 0 âˆ¼Î½Î¸ [(Q1 (Î¶ 0 ) âˆ’ Q2 (Î¶ 0 )) ] = Î³ 2 kQ1 âˆ’ Q2 k2L2 (Î½Î¸ ) ,
where the first inequality follows from HÃ¶lderâ€™s inequality for the conditional expectation and
the third equality stems from the fact that Î¶ 0 and Î¶ have the same stationary distribution Î½Î¸ .
Meanwhile, the projection operator ProjFR,M : RÎž â†’ FR,M is non-expansive. Therefore,
the operator ProjFR,M TsÎ¸ : FR,M â†’ FR,M is Î³-contraction in the L2 (Î½Î¸ )-norm. Hence ProjFR,M
admits a unique fixed point Q0 ( Â· ; Ï‰ âˆ— ) âˆˆ FR,M . By definition, Q0 ( Â· ; Ï‰ âˆ— ) is the global minimizer
of MSPBE that corresponds to the projection onto FR,M in (D.6).
2
Q.E.D.
s,k
We will show that the function class FR,M will approximately become FR,âˆž
(defined in
s,k
Assumption 5.1) as M â†’ âˆž, where FR,âˆž
is a rich reproducing kernel Hilbert space (RKHS).
s,k
âˆ—
Consequently, Q0 ( Â· ; Ï‰ ) will become the global minimum of the MSPBE (D.6) on FR,âˆž
given
Lemma D.1. Moreover, by using similar argument and technique developed in [6, Theorem 4.6],
we can establish the convergence of Algorithm 1 to Q0 ( Â· ; Ï‰ âˆ— ) as the following.
âˆš
Theorem D.2 (Convergence to Q0 ( Â· ; Ï‰ âˆ— )) Set Î·critic = min{(1 âˆ’ Î³)/8, 1/ Tcritic } in Algorithm 1. Then the output Qs ( Â· ; Ï‰Ì„) of Algorithm 1 satisfies
ï£«
ï£¶
3/2
5/4
2
h
i
R3 dÎ¶ k
R5/2 dÎ¶ k
R
d
k
Î¶
2
s
s ï£¸
Einit kQs ( Â· ; Ï‰Ì„) âˆ’ Q0 ( Â· ; Ï‰ âˆ— )kL2 (Î½Î¸ ) â‰¤ O ï£­ âˆš s + âˆš
+âˆš
,
4
Tcritic
M
M
where the expectation is taken with respect to the random initialization.
The proof of Theorem D.2 is straightforward from [6, Theorem 4.6] and hence omitted.

25

Î¸

Convergence to QÎ 
s

D.1.3

Next, we analyze the error between the global minimizer of (D.6) and the team-decentralized
Î¸
Q-function QÎ 
(defined in (3.10)) to complete the convergence analysis. Different from the
s
single-agent case as in Cai et al. [6], we have to bound an additional error from using the
localized information in the critic update, in addition to the neural network approximationoptimization error.
Î¸

Proof. Proof of Theorem 5.4 First recall that by Lemma 3.5, QÎ 
satisfies the (c, Ï)-exponential
Î¸
âˆš s
max
bÎ 
, Ï = Î³. Now, let Q
be any localized
decay property in Definition 3.4, with c = r1âˆ’Î³
s
Q-function in (Local Q-function), then
Î¸
k+1
b Î Î¸ k
,
QÎ 
s (Î¶) âˆ’ Qs (Î¶s ) â‰¤ cÏ

âˆ€Î¶ âˆˆ Îž.

(D.7)

By the triangle inequality and (a + b)2 â‰¤ 2(a2 + b2 ),
2

2
Î Î¸
âˆ—
âˆ—
Î Î¸
â‰¤ kQs ( Â· ; Ï‰Ì„) âˆ’ Q0 ( Â· ; Ï‰ )kL2 (Î½Î¸ ) + Qs (Â·) âˆ’ Q0 ( Â· ; Ï‰ ) 2
Qs ( Â· ; Ï‰Ì„) âˆ’ Qs (Â·) 2
L (Î½Î¸ )
L (Î½Î¸ )


2
âˆ— 2
Î Î¸
âˆ—
â‰¤ 2 kQs ( Â· ; Ï‰Ì„) âˆ’ Q0 ( Â· ; Ï‰ )kL2 (Î½Î¸ ) + Qs (Â·) âˆ’ Q0 ( Â· ; Ï‰ ) 2
.
L (Î½Î¸ )

(D.8)
The first term in (D.8) is studied in Theorem D.2 and it suffices to bound the second term. By
b Î Î¸ and ProjF
b Î Î¸ , we have
interpolating two intermediate terms Q
Q
s
s
R,M
Î¸

âˆ—
QÎ 
s (Â·) âˆ’ Q0 ( Â· ; Ï‰ )

L2 (Î½Î¸ )

Î¸
Î¸
b Î Î¸
bÎ 
b Î Î¸
â‰¤ QÎ 
+ Q
s (Â·) âˆ’ Qs (Â·)
s (Â·) âˆ’ ProjFR,M Qs (Â·)
L2 (Î½Î¸ )
L2 (Î½Î¸ )
|
{z
} |
{z
}

(I)

(II)

Î¸

bÎ 
+ Q0 ( Â· ; Ï‰ âˆ— ) âˆ’ ProjFR,M Q
.
s (Â·)
L2 (Î½Î¸ )
|
{z
}

(D.9)

(III)

k+1

First, we have (I) â‰¤ cÏ

according to (D.7). To bound (III), we have
Î¸

bÎ 
(III) = ProjFR,M TsÎ¸ Q0 ( Â· ; Ï‰ âˆ— ) âˆ’ ProjFR,M Q
s (Â·)

L2 (Î½Î¸ )

Î¸

â‰¤ ProjFR,M TsÎ¸ Q0 ( Â· ; Ï‰ âˆ— ) âˆ’ ProjFR,M TsÎ¸ QÎ 
s (Â·)
Î¸

â‰¤ Î³ Q0 ( Â· ; Ï‰ âˆ— ) âˆ’ QÎ 
s (Â·)

Î¸

L2 (Î½Î¸ )

Î¸

= Î³ Q0 ( Â· ; Ï‰ âˆ— ) âˆ’ QÎ 
s (Â·)

Î¸

L2 (Î½Î¸ )
Î¸

bÎ 
+ TsÎ¸ QÎ 
s (Â·) âˆ’ Qs (Â·)
Î¸

L2 (Î½Î¸ )

Î¸

bÎ 
+ ProjFR,M TsÎ¸ QÎ 
s (Â·) âˆ’ ProjFR,M Qs (Â·)
L2 (Î½Î¸ )

Î¸

bÎ 
+ QÎ 
s (Â·) âˆ’ Qs (Â·)
L2 (Î½Î¸ )
{z
}
|
(I)

â‰¤ Î³ Q0 ( Â· ; Ï‰

âˆ—

Î¸
) âˆ’ QÎ 
s (Â·)

k+1

L2 (Î½Î¸ )

+ cÏ

.

(D.10)

The first line in (D.10) is due to the fact that Q0 (Â·; Ï‰ âˆ— ) is the unique fixed point of the operator ProjFR,M TsÎ¸ , (as proved in Lemma D.1); the third line in (D.10) is because the operator
ProjFR,M TsÎ¸ is a Î³-contraction in the L2 (Î½Î¸ ) norm, and ProjFR,M is non-expansive; the fourth
Î¸

line in (D.10) uses the fact that QÎ 
is the unique fixed point of TsÎ¸ ; and the last line comes
s
from the fact that (I) â‰¤ cÏk+1 . Therefore, combining the self-bounding inequality (D.10) with
(D.9) and the bound on (I) gives us
ï£«
ï£¶
Î¸

âˆ—
QÎ 
s (Â·) âˆ’ Q0 ( Â· ; Ï‰ )

L2 (Î½Î¸ )

â‰¤

ï£·
1 ï£¬
ï£¬ k+1
ï£·
b Î Î¸ (Â·) âˆ’ ProjF
b Î Î¸ (Â·)
+ Q
Q
ï£¬2cÏ
ï£·,
s
s
R,M
1âˆ’Î³ ï£­
L2 (Î½Î¸ ) ï£¸
|
{z
}
(II)

26

L2 (Î½Î¸ )

and consequently,
ï£«
Î¸

âˆ—
QÎ 
s (Â·) âˆ’ Q0 ( Â· ; Ï‰ )

2
L2 (Î½Î¸ )

â‰¤

ï£¶

ï£¬
ï£·
2
Î¸
1
ï£¬ 2 2k+2
ï£·
Î Î¸
bÎ 
b
8c
Q
Q
Ï
+
2
(Â·)
âˆ’
Proj
(Â·)
ï£¬
ï£·.
FR,M s
s
(1 âˆ’ Î³)2 ï£­
L2 (Î½Î¸ ) ï£¸
|
{z
}
(II)

(D.11)
Plugging (D.11) into (D.8) yields


2
Î¸
(Â·)
Einit Qs ( Â· ; Ï‰Ì„) âˆ’ QÎ 
s
L2 (Î½Î¸ )



h
i
2
Î¸
2
âˆ—
â‰¤ 2 Einit kQs ( Â· ; Ï‰Ì„) âˆ’ Q0 ( Â· ; Ï‰ âˆ— )kL2 (Î½Î¸ ) + Einit QÎ 
(Â·)
âˆ’
Q
(
Â·
;
Ï‰
)
0
s
L2 (Î½Î¸ )
ï£®
ï£¹
ï£«
ï£¶
3/2
5/4
ï£¯
ï£º
R3 dÎ¶ k
R5/2 dÎ¶ k
2
R2 dÎ¶sk
4
ï£¯ b Î Î¸
ï£º
Î Î¸
2 2k+2 ï£¸
s
b
âˆš
â‰¤Oï£­ âˆš s + âˆš
Q
(Â·)
âˆ’
Proj
Q
(Â·)
+
+
c
+
E
Ï
ï£¯
ï£º.
init
FR,M s
4
(1 âˆ’ Î³)2
ï£° s
L2 (Î½Î¸ ) ï£»
M
M
T
|
{z
}
(II)

(D.12)
b Î Î¸ and the class FR,M . As discussed in Section
Term (II) measures the distance between Q
s
s,k
D.1.1, the function class FR,M converges to FR,âˆž
(defined in Assumption 5.1) as M â†’ âˆž.
Consequently, term (II) decreases as the neural network gets wider. To quantitatively characs,k
terize the approximation error between FR,M and FR,âˆž
, one needs the following lemma from
Rahimi and Recht [46] and [6, Proposition 4.3]:
Lemma D.3 Assume Assumption 5.1, we have
ï£®

ï£¹

ï£¯
ï£º
2
ï£º
ï£¯ b Î Î¸
Î Î¸
b
(Â·)
âˆ’
Proj
Q
(Â·)
Einit ï£¯ Q
ï£ºâ‰¤O
FR,M s
ï£° s
L2 (Î½Î¸ ) ï£»
|
{z
}

R2 dÎ¶sk
M

!
.

(D.13)

(II)

With this lemma, Theorem 5.4 follows immediately by plugging (D.13) into (D.12), and
âˆš
max
setting c = r1âˆ’Î³
, Ï = Î³, Tcritic = â„¦(M ) in (D.12).
2
Q.E.D.

D.2

Proof of Theorem 5.10: Convergence of Actor Update

The proof of Theorem 5.10 consists of two steps: the first step in Section D.2.1 shows that the
actor update converges to a stationary point of J (4.1), and the second step in Section D.2.2
bridges the gap between the stationary point and the optimality.

For the rest of this section, we use Î· to denote Î·actor and Bs to denote Bsactor := Î¸s âˆˆ
âˆš
Q
RM Ã—dÎ¶s : kÎ¸s âˆ’ Î¸s (0)kâˆž â‰¤ R/ M for ease of notation. Meanwhile, define B = sâˆˆS Bs , the
product space of Bs â€™s, which is a convex set in RM Ã—dÎ¶ .
D.2.1

Convergence to Stationary Point

Definition D.4 A point Î¸e âˆˆ B is called a stationary point of J(Â·) if it holds that
e > (Î¸ âˆ’ Î¸)
e â‰¤ 0,
âˆ‡Î¸ J(Î¸)

27

âˆ€Î¸ âˆˆ B.

(D.14)

Define the following mapping G from RM Ã—dÎ¶ to itself:
G(Î¸) := Î· âˆ’1 Â· [ProjB (Î¸ + Î· Â· âˆ‡Î¸ J(Î¸)) âˆ’ Î¸] .

(D.15)

e = 0 (Sra et al. [49]). Now denote
It is well-known that (D.14) holds if and only if G(Î¸)
Ï(t) := G(Î¸(t)), where Î¸(t) = {Î¸s (t)}sâˆˆS is the actor parameter updated in Algorithm 2 in
iteration t.
To show that Algorithm 2 converges to a stationary point, we focus on analyzing kÏ(t)k2 .
Theorem D.5 Assume Assumptions 5.5 - 5.7. Set Î· = (Tactor )âˆ’1/2 and assume 1 âˆ’ LÎ· â‰¥
1/2, where L is the Lipschitz constant in Assumotion 5.7. Then the output {Î¸(t)}tâˆˆ[Tactor ] of
Algorithm 2 satisfies

 8Ï„ 2 Î£2 |S|
4
+âˆš
E[J(Î¸(Tactor + 1)) âˆ’ J(Î¸(1))] + Q (Tactor ). (D.16)
E kÏ(t)k22 â‰¤
B
tâˆˆ[Tactor ]
Tactor
min

Here Q measures the error accumulated from the critic steps which is defined as
1/2

Q (Tactor ) =

32Ï„ DRdÎ¶s |S|
(1 âˆ’ Î³)Î·Tactor

Â·

TX
actor X


E

Î¸(t)
Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QsÎ  (Â·)

t=1 sâˆˆS


L2 (Î½Î¸(t) )



TX
actor X
2
16Ï„ 2 D2 |S|2
Î Î¸(t)
+
(Â·)
,
E Qs ( Â· ; Ï‰Ì„s , t) âˆ’ Qs
Â·
(1 âˆ’ Î³)2 Tactor t=1
L2 (Î½Î¸(t) )

(D.17)

sâˆˆS

where {Qs ( Â· ; Ï‰Ì„s , t)}sâˆˆS is the output of the critic update at step t in Algorithm 2. All expectations in (D.16) and (D.17) are taken over all randomness in Algorithm 1 and Algorithm
2.
Proof. Proof of Theorem D.5
Let t âˆˆ [Tactor ], we first lower bound the difference between the expected total rewards of
Î Î¸(t+1) and Î Î¸(t) . By Assumption 5.7, âˆ‡Î¸ J (Î¸) is L-Lipschitz continuous. Hence by Taylorâ€™s
expansion,
>

2

J (Î¸(t + 1)) âˆ’ J (Î¸(t)) â‰¥ Î· Â· âˆ‡Î¸ J (Î¸(t)) Î´(t) âˆ’ L/2 Â· kÎ¸(t + 1) âˆ’ Î¸(t)k2 ,

(D.18)

where Î´(t) = (Î¸(t + 1) âˆ’ Î¸(t)) /Î·. Meanwhile denote Î¾s (t) = gbs (Î¸(t))âˆ’E [b
gs (Î¸(t))], where gbs (Î¸(t))
is defined in (4.14) and the expectation is taken over ÏƒÎ¸(t) given {Ï‰Ì„s }sâˆˆS . Then
>

âˆ‡Î¸ J (Î¸(t)) Î´(t) =

X

>

âˆ‡Î¸s J (Î¸(t)) Î´s (t)

sâˆˆS

=

Xh

i
>
(âˆ‡Î¸s J (Î¸(t)) âˆ’ E [b
gs (Î¸(t))]) Î´s (t) âˆ’ Î¾s (t)> Î´s (t) + gbs (Î¸(t))> Î´s (t) ,

sâˆˆS

(D.19)
where Î´s (t) := (Î¸s (t + 1) âˆ’ Î¸s (t)) /Î·. The first term in (D.19) represents the error of estimating
âˆ‡Î¸s J (Î¸(t)) using
ï£®"
ï£¹
#
X

1
E [b
gs (Î¸(t))] =
EÏƒ ï£°
Qy Âµ(Nyk ), h(Nyk ); Ï‰Ì„y , t âˆ‡Î¸s log Î Î¸s (h(s) | Âµ(s))ï£» .
1 âˆ’ Î³ Î¸(t)
k
yâˆˆNs

To bound the first term, first notice that
ï£®ï£®
ï£¹
ï£¹
X

1
EÏƒ ï£°ï£°
Qy Âµ(Nyk ), h(Nyk ); Ï‰Ì„y , t ï£» âˆ‡Î¸s log Î Î¸s (h(s) | Âµ(s))ï£» .
E [b
gs (Î¸(t))] =
1 âˆ’ Î³ Î¸(t)
yâˆˆS

28


This is because for all y 6âˆˆ Nsk , Qy Âµ(Nyk ), h(Nyk ); Ï‰Ì„y is independent of s and consequently, we
can verify that
ï£®ï£®
ï£¹
ï£¹
X

EÏƒÎ¸(t) ï£°ï£°
Qy Âµ(N k (y)), h(N k (y)); Ï‰Ì„y , t ï£» âˆ‡Î¸s log Î Î¸s (h(s) | Âµ(s))ï£» = 0.
y6âˆˆNsk

Therefore, following the similar computation in Lemma D.2, Cai et al. [6], we have
>

(âˆ‡Î¸s J (Î¸(t)) âˆ’ E [b
gs (Î¸(t))]) Î´s (t) â‰¤

1/2
4Ï„ DRdÎ¶s X

(1 âˆ’ Î³)Î·

Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QÎ¸(t)
(Â·)
s

sâˆˆS

L2 (Î½Î¸(t) )

. (D.20)

To bound the second term in (D.19), we simply have
Î¾s (t)> Î´s (t) â‰¤ kÎ¾s (t)k22 + kÎ´s (t)k22 .

(D.21)

To handle the last term in (D.19), we have
gbs (Î¸(t))> Î´s (t) âˆ’ kÎ´s (t)k22 = Î· âˆ’1 Â· (Î·b
gs (Î¸(t)) âˆ’ (Î¸s (t + 1) âˆ’ Î¸s (t)))> Î´s
>
=Î· âˆ’1 Â· Î¸s (t + 1/2) âˆ’ ProjBs (Î¸s (t + 1/2)) Î´s (t)
>

=Î· âˆ’2 Â· Î¸s (t + 1/2) âˆ’ ProjBs (Î¸s (t + 1/2))
ProjBs (Î¸s (t + 1/2)) âˆ’ Î¸s (t) â‰¥ 0

(D.22)

Here we write Î¸s (t) + Î·b
gs (Î¸(t)) as Î¸s (t + 1/2) to simplify the notation. The last inequality comes
from the property of the projection onto a convex set.
Therefore, combining (D.19), (D.20), (D.21) and (D.22) suggests
>

âˆ‡Î¸s J (Î¸(t)) Î´s (t) â‰¥ âˆ’


1/2
4Ï„ DRdÎ¶s X
(1 âˆ’ Î³)Î·

Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QÎ¸(t)
(Â·)
s

sâˆˆS



1
2
2
kÎ´
+
s (t)k2 âˆ’ kÎ¾s (t)k2 .
2
L2 (Î½Î¸(t) )

Consequently,
1/2

>

âˆ‡Î¸ J (Î¸(t)) Î´(t) â‰¥ âˆ’

4Ï„ DRdÎ¶s

(1 âˆ’ Î³)Î·

|S|

X

Î¸(t)

Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QsÎ 

sâˆˆS

(Â·)



1
+ kÎ´(t)k22 âˆ’ kÎ¾(t)k22 .
2
2
L (Î½Î¸(t) )
(D.23)

Thus, by plugging (D.23) into (D.18) and by Assumption 5.5, we have

1âˆ’LÂ·Î· 
Ï„ 2 Î£2 |S|
E kÎ´(t)k22 â‰¤ Î· âˆ’1 Â· E [J(Î¸(t + 1)) âˆ’ J(Î¸(t))] +
2
2B
1/2
4Ï„ DRdÎ¶s |S| X
Î¸(t)
+
Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QsÎ  (Â·) 2
.
(1 âˆ’ Î³)Î·
L (Î½Î¸(t) )

(D.24)

sâˆˆS

Here the expectation is taken over ÏƒÎ¸(t) given {Ï‰Ì„s }sâˆˆS .
Now, in order to bridge the gap between kÎ´(t)k2 in (D.24) and kÏ(t)k2 = kG(Î¸(t))k2 in
(D.15), we next will bound the difference kÎ´(t) âˆ’ Ï(t)k2 . We start with defining a local gradient
mapping Gs from RM Ã—dÎ¶ to RM Ã—dÎ¶s :


Gs (Î¸) := Î· âˆ’1 Â· ProjBs (Î¸s + Î· Â· âˆ‡Î¸s J(Î¸)) âˆ’ Î¸s .
(D.25)
Since Bs is an lâˆž -ball around the initialization, it is easy to verify that Gs (Î¸) = (G(Î¸))s .
Therefore, we can further define Ïs (t) = Gs (Î¸(t)) and the following decomposition holds:
X
kÎ´(t) âˆ’ Ï(t)k22 =
kÎ´s (t) âˆ’ Ïs (t)k22 .
sâˆˆS

29

From the definitions of Î´s (t) and Ïs (t),
kÎ´s (t) âˆ’ Ïs (t)k2 = Î· âˆ’1 Â· ProjBs (Î¸s + Î· Â· âˆ‡Î¸s J(Î¸)) âˆ’ Î¸s âˆ’ ProjBs (Î¸s + Î· Â· gbs (Î¸)) + Î¸s 2
= Î· âˆ’1 Â· ProjBs (Î¸s + Î· Â· âˆ‡Î¸s J(Î¸)) âˆ’ ProjBs (Î¸s + Î· Â· gbs (Î¸)) 2
â‰¤ Î· âˆ’1 Â· kÎ¸s + Î· Â· âˆ‡Î¸s J(Î¸) âˆ’ Î¸s + Î· Â· gbs (Î¸)k2 = kâˆ‡Î¸s J(Î¸) âˆ’ gbs (Î¸)k2
Following similar calculations in [6, Lemma D.3],
!2

 2Ï„ 2 Î£2

8Ï„ 2 D2
E kâˆ‡Î¸s J(Î¸) âˆ’ gbs (Î¸)k22 â‰¤
+
B
(1 âˆ’ Î³)2

Î¸(t)
Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QÎ 
(Â·)
s

X
sâˆˆS

2Ï„ 2 Î£2
8Ï„ 2 D2 |S|
â‰¤
+
B
(1 âˆ’ Î³)2

X

Î¸(t)

Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QsÎ 

L2 (Î½Î¸(t) )

!

2

(Â·)

sâˆˆS

L2 (Î½Î¸(t) )

.
(D.26)

The expectation is taken over ÏƒÎ¸(t) given {Ï‰Ì„s }sâˆˆS . Consequently,

 2Ï„ 2 Î£2 |S| 8Ï„ 2 D2 |S|2
E kÎ´(t) âˆ’ Ï(t)k22 â‰¤
+
B
(1 âˆ’ Î³)2

X

Î¸(t)
Qs ( Â· ; Ï‰Ì„s , t) âˆ’ QÎ 
(Â·)
s

sâˆˆS

!

2
L2 (Î½Î¸(t) )

. (D.27)

âˆš
Set Î· = 1/ Tactor and take (D.24) and (D.27), we obtain (D.16) from the following estimations:
min
tâˆˆ[Tactor ]



E kÏ(t)k22 â‰¤
â‰¤

1
Tactor
2
Tactor

Â·

TX
actor

kÏ(t)k22 â‰¤

t=1

Â·

TX
actor

2
Tactor

Â·

TX
actor





E kÎ´(t) âˆ’ Ï(t)k22 + E kÎ´(t)k22

t=1





E kÎ´(t) âˆ’ Ï(t)k22 + 2(1 âˆ’ L Â· Î·)E kÎ´(t)k22

t=1

4
8Ï„ 2 Î£2 |S|
+âˆš
E[J(Î¸(Tactor + 1)) âˆ’ J(Î¸(1))] + Q (Tactor ),
â‰¤
B
Tactor
where Q measures the error accumulated from the critic steps which is defined in (D.17), i.e.,
1/2

Q (Tactor ) =

32Ï„ DRdÎ¶s |S|
(1 âˆ’ Î³)Î·Tactor

Â·

TX
actor X


E

Î¸(t)

Qs ( Â· ; Ï‰Ì„s ) âˆ’ QÎ 
s


(Â·)

t=1 sâˆˆS

L2 (Î½Î¸(t) )



TX
actor X
2
16Ï„ 2 D2 |S|2
Î Î¸(t)
+
Â·
E Qs ( Â· ; Ï‰Ì„s ) âˆ’ Qs
(Â·)
.
(1 âˆ’ Î³)2 Tactor t=1
L2 (Î½Î¸(t) )
sâˆˆS

Here the expectations in (D.16) and (D.17) are taken over all randomness in Algorithm 1 and
Algorithm 2.
2
Q.E.D.
D.2.2

Bridging the gap between Stationarity and Optimality

Recall that ÏƒÎ¸ in (4.2) denotes the state-action visitation measure under policy Î Î¸ . Denote ÏƒÌ„Î¸
as the state visitation measure under policy Î Î¸ . Consequently,
ÏƒÌ„Î¸ (Âµ)Î Î¸ (h | Âµ) = ÏƒÎ¸ (Âµ, h).
Following similar steps in the proof of [6, Theorem 4.8], one can characterize the global
optimality of the obtained stationary point Î¸e âˆˆ B as the following.

30

Lemma D.6 Let Î¸e âˆˆ B be a stationary point of J(Â·) satisfying condition (D.14) and let Î¸âˆ— âˆˆ B
be the global maximum point of J(Â·) in B. Then the following inequality holds:


X
e â‰¤ 2rmax inf u e(Âµ, h) âˆ’
(1 âˆ’ Î³) J(Î¸âˆ— ) âˆ’ J(Î¸)
Ï†Î¸es (Âµ(s), h(s))> Î¸s
Î¸
1 âˆ’ Î³ Î¸âˆˆB
sâˆˆS

,
L2 (Ïƒ

P
dÏƒÌ„Î¸âˆ—
dÏƒÎ¸âˆ— dÏƒÌ„Î¸âˆ—
>e
Î¸âˆ—
where uÎ¸e(Âµ, h) := dÏƒ
sâˆˆS Ï†Î¸es (Âµ(s), h(s)) Î¸s , and dÏƒÎ¸e , dÏƒÌ„Î¸e
dÏƒÎ¸e (Âµ, h)âˆ’ dÏƒÌ„Î¸e (Âµ)+

(D.28)

e)
Î¸

are the Radon-

Nikodym derivatives between the corresponding measures.

Proof. Proof of Lemma D.6 First recall that by (4.8), for any Î¸ âˆˆ B,
h Î¸e
i
X
X
e > (Î¸ âˆ’ Î¸)
e s, Âµ, h)> (Î¸s âˆ’ Î¸es ) ,
e =
e > (Î¸s âˆ’ Î¸es ) = Ï„
âˆ‡Î¸ J(Î¸)
EÏƒÎ¸e QÎ  (Âµ, h) Â· Î¦(Î¸,
âˆ‡Î¸s J(Î¸)
1âˆ’Î³
sâˆˆS

sâˆˆS

in which Î¦(Î¸, s, Âµ, h) := Ï†Î¸s (Âµ(s), h(s)) âˆ’ Eh(s)0 âˆ¼Î Î¸ss (Â·|Âµ(s)) [Ï†Î¸s (Âµ(s), h0 (s))] is defined in (4.7).
Since Î¸e âˆˆ B is a stationary point of J(Â·),
h Î¸e
i
X
e s, Âµ, h)> (Î¸s âˆ’ Î¸es ) â‰¤ 0, âˆ€Î¸ âˆˆ B.
EÏƒÎ¸e QÎ  (Âµ, h) Â· Î¦(Î¸,
(D.29)
sâˆˆS
e
Î¸

e
Î¸

e
Î¸

Denote AÎ  (Âµ, h) := QÎ  (Âµ, h) âˆ’ V Î  (Âµ) as the advantage function under policy Î Î¸ .
e

holds from the definition that Ehâˆ¼Î Î¸e(Â·|Âµ) [A
sup(Âµ,h)âˆˆÎž A

Î Î¸

e

(Âµ, h) â‰¤ 2 supÂµâˆˆP N (S) V

Î Î¸

e

Î Î¸

e

(Âµ)

(Âµ, h)] = V

Î Î¸

e

(Âµ) âˆ’ V

Î Î¸

e

It

(Âµ) = 0. Meanwhile,

max
.
â‰¤ 2r1âˆ’Î³

e
Î¸

e s, Âµ, h)] = 0, we have for any s âˆˆ S,
Given that Ehâˆ¼Î Î¸e(Â·|Âµ) [AÎ  (Âµ, h)] = 0 and Ehâˆ¼Î Î¸e(Â·|Âµ) [Î¦(Î¸,
i
h Î¸e
e s, Âµ, h) = 0,
EÏƒÎ¸e V Î  (Âµ) Â· Î¦(Î¸,
and
(D.30)
h Î¸e
h
ii
EÏƒÎ¸e AÎ  (Âµ, h) Â· Eh(s)0 âˆ¼Î Î¸es (Â·|Âµ(s)) Ï†Î¸es (Âµ(s), h0 (s)) = 0.

(D.31)

s

Combining (D.29) with (D.30) and (D.31),
h Î¸e
i
X
>
EÏƒÎ¸e AÎ  (Âµ, h) Â· Ï†Î¸es (Âµ(s), h(s)) (Î¸s âˆ’ Î¸es ) â‰¤ 0,

âˆ€Î¸ âˆˆ B.

(D.32)

sâˆˆS

Moreover, by the Performance Difference Lemma (Kakade and Langford [32]),


Ei
hD e
b = EÏƒÌ„ âˆ— AÎ Î¸ (Âµ, Â·), Î Î¸âˆ— (Â· | Âµ) âˆ’ Î Î¸e(Â· | Âµ) .
(1 âˆ’ Î³) Â· J(Î¸âˆ— ) âˆ’ J(Î¸)
Î¸

(D.33)

Combining (D.33) with (D.32), it holds that for any Î¸ âˆˆ B,


b
(1 âˆ’ Î³) Â· J(Î¸âˆ— ) âˆ’ J(Î¸)
hD Î¸e
Ei X
h Î¸e
i
âˆ—
e
>
â‰¤ EÏƒÌ„Î¸âˆ— AÎ  (Âµ, Â·), Î Î¸ (Â· | Âµ) âˆ’ Î Î¸ (Â· | Âµ) âˆ’
EÏƒÎ¸e AÎ  (Î¶) Â· Ï†Î¸es (Î¶s ) (Î¸s âˆ’ Î¸es )
sâˆˆS

"
= EÏƒÎ¸e A

Î Î¸

e

(Âµ, h) Â·

X
dÏƒÎ¸âˆ—
dÏƒÌ„Î¸âˆ—
(Âµ, h) âˆ’
(Âµ) âˆ’
Ï†Î¸es (Âµ(s), h(s))> (Î¸s âˆ’ Î¸es )
dÏƒÎ¸e
dÏƒÌ„Î¸e

!#
.

(D.34)

sâˆˆS

Therefore,


b
(1 âˆ’ Î³) Â· J(Î¸âˆ— ) âˆ’ J(Î¸)
â‰¤

X
2rmax
dÏƒÎ¸âˆ—
dÏƒÌ„Î¸âˆ—
inf
(Âµ, h) âˆ’
(Âµ) âˆ’
Ï†Î¸es (Âµ(s), h(s))> (Î¸s âˆ’ Î¸es )
1 âˆ’ Î³ Î¸âˆˆB dÏƒÎ¸e
dÏƒÌ„Î¸e
sâˆˆS

=

X
2rmax
inf uÎ¸e(Âµ, h) âˆ’
Ï†Î¸es (Âµ(s), h(s))> Î¸s
1 âˆ’ Î³ Î¸âˆˆB
sâˆˆS

31

,
L2 (Ïƒ

e)
Î¸

L2 (ÏƒÎ¸e)

(D.35)

P
dÏƒÌ„Î¸âˆ—
dÏƒÎ¸âˆ— dÏƒÌ„Î¸âˆ—
>e
Î¸âˆ—
where uÎ¸e(Âµ, h) := dÏƒ
sâˆˆS Ï†Î¸es (Âµ(s), h(s)) Î¸s , and dÏƒÎ¸e , dÏƒÌ„Î¸e are the RadondÏƒÎ¸e (Âµ, h)âˆ’ dÏƒÌ„Î¸e (Âµ)+
Nikodym derivatives between corresponding measures.
2
Q.E.D.
To further bound the right-hand-side of (D.28) in Lemma D.6, define the following function
class
"
#

M
X
X

1
>
>
âˆš
FeR,M = f0 (Î¶; Î¸) :=
1 [Î¸s (0)]m Î¶s > 0 [Î¸s ]m Î¶s :
M m=1
sâˆˆS
{z
}
|
(?)


âˆš
Î¸s âˆˆ RM Ã—dÎ¶s , kÎ¸s âˆ’ Î¸s (0)kâˆž â‰¤ R/ M ,

(D.36)

given an initialization Î¸s (0) âˆˆ RM Ã—dÎ¶s , s âˆˆ S and b âˆˆ RM .
FeR,M (D.36) is a local linearization of the actor neural network. More specifically, term (?)
in (D.36) locally linearizes the decentralized actor neural network f (Î¶s ; Î¸s ) (4.4) with respect to
Î¸s . Any f0 (Î¶; Î¸) âˆˆ FeR,M is a sum
P of |S| inner products between feature mapping Ï†Î¸s (0) (Â·) (4.6)
and parameter Î¸s : f0 (Î¶; Î¸) = sâˆˆS Ï†Î¸s (0) (Î¶s ) Â· Î¸s . As the width of the neural network M â†’ âˆž,
FeR,M converges to FR,âˆž (defined in Assumption 5.9). The approximation error between FeR,M
and FR,âˆž is bounded in the following lemma.
Lemma D.7 For any function f (Î¶) âˆˆ FR,âˆž defined in Assumption 5.9, we have
!


1/2
|S|RdÎ¶s
.
Einit f (Â·) âˆ’ ProjFeR,M f (Â·) 2
â‰¤O
L (ÏƒÎ¸e)
M 1/2

(D.37)

Lemma D.7 follows from Rahimi and Recht [46] and [6, Proposition 4.3]. The factor |S| stems
from the fact that FR,âˆž can be decomposed into |S| independent reproducing kernel Hilbert
spaces. With Lemma D.7, we are ready to establish an upper bound for the right-hand-side of
(D.28) in the following proposition.
Proposition D.8 Under Assumption 5.9, let Î¸e âˆˆ B be a stationary point of J(Â·) and let Î¸âˆ— âˆˆ B
be the global maximum point of J(Â·) in B. Then the following inequality holds:
!
3/4


|S|R3/2 dÎ¶s
âˆ—
e â‰¤O
(1 âˆ’ Î³) J(Î¸ ) âˆ’ J(Î¸)
.
(D.38)
M 1/4
Proof. Proof of Proposition D.8 First by the triangle inequality,
inf uÎ¸e(Î¶) âˆ’

Î¸âˆˆB

X
sâˆˆS

Ï†Î¸es (Î¶s )> Î¸s

â‰¤ uÎ¸e(Î¶) âˆ’ ProjFeR,M uÎ¸e(Î¶)
L2 (ÏƒÎ¸e)

+ inf ProjFeR,M uÎ¸e(Î¶) âˆ’
Î¸âˆˆB

(D.39)

L2 (ÏƒÎ¸e)

X
sâˆˆS

Ï†Î¸es (Î¶s )> Î¸s

,
L2 (ÏƒÎ¸e)

P
b
e
where FeR,M is defined in (D.36). We denote ProjFeR,M uÎ¸e(Î¶) =
sâˆˆS Ï†Î¸s (0) (Î¶s ) Â· Î¸s âˆˆ FR,M
for some Î¸b âˆˆ B. Therefore, by Lemma D.7, the first term on the right-hand-side of (D.39) is
bounded by (D.37):
!
1/2
X
|S|RdÎ¶s
uÎ¸e(Î¶) âˆ’
Ï†Î¸s (0) (Î¶s ) Â· Î¸bs
â‰¤O
.
M 1/2
sâˆˆS
2
L (ÏƒÎ¸e)

The following Lemma D.9 is a direct application of [53, Lemma E.2], which is used to bound
the second term on the right-hand-side of (D.39).
32

âˆš

Lemma D.9 It holds for any Î¸s , Î¸s0 âˆˆ Bs = Î±s âˆˆ RM Ã—dÎ¶s : kÎ±s âˆ’ Î¸s (0)kâˆž â‰¤ R/ M
3/4



Einit kÏ†Î¸s (Î¶s )> Î¸s0 âˆ’ Ï†Î¸s (0) (Î¶s )> Î¸s0 kL2 (ÏƒÎ¸ ) â‰¤ O

R3/2 dÎ¶s

that

!
,

M 1/4

(D.40)

where the expectation is taken over random initialization.
Taking Î¸ = Î¸e and Î¸0 = Î¸b in Lemma D.9 gives us
3/4

X

Ï†Î¸s (0) (Î¶s ) Â· Î¸bs âˆ’ Ï†Î¸es (Î¶s )> Î¸bs

L2 (ÏƒÎ¸e)

sâˆˆS

â‰¤O

|S|R3/2 dÎ¶s

!

M 1/4

Therefore, by Lemma D.1,
3/4



X
e â‰¤ inf u e(Î¶) âˆ’
Ï†Î¸es (Î¶s )> Î¸s
(1 âˆ’ Î³) J(Î¸âˆ— ) âˆ’ J(Î¸)
Î¸
Î¸âˆˆB

sâˆˆS

â‰¤O

|S|R3/2 dÎ¶s

L2 (ÏƒÎ¸e)

M 1/4

!
.
2
Q.E.D.

Now we are ready to establish Theorem 5.10.
Proof. Proof of Theorem 5.10 Following similar calculations as in [53, Section H.3], we obtain
that at iteration t âˆˆ [Tactor ],
âˆ‡Î¸ J(Î¸(t))> (Î¸ âˆ’ Î¸(t)) â‰¤ 2(R +

Î· Â· rmax
) Â· kÏ(t)k2 , âˆ€Î¸ âˆˆ B.
1âˆ’Î³

(D.41)

e Having
The right-hand-side of (D.41) quantifies the deviation of Î¸(t) from a stationary point Î¸.
(D.41) and following similar arguments for Lemma D.6 and Proposition D.8, we can show that
3/4

(1 âˆ’ Î³)

min
tâˆˆ[Tactor ]

âˆ—

E [J(Î¸ ) âˆ’ J(Î¸(t))] â‰¤ O

|S|R3/2 dÎ¶s
M 1/4

!



Î· Â· rmax
+2 R+
Â· min E[kÏ(t)k2 ].
1âˆ’Î³
tâˆˆ[Tactor ]
(D.42)

Here the last term mintâˆˆ[Tactor ] E[kÏ(t)k2 ] is bounded by (D.16) in Theorem D.5, while the term
Q (Tactor ) in (D.17) can be upper bounded by Theorem 5.4. Finally with the parameters stated
in Theorem 5.10, the following statement holds by straightforward calculation:



min E [J(Î¸âˆ— ) âˆ’ J(Î¸(t))] â‰¤ O |S|1/2 B âˆ’1/2 + |S||A|1/4 Î³ k/8 + (Tactor )âˆ’1/4 .
tâˆˆ[Tactor ]

2
Q.E.D.

33

