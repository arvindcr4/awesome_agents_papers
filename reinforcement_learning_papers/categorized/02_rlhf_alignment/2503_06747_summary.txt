Here is the extracted information in the requested format:

**PAPER:** Fully-Decentralized MADDPG with Networked Agents
**TITLE:** Fully-Decentralized MADDPG with Networked Agents
**ARXIV_ID:** 2503.06747v1
**RESEARCH_METHOD:** 02_rlhf_alignment
**METHOD_DESCRIPTION:** This paper proposes a fully decentralized version of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm, which is a type of actor-critic algorithm for multi-agent reinforcement learning. The authors introduce surrogate policies to decentralize the training process and allow for local communication between agents during training. They also propose two variants of the algorithm, one with a hard consensus update and another with a soft consensus update, which enable the agents to share critic parameters through a communication network.
**KEY_CONTRIBUTIONS:**
* Developed a fully decentralized version of the MADDPG algorithm
* Introduced surrogate policies to decentralize the training process
* Proposed two variants of the algorithm with hard and soft consensus updates
* Evaluated the algorithms in cooperative, adversarial, and mixed settings
* Compared the performance of the decentralized algorithms with the original MADDPG algorithm

Let me know if you need any further assistance!
