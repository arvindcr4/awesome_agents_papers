arXiv:1605.07736v2 [cs.LG] 31 Oct 2016

Learning Multiagent Communication
with Backpropagation
Sainbayar Sukhbaatar
Dept. of Computer Science
Courant Institute, New York University
sainbar@cs.nyu.edu

Arthur Szlam
Facebook AI Research
New York
aszlam@fb.com

Rob Fergus
Facebook AI Research
New York
robfergus@fb.com

Abstract
Many tasks in AI require the collaboration of multiple agents. Typically, the
communication protocol between agents is manually specified and not altered
during training. In this paper we explore a simple neural model, called CommNet,
that uses continuous communication for fully cooperative tasks. The model consists
of multiple agents and the communication between them is learned alongside their
policy. We apply this model to a diverse set of tasks, demonstrating the ability
of the agents to learn to communicate amongst themselves, yielding improved
performance over non-communicative agents and baselines. In some cases, it
is possible to interpret the language devised by the agents, revealing simple but
effective strategies for solving the task at hand.

1

Introduction

Communication is a fundamental aspect of intelligence, enabling agents to behave as a group, rather
than a collection of individuals. It is vital for performing complex tasks in real-world environments
where each actor has limited capabilities and/or visibility of the world. Practical examples include
elevator control [4] and sensor networks [6]; communication is also important for success in robot
soccer [27]. In any partially observed environment, the communication between agents is vital to
coordinate the behavior of each individual. While the model controlling each agent is typically
learned via reinforcement learning [2, 30], the specification and format of the communication is
usually pre-determined. For example, in robot soccer, the bots are designed to communicate at each
time step their position and proximity to the ball.
In this work, we propose a model where cooperating agents learn to communicate amongst themselves
before taking actions. Each agent is controlled by a deep feed-forward network, which additionally
has access to a communication channel carrying a continuous vector. Through this channel, they
receive the summed transmissions of other agents. However, what each agent transmits on the
channel is not specified a-priori, being learned instead. Because the communication is continuous,
the model can be trained via back-propagation, and thus can be combined with standard single
agent RL algorithms or supervised learning. The model is simple and versatile. This allows it to be
applied to a wide range of problems involving partial visibility of the environment, where the agents
learn a task-specific communication that aids performance. In addition, the model allows dynamic
variation at run time in both the number and type of agents, which is important in applications such
as communication between moving cars.
We consider the setting where we have J agents, all cooperating to maximize reward R in some
environment. We make the simplifying assumption of full cooperation between agents, thus each
agent receives R independent of their contribution. In this setting, there is no difference between
each agent having its own controller, or viewing them as pieces of a larger model controlling all
agents. Taking the latter perspective, our controller is a large feed-forward neural network that maps
inputs for all agents to their actions, each agent occupying a subset of units. A specific connectivity
29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

structure between layers (a) instantiates the broadcast communication channel between agents and
(b) propagates the agent state.
We explore this model on a range of tasks. In some, supervision is provided for each action while
for others it is given sporadically. In the former case, the controller for each agent is trained by
backpropagating the error signal through the connectivity structure of the model, enabling the agents
to learn how to communicate amongst themselves to maximize the objective. In the latter case,
reinforcement learning must be used as an additional outer loop to provide a training signal at each
time step (see Appendix A for details).

2

Communication Model

We now describe the model used to compute the distribution over actions p(a(t)|s(t), θ) at a given
time t (omitting the time index for brevity). Let sj be the jth agent’s view of the state of the
environment. The input to the controller is the concatenation of all state-views s = {s1 , ..., sJ },
and the controller Φ is a mapping a = Φ(s), where the output a is a concatenation of discrete
actions a = {a1 , ..., aJ } for each agent. Note that this single controller Φ encompasses the individual
controllers for each agents, as well as the communication between agents.
2.1

Controller Structure

We now detail our architecture for Φ that is built from modules f i , which take the form of multilayer
neural networks. Here i ∈ {0, .., K}, where K is the number of communication steps in the network.
Each f i takes two input vectors for each agent j: the hidden state hij and the communication cij ,
and outputs a vector hi+1
j . The main body of the model then takes as input the concatenated vectors
0
0
0
0
h = [h1 , h2 , ..., hJ ], and computes:
hi+1
j
ci+1
j

= f i (hij , cij )
1 X i+1
=
hj 0 .
J −1 0

(1)
(2)

j 6=j

In the case that f i is a single linear layer followed by a non-linearity σ, we have: hi+1
= σ(H i hij +
j
i i
i+1
i i
C cj ) and the model can be viewed as a feedforward network with layers h
= σ(T h ) where hi
i
i
i
i
is the concatenation of all hj and T takes the block form (where C̄ = C /(J − 1)):


Hi
 C̄ i
 i

T i =  C̄
 .
 ..
C̄ i

C̄ i
Hi
C̄ i
..
.
C̄ i

C̄ i
C̄ i
Hi
..
.
C̄ i

...
...
...
..
.
...


C̄ i
i
C̄ 
C̄ i 
,
.. 
. 
Hi

A key point is that T is dynamically sized since the number of agents may vary. This motivates the
the normalizing factor J − 1 in equation (2), which rescales the communication vector by the number
of communicating agents. Note also that T i is permutation invariant, thus the order of the agents
does not matter.
At the first layer of the model an encoder function h0j = r(sj ) is used. This takes as input state-view
sj and outputs feature vector h0j (in Rd0 for some d0 ). The form of the encoder is problem dependent,
but for most of our tasks it is a single layer neural network. Unless otherwise noted, c0j = 0 for all j.
At the output of the model, a decoder function q(hK
j ) is used to output a distribution over the space of
actions. q(.) takes the form of a single layer network, followed by a softmax. To produce a discrete
action, we sample from this distribution: aj ∼ q(hK
j ).
Thus the entire model (shown in Fig. 1), which we call a Communication Neural Net (CommNet), (i)
takes the state-view of all agents s, passes it through the encoder h0 = r(s), (ii) iterates h and c in
equations (1) and (2) to obtain hK , (iii) samples actions a for all agents, according to q(hK ).
2.2 Model Extensions
Local Connectivity: An alternative to the broadcast framework described above is to allow agents
to communicate to others within a certain range. Let N (j) be the set of agents present within
2

38 by
controller
istheir
a large
feed-forward
neural
network
that
maps
inputs
for
all agents
to theirstructure
actions,between
each layers
45
will Address
be treated
ascontroller
such
the
reinforcement
learning.
In network
the
continuous
case,
signals
passed
actions,
each
agent
occupying
amaps
subset
ofthe
units.
A
connectivity
38
is adifferent
large
feed-forward
neural
that
inputs
for
allspecific
agents
tothe
their
actions,
each
46of5between
agents
are
no
than
hidden
inused
ab(s,
neural
network;
thus
credit
assignment
the
agents.
5
agents.
agents.
5 we
agents.
email
51 states
states
✓),
computed
via
an
extra
head
model
the
4abstract
architecture
??.
Second,
show
this
architecture
can
be
to
control
groups
of
cooperating
55
Here
r(t)
is reward
given
at
time
t,thus
and
the
hyperparameter
↵onisfor
for
balancing
the
reward
and theprobabilities. Beside
39different
agenthow
occupying
a states
subset
of
units.
Anetwork;
specific
connectivity
structure
between
layers
(a)producing
instantiates
theaction
46 0 between
agents
are
no
than
hidden
in
a
neural
credit
assignment
for
the
0
0 email
0
(a)
instantiates
the
broadcast
communication
channel
between
agents
and
(b)
propagates
the agent
h
=
[h
,
h
,
...,
h
],
and
computes:
can be performed
standard
backpropagation
(within
the outer
RLpolicy
loop).
agent
occupying
acommunication
subsetusing
ofobjectives,
units.
Amaximizing
specific
connectivity
structure
between
layers
(a) state
instantiates
the areofalso
0
0Abstract
2 47 communication
J 39 can
50 , hagents.
52 channel
the
expected
reward
with
gradient,
the
models
trained to minimize the
56 using
baseline
set
to
0.03
in
all
experiments.
40 performed
broadcast
between
agents
and
(b)
propagates
the
agent
in
the
manner
...,1 h
communication
be
standard
backpropagation
(within
the
outer
RL
loop).
1 2 ,47
J ], and computes:
state. channel between agents and (b) propagates the agent state in the manner of
40
broadcast
communication
between
the
baseline
value
and
actual
reward.
Thus,
after
finishing
an episode, we update
i+1with ai state
i 53
i distance
ymous
Author(s)
Anonymous
48 Author(s)
We use policy
[33]
specific
baseline
for
delivering
gradient
to
the
model.
41 gradient
an
RNN.
= fspecific
(hj , cjbaseline
)
(1)
i+1withhjai state
Author(s)
nymous
Author(s)
486
We
use
policy
[33]
for delivering
gradient
to
41 gradient
an
RNN.
2
Model
2
Model
2
6Model
2
Model
ion
54..., the
parameters
by at each
hin
f 3(hij ,Communication
cij )s(1),
(1)theofmodel.
49 6
Denote
the
states
an=episode
by
s(Tmodel
), and
the actionsa✓taken
those states
Affiliation
Affiliation
roduction
j
Model
Abstract
6 Affiliation
2 49 Model
57 by
3iss(1),
Model
42an Because
agents
reward,
but
necessarily
supervision
for of
each
reinforcement
Denote
thea(1),
states
...,will
s(Treceive
), the
andepisode.
the actions
taken
at each
of
those
states
on
2 not
50
as
...,ina(T
),episode
where
Tthe
the
length
of
The
baseline
is a scalar
function
theaction,
!
!2 3
Address
Address
Abstract
42
will
receive
reward,
but
not
necessarily
supervision
for
action,
reinforcement
Xepisode.
T
Teach
T
1to
43
learning
islength
used
expected
future
reward.
We explore
two
forms
of
communication
within
50
as a(1),
..., a(T
),Because
where
Tthe
is agents
the
ofmaximize
the
The
baseline
iscompute
a scalar
function
f
the
X
X
X
smake
i+1
i+1
Address
51
states
b(s,
✓),
computed
via
an
extra
head
on
the
model
producing
the
action
probabilities.
Beside
We
now
describe
the
model
used
to
p(a(t)|s(t),
✓)
at
a
given
time
t
(omitting
the
time
X
@
log
p(a(t)|s(t),
✓)
@
iforms
i
i
email
two
contributions.
we
simplify
and
extend
the
graph
neural
email
cthe
=
hthe
.of
(2)
inetwork
1to
43 the
learning
isconsists
used
maximize
expected
future
reward.
We
explore
two
of
0 consists
work
we
make
two
contributions.
First,
we
simplify
and
extend
the
neural
etwork
simplest
The
7simplest
form
The
of
7First,
simplest
form
the
The
model
of
simplest
form
consists
model
of
form
the
consists
model
of
of
multilayer
model
neural
consists
of
multilayer
networks
neural
of
multilayer
networks
neural
fofcase,
that
networks
neural
take
fr(i)
that
as
networks
input
take
f time
that
vectors
input
take
f i↵ that
as
vectors
input
take
vectors
input
4
j58
jand
i+1
44
the
controller:
(i)
(ii)
continuous.
In
the
former
communication
is✓)as
an
action,
and
7 The
The
simplest
form
of
model
of
multilayer
neural
networks
fgraph
that
take
as
input
vectors
We
now
describe
the
model
used
to
compute
p(a(t)|s(t),
✓)
atcommunication
given
t within
(ommiting
the r(i)
time
517
states
✓),
computed
via
extra
head
on
the
model
producing
the
action
probabilities.
✓multilayer
b(s(t),
b(s(t),vectors
✓) 5 .
52 b(s,
maximizing
the
expected
reward
with
policy
gradient,
the
models
are
also
trained
to Beside
minimize
the
J
1discrete
index
for
Let
s=
the
jth
agent’s
view
the
state
of
the
environment.
The
input
toas
the
email
cii+1
=an
h
.j(ii)
j be
0 brevity).
0 6=
i ??. we
ii ishow how
i+1
0 @✓
0case, (2)
0
j hwill
jto
iof
ithe
ishow
i+1
i+1
i+1
i+1
0 the
0is
0action,
0
0 and
0
0
0to the
. Second,
this
architecture
can
be
used
control
groups
of
cooperating
@✓
44
the
controller:
(i)
discrete
and
continuous.
In
the
former
communication
an
j
ure
Second,
we
how
this
architecture
can
be
used
to
control
groups
of
cooperating
8 hh
and
c
and
output
a
vector
.
The
model
takes
as
input
a
set
of
vectors
{h
,
h
,
...,
h
},
and
45
be
treated
as
such
by
the
reinforcement
learning.
In
the
continuous
case,
the
signals
59
index
for
brevity).
Let
s
be
the
jth
agent’s
view
of
the
state
of
environment.
The
52
maximizing
expected
reward
with
policy
gradient,
the
models
are
also
trained
to
minimize
the
J
and
8 ch 53
and
and
8distance
output
ch and
and
8 aoutput
vector
h and
and
abaseline
output
hcvector
and
.jvalue
The
output
vector
model
. actual
The
a vector
takes
model
. The
takes
model
input
. The
as
atakes
model
set
input
of
avectors
takes
set
input
of1i=t
as
vectors
{h
set
input
,},ofand
avectors
,{h
...,
set
,hcontroller
of
h2vectors
},
,{h
...,
and
,passed
h
h0minput
},
,{h
...,
and
,hh0m02 },
, ...,
and
h0m }, and
j as
t=1
i=t
m {s
1
2 as
between
theModule
reward.
Thus,
after
finishing
an
episode,
we
update
controller
is
the
concatenation
of
all
state-views
s
=
,
...,
s
the
is
a
mapping
1
2
1
1
2
1
0 6=j and
J
th
for
agent
communication
step
CommNet
model is the
33
2concatenation
Problem
Formulation
45
will
be between
treated
such
by
the
reinforcement
In
continuous
the
signals
9
computes
60
controller
iswhere
the
of
all
state-views
s of
=network;
{s
, update
...,case,
sactions
controller
mapping
46
agents
are
no
different
than
hidden
in
athe
neural
thus
credit
539 distance
between
baseline
and
reward.
after
an
episode,
we
J }, and
54 9ithe
model
parameters
✓value
byas
a
= actual
(s),
theThus,
output
alearning.
isinishing
astates
concatenation
discrete
athe
=assignment
{a1 , passed
..., aJ }for
fora each
agent.
computes
computes
9the
computes
i+1
i1 i
i+1
Incomputes
the54case
that
f isparameters
a46single
linear
layer
followed
by
a
nonlinearity
,
we
have:
h
=
(H
+
i be
iperformed
i the
between
agents
no
different
than
hidden
states
in
neural
network;
thus
credit
assignment
for
55
Here
r(t)
is
reward
given
at
time
t,
and
the
hyperparameter
↵Jthe
foreach
balancing
61 are
a
=
(s),
where
output
a
is
a
concatenation
of
discrete
actions
a
=
{a
,
...,
a
}is
for
47
communication
can
using
tandard
backpropagation
(within
the
outer
RL
loop).
j
2
3
j
the
model
✓
by
1
hNote
=that
f (h
, single
cWe
) consider
this
controller
encompasses
individual
controllers
for
each
agents,
asRagent.
well
asthe reward and the
i+1
i+1
i+1
i+1
i+1
i
i
i
i
i
i
i
i
i
i
i
!
!
i
itheiM
j
j
j
2
34
the
setting
where
we
have
agents,
l
cooperating
to
maximize
reward
in
some
ase
fAbstract
is amodel
singlecan
linear
bybecommunication
athat
nonlinearity
we
have:
hjj=
=
(H
T layer followed
T 56
T(h
i that
i
i+1
i j=
h
=
h
fcontroller
(h
, ch
fjmake
(h
,=
csimplifying
h
fjjto
)(T
,ih)cinassumption
fjindividual
)+(hexperiments.
,3the
cithat
)outer
Abstract
47
communication
can
performed
using
standard
backpropagation
RLfor
loop).
baseline
objectives,
set
0.03
all
2
62feedforward
Note
this
encompasses
the
each agents,
as wellofastheir
X
X
X
0 c ) and
j=
j(within
j2h
jcontrollers
j35 single
j,!
j)
C
the
be
viewed
as
network
with
layers
h
h
where
the
between
agents.
@
log
p(a(t)|s(t),
✓)
@
!
X
environment.
We
the
each
agent
receives
R,
independent
j
We use
policy
gradientr(i)
[33] between
with
a state
specific
forb(s(t),
gradient to the model.
4 feedforward
T ✓ =10 48
Tcommunication
T i ibaseline
i delivering
Abstract
ct
i+1
i+1
10
10 be
b(s(t),
✓)
↵
✓) 5a.each
del
X
X
X
63✓) c
the
agents.
nd
the
model can
viewed
as
network
with
hi+1
= ...,
(T
hX
) and
where
hactions
36
In
this
setting,
there
isr(i)
no difference
between
agent having its own controller, or
X
X
=he
h
;layers
@we
log
p(a(t)|s(t),
@X
0 contribution.
two
contributions.
First,
and
extend
graph
neural
network
is
the
concatenation
of
all
hijsimplify
and
T
takes
the
block
form:
j states
jgraph
@✓tanh
@✓
48
We
use Denote
policy
gradient
[33]
with
aStructure
state
baseline
for
delivering
ataken
gradient
to
the
49
the
in
an
episode
byspecific
s(1),
s(T
),i+1
the
at each
of model.
those states
5
4simplify
3.1
Controller
tions.
First,
we
and
extend
the
neural
network
i+1
i+1
i+1
i+1
i+1
i+1
i+1
✓
=
r(i)
b(s(t),
✓)
↵
r(i)
b(s(t),
✓)
.
t=1
i=t
i=t
37
viewing
them
as
pieces
of
a
larger
model
controlling
all
agents.
Taking the latter perspective, our
i
0 6=
c
=
c
h
=
c
;
h
=
c
;
h
=
;
h
;
j
j
nd,
we
show
how
this
architecture
can
be
used
to
control
groups
of
cooperating
tributions.
First,
we
simplify
and
extend
the
graph
neural
network
0
0
0
0
ncatenation
of
all
h
and
T
takes
the
block
form:
@✓
64 ...,
One
obvious
choice
is
a
multi-layer
neural
network,
which
could
extracteach
j3857
jfor
jfully-connected
j
j
j
j
j
50 @✓
as
a(1),
a(T
),
where
T
is
the
ength
of
the
episode.
The
baseline
is
a
scalar
function
theactions,
49
Denote
the
states
in
an
episode
by
s(1),
...,
s(T
),
and
the
actions
taken
at
each
of
those
statestoof
0
1
j
3
Model
First,
e
simplify
and
extend
the
graph
neural
network
i
controller
is
a
large
feed-forward
neural
network
that
maps
inputs
for
all
agents
their
i
i
i
i
mean
i input
t=1
i=t
i=t
how
this
architecture
can
be
used
to
control
groups
of
cooperating
We
now
detail
our
architecture
for
that
communication
without
losing modularity.
is built
m
ofhow
the
model
consists
of
multilayer
neural
networks
fthe
that
take
as
vectors
H),atfeatures
C
C
...
plest
form
of
the
model
consists
ofto
multilayer
neural
networks
that
take
as
input
vectors
0extra
0allows
0the
how
this
can
be
used
control
groups
of
Anonymous
Anonymous
Anonymous
Author(s)
Anonymous
Author(s)
0 architecture
65
from
shyperparameter
and
use
them
o
good
actions
RL
framework.
model
would
55
Here
r(t)
reward
given
time
t, Th
and
↵
isAuthor(s)
for
balancing
reward
and
the
jC
6=
jf
j 0subset
6=
jpredict
jmodel
6=
j
jnetwork).
6=Author(s)
jwith
510
states
b(s,
✓),
computed
via
an
head
on
the
producing
action
probabilities.
Beside
agent
occupying
of
units.
A
specific
connectivity
structure
between
layers
(a) instantiates
the
50
asisand
a(1),
...,
a(T
where
is39cooperating
the
of
the
episode.
The
baseline
isthe
aour
scalar
function
ofThis
the..,
1
i length
1
We
set
c
=
0
for
all
j,
i
2
{0,
..,
K}
(we
will
call
K
the
number
of
hops
in
the
i+1
0
0
0
i
i
i
i
i
i
i
j
i
i+1
0
0
0
from
modules
f
,
which
take
the
form
of
ultilayer
neural
networks.
Here
i
2
{0,
K},
where
Kof
33
2
Problem
Formulation
his
can
be
used
to
control
groups
of
cooperating
tput
a55
vector
h
.
The
model
takes
as
input
a
set
of
vectors
{h
,
h
,
...,
h
},
and
Connecting
Neural
Models
H
C
C
...
C
C
H
C
...
C
B
C
andarchitecture
output
a
vector
h
.
The
model
takes
as
input
a
et
of
vectors
{h
h
,
...,
h
},
and
40
broadcast
communication
channel
between
agents
and
(b)
propagates
the
agent
state
in
the
manner
m
1
2
Affiliation
Affiliation
Affiliation
Affiliation
66
allow
agents
to
communicate
with
each
other
and
share
views
of
the
environment.
However,
it
56
baseline
objectives,
set
to
0.03
in
all
experiments.
52 b(s,
maximizing
the
expected
reward
ith
policy
gradient,
the
models
are
also
trained
to
minimize
the
mreward
1the
2model
Here
r(t)
is
reward
given
at
time
t,
and
the
hyperparameter
↵
is
for
balancing
the
and
the
K
51
states
✓),
computed
via
an
extra
head
on
the
model
producing
the
action
probabilities.
Beside
58
We
now
describe
used
to
compute
p(a(t)|s(t),
✓)
at
a
given
time
t
(ommiting
the time
B
C
2
If desired,
can
take11
the
final
output
them
that
the
model
outputs
awill
vector
0
0 i i hj and
is
the
number
of
communication
steps
in
the
network.
i and
i ij,
i {0,
i so(we
i02
i0
i(we
41
an
RNN.
11
setbaseline
We
c0j we
=
11
set
0 We
cfor
set
all
0set
We
cB
j,
for
and
=
set
allB
0H
i67cj,
for
{0,
all
..,
for
2
K}
and
all
iC
j,
..,
2K}
and
will
{0,
i,to..,
call
2
K}
will
{0,
K
(we
..,
the
call
K}
will
number
K
(we
the
call
number
K
of
the
call
hops
number
K
of
inMthe
hops
the
number
network).
of
in
hops
the
network).
of
inenvironment.
hops
the
network).
in theThe
network).
C
C
H
...
Cvalue
C
...
C59directly,
s We
is=
inflexible
with
respect
the
composition
and
number
oftrained
agents
itfagents,
controls;
cannot
deal
well
Address
Address
Address
Address
C
53
between
the
baseline
and
actual
reward.
Thus,
after
finishing
an
episode,
we
update
56
objectives,
toCdistance
0.03
in
experiments.
j =
jT
jall
52
maximizing
the
expected
reward
with
policy
gradient,
the
models
are
also
to
minimize
the
=
34
We
consider
the
setting
where
we
have
all
cooperating
to
maximize
reward
R
intosome
index
for
brevity).
Let
s
be
the
jth
agent’s
view
the
state
of
the
input
the
3
corresponding
to
each
input
vector,
or
we
can
feed
them
into
another
network
to
get
a
single
vector
or
j
B
C
B
C
Connecting
Neural
Models
i
i
i
i+1
i i i+1
i 68i . with
iKi. iagents
i takes
. K
..Kthe
42
Because
agents
will
receive
reward,
but
not
necessarily
supervision
each
reinforcement
and
leaving
the
group
and
even
order
ofstate
the
must
beaction,
fixed.
On outputs
the
Each
input
vectors
for
each
agent
j:the
thehat
hidden
hthe
and
the
communication
cvector
K
.iand
i 54
email
email
email
email
h
fCdesired,
(h
,Cccan
the
model
parameters
✓joining
by
H
C
A
j{s
j , independent
Bthe
C
35is irectly,
environment.
We
make
the
simplifying
assumption
that
each
agent
receives
R,
of their
=
fh
(h
,f...
cthe
53 desired,
distance
between
the
value
and
actual
reward.
Thus,
after
finishing
an
episode,
we
update
..baseline
.two
j@
j..)
jtake
desired,
12 output.
If 57desired,
12
we
IfModel
we
can
If
we
take
the
we
take
final
can
take
final
the
hthem
output
final
and
h
them
output
and
so
irectly,
them
utput
that
directly,
so
them
model
directly,
the
so
that
outputs
model
so
that
outputs
afor
vector
outputs
model
a vector
awithin
a avector
60
controller
the
concatenation
of
all
state-views
s agents
=
,model
...,
softhe
and
the
controller
is
mapping
T12
==
,
4 Ifscalar
3 can
jand
j ..)houtput
jfinal
1
J },
.
.
j
j
j
j
43
learning
is
used
to
maximize
expected
future
reward.
We
explore
two
forms
communication
BX
C
i+1
Connecting
Neural
Models
69
other
hand,
ifa.no
is used
then
wesetting,
can write
atakes
(s
where
is a its own controller, or
2✓outputs
3
contribution.
In
this
there
is=
no{difference
between
each
agent
having
1 ), ...,
J )},
.. vector,
.. the
..vector,
and
hthem
. 36The
main
body
ofanother
the
model
then
as(s
input
the
concatenated
vectors
!or
. .each
54each
the2the
model
parameters
by
2
iFormulation
iiinput
i ake
ivector,
j (s),
61
=
where
the
output
a is!
athem
concatenation
f
discrete
actions
asingle
{a
...,
aJ } or
for
each agent.
Aacommunication
@
X
he
model
consists
of
multilayer
neural
networks
that
as
input
vectors
..vector
44
the
controller:
(i)
discrete
and
(ii)
continuous.
the
former
case,
communication
is =
an
action,
and
1a,vector
corresponding
13First,
13and
to
corresponding
13
input
to
corresponding
each
input
to
or
we
can
or
vector,
feed
input
we
can
feed
we
into
them
ranother
feed
we
into
can
them
feed
into
network
to
into
get
another
network
to
single
get
avector
network
to
single
get
athe
vector
to
get
or
single
vector
orour
i Model
tions.
we
simplify
extend
neural
network
T graph
Tcan
Ta
we
simplify
extend
graph
57
3 corresponding
33
Problem
C;each
C
...
i+1
i+1and
33
Problem
Formulation
Anonymous
37
viewing
them
asnetwork
pieces
of another
aIn
larger
model
controlling
all
agents.
Taking
the
latter perspective,
X
X
X
70
per-agent
applied
independently.
This
communication-free
model
satisfies
flexibility
5rst,If
each
linear
layer
followed
by
ato
nonlinearity
:or
.c2i+1
.neural
.0Cfnetwork
i+1
0. controller
00
0H
i+1f is a simple c
0 Note
0 Author(s)
@
log
p(a(t)|s(t),
✓)
@
=
h
2
3
i
0set
h
=
[h
,
h
,
...,
h
],
and
computes:
45
will
be
treated
as
such
by
the
reinforcement
learning.
In
the
continuous
case,
the
signals
passed
=
h
;
62
that
this
single
controller
encompasses
the
individual
controllers
for
each agents,
as welleach
as
i
58
We
now
describe
the
model
used
to
compute
p(a(t)|s(t),
✓)
at
a
given
time
t
(ommiting
the
time
vector
h
.
The
model
takes
as
input
a
of
vectors
{h
,
h
,
...,
h
},
and
0
j
j
!
!
1 of
21
JCommNet
1 cooperating
5
4
2
j
Figure
1:
An
overview
of
our
model.
Left:
view
of
module
f
for
a
single
agent
j.
Note
j
how
this
can
be
used
to
control
groups
2
Affiliation
i
i
i
i
38
controller
is
a
large
feed-forward
neural
network
that
maps
inputs
for
all
agents
to
their
actions,
✓
=
r(i)
b(s(t),
✓)
↵
r(i)
b(s(t),
✓)
.
architecture
can
be
used
to
control
groups
of
cooperating
scalar
14
output.
scalar
14
output.
scalar
14
output.
calar
output.
lsimplify
consists
of
multilayer
neural
networks
f
that
take
as
input
vectors
71
requirements
,
but
is
not
able
to
coordinate
their
actions.
First,
wearchitecture
simplify
and
extend
the
graph
neural
network
T Cnetwork
Tagents are no different than hidden states
T in a neural network; thus credit assignment for the
and 59extend
the
graph
neural
C
...iagent’s
H
between
0 6=X
X
X
i+1
i 46
ithe
iAddress
0 6=
j34C
jLet
@✓
@✓
index
for
s
be
the
jth
view
of
the
state
of
the
environment.
The
input
to
the
63
communication
between
agents.
i+1
0
0
0
@
log
p(a(t)|s(t),
✓)
@
34brevity).
We
consider
the
setting
where
we
have
M
agents,
all
cooperating
to
maximize
reward
R
in
some
We
consider
the
setting
where
we
have
M
agents,
all
cooperating
to
maximize
reward
R
in
some
j
j
j
39
agent
occupying
a
subset
of
units.
A
specific
connectivity
structure
between
layers
(a)
instantiates
the
h
=
(A
h
+
B
c
),
i+1 Middle:
i standard
i backpropagation
that
parameters
shared
across
agents.
communication
step,
where
each
58 model
now
describe
the
used
✓)r(i)
atAbstract
all
given
time
(ommiting
.can
The
takes
a✓model
set
ofcooperating
vectors
{h
, hiare
...,
hcommunication
t=1compute
i=t
i=t b(s(t),(within
j and
Abstract
Abstract
5
4We
jito
47
can
be performed
using
the. outer
RL loop).
ssists
architecture
can
be as
used
groups
of
cooperating
m },
1
2 ,jp(a(t)|s(t),
hAbstract
=
(haij single
,independent
cthe
)istime
(1) state in the manner of
=each
b(s(t),
✓)
↵:f nonlinearity
✓)and (b)
ure
beWe
used
to
control
groups
ofthe
i input
i+1
i is
i control
email
j R,
j:agent
environment.
We
make
the
simplifying
that
each
agent
receives
independent
of their
35
environment.
make
the
simplifying
assumption
that
each
receives
of their
the
of
all
state-views
=by
{sassumption
, ...,
},vectors
and
the
controller
athe
mapping
broadcast
communication
channel
between
agents
propagates the agent
J
of
multilayer
neural
networks
f@✓
that
take
as
Anonymous
Author(s)
each
15
fIfi60
each
is
15
a{0,
simple
fIf..,
each
is
ato
linear
simple
fIf
alayer
linear
simple
fjth
is
followed
alayer
linear
simple
followed
by
layer
linear
aStructure
nonlinearity
followed
layer
a140input
nonlinearity
followed
by
nonlinearity
by
aThe
:tor(i)
:multi-layer
hcontroller
=
f15
(h
, csi35ijconcatenation
)is
@✓R,
59
index
for
Let
be
the
agent’s
view
of
the
state
of
the
environment.
input
jagents
0 Ifall
jbrevity).
modules
propagate
their
internal
state
h,
as
well
as
broadcasting
a
communication
vector
c
on
jwill
64is
One
obvious
choice
for
is
fully-connected
neural
network,
which could extract
or
j,
and
i
2
K}
(we
call
K
the
number
of
hops
in
the
etwork).
72
3.1
Controller
X
48
We
use
policy
gradient
[33]
with
a
state
specific
baseline
for
delivering
gradient
to
the
model.
36
contribution.
In
this
setting,
there
is
no
difference
between
each
agent
having
its
own
controller,
or
=
0
for
all
j,
and
2
{0,
..,
K}
(we
will
call
K
the
number
of
hops
in
the
network).
36
contribution.
In
is
setting,
there
no
difference
between
each
agent
having
its
own
controller,
or
t=1
41
an
RNN.
i=t
i=t
61
a
=
(s),
where
the
output
a
is
a
concatenation
of
discrete
actions
a
=
{a
,
...,
a
}
for
each
agent.
6j then the model can be viewed
a feedforward
network
with
1is↵Ja is
0,layers
0s and
0hyperparameter
1
Affiliation
istate-views
55 as
Here
r(t)
is
reward
given
at
time
t,
the
for
balancing
the reward
and the
Anonymous
Author(s)
i+1
i+1
i+1 takes
i is the
i as
iconcatenation
60model
of
all
=
{s
...,
},
and
controller
mapping
The
input
acontroller
set
of
vectors
h
,hi+1
...,
hcontrollers
},
and
1red).
J
of
multilayer
neural
networks
f
that
as
input
vectors
49
Denote
the
states
in
an
episode
by Φ,
s(1),
...,
s(T
),has
and
actionsour
at each
of thosetwo
states
ci+1
=
. theactions
(2)This model would
common
channel
(shown
in
Right:
full
model
showing
input
states
staken
forour
each
K
37 1viewing
them
as
pieces
of
a{h
larger
model
controlling
all
agents.
Taking
the
latter
65
features
from
sthe
and
use
them
predict
with
RLagent,
framework.
0 perspective,
hcontroller
fNote
(hfinal
, cX
)1this
37
viewing
them
as
pieces
of
atake
larger
model
controlling
all
agents.
Taking
theto
latter
perspective,
our
K
i+1
i+1
m
2
1
abstract
abstract
1 Address
abstract
abstract
joutputs
jgood
j
j
62= the
that
single
encompasses
the
individual
for
each
agents,
well
as
i+1
i+1
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
j take
an
the
final
h
and
output
them
directly,
so
that
the
model
outputs
a
vector
Affiliation
d, take
we61can
h
and
output
them
directly,
so
that
the
model
a
vector
56
baseline
objectives,
set
to
0.03
in
all
experiments.
42
Because
the
agents
will
receive
reward,
but
not
necessarily
supervision
for
each
reinforcement
c
=
h
;
J
1
i (A
50
as
a(1),
),
where
T 1B
is,each
the
length
of
the
The
baseline
is a the
scalar
function
of theaction,However,
h=j detail
=
h
=
h...,jagents
ha(T
+
(A
Bmaps
=
hcthat
hinputs
),
+
(A
=
hcjjallthe
+
(A
B
heach
cagents
+
B
cthe
),
jj where
j 0r(t)
a = 63 (s),
thejcontroller
output
aisis
concatenation
of
discrete
actions
a
...,
}for
for
38
controller
isi+1
agiven
large
feed-forward
eural
network
maps
inputs
all
to
their
actions,
each and
0),
73a
We
now
the
architecture
for
that
has
of
communication-free
model but
0
0allow
0
38
aAuthor(s)
large
feed-forward
neural
network
for
agents
actions,
each
J
Anonymous
55
Here
is
reward
at
time
t,
and
the
hyperparameter
↵),
ismodularity
for
the
reward
6=episode.
jagent.
H
(T
H
j=
jcommunicate
jbalancing
jtheir
jfuture
66
to
with
ach
other
and
share
views
of the
it
j),
jthat
j{a
communication
steps
and
the
output
actions
for
agent.
Anonymous
Author(s)
Connecting
Neural
Models
X
the
communication
between
agents.
Connecting
Neural
Models
43
learning
isingle
used
to
maximize
expected
reward.
We
explore
two environment.
forms
of communication within
Address
bstract
model
takes
as
input
a
set
of
vectors
{h
,
,
...,
h
},
and
ail
each input
vector,
or
we
can
feed
them
into
another
network
to
get
a
single
vector
or
0 6=jor
i
51
states
b(s,
✓),
computed
via
an
extra
head
on
the
model
producing
the
action
probabilities.
Beside
nding
to
each
input
vector,
we
can
feed
them
into
another
network
to
get
a
vector
or
j
39
agent
occupying
a
subset
of
units.
A
specific
connectivity
structure
between
layers
(a)
instantiates
i+1
i+1
m
1
2
39
agent
occupying
a
subset
of
units.
A
specific
connectivity
structure
between
layers
(a)
instantiates
the
62 c
Note =
that thish56single
the
individual
controllers
for
achmodules
agents,
as
74 encompasses
stillset
allows
communication.
is built
from
fas
,well
which
take
the
form
of
multilayer
neural cannot
baseline
objectives,
to
in
all
experiments.
; icontroller
67
is
inflexible
with
respect
to
the
composition
and
number
of trained
agents
itminimize
controls;
well
33Problem
2 0.03
Problem
Formulation
Affiliation
0Affiliation
44 expected
the
controller:
(i)
discrete
and
(ii)
continuous.
In
the
former
case,
communication
is an deal
action,
and
33
2
Formulation
j
i+1
j
i
i
multilayer
52email
maximizing
the
reward
with
policy
gradient,
the
models
are
also
to
the
i+1
i
i
i
40
broadcast
communication
channel
between
agents
and
(b)
propagates
the
agent
state
in
the
manner
of
40
broadcast
communication
channel
between
gents
(b)
propagates
thenumber
agent
state
in the
manner
of, we
utput.
imodel
64
One
obvious
choice
for
isInfeedforward
athe
fully-connected
multi-layer
neural
network,
which
could
extract
7 then
where
Tthen
is
written
in
block
form
case
that
f2
is
afeedforward
single
linear
followed
by
agroup
nonlinearity
have:
h
=agents
(H
hmust
+ be
63
communication
between
agents.
16
model
16K}
the
then
can
model
16
be
the
viewed
can
model
be
the
viewed
as
can
anetworks.
be
viewed
as
can
ahops
feedforward
be
as
network
a{0,
as45and
network
awith
feedforward
layers
network
with
network
with
layers
with
layers
ij
Avg.
75
Here
iviewed
..,
K},
where
Ktreated
is
the
communication
layers
inInthe
network.
hithe
=
f026=jneural
(h
,will
)then
jcase,
will
belayer
as layers
such
byof
the
reinforcement
learning.
continuous
thefixed.
signalsOn
passed
sists
of
multilayer
networks
that
ake
as
input
vectors
j, and
{0,
..,
K
of
the
network).
57
Model
68 in
with
agents
joining
and
leaving
the
and
even
the
order
of
the
the
1 (we
abstract
53
distance
between
the
baseline
value
and
actual
reward.
Thus,
after
finishing
anjthe
episode,
we update
multilayer
neural
ffrom
input
vectors
jnetworks
j2the
413take
an the
RNN.
Address
NN
1 jAddress
1call
2Introduction
1fas00number
2Introduction
10to
Introduction
41
anthat
RNN.
i 0We
i+1
i in
i some
i
65
features
h2Introduction
and
use
them
good
actions
with
ourwe
RL
framework.
This
model
would
1
0 the
0the
46where
between
agents
are
nocooperating
different
than
hidden
states
in a reward
neural
network;
thus
credit
assignment foristhe
34
consider
setting
have
M
agents,
all
cooperating
to
maximize
R
i i{h
ipredict
i...,
iand
i+1model
0
C
c
)
and
model
can
be
viewed
as
feedforward
network
with
layers
h
=
(T
h
)
where
h
K
i
i
i
ayer
54
the
model
parameters
✓
by
i
i
69
other
hand,
if
no
communication
is
used
then
we
can
write
a
=
{
(s
),
...,
(s
a
mple
linear
ayer
followed
by
a
nonlinearity
:
The
takes
as
input
a
set
of
vectors
,
h
,
},
j
34
We
consider
the
setting
where
we
have
M
agents,
all
to
maximize
reward
R
in
some
A
B
B
...
B
1
J )}, where
emultilayer
the
final
hjinput
and
output
them
directly,
so
that
the
model
outputs
a
vector
is
a
simple
linear
layer
followed
by
a
nonlinearity
:
takes
as
a
set
of
vectors
{h
,
h
,
...,
h
},
and
iemail
64
One
obvious
choice
for
is
fully-connected
multi-layer
neural
network,
which
could
extract
76
Each
f
takes
two
input
vectors
for
each
agent
j:
the
hidden
state
h
d
the
communication
c
,
communication
range
of
agent
j.
Then
(2)
becomes:
m
1
2
X
i+1
i+1
i
i
i+1
i
i
i+1
i
i
i
Avg.
42
Because
the
agents
will
receive
reward,
but
ot
necessarily
supervision
for
each
action,
reinforcement
hdel
=
f
(h
,
c
)
m
1
2
j
j
66
allow
agents
to
communicate
with
each
other
and
share
views
of
the
environment.
However,
it
neural
networks
fasWe
that
ake
as
vectors
47
communication
can
beeach
performed
using
standard
{0, ..,i+1
K}
(wej will
call42i+1
KBecause
the
number
ofis
hops
in
he
network).
57
3them
Model
35 input
Weof
make
the
assumption
that
each
agent
receives
R,
independent
of(within
their 2 the outer RL loop).
email
iHsimplifying
iR, independent
ibackpropagation
the agents
will
receive
reward,
but
not
necessarily
upervision
for
each
action,
reinforcement
jfeed
er
networks
ffrom
that
take
input
j2neural
H
=
H
(T
=
H
),(T
=
H
H
),
(T
=
H
),
(T
H
),This
Connecting
Neural
Models
iAbstract
ienvironment.
imodel
ivector
Connecting
Neural
Models
35vectors
nvironment.
We
make
the
simplifying
assumption
that
agent
receives
of their
the
concatenation
all
h
and
T
takes
the
block
form
(where
C̄
=
C
/(J
1)):
NN
70a...
per-agent
controller
applied
independently.
communication-free
model
satisfies
the
flexibility
58use
now
describe
the
used
to
compute
p(a(t)|s(t),
✓)
at
given
time
t
(ommiting
the
time
i+1
nput
vector,
or
we
can
into
another
network
to
get
single
or
T
T
T
B
A
B
B
65c features
h
s
and
them
to
predict
good
actions
with
our
RL
framework.
This
model
would
j
B
C
43Anonymous
learning
is
used
o
maximize
expected
future
reward.
We
explore
two
forms
of
communication
within
Author(s)
0
0
0
X
X
X
67
is
to,contribution.
the
composition
and
of
agents
itdifference
cannot
deal
well
=
hthis
;hi+1
77
and
vector
hthis
.there
The
body
ofcontrols;
the
model
then
takes
as
input
the
concatenated
vectors
36
contribution.
In
there
is
between
each
agent
having
its
own
controller,
or a network
0with
0this
0make
43Anonymous
learning
is
used
to
maximize
expected
future
reward.
We
explore
two
forms
ofwe
communication
within
Ktakes
i+1
ithis
imodel
0Introduction
log
p(a(t)|s(t),
✓)
3 inflexible
3of
work
In
3that
we
work
In
3
we
In
two
make
contributions.
two
make
contributions.
we
two
First,
contributions.
two
we
First,
contributions.
simplify
we
First,
simplify
and
First,
xtend
simplify
and
we
the
xtend
simplify
and
graph
the
extend
neural
and
raph
the
extend
neural
etwork
graph
thenetwork
neural
graph
neural network
2aIn
1directly,
Abstract
X
iwork
iwe
ia
j number
odel
as
input
set
vectors
{h
h
,outputs
...,
h
},
and
1main
B
Csetting,
ji allow
j=
36
In
this
setting,
is
o
difference
between
each
agent
having
its own
controller,
or
and
output
them
so
the
outputs
abe
vector
48
We
use
policy
gradient
[33]
with
ait
specific
baseline
for
delivering
ithis
iwork
imake
aX
set
of
vectors
{h
,respect
...,
hAffiliation
},
and
1no
+Author(s)
B
cBbrevity).
),
m
1
i+1
ih
ito
44
controller:
(i)
discrete
and
(ii)
continuous.
the
former
case,
communication
isstate
an
action,
and
71
requirements
,In
but
is
not
able
to
coordinate
their
actions.
for
Let
sthem
jth
agent’s
ofcommunication
the
state
of
the
environment.
The
input
tob(s(t),
the
✓the
=
r(i)
✓)
r(i)
✓)
. gradient to the model.
hAffiliation
(A
h237
+
B
c(ii)
),
i=
m
159,(A
1alashjinput
i 68
i+1
i+1
B
A
...
B
agents
communicate
with
each
other
share
views
of
the
environment.
However,
jcontinuous.
jthe
jdiscrete
jagents
B
C
viewing
as
pieces
of
aDenote
larger
model
controlling
all
agents.
Taking
the
latter
perspective,
our
jarchitecture
jand
44
the
controller:
(i)
and
In.how
the
ormer
isb(s(t),
an
action,
and
j2index
with
joining
and
leaving
the
group
and
even
e
order
ofview
the
must
be
fixed.
On
the
hi6617
=
f i17
(h
,where
carchitecture
) 17
Tdescribe
=
✓case,
✓ of
0Inagents
1
=
f
(h
,
c
)
4
4 another
architecture
4
of
??.
architecture
Second,
4
of
??.
Second,
we
of
??.
show
Second,
we
of
how
??.
show
this
Second,
we
show
this
we
how
architecture
show
this
can
how
architecture
be
used
this
can
architecture
to
control
used
can
be
to
groups
control
used
can
be
to
groups
control
used
cooperating
to
of
groups
control
cooperating
of
groups
cooperating
of cooperating
h
(3)
c
=
i+1
i+1
j
j
j
0
49architecture
the
states
in
an
episode
by
s(1),
...,
s(T
),
and
the
actions
taken
at
each
of those states
37
viewing
them
as
pieces
of
a
larger
model
controlling
all
agents.
Taking
the
latter
perspective,
our
0be
58
We
now
the
model
used
to
compute
p(a(t)|s(t),
✓)
at
a
given
time
t
(ommiting
the
time
B
C
where
T
where
is
written
T
is
written
in
block
T
where
is
written
in
orm
block
T
is
written
in
form
block
in
form
block
orm
i
i
i
i
45
will
be
treated
as
such
by
the
reinforcement
learning.
the
continuous
case,
the
signals
passed
j
j
tor,
or
we
can
feed
them
into
etwork
to
get
a
single
vector
or
j
t=1
j
i=t
i=t
j
=
6
j
Address
1
1
abstract0 3 with
60 be
controller
the
all
state-views
scontinuous
=network
{s
, graph
...,
},theand
the
controller
a mapping
.isuch
..concatenation
.reinforcement
H
C̄
...signals
C̄ for
67
inflexible
respect
to
the
composition
and
of
agents
itof
controls;
cannot
deal
well
1
38
controller
isincludes
a large
feed-forward
neural
that
maps
inputs
toistheir
actions,
each is a scalar function of the
=i 1isfollowed
45
will
treated
astwo
by
learning.
the
case,
passed
Assuming
the
identity
agent
j.
|N
(j)|
Inonlinearity
this
we
make
contributions.
simplify
and
xtend
neural
Address
69 hother
ifwork
no
communication
is the
used
then
weof..we
can
write
=
{neural
(s
),the
...,
(sC̄
)},
isall
a agents
@
A
j number
.First,
1 network;
JJ
50 states
as In
a(1),
...,
a(T
where
iswhere
the
length
of
the
episode.
The
baseline
near
layer
byabstract
a;hand,
:agents.
ji beX
j5feedforward
38
controller
is
adifferent
large
neural
network
maps
inputs
fornetwork
all C
agents
toinput
their
actions,
ach
X
46 5between
agents
are
no
than
hidden
in
athe
thus
assignment
for
the
agents.
5 feedforward
agents.
5. Let
agents.
0),
.feed-forward
i that
i (j)
iT credit
+1
email
ibeas
iviewed
an
viewed
a
network
with
layers
.
.
59
index
for
brevity).
s
be
the
jth
agent’s
view
of
state
of
the
environment.
The
to
the
j
∈N
model
can
as
a
network
with
layers
B
i
j
39
agent
occupying
a
subset
of
units.
A
specific
connectivity
tructure
between
layers
(a)
instantiates
the
61
a
=
(s),
where
the
output
a
is
a
concatenation
of
discrete
actions
a
=
{a
,
...,
a
}
for
each
agent.
0
0
0
0
1
1
1
1
46
between
agents
are
no
different
than
hidden
states
in
a
neural
etwork;
thus
credit
assignment
for
the
C̄
H
C̄
...
C̄
0
0
0
0
4
architecture
of
??.
Second,
we
show
how
this
rchitecture
can
be
used
to
control
groups
of
cooperating
i+1
i+1
1
J
68
with
agents
joining
and
leaving
the
group
and
even
e
order
of
the
agents
must
be
fixed.
On
the
email
c0j )78per-agent
5572aHere
is
reward
given
time
, and
hyperparameter
for
balancing
thethe
reward
and theprobabilities. Beside
i r(t)
i of
istandard
i specific
iat
iconnectivity
i the
ian
ibetween
i isthe
controller
applied
independently.
This
communication-free
model
atisfies
the
flexibility
3.1
Structure
51i A
states
b(s,
✓),
computed
via
extra
head
on
producing
the action
hj=, =
cj c)fj h(h
B
C loop).
h =
,47...,
h
andagent
computes:
communication
can
be performed
using
backpropagation
the
outer
RL
39
occupying
subset
units.
structure
layers
(a)model
instantiates
0j ,;70
i
i A
i
iController
=
h
;0 ,47i[h
1 ,hh0Abstract
J ],
B
B
B
A
...
B
B
A
...
B
B
B
...
B
B
...
i(within
i (within
i (b)
i CB
ji+1
ji+1
j5010if
B
B
Bof
...
A
controller
the
concatenation
allA
state-views
sB
=
{s
sexpected
},
and
the
controller
is
mapping
40single
broadcast
communication
channel
between
agents
and
propagates
the
agent
state the
in
manner
ofalso trained to minimize the
B
78
h
=requirements
[h
,60h
...,
and
can
be
performed
using
standard
backpropagation
the
outer
RL
1J2],K
agents.
i communication
i is
i is
0,followed
..,69X
K}
will
the
number
hops
in
network).
1 ,C̄...,
C̄
H
...
C̄
62
Note
that
this
controller
ncompasses
individual
controllers
each
agents,
asthe
well
j 0 6=
56 of
baseline
objectives,
set
to
0.03
in
experiments.
2 call
Figure
1:
Blah.
other
no
communication
is
used
then
we
can
write
athe
=
{maximizing
...,
(sall
)},
where
isloop).
a for
52
the
reward
with
policy
gradient,
models
i+1
inot
bstract
71ahand,
but
to
coordinate
their
actions.
h
=
(A
h
),computes:
1
JJ
T(sibaseline
=),the
,the
i+1
icommunication
B
C
40iable
broadcast
agents
and
(b)(j)
propagates
the
agent
state
in
the manner
ofasare
er
by(we
nonlinearity
:, B
j +
jWe
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i+1and
0j6=
ichannel
i the
i between
Anonymous
Author(s)
Anonymous
Author(s)
H
=
(T
H
),
48the
use
policy
gradient
[33]
with
a
state
specific
for
delivering
gradient
to
model.
As
agents
move,
enter
exit
environment,
N
will
change
over
time.
In
this
setting,
41
an
RNN.
H
=
(T
H
),
j
j
0
C
.. 2for
. and
h
=
fnow
(hjA
,detail
)...
(1)
61
a=
(s),
where
theRNN.
a is
aA
concatenation
of
discrete
actions
a..B
=B
{a
, ...,
amodel.
}modularity
for
each agent.
Bbetween
B
B
A
B
B
...
A
B
...
B
. .value
B
B
B
B
C
C
C
Cstates
i+1Anonymous
+1i+1
2 j 6=
1jAnonymous
Introduction
63 use
the
communication
53jB
distance
between
the
baseline
actual
eward.
Thus,
finishing an episode,
we update
1...
J
i+1
i state
iagents.
iB
Author(s)
Author(s)
per-agent
controller
applied
independently.
This
communication-free
model
satisfies
flexibility
K
73
We
the
architecture
has
the
of
the after
communication-free
model
but
486
We
policy
[33]
with
baseline
forB
delivering
athe
gradient
to
the
.. B
41 gradient
an
@
A
2
Model
2that
Model
2so
6
Model
2output
hin
fjainterpretation
(h
,can
cispecific
)s(1),
(1)
=
;Introduction
.each
49 6
Denote
the
states
an
episode
by
s(T
and
the
actions
taken
at
of
those
Affiliation
8h and
The
key
isi6call
that
T
is
dynamically
sized,
and
the
matrix
be
dynamically
the
Affiliation
0
.because
. that
.N
2 hidea
output
them
that
the
model
a),individual
vector
joutputs
B
B
B
B
C
Ceachiasaction,
;70
jModel
j1Affiliation
i=
ijModel
i...,iand
ias
ii C
isized
igraph,
iC
ibeing
our
model
has
natural
aiB
dynamic
with
(j)
the set
vertices
12
i+1
i6 Affiliation
idirectly,
54
model
parameters
✓ built
by
K}
(we
will
K
the
number
of
hops
in
the
network).
Abstract
62
Note
this
single
controller
encompasses
the
controllers
for
each
agents,
well
astakeofthe
viewed
a(we
feedforward
network
with
layers
j 0 key
42
Because
the
agents
will
receive
reward,
but
not
necessarily
supervision
for
reinforcement
49
Denote
the
states
ina(T
an),B
pisode
by
s(1),
...,
s(T
),
the
actions
taken
at
each
of
those
states
Figure
1:
Blah.
Model
i
i
i
57number
3
71asidea
requirements
,
but
is
not
able
to
coordinate
their
actions.
i motivates
iis B
The
is
that
T
is
dynamically
sized.
First,
the
of
agents
may
vary.
This
B
B
B
A
B
B
...
A
B
B
..
A
B
B
...
A
...
B
74
still
allows
communication.
is
from
modules
f
,
which
form
of
multilayer
neural
h
=
(A
h
+
B
c
),
79
B
B
B
C
C
C
C
50
as
a(1),
...,
where
T
is
the
length
of
the
episode.
The
baseline
scalar
function
of
the
72
3.1
Controller
Structure
0,
..,
K}
will
call
K
the
number
of
hops
in
the
network).
Address
en
block
form
jform
j 64
Address
Abstract
C̄necessarily
C̄is a C̄
Hfor
9(we
blocks
are
applied
by type,
rather
than
coordinate.
One
obvious
choice
for
is 1to
areceive
fully-connected
multi-layer
neural
network,
which
could
extract
isin
written
infeed
block
T
=
Tjsimplify
=
T
=
T
=The
. network
.the...xplore
. represent
. communication
42 by
will
reward,
but
not
supervision
each
action,
reinforcement
will
K we
the
number
ofa(1),
hops
in
the
network).
X
j 0jcan
6=j79call
43 B
learning
islength
used
maximize
future
reward.
We
two
of
within
50
as
...,
a(T
),Because
where
Tthe
is agents
the
the
episode.
The
baseline
scalar
function
fforms
the
connected
to
vertex
at
the
current
time.
edges
within
the
communication
2
B
B
B
C
C
C
63
the
communication
between
agents.
or
we
them
into
network
to
get
single
vector
or
Address
i+1
i+1
Address
3 the
In
this work
make
two
contributions.
First,
the
graph
states
b(s,
✓),
computed
via
an
extra
head
on
the.xpected
the
action
probabilities.
Beside
T of
T
normalizing
factor
Jmake
151ianother
in7simplest
equation
(2),
which
resacles
the
communication
vector
by
the
75
networks.
Here
ifuture
2
{0,
..,
K},
where
K
is
number
of
communication
layers
invectors
the
network.
X
i+1
i65
ithe
i
..form
.and
..ofextend
..of
..model
..continuous.
..neural
.compute
..In
.. RL
..graph
Kthe
email
email
cthe
=
h
(2)
X
X
X
.Tproducing
.C
.✓)
1to
0 consists
43 h
learning
is
used
maximize
expected
reward.
We
explore
two
forms
within
features
rom
and
use
them
to
predict
good
actions
with
our
This
would
3 them
In this
work
we
two
contributions.
First,
we
simplify
and
xtend
the
graph
neural
etwork
i.
0
1smodel
The
7simplest
The
form
The
of
7simplest
form
the
The
of
simplest
consists
model
of
form
the
consists
model
of
multilayer
the
model
of
multilayer
neural
consists
of
multilayer
networks
neural
of
multilayer
networks
neural
fframework.
that
networks
neural
take
f..aticommunication
that
as
networks
input
take
fmodel
that
vectors
input
take
f i the
that
as
vectors
input
take
as
input
vectors
H7??.
=
(T
H
),
output
directly,
so
that
the
odel
outputs
a
vector
j58
j.and
og
p(a(t)|s(t),
@
@
@
@
A
A
A
A
i+1
i+1
0
1
44
the
controller:
(i)
discrete
(ii)
the
former
case,
communication
isas
an
action,
and
and
output
hem
so
that
the
model
outputs
a
vector
abstract
51
states
b(s,
✓),
computed
via
an
extra
ead
on
the
model
producing
the
action
probabilities.
Beside
i
i
i
i
.
.
.
.
.
.
.
.
.
7directly,
The
simplest
form
of
the
model
consists
of
multilayer
neural
networks
f
that
take
as
input
vectors
We
now
describe
he
model
used
to
p(a(t)|s(t),
✓)
given
time
t
(ommiting
theof
time
i
i
i
i
52
maximizing
the
expected
reward
with
policy
gradient,
the
models
are
also
trained
to
minimize
the
tput
them
directly,
so
that
the
model
outputs
a
vector
J
1
s
a
feedforward
network
with
layers
mail
4
architecture
of
Second,
we
show
how
this
architecture
can
be
used
to
control
groups
of
cooperating
email
c
=
h
.
(2)
channel
between
agents,
with
(3)
being
equivalent
to
belief
propagation
[23].
Furthermore,
use
.
.
.
.
73 isWe
now
detail
the
architecture
for
that
has
the
modularity
of
the
communication-free
model
but
0.based
A
B
B
...
B
✓
=
r(i)
b(s(t),
✓)
r(i)
b(s(t),
✓)
.
idea
is
that
T
dynamically
sized.
First,
the
number
of
agents
vary.
This
motivates
0may
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A
B
...
B
number
of
communicating
agents.
Second,
the
blocks
are
applied
on
category,
rather
than
by
j
j
i
i
i
i
+1
i+1
i+1
i+1
0
0
0
0
0
0
0
0
0
0
0
0
44
the
controller:
(i)
discrete
and
(ii)
continuous.
In
the
former
case,
communication
is
an
action,
and
i
i
i+1
0
0
0
j
=
6
j
72
3.1
Controller
Structure
abstract
}
(we
will
call
K
the
number
of
hops
in
the
network).
i
i
architecture
??.
we
show
how
this
architecture
can
be
used
to
control
groups
of
allow
agents
to
communicate
with
each
other
and
share
views
of
the
environment.
However,
ithand
64of
One
obvious
choice
for
is
awith
fully-connected
multi-layer
neural
which
could
extract
45
will
be
treated
as
such
byinput
the
reinforcement
learning.
In
the
continuous
the
signals
52
the
expected
reward
policy
gradient,
the
models
are
also
trained
to
minimize
the
Jh
1i .single
index
for
brevity).
Let
be
the
jth
agent’s
view
of
he
tate
of
The
input
to,hh
the
hofand
c66
aand
vector
hand
.a
The
model
takes
as
a. sset
of
vectors
, network,
...,
h
},
and
8 8 into
8 maximizing
cSecond,
h
and
and
8output
output
h
and
8 to
aoutput
vector
h
and
output
c59vector
The
output
vector
.(NIPS
vector
takes
model
The
as
takes
model
input
.vectors
The
as
a{h
takes
model
set
input
of
as
avectors
takes
set
input
of
as
avectors
{h
set
input
,the
ofcase,
aenvironment.
vectors
,{h..,
set
of
hm
vectors
},
,{h
...,
and
h
,{h
...,
and
, ...,
and
hm }, andcij ,
llrfeed
call
K 4the
number
hops
in
the
network).
✓cooperating
iThe
jthinput
53
distance
between
the
baseline
value
actual
Thus,
after
an
we
update
76
fmodel
takes
two
agent
j:
h,passed
the
communication
we
can
feed
them
another
network
get
a Module
ingle
vector
or
m
1for
2 ,each
1 he
2hidden
1 ,h
2tate
1
2 },
2 },
0 6=ji and
iand
iand
5Submitted
agents.
j ✓m
iinetwork
inets
can
feed
them
into
another
to
get
aand
vector
or
them
into
another
network
to
get
single
vector
or
t=1
jEach
74this
still
allows
communication.
is
built
from
modules
,our
the
form
of
multilayer
neural
i=t case,
i=t1Li m
ivalue
ifreward.
iwhich
i reward.
itake
icommunication
ivector
ifinishing
i continuous
iepisode,
iof
29th
Conference
on
Neural
Information
Processing
Systems
2016).
Do
not
distribute.
for
agent
step
CommNet
model
multi-layer
atC
each
vertex
makes
model
similar
to
an
instantiation
the
GGSNN
work
of
45
will
be
treated
as
such
byB
the
reinforcement
learning.
In
the
the
signals
passed
lock
form
B
A
B
...
B
normalizing
factor
J
1
in
equation
(2),
which
resacles
the
communication
by
the
i65computes
i53
coordinate.
In
simple
form
of
the
model
“category”
refers
to
either
“self”
or
“teammate”;
but
as
B
A
...
B
5 i+1
agents.
46
between
agents
are
no
different
than
hidden
states
in
a
neural
network;
thus
credit
assignment
for
the
B
C
60
controller
is
the
concatenation
of
all
state-views
=
{s
,
...,
s
},
and
the
controller
is
a
mapping
9B
distance
between
the
baseline
and
actual
Thus,
after
inishing
an
episode,
we
update
B
B
B
B
B
...
B
B
A
B
...
B
B
A
...
B
A
...
A
67
is
inflexible
with
respect
to
the
composition
and
number
of
agents
it
controls;
cannot
deal
well
1
J
54
the
model
parameters
✓
by
features
h
from
s
and
use
them
o
predict
good
actions
with
our
RL
framework.
This
model
would
i+1
9
computes
9
computes
9
computes
9
computes
H sodirectly,
=80networks.
(T
),that
ithe
utput
them
so
the
model
outputs
a 61isvector
C
n1 directly,
B
C
75that
Here
ial.
2fAii[15].
{0,
where
K
the
number
communication
layers
ini+1
the
network.
m
the
model
a...
vector
icase
ioutputs
77
and
outputs
vector
hstates
. in
The
main
body
of
model
input
theagent.
concatenated
vectors
InBH
the
that
isparameters
a..,
single
linear
layer
followed
by
aof
have:
h
=
(H
hij(within
+
i+1
i be
iperformed
46
between
agents
are
no
ifferent
than
hidden
neural
network;
thus
credit
assignment
iK},
73 see
We
now
the
for
that
has
the
modularity
of
the
communication-free
model
but
aimore
=
(s),
where
output
ajis,“broadcast
iswe
concatenation
ofiidiscrete
athen
=
{a1akes
,RL
...,for
aas
}is
for
47
communication
can
using
tandard
backpropagation
the
outer
loop).
et
0
1
54 iarchitecture
model
✓iC
by
i detail
we
will
below,
the
communication
can
be
complicated
than
to
Jthe
Introduction
of
communicating
agents.
Second,
blocks
are
applied
based
on
category,
than
llowed
by
aTthe
nonlinearity
:ithe
B
B
(h
,nonlinearity
cija)the
i=
iB
ithe
iarchitecture
55
Here
r(t)
given
at
time
the
foreach
balancing
the reward and the
i+1
B
68
with
joining
and
leaving
the
and
even
f
the
agents
must
be
fixed.
the
i+1
i+1
i+1
i+1
B
B
A
B
irather
ireward
iathe
iorder
iby
ijthe
iall”,
it, and
i actions
i hyperparameter
jgroup
B
C
2 However,
jby
66
agents
to
communicate
with
and
share
views
of
environment.
itOn
....
A
B
B
Bagents
80
In
case
fAbstract
is
a...
single
linear
followed
aeach
nonlinearity
,i (h
we
have:
hmultilayer
=
(H
h
+
=
.i=
T layer
Tother
T(h
i that
ii Tallow
i+1
i j=
ij
=
h
f
=
,
c
h
f
)
(h
=
,
c
h
f
)
,
c
f
)
(h
,
c
)
Abstract
n
feed
them
into
another
network
to
get
a
single
vector
or
47
communication
can
be
performed
using
standard
backpropagation
(within
the
outer
RL
loop).
62feedforward
Note
that
this
single
controller
encompasses
the
individual
controllers
for
each
agents,
as
well
as
j
j
B
C
X
X
X
10
j
j
j
j
j
j
i
i
troduction
81
C
c
)
and
he
model
can
be
viewed
as
network
with
layers
h
=
(T
h
)
where
h
j
j
j
j
B
C
m
into
another
network
to
get
single
vector
or
74
still
allows
communication.
is
built
from
modules
f
,
which
take
the
form
of
neural
log
p(a(t)|s(t),
✓)
56invariant,
baseline
objectives,
set
to of
0.03
all
experiments.
X
so218
may
categories.
Note
also
Tto
isto
permutation
thus
the
order
the
2dynamically
..18
..The
..Tthat
m
76i18
Each
fjof
kes
two
vectors
for
agent
j:
the
idden
state
hagents.
d
the
c✓)j , (s
ate.
In
this
simple
the
model
refers
either
“self”
or
“teammate”;
as
ihand,
ther
if48
no
communication
isand
used
then
we
can
write
=r(i)
{inencoding
(s
),
...,
where
abecause
.is
..input
..=
..cand
We
use
policy
gradient
[33]
with
state
specific
baseline
for
delivering
a.)},
gradient
to
the
lowed
by681Model
arequire
nonlinearity
:i69Skip
6nonlinearity
key
The
idea
key
ismore
The
that
idea
Tinflexible
is
is
that
idea
dynamically
key
T
is
that
idea
dynamically
is
is
sized,
hat
dynamically
T
is
ized,
dynamically
matrix
sized,
the
and
can
matrix
sized,
the
and
can
matrix
ynamically
the
dynamically
can
matrix
be
can
be
dynamically
because
the
sized
the
sized
the
the
0Jsized
. For
1the
T“category”
Tcommunication
Tbut
67
iskey
with
respect
the
composition
and
number
agents
it
controls;
cannot
deal
well
i communication
ibe
i 1sized
jbe
i+1
i+1
Abstract
@
A
10
10
10
10
✓C
b(s(t),
✓)
b(s(t),
yand
aThe
:iform
A
...
B
.be
A
63
the
between
2Abstract
Model
X
X
X
.each
some
tasks,
itr(i)
is
to
have
the
hbecause
as
anis model.
input
for because
isimplify
CB
cB
the
model
can
viewed
s
feedforward
network
with
hi+1
=of
(T
hX
)input
where
hactions
Assuming
suseful
includes
the
identity
of
agent
j.
h
;layers
X
X
X
jstate
log
p(a(t)|s(t),
✓)
.we
In
this
work
we
make
two
contributions.
First,
and
extend
he
graph
neural
etwork
j present
j ) iand
.@
.and
75does
networks.
iconcatenation
2B{0,
..,
K},
isbe
the
number
of=
communication
ayers
in
the
network.
.applied
j states
j episode
82
is
the
of
hcan
T
takes
the
block
form:
i+1
✓tanh
✓
i+1
. Connections:
.all
.type,
48
We
use
policy
gradient
[33]
with
pecific
baseline
forb(s(t),
delivering
ataken
gradient
to
the
model.
B
C
49
Denote
the
in
an
by
s(1),
...,
s(T
),i+1
and
he
at each
of
those states
not
matter.
jK
0
1
70vector
er-agent
controller
independently.
This
communication-free
model
satisfies
the
flexibility
ake
two
contributions.
First,
we
simplify
and
extend
the
graph
neural
network
i Here
i .applied
iwhere
i+1
i+1
i+1
i+1
i+1
i+1
i+1
see
below,
the
communication
architecture
more
complicated
than
“broadcast
to
all”,
dagents
by
a
nonlinearity
:
✓
=
r(i)
b(s(t),
✓)
r(i)
✓)
.
t=1
77
and
outputs
a
h
.
The
main
body
of
the
model
then
takes
as
input
the
concatenated
vectors
i
i
i
i
i=t
i=t
68
with
agents
joining
and
leaving
the
group
and
even
the
order
f
the
agents
must
be
fixed.
On
the
blocks
19
are
blocks
applied
19
are
blocks
19
by
type,
are
blocks
applied
by
rather
type,
are
applied
than
by
rather
type,
by
than
by
coordinate.
rather
by
coordinate.
han
rather
by
than
coordinate.
by
coordinate.
i
i
B
B
A
...
B
h
=
(A
h
+
B
c
),
j
=
6
j
j
B
C
cgroups
=
=
cafor
=j 0episode.
cthe
h=j step
; itaken
;have:
ctarchitecture
iishow
ihow
A82 of=is
B
B First,
B
??.
Second,
we
can
be
used
to
control
of
work
contributions.
First,
we
simplify
extend
graph
neural
network
communication
beyond
the
first
Thus
agent
at
i,hwe
0The
64 the
One
obvious
choice
for
multi-layer
neural
which
could
i+1
iihjarchitecture
i. Denote
i in
iithe
i ...
concatenation
ofjrequirements
all
and
T and
takes
the
block
form:
T
jB
1 d
✓the
✓j;fully-connected
j iwe
jtheir
j; j actions
jis0s(T
j 0 at
50
as
a(1),
...,
a(T
),graph
where
T
isccooperating
the
length
of),
the
baseline
is a network,
scalar
function
the extract
49
states
an
episode
by
s(1),
...,
and
each
of those
statesof
ii,isteps
i layer.
B
Bthis
...
A
i make
itwo
wo
we
simplify
extend
the
neural
network
iiform
iiModel
B
B
...
A
C
iable
ijthen
71
but
is
not
to
coordinate
actions.
57
3
h
=
(A
hwe
+
cthe
),
iwrite
mean
t=1
i=ticontrol
i=t
The
simplest
of
model
consists
of
multilayer
neural
tworks
fthe
that
as
input
vectors
may
require
more
Note
also
that
Tand
is
permutation
invariant,
hus
the
order
of
the
0multilayer
Second,
we
show
how
architecture
can
be
used
to
groups
of
cooperating
76
Each
two
vectors
for
agent
j:
the
hidden
state
htake
and
the
communication
c ,reward
iarchitecture
69
ther
hand,
no
communication
is
used
we
can
=
{balancing
(s
),vectors
...,
(sisJthe
where
is aofThis
.model
..input
..ifB
j.categories.
jthis
H
C)computed
C
...
(A
h
+7Second,
B
cB
j7the
by
acontributions.
nonlinearity
:iB
.55
agents.
The
simplest
form
of
the
model
consists
of
networks
that
take
as
input
0
65
features
h
from
shyperparameter
and
use
them
actions
our
RL
framework.
model would
1producing
0good
0the
ture
of
??.
show
how
this
can
be
to
control
of
At
first
of
the
function
h
=
p(s
isneural
used.
This
as
input
state-view
@
A
jfi),
(t)
iseach
reward
given
atK}
time
t, T
and
isthe
for
and
the
inearity
B
...
B
jC0jtakes
6=
jf
j0 o
6=pisode.
jpredict
jmodel
6=
j the
jnetwork).
6=jjwith
51 used
b(s,
✓),
via
an
extra
head
on
action
probabilities.
..takes
.encoder
B
C
11 .We
set
can
=
0Here
for
all
j,
istates
2
{0,
..,),
(we
will
call
K
the
number
of
hops
in
50
as
a(1),
...,
a(T
where
iscooperating
the
length
of
the
The
baseline
a)},
scalar
function
theBeside
jgroups
j
ij B: layer
i iA
i+1
0
0
0
j
.
i
i
i
i
i
i
i
.
.
.
i
i+1
0
0
0
Assuming
s
includes
the
identity
of
agent
j.
i+1
ih
0...,model
jih
8 not
hshow
a55
vector
. dis
The
takes
as
input
ainset
of
vectors
{h
,vectors
h2fthe
,isi...,
h{h
and
d,oeswe
this
architecture
can
be
used
to
groups
iand
ivector
ioutput
Band
C
66
allow
agents
to
communicate
with
and
share
views
ftrained
the
environment.
However,
Hmaximizing
C
...
Ccontrol
H
C
C
matter.
8and
and
c12
a56
hmodel
. given
The
model
takes
as
input
a...irectly,
etof
of1cooperating
,other
hthe
},
and
70
per-agent
controller
applied
independently.
This
communication-free
satisfies
the
flexibility
m
2the
ii chhow
i output
KC
baseline
objectives,
set
to
0.03
all
experiments.
52
the
expected
reward
with
policy
gradient,
models
are
also
to minimize
the time
hB0i+1
.vector
The
main
body
of
the
model
then
takes
as
input
concatenated
vectors
mreward
h
=
(h
, 1},each
hmodel
).the
(4) itt (ommiting the time
2
r(t)
ath
time
and
hyperparameter
for
balancing
and
0ireward
51
states
b(s,
✓),
computed
via
anC58
extra
head
one
the
model
producing
action
probabilities.
Beside
i T 77
Ifc...
desired,
we
can
take
the
final
and
output
them
that
the
odel
outputs
athe
vector
B
B
A
eedforward
network
with
layers
hcomputes
+
Baand
),layers
iHere
hat
is
dynamically
sized,
and
the
matrix
can
be
dynamically
sized
because
jcomposition
j ,dependent,
jthe
B
C
We
now
describe
the
used
to
compute
p(a(t)|s(t),
✓) at
a given
jrespect
srd
and
vector
h
R
for=
some
diand
).
form
of
the
encoder
is(we
problem
idea
isoutputs
that
Toutputs
is
sized,
and
the
matrix
be
dynamically
sized
because
0
0
0t,is
j The
i ij,
i {0,
i so
eedforward
network
with
iset
ican
i0
i(we
i(A
iidynamically
0
Tj=
=icomputes
jfeature
j11
inflexible
with
the
and
number
of agents
it episode,
controls;
cannot
deal
well
B
B
B
...
A
9network
jcj0(in
11
We
set
We
=
11
set
0
We
c1Controller
for
11
set
all
029th
We
cj,
for
=
all
0H
i67j,
for
2
and
=
all
..,
for
2
K}
and
all
j,
..,
2another
K}
and
will
{0,
i,to..,
call
2
K}
will
{0,
K(we
..,
the
call
K}
will
number
K
(we
the
call
will
number
K
of
the
call
hops
number
K
of
in(NIPS
the
hops
the
number
network).
of
in
hops
the
network).
of
in
hops
the
network).
in
theThe
network).
with
layers
C
C
H
...
Cvalue
C
C{0,
...
C
72
3.1
Structure
ihi9 +
B
C
53
distance
between
the
baseline
and
actual
reward.
Thus,
after
finishing
an
we
update
71
requirements
,
but
is
ot
able
to
coordinate
their
actions.
56
baseline
bjectives,
et
to
0.03
n
all
xperiments.
=
(A
B
c
),
j
j
j
j
13
corresponding
to
each
input
vector,
or
we
can
feed
them
into
network
to
get
a
single
vector
or
52
maximizing
the
expected
reward
with
policy
radient,
the
models
are
also
trained
to
minimize
the
T
=
ction
Submitted
Submitted
to
29th
Submitted
Conference
to
29th
Submitted
Conference
to
29th
n
Neural
Conference
to
on
Information
Neural
Conference
on
Information
Neural
Processing
on
Information
Neural
Processing
Systems
Information
Processing
Systems
(NIPS
Processing
2016).
Systems
(NIPS
Do
2016).
Systems
(NIPS
not
distribute.
Do
2016).
not
distribute.
Do
2016).
not
distribute.
Do
not
distribute.
0
2
Model
59
index
for
brevity).
Let
be
the
jth
agent’s
view
f
the
state
of
the
environment.
input to the
.
.
.
.
j
j
+
B
c
),
. .coordinate.
d by
than
by
joining
and
leavingstate-view
the group jand
even e order of the agents must be fixed. On the
i+1
i i i+1
ij embedding
iis
but
fortype,
most
of
tasks
they
consist
of a lookup-table
bags
of
vectors
thereof).
Unless
rst
layer
the
model
an
encoder
function
=fCthe
p(s
)c68=
used.
This
takes
as
input
re
applied
by
rather
than
by
coordinate.
@
j of
iKi. iagents
i(or
.. rather
..our
.. If
.. A
..) fwith
..) K
..K
1 type,
K
.iand
ihjagent
54
model
parameters
✓
by
C
H
...
C
h
=
(h
,
14
scalar
output.
.
(h
,
c
i+1
i
i
53
distance
between
the
baseline
value
and
actual
reward.
Thus,
after
finishing
an
episode,
we
update
.
.
Assuming
s
includes
the
identity
of
j.
j
j
12
desired,
12
If
desired,
12
we
can
If
desired,
12
we
take
can
If
the
desired,
we
take
final
can
the
h
we
take
final
and
can
the
h
output
take
final
the
h
them
output
final
and
h
irectly,
them
output
and
so
irectly,
them
utput
that
directly,
the
so
them
that
model
directly,
the
so
that
outputs
model
the
so
that
outputs
model
a
vector
the
outputs
model
a
vector
outputs
a
vector
a avector
j
60
controller
is
the
concatenation
of
all
state-views
s
=
{s
,
...,
s
},
and
the
controller
is
mapping
j
i+1
i
i
T
=
,
57
3
Model
jhand,
j . jif no
0 Hand
j 69 .sized
1
J )}, where
.
2
other
communication
is
used
then
we
can
write
a
=
{
(s
),
...,
(s
is
a
odel
1is dynamically
i noted,
i sized,
.
.
j
j
j
the
matrix
can
be
dynamically
because
the
1
J
H
=
(T
),
0
d
otherwise
c
=
0
for
all
j.
H
=i of13jFirst,
(T
H
),0ii Model
Temporal
Recurrence:
We
also
explore
having
the
network
be
aathem
recurrent
eural
network
(RNN).
10
73
We
now
detail
the
architecture
for
that
has
the
modularity
ofThis
he
communication-free
model
but
=
(T
),B
.. form
.. the
..vector,
.. feed
10H
rward
network
with
layers
i form
i model
i encoder
.nonlinearity
54 ach
the
model
parameters
by
utputs
feature
vector
h
(in
R
for
some
d13
The
of
the
isanetwork
problem
dependent,
2
X
61
=
(s),
where
the
output
a is
concatenation
ofmodel
discrete
actions
a
=get
{aor
...,
aJ } or
for
each agent.
iX
i✓
i ake
ivector,
ward
network
with
layers
0 ).
implest
the
consists
of
multilayer
neural
networks
f
that
as
input
vectors
.
1a,vector
70
per-agent
controller
applied
independently.
communication-free
satisfies
the
flexibility
B
B
...
A
corresponding
13
13
to
corresponding
input
to
corresponding
each
vector,
input
to
each
or
input
to
we
each
can
or
vector,
input
we
can
them
or
feed
we
into
can
them
r
another
feed
we
into
can
them
another
network
feed
into
another
network
to
into
get
a
another
network
to
single
get
a
vector
network
to
single
get
or
a
vector
to
single
single
vector
or
15
each
fcorresponding
a and
simple
linear
layer
followed
by
a
:
eThe
make
two
contributions.
First,
we
simplify
and
extend
graph
neural
jIfwe
T
T
T
eype,
two
contributions.
simplify
extend
the
graph
neural
network
57
3
C
C
C
...
H
ork
with
layers
rather
than
by
coordinate.
.
72
3.1
Controller
Structure
i
i+1
i+1
X
X
X
.ci+1
. ; replacing
. the
i+1
i
0
0 Note
0
log
p(a(t)|s(t),
✓)
162
74
till
allows
communication.
is
built
from
f Unless
which
take
the
form
ofand
multilayer
i individual
c14
=
hacontrol
This
is
achieved
by
communication
step
iactions.
Eqn.
(1)
by 2aneural
time
that
controller
encompasses
the
controllers
fort,each agents, as well as
hbags
;{h
0=
58 lookup-table
We
now
the
model
used
to
✓)single
at
a, given
time
tin
(ommiting
the
time
hi and
cour
and
output
vector
hi+1
.can
The
takes
as
input
et
of
vectors
, p(a(t)|s(t),
...,
hmodules
}, this
and
0 compute
requirements
,cooperating
but
is
not
able
to
coordinate
their
most
of
tasks
they
consist
of
embedding
(or
ofH
vectors
thereof).
jdescribe
K
joutput.
jsimply
jDo
Figure
overview
our
model.
Left:
view
of
module
for(2)
ab(s(t),
single
agent .step
j. Note
??.
Second,
we
show
how
this
architecture
can
be
used
to
groups
1
2vectors
i1:
itake
ihdistribution
✓An
=iiof
r(i)
b(s(t),
✓)
r(i)
✓)
ond,
we
show
how
this
architecture
bemodel
used
to
control
groups
cooperating
14amodel,
scalar
14
output.
scalar
14
output.
scalar
output.
calar
mplest
form
ofon
the
model
consists
of
multilayer
neural
networks
f71
that
as
input
two
contributions.
First,
we
simplify
and
extend
the
graph
neural
network
i+1
i of
iof
iCommNet
T is
T m not distribute.
T
Conference
Neural
Information
Processing
Systems
(NIPS
2016).
not
distribute.
ntributions.
First,
we
simplify
and
extend
the
graph
neural
etwork
the
output
of
the
decoder
function
q(h
)
used
to
output
a
over
the
space
of
C
C
C
...
dAt
o
29th
Conference
on
Neural
Information
Processing
Systems
(NIPS
2016).
Do
+1
i
i
t
t
X
X
X
h
=
(A
h
+
B
c
),
j
0 6=
i+1
imatrix
the
communication
between
agents.
✓be
59be
index
for
brevity).
Let
sbecause
be
jth
agent’s
of✓)number
the
state
of
the
nvironment.
The
input
to the
the network.
75
networks.
Here
2jof
{0,
..,
where
K
is
ofstep,
communication
ayers
in
iicomputes
0are
0shared
0view
log
✓)
j 63
j the
i output
jiet
jj2
j 0K},
6=
jp(a(t)|s(t),
sized,
and
the
can
dynamically
sized
and
using
the
same
fthe
all
At
every
time
actions
will
sampled
from
q(h
=
(T
that
the
parameters
across
alli=t
Middle:
a single
communication
step,
each
58
We
now
describe
the
used
to
compute
p(a(t)|s(t),
at
aagents.
given
time
t (ommiting
the
time
cshow
and
vector
hi+1
model
takes
as
input
a✓model
vectors
{h
,the
...,t.
h✓the
and
t=1
i=t b(s(t),
se
noted,
c0j aH
=
0),ifor
all
j.The
j ). Note
H
=
(T
H
),.orm
ond,
we
show
how
this
architecture
can
be layer
to
groups
of1for
m },
2 ,has
73
We
now
detail
that
of
the
communication-free
model
Tmically
),
istate-views
=each
r(i)
✓)the
✓)but
. where
how
this
architecture
can
be
to
control
groups
ofmodule
cooperating
i the
iconcatenation
ifor
0
1
i control
is
the
of
all
smodularity
=by
{s
, produce
...,
sb(s(t),
},vectors
is
athe
mapping
actions.
q(.)
of
ai60each
single
network,
followed
by
a by
softmax.
To
afor
discrete
iNeural
itakes
iithe
iused
Jnvironment.
ofH
model
consists
of
multilayer
neural
networks
fcooperating
that
take
as
72
3.1
Controller
15
If
each
15
fIf
is
15
ai+1
simple
fIfused
each
is
15
aarchitecture
linear
simple
If
alayer
linear
simple
fjth
is
followed
alayer
linear
simple
followed
layer
linear
aStructure
nonlinearity
followed
layer
a1iinput
nonlinearity
followed
by
aand
: as
nonlinearity
bycontroller
:asnonlinearity
:tor(i)
:multi-layer
hcontroller
=
f(NIPS
(h
, c2016).
)is
1
taThe
i
i vector
✓
✓
er
than
by
coordinate.
59
index
for
brevity).
Let
s
be
the
agent’s
view
of
the
state
of
the
input
ence
on
Processing
Systems
Do
not
distribute.
jf
jas
es
64
One
obvious
choice
is
fully-connected
neural
network,
jcan
i the
i11set
0Information
A
B
B
B
160 ...
then
the
model
be
viewed
a
feedforward
network
with
layers
agents
modules
propagate
their
internal
state
h,
well
broadcasting
a
communication
c which could extract
j
that
agents
can
leave
or
join
the
swarm
at
any
time
step.
If
f
is
a
single
layer
network,
we
obtain
76
Each
f
takes
two
input
vectors
for
each
agent
j:
the
hidden
state
h
and
the
communication
c
,
We
set
c
=
0
for
all
j,
and
2
{0,
..,
K}
(we
will
call
K
the
number
of
hops
in
the
network).
11
We
c
=
0
for
all
j,
and
i
2
{0,
..,
K}
(we
will
call
K
the
number
of
ops
the
network).
t=1
i=t
i=t
still
allows
communication.
isisastate-views
built
from
modules
,from
which
take
the
form
multilayer
neural
B0B
...
a=
(s),
where
the
output
is
a concatenation
discrete
actions
= {a
a }isof
foror
each
agent. the
1
0of
0sfhand
0hyperparameter
j 74ithe
j framework.
1 , ...,
j i+1
55
Here
r(t)
reward
given
at
time
t,
the
balancing
reward
the
action,
sample
this
distribution.
Kon
i+161
i as
iconcatenation
iconsists
iBifrom
60model
isiq(h
the
f
all
{s
,red).
...,
},
and
controller
isjJpredict
a mapping
iwe
iB
65=
features
sthe
and
use
them
to
good
actions with
our and
RL
ut
vector
h
.aof
The
takes
input
acontroller
set
ofi+1
vectors
{h
h
,i+1
...,
hcontrollers
},
and
1input
J
model
multilayer
neural
fobjectives,
that
take
vectors
aingle
common
channel
(shown
in
Right:
full
model,
showing
input
states
for
each
agent,
two
tbut This model would
hcontroller
fNote
, ciX
)2
utput
of
model,
decoder
function
)networks
isi+1
used
to
output
a ith
distribution
over
the
space
of
K
i the
ias
i+1
m
1other.
2the
i+1
A
B
...
ia B
ithe
ican
K(hfinal
B
...
B
j=
jj
C
62= B
that
this
encompasses
the
individual
for
each
agents,
as
well
as
i+1
i+1
i+1
73
We
now
detail
architecture
for
that
has
the
modularity
of
the
communication-free
model
i
i
i
i
i
i
i
i
i
i
i
i
i
i
jplain
RNNs
that
communicate
each
In
later
experiments,
we
also
use
an
LSTM
as
an
f
75
networks.
Here
{0,
..,
K},
where
K
is
he
number
of
communication
layers
in
the
network.
12 A...
If
desired,
we
can
take
the
h
and
output
them
directly,
so
that
model
outputs
a
vector
H
=
(T
H
),
56
baseline
et
to
0.03
in
all
experiments.
c
h
;
12
If
desired,
we
take
the
final
h
and
output
them
directly,
so
that
the
model
outputs
a
vector
AB
B
B
77
and
outputs
vector
h
.
The
main
body
of
the
model
then
takes
as
input
the
concatenated
vectors
C 61 ai=C63 (s),
j output
hj and
=ime
(A
=jagents
h
+
Bisto
=
hcbuilt
h),
+
(A
B, ...,
=
hcjmodules
+(A
B
heach
cfjbalancing
+
Bother
cjthe
),and
jj where
66hdiscrete
allow
with
share
views
However, it
j r(t) is
the
a is
of
actions
a
}isfor
agent.
0the
0
0hyperparameter
i ),
ja concatenation
1
J
B takes
55
Here
reward
given
at communication.
t,
and
the
for
reward
theof the environment.
j=
jcommunicate
ja),
jeach
joutput
j(A
j{a
communication
steps
actions
for
each
i i+1
X
the
communication
between
agents.
74
still
from
which take
the
formand
of multilayer
neural
i13entire
iAito
iawhich
iainput
ctor
.corresponding
The
takes
as
acontroller
setthem
of
vectors
{h
,h
...,
},
and
CB
Thus
model,
we
Communication
Neural
Net
takes
theagent.
stateq(.)
form
of
single
layer
network,
followed
by
aallows
softmax.
To
produce
avector
discrete
iB
imodel
i the
iithe
i1
toSystems
each
input
vector,
or
we
can
feed
them
another
network
to
get
vector
or,well
B
B
...
B
jcan
i+1
i+1
C
m(i)
1(CommNN),
2
13
corresponding
each
input
vector,
or
we
feed
into
another
network
to,{0,
get
ah
single
or
1
67
is
inflexible
with
respect
the composition
agents
it controls; cannot deal well
62
this
encompasses
he
controllers
forsingle
agents,
as
as and number
B
A
...
B
ithat
i number
iof
Neural
Information
Processing
(NIPS
2016).
Do
not
distribute.
C
=
.call
iBB
AiB
...
B
B
...
B
baseline
objectives,
seteach
tointo
0.03
inindividual
experiments.
cNote
=
h56
;6=jinput
C
17
where
Tjmodule.
isfwritten
injsingle
block
form
0i
75
networks.
Here
iall
2
.., K},
where
K
iseach
theto
of
communication
layers
in
the
network.
0
76
Each
takes
two
vectors
for
agent
j:
the
hidden
state
and
the
communication
c
,
i+1
i
i
multilayer
B
C
.
...
B
1
1
j layers
j of the agents must be fixed. On the
14 from
scalar
output.
view
of
all
s,multilayer
passes
itthe
the
encoder
h
p(s),
(ii)
iterates
h
and
in
equations
(1)
i=
64
One
obvious
choice
for
a fully-connected
neural
network,
which
could
extract
0i
68
agents
joining
and
leaving
the group
and
even
the order
..agents
..16
..neural
we
sample
this
C
63
communication
between
agents.
then
16
then
model
16K}
the
then
can
model
be
the
viewed
can
be
the
viewed
as
can
model
ais
feedforward
be
viewed
as
can
ahops
feedforward
be
viewed
aswith
network
aj.multi-layer
feedforward
asnetwork
acwith
feedforward
layers
network
with
network
with
layers
with
layers
C
ij57
14B
Avg.
includes
the
identity
of
agent
i .@
ithrough
hithe
=
f0 6=Assuming
(h
,will
)sthen
.and
jmodel
iset
iioutput.
ithe
i distribution.
ijneural
orm
of
the
model
consists
of
multilayer
networks
fas
that
vectors
=
0 for
all
j,
{0,
..,
(we
call
K
the
number
of
in
the
network).
3take
Model
A
j16
the
model
of
fcfrom
that
input
vectors
jnetworks
..ciB
..K
j2
.scalar
.C
i take
iinput
i as
i
i
j consists
i+1
A=
B
...
B
i We
...ii+1
B
NN
A
B
B
A
...
AEach
B
B
...q(h
BKi hand,
76 main
f0ipredict
takes
two
for
each
the
hidden
state
dwrite
the communication
65 actions
features
h
s and
use
good
actions
with
RL
framework.
This
model
would
..A
.B
B
..C
.take
.houtputs
other
if our
no
communication
isconcatenated
used
then
wehjcan
a = { (s1 ), ...,cj ,(sJ )}, where is a
0 of69
0 input
...
Bobain
77 . model
and
alayer
vector
hfor
.agents,
body
the
model
then
takes
asagent
inputj:could
the
vectors
(2)
to
h
,The
(iii)
samples
a
for
all
according
to
).vectors
i+1
i+1
0The
0a them
0to
K
itakes
iB
iC
multilayer
.
.
output
a
vector
h
.
as
input
a
set
of
vectors
{h
h
,
...,
h
},
and
.
.
ican
j
C
i
i
i
i
15
If
each
f
is
a
simple
linear
followed
by
nonlinearity
:
If
desired,
we
the
final
and
output
them
directly,
so
that
the
model
outputs
a
vector
aeand
vector
h
The
model
takes
as
input
a
set
of
vectors
{h
,
,
...,
h
},
and
i
64
One
obvious
choice
is
a
fully-connected
multi-layer
neural
network,
which
extract
A
key
point
is
that
T
is
dynamically
sized
since
the
number
of
agents
may
vary.
This
motivates
thesatisfies the flexibility
m
1
2
X
i
i
0
i+1
i+1
i
i+1
i
i
i+1
i
i
i
i+1
i
3
Related
Work
Avg.
h
=
f
(h
,
c
)
jlayer
15
Ififor
each
fmultilayer
isand
a of
linear
followed
by
a
onlinearity
:
m
1 77ake
2vectors
B
C
B
A
Bthe
...=
B(i)
66
allow
agents
to
communicate
with
each
other
and
share
views
of
the
environment.
However,
it
ij ntire
i
i
i
i j,...
isimple
i
C
he
model
consists
multilayer
neural
networks
f
that
as
input
vectors
model,
which
we
call
Communication
Neural
Net
(CommNN),
takes
the
statec
=
0
all
2
{0,
..,
K}
(we
will
call
K
the
number
of
hops
in
network).
57
3
Model
70
per-agent
controller
applied
independently.
This
communication-free
model
and
outputs
a
vector
h
.
The
main
body
of
the
model
then
takes
as
input
the
concatenated
vectors
i
i
j
j
B
A
B
el
consists
of
neural
networks
f
that
take
as
input
j
H
H
(T
=
H
H
),
(T
=
H
H
),
(T
=
H
),
(T
H
),
C
i+1
j ivector
BA
BAi input
A
..ii+1
....C
i ...
NN
Bvector
B
...
58use
We
now
describe
the
used
to
p(a(t)|s(t),
✓)the
at
a given
time t (ommiting
.features
.cB
corresponding
to...each
vector,
orC
we is
can
feed
them
into
another
network
toA
single
or1offramework.
BB...
ifactor
igood
imodel
65
h.. inflexible
from
si+1
and
them
predict
actions
with
ourcompute
RL
This
would
C
0B
0get
J
1ain
equation
(2),
which
rescales
communication
vector bythethetime
number
@
A
B
B
i
0;normalizing
67
respect
the
composition
and
agents
it able
controls;
cannot
deal
=
hincludes
B
...
Bcnumber
i+1
0with
0oT
0to0to
Ktakes
.output
0the
71
requirements
but
is not
to model
coordinate
theirwell
actions.
ih
imij.
h
. The
model
as
input
set
of
vectors
,brevity).
, ...,
hLet
},
and
=
. equations
ji allow
j{h
all
s,C
passes
the
encoder
hjoining
=
(ii)
iterates
in
(1)
ed,
we
can
take
final
and
them
that
the
model
outputs
aand
vector
h
. The
as
a1X
of
vectors
...,
h{h
},
and
2order
.iC
..agents
.. ..hthrough
i+1
iagents
i ai+1
ito
i ,the
ip(s),
i1
Assuming
sagents
identity
agent
for
sh
be
the
jth
agent’s
view
of
the state
of
the
environment.
The
input
the
iof
.the
jinput
hhi+1
=
(A
h2ijof
B
c. jand
),
output.
.
.
.
1 +
m
159,communicating
jdirectly,
i+1 it
i66
communicate
with
each
other
share
views
ofTthe
environment.
However,
itOn
jwith
iscalar
i ..model
itakes
iset
A
.
.
.
j2index
68
with
and
leaving
the
group
and
even
the
the
agents
must
be
fixed.
the
.
h
=
f
(h
,
c
)
=
(A
h
+
B
c
),
.
Our
model
combines
a
deep
network
reinforcement
learning
[9,
21,
14].
Several
recent
.
of
agents.
Note
also
that
is
permutation
invariant,
thus
the
order
of to
theworks
agents
Assuming
s
includes
the
identity
of
agent
j.
3.2
Model
Extensions
j
h
=
f
(h
,
c
)
i+1
i+1
j
j
j
j
j
.
.
.
.
.
0
K
K
j
A
Bto
A
...
B
58
We
now
describe
the
model
used
to
compute
p(a(t)|s(t),
✓)
at
a
given
time
t
(ommiting
the
time
17
where
17
T
where
is
17
written
T
where
is
17
written
in
block
T
where
is
written
in
form
block
T
is
written
in
form
block
in
form
block
form
.
jcan
jC
jsamples
onding
to
each
input
vector,
or
we
feed
them
into
another
etwork
to
et
a
single
vector
or
.
. .B
.
.
j
=
6
j
.
.
.
.
60
controller
is
the
concatenation
of
all
state-views
s
=
{s
,
...,
s
},
and
the
controller
is
a
mapping
ly
sized,
and
the
matrix
can
be
dynamically
sized
because
the
obain
h
,
(iii)
actions
a
for
all
agents,
according
to
q(h
).
67
is
inflexible
with
respect
to
the
composition
and
number
of
agents
it
controls;
cannot
deal
well
1
J
=
h
;
i ahand,
.Bf iimatrix
0by
69 .
other
if nonot
communication
is used
then we domains,
can write a =
{ (sas
..., (s
where
isAtari
a
. athen
1 ), Go
J )},26]
If each
is
simple
linear
layer
followed
nonlinearity
: network
ji i idynamically
does
matter.
j sized
and
can
be
because
the
X
i Let
ij with
ithe
iController
have
applied
these
methods
to
multi-agent
such
[17,
and
games
[31],
but
i+1
B
B
A
i ...
ibe
iviewed
72
3.1
Structure
X
C
59as
index
for
brevity).
s
be
jth
agent’s
view
of
the
state
of
the
environment.
The
input
to
the
utput.
ithe
16 ii+1
the
model
can
a
feedforward
layers
i
i
B
B
B
...
A
61
a
=
(s),
where
the
output
a
is
a
concatenation
of
discrete
actions
a
=
{a
,
...,
a
}
for
each
agent.
i+1
i+1
1 i J
with
joining
andnetwork
leaving
the independently.
group
the
the
=
falternative
(h
casagents
ievenThis
i communication-free
iorder
i of
i the
i agents
ii must
imodel
i besatisfies
iifixed.i On
controller
applied
thei...
flexibility
feedforward
with
layersand
B
BAhthe
A
.. and
.... hi+1
.=0j6=,A
i 16
(h
, be
cj c)68viewed
an
bythen
coordinate.
j )aper-agent
.j=.An
jcan
hto
; controller
ji model
A
B
B
B
A
...
B
B
B
A
...
B
B
B
...
BtheB
Local
Connectivity:
the
broadcast
framework
described
above
is
to
agents
ji+1
ji+1
hother
;70
jK}
j 0if
ordinate.
60
the
concatenation
ofhops
allA
state-views
=allow
{s
, ...,
},2 and
controller
aamapping
1 K
i 62
i is
i is
i..=2fclayer
..,69X
(we
will
the
number
of
in
the
1communication.
assume
full
visibility
of
the
environment
and
lack
isagents,
rich
j {0,
Note
that
this
single
controller
encompasses
individual
controllers
forB
eachis
as literature
well as
j.0they
i ...
Figure
1:
Blah.
noicall
communication
is
used
then
we
can
write
adetail
=
{isnetwork).
(s
),the
...,
(ssiJJ)},
where
is
aThere
71ahand,
requirements
,
but
not
able
to
coordinate
their
actions.
h
=
(A
h
+
B
c
),
1architecture
i+1
i
2
.
fall
is aj,
simple
linear
followed
by
nonlinearity
:
j
j
.
.
i
i
i
i
i
i
i
i
i
i
i
i
i
j
0
73
We
now
the
for
that
has
the
modularity
of
the
communication-free
model but
18
The
key idea
is
dynamically
sized,
and
the
can
be
the
X
H
H
),
j 6=is
i+1
iLet
i=
61 T a
=
where
the(T
output
a matrix
iscommunication-free
aA
concatenation
discrete
actions
aB
=the
{a1robotics
, .., aJB} fordomain
each agent.
to
communicate
others
aj that
certain
range.
(j)
beThis
the
set
ofBagents
present
within
odel
Extensions
B
B
Adynamically
Bthe
...of
B
A
BB...sized
B
A
Bbecause
...in
B
...
j 0 6=
i+1
63
the
communication
between
agents.
70j within
per-agent
controller
applied
independently.
model
satisfies
the
flexibility
K
i+1
i+1andto
Htype,
= (s),
(Tthan
Hso
),N
on
multi-agent
reinforcement
learning
(MARL)
[2],
particularly
[19,
27,form
6, of multilayer neural
ci+1
=
hfeedforward
; icall
lly
sized,
the
matrix
can
be
dynamically
sized
0applied
74 because
still
allows
communication.
is
built
from
modules
f ias
, which
take the
19h and
are
by
rather
by
coordinate.
d,
and
the
matrix
can
be
dynamically
sized
because
the
2
take
the
final
h
output
them
directly,
that
the
model
outputs
a
vector
c
=
;
i
i
i
i
j
0blocks
j
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
1
i+1
i
i
i
and
i
2
{0,
..,
K}
(we
will
K
the
number
of
hops
in
the
network).
j
62
Note
that
this
single
controller
encompasses
the
individual
controllers
for
each
agents,
well
as
j
then
the
model
can
be
viewed
as
a
network
with
layers
j
Figure
1:i Blah.
matrix
be..,
sized
because
the
i the
i may
communication
range
agent
Then
(2)
becomes:
71 A
requirements
,isB
but
able
to
coordinate
their
actions.
83 {0,
The
idea
is
that
T+
dynamically
sized.
First,
the
number
agents
vary.
This
motivates
B
Bnetworks.
A
BT
B
...
Ai 2B
B{0,
Bmulti-layer
.....,A
B
Bwhere
...neural
A
Bisnetwork,
...
B which
h
=
(A
hjcall
cis
),64not
Bach
Bij,can
B
...
72
3.1
Controller
Structure
for
all
i dynamically
2
.,key
K}
(we
will
K
number
of
hops
in
jthe
One
obvious
choice
for
isBiaof
fully-connected
could
75 network).
Here
K
the. number
of
layers in the network.
17
where
Tor
isof
written
inj.
block
form
Tthe
=
T
=
T
=
=
.K},
.[13,
. communication
22,
3].
Amongst
fully
cooperative
algorithms,
many
approaches
16,
35]
avoid
the extract
need for
j, and
2and
{0,
will
K
the
number
of
hops
in
network).
j 0jcan
6=jcall
0 6=
lan
Information
Systems
(NIPS
Do
not
distribute.
jK}
j (we
oordinate.
632016).
the
communication
between
input
vector,
we
feed
them
into
network
to
get
ato
single
vector
by
coordinate.
17 Processing
where
T
isSystems
written
in
block
form
KProcessing
84alternative
the
normalizing
factor
J 65
1ianother
inthat
equation
(2),
which
resacles
communication
the
ion
(NIPS
2016).
Do
not
distribute.
i+1
onnectivity:
An
to
the
broadcast
framework
described
above
to
.. agents.
..a vector
.. is
..the
..aallow
..vector
.. agents
....actions
... or
.. vector
.... . .ourby
.. RL
.. framework.
.. This model would
Kthe
.
.
.
features
h
from
s
and
use
them
predict
good
with
X
Kfinal
H
=
(T
H
),
he
final
h
and
output
them
directly,
so
the
model
outputs
i
i
can
take
the
h
and
output
them
directly,
so
that
the
model
outputs
.
.
1
i
i
i
i
communication
by
making
strong
assumptions
about
visibility
of
other
agents
and
the
environment.
ke
the
final
h
and
output
them
directly,
so
that
the
model
outputs
a
vector
jviewed
e j,
model
can
be
a
feedforward
network
with
layers
jas
76
Each
f
takes
two
input
vectors
for
each
agent
j:
the
hidden
state
h
and
the communication cij ,
Submitted
o
Conference
on
Neural
Information
Processing
Systems
(NIPS
2016).
Do
not
distribute.
i+1
i+1
.
.
.
.
7329th
We
now
detail
the
architecture
for
that
has
the
modularity
of
the
communication-free
model
but
83
The
key
idea
is
that
T
is
dynamically
sized.
First,
the
number
of
agents
may
vary.
This
motivates
j
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
j
A
B
B
...
B
85
number
of
communicating
agents.
Second,
the
blocks
are
applied
based
on
category,
rather
than
by
72K
Controller
Structure
and
i 2
{0,
.., will
K}
(wecan
will
call
the
number
ofi (j)
hops
in
ihops
iinallow
ifor
66
agents
to
communicate
withor
each
other
and
share
views
of the environment.
However, it
municate
toor
others
within
a3.1
certain
range.
Let
N
beto
agents
present
within
64of
One
obvious
choice
isof
a fully-connected
multi-layer
neural
network,
which could extract
cK
=
hBthe
.set
(3)
iinput
2each
{0,
..,
K}
(we
call
the
number
the
network).
0the
i
to
input
vector,
or
we
feed
them
into
another
network
get
anetwork).
single
vector
A1communication,
B
B
...
i+1
jdynamically
jvector
i
i
i
put
vector,
or
we
can
feed
them
into
another
network
o
get
a
single
vector
or
vector,
we
can
feed
them
into
another
network
to
get
single
or
74
till
allows
communication.
is
built
from
modules
f
,
which
take
the
form
of
multilayer
neural
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
ed,
and
he
matrix
can
be
sized
because
the
Others
use
but
with
a
pre-determined
protocol
[32,
20,
39,
18].
77
and
outputs
a
vector
h
.
The
main
body
of
the
model
then
takes
as
input
the
concatenated vectors
where
T
is
in
block
form
84written
the
the
normalizing
factor
J
in
equation
(2),
which
resacles
the
communication
vector
by
the
i+1
i
i
86
coordinate.
In
this
simple
form
of
the
model
“category”
refers
to
either
“self”
or
“teammate”;
but
as
|N67(j)|
Bhinflexible
A s and
Bwith
Bo
K
B
B
B
B
B
...actions
BB
A
B
...
Blayers
B
ARL
...
A
...
A model
is
respect
topredict
the B
composition
and
of B
agents
it controls;
cannot
deal well
j number
65
features
them
good
with
our
framework.
This
would
i),
ifrom
i use...
Kfinal
H sodirectly,
=
(T
H
0 2N (j)
nication
range
of87them
agent
j.73them
Then
(2)
becomes:
e the
h
and
output
so
that
the
model
outputs
a isvector
75
networks.
Here
i 2Bji{0,
..,
where
K
the
number
ofcategory,
communication
in to
theall”,
network.
B
Aioutputs
B
nal
hlinear
and
output
directly,
that
the
model
a...
vector
j number
iK},
iare
imore
We
now
detail
architecture
for
that
has
the
modularity
of
the
communication-free
model
but
j Processing
we
will
see
below,
communication
can
be
complicated
than
“broadcast
85 layer
of
communicating
agents.
Second,
the
blocks
applied
based
on
rather
than
by
ple
followed
by
a
nonlinearity
:
i
ation
Systems
(NIPS
2016).
Do
not
ithe
ithe
i 68 distribute.
iarchitecture
with
agents
joining
and
leaving
the
group
and
even
the
order
f
the
agents
must
be
fixed.
On
the
B
B
A
...
B
66
allow
agents
to
communicate
with
each
other
and
share
views
of
the
environment.
However,
it
Ai another
BB
B vector
coordinate.
iB
i ...
ito
i
T(NIPS
=
i 1 Assuming
input
orfeed
we88Processing
can
feed
them
into
network
get
ainvolve
single
vector
or
essing
Systems
(NIPS
2016).
DoA
not
distribute.
ilearning
sjthen
includes
agent
ito
i the
few
notable
approaches
to
communicate
agents
under
partial
visibility:
orvector,
we86in
canblock
them
into
another
network
get
single
or
A
...
B
allows
communication.
is
built
from
modules
f .dynamically
,the
which
take
the
form
ofbetween
multilayer
neural
X
lctor,
Information
Systems
Do
not
distribute.
so74
may
more
categories.
Note
also
that
isto
permutation
invariant,
thus
theidentity
order
the
Tsimple
is
written
form
76
Each
f of
kes
two
input
vectors
for
j:
the
idden
state
hthe
d
the
communication
cij , (s
coordinate.
In
this
simple
the
model
“category”
refers
either
“self”
or
“teammate”;
but
as
ihand,
69
ther
if
no
communication
isand
used
we
can
write
=dynamically
{ (sj.
), dynamically
...,
)},
where
is abecause
..2016).
.T
..and
linear
layer
followed
bystill
arequire
nonlinearity
:iB
T18
=
.hat
18
18
key
The
idea
key
is
The
that
idea
18
Tinflexible
is
The
is
that
idea
dynamically
key
T
is
is
idea
dynamically
is
is
sized,
dynamically
Tagent
isized,
matrix
sized,
the
and
can
matrix
sized,
and
can
matrix
ynamically
the
beofof
can
matrix
be
can
because
be
dynamically
because
the
sized
the
sized because
the
the
167
.T
1sized
Jsized
iskey
with
respect
the
composition
and
number
of
agents
it
controls;
cannot
deal
well
jbe
linear
layer
followed
byand
aThe
nonlinearity
:iform
B
A
... K},
B
i+1
i+1
..that
.to.ach
75does
networks.
Here
i 2B
{0,
..,
where
K.. isbethe
number
of communication
layers
in to
theall”,
network. tabular-RL
.
.
.
i+1
i+1
.
.
i
i
i
i
c
=
h
.
(3)
.
89
agents
not
matter.
Kasai
et
al.
[11]
and
Varshavskaya
et
al.
[34],
both
use
distributed
approaches
for
0
70
per-agent
controller
applied
independently.
This
communication-free
model
satisfies
the
flexibility
i
i
i
i
we will
see
below,
the
communication
architecture
can
more
complicated
than
“broadcast
ear layer87followed
by
a
nonlinearity
:
77(A
atype,
vector
h
.i. The
main
body
of
hegroup
model
thenthan
takesthe
as input
thef concatenated
vectors
.
.
.
.
.
iBand
ioutputs
i... applied
jiiare
jtype,
68 +
with
agents
joining
and
leaving
the
and
even
order
the
agents
must
be
fixed.
On
the
19
blocks
19 A
blocks
applied
19
are
blocks
applied
19
by
are
blocks
by
rather
are
applied
than
by
rather
type,
by
than
by
coordinate.
rather
type,
by
than
coordinate.
rather
by
coordinate.
by
coordinate.
B
A
B
h
=
h
B
c
),
j
B
B
...
B
3
.
.
.
i+1
i
i
i
i
T =
. 1i
j
|Nj(j)|
j

Abstract

Connecting
Connecting
Connecting
Neural
Connecting
Neural
Models
Neural
Models
Neural
Models
Models

Connecting
Neural
Models
Connecting
Neural
Models

nnecting
Neural
Models
Connecting
Neural
Models

0

0

0

0

i 76=
i Each
i ifhi categories.
i the
B
B
B,[7]
...
A
71
requirements
but
is
able
toinvariant,
coordinate
actions.
h(A
(A
+:iB
cj ),
0 2N
so may
more
Note
also
that
Teach
is
permutation
thus
order
the
0 not
vectors
for
agent
the
state
htheir
dwrite
the
communication
tasks.
Jim
use
an
algorithm,
rather
69
ther
hand,
no
communication
is volutionary
used
then
we
can
a of
=state-view
{ (s1 ),than
...,cij , (sreinforcement
)}, where is alearning.
j.takes
. two
hai+1
hfirst
+:isimulated
B
cj ),of
jthe
inear
layer88 followed
by
arequire
nonlinearity
j.input
. .(j)
iiGiles
i .if&
J2
jtakes
90 = At
layer
the
an
encoder
function
=j:p(s
) hidden
is used.
This
as
input
j B
j nonlinearity
yer
followed
byand
A
B
.. B
1 .model
A ofhjagent
.B .. ...
. B... .. B
Assuming
si+1
the identity
j. j
j includes

i+1does not
i outputs
ii
ivector
ial.
89
agents
matter.
per-agent
controller
applied
independently.
This
model
satisfies
flexibilitymessage
iGuestrin
ia70
0ji .use
dDo
77(A
The
main
body
of
the
model
takes
ascommunication-free
input
the
concatenated
vectors
et
[8]
single
MDP
to the
control
asized
collection
ofthe
agents,
via athefactored
i Systems
Band
AB
B
be viewed
as
aia feedforward
with
hi+1
hnetwork
+
),
idynamically
ic...
ih
ia
mation
(NIPS
2016).
ot
distribute.
sidea
and
outputs
vector
h
RA
for
some
dlarge
form
of
encoder
is
problem
dependent,
The
key
TB
is
sized,
and
the
matrix
can
bethen
dynamically
because
can
be viewed
feedforward
with
layers
ii that
inetwork
j=
0 ). The
=iis
.layers
jfeature
jrequirements
1Controller
B
B
...
j91
j (in
viewed
as18Processing
a feedforward
network
with
layers
i+1
3.1
i T
71
, but is not Structure
able to coordinate their actions.
has
=
(A
hi +
B
ci ), B
0

72
Submitted
to.our
29th
Submitted
Conference
tothey
29th
Submitted
to
29th
on
Neural
Conference
to
on
Information
Neural
Conference
Neural
on
Information
Neural
Processing
Systems
Processing
Systems
(NIPS
2016).
Systems
(NIPS
Do
2016).
Systems
(NIPS
not distribute.
Do
2016).
(NIPS
not model
distribute.
Do
2016).
notuses
distribute.
Doanot distribute.
029th
j. passing
jtasks
..framework
.. Conference
hj 190key
=blocks
(A
hare
+Submitted
B
jthe
The
idea
T cis
sized,
and
the
matrix
can
be
dynamically
sized
because
the
. .encoder
92is
but
for
most
of
consist
of
lookup-table
embedding
(or
bags
of
vectors
thereof).
Unless
where
the
messages
areProcessing
learned.
In
contrast
toProcessing
these
approaches,
our
At
first
layer
of
the
an
function
=
p(s
)on
isInformation
used.
This
takes
asInformation
input
state-view
applied
by
rather
by
coordinate.
jthat
j ),
..ynamically
..model
1 type,
jagent
. than
i+1
i jiincludes
. and
. the identity
of
j. j sized because
imatrix
0i H
3
2
i+1
ii+1
i Assuming
The
key
idea
is
that
T
is
dynamically
sized,
the
can
be
dynamically
the
H
=
(T
),
19
blocks
are
applied
by
type,
rather
than
by
coordinate.
0
93
otherwise
noted,
c
=
0
for
all
j.
H
=i network
(T
Hlayers
),0 for
H outputs
= network
(T
Hideep
),vector
73
Wesome
now
detail
the form
architecture
for that
the modularity
of the communication-free model but
ewed
as
network
with
i
jBlayers
for
both
agent
and
communication.
sj and
feature
h
d0 ). control
The
of the
encoder
is has
problem
dependent,
viewed
as91aapplied
a feedforward
feedforward
with
B
...
A
j (in
asblocks
a feedforward
network
layers
are
by type, with
rather
thanB
by coordinate.
72
3.1R74
Controller
allowsStructure
communication.
is built
from modules
f iUnless
, which take the form of multilayer neural
92 form
but for
most
ofoutput
our
tasks
they
consist
ofInformation
a still
lookup-table
embedding
(orto(NIPS
bags
of2016).
vectors
thereof).
K
tten
in
block
94
At
the
of
the
model,
a
decoder
function
q(h
)
is
used
output
a
distribution
over
the
space
of work
Submitted
o
29th
Conference
on
Neural
Processing
Systems
Do
not
distribute.
i+1
i
i
form
jclosest
imatrix
75
networks.
Here
2 {0,because
K}, where
the number
communication
layers
the network.
a MARL
perspective,
theisized
tois ours
is theof
ofinFoerster
2..,
i+1
ii+1
i sized,
yblock
is that
T otherwise
is=dynamically
and
the
be
dynamically
the hasKthe
H(T
=
(T
H
inidea
block
93form
noted,
c0jFrom
0),ifor
all
j.Weofcan
Hactions.
(T
H
),
73
now
detail
the
architecture
forapproach
that
modularity
ofconcurrent
the
communication-free
model butet al. [5].
H
H
),i=
Submitted
toConference
29th
Conference
oni=
Neural
Information
Systems
(NIPS
2016).
Do
not
distribute.
95
q(.)
takes
form
single
layer
network,
followed
by
a
softmax.
To
produce
a
discrete
iithe
i aProcessing
i
i
are
applied
by29th
type,
rather
than
by
coordinate.
Submitted
to
on
Neural
Information
Processing
Systems
(NIPS
2016).
Do
not
distribute.
i
i
i
AB B
B
... still
Ba76allows
This
also
deep
reinforcement
learning
in
multi-agent
partially
observable
tasks,
specifically
Each
f
takes
two
input
vectors
for
each
agent
j:
the
hidden
state
h
and
the
communication
cij ,
74uses
communication.
is
built
from
modules
f
,
which
take
the
form
of
multilayer
neural
A
B
...
B
j
96
action,ofiwe
distribution.
i sample
ithe this
iq(hK ) is used toi+1
i B i ifrom
block
form94 At the
model,
a
decoder
function
output
a
distribution
over
the
space
of
B
...
B
i output
i BAithe
i
A
B
...
B
rm
j
75
networks.
Here
i
2
{0,
..,
K},
where
K
is
the
number
of
communication
layers
in
the
network.
two
problems
spirit
task)
which
communication.
B A B
... riddle
B
77
and(similar
outputs ain
vector
hj to. our
The levers
main body
of the
modelnecessitate
then takes as multi-agent
input the concatenated
vectors
ck form95 actions.
i ientire
i
iAii of...
ia Communication
97i i Thus
the
model,
call
Neural
Net (CommNN),
the istateq(.)
form
single
followed
by a softmax.
To produce(i)a takes
discrete
i B
itakes
iithe
B
B
B
B76i. iawhich
...weflayer
B1 network,
ii
i. (NIPS
i
ed to 29th Conference
onB=
Neural
Information
Processing
Systems
2016).
Do
not
T
iBB
B
AiB
...A...
B
A
B
0distribute.
Each
takes
two
input
vectors
for
each
agent
j:
the
hidden
state
h
and
the
communication
c
,
T
=
A
B
...
B
j
j
98
view
of
all
agents
s,
passes
it
through
the
encoder
h
=
p(s),
(ii)
iterates
h
and
c
in
equations
(1)
.. . ii ifrom
. iithe
. this
.. i
96
action,
sj includes the identity of agent j.
i sample
. .i distribution.
iwe
i
.. iA
...
B
i ..Ai B
i.. .B
...h....KB
K
B..toBBobain
B
...
BAssuming
. outputs
B i AiT99B
B
...
77 . samples
and
a vector
The main
body oftothe
model
and
(2)
,A
(iii)
actions
a
forhi+1
according
q(h
). then takes as input the concatenated vectors
.
=
.
.
.
.
jall .agents,
i
i
i
iB
iAB
i model,
i B we calliia Communication Neural Net (CommNN),
97i Thus
3 (i) takes the statei iB...
BA
B
A
.iwhich
.... . B
..
i ...
B
. 1.
T
=B iBBithe
AiBentire
B..i B
...passes
Ai ...
Ti = B
98
all
it. .. through
h0 =the
p(s),
(ii) of
iterates
(1)
2
.. .3.2
Assuming
sj includes
identity
agent j.h and c in equations
.....i
.
.
i ..agents
i ...Ks, Extensions
i. the encoder
..i view
.. 100 of
.
Model
...
B be dynamically
. samples
.B..to obain
. .B
...and
. i can
that
T is
sized,
the
matrix
sized
because
the K ).
and
(2)
,A
(iii)
actions
a
for
agents,
according
to q(h
i allbecause
.B ih
T99.dynamically
=
.
.sized,
Ts is
dynamically
and
the
matrix
can
be
dynamically
sized
the
...
A
i
i
i
B
.. BAii B..... A.B.An
..
i rather
i101
i B
ied by
type,
than
by
Connectivity:
alternative
to the broadcast framework described above is to allow agents
Bthan
Bby
B..Local
... coordinate.
type,
rather
coordinate.
2
.
.
.
.
.
to communicate
100
3.2102 Model
Extensions to others within a certain range. Let N (j) be the set of agents present within

18

Like our approach, the communication is learned rather than being pre-determined. However, the
agents communicate in a discrete manner through their actions. This contrasts with our model where
multiple continuous communication cycles are used at each time step to decide the actions of all
agents. Furthermore, our approach is amenable to dynamic variation in the number of agents.
The Neural GPU [10] has similarities to our model but differs in that a 1-D ordering on the input is
assumed and it employs convolution, as opposed to the global pooling in our approach (thus permitting
unstructured inputs). Our model can be regarded as an instantiation of the GNN construction of
Scarselli et al. [25], as expanded on by Li et al. [15]. In particular, in [25], the output of the model
is the fixed point of iterating equations (3) and (1) to convergence, using recurrent models. In [15],
these recurrence equations are unrolled a fixed number of steps and the model trained via backprop
through time. In this work, we do not require the model to be recurrent, neither do we aim to reach
steady state. Additionally, we regard Eqn. (3) as a pooling operation, conceptually making our model
a single feed-forward network with local connections.

4

Experiments

4.1 Baselines
We describe three baselines models for Φ to compare against our model.
Independent controller: A simple baseline is where agents are controlled independently without
any communication between them. We can write Φ as a = {φ(s1 ), ..., φ(sJ )}, where φ is a per-agent
controller applied independently. The advantages of this communication-free model is modularity
and flexibility1 . Thus it can deal well with agents joining and leaving the group, but it is not able to
coordinate agents’ actions.
Fully-connected: Another obvious choice is to make Φ a fully-connected multi-layer neural network,
that takes concatenation of h0j as an input and outputs actions {a1 , ..., aJ } using multiple output
softmax heads. It is equivalent to allowing T to be an arbitrary matrix with fixed size. This model
would allow agents to communicate with each other and share views of the environment. Unlike our
model, however, it is not modular, inflexible with respect to the composition and number of agents it
controls, and even the order of the agents must be fixed.
Discrete communication: An alternate way for agents to communicate is via discrete symbols, with
the meaning of these symbols being learned during training. Since Φ now contains discrete operations
and is not differentiable, reinforcement learning is used to train in this setting. However, unlike
actions in the environment, an agent has to output a discrete symbol at every communication step.
But if these are viewed as internal time steps of the agent, then the communication output can be
treated as an action of the agent at a given (internal) time step and we can directly employ policy
gradient [37].
At communication step i, agent j will output the index wji corresponding to a particular symbol,
sampled according to:
wji ∼ Softmax(Dhij )
(5)
where matrix D is the model parameter. Let ŵ be a 1-hot binary vector representation of w. In our
broadcast framework, at the next step the agent receives a bag of vectors from all the other agents
(where ∧ is the element-wise OR operation):
^
ci+1
=
ŵji 0
(6)
j
j 0 6=j

4.2 Simple Demonstration with a Lever Pulling Task
We start with a very simple game that requires the agents to communicate in order to win. This
consists of m levers and a pool of N agents. At each round, m agents are drawn at random from
the total pool of N agents and they must each choose a lever to pull, simultaneously with the other
m − 1 agents, after which the round ends. The goal is for each of them to pull a different lever.
Correspondingly, all agents receive reward proportional to the number of distinct levers pulled. Each
agent can see its own identity, and nothing else, thus sj = j.
1

Assuming sj includes the identity of agent j.

4

We implement the game with m = 5 and N = 500. We use a CommNet with two communication
steps (K = 2) and skip connections from (4). The encoder r is a lookup-table with N entries of
128D. Each f i is a two layer neural net with ReLU non-linearities that takes in the concatenation
of (hi , ci , h0 ), and outputs a 128D vector. The decoder is a linear layer plus softmax, producing
a distribution over the m levers, from which we sample to determine the lever to be pulled. We
compare it against the independent controller, which has the same architecture as our model except
that communication c is zeroed. The results are shown in Table 1. The metric is the number of
distinct levers pulled divided by m = 5, averaged over 500 trials, after seeing 50000 batches of size
64 during training. We explore both reinforcement (see Appendix A) and direct supervision (using
the solution given by sorting the agent IDs, and having each agent pull the lever according to its
relative order in the current m agents). In both cases, the CommNet performs significantly better
than the independent controller. See Appendix B for an analysis of a trained model.
Model Φ
Independent
CommNet

Training method
Supervised Reinforcement
0.59
0.59
0.99
0.94

Table 1: Results of lever game (#distinct levers pulled)/(#levers) for our CommNet and independent
controller models, using two different training approaches. Allowing the agents to communicate
enables them to succeed at the task.
4.3 Multi-turn Games
In this section, we consider two multi-agent tasks using the MazeBase environment [28] that use
reward as their training signal. The first task is to control cars passing through a traffic junction to
maximize the flow while minimizing collisions. The second task is to control multiple agents in
combat against enemy bots.
We experimented with several module types. With a feedforward MLP, the module f i is a single
layer network and K = 2 communication steps are used. For an RNN module, we also used a single
layer network for f t , but shared parameters across time steps. Finally, we used an LSTM for f t . In
all modules, the hidden layer size is set to 50. MLP modules use skip-connections. Both tasks are
trained for 300 epochs, each epoch being 100 weight updates with RMSProp [33] on mini-batch of
288 game episodes (distributed over multiple CPU cores). In total, the models experience ∼8.6M
episodes during training. We repeat all experiments 5 times with different random initializations, and
report mean value along with standard deviation. The training time varies from a few hours to a few
days depending on task and module type.
4.3.1 Traffic Junction
This consists of a 4-way junction on a 14 × 14 grid as shown in Fig. 2(left). At each time step, new
cars enter the grid with probability parrive from each of the four directions. However, the total number
of cars at any given time is limited to Nmax = 10. Each car occupies a single cell at any given time
and is randomly assigned to one of three possible routes (keeping to the right-hand side of the road).
At every time step, a car has two possible actions: gas which advances it by one cell on its route or
brake to stay at its current location. A car will be removed once it reaches its destination at the edge
of the grid.
Two cars collide if their locations overlap. A collision incurs a reward rcoll = −10, but does not affect
the simulation in any other way. To discourage a traffic jam, each car gets reward of τ rtime = −0.01τ
at every time step, where τ is the number time steps passed since the car arrived. Therefore, the total
reward at time t is:
Nt
X
t
r(t) = C rcoll +
τi rtime ,
i=1

where C t is the number of collisions occurring at time t, and N t is number of cars present. The
simulation is terminated after 40 steps and is classified as a failure if one or more more collisions
have occurred.
Each car is represented by one-hot binary vector set {n, l, r}, that encodes its unique ID, current
location and assigned route number respectively. Each agent controlling a car can only observe other
cars in its vision range (a surrounding 3 × 3 neighborhood), but it can communicate to all other cars.
5

Independent
Enemy bot

Attack actions
(e.g. attack_4)

3

4

Car exiting

3

1

5

Firing range

5

3 possible
routes

2

Visual range

Visual range

Discrete comm.

4 movement actions

1

CommNet

100%

2

Failure rate

New car
arrivals

4

10%

1%

1x1

3x3

5x5

7x7

Vision range

Figure 2: Left: Traffic junction task where agent-controlled cars (colored circles) have to pass the
through the junction without colliding. Middle: The combat task, where model controlled agents (red
circles) fight against enemy bots (blue circles). In both tasks each agent has limited visibility (orange
region), thus is not able to see the location of all other agents. Right: As visibility in the environment
decreases, the importance of communication grows in the traffic junction task.
The state vector sj for each agent is thus a concatenation of all these vectors, having dimension
32 × |n| × |l| × |r|.
In Table 2(left), we show the probability of failure of a variety of different model Φ and module
f pairs. Compared to the baseline models, CommNet significantly reduces the failure rate for all
module types, achieving the best performance with LSTM module (a video showing this model
before and after training can be found at http://cims.nyu.edu/~sainbar/commnet).

We also explored how partial visibility within the environment effects the advantage given by
communication. As the vision range of each agent decreases, the advantage of communication
increases as shown in Fig. 2(right). Impressively, with zero visibility (the cars are driving blind) the
CommNet model is still able to succeed 90% of the time.
Table 2(right) shows the results on easy and hard versions of the game. The easy version is a junction
of two one-way roads, while the harder version consists from four connected junctions of two-way
roads. Details of the other game variations can be found in Appendix C. Discrete communication
works well on the easy version, but the CommNet with local connectivity gives the best performance
on the hard case.
4.3.2 Analysis of Communication
We now attempt to understand what the agents communicate when performing the junction task.
We start by recording the hidden state hij of each agent and the corresponding communication
vectors c̃i+1
= C i+1 hij (the contribution agent j at step i + 1 makes to the hidden state of other
j
agents). Fig. 3(left) and Fig. 3(right) show the 2D PCA projections of the communication and hidden
state vectors respectively. These plots show a diverse range of hidden states but far more clustered
communication vectors, many of which are close to zero. This suggests that while the hidden state
carries information, the agent often prefers not to communicate it to the others unless necessary. This
is a possible consequence of the broadcast channel: if everyone talks at the same time, no-one can
understand. See Appendix D for norm of communication vectors and brake locations.

Model Φ
Independent
Fully-connected
Discrete comm.
CommNet

Module f () type
MLP
RNN
LSTM
20.6± 14.1 19.5± 4.5 9.4± 5.6
12.5± 4.4 34.8± 19.7 4.8± 2.4
15.8± 9.3
15.2± 2.1 8.4± 3.4
2.2± 0.6
7.6± 1.4
1.6± 1.0

Model Φ
Independent
Discrete comm.
CommNet
CommNet local

Other game versions
Easy (MLP) Hard (RNN)
15.8± 12.5
26.9± 6.0
1.1± 2.4
28.2± 5.7
0.3± 0.1
22.5± 6.1
21.1± 3.4

Table 2: Traffic junction task. Left: failure rates (%) for different types of model and module function
f (.). CommNet consistently improves performance, over the baseline models. Right: Game variants.
In the easy case, discrete communication does help, but still less than CommNet. On the hard version,
local communication (see Section 2.2) does at least as well as broadcasting to all agents.
6

3

40

STOP
30

A

B

2

20

1

C
10

A

0

0

B

−1
−10

−2
−20

−3

−30

−40
−20

C2

C1
−10

0

10

20

30

40

50

60

70

−4
−4

−3

−2

−1

0

1

2

3

Figure 3: Left: First two principal components of communication vectors c̃ from multiple runs on
the traffic junction task Fig. 2(left). While the majority are “silent” (i.e. have a small norm), distinct
clusters are also present. Middle: for three of these clusters, we probe the model to understand
their meaning (see text for details). Right: First two principal components of hidden state vectors h
from the same runs as on the left, with corresponding color coding. Note how many of the “silent”
communication vectors accompany non-zero hidden state vectors. This shows that the two pathways
carry different information.
To better understand the meaning behind the communication vectors, we ran the simulation with
only two cars and recorded their communication vectors and locations whenever one of them braked.
Vectors belonging to the clusters A, B & C in Fig. 3(left) were consistently emitted when one of the
cars was in a specific location, shown by the colored circles in Fig. 3(middle) (or pair of locations for
cluster C). They also strongly correlated with the other car braking at the locations indicated in red,
which happen to be relevant to avoiding collision.
4.3.3 Combat Task
We simulate a simple battle involving two opposing teams in a 15×15 grid as shown in Fig. 2(middle).
Each team consists of m = 5 agents and their initial positions are sampled uniformly in a 5 × 5
square around the team center, which is picked uniformly in the grid. At each time step, an agent can
perform one of the following actions: move one cell in one of four directions; attack another agent
by specifying its ID j (there are m attack actions, each corresponding to one enemy agent); or do
nothing. If agent A attacks agent B, then B’s health point will be reduced by 1, but only if B is inside
the firing range of A (its surrounding 3 × 3 area). Agents need one time step of cooling down after
an attack, during which they cannot attack. All agents start with 3 health points, and die when their
health reaches 0. A team will win if all agents in the other team die. The simulation ends when one
team wins, or neither of teams win within 40 time steps (a draw).
The model controls one team during training, and the other team consist of bots that follow a hardcoded policy. The bot policy is to attack the nearest enemy agent if it is within its firing range. If not,
it approaches the nearest visible enemy agent within visual range. An agent is visible to all bots if it
is inside the visual range of any individual bot. This shared vision gives an advantage to the bot team.
When input to a model, each agent is represented by a set of one-hot binary vectors {i, t, l, h, c}
encoding its unique ID, team ID, location, health points and cooldown. A model controlling an agent
also sees other agents in its visual range (3 × 3 surrounding area). The model gets reward of -1 if the
team loses or draws at the end of the game. In addition, it also get reward of −0.1 times the total
health points of the enemy team, which encourages it to attack enemy bots.
Module f () type
Model Φ
MLP
RNN
LSTM
Independent
34.2± 1.3 37.3± 4.6 44.3± 0.4
Fully-connected 17.7± 7.1
2.9± 1.8
19.6± 4.2
Discrete comm. 29.1± 6.7 33.4± 9.4 46.4± 0.7
CommNet
44.5± 13.4 44.4± 11.9 49.5± 12.6

Other game variations (MLP)
Model Φ
m=3
m = 10 5 × 5 vision
Independent 29.2± 5.9 30.5± 8.7
60.5± 2.1
CommNet 51.0± 14.1 45.4± 12.4 73.0± 0.7

Table 3: Win rates (%) on the combat task for different communication approaches and module
choices. Continuous consistently outperforms the other approaches. The fully-connected baseline
does worse than the independent model without communication. On the right we explore the
effect of varying the number of agents m and agent visibility. Even with 10 agents on each team,
communication clearly helps.
7

Table 3 shows the win rate of different module choices with various types of model. Among
different modules, the LSTM achieved the best performance. Continuous communication with
CommNet improved all module types. Relative to the independent controller, the fully-connected
model degraded performance, but the discrete communication improved LSTM module type. We
also explored several variations of the task: varying the number of agents in each team by setting
m = 3, 10, and increasing visual range of agents to 5 × 5 area. The result on those tasks are shown
on the right side of Table 3. Using CommNet model consistently improves the win rate, even with
the greater environment observability of the 5×5 vision case.
4.4 bAbI Tasks
We apply our model to the bAbI [36] toy Q & A dataset, which consists of 20 tasks each requiring
different kind of reasoning. The goal is to answer a question after reading a short story. We can
formulate this as a multi-agent task by giving each sentence of the story its own agent. Communication
among agents allows them to exchange useful information necessary to answer the question.
The input is {s1 , s2 , ..., sJ , q}, where sj is j’th sentence of the story, and q is the question sentence.
We use the same encoder representation as [29] to convert them to vectors. The f (.) module consists
of a two-layer MLP with ReLU non-linearities. After K = 2 communication steps, we add the
final hidden states together and pass it through a softmax decoder layer to sample an output word y.
The model is trained in a supervised fashion using a cross-entropy loss between y and the correct
answer y ∗ . The hidden layer size is set to 100 and weights are initialized from N (0, 0.2). We train
the model for 100 epochs with learning rate 0.003 and mini-batch size 32 with Adam optimizer [12]
(β1 = 0.9, β2 = 0.99,  = 10−6 ). We used 10% of training data as validation set to find optimal
hyper-parameters for the model.
Results on the 10K version of the bAbI task are shown in Table 4, along with other baselines (see
Appendix E for a detailed breakdown). Our model outperforms the LSTM baseline, but is worse
than the MemN2N model [29], which is specifically designed to solve reasoning over long stories.
However, it successfully solves most of the tasks, including ones that require information sharing
between two or more agents through communication.
LSTM [29]
MemN2N [29]
DMN+ [38]
Independent (MLP module)
CommNet (MLP module)

Mean error (%)
36.4
4.2
2.8
15.2
7.1

Failed tasks (err. > 5%)
16
3
1
9
3

Table 4: Experimental results on bAbI tasks.

5

Discussion and Future Work

We have introduced CommNet, a simple controller for MARL that is able to learn continuous
communication between a dynamically changing set of agents. Evaluations on four diverse tasks
clearly show the model outperforms models without communication, fully-connected models, and
models using discrete communication. Despite the simplicity of the broadcast channel, examination
of the traffic task reveals the model to have learned a sparse communication protocol that conveys
meaningful information between agents. Code for our model (and baselines) can be found at
http://cims.nyu.edu/~sainbar/commnet/.
One aspect of our model that we did not fully exploit is its ability to handle heterogenous agent types
and we hope to explore this in future work. Furthermore, we believe the model will scale gracefully
to large numbers of agents, perhaps requiring more sophisticated connectivity structures; we also
leave this to future work.
Acknowledgements
The authors wish to thank Daniel Lee and Y-Lan Boureau for their advice and guidance. Rob Fergus
is grateful for the support of CIFAR.

References
[1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
[2] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning.
Systems, Man, and Cybernetics, IEEE Transactions on, 38(2):156–172, 2008.

8

[3] Y. Cao, W. Yu, W. Ren, and G. Chen. An overview of recent progress in the study of distributed multi-agent
coordination. IEEE Transactions on Industrial Informatics, 1(9):427–438, 2013.
[4] R. H. Crites and A. G. Barto. Elevator group control using multiple reinforcement learning agents. Machine
Learning, 33(2):235–262, 1998.
[5] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate to solve riddles
with deep distributed recurrent Q-networks. arXiv, abs/1602.02672, 2016.
[6] D. Fox, W. Burgard, H. Kruppa, and S. Thrun. Probabilistic approach to collaborative multi-robot
localization. Autonomous Robots, 8(3):325––344, 2000.
[7] C. L. Giles and K. C. Jim. Learning communication for multi-agent systems. In Innovative Concepts for
Agent Based Systems, pages 377—-390. Springer, 2002.
[8] C. Guestrin, D. Koller, and R. Parr. Multiagent planning with factored MDPs. In NIPS, 2001.
[9] X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang. Deep learning for real-time atari game play using
offline monte-carlo tree search planning. In NIPS, 2014.
[10] L. Kaiser and I. Sutskever. Neural gpus learn algorithms. In ICLR, 2016.
[11] T. Kasai, H. Tenmoto, and A. Kamiya. Learning of communication codes in multi-agent reinforcement
learning problem. IEEE Conference on Soft Computing in Industrial Applications, pages 1–6, 2008.
[12] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[13] M. Lauer and M. A. Riedmiller. An algorithm for distributed reinforcement learning in cooperative
multi-agent systems. In ICML, 2000.
[14] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of
Machine Learning Research, 17(39):1–40, 2016.
[15] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In ICLR, 2015.
[16] M. L. Littman. Value-function reinforcement learning in markov games. Cognitive Systems Research,
2(1):55–66, 2001.
[17] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver. Move evaluation in go using deep convolutional
neural networks. In ICLR, 2015.
[18] D. Maravall, J. De Lope, and R. Domnguez. Coordination of communication in robot teams by reinforcement learning. Robotics and Autonomous Systems, 61(7):661–666, 2013.
[19] M. Matari. Reinforcement learning in the multi-robot domain. Autonomous Robots, 4(1):73–83, 1997.
[20] F. S. Melo, M. Spaan, and S. J. Witwicki. Querypomdp: Pomdp-based communication in multiagent
systems. In Multi-Agent Systems, pages 189–204, 2011.
[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,
A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, D. Wierstra, S. Legg, and D. Hassabis.
Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
[22] R. Olfati-Saber, J. Fax, and R. Murray. Consensus and cooperation in networked multi-agent systems.
Proceedings of the IEEE, 95(1):215–233, 2007.
[23] J. Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, 1982.
[24] B. Peng, Z. Lu, H. Li, and K. Wong. Towards Neural Network-based Reasoning. ArXiv preprint:
1508.05508, 2015.
[25] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model.
IEEE Trans. Neural Networks, 20(1):61–80, 2009.
[26] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484–489, 2016.
[27] P. Stone and M. Veloso. Towards collaborative and adversarial learning: A case study in robotic soccer.
International Journal of Human Computer Studies, (48), 1998.
[28] S. Sukhbaatar, A. Szlam, G. Synnaeve, S. Chintala, and R. Fergus. Mazebase: A sandbox for learning
from games. CoRR, abs/1511.07401, 2015.
[29] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. NIPS, 2015.
[30] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998.
[31] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, and R. Vicente. Multiagent cooperation
and competition with deep reinforcement learning. arXiv:1511.08779, 2015.
[32] M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In ICML, 1993.
[33] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
[34] P. Varshavskaya, L. P. Kaelbling, and D. Rus. Distributed Autonomous Robotic Systems 8, chapter Efficient
Distributed Reinforcement Learning through Agreement, pages 367–378. 2009.
[35] X. Wang and T. Sandholm. Reinforcement learning to play an optimal nash equilibrium in team markov
games. In NIPS, pages 1571–1578, 2002.
[36] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards ai-complete question answering: A set of
prerequisite toy tasks. In ICLR, 2016.

9

[37] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
In Machine Learning, pages 229–256, 1992.
[38] C. Xiong, S. Merity, and R. Socher. Dynamic memory networks for visual and textual question answering.
ICML, 2016.
[39] C. Zhang and V. Lesser. Coordinating multi-agent reinforcement learning with limited communication. In
Proc. AAMAS, pages 1101–1108, 2013.

A

Reinforcement Training

We use policy gradient [37] with a state specific baseline for delivering a gradient to the model. Denote the
states in an episode by s(1), ..., s(T ), and the actions taken at each of those states as a(1), ..., a(T ), where T
is the length of the episode. The baseline is a scalar function of the states b(s, θ), computed via an extra head
on the model producing the action probabilities. Beside maximizing the expected reward with policy gradient,
the models are also trained to minimize the distance between the baseline value and actual reward. Thus after
finishing an episode, we update the model parameters θ by

!
!2 
T
T
T
X
X
X
∂
log
p(a(t)|s(t),
θ)
∂

∆θ =
r(i) − b(s(t), θ) − α
r(i) − b(s(t), θ)  . (7)
∂θ
∂θ i=t
t=1
i=t
Here r(t) is reward given at time t, and the hyperparameter α is for balancing the reward and the baseline
objectives, which set to 0.03 in all experiments.

B

Lever Pulling Task Analysis

Figure 4: 3D PCA plot of hidden states of agents
Here we analyze a CommNet model trained with supervision on the lever pulling task. The supervision uses the
sorted ordering of agent IDs to assign target actions. For each agent, we concatenate its hidden layer activations
during game playing. Fig. 4 shows 3D PCA plot of those vectors, where color intensity represents agent’s ID.
The smooth ordering suggests that agents are communicating their IDs, enabling them to solve the task.

C

Details of Traffic Junction

We use curriculum learning [1] to make the training easier. In first 100 epochs of training, we set parrive = 0.05,
but linearly increased it to 0.2 during next 100 epochs. Finally, training continues for another 100 epochs. The
learning rate is fixed at 0.003 throughout. We also implemented additional easy and hard versions of the game,
the latter being shown in Fig.2.
The easy version is a junction of two one-way roads on a 7 × 7 grid. There are two arrival points, each with two
possible routes. During curriculum, we increase Ntotal from 3 to 5, and parrive from 0.1 to 0.3.
The harder version consists from four connected junctions of two-way roads in 18 × 18 as shown in Fig. 5.
There are 8 arrival points and 7 different routes for each arrival point. We set Ntotal = 20, and increased parrive
from 0.02 to 0.05 during curriculum.

10

4 junctions

Figure 5: A harder version of traffic task with four connected junctions.

D

Traffic Junction Analysis

Here we visualize the average norm of the communication vectors in Fig. 6(left) and brake locations over
the 14 × 14 spatial grid in Fig. 6(right). In each of the four incoming directions, there is one location where
communication signal is stronger. The brake pattern shows that cars coming from left never yield to other
directions.
80

0.35

70

0.3

60

0.25

50

0.2

40
0.15

30

0.1

20
10

0.05

0

0

Figure 6: (left) Average norm of communication vectors (right) Brake locations

E

bAbI Tasks Details

Here we give further details of the model setup and training, as well as a breakdown of results in Table 5.
Let the task be {s1 , s2 , ..., sJ , q, y ∗ }, where sj is j’th sentence of story, q is the question sentence and y ∗ is the
correct answer word (when answer is multiple words, we simply concatenate them into single word). Then the
input to the model is
h0j = r(sj , θ0 ), c0j = r(q, θq ).
Here, we use simple position encoding [29] as r to convert sentences into fixed size vectors. Also, the initial
communication is used to broadcast the question to all agents. Since the temporal ordering of sentences is
relevant in some tasks, we add special temporal word “t = J − j” to sj for all j.
For f module, we use a 2 layer network with skip connection, that is
hi+1
= σ(Wi σ(H i hij + C i cij + h0j )),
j
where σ is ReLU non-linearity (bias terms are omitted for clarity). After K = 2 communication steps, the
model outputs an answer word by
J
X
y = Sof tmax(D
hK
j )
j=1

Since we have the correct answer during training, we will do supervised learning by using cross entropy cost on
{y ∗ , y}. The hidden layer size is set 100 and weights are initialized from N (0, 0.2). We train the model 100
epochs with learning rate 0.003 and mini-batch size 32 with Adam optimizer [12] (β1 = 0.9, β2 = 0.99,  =
10−6 ). We used 10% of training data as validation set to find optimal hyper-parameters for the model.

11

Error on tasks (%)
2
3
15
16
17
LSTM [29]
81.9 83.1 78.7 51.9 50.1
MemN2N [29]
0.3 2.1 0.0 51.8 18.6
DMN+ [38]
0.3 1.1 0.0 45.3 4.2
Neural Reasoner+ [24]
0.9
Independent (MLP module) 69.0 69.5 29.4 47.4 4.0
CommNet (MLP module)
3.2 68.3 0.0 51.3 15.1

18 19
6.8 90.3
5.3 2.3
2.1 0.0
1.6
0.6 45.8
1.4 0.0

Mean error Failed tasks
(%)
(err. > 5%)
36.4
16
4.2
3
2.8
1
15.2
9
7.1
3

Table 5: Experimental results on bAbI tasks. Only showing some of the task with high errors.

12

