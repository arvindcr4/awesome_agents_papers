Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A
Context Modeling Problem
Chao Li1 , Bingkun BAO1 , Yang Gao2

arXiv:2509.15519v1 [cs.LG] 19 Sep 2025

1

School of Computer Science, Nanjing University of Posts and Telecommunications
2
School of Intelligence Science and Technology, Nanjing University

Abstract
This paper studies fully decentralized cooperative multi-agent
reinforcement learning, where each agent solely observes the
states, its local actions, and the shared rewards. The inability
to access other agentsâ€™ actions often leads to non-stationarity
during value function updates and relative overgeneralization
during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address
both issues simultaneously, due to their inability to model the
joint policy of other agents in a fully decentralized setting. To
overcome this limitation, we propose a novel method named
Dynamics-Aware Context (DAC), which formalizes the task,
as locally perceived by each agent, as an Contextual Markov
Decision Process, and further addresses both non-stationarity
and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary
local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using
latent variables and refers to them as contexts. For each agent,
DAC introduces a context-based value function to address the
non-stationarity issue during value function update. For value
function estimation, an optimistic marginal value is derived to
promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we
evaluate DAC on various cooperative tasks (including matrix
game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.

1

Introduction

Multi-agent reinforcement learning (MARL) has emerged as
a powerful technique for addressing cooperative tasks, driving substantial advancements in both algorithms (e.g., value
decomposition methods (Sunehag et al. 2017; Rashid et al.
2020b; Son et al. 2019; Wang et al. 2020a) and multi-agent
policy gradient methods (Lowe et al. 2017; Foerster et al.
2018; Yu et al. 2022; Zhong et al. 2024)) and applications
(e.g., traffic signal control (Wang et al. 2020b), autonomous
vehicles (Zhou et al. 2021), and vaccine allocation task (Hao
et al. 2023)). Most of these advances depend on the centralized training (often with decentralized execution) paradigm,
where global information, particularly the joint actions of all
agents, is accessible during training. However, such direct
Copyright Â© 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

access to other agentsâ€™ actions is often unattainable in realworld domains. For instance, in industrial automation scenarios, robots from different companies can not share action
information due to privacy concerns or limited communication capabilities. In such cases, the fully decentralized learning is necessitated, where each agent learns solely from its
individual experiences, without access to the actions of other
agents, during both training and execution periods.
However, developing efficient cooperative policies under
the fully decentralized learning paradigm is challenging due
to two key challenges arising from the lack of access to other
agentsâ€™ actions. The first is non-stationarity during per-agent
value function updates. Since treating other agents as part of
the environment, the task dynamics perceived by each agent
becomes non-stationary due to the evolving policies of other
agents, undermining the convergence of value function updates (Jiang, Su, and Lu 2024). The second is relative overgeneralization, wherein the value estimations of each agentâ€™s
local cooperative actions may be biased by other agentsâ€™ exploratory or sub-optimal action selections, hindering agents
from selecting the optimal joint action (Matignon, Laurent,
and Le Fort-Piat 2012). As a result, fully decentralized learning typically suffers from low efficiency and sub-optimal solutions, limiting efficient multi-agent cooperation.
Accordingly, we classify existing value-based MARL approaches into two categories. The first category aims to solve
the non-stationarity issue. These methods either ensure stationary transition data by directly accessing other agentsâ€™ actions (Sunehag et al. 2017; Rashid et al. 2020b), designing
multi-agent importance sampling weights, fingerprints (Foerster et al. 2017), and constructing ideal transition probabilities (Jiang and Lu 2022), or enable stationary policy updates
using alternative policy updates (Su et al. 2024). The second
category primarily targets the relative overgeneralization issue, typically by rectifying the learned factored global action
value function (Son et al. 2019; Wang et al. 2020a; Rashid
et al. 2020a) or employing optimistic or lenient value function updates (Lauer and Riedmiller 2000; Matignon, Laurent, and Le Fort-Piat 2007; Omidshafiei et al. 2017; Panait,
Sullivan, and Luke 2006; Wei and Luke 2016) to encourage the selection of optimal joint actions. Although both issues stem from the lack of other agentsâ€™ action information,
existing fully decentralized MARL methods address either
the non-stationarity issue or the relative overgeneralization

problem in isolation, due to their inability to model the joint
policy of other agents in a fully decentralized setting. Consequently, they fail to simultaneously resolve both challenges.
To overcome this limitation, we propose a novel method
named Dynamics-Aware Context (DAC), which formalizes
the task, as locally perceived by each agent, as an Contextual
Markov Decision Process (CMDP) (Hallak, Di Castro, and
Mannor 2015), and further addresses both non-stationarity
and relative overgeneralization from a dynamics-aware context modeling perspective. Specifically, DAC attributes the
non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy of other agents. Then, drawing upon ideas
from concept drift literature (Lu et al. 2018), DAC employs a
sliding window alongside per-agent local trajectory to model
step-wise dynamics distribution using latent variables. Since
each agentâ€™s task dynamics is determined by the joint policy
of other agents, these variables implicitly represent the other
agentsâ€™ joint policy at each time step. Accordingly, we refer
to them as contexts and learn a context-based value function
for each agent to address the non-stationarity issue in value
function updates. During value function estimation, an optimistic marginal value is derived to discard the effects caused
by other agentsâ€™ sub-optimal actions, thereby facilitating the
selection of optimal joint actions and addressing the relative
overgeneralization problem. The above enables effective cooperative policy learning in a fully decentralized manner.
Empirically, we evaluate DAC across various cooperative
tasks, including the matrix game, predator and prey, and the
StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al.
2019). The results demonstrate significant performance gain
against multiple baselines, validating DACâ€™s effectiveness.

2

Related Work

In this section, we classify current value-based MARL methods into two categories and give a brief introduction to them.
The first category of works addresses the non-stationarity
problem by constructing stationary transition data or policy
updates. Specifically, canonical value decomposition methods such as VDN (Sunehag et al. 2017) and QMIX (Rashid
et al. 2020b)) assume direct access to agentsâ€™ joint actions to
ensure stationary transitions during training. However, these
methods often suffer from the relative overgeneralization issue due to the representational limitation of their learned factored global action value functions (Gupta et al. 2021). For
independent Q-learning (IQL) (Tan 1993) agents, the multiagent importance sampling (Foerster et al. 2017) technique
assumes direct access to other agentsâ€™ policies and calculates
an importance weight to decay obsolete data during experience replay. Multi-agent fingerprints (Foerster et al. 2017)
uses the training iteration numbers and the exploration rates
of other agents to estimate their policies, and augments peragent local transitions with these estimations. However, such
direct access to other agentsâ€™ information assumed in above
methods is unattainable in practice. I2Q (Jiang and Lu 2022)
addresses this by shaping ideal transition probabilities for
each IQL agent in a fully decentralized manner, and guarantees convergence to the optimal joint policy. In comparison
to I2Qâ€™s approach of addressing non-stationarity and relative

overgeneralization issues by shaping ideal transition probabilities, this work aims for a novel context-aware framework
to tackle both issues. In addition, to ensure stationary policy
updates, MA2QL (Su et al. 2024) enforces sequential policy
updates among IQL agents. When an agent updates its local
policy, all othersâ€™ policies remain fixed. Despite its promise,
the sequential policy update typically leads to sample inefficiency, as it lacks the capacity for parallel policy updates.
The second category of works addresses the relative overgeneralization issue by rectifying the learned factored global
action value function or updating per-agent local value function in optimistic or lenient manners. Specifically, for value
decomposition methods with representational limitations,
weighted QMIX (Rashid et al. 2020a) places more weights
on potentially optimal joint actions during value updates to
exclusively recover correct value estimations for these critical actions. QTRAN (Son et al. 2019) and QPLEX (Wang
et al. 2020a) incorporate additional complementary terms to
correct the discrepancy between the learned factored global
action value functions and the true joint ones. For IQL, distributed Q-learning (Lauer and Riedmiller 2000) employs an
optimistic value function for each agent to discard the effect
caused by other agentsâ€™ exploratory or sub-optimal actions.
This enables agents to identify and select their local cooperative actions, thus addressing the relative overgeneralization
problem. However, due to the high optimism, distributed Qlearning is vulnerable to stochasticity. Hysteretic Q-learning
(Matignon, Laurent, and Le Fort-Piat 2007; Omidshafiei
et al. 2017) avoids this by updating per-agent value function
using two learning rates for positive and negative temporal
difference errors, respectively. Lenient learning (Panait, Sullivan, and Luke 2006; Wei and Luke 2016) shifts from optimistic to standard value function update using gradually decreasing lenience. However, the optimistic update may cause
value overestimation, particularly when the value function is
approximated using neural networks. Moreover, the neglect
of non-stationarity further hinders efficient policy learning.
In summary, existing fully decentralized MARL methods
fail to address both non-stationarity and relative overgeneralization in a unified manner. To address this limitation, this
work proposes to formalize the task perceived by each agent
as an CMDP, and tackles both issues from a dynamics-aware
context modeling perspective, thereby effectively promoting
fully decentralized cooperative policy learning.

3

Preliminary

In this section, we formalize the task addressed by this work,
and review the non-stationarity and relative overgeneralization issues in decentralized learning, as well as the CMDP.

3.1

Multi-Agent Markov Decision Process

We consider a cooperative multi-agent task that can be modeled as an Multi-Agent Markov Decision Process (MMDP)
âŸ¨N, S, A, P, R, Î³âŸ©, where N = {1, 2, . . . , n} is the agent set
and S denotes the state space. A = A1 Ã— A2 Ã— . . . An is all
agentsâ€™ joint action space and Ai is the local action space of
agent i âˆˆ N . At each time step t, each agent i observes the
state st âˆˆ S and selects its local action ait âˆˆ Ai based on

its decentralized policy Ï€ i (ait |st ). Based on the joint action
of all agents at = (a1t , a2t , . . . , ant ), the environment transits
to the next state st+1 according to the state transition function P (st+1 |st , at ) and provides all agents with the same reward R(st , at ). The goal is to learn the optimal joint policy
Ï€ = (Ï€ 1 , Ï€ 2 , . . . P
, Ï€ n ) that maximizes the expected cumulaâˆ
tive return EÏ€,P [ t=0 Î³ t rt ], where Î³ is a discount factor.
We consider the fully decentralized learning, where each
agent i observes only the state st , its local action ait , and
the shared reward rt . For each decentralized agent i, the perceived task can be modeled as an Markov Decision Process
(MDP) âŸ¨S, Ai , P i , Ri , Î³âŸ© with dynamics defined as follows:
X
P i (st+1 |st , ait ) =
Ï€ âˆ’i (aâˆ’i
t |st )P (st+1 |st , at ),
aâˆ’i
t
(1)
X
Ri (st , ait ) =
Ï€ âˆ’i (aâˆ’i
t |st )R(st , at ),
âˆ’i
at

where Ï€ âˆ’i and aâˆ’i
t respectively denote the joint policy and
the joint action of other agents âˆ’i except for agent i.
Non-stationarity. As illustrated in Eq. (1), each agent iâ€™s
local task dynamics, denoted by P i and Ri , depend on other
agents âˆ’iâ€™s joint policy Ï€ âˆ’i . Since other agents continually
change their policies, the local task dynamics of each agent
i becomes non-stationary. In addition, when employing offpolicy experience replay, the sampled data can be viewed as
following P i and Ri with respect to the average joint policy
Ï€Ì„ âˆ’i of other agents âˆ’i over the course of training. This nonstationarity undermines the convergence of value functions.
Relative overgeneralization. Due to the absence of other
agentsâ€™ action information, the value estimation of per-agent
local cooperative actions may be biased by other agentsâ€™ exploratory or sub-optimal actions. As a result, the sub-optimal
joint actions are preferred over the optimal ones, a problem
known as relative overgeneralization.
Specifically, for each decentralized agent i, its local value
function Qi (st , ait ) can be regarded as a projection regarding
the joint action value function Q(st , ait , aâˆ’i
t ). IQL adheres to
an average-based projection, which is defined as follows:
X
Ï€
i âˆ’i
Qi,Ï€ (st , ait ) =
Ï€ âˆ’i (aâˆ’i
(2)
t |st )Q (st , at , at ),
aâˆ’i
t

Ï€

(st , ait , aâˆ’i
t ) represents the joint action value funci
âˆ’i

where Q
tion given a joint policy Ï€ = (Ï€ , Ï€ ). As shown in Eq. (2),
Qi following the average-based projection is easily affected
by other agentsâ€™ sub-optimal actions, thus suffering from the
relative overgeneralization issue. In contrast, the maximumbased (optimistic) projection is defined below:
Qi,opt (st , ait ) = maxaâˆ’i Qâˆ— (st , ait , aâˆ’i
t ),
t

(3)

where Qâˆ— (st , ait , aâˆ’i
t ) is the joint action value function of an
optimal joint policy Ï€ âˆ— . This optimistic projection assumes
that other agents âˆ’i always select their cooperative actions,
thus eliminating their effects on agent iâ€™s local value estimations. Both distributed Q-learning and hysteretic Q-learning
approximate Qi,opt (st , ait ) by an optimistic value function
update. In contrast, our method estimates it by an optimistic
marginal value derived from a context-based value function.
We detail the distinction between them in Appendix. A.

3.2

Contextual Markov Decision Process

A contextual Markov Decision Process (CMDP) is often defined as a tuple âŸ¨C, S, A, M (c)âŸ©, where C denotes the context space, S represents the state space, and A is the action
space. For each context c âˆˆ C, the function M (c) specifies a
MDP âŸ¨S, A, P c , Rc , Î³âŸ©. As a result, CMDP defines a family
of MDPs that share the same state and action spaces but differ in the state transition and reward functions. In this work,
we employ CMDP to model the non-stationary task dynamics locally perceived by each agent, where the contexts are
associated with other agentsâ€™ joint policies.

4

Methodology

In this section, we give a comprehensive introduction to our
method, DAC. We begin by proposing the task formalization
based on CMDP, and then delve into the process of modeling
dynamics-aware contexts. Subsequently, for each agent, we
learn a context-based value function and derive an optimistic
marginal value, thereby addressing both non-stationarity and
relative overgeneralization issues. Finally, we summarize the
overall learning procedure of DAC.

4.1

Task Formalization

As detailed in Sec. 3.1, the task, as locally perceived by each
agent i, can be modeled as an MDP âŸ¨S, Ai , P i , Ri , Î³âŸ©, where
the state transition dynamics P i and the reward function Ri
depend on other agents âˆ’iâ€™ joint policy Ï€ âˆ’i . Considering
all possible Ï€ âˆ’i , the perceived task of agent i can be decomposed into a family of MDPs that share the same state and
action spaces but differ in their transition and reward functions, each corresponding to an unique Ï€ âˆ’i . By associating
each context c with a specific Ï€ âˆ’i , we propose to formalize
the perceived task of each agent i as a CMDP below:
âŸ¨C, S, Ai , M (c)âŸ©, M (c) : c â†’ âŸ¨S, Ai , Pci , Rci , Î³âŸ©,

(4)

where S represents the state space and Ai is the local action
space of agent i. Note that the state transition dynamics Pci
and the reward function Rci are explicitly conditioned on the
context c âˆˆ C, each corresponding to an unique Ï€ âˆ’i .
When each agent operates in a fully decentralized manner,
under the above CMDP formalization, the task dynamics is
determined by the underlying context, which corresponds to
other agentsâ€™ current joint policy. When an agent encounters
different contexts at different time steps, the same states and
local actions lead to different next states and rewards due to
the distinct task dynamics. Consequently, the lack of context
information hinders each agent from fully capturing the task
dynamics, leading to the non-stationarity problem.
For each agent, this CMDP formalization attributes nonstationarity to switches between unobserved contexts, and
provides a principled framework to address this problem by
explicit context modeling. The context can be instantiated as
either an estimate of the current joint policy of other agents
or a representation of the current agentâ€™s task dynamics distribution. By augmenting per-agent local transitions with the
inferred contexts, the resulting transitions become stationary
and enable stationary fully decentralized policy learning.

Time step 1

ğ‘ 2

ğ‘ 1

Time step t
â€¦

ğ‘Ÿ1

ğ‘ ğ‘¡+1

ğ‘ ğ‘¡

ğ‘1ğ‘–

approximation by minimizing the KL-divergence between
them (Detailed derivation can be found in Appendix. B):
i
i
DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit |Ï„tâˆ’k+1:t
))
i
i
= log pi (Ï„tâˆ’k+1:t
) + DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit )) (6)

ğ‘ğ‘¡ğ‘–

i i
i
i
âˆ’ Eqi (cit |Ï„tâˆ’k+1:t
) log p (Ï„tâˆ’k+1:t |ct ),

ğ‘Ÿğ‘¡

ğ‘1ğ‘–

where pi (cit ) denotes the true prior distribution of the latent
i
variable, and log pi (Ï„tâˆ’k+1:t
) is the evidence that can be regarded as a constant. Based on Eq. (6), to minimize the term
i
i
)||pi (cit |Ï„tâˆ’k+1:t
)), we aim to maximize
DKL (q i (cit |Ï„tâˆ’k+1:t
the following equation:

ğ‘ğ‘¡ğ‘–

Figure 1: A general case where the context changes every (or
every few) time steps within agent iâ€™s local task, depending
on the update interval of other agentsâ€™ joint policy. In the
above plot, the dynamics is determined by ci1 at time step 1
and by cit at time step t. Empty and solid circles respectively
represent observable and unobservable stochastic variables.

The frequency of context changes directly determines the
degree of non-stationarity. In this work, we consider a general case where contexts change between time steps, as illustrated in Fig. 1. In such case, other agents update their joint
policy every (or every few) time steps, and the contexts shift
accordingly within the current agentâ€™s CMDP over the same
interval. We take a step toward explicitly modeling contexts
within this case to address the non-stationarity problem.

i i
i
i
max Eqi (cit |Ï„tâˆ’k+1:t
) log p (Ï„tâˆ’k+1:t |ct )
|
{z
}
1
âƒ
i
âˆ’ DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit )) .
|
{z
}
2
âƒ

(7)

1 denotes the reconstruction likelihood
In Eq. (7), term âƒ
that ensures the learned latent variable cit contains sufficient
i
2
information of the trajectory segment Ï„tâˆ’k+1:t
, and term âƒ
i
ensures the latent variable ct is close to its prior distribution.
1 we expand it as follows:
For optimizing term âƒ,
i
pi (Ï„tâˆ’k+1:t
|cit ) = pi (stâˆ’k+1 )
t
Y

pi (aih |sh )pi (sh+1 , rh |sh , aih , cit ),

(8)

h=tâˆ’k+1

4.2

Dynamics-Aware Context Modeling

For each agent, we propose to represent its real-time task dynamics distribution using latent variables, and refer to them
as contexts. Since other agents may update their joint policy
every (or every few) time steps, the task dynamics distribution within current agentâ€™s local trajectory changes over the
same time interval. This parallels the setting of concept drift,
where the underlying data distribution evolves over time. To
address such shifts, maintaining a sliding window to hold the
latest data within the data stream has proven effective in capturing the real-time data distribution, ensuring model adaptability and accuracy in dynamic settings (Lu et al. 2018).
Motivated by this insight, we cast the per-agent real-time
task dynamics distribution modeling as a concept drift problem, and propose DAC as a solution. At first, DAC employs
a sliding window alongside per-agent local trajectory to hold
the latest k transitions. Specifically, at time step t, the sliding
i
window is instantiated as the trajectory segment Ï„tâˆ’k+1:t
of
agent i, which contains transitions from t âˆ’ k + 1 to t:
i
Ï„tâˆ’k+1:t
=(stâˆ’k+1 , aitâˆ’k+1 , rtâˆ’k+1 , stâˆ’k+2 , . . . ,

st , ait , rt , st+1 ).

(5)

i
For modeling the task dynamics distribution of Ï„tâˆ’k+1:t
,
we assume that this distribution can be represented by a latent variable cit , and the underlying mapping from the trajectory segment to the variable follows an unknown probability
i
distribution pi (cit |Ï„tâˆ’k+1:t
). We learn an additional distrii i i
bution q (ct |Ï„tâˆ’k+1:t ) to approximate it, and optimize this

where the initial state distribution pi (stâˆ’k+1 ) is determined
by the environment and pi (aih |sh ) denotes agent iâ€™s decentralized policy only conditioned on the state. Therefore, we
ignore these two components and rewrite Eq. (7) as follows:
i
maxEqi (cit |Ï„tâˆ’k+1:t
)

t
X

log pi (sh+1 , rh |sh , aih , cit )

h=tâˆ’k+1
i
âˆ’ DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit )).

(9)
Accordingly, DAC is capable of representing the real-time
task dynamics distribution using the latent variable cit , which
i
is derived by the learned distribution q i (cit |Ï„tâˆ’k+1:t
). Then,
these variables are used as contexts to enable stationary policy learning, as detailed in subsequent sections.
Context-based value function. For each agent i, we learn
a value function Qi (st , ait , cit ), which is additionally conditioned on the context cit , besides the state st and the local
action ait . The incorporation of contexts brings two benefits.
On the one hand, the context is derived from each agent iâ€™s
i
local trajectory segment based on q i (cit |Ï„tâˆ’k+1:t
), enabling
fully decentralized policy learning via Qi (st , ait , cit ). On the
other hand, the context alleviates the non-stationarity caused
by other agentsâ€™ evolving joint policy, therefore facilitating
stationary update of Qi (st , ait , cit ). Based on the augmented
transition (st , ait , rt , cit , st+1 , cit+1 ), Qi is updated as below:
LC (Î¸i ) = E(st ,ait ,cit ,rt ,st+1 ,cit+1 )âˆ¼Di
(rt + Î³ max
Qi (st+1 , ait+1 , cit+1 ) âˆ’ Qi (st , ait , cit ))2 ,
i
at+1

(10)

t-k

ğ‘  ğ‘ğ‘–

t-k+1

ğ‘Ÿ

t-k+2

ğ‘  ğ‘ğ‘–

ğ‘Ÿ

t+1

t

ğ‘  â€¦ ğ‘  ğ‘ğ‘–

ğ‘Ÿ

(ğ‘ , ğ‘ğ‘– , ğ‘Ÿ, ğ‘ )ğ‘¡âˆ’ğ‘˜+1 , (ğ‘ , ğ‘ğ‘– , ğ‘Ÿ, ğ‘ )ğ‘¡âˆ’ğ‘˜+2 , â€¦ , (ğ‘ , ğ‘ğ‘– , ğ‘Ÿ, ğ‘ )ğ‘¡

ğ‘ ğ‘¡

FC + ReLU

FC + ReLU

ğ‘ 

ğ‘§ğ‘¡âˆ’ğ‘˜+1 , ğ‘§ğ‘¡âˆ’ğ‘˜+2 , â€¦ , ğ‘§ğ‘¡

GRU

FC
ğ‘¤ğ‘¡âˆ’ğ‘˜+1 , ğ‘¤ğ‘¡âˆ’ğ‘˜+2 , â€¦ , ğ‘¤ğ‘¡

Encoder

Ã—

FC

FC + Gumbel Softmax

ğ‘„Sğ‘– (ğ‘ ğ‘¡ , ğ‘ğ‘¡ğ‘– )

(b)

ğ‘ğ‘¡ğ‘–
ğ‘ â„ , ğ‘â„ğ‘– â„ âˆˆ [ğ‘¡ âˆ’ ğ‘˜ + 1, ğ‘¡]

ğ‘ ğ‘¡

FC + ReLU

FC + ReLU
ğ‘ğ‘¡ğ‘–

Decoder
(a)

Next State

Reward

ğ‘ğ‘¡ğ‘–

FC + ReLU

FC + ReLU

FC

FC

ğ‘ğ‘– (ğ‘ ğ‘¡GRU
, ğ‘ğ‘¡ğ‘– , ğ‘ğ‘¡ğ‘– )

ğ‘ğ‘¡ğ‘–

FC

ğ‘ â„+1 , â„ âˆˆ [ğ‘¡ âˆ’ ğ‘˜ + 1, ğ‘¡] ğ‘Ÿâ„ , â„ âˆˆ [ğ‘¡ âˆ’ ğ‘˜ + 1, ğ‘¡]

(c)

ğ‘„ğ‘– (ğ‘ ğ‘¡ , ğ‘ğ‘¡ğ‘– , ğ‘ğ‘¡ğ‘– )

Figure 2: The architecture of DAC. (a) The VAE-like network which contains the encoder and decoder modules. (b) The value
function QiS . (c) The context-based value function Qi . Note that DAC maintains a sliding window to hold the latest k transitions.
where Î¸i represents the parameter of Qi and we sample transitions from agent iâ€™s replay buffer Di to conduct the update.
Optimistic Marginal Value. There are still two problems
during value function estimation: (1) The relative overgeneralization hinders agents from identifying and selecting their
local cooperative actions; and (2) Each agent can not select
actions based on the context-based value function, where the
context based on real-time task dynamics modeling is available only when the current transition (i.e., (st , ait , rt , st+1 ))
is finished. To deal with these two problems, we propose the
optimistic marginal value for each agent, as defined below:
Ï•i (st , ait ) = max Qi (st , ait , c),
câˆˆC

(11)

where we shape marginal value functions of per-agent local
actions using the maximum context-based value estimations
across all possible contexts. Eq. (11) adheres to an optimistic
belief that other agents always select their cooperative actions, and thus the optimistic marginal value of per-agent
local action can reach the optimal (maximum) value over all
possible contexts maxcâˆˆC Qi (st , ait , c). Consequently, for
each agent, the optimistic marginal value discards the effects
caused by other agentsâ€™ exploratory or sub-optimal actions,
and further enables accurate identifications and selections of
per-agent local cooperative actions, therefore addressing the
relative overgeneralization and leading to the cooperation.
Implementation. For efficiently enumerating all possible
contexts in Eq. (11), we propose to construct a discrete context space by introducing a VAE-like network (Kingma and
Welling 2014). As illustrated in Fig. 2, we achieve the mapi
ping function (distribution) q i (cit |Ï„tâˆ’k+1:t
) by the encoder.
For shaping discrete contexts, the learned contexts should be
i
close to a discrete prior, i.e., DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit )) in
Eq. (7) and Eq. (9). We achieve this by using Gumbel Softmax as the activation function for the encoderâ€™s output. Such
encoder directly generates discrete contexts, thus eliminati
ing the need to optimize DKL (q i (cit |Ï„tâˆ’k+1:t
)||pi (cit )).
In addition, the decoder outputs reconstructed objectives

Pt

i
i
i
h=tâˆ’k+1 log p (sh+1 , rh |sh , ah , ct ) in Eq. (9). As a result,

both encoder and decoder are optimized using the maximum
reconstruction likelihood objective, which is defined below:
i
i
LV (Ï‰ i ) = âˆ’EÏ„tâˆ’k+1:t
âˆ¼D i ,cit âˆ¼q i (cit |Ï„tâˆ’k+1:t
)

t
X

log pi (sh+1 , rh |sh , aih , cit ),

(12)

h=tâˆ’k+1

where Ï‰ i is the parameter of encoder and decoder. The Gumbel Softmax enables differentiable updates of the encoder.

4.3

Overall Learning Procedure

Based on the optimistic marginal value, each agent i can select its local cooperative action by arg maxait Ï•i (st , ait ) =
arg maxait maxcâˆˆC Qi (st , ait , c). However, accurately learning the context-based value function necessitates a thorough
coverage over the entire trajectory space, which is typically
unavailable during the early training period and leads to suboptimal outcomes. To address this issue, we separately learn
a value function QiS (st , ait ) and update it as follows:
LS (Ïƒ i ) = E(st ,ait ,rt ,st+1 )âˆ¼Di
(rt + Î³ max
QiS (st+1 , ait+1 ) âˆ’ QiS (st , ait ))2 , (13)
i
at+1

where Ïƒ i is the parameter of QiS . We make each agent select
its actions based on QiS to generate informative trajectories
during the early training process and accordingly provide an
efficient update of Qi (st , ait , cit ) following Eq. (10). Furthermore, we propose an auxiliary supervision loss below:
LSUP (Ïƒ i ) = DKL (Ï€Si (Â·|st )||Ï€ i (Â·|st )),

(14)

where Ï€Si (Â·|st ) and Ï€ i (Â·|st ) respectively represent the Boltzmann policy with respect to QiS (st , ait ) and Ï•i (st , ait ), which

A2
1

A

1

a
a2
a3

a1

a2

a3

8
-12
-12

-12
6
0

-12
0
6

c
A1
a1 (8.03)
a2 (5.96)
a3 (5.95)

(a) Payoff matrix.

0

1

2

3

X
X
âˆ’0.7
X

âˆ’11
âˆ’12
âˆ’12

8.03
5.96
5.95

X
X
âˆ’5.0
X

0.01
0.06

(b) Q1 (s, a, c) and Ï•1 (s, a).

0.00
âˆ’0.0

c
A2
a1 (8.03)
a2 (5.96)
a3 (5.94)

0

1

2

3

X
X
âˆ’0.7
X

âˆ’11
âˆ’12
âˆ’12

8.03
5.96
5.94

X
X
âˆ’5.0
X

0.01
0.07

0.01
âˆ’0.0

(c) Q2 (s, a, c) and Ï•2 (s, a).

Table 1: The payoff matrix and value functions learned by DAC. We set the number of discrete contexts to 4. The context-based
value functions Q1 (s, a, c) and Q2 (s, a, c) for all contexts c are presented in (b) and (c), respectively. The optimistic marginal
X X
values, Ï•1 (s, a) and Ï•2 (s, a), appear in the first column. Qi (s, a, c) with unattainable value is marked by QiX
(s, a,X
c).
X
are defined as follows:
exp(QiS (st , ait ))
,
Ï€Si (ait |st ) = P
i
i
ai âˆˆAi exp(QS (st , a ))
exp(Ï•i (st , ait ))
Ï€ (ait |st ) = P
.
i
i
ai âˆˆAi exp(Ï• (st , a ))

(15)

i

In comparison to Ï€Si , Ï€ i is capable of addressing both nonstationarity and relative overgeneralization, and we make Ï€Si
imitate it. As a result, we update QiS using the loss function
L(Ïƒ i ) = LS (Ïƒ i ) + Î²LSUP (Ïƒ i ), where Î² is a scaling factor.
As a summary, the learning processes of QiS and Qi form
a positive feedback loop: QiS generates informative trajectories to ensure effective updates of Qi , while Qi guides QiS in
selecting agentsâ€™ local cooperative actions to produce more
informative trajectories. This mutual reinforcement facilitates efficient, fully decentralized cooperative policy learning. More algorithmic details are available in Appendix. C.

5

Experiment

In this section, we design experiments to answer the following questions: (1) Can DAC benefit fully decentralized cooperative policy learning by addressing non-stationarity and
relative overgeneralization issues? (2) If so, which component contributes the most to its performance gain?
For question (1), we compare our method against multiple
fully decentralized value-based MARL baselines, including
IQL (Tan 1993), Hysteretic Q-learning (Matignon, Laurent,
and Le Fort-Piat 2007), and I2Q (Jiang and Lu 2022), on
the matrix game, predator and prey, and the StarCraft MultiAgent Challenge (SMAC). For question (2), we conduct ablation studies to assess the effectiveness of each component
in DAC. All results are illustrated with the median performance and the standard error across five random seeds. More
experimental details can be found in Appendix D.

5.1

Comparison Results

Matrix Game. We begin by evaluating all methods in a didactic matrix game. As shown in Tab. 1a, two agents within
this game need to select the optimal joint action (a1 , a1 ) to
achieve the best reward +8. However, under the fully decentralized learning paradigm, each agent maintains higher
value estimations regarding its local actions a2 and a3 , when
other agents uniformly select their actions. This leads to the
relative overgeneralization problem where sub-optimal joint
actions are preferred over the optimal one.

Fig. 3 (a) shows the comparison results of all methods in
the matrix game. One can observe that IQL struggles in the
sub-optimal joint actions with +6 rewards, demonstrating
that the average-based projection (Eq. (2)) followed by IQL
is susceptible to the relative overgeneralization problem. In
contrast, DAC presents an optimistic marginal value for each
agent and accordingly discards the effects caused by othersâ€™
sub-optimal actions. As a result, DAC succeeds in selecting
the optimal joint action with +8 reward. This also applies to
Hysteretic Q-learning, which adheres to an optimistic value
function update and demonstrates efficiency in simple tasks.
However, when faced with complex cooperative tasks, such
optimistic value function update typically leads to overestimation for value function approximated by neural networks,
leading to poor performance. We validate this insight in subsequent experiments. Similarly, I2Q shapes ideal transitions
by implicitly assuming other agents follow cooperative policies, which adheres to an optimistic belief, and learning policies on these transitions leads to the optimal joint policy.
Furthermore, to analyze the representational capabilities
of the contexts, we present the per-agent context-based value
function and optimistic marginal value learned by DAC. As
depicted in Tab. 1b and Tab. 1c, for agent 1 or 2, the contextbased value function accurately approximates the rewards of
all possible joint actions, which validates the effectiveness of
dynamics-aware contexts in representing other agentsâ€™ joint
policies. Furthermore, the optimistic marginal values of peragent local actions satisfy the optimistic property, i.e., equal
to the highest rewards only achieved when other agents select their cooperative actions. By enforcing per-agent value
function to imitate the optimistic marginal value by Eq. (14),
the efficient selection of optimal joint actions are achieved.
Predator and Prey. To further assess the effectiveness of
DAC in addressing relative overgeneralization, we adopt the
modified predator-prey (Son et al. 2019), where two predators receive a team reward of +1 when they simultaneously
capture the single prey, otherwise âˆ’P penalty for sole hunting. P controls the degree of relative overgeneralization.
We test four settings P = 0.0, 0.3, 0.5, 1.0. In general, a
small penalty (P = 0.0) does not induce relative overgeneralization, and thus all methods succeed in learning cooperative prey-capturing policies with high rewards, as illustrated
in Fig. 3 (b). As P increases and relative overgeneralization
becomes more severe, IQL fails to learn effective policies in
scenarios with P = 0.3, 0.5, and 1.0. In contrast, Hysteretic
Q-learning and DAC achieve high rewards in scenario with

DAC (ours)

IQL

Hysteretic-Q
(b) P = 0.0

(a) Matrix Game

I2Q
(c) P = 0.3

6
4
2

75

Test Return

Test Return

Test Return

8

50
25

0

1

2

3

4

5

0.0

0.5

1.0

1e4

Time Steps

2.0

2.5

25

3.0

0.0

0.5

1.0

1e6

(e) P = 1.0

25
0

2.0

2.5

3.0

1e6

(f) 5m_vs_6m
Test Win Rate

50

1.5

Time Steps

75

75

Test Return

Test Return

1.5

Time Steps

(d) P = 0.5

50
25
0

0.6
0.4
0.2
0.0

0.0

0.5

1.0

1.5

2.0

2.5

3.0

0.0

0.5

1.0

1e6

Time Steps

1.5

2.0

2.5

(g) 3s_vs_4z

3.0

0.25

1.0

Time Steps

1.5

2.0

1.5

2.0

1e6

(i) 2s_vs_1sc
1.00

0.6
0.4
0.2
0.0

0.00

1.0

Time Steps

Test Win Rate

Test Win Rate

0.50

0.5

0.5

(h) 3s_vs_5z

0.75

0.0

0.0

1e6

Time Steps

1.00

Test Win Rate

50

0

0

0

75

0.75
0.50
0.25
0.00

0.0

0.5

1e6

1.0

Time Steps

1.5

2.0

0.0

1e6

0.5

1.0

Time Steps

1.5

2.0

1e6

Figure 3: Comparison results in the matrix game, modified predator-prey, and several SMAC maps.
P = 0.3 and 0.5, due to their capabilities of selecting cooperative actions based on optimistic value estimates. Notably,
Hysteretic Q-learning suffers from degraded asymptotic performance due to the over-estimation issue. In the challenging
scenario with P = 1.0, only DAC succeeds in learning an effective policy yielding high rewards, while all other baseline
methods fail. We attribute DACâ€™s consistent superiority to its
decoupled learning framework, which separately models the
stationary context-based value function and the optimistic
marginal value. I2Q demonstrates poor performance in scenarios with P = 0.3, 0.5, and 1.0, suggesting that shaping
ideal transition probabilities is difficult in certain tasks.

SMAC. To validate the scalability of DAC in more complex tasks, we evaluate all methods on several SMAC maps,
including 2s vs 1sc, 3s vs 4z, 3s vs 5z, and 5m vs 6m. As
depicted in Fig 3 (f) - (i), Hysteretic Q-learning exhibits poor
performance across all maps, due to the overestimation and
non-stationarity issues exacerbated in complex tasks. While
IQL achieves competitive performance on most maps, DAC
outperforms IQL on 5m vs 6m, 3s vs 4z, and 3s vs 5z, validating DACâ€™s effectiveness in complex tasks. On 2s vs 1sc,
DAC performs comparably to IQL. We hypothesis that this
map poses few challenges from non-stationarity and relative
overgeneralization, and thus the benefit brought by DAC is
not obvious. In addition, I2Q suffers from poor performance
across all maps. We attribute this to its incapability of shaping ideal transition probabilities under partial observability.

5.2

Ablation Study

To examine the effects of DACâ€™s components: (1) the sliding
window size (k), (2) the number of contexts (m), and (3) the
scaling factor (Î²), we set k to 10, 15, 20, 30, m to 5, 10, 20,
30, and Î² to 0.001, 0.01, 0.1, 1.0 as multiple baselines. The
results, as presented in Appendix E, indicate that an excessively large m results in inefficient updates of the contextbased value function due to the expanded context space. Regarding k, a small value may lead to abrupt fluctuations in
the inferred dynamics distribution, while a large value may
fail to capture timely distributional changes. We empirically
identify k = 20 as a suitable choice for 5m vs 6m. The scaling factor is critical to maintain the positive feedback loop
between QiS and Qi . Our experiments suggest that Î² = 0.01
performs effectively across complex SMAC maps.

6

Conclusion

This paper presents DAC as a unified framework to address
both non-stationarity and relative overgeneralization inherent in fully decentralized cooperative MARL. DAC formalizes the local task, as perceived by each agent, as a CMDP,
and constructs contexts by modeling step-wise task dynamics distribution. Subsequently, for each agent, DAC learns a
context-based value function to enable stationary policy updates, and derives an optimistic marginal value to encourage
the selection of cooperative actions. Extensive experiments
across various cooperative tasks validate its effectiveness.
Limitation and Future Work. There are two limitations
that warrant further exploration. First, DAC requires a com-

prehensive coverage of the trajectory space, while separately
learning a decentralized value function and maintaining mutual reinforcement with the context-based value function can
be challenging in complex tasks. We plan to address this by
integrating DAC with efficient exploration techniques. Second, the optimistic marginal value computation becomes increasingly costly as the context space grows large. This can
be alleviated by employing sampling-based heuristic search
approaches to find approximate maxima and accordingly enable moderate complexity. We leave them for future work.

References
Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and
Whiteson, S. 2018. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial
intelligence, volume 32.
Foerster, J.; Nardelli, N.; Farquhar, G.; Afouras, T.; Torr,
P. H.; Kohli, P.; and Whiteson, S. 2017. Stabilising experience replay for deep multi-agent reinforcement learning. In
International conference on machine learning, 1146â€“1155.
PMLR.
Gupta, T.; Mahajan, A.; Peng, B.; BoÌˆhmer, W.; and Whiteson, S. 2021. Uneven: Universal value exploration for multiagent reinforcement learning. In International Conference
on Machine Learning, 3930â€“3941. PMLR.
Hallak, A.; Di Castro, D.; and Mannor, S. 2015. Contextual markov decision processes.
arXiv preprint
arXiv:1502.02259.
Hao, Q.; Huang, W.; Feng, T.; Yuan, J.; and Li, Y. 2023. Gatmf: Graph attention mean field for very large scale multiagent reinforcement learning. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and
Data Mining, 685â€“697.
Jiang, J.; and Lu, Z. 2022. I2Q: A fully decentralized Qlearning algorithm. Advances in Neural Information Processing Systems, 35: 20469â€“20481.
Jiang, J.; Su, K.; and Lu, Z. 2024. Fully decentralized cooperative multi-agent reinforcement learning: A survey. arXiv
preprint arXiv:2401.04934.
Kingma, D. P.; and Welling, M. 2014. Auto-Encoding Variational Bayes. In International Conference on Learning Representations.
Lauer, M.; and Riedmiller, M. A. 2000. An algorithm
for distributed reinforcement learning in cooperative multiagent systems. In Proceedings of the seventeenth international conference on machine learning, 535â€“542.
Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Pieter Abbeel, O.;
and Mordatch, I. 2017. Multi-agent actor-critic for mixed
cooperative-competitive environments. Advances in neural
information processing systems, 30.
Lu, J.; Liu, A.; Dong, F.; Gu, F.; Gama, J.; and Zhang, G.
2018. Learning under concept drift: A review. IEEE transactions on knowledge and data engineering, 31(12): 2346â€“
2363.
Matignon, L.; Laurent, G. J.; and Le Fort-Piat, N. 2007. Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In 2007

IEEE/RSJ International Conference on Intelligent Robots
and Systems, 64â€“69. IEEE.
Matignon, L.; Laurent, G. J.; and Le Fort-Piat, N. 2012.
Independent reinforcement learners in cooperative markov
games: a survey regarding coordination problems. The
Knowledge Engineering Review, 27(1): 1â€“31.
Omidshafiei, S.; Pazis, J.; Amato, C.; How, J. P.; and Vian, J.
2017. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International
Conference on Machine Learning, 2681â€“2690. PMLR.
Panait, L.; Sullivan, K.; and Luke, S. 2006. Lenient learners
in cooperative multiagent systems. In Proceedings of the
fifth international joint conference on Autonomous agents
and multiagent systems, 801â€“803.
Rashid, T.; Farquhar, G.; Peng, B.; and Whiteson, S.
2020a. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 33:
10199â€“10210.
Rashid, T.; Samvelyan, M.; De Witt, C. S.; Farquhar, G.; Foerster, J.; and Whiteson, S. 2020b. Monotonic value function
factorisation for deep multi-agent reinforcement learning.
The Journal of Machine Learning Research, 21(1): 7234â€“
7284.
Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Foerster, J.; and Whiteson, S. 2019. The starcraft multi-agent
challenge. arXiv preprint arXiv:1902.04043.
Son, K.; Kim, D.; Kang, W. J.; Hostallero, D. E.; and Yi, Y.
2019. Qtran: Learning to factorize with transformation for
cooperative multi-agent reinforcement learning. In International conference on machine learning, 5887â€“5896. PMLR.
Su, K.; Zhou, S.; Jiang, J.; Gan, C.; Wang, X.; and Lu, Z.
2024. Multi-Agent Alternate Q-Learning. In Proceedings
of the 23rd International Conference on Autonomous Agents
and Multiagent Systems, 1791â€“1799.
Sunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W. M.; Zambaldi, V.; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,
J. Z.; Tuyls, K.; et al. 2017. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296.
Tan, M. 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth
international conference on machine learning, 330â€“337.
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2020a.
Qplex: Duplex dueling multi-agent q-learning.
arXiv
preprint arXiv:2008.01062.
Wang, X.; Ke, L.; Qiao, Z.; and Chai, X. 2020b. Large-scale
traffic signal control using a novel multiagent reinforcement
learning. IEEE transactions on cybernetics, 51(1): 174â€“187.
Wei, E.; and Luke, S. 2016. Lenient learning in independentlearner stochastic cooperative games. Journal of Machine
Learning Research, 17(84): 1â€“42.
Yu, C.; Velu, A.; Vinitsky, E.; Gao, J.; Wang, Y.; Bayen, A.;
and Wu, Y. 2022. The surprising effectiveness of ppo in
cooperative multi-agent games. Advances in Neural Information Processing Systems, 35: 24611â€“24624.

Zhong, Y.; Kuba, J. G.; Feng, X.; Hu, S.; Ji, J.; and Yang, Y.
2024. Heterogeneous-agent reinforcement learning. Journal
of Machine Learning Research, 25(32): 1â€“67.
Zhou, M.; Luo, J.; Villella, J.; Yang, Y.; Rusu, D.; Miao,
J.; Zhang, W.; Alban, M.; Fadakar, I.; Chen, Z.; et al.
2021. Smarts: An open-source scalable multi-agent rl training school for autonomous driving. In Conference on robot
learning, 264â€“285. PMLR.

