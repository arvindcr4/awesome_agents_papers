Here is the extracted information:

**PAPER:** Multi-Agent Reinforcement Learning with Selective State-Space Models
**TITLE:** Multi-Agent Reinforcement Learning with Selective State-Space Models
**ARXIV_ID:** 2410.19382v2
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** The paper proposes a new architecture for multi-agent reinforcement learning (MARL) called Multi-Agent Mamba (MAM), which replaces the attention mechanism in the Multi-Agent Transformer (MAT) with Mamba blocks. Mamba blocks are a type of selective state-space model that allows for efficient and scalable processing of sequential data. The authors demonstrate the effectiveness of MAM in various MARL benchmarks, showing that it matches the performance of MAT while offering improved efficiency and scalability.
**KEY_CONTRIBUTIONS:**
* Introduction of the Multi-Agent Mamba (MAM) architecture, which replaces attention in MAT with Mamba blocks
* Demonstration of MAM's effectiveness in various MARL benchmarks, including Robotic Warehouse, StarCraft Multi-Agent Challenge, and Level-Based Foraging
* Comparison of MAM's performance with MAT and MAPPO, showing that MAM matches or outperforms MAT in 18 out of 21 tasks
* Analysis of the computational efficiency of MAM, showing that it scales better than MAT in terms of the number of agents
* Ablation study of the hyperparameters of MAM, showing that smaller values of the hidden state dimension and projection dimension can lead to better performance and stability.
