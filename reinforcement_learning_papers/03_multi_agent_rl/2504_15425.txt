Robotics: Science and Systems 2025
Los Angeles, California, June 21-June 25, 2025

Solving Multi-Agent Safe Optimal Control with
Distributed Epigraph Form MARL
Songyuan Zhang‚àó‚Ä† , Oswin So‚àó‚Ä† , Mitchell Black‚Ä° , Zachary Serlin‚Ä° and Chuchu Fan‚Ä†
‚Ä† Department of Aeronautics and Astronautics, MIT, Cambridge, Massachusetts, USA

Email: {szhang21, oswinso, chuchu}@mit.edu

‚Ä° MIT Lincoln Laboratory, Lexington, Massachusetts, USA

arXiv:2504.15425v1 [cs.RO] 21 Apr 2025

Email: {mitchell.black, Zachary.Serlin}@ll.mit.edu

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

Fig. 1: Two agents using Def-MARL to safely and collaboratively inspect a moving target. We propose a novel safe
MARL algorithm, Def-MARL, that solves the multi-agent safe optimal control problem. Def-MARL translates the original
problem to its epigraph form to avoid unstable training and extends the epigraph form to the CTDE paradigm for distributed
execution. (a): Long exposure photo of the trajectories of the drones. The trajectory of the target is shown in green and that
of the agents is shown in blue. (b)-(i): Snapshots of the agents‚Äô policy. Using Def-MARL, the agents learn to collaborate to
maintain visual contact with the target at all times, with each agent being responsible only when the target is on their side.
Abstract‚ÄîTasks for multi-robot systems often require the
robots to collaborate and complete a team goal while maintaining
safety. This problem is usually formalized as a constrained
Markov decision process (CMDP), which targets minimizing
a global cost and bringing the mean of constraint violation
below a user-defined threshold. Inspired by real-world robotic
applications, we define safety as zero constraint violation. While
many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms
suffer from unstable training in this setting. To tackle this, we
use the epigraph form for constrained optimization to improve
training stability and prove that the centralized epigraph form
problem can be solved in a distributed fashion by each agent.
This results in a novel centralized training distributed execution
MARL algorithm named Def-MARL. Simulation experiments
on 8 different tasks across 2 different simulators show that
Def-MARL achieves the best overall performance, satisfies safety
constraints, and maintains stable training. Real-world hardware
experiments on Crazyflie quadcopters demonstrate the ability
of Def-MARL to safely coordinate agents to complete complex
collaborative tasks compared to other methods.1

I. I NTRODUCTION
Multi-agent systems (MAS) play an integral role in our
aspirations for a more convenient future with examples such
‚àó Equal contribution.
1 Project website: https://mit-realm.github.io/def-marl/.
DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.

as autonomous warehouse operations [37], large-scale autonomous package delivery [45], and traffic routing [76].
These tasks often require designing distributed policies for
agents to complete a team goal collaboratively while maintaining safety. To construct distributed policies, multi-agent
reinforcement learning (MARL) under the centralized training distributed execution (CTDE) paradigm [84, 25] has
emerged as an attractive method. To incorporate the safety
constraints, most MARL algorithms either choose to carefully
design their objective function to incorporate soft constraints
[69, 58, 79, 74, 54, 57], or model the problem using the
constrained Markov decision process (CMDP) [3], which
asks for the mean constraint violation to stay below a userdefined threshold [32, 38, 18, 41, 26, 90]. However, real-world
robotic applications always require zero constraint violation.
While this can be addressed by setting the constraint violation
threshold to zero in the CMDP, in this setting the popular
Lagrangian methods experience training instabilities which
result in sharp drops in performance during training, and nonconvergence or convergence to poor policies [66, 35, 24, 36].
These concerns have been identified recently, resulting in a
series of works that enforce hard constraints [82, 89, 66, 24,
86] using techniques inspired by Hamilton-Jacobi reachability
[71, 44, 49, 46, 5] in deep reinforcement learning (RL) for
the single-agent case and have been shown to improve safety

compared to other safe RL approaches significantly. However,
to the best of our knowledge, theories and algorithms for safe
RL are still lacking for the multi-agent scenario, especially
when policies are executed in a distributed manner. While
single-agent RL methods can be directly applied to the MARL
setting by treating the MAS as a centralized single agent, the
joint action space grows exponentially with the number of
agents, preventing these algorithms from scaling to scenarios
with a large number of agents [33, 69, 23].
To tackle the problem of zero constraint violation in multiagent scenarios with distributed policies2 while achieving high
collaborative performance, we propose Distributed epigraph
form MARL (Def-MARL) (Fig. 1). Instead of considering
the CMDP setting, Def-MARL directly tackles the multi-agent
safe optimal control problem (MASOCP), whose solution
satisfies zero constraint violation. To solve the MASOCP,
Def-MARL uses the epigraph form technique [10], which
has previously been shown to yield better policies compared
to Lagrangian methods in the single-agent setting [66]. To
adapt to the multi-agent setting we consider in this work, we
prove that the centralized epigraph form of MASOCP can be
solved in a distributed fashion by each agent. Using this result,
Def-MARL falls under the CTDE paradigm.
We validate Def-MARL using 8 different tasks from 2
different simulators, multi-particle environments (MPE) [40]
and Safe Multi-agent MuJoCo [32], with varying numbers
of agents, and compare its performance with existing safe
MARL algorithms using the penalty and Lagrangian methods.
The results suggest that Def-MARL achieves the best performance while satisfying safety: it is as safe as conservative
baselines that achieve high safety but sacrifice performance,
while matching the performance of unsafe baselines that
sacrifice safety for high performance. In addition, while the
baseline methods require different choices of hyperparameters to perform well in different environments and suffer
from unstable training because of zero constraint violation
threshold, Def-MARL is stable in training using the same
hyperparameters across all environments, indicative of the
algorithm‚Äôs robustness to environmental changes.
We also perform real-world hardware experiments using
the Crazyflie (CF) drones [27] on two complex collaborative
tasks and compare Def-MARL with both centralized and
decentralized model predictive control (MPC) methods [29].
The results indicate that Def-MARL finishes the tasks with
100% safety rates and success rates, while the MPC methods
get stuck in local minima or have unsafe behaviors.
To summarize, our contributions are presented below:
‚Ä¢ Drawing on prior work that addresses the training instability of Lagrangian methods in the zero-constraint
violation setting, we extend the epigraph form method
from single-agent RL to MARL, improving upon the
training instability of existing MARL algorithms.
2 In this paper, the policies are distributed if each agent makes decisions
using local information/sensor data and information received via message
passing with other agents [25], although this setting is sometimes called
‚Äúdecentralized‚Äù in MARL [83].

We present theoretical results showing that the outer problem of the epigraph form can be decomposed and solved
in a distributed manner during online execution. This
allows Def-MARL to fall under the CTDE paradigm.
‚Ä¢ We illustrate through extensive simulations that, without
any hyperparameter tuning, Def-MARL achieves stable
training and is as safe as the most conservative baseline
while simultaneously being as performant as the most
aggressive baseline across all environments.
‚Ä¢ We demonstrate on Crazyflie drones in hardware that
Def-MARL can safely coordinate agents to complete
complex collaborative tasks. Def-MARL performs the
task better than centralized/decentralized MPC methods
and does not get stuck in suboptimal local minima or
exhibit unsafe behaviors.
‚Ä¢

II. R ELATED WORK
Unconstrained MARL. Early works that approach the
problem of safety for MARL focus on navigation problems
and collision avoidance [14, 13, 21, 64], where safety is
achieved by a sparse collision penalty [39], or a shaped
reward penalizing getting close to obstacles and neighboring
agents [14, 13, 21, 64]. However, adding a penalty to the
reward function changes the original objective function, so the
resulting policy may not be optimal for the original constraint
optimization problem. In addition, the satisfaction of collision
avoidance constraints is not necessarily guaranteed by even
the optimal policy [47, 21, 39].
Shielding for Safe MARL. One popular method that
provides safety to learning-based methods is using shielding
or a safety filter [25]. Here, an unconstrained learning method
is paired with a shield or safety filter using techniques such
as predictive safety filters [88, 50], control barrier functions
[11, 55], or automata [19, 77, 48, 7]. Such shields are often
constructed before the learning begins and are used to modify
either the feasible actions or the output of the learned policy
to maintain safety. One benefit is that safety can be guaranteed during both training and deployment since the shield
is constructed before training. However, they require domain
expertise to build a valid shield, which can be challenging in
the single-agent setting and even more difficult for MAS [25].
Other methods can automatically synthesize shields but face
scalability challenges [48, 20]. Another drawback is that the
policy after shielding might not consider the same objective
as the original policy and may result in noncollaborative
behaviors or deadlocks [56, 85, 87].
Constrained MARL. In contrast to unconstrained MARL
methods, which change the constraint optimization problem
to an unconstrained problem, constrained MARL methods
explicitly solve the CMDP problem. For the single-agent
case, prominent methods for solving CMDPs include primal
methods [78], primal-dual methods using Lagrange multipliers
[8, 70, 35, 36], and trust-region-based approaches [1, 35].
These methods provide guarantees either in the form of asymptotic convergence guarantees to the optimal (safe) solution
[8, 70] using stochastic approximation theory [59, 9], or

recursive feasibility of intermediate policies [1, 60] using
ideas from trust region optimization [61]. The survey [31]
provides an overview of the different methods of solving
safety-constrained single-agent RL. In multi-agent cases, however, the problem becomes more difficult because of the nonstationary behavior of other agents, and similar approaches
have been presented only recently [32, 38, 18, 41, 26, 90, 15].
However, the CMDP setting they handle makes it difficult
for them to handle hard constraints, and results in poor
performance with zero constraint violation threshold [24].
Model predictive control. Distributed MPC methods
have been proposed to handle MAS, incorporating multi-agent
path planning, machine learning, and distributed optimization
[75, 72, 92, 22, 42, 16, 52]. However, the solution quality of
nonlinear optimizers used to solve MPC when the objective
function and constraints are nonlinear highly depends on
the initial guess [73, 29]. Moreover, the real-time nonlinear
optimizers typically require access to (accurate) first and
second-order derivatives [53, 29], which present challenges
when trying to solve tasks that have non-differentiable or
discontinuous cost functions and constraints such as the ones
we consider in this work.
III. P ROBLEM SETTING AND PRELIMINARIES
A. Multi-agent safe optimal control problem
We consider the multi-agent safe optimal control problem
(MASOCP) as defined below. Consider a homogeneous MAS
with N agents. At time step k, the global state and control
input are given by xk ‚àà X ‚äÜ Rn and uk ‚àà U ‚äÜ Rm ,
respectively. The global control vector is defined by concatenation uk := [uk1 ; . . . ; ukN ], where uki ‚àà Ui is the control input
of agent i. We consider the general nonlinear discrete-time
dynamics for the MAS:
xk+1 = f (xk , uk ),

(1)

where f : X √ó U ‚Üí X is the global dynamics function. We
consider the partially observable setting, where each agent has
a limited communication radius R > 0 and can only communicate with other agents or observe the environment within its
communication region. Denote oki = Oi (xk ) ‚àà O ‚äÜ Rno as
the vector of the information observed by agent i at the time
step k, where Oi : X ‚Üí O is an encoding function of the
information shared from neighbors of agent i and the observed
data of the environment. We allow multi-hop communication
between agents, so an agent may communicate with another
agent outside its communication region if a communication
path exists between them.
Let the avoid/unsafe set of agent i be Ai := {oi ‚àà O :
hi (oi ) > 0}, for some function hi : O ‚Üí R. The global
avoid set is then defined as A := {x ‚àà X : h(x) > 0},
where h(x) = maxi hi (oi ) = maxi hi (Oi (x)). In other words,
‚àÉi, s.t. oi ‚àà Ai ‚áê‚áí x ‚àà A. Given a global cost function l :
X √ó U ‚Üí R describing the task for the agents to accomplish3 ,
3 The cost function l is not the cost in CMDP. Rather, it corresponds to the
negation of the reward in CMDP.

we aim to find distributed control policies œÄi : O ‚Üí Ui such
that starting from any given initial states x0 ‚àà
/ A, the policies
keep the agents outside the avoid set A and minimize the
infinite horizon cost. In other words, denoting œÄ : X ‚Üí U
as the joint policy such that œÄ(x) = [œÄ1 (o1 ); . . . ; œÄN (oN )] =
[œÄ1 (O1 (x)); . . . ; œÄN (ON (x))], we aim to solve the following
infinite-horizon MASOCP for a given initial state x0 :
‚àû
X

min

{œÄi }N
i=1

s.t.

l(xk , œÄ(xk ))

(2a)

k=0

hi (Oi (xk )) ‚â§ 0,

x

k+1

k

k

= f (x , œÄ(x )),

‚àÄi ‚àà {1, . . . , N }, k ‚â• 0, (2b)
k ‚â• 0.

(2c)

Note that the safety constraint (2b) differs from the average
constraints considered in CMDPs [3]. Consequently, instead
of allowing safety violations to occur as long as the mean
constraint violation is below a threshold, this formulation
disallows any constraint violation. From hereon, we omit the
dynamics constraint (2c) for conciseness.
B. Epigraph form
Existing methods are unable to solve (2) well. This has been
observed previously in the single-agent setting [82, 89, 66, 24].
We show later that the poor performance of methods that
tackle the CMDP setting to the constrained problem (2)
also translates to the multi-agent setting, as we observe a
similar phenomenon in our experiments (Section V). Namely,
although unconstrained MARL can be used to solve (2)
using the penalty method [51], this does not perform well
in practice, where a small penalty results in policies that
violate constraints, and a large penalty results in higher total
costs. The Lagrangian method [32] can solve the problem
theoretically, but it suffers from unstable training and has
poor performance in practice when the constraint violation
threshold is zero [66, 24]. In this section, we introduce a new
method of solving (2) that can mitigate the above problems
by extending prior work [66] to the multi-agent setting.
Given a constrained optimization
problem with objective
P‚àû
k
k
function J (e.g., J =
l(x
,
œÄ(x
)) as in (2a)), and
k=0
constraints h (e.g., (2b)):
min
œÄ

J(œÄ)

s.t.

h(œÄ) ‚â§ 0,

(3)

its epigraph form [10] is given as
min
œÄ,z

z

s.t.

h(œÄ) ‚â§ 0,

J(œÄ) ‚â§ z,

(4)

where z ‚àà R is an auxiliary variable. In other words, we add
a constraint to enforce z as an upper bound of the cost J(œÄ),
then minimize z. The solution to (4) is identical to the original
problem (3) [10]. Furthermore, (4) is equivalent [66] to
min z

(5a)

z

s.t.

min Jz (œÄ, z) := max{h(œÄ), J(œÄ) ‚àí z} ‚â§ 0
œÄ

(5b)

As a result, the original constrained problem (3) is decomposed into the following two subproblems:

1) An unconstrained inner problem (5b), where, given an
arbitrary desired cost upper bound z, we find œÄ such that
Jz (œÄ, z) is minimized, i.e., best satisfies the constraints
h ‚â§ 0 and J ‚â§ z.
2) A 1-dimensional constrained outer problem (5a) over z,
which finds the smallest cost upper bound z such that z
is indeed a cost upper bound (J ‚â§ z) and the constraints
of the original problem h(œÄ) ‚â§ 0 holds.
Comparison with the Lagrangian method. Another popular way to solve MASOCP (2) is the Lagrangian method [32].
However, it suffers from unstable training when considering
the zero constraint violation [66, 35] setting.
P‚àûMore specifically,
this refers to the case with constraints k=0 c(xk ) ‚â§ 0 for
c : X ‚Üí R‚â•0 non-negative. Since h can be negative, we can
convert our problem setting (3) to the zero constraint violation
setting by taking c(x) := max{0, h(x)}. Then, (3) reads as
min
œÄ

J(œÄ)

s.t.

‚àû
X
k=0

max{0, h(xk )} ‚â§ 0.

(6)

Œª‚â•0

œÄ

JŒª (œÄ, Œª) := J(œÄ) + Œª

‚àû
X

i
k‚â•œÑ
k
= max max hi (oi ) = max Vih (oœÑi ; œÄ).
i
i
k‚â•œÑ
k‚â•œÑ

(9)

Here, we interchange the max to define the local per-agent
functions Vih (oœÑi ; œÄ) = maxk‚â•œÑ hi (oki ). Each Vih uses only
the agent‚Äôs local observation and thus is distributed. We now
introduce the auxiliary variable z for the desired upper bound
of V l , allowing us to restate (2) concisely as
V l (x0 ; œÄ)

min

{œÄi }N
i=1

s.t.

V h (x0 ; œÄ) ‚â§ 0.

(10)

The epigraph form (5) of (10) then takes the form
min z

(11a)

z

s.t.


min max max Vih (oœÑi ; œÄ), V l (xœÑ ; œÄ) ‚àí z ‚â§ 0.
i
{œÄi }N
i=1 |
{z
}
(11b)

max{h(xk ), 0},

(7)

k=0

where Œª is the Lagrangian multiplier and
P‚àûis updated with gradi‚àÇ
JŒª (œÄ, Œª) = k=0 max{h(xk ), 0} ‚â•
ent ascent. However, ‚àÇŒª
0, so Œª continuously increases and never decreases. As
‚àÇ
k
‚àÇœÄ JŒª (œÄ, Œª) scales linearly in Œª when h(x ) > 0 for some k,
a large value of Œª causes a large gradient w.r.t x, and makes
the training unstable. Note that for the epigraph form, since z
does not multiply with the cost function J but is added to J
‚àÇ
in (5b), the gradient ‚àÇœÄ
Jz (œÄ, z) does not scale with the value
of z resulting in more stable training. We validate this in our
experiments (Section V).
IV. D ISTRIBUTED EPIGRAPH FORM MULTI - AGENT
REINFORCEMENT LEARNING

In this section, we propose the Distributed epigraph form
MARL (Def-MARL) algorithm to solve MASOCP (2) using
MARL. First, we transfer MASOCP (2) to its epigraph form
with an auxiliary variable z to model the desired cost upper
bound. The epigraph form includes an inner problem and
an outer problem. For distributed execution, we provide a
theoretical result that the outer problem can be solved distributively by each agent. This allows Def-MARL to fit the CTDE
paradigm, where in centralized training, the agents‚Äô policies
are trained together given the desired cost upper bound z,
and in distributed execution, the agents distributively find the
smallest cost upper bound z that ensures safety.
A. Epigraph form for MASOCP
To rewrite MASOCP (2) into its epigraph form (5), we first
define the cost-value function V l for a joint policy œÄ using
the standard optimal control notation [6]:
X
V l (xœÑ ; œÄ) :=
l(xk , œÄ(xk )).
(8)
k‚â•œÑ

V h (xœÑ ; œÄ) := max h(xk ) = max max hi (oki )

:=V (x0 ,z;œÄ)

The Lagrangian form of (6) is then
max min

We also define the constraint-value function V h as the maximum constraint violation:

By interpreting the left-hand side of (11b) as a new policy
optimization problem, we define the total value function V as
the objective function to (11b). This can be simplified as
V (xœÑ , z; œÄ) = max{max Vih (oœÑi ; œÄ), V l (xœÑ ; œÄ) ‚àí z}
i

= max max{Vih (oœÑi ; œÄ), V l (xœÑ ; œÄ) ‚àí z}
i

(12)

œÑ

= max Vi (x , z; œÄ),
i

Again, we interchange the max to define Vi (xœÑ , z; œÄ) =
max{Vih (oœÑi ; œÄ), V l (xœÑ ; œÄ) ‚àí z} as the per-agent total value
function. Using this to rewrite (11) then yields
min z

(13a)

z

s.t. min max Vi (x0 , z; œÄ) ‚â§ 0.
œÄ

i

(13b)

This decomposes the original problem (2) into an unconstrained inner problem (13b) over policy œÄ and a constrained
outer problem (13a) over z. During offline training, we solve
the inner problem (13b): for parameter z, find the optimal
policy œÄ(¬∑, z) to minimize V (x0 , z; œÄ). Note that the optimal
policy of the inner problem depends on z. During execution,
we solve the outer problem (13a) online to get the minimal z that satisfies constraint (13b). Using this z in the zconditioned policy œÄ(¬∑, z) found in the inner problem gives
us the optimal policy for the overall epigraph form MASOCP
(EF-MASOCP).
To solve the inner problem (13b), the total value function V
must be amenable to dynamic programming, which we show
in the following proposition.
Proposition 1: Dynamic programming can be applied to
EF-MASOCP (13), resulting in
V (xk , z k ; œÄ) = max{h(xk ), V (xk+1 , z k+1 ; œÄ)},
z k+1 = z k ‚àí l(xk , œÄ(xk )).

(14)

Centralized training

Data collection
ùëú!" , ùëß "
ùúã! ‚ãÖ, ùëß
ùëú$" , ùëß "

ùúã' ‚ãÖ, ùëß

MSE

Centralized cost-value function
ùëâ & ùë•, ùëß

Distributed constraint-value
functions (ùëâ)( ‚â§ 0 ‚áíSafe)
ùëß)# = argmin ùëß

ùëú$# , ùëß #

# #
ùëú'
,ùëß

ùëâ!( ùëú! , ùëß ùëâ$( ùëú$ , ùëß ‚ãØ ùëâ'( ùëú' , ùëß

Total value function
ùëâ = max max ùëâ)( ùëú) , ùëß , ùëâ & ùë•, ùëß ‚àí ùëß

ùúã) = ùúã) ‚ãÖ, ùëß)#

)

ùë• #%! = ùëì ùë• # , ùúã ùë•, ùëß
ùëß #%! = ùëß # ‚àí ùëô ùë• # , ùë¢#

*

s. t. ùëâ)( ‚ãÖ, ùëß ‚â§ 0
‚áí

" "
ùëú'
,ùëß

MSE

‚ãØ

‚ãØ

‚ãØ

ùúã$ ‚ãÖ, ùëß

ùëú!# , ùëß #

PPO policy loss

ùúã! ‚ãÖ, ùëß!

ùúã$ ‚ãÖ, ùëß$ ‚ãØ ùúã' ‚ãÖ, ùëß'

ùëß!

Distributed execution

Fig. 2: Def-MARL algorithm. Randomly sampled initial states and z 0 are used to collect trajectories in x and z using the
current policy œÄ. In the centralized training (orange blocks), distributed constraint-value functions Vih and policies œÄi and a
centralized cost-value function V l are jointly trained. During distributed execution (green blocks), the distributed Vih are used
to solve the outer problem (15b) to compute the optimal zi , which is used in each agent‚Äôs z-conditioned policy.

The proof of Proposition 1 is provided in Appendix A following the proof of the single-agent version [66]. In other words,
for a given cost upper bound z k , the value function V at the
current state xk can be computed using the value function
at the next state xk+1 but with a different cost upper bound
z k+1 = z k ‚àíl(xk , œÄ(xk )) which itself is a function of z k . This
can be interpreted as a ‚Äúdynamics‚Äù for the cost upper bound z.
Intuitively, if we wish to satisfy the upper bound z k but suffer
a cost l(xk , œÄ(xk )), then the upper bound at the next time step
should be smaller by l(xk , œÄ(xk )) so that the total cost from
xk remains upper bounded by z k . Additional discussion on
Proposition 1 is provided in Appendix C.
Remark 1 (Effect of z on the learned policy): From (12),
for a fixed x and œÄ, observe that for z large enough (i.e.,
V l (x; œÄ)‚àíz is small enough), we have V (x, z; œÄ) = V h (x; œÄ).
Consequently, taking a gradient step on V (x, z; œÄ) equals
taking a gradient step on V h (x; œÄ), which reduces the constraint violation. Otherwise, V (x, z; œÄ) = V l (x; œÄ)‚àíz. Taking
gradient steps on V (x, z; œÄ) equals taking gradient steps on
V l (x; œÄ), which reduces the total cost.

the advantage with the generated advantage estimation (GAE)
[62] for the i-th agent, Ai [63], instead of using the cost function V l [80], we apply the decomposed total value function
max{Vœàh (oi , z), Vœïl (x, z) ‚àí z}. We perform trajectory rollouts
following the dynamics for x (1) and z (14) using the learned
policy œÄŒ∏ , starting from random sampled x0 and z 0 . After
collecting the trajectories, we train the cost-value function Vœïl
and the constraint-value function Vœàh via regression and use
the PPO policy loss to update the z-conditioned policy œÄŒ∏ .
C. Solving the outer problem during distributed execution
During execution, we solve the outer problem of EFMASOCP (13) online. However, the outer problem is still centralized because the constraint (13b) requires the centralized
cost-value function V l . To achieve a distributed policy during
execution, we introduce the following theoretical result:
Theorem 1: Assume no two unique values of z achieves the
same unique cost. Then, the outer problem of EF-MASOCP
(5a) is equivalent to the following:
z = max zi

(15a)

i

B. Solving the inner problem using MARL
Following So and Fan [66], we solve the inner problem
using centralized training with proximal policy optimization
(PPO) [63]. We use a graph neural network (GNN) backbone
for the z-conditioned policy œÄŒ∏ (oi , z), cost-value function
Vœïl (x, z), and the constraint-value function Vœàh (oi , z) with
parameters Œ∏, œï, and œà, respectively. Note that other neural
network (NN) structures can be used as well. The implementation details are introduced in Appendix E.
Policy and value function updates. During centralized
training, the NNs are trained to solve the inner problem
(13b), i.e., for a randomly sampled z, find policy œÄ(¬∑, z) that
minimizes the total value function V (x0 , z; œÄ). We follow
MAPPO [80] to train the NNs. Specifically, when calculating

zi = min
‚Ä≤
z

s.t.

z

‚Ä≤

Vih (oi ; œÄ(¬∑, z ‚Ä≤ )) ‚â§ 0,

i = 1, ¬∑ ¬∑ ¬∑ , N.

(15b)

The proof is provided in Appendix B. Theorem 1 enables
computing z without the use of the centralized V l during
execution. Specifically, each agent i solves the local problem
(15b) for zi , which is a 1-dimensional optimization problem
and can be efficiently solved using root-finding methods (e.g.,
[12]) as in [66], then communicates zi among the other
agents to obtain the maximum (15a). One challenge is that
this maximum may not be computable if the agents are not
connected. However, in our problem setting, if one agent is not
connected, it does not appear in the observations o of other
connected agents. Therefore, it would not contribute to the

Agent

TARGET

Goal/landmark

S PREAD

Obstacle

Target positions

F ORMATION

S AFE H ALF C HEETAH (2 X 3)

L INE

Agent-agent/obstacle

C ORRIDOR

Agent-goal

C ONNECT S PREAD

S AFE C OUPLED H ALF C HEETAH (4 X 3)

Fig. 3: Simulation Environments. Visualization of the (top) modified MPE [40] and (bottom) Safe Multi-agent MuJoCo [32]
environments we consider.

constraint-value function V h of other agents. As a result, it is
sufficient for only the connected agents to communicate their
zi . Furthermore, we observe experimentally that the agents
can achieve low cost while maintaining safety even if zi is
not communicated (see Section V-C). Thus, we do not include
zi communication for our method. The overall framework of
Def-MARL is provided in Fig. 2.
Dealing with estimation errors. Since there may be errors
estimating V h using NN, we can reduce the resulting safety
violation by modifying h to add a buffer region. Specifically,
for a constant ŒΩ > 0, we modify h such that h ‚â• ŒΩ when
the constraints are violated and h ‚â§ ‚àíŒΩ otherwise. We then
modify (15b) to Vœàh (oi , zi ) ‚â§ ‚àíŒæ, where Œæ ‚àà [0, ŒΩ] is a
hyperparameter (where we want Œæ ‚âà ŒΩ to emphasize more
on safety). This makes z more robust to estimation errors of
V h . We study the importance of Œæ in Section V-C.
V. S IMULATION EXPERIMENTS
In this section, we design simulation experiments to answer
the following research questions:
(Q1): Does Def-MARL satisfy the safety constraints and
achieve low cost with constant hyperparameters across
all environments?
(Q2): Can Def-MARL achieve the global optimum of the
original constrained optimization problem?
(Q3): How stable is the training of Def-MARL?
(Q4): How well does Def-MARL scale to larger MAS?
(Q5): Does the learned policy from Def-MARL generalize
to larger MAS?
Details for the implementation, environments, and hyperparameters are provided in Appendix E.

A. Setup
Environments. We evaluate Def-MARL in two sets of
simulation environments: modified Multi-agent Particle Environments (MPE) [40], and Safe Multi-agent MuJoCo environments [32] (see Fig. 3). In MPE, the agents are assumed
to have double integrator dynamics with bounded continuous
action spaces [‚àí1, 1]2 . We provide the full details of all tasks
in Appendix E. To increase the difficulty of the tasks, we add
3 static obstacles to these environments. For Safe Multi-agent
MuJoCo environments, we consider S AFE H ALF C HEETAH
2 X 3 and S AFE C OUPLED H ALF C HEETAH 4 X 3. The agents
must collaborate to make the cheetah run as fast as possible
without colliding with a moving wall in front. To design the
constraint function h, we let ŒΩ = 0.5 in all our experiments
and Œæ = 0.4 when solving the outer problem.
Baselines. We compare our algorithm with the state-ofthe-art (SOTA) MARL algorithm InforMARL [51] with a
constraint-penalized cost l‚Ä≤ (x, u) = l(x, u) + Œ≤ max{h(x), 0},
where Œ≤ ‚àà {0.02, 0.1, 0.5} is a penalty parameter, and denote
this baseline as Penalty(Œ≤). We also consider the SOTA
safe MARL algorithm MAPPO-Lagrangian [30, 32]4 . In addition, because the learning rate of the Lagrangian multiplier
Œª is tiny (10‚àí7 ) in the official implementation of MAPPOLagrangian [32], the value of Œª during training will be largely
determined by the initial value Œª0 of Œª. We thus consider
two Œª0 ‚àà {1, 5}. Moreover, to compare the training stability,
we consider increasing the learning rate of Œª in MAPPOLagrangian to 3 √ó 10‚àí3 .5 For a fair comparison, we reimplement MAPPO-Lagrangian using the same GNN backbone as
4 We omit the comparison with MACPO [30, 32] as it was shown to perform
similarly to MAPPO-Lagrangian but have significantly worse time complexity
and wall clock time for training.
5 This is the smallest learning rate for Œª that does not make MAPPOLagrangian ignore the safety constraint. We set Œª0 = 0.78 following [32].

0.8

0.6
0.2

0.4
Cost

0.6

0.0

Line

1.0
Safety rate

Safety rate

1.0

0.9

0.8

0.5
Cost

1.0

0.0

Corridor

0.5
Cost

0.5
Cost

ConnectSpread
1.0

0.8
0.6

1.0

200

0.9

Cost

400

Safe Coupled HalfCheetah 4x3
1.00
0.75
0.50
0.25

0.5
Def-MARL (ours)
Penalty(0.02)

0.5

1.0

0.4
0.8
0.0

Safe HalfCheetah 2x3
1.0

0.6

Safety rate

0.6

Formation

Safety rate

0.8

1.0
Safety rate

Safety rate

Safety rate

Spread

1.0

Safety rate

Target

1.0

1.0
1.5
Cost
Penalty(0.1)
Penalty(0.5)

0.8

0.5

1.0
Cost
Lagr(1)
Lagr(5)

200

400
Cost

600

Lagr(lr)

Fig. 4: Comparison on modified MPE (N = 3) and Safe Multi-agent MuJoCo. Def-MARL is consistently closest to the
top-left corner in all environments, achieving low cost with near 100% safety rate. The dots show the mean values and the
error bars show one standard deviation.

used in Def-MARL and InforMARL, denoted as Lagr(Œª0 )
and Lagr(lr) for the increased learning rate one. We run
each method for the same number of update steps, which is
large enough for all the methods to converge.
Evaluation criteria. Following the objective of MASOCP,
we use the cost and safety rate as the evaluation criteria for the
performance of all algorithms.
The cost is the cumulative cost
PT
over the trajectory k=0 l(xk , uk ). The safety rate is defined
as the ratio of agents that remain safe over the entire trajectory,
i.e., hi (oki ) ‚â§ 0, ‚àÄk, over all agents. Unlike the CMDP setting,
we do not report the mean of constraint violations over time
but the violation of the hard safety constraints.
B. Results
We train all algorithms with 3 different random seeds and
test the converged policies on 32 different initial conditions.
As discussed in Section IV-C, we disable the communication
of zi between agents (investigated in Section V-C). We draw
the following conclusions.
(Q1): Def-MARL achieves the best performance with
constant hyperparameters across all environments. First,
we plot the safety rate (y-axis) and cumulative cost (x-axis) for
each algorithm in Fig. 4. Thus, the closer an algorithm is to the
top-left corner, the better it performs. In both MPE and Safe
Multi-agent MuJoCo environments, Def-MARL is always
closest to the top-left corner, maintaining a low cost while
having near 100% safety rate. For the baselines Penalty
and Lagr, their performance and safety are highly sensitive
to their hyperparameters. While Penalty with Œ≤ = 0.02

and Lagr with Œª0 = 1 generally have low costs, they
also have frequent constraint violations. With Œ≤ = 0.5 or
Œª0 = 5, they prioritize safety but at the cost of high cumulative
costs. Def-MARL, however, maintains a safety rate similar
to the most conservative baselines (Penalty(0.5) and
Lagr(5)) but has much lower costs. We point out that no
single baseline method behaves considerably better on all the
environments: the performance of the baseline methods varies
wildly between environments, demonstrating the sensitivity of
these algorithms to the choice of hyperparameters. On the
contrary, Def-MARL, performs best in all environments, using
a single set of constant hyperparameters, which demonstrates
its insensitivity to the choice of hyperparameters.
(Q2): Def-MARL is able to reach the global optimum of
the original problem. An important observation is that for
Penalty and Lagr with a non-optimal Œª, the cost function
optimized in their training process is different from the original
cost function. Consequently, they can have different optimal
solutions compared to the original problem. Therefore, even
if their training converges, they may not reach the optimal
solution to the original problem. In Fig. 5, the converged states
of Def-MARL and four baselines are shown. Def-MARL
reaches the original problem‚Äôs global optimum and covers all
three goals. On the contrary, the optima of Penalty(0.02)
and Lagr(1) are changed by the penalty term, so they
choose to leave one agent behind to have a lower safety
penalty. With an even more significant penalty, the optima
of Penalty(0.5) and Lagr(5) are changed dramatically,
and they forget the goal entirely and only focus on safety.

Agent

Goal/landmark

Def-MARL (ours)

Obstacle

Target positions

Penalty(0.02)

Agent-agent/obstacle

Penalty(0.5)

Lagr(1)

Agent-goal

Lagr(5)

Fig. 5: Converged states in C ORRIDOR. Def-MARL achieves the global minimum, while other baselines converge to a
different optimum (partly) due to training using a different cost function.
1.0

0.5

Spread

1.00

1.0
0.5

Safety rate

Safety rate

Cost

1.0

Target

Cost

Target
1.5

0.5

Spread

0.75
0.50
0.25

0.00.0

0.5
Step

1.0
√ó105

0.0
0.0

0.5
Step

1.0
√ó105

Def-MARL (ours)

0.00.0

0.5
Step
Lagr(lr)

1.0
√ó105

0.0

0.5
Step

1.0
√ó105

Fig. 6: Training Curves in TARGET and S PREAD. Def-MARL has a smoother, more stable training curve compared to
Lagr(lr). We plot the mean and shade the ¬±1 standard deviation.
TABLE I: Policy Generalization. Testing Def-MARL on
TARGET with more agents after training with N = 8 agents.
# Agent

32

128

512

Safety rate
Cost

99.8 ¬± 0.2
‚àí0.387 ¬± 0.029

99.6 ¬± 0.4
‚àí0.408 ¬± 0.015

99.5 ¬± 0.3
‚àí0.410 ¬± 0.009

(Q3): Training of Def-MARL is more stable. To compare the training stability of Def-MARL and the Lagrangian
method Lagr(lr), we plot their cost and safety rate during
training in Fig. 6. Def-MARL has a smoother curve compared
to Lagr(lr), supporting our theoretical analysis in Section III-B. Due to space limits, the plots for other environments
and other baseline methods are provided in Appendix E-D.
(Q4): Def-MARL can scale to more agents while maintaining high performance and safety, but is limited by
GPU memory due to centralized training. We test the limits
of Def-MARL on the number of agents during training by
comparing all methods on N = 5, 7 with F ORMATION and
L INE, and N = 8, 12, 16 with TARGET (Fig. 7). We were
unable to increase N further due to GPU memory limitations
due to the use of centralized training. For this experiment, we
omit Lagr(lr) as it has the worst performance in MPE with
N = 3. Def-MARL is closest to the upper left corner in all
environments, and its performance does not decrease with an

increasing number of agents.
(Q5): The trained policy from Def-MARL generalizes to
much larger MAS. To test the generalization capabilities of
Def-MARL, we test a policy trained with N = 8 on much
larger MASs with up to N = 512 on TARGET (Table I) with
the same agent density to avoid distribution shift. Def-MARL
maintains a high safety rate and low costs despite being
applied on a MAS with 64 times more agents.
C. Ablation studies
Here we do ablation studies on the communication of zi ,
and study the hyperparameter sensitivity of Def-MARL.
Is communicating zi necessary? As introduced in Section IV-C, theoretically, all connected agents should communicate and reach a consensus on z = maxi zi . However, we
observe in Section V-B that the agents can perform well even
if agents take z ‚Üê zi without communicating to compute
the maximum. We perform experiments on MPE (N = 3)
to understand the impact of this approximation in Table II
and see that using the approximation does not result in much
performance difference compared to communicating zi and
using the maximum.
Varying Œæ in the outer problem. To robustify our
approach against estimation errors in V h , we solve for a
zi that is slightly more conservative by modifying (15b) to
Vœàh (oi , zi ) ‚â§ ‚àíŒæ (Section IV-C). We now perform experiments

Formation (N = 7)

0.9

0.9

0.8

0.5
Cost
Target (N = 8)

Safety rate

Safety rate
0.5
Cost

1.0

0.8
0.7

0.4

0.80.5

0.75
0.50

0.9

0.25
0.8 1

1.0
1.5
Cost

0.8

1.00

1.0

0.9

0.9

0.7

0.25 0.50 0.75
Cost
Target (N = 16)

1.0

0.8

1.0

0.9

Target (N = 12)

1.0

0.6

0.2
Cost

Safety rate

0.8
0.0

Line (N = 7)

1.0
Safety rate

1.0
Safety rate

Safety rate

1.0

Line (N = 5)
Safety rate

Formation (N = 5)

2
Cost

0.25
0.50
Cost

Def-MARL (ours)
Penalty(0.02)
Penalty(0.1)
Penalty(0.5)
Lagr(1)
Lagr(5)

0.00.0

3

0.5

1.0

Fig. 7: Comparison on larger-scale modified MPE. Def-MARL remains in the top-left corner even when the number of
agents increases. The dots show the mean and the error bars show one standard deviation.
TABLE II: Effect of zi communication (Section IV-C) in different environments.
Environment
TARGET
S PREAD
F ORMATION
L INE
C ORRIDOR
C ONNECT S PREAD

No communication (z ‚Üê zi )
Safety rate
Cost

Communication (z = maxi zi )
Safety rate
Cost

97.9 ¬± 1.5
99.0 ¬± 0.9
98.3 ¬± 1.0
98.6 ¬± 0.5
97.9 ¬± 1.8
97.9 ¬± 1.7

96.9 ¬± 3.0
98.6 ¬± 1.3
98.3 ¬± 1.8
98.3 ¬± 0.5
98.6 ¬± 1.9
99.0 ¬± 0.8

0.196 ¬± 0.108
0.162 ¬± 0.144
0.123 ¬± 0.940
0.117 ¬± 0.540
0.247 ¬± 0.390
0.324 ¬± 0.187

to study the effect of different choices of Œæ (Table III) on
L INE (N = 3). The results show that higher values of Œæ
result in higher safety rates and slightly higher costs, while the
reverse is true for smaller Œæ. This matches our intuition that
modifying (15b) can help improve constraint satisfaction when
the learned V h has estimation errors. We thus recommend
choosing Œæ close to ŒΩ. We also provide the sensitivity analysis
on more hyperparameters in Appendix E.
VI. H ARDWARE EXPERIMENTS
Finally, we conduct hardware experiments on a swarm of
Crazyflie (CF) drones [27] to demonstrate Def-MARL‚Äôs ability
to safely coordinate agents to complete complex collaborative
tasks in the real world. We consider the following two tasks.
C ORRIDOR . A swarm of drones collaboratively gets
through a narrow corridor and reaches a set of goals
without explicitly assigning drones to goals.
‚Ä¢ I NSPECT. Two drones collaborate to maintain direct visual contact with a target drone that follows a path shaped
like an eight while staying out of the target drone‚Äôs avoid
‚Ä¢

0.214 ¬± 0.141
0.171 ¬± 0.128
0.126 ¬± 0.100
0.121 ¬± 0.630
0.255 ¬± 0.470
0.339 ¬± 0.201

TABLE III: Effect of varying Œæ
(Section IV-C) for L INE (N =
3) with fixed ŒΩ = 0.5.
Œæ

Safety rate

Cost

0.5
0.4
0.2
0.0

100.0 ¬± 0.0
98.6 ¬± 0.5
96.5 ¬± 0.5
93.4 ¬± 0.020

0.127 ¬± 0.061
0.117 ¬± 0.540
0.108 ¬± 0.044
0.102 ¬± 0.035

zone. Visual contact only occurs when the line of sight
to the target drone is not blocked by obstacles.
For both tasks, all drones have collision constraints with other
drones and static obstacles. We visualize the tasks in Fig. 8.
Baselines. Def-MARL has demonstrated superior performance among RL methods in simulation experiments. Consequently, we do not consider RL methods as baselines for
the hardware experiments. Instead, we present a comparative
analysis between Def-MARL and model predictive control
(MPC), a widely employed technique in practical robotic applications. Notably, MPC necessitates model dynamics knowledge, whereas Def-MARL does not. We compare Def-MARL
against the following two MPC baselines.
Decentralized. We consider a decentralized MPC method
(DMPC), where each drone tries to individually minimize
the total cost and prevents collision with other controlled
drones by assuming a constant velocity motion model
using the current measurement of their velocity.
‚Ä¢ Centralized. We also test against a centralized MPC
(CMPC) method to better disentangle phenomena related
‚Ä¢

Obstacle
CF1

Avoid region

Obstacle

CF2

Observation range
of CF2
Target

CF3

CF1

CF2

Observation range
of CF1 (blocked by
the obstacle)

(a) C ORRIDOR

(b) I NSPECT

Fig. 8: Hardware tasks. We perform hardware experiments using a swarm of CF drones on the C ORRIDOR and I NSPECT
tasks. In C ORRIDOR, the team must cross a narrow corridor and cover a set of goals collectively without prior assignment. In
I NSPECT, the team must maintain visual contact with the target drone while staying out of the avoid zone around the target.

Def-MARL

CMPC

CMPCwA

DMPC

Fig. 9: Hardware Results on C ORRIDOR (N = 3). Left to right: key frames of the trajectories generated by different
algorithms. Arrows with different colors indicate the positions of different drones. Def-MARL (top) finishes the task with
100% success rate because the drones learn to cross the corridor one by one. CMPC and CMPCwA (middle) sometimes get
stuck in local minima and cannot finish the task because of the highly nonconvex cost function. DMPC (bottom) has unsafe
cases where the agents cross the unsafe radius so they cannot reach the goals because the MPC problem becomes infeasible.

to numerical nonlinear optimization and performing decentralized control. This method uses the same cost
function used by Def-MARL.
Both MPC methods are implemented in CasADi [4] with the
SNOPT nonlinear optimizer [28]. Details for the hardware
setup and experiment videos are provided in Appendix F.
A. C ORRIDOR
We run each algorithm from 16 random initial conditions
and use the task success rate to measure their performance.
The task is defined to be successful if all the goals are covered
by the agents and all agents stay safe during the whole task.

Accordingly, the success rate is defined as the number of
successful tasks divided by 16. In our tests, the success rates of
Def-MARL, CMPC, and DMPC are 100%, 0%, and 62.5%. To
analyze this result, we visualize the trajectory of Def-MARL
and some failure cases of the baselines in Fig. 9.
CMPC is prone to local minima. We first compare
Def-MARL with CMPC. Since we sum the distance from each
goal to the closest drone, the cost function in this task is very
nonconvex. Consequently, CMPC results in very suboptimal
solutions, where only the closest drone to the goals attempts
to reach the goals, with the remaining drones left on the other
side. To alleviate this issue, although the original task does

Fig. 10: Hardware Results of Def-MARL on C ORRIDOR (N = 7). Even in this crowded environment, Def-MARL maintains
a success rate of 100%.

Def-MARL

CMPC

DMPC

Fig. 11: Hardware Results on I NSPECT. The CF drone overlayed with the yellow/green sphere is the target. The sphere turns
green when the target is observed and yellow otherwise. The CF drones overlayed with blue spheres are agents, which turn
red if the agents become unsafe. The red spheres around the target show the avoid zone that agents cannot enter. Def-MARL
finishes the task with safe and collaborative behaviors. For example, they learn to wait on two sides of the obstacles and take
turns to observe the target. CMPC gets stuck in local minima and only moves the closest drone to the target, leaving the other
drone stationary. DMPC makes both agents chase after the target without collaboration, and even has unsafe cases.

not have an explicit goal allocation, we provide a handicap to
the CMPC methods and rerun the experiments with explicitly
assigned goals for each drone. This simplifies the optimization
problem by removing the discrete nature of goal assignment.
We name this baseline CMPCwA (CMPC with assignment).
However, even with explicit goal assignments, we still see that
sometimes one of the drones in the team gets stuck behind
the corridor, resulting in a success rate of 87.5%. In contrast,
Def-MARL does not succumb to this local minimum and
completes the task with a success rate of 100%.
DMPC has unsafe cases. As DMPC without goal assignment
will also suffer from similar issues as CMPC, we choose
to assign goals for each agent in this baseline. This results
in a simpler problem, as explained above. However, unlike
CMPCwA, the agents using DMPC do not know the actual
actions of the other agents and can only make predictions
based on their observations because of the decentralized nature
of DMPC. Therefore, collisions may occur if the other agents
behave significantly differently from the predictions. In the
DMPC row of Fig. 9, the agents collide6 in the middle of
the tasks, causing the MPC optimization problem to become

6 For our safety, the safety radius of the drones is larger than their actual

radius. Here, we mean they have entered each others‚Äô safety radius, although
they have not collided in reality.

infeasible and preventing the agents from reaching their goals.7
Def-MARL can scale up to 7 agents. We also test the
scalability of Def-MARL with 7 CF drones in the same
environment. Notably, the size of the environment remains unchanged, so the environment becomes much more crowded and
thus more challenging. We test Def-MARL with 9 different
random initial conditions, and it maintains a success rate of
100%. We visualize one of the trajectories in Fig. 10. Note that
we were limited to only 7 drones here due to only having 7
drones available. However, given the simulation results, we are
hopeful that Def-MARL can scale to larger hardware swarms.
B. I NSPECT
We also run each algorithm from 16 different random initial
conditions. Note that the agents may not be able to observe
the target at their initial positions at the first step. In this
environment, the team of two drones should maximize the
duration where the goal is observed by at least one drone.
Measuring the task performance by the number of timesteps
where the target is not visible, we obtain that the performance
of Def-MARL, CMPC, and DMPC are 85.5 ¬± 42.9, 206 ¬± 53.2,
and 251 ¬± 59.1, respectively. We also report the safety rate,
7 Some MPC-based methods can solve the C ORRIDOR environment [43, 68]
but assume pre-assigned goals. Additionally, these approaches need additional
methods for collision avoidance (e.g., Buffered Voronoi Cells [91], on-demand
collision avoidance approaches [42]), which require more domain knowledge.

defined as the ratio of tasks where all agents stay safe, which
are 100%, 100%, and 43.75%, respectively. We visualize the
trajectories of different methods in Fig. 11.
Agents using Def-MARL have better collaboration. The
I NSPECT is designed such that collaboration between the
agents is necessary to observe the target without any downtime. One agent cannot observe the target all the time on
its own because the agent‚Äôs observation can be blocked by
obstacles. It also cannot simply follow the target because of the
avoid region. Using Def-MARL, the agents learn collaborative
behaviors such as waiting on each side of the obstacles and
taking turns to observe the target when the target is on their
side. The MPC methods, however, do not have such global
optimal behavior but get stuck in local minima. For example,
CMPC only moves the closest drone to the target, leaving
the other drone stationary, while DMPC makes both agents
chase after the target without collaboration. Therefore, both
MPC methods have small periods of time where neither drone
has visual contact with the target. In addition, similar to
in C ORRIDOR, we observe that DMPC sometimes results in
collisions due to the lack of coordination between drones.
VII. C ONCLUSION
To construct safe distributed policies for real-world multiagent systems, this paper introduces Def-MARL for the multiagent safe optimal control problem, which defines safety as
zero constraint violation. Def-MARL takes advantage of the
epigraph form of the original problem to address the training instability of Lagrangian methods in the zero-constraint
violation setting. We provide a theoretical result showing that
the centralized epigraph form can be solved in a distributed
fashion by each agent, which enables distributed execution of
Def-MARL. Simulation results on MPE and the Safe Multiagent MuJoCo environments suggest that, unlike baseline
methods, Def-MARL uses a constant set of hyperparameters
across all environments, and achieves a safety rate similar
to the most conservative baseline and similar performance
to the baselines that prioritize performance but violate safety
constraints. Hardware results on the Crazyflie drones demonstrate Def-MARL‚Äôs ability to solve complex collaborative
tasks safely in the real world.
VIII. L IMITATIONS
The theoretical analysis in Section IV-C suggests that the
connected agents must communicate z and reach a consensus. If the communication on z is disabled, although our
experiments show that the agents still perform similarly, the
theoretical optimality guarantee may not be valid. In addition,
the framework does not consider noise, disturbances in the
dynamics, or communication delays between agents. Finally,
as a safe RL method, although safety can be theoretically
guaranteed under the optimal value function and policy, this
does not hold under inexact minimization of the losses. We
leave tackling these issues as future work.

ACKNOWLEDGMENTS
This work was partly supported by the Under Secretary
of Defense for Research and Engineering under Air Force
Contract No. FA8702-15-D-0001. In addition, Zhang, So, and
Fan are supported by the MIT-DSTA program. Any opinions,
findings, conclusions, or recommendations expressed in this
publication are those of the authors and don‚Äôt necessarily
reflect the views of the sponsors.
¬© 2025 Massachusetts Institute of Technology.
Delivered to the U.S. Government with Unlimited Rights,
as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014).
Notwithstanding any copyright notice, U.S. Government rights
in this work are defined by DFARS 252.227-7013 or DFARS
252.227-7014 as detailed above. Use of this work other than
as specifically authorized by the U.S. Government may violate
any copyrights that exist in this work.
R EFERENCES
[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter
Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pages 22‚Äì31.
PMLR, 2017.
[2] Akshat Agarwal, Sumit Kumar, Katia Sycara, and
Michael Lewis. Learning transferable cooperative behavior in multi-agent team. In International Conference
on Autonomous Agents and Multiagent Systems (AAMAS‚Äô2020). IFMAS, 2020.
[3] Eitan Altman. Constrained Markov decision processes.
Routledge, 2004.
[4] Joel AE Andersson, Joris Gillis, Greg Horn, James B
Rawlings, and Moritz Diehl.
Casadi: a software
framework for nonlinear optimization and optimal control. Mathematical Programming Computation, 11:1‚Äì36,
2019.
[5] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J
Tomlin. Hamilton-jacobi reachability: A brief overview
and recent advances. In 2017 IEEE 56th Annual Conference on Decision and Control (CDC), pages 2242‚Äì2253.
IEEE, 2017.
[6] Dimitri Bertsekas. Dynamic programming and optimal
control: Volume I, volume 4. Athena scientific, 2012.
[7] Suda Bharadwaj, Roderik Bloem, Rayna Dimitrova,
Bettina Konighofer, and Ufuk Topcu. Synthesis of
minimum-cost shields for multi-agent systems. In ACC.
IEEE, 2019.
[8] Vivek S Borkar. An actor-critic algorithm for constrained
markov decision processes. Systems & Control Letters,
54(3):207‚Äì213, 2005.
[9] Vivek S Borkar. Stochastic Approximation: A Dynamical
Systems Viewpoint, volume 48. Springer, 2009.
[10] Stephen P Boyd and Lieven Vandenberghe. Convex
optimization. Cambridge university press, 2004.
[11] Zhiyuan Cai, Huanhui Cao, Wenjie Lu, Lin Zhang, and
Hao Xiong. Safe multi-agent reinforcement learning
through decentralized multiple control barrier functions.
arXiv preprint arXiv:2103.12553, 2021.

[12] Tirupathi R Chandrupatla.
A new hybrid
quadratic/bisection algorithm for finding the zero
of a nonlinear function without using derivatives.
Advances in Engineering Software, 28(3):145‚Äì149,
1997.
[13] Yu Fan Chen, Michael Everett, Miao Liu, and Jonathan P
How. Socially aware motion planning with deep reinforcement learning. In 2017 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 1343‚Äì1350. IEEE, 2017.
[14] Yu Fan Chen, Miao Liu, Michael Everett, and Jonathan P
How. Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning. In
2017 IEEE International Conference on Robotics and
Automation (ICRA), pages 285‚Äì292. IEEE, 2017.
[15] Ziyi Chen, Yi Zhou, and Heng Huang. On the duality
gap of constrained cooperative multi-agent reinforcement
learning. In The Twelfth International Conference on
Learning Representations, 2024.
[16] Christian Conte, Tyler Summers, Melanie N Zeilinger,
Manfred Morari, and Colin N Jones. Computational
aspects of distributed optimization in model predictive
control. In 2012 IEEE 51st IEEE conference on decision
and control (CDC), pages 6819‚Äì6824. IEEE, 2012.
[17] Philip Dames, Pratap Tokekar, and Vijay Kumar. Detecting, localizing, and tracking an unknown number of
moving targets using a team of mobile robots. The
International Journal of Robotics Research, 36(13-14):
1540‚Äì1553, 2017.
[18] Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran
Wang, and Mihailo Jovanovic. Provably efficient generalized lagrangian policy optimization for safe multi-agent
reinforcement learning. In Learning for Dynamics and
Control Conference, pages 315‚Äì332. PMLR, 2023.
[19] Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Amato, RuÃàdiger Ehlers, Ufuk Topcu, and Lu Feng. Safe
multi-agent reinforcement learning via shielding. arXiv
preprint arXiv:2101.11196, 2021.
[20] Ingy ElSayed-Aly, Suda Bharadwaj, Christopher Amato,
RuÃàdiger Ehlers, Ufuk Topcu, and Lu Feng. Safe multiagent reinforcement learning via shielding. AAMAS ‚Äô21,
2021.
[21] Michael Everett, Yu Fan Chen, and Jonathan P How. Motion planning among dynamic, decision-making agents
with deep reinforcement learning. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 3052‚Äì3059. IEEE, 2018.
[22] Giuseppe Fedele and Giuseppe FranzeÃÄ. A distributed
model predictive control strategy for constrained multiagent systems: The uncertain target capturing scenario.
IEEE Transactions on Automation Science and Engineering, 2023.
[23] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras,
Nantas Nardelli, and Shimon Whiteson. Counterfactual
multi-agent policy gradients. In Proceedings of the AAAI
conference on artificial intelligence, volume 32, 2018.

[24] Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert,
and Sicun Gao. Iterative reachability estimation for safe
reinforcement learning. Advances in Neural Information
Processing Systems, 36, 2024.
[25] Kunal Garg, Songyuan Zhang, Oswin So, Charles Dawson, and Chuchu Fan. Learning safe control for multirobot systems: Methods, verification, and open challenges. Annual Reviews in Control, 57:100948, 2024.
[26] Nan Geng, Qinbo Bai, Chenyi Liu, Tian Lan, Vaneet
Aggarwal, Yuan Yang, and Mingwei Xu. A reinforcement learning framework for vehicular network routing
under peak and average constraints. IEEE Transactions
on Vehicular Technology, 2023.
[27] Wojciech Giernacki, Mateusz SkwierczynÃÅski, Wojciech
Witwicki, Pawe≈Ç WronÃÅski, and Piotr Kozierski. Crazyflie
2.0 quadrotor as a platform for research and education
in robotics and control engineering. In 2017 22nd
International Conference on Methods and Models in
Automation and Robotics (MMAR), pages 37‚Äì42. IEEE,
2017.
[28] Philip E Gill, Walter Murray, and Michael A Saunders.
Snopt: An sqp algorithm for large-scale constrained
optimization. SIAM review, 47(1):99‚Äì131, 2005.
[29] Lars Grne and Jrgen Pannek. Nonlinear model predictive
control: theory and algorithms. Springer Publishing
Company, Incorporated, 2013.
[30] Shangding Gu, Jakub Grudzien Kuba, Munning Wen,
Ruiqing Chen, Ziyan Wang, Zheng Tian, Jun Wang,
Alois Knoll, and Yaodong Yang. Multi-agent constrained
policy optimisation. arXiv preprint arXiv:2110.02793,
2021.
[31] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian
Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A
review of safe reinforcement learning: Methods, theory
and applications. arXiv preprint arXiv:2205.10330, 2022.
[32] Shangding Gu, Jakub Grudzien Kuba, Yuanpei Chen,
Yali Du, Long Yang, Alois Knoll, and Yaodong Yang.
Safe multi-agent reinforcement learning for multi-robot
control. Artificial Intelligence, 319:103905, 2023.
[33] Carlos Guestrin, Michail Lagoudakis, and Ronald Parr.
Coordinated reinforcement learning. In ICML, volume 2,
pages 227‚Äì234. Citeseer, 2002.
[34] Matthew Hausknecht and Peter Stone. Deep recurrent
q-learning for partially observable mdps. In 2015 aaai
fall symposium series, 2015.
[35] Tairan He, Weiye Zhao, and Changliu Liu. Autocost:
Evolving intrinsic cost for zero-violation reinforcement
learning. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 37, pages 14847‚Äì14855,
2023.
[36] Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang,
and Yaodong Yang. Safedreamer: Safe reinforcement
learning with world models. In The Twelfth International
Conference on Learning Representations, 2024.
[37] Ajay Kattepur, Hemant Kumar Rath, Anantha Simha, and
Arijit Mukherjee. Distributed optimization in multi-agent

robotics for industry 4.0 warehouses. In Proceedings of
the 33rd Annual ACM Symposium on Applied Computing,
pages 808‚Äì815, 2018.
[38] Chenyi Liu, Nan Geng, Vaneet Aggarwal, Tian Lan,
Yuan Yang, and Mingwei Xu. Cmix: Deep multiagent reinforcement learning with peak and average constraints. In Machine Learning and Knowledge Discovery
in Databases. Research Track: European Conference,
ECML PKDD 2021, Bilbao, Spain, September 13‚Äì17,
2021, Proceedings, Part I 21, pages 157‚Äì173. Springer,
2021.
[39] Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao
Zhang, and Jia Pan. Towards optimally decentralized
multi-robot collision avoidance via deep reinforcement
learning. In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pages 6252‚Äì6259.
IEEE, 2018.
[40] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI
Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments.
Advances in neural information processing systems, 30,
2017.
[41] Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer BasÃßar,
and Lior Horesh. Decentralized policy gradient descent
ascent for safe multi-agent reinforcement learning. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8767‚Äì8775, 2021.
[42] Carlos E Luis and Angela P Schoellig. Trajectory
generation for multiagent point-to-point transitions via
distributed model predictive control. IEEE Robotics and
Automation Letters, 4(2):375‚Äì382, 2019.
[43] Carlos E Luis, Marijan Vukosavljev, and Angela P
Schoellig. Online trajectory generation with distributed
model predictive control for multi-robot motion planning.
IEEE Robotics and Automation Letters, 5(2):604‚Äì611,
2020.
[44] John Lygeros. On reachability and minimum cost optimal
control. Automatica, 40(6):917‚Äì927, 2004.
[45] Hang Ma, Jiaoyang Li, TK Kumar, and Sven Koenig.
Lifelong multi-agent path finding for online pickup and
delivery tasks. arXiv preprint arXiv:1705.10868, 2017.
[46] Kostas Margellos and John Lygeros. Hamilton‚Äìjacobi
formulation for reach‚Äìavoid differential games. IEEE
Transactions on automatic control, 56(8):1849‚Äì1861,
2011.
[47] Pierre-FrancÃßois Massiani, Steve Heim, Friedrich
Solowjow, and Sebastian Trimpe. Safe value functions.
IEEE Transactions on Automatic Control, 68(5):
2743‚Äì2757, 2023.
[48] Daniel Melcer, Christopher Amato, and Stavros Tripakis.
Shield decentralization for safe multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems, 2022.
[49] Ian M Mitchell, Alexandre M Bayen, and Claire J
Tomlin. A time-dependent hamilton-jacobi formulation
of reachable sets for continuous dynamic games. IEEE

Transactions on automatic control, 50(7):947‚Äì957, 2005.
[50] Simon Muntwiler, Kim P Wabersich, Andrea Carron,
and Melanie N Zeilinger. Distributed model predictive
safety certification for learning-based control. IFACPapersOnLine, 53(2):5258‚Äì5265, 2020.
[51] Siddharth Nayak, Kenneth Choi, Wenqi Ding, Sydney
Dolan, Karthik Gopalakrishnan, and Hamsa Balakrishnan. Scalable multi-agent reinforcement learning through
intelligent information aggregation. In International
Conference on Machine Learning, pages 25817‚Äì25833.
PMLR, 2023.
[52] Angelia NedicÃÅ and Ji Liu. Distributed optimization
for control. Annual Review of Control, Robotics, and
Autonomous Systems, 1:77‚Äì103, 2018.
[53] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.
[54] Bei Peng, Tabish Rashid, Christian Schroeder de
Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin
BoÃàhmer, and Shimon Whiteson. Facmac: Factored multiagent centralised policy gradients. Advances in Neural
Information Processing Systems, 34:12208‚Äì12221, 2021.
[55] Marcus A Pereira, Augustinos D Saravanos, Oswin So,
and Evangelos A Theodorou. Decentralized safe multiagent stochastic optimal control using deep fbsdes and
admm. arXiv preprint arXiv:2202.10658, 2022.
[56] Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen,
and Chuchu Fan. Learning safe multi-agent control with
decentralized neural barrier certificates. In International
Conference on Learning Representations, 2021.
[57] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon
Whiteson. Weighted qmix: Expanding monotonic value
function factorisation for deep multi-agent reinforcement
learning. Advances in neural information processing
systems, 33:10199‚Äì10210, 2020.
[58] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder
De Witt, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Monotonic value function factorisation for
deep multi-agent reinforcement learning. Journal of
Machine Learning Research, 21(178):1‚Äì51, 2020.
[59] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
pages 400‚Äì407, 1951.
[60] Harsh Satija, Philip Amortila, and Joelle Pineau. Constrained markov decision processes via backward value
functions. In International Conference on Machine
Learning, pages 8502‚Äì8511. PMLR, 2020.
[61] John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning,
pages 1889‚Äì1897. PMLR, 2015.
[62] John Schulman, Philipp Moritz, Sergey Levine, Michael
Jordan, and Pieter Abbeel. High-dimensional continuous
control using generalized advantage estimation. arXiv
preprint arXiv:1506.02438, 2015.
[63] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347, 2017.
[64] Samaneh Hosseini Semnani, Hugh Liu, Michael Everett,
Anton De Ruiter, and Jonathan P How. Multi-agent
motion planning for dense and dynamic environments
via deep reinforcement learning. IEEE Robotics and
Automation Letters, 5(2):3221‚Äì3226, 2020.
[65] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui
Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised
classification. arXiv preprint arXiv:2009.03509, 2020.
[66] Oswin So and Chuchu Fan. Solving stabilize-avoid
optimal control via epigraph form and deep reinforcement learning. In Proceedings of Robotics: Science and
Systems, 2023.
[67] Oswin So, Cheng Ge, and Chuchu Fan.
Solving
minimum-cost reach avoid using reinforcement learning.
In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.
[68] Enrica Soria, Fabrizio Schiano, and Dario Floreano.
Predictive control of aerial swarms in cluttered environments. Nature Machine Intelligence, 3(6):545‚Äì554, 2021.
[69] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo,
Karl Tuyls, et al.
Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296, 2017.
[70] Chen Tessler, Daniel J. Mankowitz, and Shie Mannor.
Reward constrained policy optimization. In International
Conference on Learning Representations, 2019.
[71] Claire J Tomlin, John Lygeros, and S Shankar Sastry. A
game theoretic approach to controller design for hybrid
systems. Proceedings of the IEEE, 88(7):949‚Äì970, 2000.
[72] Charbel Toumieh and Alain Lambert. Decentralized
multi-agent planning using model predictive control and
time-aware safe corridors. IEEE Robotics and Automation Letters, 7(4):11110‚Äì11117, 2022.
[73] Panagiotis Tsiotras, Efstathios Bakolas, and Yiming
Zhao. Initial guess generation for aircraft landing trajectory optimization. In AIAA Guidance, Navigation, and
Control Conference, page 6689, 2011.
[74] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and
Chongjie Zhang. Qplex: Duplex dueling multi-agent qlearning. arXiv preprint arXiv:2008.01062, 2020.
[75] Peng Wang and Baocang Ding. A synthesis approach
of distributed model predictive control for homogeneous
multi-agent system with collision avoidance. International Journal of Control, 87(1):52‚Äì63, 2014.
[76] Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang,
Huawei Huang, and Dapeng Oliver Wu. Multi-agent
deep reinforcement learning for urban traffic light control
in vehicular networks. IEEE Transactions on Vehicular
Technology, 69(8):8243‚Äì8256, 2020.
[77] Wenli Xiao, Yiwei Lyu, and John Dolan. Model-based
dynamic shielding for safe and efficient multi-agent reinforcement learning. arXiv preprint arXiv:2304.06281,

2023.
[78] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo:
A new approach for safe reinforcement learning with
convergence guarantee. In International Conference on
Machine Learning, pages 11480‚Äì11491. PMLR, 2021.
[79] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao,
Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939,
2020.
[80] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao,
Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative multi-agent games.
Advances in Neural Information Processing Systems, 35:
24611‚Äì24624, 2022.
[81] Dongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen.
Reachability constrained reinforcement learning. In
International conference on machine learning, pages
25636‚Äì25655. PMLR, 2022.
[82] Mario Zanon and SeÃÅbastien Gros. Safe reinforcement
learning using robust mpc. IEEE Transactions on Automatic Control, 66(8):3638‚Äì3652, 2020.
[83] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang,
and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In International conference on machine learning, pages 5872‚Äì
5881. PMLR, 2018.
[84] Kaiqing Zhang, Zhuoran Yang, and Tamer BasÃßar. Multiagent reinforcement learning: A selective overview of
theories and algorithms. Handbook of reinforcement
learning and control, pages 321‚Äì384, 2021.
[85] Songyuan Zhang, Kunal Garg, and Chuchu Fan. Neural graph control barrier functions guided distributed
collision-avoidance multi-agent control. In Conference
on Robot Learning, pages 2373‚Äì2392. PMLR, 2023.
[86] Songyuan Zhang, Oswin So, Mitchell Black, and Chuchu
Fan. Discrete GCBF proximal policy optimization for
multi-agent safe optimal control. In The Thirteenth
International Conference on Learning Representations,
2025.
[87] Songyuan Zhang, Oswin So, Kunal Garg, and Chuchu
Fan. GCBF+: A neural graph control barrier function
framework for distributed safe multiagent control. IEEE
Transactions on Robotics, 41:1533‚Äì1552, 2025.
[88] Wenbo Zhang, Osbert Bastani, and Vijay Kumar. Mamps:
Safe multi-agent reinforcement learning via model predictive shielding. arXiv preprint arXiv:1910.12639,
2019.
[89] Weiye Zhao, Tairan He, and Changliu Liu. Model-free
safe control for zero-violation reinforcement learning. In
5th Annual Conference on Robot Learning, 2021.
[90] Youpeng Zhao, Yaodong Yang, Zhenbo Lu, Wengang
Zhou, and Houqiang Li. Multi-agent first order constrained optimization in policy space. Advances in Neural
Information Processing Systems, 36, 2024.
[91] Dingjiang Zhou, Zijian Wang, Saptarshi Bandyopadhyay,

and Mac Schwager. Fast, on-line collision avoidance for
dynamic vehicles using buffered voronoi cells. IEEE
Robotics and Automation Letters, 2(2):1047‚Äì1054, 2017.
[92] Edward L Zhu, Yvonne R StuÃàrz, Ugo Rosolia, and
Francesco Borrelli. Trajectory optimization for nonlinear
multi-agent systems using decentralized learning model
predictive control. In 2020 59th IEEE Conference on
Decision and Control (CDC), pages 6198‚Äì6203. IEEE,
2020.

A PPENDIX A
P ROOF OF P ROPOSITION 1
Proof: Under the dynamics xk+1 = f (xk , œÄ(xk )), we have
Ô£±
Ô£º
Ô£≤
Ô£Ω
X
V (xk , z k ; œÄ) = max max h(xp ),
l(xp , œÄ(xp )) ‚àí z k
Ô£≥ p‚â•k
Ô£æ
p‚â•k
Ô£±
Ô£º
Ô£≤
Ô£Ω
X
= max max{h(xk ), max h(xp )},
l(xp , œÄ(xp )) + l(xk , œÄ(xk )) ‚àí z k
Ô£≥
Ô£æ
p‚â•k+1
p‚â•k+1
Ô£º
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£≤
h
iÔ£¥
X
= max max{h(xk ), max h(xp )},
l(xp , œÄ(xp )) ‚àí z k ‚àí l(xk , œÄ(xk ))
Ô£¥
p‚â•k+1
Ô£¥
Ô£¥
{z
}Ô£¥
|
p‚â•k+1
Ô£æ
Ô£≥
k+1
:=z
Ô£±
Ô£±
Ô£ºÔ£º
Ô£≤
Ô£≤
Ô£ΩÔ£Ω
X
= max h(xk ), max max h(xp ),
l(xp , œÄ(xp )) ‚àí z k+1
Ô£≥
Ô£≥p‚â•k+1
Ô£æÔ£æ
p‚â•k+1

= max h(xk ), V (xk+1 , z k+1 ; œÄ) ,

(16)

where we have defined z k+1 = z k ‚àí l(xk , œÄ(xk )) in the third equation.
A PPENDIX B
P ROOF OF T HEOREM 1
To prove Theorem 1, first, we prove several lemmas:
Lemma 1: For any fixed state x, let z ‚àó denote the solution of (15), i.e.,
min z,

(17a)

z

s.t.

V h (x; œÄ(¬∑, z)) ‚â§ 0,

(17b)

and let œÄ ‚àó denote œÄ(¬∑, z ‚àó ), i.e., it is the optimal policy for z ‚àó :
œÄ ‚àó = arg min V (x, z ‚àó ; œÄ).

(18)

œÄ

Then, no other safe policy œÄÃÉ exists that has a strictly lower cost than œÄ ‚àó while satisfying the constraints, i.e.,
V h (x; œÄÃÉ) ‚â§ 0
l

(19a)
l

‚àó

V (x; œÄÃÉ) < V (x; œÄ ).

(19b)

In other words, œÄ ‚àó is the optimal solution of the original constrained optimization problem
min V l (x; œÄ),

(20a)

œÄ

s.t.

V h (x; œÄ) ‚â§ 0.

(20b)

Before proving this lemma, we first prove the following lemma.
Lemma 2: Suppose that such a œÄÃÉ exists. Then, there exists a z ‚Ä† := V l (x; œÄÃÉ) ‚àí V h (x; œÄÃÉ) for which the optimal policy œÄ ‚Ä†
for z ‚Ä† satisfies the conditions for œÄÃÉ in (19a) and (19b), i.e.,
V h (x; œÄ ‚Ä† ) ‚â§ V h (x; œÄÃÉ) ‚â§ 0,
l

‚Ä†

l

l

(21a)
‚àó

V (x; œÄ ) ‚â§ V (x; œÄÃÉ) < V (x; œÄ ).

(21b)

Proof: Since œÄ ‚Ä† is optimal for z ‚Ä† , we have that
V (x, z ‚Ä† ; œÄ ‚Ä† ) ‚â§ V (x, z ‚Ä† ; œÄÃÉ).

(22)

This implies that, by definition of z ‚Ä† ,
n
o
n
o
max V h (x; œÄ ‚Ä† ), V l (x; œÄ ‚Ä† ) ‚àí z ‚Ä† ‚â§ max V h (x; œÄÃÉ), V l (x; œÄÃÉ) ‚àí z ‚Ä† ,
h

(23)

= V (x; œÄÃÉ).

(24)

V h (x; œÄ ‚Ä† ) ‚â§ V h (x, œÄÃÉ),

(25)



V l (x; œÄ ‚Ä† ) ‚àí V l (x; œÄÃÉ) ‚àí V h (x; œÄÃÉ) ‚â§ V h (x, œÄÃÉ),

(26)

In particular,
and

=‚áí V l (x; œÄ ‚Ä† ) ‚â§ V l (x; œÄÃÉ).

(27)

which proves (21a) and (21b).
We are now ready to prove Lemma 1.
Proof of Lemma 1: We prove this by contradiction.
Suppose that such a œÄÃÉ exists. By Lemma 2, there exists z ‚Ä† and œÄ ‚Ä† that satisfies the conditions for œÄÃÉ in (19a) and (19b).
Since œÄ ‚àó is optimal for z ‚àó , this implies that
n
o
n
o
max V h (x; œÄ ‚àó ), V l (x; œÄ ‚àó ) ‚àí z ‚àó ‚â§ max V h (x; œÄ ‚Ä† ), V l (x; œÄ ‚Ä† ) ‚àí z ‚àó .
(28)
We now consider two cases depending on the value of the max on the right.
n
o
Case 1 (V h (x; œÄ ‚Ä† ) ‚â§ V l (x; œÄ ‚Ä† ) ‚àí z ‚àó ): For this case, max V h (x; œÄ ‚Ä† ), V l (x; œÄ ‚Ä† ) ‚àí z ‚àó = V l (x; œÄ ‚Ä† ) ‚àí z ‚àó . This implies
that
V l (x; œÄ ‚àó ) ‚àí z ‚àó ‚â§ V l (x; œÄ ‚Ä† ) ‚àí z ‚àó
‚áê‚áí
V l (x; œÄ ‚àó ) ‚â§ V l (x; œÄ ‚Ä† ).
(29)
However, this contradicts our assumption that V l (x; œÄ ‚Ä† ) ‚â§nV l (x; œÄÃÉ) < V l (x; œÄ ‚àó ) from o
(19b).
Case 2 (V h (x; œÄ ‚Ä† ) > V l (x; œÄ ‚Ä† ) ‚àí z ‚àó ): For this case, max V h (x; œÄ ‚Ä† ), V l (x; œÄ ‚Ä† ) ‚àí z ‚àó = V h (x; œÄ ‚Ä† ). This implies that
V h (x; œÄ ‚àó ) ‚â§ V h (x; œÄ ‚Ä† )

(30)

and
V l (x; œÄ ‚àó ) ‚àí z ‚àó ‚â§ V h (x; œÄ ‚Ä† )

=‚áí

V l (x; œÄ ‚àó ) ‚àí V h (x; œÄ ‚Ä† ) ‚â§ z ‚àó .

(31)

‚Ä†

However, if we examine the definition of z , we have that
z ‚Ä† = V l (x; œÄÃÉ) ‚àí V h (x; œÄÃÉ)
l

h

(32)

‚Ä†

‚â§ V (x; œÄÃÉ) ‚àí V (x; œÄ )

(from (21a) and (30))

(33)

(from (19b))

(34)

‚â§ z‚àó

(from (31)).

(35)

l

‚àó

h

‚Ä†

< V (x; œÄ ) ‚àí V (x; œÄ )

This contradicts our definition of z ‚àó being the optimal solution of (17), since z ‚Ä† satisfies V h (x; œÄ(¬∑, z ‚Ä† )) ‚â§ 0 but is also
strictly smaller than z ‚àó .
Since both cases lead to a contradiction, no such œÄÃÉ can exist.
We also prove the following lemma.
Lemma 3: For any fixed state x, let z ‚àó denote the solution of (15), i.e.,
min z,
z

s.t.

V h (x; œÄ(¬∑, z)) ‚â§ 0,

(36a)
(36b)

and let œÄz‚àó denote œÄ(¬∑, z ‚àó ), i.e., it is the optimal policy for z ‚àó :
œÄz‚àó = arg min V (x, z ‚àó ; œÄ).

(37)

œÄ

Assuming that there does not exist another z such that V l (x; œÄz ) = V l (x; œÄz‚àó ). Then for any œµ ‚â• 0,
V h (x; œÄz‚àó +œµ ) ‚â§ V h (x; œÄz‚àó ).

(38)

Proof: Since œÄz‚àó +œµ is optimal for z = z ‚àó + œµ,
max{V h (x; œÄz‚àó +œµ ), V l (x; œÄz‚àó + œµ) ‚àí (z ‚àó + œµ)} ‚â§ max{V h (x; œÄz‚àó ), V l (x; œÄz‚àó ) ‚àí (z ‚àó + œµ)}.
h

(39)

‚àó

l

For the max on the right-hand side, if V (x; œÄz‚àó ) ‚â• V (x; œÄz‚àó ) ‚àí (z + œµ), then we immediately obtain our desired result
V h (x; œÄz‚àó +œµ ) ‚â§ V h (x; œÄz‚àó ).

(40)

We thus suppose that V h (x; œÄz‚àó ) ‚â• V l (x; œÄz‚àó ) ‚àí (z ‚àó + œµ), and obtain that

V h (x; œÄz‚àó +œµ ) ‚â§ V l (x; œÄz‚àó ) ‚àí (z ‚àó + œµ),
l

(41a)

l

(41b)

max{V h (x; œÄz‚àó ), V l (x; œÄz‚àó ) ‚àí z ‚àó } ‚â§ max{V h (x; œÄz‚àó +œµ ), V l (x; œÄz‚àó +œµ ) ‚àí z ‚àó }.

(42)

V (x; œÄz‚àó +œµ ) ‚â§ V (x; œÄz‚àó ).

Now, since œÄz‚àó is optimal for z = z ‚àó ,

We now split into two cases depending on the max on the right-hand side.
Case 1 (V h (x; œÄz‚àó +œµ ) ‚â• V l (x; œÄz‚àó +œµ ) ‚àí z ‚àó ): This implies that

V l (x; œÄz‚àó ) ‚àí z ‚àó ‚â§ V h (x; œÄz‚àó +œµ ),

(43)

V h (x; œÄz‚àó +œµ ) ‚â§ V l (x; œÄz‚àó ) ‚àí (z ‚àó + œµ) ‚â§ V h (x; œÄz‚àó +œµ ) ‚àí œµ,

(44)

hence,
which is a contradiction, so this case does not occur.
Case 2 (V h (x; œÄz‚àó +œµ ) < V l (x; œÄz‚àó +œµ ) ‚àí z ‚àó ): This implies that

V l (x; œÄz‚àó ) ‚àí z ‚àó ‚â§ V l (x; œÄz‚àó +œµ ) ‚àí z ‚àó ,

(45)

hence, V l (x; œÄz‚àó ) ‚â§ V l (x; œÄz‚àó +œµ ). Combining this with (41b) gives us that V l (x; œÄz‚àó ) = V l (x; œÄz‚àó +œµ ). However, from our
assumption, this implies that œÄz‚àó = œÄz‚àó +œµ and thus our desired result of V h (x; œÄz‚àó +œµ ) ‚â§ V h (x; œÄz‚àó ).

We can now prove Theorem 1, which follows as a consequence of Lemma 1 and Lemma 3.
Proof of Theorem 1: Since V h (x; œÄ) = maxi Vih (xi , oi ; œÄ), Lemma 1 implies that Equation (13) is equivalent to
z ‚àó := min{z | max Vih (xi , oi ; œÄ(¬∑, z)) ‚â§ 0}.
z

i

(46)

We now show that this is equivalent to the following distributed implementation (equal to (15)):
zi := min{z | Vih (xi , oi ; œÄ(¬∑, z)) ‚â§ 0},

zdistr = max zi .
i

We will now prove equality via a double inequality proof.

(47)
(48)

(zdistr ‚â§ z ‚àó ): By definition of z ‚àó ,

Vih (xi , oi ; œÄ(¬∑, z ‚àó )) ‚â§ 0,

However, since zi (47) is optimal, zi ‚â§ z ‚àó . Hence,

‚àÄi.

zdistr = max zi ‚â§ z ‚àó .

(50)

i

(zdistr ‚â• z ‚àó ): By definition of zdistr ,

zdistr ‚â• zi ,

(49)

‚àÄi.

(51)

Using Lemma 3, this implies that
Vih (xi , oi ; œÄ(¬∑, zdistr )) ‚â§ 0,

‚àÄi.

(52)

Hence, zdistr satisfies maxi Vih (xi , oi ; œÄ(¬∑, zdistr )) ‚â§ 0. Since z ‚àó is the smallest z that still satisfies this constraint,
z ‚àó ‚â§ zdistr .

(53)

We have thus proved that z ‚àó = zdistr , i.e., z ‚àó can be computed in a distributed fashion.
A PPENDIX C
D ISCUSSION ON I MPORTANCE OF P ROPOSITION 1
Establishing Proposition 1 is key to Def-MARL. Namely,
1) Satisfying dynamic programming implies that the value function is Markovian. In other words, for a given z 0 , the value
at the kth timestep is only a function of z k and xk instead of the z 0 and the entire trajectory up to the kth timestep.
2) Consequently, this implies that the optimal policy will also be Markovian and is only a function of z k and xk .
3) Rephrased differently, since the value function is Markovian, this implies that, for a given z 0 and x0 , the value at the
kth timestep is equal to the value (at the initial timestep) of a new problem where we start with zÃÉ 0 = z k and xÃÉ0 = xk .
4) Since we relate the value function of consecutive timesteps, given a value function estimator, we can now control the
bias-variance tradeoff of the value function estimate by using k-step estimates instead of the Monte Carlo estimates.
5) Instead of only using the k-step estimates for a single choice of k, we can compute a weighted average of the k-step
estimates as in GAE to further control the bias-variance tradeoff.
A PPENDIX D
A LGORITHM P SEUDOCODE
We describe the centralized training process of Def-MARL in Algorithm 1 and the distributed execution process in
Algorithm 2.
Algorithm 1 Def-MARL centralized training
Initialize: Policy NN œÄŒ∏ , cost value function NN Vœïl , constraint value function NN Vœàh .
while Training not end do
Randomly sampling initial conditions x0 , and the initial z 0 ‚àà [zmin , zmax ].
Use œÄŒ∏ to sample trajectories {x0 , . . . , xT }, with z dynamics (14).
Calculate the cost value function Vœïl (x, z) and the constraint value function Vœàh (oi , z).
Calculate GAE with the total value function (12).
Update the value functions Vœïl and Vœàh using TD error.
Update the z-conditioned policy œÄŒ∏ (¬∑, z) using PPO loss.
end while

A PPENDIX E
S IMULATION EXPERIMENTS
A. Computation resources
The experiments are run on a 13th Gen Intel(R) Core(TM) i7-13700KF CPU with 64GB RAM and an NVIDIA GeForce
RTX 3090 GPU. The training time is around 6 hours (105 steps) for Def-MARL and Lagr, and around 5 hours for Penalty.

Algorithm 2 Def-MARL distributed execution
Input: Learned policy NN œÄŒ∏ , constraint value function NN Vœàh .
for k = 0, . . . , T do
Get zi for each agent by solving the distributed EF-MASOCP outer problem (15b).
if z communication enabled then
The connected agents j communicate zj and reach a consensus z = maxj zj .
Set zi = z for all agents in the connected graph.
end if
Get decentralized policy œÄi (¬∑) = œÄŒ∏ (¬∑, zi ).
Execute control uki = œÄi (oki ).
end for
B. Environments
1) Multi-partical environments (MPE): We use directed graphs G = (V, E) to represent MPE, where V is the set of
nodes containing the objects in the multi-agent environment (e.g., agents Va , goals Vg , landmarks Vl , and obstacles Vo ).
E ‚äÜ {(i, j) | i ‚àà Va , j ‚àà V} is the set of edges, denoting the information flow from a sender node j to a receiver agent i. An
edge (i, j) exists only if the communication between node i and j can happen, which means the distance between node i and
j should be within the communication radius R in partially observable environments. We define the neighborhood of agent i
as Ni := {j | (i, j) ‚àà E}. The node feature vi includes the states of the node xi and a one-hot encoding of the type of the
node i (e.g., agent, goal, landmark, or obstacle), e.g., [0, 0, 1]‚ä§ for agent nodes, [0, 1, 0]‚ä§ for goal nodes, and [1, 0, 0]‚ä§ for
obstacle nodes. The edge feature eij includes the information passed between the sender node j and the receiver node i (e.g.,
relative positions and velocities).
We consider 6 MPE: TARGET, S PREAD, F ORMATION, L INE, C ORRIDOR, and C ONNECT S PREAD. In each environment, the
agents need to work collaboratively to finish some tasks:
‚Ä¢ TARGET [51]: Each agent tries to reach its preassigned goal.
‚Ä¢ S PREAD [17]: The agents are given a set of (not preassigned) goals to cover.
‚Ä¢ F ORMATION [2]: Given a landmark, the agents should spread evenly on a circle with the landmark as the center and a
given radius.
‚Ä¢ L INE [2]: Given two landmarks, the agents should spread evenly on the line between the landmarks.
‚Ä¢ C ORRIDOR : A set of agents and goals are separated by a narrow corridor, whose width is smaller than 4ra where ra is
the radius of agents. The agents should go through the corridor and cover the goals.
‚Ä¢ C ONNECT S PREAD : A set of agents and goals are separated by a large obstacle with a diameter larger than the
communication radius R. The agents should cover the goals without colliding with obstacles or each other while also
maintaining the connectivity of all agents.
We consider N = 3 agents for all environments and N = 5 and 7 agents in the F ORMATION and L INE environments. To
make the environments more difficult than the original ones [51], we add 3 static obstacles in the first 4 environments.
In our modified MPE, the state of the agent i is given by xi = [pxi , pyi , vix , viy ]‚ä§ , where [pxi , pyi ]‚ä§ := p ‚àà R2 is the position
of agent i, and [vix , viy ] is the velocity. The control inputs are given by ui = [axi , ayi ]‚ä§ , i.e., the acceleration along each axis.
The joint state is defined by concatenation: x = [x1 ; . . . ; xN ]. The agents are modeled as double integrators with dynamics

‚ä§
xÃái = vix viy axi ayi .
(54)
The agents‚Äô control inputs are limited by [‚àí1, 1], and the velocities are limited by [‚àí1, 1]. The agents have a radius ra =
0.05, and the communication radius is assumed to be R = 0.5. The area side length L is 1.0 for the C ORRIDOR and the
C ONNECT S PREAD environments and 1.5 for other environments. The radius of the obstacles ro is 0.4 in the C ORRIDOR
environment, 0.25 in the C ONNECT S PREAD environment, and 0.05 for other environments. All environments use a simulation
time step of 0.03s and a total horizon of T = 128.
The observation of the agents oi includes the node features of itself, its neighbors j ‚àà Ni , and the edge features of the edge
connecting agent i and its neighbors. The node features include neighbors‚Äô states xj and its type ([0, 0, 1] for agents, [0, 1, 0]
for goals and landmarks, and [1, 0, 0]‚ä§ for the obstacles). The edge features are the relative states eij = xi ‚àí xj .
The constraint function h contains two parts for all environments except for C ONNNECT S PREAD, including agent-agent and
agent-obstacles collisions. In the C ONNECT S PREAD environment, another constraint regarding the connectivity of the agent
graph is considered. For the agent-agent collision, we use the h function defined as


ha (oi ) = 2ra ‚àí min ‚à•pi ‚àí pj ‚à• + ŒΩsign 2ra ‚àí min ‚à•pi ‚àí pj ‚à• ,
(55)
j‚ààNi

j‚ààNi

h value

0.5
0.0
0.5
0.0

0.1
Distance

0.2

Fig. 12: h value with respect to distance.

where sign is the sign function, and ŒΩ = 0.5 in all our experiments. This represents a linear function w.r.t. the inter-agent
distance with a discontinuity at the safe-unsafe boundary (Fig. 12). For the agent-obstacle collision, we use


ho (oi ) = ra + ro ‚àí mino ‚à•pi ‚àí pj ‚à• + ŒΩsign ra + ro ‚àí mino ‚à•pi ‚àí pj ‚à• ,
(56)
j‚ààNi

j‚ààNi

where Nio is the observed obstacle set of agent i. Then, the total h function is defined as h(oi ) = max{ha (oi ), ho (oi )} for
environments except for C ONNECT S PREAD. For C ONNECT S PREAD, we also consider the connectivity constraint


hc (oi ) = max mino ‚à•pi ‚àí pj ‚à• ‚àí R‚Ä≤ + ŒΩsign max mino ‚à•pi ‚àí pj ‚à• ‚àí R‚Ä≤ ,
i

j‚ààNi

i

j‚ààNi

(57)

where R‚Ä≤ = 0.45 is the required maximum distance for connected agents such that if the distance between two agents is larger
than R‚Ä≤ , they are considered disconnected. Note that this cost is only valid with agent number N ‚â§ 3. For a larger number
of agents, the second-largest eigenvalue of the graph Laplacian matrix can be used. Still, since we only use this environment
with 3 agents, we use this cost to decrease the complexity. Then, the total h function of the C ONNECT S PREAD environment
is defined as h(oi ) = max{ha (oi ), ho (oi ), hc (oi )}.
Two types of cost functions are used in the environments. The first type is the Target cost used in the TARGET environment,
which is defined as
N



1 X
0.01‚à•pi ‚àí pgoal
‚à• + 0.001sign ReLU(‚à•pi ‚àí pgoal
l(x, u) =
‚à• ‚àí 0.01) + 0.0001‚à•ui ‚à•2 .
(58)
i
i
N i=1
The first term penalizes the agents if they cannot reach the goal, the second term penalizes the agents if they cannot reach the
goal exactly, and the third term encourages small controls. The second type is the Spread cost used in all other environments,
defined as
N




1 X
l(x, u) =
min 0.01‚à•pi ‚àí pgoal
‚à• + 0.001sign ReLU(‚à•pi ‚àí pgoal
‚à• ‚àí 0.01) + 0.0001‚à•uj ‚à•2 .
(59)
j
j
N j=1 i‚ààVa
Instead of matching the agents to their preassigned goals, each goal finds its nearest agent and penalizes the whole team with
the distance between them. In this way, the optimal policy of the agents is to cover all goals collaboratively.
2) Safe multi-agent MuJoCo environments: We also test on the S AFE H ALF C HEETAH (2 X 3) and S AFE C OUPLED
H ALF C HEETAH (4 X 3) tasks from the Safe Multi-Agent Mujoco benchmark suite [32]. Each agent controls a subset of joints
and must cooperate to minimize the cost (which we take to be the negative of the reward in the original work) while avoiding
violating safety constraints. The task is parametrized by the two numbers in the parentheses, where the first number denotes
the number of agents, while the second number denotes the number of joints controlled by each agent. The goal for the S AFE
H ALF C HEETAH and S AFE C OUPLED H ALF C HEETAH tasks is to maximize the forward velocity but avoid colliding with a
wall in front that moves forward at a predefined velocity.
Note: Although this is not a homogeneous MAS, since each agent has the same control space (albeit with different
dynamics), we can convert this into a homogeneous MAS by augmenting the state space with a one-hot vector to identify
each agent, then augmenting the dynamics to use the appropriate per-agent dynamics function. This is the approach taken in
the official implementation of Safe Multi-Agent Mujoco from Gu et al. [32]. For more details, see Gu et al. [32].

C. Implementation details and hyperparameters
We parameterize the z-conditioned policy œÄŒ∏ (oi , z), cost-value function Vœïl (x, z), and the constraint-value function Vœàh (oi , z)
using graph transformers [65] with parameters Œ∏, œï, and œà, respectively. Note that the policy and the constraint-value function
are decentralized and take only the local observation oi as input, while
P the cost-value function is centralized. In each layer of the
graph transformer, the node features are updated with vi‚Ä≤ = W1 vi + j‚ààNi Œ±ij (W2 vj + W3 eij ), where Wi are learnable weight
matrices, and the Œ±ij is the attention weight between agent i and agent j computed as Œ±ij = softmax( ‚àö1c (W4 xi )‚ä§ (W5 xj )),
where c is the first dimension of Wi . In this way, the observation oi is encoded. If the environment allows M -hop information
passing, we can apply the node feature update M times so that agent i can receive information from its M -hop neighbors.
After the information passing, the updated node features vi‚Ä≤ are concatenated with the encoded z vector W7 z, then passed to
another NN or a recurrent neural network (RNN) [34] to obtain the outputs. œÄŒ∏ and Vœàh have the same structure as introduced
above with different output dimensions because they are decentralized. For the centralized V l (x, z), the averaged node features
after information passing are concatenated with the encoded z and passed to the final layer (NN or RNN) to obtain the global
cost value for the whole MAS.
When updating the neural networks, we follow the PPO [63] structure. First, we calculate the target cost-value function
l
h
Vtarget
and the target constraint-value function Vtarget
using GAE estimation [62], and then backpropagate the following
mean-square error to update the value function parameters œï and œà:
M

LV l (œï) =

1 X l k k
l
‚à•Vœï (x , z ) ‚àí Vtarget
(xk , z k )‚à•2 ,
M

LV h (œà) =

1 XX h k k
h
‚à•Vœà (oi , z ) ‚àí Vtarget
(oki , z k )‚à•2 ,
MN
i=1

(60)

k=1

M

N

(61)

k=1

where M is the number of samples. Then, we calculate the advantages Ai for each agent with the total value function
Vi (x, z) = max{Vœïl (x, z) ‚àí z, Vœàh (oi , z)} following the same process as in PPO by replacing V l with V , and backpropagate
the following PPO policy loss to update the policy parameters Œ∏:
"
(
)#


M N
œÄŒ∏ (oki , z k )
1 XX
œÄŒ∏ (oki , z k )
k k
k k
(62)
min
LœÄ (Œ∏) =
Ai (x , z ), clip
, 1 ‚àí œµclip , 1 + œµclip Ai (x , z ) .
MN
œÄold (oki , z k )
œÄold (oki , z k )
k=1 i=1
Most of the hyperparameters of Def-MARL are shared with Penalty and Lagr. The values of the share hyperparameters
are provided in Table IV.
TABLE IV: Shared hyperparameters of Def-MARL, Penalty, and Lagr.
Hyperparameter
policy GNN layers
massage passing dimension
GNN output dimension
number of attention heads
activation functions
GNN head layers
optimizer
discount Œ≥
policy learning rate
V l learning rate
network initialization

Value
2
32
64
3
ReLU
(32, 32)
Adam
0.99
3e-4
1e-3
Orthogonal

Hyperparameter

Value

RNN type
RNN data chunk length
RNN layers
number of sampling environments
gradient clip norm
entropy coefficient
GAE Œª
clip œµ
PPO epoch
batch size
layer normalization

GRU
16
1
128
2
0.01
0.95
0.25
1
16384
True

Apart from the shared hyperparameters, Def-MARL has additional hyperparameters, as shown in Table V. In addition, zmin
and zmax are the lower and upper bounds of z while sampling z in training. Since zmin represents an estimate of the minimum
cost incurred by the MAS, we set it to a small negative number ‚àí0.5. We set zmax differently depending on the complexity of
the environment. For MPE, with maximum simulation timestep T , we estimate it in the MPE environments using the following
equation:
zmax = Àúlmax ‚àó T,
Àúlmax = initdistmax wdistance + wreach + umax wcontrol ,

(63)
(64)

where Àúlmax is a conservative estimate of the maximum cost l. This is conservative in the sense that this reflects the case
where 1) the agents and goals are initialized with the maximum possible distance (initdistmax ); 2) the agents do not reach
their goal throughout their trajectory; 3) the agents incur the maximum control cost for all timesteps. wdistance , wreach , and

0.0

0.0 0

1
Step

0

Cost

Safety rate
1
Step

√ó10

2

5

1
Step

2
√ó105

Safety rate

0.0 0

Step

0

1
Step

Def-MARL (ours)
Lagr(1)

1

√ó10

2

5

00

1
Step

Penalty(0.02)
Lagr(5)

Step

1

√ó105

ConnectSpread

0.5

0.0
√ó10

1.0
√ó105

0.5

1.0

1

0.5
Step
Line

0.0 0

√ó105

ConnectSpread

0.5

0.0

0.0
1.0

0.5

Corridor

1.0

1

00

0.5

0.5

1.0
√ó105

1.0

2
√ó105

Corridor

0.5
Step
Line

Cost

Safety rate

Cost

0.5

00.0

1.0
√ó105

Formation

1.0

1.0

1

Safety rate

Formation

0.5
Step

Spread

1.0

Safety rate

1.0
√ó105

0.5

Cost

0.5
Step

Spread

Cost

1

00.0

Target

1.0
Safety rate

Cost

Target

5

0

1
Step

√ó105

Penalty(0.1)
Penalty(0.5)

Fig. 13: Cost and safety rate of Def-MARL and the baselines during training in MPE.
wcontrol denote the corresponding weights of the different cost terms in the cost function l in (58) and (59). For the multi-agent
MuJoCo environments, we first train the agents with (unconstrained) MAPPO with different random seeds, record the largest
cost incurred, double it, and then use that as zmax .
TABLE V: Hyperparameters of Def-MARL.
Hyperparameter

Value

V h GNN layers

2 for ConnectSpread, 1 for others
8
Chandrupatla‚Äôs method [12]

z encoding dimension
outer problem solver

All the hyperparameters remain the same in all environments or are pointed out in the tables except for the training steps.
The training step is 105 in the TARGET and the S PREAD environments, 1.5 √ó 105 in the L INE environment, and 2 √ó 105 in
other MPE. For the Safe Multi-agent MuJoCo environments, we set the training step to 7 √ó 103 .
D. Training curves
To show the training stability of Def-MARL, we have shown the cost and safety rate of Def-MARL and Lagr(lr) during
training in the TARGET and S PREAD environments in the main pages (Fig. 6). Due to page limits, we provide the plots
for other environments here in Fig. 13, Fig. 14, and Fig. 15. The figures show that Def-MARL achieves stable training in
all environments. Specifically, as shown in Fig. 14, while Lagr(lr) suffers from training instability because the constraint
violation threshold is zero (as discussed in Section III-B), Def-MARL is much more stable.
E. More comparison with the Lagrangian method
In this section, we provide more comparisons between Def-MARL and the Lagrangian method, where we change the
constraint-value function of the Lagrangian method from the sum-over-time (SoT) form to the max-over-time (MoT) form.

0.0
0.0

1
Step

0

Cost

Safety rate
1
Step

√ó10

2

1
Step

0.0

Safety rate
Safety rate

0.0 0

2
√ó105

Step

0

1
Step

1

√ó10

00

2

5

1
Step

0

Def-MARL (ours)

Step

1

√ó105

ConnectSpread

0.5

0.0
√ó10

1.0
√ó105

Line

1.0

1

0.5
Step

0.5

√ó105

ConnectSpread

0.5

5

0.0
1.0

0.5

Corridor

1.0

1

00

0.5

0.5

1.0
√ó105

1.0

2
√ó105

Corridor

0.5
Step
Line

Cost

Safety rate

Cost

0.5

0.00.0

1.0
√ó105

Formation

1.0

1.0

0.5
Step

0.5

Safety rate

1.0
√ó105

Formation

0.0 0

0.5

Spread

1.0

1.0

Cost

0.5
Step

Spread

Cost

1

00.0

Target

1.0
Safety rate

Cost

Target

5

0

1
Step

√ó105

Lagr(lr)

Fig. 14: Cost and safety rate of Def-MARL and Lagr(lr) during training in MPE.

Safe HalfCheetah 2x3

Safe Coupled
HalfCheetah 4x3

Safe HalfCheetah 2x3
1.0

Safe Coupled
1.0 HalfCheetah 4x3

800

5
√ó103
Step
Def-MARL (ours)
Lagr(1)

Safety rate

0.6

400

0.4

0.6
0.4

200

0.2
00

0.8

600
Cost

Cost
200

Safety rate

0.8
400

0.2
0

5
√ó103
Step
Penalty(0.02)
Lagr(5)

00

5
√ó103
Step
Penalty(0.1)
Lagr(lr)

0

Step
Penalty(0.5)

5

√ó103

Fig. 15: Cost and safety rate of Def-MARL and all baselines during training in Safe Multi-agent MuJoCo environments.

Using the MoT form, the constraint-value function of the Lagrangian method becomes the same as the one used in Def-MARL
(Equation (9)). We create 3 more baselines using this approach with different learning rates (lr) of the Lagrangian multiplier
Œª, where lr(Œª) ‚àà {0.1, 0.2, 0.3}. The baselines are called Lagr-MoT. We compare Def-MARL with the new baselines in the
TARGET environment, and the results are presented in Fig. 16. We can observe that the Lagrangian method has very different
performance with different learning rates of Œª. With lr(Œª) = 0.1, the learned policy is unsafe, and with lr(Œª) = 0.2 or 0.3,
the training is unstable and the cost of the converged policy is much higher than Def-MARL. In addition, we also plot the Œª
values during training in Fig. 16. It shows that Œª keeps increasing without converging to some value, which also suggests the

Cost

1.0

0.5

0.0 0

1.0

20

0.8

15

0.6

10

Œª

Safety rate

1.5

0.4

50000
Step

0.2

100000

Def-MARL (ours)
Lagr-MoT (lr(Œª) = 0.1)
Lagr-MoT (lr(Œª) = 0.2)

5

0

50000
Step

Lagr-MoT (lr(Œª) = 0.3)

0 0

100000

50000
Step

100000

Fig. 16: Cost and safety rate of Def-MARL and Lagr-MoT with different learning rates of Œª during training in the TARGET
environment, and the Œª values during training.

instability of the Lagrangian method.
F. Sensitivity analysis on the choice of zmax
In Appendix E-C, we have introduced how to determine the sampling interval of z. Here, we perform experiments in the
S PREAD environment to study the sensitivity of Def-MARL on the choice of zmax . In this experiment, we scale the value
of zmax used for sampling z, and denote by zmax,orig the original value used in the experiments in the main pages, i.e.,
zmax /zmax,orig = 1.0 uses the same value as in the main pages. We report the safety rates and the costs of the Def-MARL
policies trained with different zmax in Table VI. We see that both safety and costs do not change much even when our estimate
of the maximum cost zmax changes by up to 50%. If zmax is too large (e.g., 2 zmax,orig ), the policy becomes too conservative
because not enough samples of z that are near z ‚àó are observed, reducing the sample efficiency. On the other hand, when zmax
is too small (e.g., 0.25 zmax,orig ), there may be states where the optimal z ‚àó does not fall within the sampled range. This causes
the rootfinding step to be inaccurate, as V h will be queried at values of z that were not seen during training, resulting in safety
violations.
TABLE VI: Safety and Cost of Def-MARL policies trained with different zmax .
zmax /zmax,orig

Safety rate

Cost

0.25
0.5
1.0
1.5
2.0

93.8 ¬± 2.4
98.0 ¬± 1.4
99.0 ¬± 0.9
99.0 ¬± 0.0
99.0 ¬± 0.1

0.152 ¬± 0.100
0.155 ¬± 0.104
0.162 ¬± 0.144
0.165 ¬± 0.100
0.228 ¬± 0.109

G. Effect of zi communication with more agents
To test the effect of zi communication with more agents, we increase the number of agents to 512 (at constant density) in
TARGET environment during the test (Table VII). Without communication, we only see a 0.3% drop in safety rate going from
N = 32 to N = 512. With z communication, safety does not decrease with increased N .
TABLE VII: Effect of zi communication in larger scale environments.
N

No communication (z ‚Üê zi )
Safety rate
Cost

Communication (z = maxi zi )
Safety rate
Cost

32
128
512

99.8 ¬± 0.2
99.6 ¬± 0.4
99.5 ¬± 0.3

99.8 ¬± 0.2
99.8 ¬± 0.3
99.9 ¬± 0.1

‚àí0.387 ¬± 0.029
‚àí0.408 ¬± 0.015
‚àí0.410 ¬± 0.009

‚àí0.416 ¬± 0.052
‚àí0.491 ¬± 0.090
‚àí0.608 ¬± 0.210

H. Code
The code of our algorithm and the baselines are provided on our project website https://mit-realm.github.io/def-marl/.

A PPENDIX F
H ARDWARE EXPERIMENTS
A. Implementation details
Hardware platform. We perform hardware experiments using a swarm of Crazyflie (CF) 2.1 platform [27]. We use two
Crazyradios to communicate with the CFs and use the Lighthouse localization system to perform localization with four SteamVR
Base Station 2.0. The computation is split into two parts: Onboard computation is performed on the CF microcontroller unit
(MCU), and the offboard computation is performed on a laptop connected to the CFs over Crazyradio. We use the crazyswarm2
ROS2 package to communicate with the CFs, and a single ROS2 node for the off-board computations at 50Hz.
Control framework. Given a state estimation of the CFs using the Lighthouse system, the laptop inputs the states to
the algorithm models (Def-MARL, CMPC, or DMPC), which then outputs the accelerations. Then, the laptop computes the
desired next positions, velocities, and accelerations for the CFs, and sends the information to the CF MCU. After receiving
the information, the CF MCU uses the onboard PID controller to track the desired position, velocity, and acceleration, which
outputs the motor commands. Distributed execution is simulated on the laptop by only giving each agent local information as
input to their distributed policies.
B. Tasks
C ORRIDOR. The C ORRIDOR task uses the same cost and constraint function as the simulation C ORRIDOR environment,
which has been introduced in Appendix E-B.
I NSPECT. In this task, two CF drones need to work collaboratively to observe a moving target CF drone while maintaining
collision-free with each other and the obstacles. The constraint function h is defined the same as the simulation environments
defined as h(oi ) = max{ha (oi ), ho (oi )} with ha and ho defined in (55) and (56). The cost function is given by
N

1 X
laction (ui ),
l(x, u) = lvisibility (x) + 0.1 min ldist (xi , x) +
i
N i=1

(65)

where lvisibility (x) = 0 if the target is observable by at least one agent and lvisibility (x) = 0.01 otherwise. For ldist (xi , x), we
have ldist (xi , x) = 0 if the target is observable by agent i, otherwise, it is defined as the distance between the farthest observed
point on the agent-target line and the target. Finally, laction (ui ) is defined as laction (ui ) = 0.00001‚à•ui ‚à•2 .
C. Videos
The videos of our hardware experiments are provided in the supplementary materials named ‚ÄòCorridor.mp4‚Äô and ‚ÄòInspect.mp4‚Äô.
A PPENDIX G
C ONVERGENCE
In this section, we analyze the convergence of the inner RL problem (13b) to a locally optimal policy.
Since we solve the inner RL problem (13b) in a centralized fashion, it can be seen as an instantiation of single-agent RL,
but with a per-agent independent policy. Define the augmented state xÃÉ ‚àà XÃÉ := X √ó R as [x, z], which follows the dynamics
fÀú : XÃÉ √ó U ‚Üí XÃÉ defined as
 

fÀú [xk , z k ], uk = f (xk , uk ), z k ‚àí l(xk , uk ) .
(66)
The inner RL problem (13b) can then be stated as
min
œÄ

max h(xk , œÄ(xÃÉk ))
k‚â•0
k+1

s.t. xÃÉ

= fÀú(xÃÉk , œÄÃÉ(xÃÉk )), k ‚â• 0.

(67a)
(67b)

This is an instance of a single-agent RL avoid problem. Consequently, applying the results from [81, Theorem 5.5] or [67,
Theorem 4] gives us that the policy œÄ converges almost surely to a locally optimal policy.
A PPENDIX H
O N THE EQUIVALENCE OF THE MASOCP AND ITS EPIGRAPH FORM
In Section III-B, we state that the Epigraph form (5) of a constrained optimization problem is equivalent to the original
problem (3). This has been proved in So and Fan [66]. To make this paper more self-contained, we also include the proof
here.

Proof: For a constrained optimization problem (3), its epigraph form [10, pp 134] is given by
min z,
œÄ,z

s.t.

h(œÄ) ‚â§ 0,

J(œÄ) ‚â§ z,

(68a)
(68b)
(68c)

where z ‚àà R is an auxiliary variable. Here, (68b) and (68c) can be combined, which leads to the following problem:
min z,
œÄ,z

max {h(œÄ), J(œÄ) ‚àí z} ‚â§ 0.

s.t.

(69a)
(69b)

Using this form, So and Fan [66, Theorem 3] shows that the minimization of x can be moved into the constraint, which yields
min z,
z

s.t.
This is the same as (5).

min max {h(œÄ), J(œÄ) ‚àí z} ‚â§ 0.
œÄ

(70a)
(70b)

