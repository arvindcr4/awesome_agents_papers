Paper: Multi_Turn_RLHF.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Multi-turn Reinforcement Learning from Preference Human Feedback  arXiv:2405.14655v2 [cs.LG] 2 Dec 2024  Lior Shani ∗ 1 liorshani@google.com Oran Lang 1 Bilal Piot 2  Aviv Rosenberg ∗ 1 avivros@google.com  Daniele Calandriello 2 Idan Szpektor 1  Avital Zipori 1  Avinatan Hassidim 1  Asaf Cassel ∗ 1 3 acassel@mail.tau.ac.il Hila Noga 1  Orgad Keller 1  Yossi Matias 1  Rémi Munos 2  Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in vario...

Specific Evidence for Classification:
- "Multi-turn Reinforcement Learning from Preference Human Feedback  arXiv:2405.14655v2 [cs.LG] 2 Dec 2024  Lior Shani ∗ 1 liorshani@google.com Oran Lang 1 Bilal Piot 2  Aviv Rosenberg ∗ 1 avivros@google.com  Daniele Calandriello 2 Idan Szpektor 1  Avital Zipori 1  Avinatan Hassidim 1  Asaf Cassel ∗ 1 3 acassel@mail.tau.ac.il Hila Noga 1  Orgad Keller 1  Yossi Matias 1  Rémi Munos 2  Abstract Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks."
- "Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
