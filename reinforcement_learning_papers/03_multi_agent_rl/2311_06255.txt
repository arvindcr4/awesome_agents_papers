Privacy-Engineered Value Decomposition Networks for Cooperative
Multi-Agent Reinforcement Learning

arXiv:2311.06255v1 [cs.MA] 13 Sep 2023

Parham Gohari, Matthew Hale, and Ufuk Topcu
Abstract— In cooperative multi-agent reinforcement learning
(Co-MARL), a team of agents must jointly optimize the team’s
long-term rewards to learn a designated task. Optimizing
rewards as a team often requires inter-agent communication
and data sharing, leading to potential privacy implications.
We assume privacy considerations prohibit the agents from
sharing their environment interaction data. Accordingly, we
propose Privacy-Engineered Value Decomposition Networks
(PE-VDN), a Co-MARL algorithm that models multi-agent
coordination while provably safeguarding the confidentiality of
the agents’ environment interaction data. We integrate three
privacy-engineering techniques to redesign the data flows of
the VDN algorithm—an existing Co-MARL algorithm that
consolidates the agents’ environment interaction data to train
a central controller that models multi-agent coordination—and
develop PE-VDN. In the first technique, we design a distributed
computation scheme that eliminates Vanilla VDN’s dependency
on sharing environment interaction data. Then, we utilize a
privacy-preserving multi-party computation protocol to guarantee that the data flows of the distributed computation scheme
do not pose new privacy risks. Finally, we enforce differential
privacy to preempt inference threats against the agents’ training
data—past environment interactions—when they take actions
based on their neural network predictions. We implement
PE-VDN in StarCraft Multi-Agent Competition (SMAC) and
show that it achieves 80% of Vanilla VDN’s win rate while
maintaining differential privacy levels that provide meaningful
privacy guarantees. The results demonstrate that PE-VDN can
safeguard the confidentiality of agents’ environment interaction
data without sacrificing multi-agent coordination.

I. INTRODUCTION
Cooperative multi-agent reinforcement learning (CoMARL) is a machine learning problem in which multiple
agents work together to optimize performance in a common
task. The agents interact with an environment that rewards
their actions as a team and must learn a decision-making
scheme that maximizes the team’s long-term rewards through
trial and error [1]. Co-MARL algorithms can extend the capabilities of single-agent reinforcement learning algorithms
to complete complex tasks that involve multiple agents;
for instance, in self-driving vehicles where reinforcement
learning is a popular approach for autonomous driving [2],
Co-MARL may enable a fleet of autonomous vehicles to
cooperate and reduce traffic congestion [3].
P. Gohari is with the Department of Electrical and Computer Engineering,
and Robert Strauss Center for International Security and Law, University of
Texas at Austin, Austin, TX, USA pgohari@utexas.edu
M. Hale is with the Faculty of Mechanical and Aerospace Engineering,
University of Florida, Gainesville, FL, USA matthewhale@ufl.edu
U. Topcu is with the Faculty of Aerospace Engineering and Engineering Mechanics, University of Texas at Austin, Austin, TX, USA

utopcu@utexas.edu

We study modeling multi-agent coordination in Co-MARL
systems with privacy in mind. Effective coordination often requires inter-agent communication and data sharing.
However, sharing data may have privacy ramifications in
situations where agents represent privacy-sensitive entities or
handle privacy-sensitive information. For example, sharing
a self-driving vehicle’s environment interaction data may
reveal commuting patterns and sensitive locations such as
home, places of worship, and nightlife activities [4].
We assume that any data sharing that reveals the agents’
interactions with the environment violates privacy. The presence of privacy-sensitive information in the agents’ environment interaction data could complicate Co-MARL algorithms that rely on sharing them. Centralized training
algorithms [5] are major examples in which a central node
consolidates the environment interaction data of all agents
and trains a central controller that determines the team’s
actions. The central controller accounts for the dynamics of
how the training of one agent affects others and effectively
models multi-agent coordination. By requiring the agents to
share their environment interaction data, centralized training
methods expose the agents’ sensitive information to various
privacy risks such as data breaches and unauthorized access.
Accordingly, we raise the following research question:
Is it possible to redesign the data flows of centralized training algorithms to safeguard the confidentiality of the agents’ environment interaction data
without losing multi-agent coordination?
We show that it is indeed possible to design such an algorithm and propose Privacy-Engineered Value Decomposition
Networks (PE-VDN). Instead of relying on a central node
that consolidates the agents’ environment interaction data,
PE-VDN establishes multi-agent coordination by creating
peer-to-peer communication channels. Additionally, the algorithm incorporates additional privacy-enhancing techniques
to ensure that its established information flows do not undermine the confidentiality of the agents’ privacy-sensitive
environment interaction data.
We develop PE-VDN based on the Value Decomposition
Networks (VDN) algorithm in [6]—referred to as Vanilla
VDN, hereafter. Vanilla VDN is a centralized training and
decentralized execution (CTDE) Co-MARL algorithm that
uses a specially structured function approximator to estimate
the team’s action values and compute optimal actions. The
function approximator consists of multiple branches of neural
networks, each of which is designated for one agent. These
neural networks use their designated agent’s environment

interaction data as input, and the summation of their outputs
estimates the team’s action value. The coupling of the neural
networks during training takes multi-agent coordination into
account. Once training concludes, the algorithm distributes
the final neural network branches back to their designated
agents so that they can use them to make their own decisions
at runtime. This decentralized execution feature is well-suited
to our privacy goals because it eliminates the need for sharing
any data during execution.
We use the Vanilla VDN algorithm as PE-VDN’s starting
point and incorporate three privacy-engineering techniques
to modify data flows and satisfy our privacy goals. First,
we decentralize the vanilla algorithm’s training. We demonstrate that the gradients that Vanilla VDN computes for
centralized training are only coupled by a summation term
that aggregates the output of all neural network branches
in the action-value function approximator. We propose an
equivalent distributed computation scheme for the gradients
that enables the agents to locally maintain and optimize their
dedicated neural network branches themselves. The resulting
decentralized training algorithm computes the same gradients
as Vanilla VDN and only requires the agents to share their
neural network outputs with each other; hence, it mitigates
the privacy risks of sharing environment interaction data.
In the second privacy measure, we integrate a privacypreserving multi-party computation protocol with the distributed computation of the gradients. The protocol enables
the agents to compute the summation term that couples their
gradients while hiding the value of their neural network
outputs from each other. Since the agents only need the summation term for decentralized training, revealing their neural
network outputs for computing the summation unnecessarily exposes information that correlates with their sensitive
environment interaction data. The protocol obfuscates the
neural network outputs with correlated random numbers that
act as encryption keys, then splits the resulting values into
encrypted peer-to-peer messages. The protocol guarantees
that as long as no party gains access to all of the peer-topeer messages that an agent emits, the agents’ neural network
outputs remain a secret.
The third and last privacy-protection layer that we design
for PE-VDN protects the agents’ environment interaction
data against indirect inference threats. The agents choose
their actions based on their internal neural network’s predictions and unintentional information leaks can occur when
neural networks release their predictions. For example, socalled membership and attribute inference attacks, and inversion attacks are all known to make accurate inferences
about the training data of a neural network by mere input and
output observations [7]. In PE-VDN, the neural networks that
determine the agents’ actions are trained with environment
interaction data. Our third privacy-engineering technique
preempts inference threats against the agents’ environment
interaction data.
The DP-SGD algorithm [8] is a privacy-preserving training
algorithm that can disrupt inference attacks against the training dataset of neural networks. DP-SGD enforces differential

privacy during training and helps protect the confidentiality
of the training data when external entities interact with
the trained models [9]. Intuitively, DP-SGD guarantees that
two training datasets that differ in a single record produce
approximately statistically indistinguishable neural network
parameters. If the agents train their internal neural networks
with DP-SGD, then the plausible-deniability guarantees that
DP-SGD provides can reduce the risks of indirect information leakage about the agents’ past environment interaction
data.
The DP-SGD algorithm can be seamlessly integrated with
our last two privacy-engineering techniques; however, its
differential privacy analysis does not readily apply to deep
reinforcement learning algorithms. DP-SGD was originally
developed for supervised machine learning algorithms with
static training datasets, whereas in deep reinforcement learning, a stream of environment interaction data continuously
enters a replay buffer and is used for training. We provide a
theoretical analysis that leverages DP-SGD’s Moments Accountant method [8] and Maximum-Overlap Parallel Composition [10] to compute the differential privacy level of DPSGD when applied to deep reinforcement learning.
We implement a Python library for PE-VDN and test it in
the StarCraft Multi-Agent Competition (SMAC) suite [11].
In the numerical results, we dissect the different privacyengineering components and study the trade-offs between
privacy, precision, and performance. Our results show that
within the acceptable differential privacy range of 0 and 10
[12], the agents can achieve 80% of Vanilla VDN’s win rate.
II. PRELIMINARIES
In this section, we cover some technical background on
Co-MARL’s problem formulation and review Vanilla VDN.
Then, we describe our privacy objectives for Co-MARL in
the format of a technical privacy policy.
A. Dec-POMDP
Co-MARL algorithms typically model multi-agent cooperation using decentralized partially observable Markov
decision processes (Dec-POMDPs) [13]. A Dec-POMDP G is
a tuple G = hN , S, A, O, Z, P, R, γi where N is the set of
agents and N = |N |; S, A, and O are the sets of all possible
states, actions, and observations, respectively; Z : S 7→ O is
an observation function; P : S N × AN × S N 7→ [0, 1] is the
environment’s transition probabilities; R : S N ×AN ×S N 7→
R is the reward function; and γ ∈ [0, 1) is the discount factor.
A team policy, denoted π := (πi )i∈N , determines the
actions that each of the agents must take at every environment observation. Similar to the notation used for the team
policy, we use bold symbols to denote team actions and
observations. In POMDPs, the policy typically incorporates
a history of past environment observations and actions. Let
τ t = h(o1 , a1 ), . . . , (ot−1 , at−1 ), ot i denote the team’s
history at time t where, for all k ≤ t, ak ∈ AN and o ∈ ON .
Then, π(at | τ t ) is the probability that the team takes action
at when its history is τ t .

Given a team policy π, a value function V π evaluates the
expected total reward that the policy accumulates, i.e.,
"∞
#
X
π
t−1
V (s) = E
γ R(st , at , st+1 ) | s1 = s ,
(1)
t=1

where at ∼ π and st+1 ∼ P(st+1 | st , at ). An action-value
function Qπ is similarly defined as
Qπ (s, a) = E [R(s, a, s′ ) + γV π (s′ ) | s′ ∼ P(s′ | s, a)] .
(2)
The goal of solving a Dec-POMDP is to find an optimal
action-value function Q∗ and a corresponding optimal policy
∗
π ∗ such that Qπ (s, a) = Q∗ (s, a) = supπ Qπ (s, a). In
Co-MARL, the goal is to find an optimal policy without
knowing the underlying transition probabilities.
B. Deep Q-Learning
Deep Q-Learning (DQN) [14] is a variant of tabular Qlearning that uses deep neural networks to approximate the
Q-function. By using neural networks, DQN can handle highdimensional and continuous state spaces. DQN uses a replay
buffer to train the neural network that approximates the Qfunction. The replay buffer is a fixed-length database of
the agent’s past environment interaction data in the form
of hst , at , st+1 , R(st , at , st+1 )i, or in short hs, a, s′ , ri. The
replay buffer is typically filled as follows: the most recent
data replaces the oldest entry. However, other methods that
use specific heuristics to identify the entry that must be
replaced with incoming data exist as well [15].
With E denoting a minibatch of the replay buffer, DQN
minimizes the Bellman error loss function defined as follows:
2
X 

π
′ ′ −
π
−
Q
(s,
a;
θ)
r + γ max
s
,
a
;
θ
Q
,
ℓ(θ) =
′
a

hs,a,s′ ,ri∈E

(3)
where θ− is called the target parameters. These parameters
are copied from θ periodically to stabilize training.
C. Multi-Agent DQN

Both tabular Q-learning and DQN can be used to solve
Co-MARL problems. Since the agents in Co-MARL are
rewarded together as a team, a single Q-function can represent the action values of the entire team, i.e., the team
can be treated as a single agent with a multi-dimensional
action. Such bundling of the agents reduces Co-MARL to
single-agent reinforcement learning and is the main basis
of centralized training methods. Bundling agents requires
a consolidated replay buffer to support DQN. In the consolidated replay buffer, all parameters except the rewards
are replaced with their team versions—team states and team
actions and thus hs, a, s′ , ri is used instead of hs, a, s′ , ri.
We also replace all state values s with histories τ to support
partial observability. The resulting loss function is
2
X 

π
′
′ −
π
r + γ max
Q τ , a ; θ − Q (τ , a; θ) .
′
hτ ,a,τ ′ ,ri∈E

a

(4)

Once training concludes, the trained Q-function for the
team supports the decision-making of all agents. In this case,
the agents must aggregate their environment observations to
execute the learned policy too. Therefore, this algorithm is
an instance of centralized training and centralized execution.
Alternatively, each agent may attribute the team rewards
to itself and use DQN to train a policy independently. This
approach is often called independent Q-learning. Independent
training allows the agents to execute their policy without
having to share their environment observations; however, as
opposed to centralized training, independent training does
not take multi-agent cooperation into account but it often
scales better with the number of agents [1].
In CTDE methods, the agents use centralized training to
learn the team’s optimal Q-function but then decompose it
into a set of local Q-functions that allow for decentralized
execution. The agents in CTDE methods choose the action
that maximizes their local Q-function; therefore, similar to
independent Q-learning, the agents need not share data to
execute the team’s policy. The difference between these local
Q-functions and those obtained via independent training,
however, is that the former takes multi-agent coordination
into account by design during training.
CTDE methods typically assume that the agents’ individually optimal actions amount to the optimal action for the
team. This so-called decentralizability assumption justifies
decomposing the team’s Q-function into local Q-functions
and supports decentralized execution. The formal definition
of decentralizability is as follows:
Definition 1: A reinforcement learning task is decentralizable if there exists a collection of local action-value functions
{Qi }i∈N such that, for all team histories τ , team actions a,
and agents i ∈ N ,


(5)
arg max Qπ (τ , a) = arg max Qi (τi , ai ).
a

i

ai

D. Vanilla VDN

The Vanilla VDN algorithm leverages decentralizability
by approximating the team’s central Q-function with the
summation of some local Qi functions. That is, the algorithm
assumes that there exist {Qi }i∈N such that, for all joint
policies π, histories τ , and actions a,
X
Qπ (τ , a) =
Qi (τi , ai ).
(6)
i∈N

This additivity assumption implies decentralizability in (5);
however, not all decentralizable tasks satisfy additivity.
In the centralized training phase of Vanilla VDN, the
agents must send their environment interaction data to a
central node to create a consolidated replay buffer and train
the team’s Q-function. The optimizer first draws a minibatch
of the consolidated replay buffer, denoted E. Let, e =
hτ , a, τ ′ , ri ∈ E be an element of the minibatch, where τ =
(τi )i∈N , a = (ai )i∈N , and τ ′ = (τi′ )i∈N denote the team’s
history, action, and next-step history, respectively. With θi
denoting the parameters of agent i’s dedicated portion of the

Q-function, Vanilla VDN’s objective function is as follows:

•
•

ℓV DN (e; {θi }i∈N ) =
!
 2
X

′ ′ −
r+
γ max
Qi τi , a ; θi − Qi (τi , ai ; θi )
.
′
i∈N

Sharing environment interaction data is forbidden.
Under mathematical guarantees that ensure the confidentiality of the sender agent’s environment interaction
data and that of other agents, the agents may share other
types of data for higher team rewards.

a

(7)

Vanilla VDN uses backpropagation to compute the gradient of the objective function with respect to each θi and uses
gradient descent methods to update the Q-function’s weights
and biases. That is, for all neural network branches {θi }i∈N ,
the optimizer performs the update
X ∂ℓVDN (e; θ1 , . . . , θN )
θi ← θi − α
,
(8)
∂θi
e∈E

where α is the learning rate. Once centralized training
concludes, the optimizer distributes the agents’ designated
portions—namely Qi (·, ·; θi ) for each i ∈ N —back to the
agents to support decentralized execution.
E. Technical Privacy Policy
According to the Contextual Integrity Theorem [16], privacy is a socially created need and a contextual concept. The
theorem explains that inappropriate information flows lead to
privacy concerns in society. What constitutes inappropriate
information flows according to the theorem depends on five
parameters: the subject of the information flow, its senders
and recipients, information types, and transmission principles
such as obtaining consent. Besides privacy concerns, the
theorem contends that the interests of the affected parties,
ethical and political values, and contextual functions, purposes, and values ultimately determine the ethical legitimacy
of information flows in society.
Based on Contextual Integrity, algorithms do not have
innate privacy issues because they can be deployed in various contexts and may or may not establish inappropriate
information flows. As a result, we approach privacy for CoMARL with a scenario-based privacy analysis methodology.
That is, we assume that certain privacy concerns exist and
develop algorithmic solutions to address them.
We assume that the agents’ environment interaction data
in Co-MARL contain privacy-sensitive information such that
any information flow that carries this information type is
inappropriate. The assumption implies that the existence of
a central node that consolidates replay buffers in centralized
training algorithms violates privacy. Moreover, any information flow that does not necessarily carry environment
interaction data but exposes it to indirect inference reasoning
violates privacy as well.
We formalize our privacy objectives in the format of a
technical privacy policy. This policy is not the same as those
displayed on websites, but, similar to R. Anderson’s security
policy for security engineering [?], it is rather a short and
verifiable statement of the specifications that our privacy
assumptions imply for Co-MARL. The technical privacy
policy that we consider is as follows:

III. RELATED WORK
Our main research objective is to design a Co-MARL algorithm that is subject to a technical privacy policy governing
the handling of the agents’ environment interaction data. In
this section, we briefly review the key research themes that
are relevant to our research objective.
The survey in [17] and its references to previous related
surveys provide an overview of privacy and security for
multi-agent machine learning. These surveys point to numerous privacy threats against distributed learning systems
and are closely related to the technical privacy policy that
we consider. Furthermore, these surveys refer to federated
learning, secure multi-party computation, and differential
privacy as three prominent defensive mechanisms for general
machine learning tasks. We incorporate all three mechanisms
in the design of PE-VDN.
Next, we review existing works in designing privacy-aware
deep reinforcement learning agents. The single-agent deep
reinforcement learning algorithm in [18] is a differentially
private algorithm that protects the confidentiality of the
agent’s rewards. The work in [19] develops a single-agent
tabular reinforcement learning algorithm that satisfies joint
differential privacy—a relaxation of conventional differential
privacy. As opposed to the first work, we consider the
multi-agent setup, protect the agents’ environment interaction
data which includes rewards, and instead of developing a
customized differential privacy mechanism that perturbs the
objective function, we use verified and open-source libraries.
Compared with the second algorithm, we consider a multiagent setup, take continuous observations into account, and
do not use differential privacy relaxations. The single-agent
algorithm in [20] generalizes enforcing joint differential
privacy to continuous state and action spaces; however, it
is restricted to linear function approximators as opposed to
this work’s use of deep neural networks.
The decentralized multi-agent deep reinforcement learning
algorithms in [21] and [22] closely relate to the technical
privacy policy that we consider and both of the algorithms
satisfy the policy’s first requirement. However, the algorithms
do not consider team rewards and assume that the agents
are rewarded individually. Moreover, the algorithms do not
address the privacy risks associated with the inter-agent
communication frameworks that they propose.
IV. P RIVACY-E NGINEERING VANILLA VDN
We develop three privacy-engineering techniques to redesign Vanilla VDN’s information flows and satisfy the
privacy requirements that we expressed in the technical
privacy policy. In this section, we describe each of the
three technical privacy-enhancing techniques that we use to
develop PE-VDN.

A. Decentralized Training
The Vanilla VDN algorithm’s update rule in (8) requires
the loss function’s gradients with respect to the parameters of
all neural network branches within the function approximator. Recall that, in Vanilla VDN, these branches are dedicated
to specific agents for decentralized execution. Evaluating the
gradient of the loss function in (7), for every branch θi and
minibatch sample e, we can write
∂ℓVDN (e; θ1 , . . . , θN )
=
∂θi
!

X

−
−2 r+
γ max
Qi τi′ , a′ ; θi − Qi (τi , ai ; θi )
′
a

i∈N

|

{z

}

A

·

∂Qi (τi , ai ; θi )
. (9)
∂θi
|
{z
}
B

From the gradient expression and its separation into terms
A and B, we can observe that the A term is the only factor
that couples the gradients of the branches and is a function of
all of the agents’ environment interaction data. The B term,
however, is the gradient of the ith branch, and computing it
only requires the parameters of that branch and its dedicated
agent’s environment interaction data.
In PE-VDN, the agents maintain and train their dedicated
branch of the VDN themselves. They compute their local
gradients by cooperating with other agents to compute the A
term in (9) together, which is the coupling term that accounts
for multi-agent coordination. We now show how the agents
can cooperate to compute the A term without sharing their
environment interaction data.
We require every agent i to compute the message

mi = γ max
(10)
Qi τi′ , a′ ; θi− − Qi (τi , ai ; θi )
′
a

and share it with all other agents. Each mi value can be
computed using the ith agent’s environment interaction data
and the current parameters of its local Qi function. If every
agent broadcasts its message mi to the other agents and
locally computes its B term, it can then update its Qi function
just as Vanilla VDN would have updated that agent’s dedicated branch. The resulting distributed computation scheme
eliminates the need for sharing environment interaction data
with a central node and satisfies the first requirement of our
technical privacy policy.
B. Privacy-Preserving Multi-Party Summation

Although the summation in the A term can be easily
computed by requiring every agent to broadcast their mi
messages, doing so may lead to unintentional information
leakage about the agents’ environment interaction data.
The mi values are functions of their corresponding agent’s
privacy-sensitive environment interaction data and correlate
with them. We now show how the agents can compute the
desired summation in term A in (9) while hiding their mi
values from one another.

We use a privacy-preserving multi-party computation technique called secret sharing [23] to compute the A term while
hiding the underlying mi values. Secret sharing refers to
the process of dividing a secret into n pieces such that the
observation of the pieces reveals no information about the
underlying secret unless a sufficient number of them are
available. Precisely, a (k, n)-secret sharing of s guarantees
that observers with access to up to k − 1 shares can learn no
information about s.
We use additive secret sharing which is an efficient (n, n)secret sharing scheme [24]. In particular, let s be the secret
value over a finite filed Zp := {0, 1, . . . , p − 1}. Then,
additive secret sharing splits the secret s ∈ Zp into n
shares S(s) = (r1 , r2 , . . . , rn ) such that r1 , . . . , rn−1 are
chosen
uniformly at random from Zp , and rn = p −
P
n−1
r
i=1 i mod p + s. Additive secret sharing ensures that
a complete set of n sharesPaccurately reconstruct the secret
via S −1 (r1 , . . . , rn ) := ( ni=1 ri ) mod p. Otherwise, for
any given set of shares with less than n shares, every element
of Zp is equally likely to generate those shares. Additive
secret sharing is additively homomorphic, i.e., for secrets
s1 , . . . , sm , all in Zp , we have that
!
m
m
X
X
−1
S(si ) ,
(11)
si = S
i=1

i=1

where the right-hand side of (11) is the element-wise summation of the secret shares.
The additive homomorphism of additive secret sharing
ensures the accuracy of the following three-step privacypreserving n-party summation protocol: first, each party
invokes additive secret sharing on its secret value and sends
each of the other agents a piece of the shares. Then, the
agents locally compute the sum of the shares that they
receive from other agents and broadcast the results. Finally,
the agents recover the summation of their secret values by
computing the sum of the broadcast values.
Returning to the problem of computing the A term in the
gradient expression in (9), the privacy-preserving summation
protocol based on additive secret sharing can be deployed
with one intermediate step. Additive secret sharing applies
to finite-field integers and we need to encode the mi values
in that format. We use the encoding

enc(x) = int 10PRECISION · x
mod p,
(12)
in which PRECISION denotes the number of decimal places
that the encoding preserves. To reverse the encoding back to
floating point, we use the decoding function
(
x/10PRECISION
if x ≤ p/2,
dec(x) =
(13)
PRECISION
(x − p)/10
if x > p/2.

To summarize, in order to compute the summation in the
A term, the agents send the secret shares S(enc(mi )) to one
another via peer-to-peer messaging. Then, each agent computes the summation of its received shares and broadcasts
the result. Finally, each agent reconstructs a copy of the A
term with the summation of the broadcast values. Integrating

additive secret sharing with the distributed computation of
the gradients ensures that inter-agent communications in the
decentralized training of PE-VDN do not undermine the
confidentiality of the agents’ environment interaction data.
C. Training with Differential Privacy
When the agents choose actions based on their neural
network’s action-value predictions, they may unintentionally
leak information about their environment interaction data as
well. It has been observed that the predictions of neural
networks could mirror specific relationships in the training
dataset that were not intended to be exposed [25]. For example, text-generative neural networks may complete certain
input prompts with memorized phrases within their training
datasets [26]. Similarly, the actions of deep reinforcement
learning agents—including independent training algorithms
in Co-MARL—may reveal certain sensitive characteristics of
their training experience as well [27].
A machine learning algorithm that trains a neural network
must map a training dataset to a set of parameters that
determine the neural network’s weights and biases. Under fixed hyperparameters and pseudo-randomization seeds,
training algorithms are deterministic mappings. These deterministic mappings may produce outputs that external
observers can accurately associate with certain training data.
Privacy-enhancing techniques based on differential privacy
can disrupt inference privacy threats against the training
datasets [25]. Differentially private training algorithms can
provably generate model parameters that are approximately
statistically indistinguishable from those trained with datasets
that differ in only one element. This guarantee establishes a
plausible deniability argument against the accuracy of privacy threats that base their attacks on the learned parameters
of a neural network or its predictions.
A formal definition of (ǫ, δ)-differential privacy is as follows: let (Ω, F , Pr) be a probability space and f : Ω × D 7→
R be a stochastic mapping that maps a dataset D ∈ D to its
image under f . The mapping f satisfies (ǫ, δ)-differential
privacy if, for all datasets D ∈ D and D′ ∈ D, whose
elements are identical except for one, and all subsets S ⊆ R,
Pr[f (D) ∈ S] ≤ exp(ǫ) · Pr[f (D′ ) ∈ S] + δ.

(14)

The DP-SGD algorithm [8] is a differentially private
supervised learning algorithm for neural networks and enforces differential privacy by repeatedly injecting calibrated
Gaussian noise into the gradients. The algorithm’s so-called
Moments Accountant subroutine tracks the differential privacy level—ǫ and δ—of the composition of the iterations
throughout the training.
As opposed to typical gradient descent algorithms that
draw fixed-length minibatches, DP-SGD uses Poisson sampling, which refers to a sampling method that chooses each
of the minibatch elements with a fixed probability. Moreover,
DP-SGD clips the gradients to a fixed threshold C. That is,
with gx denoting the gradient for sample x, DP-SGD uses
−1
the mapping ClipC (gx ) := gx · max (1, kgxk2 /C) .

Algorithm 1: PE-VDN Pseudocode
Input: Team of Agents, Environment E,
Hyperparameters H
Output: Trained Agents
1 total steps ← 0;
2 q ← expected batch size/buffer size;
3 while total steps < max steps do
4
RUN an episode and add the episode length to
total steps ;
5
for every agent i do
6
APPEND environment interaction data
hτi , ai , τi′ , ri to local replay buffer;
7
CHOOSE minibatch indices I using Poisson
sampling with sample rate q;
8
BROADCAST the indices to all agents;
9
for every agent i do
10
COMPUTE mi -values for all entries in I
using (10) ;
11
ENCODE mi -values using (12) ;
12
ENCRYPT the encodings using additive
secret sharing;
13
STORE the ith share locally;
14
DISTRIBUTE the remaining shares;
15
WAIT for all agents to distribute their shares;
16
COMPUTE the summation of the received
shares and the locally stored share;
17
BROADCAST the outcome;
18
WAIT for all agents to broadcast theirs;
19
DECRYPT the broadcast values by reversing
additive secret sharing;
20
DECODE the results using (13) to compute
term A in (9);
21
COMPUTE local gradients using (9);
22
AVERAGE gradients along the episode for
every minibatch sample;
23
CLIP gradients with threshold C;
24
AGGREGATE clipped gradients across
minibatch samples;
25
ADD zero-mean Gaussian Noise with
σ = noise multiplier · C to the
aggregated gradients;
26
UPDATE parameters using SGD with
probability 1/buffer throughput;

We integrate DP-SGD with our earlier decentralization
and the privacy-preserving multi-party summation protocol
contributions as follows: instead of performing Poisson sampling on a static labeled dataset as in supervised learning,
we perform Poisson sampling on the stream of environment
interaction data that gets loaded onto the agents’ replay
buffers. The integration of the DP-SGD concludes the design
of PE-VDN and Algorithm 1 describes its pseudocode.
The application of DP-SGD to the contents of the agents’
replay buffers makes the existing differential privacy analysis
of DP-SGD ill-suited to PE-VDN. Specifically, the differ-

ential privacy analysis of the DP-SGD algorithm via the
Moments Accountant method [8] only applies to the contents
of the replay buffer for a single iteration, not the agents’
entire collection of environment interaction data. As the
last remaining puzzle piece, we now state a theorem that
computes the differential privacy level of PE-VDN.
Theorem 1: Fix a Poisson sampling rate q, noise variance
σ 2 , and δ. Let (ǫ(T ), δ) be the differential privacy level
that the Moments Accountant method computes for DPSGD under q, σ 2 , and δ after T iterations. Then, with the
same parameters, PE-VDN is (ǫ̃, δ̃)−differentially private
with ǫ̃ = buffer throughput−1 ·ǫ(buffer size), and
δ̃ = buffer throughput−1 · δ.
Due to space limitations, we refer the reader to the
extended version of the paper1 for the full proof of the
Theorem. Intuitively, the proof leverages the property that
once an episode leaves the replay buffer, it no longer remains
part of the training. The proof formalizes this property in the
context of Maximum Overlap Composition Theorem in [10]
to compute the differential privacy of Algorithm 1.
V. N UMERICAL E XPERIMENTS
Based on the pseudocode in Algorithm 1, we implement
a Python library for PE-VDN2 . We use the StarCraft MultiAgent Competition (SMAC) suite [11], a machine learning application programming interface (API) for using CoMARL to learn how to play StarCraft II. Our experiments
take place in SMAC’s 3m environment in which three agents
must cooperate to kill three pre-trained enemies.
In the experiments, we dissect the different components
of PE-VDN to empirically evaluate their effects on team
rewards. First, we consider the decentralization of Vanilla
VDN’s training. We do not expect that decentralization
will affect performance because the distributed computation
scheme in Section IV-A (PE-VDN A) computes the same
gradients as Vanilla VDN. We also do not anticipate any
significant performance drop due to the privacy-preserving
multi-party summation protocol (PE-VDN B) because the
only source of inaccuracy that it introduces is the encoding
function’s quantization error.
We periodically evaluate the agents’ win rate during
training and plot the results in Figure 1. The win rates
confirm that Vanilla VDN, PE-VDN A, and PE-VDN B with
PRECISION = 5 all perform similarly. Moreover, recall
that the technical privacy policy does not allow any data
sharing that does not benefit the agents in terms of the
team rewards. The superior performance of Vanilla VDN,
PE-VDN A, and PE-VDN B compared with Independent QLearning in Figure 1a indicates that the agents indeed benefit
from cooperation. Although, Vanilla VDN violates the first
prong of the technical privacy policy.
In the next experiment, we evaluate the effects of enforcing
differential privacy via the DP-SGD algorithm. DP-SGD’s
injection of noise into the gradients may affect performance
1 See here.
2 All codes and instructions are provided here.

negatively. In the supervised learning setup where DP-SGD
has been primarily studied, it has been reported that the
performance of the models trained with DP-SGD is more
sensitive to the choice of training hyperparameters than nondifferentially private counterparts [12]. In particular, hyperparameter tuning may play a key role in model accuracy,
training stability, and sample complexity [12]. Clipping gradients may add bias, perturbing the gradients may deflect the
parameters away from local minima and destabilize training,
and achieving meaningful ǫ and δ values—ǫ < 3 and
δ < 1/n1.1 , where n is the training dataset’s size [12]—may
require large training datasets.
In our implementations, we use the Opacus [28] toolbox which is an open-source library for DP-SGD. We use
Opacus’ built-in Moments Accountant method to track PEVDN’s differential privacy levels as stated in Theorem 1.
Without differential privacy, we used the Adam optimizer
with a learning rate of 5 · 10−4 . In light of the guidelines in
[12], we found that using the SGD optimizer with weight decay 0.01, momentum 0.9, and a significantly higher learning
rate 5 · 10−3 performs better for DP-SGD. Moreover, we use
an anchoring technique to improve the training’s stability further. In this technique, once the periodic win-rate evaluation
returns rates that are beyond a specific threshold, we save the
parameters as an anchor model. Then we repeatedly increase
the threshold and penalize the rest of the training with the
current parameters’ distance from the anchor. With these
techniques, Figure 1b shows the performance of PE-VDN
under (2.90, 4.9 · 10−4 )-differential privacy and benchmarks
it with PE-VDN B with Poisson sampling.
VI. CONCLUSIONS & FUTURE WORK
In this work, we proposed the PE-VDN algorithm which
incorporates three privacy-engineering techniques to protect
the confidentiality of the agents’ environment interaction data
in Vanilla VDN. In particular, we re-engineered the data
flows of Vanilla VDN to achieve the following: decentralized
training without compromising multi-agent coordination,
privacy-preserving inter-agent communication and computation, and differentially private neural network training.
For future work, we aim to develop similar decentralization and privacy-preserving communication schemes for
more sophisticated centralized training and decentralized
execution algorithms such as QMIX [29] and QTRAN [30].
These algorithms improve upon the performance of the VDN
algorithm by relaxing the additivity assumption in (6). In
particular, the QTRAN algorithm only requires decentralizability as stated in (5).
R EFERENCES
[1] Y. Yang and J. Wang, “An overview of multi-agent reinforcement
learning from game theoretical perspective,” 2020.
[2] S. Aradi, “Survey of deep reinforcement learning for motion planning
of autonomous vehicles,” IEEE Transactions on Intelligent Transportation Systems, 2022.
[3] J. A. Fax and R. M. Murray, “Information flow and cooperative control
of vehicle formations,” IEEE Transactions on Automatic Control,
2004.

Win Rate

IQL

Vanilla VDN

PE-VDN A

PE-VDN B

PE-VDN B

1

1

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

PE-VDN C (Anchor)

PE-VDN C

2

4
·105

0
0

1

2
Total Environment Steps
(a)

3

4
·105

0

1

Total Environment Steps
(b)

3

Fig. 1. Comparison of the win rate of different algorithms in SMAC’s 3m environment. The left plot compares win rates of Vanilla VDN, Independent
Q-Learning (IQL), PE-VDN with only decentralized training (PE-VDN A), and PE-VDN with decentralized training and privacy-preserving multi-party
summation (PE-VDN B) algorithms. The right plot demonstrates the win rate of PE-VDN with differential privacy (PE-VDN C). The solid line represents
the anchor model’s win rate and the volatile dashed line is the win rate of the running model.

[4] Y. Li, X. Tao, X. Zhang, J. Liu, and J. Xu, “Privacy-preserved federated
learning for autonomous driving,” IEEE Transactions on Intelligent
Transportation Systems, 2022.
[5] P. K. Sharma, R. Fernandez, E. G. Zaroukian, M. R. Dorothy,
A. Basak, and D. E. Asher, “Survey of recent multi-agent reinforcement learning algorithms utilizing centralized training,” in Artificial
intelligence and machine learning for multi-domain operations applications III, SPIE, 2021.
[6] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and
T. Graepel, “Value-decomposition networks for cooperative multiagent learning based on team reward,” in Proceedings of the 17th
International Conference on Autonomous Agents and MultiAgent Systems, AAMAS, International Foundation for Autonomous Agents and
Multiagent Systems / ACM, 2018.
[7] X. Zhang, C. Chen, Y. Xie, X. Chen, J. Zhang, and Y. Xiang, “A
survey on privacy inference attacks and defenses in cloud-based deep
neural network,” Computer Standards & Interfaces, 2023.
[8] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, pp. 308–318, ACM, 2016.
[9] T. Ha, T. K. Dang, T. T. Dang, T. A. Truong, and M. T. Nguyen, “Differential privacy in deep learning: An overview,” in 2019 International
Conference on Advanced Computing and Applications, ACOMP, IEEE
Computer Society, 2019.
[10] J. Smith, H. J. Asghar, G. Gioiosa, S. Mrabet, S. Gaspers, and P. Tyler,
“Making the most of parallel composition in differential privacy,”
Proceedings of Privacy Enhancing Technologies, 2022.
[11] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli,
T. G. J. Rudner, C. Hung, P. H. S. Torr, J. N. Foerster, and S. Whiteson,
“The StarCraft Multi-Agent Challenge,” 2019.
[12] N. Ponomareva, H. Hazimeh, A. Kurakin, Z. Xu, C. Denison, H. B.
McMahan, S. Vassilvitskii, S. Chien, and A. Thakurta, “How to dp-fy
ML: A practical guide to machine learning with differential privacy,”
2023.
[13] S. Guicheng and W. Yang, “Review on Dec-POMDP model for MARL
algorithms,” in Smart Communications, Intelligent Algorithms and
Interactive Methods: Proceedings of 4th International Conference on
Wireless Communications and Applications ICWCA, Springer, 2022.
[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski,
S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through
deep reinforcement learning,” Nature, 2015.
[15] G. Dao and M. Lee, “Relevant experiences in replay buffer,” in IEEE
Symposium Series on Computational Intelligence, SSCI, IEEE, 2019.
[16] H. Nissenbaum, “Contextual integrity up and down the data food
chain,” Theoretical Inquiries in Law, vol. 20, no. 1, pp. 221–256,
2019.

[17] C. Ma, J. L. K. Wei, B. Liu, M. Ding, L. Yuan, Z. Han, and H. V.
Poor, “Trusted AI in multi-agent systems: An overview of privacy and
security for distributed learning,” 2022.
[18] B. Wang and N. Hegde, “Privacy-preserving q-learning with functional
noise in continuous spaces,” in Advances in Neural Information
Processing Systems 32: Annual Conference on Neural Information
Processing Systems, 2019.
[19] G. Vietri, B. Balle, A. Krishnamurthy, and Z. S. Wu, “Private reinforcement learning with PAC and regret guarantees,” in Proceedings
of the 37th International Conference on Machine Learning, ICML,
Proceedings of Machine Learning Research, PMLR, 2020.
[20] X. Zhou, “Differentially private reinforcement learning with linear
function approximation,” Proceedings of the ACM on Measurement
and Analysis of Computing Systems, 2022.
[21] C. Qu, S. Mannor, H. Xu, Y. Qi, L. Song, and J. Xiong, “Value propagation for decentralized networked deep multi-agent reinforcement
learning,” in Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems, 2019.
[22] Y. Ye, Y. Tang, H. Wang, X. Zhang, and G. Strbac, “A scalable privacypreserving multi-agent deep reinforcement learning approach for largescale peer-to-peer transactive energy trading,” IEEE Transactions on
Smart Grid, 2021.
[23] A. Shamir, “How to share a secret,” Communications of the ACM,
1979.
[24] L. Xiong, W. Zhou, Z. Xia, Q. Gu, and J. Weng, “Efficient privacypreserving computation based on additive secret sharing,” 2020.
[25] C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li, and Y. Gao, “A survey on
federated learning,” Knowledge-Based Systems, 2021.
[26] B. Jayaraman, E. Ghosh, H. A. Inan, M. Chase, S. Roy, and W. Dai,
“Active data pattern extraction attacks on generative language models,”
2022.
[27] X. Pan, W. Wang, X. Zhang, B. Li, J. Yi, and D. Song, “How you
act tells a lot: Privacy-leaking attack on deep reinforcement learning,”
in Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, AAMAS, International Foundation for
Autonomous Agents and Multiagent Systems, 2019.
[28] A. Yousefpour, I. Shilov, A. Sablayrolles, D. Testuggine, K. Prasad,
M. Malek, J. Nguyen, S. Ghosh, A. Bharadwaj, J. Zhao, G. Cormode,
and I. Mironov, “Opacus: User-friendly differential privacy library in
pytorch,” 2021.
[29] T. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster,
and S. Whiteson, “Monotonic value function factorisation for deep
multi-agent reinforcement learning,” Journal of Machine Learning
Research, 2020.
[30] K. Son, D. Kim, W. J. Kang, D. Hostallero, and Y. Yi, “QTRAN:
learning to factorize with transformation for cooperative multi-agent
reinforcement learning,” in Proceedings of the 36th International
Conference on Machine Learning, ICML, Proceedings of Machine
Learning Research, PMLR, 2019.

