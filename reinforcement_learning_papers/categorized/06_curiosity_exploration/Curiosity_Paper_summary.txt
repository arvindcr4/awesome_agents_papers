Here is the extracted information:

**PAPER**: Curiosity-driven Exploration by Self-supervised Prediction
**TITLE**: Curiosity-driven Exploration by Self-supervised Prediction
**RESEARCH_METHOD**: 06_curiosity_exploration
**METHOD_DESCRIPTION**: This paper proposes a curiosity-driven exploration method that uses self-supervised prediction to learn a feature space that is relevant for predicting the agent's actions. The method consists of two neural networks: an inverse dynamics model that predicts the agent's action given its current and next states, and a forward dynamics model that predicts the feature representation of the next state given the current state and action. The prediction error of the forward model is used as an intrinsic reward signal to encourage the agent to explore its environment.
**KEY_CONTRIBUTIONS**:
* The proposed method scales to high-dimensional continuous state spaces like images and bypasses the difficulties of directly predicting pixels.
* The method ignores the aspects of the environment that cannot affect the agent, making it robust to uncontrollable factors.
* The approach enables an agent to learn generalizable skills even in the absence of an explicit goal.
* The method is evaluated in two environments: VizDoom and Super Mario Bros, and outperforms baseline methods in terms of exploration efficiency and generalization to novel scenarios.
