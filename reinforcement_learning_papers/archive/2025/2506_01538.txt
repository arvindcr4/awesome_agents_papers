IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MAY, 2025

1

LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for
Cooperative Policy Generation

arXiv:2506.01538v2 [cs.RO] 3 Jun 2025

Guobin Zhu1,2 , Rui Zhou1 , Wenkang Ji2 , and Shiyu Zhao2

Abstract—Although Multi-Agent Reinforcement Learning
(MARL) is effective for complex multi-robot tasks, it suffers
from low sample efficiency and requires iterative manual reward
tuning. Large Language Models (LLMs) have shown promise
in single-robot settings, but their application in multi-robot
systems remains largely unexplored. This paper introduces a
novel LLM-Aided MARL (LAMARL) approach, which integrates
MARL with LLMs, significantly enhancing sample efficiency
without requiring manual design. LAMARL consists of two
modules: the first module leverages LLMs to fully automate
the generation of prior policy and reward functions. The second
module is MARL, which uses the generated functions to guide
robot policy training effectively. On a shape assembly benchmark,
both simulation and real-world experiments demonstrate the
unique advantages of LAMARL. Ablation studies show that the
prior policy improves sample efficiency by an average of 185.9%
and enhances task completion, while structured prompts based
on Chain-of-Thought (CoT) and basic APIs improve LLM output
success rates by 28.5%-67.5%. Videos and code are available at
https://windylab.github.io/LAMARL/
Index Terms—Multi-robot systems, Shape assembly, Multi-agent
reinforcement learning, Large language models.

(a) Real-world robots

(b) LAMARL (Ours)

Good uniformity

(c) Mean-shift

Worse uniformity

(d) MDR

(e) AIRL

Poor uniformity

I. I NTRODUCTION

M

ULTI-robot systems, through the coordination of multiple simple agents, can achieve a variety of complex
R (t = 25 s)
O (t = 50 s)
B (t = 75 s)
O (t = 100 s)
T (t = 125 s)
tasks, such as collaborative transportation [1], [2], formation
control [3]–[5], and shape assembly [6]. Under distributed Fig. 1: Real-world experimental snapshots. (a) shows eight robots arranged in
straight line. (b)-(e) illustrate the performance of four methods in assembling
control, they also exhibit flexibility, scalability, and robustness arobots
into different shapes. Green/red suns indicate satisfactory/unsatisfactory
[1]. Hence, tremendous endeavors have been made to establish uniformity.
such systems. However, it is challenging to design an efficient
multi-robot system without specialized expertise.
Traditional methods have mainly relied on control-theorystrategy is automatically optimized during training without
based designs [6]–[8], which involve precise modeling of
requiring precise models of the environment or robots, relying
the environment and robots, followed by careful design of
instead on an appropriate reward mechanism, leading to
control strategies to achieve optimal performance. However,
widespread attention on MARL. With MARL, the challenge of
these approaches are often task-specific, inflexible, and require
designing complex control algorithms shifts to that of designing
specialized expertise, resulting in high labor and time costs [9].
appropriate reward functions [12]. However, manually designThe development of multi-agent reinforcement learning
ing reward functions for multi-robot systems is nontrivial, as
(MARL) has recently opened new opportunities for multiit requires a deep understanding of the task logic and often
robot system research [9]–[11]. In MARL, each robot’s control
requires iterative adjustments. Additionally, since policies are
optimized through trial and error, MARL typically suffers from
Manuscript received: January, 26, 2025; Revised March, 28, 2025; Accepted
May, 29, 2025. This paper was recommended for publication by Editor Kober, low sample efficiency.
Jens upon evaluation of the Associate Editor and Reviewers’ comments. This
Therefore, researchers in the MARL field are increasingly
work was supported by the STI 2030-Major Projects (No.2022ZD0208804),
National Natural Science Foundation of China (No.62473017). (Corresponding focused on automating the design of rewards. Currently, apauthor: Shiyu Zhao.)
proaches to this issue can be classified into three types. The first
1 School of Automation Science and Electrical Engineering, Beihang
type is exploration-based methods (e.g., count-based, curiosityUniversity, Beijing, China.
2 WINDY Lab, Department of Artificial Intelligence, Westlake University,
based) [13]–[15], which encourage agents to explore unseen
Hangzhou, China.
states for more feedback. However, continuous exploration
E-mail: {zhugb, zhr}@buaa.edu.cn, {jiwenkang, zhaoshiyu}@westlake.
may hinder training convergence [13]. The second type is
edu.cn.
Digital Object Identifier (DOI): see top of this page.
agent-based methods [16], [17], which automatically learn a

2

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MAY, 2025

parametric reward by optimizing both the policy and reward
simultaneously using RL algorithms, thus avoiding manual
design. However, reward optimization is not a standard Markov
decision process and may not guarantee the discovery of an
ideal reward. The third type is expert-based methods [18]–[20],
which derive rewards from expert knowledge or data. Inverse
reinforcement learning (IRL) [19], [20] is a representative
method, which can recover a reward from expert data without
manual design. However, it requires a large amount of expert
data to obtain an effective reward.
The rise of large language models (LLMs) has introduced
a new paradigm for multi-robot system design [21]–[23].
Leveraging their extensive knowledge, LLMs have been applied
to various aspects of robotics, such as perception and decisionmaking [21]. However, most applications remain focused on
single-robot scenarios, with multi-robot collaboration largely
unexplored. In single-robot scenarios, LLMs are primarily
applied in two ways. The first involves deploying the LLM
directly on the robot for online perception and decision-making
[24]. This approach requires significant computational resources
[24], [25], while multi-robot systems often rely on simple
agents with very limited resources. Moreover, the probabilistic
nature of LLMs makes the reliability and repeatability of this
approach difficult to guarantee [26]. The second way involves
generating code offline, as demonstrated in works like Code
as Policies [27], where robots execute code-based policies to
perform tasks. This approach is computationally efficient and
offers good reproducibility. However, it is limited to simpler
tasks, and its success is not always guaranteed due to issues
such as the “hallucination” problem with LLMs [28], [29].
Several recent works have integrated LLM’s knowledge
capabilities with RL to enhance its efficiency. For example,
LLMs have been used to generate policy functions, reducing
exploration in RL and assisting robots in task completion [30].
LLMs have also been applied to generate reward functions
[31]–[33], guiding robots to complete tasks without manual
design. However, these approaches are limited to single-robot
scenarios and typically handle only simple tasks. In contrast
to single-robot systems, multi-robot systems involve agents
with local perspectives and require coordination with neighbors.
Given the complexity of such tasks, LLMs lack task-specific
knowledge and have limited reasoning capabilities, making
them prone to issues like hallucination [28], [34].
To overcome the challenges outlined above, this paper
presents a novel LLM-aided MARL approach called LAMARL
that can design cooperative policies for complex multi-robot
systems autonomously. This approach can preserve MARL’s
ability to tackle complex tasks while leveraging LLMs’ knowledge capabilities. The technical contributions are as follows. 1)
We design an LLM-aided function generation module capable
of outputting prior policy and reward functions. This process
involves user instruction input, constraint analysis, function
generation, and function review, with each step being fully
automated by LLMs. It eliminates the need for manual intervention and greatly enhances design efficiency. 2) In the MARL
module, the LLM-generated prior policy is incorporated into
the actor loss to ensure robots possess the fundamental ability
to complete tasks. Meanwhile, the LLM-generated reward is

integrated into the MARL environment to guide the training
of robot policies effectively. 3) To evaluate the efficiency of
LAMARL, we evaluated it on the shape assembly task as shown
in Fig. 1. Such a task is a long-standing, challenging benchmark
in the research community [6]. Comparative experiments in
simulation and real-world experiments show that LAMARL
achieves performance comparable to optimal solutions without
manual design or expert data. Ablation studies further reveal
that the prior policy improves sample efficiency by an average
of 185.9%, supports LLM-generated rewards in complex tasks,
and that structured prompts using APIs and Chain-of-Thought
(CoT) [35] enhance LLM success rates by 28.5%-67.5%.
To the best of our knowledge, this is the first work that
combines LLMs with MARL to achieve fully autonomous
policy generation for multi-robot tasks.
II. M ETHODOLOGY
The method framework, as shown in Fig. 2, consists of
two modules. The first is the LLM-aided function generation
module, which autonomously generates the prior policy and
reward function using LLMs. The second is the MARL module,
where the LLM-generated functions guide the training of robot
policies.
A. LLM-Aided Function Generation
The LLM-aided function generation module consists of four
steps. The first step is user instruction input, which consists
of a task description and auxiliary prompt. Each component is
expressed in natural language. The task description outlines the
user’s desired objectives. Auxiliary prompt provides additional
context to help the LLM understand and solve the task,
including CoT and basic APIs. CoT refers to a series of
logically connected questions that guide the LLM to approach
the task step by step. Basic APIs are pre-implemented functions
that can be directly utilized. The LLM can extract deterministic
information from them. Ablation experiments will show that
CoT and basic APIs significantly improve function generation
success rates.
The second step is constraint analysis and processing, during
which the LLM addresses the questions outlined in CoT. First,
the LLM analyzes the constraints that the robot may need to satisfy to complete the task. These constraints are then categorized
into basic and complex constraints. Basic constraints are simple
conditions with fewer steps and simpler implementation, like
collision avoidance, while complex constraints involve more
steps and handle intricate conditions, such as exploration. Next,
the LLM identifies the basic skills the robot should possess
based on the basic constraints. Additionally, the LLM identifies
the key task sub-goals that must be achieved to successfully
accomplish the task based on the basic and complex constraints.
The third step is function generation, including the policy
and reward functions, both of which are written in Python code.
At this stage, the LLM generates the policy function based on
the basic skills identified in the previous step. For each basic
skill, an action is created, and the output of the prior policy is
the combination of all actions. This enables the policy function
to execute all basic skills. Simultaneously, the LLM designs

ZHU et al.: LAMARL: LLM-AIDED MULTI-AGENT REINFORCEMENT LEARNING FOR COOPERATIVE POLICY GENERATION

(a) LLM-aided function generation

(b) MARL

User instruction

Task
description

Tuning

+

Basic APIs

Complex
constraints

Skill analysis via
LLM
Sub-goal analysis
via LLM

Prior
action

LAMARL

Chain-of-Thought

2
…

Constraint analysis via LLM

Basic
constraints

Samples

Replay buffer

Auxiliary prompt

1

3

Training
Observation

Robot

Prior policy

RL policy

RL action
Generated prior policy

Basic skills
Key task
sub-goals

Function
generation via
LLM

Environment

Prior policy
function
Reward function

Function
review via
LLM/human

Pass
Not pass

Generated
reward
Generated
policy and
reward
function

Fig. 2: Method overview. (a) describes the process of using LLMs to generate the prior policy and reward functions, including user instruction input, constraint
analysis, function generation, and function review. (b) describes how the prior policy and reward are integrated into MARL.

the reward function based on the identified key sub-goals. Each
sub-goal corresponds to a condition, and when the condition is
met, it is evaluated as true. If all conditions are true, the task
is considered complete, and a reward value of 1 is returned;
otherwise, the reward is 0. During this step, basic APIs are
crucial for helping the LLM understand complex tasks and
ensuring output accuracy.
The fourth step is function review. First, the LLM verifies whether the policy function implements the basic skills
identified in Step 2. Second, the LLM checks whether the
reward function evaluates all key sub-goals. If both functions
pass, they are integrated into the MARL module. If not, the
LLM highlights the issues and requests user adjustments.
Alternatively, this step can be performed via humans.
B. MARL

III. TASK S TATEMENT
In the following, we apply LAMARL to the shape assembly
task, which also serves to demonstrate the function generation
procedure. As shown in Fig. 3(c), this task requires a group
of robots to assemble into a specified shape while maintaining
equal spacing, avoiding collisions, and ensuring a uniform
formation. As a longstanding benchmark in cooperative control,
it presents three key challenges: first, the robots can only obtain
information about the local area and neighbors; second, the
robots are homogeneous, and there is no goal assignment;
third, there is a global constraint that makes distributed control
difficult to implement. The state-of-the-art approach achieves
efficient control performance through labor-intensive design and
fine-tuning [6]. However, this approach requires the total area
occupied by the robots to match the target region, necessitating
parameter adjustments for different region sizes and limiting
adaptability.
We now address this task using LAMARL. As it involves
RL, we will also describe the environment, along with the
action and observation vectors design.

The MARL module consists of two parts. The first part
involves integrating the LLM-generated prior policy into MARL
algorithms. Using Multi-Agent Deep Deterministic Policy
Gradient (MADDPG) algorithm as an example, the actions
generated by the prior policy are stored in the buffer for
updating the actor. Specifically, the original actor objective A. Environment Description
is to maximize Q, where Q is the critic, but in LAMARL,
Region Description: The target region has a connected shape
it is modified to maximize Q − α(a − aprior )2 , where α is (see Fig. 3(a)) and is discretized into a grid composed of ncell
the weight, a is the RL action and aprior is the prior action. cells. The side length of each cell is denoted as lcell , and the
This regularization term ensures the RL policy mimics the center of each cell represents its position.
prior policy and thereby equips the robots with basic task
Robot Description: The robot set is denoted as A = {1,
skills. Although MADDPG is used as an example, LAMARL . . . , nrobot }. Each robot is represented as a disk, and its state is
is compatible with other policy-based algorithms as well.
x = [pT , vT ]T , where, p and v are the position and velocity,
The second part involves integrating the LLM-generated respectively. The robot’s movement is driven by the active and
reward into the MARL module. This is done by incorporating passive forces (see Fig. 3(b)). The active force, fa , is a selfthe reward into the MARL environment without modifying the generated force, which is the output of the actor network. The
underlying MARL algorithm. While the prior policy equips passive force, fb , is an elastic force following Hooke’s Law.
the robots with basic task-solving abilities, it is insufficient for Thus, the robot’s dynamics is ṗi = vi , v̇i = (fa + fb )/mi , i ∈
completing complex tasks. It is the addition of the reward that A, where mi is the mass of robot i.
addresses this limitation. The reward function checks if all key
Each robot has a sensing radius, rsense , within which it
subgoals are met, facilitating the training of a task-completing can perceive neighbors and cells. When a robot perceives a
policy.
neighbor, it can obtain the neighbor’s state; when it perceives

Ap

4

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MAY, 2025

(a) Region description

rsense
robotj

The target cell position of robotj

pt

Sensed region

pcell

ravoid

The target cell position of robotk
(b) Robot description

p
v

Passive force fb

1

ravoid

Homogeneous robot swarm

Neighbor2

Collision

Part 1
(1 x 4)

2

Part 2
(1 x 4nhn)

3

Part 3
(1 x 2)

4

Part 4
(1 x 2nhc)

Part 2 form with nhn – 1 neighbors:

v
fb

Occupied cells

Observation form:

Neighbors
robotk

roboti

No collision

Enter

rsense

rsense

*
0
0
0
0

Active force fa

v

Assemble

pt

Neighbor3
Unoccupied/
Observed
cells

Approximately
equal regions

Occupied cells

Cells

Neighbor1

(c) Expected state

roboti

lcell

Local perspective of roboti

(d) Observation vector design

Inside the shape

Zero
padding

Fig. 3: Environment description. (a) shows the region details, (b) shows the robot swarm setup, (c) depicts the desired assembly state, and (d) represents the
observation vector design.

a cell, it can obtain the cell’s position. Each robot also has
a collision radius, ravoid . A collision occurs if the inter-robot
distance is less than 2ravoid , and a cell is considered occupied
by a robot if the robot-to-cell distance is less than ravoid .

to be satisfied to complete the task: “ Entering the target
region ”, “ Collision avoidance ”, “ Synchronization
with neighbors ”, and “ Exploration of unoccupied
cells ”, answering the first question. It then categorizes the
first three as basic constraints and the last one as a complex
constraint, thus answering the second question. Next, the LLM
B. Action and Observation
infers that the robot’s basic skills should include “ Movement
Action: Each robot’s action is a two-dimensional vector with towards the target region ”, “ Collision avoidance ”,
components along the x and y axes. This vector indicates the and “ Synchronization with
active force fa exerted on the robot.
neighbors ”, answering the third question. Simultaneously,
Observation: Based on Section III-A, we design the obser- the LLM refines all constraints, identifying constraints 1, 2,
vation vector to consist of four parts (see Fig. 3(d)). The first and 4 as the three essential sub-goals for task completion,
part is the robot’s own state, the second is the relative state of answering the fourth question. Under the CoT’s guidance,
its neighbors, the third is the relative position of the target cell, the LLM progressively analyzes the task, and the results are
and the fourth is the relative positions of unoccupied/observed intuitively correct.
cells within rsense . The maximum number of neighbors and
Based on these basic skills and key sub-goals, the LLM
observed cells are denoted as nhn and nhc , respectively. designs the prior policy and reward function as shown in
Hence, each robot’s observation is a (6 + 4nhn + 2nhc )- Fig. 4(c)(d). For instance, the LLM-generated prior policy
dimensional vector. For example, the robot i’s observation includes three forces: an attraction force for entering a region,
is oi = [xTi , xTji,1 , . . . , xTji,nhn , pTti , pTki,1 . . . , pTki,nhc ]T , where a repulsion force between neighbors, and a synchronization
xji = xj −xi , j ∈ Ni , pki = pk −pi , k ∈ Ci , Ni /Ci is the sets force for fewer oscillations. The output of the policy is the
of neighbors/observed cells of robot i. pti = pt − pi , where combination of these three forces. Similarly, the reward function
pt is the position of the target cell.
determines whether the robot has entered the region, avoided
Note that if the number of neighbors or observed cells collisions with neighbors, and completed the exploration. The
within rsense is less than nhn or nhc , respectively, the remaining task is considered complete only when all three conditions
elements of the observation vector are padded with zeros (see are met, at which point a reward value of 1 is returned.
Fig. 3(d)). If the number of neighbors exceeds nhn , only the nhn Both functions passed the review and were deemed correct.
nearest neighbors are considered. If the number of observed Therefore, we will apply them to MARL.
cells exceeds nhc , nhc cells are selected randomly from the set
of observed cells. These adjustments ensure a fixed observation
D. Algorithm and Training
vector dimensionality.
We adopt the MADDPG algorithm for the shape assembly
task due to its support for continuous action spaces and its
C. Function Generation
off-policy nature, which ensures high sample efficiency and
The inputs and outputs of the LLM-aided function generation makes it well-suited for complex tasks. MADDPG employs an
module are illustrated in Fig. 4. The user instruction is shown actor-critic architecture [36]. In this paper, both the actor and
in Fig. 4(a). Based on the user instruction, the LLM analyzes critic are MLPs, with Leaky-ReLU in hidden layers, Tanh for
the task and determines that the following constraints may need the actor’s output, and no activation for the critic’s output.

ZHU et al.: LAMARL: LLM-AIDED MULTI-AGENT REINFORCEMENT LEARNING FOR COOPERATIVE POLICY GENERATION

(a)

Task description
The robot swarm need
to assemble into a
specific shape that
aligns with the shape
of the region. In such
a shape, robots should
avoid collisions,
synchronize movements
with neighbors to
reduce oscillations,
and attempt to explore
unoccupied cells.

Chain of Thought

Consider what constraints need to be satisfied to complete the task.
Consider what constitutes basic constraints and what constitutes
complex constraints.
Consider what basic skills the robots should possess.
Consider what task sub-goals must be included in the reward function.

Basic APIs

Skill analysis

get_neighbor_id_list
get_robot_position_and_velocity
get_unoccupied_cells_position
get_target_cell_position
is_within_target_region

Prior policy

def robot_policy(robot_id):
function generation
……
(c) # Calculate attraction force toward the target
target_position = get_target_cell_position(robot_id)
……
if distance_to_target > 0:
target_force = attraction_strength * direction_to_target / distance_to_target
……
# Compute repulsion force
Basic skill 1
……
neighbors = get_neighbor_id(robot_id)
for neighbor_id in neighbors:
neighbor_position, neighbor_velocity = get_position_and_velocity(neighbor_id)
……
if 0 < distance_to_neighbor < repulsion_range:
repulsion_force = repulsion_strength * direction_to_neighbor /
(distance_to_neighbor ** 2)
total_force += repulsion_force
Basic skill 2
# Compute synchronization force
Basic skill 3
if neighbor_velocities:
……
sync_force = sync_strength * (average_velocity - velocity)
total_force += sync_force
……
return total_force

(b)
Constraint
analysis

5

Basic constraints

1 Entering the target region
2 Collision avoidance
3 Synchronization with neighbors

Complex constraints
4 Exploration of unoccupied cells

Sub-goal analysis

Basic skills

Key task sub-goals

Movement towards the target region
Collision avoidance
Synchronization with neighbors

Entering the target region
Collision avoidance
Exploration of unoccupied cells

Reward function

def compute_reward(robot_ids):
……
generation
for idx, robot_id in enumerate(robot_ids):
(d)
# Check if the robot is within the target region
in_target = is_within_target_region(robot_id)
if not in_target:
Key task sub-goal 1
continue # Reward remains 0

Function
review
Generated
functions

# Check for collisions with neighbors
for neighbor_id in neighbors:
……
if distance < (2 * collision_radius):
collision = True
break
if collision:
Key task sub-goal 2
continue # Reward remains 0
# Check if the robot has explored unoccupied areas
unoccupied_cells = get_unoccupied_cells_position(robot_id)
centroid = np.mean(unoccupied_cells, axis=0)Key task sub-goal 3
……
is_explored = distance_to_centroid < threshold
if is_explored:
rewards[0, idx] = 1 # Task completed for this robot
return rewards

Fig. 4: The workflow of the shape assembly task. (a) represents the user instruction, (b) represents constraint analysis and processing, and (c) and (d) represents
the LLM-generated policy and reward function, respectively.

Besides modifying the actor loss as described in Section
II-B, we also need to adjust the critic input. In the original
MADDPG, the critic is represented as Q(x1 , . . . , xnrobot , a1 ,
. . . , anrobot ), where ai = µ(oi ) and µ is the actor. The critic
input includes global information. We revised it to Q(oi , ai )
so that the critic only receives the observation and action of the
individual robot. Such a modification is necessary to support
large-scale shape assembly tasks.
IV. S IMULATION E XPERIMENTS
This section builds on the shape assembly task to examine
two main aspects: a comparative experiment to highlight the
advantages of LAMARL over several representative methods,
and an ablation study to evaluate the importance of the prior
policy and structured prompts. We begin by defining two
metrics to assess each method’s performance.
A. Evaluation Metrics
Coverage rate (M1 ) is defined as noccupied /ncell , where
noccupied is the total number of cells occupied by the robots. A
higher M1 value indicates greater area coverage by the robot
swarm.
P
2
Uniformity (M2 ) is defined as
i∈A (nv,i − nv ) /nrobot ,
where nv,i is the number of cells in the Voronoi region of
robot i, and nv is the average value of {nv,1 , . . . , nv,nrobot }. A
smaller M2 value indicates a more uniform distribution of the
robot swarm.
B. Comparison Experiments
Algorithm setup: We compare the LAMARL method with
three representative approaches. The first approach is the

traditional control method (Mean-shift) [6], which includes
three velocity rules: entry velocity vent , exploration velocity
vexp , and interaction velocity vint . Each rule is carefully
designed by hand. The Mean-shift method currently achieves
the most optimal assembly behavior, with high values of M1
and M2 . For algorithm details, see [6].
The second is RL with manually designed rewards (MDR).
This method uses the same action and observation forms
as the LAMARL scheme, but the reward is manually specified. Inspired by the Mean-shift algorithm, the reward design considers three conditions: 1) the robot is within the
shape,
P 2) the robot
P avoids collisions with neighbors, and 3)
| k∈Ci ρk pk / k∈Ci ρk −pi | ≤ δ, i ∈ A, where ρk = 0.5(1+
−pi∥
cos π ∥prksense
), and δ = 0.05. This third condition encourages
the exploration of unoccupied areas. If all conditions are met,
the reward is 1; otherwise, it is 0.
The third approach is IRL, which does not require manual
reward design but relies on expert data. We use the Meanshift method as the expert policy and collect 1.5 × 106 data
points of (o, a) over time to form an expert buffer. The classic
AIRL algorithm [19] is then applied to recover the reward.
AIRL consists of two loops: an inner loop for RL and an outer
loop for the discriminator, which outputs the reward. While the
discriminator recovers the reward, the RL loop trains the robot’s
policy to convergence. The discriminator’s hyperparameters
are set as follows: hidden dim = 180, number of hidden layers
= 4, lr-discriminator = 1e-3, batch size (expert buffer) = 3072.
For algorithm details, see [19].
Note that the Mean-shift approach does not require training,
whereas both MDR and AIRL involve RL training. The RL
algorithms for MDR and AIRL are identical to the MADDPG

6

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MAY, 2025

Minimum speed

Maximum speed

(a) LAMARL(Ours)

No collision

Good uniformity

(b) Mean-shift

v (m / s)

(c) MDR
Collisions

(d) AIRL

Initial position (t = 0 s)

Poor uniformity

Collisions

R (t = 30 s)

L (t = 60 s)

(t = 90 s)

R (t = 120 s)

Poor uniformity

O (t = 150 s)

B (t = 180 s)

O (t = 210 s)

T (t = 240 s)

Fig. 5: Comparison of shape assembly performance. A group of 30 robots start from the initial position and sequentially assemble different shapes. The curves
represent the motion trajectories, with the curve colors indicating the robots’ velocities. The first green/red suns indicate without/with collision, and the second
green/red suns indicate satisfactory/unsatisfactory uniformity.

used in LAMARL, except that the actor lacks a regularization
term. The MADDPG hyperparameters for LAMARL, MDR,
and AIRL are as follows: the number of episodes = 3000,
episode length = 200, batch size = 512, hidden dim = 180,
number of hidden layers = 3, lr-critic = 1e-3, lr-actor = 1e-4,
exploration rate = 0.6, noise scale = 0.1, gamma = 0.99. During
training, we randomize the region shape instead of fixing it,
ensuring better policy adaptability.
Task setup: The setup consists of two parts. The first part
involves shape parameters, where the target region shape is
predefined on a canvas, and an image is generated. We can
then calculate ncell and lcell based on the pixel count and
side length in this image. The second part involves robot
parameters. We set nrobot = 30, rsense = 0.4 m, ravoid = 0.1 m,
nhn = 6, and nhc = 80. Note that the above parameters must
2
2
satisfy 4nrobot ravoid
≤ ncell lcell
to ensure there is enough space
within the target region to accommodate all the robots. These
parameters are part of the MARL environment, while the
LLM is only tasked for generating the prior policy and reward
functions; therefore, they need to be manually configured.
Results comparison: Fig. 5 illustrates the assembly results
of four methods across various target shapes. Intuitively, the
LAMARL method performs similarly to the Mean-shift method,
with no robot collisions and a uniform distribution across the
target areas. In contrast, both the MDR and AIRL methods show
inadequate performance, with multiple collisions and lower
uniformity. Notably, the AIRL method, influenced by expert
data, performs poorly on several shapes. Table I summarizes
the detailed M1 and M2 values for different shapes, where
each data point represents the “mean (standard deviation)”
over 300 time steps. “Avg” indicates the average across all
26 letters; however, due to space limitations, we only present
the letters corresponding to Fig. 5. It demonstrates that the

TABLE I: Detailed metrics
M1

LAMARL(Ours)

Mean-shift [6]

MDR

AIRL [19]

“R”
“L”
“✚”
“B”
“O”
“T”
Avg

0.55 (0.07)
0.61 (0.10)
0.54 (0.08)
0.54 (0.06)
0.54 (0.10)
0.59 (0.06)
0.5837 (0.07)

0.55 (0.06)
0.63 (0.10)
0.55 (0.09)
0.53 (0.06)
0.52 (0.07)
0.60 (0.07)
0.5815 (0.07)

0.54 (0.06)
0.55 (0.10)
0.48 (0.06)
0.52 (0.07)
0.53 (0.09)
0.57 (0.07)
0.5634 (0.07)

0.53 (0.07)
0.53 (0.12)
0.49 (0.09)
0.50 (0.08)
0.53 (0.12)
0.54 (0.07)
0.5411 (0.08)

M2

LAMARL(Ours)

Mean-shift [6]

MDR

AIRL [19]

“R”
“L”
“✚”
“B”
“O”
“T”
Avg

0.12 (0.10)
0.12 (0.10)
0.14 (0.09)
0.12 (0.07)
0.12 (0.07)
0.10 (0.08)
0.1189 (0.08)

0.12 (0.08)
0.10 (0.09)
0.13 (0.13)
0.13 (0.08)
0.13 (0.07)
0.09 (0.08)
0.1211 (0.08)

0.13 (0.10)
0.15 (0.11)
0.19 (0.09)
0.15 (0.11)
0.13 (0.12)
0.15 (0.08)
0.1533 (0.09)

0.17 (0.09)
0.20 (0.13)
0.20 (0.12)
0.18 (0.10)
0.13 (0.09)
0.18 (0.09)
0.1821 (0.10)

LAMARL method shows comparable performance to Meanshift, with LAMARL being less effective on certain shapes
like “L” and “T”. This is due to the relatively simple LLMgenerated reward, which limits LAMARL’s effectiveness in
handling certain complex scenarios (e.g., narrow passages and
multiple corners). Nevertheless, both LAMARL and Meanshift outperform the MDR and AIRL methods overall. More
importantly, the core advantage of LAMARL over the other
three methods lies in the following aspects:
1) LAMARL vs. Mean-shift method: First, LAMARL
enables fully automated policy generation without the need for
manual design. Second, LAMARL offers better adaptability,
2
2
as it only requires 4nrobot ravoid
≤ ncell lcell
, whereas Meanshift requires exact equality, necessitating the calculation of
different ravoid for each shape. 2) LAMARL vs. MDR method:
LAMARL requires no manual reward design. 3) LAMARL vs.

ZHU et al.: LAMARL: LLM-AIDED MULTI-AGENT REINFORCEMENT LEARNING FOR COOPERATIVE POLICY GENERATION

(a)

SE: 188.9%
Episodes:
720

M1: 0.35,
M2: 0.11

(b)

M1: 0.35,
M2: 0.15

SE: 206.9%
Episodes:
1200

Good uniformity

M1: 0.52,
M2: 0.07
No
collision

7

M1: 0.47,
M2: 0.12
Collision

Episodes:
580

Episodes: 1360
(c)

SE: 194.1%
Episodes:
680

M1: 0.34,
M2: 0.09

Episodes: 4000

Convergent value

With prior policy Without prior policy

(d)

M1: 0.33,
M2: 0.13

SE: 161.6%
Episodes: 1180

Initial
position

M1: 0.49,
M2: 0.07

M1: 0.46,
M2: 0.10
Poor
uniformity

Episodes:
730
Episodes: 1320

Episodes: 4000

Fig. 6: Ablation results of the prior policy. Each subplot’s x-axis represents the number of training episodes, and the y-axis denotes uniformity M2 . “SE”
(sample efficiency) is calculated as the ratio of the x-coordinate at M2 ’s first convergence with and without a prior policy. For example, for the shape “R”, SE
= 1360 / 720 = 188.9%.

AIRL method: First, LAMARL does not require expert data.
Second, even when provided with large amounts of expert data,
AIRL remains imitation learning by nature. Its best performance
can only approach that of the Mean-shift method.
C. Ablation Experiments

1 and ⃝
2 indicate removing
TABLE II: Ablation results of structured prompt. ⃝
basic APIs and CoT, respectively
Case

Full prompt

Function generation success rate

68.5%

1
⃝

2
⃝

2.50% 40.00%

1 and ⃝
2
⃝
1.00%

200 trials with the OpenAI o1-preview model. The results are
This section consists of two parts: the first discusses the summarized in Table II. It indicates that: 1) Without basic
impact of the prior policy on algorithm performance, and the APIs, the success rate is very low, as the LLM struggles
second examines the effect of a structured prompt on function to effectively reason about specific and complex tasks. This
highlights that APIs can provide the LLM with fundamental,
generation.
Ablation experiment 1: We conducted evaluations across all deterministic information, enhancing the LLM’s accuracy;
26 letters; however, due to space limitations, we present only a 2) The success rate will also decrease when the LLM lacks
subset of the results in Fig. 6. Fig. 6 illustrates the variation in CoT’s explicit guidance on the problem-solving thought, as its
uniformity with and without the prior policy. For example, with reasoning ability is limited and it cannot succeed in a single
the shape “R”, the uniformity converges after 720 episodes step. This shows that CoT aids task comprehension and
with the prior policy, achieving a good uniformity (M2 = 0.11). improves accuracy; 3) The absence of multiple components
In contrast, convergence is reached only after 1360 episodes simultaneously increases the task’s difficulty, further reducing
without the prior policy, with a uniformity of 0.15, which is the success rate.
worse than 0.11. Similar results are observed for other shapes.
V. R EAL - WORLD E XPERIMENTS
This demonstrates that the prior policy significantly improves
sample efficiency, with an average increase of 185.9% across
This section aims to validate the practical feasibility of
all 26 letters. Furthermore, for certain shapes like “L” and “T”, LAMARL through comparative experiments conducted on
even with 4000 episodes of training, the policy trained without a physical platform. As shown in Fig. 1(a), we selected
the prior policy still encounters collisions and poor uniformity. eight omnidirectional robots and tasked them with sequentially
This further highlights the limitations of the LLM-generated assembling into various shapes. All four methods were deployed
reward, while the LLM-generated policy effectively assists on the robots for comparison. For parameter settings, only ravoid
in completing complex tasks.
and rsense were adjusted to ravoid = 0.5 m and rsense = 0.9 m,
Ablation experiment 2: In fact, the LLM’s outputs are while all other training and task parameters remained consistent
inherently unstable, and there is no guarantee of consistently with those used in simulation.
generating successful functions. On one hand, the complexity
The experimental results are presented in Fig. 1. Visually,
of tasks may lead to more diverse outputs. On the other hand, LAMARL demonstrates superior performance, achieving welleven with identical inputs, the LLM’s outputs can vary. Hence, formed and uniformly distributed shapes without collisions
this ablation study examines the effect of structured prompts across all shapes. The Mean-shift method also performs well
on the success rate of LLM-generated functions, where success overall, except for a slight uniformity issue in shape “B” likely
is defined as the logical consistency of the generated policy due to the small number of robots. In contrast, the other two
and reward function with those in Fig. 4.
methods show deficiencies, particularly in assembling complex
We examined the impact of removing the CoT and basic shapes such as “R” and “T”, where the uniformity of the
APIs components of the prompt, individually or together, over formations significantly deviates from the expected shapes. In

8

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED MAY, 2025

summary, the conclusions from the real-world experiments
align with those from the simulation experiments, thereby
confirming the feasibility of LAMARL.
VI. C ONCLUSION
This paper proposed the LAMARL approach, which consists
of two modules: the first, LLM-aided function generation,
where the LLM autonomously handles the process from user
input to function code; and the second, MARL, which integrates
the LLM-generated functions, eliminating the need for manual
design. Simulations and real-world experiments demonstrated
LAMARL’s unique advantages, while ablation studies highlighted the importance of prior policies and structured prompts.
Indeed, the relatively simple LLM-generated reward was less
effective than the carefully designed Mean-shift method for
certain complex shapes, which could be the focus of future
work.
It is worth emphasizing that although this paper focuses on
the shape assembly task, the proposed method demonstrates
strong generalizability. This is supported by two main reasons:
first, the last three steps of the LLM-aided function generation
module are fully automated and task-independent; second,
adapting to new tasks only requires minor changes to the
CoT and APIs, since the first four CoT steps and the first two
API components in Fig. 4 are generally applicable. Extending
to other tasks is also the focus of future work.
R EFERENCES
[1] V.-L. Heuthe, E. Panizon, H. Gu, and C. Bechinger, “Counterfactual
rewards promote collective transport using individually controlled swarm
microrobots,” Science Robotics, vol. 9, no. 97, p. eado5888, 2024.
[2] H. Farivarnejad and S. Berman, “Multirobot control strategies for collective transport,” Annual Review of Control, Robotics, and Autonomous
Systems, vol. 5, no. 1, pp. 205–219, 2022.
[3] H. Oh, A. R. Shirazi, C. Sun, and Y. Jin, “Bio-inspired self-organising
multi-robot pattern formation: A review,” Robotics and Autonomous
Systems, vol. 91, pp. 83–100, 2017.
[4] K.-K. Oh, M.-C. Park, and H.-S. Ahn, “A survey of multi-agent formation
control,” Automatica, vol. 53, pp. 424–440, 2015.
[5] S. Zhao and D. Zelazo, “Bearing rigidity theory and its applications for
control and estimation of network systems: Life beyond distance rigidity,”
IEEE Control Systems Magazine, vol. 39, no. 2, pp. 66–83, 2019.
[6] G. Sun, et al., “Mean-shift exploration in shape assembly of robot
swarms,” Nature Communications, vol. 14, no. 1, p. 3476, 2023.
[7] R. Van Parys and G. Pipeleers, “Distributed mpc for multi-vehicle systems
moving in formation,” Robotics and Autonomous Systems, vol. 97, pp.
144–152, 2017.
[8] P. Roque, P. Miraldo, and D. V. Dimarogonas, “Multi-agent formation
control using epipolar constraints,” IEEE Robotics and Automation
Letters, vol. 9, no. 12, pp. 11 002–11 009, 2024.
[9] L. C. Garaffa, M. Basso, A. A. Konzen, and E. P. de Freitas, “Reinforcement learning for mobile robotics exploration: A survey,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 34, no. 8,
pp. 3796–3810, 2021.
[10] R. Konda, H. M. La, and J. Zhang, “Decentralized function approximated
q-learning in multi-robot systems for predator avoidance,” IEEE Robotics
and Automation Letters, vol. 5, no. 4, pp. 6342–6349, 2020.
[11] B. Brito, M. Everett, J. P. How, and J. Alonso-Mora, “Where to go next:
Learning a subgoal recommendation policy for navigation in dynamic
environments,” IEEE Robotics and Automation Letters, vol. 6, no. 3, pp.
4616–4623, 2021.
[12] S. Zhao, Mathematical Foundations of Reinforcement Learning. Springer
Nature Press, 2024.
[13] M. Yuan, B. Li, X. Jin, and W. Zeng, “Automatic intrinsic reward shaping
for exploration in deep reinforcement learning,” in Proceedings of the
International Conference on Machine Learning, 2023, pp. 40 531–40 554.

[14] G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos, “Countbased exploration with neural density models,” in Proceedings of the
International Conference on Machine Learning, 2017, pp. 2721–2730.
[15] R. Raileanu and T. Rocktäschel, “Ride: Rewarding impact-driven
exploration for procedurally-generated environments,” in Proceedings of
the International Conference on Learning Representations, 2020.
[16] H. Ma, K. Sima, T. V. Vo, D. Fu, and T.-Y. Leong, “Reward shaping for
reinforcement learning with an assistant reward agent,” in Proceedings
of the International Conference on Machine Learning, 2024, pp. 33 925–
33 939.
[17] Z. Zheng, J. Oh, and S. Singh, “On learning intrinsic rewards for policy
gradient methods,” in Proceedings of the Annual Conference on Neural
Information Processing Systems, vol. 31, 2018.
[18] E. Bıyık, D. P. Losey, M. Palan, N. C. Landolfi, G. Shevchuk, and
D. Sadigh, “Learning reward functions from diverse sources of human
feedback: Optimally integrating demonstrations and preferences,” The
International Journal of Robotics Research, vol. 41, no. 1, pp. 45–67,
2022.
[19] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adverserial
inverse reinforcement learning,” in Proceedings of the International
Conference on Learning Representations, 2018.
[20] J.-D. Choi and K.-E. Kim, “Inverse reinforcement learning in partially
observable environments,” Journal of Machine Learning Research, vol. 12,
pp. 691–730, 2011.
[21] T. Guo, et al., “Large language model based multi-agents: A survey
of progress and challenges,” in Proceedings of the International Joint
Conference on Artificial Intelligence, 2024, pp. 8048–8057.
[22] S. Nayak, et al., “Long-horizon planning for multi-agent robots in
partially observable environments,” in Multi-modal Foundation Model
meets Embodied AI Workshop @ ICML2024, 2024.
[23] Z. Mandi, S. Jain, and S. Song, “Roco: Dialectic multi-robot collaboration
with large language models,” in Proceedings of the IEEE International
Conference on Robotics and Automation, 2024, pp. 286–299.
[24] Y. Kim, D. Kim, J. Choi, J. Park, N. Oh, and D. Park, “A survey on
integration of large language models with intelligent robots,” Intelligent
Service Robotics, vol. 17, no. 5, pp. 1091–1107, 2024.
[25] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan, “Scalable multi-robot
collaboration with large language models: Centralized or decentralized
systems?” in Proceedings of the IEEE International Conference on
Robotics and Automation, 2024, pp. 4311–4317.
[26] Y. Abbasi-Yadkori, I. Kuzborskij, A. György, and C. Szepesvari, “To
believe or not to believe your LLM: Iterativeprompting for estimating
epistemic uncertainty,” in Proceedings of the Annual Conference on
Neural Information Processing Systems, 2024.
[27] J. Liang, et al., “Code as policies: Language model programs for
embodied control,” in Proceedings of the IEEE International Conference
on Robotics and Automation, 2023, pp. 9493–9500.
[28] L. Huang, et al., “A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions,” ACM Transactions
on Information Systems, 2023.
[29] W. X. Zhao, et al., “A survey of large language models,” arXiv preprint
arXiv:2303.18223, 2023.
[30] L. Chen, Y. Lei, S. Jin, Y. Zhang, and L. Zhang, “Rlingua: Improving
reinforcement learning sample efficiency in robotic manipulations with
large language models,” IEEE Robotics and Automation Letters, vol. 9,
no. 7, pp. 6075–6082, 2024.
[31] H. Li, et al., “Auto mc-reward: Automated dense reward design with
large language models for minecraft,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2024, pp.
16 426–16 435.
[32] Y. J. Ma, et al., “Eureka: Human-level reward design via coding large
language models,” in Proceedings of the International Conference on
Learning Representations, 2024.
[33] T. Xie, et al., “Text2reward: Reward shaping with language models for
reinforcement learning,” in Proceedings of the International Conference
on Learning Representations, 2024.
[34] P. Li, V. Menon, B. Gudiguntla, D. Ting, and L. Zhou, “Challenges
faced by large language models in solving multi-agent flocking,” arXiv
preprint arXiv:2404.04752, 2024.
[35] J. Wei, et al., “Chain-of-thought prompting elicits reasoning in large
language models,” in Proceedings of the Annual Conference on Neural
Information Processing Systems, 2022, pp. 24 824–24 837.
[36] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environments,”
in Proceedings of the Annual Conference on Neural Information
Processing Systems, 2017, pp. 6382–6393.

