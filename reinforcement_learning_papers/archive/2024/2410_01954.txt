C OMA DICE: O FFLINE C OOPERATIVE M ULTI -AGENT R EIN FORCEMENT L EARNING WITH S TATIONARY D ISTRIBUTION
S HIFT R EGULARIZATION
The Viet Bui
Singapore Management University, Singapore
theviet.bui.2023@phdcs.smu.edu.sg

Thanh Hong Nguyen
University of Oregon Eugene, Oregon, United States
thanhhng@cs.uoregon.edu

arXiv:2410.01954v1 [cs.LG] 2 Oct 2024

Tien Mai
Singapore Management University, Singapore
atmai@smu.edu.sg

A BSTRACT
Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions.
While promising results have been demonstrated in single-agent settings, offline multi-agent
reinforcement learning (MARL) presents additional challenges due to the large joint state-action
space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional
shift, which arises when the target policy being optimized deviates from the behavior policy that
generated the data. This problem is exacerbated in MARL due to the interdependence between
agentsâ€™ local policies and the expansive joint state-action space. Prior approaches have primarily
addressed this challenge by incorporating regularization in the space of either Q-functions or
policies. In this work, we introduce a regularizer in the space of stationary distributions to
better handle distributional shift. Our algorithm, ComaDICE, offers a principled framework for
offline cooperative MARL by incorporating stationary distribution regularization for the global
learning policy, complemented by a carefully structured multi-agent value decomposition strategy
to facilitate multi-agent training. Through extensive experiments on the multi-agent MuJoCo
and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance
compared to state-of-the-art offline MARL methods across nearly all tasks.

1

I NTRODUCTION

Over the years, deep RL has achieved remarkable success in various decision-making tasks (Levine et al., 2016;
Silver et al., 2017; Kalashnikov et al., 2018; Haydari & YÄ±lmaz, 2020). However, a significant limitation of deep
RL is its need for millions of interactions with the environment to gather experiences for policy improvement.
This process can be both costly and risky, especially in real-world applications like robotics and healthcare. To
address this challenge, offline RL has emerged, enabling policy learning based solely on pre-collected demonstrations (Levine et al., 2020). Despite this advancement, offline RL faces a critical issue: the distribution shift between
the offline dataset and the learned policy (Kumar et al., 2019). This distribution shift complicates value estimation
for unseen states and actions during policy evaluation, resulting in extrapolation errors where out-of-distribution
(OOD) state-action pairs are assigned unrealistic values (Fujimoto et al., 2018).
To tackle OOD actions, many existing works impose action-level constraints, either implicitly by regulating the
learned value functions or explicitly through distance or divergence penalties (Fujimoto et al., 2019; Kumar et al.,
2019; Wu et al., 2019; Peng et al., 2019; Fujimoto & Gu, 2021; Xu et al., 2021). Only a few recent studies have
addressed both OOD actions and states using state-action-level behavior constraints (Li et al., 2022; Zhang et al.,
2022; Lee et al., 2021; 2022; Mao et al., 2024). In particular, there is an important line of work on DIstribution
Correction Estimation (DICE) (Nachum & Dai, 2020) that constrains the distance in terms of the joint stateaction occupancy measure between the learning policy and the offline policy. These DICE-based methods have
demonstrated impressive performance results on the D4RL benchmarks (Lee et al., 2021; 2022; Mao et al., 2024).
It is important to note that that all the aforementioned offline RL approaches primarily focus on the single-agent
setting. While multi-agent setting is prevalent in many real-world sequential decision-making tasks, offline MARL
remains a relatively under-explored area. The multi-agent setting poses significantly greater challenges due to
the large joint state-action space, which expands exponentially with the number of agents, as well as the interdependencies among the local policies of different agents. As a result, the offline data distribution can become
quite sparse in these high-dimensional joint action spaces, leading to an increased number of OOD state-action

pairs and exacerbating extrapolation errors. A few recent studies have sought to address the negative effects of
sparse data distribution in offline MARL by adapting the well-known centralized training decentralized execution
(CTDE) paradigm from online MARL (Oliehoek et al., 2008; Kraemer & Banerjee, 2016), enabling data-related
regularization at the individual agent level. Notably, some of these works (Pan et al., 2022; Shao et al., 2024;
Wang et al., 2022b) extend popular offline single-agent RL algorithms, such as CQL (Kumar et al., 2020) and
SQL/EQL (Xu et al., 2023), within the CTDE framework.
In our work, we focus on addressing the aforementioned challenges in offline cooperative MARL. In particular,
we follow the DICE approach to address both OOD states and actions, motivated by remarkable performance of
recent DICE-based methods in offline single-agent RL. Similar to previous works in offline MARL, we adopt
the CTDE framework to handle exponential joint state-action spaces in the multi-agent setting. We remark that
extending the DICE approach under this CTDE framework is not straightforward given the complex objective of
DICE that involves the f-divergence in stationary distribution between the learning joint policy and the behavior
policy. Therefore, the value decomposition in CTDE needs to be carefully designed to ensure the consistency in
optimality between the global and local policies. In particular, we provide the following main contributions:
â€¢ We propose ComaDICE, a new offline MARL algorithm that integrates DICE with a carefully designed
value decomposition strategy. In ComaDICE, under the CTDE framework, we decompose both the global
value function Î½ tot and the global advantage functions Atot
Î½ , rather than using Q-functions as in previous
MARL works. This unique factorization approach allows us to theoretically demonstrate that the global
learning objective in DICE is convex in local values, provided that the mixing network used in the value
decomposition employs non-negative weights and convex activation functions. This significant finding
ensures that our decomposition strategy promotes an efficient and stable training process.
â€¢ Building on our decomposition strategy, we demonstrate that finding an optimal global policy can be
divided into multiple sub-problems, each aims to identify a local optimal policy for an individual agent.
We provide a theoretical proof that the global optimal policy is, in fact, equivalent to the product of the
local policies derived from these sub-problems.
â€¢ Finally, we conduct extensive experiments to evaluate the performance of our algorithm, ComaDICE,
in complex MARL environments, including: multi-agent StarCraft II (i.e., SMACv1 (Samvelyan et al.,
2019), SMACv2 (Ellis et al., 2022)) and multi-agent Mujoco (de Witt et al., 2020) benchmarks. Our
empirical results show that our ComaDICE outperforms several strong baselines in all these benchmarks.

2

R ELATED W ORK

Offline Reinforcement Learning (offline RL). Offline RL focuses on learning policies from pre-collected
datasets without any further interactions with the environment (Levine et al., 2020; Prudencio et al., 2023). A
significant challenge in offline RL is the issue of distribution shift, where unseen actions and states may arise during
training and execution, leading to inaccurate policy evaluations and suboptimal outcomes. Consequently, there
is a substantial body of literature addressing this challenge through various approaches (Prudencio et al., 2023).
In particular, some studies impose explicit or implicit policy constraints to ensure that the learned policy remains
close to the behavioral policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Kostrikov et al., 2021;
Peng et al., 2019; Nair et al., 2020; Fujimoto & Gu, 2021; Xu et al., 2021; Cheng et al., 2024; Li et al., 2023).
Others incorporate regularization terms into the learning objectives to mitigate the value overestimation on OOD
actions (Kumar et al., 2020; Kostrikov et al., 2021; Xu et al., 2022c; Niu et al., 2022; Xu et al., 2023; Wang et al.,
2022b). Uncertainty-based offline RL methods seek to balance conservative approaches with naive off-policy RL
techniques, relying on estimates of model, value, or policy uncertainty (Agarwal et al., 2020; An et al., 2021; Bai
et al., 2022). Offline model-based algorithms focus on conservatively estimating the transition dynamics and reward
functions based on the pre-collected datasets (Kidambi et al., 2020; Yu et al., 2020; Matsushima et al., 2020; Yu
et al., 2021). Some other methods impose action-level regularization through imitation learning techniques (Xu
et al., 2022b; Chen et al., 2020; Zhang et al., 2023; Zheng et al., 2024; Brandfonbrener et al., 2021; Xu et al.,
2022a). Finally, while a majority of previous works target OOD actions only, there are a few recent works attempt
to address both OOD states and actions (Li et al., 2022; Zhang et al., 2022; Lee et al., 2021; 2022; Sikchi et al.,
2023; Mao et al., 2024). Our work on offline MARL follow the DICE-based approach, as motivated by compelling
performance of DICE-based algorithms in single-agent settings (Lee et al., 2021; 2022; Sikchi et al., 2023; Mao
et al., 2024).
Offline Multi-agent Reinforcement Learning (offline MARL). While there is a substantial body of literature
on offline single-agent RL, research on offline MARL remains limited. Offline MARL faces challenges from
both distribution shiftâ€”characteristic of offline settingsâ€”and the exponentially large joint action space typical
of multi-agent environments. Recent studies have begun to merge advanced methodologies from both offline RL
and MARL to address these challenges (Yang et al., 2021; Pan et al., 2022; Shao et al., 2024; Wang et al., 2022b)
Specifically, these works employ local policy regularization within the centralized training with decentralized
execution (CTDE) framework to mitigate distribution shift. The CTDE paradigm, well-established in online MARL,

facilitates more efficient and stable learning while allowing agents to operate in a decentralized manner (Oliehoek
et al., 2008; Kraemer & Banerjee, 2016). For instance, Yang et al. (2021) utilize importance sampling to manage
local policy learning on OOD samples. Both works by Pan et al. (2022) and Shao et al. (2024) are built upon
CQL (Kumar et al., 2020), a prominent offline RL algorithm for single-agent scenarios. Matsunaga et al. (2023)
adopt the Nash equilibrium solution concept in game theory to iteratively update best responses of individual agents.
Finally, OMIGA (Wang et al., 2022b) establishes the equivalence between global and local value regularization
within a policy constraint framework, making it the current state-of-the-art algorithm in offline MARL.
Beyond this main line of research, some studies formulate offline MARL as a sequence modeling problem,
employing supervised learning techniques to tackle the issue (Meng et al., 2023; Tseng et al., 2022), while others
adhere to decentralized approaches (Jiang & Lu, 2023).

3

P RELIMINARIES

Our work focuses on cooperative multi-agent RL, which can be modeled as a multi-agent Partially Observable
Markov Decision Process (POMDP), defined by the tuple M = âŸ¨S, A, P, r, Z, O, n, N , Î³âŸ©. Here, n is number
of agents, N
Q = {1, . . . , n} is the set of agents, s âˆˆ S represents the true state of the multi-agent environment,
and A = iâˆˆN Ai is the set of joint actions, where Ai is the set of individual actions available to agent
i âˆˆ N . At each time step, each agent i âˆˆ {1, 2, . . . , n} selects an action ai âˆˆ Ai , forming a joint action
a = (a1 , a2 , . . . , an ) âˆˆ A. The transition dynamics P (sâ€² |s, a) : S Ã— A Ã— S â†’ [0, 1] describe the probability of
transitioning to the next state sâ€² when agents take an action a from the current state s. The discount factor Î³ âˆˆ [0, 1)
represents the weight given to future rewards. In a partially observable environment, each agent receives a local
observation oi âˆˆ Oi based on the observation function Zi (s) : S â†’ Oi , and we denote the joint observation as
o = (o1 , o2 , . . . , on ). In cooperative MARL, all agents share a global reward function r(o, a) : O Ã— A â†’ R.
The goal of all agents is to learn
Pâˆ a joint policy Ï€ tot = {Ï€1 , . . . , Ï€n } that collectively maximize the expected
discounted returns E(o,a)âˆ¼Ï€ tot [ t=0 Î³ t r(ot , at )]. In the offline MARL setting, a pre-collected dataset D is obtained
by sampling from a behavior policy Âµtot = {Âµ1 , . . . , Âµn }, and the policy learning is conducted soly based on D,
with no interactions with the environment. We also define the occupancy measure (or stationary distribution) as
follows:
Xâˆ
ÏÏ€ tot (o, a) = (1 âˆ’ Î³)
P (ot = o, at = a)
t=0

which represents distribution visiting the pair (observation, action) (ot , a1 ) when following the joint policy Ï€ tot ,
where s0 âˆ¼ P0 , at âˆ¼ Ï€ tot (Â·|st ) and st+1 âˆ¼ P (Â·|st , at ).

4

C OMA DICE: O FFLINE C OOPERATIVE M ULTI -AGENT RL WITH S TATIONARY
D ISTRIBUTION C ORRECTION E STIMATION

We consider an offline cooperative MARL problem where the goal is to optimize the expected discounted joint
reward. In this work, we focus on the DICE objective function Nachum & Dai (2020), which incorporates a
stationary distribution regularizer to capture the divergence between the occupancy measures of the learning policy,
Ï€ tot , and the behavior policy, Âµtot , formulated as follows:
maxÏ€ tot E(o,a)âˆ¼ÏÏ€ tot [r(o, a)] âˆ’ Î±Df (ÏÏ€ tot âˆ¥ ÏÂµtot )
(1)
h  Ï€ tot i
where Df (ÏÏ€ tot âˆ¥ ÏÂµtot ) = E(o,a)âˆ¼ÏÏ€ tot f ÏÏÂµtot
is the f-divergence between the stationary distribution ÏÏ€ tot
of the learning policy and ÏÂµtot of the behavior policy. In this work, we consider f (Â·) to be strictly convex and
differentiable. The parameter Î± controls the trade-off between maximizing the reward and penalizing deviation
from the offline datasetâ€™s distribution (i.e., penalizing distributional shift). When Î± = 0, the problem becomes the
standard offline MARL, where the objective is to find a joint policy that maximizes the expected joint reward. On
the other hand, when Î± â‰« 1, the problem shifts towards imitation learning, aiming to closely mimic the behavioral
policy.
This DICE-based approach offers the advantage of better capturing the system dynamics inherent in the offline data.
Such stationary distributions, ÏÏ€ tot and ÏÂµtot , however, are not directly available. We will discuss how to estimate
them in the next subsection.
4.1

C ONSTRAINED O PTIMIZATION IN THE S TATIONARY D ISTRIBUTION S PACE

We first formulate the learning problem in Eq. 1 as a constrained optimization on the space of ÏÏ€ tot :
maxÏÏ€ tot
s.t.

E(o,a)âˆ¼ÏÏ€ tot [r(o, a)] âˆ’ Î±Df (ÏÏ€ tot âˆ¥ ÏÂµtot )
X
X
ÏÏ€ tot (s, aâ€² ) = (1 âˆ’ Î³)p0 (s) + Î³
ÏÏ€ tot (sâ€² , aâ€² )P (s|aâ€² , sâ€² ), âˆ€s âˆˆ S
â€²
â€² â€²
a

a ,s

(2)
(3)

When f is convex, (2-3) becomes a convex optimization problem, as it involves maximizing a concave objective
function subject to linear constraints. We now consider the Lagrange dual of (2-3):

  Ï€ tot
Ï
(s, a)
L(Î½ tot ,ÏÏ€ tot ) = E(o,a)âˆ¼ÏÏ€ tot [r(o, a)] âˆ’ Î±E(s,a)âˆ¼ÏÂµtot f
ÏÂµtot (s, a)
X

X
X
Ï€ tot
Ï€ tot â€² â€²
â€²
â€² â€²
(s,
a
Ï
(s
âˆ’
Î½ tot (s)
Ï
)
âˆ’
(1
âˆ’
Î³)p
(s)
âˆ’
Î³
,
a
)P
(s|a
,
s
)
(4)
0
â€² â€²
â€²
s

a ,s

a

tot

where Î½ (s) is a Lagrange multiplier. Since (2-3) is a convex optimization problem, it is equivalent to the following
minimax problem over the spaces of Î½ tot and ÏÏ€ tot :

minÎ½ tot maxÏÏ€ tot L(Î½ tot , ÏÏ€ tot )
Furthermore, we observe that L(Î½ tot , ÏÏ€ tot ) is linear in Î½ tot and concave in ÏÏ€ tot , so the minimax problem has
a saddle point, implying: minÎ½ tot maxÏÏ€ tot {L(Î½ tot , ÏÏ€ tot )} = maxÏÏ€ tot minÎ½ tot {L(Î½ tot , ÏÏ€ tot )} . In a manner
Ï€ tot
(s,a)
analogous to the single-agent case, by defining wÎ½tot (s, a) = ÏÏÂµtot (s,a)
, the Lagrange dual function can be simplified
into the more compact form (with detailed derivations are in the appendix):



L(Î½ tot , wtot ) = (1 âˆ’ Î³)Esâˆ¼p0 [Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot âˆ’Î±f wÎ½tot (s, a) + wÎ½tot (s, a)Atot
Î½ (s, a)
tot
where Atot
as:
Î½ is an â€œadvantage functionâ€ defined based on Î½
tot
Atot
(s, a) âˆ’ Î½ tot (s)
Î½ (s, a) = q
tot

â€²

tot

(5)
tot

tot

with q (s, a) = r(Z(s), a) + Î³Esâ€² âˆ¼P (Â·|s,a) [Î½ (s )]. It is important to note that Î½ (s) and q (s, a) can be
interpreted as a value function and a Q-function, respectively, arising from the decomposition of the stationary
distribution regularizer. We can now write the learning problem as follows:
minÎ½ tot maxwtot â‰¥0 {L(Î½ tot , wtot )}
tot

tot

tot

(6)

tot

It can be observed that L(Î½ , w ) is linear in Î½ and concave in w , which ensures well-behaved properties
in both the Î½ tot - and wtot -spaces. A key feature of the above minimax problem is that the inner maximization
problem has a closed-form solution, which greatly simplifies the minimax problem, making it no longer adversarial.
We formalize this result as follows:

e tot ) , where
Proposition 4.1. The minimax problem in Eq. 6 is equivalent to minÎ½ tot L(Î½

 tot

tot
tot
âˆ— AÎ½ (s, a)
e
L(Î½ ) = (1 âˆ’ Î³)Esâˆ¼p0 [Î½ (s)] + E(s,a)âˆ¼ÏÂµtot Î±f
Î±
Here, f âˆ— is convex conjugate of f , i.e., f âˆ— (y) = suptâ‰¥0 {ty âˆ’ f (t)}. Moreover, if Î½ tot is parameterized by Î¸, the
e tot ) w.r.t. Î¸ is given as follows:
first order derivative of L(Î½


e tot ) = (1 âˆ’ Î³)Esâˆ¼p [âˆ‡Î¸ Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot âˆ‡Î¸ Atot (s, a)wtotâˆ— (s, a)
âˆ‡Î¸ L(Î½
Î½
Î½
0
where wÎ½totâˆ— (s, a) = max{0, f â€²
of f .
4.2

âˆ’1

â€²âˆ’1
(Atot
(Â·) is the inverse function of the first-order derivative
Î½ (s, a)/Î±)}, with f

VALUE FACTORIZATION

Directly optimizing minÎ½ tot {L(Î½ tot , wÎ½totâˆ— )} in multi-agent settings is generally impractical due to the large state
and action spaces. Therefore, we follow the idea of value decomposition in the well-known CTDE framework in
cooperative MARL to address this computational challenge. However, it is not straightforward to extend the DICE
approach within this CTDE framework due to the complex objective of DICE, which involves the f-divergence
between the learnt joint policy and the behavior policy in stationary distributions. It is thus crucial to carefully
design the value decomposition in CTDE to ensure optimality consistency between the global and local policies.
Specifically, we adopt a factorization approach that decomposes the value function Î½ tot (s) (or global Lagrange
multipliers) into local values using mixing network architectures. Let Î½ (s) = {Î½1 (o1 ), . . . , Î½n (on )} represent a
collection of local â€œvalue functionsâ€ and let AÎ½ (s, a) = {Ai (oi , ai ), i = 1, ..., n} represent a collection of local
advantage functions. The local advantage functions are computed as Ai (oi , ai ) = qi (oi , ai ) âˆ’ Î½i (oi ) for all i âˆˆ N ,
where q(s, a) = {qi (oi , ai ), i = 1, ..., n} is a vector of local Q-functions. To facilitate centralized learning, we
create a mixing network, MÎ¸ , where Î¸ are the learnable weights, that aggregates the local values to form the global
value and advantage functions as follows:
Î½ tot (s, a) = MÎ¸ [Î½ (s)],

Atot
Î½ (s, a) = MÎ¸ [q(s, a) âˆ’ Î½ (s)],

where each network takes the vectors Î½ (s) or AÎ½ (s, a) as inputs and outputs Î½ tot and Atot
Î½ , respectively. Under this
architecture, the learning objective becomes:



MÎ¸ [q(s, a) âˆ’ Î½ (s)]
âˆ—
e
L(Î½ , Î¸) = (1 âˆ’ Î³)Esâˆ¼p0 [MÎ¸ [Î½ (s)]] + E(s,a)âˆ¼ÏÂµtot Î±f
,
Î±

with the observation that AÎ½ (s, a) can be expressed as a linear function of Î½ . There are different ways to construct
the mixing network MÎ¸ ; prior works often employ a single linear combination (1-layer network) or a two-layer
network with convex activations such as ReLU, ELU, or Maxout. In the following, we show a general result
stating that the learning objective function is convex in Î½ , provided that the mixing network is constructed with
non-negative weights and convex activations.
Theorem 4.2. If the mixing network MÎ¸ [Â·] is constructed with non-negative weights and convex activations, then
e , Î¸) is convex in Î½ .
L(Î½
e , Î¸) is convex in Î½ when using any multi-layer feed-forward mixing networks with
Theorem 4.2 shows that L(Î½
non-negative weights and convex activation functions. This finding is highly general and non-trivial, given the
nonlinearity and complexity of both the function (in terms of Î½ ) and the mixing networks. Previous work has often
focused on single-layer (Wang et al., 2022b) or two-layer mixing structures (Rashid et al., 2020; Bui et al., 2024),
emphasizing that such two-layer networks can approximate any monotonic function arbitrarily closely as network
width approaches infinity (Dugas et al., 2009). In our experiments, we test two configurations for the mixing
network: a linear combination (or 1-layer) and a 2-layer feed-forward network. While 2-layer mixing structures
have shown strong performance in online MARL (Rashid et al., 2020; Son et al., 2019; Wang et al., 2020), we
observe in our offline settings that the linear combination approach provides more stable results.
4.3

P OLICY E XTRACTION

Let Î½ âˆ— be an optimal solution to the training problem with mixing networks, i.e.,
e , Î¸).
min L(Î½

(7)

Î½ ,Î¸

We now need to extract a local and joint policy from this solution. Based on Prop. 4.1, given Î½ âˆ— , we can compute
this occupancy ratio as follows: :



MÎ¸ [AÎ½ âˆ— (s, a)]
totâˆ—
â€² âˆ’1
w (s, a) = max 0, f
.
Î±
totâˆ—

Âµtot

(s,a)Â·Ï
(s,a)
The global policy can then be obtained as follows: Ï€ âˆ—tot (a|s) = P â€² w wtotâˆ—
(s,aâ€² )Â·ÏÂµtot (s,aâ€² ) . This computation,
a âˆˆA
however, is not practical since ÏÂµtot is generally not available and might not be accurately estimated in the offline
setting. A more practical way to estimate the global policy, Ï€ âˆ—tot , as the result of solving the following weighted
behavioral cloning (BC):

max

Ï€ tot âˆˆÎ tot

E(s,a)âˆ¼ÏÏ€ âˆ—tot [log Ï€ tot (a|s)] =

max

Ï€ tot âˆˆÎ tot

E(s,a)âˆ¼ÏÂµtot [wtotâˆ— (s, a) log Ï€ tot (a|s)]

(8)

where Î tot represents the feasible set of global policies. Here weQ
assume that Î tot contains decomposable global
policies, i.e., Î tot = {Ï€ tot | âˆƒÏ€i , âˆ€i âˆˆ N such that Ï€ tot (a|s) = iâˆˆN Ï€i (ai |oi )}. In other words, Î tot consists
of global policies that can be expressed as a product of local policies. This decomposability is highly useful for
decentralized learning and has been widely adopted in MARL (Wang et al., 2022b; Bui et al., 2024; Zhang et al.,
2021).
While the above weighted BC appears practical, as (s, a) can be sampled from the offline dataset generated by
ÏÏ€ tot , and since wtotâˆ— (s, a) is available from solving 7, it does not directly yield local policies, which are essential
for decentralized execution. To address this, we propose solving the following weighted BC for each local agent
i âˆˆ N:


maxÏ€i E(s,a)âˆ¼D wtotâˆ— (s, a) log Ï€i (ai |oi ) .
This local WBC approach has several attractive properties. First, wtotâˆ— (s, a) explicitly appears in the local policy
optimization and is computed from global observations and actions. This enables local policies to be optimized
with global information, ensuring consistency with the credit assignment in the multi-agent system. Furthermore,
as shown in Proposition 4.3 below, the optimization of local policies through the lobcal WBC is highly consistent
with the global weighted BC in 8.
Q
âˆ—
Proposition 4.3. Let Ï€ âˆ— be the optimal solution to 4.3. Then Ï€tot
(a|s) = iâˆˆN Ï€iâˆ— (ai |oi ) is also optimal for the
global weighted BC in 8.
Here we note that consistency between global and local policies is a critical aspect of centralized training with
CTDE. Previous MARL approaches typically achieve this by factorizing the Q- or V-functions into local functions,
and training local policies based on these local ones (Rashid et al., 2020; Wang et al., 2020; Bui et al., 2024).
However, in our case, there are key differences that prevent us from employing such local values to derive local
policies. Specifically, we factorize the Lagrange multipliers Î½ tot to train the stationary distribution ratio wtot .
While local w values can be extracted from the local Î½i , these local w values do not represent a local stationary
distribution ratio and therefore cannot be used to recover local policies.

Algorithm 1 ComaDICE: Offline Cooperative MARL with Stationary DIstribution Correction Estimation
1: Input: Parameters Î¸, Ïˆq , ÏˆÎ½ , Î·i and the corresponding learning rates Î»Î¸ , Î»Ïˆq , Î»ÏˆÎ½ , Î»Î· , respectively. Offline

data D.
2: Output: Local optimized polices Ï€i .
3: # Training the occupancy ratio wtotâˆ—
4: for a certain number of training steps do
5:
Ïˆq = Ïˆq âˆ’ Î»Ïˆq âˆ‡Ïˆq L(Ïˆq ) # Update Q-function towards the MSE in 9

e Î½ , Î¸) # Update Î¸ to minimize the loss in 10
6:
Î¸ = Î¸ âˆ’ Î»Î¸ âˆ‡Î¸ L(Ïˆ
e Î½ , Î¸) # Update ÏˆÎ½ to minimize the loss in 10
7:
ÏˆÎ½ = ÏˆÎ½ âˆ’ Î»ÏˆÎ½ âˆ‡ÏˆÎ½ L(Ïˆ
8: end for
9: # Training local policy
10: for a certain number of training steps do
11:
Î·i = Î·i + Î»Î· âˆ‡Î·i LÏ€ (Î·i ) # Update the local policy by optimizing 11
12: end for
13: Return Ï€i (ai |oi ; Î·i ), i = 1, ..., n

5

P RACTICAL A LGORITHM

Let D represent the offline dataset, consisting of sequences of local observations and actions gathered from a global
behavior policy Ï€ tot . To train the value function Î½ , we construct a value network Î½i (oi ; ÏˆÎ½ ) for each local agent i,
along with a network for each local Q-function qi (oi , ai ; Ïˆq ), where ÏˆÎ½ and Ïˆq are learnable parameters for the
local value and Q-functions. Each local advantage function is then calculated as follows: The global value function
and advantage function are subsequently aggregated using two mixing networks with a shared set of learnable
parameters Î¸:
s
Î½ tot (s) = MsÎ¸ [Î½ (s; ÏˆÎ½ )], Atot
Î½ (s, a) = MÎ¸ [q(s, a; Ïˆq ) âˆ’ Î½ (s; ÏˆÎ½ )],
where MsÎ¸ [Â·] represents a linear combination of its inputs with non-negative weights, such that MsÎ¸ [Î½ (s; ÏˆÎ½ )] =
Î½ (s; ÏˆÎ½ )âŠ¤ WÎ¸s + bsÎ¸ , where WÎ¸s and bsÎ¸ are weights of the mixing network.1 It is important to note that WÎ¸s and bsÎ¸ are
generated by hyper-networks that take the global state s and the learnable parameters Î¸ as inputs. In this context,
we employ the same mixing network MsÎ¸ to combine the local values and advantages. However, our framework is
flexible enough to allow the use of two different mixing networks for Î½ tot and Atot
Î½ .
In our setting, the relationship between the global Q-function, value, and advantage functions is described in Eq. 5.
tot â€²
Specifically, we have: Atot
(s )] âˆ’ Î½ tot (s). To capture this relationship, we
Î½ (s, a) = r(Z(s), a) + Î³Esâ€² âˆ¼P (Â·|s,a) [Î½
train the Q-function by optimizing the following MSE loss:
X
2
tot â€²
minq
Atot
(s ) âˆ’ Î½ tot (s) .
Î½ (s, a) âˆ’ r(Z(s), a) + Î³Î½
â€²
(s,a,s )âˆ¼D

This is equivalent to:
minÏˆq Lq (Ïˆq ) =



X
(s,a,sâ€² )âˆ¼D

MsÎ¸ [q(s, a; Ïˆq ) âˆ’ Î½ (s; ÏˆÎ½ )]

2
â€²
âˆ’ r(Z(s), a) + Î³MsÎ¸ [Î½ (sâ€² ; ÏˆÎ½ )] âˆ’ MsÎ¸ [Î½ (s; ÏˆÎ½ )]

(9)

For the primary loss function used to train the value function, we leverage transitions from the offline dataset to
e resulting in the following loss function for offline training:
approximate the objective L,

 s

e Î½ , Î¸) = (1 âˆ’ Î³)Es âˆ¼D [Ms0 [Î½ (s0 ; ÏˆÎ½ )]] + E(s,a)âˆ¼D Î±f âˆ— MÎ¸ [q(s, a; Ïˆq ) âˆ’ Î½ (s; ÏˆÎ½ )]
L(Ïˆ
(10)
0
Î¸
Î±
e Î½ , Î¸), we compute the occupancy ratio: wtotâˆ— (s, a) =
As mentioned,
after obtaining (Î½ âˆ— , Î¸âˆ— ) by solving
minÏˆÎ½ ,Î¸ L(Ïˆ
Î½
n

o
s
âˆ—
s
âˆ’1 MÎ¸âˆ— [Î½ (s)]âˆ’MÎ¸âˆ— [q(s,a;Ïˆq )]
max 0, f â€²
.
To
train
the
local
policy
Ï€
(a
|o
),
we
represent
it
using
a policy
i
i
i
Î±
network Ï€i (ai |oi ; Î·i ), where Î·i are the learnable parameters. The training process involves optimizing the following
weighted behavioral cloning (BC) objective:
X
maxÎ·i LÏ€ (Î·i ) =
wÎ½totâˆ— (s, a) log(Ï€i (ai |oi ; Î·i )).
(11)
(s,a)âˆ¼D

Our ComaDICE algorithm consists of two primary steps. The first step involves estimating the occupancy ratio
wtotâˆ— from the offline dataset. The second step focuses on training the local policy by solving the weighted BC
problem using wtotâˆ— . In the first step, we simultaneously update the Q-functions Ïˆq , the mixing network parameters
Î¸, and the value function ÏˆÎ½ , aiming to minimize the mean squared error (MSE) in Eq. 9 while optimizing the main
loss function in Eq. 10.
1
In our experiments, we use a single-layer mixing network due to its superior performance compared to a two-layer structure,
though our approach is general and can handle any multi-layer feed-forward mixing network.

BC

BCQ

CQL

ICQ

OMAR

OMIGA

ComaDICE
(ours)

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

36.9Â±8.7
36.2Â±10.6
19.4Â±4.6
37.5Â±4.4
13.8Â±1.5

16.2Â±2.3
9.4Â±5.6
10.0Â±4.1
6.2Â±2.0
1.2Â±1.5

10.0Â±4.1
26.2Â±7.6
10.6Â±5.4
11.9Â±4.1
0.0Â±0.0

36.9Â±9.1
28.1Â±6.6
12.5Â±4.4
32.5Â±8.1
12.5Â±5.6

21.2Â±4.1
13.8Â±7.0
12.5Â±3.4
23.8Â±2.5
11.2Â±7.8

33.1Â±5.4
40.0Â±10.7
16.2Â±6.1
36.2Â±5.1
12.5Â±8.1

46.2Â±6.1
50.6Â±8.7
20.0Â±4.2
47.5Â±7.8
13.8Â±5.8

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

30.0Â±4.2
29.4Â±5.8
16.2Â±3.6
26.2Â±10.4
4.4Â±4.2

12.5Â±6.2
6.9Â±6.1
3.8Â±4.6
5.0Â±3.2
0.0Â±0.0

9.4Â±7.9
9.4Â±5.6
7.5Â±6.4
10.6Â±4.2
0.0Â±0.0

23.1Â±5.8
16.9Â±5.8
5.0Â±4.2
15.6Â±3.4
7.5Â±6.1

14.4Â±4.7
15.0Â±4.6
9.4Â±5.6
7.5Â±7.3
5.0Â±4.2

28.1Â±4.4
29.4Â±3.2
12.5Â±5.2
21.9Â±4.4
4.4Â±2.5

30.6Â±8.2
32.5Â±5.8
19.4Â±5.4
29.4Â±3.8
9.4Â±5.2

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

26.9Â±10.0
25.0Â±2.8
13.8Â±4.7
8.1Â±1.5
7.5Â±3.2

14.4Â±4.2
5.6Â±4.6
9.4Â±5.2
2.5Â±1.2
0.6Â±1.3

14.4Â±5.8
5.6Â±4.6
6.2Â±4.4
1.2Â±1.5
1.2Â±1.5

18.8Â±7.1
15.6Â±7.4
10.6Â±6.7
10.0Â±7.8
7.5Â±3.2

13.8Â±6.1
19.4Â±2.3
10.6Â±3.8
12.5Â±4.4
3.8Â±2.3

21.9Â±5.9
23.8Â±6.4
13.8Â±6.7
10.0Â±2.3
4.4Â±4.2

31.2Â±7.7
33.8Â±11.8
19.4Â±3.6
9.4Â±6.2
11.2Â±4.2

Instances

Table 1: Comparison of winrates for ComaDICE and baselines across SMACv2 tasks.

6

E XPERIMENTS

6.1

E NVIRONMENTS

We utilize three standard MARL environments: SMACv1 (Samvelyan et al., 2019), SMACv2 (Ellis et al., 2022),
and Multi-Agent MuJoCo (MaMujoco) (de Witt et al., 2020), each offering unique challenges and configurations
for evaluating cooperative MARL algorithms.
SMACv1. SMACv1 is based on Blizzardâ€™s StarCraft II. It uses the StarCraft II API and DeepMindâ€™s PySC2 to
enable agent interactions with the game. SMACv1 focuses on decentralized micromanagement scenarios where
each unit is controlled by an RL agent. Tasks like 2c vs 64zg and 5m vs 6m are labeled hard, while 6h vs 8z and
corridor are super hard. The offline dataset, provided by Meng et al. (2023), was generated using MAPPO-trained
agents (Yu et al., 2022).
SMACv2. In comparison to SMACv1, SMACv2 introduces increased randomness and diversity by randomizing
start positions, unit types, and modifying sight and attack ranges. This version includes tasks such as protoss,
terran, and zerg, with instances ranging from 5 vs 5 to 20 vs 23, increasing in difficulty. Our offline dataset for
SMACv2 was generated by running MAPPO for 10 million training steps and collecting 1,000 trajectories, ensuring
medium quality but comprehensive coverage of the learning process. To the best of our knowledge, we are the first
to explore SMACv2 in offline MARL, whereas most prior work has used this environment in online settings.
MaMujoco. MaMujoco serves as a benchmark for continuous cooperative multi-agent robotic control. Derived
from the single-agent MuJoCo control suite in OpenAI Gym (Brockman et al., 2016), it presents scenarios where
multiple agents within a single robot must collaborate to achieve tasks. The tasks include Hopper-v2, Ant-v2, and
HalfCheetah-v2, with instances labeled as expert, medium, medium-replay, and medium-expert. The offline dataset
was created by (Wang et al., 2022b) using the HAPPO method (Wang et al., 2022a).
6.2

BASELINES

We consider the following baselines, which represent either standard or state-of-the-art (SOTA) methods for offline
MARL: (i) BC (Behavioral Cloning); (ii) BCQ (Batch-Constrained Q-learning) (Fujimoto et al., 2019) â€“ an offline
RL algorithm that constrains the policy to actions similar to those in the dataset to reduce distributional shift,
adapted for offline MARL settings; (iii) CQL (Conservative Q-Learning) (Kumar et al., 2020) â€“ a method that
stabilizes offline Q-learning by penalizing out-of-distribution actions, ensuring conservative value estimates; (iv)
ICQ (Implicit Constraint Q-learning) (Yang et al., 2021) â€“ an approach using importance sampling to manage
out-of-distribution actions in multi-agent settings; (v) OMAR (Offline MARL with Actor Rectification) (Pan et al.,
2022) â€“ a method combining CQL with optimization techniques to ensure the global validity of local regularizations,
promoting cooperative behavior; (vi) OMIGA (Offline MARL with Implicit Global-to-Local Value Regularization)
(Wang et al., 2022b) â€“ a SOTA method that transforms global regularizations into implicit local ones, optimizing
local policies with global insights.
We used experimental results contributed by the authors of OMIGA (Wang et al., 2022b) as our baselines. They
provided both the results and source code for all the baseline methods. This source code was also employed to run

CQL

ICQ

OMIGA

ComaDICE
(ours)

Hopper

expert
medium
m-replay
m-expert

77.9Â±58.0
44.6Â±20.6
26.5Â±24.0
54.3Â±23.7

159.1Â±313.8
401.3Â±199.9
31.4Â±15.2
64.8Â±123.3

754.7Â±806.3
501.8Â±14.0
195.4Â±103.6
355.4Â±373.9

859.6Â±709.5
1189.3Â±544.3
774.2Â±494.3
709.0Â±595.7

2827.7Â±62.9
822.6Â±66.2
906.3Â±242.1
1362.4Â±522.9

Ant

expert
medium
m-replay
m-expert

1317.7Â±286.3
1059.6Â±91.2
950.8Â±48.8
1020.9Â±242.7

1042.4Â±2021.6
533.9Â±1766.4
234.6Â±1618.3
800.2Â±1621.5

2050.0Â±11.9
1412.4Â±10.9
1016.7Â±53.5
1590.2Â±85.6

2055.5Â±1.6
1418.4Â±5.4
1105.1Â±88.9
1720.3Â±110.6

2056.9Â±5.9
1425.0Â±2.9
1122.9Â±61.0
1813.9Â±68.4

Half
Cheetah

expert
medium
m-replay
m-expert

2992.7Â±629.7
2590.5Â±1110.4
-333.6Â±152.1
3543.7Â±780.9

1189.5Â±1034.5
1011.3Â±1016.9
1998.7Â±693.9
1194.2Â±1081.0

2955.9Â±459.2
2549.3Â±96.3
1922.4Â±612.9
2834.0Â±420.3

3383.6Â±552.7
3608.1Â±237.4
2504.7Â±83.5
2948.5Â±518.9

4082.9Â±45.7
2664.7Â±54.2
2855.0Â±242.2
3889.7Â±81.6

Table 2: Average returns for ComaDICE and baselines on MaMuJoCo benchmarks.

Protoss

BC

5

40%
20%

protoss_10_vs_10
60%
40%
20%
0%
100
0
60%
40%
20%
0%
100
0
40%
20%
0%
100

0

Terran

0

40%
30%50
20%
10%
0%

40%
terran_10_vs_10
50
100
20%

0

30%
20%
10%
0%

zerg_10_vs_10
50
100
Zerg

5

BCQ

Instances

50

50

30%
20%
10%
0%

40%
20%
0

100

0

0

BCQ

CQL

ICQ

OMAR

OMIGA

40%

40%

40%

20%

20%

20%

protoss_10_vs_11
0

50

protoss_20_vs_20

40%
20%
0%

40%
terran_10_vs_11
50
100
20%

0

50

40%

5 vs 5

40%

0

50
0

50

20%
0%

100

10 vs 10

20%

50

protoss_20_vs_23

40%
30%
0
20%
10%
0%

20%20%

0

0

0

40%

40%
terran_20_vs_20
50
100

50

0

40%
terran_20_vs_23
50
100

0

zerg_20_vs_23
50
100

0

20%

50

0

40%
20%

40%

20% 0%

20%

10%

0

50

50

10 vs 11

100

0

50

0

15%
10%
5%
0%

zerg_20_vs_20
50
100

20% 40%
10%
0% 20%

20%

0

50

40% 20%

zerg_10_vs_11
50
100

50

0

ComaDICE (ours)

0

50

20 vs 20

50

0

50

100

50

20 vs 23

Figure 1: Evaluation curves of ComaDICE and baselines over time on SMACv2 tasks.

these baselines for the SMACv2 environment. All hyperparameters were kept at their default settings, and each
experiment was conducted with five different random seeds to ensure robustness and reproducibility of the results.

6.3

M AIN C OMPARISON

We now present a comprehensive evaluation of our proposed algorithm, ComaDICE, against several baseline
methods in offline MARL. The baselines selected for comparison include both standard and SOTA approaches,
providing a robust benchmark to assess the effectiveness of ComaDICE.
Our evaluation focuses on two primary metrics: returns and winrates. Returns are the average rewards accumulated
by the agents across multiple trials, providing a measure of policy effectiveness. Winrates, applicable in competitive
environments such as SMACv1 and SMACv2, indicate the success rate of agents against opponents, reflecting the
algorithmâ€™s robustness in adversarial settings.
The experimental results, summarized in Tables 1 and 2, demonstrate that ComaDICE consistently achieves superior
performance compared to baseline methods across a range of scenarios. Notably, ComaDICE excels in complex
tasks, highlighting its ability to effectively manage distributional shifts in challenging environments.
Figures 1 illustrates the learning curves for each algorithm, showing that ComaDICE not only outperforms other
methods in terms of mean returns but also exhibits lower variance, indicating more stable and reliable performance.
These findings underscore the robustness and adaptability of ComaDICE, setting a new benchmark for offline
MARL.

60%

60%

60%

40%

40%

40%

20%

20%

20%

0.01 0.1 1

protoss

10 100

0.01 0.1 1

10 100

terran

0.01 0.1 1

zerg

10 100

2k

2k

2k

0

0

0

0.01 0.1

1

10 100

Hopper

0.01 0.1

1

Ant

10 100

0.01 0.1

1

10 100

HalfCheetah

Figure 2: Impact of regularization parameter Î± on performance in different environments.
6.4

A BLATION S TUDY - I MPACT OF THE R EGULARIZATION PARAMETER A LPHA

We investigate how varying the regularization parameter alpha (Î±) affects the performance of our ComaDICE
algorithm. The parameter Î± is crucial for balancing the trade-off between maximizing rewards and penalizing deviations from the offline datasetâ€™s distribution. We conducted experiments with Î± values ranging from
{0.01, 0.1, 1, 10, 100}, evaluating performance using winrates in the SMACv2 environment and returns in the
MaMujoco environment. These results, illustrated in Figure 2, highlight the sensitivity of ComaDICE to different Î±
values. In particular, we observe that ComaDICE achieves optimal performance when Î± is around 10, suggesting
that the stationary distribution regularizer plays a essential role in the success of our algorithm.
In our appendix, we provide additional ablation studies to analyze the performance of our algorithm using different
forms of f-divergence functions, as well as comparisons between 1-layer and 2-layer mixing network structures.
The appendix also includes proofs of the theoretical claims made in the main paper, details of our experimental
settings, and other experimental information.

7

C ONCLUSION , F UTURE W ORK AND B ROADER I MPACTS

Conclusion. In this paper, we propose ComaDICE, a principled framework for offline MARL. Our algorithm
incorporates a stationary distribution shift regularizer into the standard MARL objective to address the conventional
distribution shift issue in offline RL. To facilitate training within a CTDE framework, we decompose both the
global value and advantage functions using a mixing network. We demonstrate that, under our mixing architecture,
the main objective function is concave in the value function, which is crucial for ensuring stable and efficient
training. The results of this training are then utilized to derive local policies through a weighted BC approach,
ensuring consistency between global and local policy optimization. Extensive experiments on SOTA benchmark
tasks, including SMACv2, show that ComaDICE outperforms other baseline methods.
Limitations and Future Work: There are some limitations that are not addressed within the scope of this paper.
For instance, we focus solely on cooperative learning, leaving open the question of how the approach would perform
in cooperative-competitive settings. Extending ComaDICE to such scenarios would require considerable effort and
is an interesting direction for future research. Additionally, in our training objective, the DICE term is designed to
reduce the divergence between the learning policy and the behavior policy. As a result, the performance of the
algorithm is heavily dependent on the quality of the behavior policy. Although this reliance may be unavoidable,
future research should focus on mitigating the influence of the behavior policy on training outcomes. Furthermore,
our algorithm, like other baselines, still requires a large amount of data to achieve desirable learning outcomes.
Improving sample efficiency would be another valuable area for future research.
Broader Impacts: The development of an offline MARL algorithm using a stationary distribution shift regularizer
could lead to improved performance in tasks where real-time interaction is costly, such as robotics, autonomous
driving, and healthcare. It could also promote safer exploration and wider adoption of offline learning in high-stakes
environments. On the negative side, since the algorithm relies heavily on the behavior policy, if the behavior policy
is flawed or biased, the performance of the learnt policy could also suffer. This could reinforce preexisting biases or
suboptimal behaviors in real-world applications. Moreover, like any AI technology, there is a risk of the algorithm
being applied in unintended or harmful ways, such as in surveillance or military applications, where multi-agent
systems could be used to manipulate environments or people without adequate oversight.

E THICAL S TATEMENT
Our work introduces ComaDICE, a framework for offline MARL, aimed at improving training stability and
policy optimization in complex multi-agent environments. While this research has significant potential for
positive applications, particularly in domains such as autonomous systems, resource management, and multi-agent
simulations, it is crucial to address the ethical implications and risks associated with this technology.
The deployment of reinforcement learning systems in real-world, multi-agent settings raises concerns about
unintended behaviors, especially in safety-critical domains. If the policies learned by ComaDICE are applied

without proper testing and validation, they may lead to undesirable or harmful outcomes, especially in areas such
as autonomous driving, healthcare, or robotics. Additionally, bias in the training data or simulation environments
could result in suboptimal policies that unfairly impact certain agents or populations, potentially leading to ethical
concerns regarding fairness and transparency.
To mitigate these risks, we emphasize the need for extensive testing and validation of policies generated using
ComaDICE, particularly in real-world environments where the consequences of errors could be severe. It is also
essential to ensure that the datasets and simulations used in training are representative, unbiased, and carefully
curated. We encourage practitioners to use human oversight and collaborate with domain experts to ensure that
ComaDICE is applied responsibly, particularly in high-stakes settings.

R EPRODUCIBILITY S TATEMENT
In order to facilitate reproducibility, we have submitted the source code for ComaDICE, along with the datasets
utilized to produce the experimental results presented in this paper (all these will be made publicly available
if the paper gets accepted). Additionally, in the appendix, we provide details of our algorithm, including key
implementation steps and details needed to replicate the results. The hyper-parameter settings for all experiments
are also included to ensure that others can reproduce the findings under the same experimental conditions. We
invite the research community to explore and apply the ComaDICE framework in various environments to further
validate and expand upon the results reported in this work.

R EFERENCES
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement
learning. In International conference on machine learning, pp. 104â€“114. PMLR, 2020.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning
with diversified q-ensemble. Advances in neural information processing systems, 34:7436â€“7447, 2021.
Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang.
Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. arXiv preprint arXiv:2202.11566,
2022.
David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation.
Advances in neural information processing systems, 34:4933â€“4946, 2021.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. Openai gym, 2016. URL http://arxiv.org/abs/1606.01540.
The Viet Bui, Tien Mai, and Thanh Hong Nguyen. Inverse factorized q-learning for cooperative multi-agent
imitation learning. Advances in Neural Information Processing Systems, 38, 2024.
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action imitation
learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:
18353â€“18363, 2020.
Peng Cheng, Xianyuan Zhan, Wenjia Zhang, Youfang Lin, Han Wang, Li Jiang, et al. Look beneath the surface:
Exploiting fundamental symmetry for sample-efficient offline rl. Advances in Neural Information Processing
Systems, 36, 2024.
Christian Schroeder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip Torr, Wendelin BoÌˆhmer, and Shimon
Whiteson. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. arXiv
preprint arXiv:2003.06709, 19, 2020.
Charles Dugas, Yoshua Bengio, FrancÌ§ois BeÌlisle, Claude Nadeau, and ReneÌ Garcia. Incorporating functional
knowledge in neural networks. Journal of Machine Learning Research, 10(6), 2009.
Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N Foerster, and Shimon
Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2212.07489, 2022.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in
neural information processing systems, 34:20132â€“20145, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods.
In International conference on machine learning, pp. 1587â€“1596. PMLR, 2018.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In
International conference on machine learning, pp. 2052â€“2062. PMLR, 2019.
Ammar Haydari and Yasin YÄ±lmaz. Deep reinforcement learning for intelligent transportation systems: A survey.
IEEE Transactions on Intelligent Transportation Systems, 23(1):11â€“32, 2020.
Jiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. In ECAI, pp. 1148â€“1155,
2023.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan
Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based
robotic manipulation. In Conference on robot learning, pp. 651â€“673. PMLR, 2018.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline
reinforcement learning. Advances in neural information processing systems, 33:21810â€“21823, 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher
divergence critic regularization. In International Conference on Machine Learning, pp. 5774â€“5783. PMLR,
2021.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing, 190:82â€“94, 2016.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. Advances in neural information processing systems, 32, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement
learning. Advances in Neural Information Processing Systems, 33:1179â€“1191, 2020.
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offline policy optimization via stationary distribution correction estimation. In International Conference on Machine Learning, pp.
6120â€“6130. PMLR, 2021.
Jongmin Lee, Cosmin Paduraru, Daniel J Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung Kim, and Arthur
Guez. Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation.
arXiv preprint arXiv:2204.08957, 2022.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies.
Journal of Machine Learning Research, 17(39):1â€“40, 2016.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and
perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Jianxiong Li, Xiao Hu, Haoran Xu, Jingjing Liu, Xianyuan Zhan, and Ya-Qin Zhang. Proto: Iterative policy
regularized offline-to-online reinforcement learning. arXiv preprint arXiv:2305.15669, 2023.
Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan. Dealing with the unknown: Pessimistic offline
reinforcement learning. In Conference on Robot Learning, pp. 1455â€“1464. PMLR, 2022.
Liyuan Mao, Haoran Xu, Weinan Zhang, and Xianyuan Zhan. Odice: Revealing the mystery of distribution
correction estimation via orthogonal-gradient update. arXiv preprint arXiv:2402.00348, 2024.
Daiki E Matsunaga, Jongmin Lee, Jaeseok Yoon, Stefanos Leonardos, Pieter Abbeel, and Kee-Eung Kim. Alberdice: addressing out-of-distribution joint actions in offline multi-agent rl via alternating stationary distribution
correction estimation. Advances in Neural Information Processing Systems, 36:72648â€“72678, 2023.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient
reinforcement learning via model-based offline optimization. arXiv preprint arXiv:2006.03647, 2020.
Linghui Meng, Muning Wen, Chenyang Le, Xiyun Li, Dengpeng Xing, Weinan Zhang, Ying Wen, Haifeng Zhang,
Jun Wang, Yaodong Yang, et al. Offline pre-trained multi-agent decision transformer. Machine Intelligence
Research, 20(2):233â€“248, 2023.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint arXiv:2001.01866,
2020.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Haoyi Niu, Yiwen Qiu, Ming Li, Guyue Zhou, Jianming Hu, Xianyuan Zhan, et al. When to trust your simulator:
Dynamics-aware hybrid offline-and-online reinforcement learning. Advances in Neural Information Processing
Systems, 35:36599â€“36612, 2022.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions for
decentralized pomdps. Journal of Artificial Intelligence Research, 32:289â€“353, 2008.
Ling Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better amid conservatism: Offline multi-agent
reinforcement learning with actor rectification. In International conference on machine learning, pp. 17221â€“
17237. PMLR, 2022.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Rafael Figueiredo Prudencio, Marcos ROA Maximo, and Esther Luna Colombini. A survey on offline reinforcement
learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems,
2023.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon
Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of
Machine Learning Research, 21(1):7234â€“7284, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ
Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. arXiv preprint arXiv:1902.04043, 2019.
Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning
for offline multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.
Harshit Sikchi, Amy Zhang, and Scott Niekum. Imitation from arbitrary experience: A dual unification of
reinforcement and imitation learning methods. In Workshop on Reincarnating Reinforcement Learning at ICLR
2023, 2023.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,
Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature,
550(7676):354â€“359, 2017.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize
with transformation for cooperative multi-agent reinforcement learning. In International conference on machine
learning, pp. 5887â€“5896. PMLR, 2019.
Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent reinforcement
learning with knowledge distillation. Advances in Neural Information Processing Systems, 35:226â€“237, 2022.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. arXiv preprint arXiv:2008.01062, 2020.
Jun Wang, Yaodong Yang, and Zongqing Wang. Trust region policy optimization in multi-agent reinforcement
learning. arXiv preprint arXiv:2109.11251, 2022a.
Xiangsen Wang, Haoran Xu, Yinan Zheng, and Xianyuan Zhan. Offline multi-agent reinforcement learning with
implicit global-to-local value regularization. Advances in Neural Information Processing Systems, 36, 2022b.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint
arXiv:1911.11361, 2019.
Haoran Xu, Xianyuan Zhan, Jianxiong Li, and Honglei Yin. Offline reinforcement learning with soft behavior
regularization. arXiv preprint arXiv:2110.07395, 2021.
Haoran Xu, Li Jiang, Li Jianxiong, and Xianyuan Zhan. A policy-guided imitation approach for offline reinforcement
learning. Advances in Neural Information Processing Systems, 35:4085â€“4098, 2022a.
Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning
from suboptimal demonstrations. In Proceedings of the 39th International Conference on Machine Learning, pp.
24725â€“24742, 2022b.
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline reinforcement
learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8753â€“8760, 2022c.

Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline
rl with no ood actions: In-sample learning via implicit value regularization. arXiv preprint arXiv:2303.15810,
2023.
Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao.
Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. Advances in
Neural Information Processing Systems, 34:10299â€“10312, 2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:
24611â€“24624, 2022.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu
Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:
14129â€“14142, 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo:
Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:
28954â€“28967, 2021.
Hongchang Zhang, Jianzhun Shao, Yuhang Jiang, Shuncheng He, Guanwen Zhang, and Xiangyang Ji. State
deviation correction for offline reinforcement learning. In Proceedings of the AAAI conference on artificial
intelligence, volume 36, pp. 9022â€“9030, 2022.
Qin Zhang, Linrui Zhang, Haoran Xu, Li Shen, Bowen Wang, Yongzhe Chang, Xueqian Wang, Bo Yuan, and
Dacheng Tao. Saformer: A conditional sequence modeling approach to offline safe reinforcement learning.
arXiv preprint arXiv:2301.12203, 2023.
Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy
of maximum-entropy multi-agent reinforcement learning. In International conference on machine learning, pp.
12491â€“12500. PMLR, 2021.
Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, and Jingjing Liu. Safe
offline reinforcement learning with feasibility-guided diffusion model. arXiv preprint arXiv:2401.10700, 2024.

APPENDIX
Our appendix includes the following:
â€¢ Proofs of the theoretical claims presented in the main paper.
â€¢ Details of our experimental settings.
â€¢ Detailed numerical results from the ablation study investigating the impact of Î± on ComaDICEâ€™s performance.
â€¢ An ablation study assessing ComaDICEâ€™s performance with different forms of f-divergence functions.
â€¢ An ablation study comparing ComaDICEâ€™s performance using 1-layer versus 2-layer mixing networks.

C ONTENTS
A Missing Proofs

15

A.1 Proof of Proposition 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

A.2 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

A.3 Proof of Proposition 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

B Additional Details

18

B.1 Offline Multi-Agent Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

B.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

B.3 Additional Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

B.4 Ablation Study: Different Values of Alpha . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

B.5 Ablation Study: Different Forms of f-divergence . . . . . . . . . . . . . . . . . . . . . . . . . . .

26

B.6 Ablation Study: Different Types of Mixer Network . . . . . . . . . . . . . . . . . . . . . . . . .

29

A

M ISSING P ROOFS

A.1

P ROOF OF P ROPOSITION 4.1

Proposition. The minimax problem in 6 is equivalent to minÎ½ tot

n
o
e tot ) , where
L(Î½



 tot
e tot ) = (1 âˆ’ Î³)Esâˆ¼p [Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot Î±f âˆ— AÎ½ (s, a)
L(Î½
0
Î±
where f âˆ— is convex conjugate of f , i.e., f âˆ— (y) = suptâ‰¥0 {ty âˆ’ f (t)}. Moreover, if Î½ tot is parameterized by Î¸, the
e tot ) w.r.t. Î¸ is given as
first order derivative of L(Î½


totâˆ—
e tot ) = (1 âˆ’ Î³)Esâˆ¼p [âˆ‡Î¸ Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot âˆ‡Î¸ Atot
âˆ‡Î¸ L(Î½
(s, a)
Î½ (s, a)wÎ½
0
where wÎ½totâˆ— (s, a) = max{0, f â€²
of f .

âˆ’1

â€²âˆ’1
(Atot
(Â·) is the inverse function of the first-order derivative
Î½ (s, a)/Î±)}, where f

Proof. We write the Lagrange dual function as:
L(Î½

tot

Ï€ tot

,Ï

âˆ’

)=E

(o,a)âˆ¼ÏÏ€ tot

X


  Ï€ tot
Ï
(s, a)
[r(o, a)] âˆ’ Î±E(s,a)âˆ¼ÏÂµtot f
ÏÂµtot (s, a)

ï£«
ï£¶
X
X
Î½ tot (s) ï£­
ÏÏ€ tot (s, aâ€² ) âˆ’ (1 âˆ’ Î³)p0 (s) âˆ’ Î³
ÏÏ€ tot (sâ€² , aâ€² )P (s|aâ€² , sâ€² )ï£¸
aâ€²

s

aâ€² ,sâ€²

  Ï€ tot

Ï
(s, a)
tot
Âµ
=
Î½ (s)(1 âˆ’ Î³)p0 (s) âˆ’ Î±E(s,a)âˆ¼Ï tot f
ÏÂµtot (s, a)
s
X

+
ÏÂµtot (s, a) r(o, a) + Î³Esâ€² âˆ¼P (Â·|s,a) Î½ tot (sâ€² ) âˆ’ Î½ tot (s)
X

s,a




= (1 âˆ’ Î³)Esâˆ¼p0 [Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot âˆ’Î±f wÎ½tot (s, a) + wÎ½tot (s, a)Atot
Î½ (s, a)

(12)

Ï€ tot

(s,a)
where wÎ½tot (s, a) = ÏÏÂµtot (s,a)
. We now see that, for each (s, a), each component âˆ’Î±f (wÎ½tot (s, a)) +
tot
tot
wÎ½ (s, a)AÎ½ (s, a) is maximized at:

 tot

AÎ½ (s, a)
tot
tot
tot
âˆ—
max
âˆ’Î±f
w
(s,
a)
+
w
(s,
a)A
(s,
a)
=
f
Î½
Î½
Î½
wtot â‰¥0
Î±

where f âˆ— is the (variant) convex conjugate of the convex function f . We then obtain:

 tot

AÎ½ (s, a)
tot
tot
tot
tot
âˆ—
e
max L(Î½ , w ) = L(Î½ ) = (1 âˆ’ Î³)Esâˆ¼p0 [Î½ (s)] + E(s,a)âˆ¼ÏÂµtot Î±f
wtot â‰¥0
Î±
Moreover, consider the maximization problem maxwtot â‰¥0 T (wtot (s, a)) = âˆ’Î±f (wÎ½tot (s, a))+wÎ½tot (s, a)Atot
Î½ (s, a).
Taking its first-order derivative w.r.t wtot (s, a) yields:
âˆ’Î±f â€² (wtot (s, a)) + Atot
Î½ (s, a)
 tot

 tot

AÎ½ (s,a)
AÎ½ (s,a)
So, if f â€²âˆ’1
â‰¥ 0, then wtotâˆ— (s, a) = f â€²âˆ’1
â‰¥ 0 is optimal for the maximization problem.
Î±
Î±
 tot

 tot

AÎ½ (s,a)
tot
tot
â€²âˆ’1 AÎ½ (s,a)
Otherwise, if f â€²âˆ’1
<
0,
we
see
that
T
(w
(s,
a))
is
increasing
when
w
(s,
a)
â‰¤
f
Î±
Î±
 tot

tot
â€²âˆ’1 AÎ½ (s,a)
and decreasing when w (s, a) â‰¥ f
, implying that the maximization problem has an optimal
Î±
solution at wtotâˆ— (s, a) = 0. So, putting all together, wÎ½totâˆ— (s, a) = max{0, f â€²
maximization problem maxwtot â‰¥0 T (wtot (s, a)).

âˆ’1

(Atot
Î½ (s, a)/Î±)} is optimal for the

e tot ), we note that, for any y âˆˆ R, âˆ‡f âˆ— (y) = tâˆ— , where y âˆ— = argmax (ty âˆ’ f (t)). Thus,
To get derivatives of L(Î½
tâ‰¥0

 tot
âˆ— AÎ½ (s,a)
can be computed as:
the first-order derivative of f
Î±
 tot

AÎ½ (s, a)
âˆ‡Î¸ Atot
Î½ (s, a) totâˆ—
âˆ—
âˆ‡Î¸ f
=
w (s, a)
Î±
Î±
which implies:


e tot ) = (1 âˆ’ Î³)Esâˆ¼p [âˆ‡Î¸ Î½ tot (s)] + E(s,a)âˆ¼ÏÂµtot âˆ‡Î¸ Atot (s, a)wtotâˆ— (s, a)
âˆ‡Î¸ L(Î½
Î½
Î½
0
we complete the proof.

A.2

P ROOF OF T HEOREM 4.2

Theorem. Assume the mixing network MÎ¸ [Â·] is constructed with non-negative weights and convex activations,
e , Î¸) is convex in Î½ .
then L(Î½
e , Î¸).
Proof. We first introduce the following lemma, which is essential to validate the convexity of L(Î½
Lemma A.1. If the mixing network are multi-level feed-forward, constructed with non-negative weights and convex
activations, then MÎ¸ [Î½ (s)] and MÎ¸ [q(s, a) âˆ’ Î½ (s)] are convex in Î½
Proof. To simplify the proof, we first prove a general result stating that if MÎ¸ [X] is a multi-level feed-forward
network with non-negative weights and convex activations, then MÎ¸ [X] is convex in X. To start, we note that any
N -layer feed-forward network with input X can be defined recursively as
F 0 (X) = X

(13)




F n (X) = Ïƒ n F nâˆ’1 (X) Ã— Wn + bn , n = 1, . . . , N

(14)

where Ïƒ n is a set of activation functions applied to each element of vector F nâˆ’1 (X), and Wn and bn are the weights
and biases, respectively, at layer n. Therefore, we will prove the result by induction, i.e., F n (X) is convex and
non-decreasing in X for n = 0, . . .. Here we note that F n (X) is a vector, so when we say â€œF n (X) is convex and
non-decreasing in X,â€ it means each element of F n (X) is convex and non-decreasing in X.
We first see that the claim indeed holds for n = 0. Now let us assume that F nâˆ’1 (X) is convex and non-decreasing
in X; we will prove that F n (X) is also convex and non-decreasing in X. The non-decreasing property can be easily
verified as we can see, given two vectors X and Xâ€² such that X â‰¥ Xâ€² (element-wise comparison), we have the
following chain of inequalities:
(a)

F nâˆ’1 (X) â‰¥ F nâˆ’1 (Xâ€² )
(b)

Ïƒ n (F nâˆ’1 (X)) â‰¥ Ïƒ n (F nâˆ’1 (Xâ€² ))
(c)

Ïƒ n (F nâˆ’1 (X)) Ã— Wn + bn â‰¥ Ïƒ n (F nâˆ’1 (Xâ€² )) Ã— Wn + bn
where (a) is due to the induction assumption that F nâˆ’1 (X) is non-decreasing in X, (b) is because Ïƒ n is also
non-decreasing, and (c) is because the weights Wn are non-negative.
To verify the convexity of F n (X), we will show that for any X, Xâ€² , and any scalar Î± âˆˆ (0, 1), the following holds:
Î±F n (X) + (1 âˆ’ Î±)F n (X) â‰¥ F n (Î±X + (1 âˆ’ Î±)Xâ€² )

(15)

To this end, we write:


Î±F n (X) + (1 âˆ’ Î±)F n (Xâ€² ) = Î±Ïƒ n (F nâˆ’1 (X)) + (1 âˆ’ Î±)Ïƒ n (F nâˆ’1 (Xâ€² )) Ã— Wn + bn


(d) 
â‰¥ Ïƒ n Î±F nâˆ’1 (X) + (1 âˆ’ Î±)F nâˆ’1 (Xâ€² ) Ã— Wn + bn


(e) 
â‰¥ Ïƒ n F nâˆ’1 (Î±X + (1 âˆ’ Î±)Xâ€² ) Ã— Wn + bn
= F n (Î±X + (1 âˆ’ Î±)Xâ€² )
where (d) is due to the assumption that activation functions Ïƒ n are convex and Wn â‰¥ 0, and (e) is because
Î±F nâˆ’1 (X) + (1 âˆ’ Î±)F nâˆ’1 (Xâ€² ) â‰¥ F nâˆ’1 (Î±X + (1 âˆ’ Î±)Xâ€² ) (because F nâˆ’1 (X) is convex in X, by the induction
assumption), and the activation functions Ïƒ n are non-decreasing and Wn â‰¥ 0. So, we have:
Î±F n (X) + (1 âˆ’ Î±)F n (Xâ€² ) â‰¥ F n (Î±X + (1 âˆ’ Î±)Xâ€² )
implying that F n (X) is convex in X. We then complete the induction proof and conclude that F n (X) is convex and
non-decreasing in X for any n = 0, . . . , N .
From the result above, since both Î½ (s) and q(s, a) âˆ’Î½ (s) are linear in Î½ , it follows that MÎ¸ [Î½ (s)] and MÎ¸ [q(s, a) âˆ’
Î½ (s)] are convex with respect to Î½ .
e , Î¸) with respect to Î½ . Directly verifying the convexity of this
We are now ready to prove the convexity of L(Î½


function is challenging, as it involves some complicated components such as f âˆ—
to analyze. However, we recall that:
e , Î¸) = max L(Î½ , Î¸, wtot ),
L(Î½
tot
w

â‰¥0

MÎ¸ [q(s,a)âˆ’Î½ (s)]
Î±

, which is difficult

where

L(Î½ , Î¸, wtot ) = (1 âˆ’ Î³)Esâˆ¼p0 [MÎ¸ [Î½ (s)]]



+ E(s,a)âˆ¼ÏÂµtot âˆ’Î±f wÎ½tot (s, a) + wÎ½tot (s, a)MÎ¸ [q(s, a) âˆ’ Î½ (s)] .

From Lemma A.1, we know that MÎ¸ [Î½ (s)] and MÎ¸ [q(s, a) âˆ’ Î½ (s)] are convex in Î½ , thus L(Î½ , Î¸, wtot ) is also
e , Î¸) as follows. Let Î½ 1 and Î½ 2 be
convex in Î½ . We now follow the standard approach to verify the convexity of L(Î½
two feasible value functions. Given any Î² âˆˆ (0, 1), we will prove that:
e Î½ 1 , Î¸) + (1 âˆ’ Î²)L(Î½
e Î½ 2 , Î¸) â‰¥ L(Î²Î½
e Î½ 1 + (1 âˆ’ Î²)Î½Î½ 2 , Î¸).
Î² L(Î½

(16)

e Î½ , Î¸) = maxwtot â‰¥0 L(Î½Î½ , Î¸, wtot ),
To see why this should hold, we recall that L(Î½Î½ , Î¸, wtot ) is convex in Î½ and L(Î½
leading to the following chain of inequalities:
e 1 , Î¸) + (1 âˆ’ Î²)L(Î½
e 2 , Î¸) = Î² max L(Î½ 1 , Î¸, wtot ) + (1 âˆ’ Î²) max L(Î½ 2 , Î¸, wtot )
Î² L(Î½
wtot
wtot

1
tot
â‰¥ max
Î²L(Î½ , Î¸, w ) + (1 âˆ’ Î²)L(Î½ 2 , Î¸, wtot )
wtot

â‰¥ max
L(Î²Î½ 1 + (1 âˆ’ Î²)Î½ 2 , Î¸, wtot )
tot
w

e 1 + (1 âˆ’ Î²)Î½ 2 , Î¸).
= L(Î²Î½
e , Î¸) in Î½ , as desired.
The last inequality directly confirms Eq. 16, implying the convexity of L(Î½
A.3

P ROOF OF P ROPOSITION 4.3

âˆ—
Proposition. Let Ï€ âˆ— be the optimal solution to 4.3. Then Ï€tot
(a|s) =
global weighted BC problem 8.

Q

âˆ—
iâˆˆN Ï€i (ai |oi ) is also optimal for the

Q
âˆ—
(a|s) = iâˆˆN Ï€iâˆ— (ai |oi ) is optimal for the global WBC problem 8, we need to verify that
Proof. To prove that Ï€tot




E(s,a)âˆ¼ÏÂµtot wtotâˆ— (s, a) log Ï€ tot (a|s) â‰¤ E(s,a)âˆ¼ÏÂµtot wtotâˆ— (s, a) log Ï€ âˆ—tot (a|s)
for any global policy Ï€ tot âˆˆ Î tot .
Since Ï€ tot is decomposable, there exist local policies Ï€i such that
Y
Ï€ tot (a|s) =
Ï€i (ai |oi ).
iâˆˆN

As a result, we have the following inequalities:
"
#
X
 totâˆ—

totâˆ—
E(s,a)âˆ¼ÏÂµtot w (s, a) log Ï€ tot (a|s) = E(s,a)âˆ¼ÏÂµtot w (s, a)
log Ï€i (ai |oi )
iâˆˆN

=

X



E(s,a)âˆ¼ÏÂµtot w

totâˆ—


(s, a) log Ï€i (ai |oi )

iâˆˆN

â‰¤

X
iâˆˆN

=

X



max
E(s,a)âˆ¼ÏÂµtot wtotâˆ— (s, a) log Ï€iâ€² (ai |oi )
â€²
Ï€i



E(s,a)âˆ¼ÏÂµtot wtotâˆ— (s, a) log Ï€iâˆ— (ai |oi )

iâˆˆN



= E(s,a)âˆ¼ÏÂµtot wtotâˆ— (s, a) log Ï€ âˆ—tot (a|s) ,
which directly implies that Ï€ âˆ—tot is optimal for the global WBC problem 8.

B

A DDITIONAL D ETAILS

B.1

O FFLINE M ULTI -AGENT DATASETS

Instances

Trajectories

Samples

Agents

State
dim

Obs
dim

Action
dim

Average
returns

2c vs 64zg

poor
medium
good

0.3K
1.0K
1.0K

21.7K
75.9K
118.4K

2
2
2

675
675
675

478
478
478

70
70
70

8.9Â±1.0
13.0Â±1.4
19.9Â±1.3

5m vs 6m

poor
medium
good

1.0K
1.0K
1.0K

113.7K
138.6K
138.7K

5
5
5

156
156
156

124
124
124

12
12
12

8.5Â±1.2
11.0Â±0.6
20.0Â±0.0

6h vs 8z

poor
medium
good

1.0K
1.0K
1.0K

145.5K
177.1K
228.2K

6
6
6

213
213
213

172
172
172

14
14
14

9.1Â±0.8
12.0Â±1.3
17.8Â±2.1

corridor

poor
medium
good

1.0K
1.0K
1.0K

307.6K
756.1K
601.0K

6
6
6

435
435
435

346
346
346

30
30
30

4.9Â±1.7
13.1Â±1.3
19.9Â±1.0

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

1.0K
1.0K
1.0K
1.0K
1.0K

60.8K
68.3K
62.9K
76.7K
65.0K

5
10
10
20
20

130
310
327
820
901

92
182
191
362
389

11
16
17
26
29

16.8Â±6.3
15.7Â±5.2
15.3Â±5.7
16.2Â±4.7
14.0Â±4.5

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

1.0K
1.0K
1.0K
1.0K
1.0K

47.6K
56.4K
52.5K
63.0K
51.3K

5
10
10
20
20

120
290
306
780
858

82
162
170
322
346

11
16
17
26
29

15.2Â±7.2
14.7Â±6.2
12.1Â±5.7
14.0Â±6.0
11.7Â±5.7

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

1.0K
1.0K
1.0K
1.0K
1.0K

27.5K
31.9K
30.9K
35.4K
32.8K

5
10
10
20
20

120
290
306
780
858

82
162
170
322
346

11
16
17
26
29

10.4Â±5.0
14.7Â±6.0
12.0Â±5.1
12.3Â±4.2
10.8Â±4.0

Hopper

expert
medium
m-replay
m-expert

1.5K
4.0K
4.2K
5.5K

999K
915K
1311K
1914K

3
3
3
3

42
42
42
42

14
14
14
14

1
1
1
1

2452.0Â±1097.9
723.6Â±211.7
746.4Â±671.9
1190.6Â±973.4

Ant

expert
medium
m-replay
m-expert

1.0K
1.0K
1.8K
2.0K

1000K
1000K
1750K
2000K

2
2
2
2

226
226
226
226

113
113
113
113

4
4
4
4

2055.1Â±22.1
1418.7Â±37.0
1029.5Â±141.3
1736.9Â±319.6

Half
Cheetah

expert
medium
m-replay
m-expert

1.0K
1.0K
1.0K
2.0K

1000K
1000K
1000K
2000K

6
6
6
6

138
138
138
138

23
23
23
23

1
1
1
1

2785.1Â±1053.1
1425.7Â±520.1
655.8Â±590.4
2105.4Â±1073.2

Table 3: Overview of datasets used in experiments, including details of trajectories, samples, agent counts, and
state, observation, and action space dimensions across SMACv1, SMACv2, and MaMujoco environments, with
average returns indicating performance levels.

B.2

I MPLEMENTATION D ETAILS

Our experiments were implemented using PyTorch and executed in parallel on a single NVIDIAÂ® H100 NVL
Tensor Core GPU. Our study required running a large number of sub-tasks, specifically 1,365 in total (i.e., 39
instances across 7 algorithms with 5 different random seeds each).

Update

Update

Update

Hyper Network

+

Linear

Mixing Network
Linear

Mixing Network

...
...

Agent
Network

...

Agent
Network

Figure 3: Our ComaDICE model architecture.

The offline datasets for each instance are substantial, reaching sizes of up to 7.4 GB. To manage this, we developed a
preprocessing step designed to optimize data handling and improve computational efficiency. This process involves
reading all transitions from each dataset and combining individual trajectory files into a single large NumPy object
that contains batches of trajectories. In this step, we define the data type for each element, such as states (float32),
actions (int64), and dones (bool), ensuring consistent and efficient data storage. The processed data is then saved
into a compressed NumPy file, which significantly boosts computing performance.
Despite these optimizations, loading the entire dataset still requires a large amount of RAM. By leveraging parallel
processing and efficient data management strategies, we effectively managed the extensive computational and
memory demands of our experiments. This approach allowed us to handle the large-scale data and complex
computations necessary for our study.
B.2.1

H YPER - PARAMETERS

Hyperparameter

Value

Optimizer
Learning rate (Q-value and policy networks)
Tau (Ï„ )
Gamma (Î³)
Batch size
Agent hidden dimension
Mixer hidden dimension
Number of seeds
Number of episodes per evaluation step
Number of evaluation steps
Lambda scale (Î»)
Alpha (Î±)
f-divergence

Adam
1 Ã— 10âˆ’4
0.005
0.99
128
256
64
5
32
100
1.0
10
soft-Ï‡2

Table 4: Hyperparameters for our algorithm

In our study, we developed two versions of our algorithm: a continuous version for MaMujoco using Gaussian
distributions (torch.distributions.Normal), and a discrete version for SMACv1 and SMACv2 using Categorical
distributions (torch.distributions.Categorical). In the discrete setting, action probabilities are computed using
softmax over available actions only, ensuring zero probability for unavailable actions, which enhances the accuracy
of log likelihood calculations. Key hyperparameters are listed on the Table 4. Experiments were conducted with 5
seeds, 32 episodes per evaluation step, and 100 evaluation steps.
B.3

We evaluate the performance of our ComaDICE algorithm using two key metrics: mean and standard deviation
(std) of returns and winrates. Returns measure the average rewards accumulated by agents, calculated across five
random seeds to ensure robustness, while winrates, applicable only to competitive environments like SMACv1 and
SMACv2, indicate the success rate against other agents. For cooperative settings such as MaMujoco, winrates are
not applicable. We also include figures showing evaluation curves, highlighting how each methodâ€™s performance
evolves during training with offline datasets. These metrics and visualizations provide a comprehensive overview
of our algorithmâ€™s effectiveness and consistency in various MARL tasks.
B.3.1

R ETURNS

Tables 5, 6, and 7 present the returns from our experimental results across the SMACv1, SMACv2, and Multi-Agent
MuJoCo environments, highlighting the performance of our proposed algorithm, ComaDICE, alongside baseline
methods such as BC, BCQ, CQL, ICQ, OMAR, and OMIGA. Our results demonstrate that ComaDICE consistently
achieves superior returns, particularly excelling in more complex difficulty tasks. Figures 4, 5, and 6 illustrate the
learning curves for these algorithms, showing that ComaDICE not only outperforms other algorithms in terms of
mean returns but also exhibits lower standard deviation, indicating robust and stable performance. This suggests that
ComaDICE effectively handles distributional shifts in offline settings. These findings underscore our algorithmâ€™s
adaptability and effectiveness in diverse multi-agent coordination scenarios, setting a new benchmark in offline
MARL.
Instances

BC

BCQ

CQL

ICQ

OMAR

OMIGA

ComaDICE

2c vs 64zg

poor
medium
good

11.6Â±0.4
13.4Â±1.9
17.9Â±1.3

12.5Â±0.2
15.6Â±0.4
19.1Â±0.3

10.8Â±0.5
12.8Â±1.6
18.5Â±1.0

12.6Â±0.2
15.6Â±0.6
18.8Â±0.2

11.3Â±0.5
10.2Â±0.2
17.3Â±0.8

13.0Â±0.7
16.0Â±0.2
19.1Â±0.3

12.1Â±0.5
16.3Â±0.7
20.3Â±0.1

5m vs 6m

poor
medium
good

7.0Â±0.5
7.0Â±0.8
7.0Â±0.5

7.6Â±0.4
7.6Â±0.1
7.8Â±0.1

7.4Â±0.1
7.8Â±0.1
8.1Â±0.2

7.3Â±0.2
7.8Â±0.3
7.9Â±0.3

7.3Â±0.4
7.1Â±0.5
7.4Â±0.6

7.5Â±0.2
7.9Â±0.6
8.3Â±0.4

8.1Â±0.5
8.7Â±0.4
8.7Â±0.5

6h vs 8z

poor
medium
good

8.6Â±0.8
9.5Â±0.3
10.0Â±1.7

10.8Â±0.2
11.8Â±0.2
12.2Â±0.2

10.8Â±0.5
11.3Â±0.3
10.4Â±0.2

10.6Â±0.1
11.1Â±0.3
11.8Â±0.1

10.6Â±0.2
10.4Â±0.2
9.9Â±0.3

11.3Â±0.2
12.2Â±0.2
12.5Â±0.2

11.4Â±0.6
12.8Â±0.2
13.1Â±0.5

corridor

poor
medium
good

2.9Â±0.6
7.4Â±0.8
10.8Â±2.6

4.5Â±0.9
10.8Â±0.9
15.2Â±1.2

4.1Â±0.6
7.0Â±0.7
5.2Â±0.8

4.5Â±0.3
11.3Â±1.6
15.5Â±1.1

4.3Â±0.5
7.3Â±0.7
6.7Â±0.7

5.6Â±0.3
11.7Â±1.3
15.9Â±0.9

6.4Â±0.5
12.9Â±0.6
18.0Â±0.1

Table 5: Comparison of average returns for ComaDICE and baselines on SMACv1 benchmarks.

BC

5

5

A DDITIONAL E XPERIMENTAL D ETAILS

100
0
40%
20%
0%
100

0

ICQ

OMAR

OMIGA

ComaDICE (ours)

8

8

8

15

15

6

6

6

10

10

10

4

4

0%

100
0
60%
40%
20%
0%

CQL

15

protoss_10_vs_10
0
50
2c vs 64zg40%
30%
poor 20%
10%

60%
40%
20%
0%

BCQ

12
terran_10_vs_10
50
100
10

0

50

0
50
0
50 protoss_20_vs_20
0protoss_10_vs_11
50
5m vs 6m40%
2c
vs 64zg
2c vs 64zg
30%
40%
poor 20%
good
20%
medium
10%

0%

12

0

30% 10
20%
10%
0%
0

6h vs 8z100 0
zerg_10_vs_10
50
poor 30%
20%
10%

12
terran_10_vs_11
50
100
10

0

50

0%

0

40%
20%
0%50

15
terran_20_vs_20
100
1050
5
0

50

4

protoss_20_vs_23
0
50
50
5m vs 6m
5m vs 6m
good
medium

0

15

0 terran_20_vs_23
50
100
10
20% 10
15% 5
5
10%
5%
0%
0
0
50
15

corridor100 0
6h vs0 8zzerg_20_vs_20
zerg_10_vs_11
6h vs 50
8z
100
50
poor 20%
good
medium
20%
10%

10%

50

corridor
zerg_20_vs_23
corridor
50
100
good
medium

0%comparing the returns achieved
0%
Figure 4:0%
Evaluation of SMACv1 tasks
by ComaDICE and baselines.
50

100

0

50

100

0

50

100

0

50

100

Instances

BCQ

CQL

ICQ

OMAR

OMIGA

ComaDICE

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

13.2Â±0.7
12.0Â±1.9
11.2Â±0.5
13.1Â±0.5
11.2Â±0.5

6.8Â±1.6
7.7Â±1.3
5.2Â±1.4
4.8Â±0.6
3.5Â±0.6

9.3Â±1.6
11.3Â±0.9
7.9Â±0.8
10.5Â±0.9
5.6Â±0.7

10.7Â±1.2
10.4Â±1.6
10.3Â±0.7
11.8Â±0.5
10.2Â±0.7

8.9Â±0.8
8.8Â±0.6
8.0Â±0.3
9.1Â±0.5
7.4Â±0.7

14.3Â±1.4
14.2Â±1.5
12.1Â±0.5
14.0Â±0.9
13.0Â±1.1

14.4Â±1.1
14.6Â±1.8
13.2Â±0.9
14.8Â±1.0
13.3Â±0.9

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.8Â±1.4
10.3Â±0.3
9.0Â±0.7
10.8Â±0.8
7.2Â±1.0

6.4Â±1.1
4.6Â±0.4
3.6Â±1.1
3.9Â±0.6
1.2Â±1.0

6.5Â±0.9
6.8Â±0.6
5.5Â±0.2
4.3Â±0.6
1.6Â±0.2

6.8Â±0.6
8.7Â±1.4
5.5Â±0.9
8.3Â±0.3
5.3Â±0.5

6.9Â±0.6
7.6Â±1.0
5.9Â±0.7
7.3Â±0.4
5.1Â±0.3

10.5Â±1.2
10.1Â±0.6
8.8Â±1.4
10.5Â±0.7
7.9Â±0.6

10.7Â±1.5
11.8Â±0.9
9.4Â±0.9
11.8Â±0.5
8.2Â±0.7

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.5Â±2.2
11.0Â±0.8
9.2Â±1.1
9.3Â±0.5
8.5Â±0.7

6.6Â±0.2
7.3Â±1.0
7.6Â±0.9
3.7Â±0.4
3.3Â±0.3

6.7Â±0.5
7.2Â±0.3
6.7Â±0.4
4.7Â±0.3
4.1Â±0.6

6.5Â±0.9
7.7Â±1.1
6.8Â±1.0
6.9Â±0.5
6.9Â±0.5

7.7Â±0.9
7.5Â±0.8
6.5Â±1.0
6.9Â±0.8
5.7Â±0.4

8.9Â±1.1
11.8Â±1.6
9.5Â±1.2
9.2Â±0.5
9.8Â±0.6

10.7Â±2.0
11.5Â±1.0
11.0Â±0.9
9.4Â±1.2
10.5Â±0.8

Table 6: Comparison of average returns for ComaDICE and baselines on SMACv2 tasks.

BC

5

5

BC

100
0
40%
20%
0%
100

0

ICQ

OMIGA

ComaDICE (ours)

15

15

10

10

10

10

10

5

5

5

5

5

protoss_10_vs_11
0
50
protoss
40%
protoss
40%
30%
20%
10 vs 1020%
5 vs 5
10%
50

10
terran_10_vs_10
50
100
5

0

30%
20%
10%
0%

0%

0

10
terran_10_vs_11
50
100
5

0

50

50

40%
20%
0%

terran
terran zerg_10_vs_11
zerg_10_vs_10
50
100
0
50
100
10
vs 10
5
vs
5
30%
20%
10%
0%

10

50

OMAR

15

0%

100
0
60%
40%
20%
0%

CQL

15

protoss_10_vs_10
0
60%
40%
20%
0%

BCQ

15

5

20%
10%
0%

10

100

0

0

50

zerg
5 vs 5

5

50
0

100
50

zerg
10 vs 10

protoss_20_vs_20
0
50
50 protoss_20_vs_23
0
50
40%
protoss
protoss
protoss
30%
20%
10 vs 11
20 vs 20
20 vs 23
10%
0%

10

0

5

10
terran_20_vs_20
50
100
5

0

0

50

0

10
terran_20_vs_23
50
100
5

0

50

terran
terran zerg_20_vs_23
terran
zerg_20_vs_20
10020 vs
0 20
50 20 vs 23
100
10 vs50
11

10

10

5

5

0

50
0

0
20%
15%
10%
5%
0% 50

50

zerg
10 vs 11

20%
10%
0%

100
0

10

0
50

zerg
20 vs 20

5

50
0

100
50

zerg
20 vs 23

Figure 5: Evaluation of SMACv2 tasks comparing the returns achieved by ComaDICE and baselines.

Instances

medium

m-replay

m-expert

Hopper

BC
BCQ
CQL
ICQ
OMAR
OMIGA
ComaDICE

209.8Â±191.1
77.9Â±58.0
159.1Â±313.8
754.7Â±806.3
2.4Â±1.5
859.6Â±709.5
2827.7Â±62.9

511.9Â±7.4
44.6Â±20.6
401.3Â±199.9
501.8Â±14.0
21.3Â±24.9
1189.3Â±544.3
822.6Â±66.2

133.3Â±53.5
26.5Â±24.0
31.4Â±15.2
195.4Â±103.6
3.3Â±3.2
774.2Â±494.3
906.3Â±242.1

155.3Â±111.5
54.3Â±23.7
64.8Â±123.3
355.4Â±373.9
1.4Â±0.9
709.0Â±595.7
1362.4Â±522.9

Ant

BC
BCQ
CQL
ICQ
OMAR
OMIGA
ComaDICE

2046.3Â±6.2
1317.7Â±286.3
1042.4Â±2021.6
2050.0Â±11.9
312.5Â±297.5
2055.5Â±1.6
2056.9Â±5.9

1421.1Â±7.9
1059.6Â±91.2
533.9Â±1766.4
1412.4Â±10.9
-1710.0Â±1589.0
1418.4Â±5.4
1425.0Â±2.9

994.0Â±20.3
950.8Â±48.8
234.6Â±1618.3
1016.7Â±53.5
-2014.2Â±844.7
1105.1Â±88.9
1122.9Â±61.0

1561.7Â±64.8
1020.9Â±242.7
800.2Â±1621.5
1590.2Â±85.6
-2992.8Â±7.0
1720.3Â±110.6
1813.9Â±68.4

Half
Cheetah

BC
BCQ
CQL
ICQ
OMAR
OMIGA
ComaDICE

3251.2Â±386.8
2992.7Â±629.7
1189.5Â±1034.5
2955.9Â±459.2
-206.7Â±161.1
3383.6Â±552.7
4082.9Â±45.7

2280.3Â±178.2
2590.5Â±1110.4
1011.3Â±1016.9
2549.3Â±96.3
-265.7Â±147.0
3608.1Â±237.4
2664.7Â±54.2

1886.2Â±390.8
-333.6Â±152.1
1998.7Â±693.9
1922.4Â±612.9
-235.4Â±154.9
2504.7Â±83.5
2855.0Â±242.2

2451.9Â±783.0
3543.7Â±780.9
1194.2Â±1081.0
2834.0Â±420.3
-253.8Â±63.9
2948.5Â±518.9
3889.7Â±81.6

Table 7: Comparison of average returns for ComaDICE and baselines on MaMujoco benchmarks.

BC

5

5

expert

protoss_10_vs_10
60%
40%
20%
0%
100
0
60%
40%
20%
0%
100
0
40%
20%
0%
100

0

30%
20%
10%
0%

zerg_10_vs_10
50
100

100

ICQ

OMAR

OMIGA

3k

3k

3k

2k

2k

2k

2k

1k

1k

1k

1k

0

0

0

0

0
1k

protoss_10_vs_11
0
50
0
50
0
50 protoss_20_vs_20
Hopper 40%
Hopper
Hopper
30%
40%
20%
m-replay20%
expert
medium
10%
0%

2k

terran_10_vs_11
50
1k 100

0

0

0

0

30%
20%
10% 4k
0%

50

CQL

3k

40%
30%
20%
10%
0% 2k

terran_10_vs_10
50
100

BCQ

50

0

0

50

0
0

terran_20_vs_20
100
1k 50
0
0

50

50

20%
15%
10%
5%
0%

50

HalfCheetah
expert

2k

10%
0%

2k

0

100

0

0

50

HalfCheetah
medium

0

terran_20_vs_23
50
100

0
1k
0
0

10% 4k
0%

4k
2k

protoss_20_vs_23
50
Hopper
m-expert

0

zerg_10_vs_11
Ant 100 0
Ant50
100 Ant
0 zerg_20_vs_20
50
m-replay20%
expert
medium
20%
4k

2k

40%
20%
0%

0%

2k

0

ComaDICE (ours)

50
0

100
50

HalfCheetah
m-replay

50

zerg_20_vs_23
Ant50
100
m-expert

2k

0

50

0

0

100

50

HalfCheetah
m-expert

Figure 6: Evaluation of MaMujoco tasks comparing the returns achieved by ComaDICE and baselines.

B.3.2

W INRATES

In this section, we analyze the winrates of our ComaDICE algorithm across various multi-agent reinforcement
learning scenarios. Winrates are crucial in competitive environments like SMACv1 and SMACv2, as they measure
the algorithmâ€™s success against other agents. Our results demonstrate that ComaDICE consistently achieves higher
winrates compared to baseline methods. Notably, ComaDICE performs well across both simple and complex tasks,
reflecting its robustness and adaptability. As shown in Tables 8 and 9, as well as Figures 7 and 8, ComaDICE not
only excels in average winrates but also exhibits lower variance, indicating stable performance across different
trials. These findings highlight ComaDICEâ€™s ability to effectively manage distributional shifts.

Instances

BCQ

CQL

ICQ

OMAR

OMIGA

ComaDICE

2c vs 64zg

poor
medium
good

0.0Â±0.0
1.9Â±1.5
31.2Â±9.9

0.0Â±0.0
2.5Â±3.6
35.6Â±8.8

0.0Â±0.0
2.5Â±3.6
44.4Â±13.0

0.0Â±0.0
1.9Â±1.5
28.7Â±4.6

0.0Â±0.0
1.2Â±1.5
28.7Â±9.1

0.0Â±0.0
6.2Â±5.6
40.6Â±9.5

0.6Â±1.3
8.8Â±7.0
55.0Â±1.5

5m vs 6m

poor
medium
good

2.5Â±1.3
1.9Â±1.5
2.5Â±2.3

1.2Â±1.5
1.2Â±1.5
1.9Â±2.5

1.2Â±1.5
2.5Â±1.2
1.9Â±1.5

1.2Â±1.5
1.2Â±1.5
3.8Â±2.3

0.6Â±1.2
0.6Â±1.2
3.8Â±1.2

6.9Â±1.2
2.5Â±3.1
6.9Â±1.2

4.4Â±4.2
7.5Â±2.5
8.1Â±3.2

6h vs 8z

poor
medium
good

0.0Â±0.0
1.9Â±1.5
8.8Â±1.2

0.0Â±0.0
1.9Â±1.5
8.8Â±3.6

0.0Â±0.0
1.9Â±1.5
7.5Â±1.5

0.0Â±0.0
2.5Â±1.2
9.4Â±2.0

0.0Â±0.0
1.9Â±1.5
0.6Â±1.3

0.0Â±0.0
1.2Â±1.5
5.6Â±3.6

1.9Â±3.8
3.1Â±2.0
11.2Â±5.4

corridor

poor
medium
good

0.0Â±0.0
15.0Â±2.3
30.6Â±4.1

0.0Â±0.0
23.1Â±1.5
42.5Â±6.4

0.0Â±0.0
14.4Â±1.5
5.6Â±1.2

0.6Â±1.3
22.5Â±3.1
42.5Â±6.4

0.0Â±0.0
11.9Â±2.3
3.1Â±0.0

0.0Â±0.0
23.8Â±5.1
41.9Â±6.4

0.6Â±1.3
27.3Â±3.4
48.8Â±2.5

Table 8: Comparison of average winrates for ComaDICE and baselines on SMACv1 benchmarks.

BC
40%
20%

5

5

BC

0%

100
0
60%
40%
20%
0%
100
0
40%
20%
0%
100

0

terran_10_vs_10
10%
50
100

30%
20%
10%
0%
50

5%
0

40%

20%

20%

ICQ

OMAR

OMIGA

0%

10% 100
010%terran_10_vs_11
50
0
40%
5%
5%
20%
0% 50
0

10%
0%

0

ComaDICE (ours)

10%

10%

10%

5%

5%

5%

protoss_10_vs_11
0
50 protoss_20_vs_20
50
0
50
2c
vs
5m vs 6m40%
2c vs 64zg
30%
40% 64zg
20%good
poor 20%
medium
10%

zerg_10_vs_10
6h vs 100
8z
50
0
poor 30%
20%
50

CQL

40%

protoss_10_vs_10
0
50
40%
2c vs 64zg
30%
poor 20%
10%

60%
40%
20%
0%

BCQ

protoss_20_vs_23
0
50
0
50
5m vs 6m
5m vs 6m
good
medium

0%

terran_20_vs_20
40%
50
100

40%
40%
0 terran_20_vs_23
50
100
20%
20%
15% 20%
10%
5%
0%
0

20%
0

50

50

zerg_10_vs_11
corridor
6h vs
508z
1006h vs
0 8zzerg_20_vs_20
50
100
0
poor 20%
medium
20%good
10%
0%

10%
0%

0

50

50

zerg_20_vs_23
corridor
corridor
50
100
good
medium

Figure 7: Evaluation of SMACv1 tasks comparing the winrates achieved by ComaDICE and baselines.
100

0

50

100

BC

BCQ

CQL

ICQ

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

36.9Â±8.7
36.2Â±10.6
19.4Â±4.6
37.5Â±4.4
13.8Â±1.5

16.2Â±2.3
9.4Â±5.6
10.0Â±4.1
6.2Â±2.0
1.2Â±1.5

10.0Â±4.1
26.2Â±7.6
10.6Â±5.4
11.9Â±4.1
0.0Â±0.0

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

30.0Â±4.2
29.4Â±5.8
16.2Â±3.6
26.2Â±10.4
4.4Â±4.2

12.5Â±6.2
6.9Â±6.1
3.8Â±4.6
5.0Â±3.2
0.0Â±0.0

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

26.9Â±10.0
25.0Â±2.8
13.8Â±4.7
8.1Â±1.5
7.5Â±3.2

14.4Â±4.2
5.6Â±4.6
9.4Â±5.2
2.5Â±1.2
0.6Â±1.3

Instances

0

50

100

0

50

100

OMAR

OMIGA

ComaDICE

36.9Â±9.1
28.1Â±6.6
12.5Â±4.4
32.5Â±8.1
12.5Â±5.6

21.2Â±4.1
13.8Â±7.0
12.5Â±3.4
23.8Â±2.5
11.2Â±7.8

33.1Â±5.4
40.0Â±10.7
16.2Â±6.1
36.2Â±5.1
12.5Â±8.1

46.2Â±6.1
50.6Â±8.7
20.0Â±4.2
47.5Â±7.8
13.8Â±5.8

9.4Â±7.9
9.4Â±5.6
7.5Â±6.4
10.6Â±4.2
0.0Â±0.0

23.1Â±5.8
16.9Â±5.8
5.0Â±4.2
15.6Â±3.4
7.5Â±6.1

14.4Â±4.7
15.0Â±4.6
9.4Â±5.6
7.5Â±7.3
5.0Â±4.2

28.1Â±4.4
29.4Â±3.2
12.5Â±5.2
21.9Â±4.4
4.4Â±2.5

30.6Â±8.2
32.5Â±5.8
19.4Â±5.4
29.4Â±3.8
9.4Â±5.2

14.4Â±5.8
5.6Â±4.6
6.2Â±4.4
1.2Â±1.5
1.2Â±1.5

18.8Â±7.1
15.6Â±7.4
10.6Â±6.7
10.0Â±7.8
7.5Â±3.2

13.8Â±6.1
19.4Â±2.3
10.6Â±3.8
12.5Â±4.4
3.8Â±2.3

21.9Â±5.9
23.8Â±6.4
13.8Â±6.7
10.0Â±2.3
4.4Â±4.2

31.2Â±7.7
33.8Â±11.8
19.4Â±3.6
9.4Â±6.2
11.2Â±4.2

Table 9: Comparison of average winrates for ComaDICE and baselines on SMACv2 benchmarks.

BC

5

5

100
0
60%
40%
20%
0%
100
0
40%
20%
0%
100

0

CQL

ICQ

OMIGA

ComaDICE (ours)

40%

40%

40%

40%

20%

20%

20%

20%

20%

protoss_10_vs_11
0
50
50
protoss
40%
10 vs 10
20%

0

40% protoss
30%
20% 5 vs 5
10%
0%

40%
terran_10_vs_10
50
100

0

20%
30%

20%
10%
0
0%

20%

40%
20%
50
0%

0

50

20%

10%
40%

40%

20%

20%

0%

100
0

0

protoss_20_vs_20
0
50
50 protoss_20_vs_23
0
50
40%
protoss
protoss
protoss
30%
20%
10 vs 11
20 vs 20
20 vs 23
10%

0%

40%
terran_10_vs_11
50
100

terran
terran
0 zerg_10_vs_11
50
10
vs100
10
5
vs
5
30%

zerg_10_vs_10
50
100

50

OMAR

40%

protoss_10_vs_10
60%
40%
20%
0%

BCQ

20%
10%
0%

50

100
0

50

zerg
5 vs 5

0%

40%
terran_20_vs_20
50
100

40%

0

20%

20%

0

0

50

0

40%

20%

20%

0

zerg
10 vs 10

40%
terran_20_vs_23
50
100
20%

0

50

terran
terran zerg_20_vs_23
terran
zerg_20_vs_20
100
50 20 vs 23 100
10 vs5011
20 vs0 20

40%

50

0
20%
15%
10%
5%
0%50

50
0

50

zerg
10 vs 11

20%
10%
0%

100
0

40%

0

20%

50

zerg
20 vs 20

50
0

100
50

zerg
20 vs 23

Figure 8: Evaluation of SMACv2 tasks comparing the winrates achieved by ComaDICE and baselines.

B.4

A BLATION S TUDY: D IFFERENT VALUES OF A LPHA

We provide more experimental details for ablation study assessing the impact of varying the regularization parameter
alpha (Î±) on the performance of our ComaDICE.

B.4.1

R ETURNS

Our results, in Tables 10, 11, and 12, show that the performance of ComaDICE is sensitive to the choice of Î±.
Lower values of Î± tend to prioritize imitation learning, leading to suboptimal performance in terms of returns,
whereas higher values facilitate better adaptation to the offline data, achieving superior returns. Notably, an Î± value
of 10 consistently yielded the best results across most tasks, indicating an optimal balance between exploration
and exploitation in offline settings. This ablation study underscores the importance of selecting an appropriate
Î± to enhance the algorithmâ€™s robustness and effectiveness in handling distributional shifts in offline multi-agent
reinforcement learning scenarios.

Instances

Î± = 0.01

Î± = 0.1

Î±=1

Î± = 10

Î± = 100

2c vs 64zg

poor
medium
good

10.6Â±0.5
9.6Â±0.5
11.1Â±1.4

11.1Â±0.4
13.1Â±0.8
9.6Â±2.7

11.1Â±0.1
12.5Â±2.4
17.4Â±0.5

12.1Â±0.5
16.3Â±0.7
20.3Â±0.1

11.8Â±0.2
16.0Â±0.3
19.9Â±0.1

5m vs 6m

poor
medium
good

5.7Â±0.1
5.6Â±0.1
5.7Â±0.1

5.1Â±0.3
5.3Â±0.2
5.7Â±0.2

7.1Â±0.7
7.8Â±0.8
7.8Â±0.5

8.1Â±0.5
8.7Â±0.4
8.7Â±0.5

7.7Â±0.3
8.5Â±0.7
8.8Â±0.8

6h vs 8z

poor
medium
good

8.5Â±0.2
8.5Â±0.6
7.9Â±0.1

9.6Â±0.3
10.5Â±0.8
9.5Â±0.6

10.0Â±0.3
10.7Â±0.5
11.3Â±0.6

11.4Â±0.6
12.8Â±0.2
13.1Â±0.5

10.7Â±0.4
12.3Â±0.3
12.8Â±0.4

corridor

poor
medium
good

2.1Â±0.4
1.7Â±1.0
4.7Â±2.4

3.7Â±1.0
2.2Â±1.7
3.8Â±5.0

6.1Â±0.8
11.3Â±0.3
15.7Â±0.3

6.4Â±0.5
12.9Â±0.6
18.0Â±0.1

5.0Â±1.1
13.3Â±0.1
17.4Â±0.1

Table 10: Impact of alpha on returns for ComaDICE and baselines in SMACv1.

Î± = 0.01

Î± = 0.1

Î±=1

Î± = 10

Î± = 100

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

12.2Â±1.0
12.8Â±0.9
9.9Â±1.1
10.3Â±0.5
8.0Â±2.3

13.1Â±1.3
14.0Â±0.8
11.1Â±0.8
11.1Â±1.0
11.2Â±1.2

13.2Â±1.1
13.4Â±1.2
11.3Â±1.2
12.2Â±0.9
11.7Â±0.6

14.4Â±1.1
14.6Â±1.8
13.2Â±0.9
14.8Â±1.0
13.3Â±0.9

14.0Â±2.0
14.1Â±1.3
12.2Â±1.1
13.2Â±0.4
13.2Â±0.5

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

11.1Â±1.8
8.5Â±0.8
7.5Â±0.7
6.2Â±1.1
5.5Â±1.1

10.1Â±1.2
10.3Â±0.7
8.6Â±2.1
6.4Â±1.7
6.5Â±1.6

9.0Â±1.0
10.4Â±1.1
8.5Â±1.6
9.1Â±0.7
6.5Â±0.8

10.7Â±1.5
11.8Â±0.9
9.4Â±0.9
11.8Â±0.5
8.2Â±0.7

12.6Â±1.9
11.8Â±1.7
9.6Â±0.9
9.3Â±0.6
8.2Â±0.4

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

7.9Â±0.6
10.9Â±1.5
10.1Â±2.5
8.0Â±0.5
9.1Â±1.1

9.3Â±0.9
11.4Â±1.5
9.1Â±1.2
9.2Â±1.3
10.0Â±0.7

10.5Â±1.4
11.8Â±0.7
10.0Â±1.2
9.2Â±1.0
10.4Â±0.6

10.7Â±2.0
11.5Â±1.0
11.0Â±0.9
9.4Â±1.2
10.5Â±0.8

10.4Â±1.2
10.9Â±2.2
9.8Â±0.8
10.5Â±0.9
10.1Â±0.7

Instances

Table 11: Impact of alpha on returns for ComaDICE and baselines in SMACv2.

Î± = 0.01

Î± = 0.1

Î±=1

Î± = 10

Î± = 100

Hopper

expert
medium
m-replay
m-expert

147.3Â±67.9
149.6Â±96.8
165.6Â±104.1
119.1Â±77.1

107.9Â±65.5
107.5Â±66.9
109.6Â±38.7
95.6Â±69.5

545.7Â±820.6
244.7Â±267.5
155.6Â±61.6
58.8Â±26.1

2827.7Â±62.9
822.6Â±66.2
906.3Â±242.1
1362.4Â±522.9

2690.7Â±58.6
807.5Â±122.2
186.5Â±16.8
1358.4Â±595.1

Ant

expert
medium
m-replay
m-expert

1016.4Â±196.5
907.3Â±32.2
969.1Â±21.9
915.8Â±364.1

1179.0Â±273.7
1000.0Â±90.4
978.4Â±39.6
1132.9Â±282.2

1927.7Â±174.1
1424.3Â±3.1
944.6Â±28.9
738.5Â±250.2

2056.9Â±5.9
1425.0Â±2.9
1122.9Â±61.0
1813.9Â±68.4

1950.0Â±3.3
1354.6Â±2.5
1072.1Â±41.4
1559.6Â±86.8

Half
Cheetah

expert
medium
m-replay
m-expert

1068.9Â±635.2
575.9Â±724.8
412.3Â±310.5
-107.5Â±298.1

935.2Â±905.9
445.2Â±403.9
233.5Â±270.1
-275.9Â±544.5

3637.0Â±80.9
2690.0Â±92.4
861.6Â±173.5
1136.9Â±1608.3

4082.9Â±45.7
2664.7Â±54.2
2855.0Â±242.2
3889.7Â±81.6

3843.7Â±149.4
2523.4Â±59.0
2557.4Â±241.5
3605.6Â±70.4

Instances

Table 12: Impact of alpha on returns for ComaDICE and baselines in MaMujoco.

20

20

20

20

15

15

15

15

10

10

10

10

5

5

5

5

0.01 0.1

1

10 100

15

15

15

10

10

10

5

5

0.01 0.1

1

10 100

Protoss

0.01 0.1

1

10 100

5
0.01 0.1

1

10 100

Terran

0.01 0.1

5m vs 6m

2c vs 64zg

0.01 0.1

1

Zerg

10 100

1

10 100

0.01 0.1

6h vs 8z

2k

2k

0

0

0.01 0.1

1

Hopper

1

10 100

corridor

10 100

2k
0
0.01 0.1

1

10 100

Ant

0.01 0.1

1

10 100

HalfCheetah

Figure 9: Impact of alpha on returns for ComaDICE and baselines.
B.4.2

W INRATES

In the A.4.2 section of the appendix, we investigate the impact of varying Î± on winrates across different multi-agent
reinforcement learning environments. We observe that an intermediate Î± value of 10 consistently yields optimal
results, suggesting it strikes an effective balance between conservative policy adherence and exploration of the
offline dataset. This section underscores the importance of fine-tuning Î± to enhance the robustness and efficacy of
the ComaDICE algorithm in managing distributional shifts within competitive multi-agent settings.

Instances

Î± = 0.01

Î± = 0.1

Î±=1

Î± = 10

Î± = 100

2c vs 64zg

poor
medium
good

0.0Â±0.0
0.0Â±0.0
0.6Â±1.2

0.0Â±0.0
1.9Â±3.8
0.0Â±0.0

0.0Â±0.0
5.0Â±5.1
40.6Â±4.0

0.6Â±1.3
8.8Â±7.0
55.0Â±1.5

0.6Â±1.3
8.8Â±4.6
51.9Â±1.5

5m vs 6m

poor
medium
good

0.0Â±0.0
0.0Â±0.0
0.0Â±0.0

0.0Â±0.0
0.0Â±0.0
0.0Â±0.0

4.4Â±4.7
8.1Â±6.4
6.2Â±4.4

4.4Â±4.2
7.5Â±2.5
8.1Â±3.2

1.9Â±1.5
7.5Â±3.8
10.0Â±6.1

6h vs 8z

poor
medium
good

0.0Â±0.0
0.0Â±0.0
0.0Â±0.0

0.0Â±0.0
0.6Â±1.3
0.0Â±0.0

1.9Â±3.8
1.9Â±1.5
7.5Â±5.8

1.9Â±3.8
3.1Â±2.0
11.2Â±5.4

0.6Â±1.3
3.1Â±2.0
7.5Â±7.3

corridor

poor
medium
good

0.0Â±0.0
0.0Â±0.0
0.0Â±0.0

0.6Â±1.2
0.0Â±0.0
4.4Â±8.8

0.0Â±0.0
30.0Â±5.1
48.8Â±4.7

0.6Â±1.3
27.3Â±3.4
48.8Â±2.5

1.2Â±1.5
34.4Â±2.8
49.4Â±3.6

Table 13: Impact of alpha on winrates for ComaDICE and baselines in SMACv1.

Î± = 0.01

Î± = 0.1

Î±=1

Î± = 10

Î± = 100

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

20.6Â±10.0
19.4Â±6.1
0.0Â±0.0
1.2Â±1.5
0.0Â±0.0

31.9Â±6.1
25.0Â±3.4
6.2Â±9.7
8.8Â±7.8
1.9Â±2.5

50.0Â±2.8
45.0Â±11.1
18.8Â±8.1
28.1Â±8.6
9.4Â±6.6

46.2Â±6.1
50.6Â±8.7
20.0Â±4.2
47.5Â±7.8
13.8Â±5.8

46.2Â±8.5
51.2Â±7.6
29.4Â±8.3
40.6Â±6.2
17.5Â±5.1

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

25.6Â±4.6
15.0Â±8.7
3.8Â±2.3
0.6Â±1.2
0.6Â±1.3

22.5Â±7.2
28.7Â±7.2
13.8Â±9.2
2.5Â±3.6
2.5Â±3.6

30.6Â±4.1
33.8Â±9.4
14.4Â±9.2
18.8Â±2.0
2.5Â±3.6

30.6Â±8.2
32.5Â±5.8
19.4Â±5.4
29.4Â±3.8
9.4Â±5.2

41.2Â±4.6
43.8Â±7.1
16.2Â±10.3
21.9Â±3.4
6.2Â±2.0

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.0Â±4.6
13.8Â±9.0
9.4Â±9.5
0.0Â±0.0
1.2Â±1.5

20.0Â±5.8
20.6Â±8.3
12.5Â±6.8
1.9Â±1.5
3.8Â±2.3

28.7Â±4.6
29.4Â±9.0
16.9Â±3.2
6.9Â±6.1
12.5Â±4.0

31.2Â±7.7
33.8Â±11.8
19.4Â±3.6
9.4Â±6.2
11.2Â±4.2

25.0Â±8.6
31.9Â±6.7
17.5Â±9.2
12.5Â±4.0
11.9Â±6.1

Instances

Table 14: Impact of alpha on winrates for ComaDICE and baselines in SMACv2.

40%

40%

40%

40%

20%

20%

20%

20%

0.01 0.1 1

10 100

0.01 0.1 1

2c vs 64zg

10 100

0.01 0.1 1

10 100

5m vs 6m

6h vs 8z

60%

60%

60%

40%

40%

40%

20%

20%

20%

0.01 0.1 1

protoss

10 100

0.01 0.1 1

10 100

terran

0.01 0.1 1

10 100

corridor

0.01 0.1 1

10 100

zerg

Figure 10: Impact of alpha on winrates for ComaDICE and baselines.

B.5

A BLATION S TUDY: D IFFERENT F ORMS OF F - DIVERGENCE

We conduct an ablation study to examine the effects of different functions of f -divergence on the performance of
our ComaDICE algorithm across various multi-agent reinforcement learning environments. The study specifically
evaluates three types of f -divergence: Kullback-Leibler (KL), Ï‡2 , and Soft-Ï‡2 .

KL-Divergence: This is a well-known measure of how one probability distribution diverges from a second,
expected probability distribution. It is defined as:
fKL (x) = x log x âˆ’ x + 1
The corresponding inverse derivative, which is used in optimization, is:
â€² âˆ’1
(fKL
) (x) = exp(x âˆ’ 1)

KL-divergence can lead to numerical instability due to the exponential function, especially when the values become
large.
Ï‡2 -Divergence: This divergence measures the difference between two probability distributions by considering
the square of the differences. It is expressed as:
fÏ‡2 (x) =

1
(x âˆ’ 1)2
2

The inverse derivative is:
(fÏ‡â€² 2 )âˆ’1 (x) = x + 1
While this function avoids the exponential instability seen in KL-divergence, it may suffer from zero gradients for
negative values, which can slow down or halt training.
Soft-Ï‡2 Divergence: This function combines the forms of KL and Ï‡2 divergences to mitigate both numerical
instability and the dying gradient problem. It is defined piecewise as:

x log x âˆ’ x + 1 if 0 < x < 1
fSoft-Ï‡2 (x) = 1
2
if x â‰¥ 1
2 (x âˆ’ 1)
The inverse derivative is:
â€²
âˆ’1
(fSoft-Ï‡
(x) =
2)



exp(x) if x < 0
x+1
if x â‰¥ 0

This choice provides a stable optimization process by maintaining non-zero gradients and avoiding large exponential
values, making it suitable for reinforcement learning tasks.
We assess their impact on both returns and winrates in environments such as SMACv1, SMACv2, and MaMujoco.
Our results, detailed in Tables 15-19, reveal that the choice of f -divergence function significantly influences the
algorithmâ€™s effectiveness. For instance, the Soft-Ï‡2 divergence consistently yields superior returns and competitive
winrates across most scenarios, suggesting its robustness in managing distributional shifts in offline settings.
Conversely, while Soft-Ï‡2 divergence also performs well, particularly in environments with higher complexity,
KL divergence shows varying results, indicating its sensitivity to specific task dynamics. This comprehensive
analysis underscores the importance of selecting an appropriate f -divergence function to optimize ComaDICEâ€™s
performance in diverse multi-agent reinforcement learning contexts.
B.5.1

R ETURNS
Instances

fÏ‡2 (x)

fKL (x)

fSoft-Ï‡2 (x)

2c vs 64zg

poor
medium
good

11.6Â±0.2
16.1Â±0.6
19.7Â±0.1

11.1Â±0.3
15.7Â±0.3
19.3Â±0.1

12.1Â±0.5
16.3Â±0.7
20.3Â±0.1

5m vs 6m

poor
medium
good

7.8Â±0.4
8.1Â±0.5
8.7Â±0.6

7.5Â±0.5
7.7Â±0.4
8.1Â±0.4

8.1Â±0.5
8.7Â±0.4
8.7Â±0.5

6h vs 8z

poor
medium
good

10.5Â±0.3
12.9Â±0.4
12.7Â±0.4

10.0Â±0.2
12.4Â±0.5
12.4Â±0.5

11.4Â±0.6
12.8Â±0.2
13.1Â±0.5

corridor

poor
medium
good

6.5Â±0.5
12.7Â±0.7
17.3Â±0.1

6.1Â±0.4
12.0Â±0.7
16.9Â±0.1

6.4Â±0.5
12.9Â±0.6
18.0Â±0.1

Table 15: Impact of f -divergence on returns for ComaDICE and baselines in SMACv1.

fÏ‡2 (x)

fKL (x)

fSoft-Ï‡2 (x)

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

14.6Â±0.5
14.7Â±1.3
12.8Â±1.0
12.7Â±0.3
12.4Â±0.9

13.6Â±0.9
13.7Â±1.6
11.4Â±1.7
13.1Â±0.7
12.5Â±0.7

14.4Â±1.1
14.6Â±1.8
13.2Â±0.9
14.8Â±1.0
13.3Â±0.9

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

11.1Â±1.2
9.8Â±0.9
8.9Â±0.8
10.5Â±0.5
8.2Â±0.4

12.7Â±2.0
10.7Â±1.3
8.9Â±1.0
10.2Â±0.7
7.4Â±0.7

10.7Â±1.5
11.8Â±0.9
9.4Â±0.9
11.8Â±0.5
8.2Â±0.7

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.0Â±0.8
12.4Â±1.2
8.9Â±0.4
9.0Â±0.8
10.2Â±1.0

9.6Â±1.5
10.3Â±1.1
9.1Â±1.1
9.0Â±0.6
9.3Â±0.8

10.7Â±2.0
11.5Â±1.0
11.0Â±0.9
9.4Â±1.2
10.5Â±0.8

Instances

Table 16: Impact of f -divergence on returns for ComaDICE and baselines in SMACv2.

fÏ‡2 (x)

fKL (x)

fSoft-Ï‡2 (x)

Hopper

expert
medium
m-replay
m-expert

2625.0Â±191.3
794.4Â±69.2
221.3Â±58.0
1294.1Â±520.4

2018.7Â±972.0
295.5Â±227.1
129.9Â±55.0
105.5Â±103.9

2827.7Â±62.9
822.6Â±66.2
906.3Â±242.1
1362.4Â±522.9

Ant

expert
medium
m-replay
m-expert

1945.2Â±2.8
1359.2Â±3.2
1111.1Â±57.8
1655.9Â±42.8

1884.1Â±27.8
1346.2Â±49.8
987.5Â±33.9
1182.5Â±405.1

2056.9Â±5.9
1425.0Â±2.9
1122.9Â±61.0
1813.9Â±68.4

Half
Cheetah

expert
medium
m-replay
m-expert

3860.6Â±91.5
2532.3Â±81.9
2729.9Â±241.5
3665.2Â±74.0

3830.0Â±88.8
2347.8Â±171.8
1258.5Â±1015.4
3601.0Â±155.6

4082.9Â±45.7
2664.7Â±54.2
2855.0Â±242.2
3889.7Â±81.6

Instances

Table 17: Impact of f -divergence on returns for ComaDICE and baselines in MaMujoco.

B.5.2

W INRATES

Instances

fÏ‡2 (x)

fKL (x)

fSoft-Ï‡2 (x)

2c vs 64zg

poor
medium
good

0.0Â±0.0
13.1Â±4.6
55.6Â±3.1

0.0Â±0.0
10.6Â±3.8
54.4Â±1.5

0.6Â±1.3
8.8Â±7.0
55.0Â±1.5

5m vs 6m

poor
medium
good

3.8Â±3.1
6.2Â±2.8
8.8Â±3.6

3.8Â±3.6
5.0Â±3.8
6.9Â±3.1

4.4Â±4.2
7.5Â±2.5
8.1Â±3.2

6h vs 8z

poor
medium
good

0.0Â±0.0
5.0Â±2.5
9.4Â±4.4

0.0Â±0.0
5.0Â±3.8
9.4Â±2.0

1.9Â±3.8
3.1Â±2.0
11.2Â±5.4

corridor

poor
medium
good

1.2Â±1.5
31.2Â±6.2
49.4Â±5.4

1.2Â±1.5
28.1Â±5.9
48.1Â±1.5

0.6Â±1.3
27.3Â±3.4
48.8Â±2.5

Table 18: Impact of f -divergence on winrates for ComaDICE and baselines in SMACv1.

fÏ‡2 (x)

fKL (x)

fSoft-Ï‡2 (x)

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

52.5Â±4.1
48.1Â±7.6
22.5Â±8.7
38.1Â±2.3
16.9Â±4.2

46.2Â±7.2
55.0Â±9.8
20.6Â±6.1
41.2Â±7.8
15.0Â±3.6

46.2Â±6.1
50.6Â±8.7
20.0Â±4.2
47.5Â±7.8
13.8Â±5.8

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

41.2Â±7.2
30.6Â±4.1
15.6Â±11.5
33.8Â±6.4
5.6Â±4.1

38.8Â±10.6
36.2Â±10.8
15.0Â±7.5
28.7Â±11.8
8.1Â±4.2

30.6Â±8.2
32.5Â±5.8
19.4Â±5.4
29.4Â±3.8
9.4Â±5.2

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

29.4Â±9.0
31.2Â±7.7
11.2Â±1.5
7.5Â±3.2
10.6Â±3.2

33.1Â±13.3
26.2Â±5.1
16.2Â±7.2
11.2Â±7.0
10.0Â±2.3

31.2Â±7.7
33.8Â±11.8
19.4Â±3.6
9.4Â±6.2
11.2Â±4.2

Instances

Table 19: Impact of f -divergence on winrates for ComaDICE and baselines in SMACv2.

B.6

A BLATION S TUDY: D IFFERENT T YPES OF M IXER N ETWORK

In this section, we explore the impact of using different types of mixer networks within the ComaDICE algorithm.
We introduce two settings for the mixer network within the ComaDICE algorithm: 1-layer and 2-layer settings.
The mixer network plays a crucial role in aggregating local value functions into a global value function, which is
essential for effective policy optimization in multi-agent reinforcement learning (MARL) settings. By examining
various mixer network architectures, we aim to understand how these configurations affect the performance and
stability of the ComaDICE algorithm. The comparisons are presented in Tables 20-24, showing both average
returns and win rates. The results clearly demonstrate that the 1-layer configuration performs better, providing more
stable training outcomes across nearly all tasks. This contradicts the findings in many prior online MARL studies
(Rashid et al., 2020; Son et al., 2019; Wang et al., 2020), which may be due to over-fitting issues in offline learning.

B.6.1

R ETURNS

Instances

ComaDICE (ours)
1-layer
2-layer

2c vs 64zg

poor
medium
good

12.1Â±0.5
16.3Â±0.7
20.3Â±0.1

11.5Â±0.9
11.2Â±0.8
9.0Â±2.2

5m vs 6m

poor
medium
good

8.1Â±0.5
8.7Â±0.4
8.7Â±0.5

3.8Â±1.1
0.8Â±0.3
7.7Â±0.1

6h vs 8z

poor
medium
good

11.4Â±0.6
12.8Â±0.2
13.1Â±0.5

10.3Â±0.3
9.1Â±0.6
8.3Â±0.5

corridor

poor
medium
good

6.4Â±0.5
12.9Â±0.6
18.0Â±0.1

1.5Â±0.7
3.9Â±1.7
2.6Â±2.3

Table 20: Average returns for ComaDICE and baselines on SMACv1 with different mixer settings.

ComaDICE (ours)
1-layer
2-layer

Instances

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

14.4Â±1.1
14.6Â±1.8
13.2Â±0.9
14.8Â±1.0
13.3Â±0.9

10.5Â±1.4
11.2Â±1.6
9.5Â±0.4
9.5Â±0.9
7.1Â±2.2

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.7Â±1.5
11.8Â±0.9
9.4Â±0.9
11.8Â±0.5
8.2Â±0.7

8.3Â±0.8
8.8Â±1.1
6.4Â±1.2
7.8Â±0.9
6.6Â±0.9

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

10.7Â±2.0
11.5Â±1.0
11.0Â±0.9
9.4Â±1.2
10.5Â±0.8

7.8Â±1.1
9.7Â±0.6
7.9Â±0.7
7.8Â±0.6
8.0Â±0.5

Table 21: Average returns for ComaDICE and baselines on SMACv2 with different mixer settings.

ComaDICE (ours)
1-layer
2-layer

Instances

Hopper

expert
medium
m-replay
m-expert

2827.7Â±62.9
822.6Â±66.2
906.3Â±242.1
1362.4Â±522.9

483.7Â±349.7
648.4Â±245.9
441.9Â±260.8
402.3Â±288.2

Ant

expert
medium
m-replay
m-expert

2056.9Â±5.9
1425.0Â±2.9
1122.9Â±61.0
1813.9Â±68.4

1583.0Â±160.4
1198.9Â±53.9
1041.8Â±38.4
1426.6Â±171.4

Half
Cheetah

expert
medium
m-replay
m-expert

4082.9Â±45.7
2664.7Â±54.2
2855.0Â±242.2
3889.7Â±81.6

2159.4Â±658.0
2026.7Â±244.3
1299.2Â±196.1
1336.3Â±381.9

Table 22: Average returns for ComaDICE and baselines on MaMujoco with different mixer settings.

B.6.2

W INRATES

Instances

ComaDICE (ours)
1-layer
2-layer

2c vs 64zg

poor
medium
good

0.6Â±1.3
8.8Â±7.0
55.0Â±1.5

0.0Â±0.0
3.8Â±3.6
19.4Â±5.0

5m vs 6m

poor
medium
good

4.4Â±4.2
7.5Â±2.5
8.1Â±3.2

3.1Â±0.0
1.2Â±1.5
3.1Â±0.0

6h vs 8z

poor
medium
good

1.9Â±3.8
3.1Â±2.0
11.2Â±5.4

0.0Â±0.0
0.0Â±0.0
1.9Â±2.5

corridor

poor
medium
good

0.6Â±1.3
27.3Â±3.4
48.8Â±2.5

0.0Â±0.0
11.2Â±2.5
23.1Â±8.1

Table 23: Average winrates for ComaDICE and baselines on SMACv1 with different mixer settings.

Instances

ComaDICE (ours)
1-layer
2-layer

Protoss

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

46.2Â±6.1
50.6Â±8.7
20.0Â±4.2
47.5Â±7.8
13.8Â±5.8

31.9Â±3.6
32.5Â±5.8
10.6Â±7.3
21.9Â±4.0
6.9Â±5.4

Terran

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

30.6Â±8.2
32.5Â±5.8
19.4Â±5.4
29.4Â±3.8
9.4Â±5.2

25.6Â±4.6
28.1Â±3.4
12.5Â±4.0
11.2Â±3.2
3.1Â±2.0

Zerg

5 vs 5
10 vs 10
10 vs 11
20 vs 20
20 vs 23

31.2Â±7.7
33.8Â±11.8
19.4Â±3.6
9.4Â±6.2
11.2Â±4.2

20.6Â±4.7
21.2Â±7.2
13.1Â±4.1
5.6Â±1.3
3.1Â±3.4

Table 24: Average winrates for ComaDICE and baselines on SMACv2 with different mixer settings.

