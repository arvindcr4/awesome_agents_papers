A ROADMAP T OWARDS I MPROVING M ULTI -AGENT
R EINFORCEMENT L EARNING WITH C AUSAL D ISCOVERY AND
I NFERENCE

arXiv:2503.17803v1 [cs.LG] 22 Mar 2025

A P REPRINT
Giovanni Briglia
Department of Sciences and Method for Engineering
University of Modena and Reggio Emilia
Reggio Emilia, RE 42122
giovanni.briglia@unimore.it

Stefano Mariani
Department of Sciences and Method for Engineering
University of Modena and Reggio Emilia
Reggio Emilia, RE 42122
stefano.mariani@unimore.it

Franco Zambonelli
Department of Sciences and Method for Engineering
University of Modena and Reggio Emilia
Reggio Emilia, RE 42122
franco.zambonelli@unimore.it

March 25, 2025

A BSTRACT
Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning
process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation
capabilities, safety and interpretability of behaviour. However, applications of causal reasoning
to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in
investigating the opportunities and challenges of applying causal reasoning in MARL. We measure
the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly
requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of
collaboration between agents. Then, we discuss the positive as well as negative results achieved,
giving us the chance to outline the areas where further research may help to successfully transfer
causal RL to the multi-agent setting.
Keywords Multi-Agent Reinforcement Learning · Causal Discovery · Causal Inference · Structural Causal Model

1

Introduction

Reinforcement Learning (RL) is now an established approach to let a software agent autonomously learn how to achieve
a task in a complex environment requiring sequential decision-making. There, the agent learns from the experience of a
reward signal obtained via repeated interactions with such environment (Sutton and Barto (1998)). Despite its success,
there are still open issues that nurture research in RL. For instance: how to generalise to novel tasks and/or unseen
environments (Mutti et al. (2023a)), how to guarantee that the learned policy adheres to desired safety boundaries
(Munos et al. (2016)), and how to represent the learned policies in a human-interpretable way (Madumal et al. (2020)).
Different approaches have been proposed in the literature for each of these open issues. Here, we focus on those
leveraging causal discovery and inference, that is, the learning and exploitation of cause-effect relationships in the RL
system composed by the learning agent and its environment (Gershman (2017)).
Causal RL, in fact, is a thriving field of research aimed at combining causal models with RL. These models, as
formulated in Pearl (1995), are mathematical frameworks meant to capture cause-effect relationships amongst variables
in a system, and to enable qualitative and quantitative reasoning over such relations. Their natural link to RL, as well as

Causal MARL: a Roadmap

A P REPRINT

the benefits they could bring to it, have already been described and demonstrated, for instance by Deng et al. (2023);
Zeng et al. (2023)—at least for single-agent RL. In fact, learning a causal model of a RL system (there including the
learning agent and its environment) has been shown to positively impact RL in several ways. In particular: consequences
of agent actions in given states may be reliably predicted, and actions achieving desired goals reliably planned.
However, transferring causal RL approaches to Multi-Agent Reinforcement Learning (MARL) is still relatively unexplored, as witnessed by Grimbly et al. (2021). In a multi-agent setting, in fact, RL on its own already becomes much
more complicated with respect to the single-agent case. The main reason is that multiple agents learning concurrently
in a shared environment induce non-stationarity of the environment state transitions and an exponentially large joint
action space Du and Ding (2021). In addition, causal discovery becomes much more difficult, too: the sheer size of the
causal model to be learnt, in the number of variables and especially of their links, quickly becomes computationally
cumbersome when considering the whole Multi-Agent System (MAS) Wang et al. (2024). In this paper, we take a step
in a broad quantitative investigation of the opportunities and challenges of applying causal reasoning in MARL.
In particular, we focus on Structural Causal models (SCMs) and interventions as defined by Pearl (2001). We develop a
conceptual and software framework that augments “vanilla” MARL algorithms with causal discovery and incorporates
the learnt causal model into the agents’ decision-making process. This serves to prune the action space of risky actions
and nudge the agent towards desired states, via causal inference (i.e. prediction and planning). We call this framework
Causality-Driven Reinforcement Learning (CDRL, for short). The goal of the augmentation is to improve the efficacy,
efficiency, and safety of the learning process and learnt policy.
To keep the approach virtually applicable to any MARL environment, we consider for the causal discovery problem the
same variables that define any (MA)RL problem, with no additional expert knowledge: the action space, the observation
space, and the reward space. Accordingly, we only aim to learn the causal relationships linking actions to states, and
states to rewards in turn, not all the possible links (e.g. between states). We call this a minimal causal model, or, a causal
model of the “core” environment dynamics. We measure the impact of such a simple form of causal augmentation
in three state-of-the-art MARL scenarios increasingly requiring cooperation, and featuring partial observability as
well as continuous action and observation spaces. These features make the problem of causal discovery particularly
difficult, as agents are forced to learn partial causal models of their environment, and interventions are computationally
complicated by the continuous nature of action and state variables. As the vanilla MARL algorithm to be causally
augmented, we choose three state-of-the-art MARL algorithms exploiting various degrees of collaboration between
agents (e.g. parameters sharing, value function decomposition, joint action space factorisation, etc.). Finally, we discuss
the positive as well as negative results achieved, giving us the chance to outline the areas where further research may
help to successfully transfer causal RL to the multi-agent setting.
The paper is structured as follows: Section 2 provides background information and related works; Section 3 describes
our approach to causal augmentation of MARL algorithms; Section 4 evaluates our approach in state-of-the-art scenarios
to provide quantitative ground for the research roadmap discussed in Section 5; finally, Section 6 concludes the paper.

2

Background and Related Works

In this section, we quickly recall the very basics of MARL and causal reasoning, by focusing on SCMs and on the
additional challenges that concurrent learning brings along in MARL with respect to single-agent RL, then we overview
the state of the art for both causal RL and causal MARL.
2.1

From Causal RL to Causal MARL

Causal models are a particularly desirable kind of model of (causal) dependencies between variables in a domain (e.g.,
an RL environment), as they enable to disambiguate true causation between state variables from mere correlation
(Pearl (2014)). They are known to capture relationships and express reasoning that is out of reach for purely statistical
machine learning approaches (Pearl (2019)), and their connections with and usefulness to RL research has already
been motivated, for instance by Schölkopf et al. (2021); Gershman (2017). In the context of RL, a causal model helps
understand (and thus predict) how actions (cause) may change the state of the environment (effect), and, in turn, how to
plan actions (cause) to achieve a desired state (effect).
A Structural Causal Model (SCM) is a specific formalism to represent causal models that expresses the causal
relationships among variables using a Directed Acyclic Graph (DAG) alongside a set of structural equations. In an
SCM, each node in the DAG corresponds to a variable in the system, and directed edges between nodes indicate direct
causal influences. The structural equations specify how each variable is causally determined by its parent variables in
the graph—quantitatively.
2

Causal MARL: a Roadmap

A P REPRINT

Causal discovery is the process of learning such models based on data, possibly interventions, and identifiability
conditions (see Peters et al. (2012)). In the context of RL, the data are, for instance, the action-state-reward trajectories
produced by the agent while acting in the environment. Interventions are a fundamental mechanism for causal discovery,
and also for causal inference, that is the analysis of how variables’ values change given others. They involve deliberately
manipulating certain variables within a system (by changing their values) to observe their effects on others, thereby
unravelling the causal relationships at play Fu and Zhou (2013). In the case of the model of an RL environment, this
translates to being able to plan what actions are needed to bring about a desired effect (i.e. a state of the environment)
and to predict the effect of actions carried out in a given state. Interventions have been formalised with the do
operator Huang and Valtorta (2006), which enables to express statements like P (Y | do(X = x)) to represent the
probability distribution of variable Y given an intervention setting variable X to value x.
These models (as well as other frameworks for causal reasoning) can be used in both single-agent and multi-agent RL
with the purpose of improving the learning process and policies in a number of directions (e.g. efficacy, efficiency, safety,
generalisation, interpretability, etc.). However, multi-agent settings are notably more complex than single-agent ones,
both for the typical additional challenges of MARL with respect to single-agent RL, and for the inherent complexity of
causal discovery (first and foremost), and causal inference when many variables and potential causal relationships are at
play. For the former, the issues are mostly about the induced non-stationarity of environment state transitions due to
concurrent learning of agents, credit assignment, and equilibrium selection issues. For the latter, most causal discovery
algorithms entails some sort of conditional independence testing and/or experimental design routine that scales poorly
in the number of nodes and/or causal relationships. Also, when multiple agents want to learn a causal model of their
relationships with the environment, they would likely need to also model other agents’ influences.
The next Subsection overviews the state-of-the-art in this regard, as a confirmation that causal MARL is still relatively
unexplored yet.
2.2

Causal (MA)RL

Before overviewing the landscape of causal RL for both single-agent and multi-agent systems, a premise is necessary.
Lots of literature uses causal terminology and claims to perform causal reasoning to some extent, and there are many
actual methods to support causal reasoning nowadays. However, in this paper, we are mostly concerned with explicit
causal reasoning frameworks, especially those grounded in Structural Causal Models (SCMs) and the notion of
intervention to separate association from causation in the causal ladder Pearl (2001)—as popularised first and foremost
by Judea Pearl. The reason for adopting this specific view, albeit perhaps narrow, is that it is mathematically solid,
has been made operational by many works (starting from the do-calculus Huang and Valtorta (2006)), and fosters an
explicit representation of causal models—that is, amenable of rigorous and mechanic manipulation. Other approaches,
instead, cannot claim all of these nice properties. For instance, many approaches adopt deep learning to learn causal
relationships between variables in a latent space, such as Hosoya (2019); Yao et al. (2018); Le et al. (2024); Pina
et al. (2023). Other use ad-hoc or statistical notions underlying causal relationships and especially to quantify causal
effects, for instance based on information-theoretic and/or statistical measures such as conditional mutual information.
Examples are Yang et al. (2023); Li et al. (2022, 2024); Reizinger et al. (2024) Some approaches use both, such as
Jiang et al. (2024); Liesen et al. (2024).
Narrowing down our attention to the learning (discovery) and/or exploitation (inference) of explicit models of causeeffect relationships adhering to the premise disclosed above, single-agent causal RL is more mature, whereas causal
MARL is less explored—especially for causal discovery. The majority of contributions focus on causal inference
or induction (as defined by Genewein et al. (2020)), that is, exploiting known causal structure to estimate effects of
variables on others. An example is the work by Seitzer et al. (2021), where conditional mutual information is used
as a basis to define a measure of “causal influence” enabling a learning agent to detect when it is in control of some
environment dynamics. With such information, the agent can improve exploration. Mutti et al. (2023a,b), instead,
aim to exploit causality to improve generalisation abilities of an RL agent. They define causality as the common
structure across MDPs and invariant in time, such as the law of motion according to physics. Hence they target a set
of environment assumed to share such common structure, and show how learning the causal structure by sampling
different environments makes the agent generalise to unseen environments—but with the same common structure. For
causal discovery, instead, Lu et al. (2021) propose invariant Causal Representation Learning (iCaRL), an approach
that enables out-of-distribution (OOD) generalization not only for RL but potentially for machine learning in general.
Kocaoglu et al. (2017) adopts an experimental design approach to propose two causal discovery algorithms.
Application of causal reasoning in multi-agent RL (MARL) has been envisioned by Grimbly et al. (2021) but is
significantly less explored to date compared to single-agent RL. Here, causal inference is more common than discovery,
and mostly used to adjust individual rewards based on detected causal relationships, with the goal of enhancing
credit assignment and promote cooperation. For instance, Wang et al. (2023) leverage causal modelling in offline
3

Causal MARL: a Roadmap

A P REPRINT

Figure 1: Causal augmentation architecture: the agent interacts with the environment first to learn a minimal causal
model, then to learn an action policy. There, in the action selection step, causal inference modulates the action space by
acting as a filter (action mask).

multi-agent reinforcement learning by utilizing a Dynamic Bayesian Network to capture causal relationships between
agents’ actions, states, and rewards. This approach decomposes team rewards into individual contributions, facilitating
precise and interpretable credit assignment that promotes cooperation and improves policy learning. Another recent
contribution, by Du et al. (2024), introduces a MARL approach for identifying situation-dependent causal influences
between agents. By detecting states where one agent’s actions significantly impact another, they promote more effective
coordination. Their MACGM framework formalizes the CGM paradigm in a multi-agent setting and incorporates an
intrinsic reward to enhance awareness of causal influences among agents.

3

Proposed Causal Augmentation

Our proposed causal augmentation amounts to undertake two key consecutive tasks—depicted in Figure 1: (1) causal
discovery of a (minimal) causal model that relates actions, states, and the reward signal, and (2) leveraging this
knowledge to provide the agent with an action filter (or, mask) via causal inference. As the general MARL problem
formulation to apply our proposed causal augmentation, we take as a reference Partially Observable Markov Games
(POSGs), following Bettini et al. (2023); Liu et al. (2022). All the experimental scenarios used for our investigation in
Section 4 abide to this standard formulation. It is worth noting that the simplicity of our proposed causal augmentation
enjoys a few desirable properties: it is algorithm-agnostic, meaning that it can be applied to augment virtually any
MARL (and RL) algorithm; can be used with tabular methods (such as Q-learning and other TD-learning techniques) as
well as non-tabular ones using any kind of (deep) neural networks as function approximations; it is compatible with
both continuous and discrete action spaces.
3.1

Causal Discovery in MARL

We treat the agent’s actions on the environment as interventions on the system’s variables. Thus, the data in input to
the causal discovery process consists of the agent’s trajectory, represented by a sequence of observation-action-reward
triplets. The output is the learned structural causal model, composed by a DAG, and the Conditional Probability
Distributions (CPDs) of each node therein, used as the structural equations of the model. This enables inferences such as
action planning and future state prediction (as described in the next subsection). Causal discovery proceeds as follows:
1. each learning agent repeatedly acts in the environment by following any arbitrary policy (e.g. simply ϵ-greedy
Q-learning);
2. the agent trajectories are collected;
3. the PC algorithm by Spirtes et al. (2000) as implemented in Zheng et al. (2024) is fed with such trajectories,
independently for each agent, and learning of the causal model starts;
4. in the process, the DAG is constrained to have the reward variable as a children of any (learnt) subset of the
action and/or state variables, and not to be a parent of any other node.
As the specific data structure representing such a model, we rely on a (Causal) Bayesian Network (CBN), whose DAG
corresponds to the causal DAG, and whose CPDs play the role of the causal structural equations.
4

Causal MARL: a Roadmap

3.2

A P REPRINT

Causal inference in MARL

At this stage, the learnt causal model is exploited to perform causal inference, that is, predicting the effects of actions
on observable states, and planning actions to achieve desired rewards. The input to this stage is the current observation,
as in any RL algorithm, and the output is a pool of admissible actions, filtered according to causal inference as follows.
1. At each action selection step, we manipulate the CBN by conditioning on the observation variables in the
Markov Blanket Pellet and Elisseeff (2008) of the reward variable, that is, we set their values to the one in the
current observation.
2. Then, we perform interventions on each action variable, by iteratively assigning to each of them every admissible value—exploiting Pearl’s do operator. This process enables to compute the probability distribution of the
reward variable (prediction), conditioned on the observations, and hypothesizing a given action (planning)—the
intervention.
3. Based on these distributions, we determine a likability score for each action, by weighting the rewards with
their respective probabilities.
4. Upon these scores we compute an action mask based on a pluggable filtering strategy. For instance, for the
assessment in Section 4, we look at percentiles: we filter out actions with scores below the 25th percentile of
the whole batch of scores, and, in case there are actions above the 75th percentile, we filter out also any other
action (to nudge the agent to choose amongst these best ones).
5. The mask is applied to the action pool, and the MARL algorithm can select amongst them.
The entire process is outlined in Algorithm 1.
Algorithm 1 Causal inference stage of the causal augmentation, executed at each agent-environment interaction (or in
batches, by replacing obscur with the batch of observations)
Require: CBN : Causal Bayesian Network, obscur : current observation
1. Condition nodes in the Markov Blanket of the reward node with obscur ;
2. Compute the reward inference data structure outlined in Equation 1;
3. Compute the likability scores for each action;
4. Compute the action mask based on an arbitrary strategy (e.g. percentiles);
5. Apply the obtained action mask to the action select step of the (MA)RL algorithm.
To further clarify the approach, the following data structure is a convenient representation of the output of step §2:

1
1
k
k

do(a1 )|obscur : {r1 : P(r1 ), . . . , r1 : P(r1 )},
(1)
reward inference = ...


m
m
1
1
do(an )|obscur : {rn : P(rn ), . . . , rn : P(rn )}
1...
where a1...n denotes an action value, obscur denotes the current observation, r1...n
denotes the set of reward values
seen for action n, | denotes conditioning, and P denotes a probability distribution.

Notice that such a formulation hints at discrete action and reward spaces just for the sake of clarity. In the case of
continuous spaces, any technique for carrying out causal interventions on continuous variables can be applied—e.g.,
discretisation, soft interventions Kocaoglu et al. (2019) working with distribution shifts (see the roadmap in Section 5).

4

Application to MARL

Within the framework described above, we want to assess whether, and to what extent, our proposed causal augmentation
is able to improve efficacy, efficiency, and/or safety of the MARL learning process and resulting policy. To this end, a set
of “vanilla” MARL algorithms are augmented with causal discovery, and then incorporate the learnt causal model into
the agents’ decision-making process. In fact, the model is used to perform causal inference (prediction and planning) on
variables representing states, actions, and the reward signal, to obtain an action filter. Such filter serves to modulate
the action space available to the agent dynamically, by pruning risky actions, while nudging the agent towards those
more likely to lead to desired states (e.g. with higher expected rewards). By comparing the performance of the causally
augmented version of the algorithms with their vanilla versions, in accomplishing tasks requiring different levels of
cooperation, we can discuss the opportunities and challenges of applying causal reasoning to MARL problems.
5

Causal MARL: a Roadmap

(a) Navigation

(b) Flocking

A P REPRINT

(c) Give-Way

Figure 2: Experimented multi-agent tasks from the VMAS simulator by Bettini et al. (2022).

Accordingly, this section aims to address the following research questions (RQs):
1. Can the augmentation of a MARL algorithm with a minimal causal model of the environment dynamics
improve the learning process in terms of efficacy, efficiency, and safety? (RQ1)
2. If improvements are observed, which combinations of tasks and algorithms exhibit them the most? (RQ2)
And why?
3. If not, which combinations of tasks and algorithms exhibit the worst difference in performance? (RQ3) And
why?
In the following, we provide quantitative measures to objectively respond to the first part of each question. Then, next
section speculates on the “why” part, grounded on these quantitative results.
4.1

Experimental Settings

Environments. We use the VMAS simulator by Bettini et al. (2022) that offers a variety of multi-agent environments.
In particular, we consider three task environments chosen as they require different degrees of cooperation to the agents:
navigation, flocking, and give-way—increasingly requiring cooperation. All of them feature partial observability,
continuous observation and action spaces, and dense rewards. Figure 2 provides a visual representation of these
environments. The Navigation scenario (Figure 2a) requires each agent to reach its own colour-coded goal location
while avoiding collisions with others. In Flocking (Figure 2b) agents must coordinate to circle around a given goal
location as a cohesive group while evading obstacles. Finally, in Give-Way (Figure 2c) agents need to walk down a
narrow hallway by letting each other through.
RL Algorithms. We take off-the-shelf algorithms from the BenchMARL framework by Bettini et al. (2024) and
augment them with our causal discovery and inference pipeline. Algorithms are chosen based on their support to and
exploitation of agent collaboration, similar to environments: Independent Q-Learning (IQL) is non-cooperative (agents
learn independently, see Tan (1997)), Value Decomposition Networks (VDN) factorises the joint action space based on
a shared reward function, hence promotes cooperation implicitly (Sunehag et al. (2017)), and Qmix explicitly shares
learning parameters (Rashid et al. (2020))—hence, is the most collaborative. The corresponding causal augmentations
are called in the following CausalIQL, CausalVDN, and CausalQmix—their architecture is detailed in Appendix D.
Metrics. To quantitatively support the roadmap outlined in Section 5, we tracked several performance metrics during
our experiments. They belong to two categories: scenario-independent metrics, reported for each scenario, and
scenario-dependent ones, reported only for specific ones. The former are all normalized in [0,1] and include the mean,
median, and Inter-Quartile Mean (IQM, recommended by Agarwal et al. (2021)) of the reward achieved after training,
during the evaluation stage. They also include the optimality gap, that represents how far the results are from the
optimum (i.e. 1)—displayed with both its mean and standard deviation. Scenario-dependent metrics, instead, include,
for instance, how many collisions happened during training, and the distance between agents and their goal. All the
metrics are averaged over 10 independent runs, following the recommendations by Gorsane et al. (2022).
Technical Considerations. Performing interventions on continuous variables in a computationally tractable way
is still an open issue in causal discovery and inference literature (see Wang et al. (2024)). The formal correctness
and practical performance of the few approaches proposed, such as in Lorch et al. (2021); Schindl et al. (2024);
Schweisthal et al. (2024); Wiedermann et al. (2022), is still to be fully assessed and widely agreed upon. In the roadmap
discussed in Section 5 we provide a brief overview of the most promising research directions available to date. Given
these considerations, in this preliminary work we decided to discretize observations before feeding it to the causal
6

Causal MARL: a Roadmap

(a) Navigation

(b) Flocking

A P REPRINT

(c) Give-Way

Figure 3: Aggregate scores of median, IQM, mean of normalized reward (all the higher – to the right – the better), and
optimality gap (the lower – to the left – the better), for each algorithm, and for each task.

machinery. We thus conducted a sensitivity analysis to ensure that such a discretisation does not catastrophically impact
performance—see Appendix E.
4.2

Results

In this section we report the results obtained in our experiments, as measured by our described metrics. We recall that
the goal of this assessment is not to prove that our causal augmentation approach is sufficient or best in class to improve
learning in any MARL scenario, but to provide food for thought regarding opportunities and challenges raised by the
approach—discussed in Section 5.
Figure 3 shows the scenario-independent metrics for each MARL algorithm (causally augment and vanilla) and for all
the scenarios. The vanilla versions already perform quite well, nearly reaching optimality in all the scenarios. This
suggests that further improving such performance with causal augmentation is challenging per se. However, there
are improvements in specific cases (RQ1,RQ2): CausalIQL improves IQL in the navigation task, CausalVDN and
CausalQmix improve the corresponding vanilla versions in both the flocking scenario, and the give-way scenario.
Notice that, except for CausalVDN in give-way, also the standard deviation of the metrics is reduced. In the other
cases, causal augmentation does not affect performance notably or even worsen them, as in the case of CausalIQL in
the flocking and give-way tasks (RQ3). Recall that tasks and algorithms have different degress of “cooperativeness”
required/leveraged. This could explain variance of these mixed results. In particular, IQL is an independent learning
algorithm, hence agents do not cooperate in any way during training (on the contrary, the hinder each other with induced
non stationarity). Thus IQL naturally struggles in highly cooperative tasks, such as flocking and give-way. Here, the
causal augmentation cannot provide much help, especially cause the naive approach exploited here does not leverage
agent cooperation (not in discovery, nor in inference).
To investigate further and provide all the possible evidence to the roadmap coming in Section 5, we now turn to some
scenario-dependent metrics.
In Figure 4 we zoom-in on a reward metric specific to the navigation scenario, pos_rew, that measures how close
each agent is to its own goal (the higher the better). Here, causal augmentation consistently improves the vanilla
baselines both in average performance and variance (RQ2), except for Qmix (RQ3). Again, the reason can be
found under the cooperation perspective: while navigation is the least cooperative scenario tested, Qmix is the

Figure 4: Data distribution for metric posrew measuring the closeness of agents to goal in the Navigation task (the
higher the better).
7

Causal MARL: a Roadmap

A P REPRINT

(b) Collisions (higher values are better)

(a) Distance reward (higher is better)

Figure 5: Data distribution for reward metrics considering the inverse of the distance to goal and the collisions between
agents, in the Flocking task.
most collaborative algorithm. Hence, Qmix alone is able to successfully learn cooperative policies, and the causal
augmentation (independently applied to each agent) cannot help further.
Figure 5 instead zooms-in on scenario-dependent metrics for the flocking task: the agent_distance_rew metric, similar
to the pos_rew in the navigation scenario, and the agent_collision_rew metric, which tracks the negative rewards
agents accumulated due to collisions. We observe mixed results (RQ2,RQ3): CausalVDN slightly improves both
metrics, although collisions only in variance; CausalQmix only improves collisions, and CausalIQL does not really
improve either one, as the slight improvement in distance reward data distribution is balanced by increased variance.
The same aforementioned considerations about cooperativeness do apply here. A particularly relevant result, though, is
that collisions are better handled by the causal augmentation. This is the result of focussing on “core” environment
dynamics, and on trying to avoid “catastrophic” actions. Hence, causal augmentation can open up novel ways of
pursuing safety in RL (training and learnt policies).
Finally, in Figure 6 we zoom-in on the mean episodic reward of the give-way task, that confirms the exceptional
difficulty of CausalIQL in keeping up with all the other approaches, even its vanilla version (RQ3). Not surprisingly,
the causal augmentation struggles in the “worst combination” of task/algorithm: independent learning (IQL) exploited
in the most highly collaborative scenario (Give-Way).

5

Roadmap towards Causal MARL

Given the preliminary results presented in previous section, we can now speculate on the promising research directions
opened, and open issues raised, by attempting to apply causal discovery and inference in MARL settings.
Causal discovery policy. Our naive causal discovery approach exploits a random policy to build the causal graph.
Obviously, that is inefficient as, generally speaking, there is no guarantee that the state-action-reward trajectories
generated are informative enough with respect to the causal relationships to be learnt. A random policy also likely
induce high variance, as demonstrated by the corresponding high variance of some of the results shown in Section 4. A
promising way forward is to use experimental design principles to plan highly informative interventions (i.e. actions)
in the environment yielding relevant causal information, as proposed by Kocaoglu et al. (2017). Notice that our

Figure 6: Give-Way task, episode_reward_mean metric (higher is better).
8

Causal MARL: a Roadmap

A P REPRINT

naive causal augmentation assumes that causal discovery precedes the RL stage, mostly for simplicity. Conceptually,
though, nothing prevents an alternative view in which causal discovery and RL (there including the causal inference
augmentation) proceed in parallel. In that case, deciding whether to use the same RL policy under training to guide
causal discovery, or another arbitrary one, may be a though choice worth some formal and experimental analysis.
Causal model assessment. The learnt causal model should be evaluated for correctness before being used for causal
inference in the RL stage, as an incorrect model can worsen RL performance. However, validation of a learnt causal
model is far from trivial, especially in MARL environments. First of all, the best way to assess the correctness of a
causal model is to compare it to a ground truth model. Though, in MARL, having access to the ground truth would
translate to having access to a model of the agent-environment dynamics—making RL superfluous in the first place.
Assessing a causal model without access to a ground truth, instead, is still an open issue—see Karimi-Mamaghan et al.
(2024). In our experiments, we simply blindly “trust” the learnt model, hence an incorrect one may explain the bad
performance in some tasks—e.g. Give-Way.
Intervention on continuous variables. The notion of intervention is a cornerstone of the definition of causality given
by Pearl (2001). The core idea is to set the value of a variable to a specific one amongst the admissible domain. However,
this conceptually simple mechanism may quickly become computationally intractable, especially when performing
causal discovery. There, one may want to try every possible combination of variable values to uncover potential causal
effects on other variables. In the case of continuous variables this brute-force mechanism is unfeasible. Discretisation
offers a practical workaround to this issue, but it is not a principled solution, and may cause loss of relevant information.
However, one promising research direction is to resort to soft interventions (see Kocaoglu et al. (2019)) in the form
of distributional shifts, or “tilts” as defined in Schindl et al. (2024), to probability density functions representing the
domain of continuous variables.
Beyond the action filter. In this paper, we used causal reasoning to learn a causal model to be used as an action space
filter. However, this is not the only way in which causal reasoning and (MA)RL can be integrated. Using counterfactuals
for credit assignment, for instance, is another possible integration, as reported in Section 2. Furthermore, it is not
straightforward to apply such filter in fully differentiable deep RL architectures, as the network is a black-box and
the action pool cannot be easily filtered before action selection. Another challenge is balancing the RL algorithm’s
exploration-exploitation trade-off, which is designed to prevent overly greedy behaviour, with the “guidance” provided
by the causal filter. On the one hand, one may want to rely as much as possible on the causal model to guide the agent in
choosing actions “rationally”; on the other hand, one must not overlap with the tasks for which RL is good—optimising
the action policy.
Agents influences. The naive causal augmentation approach that we put to test in this work is akin to an independent
learning setting in MARL: each agent attempts to learn a causal model of the MARL system in isolation, independently
of other agents. As for MARL, this approach suffers from the problem of non-stationarity, as the dynamics to learn are
influenced by learning agents in turn. We speculate that most of the failures in performance shown in Section 4 can
be attributed to this limitation. Finding ways to foster collaboration amongst agents not only in the MARL stage, but
during causal discovery, too, is likely to be key for learning correct and complete causal models. A promising research
direction here started to appear in the pervasive systems and MAS domains, for instance by Mariani and Zambonelli
(2024) and Meganck et al. (2005). The former assume that computational nodes in a distributed environment cooperate
to learn so-called “minimal causal networks” by coordinating interventions and exchanging data about variables’ values
probability distributions. The latter aims at letting agents in a MAS collaborate in building a global causal model by
linking individual models via “overlap variables”.
Formal account. Besides experimental results, having formal proofs of convergence would be a cornerstone achievement. For causal MARL, and in particular for the kind of causal augmentation sketched in this paper, a first step would
be to formalise the notion of what we called a “minimal causal model”, or, perhaps equivalently, a causal model of
the “core environment dynamics”. Having a rigorous definition of what are the causal dynamics “relevant” for any
given scenario-task pair would be a first step into a proper mathematical characterisation of causal MARL. It is worth
mentioning two specific research works that, in our opinion, are going in this direction. Mutti et al. (2023a) prove
that, given assumptions on the “common dynamics” underlying a family of environments, perfect generalisation up
to any desired error bound can be achieved via causal model-based RL. Such common dynamics may exactly be the
one captured by a minimal causal model. Liesen et al. (2024) formulates a definition of a “minimal RL environment”,
linked to the improvement of learnt policy performance. Albeit not causal, this work can help shed light about how
“core” environment dynamics may be defined.

9

Causal MARL: a Roadmap

6

A P REPRINT

Conclusions

In this paper, we formulated a research roadmap shedding light on the open issues to deal with and promising directions
to follow to successfully transfer the recent successful efforts in causal RL from a single-agent setting to MARL. To
better ground our speculations, also informed by state-of-the-art literature, we performed experiments by augmenting
popular MARL algorithms with causal discovery and inference, and testing the resulting causal versions on challenging
MARL scenarios. The mixed results obtained, on the one hand show improvements even with the simple independent
causal discovery approach adopted, on the other hand also exhibit limitations, especially in highly cooperative tasks.
Based on this quantitative analysis, our roadmap details a few crucial steps that have to be made in the short term to
advance the state of the art in causal MARL. In this context, our proposed causal augmentation is a first step on top of
which the outlined roadmap can be developed.

References
Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive computation and machine
learning. MIT Press, 1998. URL https://www.worldcat.org/oclc/37293240.
Mirco Mutti, Riccardo De Santi, Emanuele Rossi, Juan Felipe Calderón, Michael M. Bronstein, and Marcello Restelli.
Provably efficient causal model-based reinforcement learning for systematic generalization. In Thirty-Seventh
Conference on Artificial Intelligence, 2023a. doi:10.1609/AAAI.V37I8.26109.
Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and efficient off-policy reinforcement
learning. 2016. URL http://arxiv.org/abs/1606.02647.
Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning through a causal
lens. In The Thirty-Fourth Conference on Artificial Intelligence, 2020. doi:10.1609/AAAI.V34I03.5631.
Samuel J. Gershman. Reinforcement Learning and Causal Models. In The Oxford Handbook of Causal Reasoning.
Oxford University Press, 2017. ISBN 9780199399550. doi:10.1093/oxfordhb/9780199399550.013.20.
Judea Pearl. From Bayesian Networks to Causal Networks. Springer US, 1995. doi:10.1007/978-1-4899-1424-8_9.
Zhihong Deng, Jing Jiang, Guodong Long, and Chengqi Zhang. Causal reinforcement learning: A survey. 2023.
doi:10.48550/ARXIV.2307.01452.
Yan Zeng, Ruichu Cai, Fuchun Sun, Libo Huang, and Zhifeng Hao. A survey on causal reinforcement learning. 2023.
URL https://arxiv.org/abs/2302.05209.
St John Grimbly, Jonathan Shock, and Arnu Pretorius. Causal multi-agent reinforcement learning: Review and open
problems. arXiv preprint arXiv:2111.06721, 2021.
Wei Du and Shifei Ding. A survey on multi-agent deep reinforcement learning: from the perspective of challenges and
applications. Artifificial Intelligence Review, 2021. doi:10.1007/S10462-020-09938-Y.
Lei Wang, Shanshan Huang, Shu Wang, Jun Liao, Tingpeng Li, and Li Liu. A survey of causal discovery based on functional causal model.
Engineering Applications of Artificial Intelligence, 2024.
doi:https://doi.org/10.1016/j.engappai.2024.108258.
Judea Pearl. Causality: Models, reasoning, and inference. Cambridge University Press, 2001. doi:10.1016/S09252312(01)00330-7.
Judea Pearl. Graphical models for probabilistic and causal reasoning. In Computing Handbook, Third Edition:
Computer Science and Software Engineering. CRC Press, 2014.
Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Commununication of ACM,
2019. doi:10.1145/3241036.
Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh
Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 2021.
doi:10.1109/JPROC.2021.3058954.
Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Schölkopf. Identifiability of causal graphs using functional
models. 2012. URL http://arxiv.org/abs/1202.3757.
Fei Fu and Qing Zhou. Learning sparse causal gaussian networks with experimental intervention: Regularization and
coordinate descent. Journal of the American Statistical Association, 2013. doi:10.1080/01621459.2012.754359.
Yimin Huang and Marco Valtorta. Pearl’s calculus of intervention is complete. In Proceedings of the 22nd Conference in Uncertainty in Artificial Intelligence. AUAI Press, 2006. URL https://dslpitt.org/uai/
displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=1301&proceeding_id=22.
10

Causal MARL: a Roadmap

A P REPRINT

Haruo Hosoya. Group-based learning of disentangled representations with generalizability for novel contents. In
Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, 2019.
doi:10.24963/IJCAI.2019/348.
Jiayu Yao, Taylor Killian, George Konidaris, and Finale Doshi-Velez. Direct policy transfer via hidden parameter
markov decision processes. 2018.
Hao Duong Le, Xin Xia, and Zhang Chen. Multi-agent causal discovery using large language models. arXiv preprint
arXiv:2407.15073, 2024.
Rafael Pina, Varuna De Silva, and Corentin Artaud. Discovering causality for efficient cooperation in multi-agent
environments. arXiv preprint arXiv:2306.11846, 2023.
Shantian Yang, Bo Yang, Zheng Zeng, and Zhongfeng Kang. Causal inference multi-agent reinforcement learning for
traffic signal control. Information Fusion, 2023. doi:10.1016/J.INFFUS.2023.02.009.
Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Changjie Fan, Fei Wu, and Jun Xiao. Deconfounded
value decomposition for multi-agent reinforcement learning. In International Conference on Machine Learning, Proceedings of Machine Learning Research, 2022. URL https://proceedings.mlr.press/v162/li22l.
html.
Zhiyuan Li, Lijun Wu, Kaile Su, Wei Wu, Yulin Jing, Tong Wu, Weiwei Duan, Xiaofeng Yue, Xiyi Tong,
and Yizhou Han. Coordination as inference in multi-agent reinforcement learning. Neural Networks, 2024.
doi:10.1016/J.NEUNET.2024.106101.
Patrik Reizinger, Siyuan Guo, Ferenc Huszár, Bernhard Schölkopf, and Wieland Brendel. Identifiable exchangeable
mechanisms for causal structure and representation learning. arXiv preprint arXiv:2406.14302, 2024.
Kun Jiang, Wenzhang Liu, Yuanda Wang, Lu Dong, and Changyin Sun. Discovering latent variables for the
tasks with confounders in multi-agent reinforcement learning. IEEE CAA Journal of Automatica Sinica, 2024.
doi:10.1109/JAS.2024.124281.
Jarek Liesen, Chris Lu, Andrei Lupu, Jakob N Foerster, Henning Sprekeler, and Robert T Lange. Discovering minimal
reinforcement learning environments. arXiv preprint arXiv:2406.12589, 2024.
Tim Genewein, Tom McGrath, Grégoire Delétang, Vladimir Mikulik, Miljan Martic, Shane Legg, and Pedro A Ortega.
Algorithms for causal reasoning in probability trees. arXiv preprint arXiv:2010.12237, 2020.
Maximilian Seitzer, Bernhard Schölkopf, and Georg Martius.
Causal influence detection for improving efficiency in reinforcement learning.
In 34th Annual Conference on Neural Information Processing Systems, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
c1722a7941d61aad6e651a35b65a9c3e-Abstract.html.
Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, and Giorgia Ramponi. Exploiting causal graph
priors with posterior sampling for reinforcement learning. arXiv preprint arXiv:2310.07518, 2023b.
Chaochao Lu, Yuhuai Wu, Jośe Miguel Hernández-Lobato, and Bernhard Schölkopf. Nonlinear invariant risk minimization: A causal approach. arXiv preprint arXiv:2102.12353, 2021.
Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim.
Experimental design for learning causal graphs with latent variables.
In 30th Annual Conference on Neural Information Processing Systems, 2017.
URL https://proceedings.neurips.cc/paper/2017/hash/
291d43c696d8c3704cdbe0a72ade5f6c-Abstract.html.
Ziyan Wang, Yali Du, Yudi Zhang, Meng Fang, and Biwei Huang. MACCA: offline multi-agent reinforcement learning
with causal credit assignment. 2023. doi:10.48550/ARXIV.2312.03644.
Xiao Du, Yutong Ye, Pengyu Zhang, Yaning Yang, Mingsong Chen, and Ting Wang. Situation-dependent causal
influence-based cooperative multi-agent reinforcement learning. In Thirty-Eighth Conference on Artificial Intelligence.
AAAI Press, 2024. doi:10.1609/AAAI.V38I16.29684.
Matteo Bettini, Ajay Shankar, and Amanda Prorok. Heterogeneous multi-robot reinforcement learning. arXiv preprint
arXiv:2301.07137, 2023.
Qinghua Liu, Csaba Szepesvári, and Chi Jin.
Sample-efficient reinforcement learning of partially observable markov games.
In 35th Annual Conference on Neural Information Processing Systems, 2022.
URL http://papers.nips.cc/paper_files/paper/2022/hash/
743459dae9b2c5d2904e5432d5298128-Abstract-Conference.html.
Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search, Second Edition. Adaptive
computation and machine learning. MIT Press, 2000.
11

Causal MARL: a Roadmap

A P REPRINT

Yujia Zheng, Biwei Huang, Wei Chen, Joseph D. Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter
Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python. J. Mach. Learn. Res., 25:60:1–60:8, 2024. URL
https://jmlr.org/papers/v25/23-0970.html.
Jean-Philippe Pellet and André Elisseeff. Using markov blankets for causal structure learning. Journal of Machine
Learning Research, 2008. doi:10.5555/1390681.1442776.
Murat Kocaoglu, Amin Jaber, Karthikeyan Shanmugam, and Elias Bareinboim. Characterization and learning
of causal graphs with latent variables from soft interventions. In 32nd Annual Conference on Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
c3d96fbd5b1b45096ff04c04038fff5d-Abstract.html.
Matteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. VMAS: A vectorized multi-agent simulator
for collective robot learning. In 16th International Symposium on Distributed Autonomous Robotic Systems, Springer
Proceedings in Advanced Robotics. Springer, 2022. doi:10.1007/978-3-031-51497-5_4.
Matteo Bettini, Amanda Prorok, and Vincent Moens. Benchmarl: Benchmarking multi-agent reinforcement learning.
Journal of Machine Learning Research, 2024. URL https://jmlr.org/papers/v25/23-1612.html.
Ming Tan. Multi-agent reinforcement learning: Independent versus cooperative agents. In International Conference on
Machine Learning, 1997. URL https://api.semanticscholar.org/CorpusID:272885126.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc
Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent
learning. arXiv preprint arXiv:1706.05296, 2017.
Tabish Rashid, Mikayel Samvelyan, Christian Schröder de Witt, Gregory Farquhar, Jakob N. Foerster, and Shimon
Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine
Learning Research, 2020. URL https://jmlr.org/papers/v21/20-081.html.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems, 2021. URL https://proceedings.neurips.
cc/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html.
Rihab Gorsane, Omayma Mahjoub, Ruan de Kock, Roland Dubb, Siddarth Singh, and Arnu Pretorius. Towards
a standardised performance evaluation protocol for cooperative MARL. In 35th Annual Conference on Neural
Information Processing Systems, 2022. URL http://papers.nips.cc/paper_files/paper/2022/
hash/249f73e01f0a2bb6c8d971b565f159a7-Abstract-Conference.html.
Lars Lorch, Jonas Rothfuss, Bernhard Schölkopf, and Andreas Krause. Dibs: Differentiable bayesian structure learning.
In 34th Annual Conference on Neural Information Processing Systems, 2021. URL https://proceedings.
neurips.cc/paper/2021/hash/ca6ab34959489659f8c3776aaf1f8efd-Abstract.html.
Kyle Schindl, Shuying Shen, and Edward H Kennedy. Incremental effects for continuous exposures. arXiv preprint
arXiv:2409.11967, 2024.
Jonas Schweisthal, Dennis Frauen, Maresa Schröder, Konstantin Hess, Niki Kilbertus, and Stefan Feuerriegel. Learning
representations of instruments for partial identification of treatment effects. arXiv preprint arXiv:2410.08976, 2024.
Wolfgang Wiedermann, Bixi Zhang, Wendy Reinke, Keith C. Herman, and Alexander von Eye. Distributional causal effects: Beyond an "averagarian" view of intervention effects. Psychological Methods, 2022. doi:10.1037/met0000533.
Amir Mohammad Karimi-Mamaghan, Panagiotis Tigas, Karl Henrik Johansson, Yarin Gal, Yashas Annadani, and Stefan
Bauer. Challenges and considerations in the evaluation of bayesian causal discovery. In Forty-first International
Conference on Machine Learning. OpenReview.net, 2024. URL https://openreview.net/forum?id=
bqgtkBDkNs.
Stefano Mariani and Franco Zambonelli. Distributed discovery of causal networks in pervasive environments. In IEEE
International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events.
IEEE, 2024. doi:10.1109/PERCOMWORKSHOPS59983.2024.10502971.
Stijn Meganck, Sam Maes, Bernard Manderick, and Philippe Leray. Distributed learning of multi-agent causal models.
In IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2005.
Omry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019. URL https:
//github.com/facebookresearch/hydra.

12

Causal MARL: a Roadmap

A

A P REPRINT

Codebase

The code and execution instructions are provided in the submitted supplementary material. Documentation has been
written for all project files to ease the reproduction process.
A.1

Hyperparameters

The experiment configurations follow the structure provided by the BenchMARL framework by Bettini et al. (2024),
utilizing Hydra (Yadan (2019)) to separate YAML configuration files from the Python codebase.

B

Computational Resources Used

This work required extensive computational resources. Specifically, for running the final experiments across multiple
seeds (10, as recommended by Gorsane et al. (2022)), we estimate approximately 400 hours of high-performance
computing (HPC) time. The experiments were conducted using a server with an Intel(R) Xeon(R) CPU E5-2640 v4 @
2.40GHz (20 cores) and 126GB of RAM, and an NVIDIA Titan X (Pascal) GPU with 12GB of RAM.

C

Computational Complexity

In this section, we focus on analyzing the computational complexity of causal discovery and inference, specifically of
building the Causal Bayesian Network (CBN) used to incorporate causal knowledge.
C.1

Causal Discovery

Causal discovery involves learning the graph structure, and we use the PC algorithm by Spirtes et al. (2000), a
constraint-based method with complexity based on two main phases:
1. Skeleton Construction. The algorithm performs conditional independence tests to decide if an edge exists
between pairs of variables. In the worst case, the complexity is O(pd 2d ), where p is the number of variables
(nodes) and d is the maximum degree (edges per node).
2. Edge Orientation. Once the skeleton (undirected graph) is built, edges are oriented using v-structures, with a
complexity of O(p2 ).
C.2

Causal Inference

Once the graph is constructed, interventions are computed by manipulating the joint probability distribution over
the nodes in the Markov blanket of the reward. The complexity depends on the number of variables and their
interdependencies within the blanket. Generally, exact inference in a CBN can be exponential in the network size,
especially for more complex structures.

13

Causal MARL: a Roadmap

D

A P REPRINT

Architecture of Causal MARL Algorithms

Figure 1 shows the actual architecture of the causal augmentation of the vanilla algorithms chosen.

Figure 7: Specific CDRL architecture employed for BenchMARL algorithms: the policy considers both the actor-critic
output, similar to the baselines, as well as the actions that are selectable and those that are not, based on the causal
action mask. The key portion of this process, the forward pass, is detailed in Algorithm 1.

14

Causal MARL: a Roadmap

E

A P REPRINT

Sensitive Analysis

In this section, we outline the sensitivity analysis carried out to identify the optimal set of approximation parameters
(e.g. discretisation bins and method).
VMAS by Bettini et al. (2022) is a vectorized 2D physics engine in PyTorch with challenging multi-robot scenarios,
where: the observation space is continuous, with different size based on the task under consideration, and the action
space can be discrete or continuous, based on the user choice.
In this work, we evaluate our Causality-Driven RL framework across the navigation, flocking, and give-way scenarios,
as shown in Figure 2. We rely on discrete actions (provided by VMAS), discrete variables, and a limited observation
space. For tasks involving Lidar sensor (navigation and flocking), we simplify the information to reduce the number
of features. Specifically, for a given N number of Lidar sensors, we use the sensor with the maximum value and its
corresponding value as features, and so on, resulting in two features per sensor. Once the observation space is reduced,
we discretize all features into M bins.
To summarize, the parameters we are investigating through this sensitivity analysis are:
• L: the number of samples required to acquire accurate causal knowledge
• M : the number of bins needed to discretize the observation space without losing significant information
• N : the number of sensors to consider that best capture the collision dynamics
The policy for acquiring samples during this process is entirely random. To reduce exploration inefficiencies, we
constrained the environment’s size in scenarios where it wasn’t already limited (e.g., the navigation scenario).

Figure 8: Sensitive analysis architecture summary.
The sensitivity analysis we developed is structured as shown in Figure 8. Specifically, two key tasks are considered:
1. Causal Discovery: This task involves learning the causal structure, represented as a Directed Acyclic Graph
(DAG), from data. To evaluate the quality of the learned DAG we use several graph-based metrics, which are
grouped into two categories:
(a) Graph-distance metrics: Structural Hamming Distance, Structural Intervention Distance, Frobenius
Norm.
(b) Graph-similarity metrics: Jaccard Similarity, Degree Distribution Similarity and Clustering Coefficient
Similarity.
Given the large observation space and continuous variables within, applying the PC algorithm Spirtes et al.
(2000) directly would be computationally expensive and challenging. Therefore, we approximate the data
using parameters L = 106 , M = 100, and N = 4, perform causal discovery on this approximated dataset, and
treat the resulting DAG as the ground truth.
2. Causal Inference: This task focuses on analyzing the response of an effect variable when its cause is modified.
Specifically, we aim to predict the value of the “reward” variable based on the observation and the action taken.
For the ground truth, we use the original continuous data collected from VMAS, while the “predicted values”
are generated from a Causal Bayesian Network trained on the approximated data and the learned DAG from
that data. To assess the quality of our predictions, we apply several value-distance metrics, grouped into two
categories:
• Binary-distance metrics: Accuracy, Precision, F1-score, Recall.
15

Causal MARL: a Roadmap

A P REPRINT

• Value-distance metrics: Mean Absolute Error, Mean Squared Error, Root Mean Square Error, Median
Absolute Error.
We rescale the results of each metric to a range between 0 and 1 (in the causal-distance metrics category, we use a
fully connected graph as an edge case) and adjust the values so that the maximum value is set to 1. Then, by averaging
the metrics within each category, we obtain four values – one for each category – for every combination of sensitivity
analysis parameters. To select the optimal configuration, we multiply the averaged values from each metric category.
The best configuration is the one with the highest final value.
Here, we present the set of sensitivity parameters tested for each task and the best parameters configurations:
• Navigation:
– L: [500000, 1000000];
– M : [10, 15, 20, 25];
– N : [1, 2];
– Best: L=1000000 and M =10 and N =2. Score: 0.146.
• Flocking:
– L: [500000, 1000000];
– M : [10, 15, 20, 25];
– N : [1, 2];
– Best: L=500000 and M =10 and N =1. Score 0.444.
• Give-way:
– L: [500000, 1000000];
– M : [10, 15, 20, 25];
– Best: L=500000 and M =10. Score: 0.0 (selected due to reduced computational expense).
Figures 9, 10, and 11 present the confusion matrices for each configuration of sensitivity parameters, organized by
task. In these plots, only two parameters are displayed along the axes. For tasks with three parameters (navigation and
flocking), the two most significant parameters were selected for display based on an importance analysis performed
using a RandomForestRegressor.

(a) Binary-distance met(b) Value-distance metrics
rics

(c) Graph similarity met(d) Graph distance metrics
rics

Figure 9: Sensitive analysis for the Navigation task.

(a) Binary-distance met(b) Value-distance metrics
rics

(c) Graph similarity met(d) Graph distance metrics
rics

Figure 10: Sensitive analysis for the Flocking task.
16

Causal MARL: a Roadmap

(a) Binary-distance met(b) Value-distance metrics
rics

A P REPRINT

(c) Graph similarity met(d) Graph distance metrics
rics

Figure 11: Sensitive analysis for the Give-Way task.

F

Learnt Causal DAGs

In this section, we present the extracted DAGs for each task, based on the best approximations identified through the
sensitivity analysis outlined earlier.

(a) Complete DAG

(b) Minimal DAG

Figure 12: Navigation scenario: Left, the complete DAG outcomes from Causal Discovery. Right, the minimal DAG
considered after our assumptions.

17

Causal MARL: a Roadmap

(a) Complete DAG

A P REPRINT

(b) Minimal DAG

Figure 13: Flocking scenario: Left shows the complete DAG from Causal Discovery, while Right displays the minimal
DAG after applying our assumptions.

(a) Complete DAG

(b) Minimal DAG

Figure 14: Give-Way scenario: Left shows the complete DAG from Causal Discovery, while Right displays the minimal
DAG after applying our assumptions.

18

