Published as a conference paper at ICLR 2022

P ESSIMISTIC B OOTSTRAPPING FOR U NCERTAINTYD RIVEN O FFLINE R EINFORCEMENT L EARNING
Chenjia Bai
Harbin Institute of Technology

Lingxiao Wang
Northwestern University

Zhuoran Yang
Princeton University

baichenjia255@gmail.com

arXiv:2202.11566v1 [cs.LG] 23 Feb 2022

Zhihong Deng
University of Technology Sydney

Animesh Garg
University of Toronto
Vector Institute, NVIDIA

Peng Liu
Harbin Institute of Technology

Zhaoran Wang
Northwestern University

A BSTRACT
Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy
algorithms to offline RL usually fails due to the extrapolation error caused by
the out-of-distribution (OOD) actions. Previous methods tackle such problems
by penalizing the Q-values of OOD actions or constraining the trained policy to
be close to the behavior policy. Nevertheless, such methods typically prevent the
generalization of value functions beyond the offline data and also lack a precise
characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without
explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic
updates by penalizing the value function based on the estimated uncertainty. To
tackle the extrapolating error, we further propose a novel OOD sampling method.
We show that such OOD sampling and pessimistic bootstrapping yields a provable
uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning
for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better
performance compared to the state-of-the-art algorithms.

1

I NTRODUCTION

Deep Reinforcement Learning (DRL) (Sutton & Barto, 2018) achieves remarkable success in a variety of tasks. However, in most successful applications, DRL requires millions of interactions with
the environment. In real-world applications such as navigation (Mirowski et al., 2018) and healthcare (Yu et al., 2019), acquiring a large number of samples by following a possibly suboptimal policy
can be costly and dangerous. Alternatively, practitioners seek to develop RL algorithms that learn
a policy based solely on an offline dataset, where the dataset is typically available. However, directly adopting online DRL algorithms to the offline setting is problematic. On the one hand, policy
evaluation becomes challenging since no interaction is allowed, which limits the usage of on-policy
algorithms. On the other hand, although it is possible to slightly modify the off-policy value-based
algorithms and sample solely from the offline dataset in training, such modification typically suffers
from a significant performance drop compared with their online learning counterpart (Levine et al.,
2020). An important reason for such performance drop is the so-called distributional shift. Specifically, the offline dataset follows the visitation distribution of the behavior policies. Thus, estimating
the Q-functions of the corresponding greedy policy with the offline dataset is biased due to the difference in visitation distribution. Such bias typically leads to a significant extrapolation error for
DRL algorithms since the estimated Q-function tends to overestimate the out-of-distribution (OOD)
actions (Fujimoto et al., 2019).
1

Published as a conference paper at ICLR 2022

To tackle the distributional shift issue in offline RL, previous successful approaches typically fall
into two categories, namely, policy constraints (Kumar et al., 2019; Fujimoto & Gu, 2021) and conservative methods (Kumar et al., 2020; Yu et al., 2020; 2021). Policy constraints aim to restrict
the learned policy to be close to the behavior policy, thus reducing the extrapolation error in policy evaluation. Conservative methods seek to penalize the Q-functions for OOD actions in policy
evaluation and hinge on a gap-expanding property to regularize the OOD behavior. Nevertheless,
since policy constraints explicitly confine the policy to be close to the behavior policy, such method
tends to be easily affected by the non-optimal behavior policy. Meanwhile, although the conservative algorithms such as CQL (Kumar et al., 2020) do not require policy constraints, CQL equally
penalizes the OOD actions and lacks a precise characterization of the OOD data, which can lead
to overly conservative value functions. To obtain a more refined characterization of the OOD data,
uncertainty quantification is shown to be effective when associated with the model-based approach
(Yu et al., 2020; Kidambi et al., 2020), where the dynamics model can be learned in static data thus
providing more stable uncertainty in policy evaluation. Nevertheless, model-based methods need
additional modules and may fail when the environment becomes high-dimensional and noisy. In
addition, the uncertainty quantification for model-free RL is more challenging since the Q-function
and uncertainty quantifier need to be learned simultaneously (Yu et al., 2021).
To this end, we propose Pessimistic Bootstrapping for offline RL (PBRL), an uncertainty-driven
model-free algorithm for offline RL. To acquire reliable Q-function estimates and their corresponding uncertainty quantification, two components of PBRL play a central role, namely, bootstrapping
and OOD sampling. Specifically, we adopt bootstrapped Q-functions (Osband et al., 2016) for uncertainty quantification. We then perform pessimistic Q-updates by using such quantification as a
penalization. Nevertheless, solely adopting such penalization based on uncertainty quantification is
neither surprising nor effective. We observe that training the Q-functions based solely on the offline
dataset does not regularize the OOD behavior of the Q-functions and suffers from the extrapolation
error. To this end, we propose a novel OOD sampling technique as a regularizer of the learned Qfunctions. Specifically, we introduce additional OOD datapoints into the training buffer. The OOD
datapoint consists of states sampled from the training buffer, the corresponding OOD actions sampled from the current policy, and the corresponding OOD target based on the estimated Q-function
and uncertainty quantification. We highlight that having such OOD samples in the training buffer
plays an important role in both the Q-function estimation and the uncertainty quantification. We
remark that, OOD sampling controls the OOD behavior in training, which guarantees the stability of
the trained bootstrapped Q-functions. We further show that under some regularity conditions, such
OOD sampling is provably efficient under the linear MDP assumptions.
We highlight that PBRL exploits the OOD state-action pairs by casting a more refined penalization
over OOD data points, allowing PBRL to acquire better empirical performance than the policyconstraint and conservatism baselines. As an example, if an action lies close to the support of offline
data but is not contained in the offline dataset, the conservatism (Kumar et al., 2020) and policy
constraint (Fujimoto et al., 2019) methods tend to avoid selecting it. In contrast, PBRL tends to
assign a small Q-penalty for such an action as the underlying epistemic uncertainty is small. Hence,
the agent trained with PBRL has a higher chance consider such actions if the corresponding value
estimate is high, yielding better performance than the policy constraint and conservatism baselines.
Our experiments on the D4RL benchmark (Fu et al., 2020) show that PBRL provides reasonable
uncertainty quantification and yields better performance compared to the state-of-the-art algorithms.

2

P RELIMINARIES

We consider an episodic MDP defined by the tuple (S, A, T, r, P), where S is the state space, A is
the action space, T ∈ N is the length of episodes, r is the reward function, and P is the transition
distribution.
 PT −1 t The goal of RL is to find a policy π that maximizes the expected cumulative rewards
E
i=0 γ ri ], where γ ∈ [0, 1) is the discount factor in episodic settings. The corresponding
Q-function of the optimal policy satisfies the following Bellman operator,


T Qθ (s, a) := r(s, a) + γEs0 ∼T (·|s,a) max
Qθ− (s0 , a0 ) .
(1)
0
a

where θ is the parameters of Q-network. In DRL, the Q-value is updated by minimizing the TDerror, namely E(s,a,r,s0 ) [(Q−T Q)2 ]. Empirically, the target T Q is typically calculated by a separate
target-network parameterized by θ− without gradient propagation (Mnih et al., 2015). In online RL,
2

Published as a conference paper at ICLR 2022

one typically samples the transitions (s, a, r, s0 ) through iteratively interacting with the environment.
The Q-network is then trained by sampling from the collected transitions.
In contrast, in offline RL, the agent is not allowed to interact with the environment. The experiences
are sampled from an offline dataset Din = {(sit , ait , rti , sit+1 )}i∈[m] . Naive off-policy methods such
as Q-learning suffer from the distributional shift, which is caused by different visitation distribution
of the behavior policy and the learned policy. Specifically, the greedy action a0 chosen by the target
Q-network in s0 can be an OOD-action since (s0 , a0 ) is scarcely covered by the dateset Din . Thus,
the value functions evaluated on such OOD actions typically suffer from significant extrapolation
errors. Such errors can be further amplified through propagation and potentially diverges during
training. We tackle such a challenge by uncertainty quantification and OOD sampling.

3

P ESSIMISTIC B OOTSTRAPPING FOR O FFLINE RL

3.1

U NCERTAINTY Q UANTIFICATION WITH B OOTSTRAPPING

In PBRL, we maintain K bootstrapped Q-functions in critic to quantify the epistemic uncertainty.
Formally, we denote by Qk the k-th Q-function in the ensemble. Qk is updated by fitting the
following target
h
i
b s0 ∼P (·|s,a),a0 ∼π(·|s) Qk− (s0 , a0 ) .
Tb Qkθ (s, a) := r(s, a) + γ E
(2)
θ
Here we denote the empirical Bellman operator by Tb , which estimates the expectation
E[Qkθ− (s0 , a0 ) | s, a] empirically based on the offline dataset. We adopt such an ensemble technique
from Bootstrapped DQN (Osband et al., 2016), which is initially proposed for the online exploration
task. Intuitively, the ensemble forms an estimation of the posterior distribution of the estimated Qfunctions, which yields similar value on areas with rich data and diversely on areas with scarce data.
Thus, the deviation among the bootstrapped Q-functions yields an epistemic uncertainty estimation,
which we aim to utilize as a penalization in estimating the Q-functions. Specifically, we introduce
the following uncertainty quantification U(s, a) based on the Q-functions {Qk }k∈[K] ,
r
2
1 XK  k
k
U(s, a) := Std(Q (s, a)) =
Q (s, a) − Q̄(s, a) .
(3)
k=1
K
Here we denote by Q̄ the mean among the ensemble of Q-functions. From the Bayesian perspective,
such uncertainty quantification yields an estimation of the standard deviation of the posterior of Qfunctions. To better understand the effectiveness of such uncertainty quantification, we illustrate
with a simple prediction task. We use 10 neural networks with identical architecture and different
initialization as the ensemble. We then train the ensemble with 60 datapoints in R2 plane, where
the covariate x is generated from the standard Gaussian distribution, and the response y is obtained
by feeding x into a randomly generated neural network. We plot the datapoints for training and the
uncertainty quantification in Fig. 1(a). As shown in the figure, the uncertainty quantification rises
smoothly from the in-distribution datapoints to the OOD datapoints.
In offline RL, we perform regression (s, a) → Tb Qk (s, a) in Din to train the bootstrapped Qfunctions, which is similar to regress x → y in the illustrative task. The uncertainty quantification
allows us to quantify the deviation of a datapoint from the offline dataset, which provides more
refined conservatism compared to the previous methods (Kumar et al., 2020; Wu et al., 2019).
3.2

P ESSIMISTIC L EARNING

We now introduce the pessimistic value iteration based on the bootstrapped uncertainty quantification. The idea is to penalize the Q-functions based on the uncertainty quantification. To this end, we
propose the following target for state-action pairs sampled from Din ,
i
h
b s0 ∼P (·|s,a),a0 ∼π(·|s) Qk− (s0 , a0 ) − βin Uθ− (s0 , a0 ) ,
(4)
Tb in Qkθ (s, a) := r(s, a) + γ E
θ
where Uθ− (s0 , a0 ) is the uncertainty estimation at (s0 , a0 ) based on the target network, and βin is a
b s0 ∼P (·|s,a),a0 ∼π(·|s) is obtained by sampling the
tuning parameter. In addition, the empirical mean E
3

Published as a conference paper at ICLR 2022

1.0

Target Q-net 1

0.8

Offline
Dataset

0.6

Target Q-net 2
…

0.4

Target Q-net K

0.2

1.0
0.8
0.6

Actor
PBRL for
offline data
PBRL for
OOD data

Main Q-nets
OOD
sampling

TD-error (in)

Main Q-net 1
Main Q-net 2

0.4

…

0.2

Main Q-net K

(a) Uncertainty

Pessimistic

Gradients

Pseudo TD-error (OOD)

Pessimistic

(b) PBRL: overall architecture

Figure 1: (a) Illustration of the uncertainty estimations in the regression task. The white dots represent data points, and the color scale represents the bootstrapped-uncertainty values in the whole
input space. (b) Illustration of the workflow of PBRL. PBRL splits the loss function into two components. The TD-error (in) represents the regular TD-error for in-distribution data (i.e., from the
offline dataset), and pseudo TD-error (ood) represent the loss function for OOD actions. In the
update of Q-functions, both losses are computed and summed up for the gradient update.
transition (s, a, s0 ) from Din and further sampling a0 ∼ π(· | s0 ) from the current policy π. We denote
by Tb in Qkθ (s, a) the in-distribution target of Qkθ (s, a) and write Tb in to distinguish the in-distribution
target from that of the OOD target, which we introduce in the sequel.
We remark that there are two options to penalize the Q-functions through the operator Tb in . In
Eq. (4), we penalize the next-Q value Qkθ− (s0 , a0 ) with the corresponding uncertainty Uθ− (s0 , a0 ).
Alternatively, we can also penalize the immediate reward by r̂(s, a) := r(s, a) − Uθ (s, a) and use
r̂(s, a) in place of r(s, a) for the target. Nevertheless, since the datapoint (s, a) ∈ Din lies on richdata areas, the penalization Uθ (s, a) on the immediate reward is usually very small thus having less
effect in training. In PBRL, we penalize the uncertainty of (s0 , a0 ) in the next-Q value.
Nevertheless, our empirical findings in §D suggest that solely penalizing the uncertainty for indistribution target is insufficient to control the OOD performance of the fitted Q-functions. To
enforce direct regularization over the OOD actions, we incorporate the OOD datapoints directly in
training and sample OOD data of the form (sood , aood ) ∈ Dood . Specifically, we sample OOD states
from the in-distribution dataset Din . Correspondingly, we sample OOD actions aood by following
the current policy π(· | sood ). We highlight that such OOD sampling requires only the offline dataset
Din and does not require additional generative models or access to the simulator.
It remains to design the target for OOD samples. Since the transition P (· | sood , aood ) and reward
r(sood , aood ) are unknown, the true target of OOD sample is inapplicable. In PBRL, we propose a
novel pseudo-target for the OOD datapoints,
Tb ood Qkθ (sood , aood ) := Qkθ (sood , aood ) − βood Uθ (sood , aood ) ,

(5)

which introduces an additional uncertainty penalization Uθ (sood , aood ) to enforce pessimistic Qfunction estimation, and βood is a tuning parameter. For OOD samples that are close to the indistribution data, such penalization is small and the OOD target is close to the Q-function estimation. In contrast, for OOD samples that are distant away from the in-distribution dataset, a larger
penalization is incorporated into the OOD target. In our implementation, we introduce an additional truncation to stabilize the early stage training as max{0, T ood Qkθ (sood , aood )}. In addition,
we remark that βood is important to the empirical performance. Specifically,
• At the beginning of training, both the Q-functions and the corresponding uncertainty quantifications are inaccurate. We use a large βood to enforce a strong regularization on OOD actions.
• We then gradually decrease βood in the training process since the value estimation and uncertainty quantification becomes more accurate in training. We remark that a smaller βood requires
more accurate uncertainty estimate for the pessimistic target estimation Tb ood Qk . In addition,
a decaying parameter βood stabilizes the convergence of Qkθ (sood , aood ) in the training from
the empirical perspective.
4

Published as a conference paper at ICLR 2022

Incorporating both the in-distribution target and OOD target, we conclude the loss function for critic
in PBRL as follows,




b (s,a,r,s0 )∼D (Tb in Qk − Qk )2 + E
b sood ∼D ,aood ∼π (Tb ood Qk − Qk )2 ,
Lcritic = E
(6)
in
in
where we iteratively minimize the regular TD-error for the offline data and the pseudo TD-error for
the OOD data. Incorporated with OOD sampling, PBRL obtains a smooth and pessimistic value
function by reducing the extrapolation error caused by high-uncertain state-action pairs.
Based on the pessimistic Q-functions, we obtain the corresponding policy by solving the following
maximization problem,
h
i
b s∼D ,a∼π(·|s)
min Qk (s, a) ,
(7)
πϕ := max E
in
ϕ

k=1,...,K

where ϕ is the policy parameters. Here we follow the previous actor-critic methods (Haarnoja et al.,
2018; Fujimoto et al., 2018) and take the minimum among ensemble Q-functions, which stablizes
the training of policy network. We illustrate the overall architecture of PBRL in Fig. 1(b).
3.3

T HEORETICAL C ONNECTIONS TO LCB- PENALTY

In this section, we show that the pessimistic target in PBRL aligns closely with the recent theoretical
investigation on offline RL (Jin et al., 2021; Xie et al., 2021a). From the theoretical perspective, an
appropriate uncertainty quantification is essential to the provable efficiency in offline RL. Specifically, the ξ-uncertainty quantifier plays a central role in the analysis of both online and offline RL
(Jaksch et al., 2010; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021; Xie et al., 2021a;b).
Definition 1 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization {Γt }t∈[T ] forms a
ξ-Uncertainty Quantifier if it holds with probability at least 1 − ξ that
|Tb Vt+1 (s, a) − T Vt+1 (s, a)| ≤ Γt (s, a)
for all (s, a) ∈ S × A, where T is the Bellman equation and Tb is the empirical Bellman equation
that estimates T based on the offline data.
In linear MDPs (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021) where the transition kernel and
reward function are assumed to be linear to the state-action representation φ : S × A → Rd , The
following LCB-penalty (Abbasi-Yadkori et al., 2011; Jin et al., 2020) is known to be a ξ-uncertainty
quantifier for appropriately selected {βt }t∈[T ] ,

1/2
Γlcb (st , at ) = βt · φ(st , at )> Λ−1
,
(8)
t φ(st , at )
Pm
where Λt = i=1 φ(sit , ait )φ(sit , ait )> + λ · I accumulates the features of state-action pairs in Din
and plays the role of a pseudo-count intuitively. We remark that under such linear MDP assumptions, the penalty proposed in PBRL and Γlcb (st , at ) in linear MDPs is equivalent under a Bayesian
perspective. Specifically, we make the following claim.
Claim 1. In linear MDPs, the proposed bootstrapped uncertainty βt · U(st , at ) is an estimation to
the LCB-penalty Γlcb (st , at ) in Eq. (8) for an appropriately selected tuning parameter βt .
We refer to §A for a detailed explanation and proof. Intuitively, the bootstrapped Q-functions estimates a non-parametric Q-posterior (Osband et al., 2016; 2018a). Correspondingly, the uncertainty
quantifier U(st , at ) estimates the standard deviation of the Q-posterior, which scales with the LCBpenalty in linear MDPs. As an example, we show that under the tabular setting, Γlcb (st , at ) is
approximately proportional to the reciprocal pseudo-count of the corresponding state-action pair in
the dataset (See Lemma 2 in §A). In offline RL, such uncertainty quantification measures how trustworthy the value estimations on state-action pairs are. A low LCB-penalty (or high pseudo-count)
indicates that the corresponding state-action pair aligns with the support of offline data.
Under the linear MDP or Bellman-consistent assumptions, penalizing the estimated value function
based on the uncertainty quantification is known to yield an efficient offline RL algorithm (Jin et al.,
2021; Xie et al., 2021a;b). However, due to the large extrapolation error of neural networks, we find
that solely penalizing the value function of the in-distribution samples is insufficient to regularize
the fitted value functions of OOD state-action pairs.
5

Published as a conference paper at ICLR 2022

A key to the success of linear MDP algorithms (Jin et al., 2020; Wang et al., 2020a; Jin et al., 2021)
is the extrapolation ability through L2 -regularization in the least-squares value iteration (LSVI),
which guarantees that the linear parameterized value functions behave reasonably on OOD stateaction pairs. From a Bayesian perspective, such L2 -regularization enforces a Gaussian prior on the
estimated parameter of linear approximations, which regularizes the value function estimation on
OOD state-action pairs with limited data available. Nevertheless, our empirical study in §D shows
that L2 -regularization is not sufficient to regularize the OOD behavior of neural networks.
To this end, PBRL introduces a direct regularization over an OOD dataset. From the theoretical
perspective, we observe that adding OOD datapoint (sood , aood , y) into the offline dataset leads to an
equivalent regularization to the L2 -regularization under the linear MDP assumption. Specifically, in
linear MDPs, such additional OOD sampling yields a covariate matrix of the following form,
Xm
X
e=
φ(sood , aood )φ(sood , aood )> ,
Λ
φ(sit , ait )φ(sit , ait )> +
(9)
i=1
(sood ,aood ,y)∈Dood
P
where the matrix Λood = (sood ,aood ,y)∈Dood φ(sood , aood )φ(sood , aood )> plays the role of the λ·I prior
in LSVI. It remains to design a proper target y in the OOD datapoint (sood , aood , y). The following
theorem show that setting y = T Vh+1 (sood , aood ) leads to a valid ξ-uncertainty quantifier under the
linear MDP assumption.
Theorem 1. Let Λood  λ · I. Under the linear MDP assumption, for all the OOD
datapoint
√
(sood , aood , y) ∈ Dood , if we set y = T Vt+1 (sood , aood ), it then holds for βt = O T · d · log(T /ξ)

1/2
> −1
forms a valid ξ-uncertainty quantifier.
that Γlcb
t (st , at ) = βt φ(st , at ) Λt φ(st , at )
We refer to §A for a detailed discussion and proof. Theorem 1 shows that if we set y =
T Vt+1 (sood , aood ), the bootstrapped uncertainty based on disagreement among ensembles is a valid
ξ-uncertainty quantifier. However, such an OOD target is impossible to obtain in practice as it requires knowing the transition at the OOD datapoint (sood , aood ). In practice, if TD error is sufficiently
minimized, then Q(sood , aood ) should well estimate the target T Vt+1 . Thus, in PBRL, we utilize
y = Q(sood , aood ) − Γlcb (sood , aood )

(10)
lcb

ood

ood

as the OOD target, where we introduce an additional penalization Γ (s , a ) to enforce pessimism. In addition, we remark that in theory, we require that the embeddings of the OOD sample
are isotropic in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower
bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating actions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.

4

R ELATED W ORKS

Previous model-free offline RL algorithms typically rely on policy constraints to restrict the learned
policy from producing the OOD actions. In particular, previous works add behavior cloning (BC)
loss in policy training (Fujimoto et al., 2019; Fujimoto & Gu, 2021; Ghasemipour et al., 2021),
measure the divergence between the behavior policy and the learned policy (Kumar et al., 2019;
Wu et al., 2019; Kostrikov et al., 2021), apply advantage-weighted constraints to balance BC and
advantages (Siegel et al., 2020; Wang et al., 2020b), penalize the prediction-error of a variational
auto-encoder (Rezaeifar et al., 2021), and learn latent actions (or primitives) from the offline data
(Zhou et al., 2020; Ajay et al., 2021). Nevertheless, such methods may cause overly conservative
value functions and are easily affected by the behavior policy (Nair et al., 2020; Lee et al., 2021b).
We remark that the OOD actions that align closely with the support of offline data could also be
trustworthy. CQL (Kumar et al., 2020) directly minimizes the Q-values of OOD samples and thus
casts an implicit policy constraint. Our method is related to CQL in the sense that both PBRL and
CQL enforce conservatism in Q-learning. In contrast, we conduct explicit uncertainty quantification
for OOD actions, while CQL penalizes the Q-values of all OOD samples equally.
In contrast with model-free algorithms, model-based algorithms learn the dynamics model directly
with supervised learning. Similar to our work, MOPO (Yu et al., 2020) and MOReL (Kidambi
6

Published as a conference paper at ICLR 2022

et al., 2020) incorporate ensembles of dynamics-models for uncertainty quantification, and penalize
the value function through pessimistic updates. Other than the uncertainty quantification, previous
model-based methods also attempt to constrain the learned policy through BC loss (Matsushima
et al., 2020), advantage-weighted prior (Cang et al., 2021), CQL-style penalty (Yu et al., 2021), and
Riemannian submanifold (Tennenholtz et al., 2021). Decision Transformer (Chen et al., 2021) builds
a transformer-style dynamic model and casts the problem of offline RL as conditional sequence
modeling. However, such model-based methods may suffer from additional computation costs and
may perform suboptimally in complex environments (Chua et al., 2018; Janner et al., 2019). In
contrast, PBRL conducts model-free learning and is less affected by such challenges.
Our method is related to the previous online RL exploration algorithms based on uncertainty quantification, including bootstrapped Q-networks (Bai et al., 2021; Lee et al., 2021a), ensemble dynamics (Sekar et al., 2020), Bayesian NN (O’Donoghue et al., 2018; Azizzadenesheli et al., 2018), and
distributional value functions (Mavrin et al., 2019; Nikolov et al., 2019). Uncertainty quantification
is more challenging in offline RL than its online counterpart due to the limited coverage of offline
data and the distribution shift of the learned policy. In model-based offline RL, MOPO (Yu et al.,
2020) and MOReL (Kidambi et al., 2020) incorporate ensemble dynamics-model for uncertainty
quantification. BOPAH (Lee et al., 2020) combines uncertainty penalization and behavior-policy
constraints. In model-free offline RL, UWAC (Wu et al., 2021) adopts dropout-based uncertainty
(Gal & Ghahramani, 2016) while relying on policy constraints in learning value functions. In contrast, PBRL does not require additional policy constraints. In addition, according to the study in
image prediction with data shift (Ovadia et al., 2019), the bootstrapped uncertainty is more robust to
data shift than the dropout-based approach. EDAC (An et al., 2021) is a concurrent work that uses
the ensemble Q-network. Specifically, EDAC calculates the gradients of each Q-function and diversifies such gradients to ensure sufficient penalization for OOD actions. In contrast, PBRL penalizes
the OOD actions through direct OOD sampling and the associated uncertainty quantification.
Our algorithm is inspired by the recent advances in the theory of both online RL and offline RL.
Previous works propose provably efficient RL algorithms under the linear MDP assumption for both
the online setting (Jin et al., 2020) and offline setting (Jin et al., 2021), which we follow for our
analysis. In addition, previous works also study the offline RL under the Bellman completeness
assumptions (Modi et al., 2021; Uehara et al., 2021; Xie et al., 2021a; Zanette et al., 2021) and the
model-based RL under the kernelized nonlinear regulator (KNR) setting (Kakade et al., 2020; Mania
et al., 2020; Chang et al., 2021). In contrast, our paper focus on model-free RL.

5

E XPERIMENTS

In experiments, we include an additional algorithm named PBRL-prior, which is a slight modification of PBRL by incorporating random prior functions (Osband et al., 2018b). The random
prior technique is originally proposed for online exploration in Bootstrapped DQN (Osband et al.,
2016). Specifically, each Q-function in PBRL-Prior contains a trainable network Qkθ and a prior
network pk , where pk is randomly initialized and is fixed in training. The prediction of each
Q-function is the sum of the trainable network and the fixed prior, Qkp = Qkθ + pk , where Qkθ
and pk shares the same network architecture. The random prior function increases the diversity among ensemble members and improves the generalization of bootstrapped functions (Osband
et al., 2018b). We adopt SAC (Haarnoja et al., 2018) as the basic actor-critic architecture for both
PBRL and PBRL-prior. We refer to §B for the implementation details. The code is available at
https://github.com/Baichenjia/PBRL.
In D4RL benchmark (Fu et al., 2020) with various continuous-control tasks and datasets, we compare the baseline algorithms on the Gym and Adroit domains, which are more extensively studied in
the previous research. We compare PBRL and PBRL-Prior with several state-of-the-art algorithms,
including (i) BEAR (Kumar et al., 2019) that enforces policy constraints through the MMD distance,
(ii) UWAC (Wu et al., 2021) that improves BEAR through dropout uncertainty-weighted update,
(iii) CQL (Kumar et al., 2020) that learns conservative value functions by minimizing Q-values of
OOD actions, (iv) MOPO (Yu et al., 2020) that quantifies the uncertainty through ensemble dynamics in a model-based setting, and (v) TD3-BC (Fujimoto & Gu, 2021), which adopts adaptive BC
constraint to regularize the policy in training.
7

Published as a conference paper at ICLR 2022

PBRL

PBRL-Prior
13.1 ±1.2
31.6 ±0.3
8.8 ±6.3

Random

TD3-BC

HalfCheetah 2.3 ±0.0
Hopper
3.9 ±2.3
Walker2d
12.8 ±10.2

Medium

MOPO

HalfCheetah 43.0 ±0.2
Hopper
51.8 ±4.0
Walker2d
-0.2 ±0.1

Medium
Replay

CQL

HalfCheetah 36.3 ±3.1 35.9 ±3.7 45.5 ±0.7 69.2 ±1.1
Hopper
52.2 ±19.3 25.3 ±1.7 88.7 ±12.9 32.7 ±9.4
Walker2d
7.0 ±7.8 23.6 ±6.9 81.8 ±2.7 73.7 ±9.4

Medium
Expert

UWAC

HalfCheetah 46.0 ±4.7 42.7 ±0.3 75.6 ±25.7 70.3 ±21.9 90.7 ±4.3 92.3 ±1.1 93.6 ±2.3
Hopper
50.6 ±25.3 44.9 ±8.1 105.6 ±12.9 60.6 ±32.5 98.0 ±9.4 110.8 ±0.8 111.2 ±0.7
Walker2d
22.1 ±44.9 96.5 ±9.1 107.9 ±1.6 77.4 ±27.9 110.1 ±0.5 110.1 ±0.3 109.8 ±0.2

Expert

BEAR

HalfCheetah 92.7 ±0.6 92.9 ±0.6 96.3 ±1.3 81.3 ±21.8 96.7 ±1.1 92.4 ±1.7 96.2 ±2.3
Hopper
54.6 ±21.0 110.5 ±0.5 96.5 ±28.0 62.5 ±29.0 107.8 ±7
110.5 ±0.4 110.4 ±0.3
Walker2d 106.6 ±6.8 108.4 ±0.4 108.5 ±0.5 62.4 ±3.2 110.2 ±0.3 108.3 ±0.3 108.8 ±0.2
Average

2.3 ±0.0 17.5 ±1.5 35.9 ±2.9 11.0 ±1.1
2.7 ±0.3 7.9 ±0.4 16.7 ±12.2 8.5 ±0.6
2.0 ±0.4 5.1 ±1.3 4.2 ±5.7
1.6 ±1.7

11.0 ±5.8
26.8 ±9.3
8.1 ±4.4

42.2 ±0.4 47.0 ±0.5 73.1 ±2.4 48.3 ±0.3
50.9 ±4.4 53.0 ±28.5 38.3 ±34.9 59.3 ±4.2
75.4 ±3.0 73.3 ±17.7 41.2 ±30.8 83.7 ±2.1

57.9 ±1.5 58.2 ±1.5
75.3 ±31.2 81.6 ±14.5
89.6 ±0.7 90.3 ±1.2

44.6 ±0.5 45.1 ±8.0 49.5 ±0.8
60.9 ±18.8 100.6 ±1.0 100.7 ±0.4
81.8 ±5.5 77.7 ±14.5 86.2 ±3.4

38.78 ±10.0 50.41 ±2.7 67.35 ±9.1 53.3 ±16.3 67.55 ±3.8 74.37 ±5.3 76.66 ±2.4

Table 1: Average normalized score and the standard deviation of all algorithms over five seeds in
Gym. The highest performing scores are highlighted. The score of TD3-BC is the reported scores
in Table 7 of Fujimoto & Gu (2021). The scores for other baselines are obtained by re-training with
the ‘v2’ dataset of D4RL (Fu et al., 2020).
Results in Gym domain. The Gym domain includes three environments (HalfCheetah, Hopper, and Walker2d) with five dataset types (random, medium, medium-replay, medium-expert,
and expert), leading to a total of 15 problem setups. We train all the baseline algorithms in the
latest released ‘v2’ version dataset, which is also adopted in TD3-BC (Fujimoto & Gu, 2021)
for evaluation. For methods that are originally evaluated on the ‘v0’ dataset, we retrain with
their respective official implementations on the ‘v2’ dataset. We refer to §B for the training details. We train each method for one million time steps and report the final evaluation performance through online interaction. Table 1 reports the normalized score for each task and the corresponding average performance. We find CQL and TD3-BC perform the best among all baselines, and PBRL outperforms the baselines in most of the tasks. In addition, PBRL-Prior slightly
outperforms PBRL and is more stable in training with a reduced variance among different seeds.
Average Performance in D4RL
We observe that compared with the baseline algorithms, PBRL has strong advantages in the nonoptimal datasets, including medium, medium-replay,
and medium-expert. In addition, compared with the
policy-constraint baselines, PBRL exploits the optimal trajectory covered in the dataset in a theoretically grounded way and is less affected by the beBEAR
havior policy. We report the average training curves
UWAC
MOPO
in Fig. 2. In addition, we remark that the perforCQL
TD3-BC
mance of PBRL and PBRL-Prior are weaker than
PBRL (ours)
TD3-BC and CQL in the early stage of training, inPBRL-Prior (ours)
dicating that the uncertainty quantification is inaccuGradient Steps (thousands)
rate initially. Nevertheless, PBRL and PBRL-Prior
Figure 2: Average training curve in Gym.
converge to better policies that outperform the baselines in the learning of uncertainty quantifiers, demonstrating the effectiveness of uncertainty penalization and OOD sampling.
80

D4RL Normalized Score

70
60
50
40
30
20
10

0

0

200

400

600

800

1000

Results in Adroit domain. The adroit tasks are more challenging than the Gym domain in task
complexity. In addition, the use of human demonstration in the dataset makes the task even more
challenging in the offline setting. We defer the results to §E. We observe that CQL and BC have the
best average performance in all baselines, and PBRL outperforms baselines in most of the tasks.
8

Published as a conference paper at ICLR 2022

walker2d-medium-replay-v2 (Uncertainty)

walker2d-medium-v2 (Uncertainty)

walker2d-medium-replay-v2 (Q-value)

160

walker2d-medium-v2 (Q-value)

7

16

300
14

6

140

250

12

5
120

10

200
4

8

100

6

U-Offline
U-Policy
U-RandomAction
U-Noise1
U-Noise2

4
2
0

150

3

Q-Offline
Q-Policy
Q-RandomAction
Q-noise1
Q-noise2

80

60

U-Offline
U-Policy
U-RandomAction
U-Noise1
U-Noise2

2

1

0
0

200

400

600

800

1000

Gradient Steps (thousands)

0

200

400

600

800

1000

0

Gradient Steps (thousands)

200

400

600

800

Gradient Steps (thousands)

(a) Walker2d-Medium-Replay

1000

Q-Offline
Q-Policy
Q-RandomAction
Q-noise1
Q-noise2

100

50

0
0

200

400

600

800

1000

Gradient Steps (thousands)

(b) Walker2d-Medium

Figure 3: The uncertainty and Q-value for different state-action sets in the training process.
Uncertainty quantification. To verify the effectiveness of the bootstrapped uncertainty quantification, we record the uncertainty quantification for different sets of state-action pairs in training. In
our experiments, we consider sets that have the same states from the offline dataset but with different types of actions, including (i) aoffline , which is drawn from the offline in-distribution transition;
(ii) apolicy , which is selected by the training policy; (iii) arand , which is uniformly sampled from
the action space of the corresponding task; (iv) anoise1 = aoffline + N (0, 0.1), which adds a small
Gaussian noise onto the offline action to represent state-action pair that is close to in-distribution
data; and (v) anoise2 = aoffline + N (0, 0.5), which adds a large noise to represent the OOD action.
We compute the uncertainty and Q-value in ‘Walker2d’ task with two datasets (‘medium-replay’ and
‘medium’). The results are shown in Fig. 3. We observe that (i) PBRL yields large uncertainties for
(s, anoise2 ) and (s, arandom ), indicating that the uncertainty quantification is high on OOD samples.
(ii) The (s, aoffline ) pair has the smallest uncertainty in both settings as it is an in-distribution sample.
(iii) The (s, anoise1 ) pair has slightly higher uncertainty compared to (s, aoffline ), showing that the
uncertainty quantification rises smoothly from the in-distribution actions to the OOD actions. (iv)
The Q-function of the learned policy (s, apolicy ) is reasonable and does not deviate much from
the in-distribution actions, which shows that the learned policy does not take actions with high
uncertainty due to the penalty with uncertainty quantification. In addition, we observe that there is no
superior maximum for OOD actions, indicating that by incorporating the uncertainty quantification
and OOD sampling, the Q-functions obtained by PBRL does not suffer from the extrapolation error.
Ablation study. In the following, we briefly report the result of the ablation study. We refer to
§C and §D for the details. (i) Number of bootstrapped-Q. We attempt different numbers K of
bootstrapped-Q in PBRL, and find the performance to be reasonable for K ≥ 6. (ii) Penalization
in Tb in . We conduct experiments with the penalized reward discussed in §3.2 and find the penalized
reward does not improve the performance. (iii) Factor βin in Tb in . We conduct experiments with
different βin from {0.1, 0.01, 0.001, 0.0001} to study the sensitiveness. Our experiments show that
PBRL performs the best for βin ∈ [0.0001, 0.01]. (iv) Factor βood . We use a large βood at the initial
training stage to enforce strong regularization over the OOD actions, and gradually decrease βood
in training. We conduct experiments by setting βood to different constants, and find our decaying
strategy generalizes better among tasks. (v) The learning target of OOD actions. We change the
learning target of OOD actions to the most pessimistic zero target y = 0 and find such a setting
leads to overly pessimistic value functions with suboptimal performance. (vi) Regularization types.
We conduct experiments with different regularization types, including our proposed OOD sampling,
L2 -regularization, spectral normalization, pessimistic initialization, and no regularization. We find
OOD sampling the only reasonable regularization strategy and defer the complete report to §D.

6

C ONCLUSION

In this paper, we propose PBRL, an uncertainty-based model-free offline RL algorithm. We propose bootstrapped uncertainty to guide the provably efficient pessimism, and a novel OOD sampling
technique to regularize the OOD actions. PBRL is closely related to the provable efficient offline RL
algorithm under the linear MDP assumption. Our experiments show that PBRL outperforms several
strong offline RL baselines in the D4RL environments. PBRL exploits the optimal trajectories contained in the suboptimal dataset and is less affected by the behavior policy. Meanwhile, we show
that PBRL produces reliable uncertainty quantifications incorporated with OOD sampling.
9

Published as a conference paper at ICLR 2022

R EFERENCES
Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic
bandits. In Advances in neural information processing systems, volume 24, pp. 2312–2320, 2011.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. In Advances in neural information processing systems, 2021.
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on
Learning Representations, 2021.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In Advances in neural information processing
systems, 2021.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, 2017.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA),
pp. 1–9. IEEE, 2018.
Chenjia Bai, Lingxiao Wang, Lei Han, Jianye Hao, Animesh Garg, Peng Liu, and Zhaoran Wang.
Principled exploration via optimistic bootstrapping and backward induction. In International
Conference on Machine Learning, pp. 577–587. PMLR, 2021.
Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin. Behavioral priors and
dynamics models: Improving performance and domain transfer in offline rl. arXiv preprint
arXiv:2106.09119, 2021.
Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating covariate shift in imitation learning via offline data without great coverage. arXiv preprint
arXiv:2106.03207, 2021.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. In Advances in neural information processing systems, 2021.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information
Processing Systems, 31, 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In
Advances in neural information processing systems, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, pp. 1050–1059.
PMLR, 2016.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expectedmax q-learning operator for simple yet effective offline and online rl. In International Conference
on Machine Learning, pp. 3682–3691. PMLR, 2021.
10

Published as a conference paper at ICLR 2022

Florin Gogianu, Tudor Berariu, Mihaela C Rosca, Claudia Clopath, Lucian Busoniu, and Razvan
Pascanu. Spectral normalisation for deep reinforcement learning: An optimisation perspective.
In International Conference on Machine Learning, volume 139, pp. 3734–3744, 2021.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861–1870. PMLR, 2018.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 2010.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. Advances in Neural Information Processing Systems, 32:12519–
12530, 2019.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021.
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theoretic regret bounds for online nonlinear control. arXiv preprint arXiv:2006.12466,
2020.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21810–21823, 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, volume 32, pp. 11784–11794, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 1179–1191, 2020.
Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. Batch reinforcement
learning with hyperparameter gradients. In International Conference on Machine Learning, pp.
5725–5735. PMLR, 2020.
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In International Conference on Machine Learning, pp. 6131–6141. PMLR, 2021a.
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online
reinforcement learning via balanced replay and pessimistic q-ensemble. In Annual Conference on
Robot Learning, 2021b.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li FeiFei, Silvio Savarese, Yuke Zhu, and Roberto Martı́n-Martı́n. What matters in learning from offline
human demonstrations for robot manipulation. In Annual Conference on Robot Learning, 2021.
Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. arXiv preprint arXiv:2006.10277, 2020.
11

Published as a conference paper at ICLR 2022

Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deploymentefficient reinforcement learning via model-based offline optimization. In International Conference on Learning Representations, 2020.
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional
reinforcement learning for efficient exploration. In International conference on machine learning,
pp. 4424–4434. PMLR, 2019.
Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis
Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in
cities without a map. Advances in Neural Information Processing Systems, 31:2419–2430, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518:529–533, 2015.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank MDPs. arXiv preprint arXiv:2102.07035,
2021.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Informationdirected exploration for deep reinforcement learning. In International Conference on Learning
Representations, 2019.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in Neural Information Processing Systems, volume 29, pp. 4026–
4034, 2016.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018a.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. Advances in Neural Information Processing Systems, 31, 2018b.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems,
32:13991–14002, 2019.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In International Conference on Machine Learning, pp. 3836–3845,
2018.
Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. arXiv preprint
arXiv:2106.06431, 2021.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. In International Conference on Machine
Learning, pp. 8583–8592. PMLR, 2020.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavioral modelling priors for offline reinforcement learning. In International Conference on Learning Representations, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
12

Published as a conference paper at ICLR 2022

Guy Tennenholtz, Nir Baram, and Shie Mannor. GELATO: geometrically enriched latent model for
offline reinforcement learning. CoRR, abs/2102.11327, 2021.
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline
RL in low-rank MDPs. arXiv preprint arXiv:2110.04652, 2021.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement learning with linear function approximation. In Advances in neural information processing
systems, 2020a.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. Advances in Neural Information Processing Systems, 33, 2020b.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In
International Conference on Machine Learning, volume 139, pp. 11319–11328, 2021.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. In Advances in neural information processing systems, 2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895,
2021b.
Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995–7004. PMLR, 2019.
Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv
preprint arXiv:1908.08796, 2019.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In Advances in Neural
Information Processing Systems, volume 33, pp. 14129–14142, 2020.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. In Advances in neural information
processing systems, 2021.
Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in neural information processing systems, 2021.
Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In Conference on Robot Learning, 2020.

13

Published as a conference paper at ICLR 2022

A

T HEORETICAL P ROOF

A.1

BACKGROUND OF LCB- PENALTY IN LINEAR MDP S

In this section, we introduce the provably efficient LCB-penalty in linear MDPs (Abbasi-Yadkori
et al., 2011; Jin et al., 2020; 2021). We consider the setting of γ = 1 in the following. In linear
MDPs, the feature map of the state-action pair takes the form of φ : S × A → Rd , and the transition
kernel and reward function are assumed to be linear in φ. As a result, for any policy π, the stateaction value function is also linear in φ (Jin et al., 2020), that is,
Qπ (sit , ait ) = w
bt> φ(sit , ait ).

(11)

The parameter wt can be solved in the closed-form by following the Least-Squares Value Iteration
(LSVI) algorithm, which minimizes the following loss function,
w
bt = min

w∈Rd

m
X

φ(sit , ait )> w − r(sit , ait ) − Vt+1 (sit+1 )

2

+ λ · kwk22 ,

(12)

i=1

where Vt+1 is the estimated value function in the (t + 1)-th step, and r(sit , ait ) + Vt+1 (sit+1 ) is the
target of LSVI. The explicit solution to (12) takes the form of
w
bt = Λ−1
t

m
X


φ(sit , ait ) Vt+1 (sit+1 ) + r(sit , ait ) ,

Λt =

i=1

m
X

φ(sit , ait )φ(sit , ait )> + λ · I.

(13)

i=1

Here Λt accumulate the state-action features from the training buffer. Based on the solution of wt ,
the action-value function can be estimated by Qt (st , at ) ≈ w
bt> φ(st , at ). In addition, in offline RL
with linear function assumption, the following LCB-penalty yields an uncertainty quantification,

1/2
Γlcb (st , at ) = βt · φ(st , at )> Λ−1
,
(14)
t φ(st , at )
which measures the confidence interval of the Q-functions with the given training data (Abbasib t (st , at )
Yadkori et al., 2011; Jin et al., 2020; 2021). In offline RL, the pessimistic value function Q
lcb
penalizes Qt by the uncertainty quantification Γ (st , at ) as a penalty as follows,
b t (st , at ) = Qt (st , at ) − Γlcb (st , at )
Q
= wt> φ(st , at ) − Γlcb (st , at ),

(15)

where wt is defined in Eq. (12). Under the linear MDP setting, such pessimistic value iteration
is known to be information-theoretically optimal (Jin et al., 2021). In addition, exploration with
Γlcb (st , at ) as a bonus is also provably efficient in the online RL setting (Abbasi-Yadkori et al.,
2011; Jin et al., 2020).
A.2

C ONNECTION BETWEEN THE BOOTSTRAPPED UNCERTAINTY AND Γlcb

In the sequel, we consider a Bayesian linear regression perspective of LSVI in Eq. (12). According
to the Bellman equation, the objective of LSVI is to approximate the Bellman target bit = r(sit , ait ) +
Vt+1 (sit+1 ) with the Q-function Qt , where Vt+1 is the estimated value function in the (t+1)-th step.
In linear MDPs, We parameterize the Q-function by Qt (st , at ) = w
bt> φ(st , at ). We further define
the noise  in this least-square problem as follows,
 = bit − wt> φ(st , at ),

(16)

where wt is the underlying true parameter. In the offline dataset with Din = {(sit , ait , sit+1 )}i∈[m] ,
we denote by w
bt the Bayesian posterior of w given the dataset Din . In addition, we assume that

we are given a Gaussian prior of the parameter w ∼ N (0, I/λ) as a non-informative prior. The
following Lemma establishes connections between bootstrapped uncertainty and the LCB-penalty
Γlcb .
Lemma 1 (Formal Version of Claim 1). We assume that  follows the standard Gaussian distribution
N (0, 1) given the state-action pair (sit , ait ). It then holds for the posterior wt given Din that


i i
Varwbt Qt (sit , ait ) = Varwbt φ(sit , ait )> w
bt = φ(sit , ait )> Λ−1
∀(sit , ait ) ∈ S × A. (17)
t φ(st , at ),
14

Published as a conference paper at ICLR 2022

Proof. The proof follows the standard analysis of Bayesian linear regression. Under the assumption
that  ∼ N (0, 1), we obtain that

bit | (sit , ait ), w
b∼N w
bt> φ(sit , ait ), 1 .
(18)
Recall that we have the prior distribution w ∼ N (0, I/λ). Our objective is to compute the posterior
density w
bt = w | Din . It holds from Bayes rule that
log p(w
b | Din ) = log p(w)
b + log p(Din | w)
b + Const.,

(19)

where p(·) denote the probability density function of the respective distributions. Plugging (18) and
the probability density function of Gaussian distribution into (19) yields
log p(w
b | Din ) = −kwk
b 2 /2 −

m
X

kw
b> φ(sit , ait ) − yti k2 /2 + Const.

= −(w
b − µt )

i=1
Λ−1
b − µt )/2 + Const.,
t (w

φ(sit , ait )yti ,

Λt =

>

where we define
µt = Λ−1
t

m
X
i=1

m
X

φ(sit , ait )φ(sit , ait )> + λ · I.

(20)

i=1

Then we obtain that w
bt = w | Din ∼ N (µt , Λ−1
t ). It then holds for all (st , at ) ∈ S × A that


i i
Var φ(sit , ait )> w
bt = Var Qt (sit , ait ) = φ(sit , ait )> Λ−1
t φ(st , at ),

(21)

which concludes the proof.
In Lemma 1, we show
 that the standard deviation of the Q-posterior is equivalent to the LCBpenalty Var Q(st , at ) = φ(st , at )> Λ−1
t φ(st , at ) introduced in §A.1. Recall that our proposed
bootstrapped uncertainty takes the form of

U(st , at ) ≈ Std Qk (st , at ) ,
(22)
which is the standard deviation of the bootstrapped Q-functions. Such bootstrapping serves as an
estimation of the posterior of Q-functions (Osband et al., 2016). Thus, our proposed uncertainty
quantification can be seen as an estimation of the LCB-penalty under the linear MDP assumptions.
In addition, under the tabular setting where the states and actions are finite, the LCB-penalty takes a
simpler form, which we show in the following lemma.
Lemma 2. In tabular MDPs, the bootstrapped uncertainty U(s, a) is approximately proportional to
the reciprocal-count of (s, a), that is,
U(s, a) ≈ Γlcb (s, a)/βt = p

1
.
Ns,a + λ

(23)

Proof. In tabular MDPs, we consider the joint state-action space d = |S| × |A|. Then j-th stateaction pair can be encoded as a one-hot vector as φ(s, a) ∈ Rd , where j ∈ [0, d − 1], thus is a special
case of the linear MDP (Yang & Wang, 2019; Jin et al., 2020). Specifically, we define
 0
 0 ··· 0 ··· 0
..
.
..
 .
 .. . . .
.
d
>
d×d



1
0 ∈ R
φ(sj , aj ) =  1 ∈ R ,
φ(sj , aj )φ(sj , aj ) =  0
,
(24)

..
..
. . ..
..
.
.
0 ··· 0 ··· 0

0

where
of φ(sj , aj ) is 1 at the j-th entry and 0 elsewhere. Then the matrix Λj =
Pm thei value
i
i
i >
>
φ(s
,
a
)φ(s
j
j
j , aj ) + λ · I is the sum of φ(sj , aj )φ(sj , aj ) over (sj , aj ) ∈ Din , which
i=0
takes the form of


···
···

n0 +λ
0
0
n1 +λ




Λj = 



..
.

0

..
.

0

..
···

···

..
.

.
nj +λ

0

..

..
.

···

15

0
0

.




,



nd−1 +λ

(25)

Published as a conference paper at ICLR 2022

where the j-th diagonal element of Λj is the corresponding counts for state-action (sj , aj ), i.e.,
nj = Nsj ,aj .
It thus holds that

1/2
=p
φ(sj , aj )> Λ−1
j φ(sj , aj )

1
Nsj ,aj + λ

,

(26)

which concludes the proof.
A.3

R EGULARIZATION WITH OOD S AMPLING

In this section, we discuss how OOD sampling plays the role of regularization in RL, which regularizes the extrapolation behavior of the estimated Q-functions on OOD samples.
Similar to §A.2, we consider the setup of LSVI-UCB (Jin et al., 2020) under linear MDPs. Specifically, we assume that the transition dynamics and reward function takes the form of
Pt (st+1 | st , at ) = hψ(st+1 ), φ(st , at )i,

r(st , at ) = θ> φ(st , at ),

∀(st+1 , at , st ) ∈ S × A × S,
(27)
where the feature embedding φ : S ×A 7→ Rd is known. We further assume that the reward function
r : S × A 7→ [0, 1] is bounded and the feature is bounded by kφk2 ≤ 1. Given the dataset Din , LSVI
iteratively minimizes the least-square loss in Eq. (12). Recall that the explicit solution to Eq. (12)
takes the form of
m
m
X
X

i i
i
i i
φ(sit , ait )φ(sit , ait )> + λ · I. (28)
w
bt = Λ−1
φ(s
,
a
)
V
(s
)
+
r(s
,
a
)
,
Λ
=
t+1 t+1
t
t
t t
t t
i=1

i=1

We remark that for the regression problem in Eq. (12), the L2 -regularizer λ · kwk22 enforces a Gaussian prior under the notion of Bayesian regression. Such regularization ensures that the linear function approximation φ> wti extrapolates well outside the region covered by the dataset Din .
However, as shown in §D, we observe that such L2 -regularization is ineffective for offline DRL.
To this end, we propose OOD sampling in our proposed PBRL. To demonstrate the effectiveness of
OOD sampling as a regularizer, we consider the following least-squares loss with OOD sampling
and without the L2 -regularizer,
w
eti = min

w∈Rd

m
X

2
φ(sit , ait )> w − r(sit , ait ) − Vt+1 (sit+1 ) +

i=1

2
φ(sood , aood )> w − y .

X
(sood ,aood ,y)∈Dood

(29)
The explicit solution of Eq. (29) takes the form of
X
m

e −1
w
eti = Λ
φ(sit , ait ) r(sit , ait ) + Vt+1 (sit+1 ) +
t
i=1

where
et =
Λ

m
X
i=1

X


φ(sood , aood )y ,

(30)

(sood ,aood ,y)∈Dood

X

φ(sit , ait )φ(sit , ait )> +

φ(sood , aood )φ(sood , aood )> .

(31)

(sood ,aood ,y)∈Dood

Hence, if we further set y = 0 for all (sood , aood , y) ∈ Dood , then (29) enforces a Gaussian prior with
the covariance matrix Λ−1
ood , where we define
X
Λood =
φ(sood , aood )φ(sood , aood )> .
(32)
(sood ,aood ,y)∈Dood

Specifically, if we further enforce Dood = {(sj , aj , 0)}j∈[d] with φ(sj , aj ) = λ · ej , where ej ∈ Rd
is the unit vector with the j-th entry equals one, it further holds that Λood = λ · I and Eq. (29)
is equivalent to Eq. (12). In addition, under the tabular setting, by following the same proof as in
Lemma 2, having such OOD samples in the training is equivalent to setting the count in Eq. (26) to
be
es ,a = Ns ,a + Nsood,a ,
N
j
j
j
j
j
j
16

Published as a conference paper at ICLR 2022

is the occurrence of (sj , aj ) in
where Nsj ,aj is the occurrence of (sj , aj ) in the dataset and Nsood
j ,aj
the OOD dataset.
However, to enforce such a regularizer without affecting the estimation of value functions, we need
to set the target y of the OOD samples to zero. In practice, we find such a setup to be overly
pessimistic. Since the Q-network is smooth, such a strong regularizer enforces the Q-functions to
be zero for state-action pairs from both the offline data and OOD data, as show in Fig. 12 and Fig. 13
of §C. We remark that adopting a nonzero OOD target y does not hinder the effect of regularization
e t . However, adopting nonzero OOD target
as it still imposes the same prior in the covariate matrix Λ
may introduce additional bias in the value function estimation and the corresponding uncertainty
quantification. To maintain a consistent and pessimistic estimation of value functions, one needs to
carefully design the nonzero OOD target y.
To this end, we recall the definition of a ξ-uncertainty quantifier in Definition 1 as follows.
Definition 2 (ξ-Uncertainty Quantifier (Jin et al., 2021)). The set of penalization {Γt }t∈[T ] forms a
ξ-Uncertainty Quantifier if it holds with probability at least 1 − ξ that
|Tb Vt+1 (s, a) − T Vt+1 (s, a)| ≤ Γt (s, a)
for all (s, a) ∈ S × A, where T is the Bellman operator and Tb is the empirical Bellman operator
that estimates T based on the data.
We remark that here we slightly abuse the notation T of Bellman operator and write T V (s, a) =
E[r(s, a)+V (s0 ) | s, a]. Under the linear MDP setup, the empirical estimation Tb Vt+1 is obtained via
fitting the least-squares loss in Eq. (29). Thus, the empirical estimation Tb Vt+1 takes the following
explicit form,
Tb Vt+1 (st , at ) = φ(st , at )> w
et ,
where w
et is the solution to the least-squares problem defined in Eq. (30). We remark that such ξuncertainty quantifier plays an important role in the theoretical analysis of RL algorithms, both for
online RL and offline RL (Abbasi-Yadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin
et al., 2020; 2021; Xie et al., 2021a;b). Our goal is therefore to design a proper OOD target y such
that we can obtain ξ-uncertainty quantifier based on the bootstrapped value functions. Our design is
motivated by the following theorem.
ood ood
Theorem 2. Let Λood  λ · I. For all the OOD
√ datapoint (s
 , a , y) ∈ Dood , if we set y =
ood ood
T Vt+1 (s , a ), it then holds for βt = O T · d · log(T /ξ) that

1/2
> −1
Γlcb
t (st , at ) = βt φ(st , at ) Λt φ(st , at )
forms a valid ξ-uncertainty quantifier.
Proof. Recall that we define the empirical Bellman operator Tb as follows,
Tb Vt+1 (st , at ) = φ(st , at )> w
et ,
It suffices to upper bound the following difference between the empirical Bellman operator and
Bellman operator
T Vt+1 (s, a) − Tb Vt+1 (s, a) = φ(s, a)> (wt − w
et ).
Here we define wt as follows
Z
wt = θ +
Vt+1 (st+1 )ψ(st+1 )dst+1 ,
(33)
S

where θ and ψ are defined in Eq. (27). It then holds that
T Vt+1 (s, a) − Tb Vt+1 (s, a) = φ(s, a)> (wt − w
et )
e −1
= φ(s, a)> wt − φ(s, a)> Λ
t

m
X


i
φ(sit , ait ) r(sit , ait ) + Vt+1
(sit+1 )

i=1

X

e −1
− φ(s, a)> Λ
t

(sood ,aood ,y)∈Dood

17

φ(sood , aood )y.

(34)

Published as a conference paper at ICLR 2022

e t and wt in Eq. (31)
where we plug the solution of w
et in Eq. (30). Meanwhile, by the definitions of Λ
and Eq. (33), respectively, we have
e −1
e
φ(s, a)> wt = φ(s, a)> Λ
t Λt wt
X
m
e −1
= φ(s, a)> Λ
φ(sit , ait )T Vt+1 (st , at ) +
t
i=1

X

ood

φ(s

ood

,a

ood

)T Vt+1 (s

ood

,a


) .

(sood ,aood ,y)∈Dood

(35)
Plugging (35) into (34) yields
T Vt+1 (s, a) − Tb Vt+1 (s, a) = (i) + (ii),

(36)

where we define
e −1
(i) = φ(s, a)> Λ
t

m
X


i
φ(sit , ait ) T Vt+1 (sit , ait ) − r(sit , ait ) − Vt+1
(sit+1 ) ,

i=1

e −1
(ii) = φ(s, a)> Λ
t

X


φ(sood , aood ) T Vt+1 (sood , aood ) − y .

(sood ,aood ,y)∈Dood

Following the standard analysis based on the concentration of self-normalized process (AbbasiYadkori et al., 2011; Azar et al., 2017; Wang et al., 2020a; Jin et al., 2020; 2021) and the fact that
Λood  λ · I, it holds that

1/2
,
(37)
|(i)| ≤ βt · φ(st , at )> Λ−1
t φ(st , at )
√

with probability at least 1 − ξ, where βt = O T · d · log(T /ξ) . Meanwhile, by setting y =
T Vt+1 (sood , aood ), it holds that (ii) = 0. Thus, we obtain from (36) that

1/2
(38)
|T Vt+1 (s, a) − Tb Vt+1 (s, a)| ≤ βt · φ(st , at )> Λ−1
t φ(st , at )
for all (s, a) ∈ S × A with probability at least 1 − ξ.
Theorem 2 allows us to further characterize the optimality gap of the pessimistic value iteration. In
particular, the following corollary holds.
Corollary 1 (Jin et al. (2021)). Under the same conditions as Theorem 2, it holds that
V ∗ (s1 ) − V π1 (s1 ) ≤

T
X



Eπ∗ Γlcb
t (st , at ) | s1

t=1

Proof. See e.g., Jin et al. (2021) for a detailed proof.
We remark that the optimality gap in Corollary 1 is information-theoretically optimal under the linear
MDP setup with finite horizon (Jin et al., 2021). Intuitively, for an offline dataset with sufficiently
good coverage on the optimal trajectories such as the experience from experts, such gap is small.
For a dataset collected from random policy, such a gap can be large. Our experiments also support
such intuition empirically, where the score obtained by training with the expert dataset is higher than
that with the random dataset.
Theorem 2 shows that if we set y = T Vt+1 (sood , aood ), then our estimation based on disagreement
among ensembles is a valid ξ-uncertainty quantifier. However, such OOD target is impossible to
obtain in practice as it requires knowing the transition at the OOD datapoint (sood , aood ). In practice,
if TD error is sufficiently minimized, then Qt+1 (s, a) should well estimate the target T Vt+1 . Thus,
in practice, we utilize
y = Qt+1 (sood , aood ) − Γt+1 (sood , aood )
as the OOD target, where we introduce an additional penalization Γt+1 (sood , aood ) to enforce the
pessimistic value estimation.
In addition, we remark that in theory, we require that the embeddings of the OOD sample are
isotropic, in the sense that the eigenvalues of the corresponding covariate matrix Λood are lower
18

Published as a conference paper at ICLR 2022

bounded. Such isotropic property can be guaranteed by randomly generating states and actions. In
practice, we find that randomly generating states is more expensive than randomly generating actions. Meanwhile, we observe that randomly generating actions alone are sufficient to guarantee
reasonable empirical performance since the generated OOD embeddings are sufficiently isotropic.
Thus, in our experiments, we randomly generate OOD actions according to our current policy and
sample OOD states from the in-distribution dataset.

B

I MPLEMENTATION D ETAIL

B.1

A LGORITHMIC D ESCRIPTION

Algorithm 1 PBRL algorithm
1: Initialize: K bootstrapped Q-networks and target Q-networks with parameter θ and θ − , policy
π with parameter ϕ, and hyper-parameters βin , βood
2: Initialize: total training steps H, current frame h = 0
3: while h < H do
4:
Sample mini-batch transitions (s, a, r, s0 ) from the offline dataset Din
5:
# Critic Training for offline data.
6:
Calculate the bootstrapped uncertainty Uθ− (s0 , a0 ) through the target-networks.
7:
Calculate the Q-target in Eq. (4) and the resulting TD-loss |Qkθ (s, a) − Tb in Qk (s, a)|2 .
8:
# Critic Training for OOD data
9:
Perform OOD sampling and obtains Nood OOD actions aood for each s.
10:
Calculate the bootstrapped uncertainty Uθ (s, aood ) for OOD actions through Q-networks.
11:
Calculate the Q-target in Eq. (5) and the pseudo TD-loss |Qkθ (s, aood )− Tb ood Qk (s, aood )|2 .
12:
Minimize |Qkθ (s, a)− Tb in Qk (s, a)|2 +|Qkθ (s, aood )− Tb ood Qk (s, aood )|2 to train θ by SGD.
13:
# Actor Training
14:
Improve πϕ by maximizing mink Qk (s, aπ ) − log πϕ (aπ |s) with entropy regularization.
15:
Update the target Q-network via θ− ← (1 − τ )θ− + τ θ.
16: end while

B.2

H YPER - PARAMETERS

Most hyper-parameters of PBRL follow the SAC implementations in https://github.com/
rail-berkeley/rlkit. We use the hyper-parameter settings in Table 2 for all the Gym domain
tasks. We use different settings of βin and βood for the experiment for Adroit domain and fix the
other hyper-parameters the same as Table 2. See §E for the setup of Adroit. In addition, we use the
same settings for discount factor, target network smoothing factor, learning rate, and optimizers as
CQL (Kumar et al., 2020).
Table 2: Hyper-parameters of PBRL
Hyper-parameters

Value

Description

K
Q-network
βin
βood

10
FC(256,256,256)
0.01
5.0 → 0.2

τ
γ
lr of actor
lr of critic
Optimizer
H
Nood

0.005
0.99
1e-4
3e-4
Adam
1M
10

The number of bootstrapped networks.
Fully Connected (FC) layers with ReLU activations.
The tuning parameter of in-distribution target Tb in .
The tuning parameter of OOD target Tb ood . We perform
linearly decay within the first 50K steps, and perform exponentially decay in the remaining steps.
Target network smoothing coefficient.
Discount factor.
Policy learning rate.
Critic learning rate.
Optimizer.
Total gradient steps.
Number of OOD actions for each state.

19

Published as a conference paper at ICLR 2022

Baselines. We conduct experiments on D4RL with the latest ‘v2’ datasets. The dataset is released
at http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2_
old/. We now introduce the implementations of baselines. (i) The implementation of CQL
(Kumar et al., 2020) is adopted from the official implementation at https://github.com/
aviralkumar2907/CQL. In our experiment, we remove the BC warm-up stage since we
find CQL performs better without warm-up for ‘v2’ dataset. (ii) For BEAR (Kumar et al.,
2019), UWAC (Wu et al., 2021) and MOPO (Yu et al., 2020), we adopt their official implementations at https://github.com/aviralkumar2907/BEAR, https://github.com/
apple/ml-uwac, and https://github.com/tianheyu927/mopo, respectively. We
adopt their default hyper-parameters in training. (iii) Since the original paper of TD3-BC (Fujimoto
& Gu, 2021) reports the performance of Gym in ‘v2’ dataset in the appendix section, we directly cite
the reported scores in Table 1. The learning curve reported in Fig. 2 is trained by implementation
released at https://github.com/sfujim/TD3_BC.
Computational cost comparison. In the sequel, we compare the computational cost of PBRL
against CQL. We conduct such a comparison based on the Halfcheetah-medium-v2 task. We measure the number of parameters, GPU memory, and runtime per epoch (1K gradient steps) for both
PBRL and CQL in the training. We run experiments on a single A100 GPU. We summarize the result in Table 3. We observe that, similar to the other ensemble-based methods such as Bootstrapped
DQN (Osband et al., 2016), IDS (Nikolov et al., 2019), and Sunrize (Lee et al., 2021a), our method
requires extra computation to handle the ensemble of Q-networks. In addition, we remark that a
large proportion of computation for CQL is due to the costs of logsumexp over multiple sampled
actions (Fujimoto & Gu, 2021), which we do not require.
Table 3: Comparison of computational costs.
CQL
PBRL

B.3

Runtime (s/epoch)

GPU memory

Number of parameters

30.3
52.1

1.1G
1.7G

0.42M
1.52M

R EMARKS ON FORMULATIONS OF PESSIMISM IN ACTOR AND CRITIC

We use different formulations to enforce pessimism in actor and critic. In the critic training, we
use the penalized Q-function as in Eq. (4) and Eq. (5). While in the actor training, we use the
minimum of ensemble Q-function as in Eq. (7). According to the analysis in EDAC [6], using the
minimum of ensemble Q-function minj=1,...,K Qj as the target is approximately equivalent to using
Q̄ − β0 · Std(Qj ) with a fixed β0 as the target. In contrast, in the critic training of PBRL, we tune
different factors (i.e., βin and βood ) for the in-distribution target and the OOD target, which yields
better performance for the critic estimation.
In the actor training, since we already have pessimistic Q-functions learned by the critic, it is not
necessary to enforce large penalties in the actor. To see such a fact, we refer to the ablation study
in Fig. 9, where utilizing the mean as the target achieves reasonable performances. We remark that
taking the minimum as the target avoids possible large values at certain state-action pairs, which
may arise due to the numerical computation in fitting neural networks. As suggested by our ablation study, taking the minimum among the ensemble of Q-functions as the target achieves the best
performance. Thus, we use the minimum among the ensemble of Q-functions as the target in PBRL.

20

Published as a conference paper at ICLR 2022

C

A BLATION S TUDY

In this section, we present the ablation study of PBRL. In the training, we perform online interactions to evaluate the performance for every 1K gradient steps. Since we run each method for 1M
gradient steps totally, each method is evaluated 1000 times in training. The experiments follow such
evaluation criteria, and each curve is drawn by 1000 evaluated scores through online interaction.
Number of bootstrapped-Q. We conduct experiments with different numbers of bootstrapped
Q-networks in PBRL, and present the performance comparison in Fig. 4. We observe that the
performance of PBRL is improved with the increase of the bootstrapped networks. We remark that
since the training of offline RL is more challenging than online RL, it is better to use sufficient
bootstrapped Q-functions to obtain a reasonable estimation of the non-parametric Q-posterior. We
observe from Fig. 4 that using K = 6, 8, and 10 yields similar final scores, and a larger K leads to
more stable performance in the training process. We adopt K = 10 for our implementation.
K=2

K=4

K=6

K=8

walker2d-medium-v2

K=10

hopper-medium-replay-v2
100

80

d4rl score

80
60
60
40
40
20
20
0
0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 4: The Ablation on the number of bootstrapped Q-functions.
Uncertainty of in-distribution target. We compare different kinds of uncertainty penalization in
Tb in for in-distribution data. (i) Penalizing the immediate reward only. (ii) Penalizing the next-Q
value only, which is adopted in PBRL. (iii) Penalizing both the immediate reward and next-Q value.
We present the comparison in Fig. 5. We observe that the target-Q penalization performs the best,
and adopt such penalization in the proposed PBRL algorithm.
Penalty only for reward

Penalty only for next-Q

walker2d-medium-v2

Penalty for both reward and next-Q
hopper-medium-replay-v2

100
80

d4rl score

80
60
60
40
40
20

0

20

0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 5: The Ablation on penalty strategy for the in-distribution data.
Tuning parameter βin . We conduct experiments with βin ∈ {0.1, 0.01, 0.001, 0.0001} to study
the sensitivity of PBRL to the tuning parameter βin for the in-distribution target. We present the
21

Published as a conference paper at ICLR 2022

comparison in Fig. 6. We observe that in the ‘medium’ dataset generated by a single policy, the
performance of PBRL is insensitive to βin . One possible reason is that since the ‘medium’ dataset is
generated by a single policy, the offline dataset tends to concentrate around a few trajectories and has
low uncertainty. Thus, the magnitude of βin has a limited effect on the penalty. Nevertheless, in the
‘medium-replay’ dataset, since the data is generated by various policies, the uncertainty of offline
data is larger than that of the ‘medium’ dataset (as shown in Fig. 3). Correspondingly, the performance of PBRL is affected by the magnitude of βin . Our experiment shows that PBRL performs the
best for βin ∈ [0.0001, 0.01]. We adopt βin = 0.01 for our implementation.
βin=0.0001

βin=0.001

βin=0.01

walker2d-medium-v2

βin=0.1

hopper-medium-replay-v2
100

80

d4rl score

80
60
60

40

40

20

20

0
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 6: The Ablation on the tuning parameter β in in the in-distribution target.
Tuning parameter βood . We use a large βood initially to enforce strong OOD regularization in
the beginning of training, and then decrease βood linearly while the training evolves. We conduct
experiments with constant settings of βood ∈ {0.01, 0.1, 1.0}. We observe that in the ‘medium’
dataset, a large βood = 1.0 performs the best since the samples are generated by a fixed policy with a
relatively concentrated in-distribution dataset. Thus, the OOD samples tend to have high uncertainty
and are less trustworthy. In contrast, in the ‘medium-replay’ dataset, a small βood ∈ {0.1, 0.01}
performs reasonably well since the mixed dataset has larger coverage of the state-action space and
the uncertainty of OOD data is smaller than that of the ‘medium’ dataset. Thus, adopting a smaller
βood for the ‘medium-replay’ dataset allows the agent to exploit the OOD actions and gain better
performance. To match all possible situations, we propose the decaying strategy. Empirically, we
find such a decaying strategy performs well in both the ‘medium’ and ‘medium-replay’ datasets.
Constant-0.01

Constant-0.1

Constant-1.0

walker2d-medium-v2

Decay (ours)

hopper-medium-replay-v2
100

80

d4rl score

80
60
60
40
40
20

20

0
0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 7: The Ablation on the tuning parameter β ood in the OOD target.
We also record the Q-value for (s, a) ∼ Din in the training process. As shown in Fig. 8, since both
the in-distribution actions and OOD actions are represented by the same Q-network, a large con22

Published as a conference paper at ICLR 2022

stant βood makes the Q-value for in-distribution action overly pessimistic and leads to sub-optimal
performance. It is desirable to use the decaying strategy for βood in practice.
Constant-0.01

Constant-0.1

Constant-1.0

walker2d-medium-v2

Decay (ours)

hopper-medium-replay-v2

350
200

Q Offline

300
250

150

200
100

150
100

50

50
0

0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 8: With different settings of β ood , we show the Q-value for state-action pairs sampled from
Din in the training process.
Actor training. We evaluate different actor training schemes in Eq. (7), i.e., the actor follows the
gradients of ‘min’ (in PBRL), ‘mean’ or ‘max’ value among K Q-functions. The result in Fig. 9
shows training the actor by maximizing the ‘min’ among ensemble-Q performs the best. In the
‘medium-replay’ dataset, since the uncertainty estimation is difficult in mixed data, taking ‘mean’
in the actor can be unstable in training. Taking ‘max’ in the actor performs worse in both tasks due
to overestimation of Q-functions.
Actor: Max

Actor: Mean

walker2d-medium-v2
100

80

d4rl score

Actor: Min
hopper-medium-replay-v2

80

60

60
40
40
20
20
0
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 9: The Ablation on action selection scheme of the actor.
Number of OOD samples. The pessimism in critic training is implemented by sampling OOD
actions and then performing uncertainty penalization, as shown in Eq. (6). In each training step,
we perform OOD sampling and sample Nood actions from the learned policy. We perform an ablation study with Nood ∈ {0, 2, 5, 10}. According to Fig 10, the performance is poor without OOD
sampling (i.e., Nood = 0). We find the performance becomes better with a small number of OOD
actions (i.e., Nood = 2). Also, PBRL is robust to different settings of Nood .
OOD Target. In §A.3, we show that setting the learning target of OOD samples to zero enforces a
Gaussian prior to Q-function under the linear MDP setting. However, in DRL, we find such a setup
leads to overly pessimistic value function and performs poorly in practice, as shown in Fig. 11.
Specifically, we observe that such a strong regularizer enforces the Q-functions to be close to zero
for both in-distribution and OOD state-action pairs, as shown in Fig. 12 and Fig. 13. In contrast,
23

Published as a conference paper at ICLR 2022

N-OOD: 0

N-OOD: 2

N-OOD: 5

walker2d-medium-v2

N-OOD: 10

hopper-medium-replay-v2
100

80

d4rl score

80
60
60
40
40
20

0

20

0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 10: The Ablation on different number of OOD actions. the performance becomes better even
with very small amout of OOD actions.
our proposed PBRL target performs well and does not yield large extrapolation errors, as shown in
Fig. 12 and Fig. 13.
Zero target for OOD

PBRL

walker2d-medium-v2

hopper-medium-replay-v2
100

80

d4rl score

80
60
60
40
40
20

0

20

0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 11: The Ablation on OOD target with y ood = 0 (normalized scores).

Zero target for OOD

PBRL

walker2d-medium-v2

hopper-medium-replay-v2
200

300

175

250

Q Offline

150
200

125
100

150

75

100

50
50

25
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 12: The Ablation on OOD target with y ood = 0. Q-offline is the Q-value for (s, a) pairs
sampled from the offline dataset, where a follows the behavior policy.

24

Published as a conference paper at ICLR 2022

Zero target for OOD

PBRL

walker2d-medium-v2

hopper-medium-replay-v2
200

300

175

Q CurrPolicy

250

150
200

125

150

100
75

100

50
50
0

25
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 13: The Ablation on OOD target with y ood = 0. Q-CurrPolicy is the Q-value for (s, aπ )
pairs, where aπ ∼ π(a|s) follows the training policy π.

25

Published as a conference paper at ICLR 2022

D

R EGULARIZATION FOR PBRL

In this section, we compare different regularization methods that are popular in other Deep Learning
(DL) literature to solve the extrapolation error in offline RL. Specifically, we compare the following
regularization methods.
• None. We remove the OOD-sampling regularizer in PBRL and train bootstrapped Q-functions
solely based on the offline dataset through Tb in .
• L2 -regularizer. We remove the OOD-sampling regularizer and use L2 normalization instead.
As we discussed in §3.3, in linear MDPs, LSVI utilizes L2 regularization to control the extrapolation behavior of Q-functions in OOD samples. In DRL, we add the L2 -norm of weights
in the Q-network in loss functions to conduct L2 -regularization. We attempt two scale factors
{1e − 2, 1e − 4} in our experiments.
• Spectral Normalization (SN). We remove the OOD-sampling regularizer and use SN instead.
SN constrains the Lipschitz constant of layers, which measures the smoothness of the neural
network. Recent research (Gogianu et al., 2021) shows that SN helps RL training when applied
to specific layers of the Q-network. In our experiment, we follow Gogianu et al. (2021) and
consider two cases, namely, applying SN in the output layer (denoted by SN[-1]) and applying
SN in both the output layer and the one before it (denoted by SN[-1,-2]), respectively.
• Pessimistic Initialization (PI). Optimistic initialization is simple and efficient for RL exploration, which initializes the Q-function for all actions with a high value. In online RL, such
initialization encourages the agent to explore all actions in the interaction. Motivated by this
method, we attempt a pessimistic initialization to regulate the OOD behavior of the Q-network.
In our implementation, we draw the initial value of weights and a bias of the Q-networks from
the uniform distribution Unif(a, b). We try two settings in our experiment, namely, (i) PI-small
that sets (a, b) to (−0.2, 0), and (ii) PI-large that sets (a, b) to (−1.0, 0). In both settings, we
remove the OOD sampling and use PI instead.
We illustrate (i) the normalized performance in the training process, (ii) the Q-value along the trajectory of the training policy, and (iii) the uncertainty quantification along the trajectory of the training
policy. We present the results in Fig. 14, Fig. 15, and Fig. 16, respectively. In the sequel, we discuss
the empirical results.
• We observe that OOD sampling is the only regularization method with reasonable performance.
Though L2 -regularization and SN yield reasonable performance in supervised learning, they
do not perform well in offline RL.
• In the ‘medium-replay’ dataset, we observe that PI and SN can gain some score in the early
stage of training. Nevertheless, the performance drops quickly along with the training process.
We conjecture that both PI and SN have the potential to be effective with additional parameter
tuning and algorithm design.
In conclusion, previous regularization methods for DL and RL are not sufficient to handle the distribution shift issue in offline RL. Combining such regularization techniques with policy constraints
and conservatism methods may lead to improved empirical performance, which we leave for future
research.

26

Published as a conference paper at ICLR 2022

PI-large

PI-small

SN[-1,-2]

L2(1e-4)

SN[-1]

L2(1e-2)

walker2d-medium-v2

None

PBRL

hopper-medium-replay-v2
100

80

d4rl score

80
60
60
40

40

20

0

20

0

200

400

600

800

0

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 14: Comparision of different regularizers (normalized score)

PI-large

PI-small

SN[-1,-2]

L2(1e-4)

SN[-1]

L2(1e-2)

Q value of the Current Policy

walker2d-medium-v2

None

PBRL

hopper-medium-replay-v2

5000

400
350

4000

300
3000

250
200

2000

150
100

1000

50
0

0
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 15: Comparision of different regularizers (Q-value along trajectories of the training policy)

PI-large

PI-small

SN[-1,-2]

L2(1e-4)

SN[-1]

L2(1e-2)

Uncertainty of the Current Policy

walker2d-medium-v2

None

PBRL

hopper-medium-replay-v2

5000

25

4000

20

3000

15

2000

10

1000

5

0

0
0

200

400

600

800

1000

0

200

400

600

800

1000

Gradient Steps (thousands)

Figure 16: Comparision of different regularizers (Uncertainty along trajectories of the training
policy)

27

Published as a conference paper at ICLR 2022

E

E XPERIMENTS IN A DROIT D OMAIN

In Adroit, the agent controls a 24-DoF robotic hand to hammer a nail, open a door, twirl a pen,
and move a ball, as shown in Fig. 17. The Adroit domain includes three dataset types, namely,
demonstration data from a human (‘human’), expert data from an RL policy (‘expert’), and fifty-fifty
mixed data from human demonstrations and an imitation policy (‘cloned’). The adroit tasks are more
challenging than the Gym domain in task complexity. In addition, the use of human demonstration
in the dataset also makes the task more challenging. We present the normalized scores in Table 4.
We set βin = 0.0001 and βood = 0.01. The other hyper-parameters in Adroit follows Table 2.
In Table 4, the scores of BC, BEAR, and CQL are adopted from the D4RL benchmark (Fu et al.,
2020). We do not include the scores of MOPO (Yu et al., 2020) as it is not reported and we cannot
find suitable hyper-parameters to make it work in the Adroit domain. We also attempt TD3-BC
(Fujimoto et al., 2018) with different BC weights and fail in getting reasonable score for most of
the tasks. In addition, since UWAC (Wu et al., 2021) has a different evaluation process, we re-train
UWAC with the official implementation and report the scores in Table 4.
In Table 4, we add the average score without ‘expert’ dataset since the scores of ‘expert’ dataset
dominate that of the rest of the datasets. We find CQL (Kumar et al., 2020) and BC have the best
average performance among all baselines. Meanwhile, we observe that PBRL slightly outperforms
CQL and BC. We remark that the human demonstrations stored in ‘human’ and ‘cloned’ are inherently different from machine-generated ‘expert’ data since (i) the human trajectories do not follow
the Markov property, (ii) human decision may be affected by unobserved states such as prior knowledge, distractors, and the action history, and (iii) different people demonstrate differently as their
solution policies. Such characteristic makes the ‘human’ and ‘cloned’ dataset challenging for offline RL algorithms. In addition, we remark that the recent study in robot learning from human
demonstration also encounter such challenges (Mandlekar et al., 2021).

Figure 17: Illustration of tasks in Adroit domain.
PBRL

BEAR

UWAC

CQL

MOPO

TD3-BC

Pen
Hammer
Door
Relocate
Pen
Hammer
Door
Relocate
Pen
Hammer
Door
Relocate

34.4
1.5
0.5
0.0
56.9
0.8
-0.1
-0.1
85.1
125.6
34.9
101.3

-1.0
0.3
-0.3
-0.3
26.5
0.3
-0.1
-0.3
105.9
127.3
103.4
98.6

10.1 ±3.2
1.2 ±0.7
0.4 ±0.2
0.0 ±0.0
23.0 ±6.9
0.4 ±0.0
0.0 ±0.0
-0.3 ±0.0
98.2 ±9.1
107.7 ±21.7
104.7 ±0.4
105.5 ±3.2

37.5
4.4
9.9
0.2
39.2
2.1
0.4
-0.1
107.0
86.7
101.5
95.0

-

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.3
0.0
0.0
0.0

35.4 ±3.3
0.4 ±0.3
0.1 ±0.0
0.0 ±0.0
74.9 ±9.8
0.8 ±0.5
4.6 ±4.8
-0.1 ±0.0
137.7 ±3.4
127.5 ±0.2
95.7 ±12.2
84.5 ±12.2

Average
Average w/o expert

36.73
11.74

38.41
3.21

37.57 ±3.8
4.35 ±1.4

40.31
11.70

-

0.02
0.02

46.79 ±3.9
14.52 ±2.3

Expert

Cloned

Human

BC

Table 4: Average normalized score over 3 seeds in Adroit domain. The highest performing scores
are highlighted. Among all methods, PBRL and CQL outperform the best of the baselines.

28

Published as a conference paper at ICLR 2022

F

R ELIABLE E VALUATION FOR S TATISTICAL U NCERTAINTY

Recent research (Agarwal et al., 2021) proposes reliable evaluation principles to address the statistical uncertainty in RL. Since the ordinary aggregate measures like mean can be easily dominated by
a few outlier scores, Agarwal et al. (2021) presents several efficient and robust alternatives that are
not unduly affected by outliers and have small uncertainty even with a handful of runs. In this paper,
we adopt these evaluation methods for each method in Gym domain with Mtask ∗ Nseed runs.
• Stratified Bootstrap Confidence Intervals. The Confidence Intervals (CIs) for a finite-sample
score estimates the plausible values for the true score. Bootstrap CIs with stratified sampling
can be applied to small sample sizes and is better justified than sample standard deviations.
• Performance Profiles. Performance profiles reveal performance variability through score distributions. A score distribution shows the fraction of runs above a certain score and is given by
PM 1 PN
1
F̂ (τ ) = F̂ (τ ; x1:M,1:N ) = M
m=1 N
n=1 1[xm,n ≥ τ ].
• Aggregate Metrics. Based on bootstrap CIs, we can extract aggregate metrics from score distributions, including median, mean, interquartile mean (IQM), and optimality gap. IQM discards
the bottom and top 25% of the runs and calculates the mean score of the remaining 50% runs.
Optimality gap calculates the amount of runs that fail to meet a minimum score of η = 50.0.
The result comparisons are give in Fig. 18 and Fig. 19. Specifically, Fig. 18 shows aggregate metrics
based on 95% bootstrap CIs, and Fig. 19 shows performance profiles based on score distribution.
For both evaluations, our PBRL and PBRL-prior outperforms other methods with small variability.
Median

IQM

Mean

Optimality Gap

PBRL-prior
PBRL
TD3-BC
CQL
MOPO
UWAC
BEAR
40

60

80

40

60

80

45

60

75

10

15

20

Normalized Score

Figure 18: Aggregate metrics on D4RL with 95% CIs based on 15 tasks and 5 seeds for each task.
Higher mean, median and IQM scores, and lower optimality gap are better. The CIs are estimated
using the percentile bootstrap with stratified sampling. Our methods perform better than baselines.
BEAR

UWAC

MOPO

CQL

TD3-BC

PBRL-prior

1.00

Fraction of tasks with score > τ

Fraction of runs with score > τ

1.00

PBRL

0.75

0.50

0.25

0.00

0.75

0.50

0.25

0.00
0

20

40

60

80

100

120

Normalized Score (τ)

0

20

40

60

80

100

120

Normalized Score (τ)

Figure 19: Performance profiles on D4RL based on score distributions (left), and average score
distributions (right). Shaded regions show pointwise 95% confidence bands based on percentile
bootstrap with stratified sampling. The τ value where the profiles intersect y = 0.5 shows the
median, and the area under the performance profile corresponds to the mean.

29

