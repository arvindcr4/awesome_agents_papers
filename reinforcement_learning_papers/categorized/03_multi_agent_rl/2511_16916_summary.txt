Here is the extracted information in the requested format:

**PAPER:** Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving

**TITLE:** Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving

**ARXIV_ID:** 2511.16916v1 [cs.AI]

**RESEARCH_METHOD:** 03_multi_agent_rl

**METHOD_DESCRIPTION:** This paper proposes a novel Hybrid Differential Reward (HDR) mechanism for multi-vehicle cooperative driving. The HDR mechanism combines two complementary differential signals: Temporal Difference Reward (TRD) and Action Gradient Reward (ARG). TRD ensures optimal policy invariance and provides the correct direction for long-term optimization, while ARG directly measures the marginal utility of actions and improves the signal-to-noise ratio of local policy gradients.

**KEY_CONTRIBUTIONS:**

* The paper formally elucidates the issue of low signal-to-noise ratio (SNR) in policy gradients associated with traditional state-based reward functions under high-frequency decision-making.
* The HDR framework is proposed, which integrates TRD and ARG to address the issue of vanishing reward differences.
* The multi-vehicle cooperative driving task is formulated as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set.
* The HDR mechanism is instantiated and a computable derivation scheme is provided within the POMDPG framework.
* Extensive validation is performed using both online planning (MCTS) and offline learning (QMIX, MAPPO, MADDPG) algorithms, demonstrating the effectiveness of the HDR mechanism in achieving faster convergence and higher final performance.
