AI Communications 0 (0) 1
IOS Press

1

arXiv:2208.01769v1 [cs.MA] 2 Aug 2022

Deep Reinforcement Learning for
Multi-Agent Interaction
Ibrahim H. Ahmed, Cillian Brewitt, Ignacio Carlucho, Filippos Christianos, Mhairi Dunion,
Elliot Fosong, Samuel Garcin, Shangmin Guo, Balint Gyevnar, Trevor McInroe, Georgios Papoudakis,
Arrasy Rahman, Lukas Schäfer, Massimiliano Tamborski, Giuseppe Vecchio, Cheng Wang and
Stefano V. Albrecht ∗
Autonomous Agents Research Group, School of Informatics, University of Edinburgh, United Kingdom
Abstract. The development of autonomous agents which can interact with other agents to accomplish a given task is a core area
of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops
novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and
multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent
communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sampleefficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article
provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions.
Keywords: Deep Reinforcement Learning, Multi-Agent Reinforcement Learning, Ad Hoc Teamwork, Agent/Opponent
Modelling, Goal Recognition, Autonomous Driving, Multi-Robot Warehouse

1. Autonomous Agents Research Group
The Autonomous Agents Research Group1 is a research group led by Dr. Stefano V. Albrecht in the School
of Informatics, University of Edinburgh. The long-term goal of the group is to develop artificial intelligence and
machine learning technologies which enable autonomous agents (such as robots and software agents) to solve tasks
in complex environments. The group has a strong focus on problems of coordination and cooperation in multiagent systems, in which multiple autonomous agents interact in a shared environment. Current research focuses on
algorithms for deep reinforcement learning (RL) and multi-agent reinforcement learning (MARL). The group is also
involved in the development of industry applications, including in the areas of autonomous driving (with industry
partner Five AI) and multi-robot warehouse logistics (with industry partner Dematic/KION). We are a member of
the ELLIS European network of excellence in machine learning research.
This article provides an overview of the work conducted in our group and our research contributions. We start
by providing a brief overview of the main research strands in Section 2, followed by more detailed descriptions of
research highlights in Section 3. Section 4 provides descriptions of several of our open-source code repositories.
Finally, in Section 5 we discuss important open problems in the literature before concluding in Section 6.

2. Research Strands
Research in the group focuses on the following research strands:
* Corresponding author. E-mail: s.albrecht@ed.ac.uk.
1 Homepage: https://agents.inf.ed.ac.uk, Blog: https://agents.inf.ed.ac.uk/blog

0921-7126/$35.00 © 0 – IOS Press and the authors. All rights reserved

2

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

Multi-Agent Reinforcement Learning Scalable learning of coordinated agent policies and inter-agent communication in multi-agent systems is a long-standing open problem. We tackle this problem by developing algorithms for multi-agent deep RL, in which multiple agents learn how to communicate and (inter-)act optimally
to achieve a specified goal [1–6]. While deep RL has enabled scalability to large state spaces, the goal of
MARL is to allow efficient scalability in the number of agents where the joint decision space would otherwise
be intractable for centralised approaches.
Decision Making and Modelling Other Agents Our long-term goal is to create autonomous agents capable of robust goal-directed interaction with other agents, focusing on ad hoc teamwork problems [7–10] that require
fast and effective adaptation without opportunities for prior coordination between agents. We develop algorithms which enable agents to reason about the behaviours, capabilities, and composition of other agents from
limited observations [11, 12]. These inferences are used in combination with RL and planning techniques for
effective decision-making.
Single-Agent Deep Reinforcement Learning While the group’s primary focus is on multi-agent systems, we are
also actively contributing to research in areas of single-agent RL. We develop single-agent RL algorithms
that can learn optimal and robust policies through minimal interactions with the environment. To this end, we
are developing algorithms that leverage techniques such as intrinsic motivation, curriculum learning, causal
inference, and representation learning in high-dimensional state spaces [13–16] to achieve sample-efficient
learning.
Autonomous Driving in Urban Environments We develop algorithms for autonomous driving in challenging urban environments, enabling autonomous vehicles to make fast, robust, and safe decisions by reasoning about
the actions and intent of other actors in the environment [17–22]. Research topics include: complex state estimation in uncertain and dynamic environments; efficient reasoning about intent from limited data; and computing robust plans with specified safety-compliance under conditions of dynamic, uncertain observations and
limited compute budget. We collaborate closely with Five AI, a UK-based company developing autonomous
driving technologies.
Quantum-Secure Authentication and Key Agreement Classical protocols for authentication and key establishment relying on public-key cryptography are vulnerable to quantum computing. We develop a novel, quantumresistant approach to authentication and key agreement based on the complexity of interaction in multi-agent
systems, supporting mutual and group authentication and forward secrecy [23]. We leverage recent progress
in generative adversarial training [24] and deep RL to maximise our system’s security against intruders and
modelling attacks.

3. Research Highlights
3.1. Multi-Agent Reinforcement Learning
One issue that predominantly affects MARL research is the lack of reproducibility of state-of-the-art algorithms
and the insufficient number of standardised benchmark environments. As a result, measuring the progress of research in MARL proves challenging. To address this issue, we benchmarked nine MARL algorithms in five multiagent environments within which we defined 25 different cooperative tasks [2]. We evaluated three classes of MARL
algorithms: independent learners, centralised policy gradient learners, and value decomposition learners. Additionally, we open-sourced two multi-agent environments, Level-Based Foraging (LBF) and the Multi-Robot Warehouse
(RWARE), as well as the Extended PyMARL (EPyMARL) repository, which extends PyMARL [25] to include more
algorithms, additional implementation details, and compatibility with more environments. We provide more information about the environments and open-source repositories in Section 4. Our research [2] presented standardised
performance metrics throughout training and provided the intuition behind our evaluation findings.

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

3

Exploration is a challenging problem in MARL, especially in sparse reward settings. Our work, Shared Experience
Actor (SEAC) [3], approached this problem by combining the experiences generated by different agents into more
informative learning gradients using the basic idea of off-policy correction via importance weighting. In this work,
we also discussed how the distinct behaviours of SEAC agents help with exploration. Our results showed that SEAC
significantly improves sample efficiency and greatly increases returns at policy convergence.
MARL systems with a large number of agents can lead to inefficient training of agent policies. Parameter sharing
between agents is a common approach in MARL to improve training efficiency since it decreases the number of
trainable parameters. However, naïvely sharing all parameters between all agents leads to a lack of diversity in
agent behaviour. To avoid this, in our work Selective Parameter Sharing (SePS) [4], we developed a method to
automatically identify agents that can benefit from parameter sharing. SePS uses an encoder-decoder architecture
to encode agent identities into an embedding space. Then, depending on the reward and observation functions of
the individual agents, identity embeddings are clustered with an unsupervised clustering method. Furthermore, we
provided a rigorous empirical analysis of the impact of parameter sharing and showed that SePS can be coupled
with existing MARL algorithms to achieve significantly improved performance.
Inter-agent communication is a common method for facilitating cooperation among agents. Such cooperation
leads to agents negotiating a communication protocol during the learning process [26]. In a discrete communication
channel, such emergent communication protocols are referred to as “emergent languages" since they use discrete
symbols [27]. Similar to how natural languages help humans complete tasks, we expect emergent languages to assist
agents in completing their given tasks. The degree to which agents can embed relevant knowledge in their communication channels is referred to as the “expressivity" of their communication. In our work [5], we demonstrated that
mutual information is an insufficient measurement for expressivity. Instead, we proposed to measure expressivity as
a partial ordering of languages, ranked by their generalisation across tasks. Ultimately, we found that expressivity
of emergent languages is a trade-off between the complexity and unpredictability of the context for interpreting the
messages sent from speaking agents to acting agents. As a result, we established that we need to increase the complexity and variation of samples in batches during training to achieve communication protocols that are universally
helpful across different tasks.
3.2. Ad Hoc Teamwork
Real-world multi-agent applications may include robotic agents from different manufacturers or robots with heterogeneous physical characteristics and abilities. However, jointly training such groups of agents may not be feasible
or possible for various reasons. Therefore, we have focused on the problem known as ad hoc teamwork, wherein
agents need to be capable of cooperating on the fly without prior coordination. The ad hoc teamwork challenge is
rooted in real-world robotics applications and was described in the seminal paper by Stone et al. [28]. Since the
introduction of the challenge, a substantial number of publications have addressed different aspects of the ad hoc
teamwork problem [29, 30].
In many multi-agent applications, an autonomous agent must be able to cooperate on the fly with diverse types of
other agents that may dynamically enter and leave the environment. This problem is known as open ad hoc teamwork
and is a significant problem faced in the ad hoc teamwork setting. To address team openness, we introduced Graphbased Policy Learning (GPL) [9]. Our method leverages graph neural networks (GNNs) to train a policy that is robust
to dynamic team composition and team sizes. GPL learns a joint action value model based on coordination graphs,
allowing the learning agent to discern the effects of each teammate’s action on the overall return. Furthermore, we
enhanced the joint action value computation with an action prediction module that learns to predict teammates’
actions. By modelling the uncertainty in teammates’ actions selection, we improved the learner’s decision-making
capabilities. Standard RL algorithms can then use the learned action values to obtain optimal policies in the ad
hoc teamwork setting. We tested GPL in several multi-agent environments and compared it against a number of
baselines, showing that our method can generalise better in open team settings and under previously unseen team
compositions.
An important ability to solve ad hoc teamwork problems is agent modelling, wherein an agent reasons about the
behaviour of other agents. However, agent modelling under the assumption of partial observability is an open problem [11]. To address this issue, we proposed Local Information Agent Modelling (LIAM) [10]. LIAM is an encoder-

4

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

decoder model that learns representations that connect the trajectory of the controlled agent with the trajectory of
the modelled agent. LIAM is trained in a centralised fashion by using the trajectories of all agents in the environment to learn representations of different agent policies. However, during execution, the controlled agent operates
in a decentralised manner by generating representations of the modelled agent using only the local trajectory of the
controlled agent. We evaluated LIAM in several multi-agent environments, and demonstrated that LIAM achieves
robust agent modelling under significant partial observability.
Our work in ad hoc teamwork extends beyond the development of novel algorithms. Recently, we organised a
workshop on ad hoc teamwork2 held at the International Joint Conference on Artificial Intelligence (IJCAI) 2022, in
which we discussed recent advances in the field. Additionally, we conducted a survey on ad hoc teamwork [7] with
three clear objectives in mind. First, we provided a clear definition of the scope of the ad hoc teamwork problem by
outlining the key assumptions and subtasks required to address the problem in full. Second, we provided a concise
description of current ad hoc teamwork literature regarding the types of solutions used and the evaluation domains
of these solutions. Third, we discussed current open problems in the ad hoc teamwork literature.
3.3. Single-Agent Reinforcement Learning
While research presented so far addresses the problem of coordination in systems with multiple agents, developing
a single agent that can perform complex tasks autonomously in real-world scenarios remains still largely unresolved.
In such scenarios, we require agents that are able to learn policies with minimal interactions with the environment,
while at the same time achieving high generalisation capabilities. To achieve this goal, we investigate several topics
such as intrinsically-motivated exploration, policy evaluation, and casual models. In the following paragraphs, we
highlight our published work in single-agent RL, and briefly explain our contributions and the results found.
Intrinsic rewards are an effective method for encouraging RL agents to explore [31, 32]. However, the exploration
process may suffer from instability caused by non-stationary reward shaping and sensitivity to hyperparameters. To
address the instabilities arising from these challenges, we proposed Decoupled RL (DeRL) [16], a general framework that trains separate policies for intrinsically-motivated exploration and exploitation. DeRL trains an exploration policy on the combined intrinsic and extrinsic rewards; the exploration policy is used to collect data for the
exploitation policy, which is trained only on extrinsic rewards. DeRL’s training regime decouples the exploitation
policy from the instability introduced by intrinsic rewards while making use of the exploration of these intrinsic
rewards. We evaluated DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards.
Our results showed that intrinsically-motivated RL suffers from instability while DeRL is more robust to varying
scales and rate of decay of intrinsic rewards. We also found that DeRL converges to the same evaluation returns in
fewer interactions as intrinsically-motivated baselines in several tasks. Lastly, we discussed the challenge of distribution shift introduced by diverging exploration and exploitation policies throughout training. Such divergence can
introduce instability in itself, and we showed that divergence constraint regularisers could successfully minimise
such instability caused by the divergence of both trained policies.
Policy evaluation deals with estimating the expected returns of a given policy in an environment of interest.
Many works have studied how to efficiently use a static set of previously collected data for policy evaluation [33,
34]. However, comparatively less research has considered how to improve data collection for data-efficient policy
evaluation. One widely-used method is to use the evaluation policy to collect a set of i.i.d. trajectories and then use
the Monte Carlo estimator that computes the average return as its estimate. Given an infinite number of trajectories,
the likelihood of each trajectory in the collected set will converge to its true probability under the evaluation policy
with Monte Carlo return estimates converging to the true returns under the evaluation policy. However, given a finite
number of samples, such i.i.d. data collection may yield high variance estimates due to sampling error caused by
observing trajectories in the data at a different proportion than their true likelihood under the evaluation policy.
Our work showed how non-i.i.d. sampling can lower sampling error and increase the accuracy of the Monte Carlo
estimator on a (finite) collected set of trajectories [35]. We introduced two non-i.i.d. data collection methods for
policy evaluation, both of which consider previously collected data when collecting future data to reduce sampling
error in the entire data set without using off-policy corrections. Both data collection methods follow the simple idea
2 https://sites.google.com/view/ad-hoc-teamwork

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

5

that we should more frequently sample trajectories that are underrepresented in our data with respect to their true
likelihood under the evaluation policy. Equally, trajectories that occur at higher proportions in the data than their true
likelihood under the evaluation policy should be sampled less frequently in further data collection. Our empirical
results showed that our methods produce data with lower sampling error for finite-sized data sets and lead to lower
mean-squared error in policy evaluation for any data set size compared to i.i.d. sampling.
3.4. Autonomous Driving
We have partnered with UK-based company Five AI to develop planning and prediction methods for autonomous
driving, with a particular focus on goal recognition, interpretability, and verifiability.
We developed an integrated goal recognition and motion planning system, called Interpretable Goal-based Prediction and Planning (IGP2) [17]. IGP2 infers posterior probabilities of possible goals and trajectories of other vehicles using rational inverse planning. The goal and trajectory predictions inform a motion planner based on Monte
Carlo Tree Search (MCTS), which repeatedly forward-simulates the present states to determine the best sequence
of actions for the ego vehicle. By modelling the actions of all vehicles using high-level manoeuvres and macro actions, IGP2 generates manoeuvre plans over extended horizons for which we can extract intuitive explanations based
on rationality principles. We tested IGP2 in diverse simulated urban driving scenarios, demonstrating its ability to
robustly discover goals and driving intentions of nearby vehicles and exploit this information to generate efficient
driving plans for the ego vehicle.
We extended IGP2 by adding the ability to infer the presence of occluded objects. Our approach, Goal and
Occluded Factor Inference (GOFI) [18], jointly models the probability of occluded objects and the goals of other
vehicles. If an observed vehicle’s behaviour seems rational given the presence of an occluded object but seems
irrational with no occluded object present, then GOFI uses this information to increase its belief that an occluded
object is present. Like IGP2, GOFI uses the inferred goal and occluded object probabilities to inform an MCTS
planning procedure. In a range of simulated driving scenarios, we showed that GOFI is able to reduce the number
of collisions with occluded objects or other vehicles.
Finally, we developed a goal recognition method for autonomous vehicles called Goal Recognition with Interpretable Trees (GRIT) [19]. GRIT can infer the goals of vehicles by constructing decision trees from vehicletrajectory data. For goal recognition methods to be useful and safe in real-world settings, they must be fast, accurate,
interpretable, and verifiable. Unlike previous methods, GRIT is the first method which satisfies all four of these
objectives. We evaluated GRIT across four urban driving scenarios from two vehicle-trajectory datasets. Our experiments found that GRIT achieved similar accuracy to deep learning based methods and higher accuracy than several
other baselines. We also showed that GRIT produces trees which are human-interpretable and that can be formally
verified by mapping them into propositional logic and applying satisfiability modulo theories (SMT) solvers.
3.5. Secure Authentication and Key Agreement
Secure authentication and key agreement (AKE) protocols are the foundation for communication over computer
networks. Existing protocols, whether symmetric or asymmetric in design, rely on number-theoretic problems for
their security and are vulnerable to recent advances in quantum computing. As computing power increases at a fast
pace, cryptographic security based on computational hardness cannot be persistently guaranteed from a mathematical perspective. To counteract this issue, we introduced a symmetric AKE protocol called Authentication via MultiAgent Interaction (AMI) [23]. AMI is aligned with information-theoretic security, which does not rely on computational hardness and is quantum-safe. AMI is a novel formulation of symmetric AKE as a multi-agent system,
where communicating parties are treated as autonomous agents whose behaviour within the protocol is governed by
private agent models used as the long-term master keys. AMI’s multi-agent interaction process produces interaction
transcripts used for authentication, generation of session keys, and a key-evolving scheme for forward secrecy.
In [23], we provided an authentication test based on a statistical hypothesis testing algorithm [36], which we
showed to be highly accurate in not only recognising legitimate parties, but also in detecting different adversarial
strategies utilising data observed from prior protocol sessions. These include a random attack, a replay attack, and
a key recovery attack (model reconstruction via maximum likelihood estimate). Outside of [23], we also developed

6

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

authentication tests based on a neural network classifier, trained on interaction transcripts in two different ways - via
supervised learning, and within a generative adversarial network (GAN) [24]. In particular, the use of a GAN allows
for generative modelling of a legitimate system user, as a unique adversarial training mechanism against imitation
attacks. The benefit of classifier-based authentication is that the master key is not required at run-time, which is a
significant departure from existing symmetric AKE protocols.
AMI can also be extended beyond a simple client-server setting for group authentication, wherein a cluster of
more than two agents in a larger multi-agent system establish secure communication for either a centralised system
(trusted third-party is present) or decentralised system. We released the PyAMI open-source framework supporting
different modes of one-way, mutual, and group authentication, to practically demonstrate the AMI protocol on a live
network. PyAMI consists of a multi-agent system where agents run on geographically distant remote machines and
communicate over network sockets using TCP. During an interaction process, server and client machines transmit
actions over the network to build a shared interaction transcript. After successful authentication, the remote parties
compute an identical session key using the AMI key agreement algorithm.
Finally, we demonstrated how agent models can be optimised to achieve desirable behaviour within the protocol.
More specifically, we used the PPO reinforcement learning algorithm [37] to train the server’s behavioural model
(as a neural network) for sample-efficient authentication, wherein the server intelligently probes the opposing party
with fewer but higher-quality queries. The training objective was to produce an interaction transcript between server
and adversarial client which resulted in a sufficiently low p-value in the statistical hypothesis test, from the fewest
number of time steps possible. We showed that the number of required samples to reject an adversary can thus be
decreased by up to 70%. An additional benefit of having fewer interactions is that less data from a private master
key is publicly observable, which can strengthen a protocol’s information-theoretic security.

4. Code Repository
Our group maintains a substantial collection of open-source code repositories3 . These repositories provide implementations of all of our published algorithms and are actively maintained. In this section, we provide a brief
description of some of our main repositories.
One of our most prominent open-source repositories is EPyMARL4 [2]. EPyMARL is based on PyMARL [25],
a MARL framework that includes the implementations of basic MARL algorithms. EPyMARL extends PyMARL
by including additional algorithms, such as IA2C [38], IPPO [37], MADDPG [39], and MAPPO [40]. Our implementation focuses on consistency, which allows for fair comparisons between algorithms and provides additional
implementation options that give users flexibility. For example, one of these options enables the user to select between sharing and no-sharing of parameters between agents. Furthermore, EPyMARL includes options to choose
implementation details such as hard or soft target-network updates, entropy regularisation, reward standardisation,
and fully-connected or recurrent networks. In contrast to PyMARL, which is limited to training algorithms in the
Starcraft Multi-Agent Challenge (SMAC) [25], EPyMARL provides general support for multi-agent environments
following the standard Gym API.
The open-source implementation of our integrated goal recognition and motion planning system IGP2 [17] for
autonomous vehicles is available in the repository called IGP25 . The repository provides instructions on how to
install and run IGP2 and is accompanied by a blog post that further explains our algorithm with videos and images.
The IGP2 goal recognition module can be run in two different ways: i) using state-of-the-art datasets, such as
inD [41] and roundD [42] or ii) using the CARLA simulator [43]. Additionally, we include a visualisation tool that
can be used in conjunction with the datasets to provide information in real-time.
We provide an implementation of PyAMI [23] code and documentation6 . PyAMI is a Python implementation of
AMI, the authentication and key generation protocol developed in [23]. It runs the AMI protocol in a multi-agent
system consisting of multiple remote machines communicating over network sockets via TCP (it may also be run
3 https://github.com/uoe-agents
4 https://github.com/uoe-agents/epymarl
5 https://github.com/uoe-agents/IGP2
6 https://github.com/uoe-agents/PyAMI

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

7

entirely on the local machine in simulation mode for ease of use). Instructions are provided for the local directory
structure, as well as the JSON file format for agent parameters on each remote machine. Run commands are also
provided for supported deployments, as either a centralised or decentralised multi-agent system design.
Additionally, our repository hosts three of our custom multi-agent environments. First, Level-Based Foraging
(LBF)7 [44] is a set of mixed cooperative-competitive tasks that require coordination. Agents in LBF move around
a grid-world map and collect items. Both items and agents are assigned a level such that agents can only pick up
an item if the sum of the levels of the agents involved is equal to or greater than the item level. The second environment is the Multi-Robot Warehouse (RWARE)8 , which simulates a warehouse with robots moving and delivering requested goods. RWARE is challenging due to sparse rewards, collision dynamics, and multi-step tasks. The
repository includes variants of the environment that allow the user to change the size of the map, the number of
agents, and the difficulty level. The third environment is PressurePlate9 . In this environment, a group of agents must
cooperate in traveling from one end of a gridworld to the other. Passage between rooms is blocked by doors that
only open when an agent remains standing on a plate on the ground. PressurePlate is difficult because plates only
respond to specific agents, and completing the task requires some agents to stay behind while the remainder of the
group progresses through the environment. These environments are built in Python and use the standard OpenAI
Gym API format. For further details, see our blog post on MARL environments10 .

5. Open Problems
In the following subsections, we outline current open problems in the literature as prioritised by our own research
group. We believe finding solutions to these problems will contribute towards achieving autonomous agents that can
be robustly deployed in practical applications.
5.1. Generalisation in Reinforcement Learning
A key factor preventing the broader adoption of RL algorithms is its comparatively limited ability to generalise
to even small variations from its training conditions. In comparison, generalisation in supervised learning settings is
well understood [45]. Despite recent efforts to characterise and define generalisation in the context of RL [46–48],
a more precise formalisation is still required. Indeed, generalisation is a complex problem within RL and includes
many different challenges. Each generalisation problem is primarily determined by the similarities and differences
across tasks that agents are trained and evaluated within. Without any assumptions, training and evaluation tasks
could be arbitrarily different and hence no generalisation could be feasibly expected. Fruitful research in RL generalisation requires that we enforce consistency in task aspects such as state and action spaces, transition dynamics, or
reward functions. Moving from single-agent to multi-agent systems further increases the complexity and number of
factors impacting the challenge of generalisation [6]. In multi-agent systems, tasks might include a varying number
of agents, and agents also need to generalise to other agents with which they will cooperate or compete. In particular,
the latter challenge directly relates to ad hoc teamwork discussed in Section 3.2.
One commonly employed approach to address generalisation in RL is to train the agent over multiple tasks that are
randomly sampled from a distribution [49]. The definition of this distribution characterises the type of generalisation
being targeted. However, this process is not sample efficient. Indeed, randomly sampled tasks may be too easy or
too hard, given the agent’s current stage of training. On the other hand, if the tasks are not sampled but randomly
generated, they may be irrelevant to the target task distribution or even be impossible to solve.
For generalisation in the supervised learning setting, the training data distribution can be as important as the choice
of the learning algorithm. Some recent research has also shown that this is the case in the RL setting. Curating training tasks causes agents to require less training data and reach higher asymptotic performance than if they had been
trained on randomly sampled tasks [50, 51]. This exciting new research direction leads to several open problems.
7 https://github.com/uoe-agents/lb-foraging
8 https://github.com/uoe-agents/robotic-warehouse
9 https://github.com/uoe-agents/pressureplate
10 https://agents.inf.ed.ac.uk/blog/multiagent-learning-environments/

8

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

One problem is measuring an agent’s generalisation ability over different regions of the target task distribution. This
ability would allow us to intelligently sample portions of the target task distribution for the agent’s future training.
Furthermore, it remains unclear how to best sample these “weak point" regions of the target task distribution such
that the agent receives a meaningful learning signal.
Lastly, a worthwhile research direction is to investigate formulations of objective functions directly adapted to
the generalisation objective. For example, we could choose an objective that ensures the agent does not fail at any
tasks in the target distribution instead of simply maximising expected returns across the target task distribution.
It may not be possible to define this objective as a single real-valued function to maximise. Instead, it has been
proposed to formulate the generalisation objective as optimising a performance distribution over the task space by
Pareto optimality [52] or as a game played between the agent and a task generator [53, 54]. However, the space of
possible generalisation objectives remains largely unexplored, and some objectives may be more adapted than others
to achieve specific types of generalisation.
5.2. Causal Reinforcement Learning
While most approaches proposed to address generalisation are centred around model-free RL, model-based RL
methods have also shown success in solving many tasks [55]. One way to leverage the benefits of learning such
models in a structured way is to learn causal models of the environments. However, it is difficult to provide an
RL agent with an accurate causal model in real-world scenarios, so the agent must learn the model through its
interactions. Multi-agent systems provide a practical setting to learn causal models because the agents can access
observational data from observing other agents and interventional data from performing their own actions. Together,
these can provide the required dataset for causal structure learning. Furthermore, access to the causal model for
decision-making is beneficial in multi-agent systems because it allows predictions to be adjusted for unobserved
confounders to handle the unobserved factors affecting the decisions of other agents.
In addition to learning the causal model directly, causality techniques could be used to improve the representations
learned by RL agents in image-based environments. Disentangled representation learning aims to separate distinct,
informative factors of variation in an image to identify the ground-truth factors that generated the image. Access
to a disentangled representation could improve policy learning and generalisation to unseen tasks [15]. However, it
remains an open problem to learn such a representation in an RL setting.
5.3. Open Challenges in Ad Hoc Teamwork
Although our group has made progress towards solving the full ad hoc teamwork challenge, particularly in the
case of open team settings, most of the work in the literature addresses the problem under different assumptions that
may not hold in real-world applications. One of such assumptions is that the learner has full observability of the
environment, which is not a realistic assumption for most robotics cases. In [56], we discussed how ad hoc teamwork
agents can aid in marine applications, particularly in search and rescue operations. We evaluated current challenges
that need to be addressed in order to achieve such a system, and we discussed future research directions. We argued
that developing ad hoc agents that are able to communicate is of critical importance [57], but that these agents must
be able to do so under realistic restrictions imposed by the available communication channels.
Another aspect of the ad hoc challenge that is not usually addressed is cases in which the learner needs to work
with teammates that are able to learn or adapt their behaviour over time. Changes in the policies or in the behaviour
of teammates can create uncertainties that can make learning difficult for the ad hoc agent. This in turn highlights
another challenge faced by ad hoc teamwork methods, that of teammate generation. During the learning stages, to
achieve general policies that can be effective against a large number of teammates, the ad hoc agent needs to encounter and interact with teammates with different policies. Generally, teammates’ policies are developed manually;
however, as the tasks to solve become more and more complex, generating teammates becomes a more arduous task
[58]. Currently, we are exploring different alternatives for the automatic generation of teammates that might help
the learner to generalise more robustly to a wide set of possible teammates [59].
Recently, at the 2022 IJCAI Workshop on ad hoc teamwork, we proposed a novel problem related to ad hoc
teamwork called few-shot teamwork [60]. This is a special case in which teams of agents, which have been trained

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

9

independently to each solve a different task, are then combined together into a new team that has to learn and adapt
to solve an unseen but related task. The capability to adapt to new tasks in the manner could also help to accelerate
MARL by making it possible to decompose a complex task into simpler sub-tasks with smaller teams, and later
combining agents skilled at these sub-tasks and training on the complex task.
5.4. Autonomous Driving
While our goal recognition method for autonomous vehicle goal recognition, GRIT, has shown some advantages
with respect to other methods, there are still some open problems. On main concern is that GRIT assumes full
observability of the scene. However, in autonomous driving, there is often missing information due to occlusions.
Therefore, GRIT requires specialised decision trees to be trained in advance for each possible goal location in fixed
scenarios. In recent and ongoing work we addressed these issues using a new goal recognition method named Goal
Recognition with Interpretable trees under Occlusion (OGRIT) [20] which builds on the GRIT method. OGRIT is
designed to handle occlusions and generalise to unseen scenarios, while still being fast, accurate, interpretable and
verifiable. Another open problem is improving the accuracy of interpretable and verifiable goal recognition methods.
In some scenarios, a deep learning baseline was found to achieve slightly higher accuracy than GRIT [19], so there
is still room to improve the accuracy of methods such as GRIT.
Another important question related to autonomous driving is whether we can generate intuitive dialogue-oriented
explanations automatically in a way that builds transparency and trust in our system while making sure we match the
passengers’ expectations and correctly develop their understanding of our system. IGP2 is particularly well-suited
for building trust in autonomous vehicles, as it was designed from the start with interpretability in mind. Using
this inherent interpretability, we were able to extract intuitive explanations for the actions of autonomous vehicles
powered by IGP2 [17]. Our next step is to build an automatic explanation generation system on top of IGP2 that
can create such intuitive explanations automatically in response to natural language queries by human users. In a
paper which came runner-up for the best paper award at the 2022 IJCAI Workshop on Artificial Intelligence for
Autonomous Driving we proposed an initial prototype system for such an explanation generation system [21].
While these issues relate to autonomous agents driving in urban environments, we are also interested in cases in
which agents need to navigate through outdoor unstructured environments. However, outdoor environments have a
high degree of variation, for example, changes in seasons, weather conditions, and the general position of objects.
Therefore, in order to develop learning agents for navigating these types of environments a large amount of data is
needed. In order to accelerate the development of such types of agents we recently introduced MIDGARD [22], a
simulation environment specifically designed for navigation in cluttered outdoor environments. MIDGARD utilises
the Unreal Engine to achieve photorealistic scenarios. Additionally, it provides different scenes, such as Forests and
Meadows, and supports the procedural generation of objects. We hope that the introduction of this simulator will
help further the research in outdoor navigation.

6. Conclusion
The development of autonomous agents which can interact effectively with other agents to accomplish a given
task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous
Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific
focus on deep reinforcement learning and multi-agent reinforcement learning. The group’s research has been at the
forefront of these areas, including scalable learning of coordinated agent policies and inter-agent communication;
reasoning about the behaviours, goals, and composition of other agents from limited observations; and sampleefficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning.
This article provided an overview of the ongoing research portfolio of the group and discussed open problems for
future research directions.

10

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

Acknowledgements
Research in the Autonomous Agents Research Group has been funded by: UK Research and Innovation (UKRI),
UK Engineering and Physical Sciences Research Council (EPSRC), Alan Turing Institute (ATI), Royal Society,
Royal Academy of Engineering (RAEng), Defense Advanced Research Projects Agency (DARPA), US Office of
Naval Research (ONR), and industry sponsors Google, Five AI, and Dematic/KION.

References
[1] G. Papoudakis, F. Christianos, A. Rahman and S.V. Albrecht, Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning,
arXiv preprint arXiv:1906.04737 (2019).
[2] G. Papoudakis, F. Christianos, L. Schäfer and S.V. Albrecht, Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in
Cooperative Tasks, in: Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS), 2021.
[3] F. Christianos, L. Schäfer and S.V. Albrecht, Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning, in: 34th Conference
on Neural Information Processing Systems (NeurIPS), 2020.
[4] F. Christianos, G. Papoudakis, A. Rahman and S.V. Albrecht, Scaling Multi-Agent Reinforcement Learning with Selective Parameter
Sharing, in: International Conference on Machine Learning (ICML), 2021.
[5] S. Guo, Y. Ren, K. Mathewson, S. Kirby, S.V. Albrecht and K. Smith, Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability, in: International Conference on Learning Representations (ICLR), 2022.
[6] L. Schäfer, F. Christianos, A. Storkey and S.V. Albrecht, Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning, arXiv preprint arXiv:2207.02249 (2022).
[7] R. Mirsky, I. Carlucho, A. Rahman, E. Fosong, W. Macke, M. Sridharan, P. Stone and S.V. Albrecht, A Survey of Ad Hoc Teamwork:
Definitions, Methods, and Open Problems, in: European Conference on Multi-Agent Systems (EUMAS), 2022.
[8] S.V. Albrecht, S. Liemhetcharat and P. Stone, Special Issue on Multiagent Interaction without Prior Coordination: Guest Editorial, Autonomous Agents and Multi-Agent Systems 31 (2017), 765–766. http://dx.doi.org/10.1007/s10458-016-9358-0.
[9] A. Rahman, N. Höpner, F. Christianos and S.V. Albrecht, Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning, in:
International Conference on Machine Learning (ICML), 2021.
[10] G. Papoudakis, F. Christianos and S.V. Albrecht, Agent Modelling under Partial Observability for Deep Reinforcement Learning, in:
Proceedings of the Neural Information Processing Systems (NeurIPS), 2021.
[11] S.V. Albrecht and P. Stone, Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems, Artificial Intelligence 258 (2018), 66–95, DOI: 10.1016/j.artint.2018.01.002.
[12] S.V. Albrecht, P. Stone and M.P. Wellman, Special Issue on Autonomous Agents Modelling Other Agents: Guest Editorial, Artificial
Intelligence 285 (2020). https://doi.org/10.1016/j.artint.2020.103292.
[13] T. McInroe, L. Schäfer and S.V. Albrecht, Learning Temporally-Consistent Representations for Data-Efficient Reinforcement Learning,
arXiv preprint arXiv:2110.04935 (2021).
[14] T. McInroe, L. Schäfer and S.V. Albrecht, Learning Representations for Control with Hierarchical Forward Models, arXiv preprint
arXiv:2206.11396 (2021).
[15] M. Dunion, T. McInroe, K.S. Luck, J. Hanna and S.V. Albrecht, Temporal Disentanglement of Representations for Improved Generalisation
in Reinforcement Learning, arXiv preprint arXiv:2207.05480 (2022).
[16] L. Schäfer, F. Christianos, J.P. Hanna and S.V. Albrecht, Decoupled Reinforcement Learning to Stabilise Intrinsically-Motivated Exploration, in: International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2022.
[17] S.V. Albrecht, C. Brewitt, J. Wilhelm, B. Gyevnar, F. Eiras, M. Dobre and S. Ramamoorthy, Interpretable Goal-based Prediction and
Planning for Autonomous Driving, in: IEEE International Conference on Robotics and Automation (ICRA), 2021.
[18] J.P. Hanna, A. Rahman, E. Fosong, F. Eiras, M. Dobre, J. Redford, S. Ramamoorthy and S.V. Albrecht, Interpretable Goal Recognition
in the Presence of Occluded Factors for Autonomous Vehicles, in: IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), 2021.
[19] C. Brewitt, B. Gyevnar, S. Garcin and S.V. Albrecht, GRIT: Fast, Interpretable, and Verifiable Goal Recognition with Learned Decision
Trees for Autonomous Driving, in: IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.
[20] C. Brewitt, M. Tamborski and S.V. Albrecht, Verifiable Goal Recognition for Autonomous Driving with Occlusions, 2022.
[21] B. Gyevnar, M. Tamborski, C. Wang, C.G. Lucas, S.B. Cohen and S.V. Albrecht, A Human-Centric Method for Generating Causal Explanations in Natural Language for Autonomous Vehicle Motion Planning, in: IJCAI Workshop on Artificial Intelligence for Autonomous
Driving, 2022.
[22] G. Vecchio, S. Palazzo, D.C. Guastella, I. Carlucho, S.V. Albrecht, G. Muscato and C. Spampinato, MIDGARD: A Simulation Platform for
Autonomous Navigation in Unstructured Environments, in: ICRA Workshop on Releasing Robots into the Wild: Simulations, Benchmarks,
and Deployment (ICRA), 2022.
[23] I.H. Ahmed, J.P. Hanna, E. Fosong and S.V. Albrecht, Towards Quantum-Secure Authentication and Key Agreement via Abstract MultiAgent Interaction, in: International Conference on Practical Applications of Agents and Multi-Agent Systems (PAAMS), 2021.

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

11

[24] M. Wiatrak, S.V. Albrecht and A. Nystrom, Stabilizing Generative Adversarial Networks: A Survey, arXiv preprint arXiv:1910.00927
(2019).
[25] M. Samvelyan, T. Rashid, C.S. de Witt, G. Farquhar, N. Nardelli, T.G.J. Rudner, C.-M. Hung, P.H.S. Torr, J. Foerster and S. Whiteson, The
StarCraft Multi-Agent Challenge, CoRR abs/1902.04043 (2019).
[26] A. Lazaridou, A. Peysakhovich and M. Baroni, Multi-Agent Cooperation and the Emergence of (Natural) Language, CoRR abs/1612.07182
(2016). http://arxiv.org/abs/1612.07182.
[27] M. Mul, D. Bouchacourt and E. Bruni, Mastering emergent language: learning to guide in simulated navigation, arXiv preprint
arXiv:1908.05135 (2019).
[28] P. Stone, G.A. Kaminka, S. Kraus and J.S. Rosenschein, Ad Hoc Autonomous Agent Teams: Collaboration without Pre-Coordination, in:
AAAI Conference on Artificial Intelligence, AAAI Press, Atlanta, GA, USA, 2010, pp. 1504–1509.
[29] R. Mirsky, W. Macke, A. Wang, H. Yedidsion and P. Stone, A penny for your thoughts: the value of communication in ad hoc teamwork, in:
Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, 2021, pp. 254–260.
[30] F.S. Melo and A. Sardinha, Ad hoc teamwork by learning teammates’ task, Autonomous Agents and Multi-Agent Systems 30(2) (2016),
175–219.
[31] A.G. Barto, Intrinsic motivation and reinforcement learning, in: Intrinsically motivated learning in natural and artificial systems, Springer,
2013, pp. 17–47.
[32] P.-Y. Oudeyer and F. Kaplan, What is intrinsic motivation? A typology of computational approaches, Frontiers in neurorobotics 1 (2009),
6.
[33] D. Precup, R.S. Sutton and S. Singh, Eligibility traces for off-policy policy evaluation, in: Proceedings of the 17th International Conference
on Machine Learning (ICML), 2000, pp. 759–766.
[34] P.S. Thomas and E. Brunskill, Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning, in: Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.
[35] R. Zhong, J.P. Hanna, L. Schäfer and S.V. Albrecht, Robust On-Policy Data Collection for Data-Efficient Policy Evaluation, in: NeurIPS
Workshop on Offline Reinforcement Learning (OfflineRL), 2021.
[36] S.V. Albrecht and S. Ramamoorthy, Are You Doing What I Think You Are Doing? Criticising Uncertain Agent Models, in: Proceedings of
the 31st Conference on Uncertainty in Artificial Intelligence, 2015, pp. 52–61.
[37] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov, Proximal Policy Optimization Algorithms., CoRR abs/1707.06347 (2017).
[38] T. Chu, J. Wang, L. Codecà and Z. Li, Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control, IEEE Transactions
on Intelligent Transportation Systems 21 (2020), 1086–1095.
[39] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel and I. Mordatch, Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,
in: Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, Curran Associates Inc., Red
Hook, NY, USA, 2017, pp. 6382–6393–. ISBN 9781510860964.
[40] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A.M. Bayen and Y. Wu, The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games,
arXiv preprint arXiv:2103.01955 (2021).
[41] J. Bock, R. Krajewski, T. Moers, S. Runde, L. Vater and L. Eckstein, The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections, in: 2020 IEEE Intelligent Vehicles Symposium (IV), 2020, pp. 1929–1934.
doi:10.1109/IV47402.2020.9304839.
[42] R. Krajewski, T. Moers, J. Bock, L. Vater and L. Eckstein, The rounD Dataset: A Drone Dataset of Road User Trajectories at
Roundabouts in Germany, in: 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), 2020, pp. 1–6.
doi:10.1109/ITSC45102.2020.9294728.
[43] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez and V. Koltun, CARLA: An Open Urban Driving Simulator, in: Proceedings of the 1st
Annual Conference on Robot Learning, 2017, pp. 1–16.
[44] S.V. Albrecht and S. Ramamoorthy, A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent
Systems, in: Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems, St. Paul, Minnesota, USA,
2013.
[45] K. Kawaguchi, L.P. Kaelbling and Y. Bengio, Generalization in deep learning, arXiv preprint arXiv:1710.05468 (2017).
[46] R. Kirk, A. Zhang, E. Grefenstette and T. Rocktäschel, A Survey of Generalisation in Deep Reinforcement Learning, arXiv preprint
arXiv:2111.09794 (2021).
[47] D. Malik, Y. Li and P. Ravikumar, When Is Generalizable Reinforcement Learning Tractable?, in: Advances in Neural Information Processing Systems, 2021.
[48] D. Ghosh, J. Rahme, A. Kumar, A. Zhang, R.P. Adams and S. Levine, Why Generalization in RL is Difficult: Epistemic POMDPs and
Implicit Partial Observability, in: Advances in Neural Information Processing Systems, 2021.
[49] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba and P. Abbeel, Domain randomization for transferring deep neural networks from
simulation to the real world, in: 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE, 2017, pp. 23–30.
[50] M. Jiang, E. Grefenstette and T. Rocktäschel, Prioritized Level Replay, arXiv preprint arXiv:2010.03934 (2021).
[51] J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J.N. Foerster, E. Grefenstette and T. Rocktaschel, Evolving Curricula with RegretBased Environment Design, arXiv preprint arXiv:2203.01302 (2022).
[52] D. Open-Ended Learning Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg, M. Mathieu
et al., Open-ended learning leads to generally capable agents, arXiv preprint arXiv:2107.12808 (2021).
[53] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch and S. Levine, Emergent complexity and zero-shot transfer via unsupervised environment design, NIPS (2020).

12

I.H. Ahmed et al. / Deep Reinforcement Learning for Multi-Agent Interaction

[54] L. Pinto, J. Davidson, R. Sukthankar and A.K. Gupta, Robust Adversarial Reinforcement Learning, in: ICML, 2017.
[55] T.M. Moerland, J. Broekens, A. Plaat and C.M. Jonker, Model-based Reinforcement Learning: A Survey, arXiv preprint arXiv:2006.16712
(2020).
[56] I. Carlucho, A. Rahman, W. Ard, E. Fosong, C. Barbalata and S.V. Albrecht, Cooperative Marine Operations Via Ad Hoc Teams, in: IJCAI
Workshop on Ad Hoc Teamwork, 2022.
[57] W. Macke, R. Mirsky and P. Stone, Expected value of communication for planning in ad hoc teamwork, in: Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 35, 2021, pp. 11290–11298.
[58] M. Jacob, S. Devlin and K. Hofmann, “It’s Unwieldy and It Takes a Lot of Time” — Challenges and Opportunities for Creating Agents
in Commercial Games, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment 16(1) (2020),
88–94.
[59] A. Rahman, E. Fosong, I. Carlucho and S.V. Albrecht, Towards Robust Ad Hoc Teamwork Agents By Creating Diverse Training Teammates, in: IJCAI Workshop on Ad Hoc Teamwork, 2022.
[60] E. Fosong, A. Rahman, I. Carlucho and S.V. Albrecht, Few-Shot Teamwork, in: IJCAI Workshop on Ad Hoc Teamwork, 2022.

