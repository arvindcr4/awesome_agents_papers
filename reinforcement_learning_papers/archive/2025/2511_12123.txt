HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent
Reinforcement Learning
Zejiao Liu1 * , Junqi Tu2 * , Yitian Hong2 * , Luolin Xiong2 , Yaochu Jin3† , Yang Tang2† , Fangfei Li1†
1

arXiv:2511.12123v1 [cs.LG] 15 Nov 2025

2

The School of Mathematics, East China University of Science and Technology, Shanghai, China
The Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of
Science and Technology, Shanghai, China
3
The School of Engineering, Westlake University, Hangzhou, China
{liuzejiao, 23012389, y20200105}@mail.ecust.edu.cn, xiongluolin@gmail.com, jinyaochu@westlake.edu.cn, {yangtang,
lifangfei}@ecust.edu.cn

Abstract
In cooperative Multi-Agent Reinforcement Learning
(MARL), efficient exploration is crucial for optimizing the
performance of joint policy. However, existing methods
often update joint policies via independent agent exploration,
without coordination among agents, which inherently
constrains the expressive capacity and exploration of joint
policies. To address this issue, we propose a conductor-based
joint policy framework that directly enhances the expressive
capacity of joint policies and coordinates exploration.
In addition, we develop a Hierarchical Conductor-based
Policy Optimization (HCPO) algorithm that instructs policy
updates for the conductor and agents in a direction aligned
with performance improvement. A rigorous theoretical
guarantee further establishes the monotonicity of the joint
policy optimization process. By deploying local conductors,
HCPO retains centralized training benefits while eliminating
inter-agent communication during execution. Finally, we
evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and
Multi-agent Particle Environment. The results indicate that
HCPO outperforms competitive MARL baselines regarding
cooperative efficiency and stability.

Introduction
Cooperative Multi-Agent Reinforcement Learning (MARL)
methods have driven significant progress across various
fields, including autonomous driving (Chen et al. 2025),
robot cooperative control (Gu et al. 2023), and smart
grid (Zhang et al. 2022). However, the increasing number
of agents in the environment leads to the exponential growth
of the state space and the joint action space, which brings the
scalability challenge in MARL. A widely adopted solution
to tackle this challenge is the Centralized Training with Decentralized Execution (CTDE) paradigm (Feng et al. 2024;
Na and Moon 2024). It updates agents’ policies with global
information during training, while ensuring that agents make
* These authors contributed equally.
†

Corresponding authors.
Copyright © 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

decisions only based on their own local information during execution. Typical CTDE algorithms such as MADDPG (Lowe et al. 2017), QMIX (Rashid et al. 2020), and
MAPPO (Yu et al. 2022) attract widespread attention for
their enhanced coordination and overall effectiveness.
Under the CTDE paradigm, efficient exploration is important in MARL (Zheng et al. 2021; Xu, Zhang, and
Huang 2023; Zhang et al. 2023b). Since parameter sharing restricts the behavioral diversity among agents, consequently impairing their exploration capabilities and impeding task completion (Li, Pan, and Zhang 2024; Li
and Zhu 2025), researchers have developed heterogeneous
MARL algorithms (Kuba et al. 2022; Li, Pan, and Zhang
2024). Specifically, to tackle the non-stationarity problem arising from simultaneous agent decisions in heterogeneous settings, sequential update algorithms such as
Heterogeneous-Agent Trust Region Policy Optimisation
(HATRPO) (Kuba et al. 2022) and Agent-by-agent Policy
Optimization (A2PO) (Wang et al. 2023) have been proposed. This technique allows subsequent agents to integrate
the action and policy information of previous agents within
each training iteration. However, most of the existing CTDE
algorithms, such as HATRPO and A2PO, presume that joint
policy is expressed as the product of individual policies. This
limits the expressive capacity of joint policy, making it difficult for multi-agent systems to explore the optimal joint
policy during training. Therefore, we propose a hierarchical
joint policy framework to address the limitation in expression and enhance exploration for better performance.
Inspired by soccer training and competition, we propose a
conductor-based method to enhance the ability of joint policy expression and exploration, as shown in Figure 1. A centralized conductor provides instructions to the entire team
by considering the on-field status of both sides. Specifically,
the team can be instructed to adopt offensive instructions
such as high pressing, flank attack, or defensive instructions
such as zonal marking and low block. Players make decisions according to the instruction of the conductor and their
own observations. For example, after the conductor chooses
and broadcasts the “high pressing” instruction to all blue

• Extensive Experimental Validation: Comprehensive
evaluations on MARL benchmarks demonstrate the superior performance of HCPO over strong MARL baselines. The results show improvements in cooperative efficiency, policy stability, and exploration.

Related Work

Figure 1: Visualization of conductor-based instructional impact in multi-agent learning: Blue and red dots represent
players from opposing teams, with the blue team conductor providing strategic instructions.

teammates, each agent produces its own action by synthesizing this global instruction with its local observation. Consequently, some players drop back to consolidate defensive
coverage, while others advance to compress space, jointly
generating a coordinated pressing pattern.
In this work, we develop a heterogeneous sequential
MARL framework, termed Hierarchical Conductor-based
Policy Optimization (HCPO). Firstly, a joint policy model
based on the conductor’s instructions is constructed for
strong expressive capacity, which is inspired by a Gaussian
mixture model. Subsequently, we establish a theoretically
grounded update mechanism with strict monotonic improvement guarantees. By decomposing the joint policy optimization into conductor-level instruction and agent-level execution, we derive dual trust region constraints that ensure robust policy evolution. To enable practical decentralized execution, we further deploy local conductors to all agents and
adapt their policies through the cross-entropy method with
the centralized conductor, eliminating dependency on interagent communication. In addition, we perform extensive
empirical validation across standardized benchmarks, including StarCraft II Multi-Agent Challenge (SMAC), Multiagent MuJoCo (MA-MuJoCo), and Multi-agent Particle
Environment (MPE) environments. The results show that
HCPO outperforms existing competitive MARL algorithms.
The main contributions are summarized as follows.
• Hierarchical Conductor-based Policy Expression: To
enhance policy expressive capacity and guide multiagent exploration, we propose a conductor-based joint
policy framework: π mar (a|s) ≜ EM ∼w(·|s) π(a|s, M ).
• HCPO Algorithm and Monotonic Improvement
Guarantee: With the above conductor-based joint policy expression, we derive a new decomposition approach
that breaks down the joint policy’s KL divergence into
two components: the conductor policies’ KL divergence
and the agent policies’ KL divergence. Then, we present
a policy improvement inequality and design a two-level
policy update mechanism for the conductor and agents.
Finally, we prove that HCPO can ensure a monotonic improvement of the joint policy.

Exploration: In MARL, the exponential expansion of joint
state and action spaces as the number of agents increases
severely challenges their ability to efficiently identify highvalue states and actions (Liu et al. 2021b). To improve the
performance of MARL algorithm, research on the exploration of state and policy spaces is crucial. In terms of state
space exploration, the classical work Multi-Agent Variational Exploration (MAVEN) (Mahajan et al. 2019) guides
multi-agent systems to learn diverse exploration patterns by
maximizing the mutual information between agent trajectories and latent variables. To further improve exploration
efficiency, methods like (Jo et al. 2024) and (Li and Zhu
2025) are proposed. Regarding policy space exploration, the
current research focuses on constructing policy diversity incentive mechanisms (Dou et al. 2024). For example, the paper (Xu, Zhang, and Huang 2023) proposes an exploration
method based on joint policy diversity for sparse-reward
multi-agent tasks. It drives agents to explore new policies
by maximizing the cross-entropy between the current joint
policy and previous joint policies. However, most existing
methods, either explicitly or implicitly, assume that the joint
policy is the product of individual policies (Kuba et al. 2022;
Dou et al. 2024; Jo et al. 2024). This decoupled method limits the expressive capacity of joint policy, and sometimes
hampers agents’ exploration during training. To address this,
we design a conductor-based framework that enhances the
joint policy’s expressive capacity. Specifically, our method
provides instructions (latent variables (Mahajan et al. 2019;
Ibrahim and Fayad 2022)) to guide agents’ exploration in the
policy space during training, enabling them to explore new
and potentially high-value policies that traditional methods
might overlook.
Hierarchical mechanism: In recent years, hierarchical
mechanisms have been increasingly adopted (Vezhnevets
et al. 2017; Ahilan and Dayan 2019; Paolo et al. 2025).
MAVEN (Mahajan et al. 2019) embeds a latent space for
hierarchical control within the CTDE framework, which alleviates the expressive limitations introduced by the monotonicity hypothesis in QMIX (Rashid et al. 2020). Through
MAVEN, agents condition their behavior on the shared latent variable to enhance exploration and mitigate issues related to suboptimal policies. Furthermore, HAVEN (Xu et al.
2023) proposes a QMIX-style policy optimization framework with a dual coordination mechanism between layers
and agents. Skill discovery represents another key direction in hierarchical MARL (He, Shao, and Ji 2020; Zhang
et al. 2023a), enabling agents to autonomously learn diverse teamwork skills without requiring manually designed
rewards (Liu et al. 2022, 2025). For example, hierarchical
learning with skill discovery method (Yang, Borovikov, and
Zha 2020) is a two-level MARL algorithm that uses latent skill variables and intrinsic rewards for unsupervised

skill discovery. The hierarchical multi-agent skill discovery (Yang et al. 2023) further extends the research on skill
discovery by introducing team and individual skills. By
employing the probabilistic graphical model, it formulates
multi-agent skill discovery as an inference problem and
leverages transformer structure to assign skills for coordination. To handle dynamic team composition, COPA (Liu et al.
2021a) proposes a coach-player hierarchy where a centralized coach periodically broadcasts strategies derived from
global information. This design, however, retains the monotonicity constraint of QMIX and relies on communication
during execution. Consequently, existing methods mainly
use the monotonicity hypothesis imposed by QMIX and optimize hierarchical policies through variational inference.
In contrast, our HCPO provides a theoretical guarantee of
monotonic improvement in joint policy performance, without requiring any hypothesis regarding the joint action-value
function. Additionally, our approach combines a conductorbased framework with trust region and sequential update
methods, diverging from the mutual information and variational inference approaches commonly employed.

Background
Cooperative MARL Problem Formulation
In this paper, we consider fully cooperative multi-agent
task as a Decentralized Markov Decision Process (DECMDP) (Bernstein et al. 2002; Kuba et al. 2022; Wang
et al. 2023), which is usually modeled as a tuple G =
⟨N , S, A, P, r, γ⟩. Here, N = {1, 2, · · · , N } is the set of
agents. S and A represent the state space of the environment and the whole joint action space, respectively. Each
QN
i
agent i takes action ai ∈ Ai , and A =
i=1 A . P :
S × A × S → [0, 1] is the transition probability function. r :
S × A → R is the reward function shared by all agents and
γ ∈ [0, 1) is the discount factor. To improve coordinated exploration, we propose a hierarchical conductor-based framework that adapts agent policies with the instructions from
the conductor. Specifically, let M denote the conductor’s instructional decision, sampled from a centralized instruction
preference distribution w(·|s). The multi-agent joint policy
π mar is formulated as the expectation over all possible instructions M : π mar (a|s) ≜ EM ∼w(·|s) π(a|s, M ), where
a = (a1 , a2 , · · · , aN ) ∈ A is the joint action. To facilitate decentralized execution, we equip each agent with an
independent local conductor wi (·|oi ), sharing the same instruction space as the centralized conductor. For clarity, the
term “conductor” is primarily used to denote the centralized
conductor unless otherwise specified. Hereafter, we assume
that the conductor has K discrete instructions available to
choose at each time. For any given instruction M , the corresponding instruction-conditional joint policy is defined as
the product of individual agent policies conditioned on M ,
QN
i.e. π(a|s, M ) = i=1 π i (ai |s, M ). Therefore, under the
conductor-based framework, our goal is to maximize the expected cumulative reward:
"∞
#
X
t
J(π mar ) ≜ Es0:∞
γ r(st , at ) .
(1)
,M0:∞
,a0:∞
ρπ
w
π
mar

t=0

0:∞
In the above equation, we denote “s0:∞ ∼ ρ0:∞
π mar ” as “sρπmar ”,
0:∞
“M0:∞ ∼ w0:∞ ” as “Mw ”, and “a0:∞ ∼ π 0:∞ (M0:∞ )”
as “a0:∞
π ” for the sake of brevity. Hereafter, we use this notation wherever no ambiguity arises. Consequently, the state
value function Vπmar (s) is defined as the expected cumulative
return under the multi-agent joint policy π mar :
∞
X
0:∞
1:∞
[
Vπmar (s) ≜ EM0:∞
γ t rt |s0 = s ].
,aπ ,sρπ
w
mar

(2)

t=0

Similarly,
we
define
the
action
value P function
Qπmar (s, a)
∞
[ t=0 γ t rt |s0 = s, a0 = a].
Es1:∞
,M1:∞
,a1:∞
w
π
ρπ

state≜
The

mar

joint advantage function is written as: Aπmar (s, a) ≜
Qπmar (s, a) − Vπmar (s).

Sequential Policy Update Mechanism
To address non-stationarity issues in MARL, the sequential
update for agents has been widely investigated (Kuba et al.
2022; Wang et al. 2023; Dou et al. 2024; Wan et al. 2025).
HATRPO extends the single-agent Trust Region Policy Optimization (TRPO) to multi-agent domain, utilizing multiagent advantage decomposition to facilitate the sequential
updates of agent policies. In HATRPO, each agent updates
its policy parameters through the following protocol:
h
i
in
in
i1:n−1
in
i1:n−1
θk+1
= arg max
E
A
s,
a
,
a
,
i
n
π
s,a
,a
θ
i
k

θ n

subject to Es∼ρπθ

k

(3)
h

i
DKL πθinin (·|s), πθinin (·|s) ≤ δ.
k

(4)
In (3), the compact notation Es,ai1:n−1 ,ain stands for the full
in
expectation Es∼ρ ,ai1:n−1 ∼πi1:n−1 ,ain ∼π . Here, θk+1
πθ

k

θ

i1:n−1
k+1

i
θ n
k

denotes the policy parameter for agent in in episode k + 1.
DKL (·, ·) is the KL-divergence between two policies, and δ
is a hyperparameter. It is important to note that during the
in
k + 1-th episode, policy θk+1
leverages the updated policies
i
π 1:n−1
from
the
preceding
agents
i1:n−1 . This is the core
i1:n−1
θ k+1

idea behind the sequential update mechanism.

Methods
Value Functions
In the previous section, we presented a conductor-based
framework that enhances learning through instruction guidance. To evaluate this framework, we now need quantitative metrics to assess both the learning process and
policy performance. First, we define the value function
for the conductor’s
M as: Qπmar (M
 Pinstruction
 |s) ≜
∞
t
1:∞
1:∞
Ea0:∞
r
|s
=
s,
M
=
M
. The inγ
t 0
0
,sρπ ,Mw
t=0
π
mar
struction advantage function is written as:
Aπmar (M |s) ≜ Qπmar (M |s) − Vπmar (s).

(5)

Here, Aπmar (M |s) measures the relative benefit of instruction M compared to all other instructions M ′ ∼ w (·|s).

Figure 2: The overall framework of HCPO. (a) Centralized training: A two-level policy update mechanism with a virtual
centralized conductor is proposed, leveraging well-designed advantage functions. (b) Policy update for agents: Here, local
agents’ policies denoted by orange ellipses are optimized through sequential updates, and local conductors’ policies denoted by
blue rectangles are optimized through the cross-entropy method. During this iteration, policies with shaded outlines represent
updated versions, while those without shading indicate unmodified ones. (c) Decentralized execution: HCPO enables agents
to make decisions based only on local information.
By choosing M to maximize Aπmar (M |s) with generalized
advantage estimation (GAE) (Schulman et al. 2018), we optimize the conductor’s policy w, as illustrated in Figure 2(i).
Additionally, we define a joint action advantage function for
agents’ joint actions as follows:
Aπmar (a|s, M ) ≜ Qπmar (s, a) − Qπmar (M |s).
(6)
The advantage function Aπmar (a|s, M ) evaluates the joint
action a over all other possible joint actions a′ ∼ π(·|s, M ).
As illustrated in Figure 2(ii), maximizing this advantage function enables the optimization of the instructionconditional joint policy π(·|s, M ), thereby favoring actions
that yield superior expected returns. Based on the above definitions, we can find that:
Aπmar (s, a) = Aπmar (M |s) + Aπmar (a|s, M ).
(7)
j
After the conductor chooses an instruction M , j ∈
{1, 2, · · · , K}, we
 update each individual agent’s policy π il ail |s, M j in the order determined by the random
perturbation set i1:N = {i1 , i2 , · · · , iN }. In addition, at the
state s, based on the conductor’s any instruction M j , after
the previous agents i1:l take joint action ai1:l , we define the
expected value of ai1:l as:
i1:l
Qπ
(ai1:l |s, M j ) ≜ Ea−i1:l ∼π−i1:l (·|s,M j ),s1:∞ ,M1:∞ ,a1:∞
mar
ρπ mar
w
π
0
0
"∞
#
X
γ t rt |s0 = s, M0 = M j , ai01:l = ai1:l ,
t=0

(8)
where −i1:l is the complement of i1:l .

Lemma 1. For any instruction M j , j ∈ {1, 2, ..., K} chosen by the conductor, the conditional Q-function for agents
i1:l satisfies:


Qiπ1:lmar (ai1:l |s, M j ) = Ea−i1:l ∼π−i1:l (·|s,M j ) Qπmar (s, a) ,
(9)

where a = ai1:l , a−i1:l .
The proof is proposed in Appendix A.1. This lemma provides the basis for the subsequent definitions of advantage
functions, which are crucial for evaluating the relative advantages of specific actions compared to average actions.
Specifically, at the state s, based on the conductor’s any action M j , after the agents i1:l first take joint action ai1:l, and
agents −i1:l take joint action a−i1:l ∼ π −i1:l ·|s, M j , we
define the advantage function for ai1:l as:

Aiπ1:lmar ai1:l |s, M j ≜ Qiπ1:lmar (ai1:l |s, M j ) − Qπmar (M j |s).
(10)

It is noted that when l = 0, we have Aiπ1:lmar ai1:l |s, M j =
0. Besides, at the state s, based on the conductor’s any instruction M j , for any individual agent il , we define the ad′
vantage of its individual action ail over all actions ail ∼
il ′
j
π (·|s, M ):
Aiπl mar (ai1:l−1 , ail |s, M j ) ≜ Qiπ1:lmar (ai1:l |s, M j )
i

i1:l−1
− Qπ1:l−1
|s, M j ). (11)
mar (a

In the subsequent part, we introduce the conductor-based
multi-agent advantage function decomposition lemma. As
shown in Figure 2(ii), the lemma is designed to facilitate the transition from updating the instruction-conditional

joint policy π(a|s,M j ) to updating individual agents’ policies π il ail |s, M j , l ∈ N .
Lemma 2. (Conditional Advantage Decomposition) Consider a cooperative Markov game with a joint policy π mar .
For any state s, any instruction M j , and any subset of agents
i1:n = {i1 , i2 , · · · , in } ⊆ N , the following equation holds
for all states s, joint actions ai1:n , and M j ∼ w:
n
 X

i1:n
j
|s,
M
Aiπ1:n
a
=
Aiπl mar ai1:l−1 , ail |s, M j .
mar
l=1

(12)
The proof is proposed in Appendix A.1.

Quantifying Policy Updates for HCPO
TRPO (Schulman et al. 2015) is a reinforcement learning algorithm that improves learning stability by constraining policy update magnitude. In this section, we explore
a conductor-based mechanism that incorporates TRPO to
measure the difference in expected returns between the new
policy and its predecessor. This analysis serves as the foundation for designing effective policy update algorithms.
Proposition 1. As defined in Equation (1), the relationship
between the expected return of the new policy π̄ mar and the
old policy π mar is expressed as:
"∞
#
X
J(π̄ mar ) = J(π mar ) + Eτ ∼π̄mar
γ t Aπmar (st , at ) ,
t=0

(13)
where τ := (s0 , M0 , a0 , s1 , M1 , a1 , · · · ).
The proof is proposed in Appendix A.1. Similar
to HATRPO, we introduce the approximation function
Lπmar (π̄ mar ), which serves as an alternative objective function for the new policy’s performance function J(π̄ mar ):
Lπmar (π̄ mar ) = J(π mar ) + Es∼ρπmar ,M ∼w̄,a∼π̄ [Aπmar (s, a)] ,
(14)
where the compact notation Es∼ρπmar ,M ∼w̄,a∼π̄ stands for
the full expectation Es∼ρπmar ,M ∼w̄(·|s),a∼π̄(·|s,M ) . Therefore, we can derive the following theorem.
Theorem 1. Under the proposed conductor-based framework, a significant policy improvement inequality holds for
the joint policy π mar :
h
J(π̄ mar ) ⩾ J(π mar ) + Es∼ρπmar EM ∼w̄(·|s) Aπmar (M |s)
−CDmax
KL (w, w) + EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπ mar (a|s, M )
K
X

i
− max C
w M j |s DKL π(·|s, M j ), π(·|s, M j ) ,
j=1

(15)
4γ max

|A

(s,a)|

s,a
π mar
where C =
, Dmax
=
KL (wk , w̄)
(1−γ)2
maxs DKL (wk (·|s) , w̄ (·|s)).
For proof see Appendix A.2. The inequality quantifies the
expected return difference between the new policy π̄ mar and
the existing policy π mar under the conductor-based mechanism. This finding offers precise guidance for the subsequent
policy update process.

Guaranteed Monotonic Joint Policy Optimization
In this section, we formulate the policy update mechanisms
for both the centralized conductor’s policy and individual
agents’ policies. Our theoretical analysis aims to establish
the monotonic improvement guarantee for the conductorbased joint policy π mar through a two-level optimization
framework. This guarantee constitutes a critical theoretical foundation, ensuring progressive performance improvement through successive policy iterations while validating
the conductor’s instructions. The specific two-level policy
update mechanisms in episode k + 1 are as follows:
(i) The conductor’s policy w (·|s) = wk (·|s) is updated
first according to the rule:
"
wk+1 = arg max Es∼ρπmar,k ,M ∼w̄ Aπmar,k (M |s)
w̄

#
− CDmax
KL (wk , w̄)

.

(16)

(ii) For each M j where j ∈ {1, 2, ..., K}, the agents sequentially update their policies in accordance with the order
i1:N , following the update rule:
il
πk+1
(·|s, M j )
h



i1:l−1
= arg max wk+1 M j |s Liπ1:lmar,k π k+1
, π̄ il |s, M j
π̄ il (·|s,M j )



− max Cwk M j |s DKL πkil (·|s, M j ), π̄ il (·|s, M j ) ,
(17)


i1:l−1
where
Liπ1:lmar,k π k+1
, π̄ il |s, M j
≜
h
i

Eai1:l−1 ∼πi1:l−1 ,ail ∼π̄il Aiπl mar,k ai1:l−1 , ail |s, M j . For
k+1

proof see Appendix A.4. Equation (17) presents a policy
update mechanism using wk to regulate the update magnitude between the agents’ new and existing policies. This
approach optimizes the exploration-exploitation trade-off,
helps avoid local optima, and enhances the adaptability
and effectiveness of the policy update process. In HCPO’s
practical implementation, we employ the CTDE framework to mitigate the limitations posed by communication
interference, as shown in Figure 2(c). During training, we
incorporate a virtual centralized conductor w parameterized
by Ψ, and each individual agent is equipped with a local
conductor network wi (parameterized by ψ i ) and an actor
network π i (parameterized by θi ). To maintain the theoretical assumption that agents update their policies using
only local observations and the broadcast instruction M ,
we employ a two-stage training protocol. First, we collect
experience with the centralized conductor and optimize its
policy parameter Ψ. Then, every agent’s actor network θi is
sequentially updated to improve the policy. Second, we fix
Ψ and distill its policy into ψ i via cross-entropy loss (Chen
et al. 2024), enabling fully decentralized execution at
evaluation. We summarize our HCPO as Algorithm 1 of
Appendix B.

Figure 3: Performance comparison on SMAC. With the conductor-based joint policy enhancing learning efficiency, HCPO
reliably outperforms all baselines.

Experiments
In this section, we evaluate HCPO on standard MARL
benchmarks including SMAC (Samvelyan et al. 2019), MAMuJoCo (Li, Pan, and Zhang 2024), and MPE (Lowe
et al. 2017). It is compared against strong baselines: HATRPO (Kuba et al. 2022), HAPPO (Kuba et al. 2022),
A2PO (Wang et al. 2023), HAA2C (Zhong et al. 2024). Detailed experimental setup and results are presented below.

(a)

behind the zealots. Figure 4(b) presents the overall force
division into two groups, with two stalkers providing
support. These coordinated actions highlight the agents’
effective collaboration, driven by the latent instructions of
HCPO, which ultimately leads to victory. Detailed analysis
and the evaluation method are presented in Appendix C.1.

(b)

Figure 4: Effective coordination in SMAC on the 3s5z map:
A visual analysis of agent strategies.

Settings and Performance
SMAC: We evaluate algorithms on five SMAC
maps, including the widespread hierarchical algorithm
MAVEN (Mahajan et al. 2019). Each algorithm is tested
with five different random seeds to ensure the robustness
and reliability. The results demonstrate that our HCPO
achieves outstanding performance in all test maps, as
illustrated in Figure 3, with shadows showing standard
deviation across different runs. Specifically, HCPO is the
first to achieve a 90% winning rate on all maps, enhancing
exploration and learning efficiency. Furthermore, HCPO
exhibits the lowest standard deviation, demonstrating its
high stability. In addition, we visualize the gameplay
scenarios on the 3s5z map in Figure 4. Figure 4(a) shows
the early game strategy where our team uses one stalker
to draw enemy fire, while two other stalkers attack from

Figure 5: Comparative evaluation on MA-MuJoCo.
MA-MuJoCo: We compare HCPO with four advanced
on-policy MARL algorithms using three different random
seeds. As shown in Figure 5, HCPO not only achieves significantly higher final returns but also exhibits a lower standard deviation, indicating superior exploration and stability. Specifically, in HalfCheetah-v2-2×3, HCPO achieves
around 23.42% higher final returns than the next best algorithm HAA2C. In Figure 6, we employ t-SNE (t-Distributed
Stochastic Neighbor Embedding) technique to project the
states explored by HCPO, HATRPO, and A2PO algorithms
during the early stage of training in the Walker2d-v2-6×1

Figure 6: Exploration comparison: t-SNE visualization and entropy analysis in Walker2d-v2-6×1.
task onto a 2-D plane. Through entropy analysis and calculation of the average nearest neighbor distance, we can
observe that HCPO demonstrates superior exploration. Detailed analysis is presented in Appendix C.2.
MPE: The results on MPE under three random seeds are
shown in Figure 7. HCPO exhibits rapid policy improvement
in the early stage (0-2 million steps) of training, indicating that HCPO has a high cooperative efficiency and sufficient exploration. Furthermore, compared with HATRPO
and A2PO, HCPO shows significant stability and robustness. More experiments are illustrated in Appendix C.3.

the variant without any conductor. Furthermore, replacing
the learned instruction preference distribution with a nonlearning conductor that outputs uniformly random instructions leads to inferior performance. These findings validate
the effectiveness of our proposed update mechanisms. We
present detailed analysis in Appendix C.4.

(a)

(b)

(c)

(d)

Figure 7: Performance comparison of HCPO and other
strong MARL algorithms across different MPE tasks.

Ablation Studies
For several key components of HCPO, we conduct ablation
studies with results shown in Figure 8. Figure 8(a) examines the impact of the conductor and varying the number
of instructions (hyperparameter K) on performance. HCPO
with the conductor shows a faster increase in winning rate
and a higher final rate than without any conductor, demonstrating its effectiveness in boosting cooperation efficiency.
The performance is also influenced by K. It is important
to balance performance and resource consumption when selecting K. In Figure 8(b), we examine the hyperparameter
δ1 , which represents the KL-divergence constraint during
the conductor’s policy update. In Figure 8(c), we evaluate
HCPO under four conductor configurations: a centralized
conductor, a random conductor (non-learning baseline), no
conductor and local conductors (the core of our HCPO algorithm). Figure 8(d) presents the final episode returns in
a boxplot format. The results show that HCPO with local
conductors achieves a median return comparable to that of
a centralized conductor, while substantially outperforming

Figure 8: Ablation studies.

Conclusion and Future Work
In this paper, we introduce HCPO, a hierarchical MARL algorithm designed to enhance the expressive capacity of joint
policies and improve exploration. By leveraging specific advantage functions, we propose a two-level policy update
mechanism with monotonic improvement guarantees without a monotonicity hypothesis in QMIX. Based on the crossentropy method, each agent is equipped with a local conductor. This not only improves cooperation but also avoids the
limitations imposed by communication constraints. Through
comprehensive experiments on SMAC, MA-MuJoCo, and
MPE, HCPO demonstrates superior performance compared
to competitive algorithms. The main limitation of this work
is that we design an on-policy algorithm for our hierarchical
framework. In the future, we plan to integrate the conductorbased mechanism of HCPO into off-policy algorithms to improve sample efficiency.

Acknowledgments
This work was supported by National Natural Science
Foundation of China under Grants 62233005, U2441245,
62573198, 62173142, and in part by the Shanghai Institute
for Mathematics and Interdisciplinary Sciences (SIMIS) under Grant SIMIS-ID-2025-SP.

References
Ahilan, S.; and Dayan, P. 2019.
Feudal MultiAgent Hierarchies for Cooperative Reinforcement Learning.
arXiv:1901.08492.
Bernstein, D. S.; Givan, R.; Immerman, N.; and Zilberstein, S. 2002. The Complexity of Decentralized Control
of Markov Decision Processes. Mathematics of Operations
Research, 27(4): 819–840.
Chen, X.; Wang, X.; Zhao, W.; Wang, C.; Cheng, S.; and
Luan, Z. 2025. Hierarchical Deep Reinforcement Learning
based Multi-Agent Game Control for Energy Consumption
and Traffic Efficiency Improving of Autonomous Vehicles.
Energy, 323: 135669.
Chen, Y.; Mao, H.; Mao, J.; Wu, S.; Zhang, T.; Zhang, B.;
Yang, W.; and Chang, H. 2024. PTDE: Personalized Training with Distilled Execution for Multi-Agent Reinforcement Learning. In Proceedings of the 33rd International
Joint Conference on Artificial Intelligence (IJCAI-24), 31–
39. Jeju, South Korea: IJCAI Organization.
Dou, H.; Dang, L.; Luan, Z.; and Chen, B. 2024. Measuring Mutual Policy Divergence for Multi-Agent Sequential
Exploration. In Advances in Neural Information Processing
Systems (NeurIPS 2024), volume 37, 76265–76288.
Feng, P.; Liang, J.; Wang, S.; Yu, X.; Ji, X.; Chen, Y.;
Zhang, K.; Shi, R.; and Wu, W. 2024. Hierarchical
Consensus-Based Multi-Agent Reinforcement Learning for
Multi-Robot Cooperation Tasks. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
642–649. Abu Dhabi, United Arab Emirates: IEEE.
Gu, S.; Kuba, J. G.; Chen, Y.; Du, Y.; Yang, L.; Knoll, A.;
and Yang, Y. 2023. Safe Multi-Agent Reinforcement Learning for Multi-Robot Control. Artificial Intelligence, 319:
103905.
He, S.; Shao, J.; and Ji, X. 2020.
Skill Discovery
of Coordination in Multi-agent Reinforcement Learning.
arXiv:2006.04021.
Ibrahim, M.; and Fayad, A. 2022. Hierarchical Strategies for Cooperative Multi-Agent Reinforcement Learning.
arXiv:2212.07397.
Jo, Y.; Lee, S.; Yeom, J.; and Han, S. 2024. FoX: FormationAware Exploration in Multi-Agent Reinforcement Learning.
In Proceedings of the 38th AAAI Conference on Artificial
Intelligence, volume 38, 12985–12994. Vancouver, Canada:
AAAI Press.
Kuba, J. G.; Chen, R.; Wen, M.; Wen, Y.; Sun, F.; Wang,
J.; and Yang, Y. 2022. Trust Region Policy Optimisation in
Multi-Agent Reinforcement Learning. In Proceedings of the
10th International Conference on Learning Representations
(ICLR 2022), 1046. Virtual Only.

Li, T.; and Zhu, K. 2025. Toward Efficient Multi-Agent Exploration with Trajectory Entropy Maximization. In Proceedings of the 13th International Conference on Learning
Representations (ICLR 2025). Singapore.
Li, X.; Pan, L.; and Zhang, J. 2024. Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement
Learning. In Advances in Neural Information Processing
Systems (NeurIPS 2024).
Liu, B.; Liu, Q.; Stone, P.; Garg, A.; Zhu, Y.; and Anandkumar, A. 2021a. Coach-Player Multi-Agent Reinforcement
Learning for Dynamic Team Composition. In Proceedings
of the 38th International Conference on Machine Learning
(ICML 2021), 6860–6870. Virtual Only: PMLR.
Liu, I.-J.; Jain, U.; Yeh, R. A.; and Schwing, A. 2021b. Cooperative Exploration for Multi-Agent Deep Reinforcement
Learning. In Proceedings of the 38th International Conference on Machine Learning (ICML 2021), 6826–6836. Virtual Only: PMLR.
Liu, S.; Shu, Y.; Guo, C.; and Yang, B. 2025. Learning Generalizable Skills from Offline Multi-Task Data for MultiAgent Cooperation. In Proceedings of the 13th International
Conference on Learning Representations (ICLR 2025). Singapore.
Liu, Y.; Li, Y.; Xu, X.; Dou, Y.; and Liu, D. 2022. Heterogeneous Skill Learning for Multi-Agent Tasks. In Advances
in Neural Information Processing Systems (NeurIPS 2022),
volume 35, 37011–37023.
Lowe, R.; Tamar, A.; Harb, J.; Pieter Abbeel, O.; and
Mordatch, I. 2017. Multi-Agent Actor-Critic for Mixed
Cooperative-Competitive Environments. In Advances in
Neural Information Processing Systems (NeurIPS 2017),
volume 30, 6379–6390.
Mahajan, A.; Rashid, T.; Samvelyan, M.; and Whiteson, S.
2019. MAVEN: Multi-Agent Variational Exploration. In Advances in Neural Information Processing Systems (NeurIPS
2019), volume 32.
Na, H.; and Moon, I.-C. 2024. LAGMA: Latent GoalGuided Multi-Agent Reinforcement Learning. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), 37122–37140. Vienna, Austria: PMLR.
Paolo, G.; Benechehab, A.; Cherkaoui, H.; Thomas, A.;
and Kégl, B. 2025.
TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning.
arXiv:2502.15425.
Rashid, T.; Samvelyan, M.; De Witt, C. S.; Farquhar, G.; Foerster, J.; and Whiteson, S. 2020. Monotonic Value Function
Factorisation for Deep Multi-Agent Reinforcement Learning. Journal of Machine Learning Research, 21(178): 1–51.
Samvelyan, M.; Rashid, T.; de Witt, C. S.; Farquhar, G.;
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Foerster, J.; and Whiteson, S. 2019. The StarCraft Multi-Agent
Challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS’19), 2186–2188. Montreal, Canada: IFAAMAS.
Schulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz,
P. 2015. Trust Region Policy Optimization. In Proceedings

of the 32nd International Conference on Machine Learning
(ICML 2015), 1889–1897. Lille, France: PMLR.
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel,
P. 2018. High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv:1506.02438.
Vezhnevets, A. S.; Osindero, S.; Schaul, T.; Heess, N.; Jaderberg, M.; Silver, D.; and Kavukcuoglu, K. 2017. Feudal Networks for Hierarchical Reinforcement Learning. In
Proceedings of the 34th International Conference on Machine Learning (ICML 2017), 3540–3549. Sydney, Australia: PMLR.
Wan, X.; Yang, C.; Yang, C.; Song, J.; and Sun, M. 2025.
SrSv: Integrating Sequential Rollouts with Sequential Value
Estimation for Multi-agent Reinforcement Learning. In Proceedings of the 39th AAAI Conference on Artificial Intelligence, volume 39, 23333–23342. Philadelphia, Pennsylvania: AAAI Press.
Wang, X.; Tian, Z.; Wan, Z.; Wen, Y.; Wang, J.; and Zhang,
W. 2023. Order Matters: Agent-by-agent Policy Optimization. In Proceedings of the 11th International Conference
on Learning Representations (ICLR 2023). Kigali, Rwanda.
Xu, P.; Zhang, J.; and Huang, K. 2023. Exploration via Joint
Policy Diversity for Sparse-Reward Multi-Agent Tasks. In
Proceedings of the 32nd International Joint Conference on
Artificial Intelligence (IJCAI-23), 326–334. Macao, China:
IJCAI Organization.
Xu, Z.; Bai, Y.; Zhang, B.; Li, D.; and Fan, G. 2023.
HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism. In Proceedings of the 37th AAAI Conference on Artificial Intelligence, volume 37, 11735–11743. Washington, USA: AAAI
Press.
Yang, J.; Borovikov, I.; and Zha, H. 2020. Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill
Discovery. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS’20). Virtual Only: IFAAMAS.
Yang, M.; Yang, Y.; Lu, Z.; Zhou, W.; and Li, H. 2023. Hierarchical Multi-Agent Skill Discovery. In Advances in Neural Information Processing Systems (NeurIPS 2023), volume 36, 61759–61776.
Yu, C.; Velu, A.; Vinitsky, E.; Gao, J.; Wang, Y.; Bayen, A.;
and Wu, Y. 2022. The Surprising Effectiveness of PPO in
Cooperative Multi-Agent Games. In Advances in Neural Information Processing Systems (NeurIPS 2022), volume 35,
24611–24624.
Zhang, F.; Jia, C.; Li, Y.-C.; Yuan, L.; Yu, Y.; and Zhang, Z.
2023a. Discovering Generalizable Multi-Agent Coordination Skills from Multi-Task Offline Data. In Proceedings of
the 11th International Conference on Learning Representations (ICLR 2023). Kigali, Rwanda.
Zhang, S.; Cao, J.; Yuan, L.; Yu, Y.; and Zhan, D.-C. 2023b.
Self-Motivated Multi-Agent Exploration. In Proceedings of
the 22nd International Conference on Autonomous Agents
and MultiAgent Systems (AAMAS’23), 476–484. London,
United Kingdom: IFAAMAS.

Zhang, Y.; Yang, Q.; An, D.; Li, D.; and Wu, Z. 2022. Multistep Multiagent Reinforcement Learning for Optimal Energy Schedule Strategy of Charging Stations in Smart Grid.
IEEE Transactions on Cybernetics, 53(7): 4292–4305.
Zheng, L.; Chen, J.; Wang, J.; He, J.; Hu, Y.; Chen, Y.;
Fan, C.; Gao, Y.; and Zhang, C. 2021. Episodic MultiAgent Reinforcement Learning with Curiosity-Driven Exploration. In Advances in Neural Information Processing
Systems (NeurIPS 2021), volume 34, 3757–3769.
Zhong, Y.; Kuba, J. G.; Feng, X.; Hu, S.; Ji, J.; and Yang, Y.
2024. Heterogeneous-Agent Reinforcement Learning. Journal of Machine Learning Research, 25(32): 1–67.

Appendix
A. Additional Details for HCPO Framework
A.1 Useful Lemmas
Lemma 1. For any instruction M j , j ∈ {1, 2, ..., K} chosen by the conductor, the conditional Q-function for agents i1:l
satisfies:


Qiπ1:lmar (ai1:l |s, M j ) = Ea−i1:l ∼π−i1:l (·|s,M j ) Qπmar (s, a) ,
(18)

where a = ai1:l , a−i1:l .
Proof.
∞
i
h
X
i1:l
−i1:l
t
j
i1:l
1:∞
1:∞
Qiπ1:lmar (ai1:l |s, M j ) = Ea−i1:l ∼π−i1:l (·|s,M j ) Es1:∞
[
=
a
]
γ
r
|s
=
s,
M
=
M
,
a
,
a
t 0
0
,Mw ,aπ
0
0
ρπ
0

mar

0

t=0

∞
h
i
X
1:l
1:∞
1:∞
= Ea−i1:l ∼π−i1:l (·|s,M j ) Es1:∞
[
γ t rt |s0 = s, ai01:l = ai1:l , a−i
]
,Mw ,aπ
0
ρπ
0

mar

0

t=0



= Ea−i1:l ∼π−i1:l (·|s,M j ) Qπmar (s, a) .

(19)

Lemma 2. (Conditional Advantage Decomposition) Consider a cooperative Markov game with a joint policy π mar . For any
state s, any instruction M j , and any subset of agents i1:n = {i1 , i2 , · · · , in } ⊆ N , the following equation holds for all states
s, joint actions ai1:n , and M j ∼ w:
n
 X

i1:n
j
a
|s,
M
=
Aiπl mar ai1:l−1 , ail |s, M j .
Aiπ1:n
mar

(20)

l=1

Proof. According to the definition of the advantage function as given in (11), we proceed with the right-hand side as follows:
n
n h
i
X
 X
i
i1:l−1
j
Aiπl mar ai1:l−1 , ail |s, M j =
Qiπ1:lmar (ai1:l |s, M j ) − Qπ1:l−1
(a
|s,
M
)
mar
l=1

l=1

= Qiπ1:n
(ai1:n |s, M j ) − Qπmar (M j |s)
mar
(ai1:n |s, M j ).
= Aiπ1:n
mar

(21)

Proposition 1. As defined in (1), the relationship between the expected return of the new policy π̄ mar and the old policy π mar is
expressed as:
"∞
#
X
t
J(π̄ mar ) = J(π mar ) + Eτ ∼π̄mar
γ Aπmar (st , at ) ,
(22)
t=0

where τ := (s0 , M0 , a0 , s1 , M1 , a1 , · · · ).
Proof. We begin by expanding the expectation term involving the advantage function:
"∞
#
"∞
#
X
X
t
t
Eτ ∼π̄mar
γ Aπmar (st , at ) = Eτ ∼π̄mar
γ [Qπmar (st , at ) − Vπmar (st )]
t=0

t=0

= Eτ ∼π̄mar

"∞
X

#
t

γ [rt + γVπmar (st+1 ) − Vπmar (st )]

t=0

"
= Eτ ∼π̄mar −Vπmar (s0 ) +

∞
X

#
t

γ rt .

(23)

t=0

By the definition of (1) and (2), we can obtain that:
J(π mar ) = Es0 ,M0 ,a0 ,s1 ,M1 ,a1 ,···

∞
X
t=0

!
t

γ rt

,

(24)

Vπmar (s0 ) = EM0 ,a0 ,s1 ,M1 ,a1 ,··· [

∞
X

γ t rt |s0 = s],

(25)

t=0

J(π mar ) = Es0 [Vπmar (s0 )] .

(26)

Therefore, (23) can be further simplified into:
#
"
"∞
#
∞
X
X
t
t
γ Aπmar (st , at ) = Eτ ∼π̄mar −Vπmar (s0 ) +
Eτ ∼π̄mar
γ rt
t=0

t=0

= −J(π mar ) + J(π̄ mar ).

A.2 Proof of Theorem 1
Theorem 1. Under the proposed conductor-based framework, a significant policy improvement inequality holds for the joint
policy π mar :
h
J(π̄ mar ) ⩾ J(π mar ) + Es∼ρπmar EM ∼w̄(·|s) Aπmar (M |s) − CDmax
KL (w, w)
+EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ) − max C

K
X

i

w M j |s DKL π(·|s, M j ), π(·|s, M j ) ,

(27)

j=1

where C =

4γ maxs,a |Aπmar (s,a)|
, Dmax
KL (wk , w̄) = maxs DKL (wk (·|s) , w̄ (·|s)).
(1−γ)2

Proof. First, from the alternative objective function (14), we can obtain that:
Lπmar (π̄ mar ) = J(π mar ) + Es∼ρπmar ,M ∼w̄(·|s),a∼π̄(·|s,M ) [Aπmar (s, a)]
= J(π mar ) + Es∼ρπmar ,M ∼w̄(·|s),a∼π̄(·|s,M ) [Aπmar (M |s) + Aπmar (a|s, M )]

= J(π mar ) + Es∼ρπmar [EM ∼w̄(·|s) Aπmar (M |s) + EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ) .

(28)

Therefore, combining Theorem 1 in Kuba et al. (2022), we can derive the following inequality:
J(π̄ mar ) ⩾ Lπmar (π̄ mar ) − CDmax
KL (π mar , π̄ mar )
h
i
= J(π mar ) + Es∼ρπmar EM ∼w̄(·|s) Aπmar (M |s) + EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ) − CDmax
KL (π mar , π̄ mar ),
(29)
4γ max

|A

(s,a)|

s,a
π mar
Kuba et al. (2022). Next, we simplify Dmax
where C =
KL (π mar , π̄ mar ) by expressing it in terms of the
(1−γ)2
instruction preference distribution w (·|s) and the instruction-conditional joint policy π(a|s, M ):

DKL (π mar (·|s), π̄ mar (·|s)) ⩽DKL (w (·|s) , w (·|s)) +

K
X



w M j |s DKL π(·|s, M j ), π(·|s, M j ) ,

(30)

j=1

where the detail derivation of (30) can be found in section A.3. By taking maximum over state s, we have:
max
Dmax
KL (π mar , π̄ mar )⩽ DKL (w, w) + max

K
X



w M j |s DKL π(·|s, M j ), π(·|s, M j ) .

j=1

In summary, (29) can be further decomposed into:
h
J(π̄ mar ) ⩾ J(π mar ) + Es∼ρπmar EM ∼w̄(·|s) Aπmar (M |s) − CDmax
KL (w, w)
+EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ) − max C

K
X
j=1


i
w M j |s DKL π(·|s, M j ), π(·|s, M j ) .

(31)

A.3 Derivation of the Equation (30) for KL-divergence
DKL (π mar (·|s), π̄ mar (·|s)) = Ea∼πmar [log π mar (a|s) − log π̄ mar (a|s)]
X
π mar (a|s)
=
π mar (a|s) log
π̄ mar (a|s)
a

PK
j
j
X
j=1 w M |s π(a|s, M )
=
π mar (a|s) log PK
j
j
a
j=1 w (M |s)π(a|s, M )

PK
K
j
j
XX

j=1 w M |s π(a|s, M )
j
j
=
w M |s π(a|s, M ) log PK
j
j
a j=1
j=1 w (M |s)π(a|s, M )
#
"

K
XX

w M j |s π(a|s, M j )
j
j
(by log sum inequality)
⩽
w M |s π(a|s, M ) log
w (M j |s) π(a|s, M j )
a j=1

K
XX

w M j |s
π(a|s, M j )
j
j
=
w M |s π(a|s, M ) [log
+
log
]
w (M j |s)
π(a|s, M j )
a j=1

K
K
X
XX


w M j |s X
π(a|s, M j )
j
j
j
j
w M |s log
w
M
|s
π(a|s,
M
)
log
=
π(a|s,
M
)
+
w (M j |s) a
π(a|s, M j )
a j=1
j=1
= DKL (w (·|s) , w (·|s)) +

K
X



w M j |s DKL π(·|s, M j ), π(·|s, M j ) .

(32)

j=1

A.4 Proof of Guaranteed Monotonic Joint Policy Optimization for HCPO
Before proof of monotonic improvement guarantee, we simplify certain in inequality (27) to prepare for the subsequent steps.
We begin by dealing with EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ), which is vital for assessing the influence of policy updates on
the system’s overall performance.


EM ∼w̄(·|s),a∼π̄(·|s,M ) Aπmar (a|s, M ) = EM ∼w̄(·|s) Ea∼π̄(·|s,M ) Aπmar (a|s, M )
=

K
X



w̄ M j |s Ea∼π̄(·|s,M j ) Aπmar (a|s, M j ) .

(33)

j=1

To facilitate subsequent representations, define:



Liπ1:lmar π̄ i1:l−1 , π̂ il |s, M ≜ Eai1:l−1 ∼π̄i1:l−1 (·|s,M ),ail ∼π̂il (·|s,M ) Aiπl mar ai1:l−1 , ail |s, M .

(34)

It is easy to obtain that for any π̄ i1:l−1 , there is:



Liπ1:lmar π̄ i1:l−1 , π il |s, M = Eai1:l−1 ∼π̄i1:l−1 (·|s,M ),ail ∼πil (·|s,M ) Aiπl mar ai1:l−1 , ail |s, M



= Eai1:l−1 ∼π̄i1:l−1 (·|s,M ) Eail ∼πil (·|s,M ) Aiπl mar ai1:l−1 , ail |s, M
= 0.

(35)

Consider agents updated in order i1:N , ∀M j , j ∈ {1, 2, ..., K}, we have:
Ea∼π̄(·|s,M j ) Aπmar (a|s, M j ) = Eai1:N ∼π̄i1:N (·|s,M j ) Aiπ1:N
(ai1:N |s, M j )
mar
(by (20)) = Eai1:N ∼π̄i1:N (·|s,M j )

N
X

Aiπl mar ai1:l−1 , ail |s, M j



l=1

(by (34)) =

N
X
l=1


Liπ1:lmar π̄ i1:l−1 , π̄ il |s, M j .

(36)

In addition, for DKL in the last item in (30), there is:




DKL π(·|s, M j ), π(·|s, M j ) = Ea∼π(·|s,M j ) log π(a|s, M j ) − log π̄(a|s, M j )
N
N
h
i
Y
Y
= Ea∼π(·|s,M j ) log ( π il (ail |s, M j ) ) − log ( π̄ il (ail |s, M j ) )
l=1

= Ea∼π(·|s,M j )

"N
X

l=1
il

il

j

logπ (a |s, M ) −

l=1

=

N
X

N
X

#
il

il

j

logπ̄ (a |s, M )

l=1

Eail ∼πil (·|s,M j ),a−il ∼π−il (·|s,M j ) [log π il (ail |s, M j ) − log π̄ il (ail |s, M j ) ]

l=1

=

N
X


DKL π il (·|s, M j ), π̄ il (·|s, M j ) .

(37)

l=1

Then, we can prove the monotonic improvement guarantee for the performance of the joint policy π mar , under the following
two-level policy update mechanisms:

(i) The conductor’s policy w (·|s) = wk (·|s) is updated first according to the rule:

"
wk+1 = arg max
w̄

#
Es∼ρπmar,k ,M ∼w̄ Aπmar,k (M |s) − CDmax
KL (wk , w̄)

.

(38)

(ii) For each M j where j ∈ {1, 2, ..., K}, the agents update their policies sequentially according to the order i1:N , with the
following update rule:

h



i1:l−1
il
πk+1
(·|s, M j ) = arg max wk+1 M j |s Liπ1:lmar,k π k+1
, π̄ il |s, M j
π̄ il (·|s,M j )


i
−max Cwk M j |s DKL πkil (·|s, M j ), π̄ il (·|s, M j ) .
s

(39)

Proof. Starting from (27), combining (38) and (39), we can obtain:


J(π mar,k+1 ) ⩾ J(π mar,k ) + Es∼ρπmar,k EM ∼wk+1 (·|s) Aπmar,k (M |s) − CDmax
KL (wk , wk+1 )
h
+ Es∼ρπmar,k EM ∼wk+1 (·|s),a∼πk+1 (·|s,M ) Aπmar,k (a|s, M )
− max C
s

K
X


i
wk M j |s DKL π k (·|s, M j ), π k+1 (·|s, M j )

j=1



= J(π mar,k ) + Es∼ρπmar,k EM ∼wk+1 (·|s) Aπmar,k (M |s) − CDmax
KL (wk , wk+1 )
+ Es∼ρπmar,k

K
hX



wk+1 M j |s Ea∼πk+1 (·|s,M j ) Aπmar,k (a|s, M j )

j=1

−max C
s

K
X


i
wk M j |s DKL π k (·|s, M j ), π k+1 (·|s, M j ) (combine (33))

j=1



=J(π mar,k ) + Es∼ρπmar,k EM ∼wk+1 (·|s) Aπmar,k (M |s) − CDmax
KL (wk , wk+1 )
+ Es∼ρπmar,k

K
hX

N


X
i1:l−1
il
wk+1 M |s
Liπ1:lmar,k π k+1
, πk+1
|s, M j
j

j=1

l=1

−max C
s

K
X

N
X
i
il
DKL πkil (·|s, M j ), πk+1
wk M |s
(·|s, M j ) (combine (36)(37))

j=1

j

(40)

l=1



⩾ J(π mar,k ) + Es∼ρπmar,k EM ∼wk+1 (·|s) Aπmar,k (M |s) − CDmax
KL (wk , wk+1 )
+ Es∼ρπmar,k

K
N X
hX





i1:l−1
il
wk+1 M j |s Liπ1:lmar,k π k+1
, πk+1
|s, M j

l=1 j=1


i
il
− max Cwk M j |s DKL πkil (·|s, M j ), πk+1
(·|s, M j )
s


⩾ J(π mar,k ) + Es∼ρπmar,k EM ∼wk (·|s) Aπmar,k (M |s) − CDmax
KL (wk , wk )
X
K h
N X



i1:l−1
+ Es∼ρπmar,k
wk+1 M j |s Liπ1:lmar,k π k+1
, πkil |s, M j
l=1 j=1



i
−max Cwk M j |s DKL πkil (·|s, M j ), πkil (·|s, M j )
(combine (38)(39))
s


= J(π mar,k ) + Es∼ρπmar,k EM ∼wk (·|s) Aπmar,k (M |s) − 0
X
N X
K h


i
 i
i1:l−1
il
j
j
1:l
+ Es∼ρπmar,k
wk+1 M |s Lπmar,k π k+1 , πk |s, M − 0 .
l=1 j=1

By the meaning of the advantage function and the equation (35), there is:


i1:l−1
EM ∼wk (·|s) Aπmar,k (M |s) = 0, Liπ1:lmar,k π k+1
, πkil |s, M j = 0.
Therefore, (40) can be reduced to:
J (π mar,k+1 ) ⩾ J(π mar,k ) + 0 = J(π mar,k ).

(41)

B. Algorithms
Algorithm 1: Pseudocode of HCPO
Input: Minibatch size B1 , number of agents N , training times Λ, steps per episode T
1: Initialization: Global V-value network {ϕ0 }, centralized conductor’s actor network {Ψ0 }, local conductors’ actor networks
{ψ0i , ∀i ∈ N }, agents’ actor networks {θ0i , ∀i ∈ N }, replay buffer B1
2: for k = 0, 1, · · · , Λ − 1 do
3:
Collect a set of trajectories by running π mar,k (at |st ) = EMt ∼wΨk (·|st ) π θk (at |st , Mt ) and save transitions
{(st , Mt , at , st+1 , rt ), ∀t ∈ T } into B1
4:
Sample a random minibatch of B1 episodes from replay buffer B1
5:
Compute Aπmar,k (Mb,t |sb,t ) and Aπmar,k (ab,t |sb,t , Mb,t ) based on global V-value network with generalized advantage
estimation (GAE)
6:
Update Ψk+1 = Ψk + αj1 β̂conductor,k x̂conductor,k as designed in Algorithm 2
7:
Draw a random permutation of agents i1:N
8:
Set Θi1 (ab,t |sb,t , Mb,t ) = Aπmar,k (ab,t |sb,t , Mb,t )
9:
for agent il = i1 , · · · , iN do
il
10:
Update ψk+1
by minimizing the cross-entropy between ψkil and Ψk+1 : Licel = −EwΨk+1 [log ψkil ]
il
il
l
11:
Update agent il ’s policy by θk+1
= θkil + αj2 β̂agent,k
x̂iagent,k
as designed in Algorithm 3
12:
Compute


l
l
π ilil
aib,t
| oib,t
, Mb,t
θ
 Θi1:l (ab,t |sb,t , Mb,t )
Θi1:l+1 (ab,t |sb,t , Mb,t ) = k+1
il
il
il
π il ab,t | ob,t , Mb,t
θk

13:
14:

unless l = N
end for
Update V-value network by following formula:
B

ϕk+1 = arg min
ϕ

15:
Clear the replay buffer B1 and minibatch B1
16: end for

T

1 X
2
1 X
Vϕ (st ) − R̂t
B1 T
t=0

b=1

Algorithm 2: Conjugate Gradient Approach for Updating Centralized Conductor’s Policy Parameters
Input: Minibatch size B1 , episode number k, steps per episode T
Output: New centralized conductor’s policy parameter Ψk+1
1: Estimate the gradient of the conductor’s maximization objective
B

ĝconductor,k =

T

1 X
1 X
∇Ψk log wΨk (Mb,t |sb,t )Aπmar,k (Mb,t |sb,t )
B1
t=1

b=1

Use the conjugate gradient algorithm to compute the update direction
−1

x̂conductor,k ≈ Ĥ conductor,k ĝconductor,k
where Ĥ conductor,k is the Hessian of the average KL-divergence
B1 X
T

1 X
DKL wΨk (·|sb,t ), wΨ (·|sb,t )
B1 T
t=1
b=1

2: Estimate the maximal step size allowing for meeting the KL-constraint

s
β̂conductor,k ≈

2δ1
(x̂conductor,k

)⊤ Ĥ

conductor,k (x̂conductor,k )

3: Update centralized conductor’s policy by Ψk+1 = Ψk + αj1 β̂conductor,k x̂conductor,k

Algorithm 3: Conjugate Gradient Approach for Updating Local Agent’s Policy Parameters
Input: Minibatch size B1 , episode number k, steps per episode T , agent index il
il
Output:New Agent’s policy parameter θk+1
1: Estimate the gradient of the agent’s maximization objective
B

il
ĝagent,k
=

T

1 X


1 X
l
l
wΨk+1 (Mb,t |sb,t )∇θil log π ilil aib,t
| oib,t
, Mb,t Θi1:l (ab,t |sb,t , Mb,t )
θk
k
B1
t=1

b=1

Use the conjugate gradient algorithm to compute the update direction
il

il
l
x̂iagent,k
≈ Ĥ agent,k −1 ĝagent,k
il

where Ĥ agent,k is the Hessian of the average KL-divergence


B1 X
T

1 X
il
il
il
il
ψk
Mb,t |sb,t DKL π il (·|ob,t , Mb,t ), πθil (·|ob,t , Mb,t )
w
θk
B1 T
t=1
b=1

2: Estimate the maximal step size allowing for meeting the KL-constraint

v
u
u
il
β̂agent,k
≈t
i

i

i

2δ2
il
il
l
(x̂agent,k )⊤ Ĥ agent,k (x̂iagent,k
)
i

l
l
l
3: Update agent il ’s policy by θk+1
= θkl + αj2 β̂agent,k
x̂agent,k

C. Experimental Details
C.1 StarCraftII Multi-agent Challenge
In the SMAC environment, we basically adhere to the official implements and hyperparameter settings of MAVEN1 Mahajan
et al. (2019), HAPPO and HATRPO2 Kuba et al. (2022), A2PO3 Wang et al. (2023), HAA2C4 Zhong et al. (2024), as shown
in Table 1. We adopt the evaluation method from MAPPO Yu et al. (2022) and compare HCPO against other algorithms on
five maps. After each training iteration, 32 evaluation games are played and the winning rate of these 32 games is calculated.
Finally, we take the median winning rate of the last ten evaluations as the performance metric for each random seed and report
the average median winning rate based on the five random seeds in Table 2. We can observe that HCPO achieves the highest
average median winning rate across five maps, reaching an impressive 97.82%. This result highlights its superior performance
compared to other algorithms. Moreover, HCPO also exhibits the lowest standard deviation, demonstrating its high stability.
hyperparameters

value

hyperparameters

value

hyperparameters

value

critic lr
gamma
batch size
training threads
rollout threads
episode length
use huber loss
accept-ratio

5e-4
0.95
3200
32
16
200
True
0.5

optimizer
gamma in corridor
gain
actor network
hypernet embed
activation
conductor kl-threshold
K

Adam
0.99
0.01
mlp
64
ReLU
0.01
10

stacked-frames
optim eps
hidden layer
num mini-batch
max grad norm
hidden layer dim
kl-threshold

1
1e-5
1
1
10
64
0.06

Table 1: Common hyperparameters in SMAC

Task

Difficulty

HCPO

HAPPO

HATRPO

HAA2C

A2PO

MAVEN

3s5z
5m vs 6m
8m vs 9m
10m vs 11m
corridor

hard
hard
hard
hard
super

100(0.0)
93.8(2.9)
100(0.7)
98.4(1.6)
96.9(1.6)

89.1(2.3)
90.6(5.4)
81.2(6.9)
85.9(8.4)
96.9(0.7)

100(1.4)
70.3(8.7)
90.6(2.4)
93.8(2.9)
90.6(2.3)

67.2(11.8)
0.0(0.0)
18.8(2.7)
20.3(11.1)
85.9(5.7)

98.4(1.3)
92.2(5.9)
93.8(4.5)
96.9(2.2)
93.8(2.4)

0.0(0.0)
43.8(1.4)
18.8(2.9)
18.8(2.6)
40.6(4.1)

Overall

/

97.82(1.36)

88.74(4.74)

89.06(3.54)

38.44(6.26)

95.02(3.26)

24.4(2.2)

Table 2: Average evaluation median winning rate and standard deviation (across five seeds) within SMAC scenarios for distinct
methods
Taking the 3s5z map as an example, after the completion of the training process (10 million steps) for all agents’ local
conductors and local actors, we visualize the gameplay scenarios involving our allies and the enemies in Figure 9. Figure 9a
depicts the initial phase of the game, with both teams launching their attacks. The Figure 9b captures a moment where one of
the stalkers is intentionally drawing the enemy fire, acting as a decoy to distract and absorb the enemy’s attention and attacks.
This instruction allows the other two stalkers to approach from behind the zealots, positioning them for a surprise attack on
the enemy’s flank. The green circles highlight the units under our control, with the health and energy bars visible above each
unit, indicating their current status. The blue line pointing towards the enemy unit signifies the attack direction of our stalker,
demonstrating the coordination between our units. Figure 9c illustrates a strategic (instruction-driven) moment where our forces
are effectively split into two groups. The green arrows represent the movement direction of our stalkers. These two stalkers have
identified allies in the yellow circle on the right with low health and automatically move to provide support. This self-directed
action, triggered by the agent’s local assessment of the situation (observation and instruction), exemplifies the autonomous
decision-making capabilities of our agents under the guidance of HCPO’s latent instructions. These instructions underscore the
significance of actions and strategic positioning in achieving tactical advantage in the game. The experiments are performed on
a server with 3 RTX 4090 GPUs, and 3 Xeon(R) Gold 6430 16-core CPUs, and 128GB Ram.
1

https://github.com/starry-sky6688/MARL-Algorithms/
https://github.com/PKU-MARL/TRPO-PPO-in-MARL
3
https://github.com/xihuai18/A2PO-ICLR2023
4
https://github.com/PKU-MARL/HARL
2

(a)

(b)

(c)

Figure 9: Effective coordination in SMAC on the 3s5z map: A visual analysis of agent strategies.

C.2 Multi-agent MuJoCo
The detailed experimental hyperparameters are listed in Table 3. We present a comparison of the return performance of different
baselines across four tasks in Figure 10. The median return values of HCPO algorithm reach the highest levels, demonstrating
its superior performance in these tasks. Additionally, the return values exhibit a narrow interquartile range, indicating high
stability across multiple experimental runs. Less outliers further imply the robustness of HCPO during training, as it generates
returns without being compromised by occasional suboptimal runs. In summary, these findings suggest that HCPO maintains
efficiency and reliability across a variety of tasks.
In Figure 11, we employ the t-SNE (t-Distributed Stochastic Neighbor Embedding) technique to visualize the states explored
by the HCPO, HATRPO, and A2PO during the early stages of training in the Walker2d-v2-6×1. Each dot in the plots represents
a state explored by the algorithms. We use orange shading to indicate state probability density within each grid cell, defined
as the fraction of samples falling in that cell relative to the total number of samples, with darker colors representing higher
visitation frequencies. By examining the color distribution across the plots, we can visually assess the areas and densities
explored by each algorithm. The figure also provides the average nearest neighbor distance (Avg. Dist.) and entropy values for
each algorithm. A larger average nearest neighbor distance (e.g., HCPO’s 0.72) implies greater separation among visited states,
reflecting better coverage diversity, while entropy measures exploration breadth. In summary, these metrics show that HCPO
exhibits well-balanced exploration characteristics.
hyperparameters

value

hyperparameters

value

hyperparameters

value

critic lr
gamma
gain
std y coef
std x coef
activation
conductor kl-threshold
K

5e-3
0.99
0.01
0.5
1
ReLU
0.01
10

optimizer
optim eps
hidden layer
actor network
max grad norm
hidden layer dim
kl-threshold

Adam
1e-5
1
mlp
10
128
0.005

num mini-batch
batch size
training threads
rollout threads
episode length
eval episode
accept-ratio

1
4000
8
20
200
40
0.5

Table 3: Common hyperparameters in MA-MuJoCo

Figure 10: Comparison of the return performance of different baselines in the MuJoCo task.

C.3 Multi-agent Particle Environment
We present our detailed experimental hyperparameters in Table 4. The results of each algorithm under three random seeds on
MPE benchmark are shown in Figure 12. In general, HCPO demonstrates comparable performance across all three tasks,

Figure 11: Exploration comparison: t-SNE visualization and entropy analysis in Walker2d-v2-6×1.

highlighting its adaptability and effectiveness in multi-agent settings. Specifically, in simple spread v2-discrete and simple speaker listener v3-discrete tasks, HCPO exhibits rapid policy improvement in the early stage of training (0-2 million
steps). This indicates that HCPO algorithm has a high cooperative efficiency and sufficient exploration. Although HCPO shows
slower initial cooperation efficiency in the simple reference v2-discrete task compared to A2PO, it ultimately outperforms
A2PO in final convergence performance, confirming its long-term learning advantage. Furthermore, compared with HATRPO
and A2PO, HCPO algorithm shows significant stability and robustness. In summary, HCPO not only maintains competitive
convergence speed but also matches or exceeds the performance of the strong baselines through its hierarchical mechanism.
hyperparameters

value

hyperparameters

value

hyperparameters

value

actor lr
critic mini batch
network
clip param
conductor kl-threshold
K

5e-4
1
mlp
0.2
0.01
10

critic lr
gamma
linear lr decay
entropy coef
kl-threshold
eval episode

5e-4
0.99
False
0.01
0.005
40

actor mini batch
batch size
critic epoch
backtrack coef
accept-ratio

1
4000
5
0.8
0.5

Table 4: Common hyperparameters in MPE

Figure 12: Performance comparison of HCPO and other strong MARL algorithms across different MPE tasks.

C.4 Ablation Studies
For several key components of HCPO, we conduct ablation studies with results shown in Figure 13. In Figure 13a, we explore
the impact of removing the conductor and varying the number of instructions (hyperparameter K) on performance. HCPO
with the conductor shows a faster increase in winning rate and a higher final rate than without a conductor, demonstrating its
effectiveness in boosting cooperation efficiency. The performance is also influenced by K. Although increasing K enhances
adaptability to the environment, beyond a certain threshold, further increments in K introduce unnecessary complexity without
substantial performance gains. Therefore, it is important to balance performance and resource consumption when selecting K.

In Figure 13b, we examine the hyperparameter δ1 , which represents the KL-divergence constraint of centralized conductor’s
policy. Results show that HCPO is relatively insensitive to δ1 , but the configuration with δ1 = 0.01 demonstrates better performance compared to the other settings. In Figure 13c, we evaluate HCPO under four conductor configurations: a centralized
conductor with global information, a random conductor (non-learning baseline), no conductor and local conductors only based
on local observations (trained via cross-entropy learning, constituting the core of our HCPO algorithm). Figure 13d presents
the final episode returns in a boxplot format. The results show that HCPO with local conductors achieves a median return comparable to that of a centralized conductor, while substantially outperforming the variant without any conductor. Furthermore,
replacing the learned instruction preference distribution with a non-learning conductor that outputs uniformly random instructions leads to inferior performance. These findings collectively validate the effectiveness of our proposed update mechanisms.

(a)

(b)

(c)

(d)

Figure 13: Ablation studies.

