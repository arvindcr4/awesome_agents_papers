JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

1

Multi-Agent Reinforcement Learning for Connected and Automated
Vehicles Control: Recent Advancements and Future Prospects

arXiv:2312.11084v3 [cs.RO] 19 Oct 2024

Min Hua1 , Xinda Qi2 , Dong Chen3 , Kun Jiang4 , Zemin Eitan Liu5 , Quan Zhou1,6 , Hongming Xu1∗

Abstract—Connected and automated vehicles (CAVs) are considered as a potential solution for future transportation challenges, aiming to develop systems that are efficient, safe, and
environmentally friendly. However, CAV control presents significant challenges due to the complexity of interconnectivity and
coordination required among vehicles. Multi-agent reinforcement
learning (MARL), which has shown notable advancements in
addressing complex problems in autonomous driving, robotics,
and human-vehicle interaction, emerges as a promising tool to
enhance CAV capabilities. Despite its potential, there is a notable
absence of current reviews on mainstream MARL algorithms for
CAVs. To fill this gap, this paper offers a comprehensive review
of MARL’s application in CAV control. The paper begins with
an introduction to MARL, explaining its unique advantages in
handling complex and multi-agent scenarios. It then presents
a detailed survey of MARL applications across various control
dimensions for CAVs, including critical scenarios such as platooning control, lane-changing, and unsignalized intersections.
Additionally, the paper reviews prominent simulation platforms
essential for developing and testing MARL algorithms. Lastly, it
examines the current challenges in deploying MARL for CAV
control, including macro-micro optimization, communication,
mixed traffic, and sim-to-real challenges. Potential solutions
discussed include hierarchical MARL, decentralized MARL,
adaptive interactions, and offline MARL.
Note to Practitioners—This paper presents multi-agent reinforcement learning as a solution to the challenges of controlling
connected and automated vehicles in complex scenarios like
platooning, lane changes, and intersections. MARL offers a more
adaptive approach than traditional methods, potentially improving traffic flow, safety, and fuel efficiency in real-world settings
and leading to more efficient and reliable transportation systems.
The paper also reviews current MARL algorithms and simulation
platforms, providing a resource for implementing these advanced
control strategies in practice. However, the deployment of MARL
in real-world CAV systems is still in its early stages and faces
challenges such as ensuring reliable communication between
vehicles and managing mixed traffic environments that include
both automated and human-driven vehicles. Future research is
needed to address these challenges and validate the approach in
diverse and dynamic traffic conditions. This paper serves as a
stepping stone for practitioners aiming to develop more reliable
and adaptive CAV systems in the near future.
Index Terms—Connected and automated vehicles, multi-agent
1 Min Hua, Quan Zhou, and Hongming Xu are with the School of Engineering, University of Birmingham, Birmingham, B15 2TT, UK. (e-mail:
mxh623@student.bham.ac.uk, q.zhou@bham.ac.uk, h.m.xu@bham.ac.uk); ∗
is the corresponding author.
2 Xinda Qi is with the Department of Electrical and Computer Engineering,
Michigan State University, MI, USA. (e-mail: qixinda@msu.edu).
3 Dong Chen is with the Environmental Institute & Link Lab & Computer
Science, University of Virginia, VA, USA. (e-mail: dqc4vv@virginia.edu).
4 Kun Jiang is with the School of Automation, Southeast University,
Nanjing, China (e-mail: kjiang@seu.edu.cn).
5 Zemin Eitan Liu is with the Chemical and Petroleum Engineering
Department, University of Pittsburgh, Pittsburgh, PA, USA. (e-mail: eliuzm@163.com).
6 Quan Zhou is also with the School of Automotive Studies, Tongji
University, Shanghai 201804, China

reinforcement learning, intelligent transportation systems, and
Vehicle control.

I. I NTRODUCTION
HE transportation industry is experiencing a transformative shift driven by advancements in automation, artificial
intelligence (AI), the Internet of Things (IoT), and sensor
technologies [1], [2]. A central element of this shift is the
development of automated vehicles (AVs), which promise to
mitigate traffic congestion, enhance road safety, and increase
accessibility [3], [4]. The evolution of this field has led to
the emergence of connected and automated vehicles (CAVs),
which integrate AVs into a cohesive, networked transportation
system through advanced communication technologies such
as vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I),
and vehicle-to-cloud (V2C) [5]. This integration significantly
expands the operational capabilities of AVs, enabling extended
perception, enhanced collaborative decision-making, and improved traffic efficiency [6], [7]. However, the realization of an
effective CAV framework that harmonizes control, computing,
and communication presents substantial challenges, particularly in managing the intricate interactions between AVs and
other road users, including pedestrians, cyclists, and humandriven vehicles (HDVs) [8]. Addressing these challenges is
critical for advancing automation in transportation and ensuring the safe, efficient deployment of CAV systems.
Control of CAVs is pivotal in enhancing transportation
safety and sustainability, integrating with disciplines like energy management, urban planning, and social contexts [9],
[10]. Conventional optimization methods have addressed CAV
control challenges, such as using model predictive control
(MPC) to coordinate platoon behavior, minimizing control
delay, and reducing traffic oscillations [11]. Additionally, nonconvex optimal control problems, such as CAV coordination
at intersections, have been tackled with semidefinite relaxation
techniques [12]. However, these approaches often depend on
precise system modeling, which is not always available [13],
and require substantial computational resources, making them
impractical for real-time CAV control [14].
Besides, reinforcement learning (RL) has gained increasing
attention within the research field due to its outstanding
abilities in addressing sequential decision-making tasks, such
as gaming [15], robotics [16], [17], behavioral planning [18],
intelligent energy management [19]–[21]. Similarly, the AV
control has begun exploring the potential of employing RL for
various traffic scenarios [22], [23]. For instance, in [22], a safe
RL framework based on an improved double deep Q-Network
(DDQN) [24] is proposed for highway lane-changing, resulting
in zero collisions. In [25], a RL-based model for plattoning

T

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

control is introduced to enhance fuel economy, driving efficiency, and safety at signalized intersections through real-time
optimization. This approach featuring an effective reward function, demonstrates strong performance under varying traffic
demands and traffic light cycles with different durations. Taking into account the role of HDVs in the control of AVs, Shi et
al. employ a distributed proximal policy optimization (DPPO)
control strategy that allows them to learn from and respond
to the behavior of the HDVs, optimizing performance at both
the local subsystem level and the broader mixed traffic context
[26]. Qu et al. introduce a control approach utilizing the
deep deterministic policy gradient (DDPG) algorithm aimed
at reducing traffic fluctuations and enhancing fuel economy.
However, these approaches only consider a single agent (i.e.,
AV) and exhibit poor generalization when multiple agents are
involved. Additionally, their study focuses solely on individual
vehicle control and does not leverage shared information [27].
Multi-agent reinforcement learning (MARL) represents a
significant research direction within artificial intelligence, extending the concepts of single-agent RL to scenarios involving multiple interacting agents. This innovative approach
has found applications across a broad spectrum of fields.
Notably, the gaming and simulation industry has utilized
MARL to create more complex and interactive environments
[28]. Furthermore, within the realm of finance, MARL models
are employed to simulate the dynamics of markets and the
behaviors of agents [29]. In traffic control and management,
MARL algorithms have been instrumental in optimizing traffic
flows and reducing congestion. The realm of AVs and robotics
has also benefited greatly, with MARL enabling cooperative
navigation and coordination among vehicles and facilitating
collaborative tasks in AVs and robotics [30]. Extended to CAV
control, each vehicle acts as an independent agent that learns
from its interactions with the surrounding environment, which
includes other vehicles, traffic signals, and road conditions.
Each CAV learns not only to maneuver safely and efficiently
on its own but also to cooperate and communicate with other
CAVs [31]. Therefore, MARL offers a valuable strategy for
improving the performance and efficiency of CAVs, paving
the way for more sophisticated, collaborative, and flexible
transportation networks.
In scenarios involving multiple agents, MARL has emerged
as a promising tool for optimizing the collective behaviors,
showing significant potential for advancing the control strategies employed in the CAV ecosystem [32]. Fig. 1 illustrates
a comprehensive multi-agent system, i.e., CAVs, and depicts
a holistic approach to CAV operation, from sensory input and
data processing to inter-agent communication and eventual
decision-making and motion execution. On the left, real-time
environmental data from various sensors, including GPS/INS,
cameras, radar, and lidar, is gathered and then integrated via
sensor fusion. Within multi-agent system, each AV maintains
its own data repository while accessing shared information,
facilitating inter-agent communication for enhanced decisionmaking and situational awareness [33]. This system includes
modules for decision-making and motion planning, which use
data and inter-vehicle communication to make decisions and
plan movements. Finally, the steering control and powertrain

2

control modules demonstrate how these decisions translate
into physical actions, such as turning the steering wheel or
adjusting the engine’s or motor operation.
A good example of large-scale traffic signal control is
developed [34] through a highly scalable and decentralized
MARL algorithm, demonstrating superior control performance
over the other leading decentralized MARL algorithms. Furthermore, in [32], the challenge of on-ramp merging in mixed
traffic scenarios has been explored with a scalable and safe
MARL algorithm that utilizes a parameter-sharing technique
to enhance safety and scalability. Despite recent progress,
the development of efficient and scalable MARL algorithms
still presents significant challenges. The primary concern is
guaranteeing the robustness and safety of these algorithms,
especially in the face of diverse and uncertain traffic scenarios
[32]. Furthermore, the dynamic behaviors of CAVs lead to
non-stationary environments, adding layers of complexity to
the development of MARL algorithms [34]. Another challenge
lies in the lack of realistic simulators capable of accurately
modeling traffic scenarios and other road users, which are essential for effective training of MARL algorithms [35]. Lastly,
the real-world implementations of these algorithms remain an
uphill task due to sim-to-real gaps [32], and technological,
legislative, and societal barriers . While a few articles describe
the potential of MARL in CAVs [36], [37], none of them,
to the best of our knowledge, are specifically devoted to the
application of MARL for CAV control.
This paper is thus intended to deliver a comprehensive
and systematic review of MARL within the realm of CAVs,
such as lane changing, platooning control, traffic signal cooperation, and on-ramp merging. Through this review, we not
only provide a clear, up-to-date understanding of the current
classic MARL algorithms applied in CAV control, but we also
outline the potential direction for future work in this area. The
main contributions of the work presented in this paper are
summarized as follows:
1) We survey the recent developments in MARL algorithms, discussing their diverse applications in aspects of
CAV control based on the extent of control dimensions.
Furthermore, we provide an extensive examination of
the leading simulation platforms employed in MARL
research for CAVs.
2) We highlight and explore the technical challenges that
MARL faces in these applications and discuss potential
research directions to address these challenges.
3) The work, including a well-classified list of relevant
papers, has been presented at the following site: https://
github.com/huahuaedi/MARL_in_CAV_control_review.
This paper is organized as follows: Section II introduces
the basics of RL and MARL; A detailed review of MARL
applied in CAV control from different control dimensions and
mainstream simulation platforms are described in section III.
Section IV summarizes the remaining challenges and opportunities. Finally, Section V concludes the review.
II. M ETHODOLOGIES
In this section, we begin with a comprehensive overview
of the fundamental principles of RL. We then delve into

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

3

Fig. 1. The multi-agent control system for CAVs: the left side represents the information inputs of the overall control system, while the right side depicts
the studied multi-agent control system for CAVs.

various prominent MARL algorithms, establishing the context
to enhance the understanding of our review.
A. Preliminaries of RL
RL, frequently modeled through a mathematical framework
known as a Markov Decision Process (MDP), has risen as an
effective method for data-driven sequential decision-making
[38]. Recently, deep neural networks (DNN) have substantially
elevated the capability of RL to manage complex problems
[39]. Key developments include sophisticated algorithms such
as the Deep Q-Network (DQN) [39], Deep Deterministic
Policy Gradient (DDPG) [40], and Advantage Actor-Critic
(A2C) [41]. For instance, the AlphaStar, which operates on
principles similar to DQN, marked a significant milestone
by outperforming professional esports players in StarCraft II,
illustrating the power of RL in strategic game environments
[42]. Moreover, the development of the autonomous driving
system by leveraging DDPG exemplifies the potential of RL,
showcasing how vehicles can autonomously navigate through
environments with dynamic obstacles [43].
In the context of a RL framework (as shown in Fig. 2), a
learner known as the agent navigates through the environment using a process of trial and error. This agent assesses
situations, undertakes specific actions in the environment, and
consequently receives feedback in the form of a reward signal
and a new state. The feedback, provided by the environment,
serves as an indicator to the agent about the effectiveness of its
actions, signaling whether they have a positive or negative impact. Many RL problems are framed as MDP, which provide a
mathematical framework for modeling decision making where
outcomes are partly random and partly under the control of
a decision maker. MDPs are characterized by states, actions,
transition dynamics, and rewards with M = (S, A, P, R),
where it is defined as follows:
1) State space S: a representation of the environment at a
time stept. It can include all information necessary for
the agent to make a decision. An observation o offers
an incomplete state description, potentially missing full
details. In fully observed environments, the observation

is the same as the state. In partially observed environments, the observation might contain less information
than the state.
2) Action space A: the set of possible moves or decisions
the agent can make in a given state. The set of actions
available can depend on the state. The action spaces fall
into two categories: discrete or continuous.
3) Reward R(st , at , st+1 ): a scalar feedback signal given to
the agent from the environment after performing action
at in state st . The agent’s objective is to maximize the
cumulative reward over time.
4) Transition Probability Pss′ (St+1 = s′ |St = s): the
chance of an agent transitioning from one state to a
different state.

Fig. 2. Illustration of reinforcement learning (RL).

In the following subsections, we will explore three prevalent
RL algorithms, including Value-Based Methods, Policy-Based
Methods, and Actor-Critic Methods.
1) Value-Based Methods: In the domain of value-based
methods within RL, the Q-function, represented as Qθ , is
usually parameterized by a set of parameters denoted as θ.
This parameterization facilitates the use of different function
approximators for estimating Q-values, including traditional
Q-tables [44] and the complex Deep Neural Networks (DNN)
[39]. The core mechanism for updating these parameters relies
on the temporal difference (TD), expressed as (T Qθ− −
Qθ )(st , at ), where T signifies the dynamic programming
operator and Qθ− represents a version of the model parameters
θ− that is temporarily fixed to stabilize updates [45]. To

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

4

mitigate the variance in Q-value estimates and enhance the
exploration ability of the algorithm, strategies such as the
ϵ − greedy method and experience replay mechanism are
frequently employed within deep Q-learning frameworks [46].
The optimal action a∗ (s) can be described as:
a∗ (s) = arg max Q∗ (st = s, at = a),

(1)

Despite considerable advancements, single-agent RL algorithms frequently encounter scalability challenges, especially
in complex, real-world physical environments that involve
multiple agents. These difficulties stem from inherent nonstationarities and the partial observability characteristic on
multi-agent scenarios [50].

a

Some widely recognized deep Q-learning-based algorithms
include DQN [47], double deep Q-Network (DDQN) [24], and
hindsight experience replay (HER) [48].
2) Policy-Based Methods: Diverging from the value-based
Q-learning approach, policy-based methods prioritize direct
manipulation of the policy itself, denoted as πθ , via a distinct
set of parameters θ. The primary goal of adjusting θ revolves
around amplifying the likelihood of selected actions along
the overall rewards accumulation. This process is guided by
a specifically defined loss function, which is mathematically
represented as:
T
X
∇θ L(πθ ) = E [
∇θ log πθ (at |st )Rt ],
τ ∼πθ

(2)

t=0

Subsequently, the parameters of the policy network are updated by employing stochastic gradient ascent, facilitating an
iterative improvement of the policy, as detailed by
θk+1 = θk + α∇θ L(πθ ),

(3)

In contrast to Q-learning methods, policy gradient techniques
demonstrate enhanced resilience to the non-stationary dynamics characterizing individual trajectories, albeit at the cost of
potentially higher variance in outcomes [34]. Among the most
prominent algorithms that embody the principles of policy gradients are Deep Deterministic Policy Gradient (DDPG) [40],
Soft Actor-Critic (SAC) [49], and Asynchronous Advantage
Actor-Critic (A3C) [41].
3) Actor-Critic Methods: Actor-critic methods aim to mitigate the high variability in outcomes often seen with policy
gradient techniques by integrating an advantage function.
This function enhances the policy gradient methodology by
utilizing both the policy update (actor) and a value estimation function (critic) [41]. The advantage function, a pivotal
element in this approach, is defined as:
Aπ (st , at ) = Qπθ (st , at ) − Vw (st ),

(4)

where the update of parameters θ is guided by a policy loss
function articulated as:
T
X
∇θ L = E [
∇θ log πθ (at |st )At ],
πθ

(5)

t=0

Concurrently, the critic value function is refined through:
L = min E[(Rt + γVw− (st ) − Vw (st ))2 ],
w

D

(6)

with D denoting the experience replay buffer, a repository
for history experiences. This buffer, in collaboration with
parameters from preceding iterations often employed within
a target network, facilitates the learning process [32].

B. Preliminaries of MARL
In systems with multiple agents (e.g., AVs in Fig. 3), not
only the reward agents themselves but also the reward of
their neighbors will be influenced by their actions. MARL
has been widely used in various complex systems, including
managing traffic [34], strategic gameplay [51], optimizing
wireless network resources [52], and configuring power grids
[53], to highlight a few areas. A fundamental approach within
this field is Independent Q-learning (IQL), where each agent’s
Q-function is primarily developed based on local actions,
as Qi (s, a) ≈ Qi (s, ai ), simplifying the learning process.
Similarly, the Independent Advantage Actor-Critic (IA2C) [45]
represents a variant of MARL, indicating the breadth of strategies explored in this space However, both IQL and IA2C face
challenges in applications due to the partial observability and
dynamic environment, as they operate under the assumption
that all other agents’ actions are part of the environment,
which complicates learning when agents’ strategies continually
evolve [34].
Multi-agent systems are often treated as networks G =
(ν , ε) without a central controller [34], [53], where agents
i ∈ ν communicate with nearby peers Ni := {j|εij ∈ ε)} via
the edge connections εij , i ̸= j. This decentralized framework,
where each agent only perceives part of the environment,
reflects real-world scenarios like AVs detecting surrounding
vehicles. This structure effectively simulates the dynamic
environment as a partially observable MDP (POMDP) MG =
({Ai , Si , Ri }i⊆ν , T ), with the system’s behavior determined
by the collective actions (A = A1 × A2 × · · · × AN ) of all
agents and the state transitions (with the state: S = S1 ×
S2 ×· · ·×SN ) governed by probability distributions T (s′ |s, a).
Rewards play a crucial role in guiding agents toward achieving
goals, which vary across cooperative, competitive, and mixed
scenarios. In cooperative environments, agents work together
towards a common goal, possibly sharing a uniform reward
(R1 = R2 = · · · = RN ) or aiming to optimize an average
reward that accommodates differences among agents (teamaverage reward [54]). Conversely, competitive scenarios feature agents with individualistic
goals, often modeled as zeroP
sum games (R̄ = N1 i∈ν Ri (s, a, s′ )), where one agent’s
gain is the loss of another. Mixed environments (also known
as general sum game settings) allow for both cooperation and
competition, with agents pursuing individual yet not directly
conflicting objectives.
The majority of applications reviewed in Section III of this
paper mainly focus on cooperative scenarios, highlighting the
importance of collaboration in achieving shared objectives in
multi-agent systems.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

5

Fig. 3. Illustration of multi-agent reinforcement learning (MARL).

C. Training and execution strategies in MARL
In MARL settings, different approaches and frameworks are
employed to tackle the complex challenges of coordinating and
training agents in decentralized environments. This subsection explores two fundamental paradigms: Centralized Training with Decentralized Execution (CTDE) and Decentralized
Training with Decentralized Execution (DTDE), each offering
distinct insights and trade-offs in the pursuit of effective multiagent learning [55]–[57].
1) Centralized training with decentralized execution
(CTDE): The majority of MARL algorithms adhere to centralized training with a decentralized execution (CTDE) framework. In this framework, the decentralized problem is initially
transformed into a centralized one, solvable by a central
controller, which gathers essential training information, e.g.,
observation, reward, and global state information, for the
training process. Following this data collection, the centralized
value functions are learned based on the information of all
agents, and then the gradient from the centralized value
function is used to train the policy of each agent. But in the
process of execution, each agent outputs action according to
its individual observation (see Fig. 4).
It is important to acknowledge the inherent trade-offs of
the CTDE. One significant strength is the enhanced learning
stability enabled by centralized training, which can result in
more robust policies [58]. However, it is crucial to recognize
that maintaining a centralized controller comes with its own
set of challenges. Firstly, it can be prohibitively expensive and
infeasible in certain scenarios, particularly in large-scale or
resource-constrained environments. Additionally, the centralized controller introduces privacy concerns, as it necessitates
the sharing of sensitive information among agents. Moreover,
the central controller becomes a single point of failure, rendering the whole system vulnerable to disruptions. These tradeoffs highlight the need for careful consideration of the CTDE
framework’s suitability in specific MARL applications, taking
into account the advantages and drawbacks it presents.
2) Decentralized training with decentralized execution
(DTDE): In decentralized settings, individual agents acquire
knowledge independently, devoid of direct interactions. Each
agent operates with its distinct set of observations, policies,
and algorithms, utilizing the environment as the exclusive
channel for engagement. Commonly employed algorithms
within this framework encompass independent Q-learning
(IQL) [59] and independent advantage actor-critic (IA2C) [45],
frequently adopted as foundational benchmarks. Although

Fig. 4. Illustration of centralized training with decentralized execution
(CTDE) in MARL.

these algorithms enable autonomous learning by agents, they
may not fully harness the collaborative potential [60]. To
address non-stationarity concerns, [61] proposes a fully decentralized MARL framework. In this framework, each agent
makes decisions based on both its local observation and the
messages exchanged with neighboring agents via the network,
offering convergence guarantees. Fig. 5 shows the DTDE
pattern based on the actor-critic framework.

Fig. 5. Illustration of decentralized training with decentralized execution
(DTDE) in MARL.

D. MARL algorithm variants
In this subsection, we have exclusively reviewed the common MARL algorithms that find utility in CAV applications.
Further, the MARL algorithms are divided into four categories:
value function decomposition, learning to communicate, hierarchical structure, and causal inference, which are rooted in
the inherent complexity and diverse requirements of multiagent environments, particularly in CAV control. Table I
offers an extensive overview of key works from four distinct
perspectives within various settings.
1) Value function decomposition: The challenge of credit
assignment in cooperative settings has emerged as a significant
area of research interest. The shared rewards in a fully
cooperative environment make it difficult to distinguish the
contribution of each agent, some agents tend to be lazy [81].
To solve the above problem, some studies learn different value

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

6

TABLE I
A N OVERVIEW OF THE PRIMARY WORKS FOR MARL ALGORITHMS .
Categories

Work

MARL
algorithm

Value Function
Decomposition

[62], 2018
[63], 2018
[64], 2019
[65], 2020
[66], 2021
[67], 2023

VDN
QMIX
QTRAN
WQMIX
AVD-Net
PER

Decomposing value functions using linear summation
Enforcing monotonicity constraints between joint and individual value functions
Relaxing the non-negative weight network in Qmix
Introducing weighting functions and non-monotonic mixing networks
An attention-based approach to capitalize on the coordination relations between agents
Both the reward of an agent acting on its own and cooperating with other observable agents

Learning to
Communicate

[68], 2017
[69], 2016
[50], 2020
[70], 2020

FingerPrint
DIAL
NeurComm
IMAC

Merging low-dimensional policy fingerprints in the state space of neighboring agents
The agent continuously encodes information and sends it to other recipients
Encoding and concatenating communicating signals rather than aggregating them
Learning an efficient communication protocol based on the information bottleneck principle

[71], 2019
[72], 2018
[73], 2019
[74], 2019
[75], 2023

FHM
HQMIX
HSD
RODE
HAVEN

Establishing a feudal hierarchy structure
Including hierarchical QMIX and hierarchical communication network
Training a cooperative decentralized policy for high-level skill selection
Introducing a method to clearly divide the action space through action clustering
Designing a dual coordination mechanism within a two-level hierarchical structure

[76], 2018
[77], 2023
[78], 2023
[79], 2022
[80], 2022

CIR
ICL
LAIES
FD-MARL
DVD

Agents receive a reward based on their causal influence on the actions of others
Evaluating the causal effect of each agent’s observations on team collaboration performance
Calculating the causal effect of their actions on external states using the do-calculus process
Constructing continuous communication protocols based on causal analysis
Deconfounded value function decomposition based on causal effects

Hierarchical
Structure

Causal
Inference

Novelty

functions to distinguish the contribution of each agent. Foerster
et al. [82] and Guo et al. [83] introduced the counterfactual
baseline principle to learn different value functions by centralized learning method in the context of a shared team reward
environment. Additionally, Hou et al. have developed a credit
allocation mechanism based on role attention, which facilitates
the learning process of role policies in multi-agent systems
by managing how credit is allocated among agents [84]. Liu
et al. construct the causal effect of the agent’s actions on the
external state from the perspective of a causal diagram, solving
the problem of lazy agents [78].
Among the above methods to solve credit assignment,
value function decomposition is considered to be an important research direction. The main concept and feature of
value function decomposition is to decompose a joint value
function into individual components to identify the distinct
contributions of each agent. In the process, many useful value
function methods were proposed to solve increasingly complex
problems. VDN decomposes the joint action-value function by
breaking it down into a sum of the action-value functions for
individual agents. This linear decomposition method yields
favorable results in certain simple problems [62]. Different
from the linear value function decomposition method of VDN,
QMIX enhances the representational capacity of the joint
value function by enforcing monotonicity constraints between
joint and the individual value functions. This approach allows
for handling increasingly complex scenarios effectively [63].
QTRAN further improves the representation ability of joint
functions by relaxing the non-negative weight network in
QMIX [64]. WQMIX extends the value function method to
complex scenarios of non-monotonic rewards by introducing
weighting functions and non-monotonic mixing networks [65].
After that, some variant algorithms based on VDN and QMIX

were used to solve different specific problems [66], [67].
2) Learning to Communicate: Research in MARL can be
categorized into two distinct groups according to their methods
of communication [50]. The first group operates without communication among agents and centers its efforts on stabilizing
training by employing advanced value estimation methods. For
example, MADDPG [85] extends DDPG to the MARL setting
by employing a centralized critic network to enable the global
value estimate. Similarly, COMA [86] modifies the actor-critic
method for use in MARL, calculating the advantage function
for each agent by employing a centralized critic in conjunction
with a counterfactual baseline. The second group investigates
heuristic communication protocols, which may involve either
direct message sharing or learnable communication protocols.
For instance, in FingerPrint [68], the low-dimensional policy
fingerprints are directly shared and incorporated into the
state space of neighboring agents. In DIAL [69], each DQN
agent concurrently produces a communicated message while
assessing action values. This message is encoded and subsequently combined with additional input signals at the receiving
end. NeurComm proposed in [50] argues that encoding and
concatenating communicating signals, rather than aggregating
them, offers distinct advantages in mitigating information
loss during communication. IMAC method learns an efficient
communication protocol as well as scheduling based on the
information bottleneck principle [70]. Their method involves
the development of a novel differentiable communication protocol, which incorporates information related to state, policy,
and the encoded data from neighboring agents. Subsequently,
this information is encoded and concatenated into the state
space of the neighboring agents.
3) Hierarchical Structure: In the realm of multi-agent systems, a plethora of challenges are encountered, encompassing

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

issues like sparse rewards, sequential decision-making, and
limited transfer capabilities. To address these challenges effectively, the adoption of a hierarchical approach is unequivocally
advantageous. The fundamental tenet of hierarchical multiagent reinforcement learning revolves around the acquisition
of hierarchical strategies for decision-making across varying
levels of temporal abstraction. Current research in this domain broadly categorizes hierarchical multi-agent reinforcement learning into two paradigms: option-based hierarchical
structures [87], [88] and goal-based hierarchical structures
[89], [90], both of which have found applications in diverse
multi-agent scenarios.
Feudal Multi-agent Hierarchies (FMH) establish a feudal
hierarchy within a multi-agent context, yielding noteworthy
results. However, a primary limitation of this approach is its
inherent challenge in tackling fully collaborative tasks with
shared rewards [71]. To address the predicament of multiagent scenarios characterized by sparse and delayed rewards,
several hierarchical structures with temporal abstraction have
been proposed, including hierarchical QMIX and hierarchical
communication network methods [72].
Furthermore, recognizing the limitations of traditional approaches, which typically rely on manually defined highlevel action spaces, the Hierarchical Learning with Skill
Discovery (HSD) model was conceived. This model aims to
autonomously train decentralized policies for high-level skill
selection while simultaneously developing independent lowlevel policies for the execution of these skills, thereby enhancing adaptability to changing scenarios [73]. On the other hand,
learning roles to decompose (RODE) is introduced for clear
action space partitioning through action clustering, where each
action corresponds to a distinct subspace, this innovation substantially mitigates the challenge of extensive parameter tuning
associated in HSD [74]. Moreover, HierArchical Value dEcompositioN (HAVEN) introduces a dual coordination mechanism,
encompassing inter-layer and inter-agent strategies, facilitated
by the formulation of a reward function within a two-level
hierarchical structure. This approach effectively addresses the
instability issue through the concurrent optimization of strategies at all levels and inter-agent coordination [75]. Hierarchical
MARL is recognized as a promising avenue for addressing
collaborative decision-making challenges in complex, largescale scenarios. Nonetheless, it faces the challenge of difficulty
in designing complex dynamic hierarchical structures and
difficult migration of policies.
4) Causal Inference: There are multiple complex variables
in MARL, and it is usually difficult to directly discover their
internal relationships. Therefore, some recent research has
begun to combine MARL with causal inference to discover
the causal relationships between agents or variables, and
further understand the operating mechanism of agents through
intervention and inference, thereby motivating the agent to
conduct more targeted learning [91]. Pearl’s three-level causal
model is considered a powerful tool for constructing causal
relationships between variables [92]. Causal influence reward
methods promote the collaborative performance of agents
by rewarding those agents that have a causal influence on
the actions of other agents, where this causal influence is

7

evaluated by counterfactual reasoning [76]. Rafael Pina et
al proposed the independent causal learning (ICL) algorithm
to evaluate the causal effect of each agent’s observations on
team collaboration performance and solve the credit allocation
problem in the independent learning framework [77].
There are also other works that combine causal inference
from different perspectives under the framework of MARL.
LAIES mathematically define the concept of fully lazy agents
and teams by calculating the causal effect of their actions on
external states using the do-calculus process, which solved
the sparse reward problem in MARL [78]. FD-MARL allows
agents to modify communication messages by choosing the
counterfactual that bears the most significant influence on
others, the continuous communication based on causal analysis
enables efficient information transformation in a fully decentralized manner [79]. The deconfounded value decomposition (DVD) method investigates value function decomposition
from the perspective of causal inference, which cuts off the
backdoor confounding path from the global state to the joint
value function [80]. In addition, causal inference has also been
applied to some practical application scenarios, such as traffic
signal control [93] and human-computer interaction decisionmaking [94].
III. T OWARDS APPLICATIONS OF MARL IN CAV S
CAVs have garnered substantial interest for their potential to reshape the transportation landscape, ushering in an
era marked by heightened efficiency, enhanced safety, and
bolstered sustainability. Operating within networked environments, CAVs engage in interactions with multiple vehicles,
promoting cooperative driving. For example, CAVs can acquire the capabilities to collaborate, execute seamless merges,
and navigate intersections with precision, thereby advancing
traffic safety and optimizing flow [106]. These transformative
capabilities are increasingly being explored in tandem with
MARL to further optimize CAV interactions within networked
environments. Within the realm of MARL, CAVs are designed
to engage in intelligent interactions with multiple vehicles,
thus promoting cooperative driving strategies.
In this section, we will comprehensively explore the recent
strides made in the utilization of MARL within CAV applications. Our examination will be structured according to various
dimensions of cooperation, each correlated with a specific
number of control components.
• One-dimensional cooperation corresponds to scenarios
involving control along a single control direction, such
as either longitudinal control or lateral control.
• Two-dimensional cooperation extends the scope to include both longitudinal and lateral control components,
reflecting the increased complexity in the coordination
and decision-making of CAVs.
• Three-dimensional cooperation further augments the
challenges and opportunities by incorporating additional
constraints, such as time limits, which encompass aspects
like traffic light control and on-ramp merging.
By categorizing these advancements based on control components, we aim to provide a comprehensive perspective on

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

8

TABLE II
A N OVERVIEW OF THE PRIMARY WORKS FROM 1-D AND 2-D COOPERATION FOR MARL APPLIED IN CAV CONTROL .
Scenarios

Typical
application

Work

MARL
algorithm

[95], 2020

LSTM-Based
MA-Reinforce

Metrics

Novelty
• Individual and global rewards design

• Convergence stability

• A LSTM-based communication protocol

• Travel time

• Local channel data incorporation

[96], 2020

1-D
Cooperation

[50], 2020

NeurComm

[97], 2021

C-HARL

Platooning
control

[98], 2022

[99], 2023

[100], 2021

[101], 2021

2-D
Cooperation

MA-DDQN

Cooperative
lane
changing

MADDPG
Distributed
PPO

MAPPO

Zero-sum
DQN

[102], 2021

MADQN

[103], 2022

QMIX

[104], 2022

Bi-level
DQN

[105], 2022

MA2C

• Sum-rate of V2N
• Packet probability of V2V

• A different reward design including the weighted

sum-rates and the transmission time
• A spatial discount factor to stabilize training
• A new differentiable communication protocol on
both agent states and behaviors

• Average headway
• Average velocity
• Convergence

• Cooperative hierarchical attention RL (C-HARL)

• Success rate of message

• Meta-learning with GNN integration

• Communication delay

• A Stackelberg game to model interactions

• Convergence

• Decisions sharing with social partners within the

• Defined utility value

platoon due to social effects
• State fusion strategy with the equilibrium to
stabilize traffic oscillations
• A reward function design in quadratic form
• A decentralized cooperative lane-changing design
• A novel reward function design considering
eliminating traffic shock waves
• The delay of an individual vehicle and overall
traffic efficiency in the reward function
• A lane change model with limited sensing
without V2X communications
• Interactions and negotiations model establishment
between multiple agents
• An experience replay mechanism to tackle the
non-stationarity
• The simplification of system complexity through
reinforcement learning
• The fairness in cooperation to allow for independent
lane-changing and overtaking
• An innovative reward function designed with their
own benefits and the impact on overall traffic
• The driving intentions of the surrounding vehicles
encoded into the observation space

• Social effect
• Driving comfort

In this subsection, we will conduct a detailed review of onedimensional cooperation applications, which primarily focus
on longitudinal control. One prominent example of such application is platooning control, where CAVs operate in closelyknit formations and collaborate to enhance efficiency, safety,
and fuel economy. This is achieved by minimizing aerodynamic drag and optimizing traffic flow through synchronized
control strategies and inter-vehicle coordination. Platooning
control stands as a noteworthy illustration of how MARL
contributes to the advancement of transportation systems.

• Traffic throughput
• Number of Stops
• Fuel efficiency
• Convergence
• Stability
• Travel efficiency

• Timeout
• Accident
• Success rate

• Collaborative effect
• Travel efficiency

• Comfort
• Safety
• Travel efficiency

• Safety

• Multi-objective reward function

A. One-dimensional cooperation

• Travel efficiency

• Driving comfort

• A innovative local reward and parameter-sharing

the evolving landscape of MARL within CAV applications,
with a focus on the intricacies of cooperation and control
in varying dimensions. Additionally, we provide an extensive
examination of the leading simulation platforms employed in
MARL research for CAVs.

• Stability

• Travel efficiency

As shown in Fig. 6, the problem of platooning control
is commonly addressed through a model-free, multi-agent
network approach [50]. In this framework, each agent, symbolizing an AV, has the ability to communicate with both
preceding and following vehicles via vehicle-to-vehicle (V2V)
communication channels. This setup aims to ensure safe, fuelefficient, and smooth vehicle following operations, while also
maximizing the advantages of driving in close formation [107].
Given N vehicles in a platoon, the control objective is to
maintain a desired spacing Li and ensure velocity matching.
The state vector [108] for each vehicle i at time t is:

ẋi (t) =

0
0

1
0




xi (t) +

0
1


(kp ((xi−1 (t) − xi (t)) − Li )
(7)

+kd (vi−1 (t) − vi (t)))
where xi (t) is the position of the ith vehicle at time t, vi (t)
is the velocity of the ith vehicle at time t, Li is the desired

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

9

inter-vehicle distance (spacing) between vehicle i and i + 1,
T
xi (t) = [xi (t), vi (t)] is the state vector, kp and kd are the
proportional gain for position error and the derivative gain for
velocity error respectively. To optimize the platoon control, an
objective function J can be defined, incorporating both spacing
error and velocity error over a time horizon T :
J=

N Z T
X
i=1


αe2x,i (t) + βe2v,i (t) dt

(8)

0

where α and β are weighting factors that balance the importance of position and velocity errors. Thus, cooperative
strategies would be developed to achieve the collective goal,
adapting their behaviors in response to the actions of other
vehicles within the network.

Fig. 6. One-dimensional cooperation scenarios: the case of platooning control.

One critical task of platooning control is to achieve string
stability, which refers to the ability of a line of CAVs to maintain a stable and orderly formation (e.g., stable distance and
speed) as they travel in close proximity to each other [109].
Notably, MARL has been proposed to address and enhance
the achievement of string stability in the platooning control
scenario, contributing to the development of more efficient and
coordinated platooning control strategies. For instance, in [95],
a MARL approach based on a robust communication protocol
with Long Short-Term Memory (LSTM) [110] is introduced,
resulting in stable platooning control. In addition, Li et al.
introduce the Communication Proximal Policy Optimization
(CommPPO) algorithm to enhance stability, which adapts to
varying agent numbers and supports different dynamics of platooning control. CommPPO incorporates a predecessor-leaderfollower communication protocol to facilitate the transmission
of both global and local state information among agents.
Notably, CommPPO introduces a novel reward communication channel, effectively mitigating issues related to spurious
rewards and mitigating the problem of “lazy agents”, which
are commonly encountered in other MARL approaches [111].
In [50], the platooning control problem is formulated as a
spatiotemporal MDP. They enhance system stability by introducing a spatial discount factor for local agents. Furthermore,
they introduce a novel differentiable communication protocol,
NeurComm, which mitigates non-stationarity, leading to improved learning efficiency and control performance.
Efficient communication among CAVs in platooning control
also poses a fundamental challenge in achieving seamless
coordination and maintaining the desired following properties, such as precise inter-vehicle spacing and synchronized

maneuvers. For instance, Liu et al. introduce a Multi-Agent
Hierarchical Attention RL (MAHARL) framework, which
incorporates the Graph Attention Network (GAT) at each
hierarchical level to account for the reciprocal influences
between agents. The hierarchical architecture of MAHARL
empowers agents with foresight, enabling them to make favorable decisions, even in the absence of immediate incentives,
and consider long-term rewards, as elaborated in their work
[97]. Additionally, in scenarios where complete information is
lacking, Li et al. [98] develop a game-theoretic framework to
capture the strategic interactions between the leading vehicle
and the following vehicles. To overcome the absence of full
information, the equilibrium solution in platooning control is
identified through backward induction, coupled with information collection from the following vehicles.
Mixed platooning control involves the simultaneous presence of multiple vehicle platoons within a network, necessitating a sophisticated level of coordination and communication
both within (intra-platoon) and between (inter-platooning)
these platoons. [112]. Parvini et al. have introduced two
advanced MARL algorithms specifically designed to address
the challenges of mixed platooning control. These include the
modified MADDPG and its variant, the modified MADDPG
with task decomposition. In these frameworks, the leaders of
the platoons are treated as independent agents that interact
with the environment to determine the most effective policies
for platoon management. Notably, these algorithms employ
multiple critics to estimate both global and local rewards,
thereby fostering cooperative behavior among the agents. In
the second algorithm, individual rewards are further decomposed into task-specific sub-reward functions, offering insights
into their work as detailed in [113].
Even though CAVs have made significant advancements,
there will be an extended period during which CAVs will
coexist with human-driven vehicles (HDVs), a situation commonly referred to as mixed-traffic scenarios [45]. Different
penetration rates can be assessed to understand the impact of
mixed traffic. In the study presented in [114], the impact of
the penetration rate of CAVs on the energy efficiency of the
traffic network is examined, with a specific emphasis on a cooperative eco-driving system that utilizes longitudinal control.
A noteworthy aspect of this research is the introduction of a
role transition protocol for CAVs, enabling them to smoothly
transition between leading and following positions within a
vehicle string. Lu et al. enhance the evaluation by integrating
several key elements, including altruism control, a quantitative
car-following strategy, a refined platoon reward function, and
a collision avoidance method, as outlined in their work [115].
In [99], the authors introduce the concept of characterizing
consecutive HDVs as a collective entity, referred to as AHDV,
to minimize stochastic variability and leverage macroscopic
characteristics for the control of following CAVs. They employ
a control strategy built on distributed proximal policy optimization (DPPO) to anticipate disturbances and downstream
traffic conditions in mixed traffic scenarios. In vehicular environments characterized by high mobility, the use of traditional
centralized optimization methods that rely on global channel
information can be impractical. In [96], the authors model each

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

10

CAV as an individual agent, which makes decisions based
on local information and communication with neighboring
vehicles, without relying on a centralized controller.
B. Two-dimensional cooperation
Compared to one-dimensional cooperation, twodimensional cooperation addresses problems related to
both longitudinal and lateral control, with its most typical
application being cooperative lane changing. As shown in
Fig. 7, CAVs communicate and coordinate through V2V
and V2I communication channels, enabling them to make
decisions and acquire effective lane-changing strategies within
shared driving environments [116].
Generally, the lane-changing mathematical problem [117]
can be expressed as follows:
Z tf " 
2
min J =
α1 (xi−1 (t) − xi (t) − Li − ∆xsafe ) +

ui (t)

t0


2
(xi (t) − xi+1 (t) − Li+1 − ∆xsafe ) + α2 (tf − t0 )
2


d
2
2
ux,i (t) +
+ α3 ux,i (t) + uy,i (t) +
dt
#

2 
d
dt
uy,i (t)
dt
(9)
Subject to the vehicle dynamics:
ẋi (t) = Axi (t) + Bui (t)

(10)

where α1 , α2 , and α3 are the corresponding weights for safety,
efficiency, and comfort,respectively. Other

 may also
 objectives
0 0
0 0 1 0
 0 0 
 0 0 0 1 



be considered; A = 
 0 0 0 0  , B =  1 0 ,
0 1
0 0 0 0
T
xi (t) = [xi (t), yi (t), vx,i (t), vy,i (t)] is the state vector for
vehicle i; xi (t) and yi (t) are the longitudinal and lateral
position of the ith vehicle at time t; vx,i (t) and vy,i (t)
are longitudinal and lateral velocity of the ith vehicle at
T
time t. ui (t) = [ux,i (t), uy,i (t)] is the control input vector
for vehicle i, ux,i (t) and uy,i (t) are the control input for
longitudinal and lateral acceleration respectively; Li is the
length of the ith vehicle; ∆xsafe is the safety distance to avoid
collision; wlane is the width of the lane.
And the constraints are expressed as:
xi−1 (t) − xi (t) ≥ Li + ∆xsafe
xi (t) − xi+1 (t) ≥ Li+1 + ∆xsafe
yi (t) ∈ {current lane, target lane}

(11)

0 ≤ yi (t) ≤ wlane
To achieve a safe, efficient, and comfortable lane-change
maneuver, Hou et al. devise a decentralized cooperative lanechanging controller employing a Multi-Agent Proximal Policy
Optimization (MAPPO) approach, which empowers each vehicle to independently learn and assess its policy and action
rewards based on local information, while still having access

Fig. 7. Two-dimensional cooperation scenario.

to global state data. The trained policies can be effectively
transferred and applied across various traffic conditions, enhancing traffic throughput from uncongested to highly congested scenarios [100]. In [103], the authors put forth a twinvehicle cooperative driving approach, leveraging the QMIX
algorithm. Their method applies RL to dynamically adapt to
changing conditions on highways, aiming to find an optimal
balance between autonomous decision-making and cooperative
interaction among the vehicles. This algorithm allows each
vehicle in the pair to independently perform lane changes
and overtaking maneuvers, even in dense traffic scenarios,
while maintaining a predetermined formation between them.
An enhanced QMIX algorithm [118] is further proposed to
enhance the flexibility and effectiveness of the collaborative
lane-changing system. They implement a stable estimation
method to mitigate the problem of overestimated joint Qvalues among agents. This approach strikes a fine balance
between maintaining formation and allowing for smooth overtaking, thus facilitating intelligent adaptation to a variety of
scenarios, including heavy traffic, light traffic, and emergencies. In an effort to enhance mobility within complex traffic
environments, in [119], instead of relying on relative distance
and semantic maps, the authors suggest the utilization of
traffic states that encapsulate the spatio-temporal interactions
between neighboring vehicles. Within the MADRL framework, three prediction models, namely the transformer-based
(TS-Transformer), generative adversarial network-based (TSGAN), and conditional variational autoencoder-based (TSCVAE) models, are developed and compared for traffic state
prediction.
Graph neural networks (GNNs) [120] have gained significant attention in MARL settings, particularly for cooperative
lane-changing applications. Chen et al. introduce a novel
algorithm based on MADQN, which combines a GCN with
a deep Q-network [121]. This approach facilitates effective
information fusion and decision-making within the MARL
framework, thereby enhancing both safety and mobility in
cooperative lane-changing scenarios. Moreover, in [122], the
authors leverage GNN in conjunction with MADDPG in their
work for multi-agent training, addressing the complexity of
real-world inputs and aiding in congestion mitigation efforts.
Safety is of paramount importance in the context of CAV
scenarios. In [123], the authors have introduced a safetyenhancing actor-critic algorithm that incorporates two innovative techniques. The first technique is the ‘truncated Q-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

function’, which effectively leverages shared information from
neighboring CAVs, ensuring scalability for large-scale CAV
systems. The second technique, “safe action mapping”, provides safety guarantees throughout both the training and execution phases by utilizing control barrier functions. Additionally,
a bi-level strategy is proposed in [104] to further enhance
safety and efficiency. At the upper level, the MADQN model
is utilized for making decisions about lane changes. This
approach acknowledges the cooperative aspect of driving by
factoring in the intentions of surrounding vehicles, enabling
implicit negotiation for right-of-way. At the lower level, a
right-of-way assignment model is utilized to ensure safety.
A novel reward function is further proposed to encourage
coordination and account for traffic impact. Moreover, Li et
al. have introduced MetaDrive, a platform that focuses on
safe driving by addressing generalizability, safety awareness,
and multi-agent decision-making. It offers diverse driving
scenarios for benchmarking single and multi-agent RL tasks
[124].
Thoughtful reward function design is paramount for the
effectiveness and efficiency of lane-changing strategies. In
[105], the authors introduce the multi-agent advantage actorcritic (MA2C) framework, which integrates a novel local
reward design and parameter-sharing scheme for cooperative
lane changing in mixed traffic scenarios. They also present
a multi-objective reward function that considers factors such
as fuel efficiency, driving comfort, and safety to facilitate
successful and efficient lane changing. In [101], a MARL
approach is explored in which agents collaborate to reach a
zero-sum game state. In contrast to cooperative driving, this
approach, known as harmonious driving, places emphasis on
achieving a balance between overall and individual efficiency
while utilizing limited sensing data from individual vehicles.
They design a reward function that promotes harmony by considering both individual and overall efficiency, as opposed to
competitive strategies that solely prioritize individual interests.
C. Three-dimensional cooperation
Three-dimensional cooperation involves the intricate coordination of longitudinal and lateral movements along with
timing, across a variety of traffic scenarios such as traffic
signal coordination, on-ramp merging, and navigating through
unsignalized intersections, as depicted in Fig. 8. To ensure
safe, efficient, and effective cooperation, CAVs need to rapidly
learn longitudinal and lateral maneuvers within tight time
constraints. The three-dimensional cooperation problem can
also be described by Eq. 9 - Eq. 11, with the added time
constraints ti, start ≤ t ≤ ti, end .
1) Traffic signal control (TSC) : TSC aims to alleviate
congestion in saturated road networks by adapting signal
timings based on real-time traffic conditions (see Fig. 8
(a)). Traditional TSC methods address congestion by solving
optimization problems to determine effective coordination and
control policies. However, this process can be time-consuming
and often requires precise system dynamics [45]. Recently,
model-free MARL approaches have shown a growing interest
in precise TSC.

11

Value-based MARL algorithms have been extensively investigated for TSC challenges. For instance, to alleviate the curse
of dimensionality and environmental nonstationarity problems,
a decentralized coordination MADQN approach is proposed
in [125] to explicitly identify and dynamically adapt agent
coordination needs during the learning process. Similarly, in
[126], the complex TSC problem is decomposed into simpler
subproblems and tackled using multiple regional agents and a
centralized global agent. Each regional agent learns its RL policy for smaller regions with reduced action spaces, while the
centralized global agent combines the RL contributions from
various regional agents to form a final Q-function for the entire
large-scale traffic grid. Notably, QMIX, an extension of VDN,
was investigated in [127] for TSC, achieving promising results.
Additionally, Liu et al. introduce a multi-agent dueling-doubledeep Q network (MAD3QN) for TSC [128]. This framework
features an innovative γ-reward design, which combines the
traditional γ-reward with a novel γ-attention-reward. By employing a spatial differentiation method for agent coordination,
their approach enables decentralized control and decoupling
of road networks, enhancing scalability and convergence. In
[129], a cooperative group-based multi-agent Q-learning for
ATSC (CGB-MATSC) is proposed. This method enhances
the learning process by incorporating a k-nearest-neighbor
approach for state representation, a pheromone-based strategy
for creating regional green-wave traffic flows, and a spatially
discounted reward system.
On the other hand, policy-based MARL algorithms have
also been widely studied for TSC. In [34], a fully scalable and
decentralized Multi-Agent Actor-Critic (MA2C) algorithm is
introduced, which incorporates an advanced discount factor to
mitigate the effects of remote agents. This approach demonstrates significant advantages over random and independent
controllers. Wu et al. introduced the multi-agent recurrent
deep deterministic policy gradient (MARDDPG) algorithm,
specifically designed for TSC in vehicular networks [130].
They utilize a strategy of CTDE, enhancing the efficiency
of the training process through parameter sharing among
the actor networks. Furthermore, the integration of LSTM
networks enables the algorithm to utilize historical data for
more effective control decisions. In [131], a MA2C approach
with a feudal hierarchy concept for TSC is presented. This
approach segments the traffic network into several regions,
each monitored by a manager agent, with traffic signal agents
acting as workers. The managers are responsible for high-level
coordination and setting objectives for the workers, who then
adjust traffic signals to meet these objectives. This hierarchical
structure fosters global coordination while ensuring the system
scalability.
Efficient communication is crucial in applying MARL to
TSC as it enables coordinated decision-making, optimal resource allocation, adaptation to dynamic environments, scalability, conflict resolution, and efficient learning, all of which
contribute to effective traffic management [132]. Liu et al.
present a novel algorithm that enhances communication efficiency through a new message exchange method and improves
congestion measurement by introducing a more comprehensive
reward calculation method. Then a clear and simple represen-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

12

Fig. 8. Three-dimensional cooperation scenarios: (a) Traffic Signal Coordination; (b) Merging on-ramps;(c) Unsignalized intersections.

tation of traffic conditions is provided, and the synchronization
between agents at different intersections with varying cycle
lengths needs to be addressed [133].
Graph-based MARL offers advantages in flexible representation, scalability, efficient message passing, a global view,
interpretable representations, and adaptability, making it suitable for various multi-agent scenarios, including TSC problems [134]. In [135], the authors present CoLight, a model
leveraging graph-attentional MARL networks. This model
facilitates communication among traffic signals, incorporating
both temporal and spatial data from neighboring intersections to a target intersection. Crucially, it achieves indexfree modeling of these neighboring intersections, enhancing
its adaptability and efficiency. In [136], Yang et al. propose
an Inductive Heterogeneous Graph Multi-agent Actor-Critic
(IHG-MA) algorithm for controlling traffic signals across
multiple intersections. This algorithm is distinguished by two
main features: Firstly, it utilizes an inductive heterogeneous
graph (IHG) neural network for representation learning, effectively encoding both heterogeneous features and structural
information of nodes and graphs to generate embeddings for
previously unseen nodes and new traffic networks. Secondly,
IHG-MA adopts the MA2C framework for policy learning,
utilizing these embeddings to compute Q-values and policies.
To account for the spatial influence of multi-intersection
traffic lights and the temporal dependency on historical traffic
conditions, Wang et al. introduce a novel spatio-temporal
MARL framework (STMARL) [137]. This framework captures spatio-temporal dependencies among multiple traffic
lights, coordinating their control via a traffic light adjacency
graph. This graph is meticulously constructed to reflect the
spatial structure of intersections and merges historical traffic
data with the prevailing traffic conditions through the use of
a recurrent neural network. Additionally, a GNN model is
designed to represent the relationships among various traffic
lights, drawing on traffic information that varies over time.
Considering the importance of shared information, relying
on heuristic methods. Jiang et al. propose a new communication form called UniComm, which incorporates numerous
observations gathered at one intersection and predicts their
impact on neighboring intersections, enhancing communica-

tion efficiency [138]. Antonio et al. leverage the multi-agent
TD3 algorithm by incorporating an LSTM to address the
issue of varying observation shapes based on the number of
vehicles, and then employ a training method called curriculum
through self-play to enable collaborative control of CAVs at
intersections [139].
2) On-ramps merging: Highway on-ramp merging task
requires the seamless integration of on-ramp CAVs into the
main traffic flow without causing collisions (see Fig. 8 (b)).
This complex maneuver necessitates that CAVs traveling in
the through lane proactively modify their velocities—slowing
down or speeding up as necessary—to create sufficient room
for CAVs on the on-ramp to merge safely. Concurrently, CAVs
on the ramp must regulate their velocities and make lane
changes promptly in a timely manner when conditions are
safe, thereby avoiding potential deadlock situations. This dual
adjustment ensures a smooth and efficient merging process,
critical for maintaining traffic flow and safety on highways
[32], [140].
In [141], a simplified mathematical formulation for an onramp merging scenario is presented to model the fundamental
interactions and solved with a MADQN algorithm that accounts for the interaction between an on-ramp merging vehicle
and a traffic vehicle in the target lane. The results indicate that
a multi-agent approach can result in reduced collision rates
compared to a single-agent approach, but this improvement is
contingent on the optimality of the traffic vehicle’s behavior
in the target lane. Zhou et al. introduce a cooperative merging
control strategy for CAVs using a distributed MADDPG
approach, which accounts for safe merging distances, acceleration limits, and factors like rear-end safety, lateral safety,
and energy consumption [142]. To address the challenge of a
dynamic environment resulting from the decentralized learning
of CAVs, a decentralized framework employing MADDPG is
presented to coordinate CAVs during highway merging [143].
This framework enables policies learned from a small group of
trained CAVs to be transferred and applied to any number of
CAVs, with a reward function that promotes high-speed travel,
resulting in smoother traffic flow while maintaining safety in
terms of rear-end and lateral collisions.
Some studies also examine the influence of human drivers

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

in on-ramp merging scenarios. For instance, Hu et al. introduce
interaction-aware decision-making with an adaptive strategies
(IDAS) approach, which utilizes a modified MA2C method
and curriculum learning to train a single policy that enables
an AV to navigate the road while taking into account the
cooperativeness of other drivers [144]. In [32], the authors
introduce the MA2C approach, which allows CAVs on both
the merge lane and through lane to cooperatively develop
a policy. This approach focuses on optimizing traffic flow
while also adjusting to the dynamics of HDVs. Their efficient
and scalable MARL framework accommodates dynamic traffic
scenarios with varying communication topologies, utilizing parameter sharing, local rewards, and action masking to promote
inter-agent cooperation and scalability. Furthermore, Chen et
al. introduce a priority-based safety supervisor designed to
minimize collision rates and accelerate the training process.
Similarly, a game-theoretic multi-agent planning approach
called GAMEPLAN is introduced in [28], which incorporates
both human drivers and CAVs in merging scenarios. This
method leverages game theory and an auction mechanism
to compute optimal actions for each agent, considering their
driving style inferred from commonly available sensors. The
algorithm assigns higher priority to more aggressive or impatient drivers and lower priority to more conservative or patient
drivers, ensuring game-theoretic optimality while preventing
collisions and deadlocks.
3) Unsignalized intersections: Intersections serve as pivotal nodes and potential bottlenecks within urban road networks. Enhancing traffic efficiency at these junctions is essential in ameliorating overall traffic throughput and alleviating congestion. Nevertheless, the management of vehicular
interactions at unsignalized intersections, as illustrated in
Fig. 8 (b), presents a notably intricate challenge [146]. Many
conventional techniques, such as rule-based [149], planningbased [150], and single-agent RL methods [151], tend to
approach the problem of intersection management as if it were
a single-agent challenge, which can not capture the complex
interactions among CAVs. Recently, MARL has emerged as a
promising and effective tool for managing traffic at unsignalized intersections.
In [146], an advanced collaborative multi-agent double Deep
Q-Network (DQN) framework is proposed for unsigned traffic
management. Their framework incorporates an efficient reward
function that accounts for both the safety and efficiency
of CAVs. Furthermore, this framework offers adaptability,
enabling transfer learning and the reuse of knowledge from
agent policies, particularly when dealing with unfamiliar traffic scenarios. In [152], the authors introduce a multi-agent
delayed-A3C (MAD-A3C) approach, which utilizes continuous, model-free DRL to train a neural network capable of
predicting acceleration and steering angles of other road users
at each time step. Their study illustrates that these CAV
agents can acquire the essential rules for effectively navigating
intersections by comprehending the priorities of other learners
in the environment, all the while ensuring safe driving along
their designated routes.
Yan et al. utilize a multi-agent policy decomposition strategy, which allows for decentralized control using local obser-

13

vations across an arbitrary number of CAVs. Notably, without
the need for reward shaping, the DQN-based method learns to
coordinate CAVs in a way that mimics traffic signal behaviors,
attaining nearly optimal throughput with control over 33% to
50% of the vehicles. Furthermore, through multi-task learning
and transfer learning, they demonstrate the generalizability of
this behavior across different inflow rates and traffic network
sizes [153].
Effective coordination among CAVs is essential to ensure
safe and efficient maneuvers at unsignalized intersections. In
[154], a decentralized MADQN algorithm is adopted and
learned to make decisions for CAVs. To enable effective
coordination among agents, the intent trajectories of other
neighboring agents are incorporated into each agent’s state
space. Furthermore, a decentralized and conflict-free coordination scheme designed for CAVs at unsignalized intersections is
proposed in [155], with the objective of enhancing intersection
management precision. They frame the challenge of safely
guiding multiple vehicles through unsignalized intersections
as a partially observable stochastic game (POSG). To facilitate
collaborative decision-making in a distributed fashion, they
introduce a cooperative multi-agent proximal optimization
algorithm (CMAPPO). In [156], a multi-layer coordination
strategy is proposed for the management of unsignalized
intersections. This architecture comprises two layers: a lowlevel layer that employs a dynamics-based algorithm to control
individual CAVs, and a high-level layer that integrates a twin
delayed deep deterministic policy gradient (TD3) algorithm,
trained in a centralized manner but executed in a decentralized fashion by multiple agents for decision-making. Their
findings reveal a successful training process, with the MATD3
algorithm achieving a remarkable 100% success rate in preventing intersection collisions. To tackle coordination between
adjacent intersections, a multi-agent-based deep reinforcement
learning scheduling (MA-DRLS) algorithm is proposed in
[145]. This approach allows each intersection agent to independently devise an optimal scheduling strategy through
information exchange with other agents. The algorithm employs DQN networks, and the use of fixed Q-targets and experience replay helps improve the neural network’s reliability
during the training process. In [157], a multi-agent joint-action
DDPG (MAJA-DDPG) framework that integrates learning and
sequential optimization is proposed for the management of
unsignalized intersections. The algorithm involves two primary
steps: Initially, a shared policy is learned to determine the order
in which vehicles cross at the intersection based on traffic state
information. Subsequently, vehicle trajectories are optimized
sequentially following the determined crossing order. Notably,
the proposed algorithm learns a shared policy that can be
implemented in a distributed manner.
To address the challenge of limited communication resources, the authors in [158] introduce an algorithm known
as Efficient Communication Method (ECM)-MA2C. This approach aims to maintain coordination performance while making efficient use of constrained communication resources. The
ECM-MA2C algorithm employs a variational auto-encoder
algorithm combined with advanced multi-head attention mechanisms to extract and retain valuable information from neigh-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

14

TABLE III
A N OVERVIEW OF THE PRIMARY WORKS FROM FOUR PERSPECTIVES IN DIFFERENT SETTINGS .
Scenarios

Typical
application

Work

MARL
algorithm

Novelty
• The state space of CAV decomposed into independent

[125], 2019

[128], 2021

MADQN

[129], 2021

CG-Based MAQ Learning
(CGB-MAQL)

[133], 2023

MA-Q
Learning

[141], 2019

MA-Q
Learning

[143], 2022

MADDPG

[142], 2022

Distributed
MADDPG

[32], 2023

MA2C

[145], 2021

MA-Q
Learning

[146], 2022

MADDQN

Traffic
signal
control

3-D
Cooperation
On-ramps
merging

MADQN

Unsignalized
intersections

[147], 2022

QMIX

[148], 2022

QMIX

and coordinated parts
• Dynamic adaptation to mitigate the curse of
dimensionality and nonstationarity

Metrics
• Travel efficiency
• Intersection delay
• Decision quality

• The γ-reward method to extend the Markov chain to

• Asymmetry road

the space-time domain
• The spatial differentiation method to amend the current
reward by recursion
• Agents organized into cooperative groups with each
agent responsible for its region
• A k-nearest-neighbor-based joint state representation
among neighboring agents
• A heuristic training mechanism by terminating poor
behavior strategies
• A new message sending and processing
• A data structure to record the latest and most
valuable message
• A new reward calculation with both queue length and
total waiting time
• Multi-agent simulator proposed to study the interactions
between a single pair of vehicles
• A simplified mathematical formulation to capture the
complexities of merging scenarios
• Transferring the policies through a few learning CAVs
in the training process
• Reward function design at high speeds
• The interference of human-driven Vehicles (HDV)
• Reward function design including rear-end
safety, lateral safety, and energy consumption
• An action masking scheme
• Parameter sharing and local rewards to encourage
inter-agent cooperation
• A novel priority-based safety supervisor
introduced to reduce collision rates
• The central section (CS) and the waiting section (WS)
to introduce a regulation scheme
• Features incorporation such as fixed Q-targets and
experienced replay
• Route-agents introduced to coordinate the actions of
multiple vehicles
• The inclusion of a collision term in the reward function
enables cooperation
• The reuse of knowledge from agents’ policies to handle
unknown traffic scenarios
• End-to-end learning to autonomously learn complex
real-life traffic dynamics
• Curriculum through Self-Play to adapt and learn in new
and complex traffic scenarios
• Network-level improvements and Q value updates using
Temporal Difference (TD) (λ)
• Reward clipping operations to enhance the QMIX

network evaluation
• Attention score
• Training time

boring agents. Experimental results demonstrate that this efficient communication scheme outperforms baseline approaches
in bandwidth-constrained environments, highlighting its effectiveness in achieving improved performance with limited
communication resources.
In [148], the authors have concentrated their efforts on the
application of a value decomposition-based MARL approach,
namely QMIX, for the coordination of multiple CAVs at

• Waiting time
• Road pheromone

• Waiting time
• Reward

• Collision rates
• Average speed

• Traffic flow

smoothness
• Fuel consumption
• Travel time

• Traffic throughput
• Collision rate

• Traffic throughput
• Waiting time

• Safety
• Travel time
• Fuel consumption

• Travel time
• Time lost due

to congestion
• Waiting time
• Average speed
• Collision rate
• Fuelconsumption

unsignalized intersections. To enhance the performance of the
original QMIX framework, several implementation enhancements have been integrated. These improvements encompass
network-level optimizations, Q-value updates using Temporal
Difference (TD) with γ, and reward clipping operations.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

15

Fig. 9. Representative scenarios in different simulators: (a) SUMO [159]; (b) CityFlow [160]; (c) SMARTS [35]; (d) MetaDrive [161]; (e) CARLA [162];
(f) MACAD [163].

D. Simulation platforms
The simulation platforms, which act as the training/learning
environments, play an important role in the performance and
adaptability of the autonomous agents. The learning platforms
provide different challenges for the planning and control of the
autonomous agents by considering different traffic scenarios
and different characters of agents (Fig. 9). These platforms
offer a virtual environment to learn and validate the CAV’s
perception, decision-making, and low-level control systems,
where the cost and the safety of the agents can be optimized.
The traffic ecosystem/simulation platform incorporates realistic models, physics-based simulations, and efficient numerical
solvers, providing a controllable environment to accelerate
the development of CAVs [164]. Comparisons of different
simulation platforms for CAVs are shown in Table IV.
SUMO (Simulation of Urban MObility) is an open-source
traffic simulation package used to model CAVs and their
interactions [159]. It provides tools for simulating traffic scenarios, including vehicle behavior and road networks, enabling
researchers to study and analyze traffic dynamics. SUMO aids
in evaluating traffic management strategies, routing algorithms,
and the impact of CAVs on traffic flow and efficiency. It
is widely used for assessing CAV behavior and optimizing
transportation systems in a virtual environment.
CityFlow, by optimizing data structures and using efficient
algorithms, improves the simulation efficiency and accelerates
the learning tasks, allowing for large-scale road networks and
traffic flow simulations [160]. It supports flexible definitions
of road networks using synthetic and real-world data and
offers a user-friendly interface for reinforcement learning.
CityFlow outperforms SUMO, being over twenty times faster
and capable of city-wide traffic simulations with interactive

rendering. It opens new possibilities for testing machine learning methods in the intelligent transportation domain, serving
as a foundation for transportation studies beyond traffic signal
control.
To mitigate the gaps between learning and reality, learning
platforms with more realistic agent characters are required, and
several simulation platforms considering vehicle dynamics and
realistic sensor data are proposed.
SMARTS (Scalable Multi-Agent RL Training School) is a
dedicated simulation platform for realistic multi-agent interaction in autonomous driving research [35]. SMARTS supports
the training and accumulation of diverse behavior models, enabling the creation of realistic and varied driving interactions.
It offers a user-friendly interface, key features, and benchmark
tasks. The platform is open-source, encouraging research on
multi-agent learning for autonomous driving.
MetaDrive stands out as an efficient and compositional driving simulator, offering infinite scene generation,
lightweight operation, realistic physics simulation, and multiple sensory inputs [161], where a trade-off between the visual
rendering and the physical simulation has been made. With
the ability to generate infinite scenes encompassing diverse
road maps and traffic settings, researchers can explore a
wide range of driving environments, paving the way for the
development of generalizable RL algorithms. These features
make it a valuable tool for researchers in autonomous driving
and robotics, enabling them to conduct impactful studies with
comprehensive realism and flexibility.
CARLA, which is an open-source simulator developed
by the team at Intel Labs, considers more comprehensive
environment and agent variances. It provides a realistic 3D
environment for testing and evaluating autonomous driving

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

algorithms, including support for multi-agent scenarios [162].
With CARLA, researchers and developers can create complex
simulations that closely resemble real-world driving conditions, enabling them to assess and refine the performance
of their autonomous driving systems. Its high-fidelity physics
engine and accurate sensor models allow for realistic perception and control experiments. Furthermore, CARLA offers
extensive customization options, facilitating the creation of
various urban environments, weather conditions, and traffic
scenarios to comprehensively evaluate the robustness and
effectiveness of autonomous driving algorithms.
MACAD (Multi-Agent Connected and Autonomous Driving)platform serves as a high-fidelity simulation environment
for CAVs, utilizing "partially observable Markov games" to
accurately depict the challenges of connected autonomous
driving with realistic assumptions and models. This platform
offers a comprehensive range of CAD simulation settings,
facilitating the exploration and advancement of Deep RL
strategies in integrated sensing, perception, planning, and
control for CAD systems across diverse operational design domains, under realistic multi-agent conditions, as highlighted by
[163]. Furthermore, MACAD categorizes multi-agent learning
scenarios according to task types, agent characteristics, and environmental factors, providing researchers with customizable
options.
More simulators for CAVs have been designed to consider
more detailed characters and versatile scenarios, which provides more tools to mitigate the gaps between the simulation
and reality.
A PCMA (pedestrian crash avoidance mitigation) system
is developed for interdependent decisions of the vehicles and
pedestrians by Trumpp et. al [165]. In the system, an AVpedestrian interaction at an unmarked crosswalk is modeled
using a Markov decision process and deep reinforcement
learning (DRL). Two pedestrian behaviors are considered: a
baseline predefined strategy and an advanced model using
DRL, turning the interaction into a multi-agent problem. The
PCAM systems are assessed based on collision rates and
traffic flow efficiency, focusing on the impact of observation
uncertainty.
Nocturne considers the limited visibility for studying the
multi-agent coordination in the 2D driving simulator, as the
complex partial observablilities of the multi-agent system also
attract the attention of the researchers. It efficiently calculates
visible features, allowing for rapid simulations. Using realworld driving data, tests showed that machine-learning agents
significantly differed from human-like coordination [166].
Generally speaking, the simulation-based evaluations are
simpler to implement, reproduce, and scale compared with
real experiments for learning. However, simulations may not
capture all the challenges associated with an actual deployment. For example, factors such as network delay, vehicle
model discrepancies, computation time, and the necessity
of implementing clock synchronization and fail-safe routines
pose challenges in real-world implementations.

16

IV. D ISCUSSION & F UTURE W ORK
CAVs leveraging MARL algorithms present a promising
frontier in the evolution of transportation. As urban centers
grapple with traffic congestion and road safety, the nuanced
and adaptive behaviors offered by MARL algorithms can
enable CAVs to respond more fluidly to the actions of other
road participants. The dynamism of MARL allows these
vehicles to cooperatively optimize traffic flows, potentially
alleviating congestion and reducing transit times. Nevertheless, the journey to this future is not without its challenges.
Interactions with unpredictable human drivers, managing the
delicate balance between exploration and safety in learning,
and navigating a complex regulatory landscape are just a few
of the hurdles. Yet, with ongoing collaborations among tech
giants, startups, and academic circles, there is a collective push
to overcome these barriers, signaling an optimistic trajectory
for the marriage of MARL and CAVs.
A. Macro-micro energy optimization of CAVs using MARL
Efficient energy management in CAVs is crucial, as it not
only ensures the optimal performance of each vehicle but
also contributes to overall energy conservation [168], [169].
This can be divided into two distinct yet complementary
approaches: microscopic energy optimization, which concentrates on maximizing the efficiency of individual components
within a single vehicle, and macroscopic energy optimization,
which aims to enhance collective energy use across a fleet of
AVs.
Microscopic energy optimization evaluates the performance
of each component, aiming for optimal functioning. Traditional methods, which segregate powertrain, chassis control,
and other controls, overlook the synergistic potential between
these elements, potentially missing out on energy savings.
To overcome this, Hua et al. present a collaborative strategy
between two learning agents within the MADRL framework
[170]. They conduct a sensitivity analysis to harmonize the
settings for two learning agents, identifying an optimal mode
that surpasses single-agent models by saving up to 4% more
energy. Yang et al. develop a multi-objective energy management system (EMS) for hybrid electric vehicles (HEV)
using a blend of game theory and RL within a MARL
framework [171]. Their strategy simultaneously addresses fuel
economy, battery charge maintenance, battery longevity, and
ultracapacitor constraints, considering the engine–generator set
(EGS) and hybrid energy storage system (HESS) as interactive
agents. Shi et al. propose an EMS that uses IQL to sustain
battery charge levels and minimize hydrogen consumption,
thereby optimizing energy for HESS [172]. Microscopic energy optimization enhances component efficiency but risks
overlooking vehicle-wide dynamics. Macroscopic optimization
addresses this by aiming to improve the energy efficiency
across a fleet of autonomous vehicles.
Macroscopic energy optimization has garnered increasing
interest, particularly concerning traffic dynamics and their
impact on energy efficiency. For instance, Wang et al. have
developed a MARL-based strategy for HEVs that synchronizes
powertrain management with car-following behaviors, thereby
reducing energy use while maintaining safe distances [173].

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

17

TABLE IV
C OMPARASIONS BETWEEN SOME TYPICAL CAV SIMULATION PLATFORMS .
Simulators
Hightway-env [167]
Nocturne [166]
SUMO [159]
CityFlow [160]
SMARTS [35]
MetaDrive [161]
CARLA [162]
MACAD [163]

Vehicle
Dynamics
✓
✓
✓
✓

3D
Scenarios
✓
✓
✓

Visual
Rendering
✓
✓

Zhang et al. have designed a MARL method that simultaneously optimizes velocity and energy management for groups
of HEVs, employing Markov games and LSTM networks for
dynamic group understanding and an innovative asynchronous
learning technique for inter-vehicle knowledge sharing [174].
Furthermore, Peng et al. utilize a MADDPG framework with
two distinct agents for adaptive cruise control and energy management, respectively, achieving domain-specific optimization
and enhancing the MADDPG’s effectiveness by more than
10% with an improved prioritized experience replay [175].
Macro-micro energy optimization: The advancement of
CAVs is set to enhance sustainable and efficient mobility,
with macro-micro energy modeling playing a key role in
precise energy consumption analysis, as depicted in Fig. 10.
This approach intertwines macro-level traffic analysis with
micro-level HEV powertrain operations. By doing so, it allows
for adaptive energy management that responds to real-time
traffic conditions and powertrain status, optimizing the use of
the electric motor and battery in various driving scenarios.
Consequently, this integrated optimization strategy not only
improves route efficiency and driving behavior but also leads
to notable energy savings and cost reductions [176].

Fig. 10. Illustration of the macro-micro energy management system control
framework.

B. Communication challenges
Current progress: Effective communication is pivotal in
MARL for CAVs, enabling collaboration, information sharing,
and coordinated decision-making through reliable protocols
[177]. First, the development of advanced communication
protocols tailored specifically for CAVs is a pressing need. Existing research focuses on protocols that enable efficient data

Pedestrians
✓
✓
✓
✓

Light
Weighted
✓
✓
✓
✓
✓
✓
-

Customized
Maps
✓
✓
✓
✓
✓
✓
✓

Data
Importing
✓
✓
✓
✓
✓
✓

sharing [178]. For instance, research endeavors have delved
into the utilization of graph-based MARL algorithms [121],
such as those explored in [134] and [136], to enable efficient
and adaptive message exchange among agents. Second, the
challenge of ensuring robust communication in dynamically
evolving traffic scenarios represents another critical aspect that
deserves meticulous attention [179]. For example, in [158], a
variational auto-encoder algorithm combined with advanced
multi-head attention mechanisms is proposed to extract and
retain valuable information from neighboring agents while
making efficient use of constrained communication resources.
Last but not least, it is worth noting that managing unstable
communication channels, particularly in terms of latency,
emerges as a particularly intriguing consideration in the context of MARL for CAVs [96]. In [96], the double deep Qlearning algorithm is proposed to jointly train the agents to
maximize the sum rate of V2N links while ensuring the desired
packet delivery probability for each V2V link within specified
latency constraints.
Research gaps: While communication in MARL is a wellexplored domain, its adaptation and application within CAVs
present distinctive research gaps, which necessitate further
exploration [132]. For instance, existing works often overlook
pressing real-world concerns in the realm of CAVs, such
as communication costs and the challenges posed by noisy
environments. These critical factors, when ignored, can introduce substantial obstacles to the practical deployment of CAV
technology. MARL algorithms applied in computer science domains are expected to be explored for use in CAV applications
within challenging environments marked by limited bandwidth
[180] and noisy communication channels [181]. Moreover,
the majority of current MARL algorithms in CAVs rely on
Centralized Training and Decentralized Execution (CTDE,
Sec. II), potentially posing a significant privacy threat within
the realm of communication. In the field of single-agent RL,
security and privacy issues have garnered extensive attention
[182]. However, in the context of MARL, and similarly in
the domain of CAVs, these areas have remained relatively
unexplored. This suggests a promising area for future research
that merits exploration.
C. Mixed-traffic challenges
Current progress: One of the formidable challenges posed
by CAVs stems from navigating the intricate dynamics of
mixed traffic, wherein CAVs share the road with various
other road users, including human-driven vehicles, bicycles,
e-scooters, and an array of other modes of transportation

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[117], [183], [184]. This multifaceted coexistence necessitates
advanced MARL algorithms to ensure seamless and safe
interactions among these diverse road users. Current research
into MARL algorithms for CAV applications primarily concentrates on integrating the prediction of human behavior into
the decision-making process [144], [185]. For instance, in
[32], the authors introduce a novel MARL framework with a
priority-based safety supervisor designed to preempt potential
collisions. This is achieved by predicting future trajectories,
encompassing both CAVs and HDVs and then the unsafe
actions generated by MARL agents will be replaced with safe
actions, thereby contributing to the enhancement of safety
in mixed traffic scenarios. However, it is important to note
that human-driven vehicles can introduce an additional layer
of unpredictability into the traffic environment, since mixed
traffic involves a diverse range of road users, presenting unique
challenges for the seamless integration of CAVs into the
existing road ecosystem.
Research gaps: Given the diverse spectrum of road users,
future research endeavors will encompass the development of
real-time, adaptive safety assessment and intervention strategies capable of responding to the dynamic nature of traffic conditions. Another significant aspect entails the establishment of
novel coordination and communication mechanisms between
CAVs and other road users. For instance, as the presence of
e-scooters on the roads continues to surge, there is a growing
imperative to develop efficient methods for coordinating the
interactions between CAVs and e-scooters [186]. Furthermore,
the existing MARL algorithms are typically trained within
specific scenarios, underscoring the necessity for research
aimed at enhancing their capacity to generalize across a wide
range of mixed traffic conditions and environmental contexts.
D. Sim-to-real challenges
Deep MARL has achieved significant breakthroughs in various areas of the CAV domain. Given the constraints associated
with real-world data collection, such as sample inefficiency
and cost considerations, simulation environments have become
invaluable for agent training (see Sec. III-D). Nevertheless, the
disparity between the simulated and real-world settings can
lead to a degradation in policy performance when models are
transitioned to actual CAV scenarios [187]. In certain simulator
platforms, a key objective is to enable the deployment of CAV
agents with minimal disparities between their training data and
real-world experiences [34]. Simultaneously, other research
initiatives are dedicated to enhancing safety, recognizing it as
a primary obstacle to achieving real-world online training for
complex self-driving car agents [32]. Conversely, it is crucial
to explore advanced MARL techniques, including approaches
like multi-agent offline RL [188] that leverage pre-existing
offline data to update policies with online adaptations. Such
future exploration can open new avenues for enhancing the
generalization abilities and performance of CAVs in real-world
scenarios.
V. C ONCLUSION
Recently, MARL has emerged as a focal point of interest
within the realm of CAVs. The capability of MARL to tackle

18

intricate control and coordination challenges in CAVs has
unlocked new horizons for the development of intelligent and
efficient next-generation transportation systems. This review
has undertaken a comprehensive exploration of the applications of MARL in the control of CAVs.
1) The review began with an overview of single-agent
RL techniques and an extensive survey of the diverse
landscape of MARL architectural variants, facilitating a
deeper understanding of applications to both individual
and collective agent behaviors.
2) Our analysis categorized these contributions based
on different degrees of control, ranging from onedimensional to two-dimensional and three-dimensional
collaboration. Within this multifaceted arena, MARL
has exhibited remarkable prowess, demonstrating its
promising performance in various CAV control tasks.
These include cooperative endeavors such as platooning
control, lane-changing maneuvers, on-ramp merging,
traffic signal coordination, and unsignalized intersections, among others.
3) We highlighted significant challenges in designing and
evaluating MARL methods within the CAV. Striking
a balance between performance, scalability, and safety
remains a complex challenge.
This review serves as a crucial resource, aimed at encouraging further research and the practical application of MARL
within the field of CAVs.
R EFERENCES
[1] G. Dimitrakopoulos and P. Demestichas, “Intelligent transportation
systems,” IEEE Vehicular Technology Magazine, vol. 5, no. 1, pp. 77–
84, 2010.
[2] B. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research
on cloud robotics and automation,” IEEE Transactions on automation
science and engineering, vol. 12, no. 2, pp. 398–409, 2015.
[3] S. Proia, R. Carli, G. Cavone, and M. Dotoli, “Control techniques for
safe, ergonomic, and efficient human-robot collaboration in the digital
industry: A survey,” IEEE Transactions on Automation Science and
Engineering, vol. 19, no. 3, pp. 1798–1819, 2021.
[4] J. Bi, X. Zhang, H. Yuan, J. Zhang, and M. Zhou, “A hybrid prediction method for realistic network traffic with temporal convolutional
network and lstm,” IEEE Transactions on Automation Science and
Engineering, vol. 19, no. 3, pp. 1869–1879, 2021.
[5] K. Wu, J. Hu, Z. Ding, and F. Arvin, “Finite-time fault-tolerant
formation control for distributed multi-vehicle networks with bearing
measurements,” IEEE Transactions on Automation Science and Engineering, vol. 21, no. 2, pp. 1346–1357, 2023.
[6] B. Li, W. Zhuang, H. Zhang, H. Sun, H. Liu, J. Zhang, G. Yin,
and B. Chen, “Traffic-aware ecological cruising control for connected
electric vehicle,” IEEE Transactions on Transportation Electrification,
2023.
[7] M. Hua, G. Chen, C. Zong, and L. He, “Research on synchronous
control strategy of steer-by-wire system with dual steering actuator
motors,” International Journal of Vehicle Autonomous Systems, vol. 15,
no. 1, pp. 50–76, 2020.
[8] W. He, C. Xue, X. Yu, Z. Li, and C. Yang, “Admittance-based controller
design for physical human–robot interaction in the constrained task
space,” IEEE Transactions on Automation Science and Engineering,
vol. 17, no. 4, pp. 1937–1949, 2020.
[9] T. Garg and G. Kaur, “A systematic review on intelligent transport
systems,” Journal of Computational and Cognitive Engineering, 2022.
[10] M. Hua, G. Chen, B. Zhang, and Y. Huang, “A hierarchical energy
efficiency optimization control strategy for distributed drive electric
vehicles,” Proceedings of the Institution of Mechanical Engineers, Part
D: Journal of Automobile Engineering, vol. 233, no. 3, pp. 605–621,
2019.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[11] Y. Lin, J. McPhee, and N. L. Azad, “Comparison of deep reinforcement
learning and model predictive control for adaptive cruise control,” IEEE
Transactions on Intelligent Vehicles, vol. 6, no. 2, pp. 221–231, 2020.
[12] A. Katriniok, B. Rosarius, and P. Mähönen, “Fully distributed model
predictive control of connected automated vehicles in intersections:
Theory and vehicle experiments,” IEEE Transactions on Intelligent
Transportation Systems, vol. 23, no. 10, pp. 18 288–18 300, 2022.
[13] D. Chen, K. Zhang, Y. Wang, X. Yin, Z. Li, and D. Filev,
“Communication-efficient decentralized multi-agent reinforcement
learning for cooperative adaptive cruise control,” IEEE Transactions
on Intelligent Vehicles, 2024.
[14] W. Liu, M. Hua, Z. Deng, Z. Meng, Y. Huang, C. Hu, S. Song, L. Gao,
C. Liu, B. Shuai et al., “A systematic survey of control techniques and
applications in connected and automated vehicles,” IEEE Internet of
Things Journal, 2023.
[15] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.
[16] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter, “Learning
quadrupedal locomotion over challenging terrain,” Science robotics,
vol. 5, no. 47, p. eabc5986, 2020.
[17] X. Bai, A. Fielbaum, M. Kronmüller, L. Knoedler, and J. Alonso-Mora,
“Group-based distributed auction algorithms for multi-robot task assignment,” IEEE Transactions on Automation Science and Engineering,
vol. 20, no. 2, pp. 1292–1303, 2022.
[18] X. Wang, D. Ye, L. Zhang, and X. Zhao, “Prescribed performance
tracking control for nonlinear multiagent systems with distributed
observation errors compensation,” IEEE Transactions on Automation
Science and Engineering, 2024.
[19] A. H. Ganesh and B. Xu, “A review of reinforcement learning
based energy management systems for electrified powertrains: Progress,
challenge, and potential solution,” Renewable and Sustainable Energy
Reviews, vol. 154, p. 111833, 2022.
[20] Z. E. Liu, Q. Zhou, Y. Li, S. Shuai, and H. Xu, “Safe deep reinforcement learning-based constrained optimal control scheme for hev energy
management,” IEEE Transactions on Transportation Electrification,
2023.
[21] Z. E. Liu, Y. Li, Q. Zhou, Y. Li, B. Shuai, H. Xu, M. Hua, G. Tan, and
L. Xu, “Deep reinforcement learning based energy management for
heavy duty hev considering discrete-continuous hybrid action space,”
IEEE Transactions on Transportation Electrification, 2024.
[22] D. Chen, L. Jiang, Y. Wang, and Z. Li, “Autonomous driving using
safe reinforcement learning by incorporating a regret-based human
lane-changing decision model,” in 2020 American Control Conference
(ACC). IEEE, 2020, pp. 4355–4361.
[23] H. Shu, T. Liu, X. Mu, and D. Cao, “Driving tasks transfer using deep
reinforcement learning for decision-making of autonomous vehicles in
unsignalized intersection,” IEEE Transactions on Vehicular Technology,
vol. 71, no. 1, pp. 41–52, 2021.
[24] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” in Proceedings of the AAAI conference on
artificial intelligence, vol. 30, no. 1, 2016.
[25] M. Zhou, Y. Yu, and X. Qu, “Development of an efficient driving strategy for connected and automated vehicles at signalized intersections:
A reinforcement learning approach,” IEEE Transactions on Intelligent
Transportation Systems, vol. 21, no. 1, pp. 433–443, 2019.
[26] H. Shi, Y. Zhou, K. Wu, X. Wang, Y. Lin, and B. Ran, “Connected
automated vehicle cooperative control with a deep reinforcement learning approach in a mixed traffic environment,” Transportation Research
Part C: Emerging Technologies, vol. 133, p. 103421, 2021.
[27] X. Qu, Y. Yu, M. Zhou, C.-T. Lin, and X. Wang, “Jointly dampening
traffic oscillations and improving energy consumption with electric,
connected and automated vehicles: a reinforcement learning based
approach,” Applied Energy, vol. 257, p. 114030, 2020.
[28] R. Chandra and D. Manocha, “Gameplan: Game-theoretic multi-agent
planning with human drivers at intersections, roundabouts, and merging,” IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 2676–
2683, 2022.
[29] S. Amrouni, A. Moulin, J. Vann, S. Vyetrenko, T. Balch, and
M. Veloso, “Abides-gym: gym environments for multi-agent discrete
event simulation and application to financial markets,” in Proceedings
of the Second ACM International Conference on AI in Finance, 2021,
pp. 1–9.
[30] J. Yang, J. Ni, M. Xi, J. Wen, and Y. Li, “Intelligent path planning of
underwater robot based on reinforcement learning,” IEEE Transactions

19

on Automation Science and Engineering, vol. 20, no. 3, pp. 1983–1996,
2022.
[31] E. Palacios-Morocho, S. Inca, and J. F. Monserrat, “Enhancing cooperative multi-agent systems with self-advice and near-neighbor priority
collision control,” IEEE Transactions on Intelligent Vehicles, 2023.
[32] D. Chen, M. R. Hajidavalloo, Z. Li, K. Chen, Y. Wang, L. Jiang,
and Y. Wang, “Deep multi-agent reinforcement learning for highway
on-ramp merging in mixed traffic,” IEEE Transactions on Intelligent
Transportation Systems, 2023.
[33] Z. Chen, F. Renda, A. Le Gall, L. Mocellin, M. Bernabei, T. Dangel,
G. Ciuti, M. Cianchetti, and C. Stefanini, “Data-driven methods applied
to soft robot modeling and control: A review,” IEEE Transactions on
Automation Science and Engineering, 2024.
[34] T. Chu, J. Wang, L. Codecà, and Z. Li, “Multi-agent deep reinforcement
learning for large-scale traffic signal control,” IEEE Transactions on
Intelligent Transportation Systems, vol. 21, no. 3, pp. 1086–1095, 2019.
[35] M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao, W. Zhang,
M. Alban, I. Fadakar, Z. Chen et al., “Smarts: Scalable multi-agent
reinforcement learning training school for autonomous driving,” arXiv
preprint arXiv:2010.09776, 2020.
[36] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
of multiagent deep reinforcement learning,” Autonomous Agents and
Multi-Agent Systems, vol. 33, no. 6, pp. 750–797, 2019.
[37] P. Yadav, A. Mishra, and S. Kim, “A comprehensive survey on multiagent reinforcement learning for connected and automated vehicles,”
Sensors, vol. 23, no. 10, p. 4710, 2023.
[38] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement
learning: A survey,” Journal of artificial intelligence research, vol. 4,
pp. 237–285, 1996.
[39] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control through deep reinforcement learning,”
nature, vol. 518, no. 7540, pp. 529–533, 2015.
[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015.
[41] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in International conference on machine learning,
2016, pp. 1928–1937.
[42] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al.,
“Grandmaster level in starcraft ii using multi-agent reinforcement
learning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019.
[43] B. Zhang, X. Lin, Y. Zhu, J. Tian, and Z. Zhu, “Enhancing multiuav reconnaissance and search through double critic ddpg with belief
probability maps,” IEEE Transactions on Intelligent Vehicles, pp. 1–16,
2024.
[44] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,
pp. 279–292, 1992.
[45] T. Chu and U. Kalabić, “Model-based deep reinforcement learning
for cacc in mixed-autonomy vehicle platoon,” in 2019 IEEE 58th
Conference on Decision and Control (CDC). IEEE, 2019, pp. 4079–
4084.
[46] C. Szepesvári, Algorithms for reinforcement learning. Springer Nature,
2022.
[47] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,
D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement
learning,” arXiv preprint arXiv:1312.5602, 2013.
[48] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba, “Hindsight experience replay,” Advances in neural information processing
systems, vol. 30, 2017.
[49] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic
actor,” in International conference on machine learning. PMLR, 2018,
pp. 1861–1870.
[50] T. Chu, S. Chinchali, and S. Katti, “Multi-agent reinforcement
learning for networked system control,” in International Conference
on Learning Representations, 2020. [Online]. Available: https:
//openreview.net/forum?id=Syx7A3NFvH
[51] OpenAI, :, C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak,
C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz,
S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. d. O. Pinto, J. Raiman,
T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang,
F. Wolski, and S. Zhang, “Dota 2 with large scale deep reinforcement
learning,” 2019.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[52] N. Naderializadeh, J. Sydir, M. Simsek, and H. Nikopour, “Resource
management in wireless networks via multi-agent deep reinforcement
learning,” IEEE Transactions on Wireless Communications, 2021.
[53] D. Chen, K. Chen, Z. Li, T. Chu, R. Yao, F. Qiu, and K. Lin, “Powernet: Multi-agent deep reinforcement learning for scalable powergrid
control,” IEEE Transactions on Power Systems, vol. 37, no. 2, pp.
1007–1017, 2021.
[54] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” Handbook of
reinforcement learning and control, pp. 321–384, 2021.
[55] J. Wang, M. Yuan, Y. Li, and Z. Zhao, “Hierarchical attention master–
slave for heterogeneous multi-agent reinforcement learning,” Neural
Networks, pp. 359–368, 2023.
[56] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “MAVEN:
multi-agent variational exploration,” in Advances in Neural Information
Processing Systems, 2019, pp. 7611–7622.
[57] W. Li, S. He, X. Mao, B. Li, C. Qiu, J. Yu, F. Peng, and X. Tan,
“Multi-agent evolution reinforcement learning method for machining
parameters optimization based on bootstrap aggregating graph attention
network simulated environment,” Journal of Manufacturing Systems,
pp. 424–438, 2023.
[58] D. Qiu, J. Wang, Z. Dong, Y. Wang, and G. Strbac, “Mean-field multiagent reinforcement learning for peer-to-peer multi-energy trading,”
IEEE Transactions on Power Systems, pp. 1–13, 2022.
[59] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative agents,” in Proceedings of the tenth international conference on
machine learning, 1993, pp. 330–337.
[60] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement
learning for multiagent systems: A review of challenges, solutions,
and applications,” IEEE transactions on cybernetics, vol. 50, no. 9, pp.
3826–3839, 2020.
[61] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, “Fully decentralized multi-agent reinforcement learning with networked agents,” in
International Conference on Machine Learning. PMLR, 2018, pp.
5872–5881.
[62] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and
T. Graepel, “Value-decomposition networks for cooperative multi-agent
learning based on team reward,” Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–
2087, 2018.
[63] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster,
and S. Whiteson, “QMIX: Monotonic value function factorisation for
deep multi-agent reinforcement learning,” Proceedings of the 35th
International Conference on Machine Learning, pp. 4295–4304, 2018.
[64] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran:
Learning to factorize with transformation for cooperative multi-agent
reinforcement learning,” International Conference on Machine Learning, pp. 5887–5896, 2019.
[65] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix:
Expanding monotonic value function factorisation for deep multi-agent
reinforcement learning,” Advances in neural information processing
systems, pp. 10 199–10 210, 2020.
[66] Y. Zhang, H. Ma, and Y. Wang, “Avd-net: Attention value decomposition network for deep multi-agent reinforcement learning,” 25th
International Conference on Pattern Recognition, pp. 7810–7816, 2021.
[67] S. Liu, W. Liu, W. Chen, G. Tian, J. Chen, Y. Tong, J. Cao, and
Y. Liu, “Learning multi-agent cooperation via considering actions of
teammates,” IEEE Transactions on Neural Networks and Learning
Systems, pp. 1–12, 2023.
[68] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,
and S. Whiteson, “Stabilising experience replay for deep multi-agent
reinforcement learning,” in International conference on machine learning. PMLR, 2017, pp. 1146–1155.
[69] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to
communicate with deep multi-agent reinforcement learning,” Advances
in neural information processing systems, vol. 29, 2016.
[70] R. Wang, X. He, R. Yu, W. Qiu, B. An, and Z. Rabinovich, “Learning
efficient multi-agent communication: An information bottleneck approach,” in International Conference on Machine Learning. PMLR,
2020, pp. 9908–9918.
[71] S. Ahilan and P. Dayan, “Feudal multi-agent hierarchies for cooperative
reinforcement learning,” arXiv preprint arXiv:1901.08492, 2019.
[72] H. Tang, J. Hao, T. Lv, Y. Chen, Z. Zhang, H. Jia, C. Ren, Y. Zheng,
Z. Meng, C. Fan et al., “Hierarchical deep multiagent reinforcement
learning with temporal abstraction,” arXiv preprint arXiv:1809.09332,
2018.

20

[73] J. Yang, I. Borovikov, and H. Zha, “Hierarchical cooperative multiagent reinforcement learning with skill discovery,” arXiv preprint
arXiv:1912.03558, 2019.
[74] T. Wang, T. Gupta, A. Mahajan, B. Peng, S. Whiteson, and C. Zhang,
“Rode: Learning roles to decompose multi-agent tasks,” arXiv preprint
arXiv:2010.01523, 2020.
[75] Z. Xu, Y. Bai, B. Zhang, D. Li, and G. Fan, “Haven: hierarchical
cooperative multi-agent reinforcement learning with dual coordination
mechanism,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 37, no. 10, 2023, pp. 11 735–11 743.
[76] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. A. Ortega,
D. Strouse, J. Z. Leibo, and N. de Freitas, “Intrinsic social motivation
via causal influence in multi-agent rl,” 2018. [Online]. Available:
https://openreview.net/forum?id=B1lG42C9Km
[77] R. Pina, V. De Silva, and C. Artaud, “Discovering causality for
efficient cooperation in multi-agent environments,” arXiv preprint
arXiv:2306.11846, 2023.
[78] B. Liu, Z. Pu, Y. Pan, J. Yi, Y. Liang, and D. Zhang, “Lazy agents: a
new perspective on solving sparse reward problem in multi-agent reinforcement learning,” in International Conference on Machine Learning.
PMLR, 2023, pp. 21 937–21 950.
[79] H. Wang, Y. Yu, and Y. Jiang, “Fully decentralized multiagent communication via causal inference,” IEEE Transactions on Neural Networks
and Learning Systems, 2022.
[80] J. Li, K. Kuang, B. Wang, F. Liu, L. Chen, C. Fan, F. Wu, and J. Xiao,
“Deconfounded value decomposition for multi-agent reinforcement
learning,” in International Conference on Machine Learning. PMLR,
2022, pp. 12 843–12 856.
[81] K. Jiang, W. Liu, Y. Wang, L. Dong, and C. Sun, “Credit assignment in
heterogeneous multi-agent reinforcement learning for fully cooperative
tasks,” Applied Intelligence, pp. 1–18, 2023.
[82] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
“Counterfactual multi-agent policy gradients,” Proceedings of the AAAI
Conference on Artificial Intelligence, pp. 2974–2982, 2018.
[83] D. Guo, L. Tang, X. Zhang, and Y.-C. Liang, “Joint optimization
of handover control and power allocation based on multi-agent deep
reinforcement learning,” IEEE Transactions on Vehicular Technology,
pp. 13 124–13 138, 2020.
[84] Y. Hou, M. Sun, Y. Zeng, Y.-S. Ong, Y. Jin, H. Ge, and Q. Zhang, “A
multi-agent cooperative learning system with evolution of social roles,”
IEEE Transactions on Evolutionary Computation, 2023.
[85] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” Advances in neural information processing systems, vol. 30,
2017.
[86] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
“Counterfactual multi-agent policy gradients,” in Proceedings of the
AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.
[87] P.-L. Bacon, J. Harb, and D. Precup, “The option-critic architecture,” in
Proceedings of the AAAI conference on artificial intelligence, vol. 31,
no. 1, 2017.
[88] J. Harb, P.-L. Bacon, M. Klissarov, and D. Precup, “When waiting is not
an option: Learning options with a deliberation cost,” in Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.
[89] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg,
D. Silver, and K. Kavukcuoglu, “Feudal networks for hierarchical reinforcement learning,” in International Conference on Machine Learning.
PMLR, 2017, pp. 3540–3549.
[90] O. Nachum, S. S. Gu, H. Lee, and S. Levine, “Data-efficient hierarchical reinforcement learning,” Advances in neural information processing
systems, vol. 31, 2018.
[91] S. J. Grimbly, J. Shock, and A. Pretorius, “Causal multi-agent reinforcement learning: Review and open problems,” arXiv preprint
arXiv:2111.06721, 2021.
[92] J. Pearl, “Theoretical impediments to machine learning with seven
sparks from the causal revolution,” arXiv preprint arXiv:1801.04016,
2018.
[93] S. Yang, B. Yang, Z. Zeng, and Z. Kang, “Causal inference multi-agent
reinforcement learning for traffic signal control,” Information Fusion,
vol. 94, pp. 243–256, 2023.
[94] J. Ho and C.-M. Wang, “Human-centered ai using ethical causality and
learning representation for multi-agent deep reinforcement learning,” in
2021 IEEE 2nd International Conference on Human-Machine Systems
(ICHMS). IEEE, 2021, pp. 1–6.
[95] A. Peake, J. McCalmon, B. Raiford, T. Liu, and S. Alqahtani, “Multiagent reinforcement learning for cooperative adaptive cruise control,”

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

in 2020 IEEE 32nd International Conference on Tools with Artificial
Intelligence (ICTAI). IEEE, 2020, pp. 15–22.
[96] H. V. Vu, M. Farzanullah, Z. Liu, D. H. Nguyen, R. Morawski,
and T. Le-Ngoc, “Multi-agent reinforcement learning for channel
assignment and power allocation in platoon-based c-v2x systems,”
arXiv preprint arXiv:2011.04555, 2020.
[97] B. Liu, W. Han, E. Wang, X. Ma, S. Xiong, C. Qiao, and J. Wang,
“An efficient message dissemination scheme for cooperative drivings
via multi-agent hierarchical attention reinforcement learning,” in 2021
IEEE 41st International Conference on Distributed Computing Systems
(ICDCS). IEEE, 2021, pp. 326–336.
[98] B. Li, K. Xie, X. Huang, Y. Wu, and S. Xie, “Deep reinforcement
learning based incentive mechanism design for platoon autonomous
driving with social effect,” IEEE Transactions on Vehicular Technology,
vol. 71, no. 7, pp. 7719–7729, 2022.
[99] H. Shi, D. Chen, N. Zheng, X. Wang, Y. Zhou, and B. Ran, “A deep
reinforcement learning based distributed control strategy for connected
automated vehicles in mixed traffic platoon,” Transportation Research
Part C: Emerging Technologies, vol. 148, p. 104019, 2023.
[100] Y. Hou and P. Graf, “Decentralized cooperative lane changing at
freeway weaving areas using multi-agent deep reinforcement learning,”
arXiv preprint arXiv:2110.08124, 2021.
[101] G. Wang, J. Hu, Z. Li, and L. Li, “Harmonious lane changing
via deep reinforcement learning,” IEEE Transactions on Intelligent
Transportation Systems, vol. 23, no. 5, pp. 4642–4650, 2021.
[102] K. Nagarajan and Z. Yi, “Lane changing using multi-agent dqn,” in
2021 IEEE International Conference on Autonomous Systems (ICAS).
IEEE, 2021, pp. 1–6.
[103] S. Chen, M. Wang, W. Song, Y. Yang, and M. Fu, “Multi-agent
reinforcement learning-based twin-vehicle fair cooperative driving in
dynamic highway scenarios,” in 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2022, pp.
730–736.
[104] J. Zhang, C. Chang, X. Zeng, and L. Li, “Multi-agent drl-based lane
change with right-of-way collaboration awareness,” IEEE Transactions
on Intelligent Transportation Systems, vol. 24, no. 1, pp. 854–869,
2022.
[105] W. Zhou, D. Chen, J. Yan, Z. Li, H. Yin, and W. Ge, “Multi-agent
reinforcement learning for cooperative lane changing of connected and
autonomous vehicles in mixed traffic,” Autonomous Intelligent Systems,
vol. 2, no. 1, p. 5, 2022.
[106] H. Wang, W. Hao, J. So, X. Xiao, Z. Chen, and J. Hu, “A faster
cooperative lane change controller enabled by formulating in spatial
domain,” IEEE Transactions on Intelligent Vehicles, 2023.
[107] S. Tsugawa, S. Jeschke, and S. E. Shladover, “A review of truck platooning projects for energy savings,” IEEE Transactions on Intelligent
Vehicles, vol. 1, no. 1, pp. 68–77, 2016.
[108] B. Liu, Z. Ding, and C. Lv, “Platoon control of connected autonomous
vehicles: A distributed reinforcement learning method by consensus,”
IFAC-PapersOnLine, vol. 53, no. 2, pp. 15 241–15 246, 2020.
[109] S. Feng, Y. Zhang, S. E. Li, Z. Cao, H. X. Liu, and L. Li, “String stability for vehicular platoon control: Definitions and analysis methods,”
Annual Reviews in Control, vol. 47, pp. 81–97, 2019.
[110] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[111] M. Li, Z. Cao, and Z. Li, “A reinforcement learning-based vehicle
platoon control strategy for reducing energy consumption in traffic
oscillations,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 32, no. 12, pp. 5309–5322, 2021.
[112] Y. Xu, K. Zhu, H. Xu, and J. Ji, “Deep reinforcement learning for multiobjective resource allocation in multi-platoon cooperative vehicular
networks,” IEEE Transactions on Wireless Communications, 2023.
[113] M. Parvini, M. R. Javan, N. Mokari, B. Abbasi, and E. A. Jorswieck,
“Aoi-aware resource allocation for platoon-based c-v2x networks via
multi-agent multi-task reinforcement learning,” IEEE Transactions on
Vehicular Technology, 2023.
[114] Z. Wang, G. Wu, and M. J. Barth, “Cooperative eco-driving at
signalized intersections in a partially connected and automated vehicle environment,” IEEE Transactions on Intelligent Transportation
Systems, vol. 21, no. 5, pp. 2029–2038, 2019.
[115] S. Lu, Y. Cai, L. Chen, H. Wang, X. Sun, and H. Gao, “Altruistic
cooperative adaptive cruise control of mixed traffic platoon based on
deep reinforcement learning,” IET Intelligent Transport Systems, 2023.
[116] X. He, H. Yang, Z. Hu, and C. Lv, “Robust lane change decision making for autonomous vehicles: An observation adversarial reinforcement
learning approach,” IEEE Transactions on Intelligent Vehicles, vol. 8,
no. 1, pp. 184–193, 2022.

21

[117] G. Chen, Z. Gao, M. Hua, B. Shuai, and Z. Gao, “Lane change trajectory prediction considering driving style uncertainty for autonomous
vehicles,” Mechanical Systems and Signal Processing, vol. 206, p.
110854, 2024.
[118] S. Chen, M. Wang, W. Song, Y. Yang, and M. Fu, “Multi-agent
reinforcement learning-based decision making for twin-vehicles cooperative driving in stochastic dynamic highway environments,” IEEE
Transactions on Vehicular Technology, 2023.
[119] C. Vishnu, V. Abhinav, D. Roy, C. K. Mohan, and C. S. Babu,
“Improving multi-agent trajectory prediction using traffic states on
interactive driving scenarios,” IEEE Robotics and Automation Letters,
vol. 8, no. 5, pp. 2708–2715, 2023.
[120] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li,
and M. Sun, “Graph neural networks: A review of methods and
applications,” AI open, vol. 1, pp. 57–81, 2020.
[121] S. Chen, J. Dong, P. Ha, Y. Li, and S. Labi, “Graph neural network
and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,” Computer-Aided Civil and Infrastructure
Engineering, vol. 36, no. 7, pp. 838–857, 2021.
[122] P. Y. J. Ha, S. Chen, J. Dong, R. Du, Y. Li, and S. Labi, “Leveraging
the capabilities of connected and autonomous vehicles and multi-agent
reinforcement learning to mitigate highway bottleneck congestion,”
arXiv preprint arXiv:2010.05436, 2020.
[123] S. Han, S. Zhou, J. Wang, L. Pepin, C. Ding, J. Fu, and F. Miao,
“A multi-agent reinforcement learning approach for safe and efficient
behavior planning of connected autonomous vehicles,” arXiv preprint
arXiv:2003.04371, 2020.
[124] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive:
Composing diverse driving scenarios for generalizable reinforcement
learning,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 3461–3475, 2023.
[125] Y. Wu, H. Chen, and F. Zhu, “Dcl-aim: Decentralized coordination
learning of autonomous intersection management for connected and
automated vehicles,” Transportation Research Part C: Emerging Technologies, vol. 103, pp. 246–260, 2019.
[126] T. Tan, F. Bao, Y. Deng, A. Jin, Q. Dai, and J. Wang, “Cooperative
deep reinforcement learning for large-scale traffic grid signal control,”
IEEE transactions on cybernetics, vol. 50, no. 6, pp. 2687–2700, 2019.
[127] E. Van der Pol and F. A. Oliehoek, “Coordinated deep reinforcement
learners for traffic light control,” Proceedings of learning, inference
and control of multi-agent systems (at NIPS 2016), vol. 8, pp. 21–38,
2016.
[128] J. Liu, H. Zhang, Z. Fu, and Y. Wang, “Learning scalable multiagent coordination by spatial differentiation for traffic signal control,”
Engineering Applications of Artificial Intelligence, vol. 100, p. 104165,
2021.
[129] T. Wang, J. Cao, and A. Hussain, “Adaptive traffic signal control
for large-scale scenario with cooperative group-based multi-agent
reinforcement learning,” Transportation research part C: emerging
technologies, vol. 125, p. 103046, 2021.
[130] T. Wu, P. Zhou, K. Liu, Y. Yuan, X. Wang, H. Huang, and D. O. Wu,
“Multi-agent deep reinforcement learning for urban traffic light control
in vehicular networks,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 8, pp. 8243–8256, 2020.
[131] J. Ma and F. Wu, “Feudal multi-agent deep reinforcement learning
for traffic signal control,” in Proceedings of the 19th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS),
2020, pp. 816–824.
[132] C. Zhu, M. Dastani, and S. Wang, “A survey of multi-agent reinforcement learning with communication,” arXiv preprint arXiv:2203.08975,
2022.
[133] D. Liu and L. Li, “A traffic light control method based on multi-agent
deep reinforcement learning algorithm,” Scientific Reports, vol. 13,
no. 1, p. 9396, 2023.
[134] Q. Liu, X. Li, Y. Tang, X. Gao, F. Yang, and Z. Li, “Graph reinforcement learning-based decision-making technology for connected and
autonomous vehicles: Framework, review, and future trends,” Sensors,
vol. 23, no. 19, p. 8229, 2023.
[135] H. Wei, N. Xu, H. Zhang, G. Zheng, X. Zang, C. Chen, W. Zhang,
Y. Zhu, K. Xu, and Z. Li, “Colight: Learning network-level cooperation
for traffic signal control,” in Proceedings of the 28th ACM International
Conference on Information and Knowledge Management, 2019, pp.
1913–1922.
[136] S. Yang, B. Yang, Z. Kang, and L. Deng, “Ihg-ma: Inductive heterogeneous graph multi-agent reinforcement learning for multi-intersection
traffic signal control,” Neural networks, vol. 139, pp. 265–277, 2021.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

[137] Y. Wang, T. Xu, X. Niu, C. Tan, E. Chen, and H. Xiong, “Stmarl: A
spatio-temporal multi-agent reinforcement learning approach for cooperative traffic light control,” IEEE Transactions on Mobile Computing,
vol. 21, no. 6, pp. 2228–2242, 2020.
[138] Q. Jiang, M. Qin, S. Shi, W. Sun, and B. Zheng, “Multi-agent
reinforcement learning for traffic signal control through universal
communication method,” arXiv preprint arXiv:2204.12190, 2022.
[139] G.-P. Antonio and C. Maria-Dolores, “Multi-agent deep reinforcement
learning to manage connected autonomous vehicles at tomorrow’s
intersections,” IEEE Transactions on Vehicular Technology, vol. 71,
no. 7, pp. 7033–7043, 2022.
[140] Y. Yan, L. Peng, T. Shen, J. Wang, D. Pi, D. Cao, and G. Yin, “A multivehicle game-theoretic framework for decision making and planning of
autonomous vehicles in mixed traffic,” IEEE Transactions on Intelligent
Vehicles, 2023.
[141] L. Schester and L. E. Ortiz, “Longitudinal position control for highway
on-ramp merging: A multi-agent approach to automated driving,”
in 2019 IEEE Intelligent Transportation Systems Conference (ITSC).
IEEE, 2019, pp. 3461–3468.
[142] S. Zhou, W. Zhuang, G. Yin, H. Liu, and C. Qiu, “Cooperative onramp merging control of connected and automated vehicles: Distributed
multi-agent deep reinforcement learning approach,” in 2022 IEEE 25th
International Conference on Intelligent Transportation Systems (ITSC).
IEEE, 2022, pp. 402–408.
[143] S. K. S. Nakka, B. Chalaki, and A. A. Malikopoulos, “A multi-agent
deep reinforcement learning coordination framework for connected and
automated vehicles at merging roadways,” in 2022 American Control
Conference (ACC). IEEE, 2022, pp. 3297–3302.
[144] Y. Hu, A. Nakhaei, M. Tomizuka, and K. Fujimura, “Interaction-aware
decision making with adaptive strategies under merging scenarios,” in
2019 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). IEEE, 2019, pp. 151–158.
[145] Y. Xu, H. Zhou, T. Ma, J. Zhao, B. Qian, and X. Shen, “Leveraging
multiagent learning for automated vehicles scheduling at nonsignalized
intersections,” IEEE Internet of Things Journal, vol. 8, no. 14, pp.
11 427–11 439, 2021.
[146] C. Spatharis and K. Blekas, “Multiagent reinforcement learning for
autonomous driving in traffic zones with unsignalized intersections,”
Journal of Intelligent Transportation Systems, pp. 1–17, 2022.
[147] G.-P. Antonio and C. Maria-Dolores, “Multi-agent deep reinforcement
learning to manage connected autonomous vehicles at tomorrow’s
intersections,” IEEE Transactions on Vehicular Technology, vol. 71,
no. 7, pp. 7033–7043, 2022.
[148] Z. Guo, Y. Wu, L. Wang, and J. Zhang, “Coordination for connected and automated vehicles at non-signalized intersections: A
value decomposition-based multiagent deep reinforcement learning
approach,” IEEE Transactions on Vehicular Technology, vol. 72, no. 3,
pp. 3025–3034, 2022.
[149] A. Aksjonov and V. Kyrki, “Rule-based decision-making system for
autonomous vehicles at intersections with mixed traffic environment,”
in 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE, 2021, pp. 660–666.
[150] Y. Gu, Y. Hashimoto, L.-T. Hsu, and S. Kamijo, “Motion planning
based on learning models of pedestrian and driver behaviors,” in
2016 IEEE 19th International Conference on Intelligent Transportation
Systems (ITSC). IEEE, 2016, pp. 808–813.
[151] C. Huang, J. Zhao, H. Zhou, H. Zhang, X. Zhang, and C. Ye, “Multiagent decision-making at unsignalized intersections with reinforcement
learning from demonstrations,” in 2023 IEEE Intelligent Vehicles
Symposium (IV), 2023, pp. 1–6.
[152] A. P. Capasso, P. Maramotti, A. Dell’Eva, and A. Broggi, “End-to-end
intersection handling using multi-agent deep reinforcement learning,”
in 2021 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2021, pp.
443–450.
[153] Z. Yan and C. Wu, “Reinforcement learning for mixed autonomy
intersections,” in 2021 IEEE International Intelligent Transportation
Systems Conference (ITSC). IEEE, 2021, pp. 2089–2094.
[154] C. Mavrogiannis, J. A. DeCastro, and S. S. Srinivasa, “Implicit multiagent coordination at unsignalized intersections via multimodal inference enabled by topological braids,” arXiv preprint arXiv:2004.05205,
2020.
[155] J. Zheng, K. Zhu, and R. Wang, “Deep reinforcement learning for
autonomous vehicles collaboration at unsignalized intersections,” in
GLOBECOM 2022-2022 IEEE Global Communications Conference.
IEEE, 2022, pp. 1115–1120.
[156] A. H. Hamouda, D. M. Mahfouz, C. M. Elias, and O. M. Shehata, “Multi-layer control architecture for unsignalized intersection

22

management via nonlinear mpc and deep reinforcement learning,” in
2021 IEEE International Intelligent Transportation Systems Conference
(ITSC). IEEE, 2021, pp. 1990–1996.
[157] P. Tallapragada et al., “Reinforcement learning aided sequential optimization for unsignalized intersection management of robot traffic,”
arXiv preprint arXiv:2302.05082, 2023.
[158] Z. Li, Q. Yuan, G. Luo, and J. Li, “Learning effective multi-vehicle
cooperation at unsignalized intersection via bandwidth-constrained
communication,” in 2021 IEEE 94th Vehicular Technology Conference
(VTC2021-Fall). IEEE, 2021, pp. 1–7.
[159] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd,
R. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. Wießner,
“Microscopic traffic simulation using sumo,” in 2018 21st international
conference on intelligent transportation systems (ITSC). IEEE, 2018,
pp. 2575–2582.
[160] H. Zhang, S. Feng, C. Liu, Y. Ding, Y. Zhu, Z. Zhou, W. Zhang, Y. Yu,
H. Jin, and Z. Li, “Cityflow: A multi-agent reinforcement learning
environment for large scale city traffic scenario,” in The world wide
web conference, 2019, pp. 3620–3624.
[161] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive:
Composing diverse driving scenarios for generalizable reinforcement
learning,” IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 3, pp. 3461–3475, 2022.
[162] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
An open urban driving simulator,” in Conference on robot learning.
PMLR, 2017, pp. 1–16.
[163] P. Palanisamy, “Multi-agent connected autonomous driving using deep
reinforcement learning,” in 2020 International Joint Conference on
Neural Networks (IJCNN). IEEE, 2020, pp. 1–7.
[164] S. Chen, Y. Chen, S. Zhang, and N. Zheng, “A novel integrated
simulation and testing platform for self-driving cars with hardware in
the loop,” IEEE Transactions on Intelligent Vehicles, vol. 4, no. 3, pp.
425–436, 2019.
[165] R. Trumpp, H. Bayerlein, and D. Gesbert, “Modeling interactions of autonomous vehicles and pedestrians with deep multi-agent reinforcement
learning for collision avoidance,” in 2022 IEEE Intelligent Vehicles
Symposium (IV). IEEE, 2022, pp. 331–336.
[166] E. Vinitsky, N. Lichtlé, X. Yang, B. Amos, and J. Foerster, “Nocturne:
a scalable driving benchmark for bringing multi-agent learning one step
closer to the real world,” Advances in Neural Information Processing
Systems, vol. 35, pp. 3962–3974, 2022.
[167] E. Leurent, “An environment for autonomous driving decision-making,”
https://github.com/eleurent/highway-env, 2018.
[168] J. Han, A. Sciarretta, L. L. Ojeda, G. De Nunzio, and L. Thibault,
“Safe-and eco-driving control for connected and automated electric
vehicles using analytical state-constrained optimal solution,” IEEE
Transactions on Intelligent Vehicles, vol. 3, no. 2, pp. 163–172, 2018.
[169] J. Rios-Torres and A. A. Malikopoulos, “Impact of partial penetrations
of connected and automated vehicles on fuel consumption and traffic
flow,” IEEE Transactions on Intelligent Vehicles, vol. 3, no. 4, pp. 453–
462, 2018.
[170] M. Hua, C. Zhang, F. Zhang, Z. Li, X. Yu, H. Xu, and Q. Zhou,
“Energy management of multi-mode plug-in hybrid electric vehicle
using multi-agent deep reinforcement learning,” Applied Energy, vol.
348, p. 121526, 2023.
[171] N. Yang, L. Han, R. Liu, Z. Wei, H. Liu, and C. Xiang, “Multiobjective intelligent energy management for hybrid electric vehicles
based on multi-agent reinforcement learning,” IEEE Transactions on
Transportation Electrification, 2023.
[172] W. Shi, Y. Huangfu, L. Xu, and S. Pang, “Online energy management
strategy considering fuel cell fault for multi-stack fuel cell hybrid
vehicle based on multi-agent reinforcement learning,” Applied Energy,
vol. 328, p. 120234, 2022.
[173] Y. Wang, Y. Wu, Y. Tang, Q. Li, and H. He, “Cooperative energy
management and eco-driving of plug-in hybrid electric vehicle via
multi-agent reinforcement learning,” Applied Energy, vol. 332, p.
120563, 2023.
[174] H. Zhang, J. Peng, H. Dong, F. Ding, and H. Tan, “Integrated velocity
optimization and energy management strategy for hybrid electric vehicle platoon: A multi-agent reinforcement learning approach,” IEEE
Transactions on Transportation Electrification, 2023.
[175] J. Peng, W. Chen, Y. Fan, H. He, Z. Wei, and C. Ma, “Ecological
driving framework of hybrid electric vehicle based on heterogeneous
multi agent deep reinforcement learning,” IEEE Transactions on Transportation Electrification, 2023.
[176] W.-L. Shang, M. Zhang, G. Wu, L. Yang, S. Fang, and W. Ochieng,
“Estimation of traffic energy consumption based on macro-micro

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

modelling with sparse data from connected and automated vehicles,”
Applied Energy, vol. 351, p. 121916, 2023.
[177] M. H. Rahman, M. Abdel-Aty, and Y. Wu, “A multi-vehicle communication system to assess the safety and mobility of connected
and automated vehicles,” Transportation research part C: emerging
technologies, vol. 124, p. 102887, 2021.
[178] M. Hasan, S. Mohan, T. Shimizu, and H. Lu, “Securing vehicle-toeverything (v2x) communication platforms,” IEEE Transactions on
Intelligent Vehicles, vol. 5, no. 4, pp. 693–713, 2020.
[179] R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, “Robustness and
adaptability of reinforcement learning-based cooperative autonomous
driving in mixed-autonomy traffic,” IEEE Open Journal of Intelligent
Transportation Systems, vol. 3, pp. 397–410, 2022.
[180] R. Wang, X. He, R. Yu, W. Qiu, B. An, and Z. Rabinovich, “Learning
efficient multi-agent communication: An information bottleneck approach,” in International Conference on Machine Learning. PMLR,
2020, pp. 9908–9918.
[181] B. Freed, G. Sartoretti, J. Hu, and H. Choset, “Communication learning
via backpropagation in discrete channels with unknown noise,” in
Proceedings of the AAAI conference on artificial intelligence, vol. 34,
no. 05, 2020, pp. 7160–7168.
[182] Y. Lei, D. Ye, S. Shen, Y. Sui, T. Zhu, and W. Zhou, “New challenges
in reinforcement learning: a survey of security and privacy,” Artificial
Intelligence Review, vol. 56, no. 7, pp. 7195–7236, 2023.
[183] R. Valiente, B. Toghi, M. Razzaghpour, R. Pedarsani, and Y. P. Fallah,
“Learning-based social coordination to improve safety and robustness
of cooperative autonomous vehicles in mixed traffic,” in Machine
Learning and Optimization Techniques for Automotive Cyber-Physical
Systems. Springer, 2023, pp. 671–707.
[184] S. Zhao, G. Chen, M. Hua, and C. Zong, “An identification algorithm
of driver steering characteristics based on backpropagation neural
network,” Proceedings of the Institution of Mechanical Engineers, Part
D: Journal of Automobile Engineering, vol. 233, no. 9, pp. 2333–2342,
2019.
[185] C. Dai, C. Zong, D. Zhang, M. Hua, H. Zheng, and K. Chuyo, “A bargaining game-based human–machine shared driving control authority
allocation strategy,” IEEE Transactions on Intelligent Transportation
Systems, 2023.
[186] S. Gilroy, D. Mullins, E. Jones, A. Parsi, and M. Glavin, “E-scooter
rider detection and classification in dense urban environments,” Results
in Engineering, vol. 16, p. 100677, 2022.
[187] W. Zhao, J. P. Queralta, and T. Westerlund, “Sim-to-real transfer in deep
reinforcement learning for robotics: a survey,” in 2020 IEEE symposium
series on computational intelligence (SSCI). IEEE, 2020, pp. 737–744.
[188] Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang, J. Yang, and
Q. Zhao, “Believe what you see: Implicit constraint approach for offline
multi-agent reinforcement learning,” Advances in Neural Information
Processing Systems, vol. 34, pp. 10 299–10 312, 2021.

23

