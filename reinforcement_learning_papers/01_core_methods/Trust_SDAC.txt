Trust Region-Based Safe Distributional
Reinforcement Learning for Multiple Constraints

arXiv:2301.10923v2 [cs.LG] 24 Dec 2023

1

Dohyeong Kim1 , Kyungjae Lee2 , and Songhwai Oh1
Dep. of Electrical and Computer Engineering and ASRI, Seoul National University
2
Artificial Intelligence Graduate School, Chung-Ang University
dohyeong.kim@rllab.snu.ac.kr, kyungjae.lee@ai.cau.ac.kr,
songhwai@snu.ac.kr

Abstract
In safety-critical robotic tasks, potential failures must be reduced, and multiple
constraints must be met, such as avoiding collisions, limiting energy consumption,
and maintaining balance. Thus, applying safe reinforcement learning (RL) in such
robotic tasks requires to handle multiple constraints and use risk-averse constraints
rather than risk-neutral constraints. To this end, we propose a trust region-based
safe RL algorithm for multiple constraints called a safe distributional actor-critic
(SDAC). Our main contributions are as follows: 1) introducing a gradient
integration method to manage infeasibility issues in multi-constrained problems,
ensuring theoretical convergence, and 2) developing a TD(λ) target distribution
to estimate risk-averse constraints with low biases. We evaluate SDAC through
extensive experiments involving multi- and single-constrained robotic tasks.
While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all
constraints in multi-constrained tasks and 1.78 times fewer constraint violations
in single-constrained tasks compared to safe RL baselines. Code is available at:
https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.

1

Introduction

Deep reinforcement learning (RL) enables reliable control of complex robots [Merel et al., 2020, Peng
et al., 2021, Rudin et al., 2022]. In order to successfully apply RL to real-world systems, it is essential
to design a proper reward function which reflects safety guidelines, such as collision avoidance and
limited energy consumption, as well as the goal of the given task. However, finding the reward
function that considers all such factors involves a cumbersome and time-consuming process since
RL algorithms must be repeatedly performed to verify the results of the designed reward function.
Instead, safe RL, which handles safety guidelines as constraints, is an appropriate solution. A safe
RL problem can be formulated using a constrained Markov decision process [Altman, 1999], where
not only the reward but also cost functions are defined to provide the safety guideline signals. By
defining constraints using expectation or risk measures of the sum of costs, safe RL aims to maximize
returns while satisfying the constraints. Under the safe RL framework, the training process becomes
straightforward since there is no need to search for a reward that reflects the safety guidelines.
While various safe RL algorithms have been proposed to deal with the safety guidelines, their
applicability to general robotic applications remains limited due to the insufficiency in 1) handling
multiple constraints and 2) minimizing failures, such as robot breakdowns after collisions. First, many
safety-critical applications require multiple constraints, such as maintaining distance from obstacles,
limiting operational space, and preventing falls. Lagrange-based safe RL methods [Yang et al., 2021,
Zhang and Weng, 2022, Bai et al., 2022], which convert a safe RL problem into a dual problem and
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

update the policy and Lagrange multipliers, are commonly used to solve these multi-constrained
problems. However, the Lagrangian methods are difficult to guarantee satisfying constraints during
training theoretically, and the training process can be unstable due to the multipliers [Stooke et al.,
2020]. To this end, trust region-based methods [Yang et al., 2020, Kim and Oh, 2022a], which can
ensure to improve returns while satisfying the constraints under tabular settings [Achiam et al., 2017],
have been proposed as an alternative to stabilize the training process. Still, trust region-based methods
have a critical issue. Depending on the initial policy settings, there can be an infeasible starting case,
meaning that no policy within the trust region satisfies constraints. To address this issue, we can
sequentially select a violated constraint and update the policy to reduce the selected constraint [Xu
et al., 2021]. However, this can be inefficient as only one constraint is considered per update. It will
be better to handle multiple constraints at once, but it is a remaining problem to find a policy gradient
that reflects several constraints and guarantees to reach a feasible policy set.
Secondly, as RL settings are inherently stochastic, employing risk-neutral measures like expectation
to define constraints can lead to frequent failures. Hence, it is crucial to define constraints using
risk measures, such as conditional value at risk (CVaR), as they can reduce the potential for massive
cost returns by emphasizing tail distributions [Yang et al., 2021, Kim and Oh, 2022a]. In safe RL,
critics are used to estimate the constraint values. Especially, to estimate constraints based on risk
measures, it is required to use distributional critics [Dabney et al., 2018b], which can be trained
using the distributional Bellman update [Bellemare et al., 2017]. However, the Bellman update only
considers the one-step temporal difference, which can induce a large bias. The estimation bias makes
it difficult for critics to judge the policy, which can lead to the policy becoming overly conservative
or risky, as shown in Section 5.3. In particular, when there are multiple constraints, the likelihood of
deriving incorrect policy gradients due to estimation errors grows exponentially. Therefore, there is a
need for a method that can train distributional critics with low biases.
In this paper, we propose a trust region-based safe RL algorithm called a safe distributional actor-critic
(SDAC), designed to effectively manage multiple constraints and estimate risk-averse constraints with
low biases. First, to handle the infeasible starting case by considering all constraints simultaneously,
we propose a gradient integration method that projects unsafe policies into a feasible policy set by
solving a quadratic program (QP) consisting of gradients of all constraints. It guarantees to obtain a
feasible policy within a finite time under mild technical assumptions, and we experimentally show
that it can restore the policy more stably than the existing method [Xu et al., 2021]. Furthermore,
by updating the policy using the trust region method with the integrated gradient, our approach
makes the training process more stable than the Lagrangian method, as demonstrated in Section 5.2.
Second, to train critics to estimate constraints with low biases, we propose a TD(λ) target distribution
which can adjust the bias-variance trade-off. The target distribution is obtained by merging the
quantile regression losses [Dabney et al., 2018b] of multi-step distributions and extracting a unified
distribution from the loss. The unified distribution is then projected onto a quantile distribution set
in a memory-efficient manner. We experimentally show that the target distribution can trade off the
bias-variance of the constraint estimations (see Section 5.3).
We conduct extensive experiments with multi-constrained locomotion tasks and single-constrained
Safety Gym tasks [Ray et al., 2019] to evaluate the proposed method. In the locomotion tasks, SDAC
shows 1.93 times fewer steps to satisfy all constraints than the second-best baselines. In the Safety
Gym tasks, the proposed method shows 1.78 times fewer constraint violations than the second-best
methods while achieving high returns when using risk-averse constraints. As a result, it is shown that
the proposed method can efficiently handle multiple constraints using the gradient integration method
and effectively lower the constraint violations using the low-biased distributional critics.

2

Background

Constrained Markov Decision Processes. We formulate the safe RL problem using constrained
Markov decision processes (CMDPs) [Altman, 1999]. A CMDP is defined as (S, A, P , R, C1,..,K ,
ρ, γ), where S is a state space, A is an action space, P : S × A × S 7→ [0, 1] is a transition
model, R : S × A × S 7→ R is a reward function, Ck∈{1,...,K} : S × A × S 7→ R≥0 are cost
functions, ρ : S 7→ [0, 1] is an initial state distribution, and γ ∈ (0, 1) is a discount factor. Given
a policyP
π from a stochastic policy set Π, the discounted state distributionP
is defined as dπ (s) :=
∞
∞
t
π
(1 − γ) t=0 γ Pr(st = s|π), and the return is defined as ZR (s, a) := t=0 γ t R(st , at , st+1 ),
where s0 = s, a0 = a, at ∼ π(·|st ), and st+1 ∼ P (·|st , at ). Then, the state value and state action
2

π
π
value functions are defined as: VRπ (s) := E [ZR
(s, a)|a ∼ π(·|s)] , QπR (s, a) := E [ZR
(s, a)]. By
π
π
substituting the costs for the reward, the cost value functions VCk (s) and QCk (s, a) are defined. In
the remainder of the paper, the cost parts will be omitted since they can be retrieved by replacing the
reward with the costs. Then, the safe RL problem is defined as follows with a safety measure F :
π
maximizeπ J(π) s.t. F (ZC
(s, a)|s ∼ ρ, a ∼ π(·|s)) ≤ dk ∀k,
(1)
k
P∞ t
π
where J(π) := E [ZR (s, a)|s ∼ ρ, a ∼ π(·|s)] + βE [ t=0 γ H(π(·|st ))|ρ, π, P ], β is an entropy
coefficient, H is the Shannon entropy, and dk is a threshold of the kth constraint.

Trust-Region Method With a Mean-Std Constraint. Kim and Oh [2022a] have proposed a
trust region-based safe RL method with a risk-averse constraint, called a mean-std constraint. The
definition of the mean-std constraint function is as follows:
F (Z; α) := E[Z] + (ϕ(Φ−1 (α))/α) · Std[Z],
(2)
where α ∈ (0, 1] adjusts the risk level of constraints, Std[Z] is the standard deviation of Z, and ϕ and
Φ are the probability density function and the cumulative distribution function (CDF) of the standard
normal distribution, respectively. In particular, setting α = 1 causes the standard deviation part to be
zero, so the constraint becomes a risk-neutral constraint. Also, the mean-std constraint can effectively
reduce the potential for massive cost returns, as shown in Yang et al. [2021], Kim and Oh [2022a,b].
In order to calculate the mean-std constraint, it is essential to estimate the standard deviation of the
cost return. To this end, Kim and Oh [2022a] define the square value functions:
 π

 π

π
π
SC
(s) := E ZC
(s, a)2 |a ∼ π(·|s) , SC
(s, a) := E ZC
(s, a)2 .
(3)
k
k
k
k
Since Std[Z]2 = E[Z 2 ] − E[Z]2 , the kth constraint can be written as follows:
p
(4)
Fk (π; α) = JCk (π) + (ϕ(Φ−1 (α))/α) · JSk (π) − JCk (π)2 ≤ dk ,
 π

 π

where JCk (π) := E VCk (s)|s ∼ ρ , JSk (π) := E SCk (s)|s ∼ ρ . In order to apply the trust region
method [Schulman et al., 2015], it is necessary to derive surrogate functions for the objective and
constraints. These surrogates can substitute for the objective and constraints within the trust region.
Given a behavioral policy µ and the current policy πold , we denote the surrogates as J µ,πold (π)
and Fkµ,πold (π; α). For the definition and derivation of the surrogates, please refer to Appendix A.7
and [Kim and Oh, 2022a]. Using the surrogates, a policy can be updated by solving the following
subproblem:
maximizeπ′ J µ,π (π ′ ) s.t. DKL (π||π ′ ) ≤ ϵ, Fkµ,π (π ′ , α) ≤ dk ∀k,
(5)
′
′
where DKL (π||π ) := Es∼dµ [DKL (π(·|s)||π (·|s))], DKL is the KL divergence, and ϵ is a trust
region size. This subproblem can be solved through approximation and a line search (see Appendix
A.8). However, it is possible that there is no policy satisfying the constraints of (5). In order to tackle
this issue, the policy must be projected onto a feasible policy set that complies with all constraints,
yet there is a lack of such methods. In light of this issue, we introduce an efficient feasibility handling
method for multi-constrained RL problems.
Distributional Quantile Critic. Dabney et al. [2018b] have proposed a method for approximating the
π
random variable ZR
to follow a quantile distribution. Given a parametric model, θ : S × A 7→ RM ,
π
ZR
can be approximated as ZR,θ , called a distributional quantile critic. The probability density
function of ZR,θ is defined as follows:
XM
Pr(ZR,θ (s, a) = z) :=
δθm (s,a) (z)/M,
(6)
m=1

where M is the number of atoms, θm (s, a) is the mth atom, δ is the Dirac function, and δa (z) := δ(z−
a). The percentile value of the mth atom is denoted by τm (τ0 = 0, τi = i/M ). In distributional RL,
the returns are directly estimated to get value functions, and the target distribution can be calculated
D
from the distributional Bellman operator [Bellemare et al., 2017]: T π ZR (s, a) := R(s, a, s′ ) +
γZR (s′ , a′ ), where s′ ∼ P (·|s, a) and a′ ∼ π(·|s′ ). The above one-step distributional operator can
D Pn−1
be expanded to the n-step one: Tnπ ZR (s0 , a0 ) := t=0 γ t R(st , at , st+1 ) + γ n ZR (sn , an ), where
at ∼ π(·|st ) for t = 1, ..., n. Then, the critic can be trained to minimize the following quantile
regression loss [Dabney et al., 2018b]:
L(θ) =

M
X

h
i
E(s,a)∼D EZ∼T π ZR,θ (s,a) [ρτ̄m (Z − θm (s, a))] , where ρτ (x) = x · (τ − 1x<0 ),
{z
}
m=1 |
τ̄

m (θ )
=:LQR
m

3

(7)

Figure 1: Left: Gradient integration. Each constraint is truncated to be tangent to the trust region indicated by
the ellipse, and the dashed straight lines show the truncated constraints. The solution of (8) is indicated in blue,
pointing to the nearest point in the intersection of the truncated constraints. If the solution crosses the trust region,
parameters are updated by the clipped direction, shown in red. Right: Optimization paths of the proposed and
naive method in a toy example. The description is presented in Appendix A.2. The contour graph represents
the objective of the toy example. The optimization paths exhibit distinct characteristics due to the difference that
the naive method reflects only one constraint and the proposed method considers all constraints at once.
m
D is a replay buffer, τ̄m := (τm−1 + τm )/2, and Lτ̄QR
(θm ) denotes the quantile regression loss for a
single atom. The distributional quantile critic can be plugged into existing actor-critic algorithms
because only the critic modeling part is changed.

3

Proposed Method

The proposed method comprises two key components: 1) a feasibility handling method required for
multi-constrained safe RL problems and 2) a target distribution designed to minimize estimation
bias. This section sequentially presents these components, followed by a detailed explanation of the
proposed method.
3.1

Feasibility Handling For Multiple Constraints

An optimal safe RL policy can be found by iteratively solving the subproblem (5), but the feasible
set of (5) can be empty in the infeasible starting cases. To address the feasibility issue in safe RL
with multiple constraints, one of the violated constraints can be selected, and the policy is updated
to minimize the constraint until the feasible region is not empty [Xu et al., 2021], which is called
a naive approach. However, it may not be easy to quickly reach the feasible condition if only one
constraint at each update step is used to update the policy. Therefore, we propose a feasibility
handling method which reflect all the constraints simultaneously, called a gradient integration method.
The main idea is to get a gradient that reduces the value of violated constraints and keeps unviolated
constraints. To find such a gradient, the following quadratic program (QP) can be formulated by
linearly approximating the constraints:
g ∗ = argmin
g

p
1 T
g Hg s.t. gkT g + ck ≤ 0 ∀k, ck := min( 2ϵgkT H −1 gk , Fk (πψ ; α) − dk + ζ),
2

(8)

where H is the Hessian of KL divergence between the previous policy and the current policy with
parameters ψ, gk is the gradient of the kth constraint, ck is a truncated threshold to make the kth
constraint tangent to the trust region, ϵ is a trust region size, and ζ ∈ R>0 is a slack coefficient.
The reason why we truncate constraints is to make the gradient integration method invariant to the
gradient scale. Otherwise, constraints with larger gradient scales might produce a dominant policy
gradient. Finally, we update the policy parameters using the clipped gradient as follows:
p
ψ ∗ = ψ + min(1, 2ϵ/(g ∗T Hg ∗ ))g ∗ .
(9)
Figure 1 illustrates the proposed gradient integration process. In summary, the policy is updated by
solving (5); if there is no solution to (5), it is updated using the gradient integration method. Then,
the policy can reach the feasibility condition within finite time steps.
Theorem 3.1. Assume that the constraints are differentiable and convex, gradients of the constraints
are L-Lipschitz continuous, eigenvalues of the Hessian are equal or greater than a positive value
R ∈ R>0 , and {ψ|Fk (πψ ; α) + ζ < dk , ∀k} ̸= ∅. Then, there exists E ∈ R>0 such that if
4

0 < ϵ ≤ E and a policy is updated by the proposed gradient integration method, all constraints are
satisfied within finite time steps.
Note that the first two assumptions of Theorem 3.1 are commonly used in multi-task learning [Liu
et al., 2021, Yu et al., 2020, Navon et al., 2022], and the assumption on eigenvalues is used in most
trust region-based RL methods [Schulman et al., 2015, Kim and Oh, 2022a], so the assumptions
in Theorem 3.1 can be considered reasonable. We provide the proof and show the existence of a
solution (8) in Appendix A.1. The provided proof shows that the constant E is proportional to ζ.
This means that the trust region size should be set smaller as ζ decreases. Also, we further analyze
the worst-case time to satisfy all constraints by comparing the gradient integration method and naive
approach in Appendix A.3. In conclusion, if the policy update rule (5) is not feasible, a finite number
of applications of the gradient integration method will make the policy feasible.
3.2

TD(λ) Target Distribution

The mean-std constraints can be estimated using the distributional quantile critics. Since the estimated
constraints obtained from the critics are directly used to update policies in (5), estimating the
constraints with low biases is crucial. In order to reduce the estimation bias of the critics, we propose
a target distribution by capturing that the TD(λ) loss, which is obtained by a weighted sum of several
losses, and the quantile regression loss with a single distribution are identical. A recursive method is
then introduced so that the target distribution can be obtained practically. First, the n-step targets
for the current policy π are estimated as follows, after collecting trajectories (st , at , st+1 , ...) with a
behavioral policy µ:
D

(n)

ẐR (st , at ) := Rt + γRt+1 + · · · + γ n−1 Rt+n−1 + γ n ZR,θ (st+n , ât+n ),

(10)

where Rt = R(st , at , st+1 ), and ât+n ∼ π(·|st+n ). Note that the n-step target controls the biasvariance tradeoff using n. If n is equal to 1, the n-step target is equivalent to the temporal difference
method that has low variance but high bias. On the contrary, if n increases to infinity, it becomes a
Monte Carlo estimation that has high variance but low bias. However, finding proper n is another
cumbersome task. To alleviate this issue, TD(λ) [Sutton, 1988] method considers the discounted sum
of all n-step targets. Similar to TD(λ), we define the TD(λ) loss for the distributional quantile critic
as the discounted sum of all quantile regression losses with n-step targets. Then, the TD(λ) loss for a
single atom is approximated using importance sampling of the sampled n-step targets in (10) as:
m
Lτ̄QR
(θm ) = (1 − λ)

∞
X

h
i
π Z
λi E(st ,at )∼D EZ∼Ti+1
[ρτ̄m (Z − θm (st , at ))]
R,θ (st ,at )

i=0
∞
X

##
" t+i
Y π(aj |sj )
= (1 − λ)
λ E(st ,at )∼D E
ρτ̄ (Z − θm (st , at ))
µ(aj |sj ) m
i=0
j=t+1
" t+i
#
∞
M
X
Y π(aj |sj ) X
1
(i+1)
i
ρτ̄ (Ẑ
(st , at ) − θm (st , at )) ,
≈ (1 − λ)
λ E(st ,at )∼D
µ(aj |sj ) m=1 M m R,m
i=0
j=t+1
"

i

µ
Z∼Ti+1 ZR,θ (st ,at )

(i)

(i)

(i)

(11)

D

where λ is a trace-decay value, and ẐR,m is the mth atom of ẐR . Since ẐR (st , at ) = Rt +
(i−1)

γ ẐR (st+1 , at+1 ) is satisfied, (11) is the same as the quantile regression loss with the following
single distribution Ẑttot , called a TD(λ) target distribution:
Pr(Ẑttot = z) :=

=

=

1−λ
Nt

t+i
∞
M
X
Y
π(aj |sj ) X 1
1
(1 − λ)
λi
δ (i+1)
(z)
Nt
µ(aj |sj ) m=1 M ẐR,m (st ,at )
i=0
j=t+1


t+1+i
∞
M
π(at+1 |st+1 ) X i Y π(aj |sj ) X 1
1
δẐ (1) (z) + λ
λ
δẐ (i+2) (s ,a ) (z)
M t,m
µ(at+1 |st+1 ) i=0 j=t+2 µ(aj |sj ) m=1 M R,m t t
m=1

X
M

(12)

M
X

1−λ
1
Nt+1 π(at+1 |st+1 )
tot
δ (1) (z) +λ
Pr(Rt + γ Ẑt+1
= z),
Nt m=1 M Ẑt,m
Nt µ(at+1 |st+1 ) |
{z
}
(b)
|
{z
}
(a)

where Nt is a normalization factor. If the target for time step t + 1 is obtained, the target distribution
for time step t becomes the weighted sum of (a) the current one-step TD target and (b) the shifted
5

Figure 2: Procedure for constructing the target distribution. First, multiply the target at t + 1 step by γ
and add Rt . Next, weight-combine the shifted previous target and one-step target at t step and restore the CDF
of the combined target. The CDF can be restored by sorting the positions of the atoms and then accumulating
the weights at each atom position. Finally, the projected target can be obtained by finding the positions of the
atoms corresponding to M ′ quantiles in the CDF. Using the projected target, the target at t − 1 step can be found
recursively.

previous target distribution, so it can be obtained recursively, as shown in (12). Since the definition
requires infinite sums, the recursive way is more practical for computing the target. Nevertheless, to
obtain the target in that recursive way, we need to store all quantile positions and weights for all time
steps, which is not memory-efficient. Therefore, we propose to project the target distribution into a
quantile distribution with a specific number of atoms, M ′ (we set M ′ > M to reduce information
loss). The overall process to get the TD(λ) target distribution is illustrated in Figure 2, and the
pseudocode is given in Appendix A.5. Furthermore, we can show that a distribution trained with the
π
proposed target converges to the distribution of ZR
.
Theorem 3.2. Let define a distributional operator Tλµ,π , whose probability density function is:
Pr(Tλµ,π Z(s, a) = z) ∝
"
"
!#
#
∞
i
i
X
X
Y
π(aj |sj )
i
t
i+1
′
Pr
Eµ λ
E
γ Rt +γ Z(si+1 , a ) = z
s0 = s, a0 = a .
µ(aj |sj ) a′ ∼π(·|si+1 )
t=0
i=0
j=1

(13)

π
Then, a sequence, Zk+1 (s, a) = Tλµ,π Zk (s, a) ∀(s, a), converges to ZR
.

The TD(λ) target is a quantile distribution version of the distributional operator Tλµ,π in Theorem 3.2.
Consequently, a distribution updated by minimizing the quantile regression loss with the TD(λ) target
π
converges to the distribution of ZR
if the number of atoms is infinite, according to Theorem 3.2. The
proof of Theorem 3.2 is provided in Appendix A.4. After calculating the target distribution for all
time steps, the critic can be trained to reduce the quantile regression loss with the target distribution.
To provide more insight, we experiment with a toy example in Appendix A.6, and the results show
that the proposed target distribution can trade off bias and variance through the trace-decay λ.
3.3

Safe Distributional Actor-Critic

Finally, we describe the proposed method, safe distributional actor-critic (SDAC). After collecting
trajectories, the policy is updated by solving (5), which can be solved through a line search (for more
detail, see Appendix A.8). The cost value and the cost square value functions in (4) can be obtained
using the distributional critics as follows:
Z ∞
M
1 X
π
π
QC (s, a) =
zPr(ZC
(s, a) = z)dz ≈
θm (s, a),
M m=1
−∞
(14)
Z ∞
M
1 X
π
2
π
2
SC (s, a) =
z Pr(ZC (s, a) = z)dz ≈
θm (s, a) .
M m=1
−∞
If a solution of (5) does not exist, the policy is projected into a feasible region through the proposed
gradient integration method. The critics can also be updated by the regression loss (7) between the
target distribution obtained from (12). The proposed method is summarized in Algorithm 1.

4

Related Work

Safe Reinforcement Learning. Garcıa and Fernández [2015] and Gu et al. [2022] have researched
and categorized safe RL methodologies from various perspectives. In this paper, we introduce safe RL
6

Algorithm 1 Safe Distributional Actor-Critic
π
π
Input: Policy network πψ , reward and cost critic networks ZR,θ
, ZC
, and replay buffer D.
k ,θ
Initialize network parameters ψ, θ, and replay buffer D.
for epochs = 1 to E do
for t = 1 to T do
Sample at ∼ πψ (·|st ) and get st+1 , rt = R(st , at , st+1 ), and ck,t = Ck (st , at , st+1 ) ∀k.
Store (st , at , πψ (at |st ), rt , c{1,...,K},t , st+1 ) in D.
end for
Calculate the TD(λ) target distribution (Section 3.2) using D and update the critics to minimize
the quantile loss defined in (7).
Calculate the surrogates for the objective and constraints defined in (37) using D.
Update the policy by solving (5), but if (5) has no solution, take a recovery step (Section 3.1).
end for

methods depending on how to update policies to reflect safety constraints. First, trust region-based
safe RL methods [Achiam et al., 2017, Yang et al., 2020, Kim and Oh, 2022a] find policy update
directions by approximating the safe RL problem as a linear-quadratic constrained linear program and
update policies through a line search. Yang et al. [2020] also employ projection to meet a constraint;
however, their method is limited to a single constraint and does not show to satisfy the constraint
for the infeasible starting case. Second, Lagrangian-based methods [Stooke et al., 2020, Yang et al.,
2021, Liu et al., 2020] convert the safe RL problem to a dual problem and update the policy and
dual variables simultaneously. Last, expectation-maximization (EM) based methods [Liu et al., 2022,
Zhang et al., 2022] find non-parametric policy distributions by solving the safe RL problem in E-steps
and fit parametric policies to the found non-parametric distributions in M-steps. Also, there are other
ways to reflect safety other than policy updates. Qin et al. [2021], Lee et al. [2022] find optimal state
or state-action distributions that satisfy constraints, and Bharadhwaj et al. [2021], Thananjeyan et al.
[2021] reflect safety during exploration by executing only safe action candidates. In the experiments,
only the safe RL methods of the policy update approach are compared with the proposed method.
Distributional TD(λ). TD(λ) [Precup et al., 2000] can be extended to the distributional critic to
trade off bias-variance. Gruslys et al. [2018] have proposed a method to obtain target distributions by
mixing n-step distributions, but the method is applicable only in discrete action spaces. Nam et al.
[2021] have proposed a method to obtain target distributions using sampling to apply to continuous
action spaces, but this is only for on-policy settings. A method proposed by Tang et al. [2022] updates
the critics using newly defined distributional TD errors rather than target distributions. This method is
applicable for off-policy settings but has the disadvantage that memory usage increases linearly with
the number of TD error steps. In contrast to these methods, the proposed method is memory-efficient
and applicable for continuous action spaces under off-policy settings.
Gradient Integration. The proposed feasibility handling method utilizes a gradient integration
method, which is widely used in multi-task learning (MTL). The gradient integration method finds a
single gradient to improve all tasks by using gradients of all tasks. Yu et al. [2020] have proposed
a projection-based gradient integration method, which is guaranteed to converge Pareto-stationary
sets. A method proposed by Liu et al. [2021] can reflect user preference, and Navon et al. [2022]
proposed a gradient-scale invariant method to prevent the training process from being biased by a few
tasks. The proposed method can be viewed as a mixture of projection and scale-invariant methods as
gradients are clipped and projected onto a trust region.

5

Experiments

We evaluate the safety performance of the proposed method in single- and multi-constrained robotic
tasks. For single constraints, the agent performs four tasks provided by Safety Gym [Ray et al., 2019],
and for multi-constraints, it performs bipedal and quadrupedal locomotion tasks.
5.1

Safety Gym

Tasks. We employ two robots, point and car, to perform goal and button tasks in the Safety Gym.
The goal task is to control a robot toward a randomly spawned goal without passing through hazard
7

(a) Results of the final reward sum, cost rate, and total number of CVs. The number after the algorithm name in
the legend indicates α used for the risk-averse constraint.

(b) Training curves of the point goal task according to the trace-decay λ. The solid line represents the average
value, and the shaded area shows half of the std value.

Figure 3: Safety Gym task results. The cost rates show the cost sums divided by the episode length, and the
dashed black lines indicate the threshold of the constraint. All methods are trained with five random seeds.

regions. The button task is to click a randomly designated button using a robot, where not only hazard
regions but also dynamic obstacles exist. Agents get a cost when touching undesignated buttons and
obstacles or entering hazard regions. There is only one constraint for the Safety Gym tasks, and it is
defined using (4) with the sum of costs. Constraint violations (CVs) are counted when the cost sum
exceeds the threshold. For more details, see Appendix B.
Baselines. Safe RL methods based on various types of policy updates are used as baselines. For the
trust region-based method, we use constrained policy optimization (CPO) [Achiam et al., 2017] and
off-policy trust-region CVaR (OffTRC) [Kim and Oh, 2022a], which extend the CPO to an off-policy
and mean-std constrained version. For the Lagrangian-based method, distributional worst-case soft
actor-critic (WCSAC) [Yang et al., 2022] is used, and constrained variational policy optimization
(CVPO) [Liu et al., 2022] based on the EM method is used. Specifically, WCSAC, OffTRC, and the
proposed method, SDAC, use the risk-averse constraints, so we experiment with those for α = 0.25
and 1.0 (when α = 1.0, the constraint is identical to the risk-neutral constraint).
Results. The graph of the final reward sum, cost rate, and the total number of CVs are shown in
Figure 3a, and the training curves are provided in Appendix C.1. We can interpret the results as good
if the reward sum is high and the cost rate and total CVs are low. SDAC with α = 0.25, risk-averse
constraint situations, satisfies the constraints in all tasks and shows an average of 1.78 times fewer
total CVs than the second-best algorithm. Nevertheless, since the reward sums are also in the middle
or upper ranks, its safety performance is of high quality. SDAC with α = 1.0, risk-neutral constraint
situations, shows that the cost rates are almost the same as the thresholds except for the car button. In
the case of the car button, the constraint is not satisfied, but by setting α = 0.25, SDAC can achieve
the lowest total CVs and the highest reward sum compared to the other methods. As for the reward
sum, SDAC is the highest in the point goal and car button, and WCSAC is the highest in the rest.
However, WCSAC seems to lose the risk-averse properties seeing that the cost rates do not change
significantly according to α. This is because WCSAC does not define constraints as risk measures
of cost returns but as expectations of risk measures [Yang et al., 2022]. OffTRC has lower safety
performance than SDAC in most cases because, unlike SDAC, it does not use distributional critics.
Finally, CVPO and CPO are on-policy methods, so they are less efficient than the other methods.
5.2

Locomotion Tasks

Tasks. The locomotion tasks are to train robots to follow xy-directional linear and z-directional
angular velocity commands. Mini-Cheetah from MIT [Katz et al., 2019] and Laikago from Unitree
[Wang, 2018] are used for quadrupedal robots, and Cassie from Agility Robotics [Xie et al., 2018]
is used for a bipedal robot. In order to perform the locomotion tasks, robots should keep balancing,
standing, and stamping their feet so that they can move in any direction. Therefore, we define three
constraints. The first is to keep the balance so that the body angle does not deviate from zero, and
the second is to keep the height of CoM above a threshold. The third is to match the current foot
8

(a) Training curves of the Cassie task.

(b) Training curves of the Laikago task.

(c) Training curves of the Mini-Cheetah task.

(d) Training curves of the naive and proposed methods for the Cassie task.

Figure 4: Locomotion task results. The black dashed lines indicate the thresholds, and the dotted lines in (d)
represent the thresholds + 0.025. The shaded area represents half of the standard deviation, and all methods are
trained with five random seeds.

contact state with a predefined foot contact timing. The reward is defined as the negative l2 -norm of
the difference between the command and the current velocity. CVs are counted when the sum of at
least one cost rate exceeds the threshold. For more details, see Appendix B.
Baselines. The baseline methods are identical to the Safety Gym tasks, and CVPO is excluded
because it is technically challenging to scale to multiple constraint settings. We set α to 1 for the riskaverse constrained methods (OffTRC, WCSAC, and SDAC) to focus on measuring multi-constraint
handling performance.
Results. Figure 4.(a-c) presents the training curves. SDAC shows the highest reward sums and the
lowest total CVs in all tasks. In particular, the number of steps required to satisfy all constraints is
1.93 times fewer than the second-best algorithm on average. Trust region methods (OffTRC, CPO)
stably satisfy constraints, but they are not efficient since they handle constraints by the naive approach.
WCSAC, a Lagrangian method, fails to keep the constraints and shows the lowest reward sums. This
is because the Lagrange multipliers can hinder the training stability due to the concurrent update with
policy [Stooke et al., 2020].
5.3

Ablation Study

We conduct ablation studies to show whether the proposed target distribution lowers the estimation
bias and whether the proposed gradient integration quickly converges to the feasibility condition.
In Figure 3b, the number of CVs is reduced as λ increases, which means that the bias of constraint
estimation decreases. However, the score also decreases due to large variance, showing that λ can
adjust the bias-variance tradeoff. In Figure 4d, the proposed gradient integration method is compared
with the naive approach, which minimizes the constraints in order from the first to the third constraint,
as described in Section 3.1. The proposed method reaches the feasibility condition faster than the
naive approach and shows stable training curves because it reflects all constraints concurrently.
Additionally, we analyze the distributional critics in Appendix C.2 and the hyperparameters, such
as the trust region size, in Appendix C.3. Furthermore, we analyze the sensitivity of traditional RL
algorithms to reward configuration in Appendix D, emphasizing the advantage of safe RL that does
not require reward tuning.
9

6

Limitation

A limitation of the proposed method is that the computational complexity of the gradient integration
is proportional to the square of the number of constraints, whose qualitative analysis is presented in
Appendix E.1. Also, we conducted quantitative analyses in Appendix E.2 by measuring wall clock
training time. In the mini-cheetah task which has three constraints, the training time of SDAC is the
third fastest among the four safe RL algorithms. Gradient integration is not applied when the policy
satisfies constraints, so it may not constitute a significant proportion of training time. However, its
influence can be dominant as the number of constraints increases. In order to resolve this limitation,
the calculation can be speeded up by stochastically selecting a subset of constraints [Liu et al., 2021],
or by reducing the frequency of policy updates [Navon et al., 2022]. The other limitation is that the
mean-std defined in (2) is not a coherent risk measure. As a result, mean-std constraints can be served
as reducing uncertainty rather than risk, although we experimentally showed that constraint violations
are efficiently reduced. To resolve this, we can use the CVaR constraint, which can be estimated
using an auxiliary variable, as done by Chow et al. [2017]. However, this solution can destabilize the
training process due to the auxiliary variable, as observed in experiments of Kim and Oh [2022b].
Hence, a stabilization technique should be developed to employ the CVaR constraint.

7

Conclusion

We have presented the trust region-based safe distributional RL method, called SDAC. Through the
locomotion tasks, it is verified that the proposed method efficiently satisfies multiple constraints
using the gradient integration. Moreover, constraints can be stably satisfied in various tasks due to
the low-biased distributional critics trained using the proposed target distributions. In addition, the
proposed method is analyzed from multiple perspectives through various ablation studies. However,
to compensate for the computational complexity, future work plans to devise efficient methods when
dealing with large numbers of constraints.

Acknowledgments and Disclosure of Funding
This work was partly supported by Institute of Information & Communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-01190, [SW Star
Lab] Robot Learning: Efficient, Safe, and Socially-Acceptable Machine Learning, 34%), Basic
Science Research Program through the National Research Foundation of Korea (NRF) funded by
the Ministry of Science and ICT (NRF-2022R1A2C2008239, General-Purpose Deep Reinforcement
Learning Using Metaverse for Real World Applications, 33%), and Institute of Information &
Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government
(MSIT) (No. 2021-0-01341, AI Graduate School Program, CAU, 33%).

References
J. Achiam, D. Held, A. Tamar, and P. Abbeel. Constrained policy optimization. In Proceedings of
International Conference on Machine Learning, pages 22–31, 2017.
A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. The Journal of Machine Learning Research, 22
(1):4431–4506, 2021.
E. Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Q. Bai, A. S. Bedi, M. Agarwal, A. Koppel, and V. Aggarwal. Achieving zero constraint violation for
constrained reinforcement learning via primal-dual approach. Proceedings of the AAAI Conference
on Artificial Intelligence, 36(4), 2022.
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
In Proceedings of International Conference on Machine Learning, pages 449–458, 2017.
M. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. MIT Press,
2023. http://www.distributional-rl.org.
10

H. Bharadhwaj, A. Kumar, N. Rhinehart, S. Levine, F. Shkurti, and A. Garg. Conservative safety
critics for exploration. In Proceedings of International Conference on Learning Representations,
2021.
L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/.
Software available from wandb.com.
Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone. Risk-constrained reinforcement learning with
percentile risk criteria. Journal of Machine Learning Research, 18(1):6070–6120, 2017.
W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit quantile networks for distributional
reinforcement learning. In Proceedings of International conference on machine learning, pages
1096–1105, 2018a.
W. Dabney, M. Rowland, M. Bellemare, and R. Munos. Distributional reinforcement learning with
quantile regression. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018b.
J. E. Dennis and R. B. Schnabel. Numerical Methods for Unconstrained Optimization and Nonlinear
Equations. Society for Industrial and Applied Mathematics, 1996.
J. Garcıa and F. Fernández. A comprehensive survey on safe reinforcement learning. Journal of
Machine Learning Research, 16(1):1437–1480, 2015.
A. Gruslys, W. Dabney, M. G. Azar, B. Piot, M. Bellemare, and R. Munos. The reactor: A fast and
sample-efficient actor-critic agent for reinforcement learning. In Proceedings of International
Conference on Learning Representations, 2018.
S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, Y. Yang, and A. Knoll. A review of safe
reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.
T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Proceedings of International Conference on
Machine Learning, pages 1861–1870, 2018.
B. Katz, J. D. Carlo, and S. Kim. Mini cheetah: A platform for pushing the limits of dynamic
quadruped control. In Proceedings of International Conference on Robotics and Automation, pages
6295–6301, 2019.
D. Kim and S. Oh. Efficient off-policy safe reinforcement learning using trust region conditional
value at risk. IEEE Robotics and Automation Letters, 7(3):7644–7651, 2022a.
D. Kim and S. Oh. TRC: Trust region conditional value at risk for safe reinforcement learning. IEEE
Robotics and Automation Letters, 7(2):2621–2628, 2022b.
A. Kuznetsov, P. Shvechikov, A. Grishin, and D. Vetrov. Controlling overestimation bias with
truncated mixture of continuous distributional quantile critics. In Proceedings International
Conference on Machine Learning, pages 5556–5566, 2020.
J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over
challenging terrain. Science Robotics, 5(47):eabc5986, 2020.
J. Lee, C. Paduraru, D. J. Mankowitz, N. Heess, D. Precup, K.-E. Kim, and A. Guez. COptiDICE:
Offline constrained reinforcement learning via stationary distribution correction estimation. In
Proceedings of International Conference on Learning Representations, 2022.
B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu. Conflict-averse gradient descent for multi-task learning.
In Advances in Neural Information Processing Systems, pages 18878–18890, 2021.
Y. Liu, J. Ding, and X. Liu. IPO: Interior-point policy optimization under constraints. Proceedings of
the AAAI Conference on Artificial Intelligence, 34(04):4940–4947, 2020.
Z. Liu, Z. Cen, V. Isenbaev, W. Liu, S. Wu, B. Li, and D. Zhao. Constrained variational policy
optimization for safe reinforcement learning. In Proceedings of International Conference on
Machine Learning, pages 13644–13668, 2022.
11

W. Meng, Q. Zheng, Y. Shi, and G. Pan. An off-policy trust region policy optimization method with
monotonic improvement guarantee for deep reinforcement learning. IEEE Transactions on Neural
Networks and Learning Systems, 33(5):2223–2235, 2022.
J. Merel, S. Tunyasuvunakool, A. Ahuja, Y. Tassa, L. Hasenclever, V. Pham, T. Erez, G. Wayne, and
N. Heess. Catch & carry: Reusable neural controllers for vision-guided whole-body tasks. ACM
Transactions on Graphics, 39(4), 2020.
T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust perceptive
locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022.
D. W. Nam, Y. Kim, and C. Y. Park. GMAC: A distributional perspective on actor-critic framework.
In Proceedings of International Conference on Machine Learning, pages 7927–7936, 2021.
A. Navon, A. Shamsian, I. Achituve, H. Maron, K. Kawaguchi, G. Chechik, and E. Fetaya. Multi-task
learning as a bargaining game. arXiv preprint arXiv:2202.01017, 2022.
X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa. AMP: Adversarial motion priors for
stylized physics-based character control. ACM Transactions on Graphics, 40(4), 2021.
D. Precup, R. S. Sutton, and S. P. Singh. Eligibility traces for off-policy policy evaluation. In
Proceedings of International Conference on Machine Learning, pages 759–766, 2000.
Z. Qin, Y. Chen, and C. Fan. Density constrained reinforcement learning. In Proceedings of
International Conference on Machine Learning, pages 8682–8692, 2021.
A. Ray, J. Achiam, and D. Amodei. Benchmarking Safe Exploration in Deep Reinforcement Learning.
2019.
M. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh. An analysis of categorical
distributional reinforcement learning. In Proceedings of International Conference on Artificial
Intelligence and Statistics, pages 29–37, 2018.
N. Rudin, D. Hoeller, P. Reist, and M. Hutter. Learning to walk in minutes using massively parallel
deep reinforcement learning. In Proceedings of Conference on Robot Learning, pages 91–100,
2022.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
Proceedings of International Conference on Machine Learning, pages 1889–1897, 2015.
A. Stooke, J. Achiam, and P. Abbeel. Responsive safety in reinforcement learning by PID lagrangian
methods. In Proceedings of International Conference on Machine Learning, pages 9133–9143,
2020.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):
9–44, 1988.
Y. Tang, R. Munos, M. Rowland, B. Avila Pires, W. Dabney, and M. Bellemare. The nature of
temporal difference errors in multi-step distributional reinforcement learning. In Advances in
Neural Information Processing Systems, pages 30265–30276, 2022.
B. Thananjeyan, A. Balakrishna, S. Nair, M. Luo, K. Srinivasan, M. Hwang, J. E. Gonzalez, J. Ibarz,
C. Finn, and K. Goldberg. Recovery RL: Safe reinforcement learning with learned recovery zones.
IEEE Robotics and Automation Letters, 6(3):4915–4922, 2021.
E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In Proceedings
of International Conference on Intelligent Robots and Systems, pages 5026–5033, 2012.
X. Wang. Unitree-Laikago Pro. http://www.unitree.cc/e/action/ShowInfo.php?classid=
6&id=355, 2018.
Z. Xie, G. Berseth, P. Clary, J. Hurst, and M. van de Panne. Feedback control for cassie with deep
reinforcement learning. In Proceedings of International Conference on Intelligent Robots and
Systems, pages 1241–1246, 2018.
12

T. Xu, Y. Liang, and G. Lan. CRPO: A new approach for safe reinforcement learning with convergence
guarantee. In Proceedings of International Conference on Machine Learning, pages 11480–11491,
2021.
Q. Yang, T. D. Simão, S. H. Tindemans, and M. T. J. Spaan. WCSAC: Worst-case soft actor critic
for safety-constrained reinforcement learning. Proceedings of the AAAI Conference on Artificial
Intelligence, 35(12):10639–10646, 2021.
Q. Yang, T. D. Simão, S. H. Tindemans, and M. T. J. Spaan. Safety-constrained reinforcement
learning with a distributional safety critic. Machine Learning, 112:859–887, 2022.
T.-Y. Yang, J. Rosca, K. Narasimhan, and P. J. Ramadge. Projection-based constrained policy
optimization. In Proceedings of International Conference on Learning Representations, 2020.
T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi-task
learning. In Advances in Neural Information Processing Systems, pages 5824–5836, 2020.
H. Zhang, Y. Lin, S. Han, S. Wang, and K. Lv. Conservative distributional reinforcement learning
with safety constraints. arXiv preprint arXiv:2201.07286, 2022.
J. Zhang and P. Weng. Safe distributional reinforcement learning. In J. Chen, J. Lang, C. Amato, and
D. Zhao, editors, Proceedings of International Conference on Distributed Artificial Intelligence,
pages 107–128, 2022.

13

A

Algorithm Details

A.1

Proof of Theorem 3.1

We denote the policy parameter space as Ψ ⊆ Rd , the parameter at the tth iteration as ψt ∈ Ψ, the
Hessian matrix as H(ψt ) = ∇2ψ DKL (πψt ||πψ )|ψ=ψt , and the kth constraint as Fk (ψt ) = Fk (πψt ; α).
As we focus on the tth iteration, the following notations are used for brevity: H = H(ψt ) and
gk = ∇Fk (ψt ). The proposed gradient integration at tth iteration is defined as the following
quadratic program (QP):
gt = argmin
g

1 T
g Hg
2

s.t. gkT g + ck ≤ 0 for ∀k,

(15)

q
where ck = min( 2ϵgkT H −1 gk , Fk (πψ ) − dk + ζ). In the remainder of this section, we introduce
the assumptions and new definitions, discuss the existence of a solution (15), show the convergence
to the feasibility condition for varying step size cases, and provide the proof of Theorem 3.1.
Assumption. 1) Each Fk is differentiable and convex, 2) ∇Fk is L-Lipschitz continuous, 3) all
eigenvalues of the Hessian matrix H(ψ) are equal or greater than R ∈ R>0 for ∀ψ ∈ Ψ, and 4)
{ψ|Fk (ψ) + ζ < dk for ∀k} =
̸ ∅.
Definition. Using the Cholesky decomposition, the Hessian matrix can be expressed as H = B · B T
where B is a lower triangular matrix. By introducing new terms, ḡk := B −1 gk and bt := B T gt , the
following is satisfied: gkT H −1 gk = ||ḡk ||22 . Additionally, we define the in-boundary and out-boundary
sets as:


q
IBk := ψ|Fk (ψ) − dk + ζ ≤ 2ϵ∇Fk (ψ)T H −1 (ψ)∇Fk (ψ) ,


q
OBk := ψ|Fk (ψ) − dk + ζ ≥ 2ϵ∇Fk (ψ)T H −1 (ψ)∇Fk (ψ) .
The minimum of ||ḡk || in OBk is denoted as mk , and the maximum of ||ḡk || in IBk is denoted as
Mk . Also, mink mk and maxk Mk are denoted as m and M , respectively, and we can say that m is
positive.
Lemma A.1. For all k, the minimum value of mk is positive.
Proof. Assume that there exist k ∈ {1, ..., K} such that mk is equal to zero at a policy parameter
ψ ∗ ∈ OBk , i.e., ||∇Fk (ψ ∗ )|| = 0. Since Fk is convex, ψ ∗ is a minimum point of Fk , minψ Fk (ψ) =
Fk (ψ ∗ ) < dk − ζ. However, Fk (ψ ∗ ) ≥ dk − ζ as ψ ∗ ∈ OBk , so mk is positive due to the
contradiction. Hence, the minimum of mk is also positive.
Lemma A.2. A solution of (15) always exists.
Proof. There exists a policy parameter ψ̂ ∈ {ψ|Fk (ψ) + ζ < dk for ∀k} due to the assumptions.
Let g = ψ − ψt . Then, the following inequality holds.
gkT (ψ − ψt ) + ck ≤ gkT (ψ − ψt ) + Fk (ψt ) + ζ − dk ≤ Fk (ψ) + ζ − dk . (∵ Fk is convex.)
⇒ gkT (ψ̂ − ψt ) + ck ≤ Fk (ψ̂) + ζ − dk < 0 for ∀k.
Since ψ̂ − ψt satisfies all constraints of (15), the feasible set is non-empty and convex. Also, H is
positive definite, so the QP has a unique solution.
Lemma A.2 shows the existence of solution of (15). Now, we show the convergence of the proposed
gradient integration method in the case of varying step sizes.
√
√
2ϵmR
Lemma A.3. If 2ϵM ≤ ζ and a policy is updated by ψt+1 = ψt + βt gt , where 0 < βt < 2L||b
2
t ||
and βt ≤ 1, the policy satisfies Fk (ψ) ≤ dk for ∀k within a finite time.
√

2

L||bt ||
2ϵmR ′
′
√
Proof. We can reformulate the step size as β = 2L||b
and 0 < βt′ < 1.
2 βt , where βt ≤
t ||
2 2ϵmR
Since the eigenvalues of H is equal to or bigger than R and H is symmetric and positive definite,

14

1
−1
is positive semi-definite. Hence, xT H −1 x ≤ R1 ||x||2 is satisfied. Using this fact, the
RI − H

following inequality holds:

Fk (ψt + βt gt ) − Fk (ψt ) ≤ βt ∇Fk (ψt )T gt +

L
||βt gt ||2
2

L 2
β ||gt ||2
2 t
L
= βt gkT gt + βt2 bTt H −1 bt
2
L 2
≤ −βt ck +
β ||bt ||2 .
2R t

(∵ ∇Fk is L-Lipschitz continuous.)

= βt gkT gt +

(∵ gt = B −T bt )
(∵ gkT gt + ck ≤ 0)

Now, we will show that ψ enters IBk in a finite time for ∀ψ ∈ OBk and that the kth constraint is
satisfied for√∀ψ ∈ IBk . Thus, we divide into two cases, 1) ψt ∈ OBk and 2) ψt ∈ IBk . For the first
case, ck = 2ϵ||ḡk ||, so the following inequality holds:


√
L
2
Fk (ψt + βt gt ) − Fk (ψt ) ≤ βt − 2ϵ||ḡk || +
βt ||bt ||
2R
√
(16)
≤ βt 2ϵ (−||ḡk || + mβt′ )
√
≤ βt 2ϵm(βt′ − 1) < 0.
The value of Fk decreases strictly with each update step according to (16). Hence, ψt can reach IBk
by repeatedly updating the policy. We now check whether the constraint is satisfied for the second
case. For the second case, the following inequality holds by applying ck = Fk (ψt ) − dk + ζ:
L 2
β ||bt ||2
2R √t
⇒Fk (ψt + βt gt ) − dk ≤ (1 − βt )(Fk (ψt ) − dk ) + βt (−ζ + 2ϵmβt′ ).
Fk (ψt + βt gt ) − Fk (ψt ) ≤ βt dk − βt Fk (ψt ) − βt ζ +

Since ψt ∈ IBk ,
Fk (ψt ) − dk ≤

√

2ϵ||ḡk || − ζ ≤

√

2ϵM − ζ ≤ 0.

Since m ≤ M and βt′ < 1,
−ζ +

√

2ϵmβt′ < −ζ +

√

2ϵM ≤ 0.

Hence, Fk (ψt + βt gt ) ≤ dk , which means that the kth constraint is satisfied if ψt ∈ IBk . As ψt
reaches IBk for ∀k within a finite time according to (16), the policy can satisfy all constraints within
a finite time.
Lemma A.3 shows the convergence to the feasibility √
condition in the case of varying step sizes. We
introduce a lemma, which shows ||bt || is bounded by ϵ, and finally show the proof of Theorem 3.1,
which can be considered a special case of varying step sizes.
√
Lemma A.4. There exists T ∈ R>0 such that ||bt || ≤ T ϵ.
Proof. Let us define the following sets:
I := {k|Fk (ψt ) + ζ − dk < 0}, O := {k|Fk (ψt ) + ζ − dk > 0},
U := {k|Fk (ψt ) + ζ − dk = 0}, C(ϵ) := {g|gkT g + ck (ϵ) ≤ 0 ∀k},

(17)

IG := {g|gkT g + Fk (ψt ) + ζ − dk ≤ 0 ∀k ∈ I},
q
where ck (ϵ) = min( 2ϵgkT H −1 gk , Fk (πψ ) − dk + ζ). Using these sets, the following vectors can be
defined: g(ϵ) := argming∈C(ϵ) 12 g T Hg, b(ϵ) := B T g(ϵ). Now, we will show that ||b(ϵ)|| is bounded
√
above and ||b(ϵ)|| ∝ ϵ for sufficiently small ϵ > 0.
First, the following is satisfied for a sufficiently large ϵ:
C(ϵ) = {g|gkT g + Fk (ψt ) + ζ − dk ≤ 0 ∀k}.
15

(18)

Since ψ̂ − ψt ∈ C(ϵ), where ψ̂ is defined in Lemma A.2, ||b(ϵ)|| ≤ 21 (ψ̂ − ψt )T H(ψ̂ − ψt ) for ∀ϵ.
Therefore, ||b(ϵ)|| is bounded above.
Second, let us define the following trust region size:

2
1
Fk (ψt ) + ζ − dk
ϵ̂ :=
mink∈O
> 0.
2
||g¯k ||

(19)

if , ϵ ≤ ϵ̂, the following is satisfied:
√
C(ϵ) = IG ∩ {g|gkT g + 2ϵ||g¯k || ≤ 0 ∀k ∈ O, gkT g ≤ 0 ∀k ∈ U } =
̸ ϕ.
(20)
√
T
T
Thus, OG (ϵ) := {g|gk g + 2ϵ||g¯k || ≤ 0 ∀k ∈ O, gk g ≤ 0 ∀k ∈ U } is not empty. If we define
ĝ(ϵ) := argming∈OG (ϵ) 12 g T Hg, the following is satisfied:
r
ϵ
ĝ(ϵ) =
ĝ(ϵ̂) for 0 ≤ ϵ ≤ ϵ̂.
(21)
ϵ̂
2
Then, if ϵ ≤ ϵ̂ mink∈I (dk − ζ − Fk (ψt ))/gkT ĝ(ϵ̂)) , ĝ(ϵ) = g(ϵ) since ĝ(ϵ) ∈ IG . Consequently,
by defining a trust region size:
2 !

dk − ζ − Fk (ψt )
∗
> 0,
(22)
ϵ := ϵ̂ · min 1, mink∈I
gkT ĝ(ϵ̂)
p
√
g(ϵ) = ϵ/ϵ̂ĝ(ϵ̂) for ϵ ≤ ϵ∗ . Therefore, ||b(ϵ)|| ∝ ϵ if ϵ ≤ ϵ∗ .
√
Finally, since ||b(ϵ)|| is bounded √
above and proportional to ϵ for sufficiently small ϵ, there exist a
constant T such that ||b(ϵ)|| ≤ T ϵ.
Theorem 3.1. Assume that the constraints are differentiable and convex, gradients of the constraints
are L-Lipschitz continuous, eigenvalues of the Hessian are equal or greater than a positive value
R ∈ R>0 , and {ψ|Fk (πψ ; α) + ζ < dk , ∀k} ̸= ∅. Then, there exists E ∈ R>0 such that if
0 < ϵ ≤ E and a policy is updated by the proposed gradient integration method, all constraints are
satisfied within finite time steps.
√
Proof. The proposed step size is βt = min(1, 2ϵ/||bt ||), and the sufficient conditions that guarantee
the convergence according to Lemma A.3 are followings:
√
√
2 2ϵmR
.
2ϵM ≤ ζ and 0 < βt ≤ 1 and βt <
L||bt ||2
The second condition is self-evident. To satisfy the third condition, the proposed step size βt should
satisfy the followings:
√
√
2ϵ
2 2ϵmR
2mR
<
⇔ ||bt || <
.
||bt ||
L||bt ||2
L
If ϵ < 4((mR)/(LT ))2 , the following inequality holds:
√

ϵ<

√
2mR
2mR
⇒ ||bt || ≤ T ϵ <
. (∵ Lemma A.4.)
LT
L
2

ζ
mR 2
Hence, if ϵ ≤ E = 21 min( 2M
2 , 4( LT ) ), the sufficient conditions are satisfied.

A.2

Toy Example for Gradient Integration Method

The problem of the toy example in Figure 1 is defined as:
q√
√
minimize ( 3x1 + x2 + 2)2 + 4(x1 − 3x2 + 4)2
x1 ,x2

s.t. x1 ≥ 0, x1 − 2x2 ≤ 0,

(23)

where there are two linear constraints. The initial points for the naive and gradient integration methods
are x1 = −2.5 and x2 = −3.0, which do not satisfied the two constraints. We use the Hessian matrix
for the trust region as identity matrix and the trust region size as 0.5 in both methods. The naive
method minimizes the constraints in order from the first to the second constraint.
16

A.3

Analysis of Worst-Case Time to Satisfy All Constraints

To analyze the sample complexity, we consider a tabular MDP and use softmax policy parameterization as follows (for more details, see [Xu et al., 2021]):
exp ψ(s, a)
∀(s, a) ∈ S × A.
′
a′ exp ψ(s, a )

πψ (a|s) := P

(24)

According to Agarwal et al. [2021], the natural policy gradient (NPG) update is as follows:
ψt+1 = ψt + βAπψt , πψt+1 (a|s) = πψt (a|s)

exp(βAπψt (s, a))
,
Zt (s)

(25)

where β P
is a step size, Aπψt ∈ R|S||A| is the vector expression of the advantage function, and
Zt (s) = a πψt (a|s) exp (βAπψt (s, a))/(1 − γ). Analyzing the sample complexity of trust regionbased methods is challenging since their stepsize is not fixed, so we modify the gradient integration
method to use the NPG as follows:
1
g ∗ = argming g T Hg s.t. gkT g + ck ≤ 0 ∀k ∈ {k|Fk (πψt ; α) > dk },
(26)
2
ψt+1 = ψt + βg ∗ /||g ∗ ||2 .
In the remainder, we abbreviate πψt , Aπψt , and Fk (πψt ; α) as πt , At , and Fk (πt ), respectively. Since
g ∗ always exists due to Lemma A.2, we can write the policy using Lagrange multipliers λtk ≥ 0 as
follows:
X
X
ψt+1 = ψt − β
λtk AtCk /Wt , Wt := ||
λtk AtCk ||2 ,
k

k

!
β X t t
πt+1 (a|s) = πt (a|s) exp −
λk ACk (s, a) /Zt (s),
Wt

(27)

k

where Zt (s) is a normalization factor, and λtk = 0 for Fk (πt ) ≤ dk . The naive approach can also be
written as above, except that λt is a one-hot vector, where i-th value λti is one only for corresponding
to the randomly selected constraint. Then, we can get the followings:
"
#
X
X
X
1
t
t
t
E πt+1
πt+1 (a|s)
λk ACk (s, a)/Wt
λk (Fk (πt+1 ) − Fk (πt ))/Wt =
1 − γ s∼d
a
k
k
#
"
X
1
πt+1 (a|s)Zt (s)
E πt+1
=−
πt+1 (a|s) log
β(1 − γ) s∼d
πt (a|s)
a
1
E πt+1 [DKL (πt+1 (·|s)||πt (·|s)) + log Zt (s)]
β(1 − γ) s∼d
1
E πt+1 [log Zt (s)]
≤−
(∵ DKL (π ′ ||π) ≥ 0)
β(1 − γ) s∼d
1
(∵ ||dπ /ρ||∞ ≥ 1 − γ)
≤ − Es∼ρ [log Zt (s)] ,
β

=−

We can also get the followings by using the Lemma 7 in Xu et al. [2021]:
X
1
λtk (Fk (π∗ ) − Fk (πt ))/Wt ≥ −
Es∼d∗ [DKL (π∗ ||πt ) − DKL (π∗ ||πt+1 )]
β(1 − γ)
k
X
2βCmax
−
λtk
,
(1 − γ)2 Wt

(28)

(29)

k

t
where π∗ is an optimal
P policy, and Cmax is the maximum
P value of costs. If λk > 0, Fk (πt ) −
Fk (π∗ ) > ζ. Thus, k λtk (Fk (π∗ ) − Fk (πt )) ≤ −ζ k λtk . If the policy does not satisfy the
constraints until T step, the following inequality holds by summing the above inequalities from t = 0
to T :
T
2βCmax X X t
)
λ /Wt ≤ Es∼d∗ [DKL (π∗ ||π0 )] .
(30)
β(1 − γ)(ζ −
(1 − γ)2 t=0 i i

17

PT P
P
P
Let denote T1 t=0 i λti /Wt as Et [ i λti /Wt ], and we can get Wt = || k λtk AtCk ||2 ≤
P t
k λk 2|S||A|Cmax /(1 − γ). Then, the maximum T can be expressed as:
T ≤

2|S||A|Cmax DKL
DKL
≤
=: Tmax ,
P t
2βCmax
β(1 − γ)2 ζ − 2β 2 Cmax
β(1 − γ)(ζ − (1−γ)2 )Et [ i λi /Wt ]

(31)

where we abbreviate Es∼d∗ [DKL (π∗ ||π0 )] as DKL . Finally, the policy can reach the feasible region
within Tmax steps.
The worst-case time of the naive approach is the same as the above equation, except for the λ
t
part.
vector, as mentioned earlier. In other words, only
P In tthe naive approach,
P t λP is ta one-hot
Et [ i λi /Wt ] = Et [ i λi /|| i λi AtCk ||2 ] is different. Let us assume that the advantage vector
P
follows a normal distribution. Then, the variance of i λti AtCk is smaller for λt with distributed
values than for one-hot values. Then, the reciprocal of the 2-norm becomes larger, resulting in a
decrease in the worst-case time. From this perspective, the gradient integration method has a benefit
over the naive approach as it reduces the variance of the advantage vector. Even though we cannot
officially say that the worst-case time of the proposed method is smaller than the naive method
because the advantage vector does not follow the normal distribution, we can deliver our insight on
the benefit of gradient integration method.
A.4

Proof of Theorem 3.2

π
In this section, we show that a sequence, Zk+1 = Tλµ,π Zk , converges to the ZR
. First, we rewrite
µ,π
the operator Tλ for random variables to an operator for distributions and show that the operator is
π
contractive. Finally, we show that ZR
is the unique fixed point.
π
Before starting the proof, we introduce useful notions and distance metrics. As the return ZR
(s, a)
π
π
is a random variable, we define the distribution of ZR
(s, a) as νR
(s, a). Let η be the distribution
of a random variable X. Then, we can express the distribution of affine transformation of random
variable, aX + b, using the pushforward operator, which is defined by Rowland et al. [2018], as
(fa,b )# (η). To measure a distance between two distributions, Bellemare et al. [2023] has defined the
distance lp as follows:

Z
lp (η1 , η2 ) :=

1/p
|Fη1 (x) − Fη2 (x)| dx
,
p

(32)

R

where Fη (x) is the cumulative distribution function. This distance is 1/p-homogeneous, regular,
and p-convex (see Section 4 of Bellemare et al. [2023] for more details). For functions that map
state-action pairs to distributions, a distance can be defined as [Bellemare et al., 2023]: ¯lp (ν1 , ν2 ) :=
sup(s,a)∈S×A lp (ν1 (s, a), ν2 (s, a)). Then, we can rewrite the operator Tλµ,π for random variables in
(13) as an operator for distributions as below.
1−λ
Tλµ,π ν(s, a) :=
N


× Eµ 

i
Y

∞
X

λi

i=0




h

(33)

i

η(sj , aj ) Ea′ ∼π(·|si+1 ) (fγ i+1 ,Pit=0 γ t rt )# (ν(si+1 , a′ )) s0 = s, a0 = a ,

j=1

where η(s, a) = π(a|s)
µ(a|s) and N is a normalization factor. Since the random variable Z(s, a) and the
distribution ν(s, a) is equivalent, the operators in (13) and (33) are also equivalent. Hence, we are
going to show the proof of Theorem 3.2 using (33) instead of (13). We first show that the operator
Tλµ,π has a contraction property.
Lemma A.5. Under the distance ¯lp and the assumption that the state, action, and reward spaces are
finite, Tλµ,π is γ 1/p -contractive.
18

Proof. First, the operator can be rewritten using summation as follows.
Tλµ,π ν(s, a) =

∞
1−λX

N


λi

X

X

a′ ∈A (s0 ,a0 ,r0 ,...,si+1 )

i=0

Prµ (s0 , a0 , r0 , ..., si+1 ) 
|
{z
}

=:τ
P
× π(a |si+1 )(fγ i+1 , it=0 γ t rt )# (ν(si+1 , a′ ))

i
Y


η(sj , aj )

j=1

′



∞
i
Y
X
1−λX i X X
=
λ
Prµ (τ ) 
η(sj , aj ) π(a′ |si+1 )
1s′ =si+1
N i=0
j=1
a′ ∈A τ
s′ ∈S
!
i
X Y
1rk′ =rk (fγ i+1 ,Pit=0 γ t rt′ )# (ν(s′ , a′ ))
×
′
r0:i

k=0

(34)

∞
X

X XX
1−λ
=
(fγ i+1 ,Pit=0 γ t rt′ )# (ν(s′ , a′ ))
λi
N i=0
′
a′ ∈A s′ ∈S r0:i


!
i
i
Y
Y
× Eµ 
1rk′ =rk 
η(sj , aj ) π(a′ |si+1 )1s′ =si+1
j=1

k=0

|

{z

=:ws′ ,a′ ,r′

}

0:i

∞

1−λX X X X i
′ ′
′ (f i+1 Pi
λ ws′ ,a′ ,r0:i
=
γ
, t=0 γ t rt′ )# (ν(s , a )).
N i=0 ′
′
′
s ∈S a ∈A r0:i

Since the sumPof weights
of distributions
should be one, we can find the normalization factor
P
P
∞ P
N = (1 − λ) i=0 s∈S a∈A r0:i λi ws,a,r0:i . Then, the following inequality can be derived
using the homogeneity, regularity, and convexity of lp :
lpp (Tλµ,π ν1 (s, a), Tλµ,π ν2 (s, a))
∞

= lpp

1 − λ XX X X i
λ ws,a,r0:i (fγ i+1 ,Pit=0 γ t rt )# (ν1 (s, a)),
N i=0
r
s∈S a∈A

0:i

∞
1 − λ XX X X

N
≤

λ ws,a,r0:i (fγ i+1 ,Pit=0 γ t rt )# (ν2 (s, a))

i=0 s∈S a∈A r0:i

∞ XXX
X
(1 − λ)λi ws,a,r

0:i

i=0 s∈S a∈A r0:i

!
i

N


lpp (fγ i+1 ,Pit=0 γ t rt )# (ν1 (s, a)),

(fγ i+1 ,Pit=0 γ t rt )# (ν2 (s, a))
≤

=

∞ XXX
X
(1 − λ)λi ws,a,r

0:i

i=0 s∈S a∈A r0:i
∞ XXX
X
i=0 s∈S a∈A r0:i

≤

N

lpp (fγ i+1 ,0 )# (ν1 (s, a)), (fγ i+1 ,0 )# (ν2 (s, a))

(35)


(1 − λ)λi ws,a,r0:i i+1 p
γ lp (ν1 (s, a), ν2 (s, a))
N

∞ XXX
X
(1 − λ)λi ws,a,r

0:i

i=0 s∈S a∈A r0:i



N

p
γ i+1 ¯lp (ν1 , ν2 )

p
≤ γ ¯lp (ν1 , ν2 ) .
Therefore, ¯lp (Tλµ,π ν1 , Tλµ,π ν2 ) ≤ γ 1/p ¯lp (ν1 , ν2 ).
By the Banach’s fixed point theorem, the operator Tλµ,π has a unique fixed distribution. We now show
π
that the fixed distribution is νR
.
π
Lemma A.6. The fixed distribution of the operator Tλµ,π is νR
.
19

π
π
Proof. From the definition of ZR
, the following equality holds [Rowland et al., 2018]: νR
(s, a) =
π
π ′ ′
Eπ [(fγ,r )# (νR (s , a ))]. Then, it can be shown that νR is the fixed distribution by applying the
π
operator Tλµ,π to νR
:
∞

π
Tλµ,π νR
(s, a) =


× Eµ 

i
Y

1−λX i
λ
N i=0



i

h

π
(si+1 , a′ )) s0 = s, a0 = a
η(sj , aj ) Ea′ ∼π(·|si+1 ) (fγ i+1 ,Pit=0 γ t rt )# (νR

j=1

=
=

∞
1−λX

N

π
λi Eπ (fγ i+1 ,Pit=0 γ t rt )# (νR
(si+1 , ai+1 )) s0 = s, a0 = a

i

i=0

∞
1−λX

N

(36)
h

π
π
λi νR
(s, a) = νR
(s, a).

i=0

Theorem 3.2. Let define a distributional operator Tλµ,π , whose probability density function is:
Pr(Tλµ,π Z(s, a) = z) ∝
"
"
!#
#
i
∞
i
Y
X
X
π(aj |sj )
i
t
i+1
′
Eµ λ
Pr
E
γ Rt +γ Z(si+1 , a ) = z
s0 = s, a0 = a .
µ(aj |sj ) a′ ∼π(·|si+1 )
t=0
j=1
i=0

(13)

π
Then, a sequence, Zk+1 (s, a) = Tλµ,π Zk (s, a) ∀(s, a), converges to ZR
.

Proof. The operator Tλµ,π is γ 1/p -contractive under the distance ¯lp according to Lemma A.5. Also,
π
π
the fixed distribution of the operator is νR
, which is equivalent to ZR
, according to Lemma A.6. By
µ,π
the Banach’s fixed point theorem, the sequence, Zk+1 (s, a) = Tλ Zk (s, a) ∀(s, a), converges to
π
the fixed distribution of the operator, ZR
.
A.5

Pseudocode of TD(λ) Target Distribution

We provide the pseudocode for calculating TD(λ) target distribution for the reward critic in Algorithm
2. The target distribution for the cost critics can also be obtained by simply replacing the reward part
with the cost.
Algorithm 2 TD(λ) Target Distribution
Input: Policy network πψ , critic network Zθπ , and trajectory {(st , at , µ(at |st ), rt , dt , st+1 )}Tt=1 .
Sample an action a′T +1 ∼ πψ (sT +1 ) and get ẐTtot = rT + (1 − dT )γZθπ (sT +1 , a′T +1 ).
Initialize the total weight wtot = λ.
for t = T to 1 do
(1)
Sample an action a′t+1 ∼ πψ (st+1 ) and get Ẑt = rt + (1 − dt )γZθπ (st+1 , a′t+1 ).
Set the current weight w = 1 − λ.
(1)
(tot)
Combine the two targets, (Ẑt , w) and (Ẑt , wtot ), and sort the combined target according
to the positions of atoms.
Build the CDF of the combined target by accumulating the weights at each atom.
(proj)
Project the combined target into a quantile distribution with M ′ atoms, which is Ẑt
, using
the CDF (find the atom positions corresponding to each quantile).
(tot)
(proj)
πψ (at |st )
Update Ẑt−1 = rt−1 + (1 − dt−1 )γ Ẑt
and wtot = λ µ(a
(1 − dt−1 )(1 − λ + wtot ).
t |st )
end for
(proj) T
Return {Ẑt
}t=1 .

20

A.6

Quantitative Analysis on TD(λ) Target Distribution

We experiment with a toy example to measure the bias and variance of the reward estimation according
to λ. The toy example has two states, s1 and s2 ; the state distribution is defined as an uniform;
the reward function is defined as r(s1 ) ∼ N (−0.005, 0.02) and r(s2 ) ∼ N (0.005, 0.03). We train
parameterized reward distributions by minimizing the quantile regression loss with the TD(λ) target
distribution for λ = 0, 0.5, 0.9, and 1.0. The experimental results are presented in the table below.
Table 1: Experimental results of the toy example.

λ = 0.0
λ = 0.5
λ = 0.9
λ = 1.0

5th iteration

10th iteration

15th iteration

20th iteration

25th iteration

4.813 (0.173)
4.621 (0.185)
4.141 (0.461)
2.886 (0.767)

4.024 (0.253)
3.688 (0.273)
2.237 (0.402)
1.733 (0.365)

3.498 (0.085)
2.925 (0.183)
1.389 (0.132)
1.509 (0.514)

3.131 (0.103)
2.379 (0.134)
1.058 (0.031)
1.142 (0.325)

2.835 (0.070)
2.057 (0.070)
0.923 (0.019)
1.109 (0.476)

The values in the table are the mean and standard deviation of the past five values of the Wasserstein
distance between the true reward return and the estimated distribution. Looking at the fifth iteration,
it is clear that the larger the λ value, the smaller the mean and the higher the standard deviation. At
the 25th iteration, the run with λ = 0.9 has the lowest mean and standard deviation, indicating that
training has converged. On the other hand, the run with λ = 1.0 has the biggest standard deviation,
and the mean is greater than λ = 0.9, indicating that the significant variance hinders training. In
conclusion, we measured bias and variance quantitatively through the toy example, and the results
are well aligned with our claim that λ can trade off bias and variance.
A.7

Surrogate Functions

In this section, we introduce the surrogate functions for the objective and constraints.
First, Kim and
P∞
Oh [2022a] define a doubly discounted state distribution: dπ2 (s) := (1 − γ 2 ) t=0 γ 2t Pr(st = s|π).
Then, the surrogates for the objective and constraints are defined as follows [Kim and Oh, 2022a]:



1 
β Eπ H(π ′ (·|s)) + µE ′ [QπR (s, a)] ,
s0 ∼ρ
d ,π
1−γ d
 π

 π

1
µ,π
′
JCk (π ) := E VCk (s0 ) +
E QCk (s, a) ,
s0 ∼ρ
1 − γ dµ ,π′


 π
 π
1
(s0 ) +
(s, a) ,
JSµ,π
(π ′ ) := E SC
E SC
k
k
2
µ
k
s0 ∼ρ
1 − γ d2 ,π′
−1
ϕ(Φ (α)) q µ,π ′
(π ′ ) +
Fkµ,π (π ′ ; α) := JCµ,π
JSk (π ) − (JCµ,π
(π ′ ))2 ,
k
k
α
J µ,π (π ′ ) := E [V π (s0 )] +

(37)

where µ, π, π ′ are behavioral, current, and next policies, respectively. According to Theorem 1 in
[Kim and Oh, 2022a], the constraint surrogates are bounded by DKL (π, π ′ ). We also show that the
surrogate of the objective is bounded by DKL (π, π ′ ) in Appendix A.9. As a result, the gradients of
the objective function and constraints become the same as the gradients of the surrogates, and the
surrogates can substitute the objective and constraints within the trust region.
A.8

Policy Update Rule

To solve the constrained optimization problem (5), we find a policy update direction by linearly
approximating the objective and safety constraints and quadratically approximating the trust region
constraint, as done by Achiam et al. [2017]. After finding the direction, we update the policy using a
line search method. Given the current policy parameter ψt ∈ Ψ, the approximated problem can be
expressed as follows:
1
x∗ = argmax g T x s.t. xT Hx ≤ ϵ, bTk x + ck ≤ 0 ∀k,
(38)
2
x∈Ψ
where g = ∇ψ J µ,π (πψ )|ψ=ψt , H = ∇2ψ DKL (πψt ||πψ )|ψ=ψt , bk = ∇ψ Fkµ,π (πψ ; α)|ψ=ψt , and
ck = Fk (πψ ; α) − dk . Since (38) is convex, we can use an existing convex optimization solver.
21

However, the search space, which is the policy parameter space Ψ, is excessively large, so we reduce
the space by converting (38) to a dual problem as follows:
1
g(λ, ν) = minx L(x, λ, ν) = minx {−g T x + ν( xT Hx − ϵ) + λT (Bx + c)}
2


=

−1  T −1
T
T
−1 T
T
−1 T 
g H g −2 g H B λ + λ BH
| {z B } λ + λ c − νϵ
| {z }
2ν | {z }
=:q

(39)

=:S

=:r T

−1
(q − 2rT λ + λT Sλ) + λT c − νϵ,
2ν
where B = (b1 , .., bK ), c = (c1 , ..., cK )T , and λ ∈ RK ≥ 0 and ν ∈ R ≥ 0 are Lagrange multipliers.
Then, the optimal λ and ν can be obtained by a convex optimization solver. After obtaining the
optimal values, (λ∗ , ν ∗ ) = argmax(λ,ν) g(λ, ν), the policy update direction x∗ are calculated by
1
−1
(g − B T λ∗ ). Then, the policy is updated by ψt+1 = ψt + βx∗ , where β is a step size, which
ν∗ H
can be found through a backtracking method (please refer to Section 6.3.2 of Dennis and Schnabel
[1996]).
=

Before using the above policy update rule, we should note that the existing trust-region method with
the risk-averse constraint [Kim and Oh, 2022a] and the equations (1, 37, 5) are slightly different.
There are two differences: 1) the objective is augmented with an entropy bonus, and 2) the surrogates
are expressed with Q-functions instead of value functions. To use the entropy-regularized objective
in the trust-region method, it is required to show that the objective is bounded by the KL divergence.
We present the existence of bound in Appendix A.9. Next, there is no problem using the Q-functions
because it is mathematically equivalent between the original surrogates [Kim and Oh, 2022a] and
the new ones expressed with Q-functions (37). However, we experimentally show that using the
Q-functions in off-policy settings has advantages in Appendix A.10.
A.9

Bound of Entropy-Augmented Objective

In this section, we show that the entropy-regularzied objective in (1) has a bound expressed by the
KL divergence. Before showing the boundness, we present a new function and a lemma. A value
difference function is defined as follows:
′

δ π (s) := E [R(s, a, s′ ) + γV π (s′ ) − V π (s) | a ∼ π ′ (·|s), s′ ∼ P (·|s, a)] = E ′ [Aπ (s, a)] ,
a∼π

where Aπ (s, a) := Qπ (s, a) − V π (s, a).
′

Lemma A.7. The maximum of |δ π (s) − δ π (s)| is equal or less than ϵR
ϵR = max |Aπ (s, a)|.

p

max (π||π ′ ), where
2DKL

s,a

Proof. The value difference can be expressed in a vector form,
X
′
δ π (s) − δ π (s) =
(π ′ (a|s) − π(a|s))Aπ (s, a) = ⟨π ′ (·|s) − π(·|s), Aπ (s, ·)⟩.
a

Using Hölder’s inequality, the following inequality holds:
′

|δ π (s) − δ π (s)| ≤ ||π ′ (·|s) − π(·|s)||1 · ||Aπ (s, ·)||∞
= 2DTV (π ′ (·|s)||π(·|s))maxa Aπ (s, a).
′

′

⇒ ||δ π − δ π ||∞ = maxs |δ π (s) − δ π (s)| ≤ 2ϵR maxs DTV (π(·|s)||π ′ (·|s)).
p max
′
Using Pinsker’s inequality, ||δ π − δ π ||∞ ≤ ϵR 2DKL
(π||π ′ ).
Theorem A.8. Let us assume that maxs H(π(·|s)) < ∞ for ∀π ∈ Π. The difference between the
objective and surrogate functions is bounded by a term consisting of KL divergence as:
√

J(π ′ ) − J µ,π (π ′ ) ≤

2γ
(1 − γ)2



q
q
max
max
DKL
(π||π ′ ) βϵH + ϵR 2DKL
(µ||π ′ ) ,

(40)

max
where ϵH = max |H(π ′ (·|s))|, DKL
(π||π ′ ) = max DKL (π(·|s)||π ′ (·|s)), and the equality holds

when π ′ = π.

s

s

22

Proof. The surrogate function can be expressed in vector form as follows:

′
1  µ π′
⟨d , δ ⟩ + β⟨dπ , H π ⟩ ,
J µ,π (π ′ ) = ⟨ρ, V π ⟩ +
1−γ
′

where H π (s) = H(π ′ (·|s)). The objective function of π ′ can also be expressed in a vector form
using Lemma 1 from Achiam et al. [2017],
h
i
′
′
1
J(π ′ ) =
E R(s, a, s′ ) + βH π (s) | s ∼ dπ , a ∼ π ′ (·|s), s′ ∼ P (·|s, a)
1−γ
h ′
i
′
1
=
E ′ δ π (s) + βH π (s) + E [V π (s)]
s∼ρ
1 − γ s∼dπ
′
′
′
1
= ⟨ρ, V π ⟩ +
⟨dπ , δ π + βH π ⟩.
1−γ
p max
′
γ
By Lemma 3 from Achiam et al. [2017], ||dπ − dπ ||1 ≤ 1−γ
2DKL (π||π ′ ). Then, the following
inequality is satisfied:
|(1−γ)(J µ,π (π ′ ) − J(π ′ ))|
′

′

′

′

′

′

′

′

= |⟨dπ − dµ , δ π ⟩ + β⟨dπ − dπ , H π ⟩|
′

′

≤ |⟨dπ − dµ , δ π ⟩| + β|⟨dπ − dπ , H π ⟩|
′

′

= |⟨dπ − dµ , δ π − δ π ⟩| + β|⟨dπ − dπ , H π ⟩|
′

′

′

(∵ δ π = 0)
′

≤ ||dπ − dµ ||1 ||δ π − δ π ||∞ + β||dπ − dπ ||1 ||H π ||∞
(∵ Hölder’s inequality)
q
q
2ϵR γ
max (µ||π ′ )D max (π||π ′ ) + βγϵH
max (π||π ′ ) (∵ Lemma A.7)
≤
DKL
2DKL
KL
1−γ
1−γ


q
q
√
γ
max (π||π ′ )
max (µ||π ′ ) .
=
DKL
2βϵH + 2ϵR DKL
1−γ
If π ′ = π, the KL divergence term becomes zero, so equality holds.
A.10

Comparison of Q-Function and Value Function-Based Surrogates

The original surrogate is defined as follows:
J µ,π (π ′ ) := J(π) +

 ′

1
π (a|s) π
E
A
(s,
a)
,
1 − γ dµ ,µ µ(a|s)

(41)

where Aπ (s, a) := Qπ (s, a) − V π (s, a), and the surrogate is the same as that of OffTRPO [Meng
et al., 2022] and OffTRC [Kim and Oh, 2022a]. An entropy-regularized version can be derived as:

 ′
1 
π (a|s) π
µ,π
′
′
J (π ) = J(π) +
(42)
β E [H(π (·|s))] + µE
A (s, a) .
d ,µ µ(a|s)
1 − γ dπ
Then, the surrogate expressed by Q-functions in (37), called SAC-style version, can be rewritten as:

1 
J µ,π (π ′ ) = J(π) +
β Eπ [H(π ′ (·|s))] + µE ′ [Qπ (s, a)] .
(43)
d ,π
1−γ d
In this section, we evaluate the original, entropy-regularized, and SAC-style versions in the continuous
control tasks of the MuJoCo simulators [Todorov et al., 2012]. We use neural networks with two
hidden layers with (512, 512) nodes and ReLU for the activation function. The output of a value
network is linear, but the input is different; the original and entropy-regularized versions use states,
and the SAC-style version uses state-action pairs. The input of a policy network is the state, the
output is mean µ and std σ, and actions are squashed into tanh(µ + ϵσ), ϵ ∼ N (0, 1) as in SAC
[Haarnoja et al., 2018]. The entropy coefficient β in the entropy-regularized and SAC-style versions
are adaptively adjusted to keep the entropy above a threshold (set as −d given A ⊆ Rd ). The
hyperparameters for all versions are summarized in Table 2.
The training curves are presented in Figure 5. All methods are trained with five different random
seeds. Although the entropy-regularized version (42) and SAC-style version (43) are mathematically
23

Table 2: Hyperparameters for all versions.
Parameter

Value

Discount factor γ
Trust region size ϵ
Length of replay buffer
Critic learning rate
Trace-decay λ
Initial entropy coefficient β
β learning rate

0.99
0.001
105
0.0003
0.97
1.0
0.01

(a) Ant-v3

(b) HalfCheetah-v3

(c) Hopper-v3

(d) Humanoid-v3

(e) Swimmer-v3

(f) Walker2d-v3

Figure 5: MuJoCo training curves.
equivalent, it can be observed that the performance of the SAC-style version is superior to the
regularized version. It can be inferred that this is due to the variance of importance sampling. In the
off-policy setting, the sampling probabilities of the behavioral and current policies can be significantly
different, so the variance of the importance ratio is huge. The increased variance prevents estimating
the objective accurately, so significant performance degradation can happen. As a result, using the
Q-function-based surrogates has an advantage for efficient learning.

24

B

Experimental Settings

(a) Point goal.

(b) Car button.

(c) Cassie.

(d) Laikago.

(e) Mini-Cheetah.

Figure 6: (a) and (b) are Safety Gym tasks. (c), (d), and (e) are locomotion tasks.
Safety Gym. We use the goal and button tasks with the point and car robots in the Safety Gym
environment [Ray et al., 2019], as shown in Figure 6a and 6b. The environmental setting for the goal
task is the same as in Kim and Oh [2022b]. Eight hazard regions and one goal are randomly spawned
at the beginning of each episode, and a robot gets a reward and cost as follows:
R(s, a, s′ ) = −∆dgoal + 1dgoal ≤0.3 ,
C(s, a, s′ ) = Sigmoid(10 · (0.2 − dhazard )),

(44)

where dgoal is the distance to the goal, and dhazard is the minimum distance to hazard regions. If
dgoal is less than or equal to 0.3, a goal is respawned. The state consists of relative goal position,
goal distance, linear and angular velocities, acceleration, and LiDAR values. The action space is
two-dimensional, which consists of xy-directional forces for the point and wheel velocities for the
car robot.
The environmental settings for the button task are the same as in Liu et al. [2022]. There are five
hazard regions, four dynamic obstacles, and four buttons, and all components are fixed throughout the
training. The initial position of a robot and an activated button are randomly placed at the beginning
of each episode. The reward function is the same as in (44), but the cost is different since there is no
dense signal for contacts. We define the cost function for the button task as an indicator function that
outputs one if the robot makes contact with an obstacle or an inactive button or enters a hazardous
region. We add LiDAR values of buttons and obstacles to the state of the goal task, and actions are
the same as the goal task. The length of the episode is 1000 steps without early termination.
Locomotion Tasks. We use three different legged robots, Mini-Cheetah, Laikago, and Cassie, for
the locomotion tasks, as shown in Figure 6e, 6d, and 6c. The tasks aim to control robots to follow
a velocity command on flat terrain. A velocity command is given by (vxcmd , vycmd , ωzcmd ), where
vxcmd ∼ U(−1.0, 1.0) for Cassie and U(−1.0, 2.0) otherwise, vycmd = 0, and ωzcmd ∼ U(−0.5, 0.5).
To lower the task complexity, we set the y-directional linear velocity to zero but can scale to any
non-zero value. As in other locomotion studies [Lee et al., 2020, Miki et al., 2022], central phases are
introduced to produce periodic motion, which are defined as ϕi (t) = ϕi,0 +f ·t for ∀i ∈ {1, ..., nlegs },
where f is a frequency coefficient and is set to 10, and ϕi,0 is an initial phase. Actuators of robots
are controlled by PD control towards target positions given by actions. The state consists of velocity
command, orientation of the robot frame, linear and angular velocities of the robot, positions and
speeds of the actuators, central phases, history of positions and speeds of the actuators (past two
steps), and history of actions (past two steps). A foot contact timing ξ can be defined as follows:
ξi (s) = −1 + 2 · 1sin(ϕi )≤0 ∀i ∈ {1, ..., nlegs },
(45)
where a value of -1 means that the ith foot is on the ground; otherwise, the foot is in the air. For
the quadrupedal robots, Mini-Cheetah and Laikago, we use the initial phases as ϕ0 = {0, π, π, 0},
which generates trot gaits. For the bipedal robot, Cassie, the initial phases are defined as ϕ0 = {0, π},
which generates walk gaits. Then, the reward and cost functions are defined as follows:
base
cmd 2
R(s, a, s′ ) = −0.1 · (||vx,y
− vx,y
||2 + ||ωzbase − ωzcmd ||22 + 10−3 · Rpower ),
nlegs
′

′

′

C1 (s, a, s ) = 1angle≥a , C2 (s, a, s ) = 1height≤b , C3 (s, a, s ) =

X

(1 − ξi · ξˆi )/(2 · nlegs ),

(46)

i=1

P
where the power consumption Rpower = i |τi vi |, the sum of the torque times the actuator speed, is
base
added to the reward as a regularization term, vx,y
is the xy-directional linear velocity of the base
25

frame of robots, ωzbase is the z-directional angular velocity of the base frame, and ξˆ ∈ {−1, 1}nlegs is
the current feet contact vector. For balancing, the first cost indicates whether the angle between the
z-axis vector of the robot base and the world is greater than a threshold (a = 15◦ for all robots). For
standing, the second cost indicates the height of CoM is less than a threshold (b = 0.3, 0.35, 0.7 for
Mini-Cheetah, Laikago, and Cassie, respectively), and the last cost is to check that the current feet
contact vector ξˆ matches the pre-defined timing ξ. The length of the episode is 500 steps. There is no
early termination, but if a robot falls to the ground, the state is frozen until the end of the episode.
Hyperparameter Settings. The structure of neural networks consists of two hidden layers with
(512, 512) nodes and ReLU activation for all baselines and the proposed method. The input of value
networks is state-action pairs, and the output is the positions of atoms. The input of policy networks
is the state, the output is mean µ and std σ, and actions are squashed into tanh(µ + ϵσ), ϵ ∼ N (0, 1).
We use a fixed entropy coefficient β. The trust region size ϵ is set to 0.001 for all trust region-based
methods. The overall hyperparameters for the proposed method can be summarized in Table 3.
Table 3: Hyperparameter settings for the Safety Gym and locomotion tasks.
Parameter

Safety Gym

Locomotion

Discount factor γ
Trust region size ϵ
Length of replay buffer
Critic learning rate
Trace-decay λ
Entropy coefficient β
The number of critic atoms M
The number of target atoms M ′
Constraint risk level α
threshold dk
Slack coefficient ζ

0.99
0.001
105
0.0003
0.97
0.0
25
50
0.25, 0.5, and 1.0
0.025/(1 − γ)
-

0.99
0.001
105
0.0003
0.97
0.001
25
50
1.0
[0.025, 0.025, 0.4]/(1 − γ)
mink dk = 0.025/(1 − γ)

Since the range of the cost is [0, 1], the maximum discounted cost sum is 1/(1 − γ). Thus, the
threshold is set by target cost rate times 1/(1 − γ). For the locomotion tasks, the third cost in (46)
is designed for foot stamping, which is not essential to safety. Hence, we set the threshold to near
the maximum (if a robot does not stamp, the cost rate becomes 0.5). In addition, baseline safe RL
methods use multiple critic networks for the cost function, such as target [Yang et al., 2021] or square
value networks [Kim and Oh, 2022a]. To match the number of network parameters, we use two critics
as an ensemble, as in Kuznetsov et al. [2020].
Tips for Hyperparameter Tuning.
• Discount factor γ, Critic learning rate: Since these are commonly used hyperparameters, we
do not discuss these.
• Trace-decay λ, Trust region size ϵ: The ablation studies on these hyperparameters are
presented in Appendix C.3. From the results, we recommend setting the trace-decay to
0.95 ∼ 0.99 as in other TD(λ)-based methods [Precup et al., 2000]. Also, the results show
that the performance is not sensitive to the trust region size. However, if the trust region size
is too large, the approximation error increases, so it is better to set it below 0.003.
• Entropy coefficient β: This value is fixed in our experiments, but it can be adjusted automatically as done in SAC [Haarnoja et al., 2018].
• The number of atoms M, M ′ : Although experiments on the number of atoms did not
performed, performance is expected to increase as the number of atoms increases, as in
other distributional RL methods [Dabney et al., 2018a].
• Length of replay buffer: The effect of the length of the replay buffer can be confirmed
through the experimental results from an off policy-based safe RL method [Kim and Oh,
2022a]. According to that, the length does not impact performance unless it is too short. We
recommend setting it to 10 to 100 times the collected trajectory length.
• Constraint risk level α, threshold dk : If the cost sum follows a Gaussian distribution,
the mean-std constraint is identical to the CVaR constraint. Then, the probability of the
26

worst case can be controlled by adjusting α. For example, if we set α = 0.125 and
d = 0.03/(1 − γ), the mean-std constraint enforces the probability that the average cost
is less than 0.03 during an episode greater than 95% = Φ(ϕ(Φ−1 (α))/α). Through this
meaning, proper α and dk can be found.
• Slack coefficient ζ: As mentioned at the end of Section 3.1, it is recommended to set this
coefficient as large as possible. Since dk − ζ should be positive, we recommend setting ζ to
mink dk .
In conclusion, most hyperparameters are not sensitive, so few need to be optimized. It seems that α
and dk need to be set based on the meaning described above. Additionally, if the approximation error
of critics is significant, the trust region size should be set smaller.

27

C

Experimental Results

C.1

Safety Gym

In this section, we present the training curves of the Safety Gym tasks separately according to the
risk level of constraints for better readability. Figure 7 shows The training results of the risk-neutral
constrained algorithms and risk-averse constrained algorithms with α = 1.0. Figures 8 and 9 show
the training results of the risk-averse constrained algorithms with α = 0.25 and 0.5, respectively.

Figure 7: Training curves of risk-neutral constrained algorithms for the Safety Gym tasks. The solid
line and shaded area represent the average and std values, respectively. The black dashed lines in the
second row indicate thresholds. All methods are trained with five random seeds.

Figure 8: Training curves of risk-averse constrained algorithms with α = 0.5 for the Safety Gym.

28

Figure 9: Training curves of risk-averse constrained algorithms with α = 0.25 for the Safety Gym.
C.2

Ablation Study on Components of SDAC

There are three main differences between SDAC and the existing trust region-based safe RL algorithm
for mean-std constraints [Kim and Oh, 2022a], called OffTRC: 1) feasibility handling methods in
multi-constraint settings, 2) the use of distributional critics, and 3) the use of Q-functions instead of
advantage functions, as explained in Appendix A.8 and A.10. Since the ablation study for feasibility
handling is conducted in Section 5.3, we perform ablation studies for the distributional critic and
Q-function in this section. We call SDAC with only distributional critics as SDAC-Dist and SDAC
with only Q-functions as SDAC-Q. If all components are absent, SDAC is identical to OffTRC [Kim
and Oh, 2022a]. The variants are trained with the point goal task of the Safety Gym, and the training
results are shown in Figure 10. SDAC-Q lowers the cost rate quickly but shows the lowest score.
SDAC-Dist shows scores similar to SDAC, but the cost rate converges above the threshold 0.025. In
conclusion, SDAC can efficiently satisfy the safety constraints through the use of Q-functions and
improve score performance through the distributional critics.

Figure 10: Training curves of variants of SDAC for the point goal task.

29

C.3

Ablation Study on Hyperparameters

To check the effects of the hyperparameters, we conduct ablation studies on the trust region size
ϵ and entropy coefficient β. The results on the entropy coefficient are presented in Figure 11a,
showing that the score significantly decreases when β is 0.01. This indicates that policies with high
entropy fail to improve score performance since they focus on satisfying the constraints. Thus, the
entropy coefficient should be adjusted cautiously, or it can be better to set the coefficient to zero. The
results on the trust region size are shown in Figure 11b, which shows that the results do not change
significantly regardless of the trust region size. However, the score convergence rate for ϵ = 0.01 is
the slowest because the estimation error of the surrogate increases as the trust region size increases
according to Theorem A.8.

(a) Entropy coefficient β.

(b) Trust region size ϵ.

Figure 11: Training curves of SDAC with different hyperparameters for the point goal task.

30

D

Comparison with RL Algorithms

In this section, we compare the proposed safe RL algorithm with traditional RL algorithms in the
locomotion tasks and show that safe RL has the advantage of not requiring reward tuning. We use
the truncated quantile critic (TQC) [Kuznetsov et al., 2020], a state-of-the-art algorithm in existing
RL benchmarks [Todorov et al., 2012], as traditional RL baselines. To apply the same experiment to
traditional RL, it is necessary to design a reward reflecting safety. We construct the reward through
P3
P3
a weighted sum as R̄ = (R − i=1 wi Ci )/(1 + i=1 wi ), where R and C{1,2,3} are used to train
safe RL methods and are defined in Appendix B, and R is called the true reward. The weights of the
reward function w{1,2,3} are searched by a Bayesian optimization tool1 to maximize the true reward
of TQC in the Mini-Cheetah task. Among the 63 weights searched through Bayesian optimization,
the top five weights are listed in Table 4.
Table 4: Weights of the reward function for the Mini-Cheetah task.
Reward weights

w1

w2

w3

#1
#2
#3
#4
#5

1.588
1.340
1.841
6.560
1.603

0.299
0.284
0.545
0.187
0.448

0.174
0.148
0.951
4.920
0.564

Figure 12 shows the training curves of the Mini-Cheetah task experiments where TQC is trained
using the weight pairs listed in Table 4. The graph shows that it is difficult for TQC to lower the
second cost below the threshold while all costs of SDAC are below the threshold. In particular, TQC
with the fifth weight pairs shows the lowest second cost rate, but the true reward sum is the lowest.
This shows that it is challenging to obtain good task performance while satisfying the constraints
through reward tuning.

Figure 12: Training curves of the Mini-Cheetah task. The black dashed lines show the thresholds
used for the safe RL method. The solid line represents the average value, and the shaded area shows
one-fifth of the std value. The number after TQC in the legend indicates which of the reward weights
in Table 4 is used. All methods are trained with five different random seeds.

1

We use Sweeps from Weights & Biases Biewald [2020].

31

E

Computational Cost Analysis

E.1

Complexity of Gradient Integration Method

In this section, we analyze the computational cost of the gradient integration method. The proposed
gradient integration method has three subparts. First, it is required to calculate policy gradients
of each cost surrogate, gk , and H −1 gk for ∀k ∈ {1, 2, ..., K}, where H is the Hessian matrix of
the KL divergence. H −1 gk can be computed using the conjugate gradient method, which requires
only a constant number of back-propagation on the cost surrogate, so the computational cost can be
expressed as K · O(BackProp).
Second, the quadratic problem in Section 3.1 is transformed to a dual problem, where the transformation process requires inner products between gk and H −1 gm for ∀k, m ∈ {1, 2, ..., K}. The
computational cost can be expressed as K 2 · O(InnerProd).
Finally, the transformed quadratic problem is solved in the dual space ∈ RK using a quadratic
programming solver. Since K is usually much smaller than the number of policy parameters, the
computational cost almost negligible compared to the others. Then, the cost of the gradient integration
is K · O(BackProp) + K 2 · O(InnerProd) + C. Since the back-propagation and the inner products
is proportional to the number of policy parameters |ψ|, the computational cost can be simplified as
O(K 2 · |ψ|).
E.2

Quantitative Analysis

Table 5: Training time of Safe RL algorithms (in hours). The training time of each algorithm is
measured as the average time required for training with five random seeds. The total training steps
are 5 · 106 and 3 · 106 for the point goal task and the Mini-Cheetah task, respectively.
Task

SDAC (proposed)

OffTRC

WCSAC

CPO

CVPO

Point goal (Safety Gym)
Mini-Cheetah (Locomotion)

7.96
8.36

4.86
6.54

19.07
16.41

2.61
1.99

47.43
-

We analyze the computational cost of the proposed method quantitatively. To do this, we measure
the training time of the proposed method, SDAC, and the safe RL baselines. We use a workstation
whose CPU is the Intel Xeon e5-2650 v3, and GPU is the NVIDIA GeForce GTX TITAN X. The
results are presented in Table 5. While CPO is the fastest algorithm, its performance, such as the
sum of rewards, is relatively poor compared to other algorithms. The main reason why CPO shows
the fastest computation time is that CPO is an on-policy algorithm, hence, it does not require an
insertion to (and deletion from) a replay memory, and batch sampling. SDAC shows the third fastest
computation time in all algorithms and the second best one among off-policy algorithms. Especially,
SDAC is slightly slower than OffTRC, which is the fastest one among off-policy algorithms. This
result shows the benefit of SDAC since SDAC outperforms OffTRC in terms of the returns and CV,
but the training time is not significantly increased over OffTRC. WCSAC, which is based on SAC, has
a slower training time because it updates networks more frequently than other algorithms. CVPO, an
EM-based safe RL algorithm, has the slowest training time. In the E-step of CVPO, a non-parametric
policy is optimized to solve a local subproblem, and the optimization process requires discretizing
the action space and solving a non-linear convex optimization for all batch states. Because of this,
CVPO takes the longest to train an RL agent.

32

