{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classic_rl.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/haarnoja/sac (SAC), https://github.com/deepmind/dqn (DQN)\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Papers: \n",
    "# 1. \"Playing Atari with Deep Reinforcement Learning\" (DQN)\n",
    "# 2. \"Continuous control with deep reinforcement learning\" (DDPG)\n",
    "# 3. \"Soft Actor-Critic\" (SAC)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)\n",
    "    Innovation: Q-Learning with Neural Networks + Experience Replay\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDPG(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Continuous control with deep reinforcement learning (Lillicrap et al., 2015)\n",
    "    Innovation: DQN for continuous action spaces (Actor-Critic)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_dim), nn.Tanh() # Action range [-1, 1]\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "class SAC(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Soft Actor-Critic (Haarnoja et al., 2018)\n",
    "    Innovation: Entropy Regularization for exploration/stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.zeros(1)) # Learnable temperature\n",
    "        \n",
    "        # Double Q-Learning (Two Critics) to reduce overestimation\n",
    "        self.critic1 = nn.Sequential(nn.Linear(state_dim + action_dim, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "        self.critic2 = nn.Sequential(nn.Linear(state_dim + action_dim, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "        \n",
    "        # Stochastic Policy\n",
    "        self.actor_fc = nn.Sequential(nn.Linear(state_dim, 256), nn.ReLU())\n",
    "        self.mu = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        x = self.actor_fc(state)\n",
    "        mu = self.mu(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20, 2)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        # Reparameterization trick (tanh squash)\n",
    "        u = dist.rsample()\n",
    "        action = torch.tanh(u)\n",
    "        \n",
    "        # Enforce entropy correction for tanh\n",
    "        log_prob = dist.log_prob(u) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action, log_prob.sum(dim=-1, keepdim=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}