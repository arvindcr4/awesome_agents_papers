Transformer-Based Scalable Multi-Agent Reinforcement Learning for
Networked Systems with Long-Range Interactions

arXiv:2511.13103v1 [cs.LG] 17 Nov 2025

Vidur Sinha1 , Muhammed Ustaomeroglu1 , Guannan Qu1
Abstract— Multi-agent reinforcement learning (MARL) has
shown promise for large-scale network control, yet existing
methods face two major limitations. First, they typically rely
on assumptions leading to decay properties of local agent
interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks.
Second, most approaches lack generalizability across network
topologies, requiring retraining when applied to new graphs.
We introduce STACCA (Shared Transformer Actor–Critic with
Counterfactual Advantage), a unified transformer-based MARL
framework that addresses both challenges. STACCA employs a
centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared
Graph Transformer Actor learns a generalizable policy capable
of adapting across diverse network structures. Further, to
improve credit assignment during training, STACCA integrates
a novel counterfactual advantage estimator that is compatible
with state-value critic estimates. We evaluate STACCA on
epidemic containment and rumor-spreading network control
tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of
transformer-based MARL architectures to achieve scalable and
generalizable control in large-scale networked systems.

I. I NTRODUCTION
Multi-agent networked systems are ubiquitous in modern infrastructure, spanning power grids, transportation networks, social platforms, and beyond [4], [2]. These systems
involve large numbers of interacting agents whose states and
actions are coupled by the underlying network topology and
dynamics. Although most interactions are local (e.g. power
flowing between connected nodes in an electrical grid or diseases spreading through contact), they can sometimes trigger
long-range effects, such as cascading power failures or rapid
epidemic outbreaks. Designing local decision policies that
capture complex long-range dependencies among agents is
essential for effective control in large networked systems.
Multi-agent reinforcement learning (MARL) has emerged
as a powerful paradigm for such networked control problems.
There are many general MARL algorithms (e.g. MADDPG
[6], MAPPO [14]); however, these are known to suffer from
scalability issues, as the factorized state space in networked
systems grows exponentially. More recently, Qu et al. proved
decay properties under the local interaction structure of networked systems and exploited them to develop the Scalable
Actor Critic framework [7] and applied it to decentralized
voltage control in power grids [13]. Despite the progress, two
key challenges remain in networked multi-agent systems:
1 Vidur Sinha, Muhammed Ustaomeroglu, and Guannan Qu are with
the Department of Electrical and Computer Engineering, Carnegie
Mellon University, Pittsburgh, USA. Correspondence to: Vidur Sinha
<vidursin@andrew.cmu.edu>.

(1) Long-Range Interactions. Existing MARL frameworks for networked systems rely on assumptions that show
long-range network interactions have negligible impact on
the system [7]. However, in many cases, these assumptions
do not hold, and there can be long-range interactions (e.g. a
few infected travelers can spark outbreaks in geographically
distant regions). Thus, capturing long-range interactions becomes crucial for developing successful policies.
(2) Network Generalizability. Current solutions are typically trained and evaluated on fixed networks, with limited ability to generalize to unseen topologies. Achieving
network generalizability under decentralized execution has
tremendous implications for scalability, but it is especially
difficult in real-world networks where local neighborhoods
are heterogeneous. Moreover, the large number of agents exacerbates the infamous credit-assignment problem in MARL,
making learning such policies incredibly challenging.
These challenges motivate the following question: Can we
create a MARL solution for networked systems that handles
long-range interactions and is network generalizable?
Contribution. To address this question, we introduce
STACCA (Shared Transformer Actor–Critic with Counterfactual Advantage), a unified transformer-based MARL
framework. Transformers are known for their ability to model
long-range dependencies, and their core self-attention mechanism has been adapted to many domains, including graphs
[10][11]. STACCA employs a centralized Graph Transformer
Critic that learns global, long-range dependencies among
agents, providing system-level context to aid policy learning
and enhance local decisions. Complementing this, a shared
Graph Transformer Actor learns a policy that effectively
stacca (“detaches” in Italian) from the specific network
topology seen during training, enabling zero-shot transfer
to unseen network topologies. However, to both encourage
the critic to learn long-range dependencies and expose the
actor to diverse local network structures, we must train on
relatively large networks which amplifies the challenge of
multi-agent credit assignment. To address this, STACCA
integrates a novel counterfactual advantage that leverages the
global critic’s state-value estimates to isolate each agent’s
contribution. Overall, our proposed method mitigates key
drawbacks of prior MARL approaches in networked systems
by jointly addressing long-range interaction modeling, network generalization, and credit-assignment.
Our experiments on epidemic containment and rumorspreading network control tasks demonstrate that STACCA
substantially outperforms the baseline MAPPO algorithm
and showcase its scalability and generalizability benefits.

These results highlight the potential of transformer-based
MARL architectures to achieve scalable, transferable, and
effective control in complex, large-scale networked systems.
II. P ROBLEM F ORMULATION AND BACKGROUND
In this section, we first formulate the problem as a
Networked Decentralized Partially Observable Markov Decision Process (Dec-POMDP), and use it to introduce two
network control examples: epidemic containment and rumor
spreading. We then review key ideas regarding Multi-Agent
Proximal Policy Optimization (MAPPO), the Transformer
architecture, and Graph Attention Networks (GATs).
A. Problem Formulation
We adopt a Dec-POMDP framework for networked systems (based on [7]). Consider a graph G = (N , E), where
nodes N = {1, . . . , N } are agents and E are the edges.
The global state at time t is the collection of individual
agent states, st = (s1,t , . . . , sN,t ) ∈ S = S1 × · · · ×
SN , with si,t ∈ Si . Similarly, the global action is at =
(a1,t , . . . , aN,t ) ∈ A = A1 × · · · × AN , and the global
observation is ot = (o1,t , . . . , oN,t ). Each agent’s local
observation is given by the states of the agents in its closed
(includes i) k-hop neighborhood Nik and the edges of the
corresponding subgraph: oi,t = (sNik ,t , ENik ).
A key property of our model is that state transitions
factorize across the closed 1-hop neighborhood of each
agent.
P (st+1 |st , at ) =

N
Y

Pi (si,t+1 |sNi1 ,t , aNi1 ,t )

(1)

i=1

Further, we denote r(s, a) as a team reward and γ is
the discount factor. In MARL, typically each agent acts
according to a local/decentralized policy πi (ai |oi ), using
only its local observation oi under a memoryless policy πi .
The objective is to maximize the expected team return
"∞
#
X
t
J(π) = Eπ
γ r(st , at ) ,
(2)
t=0

where π = (π1 , . . . , πN ). Correspondingly, the global statevalue function under policy π is defined as
" ∞
#
X
k
Vπ (st ) = Eπ
γ r(st+k , at+k ) st ,
(3)
k=0

and the advantage of taking joint action at in state st is
Aπ (st , at ) = Qπ (st , at ) − Vπ (st ),
(4)
P∞ k
with Qπ (st , at ) = Eπ [ k=0 γ r(st+k , at+k )|st , at ].
This general formulation applies to diverse networked
systems, including power grid management, social network
dynamics, and epidemic control. In this work, we focus on
two representative and contrasting control tasks to demonstrate its utility.
Example 1 (Epidemic Containment). In this task, the
goal is to minimize the spread of a disease by managing

some control level (e.g. quarantine/vaccination intensity).
The state of agent i is si,t = (hi,t , ci,t ), where hi,t ∈
{0(susceptible), 1(infected)} is its health status and ci,t ∈
[0, 1] is its control value. The action ai,t ∈ {−∆c, 0, ∆c} is
to decrease, maintain, or increase the control value. The transition of the state si,t → si,t+1 happens in two parts. Firstly,
the health status hi,t transitions probabilistically based on
the current state. Let Ii,t = |{j ∈ Ni1 \ {i} : hj,t = 1}|
denote the number of infected neighbors. The probability of
an agent being susceptible at the next timestep is given by:
(
δ
P (hi,t+1 = 0 | st ) =
(1 − β(ci,t ))Ii,t

if hi,t = 1,
(5)
if hi,t = 0,

where δ is the fixed probability of recovery. The effective
transmission rate β(ci,t ) = (1 − ηci,t )β0 is a function of a
base transmission rate β0 , the agent’s current control state
ci,t , and the control effectiveness η (e.g. for η = 0.9,
full-control provides at most 90% reduction in infection
probability for 1 infected neighbor). Secondly, the control
level updates deterministically based on the action: ci,t+1 =
clip(ci,t + ai,t , 0, 1). The reward function is structured to
penalize the global infection rate and control costs.
Example 2 (Rumor Spreading). In this task, the goal is to
maximize the spread of information by managing advertising
efforts. The state of agent i is si,t = (hi,t , ci,t ), where hi,t ∈
{0(unaware), 1(aware)} is its awareness status and ci,t ∈
[0, 1] is its advertising level, which we call the “boostingfactor.” The action ai,t ∈ {−∆c, 0, ∆c} is to decrease,
maintain, or increase the boosting-factor. The awareness status transitions probabilistically, where the dynamics follow
a “viral marketing” scenario with market saturation. Let
Ii,t = |{j ∈ Ni1 \ {i} : hj,t = 1}| be the number of aware
neighbors. The probability of an agent becoming unaware is
then:
(
0
if hi,t = 1
P (hi,t+1 = 0 | st ) =
Ii,t
(1 − β(st , ci,t ))
if hi,t = 0
(6)
The transmission probability β(st , ci,t ) = ci,t (1 − h̄t )κ β0
now depends on the base spreading rate β0 , the boostingfactor state ci,t , the fraction of aware nodes h̄t =
PN
1
j=1 hj,t , and a saturation exponent κ > 0. Note that the
N
saturation component in this example explicitly incorporates
long-range interactions into the transition dynamics, which
violates eq. (1), but provides a contrasting variation to the
purely local dynamics of the epidemic example. Finally, the
boosting-factor updates deterministically: ci,t+1 = clip(ci,t +
ai,t , 0, 1). Here, a reward function is designed to encourage
the aware state while penalizing the cost of the global
boosting-factor usage.
These examples highlight two core challenges we aim
to address. First, the problem of long-range interactions,
where local agent states and actions have complex, non-local
consequences. Second, the need for network-generalizability,
as an ideal control policy should be transferable and effective

both across local observations within the Dec-POMDP and
across entire network topologies without requiring retraining.
Our proposed STACCA framework is designed to address
these challenges by drawing from several methods in the
literature. We now review the relevant background on MultiAgent Proximal Policy Optimization (MAPPO), the Transformer architecture, and Graph Attention Networks (GATs).

The core of the mechanism is computing a pairwise score
between each query and all keys, which is then scaled and
normalized via softmax to produce attention weights. The
final output is a weighted sum of the values.


QK ⊤
V.
(10)
Attention(Q, K, V ) = softmax √
dk
|
{z
}
Attention Weights

B. Background: Multi-Agent Proximal Policy Optimization
Multi-Agent Proximal Policy Optimization (MAPPO)[14]
is an on-policy actor–critic algorithm that adopts the Centralized Training with Decentralized Execution (CTDE)
paradigm [5]. Under CTDE, a centralized critic has access
to global information (e.g., the joint state of the entire graph
G) during training, providing a more stable and informative
learning signal. During execution, however, each agent i
employs a decentralized actor policy πθ (ai |oi ) to select
actions using only its local observation oi . Here, we have
used parameter sharing – all agents use the same policy
neural network parameterized by θ.
At each training round, suppose the current actor parameter is θold . We collect a batch of trajectories D by executing
this policy in the environment, where at each timestep t in
trajectory τ , agent i samples an action aτi,t ∼ πθold (·|oτi,t )
based on its local observation. The next actor parameter is
obtained by optimizing the clipped surrogate objective over
all agents, trajectories, and timesteps:
h
LCLIP
(θ)
=
Ê
min ρτi,t (θ) Âτt ,
i,τ,t
π
(7)
 i
clip ρτi,t (θ), 1 − ϵ, 1 + ϵ Âτt ,
where Êi,τ,t [. . . ] denotes the empirical average over the
samples (sτi,t , aτi,t , oτi,t ), ρτi,t (θ) is the probability ratio

πθ (aτi,t |oτi,t )
τ
πθold (aτi,t |oτi,t ) , ϵ is the clipping hyperparameter, and Ât is an
estimate of the advantage function Aπold (sτt , aτt ) (see eq. (4))

which is usually shared among agents at each timestep. This
estimate is typically computed via Generalized Advantage
Estimation (GAE) [8], which uses the critic Vϕ (sτt ) as a
baseline value function. The critic parameters ϕ are updated
by minimizing the loss (e.g. mean squared error) between
predicted and bootstrapped returns Rtτ (see [8] for details):
h
i
2
LV (ϕ) = Êτ,t (Vϕ (sτt ) − Rtτ ) .
(8)
This process of collecting trajectories and updating the actor
and critic is repeated for multiple training iterations.
C. Background: Transformers and the Attention Mechanism
The Transformer architecture has been shown to be especially powerful in capturing long-range dependencies through
the self-attention mechanism [10]. Given a set of input
embeddings X = [x1 , . . . , xn ], they are first projected into
queries (Q), keys (K), and values (V) using learned weight
matrices:
Q = XWQ ,

K = XWK ,

V = XWV .

(9)

where dk denotes the dimensionality of the key (and query)
vectors. This mechanism’s global receptive field is critical for
a centralized MARL critic to model system-wide, long-range
interactions. Multi-head attention performs this computation
in parallel, capturing diverse interaction patterns that are
especially useful for a shared actor learning a generalizable
policy. Furthermore, recent theoretical work investigating
self-attention through the lens of interacting entities has
provided a strong motivation for its application in modeling
the complex dynamics of multi-agent systems[9].
D. Background: Graph Attention Networks
While the global self-attention mechanism is powerful,
most networked systems exhibit a known graph structure,
such as physical or communication networks. To leverage
this topology as an inductive bias, we employ the Graph
Attention Network (GAT) architecture, which adapts selfattention to graph domains[11].
In a GAT, instead of attending to all other agents in
the system, each agent i computes attention coefficients αij
exclusively over its connected neighbors j ∈ Ni .

exp LeakyReLU a⊤ [Whi ∥ Whj ]
. (11)
αij = P
⊤
k∈Ni exp(LeakyReLU (a [Whi ∥ Whk ]))
This approach elegantly combines the flexibility of learned,
attention-based weighting with the strong structural prior of
the graph. For our shared actor, this allows agents to learn a
generalizable, structure-aware policy by dynamically weighting information from relevant neighbors. For the centralized
critic, stacking GAT layers enables information to propagate
across the graph, allowing it to build a global value estimate
that still respects the underlying network topology.
III. M ETHODS
Shared Transformer Actor–Critic with Counterfactual Advantage (STACCA) is designed to address the two key
challenges: modeling long-range interactions and achieving
network-generalizability in network control problems. In
Section III-A, we introduce (1) a graph-transformer critic
architecture designed to learn global, long-range dependencies among agents to guide local policy learning and (2) a
graph-transformer actor architecture designed to adapt to heterogeneous local neighborhoods for network-generalizability.
Furthermore, in addressing the above challenges, we require
training on relatively large graphs to both capture longrange interactions and expose the actor to diverse local
network topologies. This, in turn, exacerbates the significant
challenge of multi-agent credit assignment. To address this,
we introduce a novel counterfactual advantage which we

cover in Section III-B. Finally, we present the complete
STACCA framework in Section III-C.
A. Graph Transformer Actor and Critic Architectures
To address the challenges of modeling long-range interactions and learning a network-generalizable shared policy,
we introduce critic and actor networks, respectively, based
on a hybrid Graph Transformer architecture. Both networks
combine the strengths of Graph Attention Networks (GATs)
for leveraging the explicit graph structure with the power
of Transformer self-attention for learning (potentially) longrange global dependencies. However, while sharing this
foundational structure, the actor and critic differ in their
scope and final aggregation mechanisms, tailored to their
distinct roles (See fig. 1).
The centralized Graph Transformer Critic processes the
entire graph to learn the global state-value function Vϕ (s).
Each node i begins with raw features si , which are first
projected through an MLP embedding transformation f (·)
to obtain initial node embeddings ei = f (si ). These embeddings are then refined by a stack of GAT layers, producing
structure-aware representations hi = g(ei ). The resulting
collection H = [h1 , . . . , hN ]⊤ serves as the Transformer
Encoder input (X in eq. (9)). This two-stage process of
extracting a structural inductive bias through the GAT layers
then refining the embeddings with Transformer Encoder to
model long-range interactions serves as the backbone for
STACCA’s critic (and actor) architectures. Following the
GAT and Transformer layers, it employs an attentional aggregation mechanism to produce a single embedding vector
representing the entire graph state. This learned, weighted
average of all node embeddings creates a stable and holistic representation, which is then passed to a feed-forward
network to produce the final scalar value estimate.
The decentralized Graph Transformer Actor is a shared
policy network πθ (ai |oi ) applied independently at each node
to produce an action distribution. The structure of the network is similar to the critic, except the input is agent/node
i’s local observation oi . The attention-based architecture
allows the shared policy to adapt naturally to heterogeneous
local topologies. After neighborhood processing, the actor
selects the ego-node embedding, which is now infused with
topological context from the GAT and Transformer layers,
as input to the policy head, enabling specialized, contextsensitive behaviors. In contrast, an averaging mechanism–
while increasing stability in the critic–can potentially push
agents into more uniform, less specialized behaviors in the
actor by "smoothing out" the local context.
B. Counterfactual Advantage Calculation
Counterfactual reasoning [3] is a powerful technique used
to address the multi-agent credit assignment problem by
isolating each agent’s contribution to the team rewards. It
does so through an advantage formulation that compares an
agent’s action to an individualized counterfactual baseline –
what would have happened if the agent had acted differently
– as opposed to the global value function baseline shown in

Fig. 1: Graph Transformer Critic (left) and Actor
(right) Architectures. Node i is represented by the
blue node. Node i’s 1-hop neighborhood (green
nodes) is its local observation at time t, oi,t .

eq. (4). However, existing methods that use this idea typically
rely on learning a joint action-value critic Q(s, a). These
methods scale poorly with the number of agents and are
incompatible with the more stable state-value critic Vϕ (s)
used in MAPPO.
We introduce a novel calculation procedure and baseline for counterfactual advantages that is compatible with
MAPPO’s state-value critic and is effective even with a large
number of agents. While a state-value critic Vϕ (s) enhances
stability compared to a joint action-value critic Qϕ (s, a),
it does not directly provide the action-dependent values
needed for counterfactual reasoning. Our method extracts this
information using an efficient, three-step procedure:
1. One-step Trajectory Branching. For each agent i
at each timestep t, we generate one-step counterfactuals.
We iterate through every possible action a′i ∈ Ai and
compute the counterfactual next state s′t+1 (a′i ) that would
have occurred if agent i had taken action a′i while all
other agents’ actions a−i,t remained the same (See fig. 2).
For continuous actions, this can still be accomplished by
discretizing or sampling from the action space.
2. Compute Advantages. We use the centralized critic Vϕ
to compute the counterfactual advantage. This is the difference between the one-step return of the agent’s taken action
and the policy-weighted average of the one-step returns of

all its possible actions:
ÂCF
i,t = [rt + γVϕ (st+1 )]
−

X

πθ (a′i |oi,t )[rt′ (a′i ) + γVϕ (s′t+1 (a′i ))] (12)

a′i ∈Ai

efficiently extracts action-dependent values from a statevalue critic Vϕ (s) and processes them into effective learning
signals. This enables compatibility with a broader variety of
MARL algorithms and scales well to many agents.
C. Proposed Method: STACCA

where rt and rt′ (a′i ) are the rewards from the actual and counterfactual actions, respectively. This provides each agent with
a personalized baseline, isolating its individual contribution.

Fig. 2: One-step trajectory branching for Node 2 in a 3
node epidemic containment environment.

We now present the complete STACCA framework integrated into the base MAPPO training loop; however,
STACCA is designed as a modular framework, and its core
components can be integrated into various actor-critic MARL
algorithms.
Specifically, the Graph Transformer Actor serves as the
policy network πθ , and the Graph Transformer Critic serves
as the value network Vϕ . The standard GAE advantage
calculation used in baseline MAPPO is replaced by our
counterfactual advantage, ÃCF . This agent-specific advantage is then used to update the shared actor network πθ via
the clipped surrogate objective (14). The centralized critic
Vϕ is trained concurrently by minimizing the huber loss or
mean-squared error between its value predictions and the
bootstrapped returns Rt over a set of trajectories D. The
high-level training loop is presented in Algorithm 1.
Algorithm 1 STACCA w/ MAPPO Training Loop
1: Initialize Graph Transformer actor πθ and critic Vϕ .

3. Amplify credit-assignment signal. A stable, lowvariance critic may produce only small differences in value
between these counterfactual branches. To amplify these crucial learning signals, we introduce an timestep-level normalization step. At each timestep, we normalize the computed
advantages across all N agents:
ÃCF
i,t =

ÂCF
i,t − µAt
σA t + ϵ

(13)

where µAt and σAt are the mean and standard deviation of
the advantages of all agents at timestep t. This procedure rescales each agent’s contribution relative to the team’s average
performance at that specific moment and is particularly
effective in systems with many agents, where these crossagent statistics are meaningful.
Finally, to integrate these agent-specific normalized advantages into the clipped surrogate objective (7), we use the
following updated objective:
h
LCLIP,CF
(θ) = Êi,τ,t min ρτi,t (θ) Ãτ,CF
,
π
i,t

i
.
clip ρτi,t (θ), 1 − ϵ, 1 + ϵ Ãτ,CF
i,t
(14)
This procedure is computationally efficient. In our networked environments, an agent’s action has a localized
effect, so constructing each counterfactual state is an O(1)
operation. The total complexity per timestep is thus O(AN )
for all agents (where A = |Ai |), which does not increase the
asymptotic complexity of the MAPPO training loop.
Our approach builds on the principles of counterfactual
reasoning, most notably introduced by COMA [3]. The onestep trajectory branching with timestep-level normalization

2: for each training iteration do
3:
Collect trajectories D and one-step counterfactual

states C with policy πθ for all agents.
Compute state values for D and C with Vϕ .
Compute bootstrapped returns using Generalized
Advantage Estimation, Rtτ , ∀τ, t.
6:
Compute counterfactual advantages Ãτ,CF
, ∀τ, i, t
i,t
using (12) (13).
7:
for Kπ epochs do
(θ) on D.
8:
Update πθ by minimizing LCLIP,CF
π
9:
end for
10:
for KV epochs do
11:
Update Vϕ by minimizing LV (ϕ) on D.
12:
end for
13: end for
4:
5:

IV. E XPERIMENTS AND A NALYSES
Our experiments are designed to answer several key questions regarding the framework’s actor and critic architectures, credit assignment mechanism, and overall effectiveness. (1) Long-Range Interactions: Do the long-range
interactions captured by the critic’s global self-attention
and the structural inductive biases learned by GAT layers
lead to better policies? (2) Network-Generalizability: Does
STACCA’s shared actor architecture improve policy learning
and successfully generalize to network topologies and scales
unseen during training? (3) Credit Assignment: Does our
proposed counterfactual advantage effectively address the
multi-agent credit assignment problem, leading to improved
and more stable learning for decentralized policies?
To answer these questions, we begin by describing the
training environments. We then present a comprehensive set

of ablation studies to validate the individual contributions
of STACCA’s core components. Finally, we conduct extensive generalization and scalability experiments to evaluate
STACCA’s ability to transfer policies to networks of varying
structures and sizes. Our results demonstrate that STACCA
not only outperforms baseline architectures but also learns
robust, generalizable policies that are effective even under
significant changes in the network environment.
A. Training Environments
For all training experiments, we use a 50-node BarabásiAlbert (BA) [1] graph with the attachment parameter m = 1.
This topology was chosen for its diverse degree distribution,
which features a few high-degree hubs and many lowdegree nodes. This heterogeneity exposes the actor to a
wide variety of local neighborhood structures during training.
Furthermore, the BA (m = 1) graph maintains a relatively
high diameter, creating scenarios where the critic must learn
long-range credit assignment to inform the actor’s policy.
In both environments, each agent observes only its 1-hop
local neighborhood. The action space is discrete, allowing
agents to either increase, decrease, or maintain their current
control (or boosting) factor by increments of 0.1, with values
clipped to the range [0, 1]. In the actor input, each node/agent
comprises of five features: (1) a binary indicator identifying
the ego node, (2) the node’s state—infected/susceptible in the
epidemic setting or aware/unaware in the rumor-spreading
setting, (3) the current control or boosting-factor level, (4)
the node degree, and (5) the shortest-path distance from the
ego node. The critic input includes the corresponding nodelevel features for all nodes, excluding the ego-node indicator
and distance features, since these are defined relative to each
agent’s local perspective.
1) Epidemic Containment Environment: In the Epidemic
Containment environment, agents apply control to limit the
spread of an infection. Each training episode begins with
three randomly selected seed nodes, a base transmission rate
of 0.15, and a control effectiveness of 0.9. The reward function is designed to balance infection mitigation with costefficiency. An exponential penalty is applied to control usage,
encouraging initial control usage, but discouraging excessive
intervention. A softened catastrophe penalty, implemented as
a flipped sigmoid function, penalizes the system when the
number of infected nodes approaches an epidemic threshold.
This is supplemented by a smaller linear penalty on the total
number of infected nodes. This reward structure incentivizes
agents to use control judiciously while preventing widespread

outbreaks (see fig. 3). Additionally, a bonus of 3 per timestep
is awarded when the infection is eradicated (0 infected
nodes), incentivizing rapid and complete elimination.
2) Rumor Spreading Environment: In this environment,
the goal is to maximize the spread of information from a set
of seed nodes (3 in this case). The dynamics are governed
by a maximum transmission rate of 0.25 (at full boosting
factor) and a market saturation factor of κ = 3, which makes
spreading more difficult as more nodes become aware. For
the reward function, an exponential penalty is applied to
boosting-factor usage and a linear reward is applied to the
aware nodes, directly incentivizing propagation.
B. STACCA Ablation Experiments
We conduct a series of ablation studies to isolate and
quantify the contribution of each key component of STACCA
(see fig. 4). To evaluate the critic architecture, we compare
the STACCA critic to a version with the global attention
mechanism removed (GAT-only) and a version with both
graph-based and global attention removed (a standard MLP
architecture). To evaluate the actor Architecture, we compare
the graph-based STACCA actor to an MLP-based actor.
Finally, we compare STACCA’s counterfactual advantage
to the standard Generalized Advantage Estimation (GAE)
with trajectory-level normalization, as is conventionally performed in MAPPO.
1) Critic Comparison: Incorporating the graph-based and
global attention mechanisms demonstrated a clear performance benefit. The MLP critic performed sub-optimally,
highlighting the value of the structural inductive bias from
GAT layers. Adding the global transformer on top of the
GAT layers yielded further improvements, with a more
pronounced effect in the rumor-spreading environment. This
suggests that the global attention mechanism is particularly
vital when task dynamics, such as the saturation component
in rumor spreading, depend on global state information.
2) Actor Comparison: The STACCA actor outperformed
the MLP Actor in both environments, with a more significant
improvement in the rumor-spreading task. However, the
primary benefit of the STACCA actor architecture lies in its
generalizability, a crucial feature evaluated in Section IV-C.
3) Advantage Comparison: The counterfactual advantage
had a significant impact on performance in both scenarios.
The conventional GAE, applied uniformly to all agents,
provides a weak and noisy learning signal in a system with
50 agents. This result underscores the importance of agentspecific credit assignment in large-scale MARL settings.
C. Network Generalization and Scalability Experiments

Fig. 3: Reward Shaping Visualization.

To evaluate the ability of STACCA’s learned policies to
generalize, we test policies trained on the 50-node BA (m =
1) graph on four unseen network topologies at two different
scales (100 and 1000 nodes). The test graphs include two
Watts-Strogatz (WS) graphs (k = 4) [12] with rewiring
probabilities p = 0.1 and p = 0.5, and two Barabási-Albert
(BA) graphs with m = 1 and m = 2.

Fig. 4: Ablation Experiments for STACCA: Comparing STACCA, to STACCA w/ MLP Actor, STACCA w/ MLP Critic,
STACCA w/ GAT-Only Critic, and STACCA w/ no counterfactual advantage for the epidemic containment environment (top
row) and rumor-spreading environment (bottom-row).

Fig. 5: Epidemic Containment Environment: Comparison of MLP Policy and STACCA Graph-Transformer Policy on 100node and 1000-node graphs of each of the 4 graph types. All examples use 25 infected seed nodes.

We compare the performance of the STACCA actor architecture against an identically trained MLP actor. For each
actor type, we use the highest-performing model based on
rewards from 10 independent training runs.
1) Epidemic Containment: As shown in fig. 5, the
STACCA policy demonstrates strong generalization. For the
100-node graphs, the STACCA actor successfully eradicates
the infection on all four graph types within 50 timesteps
using moderate control. In contrast, the MLP actor takes
nearly twice as long on three of the graphs with higher
control usage, and fails to eradicate the infection on the BA
(m = 2) graph, where ∼20% of nodes remain infected. At a
larger scale (1000 nodes), the STACCA actor still eradicates
the infection on three of the four graphs. On the more
challenging BA (m = 2) graph, it contains the infection
to 10-15% of the population. The MLP actor’s performance
degrades substantially at this scale, failing to eradicate the
infection on three of the four graphs and allowing severe

outbreaks (∼30% infected) on the BA (m = 2) graph.
The reduced performance on the BA (m = 2) graph,
especially at scale, can be attributed to its degree distribution.
As BA graphs grow, the degree of hub nodes increases
significantly, creating dense local neighborhoods unseen during training. This effect is more pronounced for m = 2,
where total edge density is approximately double that of
m = 1. The more uniform local structure of WS graphs
likely contributes to the more consistent performance.
We also observe an emergent behavior where the STACCA
policy maintains a low level of precautionary control even
after eradication. This behavior reduces the peak number
of infections in subsequent outbreaks. As shown in fig. 6,
this precautionary state is more effective than a naive,
uniform control initialization of equivalent global control
usage. However, the difference is minor, so we leave this
for further investigation.

the credit assignment problem at scale. Our experiments
show that STACCA successfully generalizes to networks of
diverse structures and significantly larger sizes than those
seen during training. These results highlight the potential of
transformer-based architectures to create scalable, adaptable,
and effective control policies for complex, real-world multiagent systems.
R EFERENCES

Fig. 6: Node initialization comparison on BA (m = 2) 1000
node graph. 25 infected seeds are introduced at t = 10 (left).
Visual toy example of a precautionary baseline state (right).

2) Rumor Spreading: As shown in fig. 7, the STACCA
actor generalizes effectively across different network structures. It achieves higher final spread on denser networks
(BA (m = 2), WS (p = 0.5)), effectively exploiting the
network structure. The MLP actor achieves a comparable
final spread but employs a significantly less efficient strategy with excessive boosting-factor usage. At 1000 nodes,
STACCA maintains its efficient, high-performance strategy.
While the final spread fraction is slightly lower, this is likely
an artifact of the fixed 100-timestep evaluation window.
The MLP policy continues to exhibit high-cost, inefficient
behavior at scale.

Fig. 7: Rumor Spreading Environment: Comparing STACCA
Policy and MLP Policy on final spread proportion and
average boosting-factor usage over 100 timesteps. 5 seed
nodes are used in all cases.
V. C ONCLUSION
In this work, we addressed two key challenges in the application of MARL solutions to networked systems: capturing
long-range dependencies and generalizing across network
topologies. We introduced STACCA, a framework whose
Graph Transformer Critic learns global, long-range system
dynamics, while its shared Graph Transformer Actor learns
a network-generalizable policy, enabling zero-shot transfer
to new environments. By integrating a novel counterfactual
baseline in the advantage formulation, STACCA addresses

[1] Albert-László Barabási and Réka Albert. Emergence of scaling in
random networks. science, 286(5439):509–512, 1999.
[2] Xin Chen, Guannan Qu, Yujie Tang, Steven Low, and Na Li. Reinforcement learning for selective key applications in power systems:
Recent advances and future challenges. IEEE Transactions on Smart
Grid, 13(4):2935–2958, 2022.
[3] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, and
Shimon Whiteson. Counterfactual multi-agent policy gradients. In
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 2798–2805. AAAI, 2018.
[4] Manuel Herrera, Marco Pérez-Hernández, Ajith Kumar Parlikad, and
Joaquín Izquierdo. Multi-agent systems and complex networks:
Review and applications in systems engineering. Processes, 8(3):312,
2020.
[5] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement
learning as a rehearsal for decentralized planning. Neurocomputing,
190:82–94, 2016.
[6] Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. Multi-agent actor-critic for mixed cooperative-competitive
environments. Advances in neural information processing systems, 30,
2017.
[7] Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement
learning of localized policies for multi-agent networked systems.
In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A.
Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors,
Proceedings of the 2nd Conference on Learning for Dynamics and
Control, volume 120 of Proceedings of Machine Learning Research,
pages 256–266. PMLR, 10–11 Jun 2020.
[8] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and
Pieter Abbeel. High-dimensional continuous control using generalized
advantage estimation. arXiv preprint arXiv:1506.02438, 6, 2015.
[9] Muhammed Ustaomeroglu and Guannan Qu. A theoretical study of
(hyper) self-attention through the lens of interactions: Representation,
training, generalization. In Forty-second International Conference on
Machine Learning, 2025.
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems, volume 30, pages 5998–6008, 2017.
[11] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana
Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In
International Conference on Learning Representations (ICLR), 2018.
[12] Duncan J Watts and Steven H Strogatz. Collective dynamics of ‘smallworld’networks. nature, 393(6684):440–442, 1998.
[13] Han Xu, Jialin Zheng, and Guannan Qu. A scalable networkaware multi-agent reinforcement learning framework for decentralized
inverter-based voltage control. CoRR, abs/2312.04371, 2023.
[14] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang,
Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in
cooperative, multi-agent games. In Advances in Neural Information
Processing Systems (NeurIPS) 2022, Datasets and Benchmarks Track,
volume 35, pages 24611–24624, 2022.

