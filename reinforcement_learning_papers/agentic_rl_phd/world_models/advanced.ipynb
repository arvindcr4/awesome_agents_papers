{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# advanced.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/werner-duvaud/muzero-general (MuZero), https://github.com/danijar/dreamerv3 (DreamerV3)\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Papers:\n",
    "# 1. \"Mastering Atari... by Planning with a Learned Model\" (MuZero)\n",
    "# 2. \"Mastering Diverse Domains through World Models\" (DreamerV3)\n",
    "\n",
    "class MuZeroNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: MuZero (Schrittwieser et al., 2019)\n",
    "    Innovation: Planning without a ground-truth simulator.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # 1. Representation Network (h): Obs -> Latent State\n",
    "        self.representation = nn.Sequential(nn.Conv2d(obs_dim, 64, 3), nn.ReLU(), nn.Flatten(), nn.Linear(1024, latent_dim))\n",
    "        \n",
    "        # 2. Dynamics Network (g): Latent + Action -> Next Latent + Reward\n",
    "        self.dynamics_state = nn.Linear(latent_dim + action_dim, latent_dim)\n",
    "        self.dynamics_reward = nn.Linear(latent_dim + action_dim, 1)\n",
    "        \n",
    "        # 3. Prediction Network (f): Latent -> Policy + Value\n",
    "        self.prediction_policy = nn.Linear(latent_dim, action_dim)\n",
    "        self.prediction_value = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def initial_inference(self, observation):\n",
    "        s = self.representation(observation)\n",
    "        p = self.prediction_policy(s)\n",
    "        v = self.prediction_value(s)\n",
    "        return s, p, v\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action):\n",
    "        x = torch.cat([hidden_state, action], dim=1)\n",
    "        next_s = self.dynamics_state(x)\n",
    "        reward = self.dynamics_reward(x)\n",
    "        p = self.prediction_policy(next_s)\n",
    "        v = self.prediction_value(next_s)\n",
    "        return next_s, reward, p, v\n",
    "\n",
    "class DreamerV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: DreamerV3 (Hafner et al., 2023)\n",
    "    Innovation: Symlog, Discrete Latents, KL Balancing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Placeholder for RSSM (Recurrent State Space Model)\n",
    "        # Innovation: Uses categorical latents instead of Gaussian\n",
    "    \n",
    "    def symlog(self, x):\n",
    "        \"\"\"\n",
    "        The magic scaling function from DreamerV3 that handles diverse reward scales.\n",
    "        symlog(x) = sign(x) * ln(|x| + 1)\n",
    "        \"\"\"\n",
    "        return torch.sign(x) * torch.log(torch.abs(x) + 1.0)\n",
    "    \n",
    "    def symexp(self, x):\n",
    "        return torch.sign(x) * (torch.exp(torch.abs(x)) - 1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}