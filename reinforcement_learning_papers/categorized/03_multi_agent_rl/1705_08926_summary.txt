Based on the provided paper, here is the extracted information:

**PAPER:** Counterfactual Multi-Agent Policy Gradients
**TITLE:** Counterfactual Multi-Agent Policy Gradients
**ARXIV_ID:** 1705.08926v3
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** Counterfactual Multi-Agent (COMA) Policy Gradients is a method that uses a centralized critic to estimate a counterfactual advantage for decentralized policies in multi-agent reinforcement learning. COMA addresses the challenges of multi-agent credit assignment by using a counterfactual baseline that marginalizes out a single agent's action, while keeping the other agents' actions fixed.
**KEY_CONTRIBUTIONS:**
* COMA uses a centralized critic to estimate the Q-function and decentralized actors to optimize the agents' policies.
* COMA uses a counterfactual baseline that marginalizes out a single agent's action, while keeping the other agents' actions fixed.
* COMA uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.
* COMA is evaluated in the testbed of StarCraft unit micromanagement and significantly improves average performance over other multi-agent actor-critic methods.
