Distributed Value Decomposition Networks with Networked
Agents
Guilherme S. Varela

Alberto Sardinha

Francisco S. Melo

Instituto Superior TÃ©cnico, INESC-ID
Lisbon, Portugal
guilherme.varela@tecnico.ulisboa.pt

PUC-Rio
Rio de Janeiro, Brazil
sardinha@inf.puc-rio.br

Instituto Superior TÃ©cnico, INESC-ID
Lisbon, Portugal
fmelo@inesc-id.pt

arXiv:2502.07635v1 [cs.LG] 11 Feb 2025

ABSTRACT
We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward.
We propose distributed value decomposition networks (DVDN)
that generate a joint Q-function that factorizes into agent-wise
Q-functions. Whereas the original value decomposition networks
rely on centralized training, our approach is suitable for domains
where centralized training is not possible and agents must learn
by interacting with the physical environment in a decentralized
manner while communicating with their peers. DVDN overcomes
the need for centralized training by locally estimating the shared
objective. We contribute with two innovative algorithms, DVDN
and DVDN (GT), for the heterogeneous and homogeneous agents
settings respectively. Empirically, both algorithms approximate the
performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL
tasks in three standard environments.

KEYWORDS
Artificial Intelligence, Machine Learning, Multi-Agent Systems,
Reinforcement Learning, Deep Learning
ACM Reference Format:
Guilherme S. Varela, Alberto Sardinha, and Francisco S. Melo. 2025. Distributed Value Decomposition Networks with Networked Agents. In ACM
Conference, Washington, DC, USA, July 2017, IFAAMAS, 21 pages.

1

INTRODUCTION

Cooperative multi-agent reinforcement learning addresses the problem of designing utility-maximizing agents that learn by interacting
with a shared environment. Representing utility functions and applying them for decision-making is challenging because of the
large combined joint observation and joint action spaces. Value
decomposition network (VDN) [30] avoid this combinatorial trap
by considering the family of ğ‘„-functions that factorize agent-wise.
The method offers a viable solution to the scalability of MARL
systems under the premise of centralized training with decentralized execution, where the outputs of the individual ğ‘„-functions are
added to form a joint ğ‘„-function.
However, in many real-world domains, the premise of centralized
training is too restrictive. For instance, in reinforcement learning
This work is licensed under a Creative Commons Attribution International 4.0 License.

based distributed1 load balancing [40] intelligent switches act as
agents to distribute multiple types of requests to a fleet of servers
in a data center. Agents assign the incoming load to the servers,
resolving requests at low latencies under quality-of-service constraints. In this domain, there is no simulatorâ€”agents must learn
online by observing queues at links and selecting the links to route
the requests. In robotic teams, there might be only simulators with
a considerable gap between the simulated environment and the
real-world. In real-world situations where communication is restricted and actions can fail in unpredictable ways, also benefit
from this approach. In RoboCup [1] tournament, soccer robots actuate as agents and are endowed with sensors and computation
onboard. Although communication has delays and link failures,
agents should cooperate as a team to score goals.
The straightforward alternative to centralized training is the
fully decentralized training approach, employing independent ğ‘„learners (IQL) [31]. As IQL agents are oblivious to the presence
of their teammates, they cannot account for the joint action, and
from the perspective of any single agent the perceived transition
probabilities of the environment are non-stationary. This approach
violates the reinforcement learning assumption that the transition
probabilities are fixed and unknown. Since individual learners do
not communicate, fully decentralized IQL precludes parameter sharing, where agents update the same policy parameters. For many
MARL tasks, parameter sharing improves sample efficiency but
requires that every agent updates the same weights. Hence in a
decentralized training setting, every agent would need one-to-one
communication to broadcast its weights and experiences to all other
agents, before performing the updates locally.
We propose a novel algorithm that combines the decentralized
training with value decomposition networksâ€™ ğ‘„-function decomposition. Starting from the loss function used in VDN, in centralized
training setting, we show that information back-propagated to
agentsâ€™ neutral network is the joint temporal difference (JTD). Our
algorithm operates in the decentralized training and decentralized
execution (DTDE) setting where agents communicate with their
closest neighbors to improve their local JTD estimations. And each
agent locally minimizes JTD. When agents are homogeneous, i.e.,
having the same individual observation and action space, we incentivize them to align their ğ‘„-functionâ€™s parameters and their
gradients; This mechanism called gradient tracking [26] enables
agents to minimize a common loss function. To the best of our
knowledge, this is the first application of gradient tracking for
policy iteration in reinforcement learning.

ACM Conference, , July 2017, Washington, DC, USA. Â© 2025 Association for Computing
Machinery. . . . $ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
. . . $15.00
1We use distributed and decentralized interchangeably.

2

BACKGROUND

We introduce three concepts from the distributed optimization literature [22] that form the key components of our method: (i) The
switching topology communication channel over which agents perform communication with closest neighbors. (ii) The consensus updates over the switching topology communication channel, where
agents agree on the value of a constant; (iii) and finally, gradient
tracking that allows agents to optimize a global function formed
by the sum of local functions, using local computation and communication. Additionally, we define decentralized partially observable
Markov decision process the mathematical framework underpinning
value decomposition network.
Switching topology communication channel: Given a set
N = {1, Â· Â· Â· , ğ‘ } of agents connected in a communication network
such that agents ğ‘– and ğ‘— can exchange messages if and only if ğ‘–, ğ‘— âˆˆ E
where E âŠ† N Ã— N denotes the edge set. Under the switching
topology regime at every round of communication ğ‘˜, there is a
different set of edges E (ğ‘˜ ) , such that agents are uncertain in advance
who their peers are going to be. This randomness is captured by a
underlying time-varying undirected graph G (ğ‘˜ ) = G (ğ‘˜ ) (N, E (ğ‘˜ ) )
where the node set N remains fixed but the edge set E (ğ‘˜ ) is allowed
to vary at each timestep ğ‘˜.
Consensus over undirected time-varying networks [37]: is
the process by which nodes initially holding scalar values asymptotically agree on their network average by interacting with closest
neighbors over a switching network topology. At each iteration,
each node replaces its own value with the weighted average of its
previous value and the values of its neighbors. More formally, let
(0)
the variable ğ‘¥ğ‘– be held by agent ğ‘–. Then, agents agree on a value
ğ‘¥Â¯ by performing the updates:
âˆ‘ï¸
(ğ‘˜ ) (ğ‘˜ )
ğ‘¥ (ğ‘˜+1) =
ğ›¼ğ‘–,ğ‘— ğ‘¥ ğ‘— ,
(1)
(ğ‘˜ )

ğ‘— âˆˆ Nğ‘–

n
o
(ğ‘˜ )
where Nğ‘–
= ğ‘– âˆª ğ‘— |(ğ‘–, ğ‘—) âˆˆ E (ğ‘˜ ) is the neighborhood of agent i
at communication step ğ‘˜, extended to include ğ‘¥ğ‘– measurement as
well. Under the time-varying regime we allow the neighborhood
to switch at every consensus update. Ahfoundational
result [37]
i
establishes the existence of the weights

ğ‘
âˆ‘ï¸

(0)

ğ‘¥ğ‘–

(2)

ğ‘˜â†’âˆ
ğ‘–

(ğ‘ number of agents). Refer to Appendix A for instructions on
building such weights locally.
Gradient tracking: Enables agents to compute the solution ğ‘¥
of a distributed optimization problem. The distributed optimization
problem arises on large scale machine learning/statistical learning
problems [6]. Let every agent in N to hold a cost function ğ‘“ğ‘– (ğ‘¥) :
Rğ‘€ â†’ R the objective of distributed optimization is to find ğ‘¥ that
minimizes the average of all the functions
ğ‘

min ğ‘“ (ğ‘¥) â‰œ

ğ‘¥ âˆˆRğ‘€

(ğ‘˜ )

and the local variable ğ‘§ğ‘–

1 âˆ‘ï¸
ğ‘“ğ‘– (ğ‘¥)
ğ‘ ğ‘–=1

(3)

, which tracks the gradient of the function
(0)

ğ‘“ (ğ‘¥), is initialized by the local gradient at point ğ‘¥ğ‘–

(0)

, i.e., ğ‘§ğ‘–

=

(0)
âˆ‡ğ‘“ğ‘– (ğ‘¥ğ‘– ). The algorithm proceeds using the update [26]:
(ğ‘˜+1)

ğ‘¥ğ‘–

=

ğ‘
âˆ‘ï¸

(ğ‘˜ )

ğ›¼ğ‘–,ğ‘— ğ‘¥ ğ‘—

(ğ‘˜ )

âˆ’ ğœ‚ğ‘§ğ‘–

,

ğ‘—=1

(4)
(ğ‘˜+1)
ğ‘§ğ‘–
=

ğ‘
âˆ‘ï¸

(ğ‘˜ )
(ğ‘˜+1)
(ğ‘˜ )
ğ›¼ğ‘–,ğ‘— ğ‘§ ğ‘— + âˆ‡ğ‘“ğ‘– (ğ‘¥ğ‘–
) âˆ’ âˆ‡ğ‘“ğ‘– (ğ‘¥ğ‘– ),

ğ‘—=1

 
where ğ›¼ğ‘–,ğ‘— ğ‘ Ã—ğ‘ are the consensus weights for a fixed strongly
connected graph, and ğœ‚ > 0 is a fixed step size.
Decentralized partially observable Markov decision process (Dec-POMDP) is the framework describing the system dynamics where fully cooperative agents interact under partially observability settingsâ€“defined by the sextuple [8]:
Â¯ ğ›¾),
M = (N, S, {Ağ‘– }ğ‘– âˆˆ N , {Oğ‘– }ğ‘– âˆˆ N , P, ğ‘…,
where N = {1, Â· Â· Â· , ğ‘ } denotes the set of interacting agents, S
is the set of global but unobserved system states, and Ağ‘– is the
set of individual action spaces. The observation space O denotes
the collection of individual observations spaces Oğ‘– . Typically an
observation ğ‘œğ‘–ğ‘¡ âˆˆ Oğ‘– is a function of the state ğ‘  ğ‘¡ . The state action
transition probability function is denoted by P, the team reward
Â¯ ğ‘¡ , ğ‘ğ‘¡ , ğ‘  ğ‘¡ +1 ), and the discount factor
function shared by the agents ğ‘…(ğ‘ 
is ğ›¾. Agents observe ğ‘œğ‘–ğ‘¡ âˆˆ Oğ‘– , choose an action from their individual
Â¯ ğ‘¡ , ğ‘ğ‘¡ , ğ‘  ğ‘¡ +1 ) .
action space ğ‘ğ‘– âˆˆ Ağ‘– and collect a common reward ğ‘…(ğ‘ 
The system transitions to the next state following the state-action
ğ‘¡
transition probability
 ğ‘¡
 function. Neither the state ğ‘  or the joint
ğ‘¡
ğ‘¡
action ğ‘ = ğ‘ 1, ..., ğ‘ ğ‘ is known to the agents.
The joint policy ğœ‹ : O â†’ Î”(A) maps the joint observation
ğ‘œ ğ‘¡ to a distribution Î” over the joint action A = A1 Ã— Â· Â· Â· Ã— Ağ‘ .
The agentsâ€™ objective is to find a joint policy ğœ‹ that maximize the
expected discounted return ğ½ (ğœ‹) over a finite horizon ğ‘‡ given by:

(ğ‘˜ )
ğ›¼ğ‘–,ğ‘—
, such that by
ğ‘ Ã—ğ‘

repeating the updates in Eqn. (1) agents produce localized approximations for the network average, i.e.,
lim ğ‘¥ğ‘–ğ‘˜ = ğ‘ âˆ’1

using local communication and local computation. The algorithm
(0)
for finding the minimizer ğ‘¥ starts from an arbitrary solution ğ‘¥ğ‘–

ğ½ (ğœ‹) = Eğ‘ ğœ‹0 âˆ¼ğœ‡ (Â·)

"ğ‘‡
âˆ‘ï¸

#
ğ‘¡

ğ‘¡

ğ‘¡

ğ›¾ ğ‘…(ğ‘  , ğ‘ , ğ‘ 

ğ‘¡ +1

) .

(5)

ğ‘¡ =0

Where 0 < ğ›¾ < 1 is a discount factor that balances the preference between collecting immediate high rewards while avoiding future low
rewarding states, and ğœ‡ (ğ‘ ) is the initial state distribution. The joint
Â¯ ğ‘¡ , ğ‘ğ‘¡ , ğ‘  ğ‘¡ +1 ).
reward ğ‘…(ğ‘  ğ‘¡ , ğ‘ğ‘¡ , ğ‘  ğ‘¡ +1 ) is the sum of team rewards Î£ğ‘–ğ‘ ğ‘…(ğ‘ 
Thus the expected discounted return captures the expected sum of
exponentially weighted joint rewards, by drawing an initial state
ğ‘  0 from ğœ‡, observing ğ‘œ 0 and following the actions prescribed by the
joint policy ğœ‹ thereafter until ğ‘‡ .

3

DISTRIBUTED VALUE DECOMPOSITION

Consider a setting where fully cooperative learners interact under a
partially observable setting. Motivated by Fact 1 which establishes
that value decomposition networks minimize the mean squared
joint temporal difference JTD. We propose distributed value decomposition networks. DVDN use peer-to-peer message exchange to
combine their local temporal difference (TD) for approximating JTD.

Thus, agents emulate VDN weight updates using the JTD surrogate.
Additionally, homogeneous agents share knowledge by pushing
both weights and gradients to the communication channel, using
gradient tracking. We formalize the setting whereby agents face
uncertainty with respect to the communication channel with hybrid
partially observable Markov decision process introduced by Santos
et al. [28].

3.1

Preliminaries

Value decomposition network [30]: is a value-based method for
multi-agent reinforcement learning. In value-based methods, the
expected discounted return in (5) is captured by a joint ğ‘„-function
that maps the joint observations ğ‘œ and joint actions ğ‘ to a real value
number (ğ‘„-value). Particularly, individual ğ‘„-functions (local observations and actions mappings to a ğ‘„-value) are estimated using
parameterized non-linear function approximation, implementing
the well known deep-ğ‘„ networ [20] architecture in single agent
RL. Then, in centralized training, agents combine their individual
ğ‘„-functions into a joint ğ‘„-function:
ğ‘„ VDN (ğ‘œ, ğ‘; ğœ”) =

ğ‘
âˆ‘ï¸

ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– )

(6)

extension from Dec-POMDPs, H-POMDP is defined by the sextuple [28]:
Â¯ ğ›¾),
H = (G, S, {Ağ‘– }ğ‘– âˆˆ N , {Oğ‘– }ğ‘– âˆˆ N , P, ğ‘…,
where G denotes an undirected episode-varying communication
graph where set of interacting agents remains fixed N but the communication edge set E is allowed to change between episodes. The
other tuple elements adhere to the standard Dec-POMDP definition:
S is the set of global but unobserved system states, and Ağ‘– is the
set of individual action spaces. The observation space O denotes
the collection of individual observations spaces Oğ‘– . Typically an
observation ğ‘œğ‘–ğ‘¡ âˆˆ Oğ‘– is a function of the state ğ‘  ğ‘¡ . The state action
transition probability function is denoted by P, the team reward
Â¯ ğ‘¡ , ğ‘ğ‘¡ , ğ‘  ğ‘¡ +1 ), and the discount factor
function shared by the agents ğ‘…(ğ‘ 
is ğ›¾.
The unknown communication graph G, is sampled from a set C
according to an unknown probability distribution ğ›½. The agents performance is measured as ğ½ğ›½ (ğœ‹) = E Gâˆ¼ğ›½ [ğ½ (ğœ‹; G)], where ğ½ (ğœ‹; G)
denotes the expected discounted return of policy ğœ‹ under an HPOMDP with communication graph G.

3.2

Joint Temporal Difference

ğ‘–=1

Where ğœ” is the concatenation of the individual network parameters
ğœ”ğ‘– , ğ‘œ and ğ‘ represent the concatenation over the agentsâ€™ observations and actions respectively. Finally, ğ‘„ VDN is said to have additive
factorization because it can be obtained directly by summing over
agentsâ€™ ğ‘„-function. The loss function used with additive factorization is:
â„“ (ğœ”; ğœ) =

i2
1 âˆ‘ï¸ h VDN
ğ‘¦
âˆ’ ğ‘„ VDN (ğ‘œ, ğ‘; ğœ”) ,
ğ‘ ğœ

(7)

where the joint trajectory ğœ = [ğœ1, ğœ2, Â· Â· Â· , ğœğ‘ ] is the concatenation
of the trajectories drawn individually by interacting with the environment. The individual trajectory ğœğ‘– of episode with timesteps
ğ‘¡ = 0, ...,ğ‘‡ , is given by:
ğœğ‘– = (ğ‘œğ‘–0, ğ‘ğ‘–0, ğ‘…Â¯1, ğ‘œğ‘–1, ğ‘ğ‘–1, ğ‘…Â¯2, Â· Â· Â· , ğ‘…Â¯ğ‘‡ , ğ‘œğ‘‡ğ‘– ).
The parameter set ğœ” = [ğœ” 1, ğœ” 2, Â· Â· Â· , ğœ” ğ‘ ] also factorizes across
ğ‘„
agents, such that each of the agent-wise policies ğœ‹ğ‘– (ğ‘œğ‘– ; ğœ”ğ‘– ) is determined only by its ğœ”ğ‘– . Like deep ğ‘„-networks, VDN uses a target
network which stabilizes training by providing the a joint target
ğ‘¦ VDN :
ğ‘¦ VDN =

ğ‘
âˆ‘ï¸

ğ‘…Â¯ + ğ›¾ max ğ‘„ğ‘– (ğ‘œğ‘–â€², ğ‘¢ğ‘– ; ğœ”ğ‘–âˆ’ ),

ğ›¿ğ‘–ğ‘¡ = ğ‘…Â¯ + ğ›¾ max ğ‘„ğ‘– (ğ‘œğ‘–â€², ğ‘¢ğ‘– ; ğœ”ğ‘–âˆ’ ) âˆ’ ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– ),

(10)

ğ‘¢ğ‘–

where the observation pair (ğ‘œğ‘– , ğ‘œğ‘–â€² ) are surrogates for the current
and next states respectively, in timestep ğ‘¡, the environment reaches
hidden state ğ‘  ğ‘¡ and emits ğ‘œğ‘–ğ‘¡ to agent ğ‘–.
Fact 1. Value decomposition network minimize the joint temporal
difference ğ›¿ = Î£ ğ‘— âˆˆ N ğ›¿ ğ‘— , where
âˆ‘ï¸
ğ›¿ ğ‘¡ğ‘— =
ğ‘…Â¯ + ğ›¾ max ğ‘„ ğ‘— (ğ‘œ â€²ğ‘— , ğ‘¢ ğ‘— ; ğœ” âˆ’
(11)
ğ‘— ) âˆ’ ğ‘„ ğ‘— (ğ‘œ ğ‘— , ğ‘ ğ‘— ; ğœ” ğ‘— ).
ğ‘¢ğ‘—

ğœ

and ğ›¿ ğ‘— âˆˆ Rğ‘‡ is agent ğ‘—â€™s temporal difference, for the joint trajectory
ğœ with length ğ‘‡ + 1.
(8)

ğ‘¢ğ‘–

ğ‘–=1

where ğ‘œğ‘–â€² is the subsequent observation to ğ‘œğ‘– and ğœ”ğ‘–âˆ’ is a periodic
copy from ğœ”ğ‘– . Similar to the joint ğ‘„-function in (6) the joint target
also factorizes into individual targets
ğ‘¦ğ‘– = ğ‘…Â¯ + ğ›¾ max ğ‘„ğ‘– (ğ‘œğ‘–â€², ğ‘¢ğ‘– ; ğœ”ğ‘–âˆ’ ).

Additive factorization is the distinguishing VDN characteristic; it is
accomplished through a centralized value decomposition layer. The
value decomposition layer sums the outputs from agentsâ€™ deep ğ‘„networks. And to emulate its behavior in the decentralized setting
we must determine: What information flows to agentsâ€™ networks
during centralized training? To answer this research question, we
must first establish the role that value decomposition layer plays
in shaping the weights of the ğ‘„-functions.
Temporal difference (TD) ğ›¿ğ‘– is the increment by which agent ğ‘–
adjusts its ğ‘„-function in the direction of the maximal ğ‘„-value, i.e.,

(9)

ğ‘¢ğ‘–

Hybrid partially observable Markov decision process (HPOMDP) is the framework where agents cooperate under partial
observability and are uncertain about the communication graph
topology. During training, agents are unaware of whom their communicating peers are going to be at any given training episode. An

Fact 1 establishes that the information flowing to weights through
the back-propagation algorithm is the sum across agents of their
temporal difference. Hence, in VDN, every agent replaces its own
TD in (10) with JTD (11), for local updates. By construction, JTD
decomposes between agents, and we use the fact that the joint
trajectory in (10) is a concatenation of the individual trajectories
ğœ ğ‘— , to perform localized approximations to (11).

3.3

Decentralized JTD

Can the effect of value decomposition layer be reproduced in the
decentralized setting? In the case where the communication graph

is fully connected the system is back to the centralized setting and
we can use VDN normally. Conversely, when communication is not
possible the system is fully decentralized. Independent ğ‘„-learners
perform weight updates in isolation by minimizing the mean square
temporal difference:

1 âˆ‘ï¸ 2
ğ›¿ .
â„“ ğœ”ğ‘– ; ğœğ‘– :=
(12)
ğ‘‡ ğœ ğ‘–
ğ‘–

Alternatively, for situations where the communication graph is
strongly connected, it is possible to generate a localized approximation for additive factorized joint ğ‘„-function from individual ğ‘„functions. Since there is no agent capable of overseeing the system,
we propose to perform the consensus updates in (1), to propagate
the temporal difference:
âˆ‘ï¸
(ğ‘˜+1)
(ğ‘˜ ) (ğ‘˜ )
ğ›¿ğ‘–
=
ğ›¼ğ‘–,ğ‘— ğ›¿ ğ‘— .
(13)

3.4

DVDN with Gradient Tracking

When the agents are homogeneous,i.e., they have the same observation and action space, it is possible to use gradient tracking to make
them agree on a common solution for the parameters. This common
solution combines weight updates from many agents improving
sample-efficiency. We assume that there is a global loss function
that factorizes additively agent-wise and the solution is a common
ğœ”. The global loss function can be expressed by replacing the loss
function (14) in the agent wise cost functions in (3):
ğ‘

1 âˆ‘ï¸
â„“ğ‘– (ğœ”)
ğ‘ ğ‘–=1

min â„“ (ğœ”; ğœ) =
ğœ”

Since the minimizer ğœ” for the global in (15) is unknown to the
agents, an alternative formulation introduces ğ‘ copies ğœ” and followed by ğ‘ equality constraints:

(ğ‘˜ )

ğ‘— âˆˆ Nğ‘–

ğ‘

min
As a result from temporal difference consensus in (13) agents receive
an updated estimation for the team temporal difference. The team
temporal difference estimator at communication step ğ‘˜ at agent ğ‘–
(ğ‘˜+1)
is denoted by ğ›¿ğ‘–
. The limit in (2) guarantees that the updates
asymptotically converge to 1 /ğ‘ ğ›¿. However, with a finite number
of updates it is not possible to guarantee that the agents will reach
consensus. Instead, we resort to truncation. In this work we perform
a single consensus step per mini-batch update.
In a practical applications, it is useful to separate temporal difference (TD) from the additional information that comes from communication. The network estimated JTD at agent ğ‘– ğ›¿Ë†ğ‘– captures the
contributions that come from network and is defined as:
(ğ‘˜+1)
(ğ‘˜ )
ğ›¿Ë†âˆ’ğ‘– = ğ‘ ğ›¿ğ‘–
âˆ’ ğ›¿ğ‘– .

The network estimated JTD at agent ğ‘– can be used to perform weight
updates in the direction that minimizes the mean square estimated
JTD at agent ğ‘–:
â„“ (ğœ”ğ‘– ; ğœğ‘– , ğ›¿Ë†âˆ’ğ‘– ) :=

(15)

2
1 âˆ‘ï¸ 
ğ›¿ğ‘– + ğ›¿Ë†âˆ’ğ‘–
ğ‘‡ ğœ

(14)

ğ‘–

Since ğ›¿Ë†âˆ’ğ‘– is the value obtained from the network of agents, it comes
after the semicolon in the LHS of (14), in the place reserved for
data. Whereas the variable in the minimization is the ğ‘„-network
parameters ğœ”ğ‘– . Similarly, ğ›¿ğ‘– is a variable which depends on the
(trajectory, parameters) (ğœğ‘– , ğœ”ğ‘– ). Figure 3 (Appendix C) compares
the error increments that shape the weight updates in VDN and
DVDN in (13) and (14).
Differently from the distributed algorithm max-push [15], where
the communication is serial and it happens on an established order,
induced by an spanning tree that is common knowledge, the updates
in (13) happen in parallel. Moreover, max-push requires the design
of joint action utility functions, to capture the value of the joint
action. Agents have a time budget to agree on what the best global
action is by solving their local problems. There are no guarantees
that max-push converges to a global joint action in graphs with
cycles, or within their allocated time budget. The updates in DVDN
have asymptotic convergence guarantees and do not require explicit
joint action modeling, or even knowledge of neighborsâ€™ actions.

ğœ”ğ‘– âˆˆÎ©,âˆ€ğ‘– âˆˆ N

â„“ (ğœ”; ğœ) =

1 âˆ‘ï¸
â„“ğ‘– (ğœ”ğ‘– )
ğ‘ ğ‘–=1

(16)

âˆ€(ğ‘–, ğ‘—) âˆˆ E

s.t. ğ‘¥ğ‘– = ğ‘¥ ğ‘—

for the parameter space Î©. We rewrite the terms in (16) making
explicit the dependency of the local loss functions in LHS with the
local experiences in (14):
ğ‘

min
ğœ”ğ‘– âˆˆÎ©,âˆ€ğ‘– âˆˆ N

â„“ (ğœ”; ğœ) =

1 âˆ‘ï¸
â„“ (ğœ”ğ‘– ; ğœğ‘– , ğ›¿Ë†âˆ’ğ‘– )
ğ‘ ğ‘–=1

(17)

âˆ€(ğ‘–, ğ‘—) âˆˆ E

s.t. ğ‘¥ğ‘– = ğ‘¥ ğ‘—

Similar to previous work [22], we propose a gradient-based strategy for solving (17). Differently from previous work in which the
objective function is strongly convex, here the objective is nonconvex. As result there is no closed form solution to the optimization problem. Moreover, we cannot guarantee that the equality
constraints are satisfied since we truncate the number of consensus
steps; Rather, we interleave gradient descent steps for minimization
with consensus steps to incentivize an alignment in the solution. A
practical reinforcement learning algorithm is initialized as follows:
At the first mini-batch iteration ğ‘˜ = 1, agents perform a forward
(1)
pass on the local networks and compute ğ›¿ğ‘– , then they perform
the consensus iteration in (13) to obtain the estimated network JTD
at agent ğ‘–,
Â© âˆ‘ï¸ (1) (1) Âª
(1)
(1)
ğ›¿Ë†âˆ’ğ‘– = ğ‘ Â­Â­
ğ›¼ğ‘–,ğ‘— ğ›¿ Â®Â® âˆ’ ğ›¿ğ‘– .
(1)
Â« ğ‘— âˆˆ Nğ‘–
Â¬
Then, agents compute the gradient of the mean square estimated
JTD (14)
(1)

ğ‘”ğ‘–

(0)

(1)

= âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘– ; ğœğ‘–
(1)

We introduce an auxiliary variable ğ‘§ğ‘–

(1)

, ğ›¿ âˆ’ğ‘– )

that tracks the team gradient

ğ‘

âˆ‡â„“ (ğœ”; ğœ) =

1 âˆ‘ï¸
âˆ‡â„“ (ğœ”ğ‘– ; ğœğ‘– ),
ğ‘ ğ‘–=1

and is initialized at the local gradient loss, i.e.,
(1)

ğ‘§ğ‘–

= ğ‘” (1) .

The update at step ğ‘˜ = 1, combines a consensus step (1) and gradient
(1)
descent with weight updates ğ‘§ğ‘– :
âˆ‘ï¸
(1)
(1) (0)
(1)
ğœ”ğ‘– =
ğ›¼ğ‘–,ğ‘— ğœ”ğ‘– âˆ’ ğœ‚ğ‘§ğ‘– .
(2)

ğ‘— âˆˆ Nğ‘–

At the second mini-batch iteration ğ‘˜ = 2, agents compute the
estimated network JTD at agent ğ‘–,
âˆ‘ï¸
(2)
(2) (2)
(2)
ğ›¿Ë†âˆ’ğ‘– = ğ‘ (
ğ›¼ğ‘–,ğ‘— ğ›¿Ë†ğ‘— ) âˆ’ ğ›¿ğ‘– .
(2)

ğ‘— âˆˆ Nğ‘–

Then, agents compute the local gradients at iteration ğ‘˜ = 2:
(2)

ğ‘”ğ‘–

(1)

(2)

= âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘– ; ğœğ‘–

(2)

, ğ›¿ âˆ’ğ‘– ).

Once difference between local gradients is available, agents update
the team gradient variable:
âˆ‘ï¸
(2)
(2) (1)
(2)
(1)
ğ‘§ğ‘– = (
ğ›¼ğ‘–,ğ‘— ğ‘§ ğ‘— ) + ğ‘”ğ‘– âˆ’ ğ‘”ğ‘– .
(2)

ğ‘— âˆˆ Nğ‘–

Finally, agents aggregate the weights and perform a gradient step
at iteration ğ‘˜ = 2 using the team gradient instead of the local
gradients:
âˆ‘ï¸
(2)
(2) (1)
(2)
ğœ”ğ‘– =
ğ›¼ğ‘–,ğ‘— ğœ” ğ‘— âˆ’ ğœ‚ğ‘§ğ‘–
(2)

ğ‘— âˆˆ Nğ‘–

ending the iteration ğ‘˜ = 2 updates. More generally, for an arbitrary
ğ‘˜:

Â© âˆ‘ï¸ (ğ‘˜ ) (ğ‘˜ ) Âª
(ğ‘˜ )
(ğ‘˜ )
ğ›¿Ë†âˆ’ğ‘– = ğ‘ Â­Â­
ğ›¼ğ‘–,ğ‘— ğ›¿ ğ‘— Â®Â® âˆ’ ğ›¿ğ‘–
(ğ‘˜ )
Â« ğ‘— âˆˆ Nğ‘–
Â¬
(ğ‘˜ )

(ğ‘˜ âˆ’1)

(ğ‘˜ )

(ğ‘˜ )

= âˆ‡â„“ (ğœ”ğ‘–
; ğœğ‘– , ğ›¿ âˆ’ğ‘– )
âˆ‘ï¸
(ğ‘˜ )
(ğ‘˜ ) (ğ‘˜ âˆ’1)
(ğ‘˜ )
(ğ‘˜ âˆ’1)
ğ‘§ğ‘– =
ğ›¼ğ‘–,ğ‘— ğ‘§ğ‘–
+ ğ‘”ğ‘– âˆ’ ğ‘”ğ‘–

ğ‘”ğ‘–

(18a)

(18b)
(18c)

(ğ‘˜ )
ğ‘— âˆˆ Nğ‘–

(ğ‘˜ )

ğœ”ğ‘–

=

âˆ‘ï¸

(ğ‘˜ )

(ğ‘˜ âˆ’1)

ğ›¼ğ‘–,ğ‘— ğœ”ğ‘–

(ğ‘˜ )

âˆ’ ğœ‚ğ‘§ğ‘–

(18d)

(ğ‘˜ )
ğ‘— âˆˆ Nğ‘–

The intuition behind the algorithm (18) is to update the auxiliary
variables, team gradient variables, ğ‘§ğ‘– in the direction that minimizes
LHS of (17). Over many iterations of ğ‘˜, the team gradient variables
(ğ‘˜ )
ğ‘§ğ‘– converge asymptotically to the average of individual gradients.
As established in the previous section, we only perform a single
update per mini-batchâ€“which is sufficient to emulate the effect of
parameter sharing in the distributed setting, and provides better results than the fully decentralized independent learner. The practical
implementation of this algorithm incorporates adaptive momentum (Adam) [14] updates to improve its performance, making it
compatible with standard VDN implementation.

4

EXPERIMENTS

We evaluate the performance of both DVDN algorithms in ten
scenarios with partial observability, and compare them to two baselines: The independent ğ‘„-learning in the fully decentralized paradigm, and value decomposition networks in the CTDE paradigm.

IQL is the lower bound baseline and VDN is the upper bound baseline. Specifically, for each scenario, in the heterogeneous agents
setting, we compare VDN to DVDN (Algorithm 1, Appendix D.1),
and in the homogeneous agents setting, we compare VDN (PS) to
DVDN (GT) (Algorithm 2, Appendix D.2).

4.1

Scenarios

We consider three environments: The first is level-based foraging [LBF, 24]2 , where agents collect fruits in a grid-world and are
rewarded if their level is greater than or equal to the fruits they are
loading. Agents perceive the X-Y positions and levels of both food
items and peers within a two block radius. Rewards are sparse and
positive, they depend on the level of the fruit item being loaded
and the level of each contributing agent. The second environment
is the multi-particle environment [MPE, 18] where agents navigate
in a continuous grid, with their trajectories dependent on past
movement actions. We modify the original environment for partial
observability3 . The third environment [MARBLER, 32] is a robotic
navigation simulator that generates physical robot dynamics. In this
environment, all agents are assumed to have full communication
and their observations are appended together. However, agents remain aware only of their own actions. The scenarios configurations
are:
â€¢ LBF/Easy: Three agents interact in a 10x10 grid-world to
collect 3 food items.
â€¢ LBF/Medium: Four agents interact in a 15x15 grid-world to
collect 5 food items.
â€¢ LBF/Hard: Three agents interact in a 15x15 grid-world to
collect 5 food items.
â€¢ MPE/Adversary: Two teammates must guard the target
landmark against the approach of a pretrained adversary
agent. They perceive their own position, their relative distance to the goal, the positions of the landmarks, and the
position and color of the closest agent. The color enables a
teammate to distinguish whether or not the other agent is
the adversary. The team is rewarded in proportion to the
negative distance from the target to its closest teammate and
penalized by the adversaryâ€™s distance to the target.
â€¢ MPE/Spread: Three agents must navigate as close as possible to one of three landmarks. They perceive their own
position, velocity, the position and velocity of the closest
agent and the position of the closest landmark. The team
is rewarded the negative distance from the agent closest
to a landmark and receive a penalty for bumping into one
another.
â€¢ MPE/Tag: Three large and slow moving predator agents
must bump into a smaller and faster pretrained prey agent.
Predator agents perceive their own position and velocity,
the position and velocity of the closest predator, and the
position and velocity of the prey. Predators collect rewards
by touching the prey.
â€¢ MARBLER/Arctic Transport (Arctic): Two drones attempt
to guide the ice robot and water robot to the goal location as
quickly as possible over ground tiles (white), ice tiles (light
2 https://github.com/uoe-agents/lb-foraging
3 https://github.com/GAIPS/multiagent-particle-envs

blue), and water tiles (dark blue). Drones move fast, while
ground robots move fast on their specialized tiles. Teammates
are penalized at each time step in proportion to the number
of the ground robots not in the destination, plus the distance
of the ground robots from their goal.
â€¢ MARBLER/Material Transport (Material): A heterogeneous team of four agents, two fast agents with low loading
capacity, and two slow agents with high loading capacity,
must collaborate to unload two zones. The teammates collect
positive rewards for loading and unloading the zones, while
collecting a small penalty per timestep.
â€¢ MARBLER/Predator Captures Prey (PCP): A heterogeneous team of two sensing agents and two capture agents
must collaborate to capture six preys. A prey can be perceived only by one of the sensing agents, and can be captured only by one of the capture agents. Teammates collect
positive rewards for sensing and capturing the prey, and a
small penalty per timestep.
â€¢ MARBLER/Warehouse: A team of six robots, three green
and three red, must navigate to the zone of their color on
the right side of the environment to receive a load. Then
they must navigate to their color zone on the left side to
unload. Since optimal paths intersect robots must coordinate
to avoid collisions. Teammates collect positive rewards for
successfully loading and unloading the zones.

4.2

Baselines

We apply the algorithmic implementations of IQL and VDN in [24]4
for the fully cooperative setting with joint rewards:
â€¢ VDN [30]: Is the CTDE paradigm baseline. For the homogeneous agents setting, we perform parameter sharing.
â€¢ IQL [20]: Implement individual deep-ğ‘„ network that minimize a local loss function (12). Since IQL belongs to the
fully decentralized approach, we only perform experiments
without parameter sharing for this algorithm.

4.3

Metrics

Performance plots: Our evaluation methodology follows that
of Papoudakis et al. [24]. We conduct ten independent, randomly
seeded, runs for each algorithm-scenario combination for five million training steps. Evaluations are performed at every 125,000
timesteps, for a total of forty one evaluation checkpoints over each
training run. Each evaluation checkpoint consists of hundred independent episodes per training seed. The average episodic return
is the average of the ten evaluation checkpoints (i.e., average over
seeds). The maximum average episodic return captures the average
episodic returns with maximum value (i.e., maximum over forty
one average episodic returns). We report the maximum average
episodic return and the 95% bootstrapped confidence interval (CI).
Ablation Plots: To measure the contributions of different factors to performance gains, we conduct an ablation study. For each
factor, we identify the evaluation checkpoint with the maximum
average episodic return (over five independent seeds) and extend
the selection to include a two-evaluation checkpoint neighborhood.
4 https://github.com/uoe-agents/epymarl

This extended selection increases the groupâ€™s sample size from a
total of five data points to twenty-five, thereby decreasing standard
deviation. We randomly resample this sample (size 20,000) to build
a 95% bootstrap confidence interval (CI). We report the resampled
mean and bootstrap CI bars.
Ranking criteria: To compare algorithmsâ€™ performances, we
refrain from using statistical language since their performance is
based on the maximum evaluation checkpoints for a small number
of independent runs, and may not necessarily follow any parametric
distribution. Instead, to discriminate the algorithmsâ€™ performances,
we use a bootstrap CI test5 . We say that algorithm A matches the
performance of algorithm B when we cannot reject the null hypothesis that their distributions have the same mean. When the
bootstrap CI test rejects the null hypothesis that both algorithms
have the same mean: We say that algorithm A outperforms algorithm B if its average is greater. Otherwise, we say that algorithm
A under performs algorithm B.

4.4

Setup

For the baselines, we use the default hyper parameters from [24], but
extend the number of training steps from two million to five million.
For both DVDN algorithms, we perform hyper parameter search
using the protocol in [24]. To generate the switching topology
network, we make a random sample from all possible strongly
connected graphs 6 for the task-specific number of agents. This
sampling scheme ensures that there is always a path between two
agents. The hyper parameters used in the experiments are in the
Appendix D.3. Open-source code is available at the DVDN Github
repository.7 .

5

RESULTS

Figure 1 illustrates the performance curves for one representative
scenario per environment. The performance curve profiles are similar between VDN and DVDN. Table 1 summarizes the results for
IQL, DVDN and VDN algorithms, separating them into settings
for heterogeneous agents and homogeneous agents. According to
our ranking criteria, highlighted results indicate the best for the
algorithm-scenario combination, asterisks mark performances that
comparable to the best performing, and double asterisks indicate
second-best performance. We argue that DVDN is an alternative
to VDN, not its replacement. Consequently, it is sufficient to either
outperform the lowest-ranked algorithm or match the performance
of the highest-ranking algorithm. For the heterogeneous setting,
DVDN matches or outperforms VDN in six of ten scenarios. For
the homogeneous setting, DVDN (GT) outperforms IQL in five of
ten scenarios.
In LBF scenarios where the reward is sparse, agents in the homogeneous setting that apply parameter sharing or gradient tracking
outperform their counterparts in heterogeneous setting. This result
is consistent with previous authors [24] claims, that larger gridworlds (and/or, fewer agents) scenarios, benefit from parameter
5 https://github.com/facebookincubator/bootstrapped
6 In the distributed optimization literature, strongly connected graphs are used in fixed

topology network studies. While the term switching topology usually applies to settings
where agents might be disconnected for some consensus iterations. We deviate from
this norm by requiring that every agent be connected at every iteration.
7 https://github.com/GAIPS/DVDN

Table 1: Maximum average episodic returns, over independent seeds, their respective 95% bootstrapped confidence interval
for all algorithms and tasks. Highlighted results are those with the higher maximum average episodic returns. The asterisk
denotes results that match the performance of the best result for the task (see Section 4.3). The double asterisk denotes results
that are second in the rank.

Env. Task.

IQL

DVDN
IQL
VDN

150

20

200

10

Episodic Return

0.6
0.4
0.2

250
300
350

1

2
3
4
Environment Timesteps

5
1e6

0

(a) LBF (Medium)

0.6
0.4
0.2
0.0

1

2
3
4
Environment Timesteps

(d) LBF (Medium)

20
30

0

10

5
1e6

0

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
Environment Timesteps
1e6

(e) MPE (Spread)

DVDN
IQL
VDN
1
2
3
4
5
1e6
Environment Timesteps

(c) MARBLER (Material)

200

300

0.94
(âˆ’0.01, 0.01)
0.79
(âˆ’0.02, 0.02)
0.56
(âˆ’0.02, 0.02)
8.73
(âˆ’0.42, 0.44)
âˆ’141.45
(âˆ’1.46, 1.36)
23.180
(âˆ’1.61, 1.68)
âˆ’28.55
(âˆ’0.84, 1.11)
21.94
(âˆ’0.56, 0.53)
125.27
(âˆ’1.46, 1.50)
28.79âˆ—
(âˆ’0.57, 0.60)

10

20

250

VDN (PS)

0

150

350

0

DVDN
IQL
VDN
1
2
3
4
5
Environment Timesteps
1e6

Episodic Return

Episodic Return

Episodic Return

0.8

(âˆ’0.02, 0.02)
0.72âˆ—âˆ—
(âˆ’0.01, 0.02)
0.52âˆ—âˆ—
(âˆ’0.01, 0.02)
8.66
(âˆ’0.27, 0.30)
âˆ’131.62
(âˆ’1.52, 1.55)
15.99
(âˆ’4.43, 4.13)
âˆ’49.50
(âˆ’3.50, 3.03)
17.32âˆ—âˆ—
(âˆ’0.83, 0.87)
124.36
(âˆ’1.85, 2.21)
29.54
(âˆ’2.36, 2.21)

(b) MPE (Spread)

DVDN (GT)
IQL
VDN (PS)

1.0

0.81
(âˆ’0.02, 0.02)
0.61
(âˆ’0.02, 0.02)
0.43
(âˆ’0.01, 0.02)
9.26
(âˆ’0.56, 0.52)
âˆ’135.81âˆ—âˆ—
(âˆ’1.74, 1.68)
23.00âˆ—
(âˆ’1.70, 1.78)
âˆ’43.51âˆ—âˆ—
(âˆ’1.64, 1.67)
12.81
(âˆ’0.49, 0.50)
130.72
(âˆ’0.86, 0.75)
21.99
(âˆ’0.43, 0.38)

Episodic Return

LBF
MPE
MARBLER
0.8

0

0.85
(âˆ’0.02, 0.02)
0.64
(âˆ’0.02, 0.02)
0.44
(âˆ’0.02, 0.02)
8.82
(âˆ’0.43, 0.40)
âˆ’144.05
(âˆ’1.62, 1.61)
18.33
(âˆ’1.40, 1.36)
âˆ’30.93
(âˆ’0.70, 0.76)
21.82
(âˆ’0.36, 0.36)
125.10
(âˆ’2.57, 3.09)
23.65âˆ—âˆ—
(âˆ’0.90, 0.93)

0.89âˆ—âˆ—

0.80
(âˆ’0.02, 0.02)
0.62âˆ—âˆ—
(âˆ’0.02, 0.02)
0.44
(âˆ’0.02, 0.02)
10.05
(âˆ’0.48, 0.43)
âˆ’137.44âˆ—
(âˆ’1.98, 1.98)
31.41
(âˆ’2.68, 2.98)
âˆ’37.56âˆ—âˆ—
(âˆ’1.01, 0.84)
18.07âˆ—âˆ—
(âˆ’1.14, 1.30)
133.02
(âˆ’0.67, 0.78)
28.74
(âˆ’0.45, 0.45)

1.0

0.0

Homogeneous
IQL
DVDN (GT)

VDN

0.81
(âˆ’0.02, 0.02)
0.61
Medium
(âˆ’0.02, 0.02)
0.43âˆ—
Hard
(âˆ’0.01, 0.02)
9.26âˆ—âˆ—
Adversary
(âˆ’0.57, 0.52)
âˆ’135.81
Spread
(âˆ’1.74, 1.68)
23.00âˆ—âˆ—
Tag
(âˆ’1.74, 1.80)
âˆ’43.51
Arctic
(âˆ’1.64, 1.65)
12.81
Material
(âˆ’0.49, 0.51)
130.72âˆ—âˆ—
PCP
(âˆ’0.81, 0.76)
21.99
Warehouse
(âˆ’0.42, 0.38)
Easy

Episodic Return

Heterogeneous
DVDN

0
10
20
30
0

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
1e6
Environment Timesteps

(f) MARBLER (Material)

Figure 1: In the columns, the results for each environment is expressed by a representative task. In the top row, heterogeneous
agents setting and in the bottom row homogeneous agents setting. The IQL curve is orange, VDN curve is chestnut and the
DVDN curve is blue. The markers represent the evaluation checkpoint and the shaded area represent the 95% bootstrap CIs.
Notably, the performance curves for VDN and DVDN are similar, showcasing the effectiveness of using JTD as training signal.

6

RELATED WORK

Distributed optimization: The set of problems in decentralized
decision-making where each agent holds a different but related
piece of information, and makes compatible decisions traces back
to Tsitsiklis [33]. These problems are characterized by local communication and local gradient minimization of a shared (convex) cost
function [22].The network averaging consensus algorithm for the

132
Maximum Average Returns

0.70
Maximum Average Returns

0.65
0.60
0.55
0.50
IQL

GT

Groups

JTD

134
136
138
140
142
144
146

GT+JTD

(a) LBF (Medium)

IQL

GT

Groups

JTD

GT+JTD

(b) MPE (Spread)

18
Maximum Average Returns

sharing. In PS, the same set of weights is updated using trajectories
of all agents. In this environment, the benefits of using GT correlate
with those of using PS.
In general, scenarios in the MPE environment show that IQL generates effective policies that are difficult to surpass by using value
decomposition. Moreover, there is little performance differences
between algorithms, even when comparing between the heterogeneous and homogeneous settings. The exception is the Tag scenario,
where DVDN outperforms both baselines in the heterogeneous
setting by a wide margin. Since DVDN produces localized approximations for the JTD, it produces policies that are more diverse, thus
during exploration phase it experiences a wider variety of states.
In fact, DVDNâ€™s best-performing policy has two predators fleeing
from the prey, causing the prey to stay still, while the third predator
bumps repeatedly into it. Because the prey moves according to a
pretrained policy, DVDN learns a way to exploit the preyâ€™s policy
by generating unseen states during the preyâ€™s training. For further
discussion on this matter, please refer to Appendix E.4.
Scenarios in the MARBLER environment showcase heterogeneous agents, sparse rewards, and agents are assumed to learn from
the concatenation of observations. In the heterogeneous setting,
DVDNâ€™s performance stands out, surpassing IQL in four scenarios
and VDN in two scenarios. Whereas in the homogeneous setting,
DVDN (GT)â€™s lags behind DVDNâ€™s. These findings contrast with
those of the LBF environment, where GT enhances DVDNâ€™s performance. This performance degradation is anticipated, given that
three out of four scenarios involve heterogeneous agents. However,
further improvement is possible by performing GT in subsets of
homogeneous teammates within the team.
The ablation plots (Fig. 2) illustrate the key factors contributing
to DVDNâ€™s algorithmic performance for each environment and in
a specific task. Starting with the base DVDN (GT) configuration
used for testing, (GT+JTD), we toggle on and off the JTD and GT
modules. The IQL group (control group) has no communication,
updating its weights by minimizing the loss as specified in (12). The
GT group performs gradient tracking exclusively. The JTD group
performs JTD consensus updating the loss in (14).
For the three tasks in Fig. (2) both JTD and GT demonstrate
improvements over IQL group, with the combined factors (GT+JTD)
performing even better. This result ratify reported results in Table 1;
Gradient tracking plays a crucial role in improving the performance
for LBF Medium task. While joint temporal difference is the main
factor enhancing the MARBLER Material task performance. In the
MARBLER environment, GT slightly deteriorates the performance,
while JTD improves it. The combined GT+JTD performance is about
equivalent to the JTD group. For the MPE Spread task, both factors
can individually enhance performance and combining them results
in superior performance.

16
14
12
10
IQL

GT

Groups

JTD

GT+JTD

(c) MARBLER (Material)

Figure 2: Ablation plots for the homogeneous setting, for
the LBF and MARBLER environments respectively. The IQL
group has no consensus (control group). The GT group performs gradient tracking. The JTD group performs joint temporal difference consensus. The GT+JTD group combines GT
and JTD consensus. For the three tasks both factors individually improve results and are better combined.

fixed strongly connected networks [36], and for switching topology
networks [37], are essential components for one-hop communication between agents; They enable their agreement on a solution
vector to the common decision problem. Large scale convex (supervised) machine learning problems can be within this decentralized
optimization framework, e.g., [6] [4], where agents agree on a solution vector while maintaining the input data private. Gradient
tracking Qu and Li [26], performs consensus both on the solution
vector and its local gradients, minimizing an objective that is the
average of all local objectives. However, those agents are not reinforcement learning based agents. In reinforcement learning the
state changes according to a transition function.
Distributed optimization and deep learning: A recent development is the investigation of distributed optimization in large
scale supervised deep learning, e.g., [12] develop a image detection
algorithm using consensus where each agent has a i.i.d sample of a
very large dataset. The focus on non-convex problems includes rigorous convergence analysis and the application of momentum based
optimizers for better sample efficiency. In the centralized setting,
momentum based optimizers, such as the Adam optimizer Kingma
and Ba [14], boost the performance of deep learning models by
incorporating the weight update history. This improvement carries over to the distributed setting, where consensus on model
weights is integrated with momentum-based weight updates. For
instance, consensus adaptive optimizer presented in Nazari et al.
[21] surpasses the performance of centralized optimizers. More
recently, Carnevale et al. [3] propose gradient tracking Adam optimizer, GTAdam. Their system combines gradient tracking and

adaptive momentum, and outperforms other distributed optimizers. In supervised deep learning, the datasets are static whereas in
reinforcement learning the datasets are dynamic. They change in
accordance to shifts in agentsâ€™ policies.
Distributed MARL: A research thread in MARL applies consensusbased mechanisms to the distributed policy evaluation problem. The
problem of estimating the total discounted joint reward under a
fixed joint policy under the fully observable setting. Agents perform
localized approximations for the state value function, which can
either be tabular or approximated using linear function approximation. Under those assumptions, there are methods that guarantee the
convergence of the policy evaluation. Examples include: DTDT [34],
Decentralized TD(0) with gradient tracking [17], and [29]. Moving
to the partially observable setting, consensus-based mechanisms
also improve the performance of belief based agents. Kayaalp et al.
[13] propose policy evaluation, with linear function approximation,
under the partially observable setting whereby agents use the consensus mechanism to share both policiesâ€™ parameters and beliefs
with closest neighbors. Petric et al. [25] proposes learning agents
that perform consensus on a tabular belief, and learn polices using
interior point methods. In this work, we propose to use non-convex
function approximation under partially observability.
Coordination graphs (CGs): is a solution to coordination problems in games where agents interact with a few neighbors to decide
on a local joint action; Agents are nodes and joint actions are edges
representing a local coordination dependency. Agents learn the
payoff function for every joint action without exploring the large
combinatorial action space. The global payoff functions is the sum
of local payoff functions. For solving the CG, Guestrin et al. [9] propose a variable elimination algorithm. However, this approach can
scale exponentially in the number of agents for densely connected
graphs [15]. Kok and Vlassis [15] propose the distributed max-plus
algorithm, that out performs variable elimination for densely connected graphs. However, the underlying CG must be fixed during
training, it induces a spanning tree that models the communication
channel, and it requires a variable number of message exchanges
so that all agents agree on a the best (local) payoff. Moreover, convergence of this message passing scheme is only guaranteed for
acyclic CGs. Both variable elimination and max-plus algorithm
have been developed to work on the tabular payoff function case.
Deep coordination graphs [DCGs, 2] generalize max-plus algorithm
to train end-to-end CGs, using parameter sharing between payoff
functions and privileged information (global state). DCGs are expressive enough to represent a rich set of ğ‘„-functions that factorize,
such as VDN and QMIX [QMIX, 27]. Particularly, all the methods assume centralized training and a fixed coordination graph topology
which is common knowledge throughout training.
Networked agents with multi-agent reinforcement learning: Zhang et al. [39] develops the first reinforcement learningbased agents with local communication, gradient minimization
with asymptotic convergence guarantees using linear function approximation. This method, networked agents, promotes decentralized training and decentralized execution paradigm where agents
learn locally and are suitable for real world infrastructure and robot
teams domains [8]. However, there are three limitations to their
work: the assumption of full state and action space observability,
the slower sample efficiency due to stochastic online learning [19]

and the risk of convergence to a sub optimal Nash-equilibrium
[38]. Chen et al. [5] extend networked agents for a mild form of partial observabilityâ€“jointly observable state, where the state is fully
observable taking into account all the of agentsâ€™ perceptions [23].
Their system adapts a well known CTDE algorithm, MADDPG [18],
to the DTDE approach, allowing agents to use consensus iterations
on weights to emulate parameter sharing. Our approach extends
previous work, by developing networked agents under partial observability and applying gradient tracking as means of performing
localized weight updates of a shared objective.

7

CONCLUSION AND FUTURE WORK

We contribute with distributed value decomposition network DVDN
whereby agents learn by performing consensus on temporal difference. DVDN replicates the gradient updates of VDN in the decentralized setting using communication. We investigate the effect of the
learning signal in two settings, heterogeneous and homogeneous
agents. Homogeneous agents also align their policiesâ€™ parameters
using gradient tracking. We implement our approach in ten scenarios with partial observability with favorable results. Our approach
is suitable to settings where there is no centralized node. For future work we intend on incorporating belief sharing to DVDN, and
the extension of other families of factorized ğ‘„-function such as
QMIX [27] to the distributed setting.

REFERENCES
[1] Emanuele Antonioni, Vincenzo Suriani, Francesco Riccio, and Daniele Nardi. 2021.
Game Strategies for Physical Robot Soccer Players: A Survey. IEEE Transactions
on Games 13, 4 (2021), 342â€“357.
[2] Wendelin BÃ¶hmer, Vitaly Kurin, and Shimon Whiteson. 2020. Deep coordination
graphs. In Proceedings of the 37th International Conference on Machine Learning
(ICMLâ€™20). JMLR.org, Article 92, 12 pages.
[3] Guido Carnevale, Francesco Farina, Ivano Notarnicola, and Giuseppe Notarstefano. 2023. GTAdam: Gradient Tracking With Adaptive Momentum for Distributed Online Optimization. IEEE Transactions on Control of Network Systems
10, 3 (2023), 1436â€“1448. https://doi.org/10.1109/TCNS.2022.3232519
[4] Tsung-Hui Chang, Mingyi Hong, Hoi-To Wai, Xinwei Zhang, and Songtao Lu.
2020. Distributed Learning in the Nonconvex World: From batch data to streaming
and beyond. IEEE Signal Processing Magazine 37, 3 (2020), 26â€“38.
[5] Dingyang Chen, Yile Li, and Qi Zhang. 2022. Communication-Efficient ActorCritic Methods for Homogeneous Markov Games. In International Conference on
Learning Representations. https://openreview.net/forum?id=xy_2w3J3kH
[6] Pedro A. Forero, Alfonso Cano, and Georgios B. Giannakis. 2010. ConsensusBased Distributed Support Vector Machines. J. Mach. Learn. Res. 11 (aug 2010),
1663â€“1707.
[7] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
Russell. 2020. Adversarial Policies: Attacking Deep Reinforcement Learning. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.
[8] Sven Gronauer and Klaus Diepold. 2022. Multi-agent deep reinforcement learning:
a survey. Artificial Intelligence Review 55, 2 (01 Feb 2022), 895â€“943.
[9] Carlos Guestrin, Michail G. Lagoudakis, and Ronald Parr. 2002. Coordinated
Reinforcement Learning. In Proceedings of the Nineteenth International Conference
on Machine Learning (ICML â€™02). 227â€“234.
[10] Jayesh K. Gupta, Maxim Egorov, and Mykel Kochenderfer. 2017. Cooperative
Multi-agent Control Using Deep Reinforcement Learning. In Autonomous Agents
and Multiagent Systems, Gita Sukthankar and Juan A. Rodriguez-Aguilar (Eds.).
Springer International Publishing, Cham, 66â€“83.
[11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and
David Meger. 2018. Deep reinforcement learning that matters. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative
Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on
Educational Advances in Artificial Intelligence (New Orleans, Louisiana, USA)
(AAAIâ€™18/IAAIâ€™18/EAAIâ€™18). AAAI Press, Article 392, 8 pages.
[12] Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar. 2017. Collaborative Deep Learning in Fixed Topology Networks. In Proceedings of the
31st International Conference on Neural Information Processing Systems (Long

Beach, California, USA) (NIPSâ€™17). Curran Associates Inc., Red Hook, NY, USA,
5906â€“5916.
[13] Mert Kayaalp, Fatima Ghadieh, and Ali H. Sayed. 2023. Policy Evaluation in
Decentralized POMDPs With Belief Sharing. IEEE Open Journal of Control Systems
2 (2023), 125â€“145. https://doi.org/10.1109/OJCSYS.2023.3277760
[14] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.).
[15] Jelle R. Kok and Nikos Vlassis. 2006. Collaborative Multiagent Reinforcement
Learning by Payoff Propagation. J. Mach. Learn. Res. 7 (Dec. 2006), 1789â€“1828.
[16] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control
with deep reinforcement learning. In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
[17] Qifeng Lin and Qing Ling. 2021. Decentralized TD(0) With Gradient Tracking.
IEEE Signal Processing Letters 28 (2021), 723â€“727.
[18] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. 2017.
Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.
Neural Information Processing Systems (NIPS) (2017).
[19] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous Methods for Deep Reinforcement Learning. In Proceedings of The 33rd
International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 48), Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR,
New York, New York, USA, 1928â€“1937.
[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2015. Playing Atari with
Deep Reinforcement Learning. Nature 518, 7540 (2015), 529â€“533.
[21] Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. 2022. DAdam: A
Consensus-Based Distributed Adaptive Gradient Method for Online Optimization.
IEEE Transactions on Signal Processing 70 (2022), 6065â€“6079.
[22] Angelia Nedic. 2020. Distributed Gradient Methods for Convex Machine Learning
Problems in Networks: Distributed Optimization. IEEE Signal Processing Magazine
37, 3 (2020), 92â€“101. https://doi.org/10.1109/MSP.2020.2975210
[23] Frans A. Oliehoek and Christopher Amato. 2016. A Concise Introduction to
Decentralized POMDPs. Springer International Publishing, Cham.
[24] Georgios Papoudakis, Filippos Christianos, Lukas SchÃ¤fer, and Stefano Albrecht.
2021. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in
Cooperative Tasks. In Proceedings of the Neural Information Processing Systems
Track on Datasets and Benchmarks, J. Vanschoren and S. Yeung (Eds.), Vol. 1.
[25] Frano Petric, Marijana Peti, and Stjepan Bogdan. 2023. Multi-agent Coordination Based on POMDPs and Consensus for Active Perception. In Intelligent
Autonomous Systems 17. Springer Nature Switzerland, Cham, 690â€“705.
[26] Guannan Qu and Na Li. 2018. Harnessing Smoothness to Accelerate Distributed
Optimization. IEEE Transactions on Control of Network Systems 5, 3 (2018), 1245â€“
1260.
[27] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
Foerster, and Shimon Whiteson. 2018. QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. In Proceedings of the 35th
International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.). PMLR, 4295â€“4304.
[28] Pedro P. Santos, Diogo S. Carvalho, Miguel Vasco, Alberto Sardinha, Pedro A.
Santos, Ana Paiva, and Francisco S. Melo. 2024. Centralized Training with
Hybrid Execution in Multi-Agent Reinforcement Learning. In Proceedings of the
23rd International Conference on Autonomous Agents and Multiagent Systems,
AAMAS 2024, Auckland, New Zealand, May 6-10, 2024. International Foundation
for Autonomous Agents and Multiagent Systems / ACM, 2453â€“2455.
[29] Jun Sun, Gang Wang, Georgios B. Giannakis, Qinmin Yang, and Zaiyue Yang.
2020. Finite-Time Analysis of Decentralized Temporal-Difference Learning with
Linear Function Approximation. In Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning
Research, Vol. 108), Silvia Chiappa and Roberto Calandra (Eds.). PMLR, 4485â€“4495.
[30] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl
Tuyls, and Thore Graepel. 2018. Value-Decomposition Networks For Cooperative
Multi-Agent Learning Based On Team Reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (Stockholm,
Sweden) (AAMAS â€™18). International Foundation for Autonomous Agents and
Multiagent Systems, Richland, SC, 2085â€“2087.
[31] Ming Tan. 1993. Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents. In In Proceedings of the Tenth International Conference on Machine
Learning. Morgan Kaufmann, 330â€“337.
[32] Reza Joseph Torbati, Shubham Lohiya, Shivika Singh, Meher Shashwat Nigam,
and Harish Ravichandar. 2023. MARBLER: An Open Platform for Standardized
Evaluation of Multi-Robot Reinforcement Learning Algorithms. In International
Symposium on Multi-Robot and Multi-Agent Systems, MRS 2023, Boston, MA, USA,

December 4-5, 2023. IEEE, 57â€“63.
[33] John N Tsitsiklis. 1985. Problems in decentralized decision making and computation.
Ph.D. Dissertation. Massachusetts Institute of Technology. Advisor(s) Michael
Athans.
[34] Gang Wang, Songtao Lu, Georgios B. Giannakis, Gerald Tesauro, and Jian Sun.
2020. Decentralized TD tracking with linear function approximation and its
finite-time analysis. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPSâ€™20). Curran
Associates Inc., Red Hook, NY, USA, Article 1154, 11 pages.
[35] Andreas Wichert and Luis Sacouto. 2021. Machine Learning â€” A Journey to Deep
Learning: with Exercises and Answers. World Scientific Pub Co Inc.
[36] Lin Xiao and S. Boyd. 2003. Fast linear iterations for distributed averaging. In 42nd
IEEE International Conference on Decision and Control (IEEE Cat. No.03CH37475),
Vol. 5. 4997â€“5002 Vol.5.
[37] Lin Xiao, Stephen Boyd, and Seung-Jean Kim. 2007. Distributed average consensus
with least-mean-square deviation. J. Parallel and Distrib. Comput. 67, 1 (2007),
33â€“46.
[38] Kaiqing Zhang, Zhuoran Yang, and Tamer BaÅŸar. 2021. Multi-Agent Reinforcement
Learning: A Selective Overview of Theories and Algorithms. Springer International
Publishing, Cham, 321â€“384.
[39] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. 2018. Fully
Decentralized Multi-Agent Reinforcement Learning with Networked Agents. In
Proceedings of the 35th International Conference on Machine Learning (Proceedings
of Machine Learning Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.).
PMLR, StockholmsmÃ¤ssan, Stockholm, 5872â€“5881.
[40] Changgang Zheng, Benjamin Rienecker, and Noa Zilberman. 2023. QCMP: Load
Balancing via In-Network Reinforcement Learning. In Proceedings of the 2nd
ACM SIGCOMM Workshop on Future of Internet Routing & Addressing (New York,
NY, USA) (FIRA â€™23). Association for Computing Machinery, 35â€“40.

A

EXTENDED BACKGROUND

The metropolis weights associated to an arbitrary graph G(N, E) is given by [37]:

ğ›¼ğ‘›,ğ‘š =

Ã
ï£±
ğ›¼ = 1 âˆ’ ğ‘šâ‰ ğ‘› ğ›¼ğ‘›,ğ‘š
ï£´
ï£´
ï£² ğ‘›,ğ‘›
ï£´
1
1+max(ğ‘‘ (ğ‘›),ğ‘‘ (ğ‘š) )
ï£´
ï£´
ï£´0
ï£³

if ğ‘› = ğ‘š
if (ğ‘›, ğ‘š) âˆˆ E

(19)

otherwise

Where ğ‘‘ (ğ‘›) is the degree of nodes ğ‘›, and E the edge set of the graph. In a distributed setting, each agent ğ‘› communicates its degree only to
its neighbors. Then, each agent ğ‘› can determine its weight ğ›¼ğ‘›,ğ‘š for each of its neighbors ğ‘š âˆˆ N . After these preliminary computations,
agents are free to perform consensus.

B

PROOF

Additive value decomposition is computed by summing the ğ‘„-values from each agentsâ€™ ğ‘„-functions. Therefore, the error increment at the
output layer of the ğ‘„-functions, following value decomposition, equals the sum of individual temporal differences. To demonstrate Fact 1,
we proceed starting with VDNâ€™s loss function, then we perform differentiation at the value decomposition layer. Finally, we show that the
error increment at the output layer of an arbitrary agent ğ‘–, following differentiation, equals the joint temporal difference.
i2
1 âˆ‘ï¸ h VDN
ğ‘¦
âˆ’ ğ‘„ VDN (ğ‘œ, ğ‘; ğœ”)
from (7)
ğ‘ ğœ
"ğ‘
#2
ğ‘
âˆ‘ï¸
1 âˆ‘ï¸ âˆ‘ï¸
â€²
âˆ’
=
ğ‘Ÿ + ğ›¾ max ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘¢ğ‘– ; ğœ”ğ‘– ) âˆ’
ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– )
ğ‘¢ğ‘–
ğ‘ ğœ ğ‘–=1
ğ‘–=1
#2
"ğ‘
ğ‘
âˆ‘ï¸
1 âˆ‘ï¸ âˆ‘ï¸
ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– )
from (9)
ğ‘¦ğ‘– âˆ’
=
ğ‘ ğœ ğ‘–=1
ğ‘–=1
#2
"ğ‘
ğ‘
âˆ‘ï¸
1 âˆ‘ï¸ âˆ‘ï¸
ğ‘ğ‘–
define ğ‘ğ‘– := ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– )
ğ‘¦ğ‘– âˆ’
=
ğ‘ ğœ ğ‘–=1
ğ‘–=1
"ğ‘
#2

1 âˆ‘ï¸ âˆ‘ï¸
=
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘–
ğ‘ ğœ ğ‘–=1

â„“ (ğœ”; ğœ) =

=

from (8) and (6)

ğ‘
âˆ’1 âˆ‘ï¸
ğ‘
ğ‘
âˆ‘ï¸

o
2
1 âˆ‘ï¸ n âˆ‘ï¸
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— .
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– + 2
ğ‘ ğœ ğ‘–=1
ğ‘–=1 ğ‘— >ğ‘–

(20)

By taking the derivative of the loss in (20) with respect to ğœ”:
ğ‘
ğ‘
âˆ’1 âˆ‘ï¸
ğ‘
âˆ‘ï¸

o



ğœ•
2 âˆ‘ï¸ n âˆ‘ï¸
â„“ (ğœ”; ğœ) = âˆ’
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ğ‘– +
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ ğ‘— + ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— âˆ‡ğœ” ğ‘ğ‘–
ğœ•ğœ”
ğ‘ ğœ ğ‘–=1
ğ‘–=1 ğ‘— >ğ‘–

=âˆ’

ğ‘
ğ‘
âˆ’1 âˆ‘ï¸
ğ‘
ğ‘
âˆ’1 âˆ‘ï¸
ğ‘
o
âˆ‘ï¸
âˆ‘ï¸



2 âˆ‘ï¸ n âˆ‘ï¸
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ğ‘– +
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ ğ‘— +
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ ğ‘—
ğ‘ ğœ ğ‘–=1
ğ‘–=1 ğ‘— >ğ‘–
ğ‘—=1 ğ‘–> ğ‘—

=âˆ’

ğ‘
ğ‘ âˆ‘ï¸
ğ‘
o
âˆ‘ï¸


2 âˆ‘ï¸ n âˆ‘ï¸
ğ‘¦ğ‘– âˆ’ ğ‘ğ‘– âˆ‡ğœ” ğ‘ğ‘– +
ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— âˆ‡ğœ” ğ‘ğ‘–
ğ‘ ğœ ğ‘–=1
ğ‘–=1 ğ‘—â‰ ğ‘–

(21)
ï£¹
ğ‘ ï£®ğ‘
ï£º
2 âˆ‘ï¸ âˆ‘ï¸ ï£¯ï£¯âˆ‘ï¸
=âˆ’
(ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— ) ï£ºï£º âˆ‡ğœ” ğ‘ğ‘–
ğ‘ ğœ ğ‘–=1 ï£¯ï£¯ ğ‘—=1
ï£º
ï£°
ï£»
ğ‘
ğ‘
âˆ‘ï¸
âˆ‘ï¸
âˆ‘ï¸

2
=âˆ’
ğ›¿ ğ‘— âˆ‡ğœ” ğ‘ğ‘–
ğ‘ ğœ ğ‘–=1 ğ‘—=1
ğ‘

=âˆ’

2 âˆ‘ï¸ âˆ‘ï¸
ğ›¿âˆ‡ğœ” ğ‘ğ‘–
ğ‘ ğœ ğ‘–=1

The joint temporal difference ğ›¿ = Î£ ğ‘— ğ›¿ ğ‘— (21) represents the sum of local temporal difference errors across agents. Using the fact that agent-wise
ğ‘„-functions factorize, making the local weight updates dependent on individual weights ğœ”ğ‘– , ğ‘– âˆˆ N . The local weight update at agent ğ‘–, is a
term from (21):
2 âˆ‘ï¸
ğœ•
2 âˆ‘ï¸
ğ›¿âˆ‡ğœ” ğ‘ğ‘– = âˆ’
ğ›¿âˆ‡ğœ”ğ‘– ğ‘ğ‘–
â–¡
(22)
â„“ (ğœ”ğ‘– ; ğœ) = âˆ’
ğœ•ğœ”ğ‘–
ğ‘ ğœ
ğ‘ ğœ
Since the weight updates are computed sequentially from the output layer to inner layers ([35]), the term in (22) consists of the product
of a constant factor ğ‘2 , a network term ğ›¿ and the gradient of inner layers âˆ‡ğœ”ğ‘– ğ‘ğ‘– . Since the gradients of the inner layers depend on local
informationâ€“we complete the demonstration.

C

EXTENDED METHOD

Figure 3 contrasts the value decomposition sum operation (a) with temporal difference consensus (b) in (13). In Figure 3 (a) Bidirectional
arrows indicate the last step of forward pass: Agents contribute with their temporal difference and receive the joint temporal difference
and back-propagation resumes. In Figure 3 (b) Bidirectional arrows indicate the temporal difference consensus step over an arbitrary
communication graph. Following consensus iteration agents perform back-propagation using the error increment ğ›¿ğ‘– + ğ›¿Ë†âˆ’ğ‘– (dashed arrow).

(a) Value decomposition

(b) TD consensus

Figure 3: To the left, the value decomposition diagram where the value decomposition layer performs addition. To the right, the
temporal difference consensus agents user peer-to-peer communication over an arbitrary strongly connected graph. Bidirectional
arrows indicate the last step of forward pass (a) or TD consensus (b). The dashed line represent back-propagation algorithm.

D

EXTENDED EXPERIMENTS

This section presents pseudo-codes for distributed value decomposition networks with and without gradient tracking, and includes the
hyper parameters for each version of the algorithm as well as baselines. For simplicity, the code listings do not include an important code
level optimization: mini-batch updates. Mini-batch updates accelerate learning by providing extra trajectories for the optimizer. Those
extra trajectories are stored in a replay buffer, and are sampled randomly at each parameter update. On a distributed system the sampled
trajectories have to be aligned to the same episode for the consensus rounds to succeed. However, implementing this synchronization scheme
is straightforward; every agent starts with the same long list of random replay buffer indicesâ€“it is their common knowledge. They draw the
trajectories by sequentially consulting the batch size indices from this list, obtaining the same trajectories from the replay buffer.

D.1

Distributed VDN

The distributed value decomposition networks in Listing 1 is a serial implementation of a synchronous distributed algorithm. Agents
communicate with their closest neighbors and have no knowledge of other teammates. It consists of three parts: initialization, local
communication and local gradient minimization.
(0)
The ğ‘„-function parameters ğœ”ğ‘– are initialized randomly, the Adam optimizer parameters [14] are a constant learning rate ğœ‚ , moving
average parameter ğ›½ 1 , moving sum of squares parameter ğ›½ 2 , and a numerical stability parameter ğœ– . Typical choices for hyper parameters are
ğ›½ 1 = 0.9, ğ›½ 2 = 0.999 and ğœ– = 10 âˆ’8 .

The outer loop variable ğ‘˜ (lines 1-8) controls both the number of training episodes and consensus rounds. In parallel, agents collect
individual trajectories ğœğ‘– (line 2). Agents compute temporal difference by performing a forward pass on their ğ‘„-networks (line 3). A new
communication graph G (ğ‘˜ ) is randomly generated at each episode ğ‘˜ as a strongly connected graph, ensuring the algorithm is not dependent
on the network topology. Agents then exchange degree information with neighbors to derive the consensus weights (line 4). Next, they
(ğ‘˜ )
perform a single iteration of consensus to obtain an estimation of the joint temporal difference ğ›¿Ë†âˆ’ğ‘– (line 5). The gradient is calculated based
(ğ‘˜ )

on the DVDN equation (14) (line 6). Finally, the weight ğœ”ğ‘–

is adjusted using the gradient and adaptive momentum weights 8 (line 7).

Algorithm 1 Distributed Value Decomposition Network
(0)

Require: ğœ”ğ‘– arbitrary, ğœ‚, ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, ğœ– = 10 âˆ’8 , AdamOptğ‘– = Adam(ğœ‚, ğ›½ 1, ğ›½ 2, ğœ–)
1: for ğ‘˜ = 1, Â· Â· Â· do
2:
Observe ğœğ‘– , âˆ€ğ‘– âˆˆ N
(ğ‘˜ )
(ğ‘˜ âˆ’1)
3:
ğ›¿ğ‘– = ğ‘…Â¯ + ğ›¾ maxğ‘¢ ğ‘„ğ‘– (ğ‘œğ‘–â€², ğ‘¢; ğœ”ğ‘–âˆ’ ) âˆ’ ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘–
)
(ğ‘˜ )

4:
5:
6:
7:

Hear communication channel and compute ğ›¼ğ‘–,ğ‘— âˆ€ğ‘— âˆˆ N using (19)
Ã
(ğ‘˜ )
(ğ‘˜ ) (ğ‘˜ ) 
(ğ‘˜ )
ğ›¿Ë†
=ğ‘
ğ›¿
âˆ’ğ›¿
(ğ‘˜ ) ğ›¼

âˆ’ğ‘–
ğ‘–,ğ‘— ğ‘—
ğ‘–
ğ‘— âˆˆ Nğ‘–
(ğ‘˜ )
(ğ‘˜ âˆ’1)
(ğ‘˜ )
Ë†
ğ‘”ğ‘– = âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘–
; ğœğ‘– , ğ›¿ âˆ’ğ‘– ) (Gradient of (14))
(ğ‘˜ )
(ğ‘˜ âˆ’1) (ğ‘˜ )
ğœ”ğ‘– = AdamOptiğ‘– .ğ‘†ğ‘¡ğ‘’ğ‘ (ğœ”ğ‘–
, ğ‘”ğ‘– )

8: end for

D.2

DVDN with Gradient Tracking

Homogeneous agents are interchangeable, having identical observation and action sets. In centralized training, multiple agents share a
single network parameterized by ğœ” to enable efficient learning from a distributed set of experiences [10]. In decentralized training, due
to truncated consensus iterations, agents may not have the same copy of parameters. We combine the ideas of Algorithm 1 with gradient
tracking (4). Our approach utilizes previous work, Carnevale et al. [3] that integrated gradient tracking with Adam optimizer. Listing 2
reports a serial implementation of the synchronous distributed value decomposition networks with gradient tracking (DVDN (GT)):
Algorithm 2 Distributed Value Decomposition Networks With Gradient Tracking
(0)

Require: ğœ”ğ‘– arbitrary, ğœ‚, ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, ğœ– = 10 âˆ’8 , AdamOptğ‘– = Adam(ğœ‚, ğ›½ 1, ğ›½ 2, ğœ–)
1: for ğ‘˜ = 1, Â· Â· Â· do
2:
Observe ğœğ‘– âˆ€ğ‘– âˆˆ N
(ğ‘˜ )
(ğ‘˜ âˆ’1)
3:
ğ›¿ğ‘– = ğ‘…Â¯ + ğ›¾ maxğ‘¢ ğ‘„ğ‘– (ğ‘œğ‘–â€², ğ‘¢; ğœ”ğ‘–âˆ’ ) âˆ’ ğ‘„ğ‘– (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘–
)
(ğ‘˜ )

4:
5:
6:
7:
8:
9:
10:
11:

Hear communication channel and compute ğ›¼ğ‘–,ğ‘— âˆ€ğ‘— âˆˆ N using (19)
Ã
(ğ‘˜ )
(ğ‘˜ ) (ğ‘˜ ) 
(ğ‘˜ )
ğ›¿Ë†
=ğ‘
ğ›¿
âˆ’ğ›¿
(18a)
(ğ‘˜ ) ğ›¼
âˆ’ğ‘–
(ğ‘˜ )

ğ‘–,ğ‘—
ğ‘— âˆˆ Nğ‘–
(ğ‘˜ âˆ’1)

ğ‘—
(ğ‘˜ )

ğ‘–

ğ‘”ğ‘– = âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘–
; ğœğ‘– , ğ›¿Ë†âˆ’ğ‘– ) (18b)
if ğ‘˜ = 1 then
(1)
(1)
(0)
ğ‘§ (1) = ğ‘”ğ‘–
and ğœ”Ëœ ğ‘– = ğœ”ğ‘–
else
Ã
(ğ‘˜ ) (ğ‘˜ âˆ’1)
(ğ‘˜ )
(ğ‘˜ âˆ’1)
(ğ‘˜ )
+ ğ‘”ğ‘– âˆ’ ğ‘”ğ‘–
ğ‘§ğ‘– = ğ‘— âˆˆ N (ğ‘˜ ) ğ›¼ğ‘–,ğ‘— ğ‘§ ğ‘—
ğ‘–
Ã
(ğ‘˜ )
(ğ‘˜ ) (ğ‘˜ âˆ’1)
(18d)
ğœ”Ëœ ğ‘– = ğ‘— âˆˆ N (ğ‘˜ ) ğ›¼ğ‘–,ğ‘— ğœ” ğ‘—

(18c)

ğ‘–

end if
(ğ‘˜ )
(ğ‘˜ ) (ğ‘˜ )
13:
ğœ”ğ‘– = AdamOptğ‘– .ğ‘†ğ‘¡ğ‘’ğ‘ (ğœ”Ëœ ğ‘– , ğ‘§ğ‘– )
14: end for
12:

(ğ‘˜ )

Algorithm 2 requires each agent to hold two internal states: The previous local gradient update ğ‘”ğ‘–
(ğ‘˜ )

previous team gradient update ğ‘§ğ‘–

(ğ‘˜ âˆ’1)

= âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘–

(ğ‘˜ )

; ğœğ‘– , ğ›¿Ë†âˆ’ğ‘– ), and the

that is a local estimation to:
âˆ‡ğœ” â„“ (ğœ”; ğœ, ğ›¿) â‰ˆ

1 âˆ‘ï¸
(ğ‘˜ âˆ’1)
(ğ‘˜ )
âˆ‡ğœ”ğ‘– â„“ (ğœ”ğ‘–
; ğœğ‘– , ğ›¿Ë†âˆ’ğ‘– ).
ğ‘
ğ‘–âˆˆN

Moreover, DVDN (GT), requires three consensus steps for every training episode:
8 This is a standard step for weight updates. Refer to https://pytorch.org/docs/stable/generated/torch.optim.Adam.html

(23)

Table 2: Hyperparameters used for hyperparameter search [24].
Hyperparameter
hidden dimension
learning rate
reward standardization
network type

target update

evaluation epsilon

epsilon anneal

Description
The number of neurons in the hidden layer of the neural networks.
Regulates the step size of the gradient updates.
Performs reward normalization.
Feed forward and fully connected (FC) or Gated Recurrent Unit
(GRU).
In 0.01 (soft) mode the target network is updated with parameters from behavior network every training step, following a
exponentially weighted moving average, with innovation rate
0.01. In 200 (hard) mode the target network is updated with a
full copy from the behavior network at every 200 training steps.
Epsilon is a hyperparameter controlling the sampling of suboptimal actions from ğ‘„-value based policies. The epsilon greedy
criteria provides a way for the agent to experiment with nongreedy actions actions to find more profitable states. Evaluation
epsilon regulates the rate of non-greedy actions taken during
evaluation checkpoints.
The number of episode steps to reach the minimum epsilon for
ğ‘„-value based policies.

(ğ‘˜ )

(ğ‘˜ )

â€¢ The network estimated JTD ğ›¿Ë†âˆ’ğ‘– used in producing a local estimation (ğ›¿ğ‘–
(ğ‘˜ )

â€¢ The team gradient ğ‘§ğ‘–

Values
64/128
0.0001/ 0.0003/ 0.0005
True
FC/GRU

200 (hard)/ 0.01 (soft)

0.0/0.05

125,000/500,000 (LBF),
125,000/500,000 (MPE),
50,000/200,000
(MARBLER).

(ğ‘˜ )

+ ğ›¿Ë†âˆ’ğ‘– ) to the joint temporal difference ğ›¿ (ğ‘˜ ) (line 5).

approximates the left-hand side of (23) (lines 8, 10).
(ğ‘˜ )

â€¢ The parameter consensus aligns parameter estimations: ğœ”ğ‘–

â‰ˆ ğœ” (ğ‘˜ ) (line 11).

The execution flow of the algorithm proceeds as follows:
The outer loop variable ğ‘˜ (lines 1-13) controls the number of training episodes and consensus rounds. In parallel, agents collect individual
trajectories ğœğ‘– (line 2), and compute temporal differences by performing a forward pass on their ğ‘„-networks (line 3). A new communication
graph G (ğ‘˜ ) is randomly generated at each episode ğ‘˜ as a strongly connected graph, ensuring the algorithm is not dependent on the network
topology. Agents exchange degree information with neighbors to derive the consensus weights (line 4). Next, they perform a single iteration
(ğ‘˜ )
of consensus to obtain an estimation of the joint temporal difference ğ›¿Ë†âˆ’ğ‘– (line 5). The gradient is calculated based on the DVDN equation
(line 6).
(1)
The first episode initializes the team gradient ğ‘§ğ‘– and consensus parameter ğœ”Ëœ (1) (line 8). Subsequent episodes update the team gradient
(ğ‘˜ âˆ’1)

by performing one consensus iteration on ğ‘§ğ‘–

and adding the difference between the current and previous local gradient (line 10). Agents
(ğ‘˜ âˆ’1)

also perform one consensus iteration on the parameters ğ‘¤ğ‘–
momentum weights (line 13).

D.3

(ğ‘˜ )

(line 11). Finally, the weight ğœ”ğ‘–

is adjusted using the gradient and adaptive

Hyperparameters

We conduct a grid search over a range of hyper parameter values for three environments. Table 2 details hyperparameters values and
descriptions. The hyperparameters of the DVDN algorithms are optimized for one scenario within each environment and remain consistent
across other scenarios within the same environment. The selected scenarios for hyperparameter tuning are LBF medium instance, Tag for the
multi-particle environment, and Simple Navigation for MARBLER. Each combination of hyperparameter is evaluated for three seeds, with
the optimal hyperparameters being those that yield the maximum average return across the seeds. For LBF and MPE environments, baseline
algorithms were not optimized and their hyperparameter configurations are as stated in [24]. In contrast, the IQL and VDN baselines in the
MARBLER environment have their hyperparameters optimized through our grid search. The VDN (PS) configuration remains unchanged
from [32]. Finally, the hyperparameter search process used two million timesteps for environments LBF and MPE, and four hundred thousand
timesteps for MARBLER.
The selected hyperparameters are reported in Table 3 and Table 4 for the heterogeneous agents and homogeneous agents settings
respectively for environments LBF and MPE. Table 5 reports the selected hyperparameters for both settings for the MARBLER environment.

Table 3: Selected hyperparameters for the heterogeneous agents setting for environments Level-based foraging and Multiparticle environment. We adopt the hyperparameters from [24] for the baselines (without parameter sharing) .

IQL

LBF
DVDN

VDN

IQL

MPE
DVDN

VDN

64
0.0003
True
0.05
GRU
250,000
200 (hard)

128
0.0001
True
0.05
GRU
500,000
0.01 (soft)

64
0.0001
True
0.05
GRU
500,000
200 (hard)

128
0.0005
True
0.0
FC
500,000
0.01 (soft)

128
0.0003
True
0.0
GRU
500,000
200 (hard)

128
0.0005
True
0.0
FC
125,000
200 (hard)

Hyperparameter
hidden dimension
learning rate
reward standardization
evaluation epsilon
network type
epsilon anneal time
target update

Table 4: Selected hyperparameters for the homogeneous agents setting. We adopt the hyperparameters from [24] for the
baselines (with parameter sharing).

IQL

LBF
DVDN (GT)

VDN (PS)

IQL

MPE
DVDN (GT)

VDN (PS)

64
0.0003
True
0.05
GRU
250,000
200 (hard)

128
0.0003
True
0.00
GRU
500,000
0.01 (soft)

128
0.0003
True
0.00
GRU
500,000
0.01 (soft)

128
0.0005
True
0.0
FC
500,000
0.01 (soft)

128
0.0005
True
0.0
GRU
125,000
200 (hard)

128
0.0005
True
0.0
FC
125,000
200 (hard)

Hyperparameter
hidden dimension
learning rate
reward standardization
evaluation epsilon
network type
epsilon anneal time
target update

Table 5: Selected hyperparameters for MARBLER environment. We adopt the hyperparameters from [32] for VDN.

Hyperparameter
hidden dimension
learning rate
reward standardization
evaluation epsilon
network type
epsilon anneal time
target update

E

Heterogeneous
IQL
DVDN
128
0.0005
True
0.00
GRU
50,000
200 (hard)

128
0.0005
True
0.00
GRU
50,000
0.01 (soft)

VDN

64
0.0005
False
0.00
GRU
50,000
200 (hard)

Homogenous
DVDN (GT) VDN (PS)
128
0.0005
True
0.05
GRU
50,000
0.01 (soft)

128
0.0003
False
0.0
GRU
50,000
200 (hard)

EXTENDED RESULTS

This section presents additional results and an explanation for DVDNâ€™s performance on the MPE (Tag) scenario.

E.1

Level-based foraging

Figure 4 (a) and (b) depict the learning curves for DVDNâ€™s performance in the Easy and Hard instances of the LBF environment, respectively.
The results corroborate those shown in Figure 1 (a), indicating that all three algorithms have similar performance, and DVDNâ€™s learning
curve mirrors VDNâ€™s learning curve, despite information loss resulting from switching topology dynamics. DVDN better approximates the
performance of VDN.
Figures 5 (a), (b), and (c) illustrate ablation plots for DVDNâ€™s key component: joint temporal difference consensus. The inclusion of JTD
consensus improves performance across the three tasks (higher is better).
Figure 6 (a) and (b) show the learning curves for DVDN (GT) in the LBF environment for Easy and Hard instances, respectively. These
results support previous findings in Figure 1 (a), where DVDN (GT)â€™s learning curve approximates to VDN (PS)â€™s. In the homogeneous
agents setting, parameter sharing and gradient tracking significantly improve the performance of both algorithms. Figure 7 (a) and (b)

DVDN
IQL
VDN

0.8
0.6
0.4
0.2
0.0

DVDN
IQL
VDN

1.0
Episodic Return

Episodic Return

1.0

0.8
0.6
0.4
0.2

0

1

2
3
4
Environment Timesteps

5
1e6

0.0

0

1

(a) Easy

2
3
4
Environment Timesteps

5
1e6

(b) Hard

Figure 4: The remaining performance plots of the algorithms for heterogeneous agents in the LBF environment, with IQL
represented in orange, VDN in chestnut, DVDN in blue. The markers represent the average episodic returns and the shaded
area represent the 95% bootstrap CIs. All algorithms have about the same performance. Particularly, DVDNâ€™s learning curve is
similar to VDNâ€™s.

0.78
0.76
0.74
0.72
IQL

Groups

JTD

(a) Easy

Maximum Average Returns

Maximum Average Returns

Maximum Average Returns

0.44

0.62

0.80

0.61
0.60
0.59
0.58
0.57
IQL

Groups

(b) Medium

JTD

0.42
0.40
0.38
0.36
0.34
0.32
0.30

IQL

Groups

JTD

(c) Hard

Figure 5: Ablation plots for heterogeneous agents in the LBF environment. The IQL (control) group has no consensus, while JTD
group has joint temporal difference consensus. Notably, JTD consensus leads to significant improvement in results across tasks.

provide ablation plots for DVDN (GT)â€™s two main components: joint temporal difference (JTD) and gradient tracking (GT). Interestingly, JTD
degrades performance individually in the hard task (Figure 7 (b)), but combining it with GT yields better results than using GT alone. As
the task difficulty increases, the benefit of JTD diminishes. We speculate that for hard tasks, a near optimal policy involves independent
behavior, such as walking to the nearest fruit and attempting a load action. This finding is supported by previous work [24].

E.2

Multi -particle environment

Figure 8 (a) and (b) show the learning curves for DVDNâ€™s performance, heterogeneous setting, for the individual scenarios belonging to the
MPE environment: Adversary, and Tag tasks respectively. For Adversary (and Spread) that have dense rewards, algorithms have similar
sample-efficiency. However, the best policies for Adversary are generated by DVDN, while the best policies for Spread are generated by IQL.
For those scenarios it is hard to assess differences in performances between DVDN and VDN. For the Tag scenario, we see that both DVDN
and VDN present fast performance growth in the first million timesteps and have their highest performing average episodic returns close to
timestep four million. The policies generated however, are very different for reasons we elaborate in a qualitative analysis in Appendix E.4.
Figure 9 (a), (b) and (c) show the ablation plots for DVDNâ€™s main component: joint temporal difference consensus. JTD consensus improves
the performance for the three tasks (higher better).
Figure 10 (a) and (b) show the learning curves for DVDN (GT) in the MPE environment for Adversary and Tag scenarios, respectively.
Similar to the heterogeneous setting, sample efficiency are approximately the same for all algorithms in the Adversary task, making it difficult
to distinguish performances between algorithms. In scenario (a), IQL outperforms other algorithms, while in the Spread scenario, DVDN (GT)
achieves the best results. However, in the Tag scenario, DVDN (GT) underperforms other approaches. Qualitative analysis (Appendix E.4)
reveals that performance comparable to other algorithms was achieved during hyperparameter tuning. Suggesting a limitation in our (and

1.0

0.8

0.8

Episodic Return

Episodic Return

1.0

0.6
0.4
0.2
0.0

0

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
1e6
Environment Timesteps

DVDN (GT)
IQL
VDN (PS)

0.6
0.4
0.2
0.0

0

1

(a) Easy

2
3
4
Environment Timesteps

5
1e6

(b) Hard

Figure 6: The remaining performance plots of the algorithms for homogeneous agents in the LBF environment, with IQL
represented in orange, VDN (PS) in chestnut, and DVDN (GT) in blue. The markers represent the average episodic returns and
the shaded area represents the 95% bootstrap CIs. All algorithms exhibit comparable performance, with DVDNâ€™s learning curve
resembling VDNâ€™s.

0.50

0.85

Maximum Average Returns

Maximum Average Returns

0.90
0.80
0.75
0.70
0.65

0.45
0.40
0.35
0.30

0.60
IQL

GT

Groups

(a) Easy

JTD

GT+JTD

IQL

GT

Groups

JTD

GT+JTD

(b) Hard

Figure 7: Extra ablation plots for homogeneous agents in the LBF environment. The IQL (control) group has no consensus.
The GT group performs gradient tracking. The JTD group performs joint temporal difference consensus. The GT+JTD group
combines gradient tracking and JTD consensus. In (a) both JTD consensus and gradient tracking independently improve
performance, and are better combined. Conversely, in (b) JTD by itself decreases the performance, but when combined with GT
it shows superior results.

previous work) experimentation protocol rather than an intrinsic flaw in our design. Additionally, the 95% bootstrap CI is wide, indicating
that a few of the 10 independent runs generated higher-quality policies.
Figure 11 (a) and (b) display ablation plots for DVDN (GT)â€™s two main components: joint temporal difference consensus (JTD) and gradient
tracking (GT). For the Tag scenario (Figure 11 (b)), JTD improves performance, while the improvement is marginal for the Adversary scenario
(Figure 11 (a)). Conversely, GT degrades performance compared to IQL in the Adversary and Tag scenarios (Figure 11 (a) and (b)). Combining
GT and JTD consensus (group GT+JTD) results in decreased performance compared to applying JTD consensus alone.

E.3

MARBLER

Figure 12 (a), (b), and (c) show the learning curves for DVDNâ€™s performance in the heterogeneous agents setting, for the remaining scenarios
belonging to the MARBLER environment: Arctic, PCP and Warehouse tasks respectively. DVDN and VDN have similar learning curves for
the three scenarios, but DVDN outperforms VDN in scenarios (b) and (c). In scenario (c), IQL outperforms VDN, but both algorithms are still
surpassed by DVDN.
Figure 13 (a), (b), (c) and (d) show the ablation plots for DVDNâ€™s main component, joint temporal difference consensus, for the scenarios
Arctic, Material, PCP and Warehouse. JTD consensus enhances performance for the scenarios (b) and (d), with minor improvements in
scenarios (a) and (c).

35

5

30

0

Episodic Return

Episodic Return

10

5
10
15

DVDN
IQL
VDN
1
2
3
4
5
1e6
Environment Timesteps

20
0

25
20
15
10

DVDN
IQL
VDN
1
2
3
4
5
Environment Timesteps
1e6

5
0

0

(a) Adversary

(b) Tag

8
6
4
2
0

IQL

Groups

TTD

(a) Adversary

136
137
138
139
140
141
142
143
144

32.5
Maximum Average Returns

10
Maximum Average Returns

Maximum of Mean Return over Seeds

Figure 8: The remaining performance plots of the algorithms for heterogeneous agents in the MPE environment, with IQL
represented in orange, VDN in chestnut, and DVDN in blue. The markers represent the average episodic returns and the shaded
area represents the 95% bootstrap CIs. For scenario (a) algorithms have about the same sample-efficiency making it hard to
discriminate between different approaches. However, for scenario (b), DVDN outperforms other approaches.

30.0
27.5
25.0
22.5
20.0
17.5
15.0

IQL

Groups

(b) Spread

JTD

IQL

Groups

JTD

(c) Tag

Figure 9: Ablation plots for heterogeneous agents in the MPE environment. The IQL (control) group has no consensus, while
JTD group has joint temporal difference consensus. JTD consensus leads to significant improvement in results for tasks (b) and
(c).

Figure 14 (a), (b), and (c) show the learning curves for DVDN (GT) performance in the MARBLER environment for Arctic, PCP and
Warehouse scenarios respectively. In scenario (a), DVDN (GT)â€™s learning curve is similar to IQLâ€™s, but it underperforms in this highly
heterogeneous agents scenario. Gradient tracking assumes that all agents follow a similar policy, but in this assumption is mostly violated in
Arctic as there are agents with different roles. Whereas VDN (PS) show a modest improvement moving from the heterogeneous setting, DVDN
(GT) shows a degradation in performance. We hypothesize that differences from VDNâ€™s loss function and DVDN (GT)â€™s loss function (15)
plays a key role in this result. However, in scenarios (a) and (b), DVDN (GT)â€™s learning curve is similar to VDN (PS)â€™s.
Figure 15 (a), (b), and (c) show ablation plots for DVDNs (GT)â€™s two main components: joint temporal difference consensus and gradient
tracking. For the three scenarios GT degrades performance The net effect of combining both GT and JTD consensus (group GT+JTD) is
beneficial in scenario (c).

E.4

Qualitative Analysis

In this section, we elaborate on the high performance for DVDN as observed in Table 1 and Figure 8. Despite being designed to emulate
VDN, DVDN surpasses both baselines by a substantial margin in Figure 8, which is unexpected. A qualitative analysis of the optimal joint
policy for VDN reveals that shows two agents assume the blocker role, performing random walk, which is effective in restraining the preyâ€™s
movement while a third agent acts as the pursuer, chasing the prey and relying on the blockers to corner it. The behavior policy of DVDN is
different: Two agents act as runners, while the third serves the pursuer. The runners agents flee from the grid, leaving its regular bounds.
The prey adopts a passive behavior, allowing the pursuer to bump into it. These discrepancies rise two questions: How is this possible? And
why does it work?

25
20
Episodic Return

Episodic Return

10
5
0
5
10
15
20
25
0

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
1e6
Environment Timesteps

15
10

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
Environment Timesteps
1e6

5
0

0

(a) Adversary

(b) Tag

10.0
9.5
9.0
8.5
8.0
7.5
7.0
6.5
6.0

Maximum Average Returns

Maximum Average Returns

Figure 10: The remaining performance plots of the algorithms for homogeneous agents in the MPE environment, with IQL in
orange, VDN (PS) in chestnut and DVDN (GT) in blue. The markers represent the average episodic returns and the shaded area
represent the 95% bootstrap CIs. DVDN (GT) underperforms other approaches. We expand the qualitative reasons for the under
performance of DVDN (GT) in (Appendix E.4).

IQL

GT

Groups

JTD

(a) Adversary

GT+JTD

26
24
22
20
18
16
14
12
10
IQL

GT

Groups

JTD

GT+JTD

(b) Tag

Figure 11: Extra ablation plots for the homogeneous in the MPE environment. The IQL (control) group has no consensus.
The GT group performs gradient tracking. The JTD group performs joint temporal difference consensus. The GT+JTD group
combines gradient tracking and JTD consensus. In (a) and (b) JTD consensus improves performance, while the groups GT and
GT+JTD underperforms IQL.
As to how two algorithms that are supposed to generate the same kind of behavior, instead generate such distinct behaviors, the answer is
exploration. Although, DVDNâ€™s learning curve is supposed to approach VDNâ€™s during random exploration the learning paths may diverge
wildly. Moreover, controlled state exploration is one factor separating the central agent from the centralized learners. DVDNâ€™s estimation of
JTD may lead to a greater variety of behavior due to information loss. In this case, those exploratory behaviors turned out to be useful. As to
why does this apparently odd behavior works. It works because the prey agent was trained using deep reinforcement learning [DDPG, 16]
and its policy is fixed, making the prey agent part of the environment from the perspective of the training predator agents. During random
state space exploration, predators generate new unseen states by the prey. Subsequently, the preyâ€™s policy shows a poor generalization, it
cannot reason about behaviors of the predators. This effect is called adversarial training, where the adversaries are the predators. Gleave
et al. [7] found that adversaries win by generating states that are adversarial, i.e., that exploit the pretrained victimâ€™s policy, not particularly
by behaving strategically.
In Figure 10 (b), DVDN (GT) underperforms the baselines. Figure 11 (b) reveals that GT has a detrimental impact on performance when
compared to IQL group. Nevertheless, this configuration was the best performing during hyperparameter grid search. In fact, if we replace
the ten randomly seeded test evaluations by the three sequential seeded hyperparameter grid search trials, performance improves drastically.
DVDN (GT) matches baselinesâ€™ performance (see Fig. 16 and Table 6). This finding suggests a limitation [24]â€™s hyperparameter search
protocol. Three trials may not be sufficient to assess experiments with high variability [11], such as predator-and-prey.

50
60
DVDN
IQL
VDN
1
2
3
4
5
1e6
Environment Timesteps

70
80

0

140
120
100
80
60
40
20
0
20

30

DVDN
IQL
VDN

DVDN
IQL
VDN

25
Episodic Return

40

Episodic Return

Episodic Return

30

20
15
10
5
0
5

0

1

2
3
4
Environment Timesteps

(a) Arctic

5
1e6

0

1

2
3
4
Environment Timesteps

(b) PCP

5
1e6

(c) Warehouse

IQL

Groups

JTD

(a) Arctic

IQL

Groups

JTD

134.0
133.5
133.0
132.5
132.0
131.5
131.0
130.5
130.0

(b) Material

29
Maximum Average Returns

19
18
17
16
15
14
13
12
11

Maximum Average Returns

36
37
38
39
40
41
42
43
44

Maximum Average Returns

Maximum Average Returns

Figure 12: The remaining performance plots of the algorithms for heterogeneous agents in the MARBLER environment, with
IQL represented in orange, VDN in chestnut, and DVDN in blue. The markers represent the average episodic returns and the
shaded area represent the 95% bootstrap CIs. For scenario (a), (b), and (c) DVDN and VDN present similar behavior during
training. DVDN outperforms with margin IQL for scenarios (a) and (c), and VDN for scenarios (b) and (c).

IQL

Groups

28
27
26
25
24
23

JTD

IQL

(c) PCP

Groups

JTD

(d) Warehouse

Figure 13: Ablation plots for heterogeneous agents in the MARBLER environment. The IQL group has no consensus, while JTD
group (control group) has joint temporal difference consensus. For scenarios (b) and (d), the JTD consensus improves results
significantly. Whereas, for scenarios (a) and (c), JTD consensus improves results marginally.

50
60
70
80

0

DVDN (GT)
IQL
VDN
1
2
3
4
5
1e6
Environment Timesteps
(a) Arctic

DVDN (GT)
IQL
VDN (PS)

30
Episodic Return

40

Episodic Return

Episodic Return

30

140
120
100
80
60
40
20
0
20

0

DVDN (GT)
IQL
VDN (PS)
1
2
3
4
5
1e6
Environment Timesteps
(b) PCP

20
10
0
10

0

1

2
3
4
Environment Timesteps

5
1e6

(c) Warehouse

Figure 14: The remaining performance of the algorithms for homogeneous agents in the MARBLER environment, with IQL in
orange, VDN (PS) in chestnut and DVDN (GT) in blue. The markers represent the average episodic returns and the shaded area
represent the 95% bootstrap CIs. In scenario (a) and (b), DVDN (GT) is outperformed by IQL, while in scenario (c) DVDN (GT)
outperforms IQL.

30

45

130.0

28

50
55
60
65
70

IQL

GT

Groups

(a) Arctic

JTD

GT+JTD

Maximum Average Returns

132.5
Maximum Average Returns

Maximum Average Returns

40

127.5
125.0
122.5
120.0
117.5
115.0

26
24
22
20
18

IQL

GT

Groups

JTD

GT+JTD

IQL

(b) PCP

GT

Groups

JTD

GT+JTD

(c) Warehouse

Figure 15: Extra ablation plots for the homogeneous in the MARBLER environment. The IQL (control) group has no consensus.
The GT group performs gradient tracking. The JTD group performs joint temporal difference consensus. The GT+JTD group
combines gradient tracking and JTD consensus. In scenario (c), JTD consensus and GT combined improve the performance.
Conversely, in (a) and (b), GT+JTD the performance degrades.

Algorithm
VDN (PS)
DVDN (GT)
IQL

PP
23.18
(âˆ’1.68, 1.63)
23.20
(âˆ’1.00, 1.40)
23.00
(âˆ’1.79, 1.72)

Table 6: Performance for predatorand-prey task, DVDN (GT) results on
Figure 16: Performance for predator-and-prey task, DVDN (GT) results on hyper- hyperparameter grid search trials.
parameter grid search trials.

