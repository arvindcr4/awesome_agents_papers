Decentralized multi-agent reinforcement learning algorithm
using a cluster-synchronized laser network

arXiv:2407.09124v1 [cs.LG] 12 Jul 2024

Shun Kotoku1 , Takatomo Mihana1,∗ , André Röhm1 , and Ryoichi Horisaki1
Department of Information Physics and Computing, Graduate School of Information Science and Technology,
The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8656, Japan.
∗
Corresponding author. Email: takatomo_mihana@ipc.i.u-tokyo.ac.jp

Abstract Multi-agent reinforcement learning (MARL) studies crucial principles that are applicable to a
variety of fields, including wireless networking and autonomous driving. We propose a photonic-based
decision-making algorithm to address one of the most fundamental problems in MARL, called the
competitive multi-armed bandit (CMAB) problem. Our numerical simulations demonstrate that chaotic
oscillations and cluster synchronization of optically coupled lasers, along with our proposed decentralized
coupling adjustment, efficiently balance exploration and exploitation while facilitating cooperative
decision-making without explicitly sharing information among agents. Our study demonstrates how
decentralized reinforcement learning can be achieved by exploiting complex physical processes controlled by
simple algorithms.

1

Introduction

Reinforcement learning [1] is a subfield of machine learning where an agent interacts with environments
through trial and error, optimizing its actions based on a reward norm. Multi-agent reinforcement learning
(MARL) [2–4] is an extended form of reinforcement learning where multiple agents simultaneously interact
with environments as well as other agents. Its applications have now spread across various areas, including
wireless networking [5], autonomous driving [6], power distribution networks [7], etc. We focus on one of the
most fundamental settings of MARL: the competitive multi-armed bandit (CMAB) problem [8].
The CMAB problem, an extension of the multi-armed bandit (MAB) problem [9], features several players
(agents) who repeatedly select among multiple slot machines (arms) with unknown hit probabilities (uncertain environments). Players need to balance two opposing strategies: exploration, which involves players
attempting to identify the profitable slots by exploring various options, and exploitation, where players attempt to focus on the most promising option by exploiting the acquired information about the environments.
The term ‘competitive’ can be misleading in the context of MARL. In this setting, it does not imply a zerosum reward distribution as it might in the literature [4]. Instead, the rewards are divided among players
when they select the same slot, and players cooperatively aim to maximize the total rewards for the team.
Therefore, avoiding selection collisions is important for effective decision-making. The CMAB problem can
be considered a form of stateless MARL, provided the reward environment remains constant.
In the past decade, several studies regarding the MAB [10–13] or CMAB [14–17] problem have been
conducted in the context of photonic accelerators [18]. Photonic accelerators involve promising approaches
to accelerate or improve performances of specific computations by utilizing photonics, such as its high propagation speed, broad bandwidth, and parallelism. Our study features a cooperative decision-making system
using a laser network [16, 17], which leverages the rapid and chaotic oscillations and synchronization observed
in optically interconnected semiconductor lasers for avoiding slot selection collisions in the CMAB problem.
Specifically, the laser-based decision-making system employs two kinds of coexisting laser interactions:
a leader-laggard relationship and cluster synchronization. The leader-laggard relationship [19, 20] observed
in coupled lasers with chaotic dynamics is similar to delay-synchronization, where the temporal waveform
of the ‘leader’ laser’s intensity precedes that of the ‘laggard’ laser by the coupling delay time. Unlike full
delay-synchronization commonly observed in delay-coupled networks, the roles of the leader and laggard
spontaneously exchange. Cluster synchronization [21–24] refers to a state where nodes in a delay-coupled
1

network separate into several clusters, and the oscillations of nodes within the same cluster synchronize
without delay.
The decision-making system assigns a subset of lasers in a network to players. Each laser corresponds
to each player’s selection of slot machines, and players select slots based on the leader laser at each decision
point. Cluster synchronization in a laser network enables players to avoid selection collisions. The previous
study [17] has revealed the balance of the leader-laggard relationship can be controlled while maintaining twocluster synchronization by adjusting coupling strengths between lasers in four laser networks, corresponding
to the CMAB problem with two players and two slots.
However, the previous work [17] has been limited to investigating laser dynamics and interactions with
parameters fixed for each setting, and it has not addressed the CMAB problem itself. Specifically, it has
not yet been discussed how coupling strengths should be dynamically adjusted. An earlier approach [15]
has also addressed photonic-based collective decision-making, but the method presumed the existence of a
centralized controller besides the players. Physics-based methods, and especially photonic approaches, for
decision-making are likely to find applications in edge-devices, which lack the local computing or networking
resources to implement traditional digital learning algorithms. In the case of such edge-devices, it is more
realistic and practical to assume that each player can only observe the result of its own slot selection. In
addition, the previous work [17] has dealt only with four-laser networks for the two-player and two-slot CMAB
problem. Selection only truly matters, when the number of arms exceeds the number of players, as otherwise
avoiding selection collisions is the only important aspect of maximizing rewards for the team.
In this study, we propose an enlarged laser network that exhibits stable cluster synchronization, as well
as an algorithm to adjust the coupling strengths of the laser network based on the player’s individually
received rewards to efficiently address the competitive multi-armed bandit (CMAB) problem with more slot
machines than players. Our numerical simulations validate the viability of our proposed laser network and
decentralized coupling adjustment algorithm for tackling the CMAB problem.
Note that we do not assert that the proposed photonic-based decision-making algorithm is superior to
existing methods, such as the Upper Confidence Bound 1 (UCB1) [25] for the single-agent configuration and
its variants for the multi-agent settings, in all aspects and can entirely replace them. Rather, we focus on
exploring the application of nontrivial photonics phenomena accompanied by straightforward and intuitive
learning algorithms. Our results show that the proposed system is indeed reaching good performance in the
explored circumstances.

2

Method and dynamics investigation

2.1

System configuration

We study the case of the competitive multi-armed bandit (CMAB) problem with two players and three slot
machines. Table 1 shows an example of a payoff matrix for the rewards in the CMAB problem with a
typical problem setting of the hit probabilities of Slots A, B, and C: PA = 0.4, PB = 0.6, PC = 0.6. We
assume a temporally static reward environment for simplicity. Each column in Table 1 represents the slot

Table 1: An example of a payoff matrix for the rewards in a competitive multi-armed bandit problem with
two players and three slot machines (PA = 0.4, PB = 0.6, PC = 0.6). The first and second elements represent
the expected rewards for Players 1 and 2. The underlined portions along the diagonal indicate where selection
collisions happen.
1

Slot A

Slot B

Slot C

A

(0.2, 0.2)

(0.6, 0.4)

(0.6, 0.4)

B

(0.4, 0.6)

(0.3, 0.3)

(0.6, 0.6)

C

(0.4, 0.6)

(0.6, 0.6)

(0.3, 0.3)

2

2

(a)

Player 2

Player 1

Laser
Laser
2A

Laser
1A

2A

Laser

(b)

Laser
1B
1B

Laser
Laser
2B
2B

Laser
1C

Laser
2C

1A

2A

1B

2B

1C

2C

Figure 1: Schematic illustration of our proposed system. (a) A six-laser network to address the competitive
multi-armed bandit problem with two players and three slot machines. r1♯ and r2♯ (♯ = bl, or, ye) represent
the attenuation rates adjusted by Players 1 and 2. κ♯ represents the total multiplicative coupling strength.
(b) Typical laser intensity waveforms of the six-laser network obtained through numerical simulations. Lasers
drawn with identical colors are synchronized.

selected by Player 1, and each row represents the slot by Player 2. The underlined portions along the diagonal
indicate cases where the players select the same slot, and the rewards are thus divided among them. Due to
this division of spoils, the team’s sum rewards are lower in such cases compared to other selection patterns.
Naturally, players are encouraged to focus on selecting the most high-paying two out of three slots in the
exploitation phase after sufficient exploration to estimate the hit probabilities of the slots. Thus, it is optimal
to narrow down their selections to Slots B and C, which yield the maximum potential rewards in the problem
setting. But at the same time, they need to avoid both selecting the same slot.
Figure 1 (a) illustrates the proposed six-laser network to address the CMAB problem with a two-player
and three-slot-machine configuration. Note that the coupling delay time between lasers, τ , is assumed to be
uniform for all connections. Lasers 1A, 1B, and 1C are allocated to Player 1, and Lasers 2A, 2B, and 2C to
Player 2. Each laser corresponds to a slot machine selected by a player.
In the collective decision-making system, players select a slot corresponding to the leader among the three
lasers assigned to them. The leader is defined by short-term cross-correlation (STCC) values in the following
3

formulas [12]:
I1A (u) − I¯1A I1B (u − τ ) − I¯1B,τ
du,
σ1A
σ1B,τ
t−τ
Z t
I1B (u) − I¯1B I1C (u − τ ) − I¯1C,τ
C1B (t) =
du,
σ1B
σ1C,τ
t−τ
Z t
I1C (u) − I¯1C I1A (u − τ ) − I¯1A,τ
C1C (t) =
du,
σ1C
σ1A,τ
t−τ
Z t

C1A (t) =

(1)
(2)
(3)

where Ik (t) denotes the laser intensity of Laser k, I¯k and σk represent the average and the standard deviation
of Ik (t), over the period of τ . I¯k,τ and σk,τ have comparable meanings, but the time window for the calculation
is shifted back by τ . Player 1 considers the laser with the smallest STCC value of the three as the leader.
Therefore, for example, Player 1 selects Slot B when C1B < C1A and C1B < C1C hold. STCC values for
Player 2, C2A , C2B , and C2C , are defined similarly to Eqs. (1)–(3).
In the six-laser network depicted in Fig. 1 (a), lasers with the same color, Lasers 1A and 2B, Lasers 1B
and 2C, and Lasers 1C and 2A, are in perfect zero-lag synchronization when the coupling strengths with the
same color are uniform, as shown in Fig. 1 (b). Here, we define cluster bl (blue) to consist of Lasers 1A and
2B, cluster or (orange) to consist of Lasers 1B and 2C, and cluster ye (yellow) to consist of Lasers 1C and
2A. Note that this color differentiation is just for convenience and is not related to the wavelength of the
lasers. Players avoid selection collisions without sharing information about their slot selections and resultant
rewards thanks to this three-cluster synchronization. For instance, when Laser 1B is regarded as the leader
and Player 1 selects Slot B, due to the cluster synchronization with Laser 1B, Laser 2C is considered the
leader by Player 2, who will then select Slot C. Thus, Players 1 and 2 always select different slot machines
and achieve conflict avoidance in the CMAB problem.

2.2

Leader probability and cluster synchronization

Our previous work [17] has revealed that the leader-laggard relationship, which corresponds to the selection
of slot machines, can be controlled by adjusting the coupling strengths while maintaining two-cluster synchronization in a four-laser network. Here, we numerically examine how the leader-laggard relationship in a
six-laser network, as illustrated in Fig. 1 (a), can be controlled with coupling strengths.
The model of chaotic semiconductor lasers is described by Lang-Kobayashi equations [26] as follows:


X
1 + iα GN [Nk (t) − N0 ]
1
dEk (t)
=
−
Ek (t) +
κl→k El (t − τ ) exp (−iωτ ),
(4)
2
dt
2
1 + ε|Ek (t)|
τp
Nk (t) GN [Nk (t) − N0 ]
dNk (t)
=J−
−
|Ek (t)|2 ,
(5)
dt
τs
1 + ε|Ek (t)|2
where Ek (t) and Nk (t) represent the complex electric field and the carrier density of Laser k at time t. The
laser intensity is the square of the absolute value of the complex electric field: Ik (t) = |Ek (t)|2 . ω represents
the angular frequency of the lasers. As a prerequisite for cluster synchronization, the laser frequencies within
the same cluster should be equal. For simplicity, we assume identical frequencies for all lasers. κl→k is the
coupling strength from Laser l to Laser k, where the value is zero between unconnected lasers.
The parameters are shown in Table 2 and are taken from references [12, 16, 17] except for the injection
current J. Previous studies set the value of J/Jth to 1.1 (Jth is injection current threshold) so that the lasers
exhibit low-frequency fluctuation dynamics. However, according to our current examination, the setting of
J/Jth = 2.0 seems more promising for stable cluster synchronization and rapid decision-making.
The necessary conditions for cluster synchronization in the six-laser network are κ1A→1B = κ2B→2C
(≡ κbl ), κ1B→1C = κ1B→2A (≡ κor ), and κ2A→1A = κ2A→2B (≡ κye ). We conduct the numerical simulations to obtain temporal waveforms of laser intensity Ik (t), with the balance of coupling strengths varied
while maintaining the requirements. Afterward, STCC values are calculated to determine the leader-laggard
relationship for each setting of coupling strengths.
Figure 2 (a) and (b) show STCC time series acquired from numerical simulations. Coupling strengths
are configured as κor = κye = 45 ns−1 and (a) κbl = 38 ns−1 , and (b) κbl = 45 ns−1 . Perfect three-cluster
4

Table 2: Parameters of the Lang-Kobayashi equations.
Symbol

Parameter

Value

GN

Gain coefficient

8.40 × 10−13 m3 s−1

N0

Carrier density at transparency

1.40 × 1024 m−3

ε

Gain saturation coefficient

2.0 × 10−23

τp

Photon lifetime

1.927 × 10−12 s

τs

Carrier lifetime

2.04 × 10−9 s

α

Linewidth enhancement factor

3.0

τ

Coupling delay time of light

5.0 × 10−9 s

J/Jth

Normalized injection current

2.0

λ

Optical wavelength of lasers

1.537 × 10−6 m

synchronization is observed for both settings. C1B and C2C are the smallest in most cases for (a), while
the frequent switching among C1A , C1B , and C1C and that among C2A , C2B , and C2C are observed for the
symmetric setting of coupling strengths, (b).
We define the leader probability as the ratio of times each laser is identified as a leader. The leader
probabilities are calculated from STCC waveforms spanning 10 000 ns after waiting for a sufficiently long
transient of 3000 ns. We repeat the computation 50 times with the initial states of the lasers randomized
each time and calculate the average leader probabilities of lasers for each κbl . κor and κye are fixed at 45 ns−1 ,
and κbl is varied from 5 ns−1 to 60 ns−1 , in increments of 1 ns−1 . The average leader probabilities of the six
lasers in the network are shown in Fig. 2 (c). The leader probabilities of 1A, 1B, and 1C (2A, 2B, and 2C) are
approximately one-third for the symmetric coupling strengths, κbl = 45 ns−1 . The leader probability of 1B
(2C) is greater than one-third with κbl smaller than 45 ns−1 , and converges toward 1.0 around κbl = 30 ns−1 .
An attempt to reduce the value of κbl smaller than 5 ns−1 brings about the cessation of the chaotic oscillations
in the laser network, and we want to focus on the region in which the chaotic leader-laggard relationship and
cluster synchronization coexist. Conversely, the leader probability of 1B (2C) is smaller than one-third for
κbl > 45 ns−1 , and converges toward 0 around κbl = 60 ns−1 .
Note that similar results are obtained when κye and κbl (κbl and κor ) are fixed and κor (κye ) is varied
because of the symmetry of the network.
Therefore, when players estimate the combination of Slots B and C is optimal after the exploration phase,
they are encouraged to decrease κbl in the exploitation phase so that Lasers 1B and 2C are more likely to be
leaders. Similarly, if the option of Slots C and A is good, players lower κor , and if that of Slots A and B is
good, they lower κye .

2.3

Decentralized coupling adjustment

The decision-making process in our proposed system is as follows. First, two players observe the laser
intensities and calculate short-term cross-correlation (STCC) values to determine the leader of the three
lasers allocated to them. Players select the slot corresponding to the leader, i.e., the one with the lowest
STCC value. In perfect cluster synchronization, players naturally and independently select different slots.
Based on the resulting slot rewards, players adjust the optical attenuation rates, represented by r1♯ and r2♯
(♯ = bl, or, ye) in Fig. 1 (a), to improve their subsequent slot selections, and to narrow down to the most
high-paying two slots. Hereinafter, the whole process is referred to as one ‘Play,’ and players attempt to
maximize the team’s accumulated rewards over a significant number of Plays.
The total multiplicative coupling strength κ♯ is denoted as κ♯ = r1♯ r2♯ κ, where κ represents the coupling
strength without amplification or attenuation. The value of κ is determined by parameters of Lang-Kobayashi
equations, and κ = 155.3 ns−1 in this paper.
Therefore, the remaining challenge is to design an algorithm, for how players should adjust their optical
attenuation rates. With the goal of being usable in edge-computing scenarios, we designed the algorithm
5

(a)

(b)

(c)

1B
1C
1A

2C
2A
2B

Figure 2: Numerical simulation results to investigate the leader probabilities of the six-laser network shown
in Fig. 1. (a) Short-term cross-correlation (STCC) waveforms calculated for coupling strength κor = κye =
45 ns−1 and κbl = 38 ns−1 . (b) κbl = 45 ns−1 . (c) The relationship between the coupling strengths and leader
probabilities. κor and κye are fixed at 45 ns−1 , and κbl is set from 5 ns−1 to 60 ns−1 .
to be simple and robust. The underlying idea is inspired by tug-of-war (TOW) dynamics [27], originally
designed for the multi-armed bandit (MAB) problem with a single player. Although several algorithms have
been proposed to solve the multi-agent multi-armed bandit problem, including the Upper Confidence Bound
1 (UCB1)-based methods [28–30], the TOW-based algorithm effectively leverages the probabilistic property
observed in photonics, and it has been applied for addressing the MAB problem with a single player using
chaotic lasers [11].
A modified version of the TOW-based algorithm for the CMAB problem with multiple players has been
proposed later [31], and it has been applied for the software-based conflict avoidance principle in decisionmaking using laser networks [15]. However, the method requires players to share information about the slot
rewards, which we consider inappropriate given the partial observability assumptions of the CMAB problem.
Our proposed method, decentralized coupling adjustment (DCA), is described by the following formulas:
Q1,X = 2P̄1,X − (P̄1,2nd + P̄1,3rd ),


rlow
r1♯ = rupp


rini + rstep Q1,S(♯)

(rini + rstep Q1,S(♯) < rlow ),
(rupp < rini + rstep Q1,S(♯) ),
(otherwise).

6

(6)

(7)

Q1,X represents the excess hit probability of Slot X (X = A, B, C) over the baseline measured by Player 1,
where P̄1,X is the observed hit probability of Slot X by Player 1, and P̄1,2nd and P̄1,3rd denote the observed
hit probability of the second and third-best slot machine for Player 1. The excess hit probability Q1,X of
the best and second slot becomes positive, whereas that of the third (worst) slot becomes negative after a
sufficient number of Plays.
To keep the coupling strengths
κ♯ = r1♯ r2♯ κ within
p
p a certain range, [κlow , κupp ], lower and upper limits of
the attenuation rates rlow ≡ κlow /κ and rupp ≡ κupp /κ are established. κlow and κupp are hyperparameters that determine the balance between exploration and exploitation. rini = (rlow + rupp )/2 is the baseline
attenuation rate. rstep is a scaling factor that affects the strengths of exploitation. S(♯) is a mapping from
coloring to slot machines: S(bl) = A, S(or) = B, S(ye) = C.
The excess hit probability of Slot X observed by Player 2, Q2,X , is defined similarly to Eq. (6), and Player
2 adjusts the attenuation rate r2♯ in the same manner as Eq. (7).
With the DCA method, players control the leader probabilities of lasers and narrow down their slot
selections. For example, under the problem setting shown in Table 1, when Slot A appears low-rewarding to
Player 1, Q1,A decreases, and r1bl = rini + rstep Q1,S(bl) = rini + rstep Q1,A is reduced. Player 2 also lowers
r2bl , and thus, κbl = r1bl r2bl κ is reduced. As shown in Fig. 2 (c), when κbl is smaller than κor and κye ,
the leader probabilities of Lasers 1B and 2C becomes higher. Therefore, players can correctly select Slots B
and C at a high rate. Notably, players share no information about observed hit probabilities of slots during
decision-making in our proposed algorithm, which sets it apart from previous studies [15, 31].

3

Decision-making simulations

3.1

Results of a single trial

We numerically solve the competitive multi-armed bandit (CMAB) problem with our decision-making algorithm to prove its effectiveness in balancing exploration and exploitation while avoiding selection conflicts.
First, we present a sample result of a single decision-making trial in this section.
To clearly illustrate the transition from exploration to exploitation, which is the essential aspect of the
MAB problem, we employ the configurations of parameters of the CMAB problem and decentralized coupling
adjustment (DCA) shown in Table 3 as control conditions. Hit probabilities of slot machines are the same as
the setting shown in Table 1. Therefore, selecting Slots B and C is an optimal and correct option. Scaling
factor rstep and lower and upper bound of coupling strengths κlow and κupp are the hyperparameters of the
DCA method, whose influence will be discussed in detail in Sec. 3.3.
The decision-making interval is set to 1.0 ns. Our preliminary analysis indicates that the decision-making
frequency should be comparable to or slower than the coupling delay time τ . Otherwise, consecutive slot
selections are positively correlated, leading to ineffective exploration and more Plays required for appropriate
exploitation.
Figure 3 presents the numerical simulation results of a single decision-making trial. Figure 3 (a) shows
the short-term cross-correlation (STCC) values. Overall, the waveforms of C1A and C2B , C1B and C2C , and
C1C and C2A remain synchronized due to the consistent three-cluster synchronization. Therefore, as shown
in Fig. 3 (b), Players 1 and 2 always select different slots, completely avoiding conflicts.

Table 3: Parameters of the competitive multi-armed bandit (CMAB) problem and decentralized coupling
adjustment (DCA) used in Sec. 3.1.
Symbol

Parameter

Value

(PA , PB , PC )

Hit probabilities of slots

(0.4, 0.6, 0.6)

rstep

Scaling factor

1.0

κlow

Lower bound of coupling strength

38 ns−1

κupp

Upper bound of coupling strength

45 ns−1

7

(a)

(b)

Player 1
Player 2

(c)

(d)

Figure 3: Numerical simulation results of a single trial of decision-making. (a) Short-term cross-correlation
(STCC). (b) Slot machines selected by Player 1 (red) and Player 2 (black). (c) The excess hit probabilities
of slots Q1,X and Q2,X (X = A, B, C). (d) Total coupling strengths κ♯ = r1♯ r2♯ κ (♯ = bl, or, ye).

In the first approximately 60 Plays, the relative order of C1A , C1B , and C1C , and that of C2A , C2B , and
C2C , frequently change, as shown in Fig. 3 (a). Hence, players explore three options in turn, as shown in
Fig. 3 (b). The fluctuations in the excess hit probabilities of slots Q1,X and Q2,X (X = A, B, C) in response
to the actual received rewards are shown in Fig. 3 (c). Based on these Q1,X and Q2,X , Players 1 and 2 adjust
the attenuation rates r1♯ and r2♯ (♯ = bl, or, ye) according to DCA, and the resulting coupling strengths
κ♯ = r1♯ r2♯ κ fluctuate in the exploration phase, as shown in Fig. 3 (d).
Subsequently, after the exploration of roughly 60 Plays, C1B and C2C are the smallest in most cases,
indicating that Lasers 1B and 2C are leading. Thus, players primarily select the optimal option, Slots B and
C, as shown in Fig. 3 (a) and (b). As expected, the excess hit probabilities for the best and second slot, Q1,B ,
Q1,C , Q2,B , and Q2,C , become positive, while those for the worst slot, Q1,A and Q2,A , become negative (see
Fig. 3 (c) after around 60 Plays). Therefore, as shown in Fig. 3 (d), κbl converges to κlow = 38 ns−1 whereas
κor and κye converge to κupp = 45 ns−1 , resulting in the appropriate focused selections of slots.
Thus, in the case of (PA , PB , PC ) = (0.4, 0.6, 0.6), our proposed algorithm operates as intended and can
successfully solve the CMAB problem. However, it is necessary to evaluate the decision-making performance
under various problem scenarios to demonstrate the robustness of the system.
8

3.2

Impact of reward distributions

In this section, we investigate the behavior of our proposed decentralized coupling adjustment (DCA) under
various reward distributions to validate its robustness. For simplicity, we assume henceforth that the optimal
combination remains Slots B and C. Considering the probabilistic nature of both the chaotic-laser-based
decision-making system and the rewards from slot machines in the competitive multi-armed bandit (CMAB)
problem, performance should be assessed as an average over multiple trials.
We introduce the correct decision ratio (CDR) [10] and regret as performance metrics. CDR(m) is defined
by the ratio at which players correctly select the most profitable set of slot machines at m-th Play over a
predefined number of trials. Each trial consists of 1000 Plays of decision-making, and this process is repeated
for 2000 trials, with the six lasers randomly initialized each time. A faster convergence of CDR to a value
closer to 1.0 indicates a better performance. Regret is the difference between the expected cumulative reward
for the perfect selections and the average actual reward. The team’s expected reward for the perfect selections
in 1000 Plays is denoted as R∗ = (P1st + P2nd ) × 1000, where P1st and P2nd represent the hit probability
of the best and second-best slot, respectively. The actual reward averaged over 2000 trials is denoted as R.
We employ absolute regret R∗ − R as well as relative regret (R∗ − R)/R∗ . Lower regret generally signifies a
better performance, but absolute regret depends strongly on the magnitude of R∗ as well.
Using the evaluation metrics, we first assess the algorithm’s performance for the following five settings, including the control condition addressed in Sec. 3.1: (PA , PB , PC ) = (0.1, 0.9, 0.9), (0.2, 0.8, 0.8), (0.3, 0.7, 0.7),
(0.4, 0.6, 0.6), (0.45, 0.55, 0.55). These cases involve symmetrical hit probabilities for the top two slots, simplifying the analysis of the system’s behavior. The smaller the difference between the good and bad slots, the
more trials are required for accurate inferences. Therefore, it is expected that the learning speed is slower
with the narrower gap between PA and PB (= PC ) if the algorithm is reasonable.
Figure 4 (a) illustrates the evolution of CDR with regard to Play for five different reward distributions
with symmetric hit probabilities for the top two slots. As shown in Fig. 4 (a), CDR gradually increases with
the number of Plays and converges to a value slightly greater than 0.95 as the number of Plays reaches 1000,
except for (PA , PB , PC ) = (0.45, 0.55, 0.55) case. When the difference between reward probabilities becomes
larger, it is easier to identify good and bad slots. Indeed, the proposed DCA method is able to quickly switch
to exploitation in such cases, as one expects from an MAB algorithm. Even with a challenging setting like
the 0.45 versus 0.55 situation, CDR clearly keeps rising, indicating the effectiveness of our proposed DCA
method.
Next, to further discuss the system’s characteristics, performance evaluations for the following five configurations are conducted: (PA , PB , PC ) = (0.1, 0.3, 0.9), (0.1, 0.3, 0.5), (0.1, 0.3, 0.3), (0.1, 0.5, 0.3), (0.1, 0.9, 0.3).
Inconsistent behaviors can be observed when comparing such asymmetric configurations. They have commonly hit probabilities of 0.3 and 0.1 for the second and third-best slots. Also, these setups include pairs
that are essentially the same but differ in the order of slot allocations, namely, (PA , PB , PC ) = (0.1, 0.3, 0.9)
and (0.1, 0.9, 0.3), and (0.1, 0.3, 0.5) and (0.1, 0.5, 0.3). Ideally, the system is expected to exhibit consistent
behavior for these pairs due to their equivalence.
Figure 4 (b) shows the CDR changes against Play for the five reward distributions, including ones with
asymmetric hit probabilities. These problem configurations also exhibit a steady increase in CDR, validating that the DCA method correctly operates in this regard. However, when comparing the results
for the pairs of equivalent problem setups, the outcomes shown in Fig. 4 (b) are not desirable. In settings (PA , PB , PC ) = (0.1, 0.9, 0.3) and (0.1, 0.5, 0.3), the convergence of the CDR is slower than in settings
(PA , PB , PC ) = (0.1, 0.3, 0.9) and (0.1, 0.3, 0.5) despite their equivalent levels of difficulty. It is even slower
compared to the setting (PA , PB , PC ) = (0.1, 0.3, 0.3), which is intuitively more challenging since the best
slot is even harder to identify there. This behavior can be attributed to the asymmetric leader probabilities
of lasers that mainly act as ‘laggards’ due to the unidirectional coupling in the six-laser network. Once stuck
in a wrong option, the ease of escaping from it can depend on the slight probability of the truly correct option being selected. Similar trends have been reported in the previous study on addressing the single-player
multi-armed bandit problem using a unidirectional ring-connected laser network [12]. While the previous
work has shown that the gap in CDR curves for rearranged orders of hit probabilities of slot expands with
more Plays, our proposed DCA is able to gradually diminish this difference, resulting in the convergence of
CDR to nearly the same value independent of the ordering of reward probabilities, as shown in Fig. 4 (b).
Table 4 displays CDR averaged over 1000 Plays, absolute regret, and relative regret in the numerical
simulations for various reward distributions. For (PA , PB , PC ) = (0.3, 0.7, 0.7), while CDR remains consis9

(a)

(b)

Figure 4: Correct decision ratio (CDR) for 2000 trials. Various reward distributions are applied. The
horizontal dotted line represents CDR = 0.95. (a) Five different reward distributions with symmetric hit
probabilities for the top two slots. (b) Five different reward distributions, including ones with asymmetry.
tently lower than for (0.1, 0.9, 0.9) and (0.2, 0.8, 0.8), absolute and relative regret are the lowest among the
three configurations. This reflects the fact that the reduced gap between good and bad slots can soften the
loss from incorrect decisions. Nevertheless, relative regret is increased for (PA , PB , PC ) = (0.4, 0.6, 0.6) and
(0.45, 0.55, 0.55) due to slow learning, as evidenced by the gradual increase in CDR. The lower five settings
exhibit noticeably higher average CDR than (PA , PB , PC ) = (0.45, 0.55, 0.55). However, relative regret is
worse because the difference in the expected rewards between the best and worst options is significant for
these settings with asymmetric hit probabilities, especially for (PA , PB , PC ) = (0.1, 0.5, 0.3) and (0.1, 0.9, 0.3),
where the worst option is more frequently selected.
Overall, the numerical simulations have demonstrated that learning of our proposed DCA method robustly
succeeds across various configurations. Naturally, the algorithm performance possibly varies depending on
the hyperparameters of DCA, whose dependence will be discussed in the following section.

3.3

Effects of hyperparameters

In this section, we examine how the hyperparameters of our proposed decentralized coupling adjustment
(DCA), described by Eqs. (6) and (7), influence the decision-making system’s performance. This analysis
reveals requirements for an effective system to address the competitive multi-armed bandit (CMAB) problem,
enabling us to enhance algorithm design. Hyperparameters of DCA are as follows: scaling factor rstep , the
lower and upper bound of coupling strength κlow and κupp . Also, there is flexibility in the definition of the
initial attenuation rate rini . Here, we highlight only rstep and κlow .
First, we focus on scaling factor rstep . From Eq. (7), rstep corresponds to the proportion by which the
attenuation rates r1♯ and r2♯ should be shifted from their baseline value rini in relation to Q1,S(♯) and Q2,S(♯)
10

Table 4: Mean of correct decision ratio (CDR) over 1000 Plays, absolute (abs.) regret, and relative (rel.)
regret in numerical simulations for various reward distributions.
(PA , PB , PC )

Mean of CDR

Abs. regret

Rel. regret

(0.1, 0.9, 0.9)

0.956

35.1

0.020

(0.2, 0.8, 0.8)

0.953

28.3

0.018

(0.3, 0.7, 0.7)

0.942

22.8

0.016

(0.4, 0.6, 0.6)

0.887

22.4

0.019

(0.45, 0.55, 0.55)

0.752

24.6

0.022

(0.1, 0.3, 0.9)

0.919

27.5

0.023

(0.1, 0.3, 0.5)

0.921

20.3

0.025

(0.1, 0.3, 0.3)

0.908

18.2

0.030

(0.1, 0.5, 0.3)

0.903

25.8

0.032

(0.1, 0.9, 0.3)

0.892

38.2

0.032

Figure 5: Correct decision ratio (CDR) for 2000 trials. Seven different values of scaling factor rstep are applied
for a setting (PA , PB , PC ) = (0.4, 0.6, 0.6). The horizontal dotted line represents CDR = 0.95.
(♯ = bl, or, ye). After sufficient exploration, Q1,S(♯) and Q2,S(♯) settle to certain values prescribed by the
problem setting (PA , PB , PC ). The resultant r1♯ and r2♯ also converge to specific values or reach the lower
or upper bound, rlow or rupp . The extent of exploitation is stronger when there is a significant difference
between coupling strengths, and thus, scaling factor rstep basically determines the strength of exploitation
after the exploration phase; exploitation can become insufficient when rstep is not large enough.
Figure 5 depicts the evolution of the correct decision ratio (CDR) as the number of Play progresses when
applying the control condition rstep = 1.0 or less than that. The hit probabilities of slot machines are set to
(PA , PB , PC ) = (0.4, 0.6, 0.6). Although CDR curves exhibit similar initial rise rates, the values at which they
converge vary depending on the value of rstep . The DCA method yields an overly exploratory strategy with
a low value of rstep , and CDR does not get sufficiently close to 1.0 despite the sufficient number of Plays, as
shown in Fig. 5.
Furthermore, the required minimum value of rstep also depends on the hit probabilities of slots since they
affect the behavior of Q1,S(♯) and Q2,S(♯) . If the difference between the good and bad slots is significant, i.e.,
the problem is not challenging, Q1,S(♯) and Q2,S(♯) converge to the larger absolute values, and exploitation
naturally becomes stronger even if rstep is the same.
Figure 6 illustrates CDR averaged for the 991–1000th Plays for various rstep and (PA , PB , PC ) configurations. As predicted, a decrease in rstep leads to lower CDR values across all problem scenarios due
11

Figure 6: The average of 991–1000th Play’s correct decision ratio (CDR) for 2000 trials against scaling factor
rstep . Five different reward distributions are applied.
to insufficient exploitation. Moreover, CDR values drop earlier with more challenging problems, such as
(PA , PB , PC ) = (0.4, 0.6, 0.6) or (0.45, 0.55, 0.55).
Next, we discuss the lower
p bound of coupling strengths κlow . κlow , which determines the lower limit
p of the
attenuation rates rlow ≡ κlow /κ, accompanied by κupp , which specifies the upper limit rupp ≡ κupp /κ,
works as a parameter that determines the balance between exploration and exploitation; if the difference
between κlow and κupp is large, the probability of selecting the estimated-optimal option increases, potentially
resulting in more effective exploitation. However, an excessively exploitative strategy can lead to incorrect
estimations and getting stuck to ineffective options, especially in challenging problem settings. Ideally, κlow
and κupp should be explored in two dimensions, but our preliminary examination has revealed that the
optimal value of κlow shifts according to κupp , and thus, we focus solely on κlow for simplicity.
Figure 7 illustrates the changes in CDR with the accumulation of Plays for six different settings of the
lower bound of coupling strength κlow , including the control condition, κlow = 38 ns−1 . Generally, the
lower value of κlow , that is, the larger the difference between κlow and κupp , the faster the initial increase
in CDR. However, the subsequent behavior depends on the specific problem configuration. For the setting
of (PA , PB , PC ) = (0.2, 0.8, 0.8) shown in Fig. 7 (a), CDR converges to the highest value among the six
configurations with κlow = 34 ns−1 . In the κlow = 40 ns−1 setting, the CDR curve quickly saturates, and
the eventual value is the lowest, resulting from an excessively exploratory strategy. On the other hand, for
the challenging configuration (PA , PB , PC ) = (0.4, 0.6, 0.6), shown in Fig. 7 (b), κlow = 38 ns−1 exhibits the
highest CDR after 1000 Plays. With the configurations of κlow = 30 ns−1 , 32 ns−1 , and 34 ns−1 , players
overexploit early selections, leading to getting stuck on incorrect selections and significantly lower CDR
values. Even so, CDR curves gradually keep recovering since the probability of selecting estimated suboptimal
options is not zero. Thus, there is a possibility that the setting of κlow = 30 ns−1 can outperform that of
κlow = 38 ns−1 in terms of CDR after further Plays.
Figure 8 shows the average of CDR at 991–1000th Play for various κlow and (PA , PB , PC ) configurations.
An exploitative strategy with κlow = 30 ns−1 is effective for (PA , PB , PC ) = (0.1, 0.9, 0.9), whereas a deliberate
approach with κlow = 38 ns−1 yields better results for (PA , PB , PC ) = (0.4, 0.6, 0.6) and (0.45, 0.55, 0.55).
Therefore, the appropriate κlow and κupp greatly depend on the number of Plays and the acceptable lower
limit of CDR or upper limit of regret. Additionally, there is potential for players to pick a better configuration
depending on whether they can roughly estimate the hit probabilities of slot machines beforehand.

12

(a)

(b)

Figure 7: Correct decision ratio (CDR) for 2000 trials. Six different values of the lower bound of coupling
strength κlow are applied. κlow determines the lower limit of the attenuation rates rlow . The horizontal
dotted line represents CDR = 0.95. (a) The problem setting of (PA , PB , PC ) = (0.2, 0.8, 0.8). (b) The
problem setting of (PA , PB , PC ) = (0.4, 0.6, 0.6).

Figure 8: The average of 991–1000th Play’s correct decision ratio (CDR) for 2000 trials against scaling factor
κlow . Five different reward distributions are applied.

13

4

Conclusion

We focused on collective decision-making utilizing a cluster-synchronized laser network [16, 17] for addressing the competitive multi-armed bandit (CMAB) problem, one of the most fundamental configurations of
multi-agent reinforcement learning (MARL). We proposed a six-laser network and decentralized coupling
adjustment (DCA), extending the system in the earlier research and enabling it to resolve the two-player
and three-slot CMAB problem with a photonic-based and straightforward learning algorithm. Our numerical
simulations demonstrated that the laser network exhibits three-cluster synchronization as intended and that
the learning algorithm effectively adapts the optical attenuation rate based on observed slot probabilities in
a distributed manner. The average performance evaluation verified that learning steadily progressed regardless of various reward distributions and examined the effects of hyperparameter configurations of the DCA
method.
We treated the CMAB problem with two players and three slots as the minimal setting under the conditions requiring exploitation. Our preliminary analysis has implied that the problem with increased players
and slots can be managed by further enlarging the laser network and increasing the number of clusters. However, the straightforward expansion leads to a prohibitive increase in the required number of lasers, indicating
the need for new principles for selecting slots other than the one based on the leader-laggard relationship.
We dealt with a time-invariant reward environment for simplicity. Time-varying environments can also
be addressed by modifying the DCA method to estimate the hit probabilities of slot machines solely from
the recent slot selection results or by introducing a memory parameter.

Funding
This research was funded in part by the Japan Society for the Promotion of Science through Grant-inAid for Research Activity Start-up (22K21269), Grant-in-Aid for Early-Career Scientists (23K16961), and
Transformative Research Areas (A) (JP22H05197).

Disclosures
The authors declare no conflicts of interest.

Data Availability
Data underlying the results presented in this paper are not publicly available at this time but may be obtained
from the authors upon reasonable request.

References
[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (MIT Press, 1998).
[2] M. Tan, Multi-agent reinforcement learning: Independent vs. cooperative agents, in Proceedings of the
tenth international conference on machine learning (1993) pp. 330–337.
[3] P. Stone and M. Veloso, Multiagent systems: A survey from a machine learning perspective, Autonomous
Robots 8, 345 (2000).
[4] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, M. Re, and S. Spanò, Multi-agent
reinforcement learning: A review of challenges and applications, Applied Sciences 11, 4948 (2021).
[5] K. Zia, N. Javed, M. N. Sial, S. Ahmed, A. A. Pirzada, and F. Pervez, A distributed multi-agent rl-based
autonomous spectrum allocation scheme in d2d enabled multi-tier hetnets, IEEE Access 7, 6733 (2019).
[6] S. Shalev-Shwartz, S. Shammah, and A. Shashua, Safe, multi-agent, reinforcement learning for autonomous driving, arXiv preprint arXiv:1610.03295 (2016).
14

[7] J. Wang, W. Xu, Y. Gu, W. Song, and T. C. Green, Multi-agent reinforcement learning for active voltage
control on power distribution networks, Advances in Neural Information Processing Systems 34, 3271
(2021).
[8] L. Lai, H. Jiang, and H. V. Poor, Medium access in cognitive radio networks: A competitive multi-armed
bandit framework, in 2008 42nd Asilomar Conference on Signals, Systems and Computers (IEEE, 2008)
pp. 98–102.
[9] H. Robbins, Some aspects of the sequential design of experiments, Bulletin of the American Mathematical
Society 58, 527 (1952).
[10] M. Naruse, W. Nomura, M. Aono, M. Ohtsu, Y. Sonnefraud, A. Drezet, S. Huant, and S.-J. Kim,
Decision making based on optical excitation transfer via near-field interactions between quantum dots,
Journal of Applied Physics 116, 1 (2014).
[11] M. Naruse, Y. Terashima, A. Uchida, and S.-J. Kim, Ultrafast photonic reinforcement learning based
on laser chaos, Scientific Reports 7, 1 (2017).
[12] T. Mihana, K. Fujii, K. Kanno, M. Naruse, and A. Uchida, Laser network decision making by lag
synchronization of chaos in a ring configuration, Optics Express 28, 40112 (2020).
[13] K. Morijiri, K. Takehana, T. Mihana, K. Kanno, M. Naruse, and A. Uchida, Parallel photonic accelerator
for decision making using optical spatiotemporal chaos, Optica 10, 339 (2023).
[14] N. Chauvet, D. Jegouso, B. Boulanger, H. Saigo, K. Okamura, H. Hori, A. Drezet, S. Huant, G. Bachelier,
and M. Naruse, Entangled-photon decision maker, Scientific Reports 9, 1 (2019).
[15] T. Mihana, K. Kanno, M. Naruse, and A. Uchida, Photonic decision making for solving competitive
multi-armed bandit problem using semiconductor laser networks, Nonlinear Theory and Its Applications,
IEICE 13, 582 (2022).
[16] H. Ito, T. Mihana, R. Horisaki, and M. Naruse, Conflict-free joint decision by lag and zero-lag synchronization in laser network, Scientific Reports 14, 4355 (2024).
[17] S. Kotoku, T. Mihana, R. André, R. Horisaki, and M. Naruse, Asymmetric leader-laggard cluster synchronization for collective decision-making with laser network, Optics Express 32, 14300 (2024).
[18] K. Kitayama, M. Notomi, M. Naruse, K. Inoue, S. Kawakami, and A. Uchida, Novel frontier of photonics
for data processing—photonic accelerator, APL Photonics 4, 090901 (2019).
[19] T. Heil, I. Fischer, W. Elsässer, J. Mulet, and C. R. Mirasso, Chaos synchronization and spontaneous
symmetry-breaking in symmetrically delay-coupled semiconductor lasers, Physical Review Letters 86,
795 (2001).
[20] K. Kanno, T. Hida, A. Uchida, and M. Bunsen, Spontaneous exchange of leader-laggard relationship in
mutually coupled synchronized semiconductor lasers, Physical Review E 95, 052212 (2017).
[21] I. Fischer, R. Vicente, J. M. Buldú, M. Peil, C. R. Mirasso, M. Torrent, and J. García-Ojalvo, Zero-lag
long-range synchronization via dynamical relaying, Physical review letters 97, 123902 (2006).
[22] M. Nixon, M. Friedman, E. Ronen, A. A. Friesem, N. Davidson, and I. Kanter, Synchronized cluster
formation in coupled laser networks, Physical review letters 106, 223901 (2011).
[23] T. Dahms, J. Lehnert, and E. Schöll, Cluster and group synchronization in delay-coupled networks,
Physical Review E 86, 016202 (2012).
[24] L. M. Pecora, F. Sorrentino, A. M. Hagerstrom, T. E. Murphy, and R. Roy, Cluster synchronization and
isolated desynchronization in complex networks with symmetries, Nature Communications 5, 1 (2014).
[25] P. Auer, N. Cesa-Bianchi, and P. Fischer, Finite-time analysis of the multiarmed bandit problem, Machine Learning 47, 235 (2002).
15

[26] R. Lang and K. Kobayashi, External optical feedback effects on semiconductor injection laser properties,
IEEE Journal of Quantum Electronics 16, 347 (1980).
[27] S.-J. Kim, M. Aono, and E. Nameda, Efficient decision-making by volume-conserving physical object,
New Journal of Physics 17, 083023 (2015).
[28] K. Liu and Q. Zhao, Distributed learning in multi-armed bandit with multiple players, IEEE transactions
on signal processing 58, 5667 (2010).
[29] D. Kalathil, N. Nayyar, and R. Jain, Decentralized learning for multiplayer multiarmed bandits, IEEE
Transactions on Information Theory 60, 2331 (2014).
[30] P. Landgren, V. Srivastava, and N. E. Leonard, Distributed cooperative decision making in multi-agent
multi-armed bandits, Automatica 125, 109445 (2021).
[31] S.-J. Kim, M. Naruse, and M. Aono, Harnessing the computational power of fluids for optimization of
collective decision making, Philosophies 1, 245 (2016).

16

