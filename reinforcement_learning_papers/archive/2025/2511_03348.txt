arXiv:2511.03348v2 [cs.MA] 6 Nov 2025

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK
M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Changxi Zhu
Department of Information and Computing Sciences
Utrecht University
Utrecht, Netherlands
c.zhu@uu.nl

Mehdi Dastani
Department of Information and Computing Sciences
Utrecht University
Utrecht, Netherlands
m.m.dastani@uu.nl

Shihan Wang
Department of Information and Computing Sciences
Utrecht University
Utrecht, Netherlands
s.wang2@uu.nl

A BSTRACT
In multi-agent deep reinforcement learning (MADRL), agents can communicate with one another to
perform a task in a coordinated manner. When multiple tasks are involved, agents can also leverage
knowledge from one task to improve learning in other tasks. In this paper, we propose Multi-task
Communication Skills (MCS), a MADRL with communication method that learns and performs
multiple tasks simultaneously, with agents interacting through learnable communication protocols.
MCS employs a Transformer encoder to encode task-specific observations into a shared message
space, capturing shared communication skills among agents. To enhance coordination among agents,
we introduce a prediction network that correlates messages with the actions of sender agents in
each task. We adapt three multi-agent benchmark environments to multi-task settings, where the
number of agents as well as the observation and action spaces vary across tasks. Experimental
results demonstrate that MCS achieves better performance than multi-task MADRL baselines without
communication, as well as single-task MADRL baselines with and without communication.
Keywords Multi-task Learning, Multi-agent Deep Reinforcement Learning, Communication

1

Introduction

Communication is essential in multi-agent deep reinforcement learning (MADRL) for enabling information sharing
and enhancing collaboration among multiple agents toward common goals [1], especially in partially observable
environments such as autonomous driving [2], sensor networks [3], and multi-robot control [4]. Recent works on
learning communication in MADRL have investigated what information to communicate [5, 6], when and with whom to
communicate [7, 8, 9] and how to integrate communication into policies [10, 11], thereby enabling flexible and dynamic
communication. However, these approaches are limited to single-task settings, where both policies and communication
protocols are designed and evaluated for only one task.
A different line of work in the MADRL literature focuses on multi-task learning, which aims to generalize policies
across multiple tasks [12]. This is achieved either via task-invariant architectures [13, 14] that exploit the shared
structure across tasks to learn a single unified model, or via task representation learning [15, 16] that explicitly models
environment dynamics to generalize to unseen tasks. However, despite these achievements, previous works on multi-task
learning in MADRL has not considered communication among agents within tasks. In contrast, humans can leverage
knowledge, in particular communication knowledge, to learn multiple tasks simultaneously.

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

In this work, we extend the scope of MADRL with communication approaches from single-task to multi-task settings.
The integration of communication into multi-task MADRL is non-trivial as it involves complex processes regarding
what, when, and how to communicate. Moreover, tasks may differ in observation and action spaces, making message
generation challenging. To address these challenges, we propose encoding task-specific observations into a shared
message space, capturing common communication skills that can generalizes across tasks, therefore enabling effective
and efficient communication in Multi-task Multi-Agent Deep Reinforcement Learning (Multi-task MADRL).
In this paper, we propose Multi-task Communication Skills (MCS), a multi-task MADRL with communication method.
MCS leverages task-invariant architectures to learn shared communication protocols across tasks, thereby improving
coordination among agents and accelerate learning performance in multi-task settings. During communication in each
task, messages are encoded by Transformer encoders and exchanged over a learned communication graph, where
unnecessary messages are pruned. The received messages are then aggregated through a shared aggregation function
and used to attend to important features in policy and critic networks. To further enhance coordination, we introduce
a prediction network that maximizes mutual information between messages and actions to encourage informative
communication. To evaluate MCS, we conduct experiments on the well-known SMAC benchmark [17], a novel
multi-task AliceBob environment, and an adapted multi-task version of Google Research Football [18]. We evaluate
both the average performance across tasks and the per-task performance. Our results show that MCS significantly
outperforms multi-task MADRL baselines without communication, as well as single-task MADRL baselines both
with and without communication. We further conduct ablation studies to assess the effects of pruning unnecessary
messages and of the prediction network on learning performance. We also analyze the sensitivity of hyperparameters
and the learned representations of messages. As a result, agents in MCS can communicate and perform efficiently and
effectively across multiple tasks with varying numbers of agents and diverse observation and action spaces.

2

Related Work

Multi-task Learning in MADRL Early approaches in multi-task MADRL, such as DEC-HDRQN [19], distilled
single-task Q-networks into a unified Q-network through a distillation process. More recent methods focus either on
task-invariant architectures [20, 13, 14, 21] or on learning task representations [22, 15, 16]. multi-task MADRL methods
based on task-invariant architectures exploit shared task structure by unifying inputs across tasks using entity-based
representations of observations and actions [23]. Specifically, REFIL [20] partitions agents into subgroups to capture
shared local coordination patterns across tasks. UPDet [13] leverages a single unified Transformer architecture to handle
varying entities across tasks. DT2GS [14] decomposes each task into multiple sub-tasks via latent variables generated
by Transformer encoders. RIT [21] applies mask techniques to selectively disable network layers corresponding to
invalid entities. On the other hand, task representations can be learned from a set of tasks based on trajectories [22],
transition and reward functions [15], or task similarity measures [16].
The above research works focus on online multi-task MADRL. A related line of research [24, 25, 26] investigates
offline multi-task MADRL which utilizes offline data across multiple independent tasks to learn generalized policies
for unseen tasks. Among them, ODIS [24] identifies coordination skills for individual agents from states and joint
actions in the offline data. HiSSD [26] adopts a hierarchical policy that jointly learns common and task-specific skills
in an offline dataset. However, we consider online MADRL without assuming the availability of offline data or state
information. Moreover, existing literature on both online and offline multi-task MADRL does not model communication
and interactions among agents within tasks, which our work explicitly addresses.
Communication in MADRL Existing research on communication in MADRL primarily focuses on the single-task
setting. While our method is the first work on multi-task setting and utilizes a different communication method
compared to the existing literature, we get inspiration from single-task MADRL with communication from two main
perspectives: what to communicate and when to communicate. First, the content of messages (what to communicate)
is generally encoded information derived from either observations [27, 7, 28, 11, 9] or intended actions [5, 29, 6].
Specifically, TGCNet [9] leverages a Transformer encoder to encode local observations as messages. ATOC [5] learns
an intention model that encodes both local observations and action intentions. IS [29] encodes imagined trajectories
capturing agents’ future action plans. MAIC [6] uses a teammate model to predict teammates’ intentions and generate
agent-specific messages. These approaches use an intention model during both training and execution. In contrast, our
method learns messages that correlate with actions using a prediction network, which is used only in training.
The decision of whether to communicate or not can be determined based on confidence or distance measures [30, 31, 32],
a binary classifier [7, 33, 8, 34], or a communication graph [10, 35, 36, 37, 38, 9]. Specifically, agents can communicate
when their confidence is low, as in VBC [30], or choose not to communicate when the previous or estimated messages
are similar to the current messages, as in TMC [31] and MBC [32]. Methods based on learnable binary classifiers,
including IC3Net [7], GACML [33], I2C [8], and T2MAC [34], assign communication labels using a threshold during
2

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

training. Most similar to our work, attention mechanisms are often employed to construct communication graphs,
enabling dynamic and flexible communication. Specifically, TarMAC [10] formulates communication as an attentionbased message aggregation process. G2ANet [35] employs a two-layer attention mechanism for selective message
aggregation. DGN [36] uses multi-head attention as a convolution kernel to aggregate information among neighboring
agents. MAGIC [37] introduces a hard attention to dynamically construct communication graphs, and CommFormer
[38] leverages attention to allocate credit to received messages within a bi-level optimization framework. More recently,
TGCNet [9] learns a dynamic and directed communication graph using a multi-key gated mechanism with multiple
hard attention modules. In contrast, our method also employs attention mechanisms but further integrates a threshold to
prune unnecessary messages.

3

Preliminaries

We extend the definition of multi-task MADRL [19] by incorporating entities and communication.
Definition 1. An Entity-Based Multi-Task Multi-Agent Reinforcement Learning with communication problem MT is
defined as:
MT := {Tk |k = 1, 2, ..., K} , Tk := ⟨I, E, S, A, O, M, Ω, P, R, γ⟩
where each task Tk is a Dec-POMDP tuple augmented with an additional message set M and entity set E. The
Dec-POMDP components are a set of agents I, a set of environment states S, a set of joint actions A, a set of joint
observations O, an observation function Ω, a transition function P , a reward function R, and a discount factor γ.
We assume that agents are a subset of entities, i.e., I ⊆ E, and states are represented as an entity-based matrix, i.e.,
e
S ⊆ R|E|×D , where De denotes the feature dimensionality, which remains the same for each entity in E. The
observation function Ω maps the entity set to the set of observable entities, Ω : E → Ê, which is used to construct
e
observations O ⊆ R|Ê|×D for Ê ⊆ E. To enable a shared policy across tasks, we unify the varying sets of agents,
entities, states, actions, observations, and messages by introducing the union sets of agents I, entities E, states S,
observations O, actions A, and messages M across tasks. Since each individual task can have its own state space, we
use S k ⊆ S to denote the state space of task k. We follow the same conventional notation for all components of the
Dec-POMDP of task k, where we have I k ⊆ I, E k ⊆ E, Ê k ⊆ E, S k ⊆ S, Ok ⊆ O, Ak ⊆ A, and M k ⊆ M.
In the partial observable environment of task k, agents may not observe the state sk ∈ S k of the environment. Each
agent i receives a local observation oki ∈ Ok and encodes it into a message mki ∼ f (oki ; θenc ) parametrized by θenc . A
joint policy parametrized by θπ is then defined as π(ak | ok , mk ; θπ ), where joint observations ok = ⟨oki , ..., okN ⟩ and
joint messages mk = ⟨mk1 , ..., mkN ⟩, with N = |I k | denoting the number of agents in task k. The goal of multi-task
MADRL with communication is to learn both the encoder f and the policy π that maximize the average expected return
across all K tasks:
" T
#
K
X
1 X
t k k
k
γ R (st , at )
Esk ∼P k , ak ∼π
LMT = max
θπ ,θenc K
t=0
k=1

where T is the length of episodes. Here, the policy π is conditioned on local observations and messages. We follow
the centralized training and decentralized execution (CTDE) paradigm to use a centralized value function as the
critic to guide the training of decentralized policies. In practice, the centralized value function is learned from joint
observations and messages to estimate expected return. For task k, we define a centralized observation-based value
function V (ok , mk ; ϕ) with parameters ϕ as:
" T
#
X
k
k
t k k
k
k
k
V (o , m ; ϕ) = Esk ∼P k , ak ∼π
γ R (st , at ) o , m , ϕ
t=0

4

Methods

In this section, we present Multi-task Communication Skills (MCS), a multi-task MADRL with communication method
that learns and performs multiple tasks simultaneously. An overview of MCS architecture is shown in Figure 1. Each
task consists of a set of agents that exchange messages through a learned communication graph. Based on received
messages and local observations, agents select and execute actions in the environment. During training, experiences
from all agents and tasks are collected jointly to enable centralized training. After training, agents deploy their learned
policies in each individual task. MCS can handle communication with varying numbers of agents, observation spaces,
and action spaces, thereby enabling generalization to multiple different environments. The entire framework, including
communication, policy, and value networks, is trained end-to-end.
3

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Figure 1: An overview of the MCS architecture. Agents communicate and act in each task, and data from multiple tasks
are used to train a shared model across tasks.

Figure 2: The network structure of MCS. Task-specific observations ok are represented in an entity-based form and
then encoded into messages mk . During communication, messages are pruned using masks C k , applied through
column-wise multiplication. Then, messages are aggregated and integrated into the policy network.

4.1

Entity-based Communication in multi-task MADRL

We first introduce how messages are generated, communicated, and integrated into policies in MCS. The network
structure used in MCS is schematically illustrated in Figure 2. We start with an entity-based representation across
multiple tasks. Due to the varying dimensionality of the observation and action spaces, the network’s inputs and
outputs shall be aligned when learning a shared communication protocol and policy across tasks. In entity-based
k
e
representations, the observation oki for each agent i in task k is represented as oki ∈ R|Ê |×D , where observable
entities Ê k may vary across tasks. However, the feature dimensionality De for each entity remains constant across
tasks (e.g., positions and velocity), which allows a shared input layer with dimensionality De to be used for message
generation. Next, given entity-based observations in task k, we employ a Transformer-based message encoder fenc ,
shared by N agents, to generate messages. Concretely, the message encoder fenc first embeds observations into an
input embedding, followed by n standard Transformer blocks with multi-head attention. The multi-head attention
k
h
outputs a hidden representation with shape R|Ê |×D for each agent i in task k, where Dh is the hidden dimensionality.
To handle the varying sizes of entities across tasks and obtain low-dimensional messages, we apply mean pooling to
aggregate features over entities, followed by a fully connected layer. This produces a message for agent i in task k,
m
denoted as mki = fenc (oki ; θenc ) ∈ RD , where Dm is the dimensionality of messages that remains constant across
tasks, and θenc represents the encoder parameters. As a result, the message encoder maps task- and agent-specific
m
observations into a common message space RD , enabling the generalization of communication across agents and
tasks. We denote the joint messages in task k as mk = ⟨mk1 , . . . , mkN ⟩ = ⟨fenc (ok1 ; θenc ), . . . , fenc (okN ; θenc )⟩. This can
be abbreviated as mk = fenc (ok ; θenc ).
4

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

We further prune unnecessary messages while leveraging an attention mechanism to measure the importance of
communication between agents. By introducing a communication threshold, we construct a communication mask
that prevents redundant messages and scales the remaining ones according to their importance. In particular, inspired
k
by Zhang et al. [9], we employ an additive attention mechanism to produce scores αi,j
∈ [0, 1], which quantify the
importance of agent i communicating with agent j in task k:

k
αi,j
= Gumbel-Softmax v ⊤ tanh Wq mki + Wk mkj ,
(1)
k
where v ⊤ , Wq , and Wk are learnable parameters shared across agents and tasks. The communication mask Ci,j
∈ [0, 1]
is defined based on the scores 1 :
 k
k
αi,j , if αi,j
> α̂
k
Ci,j =
, i ̸= j
(2)
0,
otherwise

where α̂ ∈ [0, 1] is a predefined threshold. We denote gated messages received by agent j from agent i for task k
k
as m̃ki,j = Ci,j
mki . All messages received by agent j in task k are therefore denoted as m̃kj = ⟨m̃k1,j , . . . , m̃kN,j ⟩ =
k
k
, . . . , CN,j
⟩ is the communication masks used by receiver agent j. In this way, the masks
C kj ⊙ mk , where C kj = ⟨C1,j
of redundant messages are set to 0 and therefore will not be considered in message integration.
Notably, the number of received messages can vary across tasks, which changes the input size of the policy network. To
address this, we design a task-invariant architecture capable of aggregating a variable number of received messages
while remaining efficient gradient backpropagation. We thus employ a GRU as an aggregation function fagg , which
can scale to a variable number of senders while capturing inter-agent dependencies. For each task k, the aggregation
function produces an aggregated message m̄kj upon received messages m̃kj (i.e., m̄kj = fagg (m̃kj )) as follows:

hkj,ℓ = GRU m̃kℓ,j , hkj,ℓ−1 ,
ℓ = 1, . . . , N
X
1
(3)
m̄kj =
hkj,ℓ ∈ RDh ,
N
ℓ

where hkj,ℓ is the hidden state of receiver agent j in task k when processing the ℓ-th received message.
The aggregated message m̄kj is then used to help receiver agents focus on important and relevant features in task-specific
observations when deciding an action. Specifically, we use m̄kj as the query embedding in a standard multi-head
k

e

attention module within the Transformer-based policy network. Let okj ∈ R|Ê |×D denote the observation matrix for
agent j in task k. For H̃ heads with dhead = Dh /H̃, each head h̃ ∈ {1, . . . , H̃} computes:
(h̃)

= (m̄kj )⊤ WQ ∈ R1×dhead ,

(h̃)

= okj WK ∈ R|Ê |×dhead

Vj

(h̃)

= okj WV

zjk =

 H̃

Qj
Kj

(h̃)

(h̃)

k

(h̃)

k

∈ R|Ê |×dhead ,

⊤
(h̃) (h̃)
Qj Kj
k
(h̃)
 ∈ R1×|Ê | ,
µj = softmax √
dhead
(h̃)

(h̃) 

µ Vj
h̃=1 j

Wz ∈ R1×D

(4)

h

where WQ , WK , WV , and Wz are learnable weight matrices shared across agents and tasks. Notably, the query
(h̃)
embedding Qj (h̃) and the key embedding Kj (h̃) are used to produce weighting scores µj over entities, which indicate
the relevance of the received message to each observable entity of receiver j. These weighting scores are then used to
combine the entity-based representations derived from observations. The outputs from all heads are concatenated to
form zjk , which enables receiver j to focus on the most relevant entity-based observations during communication. Then,
zjk is used in the action layer to decide the receiver’s actions. Note that the final action layer to generate task-specific
actions follows Hu et al. [13] and Tian et al. [14]. In MCS, messages are used not only in the policy network but
also as additional inputs to the critic networks. Then, each sender agent’s messages are updated through gradient
backpropagation from both the receiver agents’ policy networks and the critic networks, thereby guiding the generation
of messages that are most beneficial for communication. Within CTDE paradigm, we optimize the following objective
across all agents and tasks:
1

k
We use soft differentiable samples via the Gumbel-Softmax to generate scores αij
.

5

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Figure 3: The network structure diagram of the predictor.

L(θenc , θπ , ϕ) =
K

N

1 X 1 X
E k k [log π(aki |oki , m̄ki ; θπ )V (ok , m̄k ; ϕ)]
K
N i o ,ai

(5)

k



where m̄k = ⟨m̄k1 , . . . , m̄kN ⟩ denotes the joint aggregated messages of agents, and m̄ki = fagg C ki ⊙ fenc (ok ; θenc ) .
During centralized training, the encoder parameters θenc , policy parameters θπ , and critic parameters ϕ are shared
across agents and tasks.
4.2

Prediction Network for Coordination

With our proposed message encoder, communication is used to share encoded task-specific observations among agents,
thereby broadening each agent’s perspective of the overall environment. Despite this, sharing encoded observations does
not explicitly account for coordination, as agents may still need to align their action selections to achieve coordinated
behaviors. To address this, we further introduce a prediction network qpred that correlates the generated messages
mk with the sender agents’ actions ak , thereby enhancing coordinated action selection among agents. Concretely, we
employ an additional Transformer decoder as the prediction network (Figure 3), which is used only during training
and discarded at execution. Within multi-head attention, the prediction network takes received messages as query
embeddings. The key and value embeddings are derived from initialized actions denoted as ã (e.g., null actions). To
handle varying action dimensionalities across tasks, we apply zero-padding to actions. The predicted action distribution
is then used for maximizing the mutual information I(Ak ; M k ) between actions ak ∈ Ak and messages mk ∈ M k
for each task k. However, computing the mutual information I(Ak ; M k ) is intractable. Therefore, we maximize its
variational lower bound (known as Barber-Agakov lower bound) [39]. By replacing the conditional distribution of
actions given messages with a variational distribution qpred (ak | mk ), we have:


p(ak | mk )
I(A ; M ) = Ep(ak ,mk ) log
p(ak )


k
k
qpred (a | m ) p(ak | mk )
= Ep(ak ,mk ) log
p(ak ) qpred (ak | mk )


qpred (ak | mk )
= Ep(ak ,mk ) log
+
p(ak )
h
i
Ep(mk ) KL p(ak | mk ) ∥ qpred (ak | mk )


≥ Ep(ak ,mk ) log qpred (ak | mk ) + H(ak )
k

k

(6)

where H(ak ) ≡ − Ep(ak ) [log p(ak )] is the entropy of action distribution. The last inequality is due to the non-negativity
of the KL divergence. Since the entropy term is non-negative,
maximizing the first term is equal to maximizing the

mutual information. In practice, we estimate Ep(ak ,mk ) log qpred (ak | mk ) using sampled batch data. Given samples
6

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Algorithm 1 Multi-task Communication Skills
1: Input: Batch size B, number of tasks K, episodes L, steps T .
2: Initialize: Encoder parameters θenc , policy parameters θπ , and critic parameters ϕ. Replay buffer B k for each task

k.
3: for l = 0, 1, ..., L − 1 do
4:
for t = 0, 1, ..., T − 1 do
5:
Collect observations (o1t , ..., oK
t ) for K tasks
6:
Generate messages (m1t , ..., mK
t ) with message encoder
7:
for k = 1, ..., K do
8:
Communicate and aggregate messages into m̄kt based on Equations 1, 2, 3
9:
end for
1
K
1
K
10:
Decide actions (a1t , ..., aK
t ) based on (m̄t , ..., m̄t ) in Equation 4 and collect the rewards (rt , ..., rt )
k
k k
k
11:
Insert (ot , at , rt ) into buffer B
12:
end for
13:
for k = 1, ..., K do
14:
Sample a random minibatch of B steps from B k
15:
Generate messages mk = fenc (ok )
16:
Generate predicted action distribution qpred (ak |mk )
17:
Generate critics V (ok1 , m̄k1 ), ..., V (okN , m̄kN )
18:
end for
19:
Calculate loss −LMT based on Equation 8
20:
Update critics, actors, and messages with gradient descent
21: end for

{(akb , mkb , okb )}k∈{1,...,K},b∈{1,...,B} for K tasks and B batch-size, we maximize the following log-likelihood:
Lpred (θenc ) =

K B


1 1 XX
log qpred akb fenc okb ; θenc
KB

(7)

k=1 b=1

The loss Lpred backpropagates gradients from the predictor to the message encoder, encouraging it to generate messages
that reflect the sender agents’ behaviors and align agents’ action selections.
4.3

The Overall Learning Objective

The overall learning objective LMT of multi-task MADRL with communication is defined as:
LMT = L(θenc , θπ , ϕ) + β Lpred (θenc ),

(8)

where β is a coefficient that balances the influence of the prediction network. LMT ensures that messages are both
informative and beneficial for action selections, and that the policies and critics are learned based on communication. In
particular, the messages are optimized not only through gradients backpropagated from the receiver agents’ policy and
critic networks, but also through the auxiliary prediction network.
We further introduce Algorithm 1 to illustrate how MADRL agents communicate and update their messages, policies,
and critics in the multi-task setting. At each time step t of an episode, agents in task k observe okt = ⟨ok1,t , . . . , okN,t ⟩,
which are used to generate messages mkt (lines 5-6). Based on the communication graph defined in Equations 1 and 2
and the aggregation function in Equation 3, messages are communicated and aggregated into m̄kt at time step t (lines
7-9), which are then used to produce actions akt (line 10). The actions are executed in the environment, and rewards rtk
are collected (line 10). The resulting transitions of observations, actions, and rewards are stored in a task-specific replay
buffer (line 11). During training, mini-batches are sampled from these buffers (line 14). Lines 15–17 are then used to
compute the losses for the policy, predictor, and critic in Equation 8. Concretely, the sampled observations are first used
to generate messages, which are subsequently passed through the prediction network to produce an action distribution.
Then, gradients are backpropagated through the critics, predictors, actors, aggregation function, and message encoders
in an end-to-end manner (line 20).
7

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

(a) AliceBob-233

(b) AliceBob-344

(c) Marine

(d) Stalker–Zealot

(e) Football

Figure 4: Averaged win-rate across multiple tasks on AliceBob (a–b), SMAC (c–d), and Football (e).

5

Experiments

We evaluate our proposed method in three challenging multi-agent environments: AliceBob [40], SMAC [41], and
Google Research Football [18] 2 . These environments were originally designed for single-task MADRL and consist of
multiple cooperative tasks with challenges in coordinating agents’ behaviors. Following Tian et al. [14], we construct
multi-task settings for each environment. We then compare the following methods across these multiple tasks:
• Multi-task MADRL methods with communication. Our proposed method, MCS, is the first in this branch, which
is a Transformer-based approach that allows communication among agents for better coordination in multiple tasks.
In MCS, K tasks are learned and evaluated simultaneously.
• Multi-task MADRL methods without communication. DT2GS [14] and RIT [21] are two SOTA methods in this
branch. DT2GS is a policy-based method which uses Transformers as an encoding of entity-based observations and
decode actions for multiple tasks. RIT is a value-based method which uses padding techniques for unobservable
entities for different tasks. We compare MCS with DT2GS and RIT to evaluate the benefit of communication under
multi-task settings. Similar to MCS, K tasks are learned and evaluated simultaneously.
• Single-task MADRL methods with communication. TGCNet [9] is the most recent method that employs a multikey gated attention mechanism. In contrast to MCS, TGCNet does not consider the correlation between messages and
actions. MAIC [6] is another communication approach that promotes coordination by sharing messages correlated
with other agents’ intended actions. Notably, neither TGCNet nor MAIC considers the multi-task setting. Instead, K
tasks are learned and evaluated separately.
• Single-task MADRL methods without communication. MAT [42] is the SOTA method on many SMAC tasks,
which employs a Transformer to encode observations and generate action sequences for agents. HMASD [40]
represents the SOTA in leveraging latent skills for agent coordination. We compare MCS with MAT to evaluate the
impact of our proposed Transformer-based communication mechanism, and with HMASDA to assess the effectiveness
of the prediction network in promoting coordination. Also, K tasks are learned and evaluated separately.
The comparison examines two key aspects: (i) whether training is conducted in a multi-task or single-task setting, and
(ii) whether communication is enabled among agents. This results in two solid perspectives for comparison: Multi-task
with vs. without communication, and Multi-task vs. Single-task. By following the literature [14], we evaluate the
PK
1
averaged performance across K tasks, defined as Avg = K
k W inRate(k), where W inRate(k) denotes the
win rate on task k. The win rate W inRate(k) is estimated by running several evaluation episodes for each task at
fixed intervals during training, then averaging across episodes and tasks. We further show task-specific performance
W inRate(k) for each method, which demonstrates whether multi-task methods can outperform sing-task methods
in each particular task. The hyper-parameters of each methods are either from published papers or fine-tuned for fair
comparison. For MCS, we further fine-tune the threshold α̂ (Equation 2) and coefficient β (Equation 8), and we also
analyze the sensitivity of these parameters on the learning performance. We provide a list of critical hyperparameters
and the details of the entity-based representation for each environment in Appendix.
5.1

Evaluation Results

AliceBob Series The AliceBob environment, originally designed by Yang et al. [40], is created to explicitly
demonstrate how agents coordinate to collect diamonds in a grid world, where a diamond can only be collected if
another agent is simultaneously standing on the button of the same color. We extend this environment to a multi-task
setting where each task consists of either diamonds or food, and either buttons or keys. For example, task 0 may involve
3 pairs of entities {red diamond, red button}, {blue diamond, blue button}, {pink diamond, pink button} with 2
2

The source code will be released publicly on Github upon acceptance.

8

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

(a) AliceBob-233

(b) AliceBob-344

(c) Marine

(d) Stalker-Zealot

(e) Football

Figure 5: Ablation studies of MCS on AliceBob (a–b), SMAC (c–d), and Football (e).

agents {Alice, Bob}. We denote this configuration as 233-0, indicating 2 agents, 3 diamonds/foods, and 3 buttons/keys
with index 0. By varying entity types and associated colors (therefore different IDs), we obtain the AliceBob233 series: {233-0, 233–1, 233-2, 233–4}. To examine scalability, we further construct the AliceBob-344 series
{344–0, 344–1, 344–2, 344-4} by introducing an additional agent (Charlie) as well as an additional color and entity
(e.g., blue flower). To increase task complexity, diamonds and trees are randomly placed at the top of the environment,
while keys and buttons are randomly placed at the bottom at the beginning of each episode. Agents are also spawned at
random positions, requiring them to coordinate effectively to reach the correct and dynamically changing targets. Each
agent perceives only the relative distances between its own position and other entities, and its surrounding grids. The
reward function is defined as follows: agents receive a reward of 1 whenever a valid pair (diamond–button or food–key)
is completed, a reward of 5 when all pairs are completed (regarded as a win), a penalty of 0.5 for agent collisions, and a
penalty of 0.1 per step.
We show the averaged performance across tasks in AliceBob-233 and AliceBob-344 series in Figures 4a-4b, with
results averaged over 6 random seeds. MCS shows faster convergence than other methods in AliceBob-233 series and
significantly better performance than other methods in AliceBob-344 series. With communication, the Charlie agent in
AliceBob-344 series can be used for better reaching the goals in MCS. In contrast, RIT, which applies a unified domain
mask across tasks, achieves comparable performance in this environment, suggesting that it can effectively generalize
knowledge between tasks. DT2GS, however, does not consider inter-agent coordination and therefore fails in the tasks.
Other methods such as TGCNet and MAIC rely on training with single batch, which prevents them from efficiently
exploiting successful episodes (where agents must collect all diamonds or food for a win). We also report per-task
performance for both multi-task and single-task methods in the Appendix, where MCS consistently achieves much
higher win rate across all tasks in AliceBob 344 series.
SMAC Series. The StarCraft Multi-Agent Challenge (SMAC) is a real-time strategy game serving as a benchmark in
the MADRL community [41]. In SMAC, agents controlled by the learning algorithm must defeat all enemies, requiring
effective cooperation strategies and fine-grained micro-control of movement and attack. Following Tian et al. [14], we
construct two series of maps: the Marine series {3m, 5m_vs_6m, 8m_vs_9m, 10m_vs_11m} and the Stalker–Zealot
series {2s3z, 3s5z, 3s5z_vs_3s6z}. In these settings, the number of agents varies across tasks, testing the scalability
of multi-task methods. Moreover, the observation and action spaces differ across tasks, adding further complexity to
learn a unified model.
Figures 4c-4d reports the average performance across the Marine and Stalker–Zealot series, with results averaged over
6 random seeds. On the Marine series, MCS achieves similar performance as DT2GS, and both MCS and DT2GS
outperform other baselines, including the strong single-task method HMASD. On the Stalker–Zealot series, MCS
significantly outperforms all the other methods. Notably, HMASD suffers a severe performance drop around 1M time
steps, indicating convergence to a poor local optimum. RIT converges very quickly but with much lower win rates.
DT2GS improves only after about 3M steps, which is much slower than MCS in this series. We further report per-task
performance for both multi-task and single-task methods in the Appendix, where MCS consistently achieves the highest
win rates across all tasks in the Stalker–Zealot series.
Football Series. Google Research Football (GRF) is a physics-based 3D soccer simulator for reinforcement learning
[18], providing a challenging multi-agent benchmark environment with high stochasticity and sparse rewards. In
GRF, opponents are controlled by expert agents, which significantly increase the complexity of the state–action
space. To highlight the difficulty of the tasks, we train and evaluate using only the scoring reward: agents receive
a reward of +1 when scoring a goal, which is sparse. Episodes terminate when a goal is scored, the ball goes out
of bounds, or possession changes. To construct entity-based representations, features from the left and right teams
are grouped into four macro entities: left-team locations, left-team directions, right-team locations, and right-team
directions. Ball position and direction are appended to the appropriate entity depending on possession, and a one-hot
encoding of the active player is also included. The final entity representation remains comparable in size to the
9

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

(a) AliceBob-233

(b) AliceBob-344

(c) Marine

(d) Stalker-Zealot

(e) Football

Figure 6: The average win rate for different combinations of α̂ and β used by MCS in all environments.
original observations, which does not alter task difficulty. We construct a multi-task Football series using two maps:
{3_vs_1_with_keeper, pass_and_shoot_with_keeper}, which involve different numbers of RL agents attempting
to score from the edge of the field.
Figure 4e shows the averaged win rate across tasks in the Football series, with results further averaged over 4 random
seeds. MCS achieves significantly a higher averaged win rate than all baseline methods. RIT fails to perform effectively
in this high-dimensional, stochastic domain. The results of per-task performance are provided in the Appendix, where
MCS obtains the highest win rate in 3_vs_1_with_keeper and achieves similar performance as MAT and DT2GS in
pass_and_shoot_with_keeper.
5.2

Ablation Studies

We conduct ablation experiments on MCS to assess the contributions of the communication mask (Equation 2) and the
predictor (Equation 7), investigating whether pruning unnecessary messages and encouraging the correlation between
messages and actions improves learning performance. As can be seen from Figure 5, removing either communication
mask (MCS wo Mask) defined in Equation 2 or the prediction network (MCS wo Pre) defined in Section 4.2 can lead
to slower convergence or performance degradation. In all multi-task series, removing both communication mask and
prediction network (MCS wo (Pre & Mask)) can lead to significantly lower win rate compared to MCS.
5.3

Hyperparameters Analysis

We also conducted a sensitivity analysis of the threshold α̂ and coefficient parameter β. We report the mean of
the averaged win rate in the last 10 episodes across tasks under different values of α̂ and β in Figure 6. In all
tasks, moderate values of α̂ (e.g., 0.5 or 0.7) yield the best results, whereas larger values (e.g., 0.9) tend to degrade
performance. This indicates that limited communication may negatively affect the performance, while a moderate
amount of communication is sufficient to achieve good performance. The effect of β is more subtle and task-dependent.
In AliceBob and SMAC Stalker-Zealot series, both low values (e.g., β = 0.1) and high values (e.g., β = 1) can lead to
good performance. However, in SMAC Marine and Football series, β must be carefully tuned in combination with α̂.
We also observe that inappropriate combinations of α̂ and β, for example, α̂ = 0.9 and β = 0.5 in the AliceBob 233
series, can lead to a drastic drop in performance. This typically occurs when communication is severely limited (high α̂)
or when the message encoding is overly regularized (high β). Overall, the analysis suggests that a robust configuration
across most tasks is α̂=0.5 and β=0.1, suggesting moderate amount of communication and slowly regularize the
messages. In fact, in the finetuning stage, we adopt a strategy of adjusting α̂ around 0.5 to regulate communication,
while starting with a small value of β and gradually increasing it.
5.4

Analysis of Message Representations

We further investigate the messages learned by MCS, which correspond to the latent representations generated by our
proposed Transformer-based message encoder. To examine the impact of communication on learning, we compare the
latent representations produced by MCS with DT2GS, which also employs a Transformer to encode observations for
policy learning but does not incorporate communication. For visualization, we record the latent representations from the
last three episodes of MCS and DT2GS. We then apply UMAP [43] to project the high-dimensional latent representations
into two dimensional points, as shown in Figure 7, where each point corresponds to an agent. In AliceBob, where
different tasks differ only in the types of entities, both DT2GS and MCS learn similar latent representations across
tasks. However, MCS produces more compact message representations, indicating a shared communication pattern
among agents, which enhances their coordination. In SMAC, DT2GS tends to learn similar representations across
all tasks, even though task 2c3s should intuitively differ from the others due to the smaller number of agents and
enemies involved. In contrast, MCS can capture this difference and learns a distinct representation for 2c3s, likely
because fewer agents participate in communication. In Football, MCS also distinguishes between 3_vs_1_with_keeper
10

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

and pass_and_shoot_with_keeper, indicating that agents may adopt different shooting strategies as the number of
agents and their starting positions vary across the two tasks. Furthermore, in all tasks, DT2GS tends to spread the
representations in the latent space, treating agents independently without capturing their interactions. In contrast,
MCS forms clusters that reflect structured representations, indicating meaningful inter-agent dependencies, which is
consistent with its superior empirical performance.

6

Conclusion

We propose Multi-task Communication Skills (MCS), a multi-task MADRL with communication method that learns a
shared communication protocol across tasks with varying numbers of agents, observation spaces, and action spaces.
We introduce a prediction network that maximizes mutual information between messages and actions to promote
coordinated action selection. Empirical results show that MCS outperforms multi-task MADRL methods without
communication and single-task MADRL methods with or without communication across several benchmark multi-task
environments. Moreover, MCS exhibits meaningful patterns in the latent message representations, while the amount of
communication required may vary across different environments. In future work, we aim to develop more adaptive
strategies that dynamically prune unnecessary messages for each task. We will also investigate how the differences
across tasks influence the learned message representation.

References
[1] Changxi Zhu, Mehdi Dastani, and Shihan Wang. A survey of multi-agent deep reinforcement learning with
communication. Autonomous Agents and Multi-Agent Systems, 38(4), 2024.
[2] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for
autonomous driving. CoRR, abs/1610.03295, 2016.
[3] M. Pipattanasomporn, H. Feroze, and S. Rahman. Multi-agent systems in a distributed smart grid: Design and
implementation. In 2009 IEEE/PES Power Systems Conference and Exposition, pages 1–8, 2009.
[4] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. Int. J. Robotics
Res., 32(11):1238–1274, 2013.
[5] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. In Advances
in Neural Information Processing Systems 31 (NIPS), pages 7265–7275, 2018.
[6] Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang Zhang, Yang Yu, and Chongjie Zhang.
Multi-agent incentive communication via decentralized teammate modeling. In Thirty-Sixth AAAI Conference on
Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence,
IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event,
February 22 - March 1, 2022, pages 9466–9474. AAAI Press, 2022.
[7] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale in multiagent
cooperative and competitive tasks. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019, 2019.
[8] Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent
cooperation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin, editors, Advances in Neural Information Processing Systems 33 (NeurIPS), 2020.
[9] Zhuohui Zhang, Bin He, Bin Cheng, and Gang Li. Bridging training and execution via dynamic directed graphbased communication in cooperative multi-agent systems. In Toby Walsh, Julie Shah, and Zico Kolter, editors,
AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4,
2025, Philadelphia, PA, USA, pages 23395–23403. AAAI Press, 2025.
[10] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau.
Tarmac: Targeted multi-agent communication. In Proceedings of the 36th International Conference on Machine
Learning (ICML), pages 1538–1546, 2019.
[11] Cong Guan, Feng Chen, Lei Yuan, Chenghe Wang, Hao Yin, Zongzhang Zhang, and Yang Yu. Efficient multiagent communication via self-supervised information aggregation. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
28 - December 9, 2022, 2022.
11

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

[12] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Trans. Knowl. Data Eng., 34(12):5586–5609,
2022.
[13] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal multi-agent RL via policy decoupling
with transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021. OpenReview.net, 2021.
[14] Zikang Tian, Ruizhi Chen, Xing Hu, Ling Li, Rui Zhang, Fan Wu, Shaohui Peng, Jiaming Guo, Zidong Du,
Qi Guo, and Yunji Chen. Decompose a task into generalizable subtasks in multi-agent reinforcement learning. In
Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances
in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems
2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.
[15] Rongjun Qin, Feng Chen, Tonghan Wang, Lei Yuan, Xiaoran Wu, Yipeng Kang, Zongzhang Zhang, Chongjie
Zhang, and Yang Yu. Multi-agent policy transfer via task relationship modeling. Sci. China Inf. Sci., 67(8), 2024.
[16] Mridul Mahajan, Georgios Tzannetos, Goran Radanovic, and Adish Singla. Learning embeddings for sequential
tasks using population of agents. In Proceedings of the Thirty-Third International Joint Conference on Artificial
Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 4733–4741. ijcai.org, 2024.
[17] Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J.
Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors, Proceedings of the 18th
International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC, Canada,
May 13-17, 2019, pages 2186–2188, 2019.
[18] Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme,
Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football: A novel
reinforcement learning environment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, pages 4501–4510. AAAI Press, 2020.
[19] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. Deep decentralized
multi-task multi-agent reinforcement learning under partial observability. In Doina Precup and Yee Whye Teh,
editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2681–2690. PMLR,
2017.
[20] Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin Böhmer, Shimon Whiteson, and Fei Sha.
Randomized entity-wise factorization for multi-agent reinforcement learning. In International Conference on
Machine Learning, pages 4596–4606. PMLR, 2021.
[21] Chao Li, Shaokang Dong, Shangdong Yang, Yujing Hu, Tianyu Ding, Wenbin Li, and Yang Gao. Multi-task
multi-agent reinforcement learning with interaction and task representations. IEEE Trans. Neural Networks Learn.
Syst., 36(7):13431–13445, 2025.
[22] Lukas Schafer, Filippos Christianos, Amos Storkey, et al. Learning task embeddings for teamwork adaptation in
multi-agent reinforcement learning [c/ol]. In NeurIPS 2023 Workshop on Generalization in Planning, 2023.
[23] Christian Schröder de Witt, Jakob N. Foerster, Gregory Farquhar, Philip H. S. Torr, Wendelin Boehmer, and
Shimon Whiteson. Multi-agent common knowledge reinforcement learning. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9924–9935, 2019.
[24] Fuxiang Zhang, Chengxing Jia, Yi-Chen Li, Lei Yuan, Yang Yu, and Zongzhang Zhang. Discovering generalizable
multi-agent coordination skills from multi-task offline data. In The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[25] Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Variational offline multi-agent skill discovery. arXiv preprint
arXiv:2405.16386, 2024.
[26] Sicong Liu, Yang Shu, Chenjuan Guo, and Bin Yang. Learning generalizable skills from offline multi-task data
for multi-agent cooperation. In The Thirteenth International Conference on Learning Representations, ICLR 2025,
Singapore, April 24-28, 2025. OpenReview.net, 2025.
[27] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with backpropagation.
In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances
12

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pages 2244–2252, 2016.
[28] Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value
functions via communication minimization. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
[29] Woojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention
sharing. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021. OpenReview.net, 2021.
[30] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement learning via
variance based control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32 (NeurIPS),
pages 3230–3239, 2019.
[31] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Succinct and robust multi-agent communication with temporal message
control. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33 (NIPS), 2020.
[32] Shuai Han, Mehdi Dastani, and Shihan Wang. Model-based sparse communication in multi-agent reinforcement
learning. In Noa Agmon, Bo An, Alessandro Ricci, and William Yeoh, editors, Proceedings of the 2023
International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2023, London, United Kingdom,
29 May 2023 - 2 June 2023, pages 439–447. ACM, 2023.
[33] Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under
limited bandwidth by message pruning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages
5142–5149, 2020.
[34] Chuxiong Sun, Zehua Zang, Jiabao Li, Jiangmeng Li, Xiao Xu, Rui Wang, and Changwen Zheng. T2MAC:
targeted and trusted multi-agent communication through selective engagement and evidence-driven integration.
In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on
Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence,
IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27,
2024, Vancouver, Canada, pages 15154–15163. AAAI Press, 2024.
[35] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent game abstraction
via graph attention neural network. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), pages
7211–7218, 2020.
[36] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In
International Conference on Learning Representations, 2020.
[37] Yaru Niu, Rohan R. Paleja, and Matthew C. Gombolay. Multi-agent graph-attention communication and teaming.
In 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pages 964–973, 2021.
[38] Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Learning multi-agent communication from graph modeling
perspective. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
May 7-11, 2024. OpenReview.net, 2024.
[39] Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander A. Alemi, and George Tucker. On variational bounds
of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pages 5171–5180. PMLR, 2019.
[40] Mingyu Yang, Yaodong Yang, Zhenbo Lu, Wengang Zhou, and Houqiang Li. Hierarchical multi-agent skill
discovery. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine,
editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information
Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.
[41] Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J.
Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraft multi-agent
challenge. In Edith Elkind, Manuela Veloso, Noa Agmon, and Matthew E. Taylor, editors, Proceedings of the 18th
International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’19, Montreal, QC, Canada,
May 13-17, 2019, pages 2186–2188. International Foundation for Autonomous Agents and Multiagent Systems,
2019.
[42] Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang.
Multi-agent reinforcement learning is a sequence modeling problem. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
13

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
28 - December 9, 2022, 2022.
[43] Leland McInnes and John Healy. UMAP: uniform manifold approximation and projection for dimension reduction.
CoRR, abs/1802.03426, 2018.

14

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

(a) DT2GS in AliceBob

(b) MCS in AliceBob

(c) DT2GS in SMAC

(d) MCS in SMAC

(e) DT2GS in Football

(f) MCS in Football

Figure 7: Latent representations of DT2GS and MCS.

15

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

A

Details of Entity-based Representations

Figure 8: An illustration of the AliceBob-233 series. Agents, diamonds/food, and buttons/keys are randomly placed on
the map in each episode.

Figure 9: Football series with 3 vs 1 with keeper (Left) and Pass and shot with keeper (Right).
AliceBob series. The AliceBob environment was originally proposed for single-task MADRL without considering
entity-based representations. Following the approaches of Hu et al. [13] and Tian et al. [14], we construct an entitybased representation and adapt the AliceBob environment to multi-task settings. In the entity-based representation,
entities consist of RL agents, buttons/keys, and diamonds/food. For each entity, we extract partially observable features,
including the relative distance from each agent to all other entities and a one-hot representation of the observed entity
type. To reduce learning complexity, we further enrich each agent’s observation by incorporating features of the
surrounding grid cells using the same representation scheme. The resulting entity representation thus comprises: (i)
features of the agent itself, (ii) features of other agents, (iii) features of diamonds/food, (iv) features of keys/buttons, and
(v) features of the surrounding grids. For example, in the AliceBob-233 series, the observation shape becomes 16 × 17
where 16 is the total number of entities (8 original entities plus 8 surrounding grids), and 17 is the number of features
(2 dimensions for the relative distance and 15 dimensions for the one-hot encoding). We provide an example of the
multi-task setting in the AliceBob-233 series in Figure 8, where two agents must coordinate to collect diamonds/food
and press buttons/keys separately. The type of entities is determined before the start of the game, while their positions
are randomly generated at the beginning of each episode to create dynamic targets. Agents can leverage the knowledge
acquired from collecting diamonds or pressing buttons in one task to improve their learning performance in another
task.
SMAC series. We use the same multi-task environment as used by Hu et al. [13] and Tian et al. [14], where entities
consist of the agent itself, allied agents, and enemies. The feature set includes visibility, distance to the agent, relative x
and y positions, health, shield, and unit type of each entity. Then, for each entity, a total of 16 feature dimensions are
considered 3 .
Football series. We adapt the Google Research Football environment [18] to a multi-task setting with entity-based
representations. Starting from the raw features in simple115v2, we group them into four macro-entities: left-team
locations, left-team directions, right-team locations, and right-team directions. Ball position and direction are appended
to the appropriate entity depending on possession, and a one-hot encoding of the active player is included. This
results in 43 feature dimensions per entity, with the total number of features across all entities remaining similar
to the original raw representation, thereby preserving the overall complexity of the environment. We illustrate two
tasks from the Football series {3_vs_1_with_keeper, pass_and_shoot_with_keeper}. In 3_vs_1_with_keeper,
three left-team players start in the right half, competing against one right-team defender and the goalkeeper. In
pass_and_shoot_with_keeper, two left-team players start in the right half against one right-team defender and the
3

Implementation available at: https://github.com/Felixvillas/DT2GS

16

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Table 1: Default hyperparameters of MCS across different environments.
H YPERPARAMETER

A LICE B OB

SMAC

F OOTBALL

5 E -4
5 E -4
1 E -5
8
10
0.2
0.01
10
0.99
0.95
64
10
32
1E6
32

5 E -4
5 E -4
1 E -5
8
8
0.2
0.01
10
0.99
0.95
64
10
32
5E6
32

5 E -4
5 E -4
1 E -5
15
2
0.2
0.01
10
0.99
0.95
64
10
32
1E7
32

lr
critic_lr
opti_eps
PPO EPOCHS
MINI - BATCHES
clip_param
ENTROPY COEFFICIENT
MAX GRAD NORM

γ
GAE λ
HIDDEN SIZE
MESSAGE DIMENSION
BATCH SIZE
NUMBER OF STEPS
EVALUATION EPISODES

Table 2: Hyperparameters (α̂ threshold and β) used by MCS in different multi-task series.
Parameters
α̂
β

AliceBob 233

AliceBob 344

SMAC Marine

SMAC Stalker Zealots

Football

0.5
0.1

0.7
1.0

0.9
0.1

0.5
0.5

0.5
0.1

goalkeeper. An episode terminates when (a) the maximum duration (200 steps) is reached, (b) the ball goes out of
bounds, (c) a team scores, or (d) possession changes.

B

Critical Hyperarameters and Per-task Performance

The hyperparameters used for MCS in AliceBob, SMAC, and Google Research Football are summarized in Table 1. As
shown, the three environments share most hyperparameters, except for the number of PPO epochs and the mini-batch
size. For SMAC, we use the same hyperparameters as in DT2GS, which have already been fine-tuned. For AliceBob,
we adopt similar values but use a slightly larger mini-batch size to obtain more samples. For the Football environment,
we use the same hyperparameters as those employed in the single-task setting. Consequently, these hyperparameters
are also applied to the baseline methods (e.g., DT2GS and RIT) to ensure a fair comparison. We also report the
communication threshold (α̂) and coefficient parameter (β) used by MCS in Table 2, which correspond to the results
presented in Figures 4 and 5 of the manuscript.
The experiments were run in parallel on a cluster with 32 CPU cores and an NVIDIA A100 GPU. Table 3 reports the
wall-clock training time for each method on each multi-task series; values are averaged over the tasks within a series.
On AliceBob, the single-task methods HMASD and MAT are most compute-efficient, typically finishing within ≤ 1
hour per run. DT2GS is moderately heavier (6h), while MCS/MAIC/TGCNet sit around 4–5h. RIT is the slowest
even on small tasks (7–8h). On SMAC (Marine and Stalker Zealot) and Football, runtimes spread more widely. RIT
scales poorly (tens to > 100h), suggesting unfavorable difficulty in complex environments. MCS remains reasonable
on simpler tasks but is longer on Marines (23h). MAIC is fast on Marines (6h) yet slow on Football (29h). Overall, the
proposed MCS does not significantly increase runtime compared to other multi-task MADRL baselines on AliceBob
and Football, though it is slightly slower than DT2GS on Marines. Single-task baselines (HMASD/MAT) achieve much
lower per-series runtime as they train only one task at a time, which may come at the expense of multi-task learning
performance.
We further report the per-task performance of each method across all multi-task series, as shown in Figure 10 and
Table 4. As illustrated in Figures 10a–10b, MCS consistently achieves faster convergence and significantly higher win
rates across all tasks in the AliceBob environments. In SMAC Marines (Figure 10c), MCS outperforms DT2GS in
the 3m and 5m_vs_6m tasks, while DT2GS surpasses MCS in 8m_vs_9m and 10m_vs_11m, potentially due to the
benefits of task decomposition in the two tasks. Despite these differences, MCS and DT2GS achieve similar average
performance across tasks (as shown in the table). For single-task baselines, methods such as HMASD achieve high
win rates in 5m_vs_6m and 10m_vs_11m. However, their performance varies significantly across tasks, resulting in
17

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Table 3: Average wall-clock runtime (hours) per method across tasks under each multi-task series.
Method AliceBob 233 series AliceBob 344 series SMAC Marines SMAC Stalker Zealot Football
MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

4
6
7
1
1
4
4

4
6
8
1
1
4
4

23
7
67
6
6
6
13

10
9
81
6
8
9
12

13
10
116
8
8
29
25

lower overall average performance. In the Stalker Zealots series, MCS consistently outperforms all baselines in terms
of learning performance. In Football, MCS achieves a significantly higher win rate in the 3_vs_1_with_keeper task.
In the pass_and_shoot_with_keeper task, the single-task baseline MAT performs similar to MCS. However, MAT’s
performance drops significantly in 3_vs_1_with_keeper, where MCS maintains a consistent high win rate.

18

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

(a) AliceBob-233

(b) AliceBob-344

(c) Marine

(d) Stalker Zealots

(e) Football

Figure 10: Per-task win rate curves across different environments: AliceBob-233/344, SMAC Marine/Stalker Zealots,
and Football.

19

L EARNING C OMMUNICATION S KILLS IN M ULTI - TASK M ULTI - AGENT D EEP R EINFORCEMENT L EARNING

Table 4: Per-task win rates (mean(std)) for all benchmark multi-task series. Columns list individual tasks; the rightmost
column is the average across tasks.
AliceBob 233 series
Method

233-0

233-1

233-2

233-3

Avg

MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

0.98 (0.02)
0.05 (0.09)
1.00 (0.00)
0.64 (0.50)
0.70 (0.35)
0.00 (0.00)
0.00 (0.00)

0.98 (0.02)
0.04 (0.08)
1.00 (0.01)
0.48 (0.53)
0.64 (0.30)
0.00 (0.00)
0.00 (0.00)

0.99 (0.01)
0.04 (0.06)
1.00 (0.00)
0.16 (0.39)
0.32 (0.40)
0.00 (0.00)
0.00 (0.00)

0.98 (0.01)
0.02 (0.06)
0.99 (0.01)
0.32 (0.50)
0.82 (0.13)
0.00 (0.00)
0.00 (0.00)

0.98 (0.00)
0.04 (0.01)
1.00 (0.00)
0.40 (0.21)
0.62 (0.21)
0.00 (0.00)
0.00 (0.00)

AliceBob 344 series
Method

344-0

344-1

344-2

344-3

Avg

MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

0.98 (0.02)
0.49 (0.35)
0.83 (0.41)
0.48 (0.52)
0.27 (0.24)
0.00 (0.00)
0.00 (0.00)

0.99 (0.01)
0.56 (0.33)
0.83 (0.41)
0.49 (0.53)
0.15 (0.11)
0.00 (0.00)
0.00 (0.00)

0.99 (0.01)
0.55 (0.32)
0.83 (0.41)
0.64 (0.50)
0.10 (0.06)
0.00 (0.00)
0.01 (0.01)

0.98 (0.01)
0.47 (0.28)
0.83 (0.41)
0.64 (0.49)
0.16 (0.05)
0.00 (0.00)
0.00 (0.01)

0.99 (0.00)
0.52 (0.04)
0.83 (0.00)
0.56 (0.09)
0.17 (0.07)
0.00 (0.00)
0.00 (0.00)

SMAC Marine series
Method

3m

5m_vs_6m

8m_vs_9m

10m_vs_11m

Avg

MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

0.99 (0.00)
0.99 (0.02)
0.74 (0.07)
1.00 (0.00)
0.98 (0.02)
0.24 (0.23)
0.13 (0.26)

0.43 (0.20)
0.28 (0.03)
0.00 (0.00)
0.60 (0.24)
0.26 (0.06)
0.00 (0.00)
0.00 (0.00)

0.67 (0.11)
0.73 (0.12)
0.06 (0.04)
0.00 (0.00)
0.09 (0.07)
0.00 (0.00)
0.00 (0.00)

0.66 (0.08)
0.77 (0.16)
0.08 (0.05)
0.55 (0.31)
0.23 (0.14)
0.00 (0.00)
0.00 (0.00)

0.69 (0.23)
0.69 (0.29)
0.22 (0.35)
0.54 (0.41)
0.39 (0.40)
0.06 (0.12)
0.03 (0.07)

SMAC Stalker Zealots series
Method

2s3z

3s5z

3s5z_vs_3s6z

Avg

MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

0.97 (0.02)
0.53 (0.33)
0.65 (0.19)
0.07 (0.16)
0.83 (0.11)
0.03 (0.06)
0.11 (0.07)

0.93 (0.15)
0.63 (0.36)
0.63 (0.16)
0.00 (0.00)
0.21 (0.18)
0.00 (0.00)
0.00 (0.00)

0.86 (0.12)
0.48 (0.31)
0.00 (0.00)
0.00 (0.00)
0.02 (0.01)
0.00 (0.00)
0.00 (0.00)

0.92 (0.06)
0.55 (0.07)
0.43 (0.37)
0.02 (0.04)
0.36 (0.43)
0.01 (0.02)
0.04 (0.06)

Football series
Method

3_vs_1_with_keeper

pass_and_shoot_with_keeper

Avg

MCS
DT2GS
RIT
HMASD
MAT
MAIC
TGCNet

0.79 (0.12)
0.58 (0.15)
0.06 (0.03)
0.48 (0.12)
0.10 (0.19)
0.00 (0.00)
0.00 (0.00)

0.78 (0.07)
0.74 (0.08)
0.04 (0.04)
0.17 (0.26)
0.81 (0.05)
0.00 (0.00)
0.03 (0.05)

0.79 (0.00)
0.66 (0.12)
0.05 (0.02)
0.33 (0.22)
0.45 (0.50)
0.00 (0.00)
0.01 (0.02)

20

