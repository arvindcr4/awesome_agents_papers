Mastering Diverse Domains through World Models
Danijar Hafner,12 Jurgis Pasukonis,1 Jimmy Ba,2 Timothy Lillicrap1

Developing a general algorithm that learns to solve tasks across a wide range of
applications has been a fundamental challenge in artificial intelligence. Although
current reinforcement learning algorithms can be readily applied to tasks similar to
what they have been developed for, configuring them for new application domains
requires significant human expertise and experimentation. We present DreamerV3, a
general algorithm that outperforms specialized methods across over 150 diverse tasks,
with a single configuration. Dreamer learns a model of the environment and improves its
behavior by imagining future scenarios. Robustness techniques based on normalization,
balancing, and transformations enable stable learning across domains. Applied out of the
box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without
human data or curricula. This achievement has been posed as a significant challenge in
artificial intelligence that requires exploring farsighted strategies from pixels and sparse
rewards in an open world. Our work allows solving challenging control problems without
extensive experimentation, making reinforcement learning broadly applicable.

DMLab

Minecraft

1 task, 100M steps
9
6
3
0

Atari100k

Proprio Control

Visual Control

BSuite

Tuned experts

70
50
30
10

23 tasks

PPO
DQN
Boot DQN
Dreamer

PPO
CURL
DrQ-v2
Dreamer

PPO
D4PG
DMPO
Dreamer

26 tasks, 400K steps 18 tasks, 500K steps 20 tasks, 1M steps
130
900
900
90
600
600
50
300
300
10
0
0

PPO
Rainbow
IMPALA
Dreamer

30 tasks, 100M steps
70
50
30
10

PPO
R2D2+
10x data
IMPALA
10x data
Dreamer

16 tasks, 50M steps
70
50
30
10

b

12

Return

ProcGen

57 tasks, 200M steps
900
600
300
0

PPO
Rainbow
PPG
Dreamer

Atari

PPO
Rainbow
MuZero
Dreamer

a

PPO
TWM
IRIS
Dreamer

arXiv:2301.04104v2 [cs.AI] 17 Apr 2024

Abstract

Minecraft Diamond

8
4
0

Max
Mean
100K

1M

10M 100M

Env steps

Unified configuration

Figure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer
outperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer
also substantially outperforms a high-quality implementation of the widely applicable PPO algorithm.
b, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft
from scratch given sparse rewards, a long-standing challenge in artificial intelligence for which
previous approaches required human data or domain-specific heuristics.
1

Google DeepMind. 2 University of Toronto. Correspondence: mail@danijar.com

1

(a) Control Suite

(b) Atari

(c) ProcGen

(d) DMLab

(e) Minecraft

Figure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains,
ranging from robot locomotion and manipulation tasks over Atari games, procedurally generated
ProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and
infinite world of Minecraft. We also evaluate Dreamer on non-visual domains.

Introduction
Reinforcement learning has enabled computers to solve tasks through interaction, such as surpassing
humans in the games of Go and Dota 1,2 . It is also a key component for improving large language
models beyond what is demonstrated in their pretraining data 3,4 . While PPO 5 has become a standard
algorithm in the field of reinforcement learning, more specialized algorithms are often employed
to achieve higher performance. These specialized algorithms target the unique challenges posed
by different application domains, such as continuous control 6 , discrete actions 7,8 , sparse rewards 9 ,
image inputs 10 , spatial environments 11 , and board games 12 . However, applying reinforcement
learning algorithms to sufficiently new tasks—such as moving from video games to robotics tasks—
requires substantial effort, expertise, and computational resources for tweaking the hyperparameters
of the algorithm 13 . This brittleness poses a bottleneck in applying reinforcement learning to new
problems and also limits the applicability of reinforcement learning to computationally expensive
models or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new
domains without having to be reconfigured has been a central challenge in artificial intelligence and
would open up reinforcement learning to a wide range of practical applications.
We present Dreamer, a general algorithm that outperforms specialized expert algorithms across a
wide range of domains while using fixed hyperparameters, making reinforcement learning readily
applicable to new problems. The algorithm is based on the idea of learning a world model that equips
the agent with rich perception and the ability to imagine the future 14,15,16 . The world model predicts
the outcomes of potential actions, a critic neural network judges the value of each outcome, and an
actor neural network chooses actions to reach the best outcomes. Although intuitively appealing,
robustly learning and leveraging world models to achieve strong task performance has been an open
problem 17 . Dreamer overcomes this challenge through a range of robustness techniques based on
normalization, balancing, and transformations. We observe robust learning not only across over 150
tasks from the domains summarized in Figure 2, but also across model sizes and training budgets,
offering a predictable way to increase performance. Notably, larger model sizes not only achieve
higher scores but also require less interaction to solve a task.
To push the boundaries of reinforcement learning, we consider the popular video game Minecraft
that has become a focal point of research in recent years 18,19,20 , with international competitions held
for developing algorithms that autonomously learn to collect diamonds in Minecraft* . Solving this
*The MineRL Diamond Competitions were held in 2019, 2020, and 2021 and provided a dataset of human expert

trajectories: https://minerl.io/diamond. Competitions in the following years focused on a wide range of tasks.

2

a1

v1

a2

h1

h2

z1

h3

z2

r2

h1

z3

z1

v2

a2

dec

enc

dec

enc

dec

enc

x1

x̂1

x2

x̂2

x3

x̂3

x1

r2

h2

z2

enc

(a) World Model Learning

a1

v2

h3

z3

(b) Actor Critic Learning

Figure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete
representations zt that are predicted by a sequence model with recurrent state ht given actions at .
The inputs are reconstructed to shape the representations. The actor and critic predict actions at and
values vt and learn from trajectories of abstract representations predicted by the world model.
problem without human data has been widely recognized as a substantial challenge for artificial
intelligence because of the sparse rewards, exploration difficulty, long time horizons, and the
procedural diversity of this open world game 18 . Due to these obstacles, previous approaches resorted
to using human expert data and domain-specific curricula 19,20 . Applied out of the box, Dreamer is
the first algorithm to collect diamonds in Minecraft from scratch.

Learning algorithm
We present the third generation of the Dreamer algorithm 21,22 . The algorithm consists of three neural
networks: the world model predicts the outcomes of potential actions, the critic judges the value of
each outcome, and the actor chooses actions to reach the most valuable outcomes. The components
are trained concurrently from replayed experience while the agent interacts with the environment. To
succeed across domains, all three components need to accommodate different signal magnitudes and
robustly balance terms in their objectives. This is challenging as we are not only targeting similar
tasks within the same domain but aim to learn across diverse domains with fixed hyperparameters.
This section introduces the world model, critic, and actor along with their robust loss functions, as
well as tools for robustly predicting quantities of unknown orders of magnitude.

World model learning
The world model learns compact representations of sensory inputs through autoencoding 23 and enables planning by predicting future representations and rewards for potential actions. We implement
the world model as a Recurrent State-Space Model (RSSM) 24 , shown in Figure 3. First, an encoder
maps sensory inputs xt to stochastic representations zt . Then, a sequence model with recurrent
state ht predicts the sequence of these representations given past actions at−1 . The concatenation of

3

Context Input Open Loop Prediction
T=0 5
10
15
20

25

30

35

40

45

50

T=0

25

30

35

40

45

50

Model

True

Model

True

Context Input Open Loop Prediction

5

10

15

20

Figure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom).
Given 5 context images and the full action sequence, the model predicts 45 frames into the future
without access to intermediate images. The world model learns an understanding of the underlying
structure of each environment.
ht and zt forms the model state from which we predict rewards rt and episode continuation flags
ct ∈ {0, 1} and reconstruct the inputs to ensure informative representations:

ht = fϕ (ht−1 , zt−1 , at−1 )

 Sequence model:
Encoder:
zt ∼ qϕ (zt | ht , xt )
RSSM

 Dynamics predictor:
ẑt ∼ pϕ (ẑt | ht )
(1)
Reward predictor:
r̂t ∼ pϕ (r̂t | ht , zt )
Continue predictor:
ĉt ∼ pϕ (ĉt | ht , zt )
Decoder:
x̂t ∼ pϕ (x̂t | ht , zt )
Figure 4 visualizes long-term video predictions of the world world. The encoder and decoder use
convolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for
vector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations
are sampled from a vector of softmax distributions and we take straight-through gradients through
the sampling step 25,22 . Given a sequence batch of inputs x1:T , actions a1:T , rewards r1:T , and
continuation flags c1:T , the world model parameters ϕ are optimized end-to-end to minimize the
prediction loss Lpred , the dynamics loss Ldyn , and the representation loss Lrep with corresponding
loss weights βpred = 1, βdyn = 1, and βrep = 0.1:
hP
i
.
T
L(ϕ) = Eqϕ
(2)
t=1 (βpred Lpred (ϕ) + βdyn Ldyn (ϕ) + βrep Lrep (ϕ)) .
The prediction loss trains the decoder and reward predictor via the symlog squared loss described
later, and the continue predictor via logistic regression. The dynamics loss trains the sequence model
to predict the next representation by minimizing the KL divergence between the predictor pϕ (zt | ht )
and the next stochastic representation qϕ (zt | ht , xt ). The representation loss, in turn, trains the
representations to become more predictable allowing us to use a factorized dynamics predictor for
fast sampling during imagination training. The two losses differ in the stop-gradient operator sg(·)
and their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail
4

to contain enough information about the inputs, we employ free bits 26 by clipping the dynamics and
representation losses below the value of 1 nat ≈ 1.44 bits. This disables them while they are already
minimized well to focus learning on the prediction loss:
.
Lpred (ϕ) = − ln pϕ (xt | zt , ht ) − ln pϕ (rt | zt , ht ) − ln pϕ (ct | zt , ht )


.
Ldyn (ϕ) = max 1, KL sg(qϕ (zt | ht , xt ))
pϕ (zt | ht )
(3)


.
Lrep (ϕ) = max 1, KL
qϕ (zt | ht , xt )
sg(pϕ (zt | ht ))
Previous world models require scaling the representation loss differently based on the visual
complexity of the environment 21 . Complex 3D environments contain details unnecessary for
control and thus prompt a stronger regularizer to simplify the representations and make them
more predictable. In games with static backgrounds and where individual pixels may matter for
the task, a weak regularizer is required to extract fine details. We find that combining free bits
with a small representation loss resolves this dilemma, allowing for fixed hyperparameters across
domains. Moreover, we transform vector observations using the symlog function described later,
to prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the
representation loss.
We occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for
deep variational autoencoders 27 . To prevent this, we parameterize the categorical distributions of the
encoder and dynamics predictor as mixtures of 1% uniform and 99% neural network output, making
it impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further
model details and hyperparameters are included in the supplementary material.

Critic learning
The actor and critic neural networks learn behaviors purely from abstract trajectories of representations predicted by the world model 14 . For environment interaction, we select actions by sampling
from the actor network without lookahead planning. The actor and critic operate on model states
.
st = {ht , zt } and thus benefit from the Markovian representations
learned by the recurrent world
. P
τ
model. The actor aims to maximize the return Rt = ∞
γ
r
with
a discount factor γ = 0.997
t+τ
τ =0
for each model state. To consider rewards beyond the prediction horizon T = 16, the critic learns to
approximate the distribution of returns 28 for each state under the current actor behavior:
Actor:

at ∼ πθ (at | st )

Critic:

vψ (Rt | st )

(4)

Starting from representations of replayed inputs, the world model and actor generate a trajectory of
imagined model states s1:T , actions a1:T , rewards r1:T , and continuation flags c1:T . Because the critic
.
predicts a distribution, we read out its predicted values vt = E[vψ ( · | st )] as the expectation of the
distribution. To estimate returns that consider rewards beyond the prediction horizon, we compute
bootstrapped λ-returns 29 that integrate the predicted rewards and the values. The critic learns to
predict the distribution of the return estimates Rtλ using the maximum likelihood loss:


. P
.
.
λ
L(ψ) = − Tt=1 ln pψ (Rtλ | st )
Rtλ = rt + γct (1 − λ)vt + λRt+1
RTλ = vT (5)
While a simple choice would be to parameterize the critic as a Normal distribution, the return
distribution can have multiple modes and vary by orders of magnitude across environments. To
stabilize and accelerate learning under these conditions, we parameterize the critic as categorical
distribution with exponentially spaced bins, decoupling the scale of gradients from the prediction
5

targets as described later. To improve value prediction in environments where rewards are challenging
to predict, we apply the critic loss both to imagined trajectories with loss scale βval = 1 and to
trajectories sampled from the replay buffer with loss scale βrepval = 0.3. The critic replay loss
uses the imagination returns Rtλ at the start states of the imagination rollouts as on-policy value
annotations for the replay trajectory to then compute λ-returns over the replay rewards.
Because the critic regresses targets that depend on its own predictions, we stabilize learning by
regularizing the critic towards predicting the outputs of an exponentially moving average of its
own parameters. This is similar to target networks used previously in reinforcement learning 7 but
allows us to compute returns using the current critic network. We further noticed that the randomly
initialized reward predictor and critic networks at the start of training can result in large predicted
rewards that can delay the onset of learning. We thus initialize the output weight matrix of the
reward predictor and critic to zeros, which alleviates the problem and accelerates early learning.

Actor learning
The actor learns to choose actions that maximize return while exploring through an entropy regularizer 30 . However, the correct scale for this regularizer depends both on the scale and frequency of
rewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse
and exploit more if rewards are dense or nearby. At the same time, the exploration amount should
not be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the
return scale while preserving information about reward frequency.
To use a fixed entropy scale of η = 3 × 10−4 across domains, we normalize returns to be approximately contained in the interval [0, 1]. In practice, substracting an offset from the returns does
not change the actor gradient and thus dividing by the range S is sufficient. Moreover, to avoid
amplifying noise from function approximation under sparse rewards, we only scale down large return
magnitudes but leave small returns below the threshold of L = 1 untouched. We use the Reinforce
estimator 31 for both discrete and continuous actions, resulting in the surrogate loss function:





. PT
λ
(6)
L(θ) = − t=1 sg Rt − vψ (st ) / max(1, S) log πθ (at | st ) + η H πθ (at st )
The return distribution can be multi-modal and include outliers, especially for randomized environments where some episodes have higher achievable returns than others. Normalizing by the smallest
and largest observed returns would then scale returns down too much and may cause suboptimal
convergence. To be robust to these outliers, we compute the range from the 5th to the 95th return
percentile over the return batch and smooth out the estimate using an exponential moving average:

.
(7)
S = EMA Per(Rtλ , 95) − Per(Rtλ , 5), 0.99
Previous work typically normalizes advantages 5 rather than returns, which puts a fixed amount
of emphasis on maximizing returns over entropy regardless of whether rewards are within reach.
Scaling up advantages when rewards are sparse can amplify noise that outweighs the entropy
regularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can
fail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards
regardless of their size. Constrained optimization targets a fixed entropy on average across states 32,33
regardless of achievable returns, which is robust but explores slowly under sparse rewards and
converges lower under dense rewards. We did not find stable hyperparameters across domains
for these approaches. Return normalization with a denominator limit overcomes these challenges,
exploring rapidly under sparse rewards and converging to high performance across diverse domains.
6

Robust predictions
Reconstructing inputs and predicting rewards and returns can be challenging because the scale of
these quantities can vary across domains. Predicting large targets using a squared loss can lead to
divergence whereas absolute and Huber losses 7 stagnate learning. On the other hand, normalizing
targets based on running statistics 5 introduces non-stationarity into the optimization. We suggest
the symlog squared error as a simple solution to this dilemma. For this, a neural network f (x, θ)
with inputs x and parameters θ learns to predict a transformed version of its targets y. To read out
predictions ŷ of the network, we apply the inverse transformation:
2

.
.
(8)
ŷ = symexp f (x, θ)
L(θ) = 21 f (x, θ) − symlog(y)
Using the logarithm as transformation would not allow us to predict targets that take on negative
values. Therefore, we choose a function from the bi-symmetric logarithmic family 34 that we name
symlog as the transformation with the symexp function as its inverse:


.
.
symlog(x) = sign(x) ln |x| + 1
symexp(x) = sign(x) exp(|x|) − 1
(9)
The symlog function compresses the magnitudes of both large positive and negative values. Unlike
the logarithm, it is symmetric around the origin while preserving the input sign. This allows the
optimization process to quickly move the network predictions to large values when needed. The
symlog function approximates the identity around the origin so that it does not affect learning of
targets that are already small enough.
For potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss.
Here, the network outputs the logits for a softmax distribution over exponentially spaced bins bi ∈ B.
Predictions are read out as the weighted average of the bin positions weighted by their predicted
probabilities. Importantly, the network can output any continuous value in the interval because the
weighted average can fall between the buckets:


.
.
(10)
ŷ = softmax(f (x))T B
B = symexp( −20 ... +20 )
The network is trained on twohot encoded targets 8,28 , a generalization of onehot encoding to
continuous values. The twohot encoding of a scalar is a vector with |B| entries that are all 0 except
at the indices k and k + 1 of the two bins closest to the encoded scalar. The two entries sum up
to 1, with linearly higher weight given to the bin that is closer to the encoded continuous number.
The network is then trained to minimize the categorical cross entropy loss for classification with
soft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the
continuous values associated with the bin locations, decoupling the size of the gradients from the
size of the targets:
.
(11)
L(θ) = − twohot(y)T log softmax(f (x, θ))
Applying these principles, Dreamer transforms vector observations using the symlog functions, both
for the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward
predictor and critic. We find that these techniques enable robust and fast learning across many
diverse domains. For critic learning, an alternative asymmetric transformation has previously been
proposed 35 , which we found less effective on average across domains. Unlike alternatives, symlog
transformations avoid truncating large targets 7 , introducing non-stationary from normalization 5 , or
adjusting network weights when new extreme values are detected 36 .
7

Agents with
item (%)

100
75
50
25
0
0M

Iron Ingot

50M 100M
Env steps

100
75
50
25
0
0M

Iron Pickaxe

50M 100M
Env steps

100
75
50
25
0
0M

Diamond
Dreamer
IMPALA
Rainbow
PPO
50M 100M
Env steps

Figure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft
Diamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only
compared algorithm that manages to discover a diamond, and does so reliably.

Results
We evaluate the generality of Dreamer across 8 domains—with over 150 tasks—under fixed hyperparameters. We designed the experiments to compare Dreamer to the best methods in the literature,
which are often specifically designed and tuned for the benchmark at hand. We further compare to a
high-quality implementation of PPO 5 , a standard reinforcement learning algorithm that is known for
its robustness. We run PPO with fixed hyperparameters chosen to maximize performance across
domains and that reproduce strong published results of PPO on ProcGen 37 . To push the boundaries
of reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing
it to strong previous algorithms. Finally, we analyze the importance of individual components of
Dreamer and its robustness to different model sizes and computational budgets. All Dreamer agents
are trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A
public implementation of Dreamer that reproduces all results is available on the project website.
Benchmarks We perform an extensive empirical study across 8 domains that include continuous
and discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward
scales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results,
showing that Dreamer outperforms a wide range of previous expert algorithms across diverse
domains. Crucially, Dreamer substantially outperforms PPO across all domains.
• Atari This established benchmark contains 57 Atari 2600 games with a budget of 200M frames,
posing a diverse range of challenges 38 . We use the sticky action simulator setting 39 . Dreamer
outperforms the powerful MuZero algorithm 8 while using only a fraction of the computational
resources. Dreamer also outperforms the widely-used expert algorithms Rainbow 40 and IQN 41 .
• ProcGen This benchmark of 16 games features randomized levels and visual distractions to test
the robustness and generalization of agents 42 . Within the budget of 50M frames, Dreamer matches
the tuned expert algorithm PPG 37 and outperforms Rainbow 42,40 . Our PPO agent with fixed
hyperparameters matches the published score of the highly tuned official PPO implementation 37 .
• DMLab This suite of 30 tasks features 3D environments that test spatial and temporal reasoning 43 . In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and R2D2+
agents 35 at 1B environment steps, amounting to a data-efficiency gain of over 1000%. We note
that these baselines were not designed for data-efficiency but serve as a valuable comparison point
for the performance previously achievable at scale.

8

Return (%)

b Learning signals
100
0

Dreamer
No reward & value grads
No reconstruction grads
0 50 100
Env steps (%)

Crafter

10

500

DMLab Goals

250

0
0
0 20 40
0 100 200
6
Env steps (10 )
Env steps (106)
400M 200M 100M 50M 25M 12M

d Replay scaling

14 task mean

50

20
Return

14 task mean

Dreamer
No obs symlog
No retnorm (advnorm)
50
No symexp twohot (Huber)
No KL balance & free bits
0
0 50 100 Without all
Env steps (%)

100

c Model size scaling

DMLab Goals
450
300
9
150
0
0
0 10 20
0 100 200
Env steps (106)
Env steps (106)
64 32 16 8 4 2 1

18
Return

Return (%)

a Robustness techniques

Crafter

Figure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques
contribute to the performance of Dreamer on average, although each individual technique may only
affect some tasks. Training curves of individual tasks are included in the supplementary material.
b, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its
world model, unlike most prior algorithms that rely predominantly on reward and value prediction
gradients 7,5,8 . c, The performance of Dreamer increases monotonically with larger model sizes,
ranging from 12M to 400M parameters. Notably, larger models not only increase task performance
but also require less environment interaction. d, Higher replay ratios predictably increase the
performance of Dreamer. Together with model size, this allows practitioners to improve task
performance and data-efficiency by employing more computational resources.
• Atari100k This data-efficiency benchmark comntains 26 Atari games and a budget of only
400K frames, amounting to 2 hours of game time 17 . EfficientZero 44 holds the state-of-the-art by
combining online tree search, prioritized replay, and hyperparameter scheduling, but also resets
levels early to increase data diversity, making a comparison difficult. Without this complexity,
Dreamer outperforms the best remaining methods, including the transformer-based IRIS and
TWM agents, the model-free SPR, and SimPLe 45 .
• Proprio Control This benchmark contains 18 control tasks with continuous actions, proprioceptive vector inputs, and a budget of 500K environment steps 46 . The tasks range from classical
control over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer
sets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO 33 .
• Visual Control This benchmark consists of 20 continuous control tasks where the agent receives
only high-dimensional images as input and has a budget of 1M environment steps 46 . Dreamer
establishes a new state-of-the-art on this benchmark, outperforming DrQ-v2 and CURL 47 , which
are specialized to visual environments and leverage data augmentation.

9

• BSuite This benchmark includes 23 environments with a total of 468 configurations that
are specifically designed to test credit assignment, robustness to reward scale and stochasticity,
memory, generalization, and exploration 48 . Dreamer establishes a new state-of-the-art on this
benchmark, outperforming Boot DQN and other methods 49 . Dreamer improves over previous
algorithms especially in the scale robustness category.
Minecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge
in artificial intelligence 18,19,20 . Every episode in this game is set in a unique randomly generated
and infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes,
during which the player needs to discover a sequence of 12 items from sparse rewards by foraging
for resources and crafting tools. It takes about 20 minutes for experienced human players to obtain
diamonds 20 . We follow the block breaking setting of prior work 19 because the provided action space
would make it challenging for stochastic policies to keep a key pressed for a prolonged time.
Because of the training time in this complex domain, extensive tuning would be difficult for
Minecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in
Figures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without
using human data as was required by VPT 20 or adaptive curricula 19 . All the Dreamer agents we
trained on Minecraft discover diamonds in 100M environment steps. While several strong baselines
progress to advanced items such as the iron pickaxe, none of them discovers a diamond.
Ablations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of
14 tasks to understand their importance. The training curves of individual tasks are included in the
supplementary material. We observe that all robustness techniques contribute to performance, most
notably the KL objective of the world model, followed by return normalization and symexp twohot
regression for reward and value prediction. In general, we find that each individual technique is
critical on a subset of tasks but may not affect performance on other tasks.
To investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping
either the task-specific reward and value prediction gradients or the task-agnostic reconstruction
gradients from shaping its representations. Unlike previous reinforcement learning algorithms that
often rely only on task-specific learning signals 7,8 , Dreamer rests predominantly on the unsupervised
objective of its world model. This finding could allow for future algorithm variants that leverage
pretraining on unsupervised data.
Scaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes
ranging from 12M to 400M parameters, as well as different replay ratios on Crafter 50 and a DMLab
task 43 . The replay ratio affects the number of gradient updates performed by the agent. Figure 6
shows robust learning with fixed hyperparameters across the compared model sizes and replay ratios.
Moreover, increasing the model size directly translates to both higher task performance and a lower
data requirement. Increasing the number of gradient steps further reduces the interactions needed to
learn successful behaviors. The results show that Dreamer learns robustly across model sizes and
replay ratios and that its performance and provides a predictable way for increasing performance
given computational resources.

10

Previous work
Developing general-purpose algorithms has long been a goal of reinforcement learning research.
PPO 5 is one of the most widely used algorithms and is relatively robust but requires large amounts
of experience and often yields lower performance than specialized alternatives. SAC 32 is a popular
choice for continuous control and leverages experience replay for data-efficiency, but in practice
requires tuning, especially for its entropy scale, and struggles under high-dimensional inputs 51 .
MuZero 8 plans using a value prediction model and has been applied to board games and Atari, but the
authors did not release an implementation and the algorithm contains several complex components,
making it challenging to reproduce. Gato 52 fits one large model to expert demonstrations of multiple
tasks, but is only applicable when expert data is available. In comparison, Dreamer masters a
diverse range of environments with fixed hyperparameters, does not require expert data, and its
implementation is open source.
Minecraft has been a focus of recent research. With MALMO 53 , Microsoft released a free version
of the successful game for research purposes. MineRL 18 offers several competition environments,
which we rely on as the basis for our experiments. The MineRL competition supports agents in
exploring and learning meaningful skills through a diverse human dataset 18 . Voyager obtains items
at a similar depth in the technology tree as Dreamer using API calls to a language model but operates
on top of the MineFlayer bot scripting layer that was specifically engineered to the game and
exposes high-level actions 54 . VPT 20 trained an agent to play Minecraft through behavioral cloning
based on expert data of keyboard and mouse actions collected by contractors and finetuning using
reinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer
uses the MineRL competition action space to autonomously learn to collect diamonds from sparse
rewards using 1 GPU for 9 days, without human data.

Conclusion
We present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm
that masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across
over 150 tasks but also learns robustly across varying data and compute budgets, moving reinforcement learning toward a wide range of practical applications. Applied out of the box, Dreamer is
the first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone
in the field of artificial intelligence. As a high-performing algorithm that is based on a learned
world model, Dreamer paves the way for future research directions, including teaching agents world
knowledge from internet videos and learning a single world model across domains to allow artificial
agents to build up increasingly general knowledge and competency.
Acknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schulman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis
Yarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank
Daniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.

11

References
1. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van
Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,
et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):
484, 2016.
2. OpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018.
3. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in neural information processing systems,
35:27730–27744, 2022.
4. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi.
Coderl: Mastering code generation through pretrained models and deep reinforcement learning.
Advances in Neural Information Processing Systems, 35:21314–21328, 2022.
5. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
6. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
7. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
8. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering
atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265,
2019.
9. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo,
David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary
tasks. arXiv preprint arXiv:1611.05397, 2016.
10. Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R Devon
Hjelm. Unsupervised state representation learning in atari. Advances in neural information
processing systems, 32, 2019.
11. Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, and Marc Toussaint. Reinforcement
learning with neural radiance fields. arXiv preprint arXiv:2206.01634, 2022.
12. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
go without human knowledge. Nature, 550(7676):354, 2017.
13. Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael
Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What
matters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint
arXiv:2006.05990, 2020.
14. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM
SIGART Bulletin, 2(4):160–163, 1991.
12

15. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017
IEEE International Conference on Robotics and Automation (ICRA), pages 2786–2793. IEEE,
2017.
16. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
17. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
18. William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie
Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al.
The minerl competition on sample efficient reinforcement learning using human priors. arXiv
e-prints, pages arXiv–1904, 2019.
19. Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,
Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, et al. Multi-task
curriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint
arXiv:2106.14876, 2021.
20. Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon
Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching
unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022.
21. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
22. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
discrete world models. arXiv preprint arXiv:2010.02193, 2020.
23. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
24. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee,
and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint
arXiv:1811.04551, 2018.
25. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
26. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. Advances in neural information
processing systems, 29, 2016.
27. Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on
images. arXiv preprint arXiv:2011.10650, 2020.
28. Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on
reinforcement learning. In International Conference on Machine Learning, pages 449–458.
PMLR, 2017.
29. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
30. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement
learning algorithms. Connection Science, 3(3):241–268, 1991.

13

31. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Machine learning, 8(3-4):229–256, 1992.
32. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
33. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and
Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,
2018.
34. J Beau W Webber. A bi-symmetric log transformation for wide-range data. Measurement
Science and Technology, 24(2):027001, 2012.
35. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent
experience replay in distributed reinforcement learning. In International conference on learning
representations, 2018.
36. Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado
van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pages 3796–3803, 2019.
37. Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In
International Conference on Machine Learning, pages 2020–2027. PMLR, 2021.
38. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
environment: An evaluation platform for general agents. Journal of Artificial Intelligence
Research, 47:253–279, 2013.
39. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.
40. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will
Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
41. Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In International conference on machine learning, pages
1096–1105. PMLR, 2018.
42. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation
to benchmark reinforcement learning. In International conference on machine learning, pages
2048–2056. PMLR, 2020.
43. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich
Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv
preprint arXiv:1612.03801, 2016.
44. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari
games with limited data. Advances in Neural Information Processing Systems, 34:25476–25488,
2021.
45. Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world
models. arXiv preprint arXiv:2209.00588, 2022.

14

46. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite.
arXiv preprint arXiv:1801.00690, 2018.
47. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous
control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645,
2021.
48. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva,
Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for
reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.
49. Olivia Dizon-Paradis, Stephen Wormald, Daniel Capecci, Avanti Bhandarkar, and Damon
Woodard. Investigating the practicality of existing reinforcement learning algorithms: A
performance comparison. Authorea Preprints, 2023.
50. Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint
arXiv:2109.06780, 2021.
51. Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus.
Improving sample efficiency in model-free reinforcement learning from images. arXiv preprint
arXiv:1910.01741, 2019.
52. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.
A generalist agent. arXiv preprint arXiv:2205.06175, 2022.
53. Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for
artificial intelligence experimentation. In IJCAI, pages 4246–4247. Citeseer, 2016.
54. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,
and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.
arXiv preprint arXiv:2305.16291, 2023.
55. Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun
Wang. The 37 implementation details of proximal policy optimization. The ICLR Blog Track
2023, 2022.
56. Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani,
Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A
research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979,
2020.
57. Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared
experience replay. In International Conference on Machine Learning, pages 8545–8554. PMLR,
2020.
58. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.
arXiv preprint arXiv:1511.05952, 2015.
59. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. In International Conference on Machine Learning,
pages 1059–1071. PMLR, 2021.
60. Liu Ziyin, Zhikang T Wang, and Masahito Ueda. Laprop: Separating momentum and adaptivity
in adam. arXiv preprint arXiv:2002.04839, 2020.

15

61. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
62. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare,
and Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement
learning. arXiv preprint arXiv:1704.04651, 2017.
63. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
64. Matthijs Van Keirsbilck, Alexander Keller, and Xiaodong Yang. Rethinking full connectivity in
recurrent neural networks. arXiv preprint arXiv:1905.12340, 2019.
65. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.
66. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward,
Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl
with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.

16

Methods
Baselines
We employ the Proximal Policy Optimization (PPO) algorithm 5 , which has become a standard
choice in the field, to compare Dreamer under fixed hyperparameters across all benchmarks. There
are a large number of PPO implementations available publicly and they are known to substantially
vary in task performance 55 . To ensure a comparison that is representative of the highest performance
PPO can achieve under fixed hyperparameters across domains, we choose the high-quality PPO
implementation available in the Acme framework 56 and select its hyperparameters in Table 1
following recommendations 55,13 and additionally tune its epoch batch size to be large enough for
complex environments 42 , its learning rate, and its entropy scale. We match the discount factor to
Dreamer because it works well across domains and is a common choice in the literature 35,8 . We
choose the IMPALA network architecture that we have found performed better than alternatives 42
and set the minibatch size to the largest possible for one A100 GPU. We verify the performance of
our PPO implementation and hyperparameters on the ProcGen benchmark, where a highly tuned
PPO implementation has been reported by the PPO authors 37 . We find that our implementation
matches or slightly outperforms this performance reference.
Parameter

Value

Observation normalization
Reward normalization
Reward clipping (stddev.)
Epoch batch
Number of epochs
Minibatch size
Minibatch length
Policy trust region
Value trust region
Advantage normalization
Entropy penalty scale
Discount factor
GAE lambda
Learning rate
Gradient clipping
Adam epsilon

Yes
Yes
10
64 × 256
3
8
256
0.2
No
Yes
0.01
0.997
0.95
3 × 10−4
0.5
10−5

Table 1: PPO hyperparameters used across all benchmarks.
For Minecraft, we additionally tune and run the IMPALA and Rainbow algorithms because not
successful end-to-end learning from scratch has been reported in the literature 18 . We use the Acme
implementations 56 of these algorithms, use the same IMPALA network we used for PPO, and tuned
the learning rate and entropy regularizers. For all other benchmarks, we compare to tuned expert
algorithms reported in the literature as referenced in the results section.

17

Implementation
Experience replay We implement Dreamer using a uniform replay buffer with an online queue 57 .
Specifically, each minibatch is formed first from non-overlapping online trajectories and then filled
up with uniformly sampled trajectories from the replay buffer. We store latent states into the replay
buffer during data collection to initialize the world model on replayed trajectories, and write the
fresh latent states of the training rollout back into the buffer. While prioritized replay 58 is used by
some of the expert algorithms we compare to and we found it to also improve the performance of
Dreamer, we opt for uniform replay in our experiments for ease of implementation.
We parameterize the amount of training via the replay ratio. This is the fraction of time steps trained
on per time step collected from the environment, without action repeat. Dividing the replay ratio by
the time steps in a minibatch and by action repeat yields the ratio of gradient steps to env steps. For
example, a replay ratio of 32 on Atari with action repeat of 4 and batch shape 16 × 64 corresponds
to 1 gradient step every 128 env steps, or 1.5M gradient steps over 200M env steps.
Optimizer We employ Adaptive Gradient Clipping (AGC) 59 , which clips per-tensor gradients if
they exceed 30% of the L2 norm of the weight matrix they correspond to, with its default ϵ = 10−3 .
AGC decouples the clipping threshold from the loss scales, allowing to change loss functions or
loss scales without adjusting the clipping threshold. We apply the clipped gradients using the
LaProp optimizer 60 with ϵ = 10−20 and its default parameters β1 = 0.9 and β2 = 0.99. LaProp
normalizes gradients by RMSProp and then smoothes them by momentum, instead of computing
both momentum and normalizer on raw gradients as Adam does 61 . This simple change allows for a
smaller epsilon and avoids occasional instabilities that we observed under Adam.
Distributions The encoder, dynamics predictor, and actor distributions are mixtures of 99% the
predicted softmax output and 1% of a uniform distribution 62 to prevent zero probabilities and
infinite log probabilities. The rewards and critic neural networks output a softmax distribution over
exponentially spaced bins b ∈ B and are trained towards twohot encoded targets:


|B|
|bk+1 − x| / |bk+1 − bk | if i = k
.
. X
twohot(x)i = |bk − x| / |bk+1 − bk | if i = k + 1
k=
δ(bj < x)
(12)


j=1
0
else
The output weights of twohot distributions are initialized to zero to ensure that the agent does
not hallucinate rewards and values at initialization. For computing the expected prediction of the
softmax distribution under bins that span many orders of magnitude, the summation order matters
and positive and negative bins should be summed up separately, from small to large bins, and then
added. Refer to the source code for an implementation.
Networks Images are encoded using stride 2 convolutions to resolution 6 × 6 or 4 × 4 and then
flattened and are decoded using transposed stride 2 convolutions, with sigmoid activation on the
output. Vector inputs are symlog transformed and then encoded and decoded using 3-layer MLPs.
The actor and critic neural networks are also 3-layer MLPs and the reward and continue predictors
are 1-layer MLPs. The sequence model is a GRU 63 with block-diagonal recurrent weights 64 of 8
blocks to allow for a large number of memory units without quadratic increase in parameters and
FLOPs. The input to the GRU at each time step is a linear embedding of the sampled latent zt , of
the action at , and of the recurrent state to allow mixing between blocks.

18

Benchmarks
Protocols Summarized in Table 2, we follow the standard evaluation protocols for the benchmarks
where established. Atari 38 uses 57 tasks with sticky actions 65 . The random and human reference
scores used to normalize scores vary across the literature and we chose the most common reference
values, replicated in Table 6. DMLab 43 uses 30 tasks 66 and we use the fixed action space 36,35 . We
evaluate at 100M steps because running for 10B as in some prior work was infeasible. Because
existing published baselines perform poorly at 100M steps, we compare to their performance at 1B
steps instead, giving them a 10× data advantage. ProcGen uses the hard difficulty setting and the
unlimited level set 42 . Prior work compares at different step budgets 42,37 and we compare at 50M
steps due to computational cost, as there is no action repeat. For Minecraft Diamond purely from
sparse rewards, we establish the evaluation protocol to report the episode return measured at 100M
env steps, corresponding to about 100 days of in-game time. Atari100k 17 includes 26 tasks with
a budget of 400K env steps, 100K after action repeat. Prior work has used various environment
settings, summarized in Table 10, and we chose the environments as originally introduced. Visual
Control 46,21 spans 20 tasks with an action repeat of 2. Proprioceptive Control follows the same
protocol but we exclude the two quadruped tasks because of baseline availability in prior work 47 .

Benchmark

Tasks

Env
Steps

Minecraft
DMLab
ProcGen
Atari
Atari100K
BSuite
Proprio Control
Visual Control

1
30
16
57
26
23
18
20

100M
100M
50M
200M
400K
—
500K
1M

Action
Repeat
1
4
1
4
4
1
2
2

Env
Replay
Instances Ratio
64
16
16
16
1
1
16
16

32
32
64
32
128
1024
512
512

GPU
Days

Model
Size

8.9
2.9
16.1
7.7
0.1
0.5
0.3
0.1

200M
200M
200M
200M
200M
200M
12M
12M

Table 2: Benchmark overview. All agents were trained on a single Nvidia A100 GPU each.
Environment instances In earlier experiments, we observed that the performance of both Dreamer
and PPO is robust to the number of environment instances. Based on the CPU resources available
on our training machines, we use 16 environment instance by default. For BSuite, the benchmark
requires using a single environment instance. We also use a single environment instance for
Atari100K because the benchmark has a budget of 400K env steps whereas the maximum episode
length in Atari is in principle 432K env steps. For Minecraft, we use 64 environments using remote
CPU workers to speed up experiments because the environment is slower to step.
Seeds and error bars We run 5 seeds for each Dreamer and PPO per benchmark, with the exception
of 1 seed for ProcGen due to computational constraints, 10 seeds for BSuite as required by the
benchmark, and 10 seeds for Minecraft to reliably report the fraction of runs that achieve diamonds.
All curves show the mean over seeds with one standard deviation shaded.
Computational choices All Dreamer and PPO agents in this paper were trained on a single Nvidia
A100 GPU each. Dreamer uses the 200M model size by default. On the two control suitse, Dreamer
the same performance using the substantially faster 12M model, making it more accessible to
researchers. The replay ratio control the trade-off between computational cost and data efficiency as
analyzed in Figure 6 and is chosen to fit the step budget of each benchmark.
19

Model sizes
To accommodate different computational budgets and analyze robustness to different model sizes,
we define a range of models ranging from 12M to 400M parameters shown in Table 3. The sizes
are parameterized by the model dimension, which approximately increases in multiples of 1.5,
alternating between powers of two and power of two scaled by 1.5. This yields tensor shapes that
are multiples of 8 as required for hardware efficiency. Sizes of different network components derive
from the model dimension. The MLPs have the model dimension as the number of hidden units.
The sequence model has 8 times the number of recurrent units, split into 8 blocks of the same size as
the MLPs. The convolutional encoder and decoder layers closest to the data use 16× fewer channels
than the model dimension. Each latent also uses 16× fewer codes than the model dimension. The
number of hidden layers and number of latents is fixed across model sizes. All hyperparamters,
including the learning rate and batch size, are fixed across model sizes.
Parameters

12M

25M

50M

100M

200M

400M

Hidden size (d)
Recurrent units (8d)
Base CNN channels (d/16)
Codes per latent (d/16)

256
1024
16
16

384
3072
24
24

512
4096
32
32

768
6144
48
48

1024
8192
64
64

1536
12288
96
96

Table 3: Dreamer model sizes. The number of MLP hidden units defines the model dimension,
from which recurrent units, convolutional channels, and number of codes per latent are derived. The
number of layers and latents is constant across model sizes.

Previous Dreamer generations
We present the third generation of the Dreamer line of work. Where the distinction is useful, we refer
to this algorithm as DreamerV3. The DreamerV1 algorithm 21 was limited to continuous control,
the DreamerV2 algorithm 22 surpassed human performance on Atari, and the DreamerV3 algorithm
enables out-of-the-box learning across diverse benchmarks.
We summarize the changes that DreamerV3 introduces as follows:
• Robustness techniques: Observation symlog, KL balance and free bits, 1% unimix for all categoricals, percentile return normalization, symexp twohot loss for the reward head and critic.
• Network architecture: Block GRU, RMSNorm normalization, SiLu activation.
• Optimizer: Adaptive gradient clipping (AGC), LaProp (RMSProp followed by momentum).
• Replay buffer: Larger capacity, online queue, storing and updating latent states.

20

Hyperparameters
Name

Symbol

Value

—
B
T
—
—
—
—

5 × 106
16
64
RMSNorm + SiLU
4 × 10−5
AGC(0.3)
LaProp(ϵ = 10−20 )

βpred
βdyn
βrep
—
—

1
1
0.1
1%
1

H
1/(1 − γ)
λ
βval
βrepval
—
—
βpol
η
—
S
L
—

15
333
0.95
1
0.3
1
0.98
1
3 × 10−4
1%
Per(R, 95) − Per(R, 5)
1
0.99

General
Replay capacity
Batch size
Batch length
Activation
Learning rate
Gradient clipping
Optimizer
World Model
Reconstruction loss scale
Dynamics loss scale
Representation loss scale
Latent unimix
Free nats
Actor Critic
Imagination horizon
Discount horizon
Return lambda
Critic loss scale
Critic replay loss scale
Critic EMA regularizer
Critic EMA decay
Actor loss scale
Actor entropy regularizer
Actor unimix
Actor RetNorm scale
Actor RetNorm limit
Actor RetNorm decay

Table 4: Dreamer hyperparameters. The same values are used across all benchmarks, including
proprioceptive and visual inputs, continuous and discrete actions, and 2D and 3D domains. We do
not use any hyperparameter annealing, prioritized replay, weight decay, or dropout.

21

Minecraft
Game description With 100M monthly active users, Minecraft is one of the most popular video
games worldwide. Minecraft features a procedurally generated 3D world of different biomes,
including plains, forests, jungles, mountains, deserts, taiga, snowy tundra, ice spikes, swamps,
savannahs, badlands, beaches, stone shores, rivers, and oceans. The world consists of 1 meter sized
blocks that the player and break and place. There are about 30 different creatures that the player
can interact with or fight. From gathered resources, the player can use over 350 recipes to craft
new items and progress through the technology tree, all while ensuring safety and food supply to
survive. There are many conceivable tasks in Minecraft and as a first step, the research community
has focused on the salient task of obtaining a diamonds, a rare item found deep underground and
requires progressing through the technology tree.
Learning environment We built the Minecraft Diamond environment on top of MineRL to define a
flat categorical action space and fix issues we discovered with the original environments via human
play testing. For example, when breaking diamond ore, the item sometimes jumps into the inventory
and sometimes needs to be collected from the ground. The original environment terminates episodes
when breaking diamond ore so that many successful episodes end before collecting the item and thus
without the reward. We remove this early termination condition and end episodes when the player
dies or after 36000 steps, corresponding to 30 minutes at the control frequency of 20Hz. Another
issue is that the jump action has to be held for longer than one control step to trigger a jump, which
we solve by keeping the key pressed in the background for 200ms. We built the environment on top
of MineRL v0.4.4 18 , which offers abstract crafting actions. The Minecraft version is 1.11.2.
Observations and rewards The agent observes a 64 × 64 × 3 first-person image, an inventory
count vector for the over 400 items, a vector of maximum inventory counts since episode begin
to tell the agent which milestones it has achieved, a one-hot vector indicating the equipped item,
and scalar inputs for the health, hunger, and breath levels. We follow the sparse reward structure of
the MineRL competition environment 18 that rewards 12 milestones leading up to the diamond, for
obtaining the items log, plank, stick, crafting table, wooden pickaxe, cobblestone, stone pickaxe,
iron ore, furnace, iron ingot, iron pickaxe, and diamond. The reward for each item is only given
once per episode, and the agent has to learn to collect certain items multiple times to achieve the
next milestone. To make the return easy to interpret, we give a reward of +1 for each milestone
instead of scaling rewards based on how valuable each item is. Additionally, we give −0.01 for each
lost heart and +0.01 for each restored heart, but did not investigate whether this is helpful.

22

Supplementary material
Minecraft video predictions

Context
Prediction
T = 0 Input
5 Open
10 Loop15
20

25

30

35

40

45

50

Context
Prediction
T = 0 Input
5 Open
10 Loop15
20

25

30

35

40

45

50

Context
Prediction
T = 0 Input
5 Open
10 Loop15
20

25

30

35

40

45

50

Context
Prediction
T = 0 Input
5 Open
10 Loop15
20

25

30

35

40

45

50

T=0

25

30

35

40

45

50

Model

True

Model

True

Model

True

Model

True

Model

True

Context Input Open Loop Prediction

5

10

15

20

Figure 7: Multi-step predictions on Minecraft. The world model receives the first 5 frames as
context input and the predicts 45 steps into the future given the action sequence and without access
to intermediate images.

23

Minecraft additional results

12
Return

Dreamer
IMPALA
Rainbow
PPO

9.1
7.1
6.3
5.1

Return

Method

4

Dreamer
IMPALA
Rainbow
PPO

Figure 8: Minecraft learning curves.

% of episodes
% of episodes

Iron Ore
Iron Ingot
Iron Pickaxe
Diamond
80
80
24
1
60
60
18
0.8
40
40
12
0.5
20
20
6
0.2
0 1M 10M 100M 100K
0 1M 10M 100M 100K
0 1M 10M 100M 100K
0 1M 10M 100M
100K
Env steps
Env steps
Env steps
Env steps

100
75
50
25
0

Cobblestone

Dreamer

IMPALA

100
75
50
25
0

100
75
50
25
0

Crafting Table

100
75
50
25
0

Wooden Pickaxe

100
75
50
25
0

Stick

100
75
50
25
0

% of episodes

Planks

100
75
50
25
0

8
0 0 25M 50M 75M 100M
Env steps

Table 5: Minecraft Diamond scores at 100M
environment steps.

Log

Minecraft Diamond

Furnace

Rainbow

100
75
50
25
0

Stone Pickaxe

PPO

Figure 9: Item success rates as a percentage of episodes. Dreamer obtains items at substantially
higher rates than the baselines and continues to improve until the 100M step budget. At the budget,
Dreamer obtains diamonds in 0.4% of episodes, leaving a challenge for future research. This metric
differs from Figure 5, which shows that over the course of training, 100% of Dreamer agents obtain
one or more diamonds regardless of episode boundaries, compared to 0% of the baseline agents.

24

Return

75K
50K
25K
0

Return

1.2K
800
400
0

Return

750
500
250
0

Return

30
15
0
-15

Return

12K
8K
4K
0

Return

Atari learning curves

150K
100K
50K
0

Alien

Amidar
4K
2K
0

Bank Heist

Breakout

450K
300K
150K
0
750K
500K
250K
0

Double Dunk

Battle Zone

Centipede

Beam Rider

Hero

45K
30K
15K
0

Return

3K
2K
1K
0

Pong

15
0
-15

45K
30K
15K
0

Return

600K
300K
0

Return

Seaquest

0
-8
-16
-24

Tennis

Return

Skiing

60K
40K
20K
0

Ice Hockey

Ms Pacman

Qbert

Tutankham

200M

Env steps

75K
50K
25K
0

12K
8K
4K
0

Phoenix

300K
150K
0

Riverraid

120K
80K
40K
0

750K
500K
250K
0

200M

Venture

Krull

90K
60K
30K
0

Pitfall

0
-150
-300
120
80
40
0

Star Gunner

1.8K
1.2K
600
0

Gopher

120K
80K
40K
0

Road Runner

750K
500K
250K
0

Up N Down

Demon Attack

150K
100K
50K
0

600K
400K
200K
0

Space Invaders

30K
20K
10K
0

Boxing

90
60
30
0

Kangaroo

Name This Game

Zaxxon
0

Frostbite

75K
50K
25K
0

Jamesbond

120K
80K
40K
0

Solaris

8K
6K
4K
2K

Yars Revenge

24K
16K
8K
0

Defender

600K
400K
200K
0

Atlantis

1.5M
1M
500K
0

Bowling

240
160
80

30
15
0

300
150
0

900K
600K
300K
0
200M 0

Env steps

Freeway

180K
120K
60K
0

300K
150K
0

300K
150K
0

0

Fishing Derby

Time Pilot

Wizard Of Wor
80K
40K
0

Private Eye

-16K
-24K
-32K

Crazy Climber

75
50
25
0

Kung Fu Master MontezumaRev

Berzerk

24K
16K
8K
0

60
0
-60

Asteroids
300K
150K
0

ChopperComm

900K
600K
300K
0

Enduro

Asterix

900K
600K
300K
0

160K
80K
0

2.4K
1.6K
800
0

Gravitar

Assault

45K
30K
15K
0

Robotank

Surround

8
0
-8

Video Pinball

900K
600K
300K
0

Gamer Mean

Gamer Median Record Mean Cap

0

0

30
20
10
0

200M

Env steps
Env steps
Dreamer
MuZero
PPO

8
4
0

200M

Env steps

0.4
0.2
0

0

200M

Env steps

Figure 10: Atari learning curves.
25

Atari scores
Task
Environment steps
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Berzerk
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Defender
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
Hero
Ice Hockey
Jamesbond
Kangaroo
Krull
Kung Fu Master
Montezuma Revenge
Ms Pacman
Name This Game
Phoenix
Pitfall
Pong
Private Eye
Qbert
Riverraid
Road Runner
Robotank
Seaquest
Skiing
Solaris
Space Invaders
Star Gunner
Surround
Tennis
Time Pilot
Tutankham
Up N Down
Venture
Video Pinball
Wizard Of Wor
Yars Revenge
Zaxxon
Gamer median (%)
Gamer mean (%)
Record mean (%)
Record mean capped (%)

Random
—
228
6
222
210
719
12850
14
2360
364
124
23
0
2
2091
811
10780
2874
152
–19
0
–92
0
65
258
173
1027
–11
29
52
1598
258
0
307
2292
761
–229
–21
25
164
1338
12
2
68
–17098
1236
148
664
–10
–24
3568
11
533
0
16257
564
3093
32
0
0
0
0

Gamer
—
7128
1720
742
8503
47389
29028
753
37188
16926
2630
161
12
30
12017
7388
35829
18689
1971
–16
860
–39
30
4335
2412
3351
30826
1
303
3035
2666
22736
4753
6952
8049
7243
6464
15
69571
13455
17118
7845
12
42055
–4337
12327
1669
10250
6
–8
5229
168
11693
1188
17668
4756
54577
9173
100
100
13
13

Record
—
251916
104159
8647
1000000
10506650
10604840
82058
801000
999999
1057940
300
100
864
1301709
999999
219900
6010500
1556345
22
9500
71
38
454830
355040
162850
1000000
36
45550
1424600
104100
1000000
1219200
290090
25220
4014440
114000
21
101800
2400000
1000000
2038100
76
999999
–3272
111420
621535
77400
6
21
65300
5384
82840
38900
89218328
395300
15000105
83700
3716
123001
100
100

PPO
200M
5476
817
6673
47190
2479
539721
946
27816
7973
1186
118
98
299
51833
12667
93176
38270
8229
16
1887
43
33
1123
24792
3436
31967
12
1019
7769
9193
32335
2368
7041
19441
31412
–2
19
73
14554
14860
423995
63
1927
–29926
2368
3489
53439
6
–1
17250
225
83743
953
382306
10910
137164
13599
180
892
21
21

MuZero
200M
56835
1517
42742
879375
374146
1353617
1077
167412
201154
1698
133
100
799
774421
8945
184394
554492
142509
23
2369
58
0
17087
122025
10301
36063
26
14872
14380
11476
148936
0
51310
85331
105593
0
21
100
102129
137983
604083
70
399764
–30000
5860
3639
127417
9
0
427209
235
522962
0
775304
0
846061
58115
693
3054
66
34

Dreamer
200M
10977
3612
26010
441763
348684
1553222
1083
419653
37073
10557
250
100
384
554553
802698
193204
579875
142109
24
2166
82
34
41888
87600
12570
40677
57
24010
12229
69858
154893
1852
24079
77809
316606
0
20
26432
201084
48080
150402
132
356584
–29965
5851
15005
408961
9
–3
314947
395
614065
0
940631
99136
675774
78443
830
3381
74
38

Table 6: Atari scores.
26

Return

ProcGen learning curves

32
24
16
8
0

Bigfish

Return

Return

Climber
9
6
3
8
6
4
2

Return

Coinrun

10
8
6

Normalized Mean
0.6
0.5
0.3
0.2
0
0 25M 50M
Env steps

Dodgeball
9
6
3

Jumper

10
8
6
4
2

Leaper
8
6
4
2

8
6
4
2

12
8
4

Caveflyer
9
6
3

Heist

Miner

Return

Bossfight

12
9
6
3
0

Ninja

0

25M 50M
Env steps

Dreamer

25
20
15
10
5

8
6
4
2
24
18
12
6
0
8
6
4
2

Plunder

0

25M
Env steps

PPG

PPO

Chaser

Fruitbot

Maze

Starpilot
24
16
8
50M 0 0

25M 50M
Env steps

Figure 11: ProcGen learning curves.

27

ProcGen scores
Task

Original PPO

PPO

PPG

Dreamer

Environment steps

50M

50M

50M

50M

Bigfish
Bossfight
Caveflyer
Chaser
Climber
Coinrun
Dodgeball
Fruitbot
Heist
Jumper
Leaper
Maze
Miner
Ninja
Plunder
Starpilot

10.92
10.47
6.03
4.48
7.59
7.93
4.80
20.28
2.25
5.09
5.90
4.97
7.56
6.16
11.16
17.00

12.72
9.36
6.71
3.54
9.04
6.71
3.44
21.69
6.87
6.13
4.07
7.86
12.97
3.62
3.99
10.13

31.26
11.46
10.02
8.57
10.24
8.98
10.31
24.32
3.77
5.84
8.76
7.06
9.08
9.38
13.44
21.57

8.62
11.61
9.42
5.49
11.43
9.86
10.93
11.04
8.51
9.17
7.05
6.85
5.71
9.82
23.81
28.00

Normalized mean

41.16

42.80

64.89

66.01

Table 7: ProcGen scores. The PPO implementation we use throughout our paper under fixed
hyperparameters performs on par or better than the original PPO, which its authors describe as
highly tuned with near optimal hyperparameters 37 .

28

DMLab learning curves

Return

Return

150
100
50
0

Explore
Goals Large

Return

500
250
0

Return

Explore
Objects Large
60
40
20

Explore
Objects Small
90
60
30
0

45
30
15
0

Explore
Rewards Few

Explore
Explore
Explore
Language
Language
Rewards Many Obstructed Large Obstructed Small Quantitative
Execute
240
60
240
240
75
160
40
160
160
50
80
80
80
20
25
0
0
0
0
Language
Described

24
16
8
0

Language
Located

Natlab
Fixed Large
45
30
15

Psychlab
Continuous

Rooms
Keys Doors

Return

36
32
28
24

600
400
200
0

Lasertag
Three Small

45
30
15

Return

Return

450
300
150
0

Explore
Goals Small

Human Mean Cap
0.6
0.4
0.2
0 50M 100M
Env steps

45
30
15
0

Psychlab
Sequential

0
-0.2
-0.3

32
24
16

Lasertag
One Large

Natlab
Randomized

Psychlab
Visual Search
75
50
25
0

0.1
0
-0.1
-0.2
20
15
10
5

9
6
3
0

Lasertag
One Small

Natlab
Regrowth

Rooms
Collect Good

12
8
4
0
32
24
16
8

Lasertag
Three Large

Psychlab
Visumotor

Rooms
Exploit Deferred
40
30
20
10

Rooms
Rooms
Skymaze
Skymaze
Select Nonmatching Watermaze
Hard
Varied
32
75
75
75
24
50
50
50
16
25
25
25
8
0
0
0 50M 100M 0 50M 100M 0 50M 100M 0 50M 100M
Env steps
Env steps
Env steps
Env steps

Dreamer

IMPALA

PPO

Figure 12: DMLab learning curves.
29

DMLab scores
Task

R2D2+

IMPALA IMPALA IMPALA

PPO

Dreamer

Environment steps

10B

10B

1B

100M

100M

100M

Explore Goal Locations Large
Explore Goal Locations Small
Explore Object Locations Large
Explore Object Locations Small
Explore Object Rewards Few
Explore Object Rewards Many
Explore Obstructed Goals Large
Explore Obstructed Goals Small
Language Answer Quantitative Question
Language Execute Random Task
Language Select Described Object
Language Select Located Object
Lasertag One Opponent Large
Lasertag One Opponent Small
Lasertag Three Opponents Large
Lasertag Three Opponents Small
Natlab Fixed Large Map
Natlab Varying Map Randomized
Natlab Varying Map Regrowth
Psychlab Arbitrary Visuomotor Mapping
Psychlab Continuous Recognition
Psychlab Sequential Comparison
Psychlab Visual Search
Rooms Collect Good Objects Test
Rooms Exploit Deferred Effects Test
Rooms Keys Doors Puzzle
Rooms Select Nonmatching Object
Rooms Watermaze
Skymaze Irreversible Path Hard
Skymaze Irreversible Path Varied

174.7
460.7
60.6
83.7
80.7
75.8
95.5
311.9
344.4
497.4
617.6
772.8
0.0
31.8
28.6
49.0
60.6
42.4
24.6
33.1
30.0
30.0
79.9
9.9
38.1
46.2
63.6
49.0
76.0
76.0

316.0
482.0
91.0
100.4
92.6
89.4
102.0
372.0
362.0
465.4
664.0
731.4
0.0
0.0
32.2
57.2
63.4
47.0
34.0
38.4
28.6
29.6
80.0
10.0
62.2
54.6
39.0
47.0
80.0
100.0

137.8
302.8
55.1
75.9
46.9
68.5
57.9
214.9
304.7
140.8
618.4
413.0
0.0
0.0
10.4
37.1
53.8
40.5
25.5
16.5
30.0
0.0
0.0
9.9
37.6
36.9
63.2
50.1
46.4
69.8

64.2
196.1
34.3
50.6
34.6
53.3
23.7
118.0
1.0
–44.4
0.2
38.2
–0.1
–0.1
–0.1
12.1
13.0
35.3
15.3
11.9
30.1
0.0
0.0
9.3
34.5
24.2
4.0
23.6
7.7
32.7

21.2
115.1
22.5
38.9
19.0
25.5
12.9
70.4
0.3
–2.5
526.9
397.5
0.0
0.0
0.0
0.4
15.8
29.6
16.3
16.0
30.5
30.0
76.6
9.7
39.0
26.0
2.7
21.2
0.0
31.3

116.7
372.8
63.9
93.5
40.0
58.1
52.8
224.6
266.0
223.7
665.5
679.5
–0.1
0.0
9.0
18.8
50.5
31.2
16.7
30.7
33.8
44.3
40.1
9.9
40.4
42.4
73.1
26.1
80.9
87.7

Human mean capped (%)

85.4

85.1

66.3

31.0

35.9

71.4

Table 8: DMLab scores at 100M environment steps and larger budgets. The IMPALA agent
corresponds to “IMPALA (deep)” presented by Kapturowski et al. 35 who made the learning curves
available.

30

Atari100k learning curves

Return
Return

Chopper Command
Crazy Climber
120K
2.4K
80K
1.6K
40K
800
0
6K
4K
2K
0

Bank Heist

Frostbite

Kangaroo

Return

6K
4K
2K
0
0
-8
-16

Return

Return

600
400
200
0

Return

Amidar

Return

Alien

1.2K
800
400

1.2K
800
400
0

Pong

Seaquest

0

200K 400K
Env steps

120
80
40
0

Battle Zone

24K
16K
8K
0

Gopher

4.5K
3K
1.5K
0

1.2K
800
400
0

Demon Attack

Hero

15K
10K
5K
0

Kung Fu Master
30K
15K
0

Private Eye

4K
2K
0
180K
120K
60K
0

Boxing

90
60
30
0

Krull

7.5K
5K
2.5K

Assault

800
600
400
200

Qbert
3K
1.5K
0

Up N Down

0

200K 400K
Env steps
Dreamer

1.2
0.8
0.4
0

Gamer Mean

0

200K 400K
Env steps
PPO

Asterix

1.2K
800
400

Breakout

12
9
6
3

Freeway

6
3
0

Jamesbond

750
500
250
0

Ms Pacman

1.8K
1.2K
600

24K Road Runner
16K
8K
0
0.5
0.3
0.2
0

Gamer Median

0

200K 400K
Env steps

Figure 13: Atari100k learning curves.
31

Atari100k scores
Task
Environment steps
Alien
Amidar
Assault
Asterix
Bank Heist
Battle Zone
Boxing
Breakout
Chopper Command
Crazy Climber
Demon Attack
Freeway
Frostbite
Gopher
Hero
Jamesbond
Kangaroo
Krull
Kung Fu Master
Ms Pacman
Pong
Private Eye
Qbert
Road Runner
Seaquest
Up N Down
Gamer mean (%)
Gamer median (%)

Random Human
—
—
228
7128
6
1720
222
742
210
8503
14
753
2360
37188
0
12
2
30
811
7388
10780
35829
152
1971
0
30
65
4335
258
2412
1027
30826
29
303
52
3035
1598
2666
258
22736
307
6952
–21
15
25
69571
164
13455
12
7845
68
42055
533
11693
0
100
0
100

PPO
400K
276
26
327
292
14
2233
3
3
1005
14675
160
2
127
368
2596
41
55
3222
2090
366
–20
100
317
602
305
1502
11
2

SimPLe
400K
617
74
527
1128
34
4031
8
16
979
62584
208
17
237
597
2657
100
51
2205
14862
1480
13
35
1289
5641
683
3350
33
13

SPR
400K
842
180
566
962
345
14834
36
20
946
36700
518
19
1171
661
5859
366
3617
3682
14783
1318
–5
86
866
12213
558
10859
62
40

TWM
400K
675
122
683
1117
467
5068
78
20
1697
71820
350
24
1476
1675
7254
362
1240
6349
24555
1588
19
87
3331
9109
774
15982
96
51

IRIS
400K
420
143
1524
854
53
13074
70
84
1565
59324
2034
31
259
2236
7037
463
838
6616
21760
999
15
100
746
9615
661
3546
105
29

Dreamer
400K
1118
97
683
1062
398
20300
82
10
2222
86225
577
0
3377
2160
13354
540
2643
8171
25900
1521
–4
3238
2921
19230
962
46910
125
49

Table 9: Atari100k scores at 400K environment steps, corresponding to 100k agent steps.
Setting

SimPLe

EffMuZero

SPR

IRIS

TWM

Dreamer

Gamer score (%)
Gamer median (%)
GPU days

33
13
5.0

190
109
0.6

62
40
0.1

105
29
3.5

96
51
0.4

125
49
0.1

Online planning
Data augmentation
Non-uniform replay
Separate hparams
Increased resolution
Uses life information
Uses early resets
Separate eval episodes

—
—
—
—
—
—
—
X

X
—
X
—
X
X
X
X

—
X
X
—
X
X
—
X

—
—
—
X
—
X
X
X

—
—
X
—
—
X
—
X

—
—
—
—
—
—
—
—

Table 10: Evaluation protocols for the Atari 100k benchmark. Computational resources are
converted to A100 GPU days. EfficientMuZero 44 achieves the highest scores but changed the environment configuration from the standard 17 . IRIS uses a separate hyperparameter for its exploration
strength on Freeway.

32

Return

Cartpole Swingup Cartpole Sw. Sparse
Cheetah Run
800
750
600
600
500
400
400
250
200
200
0
0
0

Return

Return

Return

Acrobot Swingup
Ball In Cup Catch
Cartpole Balance Cartpole Bal. Sparse
1K
240
1K
900
180
800
750
600
120
600
500
300
60
250
400
0
0
0

Return

Proprioceptive control learning curves

Finger Turn Easy
Finger Turn Hard
Hopper Hop
1K
160
900
750
120
600
500
80
300
250
40
0
0
0
Pendulum Swingup
Reacher Easy
1K
1K
750
750
500
500
250
250
0
0
1K
750
500
250

Walker Stand

0

250K 500K
Env steps

1K
750
500
250
0

1K
750
500
250
0

Walker Run
600
400
200
0

Task Mean
600
400
200

250K 500K
Env steps
Dreamer
D4PG

1K
750
500
250
0

Hopper Stand

Reacher Hard

Walker Walk

0

Finger Spin

1K
750
500
250
0

0

250K 500K
Env steps
DMPO
PPO

800
600
400
200
0

Task Median

0

250K 500K
Env steps

Figure 14: DeepMind Control Suite learning curves under proprioceptive inputs.

33

Proprioceptive control scores
Task

PPO

DDPG

DMPO

D4PG

Dreamer

Environment steps

500K

500K

500K

500K

500K

Acrobot Swingup
Ball In Cup Catch
Cartpole Balance
Cartpole Balance Sparse
Cartpole Swingup
Cartpole Swingup Sparse
Cheetah Run
Finger Spin
Finger Turn Easy
Finger Turn Hard
Hopper Hop
Hopper Stand
Pendulum Swingup
Reacher Easy
Reacher Hard
Walker Run
Walker Stand
Walker Walk

6
632
523
930
240
7
82
18
281
106
0
3
1
494
288
31
159
64

100
917
997
992
864
703
596
775
499
313
36
484
767
934
949
561
965
952

103
968
999
999
860
438
650
769
620
495
68
549
834
961
968
493
975
942

124
968
999
974
875
752
624
823
612
421
80
762
759
960
937
616
947
969

134
962
990
990
852
491
614
931
793
889
113
576
788
954
938
649
964
936

Task mean
Task median

94
215

771
689

801
705

792
733

871
754

Table 11: DeepMind Control Suite scores under proprioceptive inputs.

34

Return

Cartpole Swingup Cartpole Sw. Sparse
Cheetah Run
800
800
750
600
600
500
400
400
250
200
200
0
0
0

Return

Return

Acrobot Swingup
Ball In Cup Catch
Cartpole Balance Cartpole Bal. Sparse
1K
1K
1K
240
750
750
800
160
500
500
600
80
250
250
400
0
0
0

Return

Visual control learning curves

Return

Reacher Hard

Return

1K
750
500
250
0

Finger Turn Easy
Finger Turn Hard
Hopper Hop
800
300
750
600
200
500
400
100
250
200
0
0
Pendulum Swingup
Quadruped Run
600
750
450
500
300
250
150
0
750
500
250
0
800
600
400
200

Task Mean

800
600
400
200
0

Walker Run

Finger Spin

1K
750
500
250
0

Hopper Stand

Quadruped Walk
Reacher Easy
1K
800
750
600
500
400
250
200
0
0
1K
750
500
250

Task Median

Walker Stand

0

Walker Walk

500K 1M
Env steps

1K
750
500
250
0

0

500K 1M
Env steps

800
600
400
200
0 500K 1M 0 0 500K 1M
Env steps
Env steps
Dreamer

DrQ-v2

CURL

PPO

Figure 15: DeepMind Control Suite learning curves under visual inputs.
35

Visual control scores
Task

PPO

SAC

CURL

DrQ-v2

Dreamer

Environment steps

1M

1M

1M

1M

1M

Acrobot Swingup
Ball In Cup Catch
Cartpole Balance
Cartpole Balance Sparse
Cartpole Swingup
Cartpole Swingup Sparse
Cheetah Run
Finger Spin
Finger Turn Easy
Finger Turn Hard
Hopper Hop
Hopper Stand
Pendulum Swingup
Quadruped Run
Quadruped Walk
Reacher Easy
Reacher Hard
Walker Run
Walker Stand
Walker Walk

3
829
516
881
290
1
95
118
253
79
0
4
1
88
112
487
94
30
161
87

4
176
937
956
706
149
20
291
200
94
0
5
592
54
49
67
7
27
143
40

4
970
980
999
771
373
502
880
340
231
164
777
413
149
121
689
472
360
486
822

166
928
992
987
863
773
716
862
525
247
221
903
843
450
726
944
670
539
978
768

229
972
993
964
861
759
836
589
878
904
227
903
744
617
811
951
862
684
976
961

Task mean
Task median

94
206

81
226

479
525

770
705

861
786

Table 12: DeepMind Control Suite scores under visual inputs.

36

BSuite performance spectrum

Exploration

Credit Assignment

Generalization
Basic
Memory
Noise

Scale

Dreamer
Boot DQN
DQN
PPO

Figure 16: BSuite scores visualized by category 48 . Dreamer exceeds previous methods in the
categories scale and memory. The scale category measure robustness to reward scales.

37

BSuite scores
Task

Random

PPO

AC-RNN

DQN

Boot DQN Dreamer

Bandit
Bandit Noise
Bandit Scale
Cartpole
Cartpole Noise
Cartpole Scale
Cartpole Swingup
Catch
Catch Noise
Catch Scale
Deep Sea
Deep Sea Stochastic
Discounting Chain
Memory Len
Memory Size
Mnist
Mnist Noise
Mnist Scale
Mountain Car
Mountain Car Noise
Mountain Car Scale
Umbrella Distract
Umbrella Length

0.00
0.00
0.00
0.04
0.04
0.04
0.00
0.00
0.00
0.00
0.00
0.00
0.20
0.00
0.00
0.05
0.05
0.05
0.10
0.10
0.10
0.00
0.00

0.38
0.61
0.39
0.84
0.77
0.83
0.00
0.91
0.54
0.90
0.00
0.00
0.24
0.17
0.47
0.77
0.41
0.76
0.10
0.10
0.10
1.00
0.87

1.00
0.63
0.60
0.40
0.20
0.12
0.00
0.87
0.27
0.17
0.00
0.00
0.39
0.70
0.29
0.56
0.22
0.09
0.10
0.10
0.10
0.09
0.43

0.93
0.71
0.74
0.85
0.82
0.72
0.00
0.92
0.58
0.85
0.00
0.00
0.25
0.04
0.00
0.85
0.38
0.49
0.93
0.89
0.85
0.30
0.39

0.98
0.80
0.83
0.69
0.69
0.65
0.15
0.99
0.68
0.65
1.00
0.90
0.22
0.04
0.00
0.85
0.34
0.31
0.93
0.82
0.56
0.26
0.39

0.96
0.75
0.78
0.93
0.93
0.92
0.03
0.96
0.53
0.94
0.00
0.00
0.40
0.65
0.59
0.61
0.34
0.55
0.92
0.87
0.90
0.74
0.78

Basic
Credit assignment
Exploration
Generalization
Memory
Noise
Scale

0.04
0.03
0.00
0.06
0.00
0.02
0.04

0.60
0.76
0.00
0.47
0.32
0.54
0.60

0.58
0.37
0.00
0.19
0.49
0.24
0.22

0.90
0.59
0.00
0.68
0.02
0.51
0.73

0.89
0.56
0.68
0.60
0.02
0.61
0.60

0.88
0.75
0.01
0.70
0.62
0.62
0.82

Task mean (%)
Category mean (%)

3
3

49
47

32
30

54
49

60
57

66
63

Table 13: BSuite scores for each task averaged over environment configurations, as well as aggregated performance by category and over all tasks.

38

Robustness ablations

Return

Atari
Atlantis
1.2M
800K
400K
0

0

10M

Atari
Breakout

20M

300
200
100
0

0

300
200
100
0

0

10M

20M

Return

Return

ProcGen
Bossfight
9
6
3
0

240
160
80
0

0

5M

10M

Visual Control
Acrobot Sparse

0

5M 10M
Env Steps

20M

DMLab
Nonmatching
50
25
0

8
6
4
2

120
80
40
0

0

10M

2K
1K
0

20M

1.8K
1.2K
600
0

ProcGen
Caveflyer

0

5M

0

10M

20M

PinPad
Five

0

5M

10M

Proprio Control
Dog Run

10M

Visual Control
Humanoid Run

300
150
0

0

100
75
50
25
00

4M
Normalized
Task Mean

Return (%)

Return

DMLab
Goals Small

10M

Atari
Montezuma

0

4M
Env Steps

50 100
Env Steps (%)

16
12
8
4

1.5K
1K
500
0

900
600
300
0

Crafter
Reward

0

2.5M

5M

PinPad
Six

0

5M

10M

Proprio Control
Reacher Hard

0

5M

10M

Dreamer
No obs symlog
No retnorm (advnorm)
No symexp twohot (Huber)
No KL balance & free bits
Without all

Figure 17: Individual learning curves for the robustness ablation experiment. All robustness
techniques contribute to the overall performance of Dreamer, although each individual technique
may only improve the performance on a subset of the tasks.

39

Learning signal ablations

Return

Atari
Atlantis
1.2M
800K
400K
0

0

20M

300
200
100
0

Atari
Breakout

0

300
200
100
0

9
6
3
0
300
200
100
0

0

20M
ProcGen
Bossfight

0

10M

DMLab
Nonmatching
60
40
20
0

8
6
4
2

Visual Control
Acrobot Sparse

0

Env Steps

10M

20M

0

20M

1.8K
1.2K
600
0

ProcGen
Caveflyer

0

0

10M

300
200
100
0

0

6M
Env Steps

Crafter
Reward

16
12
8
4

20M

0

PinPad
Five

0

0

100
75
50
25
00

5M
PinPad
Six

10M

Proprio Control
Dog Run

Visual Control
Humanoid Run
75
50
25
0

2.4K
1.6K
800
0

6M

1K
500
0

900
600
300
0

0

10M

Proprio Control
Reacher Hard

0

10M

Normalized
Task Mean

Return (%)

Return

Return

Return

DMLab
Goals Small

Atari
Montezuma

Dreamer
No reward & value grads
No reconstruction grads

100
Env Steps (%)

Figure 18: Individual learning curves for the learning signal ablation experiment. Dreamer relies
predominantly on the undersupervised reconstruction objective of its world model and additional
reward and value gradients further improve performance on a subset of tasks.

40

