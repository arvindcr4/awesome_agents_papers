arXiv:2311.01753v2 [cs.MA] 21 Mar 2024

RiskQ: Risk-sensitive Multi-Agent Reinforcement
Learning Value Factorization

Siqi Shenab , Chennan Maab , Chao Liab , Weiquan Liuab ,
Yongquan Fuc∗, Songzhu Meic , Xinwang Liuc , Cheng Wangab
a
Fujian Key Laboratory of Sensing and Computing for Smart Cities,
School of Informatics, Xiamen University (XMU), China
b
Key Laboratory of Multimedia Trusted Perception and Efficient Computing, XMU, China
c
School of Computer, National University of Defense Technology, China
{siqishen,cwang}@xmu.edu.cn, {yongquanf,xinwangliu}@nudt.edu.cn
{chennanma,chaoli}@stu.xmu.edu.cn

Abstract
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the
context of Multi-Agent Reinforcement Learning (MARL), learning coordinated
and decentralized policies that are sensitive to risk is challenging. To formulate the
coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive
Individual-Global-Max (RIGM) principle as a generalization of the IndividualGlobal-Max (IGM) and Distributional IGM (DIGM) principles. This principle
requires that the collection of risk-sensitive action selections of each agent should
be equivalent to the risk-sensitive action selection of the central policy. Current
MARL value factorization methods do not satisfy the RIGM principle for common
risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements.
Therefore, we propose RiskQ to address this limitation, which models the joint
return distribution by modeling quantiles of it as weighted quantile mixtures of
per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the
VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in
https://github.com/xmu-rl-3dv/RiskQ.

1

Introduction

In cooperative multi-agent reinforcement learning (MARL) [1], it is important to learn coordinated
agent policies to achieve a common goal. However, achieving this goal is challenging due to random
rewards, environmental uncertainty, and varying policies among agents. Especially, for scenarios
with partial-observability, high-stochastic rewards and state-transitions [2]. In order to efficiently
learn MARL policies, many researchers have adopted the centralized training with decentralized
execution (CTDE) [3] paradigm, which offers advantages in terms of learning speed and performance.
A popular subset of the CTDE methods is the value factorization category [4, 5, 6, 7, 8].
To learn coordinated policies, value factorization algorithms must ensure that the global argmax
operator performed on the global state-action value function yields the same outcome as a set of
individual argmax operations performed on per-agent utilities. This requirement for coordination
is known as the individual-global-max (IGM) principle [6]. The IGM principle takes only the
expected return into account, but not the entire distribution of returns that includes potential outcome
∗

Corresponding author

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

events. A method that learns the expected return return may fail in high-stochastic environments
with extremely high/low rewards but at low probabilities. For example, users may seek for a big win
with low probability in finance or avoid suffering from a huge loss on rare occasions in autonomous
driving [9, 10]. Risk refers to the uncertainty of future outcomes in multi-agent systems. By
making decisions based on risk, agents can address uncertainty better. Most of the existing MARL
value factorization methods do not extensively consider risk, which could impact their performance
negatively.
Recently, risk-sensitive reinforcement learning (RL) has achieved significant progress in the single
agent domain [11, 12]. Instead of optimizing the expectation of return, risk-sensitive RL optimizes
a risk measure based on a return distribution. Risk-sensitive value factorization methods should be
designed to learn risk-sensitive decentralized policies that are fully consistent with the risk-sensitive
centralized counterpart. In order to learn coordinated risk-sensitive policies, the IGM principle needs
to be adapted to address cases where expectations are not the only factor. Despite there are a few
approaches [13, 14] combing risk-sensitive RL with MARL, how to effectively combine risk-sensitive
reinforcement learning with MARL value factorization is still an open question.
In this work, we formulate the coordination requirement in risk-sensitive MARL as the Risk-sensitive
Individual-Global-Max (RIGM) principle. The principle requires that the optimal joint risk-sensitive
action should be equivalent to the collection of each agent’s greedy risk-sensitive actions. The
RIGM principle is a generalization of the IGM and the distributional IGM (DIGM) principle. Albeit
multiple value factorization methods [8, 7, 15] have been proposed to learn policies satisfying the
IGM or the DIGM principles, they cannot guarantee the RIGM principle. DRIMA [14] combines
risk-sensitive RL with a value factorization method. However, it does not guarantee the RIGM
principle for distorted risk measures [9]. RMIX [13] learns risk-sensitive policies which satisfy
the RIGM principle only if the risk metric is the expectation operator. It is unclear how to learn
coordinated risk-sensitive decentralized policies which satisfy the RIGM principle for general risk
measures.
We build, RiskQ, a risk-sensitive MARL value factorization algorithm that satisfies the RIGM
principle for risk metrics ψα , such as VaR (i.e., percentile) and distorted risk measurements [9],
where α is a risk parameter. In RiskQ, each agent acts greedily according to a risk value defined as
ψα [Zi ], where Zi is a per-agent return distribution utility. RiskQ models the joint return distribution
Zjt by combining per-agent return distribution utilities [Zi ]ni=1 with an attention-based mechanism.
Specifically, the quantiles θ(ω) of the return distribution Zjt is modeled as the weighted sum of the
quantiles θi (ω) of return distribution utilities, where ω is a quantile sample.
For evaluation, we conduct extensive experiments on risk-sensitive games and the StarCraft II
MARL tasks [16]. The experimental results show that RiskQ can obtain promising results in both
risk-sensitive and risk-neutral scenarios.

2

Background

2.1

Dec-POMDPs

We consider cooperative multi-agent reinforcement learning (MARL) scenarios which can be modeled
as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) [17], represented
N
N
as tuple G = ⟨S, {Ui }N
i=1 , P, r, {Oi }i=1 , {σi }i=1 , N, γ⟩ for N agents.
S is a finite set of states, and Ui is the set of discrete actions available to agent i. At state st , a
joint action of all agents is defined as ut ∈ U N = U1 × . . . × UN , with t representing the discrete
time step. After performing ut , the environment transitions to a new state st+1 ∈ S, following
the transition function st+1 ∼ P (·|st , ut ), and each agent receives a reward rt as a result of the
state transition. Due to the partial observability of the environment, each agent i receives a local
observation oti ∈ Oi , which is drawn from oti ∼ σ i (·|st ). The discounting factor is denoted by
γ ∈ [0, 1). Each agent i maintains a local action-observation history τi = (Oi × Ui )∗ , where ∗
represents 0 to T ( T denotes the time step). The global action-observation history is denoted as
τ ∈ T N := τ1 × . . . × τN . Each agent acts according to policy πi (ui |τi ), and the joint policy can be
represented as π =< π1 , . . . , πN >.
2

2.2

Value Function Factorization

For Dec-POMDPs, value function factorization methods learn factorized utility which can be used
for the execution of individual agent. The Individual-Global-Max (IGM) principle proposed in [6] is
important for the realization of value function factorization for MARL. It is defined as follows.
Definition 1 (IGM). For a joint state-action value function Qjt : T N × U N 7→ R, where τ ∈ T N
is a joint action-observation history and u is the joint action, if there exists individual state-action
functions [Qi : Ti × Ui 7→ R]N
i=1 , such that the following conditions are satisfied
arg max Qjt (τ , u) = (arg max Q1 (τ1 , u1 ), . . . , arg max QN (τN , uN )),
u

u1

un

(1)

N
then, [Qi ]N
i=1 satisfy IGM for Qjt under τ . We can state that Qjt (τ , u) is factorized by [Qi (τi , ui )]i=1 .

2.3

Distributional RL and Risk

MARL is highly stochastic, and distributional RL could be used to deal with such stochasticity.
Distributional RL [18, 19, 20, 21] models the return distribution of state-action pair through Z(τ , u)
explicitly. They model full return distribution Z(τ , u) instead of return expectation Q(τ , u). The
distribution of return can be approximated through a categorical distribution [18] or a quantile
function [19, 20].
The state-action return distribution Z(τ , u) can be modelled using quantile functions θ of a random
variable Z, which is defined as follows.
θZ (τ , u, ω) = inf{z ∈ R : ω ≤ CDFZ (z)},

∀ω ∈ [0, 1]

(2)

where CDFZ (z) is the cumulative distribution function of Z(τ , u). The quantile function θZ (ω) may
be referred as generalized inverse CDF in other literature [22]. For notation simplicity, we denote
θZ (ω) as θ(ω).
QR-DQN [19] and IQN [20] model the return function Z(τ, u) as a mixture of n Dirac functions.
n
X
Z(τ, u) =
pi (τ, u, ωi )δθ(τ ,u,ωi )
(3)
i=1

where δθ(τ ,u,ωi ) is a Dirac Delta function whose value is θ(τ , u, ωi ). ωi is a quantile sample.
pi (τ, u, ωi ) is the corresponding probability of θ(τ , u, ωi ). For QR-DQN [19], pi (τ, u, ωi ) can be
simplified as 1/n. For execution, the action with the largest expected return arg maxu E[Z(τ , u)] is
chosen.
Analogous to the IGM principle for value function factorization, the Distributional Individual-GlobalMax (DIGM) principle for value distribution proposed in [22] is defined as follows.
Definition 2 (DIGM). Given a set of individual state-action return distribution utilities [Zi (τi , ui )]N
i=1
and a joint state-action return distribution Zjt (τ , u), if the following conditions are satisfied
arg max E[Zjt (τ , u)] = (arg max E[Z1 (τ1 , u1 )], . . . , arg max E[ZN (τN , uN )]),
u

u1

uN

(4)

then, [Zi (τi , ui )]N
i=1 satisfy DIGM for Zjt under τ . We can state that Zjt (τ , u) is distributionally
factorized by [Zi (τi , ui )]N
i=1 .
In this work, we use ψα to measure the risk from a return distribution Z, where α is the risk level.
For example, the Value at Risk metric, VaRα , estimate the α-percentile from a distribution. For
VaRα , a small value for α indicates risk-averse setting, whereas a high value for α means risk-seeking
scenarios. Risk-sensitive policies act with a risk measure ψα .
Definition 3 (Value at Risk (VaR)). Value at Risk (VaR) [23] is a popular risk metric which measures
risk as the minimal reward might be occur given a confidence level α. For a random variable Z with
cumulative distribution function (CDF), the quantile function θ and a quantile sample α ∈ [0, 1],
V aRα (Z(τ , u)) = θ(τ , u, α). This metric is called percentile as well.
Definition 4 (Distortion risk measures (DRM)). Distorted risk measures are weighted expectation of
a distribution under a distortion function [9, 10, 24]. The distorted expectation of a random variable
Z under g is defined as
Z 1
ψ(Z) =
g ′ (ω)θ(ω)dω
(5)
0

3

where g(ω) ∈ [0, 1], g ′ (ω) is the derivative of g(ω). There are many distortion functions which can
reflect different risk preferences, such as CVaR[25], Wang[26], and CPW[27].
Definition 5 (Conditional Value at Risk(CVaR)).
CV aRα (Z) = EZ [z|z ≤ θ(α)]

(6)

where α is the confidence level (risk level), θ(α) is the quantile function (inverse CDF) defined
in (A.1). CVaR is the expectation of values z that are less equal than the α-quantile value (θ(α)) of
the value distribution. CVaR is a DRM whose g(ω) = min(ω/α, 1).
Please refer to the Appendix A.1 for the detailed definitions of more distortion risk measures.

3

Related Work

3.1

Value Factorization

To enable efficient decentralized execution of MARL policies, value factorization approaches are
widely adopted in MARL [28, 6, 15]. Most methods focus on the satisfaction of the IGM principle
which is important for value factorization. VDN [4] factorizes the value function as the sum of peragents’ utilities. QMIX [5] models monotonic relationships between individual utilities and the value
function. QAtten [29] and REFIL [30] use attention mechanisms for value function factorization.
QTran [6] transforms the joint state-action value function Qjt into an easy-to-factorize form through
linear constraints. QPlex [7] decomposes a value function into a value part and an advantage part.
ResQ [8] transforms a value function into the combination of a main and a residual function through
masking.
For distributional MARL, the DFAC framework [22] and ResZ [8] satisfy the DIGM principle which
is the IGM principle for distributional RL. The DFAC framework factorizes a return distribution
through mean-shape decomposition which models the mean and shape of the return distribution
separately. ResZ transforms a return distribution into the combination of a main and a residual return
distribution. We will show that the DFAC framework and ResZ can not guarantee adherence to the
RIGM principle for the VaR risk metric.
3.2

Risk-sensitive RL

Many researchers have dedicated themselves to studying risk-sensitive reinforcement learning in
single-agent settings [11, 12, 31, 32, 33, 24, 34]. O-RAAC [35] learns a full return distribution for
its critic and optimizes the actor’s policy according to a risk related metric (such as CVaR). [36]
proposes a distributional reinforcement learning algorithm for learning CVaR-optimized policies.
Recently, risk-sensitive reinforcement learning has been adopted in MARL, such as [37, 13].
RMIX [13] combines QMIX and CVaR-optimized agent policies. It learns a value function (rather
than a return distribution) for each state-action pair, which is further decomposed into per-agent’s
return distribution utilities. Because the reward for each agent is unknown in cooperative MARL,
RMIX learns agents’ utilities by using CVaR value as pseudo rewards. RMIX satisfies the RIGM
principle only when the risk metric is CVaR and the risk level α is set to 1.
DRIMA [14] separates the sources of risk into cooperation risk and environmental risk. It models
joint return distribution as a monotonic mixing of per-agent return distribution utilities. We will show
in Sec. 4 that DRIMA does not satisfy the RIGM principle for the CVaR metric.
RiskQ can be used with other MARL-based approaches: communication approaches (COMNet [38],
GraphComm [39], DIAL [40], COPA [41]), actor-critic methods (MADDPG [42], MAAC [43]), and
other approaches such as MAPPO [44], MASER [45], QRelation [46], ATM [47], and UPDet [48].

4

Risk-sensitive Value Factorization

In cooperative MARL, it is crucial to learn decentralized policies consistent with a centralized policy
that is conditioned on joint state and joint action. Especially in MARL scenarios with high-stochastic
rewards and state transitions, taking risk into consideration is of great importance. However, it is
unclear about how to coordinate agents’ policies with the consideration of risk.
4

Key to our approach is the insight that to coordinate agents with risk consideration is important to
learn risk-sensitive decentralised policies that are fully consistent with the risk-sensitive centralised
counterpart. To ensure this consistency, MARL algorithms only need to ensure that a joint risksensitive argmax operation, when performed on the joint state-action value function, yields the same
outcome as a collection of individual risk-sensitive argmax operations conducted on per-agent utilities.
This insight leads to the following definition.
4.1

Risk-sensitive Individual-Global-Max (RIGM) Principle

Definition 6 (RIGM). Given a risk metric ψα , a set of individual return distribution utilities
[Zi (τi , ui )]N
i=1 , and a joint state-action return distribution Zjt (τ , u), if the following conditions are
satisfied:
arg max ψα [Zjt (τ , u)] = (arg max[ψα [Z1 (τ1 , u1 )], . . . , arg max[ψα [ZN (τN , uN )]),
u

u1

uN

(7)

where ψα : Z × R → R is a risk metric such as the VaR or a distorted risk measure, α is its risk
level. Then, [Zi (τi , ui )]N
i=1 satisfy the RIGM principle with risk metric ψα for Zjt under under τ . We
can state that Zjt (τ , u) can be distributionally factorized by [Zi (τi , ui )]N
i=1 with risk metric ψα .
The RIGM principle is a generalization of the DIGM and the IGM principle. The DIGM principle is
a special case of the RIGM theorem for ψ = CVaR and α = 1 (the expectation operator E is equal to
CVaR1 ). If Zi is a single Dirac Delta Distribution, then the return distribution Zi becomes a single
value(i.e., Qi ), and in this case, the IGM principle is equivalent to the RIGM principle when ψ =
CVaR and α = 1.
In the following, we discuss whether existing risk-neutral value factorization methods can be simply
modified to satisfy the RIGM principle for ψα , and then discuss limitations of existing risk-sensitive
value factorization methods. There are many value factorization methods satisfying the IGM principle.
We show in Theorem 1 that simply replacing Qi with Zi is insufficient to guarantee that [Zi ]N
i=1
satisfy RIGM with risk metric ψα .
Theorem 1. Given a deterministic joint state-action value function Qjt , a joint state-action return
distribution Zjt , and a factorization function Φ for deterministic utilities:
Qjt (τ, u) = Φ(Q1 (τ1 , u1 ), ..., QN (τN , uN ))

(8)

such that [Qi ]N
i=1 satisfy IGM for Qjt under τ , the following risk-sensitive distributional factorization:
Zjt (τ, u) = Φ(Z1 (τ1 , u1 ), ..., ZN (τN , uN ))

(9)

is insufficient to guarantee that [Zi ]N
i=1 satisfy RIGM for Zjt (τ, u) with risk metric ψα .
We show that factorization methods satisfying the DIGM principle cannot guarantee the satisfaction
of the RIGM theorem for the VaR metric.
Theorem 2. Given a joint state-action return distribution Zjt , and a distributional factorization
function Φ for the return distribution utilities [Zi ]N
i=1 which satisfy the DIGM theorem, the following
risk-sensitive distributional factorization:
Zjt (τ, u) = Φ(Z1 (τ1 , u1 ), ..., ZN (τN , uN ))

(10)

is insufficient to guarantee that [Zi ]N
i=1 satisfy the RIGM principle for Zjt (τ, u) with risk metric ψα .
Recently, DRIMA [14] and RMIX [13] combine risk-sensitive RL with MARL. Albeit they have
demonstrated promising results, they do not explicitly consider the risk-sensitive coordination
requirement.
Theorem 3. DRIMA [14] does not guarantee adherence to the RIGM principle for CVaR metric.
The value function Qjt (τ , u), learned by RMIX [13], can be written as Qjt (τ , u) =
Qmix (ψα [Z1 (τ1 , u1 )], ..., ψα [ZN (τN , uN )]), where Zi (τi , ui ) represents per-agent return distribution utilities, and Qmix is a monotonically increasing function with respect to ψα [Zi (τi , ui )].
Although it seems that it satisfies the RIGM principle for any risk metric ψα , its learning algorithm seeks to find the optimal actions that arg maxu [Qjt (τ , u)] rather than
arg maxu ψα [Zjt (τ , u)]. In essence, RMIX seeks to make sure that arg maxu [Qjt (τ , u)] equal to
5

(arg maxu1 [ψα [Z1 (τ1 , u1 )], . . . , arg maxuN [ψα [ZN (τN , uN )]). By this way, RMIX satisfies the
RIGM principle only if ψα = CVaR1 . Moreover, Qjt (τ , u) is only a value function but not a return
distribution.
Please refer to Appendix B.1 for detailed proofs of Theorem 1, 2 and 3. We have discussed the
limitations of existing value factorization methods. It becomes apparent that the development of a
novel factorization method is needed, specifically one capable of effectively coordinating agents in
risk-sensitive scenarios.
4.2

RiskQ

In MARL, it is typical to model the factorization function f as either the sum of per-agent utilities or a
monotonic increasing function with respect to per-agent utilities. However, risk metrics are generally
non-additive, except for variables which are highly dependent on each other (the comonotonicity
property)
P [49, 10]. For instance, let’s consider the VaR0.5 metric, the median of a distribution, and
f =
Zi . It generally holds that VaR0.5 [Z1 + Z2 ] ̸= VaR0.5 [Z1 ]+VaR0.5 [Z2 ]. This is due to the
fact that the median of the sum of two random variables does not equate to the sum of their medians.
The non-additive property of risk metrics makes the explicit modeling of f challenging, as we cannot
model f as the sum or the monotonic mixing of per-agents’ utilities.
The key to our insights is that instead of modeling the return distribution Zjt using the form of
f (Z1 , ...Zn ) explicitly which is a common practice in MARL [4, 5, 8], we can model Zjt implicitly
through modeling it using its quantiles. We model the relationships among quantiles of the joint
return distribution and the quantiles of per-agent return distribution utilities.
RiskQ utilizes a common distributional RL technique [19], where the return distribution Zjt (τ , u) is
represented by a combination of Dirac delta functions δθ(ω) and the positions θ(ω) of the Diracs that
are determined through quantile regression. Figure 1 depicts the mixing process of the quantiles of
per-agent utilities. For a quantile sample (cumulative probability) ω, the quantile value θ(τ , u, ω)
PN
of Zjt (τ , u) is represented as the weighted sum of the quantile value i=1 ki θi (τ, ui , ω) of return
distribution utilities.
We demonstrate through the following theorem that Zjt and [Zi ]N
i=1 satisfy the RIGM principle for
both the VaR risk metric and distorted risk measures (DRM).
Theorem 4. A joint state-action return distribution
Zjt (τ , u) =

J
X

pj (τ , u, ωj )δθ(τ ,u,ωj )

(11)

ki θi (τi , ui , ωj )

(12)

j=1

θ(τ , u, ωj ) =

N
X
i=1

is distributional factorized by [Zi (τi , ui )]N
i=1 with risk metric ψα , where J is the number of Dirac
Delta functions, δθ(τ ,u,ωj ) is a Dirac Delta function at θ(τ , u, ωj ) ∈ R, θ(τ , u, ωj ) is a quantile
function with sample ωj , pj (τ , u, ωj ) is the corresponding probability for δθ(τ ,u,ωj ) of the estimated
return distribution Zjt , θi (τi , ui , ω) is the quantile function (with quantile sample ω) for the return
distribution utility Zi (τi , ui ) of agent i and ki ≥ 0.
We have shown that by modeling the quantiles of Zjt as a weighted sum of quantiles of [Zi ]N
i=1 .
[Zi ]N
satisfied
the
RIGM
theorem
for
VaR
and
DRMs
such
as
CVaR,
Wang,
and
CPW.
Detailed
i=1
proofs supporting this theorem can be found in Appendix B.2.
4.3

Neural Networks and Loss

We model the return distribution utility for each agent by a simple neural network. It takes the
observation history τi and action ui as input, then passes them through a MLP, a GRU, and a MLP,
and outputs Zi (τi , ui , ω) for each quantile sample ω. For execution, the action which maximizes
ψα [Zi (τi , ui )] is chosen.
6

RiskQ

…

( , )
MLP

MLP

GRU

…
=∑

=

…

( )

(

)

Multi-head
Attention

MLP

=∑

,
(b)

( ,

(a)

)

( ,

)

(c)

Figure 1: RiskQ overview: (a) quantiles mixing for Zjt , (b) mixer function, (c) agent return distribution utility

The mixer function mixes all the quantile θ of return distribution utility [Zi ]N
i=1 into θ(τ , u, ω) =
PN
N
k
θ
(τ
,
u
,
ω).
It
takes
[θ
(τ
,
u
,
ω)]
,
the
state
s,
the
observation
history
τ as input, and
i
i i
i
i=1
i=1 i i i
outputs θ(τ , u, ω) for each quantile sample ω. ki is modelled using a multi-head attention mechanism.
We adopt the Quantile Regression (QR) loss [19] to update the value distribution Zjt (τ k , uk , σ).
More concretely, QR aims to estimate the quantiles of the return distribution by minimizing the
e , σ − ).
distance between Zjt (τ k , uk , σ) and its target distribution y k (τ k , uk , σ) ≜ r + γZjt (τ k+1 , u
k+1
e = [e
ei = arg maxui ψα [Zi (τi , ui )]. σ are the parameters of the network, and σ − are the
u
ui ]N
i=1 , u
parameters of the target network.

5

Evaluation

We study the performance of RiskQ on risk-sensitive games (Multi-agent cliff and Car following
games), the StarCraft II Multi-Agent Challenge benchmark (SMAC) [16]. RiskQ can obtain promising
performance for risk-sensitive and risk-neutral scenarios. Ablation studies reveal the importance of
adhering to the RIGM principle for achieving good performance. Additionally, we have examined
the impact of functional representations, risk metrics and risk levels.
5.1

Experimental Setup

We select three categories of MARL value factorization methods for comparison: (i) Expected value
factorization methods: QMIX [5], QTran [6], QPlex [7], CW QMIX [15], ResQ [8]; (ii) Risk-neutral
return distribution (stochastic value) factorization methods: DMIX [22] and ResZ [8]; (iii) Risksensitive return distribution factorization methods: RMIX [13] and DRIMA [14]. For robustness, each
experiment is conducted at least 5 times with different random seeds. In general, the configuration of
RiskQ follows the setup of Weighted QMIX and ResQ. By default, the risk metric used in RiskQ is
Wang0.75 , indicating a risk-averse preference. Please refer to Appendix C for detailed experimental
setup and more experimental results.
5.2

Multi-Agent Cliff Navigation

In Multi-Agent Cliff Navigation (MACN), introduced in [13], two agents must navigate in a grid-like
world to reach the goal without falling into cliff. The agent receives a -1 reward at each time step. If
any agent reaches the goal individually, a -0.5 reward will be given to the agents. They will receive 0
reward if they reach the goal together. An episode is finished once the goal is reached by two agents
together or any agent falls into cliff (reward -100). We depict the test return of each learning algorithm
in two MACN scenarios: 4 × 11 grid map and 4 × 15 grid map. As illustrated in Figure 2 (a) and
(b), RiskQ achieves the optimal performance in both scenarios, outperforming the two risk-sensitive
methods: RMIX and DRIMA. This indicates that learning policies which satisfy the RIGM principle
could lead to promising results.
5.3

Multi-Agent Car Following game

We design Multi-Agent Car Following (MACF) Game, which is adapted from the single agent
risk-sensitive environment [35]. In MACF, there are two agents, each controlling one car, with the
7

RiskQ

10

20

20

30
40

RMIX

(c) MACF

40

300K

Environmental Steps

400K

60

500K

(d) 3s_vs_5z (random)

Test Win Rate

0.6
0.4
0.2
800K

1.2M

1.6M

Environmental Steps

2M

400K

600K

Environmental Steps

800K

1M

0.6
0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

2M

400K

600K

Environmental Steps

800K

1M

1.6M

2M

(f) 3s_vs_5z (dilemmatic)

1.0

0.8

0.0

200K

(e) 3s_vs_5z (explorative)

1.0

0.8

400K

200K

Test Win Rate

200K

DRIMA

0
2000
4000
6000
8000
10000
12000
14000

50
100K

1.0

Test Win Rate

ResZ

30

50

0.0

DMIX

(b) MACN (4 x 15)

Return

10

60

QPLEX

0

Return

Return

QMIX

(a) MACN (4 x 11)

0

0.8
0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

Figure 2: The return of the MACN (a-b) and the MACF (c) environment; the test win rate of random (d),
explorative (e) and dilemmatic (f) 3s_vs_5z scenario of SMAC.
QPLEX

0.6
0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

(d) 2s_vs_1sc

Test Win Rate

Test Win Rate

0.8
0.6
0.4
0.2
0.0

200K

400K

600K

Environmental Steps

800K

1M

DMIX

0.4
0.2
200K

400K

600K

Environmental Steps

800K

(e) 3s_vs_5z

0.4
0.2
400K

800K

1.2M

Environmental Steps

0.6
0.4
0.2
400K

1.6M

2M

800K

1.2M

Environmental Steps

1.6M

2M

1.6M

2M

(f) 27m_vs_30m

1.0

0.6

DRIMA

(c) 2c_vs_64zg

0.8

0.0

1M

0.8

0.0

RMIX

1.0

0.6

1.0

1.0

ResZ

0.8

0.0

2M

ResQ

(b) MMM

Test Win Rate

0.8

0.0

CW QMIX
1.0

Test Win Rate

Test Win Rate

QMIX

(a) MMM2

Test Win Rate

RiskQ
1.0

0.8
0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

Figure 3: Win Rate of the StarCraft Multi-Agent Scenarios.

task of one car following the other to reach a goal. Each car can observe the current position and
speed of other cars within its observation range. The agent has a fixed action space which determines
its acceleration. At each time step, agent will receive a negative reward. And the cars will crash
with some probability if their speed exceed a speed-threshold, and a negative reward is given to the
agents. To adapt the game to cooperative MARL, agents move within each other’s observation will
receive a positive reward. Once the agents reach the goal together, a big reward is given to them
and the episode is terminated. As shown in Figure 2 (c), RiskQ exhibits superior performance to
both risk-neutral and risk-sensitive algorithms. Moreover, in order to verify that this performance
improvement is due to risk considerations, we also study the number of crashes. For this value, as it
is shown in the appendix (Figure 7), albeit RiskQ does not optimize for it, RiskQ achieves zero crash
with the fastest learning rate.
5.4

StarCraft II

SMAC is a well-known benchmark which comprises two teams of agents engaging in combat
scenarios. Following the evaluation protocol of DRIMA [14] for risk-averse SMAC, we first study
the performance of RiskQ in explorative, dilemmatic and random settings for the 3s_vs_5z scenario.
In the explorative setting, agents behave heavily exploratory during training, thus they must consider
the risk brought on by heavily exploration of other agents. In the dilemmatic setting, agents have
increased exploratory behaviors and they are punished by their decreased heath. In this setting,
learning algorithms should consider risk to prevent the learning of locally optimal policies. In the
random setting, one agent performs random actions 50% of the time during testing. As depicted in
Figure 2 (d-f), RiskQ obtains the best performance. Please refer to Figure 9 in Appendix C.4 for more
8

0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

30
RiskQ
RiskQ-sum
RiskQ-Qi
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

Test Win Rate

1.0
0.8

200K

400K

600K

Environmental Steps

800K

1M

(g) 2c_vs_64zg

0.4
0.2
400K

800K

1.2M

Environmental Steps

400K

1.6M

2M

800K

1.2M

Environmental Steps

1.6M

(e) 3s_vs_5z

RiskQ
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

0.2

0.8
0.6

400K

800K

1.2M

Environmental Steps

1.6M

(h) 2c_vs_64zg

0.2
400K

800K

1.2M

0.2

Environmental Steps

0.8
0.6

1.6M

2M

400K

800K

1.2M

Environmental Steps

1.6M

2M

1.6M

2M

1.6M

2M

(f) MMM2
RiskQ
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

0.4
0.2

1.0

RiskQ-Wang0.75
RiskQ-Wang0.55
RiskQ-Wang0.65
RiskQ-Wang0.85
RiskQ-Wang0.95

(c) MMM2
RiskQ
RiskQ-sum
RiskQ-Qi
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

0.4

0.0

2M

0.4

0.0

0.6

1.0

0.6
0.4

0.8

0.0

2M

0.8

1.0

RiskQ-Wang0.75
RiskQ-CVaR0.2
RiskQ-CVaR0.3
RiskQ-VaR0.25
RiskQ-CPW0.71

0.6

0.0

RiskQ
RiskQ-sum
RiskQ-Qi
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

0.2

0.0

Test Win Rate

60

Test Win Rate

Return

20

50

0.4

1.0

10

40

0.6

0.0

(d) MACN (4 x 15)

0

0.8

Test Win Rate

0.4

1.0

Test Win Rate

0.6

(b) 3s_vs_5z

1.0

Test Win Rate

0.8

(a) 2c_vs_64zg
RiskQ
RiskQ-sum
RiskQ-Qi
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

Test Win Rate

Test Win Rate

1.0

0.8

400K

800K

1.2M

Environmental Steps

(i) MMM2
RiskQ-4-quantiles
RiskQ-8-quantiles
RiskQ-16-quantiles
RiskQ-32-quantiles
RiskQ-64-quantiles

0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

Figure 4: Ablation Study: (a-d) impact of not satisfying RIGM and different design of RiskQ mixer; (e-f) impact
of functional representation limitations; (g-i) impact of different risk metrics, risk levels, and number of quantiles

results in these risk-sensitive SMAC scenarios. Combining previous results from the MACN and the
MACF environments, we can conclude that RiskQ can yield promising results in environments that
require risk-sensitive cooperation.
Then we evaluate the performance of RiskQ in the standard SMAC setting. As demonstrated in
Figure 3 (a) and (b), RiskQ achieves the best performance in the MMM2 scenarios. In MMM2,
RMIX achieves the second best results in the end. This demonstrates that it is important to consider
risk in highly stochastic environments. RiskQ are among the best performing algorithms in the MMM
scenario. Notably, RiskQ satisfies the RIGM principle for DRMs, further suggesting the necessity of
coordinated risk-sensitive cooperation.
The test win rates for the 2c_vs_64zg and 2s_vs_1sc scenarios are shown in Figure 3 (c) and (d). For
the 2c_vs_64zg scenario, RiskQ and ResZ are the best performing algorithms, while RMIX performs
poorly in this scenario. Albeit DRIMA matches the performance of RiskQ in the end, its learning
speed is much slower than that of RiskQ. For the 2s_vs_1sc scenario, RiskQ, ResQ and ResZ achieve
optimal performance, with RiskQ achieving near-optimal performance merely after 0.3 million steps.
Furthermore, DRIMA learns slower than RiskQ, and the performance of RMIX is unstable.
For the 3s_vs_5z scenario, as illustrated in Figure 3 (e), RiskQ achieves the optimal performance after
only 1.2 million training steps. The risk-sensitive algorithms DRIMA and RMIX do not match up to
the performance of RiskQ. As for the 27m_vs_30m scenario, RiskQ is the second-best algorithm.
5.5

Ablation Study and Discussion

To investigate the reasons behind RiskQ’s promising results, we analyze different designs of RiskQ on
the SMAC and the MACN scenarios. First, we study the necessity of satisfying the RIGM principle
by making about 50% of RiskQ agents follow different risk measures. In Figure 4 (a-d), w/o RIGM
VaR1 , w/o RIGM CVaR1 and w/o RIGM CVaR0.2 indicate that about 50% of agents act according to
the VaR1 (risk-seeking), CVaR1 (risk-neutral) and the CVaR0.2 (risk-averse) metrics, respectively.
These risk measures are not the risk measure ψα = Wang0.75 used by other agents. As depicted in
Figure 4 (a-d), RiskQ performs poorly in all the three cases, highlighting the importance of satisfying
the RIGM principle that agents act according to the same risk measure.
Moreover, we analyze different designs of the RiskQ mixer through two variants: RiskQ-Sum and
RiskQ-Qi. Instead of using the attention mechanism, RiskQ-Sum models the percentile θ(τ , u, ω)
9

PN
as the sum of the percentiles of per-agent’s utilities i=1 θi (τi , ui , ω). RiskQ-Qi represents each
percentile θ(τ , u, ω) as the expectation of state-action function. As shown in Figure 4 (a-d), both
two variants perform inferior to RiskQ in most cases.
By modeling the quantiles θ(ω) of joint return distribution as the weighted sum of the quantiles θi (ω)
of each agent’s return distribution, RiskQ suffers from representation limitations. To study whether
the representation limitations impact the performance of RiskQ, we design three variants of RiskQ:
RiskQ-QMIX, RiskQ-Residual, and RiskQ-Residual-QMIX. RiskQ-QMIX models the monotonic
relations among θi (ω) and θ(ω) in the manner of QMIX [5]. RiskQ-Residual models the joint value
distribution without representation limitations by using residual functions [8]. RiskQ-Residual-QMIX
combines RiskQ-Residual and QMIX. RiskQ-QMIX and RiskQ-Residual-QMIX satisfy the RIGM
principle for the VaR metric, and RiskQ-Residual satisfies the RIGM for the VaR and DRM metrics.
Please refer to the appendix B.3, B.4 and B.5 for further details and their proofs.
We evaluate the performance of the three RiskQ variants using three different risk metrics (Wang0.75 ,
VaR0.2 , and VaR0.6 ). In Figure 4 (e) and (f), a method with its risk metric is denoted as method-metric.
For example, RiskQ-QMIX-VaR0.2 represents RiskQ-QMIX with the risk metric VaR0.2 . As can be
observed from Figure 4 (e) and (f), although these three variants can model more complex functional
relationships among quantiles, their performance is unsatisfactory for the 3s_vs_5z and MMM2
scenarios. This suggests that the representation limitation of RiskQ does not significantly impact its
performance. Finding a better network architecture to make the algorithm free from representation
limitations and have better performance is a prospective future work.
To systematically evaluate RiskQ, we evaluate the impact of various risk metrics, risk levels and
number of percentiles. The respective results are depicted in in Figure 4 (g-i). The variations in these
factors could impact the performance of RiskQ.
RiskQ uses QR-DQN [19] and IQN [20] to learn value distributions. It shares QR-DQN and
IQN’s converging property. As stated in [50] and [51], the greedy distributional Bellman update
operator of IQN is not a contraction mapping, which is an inherent drawback of the distributional
RL. Recently, [52] modified IQN with a new distributional Bellman operator, indicating the optimal
CVaR policy corresponds to a fixed point. However, but its general convergence is unclear. As
shown in Appendix (Figure 19), the new method that combines RiskQ with [52] performs poorly.
This suggests that remedying the non-contraction mapping issues may not be important enough for
performance improvement as existing risk-sensitive RL methods (e.g., IQN) are already working
well.
For risk-sensitive exploration, we have combined RiskQ with LQN [53], the results are depicted in
the Appendix (Figure 20). It shows that risk-sensitive exploration is a new direction of the future
work.

6

Conclusion

It is important to coordinate behaviors of multi-agents in risk-sensitive environments. We have
formulated the coordination requirement as the risk-sensitive individual-global-maximization (RIGM)
principle which is a generalization of the individual-globla-maximization (IGM) principle and the
Distributional IGM principle. Existing multi-agent value factorization does not satisfy the RIGM
principle for common risk metrics such as the Value at Risk (VaR) or distorted risk measures.
We propose, RiskQ, a risk-sensitive value factorization approach for Multi-Agent Reinforcement
Learning (MARL). RiskQ satisfies the RIGM theorem for the VaR and distorted risk measures via
modeling the quantile of joint state-action return distribution as weighted sum of the quantiles of
per-agent return distribution utilities. We show that RiskQ can obtain promising results through
extensive experiments.
Acknowledgement This work was partially supported by the National Natural Science Foundation
of China (No. 61972409), by the FuXiaQuan National Independent Innovation Demonstration Zone
Collaborative Innovation Platform (No.3502ZCQXT2021003), the Fundamental Research Funds
for the Central Universities (No. 0720230033), by PDL (2022-PDL-12), by the China Postdoctoral
Science Foundation (No.2021M690094). We would like thank the anonymous reviewers for their
valuable suggestions.
10

References
[1] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. Is multiagent deep reinforcement
learning the answer or the question? A brief survey. CoRR, abs/1810.05587, 2018.
[2] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. Deep
decentralized multi-task multi-agent reinforcement learning under partial observability. In
ICML, pages 2681–2690, 2017.
[3] Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos A. Vlassis. Optimal and approximate
q-value functions for decentralized pomdps. J. Artif. Intell. Res., 32:289–353, 2008.
[4] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores
Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and
Thore Graepel. Value-decomposition networks for cooperative multi-agent learning based on
team reward. In AAMAS, 2018.
[5] Tabish Rashid, Mikayel Samvelyan, Christian Schröder de Witt, Gregory Farquhar, Jakob N.
Foerster, and Shimon Whiteson. QMIX: monotonic value function factorisation for deep
multi-agent reinforcement learning. In ICML, 2018.
[6] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN:
learning to factorize with transformation for cooperative multi-agent reinforcement learning. In
ICML, 2019.
[7] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. In ICLR, 2021.
[8] Siqi Shen, Mengwei Qiu, Jun Liu, Weiquan Liu, Yongquan Fu, Xinwang Liu, and Cheng
Wang. Resq: A residual q function-based approach for multi-agent reinforcement learning value
factorization. In NeurIPS, 2022.
[9] Mary R. Hardy. Distortion risk measures : Coherence and stochastic dominance. In Economics,
2002.
[10] Alejandro Balbás, José Garrido, and Silvia Mayoral. Properties of distortion risk measures.
Method. Comput. Appl. Prob., 11(3):385–399, sep 2009.
[11] Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. In
NeurIPS, 2014.
[12] Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decisionmaking: a cvar optimization approach. In NeurIPS, pages 1522–1530, 2015.
[13] Wei Qiu, Xinrun Wang, Runsheng Yu, Rundong Wang, Xu He, Bo An, Svetlana Obraztsova,
and Zinovi Rabinovich. RMIX: learning risk-sensitive policies for cooperative reinforcement
learning agents. In NeurIPS, pages 23049–23062, 2021.
[14] Kyunghwan Son, Junsu Kim, Sungsoo Ahn, Roben Delos Reyes, Yung Yi, and Jinwoo Shin.
Disentangling sources of risk for distributional multi-agent reinforcement learning. In ICML,
pages 20347–20368, 2022.
[15] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. In NeurIPS,
2020.
[16] Mikayel Samvelyan, Tabish Rashid, Christian Schröder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon
Whiteson. The starcraft multi-agent challenge. In AAMAS, pages 2186–2188, 2019.
[17] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
Springer Briefs in Intelligent Systems. Springer, 2016.
[18] Marc G. Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In ICML, volume 70, pages 449–458, 2017.
[19] Will Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. Distributional reinforcement learning with quantile regression. In AAAI, 2018.
[20] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In ICML, volume 80, pages 1104–1113, 2018.
11

[21] Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors.
IEEE TNNLS, 2022.
[22] Wei-Fang Sun, Cheng-Kuang Lee, and Chun-Yi Lee. DFAC framework: Factorizing the value
function via quantile mixture for multi-agent distributional q-learning. In ICML, 2021.
[23] Thomas J. Linsmeier and Neil D. Pearson. Value at risk. Financial Analysts Journal, 56(2):47–
67, 2000.
[24] Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao.
Distributional soft actor critic for risk sensitive learning. ICML workshop, 2020.
[25] R.Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions. In Journal of Banking and Finance, 2002.
[26] Shaun S. Wang. A class of distortion operators for pricing financial and insurance risks. The
Journal of Risk and Insurance, 67(1):15–36, 2000.
[27] Amos Tversky. Advances in Prospect Theory: Cumulative Representation of Uncertainty, pages
44–66. 09 2000.
[28] Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, and Shih wei Liao. Rethinking the
implementation tricks and monotonicity constraint in cooperative multi-agent reinforcement
learning, 2022.
[29] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. CoRR,
abs/2002.03939, 2020.
[30] Shariq Iqbal, Christian A. Schröder de Witt, Bei Peng, Wendelin Boehmer, Shimon Whiteson,
and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In
ICML, 2021.
[31] Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-sensitive
reinforcement learning: Near-optimal risk-sample tradeoff in regret. In NeurIPS, 2020.
[32] Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki
Tanaka. Parametric return density estimation for reinforcement learning. In Peter Grünwald
and Peter Spirtes, editors, UAI, pages 368–375. AUAI Press, 2010.
[33] Takuya Hiraoka, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, and Yoshimasa Tsuruoka.
Learning robust options by conditional value at risk optimization. In NeurIPS, pages 2615–2625,
2019.
[34] Yingjie Fei, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive reinforcement learning with
function approximation: A debiasing approach. In ICML, pages 3198–3207, 2021.
[35] Núria Armengol Urpí, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement
learning. In ICLR. OpenReview.net, 2021.
[36] Shiau Hong Lim and Ilyas Malik. Distributional reinforcement learning for risk-sensitive
policies. In NeurIPS, 2022.
[37] Jihwan Oh, Joonkee Kim, and Se-Young Yun. Risk perspective exploration in distributional
reinforcement learning. NeurIPS workshop, abs/2206.14170, 2022.
[38] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication
with backpropagation. In NeurIPS, 2016.
[39] Siqi Shen, Yongquan Fu, Huayou Su, Hengyue Pan, Peng Qiao, Yong Dou, and Cheng Wang.
Graphcomm: A graph neural network based method for multi-agent reinforcement learning. In
ICASSP, 2021.
[40] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In NeurIPS, pages 2137–2145,
2016.
[41] Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar. Coachplayer multi-agent reinforcement learning for dynamic team composition. In ICML, 2021.
12

[42] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In NeurIPS, pages 6379–6390,
2017.
[43] Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In
ICML, 2019.
[44] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre M. Bayen, and Yi Wu. The
surprising effectiveness of MAPPO in cooperative, multi-agent games. CoRR, abs/2103.01955,
2021.
[45] Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung. MASER: multi-agent
reinforcement learning with subgoals generated from experience replay buffer. In ICML, 2022.
[46] Siqi Shen, Jun Liu, Mengwei Qiu, Weiquan Liu, Cheng Wang, Yongquan Fu, Qinglin Wang,
and Peng Qiao. Qrelation: an agent relation-based approach for multi-agent reinforcement
learning value function factorization. In ICASSP, 2022.
[47] Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao, Jianye Hao, and Pheng-Ann
Heng. Transformer-based working memory for multiagent reinforcement learning with action
parsing. In NeurIPS, 2022.
[48] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal multi-agent
reinforcement learning via policy decoupling with transformers. In ICLR, 2021.
[49] Ekaterina N. Sereda, Efim M. Bronshtein, Svetozar T. Rachev, Frank J. Fabozzi, Wei Sun,
and Stoyan V. Stoyanov. Distortion Risk Measures in Portfolio Optimization, pages 649–673.
Springer US, Boston, MA, 2010.
[50] Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Reinforcement Learning.
MIT Press, 2023. http://www.distributional-rl.org.
[51] Mark Rowland, Robert Dadashi, Saurabh Kumar, Rémi Munos, Marc G. Bellemare, and Will
Dabney. Statistics and samples in distributional reinforcement learning. In ICML, pages
5528–5536, 2019.
[52] Shiau Hong Lim and Ilyas Malik. Distributional reinforcement learning for risk-sensitive
policies. In NeurIPS, 2022.
[53] Xueguang Lyu and Christopher Amato. Likelihood quantile networks for coordinating multiagent reinforcement learning. In AAMAS, pages 798–806, 2020.
[54] Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj
Mahajan, Jakob N. Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for
cooperative multi-agent reinforcement learning. In NeurIPS, 2023.
[55] Fan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional reinforcement learning. In Neural Information Processing Systems, 2020.
[56] Eitan Altman. Constrained markov decision processes. 1999.
[57] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.
In ICML, volume 70, pages 22–31. PMLR, 2017.
[58] Yinlam Chow, Ofir Nachum, Edgar A. Duéñez-Guzmán, and Mohammad Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. In NeurIPS, pages 8103–8112, 2018.
[59] Núria Armengol Urpí, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement
learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

13

Appendix
A

Background

A.1

Distributional RL and Risk

In this work, we interchangeably use the terms: stochastic value function and return distribution.
The state-action return distribution Z(τ , u) can be modelled using quantile functions θ of a random
variable Z, which is defined as follows.
θZ (τ , u, ω) = inf{z ∈ R : ω ≤ CDFZ (z)},

∀ω ∈ [0, 1]

(A.1)

where CDFZ (z) is the cumulative distribution function of Z(τ , u). The quantile function θZ (ω) may
be referred as generalized inverse CDF in other literature [22]. For notation simplicity, we denote
θZ (ω) as θ(ω).
QR-DQN [19] and IQN [20] model the stochastic value function Z(τ, u) as a mixture of n Dirac
functions.
n
X
Z(τ, u) =
pi (τ, u, ωi )δθ(τ ,u,ωi )
(A.2)
i=1

where δθ(τ ,u,ωi ) is a Dirac Delta function whose value is θ(τ , u, ωi ). ωi is a quantile sample.
pi (τ, u, ωi ) is the corresponding probability of θ(τ , u, ωi ). For QR-DQN [19], pi (τ, u, ωi ) can be
simplified as 1/n. For execution, the action with the largest expected return arg maxu E[Z(τ , u)] is
chosen.
In this work, we use ψα to measure the risk from a return distribution Z, where α is the risk level.
For example, the Value at Risk metric, VaRα , estimate the α-percentile from a distribution. For
VaRα , a small value for α indicates risk-averse setting, whereas a high value for α means risk-seeking
scenarios. Risk-sensitive policies act with a risk measure ψα . In this work, we interchangeably use
the terms: stochastic value function and return distribution.
Definition 7 (Value at Risk (VaR)). Value at Risk (VaR) [23] is a popular risk metric which measures
risk as the minimal reward might be occur given a confidence level α. For a random variable Z with
cumulative distribution function (CDF), the quantile function θ and a quantile sample α ∈ [0, 1],
V aRα (Z(τ , u)) = θ(τ , u, α). This metric is called percentile as well.
Definition 8 (Distortion risk measure (DRM)). Distorted expectation metrics [9, 10, 24], such as
CVaR [25], CPW [27], and Wang [26], are weighted expectation of return distribution under a
distortion function [9, 10, 24]. The distorted expectation of a random variable Z under g is defined
as
Z 1
ψ(Z) =
g ′ (ω)θ(ω)dω
(A.3)
0

where g(ω) ∈ [0, 1], g ′ (ω) is the derivative of g(ω).
Definition 9 (Conditional Value at Risk(CVaR)).
CV aRα (Z) = EZ [z|z ≤ θ(α)]

(A.4)

where α is the confidence level (risk level), θ(α) is the quantile function (inverse CDF) defined
in (A.1). CVaR is the expectation of values z that are less equal than the α-quantile value (θ(α)) of
the value distribution. CVaR is a DRM whose g(ω) = min(ω/α, 1).
Definition 10 (Wang). Wang is a DRM proposed in [26]. Its g(ω) = Φ(Φ−1 (ω) + α), where Φ is
the CDF of the Gaussian distribution. The Wang measure is risk-averse if α > 0 or risk-seeking for
α < 0.
1
Definition 11 (CPW). CPW is a DRM proposed in [27]. Its g(ω) = ω α /(ω α + (1 − ω)α ) α .
Researchers found that α = 0.71 matches human decision preference.
Definition 12. Risk-sensitive greedy policy[20] for a value distribution Z(s, u) with a risk measure
ψα is defined as
πψα (s) = arg max ψα [Z(s, u)]
(A.5)
u

In this work, we assume the argmax operator is unique, the action with smallest index is selected to
break ties if a tie exists.
14

B

RiskQ Theorems and Proofs

In this section, we show that methods satisfying the IGM principle are insufficient to guarantee the
RIGM principle for risk metrics such as the VaR and DRM metrics in Theorem 1. And we show that
methods satisfying the DIGM principle are insufficient to guarantee the RIGM principle for the VaR
or DRM metrics in Theorem 2. Further, we show that the risk-sensitive algorithm DRIMA does not
satisfy the RIGM principle for the VaR metric in Theorem 3.
B.1

Methods satisfying IGM or DIGM principle

Simply replacing Qi with ψα (Zi ) is insufficient to guarantee [Zi ]ni=1 satisfy RIGM for VaR and
distorted expectation metrics.
Theorem 1. Given a deterministic joint action-value function Qjt , a stochastic joint action-value
function Zjt , and a factorization function Φ for deterministic utilities:
Qjt (τ, u) = Φ(Q1 (τ1 , u1 ), ..., Qn (τn , un ))

(B.6)

such that [Qi ]ni=1 satisfy IGM for Qjt under τ , the following risk-sensitive distributional factorization:
Zjt (τ, u) = Φ(Z1 (τ1 , u1 ), ..., Zn (τn , un ))

(B.7)

is insufficient to guarantee that [Zi ]ni=1 satisfy RIGM for Zjt (τ, u) with risk metric ψα such as the
VaR metric and the distorted risk measures.
Proof. We first show that VDN does not guarantee the RIGM for the VaR metric, then we show that
QMIX does not guarantee the RIGM principle for the CVaR metric (a distorted risk measures). We
prove this theorem by contradiction.
The VDN [4] algorithm is factorization methods P
that satisfy the IGM theorem but it cannot guarantee
n
the RIGM principle. VDN model Qjt (τP
, u) = i=1 Qi (τi , ui ). Simply replacing the utility Qi as
n
Zi . The function becomes Zjt (τ , u) = i=1 Zi (τi , ui ).
We consider a degenerated case where there are two agents and single full observable state s. Agents
have two actions: a and b. The probability distribution function for Zi (τi , a) and Zi (τi , b) is defined
as follows.

50% Zi = 0.25
p(Zi (τi , a)) =
(B.8)
50% Zi = 1

p(Zi (τi , b)) =

50%
50%

Zi = 0
Zi = 100

(B.9)

We assume that Z2 (τ2 , u2 ) = Z1 (τ1 , u1 ). For VDN, Zjt (τ , u) = Z1 (τ1 , u1 ) + Z2 (τ2 , u2 ). The risk
metric we consider is the percentile metric. ψα = V aR0.5 . V aRα (Z) = minZ {z|FZ (z) ≥ α}.
arg max{V aR0.5 [Z1 (τ1 , u1 )]} = a

1.25



1
V aR0.5 [Zjt (τ , u)] =

1


100

u = (a, a)
u = (a, b)
u = (b, a)
u = (b, b)

(B.10)

Assume, to the contrary, the VDN algorithm satisfy the RIGM theorem for the VaR metric. As
arg maxu {V aR0.5 [Z1 (τ1 , u1 )]} = a and arg maxu {V aR0.5 [Z2 (τ2 , u2 )]} = a, for the risk metric
ψ0.5 = V aR0.5 , to satisfy the RIGM theorem, the optimal action that maximize the risk metric
should be (a, a). However, as it is shown in (B.10), the action maximizing V aR0.5 Z(τ, u) is (b, b),
rather than (a, a), a contradiction. We have shown that VDN does not satisfy the RIGM principle for
the VaR metric.
In this following, we show that QMIX does not satisfies the RIGM theorem for the CV aR metric. QMIX is a method that satisfies the IGM theorem. It learns Qjt (τ, u) = Φ(Q1 , ..., Qn ) to
approximate the optimal policy of the state-action value function Q(τ, u), where Φ is a monotonic
15

increasing function with respect to Qi . Φ = Q31 (τ1 , u1 ) + Q32 (τ2 , u2 ). Let’s consider a simple onestep matrix game with two agents each with actions a and b. Using Φ as the factorization function,
Zjt (τ, u) = Z13 (τ1 , u1 ) + Z23 (τ2 , u2 ), where Z1 = Z2 . Zjt could lead to incorrect estimation of the
optimal actions for the risk metric ψ1 = CV aR1 . Zi (τi , a) and Zi (τi , b) is defined as follows.
Zi (τi , a) = 2

Zi (τi , b) =

3
0

50% of the time
50% of the time

(B.11)

(B.12)

Let’s assume that, for action a, Z1 (s, a) = 2 for 100% of the time; for action b, Z1 (s, b) = 3 for
50% of the time and Z1 (s, b) = 0 for 50% of the time. Clearly ψ1 [Z1 (τ1 , a)] = CV aR1 [Z1 (τ1 , a)]
= 2 and ψ1 [Z1 (τ1 , b)] = CV aR1 [Z1 (τ1 , b)] = 1.5. If Zjt and Zi satisfy the RIGM theorem, then
the optimal action for Zjt should be (a, a) = (arg maxu ψ1 [Z1 (τ1 , u1 )], arg maxu ψ1 [Z2 (τ2 , u2 )]).
However, ψ1 [Z13 (τ, a)] = 8 and ψ1 [Z13 (τ, b)] = 13.5. arg maxu ψ1 [Zjt (τ , u)] = (b, b) rather than
(a, a). A contradiction is found. We have shown that methods satisfy the IGM principle do not
guarantee the RIGM principle with risk metric ψα .
We show that DIGM factorization methods is insufficient to guarantee the satisfaction of the RIGM
theorem.
Theorem 2. Given a stochastic joint action-value function Zjt , and a distributional factorization
function Φ for the stochastic utilities which satisfy the DIGM theorem: the following distributional
factorization:
Zjt (τ, u) = Φ(Z1 (τ1 , u1 ), ..., Zn (τn , un ))
(B.13)
is insufficient to guarantee that [Zi ]ni=1 satisfy RIGM for Zjt (τ, u) with risk metric ψα .
Proof. As far as we know, the DFAC framework and the ResZ method satisfies the DIGM principle.
We first show that the DFAC framework does not guarantee the RIGM principle for the VaR metric
and then show that ResZ does not guarantee the RIGM principle as well.
The DFAC framework learns factorize a joint return distribution Zjt using mean-shape decomposition
which is defined as follows.

Zjt (τ , u) = E[Zjt (τ , u)] + (Zjt (τ , u) − E[Zjt (τ , u)])
= Zmean (τ , u) + Zshape (τ , u)
= f (Q1 (τ1 , u1 ), ..., Qn (τn , un )) + h(Z1 (τ1 , u1 ), ..., Zn (τn , un ))

(B.14)
(B.15)
(B.16)
(B.17)

where f is factorization function for deterministic utility. f P
satisfy the IGM principle. The function h
n
models the shape of Zjt . h(Z1 (τ1 , u1 ), ..., Zn (τn , un )) = i=1 (Zi (τi , ui ) − Qi (τi , ui )).
We consider a dis-generated case where there is only one agent with two actions. The stochastic
utility is defined as follows.
Z1 (τ1 , a) = 5

Z1 (τ1 , b) =

6
0

50% of the time
50% of the time

(B.18)

(B.19)

Q1 (τ1 , a) = 5 and Q1 (τ1 , b) = 3. Let f = 5Q1 (τ1 ,u1 ) . In this case, for the V aR1 risk measure
ψ1 = V aR1 , arg max ψ1 [Zjt (τ , u)] = arg max[f (Q1 )] = a. That is, the action a maximize
ψ1 [Zjt (τ , u)]. However, arg max ψ1 [Z1 (τ1 , u1 )] = b which is different from a. To satisfy the
RIGM principle, these two actions should be equal. Thus, we have shown that the DFAC framework
does not guarantee the RIGM principle for the V aR metric.
16

We prove that ResZ does not guarantee the RIGM principle for the VaR metric through an example.
The ResZ [8] algorithm is distributional factorization methods that satisfy the DIGM theorem but it
cannot guarantee the RIGM principle.
ResZ [8] learns Zjt (τ , u) = Ztot (τ , u) + wr (τ , u)Zr (τ , u), where Zr (τ , u) ≤ 0, Ztot (τ , u) =
PN
i=1 ki Zi (τi , ui ) ki ≥ 0
We prove this theorem by providing an example. Let’s assume a special case where there are two
agents and Zr (τ , u) = 0. Then, Zjt (τ , u) = Ztot (τ , u) = 2Z1 (τ1 , u1 ) + 2Z2 (τ2 , u2 ).

50% Zi = 0.25
p(Zi (τi , a)) =
(B.20)
50% Zi = 1


50% Zi = 0
50% Zi = 100

2.5 u = (a, a)



2
u = (a, b)
V aR0.5 [Zjt (τ , u)] =

2
u = (b, a)


200 u = (b, b)
p(Zi (τi , b)) =

(B.21)

(B.22)

The risk metric we consider is ψ0.5 = V aR0.5 . arg max{V aR0.5 [Z1 (τ1 , u1 )]} = a and
arg max{V aR0.5 [Z2 (τ2 , u2 )]} = a, for ResZ, to satisfy the RIGM theorem, the optimal action
that maximize the risk metric should be (a, a). However, as it is shown in (B.22), the action
maximizing V aR0.5 Zjt (τ, u) is (b, b), rather than (a, a), a contradiction.
Theorem 3. DRIMA [14], a risk-sensitive MARL algorithm, does not guarantee the RIGM principle.
Proof. DRIMA [14] learns transformed action-value estimator Ztran (τ , u, α) to approximate the
true value distribution function Zjt . The quantile value θtran (τ , u, α) for Ztran (τ , u, α) is defined
as θtran (τ , u, α) = Qmix (θ1 (τ1 , u1 , α), ..., θn (τn , un , α)), where θi (τi , ui , α) is the α−percentile
for the stochastic utility Zi (τi , ui ) of agent i.
We provide an example for DRIMA that does not satisfy the RIGM theorem for the risk-neutral
case. For the risk-neutral case, DRIMA uniformly sample the quantile sample ωj ∈ [0, 1], and the
PK
average value j=1 θ(τ , u, ωj ) of the quantiles is used for action selection, where K is the number
of samples. When K becomes infinite, DRIMA can be viewed as using the risk measure CV aR1
(the expectation operation E) to select actions. We will show that DRIMA does not satify the RIGM
principle for the CV aR1 metric.
Assuming there are two agents, each with two actions. θtran (τ , u, α) = θ13 (τ1 , u1 , α)+θ23 (τ2 , u2 , α).
Z1 = Z2 . Ztran could estimate sub-optimal actions for the risk metric ψ1 = CV aR1 . The stochastic
utilities Z1 (τ1 , a) and Z1 (τ1 , b) is defined as follows.
Z1 (τ1 , a) = Z2 (τ2 , a) = 3

5 50% of the time
Z1 (τ1 , b) = Z2 (τ2 , b) =
0 50% of the time

(B.23)
(B.24)

Clearly ψ1 [Z1 (s, a)] = 3 and ψ1 [Z1 (s, b)] = 2.5.
If DRIMA satisfies the RIGM
theorem, then the risk-sensitive optimal action for Ztran should be (a, a)
=
(arg max ψ1 [Z1 (τ1 , u1 )], arg max ψ1 [Z2 (τ2 , u2 )]).

 54
ψ1 [Ztran (τ , u)] = 89.5

125

u = (a, a)
u = (a, b) or (b, a)
u = (b, b)

(B.25)

Clearly, arg max ψ1 [Ztran (τ , u)] = (b, b) rather than (a, a). Thus, we have shown that DRIMA
does not guarantee the RIGM theorem for the CV aR1 metric.
17

B.2

RiskQ

RiskQ satisfies the RIGM principle for the VaR and distorted expection risk metrics (such as Wang,
and CPW).
Theorem 4. A stochastic joint state-action return distribution

Zjt (τ , u) =
θ(τ , u, ωj ) =

J
X
j=1
n
X

pj (τ , u, ωj )δθ(τ ,u,ωj )

(B.26)

ki θi (τi , ui , ωj )

(B.27)

i=1

is distributional factorized by [Zi (τi , ui )]N
i=1 with risk metric ψα , where J is the number of Dirac
Delta functions, δθ(τ ,u,ωj ) is a Dirac Delta function at θ(τ , u, ωj ) ∈ R, θ(τ , u, ωj ) is a quantile
function with sample ωj , pj (τ , u, ωj ) is the corresponding probability for δθ(τ ,u,ωj ) of the estimated
return, θi (τi , ui , ω) is the ω − quantile function for Zi (τi , ui ) and ki ≥ 0.

Proof. Theorem show that if the above condition are satisfied, then [Zi (τi , ui )]N
i=1 satisfy the RIGM
principle for Zjt (τ , u) with the risk metric ψα . We will show that arg maxu ψα [Zjt (τ , u)] = ū,
ū = [ūi ]N
i=1 , ūi = arg maxui ψα [Zi (τi , ui )].
We prove this theorem for the VaR and the Distorted Expectation risk metric (e.g., CVaR, CPW).
The VaR risk metric uses the α−quantile to measure risk. ψα [Zjt (τ , u)] = θ(τ , u, α) and
ψα [Zi (τi , ui )] = θi (τi , ui , α)

ψα [Zjt (τ , ū)] =
=
≥
=

n
X
i=1
n
X
i=1
n
X
i=1
n
X

ki θi (τi , ūi , α) (quantile definition)

(B.28)

ki ψα [Zi (τi , ūi )]

(B.29)

ki ψα [Zi (τi , ui )]
ki θi (τi , ui , α)

(ūi = arg max ψα [Zi (τi , ui )])

(B.30)
(B.31)

i=1

= ψα [Zjt (τ , u)]

(B.32)

Distorted expectation metrics, such as CVaR, CPW, and Wang, are weighted expectation of return
distribution under a distortion function [9, 10, 24]. The distorted expectation of a random variable Z
R1
under g is defined as ψ(Z) = 0 g ′ (w)θ(w)dw, where g(w) ∈ [0, 1], g ′ (w) is the derivative of g(w).
In the following, we show that if the above conditions are satisfied, then [Zi (τi , ui )]N
i=1 satisfy the
RIGM principle for Zjt (τ , u) with the risk metric that is a distorted expectation metric.
18

Z 1
ψα [Zjt (τ , ū)] =

g ′ (w)θ(τ , ū, w)dw

(B.33)

0

Z 1
=

g ′ (w)

0

=
=
≥
=

n
X
i=1
n
X
i=1
n
X
i=1
n
X
i=1
Z 1

=
Z 1

ki θi (τi , ūi , w)dw

(B.34)

g ′ (w)θi (τi , ūi , w)dw

(B.35)

i=1

Z 1
ki
0

ki ψα [Zi (τi , ūi )]
ki ψα [Zi (τi , ui )]
Z 1
ki

(B.36)
(ūi = arg max ψα [Zi (τi , ui )])

g ′ (w)θi (τi , ui , w)dw

(B.37)
(B.38)

0

g ′ (w)

0

=

n
X

n
X

ki θi (τi , ui , w)dw

(B.39)

i=1

g ′ (w)θ(τ , u, w)dw

(B.40)

0

= ψα [Zjt (τ , u)]

(B.41)

We have shown that by modeling the quantiles of the stochastic Zjt through weighted sum of quantiles
of [Zi ]ni=1 . [Zi ]ni=1 satisfied the RIGM theorem with risk metrics such as VaR, CVaR, Wang, and
CPW, etc.
B.3

RiskQ-QMIX

RiskQ suffers from representation limitations that it can model the weighted sum relationships among
quantiles. RiskQ-QMIX replaces the mixer of RiskQ from a simple attention moduel to QMIX. It can
model the monotonic relationship amonsg quantiles. We show that it satisfies the RIGM principle for
the VaR metric.
Theorem 5. A risk-aware stochastic joint state-action return
Zjt (τ , u) =

J
X

pj (τ , u)δθ(τ ,u,wj )

(B.42)

θ(τ , u, wj ) = Qmix (θ1 (τ1 , u1 , wj ), ...θn (τn , un , wj ))

(B.43)

j=1

is distributional factorized by [Zi (τi , ui )]N
i=1 with risk metric V aRα , where J is the number of Dirac
Delta functions, δθ(τ ,u,wj ) is a Dirac Delta function at θ(τ , u, wj ) ∈ R, θ(τ , u, wj ) is a quantile
function with quantile sample wj , pj (τ , u) is the corresponding probability for δθ(τ ,u,wj ) of the
estimated return, θi (τi , ui , w) is the w−quantile for Zi (τi , ui ).
Proof. Theorem show that if the above condition are satisfied, then [Zi (τi , ui )]N
i=1 satisfy the RIGM
principle for Zjt (τ , u) with the risk metric V aRα . We will show that arg maxu ψα [Zjt (τ , u)] = ū,
ū = [ūi ]N
i=1 , ūi = arg maxui ψα [Zi (τi , ui )].
The VaR risk metric uses α−quantile of a random variable to measure risk of the variable.
V aRα [Zjt (τ , u)] = θ(τ , u, α) and V aRα [Zi (τi , ui )] = θi (τi , ui , α)
19

V aRα [Zjt (τ , ū)] = Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α)) (quantile definition)
(B.44)
≥ Qmix (θ1 (τ1 , u1 , α), ...θn (τn , un , α)) (ūi = arg max V aRα [Zi (τi , ui )])
(B.45)
= ψα [Zjt (τ , u)]
(B.46)

(B.45) is satisfied due to the following reasons. ūi = arg max V aRα [Zi (τi , ui )], thus θi (τi , ūi , α) ≥
θi (τi , ui , α). As Qmix (θ1 (τ1 , u1 , α), ...θn (τn , un , α)) is monotonic increase with respect to
θi (τi , ui , α), Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α)) ≥ Qmix (θ1 (τ1 , u1 , α), ...θn (τn , un , α))

B.4

RiskQ-Residual

RiskQ-QMIX suffers from representation limitation that it can model the monotonic relationship
among quantiles only. It cannot model non-monotonic relationship among quantiles. ResZ [8]
decomposes a stochastic joint return distribution Zjt (τ , u) into a main function Ztot (τ , u) and a
residual function Zr (τ , u). The main function Ztot (τ u) share the same optimal policy as Zjt (τ , u).
It is show that ResZ satisfies the DIGM principle without representation limitations.
Inspired
by ResZ, we decompose a quantile function into its main quantile function
Pn
k
θ
(τi , ui , wj ) and residual quantile function θr (τ , u, wj ) with a mask function mα (τ , u).
i
i
i=1
We show that RiskQ-Residual satisfies the RIGM principle for the VaR and distorted expectations
metrics without representation limitations.
Theorem 6. A stochastic joint state-action return

Zjt (τ , u) =
θ(τ , u, wj ) =

J
X
j=1
n
X

pj (τ , u)δθ(τ ,u,wj )

(B.47)

ki θi (τi , ui , wj ) + mα (τ , u)θr (τ , u, wj )

(B.48)

i=1

is distributional factorized by [Zi (τi , ui )]N
i=1 with risk metric ψα , where θr (τ , u, w) ≤ 0, the mask
function mα (τ , u) = 0 when u = ū, otherwise 1, J is the number of Dirac Delta functions,
δθ(τ ,u,wj ) is a Dirac Delta function at percentile θ(τ , u, wj ) ∈ R, θ(τ , u, wj ) is a quantile function
with quantile wj , pj (τ , u) is the corresponding probability for δθ(τ ,u,wj ) of the estimated return,
θi (τi , ui , w) is the w−quantile for the stochastic utility function Zi (τi , ui ) and ki ≥ 0.

Proof. Theorem 6 shows that if the above conditions are satisfied, then [Zi (τi , ui )]N
i=1 satisfy the
RIGM principle for Zjt (τ , u) with the risk metric ψα . We will show that arg maxu ψα [Zjt (τ , u)] =
ū, ū = [ūi ]N
i=1 , ūi = arg maxui ψα [Zi (τi , ui )].
20

For the VaR metric
ψα [Zjt (τ , ū)] =
=
=
≥
=
≥

n
X
i=1
n
X
i=1
n
X
i=1
n
X
i=1
n
X
i=1
n
X

ki θi (τi , ūi , α) + mα (τ , ū)θr (τ , ū, α)

(B.49)

ki θi (τi , ūi , α)

(B.50)

(mr (τ , ū, α) = 0)

ki ψα [Zi (τi , ūi )] VaR definition

(B.51)

ki ψα [Zi (τi , ui )]

(B.52)

(ūi = arg max ψα [Zi (τi , ui )])

ki θi (τi , ui , α)

(B.53)

ki θi (τi , ui , α) + mr (τ , u, α)θr (τ , u, α)

(B.54)

i=1

= ψα [Zjt (τ , u)]

(B.55)

(B.54) because θr (τ , u, wj ) ≤ 0 and mr (τ , u, α) = 1
(B.49) to (B.55) mean that ū = [ūi ]N
i=1 maximizes ψα [Zjt (α, τ , u)] for the VaR metric. Thus
[Zi (τi , ui )]N
i=1 satisfies RIGM for Zjt (τ , u) with for the VaR risk metric ψα .
For distorted expectation metrics ψα such as Wang, CVaR, and CPW, and show that [Zi (τi , ui )]N
i=1
satisfy the RIGM theorem for ψα as follow.
Z 1
ψα [Zjt (τ , ū)] =
g ′ (w)θ(τ , ū, w)dw
(B.56)
0

Z 1
=

n
X
g ′ (w)[
ki θi (τi , ūi , w) + mr (τ , ū, w)θr (τ , ū, w)]dw

0

Z 1
=

g ′ (w)

0

=
=
≥
=

n
X
i=1
n
X
i=1
n
X
i=1
n
X
i=1
Z 1

=
0

Z 1
≥
0

Z 1
=

i=1
n
X

ki θi (τi , ūi , w)dw

(mr (τ , ū, w) = 0)

(B.57)
(B.58)

i=1

Z 1

g ′ (w)θi (τi , ūi , w)dw

ki

(B.59)

0

ki ψα [Zi (τi , ūi )]

(ūi = arg max ψα [Zi (τi , ui )])

ki ψα [Zi (τi , ui )]
Z 1

(B.61)

g ′ (w)θi (τi , ui , w)dw

ki

(B.60)

(B.62)

0

g ′ (w)

n
X

ki θi (τi , ui , w)dw

(B.63)

ki θi (τi , ui , w) + mr (τ , u, w)θr (τ , u, w)]dw

(B.64)

i=1
n
X

g ′ (w)[

i=1

g ′ (w)θ(τ , u, w)dw

(B.65)

0

= ψα [Zjt (τ , u)]

(B.66)
21

(B.56) to (B.65) mean that ū = [ūi ]N
i=1 maximizes ψα [Zjt (α, τ , u)] for distorted metrics such as
CV aRα . Thus [Zi (α, τi , ui )]N
satisfies
RIGM for Zjt (α, τ , u) with for distorted expectation
i=1
metrics.

B.5

RiskQ-Residual-QMIX

RiskQ-Residual-QMIX model the main quantile function as Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α)).
We show that it satisfies the RIGM principle for the VaR metric.
Theorem 7. A stochastic joint state-action return
Zjt (τ , u) =

J
X

pj (τ , u)δθ(τ ,u,wj )

(B.67)

j=1

θ(τ , u, wj ) = Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α)) + mα (τ , u)θr (τ , u, wj )

(B.68)

is distributional factorized by [Zi (τi , ui )]N
i=1 with risk metric ψα , where θr (τ , u, w) ≤ 0, the mask
function mα (τ , u) = 0 when u = ū, otherwise 1, J is the number of Dirac Delta functions,
δθ(τ ,u,wj ) is a Dirac Delta function at percentile θ(τ , u, wj ) ∈ R, θ(τ , u, wj ) is a quantile function
with quantile wj , pj (τ , u) is the corresponding probability for δθ(τ ,u,wj ) of the estimated return,
θi (τi , ui , w) is the w−quantile for the stochastic utility function Zi (τi , ui ).
For the VaR metric
ψα [Zjt (τ , ū)] = Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α)) + mα (τ , ū)θr (τ , ū, α) VaR definition
(B.69)
= Qmix (θ1 (τ1 , ū1 , α), ...θn (τn , ūn , α))) (mr (τ , ū, α) = 0)
(B.70)
≥ Qmix (θ1 (τ1 , u1 , α), ...θn (τn , un , α))) (ūi = arg max ψα [Zi (τi , ui )]) (B.71)
≥ Qmix (θ1 (τ1 , u1 , α), ...θn (τn , un , α))) + mr (τ , u, α)θr (τ , u, α)
(B.72)
= ψα [Zjt (τ , u)] VaR definition
(B.73)
(B.72) because θr (τ , u, wj ) ≤ 0 and mr (τ , u, α) = 1
B.6
B.6.1

Algorithm and Neural Networks
Neural Networks

We provide the detailed riskq network framework in Figure 1. It contains (a): the agent network, (b):
the overall framework of RiskQ, and (c): the hybrid network diagram of RiskQ.
⋅ ∶ absolute

𝑍𝑗𝑡

𝑐(𝑠𝑡 )

TD Loss

𝒘ℎ

RiskQ

𝑍𝑖 (𝜏𝑖𝑡 ,⋅)

𝜋𝑖

𝒔𝒕

∑𝑍 ℎ 𝑤ℎ

(𝑺𝑡 , 𝒇𝑡 )

𝜇𝑖𝑡

M LP

𝑍𝑗𝑡

𝐶𝐷𝐹

𝝐

𝜓𝛼 : risk function

M LP

𝑍𝑖 (𝜏𝑖𝑡 , 𝜇𝑖𝑡 )

M LP

…

M LP

𝜔𝑖 : quantiles

𝜃𝑖 𝜏𝑖 , 𝜇𝑖 , 𝜔𝑖

𝒔𝒕

𝑍 ℎ (𝜏, 𝜇)

MLP
𝜓𝛼 𝑍𝑖 𝜏𝑖𝑡 ,⋅

𝑍1 (𝜏1𝑡 , 𝜇1𝑡 )

GRU
Agent 1

𝜓𝛼

MLP

(𝑜1𝑡 , 𝜇1𝑡−1 )

(𝑜𝑖𝑡 , 𝜇𝑖𝑡−1 )
(a)

…
…
…

𝑍𝑛 (𝜏𝑛𝑡 , 𝜇𝑛𝑡 )
Agent N
𝑍1

𝑍2

𝑍𝑛

Multi-head
Attention

(𝑜𝑛𝑡 , 𝜇𝑛𝑡−1 )

(b)

(𝑺𝑡 , 𝒇𝑡 )
(c)

Figure 1: RiskQ framework

As shown in Figure 1, the agent network takes the observation-action sequence (oti , ut−1
) from agent
i
i at time step t, and channels it through a multi-layer perception architecture, specifically structured
22

as MLP-GRU-MLP, to generate the distribution Zi . Then, we can obtain the ψ(Zi ) through the
risk function, and the action uti is selected through the ϵ − greedy strategy, finally obtaining the
distribution Zi (τit , uti ). Each agent produces the distribution Zi (τit , uti ) through the agent network,
and then generates zjt through the RiskQ Mixer. The RiskQ Mixer combines the quantile sample
(cumulative probability) w of the distribution Zi , with the multi-layer attention mechanism and the
multi-layer perception network, and finally obtains the quantile value θ(τ , µ, ω) of Zjt (τ , µ), which
Pi
is represented as the weighted sum of the quantile values of stochastic utility n = ki θi (τ, µi , ω).
B.6.2

Algorithm

The RiskQ algorithm is described in Algorithm 1.
Algorithm 1 The RiskQ Algorithm
Require: risk parameter: ψα ;
Require: Initialize parameters θ of the network of agent, risk operator and RiskQ neural networks;
1: for e ∈ {1, . . . , m episodes} do
2:
Start a new episode;
3:
while episode_is_not_end do
4:
for agent i do
5:
Estimate the agent’s utility Zit (τit , uti );
6:
Calculate Risk-sensitive value ψα [Z(τit , uti )] ;
7:
Get the action ūti = arg maxu ψα [Z(τit , u)];
8:
end for
9:
Execute ūt , obtain global reward rt and the next state s′
10:
update replay buffer D;
11:
if it is time to update then
12:
Sample a batch D′ from replay buffer D;
13:
For each sample in D′ , calculate Zjt , ψα [Zjt ].
14:
Update θ by minimizing the Huber quantile loss;
15:
end if
16:
end while
17: end for

C

Evaluation

We study the performance of RiskQ on risk-sensitive games (Multi-agent cliff and Car following
games), the StarCraft II Multi-Agent Challenge benchmark (SMAC) [16]. RiskQ can obtain promising
performance for risk-sensitive and risk-neutral scenarios. Ablation studies reveal the importance of
adhering to the RIGM principle for achieving good performance. Additionally, we have examined
the impact of functional representations, distribution utilities, risk metrics and risk levels.
In this secion, we seek to answer the following questions.
1. What are the benefits brought by RiskQ?
2. What will happen if we do not satisfy the RIGM principle?
3. What’s the impact of representation limitations for RiskQ?
4. What are alternative designs for RiskQ?
We answer these questions through.
1. Evaluating RiskQ with risk-sensitive environments (Sec. C.2 C.3 C.4), and evaluating RiskQ
with risk-neutral environment (Sec. C.4). We find that RiskQ can obtain promising results for
all these environments. And we show that RiskQ allow user specify different risk-sensitive
object to optimize a different goal (Figure 9).
2. We evaluate several cases where some agents act according to another risk-metric in Figure 14. We show that it is necessary to for agent act consistently according to the same
risk-metric.
23

3. We evaluate different variants of RiskQ, which has better representation power in Sec C.6.2.
We show the the representation limitations of RiskQ does not impact its performance
significantly.
4. We evaluate different implementation of the mixer functions and utility functions.
C.1

Experimental Setup

We select three categories of MARL value factorization methods for comparison: (i) Expected value
factorization methods: QMIX [5], QTran [6], QPlex [7], CW QMIX [15], ResQ [8]; (ii) Risk-neutral
stochastic value factorization methods: DMIX [22] and ResZ [8]; (iii) Risk-sensitive stochastic value
factorization methods: RMIX [13] and DRIMA [14]. For robustness, each experiment is conducted
at least 5 times with different random seeds. In general, the configuration of RiskQ follows the setup
of Weighted QMIX and ResQ. By default, the risk metric used in RiskQ is Wang0.75 , indicating a
risk-averse preference.
The work used for comparison is listed as follows.
Table 1: Baseline algorithms

Algorithms

Brief Description

2

Facilitates a monotonic combination of individual agent utilities.
Learns a mixer of advantage functions and state value functions.
A centrally-weighted version of QMIX
Converts the joint value function/distribution into main function plus
residual function.
Integrates distributional RL with QMIX
Incorporates CVaR-optimized policies into QMIX, using present and
past observations as supplementary inputs for each RMIX agent.
Separates cooperation risk from environmental risk, employing IQN
as each agent’s utility.

QMIX [5]
QPlex3 [7]
CW QMIX4 [15]
ResQ5 / ResZ [8]
DMIX6 [22]
RMIX7 [13]
DRIMA8 [14]

We implement each algorithm based on their open-source repositories to carry out performance
analyses, with hyperparameters consistent with those in PyMARL. RiskQ is also developed within
the PyMARL framework, following the setup of WQMIX and ResQ. For DRIMA, the default
configuration in standard scenarios is cooperative risk-seeking and environmental risk-neutral. For
RiskQ, unless otherwise specified, the following default configuration is adopted: W ang0.75 is used
as the risk measurement. QR-DQN is used to model per-agent’s stochastic utility, and the quantile
number is set to 32. The RMSProp optimizer is employed with a learning rate of 0.001. Batch
size and buffer size are set to 32 and 5000, respectively. RiskQ uses TD-lambda learning with
λ = 0.6. The ϵ used in ϵ-greedy annealed from 1 to 0.05 within 100K time steps. For robustness,
each experiment is conducted at least 5 times with different random seeds. Experiments are carried
out on a clusters consists of multiple NVIDIA GeForce RTX 3090 GPUs.
C.2

Multi-Agent Cliff Navigation

As depicted in Figure 2, in Multi-Agent Cliff Navigation (MACN) which was introduced in [13],
two agents must navigate in a grid-like world to reach the goal without falling into cliff. The agent
receives a -1 reward at each time step. If any agent reaches the goal individually, a -0.5 reward is given
to the agents. They will receive 00 reward if they reach the goal together. An episode is finished once
the goal is reached by two agents together or any agent falls into cliff (reward -100). We depicted the
test return of each learning algorithm in two MACN scenarios: 4 × 11 grid map and 4 × 15 grid map.
2

https://github.com/oxwhirl/pymarl
https://github.com/wjh720/QPLEX
4
https://github.com/oxwhirl/wqmix
5
https://github.com/xmu-rl-3dv/ResQ
6
https://github.com/j3soon/dfac
7
https://github.com/yetanotherpolicy/rmix
8
https://github.com/osilab-kaist/
3

24

Here, we provide some additional baseline algorithms for performance comparison. In this
environment, we employed the default configuration of RiskQ. For DRIMA, We considered
two configurations in detail, namely DRIMA_sn
and DRIMA_sa. DRIMA_sn represents a configuration that adopts a cooperative risk-seeking
and environmental risk-neutral policy, while Start
Cliff
Goal
DRIMA_sa implies a configuration utilizing a
Figure 2: Multi-Agent cliff Navigation
cooperative risk-seeking and environmental riskneutral policy. Cooperative risk-seeking preference is adopted due to the collaborative nature of the MACN environment, as described in DRIMA,
where cooperative risk-seeking can promote cooperation among agents. However, MACN is an environment with risks, thereby we hold preference for being neutral and averse towards environmental
risk, respectively. As illustrated in Figure 3 (a) and (b), RiskQ achieves the optimal performance in
both scenarios, outperforming risk-sensitive methods: RMIX, DRIMA_sa and DRIMA_sn. This
indicates that learning policies which satisfy the RIGM principle could lead to promising results.
RiskQ

QMIX

0

CW QMIX

ResQ

ResZ

10
20

20

30
40

RMIX

DRIMA_sn

DRIMA_sa

(b) MACN (4 x 15)

30
40
50

50
60

DMIX

0
10

Return

Return

QPLEX

(a) MACN (4 x 11)

100K

200K

300K

Environmental Steps

400K

60

500K

200K

400K

600K

Environmental Steps

800K

1M

Figure 3: The return of two MACN scenarios with different map sizes.

C.3

Multi-Agent Car Following game

We design Multi-Agent Car Following
(MACF)
Game, which is adapted from
Reward
Agent 2:
Agent 1:
(𝑟 < 0)
the single agent risk-sensitive environment [35]. As depicted in Figure 4, in
MACF, there are two agents, each conGoal: Two cars
trolling one car, with the task of one car
reached the finish
line.
following the other to reach a goal. Each
car can observe the current position and
speed of other cars within its observation range. The agent has a fixed action
space which determines its acceleration.
At each time step, agent will receive a
negative reward. And the cars will crash
with some probability if their speed exOther:
1
Observation: [𝑠𝑝𝑒𝑒𝑑, 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒] Action: -1
ceed a speed-threshold, and a negative
reward is given to the agents. To adapt
Figure 4: Multi-Agent Car Following Game
the game to cooperative MARL, agents
move within each other’s observation
will receive a positive reward. Once the agents reach the goal together, a big reward is given to them
and the episode is terminated.
① Car 1 in Car 2’s field of view; ② reached the finish line (𝑟 > 0)
① Two cars collided; ② Timestep cost; ③ Distance to finish

Finish line

𝑠𝑝𝑒𝑒𝑑

-0.9

…

0

…

0.9

Acceleration increases

In MACF, rewards stem from the following sources:
1. Individual rewards when a single car reaches its destination.
2. Collaborative rewards when two cars arrive at their destination together.
25

3. Agents seeing each other within theirs observation range.
4. Negative reward at each time step.
5. Rewards based on the distance between cars to the destination.
6. Crash cost.
The configuration of these rewards impacts the environment’s preference for different policies. For
the MACF environment, we developed two different scenarios for evaluation: a pessimistic scenario
and an optimistic scenario. The source code of the MACF environment can be obtained from the
对应不同环境的奖励设置，以及小车应该采取不同的动作
对应不同环境的奖励设置，以及小车应该采取不同的动作
对应不同环境的奖励设置，以及小车应该采取不同的动作
supplementary
materials.

Reward
Reward
Setting
RewardSetting
Setting
①
Car
11in
Car
2’s
①①
Car
Car
2’s
field
Car
1inin
Car
2’sfield
field
view:
+2.5
ofofof
view:
view;+2.5
+2.5
②②
Reached
the
goal.
②
Reached
the
goal.
Reached
the
goal.
Each
car:
+10
Each
car:
+10
Each
car:
+10
All
cars:
+20
All
cars:
+20
All
cars:
+20
③③
Collisions:
-2.5
③
Collisions:
-2.5
Collisions.
-2.5
④④
Timestep
cost:-1
-1
④
Timestep
Episodecost:
cost.
-1
⑤⑤
Distance
finish:
X1
⑤
Distance
tototo
finish:
Distance
finish:X1
X1

To avoid a collision, I will
To
To avoid
avoid aa collision,
collision, I will
keep speed low and drive
not
not drive
drive too
too fast.
fast.
carefully.

RewardSetting
Setting
Reward
①Car
field
Car11ininCar
Car2’s
2’sfield
field
①
view: +1
+1
ofview;
of

S𝑡𝑎𝑦
𝑏𝑎𝑐𝑘
S𝑡𝑎𝑦
S𝑡𝑎𝑦𝑏𝑎𝑐𝑘
𝑏𝑎𝑐𝑘

Reachedthe
thegoal.
goal.
②Reached
②
Eachcar:
car:+5
+5
Each

Speed Up
Up
Speed
Up
Speed

Allcars:
cars:+10
+10
All
Collisions:-1-1
③Collisions.
③

will keep
keep distance
from
III will
distance from
Car1
and
also
keep
speed
Car1
speed
to
Car1 and
andalso
alsokeep
keep
speed
avoid
collision.
avoid
low tocollision.
avoid collision.

Timestep
cost:
④Episode
④
cost.
-1 -1
⑤Distance
Distancetotofinish:
finish:X1
X1
⑤

pessimistic
scenarios
pessimistic
scenarios
pessimistic
scenarios
Figure
5: pessimistic
scenario

I'lltrytry
trytoto
to
keep
a moderate
moderate
keep
I'llI'll
keep
a amoderate
pace
to
balance
speed
and
pace
balance
speed
and
pace
toto
balance
speed
and
safety.
safety.
safety.
𝑓𝑜𝑙𝑙𝑜𝑤
𝑓𝑜𝑙𝑙𝑜𝑤
𝑓𝑜𝑙𝑙𝑜𝑤

Slow
Slow
Slow

II need
keep
with
Car1
need
to
keep
up
with
Car1
I need
toto
keep
upup
with
Car1
while
trying
maintain
whiletrying
trying
to
maintain
while
toto
maintain
a aa
moderate
speed
moderate
speed
moderate
speed

optimistic
scenarios
optimistic
scenarios
Figure 6:
optimistic
scenario
optimistic
scenarios

In the pessimistic scenario, the reward settings, as shown in Figure 5, are adjusted to have a crash
probability of 1.0 after exceeding the speed limit. In such an environment, agents should learn
policies that can reach the destination as quickly as possible, while ensuring safety. RiskQ in this
scenario adopts CV aR0.2 as the risk measurement, which represents a risk-averse preference. To
ensure fairness, we adjusted the environmental risk of DRIMA to be averse. We have implemented
risk-averse mode for DRIMA, then we conducted experiments of DRIMA with both risk-seeking and
risk-neutral cooperative risk. From the results shown in Figure 7 (a) and (b), it is evident that RiskQ
outperforms other approaches in terms of both return and crash.
In the optimistic scenario, the reward settings, as depicted in Figure 6, are defined such that the
probability of a crash is 0.4 × the_amout_of _overspeed. The settings in this environment requires
cars to achieve a balance between speed and safety. RiskQ in this scenario adopts CV aR1.0 as the
risk measurement, which represents a risk-neutral risk preference. To ensure fairness, we adjusted
the environmental risk of DRIMA to be neutral. We conducted experiments with both risk-seeking
and risk-neutral cooperative risk. From the results shown in Figure 7 (c) and (d), it is evident that
RiskQ outperforms other approaches in terms of both return and crash.
C.4

StarCraft II

We examine the performance of RiskQ and several other algorithms using the StarCraft II Multi-Agent
Challenge (SMAC), a widely recognized benchmark in the field of MARL. The SMAC environment
consists of two competing teams of agents engaged in combat scenarios. One team is controlled
by the carefully handcrafted built-in game artificial intelligence, the other team comprises agents
guided by decentralized policies learned through MARL algorithms. Consistent with many prior
works [8, 13], we set the AI level for controlling the enemy team in SMAC to 7 (very difficult).
Each agent possesses a circular observation range and can engage in close-range combat with nearby
enemies. At each time step, the agents make decisions to either move or perform actions related
to attacking or healing. The rewards are affected by the damage inflicted on enemy units, with an
additional reward given for eliminating all enemy units. The highest achievable reward is normalized
to 20. We use the default reward schema provided by the SMAC benchmark to maintain consistency
across experiments.
Following the evaluation protocol of DRIMA for risk-averse SMAC, we first study the performance
of RiskQ in an explorative, a dilemmatic, and a random setting for the 3s_vs_5z, MMM2, and
2c_vs_64zg scenario. This setting is described as follows.
26

RiskQ

QMIX

QPLEX

CW QMIX

ResQ

ResZ

RMIX

DRIMA

300
250
200
150
100
50
0
200K

400K

600K

Environmental Steps

800K

200K

1M

Crash

0
2500
5000
7500
10000
12500
15000
17500
20000
250K

500K

750K

1.0M

Environmental Steps

400K

600K

Environmental Steps

800K

1M

(d) MACF Optimistic Scenario

(c) MACF Optimistic Scenario

Return

DMIX

(b) MACF Pessimistic Scenario

350

0
2000
4000
6000
8000
10000
12000
14000

Crash

Return

(a) MACF Pessimistic Scenario

1.25M

350
300
250
200
150
100
50
0
250K

1.5M

500K

750K

1.0M

1.25M

Environmental Steps

1.5M

Figure 7: The return and the average number of crashes of two MACF scenarios. (a) Return of the pessimistic
scenario. (b) The average number of crashes of the pessimistic scenario. (c) Return of the optimistic scenario.
(d) The average number of crashes of the optimistic scenario.

0.2

1.0
0.8
0.6

400K

800K

1.2M

Environmental Steps

1.6M

0.4
0.2

(d) MMM2 (explorative)

1.0

RiskQ
QMIX
QPLEX
ResQ
ResZ
DMIX
RMIX
DRIMA

0.8

0.2
400K

800K

1.2M

Environmental Steps

1.6M

2M

0.6

400K

800K

1.2M

Environmental Steps

1.6M

0.6
0.4

0.0

2M

400K

800K

1.2M

Environmental Steps

1.6M

2M

(f) MMM2 (random)

(e) MMM2 (dilemmatic)
RiskQ
QMIX
QPLEX
ResQ
ResZ
DMIX
RMIX
DRIMA

RiskQ
RMIX
DRIMA
QMIX
DMIX
RESQ
RESZ

1.0

0.4
0.2
0.0

0.8

RiskQ
RMIX
DRIMA
QMIX
DMIX
RESQ
RESZ

0.2

0.0

2M

0.4

0.0

0.6

1.0

Test Win Rate

0.4

0.0

Test Win Rate

0.8

(c) 3s_vs_5z (random)

(b) 3s_vs_5z (dilemmatic)
RiskQ
QMIX
QPLEX
ResQ
ResZ
DMIX
RMIX
DRIMA

Test Win Rate

0.6

1.0

Test Win Rate

Test Win Rate

0.8

(a) 3s_vs_5z (explorative)
RiskQ
QMIX
QPLEX
ResQ
ResZ
DMIX
RMIX
DRIMA

Test Win Rate

1.0

0.8
0.6
0.4
0.2

400K

800K

1.2M

Environmental Steps

1.6M

2M

0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

Figure 8: Performance Comparison in Explorative, Dilemmatic, and Random SMAC Scenarios.

1. Explorative: In the explorative setting, agents behave heavily exploratory during training,
thus they must consider the risk brought on by heavily exploration of other agents. In this
setting, the annealing time is change to 500K following the setting of QMIX [5].
2. Dilemmatic: In the dilemmatic setting, agents should consider risk to prevent the learning
of locally optimal policies. In this setting, agent could receive negative rewards which is
based on damage dealted to our agents.
3. Random: In this setting, one agent performs random actions 50% of the time.
As depicted in Figure 8, RiskQ obtains the best performance for the explorative, dilematic, and
random 3s_vs_5z. It obtains the best performance for the dilemmatic MMM2. For the explorative
MMM2 scenario, although RiskQ learn slowly in the beginning, it obtain the best performance in the
end. Combining previous results from the MACN and the MACF environments, we can conclude
that RiskQ can yield promising results in environments that require risk-sensitive cooperation.
In the standard SMAC scenarios, we compared the performance of RiskQ with various baseline
algorithms. The StarCraft II version used in this study is SC2.4.6, which is consistent with QMIX [5],
WQMIX [15], and ResQ [8]. We presented results of six SMAC scenarios (MMM2, MMM,
27

QMIX

0.6
0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

Test Win Rate

Test Win Rate

0.8
0.6
0.4
0.2
200K

400K

600K

Environmental Steps

800K

0.2
200K

400K

600K

Environmental Steps

800K

0.6
0.4
0.2
1.2M

Environmental Steps

DRIMA
(c) 2c_vs_64zg

0.6
0.4
0.2
400K

1.6M

1.2M

1.6M

2M

1.6M

2M

0.8
0.6
0.4
0.2
0.0

2M

800K

Environmental Steps

(f) 27m_vs_30m

1.0

0.8

800K

RMIX

0.8

0.0

1M

(e) 3s_vs_5z

400K

DMIX
1.0

0.4

0.0

1M

ResZ

0.6

1.0

1.0

ResQ

(b) MMM

0.8

0.0

2M

(d) 2s_vs_1sc

0.0

CW QMIX

Test Win Rate

0.8

0.0

QPLEX
1.0

Test Win Rate

Test Win Rate

RiskQ(Exp)
(a) MMM2

Test Win Rate

RiskQ
1.0

400K

800K

1.2M

Environmental Steps

Figure 9: The test win rate of different algorithms (including RiskQ(Exp) which acts based on the expectation of
the value distribution) in SMAC scenarios.

QPLEX

0.8
0.6
0.4
0.2
0.0

200K

400K

600K

Environmental Steps

CW QMIX

800K

1M

ResQ

ResZ

DMIX

DRIMA

(c) bane_vs_bane
1.0

0.8
0.6
0.4
0.2
0.0

RMIX

(b) 8m_vs_9m

1.0

Test Win Rate

Test Win Rate

QMIX

(a) 1c3s5z

Test Win Rate

RiskQ
1.0

200K

400K

600K

Environmental Steps

800K

1M

0.8
0.6
0.4
0.2
0.0

200K

400K

600K

Environmental Steps

800K

1M

Figure 10: The test win rate of different algorithms for the SMAC benchmark.

2c_vs_64zg, 2s_vs_1sc, 3s_vs_5z, 27m_vs_30m) in Figure 9, revealing that not only can RiskQ
achieve state-of-the-art performance in these scenarios, but RiskQ(Expectation) also exhibits decent
performance. The gap between RiskQ and RiskQ(Expectation) underscores the importance of considering risk. Here we provide additional results for three additional scenarios in Figure 10. As depicted
in Figure 10, RiskQ achieves competitive performance in the 1c3s5z, 8m_vs_9m, and bane_vs_bane
scenarios.
Additionally, we compared the performance between RiskQ and DRIMA that configured with
cooperative risk-seeking and environmental risk-seeking settings. The experimental results are
illustrated in Figure 12 (left). The result reveals that RiskQ(Expectation) can achieve performance on
par with DRIMA and DRIMA-SS. The consideration of risk in RiskQ allows for further performance
enhancement, thereby emphasizing the superiority of RiskQ’s performance and the importance of
risk consideration.
Moreover, since the optimization metrics of RiskQ are not expectations, it is not fair to compare only
the expectation metrics with other baselines. We compare the Wang 0.75 metric for return distribution
and win rate distribution in two standard SMAC scenarios. As IQN [20] does not guarantee to
converge to the true distribution, we want to know whether the algorithm is optimizing the risk metric
correctly. The experimental results in Figure 11 indicate that the risk-sensitive objective optimized by
RiskQ is learning gradually with time.
All the SMAC experiments were conducted using the SC2.4.6 version of StarCraft II. However,
as noted in WQMIX, "performance is not comparable across versions". Therefore, we conducted
experiments using the SC2.4.10 version of StarCraft II to compare the performance of RiskQ with
ResQ, ResZ, and fine-tuned QMIX [28] in the MMM2 scenario. As depicted in Figure 12 (right),
RiskQ maintains strong performance even in the SC2.4.10 version. This suggest that RiskQ can
obtain promising results in SC2.4.10 as well.
28

3s_vs_5z

20.0
17.5
15.0

RiskQ
RMIX
DRIMA
ResQ

Test Return (Wang075)

Test Return (Wang075)

MMM2

12.5
10.0
7.5
5.0
2.5
0.0

400K

800K

1.2M

Environmental Steps

1.6M

20

RiskQ
RMIX
DRIMA
ResQ

15
10
5
0

2M

200K

400K

0.8
0.6
0.4
0.2
0.0

400K

800K

1.2M

800K

1M

800K

1M

3s_vs_5z

RiskQ
RMIX
DRIMA
ResQ

Test Win Rate (Wang075)

Test Win Rate (Wang075)

MMM2
1.0

600K

Environmental Steps

Environmental Steps

1.6M

1.0
0.8

RiskQ
RMIX
DRIMA
ResQ

0.6
0.4
0.2
0.0

2M

200K

400K

600K

Environmental Steps

Figure 11: The Wang 0.75 metric for return distribution and win rate distribution in standard SMAC: MMM2
(Left) and 3s_vs_5z (Right).

3s_vs_5z

1.0

0.8

Test Win Rate

Test Win Rate

1.0

0.6
0.4
RiskQ
RiskQ(Expectation)
DRIMA
DRIMA-SS

0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

0.8
0.6

MMM2
RiskQ(SC_2.4.6)
RiskQ(SC_2.4.10)
ResQ(SC_2.4.6)
ResQ(SC_2.4.10)
QMIX(SC_2.4.6)
QMIX(SC_2.4.10)
Pymarl2_QMIX(SC_2.4.6)
Pymarl2_QMIX(SC_2.4.10)

0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

Figure 12: A performance comparison between RiskQ and DRIMA with different configurations under the
3s_vs_5z scenario, and the performance of algorithms with different StarCraft II versions.

C.5

SMACv2

SMACv2 [54] is a new version of SMAC with improved stochasticity, which can better demonstrate
RiskQ’s ability to handle uncertainty. We verified the performance of RiskQ against other baselines
in different SMACv2 scenarios, and the results are shown in Figure 13.
C.6
C.6.1

Ablation Study
Impact of not satisfying the RIGM principle

To investigate the reasons behind RiskQ’s promising results, we analyze different designs of RiskQ
on the SMAC, MACN, and MACF scenarios. First, we study the necessity of satisfying the RIGM
principle by making about 50% of RiskQ agents follow different risk measures. In Figure 14, w/o
RIGM VaR1 , w/o RIGM CVaR1 and w/o RIGM CVaR0.2 indicate that about 50% of agents act
according to the VaR1 (risk-seeking), CVaR1 (risk-neutral) and the CVaR0.2 (risk-averse) metrics,
respectively. These risk measures are not the risk measure ψα = Wang0.75 used by other agents. As
depicted in Figure 14 (a-c) and (e-f), RiskQ performs poorly in all the three cases, highlighting the
importance of satisfying the RIGM principle that agents act according to the same risk measure.
The performance of these three cases is comparable to that of RiskQ in the 2s_vs_1sc scenario. We
29

terran_5_vs_5

18

18

16

16

14

14

12

Return

Return

protoss_5_vs_5

RiskQ
RMIX
DRIMA
QMIX
DMIX
ResQ
ResZ

10
8
400K

800K

1.2M

Environmental Steps

1.6M

RiskQ
RMIX
DRIMA
QMIX
DMIX
ResQ
ResZ

12
10
8

2M

400K

800K

14

14

12

12

RiskQ
RMIX
DRIMA
QMIX
DMIX
ResQ
ResZ

10
8
200K

400K

600K

Environmental Steps

1.6M

2M

terran_3_vs_3
16

Return

Return

protoss_3_vs_3
16

6

1.2M

Environmental Steps

800K

10

RiskQ
RMIX
DRIMA
QMIX
DMIX
ResQ
ResZ

8
6

1M

200K

400K

600K

Environmental Steps

800K

1M

Figure 13: The test return mean of different algorithms in different SMACv2 scenarios.

speculate that this may be due to the fact that 2s_vs_1sc is a simple environment with fewer agents,
and therefore, the differences in performance are not apparent.

0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

0.4
RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

400K

800K

1.2M

Environmental Steps

1.6M

Return

0.6
0.4
RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

0.2
200K

400K

600K

Environmental Steps

800K

1M

0.6
0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

2M

(f) MACF Pessimistic Scenario
0
5000

20
30
40
RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

50
60

0.8

0.0

2M

10

0.8

RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

(e) MACN (4 x 15)

0

1.0

Test Win Rate

0.6

0.0

2M

(d) 2s_vs_1sc

0.0

0.8

0.2

(c) 2c_vs_64zg

1.0

Test Win Rate

0.6

0.0

(b) 3s_vs_5z

1.0

Return

0.8

(a) MMM2
RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

Test Win Rate

Test Win Rate

1.0

200K

400K

600K

Environmental Steps

800K

1M

10000
15000
20000

RiskQ
w/o RIGM VaR1
w/o RIGM CVaR1
w/o RIGM CVaR0.2

25000
200K

400K

600K

Environmental Steps

800K

1M

Figure 14: Ablation Study on why the RIGM principle matters.

As shown in Fig 14, not satisfying RIGM could lead to significant performance drop. We will further
explain the usefulness of RIGM further through an example and corresponding empirical results.
In cooperative MARL, the optimal action of an agent depends on the actions executed by others.
As an example for MARL combat scenarios, depicted in Fig 15, if an agent has doubts about its
teammates and leans towards a pessimistic outlook, it may evade rather than confront the enemies.
However, with RIGM, all the agents can embrace a risk-seeking strategy. This promotes the agent to
adopt a more optimistic perspective on its teammates, which in turn promotes enhanced collaboration.
Hence, the incorporation of RIGM is crucial. The DIGM principle can not be used for such scenarios
which require risk-sensitive policies.
To empirically study the example, we consider the following two non-RIGM cases on the 3s_vs_5z
map of the SMAC benchmark.
30

1. Each agent acts according to different risk-metrics for their actions. The joint optimal action
is derived from each agent’s individual risk-based optimal action, representing a non-RIGM
approach. Specifically, we assign different risk-metrics to each agent: VaR 1.0, CVaR
1.0, and Wang 0.75, corresponding to risk-seeking, risk-neutral and risk-averse policies,
respectively.
2. In the second case, each agent uses the same risk metric as before. Different the first case,
when determining the approximated optimal joint action, we uniformly apply Wang 0.75 as
the risk metric.
The empirical results are shown in Fig 15, ’Case 1’ and ’Case 2’ correspond to the first and the second
cases, respectively. The results indicate that not satisfying RIGM could lead to performance drop.
_

I don’t trust my teammates

Attack
Agent 3

Not RIGM

Agent 2
_

0.8

Enemies

I trust my teammates.

Enemies

Agent 3
_

0.6
0.4
0.2

Attack

Agent 1
_

Agent 2

Test Win Rate

_

3s_vs_5z

1.0

_

Agent 1

0.0

RIGM

RiskQ
w/o RIGM Case_1
w/o RIGM Case_2

400K

800K

1.2M

Environmental Steps

1.6M

2M

Figure 15: Without RIGM, agents might choose to run away (the upper part of Fig(Left)). With RIGM, agents
act in a consistent way. (the lower part of Fig(Left))). Fig(Right): RiskQ performs significantly better than two
cases that do not satisfy the RIGM principle.

C.6.2

Impact of representation limitations

By modeling the percentile of joint return distribution θ(ω) as the weighted sum of the percentiles
θi (ω) of each agent’s distribution, RiskQ suffers from representation limitations. To study whether
the representation limitations impact the performance of RiskQ, we design three variants of RiskQ:
RiskQ-QMIX, RiskQ-Residual, and RiskQ-Residual-QMIX.
1. RiskQ-QMIX (Sec. B.3): RiskQ-QMIX models the monotonic relations among θi (ω) and
θ(ω) in the manner of QMIX [5]. We study its performance with respect to two risk metrics:
VaR0.2 and VaR0.6 .
2. RiskQ-Residual (Sec. B.4): RiskQ-Residual models the joint value distribution without
representation limitations by using residual functions [8]. We study its performance for the
risk metric Wang0.75 .
3. RiskQ-Residual-QMIX(Sec. B.5): RiskQ-Residual-QMIX combines RiskQ-Residual and
QMIX. We study its performance with respect to two risk metrics: VaR0.2 and VaR0.6
RiskQ-QMIX and RiskQ-Residual-QMIX satisfy the RIGM principle for the VaR metric, and RiskQResidual satisfies the RIGM for the VaR and DRM metrics. As it is shown in Figure 16, albeit these
variants have better representation ability, their performance is unsatisfied for the MMM2, 3s_vs_5z,
2s_vs_1sc, and 2c_vs_64zg. This suggest that the representation limitation does not significantly
impact the performance of RiskQ.
C.6.3

Impact of different designs of RiskQ mixers

Moreover, in Figure 17, we analyze different designs of the RiskQ mixer through two variants:
RiskQ-Sum and RiskQ-Qi.
1. RiskQ-Sum: RiskQ-Sum
Pn models the percentile θ(τ , u, ω) as the sum of the percentiles of
per-agent’s utilities i=1 θi (τi , ui , ω).
31

(a) MMM2

0.8
0.6
0.4
0.2
0.0

400K

800K

1.2M

1.6M

Environmental Steps

(c) 2s_vs_1sc

Test Win Rate

Test Win Rate

0.6
0.4

RiskQ
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

0.2
400K

0.6
0.4

600K

800K

Environmental Steps

RiskQ
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

0.2

1.0

0.8

200K

0.8

0.0

2M

1.0

0.0

(b) 3s_vs_5z

1.0

RiskQ
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

Test Win Rate

Test Win Rate

1.0

1M

0.8
0.6

400K

800K

1.2M

Environmental Steps

1.6M

2M

1.25M

1.5M

(d) 2c_vs_64zg
QRWang075TDLambda06_small
RiskQ-QMIX-VaR0.2
RiskQ-QMIX-VaR0.6
RiskQ-Residual-Wang0.75
RiskQ-Residual-QMIX-VaR0.2
RiskQ-Residual-QMIX-VaR0.6

0.4
0.2
0.0

250K

500K

750K

1.0M

Environmental Steps

Figure 16: Study on whether the representation limitations impact the performance of RiskQ.

2. RiskQ-Qi: RiskQ-Qi represents each percentile θ(τ , u, ω) as the expectation of state-action
function.
As shown in Figure 17, both two variants perform inferior to RiskQ in most cases.

0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

0.2
800K

1.2M

Environmental Steps

1.6M

Return

0.6
0.4
0.2

RiskQ
RiskQ-sum
RiskQ-Qi

200K

400K

600K

Environmental Steps

800K

1M

0.6
0.4
0.2
400K

800K

1.2M

Environmental Steps

1.6M

2M

(f) MACF Pessimistic Scenario
0
2500

20

5000

30
40

7500
10000
12500

50
60

0.8

0.0

2M

RiskQ
RiskQ-sum
RiskQ-Qi

10

0.8

RiskQ
RiskQ-sum
RiskQ-Qi

(e) MACN (4 x 15)

0

1.0

Test Win Rate

0.4

400K

(c) 2c_vs_64zg

1.0

0.6

0.0

2M

(d) 2s_vs_1sc

0.0

0.8

(b) 3s_vs_5z
RiskQ
RiskQ-sum
RiskQ-Qi

Test Win Rate

1.0

Return

0.8

(a) MMM2
RiskQ
RiskQ-sum
RiskQ-Qi

Test Win Rate

Test Win Rate

1.0

15000

200K

400K

600K

Environmental Steps

800K

1M

RiskQ
RiskQ-sum
RiskQ-Qi

17500
200K

400K

600K

Environmental Steps

800K

1M

Figure 17: Analysis of different designs of RiskQ mixer.

C.6.4

Impact of different utility functions

IQN [20] and QR-DQN [19] suffer from the crossing quantile issues that a low quantile V aR0.2 could
be larger than a high quantile V aR0.5 due to its limitation. To address this limitations in MARL, we
explore the following variants.
1. QR (sorted): This method models the distribution by fixing the quantiles in the style of
QR-DQN [19], and then sorts the quantiles to avoid the quantile-crossing issues.
2. QR (unsorted): This method models the distribution by fixing the quantiles in the style of
QR-DQN [19], but does not sort the quantiles.
3. IQN (sorted): This method models the distribution by sampling the quantiles in the way of
IQN [20], and sorts the quantiles to ensure a monotonic increasing order of the quantiles.
4. IQN (unsorted): This method models the distribution by sampling the quantiles in the way
of IQN [20], but does not sort the quantiles.
32

0.8

(a) 2c_vs_64zg

1.0

QR(sorted)
QR(unsorted)
IQN(sorted)
IQN(unsorted)
QR(non-crossing)

Test Win Rate

Test Win Rate

1.0

0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

0.8

(b) MMM2
QR(sorted)
QR(unsorted)
IQN(sorted)
IQN(unsorted)
QR(non-crossing)

0.6
0.4
0.2
0.0

400K

800K

1.2M

Environmental Steps

1.6M

2M

Figure 18: Analysis of different implementations of distribution utility.

5. QR (non-crossing): This method adopts the non-crossing quantile regression to model the
distribution as proposed in [55].
As illustrated in Figure 18, we explored several different implementations of distribution utility. QR
(sorted) has promising results. So we implement the utility function of RiskQ using QR (sorted).

D

Discussion

D.1

Is risk-sensitive RL related to safe RL?

Risk management within RL can be roughly divided into two categories: safe and constrained RL, and
distributional risk-sensitive RL. Safe and constraint RL formulates the risk as some kind of constraints
within the policy optimization problem. Safe RL is often modeled as a Constrained Markov Decision
Process (CMDP)[56], in which the target is to maximize the agent reward while making agents satisfy
safety constraints. For instance, [57] proposes a Lagrangian method that provides a theoretical bound
on cost function while optimizing the policy; [58] employs the Lyapunov approach to systematically
convert dynamic programming and RL algorithms into their safe versions.
However, in complicated real-world applications, the form of risk may either be too complex or the
constraints are hard to be explicitly formulated, safe-RL algorithms can be challenging to learn. In
that case, distributional RL provides a way to utilize risk measures upon the return distributions for
risk-sensitive learning. IQN[20] considers the risk as the distortion risk measures upon sampling
distributions to realize risk-sensitive policies. [59] learns a full return distribution for its critic and
optimizes the actor’s policy according to a risk-related metric (such as CVaR) for offline RL.

D.2

Which behaviors in standard SMAC scenarios can potentially involve risk?

Risk refers to the uncertainty in MARL. It is not refer to events that will incur negative outcomes.
In the SMAC scenarios, when an agent performs an action, there are three possible sources of risk:
observation uncertainty, environmental uncertainty, and agent policy uncertainty. In standard SMAC,
as each agent can only access partial observation, their policies contain more randomness than policies
with access to the whole state. Environmental uncertainty is caused by the stochasticity of state
transitions and the reward function. Agent policy uncertainty comes from the learning process of
agents and their exploratory behaviors (e.g., epsilon-greedy). In the standard SMAC, we find that
employing a risk-averse policy (Wang 0.75) can lead to better performance. This could be attributed
to the reason that the risk-averse policy incorporates a strategy to stay alive longer; this is correlated
with the game’s objective of eliminating all opponents while staying alive.
In a standard game environment, it is uncommon that using risk-sensitive policies can lead to better
performance than risk-neutral policies. For example, IQN[20] finds that risk-averse policies can
obtain better results than risk-neutral counterparts in Atari game environments (e.g., ASTERIX and
ASSAULT). Furthermore, they also find that risk-seeking policies can obtain better performance in 3
out of 6 standard games.
33

D.3

Can RiskQ make the learned CDF function converge to the real distribution?

RiskQ uses implicit quantile network (IQN) [20] as the basis to learn value distribution, so it shares
the same converge property as IQN.
As stated in Chapter 7.5 and 7.6 of [50] and Theorem 4.3 of [51], the greedy distributional Bellman
update operator of IQN is not a contraction mapping. This is an inherent drawback of the distributional
RL. However, the approximation error of some risk metrics can be made arbitrary small by increasing
the number of statistics (Theorem 4.7 of [51]) for the policy evaluation case.
As we know, Lim and Malik [52] proposes a modification to IQN with a new distributional Bellman
operator recently. They show that the optimal CVaR policy corresponds to a fixed point of the new
operator. However, it is unclear whether it converges in general settings. We have combined RiskQ
with [52] by replacing IQN with it. As shown in Figure 19, the new method, LimAndMalki, performs
poorly. This suggests that remedying the non-contraction mapping issues may not be important
enough for performance improvement as existing risk-sensitive RL methods (e.g., IQN) are already
working well.

3m

Return

Return

MMM
20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0

RiskQ
LimAndMalki22

200K

400K

600K

Environmental Steps

800K

20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0

1M

RiskQ
LimAndMalki22

100K

200K

300K

Environmental Steps

400K

500K

Figure 19: Replacing the IQN (Dabney et al. ICML 2018) used by RiskQ with a risk-sensitive algorithm (Lim
and Malik NeurIPS 22) whose optimal CVaR policy corresponds to a fixed point of it.

D.4

In what way can risk distortion actually be used for exploration-exploitation trade-off?

Likelihood Quantile Networks (LQN) [53] assigns various learning rates for samples (from agent
exploration or suboptimality) based on likelihoods, which measures return distribution differences.
For exploration, LQN gradually anneals the parameters of its risk-based action selection strategy with
time.
We have combined RiskQ with the LQN riks-parameters annealing procedure, and call it as
RiskQ+LQN. As is depicted in Fig. 20, RiskQ+LQN performs slightly weaker than RiskQ. This
suggests that besides considering the uncertainty for exploration, coordinated exploration may be
needed as well. It also provides a new direction for future work.

Test Win Rate

0.8

MMM2

0.8

0.6
0.4
0.2
0.0

3s_vs_5z

1.0

RiskQ
RiskQ+LQN(Wang)
RiskQ+LQN(VaR)

Test Win Rate

1.0

0.6
0.4
0.2

400K

800K

1.2M

Environmental Steps

1.6M

2M

0.0

RiskQ
RiskQ+LQN(Wang)
RiskQ+LQN(VaR)

400K

800K

1.2M

Environmental Steps

1.6M

2M

Figure 20: Combining RiskQ with the risk-parameter annealing procedure of likelihood quantile network (Lyu
et al. 2019): MMM2 (Left) and 3s_vs_5z (Right). RiskQ+LQN(Wang) uses the Wang metric and anneals the
risk parameter from 0.75 to -0.75. RiskQ+LQN(VaR) uses the VaR metric, and the risk parameter gradually
anneals from 0.8 to 0.1.

34

