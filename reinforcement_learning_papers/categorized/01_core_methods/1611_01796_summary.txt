Here is the analysis of the paper in the required format:

PAPER: 1611_01796.pdf
TITLE: Modual Multitask Reinforcement Learning with Policy Sketches
ARXIV_ID: 1611.01796v2
RESEARCH_METHOD: 01_core_methods
METHOD_DESCRIPTION: The paper proposes a framework for multitask deep reinforcement learning guided by policy sketches. Policy sketches are short, ungrounded, symbolic representations of a task that describe its component parts. The approach learns modular subpolicies associated with each high-level action symbol and jointly optimizes over concatenated task-specific policies by tying parameters across shared subpolicies.
KEY_CONTRIBUTIONS:
- A general paradigm for multitask, hierarchical, deep reinforcement learning guided by abstract sketches of task-specific policies.
- A concrete recipe for learning from these sketches, built on a general family of modular deep policy representations and a multitask actorâ€“critic training objective.
- The modular structure of the approach naturally induces a library of interpretable policy fragments that are easily recombined, making it possible to evaluate the approach under various data conditions.
- Experiments show that the approach substantially outperforms previous approaches based on explicit decomposition of the Q function along subtasks, unsupervised option discovery, and standard policy gradient baselines.
