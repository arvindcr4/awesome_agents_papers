arXiv:2406.11481v3 [cs.LG] 17 Jul 2024

Constrained Reinforcement
Learning with Average Reward
Objective: Model-Based and
Model-Free Algorithms

Vaneet Aggarwal
Purdue University
vaneet@purdue.edu
Washim Uddin Mondal
Indian Institute of Technology Kanpur
wmondal@iitk.ac.in
Qinbo Bai
Purdue University
bai113@purdue.edu

Contents

1 Introduction
1.1 Chapter Organization . . . . . . . . . . . . . . . . . . . .
1.2 Some Useful Inequalities . . . . . . . . . . . . . . . . . .

3
5
6

2 Model-Based RL
2.1 Overall Model and Assumptions . . . . . . . . . . . . . .
2.2 Algorithms for Model-Based RL . . . . . . . . . . . . . . .
2.3 Regret Analysis and Constraint Violation for Optimism
Based Approach . . . . . . . . . . . . . . . . . . . . . . .
2.4 Regret Analysis and Constraint Violation for Posterior Sampling Based Approach . . . . . . . . . . . . . . . . . . . .
2.5 Evaluation Results . . . . . . . . . . . . . . . . . . . . . .
2.6 Notes and Open Problems . . . . . . . . . . . . . . . . .

8
8
11

3 Parameterized Model-Free RL
3.1 Overall Model and Assumptions . . . . . . . . . . . . . .
3.2 Algorithm for Parameterized Model-Free RL . . . . . . . .
3.3 Global Convergence Analysis . . . . . . . . . . . . . . . .
3.4 Regret Analysis and Constraint Violation Analysis . . . . .
3.5 Notes and Open Problems . . . . . . . . . . . . . . . . .
3.6 Some Auxiliary Lemmas for the Proofs . . . . . . . . . . .

44
44
47
58
73
79
80

17
32
36
41

4 Beyond Ergodic MDPs
4.1 Algorithm for Model-Based RL . . . . . . . . . . . . . . .
4.2 Notes and Open Problems . . . . . . . . . . . . . . . . .

85
85
96

References

98

Constrained Reinforcement
Learning with Average Reward
Objective: Model-Based and
Model-Free Algorithms
Vaneet Aggarwal1 , Washim Uddin Mondal2 and Qinbo Bai1
1 Purdue University; {vaneet, bai113}@purdue.edu
2 Indian Institute of Technology Kanpur; wmondal@iitk.ac.in

ABSTRACT
Reinforcement Learning (RL) serves as a versatile framework
for sequential decision-making, finding applications across
diverse domains such as robotics, autonomous driving, recommendation systems, supply chain optimization, biology,
mechanics, and finance. The primary objective in these applications is to maximize the average reward. Real-world
scenarios often necessitate adherence to specific constraints
during the learning process.
This monograph focuses on the exploration of various modelbased and model-free approaches for Constrained RL within
the context of average reward Markov Decision Processes
(MDPs). The investigation commences with an examination
of model-based strategies, delving into two foundational
methods – optimism in the face of uncertainty and posterior sampling. Subsequently, the discussion transitions
to parametrized model-free approaches, where the primal
dual policy gradient-based algorithm is explored as a solution for constrained MDPs. The monograph provides regret
guarantees and analyzes constraint violation for each of the
discussed setups.

2
For the above exploration, we assume the underlying MDP to
be ergodic. Further, this monograph extends its discussion
to encompass results tailored for weakly communicating
MDPs, thereby broadening the scope of its findings and
their relevance to a wider range of practical scenarios.

1
Introduction

Reinforcement Learning (RL) describes a class of problems where an
agent repeatedly interacts with an unknown environment. The environment possesses a state that changes as a result of the action executed by
the agent according to some pre-determined but unknown probability
law. The environment also generates feedback, which is often called the
reward. The agent’s goal is to choose a sequence of actions (based on the
sequence of observed states and rewards) that maximizes the expected
cumulative sum of rewards obtained via this procedure. This model has
found its application in a wide array of areas, ranging from networking to
transportation to robotics to epidemic control (Geng et al., 2020; Chen
et al., 2023; Al-Abbasi et al., 2019; Manchella et al., 2021; Gonzalez
et al., 2023; Ling et al., 2024). RL problems are typically analyzed via
three distinct setups−episodic, infinite horizon discounted reward, and
infinite horizon average reward. In an episodic setup, the environment
restores its initial state after a certain number of interactions. Examples
include video game-based applications where the learner restarts the
game after either winning or losing it. In a discounted setup, the learner
aims to maximize the expected discounted sum of rewards. The underlying philosophy is that the current reward, in certain applications, is
3

4

Introduction

deemed more valuable than the rewards obtained in the future. This
idea naturally fits into financial applications where the reward (money)
loses value over time due to inflation. The average reward setup, on
the contrary, places both the current and future rewards on the same
footing and aims to maximize the expected average reward computed
over an infinitely long time horizon. The basic premise of the infinite
horizon average reward setup aligns with most practical scenarios due to
its ability to capture essential long-term behaviors. Some applications in
real life require the learning procedure to respect the boundaries of certain constraints. In an epidemic control setup, for example, vaccination
policies must take the supply shortage (budget constraint) into account.
Such restrictive decision-making routines are described by a constrained
Markov Decision Process (CMDP) (Bai et al., 2023c; Agarwal et al.,
2022b; Mondal and Aggarwal, 2024b). This monograph aims to provide
the key approaches to tackle CMDP with an average reward objective.
To gain more insight into CMDPs, consider a wireless sensor network
where a device aims to update a server with its sensed values. At time
t, the sensor can either choose to send a packet which, upon successful
transmission, fetches a reward of one unit or to queue the packet and
obtain a zero reward. However, communicating a packet results in pt
power consumption. The success probability of the intended packet is
decided via a pre-determined but unknown function of pt and the current
wireless channel condition, st . The goal is to send as many packets
P
as possible while keeping the average power consumption, Tt=1 pt /T ,
within some limit, say C. The state of the environment can be described
by the pair (st , qt ) where st , as stated above, is the channel condition,
and qt is the queue length at time t. To limit the power consumption,
the agent may choose to transmit packets when the channel condition is
good or when the queue length grows beyond a certain threshold. The
agent aims to learn the policies in an online manner which requires
efficiently balancing exploration of state-space and exploitation of the
estimated system dynamics (Singh et al., 2020).
Similar to the example above, many applications require keeping
some costs low while simultaneously maximizing the rewards (Altman,
1999). This monograph discusses model-based and model-free algorithms
for the CMDP learning problem described above. A model-based algo-

1.1. CHAPTER ORGANIZATION

5

rithm aims to learn the optimal policy by creating a good estimate of
the state-transition function of the underlying CMDP. The caveat of
the model-based approach is the large memory requirement to store the
estimated parameters which effectively curtails its applicability to large
state space CMDPs. The alternative strategy, known as the model-free
approach, either directly estimates the policy function or maintains
an estimate of the Q function, which is subsequently used for policy
generation (Wei et al., 2022a). Model-free algorithms typically demand
lower memory and computational resources than their model-based
counterparts.
The problem setup, where the system dynamics are known, is extensively studied (Altman, 1999). For a constrained setup, the optimal
policy is possibly stochastic (Altman, 1999; Puterman, 2014). Even
though the problem has been widely studied in episodic and discounted
reward setups (Gattami et al., 2021; Zheng and Ratliff, 2020; Ding et al.,
2021; Bai et al., 2022b; Bai et al., 2023c), the focus of this monograph
is on the average reward setup, thus providing a comprehensive study
for the state of the art in the area.
1.1

Chapter Organization

In Chapter 2, we consider a model-based approach for learning CMDPs
with average reward and costs. We discuss posterior
√sampling-based and
e
optimism-based algorithms. We demonstrate O( T ) objective regret
and zero constraint violation for both of them. The presented results
follow the recent works of (Agarwal et al., 2022c; Agarwal et al., 2022b).
In Chapter 3, we consider a model-free approach for learning CMDP
via general parametrization. General parameterization indexes the policies by finite-dimensional parameters (e.g., weights of neural networks)
to accommodate large state spaces. The learning is manifested by updating these parameters using policy gradient (PG)-type algorithms.
This chapter primarily follows the works of (Bai et al., 2024b; Bai et al.,
e 4/5 ) objective regret
2024a) and presents an algorithm that achieves O(T
and constraint violation. Note that general parameterization subsumes
the tabular setup. Moreover, the best-known regret bound achieved by
any tabular model-free algorithm for average-reward CMDPs is Õ(T 5/6 )

6

Introduction

(Wei et al., 2022a) which is worse than the above result in terms of
orders. Due to this reason, we do not present any algorithm specific to
the tabular model-free setup.
In the previous chapters, we assumed the underlying CMDP to be
ergodic. In Chapter 4, we go beyond this assumption to consider weakly
communicating CMDPs. Note that the class of weakly communicating
CMDPs contains the set of ergodic CMDPs, and it is the largest class for
which one can hope to establish theoretical guarantees for all instances
(Bartlett and Tewari, 2009; Jaksch et al., 2010). This chapter presents


the model-based approach of (Chen et al., 2022b) and proves Õ T 2/3
objective regret and constraint violation. We note that no known modelfree algorithm currently exists that guarantees a sublinear regret and
constraint violation for weakly communicating CMDPs. This leaves
multiple open questions.
1.2

Some Useful Inequalities

In this section, we provide some important inequalities for random
variables, some of which will be used in this monograph.
Lemma 1.1 (Jensen’s Inequality). Let f : R → R be a convex function,
and let X be a random variable. If E[X] is finite, then
f (E[X]) ≤ E[f (X)].
Lemma 1.2 (Cauchy-Schwarz Inequality (Dragomir, 2003)). For any
vectors u and v in a real or complex inner product space, the CauchySchwarz Inequality holds:
|⟨u, v⟩|2 ≤ ⟨u, u⟩ · ⟨v, v⟩.
Lemma 1.3. (Chen et al., 2021, Lemma 30) For a random variable X
such that |X| ≤ C almost surely, we have: Var[X 2 ] ≤ 4C 2 Var[X].
Lemma 1.4 (Azuma-Hoeffding’s Inequality (Serfling, 1974)). Let X1 , · · · , Xn
be a Martingale difference sequence such that |Xi | ≤ c almost surely
for all i ∈ {1, 2, · · · , n}, then,
P

n
X
i=1

!

Xi ≥ ϵ

ϵ2
≤ 2 exp −
2nc2

!

(1.1)

1.2. SOME USEFUL INEQUALITIES

7

Lemma 1.5 (Any interval Azuma’s inequality, (Chen et al., 2022b)). Let
{Xi }∞
i=1 be a martingale difference sequence and |Xi | ≤ B almost
Pl+n−1
surely. Then with probability at least 1 − δ, for any l, n:
Xi ≤
i=l
q

3

.
B 2n ln 4(l+n−1)
δ
Lemma 1.6. (Chen et al., 2022a, Lemma 38) Let {Xi }∞
i=1 be a martingale difference sequence adapted to the filtration {Fi }∞
i=0 and |Xi | ≤ B
for some B > 0. Then with probability at least 1 − δ, for all n ≥ 1
simultaneously,
n
X

v
u n
uX
4B 2 n3
4B 2 n3
Xi ≤ 3t E[X 2 |Fi−1 ] ln
+ 2B ln
.
i

i=1

δ

i=1

δ

Lemma 1.7. (Weissman et al., 2003) Let p be an m-dimensional distribution and p̄ be its empirical
q estimate obtained by averaging over n
samples. Then, ∥p − p̄∥1 ≤ m ln 2δ /n with probability at least 1 − δ.
Lemma 1.8. (Cohen et al., 2020, Theorem D.3) Let {Xn }∞
n=1 be a
sequence of i.i.d random variables with expectation µ and Xn ∈ [0, B]
almost surely. Then with probability at least 1 − δ, for any n ≥ 1:
n
X

(Xi − µ)

i=1

 r




v
u

n
2n
2n u X
2n
2n 
+ B ln , 2tB
Xi ln
+ 7B ln
.
≤ min 2 Bµn ln

δ
δ
δ
δ 
i=1

Lemma 1.9. (Cohen et al., 2020, Lemma D.4) and (Cohen et al., 2021,
Lemma E.2) Let {Xi }∞
i=1 be a sequence of random variables w.r.t to the
filtration {Fi }∞
and
Xi ∈ [0, B] almost surely. Then with probability
i=0
at least 1 − δ, for all n ≥ 1 simultaneously:
n
X

E[Xi |Fi−1 ] ≤ 2

i=1

n
X
i=1

n
X
i=1

Xi ≤ 2

n
X
i=1

Xi + 4B ln

4n
,
δ

E[Xi |Fi−1 ] + 8B ln

4n
.
δ

2
Model-Based RL

We introduce the model and key assumptions studied in this chapter in
Section 2.1. Further, we describe optimism-based and posterior samplingbased algorithms in Section 2.2. Their regret guarantees are provided
in Section 2.3 and 2.4, respectively. Evaluation results are discussed in
Section 2.5 and Section 2.6 suggests some possible future directions.
2.1

Overall Model and Assumptions

We consider an infinite-horizon constrained Markov Decision Process
M = (S, A, r, c, P, ρ) where S is finite set of S states, A denotes a finite
set of A actions, P : S × A × S → [0, 1] defines the transition probability
kernel such that after executing action a ∈ A in state s ∈ S, the system
moves to state s′ ∈ S with probability P (s′ |s, a). r : S × A → [0, 1] and
c : S × A → [−1, 1] denotes the average reward obtained and average
costs incurred in the state action pair (s, a) ∈ S × A, and finally, ρ is
the distribution over the initial state.
The agent interacts with M in time-steps t ∈ 1, 2, · · · for a total of
T time-steps. We note that T is possibly unknown and s1 ∼ ρ. At each
time t, the agent observes state st and plays action at . The agent selects
an action on observing the state s using a policy π : S → ∆(A), where
8

2.1. OVERALL MODEL AND ASSUMPTIONS

9

∆(A) is the probability simplex over the action space. On following a
policy π, the long-term average reward and cost of the agent is given as:
π,P
Jg,ρ
= limτ →∞ Eπ,P



1 Xτ
g(st , at ) s1 ∼ ρ
t=1
τ



(2.1)

where Eπ,P [·] denotes the expectation over the state and action trajectory
generated from following π on transitions P , starting from an initial
distribution, ρ, and g = r, c for the average reward and cost functions
respectively. The above function can also be expressed as:
π,P
Jg,ρ
=

X

ν π,P (s, a)g(s, a)
s,a ρ

where νρπ,P is the state-action occupancy measure generated by following
policy π on MDP with transitions P , starting from an initial distribution,
ρ (Puterman, 2014). Mathematically, νρπ,P (s, a) = dπ,P
ρ (s)π(a|s) where
dπ,P
ρ (s) = limτ →∞

1 Xτ
Eπ,P 1(st = s) s1 ∼ ρ
t=1
τ




(2.2)

where 1(·) is the indicator function. For any policy π, define the induced
P
state transition matrix P π ∈ RS×S as: P π (s, s′ ) = a π(a|s)P (s′ |s, a),
∀s, s′ . If for each policy, π, the CMDP M is irreducible and aperiodic,
then it is defined to be ergodic.
Let (P π )t be the t-step state transition probability matrix induced by
π
π. Let Ts→s
′ denote the time taken by the Markov chain induced by π to
π
hit state s′ starting from state s. Further, define TM := maxπ E[Ts→s
′ ] as
the max reach time of the MDP M. We now introduce our assumption.
Assumption 2.1. The MDP, M, is assumed to be ergodic, which implies:
(a) TM < ∞, (b) dπ,P
is independent of ρ, and (c) there exists C > 0
ρ
and ζ < 1 such that ∥(P π )t (s, ·) − dπ,P ∥TV ≤ Cζ t , ∀s, ∀t ∈ {1, 2, · · · }.
Ergodicity comes with many perks. For example, for ergodic CMDPs,
the distribution νρπ,P is always well-defined and independent of ρ. This
allows us to drop the dependence of ρ from the notation of the occupancy
measures and average reward and cost functions. Moreover, ν π,P obeys
the following stationary relation.
X
s,a

P (s′ |s, a)ν π,P (s, a) =

X
a

ν π,P (s′ , a), ∀s′

10

Model-Based RL

The agent aims to maximize Jrπ,P while ensuring that Jcπ,P does not
exceed a certain threshold. Without loss of generality, we can express
the above problem as the following optimization.
max Jrπ,P

s.t. Jcπ,P ≤ 0

π

(2.3)

We shall denote a solution to the above problem as π ∗ .
Remark 2.1. We would like to emphasize the generality of the formulation stated above. Despite considering a single constraint, the algorithm
and the results developed in this chapter can be easily extended to the
case of multiple constraints.
Assumption 2.2. The rewards r(s, a), the costs c(s, a) are known to
the agent.
Assumption 2.3. There exists a policy π, and a constant δ >
p
STM (A log T )/T + (CSA log T )/(T (1 − ζ)) such that
Jcπ,P ≤ −δ

(2.4)

This assumption is standard in the constrained RL literature (Efroni
et al., 2020; Ding et al., 2021; Ding et al., 2020; Wei et al., 2022b). Here
δ is referred to as the Slater’s constant. Ding et al. (2021) assumes that
the Slater’s constant δ is known. Wei et al. (2022b) assumes that the
number of iterations of the algorithm is at least Ω̃(SAH/δ)5 for episode
length H. On the contrary, we simply assume the existence of δ and a
lower bound on the value of δ, which gets relaxed as the agent acquires
more time to interact with the environment.
Any model-based online algorithm starting with no prior knowledge
will need to obtain estimates of transition probabilities P and obtain
reward and cost values for each state-action pair. Initially, when the
algorithm does not have a good estimate of the model, it accumulates
regret and violates constraints as it does not know the optimal policy.
We define reward regret R(T ) as the difference between the cumulative
reward obtained by the algorithm and the expected cumulative reward
that could have been obtained by running the optimal policy π ∗ for T
steps. Formally, we have the following.
∗

R(T ) ≜ T Jrπ ,P −

XT
t=1

r(st , at )

(2.5)

2.2. ALGORITHMS FOR MODEL-BASED RL

11

Additionally, we define constraint violation C(T ) as follows.
C(T ) ≜
2.2

X



T
t=1

c(st , at )

, where (x)+ = max(0, x)

(2.6)

+

Algorithms for Model-Based RL

Note that if the agent is aware of the true transition P , it can solve the
following optimization to obtain the optimal occupancy measure.
max

X

r(s, a)ν(s, a)

s,a

ν

(2.7)

with the following set of constraints,
X
s,a

ν(s, a) = 1, ν(s, a) ≥ 0, ∀(s, a)

(2.8)

ν(s′ , a) =

(2.9)

X
a∈A

X
s,a

X
s,a

P (s′ |s, a)ν(s, a), ∀s′

ν(s, a)c(s, a) ≤ 0

(2.10)

Equation (2.9) denotes the constraint on the transition structure for the
underlying Markov Process. Equation (2.8) ensures that the solution is
a valid probability distribution. Equation (2.10) is the cost constraint.
Using the optimal solution ν ∗ , we can obtain the optimal policy as:
ν ∗ (s, a)
, ∀(s, a)
∗
′
a′ ∈A ν (s, a )

π ∗ (a|s) = P

(2.11)

The above problem is linear optimization and can be easily solved
with the knowledge of true transition P . However, such knowledge is not
easily available in most practical scenarios. In such cases, the concepts of
optimism-based estimation (Jaksch et al., 2010) and posterior samplingbased estimation (Osband et al., 2013; Agrawal and Jia, 2017) come to
the rescue. In the following, we will explain these approaches in detail.
2.2.1

Optimism Based Estimation

We note that the key idea for the optimistic policy approach (Jaksch et
al., 2010) is to apply upper confidence bounds in the estimation process.
We utilize these bounds to create a confidence set of transition kernels,

12

Model-Based RL

and the optimization problem also optimizes over this confidence set.
The entire process is described in Algorithm 1.
Algorithm 1 Optimism Based Reinforcement Learning (C-UCRL)
Input: S, A, r, c, K
1: Let t = 1, e = 1, ϵe = K

q

ln t
t

2: for (s, a) ∈ S × A do

Necurr (s, a) = Ne (s, a) = 0,
4:
N (s, a, s′ ) = 0, Pbe (s′ |s, a) = S1 , ∀s′ ∈ S
5: end for
6: Solve for policy πe using (2.18)
7: for t ∈ {1, 2, · · · } do
8:
Observe st , and execute at ∼ πe (·|st )
9:
Observe st+1 ∼ P (·|st , at ) and r(st , at ), c(st , at )
10:
Necurr (st , at ) = Necurr (st , at ) + 1
11:
N (st , at , st+1 ) = N (st , at , st+1 ) + 1
12:
if Necurr (s, a) = max{1,
q Ne (s, a)} for any s, a then
3:

e = e + 1, ϵe = K lnt t
14:
for (s, a) ∈ S × A do
curr (s, a)
15:
Ne (s, a) = Ne−1 (s, a) + Ne−1
16:
Necurr (s, a) = 0
17:
if Ne (s, a) > 0 then
′)
18:
Pbe (s′ |s, a) = NN(s,a,s
, ∀s′ ∈ S
e (s,a)
19:
end if
20:
end for
21:
Solve for policy πe using Eq. (2.18)
22:
end if
23: end for
13:

The algorithm proceeds in multiple epochs whose lengths are not
fixed apriori but depend on the observations made. In epoch e, the algorithm maintains three key variables: Necurr (s, a), Ne (s, a), and N (s, a, s′ )
for all state-action pairs (s, a). Necurr (s, a) stores the number of times
(s, a) are visited in epoch e and Ne (s, a) indicates the number of times
(s, a) are visited till the start of epoch e. Finally, N (s, a, s′ ) indicates the

2.2. ALGORITHMS FOR MODEL-BASED RL

13

number of times the system transitions to state s′ after taking action a
in state s. If Ne (s, a) > 0 at the beginning of epoch e, we estimate the
transition probability P (s′ |s, a), ∀s′ as: Pbe (s′ |s, a) = N (s, a, s′ )/Ne (s, a).
Using the above variables, the agent obtains the optimal policy for the
optimistic MDP by solving the following constrained optimization.
max

X

ν,P̃e

s,a

subject to:

X

r(s, a)ν(s, a)

(2.12)

ν(s, a) = 1, ν(s, a) ≥ 0,

(2.13)

ν(s′ , a) =

X

P̃e (s′ |s, a)ν(s, a),

(2.14)

P̃e (s′ |s, a) > 0,

X

P̃e (s′ |s, a) = 1

(2.15)

s,a

X
a

s,a
s′

s

P̃e (·|s, a) − Pbe (·|s, a)
X
s,a

1

≤

c(s, a)ν(s, a) ≤ −ϵe

14S log(2Ate )
1 ∨ Ne (s, a)

(2.16)
(2.17)

where s′ , s ∈ S, a ∈ A, x∨y ≜ max{x, y}, and te is the initial time of the
epoch e. Compared to (2.7)−(2.10), a couple of changes have been made
to formulate the above optimization. Firstly, the above optimization
solves for both ν and P̃e . Secondly, due to the lack of knowledge of the
true transition, P , the constraint (2.9) is replaced by (2.14)−(2.16). We
would like to clarify that (2.14) is similar to the stationarity condition
(2.9) while (2.15) dictates the criteria of a valid transition kernel. Finally,
(2.16) specifies the boundary of the confidence set. Thirdly, instead of
(2.10), we consider (2.17) where ϵe is an epoch-dependent constant given
p
by ϵe = K (log te )/te and K is a configurable parameter. The intuition
behind this modification is to choose policies conservatively to allow
room to violate constraints.
A few remarks are in order. Firstly, since (2.16) defines an L1 ball
with a nonzero radius around the estimated transition, one can always
find a transition function, P̃e , in it that has all strictly positive elements.
This, in turn, ensures that, for any policy, π, the Markov chain generated
by π and P̃e is irreducible and aperiodic, leading to the existence of an
occupancy measure obeying the stationarity condition (2.14). Secondly,
due to Assumption 2.3, one can judiciously choose a small enough ϵe
to ensure that the constraint (2.17) is also satisfied. In summary, the

14

Model-Based RL

feasible set of the above optimization can be ensured to be non-empty.
This optimization problem stated above is convex. Let νe be an
optimal solution of the optimistic MDP for the epoch e. We obtain its
corresponding policy as follows.
πe (a|s) = P

νe (s, a)
, ∀(s, a)
νe (s, a′ )

(2.18)

a′ ∈A

The agent executes the optimistic policy πe for epoch e. The system
model improves as the agent interacts with the environment, allowing
us to bound the regret. Note that the epochs change when the number
of visits to any (s, a) in the current epoch equals the aggregated number
of prior visits to (s, a). Therefore, the total number of occurrences of
any state-action pair is at most doubled during an epoch. We note that
the concept of the approach presented here is similar to that in (Jaksch
et al., 2010), while the extended value iteration has been replaced by
the solution of the optimization problem.
2.2.2

Posterior Sampling Based Estimation

In this estimation approach, we estimate the transition probabilities by
interacting with the environment. In order to solve the problem (2.7)(2.10), we need an estimate of P such that the steady-state distribution
exists for any policy π. In order to ascertain that, we note that when the
priors of the transition probabilities P (·|s, a) are a Dirichlet distribution
for each state and action pair, such a steady state distribution exists.
Proposition 2.1 formalizes the result of the existence of a steady state
distribution when the transition probability is sampled from a Dirichlet
distribution.
Proposition 2.1. For any MDP M with state space S and action space
A, let the transition probabilities P be a Dirichlet distribution. Then,
for any policy π, M will have an associated state-action distribution
ν π that obeys the following expression.
X
a

ν π (s′ , a) =

X

ν π (s, a)P (s′ |s, a), ∀s′ ∈ S.

(2.19)

s,a

Proof. Since the transition probabilities P (s′ |s, a) follow the Dirichlet
distribution, they are strictly positive. Further, as π(·|·) is a probability

2.2. ALGORITHMS FOR MODEL-BASED RL

15

distribution on actions conditioned on state, π(a|s) ≥ 0, a π(a|s) = 1.
So, there is a nonzero transition probability to reach from state s ∈ S
to state s′ ∈ S.
Now, note that all the entries of the transition probability matrix are
strictly positive. Hence, the Markov Chain induced over the MDP M by
any policy π is irreducible (since it is possible to reach any state from
any other state) and aperiodic (since it is possible to reach any state
in a single time step from any other state). Combined, these two facts
prove the existence of the steady-state distribution (Lawler, 2018).
P

Algorithm 2 Model-Based Posterior Sampling Algorithm
Parameters: K
Input: S, A, r, c
1: Let t = 1, e = 1, ϵe = K

q

ln t
t

2: Necurr (s, a) = 0, Ne (s, a) = 0, ∀(s, a)
3: Obtain the steady-state distribution νe (s, a) as the solution of the

optimization problem given by (2.7)-(2.9), (2.17) with Pe (s′ |s, a) ∼
Dir(Ne (s, a, ·)), ∀(s, a) used as the transition function.
4: Solve for policy πe using Eq. (2.18)
5: for t ∈ {1, 2, · · · } do
6:
Observe st , and execute at ∼ πe (·|st )
7:
Observe st+1 , r(st , at ) and ci (st , at ) ∀ i ∈ [d]
8:
Necurr (st , at ) = Necurr (st , at ) + 1
9:
if Necurr (s, a) = max{1, Ne (s, a)} for any s, a then
10:
for (s, a) ∈ S × A do
11:
Ne+1 (s, a) = Ne (s, a) + Necurr (s, a)
12:
e = e + 1, Necurr (s, a) = 0
13:
end forq
14:
ϵe = K lnt t
15:
Obtain the steady-state distribution νe (s, a) by solving the optimization problem given by (2.7)-(2.9), (2.17) with Pe (s′ |s, a) ∼
Dir(Ne (s, a, ·)), ∀(s, a) used as the transition function.
16:
Solve for policy πe using Eq. (2.18)
17:
end if
18: end for

16

Model-Based RL

To complete the setup for our algorithm, we make a few assumptions,
which are described below.
Assumption 2.4. The transition probabilities P (·|s, a) of the Markov
Decision Process have a Dirichlet prior for all state-action pairs (s, a).
Since we assume that transition probabilities of the MDP M follow
Dirichlet distributions, all policies on M have a steady-state distribution.
Therefore, the key approach for the algorithm is to solve the optimization
problem (2.7)-(2.10) using a sample of the transition probability with
Dirichlet priors. We further notice that the algorithm proceeds in epochs
similar to the case of optimistic reinforcement learning, and the policy
is modified only at the start of the epoch and kept the same within the
epoch. Note that the conservative constraint given by (2.17) is still used
to achieve zero constraint violations. The formal algorithm is given in
Algorithm 2.
The approach given here is an adaptation of the posterior sampling
approach (Osband et al., 2013; Agrawal and Jia, 2017), where the
optimal policy for Pe is solved using an optimization problem.
2.2.3

Optimistic Decision Making vs. Posterior Sampling Decision
Making

There is a well-known connection between posterior sampling and optimistic algorithms (Russo and Van Roy, 2014). The authors of (Osband
and Van Roy, 2017) discussed that posterior sampling can be considered a stochastically optimistic algorithm. Before each epoch, a typical
optimistic algorithm constructs a confidence set to represent the range
of MDPs that are statistically plausible given the prior knowledge and
observations. Then, a policy is selected by maximizing value simultaneously over policies and MDPs within this set. The agent then follows
this policy over the epoch. It is interesting to contrast this approach
against posterior sampling-based reinforcement learning, where instead
of maximizing over a confidence set, it samples a single statistically
plausible MDP and selects a policy to maximize the value for that MDP.
We would like to mention that optimistic approaches lack statistical
efficiency as compared to posterior sampling-based approaches due to

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

17

the sub-optimal construction of the confidence sets. Even if the state
transition dynamics are entirely known, the L1 ball of the uncertainty in
the confidence set leads to an extremely poor evaluation performance of
the regret (Osband and Van Roy, 2017; Agarwal et al., 2022b). Further,
the computational complexity of the optimistic algorithm is higher since
the search of the policy also has to consider an appropriate MDP in the
confidence bounds.
2.3

Regret Analysis and Constraint Violation for Optimism Based
Approach

In this section, we study the regret and constraint violations for the
proposed optimism-based algorithm. We note that the standard analysis
for infinite horizon tabular MDPs of UCRL2 (Jaksch et al., 2010) cannot
be directly applied as the policy πe is possibly stochastic for every epoch.
Another peculiar aspect of analyzing infinite horizon CMDPs is that
the regret grows linearly with the number of epochs (or policy switches).
This is because a new policy induces a new Markov chain, which takes
time to converge to the
√stationary distribution. The analysis still bounds
the regret by Õ(TM S AT ) as the number of epochs is upper bounded
by O(SA log T ).
Before diving into the details, we introduce a few terms that are
key to our analysis. The first one is the Q-function. We define Qπ,P
as
γ
the long-term γ-discounted expected reward for taking action a in state
s and following policy π for the MDP with transition function P . We
emphasize that the notion of discounted Q function is introduced purely
for analytical convenience, although the original objective is maximizing
the average reward. It can be shown that Qπ,P
satisfies the Bellman
γ
equation, as shown below.
Qπ,P
γ (s, a) = r(s, a) + γ

X
hs

′

P (s′ |s, a)Vγπ,P (s′ ),

where Vγπ,P (s) = Ea∼π(·|s) Qπ,P
γ (s, a)

i

We define the Bellman error Bγπ,P̃ (s, a) for the γ-discounted MDP as:
P̃
Bγπ,P̃ (s, a) = Qπ,
γ (s, a) − r(s, a) − γ

X
s′

P (s′ |s, a)Vγπ,P̃ (s, a) (2.20)

18

Model-Based RL

We also define B π,P̃ (·, ·) ≜ limγ→1 Bγπ,P̃ (·, ·) as the Bellman error for the
average reward setup. Note that the error arises if a transition different
from the true one is followed. The terms defined for the discounted MDP
are connected with the average reward setup. For example, (Puterman,
2014)[Corollary 8.2.5] showed the following ∀g ∈ {r, c}, ∀s.
Jgπ,P = lim (1 − γ)Vγπ,P (s),

(2.21)

γ→1

dπ,P (·) = lim (1 − γ)(I − γP π )−1 (s, ·)

(2.22)

γ→1

(Puterman, 2014) also showed that, for every average-reward MDP with
transition kernel, P and policy π, there exists a bias function h ∈ RS
that satisfies the Bellman equation stated below ∀s.
h(s) = rπ (s) − Jrπ,P +

X
s′

P π (s, s′ )h(s′ )

(2.23)

where rπ (s) ≜ a r(s, a)π(a|s), ∀s. The span of h is defined as sp(h) ≜
maxs h(s) − mins h(s). It can be further proven that, ∀s, s′ ,
P



h(s) − h(s′ ) = lim Vγπ,P (s) − Vγπ,P (s′ )



(2.24)

γ→1

After defining the key variables, we can now jump into bounding the
objective regret R(T ). Intuitively, the algorithm incurs regret on three
accounts. The first component originates from following the conservative
policy, which is needed to limit the constraint violations. The second
component emerges from solving for the optimal policy for the optimistic
MDP. The third component arises from the system’s stochastic behavior.
Note that the constraints are violated because of both the imperfect
MDP knowledge and the stochastic behavior. However, the conservative
policy enforces the constraint violation to stay within some limits, as
shown in the next result.
√
Theorem 2.1. For all T and K = Θ(TM S A + CSA/(1 − ζ)), the
regret R(T ) of C-UCRL algorithm is bounded as
R(T ) = O

p
1
CTM S 2 A log T
TM S AT log AT +
δ
(1 − ζ)

!

(2.25)

1
and C(T ) = 0, with probability at least 1 − T 5/4
.

The following subsections will provide the formal proof of regret
and constraint violation bounds stated in the above Theorem.

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH
2.3.1

19

Objective Regret Bound

We first provide a Lemma that bounds the gap between the long-term
average expected reward obtained by executing the optimal solution
to the original non-conservative problem for the true MDP and that
obtained via the solution to the conservative optimization of the same
MDP with ϵe constraint bound.
Lemma 2.2. Let π ∗ be the optimal non-conservative policy for the true
MDP with transition function, P and πe∗ denote the optimal policy with
ϵe conservative bound corresponding to the same MDP. If ϵe ≤ δ, we
have the following inequality.
∗

∗

Jrπ ,P − Jrπe ,P ≤ 2ϵe /δ

(2.26)

Proof Sketch: We first construct a policy for which the steady-state distribution is the weighted average of two steady-state distributions. The
first distribution is for the optimal non-conservative policy corresponding to the true MDP M. The second one is for another policy satisfying
Assumption 2.3. We show that this constructed policy satisfies the ϵe
conservative constraint. Finally, utilizing the optimality of πe∗ for the
conservative problem with ϵe constrained bound and the boundedness
of the reward function, we arrive at the conclusion.
∗

Detailed Proof: Note that ν π ,P , the stationary distribution corresponding to the optimal non-conservative policy, π ∗ , satisfies
X

∗

ν π ,P (s, a)c(s, a) ≤ 0

(2.27)

s,a

Further, from Assumption 2.3, we have a feasible policy π for which
X

ν π,P (s, a)c(s, a) ≤ −δ

(2.28)

s,a

We construct a stationary distribution ν P and obtain its corresponding
policy πe′ using the following equations ∀(s, a).
ϵe
ϵe
∗
ν (s, a) = 1 −
ν π ,P (s, a) + ν π,P (s, a)
δ
δ
P







πe′ (a|s) = ν P (s, a)/ 

(2.29)


X
s,a′

ν P (s, a′ )

(2.30)

20

Model-Based RL

For this new policy, we observe that
X

P

ν (s, a)c(s, a) =

s,a

ϵe
ϵe
∗
ν π ,P + ν π,P (s, a)c(s, a)
1−
δ
δ

s,a

ϵe
δ



= 1−
(a) ϵ
e

≤

X



X

∗

ν π ,P (s, a)c(s, a) +

s,a

ϵe X π,P
ν (s, a)c(s, a)
δ s,a

(2.31)

(−δ) = −ϵe

δ

where (a) follows from (2.27)–(2.28).
As a consequence of (2.31), one can claim that πe′ constructed in
(2.29) satisfies the ϵe conservative constraint. Further, it is given that πe∗
is the optimal solution corresponding to the ϵe conservative constrained
optimization problem. Hence, we have
X

∗

ν π ,P (s, a)r(s, a) −

X

s,a

≤

s,a

X

νπP∗ (s, a)r(s, a) −

s,a

≤

∗

ν πe ,P (s, a)r(s, a)

X

X

ν P (s, a)r(s, a)

s,a
∗



ν π ,P (s, a) − ν P (s, a) r(s, a)

s,a

≤

X
s,a

ν

π ∗ ,P

ϵe
ϵe
∗
(s, a) − 1 −
ν π ,P (s, a) − ν π,P (s, a) r(s, a)
δ
δ






≤


ϵe X  π∗ ,P
ν
(s, a) − ν π,P (s, a) r(s, a)
δ s,a

≤

(a) 2ϵ
ϵe X π∗ ,P
ϵe X π,P
e
ν
(s, a)r(s, a) +
ν (s, a)r(s, a) ≤
δ s,a
δ s,a
δ

(2.32)
where (a) follows from the fact that r(s, a) ≤ 1, ∀(s, a).
Lemma 2.2 and our construction of ϵe sequence is crucial to restrict
√
the growth of the regret due to conservative policies by Õ(TM S AT ).
We will now decompose the regret into multiple components and analyze
each part individually.

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

21

Regret breakdown
Let E denote the number of epochs observed in duration T and Te , te
denote the length and starting time of the epoch e. Moreover, following
our earlier notations, let πe∗ , πe be the optimal ϵe -conservative policies
corresponding to MDPs with true transition, P and optimistic transition,
P̃e , respectively. We have the following decomposition of the regret.
∗
R(T ) = T Jrπ ,P −

T
X

rt (st , at )

t=1
∗
= T Jrπ ,P −

E
X

∗
Te Jrπe ,P +

e=1

=

E
X



≤

=

∗



∗

∗





e=1
E
X



e=1
E
X



∗

∗

Te Jrπ ,P − Jrπe ,P +

∗

∗

∗

Te Jrπ ,P − Jrπe ,P

rt (st , at )

t=1
T
X

rt (st , at )

t=1
T
X

Te Jrπe ,P̃e −

Te Jrπe ,P̃e −

e=1



T
X

Te Jrπe ,P −

Te Jrπ ,P − Jrπe ,P +

e=1
E
X
e=1
E
X

∗
Te Jrπe ,P −

e=1
E
X

Te Jrπ ,P − Jrπe ,P +

e=1
E
(a) X

≤

∗

E
X

t=1
T
X

rt (st , at )
rt (st , at )

t=1

(2.33)



e=1

+

E te+1
X
X−1 

Jrπe ,P̃e − Jrπe ,P + Jrπe ,P − rt (st , at )



e=1 t=te

≤

E
X



∗

∗



Te Jrπ ,P − Jrπe ,P +

E te+1
X
X−1 

Jrπe ,P̃e − Jrπe ,P



e=1 t=te

e=1

+

E te+1
X
X−1 



Jrπe ,P − rt (st , at )

e=1 t=te

= R1 (T ) + R2 (T ) + R3 (T )
where (a) is a consequence of the fact that the policy πe is optimal for the
optimistic CMDP with transition P̃e corresponding to the constrained

22

Model-Based RL

bound ϵe . The first term in (2.33) is as follows.
R1 (T ) =

E
X

∗



∗

Te Jrπ ,P − Jrπe ,P



(2.34)

e=1

R1 (T ) denotes the regret incurred from not playing the optimal nonconservative policy π ∗ for the true MDP with transition, P but rather
the optimal conservative policy πe∗ for the same MDP with constraint
ϵe at epoch e. The second component is given as follows.
E te+1
X
X−1 

R2 (T ) =

Jrπe ,P̃e − Jrπe ,P



(2.35)

e=1 t=te

R2 (T ) is the difference between the expected rewards generated
from playing πe on the optimistic MDP instead of the true MDP.
E te+1
X
X−1 

R3 (T ) =

Jrπe ,P − rt (st , at )



(2.36)

e=1 t=te

R3 (T ) denotes the gap between the observed and expected rewards
obtained from playing the policy πe on the true MDP.
Bounding R1 (T )
Bounding R1 (T ) uses Lemma 2.2. We have the following set of equations:
R1 (T ) =

E te+1
X
X−1 

∗

∗

Jrπ ,P − Jrπe ,P



e=1 t=te
E te+1 −1

≤

X X 2ϵe

δ

e=1 t=te
T
(a) 2K X

=

δ

s

t=1

2K log T
=
δ
2K log T
≤
δ

log T
t
1+

(2.37)
T
X

r !

t=2

1
t

Z T r

1+
t=1

!

1
dt
t

≤

2K log T √
(2 T )
δ

where (a) follows from the definition of ϵe and the fact that log t ≤ log T
for all t ≤ T .

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

23

Bounding R2 (T )
As stated earlier, the term R2 (T ) captures the difference between the
expected rewards arising from running the policy πe on the optimistic
and true MDPs. Here, the idea of the Bellman error introduced earlier
becomes useful. Formally, we have the following lemma.
Lemma 2.3. The difference between Jrπe ,P̃e , i.e., the long-term average
reward generated via running the optimistic policy πe on the optimistic
MDP, and Jrπe ,P , i.e., the same generated via running the optimistic
policy πe on the true MDP, can be expressed as the long-term average
Bellman error. Mathematically,
Jrπe ,P̃e − Jrπe ,P =

X

ν πe ,P (s, a)B πe ,P̃e (s, a)

(2.38)

s,a

Proof. Note that for all s ∈ S, we have:
h

Vγπe ,P̃e (s) = Ea∼πe Qπγ e ,P̃e (s, a)

i





(a)

= Ea∼πe Bγπe ,P̃e (s, a) + r(s, a) + γ

X

P (s′ |s, a)Vγπe ,P̃e (s′ )

(2.39)

s′ ∈S

where (a) follows from the definition of the Bellman error. Similarly, for
the true MDP, we have,
h

i

Vγπe ,P (s) = Ea∼πe Qπγ e ,P (s, a)




= Ea∼πe r(s, a) + γ

X

(2.40)

P (s′ |s, a)Vγπe ,P (s′ )

s′ ∈S

The above equation essentially applies the Bellman equation for discounted reward MDPs. Subtracting (2.40) from (2.39), we get:
Vγπe ,P̃e (s) − Vγπe ,P (s)



X

= Ea∼πe Bγπe ,P̃e (s, a) + γ









P (s′ |s, a) Vγπe ,P̃e − Vγπe ,P̃e (s′ )

s′ ∈S

h

i

= Ea∼πe Bγπe ,P̃e (s, a) + γ

X

P πe (s, s′ ) Vγπe ,P̃e − Vγπe ,P̃e (s′ )

s′ ∈S

(2.41)

24

Model-Based RL

Let B̄γπe ,P̃e (·) ≜

πe ,P̃e (·, a). Using the vector format, we get,
a πe (a|s)Bγ

P

Vγπe ,P̃e − Vγπe ,P = (I − γP πe )−1 B̄γπe ,P̃e

(2.42)

Applying (2.21), (2.22), we deduce the following.


Jrπe ,P̃e − Jrπe ,P = lim (1 − γ) Vγπe ,P̃e (s) − Vγπe ,P (s)



γ→1

=

X
s′

=

X

πe −1

lim (1 − γ) (I − γP )

γ→1



(s, s′ ) lim B̄γπe ,P̃e (s′ )
γ→1

(2.43)

dπe ,P (s′ )πe (a|s′ )B πe ,P̃e (s′ , a)

s′ ,a

Using ν πe ,P (s, a) = dπe ,P (s)πe (a|s), ∀(s, a), we complete the proof.
Remark 2.2. Note that the Bellman error is unrelated to the Advantage
function and policy improvement lemma (Langford and Kakade, 2002).
The policy improvement lemma relates the performance of two policies
on the same MDP, whereas we bound the performance of one policy on
two different MDPs in Lemma 2.3.
The lemma described below bounds the Bellman error, which in
turn, bounds the gap between Jrπe ,P̃e and Jrπe ,P .
Lemma 2.4. The Bellman error B πe ,P̃e (s, a) for state-action pair (s, a)
in epoch e is upper bounded as follows with probability at least 1 − t−6
e ,
(

B

πe ,P̃e

(s, a) ≤ min 2,

s

14S log(2AT )
1 ∨ Ne (s, a)

)

sp(h̃)

(2.44)

where h̃ denotes a bias function associated with an MDP with transition
kernel P̃e , and a policy πe .
Proof Sketch: Bellman error bounds the impact of the difference in the
value obtained due to the difference in transition probability to the
immediate next state. We bound the difference in transition probability
between optimistic and true MDPs using the result from (Weissman
et al., 2003). This approach gives the desired result.

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

25

Detailed Proof: The definition of the Bellman error gives
!!

B

πe ,P̃e

Qπγ e ,P̃e (s, a) −

(s, a) = lim

γ→1

r(s, a) + γ

X

′

P (s |s, a)Vγπe ,P̃e

s′

!

(a)

r(s, a) + γ

= lim

γ→1

X

P̃e (s′ |s, a)Vγπe ,P̃e (s′ )

s′

!!
X

P (s |s, a)Vγπe ,P̃e (s′ )
′

−

r(s, a) + γ

X

P̃e (s |s, a) − P (s′ |s, a) Vγπe ,P̃e (s′ )

s′

= lim γ
γ→1



′

s′

= lim γ
γ→1

X



P̃e (s′ |s, a) − P (s′ |s, a) Vγπe ,P̃e (s′ )

s′

!

−

X

′

P̃e (s |s, a)Vγπe ,P̃e (s) +

X

= lim γ
=

X

X

′



′

P̃e (s |s, a) − P (s |s, a)

Vγπe ,P̃e (s′ ) − Vγπe ,P̃e (s)



!

s′





P̃e (s′ |s, a) − P (s′ |s, a) lim γ Vγπe ,P̃e (s′ ) − Vγπe ,P̃e (s)



γ→1

s′

=

P (s |s, a)Vγπe ,P̃e (s)

s′

s′

γ→1

′

X



P̃e (s′ |s, a) − P (s′ |s, a)

h̃(s′ ) − h̃(s)



s′
(b)

≤



P̃e (·|s, a) − P (·|s, a)

(c)


1

sp(h̃) ≤

s

14S log(2Ate )
sp(h̃)
1 ∨ Ne (s, a)
(2.45)

where (a) is an application of the Bellman equation and (b) follows from
Hölder’s inequality. In (c), the L1 difference of the transition functions
can be trivially bounded by 2. The bound demonstrated above holds
with probability at least 1 − t−6
e following the result by (Weissman et al.,
2003).
The next lemma bounds the average Bellman error of an epoch
using its realizations at state-action pairs visited in an epoch.

26

Model-Based RL

Lemma 2.5. With probability at least 1−1/T 6 , the cumulative expected
Bellman error is bounded as:
E
X

h

(te+1 − te )Eπe ,P B πe ,P̃e (s, a)

i

e=1

≤

E te+1
X
X−1

(2.46)
B

πe ,P̃e

(st , at ) + 4TM

q

7T log(T )

e=1 t=te

Proof. Let Ft = {s1 , a1 , · · · , st , at } be the filtration generated by the
running the algorithm for t time-steps. We now use Assumption 2.1 to
obtain the following.
E(s,a)∼πe ,P [B πe ,P̃e (s, a)]
= E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ]




+ E(s,a)∼πe ,P [B πe ,P̃e (s, a)] − E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ]
(a)

≤ E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ]
+ 2 sp(h̃)

X

πe (a|s)dπe ,P (s) − πe (a|s)(P πe )t−te +1 (ste −1 , s)

s,a

≤ E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ]
+ 2 sp(h̃)

X

πe (a|s) dπe ,P (s) − (P πe )t−te +1 (ste −1 , s)

s,a

≤ E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ]
+ 2 sp(h̃)

X

π(a|s)∥dπe ,P − (P πe )t−te +1 (ste −1 , ·)∥TV

s,a

≤ E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ] + 2 sp(h̃)

X

πe (a|s)Cζ t−te

s,a
(b)

= E(st ,at )∼πe ,P [B πe ,P̃e (st , at )|Fte −1 ] + 2 sp(h̃)CSζ t−te
(2.47)

where Equation (a) follows from Assumption 2.1 for running policy
πe starting from state ste −1 for t − te + 1 steps and from Lemma 2.4.
Equation (b) follows from bounding the total-variation distance for all
P
states and from the fact that a πe (a|s)
= 1.
h
i
π
Using this, and the fact that Eπe ,P B e ,P̃e (st , at )|Fte −1 −B πe ,P̃e (st , at )
forms a Martingale difference sequence conditioned on filtration Ft−1

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH
h

27

i

with |Eπe ,P B πe ,P̃e (st , at )|Ft−1 − B πe ,P̃e (st , at )| ≤ 4 sp(h̃), we can use
the Azuma-Hoeffding inequality to bound the summation as
E
X

h

(te+1 − te )Eπe ,P B πe ,P̃e (s, a)

i

e=1

=

E 
X

h

i

te+1 −1

X

(te+1 − te )Eπe ,P B πe ,P̃e (st , at )|Fte −1 +

≤

≤





te+1 −1

X


e=1



t=te

e=1
E
X

2CSsp(h̃)ζ t−te

Eπe ,P

t=te

E te+1
X
X−1

2CSsp(h̃) 
B πe ,P̃e (st , at )|Fte −1 +
1−ζ
i

h

B πe ,P̃e (st , at ) + 4sp(h̃) 7T log 2T +
p

e=1 t=te

2CESsp(h̃)
1−ζ
(2.48)

where the last inequality follows from the Azuma-Hoefdding inequality
with probability at least 1 − T −6 .
Bounding the term sp(h̃)
Lemma 2.6. For a MDP with rewards r(s, a) and transition probabilities
P̃e , using policy πe , the difference of bias of any two states s, and s̄ is
bounded as h̃(s) − h̃(s̄) ≤ TM , ∀s, s̄ ∈ S.
′

Proof. Note that Jrπe ,P̃e ≥ Jrπe ,P for all P ′ in the confidence set. Consider the following Bellman equation.
h̃(s) = rπe (s) − Jrπe ,P̃e +

X

P πe ,e (s, s′ )h̃(s′ ) = T h̃(s)

s′

where rπe (s) = a πe (a|s)r(s, a) and P πe ,e (s, s′ ) = a πe (a|s)P̃e (s′ |s, a).
Consider arbitrary two states s, s̄ ∈ S. Let τ = min{t ≥ 1 : st = s̄, s1 =
P
s} be a random variable. With P πe (s, s′ ) = a πe (a|s)P (s′ |s, a), define
another operator as follows.
P

P

!

T̄ h̃(s) =

min r(s, a) − Jrπe ,P̃e +
s,a

X

πe

′

′

P (s, s )h̃(s ) 1(s ̸= s̄)

s′

+ h̃(s̄)1(s = s̄).

28

Model-Based RL

Note that T̄ h̃(s) ≤ T h̃(s) = h̃(s) for all s since P̃e maximizes the reward
r over all the transition probabilities in the confidence set including the
true transition probability P . Further, for any two vectors u, v ∈ RS
with u(s) ≥ v(s), ∀s, we have T̄ u ≥ T̄ v. Hence, we have T̄ n h̃(s) ≤ h̃(s)
for all s. Hence, we have
h̃(s) ≥ T̄ n (s) = E − (Jrπe ,P̃e − min r(s, a))(n ∧ τ ) + h̃(sn∧τ )




s,a

Taking limit as n → ∞, we have h̃(s) ≥ h̃(s′ ) − TM , thus completing
the proof.

We are now ready to bound R2 (T ) using Lemma 2.3, Lemma 2.4,
and Lemma 2.5. We have the following set of equations:

R2 (T ) =

E te+1
X
X−1 

Jrπe ,P̃e − Jrπe ,P



e=1 t=te
(a)

=

E te+1
X
X−1 X
e=1 t=te

(b)

≤

E te+1
X
X−1

ν πe ,P (s, a)B πe ,P̃e (s, a)

s,a

B πe ,P̃e (st , at ) + 4TM

e=1 t=te

(c)

≤

E te+1
X
X−1
e=1 t=te

|

s

TM

q

7T log(2T ) +

2CTM SE
1−ζ

q
14S log(2AT )
2CTM SE
+4TM 7T log(2T ) +
1 ∨ Ne (s, a)
1−ζ

{z

R0

}

(2.49)
where (a), (b), (c) are consequences of Lemma 2.3, 2.5, and 2.4 respectively. Note that the term R0 in the above inequality can be bounded
as follows.

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

R0 ≤

E X
X

s

Necurr (s, a)TM

e=1 s,a

≤

X

TM

q

14SA log(2AT )

s,a
(d) X

≤

14S log(2AT )
1 ∨ Ne (s, a)

E
X
e=1

√

29

N curr (s, a)
1 ∨ Ne (s, a)

p e

q

q

TM ( 2 + 1) 14SA log(2AT ) N (s, a) + 4TM

q

7T log(2T )

s,a

v

u
q
√
u X
≤ TM ( 2 + 1) 14SA log(2AT )t
1

(e)

s,a

!

!
X

N (s, a)

s,a

q
√
√
≤ TM ( 2 + 1) 14SA log(2AT ) SAT

(2.50)
Equation (d) follows from (Jaksch et al., 2010), and (e) is a consequence
of Cauchy-Schwarz inequality. Combining (2.49) and (2.50), we obtain
a bound on R2 (T ).
Bounding R3 (T )
Bounding R3 (T ) follows mostly similar to Lemma 2.5. At each epoch,
the agent visits states according to the occupancy measure νπe ,P and
obtains the rewards. We bound the deviation of the observed visitations
to the expected visitations to each state-action pair in each epoch.
Lemma 2.7. With probability at least 1 − 1/T 6 , the difference between
the observed rewards and the expected rewards is bounded as:
E te+1
X
X−1
e=1 t=te

Eπe ,P [r(s, a)] −

E te+1
X
X−1

q

r(st , at ) ≤ 2 7T log(2T ) (2.51)

e=1 t=te

Proof. We note that Eπe ,P [r(s, a)|Ft−1 ] − r(st , at ) is a Martingale difference sequence bounded by 2 because the rewards are bounded by 1.
Hence, following the proof of Lemma 2.5 we get the required result.
Bounding the number of episodes E
The number of episodes E of the C-UCRL algorithm are bounded by
1 + 2SA + SA log(T /SA) from Proposition 18 of (Jaksch et al., 2010).

30

Model-Based RL

2.3.2

Bounding Constraint Violations

To bound the constraint violations C(T ), we break it into multiple
components. We can then bound these components individually.
Constraint breakdown
We first break down our constraint violations into multiple parts.
C(T ) =

T
X

!

c(st , at )

t=1
T
X

=
(a)

≤

+

c(st , at ) +

≤

E
X

Te Jcπe ,P̃e −

e=1

e=1

T
X

E
X

E
X

c(st , at ) −

Te Jcπe ,P̃e −

e=1

E te+1
X
X−1 

≤

E te+1
X
X−1 

+

!

Te ϵe

e=1

c(st , at ) − Jcπe ,P̃e



+

−

e=1 t=te



!

Te Jcπe ,P̃e

t=1

t=1



E
X

E
X



Te ϵe 

e=1

+

c(st , at ) − Jcπe ,P + Jcπe ,P − Jcπe ,P̃e



e=1 t=te



≤

E te+1
X
X−1 

−

E
X
e=1

c(st , at ) − Jcπe ,P



Te ϵe 
+



e=1 t=te

+

E te+1
X
X−1 

Jcπe ,P − Jcπe ,P̃e



e=1 t=te

−

E
X
e=1



Te ϵe 
+

≤ (C3 (T ) + C2 (T ) − C1 (T ))+
(2.52)
where (a) is a result of the fact the policy πe is solution of a conservative
optimization equation. The three components of the constraint violation
are defined below. The first component, C1 (T ) can be written as:
C1 (T ) =

E
X
e=1

Te ϵe

(2.53)

2.3. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
OPTIMISM BASED APPROACH

31

C1 (T ) denotes the gap left by playing the policy for ϵe -tight optimization
problem on the optimistic MDP.
C2 (T ) =

E te+1
X
X−1 

Jcπe ,P − Jcπe ,P̃e



(2.54)

e=1 t=te

C2 (T ) denotes the difference between long-term average costs incurred
by playing the policy πe on the true MDP with transitions P and the
optimistic MDP with transitions P̃ . This term is bounded similarly to
the bound of R2 (T ).
C3 (T ) =

E te+1
X
X−1 

c(st , at ) − Jcπe ,P



(2.55)

e=1 t=te

C3 (T ) denotes the difference between long-term average costs incurred
by playing the policy πe on the true MDP with transitions P and the
realized costs. This term is bounded similarly to the bound of R3 (T ).
Bounding C1 (T )
Note that C1 (T ) is a component of the constraint violation that occurs
due to the lack of knowledge of the true MDP and deviations of incurred
costs from the expected costs. We bound C1 (T ) for sufficient slackness.
With this idea, we have the following set of equations.
C1 (T ) =

E te+1
X
X−1
e=1 t=te

ϵe =

E te+1
X
X−1

s

K

log te
te

e=1 t=te
s
E te+1 −1

≥K

X

X

e=E ′ t=te
s
E te+1 −1

≥K

X

X

e=E ′

t=te

s

= K (T − tE ′ )
T
≥K
2

s

log(T /4)
te
log(T /4)
T

log(T /4)
T

log(T /4)
Kp
≥
T log T
T
4

where E ′ is some epoch for which T /4 ≤ tE ′ < T /2.

(2.56)

32

Model-Based RL

Bounding C2 (T ), and C3 (T )
We observe that the terms C2 (T ), and C3 (T ) follows the same bound
as R2 (T ) and R3 (T ) respectively. Thus, replacing r with c, we obtain
constraint violations because
√ of imperfect system knowledge and system
stochasticity √
as Õ(TM S AT ). Summing the three terms and choosing
K = Θ(TM S A) gives the required bound on constraint violation.
2.4

Regret Analysis and Constraint Violation for Posterior Sampling
Based Approach

For the C-UCRL algorithm, the true MDP lies in the confidence interval
with high probability, and hence the solution of the optimization problem
was guaranteed. However, the same is not true for the MDP with sampled
transition probabilities. We want the existence of a policy πe such that
Equation (2.17) holds. We obtain the condition for the existence of such
a policy in the following lemma. To obtain the lemma, we first state a
tighter Slater assumption as:
Assumption 2.5. There exists two constants
δ > STM
q

(CSA log T )/(T (1−ζ)) and Γ > 2STM
ζ)T 1/3 ), and a policy π such that

p

(A log T )/T +

14A log AT /T 1/3 +CST

Jcπ,P ≤ −δ − Γ

M /((1−

(2.57)

Lemma 2.8. If there exists π, such that Jcπ,P ≤ −δ − Γ, and there exist
episodes e and e + 1 with start timesteps te and te+1 respectively
satisr
fying te+1 − te ≥ T 1/3 , then for ∥P̃e (·|s, a) − P (·|s, a)∥1 ≤

14S log(2At)
Ne (s,a) ,

the policy π satisfies the following inequality.
Jcπ,P̃e ≤ −δ.

(2.58)

Jcπ,P̃e ≤ |Jcπ,P̃e − Jcπ,P | + Jcπ,P

(2.59)

Proof. We have,

We now bound the term |Jcπ,P̃e − Jcπ,P | using Bellman error. We have,
Jcπ,P̃e − Jcπ,P =

X
s,a

h

i

ν π,P (s, a)Bcπe ,P̃e (s, a) = E Bcπe ,P̃e (s, a)

(2.60)

2.4. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
POSTERIOR SAMPLING BASED APPROACH

33

where Bcπe ,P̃e (s, a) is the Bellman error for cost function. We bound the
expectation using Azuma-Hoeffding’s inequality as follows:
h

E Bcπe ,P̃e (s, a)

i

h

i

= E Bcπe ,P̃e (st , at )|Fte −1 + Cζ t−te
t

−1

t

−1

e+1
i

X  h
1
E Bcπe ,P̃e (st , at )|Fte −1 + Cζ t−te
=
te+1 − te t=te

(a)

(b)

≤
(c)

≤

e+1
i
X  h
1
CSsp(h̃)
E Bcπe ,P̃e (st , at )|Fte −1 +
te+1 − te t=te
(1 − ζ)(te+1 − te )

1
te+1 − te

p

sp(h̃) 14S log AT

X N curr (s, a)
pe

Ne (s, a)

s,a



q

+ 4sp(h̃) 7(te+1 − te ) log(te+1 − te ) +
(d)

≤

1
te+1 − te

p

sp(h̃) 14S log AT

Xq

Necurr (s, a)

s,a



q

+ 4sp(h̃) 7(te+1 − te ) log(te+1 − te ) +


(e)

p
1
sp(h̃)S 14A log AT
≤
te+1 − te

sX

CSsp(h̃)
(1 − ζ)(te+1 − te )

Necurr (s, a)

s,a



q

+ 4sp(h̃) 7(te+1 − te ) log(te+1 − te ) +
(f )

CSsp(h̃)
(1 − ζ)(te+1 − te )

CSsp(h̃)
(1 − ζ)(te+1 − te )

q
p
1
sp(h̃)S 14A log AT (te+1 − te )
te+1 − te

q
CSsp(h̃)
+ 4sp(h̃) 7(te+1 − te ) log(te+1 − te ) +
(1 − ζ)(te+1 − te )


≤

s

≤ sp(h̃) S

14A log AT
+4
(te+1 − te )

s

!

7 log(te+1 − te ) CS(1 − ζ)−1
+
(te+1 − te )
(te+1 − te )
(2.61)

where (a) is obtained by summing both sides from t = te to t = te+1 .
Equation (b) is obtained by summing over the geometric series with

34

Model-Based RL

ratio ζ. Equation (c) comes from Lemma 2.5. Equation (d) comes from
the fact that Ne (s, a) ≥ Necurr (s, a) for all s, a, and then replacing the
lower bound of Ne (s, a). Equation (e) follows from the Cauchy-Schwarz
inequality. Equation (f ) follows from the fact that the epoch length
te+1 − te is the same as the number of visitations to all state-action
pairs in an epoch.
Combining Equation (2.61) with Equation (2.59), and bounding the
∥h̃(·)∥∞ term with TM , we obtain the required result as follows:
Jcπ,P̃e ≤ |Jcπ,P̃e − Jcπ,P | + Jcπ,P
s

s

s

s

7 log(te+1 − te )
(te+1 − te )

!

14A log AT
7 log(te+1 − te )
+ 4TM
(te+1 − te )
(te+1 − te )

CTM S
+
− δ − Γ ≤ −δ
(1 − ζ)(te+1 − te )

!

14A log AT
+ 4TM
≤
TM S
(te+1 − te )

CTM S
+ Jcπ,P
+
(1 − ζ)(te+1 − te )


≤



(2.62)

TM S

where Equation (2.62) is follows from the definition of Γ in Assumption
2.5 and te+1 − te ≥ T 1/3 .
From Lemma 2.8, we observe that for a tighter Slater condition on
the true MDP, we can only obtain a weaker Slater guarantee. However,
we make that assumption to obtain the feasibility of the optimization
problem in Equation (2.17).
The Bayesian regret of the C-PSRL algorithm is defined as follows:


∗

E[R(T )] = E T Jrπ ,P −

XT
t=1



r(st , at )

(2.63)

Similarly, we define Bayesian constraint violations, C(T ), as the expected
gap between the constraint function and incurred and constraint bounds.
E[C(T )] = E
where (x)+ = max(0, x).

"
XT
t=1

 #

c(st , at )
+

2.4. REGRET ANALYSIS AND CONSTRAINT VIOLATION FOR
POSTERIOR SAMPLING BASED APPROACH

35

Now, we can use Posterior Sampling Lemma (Agarwal et al., 2022b)
∗
∗
to obtain E[Jrπ ,P |Fte ] = E[Jrπe ,P̃e |Fte ] and E[Jcπ ,P |Fte ] = E[Jcπe ,P̃e |Fte ] ∀i,
and follow the analysis similar to the analysis of Theorem 2.1 to obtain
the required regret bounds.

2.4.1

Bound on constraints

We now bound the constraint violations and prove that using a conservative policy. We can reduce the constraint violations to 0. We have:
T
X

C(T ) =

!

c(st , at )

t=1
T
X

=

≤

+

c(st , at ) −

E
X

E
X

Te Jcπe ,P̃e +

!

Te Jcπe ,P̃e

t=1

e=1

e=1

T
X

E
X

!

c(st , at ) −

t=1



≤

E te+1
X
X−1 

Te Jcπe ,P̃e + C1

e=1

+



c(st , at ) − Jcπe ,P̃e



+ C1 

e=1 t=te



≤

E te+1
X
X−1 

+

+



c(st , at ) − Jcπe ,P + Jcπe ,P − Jcπe ,P̃e



e=1 t=te



≤

E te+1
X
X−1 

+ C1 
+

c(st , at ) − Jcπe ,P

e=1 t=te



+

E te+1
X
X−1 



Jcπe ,P − Jcπe ,P̃e



+ C1 

e=1 t=te

+

≤ (C3 (T ) + C2 (T ) + C1 (T ))+
We bound C2 (T ) + C3 (T ) similar to the analysis of R(T ) by
√

CTM S 2 A
Õ TM S AT +
(1 − ζ)

!

(2.64)

We focus our attention on bounding C1 (T ). We obtain the bound

36

Model-Based RL

on C1 (T ) as:
C1 (T ) =

E
X



Te Jcπe ,P̃e



e=1

=

E
X

e=1
E
(a) X

≤

(b)

≤−

=−



E
X





e=1
E
X

e=1
E
X
e=1
E
X
e=1





Te Jcπe ,P̃e 1{Te < T 1/3 }

Te Jcπe ,P̃e 1{Te ≥ T 1/3 } +

e=1
E
X

=−



Te Jcπe ,P̃e 1{Te ≥ T 1/3 } +

T 1/3

e=1

Te ϵe 1{Te ≥ T 1/3 } + ET 1/3
(2.65)




Te ϵe 1 − 1{Te < T 1/3 } + ET 1/3
Te ϵe +

E
X

Te ϵe 1{Te < T 1/3 } + ET 1/3

e=1

≤−

E
X
Kp
T log T +
T 1/3 δ + ET 1/3
4
e=1

=−

Kp
T log T + EδT 1/3 + ET 1/3
4

where (a) is a consequence of the fact that the maximum cost is 1 and
(b) follows from following the conservative policy. Thus, choosing an
appropriate K, we can bound constraint violations by 0.
2.5

Evaluation Results

To validate the performance of the C-UCRL algorithm and the C-PSRL
algorithm, we run the simulation on the flow and service control in
a single-serve queue, which was introduced in (Altman and Schwartz,
1991). Along with validating the performance of the proposed algorithms, we also compare the algorithms against the algorithms proposed
in (Singh et al., 2020) and in (Chen et al., 2022b) for model-based constrained reinforcement learning for infinite horizon MDPs. Compared to
these algorithms, we note that our algorithm is also designed to handle

2.5. EVALUATION RESULTS

37

concave objectives of expected rewards with convex constraints on costs
with 0 constraint violations.
In the queue environment, a discrete-time single-server queue with a
buffer of finite size L is considered. The number of customers waiting in
the queue is considered as the state in this problem and thus |S| = L + 1.
Two kinds of actions, services, and flow, are considered in the problem
and control the number of customers together. The action space for
service is a finite subset A in [amin , amax ], where 0 < amin ≤ amax < 1.
Given a specific service action a, the service a customer is successfully
finished with the probability b. If the service is successful, the length of
the queue will be reduced by 1. Similarly, the space for flow is also a
finite subsection B in [bmin , bmax ]. In contrast to the service action, flow
action will increase the queue by 1 with probability b if the specific flow
action b is given. Also, we assume that there is no customer arriving
when the queue is full. The overall action space is the Cartesian product
of the A and B. According to the service and flow probability, the
transition probability can be computed and is given in Table 2.1.
Table 2.1: Transition probability of the queue system

Current State
1 ≤ xt ≤ L − 1
xt = L
xt = 0

P (xt+1 = xt − 1)
a(1 − b)
a
0

P (xt+1 = xt )
ab + (1 − a)(1 − b)
1−a
1 − b(1 − a)

P (xt+1 = xt + 1)
(1 − a)b
0
b(1 − a)

Define the reward function as r(s, a, b) and the constraints for service
and flow as c1 (s, a, b) and c2 (s, a, b), respectively. Define the stationary
policy for service and flow as πa and πb , respectively. Then, the problem
can be defined as
max

T
1X
r(st , πa (st ), πb (st ))
T →∞ T
t=1

s.t.

T
1X
c1 (st , πa (st ), πb (st )) ≥ 0
T →∞ T
t=1

πa ,πb

lim

lim

(2.66)

T
1X
c2 (st , πa (st ), πb (st )) ≥ 0
T →∞ T
t=1

lim

According to the discussion in (Altman and Schwartz, 1991), we

38

Model-Based RL

(a) Reward growth w.r.t. time

(b) Regret w.r.t. time

(c) Service constraints w.r.t. time

(d) Flow constraints w.r.t. time

Figure 2.1: Performance of the proposed C-UCRL and C-PSRL algorithms on a
flow and service control problem for a single queue with doubling epoch lengths and
linearly increasing epoch lengths. The algorithms are compared against Chen et al.
(2022b) and Singh et al. (2020). We note that the considered algorithms C-UCRL
and C-PSRL are labeled UC-CURL and PS-CURL, respectively, in the figure.

define the reward function as r(s, a, b) = 5 − s, which is a decreasing
function only dependent on the state. It is reasonable to give a higher
reward when the number of customers waiting in the queue is small.
For the constraint function, we define c1 (s, a, b) = −10a + 6 and c2 =
−8(1 − b)2 + 2, which are dependent only on service and flow action,
respectively. A higher constraint value is given if the probability for the
service and flow are low and high, respectively.
In the simulation, the length of the buffer is set as L = 5. The service
action space is set as [0.2, 0.4, 0.6, 0.8], and the flow action space is set
as [0.4, 0.5, 0.6, 0.7]. We use the length of horizon T = 5 × 105 and run
50 independent simulations of all algorithms. The experiments were run
on a 36 core Intel-i9 CPU @3.00 GHz with 64 GB of RAM. The result

2.5. EVALUATION RESULTS

39

is shown in the Figure 2.1. The average values of the cumulative reward
and the constraint functions are shown in the solid lines. Further, we
plot the standard deviation around the mean value in the shadow to
show the random error. In order to compare this result to the optimal,
we assume that the full information of the transition dynamics is known
and then use Linear Programming to solve the problem. The optimal
cumulative reward for the constrained optimization is calculated to be
4.48 with both flow constraint and service constraint values to be 0.
The optimal cumulative reward for the unconstrained optimization is
4.8, with the service constraint being −2 and the flow constraint being
−0.88.
We now discuss the performance of all the algorithms, starting with
our algorithms C-UCRL and C-PSRL. In Figure 2.1, we observe that the
proposed C-UCRL algorithm does not perform well initially. We observe
p
that this is because the confidence interval radius S log(At)/N (s, a) for
any (s, a) is not tight enough in the initial period. After the algorithms
collect sufficient samples to construct tight confidence intervals around
the transition probabilities, the algorithm starts converging toward
the optimal policy. We also note that the linear epoch modification of
the algorithm (where a new episode is triggered whenever Necurr (s, a)
becomes max{1, Ne−1 (s, a)} for any state-action pair, which results
in a linearly
√ increasing episode length with total epochs bounded by
O(SA + SAT )) works better than the doubling epoch algorithm.
This is because the linear epoch variant updates the policy quickly,
whereas the doubling epoch algorithm works with the same policy for
too long and thus loses the advantages of collected samples. For our
implementation, we choose the value of parameter K as K = 1, using
which we observe that the constraint values start converging towards
zero.
We now analyze the performance of the C-PSRL algorithm. For our
implementation of the C-PSRL algorithm, we sample the transition
probabilities using the Dirichlet distribution. Note that the true transition probabilities were not sampled from a Dirichlet distribution, and
hence, this experiment also shows the robustness against misspecified
priors. We observe that the algorithm quickly brings the reward close
to the optimal rewards. The performance of the C-PSRL algorithm is

40

Model-Based RL

significantly better than the C-UCRL algorithm. We suspect this is
because the C-UCRL algorithm wastes a large number of steps to find
an optimistic policy with a large confidence interval. This observation
aligns with the TDSE algorithm (Ouyang et al., 2017),
√ where they
show that the Thompson sampling algorithm with O( SAT ) epochs
performs empirically better than the optimism-based UCRL2 algorithm
p
(Jaksch et al., 2010) with O( SA log T ) epochs. (Osband et al., 2013)
also made a similar observation where their PSRL algorithm worked
better than the UCRL2 algorithm. Again, we set the value of parameter
K as 1, and with K = 1, the algorithm does not violate constraints. We
also observe that the standard deviation of the rewards and constraints
are higher for the C-PSRL algorithm as compared to the C-UCRL algorithm, as the C-PSRL algorithm has an additional stochastic component
that arises from sampling the transition probabilities.
After analyzing the algorithms presented in this paper, we now
analyze the performance of the algorithm by Chen et al. (2022b). They
provide an optimistic online mirror descent algorithm that also works
with conservative parameters to tightly
bound constraint violations.
√
Their algorithm also obtains a O( T ) regret bound. However, their
algorithm is designed for a linear reward/constraint setup with a single
constraint, and empirically, the algorithm is difficult to tune as it requires
additional knowledge of TM , ζ, δ, and T to fine-tune parameters used
in their algorithm. We set the value of the learning rate θ for online
mirror descent as 5 × 10−2 with an episode length of 5 × 103 . Further, we
scale the rewards and costs to ensure that they lie between 0 and 1. We
analyze the behavior of the optimistic online mirror descent algorithm
in Figure 2.1(b). We observe that the algorithm has three phases. The
first phase is the first episode where the algorithm uses a uniform policy,
which is the initial flat area till the first 5000 steps. In the second phase,
the algorithm updates the policy for the first time and starts converging
to the optimal policy with a convergence rate which matches to that
of the C-PSRL algorithm. However, after a few policy updates, we
observed that the algorithm has oscillatory behavior because the dual
variable updates require online constraint violations.
Finally, we analyze the the algorithm by Singh et al. (2020). They
also provide an algorithm that proceeds in epochs and solves an opti-

2.6. NOTES AND OPEN PROBLEMS

41

mization problem at every epoch. The algorithm considers a fixed epoch
length T 1/3 . Further, the algorithm considers a confidence interval on
each estimate of P (s′ |s, a) for all s, a, s′ triplet. The algorithm does not
perform well even though it updates the policy most frequently because
of creating confidence intervals on individual transition probabilities
P (s′ |s, a) instead of the probability vector P (s′ |s, a).
From the experimental observations, we note that the proposed
C-UCRL algorithm is suitable in cases where parameter tuning is not
possible, and the system requires tighter bounds on the deviation of
the performance of the algorithm. The C-PSRL algorithm can be used
in cases where the variance in the algorithm’s performance can be
tolerated or computational complexity is a constraint. Further, for both
algorithms, it is beneficial to use the linear increasing epoch lengths.
Additionally, the algorithm by Chen et al. (2022b) is suitable for cases
where solving an optimization equation is not feasible, for example, an
embedded system, as the algorithm updates policy using an exponential
function, which can be easily computed.
2.6

Notes and Open Problems

In the presence of constraints, some works on model-based reinforcement
learning include (Singh et al., 2020; Chen et al., 2022b; Wei et al., 2022a).
The results in this chapter are taken from (Agarwal et al., 2022c; Agarwal
et al., 2022b), which improve upon the state-of-the-art guarantees by
improving the order of either T or other terms. However, we note that
there is still a gap. Specifically, it remains an open question to see if the
regret guarantees can be improved and matched with the lower bound
in terms of S. Secondly, our bound has TM dependence, while the lower
bound depends on the square root of the diameter. Resolving the gap
is an open question.
We further note that the presentation in this chapter considers linear
utility functions with linear constraints. Recently, (Agarwal et al., 2022a;
Agarwal and Aggarwal, 2023) considered concave utility functions in
model-based reinforcement learning. Further, (Agarwal et al., 2022b)
studied the impact of convex constraints and concave utility function.
The results in this chapter are a specialization of the results in (Agarwal

42

Model-Based RL

et al., 2022b) for linear functions and constraints. This indicates that
the results extend to the general setup of concave utility function and
convex constraints. However, getting the matching lower bounds as
mentioned for linear utilities and constraints will also be important
with non-linear functions. Table 2.2 provides the key comparisons and
related works for the literature.
Algorithm(s)
conRL (Brantley et al., 2020)
MOMA (Yu et al., 2021)
TripleQ (Wei et al., 2022b)
OptPess-LP (Liu et al., 2021)
OptPess-Primal Dual (Liu et al., 2021)
UCRL-CMDP (Singh et al., 2020)
(Chen et al., 2022b)
(Wei et al., 2022a)
(Agarwal et al., 2022c)
UC-CURL (Agarwal et al., 2022b)
PS-CURL (Agarwal et al., 2022b)

Setup
FH
FH
FH
FH
FH
IH
IH
IH
IH
IH
IH

Regret
p
Õ(LH 5/2 S A/K)
p
Õ(LH 3/2 SA/K)
√
Õ( 1δ H 4 SAK −1/5 )
p
H3
Õ( δ S 3 A/K)
p
H3
Õ( δ S 3 A/K)
√
Õ( SAT −1/3 )
p
Õ( 1δ TM S SA/T )
√
1
Õ( δ SAT −1/6 )
p
Õ(TM S A/T )
p
Õ( 1δ LTM S A/T )
p
1
Õ( δ LTM S A/T )

Constraint Violation
p
O(H 5/2 S A/K)
p
Õ(H 3/2 SA/K)
0
0
Õ(H 4 S 2 A/δ)
√
Õ( SA/T 1/3 )
2 S 3 A)
Õ( δ12 TM
0
p
Õ(TM S A/T )
0
0

Non-Linear
Yes
Yes
No
No
No
No
No
No
No
Yes
Yes

Table 2.2: Overview of work for constrained reinforcement learning setups. For
finite horizon (FH) setups, H is the episode length and K is the number of episodes
for which the algorithm runs. For infinite horizon (IH) setups, TM denotes the max
reach time of the MDP, and T is the time for which the algorithm runs. L is Lipschitz
constant. Note that all IH setups assume ergodic MDPs, whereas FH setups do not
require the ergodic assumption as the system resets to the final state after every
episode.

Finally, a key assumption in this chapter is the ergodicity of the
underlying MDP. Note that it is not necessary for the MDP to be ergodic
for the sub-linear regret. The authors of (Bartlett and Tewari, 2009;
Jaksch et al., 2010) have shown sub-linear regret to be possible only when
the MDP is at least weakly communicating. From a technical perspective,
designing provable algorithms for the infinite-horizon average-reward
setting, especially for the general class of weakly communicating MDPs,
has always been more challenging than other settings (Chen et al.,
2022b). While the diameter quantifies the number of steps needed to
“recover” from a bad state in the worst case, the actual regret incurred
while “recovering” is related to the difference in potential reward between
“bad” and “good” states, which is accurately measured by the span (i.e.,
the range) sp(h∗ ) of the optimal bias function h∗ . While the diameter
is an upper bound on the bias span, it could be arbitrarily larger

2.6. NOTES AND OPEN PROBLEMS

43

(e.g., weakly-communicating MDPs may have finite span and infinite
diameter) thus suggesting that algorithms whose regret scales with the
span may perform significantly better. The authors of (Fruit
√ et al.,
2018) proposed SCOPT, which achieves a regret of Õ(sp(h∗ )S AT ) in
the absence of constraints. This is the best bound studied so far in the
weakly communicating MDPs where the diameter may
√ be infinite with
lowest order on T . However, constrained versions with T regret bounds
are open. The results for weakly mixing CMDPs will be discussed in
Chapter 4.

3
Parameterized Model-Free RL

We introduce the model and key assumptions studied in this chapter
in Section 3.1. Further, we provide a primal-dual policy gradient-based
algorithm in Section 3.2. The global convergence analysis of this algorithm is provided in Section 3.3. The regret and constraint violations
are analyzed in Section 3.4. Section 3.5 gives some notes and possible
future directions. Many notations used in this chapter are similar to
those in the previous chapter. However, we reintroduce them here for
ease of reference and overall coherence of the discussion.

3.1

Overall Model and Assumptions

This paper analyzes an infinite-horizon average-reward constrained
Markov Decision Process (CMDP) denoted as M = (S, A, r, c, P, ρ)
where S is the state space, A is the action space of size A, r : S × A →
[0, 1] is the reward function, c : S × A → [−1, 1] is the constraint cost
function, P : S × A → ∆(S) defines the state transition function where
∆(S) denotes a probability simplex over S, and ρ ∈ ∆(S) is the initial
distribution of states. A policy π : S → ∆(A) maps the current state to
an action distribution. The average reward and cost of a policy, π, is
44

3.1. OVERALL MODEL AND ASSUMPTIONS

45

defined as follows.
 TX

−1
1
Eπ
g(st , at ) s0 ∼ ρ
T →∞ T
t=0

π
Jg,ρ
≜ lim

(3.1)

where g = r, c for average reward and cost respectively. The expectation
is calculated over the distribution of all sampled trajectories {(st , at )}∞
t=0
where at ∼ π(st ), st+1 ∼ P (·|st , at ), ∀t ∈ {0, 1, · · · }. We do not explicitly
π on P for notational convenience. Moreover,
show the dependence of Jg,ρ
we also drop the dependence on ρ whenever there is no confusion.
We consider a class of policies, Π whose each element is indexed by
a d-dimensional parameter, θ ∈ Rd . We aim to maximize the average
reward function while ensuring that the average cost is above a given
threshold. Without loss of generality, we can mathematically represent
this problem as follows.
max Jrπθ s.t. Jcπθ ≤ 0

(3.2)

θ∈Rd

Let Jgπθ = Jg (θ), g ∈ {r, c}, and P πθ : S → ∆(S) be the transition
P
function induced by πθ and given by, P πθ (s, s′ ) = a∈A P (s′ |s, a)πθ (a|s),
∀s, s′ . If M is such that for every policy π, the induced function, P π is
irreducible, and aperiodic, then M is called ergodic.
Assumption 3.1. The CMDP M is ergodic.
Ergodicity is a common assumption in the literature (Pesquerel and
Maillard, 2022; Gong and Wang, 2020). If M is ergodic, then ∀θ, there
exists a unique stationary distribution, dπθ ∈ ∆|S| given as follows.
1
dπθ (s) = lim
T →∞ T

"T −1
X

#

Pr(st = s|s0 ∼ ρ, πθ )

(3.3)

t=0

Ergodicity implies that dπθ is independent of the initial distribution,
ρ, and obeys P πθ dπθ = dπθ . Hence, the average reward and cost functions
can be expressed as,
Jg (θ) = Es∼dπθ ,a∼πθ (s) [g(s, a)] = (dπθ )T g πθ

(3.4)

where g πθ (s) ≜ a∈A g(s, a)πθ (a|s), g ∈ {r, c}. Note that the functions
Jg (θ), g ∈ {r, c} are independent of the initial distribution, ρ. Furthermore, ∀θ, there exist a function Qπg θ : S ×A → R such that the following
P

46

Parameterized Model-Free RL

Bellman equation is satisfied ∀(s, a) ∈ S × A.
h

Qπg θ (s, a) = g(s, a) − Jg (θ) + Es′ ∼P (·|s,a) Vgπθ (s′ )

i

(3.5)

where g ∈ {r, c} and Vgπθ : S → R is given as,
Vgπθ (s) =

X

πθ (a|s)Qπg θ (s, a), ∀s ∈ S

(3.6)

a∈A

Observe that if Qπg θ satisfies (3.5), then it is also satisfied by Qπg θ + c
for any arbitrary, c. To uniquely define the value functions, we assume
P
that s∈S dπθ (s)Vgπθ (s) = 0. In this case, Vgπθ (s) turns out to be,
Vgπθ (s) =

∞ X h
X
t=0 s′ ∈S
"∞ 
X

=E

i

(P πθ )t (s, s′ ) − dπθ (s′ ) g πθ (s′ )


(3.7)

#

g(st , at ) − Jg (θ) s0 = s

t=0

where the expectation is computed over all πθ -induced trajectories.
Similarly, ∀(s, a), one can uniquely define Qπg θ (s, a), g ∈ {r, c} as,
Qπg θ (s, a) = E

"∞ 
X



#

g(st , at ) − Jg (θ) s0 = s, a0 = a

(3.8)

t=0

The advantage function Aπg θ : S × A → R is defined as,
Aπg θ (s, a) ≜ Qπg θ (s, a) − Vgπθ (s), ∀(s, a), ∀g ∈ {r, c}

(3.9)

Assumption 3.1 also implies the existence of a finite mixing time.
Definition 3.1. The mixing time of the CMDP M (under the ergodicity
assumption)
with respect to a parameterized
policy, πθ , is given as tθmix ≜
n
o
min t ≥ 1 (P πθ )t (s, ·) − dπθ ≤ 14 , ∀s . The overall mixing time, tmix ,
is given as tmix ≜ supθ∈Θ tθmix which is finite due to ergodicity.
Mixing time states how fast a CMDP converges to its stationary
distribution for a given policy. We define the hitting time below.
Definition 3.2. The hitting time of an ergodic CMDP M with respect
to a policy πθ , is given as, tθhit ≜ maxs∈S dπθ1(s) . The overall hitting time
is defined as thit ≜ supθ∈Θ tθhit which is finite due to ergodicity.

3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

47

Let Jr∗ ≜ Jr (θ∗ ) where θ∗ solves (3.2). For a given CMDP M, and a
time horizon T , the regret and constraint violation is defined as follows.
R(T ) ≜

TX
−1

(Jr∗ − r(st , at )) , C(T ) ≜

t=0

TX
−1

c(st , at )

(3.10)

t=0

where the algorithm executes the actions, {at }, t ∈ {0, 1, · · · } based on
the history up to time, t, and the state, st+1 is decided according to the
state transition function, P . Our goal is to design an algorithm that
achieves low regret and constraint violation bounds.
3.2

Algorithm for Parameterized Model-Free RL

We solve the constrained problem (3.2) using a primal-dual algorithm,
which is based on the following saddle point optimization.
max min JL (θ, λ) where JL (θ, λ) ≜ Jr (θ) − λJc (θ)

(3.11)

θ∈Θ λ≥0

The function, JL (·, ·), is called the Lagrange function and λ the Lagrange
multiplier. Our algorithm updates (θ, λ) following the iteration shown
below ∀k ∈ {1, · · · , K} with an initial point (θ1 , λ1 = 0).
θk+1 = θk + α∇θ JL (θk , λk ), λk+1 = P[0, 2 ] [λk + βJc (θk )]
δ

(3.12)

where α and β are learning parameters and δ is the Slater parameter
introduced in the following assumption. Finally, for any Λ, the function
PΛ [·] denotes projection onto Λ.
Assumption 3.2 (Slater condition). There exists δ > 0 and π̄ ∈ Π such
that Jcπ̄ (ρ) ≤ −δ.
Notice that in Eq. (3.12), the dual update is projected onto the set
[0, 2δ ] because the optimal dual variable for the parameterized problem
is bounded in Lemma 3.18. The gradient of JL (·, λ) can be computed by
invoking a variant of the policy gradient theorem (Sutton et al., 1999).
Lemma 3.1. The gradient of JL (·, λ) is computed as,


θ
∇θ JL (θ, λ) = Es∼dπθ ,a∼πθ (s) AπL,λ
(s, a)∇θ log πθ (a|s)



(3.13)

θ
where AπL,λ
(s, a) ≜ Aπr θ (θ) − λAπc θ (θ) and {Aπg θ }g∈{r,c} are given in (3.9).

48

Parameterized Model-Free RL

Proof. The proof works similarly for reward and cost functions. We use
the notation Jg , Vg , Qg where g = r, c and derive the following.
∇θ Vgπθ (s) = ∇θ

X



πθ (a|s)Qπg θ (s, a)

a

X

=



∇θ πθ (a|s) Qπg θ (s, a) +

X

a

πθ (a|s)∇θ Qπg θ (s, a)

a

(a) X

=





πθ (a|s) ∇θ log πθ (a|s) Qπg θ (s, a)

a

X

+



πθ (a|s)∇θ g(s, a) − Jg (θ) +

′

P (s |s, a)Vgπθ (s′ )

X

a



s′

=

X

+

X





πθ (a|s) ∇θ log πθ (a|s) Qπg θ (s, a)

a

πθ (a|s)

X

a



P (s′ |s, a)∇θ Vgπθ (s′ ) − ∇θ Jg (θ)

s′

θ
where the step (a) is a consequence of ∇θ log πθ = ∇π
πθ and the Bellman
equation. Multiplying both sides by dπθ (s), taking a sum over s ∈ S,
and rearranging the terms, we obtain the following.

∇θ Jg (θ) =

X

dπθ (s)∇θ J(θ)

s

X

=

πθ

d (s)

s

X

X





πθ (a|s) ∇θ log πθ (a|s) Qπg θ (s, a) +

a

πθ (a|s)

X

a

X

dπθ (s)

s



P (s′ |s, a)∇θ Vgπθ (s′ ) −

X

dπθ (s)∇θ Vgπθ (s)

s

s′





= Es∼dπθ ,a∼πθ (·|s) Qπg θ (s, a)∇θ log πθ (a|s)
+

X

dπθ (s)

s

X

P πθ (s′ |s)∇θ Vgπθ (s′ ) −

X
s

s′



(a)

dπθ (s)∇θ Vgπθ (s)

= Es∼dπθ ,a∼πθ (·|s) Qπg θ (s, a)∇θ log πθ (a|s)



+

X

dπθ (s′ )∇θ Vgπθ (s′ )

s′

−

X

πθ

d

(s)∇θ Vgπθ (s) = Es∼dπθ ,a∼πθ (·|s)



Qπg θ (s, a)∇θ log πθ (a|s)



s

(3.14)

3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

49

where (a) uses the fact that dπθ is a stationary distribution. Note that,




Es∼dπθ ,a∼πθ (·|s) Vgπθ (s)∇ log πθ (a|s)
"

#
X

= Es∼dπθ

Vgπθ (s)∇θ πθ (a|s)

a



= Es∼dπθ

Vgπθ (s)∇θ

X

(3.15)

!

πθ (a|s)

a





= Es∼dπθ Vgπθ (s)∇θ (1) = 0
We can, therefore, replace Qπθ in the policy gradient with the advantage
function Aπg θ (s, a) = Qπg θ (s, a) − Vgπθ (s), ∀(s, a) ∈ S × A. Thus,


∇θ Jg (θ) = Es∼dπθ ,a∼πθ (·|s) Aπg θ (s, a)∇θ log πθ (a|s)



(3.16)

Notice that the above equation works for both Jr and Jc , and thus the
proof is completed by the definition of JL,λ and AL,λ .
In typical RL scenarios, the learners do not have access to P , the
θ
state transition function, and thereby to the functions dπθ and AπL,λ
.
This makes computing the exact gradient a difficult task. In Algorithm 3,
we demonstrate how one can still obtain good estimates of the gradient
using sampled trajectories. It is worthwhile to point out that PG-type
algorithms have been widely studied in discounted reward setups. For
example, (Agarwal et al., 2021) characterizes the sample complexities
of the PG and the Natural PG (NPG) algorithms with softmax and
direct parameterization. Similar results for general parameterization are
obtained by (Liu et al., 2020; Mondal and Aggarwal, 2024a). However,
the main difference between a discounted and an average-reward setup
is that while the former assumes access to a simulator that leads to
unbiased estimates of the gradient, the latter framework primarily works
on a single sample path.
Algorithm 3 runs K = T /H epochs, where H = 16thit tmix T ξ (log T )2
is the duration of each epoch. The constant ξ is specified later. Observe
that the learner is assumed to know T . This can be relaxed utilizing

50

Parameterized Model-Free RL

the well-known doubling trick (Lattimore and Szepesvári, 2020). Additionally, it is assumed that the algorithm is aware of tmix , and thit . This
is commonly assumed in the literature (Bai et al., 2024b; Wei et al.,
2020). The first step in obtaining a gradient estimate is estimating the
advantage value for a given pair (s, a). This is obtained via Algorithm
4. At the kth epoch, a πθk -induced trajectory, Tk = {(st , at )}kH−1
t=(k−1)H
is obtained and passed to Algorithm 4 that searches for subtrajectories
within it that start with a state s, are of length N = 4tmix (log T ), and
are at least N distance apart from each other. Assume that there are M
such subtrajectories. Let the total reward and cost obtained in the ith
subtrajectory be {ri , ci } respectively and τi be its starting time. The
value functions for the kth epoch are defined as,
πθ
Q̂g k (s, a) =

πθ
V̂g k (s) =

M
1
1 X
gi 1(aτi = a) ,
πθk (a|s) M i=1

"

"

#

M
1 X
gi , ∀g ∈ {r, c}
M i=1

#

(3.17)

This leads to the following advantage estimator.
πθ

πθ

πθ

πθ

πθ

ÂL,λkk (s, a) = Âr k (s, a) − λk Âc k (s, a)
πθ

where Âg k (s, a) = Q̂g k (s, a) − V̂g k (s)

(3.18)

where g ∈ {r, c}. Finally, the gradient estimator is obtained as follows.
ˆ θ JL (θk , λk ) = 1
ωk ≜ ∇
H

tk+1 −1

X

πθ

ÂL,λkk (st , at )∇θ log πθk (at |st )

(3.19)

t=tk

where tk = (k−1)H is the starting time of the kth epoch. The parameters
are updated following (3.20). To update the Lagrange multiplier, we
need an estimation of Jg (θk ), which is obtained as the average cost of
the kth epoch. It should be noticed that we remove the first N samples
from the kth epoch because we require the state distribution to be close
enough to the stationary distribution dπθk , which is the key to make
πθ
Jˆg (θk ) close to Jg (θk ). The following lemma shows that ÂL,λkk (s, a) is a
πθ

good estimator of AL,λkk (s, a).

3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

51

Algorithm 3 Primal-Dual Parameterized Policy Gradient
1: Input: Episode length H, learning rates α, β, initial parameters
θ1 , λ1 , initial state s0 ∼ ρ(·),
2: for k ∈ {1, · · · , K = T /H} do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Tk ← ϕ
for t ∈ {(k − 1)H, · · · , kH − 1} do
Execute at ∼ πθk (·|st ), observe r(st , at ), g(st , at ) and st+1
Tk ← Tk ∪ {(st , at )}
end for
for t ∈ {(k − 1)H, · · · , kH − 1} do
πθ
Obtain ÂL,λkk (st , at ) using Algorithm 4 and Tk
end for
Using (3.19), compute ωk
Update parameters as
θk+1 = θk + αωk ,

(3.20)

λk+1 = max{0, λk + β Jˆc (θk )}
1
where Jˆc (θk ) = H−N
14: end for

13:

PkH−1

t=(k−1)H+N c(st , at )

Lemma 3.2. The following holds ∀k, ∀(s, a) and sufficiently large T .


E

2 

πθ
πθ
ÂL,λkk (s, a) − AL,λkk (s, a)

≤O

thit N 3 log T
δ 2 Hπθk (a|s)

!

=O

t2mix (log T )2
δ 2 T ξ πθk (a|s)

!

Lemma 3.2 establishes that the L2 error of our proposed advantage
estimator can be bounded above as Õ(T −ξ ). We later utilize the above
result to prove the goodness of the gradient estimator. It is to be clarified
that our Algorithm 4 is inspired by Algorithm 2 of (Wei et al., 2020).
However, while (Wei et al., 2020) take H = Õ(1), we have H = Õ(T ξ ).
This subtle change is important in proving the desired sublinear regret
for general parametrization.

52

Parameterized Model-Free RL

Algorithm 4 Advantage Estimation
1: Input: Trajectory (st1 , at1 , . . . , st2 , at2 ), state s, action a, Lagrange

multiplier λ and policy parameter θ
2: Initialize: M ← 0, τ ← t1
3: Define: N = 4tmix log2 T .
4: while τ ≤ t2 − N do

if sτ = s then
6:
M ← M + 1, τM ← τ
P +N −1
g(st , at ), ∀g ∈ {r, c}
7:
gM ← τt=τ
8:
τ ← τ + 2N .
9:
else
10:
τ ← τ + 1.
11:
end if
12: end while
5:

13: if M > 0 then

1 PM
1
gi 1(aτi = a) ,
14:
Q̂g (s, a) =
πθ (a|s) M i=1
1 PM
15:
V̂g (s) =
gi , ∀g ∈ {r, c}
M i=1
16: else
17:
V̂g (s) = 0, Q̂g (s, a) = 0, ∀g ∈ {r, c}
18: end if
19: return (Q̂r (s, a) − V̂r (s)) − λ(Q̂c (s, a) − V̂c (s))




Proof of Lemma 3.2. Fix an epoch k and assume that πθk is denoted
as π for notational convenience. Let, M be the number of disjoint subtrajectories of length N that start with the state s and are at least N
distance apart (found by Algorithm 4). Let, gk,i be the sum of rewards
or constraint (g = r, c accordingly) observed in the ith sub-trajectory
and τi denote its starting time. The advantage function estimate is,

Âπg (s, a) =






"

#

M
M
1
1 P
1 P
gk,i 1(aτi = a) −
gk,i
π(a|s) M i=1
M i=1



0

if M > 0
if M = 0
(3.21)

3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

53

Note the following,




E gk,i sτi = s, aτi = a


τX
i +N

= g(s, a) + E 



g(st , at ) sτi = s, aτi = a

t=τi +1



= g(s, a) +

X

P (s′ |s, a)E 

s′

τX
i +N



g(st , at ) sτi +1 = s′ 

t=τi +1

T



= g(s, a) +

X

′

N
−1
X

P (s |s, a) 

s′

π j

(P ) (s , ·) g π

j=0

T



= g(s, a) +

X

N
−1
X

P (s′ |s, a) 

s′

= g(s, a) +

P (s′ |s, a) 

X
s′

∞
X

X
s′

|
(b)

= g(s, a) +

X

T

(P π )j (s′ , ·) − dπ  g π + N Jgπ

j=0



−

(P π )j (s′ , ·) − dπ  g π + N (dπ )T g π

j=0


(a)

′

P (s′ |s, a) 

∞
X

T

(P π )j (s′ , ·) − dπ  g π

j=N

{z

≜Eπ
T (s,a)

}

P (s′ |s, a)Vgπ (s′ ) + N Jgπ − EπT (s, a)

s′
(c)

= Qπg (s, a) + (N + 1)Jgπ − EπT (s, a)
(3.22)

where (a) follows from the definition of Jgπ as given in (3.4), (b) is an
application of the definition of Vgπ given in (3.7), and (c) follows from
the Bellman equation. Define the following quantity.
δ π (s, T ) ≜

∞
X
t=N

(P π )t (s, ·) − dπ

1

where N = 4tmix (log2 T ) (3.23)

Using Lemma 3.12, we get δ π (s, T ) ≤ T13 which implies, |EπT (s, a)| ≤

54

Parameterized Model-Free RL

1
. Observe the following relations.
T3



E

1
gk,i 1(aτi = a) − gk,i
π(a|s)




= E gk,i sτi = s, aτi = a −





X

sτi = s
′



π(a |s)E gk,i sτi = s, aτi = a

′



a′
= Qπg (s, a) + (N + 1)Jgπ − EπT (s, a)
X
′
π

π(a |s)[Q (s, a) + (N + 1)Jgπ − EπT (s, a)]

−

a′

"

= Qπg (s, a) − Vgπ (s) −

#

ET (s, a) −

X

′

π(a |s)EπT (s, a′ )

a′

= Aπg (s, a) − ∆πT (s, a)
(3.24)
where ∆πT (s, a) ≜ ET (s, a) − a′ π(a′ |s)EπT (s, a′ ). Using the bound on
EπT (s, a), we derive, |∆πT (s, a)| ≤ T23 , which implies,
P



E

1
gk,i 1(aτi = a) − gk,i
π(a|s)
2
≤ |∆πT (s, a)| ≤ 3
T





sτi = s − Aπg (s, a)
(3.25)

Note that (3.25) cannot directly bound the bias of Âπg (s, a). This is
because the random variable M is correlated with the variables {gk,i }M
i=1 .
To decorrelate them, imagine a CMDP where the state distribution
resets to the stationary distribution, dπ after exactly N time steps since
the completion of a sub-trajectory. In other words, if a sub-trajectory
starts at τi , and ends at τi + N , then the system ‘rests’ for additional
N steps before rejuvenating with the state distribution, dπ at τi + 2N .
Clearly, the wait time between the reset after the (i−1)th sub-trajectory
and the start of the ith sub-trajectory is, wi = τi − (τi−1 + 2N ), i > 1.
Let w1 be the difference between the start time of the kth epoch and
the start time of the first sub-trajectory. Observe that,
(a) w1 only depends on the initial state, s(k−1)H and the induced
transition function, P π ,
(b) wi , where i > 1, depends on the stationary distribution, dπ , and
the induced transition function, P π ,

3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

55

(c) M only depends on {w1 , w2 , · · · } as other segments of the epoch
have fixed length, 2N .
Clearly, in this imaginary CMDP, the sequence, {w1 , w2 , · · · }, and
hence, M is independent of {gk,1 , gk,2 , · · · }. Let, E′ denote the expectation operation and Pr′ denote the probability of events in this imaginary
system. Define the following.
∆i ≜

gk,i 1(aτi = a)
− gk,i − Aπg (s, a) + ∆πT (s, a)
π(a|s)

(3.26)

where ∆πT (s, a) is defined via (3.24). Note that we have suppressed the
dependence on T , s, a, and π while defining ∆i to remove clutter. Using
(3.24), one can write E′ [∆i (s, a)|{wi }] = 0. Moreover,
E

′



=E

Âπg (s, a) − Aπg (s, a)

′



2 
2

Âπg (s, a) − Aπg (s, a)
2



+ Aπg (s, a)

M
1 X
′
=E
∆i − ∆πT (s, a)
M i=1

2

M > 0 × Pr′ (M > 0)

× Pr′ (M = 0)







!2



M > 0 × Pr′ (M > 0)

+ Aπg (s, a)

× Pr′ (M = 0)



!2



M
1 X
≤ 2E′{wi } E′ 
∆i
M i=1





{wi } w1 ≤ H − N  × Pr′ (w1 ≤ H − N )

+ 2 (∆πT (s, a))2 + Aπg (s, a)
"

(a)

≤ 2E′{wi }
+



2

× Pr′ (M = 0)

M
h
i
1 X
′
2
{w
}
w1 ≤ H − N × Pr′ (w1 ≤ H − N )
E
∆
i
i
M 2 i=1

#


2
8
π
+
A
(s,
a)
× Pr′ (M = 0)
g
T6

(3.27)
where (a) uses the bound |∆πT (s, a)| ≤ T23 derived in (3.25), and the
fact that {∆i } are zero mean independent random variables conditioned
on {wi }. Note that |gk,i | ≤ N almost surely, |Aπg (s, a)| ≤ O(tmix ) via

56

Parameterized Model-Free RL

Lemma 3.11, and |∆πT (s, a)| ≤ T23 as shown in (3.25). Combining, we
get, E′ [|∆i |2 {wi }] ≤ O(N 2 /π(a|s)) (see the definition of ∆i in (3.26)).
Invoking this bound into (3.27), we get the following result.
E

′



Âπg (s, a) − Aπg (s, a)
+

2 

≤ 2E

′



1
w1 ≤ H − N O
M


N2
π(a|s)

!

8
+ O(t2mix ) × Pr′ (w1 > H − N )
T6
(3.28)

We use Lemma 3.13 to bound the following violation probability.
3dπ (s)
Pr (w1 > H − N ) ≤ 1 −
4


′

(a)

≤



4thit T ξ (log T )−1

4

3dπ (s) dπ (s) (log T )
1
1−
≤ 3
4
T

(3.29)

where (a) follows from the fact that 4thit T ξ (log2 T ) − 1 ≥ dπ4(s) log2 T
for sufficiently large T . Finally, if M < M0 , where M0 is defined as,
H −N
M0 ≜
(3.30)
4N log T
2N +
dπ (s)
then there exists at least one wi that exceeds 4N log2 T /dπ (s), which
can happen with the following maximum probability (Lemma 3.13).
 4 log T

3dπ (s) dπ(s)
1
Pr (M < M0 ) ≤ 1 −
≤ 3
(3.31)
4
T
The above probability bound can be used to obtain the following,
P∞
1 ′


Pr (M = m)
m=1
′ 1
m
M >0 =
E
M
Pr′ (M > 0)
1
1 × Pr′ (M ≤ M0 ) +
Pr′ (M > M0 )
M0
≤
(3.32)
Pr′ (M > 0)
4N log T
2N +
1
dπ (s)


+
3
N log T
T
H
−
N
≤
≤O
1
Hdπ (s)
1− 3
T
′



3.2. ALGORITHM FOR PARAMETERIZED MODEL-FREE RL

57

Injecting (3.29) and (3.32) into (3.28), we finally obtain,
E

′



Âπg (s, a) − Aπg (s, a)
=O

2 

≤O

N 3 thit log T
Hπ(a|s)

N 3 log T
Hdπ (s)π(a|s)
!

=O

!

t2mix (log T )2
T ξ π(a|s)

!

(3.33)

(3.33) shows that our desired inequality is satisfied in the imaginary
system. We now need a mechanism to translate this result to our CMDP.
Observe that, one can express (Âπg (s, a) − Aπg (s, a))2 = f (X) where X =
(M, τ1 , T1 , · · · , τM , TM ), and Ti = (aτi , sτi +1 , aτi +1 , · · · , sτi +N , aτi +N ).
This leads to the following inequality.
P

f (X)Pr(X)
E[f (X)]
Pr(X)
= PX
≤ max ′
′
′
X Pr (X)
E [f (X)]
X f (X)Pr (X)

(3.34)

The above inequality uses the non-negativity of f (·). Observe that,
Pr(X) =Pr(τ1 ) × Pr(T1 |τ1 ) × Pr(τ2 |τ1 , T1 ) × Pr(T2 |τ2 ) × · · ·
× Pr(τM |τM −1 , TM −1 ) × Pr(TM |τM )

(3.35)

× Pr(st ̸= s, ∀t ∈ [τM + 2N, kH − N ]|τM , TM ),
Pr′ (X) =Pr(τ1 ) × Pr(T1 |τ1 ) × Pr′ (τ2 |τ1 , T1 ) × Pr(T2 |τ2 ) × · · ·
× Pr′ (τM |τM −1 , TM −1 ) × Pr(TM |τM )

(3.36)

× Pr(st ̸= s, ∀t ∈ [τM + 2N, kH − N ]|τM , TM ),
The difference between Pr(X) and Pr′ (X) arises because Pr(τi+1 |τi , Ti ) ̸=
Pr′ (τi+1 |τi , Ti ), ∀i ∈ {1, · · · , M − 1}. Note that the ratio of these two
terms can be bounded as follows,
Pr(τi+1 |τi , Ti )
Pr(sτi +2N = s′ |τi , Ti )
≤
max
′
s′ Pr (sτi +2N = s′ |τi , Ti )
Pr′ (τi+1 |τi , Ti )
Pr(sτi +2N = s′ |τi , Ti ) − dπ (s′ )
= max
1
+
s′
dπ (s′ )
(a)
thit
1
1
≤ max
1+ 3 π ′ ≤1+ 3 ≤1+ 2
′
s
T d (s )
T
T

(3.37)

where (a) is a consequence of Lemma 3.12. We have,
Pr(X)
1
≤ 1+ 2
Pr′ (X)
T


M

M

(a)

1



≤ eT2 ≤ eT ≤ O 1 +

1
T



(3.38)

58

Parameterized Model-Free RL

where (a) uses M ≤ T . Combining (3.34) and (3.38), we get,
E



Âπg (s, a) − Aπg (s, a)

1
≤O 1+
T




E

′



2 

Âπg (s, a) − Aπg (s, a)

2  (a)

≤O

!

t2mix (log T )2
T ξ π(a|s)
(3.39)

where (a) follows from (3.33). Using the definition of AL,λ , we get,
E



ÂπL,λ (s, a) − AπL,λ (s, a)

=E



≤ 2E
≤O

2 
2 

(Âπr (s, a) − Aπr (s, a)) − λ(Âπc (s, a) − Aπc (s, a))



2 

Âπr (s, a) − Aπr (s, a)

t2mix (log T )2
δ 2 T ξ π(a|s)

2

+ 2λ E



Âπc (s, a) − Aπc (s, a)

2 

!

(3.40)
This completes the proof.
3.3

Global Convergence Analysis

This section first shows that the sequence {θk , λk }K
k=1 produced by Algorithm 3 is such that their associated Lagrange sequence {JL (θk , λk )}∞
k=1
converges globally. By expanding the Lagrange function, we then exhibit
convergence of each of its components {Jg (θk , λk )}K
k=1 , g ∈ {r, c}. This
is later used for regret and constraint violation analysis. Before delving
into the details, we would like to state a few necessary assumptions.
Assumption 3.3. The score function (expressed below) is G-Lipschitz
and B-smooth. Specifically, ∀θ, θ1 , θ2 ∈ Rd , ∀(s, a),
∥∇θ log πθ (a|s)∥ ≤ G,
∥∇θ log πθ1 (a|s) − ∇θ log πθ2 (a|s)∥ ≤ B∥θ1 − θ2 ∥
Remark 3.1. The Lipschitz and smoothness properties of the score
function are commonly assumed for policy gradient analyses (Agarwal
et al., 2020; Zhang et al., 2021; Liu et al., 2020). These assumptions can
be verified for simple parameterization class such as Gaussian policies.

3.3. GLOBAL CONVERGENCE ANALYSIS

59

Combining Assumption 3.3 with Lemma 3.2 and using the gradient
estimator as given in (3.19), one can deduce the following result.
Lemma 3.3. The following inequality holds ∀k provided that assumptions 3.1 and 3.3 are true.
!
2 t2
h
i
AG
mix
E ∥ωk − ∇θ JL (θk , λk )∥2 ≤ Õ
(3.41)
δ2T ξ
Proof. Note the expression of the gradient estimator from (3.19). Define:
−1

t

ω̄k =

X
1 k+1
πθ
AL,λkk (st , at )∇θ log πθk (at |st )
H t=t

(3.42)

k

Recall the expression of the true gradient in Lemma 3.1. Assumpπθ
tion 3.3 and Lemma 3.12, establish that |AL,λkk (s, a)∇θ log πθk (a|s)| ≤
O( 1δ tmix G), ∀(s, a) ∈ S × A which implies |∇θ JL (θk , λk )| ≤ O( 1δ tmix G).
Applying Lemma 3.15, we, thus, arrive at,


h
i
1
tmix log T
E ∥ω̄k − ∇θ JL (θk , λk )∥2 ≤ O 2 G2 t2mix log T ×
δ
H
!
(3.43)
2
2
G tmix
= Õ
δ 2 thit T ξ
Finally, the term, E ∥ωk − ω̄k ∥2 can be bounded as follows.
E ∥ωk − ω̄k ∥2


−1

t

X
1 k+1
πθ
ÂL,λkk (st , at )∇θ log πθk (at |st )
H t=t

= E

k

t



−1

2
X
1 k+1
πθk
−
AL,λk (st , at )∇θ log πθk (at |st ) 
H t=t
k

(a) G

≤

2 tk+1 −1

X

H

E


πθk

πθ

ÂL,λk (st , at ) − AL,λkk (st , at )

2 

t=tk
t

−1

"

 π
2
X
X
G2 k+1
πθ
θ
E
πθk (a|st )E ÂL,λkk (st , a) − AL,λkk (st , a) st
≤
H t=t
a


#

k

(b)

≤O

AG2 t2mix (log T )2
δ2T ξ

!

(3.44)

60

Parameterized Model-Free RL

where (a) follows from Assumption 3.3 and Jensen’s inequality while (b)
results from Lemma 3.2. Using (3.43), (3.44), we conclude the result.
Lemma 3.3 claims that the gradient estimation error can be bounded
as Õ(T −ξ ). This result will be later used in proving the global convergence of our algorithm.
Assumption 3.4. Let the transferred compatible function approximation
error be defined as follows.
Ld

π∗

∗
,π ∗ (ωθ,λ , θ, λ) = E(s,a)∼ν π∗



2 

∗
θ
∇θ log πθ (a|s) · ωθ,λ
− AπL,λ
(s, a)

(3.45)
where π ∗ = πθ∗ , θ∗ solves (3.2), ν π (s, a) ≜ dπ (s)π(a|s), ∀s, a, π and
∗
= arg min E(s,a)∼ν πθ
ωθ,λ
ω∈Rd



θ
∇θ log πθ (a|s) · ω − AπL,λ
(s, a)

2 

(3.46)

2
∗ , θ, λ) ≤ ϵ
We presume that Ldπ∗ ,π∗ (ωθ,λ
bias for arbitrary λ ∈ (0, δ ] and
d
θ ∈ R where ϵbias is a positive constant.

Remark 3.2. The transferred compatible function approximation error
quantifies the expressivity of the parameterized policy class. One can
prove that ϵbias = 0 for softmax parameterization (Agarwal et al., 2021)
and linear MDPs (Jin et al., 2020). If the policy class is restricted i.e., it
does not contain all stochastic policies, ϵbias is strictly positive. However,
if the parameterization is done by a rich neural network, then ϵbias can
be assumed to be negligible (Wang et al., 2020). Such assumptions are
common in the literature (Liu et al., 2020; Agarwal et al., 2021).
∗ defined in (3.46) can be re-written as,
Remark 3.3. Note that ωθ,λ

h

∗
θ
ωθ,λ
= Fρ (θ)† Es∼dπρ θ Ea∼πθ (s) ∇θ log πθ (a|s)AπL,λ
(s, a)

i

where † is the Moore-Penrose pseudoinverse operation and Fρ (θ) is the
Fisher information matrix defined as,
Fρ (θ) = Es∼dπρ θ Ea∼πθ (·|s) [∇θ log πθ (a|s)(∇θ log πθ (a|s))T ]

3.3. GLOBAL CONVERGENCE ANALYSIS

61

Assumption 3.5. There exists a constant µF > 0 such that Fρ (θ)−µF Id
is positive semidefinite where Id is an identity matrix of dimension, d.
Assumption 3.5 is also common in the policy gradient analysis (Liu et
al., 2020). This holds for Gaussian policies with a linearly parameterized
mean. (Mondal et al., 2023) provides a concrete example of a class of
policies obeying assumptions 3.3-3.5. The Lagrange difference lemma
stated below is an important result in proving the global convergence.
Lemma 3.4. For any two policies πθ , πθ′ , the following holds ∀λ > 0.
 π′

θ
JL (θ, λ) − JL (θ′ , λ) = Es∼dπθ Ea∼πθ (s) AL,λ
(s, a)



Proof. Using the Lemma 3.14, it is obvious to see that
′

Jgπ − Jgπ =

XX

=

XX

=

XX

=

XX

s
s
s
s

′

dπ (s)(π(a|s) − π ′ (a|s))Qπg (s, a)

a
′

dπ (s)π(a|s)Qπg (s, a) −

a

X

′

dπ (s)Vgπ (s)

s
π

′
(s)π(a|s)Qπg (s, a) −

π

s
a
π′
π′
(s)π(a|s)[Qg (s, a) − Vg (s)]

d

a

d

XX

′

dπ (s)π(a|s)Vgπ (s)

a
′

= Es∼dπ Ea∼π(·|s) Aπg (s, a)




(3.47)
Since the above equation works for both reward and constraint, we can
conclude the lemma by the definition of JL (·, λ) and AL,λ .
We now present a general framework for the convergence analysis.
Lemma 3.5. If the parameters {θk , λk }K
k=1 are updated via (3.20) and
assumptions 3.3, 3.4, and 3.5 hold, then the following holds for any K,
K
K
√
1 X
GX
E
JL (θ∗ , λk ) − JL (θk , λk ) ≤ ϵbias +
∥(ωk − ωk∗ )∥
K k=1
K k



+



K
Bα X
1
∥ωk ∥2 +
E π∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
2K k=1
αK s∼dρ

(3.48)
where ωk∗ ≜ ωθ∗k ,λk , ωθ∗k ,λk is defined via (3.46), and π ∗ = πθ∗ where θ∗
is the optimal parameter solving (3.2).

62

Parameterized Model-Free RL

Proof. We start with the definition of KL divergence.
Es∼dπ∗ [KL(π ∗ (·|s)∥πθk (·|s)) − KL(π ∗ (·|s)∥πθk+1 (·|s))]


πθk+1 (a|s)
= Es∼dπ∗ Ea∼π∗ (·|s) log
πθk (a|s)
(a)

≥ E(s,a)∼ν π∗ [∇θ log πθk (a|s) · (θk+1 − θk )] −
= αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · ωk ] −

B
∥θk+1 − θk ∥2
2

Bα2
∥ωk ∥2
2

= αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · ωk∗ ]
+ αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · (ωk − ωk∗ )] −

Bα2
∥ωk ∥2
2

= α[JL (θ∗ , λk ) − JL (θk , λk )]
+ αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · ωk∗ ] − α[JL (θ∗ , λk ) − JL (θk , λk )]
+ αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · (ωk − ωk∗ )] −

Bα2
∥ωk ∥2
2

(b)

= α[JL (θ∗ , λk ) − JL (θk , λk )]


πθ

+ αE(s,a)∼ν π∗ ∇θ log πθk (a|s) · ωk∗ − AL,λkk (s, a)
+ αE(s,a)∼ν π∗ [∇θ log πθk (a|s) · (ωk − ωk∗ )] −



Bα2
∥ωk ∥2
2

(c)

≥ α[JL (θ∗ , λk ) − JL (θk , λk )]
v
u
u
− αtE


(s,a)∼ν π∗

2 

πθ
∇θ log πθk (a|s) · ωk∗ − AL,λk k (s, a)

− αE(s,a)∼ν π∗ ∥∇θ log πθk (a|s)∥2 ∥(ωk − ωk∗ )∥ −

Bα2
∥ωk ∥2
2

(d)

≥ α[JL (θ∗ , λk ) − JL (θk , λk )]
√
Bα2
− α ϵbias − αG∥(ωk − ωk∗ )∥ −
∥ωk ∥2
2

(3.49)
where (a) and (b) follows from Assumption 3.3 and Lemma 3.4 respectively. Step (c) uses the convexity of the function f (x) = x2 . Finally,

3.3. GLOBAL CONVERGENCE ANALYSIS

63

step (d) comes from Assumption 3.4. Rearranging items, we have
√
Bα
JL (θ∗ , λk ) − JL (θk , λk ) ≤ ϵbias + G∥(ωk − ωk∗ )∥ +
∥ωk ∥2
2
(3.50)
1
∗
∗
∗
+ Es∼dπρ [KL(π (·|s)∥πθk (·|s)) − KL(π (·|s)∥πθk+1 (·|s))]
α
Summing from k = 1 to K, using the non-negativity of KL divergence
and dividing the resulting expression by K, we get the desired result.
Lemma 3.5 proves that the optimality error of the Lagrange sequence
can be bounded by the average first-order and second-order norms of
the intermediate gradients. Note the presence of the ϵbias term in the
result. If the policy class is severely restricted, the optimality bound
loses its importance. Consider the expectation of the second term in
(3.48). Note the following set of inequalities.


K
K
2
1 X
1 X
E∥ωk − ωk∗ ∥ ≤
E ∥ωk − ωk∗ ∥2
K k=1
K k=1





K
1 X
=
E ∥ωk − Fρ (θk )† ∇θ JL (θk , λk )∥2
K k=1



K
2 X
E ∥ωk − ∇θ JL (θk , λk )∥2
≤
K k=1

( 









†

2

)

(3.51)

+ E ∥∇θ JL (θk , λk ) − Fρ (θk ) ∇θ JL (θk , λk )∥
(a)

≤

K
2 X
E ∥ωk − ∇θ JL (θk , λk )∥2
K k=1



K
1
2 X
1+ 2
+
K k=1
µF

!





E ∥∇θ JL (θk , λk )∥

2



where (a) follows from Assumption 3.5. The expectation of the third
term in (3.48) can be bounded as follows.
K
K
h
i
1 X
1 X
E ∥ωk ∥2 ≤
E ∥∇θ JL (θk , λk )∥2
K k=1
K k=1





K
1 X
+
E ∥ωk − ∇θ JL (θk , λk )∥2
K k=1



(3.52)


64

Parameterized Model-Free RL

In both (3.51), (3.52), E ∥ωk − ∇θ JL (θk , λk )∥2 is bounded by Lemma
3.3. To bound E ∥∇θ JL (θk , λk )∥2 , the following lemma is applied.
1
Lemma 3.6. Let Jg (·) be L-smooth, ∀g ∈ {r, c} and α = 4L(1+
2 . Then
)
δ

the following inequality holds.

K
K
288L
3 X
1 X
2
∥∇θ JL (θk , λk )∥ ≤ 2 +
∥∇θ JL (θk , λk ) − ωk ∥2 + β
K k=1
δ K
K k=1

(3.53)
Proof. By the L-smooth property of the objective function and constraint function, we know JL (·, λ) is a L(1 + λ)-smooth function. Thus,
JL (θk+1 , λk ) − JL (θk , λk )
L(1 + λk )
∥θk+1 − θk ∥2
2
L(1 + λk )α2
(a)
= α∇JL (θk , λk )T ωk −
∥ωk ∥2
2
= α ∥∇JL (θk , λk )∥2 − α⟨∇JL (θk , λk ) − ωk , ∇J(θk )⟩
≥ ⟨∇JL (θk , λk ), θk+1 − θk ⟩ −

L(1 + λk )α2
∥∇JL (θk , λk ) − ωk − ∇JL (θk , λk )∥2
2
(3.54)
(b)
α
2
2
≥ α ∥∇JL (θk , λk )∥ − ∥∇JL (θk , λk ) − ωk ∥
2
α
− ∥∇JL (θk , λk )∥2 − L(1 + λk )α2 ∥∇JL (θk , λk ) − ωk ∥2
2
− L(1 + λk )α2 ∥∇JL (θk , λk )∥2


α
2
=
− L(1 + λk )α ∥∇JL (θk , λk )∥2
2


α
2
−
+ L(1 + λk )α ∥∇JL (θk , λk ) − ωk ∥2
2
−

where step (a) holds from the fact that θt+1 = θk + αωk and step (b)
holds due to the Cauchy-Schwarz inequality. Note that,
(a)

(b)

JL (θk+1 , λk+1 ) − JL (θk+1 , λk ) = (λk − λk+1 )Jc (θk+1 ) ≥ −β

(3.55)

where (a) holds by the definition of JL (θ, λ) and step (b) is true because
|Jc (θ)| ≤ 1, ∀θ and |λk+1 − λk | ≤ β|Jˆc (θk )| ≤ β where the last inequality

3.3. GLOBAL CONVERGENCE ANALYSIS

65

uses the fact that |Jˆc (θk )| ≤ 1. Adding (3.54) and (3.55), we get,


JL (θk+1 , λk+1 ) − JL (θk , λk ) ≥


−

α
− L(1 + λk )α2 ∥∇JL (θk , λk )∥2
2


α
+ L(1 + λk )α2 ∥∇JL (θk , λk ) − ωk ∥2 − β
2


(3.56)
Summing over k ∈ {1, · · · , K}, we have,
JL (θK+1 , λK+1 ) − JL (θ1 , λ1 )
≥ −βK +

K 
X
α
k=1

−

K 
X
k=1

2



− L(1 + λk )α2 ∥∇JL (θk , λk )∥2

(3.57)

α
+ L(1 + λk )α2 ∥∇JL (θk , λk ) − ωk ∥2
2


Rearranging the terms and using λk ≤ 2δ yields,
K
X

1
JL (θK+1 , λK+1 ) − JL (θ1 , λ1 )
2
2
2 − L(1 + δ )α

∥∇JL (θk , λk )∥2 ≤ α

k=1



+βK +




 X

K
2
α
+L 1+
α2
∥∇JL (θk , λk ) − ωk ∥2
2
δ
k=1

(3.58)

1
Choosing α = 4L(1+
and dividing both sides by K, we conclude,
2
)
δ

K
16L(1 + 2δ )
1 X
[JL (θK+1 , λK+1 ) − JL (θ1 , λ1 )]
∥∇JL (θk , λk )∥2 ≤
K k=1
K

+

K
3 X
∥∇JL (θk , λk ) − ωk ∥2 + β
K k=1

(3.59)

Recall that |JL (θ, λ)| ≤ 1 + λ ≤ 1 + 2δ ≤ 3δ , ∀θ, λ, thus
K
K
1 X
288L
3 X
∥∇JL (θk , λk )∥2 ≤ 2 +
∥∇JL (θk , λk ) − ωk ∥2 + β
K k=1
δ K
K k=1

(3.60)
This concludes the proof of Lemma 3.6.

66

Parameterized Model-Free RL

Note the presence of β in (3.53). Evidently, to ensure convergence,
β must be a function of T . Invoking Lemma 3.3, we get the following
relation under the same set of assumptions and the choice of parameters
as in Lemma 3.6.
K
1 X
E ∥∇θ JL (θk , λk )∥2 ≤ Õ
K k=1

!

AG2 t2mix
δ2T ξ



+ Õ

Ltmix thit
δ 2 T 1−ξ



+β
(3.61)

Applying Lemma 3.3 and (3.61) in (3.52), we arrive at,
K
1 X
E ∥ωk ∥2 ≤ Õ
K k=1





AG2 t2mix Ltmix thit
+ 2 1−ξ
δ2T ξ
δ T

!

+β

(3.62)

K
1 X
1 p
E∥ωk − ωk∗ ∥ ≤ 1 +
β
K k=1
µF
!
√
√


AGtmix
Ltmix thit
1
Õ
+
+ 1+
µF
δT ξ/2
δT (1−ξ)/2

(3.63)

Similarly, using (3.51), we deduce the following.




Inequalities (3.62) and (3.63) lead to the following global convergence
of the Lagrange function.
Lemma 3.7. Let the parameters {θk }K
k=1 be as stated in Lemma 3.5.
If assumptions 3.1−3.5 are true, {Jg (·)}g∈{r,c} are L-smooth functions,
1
T
ξ
2
α = 4L(1+
2 , K = H , and H = 16tmix thit T (log2 T ) , then the following
)
δ

inequality holds for sufficiently large T .
K
√
1 X
E JL (π ∗ , λk ) − JL (θk , λk ) ≤ ϵbias
K k=1
!
√
√


p
AGtmix
Ltmix thit
1
+
+G 1+
Õ
β+
µF
δT ξ/2
δT (1−ξ)/2



B
+ Õ
L



AG2 t2mix Ltmix thit
+ 2 1−ξ + β
δ2T ξ
δ T

!

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
+ Õ
T 1−ξ δ




3.3. GLOBAL CONVERGENCE ANALYSIS

67

Lemma 3.7 establishes that the average difference between JL (π ∗ , λk )
√
and JL (θk , λk ) is Õ( β+T −ξ/2 +T −(1−ξ)/2 ). Expanding the function, JL ,
and utilizing the update rule of the Lagrange multiplier, we achieve the
global convergence for the objective and the constraint in Theorem 3.8
(stated below). In its proof, Lemma 3.20 (stated in the appendix) serves
as an important tool in disentangling the regret and constraint violation
rates. Interestingly, it is built upon the strong duality property of the
unparameterized optimization and has no apparent direct connection
with the parameterized setup.
Theorem 3.8. Consider the same setup and parameters as in Lemma
3.7 and set β = T −2/5 and ξ = 2/5. We have,
√



 
K

√
1 X
AG2 tmix
1
π∗
E Jr − Jr (θk ) ≤ ϵbias +
1+
Õ T −1/5
K k=1
δ
µF
K
√
√
1
1 X
E Jc (θk ) ≤ δ ϵbias + AG2 tmix 1 +
K k=1
µF











Õ T −1/5



tmix thit
+ Õ
δT 1/5
where π ∗ is a solution to the unparameterized optimization. In the
above bounds, we write only the dominating terms of T .




Theorem 3.8 establishes Õ(T −1/5 ) convergence rates for both the
objective and the constraint violation.
Proof of Theorem 3.8.
Analysis of Objective: Recall the definition that JL (θ, λ) = Jr (θ) −
λJc (θ). The following holds due to Lemma 3.7.
K
K
√
1 X
1 X
∗
∗
E Jrπ − Jr (θk ) ≤ ϵbias +
E λk Jcπ − Jc (θk )
K k=1
K k=1
√
√

 

p
1
AGtmix
Ltmix thit
+G 1 +
Õ
β+
+
µF
δT ξ/2
δT (1−ξ)/2


∗
Ltmix thit Es∼dπ∗ [KL(π (·|s)∥πθ1 (·|s))]
+Õ
T 1−ξ δ



B
+ Õ
L



AG2 t2mix Ltmix thit
+ 2 1−ξ + β
δ2T ξ
δ T







!

(3.64)

68

Parameterized Model-Free RL

We need to bound terms related to Jc . Note the following inequalities.
2 (a)

0 ≤ (λK+1 ) =

K 
X

2

2



(λk+1 ) − (λk )

k=1

=

K 
X

2
P[0, 2 ] λk + β Jˆc (θk ) − (λk )2



k=1
K 
(b) X

≤





δ

2

λk + β Jˆc (θk )



− (λk )2



k=1
(c)

≤ −2β

K
X

∗

λk (Jcπ − Jˆc (θk )) + β 2

K
X

Jˆc2 (θk )

k=1
K
X

k=1
K
X
∗
Jˆc2 (θk )
≤ −2β
λk (Jcπ − Jˆc (θk )) + 2β 2
k=1
k=1
K n
K
o
X
X
π∗
2
ˆ
Jˆc2 (θk )
= −2β
λk (Jc − Jc (θk )) + λk (Jc (θk ) − Jc (θk )) + 2β
k=1
k=1

(3.65)
Step (a) uses λ1 = 0, (b) is due to the non-expansiveness of the projection
operator, and (c) holds since πθ∗ is feasible for the conservative problem.
Rearranging items and taking the expectation, we have
K
1 X
∗
E λk (Jcπ − Jc (θk ))
K k=1





K
K
β X
1 X
E λk (Jc (θk ) − Jˆc (θk )) +
E[Jˆc2 (θk )]
≤−
K k=1
K k=1



(a)

≤−



K
h 
i
1 X
E λk Jc (θk ) − Jˆc (θk ) + β
K k=1

(3.66)

K

h
i
1 X
=−
E λk Jc (θk ) − E Jˆc (θk ) θk
+β
K k=1



(b)

K
h
i
1 X
E λk Jc (θk ) − E Jˆc (θk ) θk
K k=1



≤





(c)

+β ≤

2
+β
δT 2

where (a) results from |Jˆc,ρ (θ)|2 ≤ 1, ∀θ ∈ Θ and (b) uses the fact that
Jˆc,ρ (θk ) and λk are conditionally independent given θk . Finally, (c) is a

3.3. GLOBAL CONVERGENCE ANALYSIS

69

consequence of Lemma 3.15. Combining (3.66) with (3.64), we deduce,
K
√
1
1 X
∗
E Jrπ − Jr (θk ) ≤ ϵbias + O
+β
K k=1
δT 2
!
√
√


p
1
AGtmix
Ltmix thit
Õ
+G 1+
β+
+
µF
δT ξ/2
δT (1−ξ)/2



B
+ Õ
L





AG2 t2mix Ltmix thit
+ 2 1−ξ + β
δ2T ξ
δ T



!

(3.67)

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
+ Õ
T 1−ξ δ
!
√
√


p
√
1
AGtmix
Ltmix thit
≤ ϵbias + G 1 +
Õ
β+
+
µF
δT ξ/2
δT (1−ξ)/2




The last inequality presents only the dominant terms of β and T .
Analysis of Constraint: Since {λk }K
k=1 are derived by applying
the dual update in Algorithm 3, we have,
2 2
2 2 (a)
≤ E λk + β Jˆc (θk ) −
δ
δ



2
h
i
2
2
ˆ
= E λk −
+ 2βE Jc (θk ) λk −
+ β 2 E Jˆc2 (θk )
δ
δ



2
(b)
2
2
+ 2βE Jc (θk ) λk −
≤ E λk −
δ
δ



2
+ 2βE Jˆc (θk ) − Jc (θk ) λk −
+ β2
δ



2 2
2
(c)
= E λk −
+ 2βE Jc (θk ) λk −
δ
δ
 h

i

2
ˆ
+ 2βE E Jc (θk ) θk − Jc (θk ) λk −
+ β2
δ



2 2
2
≤ E λk −
+ 2βE Jc (θk ) λk −
δ
δ
 h

i
2
ˆ
+ β2
+ 2βE E Jc (θk ) θk − Jc (θk ) λk −
δ



(d)
2 2
2
4β
≤ E λk −
+ 2βE Jc (θk ) λk −
+
+ β2
δ
δ
δT 2

E λk+1 −

(3.68)

where (a) is due to the non-expansiveness of the projection P[0, 2 ] and
δ

70

Parameterized Model-Free RL

(b) holds because Jˆc (θ) ∈ [0, 1], ∀θ ∈ Θ according to its definition in
Algorithm 3. Finally, (c) is a consequence of the fact that Jˆc (θk ) and λk
are conditionally independent given θk whereas (d) applies |λk − 2δ | ≤ 2δ
and Lemma 3.15. Averaging (3.68) over k ∈ {1, . . . , K}, we get,
K
1 X
2
E Jc (θk ) λk −
K k=1
δ



−

≤



λ1 − 2δ

2



− λK+1 − 2δ

2

2βK
2
2
β
≤ 2
+
+
δ βK
δT 2
2

+

(3.69)

2
β
+
2
δT
2

(a)

∗

where (a) uses the presumption that λ1 = 0. Note that λk Jcπ ≤ 0, ∀k.
Adding the above inequality to (3.64) at both sides, we, therefore, have,


∗

E Jrπ −
√

K
K
1 X
2
1 X
Jr (θk ) + E
Jc (θk )
K k=1
δ
K k=1







2
β
+
2
T δ
2
!
√
√


p
1
AGtmix
Ltmix thit
Õ
+
+G 1+
β+
µF
δT ξ/2
δT (1−ξ)/2
≤

ϵbias +

B
+ Õ
L


+ Õ

2

δ 2 βK

+

AG2 t2mix Ltmix thit
+ 2 1−ξ + β
δ2T ξ
δ T

(3.70)

!

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
T 1−ξ δ



Since the functions {Jg (θk )}, k ∈ {0, · · · , K − 1}, g ∈ {r, c} are linear
in occupancy measure, there exists a policy π̄ such that the following
equation hols ∀g ∈ {r, c}.
K
1 X
Jg (θk ) = Jgπ̄
K k=1

3.3. GLOBAL CONVERGENCE ANALYSIS

71

Injecting the above relation to (3.70), we have
√
2
2
β
2
+ 2 +
+ E Jcπ̄ ≤ ϵbias + 2
δ
δ βK
T δ
2
!
√
√


p
1
AGtmix
Ltmix thit
+G 1+
Õ
β+
+
µF
δT ξ/2
δT (1−ξ)/2


∗
E Jrπ − Jrπ̄







Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
+ Õ
T 1−ξ δ


B
+ Õ
L

AG2 t2mix Ltmix thit
+ 2 1−ξ + β
δ2T ξ
δ T



(3.71)

!

By Lemma 3.20, we arrive at,


E Jcπ̄



√
≤ δ ϵbias +

2
2
δβ
+ 2+
δβK
T
2
!
√
√


p
1
AGtmix
Ltmix thit
+G 1+
Õ δ β +
+
µF
T ξ/2
T (1−ξ)/2
Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
+ Õ
T 1−ξ




!

B
AG2 t2mix Ltmix thit
+ Õ
+
+ δβ
L
δT ξ
δT 1−ξ


√
2tmix thit
≤ δ ϵbias + Õ
δβT 1−ξ
!
√
√


p
1
AGtmix
Ltmix thit
Õ δ β +
+
+G 1+
µF
T ξ/2
T (1−ξ)/2

(3.72)

The last inequality presents only the dominant terms of β and T . If
we choose β = T −η for some η ∈ (0, 1), then following (3.67) and (3.72),
we can compactly write the following inequalities.
K


√
1 X
∗
E Jrπ − Jr (θk ) ≤ ϵbias + Õ T −η/2 + T −ξ/2 + T −(1−ξ)/2 ,
K k=1





K
√
1 X
E
Jc (θk ) ≤ δ ϵbias
K k=1

"

#



+ Õ T −(1−ξ−η) + T −η/2 + T −ξ/2 + T −(1−ξ)/2



72

Parameterized Model-Free RL

The optimal values of η and ξ can be obtained by solving the
following optimization.
η ξ 1−ξ
max(η,ξ)∈(0,1)2 min 1 − ξ − η, , ,
2 2
2




(3.73)

One can easily verify that (ξ, η) = (2/5, 2/5) is the solution of the
above optimization. Therefore, the convergence rate of the objective
function can be written as follows.
K
√
1 X
1
1
∗
E Jrπ − Jr (θk ) ≤ ϵbias + O
+ 2/5
2
K k=1
δT
T
!
√
√


1
AGtmix
Ltmix thit
1
Õ
+G 1+
+
+
µF
T 1/5
δT 1/5
δT 3/10



B
+ Õ
L





δ 2 + AG2 t2mix Ltmix thit
+ 2 3/5
δ 2 T 2/5
δ T



!

(3.74)

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
T 3/5 δ
√

 

√
1
AG2 tmix
≤ ϵbias +
1+
Õ T −1/5
δ
µF




+ Õ

The last expression only considers the dominant terms of T . Similarly,
the constraint violation rate can be computed as,
K
√
tmix thit
1
1 X
δ
+ 2 + 2/5
Jc (θk ) ≤ δ ϵbias + Õ
1/5
K k=1
T
δT
T
!
√
√


1
δ + AGtmix
Ltmix thit
+G 1+
+
Õ
1/5
µF
T
T 3/10









E

B
+ Õ
L

δ 2 + AG2 t2mix Ltmix thit
+
δT 2/5
δT 3/5

!

(3.75)

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))]
T 3/5

 √

 

√
tmix thit
1
2
−1/5
≤ δ ϵbias + Õ
+
AG
t
1
+
Õ
T
mix
µF
δT 1/5




+ Õ

where the last expression contains only the dominant terms of T . This
concludes the theorem.

3.4. REGRET ANALYSIS AND CONSTRAINT VIOLATION ANALYSIS
73
3.4

Regret Analysis and Constraint Violation Analysis

In this section, we use the convergence analysis in the previous section
to bound the expected regret and constraint violation of Algorithm 3.
Note that the regret and constraint violation can be written as follows.
R(T ) = H

K
X

(Jr∗ − Jr (θk )) +

C(T ) = H

k=1

(Jr (θk ) − r(st , at ))

k=1 t∈Ik

k=1
K
X

K X
X

(Jc (θk )) −

K X
X

(3.76)

(Jc (θk ) − c(st , at ))

k=1 t∈Ik

where Ik ≜ {(k − 1)H, · · · , kH − 1}. Note that the expectation of the
first terms in regret and violation can be bounded by Theorem 3.8. The
expectation of the second term can be expanded as follows,


E



K X
X

(Jg (θk ) − g(st , at ))

k=1 t∈Ik


(a)

= E

K X
X


πθ
πθ
Es′ ∼P (·|st ,at ) [Vg k (s′ )] − Qg k (st , at )

k=1 t∈Ik



K X
X
πθ
πθ
(b)
= E
Vg k (st+1 ) − Vg k (st )
k= t∈Ik

=E

"K
X

πθ
πθ
Vg k (skH ) − Vg k (s(k−1)H )

#

k=1

=E

"K−1
X

πθ
πθ
Vg k+1 (skH ) − Vg k (skH )

#

h π
θ

πθ

+ E Vg K (sT ) − Vg 0 (s0 )

i

k=1

(3.77)
where g ∈ {r, c}. Equality (a) uses the Bellman equation and (b) follows from the definition of Qg . The first and second terms in the last
line of (3.77) can be bounded by Lemma 3.9 and 3.11 (forthcoming)
respectively.
T
Lemma 3.9. If assumptions 3.1 and 3.3 hold, then for K = H
where
2
2
H = 16tmix thit T 5 (log2 T ) , the following hold ∀k, ∀(s, a), ∀g ∈ {r, c},

74

Parameterized Model-Free RL

sufficiently large T , and an arbitrary sequence of states {sk }K
k=1 .
(a) |πθk+1 (a|s) − πθk (a|s)| ≤ G ∥θk+1 − θk ∥
K
X

(b)

k=1
K
X

(c)

E|Jg (θk+1 ) − Jg (θk )| ≤
πθ

αAG
f (T )
δthit

πθ

E|Vg k+1 (sk ) − Vg k (sk )| ≤ tmix

k=1

h√

where f (T ) ≜ Õ

i

2

AGtmix + δ T 5 +

√

αAG
f (T )
δthit


3

Ltmix thit T 10 .

Proof. Using Taylor’s expansion, we write the following ∀(s, a), ∀k.
|πθk+1 (a|s) − πθk (a|s)| = (θk+1 − θk )T ∇θ πθ̄ (a|s)
(3.78)

(a)

= πθ̄k (a|s) (θk+1 θk )T ∇θ log πθ̄k (a|s) ≤ G ∥θk+1 − θk ∥
where θ̄k is some convex combination of θk and θk+1 and (a) follows
from Assumption 3.3. Applying (3.78) and Lemma 3.14, we obtain,
K
X

E |Jg (θk+1 ) − Jg (θk )|

k=1

=
≤

K
X
k=1
K
X

E

d

πθ

(s)(πθk+1 (a|s) − πθk (a|s))Qg k (s, a)

s,a

E

k=1
K
X

≤G
(a)

X π
θk+1
X



πθ

π

d θk+1 (s) πθk+1 (a|s) − πθk (a|s) Qg k (s, a)

s,a

"
X π
θk+1

E

k=1
K
X

≤ Gα

d

πθ
(s)∥θk+1 − θk ∥|Qg k−1 (s, a)|

#

(3.79)

s,a

E

k=1

XX
a

√

(b)

πθk+1

d



(s)∥ωk ∥ · 6tmix

s

≤ 6AGαtmix K

K
X

! 12

E ∥ωk ∥2

k=1
(c)

≤ Õ



αAG h√
δthit



2
5

AGtmix + δ T +

p

Ltmix thit T

3
10

i

3.4. REGRET ANALYSIS AND CONSTRAINT VIOLATION ANALYSIS
75
Inequality (a) uses Lemma 3.11 and the rule θk+1 = θk +αωk . Step (b)
holds by the Cauchy inequality and Jensen inequality whereas (c) can be
derived using (3.62) and substituting K = T /H. This establishes the secP
ond statement. Next, recall from (3.4) that g πθ (s) ≜ a πθ (a|s)g(s, a).
Note that, for any θ, and s ∈ S, the following holds.
Vgπθ (s) =

∞ D
X

(P πθ )t (s, ·) − dπθ , g πθ

E

t=0

=

N
−1 D
X

∞ D
X

E

(P πθ )t (s, ·), g πθ − N J(θ) +

t=0

(P πθ )t (s, ·) − dπθ , g πθ

E

t=N

(3.80)
where ⟨·, ·⟩ denotes the dot product. Define the following quantity.
δ πθ (s, T ) ≜

∞
X

(P πθ )t (s, ·) − dπθ

t=N

1

where N = 4tmix (log2 T ) (3.81)

Lemma 3.12 states that for sufficiently large T , δ πθ (s, T ) ≤ T13 for
any θ and s. Combining this result with the fact that the reward function
is bounded in [0, 1], we obtain,
K
X

πθ

πθ

E|Vg k+1 (sk ) − Vg k (sk )|

k=1

≤
+

K
X
k=1
K
X

E
E

k=1

+N

K
X

N
−1 D
X
t=0
N
−1 D
X

π

π

(P πθk )t (sk , ·), g θk+1 − g πθk

E

E

t=0

E|Jg (θk+1 ) − Jg (θk )| +

k=1
K
−1
(a) X N
X

≤

+

π

(P θk+1 )t (sk , ·) − (P πθk )t (sk , ·), g θk+1

k=1 t=0
K N
−1
X
X

π

2K
T3

(3.82)

π

E (P θk+1 )t − (P πθk )t )g θk+1

∞

π

E g θk+1 − g πθk ∞

k=1 t=0

 2
i
p
3
αAGtmix h√
AGtmix + δ T 5 + Ltmix thit T 10
+ Õ
δthit




76

Parameterized Model-Free RL

where (a) follows from (3.79) and using N = 4tmix (log2 T ). Note that,
π

π

((P θk+1 )t − (P πθk )t )g θk+1
≤ P

πθk+1

((P

+ (P
(a)

πθk+1 t−1

)

πθk+1

−P

− (P

πθk

)(P

∞
πθk t−1

)

π

)g θk+1

∞

πθk t−1 πθk+1

g

)

π

(3.83)

∞

π

≤ ((P θk+1 )t−1 − (P πθk )t−1 )g θk+1
+ max P

πθk+1

s

(s, ·) − P

πθk

∞

(s, ·) 1

π

where (a) follows from (P πθk )t−1 g θk+1 ∞ ≤ 1 and the fact that each
row of P πθk sums to 1. Invoking (3.78), and θk+1 = θk + αωk , we get,
π

max ∥P θk+1 (s, ·) − P πθk (s, ·)∥1
s

= max
s

(πθk+1 (a|s) − πθk (a|s))P (s′ |s, a)

XX
s′

a

≤ G ∥θk+1 − θk ∥ max

XX

s

s′

P (s′ |s, a) ≤ αAG ∥ωk ∥

a

Plugging the above result into (3.83) and using a recursion, we get,
π

π

((P θk+1 )t − (P πθk )t )g θk+1

∞

≤

t
X

π

max P θk+1 (s, ·) − P πθk (s, ·) 1
s

t′ =1

≤

t
X

αAG ∥ωk ∥ ≤ αAtG ∥ωk ∥

t′ =1

Finally, we arrive at the following.
K N
−1
X
X

π

π

E ((P θk+1 )t − (P πθk )t )g θk+1

k=1 t=0
K N
−1
X
X

≤

∞

αtAGE ∥ωk ∥

k=1 t=0

√
≤ O(αAGN 2 K)

(a)

K
X

(3.84)

! 21

E ∥ωk ∥2

k=1
(b)

αAGtmix
≤ Õ
δthit


h√



2
5

AGtmix + δ T +

p

Ltmix thit T

3
10

i

3.4. REGRET ANALYSIS AND CONSTRAINT VIOLATION ANALYSIS
77
where (a) is a result of the Cauchy-Schwarz inequality and (b) follows
from (3.62). Moreover, notice that,
K N
−1
X
X

π

E g θk+1 − g πθk ∞

k=1 t=0
K N
−1
X
X

≤

"

≤ αAGN

(πθk+1 (a|s) − πθk (a|s))g(s, a)

E max
s

k=1 t=0
(a)

#
X

K
X

a
K
X

√

E ∥ωk ∥ ≤ αAGN K

≤ Õ



E ∥ωk ∥2

k=1

k=1
(b)

(3.85)

! 21

αAG h√
δthit



2
5

AGtmix + δ T +

p

3

Ltmix thit T 10

i

where (a) follows from (3.78) and the parameter update rule θk+1 =
θk + αωk while (b) is a consequence of (3.62). Combining (3.82), (3.84),
and (3.85), we establish the third statement.
Lemma 3.9 states that the obtained parameters are such that the
average consecutive difference in the sequence {Jg (θk )}K
k=1 , g ∈ {r, c}
decreases with time horizon, T . We would like to emphasize that Lemma
3.9 works for both reward and constraint functions. Thus, we can prove
our regret guarantee and constraint violation as follows.
Theorem 3.10. If assumptions 3.1−3.5 hold, Jg (·)’s are L-smooth,
∀g ∈ {r, c} and T are sufficiently large, then our proposed Algorithm 3
achieves the following expected regret and constraint violation bounds
1
with learning rates α = 4L(1+
and β = T −2/5 .
2
)
δ

√

E [R(T )] ≤ T ϵbias + Õ(T 4/5 ) + O(tmix )
√
E [C(T )] ≤ T δ ϵbias + Õ(T 4/5 ) + O(tmix )

(3.86)

Proof. Combining (3.76) and (3.77), we get
E[R(T )] = H

K 
X

∗



h π
θ

πθ

k=1

+E

i

Jrπ − E[Jr (θk )] + E Vr K (sT ) − Vr 0 (s0 )

"K−1
X
k=1

πθ
πθ
Vr k+1 (skH ) − Vr k (skH )

#

78

Parameterized Model-Free RL

Using the result in Theorem 3.8, Lemma 3.9 and Lemma 3.11, we get,
√

3
1
E[RegT ] ≤ T ϵbias + O
+ T 5 + O(tmix )
T
!
√
√


4
1
AGtmix 4
Ltmix thit 7
+G 1+
Õ T 5 +
T5 +
T 10
µF
δ
δ

B
+ Õ
L





δ 2 + AG2 t2mix 3 Ltmix thit 2
T5 +
T5
δ2
δ2

!

(3.87)

Ltmix thit Es∼dπ∗ [KL(π ∗ (·|s)∥πθ1 (·|s))] 2
+ Õ
T5
δ

 2
i
p
3
αAGtmix h√
+ Õ
AGtmix + δ T 5 + Ltmix thit T 10
δthit




Similarly, for the constraint violation, we have
K
X

"K−1
X

E[Jc (θk )] − E
k=1
k=1
h π
i
πθ0
θK
− E Vc (sT ) − Vc (s0 )

E[C(T )] = H

πθ
πθ
Vc k+1 (skH ) − Vc k (skH )

#

(3.88)
Using the result in Theorem 3.8, Lemma 3.9 and Lemma 3.11, we get,
√
E[C(T )] ≤ T δ ϵbias + +O(tmix )

 h
i 4

√
p
7
1
+G 1+
Õ δ + AGtmix T 5 + Ltmix thit T 10
µF


3
1
tmix thit 4
T5 +
+ δT 5
+O
δ
δT
!
2
2
2
(3.89)
B
δ + AG tmix 3 Ltmix thit 2
5
5
+ Õ
T +
T
L
δ
δ


∗

+ Õ Ltmix thit Es∼dπ∗ [KL(π (·|s)∥πθ1 (·|s))]T


+ Õ

2
5



 2
i
p
3
αAGtmix h√
AGtmix + δ T 5 + Ltmix thit T 10
δthit



This completes the proof of Theorem 3.10.
We note that this result significantly improves the best-known regret
results for model-free average reward setup with constraints even in

3.5. NOTES AND OPEN PROBLEMS
Algorithm

Regret

√ 
T

Algorithm 1 in (Chen et al., 2022b)

Õ

Algorithm 2 in (Chen et al., 2022b)

Õ T 2/3





Õ

UC-CURL and PS-CURL (Agarwal et al., 2022b)



Algorithm 2 in (Ghosh et al., 2023)

Õ

√

Violation

√ 
Õ
T



T

Algorithm 3 in (Ghosh et al., 2023)

√ 
Õ
T

Triple-QA (Wei et al., 2022a)

Õ T 5/6



Õ T

Setting

No

Tabular

No

Tabular

No

Tabular



0




(Bai et al., 2024a)



Õ T 2/3

Model-free



(dT )3/4



79

4
5



Õ

(dT )3/4



No

Linear MDP

√ 
Õ
T

No

Linear MDP

0

Yes

Tabular

Yes

General Parameterization









Õ T

4
5



Table 3.1: This table summarizes the different model-based and mode-free state-ofthe-art algorithms available in the literature for average reward CMDPs. We note
that the presented algorithm in (Bai et al., 2024a) is the first to analyze the regret
and constraint violation for average reward CMDP with general parametrization.
Here the parameter d refers to the dimension of the feature map for linear MDPs.

the tabular setup (Wei et al., 2022a), where the best-known regret was
Õ(T 5/6 ) (see Table 3.1).
3.5

Notes and Open Problems

The results in this chapter are taken from (Bai et al., 2024a) which
presents the state-of-the-art results for average reward √
CMDPs with general parameterization. However, this is far from the Ω( T ) lower bound.
For unconstrained average reward MDPs, the general parameterization
was first tackled in (Bai et al., 2024b) where a regret √
of Õ(T 3/4 ) was
achieved. Later (Ganesh et al., 2024) improved it to Õ( T ). The stateof-the-art results for discounted reward unconstrained and constrained
MDPs are achieved by (Mondal and Aggarwal, 2024a) and (Mondal and
Aggarwal, 2024b) respectively. While the unconstrained result achieves
the theoretical lower bound, the constrained one does not. In summary,
there are ample opportunities to improve the state-of-the-art results of
both discounted and average-reward CMDPs.
We note that the proposed algorithm in this chapter uses the knowledge of mixing time. Recently, a study for removing such dependence has
been conducted in the absence of constraints. In order to do that, Multilevel Monte-Carlo (MLMC) gradient estimator is incorporated in (Patel
et al., 2024), where the global convergence rate of Õ(T −1/4 ) is derived.
The result has been further extended to the optimal global convergence

80

Parameterized Model-Free RL

rate of Õ(T −1/2 ) in (Ganesh and Aggarwal, 2024), where momentumbased acceleration and Natural Actor Critic based approaches are used.
However, we note that these approaches are for unconstrained setup,
while extension to constrained setup is open.
We further note that the presentation in this chapter considers linear
utility function with linear constraints. For the discounted model-free
setup, these assumptions have been relaxed. The impact of concave
utility in the parametrized setup was studied in (Bai et al., 2022a). The
problem with concave objectives and convex constraints was studied for
tabular setup in (Bai et al., 2023b). Other related works include (Bai et
al., 2022b; Bai et al., 2023c), which studied constrained MDP for tabular
and parametrized setup, respectively. Extension of the framework in this
chapter to concave utility and convex constraints is an open direction
with average rewards.
Finally, we note that the algorithm developed in this chapter uses
several parameters that might not be easily estimated (e.g., the Lipschitz
constant). Designing a parameter-free algorithm for CMDPs is an
important avenue to explore in the future.
3.6

Some Auxiliary Lemmas for the Proofs

Lemma 3.11. (Wei et al., 2020, Lemma 14) For any ergodic MDP with
mixing time tmix , the following holds ∀(s, a), ∀π. ∀g ∈ {r, c}.
(a)|Vgπ (s)| ≤ 5tmix , (b)|Qπg (s, a)| ≤ 6tmix
Lemma 3.12. (Wei et al., 2020, Corollary 13.2) Let δ π (·, T ) be defined
as written below for an arbitrary policy π.
π

δ (s, T ) ≜

∞
X
t=N

(P π )t (s, ·) − dπ

1

, ∀s where N = 4tmix (log2 T ) (3.90)

If tmix < T /4, we have the following inequality ∀s: δ π (s, T ) ≤ T13 .
Lemma 3.13. (Wei et al., 2020, Lemma 16) Let I = {t1 + 1, t1 +
2, · · · , t2 } be a certain period of an epoch k of Algorithm 4 with length
N . Then for any s, the probability that the algorithm never visits s in

3.6. SOME AUXILIARY LEMMAS FOR THE PROOFS

81

I is upper bounded by


3dπθk (s)
1−
4

 
 ⌊I⌋
N

(3.91)

Lemma 3.14. (Wei et al., 2020, Lemma 15) The difference of the values
of the function Jg , g ∈ {r, c} at policies π and π ′ , is
′

Jgπ − Jgπ =

XX
s

′

dπ (s)(π(a|s) − π ′ (a|s))Qπg (s, a)

(3.92)

a

Lemma 3.15. (Wei et al., 2020, Lemma 7) The term Jˆc (θ) for any
θ ∈ Θ is a good estimator of Jc (θ), which means
E[Jˆc (θ)] − Jc (θ) ≤

1
T2

(3.93)

Lemma 3.16. (Dorfman and Levy, 2022, Lemma A.6) Let θ ∈ Θ be a
policy parameter. Fix a trajectory z = {(st , at , rt , st+1 )}t∈N generated by
following the policy πθ starting from some initial state s0 ∼ ρ. Let, ∇L(θ)
be the gradient that we wish to estimate over z, and l(θ, ·) is a function
such that Ez∼dπθ ,πθ l(θ, z) = ∇L(θ). Assume that ∥l(θ, z)∥ , ∥∇L(θ)∥ ≤
P
GL , ∀θ ∈ Θ, ∀z ∈ S × A × R × S. Define lQ = Q1 Q
i=1 l(θ, zi ). If
P = 2tmix log T , then the following holds as long as Q ≤ T ,


E

Q

l − ∇L(θ)

2





≤O

P
G2L log (P Q)

Q



(3.94)

Lemma 3.17 (Strong duality). For convenience, we rewrite the unparameterized problem below.
max Jrπ s.t. Jcπ ≤ 0
π∈Π

(3.95)

Define π ∗ as the optimal solution to the above problem. Define the
associated dual function as
λ
JD
≜ max Jrπ − λJcπ
π∈Π

(3.96)

λ . We have the following strong duality
and denote λ∗ = arg minλ≥0 JD
property for the unparameterized problem.
∗

∗

λ
Jrπ = JD

(3.97)

82

Parameterized Model-Free RL

Although the strong duality holds for the unparameterized problem,
the same is not true for parameterized class {πθ |θ ∈ Θ}. To formalize this
statement, define the dual function associated with the parameterized
problem as follows.
λ
JD,Θ
≜ max Jr (θ) − λJc (θ)

(3.98)

θ∈Θ

λ . The lack of strong duality states that,
and denote λ∗Θ = arg minλ≥0 JD,Θ
λ∗

Θ
in general, JD,Θ
̸= Jr (θ∗ ) where θ∗ is a solution of the parameterized
constrained optimization (3.2). However, λ∗Θ , as we demonstrate below,
must obey some restrictions.

Lemma 3.18 (Bound on λΘ ). Under Assumption 3.2, the optimal dual
variable for the parameterized problem is bounded as
∗

0 ≤ λ∗Θ ≤

Jrπ − Jr (θ̄)
1
≤
δ
δ

(3.99)

Proof. The proof follows the approach in (Ding et al., 2023, Lemma 3),
but is revised to the general parameterization setup. Let Λa ≜ {λ ≥
λ
0 | JD,Θ
≤ a} be a sublevel set of the dual function for a ∈ R. If Λa is
non-empty, then for any λ ∈ Λa ,
λ
a ≥ JD,Θ
≥ Jr (θ̄) − λJc (θ̄) ≥ Jr (θ̄) + λδ

(3.100)

where θ̄ is a Slater point in Assumption 3.2. Thus, λ ≤ (a − Jr (θ̄))/δ.
λ∗Θ
λ∗ ≤ J λ∗ = J π ∗ , then λ∗ ∈ Λ , which proves
If we take a = JD,Θ
≤ JD,Θ
a
r
D
Θ
the Lemma. The last inequality holds since Jrπ ∈ [0, 1] for any π.
Since the above inequality holds for any Θ, we also have, 0 ≤ λ∗ ≤ 1δ .
Let v(τ ) ≜ maxπ∈Π {Jrπ |Jcπ ≤ −τ }. Using the strong duality property of
the unparameterized problem (3.95), we establish the following property
of the function, v(·).
Lemma 3.19. If Assumption 3.2 holds, the following is true ∀τ ∈ R,
v(0) − τ λ∗ ≥ v(τ )

(3.101)
∗

Proof. By the definition of v(τ ), we get v(0) = Jrπ . With a slight abuse
of notation, denote JL (π, λ) = Jrπ − λJcπ . By the strong duality stated

3.6. SOME AUXILIARY LEMMAS FOR THE PROOFS

83

in Lemma 3.17, we have the following for any π ∈ Π.
Def

∗ (3.97)

λ
JL (π, λ∗ ) ≤ max JL (π, λ∗ ) = JD

∗

= Jrπ = v(0)

π∈Π

(3.102)

Thus, for any π ∈ {π ∈ Π | Jcπ ≤ −τ },
v(0) − τ λ∗ ≥ JL (π, λ∗ ) − τ λ∗
= Jrπ − λ∗ (Jcπ + τ ) ≥ Jrπ

(3.103)

Maximizing the R.H.S of this inequality over {π ∈ Π|Jcπ ≤ −τ } yields
v(0) − τ λ∗ ≥ v(τ )

(3.104)

This completes the proof of the lemma.
We note that a similar result was shown in (Bai et al., 2023a, Lemma
15). However, the setup of the stated paper is different from that of
ours. Specifically, (Bai et al., 2023a) considers a tabular setup with
peak constraints. Note that Lemma 3.19 has no direct connection with
the parameterized setup since its proof uses strong duality and the
function, v(·), is defined via a constrained optimization over the entire
policy set, Π, rather than the parameterized policy set. Interestingly,
however, the relationship between v(τ ) and v(0) leads to the lemma
stated below which turns out to be pivotal in establishing regret and
constraint violation bounds in the parameterized setup.
Lemma 3.20. Let Assumption 3.2 hold. For any constant C ≥ 2λ∗ , if
∗
there exists a π ∈ Π and ζ > 0 such that Jrπ − Jrπ + C[Jcπ ] ≤ ζ, then
Jcπ ≤ 2ζ/C

(3.105)

Proof. Let τ = −Jcπ . Using the definition of v(τ ), one can write,
Jrπ ≤ v(τ )

(3.106)

Combining Eq. (3.104) and (3.106), we obtain the following.
∗

Jrπ − Jrπ ≤ v(τ ) − v(0) ≤ −τ λ∗

(3.107)

The condition in the Lemma leads to,
∗

(C − λ∗ )(−τ ) = τ λ∗ + C(−τ ) ≤ Jrπ − Jrπ + C[Jcπ ] ≤ ζ

(3.108)

84

Parameterized Model-Free RL

Finally, we have,
−τ ≤
which completes the proof.

ζ
2ζ
≤
∗
C −λ
C

(3.109)

4
Beyond Ergodic MDPs

The previous chapters considered the case where the MDP was ergodic.
However, the key condition needed for efficient guarantees is that the
underlying MDP is at least weakly communicating (Bartlett and Tewari,
2009; Jaksch et al., 2010). This chapter provides some results for weakly
communicating MDPs. The main results in this chapter are for modelbased reinforcement learning (in Section 4.1), where a regret guarantee
of Õ(T 2/3 ) is derived. These guarantees are far from the theoretical lower
bound, leaving significant room for improvement. Such possibilities are
discussed in Section 4.2.
4.1

Algorithm for Model-Based RL

Similar to the previous two chapters, we consider a constrained Markov
Decision Process (CMDP) characterized as M = (S, A, r, c, P ) where
S, A are the state and action spaces with sizes S and A respectively,
r : S × A → [0, 1] is the reward function, c : S × A → [−1, 1] is the cost
function, and finally, P : S × A → ∆(S) is the transition kernel where
∆(·) is the probability simplex over its argument set. A policy is defined
to be a map of the following form π : S → ∆(A). For a given policy, π,
the long-term reward and cost functions, Jgπ,P , g ∈ {r, c} are defined as
85

86

Beyond Ergodic MDPs

follows.
"

Jgπ,P (s) = lim inf Eπ,P
τ →∞

τ
1X
g(st , at ) s1 = s
τ t=1

#

(4.1)

where the expectation is computed over all trajectories generated by the
policy π and the kernel P from the initial state s. As stated earlier, this
chapter considers the underlying CMDP to be weakly communicating,
which essentially means the state space S can be segregated into two
categories: the states in the first group are transient under all (stationary)
policies while any two states in the second group are communicating
under some policy. Note that the weakly communicating class subsumes
the ergodic class. One problem with this generalization is that the notion
of a stationary distribution may no longer be valid and the long-term
reward and costs are generally dependent on the initial state (unlike
that in the ergodic case). (Puterman, 1994) demonstrated that for any
s ∈ S, there exists π ⋆ that solves the following constrained optimization
max Jrπ,P (s) s. t. Jcπ,P (s) ≤ 0
π

(4.2)

and ensures that the long-term reward and cost associated with π ⋆ are
⋆
independent of s i.e., Jgπ ,P (s) = Jg⋆ , for some constants Jg⋆ , g ∈ {r, c}.
Additionally, (Puterman, 1994, Theorem 8.2.6) also showed that for any
g ∈ RS×A (i.e., g need not be restricted to reward or cost function), there
exists a bias function qgπ,P ∈ RS×A that obeys the Bellman equation
∀(s, a) ∈ S × A,
qgπ,P (s, a) + Jgπ,P (s) = g(s, a) + Es′ ∼P (s,a) [vgπ,P (s′ )],

(4.3)

where vgπ,P (s) = a∈A π(a|s)qgπ,P (s, a), and also Jgπ,P
(s) = 0, g ′ = qgπ,P ,
′
∀s. The functions q and v are analogous to the well-known Q-function
and state value function for the discounted or the finite-horizon setting.
In the rest of this chapter, we utilize the following notations. For any
f ∈ RS , we define its span as sp(f ) = maxs∈S f (s) − mins∈S f (s). When
there is no confusion, we simplify the notations Jgπ,P , qgπ,P , and vgπ,P by
dropping the dependence on P . Given a policy π and a transition kernel
P , we define the π-induced transition kernel P π such that P π (s, s′ ) =
P
′
′
⋆,ϵ is given in the same
a π(a|s)P (s |s, a), ∀(s, s ). For any ϵ ∈ (0, 1), π
P

4.1. ALGORITHM FOR MODEL-BASED RL

87
⋆,ϵ

way as π ⋆ but with the constraint threshold −ϵ. Let Jg⋆,ϵ denote Jgπ
(that is s-independent as mentioned before), ∀g ∈ {r, c}.
Weakly communicating CMDPs imposes challenges in learning as
compared to ergodic CMDPs. Specifically, there is no uniform bound for
sp(vgπ ), g ∈ {r, c} for all π (while in the ergodic case, they are bounded
by Õ(tmix ) where tmix is the mixing time defined in the earlier chapter).
It is also unclear how to obtain an accurate estimate of a policy’s bias
function as in ergodic MDPs, which is an important step for a policy
optimization algorithm. In this section, we discuss the approach given
by (Chen et al., 2022b) to solve the above problem.
The algorithm runs by dividing T into K episodes, each of length H.
Each episode considers the problem of finding an optimal non-stationary
policy for an episodic finite-horizon MDP through the lens of occupancy
measure, in which expected reward and cost are both linear functions and
easy to optimize over. Concretely, consider a fixed starting state s, a nonstationary policy π ∈ (∆A )S×[H] whose behavior can change in different
steps within an episode, and an inhomogeneous state transition kernel
P = {Ph }h∈[H] where Ph ∈ (∆S )S×A specifies the probability kernel at
step h. The corresponding occupancy measure νsπ,P ∈ [0, 1]S×A×[H]×S
is then such that νsπ,P (s′ , a, h, s′′ ) is the probability of visiting state s′
at step h, taking action a, and then transiting to state s′′ , if the learner
starts from state s, executes π for the next H steps, and the transition
kernel is P . Conversely, a function ν ∈ [0, 1]S×A×[H]×S is defined to be
an occupancy measure with respect to a starting state s, some policy
πν , and transition Pν if and only if it satisfies the following conditions.
′
′′
′
s′′ ν(s , a, 1, s ) = I{s = s}.

1. Initial state is s:

P P

2. Total mass is 1:

P P P

a

s′

a

′
′′
s′′ ν(s , a, h, s ) = 1, ∀h.

3. Flow conservation: s′′ a ν(s′′ , a, h, s′ ) =
1, s′′ ) for all s′ ∈ S, h ∈ [H − 1].
P

P

′
s′′ ν(s , a, h +

P P
a

Let the set of all such ν be Vs . For notational convenience, for a ν ∈
P
P
Vs , we define ν(s′ , a, h) = s′′ ν(s′ , a, h, s′′ ), ν(s′ , a) = h ν(s′ , a, h),
P
and ν(s′ , h) = a ν(s′ , a, h). Also, note that the corresponding policy
′ ,a,h)
πν and transition Pν can be extracted using πν (a|s′ , h) = ν(s
ν(s′ ,h) and

88

Beyond Ergodic MDPs

Algorithm 5 Finite Horizon Approximation for CMDP
Define: H = ⌈(T /S 2 A)1/3 ⌉, K = T /H.
for k = 1, . . . , K do
Observe current state sk1 = s(k−1)H+1 .
Compute occupancy measure:
νk =

argmax

⟨ν, r⟩ ,

(4.4)

ν∈Vk,sk :⟨ν,c⟩≤sp⋆c
1

where Vk,s = {ν ∈ Vs : Pν ∈ Pk } (see Eq. (4.5)).
Extract policy πk = πνk from νk .
for h = 1, . . . , H do
Play action akh ∼ πk (·|skh , h) and transit to skh+1 .
′

′′

,a,h,s )
Pν,s′ ,a,h (s′′ ) = ν(s
ν(s′ ,a,h) . Observe that Vs is a convex polytope with
polynomial constraints. If one enforces Pν to be homogeneous across
different steps of an episode, Vs would become non-convex. This forces
us to consider inhomogeneous transitions even though the true transition
is indeed homogeneous.
The entire procedure is described in Algorithm 5. At the beginning
of the kth episode, the learner observes the current state sk1 and finds an
P
occupancy measure νk ∈ Vk,sk that maximizes ⟨ν, r⟩ = s,a ν(s, a)r(s, a)
1

⋆

while satisfying ⟨ν, c⟩ ≤ sp⋆c where sp⋆c = sp(vcπ ), which is assumed to
be known to the learner. Here, Vk,sk ⊂ Vsk is such that Pν , ∀ν ∈ Vk,sk
1
1
1
lies in a standard Bernstein-type confidence set Pk defined as
n

Pk = P ′ = {Ph′ }h∈[H] , Ph′ ∈ (∆S )S×A : Ph′ (s′ |s, a) − P̄k (s′ |s, a)
q

o

≤ 4 P̄k (s′ |s, a)αk (s, a) + 28αk (s, a), ∀(s, a) ,

(4.5)

′

)
where P̄k (s′ |s, a) = NNk+(s,a,s
is the empirical transition, Nk (s, a, s′ ) is the
(s,a)
k

′

number of visits to triplet (s, a, s′ ) before episode k, αk (s, a) = N +ι(s,a) ,
k

Nk+ (s, a) = max{1, Nk (s, a)}, Nk (s, a) is the number of visits to (s, a)
before episode k, and ι′ = ln 2SAT
δ . Pk is constructed in a way such that
it contains the true transition with high probability (Lemma 4.2). With
νk , we follow πk = πνk extracted from νk for the next H steps.

4.1. ALGORITHM FOR MODEL-BASED RL

89

Note that the key optimization problem (4.4) in this algorithm can
be efficiently solved since the objective function is linear and the domain
is a convex polytope with polynomial constraints, thanks to the use of
occupancy measures. We now state the main guarantee of Algorithm 5.
Theorem 4.1. Algorithm 5 ensures the following regret and constraint
violation bounds (defined in a manner similar to that in the previous
chapter) with probability at least 1 − 10δ.








RT = Õ (1 + sp⋆r )(S 2 A)1/3 T 2/3 ,
CT = Õ (1 + sp⋆c )(S 2 A)1/3 T 2/3 .
The rest of this section provides the key steps to the result, while
for the detailed proof, the readers are referred to (Chen et al., 2022b).
For any non-stationary policy π ∈ (∆A )S×[H] , and an inhomogeneous
transition P = {Ph }h∈[H] , define the following ∀h ∈ [H].
π,P
Vg,h
(s) = Eπ,P

" H
X

#

g(sh′ , ah′ ) sh = s , ∀s

h′ =h
" H
X

Qπ,P
g,h (s, a) = Eπ,P

(4.6)
#

g(sh′ , ah′ ) sh = s, ah = a , ∀(s, a)

(4.7)

h′ =h
π,P
e
e
Additionally, Vg,H+1
(s) = Qπ,P
g,H+1 (s, a) = 0, ∀(s, a). Let P = {Ph }h∈[H]
be such that Peh (·|s, a) = P (·|s, a) (the true transition function), ∀h. We
ignore the dependency on Pe for simplicity when there is no confusion e.g.,
π denotes V π,P . For a stationary policy π ∈ (∆ )S , let π
e be the policy
Vg,h
A
g,h
eh (·|s) = π(·|s), ∀h.
that mimics π in the finite-horizon setting, that is, π
We first show that the true transition lies in the transition confidence
sets with high probability and provide some key related lemmas.

e

Lemma 4.2. With probability at least 1 − δ, Pe ∈ Pk ,∀k.
Proof. For any (s, a) ∈ S×A, s′ ∈ S, by Lemma 1.8 and NK+1 (s, a) ≤ T ,
we have with probability at least 1 − S 2δA ,
′

′

q

P (s |s, a) − P̄k (s |s, a) ≤ 4 P̄k (s′ |s, a)αk (s, a) + 28αk (s, a).
Applying a union bound over (s, a) ∈ S ×A, s′ ∈ S and utilizing Peh = P ,
the statement is proved.

90

Beyond Ergodic MDPs

Lemma 4.3. If Pe ∈ Pk , |Ph′ (s′ |s, a) − P (s′ |s, a)| ≤ 8 P (s′ |s, a)αk (s, a)+
136αk (s, a) ≜ ϵ⋆k (s, a, s′ ), ∀P ′ ∈ Pk , ∀h.
p

Proof. Using Pe ∈ Pk , we get the following ∀(s, a, s′ ):
q

P̄k (s′ |s, a) ≤ P (s′ |s, a) + 4 P̄k (s′ |s, a)αk (s, a) + 28αk (s, a).
√
Applying x2 ≤ ax + b =⇒ x ≤ a + b with b = P (s′ |s, a) + 28αk (s, a),
p
and a = 4 αk (s, a), we have
q

q

P̄k (s′ |s, a) ≤ 4 αk (s, a) +
≤

q

P (s′ |s, a) + 28αk (s, a)

q

q

P (s′ |s, a) + 10 αk (s, a)

Substituting this, the right-hand side of Eq. (4.5) can be bounded as,
q

q

4 P̄k (s′ |s, a)αk (s, a) + 28αk (s, a) ≤ 4 P (s′ |s, a)αk (s, a) + 68αk (s, a).
Using Pe , P ′ ∈ Pk , Eq. (4.5), and |Ph′ (s′ |s, a)−P (s′ |s, a)| ≤ |Ph′ (s′ |s, a)−
Pk (s′ |s, a)| + |P̄k (s′ |s, a) − P (s′ |s, a)|, the statement is proved.
Lemma 4.4. For a stationary policy π ∈ (∆A )S , if Jgπ (s) = Jgπ , ∀s ∈ S,
e
π (s) − (H − h + 1)J π | ≤ sp(v π ), ∀s ∈ S and ∀h ∈ [H].
we have |Vg,h
g
g

Proof. For any state s and h ∈ [H], we have:
e
π
Vg,h
(s) − (H − h + 1)Jgπ

= Eeπ,Pe

" H
X

#

(g(sh′ , ah′ ) − Jgπ ) sh = s

h′ =h

" H
X

#

P (s′ |sh′ , ah′ )vgπ (s′ ) sh = s
(4.3)
= Eeπ,Pe
(qgπ (sh′ , ah′ ) −
′
′
h =h
s
" H
#
X
π
π
e and Pe )
= Eeπ,Pe
(vg (sh′ ) − vg (sh′ +1 )) sh = s (definition of π
h′ =h
X

h

i

= vgπ (s) − Eeπ,Pe vgπ (sH+1 ) sh = s .
e
π (s) − (H − h + 1)J π | ≤ sp(v π ) and the proof is complete.
Hence, |Vg,h
g
g
e ⋆ and
Next, we show that the occupancy measure corresponding to π
Pe obeys the constraint of (4.4) with high probability.

4.1. ALGORITHM FOR MODEL-BASED RL

91

⋆

Lemma 4.5. If Pe ∈ Pk , νseπk ,P lies in the constraint set of (4.4).
e

1

Proof. By Lemma 4.4, we have:


e
e
π ⋆ ,P

νsk
1



⋆

⋆

e
π
, c = Vc,1
(sk1 ) ≤ sp⋆c + HJcπ ≤ sp⋆c

(4.8)

Then by Pe ∈ Pk , the statement is proved.
As the last preliminary step, we bound the bias in value function
caused by imperfect transition estimation, that is, the difference between
Pe and Pk = Pνk . The result uses Lemma 4.7- 4.9, which are described
after the main proof outline. In the following, we utilize the notation
P
that for any P ∈ ∆(S) and V ∈ RS , P · V = s∈S P (s)V (s). Moreover,
V(P, V ) denotes the variance of V (·) with respect to P (·), i.e., V(P, V ) =
P ·(V −P ·V )2 where subtraction and square operations are element-wise.
e k
π k ,P
k=1 (Vg,1 (s1 )−

PK

Lemma 4.6. With probability at least 1−4δ, we have |
√
πk ,Pk k
Vg,1
(s1 ))| = Õ( S 2 AH 2 K + H 2 S 2 A), ∀g ∈ {r, c}.

Proof. We condition on the event of Lemma 4.2, which happens with
probability at least 1 − δ. Note that with probability at least 1 − δ:
K
X

πk ,Pk k
πk k
(Vg,1
(s1 ) − Vg,1
(s1 ))

k=1
K
X

" H
X

#

πk ,Pk
=
Eπk ,P
(Pk,h − P )(skh , akh ) · Vg,h+1
k=1
h=1
" H
#
K
X
X
πk ,Pk
k k
≤
Eπk ,P
(Pk,h − P )(sh , ah ) · Vg,h+1
k=1
h=1
K X
H
 
X
πk ,Pk
≤2
(Pk,h − P )(skh , akh ) · Vg,h+1
+ Õ H 2
k=1 h=1

(Lemma 4.9)

(Lemma 1.9)


v
u
K X
H
u
X
πk ,Pk
V(P (skh , akh ), Vg,h+1
) + H 2 S 2 A (Lemma 4.8)
= Õ tS 2 A
k=1 h=1

92

Beyond Ergodic MDPs

Lemma 4.7 establishes that the following happens with probability at
least 1 − 2δ if H = (T /S 2 A)1/3 .
K
X

πk ,Pk k
πk k
Vg,1
(s1 ) − Vg,1
(s1 ) = Õ

√

S 2 AH 2 K + H 2 S 2 A



k=1

This completes the proof.
We are now ready to prove Theorem 4.1.
Proof of Theorem 4.1. We decompose RT into three terms:
RT =

T
X

⋆

J − r(st , at ) =

t=1

=

K 
X



⋆

K
X

HJ −

k=1
K 
X

e
π
HJ ⋆ − Vr,1
(sk1 ) +

k=1

⋆

H
X

!

r(skh , akh )

h=1



⋆

πk k
e
π
Vr,1
(sk1 ) − Vr,1
(s1 )

k=1

+

K
X

πk k
Vr,1
(s1 ) −

k=1

H
X

!

r(skh , akh )

h=1

The first term above is upper bounded by Ksp⋆r by Lemma 4.4. For the
second term, by Lemma 4.5 and Lemma 4.6, we have
K 
X

K 


X
πk ,Pk k
πk k
πk k
e
π⋆ k
Vr,1 (s1 ) − Vr,1 (s1 ) ≤
Vr,1
(s1 ) − Vr,1
(s1 )

k=1

√

= Õ

k=1

S 2 AH 2 K + H 2 S 2 A



.

√
The last term is of order Õ(H K) by Azuma’s inequality (Lemma 1.5).
Using the definition of H and K, we arrive at




RT = Õ (1 + sp⋆r ))(S 2 A)1/3 T 2/3 .
For constraint violations, we decompose CT as:
T
X

c(st , at ) =

t=1

+

K
X

H
X

k=1

h=1

K 
X
πk

!
πk k
c(skh , akh ) − Vc,1
(s1 )



πk ,Pk k
Vc,1 (sk1 ) − Vc,1
(s1 )

k=1

+

K 
X
πk ,Pk

Vc,1

k=1



(sk1 )

4.1. ALGORITHM FOR MODEL-BASED RL

93

√
The
√first term is Õ(H K)
 by Azuma’s inequality. The second term is
2
2
2
2
Õ
S AH K + H S A by Lemma 4.6. The third term is bounded by
⋆
Kspc due to the constraint ⟨ν, c⟩ ≤ sp⋆c in the optimization problem (4.4).
Using the definition of H and K, we get:




CT = Õ (1 + sp⋆c )(S 2 A)1/3 T 2/3 .
This completes the proof.
We now provide the three auxiliary Lemmas that were used earlier.
Lemma 4.7. Under the event of Lemma 4.2, for any utility function g ∈
P
PH
πk ,Pk
k
[−1, 1]S×A , with probability at least 1−2δ, K
k=1
h=1 V(Ph , Vg,h+1 ) =
√
Õ(H 2 (K + T ) + H 3 S 2 A) where Phk ≜ P (skh , akh ).
Proof. We decompose the variance into four terms:
K X
H
X

πk ,Pk
V(Phk , Vg,h+1
)=

k=1 h=1
H 
K X
X

=

+
+
+

K X
H 
X

Phk ·






πk ,Pk 2
πk ,Pk 2
Vg,h+1
− Phk · Vg,h+1



k=1 h=1

Phk ·

k=1 h=1
K X
H 
X
k=1 h=1
K X
H 
X
k=1 h=1
K X
H 
X



πk ,Pk
Vg,h+1

2

−



2
πk ,Pk k
Vg,h+1
(sh+1 )


2 
πk ,Pk k 2
πk ,Pk k
(sh )
Vg,h+1
(sh+1 ) − Vg,h






2

πk ,Pk k 2
k ,Pk
(skh , akh )
Vg,h
(sh ) − Qπg,h
2

k ,Pk
Qπg,h
(skh , akh )



πk ,Pk
− Phk · Vg,h+1



2 

k=1 h=1

For the first term, by Lemma 1.6, with probability at least 1 − δ,
K X
H
X



πk ,Pk
Phk · Vg,h+1

2

2



πk ,Pk k
− Vg,h+1
(sh+1 )

k=1 h=1

v

uK H

uX X 
π
,P
= Õ t
V P k , (V k k )2 + H 2 
h

g,h+1

k=1 h=1


 v
uK H

uX X 
V P k , V πk ,Pk + H 2 
= Õ H t
h

k=1 h=1

g,h+1

(Lemma 1.3)

94

Beyond Ergodic MDPs

πk ,Pk
The second term is upper bounded by 0 since Vg,H+1
(s) = 0 for s ∈ S.
The third term, with probability at least 1 − δ, can be upper bounded
as follows using the Cauchy-Schwarz inequality and Lemma 1.5.
K X
H 
X
πk ,Pk

Vg,h

k=1 h=1
K X
H
X

X

k=1 h=1

a

≤

(skh )

2

2



k ,Pk
− Qπg,h
(skh , akh )

πk (a|skh , h)

2 
2
k ,Pk
k ,Pk
Qπg,h
(skh , a) − Qπg,h
(skh , akh )



!

√ 
= Õ H 2 T


For the fourth term can be upper bounded using a2 − b2 = (a + b)(a − b)
πk ,Pk
k ,Pk
and the facts that Vg,h
, Qπg,h
≤ H:
∞

K X
H 
X
πk ,Pk

Qg,h (skh , akh )

∞

2



πk ,Pk
− Phk · Vg,h+1

2

k=1 h=1

≤ 2H

K X
H
X

πk ,Pk
k ,Pk
Qπg,h
(skh , akh ) − Phk · Vg,h+1

k=1 h=1

≤ 2H 2 K + 2H

K X
H
X

πk ,Pk
(Pk,h (skh , akh ) − Phk ) · Vg,h+1

k=1 h=1

v

u
K X
H
u
X
= Õ H 2 K + H tS 2 A
V(P k , V πk ,Pk ) + H 3 S 2 A


h

g,h+1

k=1 h=1

(Lemma 4.8)
Putting everything together, we have
K X
H
X

 v
uK H
uX X
πk ,Pk
k

V(P k , V πk ,Pk )
V(P , V
) = O Ht
h

h

g,h+1

g,h+1

k=1 h=1

k=1 h=1

+H 2 (K +

√

v

u
K X
H
u
X
T ) + H tS 2 A
V(P k , V πk ,Pk ) + H 3 S 2 A .
h

g,h+1

k=1 h=1
πk ,Pk
H
k
Solving the quadratic inequality, we obtain K
k=1
h=1 V(Ph , Vg,h+1 ) =
√
Õ(H 2 (K + T ) + H 3 S 2 A). This completes the proof.

P

P

4.1. ALGORITHM FOR MODEL-BASED RL

95

Lemma 4.8. If Vh ∈ [−B, B]S , ∀h ∈ [H], and Phk ≜ P (skh , akh ), then
under the event of Lemma 4.2, we have the following.
K X
H
X

(Pk,h (skh , akh ) − Phk ) · Vh+1

k=1 h=1


v
u
K X
H
u
X
V(P k , Vh+1 ) + BHS 2 A
= Õ tS 2 A
h

k=1 h=1

Proof. Let Ik = I{(s, a) ∈ S × A : Nk+1 (s, a) ≤ 2Nk (s, a)} and zhk (s′ ) =
Vh+1 (s′ ) − Phk · Vh+1 . By Lemma 4.3,
K X
H
X

(Pk,h (skh , akh ) − Phk ) · Vh+1 =

k=1 h=1
K X
H
X

≤

K X
H
X

(Pk,h (skh , akh ) − Phk ) · zhk

k=1 h=1

)

(

min B,

k=1 h=1
K X
H
X

≤2

X
s′

(

min B,

)
X

ϵ⋆k+1 (skh , akh , s′ )|zhk (s′ )|

+ BH

s′

k=1 h=1

Observe that

K
X

Ick .

k=1

PK

c
k=1 Ik = Õ(SA) by definition. Moreover,

K X
H X
X
k=1 h=1

ϵ⋆k (skh , akh , s′ )|zhk (s′ )|

ϵ⋆k+1 (skh , akh , s′ )|zhk (s′ )|

s′

v
K X
H
K X
H Xu
X
X
u P k (s′ )z k (s′ )2
h
h
t

+
= Õ


k=1 h=1 s′

+
Nk+1
(skh , akh )



SB

+
k , ak )
N
(s
h
h
k+1
k=1 h=1
(definition of ϵ⋆k )

v


K X
H u
X
u SV(P k , Vh+1 )
2
h
t
= Õ 
+ BS A
k=1 h=1

+
(skh , akh )
Nk+1

v
uK H
uX X

= Õ t

v
u



K X
H
uX
S
t
V(Phk , Vh+1 ) + BS 2 A
+
k , ak )
N
(s
k=1 h=1 k+1 h h
k=1 h=1

(Cauchy-Schwarz inequality)
v

u
K X
H
u
X
= Õ tS 2 A
V(P k , Vh+1 ) + BS 2 A .
h

k=1 h=1

96

Beyond Ergodic MDPs

Plugging these back completes the proof.
Lemma 4.9. (Shani et al., 2020, Lemma 1) For any policy π ∈ (∆A )S×[H] ,
π,P
two transitions P, P ′ ∈ (∆(S))S×A×[H] , and g ∈ RS×A , we have Vg,1
(s)−
′

π,P
Vg,1
(s) = Eπ,P ′

4.2

hP

i

π,P
H
′
h=1 (Ph − Ph )(sh , ah ) · Vg,h+1 |s1 = s .

Notes and Open Problems

This chapter discussed the results of weakly communicating CMDPs.
The analysis of the model-based algorithm is taken from (Chen et al.,
2022b). Readers may refer to the cited work for more detailed proof and
insights. No baseline comparisons are provided since the work mentioned
here is the first in the weakly communicating (WC) setting. It is to be
noted that no model-free algorithm is currently known that provides
provable guarantees for WC CMDPs.
√
It is known that the regret lower bound in CMDP is Ω( T ). Results
achieving this order have been exhibited for model-based and model-free
unconstrained tabular setups in (Fruit et al., 2018) and (Zhang and Xie,
2023), respectively. Specifically, (Zhang and Xie, 2023) establishes the
said result for WC MDPs. Moreover, as shown in Table 3.1, lower bound
achieving regret is also achievable in the tabular model-based constrained
setup. However, the results apply only to ergodic CMDPs. Given this
state-of-the-art, we have the following two open questions in the context
of WC CMDPs: how to design (i) a model-based algorithm having a
regret guarantee better than Õ(T 2/3 ) and (ii) a model-free algorithm
having any sublinear guarantee. Finally, we also note that the algorithm
discussed in this chapter requires the knowledge of some parameters of
the underlying CMDP (e.g., upper bound on the span). Designing a
parameter-free algorithm is another interesting open direction.

4.2. NOTES AND OPEN PROBLEMS

97

Acknowledgements
The authors would like to thank Mridul Agarwal, Amrit Singh Bedi,
Alec Koppel, Ather Gattami, and Swetha Ganesh for discussions and
comments on the manuscript.

References

Al-Abbasi, A. O., A. Ghosh, and V. Aggarwal. (2019). “Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning”. IEEE Transactions on Intelligent Transportation
Systems. 20(12): 4714–4727.
Agarwal, A., S. M. Kakade, J. D. Lee, and G. Mahajan. (2020). “Optimality and Approximation with Policy Gradient Methods in Markov
Decision Processes”. In: Proceedings of Thirty Third Conference on
Learning Theory. Ed. by J. Abernethy and S. Agarwal. Vol. 125.
Proceedings of Machine Learning Research. PMLR. 64–66. url:
http://proceedings.mlr.press/v125/agarwal20a.html.
Agarwal, A., S. M. Kakade, J. D. Lee, and G. Mahajan. (2021). “On
the theory of policy gradient methods: Optimality, approximation,
and distribution shift”. The Journal of Machine Learning Research.
22(1): 4431–4506.
Agarwal, M. and V. Aggarwal. (2023). “Reinforcement Learning for Joint
Optimization of Multiple Rewards”. Journal of Machine Learning
Research. 24(49): 1–41.
Agarwal, M., V. Aggarwal, and T. Lan. (2022a). “Multi-objective reinforcement learning with non-linear scalarization”. In: Proceedings
of the 21st International Conference on Autonomous Agents and
Multiagent Systems. 9–17.

98

REFERENCES

99

Agarwal, M., Q. Bai, and V. Aggarwal. (2022b). “Concave Utility Reinforcement Learning with Zero-Constraint Violations”. Transactions
on Machine Learning Research.
Agarwal, M., Q. Bai, and V. Aggarwal. (2022c). “Regret guarantees
for model-based reinforcement learning with long-term average constraints”. In: Uncertainty in Artificial Intelligence. PMLR. 22–31.
Agrawal, S. and R. Jia. (2017). “Optimistic posterior sampling for
reinforcement learning: worst-case regret bounds”. In: Advances in
Neural Information Processing Systems. 1184–1194.
Altman, E. and A. Schwartz. (1991). “Adaptive control of constrained
Markov chains”. IEEE Transactions on Automatic Control. 36(4):
454–462. doi: 10.1109/9.75103.
Altman, E. (1999). Constrained Markov decision processes. Vol. 7. CRC
Press.
Bai, Q., M. Agarwal, and V. Aggarwal. (2022a). “Joint Optimization of
Concave Scalarized Multi-Objective Reinforcement Learning with
Policy Gradient Based Algorithm”. Journal of Artificial Intelligence
Research. 74: 1565–1597.
Bai, Q., V. Aggarwal, and A. Gattami. (2023a). “Provably SampleEfficient Model-Free Algorithm for MDPs with Peak Constraints”.
Journal of Machine Learning Research. 24(60): 1–25.
Bai, Q., A. S. Bedi, M. Agarwal, A. Koppel, and V. Aggarwal. (2022b).
“Achieving zero constraint violation for constrained reinforcement
learning via primal-dual approach”. In: Proceedings of the AAAI
Conference on Artificial Intelligence. Vol. 36. No. 4. 3682–3689.
Bai, Q., A. S. Bedi, M. Agarwal, A. Koppel, and V. Aggarwal. (2023b).
“Achieving zero constraint violation for concave utility constrained
reinforcement learning via primal-dual approach”. Journal of Artificial Intelligence Research. 78: 975–1016.
Bai, Q., A. S. Bedi, and V. Aggarwal. (2023c). “Achieving zero constraint
violation for constrained reinforcement learning via conservative
natural policy gradient primal-dual algorithm”. In: Proceedings of
the AAAI Conference on Artificial Intelligence.

100

REFERENCES

Bai, Q., W. U. Mondal, and V. Aggarwal. (2024a). “Learning General
Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm”. arXiv
preprint arXiv:2402.02042.
Bai, Q., W. U. Mondal, and V. Aggarwal. (2024b). “Regret analysis of
policy gradient algorithm for infinite horizon average reward markov
decision processes”. In: Proceedings of the AAAI Conference on
Artificial Intelligence.
Bartlett, P. L. and A. Tewari. (2009). “REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs”.
In: Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence. 35–42.
Brantley, K., M. Dudik, T. Lykouris, S. Miryoosefi, M. Simchowitz, A.
Slivkins, and W. Sun. (2020). “Constrained episodic reinforcement
learning in concave-convex and knapsack settings”. Advances in
Neural Information Processing Systems. 33: 16315–16326.
Chen, C.-L., H. Zhou, J. Chen, M. Pedramfar, V. Aggarwal, T. Lan,
Z. Zhu, C. Zhou, T. Gasser, P. M. Ruiz, et al. (2023). “Two-tiered
Online Optimization of Region-wide Datacenter Resource Allocation
via Deep Reinforcement Learning”. arXiv preprint arXiv:2306.17054.
Chen, L., M. Jafarnia-Jahromi, R. Jain, and H. Luo. (2021). “Implicit
Finite-Horizon Approximation and Efficient Optimal Algorithms
for Stochastic Shortest Path”. Advances in Neural Information
Processing Systems.
Chen, L., R. Jain, and H. Luo. (2022a). “Improved no-regret algorithms
for stochastic shortest path with linear mdp”. In: International
Conference on Machine Learning. PMLR. 3204–3245.
Chen, L., R. Jain, and H. Luo. (2022b). “Learning Infinite-horizon
Average-reward Markov Decision Process with Constraints”. In: Proceedings of the 39th International Conference on Machine Learning.
Ed. by K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu,
and S. Sabato. Vol. 162. Proceedings of Machine Learning Research.
PMLR. 3246–3270.
Cohen, A., Y. Efroni, Y. Mansour, and A. Rosenberg. (2021). “Minimax Regret for Stochastic Shortest Path”. Advances in Neural
Information Processing Systems.

REFERENCES

101

Cohen, A., H. Kaplan, Y. Mansour, and A. Rosenberg. (2020). “Nearoptimal Regret Bounds for Stochastic Shortest Path”. In: Proceedings
of the 37th International Conference on Machine Learning. Vol. 119.
PMLR. 8210–8219.
Ding, D., X. Wei, Z. Yang, Z. Wang, and M. Jovanovic. (2021). “Provably
efficient safe exploration via primal-dual policy optimization”. In:
International Conference on Artificial Intelligence and Statistics.
PMLR. 3304–3312.
Ding, D., K. Zhang, T. Basar, and M. Jovanovic. (2020). “Natural Policy
Gradient Primal-Dual Method for Constrained Markov Decision
Processes”. Advances in Neural Information Processing Systems. 33.
Ding, D., K. Zhang, J. Duan, T. Başar, and M. R. Jovanović. (2023).
“Convergence and sample complexity of natural policy gradient
primal-dual methods for constrained MDPs”. arXiv: 2206.02346
[math.OC].
Dorfman, R. and K. Y. Levy. (2022). “Adapting to mixing time in
stochastic optimization with Markovian data”. In: International
Conference on Machine Learning. PMLR. 5429–5446.
Dragomir, S. S. (2003). “A survey on Cauchy-Bunyakovsky-Schwarz
type discrete inequalities”. J. Inequal. Pure Appl. Math. 4(3): 1–142.
Efroni, Y., S. Mannor, and M. Pirotta. (2020). “Exploration-exploitation
in constrained mdps”. arXiv preprint arXiv:2003.02189.
Fruit, R., M. Pirotta, A. Lazaric, and R. Ortner. (2018). “Efficient biasspan-constrained exploration-exploitation in reinforcement learning”.
In: International Conference on Machine Learning. PMLR. 1578–
1586.
Ganesh, S. and V. Aggarwal. (2024). “An Accelerated Multi-level Monte
Carlo Approach for Average Reward Reinforcement Learning with
General Policy Parametrization”. arXiv.
Ganesh, S., W. U. Mondal, and V. Aggarwal. (2024). “Variance-Reduced
Policy Gradient Approaches for Infinite Horizon Average Reward
Markov Decision Processes”. arXiv preprint arXiv:2404.02108.
Gattami, A., Q. Bai, and V. Aggarwal. (2021). “Reinforcement learning for constrained markov decision processes”. In: International
Conference on Artificial Intelligence and Statistics. PMLR. 2656–
2664.

102

REFERENCES

Geng, N., T. Lan, V. Aggarwal, Y. Yang, and M. Xu. (2020). “A
multi-agent reinforcement learning perspective on distributed traffic engineering”. In: 2020 IEEE 28th International Conference on
Network Protocols (ICNP). IEEE. 1–11.
Ghosh, A., X. Zhou, and N. Shroff. (2023). “Achieving Sub-linear Regret in Infinite Horizon Average Reward Constrained MDP with
Linear Function Approximation”. In: The Eleventh International
Conference on Learning Representations.
Gong, H. and M. Wang. (2020). “A Duality Approach for Regret Minimization in Average-Award Ergodic Markov Decision Processes”. In:
Learning for Dynamics and Control. PMLR. 862–883.
Gonzalez, G., M. Balakuntala, M. Agarwal, T. Low, B. Knoth, A. W.
Kirkpatrick, J. McKee, G. Hager, V. Aggarwal, Y. Xue, et al. (2023).
“ASAP: A Semi-Autonomous Precise System for Telesurgery during
Communication Delays”. IEEE Transactions on Medical Robotics
and Bionics.
Jaksch, T., R. Ortner, and P. Auer. (2010). “Near-optimal regret bounds
for reinforcement learning”. Journal of Machine Learning Research.
11(Apr): 1563–1600.
Jin, C., Z. Yang, Z. Wang, and M. I. Jordan. (2020). “Provably efficient
reinforcement learning with linear function approximation”. In: Proceedings of Thirty Third Conference on Learning Theory. Ed. by
J. Abernethy and S. Agarwal. Vol. 125. Proceedings of Machine
Learning Research. PMLR. 2137–2143.
Langford, J. and S. Kakade. (2002). “Approximately optimal approximate reinforcement learning”. In: Proceedings of ICML.
Lattimore, T. and C. Szepesvári. (2020). Bandit algorithms. Cambridge
University Press.
Lawler, G. F. (2018). Introduction to stochastic processes. Chapman
and Hall/CRC.
Ling, L., W. U. Mondal, and S. V. Ukkusuri. (2024). “Cooperating
graph neural networks with deep reinforcement learning for vaccine
prioritization”. IEEE Journal of Biomedical and Health Informatics.

REFERENCES

103

Liu, T., R. Zhou, D. Kalathil, P. Kumar, and C. Tian. (2021). “Learning
policies with zero or bounded constraint violation for constrained
mdps”. Advances in Neural Information Processing Systems. 34:
17183–17193.
Liu, Y., K. Zhang, T. Basar, and W. Yin. (2020). “An improved analysis
of (variance-reduced) policy gradient and natural policy gradient
methods”. Advances in Neural Information Processing Systems. 33:
7624–7636.
Manchella, K., M. Haliem, V. Aggarwal, and B. Bhargava. (2021).
“Passgoodpool: Joint passengers and goods fleet management with
reinforcement learning aided pricing, matching, and route planning”.
IEEE Transactions on Intelligent Transportation Systems. 23(4):
3866–3877.
Mondal, W. U. and V. Aggarwal. (2024a). “Improved sample complexity
analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward markov decision
processes”. In: International Conference on Artificial Intelligence
and Statistics. PMLR. 3097–3105.
Mondal, W. U. and V. Aggarwal. (2024b). “Sample-Efficient Constrained
Reinforcement Learning with General Parameterization”. arXiv
preprint arXiv:2405.10624.
Mondal, W. U., V. Aggarwal, and S. V. Ukkusuri. (2023). “MeanField Control based Approximation of Multi-Agent Reinforcement
Learning in Presence of a Non-decomposable Shared Global State”.
Transactions on Machine Learning Research.
Osband, I., D. Russo, and B. Van Roy. (2013). “(More) efficient reinforcement learning via posterior sampling”. In: Advances in Neural
Information Processing Systems. 3003–3011.
Osband, I. and B. Van Roy. (2017). “Why is posterior sampling better than optimism for reinforcement learning?” In: International
conference on machine learning. PMLR. 2701–2710.
Ouyang, Y., M. Gagrani, A. Nayyar, and R. Jain. (2017). “Learning unknown markov decision processes: A thompson sampling approach”.
Advances in neural information processing systems. 30.

104

REFERENCES

Patel, B., W. A. Suttle, A. Koppel, V. Aggarwal, B. M. Sadler, A. S.
Bedi, and D. Manocha. (2024). “Global Optimality without Mixing
Time Oracles in Average-reward RL via Multi-level Actor-Critic”.
In: International Conference on Machine Learning.
Pesquerel, F. and O.-A. Maillard. (2022). “IMED-RL: Regret optimal
learning of ergodic Markov decision processes”. In: NeurIPS 2022Thirty-sixth Conference on Neural Information Processing Systems.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic
dynamic programming. John Wiley & Sons.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic
Dynamic Programming. 1st. New York, NY, USA: John Wiley &
Sons, Inc. isbn: 0471619779.
Russo, D. and B. Van Roy. (2014). “Learning to optimize via posterior
sampling”. Mathematics of Operations Research. 39(4): 1221–1243.
Serfling, R. J. (1974). “Probability Inequalities for the Sum in Sampling
Without Replacement”. The Annals of Statistics. 2(1): 39–48.
Shani, L., Y. Efroni, A. Rosenberg, and S. Mannor. (2020). “Optimistic
Policy Optimization with Bandit Feedback”. In: Proceedings of the
37th International Conference on Machine Learning. 8604–8613.
Singh, R., A. Gupta, and N. B. Shroff. (2020). “Learning in Markov decision processes under constraints”. arXiv preprint arXiv:2002.12435.
Sutton, R. S., D. McAllester, S. Singh, and Y. Mansour. (1999). “Policy
gradient methods for reinforcement learning with function approximation”. Advances in neural information processing systems. 12.
Wang, L., Q. Cai, Z. Yang, and Z. Wang. (2020). “Neural Policy Gradient Methods: Global Optimality and Rates of Convergence”. In:
International Conference on Learning Representations.
Wei, C.-Y., M. J. Jahromi, H. Luo, H. Sharma, and R. Jain. (2020).
“Model-free reinforcement learning in infinite-horizon average-reward
markov decision processes”. In: International conference on machine
learning. PMLR. 10170–10180.
Wei, H., X. Liu, and L. Ying. (2022a). “A Provably-Efficient Model-Free
Algorithm for Infinite-Horizon Average-Reward Constrained Markov
Decision Processes”. In: Proceedings of the AAAI Conference on
Artificial Intelligence.

REFERENCES

105

Wei, H., X. Liu, and L. Ying. (2022b). “Triple-Q: A Model-Free Algorithm for Constrained Reinforcement Learning with Sublinear
Regret and Zero Constraint Violation”. In: International Conference
on Artificial Intelligence and Statistics. PMLR. 3274–3307.
Weissman, T., E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. (2003). “Inequalities for the L1 deviation of the empirical
distribution”. Hewlett-Packard Labs, Tech. Rep.
Yu, T., Y. Tian, J. Zhang, and S. Sra. (2021). “Provably Efficient
Algorithms for Multi-Objective Competitive RL”. In: Proceedings
of the 38th International Conference on Machine Learning. Ed. by
M. Meila and T. Zhang. Vol. 139. Proceedings of Machine Learning
Research. PMLR. 12167–12176.
Zhang, J., C. Ni, C. Szepesvari, M. Wang, et al. (2021). “On the
convergence and sample efficiency of variance-reduced policy gradient
method”. Advances in Neural Information Processing Systems. 34:
2228–2240.
Zhang, Z. and Q. Xie. (2023). “Sharper Model-free Reinforcement
Learning for Average-reward Markov Decision Processes”. In: The
Thirty Sixth Annual Conference on Learning Theory. PMLR. 5476–
5477.
Zheng, L. and L. Ratliff. (2020). “Constrained upper confidence reinforcement learning”. In: Learning for Dynamics and Control. PMLR.
620–629.

