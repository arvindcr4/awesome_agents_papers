Here is the extracted information:

**PAPER:** Modelling the Dynamic Joint Policy of Teammates with Attention Multi-agent DDPG
**TITLE:** Modelling the Dynamic Joint Policy of Teammates with Attention Multi-agent DDPG
**ARXIV_ID:** 1811.07029v1
**RESEARCH_METHOD:** 07_model_based_rl
**METHOD_DESCRIPTION:** This paper presents a novel actor-critic reinforcement learning method that embeds an attention mechanism into a centralized critic to model the dynamic joint policy of teammates in a cooperative multi-agent setting. The method, called ATT-MADDPG, uses a K-head module to estimate the Q-values for each possible action of the teammates and an attention module to weight these Q-values based on their importance. The attention weights are learned jointly with the policy and value functions using backpropagation.
**KEY_CONTRIBUTIONS:**
* The proposed ATT-MADDPG method can model the dynamic joint policy of teammates in an adaptive manner, allowing for efficient cooperation among agents.
* The attention mechanism introduces a special structure to explicitly model the teammates' policies, enabling the agents to adjust their own policies accordingly.
* The method can train decentralized policies to handle distributed tasks with continuous action space, making it applicable to real-world problems such as packet routing.
* Experimental results show that ATT-MADDPG outperforms state-of-the-art RL-based methods and rule-based methods in terms of reward, scalability, and robustness.
