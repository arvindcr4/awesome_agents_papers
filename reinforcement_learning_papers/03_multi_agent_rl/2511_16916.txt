Hybrid Differential Reward: Combining Temporal Difference and
Action Gradients for Efficient Multi-Agent Reinforcement Learning
in Cooperative Driving

arXiv:2511.16916v1 [cs.AI] 21 Nov 2025

Ye Han, Lijun Zhangâˆ— , Dejian Meng, Zhuang Zhang

Abstractâ€”In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based
reward functions suffer from the issue of vanishing reward
differences. This phenomenon results in a low signal-to-noise
ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address
this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate
how the temporal quasi-steady nature of traffic states and the
physical proximity of actions lead to the failure of traditional
reward signals. Building on this analysis, the HDR framework
innovatively integrates two complementary components: (1) a
Temporal Difference Reward (TRD) based on a global potential
function, which utilizes the evolutionary trend of potential energy
to ensure optimal policy invariance and consistency with longterm objectives; and (2) an Action Gradient Reward (ARG),
which directly measures the marginal utility of actions to provide
a local guidance signal with a high SNR. Furthermore, we
formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying
agent set and provide a complete instantiation scheme for HDR
within this framework. Extensive experiments conducted using
both online planning (MCTS) and Multi-Agent Reinforcement
Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate
that the HDR mechanism significantly improves convergence
speed and policy stability. The results confirm that HDR guides
agents to learn high-quality cooperative policies that effectively
balance traffic efficiency and safety.

I. I NTRODUCTION
As autonomous driving technology evolves from singlevehicle intelligence to collective intelligence, cooperative driving has emerged as a critical approach to mitigating traffic
congestion and enhancing road safety [1]. Unlike the reactive
decision-making of single-vehicle systems, cooperative driving
requires multiple agents to engage in strategic interactions and
game-theoretic reasoning to seek joint solutions that maximize collective benefits. Multi-Agent Reinforcement Learning
(MARL) is widely regarded as an ideal paradigm for achieving
this goal due to its capability to handle high-dimensional state
spaces and complex game dynamics [2], [3].
However, despite the significant success of MARL in discrete games such as StarCraft and Go, its application to highfrequency, continuous decision-making tasksâ€”specifically vehicle controlâ€”faces a critical yet long-overlooked challenge:
the problem of vanishing reward differences [4], [5].
Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang are with the
School of Automotive Studies, Tongji University, Shanghai 201804,
China.
{hanye_leohancnjs, tjedu_zhanglijun,

mengdejian, zhuang_zhang}@tongji.edu.cn
âˆ— Corresponding author: Lijun Zhang

The root of this problem lies in the inherent conflict
between the temporal quasi-steady nature of the physical traffic
system and the high-frequency characteristics of decisionmaking algorithms. Within a short decision interval (e.g., 0.1
seconds), vehicle inertia dictates that macroscopic states, such
as position and velocity, undergo only negligible evolution.
Traditional state-based reward functions fail to capture these
subtle changes, causing the differences in reward signals
generated by distinct actions (e.g., rapid acceleration versus
gradual acceleration) to be overwhelmed by environmental
noise. This results in an excessively low Signal-to-Noise Ratio
(SNR) for policy gradients, leading to issues such as vanishing
gradients or excessive variance during the early stages of
training. Consequently, algorithms struggle to converge to
complex cooperative policies.
Existing solutions primarily focus on reward shaping [6]
or intrinsic motivation [7]. Although classic Potential-Based
Reward Shaping (PBRS) [6] theoretically guarantees optimal
policy invariance, it fundamentally relies on state differences
(Ï•(sâ€² )âˆ’Ï•(s)). When state changes are negligible, the guidance
signal provided by PBRS remains equally weak, failing to
fundamentally resolve the issue of low gradient SNR. In other
words, existing methods address the directionality of rewards
but fail to resolve the issue of reward sensitivity.
To overcome this bottleneck, this paper proposes a novel
Hybrid Differential Reward (HDR) mechanism. HDR moves
beyond reliance solely on state evaluation by innovatively
introducing the dimension of action gradients. It integrates
two complementary signals: (1) a Temporal Difference Reward (TRD) based on a potential function, which utilizes
state evolution trends to ensure consistency with long-term
optimization objectives and theoretical completeness; and (2)
an Action Gradient Reward (ARG) based on direct action
effects, which constructs a local gradient signal with high
SNR and strong causality by directly measuring the marginal
contribution of actions to the objective.
The main contributions of this paper are as follows:
We formally define the problem of vanishing reward
differences caused by the temporal quasi-steady nature of
states and the physical proximity of actions in the context of high-frequency control, revealing the fundamental
reason for MARL training failures in this domain.
â€¢ We propose the Hybrid Differential Reward mechanism.
Through theoretical proof, we demonstrate how the TRD
component guarantees optimal policy invariance and how
the ARG component significantly enhances gradient SNR
by reshaping the local optimization landscape.
â€¢

We model the cooperative driving problem as a MultiAgent Partially Observable Markov Game (POMDPG)
with a time-varying agent set and provide a complete
instantiation scheme for HDR within this complex task.
â€¢ Experiments across various paradigms, including online
planning (MCTS) and offline learning (QMIX, MAPPO,
MADDPG), demonstrate that HDR significantly accelerates convergence and improves policy quality, validating
its effectiveness as a general reward design paradigm for
high-frequency decision-making.

â€¢

II. R ELATED W ORKS
This section reviews the modeling paradigms for multivehicle cooperative driving, with a specific focus on the evolution of reward function design and the limitations encountered
in continuous control tasks.

F = Î³Ï•(sâ€² ) âˆ’ Ï•(s) preserves optimal policy invariance. This
theory has been widely applied to accelerate RL training [17],
[18]. Additionally, the Reward Centering technique proposed
by Naik et al. [19] reduces variance by subtracting a dynamic
baseline, thereby improving learning stability.
Although the aforementioned methods are theoretically
sound, they struggle to address the sensitivity issues inherent
in high-frequency continuous control. PBRS fundamentally
relies on the temporal difference of states; when the system
is in a quasi-steady state where Ï•(sâ€² ) â‰ˆ Ï•(s), the resulting
shaping signal remains weak. In other words, existing reward
shaping techniques primarily focus on guiding the optimization direction correctly, but fail to adequately address the
effective differentiation of subtle action variations. The HDR
mechanism proposed in this paper fills this critical research
gap by introducing an Action Gradient signal directly coupled
with actions.

A. Cooperative Driving and Game Modeling
Cooperative driving aims to resolve complex right-of-way
conflicts and optimize macroscopic traffic efficiency through
active collaboration among vehicles. Early studies were predominantly based on optimal control theory, modeling the
problem as centralized Model Predictive Control (MPC) [8],
[9]. However, these methods typically treat other vehicles as
predictable dynamic obstacles, making it difficult to handle
complex strategic interactions.
To characterize dynamic games among agents, Game
Theory provides a more natural mathematical framework
[3]. Extensive research has been conducted, ranging from
Stackelberg games handling asymmetric interactions [10] to
Bayesian games addressing incomplete information [2]. Currently, Multi-Agent Markov Games have become the mainstream modeling paradigm due to their ability to unify the description of sequential decision-making, strategic interaction,
and environmental stochasticity [11], [12]. This paper adopts
this framework, extending it to accommodate the dynamic
variation in the number of agents within continuous traffic
flows.

III. P ROBLEM F ORMULATION
This section aims to establish a precise mathematical model
for the multi-vehicle cooperative decision-making problem and
to analyze the inherent structural deficiencies of traditional
reward functions within this context. This analysis serves
as the theoretical basis for the proposed Hybrid Differential
Reward (HDR) mechanism.

Coordination Zone

Inflow

Cooperative Driving
Environment

Outflow

Central
Controller

HDV

CAV with Different Intentions Observation

Fig. 1. Schematic of the cooperative driving environment under continuous
traffic flow.

B. Reward Design Dilemma and Shaping Methods
In reinforcement learning and game solving, the reward
function plays a pivotal role in guiding policy evolution.
Traditional reward design typically employs a weighted sum
formulation, comprehensively balancing multiple conflicting
objectives such as safety, efficiency, and comfort [13], [14].
However, in systems with smooth state evolution like traffic
flow, researchers have found that reward functions based on
sparse events (e.g., collisions) or macroscopic states (e.g.,
velocity) often lead to low learning efficiency [15]. Particularly
under high-frequency decision-making, the state difference
âˆ†s resulting from adjacent actions is minimal, causing the
gradient âˆ‡a R of the reward signal R(s, a) to be overwhelmed
by environmental noise [16]. This corresponds to the problem
of vanishing reward differences described previously.
To address the sparse reward problem, Ng et al. proposed
the renowned Potential-Based Reward Shaping (PBRS) theory [6], proving that an additional reward in the form of

A. Scenario Description and System Characteristics
This paper investigates the multi-vehicle cooperative driving
problem within continuous traffic flow, as illustrated in Fig. 1.
The system comprises a designated coordination zone, characterized by continuous dynamic traffic with inflow from the left
and outflow to the right. New vehicles continuously enter the
task area while others complete their tasks and exit, resulting
in a controlled agent set Nt that varies dynamically over
time rather than remaining fixed. The road segment contains a
mix of Human-Driven Vehicles (HDVs, shown in dark colors)
with stochastic behaviors and controlled Connected Automated
Vehicles (CAVs, shown in bright colors). As depicted, CAVs
with distinct colors possess different driving intentions (e.g.,
the orange vehicle intends to turn left, while the green
vehicle intends to go straight), leading to complex rightof-way competitions and potential conflicts within a limited

Based on the aforementioned characteristics, we model the
multi-vehicle cooperative driving problem as a Multi-Agent
Partially Observable Markov Game (POMDPG) with a timevarying agent set [20]. The game is defined by the tuple
G = âŸ¨{Nt }tâ‰¥0 , S, {At }tâ‰¥0 , P, R, â„¦, O, Î³âŸ©, where Nt is the
set of agents present in the coordination zone at time step
t, a key time-varying feature. S denotes the global state
space, containing complete information about all vehicles and
the road environment. At represents the joint action space,
defined as At = Ã—iâˆˆNt A(i) , whose dimension varies with
Nt . P (sâ€² |s, at ) is the state transition function, describing
vehicle dynamics, stochastic HDV behaviors, and the random
processes of vehicle entry and exit. R(s, at ) is the joint reward
function, returning a global reward value based on collective
performance. O(s, i) is the observation function, generating a
private local observation o(i) âˆˆ â„¦ for agent i. Î³ is the discount
factor.
We adopt a centralized control paradigm based on local
observations. The central decision unit outputs a joint action
at âˆˆ At based on the joint policy Ï€(at |ot ). The discrete
action space for a single agent is the Cartesian product of
(i)
the longitudinal action set Along = {AC, MT, DC} (accelerate, maintain speed, decelerate) and the lateral action set
(i)
Alat = {LC, LK, RC} (change left, keep lane, change right),
(i)
(i)
i.e., A(i) = Alat Ã— Along .
The objective of the system is to find an optimal centralized
joint policy Ï€ âˆ— that maximizes the expected total group return:
ï£®
ï£¹
T
âˆ’1 X
X
(j)
Ï€ âˆ— = argmax E ï£°
Î³ t wr(j) rt Ï€ ï£»
(1)
Ï€

(j)

where rt

t=0 jâˆˆNt

is the individual reward for vehicle j.

C. Limitations Analysis
Although the optimization objective defined in Eq. (1) is
macroscopically correct, applying it to high-frequency, continuous decision-making processes reveals inherent structural
deficiencies that lead to the problem of vanishing reward
differences. Specifically, the reward signals become too weak
and ambiguous to provide effective gradients for the learning
process. We analyze these limitations from two dimensions:
temporal and action.
1) Temporal Dimension: States exhibit temporal quasisteady characteristics. Traffic flow acts as a typical quasisteady system where macroscopic states evolve smoothly over
short intervals. Even if an agent executes an optimal action,
the state at the next time step st+1 remains highly similar to
the current state st .

 ) L U V W  2 U G H U  5 H Z D U G  ' L I I H U H Q F H

B. Multi-Agent Markov Game Modeling

 
 
 
 
 ) L U V W  2 U G H U  ' L I I H U H Q F H  R I  7 R W D O  5 H Z D U G
 ) L U V W  2 U G H U  ' L I I H U H Q F H  R I  6 W D W H  E D V H G  5 H Z D U G

 
 

    

    
    
 ' H F L V L R Q  6 W H S V

    

     

Fig. 2. Schematic of state evolution for key variables (e.g., velocity, distance)
under high-frequency decision-making. Despite policy differences, the state
difference âˆ†s between st and st+1 within a small time step âˆ†t is minimal,
causing the state-based reward difference to approach zero.

As shown in Fig. 2, this temporal quasi-steady nature causes
the state-based reward difference âˆ†Rs = R(st+1 ) âˆ’ R(st )
to be extremely small, resulting in a locally flat optimization
landscape. For algorithms relying on these signals for exploration and learning, such weak and sparse gradient signals are
virtually indistinguishable from noise.
 3 U R E D E L O L W \  ' H Q V L W \

spatiotemporal domain. Constrained by physical perception
ranges, each CAV only possesses local observations (indicated
by the halo range). A central controller is required to aggregate
these local observations, infer the global state, and compute
optimal joint actions in real-time to ensure global safety and
efficiency under uncertainty.

 

 6 W G   ' H Y         

 6 W G   ' H Y         

  

 

  

 
 

  

  
   
   
   
  D  ' L I I H U H Q F H  R I  7 R W D O  5 H Z D U G

 

   
   
   
  E  ' L I I H U H Q F H  R I  6 W D W H  5 H Z D U G

Fig. 3. Comparison of distributions between the first-order difference of the
state-based reward signal and the total reward signal. The reward difference
âˆ†R is significantly smaller than the noise Ïƒ introduced by the environment
or observations, leading to a low signal-to-noise ratio in policy gradients and
an inability to effectively distinguish marginal action utilities.

2) Action Dimension: Actions exhibit physical proximity.
Under high-frequency decision-making, two semantically distinct but physically adjacent discrete actions (e.g., â€maintain
speedâ€ vs. â€accelerateâ€) produce negligible differences between the resulting next states sâ€²1 and sâ€²2 within a extremely
short time step âˆ†t = 0.1 s. For instance, after 0.1 s, the
difference in travel distance between actions with accelerations
of 0.0 m/s2 and 1.0 m/s2 is merely 0.005 meters.
For a state-based reward function, this implies that the
difference in calculated reward values R(sâ€² ) becomes infinitesimal and is easily overwhelmed by random environmental
noise. This phenomenon leads to severe credit assignment
problems, causing instability in the optimization process and
difficulty in policy convergence. The root cause is that the reward signal evaluates only the outcome of the actionâ€”namely,
the next stateâ€”rather than the quality of the action itself.
Therefore, there is an urgent need for a mechanism that can
explicitly measure both the temporal evolution trend of states
and the marginal utility of actions, transforming a flat reward
surface into a strongly guided optimization signal.

Quasi-steady State Sequence

State-based Reward

Potential Function

Hybrid Differential Reward

reward

â€¦
â€¦

diff

diff

diff

Steady transfer

Temporal Dimension

basis

signal

time step

SNR

Intrinsic State

Action Dimension

State:
Social:

diff

Action in

: GOOD

Penaltyï¼š

BAD

Reward Reshaping

Fig. 4. Overall architecture of the Hybrid Differential Reward.

IV. M ETHODOLOGY
To address the problem of vanishing reward differences
described in Section III-C, this section proposes a novel
Hybrid Differential Reward (HDR) mechanism. By fusing two
complementary differential signals, this mechanism ensures
the correctness of long-term optimization objectives while
significantly improving the signal-to-noise ratio (SNR) for
distinguishing short-term actions.
A. HDR Framework Construction
The core idea of the HDR mechanism is to construct an
objective function JHDR , which is a weighted combination
of the Temporal Difference Reward (TRD) based on state
evolution and the Action Gradient Reward (ARG) based on
the marginal utility of actions.
1) Temporal Difference Reward (TRD): To provide a reward signal that is oriented towards long-term goals and guarantees optimal policy invariance, we introduce the Temporal
Difference Reward (TRD) based on a potential function. TRD
draws upon the theory of potential-based reward shaping but
applies it as a differential approximation of state value.
Based on a state potential function Ï•(s), we define an
augmented reward function Râ€² , which consists of the original
environmental reward R and an additional TRD signal FTRD :
Râ€² (s, a, sâ€² ) = R(s, a, sâ€² ) + FTRD (s, sâ€² ),

(2)

where the TRD signal FTRD is defined as the discounted
difference between the potential energy of the next state and
the current state:
FTRD (s, sâ€² ) â‰œ Î³Ï•(sâ€² ) âˆ’ Ï•(s).

(3)

Here, Î³ is the discount factor. The FTRD signal intuitively reflects whether the system state is evolving towards a favorable
direction (FTRD > 0) or an unfavorable one (FTRD < 0) after
executing action a.

Based on this definition, we propose the following Proposition: For any Markov Decision Process M = (S, A, P, R, Î³)
and an augmented MDP Mâ€² = (S, A, P, Râ€² , Î³) with the TRD
signal defined in Eq. (2), M and Mâ€² share the same set
of optimal policies. This ensures that the HDR mechanism
remains consistent with the original optimization problem
regarding long-term objectives.

Proof. To concisely describe the Bellman iteration process,
we introduce the Bellman optimal operator T . For the original
MDP, its application to any value function Q is defined as:
h
i
â€² â€²
(T Q)(s, a) â‰œ Esâ€² R(s, a, sâ€² ) + Î³ max
Q(s
,
a
)
.
(4)
â€²
a

By definition, the original optimal value function Qâˆ—M is the
unique fixed point of the operator T , i.e., T Qâˆ—M = Qâˆ—M .
Similarly, the operator T â€² for the augmented MDP is defined
as:
h
i
â€² â€²
(T â€² Q)(s, a) â‰œ Esâ€² Râ€² (s, a, sâ€² ) + Î³ max
Q(s
,
a
)
. (5)
â€²
a

We need to prove that the optimal value function of the
augmented MDP, Qâˆ—Mâ€² , satisfies Qâˆ—Mâ€² (s, a) = Qâˆ—M (s, a) âˆ’
Ï•(s). Let the function to be verified be QÌ‚(s, a) â‰œ Qâˆ—M (s, a) âˆ’
Ï•(s). This proposition is equivalent to verifying whether QÌ‚ is
a fixed point of the operator T â€² .
First, we introduce the original optimal state value
âˆ—
VM
(sâ€² ) â‰œ maxaâ€² Qâˆ—M (sâ€² , aâ€² ). Using the property that Ï•(sâ€² )
is independent of actions, the optimal value of QÌ‚ at the next
state sâ€² can be simplified as:
âˆ—
max
QÌ‚(sâ€² , aâ€² ) = VM
(sâ€² ) âˆ’ Ï•(sâ€² ).
â€²
a

(6)

Next, we substitute the definition of Râ€² from Eq. (2) and
Eq. (6) into the definition of operator T â€² . Examining the target
value Y (s, a, sâ€² ) inside the expectation operator, it can be

decomposed into three terms:
âˆ—
(sâ€² ) âˆ’ Ï•(sâ€² ))
R(s, a, s ) + Î³Ï•(s ) âˆ’ Ï•(s) +Î³ (VM
â€²

â€²

|

{z

FTRD

}

|

{z

max QÌ‚(sâ€² )

(7)

}

Further simplification yields:
âˆ—
Y = R(s, a, sâ€² ) + Î³VM
(sâ€² ) âˆ’ Ï•(s).

(8)

Finally, applying the expectation operator to Y :
âˆ—
(T â€² QÌ‚)(s, a) = Esâ€² [R(s, a, sâ€² ) + Î³VM
(sâ€² )] âˆ’ Ï•(s)

= (T Qâˆ—M )(s, a) âˆ’ Ï•(s)

(9)

= QÌ‚(s, a).
The derivation shows that QÌ‚ is indeed a fixed point of
T â€² . Since Ï•(s) is independent of the current action a, the
augmented policy Ï€ â€²âˆ— (s) = arg maxa QÌ‚(s, a) shares the same
maximizing parameters as the original policy Ï€ âˆ— (s). The
proposition is proved.
2) Action Gradient Reward (ARG): Although TRD addresses the correctness of long-term objectives, it relies on
macroscopic state changes sâ€² , which limits its effectiveness
in resolving the vanishing reward difference problem caused
by the physical proximity of actions. Therefore, we propose
the Action Gradient Reward (ARG), which aims to directly
evaluate the marginal utility of actions.
The core of ARG is to assess the additional return gain an
action a provides compared to an average or baseline action
abase in the current state s. This signal rARG is constructed as
an instantaneous reward strongly correlated with the current
action a and possessing high causality. It serves to reshape the
local geometry of the policy gradient surface:
rARG (s, a) âˆ âˆ‡a Qlocal (s, a)

(10)

where Qlocal is a Q-function measuring short-term action
utility. In practice, the ARG signal rARG is designed to
directly measure the immediate impact of action a on the
potential function Ï•(s) or its reduction of the instantaneous
cost function. The introduction of ARG ensures that even
when the difference between states st and st+1 is minimal,
small but goal-oriented actions a receive significant rewards,
thereby significantly improving the SNR of policy gradients.
3) Hybrid Optimization Objective: A standard MDP optimization problem aims to maximize the expected cumulative
which can
be expressed as JR (Î¸) =
Preturn,

âˆž
k
EÏ„ âˆ¼Ï€Î¸
k=0 Î³ R(sk , ak ) . We decompose and reconstruct
this objective to reflect the two differential concepts.
First, to explicitly incorporate the TRD signal, we define the
first objective function: the State Potential Elevation Objective
JTRD (Î¸). According to the TRD proposition, the goal of the
signal Î³Ï•(sâ€² ) âˆ’ Ï•(s) is to guide the agent towards states with
higher potential energy. Therefore, maximizing the expectation
of the TRD reward is equivalent to maximizing the total
potential energy gain brought by the policy over the entire
trajectory:
"âˆž
#
X
k
JTRD (Î¸) â‰œ EÏ„ âˆ¼Ï€Î¸
Î³ (Î³Ï•(sk+1 ) âˆ’ Ï•(sk ))
(11)
k=0

This objective explicitly sets the pursuit of favorable state
evolution trends as the optimization direction.
To explicitly incorporate the ARG signal, we define the
second objective function: the Action-Reward Alignment Objective JARG (Î¸). Its goal is to maximize the consistency
between the selected action and the direction of the reward
gradient. A direct definition is to maximize the expectation of
the reward function, as its gradient is directly related to the
ARG signal:
JARG (Î¸) = Esâˆ¼ÏÏ€Î¸ ,aâˆ¼Ï€Î¸ (a|s) [R(s, a)] .

(12)

Combining these two objectives linearly, we form the
foundational form of the objective function under the hybrid
differential reward framework:
JHDR (Î¸) â‰œ (1 âˆ’ Î±)JTRD (Î¸) + Î±JARG (Î¸),

(13)

where Î± âˆˆ [0, 1] is a hyperparameter used to balance the importance of long-term returns and immediate action guidance.
Having determined the hybrid optimization objective, the
core of any gradient-ascent-based algorithm is to calculate the
policy gradient of the objective function. Following Eq. (13),
the policy gradient for HDR can be expressed as:
âˆ‡Î¸ JHDR (Î¸) = (1 âˆ’ Î±)âˆ‡Î¸ JTRD (Î¸) + Î±âˆ‡Î¸ JARG (Î¸).

(14)

We now analyze the specific forms of these two gradient terms.
For the State Potential Elevation Objective JTRD (Î¸), the
single-step reward is rTRD (s, a, sâ€² ) = Î³Ï•(sâ€² )âˆ’Ï•(s). According
to the Policy Gradient Theorem,
its gradient can be written as:
Pâˆž
Î¸
âˆ‡Î¸ JTRD (Î¸) = EÏ„ âˆ¼Ï€Î¸ [ k=0 âˆ‡Î¸ log Ï€Î¸ (ak |sk )AÏ€TRD
(sk , ak )],
Ï€Î¸
where ATRD (s, a) is the advantage function corresponding to
the TRD reward under policy Ï€Î¸ .
For the Action-Reward Alignment Objective JARG (Î¸), according to the Deterministic Policy Gradient Theorem [21],
its gradient
 for a deterministic policy Ï€Î¸ (s) is: âˆ‡Î¸ JARG (Î¸) =
Esâˆ¼ÏÏ€Î¸ âˆ‡Î¸ Ï€Î¸ (s)âˆ‡a R(s, a)
.
a=Ï€Î¸ (s)

Combining these yields the general policy gradient under
the HDR framework. The HDR framework is algorithmagnostic; in principle, any algorithm capable of performing
policy optimization can apply this framework. For example, in Actor-Critic methods, the optimization of the Actor can be achieved by superimposing these two gradient terms. In deterministic policy methods like DDPG and
TD3, the loss function for the Actor can be designed as:
Lactor (Î¸) = âˆ’Es [(1 âˆ’ Î±)ATRD (s, Ï€Î¸ (s)) + Î±R (s, Ï€Î¸ (s))].
Taking the negative gradient of this loss function yields the
update direction consistent with the HDR objective.
Assumptions for HDR Objective Convergence: The following three conditions are assumed: 1) Policy Smoothness:
The policy Ï€Î¸ (s) is continuously differentiable with respect
to its parameters Î¸. For deterministic policies, the output
Ï€Î¸ (s) and the Jacobian matrix âˆ‡Î¸ Ï€Î¸ (s) are L-Lipschitz continuous with respect to Î¸. 2) Potential Function and TRD
Objective Smoothness: The potential function Ï•(s) is bounded
and differentiable. Consequently, the defined State Potential
Elevation Objective JTRD (Î¸) is continuously differentiable,
and its gradient âˆ‡Î¸ JTRD (Î¸) is LTRD -Lipschitz continuous. 3)
Reward Function and ARG Objective Smoothness: The reward

function R(s, a) is continuously differentiable with respect
to action a. Its gradient, i.e., the ARG signal Raâ€² (s, a) =
âˆ‡a R(s, a), has a bounded norm (âˆ¥Raâ€² (s, a)âˆ¥ â‰¤ CR < âˆž)
and is La -Lipschitz continuous with respect to action a.
Under these assumptions, the gradient of the Hybrid Differential Reward objective function JHDR (Î¸) is L-Lipschitz continuous. Therefore, any algorithm performing gradient ascent
on JHDR (Î¸), with an appropriate learning rate, will generate a
parameter sequence {Î¸k } that converges to a stationary point
satisfying âˆ‡Î¸ JHDR (Î¸âˆ— ) = 0.
Proof. The goal is to prove that the gradient âˆ‡Î¸ JHDR (Î¸) is
L-Lipschitz continuous. If proven, the convergence of the gradient ascent algorithm is guaranteed by standard optimization
theory. The proof proceeds in three steps:
Step 1: Analyze the properties of âˆ‡Î¸ JTRD (Î¸). According
to the assumptions, âˆ‡Î¸ JTRD (Î¸) is already LTRD -Lipschitz
continuous.
Step 2: Prove the Lipschitz continuity of âˆ‡Î¸ JARG (Î¸). Define
the gradient coupling term O(Î¸, s) as the product of the policy
Jacobian and the action gradient:
O(Î¸, s) â‰œ (âˆ‡Î¸ Ï€Î¸ (s))T âˆ‡a R(s, Ï€Î¸ (s)) .
{z
}|
{z
}
|
A(Î¸)

(15)

B(Î¸)

The objective gradient can be simplified as âˆ‡Î¸ JARG (Î¸) =
Es [O(Î¸, s)]. Consider the norm of the gradient difference and
apply Jensenâ€™s inequality to extract the expectation:
âˆ¥âˆ‡Î¸ JARG (Î¸1 ) âˆ’ âˆ‡Î¸ JARG (Î¸2 )âˆ¥
â‰¤Es âˆ¥O(Î¸1 , s) âˆ’ O(Î¸2 , s)âˆ¥.

(16)

For the internal term O, using the triangle inequality and
decomposition identity:
âˆ¥O(Î¸1 ) âˆ’ O(Î¸2 )âˆ¥
=âˆ¥A(Î¸1 )B(Î¸1 ) âˆ’ A(Î¸2 )B(Î¸2 )âˆ¥
â‰¤âˆ¥A(Î¸1 )âˆ¥âˆ¥B(Î¸1 ) âˆ’ B(Î¸2 )âˆ¥+

(17)

âˆ¥A(Î¸1 ) âˆ’ A(Î¸2 )âˆ¥âˆ¥B(Î¸2 )âˆ¥.
Based on the boundedness (bounds CA , CB ) and Lipschitz
continuity (constants LA , LB ) from the assumptions, we can
directly derive the upper bounds for each term. Specifically for
the composite function B(Î¸), using the chain rule property, we
have âˆ¥B(Î¸1 ) âˆ’ B(Î¸2 )âˆ¥ â‰¤ La LÏ€ âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥. Substituting into
the above inequality:
âˆ¥O(Î¸1 ) âˆ’ O(Î¸2 )âˆ¥
â‰¤ (CA (La LÏ€ ) + LA CB ) âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥

(18)

â‰œLARG âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥.

âˆ¥âˆ‡Î¸ JHDR (Î¸1 ) âˆ’ âˆ‡Î¸ JHDR (Î¸2 )âˆ¥
â‰¤(1 âˆ’ Î±)âˆ¥âˆ‡Î¸ JTRD (Î¸1 ) âˆ’ âˆ‡Î¸ JTRD (Î¸2 )âˆ¥+
â‰¤(1 âˆ’ Î±)LTRD âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥ + Î±LARG âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥
= ((1 âˆ’ Î±)LTRD + Î±LARG ) âˆ¥Î¸1 âˆ’ Î¸2 âˆ¥.

B. HDR Instantiation
We specifically instantiate the HDR theoretical framework
for the multi-vehicle cooperative driving task to obtain a
(i)
computable reward function rHDR .
1) Design of Driving Potential Function Ï•(s): The implementation of the TRD mechanism requires a signal that
effectively guides the policy towards the desired state evolution. The foundation is to define a potential function Ï•(s)
that quantifies state superiority or value. For vehicle i, the
value of its state is primarily determined by two dimensions:
longitudinal (efficiency in the driving direction) and lateral
(ability to maintain the target lane).
Based on these two key dimensions, we define the driving
potential function Ï•(s(i) ) for vehicle i as follows, representing
the positional superiority of the vehicle in state s(i) :


2
(xgoal âˆ’x(i) )
exp âˆ’
2Ïƒ 2
.
(20)
Ï•(s(i) ) â‰œ
(i)
Î¶ ytarg âˆ’ y (i) + 1
The numerator in Eq. (20) is a Gaussian-like function that
encourages the vehicle to travel towards the distant longitudinal target position xgoal . The closer the vehicle is to the
target, the larger this value becomes. Parameter Ïƒ controls the
longitudinal decay scale of the potential field. The denominator
is a linear penalty term encouraging the vehicle to stay in
(i)
the target lane ytarg . The greater the deviation of the vehicleâ€™s
lateral position y (i) from the target lane, the lower the potential
function value. Parameter Î¶ controls the intensity of the lateral
deviation penalty.
2) Derivation of Computable TRD Signal: Based on the
potential function above, we design the TRD reward as the
total derivative of the potential function Ï• with respect to time.
Physically, this is equivalent to the power gained by the vehicle
moving within the potential field:
(i)

This proves that âˆ‡Î¸ JARG is LARG -Lipschitz continuous.
Step 3: Prove the Lipschitz continuity of âˆ‡Î¸ JHDR (Î¸). Analyzing the Lipschitz property of the entire HDR gradient:

Î±âˆ¥âˆ‡Î¸ JARG (Î¸1 ) âˆ’ âˆ‡Î¸ JARG (Î¸2 )âˆ¥

Let LHDR = (1 âˆ’ Î±)LTRD + Î±LARG . This proves that
âˆ‡Î¸ JHDR (Î¸) is LHDR -Lipschitz continuous.
According to standard results in optimization theory, for an
objective function J(Î¸), if its gradient âˆ‡Î¸ J(Î¸) is L-Lipschitz
continuous, then for a gradient ascent algorithm Î¸k+1 = Î¸k +
Î·âˆ‡Î¸ J(Î¸k ), selecting a learning rate Î· âˆˆ (0, 2/L) guarantees
limkâ†’âˆž âˆ¥âˆ‡Î¸ J(Î¸k )âˆ¥ = 0, meaning the sequence converges to
a stationary point. Since we have proven that âˆ‡Î¸ JHDR (Î¸) is
LHDR -Lipschitz continuous, any algorithm based on gradient
ascent optimizing JHDR (Î¸) is guaranteed to converge under the
appropriate learning rate condition. The proof is complete.

(19)

rTRD â‰œ
(i)

dÏ•(s(i) (t))
= âˆ‡Ï•(s(i) ) Â· v (i) ,
dt
(i)

(21)

where v (i) = (vx , vy ) is the velocity vector of vehicle i.
(i)
)âˆ’Ï•(st )
From a finite difference perspective, rTRD â‰ˆ Ï•(st+1âˆ†t
.
It directly measures the rate of potential energy gain resulting
from state changes. It serves as a high-quality reward shaping
signal that preserves optimal policy invariance, embodying the
TRD concept of focusing on state evolution trends.
(i)
To calculate rTRD in simulation or implementation, the theoretical dot product must be converted into specific algebraic

expressions. First, combining Eq. (20), we calculate the partial
derivative of Ï•(s(i) ) with respect to position x:
xgoal âˆ’ x
âˆ‚Ï•
;
= Ï•(s(i) ) Â·
âˆ‚x
Ïƒ2

(22)

Next, we calculate the partial derivative of Ï•(s(i) ) with respect
to position y under discrete coordinates:
âˆ‚Ï•
Î¶ Â· sign(âˆ†y (i) )
= Ï•(si ) Â·
,
âˆ‚y
Î¶|âˆ†y (i) | + 1

(23)

(i)

where âˆ†y (i) = y âˆ’ ytarg .
Substituting the two terms âˆ‚Ï•/âˆ‚x and âˆ‚Ï•/âˆ‚y into Eq. (21)
yields the computable discrete form.
3) Discrete Action Gradient: According to the theoretical
definition in Section IV-A2, the ARG signal Raâ€² (s, a) =
âˆ‡a R(s, a) aims to provide gradient guidance along the direction of steepest ascent of the reward function. However,
the action space A in this study is discrete, making direct
computation of continuous derivatives with respect to actions
infeasible. To retain the theoretical properties of ARG in a
discrete decision space, we propose an approximate implementation method based on gradient direction alignment.
From a physical perspective, the gradient of the efficiency
objective Rflow with respect to longitudinal acceleration a,
âˆ‚R
âˆ‚a , typically exhibits a positive correlation (i.e., positive
acceleration improves system efficiency, provided it is safe
and within speed limits). Therefore, although precise gradient
values cannot be calculated, we can capture the direction information of the gradient. We instantiate ARG as an indicator
function to evaluate whether the currently selected discrete
action projects into the ascending half-space of the theoretical
gradient. Specifically, if the agent selects an acceleration action
(a âˆˆ AAC ) or maintains speed while in a high-speed state
(a âˆˆ AMT ), it is considered consistent with the ideal gradient
direction (i.e., âˆ‡a R Â· âƒ—a > 0). Based on this logic, we design
the ARG signal as the following gradient sign indicator:
(
1 if a(i) âˆˆ AAC âˆ¨ (a(i) âˆˆ AMT âˆ§ v (i) â‰¥ vthres ),
(i)
rARG =
0 otherwise.
(24)
where AAC and AMT represent the sets of acceleration and constant speed actions, respectively, and vthres is a preset threshold
for high-efficiency speed (set to 28m/s in this experiment).
The core advantages of this design are: 1) Preservation of
Causality: It discards the magnitude details of the gradient
but retains the crucial â€directional sign,â€ directly informing
the agent which action categories align with the local optimal
gradient. 2) Robustness: Similar to the Sign-SGD method
in optimization theory, guiding solely with gradient signs
often provides better robustness than using high-variance
precise gradient values, especially in noise-filled multi-agent
interaction environments. 3) Computational Efficiency: As
an analytical binary reward, its computational overhead is
negligible.
4) Complete HDR Reward Function: Combining the TRD
reward and ARG reward linearly yields the HDR reward:
(i)

(i)

(i)

rHDR = wTRD rTRD + wARG rARG ,

(25)

where the two weight coefficients satisfy wTRD + wTRD = 1.
Additionally, to complement the HDR reward, we introduce
three auxiliary rewards for optimizing macroscopic traffic
objectives:
1) Traffic Efficiency Reward rflow : Evaluates the macroscopic operating efficiency of the entire traffic system, defined
as the normalized average speed of all vehicles in the scene:
P
(j)
rflow = |N1 | jâˆˆN vvmax . Although the HDR reward already
provides an optimization target for the overall efficiency of
CAVs, CAV behavior also affects HDV driving performance.
Therefore, this term is introduced as a supplement for overall
traffic efficiency.
2) Safety Penalty rsafe : Penalizes dangerous driving behaviors with collision risks. Unlike sparse collision indicator
functions, we design a dense penalty signal based on Timeto-Collision (TTC) to provide more forward-looking safety
guidance. For each vehicle j, its TTC with the preceding
(j)
vehicle is denoted as TTC(j) . When v (j) > vh and H (j) > 0,
H (j)
(j)
its value is (j) (j) ; otherwise, it is âˆž. H is the distance
v

âˆ’vh

between vehicle j and the preceding vehicle, and v (j) and
(j)
vh are their respective velocities. We set a safety penalty
threshold TTCcrit . A safety penalty is triggered only when
the vehicleâ€™s TTC falls below this threshold; higher risk
results in a more severe penalty. The safety
 penalty for a
(j)
1
single vehicle rsafe is defined as âˆ’1 + exp TTC
âˆ’ TTC1(j)
crit
if 0 < TTC(j) < TTCcrit ; it takes the value 1 if a collision
occurs, and 0 otherwise. Based on this, the total safety penalty
is the sum of penalties for all vehicles in the coordination zone,
P
(j)
i.e., rsafe = jâˆˆN rsafe .
3) Frequent Lane Change Penalty rfreq : Penalizes excessively frequent lane-changing behaviors to encourage smooth
driving. Unlike hard constraints using fixed time windows, we
design a continuous penalty function that decays over time:
(j)

(j)

âˆ’Î» Â·t

(j)

rfreq = âˆ’e LC LCâˆ’ , where tLCâˆ’ is the time elapsed since
agent jâ€™s last successful lane change, and Î»LC is the coefficient
controlling the decay rate of the penalty. A maximum penalty
is applied immediately after a lane change, and the penalty
term rapidly approaches 0 through exponential decay over
time.
(i)
Linearly weighting the differential reward term rHDR with
the three metrics measuring global traffic efficiency, safety,
and smoothness yields the complete global reward value rt
for cooperative driving at time t:

X 
1
(i)
(i)
rt =
wHDR rHDR + wfreq rfreq
|NCAV |
(26)
iâˆˆNCAV
+ wflow rflow + wsafe rsafe ,
where wHDR , wflow , wsafe , and wfreq are the weight coefficients
for each term.
5) Visualization Analysis of Reward Surface: To intuitively
demonstrate the reward mechanism of the HDR function, Fig.
5 compares it visually with the state-based reward function
from literature [22]. The scenario is set as a one-way fourlane highway, where the agentâ€™s goal is to reach the rightmost
lane (Lane 0, indicated by the red star) at the end of the
segment. Each subplot is a reward heatmap, with the horizontal

  D   5 H Z D U G  I R U  & K D Q J H  5 L J K W  $ F W L R Q

 
 
 
 

   

   

 

   

  E   5 H Z D U G  I R U  & K D Q J H  / H I W  $ F W L R Q

 
 
 
 

 
 

 

 / D Q H  , Q G H [

  

  

   

   

  F   5 H Z D U G  I R U  * R  6 W U D L J K W  $ F W L R Q

 
 
 
 

 
 

 

  

   

   

 / R Q J L W X G L Q D O  3 R V L W L R Q

Fig. 5. State-based reward function

   

 
 
 
 

 
 
 
 

    
    
    
 

  

   

   

   

  D   5 H Z D U G  I R U  & K D Q J H  5 L J K W  $ F W L R Q
   
   
 

  

   

   

   

   

  E   5 H Z D U G  I R U  & K D Q J H  / H I W  $ F W L R Q
   
   
 

  

   

   

   

   

  F   5 H Z D U G  I R U  * R  6 W U D L J K W  $ F W L R Q
   
   
 

  

   

   

 / R Q J L W X G L Q D O  3 R V L W L R Q

   

   

Fig. 6. HDR reward function

In summary, the HDR framework transforms a static, actionindependent state evaluation function into a dynamic gradient
guidance field tightly coupled with actions. It provides decision signals on what action to execute at every point in space,
thereby resolving the deficiencies of traditional methods.

This section aims to empirically analyze the proposed HDR
mechanism through rigorous simulation experiments. The experiments first validate the effectiveness of the underlying
world model, followed by a comparative performance analysis of HDR against existing reward designs across different
decision-making algorithms (MCTS and MARL).
A. Experimental Scenario and Parameter Configuration

 

   

 
 
 
 

 3 R W H Q W L D O  ) X Q F W L R Q

V. E XPERIMENTAL V ERIFICATION AND A NALYSIS

 
 

 

 
 
 
 

 / D Q H  , Q G H [

axis representing longitudinal position and the vertical axis
representing lane index. The color represents the instantaneous
reward value obtained by executing a specific action at the
corresponding location.
In Fig. 5, the state-based reward function assigns values
based solely on the vehicleâ€™s distance to the target after action
execution. Its deficiency lies in the fact that for any fixed
location, executing three different actionsâ€”â€change right,â€
â€change left,â€ or â€go straightâ€â€”yields identical reward values
provided the resulting position is the same. For instance, in
the case of [22], the reward function penalizes vehicles located
in the two rightmost lanes. Consequently, when a vehicle is
in Lane 3, both â€go straightâ€ and â€change rightâ€ result in
penalization based on the final position, failing to provide the
critical action guidance that â€change rightâ€ should be executed.
This lack of action differentiation in the reward signal is the
root cause of low learning efficiency.
In contrast, Fig. 6 displays the reward surface generated
(i)
by the HDR reward function (primarily rTRD ) proposed in
this paper. The potential function plot at the top shows the
potential field designed by Eq. (20), with energy peaking at
the target point. Based on this potential field, HDR generates
completely distinct reward signals for different actions: (1)
The â€change rightâ€ action, moving the vehicle towards areas
of higher potential, receives high positive rewards in most
regions; (2) Conversely, the â€change leftâ€ action, moving away
from high potential areas, receives generally low rewards; (3)
The â€go straightâ€ action provides moderate rewards depending
on the current lane. Furthermore, it can be observed that
the HDR reward function proposed in this chapter does not
provide explicit lateral guidance at the beginning of the road,
encouraging vehicles to explore a more diverse range of
actions.

 

The simulation environment for this study is built upon
the Flow computational framework and the SUMO simulation
platform. The main scenario consists of a 250m long one-way
four-lane continuous traffic flow. The traffic is composed of a
mix of Connected Automated Vehicles (CAVs) and HumanDriven Vehicles (HDVs). During the stochastic generation of
all vehicles, the intentions of turning left, going straight, and
turning right are each assigned with a probability of 1/3.
Regarding the functionality of the four lanes: the rightmost
Lane 0 is for turning right or going straight; the middle Lanes
1 and 2 are for going straight; and the leftmost Lane 3 is
for turning left or going straight. HDV behaviors are driven

by the improved Krauss model (longitudinal car-following)
and the LC2013 model (lateral lane-changing). The decision
frequency for CAVs is set to 10 Hz. Key parameters for the
unified simulation environment are listed in Table I.
TABLE I
D ETAILED PARAMETER S ETTINGS FOR S IMULATION E XPERIMENTS

Symbol

Value

vHDV,0 , vCAV,0
vHDV,max , vCAV,max
b, tk , Îµ
aCAV,max
vthres
Î³
Ïƒ, Î¶
TTCcrit
Î»LC
wTRD , wARG
wHDR , wflow , wsafe , wfreq

8-12 m/s, Uniform Distribution
30 m/s, 30 m/s
9 m/s2 , 1.1 s, 0.5
3.5 m/s2
28 m/s
0.996
60.0, 1.0
3s
0.75
0.9, 0.1
10.0, 1.0, 2.0, 0.9

B. Comparative Methods and Evaluation Metrics
1) Comparative Methods: To comprehensively evaluate the
universality of the HDR mechanism, we compare it with two
representative reward designs:
â€¢ General Reward (GNR): A traditional reward function
designed based on macroscopic metrics of the current
state (e.g., speed, safety).
â€¢ Centering Reward (CTR): An advanced method that
reduces learning process variance by subtracting a dynamically estimated reward baseline from the immediate
reward.
â€¢ Centered HDR (CTH): Applies the centering method to
the HDR signal.
We apply the HDR, GNR, and CTR/CTH mechanisms to two
mainstream decision-making paradigms: the online planningbased MCTS algorithm and policy learning-based MARL
algorithms (QMIX, MAPPO, and MADDPG).
2) Evaluation Metrics: This paper adopts a unified evaluation metric system to assess algorithms across 5 categories comprising 8 objective indicators: Efficiency Metrics: Instant Traffic Flow Rate (Inst.Flow) and Average
Velocity (Avg.Velo.). Safety Metrics: Average Minimum
Time-to-Collision (TTC) and Average Number of Collisions
(Coll.). Task Metrics: Task Success Rate (Succ.Rate). Stability/Comprehensive Metrics: Average Absolute Jerk (Jerk),
Average Lane Change Interval (LC.Intvl.), and Average Traffic
Score (ATS).
C. Experimental Results and Analysis
1) Validation of Effectiveness on Policy Learning (MARL):
The experiments in this subsection aim to verify whether the
optimization objective provided by HDR can enhance learning
efficiency and final policy performance. MARL algorithms
need to learn a policy network capable of generalizing to

all states through extensive trial-and-error exploration from
reward signals. Therefore, the core of this experiment is to
examine which reward function can provide MARL algorithms
with more stable and efficient gradients, assisting them in
converging faster to higher-performance policies.
We tested three mainstream multi-agent reinforcement
learning algorithms: QMIX, MAPPO, and MADDPG. All
algorithms utilized unified hyperparameter settings.
Fig. 7 shows the training return curves and key performance metrics for the QMIX algorithm under the four reward
functions. Regarding convergence speed and stability, the
normalized ATS curve for HDR exhibits the fastest initial
climb and converges to the highest performance level after
approximately 6 Ã— 105 training steps. In contrast, GNR and
CTR show significantly lower learning efficiency, with slow
convergence speeds and poor final performance.
In terms of policy quality and balance, HDR not only
achieves the best overall score but also demonstrates the
best balance across various metrics in the learned policy.
It achieves the highest average speed while maintaining the
highest task success rate. Furthermore, it is the only method
where the number of collisions converges rapidly to near zero,
demonstrating superior safety.
Consistent with findings in the previous section, the performance of CTH is poor, with collision counts even rebounding
in the later stages of training. This further confirms the conflict
between the centering reward and the HDR mechanism; the
baseline subtraction operation in CTR likely disrupts the key
gradient signals provided by HDR.
Fig. 8 presents the comparative results for the MAPPO algorithm. As a policy gradient method, MAPPOâ€™s performance
is more sensitive to the quality of reward signals and the
variance of gradient estimation. Regarding convergence speed
and stability, the advantage of HDR is even more pronounced
in MAPPO. The ATS curve for HDR is the only one capable
of stably converging to a high performance level. The GNR,
CTR, and CTH reward functions all failed to learn effective
cooperative policies under the MAPPO framework, with their
ATS values and average speeds remaining at extremely low
levels and task success rates being highly unstable.
Regarding policy quality, the policy guided by HDR far
surpasses other methods in safety and driving smoothness.
This result indicates that for policy gradient algorithms, the
sparse or high-variance signals provided by GNR or CTR
are insufficient to support effective exploration in complex
multi-agent game spaces. In contrast, the dense and physically
meaningful differential signals provided by HDR are key to
successful learning.
Fig. 9 shows the results for the MADDPG algorithm. In
terms of convergence speed and stability, compared to QMIX
and MAPPO, MADDPG exhibits greater instability during
training under all reward functions, particularly with severe
fluctuations in task success rates. Nevertheless, HDR still
demonstrates the fastest initial learning speed and the highest
final ATS value.
Regarding policy quality, under the MADDPG framework,
HDR is the only method capable of learning a policy that
balances efficiency and safety. Policies guided by other reward

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

  D   & R P S D U L V R Q  R I  1 R U P D O L ] H G  $ 7 6

 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 

  H   & R P S D U L V R Q  R I  & R O O L V L R Q V

   
   

  

 

  E   & R P S D U L V R Q  R I  9 H O R F L W \

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  
  
 
 

  

 

  F   & R P S D U L V R Q  R I  6 X F F H V V  5 D W H

   

  

  

   

 / &  , Q W Y O   V 

  

   
 P L Q  7 7 &   V 

   

  

 & 7 +

  

  I   & R P S D U L V R Q  R I  0 L Q L P X P  7 7 &

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

  G   & R P S D U L V R Q  R I  / &  , Q W H U Y D O
  

  

 - H U N   P  V3  

   

 * 1 5
   

 9 D U  9 H O R    P2   V2  

   

 & 7 5
 6 X F F  5 D W H    

 $ Y J  9 H O R    P  V 

 1 R U P D O L ] H G  $ 7 6

   

   
 & R O O 

 + ' 5

  

 
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  
 

  

 

  J   & R P S D U L V R Q  R I  9 H O R F L W \  9 D U L D Q F H

  K   & R P S D U L V R Q  R I  - H U N

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

  D   & R P S D U L V R Q  R I  1 R U P D O L ] H G  $ 7 6

   

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 

  H   & R P S D U L V R Q  R I  & R O O L V L R Q V

   
   

  

 

  E   & R P S D U L V R Q  R I  9 H O R F L W \

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

   
  

   
 

 

  F   & R P S D U L V R Q  R I  6 X F F H V V  5 D W H

   

  

   

 / &  , Q W Y O   V 

  

   
 P L Q  7 7 &   V 

 & R O O 

   

  

 & 7 +

  
  
 

  

 

  I   & R P S D U L V R Q  R I  0 L Q L P X P  7 7 &

  G   & R P S D U L V R Q  R I  / &  , Q W H U Y D O
 - H U N   P  V3  

   

 * 1 5
   
 6 X F F  5 D W H    

   

 & 7 5

 9 D U  9 H O R    P2   V2  

   

 

 + ' 5

  
 $ Y J  9 H O R    P  V 

 1 R U P D O L ] H G  $ 7 6

Fig. 7. Performance of the QMIX algorithm guided by different reward functions.

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  
 
 

  

 

  J   & R P S D U L V R Q  R I  9 H O R F L W \  9 D U L D Q F H

  K   & R P S D U L V R Q  R I  - H U N

  

  D   & R P S D U L V R Q  R I  1 R U P D O L ] H G  $ 7 6

   
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  H   & R P S D U L V R Q  R I  & R O O L V L R Q V

  

 
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

   
   

  

 

  E   & R P S D U L V R Q  R I  9 H O R F L W \

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

   
  

    

 

  F   & R P S D U L V R Q  R I  6 X F F H V V  5 D W H

   
 

   

   

 / &  , Q W Y O   V 

  

   
 P L Q  7 7 &   V 

   

  

 & 7 +

  

  I   & R P S D U L V R Q  R I  0 L Q L P X P  7 7 &

  G   & R P S D U L V R Q  R I  / &  , Q W H U Y D O

  
 - H U N   P  V3  

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

 * 1 5
   
 6 X F F  5 D W H    

 

 & 7 5

 9 D U  9 H O R    P2   V2  

 

 

 & R O O 

 + ' 5

  
 $ Y J  9 H O R    P  V 

 1 R U P D O L ] H G  $ 7 6

Fig. 8. Performance of the MAPPO algorithm guided by different reward functions.

  
  
 

 

 
 
 
 
 7 U D L Q L Q J  6 W H S V  Ã—105  

  

  J   & R P S D U L V R Q  R I  9 H O R F L W \  9 D U L D Q F H

Fig. 9. Performance of the MADDPG algorithm guided by different reward functions.

  
 
 

 

  K   & R P S D U L V R Q  R I  - H U N

    

  
  
 
 + ' 5  & 7 5  * 1 5  & 7 +
  H   & R P S D U L V R Q  R I  9 H O R F L W \  9 D U L D Q F H

 
 

 + ' 5  & 7 5  * 1 5  & 7 +
  E   & R P S D U L V R Q  R I  9 H O R F L W \
 - H U N   P  V3 

 9 D U  9 H O R    P2  V2 

 + ' 5  & 7 5  * 1 5  & 7 +
  D   & R P S D U L V R Q  R I  1 R U P D O L ] H G  $ 7 6

 / &  , Q W Y O   V 

    

 

   

 
 
 + ' 5

 & 7 5  * 1 5  & 7 +
  I   & R P S D U L V R Q  R I  - H U N

  
  

 + ' 5  & 7 5  * 1 5  & 7 +
  F   & R P S D U L V R Q  R I  0 L Q L P X P  7 7 &
           
     
     

  
  
  

 + ' 5  & 7 5  * 1 5  & 7 +
  J   & R P S D U L V R Q  R I  6 X F F H V V  5 D W H

  
  
  

 + ' 5  & 7 5  * 1 5  & 7 +
  G   & R P S D U L V R Q  R I  / &  , Q W H U Y D O
     

    
 

    

    

    
 & R O O   

    

 P L Q  7 7 &   V 

    

  

 

    

 6 X F F  5 D W H    

    

 $ Y J  9 H O R    P  V 

 1 R U P D O L ] H G  $ 7 6

    

    
    

                 
 + ' 5  & 7 5  * 1 5  & 7 +
  K   & R P S D U L V R Q  R I  & R O O L V L R Q V

Fig. 10. Comparison of objective metrics for the Monte Carlo Tree Search algorithm guided by different reward functions.

efficiency (average speed), safety (average minimum TTC),
task completion rate, and driving smoothness (lane change
interval, speed variance, jerk). This result strongly proves that
the HDR reward method can provide more precise and efficient
heuristic information for online search algorithms, enabling
them to converge to high-quality cooperative policies more
quickly within limited simulations.
It is worth noting that the MCTS+CTH method performs
poorly on several metrics, particularly being inferior to other
methods in task completion rate and collision frequency. This
is likely due to a principled conflict between the two mechanisms. The core idea of centering reward is to reduce value
estimation variance by subtracting a dynamic reward baseline.
However, the design goal of the Hybrid Differential Reward
is precisely to provide an information-dense, non-zero-mean
signal with clear guidance direction through potential function
differences and action gradients. Applying CTR to HDR,
the centering operation may disrupt the carefully designed
gradient signals of HDR, interfering with its effective heuristic
guidance for the search process, thereby leading to negative
effects.
   
   
 $ Y H U D J H  $ 7 6

functions are not only inefficient but also fail to effectively
resolve collision issues. The collision frequency for CTH
reaches as high as 0.145 collisions/hour, far exceeding other
methods, indicating the worst safety performance.
In summary, whether under value decomposition-based,
policy gradient-based, or Actor-Critic MARL frameworks,
the Hybrid Differential Reward demonstrates consistent and
significant superiority. The dense, high signal-to-noise ratio
gradient signals it provides greatly enhance algorithm convergence speed and stability, enabling agents to learn highquality cooperative policies that achieve an optimal balance
among conflicting objectives such as safety, efficiency, and
task completion rates.
2) Validation of Effectiveness on Online Planning (MCTS):
This subsection selects the Monte Carlo Tree Search algorithm as a representative of online planning methods to
verify whether the HDR signal can provide more effective
decision guidance. MCTS evaluates the long-term value of
actions through forward simulation, and its search efficiency
heavily relies on the quality of reward signals during the
simulation process. Therefore, this experiment compares the
impact of different reward functions on the quality of cooperative policies derived by the MCTS algorithm under varying
computational budgets.
Specifically, this section compares the performance of
the MCTS algorithm guided by four reward functions:
MCTS+HDR, MCTS+CTR, MCTS+GNR, and MCTS+CTH.
The experiment is conducted in a specific interaction scenario with a fixed number of agents, including several CAVs
and HDVs. This setting is used to evaluate the algorithmâ€™s
performance in solving a defined, non-continuous flow game
problem.
Fig. 10 displays the performance comparison of MCTS
policies guided by the four reward functions across various
objective metrics under a fixed search budget. The results
show that the MCTS+HDR method demonstrates significant
advantages across almost all key performance dimensions.
Specifically, HDR not only achieves the highest ATS value
but also outperforms MCTS+GNR and MCTS+CTR in traffic

   
   
   
   

 + ' 5
 & 7 5
   

   
   
 1 X P E H U  R I  5 R O O R X W

 * 1 5
 & 7 +
   

Fig. 11. Requirements of different reward functions for the number of
searches.

Fig. 11 further analyzes the dependency of the algorithmâ€™s

comprehensive performance on the number of searches. The
experimental results indicate that the MCTS+HDR method
consistently maintains optimal performance across all tested
search budget levels. More importantly, the marginal growth
of the HDR curve has tended to flatten, indicating that it
approaches convergence at a lower number of searches. In
contrast, the performance curves of other methods still show
significant upward trends. This suggests that to achieve the
same performance level as HDR, other reward functions would
require a computational budget far exceeding that of HDR. In
conclusion, HDR not only improves the final quality of the
policy but also significantly enhances the search efficiency of
the MCTS algorithm.

Î± based on environmental complexity or learning stages to
further enhance model adaptability and learning efficiency.
2) Generalization and Cross-Domain Application: Future
work may extend the HDR framework to other complex multiagent systems requiring high-frequency, continuous decisionmaking, such as multi-robot collaboration and UAV swarms,
to further validate its theoretical and practical value.
3) Integration with Causal Inference: It is worthwhile
to investigate the integration of the Action Gradient Reward
(ARG) in HDR with causal inference methods in reinforcement learning (e.g., Counterfactual Reasoning) to provide
more interpretable and robust assessments of action utility.

VI. C ONCLUSION

[1] P. Ghorai, A. Eskandarian, Y.-K. Kim, and G. Mehr, â€œState estimation
and motion prediction of vehicles and vulnerable road users for cooperative autonomous driving: A survey,â€ IEEE Transactions on Intelligent
Transportation Systems, vol. 23, no. 10, pp. 16 983â€“17 002, 10 2022.
[2] Z. E. A. Kherroubi, S. Aknine, and R. Bacha, â€œNovel decision-making
strategy for connected and autonomous vehicles in highway on-ramp
merging,â€ IEEE Transactions on Intelligent Transportation Systems,
vol. 23, no. 8, pp. 12 490â€“12 502, 2022.
[3] L. Crosato, K. Tian, H. P. H. Shum, E. S. L. Ho, Y. Wang, and C. Wei,
â€œSocial interaction-aware dynamical models and decision-making for
autonomous vehicles,â€ Advanced Intelligent Systems, vol. 6, no. 3, p.
2300575, 2024.
[4] X. Lu, H. Zhao, B. Gao, W. Chen, and H. Chen, â€œDecision-making
method of autonomous vehicles in urban environments considering
traffic laws,â€ IEEE Transactions on Intelligent Transportation Systems,
vol. 23, no. 11, pp. 21 641â€“21 652, 2022.
[5] X. Tang, B. Huang, T. Liu, and X. Lin, â€œHighway decision-making
and motion planning for autonomous driving via soft actor-critic,â€ IEEE
Transactions on Vehicular Technology, vol. 71, no. 5, pp. 4706â€“4717,
2022.
[6] A. Y. Ng, D. Harada, and S. J. Russell, â€œPolicy invariance under
reward transformations: Theory and application to reward shaping,â€
in Proceedings of the Sixteenth International Conference on Machine
Learning. San Francisco, CA, USA: Morgan Kaufmann Publishers
Inc., 1999, p. 278â€“287.
[7] C. Li, T. Wang, C. Wu, Q. Zhao, J. Yang, and C. Zhang, â€œCelebrating
diversity in shared multi-agent reinforcement learning,â€ in Advances in
Neural Information Processing Systems, vol. 34. Curran Associates,
Inc., 2021, pp. 3991â€“4002.
[8] H. Guan, Q. Meng, and X. Chen, â€œDynamic lane management for
emerging mixed traffic with semi-autonomous vehicles,â€ Transportation
Research Part C: Emerging Technologies, vol. 170, p. 104914, 2025.
[9] S. Hossain, J. Lu, H. Bai, and W. Sheng, â€œA multi-vehicle, multi-lane
cooperative driving framework in mixed traffic with driving advisory,â€
IEEE Transactions on Vehicular Technology, vol. , no. , pp. 1â€“15, 2025.
[10] P. Hang, C. Lv, C. Huang, J. Cai, Z. Hu, and Y. Xing, â€œAn integrated
framework of decision making and motion planning for autonomous
vehicles considering social behaviors,â€ IEEE Transactions on Vehicular
Technology, vol. 69, no. 12, pp. 14 458â€“14 469, 2020.
[11] D. Chen, M. R. Hajidavalloo, Z. Li, K. Chen, Y. Wang, L. Jiang,
and Y. Wang, â€œDeep multi-agent reinforcement learning for highway
on-ramp merging in mixed traffic,â€ IEEE Transactions on Intelligent
Transportation Systems, vol. 24, no. 11, pp. 11 623â€“11 638, 2023.
[12] R. Valiente, B. Toghi, R. Pedarsani, and Y. P. Fallah, â€œRobustness and
adaptability of reinforcement learning-based cooperative autonomous
driving in mixed-autonomy traffic,â€ IEEE Open Journal of Intelligent
Transportation Systems, vol. 3, no. , pp. 397â€“410, 2022.
[13] X. Lu, H. Zhao, C. Li, B. Gao, and H. Chen, â€œA game-theoretic
approach on conflict resolution of autonomous vehicles at unsignalized
intersections,â€ IEEE Transactions on Intelligent Transportation Systems,
vol. 24, no. 11, pp. 12 535â€“12 548, 2023.
[14] P. Hang, C. Huang, Z. Hu, Y. Xing, and C. Lv, â€œDecision making of
connected automated vehicles at an unsignalized roundabout considering personalized driving behaviours,â€ IEEE Transactions on Vehicular
Technology, vol. 70, no. 5, pp. 4051â€“4064, 2021.
[15] M. J. Mataric, â€œReward functions for accelerated learning,â€ in Machine
Learning Proceedings 1994. San Francisco (CA): Morgan Kaufmann,
1994, pp. 181â€“189.

R EFERENCES
This paper addresses a critical challenge in multi-vehicle cooperative driving characterized by high-frequency continuous
decision-making: the problem of vanishing reward differences.
To resolve this, we propose a novel and efficient Hybrid
Differential Reward (HDR) mechanism.
First, through theoretical analysis and visualization, we formally elucidate the issue of low signal-to-noise ratio (SNR) in
policy gradients associated with traditional state-based reward
functions under high-frequency decision-making. We attribute
this issue to the quasi-steady nature of traffic states and the
physical proximity of actions. Second, we propose the HDR
framework, which innovatively integrates two complementary
differential signals:
1) Temporal Difference Reward (TRD): Defined based
on a potential function, it ensures optimal policy invariance and provides the correct direction for long-term
optimization.
2) Action Gradient Reward (ARG): By directly measuring the marginal utility of actions, it significantly
improves the SNR of local policy gradients, resolving
the issue of low action differentiation.
Subsequently, we formulate the multi-vehicle cooperative driving task as a Multi-Agent Partially Observable Markov Game
(POMDPG) with a time-varying agent set. We also provide
a complete instantiation and a computable derivation scheme
for the HDR mechanism within this complex scenario.
Finally, through extensive validation in a unified simulation environment using both online planning (MCTS) and
offline learning (QMIX, MAPPO, MADDPG) algorithms, we
demonstrate that the HDR mechanism achieves the fastest
convergence speed and the highest final performance across all
tested frameworks. The results indicate that HDR effectively
guides agents to learn cooperative driving policies that strike
an optimal balance among safety, efficiency, and driving
smoothness.
The HDR mechanism proposed in this paper provides a
general and effective paradigm for addressing reward design
challenges in high-frequency continuous control. Building on
this, future work can be explored in the following directions:
1) Adaptive Hyperparameter Adjustment: The mixing
weight Î± in this paper is currently a manually set hyperparameter. Future research could explore dynamically adjusting

[16] J. Liu, P. Hang, X. Na, C. Huang, and J. Sun, â€œCooperative decisionmaking for cavs at unsignalized intersections: A marl approach with
attention and hierarchical game priors,â€ IEEE Transactions on Intelligent
Transportation Systems, vol. 26, no. 1, pp. 443â€“456, 1 2025.
[17] M. GrzesÌ, â€œReward shaping in episodic reinforcement learning,â€ in Proceedings of the 16th Conference on Autonomous Agents and MultiAgent
Systems.
Richland, SC: International Foundation for Autonomous
Agents and Multiagent Systems, 2017, p. 565â€“573.
[18] A. Gupta, A. Pacchiano, Y. Zhai, S. Kakade, and S. Levine, â€œUnpacking
reward shaping: Understanding the benefits of reward engineering on
sample complexity,â€ in Advances in Neural Information Processing
Systems, vol. 35. Curran Associates, Inc., 2022, pp. 15 281â€“15 295.
[19] A. Naik, Y. Wan, M. Tomar, and R. S. Sutton, â€œReward centering,â€ in
Reinforcement Learning Conference, 2024.
[20] F. Kalogiannis, I. Panageas, and E.-V. Vlatakis-Gkaragkounis, â€œTowards
convergence to nash equilibria in two-team zero-sum games,â€ in The
Eleventh International Conference on Learning Representations, 2023.
[21] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
â€œDeterministic policy gradient algorithms,â€ in International conference
on machine learning. Pmlr, 2014, pp. 387â€“395.
[22] S. Chen, J. Dong, P. Ha, Y. Li, and S. Labi, â€œGraph neural network
and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,â€ Computer-Aided Civil and Infrastructure
Engineering, vol. 36, no. 7, pp. 838â€“857, 2021.

