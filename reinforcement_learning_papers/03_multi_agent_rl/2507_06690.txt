IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2025

1

Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs

arXiv:2507.06690v1 [cs.RO] 9 Jul 2025

Guobin Zhu1,2 , Rui Zhou1 , Wenkang Ji2 , Hongyin Zhang3 , Donglin Wang3 and Shiyu Zhao2

Abstract—Multi-task multi-agent reinforcement learning (M
T-MARL) has recently gained attention for its potential to
enhance MARL’s adaptability across multiple tasks. However, it
is challenging for existing multi-task learning methods to handle
complex problems, as they are unable to handle unrelated tasks
and possess limited knowledge transfer capabilities. In this paper,
we propose a hierarchical approach that efficiently addresses
these challenges. The high-level module utilizes a skill graph,
while the low-level module employs a standard MARL algorithm.
Our approach offers two contributions. First, we consider the
MT-MARL problem in the context of unrelated tasks, expanding
the scope of MTRL. Second, the skill graph is used as the
upper layer of the standard hierarchical approach, with training
independent of the lower layer, effectively handling unrelated
tasks and enhancing knowledge transfer capabilities. Extensive
experiments are conducted to validate these advantages and
demonstrate that the proposed method outperforms the latest
hierarchical MAPPO algorithms. Videos and code are available
at https://github.com/WindyLab/MT-MARL-SG

Marker

Green trajectory
Red trajectory
Future trend

Red leader

Green team

LED light
Red team

Green leader

Omni-directional
wheel
Radius = 0.15 m

Fig. 1: Real-world experiment. The red and green robot teams, moving in
flocking, engage in combat. After defeating the red team, the green team
continues flocking motion.

to distill knowledge from multiple expert policies into a single
network through supervised learning, enabling the integrated
network to adapt to diverse tasks [13], [14].
Although MTRL has demonstrated effectiveness by leverI. I NTRODUCTION
aging inter-task knowledge [15], [16], it still has two notable
ULTI-AGENT reinforcement learning (MARL) is limitations when applied to complex tasks. First, MTRL
promising for decision-making in multi-robot systems assumes that tasks share common attributes. If tasks are
and has achieved notable success in areas like games [1], [2] unrelated (e.g. adversarial and cooperative), knowledge transfer
and robotic control [3]–[5]. However, its performance tends becomes infeasible, rendering conventional MTRL methods
to be task-specific, requiring retraining for new tasks due to ineffective. Second, current MTRL rarely accounts for multiweak generalization [6]. Therefore, it is important to study how agent settings. Multi-agent systems involve inter-agent interMARL can adapt flexibly to multiple tasks.
actions and often operate under partial observability, making
In recent years, numerous studies on multi-task reinforcement MT-MARL fundamentally different from a direct extension of
learning (MTRL) have emerged [7]–[9]. These works leverage single-agent MTRL.
inter-task correlations to enhance performance across tasks.
Coincidentally, hierarchical reinforcement learning (HRL)
Existing approaches can be broadly categorized into two types. offers a promising approach. HRL is designed to decompose
The first focuses on learning multiple tasks simultaneously, complex, long-term tasks into a hierarchy of subtasks, with the
where shared attributes/knowledge (e.g. data, model parameters) high-level policy deciding which low-level skill to execute [17]–
among tasks are utilized to accelerate learning [10]. The second [20]. In multi-agent multi-task settings, HRL presents two
emphasizes sequential task learning, where previously acquired main advantages. First, it can be naturally integrated with
knowledge is retained and reused to facilitate learning when MTRL [11], [20]. In a hierarchical framework, the high-level
encountering related new tasks [11], [12]. Both approaches policy’s selection of low-level skills effectively serves as a
inherently involve knowledge transfer. A common strategy is mechanism for transferring knowledge of modular skills. Since
the tasks underlying these skills may be either related or
Manuscript received: March, 22, 2025; Revised xxx, xxx; Accepted xxx,
xxx. This paper was recommended for publication by xxx upon evaluation of unrelated, the hierarchical approach is well-suited for multi-task
the Associate Editor and Reviewers’ comments. This work was supported by scenarios. Second, HRL simplifies complex multi-agent settings
the STI 2030-Major Projects (No.2022ZD0208804), National Natural Science
by breaking problems down in a layered, step-by-step manner,
Foundation of China (No.62473017, No.62473320). (Corresponding author:
thereby reducing both environmental and task complexity [21],
Shiyu Zhao.)
1 School of Automation Science and Electrical Engineering, Beihang
[22].
University, Beijing, China.
However, existing HRL approaches still have two limitations.
2 WINDY Lab, Department of Artificial Intelligence, Westlake University,
The
first is that standard HRL cannot select multiple lowHangzhou, China.
3 MiLAB Lab, Department of Artificial Intelligence, Westlake University,
level strategies concurrently; in other words, HRL’s capacity
Hangzhou, China.
for knowledge transfer is limited. The second is that most
E-mail: {zhugb, zhr}@buaa.edu.cn, {jiwenkang, zhanghongyin, wangHRL studies consider only several dozen agents [23], and as
donglin, zhaoshiyu}@westlake. edu.cn.
Digital Object Identifier (DOI): see top of this page.
this number increases, the curse of dimensionality becomes a
Index Terms—Multi-robot systems, Multi-agent reinforcement
learning, Multi-task reinforcement learning,

M

2

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2025

High-level
(a) Graph construction

Representation vectors
with same dimensionality

Task

(b) Graph utilization
Environment
…

…

Feature
vectors

Skill1

Encoders

Skill2

TransH

Skill embedding
…

…

…

…

…

Sample
library

…

…
…

…

TransH

Relation

Skill graph

Skill

Skilln

Select
/Combination
/Further training

Skill3

Skill1

Skilln

High score

Skill2
Low score

Fig. 2: Method overview. (a) is the construction of the skill graph and (b) is the utilization of the skill graph. In each embedding layer and representation
vector, each small square represents a numerical value.
observation

significant issue. In light of these limitations, there is a need
for more general MT-MARL methods.
This paper proposes an effective hierarchical approach
to address multi-task multi-agent problems. We considered
the unrelated task settings, a scenario that has been rarely
considered in previous work. On the high level, our approach
uses a skill graph [24], inspired by the concept of knowledge
graph (KG) [25]. KG conveniently describes entities and
their relationships and enables effective knowledge transfer
when queried. Thus, our method consists of two steps. First,
we treat environments, tasks, and skills in RL as entities
and use knowledge graph embedding (KGE) [26] to learn
representations of these entities and their relationships, thereby
constructing a skill graph. Second, when queried by new
environments or tasks, all skills in the graph are evaluated via a
scoring mechanism, and the final skill is selected, combined, or
further refined from the high-scoring ones. Such a skill graph
can handle unrelated tasks and enhance effective knowledge
transfer. On the low level, our approach employs the MADDPG
algorithm [27] with a local critic instead of a centralized one.
This modification better aligns with robots’ local perception
and supports large-scale tasks. Finally, both simulation and
real-world experiments verify the effectiveness of the proposed
method.
In summary, the main contributions are as follows: 1) Our
approach can handle multiple unrelated tasks, including both
adversarial and cooperative tasks. This unrelated case has
been rarely considered in prior work since no transferable
knowledge exists. The proposed approach can handle the
unrelated case due to its hierarchical structure combined
with a skill graph. 2) Our approach enables skill selection,
combination, and further learning through a scoring mechanism,
offering effective knowledge transfer compared to standard
HRL. The scoring mechanism inherently offers a more robust
and flexible approach to standard HRL since it always produces
a list of scores, meaning that high-scoring skills that rank at
the top are always obtained. 3) We conducted experiments on
a real robotic swarm platform to validate the effectiveness of
the proposed method.

II. P RELIMINARIES
A. Knowledge Graph

…

TransH

…
…

…

…
…

…

Skill3

Relation embedding

Apply

Trained encoders

…

…

Sample
construction

?

Combine

training

Low-leve
rollouts

action
sample
……

A KG is a multi-relational graph composed of entities (nodes)
and relations (different types of edges) [28]. Mathematically,
skill1
skill
a KG is defined as G = (E, R, T ), where
E is the set of n
all entities contained in the KG. R represents
theLearning
set of and
allCollection
(a) Skill
relations. T is the set of factual triples, where a standard binary
fact is a triple (h, r, b), h and b mean the head entity and the
tail entity, and r is the relation between the head entity and
the tail entity.
Such a structure conveniently stores and links diverse
knowledge with a unified specification. When encountering a
new query, the KG can rapidly retrieve the target entity, while
its associated entities can also be easily identified. Therefore,
the KG facilitates efficient knowledge transfer [29].
B. Knowledge Graph Embedding
Although the KG provides a convenient representation of
structured knowledge, its symbolic nature poses operational
challenges. KGE addresses this issue by mapping entities and
relations onto a continuous vector space (i.e., a representation
space), thereby facilitating operations while preserving the
inherent structure of the KG.
The typical KGE process consists of three key steps [28].
First, entities and relations are represented, typically as vectors
or matrices. Second, a scoring function is defined to assess
the plausibility of a given fact triple, where observed facts
in the KG generally receive higher scores than unobserved
ones. Third, the representations of entities and relations are
learned by solving an optimization problem that maximizes
the plausibility of all facts. In essence, the KG is considered
constructed once the representations of entities and relations
have been learned.
Several KGE algorithms have been proposed, such as
TransE [30] and TransH [26]. In this study, we employ the
TransH model because it supports one-to-many, many-to-one,
and many-to-many relationships. For example, in a one-to-many
relationship, a single head entity may correspond to multiple

ZHU et al.: MULTI-TASK MULTI-AGENT REINFORCEMENT LEARNING VIA SKILL GRAPHS

tail entities. In the context of RL, this corresponds to training
multiple skills within a single environment.
III. M ETHODOLOGY
As illustrated in Fig. 2, the proposed method comprises two
main components. The first constructs the skill graph using
knowledge graph (KG) construction methods with the TransH
model. The second involves utilizing the skill graph, where
the TransH model scores all skills in the graph to infer the
most suitable one.
A. Graph Construction

3

Algorithm 1 Construction and Utilization of Skill Graph
1: Construction:
2: Initialize encoders of e and t, embeddings of s and r
3: Construct positive (e/t, re/t→s , s), negative (e/t, re/t→s ,
′

′

e /t ), (e/t, rt/e→s , s) and soft (e2 /t2 , re/t→s , s1 )
4: for iteration = 1 to M do
5:
Randomly sample a mini-batch from the library
6:
Map e and t to representation space using encoders
7:
Update encoder and embeddings by minimizing L
8: end for
9: Utilization:
10: Map enew , tnew to representation space using encoders
11: Combine the representation vectors of enew , tnew , r, s
12: Compute S = S(t,rt→s ,s) · S(e,re→s ,s) for each combination
13: Infer the most suitable skill based on scores

As described in Section II-B, the construction of the skill
graph consists of three steps. The first step involves representing
entities and relations. In the skill graph, entities include the
environment (e), task (t), and skill (s), while relations denote manner, positive, negative, and soft samples can form a sample
the correspondence from the environment to skill (re→s ) and library.
from task to skill (rt→s ). To map both entities and relations
With the samples prepared, the next step is to learn
into a unified representation vector space, there are two sub- representations. We use the following loss function:
steps (see Fig. 2(a)). First, we use feature vectors (different
L = (Sposi − 1)2 + (Snega − 0)2 + ReLU (Ssoft − 1 + δ), (1)
from representation vectors) to describe e and t. Then, we use
an environment encoder and a task encoder (MLP networks where, δ = P k (p − p )/ P k measures the similarity
j,2
j j j,1
j j
with Leaky-ReLU activation) to map their respective feature between entities
1 and 2, where kj represents the weight
vectors into the representation space. Second, we employ two coefficient and p denotes the j-th attribute of the feature
j
embedding layers (initialized orthogonally) to represent skills vector. We mainly adjust k to differentiate between various
j
and relations, respectively. Each embedding layer is a matrix types of entities. The subscript of S indicates the sample type.
where rows correspond to the number of relations/skills, and As can be seen from loss (1), we hope that the positive sample
columns to the dimensions of their representation vectors. In score is 1, the negative sample score is 0, and the soft sample
this way, entities (e/t/s) and relations (r) are represented within score is no more than 1 − δ. The value of S depends on
the same vector space.
the representation vectors of entities and relations, which are
The second step involves defining the scoring function. obtained through the encoder and embedding layers. Therefore,
We adopt the TransH model as our scoring function, which minimizing L is to optimize the parameters of encoders and
is expressed as S = exp(−λ∥(h − wr T hwr ) + dr − (b − embedding layers. The result of constructing the skill graph is
wr T bwr )∥), where λ is a normal constant. h and b denote to obtain these networks.
the head entity and tail entity representation vectors, respectively. wr and dr denote the relation-specific hyperplane and
translation vector, respectively. The scoring function measures B. Graph Utilization
the plausibility of a triple (h, r, b).
With the constructed skill graph, we can identify the most
The third step involves learning the representation vectors. suitable skill when facing a new environment enew and task
This is a supervised learning process that comprises two phases: tnew . The process involves three steps (see Fig. 2(b)). First,
sample construction and representation learning. In the termi- map the feature vectors of enew and tnew to the representation
nology of KGE, samples are categorized as positive, negative, space using their trained encoders. Second, combine their
and soft samples [26]. Positive samples correspond to correct representation vectors with the relation and skill representation
facts. During the skill collection phase (see Section IV), we vectors from the graph. Third, score each combination using
know which skills are trained under specific environments and S = S(t,rt→s ,s) · S(e,re→s ,s) and TransH model to generate a
tasks, allowing us to construct factual triples (e, re→s , s) and sequence of scores. The resulting scores indicate how well
(t, rt→s , s); these triples serve as positive samples. Negative each skill in the graph matches the query enew and tnew .
samples represent incorrect facts, which may arise from errors
If the score is close to 1 (αhigh < S ≤ 1), it indicates
′
′
in the entities (e.g. (e, re→s , e ), (t, rt→s , t )) or the relations that the query pair has been previously encountered; thus,
(e.g. (e, rt→s , s), (t, re→s , s)). Soft samples refer to triples the highest-scoring skill is directly applied. If the score is
with varying degrees of plausibility: positive samples have a in the intermediate range (αlow < S ≤ αhigh ), it suggests a
plausibility of 1, negative samples 0, and soft samples fall reasonable relationship between the skill andPthe query pair,
between 0 and 1. For example, for two similar environments and the robot’s action is derived through a = j wj aj , where
e1 and e2 , if e1 corresponds to skill s1 , then (e1 , re→s , s1 ) is j is the skill index for this score range, wj is the P
normalized
a positive sample and (e2 , re→s , s1 ) is a soft sample. Similarly, score for skill j. Here, normalized scores mean j wj = 1.
(t1 , rt→s , s1 ) is positive and (t2 , rt→s , s1 ) is soft. In this If all skill scores are low (0 ≤ S ≤ αlow ), it indicates that the

4

fa

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2025

v

fa

rperc

(a)
Active force

fb

v

(b)

fb

Passive force

v
vk

fb

Unobserved robots
Observed robots

v

(c)

(d)

v

Non-cooperative
relationship

vi

θ II

Fixed boundary
L

Robot k

Periodic boundary

ratta

θI
Robot i

(e)

Fig. 3: Motion environment illustration. (a)(b) show the active and passive force, (c)(d) shows the fixed and periodic boundaries, and (e) shows the relative
motion relationship, where θI = arccos(pki · vi /(∥pki ∥ · ∥vi ∥)),θII = arccos(pki · vk /(∥pki ∥ · ∥vk ∥)), The light green and red sectors represent
θI ≤ kI π, θII ≤ kII π and we choose kI = kII = 0.4.

query pair falls outside the skill distribution in the graph. In B. Adversarial Task
this case, the highest-scoring
skill is further optimized using
The adversarial
task involves two groups of robots, where
(b)
(a)
v
v
RLv and thenfapplied to theunobserved
query pair.robots
f
each
side
aims
to
defeat the other. The task ends when
a
f b skill graph, b
Throughout active
the construction
and utilization
observed
robots of the
one
group
force
passive
forceis eliminated. The adversarial task setup mirrors
it does not require task-related dependencies and can acquire many biological group behaviors [32], with the attack process
fixed
boundary
the
most
suitable skills through selection, combination, or illustrated in Fig. 3(e). From the green side’s perspective, a
non-cooperative
(c)
further
learning,
thereby enhancing knowledge transfer.
v
red robot is considered attacked if the following conditions are
robot k This relationship
approach offers an effective solution for multi-task
learning. met: 1) the green robot is behind the red robot (the light red
vk
The complete process is outlined in Algorithm 1.
area); 2) the green robot’s velocity is approximately directed
fb
θ II
L
vi towardsr the red robot (the light green area); and 3) the distance
atta
between them is less than the attack radius ratta .
IV. S KILL
C OLLECTION AND F EATURE C ONSTRUCTON
periodic
boundary
rperc
In our setup, each robot has a health value hp. When a
θI
(d)
This v
section presents skill collection and feature vector
robot
by an enemy (non-cooperating robot), its hp
construction through specific environments and task examples. robot i is attacked
(e)
decreases
by
△h,
with a maximum of no enemies attacking
It first introduces the environment, then describes adversarial
simultaneously.
Additionally,
if a robot is not under attack, its
and cooperative tasks—highlighting the proposed method’s
hp
increases
by
0.1△h
up
to
the initial health value hpmax . A
ability to handle unrelated tasks—and finally outlines the
robot
is
considered
dead
when
hp ≤ 0.
MARL algorithm and feature vector construction.
1) Reward: The reward can be expressed as ra = rsurv +rsitu ,
with default values rsurv = rsitu = 0. Here, rsurv represents
A. Environment Description
the survival reward. If a robot eliminates an enemy, rsurv =
The environment is shown in Fig. 3, where the robot is repre- ksurv ; if it is eliminated, rsurv = −ksurv . rsitu represents the
sented as a disk. Robots of the same/different colors represent situational reward if a robot meets the conditions to attack
cooperative/non-cooperative relationships. Taking the green an enemy, rsitu = ksitu . Both ksurv and ksitu are positive
side as an example, consider N homogeneous robots, defined constants.
2) Action: The action is a two-dimensional vector with
as A = {1, . . . , N }. Each robot’s state is x = [pT , vT ]T ,
where, p and v are the position and velocity, respectively. The components along the x and y axes. This vector indicates the
robot’s movement is driven by the active and passive force (see active force fa exerted on the robot.
3) Observation: Based on the setup, each robot’s observation
Fig. 3(a)(b)). The active force, fa , is a self-generated force,
which is the output of the actor network. The passive force, fb , vector is designed to include its own state and the relative states
is an elastic force following Hooke’s Law. Thus, the robot’s of teammates and enemies. We set the maximum number of
dynamics is ṗi = vi , v̇i = (fa + fb )/mi , i ∈ A, where teammates and enemies to nh /2. For example, the robot i’s obT
T
T
T
T
T
mi is the mass of robot i, and the velocity is bounded by servation is oi = [xi , xji,1 , . . . , xji,nh /2 , xki,1 . . . , xki,nh /2 ] ,
where,
x
=
x
−
x
,
j
∈
N
,
x
=
x
−
x
,
k
∈
O
,
N
/O
ji
j
i
i
ki
k
i
i
i
i
vmin ≤ ∥vi ∥ ≤ vmax .
The robot’s perception range is a circular area with radius are the sets of robot i’s teammates/enemies.
rperc . In the observation model, perception depends on both
Euclidean and topological distances [31], where the topological C. Cooperative Task
component limits the number of perceivable individuals. We
The cooperative task involves a single group of robots,
combine these factors and set a threshold of nh = 6 [31].
for which we select flocking—a behavior widely observed
All robots move within a bounded region of length L. in nature [32], [33]. Flocking is typically governed by three
We consider two types of boundaries: a fixed boundary (see heuristic rules: cohesion, separation, and alignment. A robot is
Fig. 3(c)), where robots cannot pass through and will be attracted to or repelled by neighbors when their distance is too
repelled upon contact, and a periodic boundary (see Fig. 3(d)), large or too small, respectively, and its velocity tends to align
where robots can exit the environment from one side and with the average of its neighbors. Ultimately, the inter-robot
reappear on the opposite side at the same velocity.
distance stabilizes at a certain value.

ZHU et al.: MULTI-TASK MULTI-AGENT REINFORCEMENT LEARNING VIA SKILL GRAPHS

1) Reward: Similar to the three flocking rules, we designed three reward criteria to achieve the same effect.
rattr = −kattr (dij − dref ) if dij > dref , and 0 otherwise.
rrepl = −krepl (drefP− dij ) if dij < dref , and 0 otherwise.
v
ralig = −kalig ∥ N1i j∈Ni ( ∥vjj ∥ − ∥vvii ∥ )∥, where kattr , krepl ,
kalig are positive constants, dij = ∥pi − pj ∥ is the distance
between i and j, dref is the final stable value, and Ni is the
number of robot i’s neighbors. Therefore, the total reward is
expressed as rf = rattr + rrepl + ralig .
2) Action: Flocking involves only the robot’s movement;
therefore, the action is the same as in the adversarial task.
3) Observation: Each robot’s observation vector is designed
to include its own state and the relative states of its neighbors.
For example, the observation of robot i is denoted as oi =
[xTi , xTji,1 , . . . , xTji,nh ]T .
D. MARL Algorithm
This paper uses a local-critic version of MADDPG (localMADDPG) to train the low-level skills. MADDPG employs an
actor-critic architecture [27]. In this paper, both the actor and
critic are MLPs, with Leaky-ReLU in hidden layers, Tanh for
the actor’s output, and no activation for the critic’s output. As
the robots are homogeneous, a single actor and critic can be
shared among cooperative robots. Specifically, the adversarial
task involves two critics and actors, while flocking involves
one of each.
The centralized critic in [27] is Q(x1 , . . . , xN , a1 , . . . , aN ),
where, ai = µ(oi ), i ∈ A and µ represents the actor. We
modified it to Q(oi , ai ); thus, the critic only inputs the robot
i’s observation and action. This mimics the local perception
in biological swarms, while the local critic enables efficient
handling of large-scale tasks. For more details on MADDPG,
please refer to [27].

5

considered alternative baselines, but most existing methods are
not directly applicable to the unrelated tasks in our study. Thus,
they do not offer a fair basis for comparison.
A. Low-level Skill Collection
1) Experimental setup: The number of robots for both teams
in the adversarial task and for the flocking task is set to N = 50.
In the adversarial task, the red team adopts an auxiliary strategy
fa = kp (pc − p) + kv (vc − v), where pc and vc are the
position and velocity of the nearest green robot. The green
team adopts the local-MADDPG strategy. In the flocking task,
to prevent swarm splitting due to local perception, several
robots are designated as leaders with fixed movements (see
Fig. 4(d)(f)). These leaders are otherwise identical to regular
agents. The task parameters are set as follows: rperc = 3 m,
mi = 1 kg, vmax = 1 m/s, vmin = 0, L = 4 ∼ 6 m, △h = 1,
hpmax = 80, no = 3, ratta = 0.3 m, dref = 0.4 ∼ 0.7 m,
ksitu = 1, ksurv = 5, kattr = 1 ∼ 2.5, krepl = 15, kalig = 2.
Hyperparameters for local-MADDPG are listed in Table I.
2) Results analysis: The experimental results are shown
in Fig. 4. In the adversarial scenario, the green team’s
performance depends on whether its local-MADDPG policy is
trained. When untrained, the red team—guided by an auxiliary
strategy—successfully maneuvers behind and eliminates it (see
Fig. 4(a)). Conversely, a trained green team can outmaneuver
and defeat the red team. (see Fig. 4(b)). In the flocking task (see
Fig. 4(c)-(f)), the robot team maintains nearly equal inter-agent
distances once a stable formation is reached. Notably, under
periodic boundary conditions, Reynolds’ three rules are well
preserved. These results demonstrate that the local-MADDPG
algorithm successfully collects low-level skills.
B. Skill Graph Application

1) Experimental setup: Suppose we have 8 flocking subtasks
with feature vectors as follows: floc 1: (1,0,0.4,2),
E. Feature Construction
floc 2: (1,0,0.8,2), floc 3: (1,0,0.4,3), floc 4: (1,0,0.8,3),
In this paper, feature vectors serve as numerical reprefloc 5: (1,0,0.4,4), floc 6: (1,0,0.8,4), floc 7: (1,0,0.4,5), floc 8:
sentations of entities (i.e., environments or tasks), captur(1,0,0.8,5). Additionally, there are 8 adversarial subtasks with
ing their attributes. An environment can be represented as
feature vectors: adve 1: (1,0,1,2,0.3), adve 2: (1,0,1,3,0.3),
e = (y, L), where y indicates the boundary type, with y = 0
adve 3: (1,0,1,4,0.3), adve 4: (1,0,1,5,0.3), adve 5: (1,0,1,2,
representing a periodic boundary and y = 1 representing a
0.4), adve 6: (1,0,1,3,0.4), adve 7: (1,0,1,4,0.4), adve 8: (1,0,
fixed boundary. An adversarial task can be represented as
1,5,0.4). Moreover, there are 2 environments: fixed: (1,6) and
t = (vmax , vmin , △h, no , ratta ), and a flocking task can be
periodic: (0,6). This yields a total of 32 skills (i.e., (8+8)×
represented as t = (vmax , vmin , dref , rperc ). Once these feature
2 = 32). Using these tasks, environments, and skills, we can
vectors are constructed, they are encoded as representation
construct samples and subsequently construct a skill graph.
vectors through environment or task encoders. By combining
The training parameters for the skill graph are set as follows:
the skill and relation embedding layers, the skill graph can
representation vector size = 96, hid size = 256, hid layers = 3,
then be constructed.
M = 500, batch size = 256, k1,t = 0, k2,t = 0, k3,t = 0,
k4,t = 3, k5,t = 1, k1,e = 0.95, k2,e = 0.05, λ = 3,
V. S IMULATION E XPERIMENTS
αhigh = 0.95, αlow = 0.85.
This section delineates three components: the collection
To illustrate the application of the skill graph, we designed
of low-level skills, the application of the skill graph, and a a scenario as shown in Fig. 5(a)-(c) where two groups of 50
direct comparative analysis between the skill graph and HRL. robots each move along predetermined paths. When the groups
The analysis aims to elucidate the advantages of integrating do not encounter each other, they perform a flocking task with
hierarchical structures with the skill graph, thereby contrasting dref = 0.4 m (stage 1). Upon encountering—defined as a robot
this approach with the standard HRL approach. Moreover, from one group coming within rperc of the opposing group’s
HRL adopts the latest MAPPO algorithm [34]. We carefully center—they engage in an adversarial task (stage 2). After one

6

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2025

(a)

Fixed
boundary

(b)
Attacking

Surrounded
t = 1.5 s

t=7s

Periodic boundary

(c)

t = 1 s t = 60 s

Fixed boundary
t = 17.5 s

t = 12.5 s

t = 1.5 s

t=6s

(d)

t = 0.5 s

t = 29 s

t = 12.5 s

Fixed boundary

Leader

=
d d=
0.6m
ref

t = 9.5 s

(e)

t = 0.5 s

t = 24 s (f)

t = 1.6 s

Leader

t = 34 s

Fig. 4: Low-level skill collection. (a) and (b) illustrate an adversarial task where the green team uses local-MADDPG, and the red team employs an auxiliary
strategy with kp = 1 and kv = 2. In (a), local-MADDPG is untrained, leading to the green team’s defeat. In (b), after training, the green team prevails over
the red team. (c)(d), (e)(f) illustrate the flocking behavior with local-MADDPG strategy under periodic and fixed boundaries, respectively.

TABLE I: Hyperparameters for local-MADDPG and MAPPO
local-MADDPG

MAPPO [34]

C. Comparative Experiments

1) Experimental setup: The setup of the skill graph is
described in Section V-B; here, we focus on the HRL setup. It
consists of two layers, both using MAPPO. The low-level
eps
1200/1800 critic lr 1e-3/1e-3 - 2400/2000 5e-4/1e-3
MAPPO shares the same observation, action, and reward
eps len 200/200 actor lr 1e-4/1e-4 - 350/200 5e-4/1e-3
structure as in Section IV, with hyperparameters listed in
0.99/0.99
buf size 5e5/5e5 gamma 0.99/0.99 - 1e5/1e5 batch
512/512 explr rate 0.1/0.1
- 2048/2048 ppo epoch 20/20
Table I. The high-level MAPPO is set as follows. Observation:
hid size 128/64
noise
0.8/0.8
- 256/128 clip para 0.1/0.2
For the green side, the observation vector is designed as
- 3/3
gae gamm 0.95/0.95
hid layers 2/3
soft-upd 0.01
og = [xTrg , ng , nr ]T , where, xrg = xr − xg represents the
relative position between team centers, and ng and nr denote
the number of robots on the green and red teams, respectively.
group is eliminated, the remaining group resumes the flocking Action: The action space is discrete, matching the number of
task (stage 3) with a different dref = 0.6 m. In such a scenario, available low-level skills; the actor outputs the index of the
the skill graph needs to infer (decide) the most suitable skill selected skill. Reward: A -1 reward is given if the selected
skill does not match the expected one. All other high-level
for the robot team in each stage.
2) Results analysis: The skill graph’s application is illustrated MAPPO hyperparameters follow those used in the flocking
in Fig. 5. In stage 1 (see Fig. 5(a)), both robot teams perform the task (Table I), except for the number of episodes (set to 200)
flocking task with dref = 0.4 m. The input environment + task and the episode length (set to 300).
To implement the comparison, we define two metrics. The
query is (1,6) + (1,0,0.4,3), and the output (see Fig. 5(d)) shows
that the skill “floc 3 fixed” achieves the highest score (0.97), first is the decision success rate, denoted as ρsucc = nsucc /
exceeding the threshold αhigh = 0.95, and is therefore selected ntotal , where, nsucc and ntotal represent the numbers of
for stage 1. Similarly, the query is (1,6) + (1,0,1,3,0.3) in stage successful and total decisions, respectively. we aim for the
2, and the highest-scoring skill “adve 2 fixed” is chosen (see method to achieve as high a success rate as possible. The
Fig. 5(e)). In stage 3, the query is (1,6) + (1,0,0.6,3). Although second is generalization which is a qualitative metric. During
this query does not match any fundamental skill in the graph, testing, we will vary certain parameters and observe changes
the graph provides the two most relevant skills, “floc 4 fixed” in ρsucc . Ideally, an effective method should exhibit stable
and “floc 3 fixed” (see Fig. 5(f)). Then, the robot’s action performance despite these parameter variations.
is a weighted combination of these two skills. This process
It is crucial to clarify that our comparison involves two
clearly demonstrates the skill graph’s selection and combination schemes: Scheme 1 (a high-level skill graph + a low-level
capabilities.
local-MADDPG) and Scheme 2 (a high-level MAPPO + a lowFig. 5(g)-(j) further demonstrates how the skill graph handles level MAPPO). Despite differences in low-level algorithms,
unfamiliar queries—not entirely unseen, but rather cases that this comparison validly demonstrates the advantages of the
may resemble previously encountered ones. For instance, when skill graph for two reasons: First, the defined metrics do not
the query input is (1,6) + (1,0,1,3), the output’s highest score depend on the low-level algorithm and exclusively evaluate
is below αlow (see Fig. 5(g)). In this case, the highest-scoring high-level performance. Second, both frameworks are modular;
skill, “floc 4 fixed”, is further trained and used for the query although replacing Scheme 2’s low-level component with the
pair. This approach results in a skill that requires less training local-MADDPG is feasible, such a hybrid configuration would
time and higher sampling efficiency compared to training from be less natural.
scratch (see Fig. 5(h)), enabling robots to quickly adapt to
2) Results analysis: Table II summarizes detailed ρsucc
unfamiliar environments and tasks. In summary, the skill graph metric, where ”dim” indicates the number of low-level skills,
demonstrates excellent knowledge transfer capabilities.
and ”base” represents the case with identical training and
Adve/Floc

Adve/Floc Adve/Floc

Adve/Floc

ZHU et al.: MULTI-TASK MULTI-AGENT REINFORCEMENT LEARNING VIA SKILL GRAPHS

(a) Stage1

=
d d=
0.4 m
ref
(b) Stage3

(c) Stage2
t = 21 s

Leader
t = 17 s
t = 43 s
=
0.6 m
d d=
ref

t=2s
(d)

0.97

7

t = 25 s

t = 24 s
(e)

0.97

(g)

t = 26.7 s

(f)

0.89 0.88

(j)

(i)

(h)

floc_4_fixed, 0.83

adve_6_fixed, 0.81

Fig. 5: Skill graph application. In (a)(b), the red lines on the green side indicate that the inter-robot distance is approximately dref . In (c), the green side uses
local-MADDPG strategy, while the red side employs an auxiliary strategy with kp = 1 and kv = 2. (d)(e)(f) show the top 16 skill scores for the three stages,
with red highlighting the skills that can be directly reused. (g) displays the top 11 skills for the query pair (1,6)+(1,0,1,3), while (h) illustrates the reward
curves for further training of the highest-scoring skills versus training from scratch. Similarly, (i)(j) correspond to (1,6)+(1,0,1,3,0.8).

TABLE II: ρsucc of high-level MAPPO
case
dim
dim=2
dim=4
dim=6
dim=8
dim=10
dim=12

(a) t = 6 s

(b) t = 12 s
0.7 m

base

(c) t = 16 s

0.7 m

init num*1.1 init num*0.9 leader v*1.4 leader v*0.6
Leader

95.2(6.8)% 93.2(8.2)%
94.6(7.4)% 93.1(8.2)%
94.4(7.2)% 93.0(8.3)%
93.7(7.7)% 91.4(8.8)%
93.2(7.5)% 90.1(7.8)%
92.5(7.6)% 90.7(7.9)%

93.9(6.9)%
90.9(9.1)%
89.7(8.0)%
88.3(8.8)%
88.0(9.2)%
86.8(9.5)%

93.8(7.9)%
93.2(8.7)%
92.9(8.5)%
92.6(8.7)%
92.1(8.6)%
90.7(8.8)%

95.1(6.3)%
94.3(7.7)%
94.1(7.1)%
93.1(8.4)%
93.0(7.7)%
92.1(8.0)%

local-MADDPG
policy
(f) t = 40 s

(e) t = 30 s

Leader

(d) t = 22 s

1m

Auxiliary
policy

kp = 1

kv = 2

Fig. 6: Snapshots of the real-world experiment. (a)(b), (c)(d), and (e)(f)
correspond to stage 1, stage 2, and stage 3, respectively.

testing parameters. Cols 2-5 (different training and testing
parameters) represent variations in the initial number (“init
num”) and leader speed (“leader v”), which aim to assess
the generalization ability of the MAPPO strategy. The table
demonstrates two key findings: First, ρsucc decreases as the
number of available skills increases. Second, ρsucc is sensitive
to parameter changes, indicating poor generalization.

VI. R EAL - WORLD E XPERIMENTS

The real experimental setup is shown in Fig. 1 and Fig. 6,
where six omnibot robots [35] are divided into red and green
teams, moving within a 5 × 5 m area. The experiment follows
the procedure illustrated in Fig. 5(a)-(c). The robot structure
Many parameters can highlight hierarchical MAPPO’s weak is depicted in Fig. 1, and its state is tracked using a motion
generalization, but we report only two due to space limits. Its capture system. The robot skill library is mostly consistent
underperformance stems from two reasons. First, the high-level with that mentioned in Section V-B, with three differences.
strategy depends on low-level skills during training, creating a First, the flocking’s dref is adjusted from 0.4/0.8 m to 0.7/1.3
tight coupling where many variations in low-level parameters m. Second, the boundary length is reduced from 6 m to 5 m.
can impact high-level decision-making. Second, the high-level Third, training episodes for flocking are increased from 1200
strategy only selects or rejects low-level skills, which is too to 3000. We set dref to 0.7 m for stage 1 and 1 m for stage 3.
rigid and overlooks the inherent rationality of those skills. All other parameters match those in the Simulation.
In contrast, the skill graph has three advantages. First, its
In stages 1, 2, and 3, the query pairs are (1,5) + (1,0,0.7,3),
construction is independent of low-level skills, ensuring that (1,5) + (1,0,1,3,0.3), and (1,5) + (1,0,1,3), respectively, and the
changes in low-level parameters do not affect it, leading to good skill graph outputs skills “floc 3 fixed”, “adve 2 fixed”, and
generalization. Second, the skill graph selects skills based on “floc 4 fixed” and “floc 3 fixed” for robot team. As shown in
their scores without requiring a perfect score, (i.e. 1), resulting Fig. 6, both the red and green teams first form flocking with
in ρsucc = 100%. Third, the skill graph can combine or further dref = 0.7 m, then enter the adversarial phase, where the green
optimize high-scoring skills, a capability that hierarchical team defeats the red team. Finally, the green team transitions
MAPPO lacks, i.e. the skill graph exhibits better knowledge to a flocking with dref = 1 m. The entire process demonstrates
transfer than hierarchical MAPPO.
the practical feasibility of the method presented in this paper.

8

IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2025

VII. C ONCLUSION
This paper proposed an effective hierarchical approach
for MT-MARL, with a skill graph at the upper layer. The
skill graph demonstrated three advantages: it did not require
related attributes among low-level tasks, offered superior
knowledge transfer compared to standard hierarchical methods,
and was trained independently of low-level skills with good
generalization. Simulations and real-world experiments were
conducted to validate the advantages of the proposed approach.
It is worth noting that the fact that the high-level skill
graph does not require lower-level tasks to be related does
not imply that it can transfer knowledge in entirely unseen
scenarios—these are two separate issues. When the latter occurs,
the scores of all skills in the library tend to be low due to
the absence of transferable knowledge, and new skills must be
learned from scratch. This also represents a potential direction
for future research.
R EFERENCES
[1] O. Vinyals, et al., “Grandmaster level in starcraft ii using multi-agent
reinforcement learning,” nature, vol. 575, no. 7782, pp. 350–354, 2019.
[2] Z.-J. Pang, R.-Z. Liu, Z.-Y. Meng, Y. Zhang, Y. Yu, and T. Lu, “On
reinforcement learning for full-length game of starcraft,” in Proceedings
of the AAAI Conference on Artificial Intelligence, 2019, pp. 4691–4698.
[3] T. Fan, P. Long, W. Liu, and J. Pan, “Distributed multi-robot collision
avoidance via deep reinforcement learning for navigation in complex
scenarios,” The International Journal of Robotics Research, vol. 39, no. 7,
pp. 856–892, 2020.
[4] C. De Souza, R. Newbury, A. Cosgun, P. Castillo, B. Vidolov, and
D. Kulić, “Decentralized multi-agent pursuit using deep reinforcement
learning,” IEEE Robotics and Automation Letters, vol. 6, no. 3, pp.
4552–4559, 2021.
[5] D. Johnson, G. Chen, and Y. Lu, “Multi-agent reinforcement learning
for real-time dynamic production scheduling in a robot assembly cell,”
IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 7684–7691,
2022.
[6] F. Zhang, C. Jia, Y.-C. Li, L. Yuan, Y. Yu, and Z. Zhang, “Discovering
generalizable multi-agent coordination skills from multi-task offline
data,” in Proceedings of the International Conference on Learning
Representations, 2023, pp. 1–24.
[7] Y. Zhang and Q. Yang, “A survey on multi-task learning,” IEEE
transactions on knowledge and data engineering, vol. 34, no. 12, pp.
5586–5609, 2021.
[8] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian, “Deep
decentralized multi-task multi-agent reinforcement learning under partial
observability,” in Proceedings of the International Conference on Machine
Learning, 2017, pp. 2681–2690.
[9] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt, and
H. Van Hasselt, “Multi-task deep reinforcement learning with popart,”
in Proceedings of the AAAI Conference on Artificial Intelligence, 2019,
pp. 3796–3803.
[10] B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient
descent for multi-task learning,” in Proceedings of the Advances in Neural
Information Processing Systems, 2021, pp. 18 878–18 890.
[11] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, and S. Mannor, “A deep
hierarchical approach to lifelong learning in minecraft,” in Proceedings
of the AAAI conference on Artificial Intelligence, 2017, pp. 1553–1561.
[12] B. Liu, L. Wang, and M. Liu, “Lifelong federated reinforcement learning:
A learning architecture for navigation in cloud robotic systems,” IEEE
Robotics and Automation Letters, vol. 4, no. 4, pp. 4555–4562, 2019.
[13] E. Parisotto, J. L. Ba, and R. Salakhutdinov, “Actor-mimic: Deep multitask
and transfer reinforcement learning,” in Proceedings of the International
Conference on Learning Representations, 2016, pp. 1–16.
[14] A. A. Rusu, et al., “Policy distillation,” arXiv preprint arXiv:1511.06295,
2015.
[15] Z. Xu, K. Wu, Z. Che, J. Tang, and J. Ye, “Knowledge transfer in multitask deep reinforcement learning for continuous control,” in Proceedings
of the Advances in Neural Information Processing Systems, 2020, pp.
15 146–15 155.

[16] L. Sun, H. Zhang, W. Xu, and M. Tomizuka, “PaCo: Parametercompositional multi-task reinforcement learning,” in Proceedings of the
Advances in Neural Information Processing Systems, 2022, pp. 21 495–
21 507.
[17] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek, “Hierarchical reinforcement learning: A comprehensive survey,” ACM Computing Surveys
(CSUR), vol. 54, no. 5, pp. 1–35, 2021.
[18] R. Gieselmann and F. T. Pokorny, “Planning-augmented hierarchical
reinforcement learning,” IEEE Robotics and Automation Letters, vol. 6,
no. 3, pp. 5097–5104, 2021.
[19] S. Christen, L. Jendele, E. Aksan, and O. Hilliges, “Learning functionally
decomposed hierarchies for continuous control tasks with path planning,”
IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 3623–3630,
2021.
[20] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement
learning with policy sketches,” in Proceedings of the International
Conference on Machine Learning, 2017, pp. 166–175.
[21] K. Lee, S. Kim, and J. Choi, “Adaptive and explainable deployment
of navigation skills via hierarchical deep reinforcement learning,” in
Proceedings of the IEEE International Conference on Robotics and
Automation (ICRA), 2023, pp. 1673–1679.
[22] R. Wang, D. Zhao, A. Gupte, and B.-C. Min, “Initial task allocation
in multi-human multi-robot teams: An attention-enhanced hierarchical
reinforcement learning approach,” IEEE Robotics and Automation Letters,
vol. 9, no. 4, pp. 3451–3458, 2024.
[23] K. Zhang, Z. Yang, and T. Başar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” Handbook of
reinforcement learning and control, pp. 321–384, 2021.
[24] H. Zhang, et al., “RSG: Fast learning adaptive skills for quadruped robots
by skill graph,” arXiv preprint arXiv:2311.06015, 2023.
[25] S. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip, “A survey on
knowledge graphs: Representation, acquisition, and applications,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 33, no. 2,
pp. 494–514, 2021.
[26] Z. Wang, J. Zhang, J. Feng, and Z. Chen, “Knowledge graph embedding
by translating on hyperplanes,” in Proceedings of the AAAI Conference
on Artificial Intelligence, 2014, pp. 1112–1119.
[27] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environments,”
in Proceedings of the Advances in Neural Information Processing Systems,
2017, pp. 6379–6390.
[28] Q. Wang, Z. Mao, B. Wang, and L. Guo, “Knowledge graph embedding: A
survey of approaches and applications,” IEEE Transactions on Knowledge
and Data Engineering, vol. 29, no. 12, pp. 2724–2743, 2017.
[29] J. Xu, M. He, and Y. Jiang, “A novel framework of knowledge transfer
system for construction projects based on knowledge graph and transfer
learning,” Expert Systems with Applications, vol. 199, p. 116964, 2022.
[30] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko,
“Translating embeddings for modeling multi-relational data,” in Proceedings of the Advances in Neural Information Processing Systems, 2013,
pp. 2787–2795.
[31] J. Li, L. Li, and S. Zhao, “Predator–prey survival pressure is sufficient
to evolve swarming behaviors,” New Journal of Physics, vol. 25, no. 9,
p. 092001, 2023.
[32] M. Dorigo, G. Theraulaz, and V. Trianni, “Swarm robotics: Past, present,
and future [point of view],” Proceedings of the IEEE, vol. 109, no. 7,
pp. 1152–1165, 2021.
[33] G. Vásárhelyi, C. Virágh, G. Somorjai, T. Nepusz, A. E. Eiben, and
T. Vicsek, “Optimized flocking of autonomous drones in confined
environments,” Science Robotics, vol. 3, no. 20, p. eaat3536, 2018.
[34] C. Yu, et al., “The surprising effectiveness of PPO in cooperative multiagent games,” in Proceedings of the Advances in Neural Information
Processing Systems, 2022, pp. 24 611–24 624.
[35] Z. Ma, et al., “Omnibot: A scalable vision-based robot swarm platform,”
in Proceedings of the IEEE International Conference on Control &
Automation (ICCA), 2024, pp. 975–980.

