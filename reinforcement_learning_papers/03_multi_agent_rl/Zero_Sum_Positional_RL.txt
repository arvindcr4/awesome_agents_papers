Zero-Sum Positional Differential Games as a Framework for
Robust Reinforcement Learning: Deep Q-Learning Approach

Anton Plaksin 1 Vitaly Kalev 2

arXiv:2405.02044v1 [cs.LG] 3 May 2024

Abstract

(see also Robust Adversarial RL (Pinto et al., 2017)), in
which such uncertainty or disturbances are interpreted as
actions of a second adversarial agent, and thus the problem is reduced to seeking the agents’ policies robust to any
opponent’s actions.

Robust Reinforcement Learning (RRL) is
a promising Reinforcement Learning (RL)
paradigm aimed at training robust to uncertainty
or disturbances models, making them more efficient for real-world applications. Following this
paradigm, uncertainty or disturbances are interpreted as actions of a second adversarial agent,
and thus, the problem is reduced to seeking the
agents’ policies robust to any opponent’s actions.
This paper is the first to propose considering the
RRL problems within the positional differential
game theory, which helps us to obtain theoretically justified intuition to develop a centralized Qlearning approach. Namely, we prove that under
Isaacs’s condition (sufficiently general for realworld dynamical systems), the same Q-function
can be utilized as an approximate solution of both
minimax and maximin Bellman equations. Based
on these results, we present the Isaacs Deep QNetwork algorithms and demonstrate their superiority compared to other baseline RRL and MultiAgent RL algorithms in various environments.

The fundamental difficulty of RRL problems, as a particular
(zero-sum) case of Multi-Agent RL (MARL) problems, is
the non-stationarity of the environment from the point of
each agent’s view (see, e.g., (Busoniu et al., 2008; Zhang
et al., 2021)). Often, this leads to the failure of the decentralized (independent) learning (Lanctot et al., 2017) and
motivates the design of a centralized approach allowing
agents to exchange information during the learning (see,
e.g., (Lowe et al., 2017; Foerster et al., 2018)). The exchange can utilize a shared memory, a shared policy, but
more often, a shared Q-function, which requires an adequate theory within which such a function is well-defined
and exists.
According to the Markov game theory (see, e.g., (Shapley,
1953; Bertsekas, 1976; Van Der Wal, 1980)), any zero-sum
game has a value (Nash equilibrium) which can be used to
construct such a shared Q-function (Littman, 1994). However, even in the simplest examples of Markov games (e.g.,
paper-rock-scissors), optimal policies may not be pure (deterministic) and, therefore, can provide only the expected
value of a payoff. That is, the optimal agent can guarantee a
certain payoff only on average across runs but not for every
run. Thus, the Markov game theory may be inappropriate
if, according to the problem statement (for example, in the
case of developing expensive or safe and stable control systems), it is required to seek robust policies guaranteeing a
deterministic payoff.

1. Introduction
In the last ten years, neural network models trained by Reinforcement Learning (RL) algorithms (Sutton & Barto,
2018) have demonstrated outstanding performance in various game and physics simulators (see, e.g., (Mnih et al.,
2015; Silver et al., 2017; OpenAI, 2018; Vinyals et al., 2019;
Liu et al., 2022)). However, the usage of such models for
practical problems is still limited due to their instability to
uncertainty or disturbances occurring in the real world. One
promising paradigm to overcome these difficulties is Robust
Reinforcement Learning (RRL) (Morimoto & Doya, 2000)

In this paper, we make the following contributions to the
RRL research:
• We are the first to propose considering the RRL
paradigm within the framework of the positional differential game theory (Krasovskii & Subbotin, 1987;
Subbotin, 1995). The main difference between this
theory and the theory of Markov games is continuous dynamics, which makes it possible to study pure
optimal policies providing deterministic payoff.

1

Yandex, Moscow, Russia 2 IMM UB RAS, Yekaterinburg, Russia.
Correspondence to: Anton Plaksin
<a.r.plaksin@gmail.com>.
Proceedings of the 41 st International Conference on Machine
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).

1

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

• We prove that under Isaacs’s condition (6) (sufficiently
general and easily verifiable for real-world dynamical
systems), the same Q-function can be utilized as an
approximate solution of both minimax and maximin
Bellman equations. We also indicate a condition when
this Q-function can be decomposed (Sunehag et al.,
2017). Thus, we present a theoretically justified intuition for developing a centralized Q-learning.

Ding et al., 2022), where the main idea was to solve minimax Bellman equations in the class of mixed policies. The
presented in this paper IDQN and DIDQN algorithms of
solving zero-sum differential games seek solutions of such
equations in pure policies, significantly reducing running
time and improving performance (see the comparison with
NashDQN in Experiments).
The first formalization of RRL problems within the framework of differential games was proposed in fundamental
paper (Morimoto & Doya, 2000), where a particular class of
games called H∞ -control (Başar & Bernhard, 1995; Zhou
et al., 1996) was considered. This approach was further
developed in (Al-Tamimi et al., 2007; Han et al., 2019;
Zhai et al., 2022). We do not consider the algorithms from
these papers in our experiments since the H∞ -control theory represents another perspective on optimality compared
to the classical differential game theory (Isaacs, 1965). In
particular, the problem formalization for the agents is not
symmetric, and therefore, the concept of Nash equilibrium
is not usually considered. Nevertheless, we note that paper (Al-Tamimi et al., 2007) established the existence of
shared Q-functions for linear differential games, which can
be interpreted as a particular case of our theoretical results
(Theorem 4.1).

• Taking this intuition into account, we present the Isaacs
Deep Q-Networks (IDQN) and Decomposed Isaacs
Deep Q-Networks (DIDQN) algorithms as natural extensions of the single-agent DQN algorithm proposed
in (Mnih et al., 2015) for training of neural networks to
efficiently solve continuous high-dimensional RL tasks.
The experiment results demonstrate the superiority of
the presented algorithms compared to RRL and MARL
baselines.
• We offer to test RRL algorithms on new environments originating from differential game examples
with known accurate solutions. Such environments
can serve as additional reliable tests in future research
of RRL algorithms.
• We consider a framework for thoroughly evaluating the
robustness of trained policies based on using various
RL algorithms with various parameters (see Fig. 2).
We hope this framework will become the new standard
in research of continuous RRL problems as well as
continuous MARL problems in the zero-sum setting.

Developing RL algorithms to solve pursuit-evasion differential games within the classical theory was studied in (Wang
et al., 2019; Jiang et al., 2020; Xu et al., 2022; Selvakumara & Bakolas, 2022). However, such a class of games
seems the most complex for directly applying RL since the
agent has too little chance of reaching the aim (capture)
in the exploration stage and may not have enough informative samples for further learning. To overcome these
difficulties, these papers suggest modifying reward functions, which naturally increase the number of informative
samples but, in fact, change the problem statement. Finitehorizon differential games considered in our paper do not
have uninformative samples and thus seem more suitable
for applying RL algorithms (Harmon et al., 1996).

2. Related Work
Recently, the RRL paradigm, which in some ways inherits
ideas of the previously developed approaches to robustness
(Iyengar, 2005; Nilim & Ghaoui, 2005; Wiesemann et al.,
2012), was successfully used as an effective tool for finding
a policy robust to various environment’s physical parameters
(such as mass, friction, etc.) (see, e.g., (Pinto et al., 2017;
Abdullah et al., 2019; Tessler et al., 2019; Zhai et al., 2022)).
This paper does not study such properties of policies, focusing only on their robustness to dynamical uncertainty or
disturbances interpreted as an adversary agent.

The closest study to our paper is (Li et al., 2022) developing DQN for solving infinite-horizon reach-avoid zero-sum
differential games. The fundamental difference of the algorithm proposed in (Li et al., 2022) is that the second player
knows the first agent’s next action in advance. Note that this
approach does not require a shared Q-function, and therefore we also test it in our experiments (see CounterDQN) to
assess the significance of the shared Q-function usage for
algorithms’ performance.

As mentioned above, RRL problems can be naturally considered as non-cooperative case of Markov games (Pinto
et al., 2017) and solved by the corresponding algorithms of
decentralized (Tampuu et al., 2017; Gleave et al., 2019) or
centralized (Lowe et al., 2017; Li et al., 2019; Kamalaruban
et al., 2020) learning. We consider some of these algorithms
as baselines for the comparative analysis in our experiments
(see Experiments).

There are extensive studies (see, e.g., (Patsko, 1996; Bardi
& Dolcetta, 1997; Cardaliaguet et al., 1999; Kumkov et al.,
2005; Kamneva, 2019; Lukoyanov & Gomoyunov, 2019))
on numerical methods for solving finite-horizon zero-sum
differential games. We use some results from these papers

DQN extension to zero-sum Markov games was carried
out in (Fan et al., 2020; Zhu & Zhao, 2020; Phillips, 2021;
2

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

for additional verification of algorithms’ performance in
our experiments. Note that these methods are mainly capable of solving low-dimensional differential games and
cannot be scaled due to the curse of dimensionality. Thus,
the research presented in this paper contributes not only to
algorithms effectively solving RRL problems, but also to
the field of heuristic numerical methods for solving complex
large-dimensional zero-sum differential games.

By a policy of the first agent, we mean an arbitrary function
πu : [0, T ] × Rn 7→ U. Then the pair {πu , ∆} defines a
control law that forms the piecewise constant (and therefore,
measurable) function u(·) according to the step-by-step rule
u(t) = πu (ti , x(ti )),

t ∈ [ti , ti+1 ),

i = 0, 1, . . . , m.

This law, together with a function v(·), uniquely determines
the quality index value J (2). The guaranteed result of the
policy πu and the optimal guaranteed result of the first agent
are defined as

3. Positional Differential Games
Recent studies consider RRL problems within the framework of zero-sum Markov games in pure or mixed policies. In the case of pure policies, it is known (e.g., paperrock-scissors) that Markov games may not have a value
(Nash equilibrium), which conceptually prevents the development of centralized learning algorithms based on shared
Q-functions. In the case of mixed policies, such formalization may also be inappropriate if, according to the problem
statement (for example, in the case of developing expensive
or safe control systems), it is required to seek robust policies
guaranteeing a deterministic payoff value. In this section,
we describe the framework of the positional differential
games, which allows us, on the one hand, to consider the
pure agents’ policies and deterministic payoffs and, on the
other hand, to obtain the fact of the existence of a value in a
reasonably general case.

Vuπu (τ, w) = lim

sup

sup J,

δ→0+ ∆ : d(∆)≤δ v(·)

Vu (τ, w) = inf Vuπu (τ, w).

(4)

πu

Similarly, for the second agent, we consider a policy
πv : [0, T ] × Rn 7→ V, control law {πv , ∆} that forms actions v(t) and define the guaranteed result of πv and the
optimal guaranteed result as
Vvπv (τ, w) = lim

inf

inf J,

δ→0+ ∆ : d(∆)≤δ u(·)

Vv (τ, w) = sup Vvπv (τ, w).

(5)

πv

The fundamental fact (presented as Theorem 12.3 in (Subbotin, 1995)) of the positional differential game theory on
which we rely to obtain our theoretical results (Theorem 4.1
below) is as follows: if the functions f and f 0 satisfy
Isaacs’s condition (or the saddle point condition in a small
game (Krasovskii & Subbotin, 1987) in other terminology)

min max ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v)
u∈U v∈V

(6)
= max min ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v)

Let (τ, w) ∈ [0, T ) × Rn . Consider a finite-horizon zerosum differential game described by the differential equation
d
x(t) = f (t, x(t), u(t), v(t)), t ∈ [τ, T ],
(1)
dt
with the initial condition x(τ ) = w and the quality index
Z T
J = σ(x(T )) +
f 0 (t, x(t), u(t), v(t))dt.
(2)

v∈V u∈U

τ

Here t is a time variable; x(t) ∈ Rn is a vector of the
motion; u(t) ∈ U and v(t) ∈ V are control actions of
the first and the second agents, respectively; U ⊂ Rk and
V ⊂ Rl are compact sets; the function σ(x), x ∈ Rn
is continuous; the functions f (t, x, u, v) and f 0 (t, x, u, v)
satisfy conditions (see Appendix A for details) sufficient to
ensure the existence and uniqueness of a motion x(·) for
each pair of Lebesgue measurable functions (u(·), v(·)).

for any t ∈ [0, T ] and x, s ∈ Rn , then, differential game (1),
(2) has a value (Nash equilibrium):
V (τ, w) = Vu (τ, w) = Vv (τ, w),

(τ, w) ∈ [0, T ] × Rn .

The first agent, utilizing the actions u(t), tends to minimize
J (see (2)), while the second agent aims to maximize J
utilizing the actions v(t).

Note that the important consequence from this fact is that
if the equality (6) holds, then any additional knowledge
for agents, for example, about the history of the motion
x(ξ), ξ ∈ [0, t], or opponent current actions, does not improve their optimal guaranteed results. Thus, the control
laws {πu , ∆} and {πv , ∆} are sufficient to solve zero-sum
differential games optimally.

Following the positional approach to the differential games
(Krasovskii & Subbotin, 1987; Subbotin, 1995), let us define
the following mathematical constructions. Denote

∆ = ti : τ = t0 < t1 < . . . < tm+1 = T ,
(3)
d(∆) = max ∆ti , ∆ti = ti+1 − ti , .

Also, note that, in the models of real-world dynamical systems, condition (6) is met quite often. For example, it is
met in the case when the agents’ actions u and v can be
decomposed in functions f and f 0 (see (10)), or, in particular, when one part of the coordinates of f depends on u and
another part depends on v.

i=0,1,...,m

3

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

4. Shared Q-function for Approximate
Bellman Equations

paper-rock-scissors game), which conceptually prevents the
development of centralized learning algorithms based on
shared Q-functions in the general case of Markov games (7).
Moreover, even if the Markov game is a time discretization
(7) of some differential game (1), (2), we may get

To solve differential games by RL algorithms, first of all, it is
necessary to discretize the games in time. In this section, we
describe such a discretization, introduce the corresponding
game-theoretic constructions, discuss their connection with
the Markov games, and present the main theoretical results.

∆
Q∆
u (ti , x, u, v) ↛ Qv (ti , x, u, v) as d(∆) → 0.

We describe an example of such a case in Appendix C.
Nevertheless, if differential game (1), (2) satisfies the Isaacs
condition (6), then we obtain following main result.

Let us fix a partition ∆ (3). For each (ti , x) ∈ ∆ × Rn ,
i ̸= m + 1, consider the discrete-time differential game
(Fleming, 1961; Friedman, 1971)

Theorem 4.1. Let Isaacs’s condition (6) holds. Let the
value function V (τ, w) be continuously differentiable at
every (τ, w) ∈ [0, T ] × Rn . Then the following statements
are valid:

xj+1 = xj + ∆tj f (tj , xj , uj , vj ), j = i, i + 1, . . . , m,
J ∆ = σ(xm+1 ) +

m
X

∆tj f 0 (tj , xj , uj , vj ),

(7)

a) For every compact set D ⊂ Rn and ε > 0, there exists
δ > 0 with the following property. For every partition ∆
satisfying d(∆) ≤ δ, there exists a continuous function
Q∆ (ti , x, u, v), (ti , x, u, v) ∈ ∆ × Rn × U × V, such that

j=i

where xi = x and uj ∈ U, vj ∈ V. Note that this game
can be formalized (see Appendix B for details) as a Markov
game (S ∆ , U, V, P ∆ , R∆ , γ), where S ∆ is a state space
consisting of the states s = (ti , x) ∈ ∆ × Rn , U and V are
the action spaces of the first and second agents, respectively,
P ∆ is the transition distribution which is a delta distribution
in the case under consideration, R∆ is the reward function,
γ = 1 is a discount factor.

Q∆ (ti , x, u, v) − r
− min
max
Q∆ (ti+1 , x′ , u′ , v ′ ) ≤ ∆ti ε,
′
′
u ∈U v ∈V

− max
min
Q∆ (ti+1 , x′ , u′ , v ′ ) ≤ ∆ti ε
′
′

We naturally define the pure agents’ policies as πu∆ : S ∆ 7→
U and πv∆ : S ∆ 7→ V, their guaranteed results as
n
π∆
Vu u (ti , x) = max J ∆ : xi = x,
vi ,...,vm
o
xj+1 = xj +∆tj f (tj , xj , πu∆ (tj , xj ), vj ), j = i, . . . , m ,
π∆

v ∈V u ∈U

for any (ti , x) ∈ ∆ × D, i ̸= m + 1, u ∈ U, v ∈ V, where
r = ∆ti f 0 (ti , x, u, v), x′ = x + ∆ti f (ti , x, u, v) and we
put Q∆ (tm+1 , x′ , u′ , v ′ ) = σ(tm+1 ).
Moreover, if the functions f and f 0 have the form

n
J ∆ : xi = x,

f (t, x, u, v) = f1 (t, x, u) + f2 (t, x, v),

o
xj+1 = xj +∆tj f (tj , xj , uj , πv∆ (tj , xj )), j = i, . . . , m ,

f 0 (t, x, u, v) = f10 (t, x, u) + f20 (t, x, v),

Vv v (ti , x) =

min

ui ,...,um

∆
Q∆ (ti , x, u, v) = Q∆
1 (ti , x, u) + Q2 (ti , x, v).

∆
πu

′
Q∆
u (ti , x, u, v) = r + inf Vu (ti+1 , x ),

for any (ti , x, u, v) ∈ ∆ × Rn × U × V.

πv∆

′
Q∆
v (ti , x, u, v) = r + sup Vv (ti+1 , x ),

b) Let (τ, w) ∈ [0, T ) × Rn and ε > 0. There exists a
compact set D ⊂ Rn and δ > 0 with the following property.
For every partition ∆ satisfying diam(∆) < δ, there exists
a continuous function Q∆ (ti , x, u, v), (ti , x, u, v) ∈ ∆ ×
Rn × U × V satisfying (9), such that the policies

πv∆

where r = ∆ti f 0 (ti , x, u, v) and x′ = x+∆ti f (ti , x, u, v).
Due to this definition, one can show these Q-functions satisfy the following Bellman optimality equations:

πu∆ (ti , x) = Argmin max Q∆ (ti , x, u, v),

′
′ ′
Q∆
max
Q∆
u (ti , x, u, v) = r+ min
u (ti+1 , x , u , v ),
′
′
u ∈U v ∈V

′
′ ′
Q∆
min Q∆
v (ti , x, u, v) = r+max
v (ti+1 , x , u , v ).
v ′ ∈V u′ ∈U

(10)

∆
then there exists Q∆
1 (ti , x, u) and Q2 (ti , x, v) such that

and the optimal action-value functions (Q-functions) as
∆
πu

(9)

Q∆ (ti , x, u, v) − r

v∈V
u∈U
∆
πv (ti , x) = Argmax min Q∆ (ti , x, u, v),
u∈U
v∈V

(8)

provide the inequalities

The crucial and motivating our paper theoretical fact is
that the equality Vu∆ (ti , z) = Vv∆ (ti , z) and, as a conse∆
quence, the equality Q∆
u (ti , x, u, v) = Qv (ti , x, u, v) does
not hold in even the simplest Markov game examples (e.g.,

π∆

Vu u (τ, w) ≤ V (τ, w) + εT
π∆

Vv v (τ, w) ≥ V (τ, w) − εT.
4

(11)

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

correspond to u, and the columns correspond to v. Agents
act according to the ζ-greedy mixed policies

Thus, Theorem 4.1 (proved in Appendix D) establishes the
existence of function Q∆ which is an approximate solution
of both minimax and maximin Bellman optimality equations (8), and this shared Q-function can be exploited to
construct policies providing near-optimal value in differential game (1), (2). Besides, if condition (10) holds, then the
dependence of Q∆ on the agents’ actions u and v can be
separated.

ui ∼ (1 − ζ)πuθ (· | ti , xi ) + ζπuunif orm (·),
vi ∼ (1 − ζ)πvθ (· | ti , xi ) + ζπvunif orm (·),
where πuθ (· | ti , xi ) and πvθ (· | ti , xi ) are the optimal
mixed policies in the matrix game Qθ (ti , xi , ·, ·),
ζ ∈ [0, 1] is an exploration parameter, and store
(ti , xi , ui , vi , ri , ti+1 , xi+1 ) into the buffer D. Simultaneously, the minibatch {(tj , xj , uj , vj , rj , t′j , x′j )}kj=1 is taken
from D and the loss function

Looking at Theorem 4.1, one may wonder whether the
Isaacs’s condition implies the existence of a Nash equilibrium in discrete-time games (7). The answer to this question
is negative, even in the simplest case of differential games.
We provide such an example in Appendix E. Thus, the result
of Theorem 4.1 is, in this sense, the best that can be obtained
for a reasonably general case.

L(θ) =

In order to further theoretically justified apply the RL algorithms operating in finite action spaces to solve differential
games (1), (2), we also present the following generalization
of Theorem 4.1.

is utilized to update θ, where V θ (t′j , x′j ) is a Nash equilibrium of the matrix game Qθ (tj+1 , xj+1 , ·, ·). During the
learning, the exploration parameter ζ is decreasing from 1
to 0. After learning, we obtain mixed policies πuθ (· | t, x)
and πvθ (· | t, x), which we hope will be near-optimal.

Remark 4.2. Let Isaacs’s condition (6) holds. Let the value
function V (τ, w) be continuously differentiable at every
(τ, w) ∈ [0, T ] × Rn . If finite the sets U∗ and V∗ satisfy the
equalities

min max ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v)
u∈U∗ v∈V∗

= min max ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v) ,

Multi-agent DQN (MADQN). Next, we propose to consider another natural extension of the DQN algorithm following the idea from (Lowe et al., 2017). Each agent
uses its own Q-function approximated by neural networks
Qθu (t, x, u, v) and Qθv (t, x, u, v) for the first agent and the
second agent, respectively, and act according to the ζ-greedy
policies choosing greedy actions

u∈U v∈V

max min ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v)

k

1 X θ
Q (tj , xj , uj , vj ) − rj − γV θ (t′j , x′j ) ,
k j=1



v∈V∗ u∈U∗


= max min ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v) ,

ui = πuθ (ti , xi ) := Argmin max Qθu (ti , xi , u, v),

v∈V u∈U

u∈U∗

then these sets can be used instead of U and V in all statements of Theorem 4.1.

v∈V∗

vi = πvθ (ti , xi ) := Argmax min Qθv (ti , xi , u, v)
v∈V∗

(12)

u∈U∗

with probability 1 − ζ and any action uniformly on U∗ and
V∗ with probability ζ. For learning on the minibatches
{(tj , xj , uj , vj , rj , t′j , x′j )}kj=1 , each of them uses its own
loss function

Thus, Remark 4.2 (proved in Appendix D) allows us to
approximate the sets U and V by finite sets U∗ and V∗ without losing the theoretical results of the theorem, which is
essential for further developing RL algorithms.

k

5. Two-agent Deep Q-Networks

Lu (θu ) =

In this section, we describe various approaches to extending
the DQN algorithm (Mnih et al., 2015) to solve differential
games (1), (2), considering ideas from previous research
and our theoretical results given above.

1 X  θu
Q (tj , xj , uj , vj ) − rj
k j=1
θu

−γ min
max
Q
′
′
u ∈U∗ v ∈V∗

(t′j , x′j , u′ , u′ )

(13)
2

,

k

Lv (θv ) =
5.1. Baseline Zero-Sum Algorithms

1 X  θv
Q (tj , xj , uj , vj ) − rj
k j=1
2
θv ′
′
′
′
−γ max
min
Q
(t
,
x
,
u
,
u
)
.
j
j
′
′

NashDQN. First, we consider the NashDQN (Ding et al.,
2022) (or the similar MinimaxDQN (Fan et al., 2020)) algorithm naturally extending DQN to zero-sum Markov games.
Both agents utilize a shared Q-function approximated by a
neural network Qθ (t, x, u, v). The input of the neural network is (t, x), and the output is the matrix in which the rows

v ∈V∗ u ∈U∗

In this case, after learning, we already obtain pure agents’
policies πuθ (t, x) and πvθ (t, x).
Note that this algorithm is also centralized since the agents
have a shared memory containing opponent actions.
5

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Figure 1. Visualization of the second agent’s actions in the games based on the MuJoCo tasks

CounterDQN. Following the idea from (Li et al., 2022),
we can complicate the first agent’s learning by assuming
that the second agent knows its next action in advance. In
this case, in contrast to MADQN, the greedy second agent’s
policy is

simplifying calculations of minimums and maximums in
(12) and (14) as well as the learning on the whole.

πvθ (ti , xi , ui ) = Argmax Qθu (ti , xi , ui , v),

Algorithms. In our experiments, we test the following
algorithms: the DDQN algorithm (van Hasselt et al., 2016)
for decentralized (simultaneous) learning (2xDDQN) as
the most straightforward approach to solve any multi-agent
tasks; the PPO algorithm (Schulman et al., 2017) for alternating learning proposed in (Pinto et al., 2017) as an approach
for solving RARL problems (RARL); the MADDPG algorithm from (Lowe et al., 2017); the NashDQN, MADQN,
CounterDQN, IDQN, DIDQN algorithms described above.
We do one training step per timestamps, and the number of
timestamps is the same for all algorithms. The algorithms’
parameters are detailed in Appendix F.

6. Experiments

v∈V∗

and hence, only the first Bellman equation in (8) needs to be
solved, i.e., only loss function (13) must be minimized. After learning, we obtain the first agent’s pure policy πuθ (t, x)
and the second agent’s counter policy πvθ (t, x, u). Thus, if
we want to obtain the second agent’s pure strategy πvθ (t, x),
we must conduct symmetric learning for them.
5.2. Our Algorithms
Isaacs’s DQN (IDQN). Now, we modify MADQN utilizing the approximation Qθ (t, x, u, v) for the shared Qfunction Q∆ (t, x, u, v) from Theorem 4.1. Then, the agents’
actions are chosen similar to MADQN, in which Qθu and
Qθv are replaced by Qθ , and the parameter vector θ is updated according to the loss function

Environments. We consider the following five zero-sum
differential games. EscapeFromZero (S = R2 , U ⊂ R2 ,
V ⊂ R2 ) is presented in (Subbotin, 1995) as an example
of a game where the first agent can move away from zero
more than 0.5 only utilizing a discontinuous policy (V =
−0.5). In GetIntoCircle (S = R3 , U ⊂ R1 , V ⊂ R1 ) from
(Kamneva, 2019) and GetIntoSquare (S = R3 , U ⊂ R1 ,
V ⊂ R1 ) from (Patsko, 1996), the first agent tends to be as
close to zero as possible, but these papers show that the best
results it can guarantee to achieve are to be on the border
of a circle (V = 0) and a square (V = 1), respectively.
HomicidalChauffeur (S = R5 , U ⊂ R1 , V ⊂ R1 ) and
Interception (S = R11 , U ⊂ R2 , V ⊂ R2 ) are the games
from (Isaacs, 1965) and (Kumkov et al., 2005), in which the
first player wants to be as close as possible to the second
agent at the terminal time. We also consider three games
based on Mujoco tasks from (Todorov et al., 2012), in which
we introduce the second agent acting on the tip of the rod
in InvertedPendulum (S = R5 , U ⊂ R1 , V ⊂ R1 ), on the
tail of Swimmer (S = R9 , U ⊂ R2 , V ⊂ R1 ), or controlling
the rear bottom joint of HalfCheetah (S = R18 , U ⊂ R5 ,
V ⊂ R1 ) (see Fig. 1). A detailed description of all the above
games is provided in Appendix G.

k

L(θ) =

2
1X θ
Q (tj , xj , uj , vj ) − yj ,
k j=1

where
yj = rj +

γ
min max Qθ (t′j , x′j , u′ , v ′ )
2 u′ ∈U∗ v′ ∈V∗

θ ′
′
′ ′
+ max
min
Q
(t
,
v
)
.
,
x
,
u
j
j
′
′

(14)

v ∈V∗ u ∈U∗

We use this formula to provide symmetrical learning for the
agents since Qθ may not satisfy the equality
min max Qθ (t′j , x′j , u′ , v ′ ) = max
min
Qθ (t′j , x′j , u′ , v ′ ).
′
′

u′ ∈U∗ v ′ ∈V∗

v ∈V∗ u ∈U∗

Decomposed Isaacs’s DQN (DIDQN) Finally, according to Theorem 4.1, we can approximate the function
Q∆ (t, x, u, v) by the network
Qθ (t, x, u, v) = Qθ1 (t, x, u) + Qθ2 (t, x, v),
6

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Figure 2. Evaluation scheme of trained policies based on using various RL algorithms with various parameters (see Evaluation scheme).

Evaluation scheme. We consider the following scheme
(see Fig. 2) for thoroughly evaluating the robustness of
trained policies. In the first stage, agents learn (decentralized or centralized, depending on an algorithm). In the
second stage, we fix the trained first agent’s policy πu and
solve the obtained single-agent RL problem from the point
of the second agent’s view using various baseline RL algorithms such as DDQG (van Hasselt et al., 2016), DDPG
(Li et al., 2019), CEM (Amos & Yarats, 2020), A2C (Mnih
et al., 2016), PPO (Schulman et al., 2017), SAC (Haarnoja
et al., 2018) with various hyperparameters (see Appendix
F for details). After that, we choose the maximum value of
quality index J (2) (sum of rewards) in these running and
put it into the array “maximum values of the quality index”.
We believe this maximum value approximates the guaranteed result Vuπu (4). The third step is symmetrical to the
second one and is aimed at obtaining an approximation for
Vvπv (5). We repeat these three stages 5 times, accumulating
“maximum values of the quality index” and “minimum values of the quality index” arrays. Then, we illustrate the data
of these arrays as shown in Fig. 2. The boldest bar describes
the best guaranteed results of the agents out of 5 runnings,
the middle bar gives us the mean values, and the faintest
bar shows the worst results in 5 runnings. The width of the

bars illustrates the exploitability of both agents, that is, the
difference between the obtained approximations of Vuπu and
Vvπv . If they are close to the optimal guaranteed results Vu
and Vv , then the width should be close to zero (if a value
exists (Vu = Vv )). Thus, looking at such a visualization,
we can make conclusions about the stability (with respect
to running) and efficiency (with respect to exploitability) of
the algorithms.
Experimental results. Fig. 3 shows the experimental
results of the algorithms and the accurate values (dotted
line) when we know them. First of all, we note that the
2xDDQN, RARL, and NashDQN algorithms show the worst
performance. In the case of 2xDDQN and RARL, the reason
is quite typical for decentralized learning (Lanctot et al.,
2017). An agent overfits against a specific opponent and
loses the ability to resist other opponents. In the case of
NashDQN, the reason, apparently, lies in the stochasticity
of the trained policies aimed at giving results on average but
not guaranteed.
The MADDPG demonstrates the satisfactory borders of
guaranteed results only in 2 games (GetInfoCircle and GetInfoSquare). Regarding average by runnings, the algorithm
7

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

EscapeFromZero

GetIntoCircle

GetIntoSquare

HomicidalChauffeur

Interception

InvertedPendulum

Swimmer

HalfCheetah

Figure 3. Experimental results of the considered 8 algorithms (see Algorithms) in 8 differential games (see Environments).

is also well in HomicidalChauffeur, InvertedPendulum, and
Swimmer, which reflects, on the one hand, the potential
ability of MADDPG to find policies close to optimal, but,
on the other hand, its instability with respect to running.

mance in all games, reflecting the advantage of utilizing
pure policies and shared Q-functions. These algorithms
show similar performance except InvertedPendulum where
DIDQN is clearly better.

The MADQN is generally better than the algorithms discussed above, but it still inferiors to IDQN and DIDQN in
all games.

Thus, we conclude the following: centralized learning is
much more efficient than decentralized learning, solving the
Bellman equation in pure policies gives better results than
in mixed ones, a shared Q-function makes learning more
stable than two independent Q-functions, and the Q-function
decomposition can provide an advantage in some tasks.

The CounterDQN algorithm gives worse results than
MADQN in almost all games (except HomicidalChauffeur
and InvertedPendulum), which apparently indicates that it
is more efficient for agents to have more learning time steps
than information about the opponent’s actions.
The IDQN and DIDQN algorithms show the best perfor8

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

7. Limitations

Başar, T. and Bernhard, P. H∞ Optimal control and related
minimax design problems. Birkhäuser, Basel, 1995.

Although Isaacs’s condition is quite common and can often
be verified by relying only on general ideas about dynamics,
there are cases when it is not fulfilled (see Appendix C).
In these cases, Theorem 4.1 is not valid, and therefore, it
seems more theoretically justified to use MADQN instead
of IDQN and DIDQN.

Bertsekas, D. P. Dynamic Programming and Stochastic
Control. Academic Press, Inc., New York, San Francisco,
London, 1976.
Busoniu, L., Babuska, R., and Schutter, B. D. A comprehensive survey of multiagent reinforcement learning. IEEE
Transactions on Systems, Man and Cybernetics Part C:
Applications and Reviews, 38(2):156–172, 2008.

An essential limitation of MADQN, IDQN, and DIDQN, as
well as the basic DQN, is the action space’s finiteness. In
our paper, we show (see Remark 4.2) that the action space
discretization leaves the results of Theorem 4.1 valid under certain conditions. However, modifying the proposed
algorithms for continuous action space is a promising direction for further research that can improve their performance,
especially for high-dimensional action spaces.

Cardaliaguet, P., Quincampoix, M., and Saint-Pierre, P. Setvalued numerical analysis for optimal control and differential games. Annals of the International Society of
Dynamic Games, 4:177–247, 1999.
Ding, Z., Su, D., Liu, Q., and Jin, C. A deep reinforcement
learning approach for finding non-exploitable strategies in
two-player atari games. arXiv preprint arXiv:2207.08894,
2022.

The proposed IDQN and DIDQN algorithms can be interpreted not only as algorithms for solving RRL problems but
also as algorithms for solving zero-sum differential games.
In this sense, it should be emphasized that the development
of the shared Q-function concept to the general case of
multi-agent differential games is non-trivial and is complicated by the fact that there are no simple and sufficiently
general conditions (analogous to Isaacs’s condition) under
which such games have an equilibrium in positional (feedback) policies. Nevertheless, in some classes of games in
which the existence of Nash equilibrium is established, such
investigations can be promising.

Fan, J., Wang, Z., Xie, Y., and Yang, Z. A theoretical
analysis of deep q-learning. Proceedings of the 2nd Conference on Learning for Dynamics and Control, PMLR,
120:486–489, 2020.
Fleming, W. H. The convergence problem for differential
games. Journal of Mathematical Analysis and Applications, 8:102–116, 1961.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and
Whiteson, S. Counterfactual multi-agent policy gradients. Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 2974–2982, 2018.

Impact Statements
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none of which we feel must be
specifically highlighted here.

Friedman, A. Differential Games. Intersic., New York,
1971.
Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S.,
and Russell, S. Adversarial policies: Attacking deep
reinforcement learning. arXiv preprint arXiv:1905.10615,
2019.

References
Abdullah, M. A., Ren, H., Ammar, H. B., Milenkovic, V.,
Luo, R., Zhang, M., and Wang, J. Wasserstein robust
reinforcement learning. arXiv preprint arXiv:1907.13196,
2019.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actorcritic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor. Proceedings of the 35 th
International Conference on Machine Learning, PMLR,
80:1861–1870, 2018.

Al-Tamimi, A., Lewis, F. L., and Abu-Khalaf, M. Modelfree q-learning designs for linear discrete-time zero-sum
games with application to h-infinity control. Automatica,
43(3):473–481, 2007.
Amos, B. and Yarats, D. The differentiable cross-entropy
method. Proceedings of the 37th International Conference on Machine Learning, 119:291–302, 2020.

Han, M., Tian, Y., Zhang, L., Wang, J., and Pan, W. h∞
model-free reinforcement learning with robust stability
guarantee. NeurIPS 2019 Workshop on Robot Learning:
Control and Interaction in the Real World, 2019.

Bardi, M. and Dolcetta, I. C. Optimal Control and Viscosity Solutions of Hamilton-Jacobi-Bellman Equations.
Birkhauser, Boston, 1997.

Harmon, M. E., III, L. C. B., and Klopf, A. H. Reinforcement learning applied to a differential game. Adaptive
Behavior, 4(28):3–28, 1996.
9

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Isaacs, R. Differential Games: A Mathematical Theory
with Applications to Warfare and Pursuit, Control and
Optimization. John Wiley and Sons, Inc., New York,
London, Sydney, 1965.

Muller, P., Haarnoja, T., Tracey, B., Tuyls, K., Graepel,
T., and Heess, N. From motor control to team play in
simulated humanoid football. Science Robotics, 7:69,
2022.

Iyengar, G. Robust dynamic programming. Mathematics of
Operations Research, 30(2):257–280, 2005.

Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I. Multi-agent actor-critic for mixed cooperativecompetitive environments. Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 6382–6393, 2017.

Jiang, F., Guo, X., Zhang, X., Zhang, Z., and Dong, D.
Approximate soft policy iteration based reinforcement
learning for differential games with two pursuers versus
one evader. Proceedings of the 5th International Conference on Advanced Robotics and Mechatronics (ICARM),
pp. 471–476, 2020.

Lukoyanov, N. and Gomoyunov, M. Differential games on
minmax of the positional quality index. Dynamic Games
and Applications, 9:780—-799, 2019.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,
Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,
Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015. doi: 10.1038/nature14236.

Kamalaruban, P., Huang, Y.-T., Hsieh, Y.-P., Rolland, P.,
Shi, C., and Cevher, V. Robust reinforcement learning
via adversarial training with langevin dynamics. 34th
Conference on Neural Information Processing Systems,
2020.
Kamneva, L. Computation of solvability set for differential
games in the plane with simple motion and non-convex
terminal set. Dynamic Games and Applications, 9:724–
750, 2019.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T. P., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. Proceedings of The 33rd International Conference on Machine Learning, PMLR, 48:1928–1937, 2016.

Krasovskii, N. N. and Subbotin, A. I. Game-Theoretical
Control Problems. Springer-Verlag, New York, Berlin,
Heidelberg, London, Paris, Tokyo, 1987.

Morimoto, J. and Doya, K. Robust reinforcement learning.
Advances in Neural Information Processing Systems 13,
2000.

Kumkov, S. S., Patsko, V. S., and Shinar, J. On level sets
with “narrow throats” in linear differential games. International Game Theory Review, 7(3):285–311, 2005.

Nilim, A. and Ghaoui, L. E. Robust control of markov
decision processes with uncertain transition matrices. Operations Research, 53(5):780–798, 2005.

Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,
Tuyls, K., Perolat, J., Silver, D., and Graepel, T. A unified game-theoretic approach to multiagent reinforcement
learning. Proceedings of the 31st Conference on Neural
Information Processing Systems, 2017.

OpenAI. Openai five. https://blog.openai.com/openai-five/,
2018.
Patsko, V. Special aspects of convex hull constructing in
linear differential games of small dimension. IFAC Proceedings Volumes, 29(8):19–24, 1996.

Li, J., Lee, D., Sojoudi, S., and Tomlin, C. J. Infinitehorizon reach-avoid zero-sum games via deep reinforcement learning. arXiv preprint arXiv:2203.10142, 2022.

Phillips, P.
Reinforcement learning in two player
zero sum simultaneous action games. arXiv preprint
arXiv:2110.04835, 2021.

Li, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S.
Robust multi-agent reinforcement learning via minimax
deep deterministic policy gradient. Proceedings of the
AAAI Conference on Artificial Intelligence, 33(1):4213–
4220, 2019.

Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Robust adversarial reinforcement learning. Proceedings of
the 34 th International Conference on Machine Learning,
70:2817–2826, 2017.

Littman, M. L. Markov games as a framework for multiagent reinforcement learning. International Conference
on Machine Learning, pp. 157–163, 1994.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Liu, S., Lever, G., Wang, Z., Merel, J., Eslami, S. M.,
Hennes, D., Czarnecki, W. M., Tassa, Y., Omidshafiei, S.,
Abdolmaleki, A., Siegel, N. Y., Hasenclever, L., Marris,
L., Tunyasuvunakool, S., Song, H. F., Wulfmeier, M.,

Selvakumara, J. and Bakolas, E. Neurocomputing. Neurocomputing, 475:1–14, 2022.
10

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Shapley, L. S. Stochastic games. Proceedings of the National Academy of Sciences, 39:1095–1100, 1953.

Wiesemann, W., Kuhn, D., and Rustem, B. Robust markov
decision processes. Mathematics of Operations Research,
38(1), 2012.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L.,
Van Den Driessche, G., Graepel, T., and Hassabis, D.
Mastering the game of go without human knowledge.
Nature, 550:354–359, 2017.

Xu, C., Zhang, Y., Wang, W., and Dong, L. Pursuit and
evasion strategy of a differential game based on deep
reinforcement learning. Frontiers in Bioengineering and
Biotechnology, 10:1–12, 2022.
Zhai, P., Luo, J., Dong, Z., Zhang, L., Wang, S., and Yang,
D. Robust adversarial reinforcement learning with dissipation inequation constraint. The Thirty-Sixth AAAI
Conference on Artificial Intelligence, 36(5), 2022.

Subbotin, A. Generalized Solutions of First Order PDEs:
the Dynamical Optimization Perspective. Birkhäuser,
Berlin, 1995.

Zhang, K., Yang, Z., and Başar, T. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2021.

Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo,
J. Z., Tuyls, K., and Graepel, T. Value-decomposition
networks for cooperative multi-agent learning based on
team reward. arXiv preprint arXiv:1706.05296, 2017.

Zhou, K., Doyle, J. C., and Glover, K. Robust Optimal
Control. Prentice Hall, New Jersey, 1996.

Sutton, R. S. and Barto, A. G. Reinforcement Learning
An Introduction (Second Edition). The MIT Press, Cambridge, Massachusetts, 2018.

Zhu, Y. and Zhao, D. Online minimax q network learning for
two-player zero-sum markov games. IEEE Transactions
on Neural Networks and Learning Systems, 33(3):1228–
1241, 2020.

Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus,
K., Aru, J., Aru, J., and Vicente, R. Multiagent cooperation and competition with deep reinforcement learning.
PloS One, 12(4), 2017.
Tessler, C., Efroni, Y., and Mannor1, S. Action robust
reinforcement learning and applications in continuous
control. Proceedings of the 36th International Conference
on Machine Learning, 97, 2019.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
engine for model-based control. International Conference
on Intelligent Robots and Systems, 2012.
Van Der Wal, J. Stochastic dynamic programming : successive approximations and nearly optimal strategies for
Markov decision processes and Markov games. Mathematisch Centrum, Amsterdam, 1980.
van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. Proceedings of
the Thirtieth AAAI Conference on Artificial Intelligence
(AAAI-16), 30(1):2094–2100, 2016.
Vinyals, O., Babuschkin, I., Czarnecki, W., Mathieu, M.,
Dudzik, A., Chung, J., and et.at. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575:350–354, 2019.
Wang, M., Wang, L., and Yue, T. An application of continuous deep reinforcement learning approach to pursuitevasion differential game. Proceedings of the IEEE 3rd
Information Technology,Networking,Electronic and Automation Control Conference, pp. 1150–1155, 2019.
11

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

A. Appendix
Typical conditions for the positional differential game theory (see, e.g., p. 116 in (Subbotin, 1995)) are the following:
• The functions f (t, x, u, v) and f 0 (t, x, u, v) are continuous.
• There exists cf > 0 such that

f (t, x, u, v) + f 0 (t, x, u, v) ≤ cf 1 + ∥x∥ ,

(t, x, u, v) ∈ [0, T ] × Rn × U × V.

• For every α > 0, there exists λf > 0 such that
f (t, x, u, v) − f (t, y, u, v) + f 0 (t, x, u, v) − f 0 (t, y, u, v) ≤ λf ∥x − y∥
for any t ∈ [0, T ], x, y ∈ Rn : max{∥x∥, ∥y∥} ≤ α, u ∈ U, and v ∈ V.
In particular, these conditions provide the existence and uniqueness of the motion x(·) for each Lebesgue-measurable
functions u(·) : [τ, T ] 7→ U and v(·) : [τ, T ] 7→ V, where we mean by the motion a Lipschitz continuity function
x(·) : [τ, T ] 7→ Rn satisfying condition x(τ ) = w and equation (1) almost everywhere.

B. Appendix
Let us show that game (7) can be formalized as a Markov game (S ∆ , U, V, P ∆ , R∆ , γ). First, put

S ∆ = ∆ × Rn ∪ sT ,
where sT is some fictional terminal state. Next, for every s = (ti , x) ∈ ∆ × Rn , i ̸= m + 1, u ∈ U, and v ∈ V, we define
the transition distribution and the reward function by
P(s′ |s, u, v) = δ(s′ ),

R(s, u, v) = ∆ti f 0 (ti , x, u, v),

where s′ = (ti+1 , x′ ), x′ = x + ∆ti f (ti , x, u, v), and δ is the Dirac delta distribution. For s = (tm+1 , x) ∈ ∆ × Rn , we set
P(s′ |s, u, v) = δ(s′ = sT ),

R(s, u, v) = σ(x),

u ∈ U,

v ∈ V.

R(sT , u, v) = 0,

u ∈ U,

v ∈ V.

In order to make the game formally infinite, we put
P(s′ |sT , u, v) = δ(s′ = sT ),

C. Appendix
Let us consider the differential game
d
x(t) = cos(u + v),
dt

x(t) ∈ R,


u(t), v(t) ∈ U = V = w ∈ R : |w| ≤ π ,

x(0) = (0, 0),

t ∈ [0, 1],

J = x(1).

First, note that Isaacs’s condition (6) does not hold for this game. Indeed,


min max s cos(u + v) = |s| =
̸ −|s| = max min s cos(u + v)
u∈U v∈V

v∈V u∈U

Let us consider the corresponding discrete-time game (7)
xj+1 = xj + ∆t cos(uj + vj ),

j = i, i + 1, . . . , m,

J ∆ = σ(xm+1 )

Then, by solving Bellman mimmax and maximin equations (8) backwards, we derive
∆
xi + ∆ti cos(u + v) + (1 − ti+1 ) = Q∆
u (ti , xi , u, v) > Qv (ti , xi , u, v) = xi + ∆ti cos(u + v) − (1 − ti+1 )

for any (ti , xi , u, v) ∈ ∆ × R × U × V.
12

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

D. Appendix
Denote
χ(t, x, u, v, s) = ⟨f (t, x, u, v), s⟩ + f 0 (t, x, u, v).
Lemma 1. Let condition (6) hold. Let the value function V (τ, w) = Vu (τ, w) = Vv (τ, w) be continuously differentiable at
every (τ, w) ∈ [0, T ] × Rn . Then the equations
∂
V (τ, w) + H(τ, w, ∇w V (τ, w)) = 0,
∂τ
hold for any τ ∈ [0, T ) and w ∈ Rn , where we denote

V (T, w) = σ(w),

(15)

H(t, x, s) = min max χ(t, x, u, v, s) = max min χ(t, x, u, v, s).
u∈U v∈V

v∈V u∈U

The lemma follows from two facts: the value function is a minimax (generalized) solution of Cauchy problem (15) (see
Theorem 11.4 in (Subbotin, 1995)) and a continuously differentiable minimax solution is a classical solution (see Section
2.4 in (Subbotin, 1995)).
Let us prove a). Let ε > 0 and D ⊂ Rn . Let us define a compact set D′ so that
 ′
x = x + (t′ − t)f (t, x, u, v) : t, t′ ∈ [0, T ], x ∈ D, u ∈ U, v ∈ V ⊂ D′ .
Since the value function V is continuously differentiable, there exists δ > 0 such that
V (t′ , x′ ) − V (t, x) − (t′ − t)(∂/∂t)V (t, x) − ⟨x′ − x, ∇x V (t, x)⟩ ≤ ε(t′ − t)
for any t, t′ ∈ [0, T ] satisfying 0 < t − t′ ≤ δ and any x ∈ D, x′ ∈ D′ u ∈ U, v ∈ V. Let ∆ be such that diam(∆) < δ.
Define


Q∆ (ti , x, u, v) = V (ti , x) + ∆ti (∂/∂t)V (t, x) + χ(t, x, u, v, ∇x V (t, x)) ,
(16)
where (ti , x, u, v) ∈ ∆ × Rn × U × V, i ̸= m + 1, and Q∆ (tm+1 , x, u, v) = σ(x). Then, using Lemma 1, we derive
Q∆ (ti , x, u, v) − r − min
max
Q∆ (ti+1 , x′ , u′ , v ′ )
′
′
u ∈U v ∈V

= V (ti , x) + ∆ti (∂/∂t)V (ti , x) + ⟨x′ − x, ∇x V (ti , x)⟩ − V (ti+1 , x′ ) ≤ ε∆ti
for any (ti , x, u, v) ∈ ∆ × Rn × U × V, where r = ∆ti f 0 (ti , x, u, v) and x′ = x + ∆ti f (ti , x, u, v). The statement about
decomposition of Q∆ (t, x, u, v) follows from (16). Thus, a) has proved.
Let us prove b). Let (τ, w) ∈ [0, T ) × Rn and ε > 0. Put

D = (t, x) ∈ [0, T ] × Rn : ∥x∥ ≤ (∥w∥ + 1)ecf t − 1 .
Then, we have the inclusion (τ, w) ∈ D. Note also that, for every (t, x) ∈ D, the inclusion (t′ , x′ ) ∈ D holds for t′ ∈ [t, T ]
and x′ = x + (t′ − t)f (t, x, u, v), u ∈ U, v ∈ V. Take δ > 0 according to a). Let us take a partition ∆ satisfying
diam(∆) < δ and the function Q∆ (ti , x, u, v) from (16). Let vi ∈ V, i = 0, 1, . . . , m be such that the equality
Vuπu (τ, w) = σ(xm+1 ) +

m
X

∆ti f 0 (ti , xi , πu∆ (ti , xi ), vi ),

i=0

holds, where
xi = w

xi+1 = xi + ∆ti f (ti , xi , πu∆ (ti , xi ), vi ),

i = 0, 1, . . . , m.

Then, due to (9) and (11), we derive
Vuπu (τ, w) ≤ σ(xm+1 ) +
≤ σ(xm+1 ) +

m 
X
i=0

m 
X


Q∆ (ti , xi , πu (ti , xi ), vi ) − min max Q∆ (ti+1 , xi+1 , u, v) + εT
u∈U v∈V

i=0


min max Q∆ (ti , xi , u, v) − min max Q∆ (ti+1 , xi+1 , u, v) + εT
u∈U v∈V

u∈U v∈V

≤ min max Q∆ (τ, w, u, v) + εT
u∈U v∈V

13

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Form this estimate, taking into account the definition (16) of Q∆ and Lemma 1, we obtain the first inequality in the statement
b). The second inequality can be proved by the symmetric way.
The validity of Remark 4.2 follows from the proof given above replacing
min max χ(t, x, u, v, s),

max min χ(t, x, u, v, s)

u∈U v∈V

v∈V u∈U

with
min max χ(t, x, u, v, s),

max min χ(t, x, u, v, s).

u∈U∗ v∈V∗

v∈V∗ u∈U∗

E. Appendix
We consider a differential game described by the differential equation
d
x(t) = u(t) + v(t),
dt

t ∈ [0, 1],

x(t) ∈ R,

u(t), v(t) ∈ {−1, 1},

with the initial condition x(0) = 0 and the quality index
J = x2 (1).
This differential game satisfies Isaacs’s condition (6), but the corresponding discrete-time game (7) does not have a Nash
equilibrium. Indeed, let us fix a partition ∆ (3) and consider the optimal guaranteed results for the agents in the discrete-time
game:
π∆

Vu∆ (ti , x) = inf Vu u (ti , x),
∆
πu

∆
πu

π∆

Vv∆ (ti , x) = sup Vv v (ti , x),
πv∆

πv∆

where Vu (ti , x) and Vv (ti , x) are taken from Section 4. Then, by definitions of these values, in the differential game
under consideration, one can show that
Vu∆ (ti , x) =
Vv∆ (ti , x) =

min

max Vu∆ (ti+1 , x + ∆ti (u + v)),

i ∈ 0, m,

Vu∆ (tm+1 , x) = x2 ,

Vv∆ (ti+1 , x + ∆ti (u + v))),

i ∈ 0, m,

Vv∆ (tm+1 , x) = x2 ,

u∈{−1,1} v∈{−1,1}

max

min

v∈{−1,1} u∈{−1,1}

Then, for i = m, we have
Vu∆ (ti , x) = min

max

u∈{−1,1} v∈{−1,1}

x + ∆ti (u + v)

2

= x2 + 4∆t2 ̸= x2 = max

min

v∈{−1,1} u∈{−1,1}

2
x + ∆ti (u + v) = Vv∆ (ti , x).

It means that the corresponding discrete-time game (7) does not have a Nash equilibrium.

F. Appendix
Two-agent algorithms’ parameters. We use pretty standard and the same parameters for the 2xDDQN, NashDQN,
MADQN, CounterDQN, IDQN, and DIDQN algorithms. We utilize the ADAM optimizer with the learning rate lr = 0.001,
the smoothing parameter τ = 0.01, and the batch size nbs = 64. For the time-discretization, we use the uniform partitions
∆ = {i∆t : i = 0, 1, . . . , m + 1}. The parameter ∆t, the structure of the neural networks, and the discretization of the
continuous action spaces depend on the game and are indicated in Table 1, where we define the linear mesh, square mesh,
and ball mesh as

LM (a, b, k) = a + i(b − a)/k : i = 0, 1, . . . , k , SM (a, b, k, n) = LM (a, b, k)n ,

BM (a, b, k) = (sin(α), cos(α)) ∈ R2 : α ∈ LM (a, b, k) .
In particular, we use deeper neural networks in more complex games for better results. Agents learn during 50000 timesteps,
under the linear reduction of the exploration noise ζ from 1 to 0. In the CounterDQN algorithm, each agent learns 25000
timesteps.
For the RARL approach, we apply the PPO algorithm from StableBaseline3 with the standard parameters and alternately
teach the agents every 1000 timesteps.
14

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Envirenments
EscapeFromZero
GetIntoCircle
GetIntoSquare
HomicidalChauffeur
Interception
InvertedPendulum
Swimmer
HalfCheetah

U∗
BM (0, 2π, 10)
LM (−0.5, 0.5, 10)
LM (−1, 1, 10)
LM (−1, 1, 10)
BM (0, 2π, 10)
LM (−1, 1, 9)
SM (−1, 1, 4, 2)
SM (−0.5, 0.5, 2, 5)

V∗
BM (0, 2π, 10)
LM (−1, 1, 10)
LM (−1, 1, 10)
BM (0, 2π, 10)
BM (0, 2π, 10)
LM (−0.2, 0.2, 9)
LM (−0.2, 0.2, 16)
LM (−0.5, 0.5, 32)

∆t
0.2
0.2
0.2
0.2
0.2
0.2
1.0
0.3

Hidden NN Leyers
256, 128
256, 128
256, 128
256, 128
512, 256, 128
512, 256, 128
512, 256, 128
512, 256, 128

Table 1. Parameters for the two-agents’ learning algorithms

Parameters
learning timesteps
learning rate

DDQN
5e4
1e-3

batch size
smooth param. τ
discount factor γ
percentile param.
number of steps

64
1e-2
1
—
—

DDPG
CEM
2.5e4
5e4
π : 1e-4, 1e-2
q : 1e-3
64
—
1e-3
1e-2
1
1
—
80
—
—

A2C
2.5e4
1e-3

PPO
5e4
1r-3

SAC
2.5e4
1e-3

Def.
Def.
1
—
—

64
Def.
1
—
64

Def.
1e-2
1
—
—

Table 2. Parameters for the evaluating algorithms.

Algorithms’ parameters for evaluation.
described in Table 2.

Parameters of the algorithms used in the evaluation stages (see Fig. 2) are

G. Appendix
EscapeFromZero. The game taken from p. 164 (Subbotin, 1995) describes the motion of a point on a plane that is
affected by two agents. The first agent aims to be as far away from zero as possible at the terminal time T = 2, while the
aim of the second agent is the opposite. The capabilities of the first agent’s influence are constant and are described by a unit
ball. In contrast, the capabilities of the second agent are a ball with a decreasing radius as the terminal time is approached.
Thus, the differential game is described by the differential equation
d
x(t) = u(t) + (2 − t)v(t),
dt

t ∈ [0, 2],

x(t) ∈ R2 ,

u(t), v(t) ∈ B 2 := {s ∈ R2 : ∥s∥ ≤ 1},

with the initial condition x(0) = x0 := (0, 0), and the quality index J = −∥x(2)∥. On (Subbotin, 1995) shows that
V (0, x0 ) = −0.5. This means the first agent is able to move away from zero by 0.5 at the terminal time T = 2 for any
actions of the second agent.
GetIntoCircle This game is taken from (Kamneva, 2019). The first and the second agents can move a point on the plane
vertically and horizontally, respectively. The first agent aims to drive the point as close to zero as possible at the terminal
time T = 4. The aim of the second agent is the opposite. Thus, the differential game is described as follows:
d
d
x1 (t) = v(t),
x2 (t) = u(t), t ∈ [0, 4], x(t) ∈ R2 , u(t) ∈ [−0.5, 0.5], v(t) ∈ [−1, 1],
dt
dt
x(0) = x0 = (0, 0.5),

J = ∥x(4)∥ − 4.

This game has a value V (0, x0 ) = 0, which means the optimal first agent can lead the point only to the border of a circle of
the radius r = 4.
15

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

GetIntoSquare. In the game from (Patsko, 1996), The first agent aims to drive a point on the plane as close to zero as
possible at the terminal time T = 4. The aim of the second agent is the opposite. The differential game is described as
follows:
d
d
x1 (t) = x2 (t) + v(t),
x2 (t) = −x1 (t) + u(t),
dt
dt
t ∈ [0, 4],

x(t) ∈ R2 ,

u(t), v(t) ∈ [−1, 1],

J = max{|x1 (4)|, |x2 (4)|}.

x(0) = x0 := (0.2, 0),

The game has the value V (0, x0 ) = 1, which means the optimal first agents can lead the point only to the border of a square
with the side a = 1.
HomicidalChauffeur is a well-studied example of a pursuit-evasion differential game ((Isaacs, 1965)). However, to
formalize this game within our class of differential games (1), 2) we consider its finite-horizon version:
d
x1 (t) = 3 cos(x3 (t)),
dt
d
x5 (t) = v2 ,
dt

d
x2 (t) = 3 sin(x3 (t)),
dt

t ∈ [0, 3],

x(t) ∈ R5 ,

x(0) = (0, 0, 0, 2.5, 7.5),

J=

d
d
x3 (t) = u(t),
x4 (t) = v1 (t),
dt
dt

u(t) ∈ [−1, 1], v(t) ∈ v ∈ R2 : ∥v∥ ≤ 1}

p
(x1 (4) − x4 (t))2 + (x2 (4) − x5 (4))2 .

Such a version of this game has been studied much less, and therefore, we do not know the exact value in it.
Interception. This game is taken from (Kumkov et al., 2005) and describes an air interception task. At the terminal time
T = 3, the first agent strives to be as close as possible to the second agent, but unlike the second agent, the first agent has
inertia in dynamics. The differential game is described by the differential equation
d2
d
d2
y(t)
=
F
(t),
F
(t)
=
−F
(t)
+
u(t),
z(t) = v(t), t ∈ [0, 3],
dt2
dt
dt2


d
d
d
d
x(t) = y1 (t), y2 (t), y1 (t), y2 (t), F1 (t), F2 (t), z1 (t), z2 (t), z1 (t), z2 (t) ∈ R10 ,
dt
dt
dt
dt
n
o
n
o
u21
v12
2
2
2
u(t) ∈ u ∈ R2 : (0.67)
, v(t) ∈ v ∈ R2 : (0.71)
2 + u2 ≤ (1.3)
2 + v2 ≤ 1 ,
with the initial conditions x(0) = x0 := (1, 1.1, 0, 1, 1, −2, 0, 0, 1, 0) and the quality index J = ∥y(3) − z(3)∥. Due to the
difference of the problem statement in (Kumkov et al., 2005), we cannot precisely set the value of this game. We can only
state the inequality V (0, x0 ) ≥ 1.5.
InvertedPendulum. We take the InvertedPendulum task from the MuJoCo simulator ((Todorov et al., 2012)) described by
d
x(t) = FIP (x(t), u(t)), t ∈ [0, Tf ), x(t) = (qpos0:1 (t), qvel0:1 (t)) ∈ R4 , u(t) ∈ [−1, 1],
dt
where qpos0:1 (t) = (qpos0 (t), qpos1 (t)), qvel0:1 (t) = (qvel0 (t), qvel1 (t)), and Tf is a time until which the restriction
x(t) ∈ D holds. Violation of this restriction means the pendulum falls down. Based on this task, we consider the differential
game
d
x(t) = FIP (x(t), u(t)) + e4 v(t), t ∈ [0, 3], x(t) ∈ R4 , u(t) ∈ [−1, 1], v(t) ∈ [−0.2, 0.2]
dt
where e4 = (0, 0, 0, 1), with initial condition x(0) = x0 = (0, 0, 0, 0) and the quality index
(
Z 3
1, if x(t) ∈ D holds,
J =−
[x(t) ∈ D]dt, [x(t) ∈ D] =
0, otherwise.
0
Thus, we introduced the second agent as a disturbance at the end of the rod and reformulated the problem as differential
game (1), (2), retaining the meaning.
16

Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach

Swimmer. In a similar way, we consider the Swimmer task from MuJoCo
d
x(t) = FS (x(t), u(t)), t ∈ [0, +∞), x(t) = (qpos2:4 (t), qvel0:4 (t)) ∈ R8 , u(t) ∈ [−1, 1]2 ,
dt
Z +∞
x(0) = x0 = 0 ∈ R8 , J =
r(x(t))dt,
0

introduces the second agent as a disturbance on the tail, and reformulated this task as differential game (1), (2)
d
x(t) = FS (x(t), u(t)) + e3 v(t), t ∈ [0, 20], x(t) ∈ R8 , u(t) ∈ [−1, 1]2 , v(t) ∈ [−0.2, 0.2],
dt
Z 20
x(0) = x0 = 0 ∈ R8 , J = −
r(x(t))dt.
0

HalfCheetah

is the third task from the MuJoCo simulator that we consider

d
x(t) = FHC (x(t), a(t)), t ∈ [0, +∞), x(t) = (qpos1:8 (t), qvel0:1 (t)) ∈ R17 , a(t) ∈ [−1, 1]6 ,
dt
Z +∞
x(0) = x0 = 0 ∈ R17 , J =
r(x(t))dt.
0

In this task, we determine the agents’ actions as u(t) = a1:5 (t) ∈ [−0.5, 0.5]5 and v(t) = a6 (t) ∈ [−0.5, 0.5] and
reformulated the task as differential game (1), (2)
d
x(t) = FHC (x(t), u(t), v(t)), t ∈ [0, 3), x(t) ∈ R17 , u(t) ∈ [−0.5, 0.5]5 , v(t) ∈ [−0.5, 0.5],
dt
Z 3
x(0) = x0 = 0 ∈ R17 , J = −
r(x(t))dt.
0

Here we reduce the capabilities of agents in order to make the game more interesting. Otherwise, the second agent would
always win by flipping the HalfCheetah.

17

