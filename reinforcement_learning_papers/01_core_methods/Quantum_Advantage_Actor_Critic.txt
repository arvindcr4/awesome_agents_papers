Quantum Advantage Actor-Critic for Reinforcement Learning
Michael Kölle1∗ , Mohamad Hgog1∗ , Fabian Ritz1 , Philipp Altmann1 , Maximilian Zorn1 , Jonas Stein1
and Claudia Linnhoff-Popien1
1 Institute of Informatics, LMU Munich, Munich, Germany

arXiv:2401.07043v1 [quant-ph] 13 Jan 2024

michael.koelle@ifi.lmu.de

Keywords:

Quantum Computing, Quantum Reinforcement Learning, Advantage Actor-Critic

Abstract:

Quantum computing offers efficient encapsulation of high-dimensional states. In this work, we propose a
novel quantum reinforcement learning approach that combines the Advantage Actor-Critic algorithm with
variational quantum circuits by substituting parts of the classical components. This approach addresses reinforcement learning’s scalability concerns while maintaining high performance. We empirically test multiple
quantum Advantage Actor-Critic configurations with the well known Cart Pole environment to evaluate our
approach in control tasks with continuous state spaces. Our results indicate that the hybrid strategy of using either a quantum actor or quantum critic with classical post-processing yields a substantial performance increase
compared to pure classical and pure quantum variants with similar parameter counts. They further reveal the
limits of current quantum approaches due to the hardware constraints of noisy intermediate-scale quantum
computers, suggesting further research to scale hybrid approaches for larger and more complex control tasks.

1

INTRODUCTION

The field of quantum computing (QC) is commonly
credited with the potential to solve highly complex
problems with considerable speedup compared to
classical computers, as it leverages the unique properties of quantum mechanics (Nielsen and Chuang,
2010). This potential will be realized once reliable
and usable quantum hardware becomes available, as
discussed in (Preskill, 2018). Still, even on QC hardware of current size, achievements in domains like
quantum cryptography (Pirandola et al., 2020; Shor,
1997), quantum chemistry (Cao et al., 2019; Bauer
et al., 2020; Dral, 2020), and quantum optimization (Farhi and Harrow, 2016; Cerezo et al., 2020;
Farhi et al., 2014) have shown promising application progress of QC to real-world problems. Inspired
by the success of applying classical machine learning (ML) techniques to problems like image recognition, reinforcement learning, and natural language
processing, the domain of quantum machine learning
(QML) has also recently progressed through applying ML methods to and on quantum hardware. The
common goal of QML algorithms is to leverage the
inherent parallelism of quantum interference, potentially accelerating machine learning models’ training
and inference speed (Biamonte et al., 2017).
* Authors contributed equally to this work.

Reinforcement learning (RL) is a machine learning subfield focused on training agents to interact with
an environment and learn from their experiences. RL
has achieved remarkable success in applications such
as game playing (e.g., AlphaGo (Silver et al., 2017)),
robotics (Kober et al., 2013), and autonomous driving (You et al., 2017). However, the performance of
classical RL algorithms is often limited by their sample inefficiency. This leads to slow convergence and
requires a large number of interactions with the environment. Since quantum computing offers the efficient encapsulation of high-dimensional states as one
of its significant benefits, recent research interest in
quantum reinforcement learning (QRL) has emerged
(Meyer et al., 2022b).
In this paper, we focus on actor-critic methods
(Konda and Tsitsiklis, 1999), a popular class of RL
algorithms combining policy-based and value-based
approaches. We specifically examine the Advantage
Actor-Critic (A2C) algorithm, first proposed by Mnih
et al., (Mnih et al., 2016). The A2C algorithm is
well understood in the classical field and has been
extensively benchmarked for control problems (e.g.,
(Andrychowicz et al., 2021)). In the younger context of QRL, research has been heavily focused on
value-based methods like Deep Q-Networks (Mnih
et al., 2015); however, this work focuses on actorcritic methods.

In this work, we explore variational quantum circuits (VQC) for policy gradient methods like the A2C
algorithm. VQCs are parameterized quantum circuits trained using classical optimization techniques,
suitable for the current era of noisy intermediate
scale quantum computers (Preskill, 2018). For our
quantum variations of the A2C algorithm, we utilize
VQCs, replacing classical neural networks for both
the actor and critic. We evaluate and showcase the
potential of VQCs to enhance the algorithm’s learning efficiency and accuracy in solving simple control
problems. We test our implementation on the wellunderstood but non-trivial Cart Pole control environment provided by the OpenAI gymnasium of RL domains (Brockman et al., 2016). This benchmark problem is chosen for its ability to highlight the potential
benefits of the quantum A2C algorithm, such as faster
convergence and improved stability, in solving control tasks with continuous state spaces.
We conducted experiments to evaluate different
quantum, hybrid, and classical A2C configurations.
First, we compared the performance of our quantum
A2C configurations with the classical A2C algorithm,
ensuring a similar number of parameters. Furthermore, we investigated the effectiveness of a VQC with
a classical post-processing layer. Finally, we juxtaposed the performance of this hybrid approach with
a classical A2C algorithm, maintaining a similar parameter count. Our findings indicate that the quantum hybrid A2C approach demonstrates a substantial performance advantage over the purely quantum
and classical methods. However, it is essential to
note that the quantum A2C algorithm faces potential
limitations and challenges, such as scalability issues
and hardware constraints related to the current era of
NISQ computers. These limitations may impact the
generalizability of our results, and further research is
required to fully understand the performance of quantum A2C algorithms in more extensive and complex
control tasks.
This work is structured as follows. First, we discuss the preliminaries in Section 2 and related works
in Section 3. We introduce our quantum A2C and hybrid quantum A2C architectures in Section 4. We then
go into detail about our experimental setup in Section 5 and the results in Section 6. Lastly, we conclude with summarizing our findings and indicating
future work in Section 7.

2

PRELIMINARIES

2.1

Policy Gradient and Actor-Critic
Methods

Policy gradient methods is a category within reinforcement learning, which directly optimizes policies
by estimating optimal policy weights using gradient
ascent, circumventing explicit trade-off requirements
between exploration and exploitation and learning
stochastic policies for state space exploration (Sutton et al., 1999). Distinct from Q-learning’s ε-greedy
strategy, they adeptly manage high-dimensional and
continuous action spaces and elude perceptual aliasing by generating probability distributions (Zhang
et al., 2020). Formally, policy gradient methods optimize policy parameters θ to amplify the expected
cumulative reward, mathematically expressed as:
J(θ) = Eπθ [R]

(1)

where J(θ) represents the objective function and πθ
signifies a policy. Utilizing Monte Carlo samples, the
gradient of J(θ) relative to θ can be estimated and
updated through stochastic gradient ascent, with the
algorithm seeking optimal θ∗ .
Actor-critic methods synergize value-based and
policy-based strategies, where the actor (policy function πθ (s, a)) and critic (value function Qw (s, a)) learn
their respective functionalities, often modeled using
non-linear neural network function approximators in
deep RL (Sutton and Barto, 2018). In essence:
Q(s, a; w) ≈ Qπ (s, a)

(2)

with the actor updating policy parameters via gradients from the policy gradient theorem, and the critic
estimating the value function for the current policy.
Various actor-critic algorithms, such as Asynchronous Advantage Actor-Critic (A3C), Deep Deterministic Policy Gradient, and Proximal Policy Optimization, employ the actor-critic framework to address challenges in continuous or substantial state
spaces and stochastic policies (Mnih et al., 2016; Lillicrap et al., 2015; Schulman et al., 2017). Specifically,
Advantage Actor-Critic (A2C) utilizes the advantage
function:
A(s, a) = Q(s, a) −V (s)
(3)
to update policies based on actions’ relative efficacy
in given states, demonstrating enhanced accuracy and
rapid convergence, thereby forming a robust focal
point for further RL research.

2.2

Variational Quantum Circuit

To comprehend the concept of variational quantum
circuits, examining the underlying principles of quantum circuits is essential. Similar to classical circuits, they consist of individual components: quantum
wires and quantum gates. Each quantum wire represents a quantum bit analogous to a classical circuit
wire representing a classical bit. In contrast, quantum
gates perform unitary transformations on the quantum
bits, akin to how classical gates perform operations on
classical bits (Homeister, 2018).
VQC, also known as a parametrized quantum circuit, is a quantum algorithm that employs a classical
optimization process to prepare a quantum state encoding the solution to a specific problem. A VQC
can be divided into different parts. In the first part,
the classical input data enters the circuit as arguments
and is encoded into quantum states using the initial
quantum gates in the circuit. After encoding the data,
qubits are entangled by controlled gates and rotated
by parameterized rotation gates. This sequence of operations can be repeated multiple times with different parameters; each repetition is called a layer. The
qubits are then measured, and the results are decoded
into output information (Chen et al., 2020). The parameters in the layers are then updated with classical optimization algorithms, such as gradient descent
or stochastic gradient descent, to optimize the algorithm’s objective function. This optimization is performed step-by-step, with the parameters updated in
each iteration (PennyLane Team, 2022).
|0⟩
|0⟩
U(x)

V (θ)

|0⟩
|0⟩
Figure 1: The variational quantum circuit structure comprises three main parts: the U(x)-block represents the state
preparation part, the V (θ)-block constitutes the variational
part containing trainable parameters θ, and the quantum
measurement layer.

One of the notable advantages of such circuits is
their robustness to quantum noise (Chen et al., 2022),
which can be beneficial for NISQ era quantum computers (Preskill, 2018). Moreover, they have demonstrated significant success in various machine learning

areas, such as classification (Chen et al., 2021) and
natural language processing (Di Sipio et al., 2021).

3

RELATED WORK

Previous studies have explored various aspects of
QRL, such as developing quantum algorithms and circuits for policy-based and value-based methods, exploring the potential advantages of QRL over classical
RL, and implementing QRL in real-world problems
(Kwak et al., 2021a).
This work focuses on applying VQC to RL methods, which is why in this chapter, we will discuss related works in this area, specifically highlighting the
use of VQC.
One of the earliest works introducing the idea of
using VQC for RL is presented in (Chen et al., 2019).
The authors demonstrated a proof-of-principle using
a VQC to approximate the Q-value function in Deep
Q-Learning (DQN) algorithm and tested the quantum version of the DQN on discrete environments.
In work (Skolik et al., 2022), the authors extended
the previous achievements in DQN by exploring its
performance in continuous environments. Further improvements in DQN, such as Double DQN, have also
been implemented within the VQC architecture. For
example, in the study (Heimann et al., 2022), the authors applied QRL to solve a robot navigation task
using different VQC architectures with and without
data re-uploading. In addition, in (Lockwood and
Si, 2020), VQCs were used in both DQN and Double DQN algorithms in different continuous OpenAI
Gym environments.
Moreover, significant progress has been made
in developing policy-based methods frameworks for
QRL that directly focus on learning policy functions
(Jerbi et al., 2022). For example, the works (Jerbi
et al., 2021) (Sequeira et al., 2022) used a VQC for
the basic Monte Carlo policy gradient algorithm REINFORCE.
In the field of actor-critic methods, various research approaches utilize QRL and implement VQCs
as the actor and critic networks. For instance, the
authors of (Kwak et al., 2021b) proposed a policyVQC for the Proximal Policy Optimization algorithm,
where the neural network of the actor was replaced
with a VQC. Another study (Lan, 2021) introduced
a soft actor-critic algorithm that employs VQCs as
the policy function approximator. Notably, this study
demonstrated that quantum models could perform
better than classical models even with few parameters. Furthermore, a recent work (Chen, 2023) proposed a quantum version of the A3C algorithm as a

novel QRL approach. The author used a quantum
A3C algorithm incorporating multiple VQC-based
agents (both actor and critic) trained on different environments. The results showed that the proposed quantum algorithm outperformed traditional A3C in terms
of convergence speed and sample efficiency.
During the NISQ era, quantum computers have
limited qubits available, making it challenging to process large amounts of input data. A combination of
VQC and neural networks was employed to overcome
this challenge. For instance, in (Mari et al., 2020), a
pre-processing neural network was used to reduce the
input dimension to match the number of qubits in the
VQC, enabling efficient processing of image classification tasks. Similarly, in (Chen et al., 2021) (Chen
et al., 2022), tensor networks were used to reduce input dimensionality. Moreover, post-processing techniques such as neural network layers were used to
reshape the VQC output in (Chen, 2023) and (Sequeira et al., 2022) or to enhance the VQC’s expressive power for optimal results, as demonstrated in
(Hsiao et al., 2022) and (Meyer et al., 2022a).
Overall, the studies presented in this chapter
showcase the potential of VQC in various RL methods, including DQN, policy gradient algorithms, and
actor-critic methods. As a result, applying VQC to RL
has become a promising avenue for future research. In
this paper, we aim to build upon these advances by exploring the use of VQC in Actor-Critic methods and
investigating their performance in different problem
settings. We will assess the potential benefits and limitations of using VQC in RL problems and contribute
to the ongoing development of quantum approaches
in reinforcement learning.

4

Approaches

4.1

Advantage Actor-Critic Algorithm

The first step in the A2C algorithm (Algorithm 1) involves initializing both the actor-network πθ (s) and
the critic network Vω (s) with random parameters θ
and ω. The actor-network maps the current state of
the environment to an appropriate action, while the
critic network maps the current state to its corresponding state value. Then, initialize the stochastic gradient descent optimizer Adam (Kingma and Ba, 2014)
to update the parameters θ and ω throughout the optimization process.
Once the networks have been initialized, the algorithm enters a loop that iterates for each episode.
In each iteration t, the algorithm selects an action at
based on the current state s of the environment, as de-

Initialize the environment;
Initialize the actor-network πθ (s);
Initialize the critic-network Vω (s);
Initialize Adam optimizer for θ and ω;
for each episode do
Initialize next observation state st
env.reset();
Initialize done d = False;
for each iteration t do
Select an action at based on πθ (st );
Execute action at in the environment;
Observe reward rt , new state st+1 , and
done d;
if d = True then
break;
end
Calculate TD target value
y = rt + γVω (st+1 );
Calculate advantage A = y −Vω (st );
Update the actor-network by
minimizing the loss
Lactor (θ) = −A log πθ (at );
Update the critic-network by
minimizing the loss
Lcritic (ω) = (y −Vω (st ))2 ;
Update the state st = st+1 ;
end
end
Algorithm 1: Advantage Actor-Critic Algorithm (Mnih
et al., 2016)

termined by the actor-network πθ (s). This action is
executed in the environment, and the algorithm observes the resulting reward rt , the new state st+1 , and
the done status d, which indicates whether the episode
has ended.
The next step is calculating the TD target value y.
The advantage A is estimated from the TD error (difference between the TD target value y and the predicted state value). The advantage is then used to update the actor-network by minimizing the loss function Lactor (θ). and the critic-network Vω (s) by minimizing the loss function Lcritic (ω).
Finally, the state st of the environment is updated to the new state st+1 , and the algorithm repeats
the loop for the next iteration. This process continues until the maximum number of episodes has been
reached, or d is set to True, at which point the A2C
algorithm terminates.

4.2

Baselines

In this paper, to analyze and compare the performance
of quantum algorithms, we also implemented the clas-

sical A2C with different sized neural networks. The
classical benchmarks were created using architectures
that resemble the quantum model, leading to comparable model sizes. A2C with four hidden neurons is
compared to quantum A2C, while the one with five
hidden neurons is compared to hybrid A2C, described
in Section 4.3 and Section 4.4, respectively. Fig. 2
shows the different neural networks.
The neural network architecture consists of an input layer, a single hidden layer, and an output layer.
The input layer has four neurons that encode the 4dimensional observation state. For the actor-network,
the output layer has two neurons, while the critic network has one output neuron. We used a neural network with four neurons in the hidden layer to compare it with the VQC in the quantum A2C algorithm
and five neurons compared to the VQCs with postprocessing in the hybrid A2C algorithm. To ensure
a fair comparison between the classical and quantum approaches, we selected these specific numbers
of neurons in a hidden layer such that the number of
trainable parameters in both the classical and quantum
ansatz was roughly equivalent. This was done intentionally to ensure that any differences in performance
between the two approaches could be attributed to
their inherent differences rather than the number of
trainable parameters.
For the activation function in the hidden layer,
we used the Rectified Linear Unit (ReLU) activation
function (Agarap, 2018) for both actor and critic networks. ReLU is a commonly used activation function
in neural networks and helps solve the vanishing gradient problem. Additionally, in the actor-network, we
used a softmax activation function (Bridle, 1990) for
the output layer to obtain the probability distribution.

4.3

Quantum Advantage Actor-Critic
Algorithm

In this work, we employ a specific VQC architecture,
shown in Fig. 3, for both actor and critic quantum circuits in the QA2C and HA2C algorithms. As detailed
in Section 2.2, a VQC comprises three main components: an encoding layer, repeated variational layers,
and measurements.
The proposed VQC utilizes RX quantum gates to
encode the observed state of the environment into a
quantum state. For the Cart Pole problem, the observed state is four-dimensional, necessitating four
qubits in both the actor and critic circuits to represent
the state information.
Following the encoding process, a variational
layer is applied, repeated a specific number of times
n. In this particular implementation, we use n = 2

repetitions. Each variational layer consists of four
CNOT gates to entangle all qubits and three singlequbit gates, RZ (θi ), RY (φi ), and RZ (δi ), applied to
each qubit i (Kwak et al., 2021a). The parameters
θ, φ, and δ are iteratively optimized using the classical optimization algorithm Adam (Kingma and Ba,
2014).
Lastly, each qubit’s state is measured, and the
measurement outcomes are utilized to determine the
action for the actor and the state value for the critic in
the following steps of the A2C algorithm.
4.3.1

State Encoding

In this work, the RX gate is employed for encoding,
which acts on a single qubit and performs a rotation
around the x-axis of the Bloch sphere (Nielsen and
Chuang, 2010). Given that RX rotations are periodic
with a period of 2π, different values might map to
the same quantum state, leading to inaccurate predictions. To mitigate this issue, additional operations are
applied to the observed state variables to ensure the
parameters fall within the range of [−π, π].
As shown in Table 1, the first two variables o1 and
o2 have finite ranges, whereas the last two variables o3
and o4 have infinite ranges. Consequently, we establish separate normalization and transformation rules
for these two groups of variables.
For the cart’s position o1 and the pole’s angle o2 ,
which have finite ranges, the normalization procedure
consists of simple scaling using their respective minimum and maximum values provided in Table 1:
π
π
o1 , λ2 =
o2 ,
(4)
4.8
0.418
where λ1 and λ2 denote the transformed variables that
are input into the quantum circuit.
The normalization process is more intricate than
simple scaling for the cart’s velocity o3 and the pole’s
angular velocity o4 , which have infinite ranges. To
normalize these values, we first use the arctan function to map the infinite range to the finite interval
[−π/2, π/2] and then apply simple scaling to stretch
the interval to the desired range. The process can be
expressed as follows:
λ1 =

λ3 = 2 arctan o3 , λ4 = 2 arctan o4

(5)

where λ3 and λ4 represent the transformed variables
used in the quantum circuit.
4.3.2

Measurement and Action Selection of the
Quantum Actor

The actor’s VQC in the QA2C algorithm is designed
to convert the observed state into a quantum state and

(a) Actor neural network for
comparison with QA2C

(b) Critic neural network for
comparison with QA2C

(c) Actor neural network for
comparison with HQA2C

(d) Critic neural network for
comparison with HQA2C

Figure 2: Actor and critic neural networks architectures used in A2C implementations as classical benchmarks against the
QA2C and HA2C algorithms

|0⟩

RX (λ1 )

RZ (θ1 )

RY (φ1 )

RZ (δ1 )

|0⟩

RX (λ2 )

RZ (θ2 )

RY (φ2 )

RZ (δ2 )

|0⟩

RX (λ3 )

RZ (θ3 )

RY (φ3 )

RZ (δ3 )

|0⟩

RX (λ4 )

RZ (θ4 )

RY (φ4 )

RZ (δ4 )

a measure of the quality of a given state for the agent.
The quantum circuit used in this approach is detailed
in Section 2.2. As only a single value is needed for
the estimation, the quantum critic measures just the
first qubit and utilizes the probability of measuring 0
as the estimated state value.

VARIATIONAL LAYER

Figure 3: VQC architecture utilized by QA2C and HA2C
algorithms

predict the optimal action to take. As described in
Section 2.2, we use a VQC comprising four qubits and
two variational layers. At the end of the circuit, the
qubits are measured, and the probability of measuring
0 is employed to determine the actor’s action. Given
that the action space in the Cart Pole environment is
two-dimensional, only the measurement values of the
first two qubits are considered.
To convert these values into a probability distribution, we apply the softmax function, which maps the
input vector (in this case, the probabilities of the first
two qubits) to a probability distribution that sums to 1
(Bridle, 1990). The softmax function is defined as:
exi
(6)
∑ j ex j
where xi represents the i-th element of the input vector, e denotes Euler’s number, and the sum is taken
over all elements in the input vector. After obtaining
the probability distribution, the actor selects its action
stochastically by randomly choosing an action from
the probability distribution, based on the probabilities
assigned to each action.
softmax(xi ) =

4.3.3

Measurement in Quantum Critic

In the QA2C algorithm, the quantum critic employs a
VQC to estimate the value function, which serves as

4.4

Hybrid Quantum Advantage
Actor-Critic Algorithm

In this section, we explore the potential of integrating VQCs and neural networks within the A2C algorithm. As discussed in Section 2.2, neural networks
can be employed in conjunction with VQCs as preprocessing and post-processing layers (Chen, 2023).
In the VQC architecture depicted in Fig. 3, we
measured at most two qubits for the actor and one
qubit for the critic. With the proposed hybrid architecture, we expand the VQC by incorporating a postprocessing neural network layer. This modification
allows us to measure all four qubits and scale the
VQC output to the desired measurement size. For the
actor, we reduce the output from four values to two,
corresponding to the action space of the environment.
In the case of the critic, we reduce the output from
four values to a single value needed for the state value
function.
We refer to the proposed architecture as the Hybrid Advantage Actor-Critic (HA2C), which will be
applied to the QA2C algorithm in three different approaches. The first approach entails replacing the
critic network with a hybrid VQC while retaining the
neural network for the actor. The second approach
involves substituting the actor-network with a hybrid
VQC while keeping the critic as a neural network. Finally, we replace both the actor and critic neural networks with hybrid VQCs.

(a) Hybrid VQC used as hybrid actor

(b) Hybrid VQC used as hybrid critic

Figure 4: Hybrid VQC-based hybrid actor and critic models

4.4.1

Hybrid Quantum Actor

The hybrid actor in the HA2C algorithm combines a
VQC, as described earlier in Section 2.2, with a postprocessing single-layer neural network, as illustrated
in Fig. 4a. The measurements obtained from the VQC
are used as inputs for the post-processing layer. The
neural network has four inputs and two outputs, enabling the use of measurements from all the qubits,
rather than just two, as in the quantum actor in QA2C.
The softmax activation function is employed at the
neural network output to generate the probability distribution for the actor.
4.4.2

Hybrid Quantum Critic

Similarly, the hybrid critic model comprises a combination of a VQC and a post-processing neural network
layer, as depicted in Fig. 4b. The quantum circuit produces measurements that are subsequently used as input for the neural network. The critic’s output is a single value, the state function, which means the neural
network also has only one output. This setup allows
the use of measurements from all four qubits in the
VQC to estimate the state value, providing the neural
network with a comprehensive set of information to
process.

5

EXPERIMENTAL SETUP

This paper investigates the classical A2C algorithm
with two distinct implementations, each utilizing two
neural networks, as described in Section 4.2 on the
CartPole environment. The first implementation employs four neurons in its hidden layer, while the second uses five neurons, as shown in Fig. 2. Additionally, we examine the quantum versions of A2C, which
includes two main architectures: QA2C and HA2C,
as explained in Section 4.3 and Section 4.4, respectively. Finally, for both architectures, we present three
different actor-critic implementations.
In the first implementation, we replaced the neural network for the critic with a VQC in QA2C and a

VQC with post-processing in HA2C, while the actor
remained a neural network. We refer to this as the Advantage Actor-Quantum-Critic (A2Q) algorithm. The
second implementation involved using a VQC for the
actor in QA2C and a VQC with post-processing in
HA2C, while the critic remained a neural network.
We refer to this as the Advantage Quantum-ActorCritic (Q2C) algorithm.
Finally, in the third implementation, we replaced
both the actor and critic neural networks with a VQC
in QA2C and a VQC with post-processing in HA2C.
We refer to this as the Advantage Quantum-ActorQuantum-Critic (Q2Q) algorithm.
The classical actor and critic in QA2C and HA2C
used a neural network with four and five neurons in
the hidden layer, respectively. Furthermore, in all
experiments, the QA2C and HA2C algorithms were
benchmarked against a classical A2C algorithm with
four and five neurons in its hidden layer, respectively.
Classical neural networks for actor and critic were
implemented using the popular deep learning library
PyTorch (Paszke et al., 2019), and we used PennyLane (Bergholm et al., 2018), a widely-used quantum
machine learning library, to implement VQCs. The
experiments were conducted on the compute cloud
provided by Leibniz-Rechenzentrum der Bayerischen
Akademie der Wissenschaften (LRZ), which consists
of one Intel(R) Xeon(R) Platinum 8160 CPU.

5.1

Cart Pole

The Cart Pole testing environment provided by OpenAI (Brockman et al., 2016) serves as a popular
benchmark for evaluating the performance of RL algorithms. As depicted in Fig. 5, the environment consists of a cart moving along a horizontal track and a
pole attached to the cart by a hinge joint. The agent’s
objective is to balance the pole by controlling the
cart’s position, preventing the pole from falling over.
Several factors contribute to the Cart Pole environment’s suitability as a testing environment. Firstly,
its simplicity as a control problem makes it an ideal
setting for testing new QRL algorithms. Secondly, it

5.2

Figure 5: The Cart Pole environment (Brockman et al.,
2016)

poses a relatively challenging task, as the agent must
learn to balance the pole over an extended sequence
of actions. Finally, the environment features a welldefined reward signal, allowing for easy evaluation
and comparison of different algorithms.
The observed state consists of four state variables:
the position and velocity of the cart, and the angle and
angular velocity of the pole (Gymlibrary, 2022). Table 1 summarizes the state variables and their respective minimum and maximum values. It is crucial to
note that the Cart Pole’s observed state is continuous,
meaning that state variables can take any real value
within a specific range. This aspect makes learning a
policy more challenging since the state space is considerably larger than in environments with discrete
states.
Var.
o1
o2
o3
o4

Description
Cart Position
Pole Angle
Cart Velocity
Pole Angular
Velocity

Min. Value
-4.8
-0.418
-Inf
-Inf

Max. Value
4.8
0.418
Inf
Inf

Table 1: Observed state variables in the Cart Pole environment

The RL system’s goal is to maintain the pole upright by applying actions to the cart, either moving
it to the left or the right. The reward function corresponds to the time elapsed while balancing the pole.
A positive reward is given at each time step as long
as the pole remains upright. The system is considered to have failed if the pole falls over (pole angle
not within the range of ±12°) or if the cart moves too
far away from the center (greater than ±2.4), causing the episode to terminate (Gymlibrary, 2022). On
the other hand, the system is deemed successful if the
pole remains upright for 500 steps. This reward function encourages the RL agent to keep the pole upright
for as long as possible.

Hyperparameters and Model Size

Hyperparameters, such as learning rates and discount
factors, play a crucial role in determining the performance of RL algorithms (Henderson et al., 2017).
Therefore, we conducted a small-scale hyperparameter tuning study to find a suitable learning rate α and
discount factor γ for the classical A2C, QA2C, and
HA2C algorithms. Based on the results, we selected
a learning rate α = 1 × 10−4 to be used in the Adam
optimizer and a discount factor γ = 0.99 for all algorithms. All runs were executed on nodes with Intel(R)
Core(TM) i5-4570 CPU @ 3.20GHz.
Our proposed VQC for the QA2C and HA2C algorithms is explained in Section 2.2 and visualized
in Fig. 3. The VQC employs four qubits and two
variational layers, with each layer consisting of three
single-qubit rotations, resulting in a total of 24 quantum parameters to be optimized. To ensure fair comparisons between the classical A2C and the quantum
algorithms, we implemented the A2C algorithm with
four and five neurons in the hidden layer. The exact
number of parameters for all QA2C and HA2C versions are shown in Table 2 in (a) and (b), respectively,
along with the corresponding classical A2C benchmark.
A2C4
A2Q
Q2C
Q2Q

Actor
30
30
24
24

Critic
25
24
25
24

Total
55
54
49
48

(a) QA2C model parameters

A2C5
HA2Q
HQ2C
HQ2Q

Actor
37
37
34
34

Critic
31
29
31
29

Total
68
66
65
63

(b) HA2C model parameters
Table 2: Model parameters: The total number of parameters for QA2C and HA2C algorithms is presented in tables (a) and (b) respectively, while highlighting the exact
number of parameters for the actor and critic. The first row
displays the number of parameters for the classical A2C algorithm, while the subsequent rows show the number of parameters for the three versions of QA2C and HA2C algorithms.

6

RESULTS

The first experiment aimed to compare the performance of three versions of the QA2C algorithm with
the classical A2C algorithm, which had four neurons
in the hidden layer. All algorithms were trained on
10 runs, each consisting of 450,000 and 1,000,000
steps in Fig. 6 and Fig. 7, respectively. The x-axis denotes the steps, the y-axis shows the average reward
obtained for all runs in each step, and the shaded region represents the standard deviation of the results.
Figure 7: HA2C and classical A2C performance in Cart
Pole

Figure 6: QA2C and classical A2C performance in Cart
Pole

The results revealed that none of the proposed
quantum architectures, namely A2Q, Q2C, and Q2Q,
could learn the Cart Pole environment across all
runs. In contrast, classical A2C demonstrated a stable
learning curve until a sudden drop at around 400,000
steps reaching a reward of 73, as shown in Fig. 6.
However, the reward threshold for the Cart Pole environment is 475, which the classical A2C’s average
reward did not attain, indicating its inability to solve
the environment across all runs.
The goal of the second experiment was to improve
the architecture of the VQC by combining it with a
post-processing neural network in order to achieve
better results. Fig. 7 shows the performance of the
classical A2C with 5 neurons in the hidden layer and
the three versions of the HA2C algorithm: HA2Q,
HQ2C, and HQ2Q.
The aim was to train both algorithms on 10 runs,
each with 1,000,000 steps in the Cart Pole environment. In Fig. 7, we can see an evident success for
the HA2C algorithms over classical A2C, especially
in HA2Q and HQ2Q. The proposed hybrid architecture learned the task in almost every run, while the
classical A2C succeeded in only 5 of the 10 runs.
Two additional VQCs with different single-qubit rotations were also tested in a pre-study for the QA2C
and HA2C algorithms. However, the other QA2C algorithms also failed to learn.

In conclusion, the experiments demonstrated that
the hybrid quantum-classical approach could achieve
better results than the classical A2C algorithm with
5 neurons in the hidden layer. Specifically, the HA2C
algorithms, HA2Q and HQ2Q, showed significant improvement over the classical A2C algorithm, learning
the Cart Pole environment in almost every run. This
highlights the potential of combining quantum and
classical architectures to enhance the performance of
reinforcement learning algorithms.
However, it is essential to consider the significantly longer training time for the quantum algorithms compared to the classical A2C algorithm. This
performance gap emphasizes the need for further research and optimization of quantum algorithms to reduce training time and improve their applicability in
real-world scenarios.
Future work could involve exploring alternative
quantum architectures or optimization methods to enhance performance and reduce training time. Additionally, more complex environments and tasks could
be considered to further investigate the potential of
quantum-classical hybrid approaches in reinforcement learning.

6.1

Discussion

After conducting experiments on the Cart Pole environment, it became evident that the pure quantum
A2C algorithm did not effectively learn the task. After looking at the average gradients (−0.000056), we
concluded that this was caused by vanishing gradients. Both hybrid approaches, A2Q and Q2C suffer
from the same problem using the quantum actor and
quantum critic. Additionally, like with any machine
learning algorithm, the performance of VQCs can be
influenced by several factors, including hyperparameter settings, circuit structure, and task complexity. To
improve the performance of the quantum algorithm,
future work could explore techniques such as circuit

ansatz design or gradient-free optimization to mitigate this issue (McClean et al., 2018; Chen et al.,
2022). In summary, the quantum approach did not
provide any significant advantage over classical methods.
Building upon that, we employ VQCs with classical post-processing to circumvent barren plateaus.
This proved to be more effective in addressing the
challenges of the Cart Pole environment than the classical A2C algorithm, as the hybrid quantum A2C substantially outperforms the classical A2C in learning
the environment. The VQC uses a post-processing
neural network, which may be crucial for enabling
the VQC to learn. Notably, both HA2Q and HQ2C
started to learn the task immediately, while HQ2Q
learned at a slower rate.
These experiments were conducted on a quantum simulator since current quantum hardware is not
widely available at the time of writing. This circumstance leads to significantly higher training times for
all tested quantum approaches than the classical baseline. Thus, without access to an actual quantum device, there is currently no real benefit to the quantum
approaches. Even with exclusive access to real quantum hardware, we can not say for certain if a quantum
parameter trains as fast as a classical one. It is essential to recognize that the field of quantum computing and quantum machine learning is still in its early
stages, with much research needed to gain a deeper
understanding of the capabilities and limitations of
VQCs in machine learning. As such, further exploration is necessary to unlock the full potential of this
emerging technology.

7

CONCLUSION

In this work, we investigated how quantum computing techniques could enhance the performance of the
A2C algorithm. To achieve this goal, we conducted
experiments and compared the performance of three
variations of the A2C algorithm: classical, quantum,
and hybrid A2C. In each variation, we replaced either the actor, critic, or both with a quantum circuit,
leading to a total of three different configurations. By
testing these configurations, we aimed to understand
the impact of each variation on the algorithm’s overall
performance. Furthermore, ensuring a fair comparison between the algorithms was a significant challenge in this study. Therefore, to maintain fairness,
we kept the number of parameters in the algorithms
roughly equal.
Our results show that the classical A2C outperforms the pure quantum A2C. To improve the quan-

tum A2C performance, we introduced a hybrid approach integrating a VQC with a post-processing neural network layer. We tested three configurations of
the hybrid algorithm and found that it substantially
outperformed both the quantum and classical counterparts. This paper contributes to the growing body
of evidence highlighting the potential of combining
quantum computing and classical machine learning
algorithms to improve reinforcement learning tasks’
performance.
However, further research is still necessary. For
example, tuning hyperparameters is crucial to achieving high performance in RL and QRL algorithms. In
future experiments, different VQC architectures could
be used to determine if the algorithm’s performance
can be further improved. One such technique is data
re-uploading, which was not utilized in our models
(Pérez-Salinas et al., 2020). Incorporating data reuploading may unlock additional performance gains
and may be worth investigating in future studies. Another approach that could be explored is experimenting with different encoding strategies, such as amplitude encoding (Schuld and Petruccione, 2018). To
improve convergence, one can also employ weight remapping to the quantum circuits (Kölle et al., 2023).
In addition, pre-processing neural networks could be
explored to enhance the algorithm’s performance. By
testing a range of VQC architectures, we may identify
those that are better suited to specific RL problems
and lead to improved performance. Finally, it would
be interesting to explore possible quantum variations
of other state-of-the-art actor-critic algorithms, such
as A3C or Proximal Policy Optimization and explore
their use on different problem tasks.

ACKNOWLEDGEMENTS
This research is part of the Munich Quantum Valley,
which is supported by the Bavarian state government
with funds from the Hightech Agenda Bayern Plus.

REFERENCES
Agarap, A. F. (2018). Deep learning using rectified linear
units (relu). arXiv preprint arXiv:1803.08375.
Andrychowicz, M., Raichuk, A., Stańczyk, P., Orsini, M.,
Girgin, S., Marinier, R., Hussenot, L., Geist, M.,
Pietquin, O., Michalski, M., et al. (2021). What matters for on-policy deep actor-critic methods? a largescale study. In International conference on learning
representations.
Bauer, B., Bravyi, S., Motta, M., and Chan, G. K.-L.
(2020). Quantum Algorithms for Quantum Chemistry

and Quantum Materials Science. Chemical Reviews,
120(22):12685–12717. Publisher: American Chemical Society.
Bergholm, V., Izaac, J., Schuld, M., Gogolin, C., Ahmed,
S., Ajith, V., Alam, M. S., Alonso-Linaje, G., AkashNarayanan, B., Asadi, A., Arrazola, J. M., Azad,
U., Banning, S., Blank, C., Bromley, T. R., Cordier,
B. A., Ceroni, J., Delgado, A., Di Matteo, O., Dusko,
A., Garg, T., Guala, D., Hayes, A., Hill, R., Ijaz,
A., Isacsson, T., Ittah, D., Jahangiri, S., Jain, P.,
Jiang, E., Khandelwal, A., Kottmann, K., Lang, R. A.,
Lee, C., Loke, T., Lowe, A., McKiernan, K., Meyer,
J. J., Montañez-Barrera, J. A., Moyard, R., Niu, Z.,
O’Riordan, L. J., Oud, S., Panigrahi, A., Park, C.Y., Polatajko, D., Quesada, N., Roberts, C., Sá, N.,
Schoch, I., Shi, B., Shu, S., Sim, S., Singh, A.,
Strandberg, I., Soni, J., Száva, A., Thabet, S., VargasHernández, R. A., Vincent, T., Vitucci, N., Weber, M.,
Wierichs, D., Wiersema, R., Willmann, M., Wong,
V., Zhang, S., and Killoran, N. (2018). Pennylane:
Automatic differentiation of hybrid quantum-classical
computations.
Biamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe,
N., and Lloyd, S. (2017). Quantum machine learning.
Nature, 549(7671):195–202.
Bridle, J. S. (1990). Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Soulié, F. F.
and Hérault, J., editors, Neurocomputing, pages 227–
236, Berlin, Heidelberg. Springer Berlin Heidelberg.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.
Cao, Y., Romero, J., Olson, J. P., Degroote, M., Johnson,
P. D., Kieferová, M., Kivlichan, I. D., Menke, T.,
Peropadre, B., Sawaya, N. P. D., Sim, S., Veis, L.,
and Aspuru-Guzik, A. (2019). Quantum chemistry in
the age of quantum computing. Chemical Reviews,
119(19):10856–10915.
Cerezo, M., Arrasmith, A., Babbush, R., Benjamin, S. C.,
Endo, S., Fujii, K., McClean, J. R., Mitarai, K., Yuan,
X., Cincio, L., and Coles, P. J. (2020). Variational
quantum algorithms. Nature Reviews Physics, 3:625
– 644.
Chen, S. Y.-C. (2023). Asynchronous training of quantum
reinforcement learning.
Chen, S. Y.-C., Huang, C.-M., Hsing, C.-W., Goan, H.-S.,
and Kao, Y.-J. (2022). Variational quantum reinforcement learning via evolutionary optimization. Machine
Learning: Science and Technology, 3(1):015025.
Chen, S. Y.-C., Huang, C.-M., Hsing, C.-W., and Kao, Y.J. (2021). An end-to-end trainable hybrid classicalquantum classifier. Machine Learning: Science and
Technology, 2(4):045021.
Chen, S. Y.-C., Yang, C.-H. H., Qi, J., Chen, P.-Y., Ma, X.,
and Goan, H.-S. (2019). Variational quantum circuits
for deep reinforcement learning.
Chen, S. Y.-C., Yang, C.-H. H., Qi, J., Chen, P.-Y., Ma,
X., and Goan, H.-S. (2020). Variational quantum circuits for deep reinforcement learning. IEEE Access,
8:141007–141024.

Di Sipio, R., Huang, J.-H., Chen, S. Y.-C., Mangini, S., and
Worring, M. (2021). The dawn of quantum natural
language processing.
Dral, P. O. (2020). Quantum Chemistry in the Age of
Machine Learning. The Journal of Physical Chemistry Letters, 11(6):2336–2347. Publisher: American
Chemical Society.
Farhi, E., Goldstone, J., and Gutmann, S. (2014).
A Quantum Approximate Optimization Algorithm.
arXiv:1411.4028 [quant-ph]. arXiv: 1411.4028.
Farhi, E. and Harrow, A. W. (2016). Quantum supremacy
through the quantum approximate optimization algorithm.
Gymlibrary, F. F. (2022). Cart pole - gym documentation.
Heimann, D., Hohenfeld, H., Wiebe, F., and Kirchner, F.
(2022). Quantum deep reinforcement learning for
robot navigation tasks.
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,
D., and Meger, D. (2017). Deep reinforcement learning that matters.
Homeister, M. (2018). Quantum Computing verstehen:
Grundlagen – Anwendungen – Perspektiven. Computational Intelligence. Springer Fachmedien Wiesbaden.
Hsiao, J.-Y., Du, Y., Chiang, W.-Y., Hsieh, M.-H., and
Goan, H.-S. (2022). Unentangled quantum reinforcement learning agents in the openai gym.
Jerbi, S., Cornelissen, A., Ozols, M., and Dunjko, V. (2022).
Quantum policy gradient algorithms.
Jerbi, S., Gyurik, C., Marshall, S. C., Briegel, H. J., and
Dunjko, V. (2021). Parametrized quantum policies for
reinforcement learning.
Kingma, D. P. and Ba, J. (2014). Adam: A method for
stochastic optimization.
Kober, J., Bagnell, J. A., and Peters, J. (2013). Reinforcement learning in robotics: A survey. The International
Journal of Robotics Research, 32(11):1238–1274.
Konda, V. and Tsitsiklis, J. (1999). Actor-critic algorithms. In Solla, S., Leen, T., and Müller, K., editors,
Advances in Neural Information Processing Systems,
volume 12. MIT Press.
Kwak, Y., Yun, W. J., Jung, S., Kim, J.-K., and Kim,
J. (2021a). Introduction to quantum reinforcement
learning: Theory and pennylane-based implementation. In 2021 International Conference on Information and Communication Technology Convergence
(ICTC), pages 416–420.
Kwak, Y., Yun, W. J., Jung, S., Kim, J.-K., and Kim,
J. (2021b). Introduction to quantum reinforcement
learning: Theory and pennylane-based implementation.
Kölle, M., Giovagnoli, A., Stein, J., Mansky, M. B., Hager,
J., and Linnhoff-Popien, C. (2023). Improving convergence for quantum variational classifiers using weight
re-mapping.
Lan, Q. (2021). Variational quantum soft actor-critic.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,
Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning.

Lockwood, O. and Si, M. (2020). Reinforcement learning
with quantum variational circuits.
Mari, A., Bromley, T. R., Izaac, J., Schuld, M., and Killoran, N. (2020). Transfer learning in hybrid classicalquantum neural networks. Quantum, 4:340.
McClean, J. R., Boixo, S., Smelyanskiy, V. N., Babbush,
R., and Neven, H. (2018). Barren plateaus in quantum
neural network training landscapes. Nature Communications, 9(1).
Meyer, N., Scherer, D. D., Plinge, A., Mutschler, C., and
Hartmann, M. J. (2022a). Quantum policy gradient
algorithm with optimized action decoding.
Meyer, N., Ufrecht, C., Periyasamy, M., Scherer, D. D.,
Plinge, A., and Mutschler, C. (2022b). A survey on
quantum reinforcement learning.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley,
T., Lillicrap, T. P., Silver, D., and Kavukcuoglu,
K. (2016). Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on International Conference
on Machine Learning - Volume 48, ICML’16, page
1928–1937. JMLR.org.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller,
M., Fidjeland, A. K., Ostrovski, G., Petersen, S.,
Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D.
(2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529–533. Number:
7540 Publisher: Nature Publishing Group.
Nielsen, M. and Chuang, I. (2010). Quantum Computation
and Quantum Information: 10th Anniversary Edition.
Cambridge University Press.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S.,
Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019).
Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information
Processing Systems 32, pages 8024–8035. Curran Associates, Inc.
PennyLane Team, X. (2022). Variational circuit - pennylane.
Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., and
Latorre, J. I. (2020). Data re-uploading for a universal
quantum classifier. Quantum, 4:226.
Pirandola, S., Andersen, U. L., Banchi, L., Berta, M.,
Bunandar, D., Colbeck, R., Englund, D., Gehring, T.,
Lupo, C., Ottaviani, C., Pereira, J. L., Razavi, M.,
Shaari, J. S., Tomamichel, M., Usenko, V. C., Vallone,
G., Villoresi, P., and Wallden, P. (2020). Advances in
quantum cryptography. Advances in Optics and Photonics, 12(4):1012.
Preskill, J. (2018). Quantum computing in the NISQ era
and beyond. Quantum, 2:79.
Schuld, M. and Petruccione, F. (2018). Supervised Learning with Quantum Computers. Springer Publishing
Company, Incorporated, 1st edition.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. (2017). Proximal policy optimization algorithms.
Sequeira, A., Santos, L. P., and Barbosa, L. S. (2022). Policy gradients using variational quantum circuits.
Shor, P. W. (1997). Polynomial-time algorithms for prime
factorization and discrete logarithms on a quantum
computer. SIAM Journal on Computing, 26(5):1484–
1509.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker,
L., Lai, M., Bolton, A., Chen, Y., Lillicrap,
T., Hui, F., Sifre, L., van den Driessche, G.,
Graepel, T., and Hassabis, D. (2017).
Mastering the game of Go without human knowledge. Nature, 550(7676):354–359. Bandiera abtest:
a Cg type: Nature Research Journals Number:
7676 Primary atype: Research Publisher: Nature Publishing Group Subject term: Computational
science;Computer science;Reward Subject term id:
computational-science;computer-science;reward.
Skolik, A., Jerbi, S., and Dunjko, V. (2022). Quantum
agents in the gym: a variational quantum algorithm
for deep q-learning. Quantum, 6:720.
Sutton, R. and Barto, A. (2018). Reinforcement Learning,
second edition: An Introduction. Adaptive Computation and Machine Learning series. MIT Press.
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y.
(1999). Policy gradient methods for reinforcement
learning with function approximation. In Solla, S.,
Leen, T., and Müller, K., editors, Advances in Neural Information Processing Systems, volume 12. MIT
Press.
You, Y., Pan, X., Wang, Z., and Lu, C. (2017). Virtual to
real reinforcement learning for autonomous driving.
CoRR, abs/1704.03952.
Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and
Wang, M. (2020). Variational policy gradient method
for reinforcement learning with general utilities.

