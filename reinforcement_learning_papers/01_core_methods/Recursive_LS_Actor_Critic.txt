IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

1

Recursive Least Squares Advantage
Actor-Critic Algorithms

arXiv:2201.05918v2 [cs.LG] 14 Feb 2022

Yuan Wang, Chunyuan Zhang, Tianzong Yu, Meng Ma

Abstract‚ÄîAs an important algorithm in deep reinforcement
learning, advantage actor critic (A2C) has been widely succeeded
in both discrete and continuous control tasks with raw pixel
inputs, but its sample efficiency still needs to improve more.
In traditional reinforcement learning, actor-critic algorithms
generally use the recursive least squares (RLS) technology to
update the parameter of linear function approximators for
accelerating their convergence speed. However, A2C algorithms
seldom use this technology to train deep neural networks (DNNs)
for improving their sample efficiency. In this paper, we propose
two novel RLS-based A2C algorithms and investigate their
performance. Both proposed algorithms, called RLSSA2C and
RLSNA2C, use the RLS method to train the critic network and
the hidden layers of the actor network. The main difference
between them is at the policy learning step. RLSSA2C uses an
ordinary first-order gradient descent algorithm and the standard
policy gradient to learn the policy parameter. RLSNA2C uses
the Kronecker-factored approximation, the RLS method and the
natural policy gradient to learn the compatible parameter and
the policy parameter. In addition, we analyze the complexity
and convergence of both algorithms, and present three tricks
for further improving their convergence speed. Finally, we
demonstrate the effectiveness of both algorithms on 40 games
in the Atari 2600 environment and 11 tasks in the MuJoCo
environment. From the experimental results, it is shown that our
both algorithms have better sample efficiency than the vanilla
A2C on most games or tasks, and have higher computational
efficiency than other two state-of-the-art algorithms.
Index Terms‚ÄîDeep reinforcement learning (DRL), advantage
actor-critic (A2C), recursive least squares (RLS), standard policy
gradient (SPG), natural policy gradient (NPG).

I. I NTRODUCTION
Reinforcement learning (RL) is an important machine learning methodology for solving sequential decision-making problems. In RL, the agent aims to learn an optimal policy for
maximizing the cumulative return by interacting with the
initially unknown environment [1]. For the past 40 years,
RL has roughly experienced three historical periods, namely,
tabular representation RL (TRRL), linear function approximation RL (LFARL) and deep RL (DRL). TRRL can only
solve a few simple problems with small-scale discrete state
and action spaces. LFARL can only solve some control tasks
with low-dimensional continuous state and action spaces since
its approximation capability is still limited [2]. In recent years,
by combining with various deep neural networks (DNNs),
This work was supported by the National Natural Science Foundation of
China under Grant 61762032 and Grant 11961018. (Corresponding author:
Chunyuan Zhang).
Y. Wang, C. Zhang, T. Yu and M. Ma are with the School of
Computer Science and Technology, Hainan University, Haikou 570228,
China (email: 20085400210073@hainanu.edu.cn; 990721@hainanu.edu.cn;
20081200210007@hainanu.edu.cn; 20085400210053@hainanu.edu.cn).

DRL has shown a huge potential to solve real-world complex
problems [3]-[6] and has received more and more research
interest. Similar to LFARL classified in [7], DRL can also be
classified into three categories: deep value function approximation (DVFA) [8], [9], deep policy search (DPS) [10] and deep
actor-critic (DAC) methods [11]-[17]. DAC algorithms can be
viewed as a hybrid of DVFA and DPS. They generally have
a critic for policy evaluation and an actor for policy learning.
Among three classes of DRL methods, DAC is more effective
than DVFA and DPS for online learning real-world problems,
and thus has received extensive attention.
In recent years, many novel DAC algorithms have been
proposed. According to the policy type used in the actor, they
can be roughly divided into two main subclasses. One subclass
is the DAC algorithms with the deterministic policy. The actorcritic algorithms with the deterministic policy gradient (DPG)
are first proposed in [11]. Based on this work, a few variants,
such as deep DPG (DDPG) [12], recurrent DPG (RDPG) [13]
and twin delayed DDPG (TD3) [14], have been suggested
recently, but they can solve only continuous control tasks.
The other subclass is the DAC algorithms with the stochastic
policy gradient. In this subclass, the most famous algorithm
is perhaps the asynchronous advantage actor-critic (A3C)
algorithm [15]. A3C employs multiple workers to interacting
with each own environment in parallel, uses the accumulated
samples of each worker to update the shared model and uses
the parameters of the shared model to update the model
of each worker asynchronously. Compared with DPG-type
DAC algorithms, A3C can solve both discrete and continuous
control tasks without the experience replay. However, A3C
doesn‚Äôt work better than its synchronized version [18], namely
the synchronous advantage actor-critic (A2C) algorithm [15],
which uses all samples obtained by each worker to update the
shared model and uses the shared model to select the action of
each worker synchronously. A2C is easier to implement and
has become a baseline algorithm in OpenAI. Therefore, we
will focus on A2C in this paper.
How to improve the sample efficiency of the agent is an
open issue in RL. Although A2C and A3C are very excellent,
their sample efficiency still needs to improve more. In recent
years, there have been some studies such as combining with
the experience replay [19], [20] and the entropy maximization [21] to tackle this problem. Here we mainly focus on
another way in which researchers try to use more advanced
optimization methods for gradient updates. Schulman et al.
propose the proximal policy optimization (PPO) [16], which
uses a novel objective with clipped probability ratios and
forms a pessimistic estimate of the performance of the policy.

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

Compared with the trust region policy optimization (TRPO)
[17], PPO is much simpler to implement. Byun et al. propose
the proximal policy gradient (PPG) [22], which shows that
the performance similar to PPO can be obtained by using the
gradient formula from the original policy gradient theorem.
Wu et al. propose the actor critic using Kronecker-factored
trust region (ACKTR), which uses a Kronecker-factored approximation [23] to the natural policy gradient (NPG) that
allows the covariance matrix of the gradient to be inverted
efficiently [24]. These algorithms are more effective than the
vanilla A2C or A3C with ordinary first-order optimization
algorithms, but have higher computational complexities and
run slowly. By using Kostrikov‚Äôs source code [25] to test on
Atari games, we find that PPO and ACKTR are only 1/10 and
7/12 as fast as A2C with RMSProp [26].
In LFARL, traditional actor-critic algorithms usually use the
recursive least squares (RLS) method to improve their convergence performance. In 1996, Bradtke and Barto first propose
the least squares temporal difference (LSTD) algorithm and
define a recursive version (namely RLSTD) [27]. In 1999,
Konda and Tsitsiklis first propose a class of two-time-scale
actor-critic algorithms with linear function approximations,
and point out that it is possible to use LSTD for policy
evaluation [28]. After that, Xu et al. propose an actor-critic
algorithm by using the RLSTD(Œª) algorithm as the critic
[29]. In 2003, Peter et al. first propose the natural actor-critic
(NAC) algorithm, which uses the LSTD-Q(Œª) algorithm for
policy evaluation and uses NPG for policy learning [30]. On
this basis, Park et al. propose the RLS-based NAC algorithm
[31], and Bhatnagar et al. provide four actor-critic algorithms
as well as their convergence proofs [32]. There are some
other RLS-based actor-critic algorithms such as KDHP [7] and
CIPG [33]. RLS has been the baseline optimization method
for traditional actor-critic algorithms. However, to the best of
our knowledge, there aren‚Äôt any RLS-based DAC algorithms
to be proposed, since the DNN approxiamtion is much more
complicated than the linear function approximation. In our
previous work [34], we propose a class of RLS optimization
algorithms for DNNs and validate their effectiveness on some
classification benchmark datasets. Whereas, there is a big
difference between RL and supervised learning, and there are
still some obstacles we need to overcome.
In this paper, we try to introduce the RLS optimization
into the A2C algorithm. We propose two RLS-based A2C
algorithms, called RLSSA2C and RLSNA2C, respectively.
Both of them use the same loss function as the vanilla A2C,
and employ the RLS method to optimize their critic networks
and the hidden layers of their actor networks. The main
difference between them is the policy learning. RLSSA2C
uses the standard policy gradient (SPG) and an ordinary
first-order gradient descent algorithm to update the policy
parameters, and RLSNA2C uses the NPG, the Kroneckerfactored approximation and the RLS method to learn the
compatible parameter and the policy parameters. We show
that their computational complexities have the same order as
the vanilla A2C with the first-order optimization algorithm.
In addition, we also provide some tricks for accelerating their
convergence speed. Finally, we demonstrate their effectiveness

2

on 40 games in the Atari 2600 environment and 11 tasks in
the MuJoCo environment. Experimental results show that both
RLSSA2C and RLSNA2C have better sample efficiency than
the vanilla A2C on most games. Furthermore, they can achieve
a faster running speed than PPO and ACKTR.
The rest of this paper is organized as follows. In Section
II, we introduce some background knowledge. In Section III,
we present the detail derivation of our proposed algorithms.
In Section IV, we analyze the computational complexity and
convergence of our proposed algorithms, and provide three
tricks for further improving their convergence speed and
solution. In Section V, we demonstrate the effectiveness of
our proposed algorithms on Atari games and MuJoCo tasks.
Finally, we conclude our work in Section VI.
II. BACKGROUND
In this section, we briefly review Markov decision processes
(MDPs), stochastic policy gradient, convolutional neural networks (CNNs) and the vanilla A2C algorithm. In addition, we
also introduce some notations used in this paper.
A. Markov Decision Process
In RL, a sequential decision-making problem is generally
formulated into a Markov decision process (MDP) defined as
hS, A, p, r, Œ≥i, where S is the state space, A is the action
space, p(s0t |st , at ) ‚àà [0, 1] and rt ‚àà R are the state-transition
probability distribution and the immediate reward from the
state st to the next state s0t by taking the action at at time step
t, and Œ≥ ‚àà (0, 1] is the discount factor. The action at is selected
by a policy, which can be either stochastic or deterministic. In
this paper, we focus on the former and use a tensor or matrix
Œò to parameterize it. A stochastic policy œÄ(at |st ; Œò) describes
the probability distribution of taking at in st .
For an MDP, the goal of RL is to find an optimal policy
œÄ ‚àó (also an optimal policy parameter Œò‚àó ) to maximize the
cumulative expected return J(Œò) from an initial state s0 ,
namely
X

‚àû
Œò‚àó = argmax J(Œò) = argmax EœÄ
Œ≥ t rt s0
(1)
Œò

Œò

P‚àû

t=0
t

Unfortunately, J(Œò) = EœÄ [ t=0 Œ≥ rt |s0 ] is difficult to be
calculated directly, since p(s0 |st , at ) is unknown in RL,
and s0t and rt can be obtained only by the agent‚Äôs interaction with the environment. DRL generally uses the
DAC method to solve Œò‚àó . At time step t, the critic uses
the DVFA method
P‚àûto approximate the state-value function
V œÄ (st ) = EœÄ [ i=0 Œ≥ i ri |s
0 = st ] and the action-value
P
‚àû
function QœÄ (st , at ) = EœÄ [ i=0 Œ≥ i ri |s0 = st , a0 = at ] for
evaluating the performance of the current policy œÄ, and then
the actor uses the policy gradient to update Œòt .
B. Stochastic Policy Gradient
Currently, there are two main types of policy gradients: SPG
and NPG. From the policy gradient theorem [35], SPG can be
calculated as


‚àáŒòt J(Œòt ) = EœÄ AœÄ (st , at )‚àáŒòt logœÄ(at |st ; Œòt )
(2)

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

where ‚àáŒòt J(Œòt ) denotes ‚àÇJ(Œòt )/‚àÇŒòt , and AœÄ (st , at ) is the
advantage function. AœÄ (st , at ) measures how much better than
the average it is to take an action at [36]. It is defined as
AœÄ (st , at ) = QœÄ (st , at ) ‚àí V œÄ (st )

(3)

Unlike SPG, NPG does not follow the steepest direction in the
policy parameter space but the steepest direction with respect
to the Fisher information metric [37]. It is defined as
Àú Œò J(Œòt ) = (F(Œòt ))‚àí1 ‚àáŒò J(Œòt )
‚àá
t
t

(4)

where F(Œòt ) is the Fisher information matrix defined by


F(Œòt ) = EœÄ v(‚àáŒòtlogœÄ(at |st ;Œòt))v(‚àáŒòtlogœÄ(at |st ;Œòt))T (5)
where v(¬∑) denotes reshaping the given tensor or matrix into a
column vector. To avoid computing the inverse of F(Œòt ), (4)
can also be redefined as
Àú Œò J(Œòt ) = m(wt )
‚àá
t

(6)

where m(¬∑) denotes reshaping the given vector into a matrix
or tensor, and wt is the parameter of the linear compatible
function approximator [37] defined by

AÃÉ(st , at ; wt ) = wtT v ‚àáŒòt log œÄ(at |st ; Œòt )
(7)
œÄ

AÃÉ(st , at ; wt ) is the compatible approximation of A (st , at ).
C. Convolutional Neural Network
In DAC, CNNs are widely used to approximate V œÄ (s)
and œÄ for solving control tasks with raw pixel inputs. A
CNN generally consists of some convolutional (conv) layers,
pooling layers and fully-connected (fc) layers. Since there are
no learnable parameters in pooling layers, we only review the
forward learning of conv layers and fc layers. Let l, M , Xlt , Zlt ,
Ylt , Œòlt and fl (¬∑) denote the current layer, the mini-batch size,
the mini-batch input, the pre-activation input, the activation
output, the parameter, the activation function in this layer at
current time t, respectively. For brevity, we omit the bias term
of each layer in this paper.
In a conv layer, Xlt ‚àà RM √óCl‚àí1 √óHl‚àí1 √óWl‚àí1 is convolved
k
k
with the kernel Œòlt ‚àà RCl‚àí1 √óCl √óHl √óWl and puts through
l
fl (¬∑) to form Yt ‚àà RM √óCl √óHl √óWl , where Cl , Hl , Wl , Hlk and
Wlk denote the number of output channels, the output image
height, the output image width, the kernel height and the kernel
k
k
l
width, respectively. Let XÃåt(:,:,:,:,h,w) ‚àà RM √óCl‚àí1 √óHl √óWl
denote the input selection of the output pixel Ylt(:,:,h,w) .
l
XÃåt

l
XÃÇt

M √óCl‚àí1 Hlk Wlk √óHl Wl

Reshape
and Œòlt as
‚àà R
and
l
Cl‚àí1 Hlk Wlk √óCl
, respectively. Then, Ylt(:,:,j) is defined
ŒòÃÇt ‚àà R
as
l
Ylt(:,:,j) = fl (Zlt(:,:,j) ) = fl (XÃÇt(:,:,j) ŒòÃÇlt )
(8)
In an fc layer, Xlt ‚àà RM √óNl‚àí1 is weighed to connect all
output neurons by Œòlt ‚àà RNl‚àí1 √óNl and puts through fl (¬∑) to
form Ylt ‚àà RM √óNl , where Nl denotes the number of output
neurons. Namely, Ylt is defined as
Ylt = fl (Zlt ) = fl (Xlt Œòlt )

(9)

3

D. Advantage Actor Critic
A2C is an important baseline algorithm in OpenAI. In A2C,
there are N parallel workers, a shared critic network and a
shared actor network. The critic network and the actor network
can be joint or disjoint. If both networks are joint, they will
share lower layers but have distinct output layers.
The algorithm flow of A2C can be summarized as follows. At current iteration step t, it lets each worker interact with each own environment for T timesteps, and uses
all state-transition pairs to form the mini batch Mt =
(k) (k)
(k) (k) (k)
(k)
,T
{(st,i , at,i , s0 t,i , rt,i , dt,i )}k=1,¬∑¬∑¬∑
i=1,¬∑¬∑¬∑ ,N , where dt,i ‚àà {0, 1}
(k)
denotes that the next state s0 t,i of the ith worker is the terminal
state or not at the k th timestep, and the mini-batch size M is
equal to N T . Then, it calculates the loss function defined by
L(Œ®t , Œòt ) = L(Œ®t ) + L(Œòt ) + Œ∑E(Œòt )

(10)

where Œ®t and L(Œ®t ) are the parameter and the loss function
of the critic network, Œòt and L(Œòt ) are the parameter and the
loss function of the actor network, and Œ∑E(Œòt ) is the entropy
regularization term with a small Œ∑ > 0. L(Œ®t ) is defined as
L(Œ®t ) =

1
2
A(St , At ) F
2N T

(1)

(T )

(1)

(11)
(T )

where St = [st,1 , ¬∑ ¬∑ ¬∑ , st,N ]T , At = [at,1 , ¬∑ ¬∑ ¬∑ , at,N ]T , and
A(St , At ) ‚àà RN T denotes the estimate value of AœÄ (St , At ),
which is calculated as
A(St , At ) = Q(St , At ) ‚àí V (St ; Œ®t )

(12)

where V (St ; Œ®t ) and Q(St , At ) denote the estimate values of
V œÄ (St ) and QœÄ (St , At ), respectively. The former is the actual
output of the critic network, and the latter is the desired output
of the critic network. Each element in the latter is calculated
as
( (T )
(T )
(T )
rt,i +Œ≥(1‚àídt,i )V (s0 t,i ; Œ®t ) , k = T
(k) (k)
Q(st,i , at,i ) = (k)
(13)
(k)
k+1
rt,i +Œ≥(1‚àídt,i )Q(sk+1
t,i , at,i ), k < T
(T )

where V (s0 t,i ; Œ®t ) is also approximated by the critic network.
L(Œòt ) and E(Œòt ) are defined as follows
N

T

1 XX
(k) (k)
(k) (k)
L(Œòt ) = ‚àí
A(st,i , at,i )logœÄ(at,i |st,i ;Œòt ) (14)
N T i=1
N

k=1
T

1 XX
(k) (k)
(k) (k)
E(Œòt ) = ‚àí
œÄ(at,i |st,i ;Œòt )logœÄ(at,i |st,i ;Œòt ) (15)
N T i=1
k=1

(k)

(k)

where œÄ(at,i |st,i ; Œòt ) is the actual output of the actor network. Finally, A2C uses (10) to update Œ®t and Œòt with some
first-order optimization algorithms such as RMSProp.
III. RLS- BASED A DVANTAGE ACTOR C RITIC
In this section, we try to integrate the RLS method into the
vanilla A2C with CNNs. Under the loss function defined by
(10), the RLS update rules for four different types of layers
used in critic and actor networks are derived respectively. On
the basis, we propose RLSSA2C and RLSNA2C algorithms.

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

A. Optimizing Critic Output Layer
As introduced in Section I, the critic of traditional actorcritic algorithms widely uses LSTD for policy evaluation.
From (11), (12) and (13), A(St , At ) ‚àà RN T is a temporal
difference error vector. It means we can also use LSTD for
updating the critic parameter. Let Dt = {M1 , ¬∑ ¬∑ ¬∑ , Mt }
denote the state-transition mini-batch dataset from the start to
the current iteration step t. On the basis, we define an auxiliary
least squares loss function as
t
1 X t‚àín
2
Œª
A(Sn , An ) F
LÃÉ(Œ®) =
2N T n=1

(16)

4

work [34], we propose an average-approximation method to
tackle this problem and reduce the computation burden. Using
this method, we define
NT

(17)

Œ®

In the critic network of A2C, the output layer is generally
a linear fc layer with one output neuron. In other words, the
activation function of this layer is the identity function. Thus,
from (9), V (Sn ; Œ®) is calculated as
V (Sn ; Œ®) = Xln Œ®l

(18)

where Œ®l is the parameter of this layer. Then, from (12), (16)
can be rewritten as
LÃÉ(Œ®) =

t
X

1
2
Œªt‚àín Q(Sn , An ) ‚àí Xln Œ®l F
2N T n=1

t

Let ‚àáŒ®l LÃÉ(Œ®) = 0. We can easily obtain the least squares
solution of Œ®lt+1 , namely
(21)

where Hlt+1 ‚àà RNl‚àí1 √óNl‚àí1 and blt+1 ‚àà RNl‚àí1 are defined as
t

Hlt+1 =

1 X t‚àín l T l
Œª (Xn ) Xn
N T n=1

(22)

t

blt+1 =

1 X t‚àín l T
Œª (Xn ) Q(Sn , An )
N T n=1

(23)

Next, to avoid computing the inverse of Hlt+1 and realize
online learning, we try to derive the RLS solution of Œ®lt+1 .
Rewrite (22) and (23) as the following incremental update
equations
Hlt+1 = ŒªHlt +

1
(Xl )T Xlt
NT t

(24)

1
(Xl )T Q(St , At )
(25)
NT t
However, by using the Sherman-Morrison matrix inversion
lemma [38] for (24), the recursive update of (Hlt+1 )‚àí1 still
includes a new inverse matrix, since the rightmost term in (24)
is a matrix product rather than a vector product. In our previous
blt+1 = Œªblt +

(27)

Hlt+1 = ŒªHlt + kxÃÑlt (xÃÑlt )T

(28)

blt+1 = Œªblt + kxÃÑlt qÃÑt

(29)

where k > 0 is the average scaling factor. Let Pt = (Ht )‚àí1 .
Then, using the Sherman-Morrison matrix inversion lemma
and (17), we finally obtain the following RLS update rules


kPlt xÃÑlt (xÃÑlt )T Plt
1 l
l
(30)
P ‚àí
Pt+1 ‚âà
Œª t Œª + k(xÃÑlt )T Plt xÃÑlt
kPlt xÃÑlt (qÃÑt ‚àí vÃÑt )
(31)
Œ®lt+1 ‚âà Œ®lt +
Œª + k(xÃÑlt )T Plt xÃÑlt
where vÃÑt is defined as
NT


1 X t‚àín l T
Œª (Xn ) Q(Sn , An )‚àíXln Œ®l (20)
N T n=1

Œ®lt+1 = (Hlt+1 )‚àí1 blt+1

1 X
Q(St(i,:) , At(i,:) )
N T i=1

where Xlt(i,:) ‚àà RNl‚àí1 and Q(St(i,:) , At(i,:) ) ‚àà R are the
column vectors sliced from Xlt and Q(St , At ), respectively.
On the basis, we can rewrite (24) and (25) as follows

1 X
vÃÑt =
V (St(i,:) ; Œ®t )
N T i=1

(19)

By the chain rule for Œ®l , ‚àáŒ®l LÃÉ(Œ®) can be derived as
‚àáŒ®l LÃÉ(Œ®) = ‚àí

(26)

NT

qÃÑt =

where Œª ‚àà (0, 1] is the forgetting factor. Then, the learning
problem of the current critic parameter can be described as
Œ®t+1 = argmin LÃÉ(Œ®)

1 X l
X
N T i=1 t(i,:)

xÃÑlt =

(32)

In our previous work [34], we find that the RLS optimization
can be converted into a gradient descent algorithm, which is
easier to implement by using PyTorch or TensorFlow. Based
on (10)-(12), ‚àáŒ®lt L(Œ®t , Œòt ) can be derived as
‚àáŒ®lt L(Œ®t , Œòt ) = ‚àí

1
(Xl )T (Q(St , At ) ‚àí Xlt Œ®lt )
NT t

(33)

In (28) and (29), we ever use kxÃÑlt (xÃÑlt )T and kxÃÑlt qÃÑt to replace
l T l
l T
1
1
N T (Xt ) Xt and N T (Xt ) Q(St , At ), respectively. On the basis, we can easily get
1
(Xl )T (Q(St , At ) ‚àí Xlt Œ®lt )
(34)
NT t
Thus, we can rewrite (31) as the following gradient update
form
Plt ‚àáŒ®lt L(Œ®t , Œòt )
Œ®lt+1 ‚âà Œ®lt ‚àí
(35)
Œª + k(xÃÑlt )T Plt xÃÑlt
kxÃÑlt (qÃÑt ‚àí vÃÑt ) ‚âà

Pl

where Œª+k(xÃÑlt)T Pl xÃÑl is the learning rate. That means we needn‚Äôt
t
t t
change the loss function defined by (10).
B. Optimizing Actor Output Layer
In the actor network of A2C, the output layer is also an
fc layer, which is to output œÄ(At |St ; Œòt ). For discrete control
problems, A2C often uses the Softmax function to define œÄ,
called the Softmax policy. For continuous control problems,
A2C usually uses the Gaussian function to define œÄ, called
the Gaussian policy. Unlike (11), (14) and (15) are difficult to
be converted into least squares loss functions. In fact, for the

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

same reason, many traditional actor-critic algorithms only use
the RLS method to update the critic parameter, but use the
SPG descent method to update the actor parameter. Our first
algorithm RLSSA2C will follow this method. Its actor output
layer is optimized by an ordinary first-order optimization
algorithm. For example, its update rules based on RMSProp
are defined as follows
Clt+1 = œÅClt + (1 ‚àí œÅ)‚àáŒòlt L(Œ®t , Œòt ) ‚àáŒòlt L(Œ®t , Œòt ) (36)

Œòlt+1 = Œòlt ‚àí q
(37)
‚àáŒòlt L(Œ®t , Œòt )
Œ¥ + Clt+1
where Clt is the accumulative squared gradient, œÅ is the decay
rate,  is the learning rate, Œ¥ > 0 is a small constant,
‚àáŒòlt L(Œ®t , Œòt ) is SPG, and denotes the Hadamard product.
As introduced in Section II. B, there is another type of policy
gradients, namely NPG [30], [37], which has yielded a few
novel NAC algorithms. To avoid computing the inverse of the
Fisher information matrix, some traditional NAC algorithms,
often use the RLS method to approximate the compatible
parameter Wt , and use Wt as NPG for updating the actor parameter [31], [32]. Following this way, we define an auxiliary
least squares loss function based on Dt as
LÃÉ(w) =

t
1 X t‚àín
2
Œª
A(Sn , An ) ‚àí AÃÉ(Sn , An ; w) F (38)
2N T n=1

where A(Sn , An ) is calculated by (12). From (7), each element
in AÃÉ(Sn , An ; w) is defined as
l

AÃÉ(Sn(i,:) , An(i,:) ; w) = wT GŒò
n(i,:)

(39)

l

l
l
where GŒò
n(i,:) denotes v(‚àáŒòlt log œÄ(An(i,:) |Xn(i,:) ; Œòt )) ‚àà
RNl‚àí1 Nl for simplifying notations. Then, the learning problem
of the current compatible parameter can be described as

wt+1 = argmin LÃÉ(w)

(40)

w

Let ‚àáw LÃÉ(w) = 0. We can easily obtain the least squares
solution of wt+1 , namely
wt+1 = (Hlt+1 )‚àí1 blt+1

(41)

where Hlt+1 ‚àà RNl‚àí1 Nl √óNl‚àí1 Nl and bt+1 ‚àà RNl‚àí1 Nl are
defined as follows
t
1 X t‚àín Œòl T Œòl
Hlt+1 =
Œª (Gn ) Gn
(42)
N T n=1
t

1 X t‚àín Œòl T
blt+1 =
Œª (Gn ) A(Sn , An )
N T n=1
l

l

(43)

l

Œò
Œò
T
N T √óNl‚àí1 Nl
where GŒò
. By
n = [Gn(1,:) , ¬∑ ¬∑ ¬∑ , Gn(N T,:) ] ‚àà R
using the same derivation method in Section III. A, we
can easily obtain the RLS update rules for Plt+1 and wt+1 .
‚àí1
However, here Plt+1 = Hlt+1
will be Nl2 times as complex
as in the critic output layer.
Since the forgetting factor Œª is 1 or close to 1 in general,
Hlt+1 can be approximately viewed as a Fisher information
matrix. In addition, by the chain rule, we easily get
Zln(i,:) T
Œò
Gn(i,:)
= Xln(i,:) (Gn(i,:)
)
l

(44)

5

Zl

n(i,:)
where Gn(i,:)
= ‚àá Zl

n(i,:)

log œÄ(An(i,:) |Xln(i,:) ; Œòl ) and Zln(i,:) =

(Œòl )T Xln(i,:) . Then, we have
l

Zl

l

Zl

n(i,:)
n(i,:) T
l
l
T Œò
T
(GŒò
n(i,:) ) Gn(i,:) = Xn(i,:) (Xn(i,:) ) ‚äó Gn(i,:) (Gn(i,:) ) (45)

where ‚äó is the Kronecker product. To reduce the memory and
computation burden, we use the Kronecker-factored approximation [23], [24] to rewrite (42) as
(1)

(2)

Hlt+1 ‚âà Ht+1 ‚äó Ht+1
(1)

(46)

(2)

where Ht+1 and Ht+1 are defined as follows
t

(1)

Ht+1 =

k X t‚àín l T l
Œª (Xn ) Xn
N T n=1

(47)

t

(2)

Ht+1 =

k X t‚àín Zln T Zln
Œª (Gn ) Gn
N T n=1

Zl

l

(48)

Zl

n(i,:)
n(i,:)
T
N T √óNl
, and k > 0
where GZnn = [Gn(1,:)
, ¬∑ ¬∑ ¬∑ , Gn(N
T,:) ] ‚àà R
(1)
(1) ‚àí1
is another average scaling factor. Let Pt = Ht
and
(2)
2) ‚àí1
Pt = Ht
. Now, we rewrite (47) and (48) as the
incremental forms, use the Sherman-Morrison matrix inversion
lemma for each sample in the current mini batch, and average
the recursive results. We can easily get

(1) l
NT
l
T (1) 
k X Pt Xt(i,:) (Xt(i,:) ) Pt
1 (1)
(1)
P ‚àí
(49)
Pt+1 ‚âà
l
Œª t
N T i=1 Œª + k(Xl )T P(1)
t Xt(i,:)
t(i,:)

1
(2)
Pt+1 ‚âà

Œª



Zl

(2)

(2)
Pt ‚àí

Zl

(2)

t(i,:)
t(i,:) T
NT
k X Pt Gt(i,:) (Gt(i,:) ) Pt
l
Zlt(i,:)
N T i=1
(2) Zt(i,:)
Œª + k(Gt(i,:)
)T Pt Gt(i,:)

Plugging (46) into (41) yields
(1)
(2) 
(1)
(2)
wt+1 ‚âà Pt+1 ‚äó Pt+1 blt+1 = Pt+1 m(blt+1 )Pt+1


(50)

(51)

where m(blt+1 ) denotes reshaping the vector blt+1 into an
Nl‚àí1 √ó Nl matrix. Then, the rest deviation is similar to what
we do in Section III. A. Using (43), (49) and (50), we can
obtain
NT

(1)

wt+1 ‚âà wt ‚àí v Pt m

1 X w  (2) 
G
P
N T i=1 t(i,:) t

(52)

where Gw
t(i,:) is defined as
l
 Œòlt
T Œòt
A(S
,
A
)
‚àí
w
G
Gt(i,:)
t(i,:)
t(i,:)
t
t(i,:)
Gw
l
t(i,:) =
T (1)

Zt(i,:) T (2) Zlt(i,:)
Œª+k Xlt(i,:) Pt Xlt(i,:) Œª+k Gt(i,:)
Pt Gt(i,:)

Note that we don‚Äôt use the average-approximation method used
l(1)

in Section III. A to update Pt

l(2)

, Pt

Zl

t(i,:)
and wt , since Gt(i,:)
and

Œòl

t
Gt(i,:)
of different samples sometimes have large difference
and this method will blur their difference.
Finally, the parameter of the actor output layer updated by
using NPG can be defined as

Œòlt+1 = Œòlt + Œ±m(wt+1 )
where Œ± is the learning rate.

(53)

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

C. Optimizing Fully-connected Hidden Layer
In this subsection, we discuss the RLS optimization for fc
hidden layers in the critic and actor networks. Generally, there
is a nonlinear activation function in each hidden layer, which
makes us difficult to derive the least squares solutions of Œ®lt+1
and Œòlt+1 by using the same method introduced in Section III.
A. In fact, this is the main reason why it is difficult to combine
DAC and RLS.
Here we use the equivalent-gradient method, proposed in
our previous work [34], to tackle this issue. For the current
layer l of the critic network, we define an auxiliary least
squares loss function as
LÃÉ(Œ®) =

t
1 X t‚àín l‚àó
2
Zn ‚àí Zln F
Œª
2N T n=1

(54)

l
l
l
where Zl‚àó
n is the corresponding desired value of Zn = Xn Œ® .
Then, by using the same derivation method in Section III. A,
Œ®lt+1 is defined as

Œ®lt+1 ‚âà Œ®lt +

kPlt xÃÑlt (zÃÑtl‚àó ‚àí zÃÑtl )T
Œª + k(xÃÑlt )T Plt xÃÑlt

(55)

where zÃÑtl‚àó and zÃÑtl are defined as follows
NT

zÃÑtl‚àó =
zÃÑtl =

1 X l‚àó
Z
N T i=1 t(i,:)
NT
X

1
Zl
N T i=1 t(i,:)

(56)

(57)

and the update rule of Pt is the same as (30). Furtherly, from
our previous work [34], ‚àáZlt L(Œ®t , Œòt ) can be equivalently
defined as
1
(58)
‚àáZlt L(Œ®t , Œòt ) = ‚àí
(Zl‚àó ‚àí Zlt )
¬µN T t
where ¬µ > 0 is the gradient scaling factor. Plugging (58) into
(55), we finally get
Œ®lt+1 ‚âà Œ®lt ‚àí

¬µPlt
‚àáŒ®lt L(Œ®t , Œòt )
Œª + k(xÃÑlt )T Plt xÃÑlt

(59)

Note that the derivation of Œòlt+1 , which is the parameter of
the current fc hidden layer in the actor network, is the same
as the above. For brevity, we directly give the result, namely
Œòlt+1 ‚âà Œòlt ‚àí

¬µPlt
‚àáŒòlt L(Œ®t , Œòt )
Œª + k(xÃÑlt )T Plt xÃÑlt

6

PN T PHl Wl l
1
define xÃÑlt = N T H
i=1
j=1 XÃÇt(i,:,j) to tackle this
l Wl
problem. But in practice, we find that this definition will blur
the difference among the input selections of different output
pixels, which will worsen the performance of our algorithms.
To avoid this situation, we define
NT

l

XÃÑt =
l

k

1 X l
XÃÇ
N T i=1 t(i,:,:)

(61)

k

l √óHl Wl . On the basis, similar to
where XÃÑt ‚àà RCl‚àí1 Hl W
l(1)
l(1) ‚àí1
l(2)
l(2) ‚àí1
that of Pt
= Ht
and Pt
= Ht
introduced in Section III. B, the recursive derivation of Plt+1 =
Pt
l
l ‚àí1
1
t‚àín
will yield
(XÃÇn )T XÃÇn
n=1 Œª
NT


l
l
Hl Wl
Plt XÃÑt(:,j) (XÃÑt(:,j) )T Plt
1 l
k X
l
Pt+1 ‚âà
P ‚àí
(62)
Œª t Hl Wl j=1 Œª + k(XÃÑlt(:,j) )T Plt XÃÑlt(:,j)

Finally, we can obtain the update rules for Œ®lt and Œòlt defined
as follows
 

¬µHl Wl Plt o ‚àáŒ®lt L(Œ®t , Œòt )
l
l
(63)
Œ®t+1 ‚âà Œ®t ‚àí œÑ PH W

l
l
l
l
Œª + k(XÃÑt(:,j) )T Plt XÃÑt(:,j)
j=1
 

l
o
‚àá
¬µH
W
P
l L(Œ®t , Œòt )
l
l
Œò
t
t
Œòlt+1 ‚âà Œòlt ‚àí œÑ PH W
(64)

l
l
l
T l l
Œª
+
k(
XÃÑ
t(:,j) ) Pt XÃÑt(:,j)
j=1
where o(¬∑) denotes reshaping a Cl‚àí1 √ó Cl √ó Hlk √ó Wlk tensor
into a Cl‚àí1 Hlk Wlk √ó Cl matrix, and œÑ (¬∑) does the reverse.
E. RLSSA2C and RLSNA2C Algorithms
Based on the above derivation and the vanilla A2C, both
RLSSA2C and RLSNA2C can be summarized in Algorithm
1, where LC and LA denote the total layer numbers in critic
and actor networks. Note that the autocorrelation matrix Plt in
the critic network is different from that in the actor network
and we use the same notation only for brevity. For RLSSA2C,
A
is updated by using SPG and an ordinary first-order
ŒòL
t
A
is updated by
optimization algorithm. For RLSNA2C, ŒòL
t
using the compatible parameter wt+1 , which is updated by
(1)
(2)
Pt and Pt . Except for those, the rest of RLSSA2C and
RLSNA2C is the same. In practice, to avoid instability in
training, the critic network and the actor network sometimes
are joint [15], [19]. In this case, the shared layers are updated
by the RLS optimization only once at each iteration.

(60)

where the update rule of Plt is also the same as (30).
D. Optimizing Convolutional Hidden Layer
Conv layers are at the front of a CNN. They are usually used
to learn spatial features of original state inputs. As defined
by (8), a conv layer can be viewed as a special fc layer.
That means we can also use the same RLS update rules
as (59) and (60) to optimize the conv layers of critic and
actor networks. However, there is a little different, since the
l
current input XÃÇt of the conv layer l has three dimensions
rather than two dimensions. In our previous work [34], we

IV. A NALYSIS AND I MPROVEMENT
In this section, we analyze the computational complexity
and the convergence of RLSSA2C and RLSNA2C in brief.
In addition, we also present three practical tricks for further
improving their convergence speed.
A. Theoretical Analysis
First, we analyze the computational complexity of our
proposed algorithms. Although their derivation seems complex
and tedious, but in fact they are very simple and easy to
implement. From (35), (52), (59), (60), (63) and (64), the RLS
optimization for actor and critic networks can be viewed as a

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

Algorithm 1: RLS-Based Advantage Actor Critic
l LC
l
1 Input: critic parameters {Œ®0 , P0 }l=1 ; actor parameters
l LA‚àí1
l LA
{Œò0 }l=1 , {P0 }l=1 , hyperparameters of the ordinary
(1)
(2)
first-order algorithm or {w0 , P0 , P0 , Œ±}; initial
(1) N
states {s0,i }i=1 of N workers, discount factor Œ≥,

scaling factors k and ¬µ, forgetting factor Œª,
regularization factor Œ∑.
2 for t = 0, 1, 2, ... do
3
Excute: let each worker run T timesteps and
generate
(k) (k)
(k) (k) (k) k=1,¬∑¬∑¬∑ ,T
Mt = {(st,i , at,i , s0 t,i , rt,i , dt,i )}i=1,¬∑¬∑¬∑
,N ,
(k)
(k) (k)
where at,i ‚àº œÄ(at,i |st,i , Œòt ) decided by the
actor network
4
Measure: calculate the loss function by (10)
5
Update critic network:
6
update Œ®ltC , PltC in fc output layer by (35), (30)
7
update Œ®lt , Plt in each fc hidden layer by (59),
(30)
8
update Œ®lt , Plt in each conv layer by (63), (62)

12

Update actor network:
A
by an ordinary first-order algorithm
update ŒòL
t
l(2)
l(1)
A
or wt , Pt , Pt , ŒòL
t by (52), (49), (50), (53)
l
l
update Œòt , Pt in each fc hidden layer by (60),
(30)
update Œòlt , Plt in each conv layer by (64), (62)

13

0
N
Set {st+1,i }N
i=1 = {s t,i }i=1 and discard Mt

9
10

11

(1)

(T )

special SGD optimization. For an fc hidden layer, a critic fc
output layer, an actor fc output layer in RLSNA2C and a conv
layer, the computational complexities of the RLS optimization
Cl‚àí1 Hlk Wlk
NL‚àí1
NL‚àí1 NL
are 1 + NNl‚àí1
and 1 + N
T , 1 + NT , 2 +
NT
T Hl Wl times
as those of SGD, respectively. In practice, A2C generally uses
16 or 32 workers to interact with their environments for 5‚àº20
timesteps at each iteration, that is, N T is 80‚àº640. Thus, our
RLS optimization is only several times as complex as SGD. In
Section V, experimental results show that the running speeds
of RLSSA2C and RLSNA2C are only 10%‚àº30% slower
than that of the vanilla A2C with RMSProp, but they are
significantly faster than those of PPO and ACKTR.
Next, we analyze the convergence of our proposed algorithms. As shown in Algorithm 1, RLSSA2C and RLSNA2C
are two-time-scale actor-critic algorithms. In LFARL, the
convergence of this type of algorithm has been established
[7], [32]. Namely, if the actor learning rate Œ±tA and the critic
learning rate Œ±tC satisfy the standard Robbins-Monro condition
[39] and a time-scale seperation condition limt‚Üí‚àû Œ±tA /Œ±tC =
0, actor and critic parameters will converge to asymptotic
stable equilibriums. However, unlike traditional actor-critic
algorithms, DAC algorithms use non-linear function approximations. Thus, their convergence proofs are difficult. In recent
years, there have been a few studies on this issue. Yang et al.
establish the convergence of batch actor-critic with nonlinear
function approximations and finite samples [40]. Liu et al.

7

establish nonasymptotic upper bounds of the numbers of TD
and SGD iterations, and prove that a variant of PPO and
TRPO with overparametrized neural networks converges to
the globally optimal policy at a sublinear rate [41]. Assuming
independent sampling, wang et al. prove that neural NPG
converges to a globally optimal policy at a sublinear rate [42].
Compared with the proof methods for traditional actor-critic
algorithms, these methods have more assumptions and are
more complex. Our algorithms are similar to the batch actorcritic algorithm studied in [40], and the RLS optimization
can be viewed as a special SGD optimization. Therefore, we
should establish the convergence of RLSSA2C and RLSNA2C
by using the methods in [40], [41] and [40], [42], respectively.
Intuitively, if actor and critic networks can be mapped into two
linear function approximator with some error, the convergence
proofs of RLSSA2C and RLSNA2C will be converted into the
proofs of traditional actor-critic algorithms.

B. Performance Improvement
In Section III, to simplify the calculation, we import two
scaling factors k and ¬µ. From (28), (29), (47), (48) and (58),
they should be time-variant, so we present two new definitions
for them. From (30), we can get


Plt xÃÑlt (xÃÑlt )T Plt
1 l
l
(65)
P ‚àí Œª
Pt+1 ‚âà
l T l l
Œª t
k + (xÃÑt ) Pt xÃÑt
It is clear that the average scaling factor k also plays a
role similar to the forgetting factor Œª. That is, a big k will
increase the forgetting rate, and vice versa. At the beginning
of learning, the policy œÄ is infantile and unstable, so we
should select a big k to forget the historical information and
emphasize the new samples for accelerating convergence. At
the end of learning, the policy œÄ is close to the optimal, so we
should select a small k to stabilize it. In (49), (50) and (62),
k also play the same role. Thus, we redefine k as


t
kt = max k0 ‚àí b cŒ¥k , kmin
t‚àÜ

(66)

where k0 , t‚àÜ , Œ¥k and kmin denote the initial value, the update
interval, the decay size and the lower bound of k, respectively.
From (59), (60), (63) and (64), the gradient scaling factor ¬µ is
a part of the learning rate. A big ¬µ can accelerate convergence,
but it will also cause œÄ to fall into the local optimum. Thus,
¬µ should gradually decay to a steady value. Here, we redefine
it as


t
(67)
¬µt = max ¬µ0 ‚àí b cŒ¥¬µ , ¬µmin
t‚àÜ
where ¬µ0 , Œ¥¬µ and ¬µmin denote the initial value, the decay size
and the lower bound of ¬µ, respectively. In fact, ¬µt is different
for each layer. In a DNN, deeper layers are more likely to
suffer from the gradient vanishing, so we suggest that users
choose a big ¬µ0 for these layers.
In addition, it is well known that the vanilla SGD can
be accelerated by the momentum method [43]. Our RLS
optimization is a special SGD, so we can also use this method

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

to accelerate it. Similar to in our previous work [34], (35) can
be redefined as
Œ¶lt+1 ‚âà Œ≤Œ¶lt ‚àí

Plt ‚àáŒ®lt L(Œ®t , Œòt )

Œª + k(xÃÑlt )T Plt xÃÑlt
l
Œ®t+1 ‚âà Œ®lt + Œ¶lt+1

(68)
(69)

where Œ¶lt denote the velocity matrix in the lth layer, and Œ≤
is the momentum factor. (59), (60), (63) and (64) can also
be redefined as the similar forms. Note that we only suggest
RLSSA2C to use this method, since we find that it will worsen
the RLSNA2C‚Äôs stabilities empirically .
V. E XPERIMENTAL R ESULTS
In this section, we will compare our algorithms against the
vanilla A2C with RMSProp (called RMSA2C) for evaluating
the sample efficiency, and compare them against RMSA2C,
PPO and ACKTR for evaluating the computational efficiency.
We first test these algorithms on 40 discrete control games
in the Atari 2600 environment, and then test them on 11
continuous control tasks in the MuJoCo environment.
A. Discrete Control Evaluation
Atari 2600 is the most famous discrete control benchmark
platform for evaluating DRL. It is comprised of a lots of highly
diverse games, which have high-dimensional observations with
raw pixels. In this set of experiments, we select 40 games from
the Atari environment for performance evaluation. For each
game, the state is a 4√ó 84√ó84 normalized image.
All tested algorithms have the same model architecture,
which is defined in [15]. It has 3 shared conv layers, 1 shared
fc hidden layer, 1 separate fc critic output layer and 1 separate
fc actor output layer. The first conv layer has 32 8√ó8 kernels
with stride 4, the second conv layer has 64 4√ó4 kernels with
stride 2, the third conv layer has 32 3√ó3 kernels with stride 1,
and the fc hidden layer has 512 neurons. All hidden layers use
ReLU activation functions, the fc critic output layer uses the
Identity activation function to predict the state-value function,
and the fc actor output layer uses the Softmax activation
function to represent the policy. The parameter of each layer is
initialized with the default settings of PyTorch. All algorithms
use the same loss function defined by (10), where the discount
factor Œ≥ = 0.99 and the entropy regularization factor Œ∑ = 0.01.
At each iteration step, each algorithm lets 32 parallel workers
run 5 timesteps, and uses the generated minibatch including
160 samples for training. All workers use an Intel Core i79700K CPU for trajectory sampling, and use a Nvidia RTX
2060 GPU for accelerating the optimization computation. To
avoid gradient exploding, all parameter gradients are clipped
by the L2 norm with 0.5.
Besides the above common settings, individual settings of
each algorithm are summarized as follows: 1) For RMSA2C,
the learning rate , decay factor œÅ and small constant Œ¥ in
RMSProp are set to 0.00025, 0.99 and 0.00005, respectively.
2) For RLSSA2C, all initial autocorrelation matrices are set
to Identity matrices, the forgetting factor Œª and momentum
factor Œ≤ are set to 1 and 0.5, the hyperparameters k0 , Œ¥k ,

8

kmin , ¬µ0 , Œ¥¬µ , ¬µmin , t4 of scaling factors kt and ¬µt are set to
0.1, 0.02 ,0.01, 5, 0.1, 1 and 5000, and the actor output layer
uses the same RMSProp as that in RMSA2C. Note that ¬µt is
only used for conv layers but is fixed to 1 for all fc layers.
(1)
3) For RLSNA2C, the momentum factor Œ≤ is set to 0.0, P0
(2)
and P0 are also set to Identity matrices, and the learning rate
Œ± of the actor output layer is initialized to 0.01 and decays
by 0.002 per 5000 timesteps to 0.001. The other settings are
the same as those in RLSSA2C. 4) For PPO and ACKTR, we
directly use Kostrikov‚Äôs source code and settings [25]. Note
that the settings of RMSA2C are selected from [15], and the
settings of RLSSA2C and RLSNA2C are obtained by tuning
Pong, Breakout and StarGunner games.
The convergence comparison of our algorithms against
RMSA2C on 40 Atari games trained for 10 million timesteps
is shown in Fig. 1. It is clear that RLSSA2C and RLSNA2C
outperform RMSA2C on most games. Among these three algorithms, RLSSA2C has the best convergence performance (i.e.,
sample efficiency) and stability. Compared with RMSA2C,
RLSSA2C wins on 30 games. On Alien, Amidar, Assault, Asterix, BattleZone, Boxing, Breakout, Kangaroo, Krull, KungFuMaster, MsPacman, Pitfall, Pong, Qbert, Seaquest and Tennis games, RLSSA2C is significantly superior to RMSA2C in
terms of convergence speed and convergence quality. Compared with RMSA2C, RLSNA2C also wins on 30 games.
On Asterix, Atlantis, DoubleDunk, FishingDerby, NameThisGame, Pong, Riverraid, Seaquest, UpNDown and Zaxxon
games, RLSNA2C performs very well. But compared with
RLSSA2C, it is not very stable on some games.
In Table I, we present the last 100 average episode rewards
of our algorithms and RMSA2C on 40 Atari games trained for
10 million timesteps. From Table I, RLSSA2C and RLSNA2C
obtain the highest rewards on 19 and 15 games respectively,
but RMSA2C only wins on 6 games. Notably, on Assault,
BattleZone, Gopher and Q-bert games, RLSSA2C respectively
acquires 2.6, 2.4, 3.6, and 3.1 times scores as much as
RMSA2C. On Asterix, Atlantis, FishingDerby, UpNDown and
Zaxxon games, RLSNA2C respectively acquired 2.8, 2.5, 2.7,
4.8, and 45 times scores as much as RMSA2C.
The running speed comparison of our algorithms against
RMSA2C, PPO and ACKTR on six Atari games is shown
in Table II. Among these five algorithms, RMSA2C has the
highest computational efficiency, RLSNA2C and RLSSA2C
are listed 2nd and 3rd respectively, and PPO is listed last. In
detail, RLSSA2C is only 28.1% slower than RMSA2C, but is
592.7% and 31.3% faster than PPO and ACKTR. RLSNA2C is
only 27.7% slower than RMSA2C, but is 596.3% and 31.9%
faster than PPO and ACKTR. By using Kostrikov‚Äôs source
code to test PPO and ACKTR, we find that our algorithms have
the similar performance. Therefore, our algorithms achieve a
better trade-off between high convergence performance and
low computational cost.
B. Continuous Control Evaluation
MuJoCo is a high-dimensional continuous control benchmark platform. In this set of experiments, we select 11 tasks
from the MuJoCo environment for performance evaluation.

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

1000
500
0
4M

6M

8M

0

0

10M

2M

4M

Asteriods

Episode Rewards

10M

1000

RMSA2C
RLSNA2C
RLSSA2C

6M

8M

1000

1500000
1000000
500000

2M

BeamRider

4M

6M

8M

2000
1000

100

25

2M

4000
3000
2000

600

40
20

8M

-5

20000
15000
10000

10M

Episode Rewards

4000

150
100
RMSA2C
RLSNA2C
RLSSA2C

50

4M

6M

8M

6M

8M

500

3000
2000

10M

4M

6M

8M

Episode Rewards

300
200
100
0
6M

8M

6M

8M

10000

Episode Rewards

1000
RMSA2C
RLSNA2C
RLSSA2C

0
4M

6M

Timesteps

8M

10M

10M

-6
-8
-10
-12

-16
4M

6M

8M

10M

2M

Krull

6000
4000
2000

30000

2000

25000
20000
15000
10000

0
2M

4M

6M

8M

10M

2M

6M

Timesteps

8M

10M

4M

6M

Timesteps

Pitfall

Pong

30
20

-100
-200
-300
RMSA2C
RLSNA2C
RLSSA2C

-500
4M

RMSA2C
RLSNA2C
RLSSA2C

5000

-400

2M

6M

KungFuMaster

35000

0

8000

4M

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

100

0
2M

8M

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

6M

-14

3000

10M

NameThisGame

12000

1500

500

200

Timesteps

2000

-4

0
4M

4M

IceHockey

-2

1000

2M

MsPacman

2M

Timesteps

300

4000

500

Timesteps

2500

10M

Gravitar

5000

RMSA2C
RLSNA2C
RLSSA2C

1000

10M

-50

Timesteps

Episode Rewards

4M

8M

RMSA2C
RLSNA2C
RLSSA2C

2M

0
2M

6M

400

10M

Kangaroo

2000

1500

4M

0
2M

Jamesbond

10M

0

-100

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

8M

RMSA2C
RLSNA2C
RLSSA2C

100

Timesteps

500

6M

FishingDerby

50

-15

600

RMSA2C
RLSNA2C
RLSSA2C

Episode Rewards

4M

4M

Timesteps

Timesteps

0
2M

2M

DoubleDunk

2M

1000

0

10M

200

10M

-10

10M

Gopher

5000

200

8M

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

250

6M

-25
2M

Frostbite

8M

300

-20

Timesteps

300

10M

400

0
4M

Episode Rewards

6M

8M

500

100

0

25000

6M

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

4M

Breakout

700

60

2M

Episode Rewards

4M

10M

5000

2M

Boxing

80

10M

0
2M

Episode Rewards

8M

5000

0

Episode Rewards

6M

Episode Rewards

Episode Rewards

Episode Rewards

5000

1000

Episode Rewards

4M

DemonAttack

35000
30000

8M

10000

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

10M

RMSA2C
RLSNA2C
RLSSA2C

10M

Episode Rewards

10M

Centipede

7000

400

8M

Episode Rewards

8M

Timesteps

6000

6M

-20
6M

8M

0
4M

0

0
4M

6M

BattleZone

15000

400

120

20
2M

4M

Timesteps

600

2M

Episode Rewards

Episode Rewards

3000

2M

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

30

10M

800

10M

Bowling

4000

8M

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

5000

6M

0

10M

RMSA2C
RLSNA2C
RLSSA2C

4000

200

Timesteps

7000

4M

BankHeist

1200

RMSA2C
RLSNA2C
RLSSA2C

0
4M

6000

Timesteps

2000000

0

8000

0
2M

Episode Rewards

Episode Rewards

2500000

Episode Rewards

8M

Atlantis

3000000

1500

6000

6M

RMSA2C
RLSNA2C
RLSSA2C

2000

Timesteps

2000

2M

2000
1000

Timesteps

500

3000

100

Episode Rewards

2M

200

10000

Episode Rewards

1500

300

Asterix

12000

RMSA2C
RLSNA2C
RLSSA2C

Episode Rewards

2000

4000

Episode Rewards

2500

Assault

5000

RMSA2C
RLSNA2C
RLSSA2C

400

Episode Rewards

Episode Rewards

3000

Amidar

500

RMSA2C
RLSNA2C
RLSSA2C

Episode Rewards

Alien

3500

9

10
0
-10
RMSA2C
RLSNA2C
RLSSA2C

-20
-30

2M

4M

6M

Timesteps

8M

10M

2M

4M

6M

Timesteps

8M

10M

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

Riverraid

5000

0

RMSA2C
RLSNA2C
RLSSA2C

6000

4000

2000

6M

8M

10M

2M

4M

6M

8M

Episode Rewards

600
400

2M

4M

6M

8M

30000
20000

6M

8M

10M

6000

-20

2M

Tutankham

100000

200
150
100

0
6M

8M

8M

10M

8M

10M

RMSA2C
RLSNA2C
RLSSA2C

5000
4000
3000
2000

2M

4M

6M

8M

10M

2M

4M

Timesteps

6000

6M

Timesteps

Zaxxon

WizardOfWor

7000

RMSA2C
RLSNA2C
RLSSA2C

40000
20000

10M

0

10M

60000

0
4M

8M

80000

50

2M

6M

UpNDown

120000

Episode Rewards

250

4M

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

8M

1000

Timesteps

300

6M

TimePilot

7000

RMSA2C
RLSNA2C
RLSSA2C

-25

Episode Rewards

4M

4M

Timesteps

Tennis

-15

0
2M

RMSA2C
RLSNA2C
RLSSA2C

2M

RMSA2C
RLSNA2C
RLSSA2C

Episode Rewards

0

200

10M

10000

200

400

Timesteps

RMSA2C
RLSNA2C
RLSSA2C

40000

800

600

0

10M

StarGunner

50000

RMSA2C
RLSNA2C
RLSSA2C

1000

10000

Timesteps

SpaceInvaders

1200

20000

Episode Rewards

Timesteps

Episode Rewards

4M

30000

Seaquest

800

RMSA2C
RLSNA2C
RLSSA2C

0

0
2M

Episode Rewards

Episode Rewards

Episode Rewards

Episode Rewards

10000

RoadRunner

40000

8000
RMSA2C
RLSNA2C
RLSSA2C

Episode Rewards

Qbert
15000

Episode Rewards

10

5000
4000
3000
2000
1000

1000

RMSA2C
RLSNA2C
RLSSA2C

500

0

0

10M

2M

Timesteps

4M

6M

8M

10M

2M

Timesteps

4M

6M

8M

10M

2M

4M

Timesteps

6M

Timesteps

Fig. 1. Convergence comparison of our algorithms against RMSA2C on 40 Atari games trained for 10M timesteps.

TABLE I
L AST 100 AVERAGE E PISODE R EWARDS OF OUR ALGORITHMS AND RMSA2C ON 40 ATARI G AMES T RAINED FOR 10M T IMESTEPS
Game

RMSA2C

RLSSA2C

RLSNA2C

Game

RMSA2C

RLSSA2C

RLSNA2C

Alien

975.8

1375.7

1675.0

Jamesbond

423.5

379.0

30.0

Amidar

234.9

405.6

301.1

Kangaroo

992.0

1512.0

60.0

Assault

1281.7

3346.9

3136.4

Krull

7327.0

7996.3

3715.0

Asterix

2937.0

5138.5

8140.0

KungFuMaster

20427.0

25224.0

29000.0

Asteroids

1427.4

1344.8

1626.0

MsPacman

1846.9

1916.4

2195.0

Atlantis

1085676.0

1885303.0

2747970.0

NameThisGame

6054.1

8592.8

8555.0

BankHeist

1077.1

789.5

662.0

Pitfall

-63.4

-6.7

-2.8

BattleZone

4620.0

11200.0

8100.0

Pong

19.2

20.5

19.6

BeamRider

4602.9

5225.5

5313.0

Qbert

4267.5

13064.5

12020.0

Bowling

29.1

30.9

29.3

Riverraid

7572.3

7193.4

7125.0

Boxing

93.3

97.2

100.0

RoadRunner

30705.0

33160.0

23300.0
1756.0

Breakout

389.9

460.5

445.9

Seaquest

1754.8

1728.8

Centipede

2875.6

4300.1

2950.9

SpaceInvaders

1001.2

677.4

591.0

DemonAttack

25309.3

13252.4

2619.5

StarGunner

36820.0

40808.0

33280.0

DoubleDunk

-14.4

-15.1

-2.0

Tennis

-22.4

-16.2

-22.1

FishingDerby

10.2

18.3

27.8

TimePilot

3471.0

4648.0

4480.0

Frostbite

250.7

277.2

267.0

Tutankham

211.9

236.5

227.7

Gopher

986.0

3585.0

1088.0

UpNDown

14666.5

24466.6

69848.0

Gravitar

204.5

176.5

240.0

WizardOfWor

2770.0

2626.0

1920.0

IceHockey

-11.2

-6.5

-7.4

Zaxxon

8.0

87.0

360.0

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

TABLE II
RUNNING S PEED C OMPARISON ON S IX ATARI G AMES
( Timesteps / Second )
Game

RMSA2C

PPO

ACKTR

11

VI. C ONCLUSION

RLSSA2C

RLSNA2C

Alien

2897

320

1690

2056

2021

Breakout

3033

321

1668

2057

2150

Pong

3265

324

1754

2411

2402

SpaceInvaders

3124

328

1731

2293

2306

StarGunner

3436

341

1794

2515

2512

Zaxxon

3201

336

1750

2301

2311

Mean

3159

328

1731

2272

2284

In contrast to in Atari, the state in MuJoCo is a multiple
dimensional vector, and the action space is continuous.
All tested algorithms also use two disjoint FNNs defined
in [15]. One is the critic network, the other is the actor
network. Both networks have two same fc hidden layers with
64 Tanh neurons. The critic output layer is a linear fc layer
for predicting the value function. The actor network uses a
linear fc layer with bias to represent the mean and the standard
deviation of the Gaussian policy [24]. All settings for five
tested algorithms are the same as those in Section V. A, except
for Œ± = 0.001 in RLSNA2C.
The convergence comparison of our algorithms against
RMSA2C on 11 MuJoCo tasks trained for 10 million timesteps
is shown in Fig. 2. It is clear that RLSSA2C and RLSNA2C
outperform RMSA2C on almost all tasks. Among these three
algorithms, RLSSA2C has the best convergence performance
and stability. Compared with RMSA2C, RLSSA2C wins on
10 tasks. On Ant, HalfCheetah, Hopper, Pusher, Reacher,
Striker, Swimmer, Thrower and Walker2d tasks, RLSSA2C is
significantly superior to RMSA2C in terms of convergence
speed and convergence quality. Compared with RMSA2C,
RLSNA2C also wins on 9 tasks. On Pusher, Reacher, and
Swimmer tasks, RLSNA2C performs very well.
In Table III, we present the top 10 average episode rewards of our algorithms and RMSA2C on 11 MuJoCo tasks
trained for 10 million timesteps. From Table III, RLSSA2C
and RLSNA2C obtain the highest rewards on 7 and 2 tasks
respectively, but RMSA2C only wins on 1 task.
The running speed comparison of our algorithms against
RMSA2C, PPO and ACKTR on six MuJoCo tasks is shown
in Table IV. Among these five algorithms, RMSA2C has the
highest computational efficiency, RLSSA2C and RLSNA2C
are listed 2nd and 3rd respectively, and PPO is listed last. In
detail, RLSSA2C is only 8.9% slower than RMSA2C, but is
1851.9% and 30.1% faster than PPO and ACKTR. RLSNA2C
is only 12.0% slower than RMSA2C, but is 1785.9% and
25.7% faster than PPO and ACKTR.
Obviously, RLSSA2C and RLSNA2C also perform very
well on MuJoCo tasks. In summary, our both algorithms
have better sample efficiency than RMSA2C, and have higher
computational efficiency than PPO and ACKTR.

In this paper, we proposed two RLS-based A2C algorithms,
called RLSSA2C and RLSNA2C. To the best of our knowledge, they are the first RLS-based DAC algorithms. Our both
algorithms use the RLS method to train the critic network
and hidden layers of the actor network. Their main difference
is their policy learning. RLSSA2C uses SPG and an ordinary first-order gradient descent algorithm to learn the policy
parameter, but RLSNA2C uses NPG, the Kronecker-factored
approximation and the RLS method to learn the compatible
parameter and the policy parameter. We also analyzed the
complexity and convergence of our both algorithms, and presented three tricks for further accelerating their convergence.
We tested our both algorithms on 40 Atari discrete control
games and 11 MuJoCo continuous control tasks. Experimental
results show that our both algorithms have better sample
efficiency than RMSA2C on most games or tasks, and have
higher computational efficiency than PPO and ACKTR. In
future work, we will try to establish the convergence of our
both algorithms and improve the stability of RLSNA2C.
R EFERENCES
[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.
Cambridge, MA, USA: MIT press, 2018.
[2] W. B. Powell, Approximate Dynamic Programming: Solving the Curses
of Dimensionality. Wiley, 2007.
[3] D. Silver et al., ‚ÄúMastering the game of Go without human knowledge,‚Äù
Nature, vol. 550, pp. 354-359, 2017.
[4] K. Zhu and T. Zhang, ‚ÄúDeep reinforcement learning based mobile robot
navigation: A review,‚Äù Tsinghua Sci. Tech., vol. 26, no. 5, pp. 674-691,
Oct. 2021.
[5] B. R. Kiran et al., ‚ÄúDeep reinforcement learning for autonomous driving:
A survey,‚Äù IEEE Trans. Intell. Trans. Syst., pp. 1-18, 2021.
[6] A. Tsantekidis, N. Passalis, A. S. Toufa, K. Saitas-Zarkias, S. Chairistanidis and A. Tefas, ‚ÄúPrice trailing for financial trading using deep
reinforcement learning,‚Äù IEEE Trans. Neural Netw. Learn. Syst., vol. 32,
no. 7, pp. 2837-2846, 2021.
[7] X. Xu, C. Lian, L. Zuo and H. He, ‚ÄúKernel-based approximate dynamic
programming for real-time online learning control: An experimental
study,‚Äù IEEE Trans. Control. Syst. Technol., vol. 22, no. 1, pp. 146-156,
Jan. 2014.
[8] V. Mnih et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù Nature, vol. 518, pp. 529-533, 2015.
[9] V. Hasselt, A. Guez and D. Silver. ‚ÄúDeep reinforcement learning with
double Q-learning,‚Äù in Proc. 30th AAAI Conf. Artif. Intell., Feb. 2016,
pp. 2049-2100.
[10] M. William and L. Sergey, ‚ÄúGuided policy search as approximate mirror
descent,‚Äù in Proc. 30th Adv. neural inf. proces. syst., 2016, pp. 4015-4023.
[11] N. Heess, T. Degris, D. Wierstra and M. Riedmiller, ‚ÄúDeterministic
policy gradient algorithms,‚Äù in Proc. 31th Int. Conf. Mach. Learn. (ICML),
2014, pp. 387-395.
[12] T. P. Lillicrap et al., ‚ÄúContinuous control with deep reinforcement
learning,‚Äù in Proc. 4th Int. Conf. Learn. Represent. (ICLR), 2019.
[13] N. Heess, J. J. Hunt, T. P. Lillicrap and D. Silver, ‚ÄúMemory-based
control with recurrent neural networks,‚Äù 2015. [Online]. Available:
https://arxiv.org/abs/1512.04455
[14] S. Fujimoto, H. V. Hoof and D. Meger, ‚ÄúAddressing function approximation error in actor-critic methods,‚Äù in Proc. 35th Int. Conf. Mach. Learn.
(ICML), 2018, pp. 1587-1596.
[15] V. Mnih et al., ‚ÄúAsynchronous methods for deep eeinforcement learning,‚Äù in Proc. 33th Int. Conf. Mach. Learn. (ICML), 2016, pp. 2850-2869.
[16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov,
‚ÄúProximal policy optimization algorithms,‚Äù 2017. [Online]. Available:
https://arxiv.org/abs/1707.06347
[17] J. Schulman, S. Levine, P. Moritz, M. I. Jordan and P. Abbeel, ‚ÄúTrust
region policy optimization,‚Äù in Proc. 32th Int. Conf. Mach. Learn. (ICML),
2015, pp. 1889-1897.

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

Ant

5000

1000
0

1000
500
0
RMSA2C
RLSNA2C
RLSSA2C

-500

-1000
-2000

-1000
4M

6M

8M

2M

4M

6M

8M

500

4M

6M

8M

10M

2M

RMSA2C
RLSNA2C
RLSSA2C

200

Reacher

0

-100
-150

6M

8M

RMSA2C
RLSNA2C
RLSSA2C

2M

4M

6M

8M

Swimmer

2M

4M

10
0
-10

-500

-700
6M

8M

1500

-400

-600

8M

10M

RMSA2C
RLSNA2C
RLSSA2C

2M

4M

RMSA2C
RLSNA2C
RLSSA2C

6M

8M

10M

Timesteps

Walker2d

2000

-300

-30
4M

Thrower

-200

-20
2M

6M

-100

Episode Rewards

20

-500

Timesteps

0
RMSA2C
RLSNA2C
RLSSA2C

30

-400

-700

10M

Timesteps

40

10M

-300

-600

-150

10M

Timesteps

Episode Rewards

-100

Episode Rewards

4M

8M

Striker

-100

-50

RMSA2C
RLSNA2C
RLSSA2C

-250
2M

6M

-200

-200

0

4M

Timesteps

Episode Rewards

400

RMSA2C
RLSNA2C
RLSSA2C

Timesteps

Episode Rewards

Episode Rewards

600

4000

0
2M

-50

800

6000

2000

RMSA2C
RLSNA2C
RLSSA2C

10M

Pusher

0

1000

Episode Rewards

1000

Timesteps

InvertedPendulum

1200

8000

1500

0

10M

Timesteps

InvertedDoublePendulum

10000

Episode Rewards

2000

Episode Rewards

3000

2M

Hopper

2000

1500

Episode Rewards

Episode Rewards

4000

HalfCheetah

2000

RMSA2C
RLSNA2C
RLSSA2C

12

RMSA2C
RLSNA2C
RLSSA2C

1000
500
0
-500

10M

2M

4M

Timesteps

6M

8M

Timesteps

10M

2M

4M

6M

8M

10M

Timesteps

Fig. 2. Convergence comparison of our algorithms against RMSA2C on 11 MuJoCo tasks trained for 10M timesteps.
TABLE III
T OP 10 AVERAGE E PISODE R EWARDS OF OUR A LGORITHMS AND RMSA2C ON 11 M U J O C O TASKS T RAINED FOR 10M T IMESTEPS
Game
Ant

RMSA2C

RLSSA2C

RLSNA2C

-62.0

4607.0

941.5

Game

RMSA2C

RLSSA2C

RLSNA2C

Reacher

-21.9

-4.9

-9.2

HalfCheetah

454.6

1630.3

1625.4

Striker

-336.1

-124.7

-264.3

Hopper

921.9

2068.9

2133.6

Swimmer

33.6

32.9

32.3

InvertedDoublePendulum

7957.0

9333.4

8500.0

Thrower

-106.2

-68.5

-59.3

InvertedPendulum

1000.0

1000.0

1000.0

Walker2d

1682.0

3306.9

1363.6

Pusher

-117.7

-39.0

-56.6

TABLE IV
RUNNING S PEED C OMPARISON ON S IX M U J O C O TASKS
( Timesteps / Second )
Game

RMSA2C

PPO

ACKTR

RLSSA2C

RLSNA2C

Ant

5911

372

4584

5646

5380

Hopper

7735

375

5574

7160

6909

InvertedPendulum

10076

384

6633

8997

8399

Pusher

7873

368

5558

7185

7073

Swimmer

9033

384

6068

7996

7927

Walker2d

7716

373

5438

7052

6855

Mean

8057

376

5643

7339

7091

[18] Y. Wu, E. Mansimov, S. Liao, A. Radford and J. Schulman, ‚ÄúOpenai baselines : acktr and a2c.‚Äù [Online]. Available:
https://openai.com/blog/baselines-acktr-a2c/

[19] Z. Wang et al., ‚ÄúSample efficient actor-critic with experience replay,‚Äù in
Proc. 5th Int. Conf. Learn. Represent. (ICLR), 2017.
[20] X. Gong, J. Yu, S. LaÃà and H. Lu, ‚ÄúActor-critic with familiarity-based
trajectory experience replay,‚Äù Inf. Sci., vol. 582, pp. 633-647, 2022.
[21] T. Haarnoja, A. Zhou, P. Abbeel and S. Levine, ‚ÄúSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic
actor,‚Äù in Proc. 35th Int. Conf. Mach. Learn. (ICML), Jul. 2018, pp. 29762989.
[22] J. Byun, B. Kim and H. Wang, ‚ÄúProximal policy gradient:
PPO with policy gradient,‚Äù Oct. 2020. [Online]. Available:
https://arxiv.org/abs/2010.09933
[23] J. Martens and R. Grosse, ‚ÄúOptimizing neural networks with kroneckerfactored approximate curvature,‚Äù in Proc. 32th Int. Conf. Mach. Learn.
(ICML), Jul. 2015, pp. 2398-2407.
[24] Y. Wu, E. Mansimov, S. Liao, R. Grosse and J. Ba, ‚ÄúScalable trustregion method for deep reinforcement learning using kronecker-factored
approximation,‚Äù in Proc. 31th Adv. neural inf. proces. syst., 2017, pp.
5280-5289.
[25] I. Kostrikov, ‚ÄúPyTorch implementations of reinforcement learning
algorithms,‚Äù
GitHub
repository,
2018.
[Online].
Available:
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail
[26] T. Tieleman and G. Hinton, ‚ÄúLecture 6.5-rmsprop: Divide the gradient
by a running average of its recent magnitude,‚Äù COURSERA: Neural Netw.

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS

Mach. Learn., vol. 4, pp. 26-30, 2012.
[27] S. J. Bradtke and A. G. Barto, ‚ÄúLinear least-squares algorithms for
temporal difference learning,‚Äù Mach. Learn., vol. 22, pp. 33-57, Mar.
1996.
[28] V. Konda and J. Tsitsiklis, ‚ÄúActor-critic algorithms,‚Äù in Proc. 13th Adv.
neural inf. proces. syst., 2000, pp. 1008-1014.
[29] X. Xu, H. He and D. Hu, ‚ÄúEfficient reinforcement learning using
recursive least-squares methods,‚Äù J. Artif. Intell. Res., vol. 16, no. 1, pp.
259-292, Jan. 2002.
[30] J. Peters, S. Vijayakumar and S. Schaal, ‚ÄúReinforcement learning for
humanoid robotics,‚Äù in Proc. 3th IEEE-RAS Int. Conf. Hum.-Rob., 2003,
pp. 1-20.
[31] J. Park, J. Kim and D. Kang, ‚ÄúAn RLS-based natural actor-critic
algorithm for locomotion of a two-linked robot arm,‚Äù in Proc. Lect. Notes
Comput. Sci., 2005, pp. 65-72.
[32] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh and M. Lee, ‚ÄúNatural
actor-critic algorithms,‚Äù Automatica, vol. 45, no. 11, pp. 2471-2482, Nov.
2009.
[33] L. Li, D. Li, T. Song and X. Xu, ‚ÄúActor-critic learning control based on
`2 -regularized temporal-difference prediction with gradient correction,‚Äù
IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 12, pp. 5899-5909,
Dec. 2018.
[34] C. Zhang, Q. Song, H. Zhou, Y. Ou, H. Deng and T. Yang, ‚ÄúRevisiting
recursive least squares for training deep neural networks,‚Äù 2021. [Online].
Available: https://arxiv.org/abs/2109.03220v1
[35] R. S. Sutton, D. McAllester, S. Singh and Y. Mansour, ‚ÄúPolicy gradient
methods for reinforcement learning with function approximation,‚Äù in
Proc. 12th Adv. neural inf. proces. syst., Nov. 1999, pp. 1057-1063.
[36] P. Chou, D. Maturana and S. Scherer, ‚ÄúImproving stochastic policy
gradients in continuous control with deep reinforcement learning using
the beta distribution,‚Äù in Proc. 34th Int. Conf. Mach. Learn. (ICML), 2017,
pp. 1386-1396.
[37] J. Peters and S. Schaal, ‚ÄúNatural actor-critic,‚Äù Neurocomputing, vol. 71,
no. 7-9, pp. 1180-1190, Mar. 2008.
[38] K. B. Petersen and M. S. Pedersen, The Matrix Cookbook. Technical
University of Denmark, 2012.
[39] H. Robbins and S. Monro, ‚ÄúA stochastic approximation method,‚Äù Annal.
Math. Stat., vol. 22, no. 3, pp. 400-407, 1951.
[40] Z. Yang, K. Zhang, M. Hong and T. Basar, ‚ÄúA finite sample analysis
of the actor-critic algorithm,‚Äù in Proc. 57th IEEE Conf. Decis. Control,
Dec. 2018, pp. 2759-2764.
[41] B. Liu, Q. Cai, Z. Yang and Z. Wang, ‚ÄúNeural proximal/trust region
policy optimization attains globally optimal policy,‚Äù in Proc. 33th Adv.
neural inf. proces. syst., 2019.
[42] L. Wang, Q. Cai, Z. Yang and Z. Wang, ‚ÄúNeural policy gradient methods:
Global optimality and rates of convergence,‚Äù 2019. [Online]. Available:
https://arxiv.org/abs/1909.01150
[43] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning. Cambridge,
MA: MIT Press, 2016.

13

