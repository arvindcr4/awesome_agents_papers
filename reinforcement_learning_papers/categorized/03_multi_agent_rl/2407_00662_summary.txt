Here's the extracted information:

**PAPER:** Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach
**TITLE:** Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach
**ARXIV_ID:** 2407.00662v2
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** This study introduces a multi-agent training system for Pommerman, a competitive game environment, using a combination of curriculum learning and population-based self-play. The system consists of two stages: curriculum learning, where agents learn essential skills through incremental difficulty phases, and population-based self-play, where agents compete against each other to improve their performance. The study also addresses two challenges in multi-agent training: sparse rewards and suitable matchmaking mechanisms.
**KEY_CONTRIBUTIONS:**
* Introduces a multi-agent training system for Pommerman using curriculum learning and population-based self-play.
* Proposes an adaptive annealing factor based on agents' performance to dynamically adjust the dense exploration reward during training.
* Implements a matchmaking mechanism utilizing the Elo rating system to pair agents effectively.
* Demonstrates that the trained agent can outperform top learning agents without requiring communication among allied agents.
* Evaluates the performance of the trained agent against other agents, including baseline agents and top-performing agents from previous competitions.
