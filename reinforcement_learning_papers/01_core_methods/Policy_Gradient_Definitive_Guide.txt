arXiv:2401.13662v2 [cs.LG] 1 Mar 2024

The Definitive Guide to Policy Gradients in Deep Reinforcement
Learning:
Theory, Algorithms and Implementations

Matthias Lehmann
University of Cologne

A BSTRACT
In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement
learning. While all these algorithms build on the Policy Gradient Theorem, the specific design
choices differ significantly across algorithms. We provide a holistic overview of on-policy policy
gradient algorithms to facilitate the understanding of both their theoretical foundations and their
practical implementations. In this overview, we include a detailed proof of the continuous version
of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical
algorithms. We compare the most prominent algorithms on continuous control environments and
provide insights on the benefits of regularization. All code is available at https://github.com/
Matt00n/PolicyGradientsJax.

Contents
1

Introduction

1

2

Preliminaries
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Problem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Value Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 On-Policy Policy Gradient Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
2
2
2
3
4
5

3 Theoretical Foundations of Policy Gradients
3.1 Policy Gradient Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Value Function Estimation with Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8
8
12
14

4

Policy Gradient Algorithms
4.1 REINFORCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 A3C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 TRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 V-MPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 Comparing Design Choices in Policy Gradient Algorithms . . . . . . . . . . . . . . . . . . . . . . .

14
15
15
16
19
21
23

5

Convergence Results
5.1 Literature Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Mirror Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.1 Fundamentals of Mirror Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.2 Policy Gradient Algorithms as Instances of Mirror Learning . . . . . . . . . . . . . . . . . .

25
25
25
25
26

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

5.2.3

Convergence Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

6

Numerical Experiments

33

7

Conclusion

34

Appendices

41

A Hyperparameters

41

B Extended Experiments
B.1 Comparison to RL frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Entropy Bonus in A2C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 A2C and REINFORCE with Multiple Update Epochs . . . . . . . . . . . . . . . . . . . . . . . . . .

41
41
42
42

C V-MPO: Derivation Details

42

D Auxiliary Theory

46

1

Introduction

Reinforcement Learning (RL) is a powerful set of methods for an agent to learn how to act optimally in a given
environment to maximize some reward signal. In contrast to other methods such as dynamic programming, RL achieves
this task of learning an optimal policy, which dictates the optimal behavior, via a trial-and-error process of interacting
with the environment [75]. Most early successful applications of RL use value-based methods (e.g., [84, 79, 55]), which
estimate the expected future rewards to inform the agent’s decisions. However, these methods only indirectly optimize
the true objective of learning an optimal policy [82] and are non-trivial to apply in settings with continuous action
spaces [75].
In this work, we discuss policy gradient algorithms [75] as an alternative approach, which aims to directly learn an
optimal policy. Policy gradient algorithms are by no means new [7, 78, 87, 88], but this subfield only gained traction
in recent years following the emergence of deep RL [55] with the development of various powerful algorithms (e.g.,
[54, 71, 73]). Deep RL is a subfield of RL, which uses neural networks and other deep learning methods. The increased
interest in policy gradient algorithms is due to several appealing properties of this class of algorithms. They can be
used natively in continuous action spaces without compromising the applicability to discrete spaces [77]. In contrast
to value-based methods, policy gradient algorithms inherently learn stochastic policies, which results in smoother
search spaces and partly remedies the exploration problem of having to acquire knowledge about the environment in
order to optimize the policy [75, 77]. In some settings, the optimal policy may also be stochastic itself [75]. Lastly,
policy gradient methods enable smoother changes in the policy during the learning process, which may result in better
convergence properties [77].
Our goal is to present a holistic overview of policy gradient algorithms. In doing so, we limit the scope to on-policy
algorithms, which we will define in Section 2. Thus, we exclude some popular algorithms including DDPG [50], TD3
[24] and SAC [28]. See Figure 1 for an overview of RL and the subfields we cover. Our contributions are as follows:
• We give a comprehensive introduction to the theoretical foundations of policy gradient algorithms including a
detailed proof of the continuous version of the Policy Gradient Theorem.
• We derive and compare the most prominent policy gradient algorithms and provide high quality pseudocode to
facilitate understanding.
• We release competitive implementations of these algorithms, including the, to the best of our knowledge, first
publicly available V-MPO implementation displaying performance on par with the results in the original paper.
The remainder of this paper is organized as follows. Section 2 introduces fundamental definitions in RL as well as an
overview of deep learning. Section 3 derives the theoretical foundations of policy gradient algorithms with a special
focus on proving the Policy Gradient Theorem, based on which we will construct several existing practical algorithms in
Section 4. In Section 5, we discuss convergence results from literature. Section 6, presents the results of our numerical
experiments comparing the discussed algorithms. Section 7 concludes.
1

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

RL Algorithms
Model-Free RL
Policy Gradient Algorithms

Model-Based RL

Value-Based Algorithms

Dyna-Q
MuZero

On-Policy Algorithms

Off-Policy Algorithms

DQN

...

SARSA
REINFORCE

DDPG

A3C / A2C

SAC

TRPO

...

...

PPO
V-MPO
...
Figure 1: Simplified taxonomy of RL algorithms. Subfields of RL we focus on are highlighted in gray.

2

Preliminaries

In this section, we present prerequisites for subsequent chapters. Specifically, we introduce our notation in Section 2.1
and present overviews of RL in Section 2.2 and of deep learning in Section 2.3. Furthermore, we list several well-known
definitions and results from probability theory, measure theory and analysis, which we use in our paper, in Appendix
Appendix D.
2.1 Notation
We denote the set of natural numbers by N, natural numbers including zero by N0 , real numbers by R and positive real
numbers by R+ . We denote d-dimensional real-numbered vector spaces as Rd . By P(A), we denote the power set of a
set A. Where possible, we denote random variables with capital letters and their realizations with the corresponding
lower case letters. For any probability measure P, we denote the probability of an event X = x as P(X = x). Similarly,
we write P(X = x | Y = y) for conditional probabilities. When it is clear, which random variable is referred to, we
regularly omit it to shorten notation, i.e. P(X = x) = P(x). We identify measurable spaces (A, Σ) just by the set
A as we always use the respective power set P(A) for discrete sets and the Borel algebra for intervals in Rd as the
respective σ-algebra Σ. We express most Lebesgue integrals w.r.t. theRLebesgue measure
R λ using Theorem D.9. To
simplify notation, we write integrals for measurable functions f on A as a∈A f (a) da := a∈A f (a) dλ(a). We denote
that a random variable X follows a probability distribution p by X ∼ p. For any random variable X ∼ p, we denote
by EX∼p [X] and VarX∼p [X] its expectation and variance. We denote the set of probability distributions
over some
R
measurable space A as ∆(A). We write |A| for the cardinality of a finite set A or area of a region a∈A da. For any
variable or function x, we commonly denote approximations to it by x̂.
2.2 Reinforcement Learning
In the following, we formally describe the general problem setting encountered in RL, define fundamental functions and
introduce the subfields of RL our work is further concerned with. Sections 2.2.1 and 2.2.2 are based on [75], Chapter 3.
2.2.1 Problem Setting
Each problem instance in RL consists of an agent and an environment with which he interacts to achieve some specific
goal. The environment comprises everything external to the agent and can be formalized as a Markov Decision Process
(MDP). Let an action space A be the set of all actions the agent can take and let a state space S be the set of all
possible states, i.e. snapshots of the environment at any given point in time. State and action spaces can be discrete or
continuous1 and we assume both to be compact and measurable. We write an MDP as a tuple M = (S, A, P, γ, p0 ),
where P : S × A → ∆(S × R) is the environment’s transition function, which defines the probability2 P (s′ , r | s, a)
Here, we call a state/action space continuous if it is an interval in Rd for d ∈ N.
Technically, this is the value of the probability density function for continuous distributions. However, we unify terminology by
referring to the values of probability density functions as probabilities here and in the following.
1
2

2

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

of transitioning to a new environment state s′ and receiving reward r ∈ R when the agent uses action a in state s,
γ ∈ [0, 1] is a discount rate and p0 ∈ ∆(S) is a probability distribution over potential starting states. We assume
rewards r to be bounded. In the following, our notation assumes state and action spaces to be continuous.
We call sequences of states, actions and rewards (st , at , rt+1 , st+1 , at+1 , rt+2 , . . . ,
st+k−1 , at+k−1 , rt+k , st+k ) trajectories. A one-step trajectory, i.e. a tuple (st , at , rt+1 , st+1 ) is called a transition. In
this work, we limit ourselves to episodic settings, where the agent only interacts with the environment for a finite number
of at most T steps after which the environment is reset to a starting state. An episode may however be shorter than T if a
terminal state is reached. Therefore each episode consists of a trajectory (s0 , a0 , r1 , s1 , a1 , r2 , . . . , sT̃ −1 , aT̃ −1 , rT̃ , sT̃ ),
with T̃ ≤ T . Rewards are occasionally omitted from the trajectory notation since they do not
R influence future states.
Correspondingly, we also can compute the alternative transition probabilities P (s′ | s, a) = r∈R P (s′ , r | s, a) dr.
The main goal in reinforcement learning is to solve the control problem of learning a policy π : S → ∆(A) to maximize
PT
the expected return. The return Gt := k=0 γ k rt+k+1 is the discounted sum of rewards from timestep t onwards.
Note that Gt is bounded since rewards are bounded. We denote the probability of taking action a in state s under policy
π with π(a | s). For a policy π, its stationary state distribution dπ determines the probability of being in a specific state
s ∈ S at any point in time when following π.
Let Π be the set of all possible policies. RL algorithms A : Π → Π for the control problem now iteratively learn
policies by interacting with the environment using the current policy to sample transitions, which are then used to
update the policy. We will discuss how these updates can look like in Section 2.2.3. A key characteristic of many RL
problems is a necessary trade-off between exploration and exploitation in this learning process. The agent has no prior
knowledge of the environment and thus needs to explore different transitions in order to learn which states and actions
are desirable. As state and action spaces are typically large however, exploiting the already acquired knowledge about
the environment is also crucial to guide the search process for an optimal policy to subspaces that hold most promise. A
common approach to this exploration problem is to add noise to the policy.
2.2.2 Value Functions
Based on the return, we define the value and action-value functions, which are fundamental in RL. The value function


Vπ (s) := Eπ Gt | St = s
(1)
gives the expected return from state s onwards when following policy π, which selects all subsequent actions. Thus, the
value function states how good it is to be in a specific state s given a policy π. Note that here we follow the general
convention to write this just as an expectation over π. However, it should be noted that this expectation integrates
over all subsequent states and actions that are obtained by following policy π, i.e. Equation (1) computes the expected
return given that all subsequent actions are sampled from π and all rewards and next states are sampled from P . This is
implicit in our notation here as well as in further expectations.
Next, we define the action-value function


Qπ (s, a) := Eπ Gt | St = s, At = a ,
which differs from the value function in that the very first action a is provided as an input to the function and not
determined by the policy. We observe the following relation between Vπ and Qπ :
Z
Vπ (s) =
π(a | s) Qπ (s, a) da.
a∈A

Further, we call
Aπ (s, a) := Qπ (s, a) − Vπ (s)
the advantage function, which determines how good an action a is in state s in relation to other possible actions.
From the definitions of Vπ and Qπ we can derive the so-called Bellman equations [9]. Starting from Equation (1),
we use the definition of the return Gt , explicitly write out the expectation for the first transition and then apply the
definition of Vπ again:


Vπ (s) = Eπ Gt | St = s


= Eπ Rt+1 + γGt+1 | St = s

Z
Z Z
h
i
′
′
=
π(a | s)
P (s , r | s, a) r + γEπ Gt+1 | St+1 = s
dr ds′ da
a∈A

s′ ∈S r∈R

Z

Z

Z

π(a | s)

=
a∈A


P (s′ , r | s, a) r + γVπ (s′ ) dr ds′ da

s′ ∈S r∈R

3

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Thus, we find a formulation of the value function, which depends on the value of subsequent states. Collapsing the
expectation again yields the form known as the Bellman equation of the value function:


Vπ (s) = Eπ Rt+1 + γVπ (St+1 ) .
Similarly, we can find the Bellman equation for the action-value function:


Qπ (s, a) = Eπ Rt+1 + γQπ (St+1 , At+1 ) .

(2)

Now, we can formally define what optimality means in RL. An optimal policy π ∗ is defined by Vπ∗ (s) ≥ Vπ (s)
for all states s and policies π, i.e. any optimal policy maximizes the expected return. It can be shown that in
every finite MDP, a deterministic optimal policy exists [56]. All optimal policies share the same optimal value
function V ∗ (s) := maxπ∈Π Vπ (s) and optimal action-value function Q∗ (s, a) := maxπ∈Π Qπ (s, a) and select actions
a ∈ arg maxa′ Q∗ (s, a′ ) for every state. Applying this to Equation (2) yields the Bellman optimality equation


Q∗ (s, a) = Eπ∗ Rt+1 + γQ∗ (St+1 , At+1 )


= E Rt+1 + γ max
Q∗ (St+1 , a′ )
′
a ∈A

We cite the following result without proof from [75] on how to obtain an optimal policy, which we will revisit in Section
5.
Theorem 2.1. (Generalized Policy Iteration) Let πold be the current policy. Then, Generalized Policy Iteration updates
its policy by


πnew ∈ arg max EA∼π Qπold (s, A)
π∈Π

∞

for all s ∈ S. Let πn n=0 be a sequence of policies obtained through Generalized Policy Iteration. Then, this sequence
converges to an optimal policy, i.e.
lim πn = π ∗
n→∞

and
lim Qπn = Q∗ .

n→∞

2.2.3 On-Policy Policy Gradient Methods
Finally, we will delineate the subfields of RL on which our work focuses. In this context, we will successively introduce
function approximation, policy gradient methods and the on-policy paradigm.
RL algorithms are mostly concerned with learning functions such as π, Vπ or Qπ . Early reinforcement methods learn
exact representations of these by maintaining lookup tables with entries for each possible function input [75]. While
this approach yields theoretical convergence guarantees [56], it is practically very limited. Similar states are treated
independently such that learnings do not generalize from one state to others while specific states are only rarely visited
in large state spaces [75]. Moreover, this approach is not applicable to continuous spaces. Function approximation
remedies these shortcomings by parameterizing the function to be learned. Let fθ (x) be this learnable function, where
θ are the function’s parameters, which are adjusted over the course of learning, and x are the functions inputs such
as states and actions or representations thereof. By choosing fθ to be continuous in its inputs, we can ensure that fθ
generalizes across its inputs when we fit it to sampled transitions [75]. fθ can be as simple as a linear mapping, i.e.
fθ (x) = θT x, however recent works mostly use neural networks as function approximators (e.g., [55, 72]). The field
using neural networks as function approximators is coined deep RL [55]. For the remainder of this paper, you can
consider any learned function to be a neural network unless explicitly stated otherwise, although all our statements
apply to any differentiable function approximators. We will introduce deep learning and neural networks in detail in
Section 2.3.
Policy gradient methods pose an alternative to value-based methods in RL. Most early successes in RL use value-based
methods such as Q-Learning [84] or SARSA [67], that aim at learning a sequence of value functions converging to the
optimal value function, from which an optimal policy can then be inferred. In contrast, policy-based RL, which we
focus on in this work, directly learns a parameterized policy πθ . The main idea in this learning process is to increase the
probability of those actions that lead to higher returns until we reach an (approximately) optimal policy [75]. While
this optimization problem can be approached in several ways, gradient-based methods are most commonly used [82].
Following [77], we define policy gradient methods as follows.
Definition 2.2. (Policy Gradient Algorithm) Let πθ : S → ∆(A) be a fully differentiable function with learnable
parameters θ ∈ Rd mapping states to a probability distribution over actions. Let J : Rd → R be some performance
4

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

measure of the parameters. We call any learning algorithm a policy gradient algorithm if it learns its policy πθ by
updating θ via gradient ascent (or descent) on J, i.e. its updates have the general form
θnew ← θ + α∇θ J(θ),

(3)

where α ∈ R is a step size parameter of the algorithm.
In policy-based RL, two distinct ways exist to have the policy output a probability distribution over actions, from which
actions can be sampled [75]. For discrete action spaces, we construct a discrete distribution over the action space by
normalizing the policies’ raw outputs via a softmax function [26]. In this case, we have
π(a | s) = P

exp(πθ (a | s))
.
exp(πθ (a′ | s))

a′ ∈A

For continuous action spaces, we let πθ output the mean µ and standard deviation σ of a Gaussian distribution, i.e.
πθ (s) = µθ (s), σθ (s) such that
2 !
a − µθ (s)
1
√ exp −
.
π(a | s) =
2σθ (s)2
σθ (s) 2π
This parameterization of a Gaussian distribution for the policy was first introduced by [87, 88]. As action spaces are
commonly bounded, the actions sampled from such a Gaussian are typically transformed to be within these bounds
either by clipping or by applying a squashing distribution [5]. Further, we highlight that the policies learned by policy
gradient methods in both the discrete and the continuous case are generally stochastic. This stands in contrast to
value-based methods which generally learn deterministic policies [77]. Policy gradient methods are the core focus of
this paper and will be discussed in-depth in subsequent sections.
Lastly, we delineate on-policy from off-policy algorithms. In RL, we distinguish between behavior and target policies
[75]. A behavior policy is a policy which generates the data in form of trajectories from which we want to learn, i.e.
this is the policy from which we sample actions when interacting with the environment. Conversely, the target policy is
the policy which we want to learn about to evaluate how good it is in the given environment and improve it. Algorithms
where behavior and target policy are not identical, e.g. Q-Learning [84] or DQN [55], are referred to as off-policy
algorithms. In this work, we only discuss on on-policy algorithms, where behavior and target policy are identical.
Hence, when speaking of policy gradient algorithms in the following, we always implicitly mean on-policy policy
gradient algorithms if not mentioned otherwise.
2.3 Deep Learning
In this section, we introduce deep learning as a subfield of machine learning since its methods are commonly used
in policy gradient algorithms. In recent years, deep learning has emerged as the premier machine learning method
in various fields, enabling state-of-the-art performance in domains such as computer vision (e.g., [44, 29, 22]) and
natural language processing (e.g., [83, 14]. Following [47] and [26], we define deep learning as a set of techniques
to solve prediction tasks by learning multiple levels of representations from raw data using a composition of simple
non-linear functions. This composition of functions, that we will describe in detail later, is referred to as (deep) neural
network. Deep learning stands in contrast to conventional machine learning techniques like logistic regressions, which
typically require hand-engineered representations as inputs to be effective [47]. In the following, we introduce the
general problem setting of deep learning using the notation of [10], formalize neural networks and describe how they
are trained.
Consider measurable spaces X and Y. Z := X × Y is the data space with each element z = (x, y) ∈ Z being a tuple of
input features x ∈ X and a label y ∈ Y. Let M(X , Y) be the set of measurable functions from X to Y. The problems
we encounter in deep learning are prediction tasks. Thus, the goal is to learn a mapping f ∈ M(X , Y) from inputs to
labels by minimizing some loss function L over training data S = {z (1) , . . . , z (m) } such that it generalizes to unseen
data z ∈ Z. To learn the function f , we first select a hypothesis set F ⊂ M(X , Y). Deep learning then provides
learning algorithms A : Z → F that use training data S to learn the desired function f = A(S). Before we discuss this
learning process, we will first further characterize the mapping to be learned.
In deep learning, functions in the hypothesis set F represent instances of neural networks. Note that here we limit
ourselves to feedforward networks, also called multilayer perceptrons (MLP), and will not discuss transformers [83],
recurrent (RNN) [32] or convolutional neural networks (CNN)[44].
Definition 2.3. (Feedforward Neural Network) A feedforward neural network f : X → Y is a composition of functions
f = f (n+1) ◦ · · · ◦ f (1)
5

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

(1)

a1

(2)

a1
(1)

x1

ŷ1

a2

(2)

a2
x2

f (1)

(1)

a3

f (2)

f (3)
(2)
a3

x3

(1)

ŷ2

a4

(2)
a4
(1)

a5

Figure 2: A neural network with hidden layers of sizes 5 and 4 as a directed graph.
that is differentiable almost everywhere. We refer to f (i) , i = 1, . . . , n as hidden layers, whereas f (n+1) is the
non-hidden output layer. Consequently, n denotes the number of of hidden layers in the network. Each hidden layer is
characterized by a layer width Ni . Let N0 and Nn+1 further be the size of the input and output vectors respectively.
Then, we can write each layer as


f (i) (x) = g W (i) x + b(i) ,
where x is the output of the previous layer or the network’s inputs for i = 1, W (i) ∈ RNi ×Ni−1 and b(i) ∈ RNi are the
layer’s weight matrix and bias vector respectively and g : R → R is a differentiable activation function introducing
non-linearity. g is applied element-wise.

An MLP can be characterized by its architecture a = (Ni )n+1
i=0 , g , consisting of layer sizes and the activation function
to be used. The number of layers n + 1 is also referred to as the depth of the network. The Universal Approximation
Theorem [17, 35] underpins the expressivity of neural networks: a two-layer network can already approximate any
measurable function arbitrarily well under weak conditions on the activation function. Technically, each layer can
feature a different activation function albeit this is uncommon. The activation function of the output layer is not defined
by the architecture a but is derived from the prediction task. The standard choice for activation functions in the hidden
layers is a rectified linear unit (ReLU)3 [57], due to typically fast learning [25]. See [48] for an overview of other
commonly used activation functions. The output layer typically uses no activation function for regression tasks and
sigmoid or softmax functions
for classification tasks. Each element of a layer f (i) is called a neuron. The outputs

(i)
(i)
(1)
a = f ◦ ··· ◦ f
(x) of any layer are the learned representations of the inputs x. We denote the outputs f (x),
i.e. the predictions, of the neural network with ŷ. Figure 2 depicts a neural network as an acyclic directed graph.
Selecting a hypothesis set F is done implicitly by specifying an architecture a. Hence, we denote the hypothesis set
for architecture a, whose elements are all MLPs with that architecture, by Fa . The MLPs in Fa therefore differ only
in their weights and biases. We call these the (learnable) parameters of the network and typically collect them in a
flattened parameter vector θ ∈ Rd . We denote an MLP with parameters θ as fθ .
Given a hypothesis set Fa , we now aim to learn a neural network fθ ∈ Fa , i.e. to learn parameters θ, such that we
reduce the expected loss or risk,
Z


R(f ) := EZ∼PZ L(f, Z) =
L(f, z) dPZ (z),
z∈Z

over the data distribution PZ for some appropriately chosen differentiable loss function L : F × Z → R [10, 26].
Here PZ is the image measure of Z on Z, from which the training data S = {z (1) , . . . , z (m) } and unknown out-ofsample data z is drawn. We generally assume that training z (1) , . . . , z (m) and out-of-sample data z are realizations of
i.i.d. random variables Z (1) , . . . , Z (m) , Z ∼ PZ [10]. For a given MLP fθ = A(S) trained on S, the risk becomes
3

Note that the ReLU function is not differentiable at 0. In practice, this is circumvented by using its sub-derivatives.

6

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning



R(fθ ) = EZ∼PZ L(fθ , Z) | S . In practice, noisy data results in a positive lower bound on risk, i.e. an irreducible
error [10]. Common loss functions are binary cross-entropy loss,

L(f, (x, y)) = − y · ln(f (x)) + (1 − y) · ln(1 − f (x)) ,
for (binary) classification and mean squared error (MSE),
2
L(f, (x, y)) = y − f (x) ,
for regression tasks. Sometimes, loss functions are augmented by regularization terms Ω(θ) such as an L2-penalty of
the parameters, i.e. β∥θ∥22 with β ∈ R [26].
The data distribution PZ is generally unknown. Hence, we replace it by an empirical distribution based on the sampled
training data S and use empirical risk minimization (ERM) as the learning algorithm to minimize it [26, 10].
Definition 2.4. (Empirical Risk). Given training data S = {z (1) , . . . , z (m) } and a function fθ ∈ M(X , Y), the
empirical risk is defined by
m
1 X
R̂S (fθ ) :=
L(fθ , z (i) ).
(4)
m i=1
Definition 2.5. (ERM learning algorithm). Given hypothesis set Fa and training data S, an empirical risk minimization
algorithm Aerm terminates with an (approximate4 ) minimizer fˆS ∈ Fa of empirical risk:
Aerm (S) = fˆS ∈ arg min R̂S (f ).
f ∈Fa

We approximately minimize empirical risk typically via gradient-based methods due to efficient computation of
point-wise derivatives via the backpropagation algorithm [66, 40]. Backpropagation means the practical application
of the chain rule to neural networks. The gradient of the objective function L with respects to the i-th layer’s
inputs a(i−1) can be computed by working backwards from the gradient with respects to the layer’s outputs a(i) as
P
(i)
∇a(i−1) L = j (∇a(i−1) aj ) ∂L(i) . From these gradients, the gradients with respects to the weights and biases in each
∂aj

layer can be calculated similarly. Due to this flow of information from the objective function to each of the layers, the
optimization of MLPs is also referred to as backwards pass, in contrast to the forward pass of calculating ŷ = f (x).
The full backpropagation algorithm for MLPs is formulated in Algorithm 1.
Algorithm 1 Backpropagation, pseudocode taken from [26]
Require: labels y, regularizer Ω(θ), network outputs ŷ, activated and unactivated layer outputs a(k) and h(k) for
k = 1, . . . , n, activation function g, loss L
δ ← ∇ŷ L
for k = n, . . . , 1 do
δ ← ∇h(k) L = δ ⊙ g ′ (h(k) )
▷ hadamard product if g is element-wise
∇b(k) L ← δ + ∇b(k) Ω(θ)
∇W (k) L ← δh(k−1)T + ∇W (k) Ω(θ)
δ ← ∇a(k−1) L = W (k)T δ
end for
The gradients computed via backpropagation are used to update the parameters in each layer using gradient descent.
However, due to the prohibitive computational costs of evaluating the expectation in Equation (4), computing gradients
only on a subset of the training data is generally preferred and typically also results in faster convergence [26]. At
each iteration, a batch S ′ of data with size m′ ≤ m (typically m′ ≪ m) is randomly sampled from the training data to
conduct the update [10]


1 X
Θ(k) := Θ(k−1) − αk ′
∇θ L fΘ(k−1) , z .
(5)
m
′
z∈S

Here, Θ is a random variable whose realizations are neural network parameters θ. αk is the step size or learning rate on
the k-th optimization step. The learning rate is commonly decayed over the training process to help convergence [26].
4

In practice, the empirical risk is generally highly non-convex prohibiting guaranteed convergence to a global minimum [10].

7

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

The procedure using updates as in Equation (5) is known as stochastic (minibatch) gradient descent (SGD)5 [26] and
dates back to [63, 41]. Using SGD has the additional benefit of introducing random fluctuations which enable escaping
saddle points [10]. SGD in its general form is depicted in Algorithm 2, where in our context r(θ) = R̂S (fθ ). The
K
neural network parameters θ are set to be the realization of the final Θ(K) or a convex combination of Θ(k) k=1 [10].
Algorithm 2 Stochastic Gradient Descent, pseudocode from [10]
Require: Differentiable function r : Rd → R, step sizes αk ∈ (0, ∞), k = 1, . . . , K, Rd -valued random variable Θ(0)
for k = 1, . . . , K do


Let Dk be a random variable such that E D(k) | Θ(k−1) = ∇r(Θ(k−1) )
Θ(k) ← Θ(k−1) − αk D(k)
end for
Despite the stochasticity of SGD and highly non-convex loss landscapes, SGD’s convergence can be guaranteed in some
regimes [10], and it exhibits strong performance in practice [26]. Hence, SGD and its variants are the default choice
to optimize neural networks. The most prominently used variant is Adam [42], which uses momentum [59] and an
adaptive scaling of gradients to stabilize learning. Nonetheless, the initialization of θ is also important for convergence.
Biases are commonly initialized to 0 whereas weights are randomly initialized close to 0 using various strategies [26].
Finally, note that regardless of the non-convexity of the loss landscapes, local minima are not considered problematic if
the neural networks are large enough [18, 15]
In practice, the training of neural networks is an iterative process. We alternate between choosing the network
architecture a as well as further hyperparameters of the learning algorithm such as the learning rates α, and approximately
minimizing the empirical risk for this set of hyperparameters. This is generally a trial-and-error process to find a
suitable set of hyperparameters to maximize generalization performance, i.e. to minimize risk. To approximate risk,
the trained models are typically evaluated by the empirical risk on a held-out test data set, which was not seen during
training [26]. The achieved empirical risk can be decomposed into a generalization error, an optimization error, an
approximation error and the irreducible error [10]. The generalization error is the difference between empirical and
actual risk stemming from the random sampling of training data, which may not be representative of the actual data
distribution PZ . The optimization error is the result of potentially not finding a global mimimum during the learning
process. The approximation error is the difference between the minimum achievable risk over functions in Fa and over
all f ∈ M(X , Y).

3

Theoretical Foundations of Policy Gradients

Having introduced the fundamentals of deep RL, we can now discuss policy gradient algorithms in detail. In this
section, we derive their theoretical foundations. Our main focus is going to be the Policy Gradient Theorem, on which
all policy gradient algorithms build. This theorem will be discussed in Section 3.1. Furthermore, Sections 3.2 and 3.3
introduce the theoretical justifications for additional methods that are frequently used in policy gradient algorithms.
3.1

Policy Gradient Theorem

Given an MDP M = (S, A, P, γ, p0 ), consider a parameterized policy πθ , which is differentiable almost everywhere,
and the following objective function J for maximizing the expected episodic return:
 
J(θ) = ES0 ∼p0 ,πθ G0
h

i
= ES0 ∼p0 Eπθ Gt | St = S0
h
i
= ES0 ∼p0 Vπθ (S0 )
The idea of policy gradient algorithms is to maximize J(θ) over the parameters θ by performing gradient ascent[75].

Hence, we require the gradients ∇θ J(θ), however it is a priori not obvious how the right-hand side ES0 ∼p0 ,πθ G0
π
depends on θ as changes in the policy π also affect the state distribution d . The Policy Gradient Theorem [76, 52]
yields an analytic form of ∇θ J(θ) from which we can sample gradients that does not involve the derivative of dπ .
Here, we focus on the undiscounted case, i.e. γ = 1. Note that any discounted problem instance can be reduced to the
undiscounted case by letting the reward function absorb the discount factor [70].
5

Sometimes SGD refers to updates which only involve a single data point. We however follow the nowadays common terminology
of calling any sample-based gradient descent stochastic.

8

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Theorem 3.1. (Policy Gradient Theorem) For a given MDP, let πθ be differentiable w.r.t. θ and ∇θ πθ be bounded, let
Qπθ be differentiable w.r.t. θ and ∇θ Qπθ be bounded for all s ∈ S and a ∈ A. Then, there exists a constant η such that
h
i
∇θ J(θ) = η ES∼dπθ ,A∼πθ Qπθ (S, A) ∇θ ln πθ (A | S) .
(6)
Proof. We largely follow the proof by [75] albeit in a more detailed form and extended to continuous state and action
spaces. To enhance readability, we omit subscripts θ for the policy π and all gradients ∇ but both always depend on the
parameters θ.
Starting from the definition of the objective function, we explicitly
R write out the expectation over starting states, use the
relationship between value and action-value function, Vπ (s) = a∈A π(a | s) Qπ (s, a) da, and differentiate by parts.


∇J(θ) = ∇ES∼p0 Vπ (S)
Z
=∇
p0 (s) Vπ (s) ds
s∈S

Z
=∇

Z

Z
=

π(a | s) Qπ (s, a) da ds

p0 (s)

s∈S

a∈A

Z
p0 (s)

s∈S

Z


∇π(a | s) Qπ (s, a) da +

a∈A


π(a | s) ∇Qπ (s, a) da ds.

(7)

a∈A

Note that in the last step via used the Leibniz integral rule (Theorem D.10) to swap the order of integration and
differentiation prior to applying the product rule. The conditions for Leibniz are satisfied since π(· | s)Qπ (s, ·) is
integrable for any s ∈ S and its partial derivatives exist and are bounded for all s ∈ S and a ∈ A since π and Qπ are
bounded and ∇Qπ and ∇π exist and are bounded by assumption.
Now, consider the recursive formulation of the action-value function
Z Z

P (s′ , r | s, a) r + Vπ (s′ ) dr ds′ .
Qπ (s, a) =
s′ ∈S r∈R

Due to the identity r∈R P (s′ , r | s, a) dr = P (s′ | s, a) and since realized rewards r and environment transitions for a
given action no longer depend on the policy, we can reformulate the gradients of Qπ w.r.t. θ, again using the Leibniz
integral rule.
Z Z

∇Qπ (s, a) = ∇
P (s′ , r | s, a) r + Vπ (s′ ) dr ds′
R

s′ ∈S r∈R

Z

Z

=


P (s′ , r | s, a) ∇ r + Vπ (s′ ) dr ds′

s′ ∈S r∈R

Z

Z

=

P (s′ , r | s, a) ∇Vπ (s′ ) dr ds′

s′ ∈S r∈R

Z Z
=
s′ ∈S

Z
=


P (s′ , r | s, a) dr ∇Vπ (s′ ) ds′

r∈R

P (s′ | s, a) ∇Vπ (s′ ) ds′ .

(8)

s′ ∈S

Further, note that for all s ∈ S
Z
∇Vπ (s) = ∇

π(a | s) Qπ (s, a) da

a∈A

Z
=


∇π(a | s) Qπ (s, a) da +

a∈A

Z
π(a | s) ∇Qπ (s, a) da,
a∈A

9

(9)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

which is equivalent to the inner expression in Equation (7). By using (8) and (9), we can transform (7) into a recursive
form, which we are then going to unroll subsequently to yield an explicit form. In the following, we simply notation by
defining

Z


∇π(a | s) Qπ (s, a) da.

ϕ(s) :=

(10)

a∈A

Applying (10) and (8) to (7) in order and rearranging the integrals gives

Z

Z
∇J(θ) =

∇π(a | s) Qπ (s, a)da +

p0 (s)
s∈S

Z



a∈A


π(a | s) ∇Qπ (s, a) da ds

a∈A



Z
p0 (s) ϕ(s) +
π(a | s) ∇Qπ (s, a) da ds

Z
=
s∈S

a∈A



Z
=

Z

s∈S

=

s′ ∈S

a∈A



Z


P (s′ | s, a) ∇Vπ (s′ ) ds′ da ds

Z
π(a | s)

p0 (s) ϕ(s) +
Z

Z

p0 (s) ϕ(s) +

π(a | s) P (s′ | s, a) da ∇Vπ (s′ ) ds′


ds

(11)

s′ ∈S a∈A

s∈S

In the final step, we switched the order of integration using Fubini’s Theorem (Theorem D.11), which is applicable
since ∇Vπ is bounded and π(· | s)P (· | s, ·) is a probability measure on S × A such that |π(· | s)P (· | s, ·)∇Vπ | is
integrable over the product space S × A. To unroll Equation (11) across time, we introduce notation for multi-step
transition probabilities. Let ρπ (s → s′ , k) be the probability of transitioning from state s to s′ after k steps under policy
π. We have that

ρπ (s → s′ , 0) :=

and ρπ (s → s′ , 1) :=

R
a∈A



1
0

if s = s′ ,
else

π(a | s) P (s′ | s, a) da. Now, we can recursively write

′′

Z

ρπ (s → s , k + 1) =

ρπ (s → s′ , k) ρπ (s′ → s′′ , 1) ds′ .

s′ ∈S

10

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Using this notation, iteratively substituting in (8) and (9) and applying Fubini, we can unroll (11):


Z
Z Z
′
′
′
∇J(θ) =
p0 (s) ϕ(s) +
π(a | s) P (s | s, a) da ∇Vπ (s ) ds ds
s′ ∈S a∈A

s∈S



Z
=

Z

′

s′ ∈S

Z

Z
p0 (s)

s∈S

Z

Z
p0 (s)

s′ ∈S

Z

Z
p0 (s)

ϕ(s) +

Z Z

′

!



′

′

π(a | s ) ∇Qπ (s , a) da ds

ds

a∈A



′

Z

′

′

′′

′′

′′

′′

′′

!



ρπ (s → s , 1) ∇Vπ (s ) ds

′

ds

ds

s′′ ∈S

ρπ (s → s′ , 1) ϕ(s′ ) ds′

′

′

′′

′

s′′ ∈S

!



ρπ (s → s , 1) ρπ (s → s , 1) ds

+

∇Vπ (s ) ds

ds

s′ ∈S



Z

Z

ρπ (s → s′ , 1) ϕ(s′ ) ds′ +

p0 (s) ϕ(s) +
s′ ∈S

s∈S

Z

ρπ (s → s′′ , 2) ∇Vπ (s′′ ) ds′′


ds

s′′ ∈S



Z

Z

p0 (s) ρπ (s → s, 0)ϕ(s) +

=

Z

′

s′ ∈S

s∈S

=



′

ds

ρπ (s → s , 1) ϕ(s ) +

ϕ(s) +

s∈S

=



ρπ (s → s , 1) ϕ(s ) +

ϕ(s) +
s′ ∈S

=

′

ρπ (s → s , 1) ∇Vπ (s ) ds

p0 (s) ϕ(s) +
s∈S

=

′

ρπ (s → s′ , 1) ϕ(s′ ) ds′

s′ ∈S

s∈S

Z

′′

′′

Z

′′

′′′

ρπ (s → s , 2) ϕ(s ) ds +

+
s′′ ∈S

′′′

′′′



ρπ (s → s , 3) ∇Vπ (s ) ds

ds

s′′′ ∈S

..
.
Z
=

p0 (s)

ρπ (s → s′ , t) ϕ(s′ ) ds′ ds

s′ ∈S t=0

s∈S

We set ηs (s′ ) :=

Z X
T

PT

′
t=0 ρπ (s → s , t), rearrange the integrals and multiply by 1 to obtain

Z
∇θ J(θ) =

p0 (s)

ρπ (s → s′ , t) ϕ(s′ ) ds′ ds

s′ ∈S t=0

s∈S

Z

Z X
T

Z

=

p0 (s) ηs (s′ ) ϕ(s′ ) ds ds′

s′ ∈S s∈S

R
R
Z
p (s)ηs (s′′ ) ds ds′′
s′′ ∈S Rs∈S 0
R
=
p (s)ηs (s′′ ) ds ds′′
s′′ ∈S s∈S 0

Z

p0 (s) ηs (s′ ) ds ϕ(s′ ) ds′

s′ ∈S s∈S

Z

Z

=

′′

p0 (s)ηs (s ) ds ds
s′′ ∈S s∈S

Z
p0 (s)

s∈S

′′

s′ ∈S

Z
=

Z

ηs (s′′ ) ds′′ ds

s′′ ∈S

Z

R
p0 (s)ηs (s′ ) ds
R
Rs∈S
ϕ(s′ ) ds′
p (s)ηs (s′′ ) ds ds′′
s′′ ∈S s∈S 0
dπ (s′ ) ϕ(s′ ) ds′ .

s′ ∈S

In the final step, we used the identity
R
p0 (s)ηs (s′ ) ds
Rs∈S
,
d (s ) = R
p (s)ηs (s′′ ) ds ds′′
s′′ ∈S s∈S 0
π

′

11

(12)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

which can be seen as ηs (s′ ) is the accumulate sum over probabilities of reaching s′ after any number of steps for a given
starting state. Integrating over the starting state distribution and normalizing hence yields the probability of visiting
state s′ and thereby the stationary distribution dπ over states under the current policy.
Finally, we can derive the canonical form of the Policy Gradient Theorem from (12) by using the definition of ϕ(s),
setting
Z
Z
η :=

ηs (s′′ ) ds′′ ds

p0 (s)
s′′ ∈S

s∈S

and multiplying with 1:
Z

Z

∇J(θ) =

′′

p0 (s)

=η

dπ (s′ )

s′ ∈S

Z

π

′

π

′

Z

d (s )

s′ ∈S


∇π(a | s′ ) Qπ (s′ , a) da ds′
π(a | s′ )

∇π(a | s′ )
Qπ (s′ , a) da ds′
π(a | s′ )

a∈A

Z
=η

s′ ∈S

a∈A

Z
=η

dπ (s′ ) ϕ(s′ ) ds′

ηs (s ) ds ds

s′′ ∈S

s∈S

Z

Z

′′

Z

d (s )

s′ ∈S


π(a | s′ ) ∇ ln π(a | s′ ) Qπ (s′ , a) da ds′

a∈A


h
i
= η ES∼dπ EA∼π Qπ (S, A) ∇ ln π(A | S) .

The Policy Gradient Theorem provides us with an explicit form of the policy gradients from which we can sample
gradients. This allows the use of gradient-based optimization to directly optimize the policy using the methods presented
in Section 2.3. Thus, the theorem serves as the foundation for the policy gradient algorithms which we will discuss in
Section 4.
We conclude this section with some further remarks on Equation (6). First, we note that for any starting state s ∈ S, we
have that
Z X
Z
Z
Z
T
p0 (s)
ρπ (s → s′ , t) ds′ ds
η=
p0 (s)
ηs (s′ ) ds′ ds =
s∈S

= ES∼p0

s′ ∈S

" T Z
X

s′ ∈S t=0

s∈S

#
ρπ (S → s′ , t) ds′ ,

t=0s′ ∈S

which is the average episode length6 under policy π [75]. Second, the use of gradient-based methods makes it sufficient
to sample gradients which are only proportional to the actual gradients since any constant of proportionality can be
absorbed by the learning rate parameter of the optimization algorithms. Hence, η is commonly omitted [75], i.e.
h
i
∇θ J(θ) ∝ ES∼dπθ ,A∼πθ Qπθ (S, A) ∇θ ln πθ (A | S) .
(13)
We observe that all terms on the right hand side are known or can be estimated via sampling.
3.2 Value Function Estimation with Baselines
In practice, the resulting estimates of the policy gradients can become very noisy when sampling from Equation (13).
Therefore, a main practical challenge of policy gradient algorithms is to introduce measures to reduce the variance
of the gradients while keeping the bias low [77]. In this context, a well-known and widely used technique is to use a
baseline [88] when sampling an estimate of the action-value function Qπ [27]. In this section, we show that using an
appropriately chosen baseline does not bias the estimate but can greatly reduce the variance of the sampled gradients.


Let Q̂(s, a) be a sampled estimate of Qπ (s, a), assuming E Q̂(s, a) = Qπ (s, a). Then, we can construct a new
estimator Q̂b (s, a) by subtracting some baseline b : S → R, i.e. Q̂b (s, a) = Q̂(s, a) − b(s). Our only condition towards
6

Note that ρπ (st → st+1 , 1) = 0 if the episode already terminated due to reaching a terminal state on any previous step.

12

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

b is that it does not depend on the action a, though it can depend on the state s and even be a random variable [75]. Our
sampled estimate of the gradient ∇θ J(θ) becomes

ˆ θ J(θ) = ∇θ ln πθ (a | s) Q̂(s, a) − b(s) .
∇
In expectation over the policy π, this yields
h
i
h
i
ˆ θ J(θ) = Eπ ∇θ ln πθ (A | S) Q̂(S, A) − b(S)
Eπ ∇
h
i
h
i
= Eπ ∇θ ln πθ (A | S) Q̂(S, A) − Eπ ∇θ ln πθ (A | S) b(S)
using the linearity of the expectation. Now, we show that the second part is 0. Using the Leibniz integral rule, we have
that
Z
Z
h
i
π
π
ES∼d θ ,A∼πθ ∇θ ln πθ (A | S) b(S) =
d (s)
πθ (a | s) ∇θ ln πθ (a | s) b(s) da ds
s∈S

Z
=

a∈A

dπ (s) b(s)

s∈S

πθ (a | s) ∇θ ln πθ (a | s) da ds

a∈A

Z
=

Z

π

Z
πθ (a | s)

d (s) b(s)
s∈S

a∈A

Z

π

Z

d (s) b(s) ∇θ

=
s∈S

Z
=

∇θ πθ (a | s)
da ds
πθ (a | s)

πθ (a | s) da ds
a∈A

dπ (s) b(s) ∇θ 1 ds

s∈S

=0
since π(· | s) is a probability distribution over actions. Thus, subtracting an action-independent baseline b from an
action-value function estimator Q̂ does indeed not add any bias to the gradient estimate. While here we have shown this
for a baseline which only depends on the current state, this result can be extended to baselines which depend on the
current and all subsequent states [70].
Next, we analyze the effect on the variance of the gradient estimates. Here, we only provide an approximate explanation,
see [27] for a more thorough analysis which derives bounds of the true variance. We can compute the variance using
Var[X] = E[X 2 ] − E[X]2 . Due to the above, E[X]2 is independent of the baseline in our case. This yields
h
h
i
 2 i
arg min Varπ ∇θ ln πθ (A | S) Q̂(S, A) − b(S) = arg min Eπ ∇θ ln πθ (A | S) Q̂(S, A) − b(S)
b
b



2 

2 
≈ arg min Eπ ∇θ ln πθ (A | S)
· Eπ Q̂(S, A) − b(S)
,
b

where we approximated the variance by assuming independence of the two terms in the second step. Under this

2 
approximation, the variance of sampled gradients can be minimized by minimizing Eπ Q̂(S, A) − b(S) . This is a
common least squares problem resulting in the optimal choice of b(s) = Eπ [Q̂(s, A)] (see Theorem D.8). This result
indicates that an appropriately chosen baseline can potentially significantly reduce variance of the gradients. Using this
choice for the baseline, we would like to compute gradients for sampled states and actions as


∇θ ln πθ (a | s) Qπ (s, a) − EA∼πθ [Qπ (s, A)] = ∇θ ln πθ (a | s) Qπ (s, a) − Vπ (s)
= ∇θ ln πθ (a | s)Aπ (s, a).
Here, we used the relation of the value function Vπ to Qπ and the definition of the advantage function Aπ . Despite
our approximations, this choice of a baseline turns out to yield almost the lowest possible variance of the gradients
[70]. However, note that in practice the advantage function must also be estimated. Learning this estimate typically
introduces bias [43, 76].
13

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

3.3 Importance Sampling
Importance sampling is a technique to calculate expectations under one distribution given samples from another
[65, 31, 75]. Traditionally, this is only needed in off-policy RL, where we sample transitions using a behavior policy β
but want to calculate expectations over the target policy π. However, in some implementations of on-policy algorithms
the policy may be updated before all data generated by it is processed. This makes these implementations slightly
off-policy and thus importance sampling becomes relevant even for theoretically on-policy algorithms [85]. We build
our presentation of importance sampling on [75], Section 5.5.
Given a behavior policy β, we want to estimate the value function Vπ of our target policy π. Generally, we have


Vβ (s) = Eβ Gt | St = s ̸= Vπ (s).
We can calculate the probability of a trajectory (at , st+1 , at+1 , . . . , aT −1 , sT ) under any policy π as
TY
−1

π(ak | sk )P (sk+1 | sk , ak ).

k=t

Now, we can define the importance sampling ratio.
Definition 3.2. (Importance Sampling Ratio) Given a target policy π, a behavior policy β and a trajectory τ =
(at , st+1 , at+1 , . . . , sT ) generated by β, the importance sampling ratio is defined as
QT −1
QT −1
π(ak | sk )
k=t π(ak | sk )P (sk+1 | sk , ak )
ρt:T −1 := QT −1
= QTk=t
.
−1
k=t β(ak | sk )P (sk+1 | sk , ak )
k=t β(ak | sk )
Let T be the set of possible trajectories. By multiplying returns of trajectories τ ∈ T generated by the behavior policy
β with the importance sampling ratio ρ we get




Eβ ρt:T −1 Gt | St = s = Eβ ρt:T −1 G(τ ) | St = s
=

X

ρt:T −1 G(τ )

TY
−1

τ ∈T

=

=

=

β(ak | sk ) P (sk+1 | sk , ak )

k=t

TY
−1
X QT −1 π(ak | sk )
k=t
β(ak | sk ) P (sk+1 | sk , ak )
G(τ
)
QT −1
k=t β(ak | sk )
τ ∈T
k=t

X

G(τ )

TY
−1

τ ∈T

k=t

X

TY
−1

G(τ )

τ ∈T

π(ak | sk )
β(ak | sk ) P (sk+1 | sk , ak )
β(ak | sk )
π(ak | sk )P (sk+1 | sk , ak )

k=t



= Eπ Gt | St = s
= Vπ (s).
The intuition behind this importance sampling correction is that, to evaluate π, we want to weigh returns more heavily
that are more likely under π than under β and vice versa. As an extension of the derivation above, we also get the
per-decision importance sampling ratio ρ := π(a|s)
β(a|s) [75].
Using importance sampling, we can derive the following approximate policy gradients of the target policy πθ in an
off-policy setting with behavior policy β:


πθ (A | S)
∇θ J(θ) ≈ η ES∼dβ ,A∼β
Qπθ (S, A) ∇θ ln πθ (A | S) .
β(A | S)
See [19] for a proof. Note that η now is the average episode length under β.

4

Policy Gradient Algorithms

Building on Theorem 3.1, several policy gradient algorithms have been proposed, which compute sample-based
ˆ θ J(θ) of the actual policy gradients ∇θ J(θ). This is done by constructing surrogate objectives J∗ such that
estimates ∇
14

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

ˆ θ J(θ) = ∇θ J∗ (θ). Additionally, most algorithms focus on stabilizing learning by regularizing the policy [5] and
∇
ˆ θ J(θ) [77]. In this section, we derive the most prominent7 algorithms before than comparing
reducing the variance of ∇
them in the final subsection.
4.1

REINFORCE

REINFORCE (REward Increment = Non-negative Factor × Offset Reinforcement × Characteristic Eligibility) [88] is
the earliest policy gradient algorithm. While this algorithm precedes the formulation of the Policy Gradient Theorem,
REINFORCE can be seen as a straightforward application of it. By using Monte Carlo methods [75] to estimate Qπ in
PT
Equation (13), i.e. by sampling entire episodes to compute the sample returns Gt = k=0 γ k rt+k+1 , REINFORCE
samples policy gradients
ˆ θ J(θ) = Gt ∇θ ln πθ (at | st ).
∇
Using the generic policy gradient update from Equation (3) results in the gradient ascend updates
θnew = θ + αGt ∇θ ln πθ (at | st )
where α ∈ (0, 1] is the learning rate determining the step size of the gradient steps and is set as a hyperparameter. At
times, REINFORCE is extended by subtracting some baseline value from Gt to reduce variance [88]. The pseudocode
for REINFORCE is presented in Algorithm 3.
Algorithm 3 REINFORCE
Require: α ∈ (0, 1], γ ∈ [0, 1]
Initialize θ at random
for all episodes do
Generate trajectory s0 , a0 , r1 , s1 . . . , sT under policy πθ
for t = 1, . . . , T do
PT
Gt ← k=t γ k−t rk
θ ← θ + αGt ∇θ ln πθ (at | st )
end for
end for
4.2

▷ estimate expected return Qπ
▷ update policy parameters

A3C

Instead of estimating Qπ directly via sampling as in REINFORCE, we can alternatively learn such an estimate via
function approximation. Algorithms that use this approach to learn a parameterized action-value function Q̂ϕ or value
function V̂ϕ (called critic) with parameters ϕ in addition to learning the parameterized policy πθ (called actor) are
referred to as actor-critic algorithms [75]. Note that in practice the actor and the critic may also share parameters.
The most archetypical representative of this class of algorithms is Asynchronous Advantage Actor-Critic (A3C) [54].
A3C builds on two main ideas from which the algorithm’s name originates. First, as suggested by the results from
Section 3.2, A3C learns an estimate Âϕ of the advantage function indirectly by learning an estimate V̂ϕ of the value
function. Second, A3C introduces the concept of using multiple parallel actors to interact with the environment to
stabilize training. We will discuss both ideas in detail below. The algorithm samples policy gradients
X
ˆ θ J(θ) = 1
∇
Âϕ (s, a)∇θ ln πθ (a | s),
|D|
s,a∈D

where D is a batch of transitions collected by the actors. The pseudocode for A3C is presented in Algorithm 4.
In the original work [55], the advantage function is estimated via
Âϕ (st , at ) =

k−1
X


γ i rt+i + γ k V̂ϕ (st+k ) − V̂ϕ (st ).

i=0
7

As determined by their impact on subsequent research and the adoption rate by users.

15

(14)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

To understand this estimate, observe that
Aπ (st , at ) = Qπ (st , at ) − Vπ (st )
h
i
= Eπ Rt+1 + γVπ (St+1 ) | St = st , At = at −Vπ (st )
h
i
= Eπ Rt+1 + γRt+2 + γ 2 Vπ (St+2 ) | St = st , At = at −Vπ (st )
..
.
= Eπ

hk−1
X

i
γ i Rt+i + γ k Vπ (St+k ) | St = st , At = at −Vπ (st ),

i=0

for any k ∈ N, which follows from the definition of the value and action-value functions as well as their relationship.
Sampling this n-step temporal difference [75] expression and replacing Vπ with our learned V̂ϕ yields Equation (14).
Simultaneously to updating πθ , we learn V̂ϕ by minimizing the mean squared error loss
 k−1
2

1 X X i
k
γ rt+i + γ V̂ϕ (st+k ) − V̂ϕ (st )
|D|
i=0
D

over ϕ via SGD. Note that the inner expression is identical to the right hand side in Equation (14). In Equation (14), we
compute the difference between the estimated return when choosing action at in state st and the estimated return when
in state st , under policy π respectively. However, at is sampled from π such that in expectation this difference should
be 0 for the true value function Vπ . Hence, we minimize this squared difference to optimize ϕ by treating the first term,
Pk−1 i
k
i=0 γ rt+i + γ V̂ϕ (st+k ), as independent of ϕ.
The use of multiple parallel actors is justified as follows. Deep RL is notoriously unstable, which was first resolved
by off-policy algorithms using replay buffers that store and reuse sampled transitions for multiple updates [55]. As
(1)
(k)
an alternative, [54] propose using several actors πθ , . . . , πθ to decrease noise by accumulating the gradients over
multiple trajectories. These accumulated gradients are applied to a centrally maintained copy of θ, which is then
redistributed to each actor. By doing this asynchronously, each actor has a potentially unique set of parameters at any
point in time compared to the other actors. This decreases the correlation of the sampled trajectories across actors,
which can further stabilize learning.
As a final implementation detail, the policy loss function of A3C, from which the policy gradients are obtained, is
typically augmented with an entropy bonus for the policy. Thus, the policy gradients become
!

X
X k−1
1
i
k
ˆ θ J(θ) =
γ rt+i + γ V̂ϕ (st+k ) − V̂ϕ (st ) ∇θ ln πθ (at | st ) + β∇θ H(πθ (· | st )) ,
∇
|D|
i=0
D

where H is the entropy (see Definition D.4) and the entropy coefficient β is a hyperparameter. This entropy bonus,
first proposed by [89], regularizes the policy such that it does not prematurely converges to a suboptimal policy. By
rewarding entropy, the policy is encouraged to spread probability mass over actions which improves exploration [54].
4.3 TRPO
Excessively large changes in the policy can result in instabilities during the training of RL algorithms. Even small
changes in policy parameters θ can lead to significant changes in the resulting policy and its performance. Hence,
small step sizes during gradient ascent cannot fully remedy this problem and would impair the sample efficiency of
the algorithm [3]. Trust Region Policy Optimization (TRPO) [69] mitigates these issues by imposing a trust region
constraint on the Kullback-Leibler (KL) divergence (see Definition D.5) between consecutive policies. In addition,
TRPO uses an off-policy correction through importance sampling as discussed in Section 3.3 to account for the
interleaved optimization and collection of transitions.
TRPO samples gradients
X
πθ (a | s)
ˆ θ J(θ) = 1
∇
Âϕ (s, a)∇θ
|D|
πold (a | s)
s,a∈D

and postprocesses them as detailed below to solve the approximate trust region optimization problem



πθ (A | S)
max
JTRPO (θ) = ES∼dπold ,A∼πold Âϕ (S, A)
θ
πold (A | S)


π
subject to ES∼d old DKL (πold (· | S) ∥ πθ (· | S)) ≤ δ
16

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Algorithm 4 A3C
Require: n ∈ N, α ∈ (0, 1], γ ∈ [0, 1], tMAX ∈ N, TMAX ∈ N
Initialize θ and ϕ at random
for i = 1, . . . , n do
while T ≤ TMAX do
dθ ← 0, dϕ ← 0
θ(i) ← θ, ϕ(i) ← ϕ
st ∼ p0
tstart ← t
while st not terminal and t − tstart ≤ tMAX do
at ∼ πθ(i)
st+1 , rt+1 ∼ P (st , at )
t ← t + 1, T ← T + 1
end while

0
if st is terminal
R←
Vϕ(i) (st ) else
for j = t − 1, . . . , tstart do
R ← rj + γR
A = R − Vϕ(i) (sj )
dθ ← dθ + ∇θ(i) ln πθ(i) (aj | sj )A
dϕ ← dϕ + ∇ϕ(i) (R − Vϕ(i) (sj ))2
end for
update θ and ϕ using dθ and dϕ via gradient ascent / descent
end while
end for

▷ in parallel
▷ reset gradients
▷ synchronize parameters on actors
▷ sample start state
▷ sample action
▷ sample next state and reward

▷ bootstrap if necessary

▷ accumulate gradients
▷ accumulate gradients

where πold = πθold is the previous policy and θold the corresponding parameters. This optimization problem is an
approximation to an objective with convergence guarantees, which we will show in the following. We
 start
 by presenting
[69]’s main theoretical result. Consider the objective of maximizing the expected return ES0 ∼p0 ,π G0 under policy π,
which we denote as η(π) here. Let Lπ be the following local approximation of η:
Z
Z
Lπ (π̃) = η(π) +
dπ (s)
π̃(a | s)Aπ (s, a) da ds,
s∈S

a∈A

with Lπθ (πθ ) = η(πθ ) and ∇θ Lπθ0 (πθ )|θ=θ0 = ∇θ η(πθ )|θ=θ0 [38]. Based on the total variation divergence DT V (see
Definition D.6), we define
DTmax
V (π, π̃) := max DT V (π(· | s)∥π̃(· | s)).
s∈S

Then, we have [69]:
Theorem 4.1. Let α = DTmax
V (πold , πnew ), then
η(πnew ) ≥ Lπold (πnew ) −

4εγ
α2 ,
(1 − γ)2

(15)

where ε = maxs∈S,a∈A |Aπ (s, a)|.
See the appendix in [69] for a proof. By using the relationship between total variation divergence and KL divergence
4εγ
max
DT V (π∥π̃)2 ≤ DKL (π∥π̃) [58] and setting DKL
(π, π̃) := maxs∈S DKL (π(· | s)∥π̃(· | s)) and C = (1−γ)
2 , we
derive the following lower bound for the objective from Equation (15):
max
η(πnew ) ≥ Lπold (πnew ) − CDKL
(πold , πnew )

(16)

Iteratively maximizing the right-hand side yields a sequence of policies πi , πi+1 , πi+2 , . . . with the monotonic improvement guarantee η(πi ) ≤ η(πi+1 ) ≤ η(πi+2 ) ≤ . . .. This is because we have equality in (16) for πnew = πold and
hence

 

max
max
η(πi+1 ) − η(πi ) ≥ Lπi (πi+1 ) − CDKL
(πi , πi+1 ) − Lπi (πi ) − CDKL
(πi , πi ) ,
which is non-negative as we maximize over π each iteration. Thus, we could construct a Minorization-Maximizationtype algorithm [37] which maximizes the right-hand side of Inequality (16) at each iteration and is thereby guaranteed
to converge to an optimum as the objective is bounded.
17

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Such an algorithm would be impractical as it requires evaluating the advantage function at every point in the state-action
product space S × A and the KL penalty at every point in the state space S. Hence, [69] apply several approximations
to the objective stemming from Inequality (16). We replace the KL penalty, which would yield restrictively small step
sizes given by C, by a trust region constraint:
max
θ

Lπold (πθ )

max
subject to DKL
(πold , πθ ) ≤ δ.
max
To avoid computing DKL
, we use the average KL divergence


πold
D̄KL
(π∥π̃) := ES∼dπold DKL (π(· | S)∥π̃(· | S))

between policies as heuristic constraint, which we can sample. Further, we rewrite the surrogate objective maxθ Lπold (πθ )
as an expectation over the old policy πold via importance sampling. Note that η(πold ) is a constant w.r.t θ:


Z
Z
πold
arg max Lπold (πθ ) = arg max η(πold ) +
d (s)
πθ (a | s)Aπold (s, a) da ds
θ

θ
s∈S

Z
= arg max

dπold (s)

a∈A

Z
πθ (a | s)Aπold (s, a) da ds

θ
s∈S

a∈A

πold (a | s)
πθ (a | s)Aπold (s, a) da ds
πold (a | s)
θ
s∈S
a∈A


πθ (A | S)
π
= arg max ES∼d old ,A∼πold
Aπ (S, A) .
πold (A | S) old
θ
Z

= arg max

d

πold

Z

(s)

Using these modifications, we are now left with solving the trust region problem


πθ (A | S)
max ES∼dπold ,A∼πold
Aπold (S, A)
θ
πold (A | S)
h
i
subject to ES∼dπold DKL (πold (· | S)∥πθ (· | S)) ≤ δ.

(17)

To approximately solve this constrained problem, [69] use backtracking line search, where the search direction is computed hby Taylor-expandingi (see Theorem D.12) the objective function and the constraint. Let g =
∇θ ES∼dπold ,A∼πold ππoldθ (A|S)
(A|S) Aπold (S, A) . Approximating Lπold (πθ ) to first order around θold yields
Lπold (πθ ) ≈ g T (θ − θold ),
where we again ignored the constant η(πold ). The quadratic approximation of the constraint at θold is
1
(θ − θold )T H(θ − θold ),
2
where H is the Fisher information matrix, which is estimated via
1 X ∂2
Ĥi,j =
DKL (πold (· | s)∥π(·∥s)),
|D|
∂θi ∂θj
πold
D̄KL
(π∥π̃) ≈

s∈D

albeit the full matrix is not required. We solve the resulting approximate optimization problem analytically using
Lagrangian duality methods [12] leading to
s
2δ
θnew = θold +
Ĥ −1 g.
T
g Ĥ −1 g
However, due to the Taylor approximations, this solution may not satisfy the original trust region constraint or may not
improve the surrogate objective of Problem (17). Therefore, TRPO employs backtracking line search along the search
direction H −1 g with search parameter β ∈ (0, 1):
s
2δ
θnew = θold + β m
H −1 g.
T
g H −1 g
18

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Algorithm 5 TRPO
Require: δ ∈ R, b ∈ (0, 1), K ∈ N, α ∈ (0, 1], U ∈ N, T ∈ N
Initialize θ and ϕ at random
t←0
while t ≤ T do
for i = 1, . . . , U do
a ∼ πθ
β(a | s) ← πθ (a | s)
s, r ∼ P (s, a)
t←t+1
Store (a, s, r, β(a | s)) in D
end for
for all epochs do
Compute returns R and advantages A
P
πθ (a|s)
1
g ← |D|
D ∇θ β(a|s) A

▷ sample action
▷ sample next state and reward

Compute Ĥ as the Hessian of the sample average KL-divergence
Compute d ≈ Ĥ −1 g via conjugate gradient algorithm
m←0
repeat
q
d
θ ← θold + bm dT2δ
Ĥd
m←m+1
until (sample
P loss improves and2 KL constraint satisfied) or m > K
1
dϕ ← |D|
D ∇ϕ (R − Vϕ (s))
Update ϕ using dϕ via gradient descent
end for
end while
We choose the exponent m as the smallest non-negative integer such that the trust region constraint is satisfied and
the surrogate objective improves. We circumvent the computationally expensive matrix inversion of H for the search
direction d ≈ H −1 g by computing d via the conjugate gradient algorithm [30]. To further reduce the computational
costs, the Fisher-vector products in this process can also be only calculated on a subset of the dataset D of sampled
transitions.
[69] do not specify an advantage estimator to be used in TRPO. The algorithm is commonly used with either the
estimator used by A3C or the one which we present in the next subsection. TRPO is typically used with multiple parallel
actors as A3C. The pseudocode for TRPO is presented in Algorithm 5. We remark that while being a policy-based
algorithm, TRPO does not strictly adhere to Definition 2.2 as it solves a constrained optimization problem via line
search. Yet, it does compute gradients of its objective function w.r.t. the policy parameters and therefore we treat it as a
policy gradient algorithm.
4.4 PPO
Given the complexity of TRPO, Proximal Policy Optimization (PPO) [71] is designed to enforce comparable constraints
on the divergence between consecutive policies during the learning process while simplifying the algorithm to not
require second-order methods. This is achieved by heuristically flattening the gradients outside of an approximate trust
region around the old policy. In addition, PPO uses a novel method to learn an estimate of the advantage function.
Let rθ (a | s) = ππoldθ (a|s)
(a|s) . Then, PPO uses the following estimate of the policy gradients:



1 X
ˆ
∇θ J(θ) =
Âϕ (s, a)∇θ min rθ (a | s), clip rθ (a | s), 1 − ε, 1 + ε .
|D|
s,a∈D

Here, the clip-function clip : R × R × R → R is defined by

a if x < a,
clip(x, a, b) = x if a ≤ x ≤ b,

b if b < x.
and is applied element-wise to rθ . ε is a hyperparameter.
19

(18)

JPPO (θ)

JPPO (θ)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

0

1
rθ

1+ε

0

1+ε

1
rθ

(b) A < 0

(a) A > 0

Figure 3: Illustration of the conservative clipping of PPO’s objective function, which is shown as a function of the ratio
rθ for a single transition depending on whether the advantages are positive (a) or negative (b). Replicated from [71].

This clipped objective conservatively removes the incentive for moving the new policy to far away from the old one.
Intuitively, this can be seen as follows. We distinguish two cases: positive and negative estimated advantages Â(s, a), i.e.
whether action a is good or bad. If Â(s, a) > 0, the surrogate objective JPPO (θ) increases when a becomes more likely.
Similarly, if Â(s, a) < 0, JPPO (θ) increases when a becomes less likely. Hence, we want to adjust the policy parameters
θ accordingly. However, by clipping the policy ratio rθ , this positive effect on the objective function disappears once
we move outside the clip range. This clipping process is conservative as we only clip if the objective function would
improve. If the policy is changed in the opposite direction such that JPPO (θ) decreases, rθ is not clipped due to taking
the minimum in Equation (18). Figure 3 illustrates this explanation. The pseudocode for PPO is presented in Algorithm
6.
Algorithm 6 PPO
Require: ε ∈ R, α ∈ (0, 1], γ ∈ [0, 1], λ ∈ [0, 1], U ∈ N, T ∈ N
Initialize θ and ϕ at random
t←0
while t ≤ T do
for i = 1, . . . , U do
a ∼ πθ
β(a | s) ← πθ (a | s)
s, r ∼ P (s, a)
t←t+1
Store (a, s, r, β(a | s)) in D
end for
for all epochs do
R, A ← computeGAE(v, r, λ, γ)

P
π(a|s)
π(a|s)
1
dθ ← ∇θ |D|
D min β(a|s) , clip( β(a|s) , 1 − ε, 1 + ε) A
P
1
2
dϕ ← ∇ϕ |D|
D (R − Vϕ (s))
update θ and ϕ using dθ and dϕ via gradient ascent / descent
end for
end while

▷ sample action
▷ sample next state and reward

▷ Compute returns and advantages

To compute the estimate Âϕ of the advantage function, PPO uses generalized advantage estimation (GAE) [70] to
further reduce the variance of gradients. GAE computes the estimated advantage as
Âϕ (st , at ) =

T
−1
X

(γλ)i−t δi ,

i=t

20

(19)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

where δi = ri + γ V̂ϕ (si+1 ) − V̂ϕ (si ). The value function estimate V̂ϕ is learned by minimizing
2

1 X
Âϕ (s, a) + V̂ϕ (s) − V̂ϕ (s) ,
|D|
D

where the first term is treated as independent of ϕ. GAE relates to the idea of eligibility traces [74] to use both the
sampled rewards and the current value function estimate on every time step. By computing such an exponentially
weighted estimator, GAE reduces the variance of the policy gradients at the cost of introducing a slight bias to the
value function estimate [70]. The hyperparameters γ and λ both adjust this bias-variance tradeoff. γ does so by scaling
the value function estimate V̂ whereas λ controls the dependence on delayed rewards. Note that GAE is a strict
generalization of A3C’s advantage estimate as Equation (19) reduces to Equation (14) when λ = 1. The pseudocode
for GAE is presented in Algorithm 7.
Algorithm 7 GAE
Require: γ ∈ [0, 1], λ ∈ [0, 1]
t+n
Require: rewards (rk )k=t
, values (vk )t+n+1
t=k
At , . . . , At+n ← 0
x←0
for i = t + n, . . . , t do
if transition was terminal then
ω←1
else
ω←0
end if
δ ← ri + γ · vi+1 · (1 − ω) − vi
x ← δ + γ · λ · (1 − ω) · x
Ai ← x
end for
for i = t, . . . , t + n do
Ri ← Ai + vi
end for
Beyond these main innovations, PPO uses several implementational details to improve learning. PPO conducts multiple
update epochs for each batch of data such that several gradient descent steps are based on the same transitions to
increase sample efficiency and speed up learning. Moreover, PPO commonly augments its surrogate objective with an
entropy bonus H(πθ (· | s)) and uses multiple actors similarly to A3C. Lastly, we note that further algorithms have
been proposed as modifications of PPO, e.g. Phasic Policy Gradients [16] and Robust Policy Optimization [61], which
we will not discuss further as they only modify minor details.
4.5 V-MPO
In the previous algorithms, we learn a policy from the control perspective by selecting actions to maximize expected
rewards. In this subsection, we consider an alternative formulation of RL problems, which casts them as probabilistic
inference problems of estimating posterior policies that are consistent with a desired outcome [1]. This problem is
then solved via Expectation Maximization (EM) [20]. This procedure was first proposed in the off-policy algorithm
Maximum a-posteriori Policy Optimization (MPO) [2, 1]. Here, we discuss its on-policy variant V-MPO [73], where
the "V" in the name refers to learning the value function Vπ instead of Qπ as in MPO.
The main idea of V-MPO is to find a maximum a posteriori estimate of the policy by sequentially finding a tight lower
bound on the posterior and then maximizing this lower bound. This problem can be transformed into the objective
function
JV-MPO (θ, η, ν) = Lπ (θ) + Lη (η) + Lν (θ, ν),
where Lπ is the policy loss


Â (s,a)
exp ϕ η
X

 ln πθ (a | s),
Lπ (θ) = −
(20)
P
Âϕ (s′ ,a′ )
exp
′
′
a,s∈D̃
a ,s ∈D̃
η
Lη is the temperature loss
"


#
1 X
Âϕ (s, a)
Lη (η) = ηεη + η ln
exp
η
|D̃|
a,s∈D̃

21

(21)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

and Lν is the trust-region loss
 

hh
ii
 

1 X
Lν (θ, ν) =
ν εν − sg DKL (πold (· | s) ∥ πθ (· | s))
+ sg ν DKL πold (· | s) ∥ πθ (· | s) . (22)
|D|
s∈D

Here, sg[[·]] is a stop-gradient operator, meaning its arguments are treated as constants when computing gradients, η is a
learnable temperature parameter, ν is a learnable KL-penalty parameter, εν and εη are hyperparameters, D is a batch
of transitions and D̃ ⊂ D is the half of these transitions with the largest advantages. We will provide a sketch of how
to derive this objective function in the following. We refer the interested reader to Appendix Appendix C for a more
detailed derivation.
Let pθ (s, a) = πθ (a | s)dπθ (s) denote the joint state-action distribution under policy πθ conditional on the parameters
θ. Let I be a binary random variable whether the updated policy πθ is an improvement over the old policy πold , i.e.
I = 1 if it is an improvement. We assume the conditional probability of πθ being an improvement given a state s and
an action a is proportional to the following expression
 A (s, a) 
πold
pθ (I = 1 | s, a) ∝ exp
.
(23)
η
Given the desired outcome I = 1, we seek the posterior distribution conditioned on this event. Specifically, we seek the
maximum a posteriori estimate


θ∗ = arg max pθ (I = 1)ρ(θ)
θ


(24)
= arg max ln pθ (I = 1) + ln ρ(θ) ,
θ

where ρ is some prior distribution. ln pθ (I = 1) can be rewritten as



pθ (I = 1, S, A)
ln pθ (I = 1) = ES,A∼ψ ln
+ DKL ψ ∥ pθ (·, · | I = 1)
ψ(S, A)

(25)

for some distribution ψ over S × A. Observe that, since the KL divergence is non-negative, the first term is a lower
bound for ln pθ (I = 1). Akin to EM algorithms, V-MPO now iterates by choosing the variational distribution ψ in
the expectation (E) step to minimize the KL divergence in Equation (25) to make the lower bound as tight as possible.
In the maximization (M) step, we maximize this lower bound and the prior ln ρ(θ) to obtain a new estimate of θ∗ via
Equation (24).
First, we consider the E-step. Under the proportionality assumption (23), we turn the problem of finding a variational
distribution ψ to minimize DKL (ψ ∥ pθold (·, · | I = 1)) into an optimization problem over the temperature η. This is
formulated as a constrained problem subject to a bound on the KL divergence between ψ and the previous state-action
distribution pθold while ensuring that ψ is a state-action distribution. To enable optimizing η via gradient descent, we
transform this constrained problem into an unconstrained problem via Lagrangian relaxation, which emits both the
form of the variational distribution
ψ(s, a) = R

R
s∈S

pθold (s, a) pθold (I = 1 | s, a)
p (s, a) pθold (I = 1 | s, a) da ds
a∈A θold

and the temperature loss (21)
Z
Lη (η) = ηεη + η ln

Z
exp

A

πold (s, a)

η




da ds .

s∈S a∈A

[73] find that using only the highest 50 % of advantages per batch when sampling these expressions, i.e. replacing D
with D̃, substantially improves the algorithm. The advantage function Aπ is estimated by Âϕ , which is learned as in
A3C.
Then, in the M-Step we solve the maximum a posterior estimation problem (24) over the policy parameters θ for the
constructed variational distribution ψ(s, a) and the thereby implied lower bound. This lower bound, i.e. the first term in
Equation (25), becomes the weighted maximum likelihood policy loss (20)
Z Z
Lπ (θ) = −
ψ(s, a) ln πθ (a | s) da ds
s∈S a∈A

22

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

after dropping terms independent of θ. This loss is computed over the same reduced batch D̃ as the temperature loss,
effectively assigning out-of-sample transitions a weight of zero. Simultaneously, we want to maximize the prior ρ(θ)
according to the maximization problem (24). V-MPO follows TRPO and PPO to choose a prior such that the new policy
is kept close to the previous one, i.e.


ρ(θ) = −νES∼dπold DKL πold (· | S)∥πθ (· | S) ,
with learnable parameter ν. However, optimizing the resulting sample-based maximum likelihood objective directly
tends to result in overfitting. Hence, a sequence of transformations is applied. First, the prior is transformed into a
hard constraint on the KL divergence when optimizing the policy loss. To employ gradient-based optimization, we
use Lagrangian relaxation to transform this constrained optimization problem back into an unconstrained problem
and use a coordinate-descent strategy to simultaneously optimize for θ and ν. This can equivalently be written via the
stop-gradient operator yielding the trust-region loss (22).
Algorithm 8 V-MPO
Require: η ∈ R, ν ∈ R, εη ∈ R, εν ∈ R, U ∈ N, T ∈ N
Initialize θ and ϕ at random
t←0
while t ≤ T do
for i = 1, . . . , U do
a ∼ πθ
β(a | s) ← πθ (a | s)
s, r ∼ P (s, a)
t←t+1
Store (a, s, r, β(a | s)) in D
end for
for all epochs do
Compute returns R and advantages A
Compute D̃P
1
Lν ← |D|
D ν(εν − sg(DKL (πold ∥πθ ))) + sg(ν)DKL (πold ∥πθ )
P
1
Lπ ← − |D̃| D̃ ln πθ (a | s)ψ(s, a)
P
1
A
Lη ← ηεη + η ln( |D̃|
D̃ exp η )
∂
∂
Lη , dν ← ∂ν
Lν
dθ ← ∇θ (Lπ + Lν ), dη ← ∂η
P
1
2
dϕ ← |D| D ∇ϕ (R − Vϕ (s))

▷ sample action
▷ sample next state and reward

▷ KL loss
▷ Policy loss
▷ Temperature loss
▷ Compute gradients

update θ, η, ν and ϕ using dθ, dη, dν and dϕ via gradient ascent / descent
end for
end while
The learnable parameters η and ν are Lagrangian multipliers and hence must be positive. We enforce this by projecting
the computed values to small positive values ηmin and νmin respectively if necessary. The pseudocode for V-MPO is
depicted in Algorithm 8. As implementational details, V-MPO typically uses decoupled KL constraints for the mean
and covariance of the policy in continuous action spaces following [2]. This enables better exploration without moving
the policy mean as well as fast learning by rapidly changing the mean without resulting in a collapse of the policy due
to vanishing standard deviations. In addition, V-MPO can be used with an off-policy correction via an importance
sampling ratio similarly to TRPO and PPO and uses multiple actors following A3C.
4.6 Comparing Design Choices in Policy Gradient Algorithms
Having outlined the main on-policy policy gradient algorithms, we want to shortly compare them to characterize the
main design choices in constructing such algorithms.
ˆ θ J(θ) of the policy gradients. We
The predominant differences across policy gradient algorithms lie in the estimators ∇
summarize these estimates in Table 18 . The algorithms can be distinguished along several dimensions with respects
to the gradients. First, they use different variance reduction techniques, which are especially reflected in how Qπ in
the policy gradient formula (13) is estimated. Second, various policy regularization strategies are used. Third, the
algorithms employ further lower-level details to stabilize learning. We will discuss each of these dimensions in the
following.
8

For V-MPO, we focus on the policy loss, thus ignoring the gradient of the KL loss Lν w.r.t. the policy parameters θ here.

23

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Algorithm

Gradient estimator

REINFORCE

G∇θ ln πθ (a | s)
P
1
D Â∇θ ln πθ (a | s)
|D|
P
πθ (a|s)
1
D Â∇θ πold (a|s)
|D|



P
πθ (a|s)
πθ (a|s)
1
D Â∇θ min πold (a|s) , clip πold (a|s) , 1 − ε, 1 + ε
|D|

A3C
TRPO
PPO
V-MPO

1
P

D̃ exp

Â
η



Â
D̃ exp η

P



∇θ ln πθ (a | s)

Table 1: Policy gradient estimates used by various policy gradient algorithms.

Reducing variance is important to stabilize learning and speed up convergence [77]. However, while high variance
means algorithms require more samples to converge, bias in the estimates is not resolvable even with infinite samples
[70]. All contemporary policy gradient algorithms, i.e. all presented algorithms except REINFORCE, make use of
baselines to reduce variance as discussed in Section 3.2 when approximating the unknown Qπ . While REINFORCE
samples returns as an unbiased but high-variance estimate [75], the other algorithms learn a value function V̂ to estimate
the advantage function Â. Notably, this reduces variance at the cost of introducing bias [43, 76, 70]. Further differences
arise from how advantages are estimated, albeit these strategies can be easily transferred between algorithms. PPO
uses GAE to estimate advantages, which generalizes the n-step temporal difference estimates used in A3C, TRPO
and V-MPO. In addition, V-MPO scales the advantages via the learned temperature η and only uses the top 50 % of
advantages per batch.
To stabilize learning beyond variance reduction, several regularization techniques are proposed by TRPO, PPO and
V-MPO to limit the change in policies across iterations. TRPO imposes a constraint on the KL divergence between
the newly learned and the previous policy. Thus, the policy gradients are not directly applied to update the policy
parameters θ but instead they are postprocessed to yield an approximate solution to this constrained optimization
problem. This comes at the cost of algorithmic complexity however. Whereas the other algorithms directly compute the
estimated policy gradients via automatic differentiation [86, 53], TRPO requires the estimation of a hessian and the
application of the conjugate gradient algorithm followed by a line search. PPO avoids such complexity by introducing a
heuristic, which bounds the probability ratio ππoldθ (a|s)
(a|s) , into its objective function. By conservatively clipping this ratio,
large policy changes induced by overfitting the advantage function are prevented. This also enables PPO to conduct
multiple updates on the same data to accelerate learning [71]. V-MPO too limits the KL divergence across policies.
The prior distribution is selected such that V-MPO arrives at a similar optimization problem with a penalty on the KL
divergence as TRPO. Following TRPO, V-MPO transforms this into a constrained optimization problem, albeit now
with the goal of automatically tuning the penalty parameter by applying coordinate-descent to the Lagrangian relaxation
of this constrained optimization problem.
Lastly, we point out that different lower-level details are employed by the discussed algorithm. Except for REINFORCE,
all algorithms use several actors, which are potentially updated asynchronously, and average gradients over batches of
transitions to further reduce their variance. Here, V-MPO slightly diverges from the other algorithms as it computes a
weighted average based on the advantages of the transitions, i.e. with weights
exp
P

 Aπ

θold

(s,a)



η

a,s∈D̃ exp

 Aπ

θold

(s,a)

.

η

A3C and PPO commonly use an entropy bonus to prevent premature convergence to a suboptimal policy by incentivizing
a higher standard deviation of the Gaussian output by the policy. We observe that V-MPO does not use an entropy bonus
but achieves a comparable effect in continuous action spaces by constraining the policy mean and standard deviation
separately. Lastly, TRPO and PPO include an importance sampling correction to compensate for the slight off-policy
nature of the algorithms induced by using multiple asynchronous workers. This is also mentioned as an option for
V-MPO [73].
24

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

5

Convergence Results

In this section, we discuss convergence results for policy gradient algorithms from literature. First, we present an
overview of different convergence proofs in Section 5.1. Then, we thoroughly present one selected result in Section 5.2.
5.1 Literature Overview
Several convergence results have been proposed for policy gradient algorithms. They differ along various dimensions:
the specific algorithms covered, the shown strength of convergence, the problem settings and the employed proving
techniques. The following overview of convergence results is not intended to be complete but rather shall showcase
these differences.
As previously discussed, REINFORCE uses an unbiased estimator of the policy gradients, which in expectation
therefore point in the direction of the true gradients. Hence, under common stochastic approximation assumptions
towards the step sizes, REINFORCE can be shown to converge to a locally optimal policy [75]. In the general form
of policy gradient algorithms, agnostic to the specific estimator of Qπ , showing convergence is more complex as the
estimated gradients are typically biased when using a learned value function [43, 76]. [4] and [11] consider the simplest
case where state and action spaces S and A are finite and no function approximation is used, i.e. the policy uses a
tabular parameterization with one parameter for each state-action combination. Using that the exact policy gradients can
be calculated in this case, both studies show the global convergence to an optimal policy with a linear convergence rate.
[76] and [90] generalize these results to settings with function approximation, albeit under impractical conditions on the
approximators, which are required to be linear in their inputs. The extension to function approximation comes at the
cost of only being able to proof local convergence using stochastic approximation and the Supermartingale Convergence
Theorem [64].
Finally, some convergence results exist for the specific algorithms such as TRPO and PPO. TRPO is based on Theorem
4.1, which comes with monotonic improvement guarantees. However, TRPO is only an approximation to the algorithm
stemming from Theorem 4.1, so that no such guarantee holds for TRPO in practice. We further remark that PPO is
similarly intended as an heuristic of this theoretical algorithm [71]. Nonetheless, efforts have been made to proof the
convergence of these practical algorithms. [51] show that a slightly modified version of PPO converges to a globally
optimal policy at a sublinear rate under specific assumptions. In particular, they require an overparameterized neural
network as the function approximator such that they can use infinite-dimensional mirror-descent [8] to proof the
convergence. [34] provide a proof using two time-scale stochastic approximation [39] that PPO converges to a locally
optimal policy under more realistic assumptions akin to typical learning scenarios.
5.2 Mirror Learning
In this subsection, we focus on the convergence proof provided by [46]. While primarily of theoretical interest, we
choose to discuss this particular result as it is agnostic to the selected algorithm and parameterization and can hence by
applied to a range of policy gradient algorithms. [46] introduce a framework called mirror learning, which comes with
global convergence guarantees for all policy gradient algorithms that adhere to a specific form. In the following, we
follow [46] in deriving their results. We start by giving some definitions, based on which we then present the general
form of mirror learning updates. We show that the discussed algorithms largely adhere to this form. Finally, we proof
that this implies the convergence to an optimal policy.
5.2.1 Fundamentals of Mirror Learning
From here onwards, we do not explicitly write down the policy parameters, i.e. we omit the subscript θ when describing
a policy π. [46] define the drift D and the neighborhood operator N as follows.
Definition 5.1. (Drift) A drift functional
D : Π × S → {Dπ (· | s) : ∆(A) → R}
is a map which satisfies the following conditions for all s ∈ S and π, π̄ ∈ Π:
1. Dπ (π̄ | s) ≥ Dπ (π | s) = 0,

(non-negativity)

2. Dπ (π̄ | s) has zero gradient with respects to π̄(· | s) at π̄(· | s) = π(· | s), more precisely all its Gâteaux
derivatives9 are zero, (zero gradient)

where we used Dπ π̄(· | s) | s := Dπ (π̄ | s).
For any state distribution νππ̄ ∈ ∆(S), that can depend on both π̄ and π, the drift from π̄ to π is given by


Dνπ (π̄) := Es∼νππ̄ Dπ (π̄ | s) .
9

See Definition D.16.

25

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

We require νππ̄ to be such that this expectation is continuous in π̄ and π. We call a drift trivial if Dνπ (π̄) = 0 for all
π, π̄ ∈ Π.
Definition 5.2. (Neighborhood Operator) A mapping
N : Π → P(Π)
is a neighborhood operator if it satisfies the following conditions:
1. N is continuous,

(continuity)

2. N (π) is compact for all π ∈ Π,

(compactness)

3. There exists a metric χ : Π × Π → R such that χ(π, π̄) ≤ ζ implies π̄ ∈ N (π) for all π, π̄ ∈ Π given some
ζ ∈ R+ . (closed ball)
We call N (·) = Π the trivial neighborhood operator.
With these definitions, we can define the mirror learning update rule.
Definition 5.3. (Mirror Learning Update) Let πold be the previous policy and dπold the state distribution under πold .
Further, let
h
i

 ν π̄
Mπ̄D Vπ (s) := EA∼π̄ Qπ (s, A) − ππ Dπ (π̄ | s)
d
be the mirror learning operator. Then, the mirror learning update chooses the new policy πnew as
h
i 
πnew ∈ arg max ES∼dπold Mπ̄D Vπold (S) .
(26)
π̄∈N (πold )

Under the light of this mirror learning update, the drift D from one policy to the next induces some penalty on the
objective while the neighborhood operator puts a hard constraint on the divergence of subsequent policies.
5.2.2 Policy Gradient Algorithms as Instances of Mirror Learning
Before proving the convergence of mirror learning to an optimal policy, we first show that the discussed policy gradient
algorithms can partly be seen as instances of mirror learning, i.e. use updates of the form



 νππold
πnew ∈ arg max ES∼dπold EA∼π Qπold (S, A) − πold Dπold (π | S) .
d
π∈N (πold )
A3C
A3C is a direct application of the Policy Gradient Theorem, albeit with a learned advantage function. Thus, at each
iteration it approximately solves the optimization problem
h
i
h

i
πnew ∈ arg max ES∼dπold ,A∼π Aπold (S, A) = arg max ES∼dπold EA∼π Qπold (S, A) .
π∈Π

π∈Π

This is the most trivial instantiation of mirror learning by using the trivial drift D(· | ·) = 0 and the trivial neighborhood
operator N (·) = Π. The same argumentation also applies to REINFORCE. Note that in practice however, we maximize
the expectation over πold rather than π. For this reason, these are not exact instances of mirror learning.
TRPO
TRPO’s constrained optimization problems



π(A | S)
πnew ∈ arg max E
Aπ (S, A)
old
πold (A | S) old
π∈Π


subject to ES∼dπold DKL (πold (· | S)∥π(· | S)) ≤ δ
S∼dπold ,A∼π

can be rewritten as

h

i
πnew ∈ arg max ES∼dπold EA∼π Qπold (S, A)
π∈NTRPO (πold )

with the average-KL ball as the neighborhood operator, i.e.



NTRPO (πold ) = π | ES∼dπold DKL (πold (· | S)∥π(· | S)) ≤ δ .
26

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Here, we used that

EA∼πold


Z
π(A | s)
π(a | s)
Aπold (s, A) =
Aπ (s, a)da
πold (a | s)
πold (A | s)
πold (a | s) old
a∈A
h
i
= EA∼π Aπold (s, A)

and that maximizing over the action-value function is identical to maximizing over the advantage function following
the discussion in Section 3.2. Thus, TRPO is a mirror learning instance with the trivial drift D(· | ·) = 0.
PPO
Each iteration, PPO searches for





πnew ∈ arg max Eπold min rπ (A | S), clip rπ (A | S), 1 − ε, 1 + ε Aπold (S, A) ,
π∈Π

where we write rπ (a | s) for ππ(a|s)
. We can rewrite the expectation over actions by adding zero as
old (a|s)




EA∼πold min rπ (A | s)Aπold (s, A), clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A)



= EA∼πold rπ (A | s)Aπold (s, A) − EA∼πold rπ (A | s)Aπold (s, A)



− min rπ (A | s)Aπold (s, A), clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A) .


Using the same technique as before, we can write the first expectation equivalently as EA∼π Aπold (s, A) . We now
focus on the second expectation. We replace the min operator with a max and push the first term inside the max to
obtain




EA∼πold rπ (A | s)Aπold (s, A) − min rπ (A | s)Aπold (s, A), clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A)




= EA∼πold rπ (A | s)Aπold (s, A) + max −rπ (A | s)Aπold (s, A), −clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A)


= EA∼πold max rπ (A | s)Aπold (s, A) − rπ (A | s)Aπold (s, A),


rπ (A | s)Aπold (s, A) − clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A)

 


= EA∼πold max 0, rπ (A | s) − clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A) .

This final expression is non-negative. Moreover, it is zero for π sufficiently close to πold , i.e. such that for all actions
a ∈ A we have rπ (a | s) = ππ(a|s)
∈ [1 − ε, 1 + ε], because then the clip-function reduces to the identity function
old (a|s)
w.r.t. its first argument. Thus, the derivatives of this expression must also be zero at π(· | s) = πold (· | s). These
properties are the exact conditions for a mapping to be considered a drift in the sense of Definition 5.1. With this
preparation, we can now write the PPO update as


h
i
πnew ∈ arg max ES∼dπold EA∼π Aπold (S, A) − Dπold (π | S) ,
π∈Π

where Dπold is a drift given by

 


Dπold (π | s) = EA∼πold max 0, rπ (A | s) − clip rπ (A | s), 1 − ε, 1 + ε Aπold (s, A) .
This is an instance of the mirror learning update with the trivial neighborhood operator N (·) = Π and νππold = dπold .
27

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

5.2.3 Convergence Proof
Now, we present the main theoretical result of [46].
Theorem 5.4. Let Dν be a drift, N a neighborhood operator and dπ the sampling distribution,
  all continuous in π. Let
the objective, i.e. the expected returns under a policy π, be written as J(π) = ES0 ∼p0 ,π G0 . Let π0 ∈ Π be the initial
∞
policy and the sequence of policies πn n=0 be obtained through the mirror learning update rule (26) under Dν , N
and dπ . Then,
1.

(Strict monotonic improvement)
J(πn+1 ) ≥ J(πn ) + ES∼p0

 πn+1

νπn (S)
D
(π
|
S)
π
n+1
n
dπn (S)

∀n ∈ N0 .

2. (Value function optimality)
lim Vπn = V ∗ .

n→∞

3. (Maximum attainable return)
lim J(πn ) = max J(π).

n→∞

π∈Π

4. (Policy optimality)
lim πn = π ∗ .

n→∞

Proof. We structure the proof by [46] in five steps. In step 1, we start by showing that mirror learning updates lead to
improvements under the mirror learning operator MπDn Vπn−1 , which implies improvements in the value function Vπn .
∞
In step 2, we prove that the sequence of value functions Vπn n=0 converges to some limit. In step 3, we show the
∞
existence of limit points of the sequence of policies πn n=0 , which are fixed points of the mirror learning update (26).
In step 4, we prove that these limit points are also fixed points of Generalized Policy Iteration (GPI) [75], from which
we conclude that these limit points are optimal policies in step 5. For simplicity, we proof Theorem 5.4 for discrete
state and actions spaces. However, the results are straightforward to extended to the continuous cases (see the appendix
in [46] for details).
Step 1
We start by showing by contradiction that for all n ∈ N0 and for all s ∈ S:
 πn+1



MD Vπn (s) ≥ MπDn Vπn (s).

(27)

Suppose there exists s0 ∈ S, which violates (27). We define a policy π̂ with

πn+1 (· | s) if s ̸= s0 ,
π̂(· | s) =
πn (· | s)
if s = s0 .
This way, we guarantee π̂ ∈ N (πn ) because πn+1 ∈ N (πn ) is forced by the mirror learning update (26) and the
distance between π̂ and πn is similar to the distance between πn+1 and πn at every s ̸= s0 but smaller at s = s0 .
By assumption, we have at s0 that


h
i ν π̂

Mπ̂D Vπn (s0 ) = EA∼π̂ Qπn (s0 , A) − ππnn Dπn (π̂ | s0 )
d
h
i ν πn
= EA∼πn Qπn (s0 , A) − ππnn Dπn (πn | s0 )
d


= MπDn Vπn (s0 )
 π

> MDn+1 Vπn (s0 ).

It follows that
h
h
h

i 
i 
i
h
i
π
π
ES∼dπn Mπ̂D Vπn (S) − ES∼dπn MDn+1 Vπn (S) = dπn (s0 ) Mπ̂D Vπn (s0 ) − MDn+1 Vπn (s0 ) > 0,

28

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

i
h
i
h
π
where we used that Mπ̂D Vπn (s) = MDn+1 Vπn (s) for s ̸= s0 . Thus,
h
h
i 
i 
π
ES∼dπn Mπ̂D Vπn (S) > ES∼dπn MDn+1 Vπn (S) ,
which contradicts the mirror learning update rule, i.e. that
h
h
i 
i 
π
ES∼dπn MDn+1 Vπn (S) = max ES∼dπn Mπ̄D Vπn (S) .
π̄∈N (πn )
∞
Hence, we have shown that the sequence of policies πn n=0 created by the mirror learning updates monotonically
 πn+1

increases
the
 πn
 mirror learning operator at every state. Next, we show that this property, i.e. MD Vπn (s) ≥
MD Vπn (s), implies the monotonic improvement in the value function
Vπn+1 (s) ≥ Vπn (s)
(28)
for all s ∈ S and n ∈ N0 .
By using the definitions of the value function Vπ , the action-value function Qπ , the mirror learning operator Mπ̄D Vπ
and the identity Dπ (π | s) = 0, adding zeros and rearranging, we obtain
h
i
h
i
Vπn+1 (s) − Vπn (s) = Eπn+1 R + γVπn+1 (S ′ ) − Eπn R + γVπn (S ′ )
h
i
h
i
= Eπn+1 R + γVπn+1 (S ′ ) − Eπn R + γVπn (S ′ )
π

π

π

π

νπnn+1 (s)
νπnn+1 (s)
D
(π
|
s)
−
Dπn (πn+1 | s)
π
n+1
n
dπn (s)
dπn (s)
h
i
h
i
= Eπn+1 R + γVπn+1 (S ′ ) + γVπn (S ′ ) − γVπn (S ′ ) − Eπn R + γVπn (S ′ )
+

νπnn+1 (s)
νπnn+1 (s)
D
(π
|
s)
−
Dπn (πn+1 | s)
πn
n+1
dπn (s)
dπn (s)


h
i ν πn+1 (s)
π
= Eπn+1 R + γVπn (S ′ ) − nπn
Dπn (πn+1 | s)
d (s)


h
i ν πn (s)
− Eπn R + γVπn (S ′ ) − ππnn Dπn (πn | s)
d (s)
h
i ν πn+1 (s)
π
+ γEπn+1 Vπn+1 (S ′ ) − Vπn (S ′ ) + nπn
Dπn (πn+1 | s)
d (s)


h
i ν πn+1 (s)
πn
= Eπn+1 Qπn (s, A) − πn
Dπn (πn+1 | s)
d (s)


h
i ν πn (s)
πn
− Eπn Qπn (s, A) − πn Dπn (πn | s)
d (s)
h
i ν πn+1 (s)
π
+ γEπn+1 Vπn+1 (S ′ ) − Vπn (S ′ ) + nπn
Dπn (πn+1 | s)
d (s)
 πn+1
  πn

= MD Vπn − MD Vπn
h
i ν πn+1 (s)
(29)
π
+ γEπn+1 Vπn+1 (S ′ ) − Vπn (S ′ ) + nπn
Dπn (πn+1 | s)
d (s)
h
i ν πn+1 (s)
π
≥ γEπn+1 Vπn+1 (S ′ ) − Vπn (S ′ ) + nπn
Dπn (πn+1 | s),
d (s)
where we used Inequality (27) in the final step. We take the infimum over states and replace the expectation with
another infimum over states as a lower bound:


h
i
h
i ν πn+1 (s)
πn
′
′
inf Vπn+1 (s) − Vπn (s) ≥ inf γEπn+1 Vπn+1 (S ) − Vπn (S ) + πn
Dπn (πn+1 | s)
s∈S
s∈S
d (s)


h
i ν πn+1 (s)
πn
′
′
≥ inf γ inf
V
(s
)
−
V
(s
)
+
D
(π
|
s)
π
π
π
n+1
n+1
n
n
s∈S
s′ ∈S
dπn (s)
 πn+1

h
i
νπn (s)
′
′
= γ inf
V
(s
)
−
V
(s
)
+
inf
D
(π
|
s)
.
πn+1
πn
πn
n+1
s′ ∈S
s∈S
dπn (s)
+

29

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

From this expression, we obtain
h
i
inf Vπn+1 (s) − Vπn (s) ≥
s∈S


 πn+1
1
νπn (s)
D
(π
|
s)
≥ 0,
inf
πn
n+1
1 − γ s∈S dπn (s)

π

since νπnn+1 (s) and dπn (s) are probabilities and the drift D is non-negative. Thus, we have proven the monotonic
improvement of value functions Vπn+1 (s) ≥ Vπn (s). We observe that this already implies the strict monotonic
improvement property

 πn+1
νπn (S)
J(πn+1 ) ≥ J(πn ) + ES∼p0
D
(π
|
S)
π
n+1
n
dπn (S)
for all n ∈ N0 since applying (28) and (27) sequentially to (29) yields for all s ∈ S

 
 π
Vπn+1 (s) − Vπn (s) = MDn+1 Vπn − MπDn Vπn
h
i ν πn+1 (s)
π
+ γEπn+1 Vπn+1 (S ′ ) − Vπn (S ′ ) + nπn
Dπn (πn+1 | s)
d (s)
 π
 
 νππn+1 (s)
≥ MDn+1 Vπn − MπDn Vπn + nπn
Dπn (πn+1 | s)
d (s)
π
νπ n+1 (s)
≥ nπn
Dπn (πn+1 | s).
d (s)
We obtain the desired inequality by taking the expectation over S ∼ p0 .
Step 2
From step 1, we know that the value functions increase uniformly over the state space, i.e. Vπn+1 (s) − Vπn (s) ≥ 0,
for all s ∈ S, n ∈ N0 . As the rewards r are bounded by assumption and we consider the episodic case where episode
lengths are also bounded
by T (albeit the same argument
applies for infinite time horizons via discounting), the value
P

T
k
functions Vπ (s) = Eπ
γ
R
|
S
=
s
are
also uniformly bounded. Via the Monotone Convergence
t+k+1
t
k=0
∞
Theorem (Theorem D.13), the sequence of value functions Vπn n=0 must therefore converge to some limit V .
Step 3
∞
Now, we show the existence of limit points of the sequence of policies πn n=0 and prove by contradiction that these
are fixed points of the mirror learning update (26).
∞
The sequence πn n=0 is bounded, thus the Bolzano-Weierstrass Theorem (Theorem D.14) yields the existence of
∞
limits π̄ to which some respective subsequence πni i=0 converges. We denote this set of limit points as LΠ. For each
∞
element of such a convergent subsequence πni i=0 , mirror learning solves the optimization problem
h
 i
max ES∼dπni MπD Vπni (S)
(30)
π∈N (πni )

This expression is continuous in πni due to the continuity of the value function [45], the drift and neighborhood operator
(by definition) and the sampling distribution (by assumption). Let π̄ = limi→∞ πni . Berge’s Maximum Theorem
(Theorem D.15) [6] now guarantees the convergence of the above expression, yielding
h
h
 i
 i
lim max ES∼dπni MπD Vπni (S) = max ES∼dπ̄ MπD Vπ̄ (S) .
(31)
i→∞ π∈N (πni )

π∈N (π̄)

For all i ∈ N0 , we obtain the next policy πni +1 as the argmax of Expression (30). Since this expression converges to
∞
∞
the limit in (31), there must exist some subsequence πnik +1 k=0 of πni +1 i=0 which converges to some policy π ′ ,
which is the solution to the optimization problem (31). We now show by contradiction that π ′ = π̄, which implies that
π̄ is a fixed point of the mirror learning update rule.
Suppose π ′ ̸= π̄. As π ′ is induced by the mirror learning update rule, the monotonic improvement results from step 1
yield
h
i
h
i
Qπ′ (s, a) = ER,S ′ ∼P R + γVπ′ (S ′ ) ≥ ER,S ′ ∼P R + γVπ̄ (S ′ ) = Qπ̄ (s, a)

and

 π′ 


MD Vπ̄ (s) ≥ Mπ̄D Vπ̄ (s).
30

(32)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Suppose
ES∼dπ̄

h
h
 i
 i
′
MπD Vπ̄ (S) > ES∼dπ̄ Mπ̄D Vπ̄ (S) ,

then we have for some state s
h
i ν π′ (s)
 π′ 
Dπ̄ (π ′ | s)
MD Vπ̄ (s) = Eπ′ Qπ̄ (s, A) − π̄π̄
d (s)
h
i ν π̄ (s)


> Mπ̄D Vπ̄ (s) = Eπ̄ Qπ̄ (s, A) − π̄π̄ Dπ̄ (π̄ | s)
d (s)
h
i
= Eπ̄ Qπ̄ (s, A) = Vπ̄ (s) = V (s).
In the last equality, we used that the sequence of value functions converges to some unique limit V , which implies
Vπ̄ = V . We obtain the following via this result, Inequality (32), which must be strict for s, and the non-negativity of
the drift D:


Vπ′ (s) = Eπ′ Qπ′ (s, A)


> Eπ′ Qπ̄ (s, A)
′


 ν π (s)
> Eπ′ Qπ̄ (s, A) − π̄π̄
Dπ̄ (π ′ | s)
d (s)
> V (s).
However due to Vπ′ (s) = limk→∞ Vπni +1 , this contradicts the uniqueness of the value limit, which gives Vπ′ = V .
k
Therefore, we have shown by contradiction that
h
 i
π̄ ∈ arg max ES∼dπ̄ MπD Vπ̄ (S) .
π∈N (π̄)

Step 4
∞
Following step 3, let π̄ be a limit point of πn n=0 . We will show by contradiction that π̄ is also a fixed point of GPI
(see Theorem 2.1), i.e. that for all s ∈ S




π̄ ∈ arg max EA∼π Aπ̄ (s, A) = arg max EA∼π Qπ̄ (s, A) .
(33)
π∈Π

π∈Π

From step 3, we know that

h
i
νπ̄π (S)
π̄ ∈ arg max ES∼dπ̄ ,A∼π Qπ̄ (S, A) − π̄
Dπ̄ (π | S)
d (S)
π∈Π

h
i
νπ̄π (S)
= arg max ES∼dπ̄ ,A∼π Aπ̄ (S, A) − π̄
Dπ̄ (π | S)
d (S)
π∈Π

(34)

as subtracting an action-independent baseline does not affect the argmax. Now, we assume the existence of a policy π ′
and state s with




EA∼π′ Aπ̄ (s, A) > EA∼π̄ Aπ̄ (s, A) = 0.
(35)
Let m = |A| denote the size of the action space. Then, we can write for any policy π, π(· | s) = x1 , . . . , xm−1 , 1 −
Pm−1 
i=1 xi . With this notation, we have
m

 X
EA∼π Aπ̄ (s, A) =
π(ai | s)Aπ̄ (s, ai )
i=1

=

m−1
X

m−1

X 
xi Aπ̄ (s, ai ) + 1 −
xi Aπ̄ (s, am )

i=1

=

m−1
X

i=1



xi Aπ̄ (s, ai ) − Aπ̄ (s, am ) + Aπ̄ (s, am ).

i=1

31

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning



This shows that EA∼π Aπ̄ (s, A) is an affine function of π(· | s), which implies that all its Gâteaux derivatives are
constant in ∆(A) for fixed directions. Due to Inequality (35), this further implies that the Gâteaux derivatives in
ν π (s)
direction from π̄ to π ′ are strictly positive. Additionally, we have that the Gâteaux derivatives of dπ̄π̄ (s) Dπ̄ (π | s) are
zero at π = π̄. We see this by establishing lower and upper bounds, which both have derivatives of zero due to the
independence of π and the zero-gradient property of the drift:
1
dπ̄ (s)

Dπ̄ (π̄ | s) =

νπ̄π̄ (s)
ν π (s)
1
Dπ̄ (π̄ | s) = 0 ≤ π̄π̄ Dπ̄ (π | s) ≤ π̄ Dπ̄ (π | s)
π̄
d (s)
d (s)
d (s)

recalling that Dπ̄ (π̄ | s) = 0 for any s ∈ S and using νπ̄π (s) ≤ 1. In combination, we obtain that the Gâteaux derivative

 ν π (s)
of EA∼π Aπ̄ (s, A) − dπ̄π̄ (s) Dπ̄ (π | s) is strictly positive as well. Therefore, we can find some policy π̂(· | s) by taking
a sufficiently small step from π̄(· | s) in the direction of π ′ (· | s) such that π̂ ∈ N (π̄) and

 ν π̂ (s)

 ν π̄ (s)
EA∼π̂ Aπ̄ (s, A) − π̄π̄ Dπ̄ (π̂ | s) > EA∼π̄ Aπ̄ (s, A) − π̄π̄ Dπ̄ (π̄ | s) = 0.
d (s)
d (s)
With this, we can construct a policy which contradicts Equation (34). Let π̃ be defined such that

π̄(· | x) if x ̸= s,
π̃(· | x) =
π̂(· | x) if x = s.
This guarantees π̃ ∈ N (π̄) and



 νπ̄π̃ (S)
Dπ̄ (π̃ | S)
ES∼dπ̄ EA∼π̃ Aπ̄ (S, A) − π̄
d (S)



 νπ̄π̃ (S)
π̄
= d (s) EA∼π̃ Aπ̄ (s, A) − π̄
Dπ̄ (π̃ | s)
d (s)



 ν π̂ (s)
= dπ̄ (s) EA∼π̂ Aπ̄ (s, A) − π̄π̄ Dπ̄ (π̂ | s)
d (s)
> 0,
which contradicts Equation (34), so the assumption (35) must be wrong, proving




π̄ = arg max EA∼π Aπ̄ (s, A) = arg max EA∼π Qπ̄ (s, A) .
π∈Π

π∈Π

Step 5

The main result (33) from step 4 shows that any limit point π̄ of πn n∈N is also a fixed point of GPI. Thus, as
corollaries all properties induced by GPI (see Theorem 2.1) apply to π̄ ∈ LΠ. Particularly, we have the optimality of π̄,
the value function optimality V = Vπ̄ = V ∗ and thereby also the maximality of returns as




lim J(πn ) = lim ES∼p0 Vπn (S) = ES∼p0 V ∗ (S) = max J(π).
n→∞

n→∞

π∈Π

Thus, we have shown all properties as claimed by Theorem 5.4.
We close this section with some remarks. In practice, exact updates according to the mirror learning update rule (26)
are generally infeasible. Instead, we can sample the expectation to obtain batch estimators over a batch D of transitions

ν πnew
1 X
Qπold (s, a) − ππold
D
(π
|
s)
,
πold
new
|D|
d old
s,a∈D

where Qπold has to be estimated as well. These batch estimators can also only be approximately optimized each iteration
via gradient ascent to update the policy. Given these approximations and the at-best local convergence of gradient
ascent, the outlaid convergence properties remain theoretical.
32

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

HalfCheetah

7000
5000

5000

4000
3000
2000

3000

1000

0
0

1

2

3
4
5
6
Environment steps (million)

7

8

0

Humanoid

4000
2000

2

3
4
5
6
Environment steps (million)

7

8

7

8

REINFORCE
A2C
TRPO
PPO
V-MPO

3000
Episode reward

6000

1

Hopper

4000

REINFORCE
A2C
TRPO
PPO
V-MPO

8000
Episode reward

4000

2000

1000

0

REINFORCE
A2C
TRPO
PPO
V-MPO

6000

Episode reward

6000
Episode reward

Ant

REINFORCE
A2C
TRPO
PPO
V-MPO

2000
1000
0

0

1

2

3
4
5
6
Environment steps (million)

7

8

0

1

2

3
4
5
6
Environment steps (million)

Figure 4: Comparison of rewards per episode during training on several MuJoCo tasks. For each algorithm, we report
means and standard deviations of three runs with different random seeds.

6

Numerical Experiments

Now, we empirically compare the discussed policy gradient algorithms. Consistent with the original works [54, 69,
71, 73], we compare them on the established MuJoCo task suite [80], accessed through the Gymnasium library [81].
MuJoCo features robotics simulations, where the tasks are to control and move robots of different shapes by applying
torques to each joint.
Our implementations build on the PPO implementation from the BRAX library [23] and are written in JAX [13]. For
enhanced comparability, all algorithms that estimate advantages use GAE similarly to PPO. Instead of A3C, we use
its synchronous variant A2C due to its simpler implementation. Note that A2C exhibits comparable performance as
A3C [85] and only differs in that it waits for all actors to collect transitions to update them synchronously. We modify
REINFORCE to average gradients over batches of transitions similarly as in the other algorithms since computing one
update per environment step is computationally very costly. Note that this is however likely to improve the performance
compared to a naive implementation of REINFORCE. We do not tune hyperparameters and keep choices consistent
across algorithms where possible. See Appendix Appendix A for the hyperparameters we use. The experiments were
run on a standard consumer CPU. All our implemented algorithms and the code for running the experiments can be
found at https://github.com/Matt00n/PolicyGradientsJax.
In our main experiment, we compare the performance of the algorithms in terms of the achieved episodic rewards
over the course of training. The performances in different MuJoCo tasks are presented in Figure 4. We observe that
PPO outperforms the other algorithms in three of four tasks by achieving higher episodic rewards while learning good
policies quickly. The performance difference is most prevalent on the Humanoid-task, the most challenging of the
four, where PPO learns much stronger policies than the other algorithms. In addition, we find our implementation of
PPO to be competitive with common RL libraries as shown in Appendix B.1. V-MPO and TRPO are comparable in
performance, with each of the two slightly outperforming the other on two out of four environments. We note that
V-MPO is intended for training for billions of environment steps, such that its lower performance compared to PPO in
our experiments is expected10 [73]. A2C requires more interactions with the environment to reach similar performance
10

Also see the discussions at https://openreview.net/forum?id=SylOlp4FvH on this.

33

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

levels as V-MPO and TRPO but fails to learn any useful policy in the Ant-task. This slower learning11 is at least partially
caused by A2C only using a single update epoch per batch. REINFORCE performance worst on all environments,
which is unsurprising giving the high variance of gradients in REINFORCE [75]. This also highlights the benefits of
the bias-variance trade-off by the other algorithms as discussed in Section 4.6. We find our performance-based ranking
of the algorithms to be consistent with literature (e.g., [71, 73, 5]).
Moreover, we remark that A2C is the only algorithm for which we used an entropy bonus because the learned policies
collapsed without it. We showcase this in our expended experiments in Appendix B.2. This underlines the usefulness of
the (heuristic) constraints of V-MPO, PPO and TRPO on the KL divergence, which avoid such collapses even without
any entropy bonuses. To further investigate this, we show the average KL divergences between consecutive policies
throughout training in Figure 5. Here, we approximated the KL divergence using the unbiased estimator [68]



πnew (A | s)
πnew (A | s)
D̂KL πold (· | s) ∥ πnew (· | s) = EA∼πold
− 1 − ln
πold (A | s)
πold (A | s)
for all algorithms except TRPO, which analytically calculates the exact KL divergence since it is used within the algorithm. We see that the KL divergences remain relatively constant for all algorithms after some initial movement. TRPO
displays the most constant KL divergence, which is explained by its hard constraint. With the chosen hyperparameters,
V-MPO uses the same bound on the KL divergence as TRPO, however without strictly enforcing it as outlined in the
derivation of V-MPO. Thus, V-MPO’s KL divergence exhibits slightly more variance then TRPO and also frequently
exceeds this bound. PPO’s clipping heuristic achieves a similar effect resulting in a comparable picture. Due to the
lack of constraints on the KL divergence, A2C and REINFORCE show slightly more variance. Interestingly, their
KL divergences are orders of magnitudes lower than for the other algorithms, especially for REINFORCE (note the
logarithmic scale in Figure 5). We reason this with A2C and REINFORCE using only a singly update epoch per
batch, whereas the PPO and V-MPO use multiple epochs and TRPO uses a different update scheme via line search.
In Appendix B.3, we provide experimental evidence for this hypothesis. Additionally, we note again that the entropy
bonus also stabilizes and limits the KL divergence for A2C as shown in Appendix B.2.
These findings highlight the benefits of regularization through constraining the KL divergence and incentivizing entropy.
Regularization stabilizes learning and prevents a collapse of the policy. At the same time, it allows more frequent
updates through multiple epochs per batch, which drastically increases the sample efficiency of the algorithms and
speeds up learning.

7

Conclusion

In this work, we presented a holistic overview of on-policy policy gradient methods in reinforcement learning. We
derived the theoretical foundations of policy gradient algorithms, primarily in the form of the Policy Gradient Theorem.
We have shown how the most prominent policy gradient algorithms can be derived based on this theorem. We discussed
common techniques used by these algorithms to stabilize training including learning an advantage function to limit
the variance of estimated policy gradients, constraining the divergence between policies and regularizing the policy
through entropy bonuses. Subsequently, we presented evidence from literature on the convergence behavior of policy
gradient algorithms, which suggest that they may find at least locally optimal policies. Finally, we conducted numerical
experiments on well-established benchmarks to further compare the behavior of the discussed algorithms. Here, we
found that PPO outperforms the other algorithms in the majority of the considered tasks and we provided evidence for
the necessity of regularization, by constraining KL divergence or by incentivizing entropy, to stabilize training.
We acknowledge several limitations of our work. First, we deliberately limited our scope to on-policy algorithms,
which excludes closely related off-policy policy gradient algorithms and the novelties introduced by them. Second, we
presented an incomplete overview of on-policy policy gradient algorithms as other, albeit less established, algorithms
exist (e.g., [61, 16]) and the development of further algorithms remains an active research field. Here, we focused on
the, in our view, most prominent algorithms as determined by their impact, usage and introduced novelties. Third, the
convergence results we referenced rest on assumptions that are quickly violated in practice. In particular, we want
to underline that the results based mirror learning rely on the infeasible assumption of finding a global maximizer
each iteration. Fourth, while we compared the discussed algorithms empirically and found results to be consistent
with existing literature, our analysis is limited to the specific setting we used. Different results may arise on other
benchmarks, with different hyperparameters or generally different implementations.
Finally, we note that still many questions remain to be answered in the field of on-policy policy gradient algorithm.
So far, our understanding of which algorithm performs best under which circumstances is still limited. Moreover,
11

Slow in terms of the required environment steps. Note however that A2C runs significantly faster than PPO, TRPO and V-MPO
in absolute time due to using less epochs per batch.

34

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

HalfCheetah

Ant

100

10 1
10 3
REINFORCE
A2C
TRPO
PPO
V-MPO

10 4
10 6
10 8

KL divergence

KL divergence

10 2

10 5
10 7

REINFORCE
A2C
TRPO
PPO
V-MPO

10 9

10 10

10 11
0

1

2

3
4
5
6
Environment steps (million)

7

8

0

1

2

Humanoid

7

8

Hopper

100

10 3
KL divergence

10 2
KL divergence

3
4
5
6
Environment steps (million)

10 4
10 6
REINFORCE
A2C
TRPO
PPO
V-MPO

10 8
10 10
0

1

2

10 5

REINFORCE
A2C
TRPO
PPO
V-MPO

10 7
10 9
10 11

3
4
5
6
Environment steps (million)

7

8

0

1

2

3
4
5
6
Environment steps (million)

7

8

Figure 5: Comparison of the average KL divergence across policies during training.
it is unclear whether the best possible policy gradient algorithm has yet been discovered, which is why algorithm
development remains of interest. Similarly, comprehensive empirical comparisons with other classes of RL algorithms
may yield further insights on the practical advantages and disadvantages of policy gradient algorithms and how their
performance depends on the problem settings. Finally, we observe that still only a limited number of convergence
results exist and not even all discussed algorithms are covered by these, e.g., no convergence results exist for V-MPO to
the best of our knowledge. Here, further research is needed to enhance our understanding of the convergence behavior
of policy gradient algorithms.

35

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

References
[1] Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan Belov, Nicolas
Heess, and Martin Riedmiller. Relative entropy regularized policy iteration. arXiv preprint arXiv:1812.02256,
2018.
[2] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller.
Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
[3] Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
[4] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy
gradient methods in markov decision processes. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings of
Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages
64–66. PMLR, 09–12 Jul 2020.
[5] Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard
Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in on-policy reinforcement
learning? a large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020.
[6] Lawrence M Ausubel and Raymond J Deneckere. A generalized theorem of the maximum. Economic Theory,
3(1):99–107, 1993.
[7] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834–846, 1983.
[8] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
[9] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.
[10] Julius Berner, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. The modern mathematics of deep learning.
arXiv preprint arXiv:2105.04026, pages 86–114, 2021.
[11] Jalaj Bhandari and Daniel Russo. On the linear convergence of policy gradient methods for finite mdps. In
International Conference on Artificial Intelligence and Statistics, pages 2386–2394. PMLR, 2021.
[12] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
[13] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
[14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances
in neural information processing systems, 33:1877–1901, 2020.
[15] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun. The loss surfaces of
multilayer networks. In Artificial intelligence and statistics, pages 192–204. PMLR, 2015.
[16] Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In International
Conference on Machine Learning, pages 2020–2027. PMLR, 2021.
[17] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and
systems, 2(4):303–314, 1989.
[18] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in
neural information processing systems, 27, 2014.
[19] Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839,
2012.
[20] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1–22, 1977.
[21] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman,
Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines,
2017.
36

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[23] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax - a
differentiable physics engine for large scale rigid body simulation, 2021.
[24] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods.
In International conference on machine learning, pages 1587–1596. PMLR, 2018.
[25] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the
fourteenth international conference on artificial intelligence and statistics, pages 315–323. JMLR Workshop and
Conference Proceedings, 2011.
[26] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
[27] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning Research, 5(9), 2004.
[28] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages
1861–1870. PMLR, 2018.
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[30] Magnus R Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear systems. Journal of
research of the National Bureau of Standards, 49(6):409–436, 1952.
[31] Timothy Classen Hesterberg. Advances in importance sampling. Stanford University, 1988.
[32] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
[33] Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stańczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard Hussenot, Robert Dadashi,
Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour,
Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang,
Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gómez Colmenarejo, Serkan
Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando
de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979,
2020.
[34] Markus Holzleitner, Lukas Gruber, José Arjona-Medina, Johannes Brandstetter, and Sepp Hochreiter. Convergence
proof for actor-critic methods applied to ppo and rudder. In Transactions on Large-Scale Data-and KnowledgeCentered Systems XLVIII: Special Issue In Memory of Univ. Prof. Dr. Roland Wagner, pages 105–130. Springer,
2021.
[35] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.
[36] Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and
João G.M. Araújo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms.
Journal of Machine Learning Research, 23(274):1–18, 2022.
[37] David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician, 58(1):30–37, 2004.
[38] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of
the Nineteenth International Conference on Machine Learning, pages 267–274, 2002.
[39] Prasenjit Karmakar and Shalabh Bhatnagar. Two time-scale stochastic approximation with controlled markov
noise and off-policy temporal-difference learning. Mathematics of Operations Research, 43(1):130–151, 2018.
[40] Henry J Kelley. Gradient theory of optimal flight paths. Ars Journal, 30(10):947–954, 1960.
[41] Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function. The Annals of
Mathematical Statistics, pages 462–466, 1952.
[42] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
37

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

[43] Vijay R Konda and John N Tsitsiklis. Onactor-critic algorithms. SIAM journal on Control and Optimization,
42(4):1143–1166, 2003.
[44] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. Advances in neural information processing systems, 25, 2012.
[45] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust
region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.
[46] Jakub Grudzien Kuba, Christian Schroeder de Witt, and Jakob Foerster. Mirror learning: A unifying framework of
policy optimisation. arXiv preprint arXiv:2201.02373, 2022.
[47] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.
[48] Johannes Lederer. Activation functions in artificial neural networks: A systematic overview. arXiv preprint
arXiv:2101.09957, 2021.
[49] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E. Gonzalez,
Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In International
Conference on Machine Learning (ICML), 2018.
[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and
Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[51] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization attains
globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.
[52] Peter Marbach and John N Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE
Transactions on Automatic Control, 46(2):191–209, 2001.
[53] Charles C Margossian. A review of automatic differentiation and its efficient implementation. Wiley interdisciplinary reviews: data mining and knowledge discovery, 9(4):e1305, 2019.
[54] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David
Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International
conference on machine learning, pages 1928–1937. PMLR, 2016.
[55] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik,
Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Humanlevel control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
[56] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. Adaptive
Computation and Machine Learning. MIT Press, Cambridge, MA, 2 edition, 2018.
[57] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings
of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.
[58] David Pollard. Asymptopia: an exposition of statistical asymptotic theory. In Asymptopia: an exposition of
statistical asymp-totic theory, 2000.
[59] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational
mathematics and mathematical physics, 4(5):1–17, 1964.
[60] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stablebaselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1–8,
2021.
[61] Md Masudur Rahman and Yexiang Xue. Robust policy optimization in deep reinforcement learning. arXiv
preprint arXiv:2212.07536, 2022.
[62] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941, 2017.
[63] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,
pages 400–407, 1951.
[64] Herbert Robbins and David Siegmund. A convergence theorem for non negative almost supermartingales and
some applications. In Optimizing methods in statistics, pages 233–257. Elsevier, 1971.
38

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

[65] Reuven Y. Rubinstein. Simulation and the Monte Carlo Method. Wiley, New York, first edition, 1981.
[66] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error
propagation, 1985.
[67] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37. University
of Cambridge, Department of Engineering Cambridge, UK, 1994.
[68] John Schulman. Approximating kl divergence. John Schulman’s Homepage, 2020.
[69] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889–1897. PMLR, 2015.
[70] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous
control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
[71] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
[72] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,
John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature,
529(7587):484–489, 2016.
[73] H Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W Rae, Seb
Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, et al. V-mpo: On-policy maximum a posteriori policy optimization
for discrete and continuous control. arXiv preprint arXiv:1909.12238, 2019.
[74] Richard S Sutton and Andrew G Barto. Toward a modern theory of adaptive networks: expectation and prediction.
Psychological review, 88(2):135, 1981.
[75] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second
edition, 2018.
[76] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12,
1999.
[77] Richard S Sutton, Satinder Singh, and David McAllester. Comparing policy-gradient algorithms. IEEE Transactions on Systems, Man, and Cybernetics, 2000.
[78] Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. University of Massachusetts
Amherst, 1984.
[79] Gerald Tesauro et al. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58–68,
1995.
[80] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012.
[81] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goulão,
Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet
Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023.
[82] Hado van Hasselt. Reinforcement learning lecture 5: Model-free prediction, October 2021.
[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[84] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
[85] Lilian Weng. Policy gradient algorithms. lilianweng.github.io, 2018.
[86] Robert Edwin Wengert. A simple automatic derivative evaluation program. Communications of the ACM,
7(8):463–464, 1964.
[87] Ronald J Williams. Reinforcement-learning connectionist systems. College of Computer Science, Northeastern
University, 1987.
39

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

[88] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Reinforcement learning, pages 5–32, 1992.
[89] Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms.
Connection Science, 3(3):241–268, 1991.
[90] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to
(almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586–3612, 2020.

40

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Value
Hyperparameter

REINFORCE

A2C

TRPO

PPO

V-MPO

Learning rate
3 · 10−4
3 · 10−4 3 · 10−4
Num. minibatches
1
8
8
Num. epochs
1
1
112
Discount (γ)
—
0.99
0.99
GAE parameter (λ)
—
0.95
0.95
Normalize advantages
—
True
True
Entropy bonus coef.
0
0.1
0
Max. grad. norm
0.5
0.5
0.5
Unroll length
—
2048
2048
KL target (δ)
—
—
0.01
CG damping
—
—
0.1
CG max. iterations
—
—
10
Line search max. iterations
—
—
10
Line search shrinkage factor
—
—
0.8
PPO clipping (ε)
—
—
—
Min. temp. (ηmin )
—
—
—
Min. KL pen. (νmin )
—
—
—
Init. temp. (ηinit )
—
—
—
Init. KL pen. (mean) (νµinit )
—
—
—
Init. KL pen. (std) (νσinit )
—
—
—
KL target (mean) (ενµ )
—
—
—
KL target (std) (ενσ )
—
—
—
KL target (temp.) (εη )
—
—
—
Table 2: Algorithm hyperparameters.

3 · 10−4
8
10
0.99
0.95
True
0
0.5
2048
—
—
—
—
—
0.2
—
—
—
—
—
—
—
—

3 · 10−4
8
10
0.99
0.95
False
0
0.5
2048
—
—
—
—
—
—
10−8
10−8
1
1
1
0.01
5 · 10−5
0.01

Appendices
A

Hyperparameters

We report the hyperparameters we use in our main experiments in Table 2. All algorithms use separate policy and value
networks. Policy networks use 4 hidden layers with 32 neurons respectively. Value networks use 5 layers with 256
neurons each. We use swish-activation functions [62] throughout both networks. Policy outputs are transformed to fit
the bounds of the actions spaces via a squashing function. We use the Adam optimizer [42] with gradient clipping and a
slight linear decay of the learning rates. Further, we preprocess observations and rewards by normalizing them using
running means and standard deviations and clipping them to the interval [−10, 10]. All algorithms except REINFORCE
use 8 parallel environments to collect experience. We use independent environments to evaluate the agents throughout
training. In the evaluations, agents select actions deterministically as the mode of the constructed distribution.

B

Extended Experiments

Here, we present results from further experiments. Unless indicated otherwise, we use the hyperparameters as reported
in Appendix Appendix A.
B.1

Comparison to RL frameworks

In Table 3, we compare the performance of our implementation of PPO with popular RL frameworks. Note that we did
not tune any hyperparameters for our implementations such that the reported scores should be understood as lower
bounds. We compare PPO since it is the most popular and commonly implemented of the discussed algorithms across
frameworks. In contrast, especially TRPO and V-MPO are rarely found.
12
13

TRPO uses one epoch for its policy updates but 10 epochs per batch for updating the value network.
Numbers read approximately from plots in the paper.

41

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Framework

MuJoCo version
Steps in million

CleanRL
[36]

Baselines
[21]

SB3
[60]

RLlib
[49]

ACME13
[33]

Ours

Ours

v4
1

v1
1

v3
1

v2
44

v2
10

v4
1

v4
8

HalfCheetah
2906
1669
5819 9664
6800
4332 6414
Hopper
2052
2316
2410
—
2550
895 2616
Humanoid
742
—
—
—
6600
700 7633
Ant
—
—
1327
—
5200
1258 5671
Table 3: Comparison of the mean performance of our PPO implementation with popular RL frameworks. Scores for the
frameworks are shown as reported in the respective paper or documentation.

0.1
0.01
0

4000

109
106

KL divergence

Episode reward

3000
2000
1000

103

0

100

1000

10 3
0

1

0.1
0.01
0

1012

2

3
4
5
6
Environment steps (million)

7

8

0

1

2

3
4
5
6
Environment steps (million)

7

8

Figure 6: We compare the episode reward (left) and KL divergence (right) for different values of the entropy coefficient
for A2C on HalfCheetah.

B.2

Entropy Bonus in A2C

In Figure 6, we show that using an entropy bonus improves the performance of A2C by stabilizing learning. In particular,
insufficiently low values of the entropy coefficient result in a collapse of the policy after some time. This is visible in a
drastic increase in the KL divergences (note the logarithmic scale).
B.3

A2C and REINFORCE with Multiple Update Epochs

In Figure 7, we showcase that the KL divergence is low for A2C and REINFORCE due to using only a single update
epoch per batch. On the contrary, when using multiple epochs, the policies collapse for both algorithms as visible by the
diverging KL divergence and abrupt performance loss. Note, that here we show this behavior for five epochs, however
in our tests A2C and REINFORCE display similar behaviors already when only using two epochs, albeit the policies
then only collapse after an extended period of time. Further, note that over the displayed range of environment steps,
the algorithms do not yet learn any useful policies when using a single epoch. However, performance improves for both
A2C and REINFORCE when given more time as depicted in Figure 4.

C

V-MPO: Derivation Details

In the following, we provide a more detailed derivation of the objective function of V-MPO
JV-MPO (θ, η, ν) = Lπ (θ) + Lη (η) + Lν (θ, ν),
where Lπ is the policy loss
Lπ (θ) = −

exp

X
a,s∈D̃

P

a′ ,s′ ∈D̃



Âϕ (s,a)
η

exp

42





Âϕ (s′ ,a′ )
η

 ln πθ (a | s),

(36)

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

600

109
106

400

KL divergence

Episode reward

1012

A2C, 1 epoch
A2C, 5 epochs
REINFORCE, 1 epoch
REINFORCE, 5 epochs

800

200
0
200

A2C, 1 epoch
A2C, 5 epochs
REINFORCE, 1 epoch
REINFORCE, 5 epochs

103
100
10 3
10 6

400

10 9

600
0.0

0.2

0.4
0.6
0.8
Environment steps (million)

1.0

0.0

0.2

0.4
0.6
0.8
Environment steps (million)

1.0

Figure 7: We compare the episode reward (left) and KL divergence (right) for different numbers of update epochs for
A2C and REINFORCE on HalfCheetah.
Lη is the temperature loss
"


#
1 X
Âϕ (s, a)
Lη (η) = ηεη + η ln
exp
η
|D̃|

(37)

a,s∈D̃

and Lν is the trust-region loss
 

hh
ii
 

1 X
Lν (θ, ν) =
ν εν − sg DKL (πold (· | s) ∥ πθ (· | s))
+ sg ν DKL πold (· | s) ∥ πθ (· | s) .
|D|

(38)

s∈D

Let pθ (s, a) = πθ (a | s)dπθ (s) denote the joint state-action distribution under policy πθ conditional on the parameters
θ. Let I be a binary random variable whether the updated policy πθ is an improvement over the old policy πold , i.e.
I = 1 if it is an improvement. We assume the probability of πθ being an improvement is proportional to the following
expression
 A (s, a) 
πold
pθ (I = 1 | s, a) ∝ exp
(39)
η
Given the desired outcome I = 1, we seek the posterior distribution conditioned on this event. Specifically, we seek the
maximum a posteriori estimate


θ∗ = arg max pθ (I = 1)ρ(θ)
θ


(40)
= arg max ln pθ (I = 1) + ln ρ(θ) ,
θ

where ρ is some prior distribution to be specified. Using Theorem D.7, we obtain



pθ (I = 1, S, A)
ln pθ (I = 1) = ES,A∼ψ ln
+ DKL ψ ∥ pθ (·, · | I = 1) ,
ψ(S, A)

(41)

where ψ is a distribution over S × A. Observe that, since the KL-divergence is non-negative, the first term is a lower
bound for ln pθ (I = 1). Akin to EM algorithms, V-MPO now iterates between an expectation (E) and a maximization
(M) step. In the E-step we choose the variational distribution ψ to minimize the KL divergence in Equation (41) to
make the lower bound as tight as possible. In the M-step, we maximize this lower bound and the prior ln ρ(θ) to obtain
a new estimate of θ∗ via Equation (40).
First, we consider the E-step. Minimizing DKL (ψ ∥ pθold (·, · | I = 1)) w.r.t. ψ leads to
ψ(s, a) = pθold (s, a | I = 1)
pθ (s, a) pθold (I = 1 | s, a)
= old
pθold (I = 1)
p (s, a) pθold (I = 1 | s, a)
R θold
=R
p (s, a) pθold (I = 1 | s, a) da ds
s∈S a∈A θold
43

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

using Bayes’ Theorem (Theorem D.2). Sampling from right-hand side of (39) thus yields


A
(s,a)
exp πoldη

,
ψ̂(s, a) = P
Aπold (s,a)
a,s∈D exp
η
which is the variational distribution found in the policy loss (37). [73] find that using only the highest 50 % of advantages
per batch, i.e. replacing D with D̃, substantially improves the algorithm. The advantage function Aπ is estimated by
Âϕ , which is learned identically as in A3C.
We derive the temperature loss to automatically adjust the temperature η by applying (39) to the KL term in (41), which
we want to minimize:




pθold (S, A)pθold (I = 1 | S, A)
DKL ψ ∥ p(·, · | I = 1) = DKL ψ ∥
pθold (I = 1)


(S,A) !
A
pθold (S, A) exp πoldη
= DKL ψ ∥
pθold (I = 1)


A
(s,a) !
Z Z
pθold (s, a) exp πoldη
da ds
=−
ψ(s, a) ln
ψ(s, a)pθold (I = 1)
s∈S a∈A

By applying the logarithm to the individual terms, rearranging and multiplying through by η we get

Z Z


Aπold (s, a)
DKL ψ ∥ p(·, · | I = 1) = −
ψ(s, a)
+ ln pθold (s, a)
η
s∈S a∈A

− ln pθold (I = 1) − ln ψ(s, a) da ds

Z Z
∝−
ψ(s, a) Aπold (s, a) + η ln pθold (s, a) − η ln pθold (I = 1)
s∈S a∈A


− η ln ψ(s, a) da ds
Z Z
Z
=−
ψ(s, a)Aπold (s, a) da ds + η
s∈S a∈A

Z

Z
ψ(s, a) ln

s∈S a∈A

ψ(s, a)
da ds
pθold (s, a)

Z

+λ

ψ(s, a) da ds

s∈S a∈A

with λ = η ln pθold (I = 1). To optimize η while minimizing the KL term, we transform this into a constrained
optimization problem with a bound on the KL divergence
Z Z
arg max
ψ(s, a)Aπold (s, a) da ds
ψ
s∈S a∈A

Z

Z
ψ(s, a) ln

subject to
s∈S a∈A

Z

ψ(s, a)
da ds ≤ εη ,
pθold (s, a)

Z
ψ(s, a) da ds = 1

s∈S a∈A

and then back into an unconstrained problem via Lagrangian relaxation, yielding the objective function

Z Z
J (ψ, η, λ) =
ψ(s, a)Aπold (s, a) da ds + η εη
s∈S a∈A

Z

Z

−
s∈S a∈A

ψ(s, a) ln



Z
ψ(s, a)
da ds + λ 1 −
pθold (s, a)

Z

s∈S a∈A

44


ψ(s, a) da ds .

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Differentiating w.r.t. ψ(s, a) and setting to zero yields




λ
Aπold (s, a)
exp −1 −
ψ(s, a) = pθold (s, a) exp
η
η
Normalizing over s and a confirms the already attained solution
Aπold (s,a) 
η
ψ(s, a) = R
,
R
Aπold (s,a) 
p
(s,
a)
exp
da
ds
θ
old
η
s∈S a∈A

pθold (s, a) exp

(42)

but now we can also find the optimal η by substituting this solution into J (ψ, η, λ). Doing so and dropping terms
independent of η leads to


Z Z
ψ(s, a)
η εη −
ψ(s, a) ln
da ds
pθold (s, a)
s∈S a∈A
Z Z
Z Z
(43)
ψ(s, a) ln ψ(s, a) da ds.
ψ(s, a) ln pθold (s, a) da ds − η
= ηεη + η
s∈S a∈A

s∈S a∈A

Because of Equation (42), we have


(s,a)
A
pθold (s, a) exp πoldη


ηψ(s, a) ln ψ(s, a) = ηψ(s, a) ln R
R
(s,a)
A
da ds
p (s, a) exp πoldη
s∈S a∈A θold
!


Z Z
Aπold (s, a)
da ds ,
= ψ(s, a) η ln pθold (s, a) + Aπold (s, a) − η ln
pθold (s, a) exp
η
s∈S a∈A

where the first summand cancels out the second term in (43) and the second summand no longer depends on η and thus
can be dropped. Hence, we obtain the temperature loss function
Z Z



Aπold (s, a)
Lη (η) = ηεη + η ln
exp
da ds
(44)
η
s∈S a∈A

through which we can optimize η using gradient descent.
Given the non-parametric sample-based variational distribution ψ(s, a), the M-step now optimizes the policy parameters
θ. Based on (40), we want to maximize the discussed lower bound, i.e. minimize
Z Z
pθ (I = 1, s, a)
−
ψ(s, a) ln
da ds − ln p(θ)
ψ(s, a)
s∈S a∈A

to find new policy parameters θ. Using Equations (42) and (39), the first term becomes
Z Z
pθ (I = 1, s, a)
−
ψ(s, a) ln
da ds
ψ(s, a)
s∈S a∈A
Z Z
pθ (I = 1 | s, a)pθ (s, a)
=−
ψ(s, a) ln
da ds
ψ(s, a)
s∈S a∈A
A
(s,a) 


exp πoldη
pθ (s, a)
1
=−
ψ(s, a) ln
da ds
R
A
(s,a)  R
A
(s,a) 
pθold (s, a) exp πoldη
p (s, a) exp πoldη
da ds
s∈S a∈A θold
s∈S a∈A


Z Z
pθ (s, a)
1
=−
ψ(s, a) ln
da ds.
R
Aπold (s,a) 
pθold (s, a) R
p
(s,
a)
exp
da
ds
θ
old
η
s∈S a∈A

Z

Z

s∈S a∈A

45

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Using pθ (s, a) = πθ (a | s)dπθ (s), assuming the state distribution dπ to be independent of θ and dropping terms that do
not depend on θ yields
 Z Z

 Z Z

pθ (I = 1, s, a)
arg min −
ψ(s, a) ln
da ds = arg min −
ψ(s, a) ln pθ (s, a) da ds
ψ(s, a)
θ
θ
s∈S a∈A
s∈S a∈A
 Z Z

= arg min −
ψ(s, a) ln πθ (a | s) da ds ,
θ
s∈S a∈A

which is the weighted maximum likelihood policy loss as in (36), that we compute on sampled transitions, effectively
assigning out-of-sample transitions a weight of zero.
A useful prior ρ(θ) in Equation (40) is to keep the new policy close to the previous one as in TRPO and PPO. This
translates to


ρ(θ) ≈ −νES∼dπold DKL (πold (· | S)∥πθ (· | S)) .
Since optimizing the resulting sample-based maximum likelihood objective directly tends to result in overfitting, this
prior is instead transformed into a constraint on the KL-divergence with bound εν , i.e.
 Z Z

pθ (I = 1, s, a)
da ds
arg min
−
ψ(s, a) ln
ψ(s, a)
θ
s∈S a∈A
h
i
subject to ES∼dπold DKL πold (· | S) ∥ πθ (· | S) ≤ εν .
To employ gradient-based optimization, we use Lagrangian relaxation to transform this constraint optimization problem
back into the unconstrained problem


J (θ, ν) = Lπ (θ) + ν εν − ES∼dπold DKL (πold (· | S)∥πθ (· | S)) .
(45)
This problem is solved by alternating between optimizing for θ and ν via gradient descent in a coordinate-descent
strategy. Using the stop-gradient operator sg[[·]], the objective can equivalently to this strategy be rewritten for as

 hh

h
 
i
ii
Lν (θ, ν) = ν εν − ES∼dπold sg DKL πθold (· | S) ∥ πθ (· | S)
+ sg ν ES∼dπold DKL πθold (· | S) ∥ πθ (· | S) .
Sampling this gives Equation (38). η and ν are Lagrangian multipliers and hence must be positive. We enforce this by
projecting the computed values to small positive values ηmin and νmin respectively if necessary.

D

Auxiliary Theory

Here, we list a range of well-known definitions and results that we use in our work.
Definition D.1. (Compact Space) A topological space X is called compact if for every set S of open covers of X, there
exists a finite subset S ′ ⊂ S that also is an open cover of X.
S
Theorem D.2. (Bayes’ Theorem) Let (Ω, A, P) be a probability space and i∈I Bi be a disjoint and finite partition of
Ω with Bi ∈ A and P(Bi ) > 0 for i ∈ I. Then, for all A ∈ A and all k ∈ I
P(A | Bk )P(Bk )
.
i∈I P(A | Bi )P(Bi )

P(Bk | A) = P

Theorem D.3. Let X be a random variable. Then,
 
 2
Var[X] = E X 2 − E X .

Definition D.4. (Entropy) Let (Ω, A, P) be a probability space and X ∼ P be a random variable. The entropy of X is
given by


H(X) := EX∼P − ln P(X) .
46

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Definition D.5. (Kullback-Leibler Divergence) For any measurable space A and probability densities p and q of the
respective distributions P and Q, the Kullback-Leibler divergence or relative entropy from Q to P is given by
Z
p(a)
DKL (p∥q) :=
p(a) ln
da.
q(a)
a∈A

Definition D.6. (Total Variation Divergence) For any measurable space A and probability densities p and q of the
respective distributions P and Q, the total variation variance from Q to P is given by
Z
1
DT V (p∥q) :=
p(a) − q(a)da.
2
a∈A

Theorem D.7. Let (Ω, A) be a measurable space and p and ψ be probability measures on that space. Let and X ∈ A
and Z ∈ A. Then,


p(X, Z)
ln p(X) = EZ∼ψ ln
+ DKL (ψ ∥ p(· | X)).
ψ(Z)
Theorem D.8. Let X be a random variable. Then,


min E (X − a)2 = E[X].
a

Theorem D.9. Let (A, Σ) be a measurable space with σ-finite measures
R µ and ν such that ν is absolutely continuous
in µ. Let g be a Radon-Nikodym derivative of ν w.r.t. µ, i.e. ν(A) = A g dµ for all A ∈ Σ. Let, f be a ν-integrable
function. Then,
Z
Z
(f · g) dµ.

f dν =
A

A

Theorem D.10. (Leibniz Integral Rule) Let X be an open subset of Rd , d ∈ N. Let A be a measurable set and
f : X × A → R be a function which satisfies
1. f (x, a) is a Lebesgue-integrable function of a for all x ∈ X.
2. For almost all a ∈ A, all partial derivatives exist for all x ∈ X.
3. There exists some integrable function g : A → R with |∇x f (x, a)| ≤ g(a) for all x ∈ X and almost all
a ∈ A.
Then, for all x ∈ X we have
Z
∇x

Z
∇x f (x, a)da

f (x, a)da =

a∈A

a∈A

Theorem D.11. (Fubini’s Theorem) Let A1 and A2 be measurable spacesRwith measures µ1 and µ2 and f : A1 ×A2 →
R be measurable and integrable w.r.t. the product measure µ1 ⊗ µ2 , i.e. A1 ×A2 |f | d(µ1 ⊗ µ2 ) < ∞ or f ≥ 0 almost
everywhere. Then, f (x, y) is integrable for almost all x and y and
Z Z
Z Z
f (x, y) dµ1 (x) dµ2 (y) =
f (x, y) dµ2 (y) dµ1 (x)
A1 A2

A2 A1

47

The Definitive Guide to Policy Gradients in Deep Reinforcement Learning

Theorem D.12. (Taylor’s Theorem - one-dimensional) Let k ∈ N and let f : R → R be k-times differentiable at a ∈ R.
Then, there exists a function hk : R → R such that
f (x) =

k
X
f (i) (a)
i=0

i!

(x − a)i + hk (x)(x − a)k .

∞
Theorem D.13. (Monotone Convergence Theorem) Let xn n=0 ⊂ R be a bounded and monotonically increasing
sequence. Then, the sequence converges, i.e. limn→∞ xn exists and is finite.
∞
Theorem D.14. (Bolzano-Weierstrass Theorem) Let xn n=0 ⊂ Rd , d ∈ N be a bounded sequence. Then, there exists
∞
some convergent subsequence xni i=0 .
Theorem D.15. (Berge’s Maximum Theorem) Let X and Θ be topological spaces, f : X × Θ → R be continuous on
X × Θ and C : Θ ⇒ X be a compact-valued correspondence with C(θ) ̸= ∅ for all θ ∈ Θ. Let

f ∗ (θ) = sup f (x, θ) | x ∈ C(θ)
and



C ∗ (θ) = arg max f (x, θ) | x ∈ C(θ) = x ∈ C(θ) | f (x, θ) = f ∗ (θ) .
If C is continuous at θ, then f ∗ is continuous and C ∗ is upper hemicontinuous with nonempty and compact values.
Definition D.16. (Gâteaux Derivative) Let X and Y be locally convex topological spaces, let U be an open subset of
X and F : U → Y . The Gâteaux derivative of F at x ∈ U in the direction d ∈ X is defined as
dF (x, d) = lim

h→0

F (x + rd) − F (x)
.
r

48

