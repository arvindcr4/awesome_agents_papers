Decentralizing Multi-Agent Reinforcement
Learning with Temporal Causal Information⋆
Jan Corazza1 (

arXiv:2506.07829v2 [cs.LG] 14 Oct 2025

1

), Hadi Partovi Aria2 , Hyohun Kim2 , Daniel Neider1 , and Zhe
Xu2

Research Center Trustworthy Data Science and Security of the University Alliance
Ruhr, Department of Computer Science, TU Dortmund University, Germany
{jan.corazza,daniel.neider}@tu-dortmund.de
2
Arizona State University, USA {hpartovi, hkim450, xzhe1}@asu.edu

Abstract. Reinforcement learning (RL) algorithms can find an optimal
policy for a single agent to accomplish a particular task. However, many
real-world problems require multiple agents to collaborate in order to
achieve a common goal. For example, a robot executing a task in a
warehouse may require the assistance of a drone to retrieve items from
high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn
independently and then combine their policies at execution time, but
often must satisfy constraints on compatibility of local policies to ensure
that they can achieve the global task when combined. In this paper, we
study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we
extend the formal tools used to check the compatibility of local policies
with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate
that symbolic knowledge about the temporal evolution of events in the
environment can significantly expedite the learning process in DMARL.
Keywords: Temporal Causality · Multi-Agent Reinforcement Learning ·
Reward Machines · Formal Methods.

1

Introduction

One approach to solving a multi-agent problem is to learn a centralized policy that
controls all agents simultaneously. Such a centralized controller is conceptually
straightforward, but realizing it is often impractical [6] if the number of agents
is large or there is an imposed need for decentralization. As the number of
agents increases, the state space grows exponentially, making it less tractable to
learn a centralized policy. Decentralization is imposed when agents are physically
separated, communication is limited, or privacy is a concern. Centralized policies,
by contrast, often assume seamless communication, which is unrealistic in many
⋆

Code available at https://github.com/corazza/tcdmarl

2

Jan Corazza et al.

real-world scenarios. This challenge is further exacerbated by sparse reward
signals with temporal dependencies, inherent in many real-world tasks.
Instead of learning a centralized policy, agents can learn decentralized policies
that allow them to act independently and reduce the need for communication.
These independent policies are then combined at execution time to solve the
team task. To this end, Neary et al. [7] propose a DMARL algorithm called
Decentralized Q-learning with Projected Reward Machines (DQPRM). DQPRM
decomposes a global team task specification into a set of local task specifications, one for each individual agent. Agents are trained independently to learn
policies based on their local task specifications. To ensure that the agents’ local
policies, when combined, satisfy the overall team task, DQPRM enforces strict
compatibility criteria between the local and team specifications. In Section 3,
we provide a brief overview of DQPRM.
Reward machines (RMs, formalized in Definition 1) are deterministic, finitestate automata that transduce sequences of relevant high-level events (labels)
into sequences of rewards, thereby capturing the temporal nature of a sparse
reward signal, and serving as specifications for RL tasks. Reward machines have
been widely studied in literature [5,11,3,8,4,2], but the primary focus has been on
the single-agent case. In multi-agent problems, communication limitations mean
individual agents cannot sense all events, preventing them from accessing the full
team state. DQPRM frames this limitation in terms of projected (local) RMs,
that capture a particular agent’s contribution to the team goal (with respect to
events that the agent can sense).



Æ
(2)

(G)

u1

P

D

uI

D

ð

(D)

Æ
(1)

(a) Environment

u3

(P )

G

u5

P

u2

G

u4

(b) Team Reward Machine

Fig. 1: Generator Task

To illustrate the limitations of DQPRM’s strict compatibility criteria, we will
introduce a running example called the Generator Task, depicted in Figure 1. In
this task, two agents move on a gridworld (Figure 1a), and must collaborate to
power a generator during a flooding incident.
Agent 1’s task is to prevent the flooding by closing the pipe (ð, P ), and to
unlock the door (, D) for agent 2 (in any order). The task for agent 2 is to
wait for the door to be unlocked (), i.e., for the yellow barrier to disappear,

Decentralized MARL with Temporal Causality

3

and then power the generator (, G). The team reward machine, depicted in
Figure 1b, also specifies a failure condition: if agent 2 is let into the room and
powers the generator before the flooding has been resolved, the team RM will
enter an unsafe state, u4 , from which no further actions can result in a positive
reward. The agents obtain a reward of 1 and end the task if their joint actions
transition the team RM to into an accepting state, u5 , and 0 in all steps prior.
The challenge in this example arises from the agents’ limited communication
about the task’s status. In particular, agent 1 does not communicate whether it
is safe to power up the generator (i.e., whether event ð has happened). In other
words, agent 2 does not know whether the team is collectively in state u2 ( is
unsafe) or u3 ( is safe and completes the task). Although both agents have an
individual strategy that solves the team task, this specification fails to meet the
requirements of DQPRM, which strictly prohibit outcomes to depend on local
event synchronization.
Recent work in causal reinforcement learning [8,3] proposes methods that
leverage causal knowledge of the long-term temporal evolution of high-level
environmental events to improve the exploration-exploitation trade-off. Paliwal et
al. [8] introduce Temporal Logic-based Causal Diagrams (TL-CDs), a formalism
that captures such temporal-causal knowledge by enabling experts to specify
causal relationships between LTLf formulas. During learning, temporal-causal
knowledge expressed in TL-CDs is used to predict whether further exploration
within a given episode is necessary and, if not, to short-circuit the exploration
process. Unfortunately, this method lacks rigorous theoretical guarantees. Corazza
et al. [3] extend this line of work with a provably correct method that directly
integrates temporal-causal knowledge from TL-CDs into the reward machine.
Ultimately, both methods expedite convergence to optimal policies by exploiting
knowledge of temporal causality—i.e., the long-term evolution of high-level events
within the MDP. In Section 4, we provide a brief overview of TL-CDs and their
integration with reward machines.
Our paper connects these two lines of work (decentralized MARL and temporalcausal RL). In Section 5, we propose a method to exploit temporal-causal
knowledge about the task at hand in order to (1) relax the theoretical compatibility
criteria of DQPRM, broadening the scope of multi-agent tasks that can be
decomposed and learned in a decentralized manner; and (2) short-circuit certain
explorations during decentralized learning, improving the sample efficiency of the
learning process even further. Moreover, we prove that our relaxed compatibility
criterion is consistent with the original one, in the sense that passing the original
criterion implies passing the relaxed one. In Section 6, we empirically demonstrate
our method’s effectiveness on the Generator Task and introduce the Laboratory
Task, presenting experimental results for both. An analysis of a third case study,
the Buttons Task, is provided in Appendix C.

4

2

Jan Corazza et al.

Preliminaries

Aside from acting as a task specification, reward machines provide a form of
finite memory for a team or an agent, tracking progress through the task. We
assume that during the agents’ interaction with the MDP, relevant high-level
events are labeled. Reward machines use these event labels as inputs to transition
between states and determine the appropriate rewards. By modeling the temporal
dependencies within the task, RMs guide agents through complex environments,
especially in situations where rewards are sparse. We first introduce the general
definitions of Event-based Reward Machines (RMs) and RM-based Markov
Decision Processes (RM-MDPs), which provide a foundational framework for
handling task specification and reward functions. In Section 3, we will extend this
formalism to the multi-agent setting, where decentralized coordination becomes
crucial.
Definition 1 (Event-based Reward Machine). An event-based RM R =
⟨U, uI , Σ, δ, σ, F ⟩ is a tuple where U is a finite set of states with an initial state
uI ∈ U , Σ is the set of events, δ : U × Σ → U is a (partial) transition function,
σ : U × U → R is a (partial) function mapping transitions to rewards, and F ⊆ U
is a set of terminal states that signal the end of the interaction.
When the RM is in state u ∈ U and reads an event e ∈ Σ such that
(u, e) ∈ Dom(δ), it transitions into a new state u′ = δ(u, e) and outputs a reward
σ(u, u′ ). Otherwise, the RM remains in the same state and outputs a reward of 0.
For finite event sequences, we define δ(u, ϵ) = u and δ(u, ξe) = δ(δ(u, ξ), e), where
ϵ is the empty sequence, ξ ∈ (Σ)∗ , and e ∈ Σ. Note that δ and σ are partial
functions, i.e., we do not require the transition function to be defined for every
input event from every state. The term event-based RM is a deliberate departure
from prior work [7], which conflated event-driven RMs with propositional RMs
under the generic “RM” label. We focus on task completion RMs which output a
reward of 1 upon reaching a terminal state u ∈ F , and 0 otherwise. For brevity, we
refer to task completion event-based reward machines simply as reward machines
throughout the remainder of this work.
To connect an RM with the MDP (with set of states S), we use a labeling
function L : S × U → 2Σ . The codomain of L is 2Σ to model the possibility
that certain events may occur concurrently. We restrict the labeling function so
that only one event may occur per step, per agent.3 Because of this restriction
and the compatibility criterion defined in Section 3, the order an RM reads
simultaneous events does not matter. In other words, if an RM reads an event
sequence ξ ∈ Perm(L(s, u, s′ )), the resulting state u′ = δ(u, ξ) is well-defined
(independent of the permutation of ξ).
Definition 2 (RM-MDP). A Reward Machine-based Markov Decision Process
is a tuple M = ⟨S, sI , A, p, γ, R, L⟩ consisting of a finite state space S, an
3

We explain this assumption in Section 3, where we provide background for decentralized MARL.

Decentralized MARL with Temporal Causality

5

initial state sI ∈ S, a finite set of actions A, a probabilistic transition function
p : S × U × A × S → [0, 1], a discount factor γ ∈ (0, 1], a reward machine R
which captures the reward function, and a labeling function L : S × U × S → 2Σ .
Note that the transition probabilities p(s′ | s, u, a) are conditioned on the state of
the reward machine u ∈ U .
Intuitively, if the MDP and the RM are in a joint state (s, u) ∈ S × U , and the
agent chooses an action a ∈ A, the MDP transitions to a new state s′ ∼ p(s, u, a).
The labeling function then outputs a set of simultaneous events L(s, u, s′ ), which
the RM reads in any order, and transitions into u′ = δ(u, L(s, u, s′ )).
We say that a trajectory s0 u0 a0 · · · an−1 sn un is attainable within an RMMDP M = ⟨S, sI , A, p, γ, R, L⟩ when (s0 , u0 ) = (sI , uI ), and for all i = 0, 1, . . . ,
n − 1 we have p(si , ui , ai , si+1 ) > 0 and ui+1 = δ(ui , L(si , ui , si+1 )). An event
sequence ξ = e1 . . . en is attainable if it can be obtained using a finite number of
concatenation and swap operations from the label sequence ℓi = L(si , ui , si+1 )
of an attainable trace.
The reward function of an RM-MDP is induced by the reward machine R via
R((s, u), a, (s′ , u′ )) = σ(u, u′ ). Because R is Markovian over the product space
S × U , one may use Q-learning to find an optimal policy in an RM-MDP.

3

Decentralized Multi-Agent Reinforcement Learning

In this section, we outline the baseline DQPRM algorithm proposed by Neary
et al. [7], including its assumptions and theoretical guarantees. In decentralized
MARL with reward machines, the overall team task is specified by an RM R
defined over a global set of events Σ (all events that may occur in the MDP).
However, not every agent can observe all events from Σ. In order to capture this
limitation, we define a local event set Σi for each
SN agent i = 1, . . . , N , where N is
the number of agents. We assume that Σ = i=1 Σi .
In the Generator Task from Figure 1, the local event set of agent 1 is
Σ1 = {P, D}, modeling the capabilities to fix the pipe (observe ð) and open the
door (observe ). The local event set of agent 2 is Σ2 = {D, G}, modeling the
capabilities to observe the door being unlocked () and power the generator ().
As demonstrated by the event D, local event sets may overlap, i.e., there may be
events that can be sensed by more than one agent. We call such events shared
events, and write Ie = {i | e ∈ Σi } for the set of agents that share event e. When
|Ie | > 1 for some e ∈ Σ, a synchronization mechanism ought to be simulated
during decentralized training episodes. For example, since the event D is under
the control of agent 1, during decentralized training of agent 2, we simulate the
communication of this event with probability p > 0 at every time-step.
In decentralized MARL, we aim to train each agent independently, so as to
avoid learning a centralized policy. The primary benefit of this approach is sample
efficiency, as centralized policies induce a state space that grows exponentially with
the number of agents. Other benefits include privacy (limiting shared knowledge),
and the fact that centralized policies may be infeasible due to practical concerns
(differing capabilities, limited communication).

6

Jan Corazza et al.

To facilitate decentralized training, we first need to capture the individual
contribution of each agent towards the team goal in terms of the agent’s local
event set. This can be achieved with the notion of Projected Reward Machines,
which we will now introduce. We also refer to projected reward machines as local
RMs, to highlight that the projection is defined in terms of the local event set.
To define the projection of the team RM R = ⟨U, uI , Σ, δ, σ, F ⟩ along a local
event set Σi ⊆ Σ, we first introduce an equivalence relation ∼i over states of R.
Informally, two states u1 , u2 ∈ U are ∼i -equivalent if agent i can not distinguish
between them based on events in Σi . Formally, ∼i is the smallest equivalence
relation such that for all u1 , u2 , u′1 , u′2 ∈ U we have (1) u1 ∼i u2 if there exists
an event e ∈ Σ \ Σi such that u2 = δ(u1 , e); and (2) if u1 ∼i u′1 and there exists
an event e ∈ Σi such that δ(u1 , e) = u2 and δ(u′1 , e) = u′2 , then u2 ∼i u′2 . We
denote the set of equivalence classes of ∼i with U/ ∼i , and we will write [u]i for
the ∼i -equivalence class of u ∈ U .
Condition (1) ensures that two states of R are indistinguishable for agent i
if a transition between them is triggered by an event outside of Σi . Condition
(2) (congruence) ensures that an event e ∈ Σi may only trigger transitions to a
unique successor equivalence class. An algorithm to compute this equivalence
relation with runtime O(|U |7 |Σi |2 + |U |5 |Σi | + |U |4 ) is provided in appendix C of
Wong [10]. Using ∼i , we formalize the notion of a projected RM in Definition 3.
Figure 2 depicts the projections of the team RM from the Generator Task along
the local event sets of agents 1 and 2.
Definition 3 (Projected Reward Machine). Given a reward machine R =
⟨U, uI , Σ, δ, σ, F ⟩ and local event set Σi , the projection of R along Σi is the
RM Ri = (Ui , uiI , δi , Σi , Fi ) where Ui = U/ ∼i , uiI = [uI ]i , δi : Ui × Σi → Ui
is defined as ui2 = δi (ui1 , e) for e ∈ Σi if and only if there exist u1 , u2 ∈ U
such that [u1 ] = ui1 , [u2 ] = ui2 and u2 = δ(u1 , e) (and undefined otherwise),
Fi = {ui ∈ Ui | ∃u ∈ F, ui = [u]i }, and σi (ui1 , ui2 ) = 1ui2 ∈Fi ∧ui1 ̸∈Fi .

(1)

P

u1

(1)

(1)

uI

D

D
u3

(1)

P

u2

(a) Σ1 = {P, D}

(2)

uI

D

(2)

u1

G

(2)

u2

(b) Σ2 = {D, G}

Fig. 2: Projections of the RM from Figure 1b along local event sets of agents 1
(Left) and 2 (Right).
We model a multi-agent system with N agents as an RM-MDP M =
⟨S, sI , AN , p, γ, R, L⟩ where the state space S is a Cartesian product of local
state spaces, S = S1 × · · · × SN , the action space AN is a product of individual action spaces (the same for each agent) A × · · · × A, and the transition

Decentralized MARL with Temporal Causality

7

function p : S × U × AN × S can be decomposed as a product p(s, u, a, s′ ) =
QN
i
′
i
i=1 pi (si , u , ai , si ), where u = [u]i is the projection of the team state from R
to the local RM state of agent i. The dynamics pi of agent i depend only on the
agent’s individual state and action, (si , ai ), and the events that the agent can
sense (i.e., the state of its local RM Ri ).
To fully capture the local dynamics of each agent, DQPRM relies on the notion
of local labeling functions Li : Si × Ui × Si → 2Σ , where Si is the local state space
of agent i, and Ui = U/ ∼i are the states of its local RM. Intuitively, Li (si , ui , s′i )
outputs events that could be occurring from the perspective of agent i, given that
the rest of the team state is unknown. We note that local labeling functions must
output at most a single event {e} (which corresponds to the assumption that
only one event may occur per agent per time step), and that L(s, u, s′ ) outputs
event e if and only if Li (si , ui , s′i ) output e, for all i ∈ Ie . We call such label
functions decomposable with respect to Σ1 , . . . , ΣN (or just decomposable if the
4
family {Σi }N
We define the projection of a finite
i=1 is clear from the context).
∗
event sequence ξ ∈ Σ onto the local event set Σi as Pi (ξ ′ ℓ) = Pi (ξ ′ )e if ξ = ξ ′ e
and e ∈ Σi , and Pi (ξe) = Pi (ξ) otherwise (for the empty word ϵ, Pi (ϵ) = ϵ for all
i = 1, . . . , N ).
A key requirement for the task decomposition to be successful is that policies
induced by projected RMs must combine to form a policy that satisfies the team
RM. This requirement is satisfied if the team RM is bisimilar to the parallel
composition of its projections, which we refer to as the strict decomposition
criterion. Intuitively, two reward machines R and P are bisimilar (R ∼
= P) if
there exists a relation between their states such that the transition behavior of
related states is the same (initial and final states must be related). The parallel
composition of two RMs R1 and R2 is an RM R1 ∥R2 whose states are the
Cartesian product of states of R and P, initial (terminal) states are pairs of
initial (terminal) states, and transitions are defined component-wise.5 Using the
notions of bisimilarity and parallel composition of RMs, we can formalize the
strict decomposition criterion for the team RM R and local RMs R1 , . . . , RN . The
criterion holds if R ∼
= ∥N
i=1 Ri . In that case, decentralized training of local policies
will yield a combined policy with guaranteed bounds on the probability of solving
the team task. By definition, if local RMs Ri output 1 (for all i = 1, . . . , N ), then
their parallel composition also outputs 1. And because their parallel composition
is bisimilar to the team RM, the team task is satisfied as well. The converse
also holds: if the team RM outputs a reward, then each local RM will output a
reward. This is formalized in Theorem 1 [7].
Theorem 1 (Strict Decomposition Criterion). If RM R and projections
R1 , . . . , RN satisfy the strict decomposition criterion, then ∀ξ ∈ Σ ∗ , R(ξ) = 1 if
and only if Ri (Pi (ξ)) = 1, ∀i = 1, . . . , N . Here, Pi (ξ) is the projection of ξ to Σi
defined earlier.
4
5

For more details on constructing local labeling functions see Neary et al. [7]
Formal definitions in Appendix A. Visual explanation in Appendix B.2.

8

Jan Corazza et al.

We provide the pseudocode for DQPRM in Algorithm 1. In brief, DQPRM
runs N concurrent episodes, where each agent moves independently, receiving
observations, events, and rewards decoupled from the state of the other N − 1
agents. When acting in a team setting during testing, agents must synchronize
on shared events. This is the only communication channel necessary to execute
single-agent policies, found by DQPRM, in a multi-agent RM-MDP. When the
local labeling function Li outputs a shared event e, synchronization ensures that
Ri reads e iff. Lj outputs the same event e for all j ∈ Ie . To account for this fact
during decentralized training, DQPRM simulates the synchronization signal with
probability p > 0 (Line 18). We summarize the guarantees for team performance
in Theorem 2.

Algorithm 1: Decentralized training with DQPRM
Input: Team RM R, local event sets Σ1 , . . . , ΣN , local labeling functions
L1 , . . . , LN
Output: Policies π1 , . . . , πN
1 Project R along local event sets Σi to obtain projected RMs R1 , . . . , RN ;
2 if R ∼
̸ R1 ∥ · · · ∥RN then
=
3
Reject: the strict decomposition criterion does not hold;
4 end
5 Initialize(π1 , . . . , πN );
6 for m = 1 to numEpisodes do
7
for i = 1 to N do
8
si ← siI , ui ← uiI ;
9
end
10
for t = 1 to numSteps do
11
for i = 1 to N do
12
if TaskCompletei (ui ) then
13
continue;
14
end
15
ai ← Sample(πi (si , ui ));
16
s′ ← Stepi (si , ui , ai ), u′ ← ui , ri ← 0;
// Li (si , ui , s′ ) is either ∅ or {e} ∈ Σi
17
for e ∈ Li (si , ui , s′ ) do
18
if |Ie | = 1 or Rand(0, 1) ≤ p then
19
u′ ← δ(ui , e); ri ← ri + σ(ui , u′ ); ui ← u′ ;
20
end
21
end
22
PolicyUpdate(πi , si , ui , ai , s′ , ri );
23
si ← s ′ ;
24
end
25
end
26 end
27 return (π1 , . . . , πN );

Decentralized MARL with Temporal Causality

9

Theorem 2 (Decomposition Viability). Given a team RM R, local event
sets Σ1 , . . . , ΣN , a decomposable label function L, and local labeling functions
L1 , . . . , LN , assume that agents synchronize on shared events and R ∼
= ∥N
i=1 Ri .
i i
Then for all team trajectories s0 u0 · · · sk uk and local trajectories {s0 u0 · · · sik uik }N
i=1 ,
we have R(L(s0 u0 · · · sk uk )) = 1 if and only if Ri (Li (si0 ui0 · · · sik uik )) = 1 for all
i = 1, . . . , N . Furthermore, let V π (sI ) denote the team success probability and
Viπ (siI ) the success probability of agent i. Then max{0, V1π (sI ) + · · · + VNπ (sI ) −
(N − 1)} ≤ V π (sI ) ≤ min{V1π (sI ), . . . , V π (sI )}.
Formal proofs of Theorems 1 and 2 can be found in Neary et al. [7]. Informally,
Theorem 2 states that if a task specification respects the strict decomposition criterion, decentralized training will yield local agent policies which, when combined,
satisfy the team task (depending on the probability that each agents performs
its own part of the task). This result depends critically on Theorem 1, which
guarantees a correspondence between solving local tasks and solving the team
task, and the assumption that the agents synchronize on shared events.

4

Modeling Temporal-Causal Knowledge with TL-CDs

In this section, we lay the groundwork for modeling temporal-causal knowledge
in RM-MDPs, and extend the notion of Temporal Logic-based Causal Diagrams
(TL-CDs), proposed in Paliwal et al. [8], to Event-based Reward Machines. Linear
temporal logic over finite sequences (LTLf ) is a formal reasoning system that
can capture causal and temporal properties of event sequences and RM-MDPs.
Aside from Boolean operators like ¬ and ∨, LTLf introduces temporal operators
such as Gψ (true if and only if ψ holds for every element in the sequence), Xψ
(true iff. ψ holds for the next element of the sequence), and ψUφ (true iff. ψ
holds until φ becomes true, and φ is true in some element of the sequence). We
also rely on the weak until operator ψWφ (true iff. ψ holds until φ becomes true,
but φ is not required to become true).
TL-CDs are directed graphs where nodes are labeled with LTLf formulas
and edges represent causal relationships between them, providing a structured
notation to express temporal-causal knowledge. To give the semantics of TLCD C, one can construct an equivalent LTLf formula, φC , which captures the
temporal-causal knowledge encoded in the graph. The formula induced by the
TL-CD in Figure 3a, G(D → G(¬XP )), models sequences in which the event P
never follows the event D. In other words, it captures the effect of the one-way
ramp (represented by blue arrows ← in Figure 1a), which blocks agent 1 from
returning to the region of the MDP containing the pipe (ð) after unlocking the
door ().
To capture the semantics of a given TL-CD C over event sequences in Σ ∗ , we
C
C
construct an equivalent LTLf formula, ΨΣ
, as the conjunction ΨΣ
≡ φC ∧ψΣ . The
C
formula φ
V encodes the temporal-causal knowledge expressed in C, and is defined
as φC ≡ φ▶ψ G(φ → ψ), where φ ▶ ψ iterates overWedges that connect
formulas
V
φ and ψ in the TL-CD. The formula ψΣ = G( e∈Σ (ei ∧ ( e′ ∈Σ\{e} ¬e′ )))

10

Jan Corazza et al.
¬D
D

G(¬XP )

(a) φC ≡ G(D → G(¬XP ))

qI

¬P
D

q1

⊤
P

q2

(b) DFA constructed from φC

Fig. 3: TL-CD for the Generator Task (Left) and respective Causal DFA (Right)

C
C
restricts models of ΨΣ
to event sequences in Σ ∗ . If ΨΣ
is true for an event
C
sequence ξ = e1 . . . en , we will write ξ |= ΨΣ . We will say that a TL-CD C holds
C
for an MDP M if for every event sequence ξ attainable in M, we have ξ |= ΨΣ
. In
order to simplify working with TL-CDs, we leverage the notion of deterministic
finite automata (DFAs), formalized in Definition 4.

Definition 4 (Deterministic Finite Automaton). A DFA is a tuple C =
(Q, qI , Σ, δ, F ) consisting of a finite set of states Q with an initial state qI , input
alphabet Σ, deterministic transition function δ : Q × Σ → Q, and a set of
accepting states F ⊆ Q.
If the run of the DFA C on an input string ξ ends in an accepting state q ∈ F ,
we will write ξ ∈ L(C). Every TL-CD C can be converted into an equivalent DFA
C
C [8], in the sense that for every ξ, we have ξ ∈ L(C) ⇐⇒ ξ |= ΨΣ
. We will refer
to C as the causal DFA. When illustrating causal DFAs, we will only consider
the fragment constructed from φC , as the contribution of ψΣ is always the same
and serves a technical purpose. One can obtain the full causal DFA from the
parallel composition of automata for φC and ψΣ . In Figure 3b, we illustrate the
DFA induced by the TL-CD which holds for the Generator Task.

5

Causal DQPRM

We enhance decentralized multi-agent RL by integrating temporal-causal knowledge through TL-CDs in two key ways. First, we enable valid decomposition of
tasks rejected by DQPRM through causal constraints, preserving performance
guarantees. Second, we accelerate learning by short-circuiting redundant exploration using TL-CD predictions, significantly improving sample efficiency in
decentralized training.
5.1

Relaxing the Decomposition Criterion in DQPRM

The main assumption of the method proposed in Neary et al. [7] is that the
team RM is bisimilar to the parallel composition of the projected RMs. As
demonstrated by the decomposition in Figure 2, this assumption is not always
satisfied. To illustrate this point, consider the event sequence ξ = DGP , which
induces a reward of 1 in both projections, but 0 in the team RM. Unfortunately,
this means that Theorem 2 no longer guarantees a viable decomposition into

Decentralized MARL with Temporal Causality

11

local RMs. As such, one can not expect DQPRM to solve the Generator Task
(at least not before results analogous to Theorem 1 are shown to hold).
On the other hand, by constructing a strategy for each agent manually, one
can see that decentralized learning ought to yield satisfactory results. The reason
for this lies in the fact that ð (P ) and  (D) are separated by a one-way
ramp (Figure 1a). Once agent 1 visits , then due to this one-way ramp, no
attainable trajectory leads to ð. In other words, while agent 1 satisfies its local
task (Figure 2a) by observing P and D in any order, the structure of the MDP
does not permit orders of the form P → · · · → D. Therefore, Q-learning, equipped
with knowledge of the current RM state of agent 1, ought to converge to a policy
which avoids such paths.
As TL-CDs overapproximate the set of attainable event sequences within the
MDP, they enable experts to encode temporal-causal knowledge—for instance,
the fact that problematic sequences like ξ = DGP are unattainable. Given a
team task RM R and local event sets Σ1 , . . . , ΣN where R ∼
̸ ∥N
=
i=1 Ri , our method
leverages TL-CD C to validate whether decomposition remains viable under the
causal constraints of the RM-MDP.
To relax the strict bisimilarity assumption (R ∼
= ∥N
i=1 Ri ) from Theorem 1,
we introduce a modified bisimulation check: R∥C ∼
= P∥C, where P = ∥N
i=1 Ri
represents the parallel composition of projected RMs, and R∥C, P∥C denote
their respective compositions with the causal DFA C derived from C.
The parallel composition of an RM and a causal DFA synchronizes their
transitions, meaning that R∥C and P∥C do not reward event sequences which do
not satisfy the TL-CD C. In Theorem 3 we show that our relaxed decomposition
criterion yields the same guarantees as the strict one, and in Theorem 4 we show
that the two are compatible (provided C holds for the RM-MDP).
As R and P are both deterministic finite automata, if R ∼
̸ P, the issue is
=
caused by event sequences ξ such that R(ξ) ̸= P(ξ) [1]. At its core, our method
allows experts to exploit temporal-causal knowledge by finding a TL-CD C such
that (1) C holds for the RM-MDP, i.e., for every event sequence ξ attainable in
C
M, we have ξ |= ΨΣ
; and (2) R, P, and the causal DFA C of the TL-CD C pass
the relaxed decomposition criterion, i.e. R∥C ∼
= P∥C.
The first property ensures that the TL-CD overapproximates the RM-MDP
and expresses correct causal knowledge about the environment. The second
property ensures that the causal knowledge added by the TL-CD is complete,
in the sense that for every event sequence ξ such that P(ξ) ̸= R(ξ), we have
C
ξ ̸|= ΨΣ
(i.e. ξ is not an attainable event sequence in the RM-MDP). We use
these properties to recover the important result that underpins team performance
guarantees, and formalize our relaxed decomposition criterion in Theorem 3.
In Theorem 4, we show that the relaxed and strict decomposition criteria are
compatible, and in Theorem 5 we provide a lower and upper bound on the team
success probability, analogous to Theorem 2. The proofs for the theorems in this
section are provided in Appendix B.
Theorem 3 (Relaxed Decomposition Criterion). Let R be the team task
RM, and R1 , . . . , RN be the projections of R onto the local event sets Σ1 , . . . , ΣN ,

12

Jan Corazza et al.

respectively. If C is a TL-CD with causal DFA C such that R∥C ∼
= (∥N
i=1 Ri )∥C,
then for all M-attainable event sequences ξ, we have R(ξ) = 1 if and only if
Ri (Pi (ξ)) = 1 (∀i = 1, . . . , N ). Proof in Appendix B.1.
Theorem 3 states that if one can find a TL-CD C which satisfies the two
properties outlined above, then the team task RM can be decomposed into local
task RMs such that on all attainable sequences, the team RM and the parallel
composition of local RMs output the same reward. Intuitively, if agents 1 through
N satisfy their local tasks (induce a reward of 1 in projected RMs), then their
combined behavior also induces a reward of 1 in the parallel composition of
projected RMs (which is bisimilar to the team RM).
If the strict decomposition criterion holds, it is not immediately clear that
the relaxed criterion will also hold (given an arbitrary TL-CD that holds for the
MDP). In other words, Theorem 1 and Theorem 3 present two different criteria
for decomposing the team task RM (C1 and C2, respectively), and it is natural
to ask whether these criteria are compatible. Table 1 summarizes the possible
outcomes of comparing the two criteria.

C1 C2 Explanation
;
;
✓
✓

; No decomposition (neither met)
✓ New decomposition (our method)
; Claim: Impossible combination
✓ Known decomposition (both met)

Table 1: Results from Theorems 3 & 4

Theorem 4 (Criterion Compatibility). Given a team task RM R, a family of
local event sets Σ1 , . . . , ΣN , and a TL-CD C with causal DFA C, let P = ∥N
i=1 Ri
be the parallel composition of local task DFAs. Then R ∼
= P∥C. In
= P ⇒ R∥C ∼
other words, the additional parallel composition with the causal DFA will not
change the bisimulation decision if the original parallel composition is bisimilar
to the team task DFA. Proof in Appendix B.2.
In Theorem 4 we show that a parallel composition with an appropriate causal
DFA introduced by our method will not make a task RM, that was originally
decomposable, not decomposable. In other words, the relaxed criterion is a
generalization of the original criterion, and so the two criteria are compatible.
Once the relationship between the two criteria has been established, the main
result is given by Theorem 5, which provides lower and upper bounds on the
probability of combined action success, analogous to Theorem 2. In order to
derive the results, we also assume that agents synchronize on shared events in
the team setting, and that the team labeling function L is decomposable with
respect to the local event sets with corresponding labeling functions Li .

Decentralized MARL with Temporal Causality

13

Theorem 5 (Relaxed Decomposition Viability). Given a team RM R,
local event sets Σ1 , . . . , ΣN , a decomposable label function L, and local labeling functions L1 , . . . , LN , assume that agents synchronize on shared events and
R∥C ∼
= (∥N
i=1 Ri )∥C. Then for all team trajectories s0 u0 · · · sk uk and local trajectories {si0 ui0 · · · sik uik }N
i=1 , we have R(L(s0 u0 · · · sk uk )) = 1 if and only if
Ri (Li (si0 ui0 · · · sik uik )) = 1 for all i = 1, . . . , N . Furthermore, we retain the bounds
for the team success probability V π (sI ), i.e. max{0, V1π (sI ) + · · · + VNπ (sI ) − (N −
1)} ≤ V π (sI ) ≤ min{V1π (sI ), . . . , V π (sI )}. Proof in Appendix B.3.
5.2

Expediting RL with Temporal-Causal Knowledge

In the Generator Task from Figure 1, if agent 1 unlocks the door  before fixing
the pipe ð, it will not be able to return and fix the pipe later, because its path is
blocked by a one-way ramp. Unfortunately, agent 1’s projected reward machine,
illustrated in Figure 2a, does not capture this information. Due to this mismatch,
during decentralized training episodes, agent 1 will tend to waste a large portion
of time steps exploring trajectories which do not lead to a positive reward.
As discussed in Section 4, one can use high-level symbolic knowledge in the
form of the TL-CD on Figure 3a to capture this information. While we first
exploited this knowledge to check if the task specification for the Generator Task
is decomposable into local task specifications, we will now use it to expedite
decentralized training for agent 1. Note that the same TL-CD has been applied for
both purposes: this is not always the case. In our second case study, which covers
the Laboratory Task, we use a separate TL-CD to expedite decentralized training
for two agents at once, but a different one to prove that the task specification
passes the relaxed decomposition criterion.
Designing task specifications using reward machines is an error-prone and
time-consuming task, and it is challenging to accommodate all possible scenarios
and causal structures in advance. Moreover, manually updating reward machines
can lead to unintended consequences, ultimately inducing a different optimal
policy. Therefore, we investigate how to extend DQPRM to automatically incorporate temporal-causal knowledge about the environment, and help agents
achieve a better balance exploration and exploitation, without adversely affecting
performance in the original task.
To this end, we adapt the method proposed in Corazza et al. [3] to Eventbased task-completion RMs. The method relies on finding rejecting sink states
in the causal DFA C, i.e., states qr.s. ∈ QC \ F C such that δC (q, e) = q for all
e ∈ Σi . We implicitly consider causal DFAs to have at most one rejecting sink
state qr.s. , and that an accepting state is reachable from all other states. This can
be achieved by minimization [9]. On Figure 3b, that is the state q2 . At its core,
the method exploits the fact that once a label sequence ξ induces a transition
into qr.s. , there is no continuation ξ · ξ ′ that will exit it, and are therefore, all
such label sequences unattainable.
In order to identify explorations that will not be rewarded, the method
computes R̃i , a modification of agent i’s projected RM Ri that induces the
same optimal policy [3], but embeds the temporal-causal knowledge captured

14

Jan Corazza et al.

by C. The states of R̃i are elements of U i × QC (the Cartesian product of
the states of Ri and the causal DFA of C), while the transition function δ˜i is
defined as δ˜i ((u, q), e) = (δi (u, e), δC (q, e)). The reward function σ̃i is defined as
σ̃i ((u, q), (u′ , q ′ )) = σi (u, u′ ) if q ̸= qr.s. , and −1 otherwise. 6
Because only unattainable label sequences (such as DGP in the Generator
Task ) induce transitions into rejecting sink states, setting the reward to −1 when
reaching such states does not affect the optimal policy. The final step relies on
computing the solution to the Bellman optimality equation over states of R̃i ,
V ∗ (u, q) = maxe∈Σi ,u′ =δ̃i (u,e) (σ ((u, q), (u′ , q ′ )) + V ∗ (u′ , q ′ )). In every step, the
maximum possible return from the current episode is bounded from above by
V ∗ (u, q) and 0 from below (by definition of task-completion RMs). If R̃i reaches a
state (u, q) such that V ∗ (u, q) = 0 during decentralized training, then all policies
induce the same return thereon, and the learning can stop.
We summarize our method in Algorithm 2, which we call Causal DQPRM.
The inputs for Causal DQPRM include a TL-CD C which is used to ensure the
task specification passes the relaxed decomposition criterion, along with a family
of TL-CDs C1 , . . . , CN (possibly a different one for each agent). In Algorithm 2,
qi ranges over the states of causal DFA Ci , and (ui , qi ) over the states of R̃i , the
recomputed task specification for agent i. In our experiments, we use p = 0.3.

6

Case Studies

We performed three case studies, validating our approach in the Generator
Task, described extensively throughout this paper, and two new domains: the
Laboratory Task, and the Buttons Task. This section will detail the results on the
Generator Task and the Laboratory Task, while the Buttons Task is described in
Appendix C.

6.1

Case Study 1: Generator Task

Our first case study is an ablation of our approach on the Generator Task,
with results shown in Figure 4. We compare the average steps needed for task
completion per training step. The baseline we compare against is a centralized
controller found with Q-learning, corresponding to the No TL-CD plot on Figure 4,
Centralized.
Adding temporal-causal information improves team performance, even with
centralized training. Decentralized training, enabled by our relaxed decomposition
check, is significantly more efficient, taking an order of magnitude fewer steps to
converge. In our two additional case studies, we perform the same analysis for
the Laboratory Task and the Buttons Task.

Decentralized MARL with Temporal Causality

15

Algorithm 2: Decentralized training with Causal DQPRM
: Team RM R, local event sets Σ1 , . . . , ΣN , local labeling functions
L1 , . . . , LN , TL-CDs C, C1 , . . . , CN
Output : Policies π1 , . . . , πN
1 Project R along local event sets Σi to obtain projected RMs R1 , . . . , RN ;
2 if R∥C ∼
= (∥N
i=1 Ri )∥C then
3
Reject: the relaxed decomposition criterion does not hold;
4 end
5 for i = 1 to N do
6
Compute R̃i via value iteration over Ri ∥Ci ;
7
si ← siI , (ui , qi ) ← (uiI , qIi ), stepsi ← 0;
8 end
9 Initialize(π1 , . . . , πN );
10 for t = 1 to numEpisodes ∗ numSteps do
11
for i = 1 to N do
12
if stepsi > numSteps or Vi∗ (ui , qi ) = 0 then
13
si ← siI , (ui , qi ) ← (uiI , qIi ), stepsi ← 0;
14
end
15
ai ← Sample(πi (si , ui ));
16
s′ ← Stepi (si , ui , ai ), (u′ , q ′ ) ← (ui , qi ), ri ← 0;
17
foreach e ∈ Li (si , ui , s′ ) do
18
if |Ie | = 1 or Rand(0, 1) ≤ p then
19
(u′ , q ′ ) ← δ̃((ui , qi ), e);
20
ri ← ri + σ̃((ui , qi ), u′ );
21
(ui , qi ) ← (u′ , q ′ );
22
end
23
end
24
PolicyUpdate(πi , si , ui , ai , s′ , ri );
25
si ← s′ , stepsi ← stepsi + 1;
26
end
27 end
28 return (π1 , . . . , πN );
Input

6.2

Case Study 2: Laboratory Task

In the Laboratory Task, two agents are tasked with aiding with an accident that
occurred in a laboratory. There are two possible types of accidents that may
have occurred: a fire, represented by \, or a radioactive spill, represented by ³.
Agent 1 is equipped with heat sensors, but not with radiation sensors, vice-versa
for agent 2.
The two accident types are mutually exclusive. Once the agents enter the
conveyor belt, represented by Ó, which leads them to the laboratory, either agent
1 will observe \, or agent 2 will observe ³ (with 50% probability each). Once
6

To justify this we note that, for task-completion RMs, −1 satisfies the reward bounds
computed in Corazza et al. [3].

Jan Corazza et al.
Centralized (Generator)

1,000

400

400

200

200

0

0
2

Training Steps

·1
04

Training Steps

No TL-CD
TL-CD

1.
5

600

·1
05

600

0

800

1

800

1

No TL-CD
TL-CD

5

1,000

0

Steps to Task Completion

Decentralized (Generator)

0.

16

Fig. 4: Generator Task study. Aggregated results from 50 independent runs.
Centralized (Laboratory)

1,000

200

0

0
04
·1

Training Steps

Training Steps

1.
5

200

No TL-CD
TL-CD

06

400

·1

400

1

600

0

600

4

800

2

800

5

No TL-CD
TL-CD

0.

1,000

0

Steps to Task Completion

Decentralized (Laboratory)

Fig. 5: Laboratory Task study. Aggregated results from 50 independent runs.

inside, the agents must, depending on the type of the accident, converge to a
tool which will provide aid (^ in case of fire, and m in case of radiation).
We illustrate the environment for the Laboratory Task in Figure 6a, and the
team reward machine which provides the task specification in Figure 6b. As
shown by the results in Figure 5, Causal DQPRM is able to solve this task, and
leverages temporal-causal information to significantly increase sample efficiency
compared to a centralized controller.
On Figure 7, we illustrate the local reward machines of agents 1 and 2 in the
Laboratory Task.
Our experiments were conducted on a single machine with AMD Ryzen 7
5825U and 6.9G of RAM, except for the centralized case of the Buttons task,
where we used a machine with AMD EPYC 7742 64-Core Processor and 515G of
RAM, due to the extremely large state space.

Decentralized MARL with Temporal Causality

17

^
2

u2

Æ
Ó

1

^

\

\/³

e

Ó

Æ

uI

u4

u1

³

m

m
u3

(a) Environment

(b) Team Reward Machine

Fig. 6: Laboratory Task
(1)

(1)

u2

uI

C

(1)

u1

E

R
(1)

uI

C

(1)

u1

(1)

u3

E

F
M

(1)

u3

(a) Σ1 = {C, F, E, M }

M
(1)

u2

(b) Σ2 = {C, R, E, M }

Fig. 7: Projections of the RM from Figure 6b along local event sets of agents 1
(Left) and 2 (Right).

7

Conclusion

We introduced a framework for integrating temporal-causal knowledge, formalized
via TL-CDs, into decentralized multi-agent reinforcement learning, enabling both
relaxed task decomposition and accelerated policy learning. Experimentally
validated across three case studies, our method improves the applicability of
decentralized training guarantees while achieving higher success rates and sample
efficiency compared to baseline approaches. By automating the incorporation
of expert knowledge into task specifications and learning processes, this work
enables scalable solutions to complex, temporally structured multi-agent tasks.
Acknowledgments. This work was supported in part by the National Science Foundation (NSF) under Grants CNS 2304863 and CNS 2339774, and in part by the Office
of Naval Research (ONR) under Grant N00014-23-1-2505.
Disclosure of Interests. The authors have no competing interests to declare that are
relevant to the content of this article.

References
1. Almeida, M., Moreira, N., Reis, R.: Testing the equivalence of regular languages.
In: Dassow, J., Pighizzini, G., Truthe, B. (eds.) Proceedings Eleventh International
Workshop on Descriptional Complexity of Formal Systems, DCFS 2009, Magdeburg,
Germany, July 6-9, 2009. EPTCS, vol. 3, pp. 47–57 (2009). https://doi.org/10.
4204/EPTCS.3.4, https://doi.org/10.4204/EPTCS.3.4

18

Jan Corazza et al.

2. Azran, G., Danesh, M.H., Albrecht, S.V., Keren, S.: Contextual pre-planning on
reward machine abstractions for enhanced transfer in deep reinforcement learning.
Proceedings of the AAAI Conference on Artificial Intelligence 38(10), 10953–10961
(Mar 2024). https://doi.org/10.1609/aaai.v38i10.28970, https://ojs.aaai.
org/index.php/AAAI/article/view/28970
3. Corazza, J., Aria, H.P., Neider, D., Xu, Z.: Expediting reinforcement learning by
incorporating knowledge about temporal causality in the environment. In: Locatello,
F., Didelez, V. (eds.) Proceedings of the Third Conference on Causal Learning and
Reasoning. Proceedings of Machine Learning Research, vol. 236, pp. 643–664. PMLR
(01–03 Apr 2024), https://proceedings.mlr.press/v236/corazza24a.html
4. Dohmen, T., Topper, N., Atia, G., Beckus, A., Trivedi, A., Velasquez, A.: Inferring probabilistic reward machines from non-markovian reward processes for
reinforcement learning (2022)
5. Icarte, R.T., Klassen, T.Q., Valenzano, R.A., McIlraith, S.A.: Reward machines: Exploiting reward function structure in reinforcement learning. CoRR
abs/2010.03950 (2020), https://arxiv.org/abs/2010.03950
6. Kazemi, M., Perez, M., Somenzi, F., Soudjani, S., Trivedi, A., Velasquez, A.:
Assume-guarantee reinforcement learning (2023)
7. Neary, C., Xu, Z., Wu, B., Topcu, U.: Reward machines for cooperative multi-agent
reinforcement learning. CoRR abs/2007.01962 (2020), https://arxiv.org/abs/
2007.01962
8. Paliwal, Y., Roy, R., Gaglione, J.R., Baharisangari, N., Neider, D., Duan, X., Topcu,
U., Xu, Z.: Reinforcement learning with temporal-logic-based causal diagrams (2023)
9. Sipser, M.: Introduction to the Theory of Computation. Course Technology, Boston,
MA, third edn. (2013)
10. Wong, K.C.: On the complexity of projections of discrete-event systems. Discrete
Event Dynamic Systems (1998), https://api.semanticscholar.org/CorpusID:
17637223
11. Xu, Z., Gavran, I., Ahmad, Y., Majumdar, R., Neider, D., Topcu, U., Wu, B.: Joint
inference of reward machines and policies for reinforcement learning. Proceedings of
the International Conference on Automated Planning and Scheduling 30, 590–598
(Jun 2020). https://doi.org/10.1609/icaps.v30i1.6756, http://dx.doi.org/
10.1609/icaps.v30i1.6756

Decentralized MARL with Temporal Causality

A

19

Parallel Composition and Bisimilirity of Reward
Machines

In this section, we introduce the formal definitions of parallel composition and
bisimilarity for reward machines.
Definition 5 (Parallel Composition of Reward Machines). Let R1 =
⟨U1 , u1I , Σ1 , δ1 , σ1 , F1 ⟩ and R2 = ⟨U2 , u2I , Σ2 , δ2 , σ2 , F2 ⟩ be two reward machines.
The parallel composition of R1 and R2 is a new reward machine P = ⟨U, uI , Σ, δ, σ, F ⟩
where the
– set of states is U = U1 × U2 ,
– initial states is uI = (u1I , u2I ),
– input alphabet is Σ = Σ1 ∪ Σ2
– transition function is defined as


(δ1 (u1 , e), δ2 (u2 , e))





δ2 (u2 , e) are defined
δ((u1 , u2 ), e) = (δ1 (u1 , e), u2 )



(u1 , δ2 (u2 , e))



undefined

if δ1 (u1 , e) and
if only δ1 (u1 , e) is defined
if only δ2 (u2 , e) is defined
otherwise

– set of terminal states is F = F1 × F2 ; and
– output function σ : U × U → R is defined as σ(u, u′ ) = 1 if u′ ∈ F and u ̸∈ F
and 0 otherwise.
Definition 6 (Bisimilirity of Reward Machines). Let R1 =
⟨U1 , u1I , Σ, δ1 , σ1 , F1 ⟩ and R2 = ⟨U2 , u2I , Σ, δ2 , σ2 , F2 ⟩ be two reward machines
with the same event set Σ. R1 and R2 are bisimilar (R1 ∼
= R2 ) if there exists a
relation R ⊆ U1 × U2 such that
1. (u1I , u2I ) ∈ R.
2. For every (u1 , u2 ) ∈ R and e ∈ Σ:
– (Accepting) u1 ∈ F1 if and only if u2 ∈ F2 .
– (Forth) if δ1 (u1 , e) = u′1 , then there exists u′2 ∈ U2 such that δ2 (u2 , e) =
u′2 and (u′1 , u′2 ) ∈ R.
– (Back) if δ2 (u2 , e) = u′2 , then there exists u′1 ∈ U1 such that δ1 (u1 , e) =
u′1 and (u′1 , u′2 ) ∈ R.

B

Proofs

B.1

Proof of Theorem 3 (Relaxed Decomposition Criterion)

Proof. Let R be the team task RM, and R1 , . . . , RN be the projections of R
onto the local event sets Σ1 , . . . , ΣN , respectively. Let C be a TL-CD that holds

20

Jan Corazza et al.

for M, with equivalent causal DFA C, such that R∥C ∼
= (∥N
i=1 Ri )∥C, and let
∗
ξ ∈ Σ be an M-attainable event sequence.
By assumption, we have that ξ |= φCΣ . If R(ξ) = 0, then R∥C = 0, by
definition of parallel composition. Due to R∥C ∼
= (∥N
i=1 Ri )∥C, we also have
N
(∥i=1 Ri )∥C = 0. Here, we used the fact that the pair of reward machines R∥C
and (∥N
i=1 Ri )∥C are bisimilar, and pass the strict decomposition criterion 1.
Because C(ξ) = 1 (i.e., δC (qIC , ξ) ∈ F C ), and the definition of parallel composition,
we have (∥N
i=1 Ri )(ξ) = 0. Again by the definition of parallel composition, we
have that ∃i ∈ {1, . . . , N } : Ri (Pi (ξ)) = 0. The converse direction proceeds
analogously.
B.2

Proof of Theorem 4 (Criterion Compatibility)

Proof. The proof is direct. Assume ρ ⊆ U R × U P is a bisimulation between R
and P.
Let QC be the set of states of the causal DFA C. We define the relation ρ̃ ⊆
UR∥C × UP∥C via Formula 1, for all states (uR , q1 ) ∈ UR∥C and (uP , q2 ) ∈ UP∥C .
((uR , q1 ), (uP , q2 )) ∈ ρ̃ ⇔ (uR , uP ) ∈ ρ ∧ q1 = q2

(1)

By checking the bisimulation conditions, we will show that ρ̃ is a bisimulation
between R∥C and P∥C.
P
R
P
1. ((uR
I , qI ), (uI , qI )) ∈ ρ̃ because (uI , uI ) ∈ ρ.
R
P
2. Let ((u , q1 ), (u , q2 )) ∈ ρ̃, and e ∈ Σ some event. Then (uR , uP ) ∈ ρ and
q1 = q2 = q ∈ QC . Therefore, we have the following.
– (Accepting) (uR , q) ∈ F R∥C if and only if (uP , q) ∈ F P∥C , because
(uR , q) ∈ F R∥C ⇔ uR ∈ F R ∧ q ∈ F C ⇔ uP ∈ F P ∧ q ∈ F C ⇔ (uP , q) ∈
F P∥C .
P
– (Forth) if δR∥C ((uR , q), e) = (uR
⋆ , q⋆ ), then there exists u⋆ such that
P
P
δP∥C ((u , q), e) = (u⋆ , q⋆ ) and
P
((uR
⋆ , q⋆ ), (u⋆ , q⋆ )) ∈ ρ̃.
R
– (Back) if δP∥C ((uP , q), e) = (uP
⋆ , q⋆ ), then there exists u⋆ such that
R
R
δR∥C ((u , q), e) = (u⋆ , q⋆ ) and
P
((uR
⋆ , q⋆ ), (u⋆ , q⋆ )) ∈ ρ̃.
R
One obtains uP
⋆ (u⋆ ) from the bisimulation condition on P and R. Then,
P
R
((u⋆ , q⋆ ), (u⋆ , q⋆ )) ∈ ρ̃ by definition of ρ̃. The diagrams in Figure 8, Figure 9,

and Figure 10 illustrate the back and forth bisimulation conditions.

B.3

Proof of Theorem 5 (Relaxed Decomposition Viability)

Proof. Let M = ⟨S, sI , A, p, γ, R, L⟩ be the team task RM-MDP, with local event
sets Σ1 , . . . , ΣN and decomposable label function L, L1 , . . . , LN the local labeling
functions of agents 1, . . . , N , and C the TL-CD which holds for M. Assume that

Decentralized MARL with Temporal Causality

uR

uP

ρ

δR (e)

δP (e)

ρ

uR
⋆

uR

uP

ρ

δP(e)

δR (e)

uP
⋆

21

ρ

uR
⋆

(a) Forth

uP
⋆

(b) Back

Fig. 8: Visualizing R ∼
= P.
uR , q

ρ, =

δR (e)

uP , q

uR , q

δP (e)

δR (e)

δC (e)
uR
⋆ , q⋆

ρ, =
(a) Forth

ρ, =
δP (e)

δC (e)
uP
⋆ , q⋆

uP , q

δC (e)
uR
⋆ , q⋆

ρ, =

δC (e)
uP
⋆ , q⋆

(b) Back

Fig. 9: Visualizing R ∼
= P alongside transitions from C.

the relaxed decomposition criterion holds, and that agents synchronize on shared
events.
We use ⟨Li (si0 ui0 · · · sik uik ) | Synci ⟩ to denote the synchronized local labeling
function output. Synci is a binary vector, and Syncit = 1 iff. agent i received a
synchronization signal in time-step t.
For the first claim we need to show that for all team trajectories s0 u0 · · · sk uk
and local trajectories {si0 ui0 · · · sik uik }N
i=1 , we have R(L(s0 u0 · · · sk uk )) = 1 if and
only if Ri (⟨Li (si0 ui0 · · · sik uik ) | Synci ⟩) = 1 for all i = 1, . . . , N .
We first note that s0 u0 · · · sk uk is sampled from M, which means that
L(s0 u0 · · · sk uk ) |= φCΣ . Then the result follows from Theorem 3 if we show
that Pi (L(s0 u0 · · · sk uk )) = ⟨Li (si0 ui0 · · · sik uik ) | Synci ⟩ (for every i = 1, . . . , N ).
Let ℓt denote the output of L at time-step t = 0, . . . , k, ℓit the output of Li , and
ℓ˜it the syncrhonized output of Li .
We proceed by induction over time-steps t.

22

Jan Corazza et al.

(uR , q)

(uP , q)

ρ

δR∥C (e)

δP∥C (e)

(uR
⋆ , q⋆ )

ρ

(uP
⋆ , q⋆ )

(uR , q)

δR∥C (e)

δP∥C (e)

ρ

(uR
⋆ , q⋆ )

(a) Forth

(uP , q)

ρ

(uP
⋆ , q⋆ )

(b) Back

Fig. 10: Visualizing R∥C ∼
= P∥C.

Base case: t = 0. In that case, by definition, ℓ0 = ℓi0 = ℓ˜i0 = ∅. Additionally,
by definition, u0 = uI ∈ uiI = ui0 for all i = 1, . . . , N (the initial state u0 of R is
matched by the initial states ui0 of all projections Ri ).
Assumption: Let t ∈ N, 0 < t < k such that for all i = 1, . . . , N it holds
that ℓt ∩ Σi = ℓ˜it , and ut ∈ uit .
Inductive step: By the inductive hypothesis ut ∈ uit . By the decomposability
of L, the output of Li (sit , uit , sit+1 ) = ℓit+1 is well-defined. There are three cases
to consider: (1) ℓit+1 = ℓt+1 ∩ Σi = {e} and |Ie | > 1; (2) ℓit+1 = ℓt+1 ∩ Σi = {e}
and |Ie | = 1; and (3) ℓit+1 = ∅.
In cases (2) and (3), we immediately have ℓi
= ℓ
∩ Σ = ℓi˜ (no
t+1

t+1

i

t+1

synchronization necessary), and by definition of parallel projections and the
˜ ).
inductive hypothesis, δ(ut , ℓt+1 ) = ut+1 ∈ uit+1 = δ i (uit , ℓit+1
i
In case (1), for all i ∈ Ie we have Synct+1 = 1 if e ∈ ℓt+1 , or Syncit+1 = 0
˜ = {e}, otherwise ℓi˜ = ∅. The rest of
otherwise. If Syncit+1 = 1, then ℓit+1
t+1
case (1) follows analogously to cases (2) and (3), and we may conclude that
˜ ) for all i = 1, . . . , N in all cases, i.e. the ∼ -classes of the
ut+1 ∈ δ i (uit , ℓit+1
i
projected RMs match the state of the team RM in all steps.
That, in turn, implies Pi (L(s0 u0 · · · sk uk )) = ⟨Li (si0 ui0 · · · sik uik ) | Synci ⟩ for all
i = 1, . . . , N , and the first claim is proven via Theorem 3 and L(s0 u0 · · · sk uk ) |=
φCΣ .
The second claim is that max{0, V1π (sI ) + · · · + VNπ (sI ) − (N − 1)} ≤ V π (sI ) ≤
min{V1π (sI ), . . . , V π (sI )}, where V π (sI ) denotes the team success probability,
and Viπ (sI ) the success probability of agent i. We use the notation V π to evoke
the correspondence between the success probability and the value function in
non-discounted task-completion RM-MDPs (γ = 1).
By the previous result, for any sequence of team states s0 u0 · · · sk uk , we have
R(L(s0 u0 · · · sk uk )) = 1 if and only if Ri (⟨Li (si0 ui0 · · · sik uik ) | Synci ⟩) = 1 for all
i = 1, . . . , N . This implies that the probability of completing the task captured

Decentralized MARL with Temporal Causality

23

by R under policy π is equal to the probability of simultaneously completing all
tasks captured by Ri (when π = (π1 , . . . , πN )). Then the second claim follows
by applying the Fréchet conjunction inequality to the success probabilities of the
agents.

C

Case Study 3: Buttons Task

In our third case study, we perform the same analysis for a 3-agent problem we call
the Cooperative Buttons Task. In this environment shown in Figure 11a, events
B , B1 , B2 , and B3 correspond to yellow, green, and red buttons respectively.
In this shared environment, there are three agents - A1, A2, and A3 - who
have to work together to achieve the objective of allowing A1 to reach the goal
location known as Goal. However, the path to the goal is blocked by three colored
regions - red, yellow, and green - which correspond to the paths of agents A1, A2,
and A3 respectively. To cross these colored regions, the corresponding colored
button must be pressed first. The yellow and green buttons can be pressed by
an individual agent, but the red button requires two agents to simultaneously
occupy the button’s location before it gets activated.
In order to complete the task, the agents have to follow a specific sequence of
events. First, A1 should push the yellow button, which will allow A2 to proceed
to the green button. Pressing the green button is necessary for A3 to join A2 in
pressing the red button, which will finally allow A1 to cross the red region and
reach the goal location. The sequence of events is encoded in the reward machine
depicted in Figure 11b.
B

A2

A3

A1
B1

S

S

B2

B3

Ö

(a) B, B1 , B2 , and B3 are buttons, and
Ö is the goal. One-way doors are represented by the downwards arrow. States
labeled with S are signal lights which
indicate that the agent is in the region
that cannot go to the red button.

3
A¬B
2

start

uI

B

u1

B2 ∨ B1

u3

3
AB
3

A2B3

3
A¬B
2

A3B3

3
A¬B
3

u2

u5

3
A¬B
3

u4

3
AB
2

B3

u6
Goal
u7

(b) Reward machine encoding the cooperative buttons task. Output is 1 on
transitions into accepting states, and 0
otherwise.

Fig. 11: Environment (left) and RM (right) for the cooperative buttons task.
One-way doors block agent 1 to go to the red button and push the button.

The results for this case study are shown in Figure 12.

Jan Corazza et al.
Centralized (Buttons)
1,000

1,000
800
600
400

Training Steps

·1

05

Training Steps

·1
06

2

1

0

0

1

No TL-CD
TL-CD

No TL-CD
TL-CD

200
0

Steps to Task Completion

Decentralized (Buttons)

0.
5

24

Fig. 12: Buttons Task Comparisons. Results of 10 independent runs (2 in the case
of the Centralized algorithm, which does not converge in 106 steps).

Figures 13a, 13b, and 13c show the results of projecting the task RM from
Figure 11b onto the local event sets of Σ1 , Σ2 , and Σ3 , respectively.

u24
3
A¬B
2

u1I

B

u11

B3

u12

Goal

u2I
u13

u21

B1 ∨ B2

u22

3
AB
2

B3
u23

¬B3
3
(b) Σ2 = {B, B1 , B2 , AB
}
2 , A2

(a) Σ1 = {B, B3 , G}
u3I

B

B1 ∨ B2

3
A¬B
3

u31

u32

B3

u33

3
AB
3

¬B3
3
(c) Σ3 = {B1 , B2 , AB
, B3 }
3 , A3

Fig. 13: Projections of the team task RM from Figure 11b along local event sets
of agents 1, 2, and 3.

In the cooperative button task, states labeled with S indicate that the agent
is in a region from which it cannot proceed to the red button and, consequently,
cannot reach the goal. These labels, called ’Signal lights’, serve as indicators
of non-goal-achievable states, providing critical information about the agent’s
position within the environment. This knowledge is encoded in Figure 14, which
represents the TL-CD for the cooperative button task. The TL-CD visually
outlines the causal pathways and dependencies between states, highlighting the
impact of signal lights on the agent’s progress. Complementing this diagram, the

Decentralized MARL with Temporal Causality

25

related Causal DFA formalizes these causal relationships and transitions, aiding
in understanding and predicting the agent’s behavior within the task.
¬S
S

G¬G

(a) φC ≡ G(S → G¬G)

qI

¬G
S

q1

⊤
G

q2

(b) Causal DFA for the TL-CD in Figure 14a

Fig. 14: TL-CD for the Cooperative Buttons Task (Left) and respective Causal
DFA (Right).

