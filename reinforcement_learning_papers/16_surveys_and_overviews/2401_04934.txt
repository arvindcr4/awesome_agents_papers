Fully Decentralized Cooperative Multi-Agent Reinforcement
Learning: A Survey
Jiechuan Jiang†

jiechuan.jiang@pku.edu.cn

arXiv:2401.04934v1 [cs.MA] 10 Jan 2024

School of Computer Science
Peking University

Kefan Su†

sukefan@pku.edu.cn

School of Computer Science
Peking University

Zongqing Lu

zongqing.lu@pku.edu.cn

School of Computer Science
Peking University
†

Equal contribution, alphabetic order

Abstract
Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world
cooperative tasks, but restrictions of real-world applications may require training the agents
in a fully decentralized manner. Due to the lack of information about other agents, it is
challenging to derive algorithms that can converge to the optimal joint policy in a fully
decentralized setting. Thus, this research area has not been thoroughly studied. In this
paper, we seek to systematically review the fully decentralized methods in two settings:
maximizing a shared reward of all agents and maximizing the sum of individual rewards of
all agents, and discuss open questions and future research directions.

1

Introduction

Many real-world applications require that multiple agents cooperatively accomplish a task, including traﬃc
signal control (Xu et al., 2021), power dispatch (Wang et al., 2021b), ﬁnance (Fang et al., 2023), and robot
control (Orr & Dutta, 2023). Recently, accompanied by the maturity of deep learning techniques, multiagent reinforcement learning (MARL) has been widely applied to such cooperative tasks, where a group of
agents interacts with a common environment, each agent decides its local action, and they are trained to
maximize a shared reward or the sum of individual rewards.
There are two main paradigms of cooperative MARL: centralized training with decentralized execution
(CTDE) and fully decentralized learning, according to whether the information of other agents, e.g., the
actions of other agents, can be obtained during the training process. In CTDE methods, each agent has access
to global information during the training but only relies on its local information to make decisions during
execution. A lot of CTDE methods have been proposed (Lowe et al., 2017; Sunehag et al., 2018; Rashid et al.,
2018; Son et al., 2019; Iqbal & Sha, 2019; Wang et al., 2021a; Rashid et al., 2020; Wang et al., 2020; Zhang
et al., 2021c; Su & Lu, 2022b; Peng et al., 2021; Li et al., 2022; Wang et al., 2023a;b) and achieve signiﬁcant
performance in multi-agent benchmarks, e.g., StarCraft Multi-Agent Challenge (Samvelyan et al., 2019a)
and Google research football (Kurach et al., 2020). However, in some scenarios where global information
is unavailable or the number of agents is dynamically changing, the centralized modules lose eﬀectiveness,
and fully decentralized learning is necessary (Zhang et al., 2021a). In fully decentralized learning, each
agent cannot obtain any information from other agents in both training and execution. Other agents have
to be treated as a part of the environment but are updating their policies during the training. Thus the
1

environment becomes non-stationary from the perspective of individual agents (Foerster et al., 2017; Jiang &
Lu, 2022), which violates the assumptions of almost all existing reinforcement learning methods and makes
it challenging to derive algorithms that can converge to the optimal joint policies in the fully decentralized
setting. Perhaps due to this reason, research on decentralized learning algorithms is limited. Therefore,
in this paper, we provide an overview of cooperative MARL, with a focus on fully decentralized learning
algorithms, hoping to assist researchers in gaining a clear understanding and generating more interest in this
challenging yet meaningful research direction.
Fully decentralized cooperative MARL is commonly formulated into two settings: maximizing a shared
reward of all agents (called shared reward setting) and maximizing the sum of individual rewards of all
agents (called reward sum setting). The shared reward setting is the most popular formulation in the recent
MARL methods with deep learning. We review the work of shared reward setting within two representative
frameworks: value-based methods and policy-based methods. Value-based methods focus on mitigating the
impact of non-stationary transition probabilities. Policy-based methods investigate how to guarantee the
monotonic improvement of the joint policies in fully decentralized optimization. Most of the methods in
these two categories propose new value iterations or new optimization objectives of policies, thus they can be
naturally combined with deep neural networks and are practical in high-dimensional complex tasks. Then
we present the work in the reward sum setting, where the target of algorithms is to ﬁnd a Nash equilibrium
for all agents. We also categorize existing studies into value-based algorithms and policy-based algorithms.
The value-based algorithms use Q-learning (Watkins & Dayan, 1992) to obtain the best response policy of
the other agents’ policies. The policy-based algorithms control the diﬀerence between the current policy and
the best response by the gradient dominance condition.
We structure the paper as follows. Section 2 covers the background on single-agent RL and cooperative
multi-agent RL, highlighting the fully decentralized formulation. In Section 3 and Section 4, we respectively
review the methods of shared reward setting and reward sum setting. Finally, in Section 5 we discuss
limitations, open questions, and future research directions for fully decentralized MARL.

2

Background

2.1

Single-Agent RL

Reinforcement learning is usually formulated as a Markov decision process, which is deﬁned as a tuple
hS, A, P, r, γ, ρ0 i. S and A respectively denote the state and action spaces. P (s′ |s, a) denotes the transition
probability from the state s ∈ S to the next state s′ ∈ S for the given action a ∈ A. r(s, a, s′ ) is the reward
function for evaluating transitions. γ ∈ [0, 1) is the discount factor, and ρ0 is the distribution of initial states.
At each timestep t, the agent receives the state st and selects an action at according to its policy π. The
environment transitions to the next state st+1 according to transition probability P (s′ |s, a), and the agent
receives a reward r(st , at , st+1 ). Reinforcement learning aim at learning a policy π to maximize the expected
discounted return
"∞
#
X
t
J(π) = Eπ
γ r (st , at , st+1 ) .
t

The action-value function Qπ is deﬁned as the expected return if the agent starts from state s, takes action
a, and then forever acts according to policy π:
"∞
#
X
π
t
Q (s, a) = Eπ
γ r (st , at , st+1 ) | s0 = s, a0 = a .
t=0

The state-value function V π is deﬁned as the expected return if the agent starts from state s and always
acts according to policy π:
"∞
#
X
π
π
t
V (s) = Ea∼π [Q (s, a)] = Eπ
γ r (st , at , st+1 ) | s0 = s .
t=0

2

The advantage function Aπ describes how much better it is to take an action a over acting according to π:
Aπ (s, a) = Qπ (s, a) − V π (s).
The functions corresponding to the optimal policy π ∗ are deﬁned as the optimal Q-function Q∗ and the
optimal V-function V ∗ .
RL algorithms can be commonly categorized into two frameworks, value-based and policy-based methods,
according to whether a policy is explicitly learned. We introduce the most typical methods for both types. Qlearning (Watkins & Dayan, 1992) is a representative value-based method. It updates the optimal Q-function
Q∗ using the Bellman operator:
h
i
∗
′ ′
Q
(s
,
a
)
.
T Q∗ (s, a) = Es′ ∼P r + γ max
′
a

Under this Bellman iteration, Q-learning is proved to converge to the optimal Q-function Q∗ with ﬁnite state
and action spaces. The optimal policy in Q-learning is deterministic and can be derived by greedily selecting
the action with the highest Q-value π(s) = maxa Q∗ (s, a). Q-learning is an oﬀ-policy algorithm, which can
learn using the experiences collected by any policy.

Policy-based methods directly learn a policy πθ parameterized by θ using policy gradient. The most straightforward method is REINFORCE (Williams, 1992), and with the advantage function the policy gradient ∇J(θ)
can be further formulated as
∇J(θ) = Ea∼πθ (·|s) [Aπθ (s, a)∇ log πθ (a | s)] .
However, as the policy is parameterized by θ, the update of θ according to the policy gradient may dramatically change the policy. Thus policy gradient methods are hard to guarantee monotonic policy improvement.
TRPO (Schulman et al., 2015) updates the policy by taking the largest step possible to improve performance
with the constraint of KL-divergence between new and old policies


πθ (a | s) πθ
θk+1 = arg max Es,a∼πθk
A k (s, a) , s.t. Es∼πθk [DKL (πθ (· | s)kπθk (· | s))] ≤ δ,
θ
πθk (a | s)
where πθk is the old policy for experience collection. TRPO can guarantee monotonic improvement but has
poor computation eﬃciency due to second-order optimization. PPO (Schulman et al., 2017) is a simple and
empirical approximation of TRPO, which uses a clipping trick in the objective function to make sure the
new policy is close to the old policy. The objective can be written as





πθ (a | s) πθ
πθ (a | s)
L(πθ ) = Es,a∼πθk min
A k (s, a), clip
, 1 − ǫ, 1 + ǫ Aπθk (s, a) ,
πθk (a | s)
πθk (a | s)
where ǫ controls how far away πθ is allowed to deviate from πθk . Policy-based methods are usually on-policy.
2.2

Multi-Agent RL

Extending single-agent RL to multi-agent RL, we consider multi-agent MDP hS, A, P, r, γ, ρ0 , N i. N is the
agent number. S denotes the state space. A := A1 × · · · × AN denotes the joint action space, and Ai is the
action space of agent i. Each agent i decides its own action ai ∈ Ai according to its policy πi . P (s′ |s, a)
denotes the transition probability from the state s ∈ S to the next state s′ ∈ S for the joint action a.
r(s, a, s′ ) = [r1 , · · · , ri , · · · , rN ] is the rewards of all agents. γ ∈ [0, 1) is the discount factor, and ρ0 is
the distribution of initial states. Cooperative MARL learns the joint policy π to maximize the expected
discounted return of the sum of agents’ rewards
#
"∞
N
X X
ri (st , at , st+1 ) .
J(π) = Eπ
γt
t

i=1

If the agents’ rewards are always the same, i.e., r1 = r2 =, · · · , = rN , the formulation is referred to as the
shared reward setting, where all agents maximize the global objective, which is the most popular setting of
3

MARL methods. Without this restriction, the formulation is called the reward sum setting. The former is
a special case of the latter.
According to whether the information of other agents can be obtained during the training process, cooperative
MARL algorithms can be divided into two categories: centralized training with decentralized execution
(CTDE) and fully decentralized learning. In CTDE methods, each agent has access to global information
during the training and only relies on its local information for decision-making in the execution. The most
popular framework in CTDE is value decomposition (Sunehag et al., 2018; Rashid et al., 2018; Son et al.,
2019; Wang et al., 2021a; Rashid et al., 2020; Yang et al., 2020), where the joint Q-function is factorized
into individual Q-functions by a mixer
Qtot (s, a) = mixer (Q1 (s, a1 ) , Q2 (s, a2 ) , · · · , QN (s, aN )) .
The mixer should ensure that an arg max operation performed on Qtot yields the same joint action as a
set of individual arg max operations performed on each Qi . The mixer can be a sum operation (Sunehag
et al., 2018), a weighted sum operation with positive weights produced by attention-network (Yang et al.,
2020), or a neural network with positive weights produced by hyper-network (Rashid et al., 2018). Multiagent actor-critic methods adopt vanilla centralized critic (Lowe et al., 2017; Foerster et al., 2018; Iqbal
& Sha, 2019) or value-decomposition critic (Wang et al., 2020; Peng et al., 2021). FOP (Zhang et al.,
2021c) independently decomposes the joint policy into individual policies, and MACPF (Wang et al., 2023a)
considers the dependency between individual policies in the decomposition. MAPPO (Yu et al., 2022) extends
PPO to the multi-agent setting by the centralized state-value function, and HAPPO Kuba et al. (2021)
decomposes the joint advantage function into individual advantage factions and sequentially updates the
individual policies. Some methods (Zhang et al., 2018; Konan et al., 2021; Li & He, 2020) require information
sharing with neighboring agents according to a time-varying communication channel in both training and
execution, which are categorized as networked agent setting and beyond the scope of decentralized execution.
In fully decentralized learning, each agent cannot obtain any information from other agents
in both training and execution and independently updates its own policy to maximize the
sum of all agents’ rewards. Each agent is not allowed to share its actions, experiences, neural network
parameters, etc, with other agents, and has to treat other agents as a part of the environment. Since all
agents are updating their policies during the training, the environment becomes non-stationary from the
individual agent’s perspective, making it hard to develop algorithms that can converge to the optimal joint
policy in a fully decentralized way. In the next two sections, we respectively review the existing algorithms
that try to tackle this problem in the shared reward setting and reward sum setting.

3

Shared Reward Setting

3.1

Value-Based Methods

In fully decentralized learning, the transition probability from the perspective of each agent i is
X
Pi (s′ |s, ai ) =
P (s′ |s, ai , a−i ) π−i (a−i |s),
a−i

where a−i and π−i respectively denote the joint action and joint policy of all agents except agent i. Pi depends
on the policies of other agents π−i . As other agents are updating their policies continuously, Pi becomes
non-stationary. Under the non-stationary transition probabilities, if each agent i performs independent
Q-learning (IQL) (Tan, 1993)


′ ′
Q
(s
,
a
)
,
T Qi (s, ai ) = EPi (s′ |s,ai ) r + γmax
i
i
′
ai

the convergence of Q-function is not guaranteed. Q-learning is oﬀ-policy and commonly learns from experiences stored in a replay buﬀer. However, the experiences in the replay buﬀer are collected under changing
transition probabilities Pi , thus the transition probabilities in the replay buﬀer are not only non-stationary
but also obsolete. Fingerprints (Foerster et al., 2017) adds iteration number and exploration rate to the
4

state as conditions to disambiguate the age of experiences to alleviate the problem of obsolescence. Lenient
IQL (Palmer et al., 2018) uses a decayed leniency value to motivate the agents to focus on fresh experiences. MA2QL (Su et al., 2022) lets the agents alternately perform IQL. While one agent is updating
its Q-function, other agents keep their Q-functions unchanged. MA2QL guarantees the convergence to a
Nash equilibrium, but the converged equilibrium may not be the optimal one when there are multiple Nash
equilibria. Moreover, to obtain the theoretical guarantee, it has to be trained in an on-policy manner and
each agent should collect its replay buﬀer from scratch at each training turn, which leads to poor sample
eﬃciency. Therefore, to address this issue, MA2QL is trained in an oﬀ-policy manner when combined with
neural networks. Distributed IQL (Lauer & Riedmiller, 2000) is a new operator


′ ′
Qi (s , ai ) ,
T Qi (s, ai ) = max Qi (s, ai ), r + γmax
′
ai

which can guarantee to converge to the optimal joint policy in deterministic environments. However, Distributed IQL fails in stochastic environments. Hysteretic IQL (Matignon et al., 2007) is an extension of
Distributed IQL, which applies diﬀerent learning rates w(s, ai ) to diﬀerent experiences:
w(s, ai ) =

(

1

Qi (s′ , a′i ) > Qi (s, ai )
if r + γmax
′

λ<1

else.

ai

If λ = 0, Hysteretic IQL degenerates to Distributed IQL. Hysteretic IQL mitigates the overestimation of
Distributed IQL and is more robust in stochastic environments. I2Q (Jiang & Lu, 2022) lets each agent
perform IQL on ideal transition probabilities, which are deﬁned as

∗
∗
P s′ |s, ai , π−i
(s, ai ) , π−i
(s, ai ) = arg maxQ∗ (s, ai , a−i ).
a−i

I2Q is proven to converge to the optimal joint policy under ideal transition probabilities. I2Q provides a
method to obtain the ideal transition probabilities in deterministic environments by learning a value function
′
Qss
i (s, s ) using the following operator
′
ss
′ ′′
T Qss
i (s, s ) = r + γ ′′max ′ Qi (s , s ) ,
s ∈N (s )

where N is the neighboring state set. In deterministic environments, under ideal transition probabilities,
the state transitions to
′
s′∗ = arg max Qss
i (s, s ).
s′ ∈N (s,ai )

However, how to obtain ideal transition probabilities in stochastic environments is a remaining question. For
stochastic environments, BQL (Jiang & Lu, 2023b) proposes a new operator



′ ′
Q
(s
,
a
)
,
T Qi (s, ai ) = max Qi (s, ai ), EP̃i (s′ |s,ai ) r + γmax
i
i
′
ai

where P̃i is one randomly selected of possible transition probabilities. BQL can converge to the optimal
joint policy in both deterministic and stochastic environments if all possible transition probabilities can be
sampled. Distributed IQL is a special case of BQL when the environment is deterministic. However, in
neural network implementation, due to sample eﬃciency, BQL does not maintain multiple replay buﬀers to
represent diﬀerent transition probabilities but maintains only one buﬀer like IQL. The non-stationarity in
the replay helps BQL sample diﬀerent possible transition probabilities.
3.2

Policy-Based Methods

In the shared reward setting, policy-based methods are usually extended from single-agent RL. Independent
learning is a straight but eﬀective idea for fully decentralized learning. Recently, independent PPO (IPPO)
5

(de Witt et al., 2020) has attracted the attention of the MARL community. The algorithm of IPPO is that
each agent i updates its policy πi with PPO (Schulman et al., 2017):





πi (ai |s)
πi (ai |s) old
old
A (s, ai ), clip
, 1 − ǫ, 1 + ǫ Ai (s, ai ) .
Li (πi ) = Es,ai min
πiold (ai |s) i
πiold (ai |s)
IPPO obtains good performance, comparable to CTDE methods, in the popular MARL benchmark SMAC
(Samvelyan et al., 2019b) with such a simple implementation. However, IPPO is still a heuristic algorithm
troubled by the non-stationary problem.
DPO (Su & Lu, 2022a) tries to solve the non-stationary problem following the idea of using a surrogate
function as in TRPO (Schulman et al., 2015) for single-agent RL. Suppose we use TRPO to learn a joint
policy in a centralized manner, then we have the objective
max
J(π) − J(π old ) ≥ Ljoint
(π) − C · DKL
(π old kπ) = S TRPO (π, π old )
πold
X
X
where Ljoint
(π) =
ρold (s)
π(a|s)Aold (s, a),
πold
s

a

max
where DKL
denotes the maximum KL-divergence between two policies over states and ρold (s) is the state

distribution under πold . The surrogate function can be optimized to make sure that the original objective
improves monotonically. Let π new = arg maxπ S TRPO (π, π old ), then we know that J(π new ) − J(π old ) ≥
S TRPO (π new , π old ) ≥ S TRPO (π old , π old ) = 0, which leads to J(π new ) ≥ J(π old ). So we can deﬁne an
iteration that π t+1 = arg maxπ S TRPO (π, π t ), then the sequence {J(π t )} converges combining with the
condition that J(π) is bounded. Following this principle, DPO ﬁnds a novel surrogate function that is
appropriate for fully decentralized learning:
J(π) − J(π old ) ≥

N
X

SiDPO (πi , πiold )

i=1

q
1 i
max (π old kπ ) − CD max (π old kπ ),
Lπold (πi ) − M̂ DKL
i
i
KL
i
i
N
X
X
Liπold (π i ) =
π i (ai |s)Aold
ρold (s)
i (s, ai ),
SiDPO (πi , πiold ) =
s

ai

where M̂ and C are two constants. An important property of SiDPO (πi , πiold ) is that it can be optimized
independently for each agent and make the joint policy improve monotonically, simultaneously. By deﬁning
an iteration πit+1 = arg maxπi SiDPO (πi , πit ), the sequence {J(π t )} improves monotonically. As for the
practical algorithm, DPO uses two adaptive coeﬃcients to replace the large constants M̂ and C following
the practice of PPO (Schulman et al., 2017),
q

1
avg t
avg t
(πi kπi ) ,
(1)
(πi kπi ) − βi2 DKL
Liπt (π i ) − βi1 DKL
πit+1 = arg max
πi
N
avg
where βi1 and βi2 are the adaptive coeﬃcients, DKL
is the average KL-divergence to replace the maximum
max
KL-divergence DKL
.

TVPO (Su & Lu, 2024) solves the non-stationarity problem
of policy optimization.
P from theoldperspective
(a−i |s)Aold (s, ai , a−i ) in the indeTVPO is motivated by the diﬀerence between the term a πi (ai |s)π−i
P
pendent objective Liπold (π i ) and the term a πi (ai |s)π−i (a−i |s)Aold (s, ai , a−i ) in the joint objective Ljoint
πold (π)
and proposes novel V-function and Q-function combining with f -divergence:
X
1 XX
σ−i (a−i |s)Qπ
πi (ai |s)
Vσπ (s) =
σ (s, ai , a−i ) − ωDf (πi (·|s)||σi (·|s)) ,
N i a
a
i

−i

π ′
Qπ
σ (s, ai , a−i ) = r(s, ai , a−i ) + γEs′ ∼P (·|s,ai ,a−i ) [Vσ (s )] .

Given a ﬁxed σ, TVPO deﬁnes an iteration as following:
X
X
old
σ−i (a−i |s)Qπ
πi (ai |s)
πinew = arg max
σ (s, ai , a−i ) − ωDf (πi (·|s)kσi (·|s)) ,
πi

ai

a−i

6

and proves that Vσπold (s) ≤ Vσπnew . Based on this result, if we take π old = σ = π t , π new = π t+1 , Df = DTV
(DTV is the total variation distance) and choose an appropriate ω, then the iteration becomes
X
X

t
t
πit+1 = arg max
π−i
(a−i |s)Qπ (s, ai , a−i ) − ωDTV πi (·|s)||πit (·|s)
πi (ai |s)
πi

= arg max
πi

a−i

ai

X
ai


t
t
πi (ai |s)Qπ
i (s, ai ) − ωDTV πi (·|s)||πi (·|s) .
π

t
(s) ≥ V πt−1 (s), which means
With this iteration, TVPO further proves that Vπtt+1 (s) ≥ V πt (s) ≥ Vππt−1
t

the sequence {V π } improves monotonically and converge to suboptimum. Importantly, this iteration can
be executed in a fully decentralized way. In the algorithm of TVPO, there is an issue similar to DPO and
TRPO where the large constant ω may lead to a small stepsize in the gradient update. So TVPO also uses
an adaptive coeﬃcient βi to replace ω.

4

Reward Sum Setting

′
In the reward sum
P setting,
P each agent i has an individual reward function ri (s, a, s ) and an objective
Ji (πi , π−i ) = Eπ [ t γ t i ri (st , at , st+1 )]. However, as we consider fully decentralized learning, each agent
has no access to the rewards of other
P agents. Therefore, the reward sum setting degenerates to the general
sum setting, i.e., Ji (πi , π−i ) = Eπ [ t γ t ri (st , at , st+1 )]. In the general sum setting, the optimal policies for
diﬀerent agents are usually diﬀerent and cannot be achieved simultaneously. So the target of the algorithms
in the general sum setting is to ﬁnd the Nash equilibrium (NE). A joint policy π ∗ is a Nash equilibrium if
∗
∗
). In other words, for any agent i, πi∗ is the best response of
) = maxπi Ji (πi , π−i
for any agent i, Ji (πi∗ , π−i
∗
π−i . Unfortunately, previous studies show that the complexity of ﬁnding a Nash equilibrium in a general sum
game is PPAD-complete (Goldberg, 2011), which means we can hardly ﬁnd a Nash equilibrium in practice.
So the existing studies usually need some assumption about the structure of the game or try to ﬁnd some
weaker equilibrium. In the following, we survey representative and recent studies on the general sum setting
and again divide them into value-based methods and policy-based methods.

4.1

Value-Based Methods

In this section, we will introduce two lines of research in the general sum setting: We introduce two types of
value-based methods: Decentralized Q-learning (Arslan & Yüksel, 2016) and V-learning (Jin et al., 2021).
Decentralized Q-learning is proven to asymptotically converge to a Nash equilibrium with a high probability.
V-learning is proven to converge to a coarse correlated equilibrium (CCE) with a high probability. CCE
is a weaker equilibrium than NE, which allows for the policies to be correlated while NE requires all the
policies to be independent. Unlike the asymptotic results of Decentralized Q-learning, V-learning provides
the sample complexity analysis for convergence.
4.1.1

Decentralized Q-Learning

Decentralized Q-learning (Arslan & Yüksel, 2016) relies on the assumption that the general sum game is a
weakly acyclic game. If we take all the deterministic policies in a general sum game as the nodes of a
graph and there is an edge from the policy π 1 to the policy π 2 if and only if there exists one agent i such
2
1
1
that π−i
= π−i
and πi2 is the best response of π−i
. A general sum game is a weakly acyclic game if for any
0
0
1
L
policy π , there exists a path (π , π , · · · , π ) where π L is a Nash equilibrium.
Given the assumption of the weakly acyclic game, the main idea of Decentralized Q-learning is relatively
easy to understand. If an algorithm could satisfy the condition that after K steps, the policy stays at the
equilibrium with a probability p > 0, then we can repeat the process M times which means the probability
of the event that the policy does not stay at the equilibrium is (1 − p)M . Let M → ∞ and we know that
the policy will ﬁnally converge to a NE with probability 1.
Decentralized Q-learning is an algorithm satisfying this condition with the idea of inertia policy update.
Inertia policy update means that for any agent i, if the policy πi is already the best response of the other
7

agents’ policies π−i , then agent i should keep πi unchanged; otherwise, with probability λi which corresponds
to the inertia in the policy update, agent i will remain πi , and with probability 1 − λi , πi will become any
other policy uniformly. It is simple to show that the inertia policy update satisﬁes the condition mentioned
above. If the joint policy π is already an NE, from the inertia policy update we know that the joint policy
will not be changed. Otherwise, from the property of the weakly acyclic game, we know that there exists
one path (π 0 = π, π 1 , · · · , π L ) and the probability q of moving forward one step along the path is positive
since there exists at least one situation where one policy becomes the best response in the uniform update
and all the other policies remain unchanged from the inertia. So after L steps, the policy reaches the NE
π L with probability q L > 0. Let p = q L , K = L, then we can follow the idea mentioned above to complete
the proof.
The problem remaining for Decentralized Q-learning is to judge whether a joint policy is an NE or a policy
πi is the best response of other agents. The solution is Q-learning. Given the policies of other agents
π−i ﬁxed, if the agent i updates through Q-learning, then we know that the policy πi will converge to the
optimal policy which is the best response of π−i . If all the agents update their policies through Q-learning,
the environment becomes non-stationary. So Decentralized Q-learning uses the exploration phase technique,
which divides the learning process into several exploration phases. The idea of the exploration phase is
similar to on-policy learning. In each exploration phase, all the policies will be ﬁxed and the samples will
only be used to update the Q-function. At the end of each exploration phase, the policy will be updated
from the Q-function. It is obvious that if the length of exploration phases {tk } is suﬃciently long then the
Q-function will be suﬃciently accurate and the agents can obtain the correct best response.
There are several succeeding works for Decentralized Q-learning. Asynchronous Decentralized Q-Learning
(Yongacoglu et al., 2023) discusses the situation that each agent has an independent exploration phase
sequence {tik }. Asynchronous Decentralized Q-Learning believes that the shared exploration phase sequence
{tk } is a synchronous constraint for decentralized learning and proves that even with independent exploration
phase sequence {tik }, which means the non-stationary problem will arise again, it still has the convergence
guarantee. Asynchronous Decentralized Q-Learning requires that {tik } satisﬁes the condition ∃T, R ∈ N, tik ∈
[T, RT ], which means that the diﬀerences of the update frequency should be limited. Given this condition,
the key to the proof of Asynchronous Decentralized Q-Learning is that there exists a sequence of active
phase {[τkmin , τkmax ]}, which has several properties: (1) all the agents have at least one chance to update
in the interval [τkmin , τkmax ]; (2) the total number of all the agent updates is ﬁnite and the length of the
min
interval [τkmin , τkmax ] is ﬁnite; (3) all the agents will not update their policies in the interval (τkmax , τk+1
)
and the length of this interval is at least T /N . The condition (3) means that if T is suﬃciently large, then
min
(τkmax , τk+1
) is enough for agents to obtain best response from Q-learning. The conditions (1) and (2) mean
that the probability of the joint policy moving forward one step along the path in one active phase is positive
as there exists one situation where one policy becomes the best response in one update chance and all the
policies remain unchanged in other update chance from the inertia.
Independent Team Q-learning (Yongacoglu et al., 2021) tries to ﬁnd the team optimal policy following the
idea of Decentralized Q-learning. A joint policy π ∗ is team optimal if for any agent i, Ji (π ∗ ) = maxπ Ji (π).
The team optimal policy doesn’t always exist and if a general sum game has a team optimal policy then we
call it a common interest game. Independent Team Q-learning ﬁnds the team optimal policy in the common
interest game through a similar way to inertia policy update. If a joint policy π is a team optimal policy,
with probability 1 − γ i the policy πi will stay the same and with probability γ i the policy πi will become any
policy uniformly. Otherwise, with probability 1 − κi the policy πi will be changed by a transition kernel hi
which can be chosen by the user freely and with probability κi will become any policy uniformly. Independent
Team Q-learning builds a Markov Chain of the policy and proves that the stationary distribution of this
Markov Chain will lie in the set of team optimal policies with probability one ifPγ i << κi . Independent
Team Q-learning evaluates the summation of Q-functions after k updates Sik = s Qki (s, πik (s)) and uses
the condition Sik < min{Sik−1 , Sik−2 , · · · , Sik−Wi } + di to judge whether a joint policy πik is team optimal,
where Wi is a constant and di is tolerance of the sub-optimality for agent i. This condition is eﬀective when
the Q-function is suﬃciently accurate and the joint policy has been a team optimal policy in the last W i
updates. The probability of this event is positive as the exploration phase can be suﬃciently long and a
team optimal policy can be reached by the uniform transition with probability γ i or κi .

8

4.1.2

V-Learning

V-learning (Jin et al., 2021) discusses the convergence to CCE in the episodic MDP setting. In the episodic
MDP, suppose that the horizon is H, then for each time step h ∈ {1, 2, · · · , H}, the reward function
ri,h (s, a, s′ ) and the transition probability Ph (s′ |s, a) are both related to the time step h. Moreover, the
π
policy πi,h and the value function Vi,h
are also related to the time step h.
The idea of V-learning for ﬁnding CCE is to bound the diﬀerence between the joint policy π and its best
π ,π
†,π
response maxi Vi,1 −i − Vi,1i −i . V-learning uses an optimistic approximation V̄i,h as the upper bound of the
†,π

best response Vi,1 −i . The update rule V̄i,h is

V̄i,h (sh ) ← (1 − αt )V̄i,h (sh ) + αt ri,h + V̄i,h+1 (sh+1 + βt )

where t = Nh (sh ) is visitation times of the pair (h, sh ), αt is the learning rate and βt > 0 is the bonus
which is key to the optimistic approximation. Moreover, V-learning uses a pessimistic approximation V i,h
π ,π
as the lower bound of Vi,1i −i . V i,h is similar to V̄i,h but the bonus βt is replaced with −βt . With carefully
designed αt and βt , the upper bound and lower bound are eﬀective and V-learning can use the diﬀerence
V̄i,h − V i,h to control the distance between the current policy and CCE.
The episodic MDP setting means that the policies πi,h (·|s) are relatively independent for each pair (s, h), so
the policy update can be executed by the bandit update. V-learning uses the bandit algorithm Follow-theRegularized-Leader (FTRL) to update the policy πi,h (·|s) for each pair (s, h). The bandit update has an
important property for V-learning that the regret for the bandit update is bounded and the bound is about
h (s)
as the loss or reward for the bandit over the pair (s, h) to
the update times t. V-learning uses 1 − rh +V
H
obtain a proper regret bound for the proof of the convergence.
The algorithm of V-learning is relatively simple. Suppose that training process contains K episodes, then
k
for each episode k, the value function V̄i,h is updated by the optimistic bonus and the policy πi,h
is updated
k
by the bandit update. However, the policies πi,h obtained in the training are not the output policy π̂ that
can approximate the CCE and must be saved for calculating the output policy. V-learning has an algorithm
for calculating the output policy and the formulation of π̂ is complicated. Jin et al. (2021) also prove that
the sample complexity of V-learning is O(H 5 SAmax /ǫ2 ) which means that to obtain an output policy within
the range of ǫ from the CCE, V-learning needs O(H 5 SAmax /ǫ2 ) episodes, where S is the number of state
and Amax = maxi |Ai |.
There are several related works of V-learning. However, the algorithm and the proof of V-learning are integrated tightly and these related works follow the similar idea of V-learning to obtain the sample complexity,
so the changes of these works in the algorithm are relatively small. V-learning OMD (Mao & Başar, 2023)
uses mirror descent for the bandit update of the policies instead of FTRL and obtain the sample complexity
O(H 6 SAmax /ǫ2 ) which is weaker than V-learning. Stage-based V-learning (Mao et al., 2022) divides the
learning process of Vh (s) into several stages for each pair (s, h) according to the visitation times t = Nh (s).
Vh (s) will be updated if and only if one stage of (s, h) ends. The length of the stages is a geometric series with
a common ratio 1 + 1/H. Stage-based V-learning can also obtain the sample complexity O(H 5 SAmax /ǫ2 )
as V-learning. Wang et al. (2023c) and Cui et al. (2023) follow the similar idea of policy replay to extend
V-learning from the tabular case to the function approximation case. They both save the learned policies
into a buﬀer and uniformly sample one policy from this buﬀer for the learning in the new episode. The
calculation of the output policy is still needed.
4.2

Policy-based Method

The practical algorithms of the policy-based methods are relatively straightforward. Most of them are
independent actor-critic or even REINFORCE. So the main contributions of these studies are the discussion and analysis of the convergence to the Nash equilibrium. As we mentioned before, ﬁnding a
Nash equilibrium in a general sum game is quite diﬃcult so the convergence results of these studies
are asymptotic. The main idea behind the proof of the convergence result is to control the diﬀerence
maxπ̂i Ji (π̂i , π−i ) − Ji (πi , π−i ). The gradient dominance condition is critical for controlling the diﬀerence,
9

which means Ji (π̂i , π−i ) − Ji (πi , π−i ) ≤ M maxπi′ hπi′ − πi , ∇i Ji (πi , π−i )i and M is a constant. Given the
gradient dominance condition, the problem of ﬁnding a Nash equilibrium can be changed to the problem
of controlling the bound M maxπi′ hπi′ − πi , ∇i Ji (πi , π−i )i, i.e., controlling the gradient ∇i Ji (πi , π−i ) which
can be done by the policy gradient. With the main idea of the policy-based algorithms in the general sum
setting, we will introduce some related studies in detail.
Etesami (2022) discusses the problem of the perspective of occupancy measure ρi instead of policy πi . The
occupancy measure ρi and the policy πi can be mutually transformed from each other by the property
,ai )
ρi (si , ai ) = µπi (si )π(ai |si ) and π(ai |si ) = Pρi (sρ i(s
, where µπi (si ) is the stationary distribution of the
,a′ )
a′
i

i

i

i

state given the policy πi . The beneﬁt of using the occupancy measure ρi is that we can rewrite the objective
as Ji (πi , π−i ) = Ji (ρi , ρ−i ) = hρ, ri i = ρi , vρ−i (ri ) , where vρ−i (ri ) = Eρ−i [ri (s, ai , a−i )]. It is obvious
that Ji (ρi , ρ−i ) is linear over ρi and the gradient is ∇i Ji (ρi , ρ−i ) = vρ−i (ri ), which is a stronger condition
than the gradient
dominance
condition. As for the practical algorithm, Etesami (2022) uses the term
i
h

ri (s,ai ,a−i )
as the unbiased estimator for the gradient vρ−i (ri ). The optimization objective is
πi (ai |s)
k+1
k
k
ρi = arg maxρi ρi , Ri −hi (ρi ), where hi is the regularization term. To avoid the zero in the denominator
i ,a−i )
δ
of ri (s,a
πi (ai |s) , Etesami (2022) limits the occupancy measure ρ within the space P , where δ > 0 is constant
δ
and ρ ∈ P satisﬁes ρi (si , ai ) ≥ δ, ∀i ∈ {1, 2, · · · , N }. We need to point out that the convenient property

Rik (si , ai ) = E

that Ji (ρi , ρ−i ) is linear over ρi is built on the strong assumption about the game structure. Etesami (2022)
assumes that the joint state s can be divided into s = (s1 , s2 , · · · , sN ) and the state transition Pi (s′i |si , ai )
of agent i is independent of other agents’ actions and states.
Zhang et al. (2021b); Giannou et al. (2022); Chen & Li (2022) all apply policy gradient to ﬁnd a Nash
equilibrium and discuss the theoretical results of policy gradient in the general sum setting. So the practical
algorithms of Zhang et al. (2021b); Giannou et al. (2022); Chen & Li (2022) are similar. However, these
studies provide diﬀerent theoretical results and we will focus on introducing these contents. Zhang et al.
(2021b) show the ﬁrst-order stationary policy is an equivalence to the Nash equilibrium, which means the
∗
policy π ∗ satisﬁes the condition πi − πi∗ , ∇i J(πi∗ , π−i
) ≤ 0, ∀i ∈ {1, 2, · · · , N }, ∀πi ∈ Πi . With this
property, Zhang et al. (2021b) proves that if the initial policy π 0 is within a neighborhood of a Nash
equilibrium π ∗ , then the policy sequence {π t } generated by the policy gradient algorithm will converge to
π ∗ with high probability. Zhang et al. (2021b) also provide the analysis of the sample complexity for the
local convergence result. Giannou et al. (2022) also provide proof of the local convergence result to the
ﬁrst-order stationary policy. Furthermore, Giannou et al. (2022) propose the second-order stationary policy
which means that a policy π ∗ satisﬁes (π ∗ − π)T Jac(π ∗ )(π ∗ − π) < 0, ∀π 6= π ∗ , where Jac(π ∗ ) is Jacobian
of J at π ∗ . Giannou et al. (2022) show that the second-order stationary policy is a suﬃcient condition
for the ﬁrst-order stationary policy. Giannou et al. (2022) also prove a local asymptotic convergence result
of the second-order stationary policy which is a stronger theoretical result. Chen & Li (2022) extend the
discussion into the case of the continuous state space and action space. With the property of the equivalence
between the ﬁrst-order stationary policy and the Nash equilibrium, Chen & Li (2022) change the problem
of ﬁnding a Nash equilibrium into the problem of variational inequality, which means trying to ﬁnd a
solution x∗ satisfying the condition hG(x∗ ), x∗ − xi , ∀x ∈ K, where K is the domain of x and G(x) is a
given function. In this problem, x corresponds to the policy πi and the function G(x) corresponds to the
gradient ∇i J(πi , π−i ). With these preparations, Chen & Li (2022) design a two-loop algorithm in which
the authors sequentially update a constructed strongly monotone variational inequality in the outer loop by
updating a proximal parameter and employ a single-call extra-gradient algorithm in the inner loop for solving
the constructed variational inequality. Moreover, Chen & Li (2022) provide global asymptotic convergence
results of the Nash equilibrium which means the initial policy is not required to stay within the neighborhood
of a Nash equilibrium. Instead, the results in Chen & Li (2022) need the assumption that the policy space
is a nonempty compact convex which is a much looser condition than the neighborhood condition.

10

5

Discussion

5.1

Open Questions

The research on fully decentralized MARL is still preliminary. There are many open questions worth exploring:
• Optimal joint policy. The existing works in the shared reward setting mainly focus on the convergence result. Value-based methods can converge to optimal joint policy in both deterministic and
stochastic environments, i.e., BQL, while policy-based methods can only guarantee the convergence
to suboptimal joint policy, e.g., TVPO. Therefore, how to devise a policy-based algorithm that has
the convergence to optimal joint policy is still an open question.
• Sample complexity. Most convergence results in the shared reward setting and the general sum
setting are asymptotic. So providing the analysis of the sample complexity for fully decentralized
algorithms may be an interesting direction for future work. Moreover, some decentralized algorithms
are still on-policy which may be troubled with the poor sample eﬃciency, especially in the MARL
setting. Proposing novel algorithms with better sample eﬃciency can be a critical open question.
• Coordination. Most analysis of optimal joint policy is based on the assumption that there is only
one optimal joint policy. When there are multiple optimal joint actions at some state, if each agent
arbitrarily selects one of the optimal independent actions, the joint action might not be optimal. It is
hard to learn a coordinated policy in a fully decentralized way. Existing coordination methods require
information exchange between agents (Zhang & Lesser, 2013; Böhmer et al., 2020; Li et al., 2021).
Decentralized coordination without any communication or pre-deﬁned rules is quite a challenge.
• Offline decentralized MARL. In oﬄine decentralized MARL, the agents cannot interact with the
environment to collect experiences but have to learn from oﬄine datasets pre-collected by behavior
policies. Unlike single-agent oﬄine RL, where the main cause of value estimation error is out-ofdistribution actions, the agents also suﬀer from the bias between oﬄine transition probabilities and
online transition probabilities. Still, the dataset of agent i does not contain the actions of other
agents and the agents cannot share information during train and execution. Therefore, from the
perspective of agent i, the oﬄine transition probability in the dataset is:
X
PBi (s′ |s, ai ) =
P (s′ |s, ai , a−i ) πB−i (a−i |s),
a−i

which depends on other agents’ behavior policies πB−i . The online transition probability during
execution is:
X
PEi (s′ |s, ai ) =
P (s′ |s, ai , a−i ) πE−i (a−i |s),
a−i

which depends on other agents’ learned policies πE−i . As the learned policies may greatly deviate
from behavior policies, there is a large bias between oﬄine and online transition probabilities, which
leads to value estimation error. MABCQ (Jiang & Lu, 2023a) tries to reduce the bias by normalizing
the oﬄine transition probabilities and increasing the transition probabilities of high-value states.
OTC (Jiang & Lu, 2023c) corrects the oﬄine transition probabilities using limited online experiences.
More theoretical analysis and practical methods are expected in this direction.
5.2

Partial Observation

The incomplete information about the state caused by the partial observation hinders the process of ﬁnding
the optimal policy in POMDP. The computation complexity of POMDP has been studied for decades.
Papadimitriou & Tsitsiklis (1987) show that solving the POMDP with the model information is a PSPACEcomplete problem which means it is less likely to be solved within polynomial time than the NP-complete
problem. Mundhenk et al. (2000) take one step further to show that ﬁnding the optimal policy in the POMDP
is a NPPP -complete problem, where NPPP is a class of complexity between NP and PSPACE. Vlassis et al.
11

(2012) show solving the POMDP is an NP-hard problem. On the other hand, in fully decentralized learning,
researchers focus on the convergence of algorithms facing the challenge of the non-stationary problem, either
in the shared reward setting or in the general sum setting. So combining the theoretical analysis with the
partial observation can be notoriously diﬃcult and the existing works all provide the proof of convergence
from the perspective of the state instead of the observation. Partial observation is an important property of
POMDP and is worth more attention from the MARL community. But we also would like to appeal to the
community to be more tolerant of the progress in fully decentralized learning.
5.3

CTDE vs. Fully Decentralized Learning

Why is fully decentralized learning necessary as we already have CTDE? This is the most commonly asked
question. Here we discuss the situations where CTDE is preferred and the situations where decentralized
learning should be considered. When the cooperation task is ﬁxed and centralized modules are allowed,
CTDE methods can achieve stronger performance since they guarantee convergence to the optimal joint
policy, do not suﬀer from non-stationarity, and have better sample complexity. However, there are some cases
where the information of all agents is not available due to network or privacy. Taking autonomous vehicles
as an example, agents might belong to diﬀerent companies and cannot share action information. Therefore,
we can only use decentralized learning. Moreover, it is challenging for CTDE methods to handle the varying
agent numbers and unknown policies of other agents in open-ended environments, e.g., autonomous vehicles,
robots, and online games. Without the constraints of centralized modules, decentralized learning has a high
potential in open-ended environments.
5.4

Unified Reinforcement Learning

When there is only one agent in the environment, the cooperative MARL setting will degenerate into a
single-agent RL setting. Naturally, we expect that the decentralized MARL algorithms can still guarantee
convergence to optimal policy when applied in single-agent tasks. This can provide us with a uniﬁed perspective of both single-agent RL and multi-agent RL. For example, in single-agent environments, there is
only one possible transition distribution in BQL, so BQL degenerates into vanilla Q-learning. In open-ended
environments, an agent may cooperate with other agents in certain states while being able to complete
sub-tasks independently in other states. A uniﬁed reinforcement learning framework allows each agent to
be trained by the same algorithm, eliminating the need to switch algorithms based on diﬀerent scenarios, so
it is suitable for learning in open-ended environments. However, the theory behind this uniﬁed perspective
and the associated practical algorithms require further comprehensive investigation.

References
Gürdal Arslan and Serdar Yüksel. Decentralized q-learning for stochastic teams and games. IEEE Transactions on Automatic Control, 62(4):1545–1558, 2016.
Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In International Conference on Machine Learning (ICML), 2020.
Yan Chen and Tao Li. Decentralized policy gradient for nash equilibria learning of general-sum stochastic
games. arXiv preprint arXiv:2210.07651, 2022.
Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space: Rl in
markov games with independent linear function approximation. In The Thirty Sixth Annual Conference
on Learning Theory, pp. 2651–2652. PMLR, 2023.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr,
Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent
challenge? arXiv preprint arXiv:2011.09533, 2020.
S Rasoul Etesami. Learning stationary nash equilibrium policies in n-player stochastic games with independent chains. arXiv preprint arXiv:2201.12224, 2022.
12

Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang,
Yong Yu, and Tie-Yan Liu. Learning multi-agent intention-aware communication for optimal multi-order
execution in ﬁnance. In The 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD), 2023.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet Kohli,
and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In
International Conference on Machine Learning (ICML), 2017.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI conference on artificial intelligence (AAAI), 2018.
Angeliki Giannou, Kyriakos Lotidis, Panayotis Mertikopoulos, and Emmanouil-Vasileios VlatakisGkaragkounis. On the convergence of policy gradient methods to nash equilibria in general stochastic
games. Advances in Neural Information Processing Systems, 35:7128–7141, 2022.
Paul W Goldberg. A survey of ppad-completeness for computing nash equilibria.
arXiv:1103.2709, 2011.

arXiv preprint

Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In International
Conference on Machine Learning (ICML), 2019.
Jiechuan Jiang and Zongqing Lu. I2q: A fully decentralized q-learning algorithm. In Advances in Neural
Information Processing Systems (NeurIPS), 2022.
Jiechuan Jiang and Zongqing Lu. Oﬄine decentralized multi-agent reinforcement learning. European Conference on Artificial Intelligence (ECAI), 2023a.
Jiechuan Jiang and Zongqing Lu. Best possible q-learning. arXiv preprint arXiv:2302.01188, 2023b.
Jiechuan Jiang and Zongqing Lu. Online tuning for oﬄine decentralized multi-agent reinforcement learning.
In AAAI Conference on Artificial Intelligence (AAAI), 2023c.
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, eﬃcient, decentralized
algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.
Sachin G Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in
cooperative and byzantine decentralized teaming. In International Conference on Learning Representations
(ICLR), 2021.
Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang.
Trust region policy optimisation in multi-agent reinforcement learning. In International Conference on
Learning Representations (ICLR), 2021.
Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajkac, Olivier Bachem, Lasse Espeholt, Carlos
Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research football: A novel
reinforcement learning environment. In The AAAI Conference on Artificial Intelligence (AAAI), 2020.
Martin Lauer and Martin Riedmiller. An algorithm for distributed reinforcement learning in cooperative
multi-agent systems. In International Conference on Machine Learning (ICML), 2000.
Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv preprint arXiv:2010.07916,
2020.
Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination
graphs for multi-agent reinforcement learning. In International Conference on Autonomous Agents and
Multi-Agent Systems (AAMAS), 2021.
Yueheng Li, Guangming Xie, and Zongqing Lu. Diﬀerence advantage estimation for multi-agent policy
gradients. In International Conference on Machine Learning (ICML), 2022.
13

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. Neural Information Processing Systems (NeurIPS), 2017.
Weichao Mao and Tamer Başar. Provably eﬃcient reinforcement learning in decentralized general-sum
markov games. Dynamic Games and Applications, 13(1):165–186, 2023.
Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. In International Conference on Machine Learning, pp.
15007–15049. PMLR, 2022.
Laëtitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algorithm for
decentralized reinforcement learning in cooperative multi-agent teams. In International Conference on
Intelligent Robots and Systems (IROS), 2007.
Martin Mundhenk, Judy Goldsmith, Christopher Lusena, and Eric Allender. Complexity of ﬁnite-horizon
markov decision process problems. Journal of the ACM (JACM), 47(4):681–720, 2000.
James Orr and Ayan Dutta. Multi-agent deep reinforcement learning for multi-robot applications: a survey.
Sensors, 23(7):3625, 2023.
Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep reinforcement
learning. In International Conference on Autonomous Agents and MultiAgent Systems (AAMAS), 2018.
Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes. Mathematics
of operations research, 12(3):441–450, 1987.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin
Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances in
Neural Information Processing Systems (NeurIPS), 2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning (ICML), 2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. Advances in Neural Information
Processing Systems (NeurIPS), 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim
G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The starcraft
multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019a.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019b.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International Conference on Machine Learning (ICML), 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning (ICML), 2019.
Kefan Su and Zongqing Lu. Decentralized policy optimization. arXiv preprint arXiv:2211.03032, 2022a.
Kefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International Conference on
Machine Learning (ICML), 2022b.
14

Kefan Su and Zongqing Lu. f -divergence policy optimization in fully decentralized cooperative marl, 2024.
Kefan Su, Siyuan Zhou, Chuang Gan, Xiangjun Wang, and Zongqing Lu. Ma2ql: A minimalist approach to
fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:2209.08244, 2022.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks
for cooperative multi-agent learning based on team reward. In International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International Conference on Machine Learning (ICML), 1993.
Nikos Vlassis, Michael L Littman, and David Barber. On the computational complexity of stochastic controller optimization in pomdps. ACM Transactions on Computation Theory (TOCT), 4(4):1–8, 2012.
Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized execution:
Multi-agent conditional policy factorization. In International Conference on Learning Representations
(ICLR), 2023a.
Jiangxing Wang, Deheng Ye, and Zongqing Lu. Mutual-information regularized multi-agent policy iteration.
In Advances in Neural Information Processing Systems (NeurIPS), 2023b.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent
q-learning. In International Conference on Learning Representations (ICLR), 2021a.
Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent reinforcement learning for active voltage control on power distribution networks. Advances in Neural Information Processing
Systems (NeurIPS), 2021b.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Oﬀ-policy multi-agent
decomposed policy gradients. In International Conference on Learning Representations (ICLR), 2020.
Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably eﬃcient
decentralized multi-agent rl with function approximation. arXiv preprint arXiv:2302.06606, 2023c.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279–292, 1992.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning, 8:229–256, 1992.
Bingyu Xu, Yaowei Wang, Zhaozhi Wang, Huizhu Jia, and Zongqing Lu. Hierarchically and cooperatively
learning traﬃc signal control. In AAAI Conference on Artificial Intelligence (AAAI), 2021.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten:
A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939,
2020.
Bora Yongacoglu, Gürdal Arslan, and Serdar Yüksel. Decentralized learning for optimality in stochastic dynamic teams and games with local control and global state information. IEEE Transactions on Automatic
Control, 67(10):5230–5245, 2021.
Bora Yongacoglu, Gürdal Arslan, and Serdar Yüksel. Asynchronous decentralized q-learning: Two timescale
analysis by persistence. arXiv preprint arXiv:2308.03239, 2023.
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
eﬀectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems
(NeurIPS), 2022.
Chongjie Zhang and Victor Lesser. Coordinating multi-agent reinforcement learning with limited communication. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2013.
15

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent
reinforcement learning with networked agents. In International Conference on Machine Learning (ICML),
2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A selective overview
of theories and algorithms. Handbook of Reinforcement Learning and Control, pp. 321–384, 2021a.
Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in stochastic games: stationary points, convergence,
and sample complexity. arXiv preprint arXiv:2106.00198, 2021b.
Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint
policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine
Learning (ICML), 2021c.

16

