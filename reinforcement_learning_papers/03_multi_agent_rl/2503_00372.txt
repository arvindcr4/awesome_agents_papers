arXiv:2503.00372v1 [cs.MA] 1 Mar 2025

Nucleolus Credit Assignment for Effective Coalitions in
Multi-agent Reinforcement Learning
Yugu Li

Zehong Caoâˆ—

University of South Australia
Adelaide, Australia
liyyy301@mymail.unisa.edu.au

University of South Australia
Adelaide, Australia
jimmy.cao@unisa.edu.au

Jianglin Qiao

Siyi Hu

University of South Australia
Adelaide, Australia
jianglin.qiao@unisa.edu.au

University of South Australia
Adelaide, Australia
siyi.hu@unisa.edu.au

ABSTRACT
In cooperative multi-agent reinforcement learning (MARL), agents
typically form a single grand coalition based on credit assignment to
tackle a composite task, often resulting in suboptimal performance.
This paper proposed a nucleolus-based credit assignment grounded
in cooperative game theory, enabling the autonomous partitioning
of agents into multiple small coalitions that can effectively identify
and complete subtasks within a larger composite task. Specifically,
our designed nucleolus Q-learning could assign fair credits to each
agent, and the nucleolus Q-operator provides theoretical guarantees with interpretability for both learning convergence and the
stability of the formed small coalitions. Through experiments on
Predator-Prey and StarCraft scenarios across varying difficulty levels, our approach demonstrated the emergence of multiple effective
coalitions during MARL training, leading to faster learning and
superior performance in terms of win rate and cumulative rewards
especially in hard and super-hard environments, compared to four
baseline methods. Our nucleolus-based credit assignment showed
the promise for complex composite tasks requiring effective subteams of agents.

KEYWORDS
MARL; Credit assignment; Nucleolus Q learning; Multiple small
coalitions
ACM Reference Format:
Yugu Li, Zehong Cao, Jianglin Qiao, and Siyi Hu. 2025. Nucleolus Credit
Assignment for Effective Coalitions in Multi-agent Reinforcement Learning.
In Proc. of the 24th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 â€“ 23,
2025, IFAAMAS, 9 pages.

1

INTRODUCTION

In multi-agent environments, determining the contribution of each
agent from reward signals is critical for effective cooperation to
âˆ— *Corresponding author

This work is licensed under a Creative Commons Attribution International 4.0 License.
Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2025), Y. Vorobeychik, S. Das, A. NowÃ© (eds.), May 19 â€“ 23, 2025, Detroit, Michigan,
USA. Â© 2025 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org).

complete a composite task [1, 7, 15]. In cooperative multi-agent
reinforcement learning (MARL), this process, known as Credit Assignment [10], is central to improving both the performance and
interpretability of MARL systems. Credit assignment approaches
for MARL can be broadly classified into two categories: implicit
and explicit methods.
Implicit credit assignment methods, such as VDN [31], QMIX [23],
and WQMIX [22], rely on value decomposition to indirectly infer
the contribution of each agent from the learning process. While
these methods scale efficiently, their reliance on predefined value
decomposition structures often leads to weak performance, when
the task environment does not align well with these assumptions.
Moreover, they have limited interpretability, making it difficult to
understand the specific contributions of individual agents during
task execution [6].
Explicit credit assignment methods, on the other hand, directly
evaluate each agentâ€™s contribution based on its actions, providing
better interpretability and robust performance. Benchmark algorithms in this category include COMA [8], SQDDPG [34], and
SHAQ [33]. COMA, an early approach, employs a centralized critic
with a counterfactual baseline to assess each agentâ€™s action value.
However, its ability to accurately attribute contributions from individual agents becomes restricted in complex environments. More
recent methods, such as SQDDPG and SHAQ, leverage Shapley
value, a concept from cooperative game theory, to fairly distribute rewards based on each agentâ€™s contribution. These methods
offer strong theoretical convergence guarantees and improved interpretability by evaluating the individual contributions of agents [11].
However, both implicit and explicit credit assignment methods
predominantly assume that all agents form a single grand coalition
to tackle a composite task. While promoting fairness and stability,
they occasionally lead to inefficiencies, especially in real-world
applications that involve multiple, smaller sub-teams of agents
working on different subtasks. For instance, tasks such as resource
management and emergency response (e.g., fire rescue) require
agents to form smaller, dynamic coalitions that can cooperate on
subtasks while still contributing to the overall mission [2, 5, 13]. But
existing credit assignment fails to incentivize the formation of these
smaller coalitions, resulting in larger search spaces and reduced
learning efficiency [20]. To address this challenge, it is essential to
enable the emergence of smaller, effective coalitions for cooperative

: target

Agent 1
Agent 1

Enemy 1
Enemy
1
Agent 2
Agent 2

Agent 3
Agent 3

Agent n
Agent n+1
Agent 5

Agent m+1

Grand (One) Coalition

Multi-Small Coalitions

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

ATTACK
ENEMY 1

ATTACK
ENEMY 2

ATTACK
ENEMY 3

Agent 4

Agent 6

Agent
Agent
m 1

ATTACK
ENEMY 1

Enemy 2

Enemy 3

Coalitions-based Cooperation

Action Space (Target)

Figure 1: The transition from a single grand coalition to multiple smaller, task-specific coalitions in MARL. In scenarios like
SMAC in super-hard maps: where a large number of agents are involved, forming multiple small coalitions is crucial for task
completion efficiently. Agents who attack the same enemy unit naturally form these coalitions, enabling them to work together
efficiently to achieve the mission.
MARL. We expect these small coalitions can only cooperate on subtasks, improving overall composite task performance and learning
efficiency, as shown in Fig. 1. Agents can transit from a single grand
coalition to multiple smaller, task-specific coalitions in MARL, and
ensure that these smaller coalitions remain fair, interpretable, and
stable over time.
In this paper, we propose a novel credit assignment approach
based on the nucleolus concept from cooperative game theory [27].
The nucleolus is designed to minimize the maximum dissatisfaction
among agents by providing fair reward allocations across multiple
small coalitions rather than a single grand coalition. Fairness is
achieved by calculating each agentâ€™s contribution relative to what
each coalition could achieve independently, ensuring that no agent
feels undervalued. In doing so, the nucleolus encourages agents
to remain committed to their coalitions. Without fair reward allocation, agents may become dissatisfied and leave their coalitions.
Nucleolus also improves interpretability by explicitly linking reward allocation to the agentsâ€™ contributions, making it easier to
trace why a particular coalition receives a given reward. Stability
is guaranteed as the nucleolus works to minimize excessâ€”the gap
between a coalitionâ€™s potential independent gain and its current
rewardâ€”thereby ensuring that no coalition has the incentive to deviate or reorganize, maintaining consistent cooperation throughout
the task. Thus, our introduced nucleolus-based credit assignment
for MARL will form stable and fair small coalitions. These coalitions
partition a composite task into manageable subtasks, significantly
improving both learning efficiency and MARL system performance.
Additionally, we provide theoretical guarantees for the convergence
of the proposed nucleolus-Q operator, ensuring that the action values of agents within each coalition converge to a local optimum.
Through experiments on standard benchmarks such as PredatorPrey and StarCraft Multi-Agent Challenge, we demonstrate that
our approach outperforms the existing four methods in terms of
learning efficiency and overall performance.

2 RELATED WORK
2.1 Credit Assignment in CTDE
Implicit Credit Assignment. Credit assignment is a fundamental challenge in Centralized Training with Decentralized Execution
(CTDE) [18]. VDN [31] introduces an additivity assumption, where
the global Q-value is assumed to be the sum of the local Q-values of
individual agents. This decomposition allows for simpler representation but significantly limits the expressiveness of the global Q-value.
To overcome this issue, QMIX [23] extends VDN by developing a
mixing network that allows non-linear combinations of local Qvalues to form the global Q-value. This non-linearity enhances the
representative capacity of the global Q-value but remains limited
by its implicit credit assignment mechanism, where agent contributions are not explicitly modeled. Consequently, QMIX struggles
with environments featuring non-monotonic rewards. WQMIX [22]
further looks at this issue by introducing a learnable weighting
mechanism to dynamically adjust the contributions of each agentâ€™s
local Q-value in the global Q-value computation. While WQMIX
improves adaptability, the selection of weighting coefficients often
relies on trial and error, with minimal interpretability. Moreover,
improper weight adjustments can lead to instability in the learning
process, further complicating its application in complex multi-agent
environments.
Explicit Credit Assignment. For explicit credit assignment
methods, they can directly compute the contribution of each individual agent, ensuring that each agentâ€™s policy is optimized locally.
One such method is COMA [8], which discovers a counterfactual
baseline to evaluate the difference in global reward when an agent
takes alternative actions. COMA calculates each agentâ€™s contribution to the current action by observing how global Q-value changes,
but it suffers from inefficiency issues. Because the counterfactual
baseline is derived from the current joint action rather than the
optimal one, this credit assignment may not update to an optimal joint policy. Additionally, the counterfactual calculation in
COMA increases computational complexity and reduces sample

efficiency. More recently, methods such as Rollout-based Shapley
Values [25], SQDDPG [34], and SHAQ [33] have leveraged the
Shapley value [28] from cooperative game theory to improve the
interpretability of credit assignment in MARL. Shapley value-based
methods offer a principled way to fairly distribute rewards based
on each agentâ€™s contribution. While SQDDPG builds on the DDPG
network architecture and adopts a centralized approach similar to
COMA, it is difficult for multi-agent Q-learning. SHAQ employs a
decentralized approach, allowing for more scalable credit assignment, but these Shapley value-based methods assume a single grand
coalition. In scenarios that require multi-agent cooperation based
on multiple, smaller coalitions, the aforementioned explicit credit
assignment methods do not achieve efficiency.

2.2

Nucleolus in Cooperative Games

In game theory, the Nucleolus concept [27] is a solution to identify
the most equitable allocation of resources or payoffs in a cooperative game by minimizing the maximum dissatisfaction (or excess)
among coalitions. For example, Gautam et al. [9] highlight the role
of the nucleolus in power and energy systems, emphasizing its
effectiveness in resolving resource allocation, demand response,
and system reliability issues. Similarly, Oner & Kuyzu [35] proposes
nucleolus-based cost allocation methods and develops column and
row generation techniques to solve the constrained lane covering
game for optimizing truckload transportation procurement networks, which uses a nucleolus approach to minimize costs and
fairly allocate them among shippers. These approaches are generally tailored for static reward settings with well-understood environments, rather than for maximizing rewards in MARL contexts.
In dynamic environments, where rewards are delayed, traditional
nucleolus-based methods fall short, necessitating the development
of more flexible exploration policies for agents to adapt to uncertain
conditions.

3

METHOD

We begin by developing an entity-based, partially observable coalition Markov decision process that supports coalition formation in
Subsection 3.1. Next, our proposed nucleolus-based credit assignment in Subsection 3.2: (i) define the nucleolus-Q value by incorporating the concept of the nucleolus into Q-learning to ensure both
the optimal coalition structure and optimal actions, supported by a
theorem proof, and (ii) introduce a new nucleolus-based Bellman
operator that converges to the optimal nucleolus Q-value, also with
a corresponding theorem proof.

3.1

Entity-based Coalition POMDP

We extend Decentralized Partially Observable Markov Decision
Process (Dec-POMDP) framework [14, 16, 17] by defining an Entitybased Coalition POMDP, named as EC-POMDP. Here, entities include both controllable agents and other environment landmarks,
and it is essential to distinguish the terms between a composite
task and its subtasks: a subtask represents the smallest unit of reward feedback in the environment and cannot be further divided,
whereas the collection of all subtasks is referred to as a composite task. Formally, the EC-POMDP framework is represented as
âŸ¨ğ‘†, ğ‘ , ğ´, ğ‘ƒ, ğ‘‚, ğ›¾, ğ¸, ğº, (ğ¸ğ‘” , ğ´ğ‘” , ğ‘Ÿğ‘” )ğ‘”âˆˆğº , ğ‘…, ğ¶ğ‘†âŸ©, where ğ‘† denotes a set

of states of the environment; ğ‘ = {1, . . . , ğ‘›} represents a set of
agents; ğ´ = Ã—ğ‘– âˆˆğ‘ ğ´ğ‘– is a set of joint actions of agents, each joint
action ğ‘ = (ğ‘ 1, . . . , ğ‘ğ‘› ) âˆˆ ğ´, where ğ‘ğ‘– represents the action of agent
ğ‘– âˆˆ ğ‘ ; ğ‘ƒ is the state transition function defined as ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘); ğ‘œğ‘– âˆˆ ğ‘‚
is the agentâ€™s observation; ğ›¾ represents the learning rate; ğ¸ is set
of entities, where each entity ğ‘’ âˆˆ ğ¸ has state ğ‘  ğ‘’ âˆˆ ğ‘† and let ğ‘† ğ‘’ âŠ† ğ‘†
define all possible states of entity ğ‘’; ğº = {ğ‘”1, . . . , ğ‘”ğ‘š } is the composited task of the environment with ğ‘š subtasks; each subtask ğ‘” is
defined as tuple (ğ¸ğ‘” , ğ´ğ‘” , ğ‘Ÿğ‘” )ğ‘”âˆˆğº , where ğ¸ğ‘” âŠ† ğ¸ is a set of subtaskÃ
specific entities with their states ğ‘† ğ¸ğ‘” = ğ‘’ âˆˆğ¸ğ‘” ğ‘† ğ‘’ , ğ´ğ‘” âŠ† ğ´ representing actions used to complete subtasks ğ‘” and ğ‘Ÿğ‘” : ğ‘† ğ¸ğ‘” Ã— ğ‘† Ã— ğ´ğ‘” â†’ â„
is the reward function to complete sub-task ğ‘”; ğ‘… is the total reward
for the composite task ğº; ğ¶ğ‘† = {ğ¶ğ‘” âŠ† ğ‘ |âˆ€ğ‘” âˆˆ ğº } is the coalition
Ã
structure, which satisfied with ğ‘”âˆˆğº ğ¶ğ‘” = ğ‘ and ğ¶ğ‘” âˆ© ğ¶ğ‘”â€² = âˆ… for
all ğ‘”, ğ‘”â€² âˆˆ ğº, and each ğ¶ğ‘” = {ğ‘– |âˆ€ğ‘– âˆˆ ğ‘ , ğ‘ .ğ‘¡ .ğ‘ğ‘– âˆˆ ğ´ğ‘” } is a coalition of
agents that actions of agents is needed for each subtask ğ‘”.
EC-POMDP is a Markov decision process that evolves continuously over discrete time steps. We assume that, in the initial state,
all agents are in a single grand coalition status. As subtasks are identified during the process, this grand coalition breaks into multiple
smaller coalitions, each dedicated to completing specific subtasks.
Upon completing these subtasks, these smaller coalitions will reconsolidate into a grand coalition to complete the composite task
finally.

3.2

Nucleolus-based Credit Assignment

Nucleolus in Cooperative Game. The nucleolus is a concept
of fairness used to solve payoff distribution in cooperative games.
It focuses on allocating payoffs among coalitions to minimize the
maximum dissatisfaction (excess), thereby achieving a stable and
fair distribution [27]. It provides stability and predictability, as it is
a unique and definitive solution in every game [21]. The nucleolus
is typically located within or near the core of the game, ensuring
the stability of the distribution scheme, and it balances the interests
of different coalitions to reduce the likelihood of unfair allocation.
This approach is widely applied in fields such as economics, politics,
and supply chain management to benefit distribution in cooperation
settings.
Let (ğ‘ , ğ‘¢, ğ‘£) be a cooperative game [4], where ğ‘ = {1, . . . , ğ‘›} is
the set of agents, ğ‘¢ (ğ¶) is the unity function to measure the profits
earned by a coalition ğ¶ âŠ† ğ‘ , ğ‘£ represents the total assets that can
be distributed. Let ğ‘¥ = (ğ‘¥ 1, . . . , ğ‘¥ğ‘› ) be a payoff distribution over ğ‘ .
For a coalition ğ¶, the excess ğ‘’ (ğ¶, ğ‘¥) of the coalition at ğ‘¥ is defined
as
âˆ‘ï¸
ğ‘’ (ğ¶, ğ‘¥) = ğ‘¢ (ğ¶) âˆ’
ğ‘¥ğ‘–
(1)
ğ‘– âˆˆğ¶

where ğ‘¥ğ‘– represented the payoff of agentğ‘– in coalition ğ¶. Nucleolus
is a distribution method to minimize the maximum degree of dissatisfaction of coalition in cooperative games. Formally, the optimized
process of the nucleolus is as follows:
min max ğ‘’ (ğ¶, ğ‘¥)

(2)

ğ‘¥ ğ¶ âŠ†ğ‘

Since there are ğ‘› agents, there are a total of 2ğ‘› coalitions (including
empty sets), so for a set of imputation ğ‘¥, there are always 2ğ‘› excesses.
And then sort excesses by non-increment called excesses sequence

ğœƒ (ğ‘¥), and it is defined as follows:

that a nucleolus owns: there is always a unique solution, and if the
Markov core is non-empty, the solution falls in the Markov core.

ğœƒ (ğ‘¥) = [ğ‘’ (ğ¶ 1, ğ‘¥) , ğ‘’ (ğ¶ 2, ğ‘¥) , . . . , ğ‘’ (ğ¶ 2ğ‘› , ğ‘¥)]

(3)

The final nucleolus ğ‘¥ âˆ— is a unique payoff allocation that satisfies:
âˆ—

ğ‘¥ = {ğœƒ (ğ‘¥) âª¯ ğœƒ (ğ‘¦) | âˆ€ğ‘¦ âˆˆ ğ¼ (ğ‘£)}

(4)

where âª¯ is lexicographical order and ğ¼ (ğ‘£) is the set of all possible
payoff distributions ğ‘¥, we call ğœƒ (ğ‘¥) as the Nucleolus. Note that the
nucleolus always exists and lies in the core (if the core is non-empty),
and it is unique [27].
Markov Core. In cooperative game theory, the Core [29] is the set
of feasible allocations or imputations where no coalition of agents
can benefit by breaking away from the grand coalition. For MDP,
[33] extended the definition of the core in cooperative game theory,
called Markov core. It can assess whether the payoff distribution
during the learning process in an MDP meets certain conditions,
specifically that the current payoff prevents all agents from having
the incentive to leave the current coalition and form new coalitions
to obtain greater rewards. Formally, we modify the Markov core
(MC) for our defined EC-POMDP in Subsection 3.1 as follows:
ğ‘€ğ¶ =




max ğ‘¥ğ‘– (ğ‘ , ğ‘ğ‘– ) ğ‘– âˆˆğ‘ max ğ‘¥ (ğ‘ , ğ‘ğ¶ |ğ¶ ) â‰¥ max ğ‘£ (ğ‘ , ğ‘ğ¶ ), âˆ€ğ¶ âŠ† ğ‘ , âˆ€ğ‘  âˆˆ ğ‘†
ğ‘ğ‘–

ğ‘ğ¶

Nucleolus Q-value. We use the Markov nucleolus above to assign
the global Q-value to individual Q-values. In EC-POMDP, the global
Q-value under a coalition structure ğ¶ğ‘† is defined as:
"
ğ‘„ğ¶ğ‘†,ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘ , ğ‘) = ğ‘…+ğ”¼ maxâ€²
ğ¶ğ‘†

ğ¶ âˆˆğ¶ğ‘† â€² ğ‘– âˆˆğ¶

ğ‘  â€² âˆ¼ ğ‘ƒ (Â· |ğ‘ , ğ‘)
â€²
ğ‘ğ‘– = maxğ‘ğ‘– ğ‘„ğ‘– (ğ‘  â€² , ğ‘ğ‘– )
(9)

#

Corollary 3.2. To assign the global Q-value ğ‘„ğ¶ğ‘†,ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘ , ğ‘) by
given state ğ‘  and joint action ğ‘ under coalition structure ğ¶ğ‘†, we modify
the payoff distribution as:

ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘) = ğ‘„ğ¶ğ‘†,1 (ğ‘ , ğ‘ 1 ), . . . , ğ‘„ğ¶ğ‘†,ğ‘› (ğ‘ , ğ‘ğ‘› )
(10)
Ã
where ğ‘„ğ¶ğ‘†,ğ‘– (ğ‘ , ğ‘ğ‘– ) is the individual Q-value of agent ğ‘– and
ğ‘„ğ¶ğ‘†,ğ‘– (ğ‘ , ğ‘ğ‘– )
ğ‘– âˆˆğ‘

= ğ‘„ğ¶ğ‘†,ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘ , ğ‘). Then, we model the Eq 6 to define the excess in
coalition ğ¶ in Q-learning.
âˆ‘ï¸
ğ‘’ (ğ¶, ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘)) = max ğ‘‰ (ğ‘ , ğ‘ğ¶ ) âˆ’
ğ‘„ğ¶ğ‘†,ğ‘– (ğ‘ , ğ‘ğ‘– )
(11)
ğ‘ğ¶

(5)

Markov Nucleolus. Based on the nucleolus in the cooperative
game and the Markov core, we propose the Markov nucleolus to
describe the credit assignment process in MARL. In EC-POMDP,
ğ‘£ (ğ‘ , ğ‘ğ¶ğ‘† ) is the total assets under state ğ‘  and joint action ğ‘ while
ğ¶ğ‘† is the coalition structure. For convience, we shorten ğ‘£ (ğ‘ , ğ‘ğ¶ğ‘† ) as
ğ‘£ (ğ‘ , ğ‘). ğ¼ (ğ‘£ (ğ‘ , ğ‘)) represents all possible payoff distributions from
the total assets to the individual payoff and each ğ‘¥ (ğ‘ , ğ‘) = (ğ‘¥ 1 (ğ‘ , ğ‘ 1 ),
. . . , ğ‘¥ğ‘› (ğ‘ , ğ‘ğ‘› )) âˆˆ ğ¼ (ğ‘£ (ğ‘ , ğ‘)) is one of payoff distribution. For any
coalition ğ¶, the unity function is defined as ğ‘¢ (ğ‘ , ğ‘|ğ¶) = max ğ‘£ (ğ‘ , ğ‘ğ¶ ).

ğ‘„ğ‘– (ğ‘  â€² , ğ‘ğ‘–â€² )

Next, we propose a Nucleolus Q-value based on the Markov nucleolus as follows:

ğ‘ğ¶

where ğ‘¥ğ‘– (ğ‘ , ğ‘ğ‘– ) represent the imputation of agent ğ‘– by given state
Ã
ğ‘  and action ğ‘ğ‘– , ğ‘¥ (ğ‘ , ğ‘ğ¶ |ğ¶) = ğ‘– âˆˆğ¶ ğ‘¥ğ‘– (ğ‘ , ğ‘ğ‘– ) is the imputation of
coalition ğ¶ by given state ğ‘  and joint action ğ‘ğ¶ âˆˆ ğ´ğ¶ = Ã—ğ‘– âˆˆğ¶ ğ´ğ‘– ,
and ğ‘£ (ğ‘ , ğ‘ğ¶ ) âˆˆ â„+0 is the assets of coalition ğ¶ by given state ğ‘  and
coalition joint action ğ‘ğ¶ .

âˆ‘ï¸ âˆ‘ï¸

ğ‘– âˆˆğ¶

where ğ‘‰ (ğ‘ , ğ‘ğ¶ ) is the Q-value of coalition ğ¶ with given state ğ‘  and
joint action ğ‘ğ¶ âˆˆ ğ´ = Ã—ğ‘– âˆˆğ¶ ğ´ğ‘– and ğ‘‰ (ğ‘ , ğ‘ğ¶ ) âˆˆ â„+0 . We use a similar
definition (Eq 7) of the excess sequence in Markov nucleolus as:
ğœƒ (ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘)) = [ğ‘’ (ğ¶ 1, ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘)), ğ‘’ (ğ¶ 2ğ‘› , ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘))]

(12)

âˆ— (ğ‘ , ğ‘) can be formally defined as:
Then the Nucleolus Q-value ğ‘„ğ¶ğ‘†
âˆ—
âˆ—
ğ‘„ğ¶ğ‘†
(ğ‘ , ğ‘) = {ğœƒ (ğ‘„ğ¶ğ‘†
(ğ‘ , ğ‘) ) âª¯ ğœƒ (ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘) ) | âˆ€ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘) âˆˆ ğ¼ (ğ‘„ğ¶ğ‘† (ğ‘ , ğ‘) ) }
(13)

where ğ¼ (ğ‘„ğ¶ğ‘†.ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘ , ğ‘)) is all possible payoff distribution coalition
structure ğ¶ğ‘†.
âˆ— (ğ‘ , ğ‘) = ğ‘¤ (ğ‘ , ğ‘)ğ‘„ (ğ‘ , ğ‘), where ğ‘¤ (ğ‘ , ğ‘)
Further, we suppose that ğ‘„ğ¶ğ‘†
is a vector consisted as [ğ‘¤ğ‘– (ğ‘ , ğ‘)]ğ‘–âŠ¤âˆˆğ‘ . Based on Bellmanâ€™s optimality equation, we derive the Bellman optimality equation for the
Markov nucleolus as follows:

ğ‘ğ¶

So the excess under ğ‘¥ (ğ‘ , ğ‘) is defined as follows:
ğ‘’ (ğ¶, ğ‘¥ (ğ‘ , ğ‘)) = ğ‘¢ (ğ‘ , ğ‘|ğ¶) âˆ’ ğ‘¥ (ğ‘ , ğ‘|ğ¶)
âˆ‘ï¸
= max ğ‘£ (ğ‘ , ğ‘ğ¶ ) âˆ’
ğ‘¥ğ‘– (ğ‘ , ğ‘ğ‘– )
ğ‘ğ¶

âˆ—
ğ‘„ğ¶ğ‘†
(ğ‘ , ğ‘) =ğ‘¤ (ğ‘ , ğ‘)

ğ‘  â€² âˆˆğ‘†

(6)

ğ‘– âˆˆğ¶

ğœƒ (ğ‘¥ (ğ‘ , ğ‘)) = [ğ‘’ (ğ¶ 1, ğ‘¥ (ğ‘ , ğ‘)), . . . , ğ‘’ (ğ¶ , ğ‘¥ (ğ‘ , ğ‘))]

(7)

Therefore, we can formally define the Markov Nucleolus for ECPOMDP as follows:
Definition 3.1. For EC-POMDP, the Markov Nuclelus ğ‘¥ âˆ— (ğ‘ , ğ‘) âˆˆ
ğ¼ (ğ‘£ (ğ‘ , ğ‘)) is the payoff distribution that satisfied with:
ğ‘¥ âˆ— (ğ‘ , ğ‘) = {ğœƒ (ğ‘¥ (ğ‘ , ğ‘)) âª¯ ğœƒ (ğ‘¦ (ğ‘ , ğ‘)) | âˆ€ğ‘¦ (ğ‘ , ğ‘) âˆˆ ğ¼ (ğ‘£ (ğ‘ , ğ‘))}

=ğ‘¤ (ğ‘ , ğ‘)

âˆ‘ï¸
ğ‘  â€² âˆˆğ‘†

Next, we sort the excesses of agents ğ‘ with 2ğ‘› coalitions under
ğ‘¥ (ğ‘ , ğ‘) by the non-increment as excess sequence:
2ğ‘›

âˆ‘ï¸

(8)

which is the smallest lexicographical order.
Note that the definition of the Markov nucleolus is a natural
extension of the original concept from static cooperative games
to MARL. We assume the Markov Nucleolus inherits the property

ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘) [ğ‘… + ğ›¾ maxâ€² ğ‘„ğ¶ğ‘† (ğ‘  â€², ğ‘ â€² )]
ğ¶ğ‘†,ğ‘

ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘) [ğ‘… + ğ›¾ maxâ€²
ğ¶ğ‘†,ğ‘

âˆ‘ï¸ âˆ‘ï¸

âˆ—
ğ‘„ğ¶ğ‘†,ğ‘–
(ğ‘  â€², ğ‘ğ‘–â€² )]

ğ¶ âˆˆğ¶ğ‘† ğ‘– âˆˆğ¶

(14)
Using the nucleolus-optimal Bellman equation, we derive the optimal nucleolus Q-value. To ensure the coalitionâ€™s stability, we need
to guarantee that the optimal nucleolus Q-value prevents agents
from gaining a higher Q-value by leaving the current coalition to
form a new one. And through Eq 14, we find that it is necessary
to simultaneously optimize both the coalition structure and the
action tuple < ğ¶ğ‘†, ğ‘ > to find the optimal nucleolus Q-value. But
this can result in exponential growth in the search space for the
Q-values, which typically causes Q-learning to fail. To address these
two issues, we derive Theorem 3.3 as follows:
Theorem 3.3. The Nucleolus Q-value in EC-POMDP can guarantee
(1) Each agentâ€™s individual Q-value in the optimal coalition structure is greater than it is in other coalition structures;

(2) The actions and the coalition structure exhibit consistency,
meaning that the coalition formed under the optimal actions
is also optimal.
The detailed proof for Theorem 1 is provided in Appendix A.1.
Consequently, the optimal nucleolus-based Bellman equation can
be reformulated as follows:
âˆ‘ï¸
âˆ‘ï¸
âˆ—
âˆ—
â€² â€²
ğ‘„ğ¶ğ‘†
(ğ‘ , ğ‘) = ğ‘¤ (ğ‘ , ğ‘)
ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘) [ğ‘… + ğ›¾ max
ğ‘„ğ¶ğ‘†
â€² ,ğ‘– (ğ‘  , ğ‘ )]
â€²
ğ‘

ğ‘  â€² âˆˆğ‘†

ğ‘– âˆˆğ‘

(15)
Nucleolus Q Operator. We construct a constrained MARL Q
operator to obtain the Nucleolus Q-value and provide a theorem
showing that this operator can help us achieve the optimal Nucleolus Q-value. According to the definition of the Markov Nucleolus
(Subsection 3.1), we need to minimize the maximum excess value.
Inspired by Reward-Constrained Policy Optimization (RCPO) [32]
and Discounted Reward Risk-Sensitive Actor-Critic [19], we introduce the maximum excess as a constraint ğœ‰ (ğ‘ , ğ‘) in the Q-learning
process.
âˆ‘ï¸
âˆ—
ğœ‰ (ğ‘ , ğ‘) = max [ğ‘‰ (ğ‘ , ğ‘ğ¶ ) âˆ’
ğ‘„ğ¶ğ‘†,ğ‘–
(ğ‘ , ğ‘ğ‘– )]
(16)
ğ‘ğ¶

ğ‘– âˆˆğ¶

Further, we construct the Lagrange multipliers as follows:
âˆ‘ï¸
âˆ—
ğ¿(ğœ†, ğ‘) = min max [
ğ‘„ğ¶ğ‘†,ğ‘–
(ğ‘ , ğ‘ğ‘– ) + ğœ†ğœ‰ (ğ‘ , ğ‘)]
ğœ†â‰¥0

(17)

ğ‘

ğ‘– âˆˆğ¶ğ‘†

According to RCPO, we need to implement a two-time-scale approach. This requires keeping ğœ† constant on the faster time scale,
and optimizing policy to fix the current policy on the slower time
scale and optimize ğœ†. This process allows us to identify the saddle point of Eq 17, which provides a feasible solution. Then, we
âˆ— (ğ‘ , ğ‘ ) â†’
propose an optimization operator, i.e., H : Ã—ğ‘– âˆˆğ‘ ğ‘„ğ¶ğ‘†,ğ‘–
ğ‘–
âˆ—
Ã—ğ‘– âˆˆğ‘ ğ‘„ğ¶ğ‘†,ğ‘– (ğ‘ , ğ‘ğ‘– ), with nucleolus constraints as follows:
H (Ã—ğ‘– âˆˆğ‘ ğ‘„ğ‘–âˆ— (ğ‘ , ğ‘ğ‘– )) =ğ‘¤ (ğ‘ , ğ‘)

âˆ‘ï¸

ğ‘

ğ‘

ğ‘  â€² âˆˆğ‘†

ğ‘– âˆˆğ‘

In MARL, the coalition utility function is usually unknown and must
be learned through exploration. We designed our utility function
network using a multi-Layer perceptron network [24] based on the
critic function network in RCPO. In RCPO, the temporal difference
error of the utility function network updates is as follows.


âˆ‘ï¸
âˆ—
Î”ğ‘‰ (ğ‘ , ğ‘, ğ‘  â€² ) =ğ‘… + ğœ† max [ğ‘‰ (ğ‘ , ğ‘ğ¶ ; ğœ™ âˆ’ ) âˆ’
ğ‘„ğ¶ğ‘†,ğ‘–
(ğ‘ , ğ‘ğ‘– )]
ğ‘ğ¶

+ ğœ‚ 1 max
â€²
ğ‘ğ¶

âˆ‘ï¸

ğ‘– âˆˆğ¶
â€²
âˆ’
ğ‘‰ (ğ‘  , ğ‘ğ¶ ; ğœ™ ) âˆ’

âˆ‘ï¸

â€²

ğ¶ âˆˆğ¶ğ‘† â€²

ğ‘‰ (ğ‘ , ğ‘ğ¶ ; ğœ™)

ğ¶ âˆˆğ¶ğ‘†

(20)
where ğœ‚ 1 is the discounting rate of the utility function network.
Similar to RCPO, when updating the utility function network, we
treat the constraints ğœ‰ (ğ‘ , ğ‘) as part of the reward. This indicates the
constraint term involving the utility network is not updated, so we
use the target network ğ‘‰ (Â·; ğœ™ âˆ’ ) instead.
Unlike the update of the utility function network, the update of
the Q network is performed using the Lagrange multiplier method.
This implies that we need to optimize the Q network within the
constraint conditions. Based on Eq 17, the update of network parameters ğœ”, including both the individual Q network parameters ğœ‘
and the hypernetwork parameters ğœ“ shows as follows:

âˆ‘ï¸


âˆ—
âˆ‡ğœ” ğ¿(ğœ†, ğœ”) = min ğ”¼ ğ‘… + ğœ† max [ğ‘‰ (ğ‘ , ğ‘ğ¶ ) âˆ’
ğ‘„ğ¶ğ‘†,ğ‘–
(ğœğ‘– , ğ‘ğ‘– ; ğœ‘ğ‘– )]
ğœ”

ğ‘ğ¶

+ğ›¾

âˆ‘ï¸
ğ‘– âˆˆğ¶ğ‘† â€²

âˆ’

âˆ‘ï¸

ğ‘– âˆˆğ¶

âˆ—
â€² â€² âˆ’
max
[ğ‘¤ (ğ‘  â€², ğ‘ â€² ;ğœ“ âˆ’ )ğ‘„ğ¶ğ‘†
â€² (ğœ , ğ‘ ; ğœ‘ğ‘– )]
â€²
ğ‘

âˆ—
ğ‘¤ (ğ‘ , ğ‘;ğœ“ )ğ‘„ğ¶ğ‘†
(ğœ, ğ‘; ğœ‘ğ‘– )


(21)

ğ‘– âˆˆğ¶ğ‘†

ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘)

ğ‘  â€² âˆˆğ‘†

[ğ‘… + ğ›¾ max
â€²

Thus, the proposed nucleolus Q-learning from Eq 15 can be rewritten as:
âˆ‘ï¸
âˆ‘ï¸
âˆ—
âˆ—
â€² â€²
ğ‘„ğ¶ğ‘†
(ğœ, ğ‘) = ğ‘¤ (ğ‘ , ğ‘)
ğ‘ƒ (ğ‘  â€² |ğ‘ , ğ‘) [ğ‘… +max
ğ‘„ğ¶ğ‘†
â€² ,ğ‘– (ğœğ‘– , ğ‘ğ‘– )] (19)
â€²

âˆ‘ï¸

âˆ—
â€²
ğ‘„ğ¶ğ‘†
â€² ,ğ‘– (ğ‘ , ğ‘ğ‘– ) + ğœ†ğœ‰ (ğ‘ , ğ‘)]

where ğœ‘ âˆ’ represents the target Q network parameters, and ğœ“ âˆ’
represents the target hypernetwork parameters. The update of
multiplier ğœ† is shown as follows:

ğ‘– âˆˆğ¶ğ‘† â€²

âˆ‡ğœ† ğ¿(ğœ†, ğœ”) = ğ”¼[ğœ† âˆ’ ğœ‚ 2 ğœ‰ (ğ‘ , ğ‘)]

(18)
Therefore, we demonstrate that using this new operator, Q learning can converge to the optimal Nucleolus Q-value by Theorem 3.4
below, where the detailed proof is provided in Appendix A.2.
Theorem 3.4. Nucleolus-based Bellman operator can converge
the optimal Nucleolus Q-value and the corresponding optimal joint
Ã
1
deterministic policy when ğ‘– âˆˆğ‘ maxğ‘ğ‘– ğ‘¤ğ‘– (ğ‘ , ğ‘ğ‘– ) â‰¤ ğ›¾ +ğœ†
Consequently, we believe nucleolus Q learning can help us find
the maximum social welfare based on nucleolus allocation. We can
conclude that no agent has an incentive to deviate from the current
coalition, and this allocation provides an explanation for the credit
assignment of global rewards in cooperative MARL.
Implementation in Practice. We replace the global state ğ‘  in individual Q network ğ‘„ğ‘– (ğ‘ , ğ‘ğ‘– ) with the history of local observations
ğœğ‘– âˆˆ ğœ for agent ğ‘–, resulting in ğ‘„ğ‘– (ğœğ‘– , ğ‘ğ‘– ). We handle the sequence of
local observations ğœğ‘– using a recurrent neural network (RNN) [30].

(22)

where ğœ‚ 2 is the learning rate of the update of ğœ†.
For any coalition ğ¶, the input to the unity network consists
of the current global state ğ‘  and the actions of all agents within
the coalition. The networkâ€™s output is the estimated value for this
state-action pair. The input actions are provided by the individual
Q network of each agent. To ensure a consistent input length, we
use agent IDs for position encoding, placing each agentâ€™s action in
the corresponding position based on its ID. For agents that are not
part of the coalition ğ¶, we use placeholders to fill in the missing
action inputs, maintaining a uniform input structure in different
coalitions. In addition, we present a pseudo-code for our proposed
nucleolus-based credit assignment as Algorithm 1, which provides
a step-by-step flow of the learning process.

4

EXPERIMENTAL SETUP

We validate our algorithm on two popular MARL benchmarks:
Predator-Prey [3] and the StarCraft Multi-Agent Challenge (SMAC)

Algorithm 1 Nucleolus-based Credit Assignment Algorithm
1: Initialize parameter ğœ‘ for agent Q network and target parameter
ğœ‘ âˆ’ by copying parameter ğœ‘;
2: Initialize parameter ğœ™ for coalition unity network and target
parameter ğœ™ âˆ’ by copying parameter ğœ™ ;
3: Initialize multiplier ğœ† â‰¥ 0, learning rate ğœ‚ 1 > ğœ‚ 2 > ğœ‚ 3 and

discounting rate ğ›¾;
4: Initialize replay buffer B;
5: while ğ‘„ (ğ‘œğ‘– , ğ‘ğ‘– ; ğœ”ğ‘– ) network not converge do

for ğ‘‡ ğ‘–ğ‘šğ‘’ ğ‘¡ = 1, 2, . . . ğ‘¡ğ‘œ ğ‘‡ do
Observe a global state ğ‘  and agentâ€™s observation ğ‘œğ‘– ;
8:
Select action ğ‘ğ‘– according to each agentâ€™s Q network;
Execute action ğ‘ and get next state ğ‘  â€² , each agentâ€™s next
9:
observation ğ‘œğ‘–â€² and reward ğ‘…;
10:
Store < ğ‘ , ğ‘œğ‘– âˆˆ ğ‘œ, ğ‘  â€², ğ‘, ğ‘… > to B;
11:
end for
12:
Sample K episodes from B;
13:
for ğ¸ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’ ğ‘˜ = 1, 2, . . . ğ‘¡ğ‘œ ğ· do
14:
for ğ‘‡ ğ‘–ğ‘šğ‘’ ğ‘¡ = 1, 2, ...,ğ‘‡ do
15:
Get next time step action ğ‘ â€² by maxğ‘ â€² ğ‘„ (ğ‘  â€², ğ‘ â€² );
16:
Coalition unity network update:
ğœ™ğ‘¡ +1 â† ğœ™ğ‘¡ + ğœ‚ 1 âˆ‡ğœ™ğ‘¡ [ğ‘… + ğœ†ğœ‰ (ğ‘ , ğ‘; ğœ™ âˆ’ )
+ğ›¾ğ‘‰ (ğ‘  â€², ğ‘ â€² ; ğœ™ âˆ’ ) âˆ’ ğ‘‰ (ğ‘ , ğ‘; ğœ™ğ‘¡ )]; âŠ² Eq 20
17:
Nucleolus Q network update:
ğœ‘ğ‘¡ +1 â† ğœ‘ğ‘¡ + ğœ‚ 2 âˆ‡ğœ‘ğ‘¡ [ğ‘… + ğœ†ğœ‰ (ğ‘ , ğ‘; ğœ‘ğ‘¡ )
+ğ›¾ğ‘„ (ğ‘  â€², ğ‘ â€² ; ğœ‘ âˆ’ ) âˆ’ ğ‘„ (ğ‘ , ğ‘; ğœ‘ğ‘¡ )]; âŠ² Eq 21
18:
end for
19:
Lagrange multiplier update:
ğœ† â† ğœ† âˆ’ ğœ‚ 3 ğœ‰ (ğ‘ , ğ‘);
âŠ² Eq 22
20:
Update all target parameters by copying;
21:
end for
22: end while
6:
7:

[26], against four well-known baselines, including VDN [31], QMIX [23],
WQMIX [22], and SHAQ [33]. All baseline implementations are derived from PyMARL2 [12].

Predator-Prey. The Predator-Prey environment is a testbed simulating the interaction between predators and prey. Predators must
cooperate to capture prey, while the prey attempts to evade capture
and maximize their survival time. Each agent operates based on local observations, with predators compensating for their individual
limitations through cooperation and prey using strategic movement
to escape. Predators are rewarded for successful captures, which
require at least two predators to capture a prey simultaneously. If
the prey is captured, a global reward of 10 points is granted. The
primary mission for each predator is to minimize the number of
steps required to capture prey by coordinating effectively. The state
space for each predator includes its position, velocity, the relative
displacement between itself and the prey or other predators, and
the velocity of the prey. We set 8 Predators and 4 Prey in this task,
and the training duration in the Predator-Prey environment is set
to 1 million steps for all algorithms.
StarCraft Multi-Agent Challenge. SMAC is a widely used benchmark
for evaluating MARL algorithms. In SMAC, each unit is controlled
by an RL agent, which dynamically adapts its behavior based on
local observations. Each agent is tasked with learning cooperative
strategies to optimize performance in combat scenarios. Agents
have a limited line of sight to perceive their local environment
and can only attack enemy units within their attack range. The
objective is to maximize cumulative rewards, which are calculated
based on the damage inflicted on enemy units and the number
of enemy units destroyed, with substantial rewards awarded for
achieving overall victory. In our experiment, the training duration
varies from 500k to 2.5 million steps, depending on the task difficulty. Algorithm performance is evaluated every 10k steps across
32 episodes. Detailed information on all hyperparameters of the
network can be found in Appendix B, and descriptions of each
task are present in Appendix C. The source code of our proposed
algorithm can be reviewed in Appendix D, and the details of the
computing platform are described in Appendix E.

5

RESULTS AND ANALYSIS

In this section, we present the performance of our algorithm compared to baseline methods across two different environments: PredatorPrey and SMAC. We further analyze the coalition formation process
across several tasks, highlighting how our method improves agentsâ€™
cooperation in MARL.

5.1

Figure 2: Learning performance in Predator-Prey: the turns
to catch prey on the test episode

Learning Performance

Predator-Prey. Fig. 2 shows the comparative performance of the
five algorithms in the Predator-Prey environment, focusing on convergence speed, stable performance, and variance. VDN achieves
quick convergence with low variance, yielding stable outcomes, but
its overall performance is moderate. QMIX, while slower to converge, demonstrates the best final performance after 95,000 steps,
capturing the prey in approximately 25 turns, though with greater
fluctuations. SHAQ also exhibits slower convergence and larger
performance variances.
In contrast, our proposed algorithm (nucleolus) combines rapid
convergence with highly efficient capture strategies, significantly
reducing the number of steps needed for successful captures. While
it shows relatively higher variance, the speed and effectiveness

(a) 2s3z

(b) 2c vs 64zg

(c) 3s vs 5z

(d) 5m vs 6m

(e) 8m vs 9m

(f) corridor

Figure 3: Learning performance in SMAC: median test win and rewards for easy task (a), hard (b-e) and super-hard (f) maps.
in learning make it particularly advantageous for time-sensitive
tasks. This positions our method as a strong contender, especially
in scenarios demanding fast adaptation and decision-making.
SMAC. The training curves for all algorithms in SMAC are shown
in Fig. 3, indicating varied performance across six tasks in various
difficulty levels (easy, hard and super-hard).
2s3z: Our algorithm (nucleolus) gradually increases, reaching around
80% after approximately 40,000 timesteps, and eventually approaching 100%, showing stable and fast convergence. OW-QMIX behaves
similarly to ours, ultimately also approaching 100% but slightly
lower than ours. QMIX performs slightly worse, with the final win
rate stabilizing at around 80%. SHAQ has a lower win rate throughout the training, stabilizing at around 45% with high variance.VDN
performs well, with a final win rate of about 85%, though with slight
fluctuations.
2c vs 64zg: Our algorithm (nucleolus) rises rapidly early on, reaching a win rate of over 90% after about 40,000 timesteps and stabilizing near 100%. OW-QMIX and SHAQ have a similar convergence
speed to ours, reaching around an 80% win rate at 40,000 timesteps
and stabilizing at about 90%, slightly lower than ours. QMIX and
VDN converge the slowest, with a final win rate stabilizing around
80%.
3s vs 5z: Our algorithm (nucleolus), OW-QMIX, and VDN perform well, with win rates rapidly increasing after around 300,000
timesteps and reaching approximately 80% at 1,000,000 timesteps.
The final win rate is close to 100%. QMIX lags behind the other three
algorithms, with a win rate reaching around 60% after 1,000,000

timesteps, and eventually approaching 100%. SHAQ converges the
slowest, with the final win rate not exceeding 60%.
5m vs 6m: Our algorithm (nucleolus) converges the fastest, with the
win rate rapidly rising after around 100,000 timesteps and eventually
approaching 100%. OW-QMIX and VDN never exceed a win rate of
40% throughout the training period. QMIX converges slowly and
with high variance but eventually reaches a win rate close to 100%.
SHAQ converges second fastest after ours, with a final win rate
stabilizing around 70%.
8m vs 9m: Our algorithm (nucleolus) has the fastest convergence,
with the win rate increasing rapidly early on, significantly improving after around 30,000 timesteps, and ultimately approaching 80%.
VDN converges more slowly but improves after 30,000 timesteps,
with a final win rate of about 50%. QMIX is relatively stable but
increases more slowly, with the final win rate stabilizing around
40%. SHAQ and OW-QMIX perform worse, with slow win rate
improvements, eventually reaching around 20% at 50,000 timesteps.
Corridor: Our algorithm (nucleolus) converges relatively quickly,
with a final win rate exceeding 90%, making it the best-performing
algorithm. VDN has the fastest convergence, with a final win rate
close to 80%. QMIX, SHAQ, and OW-QMIX all have win rates that
never exceed 20% throughout the training period.
Based on the performance across all tasks above, our algorithm
consistently achieved the highest win rates compared to the other
baseline algorithms, with a superior convergence speed. Notably,
in the 5m vs 6m, 8m vs 9m, and corridor tasks, our algorithm
outperformed the others in terms of final win rates. We attribute

2s3z

3s vs 5z

corridor

Grand Coalition

Multi-Coalition Formation

Coalition Shift I

Coalition Shift II

Figure 4: Visualize multiple coalition formation process in three tasks: 2s3z (easy), 3s vs 5z (hard) and corridor (super-hard)
these tasks are asymmetric, where the number of enemy agents
significantly exceeds the number of our agents, and we found more
difficult tasks require our agents to form small, dispersed alliances
to succeed. In contrast, in the 2c vs 64zg task, although the enemy
agents still outnumber our agents, the smaller number of our agents
makes it difficult to form effective coalitions, which limits the performance advantage compared to other algorithms. This enables
our algorithm to potentially excel in handling more challenging
tasks. As the number of agents increases, it can form more small
coalitions, which may lead to a significantly improved performance

5.2

Multiple Coalition Formations

To further validate the effectiveness of our coalition formation in
MARL, we visualized the coalition process on three challenging
SMAC maps: 2s3z, 3svs 5z, and corridor. Fig. 4 illustrates how
multiple coalitions are formed and adapted to complete a composite
task.
2s3z. Initially, all agents belong to the Grand Coalition. In MultiCoalition Formation, agents form multi-coalition, Coalition 2 (2
Zealots) draws fire on the 2 enemy Stalkers, while Coalition 1 (1
Stalker and 1 Zealot) takes the opportunity to attack the enemy
Zealots, and Coalition 3 (1 Stalker) repositions, looking for a target.
In Coalition Shift I, agents shift coalition structure and form new
coalitions, the remaining Stalker in Coalition 2 attacks the enemy
Stalkers, while the remaining units in Coalition 1 focus fire on the
enemy units other than the Stalkers. Eventually, in Coalition Shift
II, the remaining agents reform into a grand coalition to focus fire
on the remaining enemy Stalkers.
3s vs 5z. At the start, all agents belong to the Grand Coalition. In
Multi-Coalition Formation, agents form multi-coalition, Coalition
2 (1 Stalker) draws all enemy fire, while Coalition 1 (2 Stalkers)
takes the opportunity to attack and eliminate one enemy Zealot. In
Coalition Shift I, agents shift coalition structure and form new coalitions, the strategy is repeated, with Coalition 2 (1 Stalker) drawing

enemy fire, and Coalition 1 (2 Stalkers) focusing on eliminating the
remaining enemy units. As a final point, in Coalition Shift II, the
remaining agents reform into a grand coalition to focus fire on the
remaining enemy Zealots.
Corridor. At the beginning, all agents belong to the Grand Coalition. In Multi-Coalition Formation, agents form multi-coalition, Coalition 1 (2 Zealots) draws most of the enemy fire to create space for
the main force to reposition, while Coalition 2 (2 Zealots) and Coalition 3 (2 Zealots) form a local numerical advantage to focus fire
and eliminate smaller enemy units. In Coalition Shift I, agents shift
coalition structure and form new coalitions, the remaining agents
form a grand coalition to gradually eliminate the remaining enemy
units. Ultimately, in Coalition Shift II, this strategy is repeated until
all enemy units are eliminated.

6

CONCLUSION

We introduced a nucleolus-based credit assignment method to create multiple effective coalitions for cooperative MARL, leveraging
cooperative game theory to enable agents to form smaller, efficient
coalitions to tackle complex tasks. Unlike traditional methods that
rely on a single grand coalition, our method could ensure fair and
stable credit distribution across multiple coalitions. Experiments
in Predator-Prey and StarCraft scenarios showed that our method
consistently outperformed baseline approaches in learning speed,
cumulative rewards, and win rates, demonstrating improvements
in training efficiency, task performance, interpretability, and stability. Future research would focus on scaling nucleolus MARL to
larger multi-agent environments, considering the computational
efficiency of nucleolus calculations.

ACKNOWLEDGMENTS
This study is supported by Australian Research Council (ARC) DECRA Fellowship DE220100265 and ARC Discovery Project DP250103612.

REFERENCES
[1] Haris Aziz, Bo Li, Shiji Xing, and Yu Zhou. 2023. Possible Fairness for Allocating
Indivisible Resources. In Proceedings of the 22nd International Conference on
Autonomous Agents and Multiagent Systems. 197â€“205.
[2] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch.
2018. Emergent Complexity via Multi-Agent Competition. In International Conference on Learning Representations.
[3] Wendelin BÃ¶hmer, Vitaly Kurin, and Shimon Whiteson. 2020. Deep coordination
graphs. In International Conference on Machine Learning. 980â€“991.
[4] Rodica Branzei, Dinko Dimitrov, and Stef Tijs. 2008. Models in cooperative game
theory. Vol. 556.
[5] Trevor Campbell, Luke Johnson, and Jonathan P How. 2013. Multiagent allocation
of markov decision process tasks. In 2013 American Control Conference. IEEE,
2356â€“2361.
[6] Wubing Chen, Wenbin Li, Xiao Liu, Shangdong Yang, and Yang Gao. 2023. Learning explicit credit assignment for cooperative multi-agent reinforcement learning
via polarization policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 37. 11542â€“11550.
[7] Alessandro Farinelli, Antonello Contini, Davide Zorzi, et al. 2020. Decentralized
Task Assignment for Multi-item Pickup and Delivery in Logistic Scenarios..
In Proceedings of the 19th International Conference on Autonomous Agents and
Multiagent Systems. 1843â€“1845.
[8] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 32.
[9] Mukesh Gautam and Mohammed Benidris. 2023. Coalitional Game Theory in
Power Systems: Applications, Challenges, and Future Directions. In 2023 IEEE
Texas Power and Energy Conference. 1â€“6.
[10] Sven Gronauer and Klaus Diepold. 2022. Multi-agent deep reinforcement learning:
a survey. Artificial Intelligence Review 55, 2 (2022), 895â€“943.
[11] Alexandre Heuillet, Fabien Couthouis, and Natalia DÃ­az-RodrÃ­guez. 2021. Explainability in deep reinforcement learning. Knowledge-Based Systems 214 (2021),
106685.
[12] Jian Hu, Siying Wang, Siyang Jiang, and Musk Wang. 2023. Rethinking the
Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent
Reinforcement Learning. In The Second Blogpost Track at The International Conference on Learning Representations.
[13] Shariq Iqbal, Robby Costales, and Fei Sha. 2022. Alma: Hierarchical learning
for composite multi-agent tasks. In Advances in Neural Information Processing
Systems, Vol. 35. 7155â€“7166.
[14] Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin BÃ¶hmer, Shimon
Whiteson, and Fei Sha. 2021. Randomized entity-wise factorization for multiagent reinforcement learning. In International Conference on Machine Learning.
4596â€“4606.
[15] Seung Hyun Kim, Neale Van Stralen, Girish Chowdhary, and Huy T. Tran. 2022.
Disentangling Successor Features for Coordination in Multi-agent Reinforcement
Learning. In Proceedings of the 21st International Conference on Autonomous Agents
and Multiagent Systems. 751â€“760.
[16] Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar. 2021. Coach-player multi-agent reinforcement learning for dynamic team
composition. In International Conference on Machine Learning. 6860â€“6870.
[17] Frans A Oliehoek. 2012. Decentralized pomdps. Reinforcement learning: state-ofthe-art (2012), 471â€“503.

[18] Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. 2008. Optimal and
approximate Q-value functions for decentralized POMDPs. Journal of Artificial
Intelligence Research 32 (2008), 289â€“353.
[19] LA Prashanth and Mohammad Ghavamzadeh. 2016. Variance-constrained actorcritic algorithms for discounted and average reward MDPs. Machine Learning
105 (2016), 367â€“417.
[20] Scott Proper and Prasad Tadepalli. 2009. Solving multiagent assignment markov
decision processes. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 1. 681â€“688.
[21] Anatol Rapoport. 2012. Game theory as a theory of conflict resolution. Vol. 2.
[22] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. 2020.
Weighted qmix: Expanding monotonic value function factorisation for deep
multi-agent reinforcement learning. In Advances in Neural Information Processing
Systems, Vol. 33. 10199â€“10210.
[23] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic value function
factorisation for deep multi-agent reinforcement learning. Journal of Machine
Learning Research 21, 178 (2020), 1â€“51.
[24] Frank Rosenblatt. 1958. The perceptron: a probabilistic model for information
storage and organization in the brain. Psychological review 65, 6 (1958), 386.
[25] Franco Ruggeri, William Emanuelsson, Ahmad Terra, Rafia Inam, and Karl H.
Johansson. 2024. Rollout-based Shapley Values for Explainable Cooperative
Multi-Agent Reinforcement Learning. In 2024 IEEE International Conference on
Machine Learning for Communication and Networking. 227â€“233.
[26] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr,
Jakob Foerster, and Shimon Whiteson. 2019. The StarCraft Multi-Agent Challenge. In Proceedings of the 18th International Conference on Autonomous Agents
and MultiAgent Systems. 2186â€“2188.
[27] David Schmeidler. 1969. The nucleolus of a characteristic function game. SIAM
journal on applied mathematics 17, 6 (1969), 1163â€“1170.
[28] Lloyd S Shapley. 1953. A value for n-person games. Contribution to the Theory of
Games 2 (1953).
[29] Lloyd S Shapley and Martin Shubik. 1971. The assignment game I: The core.
International journal of game theory 1, 1 (1971), 111â€“130.
[30] Alex Sherstinsky. 2020. Fundamentals of recurrent neural network (RNN) and
long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena 404
(2020), 132306.
[31] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo,
Karl Tuyls, and Thore Graepel. 2018. Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward. In Proceedings of the
17th International Conference on Autonomous Agents and Multiagent Systems.
2085â€“2087.
[32] Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. 2019. Reward Constrained
Policy Optimization. In International Conference on Learning Representations.
[33] Jianhong Wang, Yuan Zhang, Yunjie Gu, and Tae-Kyun Kim. 2022. Shaq: Incorporating shapley value theory into multi-agent q-learning. In Advances in Neural
Information Processing Systems, Vol. 35. 5941â€“5954.
[34] Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. 2020. Shapley
Q-value: A local reward approach to solve global reward games. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 34. 7285â€“7292.
[35] Nihat Ã–ner and GÃ¼ltekin Kuyzu. 2022. Nucleolus based cost allocation methods for a class of constrained lane covering games. Computers and Industrial
Engineering 172 (2022), 108583.

