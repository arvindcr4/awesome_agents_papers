{
  "paper_id": "digirl",
  "paper_title": "DigiRL: Device Control",
  "timestamp": "20260104_170139",
  "retry": true,
  "result": {
    "requestId": "aACk1mLJUPDLN1hZjNek",
    "imageUrl": "https://storage.googleapis.com/second-petal-295822.appspot.com/elements/autoDiagram%3A3f0488254c9ec7727fc3a47115b812ee2a4b78bf7e1d9a1f568ebe28c86cb6d6.png",
    "createEraserFileUrl": "https://app.eraser.io/new?requestId=aACk1mLJUPDLN1hZjNek",
    "diagrams": [
      {
        "code": "title Autonomous RL Training\nautoNumber nested\n\n// define participants in display order\nAgent [icon: robot, color: green]\nPolicy Network [icon: brain, color: green]\nReplay Buffer [icon: database, color: purple]\nEnvironment [icon: globe, color: blue]\nReward Function [icon: award, color: gold]\nTraining Loop [icon: loop, color: orange]\nHyperparameter Tuner [icon: gear, color: red]\nEvaluation Module [icon: bar-chart, color: teal]\nLogging Service [icon: file-text, color: grey]\n\n// client starts the training process\nAgent > Training Loop: Initialize training session\nactivate Training Loop\n\nloop [label: episodes, icon: loop, color: orange] {\n  // policy rollout\n  Training Loop > Policy Network: Sample action a\u209c\n  activate Policy Network\n  Policy Network --> Training Loop: Return action a\u209c\n  deactivate Policy Network\n\n  Training Loop > Environment: Execute action a\u209c\n  activate Environment\n  Environment --> Training Loop: New state s\u209c\u208a\u2081\n  deactivate Environment\n\n  // reward computation\n  Training Loop > Reward Function: Compute r\u209c\n  activate Reward Function\n  Reward Function --> Training Loop: Reward r\u209c\n  deactivate Reward Function\n\n  // store transition\n  Training Loop > Replay Buffer: Store (s\u209c, a\u209c, r\u209c, s\u209c\u208a\u2081)\n  activate Replay Buffer\n  deactivate Replay Buffer\n\n  loop [label: minibatch updates, icon: arrows, color: purple] {\n    Training Loop > Replay Buffer: Sample minibatch\n    activate Replay Buffer\n    Replay Buffer --> Training Loop: Batch of transitions\n    deactivate Replay Buffer\n\n    Training Loop > Policy Network: Update network parameters\n    activate Policy Network\n    Policy Network --> Training Loop: Loss & new weights\n    deactivate Policy Network\n  }\n\n  opt [label: hyperparameter scheduling, icon: stopwatch, color: red] {\n    Training Loop > Hyperparameter Tuner: Adjust learning rate\n    activate Hyperparameter Tuner\n    Hyperparameter Tuner --> Training Loop: New learning rate\n    deactivate Hyperparameter Tuner\n  }\n\n  par [label: evaluation & logging, icon: parallel, color: blue] {\n    Training Loop > Evaluation Module: Evaluate current policy\n    activate Evaluation Module\n    Evaluation Module --> Training Loop: Performance metrics\n    deactivate Evaluation Module\n  and [label: metrics logging, icon: file-text, color: grey] {\n    Training Loop > Logging Service: Log metrics\n    activate Logging Service\n    Logging Service --> Training Loop: Ack\n    deactivate Logging Service\n  }\n\n  alt [label: convergence check, icon: check, color: green] {\n    break [label: convergence reached, icon: x, color: red] {\n      Training Loop - Agent: Signal training stop\n    }\n  }\n  else [label: continue training, icon: refresh, color: teal] {\n    Training Loop --> Agent: Request next episode\n  }\n}\n\ndeactivate Training Loop\nAgent <-- Training Loop: Final training results\n}",
        "diagramType": "sequence-diagram"
      }
    ]
  }
}