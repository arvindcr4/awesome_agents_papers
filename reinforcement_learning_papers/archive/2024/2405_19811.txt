Approximate Global Convergence of Independent Learning in
Multi-Agent Systems
Ruiyang Jin∗, 1 , Zaiwei Chen†, 1 , Yiheng Lin†, 2 , Jie Song∗, 2 , and Adam Wierman†, 3

arXiv:2405.19811v1 [cs.LG] 30 May 2024

∗

†

Peking University, 1 jry@pku.edu.edu, 2 jie.song@pku.edu.cn
California Institute of Technology, 1 zchen458@caltech.edu, 2 yihengl@caltech.edu, 3 adamw@caltech.edu
Abstract
Independent learning (IL), despite being a popular approach in practice to achieve
scalability in large-scale multi-agent systems, usually lacks global convergence guarantees. In this paper, we study two representative algorithms, independent Q-learning
and independent natural actor-critic, within value-based and policy-based frameworks,
and provide the first finite-sample analysis for approximate global convergence. The
results imply a sample complexity of Õ(ǫ−2 ) up to an error term that captures the
dependence among agents and characterizes the fundamental limit of IL in achieving
global convergence. To establish the result, we develop a novel approach for analyzing
IL by constructing a separable Markov decision process (MDP) for convergence analysis and then bounding the gap due to model difference between the separable MDP
and the original one. Moreover, we conduct numerical experiments using a synthetic
MDP and an electric vehicle charging example to verify our theoretical findings and
to demonstrate the practical applicability of IL.

1 Introduction
Reinforcement learning (RL) [52] has become a popular learning framework to solve sequential decisionmaking problems in the real world and has achieved great success in applications across different domains,
such as Atari games [40], the game of Go [49], robotics [21], nuclear fusion control [14], etc. While the early
literature on RL predominantly focused on the single-agent setting, motivated by the recent development in
applications involving multi-agent interactions, e.g., in areas such as multiplayer games [60] and active
voltage control [56], multi-agent reinforcement learning (MARL) has received increasing attention.
Compared to single-agent RL, the interplay of agents in MARL presents significant challenges for developing provably efficient learning algorithms. One of those challenges is scalability. Specifically, even when
all agents share an identical interest, any MARL algorithm that uses centralized training becomes computationally intractable as the number of agents increases. To address this scalability issue, various decentralized
MARL algorithms were developed, such as centralized training with decentralized execution (CTDE) [39],
localized MARL in networked systems [65], and independent learning (IL) [32]. Among them, IL has
the minimum requirements on coordination and information sharing, as each agent makes decisions purely
based on its local observations, without collecting any information from other agents. Therefore, increasing
the number of agents does not introduce additional communication or computational complexity in IL.
Although IL is simple, intuitive, and easy to implement, it is unclear whether agents can jointly achieve
any performance guarantees through IL. Note that, in IL, each individual agent overlooks the multi-agent
interactions by treating all other agents as parts of the environment, which is non-stationary due to the actions
taken by other agents. Therefore, except in cases where the underlying model has a special structure, such
as in zero-sum stochastic games [6, 10] or Markov potential games [33], IL, in general, is not guaranteed to
converge [64]. This leads to the following fundamental question:

1

Is it possible to achieve (approximate) global convergence for IL in multi-agent systems?
In this paper, we provide a positive answer to this question in the cooperative setting by establishing
the first finite-sample bounds for the approximate global convergence of IL. In addition, we introduce the
dependence level as a novel notion to capture the fundamental limit of IL. Detailed contributions of this
work are presented below.
• Approximate Global Convergence for Independent Learning. We consider two popular and representative forms of IL, one of which is a value-based algorithm called independent Q-learning (IQL), and the
other is a policy-based algorithm within the actor-critic framework called independent natural actor-critic
(INAC). We establish the first last-iterate finite-sample bounds measured by the global optimality gap. A
key feature is that the bound asymptotically converges within a fixed error term, which is proportional to
the dependence level, denoted by E, of the Markov decision process (MDP) model. Here, the dependence
level E characterizes how close the MARL model is to a model where each agent’s transitions are independent of each other. Our finite-sample bound implies that, up to the aforementioned asymptotic error
term, the sample complexity for the remaining terms to achieve an ǫ-optimality is Õ(ǫ−2 ), which is not
improvable [19].
• A Novel Approach for Analyzing Independent Learning. The main challenge in analyzing IL is that
each agent overlooks the multi-agent interactions by treating other agents as parts of the environment.
To overcome this challenge, we develop a novel approach (depicted in Figure 1) that involves (1) constructing a separable MDP consisting of local transition kernels to approximate the original MDP, (2)
analyzing IL as if they were implemented on the separable MDP, and finally, (3) bounding the error due
to model difference between the separable MDP and the original one to establish the approximate global
convergence.
The proof technique discussed above can be used beyond the analysis of IL. Specifically, in a general
stochastic iterative algorithm, as long as the random process that drives the iterative algorithm can be
approximated by a Markov chain (even though the original random process is not Markovian), our proof
technique can be potentially applied to get finite-sample guarantees. See Appendix A.6 for more details.

Figure 1: Our Roadmap for Analyzing IL.
• Validation of Approximate Global Convergence of IL. We first conduct numerical simulations on a
synthetic MDP to verify the global convergence of IL, and to demonstrate that the asymptotic optimality
gap of IL is determined by the dependence level. Then, we apply IL to an electric vehicle (EV) charging
problem to showcase the practical applicability of our algorithms, where we employ a neural network as a
form of function approximation. Notably, while our theoretical results are established for the tabular setting, our algorithms empirically achieve approximate global convergence under function approximation.

2

1.1 Related Literature
Q-learning and natural actor-critic are two popular and representative algorithms for value-based and policybased methods, respectively. In the following, we discuss the related work in Q-learning, natural actor-critic,
IL, and networked MARL.
Q-Learning. Q-learning was first proposed in [57] as a data-driven variant of the Q-value iteration for
solving the Bellman optimality equation. Due to its popularity, the convergence behavior of Q-learning
has been extensively studied. Specifically, the asymptotic convergence of Q-learning has been studied in
[22, 55] and finite-sample analysis in [9, 17, 35, 44], among many others. Practically, Q-learning has been
successfully employed in many applications in the form of the deep Q-network (DQN), such as games [40]
and control problems [29]. Although Q-learning has also been applied to multi-agent systems [53], to our
knowledge, the theoretical results in this domain are quite limited.
Natural Actor-Critic. In RL, a popular approach for finding an optimal policy is to implement gradientbased methods directly in the policy space [30, 52], a typical example of which is natural actor-critic [4],
where natural policy gradient [25, 59, 61] is used for the policy improvement and TD-learning [3, 50, 51] is
used for the policy evaluation. Finite-sample analysis of natural actor-critic (with linear function approximation) was performed in [1, 8, 27, 31] and the references therein, where the state-of-the-art sample complexity
is Õ(ǫ−2 ). However, they all focused on the single-agent setting. In MARL where each agent independently
performs natural actor-critic, existing results do not directly extend due to multi-agent interactions.
Independent Learning in MARL. IL has been widely applied in various domains, such as power systems [16, 18, 23], communication networks [36, 41, 68], etc. While IL is empirically popular, it has been
demonstrated in [54] that IL may fail for tasks where coordination among agents is needed. Therefore, it is
vital to understand when it is possible for IL to achieve global convergence [5, 64]. Although IL has been
theoretically justified to some extent in certain multi-agent scenarios, such as zero-sum stochastic games
[10, 13] and Markov potential games [15, 33], the results are still lacking for the general setting. In this
work, we establish finite-sample guarantees of IL in the cooperative setting and provide a characterization
of the optimality gap based on the dependence level.
Networked MARL. In networked MARL, each agent engages in information exchange only with its
neighbors through an interaction network. Such localized interaction structure is commonly found in applications that involve social networks, computer networks, traffic networks, etc. [37, 45, 69], and it enables
decentralized decision-making based on local observations and shared information from neighboring agents
[12]. Theoretically, the authors of [65] incorporate a consensus algorithm in the design of their networked
MARL algorithm, and provide convergence analysis under linear function approximation. Many algorithms
and analyses are proposed for the extended settings, such as continuous spaces [63], and stochastic networked MARL [66]. A more detailed review of networked MARL can be found in [64]. The works most
relevant to IL within networked MARL are the scalable algorithms designed using the exponential decay
property [37, 43, 45, 67, 69], where the value functions of each agent have exponentially decaying correlations with agents far away from it. Convergence results with respect to the range of neighbors from which
each agent collects information have been established. However, there are two key differences between our
setting and this line of work: First, we do not impose the assumption of local interaction structure which requires the next local state of an agent to be only affected by the current states of its direct neighbors; Second,
our IL approach only requires local information to implement, while the scalable algorithms in Networked
MARL require communications between agents that are within a certain range.

3

2 Problem Formulation
Consider an MARL problem with n agents. Let S and A be the global state space and the global action
space, respectively. Given i ∈ {1, 2, · · · , n} := [n], let S i (respectively, Ai ) be the state space (respectively,
the action space) of agent i. In this work, we assume that |S||A| < ∞. Let P = {Pa ∈ R|S|×|S| | a ∈ A}
be the set of transition probability matrices indexed by actions, i.e., Pa (s, s′ ) denotes the probability of
transiting to the global state s′ after taking the joint action a at the global state s. For each i ∈ [n], let
Ri : S i × Ai 7→ [0, 1] be the reward function of agent i. Note that restricting the image space of the
reward function to [0, 1] is without loss of generality because we are working with a finite MARL problem.
1 2
n
1 2
n
For any s =
P(s , s ,i· · ·i , si ) ∈ S and a = (a , a , · · · , a ) ∈ A, the total one-stage reward is given by
R(s, a) = i∈[n] R (s , a ). Let γ ∈ (0, 1) be the discount factor, which captures the weight we place on
future rewards. We denote the MDP for the MARL problem as M = (S, A, P, R, γ).
Given a joint policy π : S 7→ ∆(A), where ∆(A) stands for the probability simplex on A, its associated
Q-function Qπ ∈ R|S||A| is defined as
"∞
#
X
k
Qπ (s, a) = Eπ
γ R(Sk , Ak ) S0 = s, A0 = a , ∀ (s, a) ∈ S × A,
k=0

where we use Eπ [ · ] to indicate that the actions are chosen according to the joint policy π. We further define
the value function Vπ ∈ R|S| as Vπ (s) = Ea∼π(·|s) [Qπ (s, a)] for all s ∈ S. Given an initial distribution µ
on the states, denote Vπµ := Es∼µ [Vπ (s)] as the expected value of a policy π. The goal is to find a globally
optimal joint policy π such that Qπ (or equivalently, Vπ , Vπµ ) is uniformly maximized for all (s, a).
For an individual agent i ∈ [n], we also define Qiπ ∈ R|S||A| as the Q-function of agent i given policy π:
"∞
#
X
Qiπ (s, a) = Eπ
γ k Ri (Ski , Aik ) S0 = s, A0 = a , ∀ (s, a) ∈ S × A.
k=0

Note that we have Qπ (s, a) =

P

i
i Qπ (s, a) for all (s, a).

2.1 The Dependence Level

We now introduce the concept of dependence level for the multi-agent MDP model to characterize the
globally coupled state transitions. This is a new notion for analyzing IL that we introduce in this paper.
We first illustrate the concept of separable MDPs, which is crucial for defining the dependence level of the
multi-agent MDP. For any i ∈ [n], let P i be the set of |S i | by |S i | right stochastic matrices, and let
)
(
n
Y
P̂ai (si1 , si2 ), ∀ s1 , s2 ∈ S .
Ẑ = {P̂a }a∈A ∃ P̂ai ∈ P i s.t. P̂a (s1 , s2 ) =
i=1

We call the MDP with transition matrices {P̂a }a∈A ∈ Ẑ a separable MDP. Intuitively, while a separable
MDP is defined on the joint state-action space, it can be decomposed into n small MDPs that involve
independently on their associated local state-action spaces. We next define the dependence level of a multiagent MDP model.
Definition 2.1. An n-agent MDP with transition probability matrices {Pa }a∈A is said to be E-dependent if
and only if
min

max Pa (s, ·) − P̂a (s, ·)

{P̂a }a∈A ∈Ẑ s,a

TV

= E,

where E is called the dependence level of the MDP, and k · kTV is the total variation distance.
4

(1)

Remark. Since the quantity maxs,a kPa (s, ·)− P̂a (s, ·)kTV as a function of {P̂a }a∈A is a continuous function
and Ẑ is a compact set, the min(·) in Eq. (1) is well-defined due to the extreme value theorem [47].

To understand Definition 2.1, suppose that the original MDP M is separable. Then the dependence level
E = 0. In this case, we would expect IL to achieve global convergence because the problem is essentially
to find the optimal policies of n decoupled MDPs. More generally, the dependence level E can be positive,
in which case the original MDP is not separable, and the exact global convergence may not be achievable.
Therefore, the dependence level E serves as a measure of how close the original MDP is to the space of
separable MDPs and captures the fundamental limit of IL for global convergence. Let
P̂ = arg min{P̂a }a∈A ∈Ẑ max Pa (s, ·) − P̂a (s, ·) TV
s,a

and denote M̂ = (S, A, P̂, R, γ), which consists of n decoupled MDPs. We use the hat notation throughout
the paper to denote the counterpart quantities for the separable MDP M̂. For example, we use Q̂π ∈ R|S|×|A|
as the Q-function of a policy π under M̂.

Remark. In this work, although we assumed that the global reward is the summation of each agent’s individual reward for simplicity of presentation, this assumption can be relaxed. Specifically, for each i ∈ [n],
i
i
let Λi be the set of real-valued functions with domain
by defining the reward dependence
PS × Ai . Then,
i
i
level Er as Er = minR̂i ∈Λi ,∀ i∈[n] maxs,a |R(s, a) − i∈[n] R̂ (s , a )|, we can replace the original reward
P
function R(s, a) by i∈[n] R̂i (si , ai ) in our analysis. Note that in this case, the reward-dependence level
Er will also appear in the convergence bounds.

2.2 An Illustrative Example
In this subsection, we provide an illustrative example to further discuss the concept of dependence level.
In multi-agent systems, local information sharing and decision-making coordination can significantly
improve the performance of IL [54]. However, it is challenging to select the pertinent information to communicate and the agents with whom to collaborate. The following example illustrates how different options
of grouping and collaboration can lead to different dependence levels, which significantly affects the performance of IL.

Figure 2: Illustration of the example.
Example 2.1. Consider an MDP consisting of 3 agents. The state space of each agent is {0, 1}, and the
action space of each agent is also {0, 1}. There are different dependencies among the states of the three
5

agents with detailed transition probabilities shown in Appendix B.1. Suppose that we are allowed to group
2 of the 3 agents as one agent, in which case the 2 agents can share information and coordinate with each
other. Then, we have 3 different grouping options shown in Figure 2. After calculating the dependence level
E for each grouping option (cf. Appendix B.1), we have: Option 1 leads to E = 0.5; Option 2 leads to
E = 0.75, and Option 3 leads to E = 0.875.

3 Main Results
This section presents our main results. We first present the IQL and the INAC as representative IL algorithms
in Sections 3.1 and 3.2, respectively. Then, we present the finite-sample approximate global convergence
guarantees for both algorithms in Section 3.3. The proof sketch of our main theorems is provided in Section
3.4, with detailed proof deferred to the appendix.

3.1 Independent Q-Learning
In IQL, each agent treats all other agents as parts of the environment and implements Q-learning on its local
state-action space, as presented in Algorithm 1.
Algorithm 1 Independent Q-Learning (Agent i)
1: Input: An integer K, a behavior policy πbi , and an initialization Qi0 = 0.
2: for k = 0, 1, 2, · · · , K − 1 do
i
.
3:
Implement Aik ∼ πbi (· | Ski ) (simultaneously with all other agents), and observes Sk+1
i
4:
Update Qk according to


i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
Qk+1 (Sk , Ak ) = Qk (Sk , Ak ) + αk R (Sk , Ak ) + γ max Qk (Sk+1 , ā ) − Qk (Sk , Ak ) .
āi

5: end for
6: Output: QiK

Algorithm Details. In IQL, each agent uses a local behavior policy πbi : S i 7→ ∆(Ai ) to interact with the
environment to collect samples (cf. Algorithm 1 Line 3) and updates its local Q-function estimate Qik ∈
i
i
R|S ||A | according to Algorithm 1 Line 4, where αk is the learning rate. Note that this is an asynchronous
update because only one component (i.e., the (Ski , Aik )-th component) of the vector-valued local Q-function
is updated at each step. The update of Q-learning can be viewed as a stochastic approximation algorithm
for solving the Bellman optimality equation. See [2, 52, 57] for more details about Q-learning.
Remark. Unlike single-agent Q-learning, which is driven by a trajectory of Markovian samples, the local
sample trajectory {(Ski , Aik )}k≥0 used for IQL, in general, does not form a Markov chain. Therefore, the
convergence of the local Q-function {Qit,k }k≥0 does not directly follow from the existing results studying
single-agent Q-learning [9, 17, 35, 44], which presents a challenge in the analysis.

3.2 Independent Natural Actor-Critic
The detailed description of the INAC algorithm is presented in Algorithm 2. At a high level, INAC consists
of an actor in the outer loop and a critic in the inner loop. The actor uses independent natural policy
gradient (INPG) to update the policy and the critic uses independent TD-learning (ITD) to estimate the local
Q-function, which is needed for the actor. We next elaborate on the actor and the critic in more detail.
6

Algorithm 2 Independent Natural Actor-Critic (Agent i)
1: Input: Integers K, T , initializations θ0i = 0 and Qit,0 = 0 for all t ≥ 0.
2: for t = 0, 1, 2, · · · , T − 1 do
3:
for k = 0, 1, 2, · · · , K − 1 do
i
4:
Implement Aik ∼ πθi i (· | Ski ) (simultaneously with all other agents), and observes Sk+1
.
t

5:

Update Qit,k according to


Qit,k+1 (Ski , Aik ) = (1 − αk )Qit,k (Ski , Aik ) + αk Ri (Ski , Aik ) + γEai ∼πi (·|S i
(t)

end for
i
θt+1
= θti + ηt Qit,K .
8: end for
9: Output: π i i
θ

6:
7:

k+1


i
i
i
.
,
a
)
(S
Q
t,k k+1
)


T

Independent Natural Policy Gradient for the Actor. Policy gradient [52] is a popular approach for
solving the RL problem. The idea is to perform gradient ascent in the policy space. NPG can be viewed as
a variant of the policy gradient, where the Fisher information matrix is used as a preconditioner [25]. See
[1, 25, 31] for other equivalent formulations and interpretations of NPG.
To present the NPG algorithm, we consider using softmax policies with parameter θ ∈ R|S||A| :
πθ (a | s) = P

exp (θs,a )
, ∀ (s, a) ∈ S × A.
ā∈A exp (θs,ā )

It was shown in [1] that when using softmax policies, NPG takes the following form in the parameter space
(which is also called Q-NPG):
θt+1 = θt + ηt Q(t) ,

(2)

where ηt is the stepsize and we denote Q(t) = Qπθt . In addition, the previous update equation in the
parameter space is equivalent to the following update equation in the policy space [1]:
π(t+1) (a | s) = P

π(t) (a | s) exp{ηt Q(t) (s, a)}
, ∀ (s, a) ∈ S × A,
′
′
a′ ∈A π(t) (a | s) exp{ηt Q(t) (s, a )}

(3)

where we denote π(t) = πθt for simplicity of presentation. However, carrying out the update rule in Eq. (2)
(or Eq. (3)) would require the agents to jointly estimate the global Q-function, which, in general, cannot be
achieved with IL.
i
i
To enable the use of IL, we propose that agent i maintains its own parameter θti ∈ R|S ||A | and updates
it according to
i
i
θt+1
= θti + ηt q̂(t)
,
i

(4)

i

i ∈ R|S ||A | is the local Q-function1 of agent i associated with the policy π
where q̂(t)
(t) under the separable

MDP model M̂. Specifically, given a separable
policy π = (π 1 , π 2 , · · · , π n ) with π i : S i 7→ ∆(Ai ) for
P∞ joint
i
i
i
k
i
any agent i, we define q̂π (s , a ) = Êπ [ k=0 γ R (Ski , Aik ) | S0i = si , Ai0 = ai ] for all (si , ai ) ∈ S i × Ai
Here, we use the notation q̂πi to distinguish with the Q-function Q̂iπ given policy π and agent i, which is defined in the global
state-action space.
1

7

and i ∈ [n], where Êπ [ · ] denotes the expectation with respect to the separable transition kernel P̂ and policy
π. To make sense of Eq. (4), note that Eq. (4) is equivalent to the following update in the policy space:
i
π(t+1)
(ai | si ) = P

i (ai | si ) exp{η q̂ i (si , ai )}
π(t)
t (t)
āi ∈Ai

i (āi | si ) exp{η q̂ i (si , āi )}
π(t)
t (t)

, ∀ (si , ai ) ∈ S i × Ai .

Suppose that the original MDP M itself is separable, in which case we have M =PM̂. Then, for any
s = (s1 , s2 , · · · , sn ) ∈ S and a = (a1 , a2 , · · · , an ) ∈ A, it is clear that Qπ (s, a) = i∈[n] q̂πi (si , ai ). As
a result, when each agent updates its policy parameter θti according to Eq. (4), the joint policy obeys the
following update rule:
π(t+1) (a | s) =
=

n
Y

i
(ai | si )
π(t+1)

i=1
"
n
Y
i=1

=P

P

i (ai | si ) exp{η q̂ i (si , ai )}
π(t)
t (t)

i
i i
i
i
i i
āi ∈Ai π(t) (ā | s ) exp{ηt q̂(t) (s , ā )}

#

π(t) (a | s) exp{ηt Q(t) (s, a)}
, ∀ (s, a) ∈ S × A,
ā∈A π(t) (ā | s) exp{ηt Q(t) (s, ā)}

which is exactly the desired Q-NPG update presented in Eq. (3). In general, when the original n-agent MDP
M is not separable, Eq. (4) can be viewed as an approximation of the Q-NPG update in Eq. (3). Explicitly
characterizing such an approximation error is one of the major technical challenges in the analysis. As we
shall see later, the approximation error will be captured by the dependence level E.
i to carry out the update. To
In view of Eq. (4), each agent needs to estimate its local Q-function q̂(t)
achieve that, we use ITD, which is presented next.
Independent TD-Learning for the Critic. Within each iteration t ∈ {0, 1, · · · , T − 1} of the outer loop,
each agent performs policy evaluation independently according to Algorithm 2 Line 5. Similarly to Qlearning, TD-learning can also be viewed as a stochastic approximation algorithm for solving the Bellman
equation for policy evaluation. See [2, 51, 52] for more details about TD-learning.
i leads to the INAC presented in
Finally, combining the INPG in Eq. (4) with the ITD for estimating q̂(t)
Algorithm 2.

3.3 Finite-Sample Analysis
To present our theoretical results, we first introduce our assumption regarding the MDP model. For any
policy π, state-action pairs (s, a), (s̄, ā) ∈ S × A, and k ≥ 0, let Pπk (s̄, ā, s, a) be the probability of
visiting (s̄, ā) at time step k starting at (s, a) and following the policy π thereafter, that is, Pπk (s̄, ā, s, a) =
P((Sk , Ak ) = (s, a) | (S0 , A0 ) = (s̄, ā)), where Aℓ ∼ π(· | Sℓ ) for all ℓ ∈ {1, 2, · · · , k}.
Assumption 3.1. For any joint policy π, the induced Markov chain {(Sk , Ak )}k≥0 (from the original MDP
M) is irreducible and aperiodic with a unique stationary distribution dπ ∈ ∆(S × A). In addition, we
assume that σ := inf π mins,a dπ (s, a) > 0, and there exist M1 ≥ 0 and M2 ≥ 1 such that
max sup

max

N ⊆S×A π (s̄,ā)∈S×A

X

(s,a)∈N

(dπ (s, a) − Pπk (s̄, ā, s, a))

8



k
≤ M1 exp −
M2



, ∀ k ≥ 0.

In RL, to successfully learn an optimal policy, it is well known that having a sufficient exploration
component is necessary. Assumption 3.1 is imposed to ensure the exploration of RL agents, which states
that for any joint policy π, the agents can sufficiently explore the state-action space. This type of assumption
is commonly imposed in the existing literature studying RL algorithms, especially those with time-varying
sampling policies. See for example [27, 37, 58, 62, 70].
To proceed and state our main results,
P we need to introduce more notation. For any i ∈ [n]i and
i
i
i
i
i
′
i
i
(s̄ , ā ) ∈ S × A , let dπ (s̄ , ā ) =
(s,a)∈S×A,si =s̄i ,ai =āi dπ (s, a). Denote m = maxi∈[n] |S ||A |
i
i
′
′
and σ = inf π mini∈[n],(s̄i ,āi )∈S i ×Ai dπ (s̄ , ā ), which is strictly positive under Assumption 3.1. Next, we
present the finite-sample analysis of IQL.
Theorem 3.1. Consider {Qk }k≥0 generated by Algorithm 1. Suppose that Assumption 3.1 is satisfied and
2
α
with k0 = max(4α, 2M2 log K) and α ≥ σ′ (1−γ)
αk = k+k
. Then, for any δ′ ∈ (0, 1), with probability at
0
′
least 1 − δ , we have


2nCb
2nCa′
8nγE
1
µ
µ
√
+
,
(5)
+
Vπ ∗ − Vπ K ≤
1−γ
K
+
k
(1
− γ)3
K + k0
0
| {z }
|
{z
}
E1 : Q-Learning Convergence Error

E2 : Error due to E

where π∗ is an optimal policy, and πK = (πk1 , πk2 , · · · , πkn ) is the policy greedily induced by
i i i
i
(Q1k , Q2k , · · · , Qnk ), that is, πki (ai | si ) = 1 if and
q only if a = arg maxāi Qk (s , ā ) for all i ∈ [n],
40α
2K
where we break the tie arbitrarily, Ca′ = (1−γ)
M2 log K log 4mnM
+ log log K , and Cb =
2
δ′
′

1 σ (1+2M2 +4α) 2M2 log K+k0
,
}.
8 max{ 144M2 α log K+4M
(1−γ)2 σ′
(1−γ)2

Remark. Since IQL uses a fixed behavior policy πb to collect samples, Assumption 3.1 can be relaxed to the
following weaker assumption: the Markov chain {(Sk , Ak )} induced by πb is irreducible and aperiodic.

The proof of Theorem 3.1 is presented in Appendix A. Observe that the convergence
√ bound presented
in Eq. (5) consists of two terms. The term E1 converges to zero at a rate of Õ(1/ K), which matches
the convergence rate of Q-learning in the single-agent setting [9, 35, 44]. The term E2 is asymptotically
non-vanishing. Note that E2 is proportional to the dependence level E, and captures the fundamental limit
of IQL. In the special case where the original MDP M is separable, E2 vanishes and we have the global
convergence of IQL.
Based on Theorem 3.1, we have the following sample complexity of IQL.
8nγE
Corollary 3.2. Given ǫ > 0, for Algorithm 1 to achieve Vπµ∗ − Vπµk ≤ ǫ + (1−γ)
3 with probability at least

−2
′
2
1 − δ , the sample complexity is Õ ǫ .

As we see from Corollary 3.2, up to a model difference error that is proportional to the dependence level,
Algorithm 1 achieves a sample complexity of Õ(ǫ−2 ) to find a optimal policy. This sample complexity is
known to be optimal due to the existing lower bounds for solving RL problems [19].
Next, we present the finite-sample analysis for INAC.
Theorem 3.3. Consider {π(t) }t≥0 generated by Algorithm 2. Suppose that (1) Assumption 3.1 is satisfied,
2
(2) αk = α/(k + k0 ) with α ≥ σ′ (1−γ)
and k0 = max(4α, 2M2 log K), and (3) ηt satisfies η0 = γ log |A|
2

It was argued in [26] that, given a finite-sample bound with asymptotically non-vanishing terms on the right-hand side (RHS),
the interpretation of the finite-sample bound in terms of sample complexity can be ambiguous, as it is possible to trade-off the
vanishing terms and the non-vanishing terms to obtain ‘better’ sample complexity guarantees. In Corollary 3.2, we present the
sample complexity in this way to allow a fair comparison with the existing literature, as a finite-sample bound with non-vanishing
terms can frequently occur in RL when (1) function approximation is used, (2) off-policy sampling is used, and (3) IL is used as in
our paper.

9

and ηt ≥ 2n log |A|
1 − δ, we have

Pt−1

i=0 ηi /[(1 − γ)γ

2
Qπ∗ − Q(T ) ∞ ≤
(1 − γ)2
|



2t−1 ] for all t ≥ 0. Then, for any δ ∈ (0, 1), with probability at least


nCa
nCb
√
+
+
K + k0 K + k0
{z
}

G1 : TD-Learning Convergence Error

4nγ T −1
(1 − γ)2
| {z }

+

G2 :Actor Convergence Error

40α
where π∗ is an optimal policy, Ca is defined as Ca = (1−γ)
2
and Cb is defined in Theorem 3.1.

q

M2 log K log

8nγE
,
(1 − γ)4
| {z }

(6)

G3 :Error due to E

4mnT M2 K
δ




+ log log K ,

The detailed proof of Theorem 3.3 is deferred to Appendix A. Similarly to Theorem 3.1, the convergence
bound in Theorem 3.3 is composed of terms that converge to 0 asymptotically and a term that is proportional
to the dependence level E. Specifically, the terms G1 and
√ G2 in Eq. (6) represent the convergence error
in ITD for the critic and INPG for the actor. The O(1/ K) convergence rate of G1 and the geometric
convergence of G2 agree with the existing results in the literature on TD-learning [3, 50] and NPG [7, 31].
The term G3 is a constant independent of the number of iterations and captures the model difference error
between the original MDP and the separable one, which is proportional to the dependence level E.
At first glance, it may seem unnatural that our stepsize sequence {ηt } for INAC is increasing (see the
recursive geometric form in Theorem 3.3). To illustrate this, consider the single-agent setting. In view of
the equivalent form of Q-NPG in Eq. (3), it resembles the classical policy iteration (which has geometric
convergence) when ηt approaches infinity. This intuition was theoretically justified in [8, 28, 59], as well as
from the perspective of mirror descent [31]. Although using increasing stepsizes is theoretically justified, in
practice, excessively large stepsizes may discourage exploration. To overcome this practical issue, one can
i with probability
use a more exploration-encouraging variant of INAC, such as choosing actions based on π(t)
1 − ǫ and choosing actions uniformly at random with probability ǫ for some ǫ > 0.
Next, we derive the sample complexity of Algorithm 2 based on Theorem 3.3.
8nγE
Corollary 3.4. Given ǫ > 0, for Algorithm 2 to achieve Qπ∗ − Q(t) ∞ ≤ ǫ + (1−γ)
4 with probability at

−2
least 1 − δ, the sample complexity is Õ ǫ .

Similarly to IQL, we have an Õ(ǫ−2 ) sample complexity for INAC to find a global optimal policy up to
a model difference error.

Comparison with Results for Networked MARL. In the existing literature, the work closest to ours are
those analyzing networked MARL problems. The main idea in networked MARL is that, from each agent’s
perspective, the agents that are far away in graph distance should have a negligible impact on the agent.
This is referred to as the ‘exponential decay property’ in networked MARL [37, 45]. Therefore, by restricting the sharing of information between agents within their κ-hop neighborhood, with a properly chosen κ,
decentralized RL algorithms can achieve scalability without compromising too much on optimality. Compared to [37, 45], our algorithm does not require any information exchange among agents. Furthermore, to
rigorously establish the exponential decay property, certain assumptions must be imposed on the underlying MDP model [37, 45]. In this work, we do not impose structural assumptions on the underlying model
except the one that guarantees exploration (cf. Assumption 3.1), which is commonly made in the existing
literature. While our results are more general and applicable to networked MARL, the usefulness depends
on how small the dependence level is. The local structure in networked MDP (which limits the impact of
far-away agents) may help reduce the dependence level.

10

3.4 Proof Sketch
Our proof follows the roadmap in Figure 1. Next, we present the proof sketch of Theorem 3.3. The proof of
Theorem 3.1 follows a similar approach. The proof consists of the following 3 main steps.
Step 1: Convergence of ITD. The main challenge in analyzing ITD (and also IQL) is that, as a stochastic
approximation algorithm, the randomness in the algorithm comes from the stochastic process {(Ski , Aik )},
which does not necessarily form a Markov chain. Therefore, the existing results on the Markovian stochastic
approximation [11, 50] do not apply directly here. To overcome this challenge, inspired by [37], we model
ITD (and also IQL) as a stochastic approximation algorithm with state aggregation and show that Qit,K
approximates a solution to a variant of the projected Bellman equation, which is denoted as√Q̃it . In the end,
we obtain the following convergence result with high probability: kQit,K − Q̃it k∞ ≤ Õ(1/ K).
Step 2: Global Convergence on the Separable MDP. Following our blueprint described in Figure 1, we
analyze INAC as if it were implemented in the separable MDP M̂. Then, we take a further step for ITD to
i k
prove kQ̃it − q̂(t)
∞ ≤ O(E) using the definition of the dependence level. Altogether, we obtain the approx√
P
imate convergence for ITD: kQt,K − Q̂(t) k∞ ≤ Õ(1/ K) + O(E), where Qt,k (s, a) = i∈[n] Qit,k (si , ai )

for all (s, a) ∈ S × A, and Q̂(t) is the Q-function of the policy π(t) on M̂. Combining the results for ITD
and INPG, we have the following result for INAC:
√
(7)
kQ(T ) − Q̂π̂∗ k∞ ≤ Õ(1/ K) + O(γ T ) + O(E)

with high probability, where π̂∗ is an optimal policy of M̂.
Step 3: Bounding the Model Difference Error. With the approximate convergence to the optimal Qfunction of M̂, the last step is to bound the gap due to the model difference to get the approximate global
convergence of the original MDP. In fact, we have kQπ∗ − Q̂π̂∗ k∞ ≤ O(E), where we recall that E is the
dependence level. Combining the above inequality with Eq. (7) finishes the proof.

4 Numerical Simulations
Our last technical section presents numerical experiments for IL. First, we present the results of INAC and
IQL applied to the synthetic MDP discussed in Section 2.2 to illustrate the effects of the dependence level.
In Appendix B.3.3, we apply IQL and INAC to an EV charging problem to demonstrate that our algorithms
can be extended to the function approximation setting with approximate global convergence as well.
We run IQL and INAC with the synthetic MDP 100 times, respectively. The rewards are normalized by
the optimal average reward and shown in Figure 3, where the shaded areas denote the standard deviation.
From Figure 3, we see that both IQL and INAC with Option 1 can achieve better performance than Options
2 and 3, which justifies the convergence results in Theorems 3.1 and 3.3, where the optimality gap of IL is
controlled by the dependence levels. Comparing IQL with INAC, we find that IQL can achieve a smaller gap
than INAC with any option, which is also consistent with our theoretical results where the asymptotically
non-vanishing term (cf. E2 ) in IQL is smaller than the corresponding term G3 in INAC by a factor of
1/(1 − γ).

5 Conclusion
In this paper, we investigate the theoretical merit of IL for MARL in the cooperative setting and establish
approximate global convergence for 2 representative algorithms: IQL and INAC, both of which achieve
11

Figure 3: Performance of IQL and INAC
an Õ(ǫ−2 ) sample complexity to find a global optimal policy up to an error term that is proportional to the
dependence level. Methodologically, we propose a new method for analyzing IL by constructing a separable
MDP, where each agent has an independent local state transition model. The model difference between the
original MDP and the separable one is captured by the dependence level. Our numerical experiments justify
the theoretical findings.
There are many interesting directions for future work. First, it is worth investigating if our proof technique can be used to study other problems beyond IL, such as general non-Markovian stochastic iterative
algorithms. Second, our analysis suggests that carefully adding coordination and information sharing may
reduce the dependence level of the model while maintaining the scalability of the algorithm. However,
theoretically characterizing the trade-off between scalability and optimality is still an open question.

References
[1] Agarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. (2021). On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research,
22(1):4431–4506.
[2] Bertsekas, D. and Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena Scientific.
[3] Bhandari, J., Russo, D., and Singal, R. (2018). A finite-time analysis of temporal difference learning
with linear function approximation. In Conference On Learning Theory, pages 1691–1692.
[4] Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. (2009). Natural actor-critic algorithms.
Automatica, 45(11):2471–2482.
[5] Busoniu, L., Babuska, R., and De Schutter, B. (2008). A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
38(2):156–172.
[6] Cai, Y., Luo, H., Wei, C.-Y., and Zheng, W. (2024). Uncoupled and convergent learning in two-player
zero-sum markov games with bandit feedback. Advances in Neural Information Processing Systems, 36.

12

[7] Chen, Z. and Maguluri, S. T. (2022a). An approximate policy iteration viewpoint of actor-critic algorithms. reprint arXiv:2208.03247.
[8] Chen, Z. and Maguluri, S. T. (2022b). Sample complexity of policy-based methods under off-policy
sampling and linear function approximation. In International Conference on Artificial Intelligence and
Statistics, pages 11195–11214. PMLR.
[9] Chen, Z., Maguluri, S. T., Shakkottai, S., and Shanmugam, K. (2023). A Lyapunov theory for finitesample guarantees of Markovian stochastic approximation. Operations Research.
[10] Chen, Z., Zhang, K., Mazumdar, E., Ozdaglar, A., and Wierman, A. (2024). A finite-sample analysis
of payoff-based independent learning in zero-sum stochastic games. Advances in Neural Information
Processing Systems, 36.
[11] Chen, Z., Zhang, S., Doan, T. T., Clarke, J.-P., and Maguluri, S. T. (2022). Finite-sample analysis of
nonlinear stochastic approximation with applications in reinforcement learning. Automatica, 146:110623.
[12] Chu, T., Chinchali, S., and Katti, S. (2020). Multi-agent reinforcement learning for networked system
control. In International Conference on Learning Representations.
[13] Daskalakis, C., Foster, D. J., and Golowich, N. (2020). Independent policy gradient methods for
competitive reinforcement learning. Advances in neural information processing systems, 33:5527–5540.
[14] Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R.,
Abdolmaleki, A., de Las Casas, D., et al. (2022). Magnetic control of tokamak plasmas through deep
reinforcement learning. Nature, 602(7897):414–419.
[15] Ding, D., Wei, C.-Y., Zhang, K., and Jovanovic, M. (2022a). Independent policy gradient for largescale markov potential games: Sharper rates, function approximation, and game-agnostic convergence.
In International Conference on Machine Learning, pages 5166–5220. PMLR.
[16] Ding, L., Lin, Z., Shi, X., and Yan, G. (2022b). Target-value-competition-based multi-agent deep
reinforcement learning algorithm for distributed nonconvex economic dispatch. IEEE Transactions on
Power Systems, 38(1):204–217.
[17] Even-Dar, E., Mansour, Y., and Bartlett, P. (2003). Learning rates for Q-learning. Journal of machine
learning Research, 5(1).
[18] Foruzan, E., Soh, L.-K., and Asgarpoor, S. (2018). Reinforcement learning approach for optimal
distributed energy management in a microgrid. IEEE Transactions on Power Systems, 33(5):5749–5758.
[19] Gheshlaghi Azar, M., Munos, R., and Kappen, H. J. (2013). Minimax PAC bounds on the sample
complexity of reinforcement learning with a generative model. Machine learning, 91:325–349.
[20] Gosavi, A. (2006). Boundedness of iterates in Q-learning. Systems & control letters, 55(4):347–349.
[21] Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and
automation (ICRA), pages 3389–3396. IEEE.
[22] Jaakkola, T., Jordan, M. I., and Singh, S. P. (1994). Convergence of stochastic iterative dynamic
programming algorithms. In Advances in neural information processing systems, pages 703–710.

13

[23] Jin, R., Zhou, Y., Lu, C., and Song, J. (2022). Deep reinforcement learning-based strategy for charging
station participating in demand response. Applied Energy, 328:120140.
[24] Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In
Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–274.
[25] Kakade, S. M. (2001). A natural policy gradient. Advances in neural information processing systems,
14.
[26] Khodadadian, S., Chen, Z., and Maguluri, S. T. (2021). Finite-sample analysis of off-policy natural
actor-critic algorithm. In International Conference on Machine Learning, pages 5420–5431. PMLR.
[27] Khodadadian, S., Doan, T. T., Romberg, J., and Maguluri, S. T. (2022a). Finite sample analysis of
two-time-scale natural actor-critic algorithm. IEEE Transactions on Automatic Control.
[28] Khodadadian, S., Jhunjhunwala, P. R., Varma, S. M., and Maguluri, S. T. (2022b). On linear and
super-linear convergence of natural policy gradient algorithm. Systems & Control Letters, 164:105214.
[29] Kiumarsi, B., Lewis, F. L., Modares, H., Karimpour, A., and Naghibi-Sistani, M.-B. (2014). Reinforcement Q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics.
Automatica, 50(4):1167–1175.
[30] Konda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in neural information
processing systems, pages 1008–1014. Citeseer.
[31] Lan, G. (2023). Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. Mathematical programming, 198(1):1059–1106.
[32] Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat, J., Silver, D., and Graepel,
T. (2017). A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural
information processing systems, 30.
[33] Leonardos, S., Overman, W., Panageas, I., and Piliouras, G. (2022). Global convergence of multi-agent
policy gradient in Markov potential games. In International Conference on Learning Representations.
[34] Levin, D. A. and Peres, Y. (2017). Markov chains and mixing times, volume 107. American Mathematical Soc.
[35] Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2023). Is Q-learning minimax optimal? a tight sample
complexity analysis. Operations Research.
[36] Liang, L., Ye, H., and Li, G. Y. (2019). Spectrum sharing in vehicular networks based on multi-agent
reinforcement learning. IEEE Journal on Selected Areas in Communications, 37(10):2282–2292.
[37] Lin, Y., Qu, G., Huang, L., and Wierman, A. (2021). Multi-agent reinforcement learning in stochastic
networked systems. Advances in neural information processing systems, 34:7825–7837.
[38] Littman, M. L. (2009). A tutorial on partially observable Markov decision processes. Journal of
Mathematical Psychology, 53(3):119–125.
[39] Lowe, R., Wu, Y. I., Tamar, A., Harb, J., Pieter Abbeel, O., and Mordatch, I. (2017). Multi-agent
actor-critic for mixed cooperative-competitive environments. Advances in neural information processing
systems, 30.

14

[40] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement
learning. nature, 518(7540):529–533.
[41] Nguyen, K. K., Duong, T. Q., Vien, N. A., Le-Khac, N.-A., and Nguyen, L. D. (2019). Distributed
deep deterministic policy gradient for power allocation control in D2D-based V2V communications.
IEEE Access, 7:164533–164543.
[42] Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons.
[43] Qu, G., Lin, Y., Wierman, A., and Li, N. (2020). Scalable multi-agent reinforcement learning for
networked systems with average reward. Advances in Neural Information Processing Systems, 33:2074–
2086.
[44] Qu, G. and Wierman, A. (2020). Finite-time analysis of asynchronous stochastic approximation and
Q-learning. In Conference on Learning Theory, pages 3185–3205. PMLR.
[45] Qu, G., Wierman, A., and Li, N. (2022). Scalable reinforcement learning for multiagent networked
systems. Operations Research, 70(6):3601–3628.
[46] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The annals of mathematical
statistics, pages 400–407.
[47] Rudin, W. et al. (1976). Principles of mathematical analysis, volume 3. McGraw-hill New York.
[48] Sadeghianpourhamami, N., Deleu, J., and Develder, C. (2019). Definition and evaluation of modelfree coordination of electrical vehicle charging with reinforcement learning. IEEE Transactions on Smart
Grid, 11(1):203–214.
[49] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,
L., Lai, M., Bolton, A., et al. (2017). Mastering the game of Go without human knowledge. Nature,
550(7676):354.
[50] Srikant, R. and Ying, L. (2019). Finite-time error bounds for linear stochastic approximation and
TD-learning. In Conference on Learning Theory, pages 2803–2830.
[51] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine learning,
3(1):9–44.
[52] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
[53] Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente, R. (2017).
Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4):e0172395.
[54] Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330–337.
[55] Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine learning,
16(3):185–202.
[56] Wang, J., Xu, W., Gu, Y., Song, W., and Green, T. C. (2021). Multi-agent reinforcement learning
for active voltage control on power distribution networks. Advances in Neural Information Processing
Systems, 34:3271–3284.
15

[57] Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8:279–292.
[58] Wu, Y. F., Zhang, W., Xu, P., and Gu, Q. (2020). A finite-time analysis of two time-scale actor-critic
methods. Advances in Neural Information Processing Systems, 33:17617–17628.
[59] Xiao, L. (2022). On the convergence rates of policy gradient methods. The Journal of Machine
Learning Research, 23(1):12887–12922.
[60] Yang, Y. and Wang, J. (2020). An overview of multi-agent reinforcement learning from game theoretical perspective. Preprint arXiv:2011.00583.
[61] Yuan, R., Du, S. S., Gower, R. M., Lazaric, A., and Xiao, L. (2022). Linear convergence of natural
policy gradient methods with log-linear policies. In The Eleventh International Conference on Learning
Representations.
[62] Zeng, S., Doan, T. T., and Romberg, J. (2022). Finite-time complexity of online primal-dual natural
actor-critic algorithm for constrained Markov decision processes. In 2022 IEEE 61st Conference on
Decision and Control (CDC), pages 4028–4033. IEEE.
[63] Zhang, K., Yang, Z., and Basar, T. (2018a). Networked multi-agent reinforcement learning in continuous spaces. In 2018 IEEE conference on decision and control (CDC), pages 2771–2776. IEEE.
[64] Zhang, K., Yang, Z., and Başar, T. (2021a). Multi-agent reinforcement learning: A selective overview
of theories and algorithms. Handbook of reinforcement learning and control, pages 321–384.
[65] Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018b). Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pages
5872–5881. PMLR.
[66] Zhang, K., Yang, Z., Liu, H., Zhang, T., and Başar, T. (2021b). Finite-sample analysis for decentralized batch multiagent reinforcement learning with networked agents. IEEE Transactions on Automatic
Control, 66(12):5925–5940.
[67] Zhang, Y., Qu, G., Xu, P., Lin, Y., Chen, Z., and Wierman, A. (2023). Global convergence of localized policy iteration in networked multi-agent reinforcement learning. Proceedings of the ACM on
Measurement and Analysis of Computing Systems, 7(1):1–51.
[68] Zhong, C., Gursoy, M. C., and Velipasalar, S. (2019). Deep multi-agent reinforcement learning based
cooperative edge caching in wireless networks. In ICC 2019-2019 IEEE International Conference on
Communications (ICC), pages 1–6. IEEE.
[69] Zhou, Z., Chen, Z., Lin, Y., and Wierman, A. (2023). Convergence rates for localized actor-critic
in networked Markov potential games. In Evans, R. J. and Shpitser, I., editors, Proceedings of the
Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceedings of Machine
Learning Research, pages 2563–2573. PMLR.
[70] Zou, S., Xu, T., and Liang, Y. (2019). Finite-sample analysis for SARSA with linear function approximation. Advances in neural information processing systems, 32.

16

Appendices
A

Proofs of Theorem 3.1 and Theorem 3.3

It is intractable to directly analyze IL using existing off-the-shelf RL theory since almost all of them require
the samples to be Markovian or independent and identically distributed (i.i.d.), which is not the case for local
state-actions that are essentially partial observations from the overall MDP model. Fortunately, note that
there are some specific connections between IL and stochastic approximation with state aggregation, where
an agent maintains an estimated vector with a smaller size than that of the global state space. Therefore,
we will take a first step by showing that the update of IQL and ITD can be equivalently formulated as
a stochastic approximation algorithm with state aggregation and derive that the corresponding algorithm
converges to a fixed point. We further bound the error between the fixed point and the target Q-function on
the separable MDP. Finally, we bound the optimality gap between the original and separable MDPs to finish
the analysis.

A.1 Connections with Stochastic Approximation with State Aggregation
We follow the proof idea described in Section 3.4 by first deriving the convergence of IQL and the inner loop
of INAC, which is ITD. To make this paper self-contained, we present the results of stochastic approximation
with state aggregation [37] in Appendix A.5. Here, we show that IQL and ITD are special cases of such a
stochastic approximation algorithm.
For stochastic approximation with state aggregation (see Appendix A.5), the agent only maintains a
vector of function values the entries of which are abstract states with a much smaller size than that of the
original state space. A surjection is defined as a mapping from the original state space to the abstract state
space to decide which entry should be updated at each step.
In IQL (cf. Algorithm 1) and ITD (cf. Algorithm 3), each agent cannot estimate the Q-function for
entries of global state-action pairs only with visibility of local states and actions. Agent i only maintains a
i
i
local Q-function Qik ∈ R|S ||A | . Note that the notation of local Q-function is Qit,k for ITD in INAC, but the
subscription t is omitted here for simplicity (due to the nested-loop structure of the algorithm). Furthermore,
agent i uses Qik (s̄i , āi ) to approximate the Q-function of global state-action (s, a) when (s̄i , āi ) ∈ S i ×
Ai , (s, a) ∈ S × A and (si , ai ) = (s̄i , āi ). Therefore, IQL and ITD can be both seen as a special form
of stochastic approximation with state aggregation in a very natural way. Specifically, define the surjection
h1 : S 7→ S i and h2 : A 7→ Ai as
h1 (s) = si ,
h2 (a) = ai ,
for all s ∈ S and a ∈ A. That means when the global state-action is (s, a), agent i uses the surjections h1 (s)
i
i
and h2 (a) to decide which local entry should be updated. Formally, define the matrix Φi ∈ R|S||A|×|S ||A |
as
(
1, if h1 (s) = s̄i , h2 (a) = āi ,
∀ (s, a) ∈ S × A, (s̄i , āi ) ∈ S i × Ai .
Φi (s, a, s̄i , āi ) =
0, otherwise,
The update of IQL and ITD for any agent i ∈ [n] can be written as a general stochastic approximation
algorithm:
Qik+1 (h1 (Sk ), h2 (Ak )) = Qik (h1 (Sk ), h2 (Ak ))



+ αk F i (Φi Qik ) (Sk , Ak ) − Qik (h1 (Sk ), h2 (Ak )) + wki ,

(8)

and Qik+1 (si , ai ) = Qik (si , ai ) for all (si , ai ) 6= (h1 (Sk ), h2 (Ak )). Here in Eq. (8), F i : R|S||A| → R|S||A|
is an operator and can be defined differently for IQL and ITD. Note that {(Sk , Ak )}k≥0 is sampled using
17

different policies, i.e., πb for IQL and π(t) for ITD. For simplicity, here we do not distinguish them in
notation with the same assumption (Assumption 3.1) on them.
For IQL, the operator F i (·) is the Bellman optimality operator, which is defined for any Q ∈ R|S||A| as


[F i (Q)](s, a) = Ri (si , ai ) + γEs̄∼Pa (s,·) max Q(s̄, ā) , ∀ (s, a) ∈ S × A.
(9)
ā∈A

Consequently, the noise sequence wki for IQL is defined as
wki = Ri (Ski , Aik ) + γ max Qik (h1 (Sk+1 ), h2 (ā)) − [F i (Φi Qik )](Sk , Ak ).
ā

(10)

For ITD, we define the operator F i (·) for any Q ∈ R|S||A| as

[F i (Q)](s, a) = Ri (si , ai ) + γEs̄∼Pa (s,·),ā∼π(·|s̄) [Q(s̄, ā)] , ∀ (s, a) ∈ S × A,

(11)

wki = Ri (Ski , Aik ) + γEā∼π(·|Sk+1 ) [Qik (h1 (Sk+1 ), h2 (ā))] − [F i (Φi Qik )](Sk , Ak ).

(12)

where π = π(t) is the policy within the t-th outer loop in Algorithm 2. In ITD, the noise sequence wki is
defined as

After reformulating IQL and ITD as stochastic approximation with state aggregation, we will present the
convergence results of IQL and ITD using [37, Theorem 3.1]. We first introduce the following lemma that
provides the required conditions for our results. Note that we only consider the ℓ∞ norm here. Define Fk as
the σ-algebra generated by (S0 , A0 , · · · , Sk , Ak ).

Lemma A.1. Suppose that Assumption 3.1 is satisfied. Let F i (·) be defined as in Eq. (9) (or Eq. (11)), and
let wki be defined in Eq. (10) (or Eq. 12). Then, we have the following results.
(1) The operator F i (·) is a γ-contraction mapping with respect to k·k∞ . In addition, we have kF i (Q)k∞ ≤
γkQk∞ + 1 for all Q ∈ R|S||A|.
(2) The random process {wki } is measurable with respect to Fk+1 , and satisfies E[wki | Fk ] = 0. In
addition, we have |wki | ≤ 2/(1 − γ) almost surely for all k ≥ 0.

Proof of Lemma A.1. (1) The contraction mapping follows from standard results in MDP theory [42]. Now,
for any Q ∈ R|S||A|, we have
kF i (Q)k∞ ≤ kF i (Q) − F i (0)k∞ + kF i (0)k∞ ≤ γkQk∞ + 1,

where the last inequality follows from the fact that F i (·) is a contraction mapping and kF i (0)k∞ ≤
max(si ,ai )∈S i ×Ai Ri (si , ai ) ≤ 1.
(2) The fact that wki is measurable with respect to Fk follows from the definition of wki . We next show that
wki is conditionally mean zero. For IQL, we have
i
h
E[wki | Fk ] = E Ri (Ski , Aik ) + γ max Qik (h1 (Sk+1 ), h2 (ā)) Fk − [F i (Φi Qik )](Sk , Ak )]
ā
i
h
i
i
i
= R (Sk , Ak ) + γEs̄∼PA (Sk ,·) max Qik (s̄, ā) − [F i (Φi Qik )](Sk , Ak )
ā

k

= 0,

where the second equality follows from the Markov property and the tower property of conditional
expectations. The proof for E[wki | Fk ] = 0 in the case of ITD follows from an identical approach.

Finally, using the boundedness of Q-functions in finite MDPs [20], i.e., kQik k∞ ≤ 1/(1 − γ), we have
|wki | ≤ 2Ri (Ski , Aik ) + 2γkQik k∞ ≤ 2 + 2γ/(1 − γ) = 2/(1 − γ).

18

The results in Lemma A.1, together with Assumption 3.1, provide the conditions required for applying
[37, Theorem 3.1]. Next, we present the convergence results of IQL and ITD. We will show that the stochastic approximation in Eq. (8) converges to the unique fixed point of F i (·), denoted by Q̃i∗ for IQL and Q̃i for
ITD. Specifically, Q̃i∗ is the unique solution to the following equation:
Π1 F i (Φi x) = x,
where Π1 := ((Φi )T D1 Φi )−1 (Φi )T D1 with D1 ∈ R|S||A|×|S||A| := diag(dπb ) and F i (·) is defined in
Eq. (9). As for ITD, Q̃i is the unique solution to the equation
Π2 F i (Φi x) = x,
where Π2 := ((Φi )T D2 Φi )−1 (Φi )T D2 with D2 ∈ R|S||A|×|S||A| := diag(dπ(t) ) and F i (·) is defined in
Eq. (11). Note that Q̃i and Q̃i∗ are well defined because the operator Π1 F i (Φi ·) and Π2 F i (Φi ·) are also
contraction mappings with respect to k · k∞ [37, Proposition C.1]. The convergence results of IQL and ITD
are stated below.
α
with k0 = max(4α, 2M2 log K)
Theorem A.1. Suppose that Assumption 3.1 is satisfied, and αk = k+k
0
2
and α ≥ σ′ (1−γ) . Then, we have the following convergence bounds.

(1) For {Qk }k≥0 generated by IQL (cf. Algorithm 1), we have with probability at least 1 − δ′ /n that


1
Cb
Ca′
i
i
= Õ √
+
,
QK − Q̃∗
≤√
∞
K + k0 K + k0
K
(2) For {Qk }k≥0 generated by ITD (cf. Algorithm 3), we have with probability at least 1 − δ′ /n that


1
Cb
Ca′
i
i
+
= Õ √
,
QK − Q̃
≤√
∞
K + k0 K + k0
K
where Ca′ and Cb were defined in Theorem 3.1.
From Theorem A.1, we see that both IQL and ITD converge, which is not a straightforward result for
IL. In the following sections, we will use the results in Theorem A.1 to continue the proof of Theorem 3.1
and Theorem 3.3.

A.2 Proof of Theorem 3.1
P

i i i
i∈[n] Qk (s , a ) for all (s, a) ∈ S × A. Denote the optimal Q-function of agent i under
the separable MDP model M̂ as Q̂iπ̂∗ .

Let Qk (s, a) =

Theorem A.2. Consider {Qk }k≥0 generated by Algorithm 1. Suppose that Assumption 3.1 is satisfied, and
2
α
with k0 = max(4α, 2M2 log K) and α ≥ σ′ (1−γ)
. Then, with probability at least 1 − δ′ , we
αk = k+k
0
have
nCb
2nγE
nCa′
+
+
,
QK − Q̂π̂∗
≤√
∞
K + k0 K + k0 (1 − γ)2
where the terms Ca′ , Cb were defined in Theorem 3.1.

Instead of directly considering the approximate convergence to the optimal Q- function of π∗ on the
original MDP M, we first bound the difference between QK and the optimal Q-function on the separable
MDP M̂ in Theorem A.2. Our approach follows the roadmap illustrated in Figure 1 to overcome the
challenge of studying the convergence of IQL in the original MDP.
19

Proof of Theorem A.2. We first apply Theorem A.1 to obtain
QiK − Q̃i∗

Cb
C′
+
= Õ
≤√ a
∞
K + k0 K + k0



1
√
K



with probability at least 1 − δ′ /n. It remains to bound the difference between Φi Q̃i∗ and Q̂π̂i ∗ .
2γE
Lemma A.2 (Proof in Appendix A.4.1). It holds that kΦi Q̃i∗ − Q̂iπ̂∗ k∞ ≤ (1−γ)
2.

Combining the above results, we have with probability at least 1 − δ′ that
QK − Q̂π̂∗

∞

X

= max
s,a

≤ max
s,a

≤ max
s,a

≤
≤

i∈[n]

X

i∈[n]

i∈[n]

i∈[n]

X

Q̂iπ̂∗ (s, a)

i∈[n]

QiK (si , ai ) − Q̃i∗ (si , ai ) + (Φi Q̃i∗ )(s, a) − Q̂iπ̂∗ (s, a)

QiK − Q̃i∗
√

X

QiK (si , ai ) − Q̂iπ̂∗ (s, a)

X

X

i∈[n]

QiK (si , ai ) −

+ (Φi Q̃i∗ ) − Q̂π̂i ∗

∞

Ca′
2γE
Cb
+
+
K + k0 K + k0 (1 − γ)2



∞





(13a)

(13b)

2nγE
nCb
nCa′
+
,
+
=√
K + k0 K + k0 (1 − γ)2
where Eq. (13a) follows from the fact that (Φi Q̃i∗ )(s, a) = Q̃i∗ (si , ai ) and Eq. (13b) follows from Lemma
A.2 and Theorem A.1. The probability 1 − δ′ comes from the union bound.
Finally, to characterize the difference between the optimal Q-functions of the original MDP and the
separable one, we need the following lemma.
Lemma A.3 (Proof in Appendix A.4.2). Let Qπ∗ and Q̂π̂∗ be the optimal Q-functions under the original
MDP model M and the separable one M̂. Then, we have
Qπ∗ − Q̂π̂∗

∞

≤

2nγE
.
(1 − γ)2

Using Lemma A.3, we have with probability at least 1 − δ′ that
nCa′
4nγE
nCb
kQK − Qπ∗ k∞ ≤ kQK − Q̂π̂∗ k∞ + kQπ∗ − Q̂π̂∗ k∞ ≤ √
+
.
+
K + k0 K + k0 (1 − γ)2
To proceed and obtain the optimality gap in terms of policies, recall that we defined πK as the policy greedily
induced by QK . Denote aK,s = maxa QK (s, a) and a∗,s = maxa Qπ∗ (s, a). When aK,s 6= a∗,s , using the
previous inequality, we have
|Qπ∗ (s, aK,s ) − Qπ∗ (s, π∗,s )| ≤ √

20

2nCb
8nγE
2nCa′
+
+
.
K + k0 K + k0 (1 − γ)2

Define the advantage function as Aπ (s, a) = Qπ (s, a) − Vπ (s) for any policy π and (s, a) ∈ S × A. Then,
we have
|Aπ∗ (s, aK,s )| = |Qπ∗ (s, aK,s ) − Vπ∗ (s)|

= |Qπ∗ (s, aK,s ) − Qπ∗ (s, a∗,s )|

8nγE
2nCa′
2nCb
+
.
≤√
+
K + k0 K + k0 (1 − γ)2
Using the performance difference lemma [24], with probability at least 1 − δ′ , we have for any initial state
distribution µ that


2nCa′
2nCb
1
8nγE
1
µ
µ
√
Es∼oµπ Aπ∗ (s, aK,s ) ≤
+
,
+
Vπ ∗ − Vπ K =
K
1−γ
1−γ
(1 − γ)3
K + k0 K + k0
P
t
where oµπK is the occupancy measure defined as oµπK (s) = (1 − γ)EπK [ ∞
k=0 γ P(Sk = s | S0 ∼ µ)] for
all s ∈ S. The proof is complete.

A.3 Proof of Theorem 3.3
To prove Theorem 3.3, we first by analyzing the convergence error of the critic in Appendix A.3.1. Then we
analyze the convergence rate of the actor in Appendix A.3.2. Finally, we combine the analysis of the actor
and the critic to finish the proof.
A.3.1 Analysis of the Critic
For simplicity of presentation, we first write down only the inner loop of Algorithm 2 in the following,
where we may omit the subscript t. The results we derive for the inner loop can be easily combined with the
analysis of the outer loop using the Markov property.
Algorithm 3 Inner Loop of Algorithm 2
1: Input: Integer K, policy π i := πθi i from the outer loop, and initialization Qi0 = 0.
2: for k = 0, 1, 2, · · · , K − 1 do
i
3:
Implement Aik ∼ π i (· | Ski ) (simultaneously with all other agents), and observes Sk+1
.
4:
Update Q-function:
i
, āi ))
5:
Qik+1 (Ski , Aik ) = (1 − αk )Qik (Ski , Aik ) + αk (Ri (Ski , Aik ) + γEāi ∼πi (·|S i ) Qik (Sk+1
k+1
6: end for
Next, we provide the approximate convergence of the critic.
Theorem A.3. Consider {Qk }k≥0 generated by Algorithm 3 with input policy π = (π 1 , π 2 , · · · , π n ). Supα
2
.
with k0 = max(4α, 2M2 log K) and α ≥ σ′ (1−γ)
pose that Assumption 3.1 is satisfied, and αk = k+k
0
′
Then, with probability at least 1 − δ , we have
QK − Q̂π

∞

≤√

nCa′
2nγE
nCb
+
,
+
K + k0 K + k0 (1 − γ)2

where the constants Ca′ and Cb were defined in Theorem 3.1.

21

Proof of Theorem A.3. We have verified that ITD is a special case of the stochastic approximation algorithm
with state aggregation. Therefore, apply Theorem A.1 and we get


1
Cb
Ca′
i
i
+
= Õ √
QK − Q̃
≤√
∞
K + k0 K + k0
K
Our next step is to bound the gap between Φi Q̃i and Q̂iπ .
2γE
Lemma A.4 (Proof in Appendix A.4.3). It holds that kΦi Q̃i − Q̂iπ k∞ ≤ (1−γ)
2.

It follows from Lemma A.4 and Theorem A.3 that with probability at least 1 − δ′ , we have
kQK − Q̂π k∞ = max
s,a

≤ max
s,a

≤ max
s,a

≤
≤

X

i∈[n]

X

i∈[n]

i∈[n]

i∈[n]

X

=√

X

Q̂iπ (s, a)

i∈[n]

QiK (si , ai ) − Q̂iπ (s, a)

X

X

i∈[n]

QiK (si , ai ) −

QiK (si , ai ) − Q̃i (si , ai )
√



QiK (si , ai ) − Q̃i (si , ai ) + (Φi Q̃i )(s, a) − Q̂iπ (s, a)
∞

+ (Φi Q̃i )(s, a) − Q̂iπ (s, a)

Ca′
2γE
Cb
+
+
K + k0 K + k0 (1 − γ)2

∞



(14a)


(14b)

2nγE
nCb
nCa′
+
,
+
K + k0 K + k0 (1 − γ)2

where Eq. (14a) follows from the fact that (Φi Q̃i )(s, a) = Q̃i (si , ai ) and Eq. (14b) follows from Lemma
A.4 and Theorem A.3. The probability 1 − δ′ comes from the union bound.
A.3.2 Analysis of the Actor
For ease of presentation, we first write down the outer loop of Algorithm 2 in Algorithm 4.
Algorithm 4 Outer Loop of Algorithm 2
1: Input: Integer T and initialization θ0i = 0.
2: for t = 0, 1, 2 · · · , T − 1 do
i
= θti + ηt Qit,K
3:
θt+1
4: end for
The following theorem characterizes the convergence rates of Algorithm 4.
1 , π 2 , · · · , π n )} generated by Algorithm 4. Suppose that η satisfies
Theorem A.4. Consider {π(t) = (π(t)
t
(t)
(t)
the condition specified in Theorem 3.3. Then we have
t−1

4nγE
2 X t−j−1
4nγ t−1
γ
kQj,K − Q̂(j) k∞ +
+
.
Qπ∗ − Q(t) ∞ ≤
2
(1 − γ)
1−γ
(1 − γ)2
j=0

22

Proof of Theorem A.4. We begin by decomposing the optimality gap in the following way:
kQπ∗ − Q(t) k∞ ≤ kQ̂π̂∗ − Q̂(t) k∞ + kQπ∗ − Q̂π̂∗ k∞ + kQ̂(t) − Q(t) k∞ ,
{z
} |
{z
} |
{z
}
|

(15)

θt+1 = θt + ηt Qt,K ,

(16)

v2

v1

v3

where we recall that π̂∗ denotes the optimal policy of the separable MDP M̂. On the RHS of Eq. (15), the
term v1 captures the optimality gap of the output of Algorithm 4 with respect to the separable MDP model
M̂, and the terms v2 and v3 are both induced by the model difference between the original MDP M and the
separable one M̂.
To bound the term v1 , since M̂ is a separable MDP, when each agent implements Algorithm 4, the
update equation for the global policy parameter θt ∈ R|S||A| can be written as
P
where Qt,K (s, a) = ni=1 Qit,K (si , ai ) for all s = (s1 , · · · , sn ) ∈ S and a = (a1 , · · · , an ) ∈ A. Therefore,
we can use existing results on single-agent natural actor-critic to bound the term v1 . In particular, Theorem
2.1 in [7] provides us the following result.
1 , π 2 , , · · · , π n )}
Lemma A.5. Consider {π(t) = (π(t)
(t))
(t) t≥0 generated by each agent implementing Algorithm 4. When the stepsize ηt satisfies ηt ≥ log( mins π 1 (at,s |s) )/γ 2t−1 , where at,s = arg maxa Qt,K (s, a),
(t)
we have
t−1

2γ X t−j−1
2γ t−1
≤ γ kQ̂π∗ − Q̂(0) k∞ +
Q̂π̂∗ − Q̂(t)
γ
kQj,K − Q̂(j) k∞ +
.
∞
1−γ
(1 − γ)2
t

j=0

To apply Lemma A.5, we need to verify that the stepsizes in Theorem 3.3 satisfy the condition in Lemma
A.5. Using Eq. (16), we have for any state-action pair (s, a) that
(θt )s,a = (θt−1 )s,a + ηt−1 Qt−1,K (s, a) = (θ0 )s,a +

t−1
X
j=0

ηj Qj,K (s, a) ≤

t−1
X
j=0

t−1

ηj

X
n
ηj ,
= Q̄
1−γ
j=0

n
where Q̄ := 1−γ
and the last inequality follows from the boundedness of the Q-functions in finite MDPs
[20], that is,

Qt,K (s, a) =

n
X
i=1

Qit,K (si , ai ) ≤

n
X
i=1

kQit,K k∞ ≤ n/(1 − γ).

The previous inequality allows us to derive a lower bound for π(t) (a | s):
exp{(θt )s,a }
a′ ∈A exp{(θt )s,a′ }
1
P
=
1 + a′ 6=a exp{(θt )s,a′ − (θt )s,a }
1
≥
P
P
1 + a′ 6=a exp{2Q̄ t−1
j=0 ηj }
1
≥
.
P
|A| exp{2Q̄ t−1
j=0 ηj }

π(t) (a | s) = P

23

It follows that
log



1
mins π(t) (at,s | s)



/γ

2t−1

≤ 2Q̄

t−1
X
j=0

ηj log |A|/γ 2t−1 .

Therefore, the condition on the stepsizes in Lemma A.5 is satisfied. Now, using Lemma A.5 for the term v1
on the RHS of Eq. (15), we have
v1 = kQ̂π̂∗ − Q̂(t) k∞

t−1

≤ γ t kQ̂π∗ − Q̂(0) k∞ +

2γ X t−j−1
2γ t−1
γ
kQj,K − Q̂(j) k∞ +
1−γ
(1 − γ)2
j=0

t−1

≤

2 X t−j−1
4nγ t−1
γ
kQj,K − Q̂(j) k∞ ,
+
(1 − γ)2 1 − γ
j=0

where the last step follows from:

2n
.
1−γ
We next consider the terms v2 and v3 in Eq. (15). Using Lemma A.3, we bound the term v2 in Eq. (15) as
2nγE
v2 = kQπ∗ − Q̂π̂∗ k∞ ≤
.
(1 − γ)2
To bound v3 , we need the following lemma, which bounds the gap between Q-functions of the same
policy applied to the original MDP and the separable one.
kQ̂π∗ − Q̂(0) k∞ ≤ kQ̂π∗ k∞ + kQ̂(0) k∞ ≤

Lemma A.6 (Proof in Appendix A.4.4). Given a policy π, let Qπ and Q̂π be the Q-functions of the original
MDP M and the separable MDP M̂. Then, we have
2nγE
.
kQπ − Q̂π k∞ ≤
(1 − γ)2
Using Lemma A.6, we have
2nγE
v3 = kQ(t) − Q̂(t) k∞ ≤
.
(1 − γ)2
Theorem A.4 follows from using the upper bounds we derived for the terms v1 , v2 , and v3 in Eq. (15).
In light of the analyses for the actor and the critic, to finish proving Theorem 3.3, we combine the bounds
in Theorem A.3 and Theorem A.4. Specifically, for any j ≤ t − 1, with probability at least 1 − δ′ , we have
kQj,K − Q̂(j) k∞ ≤ √

nCa′
nCb
2nγE
+
+
.
K + k0 K + k0 (1 − γ)2

Let δ′ = δ/T in the above result. Using union bound, we have with probability at least 1 − δ that
2nγE
nCb
nCa
+
, ∀ j ≤ t − 1.
+
kQj,K − Q̂(j) k∞ ≤ √
K
+
k
(1
− γ)2
K + k0
0
Therefore with probability at least 1 − δ, we have


T
−1
T
−1
X
X
nCa
nCb
2nγE
T −j−1
T −j−1
√
γ
γ
kQj,K − Q̂{j} k∞ ≤
+
+
K + k0 K + k0 (1 − γ)2
j=0
j=0


nCa
nCb
2nγE
1
√
+
+
.
≤
1−γ
K + k0 K + k0 (1 − γ)2
Applying the bound in the above inequality to Theorem A.4 completes the proof of Theorem 3.3.
24

A.4 Proof of All Supporting Lemmas
A.4.1 Proof of Lemma A.2
Given a positive definite matrix D, define its corresponding weighted ℓ2 -norm as kxkD = (xT Dx)1/2 .
According to the definition of Π1 , Φi Π1 is the projection matrix that projects a vector in R|S||A| to the set
i
i
{Φi r | r ∈ R|S ||A | } with respect to k · kD1 . Therefore, for any Q ∈ R|S||A|, we have:
Φi Π1 Q = arg minq∈{Φi r|r∈R|S i ||Ai | } kQ − qkD1 .

(17)

We first show that Φi Π1 (·) is nonexpansive with respect to k · k∞ .
Lemma A.7. It holds that kΦi Π1 Qk∞ ≤ kQk∞ for any Q ∈ R|S||A|.
Proof of Lemma A.7. From Eq. (17), we have
Π1 Q = r∗ := arg minr∈R|S i ||Ai |

X

(s,a)∈S×A


dπb (s, a) Q(s, a) − r(h1 (s), h2 (a)) .

For any (s̄, ā) in S × A, the value of r∗ (s̄i , āi ) must satisfy
min

−1 i
i
s∈h−1
1 (s̄ ),a∈h2 (ā )

Q(s, a) ≤ r∗ (s̄i , āi ) ≤

max

−1 i
i
s∈h−1
1 (s̄ ),a∈h2 (ā )

Q(s, a),

which leads to
[Φi Π1 Q](s̄, ā) = [Π1 Q](s̄i , āi ) ≤

max

−1 i
i
s∈h−1
1 (s̄ ),a∈h2 (ā )

|Q(s, a)| .

Therefore, we have kΦi Π1 Qk∞ ≤ kQk∞ .
To proceed, recall the definition of Q̂iπ :
"∞
#
X
Q̂iπ (s, a) = Êπ
γ k Ri (Ski , Aik ) S0 = s, A0 = a ,
k=0

∀ (s, a) ∈ S × A,

where Êπ [ · ] denotes the expectation with respect to the separable MDP transition kernel P̂. Observe that
Q̂iπ (s1 , a1 ) = Q̂iπ (s2 , a2 ) when (s1 , a1 ), (s2 , a2 ) ∈ S × A and (si1 , ai1 ) = (si2 , ai2 ). Therefore, it follows
from Lemma A.7 that Φi Π1 Q̂iπ = Q̂iπ . In addition, since
kΦi Q̃i∗ − Q̂iπ̂∗ k∞ = kΦi Q̃i∗ − Φi Π1 Q̂iπ̂∗ k∞

= kΦi Π1 F∗i (Φi Q̃i∗ ) − Φi Π1 Q̂iπ̂∗ k∞

≤ kF∗i (Φi Q̃i∗ ) − Q̂iπ̂∗ k∞

≤ kF∗i (Φi Q̃i∗ ) − F∗i (Q̂iπ̂∗ )k∞ + kF∗i (Q̂iπ̂∗ ) − Q̂iπ̂∗ k∞
≤ γkΦi Q̃i∗ − Q̂iπ̂∗ k∞ + kF∗i (Q̂π̂i ∗ ) − Q̂iπ̂∗ k∞ ,

(Π1 F∗i (Φi Q̃i∗ ) = Q̃i∗ )
(Φi Π1 is nonexpansive)
(Triangle inequality)
(F∗i is a γ-contraction)

we have
1
kF i (Q̂i ) − Q̂iπ̂∗ k∞
1 − γ ∗ π̂∗
1
max γEs̄∼Pa (s,·) max Q̂iπ̂∗ (s̄, ā) − γEs̄∼P̂a (s,·) max Q̂π̂i ∗ (s̄, ā)
=
ā
ā
1 − γ (s,a)

kΦi Q̃i∗ − Q̂iπ̂∗ k∞ ≤

25

X
γ
max
Pa (s, s̄) − P̂a (s, s̄) max Q̂iπ̂∗ (s̄, ā)
ā∈A
1 − γ (s,a)
s̄∈S
X
γ
Pa (s, s̄) − P̂a (s, s̄)
max
≤
(1 − γ)2 (s,a)

≤

s̄∈S

2γE
≤
,
(1 − γ)2

1
where the last two inequalities follow from maxā∈A Q̂π̂i ∗ (s̄, ā) ≤ kQ̂iπ̂∗ k∞ ≤ 1−γ
and the definition of
dependence level E.

A.4.2 Proof of Lemma A.3
Let T∗ : R|S||A| 7→ R|S||A| be the Bellman optimality operator of the MDP model M defined as


′ ′
[T∗ Q](s, a) = R(s, a) + γEs′ ∼Pa (s,·) max
Q(s , a ) , ∀ (s, a).
′
a

The Bellman optimality operator T̂∗ for the separable MDP model M̂ is defined similarly.
Recall that π∗ and π̂∗ are the optimal policies for model M and M̂ respectively. Then, we have
kQπ∗ − Q̂π̂∗ k∞ ≤ kQπ∗ − T∗ Q̂π̂∗ k∞ + kT∗ Q̂π̂∗ − Q̂π̂∗ k∞
≤ γkQπ∗ − Q̂π̂∗ k∞ + kT∗ Q̂π̂∗ − Q̂π̂∗ k∞ ,

where last line follows from T∗ Qπ∗ = Qπ∗ and the Bellman optimality operator T∗ being a γ-contraction
mapping with respect to k · k∞ . It follows that
Qπ∗ − Q̂π̂∗

∞

≤

1
T∗ Q̂π̂∗ − Q̂π̂∗
.
1−γ
∞

(18)

Now, for all (s, a) ∈ S × A, we have
Q̂π̂∗ (s, a) − (T∗ Q̂π̂∗ )(s, a)
≤ γ

X

s′ ∈S

P̂a (s, s′ ) max
Q̂π̂∗ (s′ , a′ ) − γ
′
a

≤2γ P̂a (s, ·) − Pa (s, ·)
≤

2nγE
,
1−γ

n

X

Pa (s, s′ ) max
Q̂π̂∗ (s′ , a′ )
′

s′ ∈S

a

(19a)

TV 1 − γ

(19b)

where Eq.(19b) follows from the definition of E, and Eq. (19a) follows from the following equivalent
definition of the total variation distance [34]:
Z
Z
1
sup
f dν1 − f dν2 .
kν1 − ν2 kTV =
2 f :kf k∞ ≤1
Substituting the above result into Eq. (18), we have
Qπ∗ − Q̂π̂∗

∞

26

≤

2nγE
.
(1 − γ)2

A.4.3 Proof of Lemma A.4
Recall the definition of k · kD in the proof of Lemma A.2 in Appendix A.4.1. According to the definition of
i
i
Π2 , Φi Π2 is the projection matrix that projects a vector in R|S||A| to the set {Φi r | r ∈ R|S ||A | }. Then, for
any Q ∈ R|S||A|, we have the following:
Φi Π2 Q = arg minQ̄∈{Φi r|r∈R|S i ||Ai | } kQ − Q̄kD2 .
Next, we present a nonexpansive property of the operator Φi Π2 (·).
Lemma A.8. It holds that kΦi Π2 Qk∞ ≤ kQk∞ for any Q ∈ R|S||A|.
The proof of Lemma A.8 is similar to the proof of Lemma A.7, and is omitted here.
Observe that Q̂iπ (s1 , a1 ) = Q̂iπ (s2 , a2 ) for any policy π when (s1 , a1 ), (s2 , a2 ) ∈ S × A and (si1 , ai1 ) =
i
(s2 , ai2 ). Then, it follows from Lemma A.8 that Φi Π2 Q̂iπ = Q̂iπ . In addition, note that we have
kΦi Q̃i − Q̂iπ k∞ = kΦi Q̃i − Φi Π2 Q̂iπ k∞

= kΦi Π2 F i (Φi Q̃i ) − Φi Π2 Q̂iπ k∞
i

i

(20a)

i

≤ kF (Φ Q̃ ) − Q̂iπ k∞
≤ kF i (Φi Q̃i ) − F i (Q̂iπ )k∞ + kF i (Q̂iπ ) − Q̂iπ k∞
≤ γkΦi Q̃i − Q̂iπ k∞ + kF i (Q̂iπ ) − Q̂iπ k∞ ,

(20b)
(20c)

where Eq. (20a) follows from the fact that Q̃i is the solution of equation Π2 F i (Φi r) = r, Eq. (20b) follows
from Lemma A.8, and the last inequality Eq. (20c) follows from the fact that F i (·) is a γ-contraction in
k · k∞ . Combined with the Bellman equation, the above inequality leads to
1
kF i (Q̂iπ ) − Q̂iπ k∞
1−γ
1
=
max γEs̄∼Pa (s,·),ā∼π(·|s̄) [Q̂iπ (s̄, ā)] − γEs̄∼P̂a (s,·),ā∼π(·|s̄) [Q̂iπ (s̄, ā)]
1 − γ (s,a)
X
X
γ
max
Pa (s, s̄) − P̂a (s, s̄)
π(ā | s̄)Q̂iπ (s̄, ā)
≤
1 − γ (s,a)
s̄∈S
ā∈A
X
γ
≤
max
Pa (s, s̄) − P̂a (s, s̄)
(1 − γ)2 (s,a)

kΦi Q̃i − Q̂iπ k∞ ≤

s̄∈S

≤

2γE
,
(1 − γ)2

where the last two inequalities follow from
dependence level E.

P

1
i
i
ā∈A π( ā | s̄)Q̂π (s̄, ā) ≤ kQ̂π k∞ ≤ 1−γ and the definition of

A.4.4 Proof of Lemma A.6
Let Tπ : R|S||A| 7→ R|S||A| be the Bellman operator associated with policy π, which is defined as
[Tπ Q](s, a) = R(s, a) + γEs′ ∼Pa (s,·),a′ ∼π(·|s′ ) [Q(s′ , a′ )],

∀ (s, a) ∈ S × A.

Similarly, we define T̂π (·) as the Bellman operator associated with the policy π under the separable MDP
model M̂. It is well-known that both Tπ and T̂π are contractive operators with respect to k · k∞ , with
27

a common contraction factor γ [52]. In addition, Qπ (respectively, Q̂π ) is the unique fixed point of Tπ
(respectively, Tˆπ ).
To proceed, note that the gap between the Q-functions Qπ and Q̂π can be bounded as
kQπ − Q̂π k∞ = kQπ − Tπ Q̂π + Tπ Q̂π − Q̂π k∞

≤ kQπ − Tπ Q̂π k∞ + kTπ Q̂π − Q̂π k∞

= kTπ Qπ − Tπ Q̂π k∞ + kTπ Q̂π − Q̂π k∞

≤ γkQπ − Q̂π k∞ + kTπ Q̂π − Q̂π k∞ ,

(Triangle inequality)
(This follows from Tπ Qπ = Qπ .)

where the last inequality follows from that the Bellman operator Tπ is a γ-contraction mapping with respect
to k · k∞ . It follows that
Qπ − Q̂π

∞

1
Tπ Q̂π − Q̂π
≤
1−γ
∞
1
=
Tπ Q̂π − T̂π Q̂π
1−γ
∞
X
X
γ
Pa (s, s′ )Ea′ ∼π(·|s′ ) [Q̂π (s′ , a′ )] −
max
P̂a (s, s′ )Ea′ ∼π(·|s′ ) [Q̂π (s′ , a′ )]
=
1 − γ (s,a) ′
′
s

(21a)

s

2nγ
P̂a (s, ·) − Pa (s, ·)
(1 − γ)2
TV
2nγE
≤
,
(1 − γ)2
≤

(21b)
(21c)

where Eq. (21a) follows from the definition of Tπ and T̂π , Eq. (21b) also follows from the equivalent
definition of the total variation distance, and Eq. (21c) follows from the definition of E.

A.5 Stochastic Approximation with State Aggregation
In this section, we present the results of stochastic approximation with state aggregation in [37].
Let N = {1, 2, · · · , n} be the state space of {ik }, where {ik }∞
k=0 is the sequence of states visited by a
Markov chain. Let L = {1, 2, · · · , l}, (l ≤ n) be the abstract state space. The surjection h : N 7→ L is used
to convert every state in N to its abstraction set L. Given parameter x ∈ Rl and function F : Rn → Rl , we
consider the generalized stochastic approximation that updates x(k) ∈ Rl starting from x(0) = 0:

(22)
xh(ik ) (k + 1) = xh(ik ) (k) + αk Fik (Φx(k)) − xh(ik ) (k) + w(k) ,
xj (k + 1) = xj (k), for j 6= h(ik ), j ∈ L,

(23)

where the matrix Φ ∈ Rn×l is given by
(
1, if h(i) = j,
∀i ∈ N , j ∈ L.
Φij =
0, otherwise,
Before presenting the convergence result, we first give the assumptions required for the theorem in [37].

28

Assumption A.1. The random process {ik }k≥0 is an aperiodic and irreducible MarkovPchain on the space
S × A with stationary distribution d = {d1 , d2 , · · · , dn }. In addition, letting d′j = i∈h−1 (j) di for all
j ∈ L and σ ′ = inf j∈L d′j , there exist constants M1 ≥ 0 and M2 ≥ 1 such that
sup

X

K⊆N i∈K

di −

X
i∈K

P(ik = i | i0 = j) ≤ M1 exp(−k/M2 )

(24)

for all j ∈ L and k ≥ 0.
Assumption A.2. The Operator F (·) is a γ-contraction with respect to k · k∞ . Further, there exists some
constant C > 0 such that kF (x)k∞ ≤ γkxk∞ + C for all x ∈ Rn .
Assumption A.3. The random process w(k) is Fk+1 measurable and satisfies E[w(k) | Fk ] = 0. Further,
|w(k)| ≤ w̄ almost surely for some constant w̄.
The following theorem shows that the stochastic approximation presented in Eq. (22) converges to the
unique x∗ that solves
ΠF (Φx∗ ) = x∗ .
where Π := (ΦT DΦ)−1 ΦT D. Here, D is the diagonal matrix with diagonal components being the stationary distribution of the Markov chain {ik }, which we denote by d.
Theorem A.5. Suppose that Assumptions A.1, A.2, A.3 hold. Further, assume that there exists some constant
H
with k0 =
x̄ ≥ kx∗ k∞ such that kx(k)k∞ ≤ x̄ for all k almost surely. Let the stepsize be αk = k+k
0
2
max(4H, 2M2 log K) and H ≥ σ′ (1−γ) . Define the constants C1 := 2x̄+C + w̄, C2 := 4x̄+2C + w̄, C3 :=
2M1 (2x̄ + C)(1 + 2M2 + 4H). Then, with probability at least 1 − δ′ , we have


1
Cb
Ca′
∗
+
= Õ √
,
kx(K) − x k∞ ≤ √
K + k0 K + k0
K
where

s
 


4lM2 K
4HC
2
′
K2 log K log
Ca =
+ log log K ,
1−γ
δ′


48M2 C1 H log K + σ ′ C3 2x̄(2M2 log K + k0 )
,
.
Cb = 4 max
(1 − γ)σ ′
1−γ

A.6 Extensions
Our proof follows from the following blueprint: (1) constructing a separable MDP to approximate the
original MDP, (2) analyzing the algorithm as if it were implemented on the separable MDP, and (3) bounding
the dap due to model difference between the separable MDP and the original one. This framework can be
used to study other learning scenarios beyond IL considered in this paper. We briefly elaborate on some of
them in the following.

29

Partially Observable Markov Decision Processes (POMDPs) In sequential decision-making problems,
sometimes the agent (or agents) is not able to observe the complete state and must work with a partial
observation of the state [38]. More specifically, instead of observing a trajectory of state-actions {(Sk , Ak )},
the agent actually observes {(Ok , Ak )}, where Ok ∈ O denotes the partial observation at time k. In this
case, unlike {(Sk , Ak )}, the trajectory {(Ok , Ak )} does not form a Markov chain, which imposes major
technical difficulties in analyzing POMDPs. In light of our analysis framework, if we can approximate the
non-Markovian trajectory {(Ok , Ak )} by a Markov chain that is defined on O × A, then, it would enable a
tractable analysis of POMDPs. In this case, the approximation error (which is analogous to our dependence
level) will appear in the analysis. Studying POMDPs is another future direction of this work.
General Non-Markovian Stochastic Iterative Algorithms RL algorithms, in general, belong to the
broad framework of stochastic iterative algorithms, also known as stochastic approximation algorithms [46],
which can be represented in the following form
xk+1 = xk + αk G(xk , Yk ),
where Yk is the noise and G(·, ·) is some properly defined operator, depending on the problem of interest.
As we see in Section A.3.1, the update equation for the joint Q-function can be written as a stochastic
approximation in the form above. Moreover, the popular stochastic gradient descent algorithm in largescale continuous optimization is also a special case of stochastic approximation, where G(·, ·) is a noisy
version of the negative gradient operator. While stochastic approximation has been extensively studied in
the literature, most if not all existing results assume the noise sequence {Yk } is either an i.i.d. sequence or
forms a Markov chain. In light of our proof framework, we are able to analyze stochastic approximation
algorithms driven by non-Markovian samples, as long as the trajectory {Yk } can be approximated by a
Markov chain. Rigorously developing asymptotic and finite-sample convergence results for non-Markovian
stochastic approximation algorithms is also an interesting future direction.

B Examples and Experiments
This section provides the details of the illustrative example presented in Section 2.2 and the numerical
simulations.

B.1

Details of the Synthetic MDP

Original MDP. The state of the three agents is a 3-dimension vector denoted as s = (s1 , s2 , s3 ), where
si ∈ {0, 1} for i ∈ {1, 2, 3}. The action is denoted as a = (a1 , a2 , a3 ), where ai ∈ {0, 1} for i ∈ {1, 2, 3}.
For each agent, one unit of reward can be obtained when the value of the state variable stays unchanged and
0 is obtained otherwise. Agents 1 and 2 are strongly coupled and always have the same state value. Their
state values remain unchanged when they take the same actions. Otherwise, their state values will transit
from 0 to 1 or from 1 to 0 simultaneously. That means the following holds when s1k = s2k ,
P(s1k+1 6= s1k , s2k+1 6= s2k | aik 6= a2k ) = 1.

P(s1k+1 = s1k , s2k+1 = s2k | a1k = a2k ) = 1,

For agent 3, any action leads to the state value 0 or 1 with probability 0.5 when (1) its state value is 1 or (2)
its state value is 0 and it applies the same action to agent 2. However, when its state value is 0 and it applies
a different action compared with agent 2, the next state will be 1 with probability 1. The transition model
for agent 3 is summarized as
P(s3k+1 = 1 | s3k = 0, a2k 6= a3k ) = 1,

P(s3k+1 = 0 | s3k = 0, a2k 6= a3k ) = 0,
30

P(s3k+1 = 0 | s3k = 0, a2k = a3k ) = 0.5,

P(s3k+1 = 1 | s3k = 0, a2k = a3k ) = 0.5,
P(s3k+1 = 1 | s3k = 1) = 0.5.

P(s3k+1 = 0 | s3k = 1) = 0.5,

The different dependencies among agents induce different dependence levels with different grouping options.
The Dependence level. Suppose that we group agents 1 and 2 together, that is, they can be seen as one
agent controlling 2 action variables. Then, it is easy to derive E = 0.5 by solving the optimization problem
in Eq. (1). Suppose that agents 2 and 3 are grouped together. Then, we have E = 0.75. Similarly, we have
E = 0.875 when agents 1 and 3 are grouped together. The detailed separable MDPs for different grouping
options are shown in Figure 4.

Figure 4: The separable MDPs for different grouping options
In Figure 4, black arrows mean the same transition probabilities as the original MDP, and green arrows
mean random transition probabilities with 0.5 to any state under any state and action.

B.2

The EV Charging Problem

Consider the problem where there are multiple charging stations in a distributed network and the total charging power must remain within a time-varying capacity to guarantee safety. A natural approach to address
this problem is to formulate charging coordination as a hierarchical resource allocation problem, where the
resource is the charging capacity. Each charging station receives an allocated charging capacity which denotes the maximum power that can be used to charge an EV. Consequently, safety is guaranteed because the
total charging power consumed by all stations is upper bounded by the total charging capacity.
Consider allocating the charging capacity through a tree structure shown in Figure 5, where decisions
are made layer by layer. Agents in the same layer perform IL and decide the charging capacity assigned to
the children. Specifically, as illustrated in Figure 5, the maximum charging power is first allocated to the
top agent and it needs to decide the capacities allocated to its left and right children. Each child receives
a maximum charging capacity, which means that the total charging power induced by the charging stations
on the child’s side must be within the limit. Then, similar allocations are performed on all lower layers.
Absolute safety is guaranteed since the charging capacity is allocated layer by layer to make sure the total
charging power is within the upper limit. Finally, all charging stations on leaves charge the connected EVs
no more than the maximum charging capacity they receive.
To make decisions, each agent observes the charging capacity assigned to it and the information of its
children. The information from children is contracted from high dimensions to low dimensions for scalability before being transferred to the parents. In this formulation, the problem of EV charging is captured

31

Figure 5: Illustration of a binary tree to allocate the charging capacity.
by an MARL with various dependence levels. To see this, consider agents in the same layer. Their parents’
and children’s policies can be seen as stationary by our design of the multi-scale learning rates (see Section
B.3 for a detailed discussion). Information contraction controls the dependence level E among the agents
in the same layer, which leads to different performance gaps in our analysis. To further elaborate, consider
the following two extreme cases. In one extreme case, the contraction keeps the information unchanged, in
which case the agents within the same layer are strongly correlated because their parent makes decisions
based on their full information. In another extreme case, no information is communicated to the parents. As
a result, all agents in the same layer are completely independent.
In our setting, all agents cooperate to maximize the long-term discounted total reward of all charging
locations. Each agent can only communicate locally with its parent or children.
Each charging station j has its local information I j (t) = (pj (t), dj (t), f j (t), lj (t)) of the connected EV
at time t, where pi (t) is the rated charging power, dj (t) is the proportion of satisfied demand, f j (t) and
lj (t) are the additional time needed to fully charge the EV and the remaining time before the EV leaves.
All variables are 0 if no EV is connected or the connected EV is fully charged. The charging station j
can induce a reward of r j (t) = ∆dj (t) = dj (t + 1) − dj (t) after charging the connected EV with power
Pcj (t) = min{P j (t), pj (t)}, where P j (t) denotes the maximum charging capacity it receives. The reward
is r j (t) = 0 if the connected EV is not charged or no EV is connected.
We define the states, actions, and rewards for the agents as follows:
• States. For each agent i, the state includes the information from its children and the charging capacity
i
i
allocated to it, which can be written as si (t) = (I Cl (t), I Cr (t), P i (t)), where Cli , Cri denote the left and
right children of agent i. The information of agent i is constructed recursively from the bottom level to
i
the top as a function of its children’s information. Specifically, we define I i (t) = W1i (t)(I Cl (t))T +
i
W2i (t)(I Cr (t))T , where W1i (t), W2i (t) are both matrices that can be designed to control the information
communicated to the parents. We will discuss more about how to control the dependence level with
different choices of W1i (t), W2i (t) below.
• Actions. The action of agent i at time t is the proportion of charging capacity allocated to the left child
ai (t) ∈ [0, 1]. Consequently, the proportion of charging capacity allocated to the right child is 1 − ai (t).
• Rewards. The reward of agent i is defined recursively as the sum of the rewards of its children from the
i
i
bottom level to the top level. That is, r i (t) = r Cl (t) + r Cr (t). Thus, the reward of agent i is equal to the
total reward of all charging stations that have i as an ancestor.

B.3

Numerical Simulations

In this section, we present the details of our numerical experiments.

32

B.3.1 Experimental Setting for the Synthetic MDP
In this case, we consider the synthetic MDP in Example 2.2. Three agents are grouped into two with
different options according to Figure 2. For both IQL and INAC, we will repeat the training and testing
100 times, respectively. Each run includes 3000 steps for training and 1000 steps for testing. The discount
factor is γ = 0.99. Specifically, the inner loop of INAC has K = 100 steps. We set α =P
0.05, k0 = 4α
and αk = α/(k + k0 ) for the critic. The stepsize for the actor is set as η0 = 0.2 and ηt = i<t ηi /γ 2t−1 .
To encourage exploration during the training process, each agent takes action uniformly at random with
probability ǫk = (1 − k/K)/10. For IQL, we also set α = 0.05, k0 = 4α and αk = α/(k + k0 ). The
policy πb is set as the uniform distribution over the action space. We calculate the normalized reward relative
to the optimal expected reward of each episode consisting of 100 steps. The optimal reward can be easily
computed with the model described in Appendix B.1.
B.3.2 Experimental Setting for EV Charging

Figure 6: The structures of the 3-agent system (left) and the 15-agent system (right). The square nodes
represent the charging stations and the circle nodes represent the agents. Note that our results do not require
the interaction structure to be full binary trees.
Here we consider the 3-agent and 15-agent systems shown in Figure 6. Recall that W1i (t), W2i (t) can be
designed to derive different information contractions with different dependence levels among agents within
the same layer. We design three types of information contractions to perform IL, including full information,
average information, and no information. Full information means that each agent communicates the full
information from its children to its parent, where the dimensions of agents’ states increase exponentially
from the bottom to the top. Average information means that the information communicated to the parent
is taken as a weighted average of the information from its children. No information means that each agent
communicates nothing to its parent. If W1i (t) = 0, W2i (t) = 0, the children do not send any information
to their parents. The agent can also send average information by mixing the information of its children with
W1i (t) = diag(1, eil (t), eil (t), eil (t)) and W2i (t) = diag(1, eir (t), eir (t), eir (t)), where
i

eil (t) =
i

i

pCl (t)
i

pCl (t) + pCr (t)
i

, and eir (t) =

i

pCr (t)
i

pCl (t) + pCr (t)
i

.

We set eil (t) = eir (t) = 0 if pCl (t) = pCr (t) = 0. In other words, the contracted di (t), f i (t), and li (t) are
the weighted average of information from its children according to the rated charging power. The agent sends
i
i
i
i
the full information to its children when W1i (t) = (Ini , 0)T ∈ R2n ×n , W2i (t) = (0, Ini )T ∈ R2n ×n ,
where Ini is the identity matrix, and ni is the number of dimensions of the information from agent i’s
children.
EV arrivals are simulated with fixed arrival rates sampled from (0, 1) for each charging station. Each
EV is set with random charging demand, maximum charging power, and remaining time before leaving. We
33

Table 1: Parameters of IQL for 3-agent and 15-agent systems
3-agent system
15-agent system
Network
3 hidden layers, 64 neurons for each layer
Learning rate (Top agent)
10−4
10−5
Max. exploration prob.
1
0.1
Min. exploration prob.
0.03
0.03
Batch size
32
64
Buffer size
5000
6000

Table 2: Parameters of INAC for 3-agnet and 15-agent systems
3-agent system
15-agent system
Network
3 hidden layers, 64 neurons for each layer
Learning rate for critic (Top agent)
10−4
10−6
−5
Learning rate for actor (Top agent)
5 × 10
5 × 10−7
Max. variance
10
10
Min. variance
0.4
0.4
Batch size
32
32
Buffer size
4000
4000

set the remaining time before leaving longer than the time needed to fully charge the EV. The time interval
between two decisions is set to 1 hour.
Two other policies are considered for comparison to our algorithms. The first one is the offline optimal policy, which is non-causal and is computed via linear programming using all information collected
throughout the time window. The second is a heuristic baseline policy similar to the business-as-usual policy
that charges the EV immediately upon arrival [48], but due to the tree structure and the safety concern, the
baseline policy here selects actions based on the proportion of rated power of the children, i.e., ai (t) = eil (t).
Parameters of IQL. Considering the continuity of the state space, we use two neural networks to learn
the Q-functions. The current network is used to provide decisions and interact with the environment, and the
target network is used to learn the optimal Q-fucntion. The parameters of the current network are set as the
copy of the target network every 1000 steps. The target network is updated every 5 steps. The action space
[0, 1] is discretized as {0, 0.1, · · · , 1}. We also apply multiscale learning rates in different layers to make
sure the agents in other layers are relatively stationary to the agents in each layer. Then, only the agents
within the same layer are relatively non-stationary to each other. The learning rates are set to increase with
depth by a multiplier of 10 for Q-networks and policy networks. To encourage exploration, the actions are
given randomly with a linearly decreasing probability during the training process. The detailed parameters
are listed in Table 1.
Parameters of INAC. Due to the continuity of the state and action space, we also use neural networks
to act as the actor and critic. Similarly to IQL, we apply multiscale learning rates in different layers for
INAC. The learning rates are also set to increase with depth by a multiplier of 10 for Q-networks and policy
networks. To encourage exploration, we add Guassian noise to the action with mean 0 and decreasing
variance with steps. The detailed parameters are shown in Table 2.
In our simulations, both IQL and INAC are trained and tested for 20 times. Each run includes 40000

34

steps, and each episode includes 10 days (240 steps). The first 125 episodes are for training, and the rest of
the episodes are for testing. The normalized reward is calculated relative to the optimal average reward of
the offline optimal policy.
B.3.3 Results for EV Charging
We repeated the experiments 20 times in both the 3-agent system and the 15-agent system, respectively, and
the results are shown in Figure 7 and Figure 8. The rewards are also normalized by the average optimal
reward calculated by the offline optimal policy. The results in the 3-agent system show that IQL and INAC
exhibit similar performances. Their performances with different information contractions are very close
and much better than the baseline policy, and the performances with full information are slightly better.
This phenomenon indicates that we can aggressively contract the information without compromising the
optimality. In addition, the performance of IQL is significantly better than that of INAC.

Figure 7: Performance of IQL with different contractions in the 3-agent (left) and 15-agent (right) systems.

Figure 8: Performance of INAC with different contractions in the 3-agent (left) and 15-agent (right) systems.
In the 15-agent system, IQL or INAC with no information contraction is computationally intractable due
to the curse of dimensionality. The results show that the performance of INAC with average information

35

and no information are both around 80% of the optimal rewards, where the optimal rewards are calculated
by an offline optimal algorithm (see Appendix B.3), and the former is slightly better. The performance of
IQL with average information can significantly outperform others.

36

