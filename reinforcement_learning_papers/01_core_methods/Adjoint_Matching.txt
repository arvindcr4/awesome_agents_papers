Adjoint Matching: Fine-tuning Flow and
Diffusion Generative Models with
Memoryless Stochastic Optimal Control
Carles Domingo-Enrich1 , Michal Drozdzal1 , Brian Karrer1 , Ricky T. Q. Chen1

arXiv:2409.08861v5 [cs.LG] 7 Jan 2025

1

FAIR, Meta

Dynamical generative models that produce samples through an iterative process, such as Flow Matching
and denoising diffusion models, have seen widespread use, but there have not been many theoreticallysound methods for improving these models with reward fine-tuning. In this work, we cast reward
fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless
noise schedule must be enforced during fine-tuning, in order to account for the dependency between the
noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching
which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We
find that our approach significantly improves over existing methods for reward fine-tuning, achieving
better consistency, realism, and generalization to unseen human preference reward models, while
retaining sample diversity.
Correspondence: Carles Domingo-Enrich at cd2754@nyu.edu

Adjoint Matching (Ours)

Base model (Flow Matching) w/ Guidance

Figure 1 We introduce Adjoint Matching, a theoretically-driven yet simple algorithm for reward fine-tuning that works
for a large family of dynamical generative models, including for the first time, Flow Matching models. Text prompts:
“Beautiful colorful sunset midst of building in Bangkok Thailand ”, “Beautiful grandma and granddaughter are mixing
salad and smiling while cooking in kitchen”, “The beautiful young woman in sunglasses is standing at the background of
field and hill. She is smiling and looking over shoulder ”, “Chess, intellectual games, figure horse, chess board ”.

1

1

Introduction

Flow Matching (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Liu et al., 2023) and denoising
diffusion (Song and Ermon, 2019; Ho et al., 2020; Song et al., 2021b; Kingma et al., 2021) models are being
used for many generative modeling applications, including text-to-image (Rombach et al., 2022; Esser et al.,
2024), text-to-video (Singer et al., 2022), and text-to-audio (Le et al., 2024; Vyas et al., 2023). In most cases,
the base generative model does not achieve the desired sample quality. To improve the generated samples, it
is common to resort to techniques such as classifier-free guidance (Ho and Salimans, 2022; Zheng et al., 2023)
to get better text-to-sample alignment, or to fine-tune using human preference reward models to improve
sample quality and realism (Wallace et al., 2023a; Clark et al., 2024).
In the adjacent field of large language models, the behavior of the model is aligned to human preferences
through fine-tuning with reinforcement learning from human feedback (RLHF). Either explicitly or implicitly,
RLHF methods (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022) assume a
reward model r(x) that captures human preferences, with the goal of modifying the base generative model
such that it generates the following tilted distribution:
p∗ (x) ∝ pbase (x) exp(r(x)),

(1)

where pbase is the base generative model’s sample distribution.
Inspired by this, fine-tuning methods have been developed to improve denoising diffusion models based on
human preference data; either using a reward-based approach (Fan and Lee, 2023; Black et al., 2024; Fan
et al., 2023; Xu et al., 2023; Clark et al., 2024; Uehara et al., 2024a,b), or direct preference optimization
(Wallace et al., 2023a). However, unlike the fine-tuning methods designed for large language models, most of
the existing methods to a large degree ignore pbase and focus solely on the reward model. Reward models
can range from standard evaluation metrics such as ClipScore (Hessel et al., 2021; Kirstain et al., 2023) to
specialized models that have been trained on human preferences (Schuhmann and Beaumont, 2022; Xu et al.,
2023; Wu et al., 2023c). As these are parameterized by neural networks, they fall pray to adversarial examples
which lead to the generation of undesirable artifacts (Goodfellow et al., 2014; Mordvintsev et al., 2015). This
has led some works to consider adding regularization during fine-tuning (Fan et al., 2024; Uehara et al., 2024b)
to incentivize staying close to the base model distribution; however, there does not yet exist a simple approach
which actually provably generates from the tilted distribution (1).
The main contributions of our paper are as follows:
(i) We present a stochastic optimal control (SOC) formulation for reward fine-tuning of dynamical generative
models. Importantly, we prove that the naïve approach considered by prior works lead to a value
function bias problem that biases the fine-tuned model away from the tilted distribution (1). This
problem has also been observed by Uehara et al. (2024b) but they propose a more complicated solution
which involves training a separate generative model for the optimal noise distribution.
(ii) Instead, we propose a very simple solution: the memoryless noise schedule. This is a unique noise
schedule that completely removes the dependency between noise variables and the generated samples,
resulting in provable convergence to the tilted distribution. This allows us to fine-tune dynamical
generative models in full generality, including being the first to fine-tune noiseless Flow Matching models.
(iii) We also propose a new method for solving SOC problems, called Adjoint Matching, which combines the
scalability of gradient-based methods and the simplicity of a least-squares regression objective. This is
orthogonal to the reward fine-tuning application and can be applied to general SOC problems.
(iv) We perform extensive comparisons to baseline approaches, and analyze them from multiple perspectives
such as realism, consistency, and diversity. We find that our proposed method provides generalization to
unseen human preference reward models, better text-to-sample consistency, and retains good diversity.
In the following, sections are broken down as follows: Section 2 summarizes the algorithms used for sampling
from pre-trained Flow Matching and diffusion models, while Section 3 provides a common notation that we
will use throughout. Sections 4 and 5 form the core of our contributions. Section 4 details the value function
bias problem and our proposed solution via the memoryless noise schedule. Section 5 details the new Adjoint
Matching algorithm for solving SOC problems.
2

2

Preliminaries on dynamical generative models

We are interested in fine-tuning base generative models pbase (X1 ) where samples are generated through the
simulation of a stochastic process. That is, these models transform noise variables into a sample through
an iterative process. In particular, we discuss the specific constructions and sampling processes of Flow
Matching (Lipman et al., 2023; Liu et al., 2023; Liu, 2022; Albergo and Vanden-Eijnden, 2023) and Denoising
Diffusion Models (Ho et al., 2020; Song et al., 2021b,a). The goal of this section is to provide background
information on these methods, which we will later unify into a single consistent notation in Section 3.
Given random variables from an initial distribution X̄0 ∼ p0 = N (0, I), and X̄1 which are distributed according
to some data distribution, we define the reference flow X̄ = (X̄t )t∈[0,1] where
(2)

X̄t = βt X̄0 + αt X̄1 ,

where (αt )t∈[0,1] , (βt )t∈[0,1] are functions such that α0 = β1 = 0 and α1 = β0 = 1. Diffusion models and Flow
Matching construct generative Markov processes Xt with initial distribution X0 ∼ N (0, I) that result in flows
X = (Xt )t∈[0,1] with the same time marginals as the reference flow X̄, i.e., the random variables Xt and
X̄t have identical distribution for all times t ∈ [0, 1]. This implies X1 has the same distribution as the data
distribution, so simulating the Markov process from random noise X0 is a way to generate artificial samples1 .

2.1

Flow Matching

In its simplest form, the generative Markov process of a Flow Matching model is an ordinary differential
equation (ODE) of the form:
(3)

X0 ∼ N (0, I).

dXt = v(Xt , t) dt,

where v(Xt , t) is a parametric velocity that is optimized to match the derivative of the reference flow, i.e.,
2
d
X̄t (see e.g. Lipman et al. (2023) for details on pre-training Flow Matching
v(Xt , t) = argminv̂ E v̂(X̄t , t)− dt
models). It can then be proven that the solution of the generative process (3) has the same time marginals as
the reference flow (Lipman et al., 2023; Liu, 2022; Albergo and Vanden-Eijnden, 2023), and a commonly used
choice is αt = t and βt = 1 − t. One can also consider a family of stochastic differential equations (SDEs)
with an arbitrary state-independent diffusion coefficient2 :



σ(t)2
α̇t
dXt = v(Xt , t) +
v(Xt , t) − αt Xt
dt + σ(t) dBt ,
X0 ∼ N (0, I),
(4)
α̇t
2βt ( α βt −β̇t )
t

where (Bt )t≥0 is a Brownian motion. The generative processes in (3) and (4) have the same time marginals.
This can be seen by writing down the Fokker-Planck equations for (3) and (4), and observing that they are the
same up to a cancellation of terms (Maoutsa et al., 2020). The diffusion coefficient σ(t) in (4) is compensated
by the second term in the drift which scales proportionally as σ(t)2 .

2.2

Denoising Diffusion Models

We next discuss diffusion models, in particular the sampling scheme proposed by Denoising Diffusion Implicit
Model (DDIM; Song et al. (2021a)) which we will later relate to Denoising Diffusion Probabilistic Models
(DDPM; Ho et al. (2020)) as a particular case of the former. For sampling from a diffusion model, the DDIM
update rule3 (Song et al. (2021a), Eq. 12), typically stated in discrete time with k ∈ {0, . . . , K}, is:
√
p
√
ᾱ ϵ(Xk ,k) 
√ k
Xk+1 = ᾱk+1 Xk − 1−
+ 1 − ᾱk+1 − σk2 ϵ(Xk , k) + σk εk ,
εk ∼ N (0, I), X0 ∼ N (0, I), (5)
ᾱk
where ᾱk is an increasing sequence such that ᾱ0 = 0, ᾱK = 1, and the sequence σk is arbitrary. That is, one
samples an initial Gaussian random variable x0 , and applies the stochastic update (5) iteratively K times in
order to obtain an artificial sample XK . Updates can be interpreted as progressively denoising the iterate:
x0 is completely noisy and xK is fully denoised. The noise predictor model ϵ(xk , k) is trained to predict the
noise of xk (see e.g. Ho et al. (2020) for details on pre-training denoising diffusion models).
1 In our derivations, we will simply assume the base model has been trained perfectly during the pre-training phase.
2We use the common short-hand “over-dot” notation to denote the time derivative, i.e., ẋ = d x .
t
dt t
3We slightly depart from the notation in Song et al. (2021a) by flipping the direction of time and using ᾱ

to the αk in Song et al. (2021a) while it corresponds to the ᾱk in Ho et al. (2020).

3

k which corresponds

3

Flow Matching and diffusion models from a common perspective

We formulate Flow Matching and diffusion models in a unified framework, which we will later use throughout
the paper. Firstly, to simplify notation, we will be using continuous-time formulations. This will also directly
enable fine-tuning methods inspired by the continuous-time paradigm, which we find tends to perform better
than discrete-time counterparts in our empirical validations. Secondly, by consolidating notation, we will be
able to discuss fine-tuning of dynamical generative models that follow the same time marginals as the reference
flow (2), pre-trained with either the Denoising Diffusion or Flow Matching framework, in full generality.
To convert DDIM to a continuous-time stochastic process, we can show that the DDIM update rule (5), up to
a first-order approximation, is equivalent to the Euler-Maruyama discretization of the following SDE:
dXt =

˙t
ᾱ
2ᾱt Xt −

˙t
σ(t)2  ϵbase (Xt ,t) 
ᾱ
√
dt + σ(t)dBt ,
2ᾱt + 2
1−ᾱt

X0 ∼ N (0, I).

(6)

See Appendix B.1 for the full derivation. To go from (5) to (6), we assumed a uniform discretization of time,
k
i.e. t = K
. This results in identifying the discrete-time process (Xk )k∈{0,...,K} with a continuous-time process
(Xt )t∈[0,1] , where ᾱk := ᾱt , σk := √1K σ(t), and ϵ(Xk , k) with ϵbase (Xk , t). In relation to the reference flow (2),
√
√
the generative process in (6) has the same time marginals when αt = ᾱt and βt = 1 − ᾱt (Ho et al., 2020).
Furthermore, when viewed up to first order approximations, the DDPM sampling scheme
p (Ho et al. (2020);
Algorithm 2) can be seen as special instance of the DDIM sampling scheme when σ(t) = ᾱ˙ t/ᾱt . This results
in the following generative process:
q
˙
˙ t ϵbase (Xt ,t) 
˙t
ᾱ
√
dXt = 2ᾱᾱtt Xt − ᾱ
dt
+
X0 ∼ N (0, I),
(7)
ᾱt
ᾱt dBt ,
1−ᾱ
t

We can further consolidate notation by converting all quantities to the score function s(x, t)—defined as the
gradient of the log density of the random variable Xt —which is possible when X0 is Normal-distributed and
under the affine reference flow (2). In particular, the velocity v base from Flow Matching can be expressed in
terms of the score function (see Appendix B.4):
α̇t
α̇t
v base (x, t) = α
x + βt ( α
βt − β̇t )s(x, t).
t
t

(8)

And the noise predictor ϵbase also admits an expression in terms of the score function (see Appendix B.3):
base

.
s(x, t) = − ϵ √1−(x,t)
ᾱ
t

(9)

Plugging these two equations into (4) and (6), respectively, and rewriting them in terms of only the αt and βt
in (2), we can unify both the Flow Matching and continuous-time DDIM generative processes as:
dXt = b(Xt , t) dt + σ(t) dBt ,
X0 ∼ N (0, I),

σ(t)2
α̇t
t
where b(x, t) = κt x +
κt = α
, ηt = βt ( α̇
2 + ηt s(x, t),
αt βt − β̇t )
t

(10)
(11)

where (αt , βt ) are coefficients of the reference flow (2). We have hence expressed the generative process of a
base model, whether it is a Flow Matching or a diffusion model, as an SDE of the form (10)-(11), unified by
the choice of reference flow. This expression has been written before for DDIM, e.g. Bartosh et al. (2024a,b).

4

Fine-tuning as “memoryless” stochastic optimal control

We now discuss the crux of the problem: how to produce a fine-tuned generative model that produces samples
X1 which follow the tilted distribution involving a reward model (1). An obvious direction is to construct
a fine-tuning objective involving both the base generative model and the reward model, where the optimal
solution results in a fine-tuned generative model for the tilted distribution. However, as we will explain, this
turns out to be non-trivial, because a naïve formulation will introduce bias into the solution.
In Section 4.1, we discuss the problem formulation of stochastic optimal control, a general framework for
optimizing SDEs, and its relation to the maximum entropy reinforcement learning framework commonly used
4

for RLHF fine-tuning. Next, in Section 4.2, we discuss the initial value function bias problem which plagues
existing approaches and so far has seen no simple solution. Finally, in Section 4.3, we propose a novel simple
solution that circumvents the bias problem, by enforcing a particular diffusion coefficient, the memoryless
noise schedule, to be used during fine-tuning. This results in an extremely simple fine-tuning objective that
provably converges to a model which generates the tilted distribution (1) without any statistical bias.

4.1

Preliminaries on the stochastic optimal control problem formulation

Stochastic optimal control (SOC; Bellman (1957); Fleming and Rishel (2012); Sethi (2018)) considers
general optimization problems over stochastic differential equations, but we only need to consider a common
instantiation, the quadratic cost control-affine problem formulation:
R1


min E 0 12 ∥u(Xtu , t)∥2 + f (Xtu , t) dt + g(X1u ) ,
(12)
u∈U

s.t. dXtu = b(Xtu , t) + σ(t)u(Xtu , t) dt + σ(t)dBt ,
X0u ∼ p0
(13)
where in (13), Xtu ∈ Rd is the state of the stochastic process, u : Rd × [0, 1] → Rd is commonly referred to as
the control vector field, b : Rd × [0, 1] → Rd is a base drift, and σ : [0, 1] → Rd×d is the diffusion coefficient.
These jointly define the controlled process X u ∼ pu that we are interested in optimizing; often both b and σ
are fixed and we only optimize over the control u.
As part of the objective functional (12), we have an affine control cost 12 ∥u(Xtu , t)∥2 , a running state cost
f : Rd × [0, 1] → R and a terminal state cost g : Rd → R.
The stochastic optimal control (SOC) objective (12) can be decomposed recursively from the final time value.
It is common to define the cost functional which is the expected future cost starting from state x at time t:
i
hR

1
(14)
J(u; x, t) := EX∼pu t 12 ∥u(Xs , s)∥2 + f (Xs , s) ds + g(X1 ) Xt = x .
From here, the value function is the optimal value of the cost functional4 :
V (x, t) := minu∈U J(u; x, t) = J(u∗ ; x, t),

(15)

where u∗ is the optimal control, i.e., minimizer of (12). Furthermore, a classical result is that the value
function can be expressed in terms of the uncontrolled base process pbase (Kappen (2005), see Domingo-Enrich
et al. 2023, Eq. 8, App. B for a self-contained proof):
h
i
R1
V (x, t) = − log EX∼pbase exp(− t f (Xs , s)ds − g(X1 )) Xt = x .
(16)
A useful expression for the optimal control (which we will make use of in deriving the Adjoint Matching
objective in Section 5) is that it is related to the gradient of the value function:
u∗ (x, t) = −σ(t)⊤ ∇x V (x, t) = −σ(t)⊤ ∇x J(u∗ , x, t).

(17)

Relation to MaxEnt RL. Stochastic optimal control with the control-affine formulation (12) is the continuoustime equivalence of maximum entropy reinforcement learning (MaxEnt RL; Todorov (2006); Ziebart et al.
(2008)) with a KL regularization instead of only an entropy regularization. In particular, by the Girsanov
theorem (Theorem 2), the affine control cost is equivalent to a Kullback–Leibler (KL) divergence between the
base process pbase , when u = 0, and the controlled process pu , when conditioned on the same initial state X0
(see Appendix C.4):
hR
i

1
DKL pu (X|X0 ) pbase (X|X0 ) = EX u ∼pu 0 12 ∥u(Xtu , t)∥2 dt ,
(18)
resulting in the KL-regularized RL interpretation of (12):
i
h
R1

max EX0 ∼p0 EX∼pu (·|X0 ) 0 −f (Xtu , t)dt − g(X1u ) − DKL (pu (X|X0 ) ∥ pbase (X|X0 )) ,
u∈U

(19)

where the negative state costs correspond to intermediate and terminal rewards in the RL interpretation. The
KL divergence incentivizes the optimal solution to stay close to the distribution of the base process.
4 Note that there is a slight difference in terminology between SOC and reinforcement learning, where our cost functional is
referred to as the state value function and our value function is the optimal state value function in RL.

5

4.2

The initial value function bias problem

We next discuss why naïvely adding a KL regularization does not lead to the tilted distribution (1). From
(19), we can also show that the optimal distribution conditioned on X0 is5

R1
p∗ (X|X0 ) ∝ pbase (X|X0 ) exp − 0 f (Xt , t) dt − g(X1 ) .

(20)

This is analogous to the exponentiated reward distribution in MaxEnt RL (Rawlik et al., 2013), but since we
generalize the entropy regularization to a KL regularization, pbase acts as a prior distribution.
In order to relate this to the tilted distribution (1) that we want to achieve for fine-tuning, first notice that
the normalization constant of the right-hand side (RHS) of (20) is exactly the value function at t = 0:
h
i

R1
EX∼pbase (X|X0 ) exp − 0 f (Xt , t) dt − g(X1 ) = exp −V (X0 , 0) ,
(21)
where the equality is due to (16). Dividing the RHS of (20) by (21) and multiplying by p0 (X0 ), we obtain
the normalized distribution over the full path X,

R1
p∗ (X) = pbase (X) exp − 0 f (Xt , t) dt − g(X1 ) + V (X0 , 0) .
(22)
Setting f = 0 and g = −r, we arrive at an expression for the optimal distribution

p∗ (X0 , X1 ) = pbase (X0 , X1 ) exp r(X1 ) + V (X0 , 0) .

(23)

This unfortunately does not lead to the tilted distribution (1) because we have a bias in the optimal distribution
that is due to the value function of the initial distribution V (X0 , 0). That is to say, naïvely adding a KL
regularization (18) to the fine-tuning objective in the sense of (19) leads to a biased distribution (22) after
fine-tuning and is not equivalent to the tilted distribution (1). For instance, when the sampling procedure is
noiseless, i.e., σ(t) = 0, fine-tuning naïvely will not have any effect because X0 completely determines X1 .
This is unlike the situation for large language models (Ouyang et al., 2022; Rafailov et al., 2023), where there
is no dynamical process that samples X1 iteratively and hence no dependence on the initial noise variable
X0 . Although this KL regularization is a common objective for RLHF of large language models, it has seen
seldom use in fine-tuning diffusion models, likely due to this issue of the initial value function bias.
In the context of diffusion models, KL regularization (19) has been explored in prior works (Fan et al., 2024),
but its behavior was not well-understood and they did not relate the fine-tuned model to the tilted distribution
(1). Another direction that has been proposed is to learn the initial distribution p0 to cancel out the bias
(Uehara et al., 2024b; Tang, 2024) but this simply shifts the work into tilting the initial distribution and
requires an auxiliary model for parameterizing the optimal initial distribution. In contrast, we show in the
next section that it is possible to remove the value function bias by simply choosing a very particular noise
schedule during the fine-tuning procedure.

4.3

The memoryless noise schedule for fine-tuning dynamical generative models

In this section, we propose a very simple method of turning (23) into the tilted distribution (1) through the
use of a particular memoryless noise schedule. Throughout, we provide an intuitive explanation of why this
noise schedule is sufficient for fine-tuning while discussing the full theoretical result where we show that the
memoryless noise schedule is actually not only sufficient but also necessary.
Intuitively, the main reason we cannot arrive at the tilted distribution from (23) is due to the pbase (X0 , X1 )
distribution not factoring into X0 and X1 . Hence, we define a memoryless generative process as follows:
Definition 1 (Memoryless generative process). A generative process of the form (10)-(11) is memoryless if
X0 and X1 are independent, i.e., pbase (X0 , X1 ) = pbase (X0 )pbase (X1 ).
5 Note (20) is informal because densities over continuous-time processes are ill-defined; the formal statement is

R1

∗

dP∗
(X|X0 ) =
dPbase

exp(− 0 f (Xt , t) dt − g(X1 )), where dPdP
base denotes the Radon-Nikodym derivative. We treat this formally in the proofs.

6

κt

ηt

Flow Matching (3)

α̇t
αt

t
βt α̇
αt βt − β̇t



Memoryless Flow Matching (4)

α̇t
αt

t
βt α̇
αt βt − β̇t



DDIM (6)

˙t
ᾱ
2ᾱt

˙t
ᾱ
2ᾱt

DDPM (7)

˙t
ᾱ
2ᾱt

˙t
ᾱ
2ᾱt

Diffusion coefficient σ(t)

Memoryless Xt

General (commonly 0)
√
2ηt

No
Yes

General (commonly 0)
√
2ηt

No
Yes

Table 1 Diffusion coefficient σ(t) and the factors κt , ηt for the Flow Matching,
Memoryless Flow Matching, DDIM,
√
and DDPM generative processes. When the diffusion coefficient is σ(t) = 2ηt , the generative process is memoryless,
i.e., samples X1 will be independent of the initial noise X0 .

When the base generative process is memoryless, this implies:
R
p∗ (X1 ) = pbase (X0 )pbase (X1 ) exp(r(X1 ) + V (X0 , 0))dX0 ∝ pbase (X1 ) exp(r(X1 )).

(24)

That is, solving the SOC problem (12)-(13) with a memoryless base model will result in a fine-tuned model that
generates samples p∗ (X1 ) according to the tilted distribution (1). This memoryless property is not satisfied
generally by the family of generative processes captured by (12)-(13). For instance, the Flow Matching and
DDIM generative processes with zero diffusion coefficient (i.e., σ(t) = 0) are definitely not memoryless due to
X0 and X1 being theoretically invertible. Below, we provide the sufficient and neccessary condition for the
noise schedule in order to have a memoryless generative process.
Proposition 1 (Memoryless noise schedules). Within the family of generative processes (10)-(11), a generative
process is memoryless if and only if the noise schedule is chosen as:

Rt
σ(t)2 = 2ηt + χ(t), where χ : [0, 1] → R is s.t. ∀t ∈ (0, 1], limt′ →0+ αt′ exp − t′ χ(s)
(25)
2β 2 ds = 0.
s

where ηt is the coefficient defined in (11) (see also Table 1). In particular, we refer to σ(t) =
memoryless noise schedule.

√

2ηt as the

Due to the endpoint constraints of (αt , βt ) for the reference flow (2), the memoryless noise schedule σ(t) is
infinite at t = 0 and approaches zero at t = 1. This provides a way for the generative process to mix when close
to noise X0 while stay steadying when close to the sample X1 . Hence, the sample will have no information
about X0 due to the enormous amount of mixing with a large diffusion coefficient. Furthermore, while we
have intuitively justified the memoryless noise schedule through its independence property, our theoretical
result is actually even stronger: all generative models of the form (10)-(11) must be fine-tuned using the
memoryless noise schedule. We formalize this in the following theorem, which we prove in Appendix D.2:
Theorem 1 (Fine-tuning recipe for general noise schedule sampling). Within the family of generative processes
(10)-(11), in order to allow the use of arbitrary noise schedules and still generate samples according to the tilted
distribution (1), the fine-tuning
problem (12)-(13) with f = 0 and g = −r must be done with the memoryless
√
noise schedule σ(t) = 2ηt .
Theorem 1 states that we need to use the memoryless noise schedule for fine-tuning with the SOC objective—
or equivalently, the KL regularized reward objective (19). This is the only noise schedule that retains the
relationship between the velocity and score function, allowing the conversion to arbitrary noise schedules (e.g.,
σ(t) = 0) after fine-tuning. It is worth noting that when using the memoryless noise schedule for DDIM, this
recovers what we derived as the continuous-time limit of the DDPM generative process (7). However, the
DDPM sampler (Ho et al., 2020) is not commonly used while the DDIM sampler (Song et al., 2021a) and
Flow Matching models typically generate samples using σ(t) = 0, so an explicit conversion to the memoryless
noise schedule is necessary for fine-tuning. To the best of our knowledge, we are not aware of any existing
works that have proposed a time-varying diffusion coefficient with theoretical guarantees. Table 1 summarizes
the memoryless schedule for diffusion and Flow Matching models, which we refer to as Memoryless Flow
Matching. In Figure 2, we visualize fine-tuning a 1D model, where we see that constant σ(t) leads to biased
distributions whereas the memoryless noise schedule perfectly converges to the tilted distribution (1).
7

(a) Pre-trained FM v base (b) Fine-tuned FM v finetune (c) Fine-tuned FM v finetune (d) Fine-tuned FM v finetune
√
with σ(t) = 0.2
with σ(t) = 1.0
with memoryless σ(t) = 2ηt
Figure 2 Visualization of Theorem 1 showing that fine-tuning must be done with the memoryless noise schedule to
ensure convergence to the tilted distribution (1). (a) Shows the base Flow Matching model. (b, c) Fine-tuning using a
constant σ(t) leads to biased distributions. (d) Fine-tuning using the memoryless noise schedule leads to the correct
tilted distribution. Note that sample generation can use any noise schedule after fine-tuning, including σ(t) = 0.

For convenience, we plug the memoryless noise schedule into the controlled process for fine-tuning (13), and
express them in terms of each respective framework. Let ϵbase , v base denote the pre-trained vector fields
and ϵfinetune , v finetune the fine-tuned vector fields. Then
√ we have the following expressions for the full drift
b(x, t) + σ(t)u(x, t) and control u(x, t) when σ(t) = 2ηt :
DDIM / DDPM :
˙

˙

finetune

(x,t)
ᾱt ϵ √
b(x, t) + σ(t)u(x, t) = 2ᾱᾱtt x − ᾱ
,
1−ᾱ
t
t

q

˙t
ᾱ
finetune
(x, t) − ϵbase (x, t)).
ᾱt (1−ᾱt ) (ϵ

(26)

2
(v finetune (x, t) − v base (x, t)).
α̇
βt ( αt βt −β̇t )

(27)

u(x, t) = −

Memoryless Flow Matching:
t
b(x, t) + σ(t)u(x, t) = 2v finetune (x, t) − α̇
αt x,

u(x, t) =

r

t

Thus, to solve the SOC problem (12)-(13) in practice, we parameterize the control u in terms of ϵfinetune or
v finetune and optimize these vector fields instead. After plugging in (26)-(27), the SOC problem (12)-(13)
can then be solved using any SOC algorithm in order to perform fine-tuning, and we proposed an especially
effective algorithm next in Section 5. After fine-tuning, ϵfinetune and v finetune can simply be plugged back into
their respective generative processes (3)-(7) to sample from the tilted distribution (1) using any choice of
diffusion coefficient.

5

Adjoint Matching for control-affine stochastic optimal control

We discuss existing methods and also propose a new method for optimizing control-affine SOC problems. The
new Adjoint Matching method is a combination of the time-tested continuous adjoint method (Pontryagin,
1962) with recent developments on constructing least-squares objectives for solving SOC problems (DomingoEnrich et al., 2023). In this section, we briefly discuss preliminaries on existing methods, their pros and
cons, then detail the Adjoint Matching algorithm and its surprising connections to the prior methods. For
numerical optimization, we now assume that the control u is a parametric model with parameters θ.

8

5.1

Existing methods for stochastic optimal control

5.1.1

The adjoint method

The most basic method of optimizing the simulation of an SDE is to directly differentiate through the
simulation using gradients from the SOC objective function (Han and E, 2016). The adjoint method simply
uses the objective:

R1
L(u; X) := 0 12 ∥u(Xt , t)∥2 +f (Xt , t) dt+g(X1 ),
X ∼ pu .
(28)
This is a stochastic estimate of the control objective in (12), and the goal is to take compute the gradient
of L(u; X) with respect to the parameters θ of the control u. Due to the continuous-time nature of SDEs,
there are two main approaches to implementing this numerically. Firstly, the Discrete Adjoint method uses a
“discretize-then-differentiate” approach, where the numerical solver for simulating the SDE is simply stored
in memory then differentiated through, and it has been studied extensively (e.g., Bierkens and Kappen
(2014); Gómez et al. (2014); Hartmann and Schütte (2012); Kappen et al. (2012); Rawlik et al. (2013); Haber
and Ruthotto (2017)). This approach, however, uses an extremely large amount of memory as the full
computational graph of the numerical solver must be stored in memory and implementations often must rely
on gradient checkpointing (Chen et al., 2016) to reduce memory usage.
Secondly, the Continuous Adjoint method exploits the continuous-time nature of SDEs and uses an analytical
expression for the gradient of the control objective with respect to the intermediate states Xt , expressed as an
adjoint ODE, and then applies a numerical method to simulate this gradient itself, hence it is referred to as a
“differentiate-then-discretize” approach (Pontryagin, 1962; Chen et al., 2018; Li et al., 2020). We first define
the adjoint state as:


R1
a(t; X, u) := ∇Xt t 12 ∥u(Xt′ , t′ )∥2 +f (Xt′ , t′ ) dt′ +g(X1 ) ,
(29)

where X solves dXt = b(Xt , t) + σ(t)u(Xt , t) dt + σ(t)dBt .


This implies that EX∼pu a(t; X, u) | Xt = x = ∇x J(u; x, t), where J denotes the cost functional defined in
(14). It can then be shown that this adjoint state satisfies 6 :
h

i
1
d
T
2
a(t;
X,
u)
=
−
a(t;
X,
u)
∇
(b(X
,
t)
+
σ(t)u(X
,
t))
+
∇
f
(X
,
t)
+
∥u(X
,
t)∥
,
(30)
X
t
t
X
t
t
t
t
dt
2
(31)

a(1; X, u) = ∇g(X1 ).

The adjoint state is solved backwards in time, starting from the terminal condition (31). Computation of
(30) can be efficiently done as a vector-Jacobian product on automatic differentiation software (Paszke et al.,
2019). Once the adjoint state has been solved for t ∈ [0, 1], then the gradient of L(u; X) with respect to the
parameters θ can be obtained by integrating over the entire time interval:
1
dL
dθ = 2

R1
T
t ,t)
∥u(Xt , t)∥2 dt + 0 ∂u(X
σ(t)T a(t; X, u)dt,
∂θ
0 ∂θ

R1 ∂

(32)

where the first term is the partial derivative of L w.r.t. θ and the second term is the partial derivative
through the sample trajectory X. See Proposition 6 in Appendix E.1 for a statement and proof of this
result. The discrete and continuous adjoint methods converge to the same gradient as the step size of the
numerical solvers go to zero. Both are scalable to high dimensions and have seen their fair share of usage in
optimizing neural ODE/SDEs (Chen et al., 2018, 2021; Li et al., 2020). As the adjoint methods are essentially
gradient-based optimization algorithms applied on a highly non-convex problem, many have also reported
they can be unstable empirically (Mohamed et al., 2020; Suh et al., 2022; Domingo-Enrich et al., 2023).
5.1.2

Importance-weighted matching objectives for regressing onto the optimal control

An alternative is to consider regressing onto the optimal control u∗ , which is the approach of the cross-entropy
method (Rubinstein and Kroese, 2013; Zhang et al., 2014) and stochastic optimal control matching (SOCM;
Domingo-Enrich et al. (2023)). These methods make use of path integral theory (Kappen, 2005) to express
6 Note we use the convention that a Jacobian matrix J = ∇

x v(x) is defined as Jij =

9

∂vi (x)
.
∂xj

the optimal control through importance sampling, resulting in an importance-weighted least-squares objective
function
R1
LSOCM (u; X) := 0 ∥u(Xt , t) − û∗ (Xt , t)∥2 dt × ω(u, X),
X ∼ pu ,
(33)
where ω is an importance weighting that approximates sampling from the optimal distribution p∗ , and û∗
is a stochastic estimator of the optimal control relying on having sampled from the optimal process. We
defer to Domingo-Enrich et al. (2023) for the exact details. The functional landscape of this objective is
convex, which is argued to help yield stable training. However, the need for importance sampling renders this
impractical for high dimensional applications: the variance of the importance weighting ω grows exponentially
with dimension of the stochastic process, leading to catastrophic failure. This unfortunately means that
such importance-weighted matching objectives are impractical for fine-tuning dynamical generative models;
however, a least-squares objective is greatly coveted as it can lead to stable training and simple interpretations.

5.2

Adjoint Matching

We make two important observations which lead to our proposed method: (i ) it is possible to construct a
matching objective without any importance weighting, and (ii ) there are unnecessary terms in the adjoint
differential equation (30) that can lead to higher variance at convergence.
Firstly, we notice that we can simply match the gradient of the cost functional under the current control.
That is, while SOCM carefully constructs an importance-weighted estimator of the optimal control u∗ =
−σ(t)T ∇J(u∗ ; x, t) (17), we claim that we can actually just regress onto the target vector field −σ(t)T ∇J(u; x, t)
where u is the current control, and furthermore, this results in a gradient equal in expectation to the continuous
adjoint method. We formalize this in the following proposition, proven in Appendix E.2:
Proposition 2. Let us define, for now, the basic Adjoint Matching objective as:
R1
2
LBasic−Adj−Match (u; X) := 12 0 u(Xt , t) + σ(t)T a(t; X, ū) dt,
X ∼ pū ,

ū = stopgrad(u),

(34)

where ū = stopgrad(u) means that the gradients of ū with respect to the parameters θ of the control u are
artificially set to zero. The gradient of LBasic−Adj−Match(u; X) with respect to θ is equal to the gradient dL
dθ in
equation (32). Importantly, the only critical point of E LBasic−Adj−Match is the optimal control u∗ .
δ
δ
Critical points of L are controls u such that δu
L(u) = 0, where δu
L denotes the first variation of the functional
L. In other words, Proposition 2 states that the only control that satisfies the first-order optimality condition
for the basic Adjoint Matching objective is the optimal control, which provides theoretical grounding for
gradient-based optimization algorithms.

An intuitive way to understand the basic Adjoint Matching objective is that it is a consistency loss. The
Adjoint Matching objective is based off of the observation that the optimal control u∗ (x, t) is the unique
fixed-point of the relation u(x, t) = −σ(t)T ∇x J(u; x, t) (see Lemma 6 in Appendix E.2) and so we are directly
optimizing for a control that fits this relation, while using the adjoint state as a stochastic estimator of
∇x J(u; x, t) (29).
The basic Adjoint Matching objective in Proposition 2 does not yet yield a novel algorithm for stochastic
optimal control, because it produces the same gradient as the continuous adjoint method. This can be seen
by taking the gradient w.r.t. θ after expanding the square in (34) and removing terms that do not depend
on θ to arrive exactly at the continuous adjoint method (32). However, it provides the means of deriving a
simpler leaner objective function.
The “Lean” Adjoint. The minimizer of a least-squares objective is the conditional expectation of the regression
target, so for the Adjoint Matching objective, at the optimum we have that


u∗ (x, t) = EX∼p∗ −σ(t)T a(t; X, u∗ )|Xt = x .
(35)
Multiplying both sides by the Jacobian ∇x u∗ (x, t) and re-arranging, we get the relation


EX∼p∗ u∗ (x, t)T ∇x u∗ (x, t) + a(t; X, u∗ )T σ(t)∇x u∗ (x, t) | Xt = x = 0.
10

(36)

Algorithm 1 Adjoint Matching for fine-tuning Flow Matching models
Input: Pre-trained FM velocity field v base , step size h, number of fine-tuning iterations N .
Initialize fine-tuned vector fields: v finetune = v base with parameters θ.
for n ∈ {0, . . . , N − 1} do
q
Sample m trajectories X = (Xt )t∈{0,...,1} with memoryless noise schedule σ(t) =

 √
t
Xt+h = Xt + h 2vθfinetune (Xt , t) − α̇
X
+ hσ(t)εt ,
t
αt

α̇t
βt − β̇t ), e.g.:
2βt ( α
t

εt ∼ N (0, I),

X0 ∼ N (0, I).

For each trajectory, solve the lean adjoint ODE (38)-(39) backwards in time from t = 1 to 0, e.g.:


α̇t
ãt−h = ãt + hãTt ∇Xt 2v base (Xt , t) − α
X
ã1 = −∇X1 r(X1 ).
t ,
t

(40)

(41)

Note that Xt and ãt should be computed without gradients, i.e., Xt = stopgrad(Xt ), ãt = stopgrad(ãt ).
For each trajectory, compute the Adjoint Matching objective (37):
LAdj−Match (θ) =

P

t∈{0,...,1−h}

2
σ(t)


2
vθfinetune (Xt , t) − v base (Xt , t) + σ(t)ãt .

(42)

Compute the gradient ∇θ L(θ) and update θ using favorite gradient descent algorithm.
end
Output: Fine-tuned vector field v finetune

Notice that the terms inside the expectation in (36) show up as part of the adjoint differential equation (30),
which we have now shown to have expectation zero at the optimal solution. Therefore, we motivate the
definition of a lean adjoint state ã with the terms in (36) removed. Plugging this lean adjoint back into the
least-squares objective, we obtain our final proposed Adjoint Matching objective:
R1
2
LAdj−Match (u; X) := 12 0 u(Xt , t) + σ(t)T ã(t; X) dt,
X ∼ pū , ū = stopgrad(u),
(37)
where

d
⊤
dt ã(t; X) = −(ã(t; X) ∇x b(Xt , t) + ∇x f (Xt , t)),

(38)
(39)

ã(1; X) = ∇x g(X1 ).

Equations (38)-(39) define the lean adjoint state, and (37) is the complete Adjoint Matching objective. The
unique critical point of E[LAdj−Match ] is the optimal control, which we prove relying on Proposition 2 and
equation (36) (see Proposition 7 in Appendix E.3).
Compared to the importance sampling methods (Section 5.1.2), Adjoint Matching is a simple least-squares
regression objective and has no importance weighting. This allows it to avoid the pitfalls of high variance
importance weights and makes it as scalable as the adjoint methods while retaining the interpretation of
matching a target vector field.
Compared to the adjoint method (Section 5.1.1), Adjoint Matching produces a different gradient in expectation
than the continuous adjoint. This is because the lean adjoint state is not related to the gradient of the cost
functional anymore, i.e., (29) is not true, except at the optimum when u = u∗ . Even at the optimal solution,
since Adjoint Matching removes terms that have expectation zero, it can potentially exhibit better convergence
and lower variance than the continuous adjoint method. Additionally, computation of the lean adjoint state
(38) also exhibits a smaller computational cost due to the removal of the extra terms (no longer need the
Jacobian of the control ∇x u). We provide a rigorous derivation of Adjoint Matching and the above claims in
Appendix E.3.
Adjoint Matching can be applied to reward fine-tuning of dynamical generative models through the memoryless
SOC formulation discussed in Section 4. We provide pseudo-code for this in Algorithm 1 for Flow Matching
models and in Algorithm 2 in Appendix E.4 for denoising diffusion models.

11

6

Related work

Fine-tuning from human feedback. There are two main overarching approaches to RLHF: the reward-based
approach (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022) and direct preference
optimization (DPO; Rafailov et al. (2023)). The reward-based approach (Ziegler et al., 2020; Stiennon et al.,
2020; Ouyang et al., 2022; Bai et al., 2022) consists in learning the reward model r(x) from human preference
data, and then solving a maximum entropy RL problem with rewards produced by r(x). DPO merges the two
previous steps into one: there is no need to learn r(x) as human preference data is directly used to fine-tune
the model. However, DPO is typically only applied with a filtered dataset, and does not work explicitly with
a reward model. Furthermore, for flow and diffusion models specifically, it is possible to differentiate the
reward function, so there is a larger emphasis on reward-based approaches.
Fine-tuning for diffusion models. Among existing reward-based diffusion fine-tuning methods, Fan and Lee
(2023) interpret the denoising process as a multi-step decision-making task and use policy gradient algorithms
to fine-tune diffusion samplers. Black et al. (2024) makes use of proximal policy gradients for fine-tuning
but this does not make use of the differentiability of the reward model. Fan et al. (2023) also consider
KL-regularized rewards (19) but do not make the critical connection to the tilted distribution (1) that we
flesh out in Section 4.2. The fine-tuning algorithms of Xu et al. (2023); Clark et al. (2024) directly take
gradients of the reward model and use heuristics to try to stay close to the original base generative model,
but their behavior is not well understood and unrelated to the tilted distribution: Xu et al. (2023) takes
gradients of the reward applied on the denoised sample at different points in time, and Clark et al. (2024)
backpropagates the reward function through all or part of the diffusion trajectory. Finally, Uehara et al.
(2024b) also fine-tune diffusion models with the goal of sampling from the tilted distribution (1), but their
approach is much more involved than ours as it requires learning a value function, and solving two stochastic
optimal control problems. Additional reward fine-tuning works include Bruna and Han (2024), that provide
theoretical guarantees to sample from the tilted distribution when the reward is a quadratic function, and
Zhang et al. (2024), that propose a reward fine-tuning algorithm for the GFlowNet architecture.
Inference-time optimization methods. Some have proposed methods that do not update the base model but
instead modify the generation process directly. One approach is to add a guidance term to the velocity (Chung
et al., 2022; Song et al., 2023; Pokle et al., 2023); however, this is a heuristic and it is not well-understood
what particular distribution is being generated. Another approach is to directly optimize the initial noise
distribution (Li, 2021; Wallace et al., 2023b; Ben-Hamu et al., 2024); this is taking an opposite approach to
the inital value bias problem than us by moving all of the work into optimizing the initial distribution. A more
computationally intensive approach is to perform online estimation of the optimal control, for the purpose
of heuristically solving an optimal control problem within the sampling process (Huang et al., 2024; Rout
et al., 2024); these approaches aim to solve a separate control problem for each generated sample, instead of
performing amortization (Amos et al., 2023) to learn a fine-tuned generative model.
Optimal control in generative modeling. Methods from optimal control have been used to train dynamical
generative models parameterized by ODEs (Chen et al., 2018), SDEs (Li et al., 2020), and jump processes
(Chen et al., 2021), enabled through the adjoint method. They can be used to train arbitrary generative
processes, but for simplified constructions these have fallen in favor of simulation-free matching objectives
such as denoising score matching (Vincent, 2011) and Flow Matching (Lipman et al., 2023). The optimal
control formalism also has significance in sampling from un-normalized distributions (Zhang and Chen, 2022;
Berner et al., 2023; Vargas et al., 2023, 2022; Richter and Berner, 2024; Tzen and Raginsky, 2019). The
inclusion of a state cost has been used to solve transport problems where intermediate path distributions
are of importance (Liu et al., 2024; Pooladian et al., 2024). These collective advances naturally lead to the
consideration of the optimal control formalism for reward fine-tuning.
Conditional sampling in inverse problems. Denker et al. (2024) and Wu et al. (2023a) independently consider
a pre-trained diffusion model p(x), and an observation y on the generated sample x, as well as the analytic
likelihood p(y|x). Their aim is to sample from the posterior p(x)p(y|x), and their applications include
inpainting, class-conditional generation, super-resolution, phase retrieval, non-linear deblurring, computed
12

Fine-tuning
Method

Fine-tuning
σ(t)

None
(Base model)

N/A

Baselines

DRaFT-1
DRaFT-40
DPO

Memoryless SOC

ReFL

√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

Cont. Adjoint
λ = 12500

√
2ηt

Disc. Adjoint
λ = 12500

√
2ηt

Adj.-Matching
λ = 1000

√
2ηt

Adj.-Matching
λ = 2500

√
2ηt

Adj.-Matching
λ = 12500

√
2ηt

Sampling
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
√
√

ClipScore ↑

PickScore ↑

HPS v2 ↑

DreamSim
Diversity ↑

24.15±0.26
28.32±0.22

17.25±0.06
18.15±0.07

16.19±0.17
17.89±0.16

53.60±1.37
56.53±1.52

30.18±0.24
30.95±0.28

19.38±0.08
19.37±0.06

24.61±0.17
24.37±0.17

25.54±0.99
27.39±1.14

26.94±0.28
30.07±0.39

18.34±0.19
19.45±0.08

19.98±1.02
24.06±0.24

41.98±2.14
36.53±1.69

24.11±0.22
27.77±0.18

17.24±0.06
17.92±0.07

16.15±0.14
17.30±0.20

53.27±1.36
54.11±1.50

28.59±0.31
30.06±0.63

18.68±0.10
19.07±0.21

22.24±0.46
23.06±0.41

32.71±2.76
32.69±1.28

26.99±0.43
29.49±0.32

18.33±0.16
18.98±0.16

20.83±0.63
21.34±0.53

46.59±1.40
48.41±1.44

28.04±0.57
29.28±0.17

18.44±0.21
18.82±0.14

20.04±0.39
19.73±0.17

54.90±2.03
53.36±2.48

2ηt
0

30.36±0.22
31.41±0.22

19.29±0.08
19.57±0.09

24.12±0.17
23.29±0.18

40.89±1.50
43.10±1.76

2ηt
0

30.59±0.40
31.64±0.21

19.49±0.10
19.71±0.09

24.85±0.23
24.12±0.27

37.07±1.47
39.88±1.59

2ηt
0

30.62±0.30
31.65±0.19

19.50±0.09
19.76±0.08

24.95±0.28
24.49±0.27

34.50±1.33
37.24±1.57

Table 2 Evaluation metrics of different fine-tuning methods for text-to-image generation.
The second and third
√
columns show the noise schedules σ(t) used for fine-tuning and for sampling: σ(t) = 2ηt corresponds to Memoryless
Flow Matching, and σ(t) = 0 to the Flow Matching ODE (3). We report standard errors estimated over 3 runs of
the fine-tuning algorithm on random sets of 40000 training prompts, each evaluated over a random set of 1000 test
prompts.

tomography, and protein design. Their setting reduces to a particular case of our reward fine-tuning framework
by setting r(x) = log p(y|x). Denker et al. (2024) formulate an SOC problem, and they solve it via the
log-variance loss (Richter et al. (2020); Nüsken and Richter (2021)), and the moment loss (Nüsken and Richter,
2021)7 , which they refer to as the trajectory balance loss (Malkin et al., 2023). Wu et al. (2023a) propose
Twisted Diffusion Sampler, an algorithm based on Sequential Monte Carlo that uses increased inference-time
compute to reduce bias. A third work that also tackles the conditional sampling problem is Du et al. (2024),
which use a Lagrangian formulation that they solve approximately using Gaussian paths.

7

Experiments

We experimentally validate our proposed method on reward fine-tuning a Flow Matching base model (Lipman
et al., 2023). In particular, we use the usual setup of pre-training an autoencoder for 512×512 resolution
images, then training a text-conditional Flow Matching model on the latent variables with a U-net architecture
(Long et al., 2015), similar to the setup in Rombach et al. (2022). We pre-trained our base model using a
dataset of licensed text and image pairs. Then for fine-tuning, we consider the reward function:
r(x) := λ × RewardModel(x)

(43)

corresponding to a scaled version of the reward model, which we take to be ImageReward (Xu et al., 2023).
Different values of λ provide different tradeoffs between the KL regularization and the reward model (19).
7 See also Domingo-Enrich (2024) for a comparison among SOC losses.

13

λ = 1000

1000 itrs.
4000 itrs.

λ = 12500

λ = 2500

2000 itrs.

Adjoint Matching (Ours)

DRaFT-1

w = 4.0

w = 1.0

w = 0.0

Figure 3 Our proposed Adjoint Matching using the memoryless SOC formulation introduces a much more principled
way of trading off how close to stay to the base model while optimizing the reward model. In contrast, baseline
methods such as DRaFT-1 only optimize the reward model and must rely on early stopping to perform this trade off,
resulting in a much more sensitive hyperparameter. Samples are produced using σ(t) = 0 with the same noise sample.
Text prompts: “Handsome Smiling man in blue jacket portrait” and “Quinoa and Feta Stuffed Baby Bell Peppers”.

Text prompt: “Man sitting on sofa at home in front of
fireplace and using laptop computer, rear view ”

Text prompt: “3D World Food Day Morocco”

Figure 4 Generated samples from varying classifier-free guidance weight w, from an Adjoint Matching fine-tuned
model. Higher guidance increases text-to-image consistency but loses diversity and has use cases for generating highly
structured images such as 3D renderings. Corresponding samples from the base model can be found in Figure 7.

For evaluation and benchmarking purposes, we report metrics that separately quantify text-to-image consistency, human preference, and sample diversity, capturing the tradeoff between each aspect of generative
models (Astolfi et al., 2024). For consistency, we make use of the standard ClipScore (Hessel et al., 2021) and
PickScore (Kirstain et al., 2023); for generalization to unseen human preferences, we use the HPSv2 model
(Wu et al., 2023b); and for diversity, we compute averages of pairwise distances of the DreamSim features (Fu
et al., 2023). More details are provided in Appendix G.4.
As our baselines, we consider the DPO (Wallace et al., 2023a), ReFL (Xu et al., 2023), and DRaFT-K
algorithms (Clark et al., 2024). DPO does not use gradients from the reward function, while ReFL and
DRaFT make use of heuristic gradient stopping approaches to stay close to the base generative model. Out
of these baseline methods, we find that DRaFT-1 performs the best, so we perform additional ablation
experiments comparing to this method. Within the same SOC formulation as our method, we also consider the
14

Adj. Match. w=0
DRaFT-1 w=0

Adj. Match. w=1
DRaFT-1 w=1

Adj. Match. w=4
DRaFT-1 w=4

25.5

33.5

33.5

25.0

33.0

33.0

32.0
31.5

ClipScore

32.5

HPS v2

ClipScore

24.5
24.0
23.5
23.0
22.5

31.0

32.5
32.0
31.5
31.0

22.0
30.5
20

25

30

35

40

DreamSim Diversity

21.5

20

25

30

35

40

DreamSim Diversity

30.5
21.5 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5

HPS v2

Figure 5 Tradeoffs between different aspects of generative models: text-to-image consistency (ClipScore), sample
diversity for each prompt (DreamSim Diversity), and generalization to unseen human preferences (HPS v2). Different
points are obtained from varying values of λ for Adjoint Matching and varying number of fine-tuning iterations for the
DRaFT-1 baseline. Overall, we find our proposed method Adjoint Matching has the best Pareto fronts.

discrete and continuous adjoint methods. We provide full experimental details in Appendix G; an important
implementation detail is that we slightly offset σ(t) in order to avoid division by zero.
Evaluation results. In Table 2 we report the evaluation metrics for the baselines as well as our proposed
Adjoint Matching approach. We compare each method at roughly the same wall clock time (see the times
and number of iterations in Table 4, and comments in Appendix G.5). We find that across all metrics, our
proposed memoryless SOC formulation outperforms existing baseline methods. The choice of SOC algorithms
also obviously favors Adjoint Matching over continuous and discrete adjoint methods, which result in poorer
consistency and human preference metrics.
Ablation: base model vs. reward tradeoff. We note that the scaling in front of the reward model λ determines
how strongly the we should prefer the reward model over the base model. As such, we see a natural tradeoff
curve: higher λ results in better consistency and human preference, but lower diversity in the generated
samples. Overall, we find that Adjoint Matching performs stably across all values of λ. Our method of
regularizing the fine-tuning procedure through memoryless SOC works much better than baseline methods
which often must employ early stopping. We show the qualitative effect of varying λ in Figure 3, while for the
DRaFT-1 baseline we show the effect of varying the number of fine-tuning iterations.
Ablation: classifier-free guidance. We note that it is possible to apply classifier-free guidance (CFG; Ho
and Salimans (2022); Zheng et al. (2023)) after fine-tuning. We use the formula (1 + w)v(x, t|y) − wv(x, t),
where w is the guidance weight, v(x, t|y) is a fine-tuned text-to-image model while v(x, t) is an unconditional
image model. This is not principled as only the conditional model is fine-tuned, but generally it is unclear
what distribution guided models sample from anyhow. In Figure 5 we show the evaluation metrics with
classifier-free guidance applied. Comparing three different guidance weight values, we see a higher weight does
improve text-to-image consistency, and to some extent, human preference, but this comes at the cost of being
worse in terms of diversity. We show qualitative differences in Figure 4.

8

Conclusion

We investigate the problem of fine-tuning dynamical generative models such as Flow Matching and propose
the use of a stochastic optimal control (SOC) formulation with a memoryless noise schedule. This ensures
we converge to the same tilted distribution that the large language modeling literature uses for learning

15

from human feedback. In particular, the memoryless noise schedule corresponds to DDPM sampling for
diffusion models and a new Memoryless Flow Matching generative process for flow models. In conjunction,
we propose a novel training algorithm for solving stochastic optimal control problems, by casting SOC as a
regression problem, which we call the Adjoint Matching objective. Empirically, we find that our memoryless
SOC formulation works better than multiple existing works on fine-tuning diffusion models, and our Adjoint
Matching algorithm outperforms related gradient-based methods. In summary, we are the first to provide
a theoretically-driven algorithm for fine-tuning Flow Matching models, and we find that our approach
significantly outperforms baseline methods across multiple axes of evaluation—text-to-image consistency,
generalization to unseen human preference, and sample diversity—on large-scale text-to-image generation.

References
Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for
flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Cited on page 36.
Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The
Eleventh International Conference on Learning Representations, 2023. Cited on pages 2, 3, and 36.
Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends® in Machine Learning, 16(5):
592–732, 2023. Cited on page 12.
Pietro Astolfi, Marlene Careil, Melissa Hall, Oscar Mañas, Matthew Muckley, Jakob Verbeek, Adriana Romero Soriano,
and Michal Drozdzal. Consistency-diversity-realism pareto fronts of conditional image generative models. arXiv
preprint arXiv:2406.10429, 2024. Cited on page 14.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk,
Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,
Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,
and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback.
arXiv preprint arXiv:2204.05862, 2022. Cited on pages 2 and 12.
Grigory Bartosh, Dmitry Vetrov, and Christian A. Naesseth. Neural diffusion models. arXiv preprint arXiv:2310.08337,
2024a. Cited on page 4.
Grigory Bartosh, Dmitry Vetrov, and Christian A. Naesseth. Neural flow diffusion models: Learnable forward process
for improved diffusion modelling. arXiv preprint arXiv:2404.12940, 2024b. Cited on page 4.
Richard Bellman. Dynamic programming. Princeton Landmarks in Mathematics. Princeton University Press, Princeton,
NJ, 2010., 1957. Cited on page 5.
Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through
flows for controlled generation. arXiv preprint arXiv:2402.14017, 2024. Cited on page 12.
Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative
modeling. arXiv preprint arXiv:2211.01364, 2023. Cited on page 12.
Joris Bierkens and Hilbert J Kappen. Explicit solution of relative entropy weighted control. Systems & Control Letters,
72:36–43, 2014. Cited on page 9.
Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement
learning. In The Twelfth International Conference on Learning Representations, 2024. Cited on pages 2, 12, and 37.
Joan Bruna and Jiequn Han. Posterior sampling with denoising oracles via tilted transport.
arXiv:2407.00745, 2024. Cited on page 12.

arXiv preprint

Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations.
In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Cited on pages 9
and 12.
Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel. Learning neural event functions for ordinary differential
equations. In International Conference on Learning Representations, 2021. Cited on pages 9 and 12.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv
preprint arXiv:1604.06174, 2016. Cited on page 9.

16

Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling
for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. Cited on page 12.
Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable
rewards. In The Twelfth International Conference on Learning Representations, 2024. Cited on pages 2, 12, and 14.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge with applications
to score-based generative modeling. In Advances in Neural Information Processing Systems, volume 34, pages
17695–17709. Curran Associates, Inc., 2021. Cited on page 34.
Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano,
Emile Mathieu, Urszula Julia Komorowska, and Pietro Lio. Deft: Efficient finetuning of conditional diffusion models
by learning the generalised h-transform. arXiv preprint arXiv:2406.01781, 2024. Cited on pages 12 and 13.
Carles Domingo-Enrich. A taxonomy of loss functions for stochastic optimal control. arXiv preprint arXiv:2410.00345,
2024. Cited on page 13.
Carles Domingo-Enrich, Jiequn Han, Brandon Amos, Joan Bruna, and Ricky T. Q. Chen. Stochastic optimal control
matching. arXiv preprint arXiv:2312.02027, 2023. Cited on pages 5, 8, 9, 10, 46, and 47.
Yuanqi Du, Michael Plainer, Rob Brekelmans, Chenru Duan, Frank Noé, Carla P. Gomes, Alan Apsuru-Guzik, and
Kirill Neklyudov. Doob’s lagrangian: A sample-efficient variational approach to transition path sampling. arXiv
preprint arXiv:2410.07974, 2024. Cited on page 13.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In
Forty-first International Conference on Machine Learning, 2024. Cited on page 2.
Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. In International Conference on
Machine Learning, 2023. Cited on pages 2 and 12.
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion
models. arXiv preprint arXiv:2305.16381, 2023. Cited on pages 2 and 12.
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models.
Advances in Neural Information Processing Systems, 36, 2024. Cited on pages 2 and 6.
W.H. Fleming and R.W. Rishel. Deterministic and Stochastic Optimal Control. Stochastic Modelling and Applied
Probability. Springer New York, 2012. Cited on page 5.
Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim:
Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023.
Cited on pages 14 and 54.
Vicenç Gómez, Hilbert J Kappen, Jan Peters, and Gerhard Neumann. Policy search for path integral control. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases, pages 482–497. Springer, 2014.
Cited on page 9.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014. Cited on page 2.
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse problems, 34(1):014004, 2017.
Cited on page 9.
Jiequn Han and Weinan E. Deep learning approximation for stochastic control problems.
arXiv:1611.07422, 2016. Cited on page 9.

arXiv preprint

Carsten Hartmann and Christof Schütte. Efficient rare event simulation by optimal nonequilibrium forcing. Journal of
Statistical Mechanics: Theory and Experiment, 2012(11):P11004, 2012. Cited on page 9.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation
metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Cited on pages 2 and 14.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Cited on
pages 2, 15, and 25.

17

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information
Processing Systems, volume 33. Curran Associates, Inc., 2020. Cited on pages 2, 3, 4, and 7.
Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani,
Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv
preprint arXiv:2402.14285, 2024. Cited on page 12.
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
Openclip, July 2021. Cited on page 54.
H J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Statistical Mechanics:
Theory and Experiment, 2005(11), nov 2005. Cited on pages 5 and 9.
Hilbert J Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine learning, 87(2):159–182, 2012. Cited on page 9.
Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing
Systems, 2021. Cited on page 2.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023. Cited on pages 2, 14, and 54.
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar,
Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances
in neural information processing systems, 36, 2024. Cited on page 2.
Dongzhuo Li. Differentiable gaussianization layers for inverse problems regularized by deep generative models. arXiv
preprint arXiv:2112.03860, 2021. Cited on page 12.
Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients for stochastic
differential equations. In International Conference on Artificial Intelligence and Statistics, pages 3870–3882. PMLR,
2020. Cited on pages 9 and 12.
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative
modeling. In The Eleventh International Conference on Learning Representations, 2023. Cited on pages 2, 3, 12, 13,
and 36.
Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou, and Ricky T. Q. Chen.
Generalized schrödinger bridge matching. In The Twelfth International Conference on Learning Representations,
2024. Cited on page 12.
Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577,
2022. Cited on page 3.
Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with
rectified flow. In The Eleventh International Conference on Learning Representations, 2023. Cited on pages 2 and 3.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015. Cited on
page 13.
Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit
assignment in gflownets. arXiv preprint arXiv:2201.13259, 2023. Cited on page 13.
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker–planck equations
through gradient–log–density estimation. Entropy, 22(8):802, 2020. Cited on page 3.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine
learning. Journal of Machine Learning Research, 21(132):1–62, 2020. Cited on page 9.
Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. Google
research blog, 20(14):5, 2015. Cited on page 2.

18

Nikolas Nüsken and Lorenz Richter. Solving high-dimensional Hamilton–Jacobi–Bellman pdes using neural networks:
perspectives from the theory of controlled diffusions and measures on path space. Partial differential equations and
applications, 2:1–48, 2021. Cited on page 13.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to
follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages
27730–27744. Curran Associates, Inc., 2022. Cited on pages 2, 6, and 12.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
Advances in neural information processing systems, 32, 2019. Cited on page 9.
Ashwini Pokle, Matthew J Muckley, Ricky T. Q. Chen, and Brian Karrer. Training-free linear image inversion via
flows. arXiv preprint arXiv:2310.04432, 2023. Cited on page 12.
L.S. Pontryagin. The Mathematical Theory of Optimal Processes. Interscience Publishers, 1962. Cited on pages 8
and 9.
Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T. Q. Chen, and Brandon Amos. Neural optimal transport
with lagrangian costs. arXiv preprint arXiv:2406.00288, 2024. Cited on page 12.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. Cited on pages 6 and 12.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by
approximate inference. In Twenty-Third International Joint Conference on Artificial Intelligence, 2013. Cited on
pages 6 and 9.
Lorenz Richter and Julius Berner. Improved sampling via learned diffusions. In The Twelfth International Conference
on Learning Representations, 2024. Cited on page 12.
Lorenz Richter, Ayman Boustati, Nikolas Nüsken, Francisco Ruiz, and Omer Deniz Akyildiz. VarGrad: A low-variance
gradient estimator for variational inference. Advances in Neural Information Processing Systems, 33, 2020. Cited on
page 13.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. Cited on pages 2 and 13.
Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng
Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. arXiv
preprint arXiv:2405.17401, 2024. Cited on page 12.
Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to combinatorial optimization,
Monte-Carlo simulation and machine learning. Springer Science & Business Media, 2013. Cited on page 9.
Christoph Schuhmann and Romain Beaumont. Laion-aesthetics, 2022. Cited on page 2.
S.P. Sethi. Optimal Control Theory: Applications to Management Science and Economics. Springer International
Publishing, 2018. Cited on page 5.
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792,
2022. Cited on page 2.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on
Learning Representations, 2021a. Cited on pages 3, 7, and 31.
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse
problems. In International Conference on Learning Representations, 2023. Cited on page 12.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv preprint
arXiv:1907.05600, 2019. Cited on page 2.

19

Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning
Representations (ICLR 2021), 2021b. Cited on pages 2 and 3.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,
and Paul F Christiano. Learning to summarize with human feedback. In Advances in Neural Information Processing
Systems, volume 33, pages 3008–3021. Curran Associates, Inc., 2020. Cited on pages 2 and 12.
Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake. Do differentiable simulators give better policy
gradients? In International Conference on Machine Learning, pages 20668–20696. PMLR, 2022. Cited on page 9.
Wenpin Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond. arXiv preprint
arXiv:2403.06279, 2024. Cited on page 6.
Emanuel Todorov. Linearly-solvable markov decision problems. Advances in neural information processing systems, 19,
2006. Cited on page 5.
Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative models with latent
diffusions. arXiv:1903.01608, 2019. Cited on page 12.
Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based
fine-tuning of diffusion models: A tutorial and review. arXiv preprint arXiv:2407.13734, 2024a. Cited on page 2.
Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M
Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropyregularized control. arXiv preprint arXiv:2402.15194, 2024b. Cited on pages 2, 6, 12, and 37.
Francisco Vargas, Andrius Ovsianas, David Lopes Fernandes, Mark Girolami, Neil D Lawrence, and Nikolas Nüsken.
Bayesian learning via neural schrödinger-föllmer flows. In Fourth Symposium on Advances in Approximate Bayesian
Inference, 2022. Cited on page 12.
Francisco Vargas, Will Sussman Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In The Eleventh
International Conference on Learning Representations, 2023. Cited on page 12.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):
1661–1674, 2011. Cited on page 12.
Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang,
Robert Adkins, William Ngan, et al. Audiobox: Unified audio generation with natural language prompts. arXiv
preprint arXiv:2312.15821, 2023. Cited on page 2.
Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming
Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. arXiv preprint
arXiv:2311.12908, 2023a. Cited on pages 2, 14, 22, and 52.
Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves
classifier guidance. arXiv preprint arXiv:2303.13703, 2023b. Cited on page 12.
Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John P Cunningham. Practical and asymptotically
exact conditional sampling in diffusion models. In Advances in Neural Information Processing Systems, volume 36,
pages 31372–31403. Curran Associates, Inc., 2023a. Cited on pages 12 and 13.
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score
v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341,
2023b. Cited on pages 14 and 54.
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score
v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341,
2023c. Cited on page 2.
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:
Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. Cited on pages 2, 12, 13, 14, 22, and 51.
Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang Zhang, Josh Susskind, Navdeep Jaitly, and Shuangfei Zhai.
Improving gflownets for text-to-image diffusion alignment. arXiv preprint arXiv:2406.00633, 2024. Cited on page 12.

20

Qinsheng Zhang and Yongxin Chen. Path integral sampler: A stochastic control approach for sampling. In International
Conference on Learning Representations, 2022. Cited on page 12.
Wei Zhang, Han Wang, Carsten Hartmann, Marcus Weber, and Christof Schütte. Applications of the cross-entropy
method to importance sampling and optimal control of diffusions. SIAM Journal on Scientific Computing, 36(6):
A2654–A2672, 2014. Cited on page 9.
Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, and Ricky T. Q. Chen. Guided flows for generative
modeling and decision making. arXiv preprint arXiv:2311.13443, 2023. Cited on pages 2 and 15.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement
learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008. Cited on pages 5 and 37.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and
Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2020.
Cited on pages 2 and 12.

21

Appendix
Contents
A Additional Figures & Tables

23

B Results on DDIM and Flow Matching
B.1 The continuous-time limit of DDIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Forward and backward stochastic differential equations . . . . . . . . . . . . . . . . . . . . . .
B.2.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2.3 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 The relationship between the noise predictor ϵ and the score function . . . . . . . . . . . . . .
B.4 The relationship between the vector field v and the score function . . . . . . . . . . . . . . . .

31
31
31
33
33
34
36
36

C Stochastic optimal control as maximum entropy RL in continuous space and time
C.1 Maximum entropy RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 From maximum entropy RL to stochastic optimal control . . . . . . . . . . . . . . . . . . . .
C.3 Proof of Proposition 5: from MaxEnt RL to SOC . . . . . . . . . . . . . . . . . . . . . . . . .
C.4 Proof of equation (18): the control cost is a KL regularizer . . . . . . . . . . . . . . . . . . .

37
37
38
39
41

D Proofs of Section 4.3: memoryless noise schedule and fine-tuning recipe
D.1 Proof of Proposition 1: the memoryless noise schedule . . . . . . . . . . . . . . . . . . . . . .
D.2 Proof of Theorem 1: fine-tuning recipe for general noise schedules . . . . . . . . . . . . . . . .

42
42
43

E Loss function derivations
E.1 Derivation of the Continuous Adjoint method . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.2 Proof of Proposition 2: Theoretical guarantees of the basic Adjoint Matching loss . . . . . . .
E.3 Theoretical guarantees of the Adjoint Matching loss . . . . . . . . . . . . . . . . . . . . . . .
E.4 Pseudo-code of Adjoint Matching for DDIM fine-tuning . . . . . . . . . . . . . . . . . . . . .

46
46
48
49
50

F Adapting diffusion fine-tuning baselines to flow matching
F.1 Adapting ReFL (Xu et al., 2023) to flow matching . . . . . . . . . . . . . . . . . . . . . . . .
F.2 Adapting Diffusion-DPO (Wallace et al., 2023a) to flow matching . . . . . . . . . . . . . . . .

51
51
52

G Experimental details
G.1 Noise schedule details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.2 Selection of gradient evaluation timesteps . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.3 Loss function clipping: the LCT hyperparameter . . . . . . . . . . . . . . . . . . . . . . . . .
G.4 Computation of evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.5 Remarks on computational costs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.6 Remarks on number of sampling timesteps . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53
53
54
54
54
55
55

22

A

Additional Figures & Tables

w = 4.0

w = 1.0

w = 0.0

Rt
Figure 6 Average values of ImageReward (reward function), control cost ( 0 12 ∥u(Xtu , t)∥2 dt), and ClipScore vs.
wall-clock time for Adjoint Matching and our baselines. Lines show averages over three fine-tuning runs, evaluating on
separate test datasets of size 200. Confidence intervals show standard errors of estimates.

Text prompt: “Man sitting on sofa at home in front of
fireplace and using laptop computer, rear view ”

Text prompt: “3D World Food Day Morocco”

Figure 7 Generated samples from varying classifier-free guidance weights, from the pre-trained Flow Matching model.
Corresponding samples from the fine-tuned model can be found in Figure 4.

23

Fine-tuning
loss

Fine-tuning
σ(t)

None
(CFG = 1.0)

N/A
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

DRaFT-1
DRaFT-40
DPO
ReFL
Cont. Adjoint
λ = 12500

√
2ηt

Disc. Adjoint
λ = 12500

√
2ηt

Adj.-Matching
λ = 1000

√
2ηt

Adj.-Matching
λ = 2500

√
2ηt

Adj.-Matching
λ = 12500

√
2ηt

Sampling
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

ImageReward ↑

ClipScore
diversity ↑

PickScore
diversity ↑

Total time (s) /
# iterations

−1.384±0.040
−0.920±0.042

28.07±1.40
30.29±1.53

1.63±0.08
1.82±0.09

N/A

1.357±0.039
1.251±0.040

16.86±0.98
16.76±1.06

1.21±0.07
1.27±0.07

140k±5.9k
/ 4000

−0.560±0.138
0.424±0.042

24.07±1.37
20.99±1.54

1.64±0.12
1.67±0.08

148k±4.2k
/ 1500

−1.386±0.033
−0.957±0.040

27.80±1.40
29.81±1.43

1.62±0.08
1.68±0.10

118k±0.6k
/ 1000

0.687±0.085
0.709±0.080

19.49±1.76
18.39±1.11

1.22±0.08
1.31±0.10

173k±10.9k
/ 6000

−0.448±0.135
−0.249±0.116

26.97±1.37
26.25±1.30

1.82±0.09
1.90±0.10

153k±0.9k
/ 750

−0.557±0.113
−0.552±0.041

30.40±2.39
28.37±2.26

1.91±0.09
1.97±0.09

152k±1.5k
/ 1000

0.550±0.043
0.454±0.055

23.00±1.27
22.76±1.40

1.65±0.08
1.73±0.09

0.755±0.040
0.671±0.047

21.33±1.71
21.42±1.54

1.55±0.08
1.64±0.08

0.882±0.058
0.778±0.050

20.49±1.48
20.34±1.49

1.50±0.09
1.57±0.09

156k±1.9k
/ 1000

Table 3 Metrics for various fine-tuning methods for text-to-image generation.
The second and third columns show the
√
noise schedules σ(t) used for fine-tuning and for inference: σ(t) = 2ηt corresponds to Memoryless Flow Matching,
and σ(t) = 0 to the Flow Matching ODE (3). Confidence intervals show standard errors of estimates; computed over 3
runs of the fine-tuning algorithm on separate fine-tuning prompt datasets of size 40000 each. Test prompt sets are of
size 1000, and also different for each run.

Fine-tun.
loss
ReFL
DRaFT-1
Draft-40

Fine-tun.
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0

Adj.-Match.
λ = 1000

√

Adj.-Match.
λ = 2500

√

Adj.-Match.
λ = 12500

√

Cont. Adj.
λ = 12500

√

Disc. Adj.
λ = 12500

√

2ηt
2ηt
2ηt
2ηt
2ηt

Generat.
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

ImageReward ↑

ClipScore ↑

PickScore ↑

HPS v2 ↑

DreamSim
diversity ↑

Runtime/
#iter.

0.459±0.096
0.330±0.114

28.46±0.25
29.63±0.61

18.77±0.09
19.08±0.18

22.54±0.17
22.46±0.77

37.51±3.50
39.51±1.30

43k±2.7k
/ 1500

0.913±0.068
0.626±0.195

29.80±0.22
30.48±0.32

19.16±0.06
18.91±0.34

23.63±0.16
21.92±1.63

35.21±1.93
38.52±2.01

35k±1.5k
/ 1000

−1.427±0.267
−0.097±0.052

23.39±1.72
29.12±0.41

17.24±0.45
18.97±0.14

15.72±1.80
21.93±0.20

41.98±2.14
46.35±1.34

49k±1.4k
/ 500

0.107±0.046
0.051±0.044

29.37±0.25
30.58±0.17

19.05±0.07
19.31±0.07

22.79±0.20
21.93±0.23

46.38±1.36
48.12±1.56

0.199±0.068
0.106±0.067

29.27±0.21
30.43±0.24

19.07±0.10
19.32±0.11

22.98±0.30
22.16±0.33

45.03±1.61
47.61±1.49

0.299±0.095
0.224±0.051

29.61±0.37
30.70±0.23

19.26±0.14
19.52±0.11

23.67±0.27
22.93±0.21

43.36±1.93
44.62±1.79

−0.910±0.116
−0.681±0.051

26.29±0.44
28.50±0.19

18.06±0.16
18.69±0.11

18.86±0.88
19.90±0.50

51.60±1.97
50.87±1.52

51k±0.3k
/ 250

−0.978±0.123
−0.791±0.065

26.68±0.76
28.66±0.33

18.51±0.11
18.51±0.11

18.53±0.28
18.53±0.28

55.95±1.70
54.78±2.00

38k±0.4k
/ 250

39k±0.5k
/ 250

Table 4 Additional metrics for various fine-tuning methods for text-to-image generation, which complement the ones
in Table 2 (both tables correspond to the same
√ runs). The second and third columns show the noise schedules σ(t)
used for fine-tuning and for inference: σ(t) = 2ηt corresponds to Memoryless Flow Matching, and σ(t) = 0 to the
Flow Matching ODE (3).

24

w

Fine-tuning
loss

#iter.
/λ

Fine-tun.
σ(t)

0.0

None

N/A

N/A

1000

2ηt
0

Sampl.
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

2000

0

0

1.177±0.053

32.36±0.18

19.67±0.08

24.48±0.28

25.09±1.82

3000

0

0

1.255±0.038

32.36±0.19

19.70±0.06

24.64±0.17

23.24±1.19

4000

0

0

1.296±0.033

32.30±0.19

19.68±0.06

24.71±0.14

21.54±0.96

√
1000
√
0.0

DRaFT-1

2000
√
3000
√
4000
√
1000
√

0.0

Adj.-Match.

2500
√

1.0

None

DRaFT-1

2ηt
0
2ηt
0
2ηt
0
2ηt
0
2ηt
0

12500

2ηt
0

N/A

N/A
√

1.0

2ηt
0

1000

0
√

1.0

Adj.-Match.

2500

2ηt
0

12500

0

√

√
4.0

None

N/A

N/A
√

4.0

DRaFT-1

PickScore ↑

HPS v2 ↑

DreamSim
diversity ↑

−1.384±0.040
−0.920±0.042

24.15±0.26
28.32±0.22

17.25±0.06
18.15±0.07

16.19±0.17
17.89±0.16

53.60±1.37
56.53±1.52

0.913±0.068
0.626±0.195

29.80±0.22
30.48±0.32

19.16±0.06
18.91±0.34

23.63±0.16
21.92±1.63

35.21±1.93
38.52±2.01

1.204±0.046
1.052±0.088

29.90±0.43
30.65±0.24

19.29±0.12
19.27±0.11

24.40±0.27
23.81±0.44

28.51±1.68
32.11±2.37

1.307±0.041
1.173±0.058

29.96±0.22
30.86±0.25

19.31±0.06
19.37±0.06

24.42±0.13
24.17±0.23

26.57±1.32
29.69±1.30

1.357±0.039
1.251±0.040

30.18±0.24
30.95±0.28

19.38±0.08
19.37±0.06

24.61±0.17
24.37±0.17

25.54±0.99
27.39±1.14

0.550±0.043
0.454±0.055

30.36±0.22
31.41±0.22

19.29±0.08
19.57±0.09

24.12±0.17
23.29±0.18

40.89±1.50
43.10±1.76

0.755±0.040
0.671±0.047

30.59±0.40
31.64±0.21

19.49±0.10
19.71±0.09

24.85±0.23
24.12±0.27

37.07±1.47
39.88±1.59

0.882±0.058
0.778±0.050

30.62±0.30
31.65±0.19

19.50±0.09
19.76±0.08

24.95±0.28
24.49±0.27

34.50±1.33
37.24±1.57

−0.269±0.050
−0.123±0.041

30.41±0.22
31.83±0.17

18.74±0.07
19.28±0.07

20.47±0.18
20.95±0.16

43.82±1.24
42.59±1.23

1.123±0.051
0.856±0.167

32.06±0.19
32.32±0.25

19.69±0.06
19.38±0.34

24.56±0.17
22.88±1.54

28.25±1.55
29.98±1.86

0

0.782±0.044

33.05±0.22

20.20±0.09

24.81±0.18

32.67±1.26

2ηt
0

1.027±0.038
0.910±0.040

32.85±0.21
33.20±0.17

20.08±0.08
20.29±0.09

25.88±0.20
25.39±0.24

29.83±1.00
30.34±1.51

0

0.985±0.041

33.10±0.18

20.28±0.08

25.61±0.27

28.86±1.37

2ηt
0

0.277±0.043
0.209±0.046

32.68±0.18
32.83±0.17

19.50±0.07
19.79±0.07

22.29±0.16
22.30±0.17

35.12±0.92
32.05±1.05

2ηt
0

1.062±0.045
0.604±0.395

32.29±0.16
31.80±0.86

19.48±0.06
19.09±0.53

23.67±0.13
21.69±2.10

25.03±1.32
25.92±2.57

2ηt
0

2000

0

0

1.112±0.046

32.29±0.20

19.34±0.11

23.31±0.22

21.02±1.67

3000

0

0

1.151±0.036

32.31±0.21

19.36±0.06

23.29±0.14

19.53±1.24

4000

0

0

1.172±0.040

32.20±0.22

19.30±0.07

23.20±0.15

18.45±1.06

0
√

Adj.-Match.

ClipScore ↑

1000

1000
4.0

√

ImageReward ↑

2500

2ηt
0

12500

0

√

0

0.852±0.046

33.50±0.22

20.31±0.08

24.97±0.19

25.83±0.82

2ηt
0

1.052±0.039
0.942±0.042

33.51±0.19
33.61±0.19

20.15±0.07
20.35±0.08

25.56±0.18
25.34±0.21

26.21±0.73
24.30±0.86

0

1.007±0.052

33.48±0.20

20.29±0.08

25.50±0.29

23.48±0.81

Table 5 Evaluation metrics when using classifier-free guidance (CFG; Ho and Salimans (2022)).
LR /
Adam β1

Fine-tuning
loss

3 × 10−5

DRaFT-1

/ 0.97

Adj.-Match.
λ = 12500

2 × 10−5
/ 0.95

Disc. Adj.
λ = 12500

Fine-tun.
σ(t)
√
2ηt
√
√

2ηt
2ηt
0

Generat.
σ(t)
√
2ηt
√
√

ImageReward ↑

ClipScore ↑

PickScore ↑

HPS v2 ↑

DreamSim
diversity ↑

1.467±0.029

30.28±0.56

19.37±0.09

24.70±0.15

21.20±0.93

2ηt

1.130±0.034

31.01±0.27

19.60±0.08

25.01±0.25

26.73±0.88

2ηt
0

−1.186±0.553
−0.961±0.653

21.95±4.29
24.07±4.71

16.94±0.95
17.86±1.17

12.34±4.40
15.93±5.80

28.33±10.26
33.62±7.80

Table 6 Metrics for alternative optimization hyperparameters (learning rate and Adam β1 ).

25

Fine-tuning
loss

Fine-tuning
σ(t)

Generative
σ(t)

ImageReward ↑

ClipScore ↑

PickScore ↑

HPS v2 ↑

DreamSim
diversity ↑

Adj.-Matching
λ = 12500

1

1
0

0.009±0.077
0.454±0.055

29.18±0.51
31.41±0.22

18.66±0.09
19.57±0.09

20.75±0.32
23.29±0.18

41.33±1.24
43.10±1.76

2ηt
0

0.882±0.058
0.778±0.050

30.62±0.30
31.65±0.19

19.50±0.09
19.76±0.08

24.95±0.28
24.49±0.27

34.50±1.33
37.24±1.57

Adj.-Matching
λ = 12500

√

√
2ηt

Table 7 Comparison with an alternative fine-tuning noise schedule σ(t) = 1. We see that the initial value function
bias (Section 4.2) results in the model not having a high reward function (ImageReward is the reward function used
for fine-tuning). Its performance on other metrics are also lower than when fine-tuning with the memoryless noise
schedule, except for diversity.

#sampl.
timesteps

10

20

40

100

200

Fine-tuning
loss

Fine-tun.
σ(t)

None (Base)

N/A
√

DRaFT-1

2ηt
0

Adj.-Match.
λ = 12500

√

None (Base)

N/A
√

DRaFT-1

2ηt

2ηt
0

Adj.-Match.
λ = 12500

√

None (Base)

N/A
√

DRaFT-1

2ηt

2ηt
0

Adj.-Match.
λ = 12500

√

None (Base)

N/A
√

DRaFT-1

2ηt

2ηt
0

Adj.-Match.
λ = 12500

√

None (Base)

N/A
√

DRaFT-1
Adj.-Match.
λ = 12500

√

2ηt

2ηt
0
2ηt

Sampl.
σ(t)
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0
√
2ηt
0

ImageReward ↑

ClipScore ↑

PickScore ↑

HPS v2 ↑

DreamSim
diversity ↑

−2.279±0.001
−1.386±0.040

13.99±0.12
26.26±0.24

14.98±0.05
17.64±0.07

7.37±0.10
14.92±0.17

5.07±0.13
51.26±1.38

1.033±0.051
1.236±0.038

25.98±0.25
31.54±0.27

18.28±0.07
19.53±0.07

22.08±0.18
24.47±0.19

14.47±0.67
24.78±0.88

−2.104±0.074
0.607±0.055

17.12±0.56
31.36±0.20

15.76±0.20
19.56±0.08

11.48±1.03
23.23±0.28

9.88±0.81
33.75±1.48

−2.275±0.002
−1.017±0.055

14.58±0.13
27.92±0.19

15.07±0.05
18.01±0.07

7.47±0.10
17.17±0.15

11.27±0.33
54.69±1.45

1.301±0.039
1.255±0.038

27.09±0.24
31.14±0.25

18.93±0.07
19.43±0.06

23.78±0.20
24.52±0.16

21.05±1.12
26.15±1.11

−0.032±0.072
0.768±0.048

25.07±0.27
31.70±0.17

18.01±0.07
19.73±0.08

20.75±0.23
24.30±0.26

29.06±2.34
35.90±1.52

−1.384±0.040
−0.920±0.042

24.15±0.26
28.32±0.22

17.25±0.06
18.15±0.07

16.19±0.17
17.89±0.16

53.60±1.37
56.53±1.52

1.357±0.039
1.251±0.040

30.18±0.24
30.95±0.28

19.38±0.08
19.37±0.06

24.61±0.17
24.37±0.17

25.54±0.99
27.39±1.14

0.882±0.058
0.778±0.050

30.62±0.30
31.65±0.19

19.50±0.09
19.76±0.08

24.95±0.28
24.49±0.27

34.50±1.33
37.24±1.57

−0.881±0.041
−0.881±0.036

27.83±0.19
28.65±0.18

18.10±0.07
18.22±0.06

18.43±0.17
18.20±0.17

57.21±1.50
57.73±1.68

1.343±0.040
1.239±0.037

30.64±0.20
30.74±0.28

19.38±0.08
19.33±0.06

24.37±0.17
24.24±0.17

25.51±1.10
28.70±1.11

0.892±0.044
0.779±0.048

31.23±0.23
31.64±0.17

19.65±0.08
19.76±0.08

24.92±0.23
24.57±0.25

35.13±1.40
38.26±1.65

−0.848±0.048
−0.871±0.036

28.37±0.21
28.50±0.18

18.27±0.08
18.23±0.06

18.56±0.19
18.25±0.14

58.00±1.58
57.84±1.60

1.331±0.044
1.222±0.042

30.69±0.23
30.77±0.27

19.36±0.07
19.32±0.06

24.21±0.17
24.18±0.16

26.41±1.18
29.09±1.07

0.869±0.062
0.766±0.050

31.33±0.21
31.61±0.16

19.68±0.09
19.75±0.08

24.81±0.30
24.52±0.24

35.90±1.55
38.60±1.38

Table 8 Performance metrics for different number of sampling steps. Only the number of sampling steps is ablated;
the fine-tuned models used in all cases are the ones fine-tuned using 40 steps.

26

Base Flow Matching model

Adjoint Matching (Ours)

DRaFT-1

Figure 8 Generated samples with classifier-free guidance (w = 1) and σ(t) = 0 across ten selected prompts. Each row
corresponds to a different prompt and each image corresponds to a different random seed consistent across models.

27

Base Flow Matching model

Adjoint Matching (Ours)

DRaFT-1

Figure 9 Generated samples with classifier-free guidance (w = 1) and σ(t) = 0 across ten selected prompts with people.
Each row corresponds to a different prompt and each image corresponds to a different random seed consistent across
models.

28

None (Base)
DRaFT-1
DRaFT-40
ReFL
Cont. Adj.
λ = 12500
Disc. Adj.
λ = 12500
Adj. match.
λ = 1000
Adj. match.
λ = 2500
Adj. match.
λ = 12500

Figure 10 Generated samples without guidance (w = 0) and σ(t) = 0 across seven selected prompts. Each row
corresponds to a different finetuning algorithm. Prompts: “Seaside view poster with palm trees vector image”, “Cayucos
Beach Inn”, “Happy Summer Life- Aloha Flowers and Melon - Pattern Metal Print”, “Castle Square, Warsaw Old
Town”, “Funny girl blowing soap bubbles. High quality photo”, “Colombian man with sweatshirt over yellow wall listening
to something by putting hand on the ear ”, “man in the hood black mask masquerade”.

29

None (Base)
DRaFT-1
DRaFT-40
ReFL
Cont. Adj.
λ = 12500
Disc. Adj.
λ = 12500
Adj. match.
λ = 1000
Adj. match.
λ = 2500
Adj. match.
λ = 12500

√
Figure 11 Generated samples without guidance (w = 0) and σ(t) = 2ηt across seven selected prompts. Each row
corresponds to a different finetuning algorithm. The prompts are the same as in Figure 10.

30

B

Results on DDIM and Flow Matching

B.1

The continuous-time limit of DDIM

The DDIM inference update (Song et al., 2021a, Eq. 12) is
√
p
√
ᾱ ϵ(xk ,k) 
√ k
xk+1 = ᾱk+1 xk − 1−
+ 1 − ᾱk+1 − σk2 ϵ(xk , k) + σk ϵk ,
ᾱk

xK ∼ N (0, I).

If we let ∆ᾱk = ᾱk+1 − ᾱk , we have that
q
q
q
q
−ᾱk
ᾱk+1
ᾱk +ᾱk+1 −ᾱk
ᾱk
= 1 + ᾱk+1
= 1 + ∆ᾱᾱkk ≈ 1 + ∆
ᾱk =
ᾱk
ᾱk
2ᾱk ,

(44)

(45)

√

where we used the first-order Taylor approximation of 1 + x. And
q
q
p
p

2 =−
− ᾱᾱk+1
1
−
ᾱ
−
σ
(1
−
ᾱ
)
+
1 + ∆ᾱᾱkk (1 − ᾱk ) + 1 − ᾱk+1 − σk2
k+1
k
k
k
q
q
p
p
= − 1 + ∆ᾱᾱkk − ᾱk − ∆ᾱk + 1 − ᾱk+1 − σk2 = − 1 − ᾱk+1 + ∆ᾱᾱkk + 1 − ᾱk+1 − σk2
q
q
2
 √


√
σ2
σk
∆ᾱk
∆ᾱk
= 1 − ᾱk+1 − 1 + ᾱk (1−
+
1 − 1−ᾱkk+1 ≈ 1 − ᾱk+1 − 1 + 2ᾱk (1−
ᾱk+1 )
ᾱk+1 ) + 1 − 2(1−ᾱk+1 )
2
σk
ᾱk
√ 1
=− ∆
,
2ᾱk + 2
1−ᾱk+1

(46)
where we used the same first-order Taylor approximation. Thus, up to first-order approximations, (44) is
equivalent to
2

σk
ᾱk
∆ᾱk
√ϵ(xk ,k) + σk ϵk ,
xk−1 = 1 + ∆
xK ∼ N (0, I).
(47)
2ᾱk xk − 2ᾱk + 2
1−ᾱk+1

If we modify our notation slightly, we can rewrite this as
√
2
˙ kh
˙ kh 
ϵ(Xkh ,kh)
√
Xkh + h2ᾱᾱkh
− hσ(kh)
+ hσ(kh)ϵk ,
X(k+1)h = 1 − h2ᾱᾱkh
2
1−ᾱ
kh

X0 ∼ N (0, I).

(48)

To go from (47) to (48), we introduced a continuous time variable and a stepsize h = 1/K, and we regard
the
√
increment hᾱk as approximately equal to h times the derivative of ᾱ. We also identified σk with hσ(kh),
where σ(kh) plays the role of a diffusion coefficient. Note that equation (48) can be reverse-engineered as the
Euler-Maruyama discretization of the SDE
2
˙
˙
ϵ(X ,t) 
√ t
dXt = − 2ᾱᾱtt + 2ᾱᾱtt − σ(t)
dt + σ(t)dBt ,
X0 ∼ N (0, I).
(49)
2
1−ᾱ
t

B.2

Forward and backward stochastic differential equations

Let (κt )t∈[0,1] and (ηt )t∈[0,1] such that
R1
∀t ∈ [0, 1], ηt ≥ 0,
κ
ds = +∞,
0 1−s


R1
Rt
2 0 η1−t′ exp − 2 t′ κ1−s ds dt′ = 1.

˙

(50)

˙

α̇t
As shown in Table 1, DDIM corresponds to κt = 2ᾱᾱtt , ηt = 2ᾱᾱtt , and Flow Matching corresponds to κt = α
,
t

α̇t
ηt = βt αt βt − β̇t .

Lemma 1 (DDIM and Flow Matching fulfill the conditions (50)). The choices of (κt )t∈[0,1] and (ηt )t∈[0,1] for
DDIM and Flow Matching fulfill the conditions (50). For DDIM, we have that
Rt
R1
κ
ds = − 12 log ᾱ1−t =⇒ 0 κ1−s ds = +∞,
0 1−s
(51)


Rt
Rt
R1
Rt
2 0 ηt′ exp − 2 t′ κs ds dt′ = 1 − ᾱ1−t =⇒ 2 0 ηt′ exp − 2 t′ κs ds dt′ = 1.
For Flow Matching,
Rt

R1
κ1−s ds = − log α1−t =⇒ 0 κ1−s ds = +∞,


Rt
Rt
Rt
R1
2
2 0 ηt′ exp − 2 t′ κs ds dt′ = β1−t
=⇒ 2 0 ηt′ exp − 2 t′ κs ds dt′ = 1.
0

31

(52)
(53)

Forward and backward SDEs Consider the forward and backward SDEs
⃗ t = −κ1−t X
⃗ 0 ∼ pdata ,
⃗ t dt + √2η1−t dBt ,
dX
X

√
dXt = κt Xt + 2ηt s(Xt , t) dt + 2ηt dBt ,
X0 ∼ N (0, I),

(54)
(55)

⃗ t , and we define the score function as s(x, t) := ∇ log p⃗1−t (x). Similarly,
where we let p⃗t be the density of X
we let pt be the density of Xt . p⃗t and pt solve the Fokker-Planck equations:

∂t p⃗t = ∇ · κ1−t x⃗
pt + η1−t ∆⃗
pt ,
p⃗0 = pdata ,
(56)
 
∂t pt = ∇ · − κt x − 2ηt ∇ log p⃗1−t (Xt ) pt + ηt ∆pt ,
p0 = N (0, I).
(57)

Lemma 2 (Solution of the forward SDE). Let (κt )t≥0 , (ηt )t≥0 with ηt ≥ 0, and (ξt )t≥0 be arbitrary. The
⃗ t of the SDE
solution X

⃗ 0 ∼ pdata
⃗ t = − κ1−t X
⃗ t + ξt dt + √2η1−t dBt ,
X
(58)
dX
is
 R


R
R
R
R
⃗t = X
⃗ 0 exp − t κ1−s ds + t exp − t′ κ1−s ds ξ1−t′ dt′ + t √2η1−t′ exp − t′ κ1−s ds dBt′ ,
X
0
0
t
t
0

(59)

which has the same distribution as the random variable
q R

 R

R
R
R
⃗ 0 exp − t κ1−s ds + t exp − t′ κ1−s ds ξ1−t′ dt′ + 2 t η1−t′ exp − 2 t′ κ1−s ds dt′ ϵ,
X̂t = X
0
t
0
0
t

(60)

ϵ ∼ N (0, I).
Applying Lemma 2 with ξt ≡ 0, we obtain that p⃗1 is also the distribution of

 q R
R
R
⃗ 0 exp − t κ1−s ds + 2 t η1−t′ exp − 2 t′ κ1−s ds dt′ ϵ = ϵ,
X̂1 = X
0
t
0

(61)

where ϵ ∼ N (0, I). The third equality in (61) holds by (50). Hence we obtain that p⃗1 = N (0, I). Note also
that



∂t p⃗1−t = −∇ · κt x⃗
p1−t − ηt ∆⃗
p1−t = −∇ · − κt x − 2ηt ∇ log p⃗1−t (x) p⃗1−t + ηt ∆⃗
p1−t
(62)
Thus, p⃗1−t is a solution of the backward Fokker-Planck equation (57), which proves the following:
⃗ t,
Proposition 3 (Equality of marginal distributions). For any time t ∈ [0, 1], the densities of the solutions X
Xt of the forward and backward SDEs are equal up to a time flip: pt = p⃗1−t .
Forward and backward SDEs with arbitrary noise schedule Next, we look at the following pair of forwardbackward SDEs:


2
⃗ t = − κ1−t X
⃗ t + σ(1−t) − η1−t s(X
⃗ t , 1 − t) dt + σ(1 − t) dBt ,
⃗ 0 ∼ pdata ,
dX
X
(63)
2


2
σ(t)
X0 ∼ N (0, I),
(64)
dXt = κt Xt +
2 + ηt s(Xt , t) dt + σ(t) dBt ,
Here, the score function s is the same
√ vector field as in (64). Remark that equations (54)-(55) are a particular
case of (63)-(64) for which σ(t) = 2ηt . The Fokker-Planck equations for (63)-(64) are:
∂t p⃗t = ∇ ·
∂t pt = ∇ ·


 
2
κ1−t x + − σ(1−t)
+ η1−t s(Xt , t) p⃗t + η1−t ∆⃗
pt ,
p⃗0 = pdata ,
2

  σ(t)2
σ(t)2
− κt x −
p0 = N (0, I).
2 + ηt s(Xt , t) pt + 2 ∆pt ,

(65)
(66)

It is straight-forward to see that for any σ, the solutions p⃗t and pt of (65)-(66) are also solutions of (56)-(57).
⃗ t and Xt are equally distributed for all noise schedules σ, and they are equal to each
Hence, the marginals X
other up to a time flip.
32

The result in Proposition 3 can be made even stronger:
⃗ X be the solutions of the SDEs (63)-(64)
Proposition 4 (Equality of distributions over trajectories). Let X,
⃗ t )0≤i≤I is equal
with arbitrary noise schedule. For any sequence of times (ti )0≤i≤I , the joint distribution of (X
i
⃗
to the joint distribution of (X1−ti )0≤i≤I , or equivalently, that the probability measures P, P of the forward and
⃗ X are equal, up to a flip in the time direction.
backward processes X,
Equality of distributions over trajectories

This result states that sampling trajectories from the backward process is equivalent to sampling them from
the forward process and then flipping their order.
B.2.1

Proof of Lemma 1
˙

˙

As shown in Table 1, DDIM corresponds to κt = 2ᾱᾱtt , ηt = 2ᾱᾱtt . Thus, ηt ≥ 0 because ᾱt is increasing, and
Rt ˙
Rt
κ1−s ds = 0 2ᾱᾱ1−s
ds = − 12 0 ∂s log ᾱ1−s ds = − 12 (log ᾱ1−t − log ᾱ1 ) = − 12 log ᾱ1−t ,
1−s
R1
=⇒ 0 κ1−s ds = − 12 log ᾱ0 = +∞

Rt
R t ˙ 1−s  ′
Rt
R t ᾱ˙ ′
2 0 ηt′ exp − 2 t′ κs ds dt′ = 0 ᾱ1−t′ exp − t′ ᾱ
ᾱ1−s ds dt
1−t


R t ᾱ˙ 1−t′ ᾱ1−t ′
Rt
1
= 0 ᾱ ′ ᾱ ′ dt = ᾱ1−t 0 ∂t′ ᾱ 1 ′ dt′ = ᾱ1−t ᾱ1−t
− ᾱ11 = 1 − ᾱ1−t ,
1−t
1−t
1−t

R1
Rt
=⇒ 2 0 ηt′ exp − 2 t′ κs ds dt′ = 1 − ᾱ0 = 1.
Rt
0

(67)

(68)


α̇t
t
where we used that ᾱ1 = 1 and ᾱ0 = 0. And Flow Matching corresponds to κt = α̇
αt , ηt = βt αt βt − β̇t . We
have that ηt ≥ 0 because αt is increasing and βt is decreasing, and
Rt
R t α̇1−s
Rt
κ
ds = 0 α
ds = − 0 ∂s log α1−s ds = −(log α1−t − log α1 ) = − log α1−t ,
0 1−s
1−s
(69)
R1
=⇒ 0 κ1−s ds = − log α0 = +∞,
and


R t 1−s  ′
Rt
Rt
Rt
α̇
′
2 0 η1−t′ exp − 2 t′ κ1−s ds dt′ = 2 0 β1−t′ α1−t′ β1−t′ − β̇1−t′ exp − 2 t′ α̇
α1−s ds dt
1−t
 α1−t 2 ′
Rt
α̇1−t′
= 2 0 β1−t′ α ′ β1−t′ − β̇1−t′ α ′ dt ,
1−t

(70)

1−t

To develop the right-hand side, note that by integration by parts,
2
Rt
β 2 ′  α1−t 2
dt′
β̇1−t′ β1−t′ αα1−t′ dt′ = − 0 ∂t′ 1−t
2
α1−t′
1−t
2
 β 2 ′ α1−t 2 1 R t β1−t
2
 β2 ′
′
= − 1−t
+ 0 2 ∂t′ αα1−t′ dt′ = − 1−t
2
α
2
′
0

Rt
0

1−t

1−t

2

R t 2 α1−t α̇1−t′ ′
α1−t 2 t
+ 0 β1−t
dt .
′
α1−t′
α31−t′
0

And if we plug this into the right-hand side of (70), we obtain

 2
2
Rt
Rt
α1−t 2 t
2
2
= β1−t
− β12 αα1−t
,
2 0 η1−t′ exp − 2 t′ κ1−s ds dt′ = β1−t
= β1−t
′
α1−t′
0
1
 ′
R1
Rt
=⇒ 2 0 η1−t′ exp − 2 t′ κ1−s ds dt = β12 = 1.

(71)

(72)
(73)

where we used that β1 = 0, α1 = 1.
B.2.2

Proof of Lemma 2

We can solve this equation by variation of parameters. To simplify the notation, we replace κ1−s , η1−s and

R
⃗ t , t) = X
⃗ t exp t κ1−s ds , we get that
ξ1−s by κs , ηs and ξs . Defining f (X
0


R
R
⃗ t , t) = κ1−t X
⃗ t exp t κ1−s ds dt + exp t κ1−s ds dX
⃗t
df (X
0
0



Rt
Rt
⃗ t exp
⃗ t + ξ1−t ) dt + √2η1−t dBt
(74)
= κ1−t X
κ
ds dt + exp 0 κ1−s ds (−κ1−t X
0 1−s


Rt
Rt
√
= exp 0 κ1−s ds ξ1−t dt + 2ηt exp 0 κ1−s ds dBt .
33

Integrating from 0 to t, we get that



R
R
R ′
R
R ′
⃗ t exp t κ1−s ds = X
⃗ 0 + t exp t κ1−s ds ξ1−t′ dt′ + t √2η1−t′ exp t κ1−s ds dBt′ ,
X
(75)
0
0
0
0
0
 Rt


Rt
Rt√
Rt
Rt
′
⃗
⃗
⇐⇒ Xt = X0 exp − 0 κ1−s ds + 0 exp − t′ κ1−s ds ξ1−t′ dt + 0 2η1−t′ exp − t′ κ1−s ds dBt′ .
(76)
Since
 Rt√
2 


Rt
Rt
Rt
= 2 0 η1−t′ exp − 2 t′ κ1−s ds dt′ ,
(77)
2η1−t′ exp − t′ κ1−s ds dBt′
E
0
q


Rt√
Rt
Rt
Rt
we obtain that 0 2η1−t′ exp − t′ κ1−s ds dBt′ has the same distribution as 2 0 η1−t′ exp − 2 t′ κ1−s ds dt′ ϵ,
where ϵ ∼ N (0, 1).
B.2.3

Proof of Proposition 4

This is a result that has been used by previous works, e.g. (De Bortoli et al., 2021, Sec. 2.1), but their
derivation lacks rigor as it uses some unexplained approximations. While natural, the result is not common
knowledge in the area. We provide a derivation which is still in discrete time, and hence not completely
formal, but that corrects the gaps in the proof of De Bortoli et al. (2021).
We introduce the short-hand

σ(1−t)2
− η1−t s(x, 1 − t),
2

σ(t)2
2 + ηt s(Xt , t),

⃗b(x, t) = −κ1−t x +

(78)

b(x, t) = κt Xt +

(79)
(80)

⃗σ (t) = σ(1 − t).
Remark that b(x, t) = −⃗b(x, 1 − t) + σ(t)2 s(Xt , t).
⃗ using K + 1 equispaced timesteps:
Suppose that we discretize the forward process X
√
with ϵk ∼ N (0, 1).
xk+1 = xk + h⃗b(xk , kh) + h⃗σ (kh)ϵk ,

(81)

It is important to remark that xk+1 − xk = O(h1/2 ). Throughout the proof we will keep track of all terms up
to linear order in h, while neglecting terms of order O(h3/2 ) and higher. The distribution of the discretized
forward process is:

∥x
−xk −h⃗
b(xk ,kh)∥2
exp − k+1 2h⃗
QK−1
σ (kh)2
p⃗(x0:K ) = p⃗0 (x0 ) k=0 p⃗k+1|k (xk+1 |xk ),
where
p⃗k+1|k (xk+1 |xk ) =
(82)
(2πh⃗
σ (kh)2 )d/2
Using telescoping products, we have that
QK−1
p
⃗k (xk )
p⃗(x0:K ) = p⃗K (xK ) k=0 p⃗k+1|k (xk+1 |xk ) p⃗k+1
(xk+1 )

QK−1
= p⃗K (xK ) k=0 p⃗k+1|k (xk+1 |xk ) exp log(⃗
pk (xk )) − log(⃗
pk+1 (xk+1 ))

(83)

We can use a discrete time version of Ito’s lemma:

2
log p⃗(xk+1 , (k + 1)h) ≈ log p⃗(xk , kh) + h ∂t log p⃗(xk , kh) + ⃗σ(kh)
∆ log p⃗(xk , kh)
2
+ ⟨∇ log p⃗(xk , kh), xk+1 − xk ⟩ + O(h3/2 ).

(84)
(85)

Using equation (81) and a Taylor approximation, observe that
⟨∇ log p(xk , kh), xk+1 − xk ⟩
= ⟨∇ log p(xk+1 , (k + 1)h) − ∇2 log p(xk+1 , (k + 1)h)(xk+1 − xk ), xk+1 − xk ⟩ + O(h3/2 )
= ⟨∇ log p(xk+1 , (k + 1)h), xk+1 − xk ⟩
√
√

− ⟨h⃗b(xk , kh) + h⃗σ (kh)ϵk , ∇2 log p(xk+1 , (k + 1)h) h⃗b(xk , kh) + h⃗σ (kh)ϵk ⟩ + O(h3/2 )
= ⟨∇ log p(xk+1 , (k + 1)h), xk+1 − xk ⟩ − h⃗σ (kh)2 ∆ log p(xk+1 , (k + 1)h) + O(h3/2 ).
34

(86)

And since p⃗ satisfies the Fokker-Planck equation

2
∂t p⃗t = ∇ · (−⃗b(x, t) + ⃗σ(t)
⃗t (x))⃗
pt ,
2 ∇ log p

(87)

we have that
2

∂t log p⃗t = ∂p⃗t p⃗t t =

⃗
σ (t)
∇· (−⃗b(x,t)+ 2 ∇ log p
⃗t (x))⃗
pt
p
⃗t



2

(88)

2

= −∇ · ⃗b(x, t) + ⃗σ(t)
⃗t (x) + ⟨−⃗b(x, t) + ⃗σ(t)
⃗t (x), ∇ log p⃗t (x)⟩.
2 ∆ log p
2 ∇ log p
Hence,
∂t log p(xk , kh) = ∂t log p(xk+1 , (k + 1)h) + O(h1/2 )
2

∆ log p⃗(xk+1 , (k + 1)h)
= −∇ · ⃗b(xk+1 , (k + 1)h) + ⃗σ((k+1)h)
2

(89)

2

∇ log p⃗(xk+1 , (k + 1)h), ∇ log p⃗(xk+1 , (k + 1)h)⟩ + O(h1/2 ).
+ ⟨−⃗b(xk+1 , (k + 1)h) + ⃗σ((k+1)h)
2
If we plug (86) and (89) into (84), we obtain
log p(xk+1 , (k + 1)h) − log p(xk , kh)

2
= h −∇ · ⃗b(xk+1 , (k + 1)h)+⟨−⃗b(xk+1 , (k + 1)h)+ ⃗σ((k+1)h)
∇ log p⃗(xk+1 , (k + 1)h), ∇ log p⃗(xk+1 , (k + 1)h)⟩
2
+ ⟨∇ log p(xk+1 , (k + 1)h), xk+1 − xk ⟩ + O(h3/2 )
⃗

2

k+1 −xk −hb(xk+1 ,(k+1)h)⟩
= ⟨2h⃗σ(kh) ∇ log p(xk+1 ,(k+1)h),x
2h⃗
σ (kh)2


2
+ h − ∇ · ⃗b(xk+1 , (k + 1)h) + ⃗σ((k+1)h)
∥∇ log p⃗(xk+1 , (k + 1)h)∥2 + O(h3/2 ).
2

(90)

Applying a discrete time version of Ito’s lemma again, we have that

2
⃗b(xk , kh) = ⃗b(xk+1 , (k + 1)h) − h ∂t⃗b(xk+1 , (k + 1)h) + ⃗σ((k+1)h) ∆⃗b(xk+1 , (k + 1)h)
2
+ ∇⃗b(xk+1 , (k + 1)h)⊤ (xk − xk+1 ) + O(h3/2 )

(91)

= ⃗b(xk+1 , (k + 1)h) + ∇⃗b(xk+1 , (k + 1)h) (xk − xk+1 ) + O(h).
⊤

where ∆⃗b denotes the component-wise Laplacian of ⃗b. Thus,
log p⃗k+1|k (xk+1 |xk )

2
⃗
k −hb(xk ,kh)∥
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −x
2h⃗
σ (kh)2

⃗
⃗b(xk+1 ,(k+1)h)⊤ (xk −xk+1 ))∥2
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −xk −h(b(xk+1 ,(k+1)h)+∇
+ O(h3/2 )
2h⃗
σ (kh)2

⃗b(xk+1 ,(k+1)h)∥2
⃗
,(k+1)h)⊤ (xk −xk+1 )⟩
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −xk −h
+ ⟨xk+1 −xk ,∇b(xk+1
+ O(h3/2 )
2h⃗
σ (kh)2
⃗
σ (kh)2

2
⃗b(xk+1 ,(k+1)h)∥2
⃗ k+1 ,(k+1)h)⊤ ϵk ⟩
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −xk −h
− h⃗σ(kh) ⟨ϵk ,∇⃗σb(x
+ O(h3/2 )
h⃗
σ (kh)2
(kh)2

⃗b(xk+1 ,(k+1)h)∥2
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −xk −h
− h∆⃗b(xk+1 , (k + 1)h) + O(h3/2 )
h⃗
σ (kh)2

(92)

Combining (90) and (92), we obtain that

log p⃗k+1|k (xk+1 |xk ) − log p(xk+1 , (k + 1)h) − log p(xk , kh)

⃗
σ (kh)2 ∇ log p(xk+1 ,(k+1)h)∥2
= − d2 log 2πh⃗σ (kh)2 − ∥xk+1 −xk −hb(xk+1 ,(k+1)h)+h⃗
+ O(h3/2 )
h⃗
σ (kh)2

⃗
σ ((k+1)h)2 ∇ log p(xk+1 ,(k+1)h)∥2
= − d2 log 2πh⃗σ ((k + 1)h)2 − ∥xk+1 −xk −hb(xk+1 ,(k+1)h)+h⃗
+ O(h3/2 ).
h⃗
σ ((k+1)h)2

(93)

By Bayes rule, and taking the exponential of this equation, we obtain
p
⃗k (xk )
p⃗k+1|k (xk+1 |xk ) := p⃗k+1|k (xk+1 |xk ) p⃗k+1
(xk+1 )

=

exp −

∥xk −xk+1 +h⃗
b(xk+1 ,(k+1)h)−h⃗
σ ((k+1)h)2 ∇ log p(xk+1 ,(k+1)h)∥2
2h⃗
σ ((k+1)h)2

(2πh⃗
σ ((k+1)h)2 )d/2

35

(94)


+ O(h3/2 ).

Up to the O(h3/2 ) term, the right-hand side is the conditional Gaussian corresponding to the update
 √
xk = xk+1 + h − ⃗b(xk+1 , (k + 1)h) + ⃗σ ((k + 1)h)2 ∇ log p(xk+1 , (k + 1)h) + h⃗σ ((k + 1)h)ϵk+1 , ϵk+1 ∼ N (0, I).
(95)
If we define yk = xK−k , and we use that b(x, t) = −⃗b(x, 1 − t) + ⃗σ (t)2 ∇ log p(x, 1 − t), we can rewrite (95) as
yK−k = yK−k−1 + h − ⃗b(yK−k−1 , (K − k − 1)h) + ⃗σ ((K − k − 1)h)2 ∇ log p(yK−k−1 , (K − k − 1)h)
√
√
+ h⃗σ ((K − k − 1)h)ϵk = yK−k−1 + hb(yK−k−1 , kh) + hσ(kh)ϵK−k−1 ,
√
=⇒ yk+1 = yk + hb(yk , kh) + hσ(kh)ϵk .


(96)

⃗ If we plug (94) into (83), we
And this is the Euler-Maruyama discretization of the backward process X.
obtain that
QK−1
p⃗(x0:K ) ≈ p⃗K (xK ) k=0 p⃗k+1|k (xk+1 |xk ).
(97)
which concludes the proof, as p⃗K (xK ) is the initial distribution of the backward process, and p⃗k+1|k (xk+1 |xk )
are its transition kernels.

B.3

The relationship between the noise predictor ϵ and the score function

⃗ t has the same
Applying Lemma 2 with the choices of (κt )t≥0 and (ηt )t≥0 for DDIM, we obtain that X
distribution as
√
⃗ 0 + √1 − ᾱ1−t ϵ,
X̂t = ᾱ1−t X
ϵ ∼ N (0, 1).
(98)
⃗ t and X̂t have the same distribution, predicting the noise of X
⃗ t is equivalent to predicting the noise of
Since X
X̂t . The noise predictor ϵ can be written as:
 √

 √ᾱt X⃗ 0 √

⃗ 0 + √1 − ᾱt ϵ = x = E x−
⃗ 0 + √1 − ᾱt ϵ = x
√
| ᾱt X
ϵ(x, t) := E[ϵ|X̂1−t = x] = E ϵ| ᾱt X
1−ᾱ
t

(99)

And the score function s(x, t) := ∇ log p⃗1−t (x) admits the expression
p1−t (x)
s(x, t) := ∇ log p⃗1−t (x) = ∇⃗
p
⃗1−t (x) =

⃗ 0 )]
⃗ 0 )⃗
⃗ 0 )]
∇E[⃗
p1−t|0 (x|X
E[∇ log p
⃗1−t|0 (x|X
p1−t|0 (x|X
=
,
p
⃗1−t (x)
p
⃗1−t (x)

(100)

where
√

√

2

ᾱt )))
⃗ 0 ) = exp(−∥x− ᾱt Y1 ∥ /(2(1−
p⃗1−t|0 (x|X
=⇒ ∇ log p⃗t|1 (x|Y1 ) = − x−1−ᾱᾱttY1 .
(2π(1−ᾱt ))d/2

(101)

Plugging this into the right-hand side of (100) and using Bayes’ rule, we get
√ ⃗ √


⃗ 0 + √1 − ᾱt ϵ = x .
s(x, t) = E − x−1−ᾱᾱttX0 | ᾱt X

(102)

Comparing the right-hand sides of (99) and (102), we obtain that s(x, t) = − √ϵ(x,t)
.
1−ᾱ
t

B.4

The relationship between the vector field v and the score function

By construction (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Albergo et al., 2023), we have that
v(x, t) = E[α̇t Y1 + β̇t Y0 |x = αt Y1 + βt Y0 ]
t Y0 )
= E[ α̇t (x−β
+ β̇t Y0 |x = αt Y1 + βt Y0 ]
αt

α̇t
t
= α̇
αt x + (β̇t − αt βt )E[Y0 |x = αt Y1 + βt Y0 ],

36

(103)

where we used that Y1 = (x − βt Y0 )/αt . Also, we can write the score as follows
t (x)
s(x, t) := ∇ log pt (x) = ∇p
pt (x) =

∇E[pt|1 (x|Y1 )]
E[∇pt|1 (x|Y1 )]
E[p (x|Y1 )∇ log pt|1 (x|Y1 )]
=
= t|1
,
pt (x)
pt (x)
pt (x)

(104)

where
pt|1 (x|Y1 ) =

exp(−∥x−αt Y1 ∥2 /(2βt2 ))
(2πβt2 )d/2

t Y1
=⇒ ∇ log p⃗t|1 (x|Y1 ) = − x−α
β2
t

(105)

Plugging this back into the right-hand side of (104), we obtain
s(x, t) = −
=−

E[pt|1 (x|Y1 )

x−αt Y1
]
2
βt

pt (x)

R

R

=−

p
⃗t|1 (x|Y1 )p1 (Y1 )

x−αt Y1
dY1
2
βt

p
⃗t (x)

t Y1
t Y1
dY1 = −E[ x−α
|x = αt Y1 + βt Y0 ] = − E[Y0 |x=αβttY1 +βt Y0 ]
p1|t (Y1 |x) x−α
βt2
βt2

(106)

The last equality holds because (x − αt Y1 )/βt = Y0 . Putting together (103) and (106), we obtain that

α̇t
1
t
t
v(x, t) − α̇
v(x, t) = α̇
(107)
α̇t
αt x + βt ( αt βt − β̇t )s(x, t) ⇐⇒ s(x, t) =
αt x
βt ( α βt −β̇t )
t

Thus, the ODE (3) can be rewritten like this:
dXt
α̇t
α̇t
dt = αt Xt + βt ( αt βt − β̇t )s(Xt , t),

(108)

X0 ∼ p0 .

To allow for an arbitrary diffusion coefficient, we need to add a correction term to the drift:
dXt =

α̇t
αt Xt +



σ(t)2
α̇t
2 + βt ( αt βt − β̇t ) s(Xt , t) dt + σ(t)dBt ,

X0 ∼ p0 .

(109)

This can be easily shown by writing down the Fokker-Planck equations for (108) and (109), and observing
that they are the same up to a cancellation of terms. Finally, if we plug the right-hand side of (107) into
(109), we obtain the SDE for Flow Matching with arbitrary noise schedule (equation (4)).

C

Stochastic optimal control as maximum entropy RL in continuous
space and time

In this section, we bridge KL-regularized (or MaxEnt) reinforcement learning and stochastic optimal control.
We show that when the action space is Euclidean and the transition probabilities are conditional Gaussians,
taking the limit in which the stepsize goes to zero on the KL-regularized RL problem gives rise to the SOC
problem. A consequence of this connection is that all algorithms for KL-regularized RL admit an analog
for diffusion fine-tuning. This is not novel, but it may be useful for researchers that are familiar with RL
fine-tuning formulations.
Appendix C.4 is providing a more direct, rigorous, continuous-time connection between SOC and MaxEnt
RL, as it shows that the expected control cost is equal to the KL divergence between the distributions over
trajectories, conditioned on the starting points (see equation (18)).

C.1

Maximum entropy RL

Several diffusion fine-tuning methods (Black et al., 2024; Uehara et al., 2024b) are based on KL-regularized
RL, also known as maximum entropy RL, which we review in the following. In the classical reinforcement
learning (RL) setting, we have an agent that, starting from state s0 ∼ p0 , iteratively observes a state sk ,
takes an action ak according to a policy π(ak ; sk , k) which leads to a new state sk+1 according to a fixed
transition probability p(sk+1 |ak , sk ), and obtains rewards rk (sk , ak ). This can be summarized into a trajectory
τ = ((sk , ak ))K
k=0 . The goal is to optimize the policy π in order to maximize the expected total reward, i.e.
P
K
maxπ Eτ ∼π,p [ k=0 rk (sk , ak )].
Maximum entropy RL (MaxEnt RL; Ziebart et al. (2008)) amounts to adding the entropy H(π) of the policy
π(·; sk , k) to the reward for each step k, in order to encourage exploration and improve robustness to changes
37

PK
PK−1
in the environment: maxπ Eτ ∼π,p [ k=0 rk (sk , ak ) + k=0 H(π(·; sk , k))] 8 . As a generalization, one can
regularize using the negative KL divergence between π(·; sk , k) and a base policy πbase (·; sk , k):
PK
PK−1
maxπ Eτ ∼π,p [ k=0 rk (sk , ak ) − k=0 KL(π(·; sk , k)||πbase (·; sk , k))],

(110)

which prevents the learned policy to deviate too much from the base policy. Each policy π induces a
distribution q(τ ) over trajectories τ , and the MaxEnt RL problem (110) can be expressed solely in terms of
such distributions (Lemma 3 in Appendix C.3):
maxq Eτ ∼q [

PK

k=0 rk (sk , ak )] − KL(q||q

base

(111)

),

where q base is the distribution induced by the base policy πbase , and the maximization is over all distributions
q such that their marginal for s0 is p0 . We can further recast this problem as (Lemma 4 in Appendix C.3):
minq KL(q||q ∗ ),

where q ∗ (τ ) := q base (τ ) exp

PK



k=0 rk (sk , ak ) − V(s0 , 0) ,

(112)

where


PK
′
′
′
V(sk , k) := log Eτ ∼πbase ,p [exp
k′ =k rk (sk , ak ) |sk ]
 PK

PK−1
′
′
′
′
′
′
′
= maxπ Eτ ∼π,p
k′ =k rk (sk , ak ) −
k′ =k KL(π(·; sk , k )||πbase (·; sk , k ))|sk

(113)

is the value function. Problem (112) directly implies that the distribution induced by the optimal policy π ∗ is
the tilted distribution q ∗ (which has initial marginal p0 ).

C.2

From maximum entropy RL to stochastic optimal control

The following well-known result, which we prove in Appendix C.3, shows that in a natural sense, the
continuous-time continuous-space version of MaxEnt RL is the SOC framework introduced in Section 4.1. In
particular, when states and actions are vectors in Rd , policies are specified by a vector field u (the control),
and transition probabilities are conditional Gaussians, the MaxEnt RL problem becomes an SOC problem
when the number of timesteps grows to infinity.
Proposition 5. Suppose that
(i) The state space and the action space are Rd ,
(ii) Policies π are specified as π(ak ; sk , k) = δ(ak − u(sk , kh)), where u : Rd × [0, T ] → Rd is a vector field,
and δ denotes the Dirac delta,
(iii) Transition probabilities are conditional Gaussian densities: p(sk+1 |ak , sk ) = N (sk + h(b(sk , kh) +
σ(kh)ak ), hσ(kh)σ(kh)⊤ ), where h = T /K is the stepsize, and b and σ are defined as in Section 4.1.
Then, in the limit in which the number of steps K grows to infinity, the problem (110) is equivalent to the
SOC problem (12)-(13), identifying
• the sequence of states (sk )kk=0 with the trajectory X u = (Xtu )t∈[0,1] ,
RT
PK−1
• the running reward k=0 rk (sk , ak ) with the negative running cost − 0 f (Xtu , t) dt,
• the terminal reward rK (sK , aK ) with the negative terminal cost −g(XTu ),
PK−1
• the KL regularization Eτ ∼π,p [ k=0 KL(π(·; sk , k)||πbase (·; sk , k))] with 12 times the expected L2 norm of


R
T
the control 12 E 0 ∥u(Xtu , t)∥2 dt ,
• and the value function V(sk , k) defined in (113) with the negative value function −V (x, t) defined in
Section 4.1.
A first consequence of this result is that every loss function designed for generic MaxEnt RL problems has a
corresponding loss function for SOC problems. The geometric structure of the latter allows for additional
8 The entropy terms are usually multiplied by a factor to tune their magnitude, but one can equivalently rescale the rewards,
which is why we do not add any factor.

38

losses that do not have an analog in the classical MaxEnt RL setting; in particular, we can differentiate the
state and terminal costs.
A second consequence of Proposition 5 is that the characterization (112) can be translated to the SOC setting.
The analogs of the distributions q ∗ , q base induced by the optimal policy π ∗ and the base policy π base are the
distributions p∗ , pbase induced by the optimal control u∗ and the null control. For an arbitrary trajectory
X = (Xt )t∈[0,T ] , the relation between P∗ and Pbase is given by
RT
dP∗
(X) = exp(− 0 f (Xt , t) dt − g(XT ) + V (X0 , 0))
dPbase

(114)

where V is the value function as defined in Section 4.1. Note that this matches the statement in (22).

C.3

Proof of Proposition 5: from MaxEnt RL to SOC

Since the transition p(sk+1 |ak , sk ) is fixed, for each π we can define
π̃(ak , sk+1 ; sk , k) = π(ak ; sk , k)p(sk+1 |ak , sk ) and π̃base (ak , sk+1 ; sk , k) = πbase (ak ; sk , k)p(sk+1 |ak , sk ),
(115)
and reexpress (110) as (see Lemma 3)
PK
PK−1
minπ̃ Eτ ∼π̃ [ k=0 rk (sk , ak ) − k=0 KL(π̃(·, ·; sk , k)||π̃base (·, ·; sk , k))].

(116)

Using the hypothesis of the proposition, we can write
π̃(ak , sk+1 ; sk , k) = δ(ak − u(sk , kη))N (sk + η(b(sk , kη) + σ(kη)ak ), ησ(kη)σ(kη)⊤ )
= δ(ak − u(sk , kη))π̃(sk+1 ; sk , k),

(117)

where π̃(sk+1 ; sk , k) = N (sk + η(b(sk , kη) + σ(kη)u(sk , kη)), ησ(kη)σ(kη)⊤ ) is the state transition kernel.
We set the base policy as πbase (ak ; sk , k) = δ(ak ), and we obtain analogously that π̃(ak , sk+1 ; sk , k) =
δ(ak )π̃base (sk+1 ; sk , k) with π̃base (sk+1 ; sk , k) = N (sk + ηb(sk , kη), ησ(kη)σ(kη)⊤ ). Now, if we take K large,
u
the trajectory (sk )K
k=0 generated by π̃ can be regarded as the Euler-Maruyama discretization of a solution X
of the controlled SDE (13), while the trajectory generated by π̃base is the discretization of the uncontrolled
process X 0 obtained by setting u = 0. As a consequence
PK−1
limK→∞ Eτ ∼π̃ [ k=0 KL(π̃(·, ·; sk , k)||π̃base (·, ·; sk , k))]
PK−1
u
u
= limK→∞ Eτ ∼π̃ [ k=0 KL(π̃(·; sk , k)||π̃base (·; sk , k))] = EX u ∼Pu [log dP
dP0 (X )],

(118)

where Pu and P0 are the measures of the processes X u and X 0 , respectively. The Girsanov theorem
RT
RT
u
u
(Theorem 2) implies that log dP
) = − 0 ⟨u(Xtu , t), dBt ⟩ − 12 0 ∥u(Xtu , t)∥2 dt, which implies that
dP0 (X
RT
u
1
u
u
2
u
u
EX u ∼Pu [log dP
dP0 (X )] = − 2 EX ∼P [ 0 ∥u(Xt , t)∥ dt]. Setting the rewards rk (ak , sk ) = ηf (sk , kη) for k ∈
{0, . . . , K − 1} and rK (aK , sK ) = ηg(sk ), where f and g are as in Section 4.1, yields the following limiting
object:
limK→∞ Eτ ∼π̃ [

RT
u
u
u
u
k=0 rk (sk , ak )] = EX ∼P [ 0 f (Xt , t) dt + g(XT )].

PK

(119)

Hence, the limit of the MaxEnt RL loss (116) is the SOC loss (12).
Lemma 3. Let π̃(ak , sk+1 ; sk , k) and π̃base (ak , sk+1 ; sk , k) be as defined in (115). KL(π̃(·, ·; sk , k)||π̃base (·, ·; sk , k))]
and KL(π(·; sk , k)||πbase (·; sk , k))] are equal. Moreover, if q, q base denote the distributions over trajectories
induced by π, πbase , we have that
PK−1
KL(q||q base ) = E[ k=0 KL(π(·; sk , k)||πbase (·; sk , k))].
(120)

39

Proof. We have that
P
π̃(ak ,sk+1 ;sk ,k)
KL(π̃(·, ·; sk , k)||π̃base (·, ·; sk , k))] = ak ,sk+1 π̃(ak , sk+1 ; sk , k) log π̃base
(ak ,sk+1 ;sk ,k)
P
π(ak ;sk ,k)p(sk+1 |ak ,sk )
= ak ,sk+1 π(ak ; sk , k)p(sk+1 |ak , sk ) log πbase (ak ;sk ,k)p(sk+1 |ak ,sk )
P
π(ak ;sk ,k)
= ak ,sk+1 π(ak ; sk , k)p(sk+1 |ak , sk ) log πbase
(ak ;sk ,k)

P
P
π(ak ;sk ,k)
= ak π(ak ; sk , k)
sk+1 p(sk+1 |ak , sk ) log πbase (ak ;sk ,k)
P
π(ak ;sk ,k)
= ak π(ak ; sk , k) log πbase
(ak ;sk ,k) = KL(π(·; sk , k)||πbase (·; sk , k))].

(121)

To prove (120), by construction we can write
q(τ ) = p0 (s0 )

QK−1
k=0

q base (τ ) = p0 (s0 )

π̃(ak , sk+1 ; sk , k),

QK−1
k=0

π̃base (ak , sk+1 ; sk , k),

(122)

which means that
PK−1
π̃(ak ,sk+1 ;sk ,k)
q(τ )
KL(q||q base ) = Eτ ∼q [log qbase
] = Eτ ∼q [ k=0 log π̃base
(ak ,sk+1 ;sk ,k) ]
(τ )
PK−1
π̃(ak ,sk+1 ;sk ,k)
= k=0 Eτ ∼q0:(k+1) [log π̃base
(ak ,sk+1 ;sk ,k) ]
PK−1
P
π̃(ak ,sk+1 ;sk ,k)
= k=0 Eτ ∼q0:k [ ak ,sk+1 π̃(ak , sk+1 ; sk , k) log π̃base
(ak ,sk+1 ;sk ,k) ]
PK−1
= k=0 Eτ ∼q0:k [KL(π̃(·, ·; sk , k)||π̃base (·, ·; sk , k))]
PK−1
= k=0 Eτ ∼q0:k [KL(π(·; sk , k)||πbase (·; sk , k))]
PK−1
= Eτ ∼q0:k [ k=0 KL(π(·; sk , k)||πbase (·; sk , k))]

(123)

Here, the notation q 0:k denotes the trajectory q up to the state sk .
Lemma 4. The distribution-based MaxEnt RL formulation in (111) is equivalent to the the following problem:

PK
q base (τ ) exp
k=0 rk (sk ,ak )
,
minq KL(q||q ∗ ),
where q ∗ (τ ) := 1 P
(124)
PK
′
′
base
′
p0 (s0 )

{τ ′ |s′0 =s0 } q

(τ ) exp

k=0 rk (sk ,ak )

where the minimization is over q with marginal p0 at step zero. The optimum of the problem is q ∗ , which
satisfies the marginal constraint. The following alternative characterization of q ∗ holds:

PK
q ∗ (τ ) = q base (τ ) exp
(125)
k=0 rk (sk , ak ) − V(s0 , 0) ,
 PK

PK−1
′
′
′
′
′
′
′
where V(x, k) = maxπ Eτ ∼π,p
k′ =k rk (sk , ak ) −
k′ =k KL(π(·; sk , k )||πbase (·; sk , k ))|sk = x . (126)
Proof. Let us expand KL(q||q ∗ ):

) 
KL(q||q ∗ ) = Eτ ∼q log qq(τ
∗ (τ )

PK
= Eτ ∼q log q(τ ) − log q base (τ ) − k=0 rk (sk , ak )

P
PK
1
base ′
′
′
(τ ) exp
+ log p0 (s
{τ ′ |s′0 =s0 } q
k=0 rk (sk , ak )
0)
 PK

= KL(q||q base ) − Eτ ∼q
k=0 rk (sk , ak )


P
PK
1
base ′
′
′
+ Es0 ∼p0 log p0 (s
(τ ) exp
,
{τ ′ |s′ =s0 } q
k=0 rk (sk , ak )
0)

(127)

0

where the third equality holds because the marginal of q at step zero is p0 by hypothesis. Since the third
term in the right-hand side is independent of q, this proves the equivalence between (111) and (124).
Next, we prove that the marginal of q ∗ at step zero is p0 :
P

{τ |s0 =x} q

∗

(τ ) :=

q base (τ ) exp

P

{τ |s0 =x}

1
p0 (x)

P

{τ ′ |s′0 =x} q

40



PK

k=0 rk (sk ,ak )

base (τ ′ ) exp

PK

 = p0 (x).

′
′
k=0 rk (sk ,ak )

(128)

Now, for an arbitrary s0 , let qs0 , qs∗0 be the distributions q, q ∗ conditioned on the initial state being s0 . We
can write an analog to equation (127) for qs0 , qs∗0 :

q (τ ) 
KL(qs0 ||qs∗0 ) = Eτ ∼qs0 log qs∗0 (τ )
s0

PK
(τ ) − k=0 rk (sk , ak )
= Eτ ∼qs0 log qs0 (τ ) − log qsbase
0

PK
P
1
′
′
base ′
+ log p0 (s
k=0 rk (sk , ak )
{τ ′ |s′0 =s0 } qs0 (τ ) exp
0)

 PK
= KL(qs0 ||qsbase
) − Eτ ∼qs0
k=0 rk (sk , ak )
0

PK
P
1
′
′
base ′
+ log p0 (s
(τ ) exp
k=0 rk (sk , ak ) ,
{τ ′ |s′ =s0 } q
0)

(129)


 PK
base
0 = minqs0 KL(qs0 ||qs∗0 ) = − maxqs0 {Eτ ∼qs0
k=0 rk (sk , ak ) − KL(qs0 ||qs0 )}

P
P
K
1
base ′
′
′
+ log p0 (s
(τ ) exp
{τ ′ |s′ =s0 } q
k=0 rk (sk , ak ) .
0)

(130)

0

Hence,

0

And applying (120) from (120), we obtain that
log

1
p0 (s0 )

P

{τ ′ |s′0 =s0 } q

= maxπ Eτ ∼π,p

base

PK

(τ ′ ) exp

 PK

k=0 rk (sk , ak ) −

′
′
k=0 rk (sk , ak )



PK−1
k=0


KL(π(·; sk , k)||πbase (·; sk , k))|s0 = V(s0 , 0),

(131)

which concludes the proof.

C.4

Proof of equation (18): the control cost is a KL regularizer

Theorem 2 (Girsanov theorem for SDEs). If the two SDEs
dXt = b1 (Xt , t) dt + σ(Xt , t) dBt ,

(132)

X0 = xinit

dYt = (b1 (Yt , t) + b2 (Yt , t)) dt + σ(Yt , t) dBt ,

Y0 = xinit

(133)

admit unique strong solutions on [0, T ], then for any bounded continuous functional Φ on C([0, T ]), we have
that


RT
RT
E[Φ(X)] = E Φ(Y ) exp − 0 σ(Yt , t)−1 b2 (Yt , t) dBt − 12 0 ∥σ(Yt , t)−1 b2 (Yt , t)∥2 dt
(134)


RT
RT
= E Φ(Y ) exp − 0 σ(Yt , t)−1 b2 (Yt , t) dB̃t + 21 0 ∥σ(Yt , t)−1 b2 (Yt , t)∥2 dt ,
Rt
where B̃t = Bt + 0 σ(Ys , s)−1 b2 (Ys , s) ds. More generally, b1 and b2 can be random processes that are adapted
to filtration of B.
Consider the SDEs
dXt = b(Xt , t) dt + σ(t)dBt ,
dXtu =



b(Xtu , t) + σ(t)u(Xtu , t)

dt + σ(t)dBt ,

X0 = x0 ,

(135)

X0u = x0 .

(136)

If we let P|x0 , Pu |x0 be the probability measures of the solutions of (135) and (136), Theorem 2 implies that
R1
R1
dP|
log dPu |xx0 (X u ) = − 0 u(Xtu , t) dBt − 12 0 ∥u(Xtu , t)∥2 dt.
0

(137)

Hence,
DKL Pu |x0






dPu |
dP|
P|x0 = E log dP|xx0 (X u )|X0u = x0 = −E log dPu |xx0 (X u )|X0u = x0
0
0
R1

 R1

R1
= E 0 u(Xtu , t) dBt + 12 0 ∥u(Xtu , t)∥2 dt|X0u = x0 = E 21 0 ∥u(Xtu , t)∥2 dt|X0u = x0 ,
(138)

where we used that stochastic integrals are martingales.
41

D

Proofs of Section 4.3: memoryless noise schedule and fine-tuning
recipe

D.1

Proof of Proposition 1: the memoryless noise schedule

We consider the forward-backward SDEs (63)-(64) with arbitrary noise schedule. By Proposition 4, the
⃗ X of these two processes are equally distributed up to a time flip, which also means that
trajectories X,
their marginals satisfy p⃗t = p1−t , for all t ∈ [0, 1]. First, we develop an explicit expression for the score
function s(x, t) = ∇ log pt (x). By the properties of flow matching, we know that pt is the distribution of
the interpolation variable X̄t = βt X̄0 + αt X̄1 , where X̄0 ∼ N (0, I), X̄1 ∼ pdata are independent. Thus,
X̄t −αt X̄1
∼ N (0, I), which means that we can express the density pt as
βt
pt (x) =

∥x−αt y∥2
2
2βt
(2πβt2 )d/2

exp −

R
Rd


(139)

pdata (y) dy.

Thus,
R

d y exp

R
s(x, t) = ∇ log pt (x) = − βx2 + βα2t R
t
t

Rd

exp

∥x−αt y∥2
pdata (y) dy
2
2βt
2
∥x−αt y∥
pdata (y) dy
−
2
2βt



−



:= − x−αβt2ξt (x) ,
t

(140)

where we defined
R

y exp
Rd

ξt (x) = R

Rd

exp

∥x−αt y∥2
pdata (y) dy
2
2βt
∥x−αt y∥2
−
pdata (y) dy
2
2βt

−





(141)

.

Hence, we can rewrite the forward SDE (63) as
⃗ t = − κ1−t X
⃗t −
dX

⃗
⃗ t) 
ξ1−t (X
σ(1−t)2
dt + σ(1 − t) dBt ,
− η1−t Xt −α1−t
2
2
β1−t
2

⃗ 0 ∼ pdata
X

(142)

2

−2η1−t
−2η1−t )
⃗ t ) (where we ignore
Hence, if we substitute κ1−t ← κ1−t + σ(1−t)
, ξ1−t ← α1−t (σ(1−t)
ξ1−t (X
2
2
2β1−t
2β1−t
√
⃗ t ), 2η1−t ← σ(1 − t), we can apply Lemma 2, which yields
the dependency on X

 
R
2
⃗t = X
⃗ 0 exp − t κ1−s + σ(1−s) 2−2η1−s ds
X
0
2β1−s
 α ′ (σ(1−t′ )2 −2η1−t′ )
Rt
Rt
2
−2η1−s 
⃗ t′ ) dt′
ds 1−t 2β 2
ξ1−t′ (X
+ 0 exp − t′ κ1−s + σ(1−s)
2
2β1−s
1−t′

Rt
Rt
2
−2η1−s 
ds dBt′ .
+ 0 σ(1 − t′ ) exp − t′ κ1−s + σ(1−s)
2β 2

(143)

1−s

We simplify the recurring expression:
2

α̇1−s

σ(1−s) −2β1−s α
β1−s −β̇1−s
2
−2η1−s
1−s
1−s
κ1−s + σ(1−s)
= α̇
2
2
α1−s +
2β1−s
2β1−s



2

1−s
= σ(1−s)
+ ββ̇1−s
2β 2
1−s

(144)

Thus,
Rt

t′

Rt
2
−2η1−s 
κ1−s + σ(1−s)
ds = t′
2β 2


Rt
2
σ(1−s)2
− ∂s log β1−s ds = t′ σ(1−s)
ds −
2
2
2β1−s
2β1−s


log β1−t − log β1−t′ , (145)



Rt
Rt
2
2
−2η1−s 
exp − t′ κ1−s + σ(1−s)
ds = exp − t′ σ(1−s)
ds ββ1−t′ ,
2β 2
2β 2

(146)

1−s

which means that
1−s

1−s

1−t


′ 2
α1−t′ (σ(1−t′ )2 −2η1−t′ )
β̇
α̇
′
′
)
⃗ t′ ) = α1−t′ σ(1−t
⃗ t′ ).
ξ1−t′ (X
+ β1−t′ − α1−t′ ξ1−t′ (X
2
2
2β1−t
2β1−t
1−t
1−t
′
′

42

(147)


1−s
If we define χ(1 − s) such that σ 2 (1 − s) = 2β1−s α̇
α1−s β1−s − β̇1−s + χ(1 − s), we obtain that


R t 1−s
Rt
2
β̇1−s
χ(1−s) 
ds ββ1−t′ = exp − t′ α̇
ds ββ1−t′
exp − t′ σ(1−s)
2
2
α1−s − β1−s + 2β1−s
2β1−s
1−t
1−t
 β1−t
Rt
R t χ(1−s)  α1−t
χ(1−s) 
= exp t′ ∂s log α1−s − ∂s log β1−s − 2β 2
ds β ′ = exp − t′ 2β 2 ds α ′ ,
1−t

1−s

α1−t′

1−t

1−s

′

α̇
)
β̇
′
′
σ(1−t′ )2
⃗ t′ ) = α1−t′ χ(1−t
⃗ t′ )
+ β1−t′ − α1−t′ ξ1−t′ (X
ξ1−t′ (X
2
2
2β1−t
2β1−t
1−t
1−t
′
′

(149)

If we plug equations (148)-(149) into (146)-(147), and then those into (143), we obtain that


Rt
Rt
R
′
)
⃗ t′ ) dt′
⃗t = X
⃗ 0 exp − t χ(1−s)
+ α1−t 0 exp − t′ χ(1−s)
ds αα1−t
ds χ(1−t
ξ1−t′ (X
X
2
2
2
0 2β1−s
2β1−s
2β1−t
1
′



Rt
Rt
α̇
′
ds αα1−t′ dBt′ .
+ 0 2β1−t′ α1−t′ β1−t′ − β̇1−t′ + χ(1 − t′ ) exp − t′ χ(1−s)
2β 2
1−t

(148)

1−s

(150)

1−t

and if we take the limit t → 1 and use that α1 = 1,



Rt
Rt
R
′
)
⃗1 = X
⃗ 0 limt→1− exp − t χ(1−s)
⃗ t′ ) dt′
ds α1−t + limt→1− α1−t 0 exp − t′ χ(1−s)
ds χ(1−t
X
ξ1−t′ (X
2
2
2
0 2β1−s
2β1−s
2β1−t
′



Rt
Rt
α̇
′
ds αα1−t′ dBt′ .
+ limt→1− 0 2β1−t′ α1−t′ β1−t′ − β̇1−t′ + χ(1 − t′ ) exp − t′ χ(1−s)
2β 2
−

1−t

1−t

1−s

(151)

The assumption on χ in (25) is equivalent, up to a rearrangement of the notation and a flip in the time
variable, to the statement that for all t′ ∈ [0, 1),

Rt
limt→1− exp − t′ χ(1−s)
ds α1−t = 0.
(152)
2β 2
1−s

⃗ 0 in equation (151) is zero. Moreover, this
Hence, under assumption (25), the factor accompanying X
assumption also implies that

Rt
Rt
′
)
⃗ t′ ) dt′
limt→1− α1−t 0 exp − t′ χ(1−s)
ds χ(1−t
ξ1−t′ (X
2
2
2β1−s
2β1−t
′
(153)

 χ(1−t′ )
Rt
R1
⃗ t′ ) dt′ = 0.
′ (X
= 0 limt→1− exp − t′ χ(1−s)
ds
α
ξ
2
2
1−t
1−t
2β
2β
1−t′

1−s

If we plug (152) and (153) into (151), we obtain that



R
R
⃗ 1 = limt→1− t 2β1−t′ α̇1−t′ β1−t′ − β̇1−t′ + χ(1 − t′ ) exp − t′ χ(1−s)
ds αα1−t′ dBt′ ,
X
α
0
t 2β 2
′
1−t

1−s

1−t

(154)

⃗ and X have equal distributions over
⃗ 1 is independent of X
⃗ 0 . Next, we leverage that X
which shows that X
⃗
⃗ 1 ) is equal to the joint distribution
trajectories (Proposition 4). In particular, the joint distribution of (X0 , X
of (X1 , X0 ). We conclude that X1 and X0 are independent, which is the definition of the memorylessness
property. Hence, the assumption (25) is sufficient for memorylessness to hold.
It remains to prove that the assumption (25) is necessary. Looking at equation (150) we deduce that generally,
⃗ 0 and X
⃗ t are not independent, because the first two terms in (150) are different from
for any t ∈ [0, 1), X
⃗ 1 would not be
zero. Thus, if there existed a t′ ∈ [0, 1) such that the limit (152) is different from zero, then X
⃗
⃗
independent from Xt′ , which means that in general it would not be independent of X0 either.

D.2

Proof of Theorem 1: fine-tuning recipe for general noise schedules

The proof of this result relies heavily on the properties of the Hamilton-Jacobi-Bellman equation:
Theorem 3 (Hamilton-Jacobi-Bellman equation). If we define the infinitesimal generator
Pd
Pd
L := 12 i,j=1 (σσ ⊤ )ij (t)∂xi ∂xj + i=1 bi (x, t)∂xi ,

(155)

the value function V for the SOC problem (12)-(13) solves the following Hamilton-Jacobi-Bellman (HJB)
partial differential equation:
∂t V (x, t) = −LV (x, t) + 12 ∥(σ ⊤ ∇V )(x, t)∥2 − f (x, t),
V (x, T ) = g(x).
43

(156)

Consider forward SDEs like (63), starting from the distributions pbase and p∗ , where p∗ (x) ∝ pbase (x) exp(r(x)).
⃗ t = ⃗b(X
⃗ t , t) dt + σ(t) dBt ,
dX
⃗ t∗ = ⃗b∗ (X
⃗ t∗ , t) dt + σ(t) dBt ,
dX

⃗ 0 ∼ pbase ,
X
⃗ 0 ∼ p∗ .
X

(157)
(158)

where the drifts are defined as
⃗b(x, t) = −κ1−t x +
⃗b∗ (x, t) = −κ1−t x +



2
σ(1−t)2
− η1−t s(x, 1 − t) = −κ1−t x + σ(1−t)
− η1−t ∇ log p⃗t (x),
2
2


2
σ(1−t)2
− η1−t s∗ (x, 1 − t) = −κ1−t x + σ(1−t)
− η1−t ∇ log p⃗∗t (x),
2
2

(159)

⃗ t , respectively. p⃗t , p⃗∗t satisfy Fokker-Planck equations:
and p⃗t , p⃗∗t are the densities of Xt , X
2

∇⃗
pt ),
∂t p⃗t = ∇ · (⃗b(x, t)⃗
pt ) + ∇ · ( σ(1−t)
2
2

∇⃗
p∗t ),
∂t p⃗∗t = ∇ · (⃗b∗ (x, t)⃗
p∗t ) + ∇ · ( σ(1−t)
2

p⃗0 = pbase ,
p⃗0 = p∗ .

(160)

Plugging (159) into (160), we obtain

pt ,
∂t p⃗t = ∇ · (κ1−t x⃗
pt ) + ∇ · η1−t ∇⃗

p∗t ,
p∗t ) + ∇ · η1−t ∇⃗
∂t p⃗∗t = ∇ · (κ1−t x⃗

p⃗0 = pbase ,
p⃗0 = p∗ .

We apply the Hopf-Cole transformation to obtain PDEs for − log p⃗t (and − log p⃗∗t analogously):

∇·(κ1−t x⃗
pt )+∇· η1−t ∇⃗
pt
−∂t (− log p⃗t ) = ∂pt pt t =
pt
pt ))
= κ1−t ∇ · x + κ1−t ⟨x, ∇ log p⃗t ⟩ + η1−t ∇·(∇ log p⃗tpexp(log
t

= κ1−t d + κ1−t ⟨x, ∇ log p⃗t ⟩ + η1−t ∆ log p⃗t + ∥∇ log p⃗t ∥2 .

(161)

(162)

Hence, if we define V (x, t) = − log p⃗t (x), V ∗ (x, t) = − log p⃗∗t (x), then V and V ∗ satisfy the following
Hamilton-Jacobi-Bellman equations:

−∂t V = κ1−t d − κ1−t ⟨x, ∇V ⟩ + η1−t − ∆V + ∥∇V ∥2 ,
V (x, 0) = − log pbase (x),
(163)

∗
∗
∗
∗ 2
∗
∗
−∂t V = κ1−t d − κ1−t ⟨x, ∇V ⟩ + η1−t − ∆V + ∥∇V ∥ ,
V (x, 0) = − log p (x).
(164)
Now, define Vˆ(x, t) = V ∗ (x, t) − V (x, t). Subtracting (164) from (163), we obtain

−∂t Vˆ = −κ1−t ⟨x, ∇Vˆ⟩ + η1−t − ∆Vˆ + ∥∇V ∗ ∥2 − ∥∇V ∥2

= −κ1−t ⟨x, ∇Vˆ⟩ + η1−t − ∆Vˆ + ∥∇(Vˆ + V )∥2 − ∥∇V ∥2

= −κ1−t ⟨x, ∇Vˆ⟩ + η1−t − ∆Vˆ + ∥∇Vˆ∥2 + 2⟨∇V , ∇Vˆ⟩

= ⟨−κ1−t x + 2η1−t ∇V , ∇Vˆ⟩ + η1−t − ∆Vˆ + ∥∇Vˆ∥2

(165)


= ⟨−κ1−t x − 2η1−t s(x, 1 − t), ∇Vˆ⟩ + η1−t − ∆Vˆ + ∥∇Vˆ∥2 ,

R base
Vˆ(x, 0) = − log p∗ (x) + log pbase (x) = −r(x) + log
p
(y) exp(r(y)) dy .
Hence, Vˆ also satisfies a Hamilton-Jacobi-Bellman equation. If we define V such that Vˆ(x, t) = V (x, 1 − t),
we have that


R base
∂t V = ⟨−κt x − 2ηt s(x, t), ∇V ⟩ + ηt − ∆V + ∥∇V ∥2 ,
V (x, 1) = r(x) − log
p
(y) exp(r(y)) dy .
(166)
Using Theorem 3, we can reverse-engineer V as the value function of the following SOC problem:
 R1

R base
min E 21 0 ∥u(Xtu , t)∥2 dt−r(x)+log
p
(y) exp(r(y)) dy ,
u∈U

√
√
s.t. dXtu = κt x + 2ηt s(x, t)+ 2ηt u(Xtu , t) dt+ 2ηt dBt ,
X0u ∼ p0 .
44

(167)
(168)

√
Note that this SOC problem is equal to the problem (12)-(13) with the choices f = 0, g = −r, and σ(t) = 2ηt .
By equation (17), the optimal control of the problem (167)-(168) is of the form:

√
√
√
u∗ (x, t) = − 2ηt ∇V (x, t) = − 2ηt ∇Vˆ(x, 1 − t) = − 2ηt ∇V ∗ (x, 1 − t) − ∇V (x, 1 − t)
(169)
 √

√
= − 2ηt − ∇ log p⃗∗1−t (x) + ∇ log p⃗1−t (x) = 2ηt s∗ (x, t) − s(x, t) ,
√
⇐⇒ s∗ (x, t) = s(x, t) + u∗ (x, t)/ 2ηt .
(170)
As in (64), the backward SDEs corresponding to the forward SDEs (158) take the following form:
 ∗ ∗ 
σ(t)2
2 + ηt s (Xt , t) dt + σ(t) dBt ,

dXt∗ = κt Xt∗ +

(171)

X0∗ ∼ N (0, I).

If we plug (170) into this equation, we obtain

u∗ (Xt∗ ,t) 
σ(t)2
∗
√
dt + σ(t) dBt ,
2 + ηt s(Xt , t) +
2ηt

dXt∗ = κt Xt∗ +

σ(t)2
+ηt
2
√

⇐⇒ dXt∗ = b(Xt∗ , t) +


u∗ (Xt∗ , t) dt + σ(t) dBt ,

2ηt

X0∗ ∼ N (0, I),

(172)
(173)

X0∗ ∼ N (0, I).


σ(t)2
2 + ηt s(x, t) by definition in equation (11).

where we used that b(x, t) = κt x +

q
α̇t
The fine-tuned inference SDE for DDIM Now, for DDIM, we have that u∗ (x, t) = − αt (1−α
(ϵ∗ (x, t) −
t)
ϵbase (x, t)) by (26). Hence,
σ(t)2
+ηt
2
√

2ηt

u∗ (x, t) = −

=⇒ b(x, t) +

α̇
σ(t)2
+ 2αt
2
t

q

σ(t)2
+ηt
2
√

2ηt

q

α̇t
αt

α̇t
∗
base
(x, t)) = −
αt (1−αt ) (ϵ (x, t) − ϵ

α̇t
u∗ (x, t) = 2α
Xt −
t

2

σ(t)
α̇t
2αt + 2
2

σ(t)
α̇t
2αt + 2

α̇t
= 2α
Xt −
t

 ϵbase (Xt ,t)
√

1−αt

α̇
σ(t)2
+ 2αt
2
t

√

−

1−αt

(ϵ∗ (x, t) − ϵbase (x, t)),

α̇
σ(t)2
+ 2αt
2
t

√

1−αt

(ϵ∗ (x, t) − ϵbase (x, t))

 ϵ∗ (Xt ,t)
√

(174)

(175)

.
1−α
t

We obtain that the fine-tuned inference SDE for DDIM is
2 ∗
ϵ (Xt∗ ,t) 
α̇t
α̇t
√
dt + σ(t) dBt ,
dXt∗ = 2α
Xt∗ − 2α
+ σ(t)
2
1−α
t
t

(176)

X0∗ ∼ N (0, I),

t

which is matches the SDE (6) with the choice ϵ = ϵ∗ .
The fine-tuned inference SDE for Flow Matching

For Flow Matching, we have that u∗ (x, t) =

r

2
(v ∗ (x, t)−
α̇
βt ( αt βt −β̇t )
t

v base (x, t)) by (27). Hence,
σ(t)2
+ηt
2
√

2ηt

u∗ (x, t) =

α̇
σ(t)2
+βt ( αt βt −β̇t )
2
t

q

r

α̇

2βt ( αt βt −β̇t )

(177)

t

t

=

2
(v ∗ (x, t) − v base (x, t))
α̇
βt ( αt βt −β̇t )

 ∗
σ(t)2
1+
(v (x, t) − v base (x, t)).
α̇
2βt ( αt βt −β̇t )
t

=⇒ b(x, t) +

σ(t)2
+ηt
2
√

2ηt

∗

u (x, t) = v base (x, t) +

σ(t)2
α̇
2βt ( αt βt −β̇t )
t

+ 1+

t
v base (x, t) − α̇
αt x



 ∗
σ(t)2
(v (x, t) − v base (x, t))
α̇t
2βt ( α βt −β̇t )

(178)

t

= v ∗ (x, t) +

σ(t)2
α̇
2βt ( αt βt −β̇t )
t


α̇t
v ∗ (x, t) − α
x .
t

We obtain that the fine-tuned inference SDE for Flow Matching is

σ(t)2
α̇t
dXt∗ = v(Xt∗ , t) +
v ∗ (Xt∗ , t) − α
Xt∗ dt + σ(t) dBt ,
α̇t
t
2βt ( α βt −β̇t )
t

which matches equation (4) with the choice v = v ∗ .
45

X0∗ ∼ N (0, I),

(179)

E

Loss function derivations

E.1

Derivation of the Continuous Adjoint method

Proposition 6. The gradient dL
dθ of the adjoint loss L(u; X) defined in (28) with respect to the parameters θ
of the control can be expressed as in (32).
Proof. First, note that we can write
RT


∇θ E 0 12 ∥uθ (Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ )


RT

RT
= E 0 ∇θ uθ (Xtuθ , t)uθ (Xtuθ , t) dt + ∇θ E 0 12 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ ) |v=stopgrad(uθ ) .
(180)
To develop the second term, we apply Lemma 5. Namely, by the Leibniz rule and equation (185), we have that


RT
∇θ E 0 12 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ ) |v=stopgrad(uθ )




RT
= E ∇θ 0 12 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ ) |v=stopgrad(uθ )
RT

= E 0 (∇θ uθ )(Xtuθ (ω), t)⊤ σ(t)⊤ at (ω) dt .

(181)

Plugging the right-hand side of this equation into (180) concludes the proof.
Lemma 5. Let v be an arbitrary fixed vector field. The unique solution of the ODE


T

1
d
u
u
u
u
u
u
2
∇Xtu (b(Xt , t) + σ(t)u(Xt , t)) a(t; X , u) + ∇Xtu f (Xt , t) + 2 ∥v(Xt , t)∥
, (182)
dt a(t; X , u) = −
(183)

a(1; X u , u) = ∇g(X1u ),
satisfies:
R1

1
u ′ 2
u ′
′
u
2 ∥u(Xt′ , t )∥ +f (Xt′ , t ) dt +g(X1 ) ,
t

where X u solves dXtu = b(Xtu , t) + σ(t)u(Xtu , t) dt + σ(t)dBt .

a(t; X u , u) := ∇Xtu





Moreover, when u = uθ is parameterized by θ we have that

 RT
RT
∇θ 0 21 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ ) = 0 (∇θ uθ )(Xtuθ (ω), t)σ(t)⊤ at (ω) dt.

(184)

(185)

Proof. We use an approach based on Lagrange multipliers which mirrors and extends the derivation of
the adjoint ODE (Domingo-Enrich et al., 2023, Lemma 8). For shortness, we use the notation b̃θ (x, t) :=
b(x, t) + σ(t)uθ (x, t). Define a process a : Ω × [0, T ] → Rd such that for any ω ∈ Ω, a(ω, ·) is differentiable.
For a given ω ∈ Ω, we can write
RT

uθ
uθ
uθ
1
2
2 ∥v(Xt , t)∥ +f (Xt , t) dt+g(XT )

RT 1
= 0 2 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ )
RT
− 0 ⟨at (ω), (dXtuθ (ω) − b̃θ (Xtuθ (ω), t) dt − σ(t) dBt )⟩.



0

By stochastic integration by parts (Domingo-Enrich et al., 2023, Lemma 9), we have that
RT
RT
t
⟨at (ω), dXtuθ (ω)⟩ = ⟨aT (ω), XTuθ (ω)⟩ − ⟨a0 (ω), X0uθ (ω)⟩ − 0 ⟨Xtuθ (ω), da
dt (ω)⟩ dt.
0

46

(186)

(187)

Hence, if X0uθ = x0 is the initial condition, we have that9
RT

uθ
uθ
uθ
1
2
2 ∥v(Xt , t)∥ +f (Xt , t) dt+g(XT )

RT 1
= ∇x0 0 2 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ )

RT
uθ
t
− ⟨aT (ω), XTuθ (ω)⟩ + ⟨a0 (ω), X0uθ (ω)⟩ + 0 ⟨at (ω), b̃θ (Xtuθ (ω), t)⟩ + ⟨ da
dt (ω), Xt (ω)⟩ dt

RT
+ 0 ⟨at (ω), σ(t) dBt ⟩

∇x 0

=





0

RT
0

(188)


∇x0 Xtuθ (ω)⊤ ∇x 21 ∥v(Xtuθ , t)∥2 +f (Xtuθ (ω), t) dt + ∇x0 XTuθ (ω)⊤ ∇x g(XTuθ (ω))

− ∇x0 XTuθ (ω)⊤ aT (ω) + ∇x0 X0uθ (ω)⊤ a0 (ω)

RT
t
+ 0 ∇x0 Xtuθ (ω)⊤ ∇x b̃θ (Xtuθ (ω), t)⊤ at (ω) + ∇x0 Xtuθ (ω)⊤ da
dt (ω) dt


RT
t
= 0 ∇x0 Xtuθ (ω)⊤ ∇x 12 ∥v(Xtuθ , t)∥2 +f (Xtuθ (ω), t) + ∇x b̃θ (Xtuθ (ω), t)⊤ at (ω) + da
dt (ω) dt

+ ∇x0 XTuθ (ω)⊤ ∇x g(XTuθ (ω)) − aT (ω) + a0 (ω).
In the last line we used that ∇x0 X0uθ (ω) = ∇x0 x0 = I. If choose a such that

dat (ω) = − ∇x b̃θ (Xtuθ (ω), t)⊤ at (ω) − ∇x 21 ∥v(Xtuθ , t)∥2 +f (Xtuθ (ω), t) dt,
aT (ω) = ∇x g(XTuθ (ω)),

(189)

which is the ODE (182)-(183), then we obtain that
∇x 0

RT
0

uθ
uθ
1
2
2 ∥v(Xt , t)∥ +f (Xt , t)



(190)


dt+g(XTuθ ) = a0 (ω)

Without loss of generality, this argument can be extended from t = 0 to an arbitrary t ∈ [0, 1], which proves
the first statement of the lemma.
To prove (185), we similarly write
RT

uθ
uθ
uθ
1
2
2 ∥v(Xt , t)∥ +f (Xt , t) dt+g(XT )

RT 1
= ∇θ 0 2 ∥v(Xtuθ , t)∥2 +f (Xtuθ , t) dt+g(XTuθ )
RT
− ⟨aT (ω), XTuθ (ω)⟩ + ⟨a0 (ω), X0uθ (ω)⟩ + 0

RT
+ 0 ⟨at (ω), σ(t) dBt ⟩

∇θ

=





0

RT
0


uθ
t
⟨at (ω), b̃θ (Xtuθ (ω), t)⟩ + ⟨ da
dt (ω), Xt (ω)⟩ dt
(191)


∇θ Xtuθ (ω)⊤ ∇x 12 ∥v(Xtuθ , t)∥2 +f (Xtuθ (ω), t) dt + ∇θ XTuθ (ω)⊤ ∇x g(XTuθ (ω))

− ∇θ XTuθ (ω)⊤ aT (ω) + ∇θ X0uθ (ω)⊤ a0 (ω)

RT
t
+ 0 ∇θ Xtuθ (ω)⊤ ∇x b̃θ (Xtuθ (ω), t)⊤ at (ω) + ∇θ b̃θ (Xtuθ (ω), t)⊤ at (ω) + ∇θ Xtuθ (ω)⊤ da
dt (ω) dt


RT
t
= 0 ∇θ Xtuθ (ω)⊤ ∇x 21 ∥v(Xtuθ , t)∥2 +f (Xtuθ (ω), t) + ∇x b̃θ (Xtuθ (ω), t)⊤ at (ω) + da
dt (ω) dt
 RT
+ ∇θ XTuθ (ω)⊤ ∇x g(XTuθ (ω)) − aT (ω) + 0 (∇θ b̃θ )(Xtuθ (ω), t)⊤ at (ω) dt.
In the last line we used that ∇θ X0uθ (ω) = ∇θ x = 0. When a satisfies (189), we obtain that
RT

uθ
uθ
uθ
1
2
2 ∥v(Xt , t)∥ +f (Xt , t) dt+g(XT )
0
R
RT
T
= 0 (∇θ b̃θ )(Xtuθ (ω), t)at (ω) dt = 0 (∇θ uθ )(Xtuθ (ω), t)⊤ σ(t)⊤ at (ω) dt.

∇θ





(192)

The last equality holds because b̃θ (x, t) := b(x, t) + σ(t)uθ (x, t).
9 Unlike (Domingo-Enrich et al., 2023, Lemma 8), we use the convention that a Jacobian matrix J = ∇

Jij =

∂vi (x)
. Their definition of ∇x v is the transpose of ours.
∂xj

47

x v(x) is defined as

E.2

Proof of Proposition 2: Theoretical guarantees of the basic Adjoint Matching
loss

Let ū = stopgrad(uθ ). We can rewrite equation (32) as:
R1
R1
∇θ L(uθ ; X ū ) = 12 0 ∇θ ∥uθ (Xtū , t)∥2 dt + 0 ∇θ u(Xtū , t)T σ(t)T a(t; X ū , ū)dt
R1
= 21 0 ∇θ ∥uθ (Xtū , t) + σ(t)T a(t; X ū , ū)∥2 dt = ∇θ LBasic−Adj−Match (uθ ; X ū )

(193)
(194)

This proves the first statement of the proposition. To prove that the only critical point of the expected
basic Adjoint Matching loss is the optimal control, we first compute the first variation of E[LBasic−Adj−Match ].
Letting v : Rd × [0, T ] → Rd be arbitrary, we have that

1 R T
d
d
ū
⊤
ū
2
ū
dϵ E[LBasic−Adj−Match (u + ϵv; X )] = dϵ E 2 0 ∥(u + ϵv)(Xt , t) + σ(t) a(t, X , ū)∥ dt

RT
= E 0 ⟨v(Xtū , t), u(Xtū , t) + σ(t)⊤ a(t, X ū , ū)⟩ dt
(195)

 
RT
= E 0 ⟨v(Xtū , t), u(Xtū , t) + σ(t)⊤ E a(t, X ū , ū)|Xtū ⟩ dt


δ
=⇒ δu
E[LBasic−Adj−Match (u)(x, t) = u(x, t) + E a(t, X ū , ū)|Xtū = x
Hence, critical points satisfy that



RT
u(x, t) = −σ(t)⊤ E[a(t, X u , u)|Xtu = x] = −σ(t)⊤ E ∇Xtv t 21 ∥v(Xtv , t)∥2 +f (Xtv , t) dt+g(XTv )|X0v = x
RT


= −σ(t)⊤ ∇x E t 21 ∥v(Xtv , t)∥2 +f (Xtv , t) dt+g(XTv )|X0v = x = −σ(t)⊤ ∇J(u; x, t),
(196)
In this equation, the second equality holds by equation (184) from Lemma 5, and the third equality holds by
the Leibniz rule.
Lemma 6 shows that any control u that satisfies (196) is equal to the optimal control, which concludes the
proof.
Lemma 6. Suppose that for any x ∈ Rd , t ∈ [0, T ], u(x, t) = −σ(t)⊤ ∇x J(u; x, t). Then, J(u; ·, ·) satisfies the
Hamilton-Jacobi-Bellman equation (156). By the uniqueness of the solution to the HJB equation, we have
that J(u; x, t) = V (x, t) for any x ∈ Rd , t ∈ [0, T ]. Hence, u(x, t) = −σ(t)⊤ ∇x V (x, t) is the optimal control.
RT
Proof. Since J(u; x, t) = E t

1
u
2
u
2 ∥u(Xt , t)∥ + f (Xt , t)




ds + g(XTu )|Xtu = x , we have that


 R t+∆t

u
, t + ∆t)|Xt = x + E t
J(u; x, t) = E J(u; Xt+∆t

1
2
u
u
2 ∥u(Xs , s)∥ + f (Xs , s)



(197)


ds|Xt = x ,

which means that
 R t+∆t
u
E[J(u; Xt+∆t
, t + ∆t)|Xt = x] − J(u; x, t) E t
0=
+
∆t

1
u
2
u
2 ∥u(Xt , t)∥ + f (Xt , t)



ds|Xt = x

∆t

Recall that the generator T u of the controlled SDE (13) takes the form:


u
E f (Xt+∆t
,t)|Xt =x −f (x,t)
u
T f (x, t) := lim∆t→0
∆t
⊤

= ∂t f (x, t) + ⟨∇f (x, t), b(x, t) + σ(t)u(x, t)⟩ + Tr σ(t)σ(t)
∇2 f (x, t)
2


(198)

(199)


Hence, if we take the limit ∆t → 0 on equation (198), we obtain that:
0 = T u J(u; x, t) + 21 ∥u(x, t)∥2 + f (x, t)

(200)

⊤
= ∂t J(u; x, t) + ⟨∇J(u; x, t), b(x, t) + σ(t)u(x, t)⟩ + Tr σ(t)σ(t)
∇2 J(u; x, t) + 12 ∥u(x, t)∥2 + f (x, t).
2

Now using that u(x, t) = −σ(t)⊤ ∇x J(u; x, t), we have that
⟨∇J(u; x, t), σ(t)u(x, t)⟩ + 21 ∥u(x, t)∥2 = −∥σ(t)⊤ ∇x J(u; x, t)∥2 + 21 ∥σ(t)⊤ ∇x J(u; x, t)∥2
= − 21 ∥σ(t)⊤ ∇x J(u; x, t)∥2 .
48

(201)

Plugging this back into (200), we obtain that

⊤
0 = ∂t J(u; x, t) + ⟨∇J(u; x, t), b(x, t)⟩ + Tr σ(t)σ(t)
∇2 J(u; x, t) − 12 ∥σ(t)⊤ ∇x J(u; x, t)∥2 + f (x, t). (202)
2
And since J(u; x, T ) = g(x) by construction, we conclude that J(u; x, t) satisfies the HJB equation (156).

E.3

Theoretical guarantees of the Adjoint Matching loss

Proposition 7 (Theoretical guarantee of the Adjoint Matching loss). The only critical point of the loss
E[LAdj−Match ] is the optimal control u∗ .
Proof. Let v be an arbitrary control. If ã(t; Xv ) is the solution of the Lean Adjoint ODE (38)-(39), it satisfies
the integral equation

RT
ã(t; Xv ) = t ∇x b(Xsv , s)⊤ ã(s; Xv ) + ∇x f (Xsv , s) ds + ∇g(XTv ).
(203)
Hence,


RT


E ã(t; Xv ) Xtv = E t ∇x b(Xsv , s)⊤ ã(s; Xv ) + ∇x f (Xsv , s) ds + ∇g(XTv ) Xtv
RT




= E t ∇x b(Xsv , s)⊤ E ã(s; Xv ) Xsv + ∇x f (Xsv , s) ds + ∇g(XTv ) Xtv ,

(204)

where we used the tower property of conditional expectation in the second equality.
Similarly, if a(t; Xv , v) is the solution of the Adjoint ODE (30)-(31), it satisfies the integral equation


RT
a(t; Xv , v) = t ∇x b(Xsv , s)⊤ a(s; Xv , v) + σ(s)v(Xsv , s) + ∇x f (Xsv , s) + 21 ∥v(Xsv , s)∥2 ds + ∇g(XTv ),
(205)
and its expected value satisfies


E a(t; Xv , v) Xtv


RT
⊤
= E t ∇x b(Xsv , s) + σ(s)v(Xsv , s) a(s; Xv , v) + ∇x f (Xsv , s) + 12 ∥v(Xsv , s)∥2 ds + ∇g(XTv ) Xtv
⊤ 

RT


= E t ∇x b(Xsv , s)+σ(s)v(Xsv , s) E a(s; Xv , v) Xsv +∇x f (Xsv , s)+ 21 ∥v(Xsv , s)∥2 ds+∇g(XTv ) Xtv .
(206)
Let us rewrite E[LAdj−Match ] as follows:
 2 

RT
dt |v=stopgrad(u)
E[LAdj−Match (u)] := E 0 u(Xtv , t) + σ(t)⊤ E ã(t, Xv )|Xtv
RT


 2 
+ E 0 σ(t)⊤ E ã(t, Xv )|Xtv − ã(t, Xv )
dt |v=stopgrad(u) ,

(207)

Now, suppose that û is a critical point of E[LAdj−Match ]. By definition, this implies that the first variation of
E[LAdj−Match ] is zero. Using (207), we can write this as follows:

δ
0 = δu
E[LAdj−Match (û)](x) = 2 û(x, t) + σ(t)⊤ E[ã(t, Xû )|Xtû = x] ,
(208)
=⇒ û(x, t) = −σ(t)⊤ E[ã(t, Xû )|Xtû = x].

(209)

Hence, we have
∇x û(Xtû , t)⊤ σ(t)⊤ E[ã(t, Xû )|Xtû ] + ∇x û(Xtû , t)⊤ û(Xtû , t) = 0,


RT
⊤ 

=⇒ E t ∇x σ(s)û(Xsû , s) E ã(s; Xû ) Xsû +∇x 21 ∥û(Xsû , s)∥2 ds Xtû = 0.

(210)
(211)

If we set v = û in equation (204), and add (211) to its right-hand side, we obtain that E[ã(t, X û )|Xtû ] also
solves the integral equation


E ã(t; Xû ) Xtû
RT
⊤ 



= E t ∇x b(Xsû , s)+σ(s)û(Xsû , s) E ã(s; Xû ) Xsû +∇x f (Xsû , s)+ 21 ∥û(Xsû , s)∥2 ds+∇g(XTû ) Xtû .
(212)
49

Note that this integral equation is the same one as equation (206) when we set v = û in the latter.

Proposition
8 states
that the solution of the integral equation is unique, which means that E ã(t; Xû ) Xtû =


E a(t; Xû , û) Xtû for all t ∈ [0, T ].
Since we can reexpress the basic Adjoint Matching loss as
RT

 2 
E[LBasic−Adj−Match (u)] := E 0 u(Xtv , t) + σ(t)⊤ E a(t; Xv , v)|Xtv
dt |v=stopgrad(u)
RT


 2 
+ E 0 σ(t)⊤ E a(t; Xv , v)|Xtv − a(t; Xv , v)
dt |v=stopgrad(u) ,

(213)

we obtain that when û is a critical point of E[LAdj−Match ],
d
⊤
û
û
du E[LBasic−Adj−Match (û)](x) = 2 û(x, t) + σ(t) E[a(t; X , û)|Xt = x]

= 2 û(x, t) + σ(t)⊤ E[ã(t; Xû )|Xtû = x] = 0,



(214)





where the second equality holds because E ã(t; Xû ) Xtû = E a(t; Xû , û) Xtû , and the third equality
holds by equation (209). Thus, we deduce that the critical points of E[LAdj−Match ] are critical points
of E[LBasic−Adj−Match ]. By Proposition 2, E[LBasic−Adj−Match ] has a single critical point, which is the optimal
control u∗ , which concludes the proof of the statement for E[LAdj−Match ].
Proposition 8. Let v be an arbitrary control. Consider the integral equation:


RT
⊤
Yt = E t ∇x b(Xsv , s)+σ(s)v(Xsv , s) Ys +∇x f (Xsv , s)+ 21 ∥v(Xsv , s)∥2 ds+∇g(XTv ) Xtv ,

(215)

where t ∈ [0, T ]. This equation has a unique solution, i.e. if Y 1 , Y 2 are two solutions then Y1 = Y2 .
Proof. Let Y 1 , Y 2 be two solutions of the integral equation. We have that

RT

Yt1 − Yt2 = E t (Ys1 − Ys2 )⊤ ∇x b(Xs∗ , s) ds Xt∗ .

(216)

Thus,
∥Yt1 − Yt2 ∥



RT

 RT
(Ys1 − Ys2 )⊤ ∇x b(Xs∗ , s) ds Xt∗
(Ys1 − Ys2 )⊤ ∇x b(Xs∗ , s) ds Xt∗ ≤ E t
≤E
t
RT

 RT 


≤ E t Ys1 − Ys2 · ∇x b(Xs∗ , s) ds Xt∗ = t E Ys1 − Ys2 · ∇x b(Xs∗ , s) Xt∗ ds
1/2
1/2

RT 
2
2
ds
· E ∇x b(Xs∗ , s) Xt∗
≤ t E Ys1 − Ys2 Xt∗

(217)

And this implies that
1/2
supt′ ∈[0,t] E[∥Yt1 − Yt2 ∥2 |Xt∗′ ]
1/2

1/2
RT 
2
2
· E ∇x b(Xs∗ , s) Xt∗
ds
≤ t E Ys1 − Ys2 Xt∗
 1


1/2
RT
2
1/2
2
≤ t supt′ ∈[0,s] E Ys − Ys2 Xt∗′
· supt′ ∈[0,s] E ∇x b(Xs∗ , s) Xt∗′
ds.

(218)

1/2
Applying Grönwall’s inequality on the function f (t) = supt′ ∈[0,t] E[∥Yt1 − Yt2 ∥2 |Xt∗′ ]
, we obtain that

1
2 2
∗ 1/2
1
2
supt′ ∈[0,t] E[∥Yt − Yt ∥ |Xt′ ]
= 0 for all t ∈ [0, T ], which means that Yt = Yt almost surely. And since
1/2

1/2
RT  1
2
1
2
2 2
∥Yt − Yt ∥ ≤ t E Ys − Ys |Xt∗
· E ∇x b(Xs∗ , s) |Xt∗
ds = 0, we obtain that Y 1 = Y 2 .

E.4

Pseudo-code of Adjoint Matching for DDIM fine-tuning

Note that for each pair of equations (219)-(220), (221)-(222), (223)-(224), the first equation corresponds to
the updates in the DDPM paper, while the second equation is an Euler-Maruyama / Euler discretization of
the continuous-time object. To check that both discretizations are equal up to first order, remark that
q
q
ᾱk+1
−ᾱk
−ᾱk
1 + ᾱk+1
≈ 1 + ᾱk+1
+ O((ᾱk+1 − ᾱk )2 ).
(225)
ᾱk =
ᾱk
2ᾱk
50

Algorithm 2 Adjoint Matching for fine-tuning DDIM
Input: Pre-trained denoiser ϵbase , number of fine-tuning iterations N .
Initialize fine-tuned denoiser: ϵfinetune = ϵbase with parameters θ.
for n ∈ {0, . . . , N − 1} do
Sample m trajectories X = (Xt )t∈{0,...,1} according to DDPM, e.g.:
q
 q 1−ᾱk+1

ᾱk+1
1−ᾱk /ᾱk+1 finetune
k
Xk+1 =
ϵ
(Xk , k) +
Xk − √1−
1 − ᾱᾱk+1
εk ,
ᾱk
1−ᾱk
ᾱ
k

εk ∼ N (0, I), X0 ∼ N (0, I),
(219)

ᾱ
−ᾱk finetune
ᾱ
−ᾱk
√
Xk − ᾱ k+1
or Xk+1 = Xk + k+1
ϵ
(Xk , k) +
2ᾱk
k 1−ᾱk

q

ᾱk+1 −ᾱk
εk .
ᾱk

For each trajectory, solve the lean adjoint ODE (38)-(39) backwards in time from k = K to 0, e.g.:
q


ᾱk+1
1−ᾱk /ᾱk+1 base
√
ãk = ãk+1 + ãTk+1 ∇Xk
ϵ
(X
,
k)
−
X
,
ãK = ∇XK r(XK ),
X
−
k
k
k
ᾱk
1−ᾱk


ᾱ
−ᾱk base
ᾱk+1 −ᾱk
√
or ãk = ãk+1 + ãTk+1 ∇Xt
Xk − ᾱ k+1
ϵ
(Xk , k) ,
ãK = ∇XK r(XK ).
2ᾱk
1−ᾱ
k

k

(220)

(221)
(222)

Note that Xk and ãk should be computed without gradients, i.e., Xk = stopgrad(Xk ), ãk = stopgrad(ãk ).
For each trajectory, compute the Adjoint Matching objective (37):
q
 finetune
P
ᾱk+1
k
LAdj−Match (θ) = k∈{0,...,K−1}
1 − ᾱᾱk+1
(ϵ
(Xk , k) − ϵbase (Xk , k))
ᾱk (1−ᾱk+1 )
q

2
1−ᾱk+1
k
−
1 − ᾱᾱk+1
ãk ,
1−ᾱk
q
q
P
2
ᾱk+1 −ᾱk
ᾱk+1 −ᾱk
finetune
base
or LAdj−Match (θ) = k∈{0,...,K−1}
(ϵ
(X
,
k)
−
ϵ
(X
,
k))
−
ãk .
k
k
ᾱk (1−ᾱk )
ᾱk

(223)

(224)

Compute the gradient ∇θ L(θ) and update θ using favorite gradient descent algorithm.
end
Output: Fine-tuned vector field v finetune

F

Adapting diffusion fine-tuning baselines to flow matching

F.1

Adapting ReFL (Xu et al., 2023) to flow matching

Reward Feedback Learning (ReFL) is a diffusion fine-tuning algorithm introduced by Xu et al. (2023) which
tries to increase the reward on denoised samples. Namely, if X = (Xt )t∈[0,1] is the solution of the DDPM
SDE (7), we can denoise Xt as
√

ᾱ ϵ(Xt ,t)
√ t
X̂1 (Xt ) = Xt − 1−
.
ᾱt

(226)

This equation follows from the stochastic interpolant equation (2) if we replace X̄0 with the noise predictor
ϵ(Xt , t). And then, the ReFL optimization update is based on the gradient:
√
ᾱ ϵ (X ,t) 
√ t θ t
∇θ r(X̂1 (Xt )) = ∇θ r Xt − 1−
,
(227)
ᾱt
where the trajectories have been detached.
To adapt ReFL to Flow Matching, we need to express the denoiser map in terms of the vector field v. We
have that






v(x, t) = E β̇t X̄0 + α̇t X̄1 βt X̄0 + αt X̄1 = x = E β̇βtt βt X̄0 + αt X̄1 + α̇t − β̇βtt αt X̄1 βt X̄0 + αt X̄1 = x

= β̇βtt x + α̇t − β̇βtt αt X̂1 (x, t).
(228)


where we defined the denoiser map X̂1 (x, t) := E X̄1 |βt X̄0 + αt X̄1 = x . Hence,
X̂1 (x, t) =

β̇
t
β̇t
α̇t − β αt
t

v(x,t)− βt x

51

.

(229)

F.2

Adapting Diffusion-DPO (Wallace et al., 2023a) to flow matching

l
w
l
The Diffusion-DPO loss assumes access to ranked pairs of generated samples xw
1 ≻ x1 , where x and x are
the winning and losing samples. For DDPM, the loss implemented in practice reads (Wallace et al., 2023a,
Eq. 46):

LDPO (θ) = −E(xw
l
w
w
w
l
l
l
1 ,x1 )∼D,k∼U [0,K],xkh ∼q(xkh |x1 ),xt ∼q(xkh |x1 )

(230)

2
w
w
2
log S − β̃2 ∥εw − ϵθ (xw
kh , kh)∥ − ∥ε − ϵref (xkh , kh)∥
l

− ∥ε

− ϵθ (xlkh , kh)∥2 − ∥εl − ϵref (xlkh , kh)∥2



,

where S(x) = 1+e1−x denotes the sigmoid function, and q(x∗kh |x∗1 ) is the conditional distribution of the forward
√
√
process, i.e. x∗kh is sampled as x∗kh = γkh x∗1 + 1 − γkh ϵ, ϵ ∼ N (0, I). Following the derivation of the
2
Diffusion-DPO loss in (Wallace et al., 2023a, Sec. S4), we observe that the term − β̃2 ∥εw − ϵθ (xw
kh , kh)∥ arises
from

(231)

β̃
w 2
− 1−γ
∥x̂1 (xw
kh ) − x1 ∥ ,
kh
2

γkh

up to a constant term in θ. If we switch to the more general flow matching scheme, the analog of this term is
(232)

w 2
− ββ̃2 ∥x̂1 (xw
kh ) − x1 ∥ .
2

kh
α2
kh

Using the expression of the denoiser map in terms of the vector field v in equation (229), we can rewrite (232)
as:
β̇

− ββ̃2

kh xw
v(xw
kh
kh ,kh)− β

− xw
1

kh
β̇
α̇kh − βkh αkh
kh

2 kh
α2
kh

2

β̇

kh xw
v(xw
kh ,kh)− β
kh

= − β̃2

kh
α̇kh
αkh βkh −β̇kh

kh w
x1
− αβkh

2

(233)

.

Thus, the Diffusion-DPO loss for Flow Matching reads
LDPO (θ) = −E(xw
l
w
w
w
l
l
l
1 ,x1 )∼D,k∼U [0,K],xkh ∼q(xkh |x1 ),xt ∼q(xkh |x1 )
β̇

log S − β̃2

kh xw
vθ (xw
kh ,kh)− β
kh
kh
α̇kh
αkh βkh −β̇kh

kh w
x1
− αβkh

2

β̇

−

β̇

−

vθ (xlkh ,kh)− βkh xlkh
kh
α̇kh
αkh βkh −β̇kh



2

kh l
− αβkh
x1

kh xw
vref (xw
kh
kh ,kh)− β
kh
α̇kh
αkh βkh −β̇kh

kh w
x1
− αβkh

2

(234)

β̇

vref (xlkh ,kh)− βkh xlkh

−

kh
α̇kh
αkh βkh −β̇kh

kh l
− αβkh
x1

2 

,

(Wallace et al., 2023a, Sec. 5.1) claim that β ∈ [2000, 5000] yields good performance on Stable Diffusion 1.5
and Stable Diffusion XL-1.0, which if we translate to our notation corresponds to β̃ ∈ [4000, 10000].
l
When we have access to the reward function r, instead of a winning sample xw
1 and a losing sample x1 , we have
1
a
b
a
b
a

a pair of samples (x1 , x1 ) with winning weights S(r(x1 ) − r(x1 )) =
, S(−(r(x1 ) − r(xb1 ))) =
b
a
1+exp r(x1 )−r(x1 )

1

 . Hence, the loss (234) becomes:

1+exp −(r(xb1 )−r(xa
1 ))


LDPO (θ) = −E(xa1 ,xb1 )∼D,k∼U [0,K],xakh ∼q(xakh |xa1 ),xbt ∼q(xbkh |xb1 )
β̇

kh xa
vθ (xa
kh ,kh)− β
kh

log S − s2β̃

kh
α̇kh
αkh βkh −β̇kh

−

kh a
− αβkh
x1

2

P

s∈{±1} S


s(r(xa1 ) − r(xb1 )) ×
β̇

−

kh xa
vref (xa
kh ,kh)− β
kh
kh
α̇kh
αkh βkh −β̇kh

kh a
− αβkh
x1

2

(235)


β̇
β̇

vθ (xbkh ,kh)− βkh xbkh
vref (xbkh ,kh)− βkh xbkh
αkh b 2
αkh b 2
kh
kh
− βkh x1 −
− βkh x1
.
α̇kh
α̇kh
αkh βkh −β̇kh

αkh βkh −β̇kh

We want to emphasize that despite the similarities, even though the loss LDPO that we use (equation (235))
is very similar to the one implemented by Wallace et al. (2023a), the preference data pairs that we use
are very different from theirs. We sample the preference data from the current model, which results in
52

imperfect samples, while they consider off-policy, high-quality, curated preference samples. The reason for
this discrepancy is that the starting point of our work is a reward model, not a set of preference data, and we
only benchmark against approaches that leverage reward models for an apples-to-apples comparison. Our
experimental results on DPO (Table 2, Figure 6, Table 3) show that the resulting model performs like the
base model, or a bit worse according to some metrics. Hence, we conclude that DPO is not a competitive
alternative for on-policy fine-tune when the base model is not already good.

G

Experimental details

Unless otherwise specified, we used the same hyperparameters across all fine-tuning methods. Namely, we
used:
• K = 40 timesteps.
• Adam optimizer with learning rate 2 × 10−5 and parameters β1 = 0.95, β2 = 0.999, ϵ = 1 × 10−8 , weight
decay 1 × 10−2 , gradient norm clipping value 1. For Discrete Adjoint, these hyperparameters resulted in
fine-tuning instability (see Table 6); the results that we report in all other tables for Discrete Adjoint
were obtained with learning rate 1 × 10−5 .
• Bfloat16 precision.
• Effective batch size 40; for each run we used two 80GB A100 GPUs with batch size 20 each.
• A set of 40k fine-tuning prompts taken from a licensed dataset consisting of text and image pairs
(note that we disregarded the images). Thus, each epoch lasts 1000 iterations; see the total amount of
fine-tuning iterations for each algorithm in Table 3. For each of the three runs that we perform for each
data point that we report, the set of 40k prompts is sampled independently among a total set of 100k
prompts.

G.1

Noise schedule details

Since we use K = 40 discretization steps, the timesteps are t ∈ {0, 0.025, 0.05, 0.075, 0.1, . . . , 0.95, 0.975}.
To sample Xt+h from Xt we use equation (40). q
We use the choices αt = t, βt = 1 − t, which means that
q
q
α̇t
1−t
σ(t) = 2βt ( αt βt − β̇t ) = 2(1 − t)( t + 1) = 2(1−t)
.
t
Note that if we plug t = 0 into this expression, we obtain infinity, and if we plug t ⪅ 1, we obtain σ(t) ≈ 0.
For obvious
preasons,pthe former issue requires a fix: we simply add a small offset to the denominator of σ(t),
replacing 1/t by 1/(t + h) (note that h := 1/K = 0.025). But the latter issue is also not completely
satisfactory from a practical standpoint, because looking at the adjoint matching loss (37), we observe that
u(Xtū , t) is trained to approximate the conditional expectation of σ(t)T ã(t; X ū ). Thus, if we set σ(t) very close
to zero for t ⪅ 1, we are forcing the control u to be close to zero as well, or equivalently preventing v finetune
from deviating from v base . While this is the right thing to do from a theoretical perspective, we concluded
experimentally that setting σ(t) just slightly larger results in substantially faster fine-tuning, thanks to the
additional leeway provided to v finetune to deviate from v base . In particular, we added a small offset to the
factor 1 − t in the numerator 1 − t of σ(t): we replaced 1 − t by 1 − t + h. Thus, the expression that we used
to compute the diffusion coefficient in our experiments is
q
σ(t) = 2(1−t+h)
.
(236)
t+h
When solving the lean adjoint ODE (38)-(39) backwards in time via the Euler scheme (41), the timesteps
we use are t ∈ {1, 0.975, 0.95, 0.925, 0.9, . . . , 0.05, 0.025}. We do not actually initialize the adjoint state as
∇x g(X1 ), but rather as ∇x g(X̂1 ), where X̂1 := X1−h + hv base (X1−h , 1√− h). That is, X̂1 is obtained by
performing a final noiseless update, instead of using noise σ(1 − h) = 4h given by equation (236). The
reason for this is that the regular final iterate X1 contains some noise that was added in the final step, and
that can distort the gradient ∇x g(X1 ). By setting ã(1; X) = ∇x g(X1 ), we get rid of this bias. Note that in
the continuous time limit h → 0, X̂1 = X1 , which means that this small trick is consistent.

53

G.2

Selection of gradient evaluation timesteps


2
2
In Algorithm 1, equation (42), we state that the term σ(t)
vθfinetune (Xt , t) − v base (Xt , t) + σ(t)ãt must be
computed for all K steps in {0, . . . , 1 − h}. However, the gradient signal provided by backpropagating through
this expression for consecutive times t and t + h is quite similar. In the interest of computational efficiency, we
2
sample a subset K of timesteps, and we only compute and backpropagate the terms σ(t)
vθfinetune (Xt , t) −

2
v base (Xt , t) + σ(t)ãt for those timesteps. We construct K by sampling ten timesteps uniformly without
repetition among {0, . . . , 0.725}, and always sampling the last ten timesteps {0.75, . . . , 0.975}. This is because
fine-tuning the last ten steps (25% of the total) well is critical for good empirical performance, while the
initial steps are not as important.

G.3

Loss function clipping: the LCT hyperparameter

Note that the magnitude of σ(t)T a(t; X ū , ū) is much larger for times t ⪆ 0 than for times t ⪅ 1. The reason is
two-fold:
• As discussed in Appendix G.1, σ(t) is much larger for t ⪆ 0 than for t ⪅ 1.
• The magnitude of the lean adjoint state ã grows roughly exponentially as t goes backward in time. In
fact, if we assumed that ∇x b(Xt , t) is constant in time, this statement would be exact.

2
2
Observe that when σ(t)T a(t; X ū , ū) is large, the gradient ∇θ σ(t)
vθfinetune (Xt , t) − v base (Xt , t) + σ(t)ãt
also has a high magnitude. Including such terms in our gradient computation decreases the signal to noise
ratio of the gradient. Even more so, as discussed in Appendix G.2 for good practical performance it is critical
to get a good gradient signal from the last 25% steps. Hence, including the high-magnitude terms for t ⪅ 0 in
our gradients can muffle these other important, low-magnitude terms.

2
2
To fix this issue, we clip the terms such that σ(t)
vθfinetune (Xt , t) − v base (Xt , t) + σ(t)ãt > LCT, where
LCT stands for the loss clipping threshold. That is, the adjoint matching loss that we use in our experiments
is of the form:


P
2
2
vθfinetune (Xt , t) − v base (Xt , t) + σ(t)ãt
,
(237)
L̂Adj−Match (θ) = t∈K min LCT, σ(t)
where K is the random timestep subset described in Appendix G.2.
For adjoint matching, we set LCT = 1.6 × λ2 . Remark that LCT needs to grow quadratically with λ,
because the magnitude of the lean adjoint ã grows quadratically with λ. We set the constant 1.6 through
experimentation; all or almost all of the terms for the last ten timesteps fall below LCT, but only a fraction of
the terms (≈ 25%) for the first ten steps fall below LCT. The constant for LCT is a relevant hyperparameter
that needs to be tuned to obtain a similar behavior.
We also used loss function clipping on the continuous adjoint loss. For that loss we set LCT = 1600 × λ2 .
The reason is that the magnitude of the regular adjoint states is significantly larger than the magnitude of the
lean adjoint states (which is a big reason why adjoint matching outperforms the continuous adjoint).

G.4

Computation of evaluation metrics

We used the open_clip library (Ilharco et al., 2021) to compute ClipScores. We computed ClipScore diversity
as the variance of Clip embeddings of 40 generations for a given prompt, averaged across 25 prompts. Namely,
P40
P
1
2
k
k 2
ClipScore_Diversity = 40
(238)
k=1 25·24
1≤i<j≤25 ∥Clip(gi ) − Clip(gj )∥ ,
where gik denotes the i-th generation for the k-th prompt.
We used the transformers library to compute the PickScore processor and model (Kirstain et al., 2023).
PickScore diversity is computed in analogy with ClipScore diversity.
We used the hps library to compute values of Human Preference Score v2 (Wu et al., 2023b).
To compute Dreamsim diversity we use the dreamsim library (Fu et al., 2023). Dreamsim diversity is computed
in analogy with ClipScore diversity.
54

G.5

Remarks on computational costs

Observe from the figures reported in Table 3 that the per iteration wall-clock time of Adjoint Matching (156
seconds) is very similar to that of the Discrete Adjoint loss (152 seconds). The reason is that both algorithms
perform a similar amount of forward and backward passes on the flow matching model and the reward model.
Namely, for each sample in the batch, both algorithms perform K forward passes on the flow model to obtain
the trajectories. In order to compute the gradient of the training loss, the Discrete Adjoint loss does K
additional forward passes to evaluate the base flow model, one forward and backward pass on the reward
model, and K backward passes on the current flow model, which typically use gradient checkpointing to avoid
memory overflow. In the case of Adjoint Matching, solving the lean adjoint ODE requires one forward and
backward pass on the reward model, and K backward passes on the base flow model. Finally, computing the
gradient of the loss takes K/2 additional backward passes if we evaluate at only half of the timesteps as we
do, although this computation is much quicker because it can be fully parallelized.
Meanwhile, computing the gradient of the Continuous Adjoint loss takes 204 seconds per iteration. With
respect to Adjoint Matching, Continuous Adjoint performs additional backward passes to compute the gradients
∇Xt ∥u(Xt , t)∥2 when solving the adjoint ODE. Finally, we observe that models that directly fine-tune the
reward are quicker, but that comes with its own set of issues that we discuss throughout the paper.

G.6

Remarks on number of sampling timesteps

In our experiments and all baselines, we used 40 timesteps in the fine-tuning procedure (h = 1/40 in
Algorithm 1). The experiments reported in all tables and figures except for Table 8 were performed at 40
inference timesteps. In Table 8 (Appendix A), we show experimental results at 10, 20, 40, 100, and 200
inference timesteps, for the base model and the models fine-tuned with adjoint matching and DRaFT-1. We
make the following observations about the results:
• The metrics for Adjoint Matching at 100 and 200 timesteps are statistically equal to the ones for 40
timesteps, with slight increases in Dreamsim diversity. This suggests that fine-tuning at large numbers
of timesteps is a good idea if we want to perform inference at a large number of timesteps, as otherwise
the capabilities of the model are limited by the number of fine-tuning timesteps instead of the inference
compute. Also, at 100 and 200 timesteps the difference in performance of Adjoint Matching relative to
DRaFT-1 increases.
• The metrics for Adjoint Matching at 10 and 20 timesteps are worse than at 40 timesteps, especially for
10. The difference in performance between Adjoint Matching and DRaFT-1 vanishes at 10 timesteps for
all metrics except for diversity, for which Adjoint Matching is still clearly better.

55

