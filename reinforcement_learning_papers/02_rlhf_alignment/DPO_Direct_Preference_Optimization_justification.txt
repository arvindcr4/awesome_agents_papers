Paper: DPO_Direct_Preference_Optimization.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Direct Preference Optimization: Your Language Model is Secretly a Reward Model  arXiv:2305.18290v3 [cs.LG] 29 Jul 2024  Rafael Rafailov∗†  Archit Sharma∗†  Stefano Ermon†‡  Christopher D. Manning†  Eric Mitchell∗† Chelsea Finn†  †  Stanford University ‡ CZ Biohub {rafailov,architsh,eric.mitchell}@cs.stanford.edu  Abstract While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability col...

Specific Evidence for Classification:
- "Direct Preference Optimization: Your Language Model is Secretly a Reward Model  arXiv:2305.18290v3 [cs.LG] 29 Jul 2024  Rafael Rafailov∗†  Archit Sharma∗†  Stefano Ermon†‡  Christopher D."
- "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF)."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
