PAPER: 1602_02672.pdf
TITLE: Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks
ARXIV_ID: 1602.02672v1
RESEARCH_METHOD: 03_multi_agent_rl
METHOD_DESCRIPTION: The paper proposes deep distributed recurrent Q-networks (DDRQN), a multi-agent reinforcement learning approach that enables teams of agents to learn to solve communication-based coordination tasks. DDRQN makes three key modifications to the naive method: last-action input, inter-agent weight sharing, and disabling experience replay. The approach is evaluated on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols.
KEY_CONTRIBUTIONS:
- Introduced a new multi-agent reinforcement learning approach called deep distributed recurrent Q-networks (DDRQN) that enables teams of agents to learn to solve communication-based coordination tasks.
- Evaluated DDRQN on two multi-agent learning problems based on well-known riddles, demonstrating its ability to successfully solve such tasks and discover elegant communication protocols.
- Conducted ablation experiments to confirm that each of the main components of the DDRQN architecture are critical to its success.
- Showed that DDRQN can outperform baseline methods, including tabular Q-learning and a naive approach, in solving multi-agent coordination tasks.
- Demonstrated the potential of DDRQN to learn communication protocols in a decentralized manner, without requiring a pre-designed communication protocol.
