{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploration.py\n",
    "\n",
    "Auto-generated implementation from the Agentic RL PhD codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Implementations & References\n",
    "The following links point to the official or high-quality reference implementations for the papers covered in this notebook:\n\n",
    "- https://github.com/pathak22/noreward-rl (ICM), https://github.com/openai/random-network-distillation (RND)\n",
    "\n",
    "*Note: The code below is a simplified pedagogical implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Papers:\n",
    "# 1. \"Curiosity-driven Exploration by Self-supervised Prediction\" (ICM)\n",
    "# 2. \"Exploration by Random Network Distillation\" (RND)\n",
    "# 3. \"Efficient Imitation Learning with Double Exploration\" (ILDE)\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Intrinsic Curiosity Module (Pathak et al., 2017)\n",
    "    Innovation: Reward = Prediction Error of Inverse/Forward Dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(state_dim, 128)\n",
    "        \n",
    "        # Forward Model: predict next state feature\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(128 + action_dim, 128),\n",
    "            nn.Linear(128, 128) # Predicts phi(s_next)\n",
    "        )\n",
    "        \n",
    "        # Inverse Model: predict action taken\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 128),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def intrinsic_reward(self, state, next_state, action):\n",
    "        phi_s = self.encoder(state)\n",
    "        phi_next = self.encoder(next_state)\n",
    "        \n",
    "        # Predict next state features\n",
    "        pred_phi_next = self.forward_model(torch.cat([phi_s, action], dim=1))\n",
    "        \n",
    "        # Reward is MSE betweeen predicted and actual features\n",
    "        reward = (pred_phi_next - phi_next).pow(2).mean(dim=1)\n",
    "        return reward\n",
    "\n",
    "class RND(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Random Network Distillation (Burda et al., 2018)\n",
    "    Innovation: Reward = Error in predicting output of a fixed random network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        # Target Network (Fixed, Random weights)\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False # Fixed\n",
    "            \n",
    "        # Predictor Network (Trained to match Target)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "        \n",
    "    def intrinsic_reward(self, state):\n",
    "        target_out = self.target(state)\n",
    "        pred_out = self.predictor(state)\n",
    "        \n",
    "        # Reward = Prediction Error\n",
    "        # Novel states -> High error -> High Reward\n",
    "        reward = (target_out - pred_out).pow(2).mean(dim=1)\n",
    "        return reward\n",
    "\n",
    "class ILDE(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper: Beyond-Expert Performance... (2025)\n",
    "    Innovation: Double Exploration (Optimistic + Curiosity)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Combines an exploration bonus (like RND) with an imitation objective\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}