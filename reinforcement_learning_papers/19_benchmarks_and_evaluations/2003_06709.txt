FACMAC: Factored Multi-Agent Centralised
Policy Gradients

arXiv:2003.06709v5 [cs.LG] 7 May 2021

Bei Peng∗†

Tabish Rashid∗†

Pierre-Alexandre Kamienny‡

Christian A. Schroeder de Witt∗†

Philip H. S. Torr†

Wendelin Böhmer§

Shimon Whiteson†

Abstract
We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new
method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Like MADDPG, a popular multi-agent actor-critic method,
our approach uses deep deterministic policy gradients to learn policies. However,
FACMAC learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as
in QMIX, a popular multi-agent Q-learning algorithm. However, unlike QMIX,
there are no inherent constraints on factoring the critic. We thus also employ a
nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with
monolithic, or monotonically factored critics. In addition, FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space,
rather than optimising over each agent’s action space separately as in MADDPG.
This allows for more coordinated policy changes and fully reaps the benefits of a
centralised critic. We evaluate FACMAC on variants of the multi-agent particle
environments, a novel multi-agent MuJoCo benchmark, and a challenging set of
StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC’s
superior performance over MADDPG and other baselines on all three domains.

1

Introduction

Significant progress has been made in cooperative multi-agent reinforcement learning (MARL) under
the paradigm of centralised training with decentralised execution (CTDE) [26, 16] in recent years,
both in value-based [38, 30, 35, 31, 41, 29] and actor-critic [21, 7, 11, 6] approaches. Most popular
multi-agent actor-critic methods such as COMA [7] and MADDPG [21] learn a centralised critic
with decentralised actors. The critic is centralised to make use of all available information (i.e., it
can condition on the global state and the joint action) to estimate the joint action-value function
Qtot , unlike a decentralised critic that estimates the local action-value function Qa based only on
individual observations and actions for each agent a.5 Even though the joint action-value function
∗

Equal contribution. Correspondence to: Bei Peng <bei.peng@cs.ox.ac.uk>
University of Oxford
‡
Facebook AI Research
§
Delft University of Technology
5
COMA learns a single centralised critic for all cooperative agents due to parameter sharing. For each
agent the critic has different inputs and can thus output different values for the same state and joint action.
In MADDPG, each agents learns its own centralised critic, as it is designed for general multi-agent learning
problems, including cooperative, competitive, and mixed settings.
†

Preprint. Under review.

these actor-critic methods can represent is not restricted, in practice they significantly underperform
value-based methods like QMIX [30] on the challenging StarCraft Multi-Agent Challenge (SMAC)
[34] benchmark [32, 31].
In this paper, we propose a novel approach called FACtored Multi-Agent Centralised policy gradients
(FACMAC), which works for both discrete and continuous cooperative multi-agent tasks. Like
MADDPG, our approach uses deep deterministic policy gradients [18] to learn decentralised policies.
However, FACMAC learns a single centralised but factored critic, which factors the joint action-value
function Qtot into per-agent utilities Qa that are combined via a non-linear monotonic function, as
in the popular Q-learning algorithm QMIX [30]. While the critic used in COMA and MADDPG is
also centralised, it is monolithic rather than factored.6 Compared to learning a monolithic critic, our
factored critic can potentially scale better to tasks with a larger number of agents and/or actions. In
addition, in contrast to other value-based approaches such as QMIX, there are no inherent constraints
on factoring the critic. This allows us to employ rich value factorisations, including nonmonotonic
ones, that value-based methods cannot directly use without forfeiting decentralisability or introducing
other significant algorithmic changes. We thus also employ a nonmonotonic factorisation and
empirically demonstrate that its increased representational capacity allows it to solve some tasks that
cannot be solved with monolithic, or monotonically factored critics.
In MADDPG, a separate policy gradient is derived for each agent individually, which optimises
its policy assuming all other agents’ actions are fixed. This could cause the agents to converge to
sub-optimal policies in which no single agent wishes to change its action unilaterally. In FACMAC,
we use a new centralised gradient estimator that optimises over the entire joint action space, rather
than optimising over each agent’s action space separately as in MADDPG. The agents’ policies are
thus trained as a single joint-action policy, which can enable learning of more coordinated behaviour,
as well as the ability to escape sub-optimal solutions. The centralised gradient estimator fully reaps
the benefits of learning a centralised critic, by not implicitly marginalising over the actions of the
other agents in the policy-gradient update. The gradient estimator used in MADDPG is also known to
be vulnerable to relative overgeneralisation [44]. To overcome this issue, in our centralised gradient
estimator, we sample all actions from all agents’ current policies when evaluating the joint actionvalue function. We empirically show that MADDPG can quickly get stuck in local optima in a simple
continuous matrix game, whereas our centralised gradient estimator finds the optimal policy. While
Lyu et al. [22] recently show that merely using a centralised critic (with per-agent gradients that
optimise over each agent’s actions separately) does not necessarily lead to better coordination, our
centralised gradient estimator re-establishes the value of using centralised critics.
Most recent works on continuous MARL focus on evaluating their algorithms on the multi-agent
particle environments [21], which feature a simple two-dimensional world with some basic simulated
physics. To demonstrate FACMAC’s scalability to more complex continuous domains and to stimulate
more progress in continuous MARL, we introduce Multi-Agent MuJoCo (MAMuJoCo), a new,
comprehensive benchmark suite that allows the study of decentralised continuous control. Based
on the popular single-agent MuJoCo benchmark [4], MAMuJoCo features a wide variety of novel
robotic control tasks in which multiple agents within a single robot have to solve a task cooperatively.
We evaluate FACMAC on variants of the multi-agent particle environments [21] and our novel
MAMuJoCo benchmark, which both feature continuous action spaces, and the challenging SMAC
benchmark [34], which features discrete action spaces. Empirical results demonstrate FACMAC’s
superior performance over MADDPG and other baselines on all three domains. In particular,
FACMAC scales better when the number of agents (and/or actions) and the complexity of the task
increases. Results on SMAC show that FACMAC significantly outperforms stochastic DOP [43],
which recently claimed to be the first multi-agent actor-critic method to outperform state-of-the-art
valued-based methods on SMAC, in all scenarios we tested. Moreover, our ablations and additional
experiments demonstrate the advantages of both factoring the critic and using our centralised gradient
estimator. We show that, compared to learning a monolithic critic, learning a factored critic can: 1)
better take advantage of the centralised gradient estimator to optimise the agent policies when the
number of agents and/or actions is large, and 2) leverage a nonmonotonic factorisation to solve tasks
that cannot be solved with monolithic or monotonically factored critics.
6
We use “centralised and monolithic critic” and “monolithic critic” interchangeably to refer to the centralised
critic used in COMA and MADDPG, and “centralised but factored critic” and “factored critic” interchangeably
to refer to the critic used in our approach.

2

2

Background

We consider a fully cooperative multi-agent task in which a team of agents interacts with the same
environment to achieve some common goal. It can be modeled as a decentralised partially observable
Markov decision process (Dec-POMDP) [27] consisting of a tuple G = hN , S, U, P, r, Ω, O, γi. Here
N ≡ {1, . . . , n} denotes the finite set of agents and s ∈ S describes the true state of the environment.
At each time step, each agent a ∈ N selects a discrete or continuous action ua ∈ U , forming a joint
action u ∈ U ≡ U n . This results in a transition to the next state s0 according to the state transition
function P (s0 |s, u) : S × U × S → [0, 1] and a team reward r(s, u). γ ∈ [0, 1) is a discount factor.
Due to the partial observability, each agent a ∈ N draws an individual partial observation oa ∈ Ω
from the observation kernel O(s, a). Each agent learns a stochastic policy πa (ua |τa ) or a deterministic
policy µa (τa ), conditioned only on its local action-observation history τa ∈ T ≡ (Ω × U )∗ . The joint
stochastic policy π induces a joint action-value function: Qπ (st , ut ) = Est+1:∞ ,ut+1:∞ [Rt |st , ut ],
P∞
where Rt = i=0 γ i rt+i is the discounted return. Similarly, the joint deterministic policy µ induces
a joint action-value function denoted Qµ (st , ut ). We adopt the centralised training with decentralised
execution (CTDE) paradigm [26, 16], where policy training can exploit extra global information that
might be available and has the freedom to share information between agents during training. However,
during execution, each agent must act with only access to its own action-observation history.
VDN and QMIX. VDN [38] and QMIX [30] are Q-learning algorithms for cooperative MARL
tasks with discrete actions. They both aim to efficiently learn a centralised but factored action-value
function Qπ
tot , using CTDE. To ensure consistency between the centralised and decentralised policies,
VDN and QMIX factor Qπ
specifically,
tot assuming additivity and monotonicity, respectively.
Pn More
π
πa
VDN factors Qπ
tot into a sum of the per-agent utilities: Qtot (τ , u; φ) =
a=1 Qa (τa , ua ; φa ).
QMIX, however, represents Qπ
tot as a continuous monotonic mixingfunction of each agent’s utilities:
∂f
π
Qtot (τ , u, s; φ, ψ) = fψ s, Qπ1 1 (τ1 , u1 ; φ1 ), . . . , Qπnn (τn , un ; φn ) , where ∂Qψa ≥ 0, ∀a ∈ N . This
is sufficient to ensure that the global arg max performed on Qπ
tot yields the same result as a set
of individual arg max performed on each Qπa a . Here fψ is approximated by a monotonic mixing
network, parameterised by ψ. Monotonicity can be guaranteed by non-negative mixing weights.
These weights are generated by separate hypernetworks [10], which condition on the full state s.
QMIX is trained end-to-end to minimise the following loss:
h
2 i
L(φ, ψ) = ED y tot − Qπ
,
(1)
tot (τ , u, s; φ, ψ)
0
0 0
−
−
where the bootstrapping target y tot = r + γ maxu0 Qπ
tot (τ , u , s ; φ , ψ ). Here r is the global
−
−
reward, and φ and ψ are parameters of the target Q and mixing network, respectively, as in DQN
[24]. The expectation is estimated with a minibatch of transitions sampled from an experience replay
buffer D [19]. During execution, each agent selects actions greedily with respect to its own Qπa a .

MADDPG. MADDPG [21] is an extension of DDPG [18] to multi-agent settings. It is an actorcritic, off-policy method that uses the paradigm of CTDE to learn deterministic policies in continuous
action spaces. In MADDPG, a separate actor and critic is learned for each agent, such that arbitrary
reward functions can be learned. It is therefore applicable to either cooperative, competitive, or
mixed settings. We assume each agent a has a deterministic policy µa (τa ; θa ), parameterised by θa
(abbreviated as µa ), and let µ = {µa (τa ; θa )}na=1 be the set of all agent policies. In MADDPG, a
centralised and monolithic critic that estimates the joint action-value function Qµ
a (s, u1 , . . . , un ; φa )
is learned for each agent a separately. The critic is said to be centralised as it utilises information
only available to it during the centralised training phase, the global state s7 and the actions of all
agents, u1 , . . . , un , to estimate the joint action-value function Qµ
a , which is parameterised by φa .
This joint action-value function is trained by minimising the following loss:
h
2 i
L(φa ) = ED y a − Qµ
(s,
u
,
.
.
.
,
u
;
φ
)
,
(2)
1
n
a
a
0
0
0
−
− ; φ ). Here ra is the reward received by each
where y a = ra + γQµ
a (s , u1 , . . . , un |u0a =µa (τa ;θa
a
)
agent a, u01 , . . . , u0n is the set of target policies with delayed parameters θa− , and φ−
a are the parameters
of the target critic. The replay buffer D contains the transition tuples (s, s0 , u1 , . . . , un , r1 , . . . , rn ).
7

If the global state s is not available, the centralised and monolithic critic can condition on the joint
observations or action-observation histories.

3

The following policy gradient can be calculated individually to update the policy of each agent a:
h
i
∇θa J(µa ) = ED ∇θa µa (τa )∇ua Qµ
(s,
u
,
.
.
.
,
u
)
,
(3)
1
n
a
u =µ (τ )
a

a

a

where the current agent a’s action ua is sampled from its current policy µa when evaluating the joint
action-value function Qµ
a , while all other agents’ actions are sampled from the replay buffer D.

3

FACMAC

In this section, we propose a new approach called FACtored Multi-Agent Centralised policy gradients
(FACMAC) that uses a centralised but factored critic and a centralised gradient estimator to learn
continuous cooperative tasks. We start by describing the idea of learning a centralised but factored
critic. We then discuss our new centralised gradient estimator and demonstrate its benefit in a simple
continuous matrix game. Finally, we discuss how we adapt our method to discrete cooperative tasks.
3.1

Learning a Centralised but Factored Critic

Learning a centralised and monolithic critic conditioning on the global state and the joint action can
be difficult and/or impractical when the number of agents and/or actions is large [11]. We thus employ
value function factorisation in the multi-agent actor-critic framework to enable scalable learning
of a centralised critic in Dec-POMDPs. Another key advantage of adopting value factorisation in
an actor-critic framework is that, compared to value-based methods, it allows for a more flexible
factorisation as the critic’s design is not constrained. One can employ any type of factorisation,
including nonmonotonic factorisations that value-based methods cannot directly use without forfeiting
decentralisability or introducing other significant algorithmic changes.
Specifically, in FACMAC, all agents share a centralised critic Qµ
tot that is factored as:

µ
µa
Qtot (τ , u, s; φ, ψ) = gψ s, {Qa (τa , ua ; φa )}na=1 ,

(4)

µa
where φ and φa are parameters of the joint action-value function Qµ
tot and agent-wise utilities Qa ,
respectively. In our canonical implementation which we refer to as FACMAC, gψ is a non-linear

monotonic function parametrised as a mixing network with parameters ψ, as in QMIX [30]. To
evaluate the policy, the centralised but factored critic is trained by minimising the following loss:
h
2 i
L(φ, ψ) = ED y tot − Qµ
,
(5)
tot (τ , u, s; φ, ψ)
0
0
−
0
−
−
−
−
where y tot = r + γQµ
tot (τ , µ(τ ; θ ), s ; φ , ψ ). Here D is the replay buffer, and θ , φ , and
−
ψ are parameters of the target actors, critic, and mixing network, respectively.

Leveraging the flexibility of our approach, namely the lack of restrictions on the form of the critic,
we also explore a new nonmonotonic factorisation with full representational capacity. The joint
action-value function Qµ
tot is represented as a non-linear non-monotonic mixing function of per-agent
utilities Qµa a . This nonmonotonic mixing function is parameterised as a mixing network, with a
similar architecture to gψ in FACMAC, but without the constraint of monotonicity enforced by
using non-negative weights. We refer to this method as FACMAC-nonmonotonic. Additionally, to
better understand the advantages of factoring a centralised critic, we also explore two additional
simpler factorisation schemes. These include factoring the centralised critic Qµ
tot into a sum of
per-agent utilities Qµa a as in VDN (FACMAC-vdn), and as a sum of Qµa a and a state-dependent bias
(FACMAC-vdn-s). Our value factorisation technique is general and can be readily applied to any
multi-agent actor-critic algorithms that learn centralised and monolithic critics [21, 7, 6].
3.2

Centralised Policy Gradients

To update the decentralised policy of each agent, a naive adaptation of the deterministic policy
gradient used by MADDPG (shown in (3)) is
h
i
∇θa J(µa ) = ED ∇θa µa (τa )∇ua Qµ
(6)
tot (τ , u1 , . . . , un , s) ua =µa (τa ) .
Compared to the policy gradient used in MADDPG, the updates of all agents’ individual deterministic
policies now depend on the single shared factored critic Qµ
tot , as opposed to learning and utilising a
4

Figure 1: The overall FACMAC architecture. (a) The decentralised policy networks. (b) The
centralised but factored critic. (c) The non-linear monotonic mixing function.
monolithic critic Qµ
a for each agent. However, there are two main problems in both policy gradients.
First, each agent optimises its own policy assuming all other agents’ actions are fixed, which could
cause the agents to converge to sub-optimal policies in which no single agent wishes to change its
action unilaterally. Second, both policy gradients make the corresponding methods vulnerable to
µ
relative overgeneralisation [44] as, when agent a ascends the policy gradient based on Qµ
a or Qtot ,
only its own action ua is sampled from its current policy µa , while all other agents’ actions are
sampled from the replay buffer D. The other agents’ actions thus might be drastically different from
the actions their current policies would choose. This could cause the agents to converge to sub-optimal
actions that appear to be a better choice when considering the effect of potentially arbitrary actions
from the other collaborating agents.
In FACMAC, we use a new centralised gradient estimator that optimises over the entire joint
action space, rather than optimising over each agent’s actions separately as in both (3) and (6), to
achieve better coordination among agents. In addition, to overcome relative overgeneralisation, when
calculating the policy gradient we sample all actions from all agents’ current policies when evaluating
Qµ
tot . Our centralised policy gradient can thus be estimated as
h
i
∇θ J(µ) = ED ∇θ µ∇µ Qµ
(7)
tot (τ , µ1 (τ1 ), . . . , µn (τn ), s) ,
where µ = {µ1 (τ1 ; θ1 ), . . . , µn (τn ; θn )} is the set of all agents’ current policies and all agents share
the same actor network parameterised by θ. However, it is not a requirement of our method for all
agents to share parameters in this manner.
If the critic factorisation is linear, as in FACMAC-vdn, then the centralised gradient is equivalent to
the per-agent gradients that optimise over each agent’s actions separately. This is explored in more
detail by DOP [43], which restricts the factored critic to be linear to exploit this equivalence. A major
benefit of our method then, is that it does not place any such restrictions on the critic. As remarked by
Lyu et al. [22], merely using a centralised critic with per-agent gradients does not necessarily lead to
better coordination between agents due to the two problems outlined above. Our centralised gradient
estimator, which now optimises over the entire joint action space, is required in order to fully take
advantage of a centralised critic.
Figure 1 illustrates the overall FACMAC architecture. For each agent a, there is one policy network
that decides which individual action (discrete or continuous) to take. There is also one critic network
for each agent a that estimates the individual agent utilities Qa , which are then combined into the
joint action-value function Qtot via a non-linear monotonic mixing function as in QMIX. Qtot is
then used by our centralised gradient estimator to help the actor update its policy parameters.
To show the benefits of our new centralised gradient estimator, we compare MADDPG with the
centralised policy gradient (CPG) against the original MADDPG on a simple continuous cooperative
matrix game. Figure 2 (left) illustrates the continuous matrix game with two agents. There is a narrow
path (shown in red) starting from the origin (0, 0) to (1, 1), in which the reward gradually increases.
Everywhere else there is a small punishment moving away from the origin, increasing in magnitude
further from the origin. Experimental results are shown in Figure 2 (right). MADDPG quickly gets
stuck in the local optimum within 200k timesteps, while MADDPG (with CPG) robustly converges
5

Figure 2: Left: The Continuous Matrix Game. Right: Mean test return on Continuous Matrix Game.

Figure 3: Left: Per-agent policy gradient at the origin. For agent 1 (similarly for agent 2) it is 0 since
the gradient term assumes the other agent’s action to be fixed and thus it only considers the relative
improvements along the dotted line. Right: Our Centralised Policy Gradient correctly determines the
gradient for improving the joint action.
to the optimal policy. Figure 3 visualises the differences between the per-agent and centralised
policy gradients, demonstrating that the centralised policy gradient is necessary to take advantage of
the centralised critic. In Section 5, we further demonstrate the benefits of this centralised gradient
estimator in more complex tasks.
3.3

Discrete Policy Learning

As FACMAC requires differentiable policies and the sampling process of discrete actions from a
categorical distribution is not differentiable, we use the Gumbel-Softmax estimator [12] to enable
efficient learning of FACMAC on cooperative tasks with discrete actions. The Gumbel-Softmax estimator is a continuous distribution that approximates discrete samples from a categorical distribution
to produce differentiable samples. It is a differentiable relaxation of the Gumbel-Max trick, which
reparameterises the stochastic policies as a deterministic function of the policy parameters and some
independent noise sampled from a standard Gumbel distribution.
Moreover, we use the Straight-Through Gumbel-Softmax Estimator [12] to ensure the action
dynamics during training and evaluation are the same. Specifically, during training, we sample discrete actions ua from the original categorical distribution in the forward pass, but use
the continuous Gumbel-Softmax sample xa in the backward pass to approximate the gradients:
∇θa ua ≈ ∇θahxa . We can then update the agent’s
policy using our centralised policy gradient:
i
x
∇θ J(θ) ≈ ED ∇θ x∇x Qtot (τ , x1 , . . . , xn , s) , where x = {x1 , . . . , xn } is the set of continuous
sample that approximates the discrete agent actions.

4

Multi-Agent MuJoCo

The evaluation of continuous MARL algorithms has recently been largely limited to the simple
multi-agent particle environments [21]. We believe the lack of diverse continuous benchmarks is
one factor limiting progress in continuous MARL. To demonstrate FACMAC’s scalability to more
complex continuous domains and to stimulate more progress in continuous MARL, we develop
Multi-Agent MuJoCo (MAMuJoCo), a novel benchmark for continuous cooperative multi-agent
robotic control. Starting from the popular fully observable single-agent robotic MuJoCo [39] control
suite included with OpenAI Gym [4], we create a wide variety of novel scenarios in which multiple
agents within a single robot have to solve a task cooperatively.
6

Figure 4: Agent partitionings for MAMuJoCo environments: A) Manyagent Swimmer, B) 3Agent Hopper [3x1], C) 2-Agent HalfCheetah [2x3], D) 6-Agent HalfCheetah [6x1], E) 2-Agent
Humanoid and 2-Agent HumanoidStandup (each [1x9,1x8]), F) 2-Agent Walker [2X3], G) 2-Agent
Reacher [2x1], H) 2-Agent Ant [2x4], I) 2-Agent Ant Diag [2x4], J) 4-Agent Ant [4x2], and K)
Manyagent Ant. Colours indicate agent partitionings. Each joint corresponds to a single controllable
motor. Split partitions indicate shared body segments. Square brackets indicate [(number of agents) x
(joints per agent)]. Joint IDs are in order of definition in the corresponding OpenAI Gym XML asset
files [4]. Global joints indicate degrees of freedom of the center of mass of the composite robotic
agent.

Single-robot multi-agent tasks in MAMuJoCo arise by first representing a given single robotic agent
as a body graph, where vertices (joints) are connected by adjacent edges (body segments). We then
partition the body graph into disjoint sub-graphs, one for each agent, each of which contains one or
more joints that can be controlled. Figure 4 shows agent partitionings for MAMuJoCo environments.
Multiple agents are introduced within a single robot as partial observability arises through latency,
bandwidth, and noisy sensors in a single robot. Even if communication is free and instant when it
works, we want policies that keep working even when communication channels within the robot
malfunction. Without access to the exact full state, local decision rules become more important
and introducing autonomous agents at individual decision points (e.g., each physical component
of the robot) is reasonable and beneficial. This also makes it more robust to single-point failures
(e.g., broken sensors) and more adaptive and flexible as new independent decision points (thus
agents) may be added easily. This design also offers important benefits. It facilitates comparisons to
existing literature on both the fully observable single-agent domain [28], as well as settings with lowbandwidth communication [42]. More importantly, it allows for the study of novel MARL algorithms
for decentralised coordination in isolation (scenarios with multiple robots may add confounding
factors such as spatial exploration), which is currently a gap in the research literature.
MAMuJoCo also includes scenarios with a larger and more flexible number of agents, which takes
inspiration from modular robotics [47, 17]. Compared to traditional robots, modular robots are more
versatile, configurable, and scalable as it is easier to replace or add modules to change the degrees
of freedom. We therefore develop two scenarios named ManyAgent Swimmer and ManyAgent
Ant, in which one can configure an arbitrarily large number of agents (within the memory limits),
each controlling a consecutive segment of arbitrary length. This design is similar to many practical
modular snake robots [45, 25], which mimic snake-like motion for diverse tasks such as navigating
rough terrains and urban search and rescue. See Appendix A for more details about MAMuJoCo.

5

Experimental Results

In this section we present our experimental results on our cooperative variants of the continuous
simple tag environment introduced by Lowe et al. [21] (we refer to this environment as Continuous
7

Predator-Prey), our novel continuous benchmark MAMuJoCo, and the challenging SMAC8 [34]
benchmark with discrete action spaces. In discrete cooperative tasks, we compare with state-of-theart multi-agent actor-critic algorithms MADDPG [21], COMA [7], CentralV [7], DOP [43], and
value-based methods QMIX [30] and QPLEX [41]. In continuous cooperative tasks, we compare
with MADDPG [21] and independent DDPG (IDDPG), as well as COVDN and COMIX, two novel
baselines described below. We also explore different forms of critic factorisation. More details about
the environments, experimental setup, and training details are included in Appendix C and D.
COVDN and COMIX We find that not many multi-agent value-based methods work off the
shelf with continuous actions. To compare FACMAC against value-based approaches in continuous
cooperative tasks, we use existing continuous Q-learning approaches in single-agent settings to extend
VDN and QMIX to continuous action spaces. Specifically, we introduce COVDN and COMIX, which
use VDN-style and QMIX-style factorisation respectively and both perform approximate greedy
action selection using the cross-entropy method (CEM) [5]. CEM is a sampling-based derivative-free
heuristic search method that has been successfully used to find approximate maxima of nonconvex
Q-networks in single-agent robotic control tasks [13]. The centralised but factored Qtot allows us to
use CEM to sample actions for each agent independently and to use the per-agent utility Qa to guide
the selection of maximal actions.
In both COVDN and COMIX, CEM is used by each agent a to find an action that approximately
optimises its local utility function Qa . Specifically, CEM iteratively draws a batch of N random
samples from a candidate distribution Dk , e.g., a Gaussian, at each iteration k. The best M < N
samples (with the highest utility values) are then used to fit a new Gaussian distribution Dk+1 , and
this process repeats K times. We use a CEM hyperparameter configuration similar to Qt-Opt [13],
where N = 64, M = 6, and K = 2.9 Gaussian distributions are initialised with mean µ = 0 and
standard deviation σ = 1. Algorithm 1 and 2 in Appendix B outline the full process for CEM and
COMIX, respectively. We do not consider COVDN and COMIX significant algorithmic contributions
but instead merely baseline algorithms.
FACMAC outperforms MADDPG and other baselines in both discrete and continuous action
tasks. Figure 5 and 6 illustrate the mean episode return attained by different methods on Continuous
Predator-Prey with varying number of agents and different MAMuJoCo tasks, respectively. We can
see that FACMAC significantly outperforms MADDPG on all these continuous cooperative tasks,
both in terms of absolute performance and learning speed. On discrete SMAC tasks, Figure 7 shows
that FACMAC performs significantly better than MADDPG on 4 out of 6 maps we tested, and
achieves similar performance to MADDPG on the other 2 maps. Additionally, on all 6 SMAC maps,
FACMAC significantly outperforms all multi-agent actor-critic baselines (COMA, CentralV, and
DOP), while DOP is recently claimed to be the first multi-agent actor-critic method that outperforms
state-of-the-art valued-based methods on SMAC. FACMAC is also competitive with state-of-theart value-based methods (QMIX and QPLEX), with significantly better performance on MMM,
bane_vs_bane, MMM2, and 27m_vs_30m. These results demonstrate the benefits of our method for
improving performance in challenging cooperative tasks with discrete and continuous action spaces.
In Continuous Predator-Prey, FACMAC-vdn scales better than FACMAC when the number of agents
increases. However, on MAMuJoCo, FACMAC-vdn performs drastically worse than FACMAC in
2-Agent Humanoid and ManyAgent Swimmer (with 10 agents), demonstrating the necessity of the
non-linear mixing of agent utilities and conditioning on the central state information in order to
achieve competitive performance in such tasks. Furthermore, on SMAC, Figure 13 in Appendix
E shows that FACMAC is noticeably more stable than FACMAC-vdn and FACMAC-vdn-s across
different maps, and achieves significantly better performance on the super hard map MMM2.
Interestingly, we find that FACMAC performs similarly to COMIX on both Continuous Predator-Prey
and MAMuJoCo tasks. As FACMAC and COMIX use the same value factorisation as in QMIX
and are both off-policy, this suggests that, in these continuous cooperative tasks, factorisation of the
joint Q-value function plays a greater role in performance than the underlying algorithmic choices.
On SMAC, however, FACMAC performs significantly better than QMIX on MMM, bane_vs_bane,
MMM2, and 27m_vs_30m. For instance, on bane_vs_bane, a task with 24 agents, while QMIX
8

We utilise SC2.4.10., which is used by the latest PyMARL framework. The original results reported in
Samvelyan et al. [34] and Rashid et al. [32] use SC2.4.6. Performance is not always comparable across versions.
9
We empirically find 2 iterations to suffice.

8

(a) 3 agents and 1 prey

(b) 6 agents and 2 preys

(c) 9 agents and 3 preys

Figure 5: Mean episode return on Continuous Predator-Prey with different number of agents and
preys. The mean across 5 seeds is plotted and the 95% confidence interval is shown shaded.

Figure 6: Mean episode return on different MAMuJoCo tasks. In ManyAgent Swimmer, we configure
the number of agents to be 10, each controlling a consecutive segment of length 2. The mean across
7 seeds is plotted and the 95% confidence interval is shown shaded.
struggles to find the optimal policy with 2 million timesteps, FACMAC, with exactly the same value
factorisation, can quickly recover the optimal policy and achieve 100% test win rate. This shows the
convergence advantages of policy gradient methods in this type of multi-agent settings [40].
FACMAC scales better as the number of agents (and/or actions) and the complexity of the task
increases. As shown in Figure 5(b) and 5(c), MADDPG performs poorly if we increase the number
of agents in Continuous Predator-Prey, while both FACMAC and FACMAC-vdn achieve significantly
better performance. The monolithic critic in MADDPG simply concatenates all agents’ observations
into a single input vector, which can be quite large when there are many agents and/or entities and
make it more difficult to learn a good critic. Factoring the critic enables scalable critic learning,
by combining individual agent utilities that condition on much smaller observations into a joint
action-value function. Upon inspection of the learned policies, we find that, on both Continuous
Predator-Prey tasks with 6 agents and 9 agents, FACMAC agents learn to chase after the preys, while
MADDPG agents quickly get stuck in sub-optimal coordination patterns – learning to go after each
other most of the time. On MAMuJoCo (shown in Figure 6), similarly, the largest performance gap
between FACMAC and MADDPG can be seen on ManyAgent Swimmer (with 10 agents), a task
with the largest number of agents among three MAMuJoCo tasks tested.
On SMAC (shown in Figure 7), the largest performance gap between FACMAC and MADDPG
can be seen on the challenging MMM2 and 27m_vs_30m with a large number of agents, which are
classified as 2 super hard SMAC maps due to current methods’ poor performance [34]. We can see
that FACMAC is able to learn to consistently defeat the enemy, whereas MADDPG fails to learn
anything useful in both tasks. The second largest performance gap between FACMAC and MADDPG
can be seen on the hard map 2c_vs_64zg, where MADDPG not only performs significantly worse but
also exhibits significantly more variance than FACMAC across seeds. While there are only 2 agents
in this scenario, the number of actions each agent can choose is the largest among all 6 maps tested as
there are 64 enemies. These results further demonstrate that FACMAC scales better when the number
of agents (and/or actions) and the complexity of the tasks increases.
Factoring the critic can better take advantage of our centralised gradient estimator to optimise
the agent policies when the number of agents and/or actions is large. We conduct ablation
experiments to investigate the influence of factoring the critic and using the centralised gradient
9

(a) Easy

(b) Easy

(c) Hard

(d) Hard

(e) Super Hard

(f) Super Hard

Figure 7: Median test win % on six different SMAC maps: (a) 2s3z (easy), (b) MMM (easy), (c)
2c_vs_64zg (hard), (d) bane_vs_bane (hard), (e) MMM2 (super hard), and (f) 27m_vs_30m (super
hard). The performance of the heuristic-based algorithm is shown as a dashed line.

Figure 8: Ablations for different FACMAC components on two SMAC maps (2c_vs_64zg and
MMM2) and two MAMuJoCo tasks (ManyAgent Swimmer with 2 and 10 agents).

estimator in our method. FACMAC (without CPG) is our method without the centralised policy
gradient. It uses a naive adaptation of the deterministic policy gradient used in MADDPG (shown in
(6)). Thus, the only difference between FACMAC (without CPG) and MADDPG is the previous one
learns a non-linearly factored critic while the latter one learns a monolithic critic. We also evaluate
MADDPG with our centralised policy gradient and refer to it as MADDPG (with CPG).
Figure 8 shows the results of these ablation experiments on SMAC and MAMuJoCo. We can see
that FACMAC (without CPG) performs significantly better than MADDPG on both SMAC maps
tested, both in terms of absolute performance and learning speed, demonstrating the advantages
of factoring the critic in challenging coordination problems. With the centralised policy gradient,
MADDPG (with CPG) performs significantly better than MADDPG on 2c_vs_64zg. However, on the
harder map MMM2, MADDPG with both policy gradients fail to learn anything useful. By contrast,
FACMAC significantly outperforms FACMAC (without CPG) on MMM2, and has lower variance
10

Figure 9: Mean episode return on (Left) Continuous Matrix Game and (Right) a variant of our
Continuous Predator-Prey task (with 3 agents and 1 prey) with nonmonotonic value functions.
across seeds on 2c_vs_64zg. Furthermore, on ManyAgent Swimmer with 2 agents, our centralised
gradient estimator does not affect the performance of both MADDPG and FACMAC. However, when
the number of agents is increased to be 10 in the same task, using the centralised gradient estimator
can significantly improve the learning performance when learning a centralised but factored critic.
These results demonstrate that factoring the critic can better take advantage of our centralised gradient
estimator to optimise the agent policies when the number of agents and/or actions is large.
Nonmonotonically factored critics can solve tasks that cannot be solved with monolithic or
monotonically factored critics. In our multi-agent actor-critic framework, there are no inherent
constraints on factoring the critic, we thus also employ a nonmonotonic factorisation and refer
to it as FACMAC-nonmonotonic (as discussed in Section 3.1). As shown in Figure 9 (left), on
continuous matrix game (as discussed in Section 3.2), FACMAC-nonmonotonic can robustly learn
the optimal policy, while both FACMAC and MADDPG converge to some sub-optimal policy within
200k timesteps. On a variant of Continuous Predator-Prey with nonmonotonic value functions (see
Appendix C.1 for details about this task), Figure 9 (right) shows that both FACMAC and MADDPG
fail to learn anything useful, while FACMAC-nonmonotonic successfully learns to capture the prey.
These results demonstrate that nonmonotonically factored critics can solve tasks that cannot be solved
with monolithic or monotonically factored critics.
It is important to note that the relative performance of FACMAC and FACMAC-nonmonotonic is
task dependent. On the original Continuous Predator-Prey task (with 3 agents and 1 prey), FACMACnonmonotonic yields similar performance to FACMAC (see Figure 12 in Appendix E). On SMAC
(see Figure 13 in Appendix E), FACMAC-nonmonotonic performs similarly to FACMAC on easy
maps, but exhibits significantly worse performance on harder maps. This shows that, in this type of
tasks, using an unconstrained factored critic could lead to an increase in learning difficulty.

6

Related Work

Value function factorisation [15] has been widely employed in value-based MARL algorithms. VDN
[38] and QMIX [30] factor the joint action-value function into per-agent utilities that are combined via
a simple summation or a monotonic mixing function respectively, to ensure consistency between the
arg max of the centralised joint-action value function and the arg max of the decentralised polices.
This monotonicity constraint, however, prevents them from representing joint action-value functions
that are characterised as nonmonotonic [23], i.e., an agent’s ordering over its own actions depends on
other agents’ actions. A large number of recent works [35, 46, 31, 41, 36] thus focus on developing
new value-based MARL algorithms that address this representational limitation, in order to learn a
richer class of action-value functions.
QTRAN [35] learns an unrestricted joint action-value function and aims to solve a constrained
optimisation problem in order to decentralise it, but has been shown to scale poorly to more complex
tasks such as SMAC [23]. QPLEX [41] takes advantage of the dueling network architecture to factor
the joint action-value function in a manner that does not restrict the representational capacity, whilst
also remaining easily decentralisable, but can still fail to solve simple tasks with nonmonotonic
value functions [31]. Weighted QMIX [31] introduces a weighting scheme to place more importance
on the better joint actions to learn a richer class of joint action-value functions. QTRAN++ [36]
addresses the gap between the empirical performance and theoretical guarantees of QTRAN. Our
11

multi-agent actor-critic framework with decentralised actors and a centralised but factored critic, by
contrast, provides a more direct and simpler way of coping with nonmonotonic tasks as one can
simply factor the centralised critic in any manner without constraints. Additionally, our framework
can be readily applied to tasks with continuous action spaces, whereas these value-based algorithms
require additional algorithmic changes.
Most state-of-the-art multi-agent actor-critic methods [21, 7, 11, 6] learn a centralised and monolithic
critic conditioning on the global state and the joint action to stabilise learning. Even though the joint
action-value function they can represent is not restricted, in practice they significantly underperform
value-based methods like QMIX on the challenging SMAC benchmark [31, 32]. In contrast, FACMAC
utilises a centralised but factored critic to allow it to scale to the more complex tasks in SMAC, and
follows the centralised policy gradient instead of per-agent policy gradients.
Lyu et al. [22] recently provide some interesting insights about the pros and cons of centralised and
decentralised critics for on-policy actor-critic algorithms. One important issue that they highlight is
that merely utilising a centralised critic does not necessarily lead to the learning of more coordinated
behaviours. This is because the use of a per-agent policy gradient can lead to the agents getting stuck
in sub-optimal solutions in which no one agents wishes to change their policy, as discussed in 3.2.
Our centralised policy gradient resolves this issue by taking full advantage of the centralised training
paradigm to optimise over the joint-action policy, which allows us to reap the benefits of a centralised
critic. Since FACMAC is off-policy, we also benefit immensely from utilising a centralised critic
over a decentralised one since we avoid the issues of non-stationarity when training on older data.
Zhou et al. [48] propose to use a single centralised critic for MADDPG, whose weights are generated
by hypernetworks that condition on the state, similarly to QMIX’s mixing network without the
monotonicity constraints. FACMAC also uses a single centralised critic, but factorises it similarly to
QMIX (not just using the mixing network) which allows for more efficient learning on more complex
tasks. Of existing work, the deterministic decomposed policy gradients (DOP) algorithm proposed
by Wang et al. [43] is perhaps most similar to our own approach. Deterministic DOP is off-policy
and factors the centralised critic as a weighted linear sum of individual agent utilities and a state
bias. It is limited to only considering linearly factored critics, which have limited representational
capacity, whilst we are free to choose any method of factorisation in FACMAC to allow for the
learning of a richer class of action-value functions. While they claim to be the first to introduce the
idea of value function factorisation into the multi-agent actor-critic framework, it is actually first
explored by Bescuca [3], where a monotonically factored critic is learned for COMA [7]. However,
their performance improvement on SMAC is limited since COMA requires on-policy learning and
it is not straightforward to extend COMA to continuous action spaces. Furthermore, both works
only consider monotonically factored critics, whilst we employ a nonmonotonic factorisation and
demonstrate its benefits. We also investigate the benefits of learning a centralised but factored critic
more thoroughly, providing a better understanding about the type of tasks that can benefit more from
a factored critic. Furthermore, both deterministic DOP and LICA [48] use a naive adaptation of the
deterministic policy gradient used by MADDPG and suffer from the same problems as discussed
in Section 3.2, while our centralised policy gradients allow for better coordination across agents in
certain tasks.

7

Conclusion

This paper presented FACMAC, a multi-agent actor-critic method that learns decentralised policies
with a centralised but factored critic, working for both discrete and continuous cooperative tasks. We
showed the advantages of both factoring the critic and using the new centralised gradient estimator in
our approach. We also introduced a novel benchmark suite MAMuJoCo to demonstrate FACMAC’s
scalability to more complex continuous tasks. Our results on three different domains demonstrated
FACMAC’s superior performance over existing MARL algorithms. Future work will explore more
forms of nonmonotonic factorisation to tackle tasks with nonmonotonic value functions.

References
[1] Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing
overestimation bias in multi-agent domains using double centralized critics. arXiv preprint
arXiv:1910.01465, 2019.
12

[2] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pages 146–155. JMLR. org,
2017.
[3] Marilena Bescuca. Factorised critics in deep multi-agent reinforcement learning. In Master
Thesis, University of Oxford, 2019.
[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[5] Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the
cross-entropy method. Annals of operations research, 134(1):19–67, 2005.
[6] Yali Du, Lei Han, Meng Fang, Ji Liu, Tianhong Dai, and Dacheng Tao. LIIR: Learning
individual intrinsic reward in multi-agent reinforcement learning. In Advances in Neural
Information Processing Systems 32, pages 4405–4416, 2019.
[7] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, 2018.
[8] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. In International Conference on Machine Learning, pages
2829–2838, 2016.
[9] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control
using deep reinforcement learning. In International Conference on Autonomous Agents and
Multiagent Systems, pages 66–83. Springer, 2017.
[10] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106,
2016.
[11] Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In
International Conference on Machine Learning, pages 2961–2970, 2019.
[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
arXiv preprint arXiv:1611.01144, 2016.
[13] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,
Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint
arXiv:1806.10293, 2018.
[14] Hiroaki Kitano, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda, Eiichi Osawa, and Hitoshi
Matsubara. Robocup: A challenge problem for ai. AI magazine, 18(1):73–73, 1997.
[15] Daphne Koller and Ronald Parr. Computing factored value functions for policies in structured
mdps. In Proceedings of IJCAI, pages 1332–1339, 1999.
[16] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82–94, 2016.
[17] Haruhisa Kurokawa, Kohji Tomita, Akiya Kamimura, Shigeru Kokaji, Takashi Hasuo, and
Satoshi Murata. Distributed self-reconfiguration of m-tran iii modular robotic system. The
International Journal of Robotics Research, 27(3-4):373–386, 2008.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR, 2016.
[19] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and
teaching. Machine learning, 8(3-4):293–321, 1992.
[20] Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and Thore Graepel.
Emergent coordination through competition. arXiv preprint arXiv:1902.07151, 2019.
13

[21] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In Advances in Neural
Information Processing Systems, pages 6379–6390, 2017.
[22] Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. Contrasting centralized
and decentralized critics in multi-agent reinforcement learning. In Proceedings of the 20th
International Conference on Autonomous Agents and Multi-Agent Systems, 2021.
[23] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multiagent variational exploration. In Advances in Neural Information Processing Systems, pages
7613–7624, 2019.
[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.
[25] Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A Paradiso, and Hiroshi Ishii. Chainform: A linear integrated modular hardware system for shape changing interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, pages 87–96,
2016.
[26] Frans A. Oliehoek, Matthijs T. J. Spaan, and Nikos Vlassis. Optimal and approximate Q-value
functions for decentralized pomdps. JAIR, 32:289–353, 2008.
[27] Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.
[28] OpenAI. openai/baselines, May 2020. original-date: 2017-05-24T01:58:13Z.
[29] Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson. Softmax with
regularization: Better value estimation in multi-agent reinforcement learning. arXiv preprint
arXiv:2103.11883, 2021.
[30] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, pages 4292–4301,
2018.
[31] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation. In Advances in neural information processing systems,
2020.
[32] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob
Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent
reinforcement learning. JMLR, 21:178:1–178:51, 2020.
[33] Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange. Reinforcement learning
for robot soccer. Autonomous Robots, 27(1):55–73, 2009.
[34] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.
[35] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran:
Learning to factorize with transformation for cooperative multi-agent reinforcement learning.
arXiv preprint arXiv:1905.05408, 2019.
[36] Kyunghwan Son, Sungsoo Ahn, Roben Delos Reyes, Jinwoo Shin, and Yung Yi. Qtran++:
Improved value transformation for cooperative multi-agent reinforcement learning. arXiv
preprint arXiv:2006.12010, 2020.
[37] Peter Stone and Richard S. Sutton. Scaling reinforcement learning toward RoboCup soccer. In
Icml, volume 1, pages 537–544. Citeseer, 2001.
14

[38] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores
Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al.
Value-decomposition networks for cooperative multi-agent learning based on team reward. In
AAMAS, pages 2085–2087, 2018.
[39] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
5026–5033, 2012.
[40] Jianhao Wang, Zhizhou Ren, Beining Han, and Chongjie Zhang. Towards understanding linear
value decomposition in cooperative multi-agent q-learning. arXiv preprint arXiv:2006.00587,
2020.
[41] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020.
[42] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. NerveNet: learning structured policy
with graph neural networks. In 6th International Conference on Learning Representations,
ICLR, 2018.
[43] Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Offpolicy multi-agent decomposed policy gradients. In 9th International Conference on Learning
Representations, ICLR, 2021.
[44] Ermo Wei and Sean Luke. Lenient learning in independent-learner stochastic cooperative games.
The Journal of Machine Learning Research, 17(1):2914–2955, 2016.
[45] Cornell Wright, Austin Buchan, Ben Brown, Jason Geist, Michael Schwerin, David Rollinson,
Matthew Tesch, and Howie Choset. Design and architecture of the unified modular snake robot.
In 2012 IEEE International Conference on Robotics and Automation, pages 4347–4354. IEEE,
2012.
[46] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
preprint arXiv:2002.03939, 2020.
[47] Mark Yim, Ying Zhang, and David Duff. Modular robots. IEEE Spectrum, 39(2):30–34, 2002.
[48] Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit
assignment for multi-agent actor-critic. In Advances in neural information processing systems,
2020.

15

A

Multi-Agent MuJoCo

While several MARL benchmarks with continuous action spaces have been released, few are simultaneously diverse, fully cooperative, decentralisable, and admit partial observability. The Multi-Agent
Particle suite [21] features a few decentralisable tasks in a fully observable planar point mass toy
environment. Presumably due to its focus on real-world robotic control, RoboCup Soccer Simulation
[14, 37, 33] does not currently feature an easily configurable software interface for MARL, nor
suitable AI-controlled benchmark opponents. Liu et al. [20] introduce MuJoCo Soccer Environment,
a multi-agent soccer environment with continuous simulated physics that cannot be used in a purely
cooperative setting and does not admit partial observability.
To demonstrate FACMAC’s scalability to more complex continuous domains and to stimulate more
progress in continuous MARL, we develop Multi-Agent MuJoCo (MAMuJoCo), a novel benchmark
for continuous cooperative multi-agent robotic control. Starting from the popular fully observable
single-agent robotic MuJoCo [39] control suite included with OpenAI Gym [4], we create a wide
variety of novel scenarios in which multiple agents within a single robot have to solve a task
cooperatively. Single-robot multi-agent tasks in MAMuJoCo arise by first representing a given
single robotic agent as a body graph, where vertices (joints) are connected by adjacent edges (body
segments), as shown in Figure 4. We then partition the body graph into disjoint sub-graphs, one for
each agent, each of which contains one or more joints that can be controlled. Note that in ManyAgent
Swimmer (see Figure 4A) and ManyAgent Ant (see Figure 4K), the number of agents are not limited
by the given single robotic agent.
Each agent’s action space in MAMuJoCo is given by the joint action space over all motors controllable
by that agent. For example, the agent corresponding to the green partition in 2-Agent HalfCheetah
(Figure 4C) consists of three joints (joint ids 1, 2, and 3) and four adjacent body segments. Each joint
has an action space [−1, 1], so the action space for each agent is a 3-dimensional vector with each
entry in [−1, 1].
For each agent a, observations are constructed in a two-stage process. First, we infer which body
segments and joints are observable by agent a. Each agent can always observe all joints within its
own sub-graph. A configurable parameter k ≥ 0 determines the maximum graph distance to the
agent’s subgraph at which joints are observable (see Figure 10 for an example). Body segments
directly attached to observable joints are themselves observable. The agent observation is then
given by a fixed order concatenation of the representation vector of each observable graph element.
Depending on the environment and configuration, representation vectors may include attributes
such as position, velocity, and external body forces. In addition to joint and body-segment specific
observation categories, agents can also be configured to observe the position and/or velocity attributes
of the robot’s central torso.

1

1

1)
3

2

4

2)

1
2

3

4

3)
3

2

4

Figure 10: Observations by distance for 3-Agent Hopper (as seen from agent 1). Each corresponds to joints and body parts observable at 1) zero graph distance from agent 1, 2) one unit graph
distance from agent 1, and 3) two unit graph distances from agent 1.
Restricting both the observation distance k, as well as limiting the set of observable element categories
imposes partial observability. However, task goals remain unchanged from the single-agent variants,
except that the goals must be reached collaboratively by multiple agents: we simply repurpose the
original single-agent reward signal as a team reward signal. Default team reward is summarised in
Table 1.
16

The most similar existing environments, though not as diverse as MAMuJoCo, are the decomposed
MuJoCo environments Centipede and Snakes [42]. The latter is similar to MAMuJoCo’s 2-Agent
Swimmer. Ackermann et al. [1] evaluate on one environment similar to a configuration of 2-Agent
Ant, but, similarly to Gupta et al. [9], do not consider tasks across different numbers of agents and
MuJoCo scenarios.
Task

Goal

Special observations

Reward function

2-Agent Swimmer

Maximise +ve x-speed.

All agents can observe
velocities of the central torso.

∆x
∆t + 0.0001α

2-Agent Reacher

Fingertip (green) needs
to reach target at random
location (red).

Target is only
visible to green agent.

− kdistance from fingertip to targetk22
+α

2-Agent Ant

Maximise +ve x-speed.

−4
∆x
kexternal contact forcesk22
∆t + 5 · 10

2-Agent Ant Diag

Maximise +ve x-speed.

2-Agent HalfCheetah

Maximise +ve x-speed.

All agents can observe
velocities of the central torso.
All agents can observe
velocities of the central torso.
-

2-Agent Humanoid

Maximise +ve x-speed.

-

2-Agent
HumanoidStandup
3-Agent Hopper

Maximise +ve x-speed.

-

Maximise +ve x-speed.

4-Agent Ant

Maximise +ve x-speed.

All agents can observe
velocities of the central torso.

6-Agent HalfCheetah

Maximise +ve x-speed.

ManyAgent Swimmer

Maximise +ve x-speed.

ManyAgent Ant

Maximise +ve x-speed.

All agents can observe
velocities of the central torso.
All agents can observe
velocities of the central torso.

+0.5α + 1

−4
∆x
kexternal contact forcesk22
∆t + 5 · 10

+0.5α + 1
∆x
∆t + 0.1α
0.25 ∆x
∆t + min(10,
−6
5 · 10
kexternal contact forcesk22 )
y
∆t + min(10,
5 · 10−6 kexternal contact forcesk22 )
∆x
∆t + 0.001α + 1.0
−4
∆x
kexternal contact forcesk22
∆t + 5 · 10
+0.5α + 1
0.25 ∆x
∆t + min(10,
5 · 10−6 kexternal contact forcesk22 )
∆x
∆t + 0.0001α
−4
∆x
kexternal contact forcesk22
∆t + 5 · 10

+0.5α + 1

Table 1: Overview of tasks contained in MAMuJoCo. We define α as an action regularisation term
2
− kuk2 .

B

COVDN and COMIX

Q-learning has shown considerable success in multi-agent settings with discrete action spaces
[38, 30, 35, 31, 41]. However, performing greedy action selection in Q-learning requires evaluating
arg maxu Qtot (τ , u, s), where Qtot is the joint action-value function. In discrete action spaces, this
operation can be performed efficiently through enumeration (unless the action space is extremely
large). In continuous action spaces, however, enumeration is impossible. Hence, existing continuous
Q-learning approaches in single-agent settings either impose constraints on the form of Q-value
to make maximisation easy [8, 2], at the expense of estimation bias, or perform only approximate
greedy action selection [13]. Neither approach scales easily to the large joint action spaces inherent to
multi-agent settings, as 1) the joint action space grows exponentially in the number of agents, and 2)
training Qtot required for greedy action selection becomes impractical when there are many agents.
This highlights the importance of learning a centralised but factored Qtot . To factor large joint action
spaces efficiently in a decentralisable fashion, COVDN represents the joint action-value function
Qtot as a sum of the per-agent utilities Qa as in VDN [38], while COMIX represents Qtot as a
non-linear monotonic combination of Qa as in QMIX [30]. COVDN and COMIX are thus simple
variants of VDN and QMIX, respectively, that scale to continuous action spaces. They both perform
approximate greedy selection of actions ua with respect to utility functions Qa for each agent a
using the cross-entropy method (CEM) [5]. CEM is a sampling-based derivative-free heuristic search
method that has been successfully used to find approximate maxima of nonconvex Q-networks in a
number of single-agent robotic control tasks [13]. The centralised but factored Qtot allows us to use
CEM to sample actions for each agent independently and to use the individual utility function Qa
to guide the selection of maximal actions. Algorithm 1 outlines the full CEM process used in both
COVDN and COMIX. Algorithm 2 outlines the full process for COMIX. Note we do not consider
COVDN and COMIX significant algorithmic contributions but instead merely baseline algorithms.
17

Algorithm 1 For each agent a, we perform nc CEM iterations. Hyper-parameters di ∈ N control
how many actions are sampled at the ith iteration.
for a := 1, a ≤ N do
µa ← 0 ∈ R|Aa |
σa ← 1 ∈ R|Aa |
for i := 1, i ≤ nc do
for j := 1, j ≤ di do
v0 aj ∼ N (µa , σa )
vaj ← tanh(v0 aj )
qaj ← Qa (τa , vaj )
j ←j+1
end for
if i < nc then
U ←

 0
v al | qal ∈ topki (qa1 , . . . , qadi ), ∀l ∈ {1 . . . N }

µa ← sample_mean(U )
σa ← sample_std(U )
else
m ← arg maxj qaj
ua ← vam
end if
i←i+1
end for
a←a+1
end for
return hu1 , . . . , un i

Algorithm 2 Algorithmic description of COMIX. The function CEM is defined in Algorithm 1.
Initialise ReplayBuffer, θ, θ− , φ, φ−
for each training episode e do
s0 , z0 ← EnvInit()
for t := 0 until t = T step 1 do
ut ← CEM(Q1 , . . . , QN , τt1 , . . . , τtN )
hst+1 , zt+1 , rt i ← EnvStep(ut )
ReplayBuffer ← hst , ut , zt , rt , st+1 , zt+1 i
end for
{hsi , ui , zi , ri , s0i , zi0 i}bi=1 ∼ ReplayBuffer
yi ← ri + γ max
Qtot (s0i , zi0 , u0i ; θ −, φ− ), ∀i
u0i

2
b
P
L←
yi − Qtot (si , zi , ui ; θ, φ)
i=1

θ ← θ − α ∇θ L
φ ← φ − α ∇φ L
end for

C

Environment Details

C.1

Continuous Predator-Prey

We consider the mixed simple tag environment (Figure 11) introduced by Lowe et al. [21], which is a
variant of the classic predator-prey game. Three slower cooperating circular agents (red), each with
continuous movement action spaces ua ∈ R2 , must catch a faster circular prey (green) on a randomly
generated two-dimensional toroidal plane with two large landmarks blocking the way.
To obtain a purely cooperative environment, we replace the prey’s policy by a hard-coded heuristic,
that, at any time step, moves the prey to the sampled position with the largest distance to the closest
predator. If one of the cooperative agents collides with the prey, a team reward of +10 is emitted;
otherwise, no reward is given. In the original simple tag environment, each agent can observe the
18

predator 1
predator 2

predator 3

prey

Figure 11: Continuous Predator-Prey. Left: Top-down view of toroidal plane, with predators (red),
prey (green) and obstacles (grey). Right: Illustration of the prey’s avoidance heuristic. Observation
radii of both agents and prey are indicated.

relative positions of the other two agents, the relative position and velocity of the prey, and the relative
positions of the landmarks. This means each agent’s private observation provides an almost complete
representation of the true state of the environment.
To introduce partial observability to the environment, we add an agent view radius, which restricts
the agents from receiving information about other entities (including all landmarks, the other two
agents, and the prey) that are out of range. Specifically, we set the view radius such that the agents
can only observe other agents roughly 60% of the time. We open-source the full set of multi-agent
particle environments with added partial observability. 10
In addition, we implement a variant of our Continuous Predator-Prey task (with 3 agents and 1 prey),
where the reward function is modified to make the task nonmonotonic. Specifically, if one agent
collides with the prey while at least another one being close enough, a team reward of +10 is given.
However, if only one agent collides with the prey without any other agent being close enough, a
negative team reward of −1 is given. Otherwise, no reward is provided.

C.2

Multi-Agent MuJoCo

All MAMuJoCo environments we tested are configured according to its default configuration, where
each agent can observe only positions (not velocities) of its own body parts and at graph distances
greater than zero. In ManyAgent Swimmer, we configure the number of agents to be 10, each
controlling a consecutive segment of length 2. We thus refer to this environment as ManyAgent
Swimmer [10x2]. We set maximum observation distances to k = 0 for all three environments tested,
including 2-Agent Humanoid, 2-Agent HumanoidStandup, and ManyAgent Swimer [10x2]. Default
team reward is used (see Table 1).

C.3

SMAC

SMAC consists of a set of complex StarCraft II micromanagement tasks that are carefully designed
to study decentralised multi-agent control. The tasks in SMAC involve combat between two armies
of units. The first army is controlled by a group of learned allied agents. The second army consists
of enemy units controlled by the built-in heuristic AI. The goal of the allied agents is to defeat the
enemy units in battle, to maximise the win rate. The action space consists of a set of discrete actions:
move in four cardinal directions, attack any selected enemy (available if the enemy is within the
agent’s shooting range), stop, and noop. Hence the number of actions increases as the number of
enemies increases. All experiments on SMAC use the default reward and observation settings of the
SMAC benchmark [34].

10

https://github.com/schroederdewitt/multiagent-particle-envs/.

19

D

Experimental Details

We evaluate the performance of each method using the following procedure: for each run of a
method, we pause training every fixed number of timesteps (2000 timesteps for Continuous PredatorPrey, 4000 timesteps for MAMuJoCo, and 10000 timesteps for SMAC) and run a fixed number of
independent episodes (10 episodes for Continuous Predator-Prey and MAMuJoCo, and 32 episodes
for SMAC) with each agent performing action selection greedily in a decentralised fashion. On both
Continuous Predator-Prey and MAMuJoCo, the mean value of these episode returns are used to
evaluate the performance of the learned policies. On SMAC, we use the median test win rate (i.e.,
the percentage of the 32 episodes where the agents defeat all enemy units within the permitted time
limit) to evaluate the learned policies, as in [34].
D.1

Continuous Predator-Prey

In value-based methods COVDN and COMIX, the architecture of the shared agent network is a
DRQN with a recurrent layer comprised of a GRU with a 64-dimensional hidden state, with a fullyconnected layer before and after. In actor-critic methods FACMAC, FACMAC-vdn, MADDPG, and
IDDPG, the architecture of the shared agent network is also a DRQN with a recurrent layer comprised
of a GRU with a 64-dimensional hidden state, with a fully-connected layer before and after, while
the final output layer is a tanh layer, to bound actions. The shared critic network is a MLP with 2
hidden layers of 64 units and ReLU non-linearities. All agent networks receive the current local
observation and last individual action as input. In MADDPG, the centralised critic receives the global
state and the joint action of all agents as input. The global state consists of the joint observations of
all agents in Continuous Predator-Prey. In other actor-critic methods, there is a shared critic network
that approximates per-agent utilities, which receives each agent’s local observation and individual
action as input.
During training and testing, we restrict each episode to have a length of 25 time steps. Training lasts
for 2 million timesteps. To encourage exploration, we use uncorrelated, mean-zero Gaussian noise
with σ = 0.1 during training (for all 2 million timesteps). We set γ = 0.85 for all experiments. The
replay buffer contains the most recent 106 transitions. We train on a batch size of 1024 after every
timestep. For the soft target network updates we use τ = 0.001. All neural networks (actor and critic)
are trained using Adam optimiser with a learning rate of 0.01. To evaluate the learning performance,
the training is paused after every 2000 timesteps during which 10 independent test episodes are run
with agents performing action selection greedily in a decentralised fashion.
D.2

Multi-Agent MuJoCo

In all value-based methods, the architecture of all agent networks is a MLP with 2 hidden layers
with 400 and 300 units respectively, similar to the setting used in OpenAI Spinning Up.11 All agent
networks use ReLU non-linearities for all hidden layers. In all actor-critic methods, the architecture
of the shared agent network and critic network is also a MLP with 2 hidden layers with 400 and
300 units respectively, while the final output layer of the actor network is a tanh layer, to bound
the actions. In all value-based methods, the agent receives its current local observation as input. In
MADDPG, the centralised critic receives the global state and the joint action of all agents as input.
The global state consists of the full state information returned by the original OpenAI Gym [4]. In
other actor-critic methods, there is a shared critic network that approximates per-agent utilities, which
receives each agent’s local observation and individual action as input.
During training and testing, we restrict each episode to have a length of 1000 time steps. Training
lasts for 2 million or 4 million timesteps. To encourage exploration, we use uncorrelated, mean-zero
Gaussian noise with σ = 0.1 during training. We also use the same trick as in OpenAI Spinning
Up to improve exploration at the start of training. For a fixed number of steps at the beginning (we
set it to be 10000), the agent takes actions which are sampled from a uniform random distribution
over valid actions. After that, it returns to normal Gaussian exploration. We set γ = 0.99 for all
experiments. The replay buffer contains the most recent 106 transitions. We train on a batch size of
100 after every timestep. For the soft target network updates we use τ = 0.001. All neural networks
(actor and critic) are trained using Adam optimiser with a learning rate of 0.001. To evaluate the
11

https://spinningup.openai.com/en/latest/.

20

learning performance, the training is paused after every 4000 timesteps during which 10 independent
test episodes are run with agents performing action selection greedily in a decentralised fashion.
D.3

SMAC

For baseline algorithms DOP [43], COMA [7], CentralV [7], QMIX [32], and QPLEX [41], we
use the the same training setup as provided by their authors where the hyperparameters have been
fine-tuned on the SMAC benchmark.
Most of our training hyperparameters for FACMAC and MADDPG [21] follow [32]. In both methods,
the architecture of the shared actor network is a DRQN with a recurrent layer comprised of a GRU
with a 64-dimensional hidden state, with a fully-connected layer before and after. The shared critic
network is a MLP with 2 hidden layers of 64 units and ReLU non-linearities. Exploration is performed
during training using a scheme similar to COMA [7]. Action probabilities are produced from the final
layer of the actor network, z, via a bounded softmax distribution that lower-bounds the probability
of any given action by /|U |: P (u) = (1 − )softmaxu + /|U |, where |U | is the size of the joint
action space. Throughout the training, we anneal  linearly from 0.5 to 0.05 over 50k timesteps
and keep it constant for the rest of the training. The replay buffer contains the most recent 5000
episodes. We sample batches of 32 episodes uniformly from the replay buffer and train on fully
unrolled episodes. In MADDPG, we use a target network for the actor and critic, respectively. In
FACMAC, we use a target network for the actor, critic, and mixing network, respectively. All target
networks are periodically updated every 200 training steps. All neural networks are trained using
Adam optimiser with learning rate 0.0025 for the actor network and 0.0005 for the critic network.
We set γ = 0.99 for all experiments.
The architecture of the mixing network in FACMAC follows [32]. It consists of a single hidden layer
of 32 units with an ELU non-linearity. The weights of the mixing network are produced by separate
hypernetworks. The hypernetworks consist of a feedforward network with a single hidden layer of
64 units with a ReLU non-linearity. The output of the hypernetwork is passed through an absolute
activation function (to acheive non-negativity) and then resized into a matrix of appropriate size.

E

Additional Results on Different Critic Factorisations

Figure 12: Mean episode return on our Continuous Predator-Prey task (with 3 agents and 1 prey).

21

(a) Easy

(b) Easy

(c) Hard

(d) Hard

(e) Super Hard

(f) Super Hard

Figure 13: Median test win % on six different SMAC maps: (a) 2s3z (easy), (b) MMM (easy), (c)
2c_vs_64zg (hard), (d) bane_vs_bane (hard), (e) MMM2 (super hard), and (f) 27m_vs_30m (super
hard), comparing FACMAC with different forms of critic factorisations.

22

