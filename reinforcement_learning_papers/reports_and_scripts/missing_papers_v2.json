[
  "Actor-Critic in Deep RL (2016\u20132018)",
  "AlphaGo and Successors (2016\u20132018)",
  "Atari Games (Arcade Learning Environment)",
  "Atari and Beyond",
  "Barto, Sutton, & Anderson (1983)",
  "Bayesian RL / Posterior Confidence",
  "Bellman (1957) \u2013 The Principle of Optimality and Dynamic Programming",
  "Bertsekas & Tsitsiklis (1996)",
  "Bertsekas (1982, 1983)",
  "Blackwell (1965) \u2013 Discounted Dynamic Programming",
  "Bootstrapped & Randomized Value Functions",
  "Boyan & Moore (1995)",
  "Bradtke & Barto (1996)",
  "Continuous Control \u2013 MuJoCo and OpenAI Gym",
  "Deep RL from Human Preferences",
  "Definition and Key Differences",
  "Devlin & Kudenko (2011, 2012)",
  "Dota\u00a02 (OpenAI Five, 2019)",
  "FeUdal Networks (Hierarchical RL)",
  "Games of Strategy \u2013 Go, Chess, StarCraft",
  "Geoffrey Gordon (1995)",
  "Grzes & Kudenko (2009); Grze\u015b (2017)",
  "Hauskrecht (2000)",
  "Jaakkola, Singh & Jordan (1995)",
  "Kaelbling, Littman & Cassandra (1998)",
  "Kaelbling, Littman & Cassandra (1998) \u2013 Unified Algorithms for POMDPs",
  "Kaelbling, Littman & Moore (1996)",
  "Kakade & Langford (2002)",
  "Konda & Tsitsiklis (2000)",
  "Lagoudakis & Parr (2003)",
  "Laud & DeJong (2003)",
  "Manne (1960) \u2013 Linking MDPs to Linear Programming",
  "Memory-based Continuous Control (RDPG/RSVG)",
  "Model-Free Reinforcement Learning for Non-Markovian Decision Problems",
  "OpenAI Spinning Up (2018)",
  "Osband & Van Roy (2016)",
  "Peng & Williams (1996)",
  "Prediction Error",
  "Procgen Benchmark",
  "Puterman & Shin (1978)",
  "Radial Basis Functions (RBFs)",
  "Recurrent Policy Gradients",
  "Recurrent Replay Distributed DQN (R2D2)",
  "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach",
  "Rummery & Niranjan (1994)",
  "Separate Networks \u2013 Cons:",
  "Separate Networks \u2013 Pros:",
  "Shared Networks \u2013 Cons:",
  "Shared Networks \u2013 Pros:",
  "Smallwood & Sondik (1973)",
  "Smallwood & Sondik (1973) \u2013 Planning with Partial Observability (POMDPs)",
  "StarCraft\u00a0II (AlphaStar, 2019)",
  "Sutton & Barto (1998, 2018)",
  "Sutton & Barto (2018)",
  "TD-Gammon (1992\u20131995)",
  "Tesauro (1992, 1995)",
  "The Optimal Control of Partially Observable Markov Processes over a Finite Horizon",
  "Tile Coding (CMAC)",
  "Tsitsiklis & Van Roy (1996/1997)",
  "Tsitsiklis & Van Roy (1997)",
  "Upper Confidence Bounds (UCB)",
  "Utile Distinction Memory",
  "Watkins & Dayan (1992)",
  "Watkins & Dayan (1992) \u2013 Q-Learning",
  "Watkins (1989; Watkins & Dayan 1992)"
]