Fully-Decentralized MADDPG with Networked Agents

arXiv:2503.06747v1 [cs.LG] 9 Mar 2025

Diego Bolliger
diegobo@student.ethz.ch

Lorenz Zauter
lzauter@student.ethz.ch

Robert Ziegler
roziegler@student.ethz.ch

Abstract
In this paper, we devise three actor-critic algorithms with decentralized training for
multi-agent reinforcement learning in cooperative, adversarial, and mixed settings
with continuous action spaces. To this goal, we adapt the MADDPG algorithm
by applying a networked communication approach between agents. We introduce
surrogate policies in order to decentralize the training while allowing for local
communication during training. The decentralized algorithms achieve comparable
results to the original MADDPG in empirical tests, while reducing computational
cost. This is more pronounced with larger numbers of agents.

1

Introduction

Multi-agent reinforcement learning (MARL) is of significant practical value and has become a focus
of research in the past years. MARL extends the ideas of Reinforcement Learning (RL) to settings
where multiple agents interact in a common environment. These interactions can be competitive,
where all agents work towards their own goal, cooperative, where all agents work towards the same
goal, or a mixture of the two, e.g. agents working in teams against each other. This includes many
practical examples, such as multi robot control, coordinated flight or control in power distribution
systems.
We are interested in partially observable stochastic games (POSG), where each agent decides independently about its actions, has its own reward function and the goal is to maximize the individual reward
as well as the overall reward. The reward functions are only accessible by the agents themselves
and in the execution phase, there is only local information available. Recall that a classical single
agent RL system such as a Markov decision process assumes that the agent controls all actions and
has access to the entire state. Hence, the POSG setting poses two major challenges: how do agents
account for missing information and how can they learn to cooperate despite being partially unaware
of what the other agents are doing or aiming to do?
One actor-critic approach presented in [2] makes use of a centralized controller which has access
to all information during training, whereas the actions are deterministic and independent during
execution as described above. This is achieved by adapting the deep deterministic policy gradient
(DDPG) algorithm to the multi-agent setting (MADDPG). A different approach is taken in [6] where
both learning and execution is decentralized and information is shared through a communication
network. However, the authors assume that all agents observe the full state.
Centralized training can be unfeasible in settings of large number of agents or when communication
is limited. In this paper, we investigate possibilities of combining the algorithms from [2] and [6] with
the goal of developing fully decentralized MARL algorithms. That is, both training and execution are
decentralized. An additional goal of this paper is to enable the algorithms to work only with local
observations for each agent in training and execution, thus lifting further restrictions imposed by the
previous authors.
Foundations of Reinforcement Learning, FS2024. Department of Computer Science, ETH Zürich.

To this end, we first devise a fully decentralized version of the MADDPG algorithm and in a second
step introduce a communication network through which information can be exchanged during training.
We develop these algorithms by first considering a cooperative setting, where all agents work towards
a common goal. Then afterwards, we adapt them to mixed and adversarial settings. For the evaluation
of all our algorithms, we conduct experiments using the multi-particle environment (MPE). [4]
In concrete terms, our contributions are as follows.
• We develop and evaluate a fully decentralized version of MADDPG.
• We extend the fully decentralized MADDP to share critic parameters through a communication network during training. For this, we develop two different algorithms and evaluate
both.
• Lastly, we modify all algorithms for the adversarial setting.

2

Related Work

In their overview paper [7] from 2021 Zhang et. al. describe a variety of settings. Our work focuses
on decentralized algorithms in the setting of stochastic games, what the authors call ‘networked
algorithms’, because communication during training is decentralized through a (possibly) time
varying network.
The MADDPG algorithm from Lowe et. al. [2] is applicable for both cooperative and competitive
settings. By deploying centralized Q-learning during the training phase, the algorithm is able to handle
non-differentiable environment dynamics while yielding decentralized policies during execution time.
This is achieved by approximating the shared Q function by a neural network that has access to the
policies of all agent and coordinates accordingly. While this algorithm leads to good performance,
it uses a centralized controller with its aforementioned drawbacks. Acknowledging the centralized
nature of their algorithm, the authors propose to mitigate the required access to the policies of all N
agents by complementing each agent with N − 1 approximator networks which learn the policies of
the other agents. This however, although being more decentralized, increases the scalability problem
even further as the number of parameters scales quadratically in N .
In order to obtain scalability to large amounts of agents, Cui et. al. [1] maintain a distribution to model
the mean field of the model. With mean field control (MFC), the agents are able to correlate their
actions during training, being effectively guided by the mean field. This requires rigid assumptions
on the environment. Through a radial basis function kernel, the training is then further centralized,
leading effectively again to an approach that relies on centralized training to achieve decentralized
execution.
Zhang et. al [6] take a different approach to obtain an algorithm that trains in a decentralized manner.
The authors model the agents on a time varying communication network where the agents perform
the actor step individually. In contrast to the other approaches presented so far, their algorithm
maintains the decentralization aspect also during the training process, allowing for communication
only through connected agents. More precisely, agents average their critic parameters with connected
agents (i.e. locally) in a consensus step. While this approach performs well and is one of the few
fully decentralized approaches to MARL, we found in our experiments that the consensus update –
effectively overriding the critics of the agents with the average of their neighbors’ critics – can lead
to instability. Furthermore, Zhang et. al. require the agents to observe the full state space, thus not
allowing for partially observable settings.

2

3

Main Results

3.1

Problem Setting

For our work we are interested in the general setting of partially observable stochastic games.
Definition 1 (POSG) A partially observable stochastic game (POSG) is defined as a tuple,
(D, S, A, P, R, O, pO , δ0 ) ,
where
a) D = {1, . . . , N } = [N ] is the set of N ∈ N agents.
b) S is a finite set of states,
Q
c) A = n∈D Ai is the finite set of joint actions and Ai the set of actions of agent i,
d) P : S × A × S → [0, 1] is the transition probability function,
e) R = {Ri }i∈D is the set of individual reward functions Ri : S × A → R of the agents
i ∈ [N ],
Q
f) O = i∈D Oi is the finite set of joint observations, where Oi is the set of observations
available to agent i,
g) pO is the observation probability function, and
h) δ0 is the initial state distribution at stage t = 0.
During execution, as for the Dec-POMDP setting (see [3]) the agents of our POSG are assumed to
only act based on their individual observations and to not communicate any further. We assume
throughout that action spaces are continuous, i.e. open subsets of some Euclidean space.
To facilitate cooperation between the agents in the case of our networked algorithms (algorithm 3
and 4 in A.1) we allow communication between agents along a communication network Gt . Similar
to [6], we define the networked setting below:
Definition 2 (POSG with networked agents) A partially observable stochastic game with networked agents is a tuple
(D, S, A, P, R, O, pO , δ0 , {Gt }t≥0 ) .
The first part of the tuple is a POSG, amended by a time varying undirected graph
Gt := (D, Et ) with the vertex set D and the set of edges at time t, Et ⊂ {(i, j) ∈ D × D : i ̸= j}.
Communication between two agents i, j ∈ D at time t is only possible if (i, j) ∈ Et .
We note that in principle, agents could communicate through the network during action execution
and training, but we restrict ourselves to communication during training. This is without loss of
generality, since the former can be included as actions and observations if needed.
3.2

Fully-Decentralized MADDPG

To decentralize training of the MADDPG algorithm, see section A.1, we propose that each agent i
keeps its own local replay buffer
B
Di = (oji , aj , rij , o′j
i )j=1 ,

of size B, containing its own observations oi , the true joint actions taken a = (a1 , . . . , aN ), its own
rewards ri and next observations o′i . The local critic Qi is then updated using solely the local replay
buffer of agent i.
A key problem in decentralizing training lies in how to obtain the critic targets yi with only local
observations. MADDPG needs to access the policies and observations of all agents, which is
undesirable in our decentralized setting. The proposed policy approximation in [2] may not need
3

access to policies of other agents, but still requires access to all local observations while also
increasing computational cost.
We propose that instead of approximating the actual policies of the other agents in the optimization
target
µ′
ŷ j = rj + γQi i (x′j , â′1 , . . . , ai , . . . â′N )|â′ =µ̂′k (o′j ) ,
i

k

k

which requires knowledge of all local observation, we let each agent learn surrogate policies which
mimic the behavior of the other agents by only using local observations of the agent in question.
Concretely, we let each agent learn a joint policy

N
µθi (oi ) = µθi (oi ) k=1 ,
using the sampled policy gradient with respect to the surrogate policies


S
X
1
µ
Q i (oji , aj )|aj =[µ (oj )]  .
∇θi J ≈ ∇θi 
θi
i k
k
S j=1 i

(3.1)

Note that the surrogate policies model the other agents as one centrally controlled actor. When
interacting with the environment, each agent then uses the joint surrogate policy to select its action.
Practically speaking, we increase the output dimension of the actor network to the dimension dA
of the joint action space and use this output as a plug-in estimator for the joint action to create
approximations for the target yij for the critic update, i.e.
µ′

′j
h
i
ŷij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

We think of the surrogate policies as a form of imaginated, intrinsic representation of the other agents
which help to learn a good policy as imaginary friends. This reflects intuitively human behaviour
in learning a team task, where it is beneficial to think about the actions of the team members to
anticipate their actions.
Note that the critic still learns with transitions (oi , a, ri , o′i ) from the local replay buffer, where the
true joint actions are used, meaning that we assume that each agent’s critic has access to the true joint
actions. We remark that our algorithm, as is, can be used in the cooperative setting only, as each
agent i imagines the other agents’ policies to maximize their own (local) reward ri .
One drawback of this approach is that all agents train independently which could lead to optimizing
for non-compatible policies. We conclude that there is a trade-off between decentralization of training
and cooperation.
3.3

Fully-Decentralized MADDPG with Networked Agents

The algorithm we propose in section 3.2, in theory, may fail to lead to true cooperative behavior
since each agent might optimize for a different joint policy which might contradict each other. This
is problematic if the agents are supposed to cooperate. In the following, we propose a compromise
between cooperation and decentralization in the form of networked agents, where parameters of the
critic are passed to neighbors in a possibly time varying communication network (definition 2).
We represent the communication network at time t by a right stochastic matrix C t ∈ RN ×N , where
C t (i, j) > 0 if agent i receives information from agent j and C t (i, j) = 0 otherwise. The weights
of the communication matrix C t can be adjusted depending on the setting. If communication is not
equally possible between all agents or depends on other factors (i.e. proximity), this formulation is
able to model these situations as well. Note that the network is exclusively used to pass parameters of
the critic and the policy execution does not depend on the network.
3.3.1

Networked Agents with hard consensus update

One option to structure this communication, is to average the critics of neighboring agents after each
learning step. Thus effectively overriding the local critics by the (weighted) average of itself and its
neighbors. We call this hard consensus update, which is performed after each learning interval. The
idea is inspired by [6].
4

The consensus step is then performed, for each agent i, as
µi ←

N
X

C t (i, j)µj ,

j=1

where µk are the network parameters of the critic of agent k. This means, in the case where C t ≡ I,
we recover fully decentralized learning as in section 3.2. On the other hand, in the case where
C t (i, j) = N1 , ∀i, j, the agents effectively learn fully centralized since the same critic is used by all
agents. This requires the critics to be parameterized in the same way for all agents.
3.3.2

Networked Agents with soft consensus update

As the hard consensus update interferes quite bluntly with the learning of the individual critics, we
propose a more subtle approach that leaves the integration of the communication requirement to the
agents themselves. We call this soft consensus update.
Instead of updating the critics after every learning interval, we change the critic loss to include a
penalty term based on the squared relative error of critic parameters of connected agents:
L(θi ) =

S
N
2
X
1 X j
∥µi − µj ∥2
j
j
,.
yi − Qµ
(o
,
a
)
+
ζ
·
C t (i, j)
i
i
i
S j=1
∥µj ∥2
j=1
|
{z
}
consensus penalty

We include a hyperparameter ζ to balance individual and collective learning. For the norm on the
parameter space, we propose the standard Euclidean/Frobenius norm but other choices are possible
too. For numerical stability, a small constant should be added to the the denominator.
3.4

Extension to Adversarial and Mixed Settings

As described above, the fully-decentralized algorithms can not be applied as is to adversarial and
mixed settings, in contrast to the original MADDPG. This is because, when training the actor
determining the surrogate actions, we maximize for a joint action by the sampled policy gradient (3.1)
that maximizes the critic value which corresponds only to the own reward of the agent. Since we do
not assume access the reward of other agents, another approach must be taken. For this, we let each
agent take the assumption that adversarial agents have as goal to diminish their own reward. The actor
is then updated in two steps. For N agents forming K teams, where team k has size l, consisting of
agents k1 , . . . , kl and having adversaries k 1 , . . . , k N −l , the policy gradient is first sampled as


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

aj =[ajobs ]
k

(3.2)

k

k

h
i
meaning that we plug in the observed adversarial actions ajobs (which we assumed to be able to
k
observe) and only optimize according to the surrogate joint team action by taking a gradient ascent
step.
Then, directly following, we optimize for the surrogate adversarial actions, where the policy gradient
is sampled according to


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

ajk =[ajobs ]

and a gradient descent step is taken.
5

k

k

(3.3)

4

Empirical Results

4.1

Network architecture

In order to keep our results comparable to [2], we implemented the algorithm with simple MLPs
with five hidden layers with 256 hidden nodes. Contrary to earlier suspicions, bigger networks or
networks with a more specialized architecture such as GRUs and LSTMs did not improve results.
4.2

The communication matrix

One of the most versatile parts of our networked algorithms is their adaptability to various communication situations. As comparability is essential, we decided to construct the communication matrix
C t for N agents as follows. For cooperative settings, we choose

η 
1 − η Nη−1
...
N −1
.. 
 η
..
..

.
.
. 
N
−1


Ct =  .

.
.
η 
..
..
 ..
N −1
η
η
...
1−η
N −1
N −1
where η is a communication hyperparameter.
For mixed settings of type 1 vs. N , we use the communication matrix of the form


1
0
...
0
η
η
0 1 − η N −1 . . . N −1 



..
η 
η

.
.
.
.
0
Ct = 
N −1
N −1  .

 ..
.
..
.
.
..
.. 
..
.

.
η
0 N −1
... ... 1 − η
4.3
4.3.1

Cooperative setting
Simple Spread Environment

For training and testing of our algorithms in a cooperative setting, we use the ‘simple spread’
environment of the pettingzoo library [4]. The environment consists of N agents and N landmarks
on a 2D plane. The agents are controlled by the algorithms we are testing and the landmarks are
stationary. The goal of the agents is to cover the landmarks while avoiding collisions. They receive a
global reward based on the minimum distance of the closest agent to each landmark and are penalized
for each collision. Each agent receives the same reward
R=

N
X
i=1

min {∥ℓi − pj ∥} −

j=1,...,N

N
N X
X

1{∥pi −pj ∥<ϵ} ,

i=1 j=i+1

where ℓi is the position of landmark i, and pj is the position of agent j, and ϵ is a collision threshold.
The observations are their relative positions to the landmarks and to the other agents, as well as their
own position and velocity. The actions for each agent are to accelerate continuously in some direction
and thus changing their trajectory.
4.3.2

Simple Spread Experiment

In the first set of experiments, we are interested in the obtained scores of our new algorithms in
comparison with the standard MADDPG algorithm. Figure 1 shows how the evaluation score changes
for all four algorithms when training on the experimental setup described in section 4.3.1. We conduct
five experiments, for N = 2, 3, 4, 5, 10 agents, training them over s = 105 , 3 · 105 , 4 · 104 , 5 · 105
and 3 · 105 steps respectively. The last experiment was limited due to the computational cost.
As we can see in Figure 1 (also see Figure 4 in the appendix), the standard MADDPG outperforms
our algorithms in terms of final score for N = 2, 3 and about the same score is reached for N = 4, 5.
However for N = 4, 5, MADDPG takes longer to converge.
6

For N = 10 agents, MADDPG and the decentralized algorithm with hard consensus update are
unstable, whereas our other algorithms remain stable. However, MADDPG seems to improve towards
the end and a longer training run would have been desirable (it was not possible to repeat the
experiment for us due to the computational cost involved).
Since MADDPG has the most information during training and does not have to rely on guesses about
the other agents behaviors, it is to be expected that the performance of MADDPG is best in the
long run. However, as we see with the training runs with more than three agents, the decentralized
algorithms converge much faster. We believe that this is due to the decentralization which reduces
complexity of the critic by considering less information. This effect is most pronounced in the case
N = 10, where scaling becomes an issue.
The experiments with N = 10 agents were also performed with a sparse matrix which gave similar
results, see B.3.
The fully decentralized algorithm (Algorithm 2 or Section 3.2) performs promisingly. While
converging significantly slower for fewer agents, its score tends towards the score of MADDPG.
However, the networked algorithms are not able to outperform the fully decentralized algorithm.
These train equally fast as the fully decentralized algorithm and especially for N = 5 agents
considerably faster than the (centralized) MADDPG algorithm.
While the algorithms with hard and the soft consensus update seem to perform quite similar, the soft
consensus update is more stable in case of more aggressive updating (compare B.2).
However, we clearly notice diminishing returns when the number of agents increase compared to the
starting scores of effectively random agents. We conclude that the MADDPG algorithm performs
poorly in the simple spread already for a low number of agents. This is also confirmed by qualitative
examination of the evaluation episodes, which indicate that effective policies are only learned for 2
and 3 agents.

2 Agent Evaluation Scores

3 Agent Evaluation Scores

8

15

10

20

Score

Score

12
14
16

20
22

30

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

18

0

20000

40000

60000

Steps

80000

25

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

35

100000

0

(a) Two agents

Steps

(b) Three agents

5 Agent Evaluation Scores

10 Agent Evaluation Scores
50

30
40

75

50

100

60

125

Score

Score

50000 100000 150000 200000 250000 300000

70
80

175

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

90
100
0

100000

200000

300000

Steps

400000

150

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

200
225

500000

0

(c) Five agents

50000 100000 150000 200000 250000 300000

Steps

(d) Ten agents

Figure 1: Comparison of evaluation score averaged over 100 test episodes for the simple spread environment between standard MADDPG, the fully decentralized MADDPG, and the fully decentralized
MADDPG with hard and soft consensus update.
7

4.4

Adversarial and Mixed Setting

4.4.1

Simple Adversary Environment

For testing adversarial and mixed settings, we use the ‘simple adversary’ environment of the
pettingzoo library [4]. There are N good agents, one adversary, and N landmarks from which one is
a target landmark. All agents observe the relative position of landmarks and other agents. In addition,
the good agents (but not the adversary) observe the relative position to the target. The adversary is
rewarded by its distance to the target, and the good agents by their minimum distance. To avoid the
adversary following the others to the target, the good agents need to split up to deceive the adversary.
The rewards are

Rj = − min
∥ℓtarget − pj ∥2 − Radv
j=1,...,N

Radv = − ∥ℓtarget − padv ∥2 ,
where Rj are the rewards of the good agents, pj are the positions of the good agents and ℓtarget is
the position of the target landmark, whereas Radv and padv are the reward and the position of the
adversarial.
4.4.2

Simple Adversary Experiment

In this set of experiments we trained the agents in a 1 vs. 1 and a 1 vs. 2 setting for 105 and 3 · 105
steps respectively. The plots are included in the appendix B.4, as they show little information. Even
after extensive hyper parameter exploration, the agents do not learn effective policies at all. Since
this is a shortcoming of the underlying MADDPG algorithm, we cannot expect our algorithms to
improve on this.

5

Discussion and Outlook

We developed and evaluated three fully decentralized versions of the MADDPG algorithm, which
can be applied to cooperative, adversary, and mixed settings. The first algorithm, 2, uses an approach
of learning surrogate policies to approximate the behavior of the other agents while not relying on
further information than the own observations. The second and third algorithm, 3 and 4 additionally
use a communication network to exchange critic parameters. The hard consensus update averages
the critics of neighboring agents after each learning step, while the soft consensus update includes a
consensus penalty term.
Overall, all three algorithms show promising results that are similar to the performance of the original
MADDPG algorithm. We noticed that fully-decentralized MADDPG with networked agents and
hard consensus update can be unstable if hyperparameters are not carefully chosen. This problem
did not occur with the soft consensus step. In our experiments, all algorithms are able to learn
effective policies whenever the original MADDPG algorithm is able to do so. However, we observed
shortcomings of the original MADDPG algorithm in the cooperative setting for more than three
agents and in the adversarial setting in general.
Even with the original MADDPG algorithm, learning a good policy is difficult and the results
often lack real cooperation. By extensive experimentation, we concluded that neither the network
architecture nor the hyperparameters were at fault. Therefore, in future work, we would like to apply
the decentralization techniques we developed, i.e. learning joint surrogate policies and networked
agents approaches, to other algorithms such as MAPPO, which perform considerably better in MPE
environments [5].

8

References
[1] Kai Cui, Sascha H. Hauck, Christian Fabian, and Heinz Koeppl. Learning decentralized partially
observable mean field control for artificial collective behavior. In The Twelfth International
Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=
99tKiMVJhY.
[2] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural
information processing systems, 30, 2017.
[3] Frans A. Oliehoek. Decentralized POMDPs, pages 471–503. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2012. ISBN 978-3-642-27645-3. doi: 10.1007/978-3-642-27645-3_15. URL
https://doi.org/10.1007/978-3-642-27645-3_15.
[4] J Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan,
Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. Pettingzoo:
Gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems,
34:15032–15043, 2021.
[5] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu.
The surprising effectiveness of ppo in cooperative, multi-agent games, 2022. URL https:
//arxiv.org/abs/2103.01955.
[6] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized
multi-agent reinforcement learning with networked agents. In International Conference on
Machine Learning, pages 5872–5881. PMLR, 2018.
[7] Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms, 2021. URL https://arxiv.org/abs/1911.10635.

9

A

Appendix

A.1

MADDPG

MADDPG as presented in [2] is a version of an actor-critic algorithm for MARL with deterministic
continuous policies for each agent.
Theorem 1 (Policy gradient theorem) Let J(θi ) = Es∼P,a∼πθi [Ri ] the expected reward of each
agent i. Then we can write the gradient of the policy as:
∇θi J(θi ) = Es∼µ,ai ∼πi [∇θi log πθi (ai |oi )∇ai Qπi (x, a1 , . . . , aN )] ,

(A.1)

where x = (o1 , . . . , oN ) are the observations of all agents, Qπi (x, a1 , . . . , aN ) is a centralized actionvalue function, πθi (ai |oi ) is the parameterized policy of agent i utilizing only local observations and
θi are the policy parameters for the agents.
The authors of [2] extend this framework to deterministic policies µθ : S → A. By applying
Theorem 1 to deterministic policies Lowe at. al. find the actor update to be
h
i
∇θi J(µθi ) = Ex,ai ∼D ∇θi µθi (ai |oi )∇ai Qµ
(A.2)
i (x, a1 , . . . , aN )|ai =µθ (oi ) .
i

Further, they update the critics Qµi by minimizing the losses


L(θi ) = Ex,ai ∼D (y − Qµi (x, a1 , . . . , aN ))2 ,
where

(A.3)

′

y = ri + γQµi (x′ , a′1 , . . . , a′N )|a′k =µ′k (ok )

(A.4)

where µ′ = {µ′θ1 , . . . , µ′θN } is the set of target policies with delayed parameters θi′ . Actor, critic
and target functions are all approximated with neural networks and and the gradients are estimated
using samples from a replay buffer (x, a, x′ , a′ ) which stores past interactions between agents and
environment.

10

A.2

Algorithms: Variants of Multi-Agent Deep Deterministic Policy Gradient

Algorithm 1 shows the original algorithm for Multi-Agent Deep Deterministic Policy Gradient
(MADDPG) from [2].
Algorithm 2 is our fully-decentralized variant of the MADDPG algorithm, while algorithms 3 and 4
are the networked approaches with hard and soft consensus update respectively. Algorithms 5 to 7
show our adaptations of the previously discussed algorithms to the mixed and adversary setting.
Algorithm 1 Multi-Agent Deep Deterministic Policy Gradient
for episode = 1 to M do
Initialize a random process N for action exploration
Receive initial state x
for t = 1 to max-episode-length do
for each agent i, select action ai = µθi (oi ) + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward r and next state x’
Store transition (x, a, r, x′ ) in replay buffer D
x ← x′
for agent i = 1 to N do
Sample a random minibatch of S samples {(xj , aj , rj , x’j )} from D
′
′j
′
′
Set y j = rj + γQµ
i (x , a1 , . . . , aN )|a′k =µ′k (ojk )

P
j
j 2
Update critic by minimizing the loss: L(θi ) = S1 j y j − Qµ
i (x , a )
Update the actor policy using the sampled policy gradient:
1X
j
j
j
∇θi Qµ
∇θ i J ≈
i (x , a1 , . . . , ai , . . . aN )|ai =µi (oji )
S j
end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
end for
end for

11

Algorithm 2 Fully-Decentralized MADDPG
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
h
i
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
2
PS 
j
j
L(θi ) = S1 j=1 yij − Qµ
i (oi , a )
Update the actor policy using the sampled policy gradient:

 P
µ
S
∇θi J ≈ ∇θi S1 j=1 Qi i (oji , aj )|aj =[µ (oj )]
θi
i k
k
end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
end if
end for
end for

12

Algorithm 3 Fully-Decentralized MADDPG with Hard Consensus Update
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
h
i
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
2
PS 
j
j
L(θi ) = S1 j=1 yij − Qµ
i (oi , ai )
Update the actor policy using the sampled policy gradient:

 P
µ
S
∇θi J ≈ ∇θi S1 j=1 Qi i (oji , aj )|aj =[µ (oj )]
θi
i k
k
end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
Consensus update:
for agent i = 1 to N do
Update critic parameters:
µi ←

N
X

C t (i, j)µj

j=1

end for
end if
end for
end for

13

Algorithm 4 Fully-Decentralized Networked MADDPG with Soft Consensus Update
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
i
h
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
L(θi ) =

S
N
2 X
1 X j
j
j
yi − Qµ
(o
,
a
)
+
C t (i, j)∥µi − µj ∥
i
i
i
S j=1
j=1

Update the actor policy using the sampled policy gradient:

 P
µ
S
∇θi J ≈ ∇θi S1 j=1 Qi i (oji , aj )|aj =[µ (oj )]
θi
i k
k
end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
end if
end for
end for

14

Algorithm 5 Fully-Decentralized MADDPG for Mixed Settings
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
h
i
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
L(θi ) =

S
2
1 X j
j
j
yi − Qµ
(o
,
a
)
i
i
S j=1

Update the actor policy for team actions (gradient ascent), using the sampled policy
gradient with adversarial actions fixed:


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
 S j=1
aj =[µθ (oj )] 
k

i

i

aj =[ajobs ]
k

k

k

Update the actor policy for adversarial actions (gradient descent), using the sampled
policy gradient with team actions fixed:


S

1X


µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

ajk =[ajobs ]

k

k

end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
end if
end for
end for

15

Algorithm 6 Fully-Decentralized Networked MADDPG with Hard Consensus Update, Mixed
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
h
i
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
L(θi ) =

S
2
1 X j
j
j
yi − Qµ
(o
,
a
)
i
i
S j=1

Update the actor policy for team actions (gradient ascent), using the sampled policy
gradient with adversarial actions fixed:


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

aj =[ajobs ]
k

k

k

Update the actor policy for adversarial actions (gradient descent), using the sampled
policy gradient with team actions fixed:


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

ajk =[ajobs ]

k

k

end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
Consensus update:
for agent i = 1 to N do
Update critic parameters:
µi ←

N
X

C t (i, j)µj

j=1

end for
end if
end for
end for

16

Algorithm 7 Fully-Decentralized Networked MADDPG with Soft Consensus Update, Mixed
for episode = 1 to M do
Initialize random process N ∈ RN
Receive initial observation x := (o1 , . . . , oN )
for t = 1 to max-episode-length do
for agent i = 1 to N do


Select action ai = µθi (oi ) i + Nt w.r.t. the current policy and exploration
Execute actions a = (a1 , . . . , aN ) and observe reward ri and next observations
x′ := (o′1 , . . . , o′N )
Store (oi , ai , ri , o′i ) in replay buffer Di
end for
x ← x′
if t mod learning-interval = 0 then
for agent i = 1 to N do
i
Sample a random minibatch of S samples {(oji , aji , rj , o′j
i )} from D
µ′

′j
h
i
Set yij = rij + γQi i (o′j
i , a )|a′j = µ′ (o′j ) .
k

θi

i

k

Update critic by minimizing the loss:
L(θi ) =

S
N
2 X
1 X j
j
j
C t (i, j)∥µi − µj ∥
yi − Qµ
(o
,
a
)
+
i
i
i
S j=1
j=1

Update the actor policy for team actions (gradient ascent), using the sampled policy
gradient with adversarial actions fixed:


S
1X



µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
 S j=1
aj =[µθ (oj )] 
k

i

i

aj =[ajobs ]
k

k

k

Update the actor policy for adversarial actions (gradient descent), using the sampled
policy gradient with team actions fixed:


S

1X


µ
∇θi J ≈ ∇θi 
Qi i (oji , aj )
,
j
j
 S j=1
a =[µθ (o )] 
k

i

i

ajk =[ajobs ]

k

k

end for
Update actor and critic target network parameters for each agent i:
θi′ ← τ θi + (1 − τ )θi′
end if
end for
end for

17

B

Further experimentation

B.1

Simple spread experiment for four agents

As mentioned in section 4.3.2 the evaluation scores for four agents look similar to those of five.

4 Agent Evaluation Scores

20

5 Agent Evaluation Scores
30

30

40
50

Score

Score

40
50
60

0

70
80

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

70

60

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

90
100

50000 100000150000200000250000300000350000400000

0

Steps

(a) Four agents

100000

200000

300000

Steps

400000

500000

(b) Five agents

Figure 2: Comparison of evaluation scores with four and five agents for the simple spread environment.
Training was done for two agents (a) over 400000 steps and for five agents (b) over 500000.
B.2

Changing the communication hyperparamter

Altering the communication hyperparamter (see section 4.2) from η = 0.001 to η = 0.05 results
in slower learning of the networked algorithms and instability of the hard consensus update at
N = 3. For larger N the hard consensus algorithm did not converge anymore while the lower rate
of communication did not affect the performance of the networked algorithm with soft consensus
update.

2 Agent Evaluation Scores

6

3 Agent Evaluation Scores
15

10

20

12

25

Score

Score

8

14
16

20
22

35

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

18

0

20000

40000

60000

Steps

80000

30

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

40
45

100000

(a) Two agents

0

50000 100000 150000 200000 250000 300000

Steps

(b) Three agents

Figure 3: Comparison of evaluation scores with less connectivity over training steps for the simple
spread environment between standard MADDPG (blue), the fully decentralized MADDPG (green)
and the fully decentralized MADDPG with hard (red) and soft (orange) consensus update. Training
was done for two agents (a) over 100000 steps and for three agents (b) over 300000.

18

B.3

Sparsity of the communication matrix

A fundamental feature of the networked algorithms is the ability to control how much agents
communicate with each other during training. To test whether less communication has an influence
on the training result, we compared the communication matrix described in section 4.2 for the
collaborative setting with a sparser matrix of the following form:
1 − η η
0 ...
0 

 0
 .
Ct = 
 ..
 .
 .
.
0

..

.

..

.

..

.

..

...

.
...

..

.

..

.

..

.
0

..
.





0 .


η
1−η

However, contrary to our expectations, the effect is negligible. Both for the fully connected and
weighted communication network as well as for the circular and weighted network, the soft consensus update outperformed the hard consensus update. The computational cost however is lowered
considerably by using the sparse communication network.

10 Agent Evaluation Scores

10 Agent Evaluation Scores

50

50
60

Score

Score

60
70

70

80
80

Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

90
0

90

50000 100000 150000 200000 250000 300000

Steps

(a) Fully connected communication graph

Networked MADDPG - hard consensus
Networked MADDPG - soft consensus
0

50000 100000 150000 200000 250000 300000

Steps

(b) Sparse communication graph – circle

Figure 4: Comparison of evaluation scores with ten agents for the simple spread environment with a
fully connected communication graph (4a) and a circle (4b).

19

B.4

Adversarial and mixed settings

Score

Adversarial 1vs2 Evaluation Scores
Adversary

20
25
30
35
40
45
50
55
0

50000

100000

30

150000
Steps
'Good' Agents

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus

20
Score

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus
200000
250000
300000

10
0
10
20
0

50000

100000

150000
Steps

200000

250000

300000

Figure 6: Performance of the algorithms in a mixed setting of one agent against two. The evaluation
scores of the single agent is plotted above while the teams scores are shown below.

20

Adversarial 1vs1 Evaluation Scores
Adversary

10

Score

20
30
40
50

Score

0
30
20
10
0
10
20
30
40

0

20000

20000

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus
40000
60000
Steps
'Good' Agents

40000

Steps

80000

100000

MADDPG
Fully Decentralized MADDPG
Networked MADDPG - hard consensus
Networked MADDPG - soft consensus
60000
80000
100000

Figure 5: Performance of the algorithms in an adversarial setting of one agent against one. The two
agents are shown in separate plots.

21

