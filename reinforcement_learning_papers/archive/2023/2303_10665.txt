Major-Minor Mean Field Multi-Agent Reinforcement Learning

Kai Cui 1 Christian Fabian 1 Anam Tahir 1 Heinz Koeppl 1

arXiv:2303.10665v2 [cs.LG] 7 May 2024

Abstract

Mean field control for MARL. Dynamical control and
behavior in systems with many agents is the subject of studies in mean field games (MFG) (Huang et al., 2006; Lasry
and Lions, 2007) and mean field control (MFC) (Nourian
et al., 2012; Bensoussan et al., 2013; Carmona et al., 2023b).
Such aggregated interaction models simplify MARL in
the limit of infinite agents, whenever agents interact only
through their empirical distribution. The simplification provides a problem complexity that is independent of the exact
number of agents. The result is tractability, by avoiding otherwise exponentially large joint state-action spaces (Zhang
et al., 2021). This has led to scalable control based on
MFC (Gu et al., 2023; Carmona et al., 2023b). And indeed, in applications such aggregation is commonly found
on some level, e.g., in chemical reaction networks for aggregate molecule mass (Anderson and Kurtz, 2011), related
mass-action epidemics models (Kiss et al., 2017), or traffic
where congestion depends on the number of travelling cars
(Cabannes et al., 2022), to name just a few. See also epidemics control (Dunyak and Caines, 2021), drone swarms
(Shiri et al., 2019), self organization (Carmona et al., 2023a),
and many more financial (Carmona, 2020) or engineering
scenarios (Djehiche et al., 2017).

Multi-agent reinforcement learning (MARL) remains difficult to scale to many agents. Recent
MARL using Mean Field Control (MFC) provides
a tractable and rigorous approach to otherwise
difficult cooperative MARL. However, the strict
MFC assumption of many independent, weaklyinteracting agents is too inflexible in practice. We
generalize MFC to instead simultaneously model
many similar and few complex agents – as MajorMinor Mean Field Control (M3FC). Theoretically,
we give approximation results for finite agent control, and verify the sufficiency of stationary policies for optimality together with a dynamic programming principle. Algorithmically, we propose
Major-Minor Mean Field MARL (M3FMARL)
for finite agent systems instead of the limiting system. The algorithm is shown to approximate the
policy gradient of the underlying M3FC MDP. Finally, we demonstrate its capabilities experimentally in various scenarios. We observe a strong
performance in comparison to state-of-the-art policy gradient MARL methods.

Limitations of standard MFC. However, the strict assumption of only minor agents – i.e. independent, homogeneous agents that can be summarized by their distribution
(MF) – limits applicability. In practice, systems often consist of more than homogeneous agents, and hence one must
extend standard MFC towards major agents or environment
states that are not aggregated. For instance, in modelling car
traffic on road networks (Cabannes et al., 2022; Wu et al.,
2023), when considering only the distribution of cars (minor
agents) on the network, one cannot model major agents or
environment states, such as traffic lights or the road conditions respectively. Another example is given by the logistics
scenario in Figure 1 and in the experiments, where many
drones on a moving truck collect many packages.

1. Introduction
Recent successes of reinforcement learning (RL) (Vinyals
et al., 2019; Schrittwieser et al., 2020; Ouyang et al., 2022)
motivate the search for techniques for the multi-agent case,
referred to as multi-agent reinforcement learning (MARL).
Due to the high complexity of multi-agent control (Bernstein
et al., 2002; Daskalakis et al., 2009), exploiting problem
structure is important for scalable MARL. In this work,
we consider systems with many agents interacting through
aggregated information of all agents – the mean field (MF).
1
Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany.
Correspondence to: Kai Cui <kai.cui@tu-darmstadt.de>, Heinz
Koeppl <heinz.koeppl@tu-darmstadt.de>.

For this purpose, a first step in the continuous-time MFG
literature is to consider common noise (Carmona et al.,
2016; Perrin et al., 2020), in order to relax the unconditional independence of minor agents. Some more recent
works consider such common noise also in discrete-time
MFC (Carmona et al., 2023b; Bäuerle, 2023; Motte and

Proceedings of the 41 st International Conference on Machine
Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).

1

Major-Minor Mean Field Multi-Agent Reinforcement Learning

negligible effect on the system. So far, major agents have
only been considered in continuous-time, non-cooperative
MFGs (Nourian and Caines, 2013; Şen and Caines, 2014;
Caines and Kizilkale, 2016; Şen and Caines, 2016). To the
best of our knowledge, no such discrete-time, cooperative
framework has been formulated yet. In this work, we investigate such a framework and associated MARL algorithms.
Contribution. Existing MFC cannot model general agents
and many aggregated agents simultaneously. In essence,
we generalize the solution spaces of single-agent RL and
MFC-based MARL – frameworks for cooperative MARL
as depicted in Figure 2. This provides both tractability for
many aggregated agents and generality for arbitrary general agents. Our contribution is briefly summarized into (i)
formulating the first discrete-time MFC model with major
agents, together with establishing its theoretical properties;
(ii) providing a MFC-based MARL algorithm, which in
contrast to prior work learns on the finite problem of interest; and (iii) we perform a significant empirical evaluation,
also obtaining positive comparisons of MFC-based MARL
against state of the art, whereas prior works on MFC were
limited to verifying algorithms on one or two examples.

Figure 1. Logistics example: Many drones are modelled as minor
agent MF, while truck and package destinations are modelled by a
major agent. (See Foraging problem in Section 4.1)

Pham, 2022; 2023), or equivalently, global environment
states (Mondal et al., 2023). Essentially, this extension allows MFC to also model random environment effects such
as the arrival of new packages in the logistics example (Figure 1). Carmona et al. (2023b) provide a reformulation of
MARL into single-agent RL and consider algorithms for the
resulting Markov decision process (MDP). Bäuerle (2023)
give approximation theorems and approximate optimality
in the finite system by the limiting MFC solution with common noise, and Motte and Pham (2022; 2023) quantify the
rates of convergence explicitly. See also Table 1 for a brief
comparison between existing works. In comparison, for
the common noise setting, we contribute a new approximation analysis of MFC-based MARL algorithms, where in
contrast to prior work, we learn directly with finite agents.

Figure 2. Our M3FC-based MARL generalizes MFC-based MARL
and standard single-agent RL in the solution space of general
MARL solutions, reducing the otherwise combinatorial nature of
MARL (Zhang et al., 2021) to a tractable but still general setting.

More importantly however, a second contribution is to consider major agents. Major agents generalize common noise
or environmental states, and take actions that have a non-

Table 1. A comparison of recent related works and a subset of their results on discrete-time MFC.
prop. chaos: propagation of chaos; opt. policy: existence of optimal (stationary) policies; common noise: presence thereof; non-finite:
non-finite state-actions, e.g. compact; major agent: presence thereof; RL: RL algorithm (+ : learns / is analyzed on finite MARL problems).

Ref.

prop. chaos

opt. policy

common noise

non-finite

major agent

RL

Carmona et al. (2023b)
Gu et al. (2021; 2023)
Bäuerle (2023)
Mondal et al. (2022; 2023)
Motte and Pham (2022; 2023)
our work

✗
✓
✓
✓
✓
✓

✓
✓
✓
✗
✓
✓

✓
✗
✓
✓
✓
✓

✓
✗
✓
✗
✓
✓

✗
✗
✗
✗
✗
✓

✓
✓
✗
✓
✗
✓+

2

Major-Minor Mean Field Multi-Agent Reinforcement Learning

hP
∞

i

2. Major-Minor Mean Field Control

objective J N (π, π 0 ) := E

To begin, in this section we extend standard MFC by modelling the presence of a major agent. The generalization to
more than one major agent is straightforward. This leads to
our discrete-time major-minor MFC (M3FC) model. Overall, we obtain a formulation that allows standard MARL
handling of major agents, while tractably handling many
minor agents via MFC-based techniques.

over minor and major policies (π, π 0 ), with discount γ ∈
(0, 1) and reward function r : P(X ) → R. While an optimal behavior could be learned using standard MARL policy
gradient methods, for improved tractability we introduce the
following M3FC model in the case of many minor agents.
Remark 1. The model is as expressive as in existing MFC
(Mondal et al., 2022; Gu et al., 2023), as it also includes
(i) joint state-action MFs νt ∈ P(X × U), by splitting
time steps in two and defining new states in X ∪ X × U, (ii)
average rewards over all agents, and (iii) random rewards rti
PN
i,N
1
i
N
by r(µN
t ) ≡ N
i=1 E[rt | xt , µt ]. A finite horizon is
handled analogously (without optimal stationary policies).

Notation: By EX we denote conditional expectations given
X. The space of probability measures P(X ) on compact
metric spaces X is equipped with the 1-Wasserstein distance, unless noted otherwise (Villani, 2009). Note compactness of P(X ) on compact X by Prokhorov’s theorem (Billingsley, 2013). Hence, we sometimes use the
′
uniformly (not
R Lipschitz) ′equivalent metric dΣ (µ, µ ) :=
P
∞
−m
| fm d(µ−µ )|, for some sequence of continum=1 2
ous fm : X → [−1, 1] (Parthasarathy, 2005, Theorem 6.6).

2.2. Mean Field Control Limit
By the introduction of the MF limit, we obtain a large,
more tractable subclass of cooperative multi-agent control
problems, which may otherwise suffer from the curse of
many agents (combinatorial joint state-action space, (Zhang
et al., 2021)). We introduce the MF limit by formally taking
N → ∞: The finite-agent control problem is replaced
by a higher-dimensional single-agent MDP – the M3FC
MDP. By symmetry, we summarize minor agents into their
probability law, the MF µt ≡ L(xi,N
t ) ∈ P(X ). It replaces
its empirical analogue µN
t by a law of large numbers (LLN).
Thus, by definition, the MF µt evolves forward as

2.1. Finite-Agent System
Consider N (minor) agents i ∈ [N ] := {1, . . . , N } with
compact metric state and action spaces X , U, equipped with
random states and actions xi,N
and ui,N
at times t ∈ N,
t
t
i,N
where initial states x0 ∼ µ0 are independently sampled
from some initial distribution µ0 ∈ P(X ). In addition
to standard MFC, we also consider a single major agent,
though the framework can be extended to multiple. Consider major agent state and action spaces, X 0 , U 0 and stateactions x0,N
, u0,N
, with the major agent formally indexed
t
t
by i = 0. Given all actions, the agent states evolve according to kernels p, p0 depending on (i) the agent’s own
state-actions, (ii) the major state-actions, and (iii) the empirical MF, i.e. the P(X )-valued empirical state distribution
PN
1
µN
. This means that minor agents affect
t := N
i=1 δxi,N
t
other agents only at rate N1 . In practice, we identify minor
agents as all agents that matter through their MF µN
t . Any
remaining agents are major, such that the problem-specific
stratification into major and minor agents is always possible.

µt+1 = T (x0t , u0t , µt , µt ⊗ πt (µt ))
ZZ
=
p(· | x, u, x0t , u0t , µt )πt (du | x, µt )µt (dx), (2)
with πt (µt ) := πt (· | ·, µt ), product measures µt ⊗ πt (µt )
of measure µt and kernel πt (µt ) on X × U, and
RRdeterministic dynamics for the MF, T (x0 , u0 , µ, h) :=
p(· |
x, u, x0 , u0 , µ)h(dx, du).
Therefore, the state of the limiting system consists only of
the MF µt and major state x0t . As a result, we obtain the
limiting M3FC MDP

By symmetry, the system state at any time t is therefore
entirely given by (x0,N
, µN
t
t ). Accordingly, in MFC we
share policies between all minor agents. We consider timevariant policies π ∈ Π, π 0 ∈ Π0 from some classes of major
and minor policies Π, Π0 that depend on an agent’s own
state and (x0,N
, µN
t
t ) at all times t. Overall, for all i ∈ [N ]
and t ∈ N, the finite MFC system follows
0,N
ui,N
∼ πt (ui,N
| xi,N
, µN
t
t
t , xt
t ),

u0,N
∼ πt0 (u0,N
| x0,N
, µN
t
t
t
t ),
i,N
i,N
i,N
i,N
xt+1 ∼ p(xt+1 | xt , ut , x0,N
, u0,N
, µN
t
t
t ),
0,N
0,N
0,N
0 0,N
N
xt+1 ∼ p (xt+1 | xt , ut , µt ) .

0,N
t
, u0,N
, µN
t
t )
t=0 γ r(xt

ht ∼ π̂t (ht | x0t , µt ),

u0t ∼ πt0 (u0t | x0t , µt ),
µt+1 = T (x0t , u0t , µt , ht ),

(3a)
(3b)
(3c)

x0t+1 ∼ p0 (x0t+1 | x0t , u0t , µt )

(3d)

t
0
0
with objective J(π̂, π 0 ) = E
t ) and
t=0 γ r(xt , ut , µRR
0
0
transition dynamics for the MF T (x , u , µ, h) :=
p(· |
x, u, x0 , u0 , µ)h(dx, du). Here, we identify µt ⊗ πt (µt ) ≡
ht ∈ H(µt ) in the compact set H(µ) ⊆ P(X × U) of
desired joint state-action distributions with first marginal µ
as part of the action of the M3FC MDP.
P∞

(1a)
(1b)
(1c)
(1d)

In other words, the action of the M3FC MDP is (ht , u0t )
where ht replaces all the minor agent actions by a LLN.

The goal is then to maximize the infinite-horizon discounted
3

Major-Minor Mean Field Multi-Agent Reinforcement Learning
x0i,N

x1i,N
<latexit sha1_base64="mMLRS8HpT1jEk2pl4rCJ48+GXtw=">AAACenicfVHbSgMxEE3XW613ffQlWAqKUna9oI9FX3wSBeuFupZsOmtDk82SzIpl6Vf4qh/mv/hgekFcBQcCh3NmMmdmolQKi77/UfKmpmdm58rzlYXFpeWV1bX1G6szw6HJtdTmLmIWpEigiQIl3KUGmIok3Ea9s6F++wzGCp1cYz+FULGnRMSCM3TU/ctjLvYuBu2gvVr16/4o6F8QTECVTOKyvVZqPXQ0zxQkyCWzthX4KYY5Myi4hEHlIbOQMt5jT9ByMGEKbJiPHA9ozTEdGmvjXoJ0xP6syJmytq8il6kYdu1vbUh+a7VCK4xPwlwkaYaQ8HGnOJMUNR3OTzvCAEfZd4BxI5xZyrvMMI5uS5Ua5ZlFrf7/s6DmQxOotbTFmbsgn8EtxMYjswXNciahE+bjnIKWR5EaVNw5gt/L/wtu9uvBQf3o6rDaOJ0cpkw2yRbZJgE5Jg1yTi5Jk3CiyCt5I++lT2/L2/F2x6leaVKzQQrhHX4BcoTDSQ==</latexit>

<latexit sha1_base64="cFTdvFxoAJKXjmZxRbdhLTSChVw=">AAACenicfVHbSgMxEE3XW613ffQlWAqKUna9oI9FX3wSBeuFupZsOmtDk82SzIpl6Vf4qh/mv/hgekFcBQcCh3NmMmdmolQKi77/UfKmpmdm58rzlYXFpeWV1bX1G6szw6HJtdTmLmIWpEigiQIl3KUGmIok3Ea9s6F++wzGCp1cYz+FULGnRMSCM3TU/ctjLvYuBm2/vVr16/4o6F8QTECVTOKyvVZqPXQ0zxQkyCWzthX4KYY5Myi4hEHlIbOQMt5jT9ByMGEKbJiPHA9ozTEdGmvjXoJ0xP6syJmytq8il6kYdu1vbUh+a7VCK4xPwlwkaYaQ8HGnOJMUNR3OTzvCAEfZd4BxI5xZyrvMMI5uS5Ua5ZlFrf7/s6DmQxOotbTFmbsgn8EtxMYjswXNciahE+bjnIKWR5EaVNw5gt/L/wtu9uvBQf3o6rDaOJ0cpkw2yRbZJgE5Jg1yTi5Jk3CiyCt5I++lT2/L2/F2x6leaVKzQQrhHX4BcHbDSA==</latexit>

...

M3FC model by using a trivial major agent without actions.
We also note that, in general, common noise is equivalent
to global states, as global states can be integrated into the
minor state conditioned on the common noise. However,
for computational purposes the separation of global states
and minor agent states can be helpful, as the simplex P(X )
over minor states can be kept smaller for methods based on
discretization of the simplex.

<latexit sha1_base64="pzUJXgsZ3ixXd/X7vPzDKIDt2BA=">AAACfnicfVHbSsNAEJ3Ge73roy/BovhiTapgHwsi+Khgq9CGstlM2sVNNuxOhBL6Db7qp/k3bi+IacGBhcM5MztnZsJMCkOe911xVlbX1jc2t6rbO7t7+weHRx2jcs2xzZVU+jVkBqVIsU2CJL5mGlkSSnwJ3+4m+ss7aiNU+kyjDIOEDVIRC87IUu1epMj0D2pe3ZuGuwz8OajBPB77hxVuC3meYEpcMmO6vpdRUDBNgkscV3u5wYzxNzbAroUpS9AExdTt2D2zTOTGStuXkjtl/1YULDFmlIQ2M2E0NIvahPzVzkqtKG4GhUiznDDls05xLl1S7mR2NxIaOcmRBYxrYc26fMg042Q3VP3vp5KHSWdSSpryoEOU72i3YOKpw5JmOJMYBcUsp6QVYZgsEkpG46o9i794hGXQadT963rj6abWup8faBNO4BQuwIdbaMEDPEIbOAj4gE/4csA5dy6dq1mqU5nXHEMpnOYPrgTFbQ==</latexit>

u1i,N

u0i,N

i = 1, ... , N

<latexit sha1_base64="ck3aJmN0JZR9RAL8d2/iQDWc9WY=">AAACenicfVFNSwMxEE3X7/pZPXoJloKilF2t6FH04kkUbFXqWrLprA0mmyWZFcrSX+FVf5j/xYPpB+JacCDweG8m82YmSqWw6PufJW9mdm5+YXGpvLyyura+UdlsWZ0ZDk2upTb3EbMgRQJNFCjhPjXAVCThLnq5GOp3r2Cs0Mkt9lMIFXtORCw4Q0c9ZE+5OLgadILORtWv+6Og0yCYgCqZxHWnUmo/djXPFCTIJbO2HfgphjkzKLiEQfkxs5Ay/sKeoe1gwhTYMB85HtCaY7o01sa9BOmI/V2RM2VtX0UuUzHs2b/akPzRaoVWGJ+GuUjSDCHh405xJilqOpyfdoUBjrLvAONGOLOU95hhHN2WyjXKM4ta/f9nQc2HJlBraYsz90C+gluIjUdmC5rlTEI3zMc5BS2PIjUou3MEf5c/DVqH9eCofnzTqJ6dTw6zSLbJDtklATkhZ+SSXJMm4USRN/JOPkpf3o635+2PU73SpGaLFMJrfANsQsNG</latexit>

<latexit sha1_base64="vTqztDkmjvdR/HXLqEXUWcJD7iI=">AAACenicfVFNSwMxEE3X7/pZPXoJloKilF2t6FH04kkUbFXqWrLprA0mmyWZFcrSX+FVf5j/xYPpB+JacCDweG8m82YmSqWw6PufJW9mdm5+YXGpvLyyura+UdlsWZ0ZDk2upTb3EbMgRQJNFCjhPjXAVCThLnq5GOp3r2Cs0Mkt9lMIFXtORCw4Q0c9ZE+5OLgadPzORtWv+6Og0yCYgCqZxHWnUmo/djXPFCTIJbO2HfgphjkzKLiEQfkxs5Ay/sKeoe1gwhTYMB85HtCaY7o01sa9BOmI/V2RM2VtX0UuUzHs2b/akPzRaoVWGJ+GuUjSDCHh405xJilqOpyfdoUBjrLvAONGOLOU95hhHN2WyjXKM4ta/f9nQc2HJlBraYsz90C+gluIjUdmC5rlTEI3zMc5BS2PIjUou3MEf5c/DVqH9eCofnzTqJ6dTw6zSLbJDtklATkhZ+SSXJMm4USRN/JOPkpf3o635+2PU73SpGaLFMJrfANqNMNF</latexit>

<latexit sha1_base64="sZSkNUK753pcKn9PNVuo88lYLUE=">AAACfXicfVHLSgMxFE3Hd33r0k2wFFyUMuMD3QiiG1eiYKvQDiWTudMGk8mQ3BHK0N9wq7/l12j6QBwLXggczrnJPTk3yqSw6PufFW9hcWl5ZXWtur6xubW9s7vXtjo3HFpcS22eI2ZBihRaKFDCc2aAqUjCU/RyM9afXsFYodNHHGYQKtZPRSI4Q0d1xWXQ6MpYo23c9XZqftOfFJ0HwQzUyKzue7uVTjfWPFeQIpfM2k7gZxgWzKDgEkbVbm4hY/yF9aHjYMoU2LCYmB7RumNimmjjTop0wv6+UTBl7VBFrlMxHNi/2pj80eqlUZhchIVIsxwh5dNJSS4pajqOgMbCAEc5dIBxI5xZygfMMI4uqGqd8tyiVv+/WVKLsQnUWtrynwcgX8EFYpOJ2ZJmOZMQh8W0p6QVUaRGVbeO4G/486B93AxOmmcPp7Wr69liVskBOSRHJCDn5IrcknvSIpxk5I28k4/Kl1f3Gl5z2upVZnf2Sam8829wesQl</latexit>

µN0

µN1

x00,N

x10,N

...

<latexit sha1_base64="KtZempnD+Aiuhx13ZQGtrBR0fuk=">AAACenicfVHbSgMxEE3Xe71VffQltBQUoex6QR9FX3wSBWuVdi3ZdNYGk82SzBbK0q/wVT/Mf/HB9IK4Cg4EDufMZM7MRKkUFn3/o+TNzS8sLi2vlFfX1jc2K1vb91ZnhkOTa6nNQ8QsSJFAEwVKeEgNMBVJaEUvl2O9NQBjhU7ucJhCqNhzImLBGTrqsaOyp/x61A26lZrf8CdB/4JgBmpkFjfdrVK709M8U5Agl8zaduCnGObMoOASRuVOZiFl/IU9Q9vBhCmwYT5xPKJ1x/RorI17CdIJ+7MiZ8raoYpcpmLYt7+1Mfmt1QutMD4Lc5GkGULCp53iTFLUdDw/7QkDHOXQAcaNcGYp7zPDOLotleuUZxa1+v/PgpqPTaDW0hZn7oMcgFuIjSdmC5rlTEIvzKc5BS2PIjUqu3MEv5f/F9wfNoKjxsntce38YnaYZbJLqmSPBOSUnJMrckOahBNFXskbeS99elVv3zuYpnqlWc0OKYR3/AXXnMN6</latexit>

<latexit sha1_base64="5EqSCzYzg1nyQqTZo+tQ0Qo2cdI=">AAACenicfVHbSgMxEE3Xe71VffQltBQUoex6QR9FX3wSBWuVdi3ZdNYGk82SzBbK0q/wVT/Mf/HB9IK4Cg4EDufMZM7MRKkUFn3/o+TNzS8sLi2vlFfX1jc2K1vb91ZnhkOTa6nNQ8QsSJFAEwVKeEgNMBVJaEUvl2O9NQBjhU7ucJhCqNhzImLBGTrqsaOyp/x61PW7lZrf8CdB/4JgBmpkFjfdrVK709M8U5Agl8zaduCnGObMoOASRuVOZiFl/IU9Q9vBhCmwYT5xPKJ1x/RorI17CdIJ+7MiZ8raoYpcpmLYt7+1Mfmt1QutMD4Lc5GkGULCp53iTFLUdDw/7QkDHOXQAcaNcGYp7zPDOLotleuUZxa1+v/PgpqPTaDW0hZn7oMcgFuIjSdmC5rlTEIvzKc5BS2PIjUqu3MEv5f/F9wfNoKjxsntce38YnaYZbJLqmSPBOSUnJMrckOahBNFXskbeS99elVv3zuYpnqlWc0OKYR3/AXVjsN5</latexit>

<latexit sha1_base64="pzUJXgsZ3ixXd/X7vPzDKIDt2BA=">AAACfnicfVHbSsNAEJ3Ge73roy/BovhiTapgHwsi+Khgq9CGstlM2sVNNuxOhBL6Db7qp/k3bi+IacGBhcM5MztnZsJMCkOe911xVlbX1jc2t6rbO7t7+weHRx2jcs2xzZVU+jVkBqVIsU2CJL5mGlkSSnwJ3+4m+ss7aiNU+kyjDIOEDVIRC87IUu1epMj0D2pe3ZuGuwz8OajBPB77hxVuC3meYEpcMmO6vpdRUDBNgkscV3u5wYzxNzbAroUpS9AExdTt2D2zTOTGStuXkjtl/1YULDFmlIQ2M2E0NIvahPzVzkqtKG4GhUiznDDls05xLl1S7mR2NxIaOcmRBYxrYc26fMg042Q3VP3vp5KHSWdSSpryoEOU72i3YOKpw5JmOJMYBcUsp6QVYZgsEkpG46o9i794hGXQadT963rj6abWup8faBNO4BQuwIdbaMEDPEIbOAj4gE/4csA5dy6dq1mqU5nXHEMpnOYPrgTFbQ==</latexit>

<latexit sha1_base64="NLaCcDUefcbdg8YinUVvCqKJzFA=">AAACenicfVHbSgMxEE3XW613ffQlWAqKUna9oI9FX3wSBeuFupZsOmtDk82SzIpl6Vf4qh/mv/hgekFcBQcCh3NmMmdmolQKi77/UfKmpmdm58rzlYXFpeWV1bX1G6szw6HJtdTmLmIWpEigiQIl3KUGmIok3Ea9s6F++wzGCp1cYz+FULGnRMSCM3TU/ctj7u9dDNpBe7Xq1/1R0L8gmIAqmcRle63UeuhonilIkEtmbSvwUwxzZlBwCYPKQ2YhZbzHnqDlYMIU2DAfOR7QmmM6NNbGvQTpiP1ZkTNlbV9FLlMx7Nrf2pD81mqFVhifhLlI0gwh4eNOcSYpajqcn3aEAY6y7wDjRjizlHeZYRzdlio1yjOLWv3/Z0HNhyZQa2mLM3dBPoNbiI1HZgua5UxCJ8zHOQUtjyI1qLhzBL+X/xfc7NeDg/rR1WG1cTo5TJlski2yTQJyTBrknFySJuFEkVfyRt5Ln96Wt+PtjlO90qRmgxTCO/wC/DrDEA==</latexit>

<latexit sha1_base64="9OGyX81RW4X9kMQgcfk3997LTLs=">AAACenicfVHbSgMxEE3XW613ffQlWAqKUna9oI9FX3wSBeuFupZsOmtDk82SzIpl6Vf4qh/mv/hgekFcBQcCh3NmMmdmolQKi77/UfKmpmdm58rzlYXFpeWV1bX1G6szw6HJtdTmLmIWpEigiQIl3KUGmIok3Ea9s6F++wzGCp1cYz+FULGnRMSCM3TU/ctj7u9dDNp+e7Xq1/1R0L8gmIAqmcRle63UeuhonilIkEtmbSvwUwxzZlBwCYPKQ2YhZbzHnqDlYMIU2DAfOR7QmmM6NNbGvQTpiP1ZkTNlbV9FLlMx7Nrf2pD81mqFVhifhLlI0gwh4eNOcSYpajqcn3aEAY6y7wDjRjizlHeZYRzdlio1yjOLWv3/Z0HNhyZQa2mLM3dBPoNbiI1HZgua5UxCJ8zHOQUtjyI1qLhzBL+X/xfc7NeDg/rR1WG1cTo5TJlski2yTQJyTBrknFySJuFEkVfyRt5Ln96Wt+PtjlO90qRmgxTCO/wC+izDDw==</latexit>

u00,N
<latexit sha1_base64="dpxfG6IhKbC55eJW12GbZOog5So=">AAACenicfVFNSwMxEE3X7/pZPXoJloKilF2t6FH04kkUbFXqWrLprA0mmyWZFcrSX+FVf5j/xYPpB+JacCDweG8m82YmSqWw6PufJW9mdm5+YXGpvLyyura+UdlsWZ0ZDk2upTb3EbMgRQJNFCjhPjXAVCThLnq5GOp3r2Cs0Mkt9lMIFXtORCw4Q0c9ZE+5f3A16Pidjapf90dBp0EwAVUyietOpdR+7GqeKUiQS2ZtO/BTDHNmUHAJg/JjZiFl/IU9Q9vBhCmwYT5yPKA1x3RprI17CdIR+7siZ8ravopcpmLYs3+1Ifmj1QqtMD4Nc5GkGULCx53iTFLUdDg/7QoDHGXfAcaNcGYp7zHDOLotlWuUZxa1+v/PgpoPTaDW0hZn7oF8BbcQG4/MFjTLmYRumI9zCloeRWpQducI/i5/GrQO68FR/fimUT07nxxmkWyTHbJLAnJCzsgluSZNwokib+SdfJS+vB1vz9sfp3qlSc0WKYTX+Abz6sMM</latexit>

u10,N
<latexit sha1_base64="L6M6f4soTZ8Lc1l410ZvLII0Jp4=">AAACenicfVFNSwMxEE3X7/pZPXoJloKilF2t6FH04kkUbFXqWrLprA0mmyWZFcrSX+FVf5j/xYPpB+JacCDweG8m82YmSqWw6PufJW9mdm5+YXGpvLyyura+UdlsWZ0ZDk2upTb3EbMgRQJNFCjhPjXAVCThLnq5GOp3r2Cs0Mkt9lMIFXtORCw4Q0c9ZE+5f3A16ASdjapf90dBp0EwAVUyietOpdR+7GqeKUiQS2ZtO/BTDHNmUHAJg/JjZiFl/IU9Q9vBhCmwYT5yPKA1x3RprI17CdIR+7siZ8ravopcpmLYs3+1Ifmj1QqtMD4Nc5GkGULCx53iTFLUdDg/7QoDHGXfAcaNcGYp7zHDOLotlWuUZxa1+v/PgpoPTaDW0hZn7oF8BbcQG4/MFjTLmYRumI9zCloeRWpQducI/i5/GrQO68FR/fimUT07nxxmkWyTHbJLAnJCzsgluSZNwokib+SdfJS+vB1vz9sfp3qlSc0WKYTX+Ab1+MMN</latexit>

M3FC

2.3. Dynamic Programming
As a first step, it is well known that stationary (timeindependent) policies suffice for optimality in infinitehorizon discounted MDPs. In the following, this property is
also verified for the M3FC MDP. For the following technical
results, we assume standard Lipschitz conditions (Gu et al.,
2021; Mondal et al., 2022; Pásztor et al., 2023).

Figure 3. The dynamics (1) as a probabilistic graphical model, with
actions in grey (inputs omitted for readability). Diamonds denote
deterministic functions. M3FC abstracts minor agents i ∈ [N ] by
a LLN, considering only their MF as variables in the dotted box.

Accordingly, minor agent policies are replaced by MFC
policies π̂ mapping from current µt to desired state-action
distribution ht . The limiting M3FC model abstracts away
all the minor agents in the finite system, and considers only
the MF and the major agents, as visualized in Figure 3. The
reason for writing joint ht is mostly technical, as for deterministic π̂, we write πt = Φ(π̂t ) to reobtain agent policies
µt -a.e. uniquely by disintegration (Kallenberg, 2017) of
ht = π̂t (µt ) into µt ⊗ πt′ with decision rule πt′ ∈ P(U)X
and using πt (µt ) ≡ πt′ . Inversely, any π ∈ Π is represented
in the MFC MDP by deterministic π̂t = Φ−1 (π)t = µt ⊗πt .

Assumption 1. The transition kernels p, p0 and rewards r
are Lipschitz with constants Lp , Lp0 , Lr .
Assumption 1 is true, e.g., in finite spaces if transition matrix
entries of P are Lipschitz in the |X |-dimensional MF vector.
The sufficiency of stationary policies is obtained by the
dynamic programming principle, which can also be used
to compute exact optimal policies in the M3FC MDP. We
use the value function V ∗ as the fixed point of the Bellman
equation, V ∗ (x0 , µ) = max(h,u0 )∈H(µ)×U 0 r(x0 , u0 , µ) +
γEy0 ∼p0 (y0 |x0 ,u0 ,µ) V ∗ (y 0 , T (x0 , u0 , µ, h)).

Remark 2. Strictly speaking, in finite-agent control one
jointly select actions (u0,N
, u1,N
, . . . , uN,N
) given joint
t
t
t
0,N
1,N
N,N
states (xt , xt , . . . , xt ). But intuitively, (i) joint
states reduce to (x0,N
, µN
t
t ), while (ii) joint actions are
replaced by the LLN and sampling actions. Optimality of
MFC solutions over larger classes of heterogeneous or joint
policies is plausible, but to the best of our knowledge, general result are still limited. See also Appendix Q.

Theorem 1. Under Assumption 1, there exist optimal stationary, deterministic policies π̂, π 0 for the
M3FC MDP (3) by choosing (π̂(x0 , µ), π 0 (x0 , µ)) from
the maximizers of arg max(h,u0 )∈H(µ)×U 0 r(x0 , u0 , µ) +
γEy0 ∼p0 (y0 |x0 ,u0 ,µ) V ∗ (y 0 , T (x0 , u0 , µ, h)).
Remark 3. We obtain existence of optimal deterministic
stationary minor and major policies π̂, π 0 via optimal joint
policies π̃ ≡ π̂ ⊗ π 0 , (ht , u0t ) ∼ π̃((ht , u0t ) | x0t , µt ).

For the unfamiliar reader, in Appendix B we recap basic
deterministic MFC without major agents or common noise.
There, we recap Lipschitz approximation theorems and dynamic programming principles in compact spaces.

The results follow from classical MDP theory (HernándezLerma and Lasserre, 2012). Thus, we may solve M3FC
problems through the DPP, or approximately by using policy
gradients with stationary policies for the M3FC MDP, which
has naturally continuous actions.

Common noise and global states. In the classical sense
(Perrin et al., 2020; Motte and Pham, 2022), common noise
is given by random noise ϵ0t ∼ pϵ (ϵ0t ) sampled from a fixed
distribution pϵ , and affects all minor agents at once, xi,N
t+1 ∼
i,N
i,N
i,N 0
N
p(xt+1 | xt , ut , ϵt , µt ). This allows to model systems
with stochastic MFs and inter-agent correlation, and has
added difficulty to the theoretical analysis (Carmona et al.,
2016). Of similar interest are also “major” global states
x0,N
, which need not be sampled from fixed distributions
t
but evolve dynamically (for MFC with finite global states,
see e.g. Mondal et al. (2023)).

Next, in order to show the approximate optimality of M3FC
solutions, we first obtain propagation of chaos (Sznitman,
1991) – convergence of empirical MFs to the limiting MF.
The result theoretically backs the reduction of multi-agent
control to single-agent MDPs, as there is no loss of optimality in the finite problem by considering the M3FC problem.
We assume standard Lipschitz conditions on policies (Gu
et al., 2021; Mondal et al., 2022; Pásztor et al., 2023).

Both common noise and global states are contained in the

Assumption 2. The classes of policies Π, Π0 are equi-

2.4. Finite Agent Convergence

4

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Lipschitz sets of policies, i.e. there exists LΠ > 0 such that
for all t and π ∈ Π, πt ∈ P(U)X ×P(X ) is LΠ -Lipschitz,
and similarly for major policies π 0 ∈ Π0 .

Recalling the motivation of MFC, it is crucial to find
tractable sample-based MARL techniques for both complex
problems where other methods fail, and for problems where
we have no access to the dynamics or reward model. Relating to the former, RL has been applied before to solve MFC
given that we know the MFC model equations (Carmona
et al., 2023b; Pásztor et al., 2023; Mondal et al., 2022).
However, regarding the latter, we should instead use the
MFC formalism to give rise to novel MARL algorithms.

We note that Lipschitz policies are natural, as we usually
parametrize policies in a Lipschitz manner; in particular,
neural networks allow Lipschitz analysis (Pásztor et al.,
2023; Herrera et al., 2023; Araujo et al., 2023). The result is
that the limiting system approximates large finite systems.
Theorem 2. Fix any family of equi-Lipschitz functions F ⊆
0
0
RX ×U ×P(X ) with shared Lipschitz constant LF . Under
Assumptions 1 and 2, (x0,N
, u0,N
, µN
t
t
t ) converges weakly
0
0
to (xt , ut , µt ), uniformly over f ∈ F, (π, π 0 ) ∈ Π × Π0 ,
π̂ = Φ−1 (π) at all times t ∈ N,
h
i
0
0
sup E f (x0,N
, u0,N
, µN
→ 0. (4)
t
t
t ) − f (xt , ut , µt )

While literature usually focused analysis on the former, in
our work we analyze the proposed algorithm not on limiting M3FC MDPs, but on the more interesting finite M3FC
system. In particular, if the M3FC MDP is known, one can
instantiate finite systems of any size for training. We consider the following perspective: By Theorem 2, the M3FC
MDP is approximated well by the finite system. Therefore, we can solve the limiting M3FC MDP by applying our
proposed algorithm directly to finite M3FC systems.

f,π,π 0

√
Further, the convergence rate is O(1/ N ) if |X | < ∞.

Since we know by Theorem 1 that stationary policy suffice,
we solve the M3FC MDP (3) using stationary policies and
single-agent RL techniques but on its finite multi-agent instance (1), the combination of which we aptly refer to as
Major-Minor Mean Field MARL (M3FMARL). The result
is Algorithm 1, where we directly apply RL to multi-agent
N
systems (1) by observing next states (x0,N
t+1 , µt+1 ) and re0,N
0,N
N :=
N
wards rt
r(xt , ut , µt ). The algorithm can be
understood as a kind of hierarchical algorithm, as M3FC
MDP actions specify behavior for all minor agents at once.

The above motivates M3FC by the following near optimality result of M3FC MDP solutions in the finite system, as it
suffices to optimize over stationary M3FC policies.
Corollary 1. Under Assumptions 1 and 2, optimal deterministic M3FC MDP policies (π̂ ∗ , π 0∗ ) ∈
arg max(π̂,π0 ) J(π̂, π 0 ) with Φ(π̂ ∗ ) ∈ Π yield ε-optimal
(Φ(π̂ ∗ ), π 0∗ ) with ε → 0 as N → ∞ in the finite system,
J N (Φ(π̂ ∗ ), π 0∗ ) ≥ sup(π,π0 )∈Π×Π0 J N (π, π 0 ) − ε.
Therefore, one may solve difficult finite-agent MARL by
detouring over the corresponding M3FC MDP as depicted in
Figure 4, reducing to an MDP of a complexity independent
of the number of agents N , which we solve in Section 3.

3.1. M3FC-based Policy Gradients
The proposed algorithm can be theoretically motivated. As
shown in the following, finite-agent policy gradients (PG)
estimate the true limiting M3FC MDP PG. First, note that
finite state-actions X , U lead to continuous M3FC MDP
actions H(µ), while continuous X , U even yield infinitedimensional H(µ). Therefore, we have at least continuous

3. Major-Minor Mean Field MARL
As indicated in the prequel and in Figure 2, MARL via
M3FC generalizes both single-agent RL and MARL via
MFC in the searched policy solution space. Therefore, in
M3FC one only optimizes over a tractable, smaller solution
space of a single minor and major policy Π, Π0 . At the same
time, the framework is highly general and handles arbitrary
major agents with many minor agents simultaneously. The
reduction of MARL problems to a fixed-complexity singleagent M3FC MDP is the key. In this section, we develop
MARL algorithms based on the M3FC framework.
N -minor agent control

N →∞

M3FC
optimize

optimize (intractable)

Optimal N -minor agent control

Algorithm 1 M3FMARL
1: for n = 0, 1, . . . do
2:
for t = 0, . . . , Blen − 1 do
3:
Sample M3FC action from RL policy, i.e.
ut ≡ (u0,N
, πt′ ) ∼ π̃ θ (· | x0,N
, µN
t
t
t ).
4:
for i = 1, . . . , N do
5:
Sample i-th minor action ui,N
∼ πt′ (· | xi,N
t
t ).
6:
end for
Execute {u0,N
, u1,N
, . . .} for next reward rtN ,
7:
t
t
0,N
N
state (xt+1 , µt+1 ) and termination dt+1 ∈ {0, 1}.
8:
end for
9:
Perform an update (on policy π̃ θ ) using transitions
0,N
N
N
B = ((x0,N
, µN
t
t ), ut , rt , dt+1 , (xt+1 , µt+1 ))t≥0 .
10: end for

approx.

M3FC policy

Figure 4. Approximation of intractable N -agent control by M3FC
(blue path), the solution of which is near-optimal for large N .

5

Major-Minor Mean Field Multi-Agent Reinforcement Learning

tion to the major agent action – the matrix ξ ∈ [−1, 1]X ×U ,
which is mapped to probabilities of minor actions in any minor state πt′ (u | x) := Z −1 (ξxu +1+ϵ), for small ϵ = 10−10
and normalizer Z. For continuous X , U, we instead partition
X into M bins and represent µN
t as a histogram, mapping
ξ ∈ [−1, 1]M ×2 to diagonal Gaussian means and standard
deviations, µXi ∈ U, σXi ∈ [ϵ, 0.25+ϵ], for each of M bins
Xi ⊆ X . Major actions u0,N
are categorical or diagonal
t
Gaussian as usual. For large X , U, one could also consider
kernel-based parametrizations (Cui et al., 2024).

MDPs, complicating value-based learning.
For this reason, we mainly consider PG methods to solve
M3FC-type MARL problems. We parametrize M3FC MDP
solutions via RL policies π̃ θ with parameters θ, outputting
ξ ∈ Ξ from some compact parameter space Ξ with a Lipschitz map Γ(ξ) = πt′ to LΠ -Lipschitz minor agent decision
rules πt′ (formally, ht = µt ⊗ πt′ ). Assuming the Lipschitzness of the policy network and its gradient in all arguments,
on which there has been a great number of recent literature
(see e.g. Herrera et al. (2023); Araujo et al. (2023) and
references therein), we formulate Assumption 3.

We use two hidden layers of 256 nodes and tanh activations
for the neural networks of the policies. The neural network
policy outputs parameters of a diagonal Gaussian over the
major action u0 and matrices U as discussed above. In the
discrete Beach scenario below, the neural network instead
outputs a categorical distribution using a final softmax layer.
We used no GPUs and around 300,000 CPU core hours on
Intel Xeon Platinum 9242 CPUs. Optimal transport costs
are computed using POT (Flamary et al., 2021). Our M3FC
MDP implementation follows the gym interface (Brockman
et al., 2016), while the implementation of multi-agent RL
as in the following fulfills RLlib interfaces (Liang et al.,
2018). The RL implementations in our work are based on
MARLlib 1.0 (Hu et al., 2023a) (MIT license), which uses
RLlib 1.8 (Liang et al., 2018) (Apache-2.0 license) with
hyperparameters in Table 3, and otherwise default settings.

θ

Assumption 3. The parameter map Γ, joint policy π̃ and
log-gradient ∇θ log π̃ θ (or gradient ∇θ π̃ θ ) are LΓ , Lπ̃ ,
L∇π̃ -Lipschitz and uniformly bounded.
Then, we can apply the PG theorem (Sutton et al., 1999) for
the M3FC MDP. The M3FC MDP (3) essentially substitutes
many-agent systems (1), which are natural approximations
of the M3FC MDP by Theorem 2. Therefore, we show
that M3FMARL (Algorithm 1) – single-agent PG on the
multi-agent M3FC system – approximates the true PG of the
limiting M3FC MDP, in the case of many minor agents. In
other words, M3FMARL solves MARL by approximately
solving the single-agent M3FC MDP using policy gradients.
Theorem 3. Under Assumptions 1, 2 and 3, the approximate
PG of joint policy π̃ θ computed on the finite M3FC system
(1) in Algorithm 1 uniformly tends to the true PG of the
M3FC MDP (3), as N → ∞.

3.3. Comparison to MARL
The M3FMARL algorithm falls into the paradigm of centralized training with decentralized execution (CTDE) (Zhang
et al., 2021), as we sample a single central M3FC MDP action during training, but enable decentralized execution by
sampling πt′ separately on each agent instead. For instance,
when converged to a deterministic M3FC policy (of which
an optimal one is guaranteed to exist by Theorem 1), the
M3FC action is always trivially equal for all agents.

Importantly, the underlying MDP complexity is independent of the number of minor agents. Therefore, we would
expect Algorithm 1 to be able to perform well in M3FCtype problems, possibly compared to straightforward MARL
where each agent is handled separately. Intuitively, for many
agents, the reward signal for any single agent can become
uninformative: A cooperative, “averaged” reward remains
almost unaffected by a single agent’s actions. This wellknown credit assignment issue is therefore solved by the
hierarchical structure of M3FC, as credit is assigned to
M3FC actions, which affect all minor agents at once and
hence receive aggregated credit. Another advantage is that
MFC profits from any advances in single-agent RL.

Since we also consider continuous minor agent action spaces
in our experiments, we compare against PG methods for
MARL. In particular, we firstly consider Independent PPO
(IPPO), as PPO with independent learning (Tan, 1993) and
parameter sharing (Gupta et al., 2017), and secondly also
Multi-Agent PPO (MAPPO) with centralized critics. The
latter has repeatedly shown strong state-of-the-art performance in cooperative MARL (de Witt et al., 2020; Papoudakis et al., 2021; Yu et al., 2022). We also separate
major and minor agent policies for improved performance
of IPPO / MAPPO. For comparison, we use the same observations for the policy input as in M3FMARL. The policy
network architectures match, and the same PPO implementation and hyperparameters are shared with M3FPPO in
Table 3. Minor agents are additionally allowed to observe
their own states. More details can be found in Appendix R.

3.2. Implementation Details
We use the proximal policy optimization (PPO) algorithm
(Schulman et al., 2017) to obtain a M3FC policy πRL , instantiating the major minor mean field PPO (M3FPPO) algorithm as an instance of M3FMARL, Algorithm 1. Other
PG algorithms (A2C, leading to M3FA2C) are also compared in our experiments. We parametrize MFs in P(X ) and
joint distributions in H(µN
t ). In practice, for finite X , U, the
parametrization of P(X ) is immediate by finite-dimensional
vectors µN
t ∈ P(X ). For M3FC actions, consider – in addi6

Major-Minor Mean Field Multi-Agent Reinforcement Learning

4. Experiments

(roads) while minor drones collect packages for urban parcel
delivery (Marinelli et al., 2018). Drones fill up at package
“foraging” areas, and unload near the major agent. Lastly, in
the Potential problem, minor agents can generate a potential landscape, the gradient of which pushes the major agent
– e.g., a large object affected by magnetic active matter (Jin
and Zhang, 2021) – to be delivered to a variable target.

In this section, we demonstrate the performance of M3FPPO
on illustrative, practical problems. Unless noted otherwise,
we use M = 49 bins (M = 7 in Potential), train for around
24 hours, and train M3FPPO on the finite-agent system (1)
with N = 300 minor agents unless noted otherwise (similar
results for less agents in Appendix R). Full descriptions and
additional experiments and discussions are in Appendix R.

4.2. Evaluation
In Figure 5, we see that M3FPPO learning is stable, as
M3FPPO reduces hard-to-analyze MARL to single-agent
RL, avoiding pathologies of MARL such as non-stationarity
of multi-agent learning, or the combinatorial complexity
over numbers of agents. In Figure 6, we find similar success in directly training M3FPPO for small N instead of
transferring from high N . We conclude that M3FPPO remains applicable even with as few as 5 agents. M3FPPO
usually compares well against its A2C variant (M3FA2C)
and IPPO / MAPPO, see Table 2 and Appendix R.2. Meanwhile, IPPO / MAPPO under the same hyperparameters
as M3FPPO (large batch sizes, see Table 3) can be more
unstable and lead to worse results, see Figure 7.

4.1. Problems

Return J(π)

To verify the usefulness of M3FMARL whenever the M3FC
model (1) is accurate, we consider 5 benchmark tasks that
fulfill the M3FC modelling assumptions. To begin, the simple two Gaussian (2G) problem has no major agent and is
equipped with a time-dependent major state: A periodic,
time-variant mixture of two Gaussians µ∗t – the major state
– is noisily observed analogously to µN
t via M = 49 bins.
Minor agents should then track the mixture distribution over
time, which can find application for example in UAV-based
cellular coverage of dynamic users (Mozaffari et al., 2016).
In the Formation problem, we extend such formation control with major agents. In addition to 2G, one added major
agent tracks a moving target. Meanwhile, minor agents
instead track a formation around the dynamic major agent,
see e.g. Yang et al. (2021) for applications. The Beach
bar process is a studied classic (Arthur, 1994; Perrin et al.,
2020), where minor agents minimize their distances to a bar
and additionally avoid crowded areas. Here, the bar moves
on a discrete torus. The Foraging problem is archetypal of
swarm intelligence (Brambilla et al., 2013), and has agents
forage randomly generated foraging areas. In particular,
we can consider the logistics scenario depicted in Figure 1,
where a major package truck moves in a restricted space

−200
0.0

−400

−250

−100
0.5

Steps t

(a) −500

1.0
×108

0.0

Qualitative behavior. In Figure 8, we observe successfully trained behavior in Beach and Foraging: In Beach,
M3FPPO learns to accumulate up to 70% of agents on the
bar, as more agents on the space lead to a suboptimal reduction in rewards. In Foraging, we find that agents successfully
deplete foraging areas shown in the bottom left, moving on
afterwards. Further, M3FPPO successfully learns to form
mixtures of Gaussians in 2G, a Gaussian around a moving
major agent successfully tracking its target in Formation,
and similar success in pushing the major agent towards its
target in Potential, see Appendix R.3.

0.5

Steps t

(b) −600

1.0
×108

0.0

0.5

Steps t

1500
1000
(c) 500
1.0
0
×108

−50
2

Steps t

(d) −100

4
×108

(e)
0

1

Steps t

mean
max

2
×108

Figure 5. Training curves (mean episode return) of M3FPPO (red), with shaded standard deviation, and maximum (blue) over all three
trials (two for Foraging). (a) 2G; (b) Formation; (c) Beach; (d) Foraging; (e) Potential.
Table 2. Comparison of mean episode returns between best trained policies of standard MARL and M3FMARL methods on a system with
N = 20 agents (± 95% confidence interval, for a number of episodes as in Figure 9).

Problem
2G
Formation
Beach
Foraging
Potential

IPPO
-43.9 ± 1.1
-51.1 ± 2.4
-350.3 ± 3.4
735.3 ± 46.4
-27.1 ± 1.4

MAPPO
-26.0 ± 0.5
-101.1 ± 7.1
-342.9 ± 4.7
803.9 ± 54.6
-26.7 ± 1.7
7

M3FA2C
-30.6 ± 0.6
-79.2 ± 3.1
-424.8 ± 5.5
1398.0 ± 57.1
-50.4 ± 5.5

M3FPPO
-22.2 ± 0.56
-63.9 ± 4.2
-303.5 ± 3.4
1479.4 ± 36.3
-31.3 ± 1.3

Return J(π)

Major-Minor Mean Field Multi-Agent Reinforcement Learning
−100
0.0

0.5

1.0
×108

Steps t

0.0

0.5

1.0
×108

Steps t

−50

1000

(b) −600

(a) −500

−200

1500

−400

−250

(c)

0.0

0.5

Steps t

(d) −100

500

1.0
×108

0.0

2.5

5.0
Steps t ×108

(e)
0

1

Steps t

N=5
N=10
N=20

2
×108

Figure 6. Training curves (mean episode return vs. time steps) of M3FPPO, trained on the finite systems with N ∈ {5, 10, 20}. (a) 2G;
(b) Formation; (c) Beach; (d) Foraging; (e) Potential.
Return J(π)

1500
−100

−200

−200

−400

(a)
0

1

2

(c)

0.0

×105

Steps t

−50

1000

(b) −600
0

×106

Steps t

−400

0.5

Steps t

1.0
×106

500
0

5

Steps t

(d) −100
×105

0.0

IPPO
MAPPO
MF

(e)
0.5

Steps t

1.0
×106

Figure 7. Comparing IPPO / MAPPO vs. results of M3FPPO (MF, ours), as in Figure 5 (no maxima, N = 20).

can be more sample-efficient, as each step gives N samples
instead of just one; and (ii) M3FPPO matches or outperforms IPPO and MAPPO, despite having significantly less
control over minor agent actions: All minor agents in a
bin (with similar minor agent states) use the same action
distributions, which suffices for strong results.

Quantitative support of theory. In Figure 9, we transfer
the trained M3FPPO policy to N = 2, . . . , 50, comparing
against the performance in the limit (N = 500). As N
grows, the performance converges to the limit, supporting
Theorem 2 and Corollary 1. Any sufficiently large system
has the same limiting performance as predicted by the theory.
We thus have empirical support for scalability, and also
transferability between varying numbers of minor agents.

Decentralized execution. Lastly, decentralized execution
by agent-wise randomization – i.e. sampling M3FC actions
per agent instead of a single shared, correlated M3FC action
– has little to no effect, and can even marginally improve
performance, see e.g., Beach in Figure 9(c). Figure 9 verifies
the performance of M3FMARL as a CTDE method.

Comparison to MARL. Comparing Figures 5, 7 and Table 2, we see that (i) by experience sharing, standard MARL
(a)

(b)

t=0

(c)

t=3

(d)

t = 25

t = 50

0.06
0.04
0.02
0.00

2

2

(e)

0

t=0

2

y

0
−2
−2

4
0

2

2

0

4

2

0

0

2

x

2

0

t = 20

(f)

(g)

0

2
4

0

2

x

5. Conclusion and Discussion

4
2

0

t = 30

2

0

−2
−2

0.6
0.4
0.2
0.0

0.0

4

2
4

0.4
0.2

0.0

4
0

0.2
0.1

(h)

We have proposed a generalization of MDPs and MFC,
enabling tractable state-of-the-art MARL on general manyagent systems, with both theoretical and empirical support.
Beyond the current model and its optimality guarantees,
one could work on extended optimality conjectures in Appendix Q, refined approximations (Gast and Van Houdt,
2018), and local interactions (Qu et al., 2020b). Algorithmically, M3FC MDP actions H(µ) could move beyond
binning X to gain performance, e.g. via kernels. Lastly,
√
one may try to quantify convergence to the rate O(1/ N )
for non-finite X , as the current proof strategy would need
hard-to-verify or unrealistic dΣ -Lipschitzness.

2
4

0

t = 40

0

−2
−2

0

2

x

−2
−2

0

2

x

Return JN (π)

Figure 8. Qualitative visualization of M3FC in Beach (a-d), Foraging (e-h). (a-d): empirical MF, major agent & target in green;
(e-h): blue / green triangle: major agent / target; green / red dots:
less- / more-than-half encumbered minor agents; purple: current
foraging areas.
−50

−25
−50

(a)
20

40

Num agents N

−300

−100

(b)
20

40

Num agents N

−400

−30

1500
(c)
20

40

Num agents N

1000

(d)
20

40

Num agents N

−40

(e)
20

MF
CE
DE

40

Num agents N

Figure 9. Mean episode return of M3FC policy in finite systems as in Figure 5 over (a-c) 100, (d) 300 or (e) 500 trials (95% confidence
interval shaded). MF: CE, N = 500; CE / DE: centralized / decentralized execution.

8

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Broader Impact

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
Schneider, John Schulman, Jie Tang, and Wojciech
Zaremba. OpenAI gym. arXiv:1606.01540, 2016.

This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.

Theophile Cabannes, Mathieu Laurière, Julien Perolat,
Raphael Marinier, Sertan Girgin, Sarah Perrin, Olivier
Pietquin, Alexandre M Bayen, Eric Goubault, and Romuald Elie. Solving n-player dynamic routing games with
congestion: A mean-field approach. In Proc. AAMAS,
volume 21, pages 1557–1559, 2022.

Acknowledgements
This work has been co-funded by the LOEWE initiative
(Hesse, Germany) within the emergenCITY center, and the
Hessian Ministry of Science and the Arts (HMWK) within
the projects “The Third Wave of Artificial Intelligence - 3AI”
and hessian.AI. The authors acknowledge the Lichtenberg
high performance computing cluster of the TU Darmstadt
for providing computational facilities for the calculations
of this research. We thank anonymous reviewers for their
helpful comments to improve the manuscript.

Peter E Caines and Minyi Huang. Graphon mean field
games and the GMFG equations: ε-Nash equilibria. In
Proc. IEEE CDC, pages 286–292, 2019.
Peter E Caines and Arman C Kizilkale. ϵ-Nash equilibria for
partially observed LQG mean field games with a major
player. IEEE Trans. Automat. Contr., 62(7):3225–3234,
2016.

References

Luciano Campi and Markus Fischer. Correlated equilibria
and mean field games: a simple model. Math. Oper. Res.,
2022.

David F Anderson and Thomas G Kurtz. Continuous time
Markov chain models for chemical reaction networks.
In Design and Analysis of Biomolecular Circuits, pages
3–42. Springer, 2011.

René Carmona. Applications of mean field games in financial engineering and economic theory. arXiv:2012.05237,
2020.

Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexandre Allauzen, and Bin Hu. A unified algebraic perspective
on Lipschitz neural networks. In Proc. ICLR, pages 1–15,
2023.

René Carmona and François Delarue. Probabilistic Theory
of Mean Field Games with Applications I-II. Springer,
2018.

W Brian Arthur. Inductive reasoning and bounded rationality. Am. Econ. Rev., 84(2):406–411, 1994.

René Carmona, François Delarue, and Daniel Lacker. Mean
field games with common noise. Ann. Probab., 44(6):
3740–3803, 2016.

Nicole Bäuerle. Mean field Markov decision processes.
Appl. Math. Optim., 88(1):12, 2023.

René Carmona, Quentin Cormier, and H Mete Soner. Synchronization in a Kuramoto mean field game. Commun.
Partial. Differ. Equ., 48(9):1214–1244, 2023a.

Alain Bensoussan, Jens Frehse, and Phillip Yam. Mean field
games and mean field type control theory, volume 101.
Springer, 2013.

René Carmona, Mathieu Laurière, and Zongjun Tan. Modelfree mean-field reinforcement learning: mean-field MDP
and mean-field Q-learning. Ann. Appl. Probab., 33(6B):
5334–5381, 2023b.

Daniel S Bernstein, Robert Givan, Neil Immerman, and
Shlomo Zilberstein. The complexity of decentralized
control of Markov decision processes. Math. Oper. Res.,
27(4):819–840, 2002.
Patrick Billingsley. Convergence of probability measures.
John Wiley & Sons, 2013.

Kai Cui and Heinz Koeppl. Approximately solving mean
field games via entropy-regularized deep reinforcement
learning. In Proc. AISTATS, pages 1909–1917, 2021.

Ofelia Bonesini, Luciano Campi, and Markus Fischer. Correlated equilibria for mean field games with progressive
strategies. arXiv:2212.01656, 2022.

Kai Cui and Heinz Koeppl. Learning graphon mean field
games and approximate Nash equilibria. In Proc. ICLR,
pages 1–31, 2022.

Manuele Brambilla, Eliseo Ferrante, Mauro Birattari, and
Marco Dorigo. Swarm robotics: A review from the swarm
engineering perspective. Swarm Intell., 7(1):1–41, 2013.

Kai Cui, Anam Tahir, Mark Sinzger, and Heinz Koeppl.
Discrete-time mean field control with environment states.
In Proc. IEEE CDC, pages 5239–5246, 2021.
9

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Kai Cui, Sascha H. Hauck, Christian Fabian, and Heinz
Koeppl. Learning decentralized partially observable mean
field control for artificial collective behavior. In Proc.
ICLR, 2024.

Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Dynamic programming principles for mean-field controls
with learning. Oper. Res., 2023.
Yue Guan, Mohammad Afshari, and Panagiotis Tsiotras.
Zero-sum games between mean-field teams: Reachabilitybased analysis under mean-field sharing. In Proc. AAAI,
volume 38, pages 9731–9739, 2024.

Constantinos Daskalakis, Paul W Goldberg, and Christos H
Papadimitriou. The complexity of computing a Nash
equilibrium. SIAM J. Comput., 39(1):195–259, 2009.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei
Sun, and Shimon Whiteson. Is independent learning
all you need in the Starcraft multi-agent challenge?
arXiv:2011.09533, 2020.

Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. In Proc. NeurIPS, pages 4966–
4976, 2019.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. A
general framework for learning mean-field games. Math.
Oper. Res., 2022.

Ronald A DeVore and George G Lorentz. Constructive
approximation, volume 303. Springer Science & Business
Media, 1993.

Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer.
Cooperative multi-agent control using deep reinforcement
learning. In Proc. AAMAS, pages 66–83, 2017.

Boualem Djehiche, Alain Tcheukam, and Hamidou Tembine. Mean-field-type games in engineering. AIMS Electron. Electr. Eng., 1(1):18–73, 2017.

Onésimo Hernández-Lerma and Jean B Lasserre. Discretetime Markov control processes: basic optimality criteria,
volume 30. Springer Science & Business Media, 2012.

Alex Dunyak and Peter E Caines. Large scale systems and
SIR models: A featured graphon approach. In Proc. IEEE
CDC, pages 6928–6933. IEEE, 2021.

Onésimo Hernández-Lerma and Myriam Muñoz de Ozak.
Discrete-time Markov control processes with discounted
unbounded costs: optimality criteria. Kybernetika, 28(3):
191–212, 1992.

Rémi Flamary, Nicolas Courty, Alexandre Gramfort,
Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras,
Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud,
Hicham Janati, Alain Rakotomamonjy, Ievgen Redko,
Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J.
Sutherland, Romain Tavenard, Alexander Tong, and
Titouan Vayer. POT: Python optimal transport. J. Mach.
Learn. Res., 22(78):1–8, 2021.

Calypso Herrera, Florian Krach, and Josef Teichmann.
Local lipschitz bounds of deep neural networks.
arXiv:2004.13135, 2023.
Siyi Hu, Yifan Zhong, Minquan Gao, Weixun Wang, Hao
Dong, Xiaodan Liang, Zhihui Li, Xiaojun Chang, and
Yaodong Yang. MARLlib: A scalable and efficient multiagent reinforcement learning library. J. Mach. Learn.
Res., 2023a.

Sriram Ganapathi Subramanian, Pascal Poupart, Matthew E
Taylor, and Nidhi Hegde. Multi type mean field reinforcement learning. In Proc. AAMAS, volume 19, pages
411–419, 2020.
Sriram Ganapathi Subramanian, Matthew E Taylor, Mark
Crowley, and Pascal Poupart. Partially observable mean
field reinforcement learning. In Proc. AAMAS, volume 20,
pages 537–545, 2021.

Yuanquan Hu, Xiaoli Wei, Junji Yan, and Hengxi Zhang.
Graphon mean-field control for cooperative multi-agent
reinforcement learning. J. Franklin Inst., 2023b.

Nicolas Gast and Bruno Gaujal. A mean field approach for
optimization in discrete time. Discret. Event Dyn. Syst.,
21(1):63–101, 2011.

Minyi Huang, Roland P Malhamé, and Peter E Caines.
Large population stochastic dynamic games: closed-loop
McKean-Vlasov systems and the Nash certainty equivalence principle. Commun. Inf. Syst., 6(3):221–252, 2006.

Nicolas Gast and Benny Van Houdt. A refined mean field
approximation. ACM SIGMETRICS Perform. Eval. Rev.,
46(1):113–113, 2018.

Dongdong Jin and Li Zhang. Collective behaviors of
magnetic active matter: Recent progress toward reconfigurable, adaptive, and multifunctional swarming micro/nanorobots. Acc. Chem. Res., 55(1):98–109, 2021.

Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Meanfield controls with Q-learning for cooperative MARL:
convergence and complexity analysis. SIAM J. Math.
Data Sci., 3(4):1168–1196, 2021.

Olav Kallenberg. Random measures, theory and applications, volume 1. Springer, 2017.
10

Major-Minor Mean Field Multi-Agent Reinforcement Learning

István Z Kiss, Joel C Miller, and Péter L Simon. Mathematics of Epidemics on Networks: From Exact to Approximate Models, volume 46. Springer, 2017. doi:
10.1007/978-3-319-50806-1.

Mojtaba Nourian and Peter E Caines. ϵ-Nash mean field
game theory for nonlinear stochastic dynamical systems
with major and minor agents. SIAM J. Contr. Optim., 51
(4):3302–3331, 2013.

Jean-Michel Lasry and Pierre-Louis Lions. Mean field
games. Japanese J. Math., 2(1):229–260, 2007.

Mojtaba Nourian, Peter E Caines, Roland P Malhame, and
Minyi Huang. Nash, social and centralized solutions to
consensus problems via mean field control theory. IEEE
Trans. Automat. Contr., 58(3):639–653, 2012.

Mathieu Laurière, Sarah Perrin, Matthieu Geist, and
Olivier Pietquin. Learning mean field games: A survey.
arXiv:2205.12944, 2022.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L
Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.
In Proc. NeurIPS, pages 27730–27744, 2022.

Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz,
Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for distributed
reinforcement learning. In Proc. ICML, pages 3053–3062,
2018.

Georgios Papoudakis, Filippos Christianos, Lukas Schäfer,
and Stefano V Albrecht. Benchmarking multi-agent deep
reinforcement learning algorithms in cooperative tasks.
In Proc. NeurIPS Track Datasets Benchmarks, 2021.

Xin Liu, Honghao Wei, and Lei Ying. Scalable and sample
efficient distributed policy gradient algorithms in multiagent networked systems. arXiv:2212.06357, 2022.

Kalyanapuram Rangachari Parthasarathy. Probability measures on metric spaces, volume 352. American Mathematical Soc., 2005.

Mario Marinelli, Leonardo Caggiani, Michele Ottomanelli,
and Mauro Dell’Orco. En route truck–drone parcel delivery for optimal vehicle routing strategies. IET Intell.
Transp. Syst., 12(4):253–261, 2018.

Barna Pásztor, Andreas Krause, and Ilija Bogunovic. Efficient model-based multi-agent mean-field reinforcement
learning. Trans. Mach. Learn. Res., 2023.

Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal,
and Satish V Ukkusuri. On the approximation of cooperative heterogeneous multi-agent reinforcement learning
(MARL) using mean field control (MFC). J. Mach. Learn.
Res., 23(129):1–46, 2022.

Julien Pérolat, Sarah Perrin, Romuald Elie, Mathieu Laurière, Georgios Piliouras, Matthieu Geist, Karl Tuyls, and
Olivier Pietquin. Scaling mean field games by online
mirror descent. In Proc. AAMAS, volume 21, pages 1028–
1037, 2022.

Washim Uddin Mondal, Vaneet Aggarwal, and Satish
Ukkusuri. Mean-field control based approximation of
multi-agent reinforcement learning in presence of a nondecomposable shared global state. Trans. Mach. Learn.
Res., 2023. ISSN 2835-8856.

Sarah Perrin, Julien Pérolat, Mathieu Laurière, Matthieu
Geist, Romuald Elie, and Olivier Pietquin. Fictitious
play for mean field games: Continuous time analysis
and applications. In Proc. NeurIPS, volume 33, pages
13199–13213, 2020.

Médéric Motte and Huyên Pham. Mean-field Markov decision processes with common noise and open-loop controls. Ann. Appl. Probab., 32(2):1421–1458, 2022.

Sarah Perrin, Mathieu Laurière, Julien Pérolat, Romuald
Élie, Matthieu Geist, and Olivier Pietquin. Generalization
in mean field games by learning master policies. In Proc.
AAAI, volume 36, pages 9413–9421, 2022.

Médéric Motte and Huyên Pham. Quantitative propagation
of chaos for mean field Markov decision process with
common noise. Electron. J. Probab., 28:1–24, 2023.

Huyên Pham and Xiaoli Wei. Bellman equation and viscosity solutions for mean-field stochastic control problem.
ESAIM Contr. Optim. Calc. Var., 24(1):437–461, 2018.

Mohammad Mozaffari, Walid Saad, Mehdi Bennis, and
Mérouane Debbah. Efficient deployment of multiple
unmanned aerial vehicles for optimal wireless coverage.
IEEE Commun. Lett., 20(8):1647–1650, 2016.

Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning for networked
systems with average reward. In Proc. NeurIPS, volume 33, pages 2074–2086, 2020a.

Paul Muller, Mark Rowland, Romuald Elie, Georgios
Piliouras, Julien Perolat, Mathieu Laurière, Raphael
Marinier, Olivier Pietquin, and Karl Tuyls. Learning
equilibria in mean-field games: Introducing mean-field
PSRO. In Proc. AAMAS, volume 20, page 926–934, 2021.

Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for multi-agent
networked systems. In Proc. Learn. Dyn. Contr., pages
256–266, 2020b.
11

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Naci Saldi, Tamer Başar, and Maxim Raginsky. Markov–
Nash equilibria in mean-field games with discounted cost.
SIAM J. Contr. Optim., 56(6):4256–4287, 2018.

Ming Tan. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In Proc. ICML, pages 330–337,
1993.

Naci Saldi, Tamer Başar, and Maxim Raginsky. Partiallyobserved discrete-time risk-sensitive mean-field games.
In Proc. IEEE CDC, pages 317–322, 2019.

Rinel Foguen Tchuendom, Peter E Caines, and Minyi
Huang. Critical nodes in graphon mean field games. In
Proc. IEEE CDC, pages 166–170. IEEE, 2021.

Sina Sanjari and Serdar Yüksel. Optimal solutions to
infinite-player stochastic teams and mean-field teams.
IEEE Trans. Automat. Contr., 66(3):1071–1086, 2020.

Cédric Villani. Optimal transport: old and new, volume
338. Springer, 2009.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,
Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko
Georgiev, et al. Grandmaster level in StarCraft II using
multi-agent reinforcement learning. Nature, 575(7782):
350–354, 2019.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert,
Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur
Guez, Edward Lockhart, Demis Hassabis, Thore Graepel,
et al. Mastering atari, go, chess and shogi by planning
with a learned model. Nature, 588(7839):604–609, 2020.

Minghui Wu, Xingmin Wang, Yafeng Yin, Henry Liu, Ben
Wang, Jerome P Lynch, et al. Leveraging connected
and automated vehicles for participatory traffic control.
Technical report, University of Michigan. Center for Connected and Automated Transportation, 2023.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv:1707.06347, 2017.
Nevroz Şen and Peter E Caines. Mean field games with
partially observed major player and stochastic mean field.
In Proc. IEEE CDC, pages 2709–2715, 2014.

Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan
Zhang, and Jun Wang. Mean field multi-agent reinforcement learning. In Proc. ICML, pages 5571–5580, 2018.

Nevroz Şen and Peter E Caines. Mean field game theory
with a partially observed major agent. SIAM J. Contr.
Optim., 54(6):3174–3224, 2016.

Yue Yang, Yang Xiao, and Tieshan Li. Attacks on formation
control for multiagent systems. IEEE Trans. Cybern., 52
(12):12805–12817, 2021.

Nevroz Şen and Peter E Caines. Mean field games with
partial observation. SIAM J. Contr. Optim., 57(3):2064–
2091, 2019.

Batuhan Yardim, Semih Cayci, Matthieu Geist, and Niao
He. Policy mirror ascent for efficient and independent
learning in mean field games. In Proc. ICML, pages
39722–39754. PMLR, 2023.

Hamid Shiri, Jihong Park, and Mehdi Bennis. Massive autonomous UAV path planning: A neural network based
mean-field game theoretic approach. In Proc. IEEE
GLOBECOM, pages 1–6. IEEE, 2019.

Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao,
Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of PPO in cooperative multi-agent games.
In Proc. NeurIPS Datasets and Benchmarks, 2022.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
Daan Wierstra, and Martin Riedmiller. Deterministic
policy gradient algorithms. In Proc. ICML, pages 387–
395. PMLR, 2014.

Kaiqing Zhang, Zhuoran Yang, and Tamer Başar. Multiagent reinforcement learning: A selective overview of
theories and algorithms. In Kyriakos G. Vamvoudakis,
Yan Wan, Frank L. Lewis, and Derya Cansever, editors,
Handbook of Reinforcement Learning and Control, pages
321–384. Springer International Publishing, Cham, 2021.

Sriram Ganapathi Subramanian, Matthew E Taylor, Mark
Crowley, and Pascal Poupart. Decentralized mean field
games. In Proc. AAAI, volume 36, pages 9439–9447,
2022.
Richard S Sutton, David McAllester, Satinder Singh, and
Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proc.
NIPS, pages 1057–1063, 1999.
Alain-Sol Sznitman. Topics in propagation of chaos. In
Ecole d’été de probabilités de Saint-Flour XIX—1989,
pages 165–251. Springer, 1991.
12

Major-Minor Mean Field Multi-Agent Reinforcement Learning

A. Related Work
In this section, we provide additional context on related works. Since the introduction of MFGs in continuous and discrete
time (Huang et al., 2006; Lasry and Lions, 2007; Saldi et al., 2018), MFGs have been studied in various forms, ranging
from partially observed systems (Saldi et al., 2019; Şen and Caines, 2019) over learning-based solutions (Guo et al., 2019;
Perrin et al., 2020; Cui and Koeppl, 2021; Guo et al., 2022; Pérolat et al., 2022; Perrin et al., 2022; Yardim et al., 2023) on
graphs (Caines and Huang, 2019; Tchuendom et al., 2021; Cui and Koeppl, 2022; Hu et al., 2023b) to considering correlated
equilibria (Muller et al., 2021; Campi and Fischer, 2022; Bonesini et al., 2022).
While many works focus on non-cooperative settings with self-interested agents, this can run counter to the goal of
engineering many-agent behavior, e.g., achieving cooperative behavior in swarms of drones. Instead, we focus on the related
setting of cooperative MFC (Pham and Wei, 2018; Gu et al., 2023; Mondal et al., 2022), see also work on differential
(Carmona and Delarue, 2018), static (Sanjari and Yüksel, 2020), or discrete-time deterministic MFC (Gast and Gaujal,
2011). For the unfamiliar reader, we point towards many extensive surveys on the topic of mean field systems (Bensoussan
et al., 2013; Carmona and Delarue, 2018; Laurière et al., 2022).
In general comparison, another well-known line of mean field MARL (Yang et al., 2018; Ganapathi Subramanian et al.,
2020; 2021; Subramanian et al., 2022) focuses on approximating the influence of other agents on any particular agent by
their average actions. Relatedly, some MARL algorithms introduce approximations over agent neighborhoods based on
exponential decay (Qu et al., 2020b;a; Liu et al., 2022). In contrast, MFC assumes dependence on the entire distribution of
agents and not, e.g., pairwise terms for each neighbor, per agent.

B. Deterministic Mean Field Control
In the following, we provide proofs that were omitted in the main text. To begin, in this section we recap standard deterministic MFC. Here, our general proof technique is introduced. It generalizes to the M3FC case and allows approximation
properties and dynamic programming principles beyond finite spaces and Lipschitz continuity assumptions in compact
spaces, for MFC models under simple continuity. In standard MFC, we have the model without major agents,
N
ui,N
∼ πt (ui,N
| xi,N
t
t
t , µt ),

i,N
i,N
i,N
N
xi,N
t+1 ∼ p(xt+1 | xt , ut , µt )

(5)
(6)

while in the limit, we have the MF evolution
µt+1 = T (µt , µt ⊗ πt (µt )) :=

ZZ

p(· | x, u, µt )πt (du | x, µt )µt (dx)

(7)

µt+1 = T (µt , ht )

(8)

and MFC system
ht ∼ π̂t (ht | µt ),
P∞
with objective J(π̂) = E [ t=0 γ t r(µt )].
Dynamic Programming and Propagation of Chaos We may solve the hard finite-agent system (5) near-optimally by
instead solving the MFC MDP, allowing direct application of single-agent RL to the MFC MDP with approximate optimality
in large systems. Mild continuity assumptions are required.
Assumption B.1. The transition kernel p and reward r are continuous.
Assumption B.2. The considered class of policies Π is equi-Lipschitz, i.e. there exists LΠ > 0 such that for all t and π ∈ Π,
πt ∈ P(U)X ×P(X ) is LΠ -Lipschitz.
We note that Assumption B.1 holds true in studied finite spaces, if each transition matrix entry of P is continuous in the
|X |-dimensional MF vector on the simplex (but not necessarily Lipschitz as in (Gu et al., 2021; Mondal et al., 2022), the
conditions of which we relax for deterministic MFC).
We show a dynamic programming principle (Hernández-Lerma and Lasserre, 2012) to solve for and show existence of
a deterministic, stationary optimal policy via the value function V ∗ as the fixed point of the Bellman equation V ∗ (µ) =
maxh∈H(µ) r(µ) + γV ∗ (T (µ, h)).
13

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Theorem B.1. Under Assumptions B.1, there exists an optimal stationary, deterministic policy π̂ for (8), with π̂(µ) ∈
arg maxh∈H(µ) r(µ) + γV ∗ (T (µ, h)).
This DPP can be used for computing solutions or to show optimality of stationary policies and existence of an optimum.
Next, we show propagation of chaos (Sznitman, 1991). Here, prior proof techniques (Gu et al., 2021; Mondal et al., 2022)
are extended by our approach from finite to general compact spaces.
Theorem B.2. Fix any family of equicontinuous functions F ⊆ RP(X ) . Under Assumptions B.1 and B.2, the em−1
pirical MF converges
 weakly,
 uniformly over f ∈ F, π ∈ Π, π̂ = Φ (π), to the limiting MF at all times t ∈ N,
N
supπ∈Π supf ∈F E f (µt ) − E [f (µt )] → 0.
Importantly, propagation of chaos allows one to show approximate optimality of MFC policies in the large finite control
problem, which is of practical relevance for solving many-agent problems.
Corollary B.1. Under Assumptions B.1 and B.2, an optimal deterministic MFC policy π ∗ ∈ arg maxπ̂ J(π̂) yields ε-optimal
finite-agent policy Φ(π ∗ ) ∈ Π, J N (Φ(π ∗ )) ≥ supπ∈Π J N (π) − ε, with ε → 0 as N → ∞.

C. Continuity of MF dynamics
First, we find continuity of the MFC dynamics T , which is used in the following proofs.
Lemma C.1. Under Assumption B.1, we have T (µn , νn ) → T (µ, ν) whenever (µn , νn ) → (µ, ν),
Proof. To show T (µn , νn ) → T (µ, ν), consider any Lipschitz and bounded f with Lipschitz constant Lf , then
Z
f d(T (µn , νn ) − T (µ, ν))
ZZZ
ZZZ
=
f (x′ )p(dx′ | x, u, µn )νn (dx, du) −
f (x′ )p(dx′ | x, u, µ)ν(dx, du)
ZZ Z
Z
≤
f (x′ )p(dx′ | x, u, µn ) − f (x′ )p(dx′ | x, u, µ) νn (dx, du)
ZZZ
+
f (x′ )p(dx′ | x, u, µ)(νn (dx, du) − ν(dx, du))
≤

sup
x∈X ,u∈U

ZZZ
+

Lf W1 (p(· | x, u, µn ), p(· | x, u, µ))
f (x′ )p(dx′ | x, u, µ)(νn (dx, du) − ν(dx, du)) → 0

for the first term by 1-Lipschitzness of Lff and Assumption B.1 (with compactness implying the uniform continuity), and for
RR
the second by νn → ν and from continuity by the same argument of (x, u) 7→
f (x′ )p(dx′ | x, u, µ).

D. Proof of Theorem B.1
Proof. The MFC MDP fulfills (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.1. Here, we use (Hernández-Lerma
and Lasserre, 2012), Condition 3.3.4(b1) instead of (b2), see also alternatively (Hernández-Lerma and Muñoz de Ozak,
1992).
More specifically, for (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.1(a), the cost function −r is continuous by
Assumption B.1, therefore also bounded by compactness of P(X ), and finally also inf-compact on the state-action space of
the MFC MDP, since for any µ ∈ P(X ) the set {h ∈ H(µ) | −r(µ) ≤ c} is trivially given by H(µ) whenever −r(µ) ≤ c,
and ∅ otherwise. Here, we show that H(µ) ⊆ P(X × U) is a closed subset of the compact space P(X × U) and therefore
also
first that two measures µ, µ′ ∈ P(X ) are equal if and only if for all continuous and bounded f we have
R compact.
R Note
′
f dµ = f dµ , see e.g. (Billingsley, 2013), Theorem 1.3.
Therefore, as H(µ) is defined by its first marginal µ, H(µ) can be written as an intersection

Z
Z
\ 
H(µ) =
h ∈ P(X × U)
f ⊗ 1 dh = f dµ
f ∈Cb (X )

14

Major-Minor Mean Field Multi-Agent Reinforcement Learning

R
R
of closed sets: Since h 7→ f ⊗ 1 dh is continuous, its preimage of the closed set { f dµ} is closed. Here, ⊗ denotes the
tensor product of f with the function 1 equal one, i.e. f ⊗ 1 is the map (x, u) 7→ f (x).
Similarly, for (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.1(b), the transition dynamics T are weakly continuous,
as
R for any (µn , νn ) → (µ, ν) ∈ P(X ) × P(X R× U) we have T (µn , νn ) → T (µ, ν) by Lemma C.1 and therefore
f dδT (µn ,νn ) = f (T (µn , νn )) → f (T (µ, ν)) = f dδT (µ,ν) for any continuous and bounded f : P(X ) → R.
Furthermore, the MFC MDP fulfills (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.2 by boundedness of r from
Assumption B.1. Therefore, the desired statement follows from (Hernández-Lerma and Lasserre, 2012), Theorem 4.2.3.

E. Proof of Theorem B.2
Proof. Note that we can also show
L1 convergence statement with the absolute value inside of the
 the slightly stronger

expectation, supπ∈Π supf ∈F E f (µN
→ 0, but since this statement is only true for deterministic MFC, we
t ) − f (µt )
avoid it here to later extend our proof directly to M3FC.


The statement supπ∈Π supf ∈F E f (µN
t ) − E [f (µt )] → 0 is shown inductively over t ≥ 0. At time t = 0, it holds by
the weak LLN argument, see also the first term below. Assuming the statement at time t, then for time t + 1 we have


sup sup E f (µN
t+1 ) − f (µt+1 )
π∈Π f ∈F



N
N
N
≤ sup sup E f (µN
t+1 ) − f (T (µt , µt ⊗ πt (µt )))

(9)

π∈Π f ∈F



N
N
+ sup sup E f (T (µN
t , µt ⊗ πt (µt ))) − f (µt+1 ) .

(10)

π∈Π f ∈F

For the first term (9), first note that by compactness of P(X ), F is uniformly equicontinuous, and hence admits a nondecreasing, concave (as in (DeVore and Lorentz, 1993), Lemma 6.1) modulus of continuity ωF : [0, ∞) → [0, ∞) where
ωF (x) → 0 as x → 0 and |f (µ) − f (ν)| ≤ ωF (W1 (µ, ν)) for all f ∈ F.
We also have uniform equicontinuity of F with respect to the space (P(X ), dΣ ) instead of (P(X ), W1 ), as the identity map
id : (P(X ), dΣ ) → (P(X ), W1 ) is uniformly continuous (as both dΣ and W1 metrize the topology of weak convergence,
and P(X ) is compact), and therefore there exists a modulus of continuity ω̃ for the identity map such that for any
µ, ν ∈ (P(X ), dΣ ), by the prequel
|f (µ) − f (ν)| ≤ ωF (W1 (id µ, id ν)) ≤ ωF (ω̃(dΣ (µ, ν)))
with ω̃F := ωF ◦ ω̃, which can be replaced by its least concave majorant (again as in (DeVore and Lorentz, 1993),
Lemma 6.1).
Therefore, by Jensen’s inequality, for (9) we obtain


N
N
N
E f (µN
t+1 ) − f (T (µt , µt ⊗ πt (µt )))


N
N
N
≤ E ω̃F (dΣ (µN
t+1 , T (µt , µt ⊗ πt (µt ))))


N
N
N
≤ ω̃F E dΣ (µN
t+1 , T (µt , µt ⊗ πt (µt )))
i,N
irrespective of π, f via concavity of ω̃F . Introducing for readability xN
t ≡ {xt }i∈[N ] , we then obtain


N
N
N
E dΣ (µN
t+1 , T (µt , µt ⊗ πt (µt )))
Z

∞
X
N
N
N
=
2−m E
fm d(µN
−
T
(µ
,
µ
⊗
π
(µ
)))
t t
t+1
t
t
m=1


Z

N
N
N
N
≤ sup E ExN
fm d(µt+1 − T (µt , µt ⊗ πt (µt )))
,
t
m≥1

and by the following weak LLN argument, for the squared term and any fm
Z
2
N
N
N
N
ExN
f
d(µ
−
T
(µ
,
µ
⊗
π
(µ
)))
m
t t
t+1
t
t
t
15

Major-Minor Mean Field Multi-Agent Reinforcement Learning

"

N

i
h
1 X
fm (xi,N
fm (xi,N
t+1 )
t+1 ) − ExN
t
N i=1

= ExN
t

#2


N 
h
i 2
X
1
i,N
i,N


≤ ExN
fm (xt+1 )
fm (xt+1 ) − ExN
t
t
N i=1


=


N
h
i2 
1 X
4
i,N
i,N
N
N
f
(x
)
E
f
(x
)
−
E
≤
→0
m
m
xt
xt
t+1
t+1
2
N i=1
N

N
by bounding |fm | ≤ 1, as the cross-terms are zero by conditional independence of xi,N
t+1 given xt . By the prequel, the term
(9) hence converges to zero.

For the second term (10), we have


N
N
sup sup E f (T (µN
t , µt ⊗ πt (µt ))) − f (µt+1 )

π∈Π f ∈F



N
N
= sup sup E f (T (µN
t , µt ⊗ πt (µt ))) − f (T (µt , µt ⊗ πt (µt )))
π∈Π f ∈F



≤ sup sup E g(µN
t ) − g(µt ) → 0
π∈Π g∈G

by the induction assumption, where we defined g = f ◦ T̃ πt from the class G of equicontinuous functions with modulus of
continuity ωG := ωF ◦ ωT , where ωT denotes the uniform modulus of continuity of µt 7→ T̃ πt (µt ) := T (µt , µt ⊗ πt (µt )))
over all policies π. Here, this equicontinuity of {T̃ πt }π∈Π follows from Lemma C.1 and the equicontinuity of functions
µt 7→ µt ⊗ πt (µt ) due to uniformly Lipschitz Π as we show in the following, completing the proof by induction:
Consider µn → µ ∈ P(X ), then we have
sup W1 (µn ⊗ πt (µn ), µ ⊗ πt (µ))
Z
= sup sup
f ′ d(µn ⊗ πt (µn ) − µ ⊗ πt (µ))

π∈Π

π∈Π ∥f ′ ∥Lip ≤1

≤ sup

ZZ

f ′ (x, u)(πt (du | x, µn ) − πt (du | x, µ))µn (dx)

sup

π∈Π ∥f ′ ∥Lip ≤1

ZZ
+ sup

sup

π∈Π ∥f ′ ∥

Lip ≤1

f ′ (x, u)πt (du | x, µ)(µn (dx) − µ(dx))

where for the first term
ZZ
sup

sup

π∈Π ∥f ′ ∥

Lip ≤1

≤ sup

f ′ (x, u)(πt (du | x, µn ) − πt (du | x, µ))µn (dx)
Z Z

sup

π∈Π ∥f ′ ∥Lip ≤1

≤ sup

f ′ (x, u)(πt (du | x, µn ) − πt (du | x, µ)) µn (dx)
Z

sup

sup

π∈Π ∥f ′ ∥Lip ≤1 x∈X

f ′ (x, u)(πt (du | x, µn ) − πt (du | x, µ))

= sup sup W1 (πt (· | x, µn ), πt (· | x, µ))
π∈Π x∈X

≤ LΠ W1 (µn , µ) → 0
by Assumption B.2, and similarly for the second by first noting 1-Lipschitzness of x 7→
Z

f ′ (y, u)
πt (du | y, µ) −
LΠ + 1

Z

f ′ (x, u)
πt (du | x, µ)
LΠ + 1
16

R f ′ (x,u)

LΠ +1 πt (du | x, µ), as for y ̸= x

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Z ′
f ′ (y, u) − f ′ (x, u)
f (x, u)
≤
πt (du | y, µ) +
(πt (du | y, µ) − πt (du | x, µ))
LΠ + 1
LΠ + 1
1
1
≤
d(y, x) +
W1 (πt (· | y, µ), πt (· | x, µ))
LΠ + 1
LΠ + 1


LΠ
1
+
d(x, y)
≤
LΠ + 1 LΠ + 1
Z

(11)

Π
with LΠ1+1 + LL
= 1 ≤ 1, and therefore again
Π +1

ZZ
sup

f ′ (x, u)πt (du | x, µ)(µn (dx) − µ(dx))

sup

π∈Π ∥f ′ ∥Lip ≤1

ZZ
= sup

sup

(LΠ + 1)

π∈Π ∥f ′ ∥Lip ≤1

f ′ (x, u)
πt (du | x, µ)(µn (dx) − µ(dx))
LΠ + 1

≤ (LΠ + 1)W1 (µn , µ) → 0.
This completes the proof by induction.

F. Proof of Corollary B.1
Proof. First, we show that from uniform convergence in Theorem B.2, the finite-agent objectives converge uniformly to the
MFC limit.
Lemma F.1. Under Assumptions B.1 and B.2, the finite-agent objective converges uniformly to the MFC limit,
sup J N (π) − J(Φ−1 (π)) → 0.

(12)

π∈Π



P∞
γT
Proof. For any ε > 0, choose time T ∈ N such that t=T γ t E r(µN
≤ 1−γ
maxµ 2|r(µ)| < 2ε . By
t ) − r(µt )

PT −1 t  N
Theorem B.2, t=0 γ E r(µt ) − r(µt ) < 2ε for sufficiently large N . The result follows.
■
The approximate optimality of MFC solutions in the finite system follows immediately: By Lemma F.1, we have
J N (Φ(π ∗ )) − sup J N (π) = inf (J N (π ∗ ) − J N (π))
π∈Π

π∈Π

∗

≥ inf (J (Φ(π )) − J(π ∗ )) + inf (J(π ∗ ) − J(Φ−1 (π))) + inf (J(Φ−1 (π)) − J N (π))
N

π∈Π

π∈Π

π∈Π

ε
ε
≥ − + 0 − = −ε
2
2

for sufficiently large N , where the second term is zero by optimality of π ∗ in the MFC problem.

G. Stochastic Mean Field Control with Common Noise and Major States
For convenience, we also restate the results for MFC with major states, or common noise. We have the finite MFC system
with major states
0,N
ui,N
∼ πt (ui,N
| xi,N
, µN
t
t
t , xt
t ),

i,N
i,N
i,N
0,N
xi,N
, µN
t ),
t+1 ∼ p(xt+1 | xt , ut , xt

and objective J N (π) = E
states analogous to (8),

(13a)
0,N
0 0,N
x0,N
, µN
t )
t+1 ∼ p (xt+1 | xt

(13b)

hP
∞

i
0,N
t
N
γ
r(x
,
µ
)
analogous to (5), with the corresponding limiting MFC MDP with major
t
t
t=0

ht ∼ π̂t (ht | x0t , µt ), µt+1 = T (x0t , µt , ht ), x0t+1 ∼ p0 (x0t+1 | x0t , µt )
P∞ t 0

RR
0
with objective J(π̂) = E
p(· | x, u, x0 , µ)h(dx, du).
t=0 γ r(xt , µt ) , where T (x , µ, h) :=
17

(14)

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Assumption G.1. The transition kernels p, p0 and rewards r are Lipschitz continuous with constants Lp , Lp0 , Lr .
Assumption G.2. The class of policies Π are equi-Lipschitz, i.e. there exists LΠ > 0 such that for all t and π ∈ Π,
πt ∈ P(U)X ×P(X ) is LΠ -Lipschitz.
Theorem G.1. Under Assumption G.1, there exists an optimal stationary, deterministic policy π̂ for the MFC MDP (14)
by choosing π̂(x0 , µ) from the maximizers of arg maxh∈H(µ) r(x0 , µ) + γEy0 ∼p0 (y0 |x0 ,µ) V ∗ (y 0 , T (x0 , µ, h)), with V ∗
the unique fixed point of the Bellman equation V ∗ (x0 , µ) = maxh∈H(µ) r(x0 , µ) + γEy0 ∼p0 (y0 |x0 ,µ) V ∗ (y 0 , T (x0 , µ, h))
(value function).
0
Theorem G.2. Fix any family of equi-Lipschitz functions F ⊆ RX ×P(X ) with shared Lipschitz constant LF for all f ∈ F.
0,N
0
Under Assumption G.1, the random variable (xt , µN
t ) converges weakly, uniformly over F, Π, to (xt , µt ) at all times
t ∈ N,
h
i
N
0
sup sup E f (x0,N
,
µ
)
−
f
(x
,
µ
)
→ 0.
(15)
t
t
t
t
π∈Π f ∈F

Corollary G.1. Under Assumptions G.1 and G.2, optimal deterministic MFC policies π ∗ ∈ arg maxπ J(π) result in
ε-optimal policies Φ(π ∗ ) in the finite-agent problem with ε → 0 as N → ∞,
J N (Φ(π ∗ )) ≥ sup J N (π) − ε.

(16)

π∈Π

The proofs and interpretation are directly analogous to the M3FC case and the following proofs, by leaving out the major
agent actions, or alternatively using the M3FC results with a trivial singleton major action space, |U 0 | = 1.

H. Proof of Theorem 1
Proof. The proof is analogous to Appendix D by first showing the continuity of T (proof further below).
Lemma H.1. Under Assumption 1, for any sequence (x0n , u0n , µn , νn ) → (x0 , u0 , µ, ν) ∈ X 0 × U 0 × P(X ) × P(X × U),
we have T (x0n , u0n , µn , νn ) → T (x0 , u0 , µ, ν).
For (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.1(a), the cost function −r is continuous by Assumption 1,
therefore also bounded by compactness of X 0 × P(X ), and finally also inf-compact on the state-action space of the
M3FC MDP, since for any (x0 , µ) ∈ X 0 × P(X ) the set {(h, u0 ) ∈ H(µ) × U 0 | −r(x0 , u0 , µ) ≤ c} is given by
H(µ) × r̃−1 ((−∞, c]), where we defined r̃(u0 ) := −r(x0 , u0 , µ). Note that H(µ) is compact by the same argument as in
Appendix D, while r̃ is continuous by Assumption 1 and therefore its preimage of the closed set (−∞, c] is compact.
For (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.1(b), consider any continuous and bounded f : X 0 ×P(X ) → R.
The continuity is uniform by compactness. Hence, supx′ ∈X 0 |f (x′ , µ′n ) − f (x′ , µ′ )| → 0 as µ′n → µ′ ∈ P(X ). Thus,
whenever (x0n , u0n , µn , νn ) → (x0 , u0 , µ, ν) ∈ X 0 × U 0 × P(X ) × P(X × U), we have
ZZ
ZZ
f (x′ , µ) δTn∗ (dµ′ ) p0 (dx′ | x0n , u0n , µn ) −
f (x′ , µ) δT ∗ (dµ′ ) p0 (dx′ | x0 , u0 , µ)
Z
Z
=
f (x′ , Tn∗ ) p0 (dx′ | x0n , u0n , µn ) − f (x′ , T ∗ ) p0 (dx′ | x0 , u0 , µ)
Z
Z
≤
f (x′ , Tn∗ ) p0 (dx′ | x0n , u0n , µn ) − f (x′ , T ∗ ) p0 (dx′ | x0n , u0n , µn )
Z
Z
+
f (x′ , T ∗ ) p0 (dx′ | x0n , u0n , µn ) − f (x′ , T ∗ ) p0 (dx′ | x0 , u0 , µ)
≤ sup |f (x′ , Tn∗ ) − f (x′ , T ∗ )|
x′ ∈X 0

Z
+

f˜(x′ ) p0 (dx′ | x0n , u0n , µn ) −

Z

f˜(x′ ) p0 (dx′ | x0 , u0 , µ) → 0

for the first term by the prequel where Tn∗ := T (x0n , u0n , µn , νn ) → T ∗ := T (x0 , u0 , µ, ν) by Lemma H.1, and for the
second term by applying Assumption 1 to f˜(x′ ) := f (x′ , T ∗ ). This shows weak continuity of the dynamics.
Furthermore, the M3FC MDP fulfills (Hernández-Lerma and Lasserre, 2012), Assumption 4.2.2 by boundedness of r from
Assumption 1. Therefore, the desired statement follows from (Hernández-Lerma and Lasserre, 2012), Theorem 4.2.3.
18

Major-Minor Mean Field Multi-Agent Reinforcement Learning

I. Proof of Lemma H.1
Proof. To show T (x0n , u0n , µn , νn ) → T (x0 , u0 , µ, ν), consider any Lipschitz and bounded f with Lipschitz constant Lf ,
then
Z
f d(T (x0n , u0n , µn , νn ) − T (x0 , u0 , µ, ν))
ZZZ

=
f (x′ ) p(dx′ | x, u, x0n , u0n , µn )νn (dx, du) − p(dx′ | x, u, x0 , u0 , µ)ν(dx, du)
ZZ Z
Z
′
′
0
0
≤
f (x )p(dx | x, u, xn , un , µn ) − f (x′ )p(dx′ | x, u, x0 , u0 , µ) νn (dx, du)
ZZZ
+
f (x′ )p(dx′ | x, u, x0 , u0 , µ)(νn (dx, du) − ν(dx, du))
≤

sup
x∈X ,u∈U

ZZZ
+

Lf W1 (p(· | x, u, x0n , u0n , µn ), p(· | x, u, x0 , u0 , µ))
f (x′ )p(dx′ | x, u, x0 , u0 , µ)(νn (dx, du) − ν(dx, du)) → 0

for the first term by 1-Lipschitzness of Lff and Assumption 1 (with compactness implying the uniform continuity), and for
RR
the second by νn → ν and continuity of (x, u) 7→
f (x′ )p(dx′ | x, u, x0 , u0 , µ) by the same argument.

J. Proof of Theorem 2
h
i
0
0
Proof. The statement supf,π,π0 E f (x0,N
, u0,N
, µN
is shown inductively over t ≥ 0. At time t = 0,
t
t
t ) − f (xt , ut , µt )
it holds by the weak LLN argument, see also the first term below. Assuming the statement at time t, then for time t + 1 we
have
h
i
0,N
N
0
0
sup
sup E f (x0,N
t+1 , ut+1 , µt+1 ) − f (xt+1 , ut+1 , µt+1 )
(π,π 0 )∈Π×Π0 f ∈F

h
i
0,N
0,N
0,N
N
N
≤ sup sup E f (x0,N
t+1 , ut+1 , µt+1 ) − f (xt+1 , ut+1 , µ̂t+1 )

(17)

π,π 0 f ∈F

h
i
0,N
N
0
0
+ sup sup E f (x0,N
,
u
,
µ̂
)
−
f
(x
,
u
,
µ
)
t+1
t+1
t+1
t+1
t+1
t+1

(18)

π,π 0 f ∈F

where for readability, we again write πt (x0t , µt ) := πt (· | ·, x0t , µt ) and introduce the random variable
0,N
0,N
N
µ̂N
, u0,N
, µN
, µN
t
t+1 := T (xt
t , µt ⊗ πt (xt
t )).

By compactness of X 0 × U 0 × P(X ), F is uniformly equicontinuous, and hence admits a non-decreasing, concave (as in
(DeVore and Lorentz, 1993), Lemma 6.1) modulus of continuity ωF : [0, ∞) → [0, ∞) where ωF (x) → 0 as x → 0 and
|f (x, u, µ) − f (x′ , u′ , ν)| ≤ ωF (d(x, x′ ) + d(u, u′ ) + W1 (µ, ν)) for all f ∈ F, and analogously there exists such ω̃F with
respect to (P(X ), dΣ ) instead of (P(X ), W1 ) as in Appendix E.
i,N
For the first term (17), let xN
t ≡ {xt }i∈[N ] . Then, by the weak LLN argument,
h
i
0,N
0,N
0,N
N
N
sup sup E f (x0,N
t+1 , ut+1 , µt+1 ) − f (xt+1 , ut+1 , µ̂t+1 )
π,π 0 f ∈F



N
≤ sup E ω̃F (dΣ (µN
t+1 , µ̂t+1 ))
π,π 0

∞
X

≤ sup ω̃F
π,π 0

m=1


≤ sup ω̃F
π,π 0

!
−m

2

E



N
µN
t+1 (fm ) − µ̂t+1 (fm )





 N

N
sup E Eβt µt+1 (fm ) − µ̂t+1 (fm )

m≥1

19

Major-Minor Mean Field Multi-Agent Reinforcement Learning

"
= sup ω̃F

"

N
h
i
1 X
i,N
fm (xi,N
t+1 ) − Eβt fm (xt+1 )
N i=1



N
1 X

sup E Eβt

π,π 0

m≥1






≤ sup ω̃F  sup E Eβt 
π,π 0

m≥1

= sup ω̃F  sup

≤ ω̃F

m≥1

2
√
N

i,N
fm (xi,N
t+1 ) − Eβt fm (xt+1 )

i 2

1/2 
 


! 


N
h
i2  1/2
1 X
i,N

E Eβt fm (xi,N
t+1 ) − Eβt fm (xt+1 )
N 2 i=1


π,π 0

N i=1

h

##!


→0

(19)

for βt := (x0,N
, u0,N
, xN
t
t
t ) by bounding |fm | ≤ 1, as the cross-terms disappear.

0,N
0,N
N
For the second term (18), by noting µ̂N
, u0,N
, µN
, µN
t
t , µt ⊗ πt (xt
t )), we have
t+1 = T (xt
i
h
0,N
N
0
0
sup sup E f (x0,N
t+1 , ut+1 , µ̂t+1 ) − f (xt+1 , ut+1 , µt+1 )
π,π 0 f ∈F

Z Z
= sup sup E
π,π 0 f ∈F

0,N
0
′
′
N
0
′
f (x′ , u′ , µ̂N
, u0,N
, µN
t
t+1 )πt (du | x , µt+1 )p (dx | xt
t )

ZZ

′



′

f (x , u , µt+1 )πt0 (du′ | x′ , µt+1 )p0 (dx′ | x0t , u0t , µt )

−


Z
′
′
N
0
′
′
N
0
′
′
N
≤ sup sup E sup
f (x , u , µ̂t+1 )(πt (du | x , µt+1 ) − πt (du | x , µ̂t+1 ))

(20)

x′

π,π 0 f ∈F

h
i
0
0
+ sup sup E g(x0,N
, u0,N
, µN
t
t
t ) − g(xt , ut , µt )

(21)

π,π 0 g∈G

and analyze each term separately, where we defined the function g : X 0 × U 0 × P(X ) as
ZZ
0
0
g(x , u , µ) :=
f (x′ , u′ , T ∗ )πt0 (du′ | x′ , T ∗ )p0 (dx′ | x0 , u0 , µ)
from the class G of such functions for any policies π, π 0 , where T ∗ := T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)).

For (20), defining a modulus of continuity ω̃Π0 for Π0 as for F, we have


Z
0
′
′
N
0
′
′
N
sup sup E sup
f (x′ , u′ , µ̂N
)(π
(du
|
x
,
µ
)
−
π
(du
|
x
,
µ̂
))
t+1
t
t+1
t
t+1
π,π 0 f ∈F

x′



0
′
N
0
′
N
≤ sup E LF sup W1 (πt (· | x , µt+1 ), πt (· | x , µ̂t+1 ))
π,π 0

x′



N
≤ sup E LF ω̃Π0 (dΣ (µN
t+1 , µ̂t+1 )) ≤ LF ω̃Π0
π,π 0



2
√
N


→ 0.

Lastly, for (21), we first note that the class G of functions is equi-Lipschitz.

Lemma J.1. Under Assumptions 1 and 2, the map (x0 , u0 , µ) 7→ T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)) is Lipschitz with constant
LT := (2LΠ + 1) · (Lp + (Lp + 1)LΠ + (Lp + LΠ + 1)).
Lemma J.2. Under Assumptions 1 and 2, for any equi-Lipschitz F with constant LF , the function class G is equi-Lipschitz
with constant LG := (LF LT + LF LΠ0 LT + LF LΠ Lp0 ).
Therefore, for (21), we have
h
i
0
0
sup sup E g(x0,N
, u0,N
, µN
→0
t
t
t ) − g(xt , ut , µt )

π,π 0 g∈G

20

Major-Minor Mean Field Multi-Agent Reinforcement Learning

by the induction assumption over the class G of equi-Lipschitz functions, completing the proof by induction. The existence
of independent optimal π, π 0 follows from Remark 3. This completes the proof.
√
For finite minor states, we can quantify the convergence rate more precisely as O(1/ N ), since the two metrizations dΣ
and W1 are then Lipschitz equivalent and the above moduli of continuity simply become a multiplication with the Lipschitz
constant, so for convenience we simply use the L1 distance. The convergence in the first term (17) is immediate by the weak
LLN
h
i
0,N
0,N
0,N
N
N
sup sup E f (x0,N
,
u
,
µ
)
−
f
(x
,
u
,
µ̂
)
t+1
t+1
t+1
t+1
t+1
t+1
π,π 0 f ∈F

"

#
X

≤ sup Lf E
π,π 0

x∈X

N
µN
t+1 (x) − µ̂t+1 (x)

" "
= sup Lf
π,π 0

X

#
"
N
N
1 X
1 X
0,N
0,N
i,N
i,N
N
1x (xt+1 ) − E
1x (xt+1 ) xt , ut , µt
N i=1
N i=1

E E

x∈X

r
≤ Lf |X |

##
x0,N
, u0,N
, µN
t
t
t

4
,
N

and for the second term (18) we again use the induction assumption, completing the proof.

K. Proof of Lemma J.1
Proof. First note Lipschitz continuity of (x0 , µ) 7→ µ ⊗ πt (x0 , µ) as in Appendix E, as for any (x0∗ , µ∗ ), (x0 , µ) ∈
X 0 × P(X ), then
sup W1 (µ∗ ⊗ πt (x0∗ , µ∗ ), µ ⊗ πt (x0 , µ))
Z
= sup sup
f ′ d(µ∗ ⊗ πt (x0∗ , µ∗ ) − µ ⊗ πt (x0 , µ))

π∈Π

π∈Π ∥f ′ ∥Lip ≤1

≤ sup

ZZ
sup

π∈Π ∥f ′ ∥Lip ≤1

f ′ (x, u)(πt (du | x, x0∗ , µ∗ ) − πt (du | x, x0 , µ))µ∗ (dx)

ZZ
+ sup

sup

π∈Π ∥f ′ ∥Lip ≤1

f ′ (x, u)πt (du | x, x0 , µ)(µ∗ (dx) − µ(dx))

where for the first term
ZZ
sup

sup

π∈Π ∥f ′ ∥Lip ≤1

f ′ (x, u)(πt (du | x, x0∗ , µ∗ ) − πt (du | x, x0 , µ))µ∗ (dx)
Z Z

≤ sup

sup

≤ sup

sup

π∈Π ∥f ′ ∥

Lip ≤1

f ′ (x, u)(πt (du | x, x0∗ , µ∗ ) − πt (du | x, x0 , µ)) µ∗ (dx)
Z

sup

π∈Π ∥f ′ ∥Lip ≤1 x∈X

f ′ (x, u)(πt (du | x, x0∗ , µ∗ ) − πt (du | x, x0 , µ))

= sup sup W1 (πt (· | x, x0∗ , µ∗ ), πt (· | x, x0 , µ))
π∈Π x∈X
≤ LΠ d((x0∗ , µ∗ ), (x0 , µ))

R ′
by Assumption 2, and similarly for the second by noting 1-Lipschitzness of x 7→ fL(x,u)
πt (du | x, x0 , µ), as before in
Π +1
(11), and therefore again
ZZ
sup sup
f ′ (x, u)πt (du | x, x0 , µ)(µ∗ (dx) − µ(dx))
π∈Π ∥f ′ ∥Lip ≤1

ZZ
= sup

sup

(LΠ + 1)

π∈Π ∥f ′ ∥Lip ≤1

f ′ (x, u)
πt (du | x, x0 , µ)(µ∗ (dx) − µ(dx))
LΠ + 1
21

Major-Minor Mean Field Multi-Agent Reinforcement Learning

≤ (LΠ + 1)W1 (µ∗ , µ).
Hence, the map (x0 , u0 , µ) 7→ µ ⊗ πt (x0 , µ) is Lipschitz with constant (2LΠ + 1).

As a result, the entire map (x0 , u0 , µ) 7→ T (x0 , u0 , µ, µ ⊗ πt (x0 , µ) is Lipschitz, since for any
W1 (T (x0∗ , u0∗ , µ∗ , µ∗ ⊗ πt (x0∗ , µ∗ )), T (x0 , u0 , µ, µ ⊗ πt (x0 , µ))
ZZZ
f ′ (x′ )p(dx′ | x, u, x0∗ , u0∗ , µ∗ )πt (du | x, x0∗ , µ∗ )µ∗ (dx)
= sup
∥f ′ ∥Lip ≤1

−
≤

sup

ZZZ

sup

∥f ′ ∥Lip ≤1 (x,u)∈X ×U

ZZ
+

sup

sup

∥f ′ ∥Lip ≤1 x∈X

ZZZ
+

sup
∥f ′ ∥

≤

Lip ≤1

sup
(x,u)∈X ×U

f ′ (x′ )p(dx′ | x, u, x0 , u0 , µ)πt (du | x, x0 , µ)µ(dx)
Z
f ′ (x′ )(p(dx′ | x, u, x0∗ , u0∗ , µ∗ ) − p(dx′ | x, u, x0 , u0 , µ))
f ′ (x′ )p(dx′ | x, u, x0 , u0 , µ)(πt (du | x, x0∗ , µ∗ ) − πt (du | x, x0 , µ))

f ′ (x′ )p(dx′ | x, u, x0 , u0 , µ)πt (du | x, x0 , µ)(µ∗ (dx) − µ(dx))

W1 (p(· | x, u, x0∗ , u0∗ , µ∗ ), p(· | x, u, x0 , u0 , µ))

+ sup (Lp + 1)W1 (πt (· | x, x0∗ , µ∗ ), πt (· | x, x0 , µ))
x∈X

+

(Lp + LΠ + 1)W1 (µ∗ , µ)

sup
(x,u)∈X ×U

≤ (Lp + (Lp + 1)LΠ + (Lp + LΠ + 1)) d((x0∗ , u0∗ , µ∗ ), (x0 , u0 , µ))
|
{z
}
L∗

with Lipschitz constant LT := (2LΠ + 1) · L∗ from Assumptions 1 and 2, using the same argument as in (11).

L. Proof of Lemma J.2
Proof. For any g ∈ G, for any (x0∗ , u0∗ , µ∗ ), (x0 , u0 , µ) ∈ X 0 × U 0 × P(X ), let T∗ := T (x0∗ , u0∗ , µ∗ , µ∗ ⊗ πt (x0∗ , µ∗ )) and
T ∗ := T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)) for brevity. We have
g(x0∗ , u0∗ , µ∗ ) − g(x0 , u0 , µ)
ZZ
=
f (x′ , u′ , T∗ )πt0 (du′ | x′ , T∗ )p0 (dx′ | x0∗ , u0∗ , µ∗ )
ZZ
−
f (x′ , u′ , T ∗ )πt0 (du′ | x′ , T ∗ )p0 (dx′ | x0 , u0 , µ)
≤ sup |f (x′ , u′ , T∗ ) − f (x′ , u′ , T ∗ )|

(22)

x′ ,u′

Z

+ sup
f (x′ , u′ , T ∗ )(πt0 (du′ | x′ , T∗ ) − πt0 (du′ | x′ , T ∗ ))
x′
ZZ
+
f (x′ , u′ , T ∗ )πt0 (du′ | x′ , T ∗ )(p0 (dx′ | x0∗ , u0∗ , µ∗ ) − p0 (dx′ | x0 , u0 , µ)) .
By Lemma J.1, for (22) we obtain
sup f (x′ , u′ , T (x0∗ , u0∗ , µ∗ , µ∗ ⊗ πt (x0∗ , µ∗ ))) − f (x′ , u′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)))

x′ ,u′

≤ LF LT d((x0∗ , u0∗ , µ∗ ), (x0 , u0 , µ)).
22

(23)
(24)

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Similarly for (23), by Assumption 2 we analogously have
Z
f (x′ , u′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)))
sup
x′

(πt0 (du′ | x′ , T (x0∗ , u0∗ , µ∗ , µ∗ ⊗ πt (x0∗ , µ∗ ))) − πt0 (du′ | x′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ))))

≤ LF W1 (πt0 (· | x′ , T (x0∗ , u0∗ , µ∗ , µ∗ ⊗ πt (x0∗ , µ∗ ))), πt0 (·′ | x′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)))

≤ LF LΠ0 LT d((x0∗ , u0∗ , µ∗ ), (x0 , u0 , µ)).

Lastly, for (24), as before in (11), by Assumption 1 and 2 we have again
ZZ
f (x′ , u′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)))πt0 (du′ | x′ , T (x0 , u0 , µ, µ ⊗ πt (x0 , µ)))
(p0 (dx′ | x0∗ , u0∗ , µ∗ ) − p0 (dx′ | x0 , u0 , µ))

≤ LF LΠ W1 (p0 (· | x0∗ , u0∗ , µ∗ ), p0 (· | x0 , u0 , µ))

≤ LF LΠ Lp0 d((x0∗ , u0∗ , µ∗ ), (x0 , u0 , µ)).

Therefore, G is equi-Lipschitz with Lipschitz constant (LF LT + LF LΠ0 LT + LF LΠ Lp0 ).

M. Proof of Corollary 1
Proof. As in Lemma F.1, for any ε > 0, choose time T ∈ N such that
∞
X
t=T

h
i
γT
ε
0,N
N
0
0
γ t E r(x0,N
,
u
,
µ
)
−
r(x
,
u
,
µ
)
≤
max 2|r(µ)| < .
t
t
t
t
t
t
1−γ µ
2

By Theorem 2,
T
−1
X

h
i
ε
0
0
<
γ t E r(x0,N
, u0,N
, µN
t
t
t ) − r(xt , ut , µt )
2
t=0

for sufficiently large N . Therefore, sup(π,π0 )∈Π×Π0 J N (π, π 0 ) − J(Φ−1 (π), π 0 ) → 0.
As a result, we have
J N (Φ(π̂ ∗ ), π 0∗ ) −

sup

J N (π, π 0 ) =

(π,π 0 )∈Π×Π0

≥

inf

(J N (Φ(π̂ ∗ ), π 0∗ ) − J N (π, π 0 ))

inf

(J N (Φ(π̂ ∗ ), π 0∗ ) − J(π̂ ∗ , π 0∗ ))

(π,π 0 )∈Π×Π0

(π,π 0 )∈Π×Π0

+
+

inf

(J(π̂ ∗ , π 0∗ ) − J(π, π 0 ))

inf

(J(π, π 0 ) − J N (π, π 0 ))

(π,π 0 )∈Π×Π0
(π,π 0 )∈Π×Π0

ε
ε
≥ − + 0 − = −ε
2
2
for sufficiently large N , where the second term is zero by optimality of (π̂ ∗ , π 0∗ ) in the M3FC problem.

N. Proof of Theorem 3
First, for completeness we give the finite M3FC system equations under the assumed Lipschitz parametrization for joint
stationary M3FMARL policies1 π̃ θ used during centralized training with correlated minor agent actions, as
u0,N
, ξtN ∼ π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t ),

πt′N = Γ(ξtN ),

ui,N
∼ πt′N (ui,N
| xi,N
t
t
t ),

1
Note that deterministic joint policies π̃ θ (e.g. at convergence, or if using deterministic policy gradients (Silver et al., 2014)) are
equivalent to using separate deterministic minor and major policies in (1), see also Remark 3.

23

Major-Minor Mean Field Multi-Agent Reinforcement Learning
i,N
i,N
0,N
i,N
, u0,N
, µN
xi,N
t
t ),
t+1 ∼ p(xt+1 | xt , ut , xt

0,N
0 0,N
, u0,N
, µN
x0,N
t
t ),
t+1 ∼ p (xt+1 | xt

as well as the limiting M3FC MDP under such parametrization as
u0t , ξt ∼ π̃ θ (u0t , ξt | x0t , µt ),
µt+1 = T (x0t , u0t , µt , ht ),

πt′ = Γ(ξt ),

ht = µt ⊗ πt′ ,

x0t+1 ∼ p0 (x0t+1 | x0t , u0t , µt ).

Then, by Sutton et al. (1999), the exact policy gradient for the limiting M3FC MDP is given as
∇θ J(π̃ θ ) =

∞
X
t=T



γ t E Qθ (x0t , µt , u0t , ξt )∇θ log π̃ θ (u0t , ξt | x0t , µt )

under the action-value function
θ

0

0

Q (x , µ, u , ξ) = E

"∞
X

#
t

γ r(x0t , u0t , µt )

x00 = x0 , µ0 = µ, u00 = u0 , ξ0 = ξ

,

t=0

while the approximation for the policy gradient on the finite M3FC system is given instead by
θ
d
∇
θ J(π̃ ) =

∞
X

h
0,N
b θ (x0,N
γt E Q
, µN
, ξtN )∇θ log π̃ θ (u0,N
, ξtN
t
t
t , ut

i
N
x0,N
,
µ
)
t
t

t=T

and the finite-agent action-values
bθ

0

0

Q (x , µ, u , ξ) = E

"∞
X

#
γ r(x0,N
, u0,N
, µN
t
t
t )
t

x0,N
= x0 , µ0 = µ, u0,N
= u0 , ξ0N = ξ
0
0

,

t=0

which are obtained, e.g., by on-policy samples and using critic estimates. Note that here, the conditional expectations are
given by redefining the systems (1) and (3) with the values conditioned upon.
We then show that the approximation of the policy gradient is good for large systems, i.e.
θ
θ
d
∇
θ J(π̃ ) − ∇θ J(π̂ ) → 0

(25)

as N → ∞, uniformly over all current policy parameters θ.
Proof of Theorem 3. We use the following lemmas in the proof of Theorem 3, for which the proofs are given below.
Proposition N.1. Propagation of chaos holds for the M3FC systems with parameterized actions as in Theorem 2, i.e. under
Assumptions 1, 2 and 3, for any equi-Lipschitz family F, at all times t ∈ N uniformly,
h
i
0
0
sup E f (x0,N
, u0,N
, µN
→ 0.
(26)
t
t
t ) − f (xt , ut , µt )
f,π,π 0

b θ → Qθ as N → ∞.
Proposition N.2. Under Assumptions 1 and 2, the approximate action-values converge uniformly, Q
As a result, we obtain
θ
θ
d
∇
θ J(π̃ ) − ∇θ J(π̂ )

=

∞
X
t=0

≤

∞
X
t=0

h
i
0,N
θ 0
0
θ 0
0
b θ (x0,N
γt E Q
, µN
, ξtN )∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t , ut
t ) − Q (xt , µt , ut , ξt )∇θ log π̃ (ut , ξt | xt , µt )
γt E

h


i
0,N
0,N
b θ (x0,N
Q
, µN
, ξtN ) − Qθ (x0,N
, µN
, ξtN ) ∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t
t , ut
t , ut
t )
24

Major-Minor Mean Field Multi-Agent Reinforcement Learning

+

∞
X
t=T

+

T
−1
X
t=0

h
i
0,N
θ 0
0
θ 0
0
γ t E Qθ (x0,N
, µN
, ξtN )∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t , ut
t ) − Q (xt , µt , ut , ξt )∇θ log π̃ (ut , ξt | xt , µt )
h
i
0,N
θ 0
0
θ 0
0
γ t E Qθ (x0,N
, µN
, ξtN )∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t , ut
t ) − Q (xt , µt , ut , ξt )∇θ log π̃ (ut , ξt | xt , µt )

for any T , such that the first term disappears by Assumption 3 uniformly bounding ∇θ log π̃ θ and Proposition N.2. Note
that we bounded ∇θ log π̃ θ here, but we can also assume bounded gradients ∇θ π̃ θ instead, e.g. (27).
For the second term, we similarly uniformly bound ∇θ log π̃ θ by Assumption 3 and Q by Assumption 1, then choose T
sufficiently large.
Finally, for the last term, we note that we can write the difference as
T
−1
X
t=0

=

h
i
0,N
θ 0
0
θ 0
0
γ t E Qθ (x0,N
, µN
, ξtN )∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t
t , ut
t ) − Q (xt , µt , ut , ξt )∇θ log π̃ (ut , ξt | xt , µt )

T
−1
X

t

γ E

"∞
X

t′ =0
∞
X

t=0

−
≤

T
−1
X
t=0

γt E

+

t=0

t=0

" ∞
X

#
 0′ 0′ ′

0′
0
′
0′
0 ′
θ 0
0
γ E r(xt′ , ut′ , µt′ ) x0 = xt , µ0 = µt , u0 = ut , ξ0 = ξt ∇θ log π̃ (ut , ξt | xt , µt )
t

h
i
0,N
0,N
0′
′
0′
0′
γ t E r(x0′
, µ′0 = µN
, ξ0′ = ξtN ∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t′ , ut′ , µt′ ) x0 = xt
t , u0 = ut
t )

t′ =T ′
∞
X

−
T
−1
X

h
i
0,N
0,N
0′
′
0′
′
N
0′
′
N
γ t E r(x0′
,
u
,
µ
)
x
=
x
,
µ
=
µ
,
u
=
u
,
ξ
=
ξ
∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
′
′
′
t
t
t
t
t
t
t
0
0
t
0
0
t
t )

t=T ′

#
 0′ 0′ ′

0′
0
′
0′
0 ′
θ 0
0
γ E r(xt′ , ut′ , µt′ ) x0 = xt , µ0 = µt , u0 = ut , ξ0 = ξt ∇θ log π̃ (ut , ξt | xt , µt )
t

 ′
TX
−1
h
i
0,N
0,N
0′
′
0′
0′
γt E 
γ t E r(x0′
, µ′0 = µN
, ξ0′ = ξtN ∇θ log π̃ θ (u0,N
, ξtN | x0,N
, µN
t
t
t′ , ut′ , µt′ ) x0 = xt
t , u0 = ut
t )
t′ =0

−

′
TX
−1

t=0




0′
0
′
0′
0 ′
0′
′
θ 0
0

γ t E r(x0′
t′ , ut′ , µt′ ) x0 = xt , µ0 = µt , u0 = ut , ξ0 = ξt ∇θ log π̃ (ut , ξt | xt , µt )

where we write the conditional M3FC system and random variables in the inner expectation with a prime, bounding again
the former terms by choosing sufficiently large T ′ and using Assumptions 1 and 3, while for the latter terms we use
Proposition N.1 on the functions
ZZ


0
0′
′
0′
0
0′
0 ′
θ 0
0
0
f (x , µ) =
E r(x0′
(27)
t′ , ut′ , µt′ ) x0 = x , µ0 = µ, u0 = u , ξ0 = ξ ∇θ π̃ (u , ξ | x , µ)d(u , ξ)
for all t′ , which are uniformly Lipschitz by Assumptions 1 and 3. This completes the proof.

O. Proof of Proposition N.1
Proof. The proof is exactly analogous to the proof of Theorem 2, except that instead of using Lipschitz constants of
x0t , u0t , µt , ht 7→ T (x0t , u0t , µt , ht ), one uses Lipschitz constants of x0t , u0t , µt , ξt 7→ T (x0t , u0t , µt , µt ⊗ Γ(ξt )) via the
additional Assumption 3 on top of Assumptions 1 and 2.

P. Proof of Proposition N.2
b θ → Qθ as N → ∞ uniformly, it suffices to prove pointwise convergence due to compact support.
Proof. To show Q

Therefore, fix any x0 , µ, u0 , ξ. The convergence follows as in Corollary 1, from showing at any time t that


sup E f (x0t , u0t , µt ) x00 = x0 , µ0 = µ, u00 = u0 , ξ0 = ξ
f ∈F

25

Major-Minor Mean Field Multi-Agent Reinforcement Learning

i
h
0,N
0,N
0,N
0,N
0 N
N
→0
x
=
x
,
µ
=
µ,
u
=
u
,
ξ
=
ξ
− E f (x0,N
,
u
,
µ
)
0
t
t
0
t
0
0
over any equi-Lipschitz family of functions F, and applying for f = r (using the set F of Lr -Lipschitz functions) by
Assumption 1.
The statement is shown by considering time t = 0, and then by induction for any t ≥ 1. At time t = 0, the statement follows
from the weak LLN as in Theorem 2. For any subsequent times, we similarly have


sup E f (x0t+1 , u0t+1 , µt+1 ) x00 = x0 , µ0 = µ, u00 = u0 , ξ0 = ξ
f ∈F

h
i
0,N
0,N
0,N
N
0,N
0 N
− E f (x0,N
,
u
,
µ
)
x
=
x
,
µ
=
µ,
u
=
u
,
ξ
=
ξ
0
t+1
0
t+1
t+1
0
0


0
0
0
0
0
0
≤ sup E f (xt+1 , ut+1 , µt+1 ) x0 = x , µ0 = µ, u0 = u , ξ0 = ξ
f ∈F

h
i
0,N
0,N
0,N
N
N
− E f (x0,N
, u0,N
, µN
= x0,N , µ0 = µ, u0,N
= u0 , ξ0N = ξ
t
t , µt ⊗ Γ(ξt ))) x0
t+1 , ut+1 , T (xt
0
h
i
0,N
0,N
0,N
0,N
0,N
N
N
N
0,N
0 N
+ sup E f (x0,N
,
u
,
T
(x
,
u
,
µ
,
µ
⊗
Γ(ξ
)))
x
=
x
,
µ
=
µ,
u
=
u
,
ξ
=
ξ
0
t
t
t
t
t
0
t+1
t+1
0
0
f ∈F

i
h
0,N
0,N
N
= x0,N , µ0 = µ, u0,N
= u0 , ξ0N = ξ .
− E f (x0,N
t+1 , ut+1 , µt+1 ) x0
0
As in Theorem 2, the latter term is bounded by induction assumption, using uniform Lipschitzness of the dynamics,
x0t , u0t , µt , ξt 7→ T (x0t , u0t , µt , µt ⊗ Γ(ξt )) via Assumptions 2 and 3, while the former term is bounded as usual by the weak
LLN. This completes the proof.

Q. Extended MFC Optimalities
Intuitively, in large MF systems governed by dynamics of the form (1), almost all information of the joint state
(x0,N
, x1,N
, . . . , xN,N
) is contained in (x0,N
, µN
t
t
t
t
t ), while heterogeneous policies should by LLN be replaceable by a
shared one. To fully complete the theory of MFC, it is therefore interesting to establish the optimality of the considered MF
policies over arbitrary other policies acting on the joint state (x0,N
, x1,N
, . . . , xN,N
).
t
t
t
It seems plausible that it would be possible to extend optimality (Corollary 1) over larger classes of policies in the
finite system. In particular, at least for finite state-action spaces, (i) any joint-state
policy π(du | x0,N
, x1,N
, . . . , xN,N
)
t
t
t
P
0
0
N
P
might in the limit be replaced by an averaged policy π̄(du | x , µ) := xN ∈X N : 1
π(du
|
x
,
x
)
under
i δxi,N =µ
N
some exchangeability of agents; (ii) any optimal policy π outputting joint actions for all agents might be replaced by
an independent but identical policy for each agent, as in the limit all information is contained in the joint state-action
distribution, any of which may be approximated increasingly closely by LLN; and (iii) heterogeneous policies for each minor
agent π 1 , . . . , π N might similarly be replaced by some averaged policy π̄(π 1 , . . . , π N ), averaging the action distributions in
any specific state over the proportion of agent likelihoods in that state.
Showing such results would allow us to conclude that the policy classes Π are natural and sufficient in MF systems, including
MFC and also the competitive MFGs, as more general or heterogeneous policies will not perform much better. A result
related to (iii) has been shown for static cases (Sanjari and Yüksel, 2020; Cui et al., 2021) and more recently in MFC and its
two-team generalizations (Guan et al., 2024).

R. Experimental Details
In this section, we give lengthy experimental details that were omitted in the main text.
R.1. Problem Details
In this section, we give details to the problems considered in this work. We omit the superscript N for readability.
2G. In the 2G problem, we formally let X = [−2, 2]2 , U = [−1, 1]2 , X 0 = {0, 1, . . . 49} according to (13). We allow
noisy movement of minor agents following the Gaussian law


uit
2
2
p(xit+1 | xit , uit ) = N xit+1 xit + vmax
,
diag(σ
,
σ
)
max(1, ∥uit ∥2 )
26

Major-Minor Mean Field Multi-Agent Reinforcement Learning
Table 3. Shared hyperparameter configurations for all algorithms.

Symbol

Name

Value

γ
λ
β
ϵ
lr
Blen
blen
NSGD

Discount factor
GAE lambda
KL coefficient
Clip parameter
Learning rate
Training batch size
Mini-batch size
Gradient steps per training batch

0.99
1
0.03
0.2
0.00005
24000
4000
8

for some maximum speed vmax = 0.2, noise covariance σ 2 = 0.03 and projecting back actions u with norm larger than 1,
with the additional modification that agent positions are clipped back into X whenever the agents move out of bounds.
We then consider a time-variant mixture of two Gaussians
µ∗t :=

 1 − cos(2πt/50)

1 + cos(2πt/50)
N e1 , diag(σ∗2 , σ∗2 ) +
N −e1 , diag(σ∗2 , σ∗2 )
2
2

for unit vector e1 and covariance σ∗2 = 0.05, i.e. we have a period of 50 time steps, and let the major state follow the clock
dynamics p0 (x0 + 1 mod 50 | x0 , µ) = 1.
The goal of minor agents is to minimize the Wasserstein metric Ŵ1 under the squared Euclidean distance,
Z

′
2
Ŵ1 (µ, µ ) := inf ′
∥x − y∥2 γ(dx, dy)
γ∈Γ(µ,µ )

defined over all couplings Γ(µ, µ′ ) with first and second marginals µ, µ′ (which is strictly speaking not a metric but an
optimal transportation cost, since the squared Euclidean distance fails the triangle inequality), between their empirical
distribution and the desired mixture of Gaussians
r(x0t , µt ) = −Ŵ1 (µt , µ∗t )
which is computed numerically by the empirical distance, sampling 300 samples from µ∗t .
The initialization of minor agents is uniform, i.e. µ0 = Unif(X ), and x00 = 0. For sake of simulation, we define the episode
length T = 100 after which a new episode starts.
Formation. The Formation problem is an extension of the 2G problem, where instead X 0 = X × X and U 0 = U, the
major agent follows the same dynamics as the minor agents, and movements are noise-free, i.e. σ 2 = 0. The major agent
state x0t = (x̂0t , x∗t ) here contains both the major agent position x̂0t and its target position x∗t . The desired minor agent
distribution is centered around the major agent

µ∗t := N x̂0t , diag(σ∗2 , σ∗2 )
with covariance σ∗2 = 0.3, and is also observed by agents as in 2G via binning. Additionally, the major agent should follow
a random target x∗t following discretized Ornstein-Uhlenbeck dynamics

2
2
x∗t+1 ∼ N 0.95x∗t , diag(σtarg
, σtarg
)
2
with σtarg
= 0.02. Thus, similar to 2G, the reward function becomes

r(x0t , u0t , µt ) = −∥x̂0t − x∗t ∥2 − Ŵ1 (µt , µ∗t ).
0
The initialization of agents
 is uniform, while the target starts around zero, i.e. µ0 = Unif(X ) and µ0 = Unif(X ) ⊗
2
2
N 0, diag(σtarg , σtarg ) . For sake of simulation, we define the episode length T = 100 after which a new episode starts.

27

Major-Minor Mean Field Multi-Agent Reinforcement Learning

Beach Bar Process. In the discrete beach bar process, we consider a discrete torus X = {0, 1, . . . , 4}2 , X 0 = X × X and
actions U = U 0 = {(0, 0), (−1, 0), (0, −1), (1, 0), (0, 1)} indicating movement in any of the four cardinal directions. The
major agent state x0t = (x̂0t , x∗t ) here contains both the major agent position x̂0t and its target position x∗t . In other words,
the dynamics follow
x̂0t+1 = x̂0t + u0t

xit+1 = xit + uit

mod (5, 5).

x∗t+1 ∼ x∗t + ϵt Unif((−1, 0), (0, −1), (1, 0), (0, 1))

mod (5, 5)

mod (5, 5),

The target position follows a random walk on the torus

with walking probability ϵt ∼ Bernoulli(0.2), uniformly in any direction.
The costs are then given by the average toroidal distance d (the L1 “wrap-around” distance on the torus) between the major
agent and its target, the average distance between major and minor agents, and the crowdedness of agents
r(x0t , u0t , µt ) = −0.5d(x0t , x∗t ) − 2.5

Z

d(x, x0t )µt (dx) − 6.25

Z
µt (x)µt (dx).

The initialization of agents is uniform, while the target starts at zero, i.e. µ0 = Unif(X ) and µ00 = Unif(X ) ⊗ δ(0,0) . For
sake of simulation, we define the episode length T = 200 after which a new episode starts.
For the neural network policy, we use a one-hot encoding of major states as input, i.e. the concatenation of two 5-dimensional
one-hot vectors for the major agent position x̂0t and its target position x∗t respectively.
2
Foraging. In the Foraging problem, we
U = [−1, 1]2 = U 0 and X 0 = ([−2, 2] ×
n formally define X = [−2,i2] × i[0, 1],
S5
2
i
[−2, −1]) × n=0 [−2, 2] × [0, 1.5] . The minor agent states xt = (x̂t , x̃t ) here contain their positions x̂it ∈ [−2, 2]2
and encumbrance (or inversely, free cargo space) x̂it ∈ [0, 1]. Meanwhile, the major agent state x0t = (x̂0t , xenv
t ) here
contains both the major agent position x̂0t restricted to [−2, 2] × [−2, −1], and the current environment state xenv
t . Here, the
minor and major agents move as in Formation, though with different maximum velocities for minor agents vmax = 0.3 and
0
major agent vmax
= 0.1 respectively.

An additional environmental state consists of up to 5 spatially localized foraging areas, which is not observed by the agents.
In each time step, Nt = Pois(0.2) new foraging areas appear, up to a maximum total number of 5. The location xm
t of
each foraging area m = 1, . . . , 5 is sampled uniformly randomly from Unif(X ), while their total initial size Lm
is
sampled
t
m
from Unif([0.5, 1.5]), making up the environment state xenv
= (xm
t
t , Lt )m . At every time step, the foraging areas m are
depleted by nearby agents closer than range 0.5,
m
m
Lm
t+1 = Lt − ∆L (µt ),
m
∆Lm (µt ) := min(Lm
t+1 − Lt , min(0.1,

Z

+
(0.5 − ∥x − xm
t ∥2 ) µt (dx))

where (·)+ := max(0, ·), until they are fully depleted and disappear (Lm
t+1 ≤ 0).
Foraging minor agents simulate encumbrance, gaining it from nearby foraging areas and depositing to a nearby major agent,
by splitting the foraged amount among all nearby minor agents according to their foraged contribution, and wasting any
amount going beyond maximum encumbrance 1,
(
x̃it+1 =

(0.5−∥x−xm ∥ )+

min(1, x̃it + ∆Lm (µt ) · R (0.5−∥x−xm ∥t2 )+2 µt (dx) ) if
t

0

else.

∥xit − x0t ∥2 ≥ 0.5,

The reward at each time step is then given by the according total foraged and then deposited amount by the minor agents,
where any clipped amount is wasted.
The initialization of agents is uniform, while the environment starts empty, i.e. µ0 = Unif(X ) and µ00 = Unif(X ) ⊗ δ∅ . For
sake of simulation, we define the episode length T = 200 after which a new episode starts.
28

Return J(π)

Major-Minor Mean Field Multi-Agent Reinforcement Learning

−200

0

(a) −500

1

Steps t

2
×108

1500
1000
(c) 500
4
0.0
×108

−400

−250

−100

0

(b) −600

1

2
×108

Steps t

0

2

Steps t

−50
(d) −100

2.5

Steps t

5.0
×108

PPO
A2C

(e)
0

2

×108

Steps t

Figure 10. Training curves (mean episode return vs. time steps) of M3FPPO in red, compared to A2C in blue. (a) 2G; (b) Formation; (c)
Beach; (d) Foraging; (e) Potential.

y

0

t=5

2

0

0

2

x

(c)

t = 25

2

0

−2
−2

0

(d)

t = 50

2

0

−2
−2

2

x

(i)

y

−2
−2

(b)

0

2

x

−2
−2

t=0

(j)

0

(e)

t=0

0

t = 25

(f)

0

(k)

2

x

t = 35

1

1

1

0

0

0

0

−1

0

1

−1

x

−1

0

−1

1

x

2

−1

0

1

−1

x

(g)

t = 25

2

0

−2
−2

1

−1

t=5

0

−2
−2

2

x

2

y

2

y

t=0

y

(a)

y

2

0

2

x

(l)

t = 45

−1

x

0

−2
−2

(h)

t = 75

0

0
x

2

−2
−2

0

2

x

1

Figure 11. Qualitative visualization of learned M3FC behavior in the 2G (a-d), Formation (e-h) and Potential (i-l) problems. Red: minor
agent; blue triangle: major agent; green triangle: major agent target. (i-l): As in (e-h), with arrow for potential gradient (not to scale).

Potential. Lastly, in Potential we consider minor agents on a continuous one-dimensional torus X = [−2, 2] (where the
points −2 and 2 are identified), actions U = [−1, 1] and major state X 0 = X × X . The minor agents move as in Foraging
(wrapping around the torus instead of clipping), while the major agent follows the gradient of the potential landscape
generated by minor agents, with the goal of staying close to its current target. The major agent state x0t = (x̂0t , x∗t ) here
contains both the major agent position x̂0t and its target position x∗t . For simplicity, here we use a linear repulsive force
decreasing from N1 to 0 over a range of 1,
x̂0t+1 = x̂0t +

1
20

Z

X
xoff ∈{−4,0,4}

(1 − ∥x̂0t − x + xoff ∥2 )+

x̂0t − x + xoff
µt (dx)
∥x̂0t − x + xoff ∥2

mod [−2, 2]

where we let terms 0/0 = 0 and use the offset xoff to account for the wrap-around on the torus.
The target follows the discretized Ornstein-Uhlenbeck process
2
2
x∗t+1 ∼ N 0.99x∗t , diag(σtarg
, σtarg
)



2
with covariance σtarg
= 0.005, and gives rise to the reward function via the toroidal distance between target and major agent

r(x0t , µt ) = −d(x̂0t , x∗t ).
0
The initialization of agents
 is uniform, while the target starts around zero, i.e. µ0 = Unif(X ) and µ0 = Unif(X ) ⊗
2
2
N 0, diag(σtarg , σtarg ) . For sake of simulation, we define the episode length T = 100 after which a new episode starts.
In contrast to M = 72 = 49 in 2G, Formation and Foraging, here we use M = 7 bins for the one-dimensional problem.

Return J(π)

1500
−100
−200

−400

−200
(a)
0

1

Steps t

×106

−400
0.0

(b)
0.5

Steps t

1.0
×106

−50

1000

−600

500

(c)
0

1

Steps t

2

×106

0.0

0.5

Steps t

(d) −100
1.0
0.0
×106

(e)
0.5

Steps t

IPPO N=5
IPPO N=10
IPPO N=20
MF N=20

1.0
×106

Figure 12. Training curves (mean episode return vs. time steps) of IPPO, trained on the systems with N ∈ {5, 10, 20}. (a) 2G; (b)
Formation; (c) Beach; (d) Foraging; (e) Potential.

29

Major-Minor Mean Field Multi-Agent Reinforcement Learning
Return J(π)

1500
−100
−200

−400

−200
(a)
0

1

Steps t

×10

−400
−600

6

(b)
0

1

Steps t

2
×10

5

−50

1000

−600

500

(c)
0

1

Steps t

2

×10

6

0.0

0.5

Steps t

(d) −100

1.0
×106

0.0

(e)
0.5

Steps t

MAPPO N=5
MAPPO N=10
MAPPO N=20
MF N=20

1.0
×106

Figure 13. Training curves (mean episode return vs. time steps) of MAPPO, trained on the systems with N ∈ {5, 10, 20}. (a) 2G; (b)
Formation; (c) Beach; (d) Foraging; (e) Potential.

R.2. Comparison to M3FA2C
In Figure 10 we can see that vanilla M3FA2C typically performs worse than M3FPPO, getting stuck in worse local optima.
Here, we used the same hyperparameters as in PPO. This validates our choice of PPO for M3FMARL.
R.3. Qualitative results
In Figure 11, M3FPPO successfully learns to form mixtures of Gaussians in 2G, and a Gaussian around a moving major
agent that tracks its target in Formation. As expected in 2G, the two Gaussians at their sinusoidal peaks t = 25 and t = 50
are not perfectly tracked, in order to minimize the cost in following time steps, when the other Gaussian reappears. Finally,
in Potential the minor agents succeed in pushing the major agent towards its target, while spreading on both sides of the
major agent to be able to track any random movement of the target.
R.4. Training M3FPPO, IPPO and MAPPO on smaller systems
In Figure 6 we verified the training of M3FPPO on small finite system. Comparing to Figures 5 and 9, for M3FPPO we see
little difference between training on a small finite-agent system versus training on a large system and applying the policy on
the smaller system. For the chosen hyperparameters, the performance in the Potential problem depends on the initialization.
However, M3FPPO compares especially favorably to IPPO in Beach and Foraging, even when directly training on the finite
system. This shows that we can either (i) directly apply M3FPPO as a MARL algorithm to small systems, or (ii) train on a
fixed system, and transfer the learned behavior to systems of almost arbitrary other sizes.
Analogously, in Figures 12 and 13 we show the training results for around a day of IPPO and MAPPO for numbers of agents
N = 5, N = 10 and N = 20. As seen in the plot, the results for each number of agents is comparable to the analysis shown
in the main text. In particular, transferring M3FPPO or comparing with Figure 6, we observe that M3FPPO continues to
outperform or match the performance of IPPO and MAPPO, even in the setting with fewer agents.

30

