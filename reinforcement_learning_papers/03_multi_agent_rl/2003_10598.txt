arXiv:2003.10598v1 [cs.MA] 24 Mar 2020

Multi-Agent Reinforcement Learning for Problems
with Combined Individual and Team Reward
Hassam Ullah Sheikh

Ladislau BoÌˆloÌˆni

Department of Computer Science
University of Central Florida
Orlando, USA
hassam.sheikh@knights.ucf.edu

Department of Computer Science
University of Central Florida
Orlando, USA
lboloni@cs.ucf.edu

Abstractâ€”Many cooperative multi-agent problems require
agents to learn individual tasks while contributing to the collective success of the group. This is a challenging task for current
state-of-the-art multi-agent reinforcement algorithms that are
designed to either maximize the global reward of the team or
the individual local rewards. The problem is exacerbated when
either of the rewards is sparse leading to unstable learning. To
address this problem, we present Decomposed Multi-Agent Deep
Deterministic Policy Gradient (DE-MADDPG): a novel cooperative
multi-agent reinforcement learning framework that simultaneously learns to maximize the global and local rewards. We
evaluate our solution on the challenging defensive escort team
problem and show that our solution achieves a significantly better
and more stable performance than the direct adaptation of the
MADDPG algorithm.
Index Termsâ€”Multi-Agent Reinforcement Learning; Coordination and Collaboration; Dual-Reward Learning

I. I NTRODUCTION
Cooperative multi-agent problems are prevalent in realworld settings such as strategic conflict resolution [1], coordination between autonomous vehicles [2] and collaboration
of agents in defensive escort teams [3]. Such problems can
be modelled as dual-interest: each agent is simultaneously
working towards maximizing its own payoff (local reward)
as well as the collective success of the team (global reward).
For example, autonomous vehicles in double-lane merge conflicts must perform cooperative maneuvers without diverging
from their destination-bound nominal trajectories. Similarly,
in the case of a defensive escort team, each agent has
to maintain a specific distance from the payload to avoid
disrupting any social norms without sacrificing the security
of the payload. Despite the recent success of multi-agent
reinforcement learning (MARL) in multiplayer games like
Dota 2 [4], Quake III Capture-the-Flag [5] and Starcraft II [6]
or learning to use tools [7], learning multi-agent cooperation
while simultaneously maximizing local rewards is still an open
challenge. In this learning problem, to which we will refer
as â€œdual-reward MARLâ€, the agents are explicitly receiving
two reward signals: the global team reward and the agentâ€™s
individual local reward.
This work had been supported in part by the National Science Foundation
under grant number IIS-1409823.

Current state-of-the-art MARL algorithms can be categorized in two types. For algorithms such as COMA [8] and
QMIX [9], the goal is to maximize the global reward for the
success of the group while algorithms such MADDPG [10]
and M3DDPG [11] focus on optimizing local rewards without
any explicit notion of coordination. As shown in [12] and
in our findings in Section V, a direct adaptation of these
algorithms to dual-reward problems often leads to poor performance and unstable learning. Generally, these adaptations
happen in the reward function space where the local and
the global reward signals are combined to form an entangled
multi-objective reward function [13]. This coupling of reward
functions leads to two problems. First, the entangled reward
function becomes unfactorizable during training, causing the
learning to oscillate between optimizing either the global or the
local reward leading to a sub-optimal and unstable solution.
This problem is exacerbated when either of the rewards is
sparse, thus, leading to a bias towards the other. The second
problem is that maximizing the entangled reward function does
not correspond to maximizing the objective function.
To address these issues, we present Decomposed MultiAgent Deep Deterministic Policy Gradient (DE-MADDPG):
a novel cooperative multi-agent reinforcement learning framework built on top of deterministic policy gradients that simultaneously learns to maximize the global and the local rewards
without the need of creating an entangled multi-objective
reward function. The core idea behind DE-MADDPG is to
train two critics. The global critic, shared between all the
cooperating agents takes as input the observations and actions
of these agents and estimates the sum of the global expected
reward. The local critic receives as input only the observation
and action of the particular agent and estimates the sum of
local expected reward. The advantage of training two critics is
that the step of designing an entangled multi-objective reward
function can be skipped altogether.
To summarize, our contributions in this paper are the
following:
â€¢ We develop a dual-critic framework for multi-agent reinforcement learning that learns to simultaneously maximize the decomposed global and local rewards.
â€¢ Taking advantage of the decomposition, we treat the
global critic as a single-agent critic. This allows us to

apply performance enhancement techniques such as Prioritized Experience Replay (PER) [14] and Twin Delayed
Deep Deterministic Policy Gradients (TD3) [15] to tackle
the overestimation bias problem in Q-functions. This was
not previously feasible in the multi-agent RL setting.
â€¢ We evaluate our proposed solution on the defensive
escort team problem [3], [16] (see Figure 1) and show
that it achieves a significantly better and more stable
performance than the direct adaptation of the MADDPG
algorithm.
II. R ELATED WORK
Early theoretical work in MARL was limited to discrete
state and action spaces [17], [18], [19]. Recent work have
adopted techniques from single-agent deep RL to develop
general algorithms for high-dimensional continuous space
environments requiring complex agent interactions [1], [20],
[10].
Cooperative multi-agent learning is important since many
real-world problems can be formulated as distributed systems
with decentralized agents that must coordinate to achieve
shared objectives [21]. Similar to our work, [22] have shown
that agents whose rewards depend on all agentsâ€™ success
perform better than agents who optimize for their own success.
In the special case when all agents have a single goal and share
a global reward, COMA [8] uses a counterfactual baseline.
However, the centralized critic in these methods only focuses
on optimizing the collective success of the group. When
a global objective is the sum of agentsâ€™ individual objectives, value-decomposition methods optimize a centralized Qfunction while preserving scalable decentralized execution [9],
but do not address credit assignment. While MADDPG [10]
and M3DDPG [11] apply to agents with different reward functions, they do not specifically address the need for cooperation;
in fact, they do not distinguish the problems of cooperation
and competition, despite the fundamental difference.
To the best of our knowledge, dual-reward MARL was
not explicitly addressed in the existing literature. Among
the related problems, [23] explored the multi-goal problem
and analyzed its convergence in a special networked setting
restricted to fully-decentralized training. In contrast, we are
conducting centralized training with decentralized execution.
In contrast to multi-task MARL, which aims for generalization
among non-simultaneous tasks [24], and in contrast to hierarchical methods with top-level managers that sequentially select
subtasks [25], our decentralized agents must cooperate in
parallel to successfully achieve the global and their respective
local objectives.
III. BACKGROUND
A. Policy Gradients
Policy gradient methods have been shown to learn the
optimal policy in a variety of reinforcement learning tasks.
The main idea behind policy gradient methods is that instead
of parameterizing the Q-function to extract the policy, we
parameterize the policy using the parameters Î¸ to maximize

the objective represented as J (Î¸) = E [Rt ] by taking a step
in the direction
âˆ‡J(Î¸) = E [âˆ‡Î¸ log Ï€Î¸ (a|s) QÏ€ (s, a)]
Policy gradient methods are prone to the high variance problem. Several methods such as [26], [27] have been shown to
reduce the variability by introducing a critic, a Q-function
that tells about the goodness of a reward by working as a
baseline. [28] has shown that it is possible to extend the policy
gradient framework to deterministic policies i.e. Ï€Î¸ : S â†’ A.
In particular we can write âˆ‡J (Î¸) as


âˆ‡J(Î¸) = E âˆ‡Î¸ Ï€ (a|s) âˆ‡a QÏ€ (s, a) |a=Ï€(s)
A variation of this model, Deep Deterministic Policy Gradients
(DDPG) [29] is an off-policy algorithm that approximates the
policy Ï€ and the critic QÏ€ with deep neural networks. DDPG
uses an experience replay buffer alongside a target network to
stabilize the training. Twin Delayed Deep Deterministic Policy
Gradients (TD3) [15] improves on DDPG by addressing the
overestimation bias of the Q-function, similarly to Double Qlearning [30]. They find that approximation errors of the neural
network, combined with gradient descent make DDPG tend to
overestimate the Q-values, leading to a slower convergence.
TD3 addresses this by using two Q-networks QÏˆ1 , QÏˆ2 , along
with two target networks. The Q-functions are updated with
the target y = rt + Î³ min1,2 QÏˆi0 (s0t , a0t ), while updating the
policy with QÏˆ1 . Additionally, they introduce target policy
smoothing by adding noise in the determination of the next
action for the critic target a0t = ÂµÎ¸Ï€0 (s0 ) + , with  being
clipped Gaussian noise  = clip(N (0, Ïƒ), âˆ’c, c), where c
is a tunable parameter. Additionally, they use delayed policy
updates and only update the policy Ï€ and target network
parameters once every d critic updates.
Multi-agent deep deterministic policy gradients (MADDPG) [10] extends DDPG for the multi-agent setting where
each agent has itâ€™s own policy. The gradient of each policy is
written as


âˆ‡J(Î¸i ) = E âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QÏ€i (s, a1 , . . . , aN ) |ai =Ï€i (oi )
where s = (o1 , . . . , oN ) and QÏ€i (s, a1 , . . . , aN ) is a centralized action-value function that takes the actions of all
the agents in addition to the state of the environment to
estimate the Q-value for agent i. Since every agent has its
own Q-function, the model allows the agents to have different
action space and reward functions. The primary insight behind
MADDPG is that knowing all the actions of other agents
makes the environment stationary, even though their policy
changes.
Very recently [31] proposed Multi-Agent TD3 (MATD3)
extending MADDPG by replacing the deterministic policy
gradients with twin delayed deterministic policy gradient to
tackle the overestimation bias problem.
IV. D ECOMPOSED M ULTI -AGENT D EEP D ETERMINISTIC
P OLICY G RADIENT
We propose Decomposed Multi-Agent Deep Deterministic
Policy Gradient: a multi-agent deep reinforcement learning

(a) Random Landmarks

(b) Shopping Mall

(c) Street

(d) Pie-in-the-Face

Fig. 1: Four environments for the defensive escort team problem [16]. The team of bodyguards (blue) need to protect the VIP
(brown) in the environments, from left to right: â€Random Landmarksâ€, â€Shopping Mallâ€, â€Streetâ€ and â€Pie-in-the-Faceâ€.

the idea of MADDPG by introducing a local critic. Now the
modified policy gradient for each agent i can be written as
M ADDP G

z

}|

"

#{

âˆ‡J(Î¸i ) = Es,aâˆ¼D âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QgÏˆ (s, a1 , . . . , aN )
"

#)

+ Eoi ,ai âˆ¼D âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QÏ€i (oi , ai )
Fig. 2: An overview of the Decomposed Multi-Agent Deep
Deterministic Policy Gradient architecture. In contrast to
MADDPG which uses a single centralized critic, DEMADDPG has a global centralized critic shared between all
cooperating agents and a local critic specific to the agent.

algorithm that learns to simultaneously maximize the groupâ€™s
global reward and the agentâ€™s local rewards. Our approach
uses a two critic approach to train policies and value functions
that are optimal to maximize the global and local rewards
respectively.
The main idea is to combine MADDPG (or MATD3) for
maximizing global rewards with a standard single agent DDPG
(or TD3). Intuitively, the goal is to move the policy in the
direction that maximizes both the global and the local critic.
The resulting learning paradigm is similar to the centralized training with decentralized execution during testing used
by [10]. In this setting, additional information is provided for
the agents during training that is not available during test time.
Concretely, we consider an environment with N agents
with policies Ï€ = {Ï€1 , . . . , Ï€N } parameterized by Î¸ =
{Î¸1 , . . . , Î¸N }. The multi-agent deep deterministic policy gradient for agent i can written as


âˆ‡J(Î¸i ) = E âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QÏ€i (s, a1 , . . . , aN ) |ai =Ï€i (oi )
where s = (o1 , . . . , oN ) and QÏ€i (s, a1 , . . . , aN ) is a centralized action-value function parameterized by Ï†i that takes
the actions of all the agents in addition to the state of the
environment to estimate the Q-value for agent i. We extend

DDP G
(1)

where ai = Ï€i (oi ) is action from agent i following policy Ï€i
and D is the experience replay buffer. The global critic is QgÏˆ
is updated as:

2 
L (Ïˆ) = Es,a,r,s0 QgÏˆ (s, a1 , . . . , aN ) âˆ’ yg
where yg is defined as:
yg = rg + Î³QgÏˆ0 (s0 , a01 , . . . , a0N ) |a0 =Ï€0 (o0 )
i

i

i

0

0
where Ï€ = {Ï€10 , . . . , Ï€N
} are target policies parameterized by
0
0
0
Î¸ = {Î¸1 , . . . , Î¸N }. Similarly, The local critic is QÏ€i is updated

as:
h
i
2
L (Ï†i ) = Eo,a,r,o0 (QÏ€i (oi , ai ) âˆ’ yl )
where yl is defined as:
0

yl = rli + Î³QÏ€Ï†0i (o0i , a0i ) |a0 =Ï€0 (o0 )
i

i

i

Overestimation bias in Q-functions have been thoroughly studied in [15], [31]. This overestimation bias can be problemsome
in multi-agent settings especially in real time autonomous
systems. For example, in resolving the double lane merge
conflict in autonomous vehicles, the vehicles might consider
the current state to be near conflict resolution thus taking a
dangerous turns. To solve this problem [15] have proposed
a double critic approach to minimize the overestimation bias.
Motivated from the results in [15], we replace the Multi-Agent
Deterministic Policy Gradient of the global critic in Equation (1) with Twin Delayed Deterministic Policy Gradient.
Therefore our updated policy gradient becomes

Algorithm 1 Decomposed Multi-agent Deep Policy Gradient
g

g

1: Initialize main global critic networks QÏˆ1 and QÏˆ2 .

g
g
2: Initialize target global critic networks QÏˆ10 and QÏˆ20 .

3: Initialize each agents policy and critic networks.
4: for episode = 1 to T do
5:
for t = 1 to episodeâ€“length do
6:
Get environment state st .
7:
For each agent i, select action ati = Ï€Î¸i (oti )

Execute actions at = [at1 , . . . , atN ]
t
Receive global rgt and local
 rewards rl .
t t t t t+1
10:
Store s , a , rl , rg , s
in replay buffer.
11:
end for
12:
/* Train global critic*/


0
13:
Sample minibatch of size S sj , aj , rjg , s j from
buffer.


0j
0
14:
(a01 , . . . , a0N ) := Ï€Î¸0 i (o0j
i ), . . . , Ï€Î¸N (oN )
 0

15:
Set ygj = rgj + Î³ mini=1,2 QgÏˆi0 s j , a01 , . . . , a0N
i
16:
Update global critics by minimizing

2
1 X j
yg âˆ’ QgÏˆii sj , aj1 , . . . , ajN
S j
8:
9:

(a) Random Landmarks

(b) Shopping Mall

(c) Street

(d) Pie-in-the-Face

Fig. 3: Learning curves representing the average cumulative
global reward. The higher reward represents higher protection
to the VIP (payload).

17:

Ïˆi0 â† Ï„ Ïˆi + (1 âˆ’ Ï„ ) Ïˆi0
18:
19:
20:

"

#

âˆ‡J(Î¸i ) = Es,aâˆ¼D âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QgÏˆ11 (s, a1 , . . . , aN )
"

21:
22:

#

23:

+ Eoi ,ai âˆ¼D âˆ‡Î¸i Ï€i (ai |oi ) âˆ‡ai QÏ€i (oi , ai )
(2)
The twin global critics are updated as

2 
gi
L (Ïˆi ) = Es,a,r,s0 QÏˆi (s, a1 , . . . , aN ) âˆ’ yg

Update target network parameters

if episode mod d then
/* Train local critics and update agent policies*/
for agent i = 1 to N do


0
Sample minibatch of size S sj , aj , rjl , s j
 0

0
0
Set y j = rijl + Î³QÏ€Ï†0 o j , Ï€Î¸0 i (oij )
i
Update local critic by minimizing

2
1 X j
y âˆ’ QÏ€Ï†i oj , aji
S j

24:

Î¸i = Î¸i +

where yg is defined as:
yg = rg + Î³ min QgÏˆi0 (s0 , a01 , . . . , a0N ) |a0 =Ï€0 (o0 )
i=1,2

i

i

i

i

Similarly, the local critics can be updated using TD3 update
style but for simplicity, we will use the standard DDPG
to update the local critics. The overall algorithm to which
we refer as Decomposed Multi-Agent Deterministic Policy
Gradient (DE-MADDPG) is described in Algorithm 1. The
overview of the architecture can be seen in Figure 2.

25:
26:





1X
âˆ‡Î¸i Ï€i ai |oji âˆ‡ai QgÏˆ1 sj , aj1 , . . . , ajN
S j




+âˆ‡Î¸i Ï€i ai |oji âˆ‡ai QÏ€Ï†i oj , aji

end for
Update target network parameters for each agent i
Î¸i0 â† Ï„ Î¸i + (1 âˆ’ Ï„ ) Î¸i0
Ï†0i â† Ï„ Ï†i + (1 âˆ’ Ï„ ) Ï†0i

27:
end if
28: end for

V. E XPERIMENTS
A. Environments
We perform our experiments using the defensive escort
problem on four VIP protection environments [16], [3]. This is
a medium-size collaborative problem where a defensive escort
team of agents is learning to maintain an optimal formation

around the VIP (payload). The objective of the defenders is to
minimize the potential physical attacks as the VIP is moving in
a variety of different real world scenarios and are implemented
in the Multi-Agent Particle Environment [20]. An illustration
of the environments can be seen in Figure 1.

0

0

Sum of rewards

Sum of rewards

Sum of rewards

Sum of rewards

the other bystanders influence their movement described
by laws of particle motion [32].
2
2
â€¢ Pie-in-the-Face: This scenario models a VIP walking a
4
4
â€œred carpetâ€, with bystanders standing behind designed
6
6
lines. In this scenario, an unruly bystander breaks the
DEÂ­MADDPG(TD3 + PER)
DEÂ­MADDPG(TD3 + PER)
DEÂ­MADDPG(TD3)
DEÂ­MADDPG(TD3)
line to approach the VIP (presumably, to throw a pie in
8
8
DEÂ­MADDPG(PER)
DEÂ­MADDPG(PER)
DEÂ­MADDPG
DEÂ­MADDPG
his/her face).
10
BaselineÂ­MADDPG
BaselineÂ­MADDPG
10
BaselineÂ­DDPG
BaselineÂ­DDPG
The state of the environment is given by the locations of
0 25 50 75 100 125 150 175 200
0 25 50 75 100 125 150 175 200 the landmarks, bystanders, VIP and the defensive team. To
Number of episodes (1e2)
Number of episodes (1e2)
closely represent a real world bodyguard that has a limited
(a) Random Landmarks
(b) Shopping Mall
range of perception, the observation of each agent is the
0
0
relative physical state of the nearest M bystanders, the VIP
and the remaining members of the defensive escort team
2
2
and represented as oi = [xj,...N +M ] âˆˆ Oi where xj is the
4
4
observation of the entity j from the perspective of agent i. In
6
6
DEÂ­MADDPG(TD3 + PER)
DEÂ­MADDPG(TD3 + PER)
our experiments, we used M = 5.
DEÂ­MADDPG(TD3)
DEÂ­MADDPG(TD3)
8
8
DEÂ­MADDPG(PER)
DEÂ­MADDPG(PER)
We chose the defensive escort team problem with these
DEÂ­MADDPG
DEÂ­MADDPG
10
10
environments, because it is a representative case of a well
BaselineÂ­MADDPG
BaselineÂ­MADDPG
BaselineÂ­DDPG
BaselineÂ­DDPG
structured dual-reward collaborative MARL problem, the en12
12
0 25 50 75 100 125 150 175 200
0
20
40
60
80
100 vironments are continuous with a relatively high dimensional
Number of episodes (1e2)
Number of episodes (1e2)
state space. Furthermore, previous work using these environ(c) Street
(d) Pie-in-the-Face
ments provide strong baselines against which the proposed
Fig. 4: Learning curves of the experiments representing the algorithms can be compared. Aligning our experimental setup
sum of local rewards. Notice that similar to global rewards, with [3], [16], we chose 4 bodyguards and 10 bystanders.
the local reward learning of MADDPG and DDPG is also
Decomposing the Reward Function
unstable and reaches only at sub-optimal performance.
In this section, we review the entangled multi-objective
reward function defined in [13], [3], explain the problems with
The environment consists of the VIP (payload), defensive it and decompose it to be used by DE-MADDPG.
As mentioned in the the previous section that the goal of the
escort team and one or more classes of bystanders. The
defensive
escort team is to learn an optimal formation around
VIP (brown disk) starts from the starting point and moves
the
VIP
to
minimize the physical threat while simultaneously
towards the destination landmark (green disk). The goal of the
maintain
a
certain distance from the VIP to follow the social
defensive escort team is learn an optimal formation around the
norms.
To
achieve
both of these objectives, the multi-objective
VIP to protect it from potential physical attacks. In order to
reward
function
for
each agent i is defined as:
maintain social norms, the defensive agents need to maintain
a certain distance from the VIP. To closely simulate real
world situations and providing substantial variability, four
different scenarios were developed that differ in the number,
arrangement of the landmarks and the behavior of the different
classes of bystanders:
Random Landmark: In this fully stochastic scenario,
landmarks are placed randomly in the area. The starting
point and destination for the VIP are randomly selected
landmarks. The bystanders are performing random waypoint navigation: they choose a landmark at random,
move towards it and repeat this process till the end of
the episode.
â€¢ Shopping Mall: In this scenario, landmarks are placed at
the edge of the environment emulating shops on a street
or shopping mall. The bystanders are moving between
randomly chosen shops.
â€¢ Street: This scenario aims to model a crowded sidewalk.
The bystanders are moving in two different directions
towards waypoints that are outside the current area. However, due to their proximity to each other, the position of
â€¢

rglobal

z
rtotal =Î± âˆ’1 +

M
Y

}|
(1 âˆ’ RT (V IP, bk , R))

k=1

!{
(3)

+ (1 âˆ’ Î±) (D (VIP , xi ))
|
{z
}
rlocal

where rglobal represents the reward that each agent receives
given the formation of the team around the VIP at time-step t,
therefore, represents the main objective of the team and rlocal
represents the reward that the agent receives for maintaining
a certain distance from the VIP and is defined as
ï£±
ï£´
m â‰¤ kxi âˆ’ VIP k2 â‰¤ d
ï£²0
D (VIP , xi ) = âˆ’1 otherwise
(4)
ï£´
ï£³
where m is the minimum distance the agent has to maintain
from VIP and d is the safe distance. As per our understanding,
rtotal has two problems. The first problem is the stability

issue since the policy oscillates between optimizing the global
reward and the local reward. This stability problem exacerbates
when either of the rewards are sparse and the other reward is
dense. This phenomenon is also explained in Section V-C.
The second problem is the Î± hyper-parameter. The Î± hyperparameter assigns weight to both rewards. Given the various
difficulty settings of the simulations, security should be prioritized in some simulations as compared to the others. Moreover,
as reinforcement learning experiments take substantial amount
of time, finding an optimal Î± can be time consuming.
We solve this problem by having a dual critic architecture
where the task of the global centralized critic is to approximate
the cumulative global reward while each agentâ€™s local critic approximates its own local reward. In this particular scenario, QgÏˆ
learns to approximate rglobal while QÏ€Î¸i approximates rlocal .
The benefit of this decomposition is more stable learning and
exclusion of the Î± hyper-parameter.
B. Evaluations
To evaluate the efficacy of DE-MADDPG and its variants
we compare our results with baseline MADDPG and DDPG on
defensive escort team problem in the environments described
above. We trained the global critic with two different approaches. In the first approach, we updated the policy by using
the standard Multi-Agent Deep Deterministic Policy Gradient
mentioned in Equation (1) while in the second approach, we
updated the policy using the Twin Delayed Deep Deterministic
Policy Gradient mentioned in Equation (2). Additionally, to
mitigate the global sparse reward problem, we replace the
standard replay buffer with prioritized experience replay buffer
(PER). We performed our experiments on 8 different seeds. We
used three layered neural networks for both the critic and actor
networks. For each environment, we trained each approach
for 20, 000 episodes except the Pie-in-the-Face environment
which was trained for 10, 000 episodes.
Figure 3 shows the learning curves of our experiments.
Figure 3a corresponds to the Random Landmark environment
and it can be seen that DE-MADDPG based approaches
outperforms the MADDPG and DDPG by a significant margin.
Similar observation can be seen for the other two environments i.e., Shopping Mall, and Street. Finally, for the last
environment Pie-in-the-Face, there is little difference between
the performance of the different approaches. A possible reason
for this is that this environment, focusing on a single attacking
bystander, makes the positioning choice less complex.
Though, fig. 3 shows the learning curves that visually
represents the performance of the different approaches, it does
not quantitatively explain the improvement in performance
across different approaches. To that end, we test our trained
policies across all environments on 8 different seeds. Table I
shows the average returns of the global reward over 1000
episodes. We notice that in complex environments such as
Shopping Mall and Random Landmarks, DE-MADDPG augmented with TD3 and PER were able to achieve 55% and 73%
better performance than baseline MADDPG. Similarly, on the

slightly less complex environments Street and Pie-in-the-Face,
the performance was about 61% and 100% better.
TABLE I: Average cumulative return of the global reward over
1000 episodes over 8 seeds. Maximum value for each task is
bolded. Â± corresponds to 95% confidence interval over seeds.
Environment

DE-MADDPG
(TD3+PER)

DE-MADDPG
(TD3)

DE-MADDPG
(PER)

DE-MADDPG

MADDPG

DDPG

Shopping Mall
Rand. Landm.
Street
Pie-in-the-Face

-4.87 Â± 0.06
-4.43 Â± 0.05
-2.13 Â± 0.06
-0.07 Â± 0.002

-5.61 Â± 0.08
-4.91 Â± 0.06
-2.38 Â± 0.07
-0.06 Â± 0.002

-6.17 Â± 0.08
-5.75 Â± 0.04
-2.36 Â± 0.07
-0.11 Â± 0.003

-6.27 Â± 0.07
-6.34 Â± 0.08
-2.66 Â± 0.08
-0.07 Â± 0.004

-7.59Â± 0.13
-7.67Â± 0.13
-3.43Â± 0.13
-0.12Â± 0.003

-9.51Â± 0.12
-10.22Â± 0.16
-3.81Â± 0.11
-0.10Â± 0.006

Figure 4 and Table II shows the learning curves and the
test results of the sum of the local rewards. Similar to the
global reward, it can be seen in Figure 4 and and Table II that
DE-MADDPG based approaches outperforms MADDPG and
DDPG results. One point to be noted here is that maintaining
a certain distance from a moving payload is fairly an easy
problem for standard reinforcement learning algorithms as the
reward is dense and the state space is relatively easy but adding
an additional objective such as global reward maximization not
only had catastrophic affect on the global reward maximization
but also negatively effected the learning of a trivial task.
TABLE II: Average cumulative return of the local reward over
1000 episodes over 8 seeds. Maximum value for each task is
bolded. Â± corresponds to 95% confidence interval over seeds.
Environment

DE-MADDPG
(TD3+PER)

DE-MADDPG
(TD3)

DE-MADDPG
(PER)

DE-MADDPG

MADDPG

DDPG

Shopping Mall
Rand. Landm.
Street
Pie-in-the-Face

-0.41 Â± 0.02
-0.15 Â± 0.02
-0.12 Â± 0.01
-0.11 Â± 0.01

-0.23 Â± 0.03
-0.19 Â± 0.01
-0.15 Â± 0.02
-0.28 Â± 0.01

-0.28 Â± 0.03
-0.25 Â± 0.02
-0.16 Â± 0.02
-0.12 Â± 0.01

-0.30 Â± 0.07
-0.22 Â± 0.03
-0.18 Â± 0.08
-0.31 Â± 0.01

-1.44Â± 0.07
-1.46Â± 0.06
-1.23Â± 0.08
-0.20Â± 0.01

-0.92Â± 0.05
-0.94Â± 0.05
-0.47Â± 0.04
-0.31Â± 0.01

A common pattern can be seen in Figure 3 and Figure 4
that DE-MADDPG based approaches not only achieve higher
global and local rewards, they achieve it significantly faster as
compared to MADDPG and DDPG. For example, in complex
environments such as Random Landmarks and Shopping Mall,
DE-MADDPG (TD3 + PER) reaches the maximum performance before 2500 episodes while the baseline MADDPG
reaches its best performance for Random Landmarks and
Shopping Mall environment at around 12, 000 episodes and
20, 000 episodes respectively.
C. Stability
One of the focal point of this study was to find a stable
solution that solves the dual-reward MARL problem. In this
section, we analyse the stability of DE-MADDPG based
solutions. Though, Figure 3 shows that DE-MADDPG when
augmented with TD3 outperforms MADDPG in maximizing
global reward, the average of results across all the seeds make
it difficult to analyze the stability. For that purpose, we chose
fixed single seed runs of all the solutions on the Shopping
Mall environment and plotted their learning curves. It can be
seen in Figure 5 that all DE-MADDPG based solutions are
stable in learning to maximize global reward as compared
to baseline MADDPG. This behavior is common across all
seeds and environments except Pie-in-the-Face as it is does
not provide any complexity.

TABLE IV: Average cumulative return of the local reward
of multi-scenario learning over 1000 episodes over 8 seeds.
Maximum value for each task is bolded. Â± corresponds to
95% confidence interval over seeds.
Environment

DE-MAUPG
(TD3+PER)

DE-MAUPG
(TD3)

DE-MAUPG
(PER)

DE-MAUPG

MAUPG

UVFA

Shopping Mall
Rand. Landm.
Street
Pie-in-the-Face

-0.56 Â± 0.02
-0.18 Â± 0.02
-0.16 Â± 0.02
-0.21 Â± 0.01

-0.27 Â± 0.03
-0.12 Â± 0.02
-0.15 Â± 0.02
-0.43 Â± 0.01

-0.22 Â± 0.03
-0.22 Â± 0.02
-0.17 Â± 0.02
-0.24 Â± 0.01

-0.25 Â± 0.07
-0.11 Â± 0.02
-0.13 Â± 0.02
-0.27 Â± 0.01

-1.04Â± 0.06
-0.89Â± 0.08
-0.64Â± 0.07
-0.27Â± 0.007

-0.96Â± 0.05
-0.71Â± 0.08
-0.60Â± 0.07
-0.22Â± 0.009

Shopping Mall
Rand. Landm.
Street
Pie-in-the-Face

Average return

Average return

Average return

Average return

task multi-scenario learning. The goal here is to learn a joint
policy Ï€ that performs equally well as scenario-dependant
policy. In [16] have introduced Multi-Agent Universal Policy
Fig. 5: Visualizing the single seed learning graph of shopping Gradients (MAUPG) to solve the multi-scenario learning and
mall environment. Notice that DE-MADDPG(TD3) almost evaluated it on the VIP protection environments similar to our
experiments. The main idea behind MAUPG is to replace the
becomes flat after 10,000 episodes.
standard centralized Q-functions in MADDPG with Universal
Value Function Approximators (UVFA). Given the similarity
0
0
between MADDPG and MAUPG, we replaced all the critics
5
5
in DE-MADDPG with UVFAs. For brevity, we will refer our
10
10
multi-scenario solution as Decomposed-Multi-Agent Univer15
15
sal Policy Gradients (DE-MAUPG) For our experiments, we
20
DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3 + PER)
25
kept our settings identical to DE-MADDPG experiments and
20
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(PER)
DEÂ­MAUPG(PER)
30
trained it for 20,000 episodes. The point to note here is that
25
DEÂ­MAUPG
DEÂ­MAUPG
35
BaselineÂ­MAUPG
BaselineÂ­MAUPG
unlike scenario-dependant training, where every scenario was
30
BaselineÂ­UVFA
BaselineÂ­UVFA
40
0 25 50 75 100 125 150 175 200 trained for 20,000 episodes, all scenarios are trained in parallel
0 25 50 75 100 125 150 175 200
using one joint policy Ï€.
Number of episodes (1e2)
Number of episodes (1e2)
Figure 6 shows the learning curves of the DE-MAUPG
(a) Random Landmarks
(b) Shopping Mall
and its variants. Similar to our results from Section V-B, de0
0.0
composition based learning solutions outperform the baseline
2
2.5
MAUPG. The point to be noted here is that not our solutions
4
5.0
learn to achieve higher reward but they also learn faster. This
6
7.5
can be easily seen in Figure 6a and Figure 6b where DE8
10.0
DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3 + PER)
MAUPG (TD3 + PER) reaches the maximum performance in
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(TD3)
10
12.5
DEÂ­MAUPG(PER)
DEÂ­MAUPG(PER)
less than 2500 episodes where baselines MAUPG reaches its
12
DEÂ­MAUPG
DEÂ­MAUPG
15.0
BaselineÂ­MAUPG
BaselineÂ­MAUPG
peak performance at 12,500 episodes in Random Landmarks
14
BaselineÂ­UVFA
BaselineÂ­UVFA
17.5
0 25 50 75 100 125 150 175 200
0 25 50 75 100 125 150 175 200 environment and does not even reach its peak performance
before 19,000 episodes for the Shopping Mall environment.
Number of episodes (1e2)
Number of episodes (1e2)
Table III quantifies the improvement of DE-MAUPG in
(c) Street
(d) Pie-in-the-Face
maximzing global reward when compared to MAUPG and
Fig. 6: Learning curves representing the average cumulative UVFA. We find that the decomposition based approaches
global reward of multi-scenario learning experiments..
achieve 81% on the Shopping Mall environment and 41%
TABLE III: Average cumulative return of the global reward
on the Random Landmarks environment. Similarly Table IV
of multi-scenario learning over 1000 episodes over 8 seeds.
quantifies the improvement of DE-MAUPG in maximzing
Maximum value for each task is bolded. Â± corresponds to
local reward when compared to MAUPG and UVFA.
95% confidence interval over seeds.
E. Computational Evaluations
Environment
DE-MAUPG
DE-MAUPG
DE-MAUPG
DE-MAUPG
MAUPG
UVFA
(TD3+PER)

(TD3)

(PER)

-3.85 Â± 0.07
-3.91 Â± 0.07
-2.54Â± 0.08
-0.07 Â± 0.003

-4.34 Â± 0.08
-3.86 Â± 0.07
-3.22 Â± 0.09
-0.13Â± 0.008

-5.70 Â± 0.09
-4.65 Â± 0.08
-3.58 Â± 0.11
-0.08 Â± 0.003

-5.98 Â± 0.10
-3.54 Â± 0.08
-3.51 Â± 0.10
-0.07 Â± 0.002

-6.97Â± 0.11
-5.02Â± 0.14
-3.97Â± 0.14
-0.12Â± 0.003

-6.94Â± 0.11
-5.62Â± 0.11
-4.34Â± 0.14
-0.19Â± 0.008

D. Multi-Scenario Experiments
Multi-agent reinforcement learning is sensitive to distortions
and does not work well if the trained policies are deployed
on scenarios other than the scenario on which the policies
are trained on. This problem is generally referred as single-

In this section we evaluate the growth of parametric space
as the number of agents increases. Moreover, we empirically
evaluate the computational time for DE-MADDPG and compare it with MADDPG and DDPG. The main component of the
MADDPG that fuels its learning are the distributed centralized
Q-functions. As the number of the agents grow, the input
space of those Q-functions increase quadratically. Concretely,
assuming all the agents have identical observation and action
space, the number of trainable parameters can be represented
by O(n2 (odim + adim)). Where n is the number of agents,

0

4
6

DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(PER)
DEÂ­MAUPG
BaselineÂ­MAUPG
BaselineÂ­UVFA

8
10

2
4

3.0

6
10

0 25 50 75 100 125 150 175 200

Number of episodes (1e2)

Number of episodes (1e2)

(b) Shopping Mall

0

0

2

2

DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(PER)
DEÂ­MAUPG
BaselineÂ­MAUPG
BaselineÂ­UVFA

8
10

0 25 50 75 100 125 150 175 200

Number of episodes (1e2)
(c) Street

Sum of rewards

Sum of rewards

(a) Random Landmarks

6

DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(PER)
DEÂ­MAUPG
BaselineÂ­MAUPG
BaselineÂ­UVFA

8

0 25 50 75 100 125 150 175 200

4

1e6

3.5

4
6

DEÂ­MAUPG(TD3 + PER)
DEÂ­MAUPG(TD3)
DEÂ­MAUPG(PER)
DEÂ­MAUPG
BaselineÂ­MAUPG
BaselineÂ­UVFA

8
10
12

Number of parameters

2

Sum of rewards

Sum of rewards

0

MADDPG
DEÂ­MADDPG(TD3)
DEÂ­MADDPG
DDPG
MAUPG
DEÂ­MAUPG(TD3)
DEÂ­MAUPG
UVFA

2.5
2.0
1.5
1.0
0.5
0.0

5

10
15
20
Number of Learning Agents

25

Fig. 8: Number of trainable parameters in the main Qnetworks. Notice that 25 agents can be trained by DEMADDPG approaches using the same number of parameters
as compared to 10 agents if MADDPG is used.

TABLE V: Average training time for experiments in minutes
over 8 different seeds. Minimum value for each task is bolded.
0 25 50 75 100 125 150 175 200
Â± corresponds to standard deviation across seeds
Number of episodes (1e2)
(d) Pie-in-the-Face

Fig. 7: Learning curves representing the sum of local rewards
of multi-scenario learning experiments.

odim and adim represents the dimensionality of observation
and action space respectively. Alternatively, DE-MADDPG
solves this scalability problem by having a shared global centralized Q-function whose parametric space increases linearly
and can be represented as O(n(odim+adim)). In Figure 8, we
show the growth of number of parameters of all the main Qnetworks of MADDPG, DE-MADDPG and its TD3 variant. It
can be seen that our solution takes significantly small number
of parameters as MADDPG and MAUPG. Note that the figure
only represents the number of trainable parameters in the main
Q-networks and does not include target or policy networks as
the growth of the policy network parameters are identical and
no gradient based learning happens in the target networks. All
these statements hold true for their UVFA variants.
We empirically verified the benefits of parametric reduction
by measuring the time taken to train the experiments. It can be
seen in Table V that DE-MADDPG based approaches always
train faster than baseline MADDPG. The complete details
about the network architecture can be seen in Section VI.
We do not provide time comparisons for DDPG and UVFA
variants as the experiments were run on multiple machines
with a variety of hardware thus making it difficult for a fair
comparison.
VI. E XPERIMENTAL D ETAILS
A. Network Architecture
Both actor and critic networks for all agents consists of 2
hidden layers containing 64 units in each layer. The hidden
layers uses ReLU activation function while the output layers

Environment

DE-MADDPG
(TD3+PER)

DE-MADDPG
(TD3)

DE-MADDPG
(PER)

DE-MADDPG

MADDPG

Shopping Mall
Rand. Landm.
Street
Pie-in-the-Face

168.25 Â± 7.22
168.87 Â± 0.78
397.62 Â± 4.71
64.25 Â± 1.23

152.85 Â± 6.31
151.25 Â± 1.19
352.87 Â± 5.63
59.25 Â± 0.59

167.25 Â± 4.40
164.14 Â± 1.35
356.87 Â± 13.97
63.5Â± 1.09

149.5 Â± 1.65
146.875 Â± 1.16
340.75 Â± 16.74
55.12 Â± 1.05

268.5Â± 21.21
262.25Â± 2.86
462.75Â± 43.14
72.79Â± 1.16

TABLE VI: The parameters used for various variations of
DE-MADDPG and the baseline algorithm MADDPG in the
experiments.
Parameter

DE-MADDPG
(TD3+PER)

Episodes
20k
Replay buffer
106
Minibatch size
2048
Steps per train Qg 4
Steps per train Ql 2
Max env steps
25
PER Î±
0.6
PER Î²
0.4
PER 
1e-6
PER Î² decay
10000

DE-MADDPG
(TD3)

DE-MADDPG
(PER)

DE-MADDPG

MADDPG

DDPG

20k
106
2048
4
2
25
N/A
N/A
N/A
N/A

20k
106
2048
4
2
25
0.6
0.4
1e-6
10000

20k
106
2048
4
2
25
N/A
N/A
N/A
N/A

20k
106
2048
N/A
2
25
N/A
N/A
N/A
N/A

20k
106
2048
N/A
2
25
N/A
N/A
N/A
N/A

of both networks use linear activation function. Both networks
are initialized using Xavier normal initializers however, the
output layer of the target critics were initialized with uniform
random values between (-0.01 and 0.01) to enable one-step
look ahead learning of the critics after each training cycle.
To keep experiments as fair as possible, we initialized the
network parameters with same seed across all the experiments.
Complete hyperparameter details can be seen in Table VI. Note
that same configurations were used for multi-scenario learning
experiments.
VII. C ONCLUSIONS
In this paper, we focused on the dual-reward MARL: a
collaborative setting where a group of learning agents have to
simultaneously learn to maximize the collective global reward
and individual local reward. To solve the problem, we proposed the Decomposed Multi-Agent Deep Deterministic Policy
Gradient (DE-MADDPG) algorithm and applied it to the
problem of defensive escort team: how can agent learn a policy

to maintain an optimal formation around the VIP to protect
him/her from possible physical attack. We first demonstrated
that decomposing a multi-objective reward function leads to
higher and more stable performance. We compared our results
with the MADDPG algorithm and achieved at least 50% better
performance in terms of the collected reward. Additionally,
we showed that our solution is computationally efficient and
requires a significantly lower number of parameters while
achieving better performance than the baseline. Finally, we
showed that by replacing the standard critics with UVFAs,
our solution also outperforms MAUPG which is a baseline
algorithm for single-task multi-scenario learning in multiagent reinforcement learning.
R EFERENCES
[1] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel, â€œMultiagent reinforcement learning in sequential social dilemmas,â€ in Proc.
of the 16th Intâ€™l Conf. on Autonomous Agents and Multiagent Systems
(AAMAS-2017), pp. 464â€“473, 2017.
[2] Y. Cao, W. Yu, W. Ren, and G. Chen, â€œAn overview of recent progress
in the study of distributed multi-agent coordination,â€ IEEE Transactions
on Industrial informatics, vol. 9, no. 1, pp. 427â€“438, 2012.
[3] H. U. Sheikh, M. Razghandi, and L. BoÌˆloÌˆni, â€œLearning distributed
cooperative policies for security games via deep reinforcement learning,â€ in 2019 IEEE 43rd Annual Computer Software and Applications
Conference (COMPSAC-2019), vol. 1, pp. 489â€“494, Jul 2019.
[4] OpenAI, OpenAI Five, 2018. https://blog.openai.com/openai-five/.
[5] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman,
N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis,
K. Kavukcuoglu, and T. Graepel, â€œHuman-level performance in firstperson multiplayer games with population-based deep reinforcement
learning,â€ arXiv preprint arXiv: 1807.01281, 2018.
[6] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou,
M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard,
D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang,
T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. WuÌˆnsch, K. McKinney,
O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps,
and D. Silver, â€œGrandmaster level in starcraft ii using multi-agent
reinforcement learning,â€ Nature, 2019.
[7] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew,
and I. Mordatch, â€œEmergent tool use from multi-agent autocurricula,â€
arXiv preprint arXiv:1909.07528, 2018.
[8] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
â€œCounterfactual multi-agent policy gradients,â€ in Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-2018),
2018.
[9] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster,
and S. Whiteson, â€œQMIX: Monotonic value function factorisation for
deep multi-agent reinforcement learning,â€ in Proceedings of the 35th
International Conference on Machine Learning, pp. 4295â€“4304, 2018.
[10] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, â€œMultiagent actor-critic for mixed cooperative-competitive environments,â€ in
Advances in Neural Information Processing Systems 30, pp. 6379â€“6390,
2017.
[11] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell, â€œRobust multiagent reinforcement learning via minimax deep deterministic policy
gradient,â€ in Proc. of the Thirty-Third AAAI Conference on Artificial
Intelligence (AAAI-2019), 2019.
[12] J. Yang, A. Nakhaei, D. Isele, H. Zha, and K. Fujimura, â€œCM3:
cooperative multi-goal multi-stage multi-agent reinforcement learning,â€
arXiv preprint arXiv:1809.05188, 2018.
[13] H. U. Sheikh and L. BoÌˆloÌˆni, â€œDesigning a multi-objective reward
function for creating teams of robotic bodyguards using deep reinforcement learning,â€ in Proc. of 1st Workshop on Goal Specifications for
Reinforcement Learning (GoalsRL-2018) at ICML 2018, July 2018.

[14] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, â€œPrioritized experience
replay,â€ in International Conference on Learning Representations (ICLR2016), 2016.
[15] S. Fujimoto, H. van Hoof, and D. Meger, â€œAddressing function approximation error in actor-critic methods,â€ in Proc. of the 35th International
Conference on Machine Learning (ICML-2018), pp. 1587â€“1596, 2018.
[16] H. U. Sheikh and L. BoÌˆloÌˆni, â€œEmergence of scenario-appropriate collaborative behaviors for teams of robotic bodyguards,â€ in Proc. of the
18th International Conference on Autonomous Agents and Multi-Agent
Systems (AAMAS-2019), pp. 2189â€“2191, 2019.
[17] M. Tan, â€œMulti-agent reinforcement learning: Independent vs. cooperative agents,â€ in Proceedings of the tenth international conference on
machine learning, pp. 330â€“337, 1993.
[18] M. L. Littman, â€œMarkov games as a framework for multi-agent reinforcement learning,â€ in Machine Learning Proceedings 1994, pp. 157â€“163,
Elsevier, 1994.
[19] J. Hu and M. P. Wellman, â€œNash Q-learning for general-sum stochastic
games,â€ Journal of Machine Learning Research, vol. 4, pp. 1039â€“1069,
Nov 2003.
[20] I. Mordatch and P. Abbeel, â€œEmergence of grounded compositional
language in multi-agent populations,â€ in Proc. of AAAI International
Conference on Artificial Intelligence (AAAI-2017), 2017.
[21] L. Panait and S. Luke, â€œCooperative multi-agent learning: The state of
the art,â€ Autonomous Agents and Multi-Agent Systems, vol. 11, no. 3,
pp. 387â€“434, 2005.
[22] J. L. Austerweil, S. Brawner, A. Greenwald, E. Hilliard, M. Ho,
M. L. Littman, J. MacGlashan, and C. Trimbach, â€œHow other-regarding
preferences can promote cooperation in non-zero-sum grid games,â€ in
Proceedings of the AAAI Symposium on Challenges and Opportunities
in Multiagent Learning for the Real World, 2016.
[23] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, â€œFully decentralized
multi-agent reinforcement learning with networked agents,â€ in Proc. of
the 35th International Conference on Machine Learning (ICML-2018),
pp. 5872â€“5881, 2018.
[24] S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian, â€œDeep
decentralized multi-task multi-agent reinforcement learning under partial observability,â€ in International Conference on Machine Learning,
pp. 2681â€“2690, 2017.
[25] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg,
D. Silver, and K. Kavukcuoglu, â€œFeudal networks for hierarchical reinforcement learning,â€ in Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 3540â€“3549, JMLR. org, 2017.
[26] C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade,
I. Mordatch, and P. Abbeel, â€œVariance reduction for policy gradient with
action-dependent factorized baselines,â€ in Proc. of the 6th Intâ€™l Conf. on
Learning Representations (ICLR), 2018.
[27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal policy optimization algorithms,â€ arXiv preprint arXiv:1707.06347,
2017.
[28] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
â€œDeterministic policy gradient algorithms,â€ in Proc. of the 31st Intâ€™l
Conf. on Machine Learning (ICML-2014), pp. 387â€“395, 2014.
[29] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, â€œContinuous control with deep reinforcement
learning,â€ in Proc. of the 3rd Intâ€™l Conf. on Learning Representations
(ICLR-2015), 2015.
[30] H. Van Hasselt, A. Guez, and D. Silver, â€œDeep reinforcement learning
with double Q-learning,â€ in Proc of the Thirtieth AAAI Conference on
Artificial Intelligence (AAAI-2016), 2016.
[31] J. Ackermann, V. Gabler, T. Osa, Alec, and M. Sugiyama, â€œReducing
overestimation bias in multi-agent domains using double centralized
critics,â€ arXiv preprint arXiv:1910.01465, 2019.
[32] T. Vicsek, A. CziroÌk, E. Ben-Jacob, I. Cohen, and O. Shochet, â€œNovel
type of phase transition in a system of self-driven particles,â€ Physical
Review Letters, vol. 75, no. 6, p. 1226, 1995.

