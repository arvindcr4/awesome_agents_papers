Graph Convolutional Value Decomposition in
Multi-Agent Reinforcement Learning

arXiv:2010.04740v2 [cs.LG] 10 Feb 2021

Navid Naderializadeh
Fan H. Hung
Sean Soleyman
Deepak Khosla

nnaderializadeh@hrl.com
fhhung@hrl.com
ssoleyman@hrl.com
dkhosla@hrl.com

Information & Systems Sciences Lab
HRL Laboratories, LLC
Malibu, CA 90265, USA

Abstract
We propose a novel framework for value function factorization in multi-agent deep reinforcement learning (MARL) using graph neural networks (GNNs). In particular, we
consider the team of agents as the set of nodes of a complete directed graph, whose edge
weights are governed by an attention mechanism. Building upon this underlying graph,
we introduce a mixing GNN module, which is responsible for i) factorizing the team stateaction value function into individual per-agent observation-action value functions, and ii)
explicit credit assignment to each agent in terms of fractions of the global team reward.
Our approach, which we call GraphMIX, follows the centralized training and decentralized
execution paradigm, enabling the agents to make their decisions independently once training
is completed. We show the superiority of GraphMIX as compared to the state-of-the-art
on several scenarios in the StarCraft II multi-agent challenge (SMAC) benchmark. We
further demonstrate how GraphMIX can be used in conjunction with a recent hierarchical
MARL architecture to both improve the agents‚Äô performance and enable fine-tuning them
on mismatched test scenarios with higher numbers of agents and/or actions.

1. Introduction
Multi-agent systems are ubiquitous in numerous application areas, such as autonomous
driving (Zhao et al., 2019; Chu et al., 2020), drone swarms (Zanol et al., 2019), communication
systems (Naderializadeh et al., 2020), multi-robot search and rescue (Malaschuk and Dyumin,
2020), and smart grid (Xie et al., 2019). In many of these domains, instructive feedback is
not available, as there are no ground-truth solutions or decisions available. These phenomena
have given rise to a plethora of literature on multi-agent reinforcement learning algorithms,
with a special focus on deep-learning-driven methods over the past few years.
More recently, algorithms with centralized training and decentralized execution have
gained interest, due to their applicability in practical real-world scenarios. In (Lowe et al.,
2017; Foerster et al., 2018), policy gradient algorithms are considered, where the actors,
which are responsible for taking the actions for each agent, are decentralized, while the critic
is assumed to be centralized, trained in conjunction with the actors in an end-to-end manner
over the course of training. The authors in (Sunehag et al., 2018; Rashid et al., 2018) take
a different value-based approach to train the agents. Specifically, they introduce a value
1

Naderializadeh, Hung, Soleyman, and Khosla

factorization module (linear in the case of VDN (Sunehag et al., 2018) and a state-based
non-linear multi-layer perceptron (MLP) in the case of QMIX (Rashid et al., 2018)), which
is responsible for implicit credit assignment; i.e., decomposing the global state-action value
function to individual observation-action value functions for different agents.
One of the main drawbacks of the aforementioned algorithms is that they do not explicitly
capture the underlying structure of the team of agents in the environment, which can be
modeled using a graph topology. There have been some attempts to connect multi-agent
deep reinforcement learning (MARL) with graph representation learning methods in the
literature. As an example, (Jiang et al., 2020) propose a MARL algorithm based on the
graph convolutional network (GCN) architecture (Kipf and Welling, 2017). However, it needs
both centralized training and centralized execution (or at the very least, the agents need to
communicate with each other multiple times during the inference phase), and therefore, it
does not allow for decentralized decision making by the agents.
In this paper, we propose an algorithm for training MARL agents via graph neural
networks (GNNs). We consider a complete directed weighted graph, where each node
represents an agent, and there is a directed edge between any pair of nodes. We use an
attention mechanism to dynamically adjust the weights of the edges based on the agents‚Äô
observation-action histories during an episode. Leveraging such a graph structure, we
propose to use a mixing GNN module that produces a global state-action value function
at its output given the individual agents‚Äô observation-action value functions at the input.
Similar to (Rashid et al., 2018), a monotonicity constraint is enforced on the GNN, ensuring
that the individual agent decisions are consistent with the case where a central entity would
be responsible for making the decisions for all the agents.
We also use the mixing GNN as a backbone to derive an effective fraction of global team
reward for each of the agents based on their corresponding output node embeddings. We
use these reward fractions to minimize per-agent local losses, alongside the global loss using
the global state-action value function. The mixing GNN, the attention mechanism, and the
agent parameters are all trained centrally in an end-to-end fashion, and after training is
completed, each agent can make its decisions in a decentralized manner.
We evaluate our proposed algorithm, which we refer to as GraphMIX, on the StarCraft
II multi-agent challenge (SMAC) benchmark (Samvelyan et al., 2019) and demonstrate that
it outperforms the state-of-the-art QMIX (Rashid et al., 2018) and Weighted QMIX (Rashid
et al., 2020) algorithms across several hard and super hard scenarios. We also show that
GraphMIX can be combined with a recently-proposed hierarchical MARL framework, namely
RODE (Wang et al., 2021b), to provide a further performance improvement over both vanilla
GraphMIX and RODE approaches, as well as the capability to fine-tune agents on test
scenarios with different numbers of allied/enemy units as compared to the training scenarios.

2. Related Work
Multi-Agent Deep Reinforcement Learning A growing focus in the recent deep
reinforcement learning literature is on multi-agent cooperation (Lowe et al., 2017; Foerster
et al., 2018; Sunehag et al., 2018; Rashid et al., 2018; Papoudakis et al., 2020). Methods exist
on a spectrum from a single unified or centralized agent to independent and decentralized
agents. At one extreme end of this spectrum, a MARL problem might be reduced to a
2

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

standard deep reinforcement learning problem, with a single centralized network returning
a joint action vector for all agents. In this extreme case, issues that arise, such as the
exponential increase in the joint action space dimensionality, are general motivations for
the development of new algorithms in the MARL literature, as they create difficulties in
generalization to different numbers of agents, parameter memory scalability, and training
sample efficiency. At the other extreme, independent and decentralized agents face difficulty
as coordination becomes more complex. A recent trend that aims to find an effective
middle ground is centralized training and decentralized execution (CTDE). Methods in the
CTDE paradigm aim to produce decentralized controllers, and enforce implicit coordination
with a centralized architecture used only in training. A related trend, which skews more
toward a centralized agent, studies the impact of communication between agents during
execution (Foerster et al., 2016; Sukhbaatar et al., 2016).
Value Decomposition Methods for MARL Most related to our work is a branch
of value-based MARL methods, which decompose a joint state-action value function to allow
individual agents to be trained from a single global reward. VDN (Sunehag et al., 2018)
initially approximated a joint state-action value function over all agents‚Äô actions as the sum
of individual observation-action value functions from each agent. QMIX (Rashid et al., 2018)
observes that the joint state-action value function can more generally be represented by a
monotonic function of individual observation-action value functions. Additionally, QMIX
allows the joint state-action value function to be informed by global information, which
is unavailable to the individual observation-action value functions. The specifics of this
factorization continue to be analyzed and improved in several recent works, such as (Son
et al., 2019; Yang et al., 2020; Rashid et al., 2020; Wang et al., 2021a). In particular, the
authors in (Rashid et al., 2020) introduce a variation of QMIX, called Weighed QMIX, which
expands the representational limits of QMIX by giving a higher weight to more important
joint action vectors. Moreover, in (Wang et al., 2021a), the authors propose a new duplex
dueling value decomposition architecture, called QPLEX, which encompasses a complete
class of functions that satisfy the so-called Individual-Global-Max (IGM) principle (Son et al.,
2019).
Graph Neural Networks In order to manage arbitrarily-structured input data that
cannot be modeled as regular grids, graph neural networks (GNNs) have gained popularity as
a prominent method for incorporating neighborhood information among nodes in graph-based
data structures (Scarselli et al., 2008; Kipf and Welling, 2017; Zhou et al., 2018; Xu et al.,
2019). Similarly to convolutional and recurrent neural networks, GNNs formalize structured
treatment of data that would otherwise be concatenated and treated purely as vectors in a
high-dimensional space. Due to the flexibility in modeling structured data, GNNs have seen
widespread application in areas such as knowledge representation (Park et al., 2019), natural
language processing (Ji et al., 2019), social network analysis (Fan et al., 2019), wireless
communications (Eisen and Ribeiro, 2020), chemistry (Hu* et al., 2020), and physics (Ju
et al., 2020).
GNN-Based MARL Frameworks In MARL, GNN-based architectures have recently
been used to improve sample efficiency by adding permutation-invariance to multi-agent
critics (Liu et al., 2020) and emphasize observations of neighborhoods in individual agent
controllers (Jiang et al., 2020). Graph structures have also been tied into neural attention modules, especially when using attention mechanisms to compute graph edge weights (Veliƒçkoviƒá
3

Naderializadeh, Hung, Soleyman, and Khosla

et al., 2018; Thekumparampil et al., 2018). These mechanisms gained popularity in sentence
translation tasks for handling associations between structured data components (Vaswani
et al., 2017; Devlin et al., 2019), and they have seen use in general reinforcement learning as
well (Zambaldi et al., 2019; Baker et al., 2020; Iqbal and Sha, 2019).
In this work, we re-visit the analysis of the joint state-action value function to reflect
the geometric graphical structure of the multi-agent setting. We also demonstrate how
our method can be applied in a recently-proposed hierarchical MARL framework, called
RODE (Wang et al., 2021b), where each agent first selects a role, defined as a cluster of
actions having similar effects on the environment dynamics, and it then chooses an action
within the action subspace of the selected role. We specifically show how we can factorize both
the high-level and the low-level global value functions (for the roles and actions, respectively)
via the proposed method to further enhance the performance of the agents.

3. System Model
We consider a multi-agent environment, where a team of M agents collaborate with each other
to solve a cooperative task. In particular, we consider a decentralized partially-observable
Markov decision process (Dec-POMDP), represented by a tuple hM, S, O, Z, A, T, Rg , Œ≥i. At
each time step t, the environment is in global state s(t) ‚àà S, with S denoting the global
state space, and each agent m ‚àà {1, . . . , M } receives an observation om (t) = O(st , m) ‚àà Z,
where Z denotes the per-agent observation space, and O : S √ó {1, . . . , M } ‚Üí Z denotes the
per-agent observation function. Upon receiving its observation, each agent m ‚àà {1, . . . , M }
takes an action am (t) ‚àà A, with A denoting the per-agent action space. These actions will

cause the environment to transition to the next state s(t + 1) ‚àº T s0 |s(t), {am (t)}M
m=1 ,
with T : S √ó S √ó AM ‚Üí [0, 1] denoting the state transition function.
This transition is

M ‚ÜíR
accompanied with a global reward rg (t) = Rg s(t), {am (t)}M
,
where
R
g :S √óA
m=1
denotes the global reward function.
In this setting, the goal of the agents at each time step t is to take a joint set of
actions
{am (t)}M
m=1 so as to maximize the discounted cumulative global reward, defined as
P‚àû t‚àít
0
0
rg (t ). In order to make such decisions, each agent m ‚àà {1, . . . , M } is equipped
t0 =t Œ≥
with a policy œÄm : A √ó (Z √ó A)‚àó ‚Üí [0, 1] that determines its action given its observationaction history, where (Z √ó A)‚àó denotes the set of all possible observation-action histories. In
particular, the action of agent m attime step t is distributed as am (t) ‚àº œÄm (a|œÑm (t)), where
œÑm (t) = {om (t0 )}tt0 =1 , {am (t0 )}t‚àí1
t0 =1 denotes the set of current and past local observations,
as well as past actions of agent m at time step t. Letting œÄ denote the set of policies of all
agents, its induced joint state-action value function is defined as
"‚àû
#
X

0
QœÄ s(t), {am (t)}M
Œ≥ t‚àít rg (t0 ) ,
(1)
m=1 = E
t0 =t

where the expectation is taken with respect to the set of future states and actions.

4. GraphMIX: Graph-Based Value Function Factorization
We assume that each agent m ‚àà {1, . . . , M } is equipped with a deep recurrent Q-network
(DRQN), coupled with a policy œÄm . At each time step t, the agent chooses its action
4

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

Decentralized Agents
(Execution)

ùúè/

Agent 1

Centralized Mixing GNN
(Training)

ùëÑ## ùúè/ , ùëé/

/
$
ùúì&'()*+,,.
, ‚Ä¶ , ùúì&'()*+,,.

‚Ä¶

ùúè7

Agent ùëÄ

ùë§. , ùúì0,12'34,.

‚Ä¶

ùëÑ# ùë†, ùëé5 7
56/

Global loss
‚Ñí:8')18

ùëÑ#$ ùúè7 , ùëé7
ùëü9

‚Ñé!" = ùëÑ#! ùúè! , ùëé! ‚àà ‚Ñù

‚Ñé!$ ‚àà ‚Ñù%"

ùõº!

Local losses
‚Ñí!,8'&18 !‚ààùí±

ùë§8'&18 , Softmax

Figure 1: The GraphMIX architecture, comprising individual agents that make their decisions
in a decentralized manner, alongside a mixing GNN that is used for centralized
training of the agents.

in a decentralized manner based on its current and past local observations, alongside its
past actions. In particular, it will choose an action am (t) according to the distribution
œÄm (a|œÑm (t)). This will lead to its local observation-action value function QœÄm (œÑm (t), am (t)).
Note that such individual value functions are only implicitly defined in the case of a single
global reward emitted by the environment, and their implications will become more clear as
we describe the value function factorization procedure in this section.
Next, we model the team of agents as a directed graph, denoted by G = (V, E), where
V denotes the set of M graph nodes, each of which corresponds to an agent. Due to this
one-to-one correspondence, hereafter in this paper, we use ‚Äúagent‚Äù and ‚Äúnode‚Äù interchangeably.
Moreover, E = V √ó V denotes the set of M 2 graph edges, implying that the graph is a
complete directed graph. For two agents u, v ‚àà V, we let wuv denote the weight of the edge
from node v to node u. These edge weights can possibly be varying over time, and we will
later describe how they are determined.
To train the agents, we borrow the notion of monotonic value function factorization from (Rashid et al., 2018), where
the idea is to decompose the global state-action

value function QœÄ s(t), {am (t)}M
into
a set of local observation-action value functions
m=1
{QœÄm (œÑm (t), am (t))}M
such
that
an
increase
in the local observation-action value function
m=1
of each agent leads to a corresponding increase in the global state-action value function. To
be precise, the decomposition can be written as



œÄ1
œÄM
QœÄ s(t), {am (t)}M
=
Œ®
Q
(œÑ
(t),
a
(t)),
.
.
.
,
Q
(œÑ
(t),
a
(t))
,
1
1
M
M
mix
m=1
where the mixing function Œ®mix satisfies
‚àÇŒ®mix (x1 , . . . , xM )
‚â• 0, ‚àÄm ‚àà {1, . . . , M }.
‚àÇxm

(2)

The monotonicity condition in (2) is a sufficient condition for the Individual-Global-Max
(IGM) principle, defined by (Son et al., 2019), which ensures that if each agent takes the
5

Naderializadeh, Hung, Soleyman, and Khosla

action that maximizes its local observation-action value function, it would also be the best
action for the entire team.
In this work, we propose to use a graph-based approach for combining the local per-agent
observation-action value functions into the global state-action value function. In particular,
we leverage the aforementioned graph of the agents G to define a mixing GNN architecture,
as shown in Figure 1. In this GNN, each node v ‚àà V starts with a scalar feature, which is
the corresponding agent‚Äôs local observation-action value function, i.e.,
h0v = QœÄv (œÑv , av ),

(3)

where we have dropped the time dependence for brevity. The features are then passed
through L hidden layer(s). At the lth layer, l ‚àà {1, . . . , L}, the features of each node v ‚àà V
are updated as


l
l‚àí1
hlv = œàcombine,+
hl‚àí1
,
{h
,
w
}
(4)
uv u‚ààV\{v} ,
v
u
l
where œàcombine,+
(¬∑) denotes a monotonically-increasing (and potentially non-linear) parametrized
combining function. This implies that each node uses its own features and the other agents‚Äô
features, alongside its outgoing edge weights to map its input feature (vector) of dimension
Fl‚àí1 to an output (vector) of dimension Fl , with F0 = 1.
At the output of the Lth layer, each node v ‚àà V will end up with a feature vector, i.e.,
FL
node embedding, hL
v ‚àà R . We then define the global state-action value function as

T
L
QœÄ s, {am }M
(5)
m=1 = w+ œàreadout ({hv }v‚ààV ),
M

z
}|
{
where œàreadout : RFL √ó ¬∑ ¬∑ ¬∑ √ó RFL ‚Üí RFL is a graph readout operation (such as average/max
pooling), and w+ ‚àà RF+L is a non-negative parameter vector that maps the graph embedding
œàreadout ({hL
v }v‚ààV ) into the global state-action value function. Note that the monotonicity of
l
œàcombine,+
(¬∑) in (4) and the non-negativity of w+ in (5) guarantee that the mixing monotonicity
condition in (2) is satisfied.
In addition, we introduce another weight vector wlocal ‚àà RFL that maps each node
embedding to an effective reward fraction for the corresponding agent, defined as



T
exp wlocal
hL
v
T
L
.
Œ±v = SoftmaxV wlocal hv = P
(6)
T
L
u‚ààV exp wlocal hu
We interpret these values as the effective fraction of the global reward that each agent receives
at each time step. The significance of wlocal lies in the fact that its parameters do not need to
be non-negative, hence improving the expressive power of the mixing GNN beyond monotonic
functions.
Our proposed architecture, which we refer to as GraphMIX, is trained by minimizing the
aggregate loss
X
L = Lglobal + Œªlocal
Lv,local ,
(7)
v‚ààV

6

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

with the global loss defined as
"
2 #

X 


‚àí QœÄ s, {am }M
,
Lglobal =
rg + Œ≥ 0max0 QœÄ s0 , {a0m }M
m=1
m=1
a1 ,...,aM

i‚ààB

(8)

i

where B denotes a batch of transitions that are sampled from the experience buffer at each
round of training, and s0 and {a0m }M
m=1 respectively correspond to the environment state and
agents‚Äô actions in the following time step. Moreover, for each node v ‚àà V, the local loss is
defined as
"

2 #
X 
Lv,local =
Œ±v rg + Œ≥ max
QœÄv (œÑv0 , a0v ) ‚àí QœÄv (œÑv , av )
,
(9)
0
i‚ààB

av

i

where œÑv0 denotes the observation-action history of the agent corresponding to node v at
the following time step. Note how minimizing the local losses in (9) creates a shortcut for
backpropagating gradients to the individual agent networks, compared to the alternative path
through the mixing GNN by minimizing the global loss in (8). Moreover, such local updates
of the agent networks maintain the consistency of the global policy and the decentralized
agent policies. The balance between the two loss types is attained through a hyperparameter
Œªlocal in (7), which needs to be tuned to optimize the agents‚Äô performance in any given
environment.
Similar to (Rashid et al., 2018), we use a hypernetwork architecture to determine the
parameters of the mixing GNN during training. In particular, each of the mixing GNN
parameter matrices/vectors is derived by reshaping and taking the absolute values of the
outputs of a multi-layer perceptron, which takes the global state as the input, hence also
satisfying the monotonicity constraint for the mixing GNN.
4.1 Attention-Based Edge Weights
Inspired by the deep implicit coordination graph (DICG) architecture proposed by (Li
et al., 2020), we use an attention mechanism to define the edge weights of the graph G, i.e.,
{wuv }(u,v)‚ààE . To capture the entire observation-action history of the agents, we leverage
the hidden states of the agents‚Äô DRQNs for deriving the attention weights. In particular,
we assume that for each agent m ‚àà {1, . . . , M }, its policy œÄm internally uses a function
œâm : (Z √ó A)‚àó ‚Üí RD to map the agent‚Äôs observation-action history œÑm to a hidden state
cm = œâm (œÑm ), where we have dropped the time dependence for brevity. We first use a shared
0
encoder mechanism œÜ : RD ‚Üí RD to encode each hidden state cm to a D0 -dimensional
embedding œÜ(cm ). Then, for each pair of agents m0 , m ‚àà {1, . . . , M }, the weight of the edge
from agent m to agent m0 is defined as
T
!
WQ œÜ(cm0 )
WK œÜ(cm )
(10)
wm0 m = Softmaxm0
‚àö
0
D

0

0

where WQ , WK ‚àà RD √óD denote the query and key matrices, respectively, whose parameters
are trained in an end-to-end fashion alongside those of the agents, the shared encoder, and
the mixing GNN. The softmax operator in (10) ensures that the weights of the outgoing
edges from each node to the other nodes sum up to unity.
7

Naderializadeh, Hung, Soleyman, and Khosla

5. Experimental Evaluation
5.1 Setup
We evaluate GraphMIX on the StarCraft II multi-agent challenge (SMAC) environment
(Samvelyan et al., 2019), which provides a set of different micromanagement challenges for
benchmarking distributed multi-agent reinforcement learning methods1 . We specifically
consider the set of nine maps, which have been classified as either hard or super-hard
by (Samvelyan et al., 2019). In each map, the allied team of agents are controlled by the
MARL policy, while the enemy units are controlled by the game‚Äôs built-in AI.
We train the agents for a total of 2 √ó 106 time steps, except for four super hard maps
(6h_vs_8z, corridor, 3s5z_vs_3s6z, and 27m_vs_30m), where the training time is extended
to 5 √ó 106 time steps (Wang et al., 2021b). Moreover, every 2 √ó 104 time steps, training is
paused and the agents are evaluated on a set of 32 test episodes. On four maps (corridor,
2c_vs_64zg, bane_vs_bane, and 3s_vs_5z), we set the local loss coefficient in (7) to Œªlocal =
1, while for the rest of the maps, it is set to zero.
For the mixing GNN, we use the graph isomorphism network (GIN) architecture (Xu
et al., 2019), where the combining operation in (4) is given by
!
X
l
l
l‚àí1
hv = MLP+
wuv hu
, ‚àÄv ‚àà V, ‚àÄl ‚àà {1, . . . , L},
(11)
u‚ààV

where MLPl+ : Fl‚àí1 ‚Üí Fl denotes a multi-layer perceptron (MLP) with non-negative weights.
Further details on the experiments can be found in Appendix A.
5.2 Baseline Methods
We compare the performance of GraphMIX against three state-of-the baseline methods
of QMIX (Rashid et al., 2018), (Optimistic-)Weighted QMIX (Rashid et al., 2020) and
RODE (Wang et al., 2021b). As opposed to QMIX and Weighted QMIX, which are specifically
focused on value factorization, RODE presents a hierarchical framework for assigning roles
to agents over the course of an episode, where each role is defined as a subset of actions
that have a similar impact on the environment. In its architecture, RODE uses two mixing
networks for roles and actions, both of which are trained using QMIX. As an extension of
GraphMIX and RODE, we also present the results for a combined RODE + GraphMIX
approach, where GraphMIX is used for credit assignment for both agents‚Äô roles and actions.
The existence of two mixing GNNs implies that the loss function in (7) will have additional
terms for the local and global role mixing GNN, as well as an additional role local loss
hyperparameter Œªrole,local . The implementation details of the combined RODE + GraphMIX
method, as well as the three baselines, can be found in Appendix A.
5.3 Results
Figure 2 shows the test win rates achieved by GraphMIX, combined RODE + GraphMIX, and
the three baseline approaches on all the five super hard and four hard SMAC scenarios. The
1. We run all our experiments on version SC2.4.10 of StarCraft II, which might explain the discrepancy
between our results and the results reported in the literature. The implementation code for GraphMIX is
available at https://github.com/navid-naderi/GraphMIX.

8

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

QMIX
6h_vs_8z (Super hard)

60
40
20
0
2

3

Training steps (M)

4

80
60
40
20

5

Test win rate (%)

60
40
20
0

1

2

3

Training steps (M)

4

2

3

Training steps (M)

4

27m_vs_30m (Super hard)

60
40
20

Test win rate (%)

60
40
20
0

1

2

3

Training steps (M)

4

0.5

1.0

1.5

Training steps (M)

2.0

1.5

2.0

2c_vs_64zg (Hard)

60
40
20
0.0

0.5

80
60
40
20

1.0

1.5

2.0

1.5

2.0

Training steps (M)
3s_vs_5z (Hard)

100

0
0.0

1.0

Training steps (M)

80

5

bane_vs_bane (Hard)

100

80

0.5

0
0

5m_vs_6m (Hard)

100

20

100

80

5

40

0.0

Test win rate (%)

1

60

5

0
0

80

0
0

100

80

MMM2 (Super hard)

100

Test win rate (%)

1

3s5z_vs_3s6z (Super hard)

100

Test win rate (%)

RODE + GraphMIX

0
0

Test win rate (%)

RODE

Test win rate (%)

80

GraphMIX

corridor (Super hard)

100

Test win rate (%)

100

Test win rate (%)

Weighted QMIX (OW-QMIX)

80
60
40
20
0

0.0

0.5

1.0

1.5

Training steps (M)

2.0

0.0

0.5

1.0

Training steps (M)

Figure 2: Comparison of the test win rates achieved by GraphMIX, combined RODE +
GraphMIX, and the three baseline methods on all super hard and hard SMAC
maps.

solid curves show the median across five training runs with different random seeds, and the
shaded areas represent the 25-75 percentiles. As the figure demonstrates, GraphMIX is able
to considerably outperform QMIX and Weighted QMIX on four maps (6h_vs_8z, corridor,
27m_vs_30m, and bane_vs_bane) and five maps (6h_vs_8z, corridor, MMM2, 5m_vs_6m, and
3s_vs_5z), respectively, and it also outperforms RODE on the super hard 6h_vs_8z map.
The combined RODE + GraphMIX approach further boosts the performance, achieving
state-of-the-art results on three maps (6h_vs_8z, MMM2, and 3s5z_vs_3s6z).
As we mentioned in Section 5.1, the local loss coefficient Œªlocal in (7) was tuned to either
zero or one for each of the nine maps. Figure 3 shows the impact of including (i.e., Œªlocal = 1)
or excluding (i.e., Œªlocal = 0) the local loss term in (7) on the performance of GraphMIX in
three super hard maps, namely 6h_vs_8z, corridor, and MMM2. As the figure demonstrates,
the value of Œªlocal has a significant impact on the performance of the proposed method, and
therefore, it should be tuned based on the specific dynamics of any given environment to
optimize the performance of GraphMIX. The complete performance comparison of GraphMIX
with and without the local loss terms on all the nine maps can be found in Appendix B.
Figure 4 shows the impact of the attention mechanism for determining the edge weights
as in (10) on the performance of GraphMIX. In particular, we show the performance of a
9

Naderializadeh, Hung, Soleyman, and Khosla

GraphMIX ( local = 0)
6h_vs_8z (Super hard)

corridor (Super hard)

80
60
40
20
0

80
60
40
20
0

0

1

2

3

Training steps (M)

4

5

MMM2 (Super hard)

100

Test win rate (%)

100

Test win rate (%)

Test win rate (%)

100

GraphMIX ( local = 1)

80
60
40
20
0

0

1

2

3

Training steps (M)

4

5

0.0

0.5

1.0

1.5

Training steps (M)

2.0

Figure 3: Comparison of the performance of GraphMIX on three super hard maps with and
without the local loss terms in (7).
GraphMIX (Attention-based edge weights)
6h_vs_8z (Super hard)

corridor (Super hard)

80
60
40
20
0

80
60
40
20
0

0

1

2

3

Training steps (M)

4

5

MMM2 (Super hard)

100

Test win rate (%)

100

Test win rate (%)

Test win rate (%)

100

GraphMIX (Equal edge weights)

80
60
40
20
0

0

1

2

3

Training steps (M)

4

5

0.0

0.5

1.0

1.5

Training steps (M)

2.0

Figure 4: Ablation results on the impact of the attention mechanism on the performance of
GraphMIX across three super hard maps.

1
special case, where all the mixing graph edges have equal weights of M
(or more precisely,
1
# alive agents ). As the figure demonstrates, even though the attention mechanism does help
GraphMIX attain a better performance or faster convergence, its contribution is not as
significant as the contribution of the local loss term, and it even slightly hurts the performance
on MMM2. We hypothesize that this is due to the heterogeneity of the MMM2 map, as opposed to
the other two maps, which are homogeneous, and more sophisticated attention architectures
(in which the attention parameters are shared among agents of similar type, but are different
between agents of dissimilar types) can potentially further boost the performance. Note that
for the case of equal edge weights, the GIN update in (11) is simplified to

!
hlv = MLPl+

X

hl‚àí1
u

, ‚àÄv ‚àà V, ‚àÄl ‚àà {1, . . . , L},

(12)

u‚ààV

where the constant edge weights are absorbed into the MLP. Quite interestingly, this can be
viewed as an extension of the value decomposition network (VDN) proposed by (Sunehag et al.,
2018), where the only difference is the existence of the non-linear MLP in (12). Therefore,
GraphMIX can also be viewed as a generalization of the size-invariant and permutationinvariant VDN value factorization approach, which can adapt to different environment
dynamics thanks to the underlying attention mechanism.
10

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

GraphMIX (GIN)
6h_vs_8z (Super hard)

60
40
20

80
60
40
20
0

1

2

3

Training steps (M)

4

5

3s5z_vs_3s6z (Super hard)

80
60
40
20
0

1

2

3

Training steps (M)

4

1

2

3

Training steps (M)

4

5

40
20
0.0

0.5

80
60
40
20

1.0

1.5

2.0

1.5

2.0

Training steps (M)
3s_vs_5z (Hard)

100

0
0

60

5

5m_vs_6m (Hard)

100

Test win rate (%)

100

80

0
0

Test win rate (%)

0

MMM2 (Super hard)

100

Test win rate (%)

80

0

Test win rate (%)

corridor (Super hard)

100

Test win rate (%)

Test win rate (%)

100

GraphMIX (GCN)

80
60
40
20
0

0.0

0.5

1.0

1.5

Training steps (M)

2.0

0.0

0.5

1.0

Training steps (M)

Figure 5: Comparison of the performance of GraphMIX under GIN (Xu et al., 2019) and
GCN (Kipf and Welling, 2017) architectures for the mixing GNN on six hard and
super hard maps.

Finally, Figure 5 shows the impact of the mixing GNN architecture on the performance
of GraphMIX. In particular, we compare the performance of the GIN architecture with
that of the graph convolutional network (GCN) architecture (Kipf and Welling, 2017). As
the figure shows, the GIN-based GraphMIX approach is on par with or better than the
GCN-based GraphMIX approach on most of the maps, except for 3s5z_vs_3s6z. This shows
that while GIN demonstrates promising performance across all the different maps, the GNN
architecture provides another degree of freedom that can be tuned using different GNN
combining methods to further boost the performance of GraphMIX.
5.4 Fine-Tuning on Mismatched Scenarios
In (Wang et al., 2021b), it was shown that agents trained using the RODE framework on
the original 6v24 corridor map can be used in extended corridor scenarios with more
allied/enemy units and achieve an acceptable win rate. Such a ‚Äúrapid transfer‚Äù was possible
because RODE is insensitive to the dimensionality of the action space due to the specific
structure of the role and action agents. Nevertheless, there still existed many scenarios
(especially ones in which the enemy units far outnumbered the allied agents), where the
transferred policies did not work well. As RODE uses QMIX for role and action value
function factorization, fine-tuning the agents is not possible since the input size of the mixing
networks grows with the number of agents.
In contrast, as the mixing GNN in GraphMIX is size-invariant, the combined RODE
+ GraphMIX approach enjoys insensitivity to both the number of agents and the number
of actions. This enables us to fine-tune the agents through the mixing GNN on scenarios
in which the number of agents is different than that of the training scenario. Figure 6
shows the mean win rates achieved by combined RODE + GraphMIX, originally trained on
11

Naderializadeh, Hung, Soleyman, and Khosla

Corridor 6v24 Best
Corridor 8v30
Corridor 10v30
Corridor 12v30

80
70

Test win rate (%)

60
50
40
30
20
10
0
0.0

0.2

0.4

0.6

Fine-tuning steps (M)

0.8

1.0

Figure 6: Win rates achieved by combined RODE + GraphMIX agents, trained originally
on the 6v24 corridor map, and fine-tuned on larger corridor maps with more
agents and enemies.

the 6v24 corridor map, upon being fine-tuned on larger corridor maps with 8-12 agents
and 30 enemies, where the shaded areas represent the standard deviation. As the figure
demonstrates, the fine-tuned agents can improve their win rates when fine-tuned for as few as
106 steps, indicating the scalability of the proposed graph-based value decomposition method
due to its size invariance. More details on this experiment can be found in Appendix A..

6. Conclusion
We introduced GraphMIX, a novel approach to decompose joint state-action value functions
in multi-agent deep reinforcement learning (MARL) using a graph neural network formulation
under the centralized training and decentralized execution paradigm. Our proposed method
allows for a more explicit representation of agent-to-agent relationships by leveraging an
attention-based graph topology that models the dynamics between the agents as the episodes
progress. To build upon the factorized state-action value function‚Äôs implicit assignment of
global reward, we defined additional per-agent loss terms derived from the output node
embeddings of the graph neural network, which explicitly divide the global reward to
individual agents. Experiments in the StarCraft Multi-Agent Challenge (SMAC) benchmark
demonstrated improved performance over the state-of-the-art multi-agent deep reinforcement
learning algorithms across multiple hard and super hard scenarios. We further showed the
scalability of GraphMIX, when combined with a hierarchical role-based MARL architecture,
by enabling the agents trained on a specific scenario to be fine-tuned on test scenarios with
higher numbers of agents.
12

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

References
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew,
and Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=SkxpxJBKwS.
Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for
networked system control. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=Syx7A3NFvH.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training
of deep bidirectional transformers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
4171‚Äì4186, 2019.
Mark Eisen and Alejandro Ribeiro. Optimal wireless resource allocation with random edge
graph neural networks. IEEE Transactions on Signal Processing, 68:2977‚Äì2991, 2020.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph
neural networks for social recommendation. In The World Wide Web Conference, pages
417‚Äì426, 2019.
Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning
to communicate with deep multi-agent reinforcement learning. In Advances in neural
information processing systems, pages 2137‚Äì2145, 2016.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon
Whiteson. Counterfactual multi-agent policy gradients, 2018. URL https://aaai.org/
ocs/index.php/AAAI/AAAI18/paper/view/17193.
Hado V. Hasselt. Double Q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems
23, pages 2613‚Äì2621. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/
3964-double-q-learning.pdf.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
Q-learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
AAAI‚Äô16, page 2094‚Äì2100. AAAI Press, 2016.
Weihua Hu*, Bowen Liu*, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=HJlWWJSFDH.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In
International Conference on Machine Learning, pages 2961‚Äì2970. PMLR, 2019.
13

Naderializadeh, Hung, Soleyman, and Khosla

Tao Ji, Yuanbin Wu, and Man Lan. Graph-based dependency parsing with graph neural
networks. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pages 2475‚Äì2485, 2019.
Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HkxdQkSYDB.
Xiangyang Ju, Steven Farrell, Paolo Calafiura, Daniel Murnane, Lindsey Gray, Thomas
Klijnsma, Kevin Pedro, Giuseppe Cerati, Jim Kowalkowski, Gabriel Perdue, et al. Graph
neural networks for particle reconstruction in high energy physics detectors. arXiv preprint
arXiv:2003.11603, 2020.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representations (ICLR), 2017.
Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep
implicit coordination graphs for multi-agent reinforcement learning. arXiv preprint
arXiv:2006.11438, 2020.
Iou-Jen Liu, Raymond A Yeh, and Alexander G Schwing. PIC: permutation invariant critic
for multi-agent deep reinforcement learning. In Conference on Robot Learning, pages
590‚Äì602, 2020.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch.
Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in
neural information processing systems, pages 6379‚Äì6390, 2017.
O Malaschuk and A Dyumin. Intelligent multi-agent system for rescue missions. In Advanced
Technologies in Robotics and Intelligent Systems, pages 89‚Äì97. Springer, 2020.
Navid Naderializadeh, Jaroslaw Sydir, Meryem Simsek, and Hosein Nikopour. Resource
management in wireless networks via multi-agent deep reinforcement learning. In 2020 IEEE
21st International Workshop on Signal Processing Advances in Wireless Communications
(SPAWC), pages 1‚Äì5, 2020.
Georgios Papoudakis, Filippos Christianos, Lukas Sch√§fer, and Stefano V Albrecht. Comparative evaluation of multi-agent deep reinforcement learning algorithms. arXiv preprint
arXiv:2006.07869, 2020.
Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos. Estimating
node importance in knowledge graphs using graph neural networks. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
pages 596‚Äì606, 2019.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning, pages 4295‚Äì4304,
2018.
14

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning.
Advances in Neural Information Processing Systems, 33, 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft multi-agent challenge. In Proceedings of the 18th International
Conference on Autonomous Agents and MultiAgent Systems, pages 2186‚Äì2188, 2019.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61‚Äì80,
2008.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran:
Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning, pages 5887‚Äì5896. PMLR, 2019.
Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in neural information processing systems, pages 2244‚Äì2252,
2016.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vin√≠cius Flores
Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
In AAMAS, pages 2085‚Äì2087, 2018.
Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph
neural network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998‚Äì6008, 2017.
Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and
Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex
dueling multi-agent q-learning. In International Conference on Learning Representations,
2021a. URL https://openreview.net/forum?id=Rcmk0xxIQV.
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie
Zhang. RODE: Learning roles to decompose multi-agent tasks. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id=
TTUVg6vkNjK.
Shangyu Xie, Yuan Hong, and Peng-Jun Wan. A privacy preserving multiagent system for
load balancing in the smart grid. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems, pages 2273‚Äì2275, 2019.
15

Naderializadeh, Hung, Soleyman, and Khosla

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph
neural networks? In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=ryGs6iA5Km.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and
Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement
learning. arXiv preprint arXiv:2002.03939, 2020.
Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin,
Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan,
Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia.
Deep reinforcement learning with relational inductive biases. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=HkxaFoC9KQ.
Riccardo Zanol, Federico Chiariotti, and Andrea Zanella. Drone mapping through multiagent reinforcement learning. In 2019 IEEE Wireless Communications and Networking
Conference (WCNC), pages 1‚Äì7. IEEE, 2019.
Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou
Wang, and Ying Nian Wu. Multi-agent tensor fusion for contextual trajectory prediction.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 12126‚Äì12134, 2019.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng
Li, and Maosong Sun. Graph neural networks: A review of methods and applications.
arXiv preprint arXiv:1812.08434, 2018.

Appendix A. Implementation Details
A.1 Shared Parameters
We use a gated recurrent unit (GRU) for each of the decentralized agents in all algorithms
with D = 64 hidden units. Each agent uses an -greedy policy, where the probability of
random actions decays from 100% to 5% over 5 √ó 104 time steps, except for three super hard
maps (6h_vs_8z, 3s5z_vs_3s6z, and 27m_vs_30m), where the decay happens over 5 √ó 105
time steps to allow further exploration Wang et al. (2021b). The agents are trained over
consecutive episodes, where at the end of each episode, a batch of 32 episodes is randomly
sampled from an experience buffer of size 5000 episodes for a round of training. The learning
rate is fixed at 5 √ó 10‚àí4 . To stabilize training, double Q-learning is used (Hasselt, 2010;
Hasselt et al., 2016), where the target agent Q-network and the target mixing network
parameters are replaced with those of their main counterparts every 200 episodes. Moreover,
every 2 √ó 104 time steps, training is paused and the agents are evaluated on a set of 32 test
episodes.
A.2 GraphMIX
The encoding mechanism œÜ for the attention coefficients in (10) is implemented using a
single-layer mapping followed by a non-linearity; i.e., œÜ(cm ) = œÉ(Bcm ), where œÉ(¬∑) denotes a
16

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

0

non-linearity and B ‚àà RD √ó64 denotes the encoding matrix. We use the exponential linear
unit (ELU) as the non-linearity and set D0 = 16.
We consider a single 32-dimensional hidden layer for the mixing GNN and average
pooling readout at the output, and for the GIN combining operation in (11), we use an
MLP with a single hidden layer of size 16 and rectified linear unit (ReLU) non-linearity.
The hypernetworks are also MLPs, each with a single hidden layer of size 64 and ReLU
non-linearity.
Over the course of an episode, the agents might get killed by the opponent team. In
that case, we isolate the dead agents from the other nodes in graph G, and we remove them
from calculations of the attention weights and the GNN operations entirely. Specifically, the
readout operation in (5), the effective reward fraction calculation in (6), and the softmax
operation for deriving the edge weights in (10) are constrained to the agents that are still
alive in the corresponding time steps.
A.3 QMIX
We use a similar MLP-based architecture for the mixing network in QMIX to its counterpart
in GraphMIX; i.e., an MLP with a single hidden layer, 32 neurons per hidden layer, and
ELU non-linearity.
A.4 Weighted QMIX
We use the Optimistically-Weighted version of the Weighted QMIX algorithm. On most of
the maps, the mixing network Q‚àó in equation (9) of Rashid et al. (2020) is implemented
using a 3-layer MLP, with each hidden layer having 256 neurons and ReLU non-linearity.
For the corridor map, the weight matrix in the first layer of Q‚àó is derived by applying
softmax on the output of a hypernetwork with a single 64-dimensional hidden layer that
takes the environment state as input. Moreover, the parameter Œ± in equation (5) of Rashid
et al. (2020) is set to 0.5 on most maps, except for corridor, where it is set to 0.75.
A.5 RODE
We mostly use similar hyperparameters to those used in Wang et al. (2021b) for the number
of role clusters and role interval. Specifically, on the maps with heterogeneous enemy units,
the number of role clusters is set to k = 5, while on most maps with homogeneous enemy
units, we set the number of role clusters to k = 3. We also set the role interval to c = 5.
In our experiments on the 6h_vs_8z map, we were unable to reproduce similar win rates
for RODE to those reported in Wang et al. (2021b) using k = 3 role clusters and a role interval
of c = 7. We conducted multiple experiments using the following set of hyperparameters on
this map,
(k, c) ‚àà {(3, 3), (3, 5), (3, 6), (3, 7), (3, 8), (3, 10), (5, 3), (5, 5), (5, 7), (7, 7)},
and found that (k, c) = (5, 5) led to the best results, which are still below the win rates
reported in Wang et al. (2021b). We, therefore, set k = c = 5 for our experiments on this
map.
17

Naderializadeh, Hung, Soleyman, and Khosla

A.6 RODE + GraphMIX (Main Experiments)
As mentioned in Section 5.2, the combined RODE + GraphMIX approach introduces a new
role local loss hyperparameter Œªrole,local aside from the original local loss hyperparameter
Œªlocal in (7). Table 1 shows the local loss hyperparameters we use for the combined RODE +
GraphMIX method on the nine maps.
Œªlocal
1
0
0
0
0
0
0
0
1

Map
6h_vs_8z
corridor
MMM2
3s5z_vs_3s6z
27m_vs_30m
2c_vs_64zg
5m_vs_6m
bane_vs_bane
3s_vs_5z

Œªrole,local
1
1
0
1
0
0
0
0
0

Table 1: Local loss coefficients used for the combined RODE + GraphMIX approach.
Due to the different agent structures in RODE, we use the projected hidden states, i.e.,
zœÑi in equations (3) and (5) of Wang et al. (2021b) instead of the hidden states cm to compute
the attention coefficients in (10) for both the role and action mixing GNNs.
A.7 RODE + GraphMIX (Fine-Tuning Experiments on corridor)
We take a similar approach to that of the policy transfer experiments in Wang et al. (2021b),
where on each of the new maps, we first train an action encoder for the new action space
over 5 √ó 104 steps. Then, for each new action that was not present in the original 6v24 map,
we find the 5 nearest neighbors among the old actions in the action embedding space. We
use the average of those neighboring action embeddings as the embedding of the new action,
and add that action to the union of roles (i.e., clusters) to which those neighboring actions
belong. Note that we do not update the role representations and old action representations
based on the new action embeddings.
Moreover, similarly to Wang et al. (2021b), to keep the per-agent observation length
unchanged across the original and extended corridor maps, we only let each agent observe
the closest 5 allied units and 24 enemy units (if they are within its visible range). Moreover, to
keep the state space dimensionality constant across all the maps, we include the information
about the 6 allied units with the smallest mean distances to the enemy units, and 24 enemy
units with the smallest mean distances to the allied units. Keeping the state vector length
fixed is necessary as we still need to use the global environment state to fine-tune the mixing
GNNs on the new maps.
A.8 Hardware
We run most of our experiments on a Linux machine with a 2.40 GHz Intel¬Æ Xeon¬Æ E5-2680
v4 CPU and two 16 GB NVIDIA¬Æ Tesla¬Æ P100 GPUs, while for the Weighted QMIX
18

Graph Convolutional Value Decomposition in Multi-Agent Reinforcement Learning

GraphMIX ( local = 0)
6h_vs_8z (Super hard)

60
40
20

60
40
20

2

3

Training steps (M)

4

5

Test win rate (%)

60
40
20
0

1

2

3

Training steps (M)

4

2

3

Training steps (M)

4

80
60
40
20

5

Test win rate (%)

60
40
20
0

1

2

3

Training steps (M)

4

0.5

1.0

1.5

Training steps (M)

2.0

1.5

2.0

2c_vs_64zg (Super hard)

60
40
20
0.0

80
60
40
20

0.5

1.0

1.5

Training steps (M)

2.0

3s_vs_5z (Super hard)

100

0
0.0

1.0

Training steps (M)

80

5

bane_vs_bane (Super hard)

100

80

0.5

0
0

5m_vs_6m (Super hard)

100

20

100

Test win rate (%)

1

40

0.0

0
0

60

5

27m_vs_30m (Super hard)

100

80

80

0
0

Test win rate (%)

1

3s5z_vs_3s6z (Super hard)

100

Test win rate (%)

80

0
0

MMM2 (Super hard)

100

Test win rate (%)

80

0

Test win rate (%)

corridor (Super hard)

100

Test win rate (%)

Test win rate (%)

100

GraphMIX ( local = 1)

80
60
40
20
0

0.0

0.5

1.0

1.5

Training steps (M)

2.0

0.0

0.5

1.0

1.5

Training steps (M)

2.0

Figure 7: Comparison of the performance of GraphMIX on all the nine hard and super hard
maps with and without the local loss terms in (7).

experiments on two of the maps, namely 27m_vs_30m and bane_vs_bane, due to higher
memory requirements, we use a more powerful Linux machine with a 2.70 GHz Intel¬Æ Xeon¬Æ
Platinum 8280 CPU and four 48 GB NVIDIA¬Æ Quadro RTX 8000 GPUs.

Appendix B. Ablation Results on The Impact of The Local Loss Terms
in The Aggregate Loss
Figure 7 shows the impact of including or excluding the local loss terms in (7) (i.e., Œªlocal ‚àà
{1, 0}) on the performance of GraphMIX on all the nine hard and super hard maps. As the
figure shows, while including the local per-agent losses is essential on some of the maps, e.g.,
corridor, the performance of GraphMIX improves using only the global loss on the rest of
the maps. Therefore, it is important that this hyperparameter be tuned on any environment
of interest so as to maximize the performance gains of GraphMIX.

19

