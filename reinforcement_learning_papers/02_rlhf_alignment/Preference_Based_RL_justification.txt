Paper: Preference_Based_RL.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options  arXiv:2510.18713v2 [cs.LG] 11 Nov 2025  Joongkyu Lee Seoul National University jklee0717@snu.ac.kr  Seouh-won Yi Seoul National University uniqueseouh@snu.ac.kr  Min-hwan Oh Seoul National University minoh@snu.ac.kr  Abstract We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged—motivated by PbRL’s recent empirical success, particularly in aligning large language models (LLMs)—most existing...

Specific Evidence for Classification:
- "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options  arXiv:2510.18713v2 [cs.LG] 11 Nov 2025  Joongkyu Lee Seoul National University jklee0717@snu.ac.kr  Seouh-won Yi Seoul National University uniqueseouh@snu.ac.kr  Min-hwan Oh Seoul National University minoh@snu.ac.kr  Abstract We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency."
- "A few recent works [92, 49, 76] have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve—and can even deteriorate—as the feedback length increases, despite the richer information available."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
