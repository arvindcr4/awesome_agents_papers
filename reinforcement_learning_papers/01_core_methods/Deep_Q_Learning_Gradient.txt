Deep Q-Learning with Gradient Target Tracking

arXiv:2503.16700v3 [cs.LG] 18 Jul 2025

Bumgeun Parkâˆ—
Electrical Engineering, KAIST
j4t123@kaist.ac.kr

Taeho Lee âˆ—
Electrical Engineering, KAIST
eho0228@kaist.ac.kr

Donghwan Lee
Electrical Engineering, KAIST
donghwan@kaist.ac.kr

Abstract
This paper introduces Q-learning with gradient target tracking, a novel reinforcement learning framework that provides a learned continuous target update mechanism as an alternative to the conventional hard update paradigm. In the standard
deep Q-network (DQN), the target network is a copy of the online networkâ€™s
weights, held fixed for a number of iterations before being periodically replaced via
a hard update. While this stabilizes training by providing consistent targets, it introduces a new challenge: the hard update period must be carefully tuned to achieve
optimal performance. To address this issue, we propose two gradient-based target
update methods: DQN with asymmetric gradient target tracking (AGT2-DQN)
and DQN with symmetric gradient target tracking (SGT2-DQN). These methods
replace the conventional hard target updates with continuous and structured updates
using gradient descent, which effectively eliminates the need for manual tuning.
We provide a theoretical analysis proving the convergence of these methods in
tabular settings. Additionally, empirical evaluations demonstrate their advantages
over standard DQN baselines, which suggest that gradient-based target updates
can serve as an effective alternative to conventional target update mechanisms in
Q-learning.

1

Introduction

Recently, reinforcement learning (RL) [27] has seen remarkable success in solving sequential decisionmaking problems [21, 18, 25, 24, 7, 8, 28, 30, 1]. In particular, deep Q-network (DQN) algorithm [21]
is a widely used approach for value-based RL, and has demonstrated human-level performance in
various complex tasks, such as playing Atari games. However, standard DQN suffers from tuning
challenges due to the periodic hard update of the target network. Specifically, the target Q-network is
updated only periodically via a hard update, where every few thousand iterations, the target networkâ€™s
weights are replaced by those of the online network. This strategy stabilizes learning but introduces a
new challenge: the hard update period must be carefully tuned to achieve optimal performance. An
alternative to hard updates is incremental updates, where the target network is updated gradually at
each step. Lillicrap et al. [18] popularized this approach in deep RL, using a soft update (or Polyak
averaging), where the target parameters move toward the online parameters with a small learning rate.
While this method mitigates instability, it still requires careful tuning of the averaging weight.
To address these challenges, we introduce a novel gradient-based target update framework for Qlearning. Our approach replaces the standard hard target update with gradient-based target learning
approach, which continuously updates the target Q-function using gradient descent. In particular,
âˆ—

Equal contribution.

39th Conference on Neural Information Processing Systems (NeurIPS 2025).

we develop DQN with asymmetric gradient target tracking (AGT2-DQN) and DQN with symmetric
gradient target tracking (SGT2-DQN). AGT2-DQN replaces the hard update with a continuous,
gradient-based update for the target network. Instead of copying weights infrequently, the target
networkâ€™s parameters are treated as learnable, and they are adjusted at every step using gradient
descent on a mean squared loss between the target Q-network and online Q-network. SGT2-DQN
also uses continuous gradient-based target updates, but with a symmetric coupling between the online
and target networks. Both networks are updated with loss terms that drive them toward each otherâ€™s
predictions. Morever, SGT2-DQN adds an extra regularization term to the loss functions of both
networks that penalizes the difference between the online Q-value and target Q-value for the same
state-action. This means the online and target networks influence each otherâ€™s updates, which enforces
a more balanced interaction (each network â€œknowsâ€ about the otherâ€™s objective).
Unlike standard DQN, where the target network is updated only periodically, our methods ensure
continuous adaptation of the target function through gradient-based learning. The advantages of our
approach are supported by experiments on benchmark reinforcement learning tasks, which show that
AGT2-DQN and SGT2-DQN achieve learning performance comparable to standard DQN without
significant tuning steps.
Finally, we provide theoretical analysis, which shows that both the asymmetric and symmetric
algorithms converge to the optimal Q-values in the tabular setting. In fact, for the tabular versions
of AGT2-DQN/SGT2-DQN, called Q-learning with asymmetric gradient tracking (AGT2-QL) and
Q-learning with symmetric gradient tracking (SGT2-QL), we prove that under standard assumptions,
the online and target Q estimates will converge to Qâˆ— (the true optimal Q-function) with probability
one.

2

Related works

In the original DQN [21], the target network is a copy of the online networkâ€™s weights that is held
fixed for a number of iterations and then replaced (â€œhard updatedâ€) periodically. Researchers have
looked into several alternatives to this hard update mechanism: soft (Polyak) updates have been
introduced in [18]. Instead of copying the network every few thousand steps, an incremental update
can be done every step that updates the target parameters with a small learning rate towards the online
parameters. In [14, 15], the effect of the soft updates has been theoretically investigated for tabular
TD-learning and tabular Q-learning respectively.
DeepMellow [12] removes the target network entirely by modifying the Bellman update and achieves
faster learning without the target network. They replace the conventional max operator with a
Mellowmax operator, a smooth approximation of max that prevents over-optimistic value estimates.
Similarly, in [22], the authors propose adding a functional regularization (FR) therm to the DQN loss
and removing the target network, which is calle FR-DQN. In their FR-DQN algorithm, the Q-network
is trained with an extra loss term that measures the difference between the current Q-values and
those of a prior network (which is a periodically updated snapshot of the online network). This acts
similarly to a target network, but instead of using the prior networkâ€™s values as fixed targets, it uses
them as a reference to restrain the current networkâ€™s updates.
In summary, several strategies have been explored as alternatives to hard target updates. All these
methods share the goal of providing stability to Q-learning without the downsides of an abruptly
updated target. The new gradient target tracking approaches proposed in this paper contribute to this
landscape by offering a learned, continuous target update mechanism as yet another alternative to the
hard update paradigm.

3

Preliminaries

3.1

Markov decision problem

We consider the infinite-horizon discounted Markov decision problem and Markov decision process,
where the agent sequentially takes actions to maximize cumulative discounted rewards. In a Markov
decision process with the state-space S := {1, 2, . . . , |S|} and action-space A := {1, 2, . . . , |A|},
the decision maker selects an action a âˆˆ A at the current state s âˆˆ S, then the state transits to the
next state sâ€² âˆˆ S with probability P (sâ€² |s, a), and the transition incurs a reward r(s, a, sâ€² ) âˆˆ R, where
2

P (sâ€² |s, a) is the state transition probability from the current state s âˆˆ S to the next state sâ€² âˆˆ S under
action a âˆˆ A, and r(s, a, sâ€² ) is the reward function. Moreover, |S| and |A| denotes cardinalities of S
and A, respectively. For convenience, we consider a deterministic reward function and simply write
r(sk , ak , sk+1 ) =: rk+1 , k âˆˆ {0, 1, . . .}. A policy Ï€ maps a state s âˆˆ S to an action distribution
Ï€ : S â†’ âˆ†A , where âˆ†A denotes the set of all probability distributions over the action space A. The
objective of the Markov decision problem is to find an optimal policy, Ï€ âˆ— , such that the cumulative
discounted rewards over infinite time horizons is maximized, i.e.,
" âˆž
#
X
âˆ—
k
Ï€ := arg max E
Î³ rk+1 Ï€ ,
Ï€âˆˆÎ˜

k=0

where Î³ âˆˆ [0, 1) is the discount factor, Î˜ is the set of all policies, (s0 , a0 , s1 , a1 , . . .) is a state-action
trajectory generated by the Markov chain under policy Ï€, and E[Â·|Ï€] is an expectation conditioned on
the policy Ï€. Moreover, Q-function under policy Ï€ is defined as
" âˆž
#
X
Ï€
k
Q (s, a) = E
Î³ rk+1 s0 = s, a0 = a, Ï€ , (s, a) âˆˆ S Ã— A,
k=0
âˆ—

and the optimal Q-function is defined as Qâˆ— (s, a) = QÏ€ (s, a) for all (s, a) âˆˆ S Ã— A. Once Qâˆ— is
known, then an optimal policy can be retrieved by the greedy policy Ï€ âˆ— (s) = arg maxaâˆˆA Qâˆ— (s, a).
Throughout, we assume that the Markov decision process is ergodic so that the stationary state
distribution exists.

4

Deep Q-learning with gradient target tracking

Before introducing the main results, we first briefly review the standard DQN in [21]. DQN considers
the two optimal Q-function estimates, QÎ¸ called the online Q-network, and QÎ¸â€² called the target
Q-network. Here, Î¸ and Î¸â€² are called the online and target parameters, respectively. To update these
parameters, the following loss function is considered:
X
1 1
2
L(Î¸; B) :=
(y âˆ’ QÎ¸ (s, a))
2 |B|
â€²
(s,a,r,s )âˆˆB

where B is the mini-batch uniformly sampled from the replay buffer D, |B| denotes the size of the
mini-batch, and y, fixed as a constant, is called the target, which is defined as
y = r + 1(sâ€² )Î³maxQÎ¸â€² (sâ€² , a)
aâˆˆA

Here, 1(sâ€² ) is an indicator function defined as

0 if s = terminal state
1(s) :=
1
else
The online Q-network QÎ¸ is then updated through the gradient descent step
Î¸ â† Î¸ âˆ’ Î±âˆ‡L(Î¸; B)
â€²
where the target parameter Î¸ is fixed as a constant. Then, the target Q-network QÎ¸â€² is updated
periodically by the online parameter with period C > 0
Î¸â€² â† Î¸
Figure 1 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Cartpole
environment (Open AI gym). The results show that C = 10 gives the best learning performance
among the other choices of C. As the results indicate, achieving better learning performance requires
careful selection of C, which demands extensive trials and time.
In this paper, we propose two gradient-based target learning methods for DQN: the first algorithm
is called DQN with asymmetric gradient target tracking (AGT2-DQN), and the second is DQN
with symmetric gradient target tracking (SGT2-DQN). In AGT2-DQN, we employ two distinct Qnetworks, QÎ¸1 and QÎ¸2 , referred to as the online and target Q-networks, respectively. The parameters
Î¸1 and Î¸2 are called the online and target parameters, respectively. The corresponding loss functions
are defined as
X
1 1
2
L1 (Î¸1 ; B) :=
(y1 âˆ’ QÎ¸1 (s, a))
2 |B|
â€²
(s,a,r,s )âˆˆB

3

Figure 1: Comparison of reward curves of DQN with different C in Cartpole environment.

and
L2 (Î¸2 ; B) :=

Î² 1
2 |B|

X

2

(y2 âˆ’ QÎ¸2 (s, a))

(s,a,r,sâ€² )âˆˆB

where Î² > 0 is a constant weight, y1 and y2 are targets defined as
y1 = r + 1(sâ€² )Î³maxQÎ¸2 (sâ€² , a), y2 = QÎ¸1 (s, a)
aâˆˆA

Both Î¸1 and Î¸2 are updated through the gradient descent steps
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1 (Î¸1 ; B), Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2 (Î¸2 ; B),
where Î± > 0 is the step-size. The overall algorithm is summarized in Appendix A. Note that the
online parameter update is identical to that in the standard DQN. The primary distinction arises from
the target parameter update, which employs the following gradient descent step for the loss function
L2 :
X
1
Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2 (Î¸2 ; B) = Î¸2 + Î±
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))âˆ‡Î¸2 QÎ¸2 (s, a)
|B|
â€²
(s,a,r,s )âˆˆB

The above update treats the target network in DQN as a learnable parameter updated via gradient
descent, rather than updating it only periodically [21] or by polyak averaging [18]. This yields a
form of continuous update where the target slowly learns to predict the online Q-values (or some
combination thereof), instead of being replaced outright.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 2: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î². Cartpole
environment in OpenAI Gym is used here.

4

Figure 3: Comparison of reward curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50. Cartpole environment in OpenAI Gym is used here.

As noted in the previous section, the updates for the online and target parameters are asymmetric.
This asymmetry might lead to inefficiencies in finding the optimal solution since each update plays a
distinct role and does not consider the influence of the other.
The algorithm can be conceptualized as a two-player game, where the first player (online parameter
update) strives to solve the Bellman equation, while the second player (target parameter update)
aims to track the first playerâ€™s actions. In this setup, the utility functions of the two players are
asymmetric and do not consider the objectives of each other. This mismatch can lead to instability
and inefficiencies during the learning process. A potential alternative would be to design a game
where each playerâ€™s utility function explicitly incorporates the other playerâ€™s goal, leading to a more
balanced interaction. In such a framework, the utility functions would be symmetric, which could
potentially enhance the stability and efficiency of the algorithm.
Motivated by the previous discussions, we propose the second algorithm, referred to as SGT2DQN. Similar to AGT2-DQN, this algorithm also employs two Q-networks QÎ¸1 and QÎ¸2 , which are
called the online and target Q-networks, respectively. However, SGT2-DQN introduces a symmetric
approach to updating these networks, that aims to harmonize the interaction between them. In
particular, SGT2-DQN employs the following loss functions:
X
1 1
2
2
L1 (Î¸1 ; B) :=
[(y1 âˆ’ QÎ¸1 (s, a)) + Î²(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) ]
2 |B|
â€²
(s,a,r,s )âˆˆB

and
L2 (Î¸2 ; B) :=

1 1
2 |B|

X

2

2

[(y2 âˆ’ QÎ¸2 (s, a)) + Î²(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a)) ].

(s,a,r,sâ€² )âˆˆB

where Î² > 0 is a constant weight representing the regularization weight, and the targets y1 and y2
are defined as
y1 = r + 1(sâ€² )Î³ max QÎ¸2 (sâ€² , a), y2 = r + 1(sâ€² )Î³maxQÎ¸1 (sâ€² , a).
aâˆˆA

aâˆˆA

Next, both Î¸1 and Î¸2 are updated through the gradient descent steps
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1 (Î¸1 ; B), Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2 (Î¸2 ; B),
where Î± > 0 is the step-size. The overall algorithm is summarized in Appendix B. In SGT2-DQN,
both the online and target networks interact and adjust towards each other using gradient steps.
Moreover, this algorithm echoes the idea of double Q-learning in a gradient setting. Double Qlearning maintains two estimates that learn from each otherâ€™s predictions [9]. SGT2-DQN similarly
lets two function approximators co-evolve and correct one another.
Figure 2 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100} in Cartpole environment. The results demonstrate that their learning
efficiency is comparable to DQN. However, one can observe that they are less sensitive to the
hyperparameter Î².
5

Figure 3 illustrates the learning curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50, where the hyperparameter for each method has been selected to achieve approximately the
best performance among the tested grid points.
The results show that while DQN exhibits slightly better learning efficiency than the proposed methods
in this environment. However, as mentioned before, the latter require less efforts for hyperparameter
tuning in this case.
We conducted similar experiments across multiple environments, and the results varied depending on
the environment. For instance, comparisons of learning curves are presented in Figure 4, where the
hyperparameters were roughly tuned to achieve optimal learning performance for each method.
As shown in Figure 4(a), DQN with C = 50, AGT2-DQN with Î² = 0.1, and SGT2-DQN with Î² = 1
demonstrate comparable performance in the Acrobot environment. In contrast, Figure 4(b) illustrates
that AGT2-DQN with Î² = 0.1 and SGT2-DQN with Î² = 0.1 exhibit better learning efficiency than
DQN with C = 500. A full comparison of results, including those with different hyperparameter
settings, is provided in the Appendix J. Moreover, Appendix J also includes comparisons in other
environments, which exhibit similar trends.
Overall, DQN, AGT2-DQN, and SGT2-DQN exhibited comparable learning performance on average.
In some environments, DQN outperformed the other two methods, while in others, AGT2-DQN and
SGT2-DQN achieved better learning performance than DQN. However, we observed that AGT2-DQN
and SGT2-DQN were slightly less sensitive to hyperparameters than DQN on average. Therefore, it
is difficult to conclude that the proposed AGT2-DQN and SGT2-DQN outperform DQN. However,
we argue that they offer interesting alternatives to the hard update paradigm in DQN.

(a) Acrobot: DQN with C = 50, AGT2-DQN (b) Pendulum: DQN with C = 500, AGT2with Î² = 0.1, and SGT2-DQN with Î² = 1.
DQN with Î² = 0.1, and SGT2-DQN with Î² =
0.1.

Figure 4: Comparison of simulated reward curves of AGT2-DQN,SGT2-DQN, DQN
Beyond the tuning issue, our experience suggests that AGT2-DQN and SGT2-DQN achieve faster
learning speeds than DQN during the early learning phase. However, they frequently encounter
performance collapse after this phase. Similar to policy gradient methods, this collapse appears to
result from overshooting in the gradient target update, a phenomenon that may not occur in standard
DQN. We believe this issue can be mitigated by incorporating stabilization techniques, such as
clipping the loss function, similar to the approach used in proximal policy optimization (PPO) in [24].

5

Analysis of optimality

The proposed AGT2-DQN and SGT2-DQN aim to minimize their respective loss functions through
gradient descent steps. However, it is not currently clear how minimizing these errors in the loss
functions ensures that the computed Q-estimates are closer to the optimal Q-function. Therefore,
further investigation is required to establish a more definitive link between the reduction of loss
function errors and the accuracy of the Q-estimates in approximating the optimal Q-function. This
6

issue is the main goal of this section and will be critical for validating the effectiveness of the proposed
methods.
To simplify the analysis, let us assume that by minimizing the loss functions of AGT2-DQN, we can
approximately minimize the following expected loss functions:
h
i
2
L1 (Î¸1 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a))


Î²
2
L2 (Î¸2 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a)
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))
2
where U (S Ã— A) means the uniform distribution over the set S Ã— A. Note that we employ the uniform
distribution over S Ã— A to simplify the analysis. In practice, this would correspond to the average
distribution of S Ã— A in the replay buffer.
Next, suppose that the loss functions are minimized with
L1 (Î¸1 ) â‰¤ Îµ, L2 (Î¸2 ) â‰¤ Îµ,
where Îµ > 0 is a small number representing the maximum error of the two loss functions. Then, we
can derive the following error bounds on the corresponding solution:
Theorem 1. Suppose that the loss functions are minimized with
L1 (Î¸1 ) â‰¤ Îµ, L2 (Î¸2 ) â‰¤ Îµ.
For the expected loss functions of AGT2-DQN, we have
s
p
Îµ|S||A|
Î³
2Îµ|S||A|
âˆ—
+
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž â‰¤
1âˆ’Î³
1âˆ’Î³
Î²
s
p
2 Îµ|S||A|
Î³
2Îµ|S||A|
âˆ¥QÎ¸2 âˆ’ Qâˆ— âˆ¥âˆž â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
Its proof can be found in Appendix H. We can observe that the error converges to 0 as Îµ â†’ 0.
Moreover, the errors also depend on Î² > 0 and as Î² â†’ âˆž the error tends to be reduced.
Similarly, let us assume that by minimizing the loss functions of SGT2-DQN, we can approximately
minimize the following expected loss function:
"
2
â€²
â€²
â€²
L1 (Î¸1 ) =E(s,a)âˆ¼U (SÃ—A),s âˆ¼P (Â·|s,a)
r(s, a, s ) + Î³maxQÎ¸2 (s , a) âˆ’ QÎ¸1 (s, a)
aâˆˆA



Î²
2
+ (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) ,
2
"
2
â€²
â€²
L2 (Î¸2 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a)
r(s, a, s ) + Î³ max QÎ¸1 (s , a) âˆ’ QÎ¸2 (s, a)
aâˆˆA



Î²
2
+ (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))
2
where U (S Ã— A) means the uniform distribution over the set S Ã— A. Then, we can establish the
following error bounds on the corresponding solution:
Theorem 2. Suppose that the loss functions are minimized with
L1 (Î¸1 ) â‰¤ Îµ, L2 (Î¸2 ) â‰¤ Îµ
For the expected loss functions of SGT2-DQN, we have
s
p
Îµ|S||A|
Î³
2Îµ|S||A|
âˆ—
âˆ—
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž , âˆ¥QÎ¸2 âˆ’ Q âˆ¥âˆž â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
Its proof can be found in Appendix I. As in the AGT2-DQN case, the error vanishes as Îµ â†’ 0.
7

6

Convergence analysis for tabular case

In this section, we provide theoretical convergence analysis of AGT2-DQN and SGT2-DQN in tabular
cases for conceptual investigations, where the tabular versions of AGT2-DQN and SGT2-DQN are
called Q-learning with asymmetric gradient tracking (AGT2-QL) and Q-learning with symmetric
gradient tracking (SGT2-QL), respectively. Let us first consider the following update of AGT2-QL:


A
â€²
B â€²
A
QA
(s
,
a
)
=
Q
(s
,
a
)
+
Î±
r
+
1(s
)Î³
max
Q
(s
,
a)
âˆ’
Q
(s
,
a
)
,
k k
k
k+1
k k
k+1 k k
k
k
k
k
k
aâˆˆA

B
A
B
QB
k+1 (sk , ak ) = Qk (sk , ak ) + Î±k Î²(Qk (sk , ak ) âˆ’ Qk (sk , ak )).
where Î±k > 0 is the learning rate, Î² > 0 is a constant weight, and sâ€²k can be interpreted in the
following two ways: 1) in the Markovian observation model, sâ€²k = sk+1 , i.e., the next state and 2) in
the i.i.d. observation model, sâ€²k is the next state sampled at the current time, while it is not identical
to sk+1 . In the second i.i.d. observation model, sâ€²k is sampled independently from some distribution

d. In this paper, we consider the i.i.d. observation model for our convergence analysis to simplify the
overall analysis. However, we note that the analysis presented in this paper for the i.i.d. observation
model can be extended to the Markovian observation model with some modifications [19]. We also
acknowledge that such more sophisticated analyses are not the main focus in this paper. The main
goal of the convergence analysis in this section is to provide conceptual insights into the convergence
of the proposed algorithms. We also note that simulation results are provided in the Appendix G to
demonstrate the validity and convergence of AGT2-QL. Empirically, the results show that AGT2-QL
tends to converge faster for larger values of Î² > 0.

We note that this algorithm is similar to the so-called averaging Q-learning developed in [15], which
can be seen as a precursor. The averaging Q-learning in [15] maintains two separate estimates,
A
A
the target estimate QB
k and the online estimate Qk : the online estimate Qk is for approximating
Q-function and updated through an online manner, whereas the target estimate QB
k is for computing
the target values and updated through taking Polyakâ€™s averaging. Only the difference is the update
rule for QB
k which depends on the transition (sk , ak ).
AGT2-QL can be seen as a tabular version of AGT2-DQN due to the following insights: if we
consider the two loss functions
1
Î²
lA (QA ) := (y1 âˆ’ QA (sk , ak ))2 , lB (QB ) := (y2 âˆ’ QB (sk , ak ))2
2
2
with
y1 = rk+1 + 1(sâ€²k )Î³ max QB
y2 = QA
k (sk+1 , a),
k (sk , ak )
aâˆˆA

then each update in AGT2-QL can be interpreted as the gradient descent steps
âˆ‚lA (QA )
A
(1)
QA
k+1 (sk , ak ) = Qk (sk , ak ) âˆ’ Î±k
âˆ‚QA (sk , ak )
and
âˆ‚lB (QB )
B
QB
,
(2)
k+1 (sk , ak ) = Qk (sk , ak ) âˆ’ Î±k
âˆ‚QB (sk , ak )
In other words, AGT2-QL can be seen as an tabular and online implementation of AGT2-DQN,
whose convergence can be established as follows:
Theorem 3. Let us consider AGT2-QL and assume that the step-size satisfies
âˆž
âˆž
X
X
Î±k = âˆž,
Î±k2 < âˆž.
0 â‰¤ Î±k â‰¤ 1,
k=0

(3)

k=0

Assume that (sk , ak , sâ€²k ) are i.i.d. samples, where ak is sampled from the fixed behavior policy b and
âˆ—
B
âˆ—
sk is sampled from a fixed state distribution d. Then for any Î² > 0, QA
k â†’ Q and Qk â†’ Q with
probability one.
The corresponding convergence analysis is given in Appendix E. In AGT2-QL, the update for the
A
target estimate QB
k and the online estimate Qk are different, resulting in asymmetric updates for the
two values. We can extend the idea in AGT2-QL by symmetrizing the update rule. In particular, by
adding the two targets y1 and y2 and switching the roles of the target and online estimates, one can
8

construct the following loss functions:
1
Î²
lA (QA ) := (y1 âˆ’ QA (sk , ak ))2 + (QB (sk , ak ) âˆ’ QA (sk , ak ))2
2
2
and
1
Î²
lB (QB ) := (y2 âˆ’ QB (sk , ak ))2 + (QA (sk , ak ) âˆ’ QB (sk , ak ))2
2
2
with the targets
â€²
y1 = rk+1 + 1(sâ€²k )Î³maxQB
k (sk , a),

â€²
y2 = rk+1 + 1(sâ€²k )Î³maxQA
k (sk , a)

aâˆˆA

aâˆˆA

where Î² > 0 is a constant representing the regularization weight. Applying the gradient steps in (1)
and (2) results in the following SGT2-QL:
A
â€²
B â€²
A
QA
k+1 (sk , ak ) =Qk (sk , ak ) + Î±k {rk+1 + 1(sk )Î³ max Qk (sk , a) âˆ’ Qk (sk , ak )
aâˆˆA

A
+ Î²(QB
k (sk , ak ) âˆ’ Qk (sk , ak ))}
B
â€²
A â€²
B
QB
k+1 (sk , ak ) =Qk (sk , ak ) + Î±k {rk+1 + 1(sk )Î³ max Qk (sk , a) âˆ’ Qk (sk , ak )
aâˆˆA

B
+ Î²(QA
k (sk , ak ) âˆ’ Qk (sk , ak ))}

which can be seen as a tabular counterpart of SGT2-DQN.
A
We can observe that now the updates for the online and target estimates QB
k and Qk are symmetric.
We also note that this variant can be seen as a Q-learning counterpart of the double TD-learning
developed in [14]. In this paper, we establish its convergence as follows:
Theorem 4. Let us consider SGT2-QL and assume that the step-size satisfies (3). Assume that
(sk , ak , sâ€²k ) are i.i.d. samples, where ak is sampled from the fixed behavior policy b and sk is sampled
âˆ—
B
âˆ—
from a fixed state distribution d. Then for any Î² > 0, QA
k â†’ Q and Qk â†’ Q with probability one.

The proof is provided in the Appendix F. We note that the convergence of SGT2-QL and its analysis
have not yet been investigated in the literature. Additionally, empirical results demonstrating the
convergence of this algorithm are included in the Appendix G. These results show that SGT2-QL
generally converges faster than AGT2-QL. Moreover, they indicate that the convergence speed of
SGT2-QL is less sensitive to the weight Î² > 0 compared to AGT2-QL.

7

Conclusion

This paper introduces gradient target tracking frameworks as alternatives to the hard target update in
DQN. By replacing the periodic target updates with a continuous gradient-based tracking mechanism,
the proposed approach mitigates tuning challenges. Theoretical analysis establishes the convergence
of the proposed methods in tabular settings. These findings suggest that the gradient-based target
tracking is a promising alternative to conventional target update mechanisms in reinforcement
learning.

References
[1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and
stabilization for deep reinforcement learning. In International conference on machine learning,
pages 176â€“185, 2017.
[2] Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size Q-learning.
Systems & Control letters, 61(12):1203â€“1208, 2012.
[3] Shalabh Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic recursive algorithms for
optimization: simultaneous perturbation methods, volume 434. Springer, 2012.
[4] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447â€“469, 2000.
[5] Zaiwei Chen, Siva T Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A lyapunov theory for finite-sample guarantees of markovian stochastic approximation. Operations
Research, 72(4):1352â€“1367, 2024.
9

[6] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of machine learning
Research, 5(Dec):1â€“25, 2003.
[7] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In International conference on machine learning, pages 1587â€“1596, 2018.
[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International
conference on machine learning, pages 1861â€“1870, 2018.
[9] Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,
pages 2613â€“2621, 2010.
[10] Morris W Hirsch and Hal Smith. Monotone dynamical systems. In Handbook of differential
equations: ordinary differential equations, volume 2, pages 239â€“357. Elsevier, 2006.
[11] Hassan K Khalil. Nonlinear systems. Upper Saddle River, 2002.
[12] Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: removing the need for a target network in deep q-learning. In Proceedings of the twenty eighth
international joint conference on artificial intelligence, 2019.
[13] Harold Kushner and G. George Yin. Stochastic approximation and recursive algorithms and
applications, volume 35. Springer Science & Business Media, 2003.
[14] Donghwan Lee and Niao He. Target-based temporal-difference learning. In International
Conference on Machine Learning, pages 3713â€“3722, 2019.
[15] Donghwan Lee and Niao He. A unified switching system perspective and convergence analysis
of q-learning algorithms. Advances in Neural Information Processing Systems, 33:15556â€“15567,
2020.
[16] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous q-learning: Sharper analysis and variance reduction. IEEE Transactions on Information Theory, 68(1):448â€“473, 2021.
[17] Daniel Liberzon. Switching in systems and control. Springer Science & Business Media, 2003.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
[19] Han-Dong Lim and Donghwan Lee. Finite-time analysis of asynchronous q-learning under
diminishing step-size from control-theoretic view. IEEE Access, 2024.
[20] Hai Lin and Panos J Antsaklis. Stability and stabilizability of switched linear systems: a survey
of recent results. IEEE Transactions on Automatic control, 54(2):308â€“322, 2009.
[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[22] Alexandre PichÃ©, Joseph Marino, Gian Maria Marconi, Christopher Pal, and Mohammad Emtiyaz Khan. Beyond target networks: Improving deep q-learning with functional
regularization.
[23] AndrÃ© Platzer. Vector barrier certificates and comparison systems. In Formal Methods: 22nd
International Symposium, FM 2018, Held as Part of the Federated Logic Conference, FloC
2018, Oxford, UK, July 15-17, 2018, Proceedings, volume 10951, page 418, 2018.
[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
10

[25] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484â€“489,
2016.
[26] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine
learning, 3(1):9â€“44, 1988.
[27] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT Press,
1998.
[28] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
[29] Wolfgang Walter. Ordinary differential equations (graduate texts in mathematics). Springer,
1998.
[30] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.
Dueling network architectures for deep reinforcement learning. In International conference on
machine learning, pages 1995â€“2003, 2016.

11

A

Deep Q-learning with asymmetric gradient target tracking

A detailed pseudo code of the proposed AGT2-DQN is given in Algorithm 1.
Algorithm 1 Deep Q-learning with asymmetric gradient target tracking (AGT2-DQN)
1: Initialize replay memory D to capacity |D|
2: Randomly initialize the online parameter Î¸1 âˆˆ Rm
3: Set Î¸2 = Î¸1
4: Set k = 0
5: for Episode i âˆˆ {1, 2, . . .} do
6:
Observe s0
7:
for t âˆˆ {0, 1, . . . , Ï„ âˆ’ 1} do
8:
Take an action at according to

arg maxaâˆˆA QÎ¸1 (st , a) with probability 1 âˆ’ Îµ
at =
a âˆ¼ uniform(A) with probability Îµ
Observe rt+1 , st+1
Store the transition (st , at , rt+1 , st+1 ) in D
Sample uniformly a random mini-batch B of transitions (s, a, r, sâ€² ) from D
Set
X
1 1
2
L1 (Î¸1 ; B) :=
(y1 âˆ’ QÎ¸1 (s, a))
2 |B|
â€²

9:
10:
11:
12:

(s,a,r,s )âˆˆB

and
L2 (Î¸2 ; B) :=
where

1 1
2 |B|

X

(y2 âˆ’ QÎ¸2 (s, a))

2

(s,a,r,sâ€² )âˆˆB

y1 = r + 1(sâ€² )Î³ max QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a)
aâˆˆA

and
y2 = QÎ¸1 (s, a)
13:

Perform a gradient descent step
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1 (Î¸1 ; B),

14:
Set k â† k + 1
15:
end for
16: end for

12

Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2 (Î¸2 ; B)

B

Deep Q-learning with symmetric gradient target tracking

A detailed pseudo code of the proposed SGT2-DQN is given in Algorithm 2.
Algorithm 2 Deep Q-learning with symmetric gradient target tracking (SGT2-DQN)
1: Initialize replay memory D to capacity |D|
2: Randomly initialize the online parameter Î¸1 âˆˆ Rm
3: Set Î¸2 = Î¸1
4: Set k = 0
5: for Episode i âˆˆ {1, 2, . . .} do
6:
Observe s0
7:
for t âˆˆ {0, 1, . . . , Ï„ âˆ’ 1} do
8:
Take an action at according to

arg maxaâˆˆA QÎ¸1 (st , a) with probability 1 âˆ’ Îµ
at =
a âˆ¼ uniform(A) with probability Îµ
Observe rt+1 , st+1
Store the transition (st , at , rt+1 , st+1 ) in D
Sample uniformly a random mini-batch B of transitions (s, a, r, sâ€² ) from D
Set
X
1 1
2
2
L1 (Î¸1 ; B) :=
[(y1 âˆ’ QÎ¸1 (s, a)) + Î²(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) ]
2 |B|
â€²

9:
10:
11:
12:

(s,a,r,s )âˆˆB

and
L2 (Î¸2 ; B) :=

1 1
2 |B|

where

X

2

(s,a,r,sâ€² )âˆˆB

y1 = r + 1(sâ€² )Î³ max QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a)
aâˆˆA

and

y2 = r + 1(sâ€² )Î³ max QÎ¸1 (sâ€² , a) âˆ’ QÎ¸2 (s, a)
aâˆˆA

13:

2

[(y2 âˆ’ QÎ¸2 (s, a)) + Î²(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a)) ]

Perform a gradient descent step
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1 (Î¸1 ; B),

14:
Set k â† k + 1
15:
end for
16: end for

13

Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2 (Î¸2 ; B)

C

Convergence analysis with ODE model

C.1

Basics of nonlinear system theory

Let us consider the nonlinear system
d
xt = f (xt ), x0 = z, t âˆˆ R+ ,
(4)
dt
n
n
n
where xt âˆˆ R is the state, f : R â†’ R is a nonlinear mapping, and R+ denotes the set of
nonnegative real numbers. For simplicity, we assume that the solution to (4) exists and is unique. In
fact, this holds true so long as the mapping f is globally Lipschitz continuous.
Lemma 1 ([11, Theorem 3.2]). Let us consider the nonlinear system (4) and assume that f is globally
Lipschitz continuous, i.e.,
âˆ¥f (x) âˆ’ f (y)âˆ¥ â‰¤ Lâˆ¥x âˆ’ yâˆ¥, âˆ€x, y âˆˆ Rn ,
(5)
n
for some L > 0 and norm âˆ¥ Â· âˆ¥, then it has a unique solution x(t) for all t â‰¥ 0 and x(0) âˆˆ R .
An important concept in dealing with the nonlinear system is the equilibrium point. A point x = xe
in the state space is said to be an equilibrium point of (4) if it has the property that whenever the
state of the system starts at xe , it will remain at xe [11]. For (4), the equilibrium points are the
real roots of the equation f (x) = 0. The equilibrium point xe is said to be globally asymptotically
stable if for any initial state x0 âˆˆ Rn , xt â†’ xe as t â†’ âˆž. Now, we provide a vector comparison
principle [29, 10, 23] for multi-dimensional O.D.E., which will play a central role in the analysis
below. We first introduce the quasi-monotone increasing function, which is a necessary prerequisite
for the comparison principle.
Definition 1 (Quasi-monotone function). A vector-valued function f : Rn â†’ Rn with f :=
T
[f1 f2 Â· Â· Â· fn ] is said to be quasi-monotone increasing if fi (x) â‰¤ fi (y) holds for all i âˆˆ
{1, 2, . . . , n} and x, y âˆˆ Rn such that xi = yi and xj â‰¤ yj for all j Ì¸= i.
An example of a quasi-monotone increasing function f is f (x) = Ax where A is a Metzler matrix,
which implies the off-diagonal elements of A are nonnegative. Now, we introduce a vector comparison
principle. For completeness, we provide a simple proof tailored to our interests.
Lemma 2 (Vector comparison principle [10, Theorem 3.2]). Suppose that f and f are globally
Lipschitz continuous. Let vt be a solution of the system
d
xt = f (xt ), x0 âˆˆ Rn , âˆ€ t â‰¥ 0,
dt
assume that f is quasi-monotone increasing, and let vt be a solution of the system
d
vt = f (vt ), v0 < x0 , âˆ€t â‰¥ 0,
(6)
dt
where f (v) â‰¤ f (v) holds for any v âˆˆ Rn . Then, vt â‰¤ xt for all t â‰¥ 0.
Proof. Instead of (6), first consider
d
vÎµ (t) = f (vÎµ (t)) âˆ’ Îµ1n , vÎµ (0) < x(0), âˆ€t â‰¥ 0
dt
where Îµ > 0 is a sufficiently small real number and 1n is a vector where all elements are ones, where
we use a different notation for the time index for convenience. Suppose that the statement is not true,
and let
tâˆ— := inf{t â‰¥ 0 : âˆƒi such that vÎµ,i (t) > xi (t)} < âˆž,
and let i be such index. By the definition of tâˆ— , we have that vÎµ,i (tâˆ— ) = xi (tâˆ— ) and vÎµ,j (tâˆ— ) â‰¤ xj (tâˆ— )
for any j Ì¸= i. Then, since f is quasi-monotone increasing, we have
f i (vÎµ (tâˆ— )) â‰¤ f i (x(tâˆ— )).

(7)

On the other hand, by the definition of tâˆ— , there exists a small Î´ > 0 such that
vÎµ,i (tâˆ— + âˆ†t) > xi (tâˆ— + âˆ†t)
for all 0 < âˆ†t < Î´. Dividing both sides by âˆ†t and taking the limit âˆ†t â†’ 0, we have
vÌ‡Îµ,i (tâˆ— ) â‰¥ xÌ‡i (tâˆ— ) = f i (x(tâˆ— )).
14

(8)

By the hypothesis, it holds that
d
vÎµ (t) = f (vÎµ (t)) âˆ’ Îµ1n < f (vÎµ (t)) â‰¤ f (vÎµ (t))
dt
holds for all t â‰¥ 0. The inequality implies vÌ‡Îµ,i (t) < f i (vÎµ (t)), which in combination with (8) leads
to f i (vÎµ (tâˆ— )) > f i (x(tâˆ— )). However, it contradicts with (7). Therefore, vÎµ (t) â‰¤ x(t) holds for all
t â‰¥ 0. Since the solution vÎµ (t) continuously depends on Îµ > 0 [29, Chap. 13], taking the limit Îµ â†’ 0,
we conclude v0 (t) â‰¤ x(t) holds for all t â‰¥ 0. This completes the proof.
C.2

Switching system theory

Let us consider the particular nonlinear system, the switched linear system,
d
xt = AÏƒt xt , x0 = z âˆˆ Rn , t âˆˆ R+ ,
(9)
dt
where xt âˆˆ Rn is the state, Ïƒ âˆˆ M := {1, 2, . . . , M } is called the mode, Ïƒt âˆˆ M is called the
switching signal, and {AÏƒ , Ïƒ âˆˆ M} are called the subsystem matrices. The switching signal can be
either arbitrary or controlled by the user under a certain switching policy. Especially, a state-feedback
switching policy is denoted by Ïƒ(xt ). To prove the global asymptotic stability of the switching
system, we will use a fundamental algebraic stability condition of switching systems reported in [20].
More comprehensive surveys of stability of switching systems can be found in [20] and [17].
Lemma 3 ([20, Theorem 8]). The origin of the linear switching system (9) is the unique globally
asymptotically stable equilibrium point under arbitrary switchings, Ïƒt , if and only if there exist a full
column rank matrix , L âˆˆ RmÃ—n , m â‰¥ n, and a family of matrices, AÌ„Ïƒ âˆˆ RmÃ—n , Ïƒ âˆˆ M, with the
so-called strictly negative row dominating diagonal condition, i.e., for each AÌ„Ïƒ , Ïƒ âˆˆ M, its elements
satisfying
X
[AÌ„Ïƒ ]ii +
|[AÌ„Ïƒ ]ij | < 0, âˆ€i âˆˆ {1, 2, . . . , m},
jâˆˆ{1,2,...,n}\{i}

where [Â·]ij is the (i, j)-element of a matrix (Â·), such that the matrix relations
LAÏƒ = AÌ„Ïƒ L,

âˆ€Ïƒ âˆˆ M,

are satisfied.
A more general class of systems is the affine switching system
d
xt = AÏƒt xt + bÏƒt , x0 = z âˆˆ Rn , t âˆˆ R+ ,
(10)
dt
n
where bÏƒt âˆˆ R is the additional input vector. Due to the additional input bÏƒk , its stabilization
becomes much more challenging.
C.3

ODE-based stochastic approximation

Due to its generality, the convergence analyses of many RL algorithms rely on the ODE (ordinary
differential equation) approach [3, 13]. It analyzes convergence of general stochastic recursions by
examining stability of the associated ODE model based on the fact that the stochastic recursions with
diminishing step-sizes approximate the corresponding ODEs in the limit. One of the most popular
approach is based on the Borkar and Meyn theorem [4]. We now briefly introduce the Borkar and
Meynâ€™s ODE approach for analyzing convergence of the general stochastic recursions
Î¸k+1 = Î¸k + Î±k (f (Î¸k ) + Îµk+1 )
(11)
where f : Rn â†’ Rn is a nonlinear mapping. Basic technical assumptions are given below.
Assumption 1.
1. The mapping f : Rn â†’ Rn is globally Lipschitz continuous and there exists a function
fâˆž : Rn â†’ Rn such that
f (cx)
lim
= fâˆž (x), âˆ€x âˆˆ Rn .
câ†’âˆž
c
2. The origin in Rn is an asymptotically stable equilibrium for the ODE xÌ‡t = fâˆž (xt ).
15

3. There exists a unique globally asymptotically stable equilibrium Î¸e âˆˆ Rn for the ODE
xÌ‡t = f (xt ), i.e., xt â†’ Î¸e as t â†’ âˆž.
4. The sequence {Îµk , Gk , k â‰¥ 1} with Gk = Ïƒ(Î¸i , Îµi , i â‰¤ k) is a Martingale difference
sequence. In addition, there exists a constant C0 < âˆž such that for any initial Î¸0 âˆˆ Rn , we
have E[âˆ¥Îµk+1 âˆ¥2 |Gk ] â‰¤ C0 (1 + âˆ¥Î¸k âˆ¥2 ), âˆ€k â‰¥ 0.
Pâˆž
Pâˆž
5. The step-sizes satisfy Î±k > 0, k=0 Î±k = âˆž, k=0 Î±k2 < âˆž.
Lemma 4 ([4, Borkar and Meyn theorem]). Suppose that Assumption 1 holds. For any initial
Î¸0 âˆˆ Rn , supkâ‰¥0 âˆ¥Î¸k âˆ¥ < âˆž with probability one. In addition, Î¸k â†’ Î¸e as k â†’ âˆž with probability
one.
The Borkar and Meyn theorem states that under Assumption 1, the stochastic process (Î¸k )âˆž
k=0
generated by (11) is bounded and converges to Î¸e almost surely.

D

Assumptions and definitions

In this paper, we focus on the following setting: {(sk , ak , sâ€²k )}âˆž
k=0 are i.i.d. samples under the
behavior policy Î², where the time-invariant behavior policy is the policy by which the RL agent
actually behaves to collect experiences. Note that the notation sâ€²k implies the next state sampled at
the time step k, which is used instead of sk+1 in order to distinguish sâ€²k from sk+1 . In this paper, the
notation sk+1 indicate the current state at the iteration step k + 1, while it does not depend on sk .
For simplicity, we assume that the state at each time is sampled from the stationary state distribution
p, and in this case, the state-action distribution at each time is identically given by
d(s, a) = p(s)b(a|s), (s, a) âˆˆ S Ã— A.
In this paper, we assume that the behavior policy b is time-invariant, and this scenario excludes the
common method of using the Îµ-greedy behavior policy with Îµ > 0 because the Îµ-greedy behavior
policy depends on the current Q-iterate, and hence is time-varying. Moreover, the proposed analysis
cannot be easily extended to the analysis of Q-learning with the Îµ-greedy behavior policy. However,
we note that this assumption is common in the ODE approaches for Q-learning and TD-learning [26].
This assumption can be relaxed by considering a time-varying distribution. However, this direction
will not be addressed in this paper to simplify the presentation of the proofs.
Throughout, we make the following assumption for convenience.
Assumption 2. d(s, a) > 0 holds for all (s, a) âˆˆ S Ã— A.
This assumption ensures that every state-action pair is visited infinitely often for sufficient exploration.
It is applied when the state-action occupation frequency is given and has also been considered in [16]
and [5]. The work in [2] introduces an alternative exploration condition, known as the cover time
condition, which states that within a certain time period, every state-action pair is expected to be
visited at least once. Slightly different variations of the cover time condition have been used in [6]
and [16] for convergence rate analysis.
Throughout the paper, we will use the following matrix notations for compact dynamical system
representations:
ï£®
ï£¹
ï£®
ï£¹
ï£®
ï£¹
P1
R1
Q(Â·, 1)
ï£¯
ï£º
ï£¯
ï£º
ï£¯
ï£º
..
P := ï£° ... ï£» , R := ï£° ... ï£» , Q := ï£°
ï£»,
.
P|A|
R|A|
Q(Â·, |A|)
ï£®
ï£¹
ï£®
ï£¹
D1
d(1, a)
ï£¯
ï£º
ï£¯
ï£º
..
..
Da := ï£°
ï£» , D := ï£°
ï£»,
.
.
D|A|
d(|S|, a)
where Pa = P (Â·, a, Â·) âˆˆ R|S|Ã—|S| , Q(Â·, a) âˆˆ R|S| , a âˆˆ A and Ra (s) := E[r(s, a, sâ€² )|s, a]. Note that
P âˆˆ R|SÃ—A|Ã—|S| , R âˆˆ R|SÃ—A| , Q âˆˆ R|SÃ—A| , and D âˆˆ R|SÃ—A|Ã—|SÃ—A| . In this notation, Q-function
is encoded as a single vector Q âˆˆ R|SÃ—A| , which enumerates Q(s, a) for all s âˆˆ S and a âˆˆ A. In
particular, the single value Q(s, a) can be written as
Q(s, a) = (ea âŠ— es )T Q,
16

where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. Note also that under Assumption 2, D is a
nonsingular diagonal matrix with strictly positive diagonal elements.
For any stochastic policy, Ï€ : S â†’ âˆ†A , where âˆ†A is the set of all probability distributions over A,
we define the corresponding action transition matrix as
ï£®
ï£¹
Ï€(1)T âŠ— eT1
ï£¯ Ï€(2)T âŠ— eT2 ï£º
ï£¯
ï£º
Î Ï€ := ï£¯
(12)
..
ï£º âˆˆ R|S|Ã—|SÃ—A| ,
ï£°
ï£»
.
Ï€(|S|)T âŠ— eT|S|

where es âˆˆ R|S| . Then, it is well known that P Î Ï€ âˆˆ R|SÃ—A|Ã—|SÃ—A| is the transition probability
matrix of the state-action pair under policy Ï€. If we consider a deterministic policy, Ï€ : S â†’ A, the
stochastic policy can be replaced with the corresponding one-hot encoding vector âƒ—Ï€ (s) := eÏ€(s) âˆˆ
âˆ†A , where ea âˆˆ R|A| , and the corresponding action transition matrix is identical to (12) with
Ï€ replaced with âƒ—Ï€ . For any given Q âˆˆ R|SÃ—A| , denote the greedy policy w.r.t. Q as Ï€Q (s) :=
arg maxaâˆˆA Q(s, a) âˆˆ A. We will use the following shorthand frequently: Î Q := Î Ï€Q .

E

Convergence of AGT2-QL

In this section, we study convergence of AGT2-QL. The full algorithm is described in Algorithm 3.
We now analyze the convergence of AGT2-QL using the same switching system approach [15].
Algorithm 3 AGT2-QL
B
1: Initialize QA
0 and Q0 randomly.
2: for iteration k = 0, 1, . . . do
3:
Sample (s, a)
4:
Sample sâ€² âˆ¼ P (Â·|s, a) and r(s, a, sâ€² )
A
â€²
B â€²
A
5:
Update QA
k+1 (s, a) = Qk (s, a) + Î±k {r(s, a, s ) + Î³ maxaâˆˆA Qk (s , a) âˆ’ Qk (s, a)}
B
A
B
6:
Update QB
k+1 (s, a) = Qk (s, a) + Î±k Î²(Qk (s, a) âˆ’ Qk (s, a))
7: end for
In particular, the main theoretical tool is the Borkar and Meyn theorem [4]. To apply this framework,
it is crucial to study the corresponding ODE model and its asymptotic stability. Therefore, in the next
subsection, we first derive the ODE model for AGT2-QL.
E.1

Original system

Using the notation introduced in Appendix D, the update of can be rewritten as
n
T
A
QA
k+1 =Qk + Î±k (ea âŠ— es )(ea âŠ— es ) R
T

T A
maxQB
k (Â·, a) âˆ’ (ea âŠ— es )(ea âŠ— es ) Qk
aâˆˆA



+Î³(ea âŠ— es )(esâ€² )
n
o
T A
T B
B
QB
=Q
+
Î±
Î²
(e
âŠ—
e
)(e
âŠ—
e
)
Q
âˆ’
(e
âŠ—
e
)(e
âŠ—
e
)
Q
k
a
s
a
s
a
s
a
s
k+1
k
k
k
where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. The above update can be further expressed
as
A
A
A
QA
QB
k+1 =Qk + Î±k {DR + Î³DP Î QB
k âˆ’ DQk + Îµk+1 }
k
B
A
B
B
QB
k+1 =Qk + Î±k {Î²DQk âˆ’ Î²DQk + Îµk+1 }

where
T
T
ÎµA
QB
k+1 =(ea âŠ— es )(ea âŠ— es ) R + Î³(ea âŠ— es )(esâ€² ) Î QB
k
k
A
âˆ’ (ea âŠ— es )(ea âŠ— es )T QA
QB
k âˆ’ (DR + Î³DP Î QB
k âˆ’ DQk )
k

17

T
A
T
B
A
B
ÎµB
k+1 =(ea âŠ— es )(ea âŠ— es ) Î²Qk âˆ’ (ea âŠ— es )(ea âŠ— es ) Î²Qk âˆ’ Î²(DQk âˆ’ DQk )

As discussed in Appendix C.3, the convergence of AGT3-QL can be analyzed by evaluating the
stability of the corresponding continuous-time ODE
 A  

 A 
 A  
d
âˆ’D Î³DP Î QB
Qt
DR
Q0
Qt
t
+
,
âˆˆ R2|S||A| , ,
(13)
=
B
B
0
Î²D
âˆ’Î²D
Q
Q
dt QB
t
t
0
Using the Bellman equation (Î³DP Î Qâˆ— âˆ’ D)Qâˆ— + DR = 0, the above expressions can be rewritten
by
 A
 

 A
 
d
Î³DP (Î QB
âˆ’ Î Qâˆ— )Qâˆ—
âˆ’D Î³DP Î QB
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
t
t
+
,
=
âˆ—
âˆ—
Î²D
âˆ’Î²D
0
QB
dt QB
t âˆ’Q
t âˆ’Q
 A

Q0 âˆ’ Qâˆ—
=z âˆˆ R2|S||A| .
(14)
âˆ—
QB
0 âˆ’Q
The above system is a linear switching system discussed in Appendix C.2. More precisely, let Î˜ be
the set of all deterministic policies, let us define a one-to-one mapping Ï† : Î˜ â†’ {1, 2, . . . , |Î˜ Ã— Î˜|}
from two deterministic policies (Ï€A , Ï€B ) âˆˆ Î˜ Ã— Î˜ to an integer in {1, 2, . . . , |Î˜ Ã— Î˜|}, and define




âˆ—
âˆ’D Î³DP Î Ï€B
Î³DP (Î Ï€B âˆ’ Î Ï€ )Qâˆ—
Ai =
âˆˆ R2|SÃ—A|Ã—2|SÃ—A| , bi =
âˆˆ R2|SÃ—A|
Î²D
âˆ’Î²D
0
for all i = Ï†(Ï€A , Ï€B ) and (Ï€A , Ï€B ) âˆˆ Î˜ Ã— Î˜. Then, the above ODE can be written by the affine
switching system
d
xt = AÏƒ(xt ) xt + bÏƒ(xt ) , x0 = z âˆˆ R|S||A| ,
dt
where

 A
Qt âˆ’ Qâˆ—
xt :=
âˆ—
QB
t âˆ’Q
is the state, Ïƒ : R|S||A| â†’ {1, 2, . . . , |Î˜ Ã— Î˜|} is a state-feedback switching policy defined by
Ïƒ(xt ) := Ïˆ(Ï€QA
, Ï€QB
), and Ï€Q (s) = arg maxaâˆˆA Q(s, a).
t
t
Until now, we have derived an ODE model of AGT2-QL. The next goal is to prove its asymptotic
stability which is essential step to apply the Borkar and Meyn theorem [4]. Notably, proving the global
asymptotic stability of the above switching system without the affine term is relatively straightforward
using Lemma 3. However, existing theories do not support switching systems that include affine
terms.
To address this issue, we construct two comparison systems by leveraging the special structure of the
switching system and the greedy policy, and we establish their global asymptotic stability. Building
on the vector comparison principle introduced in Lemma 2, we then prove the asymptotic stability of
the desired affine switching system. We note that this approach was first proposed in [15], and we
follow its framework with modifications tailored to the proposed algorithms.
To proceed, defining the vector functions



  

f (x , x )
âˆ’D Î³DP Î x2 +Qâˆ— x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
f (x1 , x2 ) := 1 1 2 :=
+
,
f2 (x1 , x2 )
Î²D
âˆ’Î²D
x2
0
(14) can be written by the systems

 

d xt,1
f (x , x )
= 1 t,1 t,2 ,
f2 (xt,1 , xt,2 )
dt xt,2


âˆ—
QA
0 âˆ’Q
âˆ— ,
QB
0 âˆ’Q


z0 =

âˆ€t â‰¥ 0

(15)

with


xt,1
xt,2




=

âˆ—
QA
t âˆ’Q
B
Qt âˆ’ Qâˆ—



In the following, we present several lemmas to support the main analysis results.
Lemma 5. We have

f (x1 , x2 ) =

âˆ’Dx1 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
Î²Dx1 âˆ’ Î²Dx2
18



Proof. It can be proved through


 

âˆ’D Î³DP Î x2 +Qâˆ—
x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
f (x1 , x2 ) =
+
Î²D
âˆ’Î²D
x2
0


âˆ’Dx1 + Î³DP Î x2 +Qâˆ— x2 + Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
=
Î²Dx1 âˆ’ Î²Dx2


âˆ’Dx1 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
=
,
Î²Dx1 âˆ’ Î²Dx2
which completes the proof.
Lemma 6. f is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for f1 and f2 , separately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element
of âˆ†x1 is zero. For f1 , using Lemma 5, we have
eTi f1 (x1 + âˆ†x1 , x2 + âˆ†x2 )
= âˆ’ eTi D(x1 + âˆ†x1 ) + Î³eTi DP Î x2 +âˆ†x2 +Qâˆ— (x2 + âˆ†x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
= âˆ’ eTi Dx1 + Î³eTi DP Î x2 +âˆ†x2 +Qâˆ— (x2 + âˆ†x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
â‰¥ âˆ’ eTi Dx1 + Î³eTi DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
=eTi f1 (x1 , x2 ),
where the second line is due to âˆ’eTi Dâˆ†x1 = 0. Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eTi f2 (x1 + âˆ†x1 , x2 + âˆ†x2 ) =Î²eTi D(x1 + âˆ†x1 ) âˆ’ Î²eTi D(x2 + âˆ†x2 )
=Î²eTi D(x1 + âˆ†x1 ) âˆ’ Î²eTi Dx2
â‰¥Î²eTi Dx1 âˆ’ Î²eTi Dx2
=eTi f2 (x1 , x2 ),
where the second line is due to eTi Dâˆ†x2 = 0. Therefore, f is quasi-monotone increasing.
Lemma 7. f is globally Lipshcitz continuous.
Proof. From Lemma 5, one gets


 
 

âˆ’D
0
x1
Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— )
âˆ’Î³DP Î Qâˆ— Qâˆ—
f (x1 , x2 ) =
+
+
Î²D âˆ’Î²D
x2
0
0
Therefore, we have the inequalities
âˆ¥f (x1 , x2 ) âˆ’ f (y1 , y2 )âˆ¥âˆž

 

âˆ’D
0
x1
âˆ’D
â‰¤
âˆ’
x2
Î²D
Î²D âˆ’Î²D

0
âˆ’Î²D



y1
y2


âˆž

+ âˆ¥Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î y2 +Qâˆ— (y2 + Qâˆ— )âˆ¥âˆž




 
âˆ’D
0
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î y2 +Qâˆ— (y2 + Qâˆ— )âˆ¥âˆž
Î²D âˆ’Î²D
x2
y2
âˆž
âˆž



 


 

âˆ’D
0
x1
y1
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž
âˆ’
Î²D âˆ’Î²D
x2
y2
x2
y2
âˆž
âˆž
âˆž
indicating that f is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Our main goal here is to establish the asymptotic stability of the system given in (14). To this end,
we will apply the tools introduced in Appendix C.2. In particular, we will derive the upper and lower
comparison systems as discussed before.
19

E.2

Upper comparison system

We consider the following system which is called the upper comparison system:
  A


  A,u

 
âˆ’D Î³DP Î QB,u âˆ’Qâˆ— QA,u
d QA,u
Q0 âˆ’ Qâˆ—
âˆ’ Qâˆ—
Q0 âˆ’ Qâˆ—
âˆ’ Qâˆ—
2|S||A|
t
t
t
>
,
,
=
B
âˆ— âˆˆR
B,u
B,u
âˆ—
âˆ—
âˆ—
Q
âˆ’
Q
Î²D
âˆ’Î²D
dt QB,u
âˆ’
Q
Q
âˆ’
Q
Q
âˆ’
Q
0
t
t
0
(16)
where â€˜>â€™ in the above equation implies the element-wise inequality.
Defining the vector functions



h1 (x1 , x2 )
âˆ’D
h(x1 , x2 ) :=
:=
h2 (x1 , x2 )
Î²D

Î³DP Î x2
âˆ’Î²D

 
x1
x2

the upper comparison system can be written by the system

 

 A

d xt,1
h (x , x )
Q0 âˆ’ Qâˆ—
= 1 t,1 t,2 , x0 >
âˆ— ,
h2 (xt,1 , xt,2 )
QB
dt xt,2
0 âˆ’Q
with

  A,u

xt,1
Qt âˆ’ Qâˆ—
=
.
xt,2
QB,u
âˆ’ Qâˆ—
t

âˆ€t â‰¥ 0

In the following lemmas, we establish key properties of the upper comparison system.
Lemma 8. h is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for h1 and h2 , separately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element
of âˆ†x1 is zero. For h1 , we have
eTi h1 (x1 + âˆ†x1 , x2 + âˆ†x2 ) = âˆ’ eTi D(x1 + âˆ†x1 ) + Î³eTi DP Î x2 +âˆ†x2 (x2 + âˆ†x2 )
= âˆ’ eTi Dx1 + Î³eTi DP Î x2 +âˆ†x2 (x2 + âˆ†x2 )
â‰¥ âˆ’ eTi Dy1 + Î³eTi DP Î x2 x2
=eTi h1 (x1 , x2 ),
where the second line is due to âˆ’eTi Dâˆ†x1 = 0. Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eTi h2 (x1 + âˆ†x1 , x2 + âˆ†x2 ) =Î²eTi D(x1 + âˆ†x1 ) âˆ’ Î²eTi D(x2 + âˆ†x2 )
=Î²eTi D(x1 + âˆ†x1 ) âˆ’ Î²eTi Dx2
â‰¥Î²eTi Dx1 âˆ’ Î²eTi Dx2
=eTi h2 (x1 , x2 ),
where the second line is due to eTi Dâˆ†x2 = 0. Therefore, h is quasi-monotone increasing.
Lemma 9. h is globally Lipshcitz continuous.
Proof. We complete the proof through the inequalities
âˆ¥h(x1 , x2 ) âˆ’ h(y1 , y2 )âˆ¥âˆž


 


âˆ’D Î³DP Î x2
x1
âˆ’D Î³DP Î y2
y1
=
âˆ’
Î²D
âˆ’Î²D
x2
Î²D
âˆ’Î²D
y2
âˆž

 



y1
âˆ’D
0
x1
âˆ’D
0
+ âˆ¥Î³DP Î x2 x1 âˆ’ Î³DP Î y2 y2 âˆ¥âˆž
â‰¤
âˆ’
x2
Î²D âˆ’Î²D
y2
Î²D âˆ’Î²D
âˆž



 

âˆ’D
0
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥x1 âˆ’ y2 âˆ¥âˆž
Î²D âˆ’Î²D
x2
y2
âˆž
âˆž




 

 

âˆ’D
0
x1
y1
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž
âˆ’
Î²D âˆ’Î²D
x2
y2
x2
y2
âˆž
âˆž
âˆž
indicating that h is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
20

Lemma 10. h(x1 , x2 ) â‰¥ f (x1 , x2 ) for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| , where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. Using Î³DP (Î x2 âˆ’ Î Qâˆ— )Qâˆ— â‰¤ 0 for all x2 âˆˆ R|S||A| , we obtain


 

âˆ’D Î³DP Î x2 +Qâˆ—
x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
f (x1 , x2 ) =
+
Î²D
âˆ’Î²D
x2
0



âˆ’D Î³DP Î x2 +Qâˆ—
x1
â‰¤
Î²D
âˆ’Î²D
x2



âˆ’D Î³DP Î x2
x1
â‰¤
Î²D
âˆ’Î²D
x2
=h(x1 , x2 )
for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| . This completes the proof.
Based on the previous lemmas, we are now ready to prove that the solution of the upper comparison
system indeed upper-bounds the solution of the original system, which is the reason why (16) is
named as such.
Lemma 11. We have
  A

QA,u
âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
t
â‰¥
,
âˆ—
QB
QB,u
âˆ’ Qâˆ—
t âˆ’Q
t
where â€˜â‰¥â€™ denotes the element-wise inequality


âˆ€t â‰¥ 0,

Proof. The desired conclusion is obtained by Lemma 2 with Lemmas 7 to 10.
Next, we prove that the upper comparison system (16) is globally asymptotically stable at the origin.
Notably, the proof is simpler than that of the original system, as the upper comparison system does
not include the affine term.
Lemma 12. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium point
of the upper comparison system (16).
Proof. For notational convenience, we define Î Ïƒ , Ïƒ âˆˆ M as Î QB
such that Ïƒ = Ïˆ(Ï€QB
). Then, for
t
t
the upper comparison switching system, we apply Lemma 3 with


âˆ’D Î³DP Î Ïƒ
AÏƒ =
Î²D
âˆ’Î²D
and

I
0

Î³ 1/2 I





L=
which satisfies


I
0
âˆ’D
LAÏƒ =
Î²D
0 Î³ 1/2 I
with

Î³DP Î Ïƒ
âˆ’Î²D



0

=



âˆ’D
AÌ„Ïƒ = 1/2
Î³ Î²D

,

âˆ’D
Î³ 1/2 Î²D

Î³DP Î Ïƒ
âˆ’Î³ 1/2 Î²D




= AÌ„Ïƒ

I
0

0



Î³ 1/2 I


Î³ 1/2 DP Î Ïƒ
.
âˆ’Î²D

To check the strictly negative row dominating diagonal condition, for i âˆˆ {1, 2, . . . , |S||A|}, we have
X
X
[AÌ„Ïƒ ]ii +
|[AÌ„Ïƒ ]ij | =[âˆ’D]ii + Î³ 1/2 [D]ii
|[P Î Ïƒ ]ij |
jâˆˆ{1,2,...,n}\{i}

jâˆˆ{1,2,...,n}\{i}

â‰¤[âˆ’D]ii + Î³
â‰¤(âˆ’1 + Î³
21

1/2

1/2

[D]ii

)[D]ii < 0.

For i âˆˆ {|S||A| + 1, |S||A| + 2, . . . , 2|S||A|}, we also have
X
[AÌ„Ïƒ ]ii +
|[AÌ„Ïƒ ]ij | â‰¤ âˆ’[D]ii Î² + [D]ii Î²Î³ 1/2 = [D]ii Î²(âˆ’1 + Î³ 1/2 ) < 0
jâˆˆ{1,2,...,n}\{i}

for any Î² > 0. Therefore, the strictly negative row dominating diagonal condition is satisfied.
By Lemma 3, the origin of the upper comparison system is globally asymptotically stable.
E.3

Lower comparison system

Let us consider the so-called lower comparison system



 
d QA,l
âˆ’D Î³DP Î Ï€Qâˆ— QA,l
âˆ’ Qâˆ—
âˆ’ Qâˆ—
t
t
,
=
Î²D
âˆ’Î²D
dt QB,l
QB,l
âˆ’ Qâˆ—
âˆ’ Qâˆ—
t
t



the lower comparison system can be written by

 

d xt,1
g (x , x )
= 1 t,1 t,2 ,
g2 (xt,1 , xt,2 )
dt xt,2
for all t â‰¥ 0.

x0 <

  A

âˆ—
QA,l
Q0 âˆ’ Qâˆ—
2|S||A|
0 âˆ’Q
<
,
âˆ— âˆˆR
âˆ—
QB
QB,l
0 âˆ’Q
0 âˆ’Q
(17)
where â€˜<â€™ in the above equation implies the element-wise inequality. Here, we note that the lower
comparison system is simply a linear system. Therefore, its analysis is much simpler than the upper
and original system. Defining the vector functions

 
 
âˆ’D Î³DP Î Ï€Qâˆ— x1
g1 (x1 , x2 )
g(x1 , x2 ) :=
=
,
g2 (x1 , x2 )
x2
Î²D
âˆ’Î²D

âˆ—
QA
0 âˆ’Q
âˆ— ,
QB
0 âˆ’Q



In the sequel, we present key properties of the lower comparison system.
Lemma 13. g is globally Lipschitz continuous.
Proof. It is straightforward from the linearity. In particular, we complete the proof through the
inequalities


 


âˆ’D Î³DP Î Qâˆ—
x1
âˆ’D Î³DP Î Qâˆ—
y1
âˆ¥g(x1 , x2 ) âˆ’ g(y1 , y2 )âˆ¥âˆž =
âˆ’
Î²D
âˆ’Î²D
x2
Î²D
âˆ’Î²D
y2
âˆž



 

âˆ’D Î³DP Î Qâˆ—
x1
y1
â‰¤
âˆ’
Î²D
âˆ’Î²D
x2
y2
âˆž
âˆž
indicating that g is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 14. f (x1 , x2 ) â‰¥ g(x1 , x2 ) for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| , where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. We obtain
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
0


âˆ’Dx1 + Î³DP Î x2 +Qâˆ— x2 + Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
=
Î²Dx1 âˆ’ Î²Dx2


âˆ’Dx1 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
=
Î²Dx1 âˆ’ Î²Dx2


âˆ’Dx1 + Î³DP Î Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
â‰¥
Î²Dx1 âˆ’ Î²Dx2


âˆ’Dx1 + Î³DP Î Qâˆ— x2
=
Î²Dx1 âˆ’ Î²Dx2



âˆ’D Î³DP Î Qâˆ—
x1
=
Î²D
âˆ’Î²D
x2


f (x1 , x2 ) =

âˆ’D
Î²D

Î³DP Î x2 +Qâˆ—
âˆ’Î²D



x1
x2

=g(x1 , x2 )
22





+



for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| . This completes the proof.
Similar to the upper comparison system, we prove that the solution of the lower comparison system
indeed provides a lower bound for the solution of the original system.
Lemma 15. We have
 A
  A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â‰¥
, âˆ€t â‰¥ 0,
âˆ—
QB
QB,l
âˆ’ Qâˆ—
t âˆ’Q
t
where â€˜â‰¥â€™ denotes the element-wise inequality.
Proof. The desired conclusion is obtained by Lemma 2with Lemmas 6, 7, 13 and 14.
Moreover, the next lemma proves that the lower comparison system is also globally asymptotically
stable at the origin.
Lemma 16. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium point
of the lower comparison system (17).
Proof. The proof follows the same procedure as that of the upper comparison system. Therefore, the
detailed proof is omitted here to avoid repetition.
So far, we have established several key properties of the upper and lower comparison systems,
including their global asymptotic stability. In the next subsection, we prove the global asymptotic
stability of the original system based on these results.
E.4

Stability of the original system

We establish the global asymptotic stability of (14).
Theorem 5. For any Î² > 0, the origin is theunique globally asymptotically stable equilibrium
Qâˆ—
point of the original system (14). Equivalently,
is the unique globally asymptotically stable
Qâˆ—
equilibrium point of the original system (13).
Proof. By Lemmas 11 and 15, we have
 A,u
  A
  A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â‰¥
â‰¥
, âˆ€t â‰¥ 0
âˆ—
QB
QB,u
âˆ’ Qâˆ—
QB,l
âˆ’ Qâˆ—
t âˆ’Q
t
t
Moreover, by Lemma 16 and Lemma 12, we have
 A,u

 A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â†’
0,
â†’0
QB,u
âˆ’ Qâˆ—
QB,l
âˆ’ Qâˆ—
t
t
as t â†’ âˆž. Therefore, the state of the original system also asymptotically converges to the origin.
This completes the proof.
E.5

Numerical example

In this subsection, we present a simple example to illustrate the validity of the properties of the upper
and lower comparison systems established in the previous sections. To this end, let us consider an
MDP with S = {1, 2}, A = {1, 2}, Î³ = 0.9,




0.2 0.8
0.5 0.5
P1 =
, P2 =
,
0.3 0.7
0.7 0.3
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2, P[a = 2|s = 1] = 0.8,
P[a = 1|s = 2] = 0.7, P[a = 2|s = 2] = 0.3,
and
 
 
3
2
R1 =
, R2 =
1
1
23

Simulated trajectories of the ODE model of AGT2-QL including the upper and lower comparison
B
systems are depicted in Figure 5 for QA
t part and Figure 6 for Qt part. The results empirically prove
that the ODE model associated with AGT2-QL is asymptotically stable. Moreover, they illustrate that
the solutions of the upper and lower comparison systems bound the solution of the original system,
as established by the theory.

Figure 5: Trajectories of the O.D.E. model of AGT2-QL and the corresponding upper and lower
comparison systems (QA
t part). The solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.

E.6

Convergence of AGT2-QL

We can now apply the Borkar and Meyn theorem, Lemma 4, to prove Theorem 3. First of all, note
that the system in (15) corresponds to the ODE model in Assumption 1. The proof is completed by
examining all the statements in Assumption 1:
E.6.1

Step 1:

AGT2-QL can be expressed as the stochastic recursion in (11) with




 

Î¸1
âˆ’D Î³DP Î Î¸2
Î¸1
DR
f
:=
+
.
Î¸2
Î²D
âˆ’Î²D
Î¸2
0
Moreover, f is globally Lipschitz continuous according to Lemma 7.
To prove the first statement of Assumption 1, we note that


cÎ¸1


f
cÎ¸2
Î¸1
fâˆž
= lim
Î¸2
câ†’âˆž
c
24

Figure 6: Trajectories of the original O.D.E. model of AGT2-QL and the corresponding upper and
lower comparison systems (QB
t part). The solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.

= lim

câ†’âˆž


= lim
câ†’âˆž

âˆ’D
=
Î²D

âˆ’D
Î²D

Î³DP Î cÎ¸2
âˆ’Î²D



cÎ¸1
cÎ¸2




+

DR
0



c

Î³DP Î cÎ¸2
Î¸1
âˆ’Î²D
Î¸2


Î³DP Î Î¸2
Î¸1
âˆ’Î²D
Î¸2

âˆ’D
Î²D

where the last equality is due to the homogeneity of the policy, Ï€cÎ¸2 (s) = arg maxaâˆˆA cÎ¸2 (s, a) =
arg maxaâˆˆA Î¸2 (s, a), where Î¸2 (s, a) denotes the entry in the parameter vector Î¸2 corresponding to
the state-action pair (s, a) âˆˆ S Ã— A.
E.6.2

Step 2:

Let us consider the system



 
d Î¸1,t
âˆ’D
Î¸1,t
= fâˆž
=
Î¸2,t
Î²D
dt Î¸2,t

Î³DP Î Î¸2,t
âˆ’Î²D



Î¸1,t
Î¸2,t


.

Its global asymptotic stability around the origin can be easily proved following similar lines as in the
proof of the upper comparison system in Lemma 12.
E.6.3

Step 3:

According to Theorem

5, the ODE
model of AGT2-QL is globally asymptotically stable around the
Î¸1
Qâˆ—
optimal solution
=
.
Î¸2
Qâˆ—
25

E.6.4

Step 4:

Next, we prove the remaining parts. Recall that AGT2-QL is expressed as
QÌ„k+1 = QÌ„k + Î±k {f (QÌ„k ) + Îµk+1 }
 A 
Qk
where QÌ„k :=
and
QB
k

 A  

T
âˆ’(es âŠ— ea )(es âŠ— ea )
Î³(es âŠ— ea )eTsâ€² Î QB
Q
(es âŠ— ea )r(s, a, râ€² )
Îµk+1 :=
+
T
T
0
QB
Î²(es âŠ— ea )(es âŠ— ea )
âˆ’Î²(es âŠ— ea )(es âŠ— ea )

 A  

âˆ’D Î³DP Î QB
Q
R
âˆ’
âˆ’
0
Î²D
âˆ’Î²D
QB
and
 A 

 A  

âˆ’D Î³DP Î QB
Q
Q
DR
f
:=
+
,
0
Î²D
âˆ’Î²D
QB
QB
To proceed, let us define the history
Gk := (ÎµÌ„k , ÎµÌ„kâˆ’1 , . . . , ÎµÌ„1 , QÌ„k , QÌ„kâˆ’1 , . . . , QÌ„0 )
Pk
Moreover, let us define the corresponding process (Mk )âˆž
k=0 with Mk :=
i=1 Îµi . Then, we can
prove that (Mk )âˆž
k=0 is Martingale. To do so, we can easily prove E[Îµk+1 |Gk ] = 0 by

 A 

âˆ’(es âŠ— ea )(es âŠ— ea )T
Î³ âˆ’ (es âŠ— ea )eTsâ€² Î QB
Q
E[Îµk+1 |Gk ] =E
Gk
T
QB
Î²(es âŠ— ea )(es âŠ— ea )T âˆ’Î²(es âŠ— ea )(es âŠ— ea )

 A 
âˆ’D Î³DP Î QB
Q
âˆ’
Î²D
âˆ’Î²D
QB
=0
Using this identity, we have
E[Mk+1 |Gk ] =E

" k+1
X

#
Îµi Gk = E[Îµk+1 |Gk ] + E

i=1

=E

" k
X

" k
X

#
Îµi Gk

i=1

#
Îµi Gk =

i=1

k
X

Îµ i = Mk .

i=1

Therefore, (Mk )âˆž
k=0 is a Martingale sequence, and Îµk+1 = Mk+1 âˆ’ Mk is a Martingale difference.
Moreover, it can be easily proved that the fourth condition of Assumption 1 is satisfied by algebraic
calculations. Therefore, the fourth condition is met.

F

Convergence of SGT2-QL

In this section, we study convergence of SGT2-QL. The full algorithm is described in Algorithm 4.
Similar to ATG2-GL, we analyze the convergence of SGT2-QL based on the same switching system
Algorithm 4 SGT2-QL
B
1: Initialize QA
0 and Q0 randomly.
2: for iteration k = 0, 1, . . . do
3:
Sample (s, a)
4:
Sample sâ€² âˆ¼ P (Â·|s, a) and r(s, a, sâ€² )
A
â€²
B â€²
A
5:
Update QA
k+1 (s, a) = Qk (s, a) + Î±k {r(s, a, s ) + Î³ maxaâˆˆA Qk (s , a) âˆ’ Qk (s, a)} +
B
A
Î±k Î²(Q (s, a)k âˆ’ Qk (s, a))
B
â€²
A â€²
B
6:
Update QB
k+1 (s, a) = Qk (s, a) + Î±k {r(s, a, s ) + Î³ maxaâˆˆA Qk (s , a) âˆ’ Qk (s, a)} +
A
B
Î±k Î²(Q (s, a)k âˆ’ Qk (s, a))
7: end for
approach [15].
26

F.1

Original system

As before, our first goal is to derive an ODE model of SGT2-QL to apply the Borkar and Meyn
theorem Lemma 4. To begin with, the update rule of SGT2-QL in Algorithm 4 can be rewritten as
follows:
n
T

A
QA
k+1 =Qk + Î±k (ea âŠ— es )(ea âŠ— es ) R

T A
+ Î³(ea âŠ— es )(esâ€² )T maxQB
k (Â·, a) âˆ’ (ea âŠ— es )(ea âŠ— es ) Qk
aâˆˆA
o
T
T A
+Î²((ea âŠ— es )(ea âŠ— es ) QB
âˆ’
(e
âŠ—
e
)(e
âŠ—
e
)
Q
)
,
a
s
a
s
k
k
n
T
B
QB
k+1 =Qk + Î±k (ea âŠ— es )(ea âŠ— es ) R
T B
+ Î³(ea âŠ— es )(esâ€² )T maxQA
k (Â·, a) âˆ’ (ea âŠ— es )(ea âŠ— es ) Qk
aâˆˆA
o
T
T B
+Î²((ea âŠ— es )(ea âŠ— es ) QA
k âˆ’ (ea âŠ— es )(ea âŠ— es ) Qk ) ,

where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. The above update can be further expressed
as
A
A
B
A
A
QA
QB
k+1 = Qk + Î±k {DR + Î³DP Î QB
k âˆ’ DQk + Î²(Qk âˆ’ Qk ) + Îµk+1 }
k
B
B
A
B
B
QB
QA
k+1 = Qk + Î±k {DR + Î³DP Î QB
k âˆ’ DQk + Î²(Qk âˆ’ Qk ) + Îµk+1 }
k

where
T
T
ÎµA
QB
k+1 =(ea âŠ— es )(ea âŠ— es ) R + Î³(ea âŠ— es )(esâ€² ) Î QB
k
k
T
B
B
âˆ’ (ea âŠ— es )(ea âŠ— es )T QA
k + (ea âŠ— es )(ea âŠ— es ) Î²(Qk âˆ’ Qk )
A
B
a
âˆ’ (DR + Î³DP Î QB
QB
k âˆ’ DQk + Î²(Qk âˆ’ Qk ))
k
T
T
ÎµA
QB
k
k+1 =(ea âŠ— es )(ea âŠ— es ) R + Î³(ea âŠ— es )(esâ€² ) Î QB
k
T
B
B
âˆ’ (ea âŠ— es )(ea âŠ— es )T QA
k + (ea âŠ— es )(ea âŠ— es ) Î²(Qk âˆ’ Qk )
A
B
a
âˆ’ (DR + Î³DP Î QB
QB
k âˆ’ DQk + Î²(Qk âˆ’ Qk ))
k

As discussed in Appendix C.3, the convergence of SGT3-QL can be analyzed by evaluating the
stability of the corresponding continuous-time ODE model
 A  
 A  

 A 
âˆ’(1 + Î²)D
Î³DP Î QB
+ Î²D
d
Qt
Qt
DR
Q0
t
=
+
,
âˆˆ R2|S||A| .
B
B
Î³DP Î QA
+
Î²D
âˆ’(1
+
Î²)D
DR
Q
Q
dt QB
t
t
0
t
(18)
Using the Bellman equation (Î³DP Î Qâˆ— âˆ’ D)Qâˆ— + DR = 0, the above expressions can be rewritten
by
 A
 
 A

âˆ’(1 + Î²)D
Î³DP Î QB
+ Î²D
d
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
t
=
âˆ—
âˆ—
Î³DP Î QA
+ Î²D
âˆ’(1 + Î²)D
QB
dt QB
t âˆ’Q
t âˆ’Q
t




âˆ—
Î³DP (Î QB
âˆ’ Î Qâˆ— )Qâˆ—
QA
0 âˆ’Q
t
+
,
= z âˆˆ R2|S||A| . (19)
âˆ—
B
âˆ—
âˆ—
Î³DP (Î QA
âˆ’
Î 
)Q
Q
âˆ’
Q
Q
0
t
The above system is a linear switching system. More precisely, let Î˜ be the set of all deterministic
policies, let us define a one-to-one mapping Ï† : Î˜ â†’ {1, 2, . . . , |Î˜ Ã— Î˜|} from two deterministic
policies (Ï€A , Ï€B ) âˆˆ Î˜ Ã— Î˜ to an integer in {1, 2, . . . , |Î˜ Ã— Î˜|}, and define


âˆ’(1 + Î²)D
Î³DP Î Ï€B + Î²D
Ai =
âˆˆ R2|SÃ—A|Ã—2|SÃ—A| ,
Ï€A
Î³DP Î  + Î²D
âˆ’(1 + Î²)D


âˆ—
Î³DP (Î Ï€B âˆ’ Î Ï€ )Qâˆ—
âˆ—
bi =
âˆˆ R2|SÃ—A|
Î³DP (Î Ï€A âˆ’ Î Ï€ )Qâˆ—
27

for all i = Ï†(Ï€A , Ï€B ) and (Ï€A , Ï€B ) âˆˆ Î˜ Ã— Î˜. Then, the above ODE can be written by the affine
switching system
d
xt = AÏƒ(xt ) xt + bÏƒ(xt ) , x0 = z âˆˆ R|S||A| ,
dt
where
 A

Qt âˆ’ Qâˆ—
xt :=
âˆ—
QB
t âˆ’Q
is the state, Ïƒ : R|S||A| â†’ {1, 2, . . . , |Î˜ Ã— Î˜|} is a state-feedback switching policy defined by
Ïƒ(xt ) := Ïˆ(Ï€QA
, Ï€QB
), and Ï€Q (s) = arg maxaâˆˆA Q(s, a).
t
t
For convenience of analysis, let us define the following vector functions:


f1 (x1 , x2 )
f (x1 , x2 ) :=
f2 (x1 , x2 )


 

âˆ’(1 + Î²)D
Î³DP Î x2 +Qâˆ— + Î²D
x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
:=
+
.
Î³DP Î x1 +Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
Î³DP (Î x1 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
Then, (19) can be written by the system

 

d xt,1
f1 (xt,1 , xt,2 )
=
,
f2 (xt,1 , xt,2 )
dt xt,2
where


QA
âˆ’ Qâˆ—
0
z0 =
âˆ— ,
QB
0 âˆ’Q


âˆ€t â‰¥ 0,


  A
xt,1
Qt âˆ’ Qâˆ—
=
âˆ—
xt,2
QB
t âˆ’Q
is the state vector. Next, we present several useful lemmas that aid in the analysis of the ODE model.
Lemma 17. The function f can be represented by


âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
f (x1 , x2 ) =
.
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1 +Qâˆ— (x1 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—


Proof. The proof follows from the following algebraic manipulations:


 

âˆ’(1 + Î²)D
Î³DP Î x2 +Qâˆ— + Î²D
x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
f (x1 , x2 ) =
+
Î³DP Î x1 +Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
Î³DP (Î x1 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—


âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2 +Qâˆ— x2 + Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
=
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1 +Qâˆ— x1 + Î³DP (Î x1 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—


âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
=
.
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1 +Qâˆ— (x1 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
This completes the proof.
Lemma 18. f is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for f1 and f2 , separately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element
of âˆ†x1 is zero. For f1 , using Lemma 17, we have
eTi f1 (x1 + âˆ†x1 , x2 + âˆ†x2 )
= âˆ’ (1 + Î²)eTi D(x1 + âˆ†x1 ) + Î²D(x2 + âˆ†x2 )
+ Î³eTi DP Î x2 +âˆ†x2 +Qâˆ— (x2 + âˆ†x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
= âˆ’ (1 + Î²)eTi Dx1 + Î²D(x2 + âˆ†x2 ) + Î³eTi DP Î x2 +âˆ†x2 +Qâˆ— (x2 + âˆ†x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
â‰¥ âˆ’ (1 + Î²)eTi Dx1 + Î²Dx2 + Î³eTi DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³eTi DP Î Qâˆ— Qâˆ—
=eTi f1 (x1 , x2 ),
where the second line is due to âˆ’(1 + Î²)eTi Dâˆ†x1 = 0. Similarly, assuming that âˆ†x1 âˆˆ R|S|||A|
and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eTi f2 (x1 + âˆ†x1 , x2 + âˆ†x2 ) â‰¥ eTi f2 (x1 , x2 ),
by symmetry. Therefore, h is quasi-monotone increasing.
28

Lemma 19. f is globally Lipshcitz continuous.
Proof. From Lemma 17, one has



âˆ’(1 + Î²)D
Î²D
x1
f (x1 , x2 ) =
Î²D
âˆ’(1 + Î²)D
x2




Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— )
âˆ’Î³DP Î Qâˆ— Qâˆ—
+
+
.
Î³DP Î x1 +Qâˆ— (x1 + Qâˆ— )
âˆ’Î³DP Î Qâˆ— Qâˆ—
sing this relation, we derive the following sequence of inequalities:
âˆ¥f (x1 , x2 ) âˆ’ f (y1 , y2 )âˆ¥âˆž


 


âˆ’(1 + Î²)D
Î²D
x1
âˆ’(1 + Î²)D
Î²D
y1
âˆ’
â‰¤
Î²D
âˆ’(1 + Î²)D
x2
Î²D
âˆ’(1 + Î²)D
y2
âˆž
+ âˆ¥Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î y2 +Qâˆ— (y2 + Qâˆ— )âˆ¥âˆž
+ âˆ¥Î³DP Î x1 +Qâˆ— (x1 + Qâˆ— ) âˆ’ Î³DP Î y1 +Qâˆ— (y1 + Qâˆ— )âˆ¥âˆž

 



x1
y1
âˆ’(1 + Î²)D
Î²D
âˆ’
â‰¤
x2
y2
Î²D
âˆ’(1 + Î²)D
âˆž
âˆž
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î y2 +Qâˆ— (y2 + Qâˆ— )âˆ¥âˆž
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥Î x1 +Qâˆ— (x1 + Qâˆ— ) âˆ’ Î y1 +Qâˆ— (y1 + Qâˆ— )âˆ¥âˆž



 

âˆ’(1 + Î²)D
Î²D
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥x1 âˆ’ y1 âˆ¥âˆž
Î²D
âˆ’(1 + Î²)D
x2
y2
âˆž
âˆž
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥x2 âˆ’ y2 âˆ¥âˆž



 


 

âˆ’(1 + Î²)D
Î²D
x1
y1
x1
y1
â‰¤
âˆ’
+ 2âˆ¥Î³DP âˆ¥âˆž
âˆ’
Î²D
âˆ’(1 + Î²)D
x2
y2
x2
y2
âˆž
âˆž
âˆž
indicating that f is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
F.2

Upper comparison system

We consider the following system which is called the upper comparison system:

 
  A,u
 A,u
âˆ’(1 + Î²)D
Î³DP Î QB,u âˆ’Qâˆ— + Î²D
d
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
t
,
=
Î³DP Î QA,u âˆ’Qâˆ— + Î²D
âˆ’(1 + Î²)D
dt QB,u
QB,u
âˆ’ Qâˆ—
âˆ’ Qâˆ—
t
t
t
 A,u
  A

Q0 âˆ’ Qâˆ—
Q0 âˆ’ Qâˆ—
>
âˆˆ R2|S||A| .
(20)
B
âˆ—
âˆ—
Q
âˆ’
Q
QB,u
âˆ’
Q
0
0
Defining the following vector function:



h1 (x1 , x2 )
âˆ’(1 + Î²)D
h(x1 , x2 ) :=
:=
h2 (x1 , x2 )
Î³DP Î x1 + Î²D

Î³DP Î x2 + Î²D
âˆ’(1 + Î²)D

the upper comparison system can be written by the system

 

 A

d xt,1
h (x , x )
Q0 âˆ’ Qâˆ—
= 1 t,1 t,2 , x0 >
âˆ— ,
h2 (xt,1 , xt,2 )
QB
dt xt,2
0 âˆ’Q
where

  A,u

xt,1
Qt âˆ’ Qâˆ—
=
xt,2
QB,u
âˆ’ Qâˆ—
t



x1
x2



âˆ€t â‰¥ 0

is the state vector. Similar to the original ODE model, we derive several useful properties of the upper
comparison system.
Lemma 20. h is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for h1 and h2 , separately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element
29

of âˆ†x1 is zero. For h1 , we have
eTi h1 (x1 + âˆ†x1 , x2 + âˆ†x2 )
= âˆ’ (1 + Î²)eTi D(x1 + âˆ†x1 ) + Î²D(x2 + âˆ†x2 ) + Î³eTi DP Î x2 +âˆ†x2 (x2 + âˆ†x2 )
= âˆ’ (1 + Î²)eTi Dx1 + Î²D(x2 + âˆ†x2 ) + Î³eTi DP Î x2 +âˆ†x2 (x2 + âˆ†x2 )
â‰¥ âˆ’ (1 + Î²)eTi Dx1 + Î²D(x2 ) + Î³eTi DP Î x2 x2
=eTi h1 (x1 , x2 ),
where the second line is due to âˆ’eTi Dâˆ†x1 = 0. Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eTi h2 (x1 + âˆ†x1 , x2 + âˆ†x2 ) â‰¥ eTi h2 (x1 , x2 ),
by symmetry. Therefore, h is quasi-monotone increasing.
Lemma 21. h is globally Lipshcitz continuous.
Proof. We complete the proof through the inequalities
âˆ¥h(x1 , x2 ) âˆ’ h(y1 , y2 )âˆ¥âˆž


 


âˆ’D Î³DP Î x2
x1
âˆ’D Î³DP Î y2
y1
âˆ’
=
Î²D
âˆ’Î²D
x2
Î²D
âˆ’Î²D
y2
âˆž


 


âˆ’D
0
x1
âˆ’D
0
y1
â‰¤
âˆ’
+ âˆ¥Î³DP Î x2 x1 âˆ’ Î³DP Î y2 y2 âˆ¥âˆž
Î²D âˆ’Î²D
x2
Î²D âˆ’Î²D
y2
âˆž



 

âˆ’D
0
x1
y1
â‰¤
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž âˆ¥x1 âˆ’ y2 âˆ¥âˆž
Î²D âˆ’Î²D
x
y2
2

 

 
 âˆž

 âˆž 
x1
y1
x1
y1
âˆ’D
0
âˆ’
+ âˆ¥Î³DP âˆ¥âˆž
âˆ’
â‰¤
x2
y2
x2
y2
Î²D âˆ’Î²D
âˆž
âˆž
âˆž
indicating that h is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 22. h(x1 , x2 ) â‰¥ f (x1 , x2 ) for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| , where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. The proof follows from the inequalities


 

âˆ’(1 + Î²)D
Î³DP Î x2 +Qâˆ— + Î²D
x1
Î³DP (Î x2 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—
f (x1 , x2 ) =
+
Î³DP Î x1 +Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
Î³DP (Î x1 +Qâˆ— âˆ’ Î Qâˆ— )Qâˆ—



âˆ’(1 + Î²)D
Î³DP Î x2 +Qâˆ— + Î²D
x1
â‰¤
Î³DP Î x1 +Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2



âˆ’(1 + Î²)D
Î³DP Î x2 + Î²D
x1
â‰¤
Î³DP Î x1 + Î²D
âˆ’(1 + Î²)D
x2
=g(x1 , x2 )
for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| .
Based on the previous lemmas, we are now ready to prove that the solution of the upper comparison
system indeed upper-bounds the solution of the original system, which is the reason why (20) is
named as such.
Lemma 23. We have
  A

QA,u
âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
t
â‰¥
,
âˆ—
QB
QB,u
âˆ’ Qâˆ—
t âˆ’Q
t
where â€˜â‰¥â€™ denotes the element-wise inequality.


âˆ€t â‰¥ 0,

Proof. The desired conclusion is obtained by Lemma 2 with Lemmas 19 to 22.
30

Next, we prove that the upper comparison system (20) is globally asymptotically stable at the origin.
Lemma 24. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium point
of the upper comparison system (20).
such that Ïƒ = Ïˆ(Ï€QB
). Then, for
Proof. For notational convenience, we define Î Ïƒ , Ïƒ âˆˆ M as Î QB
t
t
the upper comparison switching system, we apply Lemma 3 with


âˆ’(1 + Î²)D
Î³DP Î Ïƒ1 + Î²D
AÏƒ1 ,Ïƒ2 =
Î³DP Î Ïƒ2 + Î²D
âˆ’(1 + Î²)D
and

I
L=
0


0
,
I

which satisfies
LAÏƒ1 ,Ïƒ2 = AÌ„Ïƒ1 ,Ïƒ2 L
with


AÌ„Ïƒ1 ,Ïƒ2 =

âˆ’(1 + Î²)D
Î³DP Î Ïƒ2 + Î²D

Î³DP Î Ïƒ1 + Î²D
âˆ’(1 + Î²)D



To check the strictly negative row dominating diagonal condition, for i âˆˆ {1, 2, . . . , |S||A|}, we have
X
X
[AÌ„Ïƒ1 ,Ïƒ2 ]ii +
|[AÌ„Ïƒ1 ,Ïƒ2 ]ij | = âˆ’ (1 + Î²)[D]ii + Î³[D]ii
|[P Î Ïƒ ]ij | + Î²[D]ii
jâˆˆ{1,2,...,n}\{i}

jâˆˆ{1,2,...,n}\{i}

â‰¤ âˆ’ (1 + Î²)[D]ii + (Î³ + Î²)[D]ii
=(Î³ âˆ’ 1)[D]ii
<0
For i âˆˆ {|S||A| + 1, |S||A| + 2, . . . , 2|S||A|}, by symmetry, we also have the same result
X
[AÌ„Ïƒ1 ,Ïƒ2 ]ii +
|[AÌ„Ïƒ1 ,Ïƒ2 ]ij | < 0
jâˆˆ{1,2,...,n}\{i}

for any Î² > 0. Therefore, the strictly negative row dominating diagonal condition is satisfied.
By Lemma 3, the origin of the upper comparison system is globally asymptotically stable.
F.3

Lower comparison system

Let us consider the so-called lower comparison system
 A,l
 
  A,l

d
âˆ’(1 + Î²)D
Î³DP Î Ï€Qâˆ— + Î²D
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
=
,
Î³DP Î Ï€Qâˆ— + Î²D
âˆ’(1 + Î²)D
dt QB,l
âˆ’ Qâˆ—
QB,l
âˆ’ Qâˆ—
t
t
 A,l
  A

Q0 âˆ’ Qâˆ—
Q0 âˆ’ Qâˆ—
<
âˆˆ R2|S||A| ,
âˆ—
B
âˆ—
âˆ’
Q
Q
âˆ’
Q
QB,l
0
0

(21)

We note that the lower comparison system is simply a linear system. Defining the vector function

 


âˆ’(1 + Î²)D
Î³DP Î Ï€Qâˆ— + Î²D
x1
g1 (x1 , x2 )
g(x1 , x2 ) :=
=
,
g2 (x1 , x2 )
Î³DP Î Ï€Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
the lower comparison system can be written by


 
d xt,1
g1 (xt,1 , xt,2 )
=
,
g2 (xt,1 , xt,2 )
dt xt,2


âˆ’ Qâˆ—
QA
0
x0 <
âˆ— ,
QB
0 âˆ’Q
for all t â‰¥ 0. Below, we present several key properties of the lower comparison system.


Lemma 25. g is globally Lipschitz continuous.
Proof. It is straightforward from the linearity. In particular, we complete the proof through the
inequalities



 

âˆ’(1 + Î²)D
Î³DP Î Ï€Qâˆ— + Î²D
x1
y1
âˆ¥g(x1 , x2 ) âˆ’ g(y1 , y2 )âˆ¥âˆž â‰¤
âˆ’
Î³DP Î Ï€Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
y2
âˆž
âˆž
31

indicating that g is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 26. f (x1 , x2 ) â‰¥ g(x1 , x2 ) for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| , where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. The proof follows directly from the inequalities


âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2 +Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
f (x1 , x2 ) =
âˆ—
âˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1 +Qâˆ— (x1 + Q ) âˆ’ Î³DP Î Qâˆ— Q


âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î Qâˆ— (x2 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—
â‰¥
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î Qâˆ— (x1 + Qâˆ— ) âˆ’ Î³DP Î Qâˆ— Qâˆ—



âˆ’(1 + Î²)D
Î³DP Î Ï€Qâˆ— + Î²D
x1
=
Î³DP Î Ï€Qâˆ— + Î²D
âˆ’(1 + Î²)D
x2
=g(x1 , x2 )
for all (x1 , x2 ) âˆˆ R|S||A| Ã— R|S||A| . This completes the proof.
Similar to the upper comparison system, we prove that the solution of the lower comparison system
indeed provides a lower bound for the solution of the original system.
Lemma 27. We have
 A
  A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â‰¥
, âˆ€t â‰¥ 0,
âˆ—
QB
QB,l
âˆ’ Qâˆ—
t âˆ’Q
t
where â€˜â‰¥â€™ denotes the element-wise inequality.
Proof. The desired conclusion is obtained by Lemma 2with Lemmas 18, 19, 25 and 26.
Moreover, the next lemma proves that the lower comparison system is also globally asymptotically
stable at the origin.
Lemma 28. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium point
of the lower comparison system (21).
Proof. For the proof, one can apply the same procedure as in the proof of the upper comparison
system.
So far, we have established several key properties of the upper and lower comparison systems,
including their global asymptotic stability. In the next subsection, we prove the global asymptotic
stability of the original system based on these results.
F.4

Stability of the original system

We establish the global asymptotic stability of (19).
Theorem 6. For any Î² > 0, the origin is theunique globally asymptotically stable equilibrium
Qâˆ—
point of the original system (19). Equivalently,
is the unique globally asymptotically stable
Qâˆ—
equilibrium point of the original system (18).
Proof. By Lemmas 23 and 27, we have
 A,u
  A
  A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â‰¥
â‰¥
, âˆ€t â‰¥ 0.
âˆ—
QB
QB,u
âˆ’ Qâˆ—
QB,l
âˆ’ Qâˆ—
t âˆ’Q
t
t
Moreover, by Lemma 28 and Lemma 24, we have
 A,u

 A,l

Qt âˆ’ Qâˆ—
Qt âˆ’ Qâˆ—
â†’
0,
â†’0
QB,u
âˆ’ Qâˆ—
QB,l
âˆ’ Qâˆ—
t
t
as t â†’ âˆž. Therefore, the state of the original system also asymptotically converges to the origin.
This completes the proof.
32

F.5

Numerical example

In this subsection, we provide a simple example to illustrate the validity of the properties of the
upper and lower comparison systems established in the previous sections. Let us consider the MDP
previously considered for AGT2-QL with S = {1, 2}, A = {1, 2}, Î³ = 0.9,




0.2 0.8
0.5 0.5
P1 =
, P2 =
,
0.3 0.7
0.7 0.3
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2, P[a = 2|s = 1] = 0.8,
P[a = 1|s = 2] = 0.7, P[a = 2|s = 2] = 0.3,
and the expected reward vectors
 
 
3
2
R1 =
, R2 =
1
1
Solutions of the O.D.E. model of SGT2-QL and the upper and lower comparison systems are depicted
B
in Figure 7 for QA
t part and Figure 8 for Qt part.

Figure 7: Trajectories of the O.D.E. model of SGT2-QL and the corresponding upper and lower
comparison systems (QA
t part). he solution of the ODE model (black line) is upper and lower bounded
by the upper and lower comparison systems, respectively (red and blue lines, respectively). This
result provides numerical evidence that the bounding rules hold.
As before, the simulation study empirically proves that the ODE model associated with SGT2-QL is
asymptotically stable. Moreover, they illustrate that the solutions of the upper and lower comparison
systems bound the solution of the original system, as established by the theory.
So far, we have examined key properties of the ODE model of SGT2-QL and proved its global
asymptotic stability, which is essential for establishing the convergence of SGT2-QL. In the next
subsection, we will analyze its convergence in detail.
33

Figure 8: Trajectories of the original O.D.E. model of SGT2-QL and the corresponding upper and
lower comparison systems (QB
t part). he solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.

F.6

Convergence of SGT2-QL

In this subsection, we apply the Borkar and Meyn theorem, Lemma 4, to prove the convergence of
SGT2-QL in Theorem 4. First of all, note that the system in (15) corresponds to the ODE model
in Assumption 1. The proof is completed by examining all the statements in Assumption 1:
F.6.1

Step 1:

SGT2-QL can be expressed as the stochastic recursion in (11) with




 

Î¸1
âˆ’(1 + Î²)D
Î³DP Î Î¸2 + Î²D
Î¸1
DR
f
:=
+
.
Î¸2
Î³DP Î Î¸1 + Î²D
âˆ’(1 + Î²)D
Î¸2
DR
Moreover, f is globally Lipschitz continuous according to Lemma 19.
To prove the first statement of Assumption 1, we note that


cÎ¸1


f
cÎ¸2
Î¸1
fâˆž
= lim
Î¸2
câ†’âˆž
c


 

âˆ’(1 + Î²)D
Î³DP Î cÎ¸2 + Î²D
Î¸1
DR
+
Î³DP Î cÎ¸1 + Î²D
âˆ’(1 + Î²)D
Î¸2
DR
= lim
câ†’âˆž
c



âˆ’(1 + Î²)D
Î³DP Î cÎ¸2 + Î²D
Î¸1
= lim
Î³DP Î cÎ¸1 + Î²D
âˆ’(1 + Î²)D
Î¸2
câ†’âˆž
34


=

âˆ’(1 + Î²)D
Î³DP Î Î¸1 + Î²D

Î³DP Î Î¸2 + Î²D
âˆ’(1 + Î²)D



Î¸1
Î¸2



where the last equality is due to the homogeneity of the policy, Ï€cÎ¸i (s) = arg maxaâˆˆA cÎ¸i (s, a) =
arg maxaâˆˆA Î¸i (s, a) for i âˆˆ {1, 2}, where Î¸i (s, a) denotes the entry in the parameter vector Î¸i
corresponding to the state-action pair (s, a) âˆˆ S Ã— A.
F.6.2

Step 2:

Let us consider the system



 


d Î¸1,t
âˆ’(1 + Î²)D
Î³DP Î Î¸2,t + Î²D
Î¸1,t
Î¸1,t
= fâˆž
=
.
Î¸2,t
Î³DP Î Î¸1,t + Î²D
âˆ’(1 + Î²)D
Î¸2,t
dt Î¸2,t
Its global asymptotic stability around the origin can be easily proved following similar lines as in the
proof of the upper comparison system in Lemma 12.
F.6.3

Step 3:

According
5, the ODE model of SGT2-QL is globally asymptotically stable around

 to Theorem

Î¸1
Qâˆ—
=
.
Î¸2
Qâˆ—
F.6.4

Step 4:

Next, we prove the remaining parts. Recall that SGT2-QL is expressed as
QÌ„k+1 = QÌ„k + Î±k {f (QÌ„k ) + Îµk+1 }
 A 
Qk
where QÌ„k :=
and
QB
k


T
T 
âˆ’(1 + Î²)(es âŠ— ea )(es âŠ— ea )
Î³(es âŠ— ea )eTsâ€² Î QB + Î²(es âŠ— ea )(es âŠ— ea )
QA
Îµk+1 :=
T
T
QB
Î³(es âŠ— ea )eTsâ€² Î QA + Î²(es âŠ— ea )(es âŠ— ea )
âˆ’(1 + Î²)(es âŠ— ea )(es âŠ— ea )








T
âˆ’(1 + Î²)D
Î³DP Î QB + Î²D
(es âŠ— ea )(es âŠ— ea ) r(s, a, sâ€² )
QA
R
+
âˆ’
+
T
R
Î³DP Î QA + Î²D
âˆ’(1 + Î²)D
QB
(es âŠ— ea )(es âŠ— ea ) r(s, a, sâ€² )
and
 A  


 A 
âˆ’(1 + Î²)D
Î³DP Î QB + Î²D
Q
DR
Q
.
:=
+
f
DR
Î³DP Î QA + Î²D
âˆ’(1 + Î²)D
QB
QB
To proceed, let us define the history
Gk := (ÎµÌ„k , ÎµÌ„kâˆ’1 , . . . , ÎµÌ„1 , QÌ„k , QÌ„kâˆ’1 , . . . , QÌ„0 )
Pk
Moreover, let us define the corresponding process (Mk )âˆž
k=0 with Mk :=
i=1 Îµi . Then, we can
prove that (Mk )âˆž
k=0 is Martingale. To do so, we can easily prove E[Îµk+1 |Gk ] = 0. Using this identity,
we have
" k+1
#
" k
#
X
X
E[Mk+1 |Gk ] =E
Îµi Gk = E[Îµk+1 |Gk ] + E
Îµi Gk
i=1

=E

" k
X

i=1

#
Îµi Gk =

i=1

k
X

Îµ i = Mk .

i=1

Therefore, (Mk )âˆž
k=0 is a Martingale sequence, and Îµk+1 = Mk+1 âˆ’ Mk is a Martingale difference.
Moreover, it can be easily proved that the fourth condition of Assumption 1 is satisfied by algebraic
calculations. Therefore, the fourth condition is met.

G

Experiments on AGT2-QL and SGT2-QL

In this section, we empirically demonstrate the convergence of the proposed AGT2-QL and SGT2-QL
algorithms.
35

First of all, we consider the simple MDP considered in the previous sections with S = {1, 2},
A = {1, 2}, Î³ = 0.9,




0.2 0.8
0.5 0.5
P1 =
, P2 =
,
0.3 0.7
0.7 0.3
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2,
P[a = 1|s = 2] = 0.7,
and the expected reward vectors
 
3
R1 =
,
1

P[a = 2|s = 1] = 0.8,
P[a = 2|s = 2] = 0.3,
 
2
R2 =
.
1

âˆ—
B
âˆ—
Simulated errors |QA
k (s, a) âˆ’ Q (s, a)|, |Qk (s, a) âˆ’ Q (s, a)|, (s, a) âˆˆ S Ã— A of AGT2-QL are
depicted in Figure 9 for the QA
part
and
in
Figure
10
for
the QB
k
k part. The results are presented for
80
different weight values Î² âˆˆ {0.01, 0.05, 0.1, 0.2, 0.5} and the diminishing step-size Î±k = 200+k
.
âˆ—
The results demonstrate the convergence of AGT2-QL to the optimal Q over time. Moreover, the
results illustrate the convergence trends for varying weight Î². In particular, the larger the Î², the faster
the convergence.

Figure 9: Simulated errors of AGT2-QL (QA
k part)
âˆ—
B
Similarly, for the same environment, simulated errors |QA
k (s, a) âˆ’ Q (s, a)|, |Qk (s, a) âˆ’
âˆ—
A
Q (s, a)|, (s, a) âˆˆ S Ã— A of AGT2-QL are depicted in Figure 11 for the Qk part and in Figure 12 for the QB
k part with different weight values Î² âˆˆ {0.01, 0.05, 0.1, 0.2, 0.5}. The results
also demonstrate the convergence, but with different convergence trends for varying weight Î². In
particular, the error evolutions show that SGT2-QL generally converges faster than AGT2-QL, and
the convergence speeds are less sensitive to Î².

36

Figure 10: Simulated errors of AGT2-QL (QB
k part)
Additionally, we conduct experiments using grid world environments, including Taxi, FrozenLake,
and CliffWalk, in OpenAI Gym. Figure 13 presents reward curves for the Taxi environment, which
shows trends similar to previous results: AGT2-QL exhibits better learning performance for higher
values of Î², whereas SGT2-QL is not significantly affected by the choice of Î².
Similarly,Figure 14 presents reward curves for the FrozenLake environment, whileFigure 15 shows
reward curves for the CliffWalk environment. All results demonstrate that the proposed approaches,
AGT2-QL and SGT2-QL, are valid and effectively learn the optimal policy. Moreover, the learning
speeds of AGT2-QL and SGT2-QL are comparable to that of standard Q-learning.

37

Figure 11: Simulated errors of SGT2-QL (QA
k part)

38

Figure 12: Simulated errors of SGT2-QL (QB
k part)

39

(a) Reward curves of AGT2-QL with (b) Reward curves of SGT2-QL with difdifferent Î²
ferent Î²

(c) Reward curves of AGT2-QL, SGT2QL, Q-learning, and double Q-learning

Figure 13: Comparison of simulated reward curves of AGT2-QL, SGT2-QL, Q-learning, double
Q-learning in Taxi environment

(a) Reward curves of AGT2-QL with (b) Reward curves of SGT2-QL with difdifferent Î²
ferent Î²

(c) Reward curves of AGT2-QL, SGT2QL, Q-learning, and double Q-learning

Figure 14: Comparison of reward curves of AGT2-QL, SGT2-QL, Q-learning, double Q-learning in
Frozenlake environment
40

(a)

(b)

(c)

Figure 15: Comparison of simulated reward curves of AGT2-QL, SGT2-QL, Q-learning, double
Q-learning in CliffWalk environment

41

H

Proof of Theorem 1

Let us assume that by minimizing the loss functions of AGT2-DQN, we can approximately minimize
the following expected loss function
h
i
2
L1 (Î¸1 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a))


Î²
2
L2 (Î¸2 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a)
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))
2
where U (S Ã— A) means the uniform distribution over the set S Ã— A. Let us suppose that the loss
functions are minimized with the error
L1 (Î¸1 ) â‰¤ Îµ, L2 (Î¸2 ) â‰¤ Îµ
To proceed further, note that using the law of iterated expectations, the expected loss functions can be
written by
h
ii
h
2
L1 (Î¸1 ) = E(s,a)âˆ¼U (SÃ—A) Esâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a)) s, a
and




Î²
2
L2 (Î¸2 ) = E(s,a)âˆ¼U (SÃ—A) Esâ€² âˆ¼P (Â·|s,a)
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a)) s, a
2

For convenience, let us define the Bellman operator
X
(T Q)(s, a) := R(s, a) + Î³
P (sâ€² |s, a)maxQ(sâ€² , a)
sâ€² âˆˆS

aâˆˆA

Using Jensenâ€™s inequality, we can prove that
h
2 i
L1 (Î¸1 ) â‰¥E(s,a)âˆ¼U (SÃ—A) Esâ€² âˆ¼P (Â·|s,a) [r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a)]
h
i
2
=E(s,a)âˆ¼U (SÃ—A) ((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))
1
2
((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))
|S||A|
for all (s, a) âˆˆ S Ã— A, which implies
p
|(T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a)| â‰¤ Îµ|S||A|, âˆ€(s, a) âˆˆ S Ã— A
Similarly, for the loss function L2 (Î¸2 ), we have


Î²
1 Î²
2
2
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) â‰¥
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))
L2 (Î¸2 ) = E(s,a)âˆ¼U (SÃ—A)
2
|S||A| 2
for all (s, a) âˆˆ S Ã— A, which implies
r
2Îµ
|QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
|S||A|, (s, a) âˆˆ S Ã— A.
(22)
Î²
â‰¥

Based on the above results, we will establish bounds on QÎ¸1 and QÎ¸2 in the sequel.
H.1

Bound on QÎ¸1

First of all, using the reverse triangle inequality leads to
||QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|| â‰¤
To proceed, we consider the two cases:

p

Îµ|S||A|,

(s, a) âˆˆ S Ã— A. (23)

Case 1: Let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| > |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|
Then, one can derive from (23)
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ âˆ¥T QÎ¸2 âˆ’ T Qâˆ— âˆ¥âˆž
42

â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ Î³âˆ¥QÎ¸2 âˆ’ Qâˆ— âˆ¥âˆž
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ Î³âˆ¥QÎ¸1 âˆ’ Qâˆ— âˆ¥âˆž âˆ’ Î³âˆ¥QÎ¸2 âˆ’ QÎ¸1 âˆ¥âˆž
r
2Îµ
âˆ—
âˆ—
â‰¥|QÎ¸1 (s, a) âˆ’ Q (s, a)| âˆ’ Î³âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž âˆ’ Î³
|S||A|,
Î²
where (22) is used in the last line. This leads to
r
p
2Îµ
|S||A|
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| â‰¤ Î³âˆ¥QÎ¸1 âˆ’ Qâˆ— âˆ¥âˆž + Îµ|S||A| + Î³
Î²
Taking the maximum on the left-hand side over (s, a) âˆˆ S Ã— A yields
r
p
2Îµ
âˆ—
âˆ—
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž â‰¤ Î³âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž + Îµ|S||A| + Î³
|S||A|
Î²
Arranging terms leads to
s
p
Îµ|S||A|
2Îµ|S||A|
Î³
âˆ—
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
Case 2: Let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| â‰¤ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|,
which leads to
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| â‰¤|(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ— )(s, a)|
â‰¤âˆ¥T QÎ¸2 âˆ’ T Qâˆ— âˆ¥âˆž
â‰¤Î³âˆ¥QÎ¸2 âˆ’ Qâˆ— âˆ¥âˆž
â‰¤Î³âˆ¥QÎ¸1 âˆ’ Qâˆ— âˆ¥âˆž + Î³âˆ¥QÎ¸2 âˆ’ QÎ¸1 âˆ¥âˆž
r
2Îµ
|S||A|.
â‰¤Î³âˆ¥QÎ¸1 âˆ’ Qâˆ— âˆ¥âˆž + Î³
Î²
where the last inequality uses (22). After simple manipulations, we have
r
Î³
2Îµ
âˆ—
|S||A|
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž â‰¤
1âˆ’Î³ Î²
Combining the two cases, one gets the following bound:
s
p
Îµ|S||A|
Î³
2Îµ|S||A|
âˆ—
âˆ¥QÎ¸1 âˆ’ Q âˆ¥âˆž â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
H.2

Bound on QÎ¸2

Next, using the reverse triangle inequality, we also have
p
Îµ|S||A|
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a) + Qâˆ— (s, a) âˆ’ QÎ¸2 (s, a)|
1âˆ’Î³
â‰¥||QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| âˆ’ |QÎ¸2 (s, a) âˆ’ Qâˆ— (s, a)||.
Now, we consider the two cases:
Case 1: Let us first consider the following case:
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| < |QÎ¸2 (s, a) âˆ’ Qâˆ— (s, a)|.
In this case, the inequality in (24) leads to
p

Îµ|S||A|
+ |QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)|
1âˆ’Î³
Applying the bound on |QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)|, we can derive
s
p
2
Îµ|S||A|
Î³
2Îµ|S||A|
|QÎ¸2 (s, a) âˆ’ Qâˆ— (s, a)| â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
|QÎ¸2 (s, a) âˆ’ Qâˆ— (s, a)| â‰¤

43

(24)

Now, taking the maximum on the left-hand side over (s, a) âˆˆ S Ã— A leads to
s
p
2 Îµ|S||A|
Î³
2Îµ|S||A|
âˆ—
âˆ¥QÎ¸2 âˆ’ Q âˆ¥âˆž â‰¤
+
.
1âˆ’Î³
1âˆ’Î³
Î²
Case 2: Next, let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)| â‰¥ |QÎ¸2 (s, a) âˆ’ Qâˆ— (s, a)|
In this case, the inequality in (24) results in
s
p
Îµ|S||A|
2Îµ|S||A|
Î³
âˆ—
âˆ—
+
|QÎ¸2 (s, a) âˆ’ Q (s, a)| â‰¤ |QÎ¸1 (s, a) âˆ’ Q (s, a)| â‰¤
1âˆ’Î³
1âˆ’Î³
Î²
where the last inequality is due to the bound on |QÎ¸1 (s, a) âˆ’ Qâˆ— (s, a)|. Taking the maximum on the
left-hand side over (s, a) âˆˆ S Ã— A leads to
s
p
Îµ|S||A|
Î³
2Îµ|S||A|
âˆ—
âˆ¥QÎ¸2 âˆ’ Q âˆ¥âˆž â‰¤
+
1âˆ’Î³
1âˆ’Î³
Î²
Combining the two cases, we have the desired conclusion.

I

Proof of Theorem 2

Let us assume that by minimizing the loss functions of SGT2-DQN, we can approximately minimize
the following expected loss function
"
2
â€²
â€²
L1 (Î¸1 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a)
r(s, a, s ) + Î³ max QÎ¸2 (s , a) âˆ’ QÎ¸1 (s, a)
aâˆˆA



Î²
2
+ (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))
2
h
2
L2 (Î¸2 ) =E(s,a)âˆ¼U (SÃ—A),sâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸1 (sâ€² , a) âˆ’ QÎ¸2 (s, a))

Î²
2
+ (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))
2
where U (S Ã— A) means the uniform distribution over the set S Ã— A. Suppose that the loss functions
are minimized with
L1 (Î¸1 ) â‰¤ Îµ, L2 (Î¸2 ) â‰¤ Îµ.
For convenience, let us define the Bellman operator
X
(T Q)(s, a) := R(s, a) + Î³
P (sâ€² |s, a) max Q(sâ€² , a)
sâ€² âˆˆS

aâˆˆA

Moreover, note that using the law of iterated expectations, the expected loss functions can be expressed
as
h
h
ii
L1 (Î¸1 ) =E(s,a)âˆ¼U (SÃ—A) Esâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a))


Î²
2
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) ,
+ E(s,a)âˆ¼U (SÃ—A)
2

2

and
h
h
ii
2
L2 (Î¸2 ) =E(s,a)âˆ¼U (SÃ—A) Esâ€² âˆ¼P (Â·|s,a) (r(s, a, sâ€² ) + Î³maxaâˆˆA QÎ¸1 (sâ€² , a) âˆ’ QÎ¸2 (s, a))


Î²
2
+ E(s,a)âˆ¼U (SÃ—A)
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))
2
Next, applying Jensenâ€™s inequality to L1 (Î¸1 ), we can prove that
"

2 #
L1 (Î¸1 ) â‰¥E(s,a)âˆ¼U (SÃ—A)
Esâ€² âˆ¼P (Â·|s,a) r(s, a, sâ€² ) + Î³ max QÎ¸2 (sâ€² , a) âˆ’ QÎ¸1 (s, a)
aâˆˆA

44



Î²
2
+ E(s,a)âˆ¼U (SÃ—A)
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))
2


=E(s,a)âˆ¼U (SÃ—A) ((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2


Î²
2
+ E(s,a)âˆ¼U (SÃ—A)
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))
2
1
2
((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))
â‰¥
|S||A|
1 Î²
2
+
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))
|S||A| 2
for all (s, a) âˆˆ S Ã— A, which implies
1 Î²
1
2
2
((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a)) â‰¤ Îµ,
(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)) â‰¤ Îµ,
|S||A|
|S||A| 2
and equivalently
s
p
2Îµ|S||A|
|(T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a)| â‰¤ Îµ|S||A|, |QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
Î²
Next, applying the same procedures as in the proof for AGT2-DQN, we arrive at
s
p
Îµ|S||A|
Î³
2Îµ|S||A|
+
.
âˆ¥QÎ¸1 âˆ’ Qâˆ— âˆ¥âˆž â‰¤
1âˆ’Î³
1âˆ’Î³
Î²
By symmetry, one can obtain the same bound for âˆ¥QÎ¸2 âˆ’ Qâˆ— âˆ¥âˆž . This completes the proof.

45

J

Additional comparison results among DQN, AGT2-DQN, and SGT2-DQN

J.1

Cartpole environment

Figure 16: Comparison of reward curves of DQN with different C in Cartpole environment. The
results show that C = 10 gives the best learning performance among the other options.
Figure 16 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Cartpole
environment. The results show that C = 10 gives the best learning performance among the other
choices of C.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 17: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in Cartpole
environment.
Figure 17 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}in Cartpole environment. The results demonstrate that their learning efficiency is comparable to DQN but without requiring extensive tuning, as they appear to be less
sensitive to Î².
Figure 18 illustrates the learning curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50 in Cartpole environment. The results show that while DQN exhibits slightly better learning
efficiency than the proposed methods in this environment, while as mentioned before the latter require
less efforts for hyperparameter tuning.

46

Figure 18: Comparison of reward curves of DQN with C = 10, AGT2-DQN with Î² = 50 and
SGT2-DQN with Î² = 50 in Cartpole environment.

47

J.2

Acrobot environment

Figure 19: Comparison of reward curves of DQN with different C in Acrobot environment.
Figure 19 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Acrobot
environment.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 20: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in Acrobot
environment.
Figure 20 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}. The results demonstrate that their learning efficiency is comparable
to DQN and slightly less sensitive to the choice of the hyperparameter Î².
Figure 21 illustrates the learning curves of DQN with C = 50, AGT2-DQN with Î² = 0.1, and
SGT2-DQN with Î² = 1, where each hyperparameter has been selected to approximately yield the
best results. The results show that DQN exhibits slightly better learning efficiency than the proposed
methods.

48

Figure 21: Comparison of reward curves of DQN with C = 50, AGT2-DQN with Î² = 0.1 and
SGT2-DQN with Î² = 1 in Acrobot environment.

49

J.3

Pendulum environment

Figure 22: Comparison of reward curves of DQN with different C in Pendulum environment.
Figure 22 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Pendulum
environment, which are sensitive to the choice of the hyperparameter C.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 23: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Pendulum environment.
Figure 23 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}. The results demonstrate that their learning efficiency is comparable to
DQN. Moreover, compared to DQN, AGT2-DQN is less sensitive to the choice of the hyperparameter
Î², while SGT2-DQN is more sensitive to the choice of the hyperparameter.
Figure 24 illustrates the learning curves of DQN with C = 500, AGT2-DQN with Î² = 0.1, and
SGT2-DQN with Î² = 0.1, where each hyperparameter has been selected to approximately yield the
best results. The results show that in this case, AGT2-DQN and SGT2-DQN exhibit better learning
efficiency than DQN.

50

Figure 24: Comparison of reward curves of DQN with C = 500, AGT2-DQN with Î² = 0.1 and
SGT2-DQN with Î² = 0.1 in Pendulum environment.

51

J.4

Mountaincar environment

Figure 25: Comparison of reward curves of DQN with different C in Mountaincar environment.
Figure 25 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Pendulum
environment.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 26: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Mountaincar environment.
Figure 26 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}.

Figure 27: Comparison of reward curves of DQN with C = 200, AGT2-DQN with Î² = 50 and
SGT2-DQN with Î² = 10 in Mountaincar environment.
52

Figure 27 illustrates the learning curves of DQN with C = 200, AGT2-DQN with Î² = 50, and
SGT2-DQN with Î² = 10, where each hyperparameter has been selected to approximately yield the
best results.
Overall, the results reveal that DQN, AGT2-DQN, and SGT2-DQN provide similar learning efficiencies and sensitivity to the hyperparameters.

53

J.5

Lunarlander environment

Figure 28: Comparison of reward curves of DQN with different C in Lunarlander environment.
Figure 28 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Lunarlander
environment.

(a) AGT2-DQN

(b) SGT2-DQN

Figure 29: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Lunarlander environment.
Figure 29 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}.

Figure 30: Comparison of reward curves of DQN with C = 10, AGT2-DQN with Î² = 0.01 and
SGT2-DQN with Î² = 0.01in Lunarlander environment.
54

Figure 30 illustrates the learning curves of DQN with C = 10, AGT2-DQN with Î² = 0.01, and
SGT2-DQN with Î² = 0.01, where each hyperparameter has been selected to approximately yield the
best results.
Overall, the results indicate that DQN, AGT2-DQN, and SGT2-DQN exhibit similar learning efficiencies and sensitivity to hyperparameters.

55

