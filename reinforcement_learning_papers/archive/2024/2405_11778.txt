Published as a conference paper at ICLR 2024

arXiv:2405.11778v1 [cs.LG] 20 May 2024

E FFICIENT M ULTI - AGENT R EINFORCEMENT L EARN ING BY P LANNING
Qihan Liu1‚àó , Jianing Ye2‚àó , Xiaoteng Ma1‚àó , Jun Yang1‚Ä† , Bin Liang1 , Chongjie Zhang3‚Ä†
1
Department of Automation, Tsinghua University
2
Institute for Interdisciplinary Information Sciences, Tsinghua University
3
Department of Computer Science & Engineering, Washington University in St. Louis
{lqh20, yejn21, ma-xt17}@mails.tsinghua.edu.cn
{yangjun603, bliang}@tsinghua.edu.cn
chongjie@wustl.edu

A BSTRACT
Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency
and hindering their applicability in more challenging scenarios. In contrast,
model-based reinforcement learning (MBRL), particularly algorithms integrating
planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by
adopting model-based approaches. However, incorporating planning and search
methods into multi-agent systems poses significant challenges. The expansive
action space of multi-agent systems often necessitates leveraging the nearlyindependent property of agents to accelerate learning. To tackle this issue, we
propose the MAZero algorithm, which combines a centralized model with Monte
Carlo Tree Search (MCTS) for policy search. We design a novel network structure
to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two
novel techniques: Optimistic Search Lambda (OS(Œª)) and Advantage-Weighted
Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark
demonstrate that MAZero outperforms model-free approaches in terms of sample
efficiency and provides comparable or better performance than existing modelbased methods in terms of both sample and computational efficiency. Our code is
available at https://github.com/liuqh16/MAZero.

1

I NTRODUCTION

Multi-Agent Reinforcement Learning (MARL) has seen significant success in recent years, with applications in real-time strategy games (Arulkumaran et al., 2019; Ye et al., 2020), card games (Bard
et al., 2020), sports games (Kurach et al., 2020), autonomous driving (Zhou et al., 2020), and multirobot navigation (Long et al., 2018). Nonetheless, challenges within multi-agent environments have
led to the problem of sample inefficiency. One key issue is the non-stationarity of multi-agent settings, where agents continuously update their policies based on observations and rewards, resulting
in a changing environment for individual agents (Nguyen et al., 2020). Additionally, the joint action
space‚Äôs dimension can exponentially increase with the number of agents, leading to an immense
policy search space (Hernandez-Leal et al., 2020). These challenges, combined with issues such
as partial observation, coordination, and credit assignment, necessitate a considerable demand for
samples in MARL for effective training (Gronauer & Diepold, 2022).
Conversely, Model-Based Reinforcement Learning (MBRL) has demonstrated its worth in terms
of sample efficiency within single-agent RL scenarios, both in practice (Wang et al., 2019) and
*
‚Ä†

Equal contribution.
Corresponding authors.

1

Published as a conference paper at ICLR 2024

theory (Sun et al., 2019). Unlike model-free methods, MBRL approaches typically focus on learning a parameterized model that characterizes the transition or reward functions of the real environment (Sutton & Barto, 2018; Corneil et al., 2018; Ha & Schmidhuber, 2018). Based on the
usage of the learned model, MBRL methods can be roughly divided into two lines: planning with
model (Hewing et al., 2020; Nagabandi et al., 2018; Wang & Ba, 2019; Schrittwieser et al., 2020;
Hansen et al., 2022) and data augmentation with model (Kurutach et al., 2018; Janner et al., 2019;
Hafner et al., 2019; 2020; 2023). Due to the foresight inherent in planning methods and their theoretically guaranteed convergence properties, MBRL with planning often demonstrates significantly
higher sample efficiency and converges more rapidly (Zhang et al., 2020). A well-known planningbased MBRL method is MuZero, which conducts Monte Carlo Tree Search (MCTS) with a valueequivalent learned model and achieves superhuman performance in video games like Atari and board
games including Go, Chess and Shogi (Schrittwieser et al., 2020).
Despite the success of MBRL in single-agent settings, planning with the multi-agent environment
model remains in its early stages of development. Recent efforts have emerged to bridge the gap by
combining single-agent MBRL algorithms with MARL frameworks (Willemsen et al., 2021; Bargiacchi et al., 2021; Mahajan et al., 2021; Egorov & Shpilman, 2022). However, the extension of
single-agent MBRL methods to multi-agent settings presents formidable challenges. On one hand,
existing model designs for single-agent algorithms do not account for the unique biases inherent
to multi-agent environments, such as the nearly-independent property of agents. Consequently, directly employing models from single-agent MBRL algorithms typically falls short in supporting
efficient learning within real-world multi-agent environments, underscoring the paramount importance of model redesign. On the other hand, multi-agent environments possess action spaces significantly more intricate than their single-agent counterparts, thereby exponentially escalating the
search complexity within multi-agent settings. This necessitates exploration into specialized planning algorithms tailored to these complex action spaces.
In this paper, we propose MAZero, the first empirically effective approach that extends the MuZero
paradigm into multi-agent cooperative environments. In particular, our contributions are fourfold:
1. Inspired by the Centralized Training with Decentralized Execution (CTDE) concept, we develop
a centralized-value, individual-dynamic model with shared parameters among all agents. We
incorporate an additional communication block using the attention mechanism to promote cooperation during the model unrolling (see Section 4.1).
2. Given the deterministic characteristics of the learned model, we have devised the Optimistic
Search Lambda (OS(Œª)) approach (see Section 4.2). It optimistically estimates the sampled returns while mitigating unrolling errors at larger depths using the parameter Œª.
3. We propose a novel policy loss named Advantage-Weighted Policy Optimization (AWPO) (see
Section 4.3) utilizing the value information calculated by OS(Œª) to improve the sampled actions.
4. We conduct extensive experiments on the SMAC benchmark, showing that MAZero achieves
superior sample efficiency compared to model-free approaches and provides better or comparable performance than existing model-based methods in terms of both sample and computation
efficiency (see Section 5).

2

BACKGROUND

This section introduces essential notations, and related works will be discussed in Appendix A.
POMDP Reinforcement Learning (RL) addresses the problem of an agent learning to act in an environment in order to maximize a scalar reward signal, which is characterized as a partially observable Markov decision process (POMDP) (Kaelbling et al., 1998) defined by (S, A, T, U, ‚Ñ¶, O, Œ≥),
where S is a set of states, A is a set of possible actions, T : S √ó A √ó S ‚Üí [0, 1] is a transition function over next states given the actions at current states, U : S √ó A ‚Üí R is the reward function. ‚Ñ¶ is
the set of observations for the agent and observing function O : S ‚Üí ‚Ñ¶ maps states to observations.
Œ≥ ‚àà [0, 1) is the discounted factor. At each timestep t, the agent acquire an observation ot = O(st )
based on current state st , choose action at upon the history of observations o‚â§t and receive corresponding reward ut = U (st , at ) from the environment. The objective
P‚àû of the agent is to learn a
policy œÄ that maximizes the expected discounted return J(œÄ) = EœÄ [ t=0 Œ≥ t ut |at ‚àº œÄ(¬∑|o‚â§t )].
2

Published as a conference paper at ICLR 2024

MuZero MuZero (Schrittwieser et al., 2020) is an MBRL algorithm for single-agent settings
that amalgamates a learned model of environmental dynamics with MCTS planning algorithm.
MuZero‚Äôs learned model consists of three functions: a representation function h, a dynamics function g, and a prediction function f . The model is conditioned on the observation history o‚â§t and a
sequence of future actions at:t+K and is trained to predict rewards rt,0:K , values vt,0:K and policies
pt,0:K , where rt,k , vt,k , pt,k are model predictions based on imaginary future state st,k unrolling k
steps from current time t. Specifically, the representation function maps the current observation
history o‚â§t into a hidden state st,0 , which is used as the root node of the MCTS tree. The dynamic
function g inputs the previous hidden state st,k with an action at+k and outputs the next hidden state
st,k+1 and the predicted reward rt,k . The prediction function f computes the value vt,k and policy
pt,k at each hidden state st,k . To perform MCTS, MuZero runs N simulation steps where each
consists of 3 stages: Selection, Expansion and Backup. During the Selection stage, MuZero starts
traversing from the root node and selects action by employing the probabilistic Upper Confidence
Tree (pUCT) rule (Kocsis & SzepesvaÃÅri, 2006; Silver et al., 2016) until reaching a leaf node:
"
#
pP
b N (s, b)
a = arg max Q(s, a) + P (s, a) ¬∑
¬∑ c(s)
(1)
a
1 + N (s, a)
where Q(s, a) denotes the estimation for Q-value, N (s, a) the visit count, P (s, a) the prior probability of selecting action a received from the prediction function, and c(s) is used to control the
influence of the prior relative to the Q-value. The target search policy œÄ(¬∑|st,0 ) is the normalized
distribution of visit counts at the root node st,0 . The model is trained minimize the following 3 loss:
reward loss lr , value loss lv and policy loss lp between true values and network predictions.
LMuZero =

K
X

[lr (ut+k , rt,k ) + lv (zt+k , vt,k ) + lp (œÄt+k , pt,k )]

(2)

k=0

Pn‚àí1
where ut+k is the real reward from environment, zt+k = i=0 Œ≥ i ut+k+i +Œ≥ t+k+n ŒΩt+k+n is n-step
return consists of reward and MCTS search value, œÄt+k is the MCTS search policy.
EfficientZero (Ye et al., 2021) proposes an asynchronous parallel workflow to alleviate the computational demands of the reanalysis MCTS . EfficientZero incorporates the self-supervised consistency
loss ls = ‚à•st,k ‚àí st+k,0 ‚à• using the SimSiam network to ensure consistency between the hidden state
from the kth step‚Äôs dynamic st,k and the direct representation of the future observation st+k,0 .
Sampled MuZero Since the efficiency of MCTS planning is intricately tied to the number of
simulations performed, the application of MuZero has traditionally been limited to domains with
relatively small action spaces, which can be fully enumerated on each node during the tree search.
To tackle larger action spaces, Sampled MuZero (Hubert et al., 2021) introduces a sampling-based
framework where policy improvement and evaluation are computed over small subsets of sampled
actions. Specifically, every time in the Expansion stage, only a subset T (s) of the full action space
A is expanded, where T (s) is on-time resampled from a policy Œ≤ base on the prior policy œÄ. After
that, in the Selection stage, an action is picked according to the modified pUCT formula.
#
"
pP
Œ≤ÃÇ
b N (s, b)
a = arg max Q(s, a) + P (s, a) ¬∑
¬∑ c(s)
(3)
Œ≤
1 + N (s, a)
a‚ààT (s)‚äÇA
where Œ≤ÃÇ is the empirical distribution of sampled actions, which means its support is T (s).
In this way, when making actual decisions at some root node state st,0 , Sampled MCTS will yield a
policy œâ(¬∑|st,0 ) after N simulations, which is a stochastic policy supported in T (st,0 ). The actual improved policy is denoted as œÄ MCTS (¬∑|st,0 ) = E[œâ(¬∑|st,0 )], which is the expectation of œâ(¬∑|st,0 ). The
randomness that the expectation is taken over is from all sampling operations in Sampled MCTS.
As demonstrated in the original paper, Sampled MuZero can be seamlessly extended to multi-agent
settings by considering each agent‚Äôs policy as a distinct dimension for a single agent‚Äôs multi-discrete
action space during centralized planning. The improved policy is denoted as œÄ MCTS (¬∑|s t,0 ).1
1

For the sake of simplicity, we will not emphasize the differences between search policy œÄ MCTS (¬∑|s t,0 ) in
multi-agent settings and œÄ MCTS (¬∑|st,0 ) in single-agent settings apart from the mathematical expressions.

3

Published as a conference paper at ICLR 2024

3

C HALLENGES IN P LANNING - BASED M ULTI - AGENT M ODEL - BASED RL

Extending single-agent Planning-based MBRL methods to multi-agent environments is highly nontrivial. On one hand, existing model designs for single-agent algorithms do not account for the
unique biases inherent to multi-agent environments, such as the nearly-independent property of
agents. This renders the direct employing a flattened model from single-agent MBRL algorithms
typically falls short in supporting efficient learning within real-world multi-agent environments. On
the other hand, multi-agent environments possess state-action spaces significantly more intricate
than their single-agent counterparts, thereby exponentially escalating the search complexity within
multi-agent settings. This, in turn, compels us to explore specialized search algorithms tailored to
complex action spaces. Moreover, the form of the model, in tandem with its generalization ability,
collectively constrains the form and efficiency of the search algorithm, and vice versa, making the
design of the model and the design of the search algorithm highly interrelated.
In this section, we shall discuss the challenges encountered in the current design of planning-based
MBRL algorithms, encompassing both model design and searching algorithm aspects. We will
elucidate how our MAZero algorithm systematically addresses these two issues in the Section 4.
3.1

M ODEL D ESIGN

Within the paradigm of centralized training with decentralized execution (CTDE), one straightforward approach is to learn a joint model that enables agents to do centralized planning within the joint
policy space. However, given the large size of the state-action space in multi-agent environments,
the direct application of a flattened model from single-agent MBRL algorithms tends to render the
learning process inefficient (Figure 6). This inefficiency stems from the inadequacy of a flattened
model in accommodating the unique biases inherent in multi-agent environments.
In numerous real-world scenarios featuring multi-agent environments, agents typically engage in
independent decision-making for the majority of instances, with collaboration reserved for exceptional circumstances. Furthermore, akin agents often exhibit homogeneous behavior (e.g., focusing
fire in the SMAC environment). For the former, a series of algorithms, including IPPO (Yu et al.,
2022), have demonstrated the efficacy of independent learning in a multitude of MARL settings.
Regarding the latter, previous research in multi-agent model-free approaches has underscored the
huge success of parameter-sharing (Rashid et al., 2020; Sunehag et al., 2017) within MARL, which
can be regarded as an exploitation of agents‚Äô homogenous biases. Hence, encoding these biases
effectively within the design of the model becomes one main challenge in multi-agent MBRL.
3.2

P LANNING IN E XPONENTIAL S IZED ACTION S PACE

Value

Bandit
The action space in multi-agent environments
100
grows exponentially with the number of agents,
rendering it immensely challenging for vanilla
MCTS algorithms to expand all actions. While
80
Sampled MuZero, in prior single-agent MBRL
AWPO (Ours)
work, attempted to address scenarios with large
BC
60
action spaces by utilizing action sampling, the
0
200
400
600
size of the environments it tackled (e.g., Go, ‚àº
OptimSteps
300 actions) still exhibits substantial disparity
compared to typical multi-agent environments
Figure 1: Bandit Experiment We compare the
(e.g., SMAC-27m vs 30m, ‚àº 1042 actions).
Behavior Cloning (BC) loss and our AdvantageIn practical applications like MARL, due to the Weighted Policy Optimization (AWPO) loss on a
significantly fewer samples taken compared to bandit with action space |A| = 100 and sampling
the original action space size, the underestima- time B = 2. It is evident that AWPO converges
tion issue of Sampled MCTS becomes more much faster than BC, owing to the effective utipronounced, making it less than ideal in terms lization of values.
of searching efficiency. This motivates us to design a more optimistic search process.
4

Published as a conference paper at ICLR 2024

Moreover, the behavior cloning (BC) loss that Sampled MCTS algorithms adopt disregards the value
information. This is because the target policy œâ(¬∑|st,0 ) only encapsulates the relative magnitude
of sampled action values while disregarding the information of the absolute action values. The
issue is not particularly severe when dealing with a smaller action space. However, in multi-agent
environments, the disparity between the sampling time B and |A| becomes significant, making it
impossible to overlook the repercussions of disregarding value information (Figure 1).

4

MAZ ERO ALGORITHM

4.1

M ODEL S TRUCTURE

The MAZero model comprises 6 key functions: a representation function hŒ∏ for mapping the current observation history oi‚â§t of agent i to an individual latent state sit,0 , a communication function eŒ∏ which generates additional cooperative features eit,k for each agent i through the attention
mechanism, a dynamic function gŒ∏ tasked with deriving the subsequent local latent state sit,k+1
based on the agent‚Äôs individual state sit,k , future action ait+k and communication feature eit,k , a reward prediction function forecasting the cooperative team reward rt,k from the global hidden state
1
N
s t,k = (s1t,k , . . . , sN
t,k ) and joint action a t+k = (at+k , . . . , at+k ), a value prediction function VŒ∏
aimed at predicting value vt,k for each global hidden state s t,k , and a policy prediction function PŒ∏
which given an individual state sit,k and generates the corresponding policy pit,k for agent i. The
model equations are shown in Equation (4).
Ô£±
Representation:
sit,0 = hŒ∏ (oi‚â§t )
Ô£¥
Ô£¥
Ô£¥
Ô£¥
1
N
1
N
Ô£¥
Ô£¥
Communication:
e1t,k , . . . , eN
t,k = eŒ∏ (st,k , . . . , st,k , at+k , . . . , at+k )
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤ Dynamic:
sit,k+1 = gŒ∏ (sit,k , ait+k , eit,k )
(4)
1
N
1
N
Ô£¥
Reward
Prediction:
r
=
R
(s
,
.
.
.
,
s
,
a
,
.
.
.
,
a
)
Ô£¥
t,k
Œ∏
t,k
t,k
t+k
t+k
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Value Prediction:
vt,k = VŒ∏ (s1t,k , . . . , sN
Ô£¥
t,k )
Ô£¥
Ô£¥
Ô£≥
i
i
Policy Prediction:
pt,k = PŒ∏ (st,k )
Notably, the representation function hŒ∏ , the dynamic function gŒ∏ , and the policy prediction function PŒ∏ all operate using local information, enabling distributed execution. Conversely, the other
functions deal with value information and team cooperation, necessitating centralized training for
effective learning.
$
ùë£!,# ùëù!,#
,

V

P

‚Ä¶

$
ùíî!,# = ùë†!,#
,

h
ùíê! = ùëú!$ ,

%
ùëù!,#

ùëü!,$

P

Reward model R

%
ùë†!,#

ùíÇ! = ùëé!$ , ‚Ä¶ , ùëé!$

‚Ä¶

h

‚Ä¶ ,

ùëú!%

Centralized
Dynamic
Block

‚Ä¶,
‚Ä¶
‚Ä¶,
‚Ä¶

$
ùë£!,$ (ùëù!,$
,

V

P

‚Ä¶,
‚Ä¶

$
ùíî!,$ = (ùë†!,$
,

h
$
ùíê!&$ =(ùëú!&$
,

‚Ä¶
‚Ä¶,

‚Ä¶

‚Ä¶
‚Ä¶,

%
ùëù!,$
)

$
ùë†!,#

P

Shared Individual
Dynamic Network g
$
ùëí!,#

%
ùë†!,$
)

$
%
ùëí!,#
, ‚Ä¶ , ùëí!,#

h

Communication
Network e

%
ùëú!&$
)

$
%
ùë†!,#
, ‚Ä¶ , ùë†!,#

$
$
(ùë†!,#
, ùëé!,#
)

$
%
ùëé!,#
, ‚Ä¶ , ùëé!,#

Figure 2: MAZero model structure Given the current observations oit for each agent, the model
separately maps them into local hidden states sit,0 using a shared representation network h. Value
prediction vt,0 is computed based on the global hidden state s t,0 while policy priors pit,0 are individually calculated for each agent using their corresponding local hidden states. Agents use the
communication network e to access team information eit,0 and generate next local hidden states sit,1
via the shared individual dynamic network g, subsequently deriving reward rt,1 , value vt,1 and policies pit,1 . During the training stage, real future observations o t+1 can be obtained to generate the
target for the next hidden state, denoted as s t+1,0 .

5

Published as a conference paper at ICLR 2024

4.2

O PTIMISTIC S EARCH L AMBDA

Having discerned that MAZero has acquired a deterministic world model, we devise a Optimistic
Search Lambda (OS(Œª)) approach to better harness this characteristic of the model.
In previous endeavors, the selection stage of MCTS employed two metrics (value score and exploration bonus) to gauge our interest in a particular action. In this context, the value score utilized the
mean estimate of values obtained from all simulations within that node‚Äôs subtree. However, within
the deterministic model, the mean estimate appears excessively conservative. Contemplating a scenario where the tree degenerates into a multi-armed bandit, if pulling an arm consistently yields a
deterministic outcome, there arises no necessity, akin to the Upper Confidence Bound (UCB) algorithm, to repeatedly sample the same arm and calculate its average. Similarly, within the tree-based
version of the UCT (Upper Confidence Trees) algorithm, averaging values across all simulations in a
subtree may be substituted with a more optimistic estimation when the environment is deterministic.
Hence, courtesy of the deterministic model, our focus narrows to managing model generalization
errors rather than contending with errors introduced by environmental stochasticity. Built upon this
conceptual foundation, we have devised a methodology for calculating the value score that places a
heightened emphasis on optimistic values.
Specifically, for each node s, we define:
(
)
X
Ud (s) =
Œ≥ k rk + Œ≥ d v(s‚Ä≤ ) s‚Ä≤ : dep(s‚Ä≤ ) = dep(s) + d

(5)

k<d

UdœÅ (s) = Top (1 ‚àí œÅ) values in Ud
X X
X
VŒªœÅ (s) =
Œªd x/
Œªd |UdœÅ (s)|
d x‚ààUdœÅ (s)

(6)
(7)

d

AœÅŒª (s, a) = r(s, a) + Œ≥VŒªœÅ (Dynamic(s, a)) ‚àí v(s)

(8)

where dep(u) denotes the depth of node u in the MCTS tree, œÅ, Œª ‚àà [0, 1] are hyperparameters, r, v,
and Dynamic are all model predictions calculated according to Equation (4).
To offer a brief elucidation on this matter, Equation (6) is the set of optimistic values at depth d, the
degree of optimism is controlled by the quantile parameter œÅ. Since the model errors magnify with
increasing depth in practice, Equation (7) calculates a weighted mean of all optimistic values in the
subtree as the value estimation of s, where the weight of different estimations are discounted by a
factor Œª over the depth.
Finally, we use the optimistic advantage (Equation (8)) to replace the value score in Equation (3) for
optimistic search, that is:
"
#
pP
N
(s,
b)
Œ≤ÃÇ
b
a = arg max AœÅŒª (s, a) + P (s, a) ¬∑
(9)
¬∑ c(s)
Œ≤
1 + N (s, a)
a‚ààT (s)‚äÇA
4.3

A DVANTAGE -W EIGHTED P OLICY O PTIMIZATION

In order to utilize the value information calculated by OS(Œª), we propose Advantage-Weighted
Policy Optimization (AWPO), a novel policy loss function that incorporates the optimistic advantage
(Equation (8)) into the behavior cloning loss.2
 œÅ

A (s,a)
In terms of outcome, AWPO loss weights the per-action behavior cloning loss by exp Œª Œ±
:
Ô£Æ
lp (Œ∏; œÄ, œâ, AœÅŒª ) = ‚àíE Ô£∞

X


œâ(a|s) exp

a‚ààT (s)

AœÅŒª (s, a)
Œ±



Ô£π
log œÄ(a|s; Œ∏)Ô£ª

(10)

2
In this section, the expectation operator is taken over the randomness of OS(Œª). For the sake of simplicity,
we will omit this in the formula.

6

Published as a conference paper at ICLR 2024

where Œ∏ denotes parameters of the learned model, œÄ(¬∑|s; Œ∏) is the network predict policy to be improved, œâ(¬∑|s) is the search policy supported in action subset T (s), AœÅŒª is the optimistic advantage
derived from OS(Œª) and Œ± > 0 is a hyperparameter controlling the degree of optimism. Theoretically, AWPO can be regarded as a cross-entropy loss between Œ∑ ‚àó and œÄ,3 where Œ∑ ‚àó is the nonparametric solution of the following constrained optimization problem:
maximize Ea‚àºŒ∑(¬∑|s) [AœÅŒª (s, a)]
Œ∑

s.t.


KL Œ∑(¬∑|s)‚à•œÄ MCTS (¬∑|s) ‚â§ œµ

To be more specific, by Lagrangian method, we have:
 œÅ


 œÅ

AŒª (s, a)
AŒª (s, a)
‚àó
MCTS
Œ∑ (a|s) ‚àù œÄ
(a|s) exp
= E œâ(a|s) exp
Œ±
Œ±

(11)

(12)

Thereby, minimizing the KL divergence between Œ∑ ‚àó and œÄ gives the form of AWPO loss:
arg min KL(Œ∑ ‚àó (¬∑|s)‚à•œÄ(¬∑|s; Œ∏))
Œ∏

1 X MCTS
œÄ
(a|s) exp
= arg min ‚àí
Z(s) a
Œ∏



AœÅŒª (s, a)
Œ±



log œÄ(a|s; Œ∏) ‚àí H(Œ∑ ‚àó (¬∑|s))

= arg min lp (Œ∏; œÄ, œâ, AœÅŒª )
Œ∏


 œÅ
P
A (s,a)
where Z(s) = a œÄ MCTS (a|s) exp Œª Œ±
is a normalizing factor, H(Œ∑ ‚àó ) stands for the entropy
of Œ∑ ‚àó , which is a constant.
Equation (11) shows that AWPO aims to optimize the value improvement of œÄ in proximity to
œÄ MCTS , which effectively combines the improved policy obtained from OS(Œª) with the optimistic
value, thereby compensating for the shortcomings of BC loss and enhancing the efficiency of policy
optimization.
Details of the derivation can be found in Appendix G.
4.4

M ODEL TRAINING

The MAZero model is unrolled and trained in an end-to-end schema similar to MuZero. Specifically,
given a trajectory sequence of length K + 1 for observation o t:t+K , joint actions a t:t+K , rewards
œÅ
ut:t+K , value targets zt:t+K , policy targets œÄ MCTS
t:t+K and optimistic advantages AŒª , the model is
unrolled for K steps as is shown in Figure 2 and is trained to minimize the following loss:
L=

K
X

(lr + lv + ls ) +

k=1

K
X

lp

(13)

k=0

where lr = ‚à•rt,k ‚àí ut+k ‚à• and lv = ‚à•vt,k ‚àí zt+k ‚à• are reward and value losses similar to MuZero,
ls = ‚à•sst,k ‚àí st+k,0 ‚à• is the consistency loss akin to EfficientZero and lp is the AWPO policy loss
(Equation (10)) under the realm of multi-agent joint action space:

 œÅ
X
AŒª (s t+k,0 , a )
log œÄ (a |s t,k ; Œ∏)
(14)
lp = ‚àí
œâ(a |s t+k,0 ) exp
Œ±
a ‚ààT (s t+k,0 )

where OS(Œª) is performed under the hidden state s t+k,0 directly represented from the actual future
observation o t+k , deriving corresponding action subset T (s t+k,0 ), search policy œâ(a |s t+k,0 ) and
QN
optimistic advantage AœÅŒª (s t+k,0 , a ). œÄ (a |s t,k ; Œ∏) = i=1 PŒ∏ (ai |sit,k ) denotes the joint policy to be
improved at the kth step‚Äôs hidden state s t,k unrolling from dynamic function.
3

It is equivalent to minimizing the KL divergence of Œ∑ ‚àó and œÄ.

7

Published as a conference paper at ICLR 2024

5

E XPERIMENTS

5.1

S TAR C RAFT M ULTI -AGENT C HALLENGE

Baselines We compare MAZero with both model-based and model-free baseline methods
on StarCraft Multi-Agent Challenge (SMAC) environments. The model-based baseline is
MAMBA (Egorov & Shpilman, 2022), a recently introduced multi-agent MARL algorithm based on
DreamerV2 (Hafner et al., 2020) known for its state-of-the-art sample efficiency in various SMAC
tasks. The model-free MARL methods includes QMIX (Rashid et al., 2020), QPLEX (Wang et al.,
2020a), RODE (Wang et al., 2020b), CDS (Li et al., 2021), and MAPPO (Yu et al., 2022).
MAMBA

QPLEX

0.6

0.6

0.4

0.4

0.6

0.4

0.0

0.0

0.0

1.0

3

4
1e5

0.0

0.2

1.0

0.4

0.6

EnvSteps
2m_vs_1z

0.8

1.0
1e6

0.4
0.2
0.0

0.0

0.2

1.0

0.4

0.6

EnvSteps
so_many_baneling

0.8

1.0
1e6

0.6

0.6

0.6

0.4

0.2

0.2

0.2

0.0

0.0

0.0

0

1

2

3

EnvSteps

4

5
1e4

0

1

2

3

EnvSteps

4

5
1e4

WinRate

0.8

0.6

WinRate

0.8

WinRate

0.8

0.4

0.0

0.2

0.4

0.6

0.8

1.0
1e6

0.0

0.2

0.4

0.6

0.8

1.0
1e5

1.0

0.8

0.4

10m_vs_11m

1.0
0.8

0.2

2

QMIX

0.6

0.2

EnvSteps
3m

CDS

8m_vs_9m

0.8

0.2
1

RODE

1.0

WinRate

0.8

WinRate

WinRate

0.8

0

WinRate

MAPPO

5m_vs_6m

WinRate

MAZero(Ours)

2c_vs_64zg

EnvSteps
2s_vs_1sc

0.4
0.2
0.0

0

1

2

3

EnvSteps

4

5
1e4

EnvSteps

Figure 3: Comparisons against baselines in SMAC. Y axis denotes the win rate and X axis denotes
the number of steps taken in the environment. Each algorithm is executed with 10 random seeds.
Figure 3 illustrates the results in the SMAC environments. Among the 23 available scenarios, we
have chosen 8 for presentation in this paper. Specifically, we have selected four random scenarios
categorized as Easy tasks and 4 Hard tasks. It is evident that MAZero outperforms all baseline algorithms across eight scenarios with a given number of steps taken in the environment. Notably, both
MAZero and MAMBA, which are categorized as model-based methods, exhibit markedly superior
sample efficiency in easy tasks when compared to model-free baselines. Furthermore, MAZero displays a more stable training curve and smoother win rate during evaluation than MAMBA. In the
realm of hard tasks, MAZero surpasses MAMBA in terms of overall performance, with a noteworthy emphasis on the 2c vs 64zg scenario. In this particular scenario, MAZero attains a higher win
rate with a significantly reduced sample size when contrasted with other methods. This scenario
involves a unique property, featuring only two agents and an expansive action space of up to 70
for each agent, in contrast to other scenarios where the agent‚Äôs action space is generally fewer than
20. This distinctive characteristic enhances the role of planning in MAZero components, similar to
MuZero‚Äôs outstanding performance in the domain of Go.
As MAZero builds upon the MuZero framework, we perform end-to-end training directly using
planning results from the replay buffer. Consequently, this approach circumvents the time overhead
for data augmentation, as seen in Dreamer-based methods. Figure 4 illustrates the superior performance of MAZero with respect to the temporal cost in SMAC environments when compared to
MAMBA.
5.2

A BLATION

We perform several ablation experiments on the two proposed techniques: OS(Œª) and AWPO. The
results with algorithms executed with three random seeds are reported in Figure 5. In particular,
we examine whether disabling OS(Œª), AWPO, or both of them (i.e., using original Sampled MCTS
and BC loss function) impairs final performance. Significantly, both techniques greatly impact final
performance and learning efficiency.
8

Published as a conference paper at ICLR 2024

MAZero(Ours)

2m_vs_1z

0.5
0.0
4

6

0.5
0.0

2.5

5.0

7.5

Timecost(hours)
2c_vs_64zg

10.0

12.5

0

5

10

15

0.5

20

Timecost(hours)

5

0.6

0.0
0

0.5
0.0

0.0

WinRate

Timecost(hours)
2s_vs_1sc

8

WinRate

2

1.0

WinRate

0.5
0.0

0

so_many_baneling

1.0

WinRate

1.0

WinRate

WinRate

1.0

MAMBA

3m

10

15

20

Timecost(hours)
5m_vs_6m

0.4
0.2
0.0

0

5

10

15

20

25

Timecost(hours)

30

0

5

10

15

20

Timecost(hours)

25

30

Figure 4: Comparisons against MBRL baselines in SMAC. The y-axis denotes the win rate, and the
X-axis denotes the cumulative run time of algorithms in the same platform.
MAZero

No OS( )

No AWPO
1.0

0.8

0.8

0.6

0.6

WinRate

WinRate

8m
1.0

0.4

No OS( ) and AWPO

2c_vs_64zg

0.4
0.2

0.2
0.0

0.0
0.0

0.2

0.4

EnvSteps

0.6

0.8

1.0
1e6

0.0

0.2

0.4

EnvSteps

0.6

0.8

1.0
1e6

Figure 5: Ablation study on proposed approaches for planning.

In our ablation of the MAZero network structure (Figure 6), we have discerned the substantial impact
of two components, ‚Äúcommunication‚Äù and ‚Äúsharing‚Äù, on algorithmic performance. In stark contrast,
the flattened model employed in single-agent MBRL, due to its failure to encapsulate the unique
biases of multi-agent environments, can only learn victorious strategies in the most rudimentary of
scenarios, such as the 2m vs 1z map.
MAZero

2m_vs_1z

No communication

No sharing

2c_vs_64zg

Flattened

5m_vs_6m

10m_vs_11m

1.0
0.8

0.8

0.6

0.6

0.6

0.4

0.0

0.0
0.0

0.2

0.4

0.6

EnvSteps

0.8

1.0
1e5

0.4
0.2

0.2

0.2

WinRate

0.4

0.8

WinRate

0.6

WinRate

WinRate

0.8

0.2

0.0
0.0

0.2

0.4

0.6

EnvSteps

0.8

1.0
1e6

0.4

0.0
0.0

0.2

0.4

0.6

EnvSteps

0.8

1.0
1e6

0.0

0.2

0.4

0.6

EnvSteps

0.8

1.0
1e6

Figure 6: Ablation study on network structure.

6

C ONCLUSION

In this paper, we introduce a model-based multi-agent algorithm, MAZero, which utilizes the CTDE
framework and MCTS planning. This approach boasts superior sample efficiency compared to stateof-the-art model-free methods and provides comparable or better performance than existing modelbased methods in terms of both sample and computational efficiency. We also develop two novel
approaches, OS(Œª) and AWPO, to improve search efficiency in vast action spaces based on sampled
MCTS. In the future, we aim to address this issue through reducing the dimensionality of the action
space in search, such as action representation.
9

Published as a conference paper at ICLR 2024

7

R EPRODUCIBILITY

We ensure the reproducibility of our research by providing comprehensive information and resources
that allow others to replicate our findings. All experimental settings in this paper are available in
Appendix B, including details on environmental setup, network structure, training procedures, hyperparameters, and more. The source code utilized in our experiments along with clear instructions
on how to reproduce our results will be made available upon finalization of the camera-ready version. The benchmark employed in this investigation is either publicly accessible or can be obtained
by contacting the appropriate data providers. Additionally, detailed derivations for our theoretical
claims can be found in Appendix G. We are dedicated to addressing any concerns or inquiries related to reproducibility and are open to collaborating with others to further validate and verify our
findings.
ACKNOWLEDGMENTS
This work was supported by the National Science and Technology Innovation 2030 - Major Project
(Grant No. 2022ZD0208804).

R EFERENCES
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304‚Äì29320, 2021.
Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and
tree search. Advances in neural information processing systems, 30, 2017.
Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, and David Silver. Planning in stochastic environments with a learned model. In International Conference on Learning
Representations, 2021.
Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation
perspective. In Proceedings of the genetic and evolutionary computation conference companion,
pp. 314‚Äì315, 2019.
Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio
Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A
new frontier for ai research. Artificial Intelligence, 280:103216, 2020.
Eugenio Bargiacchi, Timothy Verstraeten, and Diederik M Roijers. Cooperative prioritized sweeping. In AAMAS, pp. 160‚Äì168, 2021.
Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learning and search for imperfect-information games. Advances in Neural Information Processing
Systems, 33:17057‚Äì17069, 2020.
Dane Corneil, Wulfram Gerstner, and Johanni Brea. Efficient model-based deep reinforcement
learning with variational state tabulation. In International Conference on Machine Learning, pp.
1049‚Äì1058. PMLR, 2018.
Ivo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver. Policy improvement by planning
with gumbel. In International Conference on Learning Representations, 2021.
Vladimir Egorov and Aleksei Shpilman. Scalable multi-agent model-based reinforcement learning.
arXiv preprint arXiv:2205.15023, 2022.
Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial
Intelligence Review, pp. 1‚Äì49, 2022.
David Ha and JuÃàrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
10

Published as a conference paper at ICLR 2024

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains
through world models. arXiv preprint arXiv:2301.04104, 2023.
Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive
control. arXiv preprint arXiv:2203.04955, 2022.
Jinke He, Thomas M Moerland, and Frans A Oliehoek. What model does muzero learn? arXiv
preprint arXiv:2306.00840, 2023.
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. A very condensed survey and critique
of multiagent deep reinforcement learning. In Proceedings of the 19th international conference
on autonomous agents and multiagent systems, pp. 2146‚Äì2148, 2020.
Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane
Weber, David Silver, and Hado Van Hasselt. Muesli: Combining improvements in policy optimization. In International conference on machine learning, pp. 4214‚Äì4226. PMLR, 2021.
Lukas Hewing, Kim P Wabersich, Marcel Menner, and Melanie N Zeilinger. Learning-based model
predictive control: Toward safe learning in control. Annual Review of Control, Robotics, and
Autonomous Systems, 3:269‚Äì296, 2020.
Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon
Schmitt, and David Silver. Learning and planning in complex action spaces. In International
Conference on Machine Learning, pp. 4476‚Äì4486. PMLR, 2021.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. Advances in neural information processing systems, 32, 2019.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artificial intelligence, 101(1-2):99‚Äì134, 1998.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Levente Kocsis and Csaba SzepesvaÃÅri. Bandit based monte-carlo planning. In European conference
on machine learning, pp. 282‚Äì293. Springer, 2006.
Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research
football: A novel reinforcement learning environment. In Proceedings of the AAAI conference on
artificial intelligence, volume 34, pp. 4501‚Äì4510, 2020.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement learning. Advances in Neural Information
Processing Systems, 34:3991‚Äì4002, 2021.
Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent
game abstraction via graph attention neural network. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 7211‚Äì7218, 2020.
Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao Zhang, and Jia Pan. Towards optimally
decentralized multi-robot collision avoidance via deep reinforcement learning. In 2018 IEEE
international conference on robotics and automation (ICRA), pp. 6252‚Äì6259. IEEE, 2018.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.
11

Published as a conference paper at ICLR 2024

Anuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaifi,
Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for
multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 7301‚Äì
7312. PMLR, 2021.
Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, and Yi Wu. Speedyzero: Mastering atari with limited data and time. In The Eleventh International Conference on Learning
Representations, 2022.
Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics
for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 7559‚Äì7566. IEEE, 2018.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets, 2021.
Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for
multiagent systems: A review of challenges, solutions, and applications. IEEE transactions on
cybernetics, 50(9):3826‚Äì3839, 2020.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin BoÃàhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems, 34:12208‚Äì12221, 2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement
learning. The Journal of Machine Learning Research, 21(1):7234‚Äì7284, 2020.
Heechang Ryu, Hayong Shin, and Jinkyoo Park. Multi-agent actor-critic with hierarchical graph
attention network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 7236‚Äì7243, 2020.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604‚Äì609, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484‚Äì489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354‚Äì359, 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140‚Äì
1144, 2018.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In International conference on machine learning, pp. 5887‚Äì5896. PMLR, 2019.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pp. 2898‚Äì2933. PMLR, 2019.
12

Published as a conference paper at ICLR 2024

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play.
Neural computation, 6(2):215‚Äì219, 1994.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020a.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint
arXiv:1906.08649, 2019.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.
Rode: Learning roles to decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020b.
DanieÃàl Willemsen, Mario Coppola, and Guido CHE de Croon. Mambpo: Sample-efficient multirobot reinforcement learning using learned world models. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5635‚Äì5640. IEEE, 2021.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
preprint arXiv:2002.03939, 2020.
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang,
Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
6672‚Äì6679, 2020.
Jianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards global optimality in cooperative marl with the transformation and distillation framework, 2023.
Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games
with limited data. Advances in Neural Information Processing Systems, 34:25476‚Äì25488, 2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The
surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information
Processing Systems, 35:24611‚Äì24624, 2022.
Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems, 33:1166‚Äì1178, 2020.
Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent reinforcement
learning training school for autonomous driving. arXiv preprint arXiv:2010.09776, 2020.

13

Published as a conference paper at ICLR 2024

A

R ELATED WORKS

Dec-POMDP In this work, we focus on the fully cooperative multi-agent systems that can be formalized as Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Oliehoek
et al., 2016), which are defined by (N, S, A1:N , T, U, ‚Ñ¶1:N , O1:N , Œ≥), where N is the number of
agents, S is the global state space, T a global transition function, U a shared reward function
and Ai , ‚Ñ¶i , Oi are the action space, observation space and observing function for agent i. Given
state st at timestep t, agent i can only acquire local observation oit = Oi (st ) and choose action
ait ‚àà Ai according to its policy œÄ i based on local observation history oi‚â§t . The environment then
shifts to the next state st+1 ‚àº T (¬∑|st , at ) and returns a scalar reward ut = U (st , at ). The objective for all agents
a joint policy œÄ that maximizes
the expectation of discounted return

P‚àû is tto learn
i
i
i
J(œÄ ) = EœÄ
t=0 Œ≥ ut |at ‚àº œÄ (¬∑|o‚â§t ), i = 1, . . . , N .
Combining reinforcement learning and planning algorithms The integration of reinforcement
learning and planning within a common paradigm has yielded superhuman performance in diverse
domains (Tesauro, 1994; Silver et al., 2017; Anthony et al., 2017; Silver et al., 2018; Schrittwieser
et al., 2020; Brown et al., 2020). In this approach, RL acquires knowledge through the learning
of value and policy networks, which in turn guide the planning process. Simultaneously, planning
generates expert targets that facilitate RL training. For example, MuZero-based algorithms (Schrittwieser et al., 2020; Hubert et al., 2021; Antonoglou et al., 2021; Ye et al., 2021; Mei et al., 2022)
combine Monte-Carlo Tree Search (MCTS), TD-MPC (Hansen et al., 2022) integrates Model Predictive Control (MPC), and ReBeL (Brown et al., 2020) incorporate a fusion of Counterfactual
Regret Minimization (CFR). Nevertheless, the prevailing approach among these algorithms typically involves treating the planning results as targets and subsequently employing behavior cloning
to enable the RL network to emulate these targets. Muesli (Hessel et al., 2021) first combines regularized policy optimization with model learning as an auxiliary loss within the MuZero framework.
However, it focuses solely on the policy gradient loss while neglecting the potential advantages of
planning. Consequently, it attains at most a performance level equivalent to that of MuZero. To the
best of our knowledge, we are the first to combined policy gradient and MCTS planning to accurate
policy learning in model-based reinforcement learning.
Variants of MuZero-based algorithms While the original MuZero algorithm (Schrittwieser
et al., 2020) has achieved superhuman performance levels in Go and Atari, it is important to note that
it harbors several limitations. To address and overcome these limitations, subsequent advancements
have been made in the field. EfficientZero (Ye et al., 2021), for instance, introduces self-supervised
consistency loss, value prefix prediction, and off-policy correction mechanisms to expedite and stabilize the training process. Sampled MuZero (Hubert et al., 2021) extends its capabilities to complex
action spaces through the incorporation of sample-based policy iteration. Gumbel MuZero (Danihelka et al., 2021) leverages the Gumbel-Top-k trick and modifies the planning process using Sequential Halving, thereby reducing the demands of MCTS simulations. SpeedyZero (Mei et al.,
2022) integrates a distributed RL framework to facilitate parallel training, further enhancing training efficiency. He et al. (2023) demonstrate that the model acquired by MuZero typically lacks
accuracy when employed for policy evaluation, rendering it ineffective in generalizing to assess
previously unseen policies.
Multi-agent reinforcement learning The typical approach for MARL in cooperative settings involves centralized training and decentralized execution (CTDE). During the training phase, this approach leverages global or communication information, while during the execution phase, it restricts
itself to the observation information relevant to the current agent. This paradigm encompasses both
value-based (Sunehag et al., 2017; Rashid et al., 2020; Son et al., 2019; Yang et al., 2020; Wang et al.,
2020a) and policy-based (Lowe et al., 2017; Liu et al., 2020; Peng et al., 2021; Ryu et al., 2020; Ye
et al., 2023) MARL methods within the context of model-free scenarios. Model-based MARL algorithms remains relatively underexplored with only a few methods as follows: MAMBPO (Willemsen
et al., 2021) pioneers the fusion of the CTDE framework with Dyan-style model-based techniques,
emphasizing the utilization of generated data closely resembling real-world data. CPS (Bargiacchi et al., 2021) introduces dynamic and reward models to determine data priorities based on the
MAMBPO approach. Tesseract (Mahajan et al., 2021) dissects states and actions into low-rank
tensors and evaluates the Q function using Dynamic Programming (DP) within an approximate en14

Published as a conference paper at ICLR 2024

vironment model. MAMBA (Egorov & Shpilman, 2022) incorporates the word model proposed in
DreamerV2 (Hafner et al., 2020) and introduces an attention mechanism to facilitate communication. This integration leads to noteworthy performance improvements and a substantial enhancement
in sample efficiency when compared to previous model-free methods. To the best of our knowledge,
we are the first to expand the MuZero framework and incorporate MCTS planning into the context
of model-based MARL settings.

B

I MPLEMENTATION D ETAILS

B.1

N ETWORK S TRUCTURE

For the SMAC scenarios where the input observations are 1 dimensional vectors (as opposed to 2
dimensional images for board games or Atari used by MuZero), we use a variation of the MuZero
model architecture in which all convolutions are replaced by fully connected layers. The model
consists of 6 modules: representation function, communication function, dynamic function, reward
prediction function, value prediction function, and policy prediction function, which are all represented as neural networks. All the modules except the communication block are implemented as
Multi-Layer Perception (MLP) networks, where each liner layer in MLP is followed by a Rectified
Linear Unit (ReLU) activation and a Layer Normalisation (LN) layer. Specifically, we use a hidden state size of 128 for all SMAC scenarios, and the hidden layers for each MLP module are as
follows: Representation Network = [128, 128], Dynamic Network = [128, 128], Reward Network =
[32], Value Network = [32] and Policy Network = [32]. We use Transformer architecture (Vaswani
et al., 2017) with three stacked layers for encoding state-action pairs. These encodings are then
used by the agents to process local dynamic and make predictions. We use a dropout technique with
probability p = 0.1 to prevent the model from over-fitting and use positional encoding to distinguish
agents in homogeneous settings.
For the representation network, we stack the last four local observations together as input for each
agent to deal with partial observability. Additionally, the concatenated observations are processed
by an extra LN to normalize observed features before representation.
The dynamic function first concatenates the current local state, the individual action, and the communication encoding based on current state-action pairs as input features. To alleviate the problem
that gradients tend to zero during the continuous unroll of the model, the dynamic function employs
a residual connection between the next hidden state and the current hidden state.
For value
‚àöand reward prediction, we follow in scaling targets using an invertible transform h(x) =
sign(x) 1 + x ‚àí 1 + 0.001 ¬∑ x and use the categorical representation introduced in MuZero (Schrittwieser et al., 2020). We use ten bins for both the value and the reward predictions, with the predictions being able to represent values between [‚àí5, 5]. We used n‚àístep bootstrapping with n = 5 and
a discount of 0.99.
B.2

T RAINING D ETAILS

MAZero employs a similar pipeline to EfficientZero (Ye et al., 2021) while asynchronizing the
parallel stages of data collection (Self-play workers), reanalysis (Reanalyze workers), and training
(Main Thread) to maintain the reproducibility of the same random seed.
We use the Adam optimizer (Kingma & Ba, 2014) for training, with a batch size of 256 and a
constant learning rate of 10‚àí4 . Samples are drawn from the replay buffer according to prioritized
replay (Schaul et al., 2015) using the same priority and hyperparameters as in MuZero.
In practice, we re-execute OS(Œª) using a delayed target model Œ∏ÃÇ in the reanalysis stage to reduce
off-policy error. Consequently, the AWPO loss function actually used is the following equation:
!
X
AÃÇœÅŒª (sÃÇ t+k,0 , aÃÇ )
lp = ‚àí
œâÃÇ(aÃÇ |sÃÇ t+k,0 ) exp
log œÄ (aÃÇ |s t,k ; Œ∏)
Œ±
aÃÇ ‚ààTÃÇ (sÃÇ t+k,0 )

For other details, we provide hyper-parameters in Table 1.
15

Published as a conference paper at ICLR 2024

Parameter

Setting

Observations stacked
Discount factor
Minibatch size
Optimizer
Optimizer: learning rate
Optimizer: RMSprop epsilon
Optimizer: weight decay
Max gradient norm
Priority exponent(cŒ± )
Priority correction(cŒ≤ )
Evaluation episodes
Min replay size for sampling
Target network updating interval
Unroll steps
TD steps(n)
Number of MCTS sampled actions(K)
Number of MCTS simulations(N )
Quantile in MCTS value estimation(œÅ)
Decay lambda in MCTS value estimation(Œª)
Exponential factor in Weighted-Advantage(Œ±)

4
0.99
256
Adam
10‚àí4
10‚àí5
0
5
0.6
0.4 ‚Üí 1
32
300
200
5
5
10
100
0.75
0.8
3

Table 1: Hyper-parameters for MAZero in SMAC environments

B.3

D ETAILS OF BASELINE ALGORITHMS IMPLEMENTATION

MAMBA (Egorov & Shpilman, 2022) is executed based on the open-source implementation:
https://github.com/jbr-ai-labs/mamba with the hyper-parameters in Table 2.
Parameter

Setting

Batch size
GAE Œª
Entropy coefficient
Entropy annealing
Number of updates
Epochs per update
Update clipping parameter
Actor Learning rate
Critic Learning rate
Discount factor
Model Learning rate
Number of epochs
Number of sampled rollouts
Sequence length
Rollout horizon H
Buffer size
Number of categoricals
Number of classes
KL balancing entropy weight
KL balancing cross entropy weight
Gradient clipping
Trajectories between updates
Hidden size

256
0.95
0.001
0.99998
4
5
0.2
5 √ó 10‚àí4
5 √ó 10‚àí4
0.99
2 √ó 10‚àí4
60
40
20
15
2.5 √ó 105
32
32
0.2
0.8
100
1
256

Table 2: Hyper-parameters for MAMBA in SMAC environments
16

Published as a conference paper at ICLR 2024

QMIX (Rashid et al., 2020) is executed based on the open-source implementation: https://
github.com/oxwhirl/pymarl with the hyper-parameters in Table 3.
Parameter

Setting

Batch size
Buffer size
Discount factor
Actor Learning rate
Critic Learning rate
Optimizer
RMSProp Œ±
RMSProp œµ
Gradient clipping
œµ-greedy
œµ annealing time

32
5000
0.99
5 √ó 10‚àí4
5 √ó 10‚àí4
RMSProp
0.99
10‚àí5
10
1.0 ‚Üí 0.05
50000

Table 3: Hyper-parameters for QMIX in SMAC environments
QPLEX (Wang et al., 2020a) is executed based on the open-source implementation: https://
github.com/wjh720/QPLEX with the hyper-parameters in Table 4.
Parameter

Setting

Batch size
Buffer size
Discount factor
Actor Learning rate
Critic Learning rate
Optimizer
RMSProp Œ±
RMSProp œµ
Gradient clipping
œµ-greedy
œµ annealing time

32
5000
0.99
5 √ó 10‚àí4
5 √ó 10‚àí4
RMSProp
0.99
10‚àí5
10
1.0 ‚Üí 0.05
50000

Table 4: Hyper-parameters for QPLEX in SMAC environments
RODE (Wang et al., 2020b) is executed based on the open-source implementation: https://
github.com/TonghanWang/RODE with the hyper-parameters in Table 5.
Parameter

Setting

Batch size
Buffer size
Discount factor
Actor Learning rate
Critic Learning rate
Optimizer
RMSProp Œ±
RMSProp œµ
Gradient clipping
œµ-greedy
œµ annealing time
number of clusters

32
5000
0.99
5 √ó 10‚àí4
5 √ó 10‚àí4
RMSProp
0.99
10‚àí5
10
1.0 ‚Üí 0.05
50K ‚àº 500K
2‚àº5

Table 5: Hyper-parameters for RODE in SMAC environments
17

Published as a conference paper at ICLR 2024

CDS (Li et al., 2021) is executed based on the open-source implementation: https://github.
com/lich14/CDS with the hyper-parameters in Table 6.
Parameter

Setting

Batch size
Buffer size
Discount factor
Actor Learning rate
Critic Learning rate
Optimizer
RMSProp Œ±
RMSProp œµ
Gradient clipping
œµ-greedy
Œ≤
Œ≤1
Œ≤2
Œª
attention regulation coefficient

32
5000
0.99
5 √ó 10‚àí4
5 √ó 10‚àí4
RMSProp
0.99
10‚àí5
10
1.0 ‚Üí 0.05
0.05
0.5
0.5
0.1
10‚àí3

Table 6: Hyper-parameters for CDS in SMAC environments
MAPPO (Yu et al., 2022) is executed based on the open-source implementation: https://
github.com/marlbenchmark/on-policy with the hyper-parameters in Table 7.
Parameter

Setting

Recurrent data chunk length
Gradient clipping
GAE Œª
Discount factor
Value loss
Huber delta
Batch size
Mini batch size
Optimizer
Optimizer: learning rate
Optimizer: RMSprop epsilon
Optimizer: weight decay

10
10
0.95
0.99
huber loss
10.0
num envs √ó buffer length √ó num agents
batch size / mini-batch
Adam
5 √ó 10‚àí4
10‚àí5
0

Table 7: Hyper-parameters for MAPPO in SMAC environments

B.4

D ETAILS OF THE BANDIT E XPERIMENT

The bandit experiment showed in Figure 1 compares the behavior cloning loss and the AWPO loss
on a 100-armed bandit, with action values 0, ¬∑ ¬∑ ¬∑ , 99 and sampling time B = 2. The policy œÄ BC and
œÄ AWPO are parameterized by Softmax policy, which means
exp(Œ∏aBC )
œÄ BC (a; Œ∏BC ) = P
BC
b exp(Œ∏b )
exp(Œ∏aAWPO )
œÄ AWPO (a; Œ∏AWPO ) = P
AWPO
)
b exp(Œ∏b

where Œ∏BC , Œ∏AWPO ‚àà R100 are randomly initialized and are identical in the beginning.
18

Published as a conference paper at ICLR 2024

To exclude the influence of stochasticity in the search algorithm and facilitate a more precise and
fair comparison of the differences in loss functions, we made three targeted adjustments.
1. Let the subset of sampled action be T , we denote the target policy œâ(a) = I(a =
arg maxb‚ààT value(b)).
2. We calculate the expectation of the loss over the randomness of all possible T s.
3. We normalize the advantage in AWPO into 0-mean-1-std and choose Œ± = 1.
Formally, we have
lBC (Œ∏) = ‚àí

X

log œÄŒ∏ (a)tŒ∏ (a)

X

lBC (Œ∏) = ‚àí
log œÄŒ∏ (a)tŒ∏ (a) exp (adv(a))
P
K
K
P
is the expectation of œâ(a), the over line stands
where t(a) =
‚àí
b>a œÄŒ∏ (b)
b‚â•a œÄŒ∏ (b)
q

3
for stop-gradient, adv(a) = a ‚àí 99
2
2525 is the normalized advantage.

C

S TANDARDISED P ERFORMANCE E VALUATION P ROTOCOL

We report our experiments based on the standardised performance evaluation protocol (Agarwal
et al., 2021).

Figure 7: Aggregate metrics on the SMAC benchmark with 95% stratified bootstrap confidence
intervals. Higher median, interquartile mean (IQM), and mean, but lower optimality gap indicate
better performance.

Figure 8: Probabilities of improvement, i.e. how likely it is for MAZero to outperform baselines on
the SMAC benchmark.

D

M ORE A BLATIONS

In the experiment section, we list some ablation studies to prove the effectiveness of each component
in MAZero. In this section, we will display more results for the ablation study about hyperparameters.
19

Published as a conference paper at ICLR 2024

First, we perform an ablation study on the training stage optimizers, contrasting SGD and Adam.
The SGD optimizer is prevalent in most MuZero-based algorithms (Schrittwieser et al., 2020; Ye
et al., 2021), while Adam is frequently used in MARL environments (Rashid et al., 2020; Wang
et al., 2020a; Yu et al., 2022). Figure 9 indicates that Adam demonstrates superior performance and
more consistent stability compared to SGD.
Adam

SGD

8m

0.25

0.8

0.20

0.6

WinRate

WinRate

5m_vs_6m

0.30

0.4

0.15
0.10

0.2

0.05

0.0

0.00
0.0

0.5

1.0

1.5

2.0

EnvSteps

2.5

3.0
1e5

0.0

0.5

1.0

1.5

2.0

EnvSteps

2.5

3.0
1e5

Figure 9: Ablation on optimizer
K=10,N=100

K=5,N=50

8m

5m_vs_6m

1.0
0.8
0.6

0.6

WinRate

WinRate

0.8

0.4

0.4
0.2

0.2
0.0

0.0
0.0

0.2

0.4

EnvSteps

0.6

0.8

1.0
1e6

0.0

0.2

0.4

EnvSteps

0.6

0.8

1.0
1e6

Figure 10: Ablation on sampled scale
=0.75

=0.65

=0.85

=0.8

=0.95

=0.7

0.8

0.8

0.6

0.6

0.4
0.2

=0.9

=1.0

0.6

0.8

8m

1.0

WinRate

WinRate

8m

0.4
0.2

0.0

0.0
0

1

2

3

4

EnvSteps

5

6

7

8
1e5

(a) Ablation on œÅ

0.0

0.2

0.4

EnvSteps

1.0
1e6

(b) Ablation on Œª

Figure 11: Ablation for Hyper-parameters of OS(Œª)
In addition, we perform an ablation study on the sampled scale (the action sampling times K and
simulation numbers N in MCTS), which has been shown to be essential for final performance in
Sampled MuZero (Hubert et al., 2021). Given the limitations of time and computational resources,
we only compare two cases: K = 5, N = 50 and K = 10, N = 100 for easy map 8m and hard
20

Published as a conference paper at ICLR 2024

map 5m vs 6m. Figure 10 reveals that our method yields better performance with a larger scale of
samples.
We also test the impact of œÅ and Œª in our Optimistic Search Lambda (OS(Œª)) algorithm on map
8m. We test œÅ = 0.65, 0.75, 0.85, 0.95 by fixing Œª = 0.8, and test Œª = 0.7, 0.8, 0.9, 1.0 by fixing
œÅ = 0.75 for MAZero. Figure 11 shows that the optimistic approach stably improves the sample
efficiency, and the Œª term is useful when dealing with the model error.
We perform an ablation study about MCTS planning in the evaluation stage. The MAZero algorithm
is designed under the CTDE framework, which means the global reward allows the agents to learn
and optimize their policies collectively during centralized training. The predicted global reward is
used in MCTS planning to search for a better policy based on the network prior. Table 8 shows that
agents maintain comparable performance without MCTS planning during evaluation, i.e., directly
using the final model and local observations without global reward, communication and MCTS
planning.
Map

Env steps

w MCTS

w/o MCTS

performance ratio

50k
50k
50k
100k
400k
1M
1M
1M

0.985 ¬± 0.015
1.0 ¬± 0.0
0.959 ¬± 0.023
0.948 ¬± 0.072
0.893 ¬± 0.114
0.875 ¬± 0.031
0.906 ¬± 0.092
0.922 ¬± 0.064

0.936 ¬± 0.107
1.0 ¬± 0.0
0.938 ¬± 0.045
0.623 ¬± 0.185
0.768 ¬± 0.182
0.821 ¬± 0.165
0.855 ¬± 0.127
0.863 ¬± 0.023

95.0 ¬± 10.8%
100 ¬± 0.0%
97.8 ¬± 4.7%
65.7 ¬± 19.5%
86.0 ¬± 20.4%
93.8 ¬± 18.9%
94.4 ¬± 14.0%
93.6 ¬± 2.5%

100%

90.1 ¬± 11.3%

3m
2m vs 1z
so many baneling
2s vs 1sc
2c vs 64zg
5m vs 6m
8m vs 9m
10m vs 11m
average performance

Table 8: Ablation for using MCTS during evaluation in SMAC environments

E

E XPERIMENTS ON S INGLE -AGENT ENVIRONMENTS

We have considered demonstrating the effectiveness of OS(Œª) and AWPO in single-agent decision
problems with large action spaces. This might help establish the general applicability of these techniques beyond the multi-agent SMAC benchmark.
We choose the classical LunarLander environment as the single-agent benchmark, but discretize
the 2-dimensional continuous action space into 400 discrete actions. Additionally, we select the
Walker2D scenario in MuJoCo environment and discretize each dimension of continuous action
space into 7 discrete actions, i.e., 67 ‚âà 280, 000 legal actions. Tables 9 and 10 illustrates the results
where both techniques greatly impact learning efficiency and final performance.
Environment

Env steps

MAZero

w/o OS(Œª) and AWPO

LunarLander

250k
500k
1M

184.6¬±22.8
259.8¬±12.9
276.9¬±2.9

104.0¬±87.5
227.7¬±56.3
274.1¬±3.5

Table 9: Ablation for using OS(Œª) and AWPO in LunarLander environments
Environment

Env steps

MAZero

w/o OS(Œª) and AWPO

TD3

SAC

Walker2D

300k
500k
1M

3424 ¬± 246
4507 ¬± 411
5189 ¬± 382

2302 ¬± 472
3859 ¬± 424
4266 ¬± 509

1101 ¬± 386
2878 ¬± 343
3946 ¬± 292

1989 ¬± 500
3381 ¬± 329
4314 ¬± 256

Table 10: Ablation for using OS(Œª) and AWPO in Walker2D environments

21

Published as a conference paper at ICLR 2024

F

E XPERIMENTS ON OTHER M ULTI - AGENT ENVIRONMENTS

It is beneficial to validate the performance of MAZero in other tasks beyond the SMAC benchmark. We further benchmark MAZero on Google Research Football(GRF) (Kurach et al., 2020),
academy pass and shoot with keeper scenario and compare our methods with several model-free
baseline algorithms. Table 11 shows that our method outperforms baselines in terms of sample
efficiency.
Environment
academy pass and
shoot with keeper

Env steps

MAZero

CDS

RODE

QMIX

500k
1M
2M

0.123 ¬± 0.089
0.214 ¬± 0.072
0.619 ¬± 0.114

0.069 ¬± 0.041
0.148 ¬± 0.117
0.426 ¬± 0.083

0
0
0.290 ¬± 0.104

0
0
0

Table 11: Comparisons against baselines in GRF.

G

D ERIVATION OF AWPO L OSS

To prove that AWPO loss (Equation (10)) is essentially solving the corresponding constrained optimization problem (Equation (11)), we only need to prove the closed form of Equation (11) is
Equation (12).
We follow the derivation in Nair et al. (2021). Note that the following optimization problem
Œ∑ ‚àó = arg maxEa‚àºŒ∑(¬∑|s) [A(s, a)]
Œ∑

s.t. KL (Œ∑(¬∑ | s)‚à•œÄ(¬∑ | s)) ‚â§ œµ
Z
Œ∑(a | s)da = 1.

(15)

a

has Lagrangian
L(Œ∑, Œª, Œ±) =Ea‚àºŒ∑(¬∑|s) [A(s, a)]
+ Œª (œµ ‚àí DKL (Œ∑(¬∑ | s)‚à•œÄ(¬∑ | s)))


Z
+ Œ± 1 ‚àí Œ∑(a | s)da .

(16)

a

Applying KKT condition, we have
‚àÇL
= A(s, a) + Œª log œÄ(a | s) ‚àí Œª log Œ∑(a | s) + Œª ‚àí Œ± = 0
‚àÇŒ∑

(17)

Solving the above equation gives
Œ∑ ‚àó (a | s) =

1
œÄ(a | s) exp
Z(s)




1
A(s, a)
Œª

(18)

where Z(s) is a normalizing factor.
To make the KKT condition hold, we can let Œ∑ > 0 and use the LICQ (Linear independence constraint qualification) condition.
Plugging the original problem (Equation (11)) into Equation (15) completes our proof.

22

