Paper: MA_RLHF.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Published as a conference paper at ICLR 2025  MA-RLHF: R EINFORCEMENT L EARNING FROM H U MAN F EEDBACK WITH M ACRO ACTIONS  arXiv:2410.02743v2 [cs.CL] 14 Feb 2025  Yekun Chai∗ Haoran Sun∗ Huang Fang Shuohuan Wang Yu Sun Hua Wu Baidu Inc. {chaiyekun,fanghuang,wangshuohuan}@baidu.com sunhaoran0402@gmail.com  A BSTRACT Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for ...

Specific Evidence for Classification:
- "Published as a conference paper at ICLR 2025  MA-RLHF: R EINFORCEMENT L EARNING FROM H U MAN F EEDBACK WITH M ACRO ACTIONS  arXiv:2410.02743v2 [cs.CL] 14 Feb 2025  Yekun Chai∗ Haoran Sun∗ Huang Fang Shuohuan Wang Yu Sun Hua Wu Baidu Inc."
- "{chaiyekun,fanghuang,wangshuohuan}@baidu.com sunhaoran0402@gmail.com  A BSTRACT Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences."

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
