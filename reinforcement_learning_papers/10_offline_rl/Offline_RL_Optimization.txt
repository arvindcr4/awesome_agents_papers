Published as a conference paper at ICLR 2025

OGB ENCH :
B ENCHMARKING O FFLINE G OAL -C ONDITIONED RL
Seohong Park1 Kevin Frans1 Benjamin Eysenbach2
1
University of California, Berkeley 2 Princeton University
seohong@berkeley.edu

Sergey Levine1

arXiv:2410.20092v2 [cs.LG] 13 Feb 2025

A BSTRACT
Offline goal-conditioned reinforcement learning (GCRL) is a major problem in reinforcement learning (RL) because it provides a simple, unsupervised, and domainagnostic way to acquire diverse behaviors and representations from unlabeled data
without rewards. Despite the importance of this setting, we lack a standard benchmark that can systematically evaluate the capabilities of offline GCRL algorithms.
In this work, we propose OGBench, a new, high-quality benchmark for algorithms
research in offline goal-conditioned RL. OGBench consists of 8 types of environments, 85 datasets, and reference implementations of 6 representative offline
GCRL algorithms. We have designed these challenging and realistic environments
and datasets to directly probe different capabilities of algorithms, such as stitching, long-horizon reasoning, and the ability to handle high-dimensional inputs and
stochasticity. While representative algorithms may rank similarly on prior benchmarks, our experiments reveal stark strengths and weaknesses in these different
capabilities, providing a strong foundation for building new algorithms.
Project page: https://seohong.me/projects/ogbench
Repository: https://github.com/seohongpark/ogbench

maze

antmaze

humanoidmaze antsoccer

visual-maze

cube

scene

puzzle

powderworld

Figure 1: OGBench Overview. OGBench provides a variety of state- and pixel-based locomotion, manipulation,
and drawing tasks that are designed to exercise diverse challenges in offline goal-conditioned RL, such as
stitching, long-horizon reasoning, and stochastic control.

1

M OTIVATION

Why offline goal-conditioned reinforcement learning (RL)? The enduring trend in modern machine learning is to simplify domain-specific assumptions and scale up the data. In computer vision
and natural language processing, the strongest general-purpose models are trained via simple unsupervised objectives on raw, unlabeled data, such as next-token prediction, contrastive learning, and
masked auto-encoding. What analogous paradigm could enable data-driven unsupervised learning for
reinforcement learning? Ideally, such a framework should be able to produce from data a generalist
policy that can be directly queried or adapted to solve a variety of downstream tasks, much like how
generative language models trained via next-token prediction can be easily adapted to everyday tasks.
1

Published as a conference paper at ICLR 2025

We posit that a natural analogy to data-driven unsupervised learning in RL is offline goal-conditioned
RL (GCRL). The objective of offline goal-conditioned RL is fully unsupervised, remarkably simple,
and requires no domain knowledge: it merely aims to learn to reach any state from any other state in
the dataset in the fewest number of steps. However, mastering this simple objective is exceptionally
difficult: the agent needs to not only acquire diverse skills to efficiently navigate the state space, but
also have a deep, complete understanding of the underlying world and dataset. As a result, offline
goal-conditioned RL yields a highly capable general-purpose multi-task policy as well as rich, useful
representations that can be adapted to solve a variety of downstream tasks (Ghosh et al., 2023; Kim
et al., 2024). Indeed, for such simplicity and generality, interest in (offline) goal-conditioned RL has
recently surged to the extent that even a standalone workshop on goal-conditioned RL was held at a
machine learning conference.1
Why a new benchmark? Despite the importance of and increasing interest in offline goal-conditioned
RL, we currently lack a standard benchmark that can systematically assess the capabilities of
offline GCRL algorithms, such as the ability to stitch, perform long-horizon reasoning, and handle
stochasticity. Prior works in offline goal-conditioned RL (Eysenbach et al., 2022; Ma et al., 2022;
Park et al., 2023; Myers et al., 2024) have mainly used either existing datasets for standard offline
RL tasks without modification (e.g., D4RL (Fu et al., 2020)), relatively simple goal-conditioned
tasks (e.g., Fetch (Plappert et al., 2018)), or tasks tailored to demonstrate the individual abilities
of the proposed methods. This often results in limited evaluation. Prior works often evaluate their
multi-task policies only on a single task (when using datasets not originally designed for offline
GCRL), or learn relatively simple behaviors. While there exist some prior tasks tailored to evaluate
individual properties of offline GCRL such as stitching or generalization (Yang et al., 2023; Ghugare
et al., 2024), we lack a comprehensive, standardized benchmark that exhaustively assesses various
properties of offline GCRL algorithms with diverse, challenging tasks.
Therefore, we introduce a benchmark named Offline Goal-Conditioned RL Benchmark (OGBench)
in this work (Figure 1). The primary goals of this benchmark are to facilitate algorithms research
in offline goal-conditioned RL and to provide a set of complex tasks that can unlock the potential
of offline GCRL. Our benchmark introduces 8 types of environments and 85 datasets across robotic
locomotion, robotic manipulation, and drawing, and provides well-tuned reference implementations
of 6 representative offline GCRL methods. These datasets, tasks, and implementations are carefully
designed. Tasks are designed in a way that complex behaviors can naturally emerge when successfully
solved, and that they pose diverse algorithmic challenges in offline GCRL, such as goal stitching,
stochastic control, long-horizon reasoning, and more. Dataset and task difficulties are carefully
adjusted to highlight stark contrasts between algorithms across multiple criteria. The entire benchmark
is designed to minimize unnecessary computational overhead and maximize usability such that any
researcher can easily iterate and evaluate new ideas. We believe OGBench serves as a solid foundation
for developing ideal algorithms for goal-conditioned and unsupervised RL from data.

2

P ROBLEM S ETTING

The offline goal-conditioned RL problem is defined by a controlled Markov process M = (S, A, µ, p)
(i.e., a Markov decision process (MDP) without rewards) and an unlabeled dataset D, where S denotes
the state space, A denotes the action space, µ(s) ∈ ∆(S)2 denotes the initial state distribution,
and p(s′ | s, a) : S × A → ∆(S) denotes the transition dynamics function. ∆(X ) denotes the set
of probability distributions defined on a set X . The dataset D = {τ (n) }n∈{1,2,...,N } consists of
(n)

(n)

(n)

(n)

unlabeled trajectories τ (n) = (s0 , a0 , s1 , . . . , sTn ).
The objective of offline goal-conditioned RL is to learn to reach any state from any other state in the
minimum number of time steps. Formally, offline GCRL aims to learn a goal-conditioned policy
PT
π(a | s, g) : S × S → ∆(A) that maximizes the objective Eτ ∼p(τ |g) [ t=0 γ t δg (st )] for all g ∈ S,
where T ∈ N denotes the episode horizon, γ ∈ (0, 1) denotes the discount factor, p(τ | g) denotes the
distribution given by p(τ | g) = µ(s0 )π(a0 | s0 , g)p(s1 | s0 , a0 ) · · · p(sT | sT −1 , aT −1 ), and δg (·)
1
2

https://goal-conditioned-rl.github.io/2023/
We denote placeholder variables in gray throughout the paper.

2

Published as a conference paper at ICLR 2025

Table 1: Properties of benchmark tasks. We summarize the properties of benchmark tasks commonly used in
prior works (above the line) and our OGBench tasks (below the line).
Benchmark Task

Type1 Longest Task2 # Subtasks3 Test Stitching?4 Have Stoch. Tasks?5 Support Pixels?6 Multi-Goal?7 Dependency8

D4RL AntMaze
D4RL Kitchen
Roboverse
Fetch

Loco.
Manip.
Manip.
Manip.

≈ 400
≈ 250
≈ 100
≈ 50

4
1-2
1

MuJoCo
MuJoCo
PyBullet
MuJoCo

PointMaze (ours)
Loco.
AntMaze (ours)
Loco.
HumanoidMaze (ours) Loco.
AntSoccer (ours)
Loco.
Cube (ours)
Manip.
Scene (ours)
Manip.
Puzzle (ours)
Manip.
Powderworld (ours)
Draw.

≈ 600
≈ 1000
≈ 3000
≈ 1000
≈ 400
≈ 400
≈ 800
≈ 100

1-4
2-8
2-24
-

MuJoCo
MuJoCo
MuJoCo
MuJoCo
MuJoCo
MuJoCo
MuJoCo
NumPy

1

2
Environment type (locomotion, manipulation, or drawing).
The (approximate) minimum number of environment steps to solve the longest task.
4
The number of atomic behaviors (e.g., pick-and-place) in each manipulation task.
Does it contain tasks that require goal stitching?
6
7
Does it contain tasks with stochastic dynamics?
Does it support pixel-based observations?
Does it use multiple goals for evaluation?
8
The main dependency of the benchmark.
3
5

denotes the Dirac delta “function”3 at g. Note that we use the entire state space as the goal space
(i.e., a goal is simply a full state, not part of the state like only the x-y position of the agent). This
choice makes the objective fully unsupervised, making it suitable for domain-agnostic training from
unlabeled data. Our goal in this paper is to propose a new benchmark in offline GCRL. That is,
formally speaking, to specify the dynamics and dataset for each task we introduce.

3

H OW H AVE P RIOR W ORKS B ENCHMARKED O FFLINE GCRL?

While many excellent offline GCRL algorithms have been proposed so far, the community currently
lacks a standardized way to evaluate their performance, unlike other fields in RL (Brockman et al.,
2016; Tassa et al., 2018; Fu et al., 2020; Terry et al., 2021). Tasks used by prior works in offline
GCRL are often limited for providing a proper evaluation for various reasons. For example, many
works directly use tasks in existing offline RL benchmarks (not necessarily designed for offline
goal-conditioned RL), such as D4RL AntMaze and Kitchen (Fu et al., 2020). However, since the
tasks are designed for single-task offline RL, they often evaluate their multi-task policies only on
the single, original goal when using these tasks (Eysenbach et al., 2022; Park et al., 2023; Zheng
et al., 2024b; Myers et al., 2024), which results in limited evaluation. Some employ online GCRL
tasks provided by Plappert et al. (2018) (e.g., Fetch tasks) with policy-collected datasets (Ma et al.,
2022; Yang et al., 2023), but these tasks are mostly atomic (e.g., single pick-and-place) and do not
sufficiently address various challenges in offline GCRL, such as long-horizon reasoning and goal
stitching. While Roboverse (Fang et al., 2022; Zheng et al., 2024a) provides pixel-based manipulation
tasks, the tasks are still relatively atomic and it requires installing multiple, fragmented dependencies,
making it less approachable for researchers. Some prior works construct bespoke tasks and datasets
to individually study the important and specific features of their algorithms (e.g., stitching (Ma et al.,
2022; Ghugare et al., 2024; Wang et al., 2024) and stochasticity (Myers et al., 2024)), but it is often
not entirely clear how algorithms compare to one another or if the new capabilities of new methods
(e.g., stitching) come at the loss of other capabilities. These limitations of previous evaluation tasks
have motivated us to create a new benchmark. In this work, we introduce a set of diverse tasks
that cover various challenges in offline GCRL, enabling a much more thorough and multi-faceted
evaluation than previous tasks. We summarize the properties of the previous tasks and our new tasks
in Table 1 and refer to Appendix C for further discussion on related work.

4

OVERVIEW OF OGB ENCH

We now introduce our benchmark, Offline Goal-Conditioned RL Benchmark (OGBench). The
primary goal of this benchmark is to provide tasks and datasets to unlock the full potential of
offline goal-conditioned RL. To this end, we pose diverse challenges in offline GCRL throughout
the benchmark in such a way that researchers can easily test and iterate on algorithmic ideas, and
3
In a discrete MDP, δg (s) is equal to an indicator function 1{g} (s). In a continuous MDP, it is technically
not well-defined in its current form. It can be made precise using measure-theoretic notation or the distribution
theory, but we choose to avoid them for simplicity.

3

Published as a conference paper at ICLR 2025

that complex, intriguing behaviors can naturally emerge when successful. OGBench consists of 8
types of environments, 85 datasets, and reference implementations of 6 representative offline GCRL
algorithms. In the following sections, we first describe the challenges in offline GCRL (Section 5)
and then outline our core design philosophies (Section 6). We next introduce the tasks and datasets
(Section 7) and present the benchmarking results of the current algorithms (Section 8).

5

C HALLENGES IN O FFLINE G OAL -C ONDITIONED RL

Offline goal-conditioned RL, despite its simplicity, is a challenging problem. Here, we discuss the
major challenges in offline GCRL, which will motivate the design choices in our benchmark tasks.
(1) Learning from suboptimal, unstructured data: An ideal offline GCRL algorithm should be
able to learn an effective multi-task policy from diverse and suboptimal data. This is especially
important, considering that suboptimal (yet diverse) data is much cheaper to collect than curated,
expert datasets (Lynch et al., 2019), and that the very use of large, diverse, unstructured data is one of
the foundations for the success of modern machine learning. Reflecting this challenge, we provide
datasets with high diversity and varying suboptimality in this benchmark to challenge the capabilities
of offline GCRL algorithms.
(2) Goal stitching: Another important challenge is to stitch the initial and final states of different
trajectories to learn more diverse behaviors. We call this “goal stitching.” Goal stitching is different
from “regular” stitching in offline RL, which applies only when the dataset is suboptimal. Unlike
regular stitching, goal stitching applies even when the dataset only consists of optimal, expert
trajectories, because we can often acquire more diverse goal-reaching behaviors by stitching multiple
trajectories together, regardless of their optimality. For instance, an agent can stitch two atomic pickand-place behaviors to sequentially move two objects in a single episode, even when the dataset
does not contain any double pick-and-place behaviors. Goal stitching is crucial to learning diverse
behaviors in many real-world applications with high behavioral diversity and large state spaces. In
our benchmark, we introduce many tasks with large state spaces to assess the ability to stitch goals.
(3) Long-horizon reasoning: Long-horizon reasoning refers to the capability of navigating from a
starting state to a goal state that is many steps apart. This challenge is important in many real-world
tasks like autonomous driving or assembly, which may require several hours of continuous control or
achieving dozens of subtasks. To substantially challenge the long-horizon reasoning ability of offline
GCRL methods, we introduce tasks that are more than 5 times longer than previously used ones in
terms of both the episode length and the number of subtasks (Table 1).
(4) Handling stochasticity: Another prominent challenge in offline GCRL is the ability to deal with
stochastic environments. Correctly handling stochasticity is very important in practice, because virtually any real-world environment is stochastic due to partial observability. Yet, many works in offline
GCRL assume deterministic dynamics to exploit the metric structure of temporal distances (Wang
et al., 2023) or to enable hierarchical control (Park et al., 2023), at the cost of being optimistically
biased in stochastic environments (Wang et al., 2023; Park et al., 2023). Correctly handling environment stochasticity while fully exploiting the recursive subgoal structure of GCRL remains an open
problem. Since most previous tasks used to evaluate offline GCRL methods have deterministic dynamics (Table 1), we introduce several challenging tasks with stochastic dynamics in this benchmark.

6

D ESIGN P RINCIPLES

Next, we discuss the design principles underlying our benchmark tasks. The tasks are intended to
exercise the major challenges in offline GCRL in the previous section, while providing a set of highquality tasks that provide not only a toolkit for algorithms research and evaluation, but also a platform
for vividly illustrating the capabilities of offline GCRL with compelling and complex domains.
(1) Realistic and exciting tasks: The tasks should be realistic yet exciting enough while posing
diverse challenges in offline goal-conditioned RL. Imagine a robot arm watching random movements
of a puzzle and then solving it zero-shot at test time, a humanoid robot navigating through a labyrinth,
or an agent painting cool pictures using different types of brushes. In this benchmark, we design new
tasks such that these kinds of exciting behaviors can naturally emerge when an RL agent properly
stitches different trajectory segments together (up to 24; see Table 1) or successfully generalizes. At
4

Published as a conference paper at ICLR 2025

the same time, we make sure our tasks exhaustively cover major challenges in offline goal-conditioned
RL, such as long-horizon reasoning, stochastic control, and combinatorial generalization via goal
stitching (Section 5).
(2) Appropriate difficulty: The tasks and datasets should have appropriate levels of difficulty to
properly evaluate different algorithms. In other words, they should be of high quality for benchmarking. No matter how intricate or compelling a task is, it will fail to provide a useful signal for benchmarking if it is too easy, too hard, unsolvable from the given dataset, or does not clearly distinguish
between more or less effective methods. In this work, we carefully curate and adjust the difficulty
of each task and dataset such that they can provide effective guidance for algorithms research. For
some tasks, we provide multiple versions with varying difficulty, all the way up to tasks that are difficult to solve with current methods, so that the same benchmark can continuously be used to develop
new methods even in the future. Our rule of thumb is to have, for each type of tasks, at least one
task where the current state-of-the-art offline GCRL method achieves a success rate of 20-30%. This
ensures that the task is solvable from the dataset while leaving significant room for improvement.
(3) Controllable datasets: The benchmark should provide tools to control and adjust datasets for
scientific research and ablation studies. Verifying the effectiveness of algorithms in real-world
problems is surely important. However, for algorithms research, it is equally, if not more, important
to provide analysis tools to enable a rigorous, scientific understanding of challenges and algorithms.
Hence, instead of employing fixed, human-collected data, which does not always provide clear
benchmarking signals for algorithms research, we choose to focus on simulated environments and
synthetic datasets that we have full control of, and provide tools to reproduce and adjust them easily.
We note that many algorithmic ideas in RL that have made a major impact, such as DQN (Mnih et al.,
2013), PPO (Schulman et al., 2017), and CQL (Kumar et al., 2020), were originally developed in
simulated environments. Even in natural language processing, studies on synthetic, controlled datasets
have revealed the mechanisms and limitations of language models with scientific evidence (AllenZhu & Li, 2023a), and these insights have transferred to real scenarios (Allen-Zhu & Li, 2023b). We
demonstrate how such controllability of datasets reveals challenges and design principles in offline
GCRL in Section 8.2.
(4) Minimal compute requirements: The tasks should be designed to minimize unnecessary
computational overhead so that as many researchers as possible, including those from small labs
and underprivileged backgrounds, can quickly iterate on their new algorithmic ideas. This does not
mean that the tasks should be easy (indeed, some of our tasks are very challenging (but solvable)!);
it means the benchmark should focus mainly on algorithmic challenges (e.g., not requiring highresolution image processing). In our benchmark, we provide both state- and pixel-based observations
whenever possible, and minimize the size of image observations (up to 64 × 64 × 3) to reduce the
computational burden. Moreover, we carefully adjust colors, transparency, and lighting for imagebased tasks to enable pixel-based control without high-resolution images or multiple views.
(5) High code quality: The reference implementations should be very clean and well-tuned so that
researchers can directly use our implementations to build their ideas, and the benchmark should be
very easy to set up. Our benchmark environments only depend on MuJoCo (Todorov et al., 2012)
and do not require any other dependencies (Table 1). For reference implementations, we minimize
the number of file dependencies for each algorithm, largely following the spirit of the single-file
implementations of recent RL libraries (Huang et al., 2022; Tarasov et al., 2023), while maintaining a
minimal amount of additional modularity. We also extensively test and tune different design choices
and hyperparameters for each offline GCRL algorithm, to the degree that several methods achieve
even better performances than their original performances reported on previous benchmarks (Table 3).

7

E NVIRONMENTS , TASKS , AND DATASETS

We now introduce the environments, tasks, and datasets in our benchmark. They can be broadly
categorized into three groups: locomotion, manipulation, and drawing. We provide a separate
validation dataset for each dataset, and most tasks support both state- and pixel-based observations.
Videos are available at https://seohong.me/projects/ogbench.
Evaluation. Each task in OGBench accompanies five pre-defined state-goal pairs for evaluation
(Appendix F). Performance is measured by the average success rate across the five evaluation goals.
5

Published as a conference paper at ICLR 2025

For each pre-define state-goal pair, we perform multiple rollouts with slightly randomized initial and
goal states. In each evaluation episode, a goal g ∈ S (which is simply another state; see Section 2) is
given to the agent, and the episode immediately terminates when the agent reaches the goal. Each
task has its own goal success criterion, which we describe in Appendix E.1.
Variants. While OGBench is mainly designed for (unsupervised) offline goal-conditioned RL, it
supports several other problem settings as well. Before introducing our environments and datasets,
we briefly mention two variants of OGBench tasks for standard offline RL and supervised offline
goal-conditioned RL.
• singletask: Single-task variants are designed for standard (i.e., non-goal-conditioned) offline
RL. To convert a goal-conditioned task into a standard reward-maximizing task, we fix an
evaluation goal and relabel the dataset with the corresponding reward function. Each environment
provides five single-task tasks (for each of the five evaluation goals), which brings the total
number of single-task tasks to 410. See Appendix E.2 for the details.
• oraclerep: Oracle representation variants are designed for “supervised” offline goalconditioned RL. In this setting, we provide low-dimensional oracle goal representations that contain only relevant information for fulfilling the goal success criterion (e.g., only the x-y position
of the agent in maze navigation environments). This reduces the burden of goal representation
learning and may potentially be useful for analysis purposes. See Appendix E.3 for details.
7.1

L OCOMOTION TASKS

We provide four types of locomotion environments, PointMaze, AntMaze, HumanoidMaze, and
AntSoccer, with diverse variants. These environments are designed to test the agent’s long-horizon
and hierarchical reasoning abilities. They are based on the MuJoCo simulator (Todorov et al., 2012).

pointmaze

antmaze

humanoidmaze

PointMaze (pointmaze), AntMaze (antmaze), and HumanoidMaze (humanoidmaze). Maze
navigation is one of the most widely used tasks for benchmarking offline GCRL algorithms. We
provide three different types of maze navigation tasks: PointMaze, which involves controlling a 2-D
point mass, AntMaze, which involves controlling a quadrupedal Ant agent with 8 degrees of freedom
(DoF), and HumanoidMaze, which involves controlling a much more complex 21-DoF Humanoid
agent. The aim of these tasks is to control the agent to reach a goal location in the given maze. The
agent must learn both the high-level maze navigation and low-level locomotion skills that involve
high-dimensional control, purely from diverse offline trajectories.
In our benchmark, we substantially extend the original PointMaze and AntMaze tasks proposed by
D4RL (Fu et al., 2020). Unlike the original D4RL tasks, which only support Point and Ant agents
and do not challenge stitching4 , stochasticity, or pixel-based control, we support Humanoid control,
pixel-based observations, and multi-goal evaluation, while providing more challenging and diverse
types of mazes and datasets. The supported maze types are as follows:

4

See Ghugare et al. (2024) for the details about this point.

6

Published as a conference paper at ICLR 2025

medium

large

giant

teleport

• medium: This is the smallest maze, with the same layout as the original medium maze in D4RL.
• large: This is a larger maze, with the same layout as the original large maze in D4RL.
• giant: This is the largest maze, twice the size of large. It has the same size as the previous
antmaze-ultra maze by Jiang et al. (2023), but its layout is more challenging and contains
longer paths that require up to 3000 environment steps (in the case of Humanoid). This maze is
designed to substantially challenge the long-horizon reasoning capability of the agent.
• teleport: This maze is specially designed to challenge the agent’s ability to handle environment stochasticity. It has the same size as large, but contains multiple stochastic teleporters.
If the agent enters a black hole, it is immediately sent to a randomly chosen white hole. However, since one of the three white holes is a dead-end, there is always a risk in taking a teleporter.
The agent therefore must learn to avoid the black holes, without being optimistically biased by
“lucky” outcomes.
On these mazes, we collect datasets with a low-level directional policy trained via SAC (Haarnoja et al.,
2018) and a high-level waypoint controller. For each maze type, we provide three types of datasets
that pose different kinds of challenges (the figures below show example trajectories in these datasets):

navigate

stitch

explore

• navigate: This is the standard dataset, collected by a noisy expert policy that navigates the
maze by repeatedly reaching randomly sampled goals.
• stitch: This dataset is designed to challenge the agent’s stitching ability. It consists of short
goal-reaching trajectories, where the length of each trajectory is at most 4 cell units. Hence, the
agent must be able to stitch multiple trajectories (up to 8) together to complete the tasks.
• explore: This dataset is designed to test whether the agent can learn navigation skills from
extremely low-quality (yet high-coverage) data. It consists of random exploratory trajectories,
collected by commanding the low-level policy with random directions re-sampled every 10
steps, with a large amount of action noise.
We provide two types of observation modalities:

colored maze

pixels

• States: This is the default setting, where the agent has access to the full low-dimensional state
representation, including its current x-y position.
• Pixels (visual): This requires pure pixel-based control, where the agent only receives 64×64×3
RGB images rendered from a third-person camera viewpoint. Following Park et al. (2023),
7

Published as a conference paper at ICLR 2025

we color the floor to enable the agent to infer its location from the images, obviating the need
for a potentially expensive memory component. However, unlike Park et al. (2023), which
additionally provides proprioceptive information, we do not provide any low-dimensional state
information like joint angles; the agent must learn purely from image observations.

antsoccer

arena

medium

AntSoccer (antsoccer). To provide a more diverse type of locomotion task beyond simple maze
navigation, we introduce a new locomotion task, AntSoccer. This task involves controlling an Ant
agent to dribble a soccer ball. It is inspired by the quadruped-fetch task in the DeepMind Control
suite (Tassa et al., 2018). AntSoccer is significantly harder than AntMaze because the agent must
also carefully control the ball while navigating the environment. We provide two maze types: arena,
which is an open space without walls, and medium, which is the same maze as the medium one
in AntMaze. For datasets, we provide navigate and stitch. The navigate datasets consist of
trajectories where the agent repeatedly approaches the ball and dribbles it to random locations. The
stitch datasets consist of two different types of trajectories, maze navigation without the ball and
dribbling with the ball near the agent, so that stitching is required to complete the full task. AntSoccer
only supports state-based observations.
7.2

M ANIPULATION TASKS

We provide a manipulation suite with three types of robotic manipulation tasks, Cube, Scene, and
Puzzle, with diverse difficulties and complexities. They are designed to test the agent’s object manipulation, sequential generalization, and combinatorial generalization abilities. These environments are
based on the MuJoCo simulator (Todorov et al., 2012) and a 6-DoF UR5e robot arm (Zakka et al.,
2022). On these tasks, we provide “play”-style datasets (play) (Lynch et al., 2019) collected by nonMarkovian expert policies with temporally correlated noise. To support more diverse types of research (e.g., dataset ablation studies), we additionally provide more noisy datasets (noisy) collected
by Markovian expert policies with uncorrelated Gaussian noise, which we describe in Appendix B.

states

pixels

For all manipulation tasks, we support both state-based observations and pixel-based observations
with 64 × 64 × 3 RGB camera images. For pixel observations, we adjust colors and make the arm
transparent to ensure full observability. The transparent arm in the figure might appear challenging,
but the colors and transparency are carefully adjusted to minimize difficulties in visual perception, to
the extent that some methods achieve even better performance with pixels (see Table 2).

8

Published as a conference paper at ICLR 2025

single

double

cube

triple

quadruple

Cube (cube). This task involves pick-and-place manipulation of cube blocks, whose goal is to control
a robot arm to arrange cubes into designated configurations. We provide four variants, single,
double, triple, and quadruple, with different numbers (1-4) of cubes. We provide “play”-style
datasets collected by a scripted policy that repeatedly picks a random block and places it in other
random locations or on another block. At test time, the agent is given goal configurations that
require moving, stacking, swapping, or permuting cube blocks. Hence, the agent must learn not
only generalizable multi-object pick-and-place behaviors from unstructured random trajectories in
the dataset, but also long-term plans to achieve the tasks (e.g., permuting blocks requires non-trivial
sequential and logical reasoning).

1

3

2

unlock drawer

open drawer

4

put cube in drawer

scene

close drawer

Scene (scene). This task is designed to challenge the sequential, long-horizon reasoning capabilities
of the agent. It involves manipulating diverse everyday objects, such as a cube block, a window, a
drawer, and two button locks, where pressing a button toggles the lock status of the corresponding
object (the drawer or window). We provide “play”-style datasets collected by scripted policies that
randomly interact with these objects. At test time, the agent is commanded to arrange the objects
into a desired configuration. Evaluation tasks require a significant degree of sequential reasoning: for
instance, some tasks require unlocking the drawer, opening it, putting the cube in the drawer, and
closing it again (see the figure above), and the longest task involves eight atomic behaviors. Hence,
the agent must be able to plan and sequentially combine learned manipulation skills.

3x3

4x4

puzzle

4x5

4x6

Puzzle (puzzle). This task is designed to test the combinatorial generalization abilities of the
agent. It requires solving the “Lights Out” puzzle5 with a robot arm. The puzzle consists of a twodimensional array of buttons (e.g., a 4 × 6 grid), where pressing a button toggles the colors of the
pressed button and the buttons adjacent to it (typically four, except on the edges and corners; see
videos). The goal is to achieve a desired configuration of colors (e.g., turning all the buttons blue) by
pressing an appropriate combination of buttons. Since these buttons are implemented in the MuJoCo
5

https://en.wikipedia.org/wiki/Lights_Out_(game)

9

Published as a conference paper at ICLR 2025

simulator, the agent must control a robot arm to physically press the buttons. We provide four levels of
difficulty, 3x3, 4x4, 4x5, and 4x6, with different grid sizes. The datasets are collected by a scripted
policy that randomly presses buttons in arbitrary sequences. Given the enormous state space of this
task (with up to 224 = 16,777,216 distinct button states), the agent must achieve combinatorial
generalization while mastering low-level continuous control. Some evaluation task in the hardest
puzzle requires pressing more than 20 buttons, which also substantially challenges the long-horizon
reasoning capabilities of the agent. This might sound very challenging (and it is!), but we provide
different levels and enough data to ensure that they provide meaningful research signals and are
solvable (see the results in Section 8).
7.3

D RAWING TASKS

powderworld
Powderworld (powderworld). To provide more diverse tasks beyond robotic locomotion or manipulation, we introduce a drawing task, Powderworld (Frans & Isola, 2023)6 , which presents unique
challenges with extremely high intrinsic dimensionality. The goal of Powderworld is to draw a target
picture on a 32 × 32 grid using different types of “powder” brushes, where each powder brush has a
distinct physical property corresponding to a unique element. For example, the “sand” brush falls
down and piles up, and the “fire” brush burns combustible elements like “plant.” We provide three
versions of tasks, easy, medium, and hard, with different numbers of available elements (2, 5, and 8
elements, respectively). The datasets are collected by a random policy that keeps drawing arbitrary
shapes with random brushes. This Powderworld task poses unique challenges that are distinct from
the other tasks in the benchmark. First, the agent must deal with the high intrinsic dimensionality of
the states, which presents a substantial challenge in representation learning. Second, since the transitions of powder elements are mostly stochastic and unpredictable, the agent must be able to correctly
handle environment stochasticity. Third, the agent must achieve a high degree of generalization and
sequential reasoning through a deep understanding of the physics, in order to complete symmetrical,
orderly test-time drawing tasks from random, chaotic data.

8

R ESULTS

We now present and discuss the benchmarking results of existing offline goal-conditioned RL
algorithms on OGBench.
8.1

A LGORITHMS

We benchmark six representative offline GCRL algorithms: goal-conditioned behavioral cloning
(GCBC) (Lynch et al., 2019; Ghosh et al., 2021), goal-conditioned implicit {V, Q}-learning (GCIVL
and GCIQL) (Kostrikov et al., 2022; Park et al., 2023), quasimetric RL (QRL) (Wang et al., 2023),
contrastive RL (CRL) (Eysenbach et al., 2022), and hierarchical implicit Q-learning (HIQL) (Park
et al., 2023). GCBC is the simplest goal-conditioned behavioral cloning method. GCIVL and
GCIQL are offline GCRL algorithms that approximates the optimal value function using an expectile
regression (Newey & Powell, 1987). QRL is a non-traditional GCRL method that fits a quasimetric
value function with a dual objective. CRL is a “one-step” RL algorithm that fits a Monte Carlo value
function via contrastive learning and performs one-step policy improvement. HIQL is a hierarchical
RL method that extracts a two-level hierarchical policy from a single GCIVL value function. For
benchmarking, we perform a similar amount of hyperparameter tuning for each method to ensure fair
comparison. We refer the reader to Appendices D and E for the details.
6

Play here: https://kvfrans.com/powder/

10

Published as a conference paper at ICLR 2025

Table 2: Full benchmark table. We report each method’s average (binary) success rate (%) across the five testtime goals on each task. The results are averaged over 8 seeds (4 seeds for pixel-based tasks), and we report the
standard deviations after the ± sign. Numbers at or above 95% of the best value in the row are highlighted in bold.
Environment

Dataset Type

Dataset

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

navigate

pointmaze-medium-navigate-v0
pointmaze-large-navigate-v0
pointmaze-giant-navigate-v0
pointmaze-teleport-navigate-v0

9 ±6
29 ±6
1 ±2
25 ±3

63 ±6
45 ±5
0 ±0
45 ±3

53 ±8
34 ±3
0 ±0
24 ±7

82 ±5
86 ±9
68 ±7
4 ±4

29 ±7
39 ±7
27 ±10
24 ±6

79 ±5
58 ±5
46 ±9
18 ±4

stitch

pointmaze-medium-stitch-v0
pointmaze-large-stitch-v0
pointmaze-giant-stitch-v0
pointmaze-teleport-stitch-v0

23 ±18
7 ±5
0 ±0
31 ±9

70 ±14
12 ±6
0 ±0
44 ±2

21 ±9
31 ±2
0 ±0
25 ±3

80 ±12
84 ±15
50 ±8
9 ±5

0 ±1
0 ±0
0 ±0
4 ±3

74 ±6
13 ±6
0 ±0
34 ±4

navigate

antmaze-medium-navigate-v0
antmaze-large-navigate-v0
antmaze-giant-navigate-v0
antmaze-teleport-navigate-v0

29 ±4
24 ±2
0 ±0
26 ±3

72 ±8
16 ±5
0 ±0
39 ±3

71 ±4
34 ±4
0 ±0
35 ±5

88 ±3
75 ±6
14 ±3
35 ±5

95 ±1
83 ±4
16 ±3
53 ±2

96 ±1
91 ±2
65 ±5
42 ±3

stitch

antmaze-medium-stitch-v0
antmaze-large-stitch-v0
antmaze-giant-stitch-v0
antmaze-teleport-stitch-v0

45 ±11
3 ±3
0 ±0
31 ±6

44 ±6
18 ±2
0 ±0
39 ±3

29 ±6
7 ±2
0 ±0
17 ±2

59 ±7
18 ±2
0 ±0
24 ±5

53 ±6
11 ±2
0 ±0
31 ±4

94 ±1
67 ±5
2 ±2
36 ±2

explore

antmaze-medium-explore-v0
antmaze-large-explore-v0
antmaze-teleport-explore-v0

2 ±1
0 ±0
2 ±1

19 ±3
10 ±3
32 ±2

13 ±2
0 ±0
7 ±3

1 ±1
0 ±0
2 ±2

3 ±2
0 ±0
20 ±2

37 ±10
4 ±5
34 ±15

navigate

humanoidmaze-medium-navigate-v0
humanoidmaze-large-navigate-v0
humanoidmaze-giant-navigate-v0

8 ±2
1 ±0
0 ±0

24 ±2
2 ±1
0 ±0

27 ±2
2 ±1
0 ±0

21 ±8
5 ±1
1 ±0

60 ±4
24 ±4
3 ±2

89 ±2
49 ±4
12 ±4

stitch

humanoidmaze-medium-stitch-v0
humanoidmaze-large-stitch-v0
humanoidmaze-giant-stitch-v0

29 ±5
6 ±3
0 ±0

12 ±2
1 ±1
0 ±0

12 ±3
0 ±0
0 ±0

18 ±2
3 ±1
0 ±0

36 ±2
4 ±1
0 ±0

88 ±2
28 ±3
3 ±2

navigate

antsoccer-arena-navigate-v0
antsoccer-medium-navigate-v0

5 ±1
2 ±0

47 ±3
4 ±1

50 ±2
7 ±1

8 ±2
2 ±2

23 ±2
3 ±1

58 ±2
13 ±2

stitch

antsoccer-arena-stitch-v0
antsoccer-medium-stitch-v0

24 ±8
2 ±1

21 ±3
1 ±0

2 ±0
0 ±0

1 ±1
0 ±0

1 ±0
0 ±0

15 ±1
4 ±1

navigate

visual-antmaze-medium-navigate-v0
visual-antmaze-large-navigate-v0
visual-antmaze-giant-navigate-v0
visual-antmaze-teleport-navigate-v0

11 ±2
4 ±0
0 ±0
5 ±1

22 ±2
5 ±1
1 ±1
8 ±1

11 ±1
4 ±1
0 ±0
6 ±1

0 ±0
0 ±0
0 ±0
6 ±3

94 ±1
84 ±1
47 ±2
48 ±2

93 ±4
53 ±9
6 ±4
37 ±2

stitch

visual-antmaze-medium-stitch-v0
visual-antmaze-large-stitch-v0
visual-antmaze-giant-stitch-v0
visual-antmaze-teleport-stitch-v0

67 ±4
24 ±3
0 ±0
32 ±3

6 ±2
1 ±1
0 ±0
1 ±1

2 ±0
0 ±0
0 ±0
1 ±0

0 ±0
1 ±1
0 ±0
1 ±2

69 ±2
11 ±3
0 ±0
32 ±6

87 ±2
28 ±2
0 ±0
37 ±4

explore

visual-antmaze-medium-explore-v0
visual-antmaze-large-explore-v0
visual-antmaze-teleport-explore-v0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
1 ±0

0 ±0
0 ±0
19 ±8

navigate

visual-humanoidmaze-medium-navigate-v0
visual-humanoidmaze-large-navigate-v0
visual-humanoidmaze-giant-navigate-v0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

1 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

stitch

visual-humanoidmaze-medium-stitch-v0
visual-humanoidmaze-large-stitch-v0
visual-humanoidmaze-giant-stitch-v0

1 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

1 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0

cube

play

cube-single-play-v0
cube-double-play-v0
cube-triple-play-v0
cube-quadruple-play-v0

6 ±2
1 ±1
1 ±1
0 ±0

53 ±4
36 ±3
1 ±0
0 ±0

68 ±6
40 ±5
3 ±1
0 ±0

5 ±1
1 ±0
0 ±0
0 ±0

19 ±2
10 ±2
4 ±1
0 ±0

15 ±3
6 ±2
3 ±1
0 ±0

scene

play

scene-play-v0

5 ±1

42 ±4

51 ±4

5 ±1

19 ±2

38 ±3

puzzle

play

puzzle-3x3-play-v0
puzzle-4x4-play-v0
puzzle-4x5-play-v0
puzzle-4x6-play-v0

2 ±0
0 ±0
0 ±0
0 ±0

6 ±1
13 ±2
7 ±1
10 ±2

95 ±1
26 ±3
14 ±1
12 ±1

1 ±0
0 ±0
0 ±0
0 ±0

3 ±1
0 ±0
1 ±0
4 ±1

12 ±2
7 ±2
4 ±1
3 ±1

visual-cube

play

visual-cube-single-play-v0
visual-cube-double-play-v0
visual-cube-triple-play-v0
visual-cube-quadruple-play-v0

5 ±1
1 ±1
15 ±2
8 ±1

60 ±5
10 ±2
14 ±2
0 ±0

30 ±5
1 ±1
15 ±1
7 ±1

41 ±15
5 ±0
16 ±1
5 ±1

31 ±15
2 ±1
17 ±2
4 ±1

89 ±0
39 ±2
21 ±0
14 ±1

visual-scene

play

visual-scene-play-v0

12 ±2

25 ±3

12 ±2

10 ±1

11 ±2

49 ±4

visual-puzzle

play

visual-puzzle-3x3-play-v0
visual-puzzle-4x4-play-v0
visual-puzzle-4x5-play-v0
visual-puzzle-4x6-play-v0

0 ±0
10 ±1
5 ±2
2 ±1

21 ±1
60 ±5
17 ±1
15 ±1

1 ±2
16 ±4
7 ±2
2 ±1

1 ±1
0 ±0
0 ±0
0 ±0

0 ±0
10 ±6
6 ±1
3 ±1

73 ±8
60 ±41
13 ±9
9 ±6

powderworld

play

powderworld-easy-play-v0
powderworld-medium-play-v0
powderworld-hard-play-v0

0 ±0
1 ±1
0 ±0

99 ±1
50 ±4
4 ±3

93 ±5
16 ±5
0 ±0

12 ±2
3 ±1
0 ±0

22 ±5
1 ±1
0 ±0

33 ±9
22 ±14
1 ±1

pointmaze

antmaze

humanoidmaze

antsoccer

visual-antmaze

visual-humanoidmaze

8.2

B ENCHMARKING R ESULTS AND Q&A S

We present the full benchmarking results in Table 2. Figure 2 summarizes the results by showing
performances grouped by different task categories. Performances are measured by average (binary)
success rates on the five test-time goals of each task. We train the agents for 1M gradient steps (500K
11

Published as a conference paper at ICLR 2025

Manipulation

60

40

40

20

20

0

0

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

60

60

40

40

20

20

0

0

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

60

60

40

40

20

20

0

0

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

Exploratory
pixels

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

Success Rate (%)

Success Rate (%)

Stitching
states

Drawing

pixels

states

pixels

60

60

40

40

20

20

0

0

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

pixels
60
40
20
0
C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

Stochastic
Success Rate (%)

60

states

Success Rate (%)

pixels

Success Rate (%)

Success Rate (%)

Locomotion
states

states

pixels

60

60

40

40

20

20

0

0
C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

C L L L L L
B IV IQ QR CR IQ
C
H
G GC GC

Figure 2: Benchmarking offline GCRL methods. We report the performances of six offline GCRL methods
(GCBC, GCIVL, GCIQL, QRL, CRL, and HIQL), aggregated by different dataset categories (see Table 6 for
the category list). The results are averaged over the tasks in each category, and then over 8 seeds (4 seeds for
pixel-based tasks). Error bars denote 95% bootstrap confidence intervals. See Table 2 for the full results. In
general, HIQL (a method that involves hierarchical policy extraction) tends to achieve strong performance across
the board. Among the non-hierarchical methods, CRL tends to work best in locomotion tasks and GCIVL and
GCIQL tend to work best in the others.
for pixel-based tasks), and average the results over 8 seeds (4 seeds for pixel-based tasks). We discuss
the results through Q&As.
Q: Which method works best in general?
A: While no single method dominates the others across all categories in Figure 2. HIQL (a method
that involves hierarchical policy extraction) tends to achieve particularly strong performance among
the benchmarked methods, especially in locomotion and visual manipulation tasks. Among the nonhierarchical methods, CRL tends to work best in locomotion tasks and GCIQL tends to work best in
manipulation tasks. In the drawing tasks, GCIVL performs the best.
Q: Which methods are good at goal stitching?
A: To see this, we can compare the performances on the navigate and stitch datasets from the same
locomotion task in Table 2. The results suggest that, as expected, full RL-based methods like HIQL
(i.e., methods that fit the optimal value function Q∗ ) are better at stitching than one-step RL methods
like CRL (i.e., methods that fit the behavioral value function Qβ ). For example, in visual locomotion
tasks, the relative performance between HIQL and CRL is reversed on the stitch datasets (Figure 2).
Q: Which methods are good at handling stochasticity?
A: For this, we can compare the performances on the large and teleport mazes in Table 2, where
both have the same maze size, but only the latter involves stochastic transitions that incur risk. Table 2
shows that, value-only methods like HIQL and QRL (i.e., methods that do not have a separate Q
function), which are optimistically biased in stochastic environments, struggle relatively more in
stochastic teleport tasks. In contrast, CRL is generally robust to environment stochasticity, likely
because it fits a Monte Carlo value function.
Q: Which methods are good at handling pixel-based observations?
A: Although state-based and pixel-based observations generally provide the same amount of information, several methods struggle to handle image observations due to additional representational challenges. We can understand how well a method addresses such representational challenges by comparing the performances of corresponding state- and pixel-based tasks. Table 2 shows that CRL is notably
robust to the difference in input modalities, likely because it is based on a pure representation learning
objective. HIQL also achieves strong performance in pixel-based tasks, especially in visual manipulation tasks. However, these methods are still not perfect at handling image observations; for example,
HIQL achieves relatively weak performance on image drawing tasks. We suspect this is due to the difficulty of learning low-dimensional subgoal representations from states of high intrinsic dimensionality.
7
We note that Zheng et al. (2024b) use a different evaluation scheme based on the maximum performance over
evaluation epochs. We report the average performance over the last three evaluation epochs (see Appendix E).

12

Published as a conference paper at ICLR 2025

Table 3: How good are our reference implementations? Our implementations generally achieve better
performance than previously reported ones.
D4RL antmaze-large-diverse-v2

D4RL antmaze-large-play-v2

Method

Previously Reported Performance

Ours

Method

Previously Reported Performance

Ours

GCBC
GCIVL
GCIQL
QRL
CRL
HIQL

20 (Park et al., 2023)
51 (Park et al., 2023)
30 (Zeng et al., 2023)
527 (Zheng et al., 2024b)
54 (Eysenbach et al., 2022)
88 (Park et al., 2023)

41 ±7
64 ±8
64 ±10
37 ±14
79 ±6
87 ±3

GCBC
GCIVL
GCIQL
QRL
CRL
HIQL

23 (Park et al., 2023)
57 (Park et al., 2023)
40 (Zeng et al., 2023)
537 (Zheng et al., 2024b)
49 (Eysenbach et al., 2022)
86 (Park et al., 2023)

39 ±4
58 ±8
55 ±11
38 ±8
74 ±4
87 ±2

Table 4: Do not use single-goal evaluation! Only using a single state-goal pair (a common practice when
using D4RL tasks for offline GCRL) can potentially lead to inaccurate conclusions about offline GCRL methods.
OGBench always uses multi-goal evaluation. See how the rank between GCIQL and QRL is reversed with
multi-goal evaluation on the same antmaze-large maze.
Dataset

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

D4RL antmaze-large-diverse-v2 (single-goal evaluation)
D4RL antmaze-large-play-v2 (single-goal evaluation)
OGBench antmaze-large-navigate-v0 (multi-goal evaluation, ours)

41 ±7
39 ±4
24 ±2

64 ±8
58 ±8
16 ±5

64 ±10
55 ±11
34 ±4

37 ±14
38 ±8
75 ±6

79 ±6
74 ±4
83 ±4

87 ±3
87 ±2
91 ±2

Q: How good are our reference implementations?
A: We compare the performance of our reference implementations with previously reported numbers
on one of the most commonly used tasks in prior work, D4RL antmaze-large (Fu et al., 2020). Table 3 shows the comparison results, with the corresponding numbers taken from the prior works (Eysenbach et al., 2022; Park et al., 2023; Zeng et al., 2023; Zheng et al., 2024b). The results suggest that
our implementations generally achieve better performance than previously reported results, sometimes significantly surpassing them (e.g., CRL).
Q: Why should I use OGBench AntMaze instead of the D4RL one?
A: D4RL AntMaze is an excellent task for benchmarking offline RL algorithms. However, it is
limited for benchmarking offline goal-conditioned RL algorithms because it only involves a single,
fixed state-goal pair, and the datasets are tailored to this specific task (Fu et al., 2020). In contrast,
OGBench supports multi-goal evaluation (and provides much more diverse types of tasks and
datasets!). To empirically demonstrate this difference, we compare the benchmarking results on
D4RL antmaze-large-{diverse, play} and OGBench antmaze-large-navigate in Table 4.
The table suggests that single-goal evaluation is indeed limited, and is potentially prone to inaccurate
conclusions: for example, see how the ranking between GCIQL and QRL is reversed with multigoal evaluation on the same antmaze-large maze. Moreover, the performance differences between
methods are more pronounced in OGBench AntMaze, showing that OGBench provides clearer
research signals.
Q: There seem to be a lot of datasets. What should I use for my research?
A: For general offline GCRL algorithms research, we recommend starting with more “regular” datasets, such as antmaze-{large, giant}-navigate, humanoidmaze-medium-navigate,
cube-{single, double}-play, scene-play, and puzzle-3x3-play. From there, depending
on the performance on these tasks, try harder versions of them or more challenging tasks, such as
humanoidmaze-giant, antsoccer, puzzle-{4x4, 4x5, 4x6}, and powderworld.
We also provide more specialized datasets that pose specific challenges in offline GCRL (Section 5).
For stitching, try the stitch datasets in the locomotion suite as well as complex manipulation tasks
that require stitching (e.g., puzzle). For long-horizon reasoning, consider humanoidmaze-giant,
which has the longest episode length, and puzzle-4x6, which has the most semantic steps. For
stochastic control, try antmaze-teleport, which is specifically designed to challenge optimistically biased methods, and powderworld, which has unpredictable, stochastic dynamics. For learning from highly suboptimal data, consider antmaze-explore as well as the noisy datasets in the
manipulation suite, which features high suboptimality and high coverage.

13

Published as a conference paper at ICLR 2025

Q: Have you found any insights on data collection for offline GCRL?
cube-single (GCIQL)
Success Rate (%)

A: One of the main features of OGBench is that every
task is accompanied by a reproducible and controllable
data-generation script. Here, we show one example of
how this controllability can lead to practical insights and
raise open research questions. In Figure 3, we ablate
the strength of Gaussian action noise σ added to expert
actions on two manipulation tasks (cube-single-noisy
and puzzle-3x3-noisy), and measure how this affects
performance.

puzzle-3x3 (GCIQL)
100

100

75
50

50
25

0

0

0.01 0.03 0.1 0.3
Action Noise σ

0

0.01 0.03 0.1 0.3
Action Noise σ

Figure 3: Datasets must be noisy enough.

The results are quite remarkable: they show that having the right amount of noise (i.e., state coverage)
is very important for achieving good performance. For example, the performance drops from 99%
to 6% if there is no noise in expert actions, even on the most basic cube pick-and-place task. This
suggests that, we may need to prioritize coverage much more than optimality when collecting datasets
for offline GCRL in the real world as well, and failing to do so may lead to (surprising) failures in
learning. Like action noise, we believe there are many other important properties of datasets that
significantly affect performance. We believe that our fully transparent, controllable data-generation
scripts can facilitate such scientific studies.

9

R ESEARCH O PPORTUNITIES

In this section, we discuss potential research ideas and open questions.
Be the first to solve unsolved tasks! While all environments in OGBench have at least one variant
that current methods can solve to some degree, there are still a number of challenging tasks on which
no existing method achieves non-trivial performance, such as humanoidmaze-giant, cube-triple,
puzzle-4x5, powderworld-hard, and more. We ensure that sufficient data is available for those
tasks (which is estimated from the amount needed to solve their easier versions). We invite researchers
to take on these challenges and push the limits of offline GCRL with better algorithms.
How can we develop a policy that generalizes well at test time? In our experiments, we found hierarchical RL methods (e.g., HIQL) to work especially well in several tasks. Among several potential
explanations, we hypothesize that this is mainly because hierarchical RL reduces learning complexity
by having two policies specialized in different things, which makes both policies generalize better at
evaluation time. After all, test-time generalization is known to be one of the major bottlenecks in
offline RL (Park et al., 2024a). But, are hierarchies really necessary to achieve good test-time generalization? Can we develop a non-hierarchical method that enjoys the same benefit by exploiting the
subgoal structure of offline GCRL? This would be especially beneficial, not just because it is simpler,
but also because it can potentially yield better, unified representations that can potentially serve as a
“foundation model” for fine-tuning.
Can we develop a method that works well across all categories? Our benchmarking results
reveal that no method consistently performs best across the board. HIQL tends to achieve strong
performance but struggles in pixel-based locomotion and state-based manipulation. GCIQL shows
strong performance in state-based manipulation, but struggles in locomotion. CRL exhibits the
opposite trend: it excels in locomotion but underperforms in manipulation. Is there a way to combine
only the strengths of these methods to develop a single approach that achieves the best performance
across all types of tasks?
More concrete research questions. Here, we list additional, more concrete research questions that
researchers may use as a starting point for research in offline GCRL:
• Why is PointMaze so hard? Table 2 shows that PointMaze is surprisingly hard, sometimes even
harder than AntMaze for some methods. Why is this the case? Moreover, only in PointMaze
does QRL significantly outperform the other methods. What causes this difference, and are there
any insights we can take from these results?
• How should we train subgoal representations? Somewhat weirdly, HIQL struggles much more
with state-based observations than pixel-based observations on manipulation tasks. We suspect
this is related to subgoal representations, given that HIQL uses an additional learning signal
14

Published as a conference paper at ICLR 2025

from the policy loss to further train subgoal representations only in pixel-based environments
(which we found does not help in state-based environments). HIQL uses a value function-based
subgoal representation (Appendix D), but is there a better, more stable way to learn subgoal
representations for hierarchical RL and planning?
• Do we really need the full power of RL? While learning the optimal Q∗ function is in principle
better than learning the behavioral Qβ function, CRL (which fits Qβ ) significantly outperforms
GCIQL (which fits Q∗ ) on locomotion tasks. Why is this the case? Is it a problem with expectile
regression in GCIQL or with temporal difference learning itself? In contrast, in manipulation
environments, the result suggests the opposite: GCIQL is much better than CRL. Does this mean
we do need Q∗ in these tasks? Or can it be solvable even with Qβ if we use a better behavioral
value learning technique than binary NCE in CRL?
• Why can’t we use random goals when training policies? When training goal-conditioned
policies, we found that it is usually better to sample (policy) goals only from the future state in
the current trajectory (except on stitch or explore datasets; see Table 10). The fact that this
works better even in Scene and Puzzle (which require goal stitching) is a bit surprising, because it
means that the policy can still perform goal stitching to some degree even without being explicitly
trained on the test-time state-goal pairs. At the same time, it is rather unsatisfying because this
ability to stitch goals entirely depends on the seemingly “magical” generalization capabilities
of neural networks. Is there a way to train a goal-conditioned policy with random goals while
maintaining performance, so that it can perform goal stitching in a principled manner?
• How can we combine expressive policies with GCRL? In Appendix B, we show that current
offline GCRL methods often struggle with datasets collected by non-Markovian policies in
manipulation environments. Handling non-Markovian trajectory data is indeed one of the
major challenges in behavioral cloning, for which many recent BC-based methods have been
proposed (Zhao et al., 2023; Chi et al., 2023). How can we incorporate these recent advancements
in behavioral cloning into offline GCRL?

10

O UTLOOK

In this work, we introduced OGBench, a new benchmark designed to advance algorithms research
in offline goal-conditioned RL. With the experimental results, we now revisit the very first question
posed in this paper: “Why offline goal-conditioned RL?”
We hypothesize that offline goal-conditioned holds significant potential as a recipe for general-purpose
RL pre-training, which yields richer and more diverse behaviors than (arguably more prevalent)
generative pre-training, such as behavioral cloning and next-token prediction. Our experiments, albeit
preliminary, show that even current offline GCRL algorithms can to some extent acquire effective
policies for exceptionally long-horizon tasks entirely with sparse rewards, using data that is highly
suboptimal. These results are not limited to toy example domains, but show up across a range of
realistic simulated robotics and game-like settings in our benchmark. While generative objectives
might capture the data distribution, offline GCRL can learn policies that actually achieve complex
outcomes (such as beating a puzzle game) that could not be achieved simply by copying random data.
However, current offline GCRL algorithms also have limitations. As shown in our results, they often
struggle with long-horizon, high-dimensional tasks and those that require stitching, and no single
method consistently outperforms others across all tasks. This suggests that we have not yet found the
ideal algorithm that can fully realize the promise of offline GCRL as general-purpose RL pre-training.
The first step toward finding such an ideal algorithm is to set up a solid benchmark that sufficiently
challenges the limits of offline GCRL from diverse perspectives. We believe OGBench provides this
foundation, and will lead to the development of performant, scalable offline GCRL algorithms that
enable building foundation models for general-purpose behaviors.

ACKNOWLEDGMENTS
We thank Kevin Zakka for providing the initial codebase for the manipulation environments and
helping with MuJoCo implementations, Vivek Myers for providing a JAX-based QRL implementation,
and Colin Li, along with the members of the RAIL lab, for helpful discussions. This work was partly
15

Published as a conference paper at ICLR 2025

supported by the Korea Foundation for Advanced Studies (KFAS), National Science Foundation
Graduate Research Fellowship Program under Grant No. DGE 2146752, ONR under N00014-20-12383 and N00014-22-1-2773, and Qualcomm. This research used the Savio computational cluster
resource provided by the Berkeley Research Computing program at UC Berkeley.

R EPRODUCIBILITY S TATEMENT
We provide the full implementation details in Appendix E. We provide the code as well as the
exact command-line flags to reproduce the entire benchmark table, datasets, and expert policies at
https://github.com/seohongpark/ogbench.

R EFERENCES
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. ArXiv,
abs/2305.13673, 2023a.
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation.
ArXiv, abs/2309.14402, 2023b.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
In Neural Information Processing Systems (NeurIPS), 2017.
Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,
2016.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.
JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.
com/jax-ml/jax.
G. Brockman, Vicki Cheung, Ludwig Pettersson, J. Schneider, John Schulman, Jie Tang, and
W. Zaremba. OpenAI Gym. ArXiv, abs/1606.01540, 2016.
Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-conditioned reinforcement learning with
imagined subgoals. In International Conference on Machine Learning (ICML), 2021.
Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake,
and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics:
Science and Systems (RSS), 2023.
Peter Dayan and Geoffrey E. Hinton. Feudal reinforcement learning. In Neural Information
Processing Systems (NeurIPS), 1992.
Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala:
Scalable distributed deep-rl with importance weighted actor-learner architectures. In International
Conference on Machine Learning (ICML), 2018.
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging
planning and reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2019.
Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine. Contrastive learning
as goal-conditioned reinforcement learning. In Neural Information Processing Systems (NeurIPS),
2022.
Kuan Fang, Patrick Yin, Ashvin Nair, and Sergey Levine. Planning to practice: Efficient online finetuning by composing goals in latent space. In IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2022.
Kevin Frans and Phillip Isola. Powderworld: A platform for understanding generalization via rich
task distributions. In International Conference on Learning Representations (ICLR), 2023.
16

Published as a conference paper at ICLR 2025

Justin Fu, Aviral Kumar, Ofir Nachum, G. Tucker, and Sergey Levine. D4rl: Datasets for deep datadriven reinforcement learning. ArXiv, abs/2004.07219, 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In
Neural Information Processing Systems (NeurIPS), 2021.
Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach,
and Sergey Levine. Learning to reach goals via iterated supervised learning. In International
Conference on Learning Representations (ICLR), 2021.
Dibya Ghosh, Chethan Bhateja, and Sergey Levine. Reinforcement learning from passive data via
latent intentions. In International Conference on Machine Learning (ICML), 2023.
Raj Ghugare, Matthieu Geist, Glen Berseth, and Benjamin Eysenbach. Closing the gap between td
learning and supervised learning–a generalisation point of view. In International Conference on
Learning Representations (ICLR), 2024.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning (ICML), 2018.
Joey Hejna, Jensen Gao, and Dorsa Sadigh. Distance weighted supervised learning for offline
interaction data. In International Conference on Machine Learning (ICML), 2023.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). ArXiv, abs/1606.08415, 2016.
Christopher Hoang, Sungryull Sohn, Jongwook Choi, Wilka Carvalho, and Honglak Lee. Successor
feature landmarks for long-horizon goal-conditioned reinforcement learning. In Neural Information
Processing Systems (NeurIPS), 2021.
Mineui Hong, Minjae Kang, and Songhwai Oh. Diffused task-agnostic milestone planner. In Neural
Information Processing Systems (NeurIPS), 2023.
Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal
Mehta, and JoÃĢo GM AraÃšjo. Cleanrl: High-quality single-file implementations of deep
reinforcement learning algorithms. Journal of Machine Learning Research (JMLR), 23(274):1–18,
2022.
Zhiao Huang, Fangchen Liu, and Hao Su. Mapping state space using landmarks for universal goal
reaching. In Neural Information Processing Systems (NeurIPS), 2019.
Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rocktaschel, Edward Grefenstette,
and Yuandong Tian. Efficient planning in a compact latent action space. In International Conference
on Learning Representations (ICLR), 2023.
Leslie Pack Kaelbling. Learning to achieve goals. In International Joint Conference on Artificial
Intelligence (IJCAI), 1993.
Junsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-guided subgoal generation in hierarchical
reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2021.
Junsu Kim, Seohong Park, and Sergey Levine. Unsupervised-to-online reinforcement learning. ArXiv,
abs/2408.14785, 2024.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit qlearning. In International Conference on Learning Representations (ICLR), 2022.
Aviral Kumar, Aurick Zhou, G. Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2020.
17

Published as a conference paper at ICLR 2025

Jinning Li, Chen Tang, Masayoshi Tomizuka, and Wei Zhan. Hierarchical planning through goalconditioned offline reinforcement learning. IEEE Robotics and Automation Letters (RA-L), 7(4):
10216–10223, 2022.
Bo Liu, Yihao Feng, Qiang Liu, and Peter Stone. Metric residual network for sample efficient goalconditioned reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2023.
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and
Pierre Sermanet. Learning latent plans from play. In Conference on Robot Learning (CoRL), 2019.
Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i’ll go: Offline goalconditioned reinforcement learning via f-advantage regression. In Neural Information Processing
Systems (NeurIPS), 2022.
Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy
Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. In
International Conference on Learning Representations (ICLR), 2023.
Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
models: Consistency and statistical efficiency. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv,
abs/1312.5602, 2013.
Vivek Myers, Chongyi Zheng, Anca Dragan, Sergey Levine, and Benjamin Eysenbach. Learning
temporal distances: Contrastive successor features can provide a metric structure for decisionmaking. In International Conference on Machine Learning (ICML), 2024.
Soroush Nasiriany, Vitchyr H. Pong, Steven Lin, and Sergey Levine. Planning with goal-conditioned
policies. In Neural Information Processing Systems (NeurIPS), 2019.
Whitney Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica,
55:819–847, 1987.
Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goal-conditioned
rl with latent states as actions. In Neural Information Processing Systems (NeurIPS), 2023.
Seohong Park, Kevin Frans, Sergey Levine, and Aviral Kumar. Is value learning really the main
bottleneck in offline rl? In Neural Information Processing Systems (NeurIPS), 2024a.
Seohong Park, Tobias Kreiman, and Sergey Levine. Foundation policies with hilbert representations.
In International Conference on Machine Learning (ICML), 2024b.
Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metric-aware
abstraction. In International Conference on Learning Representations (ICLR), 2024c.
Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. ArXiv, abs/2502.02538, 2025.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. ArXiv, abs/1910.00177, 2019.
Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In International Conference on Machine Learning (ICML), 2007.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement
learning: Challenging robotics environments and request for research. ArXiv, abs/1802.09464,
2018.
Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for
navigation. In International Conference on Learning Representations (ICLR), 2018.
18

Published as a conference paper at ICLR 2025

Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In International Conference on Machine Learning (ICML), 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017.
Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy Zhang, and Scott Niekum.
Score models for offline goal-conditioned reinforcement learning. In International Conference on
Learning Representations (ICLR), 2024.
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. Corl:
Research-oriented deep offline reinforcement learning library. In Neural Information Processing
Systems (NeurIPS), 2023.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. ArXiv, abs/1801.00690, 2018.
Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan,
Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. Pettingzoo:
Gym for multi-agent reinforcement learning. In Neural Information Processing Systems (NeurIPS),
2021.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.
Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu,
Manuel Goulão, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: A standard
interface for reinforcement learning environments. ArXiv, abs/2407.17032, 2024.
Mianchu Wang, Rui Yang, Xi Chen, and Meng Fang. Goplan: Goal-conditioned offline reinforcement
learning by planning with learned models. Transactions on Machine Learning Research (TMLR),
2024.
Tongzhou Wang and Phillip Isola. Improved representation of asymmetrical distances with interval
quasimetric embeddings. ArXiv, abs/2211.15120, 2022.
Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal goal-reaching reinforcement learning via quasimetric learning. In International Conference on Machine Learning (ICML),
2023.
Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie
Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. In
International Conference on Learning Representations (ICLR), 2022.
Rui Yang, Yong Lin, Xiaoteng Ma, Haotian Hu, Chongjie Zhang, and T. Zhang. What is essential for
unseen goal generalization of offline goal-conditioned rl? In International Conference on Machine
Learning (ICML), 2023.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Conference on Robot Learning (CoRL), 2019.
Kevin Zakka, Yuval Tassa, and MuJoCo Menagerie Contributors. Mujoco menagerie: A collection of high-quality simulation models for mujoco, 2022. URL http://github.com/
google-deepmind/mujoco_menagerie.
Zilai Zeng, Ce Zhang, Shijie Wang, and Chen Sun. Goal-conditioned predictive coding for offline
reinforcement learning. In Neural Information Processing Systems (NeurIPS), 2023.
Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual
manipulation with low-cost hardware. In Robotics: Science and Systems (RSS), 2023.
19

Published as a conference paper at ICLR 2025

Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov,
and Sergey Levine. Stabilizing contrastive rl: Techniques for offline goal reaching. In International
Conference on Learning Representations (ICLR), 2024a.
Chongyi Zheng, Ruslan Salakhutdinov, and Benjamin Eysenbach. Contrastive difference predictive
coding. In International Conference on Learning Representations (ICLR), 2024b.

20

Published as a conference paper at ICLR 2025

Table 5: Full benchmarking results on additional noisy manipulation datasets. The table shows the
performances on both the play and noisy datasets in the manipulation suite. We report each method’s average
(binary) success rate (%) across the five test-time goals on each task. The results are averaged over 8 seeds (4
seeds for pixel-based tasks), and we report the standard deviations after the ± sign. Numbers at or above 95% of
the best value in the row are highlighted in bold.
Environment

Dataset Type

Dataset

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

play

cube-single-play-v0
cube-double-play-v0
cube-triple-play-v0
cube-quadruple-play-v0

6 ±2
1 ±1
1 ±1
0 ±0

53 ±4
36 ±3
1 ±0
0 ±0

68 ±6
40 ±5
3 ±1
0 ±0

5 ±1
1 ±0
0 ±0
0 ±0

19 ±2
10 ±2
4 ±1
0 ±0

15 ±3
6 ±2
3 ±1
0 ±0

noisy

cube-single-noisy-v0
cube-double-noisy-v0
cube-triple-noisy-v0
cube-quadruple-noisy-v0

8 ±3
1 ±1
1 ±1
0 ±0

71 ±9
14 ±3
9 ±1
0 ±0

99 ±1
23 ±3
2 ±1
0 ±0

25 ±6
3 ±1
1 ±0
0 ±0

38 ±2
2 ±1
3 ±1
0 ±0

41 ±6
2 ±1
2 ±1
0 ±0

play

scene-play-v0

5 ±1

42 ±4

51 ±4

5 ±1

19 ±2

38 ±3

noisy

scene-noisy-v0

1 ±1

26 ±5

26 ±2

9 ±2

1 ±1

25 ±4

play

puzzle-3x3-play-v0
puzzle-4x4-play-v0
puzzle-4x5-play-v0
puzzle-4x6-play-v0

2 ±0
0 ±0
0 ±0
0 ±0

6 ±1
13 ±2
7 ±1
10 ±2

95 ±1
26 ±3
14 ±1
12 ±1

1 ±0
0 ±0
0 ±0
0 ±0

3 ±1
0 ±0
1 ±0
4 ±1

12 ±2
7 ±2
4 ±1
3 ±1

noisy

puzzle-3x3-noisy-v0
puzzle-4x4-noisy-v0
puzzle-4x5-noisy-v0
puzzle-4x6-noisy-v0

1 ±0
0 ±0
0 ±0
0 ±0

42 ±19
20 ±3
19 ±0
17 ±2

94 ±3
29 ±7
19 ±0
18 ±2

0 ±0
0 ±0
0 ±0
0 ±0

30 ±6
0 ±0
3 ±2
6 ±3

51 ±11
16 ±4
5 ±1
2 ±1

play

visual-cube-single-play-v0
visual-cube-double-play-v0
visual-cube-triple-play-v0
visual-cube-quadruple-play-v0

5 ±1
1 ±1
15 ±2
8 ±1

60 ±5
10 ±2
14 ±2
0 ±0

30 ±5
1 ±1
15 ±1
7 ±1

41 ±15
5 ±0
16 ±1
5 ±1

31 ±15
2 ±1
17 ±2
4 ±1

89 ±0
39 ±2
21 ±0
14 ±1

noisy

visual-cube-single-noisy-v0
visual-cube-double-noisy-v0
visual-cube-triple-noisy-v0
visual-cube-quadruple-noisy-v0

14 ±3
5 ±1
16 ±1
9 ±0

75 ±3
17 ±4
18 ±1
0 ±0

48 ±3
22 ±2
12 ±1
2 ±2

10 ±5
6 ±2
9 ±4
0 ±0

39 ±30
6 ±3
16 ±1
8 ±2

99 ±0
59 ±3
23 ±2
12 ±8

play

visual-scene-play-v0

12 ±2

25 ±3

12 ±2

10 ±1

11 ±2

49 ±4

noisy

visual-scene-noisy-v0

13 ±2

23 ±2

12 ±4

2 ±0

15 ±2

50 ±1

play

visual-puzzle-3x3-play-v0
visual-puzzle-4x4-play-v0
visual-puzzle-4x5-play-v0
visual-puzzle-4x6-play-v0

0 ±0
10 ±1
5 ±2
2 ±1

21 ±1
60 ±5
17 ±1
15 ±1

1 ±2
16 ±4
7 ±2
2 ±1

1 ±1
0 ±0
0 ±0
0 ±0

0 ±0
10 ±6
6 ±1
3 ±1

73 ±8
60 ±41
13 ±9
9 ±6

noisy

visual-puzzle-3x3-noisy-v0
visual-puzzle-4x4-noisy-v0
visual-puzzle-4x5-noisy-v0
visual-puzzle-4x6-noisy-v0

1 ±1
7 ±3
6 ±1
2 ±1

20 ±0
47 ±3
14 ±10
12 ±8

26 ±4
49 ±7
19 ±0
17 ±1

0 ±0
0 ±0
0 ±0
0 ±0

1 ±1
6 ±2
7 ±1
2 ±1

70 ±6
84 ±4
14 ±10
14 ±2

cube

scene

puzzle

visual-cube

visual-scene

visual-puzzle

A

L IMITATIONS

While OGBench covers a number of challenges in offline goal-conditioned RL, such as long-horizon
reasoning, goal stitching, and stochastic control, there exist other challenges that our benchmark
does not address. For example, all OGBench tasks assume that the environment dynamics remain
the same between the training and evaluation environments. Also, although several OGBench tasks
(e.g., Cube, Puzzle, and Powderworld) require unseen goal generalization to some degree, our tasks
do not specifically test visual generalization to entirely new objects. Finally, we have made several
trade-offs to reduce computational cost and to focus the benchmark on algorithms research at the
expense of sacrificing realism to some degree (e.g., the use of the transparent arm in manipulation
environments, the use of synthetic (yet fully controllable) datasets, etc.). Nonetheless, we believe
OGBench can spur the development of performant offline GCRL algorithms, which can then help
researchers develop scalable data-driven unsupervised RL pre-training methods for real-world tasks.

B

A DDITIONAL DATASETS

For manipulation tasks (Cube, Scene, and Puzzle), in addition to the main play datasets, we
additionally provide noisy datasets that can potentially be useful for other types of research (e.g.,
ablation studies on datasets, comparing performances on non-Markovian and Markovian datasets,
etc.). The main difference is that the play datasets are collected by open-loop, non-Markovian
expert policies with temporally correlated noise, while the noisy datasets are collected by closedloop, Markovian expert policies with larger, uncorrelated Gaussian noise. Hence, the play datasets
generally look more “natural” than the noisy datasets, but the latter has higher state coverage (videos).
21

Published as a conference paper at ICLR 2025

Table 5 shows the full benchmark results on both the play and noisy datasets in the manipulation
suite. The results suggest that the performances on these two datasets are mostly similar, but several
methods struggle to handle narrower and non-Markovian trajectories in play datasets (e.g., GCIQL
almost perfectly solves cube-single-noisy but struggles on cube-single-play).

C

P RIOR W ORK IN G OAL -C ONDITIONED RL

The problem of reaching any goal from any state has long been considered one of the central problems
in reinforcement learning and sequential decision making (Kaelbling, 1993; Schaul et al., 2015;
Andrychowicz et al., 2017), owing to its unsupervised nature, simplicity, and generality. There
are many unique features of goal-conditioned RL that make it distinct from other (multi-task) RL
problems, such as the presence of recursive subgoal structures, metric structures, and probabilistic
interpretations. These intriguing properties have led to the development of diverse families of
online and offline GCRL algorithms based on hindsight relabeling (Andrychowicz et al., 2017),
hierarchical learning (Dayan & Hinton, 1992; Chane-Sane et al., 2021; Li et al., 2022; Park et al.,
2023), planning (Savinov et al., 2018; Eysenbach et al., 2019; Nasiriany et al., 2019; Huang et al.,
2019; Hoang et al., 2021; Kim et al., 2021; Wang et al., 2024), metric learning (Wang et al., 2023;
Park et al., 2024b; Myers et al., 2024), dual optimization (Ma et al., 2022; 2023; Sikchi et al., 2024),
weighted behavioral cloning (Yang et al., 2022; 2023; Hejna et al., 2023), generative modeling (Zeng
et al., 2023; Hong et al., 2023), and contrastive learning (Eysenbach et al., 2022; Zheng et al.,
2024a;b). In this work, we also consider the problem of offline goal-conditioned RL; however, instead
of proposing a new algorithm, we introduce a new benchmark and reference implementations to
facilitate and advance algorithms research in offline GCRL.

D

O FFLINE GCRL A LGORITHMS

In this section, we describe the six offline GCRL methods used for benchmarking in detail. We first
define four goal-sampling distributions that correspond to the current state, uniform future states,
geometric future states, and random states, respectively:
• pD
cur (g | s) denotes the Dirac delta distribution at s (i.e., g is always set to s).
• pD
traj (g | s) denotes the uniform future state distribution defined as follows: assuming s = st
in a trajectory τ = (s0 , a0 , s1 , . . . , sT )8 , we sample an index k from the uniform distribution
Unif(min(t + 1, T − 1), T − 1) (inclusive), and set g = sk .
• pD
geom (g | s) denotes the truncated geometric future state distribution defined as follows:
assuming s = st in a trajectory τ = (s0 , a0 , s1 , . . . , sT ), we sample an index k from the
geometric distribution Geom(1 − γ) (whose support starts from 1), and set g = smin(s+k,T −1) .
• pD
rand (g) denotes the uniform state distribution over the dataset D.
Additionally, pD
mixed (g | s) denotes a mixture of these four goal-sampling distributions with a mixture
ratio defined by hyperparameters, pD (·) simply denotes the uniform distribution over the dataset, and
D
we sometimes use pD
· (· | s, a) instead of p· (· | s) to denote the distribution corresponding to the
state-action pair.
Goal-conditioned behavioral cloning (GCBC). GCBC (Lynch et al., 2019; Ghosh et al., 2021)
simply performs behavioral cloning using future states in the same trajectory as goals. GCBC
maximizes the following objective to train a goal-conditioned policy π(a | s, g).
JGCBC (π) = E(s,a)∼pD (s,a),g∼pD
[log π(a | s, g)].
traj (g|s)

(1)

Goal-conditioned implicit {V, Q}-learning (GCIVL and GCIQL). GCIVL and GCIQL are goalconditioned variants of implicit Q-learning (IQL) (Kostrikov et al., 2022), which is an offline RL
algorithm that fits the optimal value functions (V ∗ or Q∗ ) using an expectile regression (Newey &
Powell, 1987). GCIQL is a straightforward goal-conditioned variant of IQL, and GCIVL is the V only variant introduced by Park et al. (2023). GCIVL fits a value function V (s, g) by minimizing the
8

If there are multiple such (τ, t) tuples in the dataset, consider the uniform mixture of them.

22

Published as a conference paper at ICLR 2025

following loss:
 2

ℓκ r(s, g) + γ V̄ (s′ , g) − V (s, g) ,
LGCIVL (V ) = Es∼pD (s),g∼pD
mixed (g|s)

(2)

where r(s, g) = 1{g} (s) − 1 denotes the (−1, 0)-sparse goal-conditioned reward function, V̄ denotes
the target value function (Mnih et al., 2013), and ℓ2κ (x) = |κ − 1{ x : x<0 } (x)|x2 denotes the expectile
loss with an expectile κ.
GCIQL fits both V (s, g) and Q(s, a, g) by jointly minimizing the following losses:
 2

LVGCIQL (V ) = E(s,a)∼pD (s,a),g∼pD
ℓ
Q̄(s,
a,
g)
−
V
(s,
g)
,
(g|s)
κ
mixed
h
i
2
′
LQ
(r(s,
g)
+
γV
(s
,
g)
−
Q(s,
a,
g))
,
(g|s)
GCIQL (Q) = E(s,a,s′ )∼pD (s,a,s′ ),g∼pD
mixed

(3)
(4)

where Q̄ denotes the target Q function (Mnih et al., 2013). We note that GCIVL is optimistically
biased in stochastic environments, but GCIQL is unbiased (Kostrikov et al., 2022; Park et al., 2023).
To extract a policy from the learned value functions, we can use either advantage-weighted regression
(AWR) (Peters & Schaal, 2007; Peng et al., 2019) or behavior-constrained deep deterministic policy
gradient (DDPG+BC) (Fujimoto & Gu, 2021). GCIVL uses the following value-only variant of the
AWR objective (Park et al., 2023):
h
i
V
α(V (s′ ,g)−V (s,g))
JAWR
(π) = E(s,a,s′ )∼pD (s,a,s′ ),g∼pD
e
log
π(a
|
s,
g)
,
(5)
mixed (g|s)
where α is the temperature hyperparameter. In our experiments, GCIQL mainly uses the following
DDPG+BC objective (which is known to be better than AWR (Park et al., 2024a)):
JDDPG+BC (π) = E(s,a)∼pD (s,a),g∼pD
[Q(s, π µ (s, g), g) + α log π(a | s, g)] ,
mixed (g|s)

(6)

where π µ (s, g) = Ea∼π(a|s,g) [a]. In discrete-action environments, GCIQL uses the following Q
version of AWR:
h
i
Q
α(Q(s,a,g)−V (s,g))
JAWR
(π) = E(s,a,s′ )∼pD (s,a,s′ ),g∼pD
e
log
π(a
|
s,
g)
.
(7)
(g|s)
mixed
In practice, we use standard double-value learning for GCIVL and GCIQL (Park et al., 2023), and Q
normalization for DDPG+BC (Fujimoto & Gu, 2021).
Quasimetric RL (QRL). QRL (Wang et al., 2023) is a goal-conditioned value learning algorithm
based on quasimetric learning, where a quasimetric means an asymmetric metric. In deterministic
environments, the shortest path length between two states d∗ (s, g) is equivalent to the optimal undiscounted goal-conditioned value function V ∗ (s, g) under the (−1, 0)-sparse reward function (Wang
et al., 2023): V ∗ (s, g) = −d∗ (s, g). The main idea of QRL is to explicitly leverage the quasimetric
property (i.e., triangle inequality) of shortest path lengths, namely d∗ (s, w) + d∗ (w, g) ≥ d∗ (s, g) for
any s, w, g ∈ S, by modeling it with a quasimetric network architecture like MRN (Liu et al., 2023)
or IQE (Wang & Isola, 2022). Concretely, QRL maximizes the following constrained optimization
objective:
maximize

Es∼pD (s),g∼pD
[d(s, g)]
rand (g)


s.t. E(s,s′ )∼pD (s,s′ ) (d(s, s′ ) − 1)2 ≤ ε2 ,

(8)
(9)

where d(s, g) is a quasimetric distance function (e.g., an IQE network) and ε is a hyperparameter that
controls the strength of the constraint.
To extract a policy from the value function V (s, g) = −d(s, g), QRL uses the value-only AWR
loss Equation (5) in discrete-action MDPs. In continuous-action MDPs, QRL additionally fits a
dynamics model f (ϕ(s), a) : Z × A → Z (Wang et al., 2023), where Z denotes a latent space and
ϕ(s) : S → Z denotes the representation function used in the quasimetric distance function: namely,
˜
d(s, g) = d(ϕ(s),
ϕ(g)) (e.g., ϕ is the interval representation function in IQE). The dynamics loss is
as follows:
h
i
Ldyn (f ) = E(s,a,s′ )∼pD (s,a,s′ ) d˜(ϕ(s′ ), f (ϕ(s), a)) + d˜(f (ϕ(s), a), ϕ(s′ )) ,
(10)

23

Published as a conference paper at ICLR 2025

where QRL jointly trains both d and f without stop-gradients. Based on the dynamics model f , QRL
maximizes the following DDPG+BC-like loss:
h
i
˜(f (ϕ(s), π µ (s, g)), ϕ(g)) + α log π(a | s, g) ,
JDDPG+BC (π) = E(s,a)∼pD (s,a),g∼pD
−
d
mixed (g|s)
(11)
where we use the same notation as Equation (6).
In practice, we employ a softplus loss shaping for the quasimetric loss and delta prediction for the
dynamics model, as in Wang et al. (2023).
Contrastive RL (CRL). CRL (Eysenbach et al., 2022) is a “one-step” GCRL algorithm that first
trains a Monte Carlo goal-conditioned value function using contrastive learning and performs a onestep policy improvement. CRL maximizes the following binary NCE objective (Ma & Collins, 2018)
with respect to f (s, a, g) : S × A × S → R:
D
JCRL (f ) = E(s,a)∼pD (s,a),g∼pD
[log σ(f (s, a, g)) + log(1 − σ(f (s, a, g − )))],
−
geom (g|s,a),g ∼prand (g)
(12)

where σ : R → (0, 1) denotes the sigmoid function. The optimal solution to the above objective is
D
given as f (s, a, g) = log(pD
geom (g | s, a)/prand (g)). Given the equivalence between the geometric
D
future goal distribution (pgeom ) and the Monte Carlo goal-conditioned Q function (QMC ) under
the (0, 1)-sparse reward function r(s, g) = 1{g} (s) (Eysenbach et al., 2022), we get the following
relation: f (s, a, g) = log QMC (s, a, g) + C(g), where C is √
a function that only depends on g. In
practice, f is parameterized as f (s, a, g) = ϕ(s, a)⊤ ψ(g)/ d with ϕ : S × A → Z = Rd and
ψ : S → Z = Rd (note that this inner-product parameterization is universal (Park et al., 2024c)), and
we use the future goals from the other states in the same batch as g − . We also employ the doublevalue learning technique for f (Eysenbach et al., 2022).
For policy extraction, in continuous-action MDPs, we employ DDPG+BC (Equation (6)) using f
instead of Q. In discrete-action MDPs, we use AWR (Equation (5)) using (f, f V ) instead of (Q, V ),
where we additionally train a contrastive value function f V (s, g) : S × S → R using
D
JCRL−V (f V ) = Es∼pD (s),g∼pD
[log σ(f V (s, g)) + log(1 − σ(f V (s, g − )))],
−
geom (g|s),g ∼prand (g)
(13)

with a similar inner product parameterization for f V .
Hierarchical implicit Q-learning (HIQL). HIQL (Park et al., 2023) is an offline GCRL algorithm
that extracts two policies from a single goal-conditioned value function. HIQL first trains GCIVL
(Equation (2)) with a parameterized value function defined as V (s, g) = Ṽ (s, ϕ(s, g)), where
ϕ : S × S → Z serves as a (state-dependent) subgoal representation function. Based on the GCIVL
value function, HIQL extracts a high-level policy π h : S × S → ∆(Z) and a low-level policy
π ℓ : S × Z → ∆(A) with the following AWR-like objectives:
h
i
h
α(V (st+k ,g)−V (st ,g))
h
JHIQL
(π h ) = E(st ,st+k )∼pD ,g∼pD
e
log
π
(ϕ(s
,
s
)
|
s
,
g)
,
t
t+k
t
(g|s
)
t
mixed
(14)
ℓ
JHIQL
(π ℓ ) = E(st ,at ,st+1 ,st+k )∼pD

h

e

α(V (st+1 ,st+k )−V (st ,st+k ))

ℓ

i

log π (at | st , ϕ(st , st+k )) ,
(15)

where we omit the arguments in pD , and k denotes a hyperparameter corresponding to the subgoal
step. For simplicity, we ignore some edge cases in the objectives above (e.g., when t + k exceeds the
trajectory boundary, in which case we truncate); we refer to Park et al. (2023) or our code for the full
details. Intuitively, the high-level policy predicts the representation of the optimal k-step subgoal,
and the low-level policy predicts the optimal action based on the predicted subgoal.
In practice, following Park et al. (2023), we use the double-value learning technique, normalize
the output of ϕ, and allow gradient flows from the low-level AWR loss into ϕ (only) in pixel-based
environments.
24

Published as a conference paper at ICLR 2025

E

I MPLEMENTATION D ETAILS

We provide the full implementation details in this section. We release the code as well as the
exact command-line flags to reproduce the entire benchmark table, datasets, and expert policies at
https://github.com/seohongpark/ogbench.
E.1

TASKS AND DATASETS

In this section, we provide further information about our tasks and datasets. We provide the basic
specifications about the environments and datasets in Tables 7 and 8.
Locomotion tasks. For OGBench AntMaze and AntSoccer, we adopt the Ant model from D4RL
AntMaze (Fu et al., 2020) (which is based on the Ant in OpenAI Gym (Brockman et al., 2016; Towers
et al., 2024), but with a more restricted joint range) and the soccer ball model from the DeepMind
Control suite (Tassa et al., 2018). For HumanoidMaze, we adopt the Humanoid model from the
DeepMind Control suite (Tassa et al., 2018).
To collect datasets, we train expert low-level directional (AntMaze and HumanoidMaze) or goalreaching (AntSoccer) policies using SAC with dense reward functions for 400K (AntMaze), 40M
(HumanoidMaze), or 12M (AntSoccer) steps. For PointMaze, we use a scripted directional expert policy. When collecting datasets, we add Gaussian noise with a standard deviation of 0.5 (pointmaze),
1.0 (explore), or 0.2 (others) to expert actions.
The success criteria for the locomotion tasks are based only on the distance between the agent (or
the ball in AntSoccer) and the goal location. In particular, the tasks do not consider joint positions
determining success, as in previous works (Fu et al., 2020; Park et al., 2023).
Manipulation tasks. We adopt the UR5e robot arm and Robotiq 2F-85 gripper models from MuJoCo
Menagerie (Zakka et al., 2022), and the drawer, window, and button box models from Meta-World (Yu
et al., 2019). The robot is end-effector controlled with a 5-D action space, where the dimensions
correspond to the displacements in the x position, y position, z position, gripper yaw, and gripper
opening. In all manipulation tasks, we place invisible walls to prevent objects from moving into
an area beyond the robot arm’s reach. These invisible walls also prevent the cube objects from
moving outside the camera viewpoint in pixel-based manipulation environments. However, since
some blind spots still exist in visual-scene even with the walls, we further filter out such rare cases
in trajectories prevent ambiguous camera observations completely.
In Puzzle, not every button configuration is reachable from the initial state. While the 3 × 3, 4 × 5,
and 4 × 6 puzzles do have this property, the 4 × 4 puzzle does not. This can be seen by computing
the rank of the nm × nm button effect matrix over F2 (the field with two elements), where n and m
denote the numbers of rows and columns, respectively. In our tasks, we ensure that every test-time
goal is solvable. Also, we note that the maximum value of the minimum number of button presses to
reach a state from another state in each puzzle is 9 (3 × 3 puzzle), 7 (4 × 4 puzzle), 20 (4 × 5 puzzle),
or 24 (4 × 6 puzzle). Each puzzle environment contains at least one evaluation goal that requires the
maximum number of presses.
The play datasets are collected by open-loop, non-Markovian scripted policies, and the noisy
datasets are collected by closed-loop, Markovian scripted policies. For the play datasets, we add
temporally correlated action noise to the expert actions to enhance state coverage. For the noisy
datasets, we first sample the degree of action noise at the beginning of each episode, and collect a
trajectory with the chosen amount of (time-independent) Gaussian action noise. This ensures high
coverage while having a sufficient number of optimal trajectories.
The success criteria for the manipulation tasks are based only on the object configurations; the arm
pose is not considered when determining success. For cubes, only the distances between the goal
positions and their current positions are considered, and their orientations are ignored.
Drawing tasks. We modify the original Powderworld environment (Frans & Isola, 2023) to make it
offline and goal-conditioned. We also re-implement Powderworld (which was originally implemented
in PyTorch) in NumPy to remove the dependency on PyTorch. We provide three versions of Powderworld tasks: powderworld-easy uses two elements (plant and stone), powderworld-medium uses
five elements (sand, water, fire, plant, and stone), and powderworld-hard uses eight elements (sand,
25

Published as a conference paper at ICLR 2025

water, fire, plant, stone, gas, wood, and ice). An action in Powderworld corresponds to drawing a
4 × 4-sized square with a specific element brush on the 32 × 32-sized board. Since naı̈vely implementing this atomic action requires up to 512-dimensional discrete actions, we split it into three sequential
8-dimensional actions that correspond to element selection, x-coordinate selection, and y-coordinate
selection. To ensure full observability, we add three additional dimensions that contain information
about the currently selected element and x coordinate to the original 32 × 32 × 3-dimensional image,
which results in a 32 × 32 × 6-dimensional observation space. When the agent selects an invalid action (which can only happen in powderworld-{easy, medium}, which has fewer than 8 elements),
the environment instead uses a randomly sampled valid action.
The datasets are collected by a scripted policy that randomly draws squares and lines or fills the entire
board with randomly selected brushes. With a probability of 0.5, it performs a random action (i.e.,
places a random element on a randomly sampled position).
For the success criterion for evaluation goals, we use the following procedure to allow for some
tolerance: For each pixel in the goal image, we check if the current image has a matching pixel that is
shifted by up to one pixel in any direction. We then compute the error as the number of pixels that do
not match, and consider the task successful if the error is below a certain threshold.

E.2

S INGLE -TASK VARIANTS

OGBench also supports standard (i.e., non-goal-conditioned) offline RL by providing single-task
variants of locomotion and manipulation tasks. To convert a goal-conditioned task into a standard
reward-maximizing task, we fix an evaluation goal and relabel the dataset with a semi-sparse reward
function. This semi-sparse reward function is defined as the negative of the number of unaccomplished
subtasks in the current state, and the episode immediately terminates when the agent completes all
subtasks of the target evaluation goal. In locomotion environments, rewards are always −1 or 0, as
there are no separate subtasks. In manipulation environments, rewards range between −ntask and 0,
where ntask denotes the number of subtasks (e.g., in puzzle-4x6, ntask = 24 as there are 24 buttons).
Each locomotion and manipulation task in OGBench provides five single-task variants that correspond
to the five evaluation goals (Appendix F), resulting in a total of 410 single-task tasks. They are named
with the suffix “singletask-task[n]” (e.g., scene-play-singletask-task2-v0), where [n]
denotes a number between 1 and 5 (inclusive). Among the five tasks in each environment, the
most representative one is chosen as the “default” task, and is aliased by the suffix “singletask”
without a task number. For example, in cube-double, the second task (standard double pickand-place; see Figure 6) is set as the default task, and cube-double-play-singletask-v0 and
cube-double-play-singletask-task2-v0 refer to the same task. Default tasks can be useful
in various ways. For instance, one may report performance only on default tasks to reduce the
computational burden, or may treat default tasks as a “training” task set for tuning hyperparameters
while using the other four tasks as a “validation” task set. We provide the list of default tasks in Table 9.
While we do not provide a separate benchmarking result on the single-task environments, a benchmarking table of several representative offline RL algorithms on 50 tasks can be found in the work by
Park et al. (2025).

E.3

O RACLE R EPRESENTATION VARIANTS

OGBench also provides oracle representation variants of locomotion and manipulation tasks, denoted by the suffix “oraclerep” (e.g., antmaze-large-navigate-oraclerep-v0). These tasks
provide low-dimensional oracle goal representations that contain only relevant information for fulfilling the goal success criterion. This corresponds to the x-y coordinates of the agent (or the ball
in antsoccer) in locomotion environments, and the positions of the cubes and the states of the objects in manipulation environments. The oraclerep tasks reduce the burden of goal representation
learning, potentially helping diagnose the bottlenecks in goal-conditioned RL algorithms. We do not
provide a separate benchmarking result for the oracle representation variants.
26

Published as a conference paper at ICLR 2025

E.4

M ETHODS

Our implementations of six offline GCRL algorithms (GCBC, GCIVL, GCIQL, QRL, CRL, and
HIQL) are based on JAX (Bradbury et al., 2018). In our benchmark, each run typically takes 2-5
hours (state-based tasks) or 5-12 hours (pixel-based tasks) on an A5000 GPU, depending on the task
and algorithm.
For benchmarking, we periodically evaluate the performance (goal success rate in percentage) of
each agent on each test-time goal with 50 rollouts every 100K steps, and report the average success
rate across the last three evaluation epochs (i.e., at 800K, 900K, and 1M steps for state-based tasks
and at 300K, 400K, and 500K steps for pixel-based tasks). That is, the performance of each agent is
averaged over 750 rollouts (3 evaluation epochs × 5 test-time goals × 50 rollouts). While we use a
relatively large number of evaluation rollouts for robustness, researchers can adjust the number of
evaluation rollouts (e.g., 20 episodes for each test-time goal) to reduce the computational burden.
We provide the full list of common hyperparameters in Table 10. We find that methods are more
sensitive to policy extraction hyperparameters (e.g., the BC coefficient in DDPG+BC) (Park et al.,
2024a), and report these in a separate table (Table 11). Specifically, for each method, we use the same
value learning hyperparameters across the benchmark except for the discount factor γ (Table 10), but
individually tune the policy extraction hyperparameters (e.g., AWR α and DDPG+BC α) for each
dataset category (Tables 10 and 11).
We apply layer normalization (Ba et al., 2016) to the value networks, but not to the policy networks.
In pixel-based environments, we use a smaller version of the IMPALA encoder (Espeholt et al., 2018).
We use random-crop image augmentation (with a probability of 0.5) for pixel-based manipulation
tasks, but not for pixel-based locomotion or drawing tasks, as we find it to be helpful mainly on
manipulation tasks. In pixel-based environments, we do not apply frame stacking for simplicity, as we
find it does not necessarily improve performance on most tasks including Visual AntMaze (although
we believe the performance on Visual HumanoidMaze can further be improved with frame stacking).
For policies, we parameterize the action distribution with a unit-variance Gaussian distribution. We
find that using a fixed standard deviation is especially important for DDPG+BC. During evaluation,
we use the deterministic mean of the learned Gaussian policy. However, in Powderworld, which has a
discrete action space, we use a stochastic policy with a temperature of 0.3 (i.e., we divide the action
logits by 0.3), as this additional stochasticity helps prevent the agent from getting stuck in certain
states.

27

Published as a conference paper at ICLR 2025

Table 6: Dataset categories. We list the dataset categories used to aggregate results in Figure 2. Note that some
datasets or tasks (e.g., PointMaze) do not belong to any of these aggregation categories (for being too simple,
too special, etc.).
Category

Datasets

Locomotion (states)

antmaze-medium-navigate-v0
antmaze-large-navigate-v0
antmaze-giant-navigate-v0
humanoidmaze-medium-navigate-v0
humanoidmaze-large-navigate-v0
humanoidmaze-giant-navigate-v0
antsoccer-arena-navigate-v0
antsoccer-medium-navigate-v0

Locomotion (pixels)

visual-antmaze-medium-navigate-v0
visual-antmaze-large-navigate-v0
visual-antmaze-giant-navigate-v0
visual-humanoidmaze-medium-navigate-v0
visual-humanoidmaze-large-navigate-v0
visual-humanoidmaze-giant-navigate-v0

Manipulation (states)

cube-single-play-v0
cube-double-play-v0
cube-triple-play-v0
cube-quadruple-play-v0
scene-play-v0
puzzle-3x3-play-v0
puzzle-4x4-play-v0
puzzle-4x5-play-v0
puzzle-4x6-play-v0

Manipulation (pixels)

visual-cube-single-play-v0
visual-cube-double-play-v0
visual-cube-triple-play-v0
visual-cube-quadruple-play-v0
visual-scene-play-v0
visual-puzzle-3x3-play-v0
visual-puzzle-4x4-play-v0
visual-puzzle-4x5-play-v0
visual-puzzle-4x6-play-v0

Drawing (pixels)

powderworld-easy-play-v0
powderworld-medium-play-v0
powderworld-hard-play-v0

Stitching (states)

antmaze-medium-stitch-v0
antmaze-large-stitch-v0
antmaze-giant-stitch-v0
humanoidmaze-medium-stitch-v0
humanoidmaze-large-stitch-v0
humanoidmaze-giant-stitch-v0
antsoccer-arena-stitch-v0
antsoccer-medium-stitch-v0

Stitching (pixels)

visual-antmaze-medium-stitch-v0
visual-antmaze-large-stitch-v0
visual-antmaze-giant-stitch-v0
visual-humanoidmaze-medium-stitch-v0
visual-humanoidmaze-large-stitch-v0
visual-humanoidmaze-giant-stitch-v0

Exploratory (states)

antmaze-medium-explore-v0
antmaze-large-explore-v0

Exploratory (pixels)

visual-antmaze-medium-explore-v0
visual-antmaze-large-explore-v0

Stochastic (states)

antmaze-teleport-navigate-v0

Stochastic (pixels)

visual-antmaze-telpeport-navigate-v0

28

Published as a conference paper at ICLR 2025

Table 7: Environment specifications. See Table 8 for the dataset specifications. Note that the episode lengths
of datasets and environments can be different.
Environment Type

Environment

State Dim.

Action Dim.

Maximum Episode Length

pointmaze

pointmaze-medium-v0
pointmaze-large-v0
pointmaze-giant-v0
pointmaze-teleport-v0

2
2
2
2

2
2
2
2

1000
1000
1000
1000

antmaze

antmaze-medium-v0
antmaze-large-v0
antmaze-giant-v0
antmaze-teleport-v0

29
29
29
29

8
8
8
8

1000
1000
1000
1000

humanoidmaze

humanoidmaze-medium-v0
humanoidmaze-large-v0
humanoidmaze-giant-v0

69
69
69

21
21
21

2000
2000
4000

antsoccer

antsoccer-arena-v0
antsoccer-medium-v0

42
42

8
8

1000
1000

visual-antmaze

visual-antmaze-medium-v0
visual-antmaze-large-v0
visual-antmaze-giant-v0
visual-antmaze-teleport-v0

64 × 64 × 3
64 × 64 × 3
64 × 64 × 3
64 × 64 × 3

8
8
8
8

1000
1000
1000
1000

visual-humanoidmaze

visual-humanoidmaze-medium-v0
visual-humanoidmaze-large-v0
visual-humanoidmaze-giant-v0

64 × 64 × 3
64 × 64 × 3
64 × 64 × 3

21
21
21

2000
2000
4000

cube

cube-single-v0
cube-double-v0
cube-triple-v0
cube-quadruple-v0

28
37
46
55

5
5
5
5

200
500
1000
1000

scene

scene-v0

40

5

750

puzzle

puzzle-3x3-v0
puzzle-4x4-v0
puzzle-4x5-v0
puzzle-4x6-v0

55
83
99
115

5
5
5
5

500
500
1000
1000

visual-cube

visual-cube-single-v0
visual-cube-double-v0
visual-cube-triple-v0
visual-cube-quadruple-v0

64 × 64 × 3
64 × 64 × 3
64 × 64 × 3
64 × 64 × 3

5
5
5
5

200
500
1000
1000

visual-scene

visual-scene-v0

64 × 64 × 3

5

750

visual-puzzle

visual-puzzle-3x3-v0
visual-puzzle-4x4-v0
visual-puzzle-4x5-v0
visual-puzzle-4x6-v0

64 × 64 × 3
64 × 64 × 3
64 × 64 × 3
64 × 64 × 3

5
5
5
5

500
500
1000
1000

powderworld

powderworld-easy-v0
powderworld-medium-v0
powderworld-hard-v0

32 × 32 × 6
32 × 32 × 6
32 × 32 × 6

8 (discrete)
8 (discrete)
8 (discrete)

500
500
500

29

Published as a conference paper at ICLR 2025

Table 8: Dataset specifications. See Table 7 for the environment specifications. Note that the episode lengths
of datasets and environments can be different.
Environment Type

Dataset Type

Dataset

# Transitions

# Episodes

Data Episode Length

navigate

pointmaze-medium-navigate-v0
pointmaze-large-navigate-v0
pointmaze-giant-navigate-v0
pointmaze-teleport-navigate-v0

1M
1M
1M
1M

1000
1000
500
1000

1000
1000
2000
1000

stitch

pointmaze-medium-stitch-v0
pointmaze-large-stitch-v0
pointmaze-giant-stitch-v0
pointmaze-teleport-stitch-v0

1M
1M
1M
1M

5000
5000
5000
5000

200
200
200
200

navigate

antmaze-medium-navigate-v0
antmaze-large-navigate-v0
antmaze-giant-navigate-v0
antmaze-teleport-navigate-v0

1M
1M
1M
1M

1000
1000
500
1000

1000
1000
2000
1000

stitch

antmaze-medium-stitch-v0
antmaze-large-stitch-v0
antmaze-giant-stitch-v0
antmaze-teleport-stitch-v0

1M
1M
1M
1M

5000
5000
5000
5000

200
200
200
200

explore

antmaze-medium-explore-v0
antmaze-large-explore-v0
antmaze-teleport-explore-v0

5M
5M
5M

10000
10000
10000

500
500
500

navigate

humanoidmaze-medium-navigate-v0
humanoidmaze-large-navigate-v0
humanoidmaze-giant-navigate-v0

2M
2M
4M

1000
1000
1000

2000
2000
4000

stitch

humanoidmaze-medium-stitch-v0
humanoidmaze-large-stitch-v0
humanoidmaze-giant-stitch-v0

2M
2M
4M

5000
5000
10000

400
400
400

navigate

antsoccer-arena-navigate-v0
antsoccer-medium-navigate-v0

1M
4M

1000
4000

1000
1000

stitch

antsoccer-arena-stitch-v0
antsoccer-medium-stitch-v0

1M
4M

5000
8000

200
500

navigate

visual-antmaze-medium-navigate-v0
visual-antmaze-large-navigate-v0
visual-antmaze-giant-navigate-v0
visual-antmaze-teleport-navigate-v0

1M
1M
1M
1M

1000
1000
500
1000

1000
1000
2000
1000

stitch

visual-antmaze-medium-stitch-v0
visual-antmaze-large-stitch-v0
visual-antmaze-giant-stitch-v0
visual-antmaze-teleport-stitch-v0

1M
1M
1M
1M

5000
5000
5000
5000

200
200
200
200

explore

visual-antmaze-medium-explore-v0
visual-antmaze-large-explore-v0
visual-antmaze-teleport-explore-v0

5M
5M
5M

10000
10000
10000

500
500
500

navigate

visual-humanoidmaze-medium-navigate-v0
visual-humanoidmaze-large-navigate-v0
visual-humanoidmaze-giant-navigate-v0

2M
2M
4M

1000
1000
1000

2000
2000
4000

stitch

visual-humanoidmaze-medium-stitch-v0
visual-humanoidmaze-large-stitch-v0
visual-humanoidmaze-giant-stitch-v0

2M
2M
4M

5000
5000
10000

400
400
400

play

cube-single-play-v0
cube-double-play-v0
cube-triple-play-v0
cube-quadruple-play-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

noisy

cube-single-noisy-v0
cube-double-noisy-v0
cube-triple-noisy-v0
cube-quadruple-noisy-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

play

scene-play-v0

1M

1000

1000

noisy

scene-noisy-v0

1M

1000

1000

play

puzzle-3x3-play-v0
puzzle-4x4-play-v0
puzzle-4x5-play-v0
puzzle-4x6-play-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

noisy

puzzle-3x3-noisy-v0
puzzle-4x4-noisy-v0
puzzle-4x5-noisy-v0
puzzle-4x6-noisy-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

play

visual-cube-single-play-v0
visual-cube-double-play-v0
visual-cube-triple-play-v0
visual-cube-quadruple-play-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

noisy

visual-cube-single-noisy-v0
visual-cube-double-noisy-v0
visual-cube-triple-noisy-v0
visual-cube-quadruple-noisy-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

play

visual-scene-play-v0

1M

1000

1000

noisy

visual-scene-noisy-v0

1M

1000

1000

play

visual-puzzle-3x3-play-v0
visual-puzzle-4x4-play-v0
visual-puzzle-4x5-play-v0
visual-puzzle-4x6-play-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

noisy

visual-puzzle-3x3-noisy-v0
visual-puzzle-4x4-noisy-v0
visual-puzzle-4x5-noisy-v0
visual-puzzle-4x6-noisy-v0

1M
1M
3M
5M

1000
1000
3000
5000

1000
1000
1000
1000

play

powderworld-easy-play-v0
powderworld-medium-play-v0
powderworld-hard-play-v0

1M
3M
5M

1000
3000
5000

1000
1000
1000

pointmaze

antmaze

humanoidmaze

antsoccer

visual-antmaze

visual-humanoidmaze

cube

scene

puzzle

visual-cube

visual-scene

visual-puzzle

powderworld

30

Published as a conference paper at ICLR 2025

Table 9: Designated default tasks for single-task environments. For single-task (singletask) variants, each
environment provides five tasks corresponding to the five evaluation goals, with the most representative one
chosen as the default task.
Environment Type

Environment

Default Task

pointmaze

pointmaze-medium-v0
pointmaze-large-v0
pointmaze-giant-v0
pointmaze-teleport-v0

task1
task1
task1
task1

antmaze

antmaze-medium-v0
antmaze-large-v0
antmaze-giant-v0
antmaze-teleport-v0

task1
task1
task1
task1

humanoidmaze

humanoidmaze-medium-v0
humanoidmaze-large-v0
humanoidmaze-giant-v0

task1
task1
task1

antsoccer

antsoccer-arena-v0
antsoccer-medium-v0

task4
task4

visual-antmaze

visual-antmaze-medium-v0
visual-antmaze-large-v0
visual-antmaze-giant-v0
visual-antmaze-teleport-v0

task1
task1
task1
task1

visual-humanoidmaze

visual-humanoidmaze-medium-v0
visual-humanoidmaze-large-v0
visual-humanoidmaze-giant-v0

task1
task1
task1

cube

cube-single-v0
cube-double-v0
cube-triple-v0
cube-quadruple-v0

task2
task2
task2
task2

scene

scene-v0

task2

puzzle

puzzle-3x3-v0
puzzle-4x4-v0
puzzle-4x5-v0
puzzle-4x6-v0

task4
task4
task2
task2

visual-cube

visual-cube-single-v0
visual-cube-double-v0
visual-cube-triple-v0
visual-cube-quadruple-v0

task2
task2
task2
task2

visual-scene

visual-scene-v0

task2

visual-puzzle

visual-puzzle-3x3-v0
visual-puzzle-4x4-v0
visual-puzzle-4x5-v0
visual-puzzle-4x6-v0

task4
task4
task2
task2

31

Published as a conference paper at ICLR 2025

Table 10: Common hyperparameters.
Hyperparameter

Value

Learning rate
Optimizer
# gradient steps
Minibatch size
MLP dimensions
Nonlinearity
Target smoothing coefficient
Discount factor γ
Image augmentation probability
GCIVL/GCIQL expectile κ
GCIQL expectile κ
QRL quasimetric
QRL latent dimension
QRL margin ϵ
CRL latent dimension
HIQL expectile κ
HIQL subgoal step k
HIQL subgoal representation dimension
D
D
D
D
Policy (pD
cur , ptraj , pgeom , prand ) ratio for pmixed
D
D
D
D
,
p
)
ratio
for
p
Value (pD
,
p
,
p
cur traj geom rand
mixed

0.0003
Adam (Kingma & Ba, 2015)
1000000 (states), 500000 (pixels)
1024 (states), 256 (pixels)
(512, 512, 512)
GELU (Hendrycks & Gimpel, 2016)
0.005
0.995 ({antmaze, pointmaze}-giant, humanoidmaze), 0.99 (others)
0.5 (pixel-based manipulation), 0 (others)
0.9
0.9
IQE (Wang & Isola, 2022)
512 (64 components × 8-dimensional latents)
0.05
512
0.7
100 (humanoidmaze), 25 (other locomotion), 10 (others)
10
(0, 0.5, 0, 0.5) (stitch), (0, 0, 0, 1) (explore), (0, 1, 0, 0) (others)
(0.2, 0, 0.5, 0.3)

32

Published as a conference paper at ICLR 2025

Table 11: Hyperparameters for policy extraction. Each cell indicates the policy extraction method and its α
value (i.e., the temperature (AWR) or the BC coefficient (DDPG+BC)).
Environment Type

Dataset Type

Dataset

GCIVL

GCIQL

QRL

CRL

HIQL

navigate

pointmaze-medium-navigate-v0
pointmaze-large-navigate-v0
pointmaze-giant-navigate-v0
pointmaze-teleport-navigate-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.0003
DDPG 0.0003
DDPG 0.0003
DDPG 0.0003

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

stitch

pointmaze-medium-stitch-v0
pointmaze-large-stitch-v0
pointmaze-giant-stitch-v0
pointmaze-teleport-stitch-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.0003
DDPG 0.0003
DDPG 0.0003
DDPG 0.0003

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

navigate

antmaze-medium-navigate-v0
antmaze-large-navigate-v0
antmaze-giant-navigate-v0
antmaze-teleport-navigate-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

stitch

antmaze-medium-stitch-v0
antmaze-large-stitch-v0
antmaze-giant-stitch-v0
antmaze-teleport-stitch-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

explore

antmaze-medium-explore-v0
antmaze-large-explore-v0
antmaze-teleport-explore-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.01
DDPG 0.01
DDPG 0.01

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.003
DDPG 0.003
DDPG 0.003

AWR 10.0
AWR 10.0
AWR 10.0

navigate

humanoidmaze-medium-navigate-v0
humanoidmaze-large-navigate-v0
humanoidmaze-giant-navigate-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1
DDPG 0.1

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0

stitch

humanoidmaze-medium-stitch-v0
humanoidmaze-large-stitch-v0
humanoidmaze-giant-stitch-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1
DDPG 0.1

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0

navigate

antsoccer-arena-navigate-v0
antsoccer-medium-navigate-v0

AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1

DDPG 0.003
DDPG 0.003

DDPG 0.3
DDPG 0.3

AWR 3.0
AWR 3.0

stitch

antsoccer-arena-stitch-v0
antsoccer-medium-stitch-v0

AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1

DDPG 0.003
DDPG 0.003

DDPG 0.3
DDPG 0.3

AWR 3.0
AWR 3.0

navigate

visual-antmaze-medium-navigate-v0
visual-antmaze-large-navigate-v0
visual-antmaze-giant-navigate-v0
visual-antmaze-teleport-navigate-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

stitch

visual-antmaze-medium-stitch-v0
visual-antmaze-large-stitch-v0
visual-antmaze-giant-stitch-v0
visual-antmaze-teleport-stitch-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 0.003
DDPG 0.003
DDPG 0.003
DDPG 0.003

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

explore

visual-antmaze-medium-explore-v0
visual-antmaze-large-explore-v0
visual-antmaze-teleport-explore-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.01
DDPG 0.01
DDPG 0.01

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.003
DDPG 0.003
DDPG 0.003

AWR 10.0
AWR 10.0
AWR 10.0

navigate

visual-humanoidmaze-medium-navigate-v0
visual-humanoidmaze-large-navigate-v0
visual-humanoidmaze-giant-navigate-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1
DDPG 0.1

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0

stitch

visual-humanoidmaze-medium-stitch-v0
visual-humanoidmaze-large-stitch-v0
visual-humanoidmaze-giant-stitch-v0

AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.1
DDPG 0.1
DDPG 0.1

DDPG 0.001
DDPG 0.001
DDPG 0.001

DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0

play

cube-single-play-v0
cube-double-play-v0
cube-triple-play-v0
cube-quadruple-play-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 1.0
DDPG 1.0
DDPG 1.0
DDPG 1.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 3.0
DDPG 3.0
DDPG 3.0
DDPG 3.0

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

noisy

cube-single-noisy-v0
cube-double-noisy-v0
cube-triple-noisy-v0
cube-quadruple-noisy-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

play

scene-play-v0

AWR 10.0

DDPG 1.0

DDPG 0.3

DDPG 3.0

AWR 3.0

noisy

scene-noisy-v0

AWR 10.0

DDPG 0.03

DDPG 0.03

DDPG 0.1

AWR 3.0

play

puzzle-3x3-play-v0
puzzle-4x4-play-v0
puzzle-4x5-play-v0
puzzle-4x6-play-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 1.0
DDPG 1.0
DDPG 1.0
DDPG 1.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 3.0
DDPG 3.0
DDPG 3.0
DDPG 3.0

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

noisy

puzzle-3x3-noisy-v0
puzzle-4x4-noisy-v0
puzzle-4x5-noisy-v0
puzzle-4x6-noisy-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

play

visual-cube-single-play-v0
visual-cube-double-play-v0
visual-cube-triple-play-v0
visual-cube-quadruple-play-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 1.0
DDPG 1.0
DDPG 1.0
DDPG 1.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 3.0
DDPG 3.0
DDPG 3.0
DDPG 3.0

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

noisy

visual-cube-single-noisy-v0
visual-cube-double-noisy-v0
visual-cube-triple-noisy-v0
visual-cube-quadruple-noisy-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

play

visual-scene-play-v0

AWR 10.0

DDPG 1.0

DDPG 0.3

DDPG 3.0

AWR 3.0

noisy

visual-scene-noisy-v0

AWR 10.0

DDPG 0.03

DDPG 0.03

DDPG 0.1

AWR 3.0

play

visual-puzzle-3x3-play-v0
visual-puzzle-4x4-play-v0
visual-puzzle-4x5-play-v0
visual-puzzle-4x6-play-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 1.0
DDPG 1.0
DDPG 1.0
DDPG 1.0

DDPG 0.3
DDPG 0.3
DDPG 0.3
DDPG 0.3

DDPG 3.0
DDPG 3.0
DDPG 3.0
DDPG 3.0

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

noisy

visual-puzzle-3x3-noisy-v0
visual-puzzle-4x4-noisy-v0
visual-puzzle-4x5-noisy-v0
visual-puzzle-4x6-noisy-v0

AWR 10.0
AWR 10.0
AWR 10.0
AWR 10.0

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.03
DDPG 0.03
DDPG 0.03
DDPG 0.03

DDPG 0.1
DDPG 0.1
DDPG 0.1
DDPG 0.1

AWR 3.0
AWR 3.0
AWR 3.0
AWR 3.0

play

powderworld-easy-play-v0
powderworld-medium-play-v0
powderworld-hard-play-v0

AWR 3.0
AWR 3.0
AWR 3.0

AWR 3.0
AWR 3.0
AWR 3.0

AWR 3.0
AWR 3.0
AWR 3.0

AWR 3.0
AWR 3.0
AWR 3.0

AWR 3.0
AWR 3.0
AWR 3.0

pointmaze

antmaze

humanoidmaze

antsoccer

visual-antmaze

visual-humanoidmaze

cube

scene

puzzle

visual-cube

visual-scene

visual-puzzle

powderworld

33

Published as a conference paper at ICLR 2025

F

E VALUATION G OALS AND P ER -G OAL B ENCHMARKING R ESULTS

Each task in OGBench provides five evaluation goals. We provide their full image descriptions in
Figures 4 to 10, and the full per-goal evaluation results in Tables 12 to 24, which share the same
format as Table 2.

task1

task2
task3
task4
{pointmaze, antmaze, humanoidmaze}-medium

task5

task1

task2
task3
task4
{pointmaze, antmaze, humanoidmaze}-large

task5

task1

task2
task3
task4
{pointmaze, antmaze, humanoidmaze}-giant

task5

task1

task2
task3
task4
{pointmaze, antmaze}-teleport

task5

Figure 4: PointMaze, AntMaze, and HumanoidMaze goals.

34

Published as a conference paper at ICLR 2025

task1

task2

task3
antsoccer-arena

task4

task5

task1

task2

task3
antsoccer-medium

task4

task5

Figure 5: AntSoccer goals.

35

Published as a conference paper at ICLR 2025

task1

task2

task3

task4

task5

horizontal

vertical1

vertical2

diagonal1

diagnoal2

cube-single

task1

task2

task3

task4

task5

single-pnp

double-pnp1

double-pnp2

swap

stack

cube-double

task1

task2

task3

task4

task5

single-pnp

triple-pnp

pnp-from-stack

cycle

stack

cube-triple

task1

task2

task3

task4

task5

double-pnp

quadruple-pnp

pnp-from-square

cycle

stack

cube-quadruple
Figure 6: Cube goals.

36

Published as a conference paper at ICLR 2025

task1

task2

task3

task4

task5

open

unlock-and-lock

rearrange-medium

put-in-drawer

rearrange-hard

scene
Figure 7: Scene goals.

37

Published as a conference paper at ICLR 2025

task1

task2

task3
puzzle-3x3

task4

task5

task1

task2

task3
puzzle-4x4

task4

task5

Figure 8: Puzzle goals.

38

Published as a conference paper at ICLR 2025

task1

task2

task3
puzzle-4x5

task4

task5

task1

task2

task3
puzzle-4x6

task4

task5

Figure 9: Puzzle goals.

39

Published as a conference paper at ICLR 2025

task1

task2

task3

task4

task5

plant

stone

square

four-squares

mosaic

powderworld-easy

task1

task2

task3

task4

task5

squares

water-plant

sandpile

two-rooms

elements

powderworld-medium

task1

task2

task3

task4

task5

bubbles

firework

three-rooms

four-squares

ice-plant

powderworld-hard
Figure 10: Powderworld goals.

40

Published as a conference paper at ICLR 2025

Table 12: Full results on PointMaze.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

pointmaze-medium-navigate-v0

task1
task2
task3
task4
task5
overall

30 ±27
3 ±2
5 ±5
0 ±1
4 ±3
9 ±6

88 ±16
95 ±10
37 ±28
2 ±2
92 ±7
63 ±6

97 ±4
76 ±29
10 ±28
0 ±0
79 ±6
53 ±8

100 ±0
94 ±17
23 ±20
94 ±14
97 ±8
82 ±5

20 ±6
45 ±25
30 ±4
28 ±29
24 ±13
29 ±7

99 ±1
87 ±7
55 ±13
82 ±12
70 ±10
79 ±5

pointmaze-large-navigate-v0

task1
task2
task3
task4
task5
overall

63 ±11
1 ±2
10 ±7
20 ±18
52 ±17
29 ±6

76 ±23
0 ±0
98 ±5
0 ±0
53 ±20
45 ±5

86 ±14
0 ±0
83 ±8
0 ±0
0 ±0
34 ±3

95 ±8
100 ±0
40 ±50
96 ±7
96 ±7
86 ±9

42 ±27
31 ±24
78 ±7
24 ±14
20 ±10
39 ±7

83 ±13
2 ±7
88 ±10
72 ±19
46 ±16
58 ±5

pointmaze-giant-navigate-v0

task1
task2
task3
task4
task5
overall

1 ±3
1 ±4
0 ±0
0 ±0
5 ±12
1 ±2

0 ±0
0 ±0
0 ±1
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

98 ±7
92 ±16
68 ±27
66 ±20
19 ±32
68 ±7

6 ±15
28 ±10
9 ±5
64 ±17
29 ±28
27 ±10

0 ±0
72 ±17
32 ±11
60 ±22
66 ±20
46 ±9

pointmaze-teleport-navigate-v0

task1
task2
task3
task4
task5
overall

1 ±2
4 ±6
50 ±4
33 ±13
38 ±6
25 ±3

33 ±12
49 ±2
46 ±5
49 ±4
48 ±4
45 ±3

0 ±1
39 ±14
31 ±19
42 ±13
9 ±9
24 ±7

0 ±0
8 ±10
2 ±5
12 ±16
1 ±2
4 ±4

3 ±3
30 ±23
26 ±6
40 ±11
20 ±15
24 ±6

5 ±5
6 ±6
39 ±9
24 ±11
17 ±8
18 ±4

pointmaze-medium-stitch-v0

task1
task2
task3
task4
task5
overall

21 ±29
32 ±35
33 ±34
0 ±0
29 ±37
23 ±18

76 ±14
79 ±23
69 ±16
41 ±37
84 ±11
70 ±14

56 ±24
26 ±19
0 ±0
0 ±0
22 ±22
21 ±9

94 ±13
81 ±34
66 ±29
68 ±32
92 ±9
80 ±12

0 ±0
0 ±0
2 ±3
0 ±0
0 ±0
0 ±1

77 ±14
61 ±23
82 ±13
92 ±6
59 ±9
74 ±6

pointmaze-large-stitch-v0

task1
task2
task3
task4
task5
overall

8 ±13
0 ±0
26 ±28
0 ±0
0 ±0
7 ±5

0 ±1
0 ±0
60 ±29
0 ±0
0 ±0
12 ±6

56 ±11
0 ±0
98 ±4
0 ±0
0 ±0
31 ±2

100 ±1
74 ±37
74 ±23
88 ±32
85 ±22
84 ±15

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

3 ±5
0 ±0
59 ±25
1 ±4
0 ±0
13 ±6

pointmaze-giant-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

99 ±2
80 ±27
3 ±5
63 ±23
4 ±8
50 ±8

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

pointmaze-teleport-stitch-v0

task1
task2
task3
task4
task5
overall

28 ±20
13 ±15
48 ±8
40 ±16
29 ±16
31 ±9

34 ±14
41 ±8
50 ±5
50 ±6
48 ±5
44 ±2

0 ±0
12 ±14
47 ±2
46 ±5
21 ±7
25 ±3

0 ±0
7 ±7
15 ±13
19 ±12
1 ±3
9 ±5

0 ±0
0 ±0
0 ±0
8 ±8
13 ±13
4 ±3

24 ±13
23 ±11
46 ±9
46 ±5
31 ±10
34 ±4

navigate

pointmaze

stitch

41

Published as a conference paper at ICLR 2025

Table 13: Full results on AntMaze.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

antmaze-medium-navigate-v0

task1
task2
task3
task4
task5
overall

35 ±9
21 ±7
24 ±6
28 ±7
37 ±10
29 ±4

81 ±10
85 ±5
60 ±13
42 ±25
92 ±3
72 ±8

63 ±9
78 ±8
71 ±8
59 ±12
85 ±7
71 ±4

93 ±2
90 ±5
86 ±6
83 ±4
88 ±8
88 ±3

97 ±1
95 ±2
92 ±3
94 ±5
96 ±2
95 ±1

94 ±2
97 ±1
96 ±2
96 ±2
96 ±2
96 ±1

antmaze-large-navigate-v0

task1
task2
task3
task4
task5
overall

6 ±3
16 ±4
65 ±4
14 ±3
18 ±4
24 ±2

16 ±12
5 ±6
49 ±18
2 ±2
5 ±2
16 ±5

21 ±6
25 ±7
80 ±5
19 ±6
26 ±9
34 ±4

71 ±15
77 ±7
94 ±2
64 ±8
67 ±9
75 ±6

91 ±3
62 ±14
91 ±2
85 ±11
85 ±3
83 ±4

93 ±3
78 ±9
96 ±2
94 ±2
94 ±3
91 ±2

antmaze-giant-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
1 ±1
0 ±0

1 ±2
17 ±5
14 ±8
18 ±6
18 ±5
14 ±3

2 ±2
21 ±10
5 ±5
35 ±9
16 ±10
16 ±3

47 ±10
74 ±5
55 ±7
69 ±5
82 ±4
65 ±5

antmaze-teleport-navigate-v0

task1
task2
task3
task4
task5
overall

17 ±5
51 ±5
22 ±3
25 ±5
14 ±6
26 ±3

35 ±5
41 ±5
36 ±8
45 ±3
38 ±6
39 ±3

26 ±5
58 ±8
31 ±5
33 ±5
26 ±9
35 ±5

31 ±6
47 ±22
35 ±6
33 ±6
28 ±8
35 ±5

35 ±5
92 ±3
47 ±4
50 ±2
44 ±3
53 ±2

37 ±5
66 ±8
37 ±5
30 ±2
41 ±8
42 ±3

antmaze-medium-stitch-v0

task1
task2
task3
task4
task5
overall

70 ±33
65 ±19
21 ±15
1 ±2
70 ±33
45 ±11

76 ±13
80 ±4
16 ±12
0 ±0
47 ±20
44 ±6

17 ±12
22 ±16
41 ±9
32 ±9
34 ±14
29 ±6

43 ±20
61 ±12
72 ±29
80 ±9
41 ±18
59 ±7

43 ±10
46 ±14
46 ±17
53 ±19
75 ±8
53 ±6

92 ±2
94 ±3
95 ±2
93 ±2
95 ±3
94 ±1

antmaze-large-stitch-v0

task1
task2
task3
task4
task5
overall

2 ±2
0 ±0
15 ±14
0 ±0
0 ±0
3 ±3

23 ±9
0 ±0
69 ±6
0 ±0
0 ±0
18 ±2

0 ±0
0 ±0
37 ±10
0 ±0
0 ±0
7 ±2

7 ±5
10 ±5
73 ±8
1 ±1
1 ±1
18 ±2

1 ±1
4 ±4
43 ±11
5 ±5
1 ±2
11 ±2

85 ±5
24 ±16
94 ±3
70 ±8
60 ±9
67 ±5

antmaze-giant-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
2 ±2
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±1
5 ±5
0 ±0
3 ±3
0 ±1
2 ±2

antmaze-teleport-stitch-v0

task1
task2
task3
task4
task5
overall

21 ±13
39 ±12
34 ±12
46 ±6
16 ±14
31 ±6

39 ±7
44 ±6
36 ±8
44 ±4
33 ±6
39 ±3

12 ±4
18 ±7
18 ±4
18 ±5
17 ±6
17 ±2

22 ±6
22 ±6
25 ±7
24 ±9
26 ±5
24 ±5

30 ±6
30 ±4
23 ±11
38 ±4
32 ±7
31 ±4

44 ±5
42 ±3
26 ±4
26 ±4
40 ±6
36 ±2

antmaze-medium-explore-v0

task1
task2
task3
task4
task5
overall

3 ±6
1 ±2
1 ±2
0 ±0
3 ±4
2 ±1

10 ±8
74 ±9
0 ±0
0 ±0
10 ±6
19 ±3

12 ±6
53 ±8
0 ±0
0 ±0
0 ±0
13 ±2

1 ±1
1 ±1
3 ±5
0 ±0
1 ±1
1 ±1

2 ±2
8 ±6
4 ±6
0 ±0
2 ±2
3 ±2

29 ±17
84 ±10
18 ±24
0 ±0
52 ±27
37 ±10

antmaze-large-explore-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

37 ±12
0 ±0
12 ±6
0 ±0
0 ±0
10 ±3

1 ±1
0 ±0
1 ±1
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±1
0 ±0
1 ±1
0 ±0
0 ±0
0 ±0

1 ±3
0 ±0
18 ±24
0 ±0
0 ±0
4 ±5

antmaze-teleport-explore-v0

task1
task2
task3
task4
task5
overall

2 ±2
0 ±0
4 ±3
2 ±2
4 ±2
2 ±1

32 ±4
2 ±4
48 ±3
47 ±5
31 ±2
32 ±2

0 ±1
8 ±6
13 ±8
14 ±8
2 ±1
7 ±3

0 ±0
0 ±1
4 ±4
4 ±4
3 ±3
2 ±2

2 ±1
5 ±4
47 ±6
16 ±12
28 ±5
20 ±2

32 ±11
33 ±17
34 ±16
37 ±19
34 ±14
34 ±15

navigate

antmaze

stitch

explore

42

Published as a conference paper at ICLR 2025

Table 14: Full results on HumanoidMaze.
Environment Type

Dataset Type

navigate

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

humanoidmaze-medium-navigate-v0

task1
task2
task3
task4
task5
overall

4 ±1
8 ±4
12 ±3
2 ±1
12 ±4
8 ±2

22 ±5
42 ±8
15 ±3
0 ±0
40 ±8
24 ±2

23 ±6
49 ±6
12 ±6
1 ±0
51 ±8
27 ±2

12 ±7
25 ±8
25 ±10
16 ±7
29 ±12
21 ±8

84 ±3
80 ±5
43 ±11
5 ±5
87 ±7
60 ±4

95 ±2
96 ±2
79 ±6
75 ±6
97 ±1
89 ±2

humanoidmaze-large-navigate-v0

task1
task2
task3
task4
task5
overall

1 ±1
0 ±0
3 ±1
2 ±1
1 ±1
1 ±0

6 ±2
0 ±0
6 ±2
0 ±0
1 ±1
2 ±1

3 ±2
0 ±0
5 ±2
1 ±1
1 ±1
2 ±1

3 ±2
0 ±0
17 ±6
4 ±2
2 ±1
5 ±1

36 ±11
0 ±0
54 ±17
23 ±11
6 ±4
24 ±4

67 ±4
2 ±3
88 ±3
42 ±11
47 ±10
49 ±4

humanoidmaze-giant-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
1 ±1
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
1 ±1
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
2 ±1
0 ±0
0 ±0
2 ±1
1 ±0

1 ±1
9 ±5
2 ±2
3 ±2
1 ±1
3 ±2

13 ±7
35 ±11
11 ±4
2 ±2
2 ±2
12 ±4

humanoidmaze-medium-stitch-v0

task1
task2
task3
task4
task5
overall

20 ±7
49 ±12
24 ±8
3 ±2
49 ±8
29 ±5

13 ±3
7 ±2
25 ±3
1 ±1
16 ±3
12 ±2

12 ±3
8 ±5
20 ±7
2 ±2
18 ±7
12 ±3

6 ±5
13 ±4
30 ±6
18 ±5
22 ±2
18 ±2

27 ±7
37 ±7
40 ±4
28 ±7
49 ±5
36 ±2

84 ±5
94 ±2
86 ±4
86 ±4
90 ±4
88 ±2

humanoidmaze-large-stitch-v0

task1
task2
task3
task4
task5
overall

3 ±4
0 ±0
20 ±11
2 ±1
2 ±2
6 ±3

2 ±1
0 ±0
3 ±2
1 ±1
1 ±1
1 ±1

1 ±1
0 ±0
1 ±1
0 ±1
0 ±0
0 ±0

0 ±0
0 ±0
16 ±7
1 ±1
0 ±0
3 ±1

0 ±0
0 ±0
13 ±3
4 ±1
3 ±1
4 ±1

21 ±5
5 ±2
84 ±4
19 ±4
12 ±2
28 ±3

humanoidmaze-giant-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
1 ±1
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
1 ±1
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±1
0 ±0

1 ±2
12 ±6
2 ±2
1 ±1
0 ±1
3 ±2

humanoidmaze

stitch

Table 15: Full results on AntSoccer.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

antsoccer-arena-navigate-v0

task1
task2
task3
task4
task5
overall

7 ±3
10 ±3
3 ±2
3 ±1
3 ±1
5 ±1

61 ±4
45 ±5
62 ±6
23 ±5
42 ±6
47 ±3

60 ±6
56 ±4
63 ±6
28 ±6
42 ±7
50 ±2

12 ±2
8 ±4
12 ±2
3 ±2
3 ±3
8 ±2

32 ±5
27 ±4
28 ±5
11 ±2
15 ±4
23 ±2

67 ±4
59 ±4
76 ±4
30 ±3
56 ±4
58 ±2

antsoccer-medium-navigate-v0

task1
task2
task3
task4
task5
overall

9 ±3
0 ±0
0 ±0
0 ±0
1 ±1
2 ±0

17 ±5
0 ±0
0 ±0
0 ±0
3 ±1
4 ±1

29 ±5
0 ±0
0 ±1
1 ±1
4 ±3
7 ±1

8 ±8
0 ±0
1 ±1
1 ±1
1 ±2
2 ±2

13 ±4
0 ±0
0 ±1
1 ±1
2 ±1
3 ±1

45 ±6
0 ±0
2 ±1
3 ±1
13 ±5
13 ±2

antsoccer-arena-stitch-v0

task1
task2
task3
task4
task5
overall

73 ±5
36 ±19
6 ±15
7 ±12
0 ±0
24 ±8

37 ±4
13 ±4
34 ±9
11 ±3
12 ±3
21 ±3

6 ±2
2 ±1
1 ±1
0 ±0
1 ±1
2 ±0

2 ±1
2 ±2
1 ±1
0 ±0
0 ±0
1 ±1

2 ±2
1 ±0
0 ±0
0 ±0
0 ±0
1 ±0

24 ±2
14 ±4
20 ±3
7 ±2
12 ±5
15 ±1

antsoccer-medium-stitch-v0

task1
task2
task3
task4
task5
overall

10 ±7
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

4 ±2
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

21 ±6
0 ±0
0 ±0
0 ±0
0 ±1
4 ±1

navigate

antsoccer

stitch

43

Published as a conference paper at ICLR 2025

Table 16: Full results on Visual AntMaze.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

visual-antmaze-medium-navigate-v0

task1
task2
task3
task4
task5
overall

17 ±6
8 ±2
17 ±1
12 ±2
4 ±2
11 ±2

30 ±7
21 ±6
24 ±5
21 ±3
16 ±5
22 ±2

16 ±3
7 ±2
16 ±4
9 ±2
6 ±2
11 ±1

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

92 ±2
94 ±2
98 ±1
94 ±2
94 ±2
94 ±1

90 ±4
92 ±7
94 ±4
94 ±2
94 ±5
93 ±4

visual-antmaze-large-navigate-v0

task1
task2
task3
task4
task5
overall

3 ±1
4 ±3
4 ±2
4 ±2
4 ±2
4 ±0

7 ±2
4 ±1
6 ±2
5 ±3
5 ±1
5 ±1

4 ±3
2 ±1
4 ±1
6 ±1
4 ±2
4 ±1

0 ±0
0 ±0
1 ±1
0 ±1
0 ±0
0 ±0

78 ±5
80 ±3
90 ±3
88 ±3
83 ±2
84 ±1

60 ±10
28 ±9
85 ±10
46 ±7
44 ±10
53 ±9

visual-antmaze-giant-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
1 ±1
0 ±0
0 ±1
1 ±1
0 ±0

0 ±0
2 ±1
0 ±0
0 ±1
2 ±3
1 ±1

0 ±0
1 ±1
0 ±0
0 ±0
1 ±1
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±1
0 ±0

17 ±2
73 ±9
22 ±6
47 ±5
77 ±5
47 ±2

2 ±1
12 ±8
2 ±3
4 ±2
13 ±11
6 ±4

visual-antmaze-teleport-navigate-v0

task1
task2
task3
task4
task5
overall

2 ±2
6 ±3
9 ±1
10 ±2
1 ±1
5 ±1

6 ±1
9 ±3
12 ±3
10 ±2
3 ±1
8 ±1

2 ±1
9 ±2
9 ±2
8 ±3
3 ±1
6 ±1

3 ±2
6 ±4
10 ±4
6 ±4
4 ±2
6 ±3

32 ±3
73 ±8
47 ±3
50 ±4
36 ±5
48 ±2

32 ±5
40 ±6
33 ±1
44 ±5
33 ±7
37 ±2

visual-antmaze-medium-stitch-v0

task1
task2
task3
task4
task5
overall

80 ±4
90 ±4
69 ±18
1 ±1
97 ±1
67 ±4

0 ±1
1 ±2
15 ±6
7 ±4
6 ±3
6 ±2

0 ±0
0 ±0
8 ±1
3 ±1
1 ±1
2 ±0

0 ±0
0 ±0
0 ±0
0 ±1
0 ±0
0 ±0

33 ±4
69 ±5
88 ±1
70 ±12
85 ±5
69 ±2

75 ±8
85 ±7
92 ±1
88 ±4
93 ±1
87 ±2

visual-antmaze-large-stitch-v0

task1
task2
task3
task4
task5
overall

26 ±11
0 ±0
73 ±14
7 ±5
11 ±5
24 ±3

0 ±0
0 ±0
3 ±2
1 ±1
0 ±0
1 ±1

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
2 ±2
1 ±1
0 ±0
1 ±1

6 ±1
2 ±1
36 ±10
8 ±1
5 ±2
11 ±3

36 ±5
3 ±2
87 ±6
7 ±4
6 ±1
28 ±2

visual-antmaze-giant-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
1 ±2
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±1
0 ±0

0 ±0
1 ±1
0 ±0
0 ±0
0 ±0
0 ±0

visual-antmaze-teleport-stitch-v0

task1
task2
task3
task4
task5
overall

37 ±4
36 ±3
17 ±6
39 ±9
29 ±1
32 ±3

2 ±2
2 ±1
2 ±1
1 ±1
1 ±1
1 ±1

1 ±1
1 ±1
2 ±1
0 ±0
1 ±1
1 ±0

0 ±0
1 ±1
3 ±4
2 ±3
1 ±1
1 ±2

20 ±5
40 ±9
32 ±9
45 ±7
22 ±9
32 ±6

36 ±5
38 ±3
36 ±5
37 ±6
38 ±5
37 ±4

visual-antmaze-medium-explore-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
1 ±2
0 ±0
0 ±0
0 ±0

visual-antmaze-large-explore-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

visual-antmaze-teleport-explore-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±1
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±1
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
3 ±1
0 ±0
2 ±2
1 ±0

0 ±0
0 ±0
38 ±8
28 ±13
27 ±18
19 ±8

navigate

visual-antmaze

stitch

explore

44

Published as a conference paper at ICLR 2025

Table 17: Full results on Visual HumanoidMaze.
Environment Type

Dataset Type

navigate

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

visual-humanoidmaze-medium-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
2 ±1
0 ±0
3 ±2
1 ±0

0 ±0
0 ±0
0 ±1
0 ±0
0 ±1
0 ±0

visual-humanoidmaze-large-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

visual-humanoidmaze-giant-navigate-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

visual-humanoidmaze-medium-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
3 ±1
0 ±0
1 ±1
1 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
3 ±2
0 ±0
0 ±0
1 ±0

0 ±0
0 ±0
1 ±2
0 ±0
0 ±0
0 ±0

visual-humanoidmaze-large-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

visual-humanoidmaze-giant-stitch-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

visual-humanoidmaze

stitch

Table 18: Full results on Cube.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

cube-single-play-v0

task1
task2
task3
task4
task5
overall

7 ±3
5 ±2
7 ±3
4 ±2
4 ±2
6 ±2

57 ±6
51 ±6
55 ±6
50 ±4
52 ±6
53 ±4

71 ±9
71 ±6
70 ±6
61 ±8
67 ±7
68 ±6

6 ±2
5 ±2
4 ±1
4 ±2
4 ±3
5 ±1

20 ±6
20 ±4
21 ±6
16 ±3
15 ±3
19 ±2

15 ±5
16 ±5
16 ±3
14 ±5
13 ±4
15 ±3

cube-double-play-v0

task1
task2
task3
task4
task5
overall

6 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

58 ±5
51 ±6
42 ±7
7 ±2
21 ±1
36 ±3

74 ±8
55 ±11
45 ±7
4 ±3
23 ±6
40 ±5

6 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

30 ±7
9 ±2
6 ±1
0 ±0
3 ±1
10 ±2

22 ±6
4 ±3
3 ±2
1 ±1
2 ±1
6 ±2

cube-triple-play-v0

task1
task2
task3
task4
task5
overall

5 ±4
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

3 ±1
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

13 ±3
0 ±0
0 ±0
0 ±0
0 ±0
3 ±1

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

19 ±5
2 ±1
1 ±1
0 ±0
0 ±0
4 ±1

12 ±6
0 ±0
0 ±0
0 ±0
0 ±0
3 ±1

cube-quadruple-play-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

cube-single-noisy-v0

task1
task2
task3
task4
task5
overall

5 ±3
7 ±5
1 ±1
16 ±5
12 ±5
8 ±3

71 ±12
70 ±11
67 ±10
76 ±10
70 ±12
71 ±9

100 ±1
100 ±0
99 ±1
98 ±2
100 ±0
99 ±1

17 ±12
23 ±8
4 ±3
47 ±14
37 ±9
25 ±6

39 ±4
39 ±7
36 ±7
36 ±4
42 ±5
38 ±2

48 ±6
39 ±7
41 ±7
36 ±6
44 ±9
41 ±6

cube-double-noisy-v0

task1
task2
task3
task4
task5
overall

7 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

53 ±11
10 ±4
1 ±1
4 ±2
4 ±2
14 ±3

64 ±8
16 ±4
6 ±4
11 ±3
20 ±4
23 ±3

16 ±5
0 ±0
0 ±0
0 ±1
0 ±0
3 ±1

9 ±5
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

10 ±3
1 ±0
1 ±1
0 ±1
0 ±1
2 ±1

cube-triple-noisy-v0

task1
task2
task3
task4
task5
overall

6 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

44 ±7
0 ±0
2 ±1
0 ±0
0 ±0
9 ±1

8 ±2
1 ±1
0 ±0
0 ±0
0 ±0
2 ±1

5 ±2
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

13 ±6
0 ±0
0 ±0
0 ±0
0 ±0
3 ±1

8 ±3
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

cube-quadruple-noisy-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

play

cube

noisy

45

Published as a conference paper at ICLR 2025

Table 19: Full results on Scene.
Environment Type

Dataset Type

play

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

scene-play-v0

task1
task2
task3
task4
task5
overall

18 ±7
1 ±1
2 ±1
3 ±2
0 ±0
5 ±1

75 ±5
62 ±8
64 ±7
7 ±4
2 ±1
42 ±4

93 ±4
82 ±8
72 ±10
8 ±3
1 ±1
51 ±4

19 ±4
1 ±1
1 ±1
5 ±2
0 ±1
5 ±1

49 ±7
12 ±4
26 ±8
5 ±2
1 ±1
19 ±2

40 ±4
40 ±9
36 ±5
55 ±5
20 ±5
38 ±3

scene-noisy-v0

task1
task2
task3
task4
task5
overall

6 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

60 ±11
42 ±11
27 ±6
3 ±3
0 ±0
26 ±5

50 ±5
52 ±13
28 ±5
0 ±0
0 ±0
26 ±2

39 ±10
2 ±1
1 ±1
3 ±3
0 ±0
9 ±2

5 ±4
0 ±0
0 ±1
0 ±0
0 ±0
1 ±1

68 ±5
29 ±6
17 ±6
10 ±8
2 ±2
25 ±4

scene

noisy

Table 20: Full results on Puzzle.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

puzzle-3x3-play-v0

task1
task2
task3
task4
task5
overall

5 ±1
2 ±1
1 ±1
1 ±1
1 ±0
2 ±0

17 ±4
4 ±2
3 ±1
3 ±1
2 ±1
6 ±1

99 ±2
96 ±3
95 ±1
91 ±3
94 ±2
95 ±1

3 ±2
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

11 ±3
2 ±1
1 ±1
2 ±1
2 ±1
3 ±1

29 ±4
11 ±3
7 ±3
5 ±1
8 ±3
12 ±2

puzzle-4x4-play-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

17 ±4
13 ±5
12 ±3
11 ±3
10 ±4
13 ±2

42 ±7
2 ±1
40 ±5
23 ±5
23 ±5
26 ±3

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

1 ±1
0 ±0
0 ±0
0 ±1
0 ±0
0 ±0

10 ±3
9 ±4
7 ±3
6 ±2
5 ±2
7 ±2

puzzle-4x5-play-v0

task1
task2
task3
task4
task5
overall

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

33 ±6
0 ±0
0 ±0
0 ±0
0 ±0
7 ±1

71 ±5
0 ±0
0 ±0
0 ±0
0 ±0
14 ±1

0 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

6 ±2
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

17 ±5
1 ±0
0 ±0
0 ±0
0 ±0
4 ±1

puzzle-4x6-play-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

43 ±8
7 ±4
0 ±0
0 ±0
0 ±0
10 ±2

52 ±6
5 ±3
0 ±0
0 ±0
0 ±0
12 ±1

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

12 ±5
7 ±3
0 ±0
0 ±0
0 ±0
4 ±1

12 ±5
2 ±1
0 ±0
0 ±0
0 ±0
3 ±1

puzzle-3x3-noisy-v0

task1
task2
task3
task4
task5
overall

4 ±2
0 ±0
0 ±0
0 ±0
0 ±0
1 ±0

89 ±8
42 ±29
26 ±20
23 ±20
31 ±19
42 ±19

100 ±0
88 ±9
99 ±1
94 ±3
88 ±5
94 ±3

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

76 ±8
26 ±9
15 ±9
12 ±9
18 ±6
30 ±6

67 ±10
54 ±11
43 ±12
41 ±13
47 ±15
51 ±11

puzzle-4x4-noisy-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

51 ±10
0 ±0
34 ±4
9 ±3
8 ±4
20 ±3

49 ±9
0 ±0
61 ±14
23 ±10
14 ±9
29 ±7

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

19 ±5
16 ±5
17 ±6
14 ±5
12 ±4
16 ±4

puzzle-4x5-noisy-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

97 ±1
0 ±0
0 ±0
0 ±0
0 ±0
19 ±0

97 ±2
0 ±0
0 ±0
0 ±0
0 ±0
19 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

16 ±9
0 ±0
0 ±0
0 ±0
0 ±0
3 ±2

21 ±5
1 ±1
1 ±1
0 ±0
0 ±0
5 ±1

puzzle-4x6-noisy-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

80 ±8
3 ±2
0 ±0
0 ±0
0 ±0
17 ±2

86 ±7
1 ±1
0 ±0
0 ±0
0 ±0
18 ±2

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

28 ±13
1 ±1
0 ±0
0 ±0
0 ±0
6 ±3

8 ±4
1 ±1
0 ±0
0 ±0
0 ±0
2 ±1

play

puzzle

noisy

46

Published as a conference paper at ICLR 2025

Table 21: Full results on Visual Cube.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

visual-cube-single-play-v0

task1
task2
task3
task4
task5
overall

12 ±4
4 ±4
6 ±5
0 ±0
0 ±0
5 ±1

70 ±3
65 ±9
49 ±2
60 ±13
55 ±6
60 ±5

42 ±12
44 ±11
24 ±9
21 ±6
20 ±7
30 ±5

68 ±9
35 ±34
41 ±27
32 ±10
30 ±8
41 ±15

47 ±20
40 ±20
33 ±17
18 ±16
16 ±10
31 ±15

93 ±1
93 ±3
84 ±5
84 ±5
88 ±2
89 ±0

visual-cube-double-play-v0

task1
task2
task3
task4
task5
overall

4 ±3
0 ±0
0 ±1
0 ±0
0 ±0
1 ±1

44 ±8
0 ±1
0 ±0
0 ±0
4 ±2
10 ±2

6 ±4
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

20 ±3
2 ±2
2 ±0
0 ±0
0 ±0
5 ±0

7 ±4
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

91 ±1
54 ±5
40 ±6
0 ±0
11 ±4
39 ±2

visual-cube-triple-play-v0

task1
task2
task3
task4
task5
overall

73 ±8
0 ±0
0 ±0
0 ±0
0 ±0
15 ±2

68 ±8
0 ±0
0 ±0
0 ±0
0 ±0
14 ±2

76 ±6
0 ±0
0 ±0
0 ±0
0 ±0
15 ±1

81 ±3
0 ±0
0 ±0
0 ±0
0 ±0
16 ±1

85 ±12
0 ±0
0 ±0
0 ±0
0 ±0
17 ±2

98 ±1
1 ±1
7 ±3
0 ±0
0 ±0
21 ±0

visual-cube-quadruple-play-v0

task1
task2
task3
task4
task5
overall

42 ±4
0 ±0
0 ±0
0 ±0
0 ±0
8 ±1

1 ±2
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

36 ±7
0 ±0
0 ±0
0 ±0
0 ±0
7 ±1

23 ±5
0 ±0
0 ±0
0 ±0
0 ±0
5 ±1

20 ±4
0 ±0
0 ±0
0 ±0
0 ±0
4 ±1

66 ±7
0 ±0
1 ±1
0 ±0
1 ±1
14 ±1

visual-cube-single-noisy-v0

task1
task2
task3
task4
task5
overall

12 ±4
17 ±6
6 ±2
14 ±2
20 ±10
14 ±3

89 ±4
48 ±14
77 ±4
82 ±2
77 ±2
75 ±3

81 ±6
0 ±0
90 ±3
35 ±11
33 ±7
48 ±3

18 ±29
0 ±0
28 ±27
2 ±1
2 ±1
10 ±5

43 ±41
28 ±13
42 ±41
43 ±35
37 ±27
39 ±30

100 ±1
100 ±1
99 ±1
100 ±1
99 ±1
99 ±0

visual-cube-double-noisy-v0

task1
task2
task3
task4
task5
overall

20 ±5
2 ±2
2 ±1
1 ±1
0 ±0
5 ±1

70 ±8
5 ±3
5 ±5
3 ±1
0 ±0
17 ±4

70 ±5
14 ±2
16 ±6
0 ±0
7 ±2
22 ±2

27 ±9
2 ±4
0 ±0
0 ±0
1 ±1
6 ±2

24 ±11
3 ±3
2 ±2
0 ±0
0 ±0
6 ±3

98 ±2
87 ±9
68 ±10
13 ±9
30 ±4
59 ±3

visual-cube-triple-noisy-v0

task1
task2
task3
task4
task5
overall

80 ±7
0 ±1
0 ±1
0 ±0
0 ±1
16 ±1

90 ±5
0 ±0
0 ±0
0 ±0
0 ±0
18 ±1

62 ±6
0 ±0
0 ±0
0 ±0
0 ±0
12 ±1

44 ±21
0 ±0
0 ±0
0 ±0
0 ±0
9 ±4

78 ±7
0 ±0
0 ±0
0 ±0
0 ±0
16 ±1

99 ±1
2 ±2
14 ±11
0 ±0
0 ±0
23 ±2

visual-cube-quadruple-noisy-v0

task1
task2
task3
task4
task5
overall

46 ±2
0 ±0
0 ±0
0 ±0
0 ±0
9 ±0

2 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

10 ±9
0 ±0
0 ±0
0 ±0
0 ±0
2 ±2

2 ±2
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

39 ±10
0 ±0
0 ±0
0 ±0
0 ±0
8 ±2

60 ±41
0 ±0
2 ±2
0 ±0
0 ±0
12 ±8

play

visual-cube

noisy

Table 22: Full results on Visual Scene.
Environment Type

Dataset Type

play

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

visual-scene-play-v0

task1
task2
task3
task4
task5
overall

59 ±7
0 ±0
0 ±0
2 ±1
0 ±0
12 ±2

84 ±4
24 ±8
16 ±8
0 ±0
0 ±0
25 ±3

56 ±4
1 ±1
0 ±0
3 ±4
0 ±0
12 ±2

44 ±6
2 ±2
0 ±0
2 ±1
0 ±0
10 ±1

52 ±6
1 ±1
0 ±0
1 ±1
0 ±0
11 ±2

80 ±6
81 ±7
61 ±11
20 ±8
3 ±2
49 ±4

visual-scene-noisy-v0

task1
task2
task3
task4
task5
overall

64 ±9
0 ±0
0 ±0
0 ±1
0 ±0
13 ±2

76 ±6
14 ±7
24 ±5
0 ±0
0 ±0
23 ±2

49 ±22
2 ±2
7 ±2
0 ±0
0 ±0
12 ±4

8 ±2
0 ±0
0 ±0
0 ±0
0 ±0
2 ±0

70 ±9
0 ±0
2 ±2
1 ±1
0 ±0
15 ±2

91 ±4
69 ±5
82 ±6
8 ±5
0 ±0
50 ±1

visual-scene

noisy

47

Published as a conference paper at ICLR 2025

Table 23: Full results on Visual Puzzle.
Environment Type

Dataset Type

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

visual-puzzle-3x3-play-v0

task1
task2
task3
task4
task5
overall

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

97 ±2
1 ±1
0 ±1
2 ±3
3 ±2
21 ±1

5 ±9
0 ±0
0 ±0
0 ±0
0 ±0
1 ±2

3 ±3
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

2 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

98 ±1
73 ±10
64 ±9
63 ±9
66 ±12
73 ±8

visual-puzzle-4x4-play-v0

task1
task2
task3
task4
task5
overall

11 ±3
19 ±5
8 ±3
6 ±2
6 ±2
10 ±1

86 ±7
8 ±5
80 ±4
65 ±9
61 ±8
60 ±5

18 ±5
28 ±12
15 ±4
10 ±5
9 ±3
16 ±4

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

10 ±7
19 ±13
8 ±7
7 ±5
4 ±4
10 ±6

70 ±47
38 ±32
66 ±44
66 ±44
60 ±41
60 ±41

visual-puzzle-4x5-play-v0

task1
task2
task3
task4
task5
overall

22 ±8
1 ±1
0 ±0
0 ±0
0 ±0
5 ±2

86 ±4
0 ±0
0 ±0
0 ±0
0 ±0
17 ±1

31 ±8
1 ±1
0 ±0
0 ±0
0 ±0
7 ±2

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

31 ±6
0 ±1
0 ±0
0 ±0
0 ±0
6 ±1

66 ±44
0 ±0
0 ±0
0 ±0
0 ±0
13 ±9

visual-puzzle-4x6-play-v0

task1
task2
task3
task4
task5
overall

12 ±4
0 ±1
0 ±0
0 ±0
0 ±0
2 ±1

66 ±3
8 ±2
0 ±0
0 ±0
0 ±0
15 ±1

10 ±3
2 ±2
0 ±0
0 ±0
0 ±0
2 ±1

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

12 ±5
2 ±1
0 ±0
0 ±0
0 ±0
3 ±1

34 ±23
8 ±6
0 ±0
0 ±0
0 ±0
9 ±6

visual-puzzle-3x3-noisy-v0

task1
task2
task3
task4
task5
overall

4 ±6
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

100 ±1
0 ±1
0 ±0
0 ±0
0 ±0
20 ±0

98 ±2
5 ±7
2 ±2
5 ±4
19 ±10
26 ±4

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

7 ±7
0 ±0
0 ±0
0 ±0
0 ±0
1 ±1

100 ±0
64 ±11
55 ±3
61 ±8
71 ±13
70 ±6

visual-puzzle-4x4-noisy-v0

task1
task2
task3
task4
task5
overall

6 ±2
16 ±7
4 ±3
2 ±1
4 ±2
7 ±3

90 ±9
1 ±1
88 ±4
36 ±10
20 ±9
47 ±3

85 ±3
1 ±1
77 ±6
47 ±17
34 ±13
49 ±7

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

4 ±3
14 ±1
4 ±4
2 ±1
5 ±4
6 ±2

98 ±2
44 ±20
95 ±2
91 ±2
94 ±1
84 ±4

visual-puzzle-4x5-noisy-v0

task1
task2
task3
task4
task5
overall

30 ±6
0 ±0
0 ±0
0 ±0
0 ±0
6 ±1

72 ±48
0 ±0
0 ±0
0 ±0
0 ±0
14 ±10

96 ±2
0 ±0
0 ±0
0 ±0
0 ±0
19 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

33 ±5
0 ±0
0 ±0
0 ±0
0 ±0
7 ±1

72 ±48
0 ±0
0 ±0
0 ±0
0 ±0
14 ±10

visual-puzzle-4x6-noisy-v0

task1
task2
task3
task4
task5
overall

10 ±2
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

61 ±41
1 ±1
0 ±0
0 ±0
0 ±0
12 ±8

82 ±4
2 ±1
0 ±0
0 ±0
0 ±0
17 ±1

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

9 ±7
0 ±0
0 ±0
0 ±0
0 ±0
2 ±1

56 ±7
12 ±10
0 ±0
0 ±0
0 ±0
14 ±2

play

visual-puzzle

noisy

Table 24: Full results on Powderworld.
Environment Type

powderworld

Dataset Type

play

Dataset

Task

GCBC

GCIVL

GCIQL

QRL

CRL

HIQL

powderworld-easy-play-v0

task1
task2
task3
task4
task5
overall

1 ±1
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

99 ±1
96 ±4
100 ±0
100 ±0
100 ±1
99 ±1

95 ±4
93 ±3
94 ±8
92 ±8
93 ±5
93 ±5

25 ±8
12 ±4
8 ±7
8 ±5
7 ±3
12 ±2

40 ±6
43 ±10
15 ±5
10 ±3
2 ±3
22 ±5

62 ±5
54 ±10
41 ±26
8 ±6
2 ±2
33 ±9

powderworld-medium-play-v0

task1
task2
task3
task4
task5
overall

0 ±0
3 ±3
2 ±3
0 ±0
0 ±0
1 ±1

81 ±12
28 ±14
99 ±2
39 ±17
2 ±2
50 ±4

7 ±14
0 ±0
72 ±14
0 ±1
0 ±0
16 ±5

2 ±3
9 ±6
4 ±5
0 ±0
0 ±0
3 ±1

0 ±1
2 ±3
2 ±2
0 ±0
0 ±0
1 ±1

26 ±20
16 ±12
60 ±37
5 ±4
1 ±2
22 ±14

powderworld-hard-play-v0

task1
task2
task3
task4
task5
overall

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
4 ±3
4 ±4
12 ±14
0 ±0
4 ±3

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
0 ±0
0 ±0
0 ±0
0 ±0
0 ±0

0 ±0
1 ±2
2 ±2
0 ±0
0 ±1
1 ±1

48

