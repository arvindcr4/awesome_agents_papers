ProSh: Probabilistic Shielding
for Model-free Reinforcement Learning
Edwin Hamel-De le Courtâˆ—

Imperial College
London, United Kingdom
e.hamel-de-le-court@imperial.ac.uk

Gaspard Ohlmannâˆ—

Mulhouse, France
gaspard.ohlmann@outlook.com

arXiv:2510.15720v2 [cs.LG] 21 Oct 2025

ABSTRACT
Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also
safe to deploy by providing formal guarantees about their safety.
To this end, we introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free algorithm for safe reinforcement
learning under cost constraints. ProSh augments the Constrained
MDP state space with a risk budget and enforces safety by applying a shield to the agentâ€™s policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in
expectation. We also show that optimality is preserved when the
environment is deterministic. Since ProSh is model-free, safety
during training depends on the knowledge we have acquired about
the environment. We provide a tight upper-bound on the cost in
expectation, depending only on the backup-critic accuracy, that is
always satisfied during training. Under mild, practically achievable
assumptions, ProSh guarantees safety even at training time, as
shown in the experiments.

KEYWORDS
Safe Reinforcement Learning, Formal Methods, Shielding, Probabilistic Temporal Logic, Stochastic Systems
âˆ— These authors contributed equally to this work.

1

INTRODUCTION

A key challenge in AI is designing agents that learn to act optimally
in unknown environments [37]. Reinforcement Learning (RL) â€“
particularly when combined with deep neural networks â€“ has made
impressive strides in domains such as games [27], robotics [23], and
autonomous driving [22]. However, ensuring safety during both
training and deployment remains a critical barrier to real-world
adoption. This has led to a growing interest in Safe RL, where agents
must optimize rewards under constraints, e.g., budgeted costs or
formal safety requirements. This paper focuses on safe learning in
Constrained Markov Decision Processes (CMDPs), where agents
maximize expected discounted reward, while keeping cumulative
expected cost below a given threshold. Traditional methods, such
as Lagrangian approaches [2, 30, 36], allow to converge toward a
safe policy, but do not provide formal guarantees on safety, neither
during training nor on the provided policy.
To tackle this problem, we introduce ProSh: a novel probabilistic shielding method that ensures safety also at training time, in
model-free RL. Unlike classic shielding methods that restrict unsafe
actions based on a model of the environment, ProSh operates on
distributions, augments the CMDP with a risk budget, and uses a
learned cost critic to guide safe exploration. Crucially, ProSh does

Francesco Belardinelli

Imperial College
London, United Kingdom
francesco.belardinelli@imperial.ac.uk

not assume access to a simulator or environment abstraction, and
is compatible with continuous environments.
We also show that optimizing over shielded policies in the augmented space is sufficient for constrained optimality in the base
CMDP in the deterministic setting. We implement ProSh as an
off-policy deep RL algorithm and evaluate it on standard Safe RL
benchmarks. Notably, ProSh turns out to be safe during training
in the experiments. To summarize, the contributions of the paper
are as follows.
(1) We introduce ProSh, a shielding method for CMDP that
assumes no knowledge of the environmentâ€™s dynamics.
(2) We provide formal safety guaranties during training, depending only on the accuracy of the learned cost critic.
(3) We show that optimizing across shielded policies in the RiskAugmented MDP suffices for constrained optimality (in the
deterministic case).
(4) We implement ProSh as a DRL algorithm and evaluate its
optimality and safety on relevant benchmarks. In particular,
ProSh leads to significantly less cost violations in expectation, even in early training.
Due to space restrictions, the proofs of all the results appear in
the supplementary material, which also includes the code for the
experiments.
Related Work. We refer to [16] for a survey on Safe Reinforcement Learning, while here we focus on the works most closely
related to our contribution. Policy-based methods are arguably the
most popular approaches to Safe RL. They usually consist of extending known RL algorithms (e.g., PPO [33], TRPO [32], or SAC
[17]) with safety constraints, either by using a Lagrangian approach
[2, 36], changing the objective function [26], or by modifying the update process [46]. Several of these algorithms have become widely
used for benchmarking against newer Safe RL methods, and are
implemented in state-of-the-art Safe RL frameworks [2, 20, 21].
Shielding restricts the agentâ€™s actions during training and deployment to ensure safety [4, 12, 19, 44]. Introduced in [4] using
LTL safety formulas, shielding has been extended to probabilistic
settings in [19], although without formal guaranties. Formal safety
under probabilistic constraints has been proved in [24]. More recent
work allows shielding without prior models: [35] uses a learned
safety critic to restrict actions, while [15] combines shielding with a
learned dynamics model. A survey on shielding methods is given in
[29]. Compared to prior approaches, we focus on the general case of
model-free, continuous environments with probabilistic constraints,
while prior approaches typically make further assumptions.
Lagrangian approaches [2, 30, 36] convert CMDPs into unconstrained problems by introducing a dual variable to penalize

cost violations in the reward objective. These methods are effective
in both discrete and continuous action spaces and are compatible
with policy gradient algorithms (e.g. TRPO [32], PPO [33]). Given
enough iterations and tuning, they provide good tradeoffs between
cost violations and reward, but cannot guaranty safety at any time,
which limits their use in safety-critical scenarios.
State augmentation techniques with parameters representing
how far the agent is from being unsafe have also been studied. In [9],
the MDP is state-augmented with Lagrange multipliers. In Saute-RL
[34], the states are augmented with the remaining safety budget,
which is used to reshape the objective. We also use an augmentation
approach, but we not only augment states but actions too, allowing
agents more freedom in the future probabilistic repartition of the
remaining safety budget.
Q-learning is a key model-free RL algorithm for discrete environments [42], known to converge in the tabular case under
mild assumptions. However, extending it to function approximation â€”especially with neural networksâ€”introduces challenges [28].
Stabilization techniques such as double Q-learning [40], dueling
networks [41], and distributional methods [7] help address instability. Classic Q-learning is prone to overestimation bias [39, 40]. To
mitigate these issues, we adopt a TD3-like method [13]. Throughout, we assume the backup ğ‘„-value approximates a fixed point of
the Bellman operatorâ€”an assumption we discuss in the paper.
Trust-region approaches such as CPO [2], FOCOPS [45], and
Safe Policy Iteration [31] are On-policy algorithms that rely on
providing local safety bounds that hold within a small neighborhood
of the current policy. This is useful to bound constraint violations of
the next policy updates if the trust region is small, but the accuracy
of the bound depends heavily on the size of the trust region. In
addition, these methods do not provide a formal guaranty that the
policy update step is safe, even late in the training.
Lyapunov-based methods rely on enforcing constraint satisfaction by projecting updates to satisfy a Lyapunov decrease condition defined relative to a fixed safe baseline policy [10, 11]. The
guarantees provided in [10, 11] depend on accurate safety critics,
local linearizations around the current policy, and a well-behaved
backup policy. We rely on restricting the agentâ€™s actions to enforce
safety, but we use a state-augmentation technique rather than a
fixed Lyapunov function, which yields explicit, model-free trainingtime bounds that depend only on the backup criticâ€™s approximation
error. Furthermore, our backup policy is learned jointly with the
main actor and used as a fallback within the shield, rather than as
a fixed reference that defines the feasible set of actions.

2.1

Key Concepts

A Markov Decision Processes (MDP) is a tuple M = âŸ¨ğ‘†, ğ´, ğ‘ƒ, ğ‘ ğ‘– , ğ‘Ÿ âŸ©,
where ğ‘† is a set of states, with ğ‘ ğ‘– as initial state1 ; ğ´ is a mapping
that associates every state ğ‘  âˆˆ ğ‘† to a nonempty finite set ğ´(ğ‘ )
of actions; for SA = {(ğ‘ , ğ‘), ğ‘ âˆˆ ğ´(ğ‘ )}, ğ‘ƒ : SA â†’ D(ğ‘†) is a
transition probability function that maps every state-action pair
(ğ‘ , ğ‘) âˆˆ SA to a probability distribution over ğ‘†; and ğ‘Ÿ : SA â†¦â†’ R
is the reward function. An MDP is finite if the sets of states and
actions are finite. Finally, a Constrained MDP (CMDP) is an MDP
additionally equipped with a cost function ğ‘ : SA â†’ R+ .
Paths. A finite (resp. infinite) path is a finite (resp. infinite) word
ğœ = ğ‘  0ğ‘ 0 . . . ğ‘ ğ‘›âˆ’1ğ‘ğ‘›âˆ’1ğ‘ ğ‘› (resp. ğœ = ğ‘  0ğ‘ 0 . . . ğ‘ ğ‘› ğ‘ğ‘› . . .), such that for
every ğ‘˜ â‰¤ ğ‘› (resp. for every ğ‘˜), ğ‘ ğ‘˜ âˆ’ is a state in ğ‘†, ğ‘ğ‘˜ âˆ’1 is an action
in ğ´(ğ‘ ğ‘˜ âˆ’1 ), and ğ‘ ğ‘˜ is in the support of ğ‘ƒ (ğ‘ ğ‘˜ âˆ’1, ğ‘ğ‘˜ âˆ’1 ).
Policies. A policy ğœ‹ is a mapping that associates every finite
path ğœ to an action of the probability distribution D(ğ´(last (ğœ ))). A
policy is memoryless if ğœ‹ (ğœ ) only depends on last (ğœ ); it is deterministic if for any finite path ğœ , ğœ‹ (ğœ ) is a Dirac distribution; it is flipping
if for any finite path ğœ , ğœ‹ (ğœ ) is a mixture of two Dirac distributions.
Ã
Throughout the paper, we let ğ‘âˆˆğ´(ğ‘  ) ğœ†ğ‘ ğ‘ denote the probability
distribution corresponding to sampling action ğ‘ with probability
ğœ†ğ‘ . As an example, if ğœ‹1 and ğœ‹2 are two deterministic policies on
M, the policy defined for any ğ‘  âˆˆ S as ğœ‹ (ğ‘ ) = 0.5ğœ‹1 (ğ‘ ) + 0.5ğœ‹ 2 (ğ‘ )
denotes the flipping policy taking in ğ‘  the actions ğœ‹ 1 (ğ‘ ) and ğœ‹ 2 (ğ‘ )
with equal probability.
For any policy ğœ‹ of M and state ğ‘  âˆˆ ğ‘†, let Mğœ‹ğ‘  be the Markov
chain induced by ğœ‹ in M starting from state ğ‘  (c.f. [6]). let Mğœ‹
denote Mğœ‹ğ‘ ğ‘– and ğ‘ƒğœ‹ the transition function of Mğœ‹ . We denote the
usual probability measure induced by the Markov chain Mğœ‹ğ‘  on
Paths (M) by probğ‘ M,ğœ‹ . We refer to [6, 8] for full details.
Discounted expectations. For any random variable
ğ‘‹ : Paths (M) â†¦â†’ R, let Eğ‘ M,ğœ‹ (ğ‘‹ ) be the expectation of ğ‘‹ w.r.t. the
probability distribution probğ‘ M,ğœ‹ , and let E M,ğœ‹ (ğ‘‹ ) denote
ğ‘–
(ğ‘‹ ). Given a path ğœ = ğ‘  0ğ‘ 0 . . . ğ‘ ğ‘› ğ‘ğ‘› . . . , we denote the disEğ‘ M,ğœ‹
Ã
counted cumulative reward along ğœ as R (ğœ ) = ğ‘¡ âˆˆN ğ›¾ğ‘Ÿğ‘¡ ğ‘Ÿ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ), and
Ã
its discounted cumulative cost as C(ğœ ) = ğ‘¡ âˆˆN ğ›¾ğ‘ğ‘¡ ğ‘ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ) , where
ğ›¾ğ‘Ÿ and ğ›¾ğ‘ are the discount factors for reward ğ‘Ÿ and cost ğ‘ respectively. For any policy ğœ‹ if a CMDP M, we denote the discounted
cumulative reward of ğœ‹ as
âˆ‘ï¸
Rğ‘ M (ğœ‹) = E Rğ‘  (ğœ ) =
ğ›¾ğ‘Ÿğ‘¡ ğ‘Ÿ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ),
E
ğœ âˆ¼ğœ‹

(ğ‘ ğ‘¡ ,ğ‘ğ‘¡ )ğ‘¡ âˆ¼ğœ‹,ğ‘  0 =ğ‘ 

ğ‘¡ âˆˆN

and the discounted cumulative cost of ğœ‹ as
âˆ‘ï¸
ğ‘ 
CM
(ğœ‹) = E Cğ‘  (ğœ ) =
ğ›¾ğ‘ğ‘¡ ğ‘ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ).
E
ğœ âˆ¼ğœ‹

(ğ‘ ğ‘¡ ,ğ‘ğ‘¡ )ğ‘¡ âˆ¼ğœ‹,ğ‘  0 =ğ‘ 

ğ‘¡ âˆˆN

ğ‘ ğ‘–
ğ‘–
We let R M (ğœ‹) = Rğ‘ M
(ğœ‹) and CM (ğœ‹) = CM
(ğœ‹), and omit M

2

BACKGROUND

In this section we provide the background on Reinforcement Learning (RL) and the notations that will be used in the rest of the paper.
This enables us to define the two key problems that we analyse: the
Reinforcement Learning Problem (RLP) [38] and the Constrained
RLP (CRLP) [1, 14].

when there is no ambiguity.
Q-values are functions ğ‘„ : S Ã— A â†’ R that estimates the
expected cost of taking action ğ‘ in state ğ‘ , and following some
policy thereafter. In our context, we are especially interested in
1 This can be assumed wlog compared to a model with an initial probability distribution
since it is always possible to add a new initial state to such a model with an action
from this initial state whose associated probability distribution is the aforementioned
initial probability distribution.

Q-values approximating the minimal discounted cost, defined as
ğ‘ 
ğ‘„ğ‘âˆ— (ğ‘ , ğ‘) := min CM
(ğœ‹).
ğœ‹ :ğœ‹ (ğ‘  )=ğ‘

We denote by ğ‘„ğ‘ any approximation of ğ‘„ğ‘âˆ— , and say it is an ğœ€approximation if âˆ¥ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— âˆ¥ âˆ â‰¤ ğœ€.

2.2

Reinforcement Learning Problems

This work is motivated by two central optimization problems in RL.
The first, classic Reinforcement Learning problem (RLP), focuses on
optimizing the expected reward [37]. The second, constrained RLP
(CRLP), introduces explicit safety requirements in the form of cost
constraints [1, 14]. Our method ProSh is designed specifically for
CRLP, and aims to provide provably safe probabilistic guarantees
even in the absence of a known model.
Definition 1 (RL Problems). Let M be an MDP, ğ›¾ğ‘Ÿ the reward
discount factor, and Î  the set of policies over M.
(ğ‘…ğ¿ğ‘ƒ) : Find policy ğœ‹ âˆ— âˆˆ Î  such that R (ğœ‹ âˆ— ) = max R (ğœ‹)
ğœ‹ âˆˆÎ 

Further, let M â€² be a CMDP, ğ›¾ğ‘Ÿ and ğ›¾ğ‘ the discount factors for rewards
and costs respectively, and ğ‘‘ > 0 be a cost threshold .
Denote Î  â‰¤ğ‘‘ the set of policies ğœ‹ such that C(ğœ‹) â‰¤ ğ‘‘.

3

PROSH: THEORETICAL FOUNDATIONS

We now present the theoretical backbone of ProSh, a scalable shielding mechanism for CRLP that provides formal safety guarantees.
Our approach uses risk-augmentation, similarly to [24], whereby
each state is paired with a risk budget representing the expected
discounted cost that the agent is allowed to incur from that point
onward. Unlike traditional shielding approaches, ProSh considers
probabilistic expectations that the main actor chooses, and enforces
these two structural properties:
â€¢ Approximate consistence. The combined risk after one
step is, in expectancy, approximately equal to the risk of the
current step.
â€¢ Approximate realizability. For every augmented state,
there exists a policy whose expected cumulative cost is approximately less than or equal to the current risk.
Our goal is to construct policies that satisfy the safety constraint
throughout training and deployment, even in the absence of a known
model. To this end, we augment the CMDP with a dynamically
updated notion of risk.

3.1

Risk-Augmented CMDPs

(CRLP) : Find policy ğœ‹ âˆˆ Î  â‰¤ğ‘‘ such that R (ğœ‹ ) = max R (ğœ‹)

Hereafter, for any ğ‘„-value ğ‘„ğ‘ of a CMDP M, for any state ğ‘  and
action ğ‘ âˆˆ ğ´(ğ‘ ), we let ğ‘„ğ‘ (ğ‘ ) denote ğ›¾ğ‘ minğ‘âˆˆğ´(ğ‘  ) (ğ‘„ğ‘ (ğ‘ , ğ‘)).

Notice that Definition 1 is not restricted to memoryless policies, as is usually the case [1, 2, 46]. This is because in the case of
two different discount factors for the reward and the cost, optimal
memoryless policies for CRLP are not guaranteed to exist [5]. Furthermore, in practical implementations, the cost discount factor is
often higher than the reward discount factor since constraint satisfaction is paramount in CRLP. The additional difficulty does not
break the optimality of the method we provide in the deterministic
case.

Definition 2 (Risk-augmented CMDP). For any CMDP M =
(ğ‘†, ğ´, ğ‘ ğ‘– , ğ‘ƒ, ğ‘…, ğ¶, ğ‘‘, ğ›¾ğ‘Ÿ , ğ›¾ğ‘ ) with non-negative costs, maximal cost ğ‘ max =

âˆ—

âˆ—

ğœ‹ âˆˆÎ  â‰¤ğ‘‘

Shielding and State-Augmentation challenges: a motivation example. We illustrate the limitations of traditional shielding techniques
through the simple CMDP M1 shown in Fig. 1, with ğ›¾ğ‘Ÿ = ğ›¾ğ‘ = 1
and cost threshold ğ‘‘ = 0.5. The goal is to solve the constrained RL
problem (CRLP) for M1 :
ğ‘0

ğ‘Ÿ =1
ğ‘ =1

ğ‘ 1

ğ‘2

ğ‘Ÿ =0
ğ‘ =0

ğ‘ 0

ğ‘ 4
ğ‘1

ğ‘Ÿ =0
ğ‘ =0

ğ‘ 2

ğ‘3

ğ‘Ÿ =0
ğ‘ =0

Figure 1: CMDP M1 with cost threshold ğ‘‘ = 0.5 and ğ›¾ğ‘Ÿ = ğ›¾ğ‘ = 1.
The optimal safe policy ğœ‹ âˆ— takes action ğ‘ 0 and ğ‘ 1 with equal
probability, yielding a cost C(ğœ‹ âˆ— ) = 0.5 and a reward R (ğœ‹ âˆ— ) = 0.5.
This solution cannot be found using hard safety enforcement such
as classic shielding [4], which would block ğ‘ 0 . Nor does a stateaugmentation based approach such as [34], as path ğ‘  0ğ‘ 0ğ‘  1ğ‘ 2ğ‘  4 has
a cumulative cost above the threshold. We claim that achieving safe
optimality requires reasoning over expectations.

ğ‘„ğ‘

||ğ‘ ||ğ¿âˆ ( A S) , and ğ‘„-value ğ‘„ğ‘ , we define Mğ‘‘ as the MDP with ğ¶ max =
ğ‘ğ‘šğ‘ğ‘¥
(1âˆ’ğ›¾ğ‘ ) , such that
â€¢ States : ğ‘† = {(ğ‘ , ğ‘¥) | ğ‘  âˆˆ ğ‘†, ğ‘¥ âˆˆ [âˆ’ğ¶ max ; ğ¶ max ]}, with initial
state ğ‘  ğ‘– = (ğ‘ ğ‘– , ğ‘‘);
â€¢ Actions : ğ´(ğ‘ , ğ‘¥) = {(ğ‘, ğ‘¦) | ğ‘ âˆˆ ğ´(ğ‘ ), ğ‘¦ âˆˆ [âˆ’ğ¶ max ; ğ¶ max ]};
â€¢ Rewards : ğ‘Ÿ ((ğ‘ , ğ‘¥), (ğ‘, ğ‘¦)) = ğ‘Ÿ (ğ‘ , ğ‘);
â€¢ Costs : ğ‘ ((ğ‘ , ğ‘¥), (ğ‘, ğ‘¦)) = ğ‘ (ğ‘ , ğ‘)
â€¢ Transition Probability Function ğ‘ƒ: for ğ‘  = (ğ‘ , ğ‘¥), ğ‘  â€² = (ğ‘  â€², ğ‘¥ â€² )
in ğ‘†, and ğ‘ = (ğ‘, ğ‘¦) âˆˆ ğ´(ğ‘ ), we have
(
0
if ğ‘¥ â€² â‰  ğ‘¦ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² )
â€²
ğ‘ƒ ğ‘ , ğ‘, ğ‘  =
ğ‘ƒ (ğ‘ , ğ‘, ğ‘  â€² ) if ğ‘¥ â€² = ğ‘¦ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² )
The risk-augmented CMDP introduces a new coordinate for
states and actions: the risk. The policy can then choose any risk for
its next actions, and the transition function ğ‘ƒ simply ensures that
the corresponding risk is spread coherently to each state in the nondeterministic case. We will introduce later the conditions on the
policy to ensure that the second parameter ğ‘¥ indeed plays the role
of a running budget, recording how much cumulative discounted
cost the agent is allowed to incur.
We now define a subset of policies on M, the valued policies.
Definition 3 (Valued policy). A policy ğœ‹Â¯ on the risk-augmented

CMDP M is valued if there exists a mapping ğ‘¦ğœ‹Â¯ : S â†’ ğ´(ğ‘ ) â†’ R
such that, for every augmented state (ğ‘ , ğ‘¥) âˆˆ S, we can write with
the notation (ğ‘, ğ‘¥)ğ›¿ = ğ›¿ğ‘,ğ‘¥ , the Dirac Delta distribution choosing the
action (ğ‘, ğ‘¥) with probability 1,
âˆ‘ï¸

ğœ‹Â¯ (ğ‘ , ğ‘¥) =
ğ‘ƒğœ‹Â¯ (ğ‘ | ğ‘ , ğ‘¥) ğ‘, ğ‘¦ğœ‹Â¯ (ğ‘ , ğ‘¥)(ğ‘) ğ›¿
ğ‘âˆˆğ´(ğ‘  )

In other words, to any underlying stateâ€“risk pair (ğ‘ , ğ‘¥), the policy first
assigns a risk value ğ‘Ÿ ğœ‹Â¯ (ğ‘ , ğ‘¥)(ğ‘) to every available action ğ‘ âˆˆ ğ´(ğ‘ ).
We call ğ‘¦ğœ‹Â¯ a valuation function, and we let Î  ğ‘£ğ‘ğ‘™ denote the set of
all valued policies.
Asking the agent to attach a risk value to every possible action is
not as restrictive as it looks. Indeed, we will show that considering
only the set of valued policies is sufficient for optimality.

3.2

Shielded Policies, Safety and Optimality

Among valued policies, we focus on those that keep the risk budget synchronized with the CMDP dynamics. We call those the
ğ‘„ğ‘ -shielded policies. In the rest of the paper, for any ğ‘„-value ğ‘„ğ‘ ,
we let ğœ‹ğ‘ (ğ‘„ğ‘ ) (ğ‘ ) = arg minğ‘âˆˆğ´(ğ‘  ) ğ‘„ğ‘ (ğ‘ , ğ‘), and when there is no
ambiguity, we write only ğœ‹ğ‘ for ğœ‹ğ‘ (ğ‘„ğ‘ ). This action is the safest
according to the estimation ğ‘„ğ‘ (ğ‘ , ğ‘) and will be used to decrease
the expected cost when necessary.
Definition 4 (ğ‘„ğ‘ -Shielded policies). Let M be a CMDP, and
let ğ‘„ğ‘ (M) be any ğ‘„-valued function defined on AS. Denote M the
corresponding augmented CMDP. A policy ğœ‹Â¯ âˆˆ Î  ğ‘£ğ‘ğ‘™ is said to be
ğ‘„ğ‘ -shielded if for any (ğ‘ , ğ‘¥) âˆˆ SÌ„, if ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ), there exist ğœ† and
{ğ‘¦ğ‘ }ğ‘âˆˆğ´(ğ‘  ) , with ğ‘¦ğ‘ â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘) for all ğ‘ âˆˆ ğ´(ğ‘ ), such that
âˆ‘ï¸
ï£±
ğœ‹Â¯ (ğ‘ , ğ‘¥) =(1 âˆ’ ğœ†)
ğ‘ƒğœ‹Â¯ ((ğ‘, ğ‘¦ğ‘ ) | (ğ‘ , ğ‘¥))(ğ‘, ğ‘¦ğ‘ )ğ›¿
ï£´
ï£´
ï£´
ï£´
ğ‘
ï£´
ï£²
ï£´
+ ğœ†(ğœ‹ğ‘ (ğ‘ ), ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )))ğ›¿ ,
ï£´
âˆ‘ï¸
ï£´
ï£´
ï£´
ğ‘ƒğœ‹ ((ğ‘, ğ‘¦ğ‘ ) | (ğ‘ , ğ‘¥))ğ‘¦ğ‘ + ğ›¾ğ‘ ğœ†ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )),
ï£´
ï£´ ğ‘¥ â‰¥ ğ›¾ğ‘ (1 âˆ’ ğœ†)
ï£³
ğ‘
and if ğ‘¥ < ğ‘„ğ‘ (ğ‘ ), there exists ğ‘§ â‰¤ ğ‘¥ such that
(
ğœ‹Â¯ (ğ‘ , ğ‘¥) = (ğœ‹ğ‘ (ğ‘ ), ğ‘§),
ğ‘¥ â‰¥ ğ›¾ğ‘ ğ‘§
Intuitively, the policies are shielded when they always allow
a budget for the actions larger than the estimated minimal cost
of taking this action. Moreover, the total budget allowed for the
actions is, in expectancy, smaller than the current budget.
We now define the corresponding Shield-Map, which takes any
memoryless valued policy ğœ‹Â¯ of M and outputs a ğ‘„ğ‘ -shielded policy.
Definition 5 (Shield-map). Let ğœ‹Â¯ be a memoryless valued policy
of the augmented MDP M. We define the shield-map Î such that, for
any state (ğ‘ , ğ‘¥) of M, if we let ğ‘¦Ëœğ‘ = max(ğ‘¦Ëœğ‘ , ğ‘„ğ‘ (ğ‘ , ğ‘)), and
âˆ‘ï¸
âˆ‘ï¸
ğœ‹Ëœ (ğ‘ , ğ‘¥) =
ğ‘ƒğœ‹Â¯ (ğ‘ | ğ‘ , ğ‘¥)(ğ‘, ğ‘¦Ëœğ‘ )ğ›¿ , ğ‘¡ = ğ›¾ğ‘
ğ‘ƒğœ‹Â¯ (ğ‘ | ğ‘ , ğ‘¥)ğ‘¦Ëœğ‘ ,
ğ‘âˆˆğ´(ğ‘  )

ğ‘âˆˆğ´(ğ‘  )

we have
ï£±
ï£´
ğœ‹Ëœ (ğ‘ , ğ‘¥)
ï£´
ï£²
ï£´
Â¯ (ğ‘ , ğ‘¥) = (ğœ‹ğ‘ (ğ‘ ), ğ‘¥/ğ›¾ğ‘ )ğ›¿
Î(ğœ‹)
ï£´
ï£´
ï£´ (1 âˆ’ ğœ†) ğœ‹Ëœ (ğ‘ , ğ‘¥)ğ›¿ + ğœ†(ğœ‹ğ‘ (ğ‘ ), ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )))ğ›¿
ï£³
where
ğ›¾ğ‘ ğ‘¡ âˆ’ ğ‘¥
ğœ†=
.
ğ›¾ğ‘ ğ‘¡ âˆ’ ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ ))

if ğ‘¡ â‰¤ ğ‘¥
if ğ‘¥ < ğ‘„ğ‘ (ğ‘ )
otherwise,

Intuitively, the shield map blends the original policy with the estimated safest action ğœ‹ğ‘ so that the resulting policy always satisfies
the ğ‘„ğ‘ -shielded condition. When the policy already respects this
safety constraint, no modification is applied. If estimation errors

lead to a state where any distribution would exceed the available
budget, the shield falls back to the safest estimated action ğœ‹ğ‘ and
allocates the entire budget to it.
Because the constraint in the definition mirrors the Bellman
equation for discounted costs, the risk ğ‘¥ is indeed an approximate
upper bound of the expected future cost of the shielded policy that
starts from the augmented state (ğ‘ , ğ‘¥). The next theorem makes
this intuition precise.
Theorem 1 (Safety Bounds for ğ‘„ğ‘ -shields). Let M be a CMDP
with cost discount factor ğ›¾ğ‘ , and let ğ‘„ğ‘âˆ— be its optimal stateâ€“action
cost function. Assume that ğ‘„ğ‘ is a ğ‘„-value, and let Î”ğ‘ = ||ğ‘„ğ‘ âˆ’
ğ‘„ğ‘âˆ— ||ğ¿âˆ ( S A ) .
For any policy ğœ‹Â¯ âˆˆ Î  val that is ğ‘„ğ‘ -shielded, the expected discounted
cost from the augmented state (ğ‘  0, ğ‘¥ 0 ) satisfies
Â¯ â‰¤ ğ‘¥0 +
C (ğ‘ 0 ,ğ‘¥ 0 ) ( ğœ‹)

2Î”ğ‘
,
1 âˆ’ ğ›¾ğ‘

whenever ğ‘¥ 0 â‰¥

ğ‘„ğ‘ (ğ‘  0 )
.
ğ›¾ğ‘

Theorem 1 shows that an ğœ€-accurate ğ‘„ğ‘ is enough to keep any
ğ‘„ğ‘ -shielded policy inside the budget, up to a small additive slack.
Moreover, the policy we consider in the augmented CMDP corresponds to policies in the original CMDP that have the same cost
and reward, as stated in the next resilts.
Definition 6 (Projection onto the base CMDP). For any
ğ‘„ğ‘ -shielded policy ğœ‹Â¯ âˆˆ Î val of M, we define the backward projection
Â¯ as the memoryful policy tracking a single scalar variable ğ‘š as
ğ‘‡ (ğœ‹)
â€¢ In state ğ‘  and with current memory ğ‘š âˆ’ ğ‘¥, sample (ğ‘, ğ‘¥ â€² ) âˆ¼
ğœ‹Â¯ (ğ‘ , ğ‘¥) and execute ğ‘ğ‘¡ in M.
â€¢ As state ğ‘  â€² is reached in M, update the memory ğ‘š â† ğ‘¥ â€² âˆ’
ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ).
We can now prove the following preservation result.
Â¯ in
Theorem 2 (Preservation). The cost and reward or ğ‘‡ (ğœ‹)
Def. 6 satisfy
Â¯
CM (ğ‘‡ (ğœ‹)) = CMÌ„ ( ğœ‹),

Â¯
R M (ğ‘‡ (ğœ‹)) = R MÌ„ (ğœ‹).

Theorem 2 states that a ğ‘„ğ‘ -shielded policy keeps its cost and
reward when mapped back to the original CMDP. This allows us
to transfer our safety results from M to M. It remains to show
that some ğ‘„ğ‘ -shielded policy attains the constrained optimum. The
next theorem precisely answers this question. Recall that flipping
policies associate to every state a mixture of two Dirac distributions.
Theorem 3 (Optimality of the shielded policies). Let M be
a deterministic CMDP with safety threshold ğ‘‘ and initial state ğ‘ ğ‘– , ğ‘„ğ‘
be a Q-value, Î”ğ‘ = ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— ||ğ¿âˆ ( S A ) .
Further, let Î  the set of all policies of M, Î ğ‘ â„ be the set of shielded
ğ‘“

policies of M, and Î  be the set of valued flipping policies.
2Î”ğ‘ ğ›¾ğ‘
The, we have the following for E = 1âˆ’ğ›¾
:
ğ‘
max

R (ğœ‹) â‰¤

max

Â¯ â‰¤
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) ( ğœ‹)

ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘

ğ‘“

ğœ‹Â¯ âˆˆÎ(Î  ), ğ‘¥ 0 â‰¤ğ›¾ğ‘ ğ‘‘+E

ğ‘“

Â¯
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) (ğœ‹),

max

ğœ‹Â¯ âˆˆÎ(Î  ), ğ‘¥ 0 â‰¤ğ›¾ğ‘ ğ‘‘+E

max

ğœ‹ âˆˆÎ ğ‘ â„ , ğ‘¥ 0 â‰¤ğ›¾ğ‘ ğ‘‘+E

Â¯
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) (ğœ‹),

We now summarize the results obtained in this section, which
represent the theoretical backbone of ProSh.

From constrained to unconstrained optimisation. Let Î  ğ‘£ğ‘ğ‘™ the set
of valued policies (potentially unsafe) of the augmented MDP, and
Î Î = Î(Î  ğ‘£ğ‘ğ‘™ ), then for Î”ğ‘ = ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— ||ğ¿âˆ ( S A ) ,
â€¢ The set of shielded policies Î Î , starting with a risk up to
ğ‘¥ 0 = ğ‘‘ + 2Î”ğ‘ is enough to reach optimality, i.e.,
max

ğœ‹Â¯ âˆˆ Î Ì„, ğ‘¥ 0 â‰¤ğ‘‘+2Î”ğ‘

Â¯ â‰¥ max R (ğœ‹).
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) (Î( ğœ‹))
ğœ‹ âˆˆÎ 

Â¯ âˆˆ Î Î starting with a risk up to
â€¢ Any shielded policy Î(ğœ‹)
ğ‘¥ 0 = ğ‘‘ + 2Î”ğ‘ has a cost satisfying
Î”ğ‘
Â¯ â‰¤ ğ‘‘ + 2Î”ğ‘ +
C (ğ‘ ğ‘– ,ğ‘¥ 0 ) (Î(ğœ‹))
.
1 âˆ’ ğ›¾ğ‘
Hence, we can optimize among all policies of the augmented
MDP and apply the shield. Alternatively, one can choose a dynamic
ğ‘¥ 0 < ğ‘‘ to ensure safety during training. This can be combined with
any approach ensuring Î”ğ‘ â†’ 0, or making it sufficiently small.

4

Theorem 4 (Safety and near-Optimality).
(i) At any step
of the algorithm, the output policy ğœ‹Â¯ satisfies
Â¯ â‰¤ ğ‘Ÿ0 +
CMÌ„ ( ğœ‹)

Â¯ â‰¥ max R (ğœ‹) âˆ’ ğœ‚
R (ğœ‹)
ğœ‹ âˆˆÎ 

Since our guarantees involve the difference Î”ğ‘ and the assumption Î”ğ‘ â†’ 0, we discuss how restrictive this assumption is.
Remark 1 (On assumption ğš«ğ’ƒ â†’ 0). We present several cases in
which Î”ğ‘ â†’ 0.
â€¢ Tabular update: With exact Q-learning and sufficient exploa.s.
ration, the classical result of [42] gives Î”ğ‘ âˆ’âˆ’â†’ 0.
â€¢ Neural networks: Recent results [3, 43] show that over-parameterised
deep Q-learning converges to the Bellman fixed point when the
optimiser drives training error to zero and the network remains
within a neighbourhood of its initialization.
â€¢ Batch (fitted) Q-evaluation: In the agnostic setting, fitted Qiteration with minimax regression loss enjoys finite-sample
âˆš
O (1/ ğ‘›) guarantees [25], so Î”ğ‘ vanishes as the data set grows.

In this section, we show how the theoretical foundations in Section
3 are used to derive learning algorithms for CRLP. We first present
a general approach and derive the corresponding safety guarantees.
We then propose our implementation of the ProSh algorithm.

Learning Algorithms

In this section we present a minimal version of the training loop
for the main actor in Algorithm 1. Then, in Sec. 4.2 we provide a
complete implementation. ProSh behaves like "classical" shielding
[4], but the shield acts on distribution rather than actions. The
shield Î uses the values provided by the critic ğ‘„ğ‘ and combines it
with the backup action to obtain a safe distribution.
Algorithm 1 ProSh: Main Actor only
1: Input: cost budget ğ‘‘, margin ğ›¿, discount ğ›¾ğ‘
2: Initialise main actor ğœ‹Â¯ğ‘Ÿ , backup critic ğ‘„ğ‘ğœƒ
ğœ“

3: for each episode do
4:

ğ‘  â† ğ‘ ğ‘– , ğ‘¥ â† ğ‘‘ âˆ’ ğ›¿
while not done do
ğœ“
6:
Shield: ğœ‡ safe â† Îğ‘„ğ‘ ( ğœ‹Â¯ğ‘Ÿ )(ğ‘ , ğ‘¥)
7:
Sample (ğ‘, ğ‘¦) âˆ¼ ğœ‡safe
8:
Execute (ğ‘, ğ‘¦), observe (ğ‘  â€², ğ‘¥ â€² ), reward ğ‘Ÿ , cost ğ‘
9:
Store transition ((ğ‘ , ğ‘¥), (ğ‘, ğ‘¦), ğ‘Ÿ, ğ‘, (ğ‘  â€², ğ‘Ÿ â€² ))
10:
ğ‘  â† ğ‘ â€², ğ‘Ÿ â† ğ‘Ÿ â€²
11:
end while
12:
Update:
â€¢ Update ğœ‹ğœƒ using shielded distributions from memory
13: end for
ğœ“
14: return projected policy ğœ‹Ëœğ‘Ÿ (via Thm. 2)
5:

The shield precedes sampling, so the exploration is budget-safe.
The choice of the RL update for ğœƒ is open (e.g., PG, PPO, A2C).
Additionally, the backup critic ğ‘„ğ‘ can be learned, either in parallel,
off-policy, or even pre-computed.
The previous results in Sec. 3 allows us to derive the following
key safety guarantees for Algorithm 1.

Î”ğ‘ = ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— || âˆ

(ii) The algorithm is asymptotically optimal in deterministic environments as Î”ğ‘ â†’ 0. More precisely, let M be a constrained
deterministic environment modeled by a CMDP, and for any
ğœ‚ > 0 and Î”ğ‘ small enough, there exists ğœ‹Â¯ âˆˆ Îğ‘ â„ with cost at
2Î”ğ‘ ğ›¾ğ‘
most ğ›¾ğ‘ ğ‘‘ + E for E = 1âˆ’ğ›¾
, such that
ğ‘

SAFE RL THROUGH PROBABILISTIC
SHIELDING

4.1

2ğ›¾ğ‘ Î”ğ‘
,
1 âˆ’ ğ›¾ğ‘

Hence, in all cases above, the safety slack ğœ… (Î”ğ‘ ) = ğ›¾ğ‘ Î”ğ‘ /(1 âˆ’ ğ›¾ğ‘ )
converges to zero, ProSh is safe up to a controlled vanishing coefficient,
and approaches true constrained optimality.

4.2

ProSh-TD3

In this section we introduce ProSh-TD3, an implementation of
ProSH based on TD3 [13], for continuous spaces. Hereafter we
describe the four key components of ProSh-TD3: the network
architecture, the implementation of the shield, the actor/critic pairs
training, and the exploration strategy.
Networks Architecture. The backup actor-critic pair follows
the TD3 architecture. The backup critic has two output heads, ğ‘„ğ‘ğœƒ 1
and ğ‘„ğ‘ğœƒ 2 , and takes as input a state-actor pair of the original environment M. The backup actor ğœ‹ğ‘ is deterministic, and takes as
input a state in M and outputs an action. The main actor-critic
pair also follows the TD3 architecture. The main critic also has two
ğœ™

ğœ‰1

ğœ‰2

heads, ğ‘„ ğ‘Ÿ and ğ‘„ ğ‘Ÿ , but takes as input a state-action pair of the
ğœ“
augmented environment M. The main actor ğœ‹ ğ‘Ÿ takes as input a
state in the augmented environment and outputs a flipping policy in
the augmented environment. Furthermore, we also maintain target
ğ‘¡ğ‘ğ‘Ÿğ‘”

ğœƒ

ğ‘¡ğ‘ğ‘Ÿğ‘”

ğœƒ

networks ğ‘„ğ‘ 1 and ğ‘„ğ‘ 2 , etc, which are Polyak averages of the
original networks, that we use for computing training targets, in
order to stabilize training.
Shield Implementation. We implement the shield using the
Shield-map in Def. 5 as a fully differentiable layer after the output of
ğœ“
the main actor. More precisely, if ğ‘  = (ğ‘ , ğ‘¥) and ğœ‹ ğ‘Ÿ (ğ‘ ) = ğœŒ (ğ‘ 1, ğ‘¦1â€² ) +
â€²
(1 âˆ’ ğœŒ)(ğ‘ 2, ğ‘¦2 ), we implement Î as
Î(ğœ‹ ğœ“ )(ğ‘ ) = (1 âˆ’ ğœ†)ğœŒ (ğ‘ 1, ğ‘¦1â€² ) + (1 âˆ’ ğœ†)(1 âˆ’ ğœŒ)(ğ‘ 2, ğ‘¦2â€² ) + ğœ†ğ‘ 3

where ğ‘¦1â€² = min(ğ‘¦1, ğ‘„ğ‘ğœƒ 1 (ğ‘ , ğ‘ 1 )), ğ‘¦2â€² = min(ğ‘¦2, ğ‘„ğ‘ğœƒ 1 (ğ‘ , ğ‘ 2 )), ğ‘ 3 =


ğœ“
ğœ™
ğœ‹ ğ‘Ÿ (ğ‘ ), ğ‘„ğ‘ğœƒ 1 (ğ‘ , ğœ‹ğ‘ (ğ‘ ) , and
ğœŒğ‘¦1â€² + (1 âˆ’ ğœŒ)ğ‘¦2â€² âˆ’ ğ‘¥

Â©
ğœ† = clip Â­Â­





Âª
, 0, 1Â®Â®

+ğœ‚
relu
Â¬
Â«
where ğœ‚ is chosen sufficiently small (âˆ¼ 10âˆ’ 8).
Actor-critic Pairs Training. We train the actor-critic pairs
with batch sampling from a replay buffer in which we store every
transition made in the augmented CMDP. For the backup critic, we
use the training target ğ‘¦ğ‘ (ğ‘, ğ‘  â€² ), proposed in [18], equal to
"
ğ‘¡ğ‘ğ‘Ÿğ‘” 

ğœƒ
ğœ“ ğ‘¡ğ‘ğ‘Ÿğ‘” â€²
ğ‘ + ğ›¾ğ‘ ğ›½ min ğ‘„ğ‘ ğ‘—
ğ‘  â€², ğœ‹ğ‘
(ğ‘  ) + ğœ–
ğœŒğ‘¦1â€² + (1 âˆ’ ğœŒ)ğ‘¦2â€² âˆ’ ğ‘„ğ‘ğœƒ 1 (ğ‘ , ğœ‹ğ‘ (ğ‘ ))

ğ‘— âˆˆ {1,2}

+



1 âˆ’ ğ›½ âˆ‘ï¸ ğœƒ ğ‘¡ğ‘ğ‘Ÿğ‘”
ğœ“ ğ‘¡ğ‘ğ‘Ÿğ‘” â€²
ğ‘  â€², ğœ‹ğ‘
ğ‘„ğ‘ ğ‘—
(ğ‘  ) + ğœ–
2

#

ğ‘— âˆˆ {1,2}

where ğœ– is a small gaussian noise and ğ›½ is a hyperparameter. Compared to the standard TD3 training target, this target helps combat
the underestimation bias of TD3, improving the backup critic accuracy. The backup actor and the main critic are updated with the
standard TD3 training losses. The main actor is updated taking into
account the shield:
ï£¹
ï£® âˆ‘ï¸
ï£º
ï£¯
ğœ‰1
L (ğœ“ ) = âˆ’Eğ‘ âˆ¼D ï£¯ï£¯
ğœ†ğ‘˜ ğ‘„ ğ‘Ÿ (ğ‘ , ğ‘ğ‘˜ ) ï£ºï£º , where
ï£º
ï£¯ğ‘˜ âˆˆ {1,2,3}
ï£»
ï£°
âˆ‘ï¸
ğœ“
Î(ğœ‹ ğ‘Ÿ )(ğ‘ ) =
ğœ†ğ‘˜ ğ‘ğ‘˜ .

Algorithms (TD3-Lagrangian and PID-Lagrangian), and three being On-Policy Algorithms (CPO, FOCOPS, PPO-Saute). Some algorithms are safer, such as PPO-Saute, while others impose less
restrictions and seek higher rewards, such as TD3-Lagrangian. We
included them despite not always being directly comparable â€“ PPOSaute being in most cases the only algorithm that is safe during
training â€“ to present the overall performances of ProSh against
states of the art algorithms, both in terms of safety and rewards.
On each environment, the algorithms are run using three different
seeds, and the mean and standard deviation are computed. Finally,
the algorithms were implemented in the Omnisafe infrastructure
[21] and use the hyperparameters provided by Omnisafe for each
environment.

5.2

Experimental Results

Overall Performance. Across all environments, ProSh maintains
a high level of safety, with only occasional low-magnitude violations. PPO-Saute also achieves consistent safety, although this
often coincides with reduced performance and lower rewards. The
Off-Policy algorithms TD3-Lagrangian and PID-TD3 tend to exhibit
unsafe behavior in most settings, while the On-Policy methods
FOCOPS and CPO show less stable safety profiles, with costs that
fluctuate even after extended training. Taken together, the results
suggest that ProSh provides the best trade-off between safety and
performance in the evaluated environments, especially when ensuring safety both at training and deployment time is a necessity.
The Velocity Suite. In SafetyHopperVelocity and SafetyHalfCheetahVelocity, the agent has to move as quickly as possible, while
adhering to a velocity constraint.

ğ‘˜ âˆˆ {1,2,3}

2 https://github.com/DLR-RM/stable-baselines3

Reward - HalfCheetah

TD3Lag
TD3PID
ProSh

35
8000

25

Episode Reward

30

TD3Lag
TD3PID
ProSh
Max Cost

20
15
10

6000

4000

2000

5
0

0
0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

Figure 2: Comparison with Off-Policy Algorithms on Half
Cheetah. Cost threshold ğ‘‘ = 10.

Cost - HalfCheetah

40
35

Episode Cost

30
25

3000

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

2500
2000

Episode Reward

5 EXPERIMENTAL EVALUATION
5.1 Experimental setup
We implement ProSh-TD3 using the Stable-Baselines 3 implementation of TD32 . We evaluate our implementation of ProSh-TD3 on
six environments of the Safety Gymnasium benchmark suite: SafetyHalfCheetahVelocity, SafetyHopperVelocity, SafetyPointCircle,
SafetyPointGoal, SafetyCarCircle, and SafetyCarGoal are from the
Navigation Suite. More details on the environments are provided in
the comments on the experimental results. We benchmark against
diverse Constrained RL algorithms, two of them being Off-Policy

Cost - HalfCheetah

40

Episode Cost

Note that the gradients can flow through ğœ†ğ‘˜ and ğ‘ğ‘˜ in the main
actor training loss as they are computed in a fully differentiable
way.
Exploration. For a better estimation of the backup critic, we
alternate between main episodes and hybrid episodes. In the main
episodes, we sample from the (shielded) main actor. In the hybrid
episodes, we explore using the shielded main actor until a certain
step, after which we explore using the backup actor. All the actions
of the backup actor are converted to risk-augmented actions by
associating the risk of the current observed state. The corresponding
backup actor is always ğ‘„ğ‘ -shielded by construction, so the sampling
is safe during training.

20
15

Reward - HalfCheetah

CPO
FOCOPS
PPOSaute
ProSh

1500
1000
500

10
0
5
âˆ’500
0
0.0

0.2

0.4

0.6

Training Steps

0.8

1.0
1e6

0.0

0.2

0.4

0.6

Training Steps

0.8

1.0
1e6

Figure 3: Comparison with On-Policy Algorithms on Half
Cheetah. Cost threshold ğ‘‘ = 10.

Cost - PointCircle

40

50

Episode Reward

30

Reward - PointCircle

60

TD3Lag
TD3PID
ProSh
Max Cost

35

Episode Cost

Experimental Results: Safety Half Cheetah. ProSh achieves
absolute safety and stays under the cost constraint at all times.
PID-TD3 displays comparable performance. TD3-Lag fails to be
safe and does not achieve higher reward. ProSh displays higher
reward than all the on-policy algorithms (CPO, FOCOPS, PPOSaute).
Finally, FOCOPS fails to be safe and CPO violates the constraint
after several steps.

25
20
15

40

30

20

10
10

TD3Lag
TD3PID
ProSh

5

Cost - Hopper

40

Reward - Hopper
1200

35

20
15

TD3Lag
TD3PID
ProSh

0.0

0.4

0.6

0.8

Training Steps

1.0
1e6

0.0

0.4

0.6

0.8

Training Steps

1.0
1e6

800
600

Figure 6: Comparison with Off-Policy Algorithms on Point
Circle. Cost threshold ğ‘‘ = 10.

400
200

5

0

0
0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

Cost - PointCircle

40

30

Episode Cost

Figure 4: Comparison with Off-Policy Algorithms on Hopper.
Cost threshold ğ‘‘ = 10.

25

40

20
15
10

Cost - Hopper

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

35
30
25
20
15
10

1500
1250

20

CPO
FOCOPS
PPOSaute
ProSh

10

0
0

CPO
FOCOPS
PPOSaute
ProSh

0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

1000
750
500

Figure 7: Comparison with On-Policy Algorithms on Point
Circle. Cost threshold ğ‘‘ = 10.

250

5

30

5

Reward - Hopper

Episode Reward

40

Reward - PointCircle

50

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

35

Episode Cost

0.2

1000

TD3Lag
TD3PID
ProSh
Max Cost

10

0.0

0.2

Episode Reward

25

Episode Reward

Episode Cost

30

0

0

0

0
0.4

0.6

Training Steps

0.8

1.0
1e6

0.0

0.2

0.4

0.6

Training Steps

0.8

1.0
1e6

Figure 5: Comparison with On-Policy Algorithms on Hopper.
Cost threshold ğ‘‘ = 10.

Experimental Results: Safety Hopper. ProSh achieves absolute safety and stays under the cost constraint at all times. The
off-policy algorithms (TD3-Lag, PID-TD3) fail to be safe. FOCOPS
and CPO also fail to be safe (including the standard deviation),
while having comparable reward to ProSh. PPO-Saute is safe and
has the highest reward on this environment.
The Navigation Suite. We consider four environments: SafetyCarCircle, SafetyCarGoal, SafetyPointCircle, and SafetyPointGoal. The
agent controls either a simple robot (Point), which is constrained to
the 2D plane, with one actuator for turning and another for moving
forward or backward; or a slightly more complicated robot (Car),
which has two independently-driven parallel wheels and a freerolling rear wheel that require coordination to properly navigate.
Its goal is either to circle around the center of a circular area while
avoiding going outside the boundaries (Circle), or to navigate to
the Goalâ€™s location while circumventing Hazards (Goal).

Experimental Results: Safety Point Circle. ProSh achieves
absolute safety and stays under the cost constraint at all times after
a few steps. The off-policy algorithms (TD3-Lag, PID-TD3) fail to be
safe and do not achieve higher rewards. FOCOPS and CPO also fail
to be safe, while having comparable reward to ProSh. PPO-Saute
is the only other safe algorithm, but it has lower performance.
Cost - CarCircle

40

Reward - CarCircle

TD3Lag
TD3PID
ProSh
Max Cost

35
30

20

Episode Reward

0.2

Episode Cost

0.0

25
20
15
10

15

10

TD3Lag
TD3PID
ProSh

5

5
0

0
0.0

0.2

0.4

0.6

Training Steps

0.8

1.0
1e6

0.0

0.2

0.4

0.6

Training Steps

0.8

1.0
1e6

Figure 8: Comparison with Off-Policy Algorithms on Car
Circle. Cost threshold ğ‘‘ = 10.

Reward - CarCircle

25
20
15

15.0
12.5

Cost - CarGoal

20.0

CPO
FOCOPS
PPOSaute
ProSh

Reward - CarGoal

TD3Lag
TD3PID
ProSh
Max Cost

17.5
15.0

Episode Cost

Episode Cost

30

17.5

Episode Reward

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

35

10.0
7.5
5.0

10

30

Episode Reward

Cost - CarCircle

40

12.5
10.0
7.5

20

10

TD3Lag
TD3PID
ProSh

5.0
0

2.5
5

2.5
0.0

0

0.0
0.4

0.6

0.8

1.0
1e6

Training Steps

0.0

0.2

0.4

0.6

0.8

Training Steps

1.0
1e6

Figure 9: Comparison with On-Policy Algorithms on Car
Circle. Cost threshold ğ‘‘ = 10.
Experimental Results: Safety Car Circle. ProSh achieves
absolute safety and stays under the cost constraint at all times after
a few steps. The off-policy algorithms (TD3-Lag, PID-TD3) are not
safe. FOCOPS and CPO are also not safe and have very slightly
greater rewards compared to ProSh. PPO-Saute is the only other
safe algorithm, but has lower performance.
Cost - PointGoal

20.0

TD3Lag
TD3PID
ProSh
Max Cost

17.5

10.0
7.5

2.0

2.5

3.0
1e6

12.5
10.0
7.5
5.0

0.5

1.0

1.5

2.0

2.5

20
15

5

2.0

2.5

3.0
1e6

3.0
1e6

10.0
7.5

30
25
20

CPO
FOCOPS
PPOSaute
ProSh

15
10
5
0
âˆ’5
âˆ’10

0.5

1.0

1.5

2.0

2.5

3.0
1e6

0.0

0.5

1.0

1.5

Training Steps

2.0

2.5

3.0
1e6

Experimental Results: Safety Car Goal. PPO-Saute is always
safe but only achieves a tiny reward. Among the others, ProSh is
the safest algorithm, violating only the cost limit if the standard
deviation is taken into account. The two off-policy algorithms TD3Lag and TD3-PID are completely unsafe, while the two on-policy
algorithms CPO and FOCOPS are unsafe even after many steps.
Their rewards is slightly higher than ProSh, which showcases the
best tradeoff between safety and reward.

6

0

CPO
FOCOPS
PPOSaute
ProSh

0.0
1.5

2.5

10

âˆ’5

Training Steps

2.0

3.0
1e6

Reward - PointGoal

âˆ’10

1.0

1.5

Training Steps

Figure 13: Comparison with On-Policy Algorithms on Car
Goal. Cost threshold ğ‘‘ = 5.

TD3Lag
TD3PID
ProSh

2.5

0.5

1.0

0

Training Steps

Episode Reward

Episode Cost

15.0

12.5

Training Steps

25

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

0.5

5

0.0

Cost - PointGoal

17.5

0.0

15.0

0.0

Figure 10: Comparison with Off-Policy Algorithms on Point
Goal. Cost threshold ğ‘‘ = 5.

20.0

0.0

Reward - CarGoal

0.0

0.0
1.5

3.0
1e6

CPO
FOCOPS
PPOSaute
ProSh
Max Cost

17.5

10

âˆ’10

Training Steps

2.5

Cost - CarGoal

20.0

15

âˆ’5

1.0

2.0

2.5

2.5

0.5

1.5

5.0

5.0

0.0

1.0

Figure 12: Comparison with Off-Policy Algorithms on Car
Goal. Cost threshold ğ‘‘ = 5.

20

12.5

0.5

Training Steps

25

Episode Reward

15.0

Episode Cost

Reward - PointGoal

âˆ’10

0.0

Episode Reward

0.2

Episode Cost

0.0

0.0

0.5

1.0

1.5

Training Steps

2.0

2.5

3.0
1e6

Figure 11: Comparison with On-Policy Algorithms on Point
Goal. Cost threshold ğ‘‘ = 5.
Experimental Results: Safety Point Goal. Every algorithm is
unsafe except PPO-Saute, that only achieves a tiny reward. ProSh
is much safer than the other Algorithms, with a cost that evolves
smoothly below the limit on average. Off-policy algorithms largely
exceed the cost threshold, while on-policy algorithms stay closer
to the threshold but violate the constraint even after several steps.
In this environment, the higher safety of ProSh comes at the cost
of diminished reward compared to CPO and FOCOPS.

CONCLUSION

We introduced ProSh, a model-free algorithm for safe reinforcement learning based on probabilistic shielding in a risk-augmented
state space. Our method enforces strict safety during training, with
formal guarantees depending only on the approximation quality of
the backup critic ğ‘„ğ‘ . While this assumption is practically achievable
in various settings, future work could leverage a dynamic risk margin that adapts to the criticâ€™s accuracy over time, ensuring safety
even in early training stages.
Our theoretical results focus on deterministic environments. To
address the stochastic case, a risk-worthiness function could be
learned to guide risk allocation among successor states.
Finally, while our approach prioritized theoretical guarantees
over fine-tuned optimization, the experimental results already demonstrate the soundness and promise of the core shielding mechanism.
We expect performance to further improve with more refined implementations.

Acknowledgments. The research described in this paper was partially supported by the EPSRC (grant number EP/X015823/1) and
by the Moro-Barry family.

REFERENCES
[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2017. Constrained
policy optimization. In Proceedings of the 34th International Conference on Machine
Learning (ICML). PMLR, 22â€“31.
[2] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. 2019. Benchmarking
Safe Exploration in Deep Reinforcement Learning. arXiv preprint arXiv:1805.05800
(2019).
[3] Alekh Agarwal, Sham Kakade, Nan Jiang, and Wen Sun. 2020. Flambe: Structural
Complexity and Representation Learning of Low Rank MDPs. In Advances in
Neural Information Processing Systems (NeurIPS).
[4] Mohammed Alshiekh, Roderick Bloem, RÃ¼diger Ehlers, Bettina KÃ¶nighofer, Scott
Niekum, and Ufuk Topcu. 2017. Safe Reinforcement Learning via Shielding. CoRR
abs/1708.08611 (2017). arXiv:1708.08611 http://arxiv.org/abs/1708.08611
[5] Eitan Altman. 1999. Constrained Markov Decision Processes. https://api.
semanticscholar.org/CorpusID:14906227
[6] Christel Baier and Joost-Pieter Katoen. 2008. Principles of model checking. MIT
press.
[7] Marc G. Bellemare, Will Dabney, and RÃ©mi Munos. 2017. A Distributional Perspective on Reinforcement Learning. In International Conference on Machine
Learning (ICML).
[8] Dimitri P. Bertsekas and Steven E. Shreve. 2007. Stochastic Optimal Control: The
Discrete-Time Case. Athena Scientific.
[9] Miguel Calvo-Fullana, Santiago Paternain, Luiz F. O. Chamon, and Alejandro
Ribeiro. 2021. State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. CoRR abs/2102.11941 (2021).
arXiv:2102.11941 https://arxiv.org/abs/2102.11941
[10] Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. 2018. A lyapunov-based approach to safe reinforcement learning.
Advances in neural information processing systems 31 (2018).
[11] Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and
Edgar A. DuÃ©Ã±ez-GuzmÃ¡n. 2019. Lyapunov-based Safe Policy Optimization
for Continuous Control. CoRR abs/1901.10031 (2019). arXiv:1901.10031 http:
//arxiv.org/abs/1901.10031
[12] Ingy Elsayed-Aly, Suda Bharadwaj, Christopher Amato, RÃ¼diger Ehlers, Ufuk
Topcu, and Lu Feng. 2021. Safe Multi-Agent Reinforcement Learning via
Shielding. In AAMAS â€™21: 20th International Conference on Autonomous Agents
and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021, Frank
Dignum, Alessio Lomuscio, Ulle Endriss, and Ann NowÃ© (Eds.). ACM, 483â€“491.
https://doi.org/10.5555/3463952.3464013
[13] Scott Fujimoto, Herke Van Hoof, and David Meger. 2018. Addressing function approximation error in actor-critic methods. In International Conference on Machine
Learning (ICML). PMLR, 1587â€“1596.
[14] Javier GarcÄ±a and Fernando FernÃ¡ndez. 2015. A comprehensive survey on safe
reinforcement learning. Journal of Machine Learning Research 16, 1 (2015), 1437â€“
1480.
[15] Alexander W. Goodall and Francesco Belardinelli. 2024. Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous
Environments. In Proceedings of the 23rd International Conference on Autonomous
Agents and Multiagent Systems, AAMAS 2024, Auckland, New Zealand, May 610, 2024, Mehdi Dastani, Jaime SimÃ£o Sichman, Natasha Alechina, and Virginia
Dignum (Eds.). International Foundation for Autonomous Agents and Multiagent
Systems / ACM, 2291â€“2293. https://doi.org/10.5555/3635637.3663137
[16] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang,
Yaodong Yang, and Alois C. Knoll. 2022. A Review of Safe Reinforcement Learning:
Methods, Theory and Applications. CoRR abs/2205.10330 (2022). https://doi.org/
10.48550/ARXIV.2205.10330 arXiv:2205.10330
[17] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a
Stochastic Actor. CoRR abs/1801.01290 (2018). arXiv:1801.01290 http://arxiv.org/
abs/1801.01290
[18] Qiang He and Xinwen Hou. 2020. WD3: Taming the Estimation Bias in Deep
Reinforcement Learning. In 32nd IEEE International Conference on Tools with
Artificial Intelligence, ICTAI 2020, Baltimore, MD, USA, November 9-11, 2020. IEEE,
391â€“398. https://doi.org/10.1109/ICTAI50040.2020.00068
[19] Nils Jansen, Bettina KÃ¶nighofer, Sebastian Junges, Alex Serban, and Roderick
Bloem. 2020. Safe Reinforcement Learning Using Probabilistic Shields (Invited
Paper). In 31st International Conference on Concurrency Theory, CONCUR 2020,
September 1-4, 2020, Vienna, Austria (Virtual Conference) (LIPIcs, Vol. 171), Igor
Konnov and Laura KovÃ¡cs (Eds.). Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 3:1â€“3:16. https://doi.org/10.4230/LIPICS.CONCUR.2020.3
[20] Jiaming Ji, Borong Zhang, Jiayi Zhou, Xuehai Pan, Weidong Huang, Ruiyang
Sun, Yiran Geng, Yifan Zhong, Juntao Dai, and Yaodong Yang. 2023.
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark.
arXiv:2310.12567 [cs.AI] https://arxiv.org/abs/2310.12567
[21] Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun,
Weidong Huang, Yiran Geng, Mickel Liu, and Yaodong Yang. 2023. OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research.

arXiv:2305.09304 [cs.LG] https://arxiv.org/abs/2305.09304
[22] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, JohnMark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. 2019. Learning to
Drive in a Day. In International Conference on Robotics and Automation, ICRA
2019, Montreal, QC, Canada, May 20-24, 2019. IEEE, 8248â€“8254. https://doi.org/
10.1109/ICRA.2019.8793742
[23] Jens Kober, J. Bagnell, and Jan Peters. 2013. Reinforcement Learning in Robotics:
A Survey. The International Journal of Robotics Research 32 (09 2013), 1238â€“1274.
https://doi.org/10.1177/0278364913495721
[24] Edwin Hamel-De le Court, Francesco Belardinelli, and Alexander W. Goodall.
2025. Probabilistic Shielding for Safe Reinforcement Learning. In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25
- March 4, 2025, Philadelphia, PA, USA, Toby Walsh, Julie Shah, and Zico Kolter
(Eds.). AAAI Press, 16091â€“16099. https://doi.org/10.1609/AAAI.V39I15.33767
[25] Xingtu Liu. 2024. Information-Theoretic Generalization Bounds for Batch Reinforcement Learning. Entropy 26, 11 (2024), 995.
[26] Yongshuai Liu, Jiaxin Ding, and Xin Liu. 2020. IPO: Interior-Point Policy Optimization under Constraints. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020. AAAI Press, 4940â€“4947. https://doi.org/10.1609/AAAI.V34I04.5932
[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari
with Deep Reinforcement Learning. CoRR abs/1312.5602 (2013). arXiv:1312.5602
http://arxiv.org/abs/1312.5602
[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al. 2015. Human-level control through deep reinforcement learning.
Nature 518, 7540 (2015), 529â€“533.
[29] Haritz Odriozola-Olalde, Maider Zamalloa, and Nestor Arana-Arexolaleiba. 2023.
Shielded Reinforcement Learning: A review of reactive methods for safe learning.
In 2023 IEEE/SICE International Symposium on System Integration (SII). 1â€“8. https:
//doi.org/10.1109/SII55687.2023.10039301
[30] Alex Ray, Joshua Achiam, and Dario Amodei. 2019. Benchmarking Safe Exploration in Deep Reinforcement Learning. In arXiv preprint arXiv:1910.01708.
[31] Harsh Satija, Philip Amortila, and Joelle Pineau. 2020. Constrained markov
decision processes via backward value functions. In International Conference on
Machine Learning. PMLR, 8502â€“8511.
[32] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter
Abbeel. 2015. Trust Region Policy Optimization. CoRR abs/1502.05477 (2015).
arXiv:1502.05477 http://arxiv.org/abs/1502.05477
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017).
arXiv:1707.06347 http://arxiv.org/abs/1707.06347
[34] Aivar Sootla, Alexander I. Cowen-Rivers, Taher Jafferjee, Ziyan Wang,
David Henry Mguni, Jun Wang, and Haitham Ammar. 2022. Saute RL: Almost
Surely Safe Reinforcement Learning Using State Augmentation. In International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,
USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri,
Stefanie Jegelka, Le Song, Csaba SzepesvÃ¡ri, Gang Niu, and Sivan Sabato (Eds.).
PMLR, 20423â€“20443. https://proceedings.mlr.press/v162/sootla22a.html
[35] Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn.
2020. Learning to be Safe: Deep RL with a Safety Critic. CoRR abs/2010.14603
(2020). arXiv:2010.14603 https://arxiv.org/abs/2010.14603
[36] Adam Stooke, Joshua Achiam, and Pieter Abbeel. 2020. Responsive Safety in
Reinforcement Learning by PID Lagrangian Methods. In ICML.
[37] Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction (2nd ed.). MIT press, Cambridge, MA.
[38] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA.
[39] Sebastian Thrun and Anton Schwartz. 1993. Issues in using function approximation for reinforcement learning. In Proceedings of the 1993 Connectionist Models
Summer School. Lawrence Erlbaum.
[40] Hado Van Hasselt. 2010. Double Q-learning. In Advances in neural information
processing systems. 2613â€“2621.
[41] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado P. van Hasselt, Marc G. Bellemare,
and Nando de Freitas. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In International Conference on Machine Learning (ICML).
[42] Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-Learning. Machine
Learning 8, 3â€“4 (1992), 279â€“292.
[43] Zhihan Xie, Qi Cai, Ethan Zhou, Alexander Risteski, and Alexander G. Gray. 2021.
Bellman Error is a Good Proxy for Value Error. In International Conference on
Machine Learning (ICML).
[44] Wen-Chi Yang, Giuseppe Marra, Gavin Rens, and Luc De Raedt. 2023. Safe
Reinforcement Learning via Probabilistic Logic Shields. In Proceedings of the
Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023,
19th-25th August 2023, Macao, SAR, China. ijcai.org, 5739â€“5749. https://doi.org/

10.24963/IJCAI.2023/637
[45] Yiming Zhang, Quan Vuong, and Keith Ross. 2020. First order constrained
optimization in policy space. Advances in Neural Information Processing Systems
33 (2020), 15338â€“15349.
[46] Yiming Zhang, Quan Vuong, and Keith W. Ross. 2020. First Order Optimization in
Policy Space for Constrained Deep Reinforcement Learning. CoRR abs/2002.06506
(2020). arXiv:2002.06506 https://arxiv.org/abs/2002.06506

A TECHNICAL APPENDICES AND SUPPLEMENTARY MATERIAL
A.1 ProSH-TD3 implementation
We provide additional details on the implementation of ProSH-TD3 and the theoretical guarantees that ensure its safety during training.
First, we describe the full training procedure, including the update rules for both the main and backup actorâ€“critic pairs. We then present the
exact loss functions used to train each component of the algorithm. Finally, we prove that ProSH-TD3 sampling remains safe even in the
presence of exploration noise and hybrid training episodes.
A.1.1 Pseudo-code and training. The complete algorithm is outlined in Algorithm 2, including the training of the main actor and the backup
actor, using normal episode where the main actor is used for sampling, and hybrid episode where the backup actor is used for sampling after
several steps of the main actor.
Algorithm 2 ProSh-TD3
1: Input: cost budget ğ‘‘, margin ğ›¿, discount factors ğ›¾, ğ›¾ğ‘ , â„ğ‘¦ğ‘ğ‘Ÿğ‘–ğ‘‘_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦, ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦
ğœ‰ğ‘–

2: Initialize actor-critic pairs ğ‘„ğ‘ ğ‘– , ğœ‹ğ‘ , ğ‘„ ğ‘Ÿ , ğœ‹ ğ‘Ÿ , ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ â† 0, ğ¿ â† []
ğœƒ

ğœ™

ğœ“

3: for each episode do
4:
5:
6:
7:
8:
9:

10:
11:
12:

ğ‘  â† ğ‘  init, ğ‘Ÿ â† ğ‘‘ âˆ’ ğ›¿, ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘–ğ‘›ğ‘‘ â† 0
if ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ mod â„ğ‘¦ğ‘ğ‘Ÿğ‘–ğ‘‘_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ = 0 then
ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„_ğ‘ ğ‘¡ğ‘’ğ‘ â† rand(0, mean(ğ¿))
else
ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„_ğ‘ ğ‘¡ğ‘’ğ‘ â† +âˆ
end if
while not done do
if ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘–ğ‘›ğ‘‘ â‰¤ ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„_ğ‘ ğ‘¡ğ‘’ğ‘
then
 

Sample ğ‘ âˆ¼ Î ğœ‹ ğ‘Ÿ (ğ‘ ) and add noise to ğ‘
ğœ“

Execute ğ‘, observe ğ‘  â€² , reward ğ‘Ÿ , cost ğ‘
14:
else
15:
(ğ‘ , ğ‘¥) â† ğ‘ 
ğœ™
16:
Sample ğ‘ âˆ¼ ğœ‹ğ‘ and add noise to ğ‘
17:
Execute ğ‘ = (ğ‘, ğ‘¥), observe ğ‘  â€² , reward ğ‘Ÿ , cost ğ‘
18:
end if
19:
store (ğ‘ , ğ‘, ğ‘Ÿ, ğ‘, ğ‘  â€² ) in D
20:
ğ‘  â† ğ‘  â€² , ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘–ğ‘›ğ‘‘ â† ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘–ğ‘›ğ‘‘ + 1
21:
Sample from D and update critic networks by minimizing losses L (ğœƒ ğ‘– ), L (ğœ‰ğ‘– )
22:
if ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ mod ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ = 0 then
23:
update actor networks by minimizing losses L (ğœ™), L (ğœ“ )
24:
end if
25:
end while
26:
append ğ‘ ğ‘¡ğ‘’ğ‘_ğ‘–ğ‘›ğ‘‘ to ğ¿, ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ â† ğ‘’ğ‘_ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ + 1
27: end for
28: return projected policy ğœ‹Ëœ (via Thm. 2)
13:

Actor-critic pairs training. We train the actor-critic pairs with batch sampling from a replay buffer in which we store every transition
made in the augmented MDP. For the backup critic, we use the training target ğ‘¦ğ‘ (ğ‘, ğ‘  â€² ), proposed in [18], equal to
ï£®
ğ‘¡ğ‘ğ‘Ÿğ‘” 
 1 âˆ’ ğ›½ âˆ‘ï¸ ğœƒ ğ‘¡ğ‘ğ‘Ÿğ‘” 
 ï£¹ï£º
ï£¯
ğœƒ
ğœ“ ğ‘¡ğ‘ğ‘Ÿğ‘” â€²
ğœ“ ğ‘¡ğ‘ğ‘Ÿğ‘” â€²
ğ‘ + ğ›¾ğ‘ ï£¯ï£¯ğ›½ min ğ‘„ğ‘ ğ‘—
ğ‘  â€², ğœ‹ğ‘
(ğ‘  ) + ğœ– +
ğ‘„ğ‘ ğ‘—
ğ‘  â€², ğœ‹ğ‘
(ğ‘  ) + ğœ– ï£ºï£º ,
2
ï£¯ ğ‘— âˆˆ {1,2}
ï£º
ğ‘— âˆˆ {1,2}
ï£°
ï£»
where ğœ– is a small gaussian noise and ğ›½ is a hyperparameter. Compared to the standard TD3 training target, this target helps combat the
underestimation bias of TD3 [18].
Both of the backup critic heads are learned by regressing to this target:

2
ğœƒğ‘–
â€²
L (ğœƒ ğ‘– ) = E ( (ğ‘ ,ğ‘¥ ),(ğ‘,ğ‘¦),ğ‘Ÿ,ğ‘ (ğ‘  â€² ,ğ‘¥ â€² ) )âˆ¼D ğ‘„ğ‘ (ğ‘ , ğ‘) âˆ’ ğ‘¦ğ‘ (ğ‘, ğ‘  )
.
The backup actor is updated with the standard TD3 actor loss:
h
i
L (ğœ™) = âˆ’E (ğ‘ ,ğ‘¥ )âˆ¼D ğ‘„ğ‘ğœƒ 1 (ğ‘ , ğœ‹ ğœ™ (ğ‘ )) .

The main critic is updated with a loss adapted from TD3, but taking into account that the shielded main actor outputs three actions with
their respective probabilities:

2
ğœ‰ğ‘–
, where
L (ğœ‰ğ‘– ) = E (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘,ğ‘  â€² )âˆ¼D ğ‘„ ğ‘Ÿ (ğ‘ , ğ‘) âˆ’ ğ‘¦ğ‘Ÿ (ğ‘Ÿ, ğ‘  â€² )
ğ‘¡ğ‘ğ‘Ÿğ‘”

ğ‘¦ğ‘Ÿ (ğ‘Ÿ, ğ‘  â€² ) = ğ‘Ÿ + ğ›¾

âˆ‘ï¸
ğ‘˜ âˆˆ {1,2,3}

ğœƒ

ğœ†ğ‘˜ min ğ‘„ ğ‘Ÿ ğ‘—
ğ‘— âˆˆ {1,2}


ğœ“
ğ‘  â€², ğ‘ğ‘˜ + ğœ– with Î(ğœ‹ ğ‘Ÿ )(ğ‘  â€² ) =

âˆ‘ï¸

ğœ†ğ‘˜ ğ‘ğ‘˜ .

ğ‘˜ âˆˆ {1,2,3}

Similarly, the main actor is updated taking into account the shield:
ï£® âˆ‘ï¸
ï£¹
âˆ‘ï¸
ï£¯
ï£º
ğœ‰1
ğœ“
L (ğœ“ ) = âˆ’Eğ‘ âˆ¼D ï£¯ï£¯
ğœ†ğ‘˜ ğ‘„ ğ‘Ÿ (ğ‘ , ğ‘ğ‘˜ ) ï£ºï£º where Î(ğœ‹ ğ‘Ÿ )(ğ‘ ) =
ğœ†ğ‘˜ ğ‘ğ‘˜ .
ï£¯ğ‘˜ âˆˆ {1,2,3}
ï£º
ğ‘˜ âˆˆ {1,2,3}
ï£°
ï£»
Note that the gradients can flow through the ğœ†ğ‘˜ and the ğ‘ğ‘˜ in the main actor training loss as they are computed in a fully differentiable
way.
A.1.2 Safe sampling guarantees. The algorithm 2 includes noise for the exploration as well as exploration rounds to train the backup actor
and the backup critic. We will present Theorem 5 stating that the sampling is still safe and provide an upper-bound for the cost.
Theorem 5 (Safety of the shielding with noise). We consider a MDP M and any two policies ğœ‹Â¯1 and ğœ‹Â¯2 , and ğœ‰ âˆˆ [0, 1] a small number.
Then, with ğœ‹Â¯ the policy defined as
ğœ‹Â¯ (ğ‘ ) = (1 âˆ’ ğœ‰) ğœ‹Â¯1 (ğ‘ ) + ğœ‰ ğœ‹Â¯2 (ğ‘ )
for all ğ‘  âˆˆ S, the discounted cost of the policy ğœ‹Â¯ satisfies
ğœ‰ğ‘ max
1
Â¯ â‰¤ ğ¶ (ğœ‹Â¯1 ) +
ğ¶ (ğœ‹)
.
1 âˆ’ ğ›¾ğ‘ 1 âˆ’ (1 âˆ’ ğœ‰)ğ›¾ğ‘
We now consider the effective policy used during sampling, and not only the policy that the algorithm ouptuts. We aim to show that it
also satisfies the safety guarantees. In practice, ProSH alternates between two modes: standard training episodes using the main actor, and
hybrib episodes where the backup actor is used after several steps of the main actor. This alternation is used to improve the training of the
backup critic ğ‘„ğ‘ . In both cases, a small noise is added.
For every augmented state (ğ‘ , ğ‘¥), both the main actor policy and the hybrid policy, without noise, can be written as
ğœ‹Â¯ (ğ‘ , ğ‘¥) = ğœ†(ğ‘ , ğ‘¥) ğœ‹Â¯1 + (1 âˆ’ ğœ†(ğ‘ , ğ‘¥)) ğœ‹Â¯ğ‘ ,
where ğœ‹Â¯1 is the main actor and ğœ‹Â¯ğ‘ . Hence, the policy ğœ‹Â¯ is also ğ‘„ğ‘ -shielded, and its cost satisfies the estimate
2Î”ğ‘
Â¯ â‰¤ ğ‘¥0 +
ğ¶ ( ğœ‹)
.
1 âˆ’ ğ›¾ğ‘
We now consider the final policy used for sampling with noise, and define
ğœ‹Â¯0 = (1 âˆ’ ğœ‰) ğœ‹Â¯ + ğœ‰ ğœ‹Â¯ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ .
Using theorem 5, we have the upperbounds:
1
ğœ‰ğ‘ max
2Î”ğ‘
1
ğœ‰ğ‘ max
â‰¤ ğ‘¥0 +
+
.
1 âˆ’ ğ›¾ğ‘ 1 âˆ’ (1 âˆ’ ğœ‰)ğ›¾ğ‘
1 âˆ’ ğ›¾ğ‘ 1 âˆ’ ğ›¾ğ‘ 1 âˆ’ (1 âˆ’ ğœ‰)ğ›¾ğ‘
So the policy used for sampling is also safe, and the term induced by the noise can be removed by choosing
1
ğœ‰ğ‘ max
ğ‘¥0 â‰¤ ğ‘‘ âˆ’
.
1 âˆ’ ğ›¾ğ‘ 1 âˆ’ (1 âˆ’ ğœ‰)ğ›¾ğ‘
Â¯ +
ğ¶ (ğœ‹Â¯0 ) â‰¤ ğ¶ (ğœ‹)

A.2

Velocity and Navigation environments

We evaluate our method on environments from the Safe Velocity and Safe Navigation suites of Safety-Gymnasium. Specifically, we use
HalfCheetah, and Hopper (velocity tasks), and CarGoal, CarCircle, PointGoal, and PointCircle (navigation tasks).
Velocity Tasks. These tasks focus on moving forward quickly while remaining within a safe speed limit. The reward is proportional to
forward progress, and the cost is binary: it equals 1 if the agentâ€™s velocity exceeds 50% of its training-time maximum, and 0 otherwise.
â€¢ HalfCheetah: A 2D robot with 9 links and 8 joints learns to run forward via torque control.
â€¢ Hopper: A one-legged robot learns to hop forward by coordinating joint torques.
Navigation Tasks. These tasks require agents to reach goals or follow paths while avoiding unsafe regions. The reward is based on
proximity to the goal or path adherence, and the cost equals 1 upon contact with obstacles or hazardous zones.
â€¢ CarGoal vs. PointGoal: Both tasks require reaching a fixed target location.
â€“ CarGoal uses a wheeled robot with non-holonomic constraints, simulating a simplified autonomous vehicle.
â€“ PointGoal uses a free-moving point mass, which can change direction instantaneously.
â€¢ CarCircle vs. PointCircle: The goal is to follow a predefined circular trajectory.
â€“ CarCircle again uses the wheeled car agent, requiring continuous steering control.
â€“ PointCircle uses the point mass agent, allowing for simpler motion planning but still subject to the same cost constraints.

(a) CarCircle

(b) CarGoal

(c) Hopper

(d) Half Cheetah

Figure 14: On the left, two environments of the Safe Navigation suite from Safety-Gymnasium, CarCircle and CarGoal. In
CarCircle, the agent needs to circle around the center of the circle area while avoiding going outside the boundaries. In CarGoal,
the agent needs to navigate to the Goalâ€™s location while circumventing Hazards. In both case we choose ğ¿ğ‘’ğ‘£ğ‘’ğ‘™ = 1. On the right,
two environments of the Safe Velocity suite from Safety-Gymnasium, SafetyHopper and SafetyHalfCheetah. In both cases the
agent has to move as quickly as possible while adhering to a velocity constraint.

A.3

Experimental Setup

Hyperparameters. For PPO-Saute and TD3-Lag, we set ğ›¾ğ‘ = 0.995, and we set ğ›¾ğ‘†ğ‘ğ‘¢ğ‘¡ğ‘’ = 0.995 for PPO-Saute. For the other hyperparameters
for TD3-Lag and PPO-Saute , we use for each environments the default hyperparameters provided by Omnisafe [21]. As for our algorithm,
the set of hyperparameters used for every environment is given in Table 1. The more difficult the environment, the more the learning rate of
the actors are decreased and the learning rate of the critics increased, as is standard for modern TD3 implementations.
In Table 1, ğ›½ is the hyperparameter in the backup critic target.
Experimental Setup. All experiments were conducted on a PBS-managed high-performance computing cluster running Red Hat Enterprise
Linux 8.5 (Ootpa). Each compute node is equipped with 2Ã— AMD EPYC 7742 CPUs (128 cores total), 1 TB of RAM, and 8Ã— NVIDIA Quadro
RTX 6000 GPUs. We launched each experiment seed as an independent PBS job, allocating 1 CPU core, 12GB of RAM, and 1 GPU per job. A
typical training run (1 million environment steps) took approximately 4 hours.
For PPO-Saute, CPO, FOCOPS, TD3-PID, and TD3-Lag, we used Python 3.10.0, Omnisafe version 0.5.0, Torch version 2.7.0 and Cuda 11.8.
For ProSH-TD3, we used Python 3.10.0, Stable baselines3 version 2.6.0, Torch version 2.7.0 and Cuda 11.8.

Hyperparameter
Main actor LR
Backup actor LR
Main critic LR
Backup critic LR
Reward Discount factor
Cost Discount factor
policy_delay
hybrid_delay
ğ›½
net_arch
Exploration Policy

HalfCheetah

PointCircle

Hopper

CarCircle

PointGoal

CarGoal

1e-4
1e-4
1e-4
1e-4
0.99
0.995
2
2
0.75
[400,300]
N (0, 0.2)

1e-4
1e-4
1e-4
1e-4
0.99
0.995
2
2
0.75
[400,300]
N (0, 0.2)

5e-4
5e-4
1e-3
1e-3
0.99
0.995
2
2
0.75
[400,300]
N (0, 0.2)

5e-5
5e-5
1e-3
3e-4
0.99
0.995
2
2
0.75
[400,300]
N (0, 0.2)

5e-5
5e-6
1e-3
3e-4
0.99
0.995
2
3
0.75
[400,300]
N (0, 0.2)

5e-6
5e-5
1e-4
3e-4
0.99
0.995
2
3
0.75
[400,300]
N (0, 0.2)

Table 1: Hyperparameters for ProSH-TD3

A.4

Theoretical Analysis

In this section, we provide all the proofs for the theorem we have stated throughout the article.
A.4.1 Proof of Theorem 1. We recall Theorem 1:
Theorem 1. (Safety bound for ğ‘„ğ‘ -shields.)
Let M be a CMDP with cost discount factor ğ›¾ğ‘ , and let ğ‘„ğ‘âˆ— be its optimal stateâ€“action cost function. Assume that ğ‘„ğ‘ is a ğ‘„-value, and let
Î”ğ‘ = ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— ||ğ¿âˆ ( S A ) . For any policy ğœ‹Â¯ âˆˆ Î  val that is ğ‘„ğ‘ -shielded, the expected discounted cost from the augmented state (ğ‘  0, ğ‘¥ 0 ) obeys
ğ‘¥0
2Î”ğ‘
+
,
whenever ğ‘¥ 0 â‰¥ ğ‘„ğ‘ (ğ‘  0 ).
ğ›¾ğ‘ 1 âˆ’ ğ›¾ğ‘
In order to prove theorem 1, we will first prove the following lemma.
Â¯ â‰¤
C (ğ‘ 0 ,ğ‘¥ 0 ) ( ğœ‹)

Â¯ Then,
Lemma 1. Let ğ¶ğ‘› (ğ‘ , ğ‘¥) = ğ›¾ğ‘ ğ·ğ‘›âˆ’1 (ğ‘ , ğ‘¥), where ğ·ğ‘› is the expected discounted cost of the ğ‘› first steps of the policy ğœ‹.

ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘¥ + ğ‘“ğ‘› (Î”ğ‘ ), when ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘„ğ‘ (ğ‘ ) + ğ‘”ğ‘› (Î”ğ‘ ), when ğ‘¥ < ğ‘„ğ‘ (ğ‘ )
as long as the sequences ğ‘“ğ‘› (Î”ğ‘ ) and ğ‘”ğ‘› (Î”ğ‘ ) satisfy ğ‘“1 (Î”ğ‘ ) â‰¥ ğ›¾ğ‘ Î”ğ‘ , ğ‘”1 (Î”ğ‘ ) â‰¥ ğ›¾ğ‘ Î”ğ‘ and for every ğ‘› â‰¥ 2,

ğ‘“ğ‘› (Î”ğ‘ ) â‰¥ 2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1,
ğ‘”ğ‘› (Î”ğ‘ ) â‰¥ max(2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘”ğ‘›âˆ’1 (Î”ğ‘ ), 3ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ )).
Â¯
Proof. For ğœ‹Â¯ a ğ‘„ğ‘ -shielded policy of the augmented CMDP, we define the sequence ğ¶ğ‘› (ğ‘ , ğ‘¥) as (we omit the dependency on ğœ‹)
âˆ‘ï¸
âˆ‘ï¸
ğ‘ƒ (ğ‘  â€², ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² ))
ğ¶ğ‘› (ğ‘ , ğ‘¥) =(1 âˆ’ ğœ†)
ğ‘ƒğœ‹Â¯ ((ğ‘, ğ‘¦ğ‘ )|(ğ‘ , ğ‘¥))
ğ‘ â€²

ğ‘âˆˆğ´(ğ‘  )

+ğœ†

âˆ‘ï¸

â€²

ğ‘ƒ (ğ‘  |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘„ğ‘ (ğ‘  â€² )),

ğ¶ 0 (ğ‘ , ğ‘¥) = 0,

ğ‘ â€²

where ğ‘¥ â€² = ğ‘¦ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ). Note that ğ¶ğ‘› (ğ‘ , ğ‘¥) = ğ›¾ğ‘ ğ·ğ‘›âˆ’1 (ğ‘ , ğ‘¥), where ğ·ğ‘›âˆ’1 (ğ‘ , ğ‘¥) is the average discounted expected cost of the policy
Â¯ We used the fact that ğ‘ƒ ((ğ‘  â€², ğ‘¥ â€² )|(ğ‘, ğ‘¦ğ‘ )) = ğ‘ƒ (ğ‘  â€² |ğ‘) when ğ‘¥ â€² = ğ‘¦ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ). Finally, throughout this proof, when there is no
ğœ‹.
ambiguity, we will denote ğ‘ƒğ‘ = ğ‘ƒ ((ğ‘, ğ‘¦ğ‘ )|(ğ‘ , ğ‘¥)).
We will show by induction that

ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘¥ + ğ‘“ğ‘› (Î”ğ‘ ), ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘„ğ‘ (ğ‘ ) + ğ‘”ğ‘› (Î”ğ‘ ), ğ‘¥ < ğ‘„ğ‘ (ğ‘ ).
Base case, ğ‘› = 1, ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ):
In this case, we have by definition that
ğœ‹Â¯ (ğ‘ , ğ‘¥) = (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘ (ğ‘, ğ‘¦ğ‘ ) + ğœ†(ğœ‹ğ‘ (ğ‘ ), ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ ))),

ğ‘âˆˆğ´(ğ‘  )

where
ğ‘¥ â‰¥ ğ›¾ğ‘ (1 âˆ’ ğœ†)

âˆ‘ï¸
ğ‘

ğ‘ƒğ‘ğ‘¦ğ‘ + ğ›¾ğ‘ ğœ†ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )).

The cost function ğ¶ 1 (ğ‘ , ğ‘¥) satisfies by definition
âˆ‘ï¸

ğ¶ 1 (ğ‘ , ğ‘¥) =(1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘

ğ‘

+ ğœ†ğ›¾ğ‘

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ 0 (ğ‘  â€², ğ‘¥ â€² ))

ğ‘ â€²

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))(ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ¶ 0 (ğ‘  â€², ğ‘¦ â€² )),

ğ‘ â€²

so, using ğ¶ 0 = 0,
ğ¶ 1 (ğ‘ , ğ‘¥) = (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ ğ‘ (ğ‘ , ğ‘) + ğœ†ğ›¾ğ‘

ğ‘ â€²

ğ‘

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )).

ğ‘ â€²

Now, any policy taking action ğ‘ has a cost at least equal to
Ã
â€²
âˆ—
ğ‘  â€² ğ‘ƒ (ğ‘  |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )). Using ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘ || âˆ â‰¤ Î”ğ‘ gives
âˆ‘ï¸
ğ‘ƒ (ğ‘  â€², ğ‘)ğ‘ (ğ‘ , ğ‘) â‰¤ ğ‘„ğ‘ (ğ‘ , ğ‘) + Î”ğ‘ ,

Ã
â€²
âˆ—
â€²
âˆ—
ğ‘  â€² ğ‘ƒ (ğ‘  |ğ‘)ğ‘ (ğ‘ , ğ‘). Similarly, ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥
ğ‘  â€² ğ‘ƒ (ğ‘  |ğ‘)ğ‘ (ğ‘ , ğ‘), so ğ‘„ğ‘ (ğ‘ , ğ‘) â‰¥

Ã

âˆ‘ï¸

ğ‘ â€²

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¤ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + Î”ğ‘ .

ğ‘ â€²

Hence, we get
âˆ‘ï¸

ğ¶ 1 (ğ‘ , ğ‘¥) â‰¤ (1 âˆ’ ğœ†)ğ›¾ğ‘

ğ‘ƒğ‘ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğœ†ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ›¾ğ‘ Î”ğ‘ .

ğ‘

Since for any ğ‘ âˆˆ ğ´(ğ‘ ), ğ‘¦ğ‘ â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘), we obtain
ğ¶ 1 (ğ‘ , ğ‘¥) â‰¤ (1 âˆ’ ğœ†)ğ›¾ğ‘

âˆ‘ï¸

ğ‘ƒğ‘ğ‘¦ğ‘ + ğœ†ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ›¾ğ‘ Î”ğ‘ .

ğ‘

Using the weight condition, we finally obtain

ğ¶ 1 (ğ‘ , ğ‘¥) â‰¤ ğ‘¥ + ğ›¾ğ‘ Î”ğ‘ .

Base case, ğ‘› = 1, ğ‘¥ < ğ‘„ğ‘ (ğ‘ ):
In this situation, we have by the definition of a ğ‘„ğ‘ -shielded policy that
ğœ‹Â¯ (ğ‘ , ğ‘¥) = (ğœ‹ğ‘ (ğ‘ ), ğ‘§),

ğ›¾ğ‘ ğ‘§ â‰¤ ğ‘¥ .

In this case, the cost function ğ¶ 1 satisfies
âˆ‘ï¸
âˆ‘ï¸
ğ¶ 1 (ğ‘ , ğ‘¥) =
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ¶ 0 (ğ‘  â€², ğ‘„ğ‘ (ğ‘  â€² )) + ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ ))) =
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )).
ğ‘ â€²

ğ‘ â€²

Ã
Any policy taking ğ‘  â€² with probability 1 would have a cost of at least ğ‘  â€² ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )), which means
âˆ‘ï¸
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )),
ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥
ğ‘ â€²

so
ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) âˆ’ Î”ğ‘ .

ğ‘ â€²

Consequently,

ğ¶ 1 (ğ‘ , ğ‘¥) â‰¤ ğ›¾ğ‘ (ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + Î”ğ‘ ) = ğ‘„ğ‘ (ğ‘ ) + ğ›¾ğ‘ Î”ğ‘ .

Induction step, ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ):
With the same notation, ğœ‹Â¯ now satisfies
ğœ‹Â¯ (ğ‘ , ğ‘¥) = (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘ (ğ‘, ğ‘¦ğ‘ ) + ğœ†(ğœ‹ğ‘ (ğ‘ ), ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ ))),

ğ‘

where
ğ‘¥ â‰¥ ğ›¾ğ‘ (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘ğ‘¦ğ‘ + ğ›¾ğ‘ ğœ†ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )).

ğ‘

By definition, the cost functional ğ¶ğ‘› satisfies
ğ¶ğ‘› (ğ‘ , ğ‘¥) = (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ‘ƒğ‘

ğ‘

+ğœ†

âˆ‘ï¸

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² ))

ğ‘ â€²

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘„ğ‘ (ğ‘  â€² ))),

ğ‘ â€²

where ğ‘¥ â€² = ğ‘¦ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ). Since for all ğ‘ âˆˆ ğ´(ğ‘ ), ğ‘¦ğ‘ â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘), then ğ‘¥ â€² â‰¥ ğ‘„ğ‘ (ğ‘  â€² ), so we can apply the induction hypothesis and find
ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² ) â‰¤ ğ‘¥ â€² + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ),

ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘„ğ‘ (ğ‘  â€² )) â‰¤ ğ‘„ğ‘ (ğ‘  â€² ) + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).

Hence, ğ¶ğ‘› (ğ‘ , ğ‘¥) satisfies
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ (1 âˆ’ ğœ†)ğ´ + ğœ†ğµ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ),

where
ğ´=

âˆ‘ï¸

ğ‘ƒğ‘

ğ‘

ğµ=

âˆ‘ï¸

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€², ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ‘¥ â€² ),

ğ‘ â€²

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² )).

ğ‘ â€²

Now, we have
ğ‘„ğ‘âˆ— (ğ‘ , ğ‘) = ğ‘ (ğ‘ , ğ‘) +

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ‘„ğ‘âˆ— (ğ‘  â€² ) â‰¥ ğ‘ (ğ‘ , ğ‘) +

âˆ‘ï¸

ğ‘ â€²

so ğ‘„ğ‘ (ğ‘ , ğ‘) â‰¥ ğ‘ (ğ‘ , ğ‘) +

ğ‘ƒ (ğ‘  â€² |ğ‘)(ğ‘„ğ‘ (ğ‘  â€² ) âˆ’ Î”ğ‘ ),

ğ‘ â€²

â€²
â€²
â€²
ğ‘ ğ‘ƒğ‘ (ğ‘  |ğ‘) âˆ’ 2Î”ğ‘ . Using additionally ğ‘¥ = ğ‘¦ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  ), we obtain

Ã

ğ´â‰¤

âˆ‘ï¸

ğ‘ƒğ‘

âˆ‘ï¸

ğ‘ƒğ‘

ğ‘

+

âˆ‘ï¸
ğ‘

â‰¤

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ‘¦ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ))

ğ‘ â€²

ğ‘

â‰¤

âˆ‘ï¸

ğ‘ƒğ‘

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘  â€² ) âˆ’

âˆ‘ï¸

ğ‘ â€²

ğ‘

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€², ğ‘)ğ›¾ğ‘ ğ‘¦ğ‘ + 2ğ›¾ğ‘ Î”ğ‘

ğ‘ƒğ‘

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘  â€² )

ğ‘ â€²

ğ‘ â€²

ğ›¾ğ‘ ğ‘ƒğ‘ğ‘¦ğ‘ + 2ğ›¾ğ‘ Î”ğ‘ .

ğ‘

We reason in a similar way for the second term ğµ. We have
ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥ ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) +

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))(ğ‘„ğ‘ (ğ‘  â€² ) âˆ’ Î”ğ‘ ),

ğ‘ â€²

so ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥ ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) +

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ‘„ğ‘ (ğ‘  â€² ) âˆ’ 2Î”ğ‘ . Using this inequality gives for ğµ
âˆ‘ï¸
ğµâ‰¤
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² )) â‰¤ ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + 2ğ›¾ğ‘ Î”ğ‘ .

Ã

ğ‘ â€²

ğ‘ â€²

Finally, we get
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ (1 âˆ’ ğœ†)

âˆ‘ï¸

ğ›¾ğ‘ ğ‘ƒğ‘ğ‘¦ğ‘ + (1 âˆ’ ğœ†)2ğ›¾ğ‘ Î”ğ‘ + ğœ†ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + 2ğœ†ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).

ğ‘

Using the shielding condition on the weights that ğœ‹Â¯ satisfies, we finally obtain
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘¥ + 2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ) = ğ‘¥ + ğ‘“ğ‘› (Î”ğ‘ ).
Induction step, ğ‘¥ < ğ‘„ğ‘ (ğ‘ ): In this case, ğœ‹Â¯ is given by
ğœ‹Â¯ (ğ‘ , ğ‘¥) = (ğœ‹ğ‘ (ğ‘ ), ğ‘§),

ğ›¾ğ‘ ğ‘§ â‰¤ ğ‘¥ .

This means that the cost functional satisfies
âˆ‘ï¸
ğ¶ğ‘› (ğ‘ , ğ‘¥) =
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² )),

ğ‘¥ â€² = ğ‘§ âˆ’ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² ).

ğ‘ â€²

Since ğ‘§ â‰¤ ğ‘¥/ğ›¾ğ‘ , and ğ‘¥ < ğ‘„ğ‘ (ğ‘ ), we obtain ğ‘§ < ğ‘„ğ‘ (ğ‘ )/ğ›¾ğ‘ . As a result,
ğ‘¥ â€² < ğ‘„ğ‘ (ğ‘ )/ğ›¾ğ‘ âˆ’ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² ).
Now, ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) âˆ’ Î”ğ‘ , and ğ‘„ğ‘âˆ— (ğ‘ ) â‰¤ ğ›¾ğ‘ ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) as it is defined as the minimum over the actions, so
ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¥ ğ‘„ğ‘âˆ— (ğ‘ )/ğ›¾ğ‘ âˆ’ Î”ğ‘ .
Overall,

ğ‘¥ â€² < ğ‘„ğ‘ (ğ‘  â€² ) + Î”ğ‘ .
Two situations are possible here. Either ğ‘¥ â€² < ğ‘„ğ‘ (ğ‘  â€² ), or ğ‘¥ â€² â‰¥ ğ‘„ğ‘ (ğ‘  â€² ). We start with ğ‘¥ â€² < ğ‘„ğ‘ (ğ‘  â€² ).
In this case, we obtain
ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² ) â‰¤ ğ‘„ğ‘ (ğ‘  â€² ) + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ),
so we get for ğ¶ğ‘› (ğ‘ , ğ‘¥)
âˆ‘ï¸
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤
ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² )) + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).
Now, ğ‘„ğ‘ (ğ‘  â€² ) â‰¤ ğ‘„ğ‘âˆ— (ğ‘  â€² ) + Î”ğ‘ , and

ğ‘ â€²
â€²
âˆ— â€²
âˆ—
ğ‘  â€² ğ‘ƒ (ğ‘  , ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  )) = ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )), so
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ›¾ğ‘ ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).

Ã

Finally, using ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) â‰¤ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + Î”ğ‘ ,
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + 2ğ›¾ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ) =â‰¤ ğ‘„ğ‘ (ğ‘ ) + 2ğ›¾ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ) â‰¤ ğ‘„ğ‘ (ğ‘ ) + ğ‘“ğ‘› (Î”ğ‘ ).

Now, in the other situation, we have ğ‘¥ â€² â‰¥ ğ‘„ğ‘ (ğ‘  â€² ). But we also have ğ‘¥ â€² â‰¤ ğ‘„ğ‘ (ğ‘  â€² ) + Î”ğ‘ . Hence, we can write
ğ¶ğ‘›âˆ’1 (ğ‘  â€², ğ‘¥ â€² ) â‰¤ ğ‘¥ â€² + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ) â‰¤ ğ‘„ğ‘ (ğ‘  â€² ) + Î”ğ‘ + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).
So
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘¥ â€² + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ))

ğ‘ â€²

â‰¤

âˆ‘ï¸

â‰¤

âˆ‘ï¸

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘ (ğ‘  â€² ) + Î”ğ‘ + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ))

ğ‘ â€²

ğ‘ƒ (ğ‘  â€² |ğœ‹ğ‘ (ğ‘ ))ğ›¾ğ‘ (ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + ğ‘„ğ‘âˆ— (ğ‘  â€² ) + 2Î”ğ‘ + ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ))

ğ‘ â€²

â‰¤ ğ›¾ğ‘ ğ‘„ğ‘âˆ— (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + 2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ )
â‰¤ ğ›¾ğ‘ ğ‘„ğ‘ (ğ‘ , ğœ‹ğ‘ (ğ‘ )) + 3ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ) = ğ‘„ğ‘ (ğ‘ ) + 3ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ ).
â–¡
We can now go on with the proof of theorem 1.
Proof. Assume the assumptions of the theorem are satisfied. We use lemma 1 and obtain

ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘¥ + ğ‘“ğ‘› (Î”ğ‘ ), when ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘„ğ‘ (ğ‘ ) + ğ‘”ğ‘› (Î”ğ‘ ), when ğ‘¥ < ğ‘„ğ‘ (ğ‘ )
as long as the sequences ğ‘“ğ‘› (Î”ğ‘ ) and ğ‘”ğ‘› (Î”ğ‘ ) satisfy ğ‘“1 (Î”ğ‘ ) â‰¥ ğ›¾ğ‘ Î”ğ‘ , ğ‘”1 (Î”ğ‘ ) â‰¥ ğ›¾ğ‘ Î”ğ‘ and for every ğ‘› â‰¥ 2,

ğ‘“ğ‘› (Î”ğ‘ ) â‰¥ 2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1,
We can choose ğ‘“ğ‘› (Î”ğ‘ ) =

ğ‘”ğ‘› (Î”ğ‘ ) â‰¥ max(2ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘”ğ‘›âˆ’1 (Î”ğ‘ ), 3ğ›¾ğ‘ Î”ğ‘ + ğ›¾ğ‘ ğ‘“ğ‘›âˆ’1 (Î”ğ‘ )).
Ãğ‘›
ğ‘˜
ğ‘˜
ğ‘˜=1 2Î”ğ‘ ğ›¾ğ‘ and ğ‘”ğ‘› (Î”ğ‘ ) = ğ‘˜=1 3Î”ğ‘ ğ›¾ğ‘ , and we obtain for every ğ‘›

Ãğ‘›

ğ‘›
âˆ‘ï¸
ï£±
ï£´
ï£´
ï£´
ğ¶
(ğ‘ ,
ğ‘¥)
â‰¤
ğ‘¥
+
2Î”ğ‘ ğ›¾ğ‘ğ‘˜ , when ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ğ‘›
ï£´
ï£´
ï£²
ï£´
ğ‘˜=1
ğ‘›
âˆ‘ï¸
ï£´
ï£´
ï£´
ï£´
ğ¶ğ‘› (ğ‘ , ğ‘¥) â‰¤ ğ‘„ğ‘ (ğ‘ ) +
3Î”ğ‘ ğ›¾ğ‘ğ‘˜ , when ğ‘¥ < ğ‘„ğ‘ (ğ‘ )
ï£´
ï£´
ï£³
ğ‘˜=1

Subsequently, we obtain for the finite expected discounted cost
ğ‘›
ï£±
1
ğ‘¥
ğ‘¥ âˆ‘ï¸
ï£´
ï£´
ï£´
+
2Î”ğ‘ ğ›¾ğ‘ğ‘˜ âˆ’1 â‰¤
+ 2Î”ğ‘
, when ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ğ·
(ğ‘ ,
ğ‘¥)
â‰¤
ğ‘›âˆ’1
ï£´
ï£´
ğ›¾ğ‘
ğ›¾ğ‘
1 âˆ’ ğ›¾ğ‘
ï£²
ï£´
ğ‘˜=1

ğ‘›
ï£´
ï£´
ğ‘„ğ‘ (ğ‘ ) âˆ‘ï¸
1
ğ‘„ğ‘ (ğ‘ )
ï£´
ï£´
ğ·ğ‘›âˆ’1 (ğ‘ , ğ‘¥) â‰¤
+
3Î”ğ‘ ğ›¾ğ‘ğ‘˜ âˆ’1 â‰¤
+ 3Î”ğ‘
, when ğ‘¥ < ğ‘„ğ‘ (ğ‘ ).
ï£´
ï£´
ğ›¾
ğ›¾
1
âˆ’
ğ›¾ğ‘
ğ‘
ğ‘
ï£³
ğ‘˜=1
Since ğ·ğ‘› is Cauchy, we can just take the limit and obtain

2Î”ğ‘
ğ‘¥
ï£±
ï£´
ï£´
C(ğ‘ , ğ‘¥) â‰¤
+
, when ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ),
ï£´
ï£²
ï£´
ğ›¾ğ‘ 1 âˆ’ ğ›¾ğ‘
ï£´
ğ‘„ğ‘ (ğ‘ )
3Î”ğ‘
ï£´
ï£´
+
, when ğ‘¥ < ğ‘„ğ‘ (ğ‘ ).
ï£´ C(ğ‘ , ğ‘¥) â‰¤ ğ›¾
1 âˆ’ ğ›¾ğ‘
ï£³
ğ‘
â–¡
A.4.2 Proof of theorem 3. We recall Theorem 3.
Theorem 3. (Optimality of the shielded policies) Let M be a deterministic CMDP with safety threshold ğ‘‘ and initial state ğ‘ ğ‘– , ğ‘„ğ‘ be a
ğ‘“

Q-value, Î”ğ‘ = ||ğ‘„ğ‘ âˆ’ ğ‘„ğ‘âˆ— ||ğ¿âˆ ( S A ) . Then, if we let Î  the set of all policies of M, Î ğ‘ â„ be the set of shielded policies of M, and Î  be the set of
valued flipping policies, we have
max

ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘
2Î” ğ›¾

R (ğœ‹) â‰¤
ğ‘“

max

Â¯ â‰¤
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) (ğœ‹)

ğœ‹Â¯ âˆˆÎ(Î  ), ğ‘¥ 0 â‰¤ğ›¾ğ‘ ğ‘‘+E

ğ‘ ğ‘
for E = 1âˆ’ğ›¾
.
ğ‘
Before proving Theorem 3, we first need to introduce the following lemma.

max

ğœ‹ âˆˆÎ ğ‘ â„ , ğ‘¥ 0 â‰¤ğ›¾ğ‘ ğ‘‘+E

Â¯
R (ğ‘ ğ‘– ,ğ‘¥ 0 ) (ğœ‹),

ğ‘„â˜…

Lemma 2. For any deterministic MDP M and safety threshold ğ‘‘, there exists a ğ‘„ğ‘â˜…-shielded flipping memoryless policy ğœ‹ âˆ— of M ğ‘ such that
R ( ğœ‹Â¯ âˆ— ) =

max

R (ğœ‹),

ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘

and starting at initial state (ğ‘  0, ğ‘‘). Subsequently, ğœ‹Â¯ âˆ— also satisfies
C( ğœ‹Â¯ âˆ— ) â‰¤ ğ‘‘.
ğ‘šğ‘ğ‘¥
Proof. Let M = (ğ‘†, ğ´, ğ‘ ğ‘– , ğ‘ƒ, ğ‘…, ğ¶, ğ‘‘) be a deterministic CMDP with positive costs, and let ğ¶ğ‘šğ‘ğ‘¥ be equal to ğ‘1âˆ’ğ›¾
. For any state (ğ‘ , ğ‘¥) of M,
ğ‘

and any mapping ğ‘¦ : ğ´(ğ‘ ) â†¦â†’ [0; ğ¶ğ‘šğ‘ğ‘¥ ] such that ğ‘¦(ğ‘) â‰¥ ğ‘„ğ‘â˜… (ğ‘ , ğ‘), we let ğ· (ğ‘ , ğ‘¥, ğ‘¦) be the set of points ğ‘§ âˆˆ Rğ´(ğ‘  ) such that ğ‘¥ğ‘ â‰¥ 0 for all
ğ‘ âˆˆ ğ´(ğ‘ ),
âˆ‘ï¸
ğ‘¥
ğ‘§ğ‘ğ‘¦ (ğ‘) â‰¤ ,
(1)
ğ›¾ğ‘
ğ‘âˆˆğ´(ğ‘  )

and

âˆ‘ï¸

ğ‘§ğ‘ = 1.

(2)

ğ‘âˆˆğ´(ğ‘  )

For any ğ‘ , ğ‘¥, ğ‘¦, ğ· (ğ‘ , ğ‘¥, ğ‘¦) is the intersection between the convex polytope whose extreme points are all points ğ‘§ğ‘ âˆˆ Rğ´(ğ‘  ) such that for any
ğ‘, ğ‘ â€² âˆˆ ğ´(ğ‘ ), ğ‘§ğ‘ğ‘â€² = 1 if ğ‘ = ğ‘ â€² and 0 otherwise, and the (convex) half-space defined by Equation (1). Thus, for any ğ‘ , ğ‘¥, ğ‘¦, ğ· (ğ‘ , ğ‘¥, ğ‘¦) is a convex
â€²
polytope, and its extreme points are of the form ğœ†ğ‘§ğ‘ + (1 âˆ’ ğœ†)ğ‘§ğ‘ where ğ‘, ğ‘ â€² âˆˆ ğ´(ğ‘ ). Let ğ‘‰ (ğ‘ , ğ‘¥, ğ‘¦) be the extreme points of ğ· (ğ‘¥, ğ‘ , ğ‘¦). We let
ğ‘‰ğ‘â˜… (ğ‘ ) = minğ‘âˆˆğ´(ğ‘  ) ğ‘„ğ‘â˜… (ğ‘ , ğ‘), and we define MÌ‚ to be the MDP with



â€¢ States : ğ‘†Ë† = (ğ‘ , ğ‘¥) | ğ‘  âˆˆ ğ‘†, ğ‘¥ âˆˆ ğ‘‰ğ‘â˜… (ğ‘ ); ğ¶ max , with initial state ğ‘  ğ‘– = (ğ‘ ğ‘– , ğ‘‘);
Ë† ğ‘¥) equal to the set of all (ğ‘¦, ğ‘£) where ğ‘¦ : ğ´(ğ‘ ) â†¦â†’ [0; ğ¶ğ‘šğ‘ğ‘¥ ] and ğ‘¦ (ğ‘) â‰¥ ğ‘„ â˜… (ğ‘ , ğ‘) for any ğ‘ âˆˆ ğ´(ğ‘ ) and ğ‘£ âˆˆ ğ‘‰ (ğ‘ , ğ‘¥, ğ‘¦);
â€¢ Actions : ğ´(ğ‘ ,
ğ‘
â€¢ Rewards : ğ‘ŸË† ((ğ‘ , ğ‘¥), (ğ‘, ğ‘¦)) = ğ‘Ÿ (ğ‘ , ğ‘);
Ë† for ğ‘ Ë† = (ğ‘ , ğ‘¥), ğ‘ Ë†â€² = (ğ‘  â€², ğ‘¥ â€² ) in ğ‘†,
Ë† and ğ‘Ë† = (ğ‘¦, ğ‘£) âˆˆ ğ´(
Ë† ğ‘ ),
Ë† we have
â€¢ Transition Probability Function ğ‘ƒ:
(
â€²
â˜…
â˜…
â€²
0 if ğ‘¥ â‰  ğ‘¦ âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  ) for all ğ‘ âˆˆ ğ´(ğ‘ )
Ë† ğ‘£,
Ë† ğ‘ Ë†â€² ) = Ã
ğ‘ƒË† (ğ‘ ,
â€²
ğ‘ |ğ‘¥ â€² =ğ‘¦âˆ’ğ‘„ğ‘ (ğ‘ ,ğ‘)+ğ‘„ğ‘ (ğ‘  â€² ) ğ‘£ ğ‘ ğ‘ƒ (ğ‘ , ğ‘, ğ‘  ) otherwise .
First, notice that the MDP MÌ‚ satisfies the hypothesis of the lower-continuous model (Definition 8.7 of [8]), and thus admits an optimal
memoryless policy ğœ‹Ë† â˜… (Corollary 9.17.2 of [8]). Second, any memoryfull stochastic policy ğœ‹ of M such that C(ğœ‹) â‰¤ ğ‘‘ can be transformed
Ã
into a memoryfull policy ğœ‹Ë† of MÌ‚ with the same cost and reward by letting ğ‘ƒğœ‹Ë† (ğ‘ , ğ‘) = ğ‘£ âˆˆğ‘‰ (ğ‘ ,ğ‘¥,ğ‘¦) ğœ†ğ‘£ (ğ‘¦, ğ‘£), where
âˆ‘ï¸
ğ‘¦ (ğ‘) = Eğ‘ 0 ,ğ‘0 ,...âˆ¼ğœ‹,(ğ‘ 0 ,ğ‘0 )=(ğ‘ ,ğ‘)
ğ›¾ğ‘ğ‘¡ ğ‘ (ğ‘ ğ‘¡ , ğ‘ğ‘¡ ),
ğ‘¡

Ã
Ã
and where the ğœ†ğ‘£ are such that ğ‘£ âˆˆğ‘‰ (ğ‘ ,ğ‘¥,ğ‘¦) ğœ†ğ‘£ ğ‘£ = ğ‘ƒğœ‹ , ğœ†ğ‘£ âˆˆ [0; 1] and ğ‘£ âˆˆğ‘‰ (ğ‘ ,ğ‘¥,ğ‘¦) ğœ†ğ‘£ = 1. Conversely, similarly to Theorem 2, any memoryless
deterministic policy of MÌ‚ can be backwards projected to a memoryfull stochastic policy of M with the same cost and reward. Furthermore,
ğ‘„â˜…

Ë† â‰¤ ğ‘‘. The lemma
by definition of MÌ‚, any policy ğœ‹Ë† of can be seen as a ğ‘„ğ‘â˜…-shielded policy of M ğ‘ , and we thus have by Theorem 1 that C( ğœ‹)
follows.
â–¡
We now go on and provide a proof of Theorem 3.
ğ‘„âˆ—

Using Lemma 2, we have the existence of a ğ‘„ğ‘âˆ— policy ğœ‹Â¯ âˆ— of the Risk-Augmented MDP M ğµ , satisfying
ï£±
R (ğœ‹Â¯ âˆ— ) =
max
R (ğœ‹),
ï£´
ï£´
ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘
ï£´
ï£²
ï£´
ğ‘¥ 0 = ğ‘‘,
ï£´
ï£´
ï£´
ï£´ C( ğœ‹Â¯ âˆ— ) â‰¤ ğ‘‘.
ï£³

Moreover, ğœ‹Â¯ âˆ— is ğ‘„ğ‘âˆ— -shielded, valued, and flipping, so we can write for any (ğ‘ , ğ‘¥) âˆˆ S such that for ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ),
ğœ‹Â¯ âˆ— (ğ‘ , ğ‘¥) = ğ‘ƒ 1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ),
where the risks satisfy
(
ğ‘¥ = ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 ),
ğ‘¦1 â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ 1 ), ğ‘¦2 â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ 2 ).
Note that ğœ‹Â¯ âˆ— is not defined yet on the set {(ğ‘ , ğ‘¥), ğ‘¥ < ğ‘„ğ‘âˆ— (ğ‘ )}. We show now that it does not matter, since this set can not be reached from
the initial state.
Define ğ· = {(ğ‘ , ğ‘¥), ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ )}. Call S ğœ‹Â¯ âˆ— ,ğ‘› the set of states reached following ğœ‹Â¯ âˆ— in ğ‘› steps with a non-zero probability. We have,
S ğœ‹Â¯ âˆ— ,0 = {(ğ‘  0, ğ‘‘)} âŠ† ğ·. By induction, assume now that S ğœ‹Â¯ âˆ— ,ğ‘› âŠ† ğ·. For any (ğ‘ , ğ‘¥) âˆˆ ğ·,
ğœ‹Â¯ âˆ— (ğ‘ , ğ‘¥) = ğ‘ƒ 1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ),

ğ‘¦ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ).

Hence, the state ğœ‹Â¯ âˆ— can reach in one more step, using the definition of the augmented MDP, have risks satisfying
ğ‘¥ ğ‘— = ğ‘¦ ğ‘— âˆ’ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ) + ğ‘„ğ‘âˆ— (ğ‘  ğ‘— ) â‰¥ ğ‘„ğ‘âˆ— (ğ‘  ğ‘— ),
which means S ğœ‹Â¯ âˆ— ,ğ‘›+1 âŠ† ğ·.
Â¯ that will be ğ‘„ğ‘ -shielded, have a
The challenge here is that ğœ‹Â¯ âˆ— is not a ğ‘„ğ‘ -shielded policy, as ğ‘„ğ‘ â‰  ğ‘„ğ‘âˆ— . We will now construct a policy ğœ‹,
slightly increased cost, and yield the same reward.
Â¯
Definition of ğœ‹.
The initial state of ğœ‹Â¯ is (ğ‘  0, ğ‘‘ + E).
Ëœ we define
We first define ğœ‹Â¯ on the set ğ·Ëœ = {(ğ‘ , ğ‘¥), ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ) + E}. We show later that it is enough. For (ğ‘ , ğ‘¥) âˆˆ ğ·,
ğœ‹Â¯ (ğ‘ , ğ‘¥) = ğ‘ƒ1 (ğ‘ 1, ğ‘§ 1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘§ 2 ),

ğ‘§ ğ‘— = ğ‘¦ ğ‘— + E + ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘ (ğ‘ , ğ‘ ğ‘— ),

where ğ‘ƒ 1 , ğ‘ƒ 2 , ğ‘ 1 , ğ‘ 2 , ğ‘¦1 , ğ‘¦2 are defined using the image of the optimal policy ğœ‹Â¯âˆ— at (ğ‘ , ğ‘¥ âˆ’ E) as in
ğœ‹Â¯âˆ— (ğ‘ , ğ‘¥ âˆ’ E) = ğ‘ƒ1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ),
and ğ‘  1 (resp. ğ‘  2 ) is the state reached when taking ğ‘ 1 (resp. ğ‘ 2 ) in M, since M is assumed to be deterministic. Note that for ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ) + E, we
have ğ‘¥ âˆ’ E â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ), so ğœ‹Â¯âˆ— is well defined.
Ëœ Moreover, consider ğ‘† ğ‘›,ğœ‹Â¯ , the set of states reachable by ğœ‹Â¯ in ğ‘› steps. We already have
We first remark that the initial state (ğ‘  0, E) is in ğ·.
Ëœ Assume now that ğ‘† ğ‘›âˆ’1,ğœ‹Â¯ âŠ† ğ·.
Ëœ We consider a path ğœ = (ğ‘  0 . . . ğ‘ ğ‘› ) of length ğ‘›, and look at the last transition. Since ğ‘ ğ‘›âˆ’1 âˆˆ ğ·,
Ëœ it is of
ğ‘† 0,ğœ‹Â¯ âŠ† ğ·.
âˆ—
the form ğ‘ ğ‘›âˆ’1 = (ğ‘ , ğ‘¥) ğœ‹Â¯ with ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ) + E. ğœ‹Â¯ is defined as
ğœ‹Â¯ (ğ‘ , ğ‘¥) = ğ‘ƒ1 (ğ‘ 1, ğ‘§ 1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘§ 2 ),

ğ‘§ ğ‘— = ğ‘¦ ğ‘— + E + ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘ (ğ‘ , ğ‘ ğ‘— ),

where

ğœ‹Â¯âˆ— (ğ‘ , ğ‘¥ âˆ’ E) = ğ‘ƒ1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ).
Using the definition of the augmented MDP for the Q-value ğ‘„ğ‘ , the state reached after taking the action (ğ‘ ğ‘— , ğ‘§ ğ‘— ) is (ğ‘  ğ‘— , ğ‘¥ ğ‘— ) with
ğ‘¥ ğ‘— = ğ‘§ ğ‘— âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) + ğ‘„ğ‘ (ğ‘  ğ‘— ) = ğ‘¦ ğ‘— âˆ’ ğ‘ (ğ‘ , ğ‘ ğ‘— ) + E.
Since ğ‘¦ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ) = ğ‘ ğ‘— + ğ‘„ğ‘âˆ— (ğ‘  ğ‘— ) as ğœ‹Â¯âˆ— is ğ‘„ğ‘âˆ— -shielded, we have
ğ‘¥ ğ‘— â‰¥ ğ‘„ğ‘ (ğ‘  ğ‘— ) + E,
Ëœ
so ğ‘ ğ‘› âˆˆ ğ·.
ğœ‹Â¯ is ğ‘„ğ‘ -shielded.
Now, we check if the policy is ğ‘„ğ‘ -shielded. Using the definition in the case where ğ‘¥ â‰¥ ğ‘„ğ‘ (ğ‘ ), ğœ‹Â¯ is ğ‘„ğ‘ -shielded if and only if
ğ‘¥ â‰¥ ğ›¾ğ‘ (ğ‘ƒ1ğ‘§ 1 + ğ‘ƒ 2ğ‘§ 2 ),

ğ‘§ ğ‘— = ğ‘¦ ğ‘— + E + ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘ (ğ‘ , ğ‘ ğ‘— ).

We use the fact that ğœ‹Â¯ âˆ— is ğ‘„ğ‘âˆ— shielded, so
ğ‘¥ âˆ’ E â‰¥ ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 ).
Plugging this in (3) gives
ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 ) + ğ›¾ğ‘ (ğ‘ƒ1 (âˆ’ğ‘„ğ‘ (ğ‘  1 ) + ğ‘„ğ‘ (ğ‘ , ğ‘ 1 ) âˆ’ ğ‘ (ğ‘ , ğ‘ 1 ))
+ ğ‘ƒ 2 (âˆ’ğ‘„ğ‘ (ğ‘  2 ) + ğ‘„ğ‘ (ğ‘ , ğ‘ 2 ) âˆ’ ğ‘ (ğ‘ , ğ‘ 2 ))) + ğ›¾ğ‘ E
â‰¤ ğ‘¥ âˆ’ E + ğ›¾ğ‘ (ğ‘ƒ 1 (âˆ’ğ‘„ğ‘ (ğ‘  1 ) + ğ‘„ğ‘ (ğ‘ , ğ‘ 1 ) âˆ’ ğ‘ (ğ‘ , ğ‘ 1 ))
+ ğ‘ƒ 2 (âˆ’ğ‘„ğ‘ (ğ‘  2 ) + ğ‘„ğ‘ (ğ‘ , ğ‘ 2 ) âˆ’ ğ‘ (ğ‘ , ğ‘ 2 ))) + ğ›¾ğ‘ E
â‰¤ ğ‘¥ + (ğ›¾ğ‘ âˆ’ 1)E + 2ğ›¾ğ‘ Î”ğ‘ .
Here, we used that

ï£±
ğ‘„ âˆ— (ğ‘ , ğ‘ ğ‘— ) = ğ‘„ğ‘ (ğ‘  ğ‘— ) + ğ‘ (ğ‘ , ğ‘ ğ‘— ),
ï£´
ï£´
ï£² ğ‘
ï£´
|ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— )| â‰¤ Î”ğ‘ ,
ï£´
ï£´
ï£´ |ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘„ âˆ— (ğ‘  ğ‘— )| â‰¤ Î”ğ‘ ,
ğ‘
ï£³

to obtain that | âˆ’ ğ‘„ğ‘ (ğ‘  2 ) + ğ‘„ğ‘ (ğ‘ , ğ‘ 2 ) âˆ’ ğ‘ (ğ‘ , ğ‘ 2 ))| â‰¤ 2Î”ğ‘ .
Hence, ğœ‹Â¯ is ğ‘„ğ‘ -shielded when

2ğ›¾ğ‘ Î”ğ‘ â‰¤ (1 âˆ’ ğ›¾ğ‘ )E,

2Î” ğ›¾

ğ‘ ğ‘
.
so when E â‰¥ 1âˆ’ğ›¾
ğ‘

We also need to show that for everytime the policy ğœ‹Â¯ takes an action (ğ‘, ğ‘§), then ğ‘§ â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘). We consider (ğ‘ , ğ‘¥) âˆˆ ğ·Ëœ and denote
ğœ‹Â¯ (ğ‘ , ğ‘¥) = ğ‘ƒ1 (ğ‘ 1, ğ‘§ 1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘§ 2 ).

Since
ğ‘§ ğ‘— = ğ‘¦ ğ‘— + ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) + ğ‘ (ğ‘ , ğ‘ ğ‘— ) + E,

(3)

and
ğ‘¦ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— )
since ğœ‹Â¯âˆ— is ğ‘„ğ‘âˆ— -shielded, we get
ğ‘§ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ) âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) + ğ‘„ğ‘ (ğ‘  ğ‘— ) + ğ‘ (ğ‘ , ğ‘ ğ‘— ) + E â‰¥ ğ‘„ğ‘ (ğ‘  ğ‘— ) + ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ Î”ğ‘ + E.
Since ğ‘„ğ‘âˆ— (ğ‘  ğ‘— ) + ğ‘ (ğ‘ , ğ‘ ğ‘— ) = ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ) and |ğ‘„ğ‘ (ğ‘  ğ‘— ) âˆ’ ğ‘„ğ‘âˆ— (ğ‘  ğ‘— )| â‰¤ Î”ğ‘ , we finally obtain
ğ‘§ ğ‘— â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) âˆ’ 2Î”ğ‘ + E.
2ğ›¾

This is satisfied as long as 1âˆ’ğ›¾ğ‘ğ‘ â‰¥ 2.
The rewards of ğœ‹Â¯ and ğœ‹Â¯âˆ— are equal.
We now define M1 and M2 the two Markov Chains as
ğ‘„âˆ—
ğ‘„âˆ—
ï£±
ï£´
ï£² M1 = M ğœ‹Â¯âˆ—ğ‘ , the Markov Chained induced by ğœ‹Â¯âˆ— on M ğ‘ ,
ï£´
ï£´
ï£´ M2 = M ğ‘„ğ‘ , the Markov Chained induced by ğœ‹Â¯ on M ğ‘„ğ‘ .
ğœ‹Â¯
ï£³
ğ‘„âˆ—

ğ‘„ğ‘

Since M is deterministic, M ğ‘ and M are also deterministic. M1 and M2 however are not deterministic, since the policies ğœ‹Â¯ âˆ— and ğœ‹Â¯
are flipping.
The Markov Chain M1 : Has one initial state, (ğ‘¥ 0, ğ‘‘). For any (ğ‘ , ğ‘¥) reachable by ğœ‹Â¯âˆ— , ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ) so ğœ‹Â¯âˆ— is defined as
ğœ‹Â¯âˆ— (ğ‘ , ğ‘¥) = ğ‘ƒ 1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ),
for some ğ‘ƒ 1 , ğ‘ƒ 2 , ğ‘ 1 , ğ‘ 2 , ğ‘¦1 and ğ‘¦2 . Call ğ‘  1 (resp. ğ‘  2 ) the state reached when taking ğ‘ 1 (resp. ğ‘ 2 ) in M from ğ‘ . Then,
ğ‘ƒğœ‹Â¯âˆ— ((ğ‘  ğ‘— , ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— )|(ğ‘ , ğ‘¥)) = ğ‘ƒ ğ‘— , ğ‘ ğ‘— = ğ‘ (ğ‘ , ğ‘ ğ‘— ),
Â¯ ğ‘¥)) = 0 for any other ğ‘§.
Â¯ Hence, the Markov Chain M1 has two transitions starting from (ğ‘ , ğ‘¥):
and ğ‘ƒğœ‹Â¯âˆ— (ğ‘§|(ğ‘ ,
â€¢ (ğ‘ , ğ‘¥) â†’ (ğ‘  1, ğ‘¦1 âˆ’ ğ‘ 1 ) with probability ğ‘ƒ1 ,
â€¢ (ğ‘ , ğ‘¥) â†’ (ğ‘  2, ğ‘¦2 âˆ’ ğ‘ 2 ) with probability ğ‘ƒ2 .
The Markov Chain M2 : Has one initial state, (ğ‘¥ 0, ğ‘‘ + ğ·). For any (ğ‘ , ğ‘¥) reachable by ğœ‹Â¯âˆ— , ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ) so ğœ‹Â¯âˆ— is defined as
ğœ‹Â¯ (ğ‘ , ğ‘¥ + ğ·) = ğ‘ƒ1 (ğ‘ 1, ğ‘§ 1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘§ 2 ), ğ‘§ ğ‘— = ğ‘¦ ğ‘— âˆ’ ğ‘„ğ‘ (ğ‘  ğ‘— ) + ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) + ğ·,
where ğ‘ƒ 1 , ğ‘ƒ 2 , ğ‘ 1 , ğ‘ 2 , ğ‘¦1 , ğ‘¦2 are defined with ğœ‹Â¯âˆ— as
ğœ‹Â¯âˆ— (ğ‘ , ğ‘¥) = ğ‘ƒ 1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ).
and where ğ‘  1 (resp. ğ‘  2 ) is the state reached when taking ğ‘ 1 (resp. ğ‘ 2 ) in M from ğ‘ . Then,
ğ‘ƒğœ‹Â¯ ((ğ‘  ğ‘— , ğ‘¦ ğ‘— âˆ’ ğ‘ ğ‘— + ğ·)|(ğ‘ , ğ‘¥)) = ğ‘ƒ ğ‘— , ğ‘ ğ‘— = ğ‘ (ğ‘ , ğ‘ ğ‘— ),
Â¯ ğ‘¥)) = 0 for any other ğ‘§.
Â¯ Hence, the Markov Chain M2 has two transitions starting from (ğ‘ , ğ‘¥):
and ğ‘ƒğœ‹Â¯ (ğ‘§|(ğ‘ ,
â€¢ (ğ‘ , ğ‘¥ + ğ·) â†’ (ğ‘  1, ğ‘¦1 âˆ’ ğ‘ 1 + ğ·) with probability ğ‘ƒ1 ,
â€¢ (ğ‘ , ğ‘¥ + ğ·) â†’ (ğ‘  2, ğ‘¦2 âˆ’ ğ‘ 2 + ğ·) with probability ğ‘ƒ2 .
ğ‘„âˆ—

With ğœ™ : S ğ‘ â†’ S

ğ‘„ğ‘

, defined as
ğœ™ (ğ‘ , ğ‘¥) = (ğ‘ , ğ‘¥ + ğ·).

We have that
ï£±
ğœ™ (ğ‘  0, ğ‘‘) = (ğ‘  0, ğ‘‘ + ğ·),
ï£´
ï£´
ï£²
ï£´
ğ‘ƒ M1 ((ğ‘  â€², ğ‘¥ â€² )|(ğ‘ , ğ‘¥)) = ğ‘ƒ M2 (ğœ™ (ğ‘  â€², ğ‘¥ â€² )|ğœ™ (ğ‘ , ğ‘¥)),
ï£´
ï£´
ï£´ğ‘Ÿ M ((ğ‘ , ğ‘¥) â†’ (ğ‘  â€², ğ‘¥ â€² )) = ğ‘Ÿ M (ğœ™ (ğ‘ , ğ‘¥) â†’ ğœ™ (ğ‘  â€², ğ‘¥ â€² )).
2
ï£³ 1
Hence, we have for the expected discounted reward, as it is defined as an expectation, that
Â¯ = R ( ğœ‹Â¯âˆ— ).
R (ğœ‹)

A.4.3

Proof of Remark 2: better safety bound in the deterministic case.

Remark 2. In the deterministic case, one can choose instead, in the definition of the Risk-Augmented MDP, to take the risk ğ‘¥ğ‘– = ğ‘¦ğ‘– âˆ’ ğ‘ (ğ‘ , ğ‘)
instead of the risk ğ‘¥ğ‘– = ğ‘¦ğ‘– âˆ’ ğ‘„ğ‘ (ğ‘ , ğ‘) + ğ‘„ğ‘ (ğ‘  â€² ) after taking the action (ğ‘, ğ‘¦ğ‘– ) and reaching ğ‘  â€² . This is what we used in practice in that case, as the
performances are slightly better. In that case, the optimality bound is improved, and we have
max

Rğ‘ 0 (ğœ‹) â‰¤

max

ğœ‹Â¯ âˆˆÎ , ğ‘¥ 0 â‰¤ğ‘‘+2Î”ğ‘

ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘

Â¯
R (ğ‘ 0 ,ğ‘¥ 0 ) (ğœ‹).

The safety results are identical.
Proof. (Of Remark 2)
The proof is very similar to the proof of 3. Using Lemma 2 again, we have the existence of a ğ‘„ğ‘âˆ— policy ğœ‹Â¯ âˆ— of the Risk-Augmented MDP
ğ‘„âˆ—

M ğµ , satisfying
ï£±
R (ğœ‹Â¯ âˆ— ) =
max
R (ğœ‹),
ï£´
ï£´
ğœ‹ âˆˆÎ , C (ğœ‹ ) â‰¤ğ‘‘
ï£´
ï£´
ï£²
ğ‘¥ 0 = ğ‘‘,
ï£´
ï£´
ï£´
ï£´ C( ğœ‹Â¯ âˆ— ) â‰¤ ğ‘‘.
ï£³
Moreover, ğœ‹Â¯ âˆ— is ğ‘„ğ‘âˆ— -shielded, valued, and flipping, so we can write for any (ğ‘ , ğ‘¥) âˆˆ S such that for ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— (ğ‘ ),
ğœ‹Â¯ âˆ— (ğ‘ , ğ‘¥) = ğ‘ƒ 1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ),
where the risks satisfy
(
ğ‘¥ = ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 ),
ğ‘¦1 â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ 1 ), ğ‘¦2 â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ 2 ).
Â¯
Definition of ğœ‹.
ğ‘„ğ‘
We define ğ·Ëœ = {(ğ‘ , ğ‘¥), ğ‘¥ â‰¥ ğ‘„ğ‘âˆ— + 2Î”ğ‘ }, and define the policy ğœ‹Â¯ on the augmented MDP M as follows.
â€¢ The starting state of ğœ‹Â¯ is (ğ‘  0, ğ‘‘ + 2Î”ğ‘ ).
Ëœ
â€¢ For every (ğ‘ , ğ‘¥) âˆˆ ğ‘† âˆ© ğ·,
ğœ‹Â¯ (ğ‘ , ğ‘¥) = ğ‘ƒ1 (ğ‘ 1, ğ‘§ 1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘§ 2 ), ğ‘§ ğ‘— = ğ‘¦ ğ‘— + 2Î”ğ‘ ,
where ğ‘ƒ 1 , ğ‘ƒ 2 , ğ‘ 1 , ğ‘ 2 , ğ‘¦1 , and ğ‘¦2 are defined via the image of the optimal policy ğœ‹Â¯âˆ— as
ğœ‹Â¯âˆ— (ğ‘ , ğ‘¥ âˆ’ 2Î”ğ‘ ) = ğ‘ƒ1 (ğ‘ 1, ğ‘¦1 ) + ğ‘ƒ 2 (ğ‘ 2, ğ‘¦2 ).
Ëœ This means that ğœ‹Â¯ is well defined
First, Note that again, by a quick induction, all the states reachable from the initial state belong to ğ·.
Ëœ
despite having been only defined on ğ·.
ğœ‹Â¯ is ğ‘„ğ‘ -shielded.
Ëœ ğ‘¥ â‰¥ ğ‘„ âˆ— (ğ‘ ) + 2Î”ğ‘ â‰¥ ğ‘„ğ‘ (ğ‘ ). So the shielding conditions become
We verify now that ğœ‹Â¯ is ğ‘„ğ‘ -shielded. For every (ğ‘ , ğ‘¥) âˆˆ ğ·,
ğ‘
(1) For every (ğ‘ ğ‘— , ğ‘§ ğ‘— ) that the policy takes, we have ğ‘§ ğ‘— â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ).
(2) The weights satisfy
ğ‘¥ â‰¥ ğ›¾ğ‘ (ğ‘ƒ1ğ‘§ 1 + ğ‘ƒ 2ğ‘§ 2 ).
For the first one, the optimal policy ğœ‹âˆ— is ğ‘„ğ‘âˆ— shielded, so that we have
ğ‘¦ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ).
Hence,
ğ‘§ ğ‘— â‰¥ ğ‘„ğ‘âˆ— (ğ‘ , ğ‘ ğ‘— ) + 2Î”ğ‘ â‰¥ ğ‘„ğ‘ (ğ‘ , ğ‘ ğ‘— ) + Î”ğ‘ .
For the second one, we have, again because ğœ‹Â¯âˆ— is ğ‘„ğ‘âˆ— -shielded
ğ‘¥ âˆ’ 2Î”ğ‘ â‰¥ ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 ),
so
ğ›¾ğ‘ (ğ‘ƒ1ğ‘§ 1 + ğ‘ƒ 2ğ‘§ 2 ) = ğ›¾ğ‘ (ğ‘ƒ1ğ‘¦1 + ğ‘ƒ 2ğ‘¦2 + 2Î”ğ‘ ) â‰¤ (ğ‘¥ âˆ’ 2Î”ğ‘ ) + 2ğ›¾ğ‘ Î”ğ‘ â‰¤ ğ‘¥ .
So ğœ‹Â¯ is ğ‘„ğ‘ -shielded. Again, we prove that the rewards are identical by showing that the Markov Chain are identical, the details are
identitcal to the proof of theorem 3.
â–¡

A.4.4 Proof of Theorem 5: Safety preserved with additional noise. Theorem 5.(Safety of the shielding with noise) We consider a MDP M and
any two policies ğœ‹Â¯1 and ğœ‹Â¯2 , and ğœ‰ âˆˆ [0, 1] a small number. Then, with ğœ‹Â¯ the policy defined as
ğœ‹Â¯ (ğ‘ ) = (1 âˆ’ ğœ‰) ğœ‹Â¯1 (ğ‘ ) + ğœ‰ ğœ‹Â¯2 (ğ‘ )
for all ğ‘  âˆˆ S, the discounted cost of the policy ğœ‹Â¯ satisfies
Â¯ â‰¤ ğ¶ (ğœ‹Â¯1 ) +
ğ¶ (ğœ‹)

1
ğœ‰ğ‘ max
.
1 âˆ’ ğ›¾ğ‘ 1 âˆ’ (1 âˆ’ ğœ‰)ğ›¾ğ‘

Proof. We let ğ¶ğ‘› = ğ›¾ğ‘ ğ·ğ‘›âˆ’1 , where ğ·ğ‘› is the discounted cost of the ğ‘› first steps. We will use an induction to show
ğ‘›
ğ›¾ğ‘ max âˆ‘ï¸
Â¯ ğ‘ ) â‰¤ ğ¶ğ‘› ( ğœ‹Â¯1, ğ‘ ) + ğœ‰
ğ¶ğ‘› (ğœ‹,
(1 âˆ’ ğœ‰)ğ‘˜ ğ›¾ğ‘ğ‘˜ .
1 âˆ’ ğ›¾ğ‘
ğ‘˜=1

For ğ‘› = 1, we have

Â¯ ğ‘ ) =(1 âˆ’ ğœ‰)
ğ¶ 1 ( ğœ‹,

âˆ‘ï¸

Â¯ ğ‘  â€² ))
ğ‘ƒğœ‹Â¯1 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ 0 ( ğœ‹,

ğ‘âˆˆğ´(ğ‘  )

+ğœ‰

âˆ‘ï¸

Â¯ ğ‘  â€² )) â‰¤ (1 âˆ’ ğœ‰)ğ›¾ğ‘ ğ‘ max .
ğ‘ƒğœ‹Â¯2 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ 0 ( ğœ‹,

ğ‘âˆˆğ´(ğ‘  )

It is in particular smaller than the induction formula for ğ‘› = 1.
Now, assuming the property holds for ğ‘˜ â‰¤ ğ‘› âˆ’ 1. We write
âˆ‘ï¸
Â¯ ğ‘ ) =(1 âˆ’ ğœ‰)
Â¯ ğ‘  â€² ))
ğ¶ğ‘› ( ğœ‹,
ğ‘ƒğœ‹Â¯1 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 ( ğœ‹,
ğ‘âˆˆğ´(ğ‘  )

+ğœ‰

âˆ‘ï¸

Â¯ ğ‘  â€² )),
ğ‘ƒğœ‹Â¯2 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 ( ğœ‹,

ğ‘âˆˆğ´(ğ‘  )

so
ğ¶ğ‘› â‰¤(1 âˆ’ ğœ‰)

âˆ‘ï¸

Â¯ ğ‘  â€² )) + ğœ‰ğ›¾ğ‘
ğ‘ƒğœ‹Â¯1 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 (ğœ‹,

ğ‘âˆˆğ´(ğ‘  )

ğ‘ max
1 âˆ’ ğ›¾ğ‘
ğ‘›

â‰¤(1 âˆ’ ğœ‰)

âˆ‘ï¸

â€²

ğ‘ƒğœ‹Â¯1 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  |ğ‘)ğ›¾ğ‘

ğ‘âˆˆğ´(ğ‘  )

Now, we remark that

ğ‘ max
,
+ ğœ‰ğ›¾ğ‘
1 âˆ’ ğ›¾ğ‘
âˆ‘ï¸

ğ›¾ğ‘ max âˆ‘ï¸
ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 ( ğœ‹Â¯1, ğ‘  ) + ğœ‰
(1 âˆ’ ğœ‰)ğ‘˜ ğ›¾ğ‘ğ‘˜
1 âˆ’ ğ›¾ğ‘

!

â€²

ğ‘˜=1

ğ‘ƒğœ‹Â¯1 (ğ‘|ğ‘ )ğ‘ƒ (ğ‘  â€² |ğ‘)ğ›¾ğ‘ (ğ‘ (ğ‘ , ğ‘) + ğ¶ğ‘›âˆ’1 ( ğœ‹Â¯1, ğ‘  â€² )) = ğ¶ğ‘› ( ğœ‹Â¯1, ğ‘ ),

ğ‘âˆˆğ´(ğ‘  )

so the inequality becomes
ğ‘›âˆ’1

Â¯ ğ‘ ) â‰¤(1 âˆ’ ğœ‰)ğ¶ğ‘› (ğœ‹Â¯1, ğ‘ ) + (1 âˆ’ ğœ‰)ğœ‰ğ›¾ğ‘
ğ¶ğ‘› (ğœ‹,

ğ›¾ğ‘ ğ‘ max âˆ‘ï¸
ğ‘ max
(1 âˆ’ ğœ‰)ğ‘˜ ğ›¾ğ‘ğ‘˜ + ğœ‰ğ›¾ğ‘
1 âˆ’ ğ›¾ğ‘
1 âˆ’ ğ›¾ğ‘
ğ‘˜=1

â‰¤ğ¶ğ‘› (ğœ‹Â¯1, ğ‘ ) +

ğ‘›
ğœ‰ğ›¾ğ‘ ğ‘ max âˆ‘ï¸

1 âˆ’ ğ›¾ğ‘

(1 âˆ’ ğœ‰)ğ‘˜ ğ›¾ğ‘ğ‘˜ ,

ğ‘˜=1

which concludes the induction and the proof.
â–¡

