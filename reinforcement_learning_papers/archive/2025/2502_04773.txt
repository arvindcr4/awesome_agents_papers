arXiv:2502.04773v2 [cs.LG] 3 Jul 2025

An Extended Benchmarking of Multi-Agent Reinforcement
Learning Algorithms in Complex Fully Cooperative Tasks
George Papadopoulosâˆ—

Andreas Kontogiannisâˆ—

Foteini Papadopoulou

University of Piraeus
Piraeus, Greece
georgepap@unipi.gr

NTUA & Archimedes AI
Athens, Greece
andreaskontogiannis@mail.ntua.gr

Radboud University
Nijmegen, Netherlands
foteini.papadopoulou@ru.nl

Chaido Poulianou

Ioannis Koumentis

George Vouros

University of Piraeus
Piraeus, Greece
cpoulianou@uth.gr

University of Piraeus
Piraeus, Greece
iokoumen@unipi.gr

University of Piraeus
Piraeus, Greece
georgev@unipi.gr

ABSTRACT
Multi-Agent Reinforcement Learning (MARL) has recently emerged
as a significant area of research. However, MARL evaluation often
lacks systematic diversity, hindering a comprehensive understanding of algorithmsâ€™ capabilities. In particular, cooperative MARL
algorithms are predominantly evaluated on benchmarks such as
SMAC and GRF, which primarily feature team game scenarios without assessing adequately various aspects of agentsâ€™ capabilities
required in fully cooperative real-world tasks such as multi-robot
cooperation and warehouse, resource management, search and rescue, and human-AI cooperation. Moreover, MARL algorithms are
mainly evaluated on low dimensional state spaces, and thus their
performance on high-dimensional (e.g., image) observations is not
well-studied. To fill this gap, this paper highlights the crucial need
for expanding systematic evaluation across a wider array of existing benchmarks. To this end, we conduct extensive evaluation and
comparisons of well-known MARL algorithms on complex fully
cooperative benchmarks, including tasks with images as agentsâ€™
observations. Interestingly, our analysis shows that many algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform standard MARL baselines on fully cooperative benchmarks.
Finally, towards more systematic and better evaluation of cooperative MARL algorithms, we have open-sourced PyMARLzoo+, an
extension of the widely used (E)PyMARL libraries, which addresses
an open challenge from [49], facilitating seamless integration and
support with all benchmarks of PettingZoo, as well as Overcooked,
PressurePlate, Capture Target and Box Pushing.

KEYWORDS
Fully Cooperative Multi-Agent Reinforcement Learning, Benchmarking, Image-based Observations, Open-Source Framework
ACM Reference Format:
George Papadopoulos, Andreas Kontogiannis, Foteini Papadopoulou, Chaido
Poulianou, Ioannis Koumentis, and George Vouros. 2025. An Extended
âˆ— Equal Contribution.

This work is licensed under a Creative Commons Attribution International 4.0 License.
Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2025), Y. Vorobeychik, S. Das, A. NowÃ© (eds.), May 19 â€“ 23, 2025, Detroit, Michigan,
USA. Â© 2025 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org).

Benchmarking of Multi-Agent Reinforcement Learning Algorithms in Complex Fully Cooperative Tasks. In Proc. of the 24th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS 2025), Detroit, Michigan, USA, May 19 â€“ 23, 2025, IFAAMAS, 31 pages.

1

INTRODUCTION

In fully cooperative Multi-Agent Reinforcement Learning (MARL)
problems, the goal is to train learnable agents in order to maximize
their shared cumulative reward, through excessive coordination,
sharing of tasks, collaborative exploration, with appropriate decisions for timing and action, and sharing of capabilities. Fully
cooperative MARL is of remarkable interest, as it can naturally
model many real-world applications, including multi-robot collaboration [6] and warehouse [36], search and rescue [39], human-AI coordination [7], air traffic management [25], logistics networks [29],
and supply-chain optimization [23]. Recently, cooperative MARL
algorithms are mainly evaluated in settings where adversarial nonlearnable agents that interact with the learnable cooperative ones
exist: This has received a surge of approaches and methodologies,
e.g., see [21, 31, 48, 61], also drawing motivation from multiplayer
video-games and team sports.
Despite recent efforts [4, 20, 36, 63] aiming to provide a comprehensive understanding of standard cooperative MARL algorithmsâ€™
capabilities through benchmarking, MARL evaluation still lacks
systematic diversity and reliability for the following reasons:
â€¢ Most state-of-the-art (SoTA) cooperative MARL algorithms
are predominantly evaluated, and possibly overfit, as pointed
out in [16], on specific cooperative-competitive benchmarks
where a team of cooperative learning agents competes against
a team of bots with fixed policies, namely SMAC [11, 43]
and GRF [27]. However, we argue that these benchmarks do
not allow adequate evaluation of subtle issues involved in
fully cooperative MARL, including: excessive coordination
and exploration capabilities, sharing of capabilities, appropriate timing in the execution of actions, complimentary
observability, scaling to large numbers of agents, and possibly with sparse rewards. For instance, tasks from the LBF
benchmark [36],and particularly those with large grids, effectively capture more diverse requirements of fully cooperative multi-robot collaboration: They require agents to
first engage in extensive joint exploration to identify a certain
food target, followed by coordinated joint actions where all

agents must simultaneously consume that target. We conjecture that these aspects are crucial for fully cooperative,
real-world tasks. On the other hand, these benchmarks emphasize developing skills that do not play dominant roles
for fully cooperative tasks, such as countering the opponent
team (e.g., surviving enemy attacks in SMAC or tackling
opponents and preventing goals in GRF).
â€¢ Most MARL algorithms are evaluated solely on tasks with
low-dimensional (mostly tabular) state spaces, and thus their
effectiveness on real-world, high-dimensional, image-based
observations has not been studied.
â€¢ MARL evaluation does not often report the training times of
the proposed algorithms, so the results cannot be interpreted
as a function of the compute budget used [16].
To address the above challenges, the main contributions of this
paper are the following: (1) Our paper investigates the effectiveness
of established MARL algorithms and contributes a comprehensive,
updated empirical evaluation and comparison of MARL algorithms, including algorithms that have demonstrated SoTA performance in SMAC and GRF, across a wide array of complex fully
cooperative benchmarks. (2) To our knowledge, our work is the
first to include tasks with images representing high-dimensional
observations in MARL benchmarking. (3) We contribute an opensource Python MARL framework, namely PyMARLzoo+ 1 , which
extends the (E)PyMARL frameworks [36, 43] (widely used in developing established MARL algorithms, such as [21, 28, 40, 53, 54]),
facilitating seamless integration with all PettingZoo tasks, thus
addressing an open challenge from [49] (EPyMARL supports only
the MPE PettingZoo tasks, which were already integrated in [36]).
In addition to the already integrated MPE [32, 33], LBF [3] and
RWARE [36] benchmarks, our PyMARLzoo+ also integrates the
complex fully cooperative Overcooked [7], PressurePlate [1, 17],
Capture Target [35, 57] and BoxPushing [58, 59] benchmarks. (4)
Our work provides benchmarking of tasks that are indicative of
a wide array of real-world applications, and of the evaluation of
diverse requirements for fully cooperative MARL, in terms of joint
exploration and coordination. (5) Our experimental findings demonstrate that many algorithms, which have been SoTA in SMAC and
GRF, may underperform standard MARL algorithms in fully cooperative MARL tasks; thus validating the possible overfitting issues
of the current MARL evaluation highlighted in [16]. Furthermore,
we point out fully cooperative tasks that are very hard to solve
with the existing SoTA methods. (6) Our benchmarking is the first
to report the training times of algorithms as a further measure of
the algorithmâ€™s performance, so that the reported results are also
interpreted as a function of the compute budget used.

2 PRELIMINARIES
2.1 Dec-POMDPs
Fully cooperative MARL is formulated as a Dec-POMDP [34]. A DecPOMDP for an ğ‘ -agent task is a tuple âŸ¨ğ‘†, ğ´, ğ‘ƒ, ğ‘Ÿ, ğ¹, ğ‘‚, ğ‘ , ğ›¾âŸ©, where ğ‘†
is the state space, ğ´ is the joint action space ğ´ = ğ´1 Ã—Â· Â· Â·Ã—ğ´ğ‘ , ğ‘ƒ (ğ‘  â€² |
ğ‘ , ğ‘) : ğ‘† Ã—ğ´ â†’ [0, 1] is the state transition function, ğ‘Ÿ (ğ‘ , ğ‘) : ğ‘† Ã—ğ´ â†’
R is the shared reward function and ğ›¾ âˆˆ [0, 1) is the discount factor.
1 The source code is available at: https://github.com/AILabDsUnipi/pymarlzooplus

Assuming partial observability, each agent at time step ğ‘¡ does not
have access to the full state, yet it samples observations ğ‘œğ‘¡ğ‘– âˆˆ ğ‘‚ğ‘–
according to the observation function ğ¹ğ‘– (ğ‘ ) : ğ‘† â†’ ğ‘‚ğ‘– . The joint
observations are denoted by ğ‘œ âˆˆ ğ‘‚ and are sampled according to
Ã
ğ¹ = ğ‘– ğ¹ğ‘– . The action-observation history for agent ğ‘– at time ğ‘¡ is
denoted by â„ğ‘–ğ‘¡ âˆˆ ğ»ğ‘– , which includes action-observation pairs until ğ‘¡1 and ğ‘œğ‘¡ğ‘– , on which the agent can condition its individual stochastic
policy ğœ‹ğœƒğ‘– (ğ‘ğ‘–ğ‘¡ | â„ğ‘–ğ‘¡ ) : ğ»ğ‘– Ã— ğ´ğ‘– â†’ [0, 1], parameterised by ğœƒğ‘– . The
ğ‘–
joint policy is denoted by ğœ‹ğœƒ , with parameters ğœƒ âˆˆ Î˜ . The objective
is to find an optimal joint policy which satisfies the optimal
value
Ãâˆ ğ‘¡ 
function ğ‘‰ âˆ— (ğ‘ ) = maxğœƒ Eğ‘âˆ¼ğœ‹ğœƒ ,ğ‘  â€² âˆ¼ğ‘ƒ (Â· |ğ‘ ,ğ‘),ğ‘œâˆ¼ğ¹ (ğ‘  )
ğ›¾
ğ‘Ÿ
.
ğ‘¡
ğ‘¡ =0

2.2

Assumptions of interest

In addition to partial observability, in our benchmark analysis we
consider and study the following assumptions of interest: (a) we
utilize the celebrated CTDE MARL schema [32], which has been
widely adopted by the cooperative MARL community [2, 16] as it
enables conditioning approximate value functions on privileged
information in a computationally tractable manner, (b) we evaluate
fully cooperative tasks, that is, tasks where there are no adversarial
non-learnable agents (e.g., bots), or teams of opponent agents, interacting with the learnable agents, (c) we also evaluate complex
tasks with sparse-reward settings that require excessive joint exploration and cooperation to be solved, (d) we also study tasks with
image-based, high-dimensional state and observation spaces (as in
real-world tasks), which have been very frequent in single-agent
RL but not in MARL evaluation, and (e) as in [36, 63] we assume
that agents lack explicit communication channels during execution.

3

ALGORITHMS

In our benchmark analysis, we evaluate and compare a wide range
of well-known MARL algorithms (see Table 1). The selection of
these algorithms is based on the following important (but not allencompassing) criteria: (1) they have been widely used as competitive baselines by the MARL community in recent works, (2)
they have achieved SoTA performance in cooperative-competitive
MARL benchmarks, such as SMAC and GRF, and in our analysis
we aim to test their performance in fully cooperative tasks, (3) they
are based on improving joint exploration in sparse-reward tasks,
and (4) they present diversity in the RL optimization part (e.g., being value-based or actor-critic based, utilizing standard recurrent
architectures or transformer-based).

4

FULLY COOPERATIVE MARL BENCHMARKS

In this section, we highlight complex, partial observable, fully cooperative tasks from eight MARL benchmarks that we believe to be
of significant interest for MARL research. These tasks represent a
wide range of real-world applications and test diverse requirements
for fully cooperative MARL (see also Table 2).

4.1

PettingZoo

PettingZoo [49] is a Python library consisting of several MARL
tasks. In our analysis, we utilize the following fully cooperative
PettingZoo benchmarks: Entombed Cooperative, along with Pistonball and Cooperative Pong. These tasks allow challenging scenarios
that provide useful testbeds for fully cooperative MARL, using

Algorithm

Evaluated in

On/Off-Policy

RL Optimization

QMIX [40]

Off-policy

EMC [64]
MASER [22]

SMAC [43], GRF [27],
MPE [33], LBF [3],
RWARE [36]
SMAC [43], GRF [27],
MPE [33], LBF [3],
RWARE [36]
SMAC [43], MPE [33],
LBF [3], RWARE [36]
SMAC [43], GRF [27],
MPE [33], LBF [3],
RWARE [36]
SMAC [43]
SMAC [43], MA
MuJoCo [37]
SMAC [43], GRF [27],
MA MuJoCo [37]
SMAC [43]
SMAC [43]

EOI [24]
CDS [28]

GRF [27], MAgent [65]
SMAC [43], GRF [27]

MAA2C [36]

COMA [12]
MAPPO [62]

QPLEX [53]
HAPPO [26]
MAT-DEC [55]

Value-based

Network
Architectures
RNN & MLP

Intrinsic
Exploration
No

On-policy

Actor-Critic

RNN & MLP

No

On-policy

Policy Gradient

RNN & MLP

No

On-policy

Actor-Critic

RNN & MLP

No

Off-policy
On-policy

Value-based
Actor-Critic

RNN & MLP
RNN & MLP

No
No

On-policy

Actor-Critic

Transformer & MLP

No

Off-policy
Off-policy

On top of QPLEX
On top of QMIX

RNN & MLP
RNN & MLP

Off-policy
Off-policy

On top of MAA2C
On top of QPLEX

RNN & MLP
RNN & MLP

Yes: curiosity-driven
Yes: subgoal
generation
Yes: individuality
Yes: diversity &
information sharing

Table 1: Summary of the selected MARL algorithms

high-dimensional, RGB-image-based observation spaces. Indicative
references to the PettingZoo benchmark include [8, 47, 49â€“51].
4.1.1 Entombed Cooperative. Here, agents must extensively coordinate to progress as far as possible into a procedurally generated
maze. Each agent needs to quickly navigate down a constantly
evolving maze, where only part of the environment is visible. If
an agent becomes trapped, they lose. Agents can easily find themselves in dead-ends, only escapable through rare power-ups. A
major challenge is that optimal coordination requires agents to
position themselves on opposite sides of the map, as power-ups
appear on one side or the other but can be used to break through
walls symmetrically.
4.1.2 Pistonball. Pistonball is a physics-based fully cooperative
game in which the goal is to move a ball to the left boundary of
the game area by controlling a set of vertically moving pistons.
The main challenge lies in achieving highly coordinated, emergent
behavior to optimize performance in the environment. Pistonball
uses a realistic physics engine, comparable to the game Angry Birds,
adding further complexity to the required agent coordination.
4.1.3 Cooperative Pong. Cooperative Pong is a fully cooperative
variant of the classic Pong game where two agents control paddles
on opposite sides of the screen, aiming to keep the ball in play. The
challenge lies in the asymmetry between the agentsâ€”particularly
the right paddle, which has a more complex tiered shapeâ€”and their
limited observation space, restricted to their own half of the screen.
This setup requires agents to coordinate excessively and develop
cooperative strategies to maximize play time.

4.2

Overcooked

Overcooked [7] is a fully cooperative MARL benchmark, which has
been developed to address human-AI coordination. The objective
is to deliver soups as quickly as possible, with agents required to
place up to three ingredients in a pot, wait for the soup to cook, and
then deliver it. The tight space introduces significant challenges in
terms of coordination, as agents must split tasks on the fly, avoid
collisions, and coordinate effectively in order to achieve high reward.
In addition, the rewards are sparse, making the environment even
more challenging. Indicative references that use this benchmark
include [8, 47, 49]. We utilize the following three different layouts.
4.2.1 Cramped Room. Agents operate in a cramped room, that is,
a small room without obstacles. This layout focuses on the agentsâ€™
ability to maneuver in a limited space, requiring precise navigation
to avoid physical collisions with the other agent.
4.2.2 Asymmetric Advantages. Agents operate in an asymmetric
room, where each agent works in its own distinct space. In this
layout, the challenge lies in recognizing and exploiting the differing
strengths of each agent, such as speed or access to certain kitchen
resources. This task tests the agentsâ€™ ability to adapt their strategies
to complement each otherâ€™s capabilities, ensuring that each playerâ€™s
strengths are used effectively to enhance overall team performance.
4.2.3 Coordination Ring. Agents operate in a room with a ring.
They must coordinate their actions to collect onions from the bottom left corner of the room, make soups in the center left, and
deliver the dishes to the top right corner of the ring. This task is
the most challenging among the three.

MARL Benchmark
Entombed Cooperative (PettingZoo)

Pistonball (PettingZoo)

Cooperative Pong (PettingZoo)

Overcooked (Cramped Room,
Asymmetric Advantages, Coordination Ring)

Pressure Plate

Spread (MPE)

LBF

RWARE

Interesting Challenges
- RGB observations
- dead-ends
- excessive coordination to go to opposite sides
in order to break through walls symmetrically
- RGB observations
- excessive coordination for emergent behavior
and synchronization
- RGB observations
- excessive coordination due to asymmetry
between the agents
- sparse reward
- delivering tasks as quickly as possible
- agents operating in a confined space
- agent collision avoidance
- splitting tasks on the fly
- sparse reward
- excessive exploration and coordination to
sequentially unlock each room
- agent collision avoidance
- optimal landmark coverage
- very complex if ğ‘ is large
- sparse reward
- exploration to identify the food target
- excessive coordination to eat the target
simultaneously
- sparse reward and high-dimensional
observations
- excessive coordination to execute a specific
sequence of actions, at the right time, without
immediate feedback

Insights for Real-world Applications
- exploration in space missions
- search & rescue
- collective wildfire movement
- assembly line coordination
- dance and performance choreography
- collaborative video games
- performing arts
- sports collaboration
- kitchen automation
- urgent multi-robot tasks in confined spaces

- multi-robot collaboration
- escape-rooms-like tasks
- team-based problem solving
- traffic management
- distributed sensor networks
- urban planning
- multi-robot collaboration
- resource management in supply chains
- disaster response coordination
- robot warehousing
- logistics management

Table 2: Summary of the fully cooperative, partially observable, benchmark tasks

4.3

PressurePlate [1] is a fully cooperative environment, with sparsereward settings, set within a 2D grid-world composed of multiple
locked rooms that can be unlocked when an agent stands on the
corresponding pressure plate, culminating in a final room containing a goal chest. The primary challenge for agents lies in effectively
coordinating their movements and positions to sequentially unlock
each room and ultimately reach the chest. Indicative references that
use this benchmark task include [1, 17].

4.4

Benchmark

PressurePlate

PettingZoo
Overcooked
Pressure Plate
Capture Target
Box Pushing

Table 3: Newly integrated tasks (in addition to MPE, LBF and
RWARE) in PyMARLzoo+.

Multi-agent Particle Environment (MPE)

In our analysis, we utilize the well-known Spread task of the MPE
benchmark [32] that focus on effective navigation of particle agents.
Indicative references that use this benchmark task include [10, 36,
41, 66]. In this task, ğ‘ agents must cover ğ‘ landmarks while avoiding collisions. Agents are rewarded for staying close to landmarks
and penalized for collisions, creating a trade-off between coordination and collision avoidance. The task becomes more complex
as ğ‘ increases. Compared to [36], the evaluated tasks are more
challenging by using more agents and fewer training steps.

collect food by coordinating their actions. Agents can collect food
only if their combined levels meet or exceed the foodâ€™s level. The
main challenge is the sparse rewards, requiring agents to coordinate
closely to collect food simultaneously. Unlike previous work [36],
we evaluate more complex LBF tasks, even with a larger number of
agents, requiring extensive exploration and coordination. Indicative
references that use this benchmark include [7, 14, 19, 60].

4.6
4.5

Number of Newly
Integrated Tasks
49
5
3
1
1

Level Based Foraging (LBF)

Level-Based Foraging (LBF) [3] features fully cooperative gridworld environments where agents, assigned levels, must move and

Multi-Robot Warehouse (RWARE)

The Multi-Robot Warehouse (RWARE) environment simulates a
fully cooperative, partially observable grid-world where agents
must find and deliver shelves to workstations. With limited sight

and partial observations, agents face challenges such as sparse rewards, only given after successful shelf deliveries. This requires
precise action sequences, effective exploration, and excessive agent
coordination. Unlike previous work [36], this study evaluates more
challenging RWARE tasks in hard mode, demanding efficient exploration and excessive cooperation among agents. Indicative references that use RWARE include [49â€“51].

4.7

Capture Target

Capture Target [35, 57], a fully cooperative multi-agent singletarget task, presents significant challenges, aiming multiple agents
to locate and capture a flickering target on a grid. Excessive coordination is essential, as the target is only captured when all agents
converge on its location simultaneously, despite limited visibility.

4.8

Box Pushing

Box Pushing [58, 59] is a fully cooperative grid environment where
two agents must collaborate to move three boxesâ€”two small and
one largeâ€”to a target area. The main challenge is that the large
box, yielding the highest reward, can only be moved if both agents
coordinate by positioning themselves in parallel cells and pushing
simultaneously, making precise timing and teamwork essential.

5

produce robust and semantically meaningful representations, facilitating model generalization across diverse tasks by linking visual
inputs with textual descriptions. Similarly, SlimSAM incorporates a
streamlined self-attention mechanism for efficient image encoding.
In addition, we offer the option of using standard CNNs on raw
images for policy training from scratch.

PYMARLZOO+: PETTINGZOO,
OVERCOOKED, PRESSURE PLATE,
CAPTURE TARGET AND BOX PUSHING
NOW COMPATIBLE WITH (E)PYMARL

To facilitate a comprehensive understanding of MARL algorithms
through benchmarking, we open-source PyMARLzoo+, a Python
framework which extends the widely used (E)PyMARL [36, 43],
providing an integration of many SoTA algorithms on a plethora
of existing MARL benchmarks under a common framework. The
key features of our framework are presented below:
Newly Integrated Benchmarks. Our PyMARLzoo+ integrates 8
MARL benchmarks described in Section 4. More specifically, as
we illustrate in Table 3, in addition to the MPE, LBF and RWARE
benchmarks (already integrated in EPyMARL [36]), our framework
fully supports all tasks from PettingZoo, along with tasks from
Overcooked, Pressure Plate, Capture Target, and Box Pushing.
Newly Integrated Algorithms. Our PyMARLzoo+ integrates all
the algorithms described in Section 3. We note that, except for the
standard baselines (that is, MAA2C, MAPPO and QMIX) already
integrated in EPyMARL, all remaining algorithms were integrated
as part of our work. Furthermore, the widely used Prioritized Replay
Buffer [44] has been integrated into the off-policy algorithms.
Image (Observation) encoding. We incorporate three pre-trained
architectures as image encoders to transform image-based observations into tabular format for policy learning. Specifically, we
integrate the following options: ResNet18 [18], CLIP [38], and SlimSAM [9].ResNet18, commonly used in single-agent RL (e.g., in [46]),
can capture spatial hierarchies, which are beneficial for extracting
relevant features from complex visual data. CLIP, also used in single agent RL (e.g., in [15]), utilizes natural language supervision to

6 BENCHMARK ANALYSIS
6.1 Experimental Setup
In our benchmark analysis, we evaluate MARL algorithms on Entombed Cooperative, Pistonball, Cooperative Pong, Cramped Room,
Asymmetric Advantages, Coordination Ring, Pressure Plate, Spread,
LBF and RWARE benchmarks. To ensure a fair comparison of the selected algorithms, we utilize the same number of training timesteps.
Specifically, we use 10 million timesteps for MPE and LBF, 40 million
for RWARE, 5 million for PettingZoo; 20 million for PressurePlate,
40 million for the Overcookedâ€™s Cramped Room, and 100 million
timesteps for the Overcookedâ€™s Asymmetric Advantages and Coordination Ring. We note that for all PettingZoo tasks, where the
observations are RGB images, we have used a pre-trained ResNet18
model as the observation image encoder. Moreover, except HAPPO,
we utilize agents to share their policy parameters.
We adopt the experimental setup of [36]: Throughout the training of all algorithms, we conducted 100 test episodes at every
50,000-step interval to evaluate the current policy. During these
test episodes, we record the episode returns, i.e., the accumulated
Ãğ‘
rewards per episode, and compute the average return ğ‘1 ğ‘–=1
ğ‘…ğ‘¡,ğ‘–,ğ‘— ,
where ğ‘ = 100 is the number of test episodes, and ğ‘…ğ‘¡,ğ‘–,ğ‘— is the
return of the ğ‘–-th test episode at timestep ğ‘¡ of the ğ‘—-th experiment.
Our primary metric score is the mean of the unnormalized returns received from the best policy in convergence over five different seeds. As best policy in convergence, we use the policy that
received the best return of the last 50 test episodes. We present our
results using the mean and the 75% confidence interval.
We used the hyperparameter settings of the newly integrated algorithms, QPLEX, HAPPO, MAT-DEC, CDS, EOI, EMC, and MASER,
adhering to configurations suggested by the authors of their original papers for challenging tasks. For MAA2C, MAPPO, COMA
and QMIX, we employed the default parameters of EPyMARL. This
ensures that the evaluation is as consistent as possible with what is
reported in the original papers, allowing for a valid comparison between the algorithms and with results already reported. Our choice
to forego extensive tuning is further supported by: (a) preliminary
results showing no significant differences regarding algorithmsâ€™ performance across tasks, and (b) the fact that our results, are based
on the mean returns from the best policy across different seeds.
All experiments were conducted using CPUs, except for tasks
within the PettingZoo environment, where image inputs required
the use of GPUs. For these image-based tasks, GPUs were utilized for
both the frozen image-encoder and training CNN components when
a pre-trained encoder was not available. Specifically, we employed
two "g5.16xlarge" AWS instances, each featuring 64 vCPUs, 256 GB
RAM, and one A10G GPU with 24 GB of memory to manage the
computational demands of image processing.

Tasks\Algorithms
2s-8x8-3p-2f
2s-9x9-3p-2f
2s-12x12-2p-2f
4s-11x11-3p-2f
7s-20x20-5p-3f
8s-25x25-8p-5f
7s-30x30-7p-4f

QMIX

QPLEX

MAA2C

MAPPO

HAPPO

MAT-DEC

COMA

EOI

MASER

EMC

CDS

0.94 Â± 0.09
0.00 Â± 0.00
0.89 Â± 0.01
0.08 Â± 0.19
0.01 Â± 0.00
0.02 Â± 0.01
0.06 Â± 0.05

0.63 Â± 0.48
0.60 Â± 0.49
0.98 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00
0.01 Â± 0.00
0.00 Â± 0.00

0.98 Â± 0.02
0.59 Â± 0.38

0.64 Â± 0.34
0.18 Â± 0.35
0.77 Â± 0.04
0.00 Â± 0.00
0.57 Â± 0.18
0.41 Â± 0.23
0.57 Â± 0.03

0.00 Â± 0.00
0.00 Â± 0.00
0.52 Â± 0.26
0.00 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00

0.24 Â± 0.22
0.34 Â± 0.24
0.55 Â± 0.06
0.00 Â± 0.00
0.29 Â± 0.09
0.03 Â± 0.03
0.08 Â± 0.06

0.00 Â± 0.00
0.00 Â± 0.00
0.03 Â± 0.02
0.00 Â± 0.00
0.03 Â± 0.01
0.03 Â± 0.00
0.02 Â± 0.00

0.00 Â± 0.00
0.00 Â± 0.00
0.34 Â± 0.23
0.00 Â± 0.00
0.03 Â± 0.01
0.07 Â± 0.00
0.04 Â± 0.00

0.00 Â± 0.00
0.00 Â± 0.00
0.01 Â± 0.01
0.00 Â± 0.00
0.01 Â± 0.00
0.01 Â± 0.00
0.01 Â± 0.00

0.00 Â± 0.00
0.50 Â± 0.50
0.87 Â± 0.03
0.00 Â± 0.00
0.01 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00

0.00 Â± 0.00
0.00 Â± 0.00
0.91 Â± 0.02
0.00 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00

0.81 Â± 0.03
0.00 Â± 0.00
0.78 Â± 0.02
0.52 Â± 0.24
0.71 Â± 0.02

Table 4: Results in LBF tasks.

Tasks\Algorithms

QMIX

QPLEX

MAA2C

MAPPO

HAPPO

MAT-DEC

COMA

EOI

MASER

EMC

CDS

tiny-2ag-hard
tiny-4ag-hard
small-4ag-hard

0.00 Â± 0.00
0.00 Â± 0.00
0.01 Â± 0.01

0.61 Â± 0.45
11.73 Â± 10.81
0.94 Â± 0.63

2.25 Â± 0.62
6.87 Â± 7.12
1.38 Â± 1.26

2.91 Â± 0.82
17.1 Â± 5.31
4.14 Â± 2.12

1.46 Â± 2.06
24.07 Â± 0.71
9.69 Â± 3.19

6.00 Â± 8.49
27.85 Â± 19.71

0.02 Â± 0.00
0.02 Â± 0.00
0.03 Â± 0.00

6.58 Â± 3.92

0.00 Â± 0.00
0.00 Â± 0.00
0.02 Â± 0.00

1.66 Â± 0.30
0.00 Â± 0.00
0.05 Â± 0.00

4.00 Â± 2.30
27.37 Â± 6.88
4.11 Â± 0.32

0.00 Â± 0.00

12.09 Â± 4.59
0.17 Â± 0.01

Table 5: Results in RWARE tasks.

Figure 1: Aggregated Normalized episodic rewards of each benchmark.
Algorithms\Tasks
QMIX
QPLEX
MAA2C
MAPPO
HAPPO
MAT-DEC
COMA
EOI
MASER
EMC
CDS

Spread-4

Spread-5

Spread-8

âˆ’1278.26 Â± 23.13
-766.84 Â± 14.39
âˆ’1190.09 Â± 99.93
âˆ’971.17 Â± 124.22
âˆ’1032.80 Â± 45.84
âˆ’1066.62 Â± 45.98
âˆ’1176.78 Â± 33.37
âˆ’1963.23 Â± 859.27
âˆ’969.06 Â± 5.01
âˆ’1216.19 Â± 10.9
âˆ’809.20 Â± 47.22

âˆ’2531.17 Â± 586.56
âˆ’1800.53 Â± 194.49
âˆ’2312.56 Â± 222.08
âˆ’1910.20 Â± 42.86
âˆ’2000.41 Â± 98.20
âˆ’1918.88 Â± 15.76
âˆ’2003.47 Â± 51.18
âˆ’5816.66 Â± 20.34
âˆ’1939.58 Â± 113.22
âˆ’1961.75 Â± 0.71
-1641.77 Â± 14.61

âˆ’6414.48 Â± 27.27
âˆ’13260.36 Â± 6200.03
-5961.67 Â± 66.04
-5926.39 Â± 38.48
âˆ’6940.61 Â± 69.55
âˆ’6843.44 Â± 563.39
âˆ’6249.07 Â± 44.73
âˆ’13210.98 Â± 5659.53
âˆ’6242.61 Â± 14.40
âˆ’6219.14 Â± 29.55
âˆ’6250.72 Â± 15.44

Table 6: Results in Spread (MPE) tasks.

Algorithms\Envs

Pistonball

Cooperative Pong

Entompted Cooperative

QMIX
QPLEX
MAA2C
MAPPO
HAPPO
MAT-DEC
COMA
EOI
MASER
EMC
CDS

991.59 Â± 0.17
991.46 Â± 0.10
990.83 Â± 0.04
990.71 Â± 0.10

199.57 Â± 0.61
197.78 Â± 3.14

8.00 Â± 0.00
8.00 Â± 0.00
6.57 Â± 0.05
6.61 Â± 0.05
7.99 Â± 0.01
10.00 Â± 0.00
7.68 Â± 1.7
6.53 Â± 0.02
8.00 Â± 0.00
8.00 Â± 0.00
11.10 Â± 1.00

983.60 Â± 0.77
982.57 Â± 2.44
678.28 Â± 324.06
948.35 Â± 42.74
989.39 Â± 0.64
265.00 Â± 174.58
415.82 Â± 284.37

âˆ’0.33 Â± 3.47
13.22 Â± 4.61
25.62 Â± 3.36
200.00 Â± 0.00
1.10 Â± 7.51
âˆ’1.64 Â± 2.66
104.25 Â± 69.29
196.5 Â± 2.83
197.13 Â± 2.26

Table 7: Results in PettingZoo using ResNet18.

Algorithms\Envs

Cramped Room

Asymmetric Advantages

Coordination Ring

QMIX
QPLEX
MAA2C
MAPPO
HAPPO
MAT-DEC
COMA
EOI
EMC
MASER
CDS

0.00 Â± 0.00
86.67 Â± 122.57
286.80 Â± 9.34
280.00 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00
0.20 Â± 0.16
280.0 Â± 0.00
0.00 Â± 0.00
0.00 Â± 0.00
186.67 Â± 133.00

300.00 Â± 300.00
0.00 Â± 0.00
487.80 Â± 107.60
0.30 Â± 0.10
160.10 Â± 159.9
0.00 Â± 0.00
0.10 Â± 0.10
1.60 Â± 0.60
âˆ’
0.00 Â± 0.00
70.00 Â± 70.00

0.00 Â± 0.00
0.00 Â± 0.00
0.10 Â± 0.10
0.07 Â± 0.09
0.00 Â± 0.00
0.00 Â± 0.00
0.07 Â± 0.09
0.13 Â± 0.09
âˆ’
0.00 Â± 0.00
0.00 Â± 0.00

Table 8: Results in Overcooked environments.

Algorithms\Envs
QMIX
QPLEX
MAA2C
MAPPO
HAPPO
MAT-DEC
COMA
EOI
MASER
EMC
CDS

4p

6p

âˆ’210.72 Â± 17.84
âˆ’652.19 Â± 9.29
âˆ’281.59 Â± 201.77
âˆ’135.99 Â± 1.32
âˆ’258.69 Â± 224.93
âˆ’876.58 Â± 1113.53
âˆ’4391.79 Â± 108.96
âˆ’3050.64 Â± 1125.83
-88.44 Â± 2.84
âˆ’4518.23 Â± 249.68
âˆ’1926.45 Â± 260.85

âˆ’3461.77 Â± 1020.33
âˆ’5183.56 Â± 345.46
âˆ’547.39 Â± 21.00
-494.08 Â± 10.54
-584.90 Â± 201.68
âˆ’2930.77 Â± 3652.53
âˆ’12360.20 Â± 314.06
âˆ’9221.16 Â± 4796.08
âˆ’5257.08 Â± 4099.19
âˆ’12347.60 Â± 0.0
âˆ’8068.23 Â± 519.79

Table 9: Results in Pressure Plate tasks.

Algorithms
MAA2C-ResNet18
MAA2C-CNN

Episodic Reward

RAM

Training time

990.83 Â± 0.04
846.74 Â± 19.7

7ğºğµ
34ğºğµ

4ğ‘‘ : 15â„
16ğ‘‘ : 0â„

Table 10: Results in PettingZooâ€™s Pistonball comparing
ResNet18 as frozen image encoder with trainable CNNs.

require sufficiently dense rewards to effectively learn to decompose the value function. We show several cases with
sparse reward settings, such as in LBF (e.g., 2s-8x8-3p-2f, 2s9x9-3p-2f ), RWARE (tiny-4ag-hard), and Pressure Plate (4p),
where QPLEX, or even QMIX, can successfully converge to
effective policies.
6.2.2 Discussion on Algorithmsâ€™ Performance. Next, we analyse the
performance of the evaluated MARL algorithms in the selected fully
cooperative benchmark tasks.

Figure 2: ResNet18 vs Trainable CNNs: MAA2C in PettingZooâ€™s Pistonball task.

6.2

Main Results and Analysis

In this section, we present our main results and the benchmark
analysis. We show the main analytical results in Tables 4, 5, 6, 7, 8
and 9, and the averaged main results in Figure 1. To obtain these
average results, we use the normalized scores for each task of the
benchmark and average over these scores. In the tables, we write
in bold the values of the best algorithm in a specific task. If the
performance of another algorithm was not statistically significantly
different from the best algorithm, the respective value is also in
bold. Algorithms with green cell color are the best on average in
the corresponding benchmark.
6.2.1

Key Findings. Based on results, we highlight the following:
â€¢ Best Algorithms. The standard MARL algorithms, QPLEX,
MAPPO, MAA2C and CDS, demonstrate the most consistent
performance across all fully cooperative benchmarks.
â€¢ SoTA exploration-based algorithms mostly underperform. Exploration-based methods that have reached SoTA
performance in SMAC and/or GRF, that is, EOI, EMC and
MASER, significantly underperform in most benchmarks
compared to the standard methods. In some cases, they even
fail entirely, including in tasks with sparse reward settings
(e.g., see Tables 4, 5 and 8), in which these methods are expected to improve performance over baselines. The only
exception is CDS which is shown to be one of the best algorithms in RWARE and Spread.
â€¢ Value Decomposition. As demonstrated by the results of
QMIX and QPLEX, value decomposition methods tend to
converge to suboptimal policies as the number of agents
increases, significantly underperforming compared to actorcritic methods (e.g., see the results in Spread, LBF and Pressure Plate). Moreover, our findings contrast with one conclusion of [36] that suggests that value decomposition methods

QMIX, QPLEX. QMIX shows mediocre performance across most
tasks, with complete failure in some (e.g., RWARE and many LBF
tasks), yet the highest rewards in PettingZoo. Notably, QMIX is
the only method to achieve positive rewards in the sparse-reward
LBF 4s-11x11-3p-2f task. In contrast, QPLEX generally outperforms QMIX improving performance in cooperative tasks. However,
QPLEX fails in the most challenging LBF and Overcooked tasks
and struggles in large-scale scenarios such as Spread-8 and Pressure Plateâ€™s 6p. Interestingly, despite lacking advanced exploration
techniques, QPLEX notably improves over QMIX in RWARE.
MAA2C, MAPPO, COMA. MAA2C is one of the most consistent algorithms across all tasks, showing the best performance
in complex sparse-reward LBF tasks and in Spread-8 (along with
MAPPO) and Overcooked. Similarly, MAPPO also ranks as one
of the most consistent algorithms but significantly outperforms
MAA2C in RWARE, Spread-5, Cooperative Pong, and Pressure Plateâ€™s
6p. MAPPO achieves the highest rewards on average in Spread (together with CDS) and Pressure Plate. Interestingly, in the RWARE
tasks where MAPPO had been the best option in previous work [36],
HAPPO, MAT-DEC, and CDS outperform it. Conversely, COMA
has one of the lowest performances overall, with competitive results only in Spread, Entombed Cooperative, and Coordination Ring.
However, for the last two tasks, all algorithms perform poorly, so
none are truly competitive.
HAPPO, MAT-DEC. Both HAPPO and MAT-DEC do not manage
to achieve consistent performance across the evaluated benchmarks.
More specifically, both methods are shown to be among the best (together with CDS) in RWARE, while HAPPO also achieving good performance in Pressure Plate and MAT-DEC in the high-dimensional
PettingZoo tasks. However, both HAPPO and MAT-DEC perform
poorly in tasks where efficient exploration and synchronized coordination are crucial, such as in LBF and Overcooked benchmarks.
MASER, EMC. Despite MASER and EMC achieving strong results in sparse-reward SMAC tasks, they perform poorly in most
fully cooperative tasks. The notable exception is PettingZoo tasks,
where MASERâ€™s baseline, QMIX, performs better. MASER also
achieves the highest rewards in Pressure Plate (4p). Both MASERâ€™s
and EMCâ€™s poor overall performance is attributed to its reliance on
Q-values to enhance joint exploration: Since agents rarely receive
positive rewards, this leads to misleading intrinsic rewards based
on irrelevant Q-value-driven sub-goals. However, Q-values prove
to be useful for guiding exploration in LBFâ€™s 2s-12x12-2p-2f, as this
task is less sparse but also requires good exploration for success.
EOI, CDS. EOI and CDS outperform MASER and EMC in most
tasks. EOI achieves the highest rewards in Overcookedâ€™s Cramped

Environments\Algorithms

QMIX

QPLEX

MAA2C

MAPPO

HAPPO

MAT-DEC

COMA

EOI

MASER

EMC

CDS

LBF
RWARE
Spread (MPE)
Petting Zoo
Overcooked
Pressure Plate

0ğ‘‘ : 8â„
1ğ‘‘ : 22â„
0ğ‘‘ : 15â„
1ğ‘‘ : 16â„
3ğ‘‘ : 14â„
0ğ‘‘ : 22â„

0ğ‘‘ : 13â„
2ğ‘‘ : 17â„
0ğ‘‘ : 21â„
3ğ‘‘ : 11â„
5ğ‘‘ : 3â„
1ğ‘‘ : 14â„

0ğ‘‘ : 1â„
0ğ‘‘ : 9â„
0ğ‘‘ : 2â„
3ğ‘‘ : 23â„
0ğ‘‘ : 19â„
0ğ‘‘ : 4â„

0ğ‘‘ : 2â„
0ğ‘‘ : 12â„
0ğ‘‘ : 3â„
3ğ‘‘ : 10â„
1ğ‘‘ : 4â„
0ğ‘‘ : 7â„

0ğ‘‘ : 5â„
1ğ‘‘ : 1â„
0ğ‘‘ : 9â„
14ğ‘‘ : 1â„
1ğ‘‘ : 18â„
0ğ‘‘ : 20â„

0ğ‘‘ : 4â„
0ğ‘‘ : 20â„
0ğ‘‘ : 6â„
3ğ‘‘ : 6â„
2ğ‘‘ : 11â„
0ğ‘‘ : 12â„

0ğ‘‘ : 1â„
0ğ‘‘ : 9â„
0ğ‘‘ : 2â„
3ğ‘‘ : 11â„
0ğ‘‘ : 21â„
0ğ‘‘ : 4â„

0ğ‘‘ : 6â„
1ğ‘‘ : 18â„
0ğ‘‘ : 11â„
0ğ‘‘ : 23â„
3ğ‘‘ : 9â„
0ğ‘‘ : 18â„

0ğ‘‘ : 18â„
2ğ‘‘ : 16â„
1ğ‘‘ : 12â„
2ğ‘‘ : 1â„
4ğ‘‘ : 9â„
1ğ‘‘ : 6â„

2ğ‘‘ : 4â„
19ğ‘‘ : 1â„
4ğ‘‘ : 9â„
3ğ‘‘ : 23â„
13ğ‘‘ : 14â„
9ğ‘‘ : 7â„

0ğ‘‘ : 18â„
3ğ‘‘ : 2â„
4ğ‘‘ : 9â„
1ğ‘‘ : 19â„
2ğ‘‘ : 14â„
2ğ‘‘ : 1â„

Table 11: Average (wall-clock) training times over all tasks of each benchmark for all 11 algorithms.

Room (alongside MAA2C and MAPPO) and RWAREâ€™s tiny-2aghard. However, EOI lacks the consistency of the MAA2C backbone
algorithm. We attribute this inconsistency to the emphasis on individuality, which can hinder full cooperation and the high level of
coordination required in most fully cooperative tasks. In contrast,
CDS is more consistent, showing top performance in Spread and
RWARE (alongside MAT-DEC and HAPPO) tasks. CDSâ€™ improved
performance is attributed to the fact that although CDS relies on
encouraging agents to be more diverse, as EOI does, it also assures
sufficient sharing of the most useful experience of the agents.
6.2.3 Pre-trained Image-Encoder vs Trainable CNNs for Image-based
Observations. Our experimental results underscore the effectiveness
of employing a frozen pre-trained image encoder, such as ResNet18,
over trainable CNNs in multi-agent reinforcement learning (MARL)
environments. In the specific case of the PettingZooâ€™s Pistonball
task, the use of frozen ResNet18 led to more stable and higher
overall policy performance compared to its trainable counterparts.
This advantage is clearly illustrated by the smoother convergence
curves and the consistently superior performance throughout the
training steps, as shown in Figure 2. Moreover, as detailed in Table 10, the non-adaptive nature of frozen ResNet18 highlights its
efficiency in managing complex visual contexts without computational overhead. This suggests that pre-trained, static encoders are
highly beneficial, providing immediate and reliable performance
improvements. The adoption of frozen image encoders significantly
improves computational efficiency by eliminating the need for ongoing adjustments during training. This leads to shorter training
times for MARL algorithms, as detailed in Table 10. Moreover, by
converting image data into compact vector representations, these
encoders substantially lower memory requirements, facilitating the
execution of complex MARL tasks with RGB-based observations.
6.2.4 Comparison of training times. As can be clearly seen from
Table 11, off-policy algorithms generally require longer training due
to their use of large replay buffers, which process experiences from
multiple past episodes. In contrast, on-policy algorithm, benefit
from learning directly from current policy experiences, allowing
for faster training. However, in image-based, high-dimensional
environments, such as PettingZoo tasks, this parallelism can slow
down training, as it does not integrate well with pre-trained models,
such as ResNet18, which require GPU resources.
6.2.5 Open Challenges. Based on the results we discussed above,
it is evident that fully cooperative MARL tasks need careful algorithmic design, in terms of both excessive coordination and joint
exploration, as current SoTA and standard MARL methods are not
very effective. Below, we report some significant open challenges
arised from our benchmark analysis:
(a) The tasks Entombed Cooperative (PettingZoo) and Coordination

Ring (Overcooked) are the most challenging, as all evaluated algorithms indeed fail to find any effective policy. Any improvement
on these tasks would be of remarkable interest.
(b) The sparse-reward LBF tasks, with a large grid, three agents and
two foods, the sparse-reward Pressure Plate tasks, with more than
4 agents, and the sparse-reward hard RWARE tasks, with larger
grids, are quite challenging, as they require excessive coordinated
exploration, and any improvement on these is very interesting.
(c) The Spread tasks, with more than 4 agents are quite challenging,
as they require excessive coordination, and any improvement on
these without the use of agent communication during execution is
very interesting.

7

RELATED WORK

The recent rise in MARL popularity has fragmented community
standards and tools, with the frequent introduction of new libraries
such as [20, 30, 42, 49]. Among the most popular are PyMARL
[43] and EPyMARL [36], both of which have played a crucial role
in driving the influx of cooperative MARL algorithms. However,
these libraries have somewhat overlooked the integration of fully
cooperative MARL environments, pushing researchers to focus on
specific benchmarks, such as SMAC [11, 43] and GRF [27], raising
concerns about the reliability and generalizability of the proposed
algorithms [16]. Despite recent efforts [4, 20, 36, 63] aiming to
provide a comprehensive understanding of standard cooperative
MARL algorithms through benchmarking, the evaluation of fully
cooperative MARL still lacks systematic diversity and reliability.

8

CONCLUSION

In this paper, we highlight and address key concerns in the evaluation of cooperative MARL algorithms by providing an extended
benchmarking of well-known MARL methods in fully cooperative
tasks. Our extensive evaluations reveal significant discrepancies in
the performance of SoTA methods, which can eventually underperform compared to standard baselines. Based on our analysis, as well
as by open-sourcing PyMARLzoo+, this paper aims to motivate
towards more systematic evaluation of MARL algorithms, encouraging broader adoption of diverse, fully cooperative benchmarks.

ACKNOWLEDGMENTS
The research work contributed by George Papadopoulos was partially supported by the Hellenic Foundation for Research and Innovation (HFRI) under the 5th Call for HFRI PhD Fellowships (Fellowship Number: 20769). The authors also gratefully acknowledge
GRNET â€“ National Infrastructures for Research and Technology for
providing AWS resources, and the University of Piraeus for funding
the participation in the AAMAS 2025 conference and associated
travel costs.

REFERENCES
[1] Ibrahim H Ahmed, Cillian Brewitt, Ignacio Carlucho, Filippos Christianos, Mhairi
Dunion, Elliot Fosong, Samuel Garcin, Shangmin Guo, Balint Gyevnar, Trevor
McInroe, et al. 2022. Deep reinforcement learning for multi-agent interaction.
Ai Communications 35, 4 (2022), 357â€“368.
[2] Stefano V Albrecht, Filippos Christianos, and Lukas SchÃ¤fer. 2024. Multi-agent
reinforcement learning: Foundations and modern approaches. MIT Press.
[3] Stefano V. Albrecht and Peter Stone. 2017. Reasoning about Hypothetical Agent
Behaviours and their Parameters. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems (SÃ£o Paulo, Brazil) (AAMAS â€™17). International Foundation for Autonomous Agents and Multiagent Systems, Richland,
SC, 547â€“555.
[4] Matteo Bettini, Amanda Prorok, and Vincent Moens. 2024. Benchmarl: Benchmarking multi-agent reinforcement learning. Journal of Machine Learning Research 25, 217 (2024), 1â€“10.
[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John
Schulman, Jie Tang, and Wojciech Zaremba. 2016.
OpenAI Gym.
arXiv:arXiv:1606.01540
[6] Wolfram Burgard, Mark Moors, Dieter Fox, Reid Simmons, and Sebastian Thrun.
2000. Collaborative multi-robot exploration. In Proceedings 2000 ICRA. Millennium
Conference. IEEE International Conference on Robotics and Automation. Symposia
Proceedings (Cat. No. 00CH37065), Vol. 1. IEEE, IEEE, 476â€“481.
[7] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel,
and Anca Dragan. 2019. On the utility of learning about humans for human-ai
coordination. Advances in neural information processing systems 32 (2019).
[8] Changyu Chen, Ramesha Karunasena, Thanh Nguyen, Arunesh Sinha, and
Pradeep Varakantham. 2024. Generative modelling of stochastic actions with
arbitrary constraints in reinforcement learning. Advances in Neural Information
Processing Systems 36 (2024).
[9] Zigeng Chen, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2024. SlimSAM:
0.1% Data Makes Segment Anything Slim. arXiv:2312.05284 [cs.CV] https:
//arxiv.org/abs/2312.05284
[10] Filippos Christianos, Georgios Papoudakis, Muhammad A Rahman, and Stefano V
Albrecht. 2021. Scaling multi-agent reinforcement learning with selective parameter sharing. In International Conference on Machine Learning. PMLR, 1989â€“1998.
[11] Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun,
Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. 2024. Smacv2: An improved
benchmark for cooperative multi-agent reinforcement learning. Advances in
Neural Information Processing Systems 36 (2024).
[12] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings
of the AAAI conference on artificial intelligence, Vol. 32.
[13] Center for Human-Compatible AI GIthub contributors. [n.d.]. Github Overcooked.
https://github.com/HumanCompatibleAI/overcooked_ai. Overcooked GitHub
repository.
[14] Elliot Fosong, Arrasy Rahman, Ignacio Carlucho, and Stefano V Albrecht. 2024.
Learning Complex Teamwork Tasks using a Given Sub-task Decomposition.
In Proceedings of the 23rd International Conference on Autonomous Agents and
Multiagent Systems. 598â€“606.
[15] Jialu Gao, Kaizhe Hu, Guowei Xu, and Huazhe Xu. 2024. Can pre-trained textto-image models generate visual goals for reinforcement learning? Advances in
Neural Information Processing Systems 36 (2024).
[16] Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock, Roland Dubb, Siddarth
Singh, and Arnu Pretorius. 2022. Towards a standardised performance evaluation
protocol for cooperative marl. Advances in Neural Information Processing Systems
35 (2022), 5510â€“5521.
[17] Nikunj Gupta, Somjit Nath, and Samira Ebrahimi Kahou. 2023. CAMMARL:
Conformal Action Modeling in Multi Agent Reinforcement Learning. arXiv
preprint arXiv:2306.11128 (2023).
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[19] Joey Hong, Sergey Levine, and Anca Dragan. 2024. Learning to influence human
behavior with offline reinforcement learning. Advances in Neural Information
Processing Systems 36 (2024).
[20] Siyi Hu, Yifan Zhong, Minquan Gao, Weixun Wang, Hao Dong, Zhihui Li, Xiaodan
Liang, Yaodong Yang, and Xiaojun Chang. 2022. Marllib: Extending rllib for multiagent reinforcement learning. (2022).
[21] Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung. 2022. Maser:
Multi-agent reinforcement learning with subgoals generated from experience
replay buffer. In International Conference on Machine Learning. PMLR, 10041â€“
10052.
[22] Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung. 2022. Maser:
Multi-agent reinforcement learning with subgoals generated from experience
replay buffer. In International Conference on Machine Learning. PMLR, 10041â€“
10052.

[23] Chengzhi Jiang and Zhaohan Sheng. 2009. Case-based reinforcement learning for
dynamic inventory control in a multi-agent supply-chain system. Expert Systems
with Applications 36, 3 (2009), 6520â€“6526.
[24] Jiechuan Jiang and Zongqing Lu. 2021. The emergence of individuality. In
International Conference on Machine Learning. PMLR, 4992â€“5001.
[25] Andreas Kontogiannis and George A Vouros. 2023. Inherently Interpretable
Deep Reinforcement Learning Through Online Mimicking. In International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems.
Springer, 160â€“179.
[26] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun
Wang, and Yaodong Yang. 2021. Trust region policy optimisation in multi-agent
reinforcement learning. arXiv preprint arXiv:2109.11251 (2021).
[27] Karol Kurach, Anton Raichuk, Piotr StaÅ„czyk, MichaÅ‚ ZajÄ…c, Olivier Bachem,
Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier
Bousquet, et al. 2020. Google research football: A novel reinforcement learning
environment. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.
4501â€“4510.
[28] Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and
Chongjie Zhang. 2021. Celebrating diversity in shared multi-agent reinforcement
learning. Advances in Neural Information Processing Systems 34 (2021), 3991â€“4002.
[29] Xihan Li, Jia Zhang, Jiang Bian, Yunhai Tong, and Tie-Yan Liu. 2019. A Cooperative Multi-Agent Reinforcement Learning Framework for Resource Balancing in
Complex Logistics Network. In Proceedings of the 18th International Conference
on Autonomous Agents and MultiAgent Systems. 980â€“988.
[30] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018. RLlib: Abstractions
for distributed reinforcement learning. In International conference on machine
learning. PMLR, 3053â€“3062.
[31] Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Anima Anandkumar. 2021. Coach-player multi-agent reinforcement learning for dynamic team
composition. In International Conference on Machine Learning. PMLR, 6860â€“6870.
[32] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive
environments. Advances in neural information processing systems 30 (2017).
[33] Igor Mordatch and Pieter Abbeel. 2018. Emergence of grounded compositional
language in multi-agent populations. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 32.
[34] Frans A Oliehoek, Christopher Amato, et al. 2016. A concise introduction to
decentralized POMDPs. Vol. 1. Springer.
[35] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P How, and
John Vian. 2017. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning.
PMLR, 2681â€“2690.
[36] Georgios Papoudakis, Filippos Christianos, Lukas SchÃ¤fer, and Stefano V. Albrecht.
2021. Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in
Cooperative Tasks. In Proceedings of the Neural Information Processing Systems
Track on Datasets and Benchmarks (NeurIPS).
[37] Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin BÃ¶hmer, and Shimon Whiteson. 2021. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information
Processing Systems 34 (2021), 12208â€“12221.
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
In International conference on machine learning. PMLR, 8748â€“8763.
[39] Aowabin Rahman, Arnab Bhattacharya, Thiagarajan Ramachandran, Sayak
Mukherjee, Himanshu Sharma, Ted Fujimoto, and Samrat Chatterjee. 2022. Adversar: Adversarial search and rescue via multi-agent reinforcement learning. In
2022 IEEE International Symposium on Technologies for Homeland Security (HST).
IEEE, 1â€“7.
[40] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic value function
factorisation for deep multi-agent reinforcement learning. Journal of Machine
Learning Research 21, 178 (2020), 1â€“51.
[41] Jingqing Ruan, Yali Du, Xuantang Xiong, Dengpeng Xing, Xiyun Li, Linghui Meng,
Haifeng Zhang, Jun Wang, and Bo Xu. 2022. GCS: Graph-based coordination
strategy for multi-agent reinforcement learning. arXiv preprint arXiv:2201.06257
(2022).
[42] Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei
Lupu, GarÃ°ar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de
Witt, Alexandra Souly, et al. 2024. JaxMARL: Multi-Agent RL Environments
and Algorithms in JAX. In Proceedings of the 23rd International Conference on
Autonomous Agents and Multiagent Systems. 2444â€“2446.
[43] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob
Foerster, and Shimon Whiteson. 2019. The starcraft multi-agent challenge. arXiv
preprint arXiv:1902.04043 (2019).

[44] Tom Schaul. 2015. Prioritized Experience Replay. arXiv preprint arXiv:1511.05952
(2015).
[45] Sven Seuken and Shlomo Zilberstein. 2007. Improved memory-bounded dynamic
programming for decentralized POMDPs. In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (Vancouver, BC, Canada) (UAIâ€™07).
AUAI Press, Arlington, Virginia, USA, 344â€“351.
[46] Rutav M Shah and Vikash Kumar. 2021. RRL: Resnet as representation for
Reinforcement Learning. In International Conference on Machine Learning. PMLR,
9465â€“9476.
[47] Chapman Siu, Jason Traish, and Richard Yi Da Xu. 2021. Dynamic coordination
graph for cooperative multi-agent reinforcement learning. In Asian Conference
on Machine Learning. PMLR, 438â€“453.
[48] Jing Sun, Shuo Chen, Cong Zhang, Yining Ma, and Jie Zhang. 2024. DecisionMaking With Speculative Opponent Models. IEEE Transactions on Neural Networks and Learning Systems (2024).
[49] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth
Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. 2021. Pettingzoo: Gym for multi-agent reinforcement
learning. Advances in Neural Information Processing Systems 34 (2021), 15032â€“
15043.
[50] Justin K Terry, Benjamin Black, and Luis Santos. 2020. Multiplayer support for
the arcade learning environment. arXiv preprint arXiv:2009.09341 (2020).
[51] Justin K Terry, Nathaniel Grammel, Sanghyun Son, Benjamin Black, and Aakriti
Agrawal. 2020. Revisiting parameter sharing in multi-agent deep reinforcement
learning. arXiv preprint arXiv:2005.13625 (2020).
[52] Filippos Christianos Trevor McInroe. [n.d.]. Github pressureplate. https://github.
com/uoe-agents/pressureplate/tree/main. Pressureplate GitHub repository.
[53] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. 2020.
Qplex: Duplex dueling multi-agent q-learning. arXiv preprint arXiv:2008.01062
(2020).
[54] Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. 2020. ROMA:
multi-agent reinforcement learning with emergent roles. In Proceedings of the
37th International Conference on Machine Learning. 9876â€“9886.
[55] Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and
Yaodong Yang. 2022. Multi-agent reinforcement learning is a sequence modeling
problem. Advances in Neural Information Processing Systems 35 (2022), 16509â€“
16521.
[56] Yuchen Xiao, Joshua Hoffman, and Christopher Amato. 2019. Macro-ActionBased Deep Multi-Agent Reinforcement Learning. In 3rd Annual Conference on
Robot Learning.
[57] Yuchen Xiao, Joshua Hoffman, and Christopher Amato. 2020. Macro-action-based
deep multi-agent reinforcement learning. In Conference on Robot Learning. PMLR,
1146â€“1161.
[58] Yuchen Xiao, Joshua Hoffman, Tian Xia, and Christopher Amato. 2020. Learning
multi-robot decentralized macro-action-based policies via a centralized q-net.
In 2020 IEEE International conference on robotics and automation (ICRA). IEEE,
10695â€“10701.
[59] Yuchen Xiao, Weihao Tan, and Christopher Amato. 2022. Asynchronous actorcritic for multi-agent reinforcement learning. Advances in Neural Information
Processing Systems 35 (2022), 4385â€“4400.
[60] Mingyu Yang, Yaodong Yang, Zhenbo Lu, Wengang Zhou, and Houqiang Li.
2024. Hierarchical multi-agent skill discovery. Advances in Neural Information
Processing Systems 36 (2024).
[61] Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao, Jianye Hao, and
Pheng-Ann Heng. 2022. Transformer-based working memory for multiagent
reinforcement learning with action parsing. Advances in Neural Information
Processing Systems 35 (2022), 34874â€“34886.
[62] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen,
and Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent
games. Advances in Neural Information Processing Systems 35 (2022), 24611â€“24624.
[63] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu.
2020. Benchmarking multi-agent deep reinforcement learning algorithms. (2020).
[64] Lulu Zheng, Jiarui Chen, Jianhao Wang, Jiamin He, Yujing Hu, Yingfeng Chen,
Changjie Fan, Yang Gao, and Chongjie Zhang. 2021. Episodic multi-agent reinforcement learning with curiosity-driven exploration. Advances in Neural
Information Processing Systems 34 (2021), 3757â€“3769.
[65] Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang,
and Yong Yu. 2018. Magent: A many-agent reinforcement learning platform for
artificial collective intelligence. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 32.
[66] Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and Yaodong
Yang. 2024. Heterogeneous-agent reinforcement learning. Journal of Machine
Learning Research 25, 1-67 (2024), 1.

A

ENVIRONMENT API

The following Python script (Figure 3) provides an example of executing an episode in Butterflyâ€™s Pistonball task of PettingZoo using random
actions under our framework. To use a different environment, the user only needs to modify the args variable accordingly. Below, we provide
more detailed instructions. The script demonstrates: (a) importing necessary packages, (b) initializing the environment, and (c) running an
episode by repeatedly rendering the environment and applying random actions until the episode ends.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

# Import packages
from envs import REGISTRY as env_REGISTRY
import random as rnd
# Arguments for PettingZoo
args = {
" env " : " pettingzoo " ,
" env_args " : {
" key " : " pistonball_v6 " ,
}
}
# Initialize environment
env = env_REGISTRY [ args [ " env " ]](** args [ " env_args " ])
n_agns = env . n_agents
n_acts = env . get_total_actions ()
# Reset the environment
obs , state = env . reset ()
done = False
# Run an episode
while not done :
# Render the environment
env . render ()
# Insert the policy 's actions here
actions = rnd . choices ( range (0 , n_acts ) , k = n_agns )
# Apply an environment step
reward , done , info = env . step ( actions )
obs = env . get_obs ()
state = env . get_state ()
# Terminate the environment
env . close ()

Figure 3: Indicative Python script for executing an episode in PettingZooâ€™s Pistonball.

Following the example presented above (Figure 3), we provide the minimum args required for each environment, as well as all of those that
can be specified.
PettingZoo args:

args = {
" env " : " pettingzoo " ,
" env_args " : {
" key " : " pistonball_v6 " ,
}
}

Figure 4: Example of minimal args for PettingZoo tasks.

args = {
" env " : " pettingzoo " ,
" env_args " : {
" key " : " pistonball_v6 " ,
" seed " : 1 ,
" time_limit " : 900 ,
" partial_observation " : False ,
" trainable_cnn " : False ,
" image_encoder " : " ResNet18 " ,
" image_encoder_batch_size " : 10 ,
" image_encoder_use_cuda " : True ,
" centralized_image_encoding " : True ,
" kwargs " : " "
}
}

Figure 5: Example of all args for PettingZoo tasks.
where:
â€¢ key is the selected PettingZoo task. Options: "pistonball_v6", "cooperative_pong_v5", "entombed_cooperative_v3", "space_invaders_v2",
"basketball_pong_v3", "boxing_v2", "combat_jet_v1", "combat_tank_v3", "double_dunk_v3", "entombed_competitive_v3", "flag_capture_v2",
"foozpong_v3", "ice_hockey_v2", "joust_v3", "mario_bros_v3", "maze_craze_v3", "othello_v3", "pong_v3", "quadrapong_v4", "space_war_v2",
"surround_v2", "tennis_v3", "video_checkers_v4", "volleyball_pong_v2", "warlords_v3", "wizard_of_wor_v3", "knights_archers_zombies_v10",
"chess_v6", "connect_four_v3", "gin_rummy_v4", "go_v5", "hanabi_v5", "leduc_holdem_v4", "rps_v2", "texas_holdem_no_limit_v6",
"texas_holdem_v4", "tictactoe_v3", "simple_v3", "simple_adversary_v3", "simple_crypto_v3", "simple_push_v3", "simple_reference_v3",
"simple_speaker_listener_v4", "simple_spread_v3", "simple_tag_v3", "simple_world_comm_v3", "multiwalker_v9", "pursuit_v4", "waterworld_v4".
â€¢ seed specifies the random sequence of the first environment state which is helpful for reproducible evaluations. The default value is 1.
â€¢ time_limit is the maximum number of steps before episode termination. The default value is 900 for all Butterfly tasks except from
Pistonball, which is 125, and 10000 for all Atari tasks. For all SISL tasks the default value is 500, for all MPE tasks is 25,
â€¢ partial_observation defines whether to use partial observations or not. Only applicable in "space_invaders_v2" and "entombed_cooperative_v3".
The default value is False.
â€¢ trainable_cnn defines whether to encode the image observations using a pre-trained model or to return the raw images. The default
value is False.
â€¢ image_encoder is the selected image-encoder to use for encoding the image observations. Only applicable if trainable_cnn is True.
Options: "ResNet18", "SlimSAM", "CLIP". The default value is "ResNet18".
â€¢ image_encoder_batch_size is the number of images to encode at once with the selected image-encoder. Only applicable if trainable_cnn
is True. The default value is 10.
â€¢ image_encoder_use_cuda defines whether to use GPU or CPU for the selected image-encoder. In this way, the policies networks can be
in a different device than the image-encoder. The default value is True, but it automatically turns to True if no GPU available is found.
â€¢ centralized_image_encoding defines whether to encode images in a centralized manner, thus avoiding to load as many image-encoders
as the parallel environment processes, reducing the GPU memory required. However, this is only supported by our training API,
where multiple parallel processes can be managed. When it is used only with the environment API, the effect is that the raw image
observations are returned instead of the encoded representations.
â€¢ kwargs can consist of specific arguments supported by the selected task2 . The user can provide the arguments in following format:
"(â€™arg1â€™,arg1_value), (â€™arg2â€™,arg2_value), ...". For instance, we can specify the number of pistons in the pistonball_v6 task using:
"(â€™n_pistonsâ€™,4),". The default value is "", that is, no arguments provided.
Overcooked args:
args = {
" env " : " overcooked " ,
" env_args " : {
" key " : " cramped_room " ,
}
}

Figure 6: Example of minimal args for Overcooked tasks.
2 All the arguments of each task can be found on the official page of PettingZoo: https://pettingzoo.farama.org/

args = {
" env " : " overcooked " ,
" env_args " : {
" key " : " cramped_room " ,
" seed " : 1 ,
" time_limit " : 500 ,
" reward_type " : " sparse "
}
}

Figure 7: Example of all args for Overcooked tasks.
where:
â€¢ key is the selected Overcooked scenario. Options: "cramped_room", "asymmetric_advantages", "coordination_ring", "counter_circuit",
"forced_coordination".
â€¢ seed has the same functionality as in PettingZoo. The default value is 1.
â€¢ time_limit has the same functionality as in PettingZoo. The default value is 500.
â€¢ reward_type defines whether to provide sparse rewards, that is, only when an order is ready, or shaped rewards, that is, each time a
component of an order is ready. Options: "sparse", "shaped". The default value is "sparse".
Pressure Plate args:
args = {
" env " : " pressureplate " ,
" env_args " : {
" key " : " pressureplate - linear -4 p - v0 " ,
}
}

Figure 8: Example of minimal args for Pressure Plate environment.

args = {
" env " : " pressureplate " ,
" env_args " : {
" key " : " pressureplate - linear -4 p - v0 " ,
" seed " : 1 ,
" time_limit " : 500 ,
}
}

Figure 9: Example of all args for Pressure Plate environment.
where:
â€¢ key is the selected Pressure Plate scenario. Options: "pressureplate-linear-4p-v0", "pressureplate-linear-5p-v0", "pressureplate-linear6p-v0".
â€¢ seed has the same functionality as in PettingZoo. The default value is 1.
â€¢ time_limit has the same functionality as in PettingZoo. The default value is 500.
Capture Target args:
args = {
" env " : " capturetarget " ,
" env_args " : {
" key " : " CaptureTarget -6 x6 -1 t -2 a - v0 " ,
}
}

Figure 10: Example of minimal args for Capture Target environment.

args = {
" env " : " capturetarget " ,
" env_args " : {
" key " : " CaptureTarget -6 x6 -1 t -2 a - v0 " ,
" seed " : 1 ,
" time_limit " : 60 ,
" obs_one_hot " : False ,
" target_flick_prob " : 0.3 ,
" tgt_avoid_agent " : True ,
" tgt_trans_noise " : 0.0 ,
" agent_trans_noise " : 0.1 ,
}
}

Figure 11: Example of all args for Capture Target environment.

where:
â€¢ key is the selected Capture Target scenario. Options: "CaptureTarget-6x6-1t-2a-v0".
â€¢ seed has the same functionality as in PettingZoo. The default value is 1.
â€¢ time_limit has the same functionality as in PettingZoo. The default value is 60.
â€¢ obs_one_hot defines whether to return the observations in one-hot encoding format. The default value is False.
â€¢ target_flick_prob specifies the probability of not observing the target. The default value is 0.3.
â€¢ tgt_avoid_agent defines whether the target keeps moving away from the agents or not. The default value is True.
â€¢ tgt_trans_noise specifies the targetâ€™s transition probability for arriving an unintended adjacent cell. The default value is 0.0.
â€¢ agent_trans_noise specifies the agentâ€™s transition probability for arriving an unintended adjacent cell. The default value is 0.1.
Box Pushing args:
args = {
" env " : " boxpushing " ,
" env_args " : {
" key " : " BoxPushing -6 x6 -2 a - v0 " ,
}
}

Figure 12: Example of minimal args for Box pushing environment.

args = {
" env " : " boxpushing " ,
" env_args " : {
" key " : " BoxPushing -6 x6 -2 a - v0 " ,
" seed " : 1 ,
" time_limit " : 60 ,
" random_init " : True
}
}

Figure 13: Example of all args for Box Pushing environment.

where:
â€¢ key is the selected Box Pushing scenario. Options: "BoxPushing-6x6-2a-v0".
â€¢ seed has the same functionality as in PettingZoo. The default value is 1.
â€¢ time_limit has the same functionality as in PettingZoo. The default value is 60.
â€¢ random_init shows whether random initial position of agents should be applied. The default value is True.
LBF, RWARE, MPE args:

args = {
" env " : " gymma " ,
" env_args " : {
" key " : " mpe : SimpleSpeakerListener - v0 " ,
}
}

Figure 14: Example of minimal args for LBF, RWARE, and MPE environments.

args = {
" env " : " gymma " ,
" env_args " : {
" key " : " mpe : SimpleSpeakerListener - v0 " ,
" seed " : 1 ,
" time_limit " : 500 ,
}
}

Figure 15: Example of all args for LBF, RWARE, and MPE environments.
where:
â€¢ key is the selected scenario of LBF, RWARE, or MPE environments.
Options for LBF:
"lbforaging:Foraging-4s-11x11-3p-2f-coop-v2", "lbforaging:Foraging-2s-11x11-3p-2f-coop-v2", "lbforaging:Foraging-2s-8x8-3p-2f-coopv2", "lbforaging:Foraging-2s-9x9-3p-2f-coop-v2", "lbforaging:Foraging-7s-20x20-5p-3f-coop-v2", "lbforaging:Foraging-2s-12x12-2p-2fcoop-v2", "lbforaging:Foraging-8s-25x25-8p-5f-coop-v2", "lbforaging:Foraging-7s-30x30-7p-4f-coop-v2".
Options for RWARE:
"rware:rware-small-4ag-hard-v1", "rware:rware-tiny-4ag-hard-v1", "rware:rware-tiny-2ag-hard-v1".
Options for MPE:
"mpe:SimpleSpread-3-v0", "mpe:SimpleSpread-4-v0", "mpe:SimpleSpread-5-v0", "mpe:SimpleSpread-8-v0", "mpe:SimpleSpeakerListenerv0", "mpe:MultiSpeakerListener-v0".
â€¢ seed has the same functionality as in PettingZoo. The default value is 1.
â€¢ time_limit has the same functionality as in PettingZoo. The default value is 50 for LBF, 500 for RWARE, and 25 for MPE.

B

INSTALLATION

In this section, we provide the bash command needed for installing the PyMARLzoo+ environment below in Figure 16, assuming that Python
>= 3.8 is installed on the machine. This minimal installation step is intended to facilitate a smooth setup process, thereby enabling users to
access and utilize the frameworkâ€™s features efficiently and with ease.
$ pip install pymarlzooplus

Figure 16: Bash command to install PyMARLzoo+ package using the pip tool.

C

TRAINING API

The training framework of (E)PyMARL has been enhanced to support the integration of new algorithms and environments. We introduced
a novel category of algorithm modules, termed explorers, which integrate the exploration components of the CDS and EOI algorithms.
This integration facilitates a structured and transparent code repository that accommodates both general-purpose and exploration-specific
algorithms. Furthermore, the widely used Prioritized Replay Buffer [44] has been universally integrated into all off-policy algorithms,
promoting efficient experimentation without necessitating code modifications. Figure 17 illustrates command examples for training an
algorithm (<algo>) on each environment task, demonstrating the minimal user effort required. The available options for each <algo> and
<key> and <time_limit> are described in Appendix A. Examples of usage of the extra arguments for PettingZoo and Overcooked are provided
in Figure 18, in which 10 pistons and sparse reward type has been used for each environment, respectively.

LBF, RWARE, MPE
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g =gymma w i t h e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t > e n v _ a r g s . key =<key >

PettingZoo
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g = p e t t i n g z o o w i t h e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t > e n v _ a r g s . key =<key > e n v _ a r g s . kwargs =< kwargs >

Overcooked
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g = o v e r c o o k e d w i t h e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t > e n v _ a r g s . key =<key > e n v _ a r g s . r e w a r d _ t y p e =< r e w a r d _ t y p e >

PressurePlate
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g = p r e s s u r e p l a t e w i t h e n v _ a r g s . key =<key > e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t >

Capture Target
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g = c a p t u r e t a r g e t w i t h e n v _ a r g s . key =<key > e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t >

Box Pushing
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =< a l g o > âˆ’âˆ’env âˆ’ c o n f i g = b o x p u s h i n g w i t h e n v _ a r g s . key =<key > e n v _ a r g s . t i m e _ l i m i t =< t i m e _ l i m i t >

Figure 17: Example Bash commands for training with different algorithms in different environment tasks. The <algo>, and
<key> correspond to the available options as mentioned in Appendix A. The <time_limit> field corresponds to the maximum
number of steps before episode termination, therefore it should be an integer.
PettingZoo with 10 pistons in the Pistonball task for training QMIX with time limit 900 steps.
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =qmix âˆ’âˆ’env âˆ’ c o n f i g = p e t t i n g z o o w i t h e n v _ a r g s . t i m e _ l i m i t =900 e n v _ a r g s . key = " p i s t o n b a l l _ v 6 " e n v _ a r g s . kwargs = " ( ' n _ p i s t o n s ' , 1 0 ) , "

Overcooked with sparse reward type in Cramped room scenario for training QMIX with time limit 900 steps.
$ p y t h o n 3 s r c / main . py âˆ’âˆ’ c o n f i g =qmix âˆ’âˆ’env âˆ’ c o n f i g = o v e r c o o k e d w i t h e n v _ a r g s . t i m e _ l i m i t =900 e n v _ a r g s . key = " cramped_room " e n v _ a r g s . r e w a r d _ t y p e = " s p a r s e "

Figure 18: Example Bash commands for training QMIX algorithm in PettingZoo and Overcooked environment tasks.

D MULTI-AGENT REINFORCEMENT LEARNING ENVIRONMENTS
D.1 Multi-Agent Particle Environment Details
For our experiments in the MPE environment, we focused on the Spread environment, which corresponds to the cooperative navigation
environment, as described in the paper by Lowe et al. [32]. An example of the environment task is visualized in Figure 19.

Figure 19: Sample snapshot of the Spread MPE environment.
D.1.1

Observations.

The environment involves N agents and N landmarks, where the agents are tasked with navigating to and covering the landmarks while
preventing collisions with each other. Each agent observes its velocity and position, the relative position of other landmarks and agents, and
lastly, the communication from all other agents.
Oğ‘– = (ğ‘£ğ‘’ğ‘™ğ‘œğ‘ğ‘–ğ‘¡ğ‘¦_ğ‘¥, ğ‘£ğ‘’ğ‘™ğ‘œğ‘ğ‘–ğ‘¡ğ‘¦_ğ‘¦, ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¥, ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¦, ğ‘ Ã—(ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¥_ğ‘™ğ‘ğ‘›ğ‘‘ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘ , ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¦_ğ‘™ğ‘ğ‘›ğ‘‘ğ‘šğ‘ğ‘Ÿğ‘˜ğ‘ ),
(ğ‘ âˆ’ 1)Ã—(ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¥_ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ _ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡, ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’_ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¦_ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ _ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡), (ğ‘ + 1)Ã—ğ‘œğ‘›ğ‘’_â„ğ‘œğ‘¡_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’)
An example of an agentâ€™s observation space is provided below for the "SimpleSpread-3-v0" task:
array ([ -0.5
-1.0116129
-1.5796437

0.
-0.4476194
-0.8132916

0.8486883
-1.7624141
0.

-0.03767432 -0.44009647 0.24901201 ,
0.89627826 0.09657926 -0.60416883 ,
0.
0.
0.
] , dtype = float32 ) ,

This example observation vector consists of a vector, with 18 values, as it corresponds to N = 3 agents and landmarks. For N = 4 agents
and landmarks, the observation vector would contain 24 values per agent; for N = 5, it would contain 30 values, and so on.
D.1.2 Actions.
The agentâ€™s action space includes the five standard actions that an agent ğ‘– can choose from:
ğ´ğ‘– = (No action, Move left, Move right, Move down, Move up)

D.1.3 Rewards.
Each agent is awarded jointly based on the distance the agent is closest to every landmark. The distance is calculated based on the sum of
minimum distances. When agents run into one another, they are penalized locally with -1 for each collision.

D.2

Multi-Robot Warehouse (RWARE) Environment Details

The Multi-Robot Warehouse (RWARE) environment [36], designed based on real-world scenarios, depicts a warehouse that includes robots
operating and gathering shelves, transferring them and the ordered items to a specific location, the workstation. Robots move the items back
to empty shelf locations after humans have obtained the contents of a shelf. This cooperative environment has different settings that can be
configured, like the size, the number of agents, the communication, and reward options. In our experiments, the tiny (10x11), and small (10x20)
warehouse sizes were used, leveraging two and four agents on the â€™hardâ€™ difficulty level. A sample snapshot of the "rware-small-4ag-hard-v1"
task is given in Figure 20.

Figure 20: Sample snapshot of small size RWARE environment with four agents.
D.2.1 Observations.
In this environment, each agent observation space is partially observable and includes information about the 3Ã—3 grid, centered around itself,
that an agent can observe. The information that is observed incorporates the agentâ€™s own state (position, rotation, load) and the relative
state of nearby agents and shelves. Specifically, the observation space for each agent ğ‘– can be described as a vector containing the following
elements:
ğ‘‚ ğ‘– = (position_x, position_y, carrying_shelf, direction_one_hot, path_restricted, grid_info)
where:
â€¢ position_x, position_y: The agentâ€™s current position in x and y coordinates.
â€¢ carrying_shelf: A binary value (1 or 0) showing whether the agent is carrying a shelf.
â€¢ direction_one_hot: Four values of the agentâ€™s current direction (up, down, left, right) for each item in one-hot encoding.
â€¢ path_restricted: A binary value (1 or 0) indicating if the agent is on a path where shelves cannot be placed.
â€¢ grid_info: Information about the 3Ã—3 grid surrounding the agent, split into 9 groups (one for each cell). The visibility range can be
adjusted as well. Each group contains 7 elements:
â€“ A binary value indicating if an agent exists in the cell.
â€“ Four values in one-hot encoding, representing the direction of any agent in the cell.
â€“ A binary value showing the presence of a shelf.
â€“ A binary value describing if the shelf is requested for delivery.
An example of an agentâ€™s observation array on the "rware-small-4ag-hard-v1" task with a 3Ã—3 grid is provided below:
array ([5. 5. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. ,
0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. ,
0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.] , dtype = float32 ) ,

D.2.2 Actions.
The action space of this environment for each agent is comprised of four discrete possible options. For each agent i, the action space is the
following:
ğ´ğ‘– = (Turn left, Turn right, Forward, Load/Unload shelf)

Each agent can only rotate and move forward with the first three actions. The last action, relating to loading and unloading a shelf, can be
performed only when a robot is in a specific location, below a shelf.
D.2.3 Rewards.
In every timestep, a fixed number of shelves (R) is requested to be transferred to a target location, and new shelves are randomly selected
and included in the current requests. The reward value of an agent is 1 when it correctly delivers the ordered shelf. A difficulty that arises for
the agents is to both complete the deliveries and find an empty spot to return the previously delivered shelf. Since multiple actions are
needed between the shelf delivery by the agents, the reward signal is sparse.

D.3

PettingZoo Environment Details

For the PettingZoo environments[49], we have mainly used the tasks from Atari and Butterfly environments, the Entombed Cooperative, the
Pistonball, and the Cooperative Pong. In the exploration game Entombed Cooperative, the agents must assist each other cooperatively to get
through the evolving maze as far as they can. In the cooperative Pistonball physics game, the goal is to activate the pistons (agents) that
move vertically so as to navigate the ball to the left wall of the game border. The Cooperative Pong game is a collaborative version of a pong
game, where two agents (paddles) work together to continue playing with the ball for the longest period. Snapshot visualizations of these
three fully cooperative environment games are illustrated in Figure 21.

(a) Pistonball

(b) Cooperative Pong

(c) Entombed Cooperative

Figure 21: Sample snapshots of the three PettingZoo game environments.
D.3.1 Observations.
Pistonball The agentâ€™s observation space is composed of RBG values representing an image that illustrates the area occupied by the two
pistons or the wall adjacent to the agent. An example of image-based observation of an agent (piston) can be found in Figure 22 which is
composed of RGB values and is of total size (457, 120, 3). In case the image encoding technique is used, as described in the paper, the image
observations are transformed into vectors. In that case, an example observation vector of a piston in the environment with 10 agents is given
as a vector of size (512,) below:
array ([7.91885853 e -01
1.37508601 e -01
...
1.01920377 e -04
1.05303168 e +00

0.00000000 e +00 7.57015705 e -01 7.02866971 e -01 ,
0.00000000 e +00 9.95369107 e -02 9.06493664 e -02 ,

2.64243525 e -03 2.42827028 e -01 1.12921096 e -01 ,
3.09428126 e -01 1.34141669 e -01 2.01561041 e -02] , dtype = float32 ])

Figure 22: Sample image-based observation of a piston in the Pistonball environment of PettingZoo with 10 agents.

Cooperative Pong The observation space of each agent in this game is the entire screen of the game. An example of image-based
observation of an agent (paddle) can be seen in Figure 21b which is composed of RGB values and is of total size (280, 480, 3).
In case the image encoding technique is utilised, the image observations are transformed into vectors. An example observation vector of
one of the paddles in the environment is given as a vector of size (512,) below:
array ([4.37711984 e -01
3.02959293 e -01
...
3.57522756 e -01
9.99535978 e -01

5.60766220 e -01 1.59607649 e +00 1.11085367 e +00 ,
3.26711655 e -01 5.47930487 e -02 9.35874805 e -02 ,

3.15122038 e -01 3.19070041 e -01 4.04478341 e -01 ,
1.10822819 e -01 2.30887890 e -01 4.81555223 e -01] , dtype = float32 ])

Entombed Cooperative In this game, each agentâ€™s observations are the RGB values - the entire screen of the game. An example of
image-based partial observations of the two agents can be seen in Figure 23 which are composed of RGB values and are of total size (210, 160,
3).
In case the image encoding technique is utilized, the image observations are transformed into vectors. An example observation vector of
one of the agents in the environment is given as a vector of size (512,) below:
array ([1.31586099 e +00
2.60396451 e -01
...
1.10996671 e -01
1.32799280 e +00

4.78404522 e -01 1.17896628 e +00 4.77674603 e -01 ,
8.15561116 e -02 2.70262837 e -01 3.13673377 e -01 ,

1.78684831 e -01 7.65883446 e -01 8.93630207 e -01 ,
8.94335330 e -01 6.74108326 e -01 4.32223547 e -03] , dtype = float32 ])

(a) Agent 1

(b) Agent 2

Figure 23: Sample image-based partial observations of each agent in the Entombed Cooperative environment of PettingZoo.
Note that in our enhanced version, we implemented a method to mitigate flickering issues, which involves combining images from
sequential frames. Specifically, our wrapper merges images from two consecutive frames to stabilize the representation of agent positions,
thereby addressing the flickering problem. This process is repeated for four sequential steps to ensure consistency across frames. Our
method identifies the position of Agent 1 in each frame by detecting specific RGB values and determines which frame depicts the agent
more prominently. It then creates a combined image where the confirmed position of Agent 1 is reinforced. Additionally, for scenarios
requiring partial observations, our wrapper generates separate observations for each agent by removing the visual representation of the
other agent from the combined image. This approach not only stabilizes visual inputs but also supports scenarios where agents receive only
their individual perspectives, enhancing the taskâ€™s applicability for studies on cooperative perception and action under partial observability
conditions.
D.3.2 Actions.
Pistonball The actions can be given either in discrete or continuous mode. In our experiments, the discrete action space has been used in
which three actions are available for each agent i:
ğ´ğ‘– = (Move down, Stay still, Move up)
If the actions are continuous, then the range value is [-1, 1], scaled by four, and is linked to how much the pistons are moved up or down.
In that way, action 1 will raise a piston by four pixels and -1 lower by four pixels.
Cooperative Pong The discrete action space of each agent is composed by two actions. Each agent can be moved either up or down.
Therefore, the action space for each agent ğ‘– is the following:
ğ´ğ‘– = (Move up, Move down)

Entombed Cooperative The action space of each agent is composed of 18 options. In an agentâ€™s turn, the actions that can be taken are
the following:
ğ´ğ‘– = (No operation, Fire, Move up, Move right, Move left, Move down, Move upright, Move up left, Move downright, Move down left,
Fire up, Fire right, Fire left, Fire down, Fire up right, Fire up left, Fire down right, Fire down left)
D.3.3 Rewards.
Pistonball The reward of each agent is composed of the overall amount of leftward movement of the ball and the leftward movement if it
was close to the piston. In that way, each piston can take a global reward, while also a local reward is applied to those agents that are close to
the ball, meaning positioned under any area of the ball. The local reward is computed as:
local_reward = 0.5 Ã— Î”ğ‘¥ ball
where Î”ğ‘¥ ball represents the change in the ballâ€™s x-position. The global reward is calculated as:

global_reward =


Î”ğ‘¥ ball
Ã— 100 + time_penalty
ğ‘¥ start

where ğ‘¥ start is the starting position of the ball and time_penalty is a default value of -0.1. Overall, the reward for each agent is calculated
as :
ğ‘…ğ‘– = local_ratio âˆ— local_reward + (1 âˆ’ local_ratio) âˆ— global_reward.
Cooperative Pong At each time step, the reward for each agent ğ‘– when the ball remains successfully within the bounds is calculated as:
Rğ‘– =

max_reward
max_cycles

where the default value is set to 0.11. Alternatively, if the ball goes out of bounds, the agent receives a penalty determined by the parameter
off_screen_penalty, which by default is set to -10. Upon this event, the game terminates.
Entombed Cooperative The reward, identical to every single agent, in this environment, is given immediately after altering a section or
resetting a stage, where each stage is composed of 5 sections. The reward could be received only when an agent loses a life, but not when it
loses the last life as the game ends without the option of stage resetting.

D.4

Level-Based Foraging (LBF) Environment Details

The Level-Based Foraging [3] environment is a multi-agent reinforcement learning environment built on OpenAIâ€™s RL framework [5]. It
features multiple agents operating within a grid-world setting, where their shared objective is to collect as much food as possible. The
success of each agent in picking up food is governed by its level of compatibility with the task. The nature of the agents can be either
competitive or cooperative in order to reach the goal. Success requires a balance of both to achieve optimal rewards. For our experiments, we
analyzed different configurations, varying the amount of players, food, grid size as well as observation radius.
D.4.1 Observations.
In this environment, each agent has to navigate to the food scattered in the grid-world map and collect as many as possible. However, an
agent can only pick up one food item, if its level is less than or equal to the combined levels of all collaborating agents. The game is succeeded,
if all such food items are collected. Each agent has knowledge either the full state or, if specified, a partial state of the environment. This
includes the positions and levels of all entities in the map. The observation array contains N subarrays, each being the observation array of
agent ğ‘– respectfully. The observation array of each agent ğ‘– contains info on all existing food instances and agents, including themselves. The
info for each observed instance is a triplet of the form:
ğ‘‚ ğ‘– = (absolute_position_x, absolute_position_y, level)
The final observation space for each agent is of size: (x, y, level) Ã— food_count + (x, y, level) Ã— player_count = 3 Ã— (food_count + player_count)
If a food is picked up or another player is not found yet, the triplet value for that object is set as (-1, -1, 0 ).
Below is an example of an observation vector of an agent, for the environment "Foraging-4s-11x11-3p-2f-coop-v2":
array ([ 2. ,
2. ,

0. ,
3. ,

4. , -1. , -1. ,
1. , -1. , -1. ,

0. , 2. , 4. , 2. ,
0.] , dtype = float32 ) ,

The three values in the array correspond to the triplet (position_x, position_y, level) of the first food item on the map, as observed by the
agent. In this specific configuration, the environment consists of 2 food items and 3 players. Therefore, the first two triplets correspond to
the food, while the rest represent the players. The second triplet of the array in particular is out of sight, as indicated by the initialized empty
values (-1, -1, 0).

D.4.2 Actions.
For this environment, the agent can perform 6 discrete actions. At each time step, the environment returns an action list, which contains all
actions taken by the agents. Each action an agent ğ‘– can take is one of:
ğ´ğ‘– = (Noop, Move north, Move south, Move west, Move east, Pick-up)
From the actions above, "Noop" action signifies the agent remaining idle, while "Pick-up" action allows the agent to collect food from an
adjacent cell. The remaining actions result in the agentâ€™s moving in the corresponding direction on the 2D grid.
D.4.3 Rewards.
An agent is rewarded only if they have collected at least one food item. When a food item is picked up, all agents adjacent to the food cell are
rewarded accordingly, thus underlying the importance of cooperation in the game.
The reward given to each depends on the level of each participant agent so that higher-level agents get higher rewards for the same food
picked up. The reward ğ‘– for each participant agent ğ‘– is calculated by the level of the food item picked up weighted by the level of the agent.
This is then normalized by a factor of the sum of the other adjacent agent levels multiplied by the total number of foods that spawned in the
current game. All rewards are normalized so that all individual agentâ€™s rewards sum to one.

D.5

Overcooked Environment Details

In our experiments, we evaluated the performance of the proposed algorithms using the Overcooked environment, as described in [7]. The
environment is a two-agents cooperative game, where the goal is to prepare and deliver various types of soups as efficiently as possible. The
game takes place in a grid-based world, where each cell represents a distinct entity. The challenge is the ability of the agents to coordinate
effectively, manage resources and optimize task execution. The agentsâ€™ actions include navigating the grid, collecting ingredients, preparing
the soups, and finally delivering them to a designated service area. The recipes available in the game involve two different ingredients, which
can be combined in varying orders depending on the specific soup being prepared.
D.5.1 Observations.
In our experiments, we utilized a variety of environmental layouts to test different cooperation challenges. Specifically, we evaluated agent
performance across three distinct layouts: "Cramped room", "Asymmetric advantages", and "Coordination ring". Each room focuses on
different cooperation challenges.
For the "Cramped room", the agents are positioned within a a single shared space. However, the grid layout is highly restrictive, so the
environment tests the agentsâ€™ ability to coordinate in confined spaces.
For the "Asymmetric advantages", the layout is divided into separate rooms with each agent positioned in each one. This configuration
tests the agentsâ€™ high-level strategy decision-making ability. Successful coordination in this scenario relies on each agentâ€™s capacity to
optimize its actions based on the position and current actions of its co-players, highlighting the importance of inter-agent awareness and
strategic planning.
In the "Coordination ring", similar to the cramped room layout, both agents are placed in a shared space but in a more spacious room.
However, the pot (element needed for main interaction) is positioned at the top right corner of the layout, while all other interactions are
positioned in the left bottom corner. The agents are tested whether they can coordinate and navigate the environment, ensuring they can
execute distant but interconnected actions in a timely and efficient manner.
The three layouts can be seen in Figure 24, referenced by [7].

Figure 24: Sample snapshot of the Overcooked layouts. On the left is, "Cramped room", followed by the "Asymmetric advantages"
and on the right is the "Coordination ring" layout.
In each timestep, both agents have full knowledge of the state of the environment. Each agentâ€™s observation space contains their position
as well as the encoding of the next state. An overview of the observation matrix of an agent ğ‘– is :
ğ‘‚ ğ‘– = (player_i_features, other_player_features, player_i_dist_to_other_players, player_i_position)
As observed from the above, their observation matrix contains both their own and their co-playersâ€™ features, which is mostly their positional
information to all objects in their environment. Specifically, according to the source code [13], each playerâ€™s features is a flattened list which
contains:

â€¢ orientation: one-hot-encoding of the direction the player is currently facing. Possibilities are north, south, east, and west. This vector
length is 4.
â€¢ obj: one-hot-encoding of the object currently being held. If nothing held, all 0s. Possibilities are onion, soup, dish, and tomato. This
vector length is 4.
â€¢ wall_j: 0, 1 boolean value of whether the player has a wall immediately in direction j. This vector length is 4.
â€¢ closest_{onion|tomato|dish|soup|serving|empty_counter}: (dx, dy) the distance to the item on both axes. (0, 0) if it is currently held.
This vector length is 6 âˆ— 2.
â€¢ closest_soup_n_{onions|tomatoes}: an int indicating how many of the specified ingredients are inside the closest soup This vector
length is 2.
â€¢ closest_pot_j_exists: 0, 1, boolean value if there is a pot in the j direction. If not, then all the next pot features are 0. This vector length
is 1.
â€¢ closest_pot_j_{is_empty|is_full|is_cooking|is_ready}: 0, 1 boolean value for each pot statuses. O if there is no pot in j direction. This
vector length is 4.
â€¢ closest_pot_j_{num_onions|num_tomatoes}: int value indicating the number of each ingredient in jth closest pot. O if there is no pot
in j direction. This vector length is 2.
â€¢ closest_pot_j_cook_time: int value indicating seconds remaining until soup is ready. -1 if there is no soup cooking. This vector length
is 1.
â€¢ closest_pot_j: (dx, dy) distance on both axes to the pot in the j direction. (0, 0) of no pot on j direction. This vector length is 2.
Note that the final number of closest_pot features is multiplied by the number of total pots in the environment, which is 2. So the total
âˆ¥ğ‘‚ ğ‘– âˆ¥ = player_i_features + other_player_features + player_i_relative_position + player_i_absolute_position
= (2 * pot_features + player_i_other_features) + (2 * pot_features + other_player_other_features) + 2 + 2
= 2 âˆ— 20 + 56 = 96
An example of the total observation array of a sampled time step is given below:
array ([ 0. ,
1. ,
0. ,
0. ,
0. ,
2. ,
0. ,
0. ,

1. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 1. ,
0. , 0. ,
0. , 0. ,
2. , -1. ,

0. ,
0. ,
0. ,
0. ,
0. ,
1. ,
0. ,
1. ,

0. , 0. , 0. , 0. , -1. , -1. ,
0. , 2. , 1. , 0. , 0. , 1. ,
1. , -2. , 0. , 0. , 0. , 0. ,
1. , 0. , 1. , 1. , 0. , 0. ,
0. , 0. , -2. , 2. , 0. , 0. ,
1. , 0. , 0. , 0. , 0. , 0. ,
0. , 0. , 0. , 0. , 0. , 0. ,
2.] , dtype = float32 ) ,

0. , 0. , 0. ,
1. , 0. , 0. ,
0. , 0. , 0. ,
0. , 0. , 0. ,
0. , 0. , 0. ,
0. , -1. , -1. ,
1. , 0. , 1. ,

This array contains the observation space, which is of size (96), that belongs to a participating agents. All the values of the observation array
also correspond to the fields explained earlier, which correspond to their features, the other playerâ€™s features, their relative position, and
lastly their absolute position on the grid world. For example, the first 4 values {0, 1, 0, 0} represent agentâ€™s orientation in one-hot encoding.
This means the agent is facing south. The next 4 numbers {0, 0, 0, 0} correspond to the one-hot encoding of the object the agent holds, and
since all are zero values, the agent does not hold any object. The rest of the values can be interpreted as explained in the individual fields of
the playerâ€™s features. Finally, the last 4 values correspond to the relative and absolute position. So (2, âˆ’1) means that the 2nd agent is two
positions on the x-axis further, but 1 position on the y-axis below. The last 2 values (1, 2) show that the agent is in position (1, 2) of the grid.
D.5.2 Actions.
At each time step, the actions an agent can take correspond to the available directions they can move to. The agent takes 6 discrete actions as
seen below:
ğ´ğ‘– = (Move north, Move south, Move east, Move west, Noop, Interact)
The last action is not related to navigation of the agent, but if selected it interacts with a nearby object.
D.5.3 Rewards.
The reward types available are shaped or sparse and are jointly distributed to both agents. In our experiments, we have selected the sparse
type, which is given to the agents only when a soup is delivered. Once it is delivered, an agent is rewarded 0 points, if the soup is not made
according to the available recipes for the current game. If the recipe of the soup is in the bonus orders, it receives a weighted reward of the
base score value from the recipe multiplied by the additional bonus score of 2. Otherwise, the agent is rewarded with the calculated base
score of 20. While the option for bonus orders is available, our selected layouts do not include any. So the final reward for each agent is 0 if
the soup was not created with the accepted number and order of ingredients, otherwise, the agent is rewarded 20 points.

D.6

Pressureplate Environment Details

In our experiments, we utilized the Pressureplate environment [52], which is a grid-world environment requiring agent cooperation. To
progress and succeed, agents must collaborate by ensuring that the agent with the corresponding ID remains on a pressure plate in each

room, which unlocks the door for the remaining agents to move to the next room. This process continues until all agents are on their
respective plates, and the final agent reaches the treasure chest, which signifies the goal in the last room. sample snapshot of the Pressureplate
environment can be seen in Figure 25.
D.6.1 Observations.
In each new level, the grid map is divided into rooms by strategically placed walls, with each room containing a pressure plate and a
corresponding locked door. At the start of each game, agents spawn in the southernmost room, with each assigned a unique ID that matches
one of the plates. Agents have partial observability of the environment, depending on its sensor range, specified by the parameter "sight". An
example of an agentâ€™s observation array at a sampled time-step is given as:
array ([ 0. ,
0. ,
0. ,
1. ,
0. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
1. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
1. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

1. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

1. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,
0. ,

0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
5. , 13.] ,

1. , 1. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
0. , 0. ,
dtype = float32 ) ,

The observation array of each agent contains a combination of 5 flattened observation sub-arrays of dimensions ğ‘ ğ‘–ğ‘”â„ğ‘¡ Ã— ğ‘ ğ‘–ğ‘”â„ğ‘¡ each. Each
observation sub-array represents the separate layout of a different element in the environment and the grid that is observable by the agent
around them. These elements include the positions of other agents, walls, plates, doors, and goal (treasure chest), giving a total of 5 distinct
observation sub-arrays per agent. If an element of the selected layout exists in one of these grid cells, then the value of that cell position on
the respective sub-array will be 1, otherwise 0. After the layout sub-arrays are constructed, they are flattened to a final observation array of

Figure 25: Snapshot of the Pressureplate environment.
the agent. Finally, the agentâ€™s position coordinates (ğ‘¥, ğ‘¦) are concatenated in the end, and thus form the final observation array.
The total observation arrayâ€™s dimension is :
âˆ¥ğ‘‚ ğ‘– âˆ¥ = ğ‘ âˆ— (5 âˆ— (ğ‘ ğ‘–ğ‘”â„ğ‘¡ Ã— ğ‘ ğ‘–ğ‘”â„ğ‘¡) + 2)
where N is the number of agents.
D.6.2 Actions.
At each time step, each agent ğ‘– can select from 5 possible discrete actions, corresponding to their navigation on the grid-world map. These
possible actions are:
ğ´ğ‘– = (Move up, Move down, Move left, Move right, Noop)
From the above, the last action represents idleness while the rest results in moving the agent by one cell in the chosen direction.
D.6.3 Rewards.
Reward is independent of the actions of other participating agents. At each time step, the distance between each agentâ€™s position and their
corresponding pressure plate is calculated and their reward is then assigned to them, based on this distance. If the agent is in the same room
as their respective plate, the reward will be the negative normalized Manhattan distance between the agent and the plate. However, if the
agent is in a different room, the reward is calculated based on the number of rooms separating the agent from the room that contains their
plate. Maximum rewards are given when all agents are in the correct room and the final agent has reached the treasure chest.

D.7

Capture Target Environment Details

Agents in the Capture Target environment must learn the gridâ€™s transition dynamics to achieve simultaneous target capture, as quickly
as possible. Both agents and targets move inside the grid. Agents observe the targets with a flickering probability at each time step. In
the default Capture Target setup, which is presented in Figure 26, there are 2 agents and 1 target, moving inside a 6X6 grid. The targetâ€™s
observation flickering probability is set to 0.3.

Figure 26: CaptureTarget environment. Two agents learn to capture the target simultaneously.

D.7.1 Observations.
By default, the observationsâ€™ array consists of the agentsâ€™ and their targetsâ€™ normalized positions on the grid. There is also an option for
one-hot encoded observations. Targetsâ€™ positions may differ from the actual ones, due to the flickering probability applied. In the 2 agents, 1
target scenario, the observation array per time step is of size 2ğ‘¥4. For each agent ğ‘–, the observation vector is
ğ‘‚ ğ‘– = (x_position, y_position , target_x_position , target_y_position )
An example of an agentâ€™s observation array of a sample time step in a 6Ã—6 grid is given below:
array ([ 0.4

0.4

0.4

1.6] , dtype = float32 )

D.7.2 Actions.
At each time step an agent can move in one of the available directions. An agent ğ‘– can pick one of the following actions:
ğ´ğ‘– = (Move north, Move south, Move west, Move east, No move)
D.7.3 Rewards.
In the successful scenario, where the agents capture the targets simultaneously, they are given +1 reward. At any other time step, the reward
received is 0.

D.8

Box Pushing Environment Details

The Box Pushing[56] is a cooperative robotics problem introduced by Seuken and Zilberstein [45] in which the main objective is two robots
(agents) should cooperate and push a big box to the yellow space to get higher reward than pushing the other two small boxes that exist in
the environment. In the default Box Pushing Target setup, which is depicted in Figure 27, two agents, one big box and two small boxes exist
and can move in a 6Ã—6 grid.
D.8.1 Observations.
The observation space of each agent consists of 5 elements in which the possible values are 0 and 1 indicating whether the element is in
front of an agent (1) or not (0). An overview of the observation vector of an agent ğ‘– is:
ğ‘‚ ğ‘– = (small_box, large_box, empty, wall, teammate)
An example of an agentâ€™s observation array of a sample time step in a 6Ã—6 grid is given below:
array ([0. 0. 1. 0. 0.] , dtype = float32 )

The values in this observation space show that the agent has an empty cell in front of them.

Figure 27: Snapshot of the Box Pushing environment.

D.8.2 Actions.
In every time step, an agent ğ‘– can choose one of the four available actions:
ğ´ğ‘– = (Move forward, Turn left, Turn right, Stay)
The big box can be shifted when two robots are positioned facing it in adjacent cells and move forward simultaneously. The small boxes can
be moved by one grid cell when a robot is positioned toward it and performs the move forward action.
D.8.3 Rewards.
At every time step, the agents are rewarded by -0.1, and a successful push of the large box to the yellow space gives a +100 reward. In case a
small box is pushed to the goal area, a +10 reward is returned to the agent. Unsuccessful scenarios that give a -5 penalty, are considered
when an agent hits the boundary or when an agent alone tries to push the big box.

E

ADDITIONAL RESULTS

Figure 28: Episodic rewards of all 11 algorithms in LBF tasks rendering the mean and the 75% confidence interval over 5 different
seeds.

Figure 29: Episodic rewards of all 11 algorithms in MPE tasks rendering the mean and the 75% confidence interval over 5
different seeds.

Figure 30: Episodic rewards of all 11 algorithms in RWARE tasks rendering the mean and the 75% confidence interval over 5
different seeds.

Figure 31: Episodic rewards of all 11 algorithms in PettingZoo environments rendering the mean and the 75% confidence
interval over 5 different seeds.

Figure 32: Episodic rewards of all 11 algorithms in Overcooked environments rendering the mean and the 75% confidence
interval over 5 different seeds.

Figure 33: Episodic rewards of all 11 algorithms in Pressure Plate tasks rendering the mean and the 75% confidence interval
over 5 different seeds.

Algorithms\Environments
QMIX
QPLEX
MAA2C
MAPPO
HAPPO
MAT-DEC
COMA
EOI
MASER
EMC
CDS

LBF

RWARE

Spread (MPE)

Petting Zoo

Overcooked

Pressure Plate

0.29 Â± 0.18
0.32 Â± 0.17
0.64 Â± 0.13
0.46 Â± 0.11
0.08 Â± 0.08
0.22 Â± 0.08
0.02 Â± 0.01
0.07 Â± 0.05
0.01 Â± 0.00
0.23 Â± 0.16
0.13 Â± 0.14

0.00 Â± 0.00
0.16 Â± 0.12
0.13 Â± 0.06
0.29 Â± 0.15
0.42 Â± 0.22
0.41 Â± 0.29
0.00 Â± 0.00
0.23 Â± 0.12
0.00 Â± 0.00
0.02 Â± 0.02
0.42 Â± 0.26

0.79 Â± 0.12
0.64 Â± 0.30
0.81 Â± 0.11
0.83 Â± 0.11
0.80 Â± 0.14
0.80 Â± 0.14
0.81 Â± 0.12
0.50 Â± 0.25
0.82 Â± 0.12
0.81 Â± 0.12
0.83 Â± 0.13

0.40 Â± 0.28
0.40 Â± 0.28
0.34 Â± 0.31
0.34 Â± 0.31
0.34 Â± 0.30
0.40 Â± 0.28
0.23 Â± 0.21
0.32 Â± 0.30
0.37 Â± 0.30
0.16 Â± 0.07
0.21 Â± 0.11

0.21 Â± 0.19
0.06 Â± 0.06
0.53 Â± 0.27
0.19 Â± 0.18
0.11 Â± 0.10
0.00 Â± 0.00
0.00 Â± 0.00
0.19 Â± 0.18
0.00 Â± 0.00
0.00 Â± 0.00
0.18 Â± 0.10

0.86 Â± 0.11
0.77 Â± 0.15
0.97 Â± 0.01
0.98 Â± 0.01
0.97 Â± 0.01
0.85 Â± 0.07
0.32 Â± 0.26
0.51 Â± 0.20
0.79 Â± 0.17
0.32 Â± 0.26
0.60 Â± 0.20

Table 12: Mean average episodic rewards and 75% confidence interval over each benchmark tasks for each algorithm.

Tasks\Algorithms

QMIX

QPLEX

MAA2C

MAPPO

HAPPO

MAT-DEC

COMA

EOI

MASER

EMC

CDS

2s-8x8-3p-2f
2s-9x9-3p-2f
2s-12x12-2p-2f
4s-11x11-3p-2f
7s-20x20-5p-3f
8s-25x25-8p-5f
7s-30x30-7p-4f
Spread 4
Spread 5
Spread 8
tiny-2ag-hard
tiny-4ag-hard
small-4ag-hard
Pistonball
Cooperative Pong
Entomped Cooperative
Cramped Room
Asymmetric Advantages
Coordination Ring
4p
6p

0ğ‘‘ : 7â„
0ğ‘‘ : 7â„
0ğ‘‘ : 6â„
0ğ‘‘ : 7â„
0ğ‘‘ : 9â„
0ğ‘‘ : 12â„
0ğ‘‘ : 11â„
0ğ‘‘ : 10â„
0ğ‘‘ : 12â„
0ğ‘‘ : 23â„
1ğ‘‘ : 14â„
2ğ‘‘ : 2â„
2ğ‘‘ : 2â„
2ğ‘‘ : 7â„
1ğ‘‘ : 15â„
1ğ‘‘ : 4â„
1ğ‘‘ : 18â„
4ğ‘‘ : 13â„
4ğ‘‘ : 13â„
0ğ‘‘ : 20â„
1ğ‘‘ : 1â„

0ğ‘‘ : 11â„
0ğ‘‘ : 11â„
0ğ‘‘ : 11â„
0ğ‘‘ : 12â„
0ğ‘‘ : 14â„
0ğ‘‘ : 20â„
0ğ‘‘ : 18â„
0ğ‘‘ : 15â„
0ğ‘‘ : 18â„
1ğ‘‘ : 8â„
2ğ‘‘ : 3â„
3ğ‘‘ : 1â„
3ğ‘‘ : 1â„
4ğ‘‘ : 4â„
3ğ‘‘ : 17â„
2ğ‘‘ : 14â„
2ğ‘‘ : 11â„
6ğ‘‘ : 16â„
6ğ‘‘ : 6â„
1ğ‘‘ : 10â„
1ğ‘‘ : 18â„

0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 4â„
0ğ‘‘ : 8â„
0ğ‘‘ : 10â„
0ğ‘‘ : 10â„
4ğ‘‘ : 15â„
2ğ‘‘ : 20â„
4ğ‘‘ : 10â„
0ğ‘‘ : 10â„
1ğ‘‘ : 1â„
1ğ‘‘ : 0â„
0ğ‘‘ : 4â„
0ğ‘‘ : 4â„

0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 3â„
0ğ‘‘ : 3â„
0ğ‘‘ : 2â„
0ğ‘‘ : 3â„
0ğ‘‘ : 5â„
0ğ‘‘ : 11â„
0ğ‘‘ : 13â„
0ğ‘‘ : 14â„
4ğ‘‘ : 13â„
1ğ‘‘ : 13â„
4ğ‘‘ : 5â„
0ğ‘‘ : 14â„
1ğ‘‘ : 12â„
1ğ‘‘ : 11â„
0ğ‘‘ : 6â„
0ğ‘‘ : 8â„

0ğ‘‘ : 4â„
0ğ‘‘ : 4â„
0ğ‘‘ : 3â„
0ğ‘‘ : 4â„
0ğ‘‘ : 6â„
0ğ‘‘ : 10â„
0ğ‘‘ : 9â„
0ğ‘‘ : 6â„
0ğ‘‘ : 8â„
0ğ‘‘ : 14â„
0ğ‘‘ : 17â„
1ğ‘‘ : 6â„
1ğ‘‘ : 6â„
32ğ‘‘ : 6â„
5ğ‘‘ : 15â„
4ğ‘‘ : 8â„
0ğ‘‘ : 21â„
2ğ‘‘ : 6â„
2ğ‘‘ : 3â„
0ğ‘‘ : 14â„
0ğ‘‘ : 20â„

0ğ‘‘ : 3â„
0ğ‘‘ : 3â„
0ğ‘‘ : 3â„
0ğ‘‘ : 3â„
0ğ‘‘ : 4â„
0ğ‘‘ : 6â„
0ğ‘‘ : 6â„
0ğ‘‘ : 4â„
0ğ‘‘ : 5â„
0ğ‘‘ : 9â„
0ğ‘‘ : 15â„
0ğ‘‘ : 23â„
0ğ‘‘ : 22â„
3ğ‘‘ : 4â„
1ğ‘‘ : 0â„
3ğ‘‘ : 6â„
0ğ‘‘ : 18â„
2ğ‘‘ : 0â„
1ğ‘‘ : 22â„
0ğ‘‘ : 10â„
0ğ‘‘ : 14â„

0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 1â„
0ğ‘‘ : 2â„
0ğ‘‘ : 2â„
0ğ‘‘ : 4â„
0ğ‘‘ : 8â„
0ğ‘‘ : 10â„
0ğ‘‘ : 11â„
4ğ‘‘ : 5â„
2ğ‘‘ : 5â„
4ğ‘‘ : 1â„
0ğ‘‘ : 10â„
1ğ‘‘ : 3â„
1ğ‘‘ : 2â„
0ğ‘‘ : 4â„
0ğ‘‘ : 5â„

0ğ‘‘ : 6â„
0ğ‘‘ : 6â„
0ğ‘‘ : 5â„
0ğ‘‘ : 6â„
0ğ‘‘ : 6â„
0ğ‘‘ : 8â„
0ğ‘‘ : 7â„
0ğ‘‘ : 7â„
0ğ‘‘ : 9â„
0ğ‘‘ : 18â„
1ğ‘‘ : 12â„
1ğ‘‘ : 22â„
1ğ‘‘ : 21â„
1ğ‘‘ : 9â„
0ğ‘‘ : 14â„
0ğ‘‘ : 23â„
1ğ‘‘ : 15â„
4ğ‘‘ : 7â„
4ğ‘‘ : 7â„
0ğ‘‘ : 17â„
0ğ‘‘ : 20â„

0ğ‘‘ : 14â„
0ğ‘‘ : 14â„
0ğ‘‘ : 11â„
0ğ‘‘ : 14â„
0ğ‘‘ : 20â„
1ğ‘‘ : 5â„
1ğ‘‘ : 2â„
1ğ‘‘ : 1â„
1ğ‘‘ : 6â„
2ğ‘‘ : 5â„
1ğ‘‘ : 19â„
2ğ‘‘ : 12â„
2ğ‘‘ : 12â„
3ğ‘‘ : 4â„
1ğ‘‘ : 5â„
1ğ‘‘ : 18â„
2ğ‘‘ : 0â„
5ğ‘‘ : 5â„
5ğ‘‘ : 22â„
1ğ‘‘ : 2â„
1ğ‘‘ : 10â„

1ğ‘‘ : 8â„
2ğ‘‘ : 2â„
1ğ‘‘ : 6â„
2ğ‘‘ : 0â„
1ğ‘‘ : 12â„
4ğ‘‘ : 12â„
2ğ‘‘ : 18â„
3ğ‘‘ : 20â„
4ğ‘‘ : 5â„
5ğ‘‘ : 3â„
18ğ‘‘ : 11â„
19ğ‘‘ : 3â„
19ğ‘‘ : 15â„
6ğ‘‘ : 15â„
2ğ‘‘ : 20â„
2ğ‘‘ : 12â„
13ğ‘‘ : 14â„
âˆ’
âˆ’
9ğ‘‘ : 1â„
9ğ‘‘ : 13â„

0ğ‘‘ : 13â„
0ğ‘‘ : 14â„
0ğ‘‘ : 10â„
0ğ‘‘ : 13â„
0ğ‘‘ : 20â„
1ğ‘‘ : 7â„
1ğ‘‘ : 3â„
0ğ‘‘ : 19â„
1ğ‘‘ : 0â„
1ğ‘‘ : 19â„
2ğ‘‘ : 5â„
3ğ‘‘ : 13â„
3ğ‘‘ : 13â„
4ğ‘‘ : 9â„
2ğ‘‘ : 0â„
1ğ‘‘ : 11â„
2ğ‘‘ : 12â„
6ğ‘‘ : 11â„
6ğ‘‘ : 7â„
1ğ‘‘ : 16â„
2ğ‘‘ : 10â„

Table 13: Training times of all 11 algorithms in all benchmarks.

F

HYPERPARAMETERS

In this section, the hyperparameters used for each algorithm are presented in separate tables. We note that only in Spread-4 and Spread-5,
QMIX, MAA2C, COMA and MAPPO use non-sharing policy parameters.
Table 14: Hyperparameters for MAPPO
Table 15: Hyperparameters for MAA2C
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
entropy coefficient
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
n-step
epochs
clip

parallel(10)
Adam
10
64
0.0005
True
GRU
0.01
200
10
0.99
True
True
5
4
0.2

Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
entropy coefficient
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
n-step

parallel(10)
Adam
10
64
0.0005
True
GRU
0.01
200
10
0.99
True
True
5

Table 16: Hyperparameters for QMIX

Table 17: Hyperparameters for QPLEX

Name

Value

Name

Value

agent runner
batch size
optimizer
hidden dimension
learning rate
reward standardisation
network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
mixing network hidden dimension
hypernetwork dimension
hypernetwork number of layers

episode
32
Adam
64
0.0005
True
GRU
0.0
50000
1.0
0.05
200
5000
0.99
True
True
32
64
2

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
mixing network hidden dimension
hypernetwork dimension
hypernetwork number of layers

episode
RMSProp
32
64
0.0005
True
GRU
0.0
200000
1.0
0.05
200
5000
0.99
True
True
32
64
2

Table 18: Hyperparameters for MAT-DEC
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
value standardisation
actor network type
critic network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
entropy coefficient
value loss coefficient
observation agent id
observation last action
number of attention heads
number of blocks
epoch
clip
ğœ†ğºğ´ğ¸
use Huber loss
delta coefficient of Huber loss
max norm of gradients
number of mini-batches

parallel(10)
Adam
10
64
0.0005
False
True
MLP
Transformer
0.0
50000
1.0
0.05
200
10
0.99
0.01
1
True
False
1
1
15
0.2
0.95
True
10.0
10.0
1

Table 19: Hyperparameters for CDS
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
n-step
entropy coefficient
number of attention heads
attention regulation coefficient
mixing network hidden dimension
hypernetwork dimension
hypernetwork number of layers
ğ›½1
ğ›½2
ğ›½
ğ›¼
ğœ†

episode
Adam
5000
128
0.0005
True
GRU
0.0
50000
1.0
0.05
200
5000
0.99
False
True
10
0.001
4
0.001
32
64
2
0.5
1.0
0.1
0.6
0.1

Table 21: Hyperparameters for EOI
Table 20: Hyperparameters for COMA
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
entropy coefficient
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
n-step

parallel(10)
Adam
10
64
0.0003
True
GRU
0.001
200
10
0.99
True
True
10

Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
n-step
entropy coefficient
classifier (ğœ™) learning
classifier (ğœ™) batch size
classifier (ğœ™) ğ›½ 2

episode
Adam
10
128
0.0005
True
GRU
200
10
0.99
True
True
5
0.01
0.0001
256
0.1

Table 22: Hyperparameters for EMC
Table 23: Hyperparameters for MASER
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
episodic memory capacity
episodic latent dimension
soft update weight
weighting term ğœ† of episodic loss
curiosity decay rate (ğœ‚ğ‘¡ )
number of attention heads
attention regulation coefficient
mixing network hidden dimension
hypernetwork dimension
hypernetwork number of layers

episode
RMSProp
32
64
0.0005
True
GRU
0.0
50000
1.0
0.05
200
5000
0.99
True
True
1000000
4
0.005
0.1
0.9
4
0.001
32
64
2

Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
network type
evaluation epsilon
epsilon anneal
epsilon start
epsilon finish
target update
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
mixing network hidden dimension
representation network dimension
ğ›¼
ğœ†
ğœ†ğ¼
ğœ†ğ¸
ğœ†ğ·

episode
RMSProp
1
64
0.0005
False
GRU
0.0
50000
1.0
0.05
200
5000
0.99
True
True
32
128
0.5
0.03
0.0008
0.00006
0.00014

Table 24: Hyperparameters for HAPPO
Name

Value

agent runner
optimizer
batch size
hidden dimension
learning rate
reward standardisation
value standardisation
network type
entropy coefficient
buffer size
ğ›¾ (discounted factor)
observation agent id
observation last action
epochs
n-step
max norm of gradients
number of mini-batches
ğœ†ğºğ´ğ¸
clip
use huber loss
delta coefficient of huber loss

parallel(10)
Adam
10
64
0.0005
False
True
GRU
0.01
10
0.99
False
False
5
1
10
1
0.95
0.2
True
10.0

