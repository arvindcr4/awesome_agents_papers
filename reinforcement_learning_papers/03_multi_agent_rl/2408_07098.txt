QTypeMix: Enhancing Multi-Agent Cooperative Strategies through
Heterogeneous and Homogeneous Value Decomposition
Songchen Fua,b , Shaojing Zhaoa,b , Ta Lia,b,âˆ— and Yonghong Yana,b
a Laboratory of Speech and Intelligent Information Processing, Institute of Acoustics, Beijing, China

arXiv:2408.07098v1 [cs.MA] 12 Aug 2024

b University of Chinese Academy of Sciences, Beijing, China

ARTICLE INFO

ABSTRACT

Keywords:
Multi-agent reinforcement learning
Value function factorization
Markov decision process
Heterogeneous agents

In multi-agent cooperative tasks, the presence of heterogeneous agents is familiar. Compared
to cooperation among homogeneous agents, collaboration requires considering the best-suited
sub-tasks for each agent. However, the operation of multi-agent systems often involves a large
amount of complex interaction information, making it more challenging to learn heterogeneous strategies. Related multi-agent reinforcement learning methods sometimes use grouping
mechanisms to form smaller cooperative groups or leverage prior domain knowledge to learn
strategies for different roles. In contrast, agents should learn deeper role features without
relying on additional information. Therefore, we propose QTypeMix, which divides the value
decomposition process into homogeneous and heterogeneous stages. QTypeMix learns to extract
type features from local historical observations through the TE loss. In addition, we introduce
advanced network structures containing attention mechanisms and hypernets to enhance the
representation capability and achieve the value decomposition process. The results of testing
the proposed method on 14 maps from SMAC and SMACv2 show that QTypeMix achieves
state-of-the-art performance in tasks of varying difficulty.

1. Introduction
As one of the essential branches in multi-agent reinforcement learning (MARL), cooperative MARL has become
a solution for many unmanned robot cluster tasks Wang et al. (2022); Xu et al. (2018); Wang et al. (2020a). Unlike
competitive and hybrid tasks, collaborative tasks guide the actions of each agent through team rewards to ensure that
the goals of each agent are consistent and that there is no competitive relationship. In the distributed training with
decentralized execution (DTDE) paradigm, each agent learns independently without utilizing explicit information
exchange or global observation Lauer and Riedmiller (2000); De Witt et al. (2020). However, this approach of
treating other agents as part of the environment is non-stationary Zhang et al. (2021); Wong et al. (2023). The
centralized training with centralized execution (CTCE) paradigm allows for real-time information exchange or global
observation to learn the joint policy of all agents through centralized actuators Gupta et al. (2017). Although the
CTCE paradigm alleviates the non-stationary problem, most multi-agent systems have partial observability and
communication constraints, making it difficult to achieve good adaptation Gronauer and Diepold (2022). Therefore,
the centralized training with distributed execution (CTDE) paradigm Kraemer and Banerjee (2016) has been widely
applied, which combines the advantages of the previous two paradigms and is easy to apply to various tasks Oliehoek
et al. (2008); Foerster et al. (2016); Sunehag et al. (2017); Foerster et al. (2018).
Similar to single-agent reinforcement learning (RL), there are two main types of MARL algorithms in the CTDE
paradigm: value-based and policy-based methods. The value-based methods mainly study how to decompose the joint
value function learned from the team reward function into local value functions, forming individual strategies for
each agent. This type of method is generally referred to as Value Function Factorization (VFF) methods, such as
VDN Sunehag et al. (2017) and QMIX Rashid et al. (2020), which have shown good performance in many intelligent
agent tasks but are challenging to apply to tasks with continuous action spaces. On the other hand, under the CTDE
paradigm, policy-based methods directly learn parameterized policies and mainly adopt a centralized-cirtic-distributedactor structure Foerster et al. (2018); Lowe et al. (2017); Yu et al. (2022). Although policy-based methods can be applied
to tasks in continuous action spaces, they can easily lead to local optima under the guidance of team rewards.
âˆ— Corresponding author

fusongchen@hccl.ioa.ac.cn (S. Fu); zhaoshaojing@hccl.ioa.ac.cn (S. Zhao); lita@hccl.ioa.ac.cn (T. Li);
yanyonghong@hccl.ioa.ac.cn (Y. Yan)

Songchen Fu et al.: Preprint submitted to Elsevier

Page 1 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

This article mainly researches value-based methods under the CTDE paradigm. In recent years, many VFF methods
have been proposed, among which the two mainstream types of ideas are: increasing the representation ability of VFF
by continuously reducing constraints Sunehag et al. (2017); Rashid et al. (2020); Son et al. (2019); Yang et al. (2020);
Hu et al. (2021); Shen et al. (2022) and improving the algorithmâ€™s exploration ability for policy space and environment
interaction Mahajan et al. (2019); Lyu and Amato (2020); Sun et al. (2021); Gupta et al. (2021). The purpose of
increasing VFF representation ability is to find more accurate credit assignment methods, and the purpose of growing
exploration ability is to improve the sampling efficiency of agents. However, the primitive flat VFF scheme Phan
et al. (2021) has two main issues. First, it needs to work on accurately learning the impact of each agentâ€™s actions on
the team value function through a global state. Second, it faces challenges distinguishing agentsâ€™ contributions with
varying abilities in heterogeneous agent scenarios. These issues will significantly decrease performance as the number
of agents in the environment increases.
In earlier years, research in natural systems has demonstrated the effectiveness of grouping and division of labor
in collaborative labor Gordon (1996); Jeanson et al. (2005); Wittemyer and Getz (2007). Inspired by this, methods of
grouping according to specific prior rules or adaptive grouping have also been introduced into MARL, which makes
individual contributions to the team more accessible to learn. Specifically, some methods group agents using prior
knowledge Lhaksmana et al. (2018); Jiang et al. (2021), some methods automatically group agents based on specific
rules during interactions with the environment Shao et al. (2022); Zang et al. (2024), and other methods learn the
individuality or roles of the agents Wang et al. (2020b,c); Jiang and Lu (2021). However, due to the ample joint state
space of multi-agent systems and the high exploration randomness, learning how to group agents entirely through
neural networks places high demands on the design of the loss function. Therefore, we adopt a novel approach that has
yet to be explored in prior studies. Rather than grouping agents based on functionality, proximity, or alignment with
short-term goals, we focus on defining the roles various agents should fulfill within the team.
In this paper, we investigate how to make the learning process from individual action-value functions to the joint
action-value function more directional from a new perspective, thereby obtaining a more accurate representation of
the individualâ€™s impact on the team. To address the abovementioned issues, we propose a novel type-related VFF
method, QTypeMix, which performs hierarchical value decomposition based solely on agent types provided by the
environment or human input. Specifically, QTypeMix uses the global state to decompose the joint action-value function
into type-level value functions and then uses the global state, local observations, and other information to decompose
the type-level value functions into local utilities. Additionally, we design a feature extractor that extracts type-related
observation embedding from each agentâ€™s historical observations to generate the network weights for the second-layer
value decomposition process. It is important to note that while our method may seem similar to group-based VFF
methods, it differs fundamentally. Rather than dividing agents into teams by type, we guide each to learn strategies
most valuable for their specific type in the given task. Our contributions are as follows:
(1) We propose a novel dual-layer value decomposition method, QTypeMix. This method innovatively leverages
local observations and type-related embeddings to provide more direct guidance for the value function decomposition
process. Compared to existing methods that only use global states or no information, QTypeMix exploits the common
features in the historical observations of agents of the same type. This approach guides the network to different
functionalities, increasing learning efficiency.
(2) We designed a new feature extractor to derive type-related embeddings based on each agentâ€™s historical
observations, which are trained through an additional loss function. The type-related embeddings guide the neural
network in decomposing type-level value functions into local utilities, providing additional information for policy
optimization.
(3) We employed various advanced network architectures to maximize the neural networkâ€™s representational capacity and integrated the algorithm into mainstream MARL frameworks to ensure its generalization and reproducibility.
The experiments in this paper are based on selected scenarios from the StarCraft Multi-Agent Challenge (SAMC)
Samvelyan et al. (2019) and SAMCv2 Ellis et al. (2024). The final results show that QTypeMix matches or exceeds
the performance of existing SOTA methods in most scenarios and demonstrates outstanding performance in scenarios
with many agent types. Our code is available at https://github.com/linkjoker1006/pymarl3.

2. Related Work
Centralized Training with Distributed Execution: Due to the non-stationarity and partial observability problems
in multi-agent systems, MARL requires the design of an efficient multi-agent training scheme. The CTDE paradigm
Songchen Fu et al.: Preprint submitted to Elsevier

Page 2 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

cleverly combines the advantages of the DTDE and CTCE paradigms, allowing agents to access global or additional
information during training while only relying on local observations to take actions during execution. To address
the issue in the DTDE paradigm, where agents struggle to distinguish between environmental changes and changes
caused by other agents, CTDE mitigates non-stationarity and partial observability by allowing access to global states
or other agentsâ€™ observations. Conversely, to tackle the issue in the CTCE paradigm where agents must obtain global
information during execution, CTDE uses decentralized execution, significantly increasing the algorithmâ€™s flexibility
and reducing environmental requirements. Additionally, the CTDE paradigm supports both value-based Sunehag et al.
(2017); Rashid et al. (2020); Son et al. (2019); Yang et al. (2020); Shen et al. (2022); Jianye et al. (2022); Mahajan
et al. (2019); Sun et al. (2021); Gupta et al. (2021) and policy-based Foerster et al. (2018); Lowe et al. (2017); Yu et al.
(2022); Mguni et al. (2021); Chen et al. (2023) methods. The method proposed in this paper is a value-based MARL
method under the CTDE paradigm.
Value Function Factorization: The VFF method factorizes the joint action-value function into local utilities
through mixing networks, making it the most common type of value-based MARL method. VDN uses a linear
decomposition method to decompose the joint action-value function. QMIX generates a non-linear decomposition
scheme under monotonicity constraints, enhancing the representation capability of the mixing network through a
hypernetwork. QTRAN Son et al. (2019) abandons the monotonicity and additivity assumptions, performing value
decomposition by estimating error terms and proposing the Individual-Global-Max (IGM) principle. Qatten Yang
et al. (2020) derives the general form of global and individual Q-values using Taylor expansion, freeing itself
from assumptions about their relationship. HPN-QMIX Jianye et al. (2022) introduces permutation invariance (PI)
and permutation equivariance (PE) into QMIX, significantly improving model representation capability through a
hypernetwork and effectively reducing the state space of MARL. With comprehensive theoretical derivation and
ingenious algorithm implementation, this method achieves substantial performance improvements over baseline
methods in the SMAC and SMACv2 environments. Besides relaxing constraints and enhancing the representation
capability of the mixing network, another mainstream approach is to increase agent exploration ability or improve
data collection efficiency. MAVEN Mahajan et al. (2019) addresses the inefficiency in exploration caused by QMIX
constraints through committed exploration by introducing a latent space for hierarchical control. This method mixes
value-based and policy-based approaches, conditioning the value-based policy on latent variables controlled by a
hierarchical policy.
Group-based and Role-based MARL: Group-based or role-based MARL methods utilize predefined or learned
group and role information to guide value decomposition or policy optimization, often implemented through multi-level
structures. The type-based hierarchical group communication (THGC) Jiang et al. (2021) utilizes prior or predefined
rules in fields such as location, functionality, and health to the group and maintains the groupâ€™s cognitive consistency
through knowledge sharing. The Self Organized Group (SOG) Shao et al. (2022) is featured with a conductor election
(CE) and a message summary (MS) mechanism, which enable the algorithm to have zero-shot generalization ability
in terms of the dynamic number of agents and the varying partial observability. The Group oriented Multi Agent
Reinforcement Learning (GoMARL) Zang et al. (2024) enables models to automatically learn how to group without
domain knowledge guidance, decompose the joint action-value function into group-wise value functions, and further
guide agents to improve their policies in a fine-grained fashion. The Emergence of Individuality (EOI) Jiang and
Lu (2021) trains a probabilistic classifier that predicts the probability distribution of agents given observations using
intrinsic rewards. Agents tend to visit observations they are familiar with, promoting the emergence of individuality.
RODE Wang et al. (2020c) proposed a role-based dual-level structure where a role selector periodically searches the
role space, and role policies learn within the decomposed joint action space. Unlike existing methods, QTypeMix
approaches from the perspective of agent types, extracting unique observational features for each type to guide the
value decomposition process. The algorithm only utilizes the prior knowledge of agent types, which is undoubtedly
easily provided by any environment supporting heterogeneous agents.

3. Background
Decentralized Partially Observable Markov Decision Process: This paper focuses on fully cooperative multiagent tasks, where all agents in the environment attempt to maximize a joint reward function while having different
individual goals Wong et al. (2023). Similar to modeling single-agent dynamic decision-making in stochastic
environments using partially observable Markov decision process (POMDP), fully cooperative multi-agent tasks
are typically modeled as decentralized partially observable Markov decision process (Dec-POMDP) Bernstein et al.
Songchen Fu et al.: Preprint submitted to Elsevier

Page 3 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

(2002); Oliehoek et al. (2016). Dec-POMDP is defined as a tuple ğº = âŸ¨ğ‘†, ğ‘ˆ , ğ‘ƒ , ğ‘Ÿ, ğ‘, ğ‘‚, ğ‘›, ğ›¾âŸ©. ğ‘  âˆˆ ğ‘† represents the
global state of the environment. Each agent ğ‘ğ‘– âˆˆ îˆ­ = {ğ‘1 , â€¦ , ğ‘ğ‘› } takes an action ğ‘¢ğ‘–ğ‘¡ âˆˆ ğ‘ˆ at timestep ğ‘¡, forming the
joint action ğ‘¢ğ‘¡ âˆˆ ğ‘ˆ ğ‘› . The state transition distribution ğ‘ƒ (ğ‘ ğ‘¡+1 |ğ‘ ğ‘¡ , ğ’–ğ‘¡ ) âˆ¶ ğ‘† Ã— ğ‘ˆ ğ‘› Ã— ğ‘† â†’ [0, 1] governs the environmentâ€™s
state transitions caused by the joint actions. ğ‘Ÿ(ğ‘ , ğ’–) âˆ¶ ğ‘† Ã—ğ‘ˆ ğ‘› â†’ â„ defines the team reward function shared by all agents
and ğ›¾ âˆˆ [0, 1) is the discount factor. In a partially observable setting, agent ğ‘ğ‘– cannot access the global state and can
only sample local observation ğ‘§ğ‘– âˆˆ ğ‘ through the observation function ğ‘‚(ğ‘ , ğ‘–) âˆ¶ ğ‘† Ã— ğ´ â†’ ğ‘. Therefore, the actionobservation history of agent ğ‘ğ‘– is ğœ ğ‘– âˆˆ ğ‘‡ â‰¡ (ğ‘Ã—ğ‘ˆ )âˆ— on which it conditions a policy ğœ‹ ğ‘– (ğ‘¢ğ‘– |ğœ ğ‘– ) âˆ¶ ğ‘‡ Ã—ğ‘ˆ â†’ [0, 1]. The joint
âˆ‘
ğ‘˜
policy ğœ‹ is based on the joint action-value function ğ‘„ğœ‹ (ğ‘ ğ‘¡ |ğ’–ğ‘¡ ) = ğ”¼ğ‘ ğ‘¡+1 âˆ¶âˆ,ğ’–ğ‘¡+1 âˆ¶âˆ [ğ‘…ğ‘¡ |ğ‘ ğ‘¡ , ğ’–ğ‘¡ ], where ğ‘…ğ‘¡ = âˆ
ğ‘˜=0 ğ›¾ ğ‘Ÿğ‘¡+ğ‘˜ is
the discounted reward.
QMIX: In RL, Deep Q-Network (DQN) Mnih et al. (2015) is the most typical value-based method. This method
leverages deep neural networks to approximate the optimal action-value function ğ‘„âˆ— (ğ‘ , ğ‘) = maxğœ‹ ğ”¼[ğ‘Ÿğ‘¡ + ğ›¾ğ‘Ÿğ‘¡+1 +
ğ›¾ 2 ğ‘Ÿğ‘¡+2 + â‹¯ âˆ£ ğ‘ ğ‘¡ = ğ‘ , ğ‘ğ‘¡ = ğ‘, ğœ‹]. During training, DQN updates the policyâ€™s Q-value function by sampling transitions
(ğ‘ , ğ‘¢, ğ‘Ÿ, ğ‘ â€² ) with batch size ğµ from the replay buffer ğ· and minimizing the squared temporal difference (TD) error:
îˆ¸(ğœƒ) =

ğµ [(
)2 ]
âˆ‘
ğ‘¦ğ·ğ‘„ğ‘
âˆ’
ğ‘„(ğ‘ ,
ğ‘¢;
ğœƒ)
,
ğ‘

(1)

ğ‘

where ğ‘¦ğ·ğ‘„ğ‘ = ğ‘Ÿ + ğ›¾ maxğ‘¢â€² ğ‘„(ğ‘ â€² , ğ‘¢â€² ; ğœƒ âˆ’ ). ğœƒ âˆ’ are the parameters of the target network that are periodically copied from
ğœƒ and kept constant for several iterations.
After applying DQN to multi-agent systems, each agent has its independent action-value function ğ‘„ğ‘– . Although
allowing multiple agents to update their policies during training independently can be effective in terms of results
Tampuu et al. (2017), the non-stationarity caused by the impact of other agentâ€™s actions in the environment makes
convergence unguaranteed. QMIX Rashid et al. (2020) is one of the most classic value decomposition algorithms,
and the algorithm proposed in our paper is also an improvement based on this framework. It employs a mixing
network to estimate the joint action-value ğ‘„ğ‘¡ğ‘œğ‘¡ as a monotonic combination of individual Q-value ğ‘„ğ‘– of each agent.
By controlling the non-negativity of the weights, QMIX maintains the consistency between centralized and distributed
policies. Therefore, its monotonicity constraint can be expressed as:
ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
â‰¥ 0,
ğœ•ğ‘„ğ‘

âˆ€ğ‘.

(2)

This monotonicity constraint enables QMIX to meet the important Individual-Global-Max (IGM) principle
proposed in subsequent work Son et al. (2019), which states that for a joint action-value function ğ‘„âˆ— (ğ‰, ğ’–) âˆ¶ ğ‘‡ ğ‘› Ã—ğ‘ˆ ğ‘› â†’
â„, if there exist individual action-value functions [ğ‘„ğ‘– (ğœ, ğ‘¢) âˆ¶ ğ‘‡ Ã— ğ‘ˆ â†’ â„]ğ‘›ğ‘–=1 , the following holds:
â›arg maxğ‘¢1 ğ‘„1 (ğœ 1 , ğ‘¢1 )â
âŸ,
arg max ğ‘„ (ğ‰, ğ’–) = âœ
â‹®
ğ’–
âŸ
âœ
ğ‘›
ğ‘›
âarg maxğ‘¢ğ‘› ğ‘„ğ‘› (ğœ , ğ‘¢ )â 
âˆ—

âˆ€ğ‰, ğ’–.

(3)

QMIXâ€™s mixing network is also implemented through a four-layer structure similar to the hypernetworks Ha et al.
(2016). It uses the global state to generate the weights and biases for the value decomposition process, allowing QMIX
to access global information during training.
Attention mechanism: Since its introduction, the attention mechanism Vaswani et al. (2017) has been widely
applied in various research fields, and many MARL works Iqbal and Sha (2019); Das et al. (2019); Yang et al. (2020);
Zhang et al. (2022); Pu et al. (2022) have also utilized this concept. Qatten utilizes a multi-head attention mechanism to
replace the mixing network in QMIX, approximating ğ‘„ğ‘¡ğ‘œğ‘¡ . AERL Pu et al. (2022) employs the graph attention operator
to handle complex agent interactions and capture temporal correlations. An attention function can be described as
mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The
output is computed as a weighted sum of the values, where a compatibility function of the query with the corresponding
key computes the weight assigned to each value. In practice, we compute the attention function on a set of queries
simultaneously, packed into a matrix ğ‘„. The keys and values are also loaded into matrices ğ¾ and ğ‘‰ . We compute the
matrix of outputs as:
(
)
ğ‘„ğ¾ ğ‘‡
Attention(ğ‘„, ğ¾, ğ‘‰ ) = softmax âˆš
ğ‘‰.
(4)
ğ‘‘ğ‘˜
Songchen Fu et al.: Preprint submitted to Elsevier

Page 4 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

âˆš
where ğ‘‘ğ‘˜ represents the dimension of the values. This paper employs the multi-head attention mechanism to jointly
enable the model to attend to information from different representation subspaces.
Hyper Policy Network: To overcome the curse of dimensionality in MARL, where the state space grows
exponentially with the number of agents, Jianye et al. (2022) proposed Hyper Policy Network (HPN). This method
uses inductive biases of permutation invariance (PI) and permutation equivariance (PE) to reduce the multi-agent
state space, significantly improving existing MARL methodsâ€™ performance and learning efficiency. Regarding results,
the HPN series algorithms can be considered to have achieved state-of-the-art (SOTA) performance on both SMAC
and SMACv2. Therefore, we use this class of methods as the primary baseline. In recognition of the outstanding
contribution of HPN, the algorithm proposed in this paper adopts the same network structure for the module that
extracts each agentâ€™s observation features.

4. Methodology
In this section, we introduce the overall architecture of QTypeMix and provide a detailed description of its
implementation. In QTypeMix, types are defined as labels that distinguish agents with different capabilities or
attributes, meaning that agents of the same type can be considered identical. For instance, in the SMAC environment
where agent type information is provided, we use this information to categorize different unit types. Agents of the
same unit type (e.g., marines) are considered completely identical. In environments where agent type information is not
provided (e.g., Multi-Agent Particle Environment (MPE) Lowe et al. (2017)), we categorize agents based on attribute
values such as size, acceleration, and maximum speed. If a multi-agent system has ğ‘› agents îˆ­ = {ğ‘1 , â€¦ , ğ‘ğ‘› }, we can
divide them into ğ‘š types îˆ­ = {î‰€1 , â€¦ , î‰€ğ‘š }, 0 < ğ‘š < ğ‘›, where î‰€ğ‘˜ = {ğ‘ğ‘˜1 , â€¦ , ğ‘ğ‘˜ğ‘› }, ğ‘˜ âˆˆ {1, 2, â€¦ , ğ‘š}, ğ‘›ğ‘˜ = ğ‘™ğ‘’ğ‘›(î‰€ğ‘˜ ).
ğ‘˜
After categorizing by type, each agent belongs exclusively to one type, which means for âˆ€ğ‘˜, ğ‘™ âˆˆ {1, 2, â€¦ , ğ‘š} and
ğ‘˜ â‰  ğ‘™, î‰€ğ‘˜ âˆ© î‰€ğ‘™ = âˆ…. Although the experiments in this paper are conducted in SMAC and SMACv2 environments, the
above definition of types allows QTypeMix to be easily applied to most multi-agent simulation environments.

4.1. Simple type-wise value decomposition
Qtot

Total Mixer

Bias
Weighted

Qtype1

St

Bias

QtypeN

Weighted

St

Type Mixer 1

Type Mixer m

Bias

St
Weighted

Q1

Q2

Q3

Qn

Dot product

Softmax

Hyper Policy Net

Dot product

Z t1

Z t2

Z t3

Z tn
Qi

St

zti

Environment

Fig. 1: Diagram of our straightforward type-wise value decomposition method (QTypeMix-B).

A straightforward way to allow the model to distinguish between agents of various types is to introduce an additional
layer into the flat VFF scheme. Using QMIX as an example, the algorithm represents the joint action-value function
Songchen Fu et al.: Preprint submitted to Elsevier

Page 5 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

as a weighted sum of individual utilities plus a bias. Based on this setup, we add an extra value decomposition
step according to the types of agents. The entire value decomposition process becomes ğ‘„ğ‘¡ğ‘œğ‘¡ , first decomposed into
ğ‘„ğ‘˜ğ‘¡ğ‘¦ğ‘ğ‘’ , then decomposed into individual utilities ğ‘„ğ‘– . As shown in Fig. 1, this foundational algorithm is referred to as
QTypeMix-B(hereafter referred to as QTypeMix-B). By introducing the Type Mixer, we aim for the model to adopt
similar value decomposition standards for agents of the same type and ultimately emphasize the differences in value
contributed by different types of agents to the task. Like the Total Mixer, the Type Mixer also requires additional global
state information from the environment during training, which is unnecessary during execution.

W2in
matmul

m

matmul

FC
element-wise
sum

W3in

ainv
W2out

GRU

W3out

matmul

W1in

W1out

Hypernet
hpin

Hypernet
hpout

aequiv

Fig. 2: Diagram of hyper policy network.

As shown in Fig. 1, during training, the environment provides the global state and each agentâ€™s local observations
ğ‘ğ‘¡ğ‘– at each time step. The local observations ğ‘ğ‘¡ğ‘– are first processed through an HPN to compute individual utilities,
with the specific implementation detailed in Fig. 2. The HPN primarily consists of two hypernetworks, â„ğ‘ğ‘–ğ‘› and â„ğ‘ğ‘œğ‘¢ğ‘¡ .
â„ğ‘ğ‘–ğ‘› generates the weights for a linear layer, sums the results, and then passes them through a GRU layer followed by
a linear layer, achieving PI for the output actions. â„ğ‘ğ‘œğ‘¢ğ‘¡ multiplies the GRU results with network parameters generated
based on the input and then adds a bias, resulting in PE output actions. In SMAC and SMACv2, the actions related
to the agentsâ€™ movements should satisfy PI, while those about attacking enemies should satisfy PE. For more details
about HPN, please refer to Jianye et al. (2022). In this paper, we use HPN merely as a tool.
After obtaining the individual action-values of agents at the current time step, ğ‘š Type Mixers process the utilities
of agents belonging to their respective types. Finally, the type-specific Q-values output by the Type Mixers are fed
into the Total Mixer to obtain the joint action-value ğ‘„ğ‘¡ğ‘œğ‘¡ . We implement the Type Mixer using a multi-head attention
mechanism cascaded with a hypernetwork guided by the global state. The Total Mixer, on the other hand, is entirely
implemented using hypernetworks. Comparing the implementations of the two Mixers, we can see that introducing the
attention mechanism allows the model to better focus on each agentâ€™s observations. Since each attention mechanism
receives inputs from agents of the same type, we expect the model to learn the observation characteristics of each
type of agent, thereby influencing the type-specific Q-values. Upon examining the two value decomposition stages of
QTypeMix-B, it can be observed that each Type Mixer decomposes the type-specific action-value of homogeneous
agents. In contrast, the Total Mixer decomposes the joint action-value into type-specific action-values. Therefore, we
refer to the former as the homogeneous stage of value decomposition and the latter as the heterogeneous stage. With this,
we have completed the implementation of QTypeMix-B. The relevant experimental results are presented in Section 5.

4.2. Additional type information extraction
In Section 4.1, we proposed an intuitive and straightforward type-wise value decomposition method, aiming to
better evaluate the action-values of different types of agents through the Type Mixer. In the attention mechanism of the
Type Mixer, we use the agentâ€™s current local observation as the key. However, in multi-agent cooperative environments,
relying solely on the local observation at the current time step makes it difficult to find associations with the type. For
example, observing their local states at a single time step in an environment with two agents with different running

Songchen Fu et al.: Preprint submitted to Elsevier

Page 6 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
TE Loss

E1

E2

E3

En

Type Embedding Extractor

Type Embedding

Bias
Weighted

St

Zt

1

Zt

2

Zt

3

Zt

n

hit-1

St
GRU

h it

Bias

Environment

Weighted

Fig. 3: Diagram of additional type information extraction.

speeds does not allow for distinguishing between them. If we can access the historical observations of each agent, we
can quickly identify which agent runs faster and then allocate tasks more appropriately based on the mission objectives.
As shown in Fig. 3, after obtaining the global state at time step ğ‘¡ and the local observations of each agent from
the environment, we extract the type embedding ğ¸ğ‘¡ğ‘– for each agent from their historical observations {ğ‘0ğ‘– , ğ‘1ğ‘– , â€¦ , ğ‘ğ‘¡ğ‘– }
using the Type Embedding Extractor. It consists of GRU units and linear layers generated by hypernetworks. To guide
the network in updating in the desired direction, we design the TE loss in addition to the standard RL training process,
which means that the updates for TE loss and TD Loss are mutually independent. The specific definition of the TE loss
is as follows:
)
( ğ‘› ğ‘›
ğµ
âˆ‘âˆ‘
âˆ‘
(
)
ğ‘–
ğ‘—
ğ¼(ğ‘–, ğ‘—) â‹… cos ğ¸ğ‘– (ğ‘ , ğ‘†; ğœƒğ‘¡ğ‘’ ), ğ¸ğ‘— (ğ‘ , ğ‘†; ğœƒğ‘¡ğ‘’ ) ,
îˆ¸ğ‘‡ ğ¸ (ğœƒğ‘¡ğ‘’ ) =
ğ‘–

ğ‘

{
ğ¼(ğ‘–, ğ‘—) =

âˆ’1,
1,

ğ‘—

(5)

if ğ‘ğ‘– âˆˆ î‰€ğ‘˜ , ğ‘ğ‘— âˆˆ î‰€ğ‘˜ .
if ğ‘ğ‘– âˆˆ î‰€ğ‘˜ , ğ‘ğ‘— âˆ‰ î‰€ğ‘˜ .

We calculate the cosine similarity between the type embeddings of every two agents and use an indicator function ğ¼ to
control the positive or negative contributions, aiming to maximize the embedding differences between different types
and minimize the embedding differences within the same type. Under the guidance of the TE loss, the type embeddings
will incorporate more information related to the agentsâ€™ types. For example, type A agents may have higher mobility,
type B agents may possess greater attack power, and type C agents may have higher health points.

4.3. Algorithm overview
After introducing QTypeMix-B and the Type Embedding Extractor, we will present the overall architecture of
QTypeMix. As shown in Figure 4, the entire algorithm includes two mixing networks and an embedding extractor,
all of which involve using hypernetworks to generate weights and biases. To prevent the weights generated by the
hypernetworks from being zeroed out by the activation function when they are negative, QTypeMix, like QMIX,
uses ELU as the non-linear function instead of ReLU. Regarding the workflow, the agentsâ€™ local observations ğ‘ğ‘¡ğ‘– first
pass through the HPN and the Type Embedding Extractor to obtain local utilities ğ‘„ğ‘–ğ‘¡ and type embeddings ğ¸ğ‘¡ğ‘– . In
the homogeneous value decomposition stage, each agentâ€™s local observation and type embedding are concatenated
to form the key (K). The global state serves as the value (V), and the local utilities serve as the query (Q). These
are processed through a multi-head attention mechanism to calculate the type utility. The weights and biases for the
multi-head attention and the weights and biases for the weighted summation of the multi-head results are all generated
by a hypernetwork that takes the global state ğ‘†ğ‘¡ as input. In the heterogeneous value decomposition stage, the multiple
ğ‘„ğ‘˜ğ‘¡ğ‘¦ğ‘ğ‘’ are fed into the Total Mixer, where the weights and biases are entirely generated by the hypernetworks, to obtain
the estimated joint action-value ğ‘„ğ‘¡ğ‘œğ‘¡ .
As described earlier, the network update for QTypeMix consists of two backpropagation steps. Although QTypeMix
has unique characteristics in the value decomposition process, the TD error used for updating the network is consistent
Songchen Fu et al.: Preprint submitted to Elsevier

Page 7 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
Qtot

Total Mixer

Bias

St
Weighted

St

Qtype1

QtypeN

Type Mixer 1

Type Mixer m

Dot product

Softmax

Environment

Dot product

Q1

Q2

Q3

Qn
Qi

St (zti, Eti)

Hyper Policy Net

Z t1

Z t2

Z t3

Local Observation

Z tn

Type Embedding Extractor

Et1

Et2

Et3

Type Embedding

Etn

TE Loss

Fig. 4: Diagram of QTypeMix.

with other value-based methods:
îˆ¸ğ‘‡ ğ· (ğœƒ) =

ğµ
âˆ‘

[(

â€²

â€²

âˆ’

ğ‘Ÿ + ğ›¾ max
ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‘  , ğ‘¢ ; ğœƒ ) âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‘ , ğ‘¢; ğœƒ)
â€²
ğ‘¢

ğ‘

)2 ]
.

(6)

Since the gradient of the TD error passes through four neural networks, ğœƒ in Eq. 6 includes all the network parameters
in QTypeMix: (ğœƒğ‘¡ğ‘’ , ğœƒâ„ğ‘ğ‘› , ğœƒğ‘š1 , ğœƒğ‘š2 ). That is when updating the TD error, we do not freeze the parameters of the Type
Embedding Extractor. Instead, we add a weight ğ›¼ to allow the two loss functions to update together. We believe
this approach can further ensure the direction of updating and learning efficiency of the Type Embedding Extractor.
Therefore, the global optimization objective of QTypeMix is to minimize:
îˆ¸(ğœƒ) = îˆ¸ğ‘‡ ğ· (ğœƒ) + ğ›¼ â‹… îˆ¸ğ‘‡ ğ¸ (ğœƒğ‘¡ğ‘’ )

(7)

The detailed process of the QTypeMix algorithm is outlined in Appendix A.

5. Experiments
Experimental Setup: We select the commonly used SMAC Samvelyan et al. (2019) and the more challenging
SMACv2 Ellis et al. (2024) as benchmarks to evaluate the performance of QTypeMix. According to the official
recommendation, we use game version 4.6.2.69232. SMAC is a benchmark suite specifically designed for MARL
research. It is based on the real-time strategy game StarCraft II. It offers a range of tasks and scenarios that allow
researchers to test their multi-agent algorithms in complex, dynamic environments. In this environment, each allied
unit is controlled by an RL agent, which can observe the distances, relative positions, unit types, and health of all ally
and enemy units within its field of view at each time step. Built-in rules of the environment control all enemy units. To
address the shortcomings of SMAC, SMACv2 introduces three changes: random team compositions, random starting
Songchen Fu et al.: Preprint submitted to Elsevier

Page 8 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
Table 1
Detailed test results of 6 algorithms on 14 maps.
Map

Difficulty

FT-VDN

FT-QMIX

HPN-VDN

HPN-QMIX

QTypeMix-B

QTypeMix

2c vs. 64zg
3s5z vs. 3s6z
6h vs. 8z
corridor
MMM2
protoss 5 vs. 5
protoss 10 vs. 11
protoss 15 vs. 16
terran 5 vs. 5
terran 10 vs. 11
terran 15 vs. 16
zerg 5 vs. 5
zerg 10 vs. 11
zerg 15 vs. 16

Hard
Super Hard
Super Hard
Super Hard
Super Hard
SAMCv2
SAMCv2
SAMCv2
SAMCv2
SAMCv2
SAMCv2
SAMCv2
SAMCv2
SAMCv2

76.0(8.2)
55.9(7.3)
68.4(8.7)
81.8(8.5)
88.2(5.6)
61.2(8.2)
12.5(5.7)
11.1(5.7)
65.6(8.3)
39.7(8.1)
39.3(9.1)
54.6(8.3)
42.3(9.0)
41.5(9.3)

98.8(1.8)
73.2(7.3)
87.0(6.3)
92.8(4.3)
92.0(4.1)
62.9(9.5)
16.8(6.5)
14.8(6.1)
73.4(8.3)
47.9(8.5)
50.7(9.4)
60.3(8.7)
43.2(9.5)
50.5(9.3)

98.8(2.0)
93.8(4.2)
90.2(5.6)
97.4(2.6)
99.7(0.9)
76.0(8.7)
51.7(8.9)
50.8(8.6)
74.6(8.7)
63.0(9.2)
58.5(8.5)
65.8(8.5)
58.5(9.6)
59.1(8.1)

99.4(1.4)
97.7(2.9)
97.2(3.8)
97.3(2.0)
99.6(1.0)
76.9(7.5)
49.6(9.4)
56.0(9.2)
75.6(7.9)
65.8(9.2)
65.4(9.7)
68.3(7.4)
51.1(8.6)
58.8(8.5)

98.8(1.8)
97.7(2.6)
95.2(3.6)
97.0(3.0)
99.7(0.9)
78.7(7.4)
55.0(9.2)
60.8(7.9)
76.8(8.2)
67.1(8.8)
67.1(8.8)
70.5(7.6)
58.0(9.7)
58.8(9.4)

99.5(1.1)
97.9(2.1)
97.2(2.6)
97.7(2.9)
99.8(0.7)
78.8(7.3)
60.2(9.0)
62.8(8.9)
77.3(7.4)
68.9(8.7)
72.7(8.5)
71.6(7.3)
61.5(8.9)
60.9(8.8)

positions, and realistic field of view and attack range. These changes encourage agents to focus more on understanding
the observation space and prevent the learning of successful open-loop strategies (strategies conditioned only on the
time step). Besides these changes, SMACv2 and SMAC are nearly identical in other aspects. The goal of the ally
agents is to eliminate all enemy agents within a certain timeframe, and rewards are only given when enemy units
are eliminated and victory is achieved. Moreover, both environments feature discrete joint action and state spaces,
making them suitable for VFF methods. For more detailed settings regarding the environment parameters, please refer
to Appendix B.1.
Baseline Selection: Hu et al. (2021) achieves higher win rates by performing code-level optimizations on
QMIXRashid et al. (2020) and VDNSunehag et al. (2017), and releases these improvements in pymarl21 . Due to
their excellent performance among contemporary algorithms, we include them as one of the baselines, referred to as
FT-QMIX and FT-VDN. Furthermore, to address the curse of dimensionality resulting from the increased number of
agents, Jianye et al. (2022) introduces the HPN series methods incorporating PI and PE, achieving a 100% win rate
in nearly all hard and super-hard SMAC scenarios. Consequently, we chose HPN-QMIX and HPN-VDN as state-ofthe-art (SOTA) algorithms. To ensure fairness, all algorithms in this study are developed and tested using their opensource project, pymarl32 . Therefore, the subsequent sections will present the experimental results of six algorithms:
QTypeMix, QTypeMix-B, HPN-QMIX, HPN-VDN, FT-QMIX, and FT-VDN.
Evaluation Metrics: Since the ultimate goal of both SMAC and SMACv2 is to achieve victory in battles, we use
the test win rate across different scenarios as the evaluation metric. The following sections will show how the test win
rate changes for different algorithms as the number of training steps increases. Each model is trained for 10,050,000
steps, with a test of 32 episodes conducted every 10,000 steps. To ensure fairness, we use the same training parameters
for each algorithm in the same scenarios and do not perform detailed tuning for any specific algorithm. For detailed
training parameter settings, please refer to the Appendix B.2. Furthermore, we conducted longer testing periods to
obtain more objective and accurate results. Each trained model is tested for win rates over 1280 (32*40) episodes in
the SAMC scenario and 5120 (32*160) in the SMACv2 scenario. The difference in the number of test episodes is due
to the insufficient randomness in the SMAC scenario when tested in a multi-threaded parallel manner, which results
in longer testing times.

5.1. Experiments on SMAC
Since the proposed QTypeMix is a type-based VFF method, it is foreseeable that there will be no performance
improvement in scenarios involving a small number of types. In SMAC, most scenarios involve only 1 to 2 types of
agents and have limited randomness. Therefore, we selected 1 hard map and 4 super hard maps for our experiments.
1 https://github.com/hijkzzz/pymarl2
2 https://github.com/tjuHaoXiaotian/pymarl3

Songchen Fu et al.: Preprint submitted to Elsevier

Page 9 of 16

0.75
0.50
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps
corridor
1.00
0.75
0.50
0.25

1.00

3s5z_vs_3s6z

0.75
0.50
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps
MMM2
1.00

Battle Win Rate (%)

2c_vs_64zg
Battle Win Rate (%)

1.00

Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

6h_vs_8z

1.00
0.75
0.50
0.25

0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps

0.75
0.50
0.25

0.000.0 0.2 0.4 0.6 0.8 1.0 0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
1e7
Steps
Steps
QTypeMix
QTypeMix-B
HPN-QMIX
HPN-VDN

FT-QMIX

FT-VDN

Fig. 5: Test battle win rate curves of the six algorithms during the training process on 4 SMAC maps.

Fig. 5 presents the experimental results of six algorithmsâ€”QTypeMix, QTypeMix-B, HPN-QMIX, HPN-VDN,
FT-QMIX, and FT-VDNâ€”on 5 maps, which generally align with our expectations. Due to the relatively low difficulty
of the SMAC maps, our algorithm and the SOTA algorithms almost all achieve near 100% win rates. However,
differences in convergence speed can still be observed. Firstly, on maps with only one type of agent (2c vs. 64zg,
6h vs. 8z, corridor), QTypeMix shows a significant improvement in convergence rate only on 2c vs. 64zg. On the map
with two types of agents (3s5z vs. 3s6z), it slightly improves the convergence rate. On the map with three types of
agents (MMM2), where the convergence speed of HPN-QMIX is already breakneck at approximately 3,000,000 steps,
QTypeMix and QTypeMix-B achieve convergence in around 1,600,000 steps. This indicates that QTypeMix enables
the strategy to focus on practical information more quickly and efficiently, simplifying the training process. The test
results of the trained models over 1280 episodes are shown in Table 1. Fig. 6 is a box plot drawn based on these results.
QTypeMix, QTypeMix-B, and HPN-QMIX achieve SOTA performance across all tasks, with QTypeMix performing
slightly better and QTypeMix-B lacking stability. HPN-VDN also reaches SOTA performance in some scenarios.
In contrast, FT-QMIX and FT-VDN generally perform poorly in most cases. Overall, on SMAC maps, QTypeMix
undoubtedly achieves the best convergence speed and win rate performance compared to other algorithms. However,
due to the limitations of the maps, the improvement over HPN-QMIX is relatively tiny.

5.2. Experiments on SMACv2
As noted earlier, SMAC maps are not fully capable of showcasing the superior performance of QTypeMix.
Therefore, we conduct experiments on 9 maps selected from SMACv2. We group these maps according to the number
of ally and enemy agents: 5 vs. 5 (protoss 5 vs. 5, terran 5 vs. 5, zerg 5 vs. 5), 10 vs. 11 (protoss 10 vs. 11, terran 10 vs.
11, zerg 10 vs. 11), and 15 vs. 16 (protoss 15 vs. 16, terran 15 vs. 16, zerg 15 vs. 16). On these maps, regardless of the
number of allied agents, three types of units are generated with probabilities of [0.45, 0.45, 0.1]. The significant amount
of randomness introduced makes SMACv2 maps much more challenging than those in SMAC. Achieving victory is
almost impossible when there is a significant disparity in the initial unit strengths or unfavorable positioning.
As shown in Table 1 and Fig. 7, the experimental results on 9 SMACv2 maps indicate that QTypeMix exhibits the
highest win rate among the 6 algorithms on all maps. This is particularly evident in scenarios with a larger number of
ally agents (10 vs. 11 and 15 vs. 16), where the win rate of QTypeMix significantly improves. This confirms our earlier
hypothesis that QTypeMix demonstrates its superiority in scenarios with a higher number of ally agent types (although
the 5 vs. 5 series maps also feature three types of agents randomly, due to the limited number of agents, often only
two types are present simultaneously). On the 5 vs. 5 series maps, the performance of QTypeMix, QTypeMix-B, and
Songchen Fu et al.: Preprint submitted to Elsevier

Page 10 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

0.7
0.6
QTypeMix

0.9

1.0

0.8
0.7
0.6
0.5

6h_vs_8z

0.9
0.8
0.7
0.6
0.5

QTypeMix-B

HPN-QMIX

corridor

1.0
0.9
0.8
0.7
0.6

HPN-VDN

MMM2

1.00
Battle Win Rate (%)

0.8

3s5z_vs_3s6z

Battle Win Rate (%)

0.9

1.0

Battle Win Rate (%)

2c_vs_64zg
Battle Win Rate (%)

Battle Win Rate (%)

1.0

FT-QMIX

0.95
0.90
0.85
0.80
FT-VDN

Battle Win Rate (%)
Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

protoss_5_vs_5
protoss_10_vs_11
1.00
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
1e7
Steps
Steps
terran_5_vs_5
terran_10_vs_11
1.00
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
1e7
Steps
Steps
zerg_5_vs_5
zerg_10_vs_11
1.00
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
1e7
Steps
Steps
QTypeMix
QTypeMix-B
HPN-QMIX
HPN-VDN
Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

Fig. 6: Battle win rate results of 6 algorithms tested over 40 runs of 32 episodes each on 5 SMAC maps.

protoss_15_vs_16
1.00
0.75
0.50
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps
terran_15_vs_16
1.00
0.75
0.50
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps
zerg_15_vs_16
1.00
0.75
0.50
0.25
0.000.0 0.2 0.4 0.6 0.8 1.0
1e7
Steps
FT-QMIX
FT-VDN

Fig. 7: Comparison of QTypeMix and QTypeMix-B with Baseline Algorithms on SMACv2.

HPN-QMIX is similar. The test win rates in 10 vs. 11 and 15 vs. 16 series maps follow the general trend of QTypeMix
> QTypeMix-B > HPN-QMIX. Comparing the results of QTypeMix and QTypeMix-B reveals the effectiveness of
extracting type embeddings. QTypeMix provides a more precise direction for the modelâ€™s learning, resulting in faster
convergence speeds on most maps, even though its neural network is larger.
It is important to note that our objective is not to achieve the highest possible win rate for any specific algorithm
through meticulous hyperparameter tuning. Instead, we adopt the algorithm parameter settings from Hu et al. (2021),
allowing for a fair comparison by keeping common parameters consistent across all algorithms. This approach ensures
that the conclusions drawn are more objective. Although fine-tuning the training and model parameters might yield
Songchen Fu et al.: Preprint submitted to Elsevier

Page 11 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

better results on SMACv2 maps, that is beyond the scope of our study. For more detailed information on model
parameters, please refer to the Appendix B.3.

0.25
0.00
1.00

Battle Win Rate (%)

Battle Win Rate (%)

0.75

0.25
0.00
1.00

Battle Win Rate (%)

Battle Win Rate (%)

0.75

0.25
0.00

QTypeMix

0.25

0.50
0.25

0.75
0.50
0.25
0.00

QTypeMix-B

0.50
0.25

zerg_10_vs_11

0.75
0.50
0.25
0.00
1.00

terran_15_vs_16

zerg_5_vs_5

0.75

0.00
1.00

terran_10_vs_11

0.75

0.00
1.00

protoss_15_vs_16

0.50

0.50

0.00
1.00

protoss_10_vs_11

0.50

0.75

1.00

Battle Win Rate (%)

0.50

terran_5_vs_5

Battle Win Rate (%)

Battle Win Rate (%)

Battle Win Rate (%)

0.75

1.00

Battle Win Rate (%)

protoss_5_vs_5

1.00

zerg_15_vs_16

0.75
0.50
0.25
0.00

HPN-QMIX

HPN-VDN

FT-QMIX

FT-VDN

Fig. 8: Battle win rate results of 6 algorithms tested over 160 runs of 32 episodes each on 9 SMACv2 maps.

6. Conclusion
Our core concept is to enable each agent to recognize the roles they are best suited for in heterogeneous multiagent cooperative tasks. Therefore, this paper proposes a dual-layer VFF method, QTypeMix, which introduces type
information. This method divides the value decomposition process into homogeneous and heterogeneous stages based
on the type-related information provided by the environment. Utilizing fine-grained value decomposition accelerates
learning efficiency. Additionally, it extracts hidden features of different types from the agentsâ€™ historical observations,
providing a more precise direction for the training process. Experimental results on SMAC and SMACv2 indicate that
Songchen Fu et al.: Preprint submitted to Elsevier

Page 12 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition

QTypeMix performs well in scenarios with fewer agents and types and demonstrates impressive improvements in more
complex scenarios with more agents and types.
However, QTypeMix also has some issues. The larger size mixers slow down the training speed of the model. In
future work, we will explore filtering more practical information to reduce the modelâ€™s dimensionality. Additionally,
QTypeMix is just an instance of our core idea applied to the VFF method. One of our future research focuses will be
on how to apply this concept to policy-based methods.

Acknowledgements
This work is partially supported by the Goal-Oriented Project Independently Deployed by the Institute of Acoustics,
Chinese Academy of Sciences(MBDX202106).

A. Architecture details
Algorithm 1 QTypeMix
1: Input training and model parameters;
2: Initialise ğœƒ = (ğœƒğ‘¡ğ‘’ , ğœƒâ„ğ‘ğ‘› , ğœƒğ‘š1 , ğœƒğ‘š2 ), the parameters of mixing networks, agent networks, and hypernetworks;
3: Set the learning rate ğ›¼ and replay buffer ğ· = {};
4: step = 0, ğœƒ âˆ’ = ğœƒ
5: while step < max_step do
6: ğ‘¡ = 0, ğ‘ 0 = initial state;
7: while ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™ = False and ğ‘¡ < ğ‘¡ğ‘šğ‘ğ‘¥ do
8:
for each agent ğ‘ do do
ğ‘ âˆª {(ğ‘œ , ğ‘¢
9:
ğœğ‘¡ğ‘ = ğœğ‘¡âˆ’1
ğ‘¡ ğ‘¡âˆ’1 )}
10:
ğœ– = epsilon-schedule(step)
{
arg maxğ‘¢ğ‘ ğ‘„(ğœğ‘¡ğ‘ , ğ‘¢ğ‘ğ‘¡ ) with probability 1 âˆ’ ğœ–
ğ‘
ğ‘¡
11:
ğ‘¢ğ‘¡ =
randint(1, |î‰ |)
with probability ğœ–
12:
end for
13:
Get reward ğ‘Ÿğ‘¡ and next state ğ‘ ğ‘¡+1
14:
îˆ° = îˆ° âˆª {(ğ‘ ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘Ÿğ‘¡ , ğ‘ ğ‘¡+1 )}
15:
ğ‘¡ = ğ‘¡ + 1, step = step + 1
16: end while
17: if |îˆ°| > batch_size then
18:
ğ‘ â† random batch of episodes from îˆ°
19:
for each timestep ğ‘¡ in each episode in batch ğ‘ do do
20:
ğ‘„ğ‘¡ğ‘œğ‘¡ = Total-Mixer(Type-Mixer1 (ğ‘„1 (ğœğ‘¡1 , ğ‘¢1ğ‘¡ ), â€¦ , ğ‘„ğ‘› (ğœğ‘¡ğ‘› , ğ‘¢ğ‘›ğ‘¡ )), â€¦ ,
21:
Type-Mixerğ‘š (ğ‘„1 (ğœğ‘¡1 , ğ‘¢1ğ‘¡ ), â€¦ , ğ‘„ğ‘› (ğœğ‘¡ğ‘› , ğ‘¢ğ‘›ğ‘¡ )); ğœƒ)
22:
Calculate target ğ‘„ğ‘¡ğ‘œğ‘¡ using all Mixing-networks and the Type Embedding Extractor.
23:
end for
24:
Î”ğ‘„ğ‘¡ğ‘œğ‘¡ = ğ‘Ÿğ‘¡ + ğ›¾ maxğ‘¢â€² ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‘ â€² , ğ‘¢â€² ; ğœƒ âˆ’ ) âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‘ , ğ‘¢; ğœƒ)
{
(
)
âˆ‘ğ‘› âˆ‘ğ‘›
âˆ’1, if ğ‘ğ‘– âˆˆ î‰€ğ‘˜ , ğ‘ğ‘— âˆˆ î‰€ğ‘˜
ğ‘–
ğ‘—
25:
Î”ğ‘‡ = ( ğ‘– ğ‘— ğ¼(ğ‘–, ğ‘—) â‹… cos ğ¸ğ‘– (ğ‘ , ğ‘†; ğœƒğ‘¡ğ‘’ ), ğ¸ğ‘— (ğ‘ , ğ‘†; ğœƒğ‘¡ğ‘’ ) , ğ¼(ğ‘–, ğ‘—) =
1,
if ğ‘ğ‘– âˆˆ î‰€ğ‘˜ , ğ‘ğ‘— âˆ‰ î‰€ğ‘˜
2
26:
Î”ğœƒ = âˆ‡ğœƒ (Î”ğ‘„ğ‘¡ğ‘œğ‘¡ ) + ğ›¼ â‹… Î”ğ‘‡
27:
ğœƒ = ğœƒ âˆ’ lr â‹… Î”ğœƒ
28: end if
29: if target_update_interval steps have passed then
30:
ğœƒâˆ’ = ğœƒ
31: end if
32: end while

Songchen Fu et al.: Preprint submitted to Elsevier

Page 13 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
Table B.1
Key environment, model, and training parameters.
Environment

Model

Training

Parameters

Value

Parameters

Value

Parameters

difficulty
obs_last_action
state_last_action
state_timestep_number
conic_fov

7
False
True
False
False

mixing_embed_dim
hypernet_embed_dim
hpn_hyper_dim
rnn_hidden_dim
hpn_hyper_activation
other_activation

32
64
64
64
RELU
ELU

lr
gamma
buffer_size
test_interval
test_nepisode
target_update_interval
ğœ–start
ğœ–finish
epsilon_anneal_time
optimizer
ğ›¼ğ‘œğ‘ğ‘¡
max_step

Value
1e-3
0.99
5000
10000
32
200
1.0
0.05
100000
adam
0.99
1.005e7

B. Experiment details
B.1. Environment parameters
The SMAC and SMACv2 officials provide environment parameters for users to configure maps. For the sake of
fairness, we primarily use the default parameters. It is important to note that since our algorithm does not target the
exploration problem of agents, we set the parameter conic_fov to False on SMACv2 maps. This means that in our
experiments, agents have a circular field of view on SMACv2 maps, similar to SMAC maps.

B.2. Training parameters
Training parameters in Table B.1 are consistent across all maps and algorithms. ğœ– represents the probability of an
agent taking random actions during training. Typically, a larger ğœ– is used at the beginning of the algorithm to increase
the diversity of samples. As we have emphasized multiple times, we do not aim to achieve better performance through
fine-tuning parameters for any specific algorithm. Therefore, we adopt the parameter configurations from Jianye et al.
(2022), which results in some maps using unique configurations. For example, batch_size_run controls the number of
parallel environments, set to 4 on 3s5z vs. 3s6z and 8 on other maps. td_lambda is set to 0.3 on 6h vs. 8z and 0.6 on
other maps. hpn_head_num is set to 2 on 3s5z vs. 3s6z and 6h vs. 8z, and 1 on other maps. Notably, hpn_head_num
does not affect FT-QMIX and FT-VDN.

B.3. Model parameters
Model parameters are used to configure the neural networks involved in the algorithm. Table B.1 lists some important model parameters applied to all mentioned algorithms (if needed). mixing_embed_dim, hypernet_embed_dim,
hpn_hyper_dim, and rnn_hidden_dim represent the output dimension of the linear layers in the Mixers, the internal
dimension of the hypernetworks in the Mixers, the internal dimension of the hypernetworks in HPN, and the hidden
state dimension of the GRUs, respectively. n_heads sets the number of attention heads for QTypeMix and QTypeMix-B.

CRediT authorship contribution statement
Songchen Fu: Conceptualization, Data curation, Formal analysis, Methodology, Project administration, Investigation, Software, Visualization, Writing â€“ original draft, Writing â€“ review and editing. Shaojing Zhao: Methodology,
Investigation, Writing â€“ review and editing. Ta Li: Supervision, Methodology, Writing â€“ review and editing. Yonghong
Yan: Supervision, Conceptualization, Resources.

References
Bernstein, D.S., Givan, R., Immerman, N., Zilberstein, S., 2002. The complexity of decentralized control of markov decision processes. Mathematics
of operations research 27, 819â€“840.

Songchen Fu et al.: Preprint submitted to Elsevier

Page 14 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
Chen, Z., Luo, B., Hu, T., Xu, X., 2023. Ljir: Learning joint-action intrinsic reward in cooperative multi-agent reinforcement learning. Neural
Networks 167, 450â€“459.
Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., Pineau, J., 2019. Tarmac: Targeted multi-agent communication, in: International
Conference on machine learning, PMLR. pp. 1538â€“1546.
De Witt, C.S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P.H., Sun, M., Whiteson, S., 2020. Is independent learning all you need in the
starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533 .
Ellis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M., Mahajan, A., Foerster, J., Whiteson, S., 2024. Smacv2: An improved benchmark for
cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems 36.
Foerster, J., Assael, I.A., De Freitas, N., Whiteson, S., 2016. Learning to communicate with deep multi-agent reinforcement learning. Advances in
neural information processing systems 29.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., Whiteson, S., 2018. Counterfactual multi-agent policy gradients, in: Proceedings of the AAAI
conference on artificial intelligence.
Gordon, D.M., 1996. The organization of work in social insect colonies. Nature 380.
Gronauer, S., Diepold, K., 2022. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence Review 55, 895â€“943.
Gupta, J.K., Egorov, M., Kochenderfer, M., 2017. Cooperative multi-agent control using deep reinforcement learning, in: Autonomous Agents
and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, SÃ£o Paulo, Brazil, May 8-12, 2017, Revised Selected Papers 16, Springer. pp.
66â€“83.
Gupta, T., Mahajan, A., Peng, B., Boehmer, W., Whiteson, S., 2021. Uneven: Universal value exploration for multi-agent reinforcement learning,
in: Proceedings of the 38th International Conference on Machine Learning, PMLR. pp. 3930â€“3941.
Ha, D., Dai, A., Le, Q.V., 2016. Hypernetworks. arXiv preprint arXiv:1609.09106 .
Hu, J., Jiang, S., Harding, S.A., Wu, H., wei Liao, S., 2021. Rethinking the implementation tricks and monotonicity constraint in cooperative
multi-agent reinforcement learning arXiv:2102.03479.
Iqbal, S., Sha, F., 2019. Actor-attention-critic for multi-agent reinforcement learning, in: International conference on machine learning, PMLR. pp.
2961â€“2970.
Jeanson, R., Kukuk, P.F., Fewell, J.H., 2005. Emergence of division of labour in halictine bees: contributions of social interactions and behavioural
variance. Animal behaviour 70, 1183â€“1193.
Jiang, H., Shi, D., Xue, C., Wang, Y., Wang, G., Zhang, Y., 2021. Multi-agent deep reinforcement learning with type-based hierarchical group
communication. Applied Intelligence 51, 5793â€“5808.
Jiang, J., Lu, Z., 2021. The emergence of individuality, in: International Conference on Machine Learning, PMLR. pp. 4992â€“5001.
Jianye, H., Hao, X., Mao, H., Wang, W., Yang, Y., Li, D., Zheng, Y., Wang, Z., 2022. Boosting multiagent reinforcement learning via permutation
invariant and permutation equivariant networks, in: The Eleventh International Conference on Learning Representations.
Kraemer, L., Banerjee, B., 2016. Multi-agent reinforcement learning as a rehearsal for decentralized planning. Neurocomputing 190, 82â€“94.
Lauer, M., Riedmiller, M.A., 2000. An algorithm for distributed reinforcement learning in cooperative multi-agent systems, in: Proceedings of the
seventeenth international conference on machine learning, pp. 535â€“542.
Lhaksmana, K.M., Murakami, Y., Ishida, T., 2018. Role-based modeling for designing agent behavior in self-organizing multi-agent systems.
International Journal of Software Engineering and Knowledge Engineering 28, 79â€“96.
Lowe, R., Wu, Y.I., Tamar, A., Harb, J., Pieter Abbeel, O., Mordatch, I., 2017. Multi-agent actor-critic for mixed cooperative-competitive
environments. Advances in neural information processing systems 30.
Lyu, X., Amato, C., 2020. Likelihood quantile networks for coordinating multi-agent reinforcement learning., in: International Conference on
Autonomous Agents and Multiagent Systems (AAMAS).
Mahajan, A., Rashid, T., Samvelyan, M., Whiteson, S., 2019. Maven: Multi-agent variational exploration. Advances in neural information processing
systems 32.
Mguni, D.H., Jafferjee, T., Wang, J., Perez-Nieves, N., Slumbers, O., Tong, F., Li, Y., Zhu, J., Yang, Y., Wang, J., 2021. Ligs: Learnable intrinsicreward generation selection for multi-agent learning, in: International Conference on Learning Representations.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.,
2015. Human-level control through deep reinforcement learning. nature 518, 529â€“533.
Oliehoek, F.A., Amato, C., et al., 2016. A concise introduction to decentralized POMDPs. volume 1. Springer.
Oliehoek, F.A., Spaan, M.T., Vlassis, N., 2008. Optimal and approximate q-value functions for decentralized pomdps. Journal of Artificial
Intelligence Research 32, 289â€“353.
Phan, T., Ritz, F., Belzner, L., Altmann, P., Gabor, T., Linnhoff-Popien, C., 2021. Vast: Value function factorization with variable agent sub-teams.
Advances in Neural Information Processing Systems 34, 24018â€“24032.
Pu, Z., Wang, H., Liu, Z., Yi, J., Wu, S., 2022. Attention enhanced reinforcement learning for multi agent cooperation. IEEE Transactions on Neural
Networks and Learning Systems 34, 8235â€“8249.
Rashid, T., Samvelyan, M., De Witt, C.S., Farquhar, G., Foerster, J., Whiteson, S., 2020. Monotonic value function factorisation for deep multi-agent
reinforcement learning. Journal of Machine Learning Research 21, 1â€“51.
Samvelyan, M., Rashid, T., De Witt, C.S., Farquhar, G., Nardelli, N., Rudner, T.G., Hung, C.M., Torr, P.H., Foerster, J., Whiteson, S., 2019. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043 .
Shao, J., Lou, Z., Zhang, H., Jiang, Y., He, S., Ji, X., 2022. Self-organized group for cooperative multi-agent reinforcement learning. Advances in
Neural Information Processing Systems 35, 5711â€“5723.
Shen, S., Qiu, M., Liu, J., Liu, W., Fu, Y., Liu, X., Wang, C., 2022. Resq: A residual q function-based approach for multi-agent reinforcement
learning value factorization. Advances in Neural Information Processing Systems 35, 5471â€“5483.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., Yi, Y., 2019. Qtran: Learning to factorize with transformation for cooperative multi-agent
reinforcement learning, in: International conference on machine learning, PMLR. pp. 5887â€“5896.

Songchen Fu et al.: Preprint submitted to Elsevier

Page 15 of 16

QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value
Decomposition
Sun, W.F., Lee, C.K., Lee, C.Y., 2021. Dfac framework: Factorizing the value function via quantile mixture for multi-agent distributional q-learning,
in: International Conference on Machine Learning, PMLR. pp. 9945â€“9954.
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J.Z., Tuyls, K., et al., 2017.
Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 .
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., Vicente, R., 2017. Multiagent cooperation and competition with
deep reinforcement learning. PloS one 12, e0172395.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., Polosukhin, I., 2017. Attention is all you need. Advances
in neural information processing systems 30.
Wang, L., Wang, K., Pan, C., Xu, W., Aslam, N., Hanzo, L., 2020a. Multi-agent deep reinforcement learning-based trajectory planning for multi-uav
assisted mobile edge computing. IEEE Transactions on Cognitive Communications and Networking 7, 73â€“84.
Wang, T., Dong, H., Lesser, V., Zhang, C., 2020b. Roma: Multi-agent reinforcement learning with emergent roles. arXiv preprint arXiv:2003.08039
.
Wang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson, S., Zhang, C., 2020c. Rode: Learning roles to decompose multi-agent tasks. arXiv preprint
arXiv:2010.01523 .
Wang, Z., Du, J., Jiang, C., Xia, Z., Ren, Y., Han, Z., 2022. Task scheduling for distributed auv network target hunting and searching: An energyefficient aoi-aware dmappo approach. IEEE Internet of Things Journal 10, 8271â€“8285.
Wittemyer, G., Getz, W.M., 2007. Hierarchical dominance structure and social organization in african elephants, loxodonta africana. Animal
Behaviour 73, 671â€“681.
Wong, A., BÃ¤ck, T., Kononova, A.V., Plaat, A., 2023. Deep multiagent reinforcement learning: Challenges and directions. Artificial Intelligence
Review 56, 5023â€“5056.
Xu, Z., Lyu, Y., Pan, Q., Hu, J., Zhao, C., Liu, S., 2018. Multi-vehicle flocking control with deep deterministic policy gradient method, in: 2018
IEEE 14th International Conference on Control and Automation (ICCA), IEEE. pp. 306â€“311.
Yang, Y., Hao, J., Liao, B., Shao, K., Chen, G., Liu, W., Tang, H., 2020. Qatten: A general framework for cooperative multiagent reinforcement
learning. arXiv preprint arXiv:2002.03939 .
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., Wu, Y., 2022. The surprising effectiveness of ppo in cooperative multi-agent games.
Advances in Neural Information Processing Systems 35, 24611â€“24624.
Zang, Y., He, J., Li, K., Fu, H., Fu, Q., Xing, J., Cheng, J., 2024. Automatic grouping for efficient cooperative multi-agent reinforcement learning.
Advances in Neural Information Processing Systems 36.
Zhang, K., Yang, Z., BaÅŸar, T., 2021. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of
reinforcement learning and control , 321â€“384.
Zhang, L., Li, J., Shi, H., Hwang, K.S., et al., 2022. Multi-agent reinforcement learning by the actor-critic model with an attention interface.
Neurocomputing 471, 275â€“284.

Songchen Fu et al.: Preprint submitted to Elsevier

Page 16 of 16

