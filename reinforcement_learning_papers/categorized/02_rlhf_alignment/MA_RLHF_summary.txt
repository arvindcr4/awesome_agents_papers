Here is the extracted information:

**Paper Information**

* Filename: Not specified
* Title: MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions
* Research Method: 02_rlhf_alignment (Reinforcement Learning from Human Feedback Alignment)

**Method Description**

MA-RLHF is a novel framework that incorporates macro actions into Reinforcement Learning from Human Feedback (RLHF) to enhance the alignment of Large Language Models (LLMs) with human preferences. Macro actions are sequences of tokens or higher-level language constructs that reduce the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment.

**Key Contributions**

* Propose MA-RLHF, a simple yet effective RLHF framework that integrates macro actions into RLHF to align LLMs with human preferences.
* Demonstrate the effectiveness of MA-RLHF through extensive experiments across various datasets and tasks, including text summarization, dialogue generation, question answering, and code generation.
* Show that MA-RLHF achieves 1.7× to 2× faster learning efficiency in reward scores during training compared to standard token-level RLHF, without introducing additional computational costs during training or inference.
* Exhibit strong scalability across model sizes ranging from 2B to 27B parameters.

Let me know if you'd like me to extract any further information!
(NOTE: Processed using first-chunk strategy due to file size)
