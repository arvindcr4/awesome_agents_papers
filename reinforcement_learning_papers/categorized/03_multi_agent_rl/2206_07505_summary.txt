Here is the extracted information:

**PAPER**: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning
**TITLE**: Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning
**ARXIV_ID**: 2206.07505v2
**RESEARCH_METHOD**: 03_multi_agent_rl
**METHOD_DESCRIPTION**: The paper re-examines two common practices in cooperative multi-agent reinforcement learning (MARL): value decomposition and policy sharing. The authors argue that these practices can be problematic in certain scenarios, such as highly multi-modal reward landscapes, and propose alternative methods, including policy gradient methods and auto-regressive policy learning.
**KEY_CONTRIBUTIONS**:
* The paper provides a theoretical analysis of the limitations of value decomposition and policy sharing in MARL.
* The authors propose alternative methods, including policy gradient methods and auto-regressive policy learning, to address these limitations.
* The paper presents empirical results demonstrating the effectiveness of these alternative methods in various MARL benchmarks, including StarCraft Multi-Agent Challenge and Google Research Football.
* The authors provide practical suggestions for implementing effective policy gradient algorithms in general multi-agent Markov games.
