1

Deep Reinforcement Learning for Robotic Bipedal
Locomotion: A Brief Survey

arXiv:2404.17070v6 [cs.RO] 9 Nov 2025

Lingfan Bao1 , Joseph Humphreys12 , Tianhu Peng1 and Chengxu Zhou1

Abstract‚ÄîBipedal robots are gaining global recognition due
to their potential applications and the rapid advancements in
artificial intelligence, particularly through Deep Reinforcement
Learning (DRL). While DRL has significantly advanced bipedal
locomotion, the development of a unified framework capable of
handling a wide range of tasks remains an ongoing challenge.
This survey systematically categorises, compares, and analyses
existing DRL frameworks for bipedal locomotion, organising
them into end-to-end and hierarchical control schemes. Endto-end frameworks are evaluated based on their learning approaches, whereas hierarchical frameworks are examined in
terms of their layered structures that integrate learning-based
and traditional model-based methods. We provide a detailed
evaluation of the composition, strengths, limitations, and capabilities of each framework. Furthermore, this survey identifies key
research gaps and proposes future directions aimed at creating
a more integrated and efficient unified framework for bipedal
locomotion, with broad applicability in real-world environments.
Index Terms‚ÄîDeep Reinforcement Learning, Humanoid
Robots, Bipedal Locomotion, Legged Robots

I. I NTRODUCTION
Humans navigate complex environments and perform diverse locomotion tasks with remarkable efficiency using only
two legs. Bipedal robots, which closely mimic the human
form, possess distinct advantages over wheeled or tracked alternatives, particularly when traversing uneven and challenging
terrains. Furthermore, bipedal humanoid robots are specifically
designed to operate in human-centric environments, enabling
seamless interaction with tools and infrastructure intended for
human use. This makes them highly adaptable to a wide range
of tasks in such settings.
As a result, bipedal robots hold significant potential for realworld applications [1]. In manufacturing, they can perform
tasks efficiently without requiring additional tools, thereby
enhancing productivity and reducing labour demands [2], [3],
[4]. Their agility is particularly advantageous in complex
environments such as multi-level workplaces. Bipedal robots
are also well suited to tasks that involve the use of humandesigned tools, making them valuable for assisting in daily
activities, healthcare, and rehabilitation [5]. Moreover, they
show considerable promise in search-and-rescue operations,
where they can navigate hazardous and unpredictable terrains
[6], [7], [8].
This work was partially supported by the Royal Society [grant number
RG\R2\232409] and the Advanced Research and Invention Agency [grant
number SMRB-SE01-P06].
1 Department of Computer Science, University College London, UK.
chengxu.zhou@ucl.ac.uk
2 School of Mechanical Engineering, University of Leeds, UK.

(a)

(b)

(c)

(d)

(e)

Fig. 1: Representative bipedal and humanoid robots illustrating the diversity of platforms for locomotion research and
development. (a) Cassie: a torque-controlled bipedal robot
designed for agile locomotion. (b) Digit: a full-sized humanoid
robot evolved from Cassie and actuated by torque control.
(c) H1: a full-size, electric, torque-controlled humanoid robot
developed by Unitree Robotics. (d) G1: a compact humanoid
robot from Unitree featuring lightweight design and high joint
backdrivability. (e) Atlas: a fully electric humanoid robot
developed by Boston Dynamics.

Traditional approaches to bipedal locomotion control, such
as model-based methods, have been prevalent since the 1980s
[9], [10], [11]. Early methods, such as the Linear Inverted Pendulum Model (LIPM) [12], provided simplified representations
of the dynamics involved in bipedal motion, enabling easier
analysis and control. As research progressed, full dynamic
models were introduced to better capture the complexities
of real-world locomotion. Advanced methods such as Model
Predictive Control (MPC) [13], [14] and Trajectory Optimisation (TO) [15], [16], [9] exploit predefined dynamic
models to solve constrained optimal-control problems that
plan footsteps, centre-of-mass (CoM) motion, and contact
forces. While model-based approaches offer rapid convergence
and predictive capabilities, they often struggle in dynamically
complex and uncertain environments where adaptability is
essential.
Reinforcement learning (RL)-based methods, particularly
deep reinforcement learning (DRL), are effective in optimising
robot control policies through direct interaction with the
environment [17], which provides a distinct advantage. Unlike
model-based approaches, which rely on predefined dynamics and may fail under unforeseen conditions, DRL enables
robots to autonomously discover control strategies through
trial and error, achieving greater adaptability and robustness
in diverse environments. In addition, hybrid methods that
combine model-based and learning-based techniques further
enhance planning and control by leveraging the strengths of
both paradigms.

2

Despite these advancements, research in DRL-based locomotion remains highly fragmented, with inconsistencies in
training pipelines, reward formulations, observation spaces,
and evaluation setups that hinder systematic benchmarking and
slow progress towards generalisable locomotion capabilities.
Moreover, many methods are tailored to specific morphologies
or tasks, offering limited transferability across embodiments
and environments.
This fragmentation motivates the following central research
questions: To what extent has current research achieved
generalisation and robustness across diverse morphologies,
terrains, and locomotion tasks? If full generalisation has
not yet been realised, how can existing DRL approaches be
organised and extended towards a unified framework that
enables such capability in bipedal robots? In this context, the
present survey seeks to categorise emerging DRL frameworks
for bipedal locomotion, identify their key limitations, and
outline opportunities for integration and convergence towards
unification.
To address these aspects, we first clarify the ultimate goal
of robot learning: to develop systems that exhibit generalisation, adaptability, and robustness across diverse morphologies,
tasks, and environments. The unified framework is therefore
not the final destination but a conceptual scaffold emerging
from the consolidation of current DRL research efforts. Its role
is to organise fragmented methodologies through shared interfaces, training conventions, and evaluation protocols, thereby
promoting steady progress towards the broader goal of generalisable and adaptive robot-learning systems.
Guided by these definitions, this survey examines recent
advancements in DRL-based frameworks, categorising control
schemes into two primary types: (i) end-to-end and (ii) hierarchical. End-to-end frameworks directly map robot states
to joint-level control outputs, while hierarchical frameworks
decompose decision-making into multiple layers. In hierarchical systems, a High-Level (HL) planner governs navigation
and path planning, while a Low-Level (LL) controller handles
fundamental locomotion tasks. The task-level decision-making
tier interfaces directly with user commands or predefined tasks,
forming a structured approach to robotic control.
The evolution of RL in bipedal robotics has largely advanced through the end-to-end learning paradigm. Early studies in 2004 applied simple policy-gradient methods to 2D
bipeds [18], [19], while later breakthroughs in DRL enabled policy training in high-fidelity physics simulators [20],
[21], [22]. As robotic hardware matured, an increasing variety of bipedal and humanoid platforms emerged, supporting extensive evaluation of DRL-based locomotion across
diverse morphologies, as illustrated in Fig. 1. This evolution
marked the transition from purely simulation-based training to
simulation-to-real (sim-to-real) transfer, where policies trained
in simulators are deployed on physical robots. In 2020, the
first successful sim-to-real transfer of an end-to-end DRL
locomotion policy was achieved on the 3D torque-controlled
bipedal robot Cassie [23]. Subsequent work explored two
principal learning paradigms: reference-based learning, which
leverages TO-generated data or motion-capture data to guide
policy training [24], [25], [26], [27]; and reference-free learn-

ing, where policies are trained entirely from scratch to autonomously discover control strategies [28]. These developments demonstrate that end-to-end frameworks can achieve
robust and versatile locomotion skills across complex terrains
and dynamic environments [29], [30], [31].
Similarly, hierarchical structures have garnered significant
interest. Within this subset, the hybrid approach combines RLbased and model-based methods to enhance both planning
and control strategies. Hybrid architectures often integrate
learning-based and model-based modules to combine adaptability with physical consistency. One representative design
couples a learned HL planner with an LL model-based controller, forming a cascade-structure or deep-planning hybrid
scheme [16], [32], [33]. Alternatively, DRL feedback-control
hybrids embed learned control policies within model-based
feedback loops to enhance tracking precision and disturbance
rejection [34], [35]. Learned hierarchical control schemes [36]
decompose locomotion into multiple layers, each focusing on
specific functions such as navigation and fundamental locomotion skills [20], [21], [37]. To provide a clearer overview of
the current landscape, we categorise existing DRL frameworks
as shown in Fig. 2.
Current progress across both end-to-end and hierarchical
paradigms indicates that a unified framework for DRL-based
bipedal locomotion is still far from being realised. Establishing
such a framework is essential for consolidating diverse learning pipelines, standardising evaluation metrics, and enabling
transferable locomotion capabilities across different robot morphologies. As locomotion tasks become increasingly complex,
ranging from basic stabilisation to dynamic parkour and locomanipulation, the need for consistent benchmarking has intensified.The DARPA Robotics Challenge exemplified this trend
by introducing one of the first large-scale evaluation platforms
for bipedal humanoids performing real-world locomotion and
manipulation tasks [38], highlighting the importance of robustness and practical deployment.
Although several reviews discuss RL for general robotics
[17] and model-based methods for bipedal robots [9], [10],
[11], none specifically focus on DRL-based frameworks for
bipeds. To address this gap, this survey reviews relevant
literature according to the following selection criteria: (1) studies that investigate DRL frameworks specifically for bipedal
robots; (2) research involving both simulated and physical
bipedal robots; (3) approaches that improve policy transfer
from simulation to real-world environments; and (4) publications from the last five years (2018‚ÄìApril 2024) sourced
from reputable databases and conferences, including Google
Scholar, IEEE Xplore, Web of Science, arXiv, and major
robotics venues such as CoRL, RSS, ICRA, IROS, and Humanoids.
The search was conducted using the key terms ‚Äúdeep
reinforcement learning‚Äù or ‚Äúreinforcement learning‚Äù in combination with ‚Äúbipedal locomotion‚Äù, ‚Äúbipedal walking‚Äù, ‚Äúbiped
robot‚Äù, ‚Äúhumanoid robot‚Äù, or ‚Äúlegged robot‚Äù. The most relevant and impactful works were manually selected for further
review. This survey is intended for readers with a foundational
background in robotics who are transitioning to DRL methods,
providing an overview of a wide range of approaches with sim-

3

Reference-free learning
End-to-end

Residual learning
Reference-based
Guided learning

DeepRL for robotic bipedal
locomotion framework
Learned hierarchy scheme
Deep planning
hybrid scheme

Hierarchy
Hybrid control scheme

Learning-based feedback
hybrid scheme

Fig. 2: Classification of DRL-based control schemes. The approaches are broadly categorised into two main paradigms: endto-end frameworks, which learn a single policy from sensory inputs to motor commands; and hierarchical frameworks, which
decompose the control problem into multiple levels. Within the end-to-end paradigm, a key distinction is drawn between
reference-free learning (learning from scratch) and reference-based learning (tracking a predefined motion). Hierarchical
structures include hybrid control schemes, which synergistically combine learned components with traditional model-based
controllers.

plified explanations where appropriate. For clarity, throughout
this paper the term ‚Äúhumanoid robot‚Äù refers specifically to
anthropomorphic bipedal robots.
The primary contributions of this survey are:
‚Ä¢ A comprehensive summary and cataloguing of DRLbased frameworks for bipedal locomotion.
‚Ä¢ A detailed comparison of each control scheme, highlighting their strengths, limitations, and distinctive characteristics.
‚Ä¢ The identification of current challenges and the provision
of insightful future research directions.
The survey is organised as follows: Section II discusses
end-to-end frameworks, categorised by learning approaches;
Section III presents hierarchical frameworks, classified into
three main types; Section IV outlines key limitations and challenges, linking them to the preceding discussions; Section V
explores potential pathways, opportunities, and two proposed
conceptual models that extend the end-to-end and hierarchical
paradigms; finally, Section VI concludes the survey.
II. E ND - TO - END FRAMEWORK
The end-to-end DRL framework represents a holistic approach in which a single neural network (NN) policy, denoted
œÄ(¬∑) : X ‚Üí U , directly maps sensory inputs X , such as
images, LiDAR data, or proprioceptive feedback [39], together
with user commands [28] or pre-defined references [40], into
joint-level control actions U. Here, X represents the sensory
input space, U refers to the space of control actions, and
œÄ(¬∑) denotes the policy function. This framework obviates the
need for manually decomposing the problem into sub-tasks,
streamlining the control process.
End-to-end strategies primarily simplify the design of LL
tracking to basic elements, such as a proportional‚Äìderivative
(PD) controller. These methods can be broadly categorised
according to their reliance on prior knowledge into two types:
reference-based and reference-free. The locomotion skills de-

veloped through these diverse learning approaches exhibit
considerable variation in performance and adaptability.
The following sections delve into various representation
frameworks, exploring their characteristics, limitations, and
strengths in comprehensive detail. To facilitate an understanding of these distinctions, Table I provides a succinct overview
of the frameworks discussed.
A. Reference-based learning
Reference-based learning leverages prior knowledge generated offline through methods such as TO or motion capture
systems. This predefined reference typically includes data
related to the robot‚Äôs joint movements or pre-planned trajectories, serving as a foundation for the policy to develop
locomotion skills by following these established motion patterns. Generally, this approach can be divided into two primary
methods: (i) residual learning and (ii) guided learning.
1) Residual learning: The proposed framework utilises
a policy that modifies motor commands by applying action
offsets based on the current reference joint positions, allowing the biped robot to achieve dynamic locomotion through
error compensation. The state space includes proprioceptive
information such as trunk position, orientation, velocity, angular velocity, joint angles, and joint velocities, providing the
necessary sensory data for real-time adjustments. Actions are
defined by offsets, Œ¥a, which represent deviations from the
predefined desired joint positions, aÃÇ, with the final motor
commands represented as a = aÃÇ + Œ¥a. The reward function
encourages the policy to optimise locomotion performance by
considering (a) how closely the robot‚Äôs active joint angles
match the reference angles, (b) how effectively the robot
responds to user commands, and (c) additional terms that
further enhance the stability of the robot‚Äôs movements. This
holistic approach enables the biped robot to adapt to various
dynamic conditions while maintaining balance and control.
Introduced in 2018, a residual learning framework for the
bipedal robot Cassie marked a significant advancement [41].

4

TABLE I: Summary and comparison of reference-based and reference-free learning approaches for the end-to-end framework.
The dashed line in the implementation flow chart indicates optional steps.
Methods

Works

Residual
learning

[41]
[23]

Guided
learning

Reference-free
learning

[42]
[40]
[43]
[31]

Capabilities

Characteristic

Advantages and Disadvantages

A: Fast convergence speed
Adding a residual term to the D: Requires high-quality predefined
Forward walk
known motor positions at the reference, limits to specific motions,
unidirectional walk
current time step.
and lacks robustness to complex terrains.
Forward walk
Versatile walk
Versatile jump
Versatile motions

A: Accelerates the learning process
Mimic the predefined refer- and is robust across terrains.
ence and directly specifies D: Limits to the predefined motions
joint-level commands.
and lacks adaptability to unforeseen
changes in environment.

A: High potential for gait exploration, highly robust to complex ter[28]
Periodic motions Learn locomotion skills rain
[44] Stepping stones walk from scratch without any D: Requires intensive reward shap[29]
Visual walk
prior knowledge.
ing for gait patterns and is relatively expensive in computational resources

Implementation Flow Chart

User
Commands

Policy ùúã

Robots

Reference

User
Commands

Policy ùúã
Robots

Reference

Policy ùúã
User
Commands

Robots

Forward Walk involves bipeds walking straight ahead. Unidirectional Walk enables bipeds to move forward and backward within a range of desired
velocities. Omnidirectional Walk grants bipeds the ability to walk in any direction. Versatile Walk allows the bipeds to walk forward, backward, turn, and
move sideways, providing extensive movement capabilities. Periodic Motions entails the execution of various repeated gait patterns, such as walking, hopping,
or galloping. Versatile Jump refers to jumping towards different desired targets. Versatile Motions cover performing a broad array of motions, both periodic
and aperiodic such as jumping.

This framework allowed the robot to walk forward by incorporating a policy trained via Proximal Policy Optimisation (PPO)
algorithms, as detailed in Appendix A. The policy receives
the robot‚Äôs states and reference inputs, outputting a residual
term that augments the reference at the current timestep. These
modified references are then processed by a PD controller to
set the desired joint positions. Although this framework has
improved the robot‚Äôs ability to perform tasks beyond standing
[45], it has yet to be physically deployed on a bipedal robot.
As a result, it remains impractical for managing walking at
varying speeds and is constrained to movement in a single
direction.
To transition this framework to a real robot, a sim-to-real
strategy based on the previous model was demonstrated, where
the policy, trained through a residual learning approach, was
subsequently applied to a physical bipedal robot [23]. Compared to model-based methods, this training policy achieves
faster running speeds on the same platform, underlining the
considerable potential of DRL-based frameworks. However,
the robot‚Äôs movements remain constrained to merely walking
forward or backward.
A unique residual learning approach was introduced to enable omnidirectional walking, where the policy adds a residual
term to the current joint positions, allowing gradual omnidirectional walking [46]. In this case, the desired reference is
the robot‚Äôs current joint positions, which makes the approach
distinctive. However, this also limits the policy‚Äôs ability to
explore more diverse motions, restricting it to a single slow
walking pattern.
Residual learning enhances an existing control policy by
taking current joint positions or states and applying a residual
action to adjust reference actions for better performance.

Compared to other learning approaches that directly output
joint positions, it is highly sample efficient [32]. However,
when predefined references are unstable or of low quality,
residual learning may struggle, especially on complex terrains,
as the action space is bounded by the reference, limiting the
ability to handle unpredictable or uneven terrains.
2) Guided learning: Guided learning trains policies to
directly output the desired joint-level commands as actions
a, without relying on the addition of a residual term. The
state space is the same as the residual-learning approach. In
this approach, the reward structure is centred on accurately
imitating predefined reference trajectories, ensuring precise
alignment between the policy output and the reference motion.
A sim-to-real framework that employs periodic references to
initiate the training phase was proposed in [42]. In this framework, the action space directly maps to the joint angles, and
desired joint positions are managed by joint PD controllers.
The framework also incorporates a Long Short-Term Memory
(LSTM) network, as detailed in the Appendix A, which is
synchronised with periodic time inputs. However, this model is
limited to a single locomotion goal: forward walking. A more
diverse and robust walking DRL framework that includes a
Hybrid Zero Dynamics (HZD) gait library was demonstrated
[40], achieving a significant advancement by enabling a single
end-to-end policy to facilitate walking, turning, and squatting.
Despite these advancements, the parameterisation of reference motions introduces constraints that limit the flexibility of the learning process and the policy‚Äôs response to
disturbances. To broaden the capabilities of guided learning
policies, a framework capable of handling multiple targets,
including jumping, was developed [43]. This approach introduced a novel policy structure that integrates long-term

5

input/output (I/O) encoding, complemented by a multi-stage
training methodology that enables the execution of complex
jumping manoeuvrers. An adversarial motion priors approach,
employing a style reward mechanism, was also introduced
to facilitate the acquisition of user-specified gait behaviours
[27]. This method improves the training of high-dimensional
simulated agents by replacing complex hand-designed reward
functions with more intuitive controls.
While previous works primarily focused on specific locomotion skills, a unified framework that accommodates both
periodic and non-periodic motions was further developed [31]
based on the foundational work in [43]. This framework
enhances the learning process by incorporating a wide range
of locomotion skills and introducing a dual I/O history approach, marking a significant breakthrough in creating a robust, versatile, and dynamic end-to-end framework. However,
experimental results indicate that the precision of locomotion
features, such as velocity tracking, remains suboptimal.
Guided learning methods expedite the learning process by
leveraging expert knowledge and demonstrating the capacity
to achieve versatile and robust locomotion skills. Through the
comprehensive evaluation [31], it is demonstrated that guided
learning employs references without complete dependence on
them. Conversely, residual learning exhibits failures or severe
deviations when predicated on references of inferior quality.
This shortfall stems from the framework‚Äôs dependency on
adhering closely to the provided references, which narrows
its learning capabilities.
However, the benefits of reference-based learning come with
inherent limitations. Reliance on predefined trajectories often
confines the policy to specific gaits, restricting its capacity
to explore a broader range of motion possibilities [40], [47].
Moreover, such methods exhibit reduced adaptability when
confronted with novel environments or unforeseen perturbations. These limitations are further compounded by the
difficulty of acquiring high-quality and task-relevant demonstrations.
Common sources of prior knowledge include TO [16], [48],
[40], [31], human motion capture [25], teleoperation [49],
[50], and scripted controllers [21]. While informative, these
demonstrations often require adaptation due to embodiment
mismatch or limited generalisability. Motion retargeting [51],
[26], [52], as one of promising direction, addresses this by
converting human-centric motions into robot-feasible trajectories, yet it still struggles with preserving fidelity and adapting
across morphologies.
Ultimately, the success of guided learning relies not only
on using references but on accessing high-quality, adaptable demonstrations that generalise across tasks and platforms‚Äîhighlighting a key challenge in advancing robust policy learning.
B. Reference-free learning
In reference-free learning, the policy is trained using a carefully crafted reward function rather than relying on predefined
trajectories. This approach allows the policy to explore a wider
range of gait patterns and adapt to unforeseen terrains, thereby

enhancing innovation and flexibility within the learning process. The action space and observation space in this approach
are similar to the guided-learning method; however, the reward
structure differs significantly from the reference-based method.
Instead of focusing on imitating predefined motions, the reward emphasises learning efficient gait patterns by capturing
the distinctive characteristics of bipedal locomotion [47].
The concept of reference-free learning was initially explored
using simulated physics engines with somewhat unrealistic
bipedal models. A pioneering framework, which focused on
learning symmetric gaits from scratch without the use of motion capture data, was developed and validated within a simulated environment [22]. This framework introduced a novel
term into the loss function and utilised a curriculum learning
strategy to effectively shape gait patterns. Another significant
advancement was made in developing a learning method that
enabled a robot to navigate stepping stones using curriculum
learning, focusing on a physical robot model (Cassie), though
this has yet to be validated outside of simulation [53].
Considering the practical implementation of this approach,
significant efforts have been made to develop sim-to-real
reference-free frameworks, and their potential has been further
explored on physical robots. A notable example of such a
framework accommodates various periodic motions, including walking, hopping, and galloping [28]. This framework
employs periodic rewards to facilitate initial training within
simulations before successfully transitioning to a physical
robot. It has been further refined to adapt to diverse terrains
and scenarios. For instance, robust blind walking on stairs
was demonstrated through terrain randomisation techniques
in [44]. Additionally, the integration of a vision system has
enhanced the framework‚Äôs ability to precisely determine foot
locations [54], thus enabling the robot to effectively navigate
stepping stones [29]. Subsequent developments include the
incorporation of a vision system equipped with height maps,
leading to an end-to-end framework that more effectively
generalises terrain information [55].
This approach to learning enables the exploration of novel
solutions and strategies that might not be achievable through
mere imitation of existing behaviours. However, the absence
of reference guidance can render the learning process costly,
time-consuming, and potentially infeasible for certain tasks.
Moreover, the success of this method hinges critically on
the design of the reward function, which presents significant
challenges in specifying tasks such as jumping.
III. H IERARCHY FRAMEWORK
Unlike end-to-end policies that directly map sensor inputs
to motor outputs, hierarchical control schemes deconstruct
locomotion challenges into discrete, manageable layers or
stages of decision-making. Each layer within this structure
is tasked with specific objectives, ranging from navigation
to fundamental locomotion skills. This division not only
enhances the framework‚Äôs flexibility but also simplifies the
problem-solving process for each policy.
The architecture of a hierarchical framework typically comprises two principal modules: an HL planner and an LL

6

Basic Scheme
Task Command

Variations
Task Command

High-level Planner

Task Command

Task Command

High-level Planner

Low-level Controller

Low-level Controller

Robot

Robot

Robot

Robot

Fig. 3: Hierarchical control scheme diagram. This figure illustrates a hierarchical control framework for a bipedal robot,
comprising a basic scheme and three variations. (1) Basic scheme: The framework begins with a task command, followed by
an HL planner and a LL controller, which ultimately drives the robot. Each module can be replaced with a learned policy,
introducing adaptability across different control layers. (2) Variations (from left to right): (a) a deep planning hybrid scheme,
in which the HL planner is learned; (b) a feedback DRL control hybrid scheme, with a learned LL controller; and (c) a learned
hierarchical control scheme, where both layers are learned.

controller. This modular approach allows for the substitution
of each component with either a model-based method or
a learning-based policy, further enhancing adaptability and
customisation to specific needs.
Communication between the layers in a hierarchical framework is achieved through the transmission of commands. The
HL planner sets abstract goals, which the LL controller translates into specific actions, such as calculating joint movements
to follow a desired trajectory. In return, the robot sends sensor
data back to the HL planner, enabling real-time adjustments.
The tasks handled by different layers often operate on varying
time scales, adding complexity to synchronising communication between the layers.
Hierarchical frameworks can be classified into three distinct
types based on the integration and function of their components:
1) Deep planning hybrid scheme: This approach combines strategic, HL planning with dynamic LL execution,
leveraging the strengths of both learning-based and
traditional model-based methods.
2) Feedback DRL control hybrid scheme: It focuses
on integrating direct feedback control mechanisms with
DRL, allowing for real-time adjustments and enhanced
responsiveness.
3) Learned hierarchy scheme: Entirely learning-driven,
this scheme develops a layered decision-making hierarchy where each level is trained to optimise specific
aspects of locomotion.
These frameworks are illustrated in Fig. 3. Each type offers
unique capabilities and exhibits distinct characteristics, albeit
with limitations primarily due to the complexities involved in
integrating diverse modules and their interactions.
For a concise overview, Table II summarises the various
frameworks, detailing their respective strengths, limitations,
and primary characteristics. The subsequent sections will delve

deeper into each of these frameworks, providing a thorough
analysis of their operational mechanics and their application
in real-world scenarios.
A. Deep planning hybrid scheme
In this scheme, robots are pre-equipped with the ability
to execute basic locomotion skills such as walking, typically
managed through model-based feedback controllers or interpretable methods. The addition of an HL learned layer focuses
on strategic goals or the task space, enhancing locomotion
capabilities and equipping the robot with advanced navigation
abilities to effectively explore its environment.
Several studies have demonstrated the integration of an
HL planner policy with a model-based controller to achieve
tasks in world space. A notable framework optimises task
space level performance, eschewing direct joint-level and
balancing considerations [32]. This system combines a residual
learning planner with an inverse dynamics controller, enabling
precise control over task-space commands to joint-level actions, thereby improving velocity tracking, foot touchdown
location, and height control. Further advancements include
a hybrid framework that merges HZD-based residual deep
planning with model-based regulators to correct errors in
learned trajectories, showcasing robustness, training efficiency,
and effective velocity tracking [33]. These frameworks have
been successfully transferred from simulation to reality and
validated on robots such as Cassie.
However, the limitations imposed by residual learning constrained the agents‚Äô capacity to explore a broader array of
possibilities. Building on previous work [33], a more efficient
hybrid framework was developed, which learns from scratch
without reliance on prior knowledge [56]. In this approach,
a purely learning-based HL planner interacts with an LL
controller using an Inverse Dynamics with Quadratic Programming formulation (ID-QP). This policy adeptly captures

7

TABLE II: Summary and comparison of hierarchical frameworks.
Control Scheme

Works

Module

Characteristics

Deep Planning
Hybrid Scheme

[32]
[56]
[57]

Deep planning + ID
Deep planning + ID-QP
Deep planning + WPG

HL policy is learned to guide
the LL controller to complete locomotion and navigation tasks.

Feedback DRL Control
Hybrid Scheme

[48]
[34]
[35]

Gait library + feedback policy
Footstep planner + feedback policy
Model-based planner + feedback policy

LL feedback policy receives
non-learned HL planner as
input to achieve locomotion
skills.

HL policy + LL policy
HL policy + LL policy
HL policy + LL policy

Both HL planner and LL
feedback controller are
learned. LL policy focuses
on basic locomotion skills;
on the other side, HL policy
learns navigation skills.

Learned Hierarchy
Framework

[20]
[37]
[50]

dynamic walking gaits through the use of reduced-order states
and simplifies the learning trajectory. Demonstrating robustness and training efficiency, this framework has outperformed
other models and was successfully generalised across various
bipedal platforms, including Digit, Cassie, and RABBIT.
In parallel, several research teams have focused on developing navigation and locomotion planners for humanoid
robots, leveraging onboard visual perception and learned control strategies. Recent work [57] explored complex dynamic
motion tasks such as playing soccer by integrating a learned
policy with an online footstep planner that utilises weight
positioning generation (WPG) to create a CoM trajectory. This
configuration, coupled with a whole-body controller, enables
dynamic activities like soccer shooting. Although these systems demonstrate promising coordination between perception,
planning, and control, they remain limited in dynamic movement capability compared to full-sized humanoid robots, and
thus primarily address navigation and task-level execution.
Regarding generalisation, these frameworks have shown
potential for adaptation across different types of bipedal robots
with minimal adjustments, demonstrating advanced user command tracking [56] and sophisticated navigation capabilities
[57]. However, limitations are evident, notably the absence of
capabilities for executing more complex and dynamic motions,
such as jumping. Furthermore, while these systems adeptly
navigate complex terrains with obstacles, footstep planning
alone is insufficient without concurrent enhancements to the
robot‚Äôs overall locomotion capabilities. Moreover, the requisite
communication between the two distinct layers of the hierarchical framework may introduce system complexities. Enhancing both navigation and dynamic locomotion capabilities
within the HL planner remains a significant challenge.
B. Feedback DRL control hybrid scheme
In contrast to the comprehensive approach of end-to-end
policies discussed in Section II, which excels in handling

Advantages and Disadvantages
A: Enhanced command tracking capabilities, generalised across different platforms, sampling-efficient, and robust.
D: Complicated system and communication between layers, requires a precise
model, and lacks generalisation across
different tasks.
A: Short inference time, robust, navigation locomotion capabilities, interpretability.
D: Complicated system and communication between layers, which reduces
sampling efficiency.
A: Provides layer flexibility, where each
layer can be independently retrained and
reused; alleviates the challenges associated with training an end-to-end policy.
D: Inefficient sim-to-real transfer, complicated interface between layers, and an
expensive training process.

versatile locomotion skills and complex terrains with minimal
inference time, the Feedback DRL Control Hybrid Scheme
integrates DRL policies as LL controllers. These LL controllers, replacing traditional model-based feedback mechanisms, work in conjunction with HL planners that process
terrain information, plan future walking paths, and maintain
robust locomotion stability.
For instance, gait libraries, which provide predefined movement references based on user commands, have been integrated
into such frameworks [48]. Despite the structured approach of
using gait libraries, their static nature offers limited adaptability to changing terrains, diminishing their effectiveness.
A more dynamic approach involves online planning, which
has shown greater adaptability and efficiency. One notable
framework combines a conventional foot planner with an LL
DRL policy [34], delivering targeted footsteps and directional
guidance to the robot, thereby enabling responsive and varied
walking commands. Moreover, HL controllers can provide
additional feedback to LL policies, incorporating CoM or endfeet information, either from model-based methods or other
conventional control strategies. However, this work has not yet
been transferred from simulation to real-world applications.
Later, a similar structure featuring an HL foot planner and an
LL DRL-based policy was proposed [35]. This strategy not
only achieved a successful sim-to-real transfer but also enabled
the robot to navigate omnidirectionally and avoid obstacles.
A recent development has shown that focusing solely on
foot placement might restrict the stability and adaptability
of locomotion, particularly in complex maneuvers. A new
framework integrates a model-based planner with a DRL
feedback policy to enhance bipedal locomotion‚Äôs agility and
versatility, displaying improved performance [58]. This system
employs a residual learning architecture, where the DRL policy‚Äôs outputs are merged with the planner‚Äôs directives before
being relayed to the PD controller. This integrated approach
not only concerns itself with foot placement but also generates

8

comprehensive trajectories for trunk position, orientation, and
ankle yaw angle, enabling the robot to perform a wide array
of locomotion skills including walking, squatting, turning, and
stair climbing.
Compared to traditional model-based controllers, learned
DRL policies provide a comprehensive closed-loop control
strategy that does not rely on assumptions about terrain or
robotic capabilities. These policies have demonstrated high
efficiency in locomotion and accurate reference tracking [59].
Despite their extensive capabilities, such policies generally
require short inference time, making DRL a preferred approach
in scenarios where robustness is paramount or computational
resources on the robot are limited. Nonetheless, these learning
algorithms often face challenges in environments characterised
by sparse rewards, where suitable footholds like gaps or
stepping stones are infrequent [59].
Additionally, an HL planner can process critical data such
as terrain variations or obstacles and generate precise target locations for feet or desired walking paths, instead of
detailed terrain data, which can significantly expedite the
training process [35]. This capability effectively addresses the
navigational limitations observed in end-to-end frameworks.
Moreover, unlike the deep planning hybrid scheme where
modifications post-policy establishment can be cumbersome,
this hybrid scheme offers enhanced flexibility for on-the-fly
adjustments.
Despite the significant potential demonstrated by previous
studies, integrating DRL-based controllers with sophisticated
and complex HL planners still presents limitations compared
to more integrated frameworks such as end-to-end and deep
planning models. Specifically, complex HL model-based planners often require substantial computational resources to resolve problems, rely heavily on model assumptions, necessitate
extensive training periods, demand large datasets for optimisation, and hinder rapid deployment and iterative enhancements
[59].
C. Learned hierarchy framework
The Learned Hierarchy Framework merges a learned HL
planner with an LL controller, focusing initially on refining LL
policies to ensure balance and basic locomotion capabilities.
Subsequently, an HL policy is developed to direct the robot
towards specific targets, encapsulating a structured approach
to robotic autonomy.
The genesis of this framework was within a physics engine,
aimed at validating its efficiency through simulation [20].
In this setup, LL policies, informed by human motions or
trajectories generated via TO, strive to track these trajectories
as dictated by the HL planner while maintaining balance.
An HL policy is then introduced, pre-trained with longterm task goals, to navigate the environment and identify
optimal paths. This structure enabled sophisticated interactions
such as guiding a biped to dribble a soccer ball towards a
goal. The framework was later enhanced to include imitation
learning (IL), facilitating the replication of dynamic humanlike movements within the simulation environment [21].
However, despite its structured and layered approach, which
allows for the reuse of learned behaviours to achieve long-

term objectives, these frameworks has been validated only in
simulations. The interface designed manually between the HL
planner and the LL controller sometimes leads to suboptimal
behaviours, including stability issues like falling.
Expanding the application of this framework, a sim-to-real
strategy for a wheeled bipedal robot was proposed, focusing
the LL policy on balance and position tracking, while the HL
policy enhances safety by aiding in collision avoidance and
making strategic decisions based on the orientation of subgoals
[37].
To fully leverage its potential, HumanPlus has been developed as a versatile framework for humanoid robots, integrating
hierarchical learning, multimodal perception, and real-world
imitation [50]. It employs a two-layer structure, where HIT
learns from human demonstrations, trained on AMASS, and
HST acts as an LL tracking controller. Additionally, binocular
RGB vision input enhances perception, enabling precise locomanipulation and dynamic locomotion tasks such as jumping, walking, folding clothes, and rearranging objects. This
shadowing-based IL approach improves adaptability, making
it a promising framework for transferring human-like skills to
robots.
Learning complex locomotion skills, particularly when incorporating navigation elements, presents a significant challenge in robotics. Decomposing these tasks into distinct locomotion and navigation components allows robots to tackle
more intricate activities, such as dribbling a soccer ball [20].
As discussed in the previous section, the benefits of integrating
RL-based planners with RL-based controllers have been effectively demonstrated. This combination enables the framework
to adeptly manage a diverse array of environments and tasks.
Within such a framework, the HL policy is optimised
for strategic planning and achieving specific goals. This optimisation allows for targeted enhancements depending on
the tasks at hand. Moreover, the potential for continuous
improvement and adaptation through further training ensures
that the system can evolve over time, improving its efficiency
and effectiveness in response to changing conditions or new
objectives. Despite the theoretical advantages, the practical
implementation of this type of sim-to-real application for
bipedal robots remains largely unexplored.
Additionally, the training process for each policy within the
hierarchy demands considerable computational resources [37].
The intensive nature of this training can lead to a reliance on
the simulation environment, potentially causing the system to
overfit to specific scenarios and thereby fail to generalise to
real-world conditions. This limitation highlights a significant
hurdle that must be addressed to enhance the viability of
learned hierarchy frameworks in practical applications.
Besides, for the general hierarchical framework, the transition from simulation to real-world scenarios is challenging,
particularly due to the complexities involved in coordinating
two layers within the control hierarchy. Ensuring seamless
communication and cooperation between the HL planner and
LL controller is essential to avoid operational discrepancies.
The primary challenges include: (1) Task division complexity‚Äîwhile the HL planner handles strategy and provides
abstract goals, the LL Controller manages precise execution,

9

necessitating careful coordination to avoid functional overlap
and conflicts. (2) Effective communication‚Äîthe HL‚Äôs abstract
goals must be accurately interpreted and converted by the LL
into real-time actions, especially in dynamic environments.
(3) Task allocation‚Äîclear division of responsibilities between
layers is crucial to prevent redundancy and ensure smooth
system performance.
IV. L IMITATIONS AND C HALLENGES
The end-to-end and hierarchical frameworks detailed in
Sections II and III represent the state of the art in DRL-based
bipedal locomotion, demonstrating remarkable capabilities on
specific tasks. However, a substantial gap remains between
these task-oriented successes and the broader goal of achieving
generalisation and adaptability across diverse morphologies,
tasks, and environments. Bridging this gap requires more
than incremental improvements‚Äîit demands the establishment
of a unified framework that consolidates interfaces, training
conventions, and evaluation protocols to systematically address
the underlying limitations of current DRL pipelines.
As outlined in the following sections, the core challenges
underlying this gap can be grouped into three interrelated
aspects. At a foundational level, a primary difficulty involves
the limitations and challenges in achieving both generalisation
and precision (Section IV-A). This is further complicated by
the practical barrier of the sim-to-real gap in transferring
policies from simulation to physical robots (Section IV-B).
Ultimately, these issues culminate in the critical challenges of
ensuring safety and interpretability for robust deployment in
real-world, safety-critical situations (Section IV-C).
A. Generalisation and precision
A central challenge in applying DRL to bipedal locomotion
is the need to simultaneously achieve high generalisation
across diverse skills and traverse to all kinds of terrains, and
high precision in specific tasks. This remains a fundamental
obstacle to realising truly unified and capable frameworks.
This capability gap is evident in the current literature. Many
approaches excel at generalisation, demonstrating policies that
enable versatile skills such as walking and jumping [40], [31]
and can transfer to different terrains [44], [29], [60]. However,
these generalised policies often lack the fidelity required for
high-precision tasks such as exact foot placement [34], [29],
[54] or maintaining a specific velocity with minimal error
[47]. Conversely, controllers specialised for narrow domains
can achieve exceptional precision, as seen in jumping to a
precise target [43], yet they cannot generalise these capabilities
to a broader range of tasks. Thus, the development of a
single unified framework that concurrently exhibits both broad
competency and high fidelity remains largely unresolved.
This difficulty in uniting generalisation and precision is not
arbitrary but stems from several key limitations inherent in
current DRL paradigms, whether related to framework design,
task formulation, or the training process itself:
‚Ä¢ Limited terrain and gait patterns: The failure to generalise is often a direct result of training on insufficiently diverse environments or with a restricted set of behaviours.

Models trained on limited terrain are brittle when faced
with novel surfaces, while a limited gait pattern library
prevents adaptation to tasks requiring new motor skills.
‚Ä¢ Poor command tracking: The learning signals for generalisation and precision are often in direct conflict. Generalisation requires permissive signals that allow the robot
to adapt to varied terrains or recover from perturbations,
whereas precision demands restrictive signals that minimise command-tracking error. Faced with these opposing
objectives, a single policy is forced to compromise, which
often leads to poor command tracking and the sacrifice
of adaptability in favour of rigid, high-fidelity execution
[30], [31].
‚Ä¢ Inefficient sampling: Underpinning the difficulty of
solving both problems simultaneously is the inefficient
sampling of most DRL algorithms [36], [61], [62]. This
problem is severely exacerbated in tasks that depend on
sparse rewards, where feedback is infrequent and often
only supports the success of the final task. Consequently,
the immense amount of data required for an agent to
explore, discover a successful strategy, and then refine
it for both a diverse skill set for generalisation and
the fine-grained control needed for precision is often
computationally prohibitive, motivating massive parallel
simulation merely to make training tractable [63], [64],
[20], [21].
‚Ä¢ High-quality data scarcity: As highlighted in Section II-A2, the scarcity of high-quality demonstrations is
a key bottleneck. Such data provide essential guidance for
DRL, enabling policies to learn physically feasible and
natural-looking gaits while avoiding unsafe exploration
[21], [20], [65]. This scarcity stems from the difficulty of
transferring scalable human data due to embodiment mismatch [52], [51], [50], while generating feasible synthetic
data via trajectory optimisation is often computationally
expensive [15], [16].
These fundamental limitations give rise to common algorithmic challenges, such as the need for complex reward
engineering, and are directly reflected in the design of the
field‚Äôs dominant control architectures. End-to-end frameworks
attempt a holistic solution, learning a single monolithic policy
that must implicitly resolve all challenges simultaneously.
While this approach can yield highly versatile and dynamic
behaviours [60], it directly confronts the immense difficulty of
exploration from sparse rewards and the struggle of reconciling
conflicting training objectives within unstable system dynamics. This often results in a lack of the fidelity and precision that
hierarchical systems can enforce [31]. Conversely, hierarchical
frameworks are a direct architectural response to the lack of
skill compositionality. By employing a ‚Äúdivide and conquer‚Äù
strategy, they use an HL policy to sequence a library of
LL, often model-based, controllers. This structure enforces
precision and manages complex dynamics at a lower level [16],
[54], [35]. However, this results in a brittle system, imposing
a strong prior that constrains the policy‚Äôs freedom and limits
its ability to generalise to situations not anticipated by the
handcrafted controller [34].

10

B. Challenges in transferring from simulation to reality
Another challenge hindering the deployment of DRL policies on bipedal robots is the sim-to-real gap. This refers to
the significant discrepancy between a policy‚Äôs performance in
a physics simulator and its performance on actual hardware.
This gap is a critical obstacle because training directly on
physical robots is often impractical. The millions of environmental interactions required for DRL would lead to accelerated
mechanical wear, a risk of catastrophic failure, and require
constant human supervision. While simulation offers a safe
and efficient alternative, the ultimate goal of ‚Äúzero-shot‚Äù
transfer, where a policy works perfectly without any real-world
fine-tuning, is rarely achieved.
A large body of research validates impressive locomotion
skills purely within simulation, without attempting transfer
to a physical system [20], [21], [66], [67], [30]. Even when
transfer is successful, it often comes with compromises. Many
successful transfers are not truly ‚Äúzero-shot‚Äù and rely on
a subsequent phase of extensive real-world fine-tuning or
manual parameter tuning [68], [23]. In cases where policies
do transfer without fine-tuning, they often exhibit a noticeable
degradation in performance, where the robustness and agility
seen in simulation are significantly lower in the real world
[44], [29], [69].
This gap is caused by unavoidable differences between the
virtual and physical worlds, which are especially problematic
for dynamically unstable bipedal robots.
Robot dynamics modelling and actuation: Simulators
struggle to replicate the complex dynamics of a physical
bipedal robot, whose inherent instability makes it particularly sensitive to modelling errors. Factors such as motor
friction, gear backlash, and precise link inertia are often
simplified.
‚Ä¢ Contact and terrain modelling: Accurately simulating
intermittent foot‚Äìground contact is extremely difficult. A
mismatch between simulated and real-world friction or
surface properties can cause unexpected slips or bounces,
leading to loss of balance.
‚Ä¢ Sensing and state estimation: A simulated robot has
access to perfect, noise-free state information. In the real
world, these states must be estimated from noisy sensors
such as IMUs and joint encoders [23], [68]. For a bipedal
robot, precise state estimation is critical for maintaining
balance.

‚Ä¢

Simulators such as Isaac Gym [63], RoboCup3D [70],
OpenAI Gym [71], and MuJoCo [72], detailed in Appendix B,
are widely used to train policies that closely mimic real-world
physical conditions. These platforms use full-order dynamics
to better represent the complex interactions robots face, and
numerous sim-to-real frameworks [23], [73], [74] have demonstrated efficient and high-performance results. Despite these
advancements, a significant gap persists between simulation
and reality, exacerbated by the approximations made in simulation and the unpredictability of physical environments.

C. Safety-critical locomotion
Beyond performance metrics such as agility and robustness,
the practical deployment of bipedal robots in human-centric
environments is fundamentally contingent upon safety [1],
[10], [11]. This includes ensuring the robot‚Äôs own integrity
to prevent costly damage, as well as guaranteeing the safety
of the surrounding environment and any humans within it.
While many existing frameworks have demonstrated impressive locomotion skills, they often prioritise performance over
these safety considerations. This creates a critical barrier
that separates success in controlled laboratory settings from
reliable operation in the unpredictable real world.
‚Ä¢ Blind locomotion policies: Many current frameworks
rely solely on internal sensors (proprioception) such as
joint angles and IMU data [42], [44], [28], creating
a major safety risk. Lacking external perception, these
robots cannot anticipate obstacles, slopes, or slippery
surfaces, making them purely reactive and highly prone
to failure. Despite these significant safety drawbacks, this
approach is often adopted for several reasons: omitting
vision simplifies the control problem to pure motor skills
and avoids the computational cost of real-time visual
processing. Moreover, since robust blind locomotion has
already been demonstrated, vision is often treated as a
supplementary component used to enhance task-specific
precision [55] or path planning [75], rather than a core
requirement for basic stability.
‚Ä¢ Lack of physical constraint satisfaction: Many DRL
frameworks lack built-in mechanisms to guarantee physical constraint satisfaction. This gap has motivated constrained or safety-aware DRL that enforces limits via
the learning objective or auxiliary safety modules‚Äîfor
example, Safe-RL on humanoids [76], hybrid DRL with
identified low-dimensional safety models [77], footstepconstrained DRL policies [54], and reactive DRL steppers
operating under feasibility constraints on uneven terrain
[66]. This limitation makes it difficult to prevent the robot
from exceeding joint limits, applying excessive torques,
or causing self-collisions, particularly when reacting to
unexpected events. This is a key area where constrained
RL could be applied.
In summary, the pursuit of performance in DRL has often
sidelined critical safety issues. The prevalence of blind policies that cannot anticipate environmental hazards, combined
with the lack of inherent mechanisms to enforce physical
constraints, creates significant risk and hinders real-world
deployment. While these challenges are considerable, they
also define a clear path forward. The following section on
Future Directions and Opportunities explores specific research
avenues, such as vision-based learning and safe reinforcement
learning, aimed at overcoming these safety barriers and enabling the development of truly robust and reliable bipedal
robots.
V. F UTURE D IRECTIONS AND O PPORTUNITIES
Following the analysis of the surveyed frameworks and their
limitations, this section outlines a path forward for DRL-

11

Future
Pathways

multi-skill learning
traversing challenging
environments

efficient learning and
reward shaping

Current
Limitations

motion retargeting

bridge sim-to-real gap
extensive sim-toreal effort

perception-based learning

blind locomotion
lack constraints
inefficient sampling

Conceptual
Models
Multi-layered
Adaptive Model
Bipedal
Foundation Model

Constrained Learning

limited gait pattern
limited terrain

Unified
Framework

Present
Frameworks
End-to-end
Frameworks
Hierarchy
Frameworks

Challenging
Gaps

Generalization
Precision
Sim-to-Real
Safety

Fig. 4: Towards a Unified Framework: This figure illustrates the logical progression from current DRL frameworks to future
unified systems. It identifies the current limitations of existing end-to-end and hierarchical approaches, which motivate the
exploration of specific Future Pathways. These pathways inform the design of two proposed conceptual models (i) MultiLayered Adaptive Model (MLAM) and (ii) Bipedal Foundation Model (BFM) which represent potential blueprints for achieving
a generalist, unified framework.

based bipedal locomotion by exploring both direct research
avenues and emerging opportunities. We begin in Section V-A
by detailing research directions that directly respond to the
challenges identified in Section IV. Building on this foundation, Section V-B broadens the scope to explore synergistic
opportunities from related fields, such as loco-manipulation
and the application of foundation models. These discussions
culminate in Section V-C, where we propose two conceptual
models for a unified framework that represent the future
evolution of the end-to-end and hierarchical paradigms.

A. Pathways for bipedal locomotion
In relation to the research question introduced in Section I, progress in DRL-based bipedal locomotion should
be assessed not only through conventional metrics such as
reward and success rate but also by broader system-level
measures. These include generalisation breadth (across skills,
terrains, and morphologies), precision in fidelity-critical tasks
(e.g., command-tracking error and foot-placement accuracy),
safety and constraint compliance (joint, torque, and contact
feasibility), and efficiency or deployability (sample efficiency
and on-robot inference latency).
These dimensions build directly upon the challenges outlined in Section IV and together define the key pathways
for advancing bipedal locomotion. The following subsections
elaborate on these pathways, each addressing one or more of
the above aspects to guide progress towards more generalisable
and robust control systems.

1) Multi-skill learning: A fundamental goal for the next
generation of bipedal robots is to move beyond the paradigm
of single-task specialisation and towards versatile skill learning
[60], [78], [31]. This research direction focuses on enabling
robots to acquire, adapt, and deploy a broad and varied repertoire of motor skills, allowing them to handle unforeseen situations and operate effectively in unstructured environments.
To achieve such versatility, researchers are pursuing several HL pathways, which can be broadly categorised into
structured and holistic approaches. The structured approach
focuses on explicit decomposition. A prominent example is
hierarchical learning, where success depends on appropriately
dividing responsibilities; for instance, an HL planner generates
reference trajectories, while an LL DRL controller executes
them robustly [50], [56], as shown in Section III. Similarly,
skill composition employs a supervisor policy to select and
sequence LL experts to solve complex tasks [21]. A related
technique, knowledge distillation, leverages experts by first
training them and then distilling their capabilities into a single,
compact generalist policy [79].
2) Traversing challenging environments: The goal of versatile skill learning is to enable bipedal robots to traverse challenging, human-centric environments where their unique form
offers an advantage. Validating capabilities on such terrains
serves a crucial dual purpose. It tests a policy‚Äôs generalisation
across diverse settings, including stairs and uneven ground,
which is essential for real-world integration [44], [47], [60].
More critically, it benchmarks precision on treacherous paths
such as stepping stones, which demand exact foot placement

12

[29], [14], [66]. These environments are the ultimate test of
both a robot‚Äôs skill repertoire and its control fidelity.
3) Efficient learning and reward shaping: As detailed in
Section IV, while DRL has unlocked impressive capabilities
in bipedal locomotion, its reliance on training from scratch
leads to significant sample inefficiency [36], [28]. Addressing
this bottleneck is a crucial research frontier that calls for both
more efficient algorithms and more robust reward designs.
To mitigate sample inefficiency for complex skills, several
research pathways are being actively explored. A primary strategy is to leverage prior data rather than learning entirely from
scratch. Leveraging prior knowledge provides strong guidance
and reduces unsafe exploration by anchoring policies to feasible motion patterns [21], [41], [50]. Curriculum learning
further organises training from simple to progressively harder
tasks, for example standing and balancing before walking and
running, which improves stability and convergence [53], [46],
[35].
Complementing advances in algorithms is the design of
effective and robust rewards. Manual reward engineering remains a significant obstacle, since small choices can induce
reward hacking and lengthy tuning cycles [64], [21]. Phaseaware objectives are well established for cyclic gaits such
as walking [28], whereas reward design for non-periodic
skills such as jumping is less standardised and often task
specific [43]. Promising directions reduce manual effort by
adding higher-level guidance, including event-based terms,
goal-conditioned objectives, and kinematic reference tracking
[54], [24], [45]. Alternatively, learning rewards from data
through inverse methods and related approaches aims to
replace hand-crafted objectives with implicit ones inferred
from demonstrations [80]. Together, these directions seek to
minimise skill-specific tuning and improve the transferability
and reliability of learned locomotion policies.
4) Motion retargeting: As human-like agents, bipedal
robots‚Äîespecially humanoids‚Äîhave the unique advantage
of a morphology that is similar to our own. This presents
a significant opportunity: the potential to learn from vast
libraries of human motion data. While large-scale datasets
such as AMASS [81] and Motion-X [82] provide a wealth
of such data, they are inherently human-centric and cannot
be used directly, requiring substantial retargeting effort [25].
Therefore, motion retargeting emerges as a critical component
to bridge this gap. The challenge of this pathway is not merely
to transfer human movements to the robot, but to generate
trajectories that are both high in stylistic fidelity and physically feasible, adhering to the robot‚Äôs unique dynamics and
constraints. Successfully developing these retargeting methods
provides a scalable solution for accessing the data needed to
train the natural and versatile generalist policies of the future.
5) Bridging the gap from simulation to reality: Strategies
to bridge the sim-to-real gap generally follow two main
philosophies. The first aims to train policies robust enough
to tolerate the inevitable mismatch between simulation and
reality, while the second focuses on minimising the gap itself
by making the simulator a more faithful replica of the physical
world.

The first approach seeks to reduce the discrepancy by
improving the simulation‚Äôs fidelity. This is often achieved
through system identification (SI), where real-world robot data
are used to fine-tune the simulator‚Äôs parameters to create
a more accurate ‚Äúdigital twin‚Äù [68], [83]. This can include
explicitly learning complex actuator dynamics to model the
motors‚Äô behaviour [68], [84]. Other methods, such as designing specialised feedback controllers [85], also contribute by
making the system less sensitive to residual modelling errors.
In contrast, the second philosophy accepts that simulations
will always be imperfect and instead focuses on creating
highly adaptive, robust policies. The primary method here is
DR, which forces a policy to generalize by training it across
a wide range of simulated physical variations. Other various
ways, such as through end-to-end training that uses measurement histories to adapt online like in RMA [73], or via policy
distillation, where a privileged ‚Äúteacher‚Äù guides a ‚Äústudent‚Äù
policy [47] to have a knowledge of unknown information
like friction . Additionally, techniques like adversarial motion
priors [27], [26] are used to ensure the learned behaviours are
not just robust but also physically plausible.
Looking ahead, the ultimate goal remains achieving reliable
zero-shot transfer, where no real-world fine-tuning is needed.
Progress will depend on the co-development of higher-fidelity
simulations, improved hardware, and more robust control policies inherently capable of handling real-world unpredictability.
The synergy of these advancements will be crucial in finally
closing the sim-to-real gap.
6) Perception-conditioned locomotion: Integrating exteroceptive sensors such as cameras and LiDAR enables bipedal
robots to proactively plan footsteps, avoid obstacles, and adapt
to upcoming terrain. This shift from reactive to anticipatory
control is essential for navigating unstructured real-world
environments.
The vision-based pathway is a human-inspired approach
using RGB and depth cameras to capture rich data on colour,
texture, and object appearance [29], [55], [35]. In contrast,
LiDAR is an active sensing method that generates precise 3D
point clouds of the terrain. While vision provides richer data
but is sensitive to lighting, LiDAR offers robust geometric
measurements without visual detail.
Based on this sensory data, current research is exploring two
primary pathways for processing perceptual information for
control. The first involves creating an intermediate geometric
representation, such as a height map from scanners [55]. This
provides the policy with structured topographical data for
effective footstep planning. The second is a more end-to-end
approach, which utilises direct vision inputs such as RGB or
depth images as inputs to the RL policy for real-time decisionmaking [75], [86]. The former offers interpretability, while
the latter promises more nuanced, reactive behaviours learned
directly from raw perception.
Future progress requires advancing both pathways: building richer, semantic world representations and improving the
efficiency of direct perception-to-action policies. Solving the
underlying challenges of real-time processing and the perceptual sim-to-real gap will be crucial for enabling truly adaptive
locomotion in complex, real-world scenarios.

13

7) Constrained learning: While the previously discussed
pathways focus on enhancing a robot‚Äôs capabilities, a parallel
and equally critical frontier is ensuring that these capabilities
are exercised safely and reliably. To formally integrate safety,
modern approaches can be grouped by how they handle
constraints: soft constraints that guide the policy through costs
and hard constraints that strictly limit actions [76], [77].
Soft constraints encourage desirable behaviour and penalise
undesirable behaviour without forbidding it. They are well
suited to preferences or efficiency goals, for example minimising energy use, limiting peak torques, or promoting smooth
motion [76]. Hard constraints are inviolable rules that prevent
catastrophic failures. They are essential for enforcing physical limits and protecting the robot and its environment, for
example footstep feasibility, contact timing, joint and torque
bounds, and collision avoidance [54], [85]. A practical way
to enforce hard constraints is to use safety filters or shields
grounded in control theory, such as control barrier functions
and related template model checks [87].
In practice, a robust and trustworthy bipedal robot will
likely combine both ideas. Soft constraints help a policy learn
efficient and natural gaits, while hard constraints guarantee
that it will not take catastrophic actions. This combination
supports the transition from systems that are merely capable
in laboratory settings to agents that are reliable, predictable,
and safe for real-world deployment.
B. Opportunities
1) Leveraging foundation models for locomotion learning: The recent rise of Foundation Models (FMs), such
as Large Language Models (LLMs) and Vision‚ÄìLanguage
Models (VLMs), presents a transformative opportunity for
bipedal locomotion. Their powerful reasoning capabilities are
unlocking new approaches that go beyond traditional control
methods, primarily by enabling sophisticated HL task planning
and by providing novel solutions to shape the learning process
itself, particularly in automated reward design.
As HL planners, FMs provide a reasoning engine that can
bridge the gap between abstract human goals and LL motor
execution. They can interpret complex linguistic commands
or visual scenes and decompose them into a sequence of
simpler, actionable commands for an LL policy to follow. This
has been demonstrated effectively in legged robotics, where
VLMs process raw sensory data to pass structured commands
to motor controllers [88], creating a seamless link between
strategic planning and physical action.
Furthermore, FMs create a significant opportunity to overcome one of the most persistent bottlenecks in DRL: reward
design. Instead of tedious manual tuning, LLMs can dynamically generate or refine reward functions based on linguistic
descriptions of task success. Research has shown that LLMs
can translate human feedback into reward adjustments [89]
or even autonomously adjust rewards and control strategies
to self-optimise for diverse terrains [90], drastically reducing
human intervention.
The foremost opportunity lies in the deeper synergy between
these roles. The integration of the HL symbolic reasoning

of FMs with the LL, real-time control of DRL could create
a new class of highly adaptive and flexible robots. As this
rapidly evolving field progresses, as reviewed in [91], we may
see a paradigm shift towards more autonomous, self-learning
humanoid robots that can understand, reason about, and adapt
to the world with minimal human intervention.
2) Loco-manipulation tasks: While achieving stable locomotion is a foundational challenge, a bipedal robot with
only a lower body has limited practical utility, as it cannot
physically interact with its environment. The evolution of
modern humanoids to include complex upper bodies is a
critical advancement that has unlocked the opportunity for
loco-manipulation‚Äîthe dynamic integration of movement and
object interaction. Achieving such full-body coordination is
now a key benchmark for creating truly adaptable systems,
with tasks ranging from climbing and using tools to carrying
objects while navigating, as highlighted by initiatives like the
DARPA Robotics Challenge [38].
However, realising this opportunity is a significant challenge. Early studies, such as a ‚Äòbox transportation‚Äô framework
[92], often rely on inefficient, multi-policy solutions that lack
visual perception. Furthermore, dynamically interacting with
mobile objects like scooters or balls introduces even greater
complexity [93], [94].
These difficulties create significant research opportunities.
One such opportunity lies in exploring hierarchical control
approaches. By decomposing tasks into multiple layers, this
method allows for precise, modular control over different
components, which can enhance stability and adaptability to
environmental variations [56].
Alternatively, a further research opportunity is the development of end-to-end learning frameworks, which offer a more
scalable solution. Using techniques like curriculum learning
and imitation from human motion-capture data [46], [35], [31],
[49], [27], [25], a single, unified policy can be trained to handle diverse loco-manipulation tasks, representing a promising
avenue of research for creating truly versatile agents.
3) Insights from quadruped robots: While DRL remains
an emerging technology in bipedal robotics, it has firmly
established its presence in the realm of quadruped robots, another category of legged systems. The diversity of frameworks
developed for quadrupeds ranges from end-to-end, modelbased RL designed for training in real-world scenarios, where
unpredictable dynamics often prevail [95], [96], to systems
that include the modelling of deformable terrain to enhance
locomotion over compliant surfaces [97]. Furthermore, dynamic quadruped models facilitate highly adaptable policies
[98], [99], and sophisticated acrobatic motions are achieved
through IL [100].
The domain of quadruped DRL has also seen significant
advancements in complex hierarchical frameworks that integrate vision-based systems. To date, two primary versions of
such hierarchical frameworks have been developed: one where
a deep-planning module is paired with model-based control
[101] within a deep-planning hybrid scheme, and another
that combines model-based planning with LL DRL control
[59], [102] within a feedback DRL control hybrid scheme.
The latter has shown substantial efficacy; it employs an MPC

14

to generate reference motions, which are then followed by
an LL feedback DRL policy. Additionally, the Terrain-aware
Motion Generation for Legged Robots module [103] enhances
the MPC and DRL policy by providing terrain height maps
for effective foothold placements across diverse environments,
including those not encountered during training. However,
similar hierarchical hybrid control schemes have not been
thoroughly investigated within the field of bipedal locomotion.
Quadruped DRL frameworks are predominantly designed to
navigate complex terrains, but efforts to extend their capabilities to other tasks are under way. These include mimicking
real animals through motion-capture data and IL [104], [105],
as well as augmenting quadrupeds with manipulation abilities.
This is achieved either by adding a manipulator [106], [107]
or by using the robots‚Äô legs [108]. Notably, the research
presented in [107] demonstrates that loco-manipulation tasks
can be effectively managed using a single, unified, end-to-end
framework.
Despite the progress in quadruped DRL, similar advancements have been limited for bipedal robots, particularly in
loco-manipulation tasks and vision-based DRL frameworks; a
combination of their inherent instability, lack of accessibility to
researchers, and high mechanical complexity can be attributed
to this disparity between quadruped and bipedal robots. Establishing a unified framework could bridge this gap‚Äîan essential
step, given the integral role of bipedal robots with upper bodies
in developing fully functional humanoid systems. Moreover,
the potential of hybrid frameworks that combine model-based
and DRL-based methods in bipedal robots remains largely
untapped.
C. Conceptual models for unified frameworks
Motivated by our survey and the current state of the art, we
propose two conceptual models, intended as reference designs,
towards a unified locomotion framework. They build on endto-end and hierarchical paradigms and offer complementary
routes to scalable, generalisable architectures, rather than fully
realised systems.
‚Ä¢ Bipedal Foundation Models (BFMs): large-scale, pretrained models that map perception directly to action
through representation learning. Trained on diverse data
across tasks and embodiments, BFMs aim to enable generalist locomotion control by supporting rapid adaptation
via fine-tuning.
‚Ä¢ Multi-Layer Adaptive Models (MLAMs): modular,
hierarchical architectures that span from HL planning
to LL control, with each layer producing interpretable
intermediate outputs. MLAMs are designed to integrate,
substitute, and coordinate diverse policies, enabling flexible and adaptive responses across tasks and embodiments.
In the following sections, we will analyse each of these conceptual models in detail, evaluating their respective strengths
and challenges in the pursuit of a unified framework.
1) Bipedal foundation models: Inspired by Robot Foundation Models (RFMs) [91], [109], we propose the concept
of BFMs as large-scale, general-purpose models tailored for
bipedal locomotion. A BFM would be a large-scale model

pre-trained specifically to learn the shared motion priors of
dynamic balance and movement across a vast range of bipedal
tasks and physical embodiments. Unlike traditional policies
trained from scratch, a BFM would provide a foundational
understanding of stable locomotion, directly tackling the core
difficulties that make bipeds distinct from other robots. Architecturally, we envision such a model comprising a multi-modal
embedding module, a shared backbone like a transformer, and
an action decoder, drawing inspiration from models like RT2 [110].
The proposed BFM paradigm would operate in two stages.
First, IL on diverse datasets would establish the generalisable
foundation. Second, DRL would be repurposed as an efficient
fine-tuning mechanism to adapt these general priors to the
specific, and often unforgiving, dynamics of a physical robot.
The potential of this approach is highlighted by recent works,
with frameworks like FLaRe [111] enhancing generalisation
for long-horizon tasks, MOTO [112] enabling effective offlineto-online adaptation from images, and AdA [113] demonstrating in-context adaptation to novel environments. Collectively,
these approaches underscore DRL not only as a simple tuning
tool but as a central mechanism for grounding abstract foundation model priors into executable, platform-specific control
policies.
However, realising the BFM concept for bipeds presents
significant challenges. The DRL fine-tuning stage can be costly
and risky on physical hardware, and policies may overfit to
narrow dynamics or catastrophically forget the generalisable
priors acquired during pre-training [111], [113]. Furthermore,
as detailed in Sections IV-A and V-A4, the scarcity of highquality, large-scale data remains a fundamental bottleneck, as
most existing datasets are human-centric and require significant adaptation before they can be used.
2) Multi-layered adaptive models: As a complementary
path to BFMs, we propose the concept of MLAMs. Rather than
relying on large-scale pre-training, this conceptual framework
would adopt a modular, hierarchical approach. The idea is
to extend conventional hierarchical frameworks (discussed
in Section III) with explicitly adaptive layers, allowing for
the dynamic composition of specialised policies. The core
principle of this concept would be modularity, enabling each
layer to be independently optimised or replaced and providing
interpretable outputs at each stage.
A key feature we envision for MLAMs is their capacity
to dynamically compose adaptive modules for each control
tier. Each layer processes context-specific inputs and outputs
interpretable commands. The HL reasoning layer leverages
large pre-trained models such as LLMs and VLMs [114],
[115] to parse commands into sub-tasks. For instance, Vision‚ÄìLanguage Model Predictive Control [88] has been effective in quadrupedal robots, integrating linguistic and visual
inputs to optimise HL task planning. By leveraging LLMs,
a unified framework could seamlessly bridge HL strategic
planning with detailed task execution.
The mid-level planner selects or synthesises motions via
learned motion libraries [48], [40] or DRL-based planners
[116]. The LL control layer comprises various modular controllers, dynamically selected and composed based on task-

15

specific demands. These include locomotion primitives like
walking and climbing [117], adaptive tracking controllers for
whole-body tracking [50], and imitation-based skills such as
kicking and dancing [21], by utilising RL, IL, or model-based
methods. This layered architecture is exemplified by recent
work on quadrupedal robots, where LLMs are used to translate
HL commands into robust and flexible real-world behaviours
[117].
However, realising the MLAM concept would introduce
challenges distinct from BFMs. Such a framework would depend heavily on real-time multi-modal perception, which complicates data alignment across layers with differing timescales
and abstraction levels [103]. Additionally, the computational
latency incurred by HL reasoning modules like LLMs [117]
would pose limitations for tasks needing rapid reactions.
VI. C ONCLUSION
Despite significant progress in DRL for robotics, a substantial gap remains between current achievements and the
development of a unified framework capable of efficiently
handling a wide range of complex tasks. DRL research is
generally divided into two main control schemes: end-toend and hierarchical frameworks. End-to-end frameworks have
demonstrated success in handling diverse locomotion skills
[31], climbing stairs [44], and navigating challenging terrains
such as stepping stones [29]. Meanwhile, hierarchical frameworks provide enhanced capabilities, particularly in managing
both locomotion and navigation tasks simultaneously.
Each framework contributes unique strengths to the pursuit of a unified framework. End-to-end approaches simplify
control by directly mapping inputs to outputs, while referencebased and reference-free learning methods provide the versatility required for robots to acquire diverse locomotion skills.
In contrast, hierarchical frameworks improve flexibility by
structuring control into layers, allowing modular task decomposition and hybrid strategies.
While DRL has enabled remarkable progress, our survey
concludes that current frameworks face key limitations, including the tension between multi-skill generalisation and taskspecific precision, the persistent sim-to-real gap, and critical
safety concerns. To address these challenges, this survey
synthesises specific pathways for future research and identifies
key opportunities for cross-pollination from related fields,
such as FMs, loco-manipulation, and quadrupedal robotics.
These insights culminate in our proposal of two conceptual
frameworks: the BFMs, extending the end-to-end paradigm,
and the MLAMs, evolving from the hierarchical approach,
which together offer distinct blueprints for the next generation
of generalist bipedal controllers.

low-dimensional representations of complex data. In comparison with other robots of different morphologies, such
as wheeled robots, bipedal robots possess far higher DoFs
and continuously interact with their environments, which results in greater demands on DRL algorithms. In particular,
within legged locomotion, policy-gradient-based algorithms
are prevalent in bipedal locomotion research.
Designing an effective NN architecture is essential for tackling complex bipedal locomotion tasks. Multi-Layer Perceptrons (MLPs), a fundamental NN structure, excel in straightforward regression tasks with lower computational resource
requirements. A comprehensive comparison between MLPs
and the memory-based NN LSTM reveals that MLPs have
an advantage in convergence speed for many tasks [74].
However, LSTMs, as variants of Recurrent Neural Networks
(RNNs), are adept at processing time-associated data, effectively relating different states across time and modelling key
physical properties vital for periodic gaits [28] and successful sim-to-real transfer in bipedal locomotion. Additionally,
Convolutional Neural Networks (CNNs) specialise in spatial
data processing, particularly for image-related tasks, making
them highly suitable for environments where visual perception
is crucial. This diversity of NN architectures highlights the
importance of selecting an appropriate model based on the
specific requirements of bipedal locomotion tasks.
Considering DRL algorithms, recent bipedal locomotion
studies have focused on model-free RL algorithms. Unlike
model-based RL, which learns a model of the environment
but may inherit biases from simulations that do not accurately
reflect real-world conditions, model-free RL directly trains
policies through environmental interaction without relying on
an explicit environmental model. Although model-free RL requires more computational samples and resources, it can train
more robust policies that allow robots to traverse challenging
environments.
Many sophisticated model-free RL algorithms exist, which
can be broadly classified into two categories: policy-based (or
policy optimisation) and value-based approaches. Value-based
methods, e.g. Q-learning, State‚ÄìAction‚ÄìReward‚ÄìState‚ÄìAction
(SARSA), and Deep Q-learning (DQN) [66], excel only in discrete action spaces and often struggle with high-dimensional
action spaces. Q-learning is an off-policy algorithm that
directly learns the optimal Q-values, allowing it to derive
the best possible actions irrespective of the current policy.
SARSA, an on-policy variant, updates its Q-values based on

A PPENDIX A
D EEP REINFORCEMENT LEARNING ALGORITHMS
The advancement and development of RL are crucial for
bipedal locomotion. Specifically, advances in deep learning
provide deep NNs that serve as function approximators, enabling RL to handle tasks characterised by high-dimensional
and continuous spaces by efficiently discovering condensed,

Fig. 5: Diagram for RL algorithms catalogue

16

the actual actions taken, making it robust in environments
where the policy evolves during learning. DQN extends Qlearning by using deep NNs to approximate Q-values, enabling
the algorithm to tackle complex state spaces, though it still
faces challenges with high-dimensional action spaces due to
difficulties in accurate value estimation. In contrast, policybased methods, such as policy-gradient techniques, can handle
complex tasks but are generally less sample-efficient than
value-based methods.
More advanced algorithms combine both policy-based and
value-based methods. The Actor‚ÄìCritic (AC) framework simultaneously learns both a policy (actor) and a value function
(critic), combining the advantages of both approaches [118],
[119]. Popular algorithms such as Trust Region Policy Optimisation (TRPO) [120] and PPO, based on policy-based methods,
borrow ideas from AC. Moreover, other novel algorithms
based on the AC framework include Deep Deterministic Policy
Gradient (DDPG) [121], Twin Delayed Deep Deterministic
Policy Gradients (TD3) [122], A2C (Advantage Actor‚ÄìCritic),
A3C (Asynchronous Advantage Actor‚ÄìCritic) [123], and SAC
(Soft Actor‚ÄìCritic) [124]. Each algorithm has its strengths
for different tasks in bipedal locomotion scenarios. Several
key factors determine their performance, such as sample
efficiency, robustness and generalisation, and implementation
complexity. A comparative analysis [62] illustrates that SACbased algorithms excel in stability and achieve the highest
scores, while their training efficiency significantly trails behind
that of PPO, which attains relatively high scores.
In [61], PPO demonstrates robustness and computational
efficiency in complex scenarios such as bipedal locomotion,
utilising fewer resources than TRPO. In terms of training time,
PPO is much faster than SAC and DDPG [62]. Moreover,
many studies [28], [48], [42] have demonstrated its robustness
and ease of implementation. Combined with its flexibility to
integrate with various NN architectures, this has made PPO the
most popular choice in the field. Numerous studies have shown
that PPO can enable the exploration of walking [28], jumping
[43], stair climbing [44], and stepping-stone traversal [29],
demonstrating its efficiency, robustness, and generalisation.
Additionally, the DDPG algorithm integrates the Actor‚ÄìCritic framework with DQN to facilitate off-policy training, further optimising sample efficiency. In certain scenarios,
such as jumping, DDPG shows higher rewards and better
learning performance than PPO [30], [125]. TD3, developed
from DDPG, improves upon the performance of both DDPG
and SAC [124].
SAC improves exploration through its stochastic policy and
entropy-regularised objective, which encourages the agent to
maintain randomness in its actions, balancing exploration and
exploitation more effectively than DDPG and TD3. Unlike
PPO, which is an on-policy algorithm, SAC‚Äôs off-policy
nature allows it to leverage a replay buffer, reusing past
experiences for training without requiring constant interaction
with the environment. This, combined with entropy maximisation, enables SAC to achieve faster convergence in complex
environments where exploration is essential. SAC is also
known for its stability and strong performance across a wide
range of tasks [124]. While A2C offers improved efficiency

and stability compared with A3C, the asynchronous update
mechanism of A3C provides better exploration capability and
accelerates learning. Although these algorithms demonstrate
clear advantages, they are more challenging to apply owing to
their complexity compared with PPO.
A PPENDIX B
T RAINING S IMULATION E NVIRONMENT
The development of DRL algorithms and sim-to-real techniques highlights the requirement for high-quality simulators.
Creating a reliable simulation environment and conducting RL
training is challenging. The literature shows that several simulators are available, including Isaac Gym [63], RoboCup3D
[70], OpenAI Gym [71], MuJoCo [72], Orbit [126], Brax
[127], and Isaac Lab [128].
OpenAI developed Gym and Gymnasium to provide
lightweight environments for rapid testing of RL algorithms,
including simplified bipedal locomotion models. RoboCup
also serves as a benchmark platform for RL research and
development in multi-agent settings.
For physics-based simulation, MuJoCo, developed by DeepMind, and Gazebo are widely used platforms that support
a range of robotics research tasks. NVIDIA‚Äôs Isaac Gym,
although now deprecated, played an important role as a highperformance GPU-based simulator for training agents in complex environments. Its successors, such as Isaac Lab and Orbit,
continue to evolve as modern RL and robotics frameworks.
One of the most crucial aspects is the parallelisation strategy
and GPU simulation. For instance, Isaac Gym was developed
to maximise the throughput of physics-based machine learning
algorithms, with particular emphasis on simulations requiring
large numbers of environment instances executing in parallel.
Running the physics simulation on a GPU can result in significant speed-ups, especially for large scenes with thousands
of individual actors.
R EFERENCES
[1] Y. Tong, H. Liu, and Z. Zhang, ‚ÄúAdvancements in humanoid robots:
A comprehensive review and future prospects,‚Äù IEEE/CAA Journal of
Automatica Sinica, vol. 11, pp. 301‚Äì328, 2024.
[2] A. Dzedzickis, J. SubacÃåiuÃÑteÃá-ZÃåemaitieneÃá, E. SÃåutinys, U. SamukaiteÃáBubnieneÃá, and V. BucÃåinskas, ‚ÄúAdvanced applications of industrial
robotics: New trends and possibilities,‚Äù Applied Sciences, vol. 12, p.
135, 2021.
[3] M. Yang, E. Yang, R. C. Zante, M. Post, and X. Liu, ‚ÄúCollaborative
mobile industrial manipulator: a review of system architecture and applications,‚Äù in International conference on automation and computing,
2019, pp. 1‚Äì6.
[4] ‚Äú6+
Hours
Live
Autonomous
Robot
Demo,‚Äù
https://www.youtube.com/watch?v=Ke468Mv8ldM, Mar. 2024.
[5] G. Bingjing, H. Jianhai, L. Xiangpan, and Y. Lin, ‚ÄúHuman‚Äìrobot interactive control based on reinforcement learning for gait rehabilitation
training robot,‚Äù International Journal of Advanced Robotic Systems,
vol. 16, p. 1729881419839584, 2019.
[6] R. Bogue, ‚ÄúUnderwater robots: a review of technologies and applications,‚Äù Industrial Robot: An International Journal, vol. 42, pp. 186‚Äì
191, 2015.
[7] N. Rudin, H. Kolvenbach, V. Tsounis, and M. Hutter, ‚ÄúCat-like jumping
and landing of legged robots in low gravity using deep reinforcement
learning,‚Äù IEEE Transactions on Robotics, vol. 38, pp. 317‚Äì328, 2022.
[8] J. Qi, H. Gao, H. Su, L. Han, B. Su, M. Huo, H. Yu, and Z. Deng, ‚ÄúReinforcement learning-based stable jump control method for asteroidexploration quadruped robots,‚Äù Aerospace Science and Technology, vol.
142, p. 108689, 2023.

17

[9] S. Gupta and A. Kumar, ‚ÄúA brief review of dynamics and control of
underactuated biped robots,‚Äù Advanced Robotics, vol. 31, pp. 607‚Äì623,
2017.
[10] J. Reher and A. Ames, ‚ÄúDynamic walking: Toward agile and efficient
bipedal robots,‚Äù Annual Review of Control, Robotics, and Autonomous
Systems, vol. 4, 2021.
[11] J. Carpentier and P.-B. Wieber, ‚ÄúRecent progress in legged robots
locomotion control,‚Äù Current Robotics Reports, vol. 2, pp. 231‚Äì238,
2021.
[12] P. M. Wensing and D. E. Orin, ‚ÄúHigh-speed humanoid running through
control with a 3d-slip model,‚Äù in IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2013, pp. 5134‚Äì5140.
[13] L. Hou, B. Li, W. Liu, Y. Xu, S. Yang, and X. Rong, ‚ÄúDeep reinforcement learning for model predictive controller based on disturbed single
rigid body model of biped robots,‚Äù Machines, vol. 10, p. 975, 2022.
[14] J. Li and Q. Nguyen, ‚ÄúDynamic walking of bipedal robots on uneven
stepping stones via adaptive-frequency mpc,‚Äù IEEE Control Systems
Letters, vol. 7, pp. 1279‚Äì1284, 2023.
[15] A. Herzog, S. Schaal, and L. Righetti, ‚ÄúStructured contact force
optimization for kino-dynamic motion generation,‚Äù in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2016, pp.
2703‚Äì2710.
[16] T. Li, H. Geyer, C. G. Atkeson, and A. Rai, ‚ÄúUsing deep reinforcement
learning to learn high-level policies on the ATRIAS biped,‚Äù in International Conference on Robotics and Automation, 2019, pp. 263‚Äì269.
[17] M. A.-M. Khan, M. R. J. Khan, A. Tooshil, N. Sikder, M. A. P.
Mahmud, A. Z. Kouzani, and A.-A. Nahid, ‚ÄúA systematic review on
reinforcement learning-based robotics within the last decade,‚Äù IEEE
Access, vol. 8, pp. 176 598‚Äì176 623, 2020.
[18] R. Tedrake, T. Zhang, and H. Seung, ‚ÄúStochastic policy gradient reinforcement learning on a simple 3D biped,‚Äù in IEEE/RSJ International
Conference on Intelligent Robots and Systems, 2004, pp. 2849‚Äì2854.
[19] J. Morimoto, G. Cheng, C. Atkeson, and G. Zeglin, ‚ÄúA simple reinforcement learning algorithm for biped walking,‚Äù in IEEE International
Conference on Robotics and Automation, 2004, pp. 3030‚Äì3035 Vol.3.
[20] X. Peng, G. Berseth, K. Yin, and M. van de Panne, ‚ÄúDeepLoco:
dynamic locomotion skills using hierarchical deep reinforcement learning,‚Äù ACM Transactions on Graphics, vol. 36, pp. 1‚Äì13, 2017.
[21] X. Peng, P. Abbeel, S. Levine, and M. van de Panne, ‚ÄúDeepMimic:
Example-guided deep reinforcement learning of physics-based character skills,‚Äù ACM Transactions on Graphics, vol. 37, 2018.
[22] W. Yu, G. Turk, and C. K. Liu, ‚ÄúLearning symmetric and low-energy
locomotion,‚Äù ACM Transactions on Graphics, vol. 37, pp. 1‚Äì12, 2018.
[23] Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, and M. van de Panne,
‚ÄúLearning locomotion skills for cassie: Iterative design and sim-toreal,‚Äù in Conference on Robot Learning, 2020, pp. 317‚Äì329.
[24] M. Taylor, S. Bashkirov, J. F. Rico, I. Toriyama, N. Miyada, H. Yanagisawa, and K. Ishizuka, ‚ÄúLearning bipedal robot locomotion from
human movement,‚Äù in IEEE International Conference on Robotics and
Automation, 2021, pp. 2797‚Äì2803.
[25] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, ‚ÄúExpressive whole-body control for humanoid robots,‚Äù arXiv preprint
arXiv:2402.16796, 2024.
[26] A. Tang, T. Hiraoka, N. Hiraoka, F. Shi, K. Kawaharazuka, K. Kojima,
K. Okada, and M. Inaba, ‚ÄúHumanMimic: Learning natural locomotion
and transitions for humanoid robot via wasserstein adversarial imitation,‚Äù arXiv preprint arXiv:2309.14225, 2023.
[27] Q. Zhang, P. Cui, D. Yan, J. Sun, Y. Duan, A. Zhang, and R. Xu,
‚ÄúWhole-body humanoid robot locomotion with human reference,‚Äù
arXiv preprint arXiv:2402.18294, 2024.
[28] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, ‚ÄúSim-to-real learning of
all common bipedal gaits via periodic reward composition,‚Äù in IEEE
International Conference on Robotics and Automation, 2021, pp. 7309‚Äì
7315.
[29] H. Duan, A. Malik, M. S. Gadde, J. Dao, A. Fern, and J. Hurst, ‚ÄúLearning dynamic bipedal walking across stepping stones,‚Äù in IEEE/RSJ
International Conference on Intelligent Robots and Systems, 2022, pp.
6746‚Äì6752.
[30] C. Tao, M. Li, F. Cao, Z. Gao, and Z. Zhang, ‚ÄúA multiobjective collaborative deep reinforcement learning algorithm for jumping optimization
of bipedal robot,‚Äù Advanced Intelligent Systems, vol. 6, p. 2300352,
2023.
[31] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
‚ÄúReinforcement learning for versatile, dynamic, and robust bipedal
locomotion control,‚Äù arXiv e-prints, pp. arXiv‚Äì2401, 2024.

[32] H. Duan, J. Dao, K. Green, T. Apgar, A. Fern, and J. Hurst, ‚ÄúLearning
task space actions for bipedal locomotion,‚Äù in IEEE International
Conference on Robotics and Automation, 2021, pp. 1276‚Äì1282.
[33] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, ‚ÄúReinforcement
learning-based cascade motion policy design for robust 3d bipedal
locomotion,‚Äù IEEE Access, vol. 10, pp. 20 135‚Äì20 148, 2022.
[34] R. P. Singh, M. Benallegue, M. Morisawa, R. Cisneros, and F. Kanehiro, ‚ÄúLearning bipedal walking on planned footsteps for humanoid
robots,‚Äù in IEEE-RAS International Conference on Humanoid Robots,
2022, pp. 686‚Äì693.
[35] S. Wang, S. Piao, X. Leng, and Z. He, ‚ÄúLearning 3D bipedal walking
with planned footsteps and fourier series periodic gait planning,‚Äù
Sensors, vol. 23, p. 1873, 2023.
[36] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,
‚ÄúDeep reinforcement learning: A brief survey,‚Äù IEEE Signal Processing
Magazine, vol. 34, pp. 26‚Äì38, 2017.
[37] W. Zhu and M. Hayashibe, ‚ÄúA hierarchical deep reinforcement learning
framework with high efficiency and generalization for fast and safe
navigation,‚Äù IEEE Transactions on Industrial Electronics, vol. 70, pp.
4962‚Äì4971, 2023.
[38] C. G. Atkeson, B. P. W. Babu, N. Banerjee, D. Berenson, C. P. Bove,
X. Cui, M. DeDonato, R. Du, S. Feng, P. Franklin et al., ‚ÄúNo falls, no
resets: Reliable humanoid behavior in the darpa robotics challenge,‚Äù in
IEEE-RAS 15th International Conference on Humanoid Robot, 2015,
pp. 623‚Äì630.
[39] X. B. Peng and M. van de Panne, ‚ÄúLearning locomotion skills
using deeprl: Does the choice of action space matter?‚Äù in ACM
SIGGRAPH/Eurographics Symposium on Computer Animation, 2017,
pp. 1‚Äì13.
[40] Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and
K. Sreenath, ‚ÄúReinforcement learning for robust parameterized locomotion control of bipedal robots,‚Äù in IEEE International Conference
on Robotics and Automation, 2021, pp. 2811‚Äì2817.
[41] Z. Xie, G. Berseth, P. Clary, J. Hurst, and M. van de Panne, ‚ÄúFeedback
control for cassie with deep reinforcement learning,‚Äù in IEEE/RSJ
International Conference on Intelligent Robots and Systems, 2018, pp.
1241‚Äì1246.
[42] J. Siekmann, S. Valluri, J. Dao, L. Bermillo, H. Duan, A. Fern, and
J. W. Hurst, ‚ÄúLearning memory-based control for human-scale bipedal
locomotion,‚Äù in Robotics science and systems, 2020.
[43] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
‚ÄúRobust and versatile bipedal jumping control through multi-task
reinforcement learning,‚Äù in Robotics: Science and Systems, 2023.
[44] J. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst, ‚ÄúBlind bipedal
stair traversal via sim-to-real reinforcement learning,‚Äù in Robotics:
Science and Systems, 2021.
[45] C. Yang, K. Yuan, W. Merkt, T. Komura, S. Vijayakumar, and Z. Li,
‚ÄúLearning whole-body motor skills for humanoids,‚Äù in IEEE-RAS
International Conference on Humanoid Robots, 2019, pp. 270‚Äì276.
[46] D. Rodriguez and S. Behnke, ‚ÄúDeepwalk: Omnidirectional bipedal gait
by deep reinforcement learning,‚Äù in IEEE International Conference on
Robotics and Automation, 2021, pp. 3033‚Äì3039.
[47] B. van Marum, M. Sabatelli, and H. Kasaei, ‚ÄúLearning perceptive bipedal locomotion over irregular terrain,‚Äù arXiv preprint
arXiv:2304.07236, 2023.
[48] K. Green, Y. Godse, J. Dao, R. L. Hatton, A. Fern, and J. Hurst,
‚ÄúLearning spring mass locomotion: Guiding policies with a reducedorder model,‚Äù IEEE Robotics and Automation Letters, vol. 6, pp. 3926‚Äì
3932, 2021.
[49] M. Seo, S. Han, K. Sim, S. H. Bang, C. Gonzalez, L. Sentis, and
Y. Zhu, ‚ÄúDeep imitation learning for humanoid loco-manipulation
through human teleoperation,‚Äù in IEEE-RAS International Conference
on Humanoid Robots, 2023, pp. 1‚Äì8.
[50] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn,
‚ÄúHumanplus: Humanoid shadowing and imitation from humans,‚Äù
arXiv preprint arXiv:2406.10454, 2024. [Online]. Available:
https://arxiv.org/abs/2406.10454
[51] L. Penco, B. Clement, V. Modugno, E. Mingo Hoffman, G. Nava,
D. Pucci, N. G. Tsagarakis, J. B. Mouret, and S. Ivaldi, ‚ÄúRobust realtime whole-body motion retargeting from human to humanoid,‚Äù in
2018 IEEE-RAS 18th International Conference on Humanoid Robots
(Humanoids), 2018, pp. 425‚Äì432.
[52] K. Ayusawa and E. Yoshida, ‚ÄúMotion retargeting for humanoid robots
based on simultaneous morphing parameter identification and motion
optimization,‚Äù IEEE Transactions on Robotics, vol. 33, no. 6, pp. 1343‚Äì
1357, 2017.

18

[53] Z. Xie, H. Ling, N. Kim, and M. van de Panne, ‚ÄúALLSTEPS:
Curriculum-driven learning of stepping stone skills,‚Äù Computer Graphics Forum, vol. 39, pp. 213‚Äì224, 2020.
[54] H. Duan, A. Malik, J. Dao, A. Saxena, K. Green, J. Siekmann,
A. Fern, and J. Hurst, ‚ÄúSim-to-real learning of footstep-constrained
bipedal dynamic walking,‚Äù in International Conference on Robotics
and Automation, 2022, pp. 10 428‚Äì10 434.
[55] B. Marum, M. Sabatelli, and H. Kasaei, ‚ÄúLearning vision-based bipedal
locomotion for challenging terrain,‚Äù arXiv preprint arXiv:2309.14594,
2023.
[56] G. A. Castillo, B. Weng, S. Yang, W. Zhang, and A. Hereid, ‚ÄúTemplate
model inspired task space learning for robust bipedal locomotion,‚Äù in
IEEE/RSJ International Conference on Intelligent Robots and Systems,
2023, pp. 8582‚Äì8589.
[57] C. Gaspard, G. Passault, M. Daniel, and O. Ly, ‚ÄúFootstepNet: an
efficient actor-critic method for fast on-line bipedal footstep planning
and forecasting,‚Äù arXiv preprint arXiv:2403.12589, 2024.
[58] J. Li, L. Ye, Y. Cheng, H. Liu, and B. Liang, ‚ÄúAgile and versatile
bipedal robot tracking control through reinforcement learning,‚Äù arXiv
preprint arXiv:2404.08246, 2024.
[59] F. Jenelten, J. He, F. Farshidian, and M. Hutter, ‚ÄúDTC: Deep tracking
control,‚Äù Science Robotics, vol. 9, p. eadh5401, 2024.
[60] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and
K. Sreenath, ‚ÄúReal-world humanoid locomotion with reinforcement
learning,‚Äù Science Robotics, vol. 9, no. 89, p. eadi9579, 2024.
[61] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
‚ÄúProximal policy optimization algorithms,‚Äù arXiv e-prints, pp. arXiv‚Äì
1707, 2017.
[62] O. Aydogmus and M. Yilmaz, ‚ÄúComparative analysis of reinforcement
learning algorithms for bipedal robot locomotion,‚Äù IEEE Access, pp.
7490‚Äì7499, 2023.
[63] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, ‚ÄúLearning to walk
in minutes using massively parallel deep reinforcement learning,‚Äù in
Conference on Robot Learning, 2022, pp. 91‚Äì100.
[64] N. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa,
T. Erez, Z. Wang, A. Eslami, M. Riedmiller, and D. Silver, ‚ÄúEmergence of locomotion behaviours in rich environments,‚Äù arXiv preprint
arXiv:1707.02286, 2017.
[65] C. Yang, K. Yuan, S. Heng, T. Komura, and Z. Li, ‚ÄúLearning natural
locomotion behaviors for humanoid robots using human bias,‚Äù IEEE
Robotics and Automation Letters, vol. 5, pp. 2610‚Äì2617, 2020.
[66] A. Meduri, M. Khadiv, and L. Righetti, ‚ÄúDeepQ stepper: A framework
for reactive dynamic walking on uneven terrain,‚Äù in IEEE International
Conference on Robotics and Automation, 2021, pp. 2099‚Äì2105.
[67] J. Merel, Y. Tassa, D. TB, S. Srinivasan, J. Lemmon, Z. Wang,
G. Wayne, and N. Heess, ‚ÄúLearning human behaviors from motion
capture by adversarial imitation,‚Äù arXiv e-prints, pp. arXiv‚Äì1707, 2017.
[68] W. Yu, V. C. V. Kumar, G. Turk, and C. K. Liu, ‚ÄúSim-to-real
transfer for biped locomotion,‚Äù in IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2019, pp. 3503‚Äì3510.
[69] H. Park, R. Yu, Y. Lee, K. Lee, and J. Lee, ‚ÄúUnderstanding the stability
of deep control policies for biped locomotion,‚Äù The Visual Computer,
vol. 39, pp. 1‚Äì15, 2020.
[70] A. Birk, S. Coradeschi, and S. Tadokoro, RoboCup 2001: Robot Soccer
World Cup V, 2003, vol. 2377.
[71] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, ‚ÄúOpenai gym,‚Äù arXiv preprint
arXiv:1606.01540, 2016.
[72] E. Todorov, T. Erez, and Y. Tassa, ‚ÄúMuJoCo: A physics engine
for model-based control,‚Äù in IEEE/RSJ International conference on
intelligent robots and systems, 2012, pp. 5026‚Äì5033.
[73] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath, and J. Malik,
‚ÄúAdapting rapid motor adaptation for bipedal robots,‚Äù in IEEE/RSJ
International Conference on Intelligent Robots and Systems, 2022, pp.
1161‚Äì1168.
[74] R. P. singh, Z. Xie, P. Gergondet, and F. Kanehiro, ‚ÄúLearning bipedal
walking for humanoids with current feedback,‚Äù IEEE Access, vol. 11,
p. 82013‚Äì82023, 2023.
[75] K. Lobos-Tsunekawa, F. Leiva, and J. Ruiz-del Solar, ‚ÄúVisual navigation for biped humanoid robots using deep reinforcement learning,‚Äù
IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3247‚Äì3254,
2018.
[76] J. Garcƒ±ÃÅa and D. Shafie, ‚ÄúTeaching a humanoid robot to walk faster
through safe reinforcement learning,‚Äù Engineering Applications of
Artificial Intelligence, vol. 88, p. 103360, 2020.

[77] Z. Li, J. Zeng, A. Thirugnanam, and K. Sreenath, ‚ÄúBridging Modelbased Safety and Model-free Reinforcement Learning through System
Identification of Low Dimensional Linear Models,‚Äù in Proceedings of
Robotics: Science and Systems, 2022.
[78] I. Radosavovic, B. Zhang, B. Shi, J. Rajasegaran, S. Kamat,
T. Darrell, K. Sreenath, and J. Malik, ‚ÄúHumanoid locomotion as next
token prediction,‚Äù arXiv preprint arXiv:2402.19469, 2024. [Online].
Available: https://arxiv.org/abs/2402.19469
[79] X. Huang, Y. Chi, R. Wang, Z. Li, X. B. Peng, S. Shao, B. Nikolic,
and K. Sreenath, ‚ÄúDiffuseloco: Real-time legged locomotion control
with diffusion from offline datasets,‚Äù arXiv preprint arXiv:2404.19264,
2024.
[80] J. Ho and S. Ermon, ‚ÄúGenerative adversarial imitation learning,‚Äù in
International Conference on Neural Information Processing Systems,
2016, pp. 4572‚Äì4580.
[81] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. Black,
‚ÄúAmass: Archive of motion capture as surface shapes,‚Äù in 2019
IEEE/CVF International Conference on Computer Vision, 2019, pp.
5441‚Äì5450.
[82] J. Lin, A. Zeng, S. Lu, Y. Cai, R. Zhang, H. Wang, and L. Zhang,
‚ÄúMotion-x: A large-scale 3d expressive whole-body human motion
dataset,‚Äù NeurIPS, 2024.
[83] S. Masuda and K. Takahashi, ‚ÄúSim-to-real transfer of compliant bipedal
locomotion on torque sensor-less gear-driven humanoid,‚Äù in IEEE-RAS
International Conference on Humanoid Robots, 2023, pp. 1‚Äì8.
[84] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis,
V. Koltun, and M. Hutter, ‚ÄúLearning agile and dynamic motor skills
for legged robots,‚Äù Science Robotics, vol. 4, p. eaau5872, 2019.
[85] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, ‚ÄúRobust feedback
motion policy design using reinforcement learning on a 3D digit
bipedal robot,‚Äù in IEEE/RSJ International Conference on Intelligent
Robots and Systems, 2021, pp. 5136‚Äì5143.
[86] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori,
T. Haarnoja, B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic et al.,
‚ÄúNerf2real: Sim2real transfer of vision-guided bipedal motion skills
using neural radiance fields,‚Äù in IEEE International Conference on
Robotics and Automation, 2023, pp. 9362‚Äì9369.
[87] Q. Nguyen, A. Hereid, J. W. Grizzle, A. D. Ames, and K. Sreenath, ‚Äú3d
dynamic walking on stepping stones with control barrier functions,‚Äù in
IEEE Conference on Decision and Control, 2016, pp. 827‚Äì834.
[88] A. S. Chen, A. M. Lessing, A. Tang, G. Chada, L. Smith, S. Levine,
and C. Finn, ‚ÄúCommonsense reasoning for legged robot adaptation with
vision-language models,‚Äù arXiv preprint arXiv:2407.02666, 2024.
[89] K. N. Kumar, I. Essa, and S. Ha, ‚ÄúWords into action: Learning diverse
humanoid robot behaviors using language guided iterative motion
refinement,‚Äù in Workshop on Language and Robot Learning: Language
as Grounding, 2023.
[90] Y. Yao, W. He, C. Gu, J. Du, F. Tan, Z. Zhu, and J. Lu,
‚ÄúAnybipe: An end-to-end framework for training and deploying
bipedal robots guided by large language models,‚Äù 2024. [Online].
Available: https://arxiv.org/abs/2409.08904
[91] R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu,
S. Song, A. Kapoor, K. Hausman et al., ‚ÄúFoundation models in
robotics: Applications, challenges, and the future,‚Äù The International
Journal of Robotics Research, p. 02783649241281508, 2023.
[92] J. Dao, H. Duan, and A. Fern, ‚ÄúSim-to-real learning for humanoid box
loco-manipulation,‚Äù arXiv preprint arXiv:2310.03191, 2023.
[93] J. Baltes, G. Christmann, and S. Saeedvand, ‚ÄúA deep reinforcement
learning algorithm to control a two-wheeled scooter with a humanoid
robot,‚Äù Engineering Applications of Artificial Intelligence, vol. 126, p.
106941, 2023.
[94] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik, M. Wulfmeier, S. Tunyasuvunakool, N. Y. Siegel, R. Hafner
et al., ‚ÄúLearning agile soccer skills for a bipedal robot with deep
reinforcement learning,‚Äù Science Robotics, vol. 9, p. eadi8022, 2024.
[95] L. Smith, I. Kostrikov, and S. Levine, ‚ÄúDemonstrating a walk in the
park: Learning to walk in 20 minutes with model-free reinforcement
learning,‚Äù Robotics: Science and Systems Demo, vol. 2, p. 4, 2023.
[96] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, ‚ÄúDayDreamer: World models for physical robot learning,‚Äù in Conference on
Robot Learning, 2023, pp. 2226‚Äì2240.
[97] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo,
‚ÄúLearning quadrupedal locomotion on deformable terrain,‚Äù Science
Robotics, vol. 8, p. eade2256, 2023.
[98] G. Feng, H. Zhang, Z. Li, X. B. Peng, B. Basireddy, L. Yue, Z. SONG,
L. Yang, Y. Liu, K. Sreenath, and S. Levine, ‚ÄúGenloco: Generalized

19

locomotion controllers for quadrupedal robots,‚Äù in Conference on Robot
Learning, vol. 205, 2023, pp. 1893‚Äì1903.
[99] J. Humphreys and C. Zhou, ‚ÄúLearning to adapt through bio-inspired
gait strategies for versatile quadruped locomotion,‚Äù arXiv preprint
arXiv:2412.09440, 2024.
[100] Y. Fuchioka, Z. Xie, and M. van de Panne, ‚ÄúOPT-Mimic: Imitation
of optimized trajectories for dynamic quadruped behaviors,‚Äù in IEEE
International Conference on Robotics and Automation, 2023, pp. 5092‚Äì
5098.
[101] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis,
‚ÄúRLOC: Terrain-aware legged locomotion using reinforcement learning
and optimal control,‚Äù IEEE Transactions on Robotics, vol. 38, pp.
2908‚Äì2927, 2022.
[102] D. Kang, J. Cheng, M. Zamora, F. Zargarbashi, and S. Coros, ‚ÄúRL
+ Model-Based Control: Using on-demand optimal control to learn
versatile legged locomotion,‚Äù IEEE Robotics and Automation Letters,
vol. 8, pp. 6619‚Äì6626, 2023.
[103] F. Jenelten, R. Grandia, F. Farshidian, and M. Hutter, ‚ÄúTAMOLS:
Terrain-aware motion optimization for legged systems,‚Äù IEEE Transactions on Robotics, vol. 38, pp. 3395‚Äì3413, 2022.
[104] X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, and S. Levine,
‚ÄúLearning agile robotic locomotion skills by imitating animals,‚Äù in
Robotics: Science and Systems, 2020.
[105] F. Yin, A. Tang, L. Xu, Y. Cao, Y. Zheng, Z. Zhang, and X. Chen,
‚ÄúRun like a dog: Learning based whole-body control framework for
quadruped gait style transfer,‚Äù in IEEE/RSJ International Conference
on Intelligent Robots and Systems, 2021, pp. 8508‚Äì8514.
[106] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, ‚ÄúCombining
learning-based locomotion policy with model-based manipulation for
legged mobile manipulators,‚Äù IEEE Robotics and Automation Letters,
vol. 7, pp. 2377‚Äì2384, 2022.
[107] Z. Fu, X. Cheng, and D. Pathak, ‚ÄúDeep whole-body control: Learning
a unified policy for manipulation and locomotion,‚Äù in Conference on
Robot Learning, 2023, pp. 138‚Äì149.
[108] P. Arm, M. Mittal, H. Kolvenbach, and M. Hutter, ‚ÄúPedipulate: Enabling manipulation skills using a quadruped robot‚Äôs leg,‚Äù in IEEE
Conference on Robotics and Automation, 2024.
[109] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim,
Y. Xie, T. Zhang, H.-S. Fang et al., ‚ÄúToward general-purpose robots
via foundation models: A survey and meta-analysis,‚Äù arXiv preprint
arXiv:2312.08782, 2023.
[110] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., ‚ÄúRt-2: Visionlanguage-action models transfer web knowledge to robotic control,‚Äù
arXiv preprint arXiv:2307.15818, 2023.
[111] J. Hu, R. Hendrix, A. Farhadi, A. Kembhavi, R. Martin-Martin,
P. Stone, K.-H. Zeng, and K. Ehsani, ‚ÄúFlare: Achieving masterful and
adaptive robot policies with large-scale reinforcement learning finetuning,‚Äù arXiv, 2024.
[112] R. Rafailov, K. B. Hatch, V. Kolev, J. D. Martin, M. Phielipp, and
C. Finn, ‚ÄúMOTO: Offline pre-training to online fine-tuning for modelbased robot learning,‚Äù in Proceedings of The 7th Conference on Robot
Learning, ser. Proceedings of Machine Learning Research, vol. 229,
2023, pp. 3654‚Äì3671.
[113] J. Bauer, K. Baumli, F. Behbahani, A. Bhoopchand, N. BradleySchmieg, M. Chang, N. Clay, A. Collister, V. Dasagi, L. Gonzalez,
K. Gregor, E. Hughes, S. Kashem, M. Loks-Thompson, H. Openshaw, J. Parker-Holder, S. Pathak, N. Perez-Nieves, N. Rakicevic,
T. RocktaÃàschel, Y. Schroecker, S. Singh, J. Sygnowski, K. Tuyls,
S. York, A. Zacherl, and L. Zhang, ‚ÄúHuman-timescale adaptation in
an open-ended task space,‚Äù in Proceedings of the 40th International
Conference on Machine Learning, 2023.
[114] A. Irpan, A. Herzog, A. T. Toshev, A. Zeng, A. Brohan, B. A. Ichter,
B. David, C. Parada, C. Finn, C. Tan et al., ‚ÄúDo as i can, not as
i say: Grounding language in robotic affordances,‚Äù arXiv preprint
arXiv:2204.01691, 2022.
[115] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, ‚ÄúCode as policies: Language model programs for
embodied control,‚Äù in arXiv preprint arXiv:2209.07753, 2022.
[116] M. Kasaei, M. Abreu, N. Lau, A. Pereira, and L. Reis, ‚ÄúRobust biped
locomotion using deep reinforcement learning on top of an analytical
control approach,‚Äù Robotics and Autonomous Systems, vol. 146, p.
103900, 2021.
[117] Y. Ouyang, J. Li, Y. Li, Z. Li, C. Yu, K. Sreenath, and Y. Wu, ‚ÄúLonghorizon locomotion and manipulation on a quadrupedal robot with large
language models,‚Äù arXiv preprint arXiv:2404.05291, 2024.

[118] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, ‚ÄúContinuous control with deep reinforcement
learning,‚Äù in International Conference on Learning Representations,
2016.
[119] L. Liu, M. van de Panne, and K. Yin, ‚ÄúGuided learning of control
graphs for physics-based characters,‚Äù ACM Transactions on Graphics,
vol. 35, pp. 1‚Äì14, 2016.
[120] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, ‚ÄúTrust
region policy optimization,‚Äù in International Conference on Machine
Learning, 2015, pp. 1889‚Äì1897.
[121] C. Huang, G. Wang, Z. Zhou, R. Zhang, and L. Lin, ‚ÄúRewardadaptive reinforcement learning: Dynamic policy gradient optimization
for bipedal locomotion,‚Äù IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 45, pp. 7686‚Äì7695, 2023.
[122] S. Dankwa and W. Zheng, ‚ÄúTwin-delayed DDPG: A deep reinforcement
learning technique to model a continuous movement of an intelligent
robot agent,‚Äù in International conference on vision, image and signal
processing, 2019, pp. 1‚Äì5.
[123] J. Leng, S. Fan, J. Tang, H. Mou, J. Xue, and Q. Li, ‚ÄúM-A3C: A meanasynchronous advantage actor-critic reinforcement learning method for
real-time gait planning of biped robot,‚Äù IEEE Access, vol. 10, pp.
76 523‚Äì76 536, 2022.
[124] C. Yu and A. Rosendo, ‚ÄúMulti-modal legged locomotion framework
with automated residual reinforcement learning,‚Äù IEEE Robotics and
Automation Letters, vol. 7, pp. 10 312‚Äì10 319, 2022.
[125] C. Tao, J. Xue, Z. Zhang, and Z. Gao, ‚ÄúParallel deep reinforcement
learning method for gait control of biped robot,‚Äù IEEE Transactions on
Circuits and Systems II: Express Briefs, vol. 69, pp. 2802‚Äì2806, 2022.
[126] M. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan,
R. Singh, Y. Guo, H. Mazhar, A. Mandlekar, B. Babich, G. State,
M. Hutter, and A. Garg, ‚ÄúOrbit: A unified simulation framework for
interactive robot learning environments,‚Äù IEEE Robotics and Automation Letters, vol. 8, no. 6, pp. 3740‚Äì3747, 2023.
[127] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch,
and O. Bachem, ‚ÄúBrax ‚Äì a differentiable physics engine for
large scale rigid body simulation,‚Äù 2021. [Online]. Available:
https://arxiv.org/abs/2106.13281
[128] N.
A.
Robotics,
‚ÄúIsaac
lab,‚Äù
https://isaacsim.github.io/IsaacLab/main/index.html, 2023, accessed: 2025-06-06.

