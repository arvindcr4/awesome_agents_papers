Paper: RL_with_Rubric_Anchors.pdf
Category: 02_rlhf_alignment

Category Definition:
Aligning AI with human preferences using RLHF, DPO, or similar techniques.

Abstract Snippet:
Reinforcement Learning with Rubric Anchors Zenan Huang∗ , Yihong Zhuang∗ , Guoshan Lu∗ , Zeyu Qin∗ , Haokai Xu∗ , Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, Junbo Zhao†  arXiv:2508.12790v1 [cs.AI] 18 Aug 2025  Inclusion AI, Ant Group, Zhejiang University  Abstract Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI’s o-series. In RLVR, reward...

Specific Evidence for Classification:
No specific keyword match found in abstract, but file location implies fit.

Conclusion:
This paper is classified as 02_rlhf_alignment because it explicitly discusses concepts such as human feedback, preference, alignment, matching the research direction's core themes.
