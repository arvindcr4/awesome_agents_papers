PAPER: RL_with_Foundation_Priors.pdf
TITLE: Reinforcement Learning with Foundation Priors: Let the Embodied Agent Efficiently Learn on Its Own
RESEARCH_METHOD: 02_rlhf_alignment
METHOD_DESCRIPTION: The paper proposes a novel framework called Reinforcement Learning with Foundation Priors (RLFP) that leverages foundation models to provide prior knowledge for embodied agents. The RLFP framework utilizes policy, value, and success-reward prior knowledge to enable efficient autonomous learning. The authors also introduce the Foundation-guided Actor-Critic (FAC) algorithm, which is an instantiation of the RLFP framework. FAC uses the prior knowledge to guide exploration, shape rewards, and regularize policies.
KEY_CONTRIBUTIONS:
- The paper proposes a novel framework called Reinforcement Learning with Foundation Priors (RLFP) that leverages foundation models to provide prior knowledge for embodied agents.
- The authors introduce the Foundation-guided Actor-Critic (FAC) algorithm, which is an instantiation of the RLFP framework.
- The paper demonstrates the effectiveness of RLFP in efficient autonomous learning through extensive experiments on real robots and simulated environments.
- The authors show that FAC outperforms baseline methods in terms of sample efficiency and success rates, and that it is robust to variations in the quality of foundation priors.
- The paper provides a detailed analysis of the importance of each foundation prior and the impact of their quality on the performance of FAC.
