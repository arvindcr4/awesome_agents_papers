The Design and Realization of Multi-agent Obstacle Avoidance
based on Reinforcement Learning
Enyu Zhaoa,b, Chanjuan Liua*, Houfu Sua, and Yang Liua
a

b

Department of Computer Science and Technology, Dalian University of Technology, Dalian
116024, China

University of Southern California Viterbi Department of Computer Science, 941 Bloom Walk, Los
Angeles, CA 90089

Abstractâ€” Intelligence agents and multi-agent systems play
important roles in scenes like the control system of grouped
drones, and multi-agent navigation and obstacle avoidance
which is the foundational function of advanced application has
great importance. In multi-agent navigation and obstacle
avoidance tasks, the decision-making interactions and dynamic
changes of agents are difficult for traditional route planning
algorithms or reinforcement learning algorithms with the
increased complexity of the environment.
The classical multi-agent reinforcement learning algorithm,
Multi-agent deep deterministic policy gradient(MADDPG),
solved precedent algorithmsâ€™ problems of having unstationary
training process and unable to deal with environment
randomness. However, MADDPG ignored the temporal message
hidden beneath agentsâ€™ interaction with the environment.
Besides, due to its CTDE technique which let each agentâ€™s critic
network to calculate over all agentsâ€™ action and the whole
environment information, it lacks ability to scale to larger
amount of agents.
To deal with MADDPGâ€™s ignorance of the temporal
information of the data, this article proposes a new algorithm
called MADDPG-LSTMactor, which combines MADDPG with
Long short term memory (LSTM). By using agentâ€™s observations
of continuous timesteps as the input of its policy network, it
allows the LSTM layer to process the hidden temporal message.
Experimentâ€™s result demonstrated that this algorithm had better
performance in scenarios where the amount of agents is small.
Besides, to solve MADDPGâ€™s drawback of not being efficient in
scenarios where agents are too many, this article puts forward a
light-weight MADDPG (MADDPG-L) algorithm, which
simplifies the input of critic network. The result of experiments
showed that this algorithm had better performance than
MADDPG when the amount of agents was large.

I. INTRODUCTION
Intelligent agents have played important roles in daily life
such as robots in factories or drones used in warfare. With the
increased complexity of the environment, single-agent
systemâ€™s performance become unsatisfying. Thus multi-agent
system drew peopleâ€™s attention with its multi-agent structure
which enables it to finish complicated tasks in challenging
environment by letting agents to communicate and cooperate
with each other. Multi-agent system was applied in a range of

regions like traffic regulation, computer games and robotic.
The ability to navigation and obstacle avoidance is
undoubtedly the essential feature of agents in multi-agent
system as the wide utilization of mobile multi-agent system in
areas like autonomous driving cars and robots in the
storehouse.
At present, the development of autonomous navigation
technology of agent system can be roughly divided into three
directions according to the knowability of map data: method
based on static path planning, method based on real-time map
creation and positioning, and method based on reinforcement
learning algorithm.
The navigation algorithm based on static path planning
firstly models the mobile robotâ€™s activity scene, establishes the
global map of the scene, and stores the map data of the scene
in the robotâ€™s hardware device. The mainly used algorithms [47] are probabilistic roadmap method [1], rapid-exploration
random tree method [2] and artificial potential field method
[3]. However, the navigation algorithm based on static path
planning can only be applied to the scene where the map is
known.
The technology of simultaneous localization and mapping
(SLAM) was proposed by Smith [8-9] in 1986. Many
researchers [10-11] have made a variety of optimization
attempts on map creation, positioning and path planning.
However, due to the dynamic changes of environment and
agents, the acquisition of sensor data that slam relies on is
faced with uncertainty and complexity.
As the obstacle avoidance ability is in the subset of
autonomous navigation, the researches done in autonomous
navigation also included exploration in obstacle avoidance.
However, the traditional route planning algorithms like A* [12]
exhibited problems such as unable to adapt to dynamic
environments efficiently, surging time complexity with the
increase of agents. With the development made in deep
learning, deep reinforcement learning was introduced in to the
multi-agent system. Multi-agent reinforcement learning
became a solution to the obstacle avoidance and navigation
problem.

Preliminary design of multi-agent reinforcement learning
directly makes each agent run a single reinforcement learning
algorithm. IQL [13] is an example, each agent in this algorithm
executes an individual Q-learning [14] algorithm. Another
example is IDDPG with each agent run an individual DDPG
[15] algorithm. Because agent canâ€™t have access to other
agentsâ€™ action or observation, it canâ€™t tell whether the change
in environment is due to the action of other agents or just the
environmentâ€™s stochasticity. The instability in the
environment , from the agentâ€™s perspective, will cause the
agent fail to converge [15].
Multi-Agent Deep Deterministic Policy Gradient
(MADDPG) [16] brought forward the Centralised Training
and Decentralized Execution (CTDE) training method as
agents can access other agentsâ€™ action and observation during
training phase, even the global observation of the whole
environment. The CTDE training fashion can enable agent to
comprehend the environment even with other agentsâ€™
exploration to ensure the stability of the training process and
leads to a higher performance. However, itâ€™s still not perfect
as centralized training requires each agent to consider othersâ€™
action and observation in the training period, so when the
number of agents increased the joint action space will grow
exponentially. To solve this problem, Value Decomposition
Network (VDN) [17] adds all agentsâ€™ local criticâ€™s output to
gain the global action-value and QMIX [18] passes all agentsâ€™
local action-value to the mixer network to estimate the value
of the joint action of all agents. As Q-learning, the basic
reinforcement learning algorithm of QMIX is only for discrete
action space, and Factorized Multi-agent ActorCritic(FACMAC) [19] was put forward as a subsidy for
continuous action space.
As in the agent navigation and obstacle avoidance task,
each agentâ€™s observation and action follows a continuous time
series pattern, Long Short-Term Memory(LSTM) [20] and
transformer [21] can be utilized in the reinforcement learning
process to increase performance and stability. However, its
application in Multi-agent reinforcement learning area still
await exploration.
In this work, we propose two algorithms, viz., MADDPGLSTMactor and MADDPG-L to solve the obstacle avoidance
and navigation problem for multi-agent systems, especially
focusing on whether the process of time series can improve
algorithmâ€™s performance and how to enable the algorithm to
scale to larger amount of agents. The training and execution of
the algorithms and the underlying algorithms: IDDPG,
MADDPG, FACMAC, were together tested in several virtual
environments. The results suggest that the MADDPGLSTMactor algorithm performs better than the underlying
MADDPG as well as other baseline algorithms. When the
number of agents increases, the MADDPG-L algorithm
demonstrates the best performance. Therefore, the proposed
algorithms can be used as a basic algorithm for multi-agent
obstacle avoidance navigation tasks in the future.
The paper is organized as follows: Section 2 introduces the
preliminaries for later use; In Section 3, we introduce the idea
of the algorithms LSTM-MADDPG and MADDPG-L;
Section 4 discusses the experimental environment and the
comparative analysis of the experimental results; Section 5

summarizes the paper with possible research directions in the
future.
II. BACKGROUND
A. DDPG
In Deterministic Policy Gradient (DPG), the deterministic
policy ğœ‡ will output a certain action ğ‘¢ in given state ğ‘ , ğ‘¢ =
ğœ‡ğœƒ (ğ‘ ) in contrary to the Stochastic policy which gives the
probability of different actions. This simplifies the calculation
of policy gradient by only executing integration on state rather
than both on state and action. The policy gradient is calculated
in the way shown in Equation (1).
âˆ‡ğœƒ ğ½(ğœƒ) = ğ”¼ğ‘ ~ğ’Ÿ [âˆ‡ğœƒ ğœ‡ğœƒ (ğ‘¢|ğ‘ )âˆ‡ğ‘¢ ğ‘„ğœ‡ (ğ‘ , ğ‘¢)|ğ‘¢=ğœ‡ğœƒ (ğ‘ ) ]

(1)

Deep Deterministic Policy Gradient (DDPG) [11] adapts
neural network to serve as the policy network ğœ‡ and
evaluation function ğ‘„ğœ‡ . And IDDPG is the simple extension
of the DDPG in the multi-agent field by executing an
independent DDPG algorithm on each agent.
B. MADDPG
MADDPG is one of the classic algorithms for multi-agent
reinforcement learning. It adopts the training method of
Centralized Training Distribution Execution (CTDE) and
extends the Deep Deterministic Policy Gradient (DDPG) [22]
algorithm to the multi-agent domain. It uses offline learning to
train agents that perform actions within a continuous action
space. In the MADDPG algorithm, each agent will be trained
with the basic structure of actor-critic, learning the
deterministic policy network ğœ‡ğ‘ and the action evaluation
function ğ‘„ğœ‡ğ‘ exclusive to the agent ğ‘, respectively. Since each
agent's policy and action evaluation function belong only to
the agent itself, this makes it applicable to totally competitive,
totally-cooperative or mixed-setting environments.
In this article, agent aâ€™s policy network ğœ‡ğ‘ will be
parameterized by ğœƒğ‘ ,that is ğœ‡ğ‘ (ğœğ‘ ; ğœƒğ‘ ).The joint policy of all
agents will be noted as ğ ï¼Œ ğ = {ğœ‡ğ‘ (ğœğ‘ ; ğœƒğ‘ )}ğ‘›ğ‘=1 ã€‚ Each
agentâ€™s critic network is noted as ğ‘„ğœ‡ğ‘ ï¼Œ with ğœ™ğ‘ as its
parameters. The critic network will evaluate agent aâ€™s action
by the state s of the environment and the joint action u of all
agents, which is ğ‘„ğœ‡ğ‘ (ğ‘ , ğ‘¢1 , ğ‘¢2 â€¦ ğ‘¢ğ‘› ; ğœ™ğ‘ )ã€‚The critic network
of agents ğ‘„ğœ‡ğ‘ will be trained in time differential fashion,
aiming to minimize loss â„’(ğœ™ğ‘ )ï¼Œas in Equation (2)ï¼š
2

ğ¿(Ï•a ) = ğ¸ğ· [(ğ‘¦ ğ‘ âˆ’ ğ‘„ğœ‡ğ‘ (ğ‘ , ğ‘¢1 , ğ‘¢2 â€¦ ğ‘¢ğ‘› ; ğœ™ğ‘ )) ]

(2)

Where ğ‘¦ ğ‘ = ğ‘Ÿğ‘ + ğ›¾ğ‘„ğœ‡ğ‘ (ğ‘  â€² , ğ‘¢1â€² , ğ‘¢2â€² â€¦ ğ‘¢ğ‘›â€² |ğ‘¢ğ‘â€²=ğœ‡ğ‘ (ğœğ‘;ğœƒğ‘âˆ’) ; ğœ™ğ‘âˆ’ )ã€‚
ğ‘Ÿğ‘ is the reward gained by agent ğ‘ â€™s interaction with the
environmentï¼Œğœ™ğ‘âˆ’ is the parameter of target critic networkï¼Œ
(ğ‘¢1â€² , ğ‘¢2â€² , â€¦ ğ‘¢ğ‘›â€² ) represent the target action of all agents chosen
by their target policy network with parameter ğœƒğ‘âˆ’ ã€‚ The
experience tuple stored in replay buffer ğ’Ÿ is
(ğ‘ , ğ‘  â€² , ğ‘¢1 , ğ‘¢2 , â€¦ ğ‘¢ğ‘› , ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ ğ‘Ÿğ‘› )ã€‚
The policy gradient J(Î¼a ) of agent ğ‘ is calculated in
Equation (3).
âˆ‡ğœƒğ‘ ğ½(ğœ‡ğ‘ ) = ğ”¼ğ’Ÿ [âˆ‡ğœƒğ‘ ğœ‡ğ‘ (ğœğ‘ )âˆ‡ğ‘¢ğ‘ ğ‘„ğœ‡ğ‘ (ğ‘ , ğ‘¢1 , ğ‘¢2 , â€¦ ğ‘¢ğ‘› )|ğ‘¢ğ‘ =ğœ‡ğ‘(ğœğ‘) ]

(3)

It needs to be mentioned that except the action of agent a
trained currently, which is computed by agent aâ€™s policy
network, other agentsâ€™ action is just the corresponding action
stored in replay buffer.
C. VDN and QMIX
Both Value Decomposition Network (VDN) and
Monotonic Q-Value Decomposition Network (QMIX) are
multi-agent reinforcement learning algorithms that are trained
using centralized training distribution execution. Both
algorithm tasks are to train a centralized critic. Unlike the
MADDPG algorithm, the environment action evaluation
function they train will have only one output that evaluates the
ğœ‹
global action-state value, which is ğ‘„ğ‘¡ğ‘œğ‘¡
. This environmental
action evaluation function, which evaluates the global actionstate value, will be calculated in a decomposed fashion. The
practice of the value decomposition network (VDN) is to add
the environmental action evaluation functions of each agent,
as shown in Equation (4):
ğœ‹

ğ… (ğ‰,
ğ‘„ğ‘¡ğ‘œğ‘¡
ğ’–; ğœ™) = âˆ‘ğ‘›ğ‘=1 ğ‘„ğ‘ ğ‘ (ğœğ‘ , ğ‘¢ğ‘ ; ğœ™ğ‘ )

(4)

It should be noted that the environmental action evaluation
ğœ‹
function ğ‘„ğ‘ ğ‘ of each agent is only evaluated according to the
local observation record and the action of the agent ğ‘, unlike
the counterparts in MADDPG which consider the overall
environment state and the actions of all other agents.
The monotonic Q-value decomposition network (QMIX)
uses a monotonic continuous mixing function to calculate the
ğœ‹
global action-state evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡
, as shown in
Equation (5):
ğœ‹

ğœ‹

ğœ‹

ğ… (ğ‰,
ğ‘„ğ‘¡ğ‘œğ‘¡
ğ’–, ğ’”; ğœ™, ğœ“) = ğ‘“ğœ“ (ğ‘ , ğ‘„1 1 (ğœ1 , ğ‘¢1 ; ğœ™1 ), ğ‘„2 2 (ğœ2 , ğ‘¢2 ; ğœ™2 ) â€¦ ğ‘„ğ‘› ğ‘› (ğœğ‘› , ğ‘¢ğ‘› ; ğœ™ğ‘› )) (5)

The requirement for the monotonicity of the mixing
function is to ensure that when the global action-state
ğœ‹
evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡
is the largest, the actions selected by
ğœ‹
each agent are the same as those selected when the agent ğ‘„ğ‘ ğ‘
is maximized to facilitate distribution. In practical tasks, the
mixing function in QMIX is a neural network, which
guarantees monotonicity by using non-negative weights. The
monotonic QMIX algorithm still uses the time differential
algorithm for training with the target network, and find the
optimal solution by reducing the loss function ğ¿(ğœ™, ğœ“), as
shown in Equation (6):
2

ğœ‹ (ğ‰,
ğ¿(ğ“, ğœ“) = ğ¸ğ· [(ğ‘¦ ğ‘¡ğ‘œğ‘¡ âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡
ğ’–, ğ‘ ; ğ“, ğœ“)) ]

(6)

As MADDPG trains the centralized action evaluation
ğœ‡
network ğ‘„ğ‘¡ğ‘œğ‘¡ in CTDE method, that is, to learn a centralized
critic for each agent a to evaluate the overall environment and
ğœ‡
all agent actions. The action evaluation network ğ‘„ğ‘ ğ‘ becomes
more difficult to optimize as the number of agents increase
and action space surge. Because with the increase of the
number of agents and the action space of each agent in the
multi-agent system, the dimension of the input vector
(ğ‘ , ğ‘¢1 , ğ‘¢2 . . . ğ‘¢ğ‘› ) of the environmental action evaluation
ğœ‡
network ğ‘„ğ‘ ğ‘ of each agent will increase exponentially.
Therefore, the Factorized Multi-Agent Centralized Policy
Gradient Algorithm (FACMAC) proposes the use of value
factorisation to construct and train a centralized
ğœ‡
environmental action evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡ that is easier to
cope with the increase in the number of agents and the
increase in the action space.
Since FACMAC adopts the Actor-Critic structure as the
basic algorithm framework, each agent ğ‘ relys on the unique
action ğ‘¢ğ‘ calculated by its policy network ğœ‡ğ‘ according to the
local action observation history ğœğ‘ when making action
selection. Unlike VDN and QMIX, agent only depends on the
ğœ‡
action value given by the action evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡ to
choose action. This means when constructing the factored and
ğœ‡
centralized critic ğ‘„ğ‘¡ğ‘œğ‘¡ , there is no need to impose
monotonicity restrictions on the mixing function. This
enables the FACMAC to more freely decompose the
ğœ‡
centralized environmental action evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡
without considering its possible performance loss, so as to
better deal with more complex surroundings.
The centralized action-value evaluation function of
FACMAC is shown in Equation (7):
ğœ‡

ğ

ğœ‡

ğœ‡

ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‰, ğ’–, ğ‘ ; ğ“, ğœ“) = ğ‘”ğœ“ (ğ‘ , ğ‘„1 1 (ğœ1 , ğ‘¢1 ; ğœ™1 ), ğ‘„2 2 (ğœ2 , ğ‘¢2 ; ğœ™2 ) â€¦ ğ‘„ğ‘› ğ‘› (ğœğ‘› , ğ‘¢ğ‘› ; ğœ™ğ‘› )) (7)

ğ“ and ğœ™ğ‘ represent the parameters of the centralized
ğœ‡
action evaluation function ğ‘„ğ‘¡ğ‘œğ‘¡ and the local environmental
ğœ‡ğ‘
action evaluation function ğ‘„ğ‘ of each agent a, respectively.
ğ‘”ğœ“ is a nonlinear mixing function, which is not restricted by
monotonicity, and the parameter is ğœ“. The learning method is
the same as the QMIX, the target network is used to learn the
centralized action evaluation function. The loss function is
expressed in Equation (8):
ğœ‡

2

ğ¿(ğ“, ğœ“) = ğ¸ğ· [(ğ‘¦ ğ‘¡ğ‘œğ‘¡ âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‰, ğ’–, ğ‘ ; ğ“, ğœ“)) ]

(8)

Where ğ‘¦ ğ‘¡ğ‘œğ‘¡ is:

Where ğ‘¦ ğ‘¡ğ‘œğ‘¡ is:

ğ

ğ… (ğ‰â€²
ğ‘¦ ğ‘¡ğ‘œğ‘¡ = ğ‘Ÿ + ğ›¾ max
ğ‘„ğ‘¡ğ‘œğ‘¡
, ğ’–â€² , ğ‘  â€² ; ğ“ âˆ’ , ğœ“ âˆ’ )
â€²

ğ‘¦ ğ‘¡ğ‘œğ‘¡ = ğ‘Ÿ + ğ›¾ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‰â€² , ğ(ğ‰â€² ; ğœ½âˆ’ ), ğ‘  â€² ; ğ“âˆ’ , ğœ“ âˆ’ )

ğ“âˆ’ is the set of network parameters for the target critic
network of each agent, and ğœ“ âˆ’ is the target mixing network
parameter.

In the calculation of the policy gradient, FACMAC
proposes a calculation method that computes the centralised
policy gradient as in Equation (9).

ğ‘¢

D. FACMAC
Since VDN and QMIX adopt the basic idea of Q-learning,
these two algorithms are only suitable for discrete action
spaces. The factored multi-agent centralized policy gradient
algorithm (FACtored Multi-Agent centralized policy gradient,
FACMAC) can be used to deal with multi-agent
reinforcement learning tasks in continuous action spaces.

ğ

âˆ‡ğœƒ ğ½(ğ) = ğ”¼ğ’Ÿ [âˆ‡ğœƒ ğâˆ‡ğ ğ‘¸ğ‘¡ğ‘œğ‘¡ (ğœ, ğœ‡1 (ğœ1 ), ğœ‡2 (ğœ2 ) â€¦ ğœ‡ğ‘› (ğœğ‘› ), ğ‘ )] (9)

Different from the gradient calculation method of
MADDPG shown in Equation (2), in FACMAC, all agentsâ€™
actions ğ‘¢ğ‘ are calculated by the policy network ğœ‡ğ‘ of each
agent ğ‘ according to its local action observation history ğœğ‘ .
While in MADDPG, only the action of the currently trained
agent is calculated in the above way, other agentsâ€™ action are
sampled from the replay buffer.

III. THE DESIGN OF ALGORITHMS
A. MADDPG-LSTMactor
In multi-agent reinforcement learning tasks, agents
interact with the environment using their own policies. Then
they gain rewards resulting from their actionsâ€™ influence on
the environment. In this learning process, each agentâ€™s actions
and observations as well as the states of the environment all
demonstrate the features like time series. Due to LSTMâ€™s
capability of dealing with time series data, we introduce this
structure into MADDPG (which has been proved to be a
successful algorithm in multi-agent reinforcement learning
field) to see whether the performance in multi-agent obstacle
avoidance and navigation tasks can be further enhanced.
As MADDPG algorithm adopts the CTDE (centralized
training and Decentralized execution) training method, each
agentâ€™s critic network estimates the action ğ‘¢ğ‘ made by actor
network based on the environmentâ€™s state ğ‘  to guide the actor
in training phase, whereas the actor network generates action
both in training and execution phase using its observation
history as input. So ï¼Œit is obvious that comparing to the
centralized critic network, the actor network deals with time
series data more frequently and directly. Thus, we tried to
introduce LSTM in the actor network directly first.
Normally, agents in MADDPG algorithm interact with the
environment without training until thereâ€™s enough data in
replay buffer ğ’Ÿ. Then, at each time step, a batch of experience
tuples containing environment state , action ğ’– â‰¡ {ğ‘¢ğ‘ }ğ‘›ğ‘=1 ,
reward ğ’“ â‰¡ {ğ‘Ÿğ‘ }ğ’ğ’‚=ğŸ will be sampled from the replay buffer ğ’Ÿ
to train the critic network and the actor network. After the
training phase, the updated actor network will continue to
interact with the environment to generate new data. The
environment will be reset periodically when the timestep
becomes an integral multiple of preset hyper-parameter
ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’ âˆ’ ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„. However, such training process is unable
to exploit the information hidden in the time series.
In this article, agentsâ€™ policy network equipped with
LSTM takes action ğ‘¢ğ‘ on timestep ğ‘¡ based on a sequence of
its local observation ğ‰ğ’‚ â‰¡ {ğœğ‘ğ‘¡âˆ’ğ‘– }ğ‘™ğ‘–=0 rather than simply the
observation ğœğ‘ğ‘¡ of timestep ğ‘¡. We set the sequence length as a
hyper-parameter ğ‘ ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘’ âˆ’ ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ = ğ‘™ . The action-taking
process of agent ğ‘ can be defined as Equation (10).
ğ‘¢ğ‘ = ğœ‡ğ‘ ({ğœğ‘ğ‘¡âˆ’ğ‘– }ğ‘™ğ‘–=0 , â„ğ‘¡âˆ’1 , ğ‘ ğ‘¡âˆ’1 ; ğœƒğ‘ )

(10)

Figure 1. The illustration of MADDPG-LSTMactor

We realized this mechanism by creating and maintaining
a queue with the length of ğ‘ ğ‘’ğ‘ âˆ’ ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ during both training

and execution phase. The queue stores experience tuples and
is named as sequence buffer â„¬. At each timestep, the actor
network takes action ğ‘¢ğ‘ based on the observation sequence
stored in sequence buffer â„¬ and gains reward ğ‘Ÿğ‘ . The
experience tuple including environment state ğ‘ , all agentsâ€™
observation ğ‰, all agentsâ€™ action ğ’– and reward ğ’“ will be stored
into the sequence buffer. If the sequence buffer is full, the first
experience tuple will be deleted and the newest one will be
stored at the end of the queue. Meantime, the whole sequence
buffer will be stored into the replay buffer ğ’Ÿ as an element.
The sequence buffer will be emptied when the environment is
reset. During training, instead of sampling experience tuples,
the previously stored sequence buffer, which is a continuous
sequence of experience tuples, will be sampled. The
illustration figure is shown below.
In MADDPG-LSTMactor algorithm, the structure of
critic network and how it works are same to the one in
MADDPG. Thus, the way to optimize critic network and actor
network remains unchanged.
B. MADDPG-L
As a typical MARL algorithm adopts CTDE training
method, MADDPG requires agentsâ€™ critic network to
consider every agentâ€™s observation and action in training
phase to stabilize training as well as improve the algorithmâ€™s
performance. However, such demand for extra information
can be a problem when the amount of agents in the multiagent system is too large, as it will lead to the explosion of
action space. FACMAC can be a great solution to this
question. By learning a single centralized but factor critic,
which factors the joint action-value function ğ‘„ğ‘¡ğ‘œğ‘¡ into peragent utilities ğ‘„ğ‘ that are combined via a non-linear function.
Each factored action-value utility, i.e., each agentâ€™s local
critic network estimates the value of its action, which can be
written as ğ‘„ğ‘ (ğœğ‘ , ğ‘¢ğ‘ ; ğœ™ğ‘ ) . ğœğ‘ is the local observation of agent
ğ‘ , ğ‘¢ğ‘ is its action and ğœ™ğ‘ stands for the parameters of the
critic network. Then the mixing function ğ‘”ğœ“ will take all
agentsâ€™ critic networkâ€™s output and the environmentâ€™s state ğ‘ 
to give the final joint action-value estimation for all agents. In
this way, agentâ€™s local critic network only needs to calculate
its own action and observation, regardless of the number of
agents in the system. The mixing function helps to ensure that
during training phase, all agents can get information about
other agents and the environment.
However, algorithms factoring the centralized critic are
not perfect. In the baseline test of MPE (Multi-agent Particle
Environment) [23], the QMIX algorithm demonstrated the
problem that itâ€™s hard to get away from the initial state. The
result is shown in Fig. 2.
The horizontal axis is the timestep of training, while the
vertical axis is the average return of the algorithm. It can be
witnessed that the performance, which is indicated by its
average return, doesnâ€™t make any practical improvement until
the training timestep is more than 3 million. In this articleâ€™s
experiment, the FACMAC algorithm showed the same
problem because the structure design and the idea of
FACMAC are similar to QMIX. The result can be found in
chapter 4.
We suggest that this problem could happen because there
are too many networks to optimize in the training procedure.

Figure 2. The performance of QMIX in MPE

As the centralized and final critic of FACMAC is created by
the mixing function which mix the combination of all local
critics, we need to optimize all of them while training each
agent. In comparison to MADDPG, it only requires
optimizing the agentâ€™s own critic when itâ€™s the agentâ€™s turn to
be trained. So, itâ€™s harder for the FACMAC algorithm to make
solid progress and this could lead to the cold start problem
which worsen the situation. Because the algorithm is still in
the primitive stage, most of its attempts canâ€™t bring any
positive rewards, so its replay buffer is flooded with â€˜not
successfulâ€™ experience tuples for the training phase to sample.
The reason why we made such suggestion comes from the
way to calculate centralized critic in the experiment. When we
are training an agent, we need to calculate the centralized
critic, as the actor network need it to carry out gradient ascend.
As the centralized critic requires other agentsâ€™ critic result
besides the agent ğ‘ which is currently trained, we can directly
use other agentsâ€™ critic to estimate the action-value of their
own actions or use the critic network of agent ğ‘ to simulate
other agentsâ€™ critic network to produce the action-value. As in
our experiment, all agentsâ€™ critic networks share the same
structure, which makes the simulation method possible. The
advantage of using only the critic network of agent ğ‘ is it can
reduce the amount of networks to improve, which presumably
makes training easier. In our experiment, such method did
improve the performance. However, the progress is still far
from satisfaction.
In this article, we bring forward a light-weight MADDPG
algorithm, which can better scale to larger amount of agents,
we named it as MADDPG-L. The actor network remains the
same as MADDPG, we set the policy of agent ğ‘ to be
ğœ‡ğ‘ (ğœğ‘ ; ğœƒğ‘ ), and the action of agent ğ‘ to be ğ‘¢ğ‘ = ğœ‡ğ‘ (ğœğ‘ , ğœƒğ‘ ).
ğœƒğ‘ is the parameter of ğœ‡ğ‘ . What distinguish it from MADDPG
is the critic network of each agent. The notation of it is
ğœ‡
ğ‘„ğ‘ (ğ‘ , ğ‘¢ğ‘ ; ğœ™ğ‘ ), implying that the critic of each agent only
considers that action of its own instead of all agentsâ€™ action,
ğœ‡
ğœ™ğ‘ is the parameter of ğ‘„ğ‘ . Meanwhile, the environment state
ğ‘  helps the agent to get the global information it needs. This
change helps to solve the problem of action space explosion
as each agent only needs to calculate its own action regardless
of the amount of agents in the multi-agent system. The
illustration is shown in Fig. 3. The critic network is trained in
temporal difference style, the loss function of it is shown in
Equation (11).
2

â„’(ğœ™ğ‘ ) = ğ”¼ğ’Ÿ [(ğ‘¦ ğ‘ âˆ’ ğ‘„ğœ‡ğ‘ (ğ‘ , ğ‘¢ğ‘ ; ğœ™ğ‘ )) ]

(11)

Figure 3. Illustration of MADDPG-L

where ğ‘¦ ğ‘ isï¼š
ğ‘¦ ğ‘ = ğ‘Ÿğ‘ + ğ›¾ğ‘„ğœ‡ğ‘ (ğ‘  â€² , ğ‘¢ğ‘â€² |ğ‘¢ğ‘â€²=ğœ‡ğ‘ (ğœğ‘;ğœƒğ‘âˆ’) ; ğœ™ğ‘âˆ’ )
ğ‘Ÿğ‘ is the reward of agent ğ‘ â€™s interaction with the
environment, ğœ™ğ‘âˆ’ indicates the parameter of target networkï¼Œ
ğ‘¢ğ‘â€² is the action implemented by the target actor network with
its parameter notified as ğœƒğ‘âˆ’ . The experience tuples stored in
the
replay
buffer ğ’Ÿ has
the
structure
as
(ğ‘ , ğ‘  â€² , ğ‘¢1 , ğ‘¢2 , â€¦ ğ‘¢ğ‘› , ğ‘Ÿ1 , ğ‘Ÿ2 , â€¦ ğ‘Ÿğ‘› ).
The policy gradient of agent ğ‘â€™s actor network ğ½(ğœ‡ğ‘ ) is
shown in Equation (12)ï¼š
âˆ‡ğœƒğ‘ ğ½(ğœ‡ğ‘ ) = ğ”¼ğ’Ÿ [âˆ‡ğœƒğ‘ ğœ‡ğ‘ (ğœğ‘ )âˆ‡ğ‘¢ğ‘ ğ‘„ğœ‡ğ‘ (ğ‘ , ğ‘¢ğ‘ )|ğ‘¢ğ‘=ğœ‡ğ‘ (ğœğ‘ ) ] (12)
IV. EXPERIMENT
A. Simulation environment
The simulation environment is based on MPE
environment, and the environmental parameters are listed in
the Table 1.
B. The experiment scenarios and results
The experiment scenarios are Obstacle predator-prey,
Spread, Tunnel and Simple Tunnel. The Obstacle
Predator-Prey acts as the initial test environment for all
algorithms to find out their performance in obstacleavoidance and dynamic target pursuing task. The Spread
environment aims to test all algorithmsâ€™ performance in
avoiding obstacles that are relatively small and arriving at
designated destinations. Whatâ€™s more, the Spread
environment will have multiple versions including different
number of agents to discover the algorithmâ€™s ability to scale.
The Tunnel environment is meant for testing algorithmâ€™s
ability in navigating through environments where obstacles
are so huge that they form the contour. In other words, there
will be some specific routes for the agents to take and
algorithms should learn to find them. The Simple Tunnel
environment is merely the Tunnel environment with some
modifications to make it more realistic. Apart from the Simple
Tunnel environment, the algorithms to be tested are IDDPG,
MADDPG,
MADDPG-LSTMactor,
MADDPG-L,
FACMAC. In the Simple Tunnel environment, agentsâ€™
rewards are not equal and thus FACMAC is not tested as it
requires such a condition.

TABLE I.

EXPERIMENTAL ENVIRONMENT PARAMETERS

Term

Version

CPU

Intel i9 9900k

Memory Size

64GB

GPU

Colorful NVIDIA
GTX1080(VRAM 8GB)

Operation System

Ubuntu 18.04 LTS

GPU Driver version

495.46

Python version

3.8.13

Pytorch version

1.11.0

CUDA version

11.5

TABLE II. HYPERPARAMETERS OF THE EXPERIMENT
Figure 4. Illustration of Obstacle Predator Prey environment
Hyperparamters

Value

Meaning

max-episode-len

100

The length of each episode

time-steps

2000000
1
ï¼ˆdefaul
tï¼‰
0.001
ï¼ˆdefaul
tï¼‰
0.01
ï¼ˆdefaul
tï¼‰

Total interaction steps

Num-adversaries

Lr-actor

Lr-critic
Epsilon

0.1

Noise-rate

0.1

Gamma

0.95
256
ï¼ˆdefaul
tï¼‰
5
ï¼ˆdefaul
tï¼‰

Batch-size

seq-length

Other agents besides those get
trained
The learning rate of policy
network
The learning rate of critic
network
Parameter for ğœ–-ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ action
choosing policy
The rate of noise added to
action during training phase
Time elapse coefficient
Batch-size for experience tuples
sampled for training

Figure 5. Algorithmsâ€™ performance in Obstacle Predator Prey environment

The length of continuous steps
for LSTM training

environment state, demonstrated better performance than
MADDPG in the final stage.

The hyper-parameters used in the Experiment are listed in
table 2, some of them remain the same in all environments
and algorithms, others which vary from experiment to
experiment will have their default value listed here and exact
value shown in appendix.
Obstacle Predator Prey: In the Obstacle Predator-Prey
environment, a team of agents will be trained to pursuit an
adversarial agent with higher speed. The agents trained need
to learn how to chase the target(prey) while avoiding crashing
into obstacles and each other. The illustration of the
environment is Fig. 4.
The result of the experiment is shown in Fig. 5. The result
shows that algorithms using the CTDE training paradigm
generally have better performance than IDDPG, which adopts
the decentralized training method. FACMAC is the exception,
the explanation can be found in chapter 3.2. In this
environment, with the help of LSTM, MADDPG-LSTMactor
has better performance than MADDPG. MADDPG-L, which
only feeds agentsâ€™ critics with the agentâ€™s own action and the

To find out the reason why FACMAC performed far
worse than other algorithms, we did more experiments in this
environment.
The first additional experiment is to test two different
ways of getting the final centralized critic. As the calculation
of it requires other agentsâ€™ critic result besides the agent ğ‘
which is currently trained, we can directly use other agentsâ€™
critic to estimate the action-value of their own actions, or use
the critic network of agent a to simulate other agentsâ€™ critic
network to produce the action-value. As in our experiment, all
agentsâ€™ critic networks share the same structure, the
simulation method is possible. The result of these two
different calculation methods of the centralized critic in
FACMAC is shown in Fig. 6.
It can be witnessed that using agentâ€™s critic network to
simulate other agentsâ€™ critic networks can reach a better result
than directly using other agentsâ€™ critic network, though the
performance is still not comparable to other algorithms. The
mixing network could be the reason. As in the FACMACâ€™s

positive data for training. The result of this experiment is
shown in Fig. 7.

Figure 6. Performance of FACMAC with different parameter sharing
policy

Figure 8. The illustration of Spread environment

We set 1 million timestep to be the end of the first stage.
The algorithmâ€™s performance simply plunged to the level of
random policy after reaching the watershed. We altered each
ğœ‡
agentâ€™s local critic network from ğ‘„ğ‘ (ğœğ‘ , ğ‘¢ğ‘ ; ğœƒğ‘ ) to
ğœ‡
ğ‘„ğ‘ (ğ‘ , ğ‘¢ğ‘ ; ğœƒğ‘ ) and get rid of the mixing network. By doing this,
we created MADDPG-L. The input of the algorithmâ€™s critic
network doesnâ€™t have the problem of the explosion of action
space as it only requires the agentâ€™s own action, and still
provides global information.
Spread: In the Spread environment, target locations will
be randomly generated after each environment reset, a team
with the same amount of the target locations are trained to
occupy all target locations. Obstacles with random location
will also be generated in the space. Agents learn to occupy all
target locations while avoid collision with the obstacle or each
other. The illustration of this environment is Fig. 8.

Figure 7. FACMACâ€™s performance by optimizing the critic networks and
policy networks at the first stage, then training the mixer network

design that the mixing network should be considered as a part
of the critic network, the parameters of the mixing network
will also be updated while optimizing the critic network,
which brings uncertainty to the gradientâ€™s direction. This can
be proved in our second additional experiment.
In our second additional experiment, we froze the
parameters of the mixing network and only optimized agentsâ€™
local critics in the first stage of training, thus the centralized
critic was ablated. In the later stage, we started to update the
mixing networkâ€™s parameters and switched back to the normal
training process of FACMAC. The way to determine the
policy gradient also varied in two stages, Equation (13)
demonstrates how itâ€™s done in the first stage and Equation (14)
for the later stage.
ğ

âˆ‡ğœƒ ğ½(ğœ‡ğ‘ ) = ğ”¼ğ’Ÿ [âˆ‡ğœƒ ğœ‡ğ‘ âˆ‡ğœ‡ğ‘ ğ‘„ğ‘ (ğœğ‘ , ğœ‡ğ‘ (ğœğ‘ ); ğœ™ğ‘ )]
ğ

âˆ‡ğœƒ ğ½(ğ) = ğ”¼ğ’Ÿ [âˆ‡ğœƒ ğâˆ‡ğ ğ‘¸ğ‘¡ğ‘œğ‘¡ (ğœ, ğœ‡1 (ğœ1 ), ğœ‡2 (ğœ2 ) â€¦ ğœ‡ğ‘› (ğœğ‘› ), ğ‘ )]

(13)
(14)

By training agentsâ€™ local critic networks in advance, we
aim to reduce the complexity of centralized training.
Meanwhile, the successful experience tuples stored in the
replay buffer assures that the algorithm can have enough

Due to the different goal of training, the reward function
in this environment is changed from the one in the Obstacle
Predator-Prey. The reward function is set to be the negative
distance sum of all target locations to their nearest agent. In
other words, with each agent getting closer to the itâ€™s target,
the reward will get higher. We also give agents negative
rewards for having collisions with other agents or obstacles.
In order to examine algorithmsâ€™ ability to scale to larger
amount of agents, the Spread environment will have different
versions with different amount of agents, Spread-3a with 3
agents, Spread-6a with 6 agents and Spread-9a with 9 agents.
The result of algorithms in these 3 environmentsâ€™
performance are shown in Fig. 9.
From the result of experiments in those 3 environments, it
can be seen that MADDPG-LSTMactor will perform better
than MADDPG when the agents are relatively few. However,
with the increase of agents, MADDPG-LSTMactor has no
advantage over MADDPG. This could because the increased
complexity brought by LSTM. Although MADDPG-L
doesnâ€™t perform any better than MADDPG when the amount
of agents are small, it demonstrates significant better
performance than MADDPG as agents become more. It can
be concluded that in this environment, MADDPG-L can scale
to more agents better.
Tunnel and Simple Tunnel: In the Tunnel environment,
two massive obstacles will form a narrow tunnel which forces

Figure 9. Algorithmsâ€™ performance in Spread environment with different amount of agents.

the agents to plan path to go through. ğ‘ target locations
spread at one end of the tunnel with fixed positions. After
every environment reset, ğ‘ agents will emerge at fixed
positions at the other end of tunnel. They need to go through
tunnel and occupy every target location while avoid any
contact with each other or the obstacle. The illustration of the
environment is shown in Fig. 10(left).
Similar to the Spread environment, the reward function of
the Tunnel environment is also set to be the negative sum of
each target location to its nearest agent. Agents will get
penalized for collide with other agents or the obstacle.
In order to simplify the environment as well as to be closer
to the reality, some modifications are made to the Tunnel
environment. We set walls around the obstacle to limit the
agentsâ€™ mobility range. The same amount of agents and target
locations still emerge at two end of the tunnel. Whatâ€™s
different is that each agent is designated one target location of
its own to occupy. Agents also need to avoid collisions while
moving to the target locations. The modified environment is
named as the Simple Tunnel environment with its illustration
shown in Fig. 10
The reward function is changed since agents in the Simple
Tunnel environment have different goal. Their rewards are no
longer collaborative and shared. The reward function of agent
ğ‘ is determined by its distance to its designated target
location.Penalties for collisions will also be given. Since each
agentâ€™s reward function are not the same, FACMAC is not
applicable in this environment.
The experiment results are shown in Fig. 11. From the test
result, MADDPG-LSTMactor maintains its advantage over
MADDPG when the number of agents is small. In the Simple
Tunnel environment where is closer to reality, MADDPG-

Figure 10. The illustration of Tunnel environment (left) and Simple Tunnel
environment (right)

LSTMactor does not only demonstrate better performance,
but also shows faster learning speed. While MADDPG-Lâ€™s
performance has no edge over MADDPGâ€™s, MADDPG-L
also has a faster convergence speed than MADDPG.
After the experiment done in the Simple Tunnel
environment, we increase the number of agents in the Simple
Tunnel environment from 3 to 6 and name the new
environment as Simple Tunnel 6a. The test result of
algorithms in this environment is shown in Fig. 11(two on the
right). The experiment demonstrated that MADDPG-L has a
better performance than MADDPG with the increase of
agents while MADDPG-LSTMactor lost its advantage over
MADDPG in term of performance.
V. CONCLUSION
In this paper, we proposed two algorithms, viz.,
MADDPG-LSTMactor and MADDPG-L. These two
algorithms and the underlying algorithms: IDDPG,
MADDPG, FACMAC were together tested in several
virtualenvironments to examine their ability to avoid obstacle
and to cope with the increase in the number of agents.

Figure 11. Algorithmsâ€™ performance in Tunnel environmen(first on the left)) and Algorithmsâ€™ performance in Simple Tunnel environment with
different number of agents(two on the right)

After experiments, this paper can draw a preliminary
conclusion: when the number of agents is small, the
MADDPG-LSTMactor algorithm is a better multi-agent
obstacle avoidance navigation algorithm than MADDPG.
When the number of agents increases, the MADDPG-L
algorithm demonstrates better performance.
The algorithms proposed in this paper exhibits certain
capabilities in virtual environments, and can be used as a basic
in the future. Of course, a realistic obstacle avoidance
algorithm is by no means limited to using reinforcement
learning as the only solution to a problem. There are also
promising solutions of multi-agent navigation based on other
frameworks, such as meta learning [24]. By combining
geolocation, computer vision, etc., better performance may be
achieved. Multi-agent navigation in case of more difficult
scenarios, e.g., with limited communication power [25], is also
well worth exploration.

TABLE IV.

TABLE V.

Value

Num-adversaries

1

Lr-actor

0.001

Lr-critic

0.01

Batch-size

256

seq-length

3

HYPERPARAMETERS IN SPREAD-3A ENVIRONMENT
Hyperparameter

Value

Num-adversaries

0

Lr-actor

0.001

Lr-critic

0.01

Batch-size

128

seq-length

5

HYPERPARAMETERS IN SPREAD-6A AND SPREAD-9A
ENVIRONMENT
Hyperparameter
Num-adversaries
Lr-actor
Lr-critic
Batch-size
seq-length

Hyperparameter

Value

Num-adversaries

0

Lr-actor

0.001

Lr-critic

0.001

Batch-size

32

seq-length

3

ACKNOWLEDGMENT
This work was supported by the National Natural Science
Foundation of China under grants 62172072, 61872101,
61872055, U1908214, U21A20491, the Fundamental
Research Funds for the Central Universities under grant
DUT21JC18, Natural Science Foundation of Liaoning
Province of China under Grant 2021-MS-114.

[1] Lavalle S M. Rapidly-exploring random trees: A new tool for path

HYPERPARAMETERS IN OBSTACLE PREDATOR PREY
ENVIRONMENT
Hyperparameter

HYPERPARAMETERS IN SIMPLE TUNNEL-6A ENVIRONMENT

REFERENCES

APPENDIX
TABLE III.

TABLE VII.

Value
0
0.001
0.01
32
3

planning. Computer Science, 1998: 98.

[2] Kavraki L E, Svestka P, Latombe J C, et al. Probabilistic roadmaps for
path planning in high-dimensional configuration spaces. IEEE
Transactions on Robotics and Automation, 1996, 12(4): 566-580.

[3] Fakoor M, Kosari A, Jafarzadeh M. Revision on fuzzy artificial
potential field for humanoid robot path planning in unknown
environment. International Journal of Advanced Mechatronic Systems,
2015, 6(4): 174-183.

[4] Liu X, Wang Q, Xu S. Path planning based on improved probabilistic
roadmap method. Fire Control & Command Control, 2012,
037(004):121-124.

[5] Yang S, Liu Y, Yang W. A local dynamic probabilistic roadmap
method for unknown environment. Aeronautical Science & Technology,
2016, 27(04):69-73.

[6] Liu C, Han J, An K. Dynamic path planning based on an improved RRT
algorithm for roboCup robot. Robot, 2017, 39(01):8-15.

[7] Zhou F, Zhu Q, Zhao G. Path optimization of manipulator based on the
improved rapidly-exploring random tree algorithm. Journal of
Mechanical Engineering, 2011, 47(11):30-35.

[8] Smith R, Self M, Cheeseman P. Estimating uncertain spatial
relationships in robotics[M]. Autonomous robot vehicles. Springer,
New York, NY, 1990: 167-193.

[9] Smith R C, Cheeseman P. On the representation and estimation of
spatial uncertainty. The International Journal of Robotics Research,
1986, 5(4): 56-68.

[10] Zhang M. RGB-D SLAM algorithm of indoor mobile robot. Harbin
Institute of Technology, 2018.

TABLE VI.

HYPERPARAMETERS IN TUNNEL AND SIMPLE TUNNEL
ENVIRONMENT
Hyperparameter

Value

Num-adversaries

0

Lr-actor

0.001

Lr-critic

0.01

Batch-size

128

seq-length

5

[11] Li X. Research on navigation technology of indoor mobile robot based
on SLAM. Harbin Institute of Technology, 2018.

[12] Dechter R, Pearl J. Generalized best-first search strategies and the
optimality of A. Journal of the ACM (JACM), 1985, 32(3): 505-536.

[13] Tan M. Multi-Agent Reinforcement Learning: Independent vs.
Cooperative Agents. Machine Learning Proceedings, 1993:330-337.

[14] Watkins C J C H, Dayan P. Q-learning[J]. Machine learning, 1992, 8(3):
279-292.

[15] de Witt C S, Gupta T, Makoviichuk D, et al. Is independent learning all
you need in the starcraft multi-agent challenge?. arXiv preprint
arXiv:2011.09533, 2020.

[16] Lowe R, Wu Y, Tamar A, et al. Multi-agent actor-critic for mixed
cooperative-competitive environments.International Conference on
Neural Information Processing Systems, Long Beach, 2017: 6382-6393.

[17] Sunehag P, Lever G, Gruslys A, et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296,
2017.

[18] Rashid T, Samvelyan M, Schroeder C, et al. Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement
learning.International Conference on Machine Learning, Stockholm,
2018: 4295-4304.

[19] Peng B, Rashid T, de Witt C A S, et al. FACMAC: Factored MultiAgent Centralised Policy Gradients.Neural Information Processing
Systems, Online, 2021: 12208-12221.

[20] Naveed K B, Qiao Z, Dolan J M. Trajectory Planning for Autonomous
Vehicles Using Hierarchical Reinforcement Learning[C].2021 IEEE
International Intelligent Transportation Systems Conference (ITSC),
Indianapolis, 2021: 601-606.

[21] Parisotto E, Song F, Rae J, et al. Stabilizing transformers for
reinforcement learning[C].International Conference on Machine
Learning, Online, 2020: 7487-7498.

[22] Sutton R S, McAllester D, Singh S, et al. Policy gradient methods for
reinforcement learning with function approximation[J]. Advances in
neural information processing systems, 1999, 12:1-7.

[23] Opendilab,Dl-engine-docs[OL].https://di-enginedocs.readthedocs.io/zh_CN/latest/env_tutorial/multiagent_particle_zh.
html

[24] Salar A, Mo C, Mehran M, et al. Toward Observation Based Least
Restrictive Collision Avoidance Using Deep Meta Reinforcement
Learning. IEEE Robotics Autom. Lett. 2021, 6(4): 7445-7452.

[25] Yuanzhao Z, Bo D, Xuan L, et al. Decentralized Multi-Robot Collision
Avoidance in Complex Scenarios With Selective Communication.
IEEE Robotics Autom. Lett. 2021, 6(4): 8379-8386.

