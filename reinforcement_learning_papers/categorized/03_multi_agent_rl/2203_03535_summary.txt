The paper "Influencing Long-Term Behavior in Multiagent Reinforcement Learning" proposes a new framework for multiagent reinforcement learning (MARL) that addresses the challenge of non-stationarity in the presence of simultaneously learning agents. The authors introduce the concept of "active Markov games" and develop a practical approach called FURTHER (FUlly Reinforcing acTive influence witH averagE Reward) to optimize the average reward objective in this setting.

Here is the extracted information in the required format:

**PAPER:** Influencing Long-Term Behavior in Multiagent Reinforcement Learning
**TITLE:** Influencing Long-Term Behavior in Multiagent Reinforcement Learning
**ARXIV_ID:** 2203.03535v4
**RESEARCH_METHOD:** 03_multi_agent_rl
**METHOD_DESCRIPTION:** The authors propose a new framework for MARL that addresses the challenge of non-stationarity in the presence of simultaneously learning agents. They introduce the concept of "active Markov games" and develop a practical approach called FURTHER to optimize the average reward objective in this setting.
**KEY_CONTRIBUTIONS:**
* The authors propose a new framework for MARL that addresses the challenge of non-stationarity in the presence of simultaneously learning agents.
* They introduce the concept of "active Markov games" and develop a practical approach called FURTHER to optimize the average reward objective in this setting.
* The authors demonstrate the effectiveness of FURTHER in various multiagent benchmark domains, including iterated matrix games, MuJoCo RoboSumo, and MAgent Battle.
* They show that FURTHER can achieve better performance than state-of-the-art baselines, including LILI, MASAC, DRON, and MOA, in different settings.
