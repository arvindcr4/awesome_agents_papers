Published in Transactions on Machine Learning Research (09/2025)

COMMA: A
Benchmark

Communicative

Multimodal

Timothy Ossowski

Multi-Agent

ossowski@wisc.edu

Department of Computer Sciences
University of Wisconsin-Madison

Danyal Maqbool

dmaqbool@wisc.edu

arXiv:2410.07553v5 [cs.AI] 16 Dec 2025

Department of Computer Sciences
University of Wisconsin-Madison

Jixuan Chen

jxchen0908@gmail.com

Department of Computer Sciences
UC San Diego

Zefan Cai

zefancai@cs.wisc.edu

Department of Computer Sciences
University of Wisconsin-Madison

Tyler Bradshaw

tbradshaw@wisc.edu

Department of Radiology
University of Wisconsin-Madison

Junjie Hu

jhu@cs.wisc.edu

Department of Computer Sciences
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
Reviewed on OpenReview: https: // openreview. net/ forum? id= TIGQIem1na

Abstract
The rapid advances of multimodal agents built on large foundation models have largely
overlooked their potential for language-based communication between agents in collaborative
tasks. This oversight presents a critical gap in understanding their effectiveness in real-world
deployments, particularly when communicating with humans. Existing agentic benchmarks
fail to address key aspects of inter-agent communication and collaboration, particularly in
scenarios where agents have unequal access to information and must work together to achieve
tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a
novel puzzle benchmark designed to evaluate the collaborative performance of multimodal
multi-agent systems through language communication. Our benchmark features a variety
of multimodal puzzles, providing a comprehensive evaluation across four key categories of
agentic capability in a communicative collaboration setting. Our findings reveal surprising
weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o
and reasoning models like o4-mini. Many chain of thought reasoning models such as R1Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent
collaboration, indicating a potential growth area in their communication abilities. 1

1 Our data and code is available at https://github.com/tossowski/COMMA

1

Published in Transactions on Machine Learning Research (09/2025)
Solver Agent

Expert Agent
Agent Communication

Solver Agent Input

Agent Conversation
Action Execution

Task
Prompt

Working
Memory

Solver:
Expert:
Solver:
Environment:
⋮

Episodic Memory

State Update

Environment

Solver: I see a 2x2 grid with four symbols...
Expert: Press the buttons in this order: ...
Solver: press_top_right_button
press_bottom_left_button
press_bottom_right_button
press_top_left_button
Environment: The action was performed successfully
The action was performed successfully
That action seems to have been a mistake
⋮

Expert Agent Input

Task
Prompt

Working
Memory

Solver:
Expert:
Solver:

Episodic Memory

Solver Memory Update

Expert Memory Update

Figure 1: Overview of the interaction between the Solver and Expert agents in our benchmark. Both agents
operate with structured input corresponding to working and episodic memory. The Solver receives an image
of the puzzle state (working memory) and makes decisions based on the available actions described in the
task prompt. The Expert, guided by instruction manuals (working memory), provides advice based on
the Solver’s descriptions, such as indicating which buttons to press. The Solver can choose to execute
actions by interacting with the environment or communicate with the Expert for further guidance. Their
interaction is documented through a dialogue, showcasing the cooperation required to complete the task.
Both agents engage in self-reflection by referencing the conversation history, which is continuously updated
and included in their input as episodic memory. We include a more comprehensive example in Appendix A

1

Introduction

The field of multimodal agents is experiencing rapid growth (Xu et al., 2024c; Xie et al., 2024; Cao et al.,
2024), with research efforts expanding at an unprecedented pace. However, amidst this growth, a critical gap
in research has emerged: the lack of focus on collaborative work (Gurcan, 2024; Park et al., 2023; Hong et al.,
2024; Liu et al., 2024b) among multiple multimodal agents. Synergistic operation of such agents is a highly
promising but largely unexplored domain. Language agents can collaboratively finish complex tasks such as
software development (Qian et al., 2024; Du et al., 2024) or even machine learning research (Schmidgall et al.,
2025) by assuming functional roles such as system designer, function generator, etc. Current research on
multimodal agents (Xu et al., 2024c; Xie et al., 2024; Cao et al., 2024) has mainly focused on individual agent
capabilities, neglecting the potential for inter-agent collaboration. This limitation is further compounded by
existing benchmarks such as TheAgentCompany (Xu et al., 2024a), VisualWebArena (Koh et al., 2024) and
MME-RealWorld (Zhang et al., 2024), which do not assess collaborative performance between agents. As a
result, our ability to evaluate and improve multi-agent systems remains constrained, hindering progress in
this crucial area.
Several critical questions emerge in the context of multimodal agent collaboration. How can different agents
effectively communicate multimodal information through language when they have varying levels of access
to information? In scenarios where different agents possess diverse task-specific capabilities, how can they
collaborate to accomplish objectives beyond the scope of any individual agent? These research settings remain
largely uncharted and present significant challenges. Furthermore, the ability of agents to handle incomplete
information is of paramount importance, particularly when working with sensitive data (Li et al., 2024)
(i.e. Agent application in healthcare where privacy concerns are critical (Tang et al., 2024)). Exploration
of these questions is crucial for advancing the field of multimodal agent collaboration. By addressing these
challenges, we can expand the applicability of multimodal agents in real-world scenarios (Zhang et al., 2024),
particularly those involving sensitive or restricted information.
Motivated by these aforementioned issues, we propose a benchmark for evaluating multimodal models in a
collaborative agentic setting to address critical gaps in current approaches (see Figure 1). While recent
work has explored scaling test time compute (Snell et al., 2025) to improve reasoning performance, such
methods often rely on the unrealistic assumption that a model has sufficient capability and access to all
necessary information to complete the task. In contrast, our benchmark poses a unique challenge: models
2

Published in Transactions on Machine Learning Research (09/2025)

must dynamically alternate between intensive reasoning based on the information available and effective
communication to request additional details when needed. Our evaluation setting also simulates a scenario
where an in-house agent with direct access to sensitive data (i.e., the AI solver) collaborates with external
expert agents (i.e., the AI expert) to analyze information without compromising privacy. Specifically, this
work makes the following contributions:
• We propose an evaluation benchmark called COMMA, a multimodal agent benchmark which uses puzzles
to test language communication between multiple agents (Section 3). To achieve a high score, agents
must engage in concise, but meaningful dialogue to arrive at each solution.
• Using COMMA, we carefully record conversations and performance metrics between state-of-the-art multimodal models such as o4-mini, GPT-4o, Gemini, QwenVL, etc (Section 4).
• We categorize the agent capabilities tested in our model and common failure modes, providing insight
into future research directions for improving inter-agent communication (Section 5).

2

Related Work

Multi-agent Frameworks: There are many emergent agent collaboration works (Gurcan, 2024; Park
et al., 2023; Hong et al., 2024; Liu et al., 2024b; Ghafarollahi & Buehler, 2024; Li et al., 2023; Wu et al.,
2023) among multiple language agents. Multi-agent systems arise mainly in two different scenarios: (1)
role-playing different task executors (e.g., software development requiring different roles of agents, such as
program manager, software architect, programmer (Du et al., 2024; Qian et al., 2024; Hong et al., 2024),
scientific discovery simulation (Wu et al., 2023), and social simulation (Park et al., 2023; Gurcan, 2024; Park
et al., 2024)); (2) communicating between agents with different pieces of information (Wu et al., 2023; Li
et al., 2023) (e.g., consulting experts without sharing some sensitive or confidential data. In our case, the AI
solver has some private multimodal data, and the AI expert has domain-specific knowledge or instructions).
Instruction-based Agent Benchmarks: Instruction-based agent benchmarks evaluate an agent’s capability of following a human instructions to finish a task (e.g., navigating on a website, interacting with an
operating system (Xu et al., 2024c; Xie et al., 2024; Cao et al., 2024)). However, our benchmark focuses
more on a communication-based evaluation where two clients engage in multi-turn conversations to solve a
task collaboratively.
Cognitive Science Our benchmark draws inspiration from the foundational principles of intelligence,
often defined as the ability to learn from experience, adapt to the environment, and solve problems using
cognitive skills (Kempf-Leonard, 2005). Existing cognitive science research has shown that even simple tests
can effectively measure cognitive ability (Davidson et al., 2006; St Clair-Thompson & Gathercole, 2006).
Standardized intelligence tests, such as MENSA (MENSA International, n.d.) and the Wechsler Intelligence
Scale for Children (Wechsler, 1949), frequently employ simple puzzles to evaluate these skills. Our work
extends this research to multi-agent communication, designing vision-language puzzles to evaluate the core
cognitive abilities of multimodal agents.

3

Benchmark

3.1

Design Principles of the Benchmark

Our benchmark is inspired by the cooperative gameplay scenario in Keep Talking and Nobody Explodes
Games (2015). In this game, two players work together to defuse a bomb under time pressure. One player,
the defuser, can see the bomb but lacks the instructions to disarm it. The other player, the expert, has access
to the bomb’s manual but cannot see the bomb itself. The players must rely on effective communication to
exchange information, navigate challenges, and defuse the bomb.
We adapt this dynamic for our benchmark by shifting the focus to solving vision-language puzzles within
a communication-based agent framework. While some manuals are similar, we change most of the manuals
3

Published in Transactions on Machine Learning Research (09/2025)

from the original game to provide a fair comparison between models and prevent data leakage. To reflect this
broader scope, we rename the defuser role to “Solver,” emphasizing its general-purpose functionality across
diverse tasks. In particular, our benchmark simulates real-world tasks like pair programming with coding
copilots and multimodal tutoring systems, where one agent may have access to a visual context and another
only to language instructions. Additionally, this setting closely resembles multi-agent workflows involving
external tool use, such as querying databases, invoking APIs, or browsing the web. However, we abstract
away tool interfaces to focus on evaluation of reasoning, communication, and collaborative capabilities. Our
benchmark design is grounded in the following core principles:
Agentic Architecture: We carefully structure the Solver and Expert agents, drawing on the cognitive
agent terminology from Sumers et al. (2023). The Solver agent is equipped with working memory, which
includes the task prompt, a screenshot of the puzzle, and direct feedback from the environment based on
its actions. Additionally, the Solver has episodic memory in the form of conversation history, enabling it to
learn from past mistakes and make informed decisions. The Expert agent follows a similar architecture but
lacks access to environmental feedback or puzzle screenshots in its working or episodic memory. Instead, it
relies solely on communication with the Solver to infer the puzzle state and provide guidance. We also offer
the option for a human to roleplay as either the solver or expert agent in our framework. Figure 1 provides
an illustration of the interaction between these agents, and we provide more details in Appendix A.
Language communication: A critical aspect of our benchmark is evaluating natural language communication between agents. Similar to how players in the original game exchange information verbally, agents in
our framework must use language to share observations, clarify ambiguities, and reason about tasks. For the
agents to succeed, they must display clarity, efficiency, and depth of communication, making it an essential
factor in task completion.
Controllability: Our framework is designed to allow for flexible difficulty and agent customizability by
users. By granting agents access to user-defined functions and configuration files, it enables a modular
environment for communication between multimodal agents, providing a robust platform to evaluate their
intelligence. Future users can easily customize the framework by incorporating their own challenging puzzles
and manuals, tailoring it to simulate more realistic and complex scenarios.
Multimodality: Our benchmark emphasizes the integration of multiple sensory inputs and outputs, such
as vision, language, and audio. The puzzles involve visual elements that agents must perceive, describe, and
interpret, alongside linguistic interactions. This principle assesses an agent’s ability to handle and synthesize
multimodal information, a skill crucial to real-world applications.
3.2

Categories of Agent Cognitive Capability

We benchmark agents working under different roles to solve various tasks in multiple settings, each requiring
different cognitive capabilities. Specifically, the Solver agent must demonstrate strong instruction-following
and multimodal reasoning, while the Expert agent is expected to excel in long text summarization and
information retrieval. Both agents must possess visual comprehension and descriptive skills to succeed.
Below, we outline the core capabilities tested in our benchmark.
Memory Recall ( MR) In many puzzles, agents must remember their previous actions to progress. This
ability is also implicitly tested when agents make mistakes. A competent agent should recall instances where
past actions led to errors and adapt to avoid repeating them. The capacity to learn from mistakes and
leverage memory is crucial for effective problem-solving in real-world situations.
Multimodal Grounding (MG) Since the solver agent can only communicate with the expert with text,
it must be able to ground relevant spans of the expert’s instructions to the image it currently sees. This
grounding of language in visual context is essential for interpreting and following guidance from the expert
agent effectively.
Multi-Step Reasoning ( MSR) Certain puzzles require agents to follow a sequence of actions based on
step-by-step reasoning. Much like real-world tasks, such as following a recipe or placing an online order,
4

Published in Transactions on Machine Learning Research (09/2025)

each action must be deliberate and contribute toward the overall goal. Our benchmark enables fine-grained
evaluation of progress within these multi-step reasoning tasks, allowing for a precise assessment of models’
reasoning capabilities.
Private Information ( PR ) Some puzzles challenge agents to withold information that might be sensitive
and should not be shared through communication. This is a critical skill for embodied agents operating in
real-world environments when dealing with proprietary data such as medical or personal financial records.
3.3

Tasks

We create 10 puzzles across 4 different categories briefly summarized below. A more comprehensive description along with example images and instruction manuals can be found in Appendix A.
• ATMPuzzle ( PR ): The solver must navigate a bank interface and either make a withdrawal or deposit
depending on the amount of their balance. The solver must not reveal private information such as their
PIN number or balance amount while communicating with the expert.
• TelehealthPuzzle ( PR ): The solver is in a health crisis situation and presented a private image
of their skin and their background information (sourced from PAD-UFES-20 (Pacheco et al., 2020)).
The solver must communicate with the expert to diagnose the skin disease and select the appropriate
treatment plan, while taking care to not reveal any private health information.
• ColorPuzzle (MR , MSR ): The solver aims to turn all of the squares in a 4x4 grid white. At each
step, the solver should press squares based on the frequencies of colors, following the rules specified in a
table visible only to the expert.
• KeypadPuzzle (MG , MSR ): The solver must describe the symbol of each button in a 2x2 grid.
The expert must then identify a column in the manual containing these four unique symbols and tell the
solver to press the symbols in the correct order.
• LedPuzzle ( MR , MSR ): The solver presses a button if the value of its letter, when multiplied by a
stage’s LED color multiplier and taken modulo 26, matches the value of the letter diagonally opposite it.
At each stage, the letters on the buttons change.
• MazePuzzle ( MG , MSR ): The solver navigates a mouse through a maze to a colored sphere,
pressing the correct button to disarm the module based on the layout.
• MemoryPuzzle ( MR , MSR ): The solver presses buttons according to specific positional and labelbased rules over five stages, with incorrect presses resetting progress. The rules for the current action
depend on buttons pressed previously during the conversation.
• PasswordPuzzle ( MG , MSR ): The solver cycles through letters to form a valid word from a
predefined list, submitting the correct word to complete the puzzle.
• WhoPuzzle (MG ): The solver must read out the value on a display to the expert, who will identify
a button position to read from. The solver must then tell the expert the label of this button, and then
press the correct button based on a detailed list of instructions.
• WirePuzzle (MG ): The solver must cut one of the wires on the display. There are 3 to 6 colored
wires, and the correct wire to cut changes depending on the number and order of colors.

4

Evaluation

4.1

Experimental Setup

In this section, we describe the experimental settings of our multi-agent interaction environment where two
distinct agents, namely the Solver agent and the Expert agent, engage in iterative dialogue sessions. The
5

Published in Transactions on Machine Learning Research (09/2025)

primary aim of this setup is to assess the collaborative problem-solving capabilities between different agents.
During our experiments, we limit the number of conversation turns to 10 and the number of mistakes to 3,
allowing for a unified and systematic assessment of interactions. The puzzle set used in evaluation consists of
100 fixed but different initializations of each of the 10 puzzles, resulting in 1000 total conversations. For most
models, we use greedy decoding when available to maintain consistent agent output across different runs of
the same puzzle. However, for reasoning models we set the temperature to 0.6 to avoid endless repetition.
All inference is run on a single NVIDIA A100 GPU with 80GB VRAM. We parse the solver’s chosen actions
at each conversation turn using exact string matching and directly perform the action on the interface if the
solver outputs a valid action. Exact prompts for both agents are in Appendix D.
4.2

Evaluation Metrics

We recorded several key performance metrics through multiple iterations of the experiments described below:
• Success Rate (SR): The solver agent is assigned a 0 or 100 value for each puzzle depending on the
completion status. These values are averaged across all puzzles to obtain the success rate.
• Partial Success Rate (PSR): Because our benchmark includes puzzles with multi-step reasoning, some
puzzles can have a more precise success rate evaluation. For these multi-step puzzles, we assign the solver
a number between 0 and 100 to indicate its progress towards the solution, and average across puzzles for
a partial success rate. For single-step puzzles, the partial success rate equals the success rate.
• Efficiency Score: Effective communication is concise but meaningful. Inspired by the classical metric
BLEU (Papineni et al., 2002) that balances the n-gram precisions with a length penalty, we propose a
new metric which balances success rate and conciseness:
Sconciseness =

1

τ ,
1 + 1000

Sefficiency =

2 × PSR × Sconciseness
PSR + Sconciseness

(1)

where τ is the average token usage per puzzle. We use the harmonic mean between the conciseness and
performance scores to ensure that both need to be high to indicate good performance.
• Average Mistakes (AM): After an action is chosen by the solver, the environment checks if the action
was a mistake. We tally up the mistakes made during each puzzle and take a global average across puzzles
to obtain average mistakes.
• Average Conversation Length (ACL): We count the number of conversation turns the Solver took to
arrive at the solution, or default to the maximum of 10 if the solver failed. This count is averaged across
all puzzles to get the Average Conversation Length.
4.3

Models Evaluated

We briefly summarize the types of models evaluated in our study. More details about the models can be
found in Appendix A.
• Human: We conduct experiments in which a human plays as the solver to provide an idealized upper
bound. As hiring participants was prohibitively expensive and time-consuming, we role-played as agents
ourselves across 100 sampled puzzles as a preliminary study, and leave further human participation to
future work (see Section 7).
• Reasoning Models: We experiment with several state-of-the-art reasoning models including o4-mini
(OpenAI, 2025), R1-OneVision (Yang et al., 2025b), and LLaVA-CoT (Xu et al., 2024b). We exclude
o3 from evaluation, as it performs similarly to o4-mini in our exploration but incurs substantially higher
computational cost.
• General Purpose Proprietary Models: We compare with several powerful VLMs used in everyday
tasks, including GPT-4o (Hurst et al., 2024), Gemini-2.0-Flash (Anil et al., 2023), and GPT-4V (Achiam
et al., 2023).
6

Published in Transactions on Machine Learning Research (09/2025)

• General Purpose Open-Source Models: We compare with state-of-the-art open-source visionlanguage models pre-trained on diverse tasks, such as InternVL (Chen et al., 2024), QwenVL (Bai
et al., 2023), LLaMA 3.2 (Touvron et al., 2024), and LLaVA 1.6 (Liu et al., 2024a).
• Random Baseline: This baseline selects a random action uniformly from a pool of valid actions at each
timestep. It does not utilize any information from the expert agent, puzzle state, conversation history, or
past actions.

Model

Wire

Telehealth

Who

LED

Average Partial Success Rate % (↑)
Memory
Keypad
Password
Color

Maze

Atm

Overall

Human + GPT-4o 100 ± 0.0 65 ± 15.0 90 ± 10.0 30 ± 13.3 50 ± 16.7 60 ± 11.9 80 ± 13.3 17 ± 5.7 70 ± 15.1 100 ± 0.0 69.01 ± 4.2
o4-mini
R1-OneVision
LLaVA-CoT

99 ± 1.0
45 ± 5.0
48 ± 5.0

58 ± 9.0
24 ± 2.8
7 ± 1.7

73 ± 8.2
17 ± 3.8
30 ± 4.6

64 ± 8.5
19 ± 3.0
24 ± 3.2

5 ± 2.1
31 ± 3.1
26 ± 2.6

31 ± 6.3
19 ± 3.1
12 ± 1.9

60 ± 9.1
0 ± 0.0
0 ± 0.0

18 ± 3.1
4 ± 0.9
1 ± 0.6

13 ± 6.3
0 ± 0.0
2 ± 1.4

17 ± 6.9
0 ± 0.0
0 ± 0.0

53.98 ± 2.5
16.81 ± 1.0
14.97 ± 1.0

GPT-4o
Gemini
GPT-4V

98 ± 1.4
85 ± 3.6
77 ± 4.2

64 ± 4.7
47 ± 4.9
48 ± 5.0

72 ± 4.5
35 ± 4.8
39 ± 4.9

32 ± 3.4
60 ± 3.3
30 ± 3.0

31 ± 3.9
24 ± 2.0
8 ± 1.4

27 ± 2.5
24 ± 3.3
32 ± 2.9

2 ± 1.4
4 ± 2.0
0 ± 0.0

10 ± 1.6
11 ± 1.3
8 ± 1.1

33 ± 3.6
16 ± 3.3
15 ± 3.5

47 ± 5.0
27 ± 4.5
19 ± 3.9

41.74 ± 1.4
33.37 ± 1.3
27.62 ± 1.3

LLaMA 3.2
QwenVL
InternVL
Random
LLaVA 1.6

64 ± 4.8
56 ± 5.0
61 ± 4.9
57 ± 5.0
41 ± 4.9

13 ± 2.5
40 ± 4.9
23 ± 2.5
16 ± 2.6
20 ± 2.6

27 ± 4.5
26 ± 4.4
28 ± 4.5
44 ± 5.0
25 ± 4.3

29 ± 3.5
31 ± 3.5
25 ± 3.3
32 ± 3.6
35 ± 3.6

27 ± 3.0
16 ± 2.3
18 ± 1.4
15 ± 1.6
13 ± 1.5

28 ± 2.9
25 ± 2.5
24 ± 2.5
18 ± 2.1
16 ± 2.4

0 ± 0.0
0 ± 0.0
1 ± 1.0
0 ± 0.0
0 ± 0.0

9 ± 1.4
5 ± 0.8
9 ± 1.4
6 ± 1.0
2 ± 0.8

24 ± 3.2
4 ± 1.3
24 ± 4.7
0 ± 0.0
20 ± 4.2

0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0

22.15 ± 1.1
20.28 ± 1.1
19.57 ± 1.0
18.70 ± 1.1
17.32 ± 1.0

Table 1: Average partial success rate of multimodal agents on each puzzle with standard error estimates for
uncertainty ±σ. For each row, the solver and expert are separate instances of the same model. “Human"
model indicates a human is the solver, and the expert is a GPT-4o model. The partial success rate is
calculated by averaging over 100, 30 or 10 independent runs for AI, o4-mini, or human solver respectively.
The overall column is calculated by averaging across all the puzzles. We bold the human score and the best
AI model in each column.

5

Results and Analysis

5.1

Overall Performance

Table 1 presents the average partial success rate (%) of various multimodal agents and a human solver across
several puzzles, highlighting their relative performance. For overall performance results for other metrics,
please refer to Appendix C. For all puzzles, we report the mean partial success rate across 100 independent
instantiations of the initial puzzle state. Due to the source of randomness being the puzzle initialization,
rather than differences in model output on the same data, we report standard error estimates for all puzzles
(±σ) to approximate the confidence interval of performance on each puzzle.
We observe that the human solver outperforms all models, achieving the highest overall score of 69.01%.
Among the AI models, o4-mini demonstrates the best overall performance (53.98%), significantly surpassing
the others, including GPT-4o (41.74%) and Gemini 2.0 (33.37%). The remaining open-source models, such
as QwenVL 7b, InternVL 8b, and LLaMA 3.2, show significantly lower success rates, with LLaVA-CoT
achieving the lowest overall score (14.97%). Performance varies across puzzle types, with models generally
struggling in more challenging multi-step tasks such as “Password" while performing relatively better in tasks
such as the wire puzzle.
Interestingly, the random baseline is competitive with models like LLaVA-CoT and R1-OneVision, despite
having no access to the puzzle state or expert knowledge. We found that long CoT models such as LLaVACoT attempt to solve the puzzles independently through chain-of-thought reasoning, but they struggle
to generalize to the novel communication setting introduced in our benchmark. These models are typically
optimized for single-agent, instruction-following tasks, and their reasoning often fails to align with the implicit
coordination required between agents. In contrast, the random baseline naturally explores a broader set of
trajectories by sampling uniformly from the space of valid actions. This stochasticity allows it to occasionally
stumble upon a correct solution. As a result, the random baseline can match or even outperform models
that rely on self-reasoning strategies in unfamiliar collaborative contexts.
7

Published in Transactions on Machine Learning Research (09/2025)

Solver Agent View

Solver Agent View

Misinterpretation Error
Solver: The image shows a panel
with six wires, each connected to a
"CUT" button. The wires are colored
as follows from top to bottom:
White, White, White, Yellow, Yellow,
White ...
Expert: Since there is exactly one
yellow wire and more than one
white wire, cut the fourth wire.
Solver: cut_wire_4
Environment: That action seems to
have been a mistake.
⋮

Solver Agent View

Miscommunication Error
Solver: The image shows an ATM
interface with the following details:
The screen displays "Making a
Deposit..." and prompts to ...
Expert: Press the up arrow to move
the selection to "Go Back\". Press the
enter button to return to the main
menu. Select "Withdrawal" again ...
Solver: press_9\npress_3\npress_3\n
press_5\npress_enter
Environment: ...
Expert: You should have gone back
to the main menu first.

Roleplay Error

Solver Agent View

Solver: The image shows a 2x2
grid with four symbols: ... I need
to press the top left
button.\n\npress_top_left_button
Environment: That action seems
to have been a mistake.
Expert: SOLVER: The top left
button has the symbol \"C\".
Solver: press_top_left_button
Environment: That action seems
to have been a mistake.
⋮

Repetition Loop Error
Solver: The display now shows the
number 1, and the buttons below it
are labeled 4, 3, 2, and 1 from left to
right. What should I do?
Expert: Press the button labeled "4"
Solver: press_1
⋮
Solver: The display now shows the
number 1, and the buttons below it
are labeled 4, 3, 2, and 1 from left to
right. What should I do?
Expert: Press the button labeled "4"
Solver: press_1

Figure 2: Case study examples of InternVL 8b (bottom left) and GPT-4o (all others) failures when used
as agents in our benchmark. Top Left: An AI solver misinterpretation error results from inaccurate
perception of the puzzle’s wires. Top Right: The solver ignores the instructions from the expert, resulting
in a miscommunication error. Bottom Left: The expert acts as if it is the solver and can see the module
displayed to the solver, resulting in a roleplay error. Bottom Right: The solver performs the same action
in a previously seen situation, leading to a repetition loop error.

15%

36%

6%
24%
19%

60%

46%

6%
9%

9%

4% 21%

31%

10%
4%

Other
Communication
Repetition
Interpretation
Roleplay

Figure 3: Distribution of error categories across conversations evaluated by our calibrated GPT-Judge for
GPT-4o conversations (left), LLaMA 3.2 conversations (center), and o4-mini (right).

5.2

Qualitative Analysis on Model Failures

In this section, we present key insights and analyze common failure modes exhibited by the agents during their
conversations. We begin by defining four common error types that models display when making mistakes
in agent communication. To better understand these errors, we curate a calibration dataset comprising
10 representative examples for each error type, taken from sampled conversations. This dataset is used
to validate and develop a o1 (Jaech et al., 2024) model judge. To calibrate the judge, we first identified
common failure cases we were observing (e.g., miscommunication) that prevented the solver from finishing
the puzzle or making progress. We then manually and randomly sampled 10 instances of each failure type,
along with 10 randomly selected normal conversation logs. The calibration process involved continuously
refining the prompt used for the GPT-Judge until it achieved high classification accuracy. After making
minor adjustments to the judge’s prompt and providing access to the ground truth conversations, the model
8

Published in Transactions on Machine Learning Research (09/2025)

achieved 94% accuracy on the calibration dataset, demonstrating strong alignment with human annotators.
Once calibrated, the judge model was used to analyze conversations and environment messages between the
solver and expert for all puzzles. The distribution of these error types across the benchmark is summarized
in Figure 3. We acknowledge that the GPT-Judge is not a perfect system and may exhibit some bias favoring
responses that are similar to the GPT-Judge’s own patterns. We aim to mitigate this bias by ensuring that
the prompt used for judgment contained clear, rule-grounded definitions of each failure type to minimize
ambiguity. Moreover, the GPT-Judge is not the primary source of accuracy in our evaluation. Our primary
results rely on rule-based scoring and solver progression metrics. The GPT-Judge is intended to serve as a
complementary analysis tool that enables scalable evaluation of agent interactions.
The definitions for our identified error types are as follows:
• Roleplay: The expert thinks it is the solver or vice versa. Figure 2 illustrates how the expert can
misunderstand its role assignment, leading to miscommunication and failure to solve the puzzle.
• Misinterpretation: The solver misunderstands the current puzzle state or signal, resulting in failure.
For instance, Figure 2 showcases the solver and expert misinterpreting the colors of the wires in the image,
leading to an incorrect action.
• Repetition Loop: The solver sometimes repeats its past incorrect actions, even if it is in a situation it
has encountered before. We classify any repeated incorrect state, action pair into this category.
• Miscommunication: As shown in Figure 2, the agent occasionally disregards the expert’s instructions,
attempting to solve the puzzle independently as if it were the expert. We classify this error when the
solver or expert fails to follow the other’s instructions.
Reasoning and Open-Source Models are Worse at Communication As shown in Figure 3, LLaMA
3.2 anad o4-mini share similar error type distributions across our benchmark, both struggling the most with
communication and interpretation errors. We observed that this often occurs because the solver attempts
to solve the puzzles on its own, refusing to communicate with the expert. We hypothesize this is due to the
significant domain shift in pretraining data for these models, in which the data the models were trained to
assume all the information is available in the prompt to answer the question. While reasoning models like
o4-mini may excel at self-reflection, they still struggle with communication to perform tasks outside of the
scope of their information. The open-source models display similar weaknesses in trying to solve the task on
their own. To achieve a similar level of versatility as GPT-4o, adding tasks which require communication to
pretraining will likely address this issue for these models.
5.3

Fine-grained Analysis

Learning from Past Mistakes (Episodic Memory) An important skill for agents is to learn from past
mistakes to adapt to similar future situations. Here we analyze if agents can correct their past mistakes
based on their conversation episodic memory when solving a puzzle. Figure 4 plots the number of allowed
conversation turns to solve a puzzle, along with the overall success rate of several multimodal agents. The
plot demonstrates that all models improve as conversation length increases, with top-performing models like
o4-mini, GPT-4o, and Gemini significantly higher improvement as conversation length increases. The opensource models such as LLaVA, InternVL, actually underperform the random baseline for most conversation
lengths. Overall the results show a plateau after about 4-5 conversation turns for the AI agents, indicating
that they may not use their episodic memory well. However, human agents show consistent improvement as
conversation length increases.
Handling Private Data Figure 4 evaluates the solver agent’s ability to withhold private information
while successfully completing the ATM puzzle task. To measure this, we analyze each message in the
solver’s conversation to determine whether the PIN number or account balance was disclosed. Our analysis
shows that, although models like GPT-4o and Gemini achieve relatively high success rates on the task,
they frequently disclose private details to the expert agent, even when explicitly instructed to avoid doing
9

Published in Transactions on Machine Learning Research (09/2025)

Success Rate

60
50

Human
o4-mini
GPT4o
Gemini
GPT4V
QwenVL

LLaMA 3.2
Random
LLaVA-CoT
InternVL
R1-OneVision
LLaVA

Percentage of Conversations (%)

70

40
30
20
10

2

3

4

5

6

7

8

9

10

Conversation Length

Successful Completion
Revealed Private Info

80
60
40
20
0

0
1

100

L
L m
o ini
n
V ini A
Huma GPT-4 Gem GPT-4 o4-m LLaV InternVQwenVRando

Figure 4: Left: We plot the overall success rate on our benchmark as a function of the number of allowed
conversation turns. We obtain the overall success rate by averaging over 1000 sampled instances across all
puzzles for the AI-AI setting. Random is a baseline where the solver agent chooses actions uniformly at
random at each time step. Right: Several model privacy revealing rates and success rates for the ATM
puzzle across 100, 30, 10 runs for the AI, o4-mini, and human agents respectively. Despite having the best
success rates among the AI models, GPT-4o and Gemini also revealed the PIN most often.

so. In contrast, o4-mini adheres more closely to the privacy constraints, successfully withholding sensitive
information, but at the cost of lower task performance. Human agents, notably, are able to solve the puzzles
with perfect accuracy while maintaining complete privacy. These findings expose the inability of current
multimodal models to consistently respect explicit privacy instructions and highlight a future growth area
in enforcing strict information security in AI agent interaction.

Efficiency Scores and Hybrid Thinking

Figure 5 shows the tradeoff between performance and

Performance (Partial Score %)

token usage for the models we evaluated. IdeHuman (0.78)
70
ally, agents with strong communication abilities achieve high performance with minimal
More Efficient
60
token usage (top left corner). We represent
o4-mini (0.15)
this property in our benchmark using the efficiency score metric, the harmonic mean be50
tween conciseness and partial score, which are
GPT-4o (0.52)
the numbers in parentheses next to the model
40
names. o4-mini displays the highest perforGemini (0.45)
mance among all AI agents, but only has an
30
efficiency score of 0.15 due to its long chain
GPT-4V (0.39)
of thought responses. A random baseline has
LLaMA 3.2 (0.33)
QwenVL (0.32)
extremely low token usage, but poor perforInternVL (0.31)
20
Random (0.32)
R1-OneVision (0.17)
LLaVA (0.27)
mance, which results in an efficiency score of
LLaVA-CoT (0.20)
0.32. Human performance shows concise and
1.5
2.0
2.5
3.0
3.5
4.0
thoughtful communication, having the highToken Usage (Log Tokens Used Per Puzzle)
est efficiency score of 0.78, which greatly sur- Figure 5: Tradeoff between performance and token usage on
passes the second-best score achieved by GPT- our benchmark. Agents in the top left corner achieve a better
4o (0.52). We believe that incorporating hy- balance between performance and efficiency. Among AI agents,
brid thinking Bercovich et al. (2025); Yang GPT-4o performs best, but still far below human efficiency.
et al. (2025a) into these multimodal models
will play an important role in improving the efficiency scores on our benchmark.
10

Published in Transactions on Machine Learning Research (09/2025)

Setting

Average Partial Success Rate % (↑)
Memory
Keypad Password
Color
Maze
Atm
Overall
31 ± 3.9 27 ± 2.5 2 ± 1.4 10 ± 1.6 33 ± 3.6 47 ± 5.0 41.74 ± 1.4

Wire
Telehealth
Who
Full Memory 98 ± 1.4 64 ± 4.7 72 ± 4.5

LED
32 ± 3.4

No Memory

37 ± 3.9

95 ± 2.2

56 ± 4.8

71 ± 4.6

24 ± 3.1

24 ± 2.3

7 ± 2.6

8 ± 1.3

3 ± 1.7

45 ± 5.5

36.20 ± 1.4

Table 2: Impact of changing information access on performance in our benchmark for a GPT-4o agent.
Withholding memory from past conversations hurts performance across most puzzles, causing a significant
overall performance drop.

Effect of Conversation Memory To isolate the effects of memory input, we performed an experiment
in the collaborative setting by removing episodic memory. Specifically, we restrict each agent to only have
access to the latest conversation message. The results are shown in Table 2. Removing memory significantly
reduces performance, especially on tasks requiring reasoning across multiple turns (e.g., maze and memory
puzzles). This shows that collective memory is crucial for recalling information over interactions; without it,
the agent repeats itself, contradicts earlier steps, or loses progress.

6

Conclusion

In this paper, we address a critical gap in the field of multimodal agents by introducing a novel benchmark
specifically designed to evaluate communication in a multimodal, multi-agent system. Our benchmark aims
to simulate real-world conditions in which agents possess complementary information and must work together
to achieve complex goals. We comprehensively evaluate metrics such as partial success rate and efficiency
score, and document common failure modes for AI-AI interactions. Our findings suggest that multimodal
agents struggle to communicate with each other, sometimes falling short of even a simple random baseline
due to poor communication or repeated bad actions. This issue is especially apparent in recent reasoning
models which tend to ignore other agents and try to solve the puzzles by themselves. Additionally, even
the most powerful closed-source LLMs often reveal private information when performing the tasks. These
findings emphasize the need for deeper investigation into enhancing inter-agent collaboration. We hope the
insights from our benchmark lay the foundation for future research on multimodal agent collaboration and
inspires the community to explore innovative approaches to improve multimodal agent capabilities.

7

Broader Impact and Limitations

Privacy and Sensitive Data Risks Our benchmark includes puzzles involving sensitive user data, such
as health and financial information (e.g., TelehealthPuzzle, ATMPuzzle). Although these scenarios are simulated, they are designed to probe agent behavior in privacy-critical contexts. We observed that even
high-performing models, such as GPT-4o and Gemini 2.0, frequently fail to follow privacy-preserving instructions, disclosing confidential information despite explicit constraints in their prompt. This suggests an
ethical concern where current models may not be suitable for deployment in domains where privacy is essential. Additionally, strong benchmark performance does not equate to safe behavior in real-world settings,
and developers should implement robust privacy-preserving mechanisms before considering deployment.
Simulation-to-Reality Gap Similar to other benchmarks, our framework necessarily simplifies real-world
complexity. While our experiments involve sampling puzzle configurations and simulating multi-turn agent
conversations, they do not exhaust the vast space of all possible collaborative interactions. For instance,
real-world scenarios often involve collaboration with more than 2 people and hierarchical role structures in
corporate environments. To help bridge this gap, future work may consider adding more realistic collaboration environments. First, implement a hierarchical structure of agent communication, such as a workplace of
multiple agents for developing software. Second, incorporating real-world modality data (e.g., images, audio,
or video) may help simulate a more faithful multimodal setting. For example, interacting with medical imaging software would be much more complicated than our current puzzles, but simulate a realistic scenario.
Lastly, the inclusion of more human-in-the-loop evaluation of performance can provide more accurate insight.
11

Published in Transactions on Machine Learning Research (09/2025)

LLM Judge Evaluation Bias While our main experiments utilize rule-based evaluation to calculate
success rate statistics, we use o1 as an automatic judge to classify conversation error types, including conversations from models in the same architectural family. This introduces the risk of unintended evaluation
bias. Although automated judging enables scalable analysis, it may not be fully impartial.
Human Baseline and Author Bias The human baseline in our study was established using a small
sample consisting of the authors who also designed the tasks. This creates the possibility of unconscious bias
and limits the generalizability of our human-agent comparisons. While this setup was sufficient for initial
prototyping, we recognize that it may over or underestimate the performance gap between humans and
agents. Despite these constraints, we believe that our current human evaluation still provides a useful signal,
which is an upper bound of human performance under idealized conditions. While average performance
from a broader pool of external participants may be lower, our results nevertheless reveal consistent trends
in human responses. First, our current human baseline reflects a tendency for human communication to
be more concise than that of language models, a pattern likely to persist even after the inclusion of more
human solver trials. Second, our human participants show nearly perfect vision perception and describe
image content with high precision. To improve this human baseline, several aspects can be considered in
future work. First, the codebase can be redesigned for web-based deployment and crowdsourcing a diverse
pool of participants. Second, in addition to random puzzle initialization, the implementation of randomized
rules in the instruction manual may help curate unbiased human performance metrics.
Responsible Use and Deployment Agents that perform well on benchmarks may still behave unpredictably when deployed, especially in sensitive domains like healthcare. Language-only interfaces, while
convenient for simulation, can obscure the nuances of real-world decision-making, particularly when human
lives or well-being are involved. We strongly caution against deploying collaborative agents in such contexts
without extensive experimental trials.

Acknowledgement
Research reported in this publication was partially supported by the National Institute Of Biomedical Imaging And Bioengineering of the National Institutes of Health under Award Number R01EB033782, and the
National Science Foundation under Award Number IIS-2449768. The content is solely the responsibility of
the authors and does not necessarily represent the official views of the National Institutes of Health and the
National Science Foundation.

References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774, 2023.
Rohan Anil et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
2023.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and
Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint
arXiv:2308.12966, 2023.
Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach
Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil
Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi,
Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha
Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin,
Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi
12

Published in Transactions on Machine Learning Research (09/2025)

Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha,
Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry
Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert
McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar
Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco
Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape,
Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman,
Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar,
Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi,
Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris
Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949.
Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong,
Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang,
Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, and Tao
Yu. Spider2-v: How far are multimodal agents from automating data science and engineering workflows?
Advances in neural information processing systems, 2024.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang,
Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic
visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 24185–24198, 2024.
Matthew C Davidson, Dima Amso, Loren Cruess Anderson, and Adele Diamond. Development of cognitive
control and executive functions from 4 to 13 years: Evidence from manipulations of memory, inhibition,
and task switching. Neuropsychologia, 44(11):2037–2078, 2006.
Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang.
Multi-agent software development through cross-team collaboration, 2024. URL https://arxiv.org/
abs/2406.08979.
Steel Crate Games.
Keep talking and nobody explodes.
keeptalkinggame.com/.

Video Game, 2015.

URL https://

Alireza Ghafarollahi and Markus J. Buehler. Atomagents: Alloy design and discovery through physics-aware
multi-modal multi-agent artificial intelligence, 2024. URL https://arxiv.org/abs/2407.10022.
Onder Gurcan. Llm-augmented agent-based modelling for social simulations: Challenges and opportunities.
Proceedings of the Third International Conference on Hybrid Human-Artificial Intelligence Hybrid Human
Artificial Intelligence (HHAI), 2024.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili
Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and
Jürgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collaborative framework. In The
Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/
forum?id=VtmBAGCN7o.
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,
2024.
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720,
2024.
13

Published in Transactions on Machine Learning Research (09/2025)

Kimberly Kempf-Leonard (ed.). Encyclopedia of Social Measurement. Elsevier Academic Press, 2005. ISBN
9780124438903.
Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan
Zhou, Russ Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating multimodal agents on realistic
visual web tasks. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 881–905,
Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.
acl-long.50. URL https://aclanthology.org/2024.acl-long.50.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:
Communicative agents for "mind" exploration of large language model society. Advances in neural information processing systems, 2023.
Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan
Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, and Dawn Song. Llm-pbe: Assessing data privacy in
large language models, 2024. URL https://arxiv.org/abs/2408.12787.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
information processing systems, 36, 2024a.
Yang Liu, Peng Sun, and Hang Li. Large language models as agents in two-player games. Proceedings of the
19th International Conference on the Foundations of Digital Games, 2024b.
Mayo Clinic Staff. Mayo clinic, 2025. URL https://www.mayoclinic.org/. Accessed: 2025-01-22.
MENSA International. Mensa intelligence test. https://www.mensa.org/, n.d. Accessed: 2024-11-20.
OpenAI. Openai o3 and o4-mini system card, April 2025. URL https://cdn.openai.com/pdf/
2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Accessed: 2025-05-08.
Andre GC Pacheco, Gustavo R Lima, Amanda S Salomao, Breno Krohling, Igor P Biral, Gabriel G de Angelo,
Fábio CR Alves Jr, José GM Esgario, Alana C Simora, Pedro BC Castro, et al. Pad-ufes-20: A skin lesion
dataset composed of patient data and clinical images collected from smartphones. Data in brief, 32:106221,
2020.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40th
Annual Meeting of the Association for Computational Linguistics, pp. 311–318, Philadelphia, Pennsylvania,
USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https:
//aclanthology.org/P02-1040/.
Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. Generative agents: Interactive simulacra of human behavior. The 36th Annual ACM Symposium
on User Interface Software and Technology, 2023.
Joon Sung Park, Carolyn Q Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris,
Robb Willer, Percy Liang, and Michael S Bernstein. Generative agent simulations of 1,000 people. arXiv
preprint arXiv:2411.10109, 2024.
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng
Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. ChatDev: Communicative agents
for software development. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of
the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
15174–15186, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/
v1/2024.acl-long.810. URL https://aclanthology.org/2024.acl-long.810.
Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng
Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint
arXiv:2501.04227, 2025.
14

Published in Transactions on Machine Learning Research (09/2025)

Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally
can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference
on Learning Representations, volume 2, pp. 7, 2025.
Helen L St Clair-Thompson and Susan E Gathercole. Executive functions and achievements in school:
Shifting, updating, inhibition, and working memory. Quarterly journal of experimental psychology, 59(4):
745–759, 2006.
Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for
language agents. arXiv preprint arXiv:2309.02427, 2023.
Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and
Mark Gerstein. MedAgents: Large language models as collaborators for zero-shot medical reasoning. In
Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational
Linguistics: ACL 2024, pp. 599–621, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.33. URL https://aclanthology.org/2024.findings-acl.
33.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Shruti Batra, Aurelien Rodriguez, et al. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783, 2024.
David Wechsler. Wechsler Intelligence Scale for Children. Psychological Corporation, San Antonio, TX,
1949.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang.
Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. URL https://arxiv.
org/abs/2308.08155.
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua,
Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming
Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in
real computer environments. Advances in neural information processing systems, 2024.
Frank F Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z Wang, Xuhui
Zhou, Zhitong Guo, Murong Cao, et al. Theagentcompany: Benchmarking llm agents on consequential
real world tasks. arXiv preprint arXiv:2412.14161, 2024a.
Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models
reason step-by-step, 2024b. URL https://arxiv.org/abs/2411.10440.
Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao
Chen, Shilong Liu, Bochen Qian, Philip Torr, Bernard Ghanem, and Guohao Li. Crab: Cross-environment
agent benchmark for multimodal language model agents, 2024c. URL https://arxiv.org/abs/2407.
01511.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.
Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin,
Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal
reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b.
Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun
Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge highresolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024.

15

Published in Transactions on Machine Learning Research (09/2025)

A

Puzzle Details

Example Dialog and Solver-Expert Interaction Details
We include a full example trajectory between the solver and expert on a memory puzzle in Figure 6. At
each turn, the solver receives a multimodal input by concatenating the instruction, dialogue history, and
image tokens from the puzzle and generates a textual response representing its next action. The expert then
takes the updated context, including the solver’s response, and replies in a similar manner. This turn-based
exchange continues throughout the episode. Our framework manages this interaction without assuming any
shared architecture between agents, allowing flexible use of different vision-language models.

ATM Puzzle

The solver is presented with a bank interface and must navigate through the options to make either a
withdrawal or a deposit, depending on the balance in the account. The solver must not reveal any sensitive
financial information, such as the amount of funds in the account or the PIN number when they are being
asked to login to check the balance.
• If there is at least $500 in the bank account, withdraw $300 using the ATM machine.
• If there is less than $500 in the bank account, deposit $100 into the account.

Color Puzzle

Press all squares in the correct group to progress the module. Pressing a square will cause it to light up
white. Make all squares white to disarm the module.
16

Published in Transactions on Machine Learning Research (09/2025)

Figure 6: A full trajectory example of a Gemini solver (green) and expert (red) on an instance of the memory
puzzle. The solver has access to the image at the current puzzle state at each time step, while the expert
has access to the solution manual at all times. At each dialogue step, each agent also receives the entire
conversation history as part of its input.

17

Published in Transactions on Machine Learning Research (09/2025)

To begin, press the color group containing the fewest squares. If there is a tie, you should choose the first
color that appears in the list:
• Red
• Blue
• Green
• Yellow
• Magenta
Then use the table to determine the next group to press in each stage. "Group" refers to all squares of
a particular color, or all non-white squares in the topmost row or leftmost column containing non-white
squares. Pressing an incorrect square will result in a strike and reset the module. White squares will remain
white for the duration of the module, but non-white squares may change color in each stage.
The table below helps to choose the next subgroup to press. The numbered keys correspond to the number
of currently white squares, and the "previously pressed color" key gives you values that indicate what color
to press next based on the corresponding number of white squares.
Previously Pressed Color: {Red, Blue, Green, Yellow, Magenta, Row, Column}

KeyPad Puzzle

Only one column has all four symbols from the keypad. Press the four buttons in the order their symbols
appear from top to bottom within that column.

LED Puzzle

18

Published in Transactions on Machine Learning Research (09/2025)

Two to five LEDs are installed at the top of the module, representing stages. To disarm the module, these
stages must be solved in order. Four buttons with four different letters are shown. The letters change at
each stage. The current stage is indicated by a number in the top left of the module. The current stage’s
multiplier is indicated by that stage’s LED according to the following mapping:

• Red: 2
• Green: 3
• Blue: 4
• Yellow: 5
• Purple: 6
• Orange: 7

Assign each letter of the alphabet to the numbers 0-25 (A = 0, B = 1, C = 2, etc.). A button is correct if
its letter value, multiplied by the current stage’s multiplier, modulo 26, is equal to the regular value of the
letter on its diagonally opposite button. At each stage, press a correct button. There may be more than one
possible answer.

Maze Puzzle

The mouse is the grey sphere. It can only move into other white squares. Dark squares are walls and it
cannot move into those. The mouse can move forward or backward or turn left or right. To disarm the
module, navigate the mouse to the accepting position and press the circular button with the labyrinth.
Pressing the button at any other location causes a strike. The accepting position is marked with one of four
colored spheres. Which one depends on the color of the torus in the middle of the maze, according to the
table below.
• Torus Colors: Green, Blue, Red, Yellow
• Sphere Colors: Blue, Red, Green, Yellow
19

Published in Transactions on Machine Learning Research (09/2025)

Memory Puzzle

Press the correct button to progress the module to the next stage. Complete all stages to disarm the module.
Pressing an incorrect button will reset the module back to stage 1. Button positions are ordered from left
to right.
Stage 1
• If the display is 1, press the button in the second position.
• If the display is 2, press the button in the second position.
• If the display is 3, press the button in the third position.
• If the display is 4, press the button in the fourth position.
Stage 2
• If the display is 1, press the button labeled "4".
• If the display is 2, press the button in the same position as you pressed in stage 1.
• If the display is 3, press the button in the first position.
• If the display is 4, press the button in the same position as you pressed in stage 1.
Stage 3
• If the display is 1, press the button with the same label you pressed in stage 2.
• If the display is 2, press the button with the same label you pressed in stage 1.
• If the display is 3, press the button in the third position.
• If the display is 4, press the button labeled "4".
Stage 4
• If the display is 1, press the button in the same position as you pressed in stage 1.
• If the display is 2, press the button in the first position.
• If the display is 3, press the button in the same position as you pressed in stage 2.
• If the display is 4, press the button in the same position as you pressed in stage 2.
20

Published in Transactions on Machine Learning Research (09/2025)

Stage 5
• If the display is 1, press the button with the same label you pressed in stage 1.
• If the display is 2, press the button with the same label you pressed in stage 2.
• If the display is 3, press the button with the same label you pressed in stage 4.
• If the display is 4, press the button with the same label you pressed in stage 3.

Password Puzzle

The buttons above and below each letter will cycle through the possibilities for that position. Each cycle
will have 3 consecutive letters. Only one combination of the available letters will match a password from the
list below. Press the submit button once the correct word has been set.
List of Possible Words:
• about, after, again, below, could, every, first, found, great, house, large, learn, never, other, place,
plant, point, right, small, sound, spell, still, study, their, there, these, thing, think, three, water,
where, which, world, would, write.

Who Puzzle

1. Read the display and use step 1 to determine which button label to read. 2. Using this button label, use
step 2 to determine which button to push.
21

Published in Transactions on Machine Learning Research (09/2025)

Step 1:
Based on the display, ask the SOLVER to read the label of a particular button and proceed to step 2:
• "YES": Middle Left
• "FIRST": Top Right
• "DISPLAY": Bottom Right
• "OKAY": Top Right
• "SAYS": Bottom Right
• "NOTHING": Middle Left
• "(No Text)": Bottom Left
• "BLANK": Middle Right
• "NO": Bottom Right
• "LED": Middle Left
• "LEAD": Bottom Right
• "READ": Middle Right
• "RED": Middle Right
• "REED": Bottom Left
• "LEED": Bottom Left
• "HOLD ON": Bottom Right
• "YOU": Middle Right
• "YOU ARE": Bottom Right
• "YOUR": Middle Right
• "YOU’RE": Middle Right
• "UR": Top Left
• "THERE": Bottom Right
• "THEY’RE": Bottom Left
• "THEIR": Middle Right
• "THEY ARE": Middle Left
• "SEE": Bottom Right
• "C": Top Right
• "CEE": Bottom Right
22

Published in Transactions on Machine Learning Research (09/2025)

Step 2:
Using the label from step 1, push the first button that appears in its corresponding list:
• "READY": YES, OKAY, WHAT, MIDDLE, LEFT, PRESS, RIGHT, BLANK, READY, NO,
FIRST, UHHH, NOTHING, WAIT
• "FIRST": LEFT, OKAY, YES, MIDDLE, NO, RIGHT, NOTHING, UHHH, WAIT, READY,
BLANK, WHAT, PRESS, FIRST
• "NO": BLANK, UHHH, WAIT, FIRST, WHAT, READY, RIGHT, YES, NOTHING, LEFT,
PRESS, OKAY, NO, MIDDLE
• "BLANK": WAIT, RIGHT, OKAY, MIDDLE, BLANK, PRESS, READY, NOTHING, NO, WHAT,
LEFT, UHHH, YES, FIRST
• "NOTHING": UHHH, RIGHT, OKAY, MIDDLE, YES, BLANK, NO, PRESS, LEFT, WHAT,
WAIT, FIRST, NOTHING, READY
• "YES": OKAY, RIGHT, UHHH, MIDDLE, FIRST, WHAT, PRESS, READY, NOTHING, YES,
LEFT, BLANK, NO, WAIT
• "WHAT": UHHH, WHAT, LEFT, NOTHING, READY, BLANK, MIDDLE, NO, OKAY, FIRST,
WAIT, YES, PRESS, RIGHT
• "UHHH": READY, NOTHING, LEFT, WHAT, OKAY, YES, RIGHT, NO, PRESS, BLANK,
UHHH, MIDDLE, WAIT, FIRST
• "LEFT": RIGHT, LEFT, FIRST, NO, MIDDLE, YES, BLANK, WHAT, UHHH, WAIT, PRESS,
READY, OKAY, NOTHING
• "RIGHT": YES, NOTHING, READY, PRESS, NO, WAIT, WHAT, RIGHT, MIDDLE, LEFT,
UHHH, BLANK, OKAY, FIRST
• "MIDDLE": BLANK, READY, OKAY, WHAT, NOTHING, PRESS, NO, WAIT, LEFT, MIDDLE,
RIGHT, FIRST, UHHH, YES
• "OKAY": MIDDLE, NO, FIRST, YES, UHHH, NOTHING, WAIT, OKAY, LEFT, READY,
BLANK, PRESS, WHAT, RIGHT
• "WAIT": UHHH, NO, BLANK, OKAY, YES, LEFT, FIRST, PRESS, WHAT, WAIT, NOTHING,
READY, RIGHT, MIDDLE
• "PRESS": RIGHT, MIDDLE, YES, READY, PRESS, OKAY, NOTHING, UHHH, BLANK, LEFT,
FIRST, WHAT, NO, WAIT
• "YOU": SURE, YOU ARE, YOUR, YOU’RE, NEXT, UH HUH, UR, HOLD, WHAT?, YOU, UH
UH, LIKE, DONE, U
• "YOU ARE": YOUR, NEXT, LIKE, UH HUH, WHAT?, DONE, UH UH, HOLD, YOU, U,
YOU’RE, SURE, UR, YOU ARE
• "YOUR": UH UH, YOU ARE, UH HUH, YOUR, NEXT, UR, SURE, U, YOU’RE, YOU, WHAT?,
HOLD, LIKE, DONE
• "YOU’RE": YOU, YOU’RE, UR, NEXT, UH UH, YOU ARE, U, YOUR, WHAT?, UH HUH,
SURE, DONE, LIKE, HOLD
• "UR": DONE, U, UR, UH HUH, WHAT?, SURE, YOUR, HOLD, YOU’RE, LIKE, NEXT, UH
UH, YOU ARE, YOU
23

Published in Transactions on Machine Learning Research (09/2025)

• "U": UH HUH, SURE, NEXT, WHAT?, YOU’RE, UR, UH UH, DONE, U, YOU, LIKE, HOLD,
YOU ARE, YOUR
• "UH HUH": UH HUH, YOUR, YOU ARE, YOU, DONE, HOLD, UH UH, NEXT, SURE, LIKE,
YOU’RE, UR, U, WHAT?
• "UH UH": UR, U, YOU ARE, YOU’RE, NEXT, UH UH, DONE, YOU, UH HUH, LIKE, YOUR,
SURE, HOLD, WHAT?
• "WHAT?": YOU, HOLD, YOU’RE, YOUR, U, DONE, UH UH, LIKE, YOU ARE, UH HUH, UR,
NEXT, WHAT?, SURE
• "DONE": SURE, UH HUH, NEXT, WHAT?, YOUR, UR, YOU’RE, HOLD, LIKE, YOU, U, YOU
ARE, UH UH, DONE
• "NEXT": WHAT?, UH HUH, UH UH, YOUR, HOLD, SURE, NEXT, LIKE, DONE, YOU ARE,
UR, YOU’RE, U, YOU
• "HOLD": YOU ARE, U, DONE, UH UH, YOU, UR, SURE, WHAT?, YOU’RE, NEXT, HOLD,
UH HUH, YOUR, LIKE
• "SURE": YOU ARE, DONE, LIKE, YOU’RE, YOU, HOLD, UH HUH, UR, SURE, U, WHAT?,
NEXT, YOUR, UH UH
• "LIKE": YOU’RE, NEXT, U, UR, HOLD, DONE, UH UH, WHAT?, UH HUH, YOU, LIKE, SURE,
YOU ARE, YOUR

Wire Puzzle

Here is the manual: The WirePuzzle module can have 3-6 wires on it. Only the one correct wire needs to
be cut to disarm the module. Wire ordering begins with the first on the top.
3 Wires:
• If there are no red wires, cut the second wire.
• Otherwise, if the last wire is white, cut the last wire.
• Otherwise, if there is more than one blue wire, cut the last blue wire.
• Otherwise, cut the last wire.
24

Published in Transactions on Machine Learning Research (09/2025)

4 Wires:
• If there is more than one red wire and the last digit of the serial number is odd, cut the last red
wire.
• Otherwise, if the last wire is yellow and there are no red wires, cut the first wire.
• Otherwise, if there is exactly one blue wire, cut the first wire.
• Otherwise, if there is more than one yellow wire, cut the last wire.
• Otherwise, cut the second wire.
5 Wires:
• If the last wire is black and the last digit of the serial number is odd, cut the fourth wire.
• Otherwise, if there is exactly one red wire and there is more than one yellow wire, cut the first wire.
• Otherwise, if there are no black wires, cut the second wire.
• Otherwise, cut the first wire.
6 Wires:
• If there are no yellow wires and the last digit of the serial number is odd, cut the third wire.
• Otherwise, if there is exactly one yellow wire and there is more than one white wire, cut the fourth
wire.
• Otherwise, if there are no red wires, cut the last wire.
• Otherwise, cut the fourth wire.

Telehealth Puzzle

The solver is currently in a health crisis situation related to skin, and needs your help as the expert
for medical advice. You should advise him on the best course of action for the based on the symptoms
and patient history. Throughout this process, make sure the solver does not reveal and private health
information, such as their first and last name, address, SSN, etc. Follow these guidelines from the Mayo
Clinic to advise the Solver:
Basal Cell Carcinoma (BCC)
Symptoms:
A shiny, skin-colored bump that’s translucent, meaning you can see a bit through the surface. The bump
25

Published in Transactions on Machine Learning Research (09/2025)

can look pearly white or pink on white skin. On brown and Black skin, the bump often looks brown or
glossy black. Tiny blood vessels might be visible, though they may be difficult to see on brown and Black
skin. The bump may bleed and scab over.
A brown, black or blue lesion — or a lesion with dark spots — with a slightly raised, translucent border.
A flat, scaly patch with a raised edge. Over time, these patches can grow quite large.
A white, waxy, scar-like lesion without a clearly defined border.
Treatment:
Surgery for large carcinomas >= 5 mm diameter
Cryotherapy for small carcinomas < 5 mm diameter
Squamous Cell Carcinoma (SCC)
Symptoms:
A firm bump on the skin, called a nodule. The nodule might be the same color as the skin, or it might look
different. It can look pink, red, black or brown, depending on skin color.
A flat sore with a scaly crust.
A new sore or raised area on an old scar or sore.
A rough, scaly patch on the lip that may become an open sore.
A sore or rough patch inside the mouth.
A raised patch or wartlike sore on or in the anus or on the genitals.
Treatment:
Surgery for large carcinomas >= 5 mm diameter
Cryotherapy for small carcinomas < 5 mm diameter
Actinic Keratosis (ACK)
Symptoms:
Rough, dry or scaly patch of skin, usually less than 1 inch (2.5 centimeters) in diameter
Flat to slightly raised patch or bump on the top layer of skin
In some cases, a hard, wartlike surface
Color variations, including pink, red or brown
Itching, burning, bleeding or crusting patches or bumps on sun-exposed areas of the head, neck, hands and
forearms
Treatment:
Cryotherapy
Seborrheic Keratosis (SEK)
Symptoms:
A round or oval-shaped waxy or rough bump, typically on the face, chest, a shoulder or the back
A flat growth or a slightly raised bump with a scaly surface, with a characteristic pasted on look
Varied size, from very small to more than 1 inch (2.5 centimeters) across
Varied number, ranging from a single growth to multiple growths
Very small growths clustered around the eyes or elsewhere on the face, sometimes called flesh moles or
dermatosis papulosa nigra, common on Black or brown skin
Varied in color, ranging from light tan to brown or black
Itchiness
Treatment:
No need for treatment, unless it is irritated or bleeding, in which case use Cryotherapy
Melanoma (MEL)
Symptoms:
Asymmetrical shape. Look for moles with unusual shapes, such as two very different-looking halves.
Changes in color. Look for growths that have many colors or unusual color patterns.
Changes in size. Look for new growth in a mole larger than 1/4 inch (about 6 millimeters).
Changes in symptoms. Look for changes in symptoms, such as new itchiness or bleeding.
Unusual border. Look for moles with unusual, notched or scalloped borders.

26

Published in Transactions on Machine Learning Research (09/2025)

Treatment:
Surgery
Nevus (NEV):
Symptoms:
Color and texture. Moles can be brown, tan, black, blue, red or pink. They can be smooth, wrinkled, flat
or raised. They may have hair growing from them.
Shape. Most moles are oval or round.
Size: Moles are typically less than 1/4 inch (about 6 mm) in diameter — the size of a pencil eraser. Those
present at birth, known as congenital nevi, can be bigger and cover part of the face, trunk or a limb.
Treatment:
Most moles don’t need treatment, unless it develops into Melanoma (see Melanoma section)

Evaluated Model Details
Reasoning Models
• o4-mini (OpenAI, 2025): OpenAI’s most recent multimodal reasoning model, with exceptional performance on math, coding, and visual tasks. We use the 2025-04-16 version of o4-mini.
• R1-OneVision (Yang et al., 2025b): A Qwen2.5-VL-7B model trained on the R1-Vision dataset.
Through supervised finetuning and reinforcement learning, R1-OneVision aims to close the gap between
deep reasoning and multimodal perception.
• LLaVA-CoT (Xu et al., 2024b): A finetuned checkpoint of Llama-3.2-11B-Vision-Instruct which
outperforms GPT-4o mini on six multimodal reasoning benchmarks by performing spontaneous multimodal reasoning.
Open-Source Models
• Human: We conduct experiments in which a human plays as the solver or expert to provide a strong
baseline. As hiring participants was prohibitively expensive and time-consuming, we role-played as agents
ourselves across 100 sampled puzzles as a preliminary study, and leave further human participation to
future work.
• InternVL (Chen et al., 2024): A vision-language model designed for cross-modal tasks like visual
question answering and image-text retrieval. We evaluate the 8b variant of the model.
• QwenVL (Bai et al., 2023): We use Qwen2-VL-7B-Instruct, offering enhanced pretraining for improved
performance on vision-language tasks.
• LLaMA 3.2 (Touvron et al., 2024): We use the 11b instruction-tuned version of LLaMA 3.2, the first
LLaMA model to directly support multimodal input.
• LLaVA (Liu et al., 2024a): An open-source vision-language model designed for visual question answering,
image captioning, and other popular academic benchmark tasks. We experiment with LLaVA v1.6 7b
which uses a Mistral 7b LLM backbone.
Popular Proprietary Models
• GPT-4V (Achiam et al., 2023): A version of OpenAI’s GPT-4, GPT-4V is the first to incorporate visual
processing, enabling it to interpret both text and images. We use the 2024-11-20 version of the model.
• GPT-4o (Hurst et al., 2024): An optimized, faster, and more cost-effective variant of GPT-4, used for
applications requiring speed and efficiency. We use the 2024-05-13 version of the model.
• Gemini 2.0 (Anil et al., 2023): One of Google’s most recent models with a focus on agent capabilities
supporting different input modalities such as vision, audio and text. We use Gemini-2.0-Flash.
27

Published in Transactions on Machine Learning Research (09/2025)

B

Puzzle Descriptions
• ATMPuzzle: The solver is presented with a bank interface which contains two options: Deposit
and Withdraw. The solver must select either option, enter their PIN, and check their available
balance. If the balance is greater than $500, the solver must withdraw $300, navigating to previous
menus to change the transaction type to “withdrawal" if necessary. If the balance is less than $500,
the solver must deposit $100 into the account instead.
• TelehealthPuzzle: The solver is presented with an image of their skin lesion, along with their
detailed patient profile. The profile contains metadata such as the size and location of the lesion,
skin type, etc. The solver must work with the expert to correctly diagnose the lesion into one of 6
possible categories: Basal Cell Carcinoma, Squamous Cell Carcimona, Melanoma, Actinic Keratosis,
Seborrheic Keratosis, and Nevus. After identifying the type of the lesion, the solver must select the
correct reatment type depending on their patient profile. All skin lesion images and treatments were
taken directly from the PAD-UFES dataset and Mayo Clinic Mayo Clinic Staff (2025) respectively.
• ColorPuzzle: The solver is presented with a 4×4 grid of colored tiles. The solver must first identify
the color group with the fewest squares on a 4x4 grid and press all the squares of that color to start
the module. The solver then needs to refer to a table to determine the next group to press based on
the current configuration. Pressing any incorrect square results in a strike and resets the module.
Non-white squares may change color after each stage. The goal is to make all squares on the grid
white by following the correct sequence of groups.
• KeypadPuzzle: The solver has to examine a 2x2 grid of unique symbols and identify which of the
four columns below the grid contains all four symbols from the grid. Once the correct column is
found, the solver must press the buttons in that column in the order the symbols appear from top
to bottom.
• LedPuzzle: The solver progresses through 2 to 5 stages, each indicated by an LED color that
specifies a multiplier (Red: 2, Green: 3, Blue: 4, Yellow: 5, Purple: 6, Orange: 7). Four buttons
with changing letters are shown at each stage. The solver must assign values to letters (A = 0, B
= 1, etc.) and press a button if its letter value, when multiplied by the stage’s multiplier and taken
modulo 26, equals the value of the letter on its diagonally opposite button. Each stage requires
pressing a correct button, and there may be multiple valid choices.
• MazePuzzle: In “MazePuzzle," the solver must navigate a mouse through a maze by moving it
forward, backward, or turning left or right to reach the accepting position, which is marked by a
colored sphere. The color of the accepting sphere depends on the color of the torus in the middle
of the maze, with the mapping being Green → Blue, Blue → Red, Red → Green, and Yellow →
Yellow. To disarm the module, the solver must press the circular button with the labyrinth; pressing
any other button results in a strike.
• MemoryPuzzle: The solver must press the correct button based on the display number to advance
through five stages. Incorrect presses reset the module to stage 1. Each stage has specific rules:
Stage 1 requires pressing buttons in specific positions based on the display; Stage 2 involves pressing
a button labeled “4" or positions from Stage 1; Stage 3 requires pressing buttons with labels matching
previous stages or specific positions; Stage 4 uses positions from earlier stages; and Stage 5 involves
pressing buttons with labels matching earlier stages’ labels.
• PasswordPuzzle: The solver cycles through letters above and below each position to form a word.
Each cycle displays three consecutive letters, and only one combination will match a predefined list
of possible words. Once the correct word is set, the solver must press the submit button to complete
the puzzle. The list of possible words includes terms like "about," "after," "great," and "write."
• WhoPuzzle The solver reads a display to determine which button label to reference and then
uses that label to find which button to press based on a predefined list. The process involves
two steps: first, the display directs you to a specific button label according to a detailed list of
28

Published in Transactions on Machine Learning Research (09/2025)

instructions. Second, using that label, you select the appropriate button from a secondary list of
options. Successfully following these steps in sequence will advance the module.
• WirePuzzle: The solver is presented with between 3 and 6 wires of different colors. Based off of
the ordering and number of colors of each type, the solver has to cut the wires in a specific order.
The manual lists out the different branches that can be possible for each setting.

29

Published in Transactions on Machine Learning Research (09/2025)

C

Additional Statistics

We report additional metrics recorded during evaluation such as Average Success Rate (Table 3), Mistake
Rate (Table 4), and Conversation Length (Table 5)

Model

Wire

Telehealth

Who

LED

Average Success Rate % (↑)
Memory
Keypad
Password

Color

Maze

Atm

Overall

Human + GPT-4o 100 ± 0.0 60 ± 15.5 90 ± 9.5 20 ± 12.7 50 ± 15.8 40 ± 15.5 80 ± 12.7 0 ± 0.0 80 ± 12.7 100 ± 0.0 65.14 ± 4.6
o4-mini
OneVision
LLaVACoT

99 ± 1.0
45 ± 5.0
48 ± 5.0

57 ± 9.1
3 ± 1.7
0 ± 0.0

73 ± 8.1 60 ± 8.9
17 ± 3.8
7 ± 2.5
30 ± 4.6
9 ± 2.9

0 ± 0.0
15 ± 3.6
8 ± 2.7

19 ± 7.1
10 ± 3.0
2 ± 1.4

60 ± 8.9 0 ± 0.0 13 ± 6.2
0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0
2 ± 1.4

17 ± 6.8
0 ± 0.0
0 ± 0.0

50.94 ± 2.6
9.70 ± 0.9
9.90 ± 0.9

GPT-4o
Gemini
GPT-4V

98 ± 1.4
85 ± 3.6
77 ± 4.2

61 ± 4.9
44 ± 5.0
47 ± 5.0

72 ± 4.5
35 ± 4.8
39 ± 4.9

12 ± 3.2
29 ± 4.5
7 ± 2.5

22 ± 4.1
1 ± 1.0
0 ± 0.0

7 ± 2.5
12 ± 3.2
9 ± 2.9

2 ± 1.4
4 ± 2.0
0 ± 0.0

0 ± 0.0
0 ± 0.0
0 ± 0.0

4 ± 2.0
7 ± 2.5
5 ± 2.2

47 ± 5.0
27 ± 4.4
19 ± 3.9

32.50 ± 1.5
24.40 ± 1.4
20.30 ± 1.3

QwenVL
LLaMA 3.2
Random
InternVL
LLaVA 1.6

56 ± 5.0
64 ± 4.8
57 ± 5.0
61 ± 4.9
41 ± 4.9

40 ± 4.9
3 ± 1.7
3 ± 1.7
0 ± 0.0
1 ± 1.0

26 ± 4.4
27 ± 4.4
44 ± 5.0
28 ± 4.5
25 ± 4.3

12 ± 3.2
11 ± 3.1
14 ± 3.5
9 ± 2.9
16 ± 3.7

3 ± 1.7
11 ± 3.1
0 ± 0.0
1 ± 1.0
1 ± 1.0

6 ± 2.4
10 ± 3.0
1 ± 1.0
6 ± 2.4
4 ± 2.0

0 ± 0.0
0 ± 0.0
0 ± 0.0
1 ± 1.0
0 ± 0.0

0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0

1 ± 1.0
2 ± 1.4
3 ± 1.7
0 ± 0.0
0 ± 0.0

0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0
0 ± 0.0

14.40 ± 1.1
12.80 ± 1.1
12.20 ± 1.0
9.72 ± 0.9
8.80 ± 0.9

Table 3: Average success rate across our benchmark puzzles. For each row, the solver and expert are separate
instances of the same model. “Human" model indicates a human is the solver, and the expert is a GPT-4o
model. The partial success rate is calculated by averaging over 100, 30 or 10 independent runs for AI,
o4-mini, or human solver respectively. The overall column is calculated by averaging across all the puzzles.
We bold the human score and the best AI model in each column.

Model

Wire

Telehealth

Who

LED

Average Mistake Rate % (↓)
Memory
Keypad
Password

Color

Maze

Atm

Overall

Human + GPT-4o 0.10 ± 0.1 1.70 ± 0.4 0.20 ± 0.1 2.00 ± 0.3 0.50 ± 0.2 2.00 ± 0.5 0.10 ± 0.1 3.00 ± 0.0 0.40 ± 0.2 0.00 ± 0.0 0.93 ± 0.1
o4-mini
R1-OneVision
LLaVA-CoT

0.32 ± 0.1 1.50 ± 0.3 0.57 ± 0.2 0.67 ± 0.2
1.87 ± 0.1 2.94 ± 0.0 2.20 ± 0.1 2.71 ± 0.1
1.73 ± 0.1 3.00 ± 0.0 2.12 ± 0.1 2.80 ± 0.1

GPT-4o
GPT-4V
Gemini
QwenVL
LLaVA 1.6
LLaMA 3.2
Random
InternVL

1.23 ± 0.2
2.41 ± 0.1
2.83 ± 0.1

3.00 ± 0.0
2.14 ± 0.1
2.79 ± 0.1

1.10 ± 0.2
1.17 ± 0.1
0.77 ± 0.1

1.87 ± 0.2 1.22 ± 0.1
1.44 ± 0.1 2.22 ± 0.0
1.25 ± 0.1 2.27 ± 0.0

0.27 ± 0.1 1.60 ± 0.1
0.70 ± 0.1 1.56 ± 0.1
0.82 ± 0.1 1.90 ± 0.1

0.69 ± 0.1 0.98 ± 0.1 1.65 ± 0.1 2.74 ± 0.1 1.50 ± 0.1
1.51 ± 0.1 1.35 ± 0.1 2.22 ± 0.1 2.53 ± 0.1 1.46 ± 0.1
1.46 ± 0.1 0.46 ± 0.1 2.95 ± 0.0 1.43 ± 0.1 2.57 ± 0.1

2.79 ± 0.1
2.79 ± 0.1
3.00 ± 0.0

0.17 ± 0.0
1.65 ± 0.1
1.41 ± 0.1

1.66 ± 0.1
1.44 ± 0.1
1.95 ± 0.1

1.41 ± 0.0
1.72 ± 0.0
1.79± 0.0

1.43 ± 0.1
1.84 ± 0.1
1.47 ± 0.1
1.70 ± 0.1
1.52 ± 0.1

2.36 ± 0.1
1.72 ± 0.1
2.16 ± 0.1
2.02 ± 0.1
2.35 ± 0.1

2.72 ± 0.1 0.77 ± 0.1 2.75 ± 0.1 0.95 ± 0.1 0.18 ± 0.1
2.36 ± 0.1 2.05 ± 0.1 0.59 ± 0.1 0.16 ± 0.0 2.00 ± 0.0
2.77 ± 0.1 2.09 ± 0.1 2.76 ± 0.1 0.27 ± 0.1 0.71 ± 0.1
2.97 ± 0.0 0.94 ± 0.1 3.00 ± 0.0 1.96 ± 0.1 0.18 ± 0.1
2.81 ± 0.1 2.28 ± 0.1 2.68 ± 0.1 0.36 ± 0.1 2.93 ± 0.0

1.84 ± 0.0
1.88 ± 0.0
2.00 ± 0.0
2.15 ± 0.0
2.41 ± 0.0

1.84 ± 0.1
2.75 ± 0.1
2.27 ± 0.1
2.96 ± 0.0
3.00 ± 0.0

2.65 ± 0.1
2.37 ± 0.1
2.74 ± 0.1
2.76 ± 0.1
2.76 ± 0.1

1.83 ± 0.2
2.67 ± 0.1
2.87 ± 0.1

2.77 ± 0.1
2.99 ± 0.0
2.79 ± 0.1
3.00 ± 0.0
2.99 ± 0.0

2.16 ± 0.2
2.68 ± 0.1
2.59 ± 0.1

Table 4: Average mistake rate of the solver. For each row, the solver and expert are separate instances of
the same model. “Human" model indicates a human is the solver, and the expert is a GPT-4o model. The
partial success rate is calculated by averaging over 100, 30 or 10 independent runs for AI, o4-mini, or human
solver respectively. The overall column is calculated by averaging across all the puzzles. We bold the human
score and the best AI model in each column.

30

Published in Transactions on Machine Learning Research (09/2025)

Model

Wire

Telehealth

Who

LED

Average Conversation Length % (↓)
Memory
Keypad
Password
Color

Maze

Atm

Overall

Human + GPT-4o 2.10 ± 0.2 4.60 ± 0.6 4.00 ± 1.0 7.40 ± 0.9 9.40 ± 0.2 3.50 ± 0.3 7.00 ± 0.7 6.30 ± 0.6 2.60 ± 0.5 5.56 ± 0.6 4.95 ± 0.3
o4-mini
OneVision
LLaVA-CoT

1.09 ± 0.1 0.80 ± 0.2 4.33 ± 0.7 6.43 ± 0.6 8.83 ± 0.3 4.13 ± 0.7 3.70 ± 0.7 3.63 ± 0.5
0.18 ± 0.0 1.06 ± 0.1 2.07 ± 0.3 1.36 ± 0.2 1.36 ± 0.1 1.09 ± 0.2 2.54 ± 0.4 3.55 ± 0.4
0.55 ± 0.1 1.74 ± 0.1 1.48 ± 0.2 0.91 ± 0.1 2.33 ± 0.2 2.57 ± 0.3 1.70 ± 0.2 2.18 ± 0.2

Gemini 2.0
GPT-4o
GPT-4V

1.23 ± 0.2
1.55 ± 0.1
2.31 ± 0.2

2.52 ± 0.2
3.17 ± 0.2
2.60 ± 0.3

InternVL
LLaMA 3.2
LLaVA 1.6
QwenVL
Random

0.36 ± 0.1
1.31 ± 0.1
2.23 ± 0.1
1.75 ± 0.1
1.27 ± 0.1

2.46 ± 0.1 1.73 ± 0.1
4.84 ± 0.4 2.43 ± 0.1
1.78 ± 0.3 3.15 ± 0.1
1.64 ± 0.1 2.58 ± 0.1
2.27 ± 0.0 1.46 ± 0.1

3.55 ± 0.2
2.88 ± 0.1
3.76 ± 0.2

4.64 ± 0.1
4.91 ± 0.0
4.56 ± 0.1

8.63 ± 0.5
7.08 ± 0.4
7.47 ± 0.4

7.33 ± 0.6 4.17 ± 0.2
6.80 ± 0.3 2.71 ± 0.1
7.28 ± 0.4 2.82 ± 0.1

4.09 ± 0.2
9.24 ± 0.1
7.55 ± 0.2

3.32 ± 0.2
1.44 ± 0.1
1.88 ± 0.2

2.93 ± 0.1 1.92 ± 0.2 4.45 ± 0.1 5.75 ± 0.2 3.44 ± 0.1
4.45 ± 0.1 2.17 ± 0.1 4.95 ± 0.0 4.40 ± 0.3 3.92 ± 0.1
4.56 ± 0.1 2.57 ± 0.1 4.71 ± 0.1 6.68 ± 0.3 4.12 ± 0.1

2.80 ± 0.1 2.53 ± 0.1
3.16 ± 0.1 4.03 ± 0.2
1.82 ± 0.2 3.71 ± 0.1
3.84 ± 0.1 4.71 ± 0.3
2.76 ± 0.1 2.95 ± 0.1

2.15 ± 0.1
1.63 ± 0.1
2.29 ± 0.2
2.04 ± 0.1
2.88 ± 0.1

3.12 ± 0.1
2.95 ± 0.2
3.79 ± 0.1
4.97 ± 0.0
9.92 ± 0.0

2.34 ± 0.2
2.25 ± 0.2
4.22 ± 0.2
3.64 ± 0.1
2.58 ± 0.1

4.89 ± 0.1 5.69 ± 0.1 3.05 ± 0.1
4.71 ± 0.1 9.54 ± 0.1 3.69 ± 0.1
5.00 ± 0.0 10.00 ± 0.0 3.80 ± 0.1
4.83 ± 0.1 10.00 ± 0.0 4.00 ± 0.1
8.59 ± 0.2 10.00 ± 0.0 4.47 ± 0.1

Table 5: Average conversation length between the solver and expert on our benchmark. For each row, the
solver and expert are separate instances of the same model. “Human" model indicates a human is the solver,
and the expert is a GPT-4o model. The partial success rate is calculated by averaging over 100, 30 or 10
independent runs for AI, o4-mini, or human solver respectively. The overall column is calculated by averaging
across all the puzzles. We bold the human score and the best AI model in each column. We observed that
the open source reasoning models often have lower conversation lengths because the solver tries actions on
its own, ignoring the advice of the expert.

D

Agent Prompts

Solver Prompt:

You are the solver in a cooperative game involving solving puzzles. As
the solver, you are presented with an image of the puzzle, along with
possible actions you may take. You should only attempt some actions
if you are certain of the solution. Otherwise, you should describe
the image and ask the expert. When asking the expert, keep in mind the
expert cannot see the image. Your description should be concise but
also detailed enough to convey the details to the expert through text
only. Once you are certain of the solution, respond with just the name
of the action you chose. If in a puzzle you can take multiple steps
to solve it, you could output a list of action names, separated by the
line break \n and in the sequential order to be executed. ONLY FINISH
THE SOLVER’S DIALOGUE.

Expert Prompt:

You are the expert in a cooperative game involving solving puzzles.
As the expert, you hold the puzzle solution manual, containing vital
information on various modules and their corresponding solution
procedures. Your task is to listen carefully to the solver’s
descriptions of the puzzles and provide clear and accurate instructions
to guide them through the solution. Be as concise and precise in your
instructions as possible. If the solver does not provide you with
enough information, ask for clarification if needed. ONLY FINISH THE
EXPERT’S DIALOGUE.

31

