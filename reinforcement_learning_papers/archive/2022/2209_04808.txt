Graphon Mean-Field Control for Cooperative
Multi-Agent Reinforcement Learning

arXiv:2209.04808v1 [cs.MA] 11 Sep 2022

Yuanquan Hu ∗

Xiaoli Wei †

Junji Yan ‡

Hengxi Zhang §

Abstract
The marriage between mean-field theory and reinforcement learning has shown a great
capacity to solve large-scale control problems with homogeneous agents. To break the
homogeneity restriction of mean-field theory, a recent interest is to introduce graphon
theory to the mean-field paradigm. In this paper, we propose a graphon mean-field control (GMFC) framework to approximate cooperative multi-agent reinforcement learning
(MARL) with nonuniform interactions and show that the approximate order is of O( √1N ),
with N the number of agents. By discretizing the graphon index of GMFC, we further
introduce a smaller class of GMFC called block GMFC, which is shown to well approximate cooperative MARL. Our empirical studies on several examples demonstrate that
our GMFC approach is comparable with the state-of-art MARL algorithms while enjoying
better scalability.

1

Introduction

Multi-agent reinforcement learning (MARL) has found various applications in the field of transportation and simulating [50, 1], stock price analyzing and trading [32, 31], wireless communication networks [12, 11, 13], and learning behaviors in social dilemmas [33, 28, 34]. MARL,
however, becomes intractable due to the complex interactions among agents as the number of
agents increases.
A recent tractable approach is a mean-field approach by considering MARL in the regime
with a large number of homogeneous agents under weak interactions [20]. According to the
number of agents and learning goals, there are three subtle types of mean-field theories for
MARL. The first one is called mean-field MARL (MF-MARL), which refers to the empirical
average of the states or actions of a finite population. For example, [52] proposes to approximate
interactions within the population of agents by averaging the actions of the overall population
or neighboring agents. [35] proposes a mean-field proximal policy optimization algorithm for a
class of MARL with permutation invariance. The second one is called mean-field game (MFG),
which describes the asymptotic limit of non-cooperative stochastic games as the number of
agents goes to infinity [30, 27, 8]. Recently, a rapidly growing literature studies MFG for
noncooperative MARL either in a model-based way [53, 6, 26] or by a model-free approach
[25, 48, 18, 14, 44]. The third one is called mean-field control (MFC), which is closely related
to MFG yet different from MFG in terms of learning goals. For cooperative MFC, the Bellman
equation for the value function is defined on an enlarged space of probability measures, and
∗ huyq21@mails.tsinghua.edu.cn
† xiaoli_wei@sz.tsinghua.edu.cn
‡ yan-jj21@mails.tsinghua.edu.cn
§ zhanghx20@mails.tsinghua.edu.cn

1

MFC is always reformulated as a new Markov decision process (MDP) with continuous stateaction space [43]. [9] shows the existence of optimal policies for MFC in the form of mean-field
MDP and adapts classical reinforcement learning (RL) methods to the mean-field setups. [24]
approximates MARL by a MFC approach, and proposes a model-free kernel-based Q-learning
algorithm (MFC-K-Q) that enjoys a linear convergence rate and is independent of the number
of agents. [44] presents a model-based RL algorithm M3-UCRL for MFC with a general regret
bound. [2] proposes a unified two-timescale learning framework for MFG and MFC by tuning
the ratio of learning rates of Q function and the population state distribution.
One restriction of the mean-field theory is that it eliminates the difference among agents and
interactions between agents are assumed to be uniform. However, in many real world scenarios,
strategic interactions between agents are not always uniform and rely on the relative positions
of agents. To develop scalable learning algorithms for multi-agent systems with heterogeneous
agents, one approach is to exploit the local network structure of agents [45, 38]. Another
approach is to consider mean-field systems on large graphs and their asymptotic limits, which
leads to graphon mean-field theory [39]. So far, most existing works on graphon mean-field
theory consider either diffusion processes without learning in continuous time or non-cooperative
graphon mean-field game (GMFG) in discrete time. [3] considers uncontrolled graphon meanfield systems in continuous time. [17] studies MFG on an Erdös-Rényi graph. [19] studies
the convergence of weighted empirical measures described by stochastic differential equations.
[4] studies propagation of chaos of weakly interacting particles on general graph sequences. [5]
considers general GMFG and studies ε-Nash equilibria of the multi-agent system by a PDE
approach in continuous time. [29] studies stochastic games on large graphs and their graphon
limits. It shows that GMFG is viewed as a special case of MFG by viewing the label of agents
as a component of the state process. [21, 22] study continuous-time cooperative graphon meanfield systems with linear dynamics. On the other hand, [7] studies static finite-agent network
games and their associated graphon games. [49] provides a sequential decomposition algorithm
to find Nash equilibria of discrete-time GMFG. [15] constructs a discrete-time learning GMFG
framework to analyze approximate Nash equilibria for MARL with nonuniform interactions.
However, little is focused on learning cooperative graphon mean-field systems in discrete time,
except for [41, 42] on particular forms of nonuniform interactions among agents. [42] proves
that when the reward is affine in the state distribution and action distribution, MARL with
nonuniform interactions can still be approximated by classic MFC. [41] considers multi-class
MARL, where agents belonging to the same class are homogeneous. In contrast, we consider
a general discrete-time GMFC framework under which agents are allowed to interact nonuniformly on any network captured by a graphon.
Our Work In this work, we propose a general discrete-time GMFC framework to approximate
cooperative MARL on large graphs by combining classic MFC and network games. Theoretically, we first show that GMFC can be reformulated as a new MDP with deterministic dynamics
and infinite-dimensional state-action space, hence the Bellman equation for Q function is established on the space of probability measure ensembles. It shows that GMFC approximates
cooperative MARL well in terms
√ of both value function and optimal policies. The approximation error is at order O(1/ N ), where N is the number of agents. Furthermore, instead of
learning infinite-dimensional GMFC directly, we introduce a smaller class called block GMFC by
discretizing the graphon index, which can be recast as a new MDP with deterministic dynamic
and finite-dimensional continuous state-action space. We show that the optimal policy ensemble
learned from block GMFC is near optimal for cooperative MARL. To deploy the policy ensemble
in the finite-agent system, we directly sample from the action distribution in the blocks. Empirically, our experiments in Section 5 demonstrate that when the number of agents becomes large,
the mean episode reward of MARL becomes increasingly close to that of block GMFC, which

2

verifies our theoretical findings. Furthermore, our block GMFC approach achieves comparable
performances with other popular existing MARL algorithms in the finite-agent setting.
Outline The rest of the paper is organized as follows. Section 2 recalls basic notations of
graphons and introduces the setup of cooperative MARL with nonuniform interactions and its
asymptotic limit called GMFC. Section 3 connects cooperative MARL and GMFC, introduces
block GMFC for efficient algorithm design, and builds its connection with cooperative MARL.
The main theoretical proofs are presented in Section 4. Section 5 tests the performance of block
GMFC experimentally.

2

Mean-Field MARL on Dense Graphs

2.1

Preliminary: Graphon Theory

In the following, we consider a cooperative multi-agent system and its associated mean-field
limit. In this system, each agent is affected by all others, with different agents exerting different
effects on her. This multi-agent system with N agents can be described by a weighted graph
GN = (VN , EN ), where the vertex set VN = {1, . . . , N } and the edge set EN represent agents
and the interactions between agents, respectively. To study the limit of the multi-agent system
as N goes to infinity, we adopt the graphon theory introduced in [39] used to characterize the
limit behavior of dense graph sequences. Therefore, throughout the paper, we assume the graph
GN is dense and leave sparse graphs for future study.
In general, a graphon is represented by a bounded symmetric measurable function W :
I × I → I, with I = [0, 1]. We denote by W the space of all graphons and equip the space W
with the cut norm k · k
Z
W (α, β)dαdβ .
kW k = sup
S,T ⊂I

S×T

It is worth noting that each weighted graph GN = (VN , EN ) is uniquely determined by a stepgraphon WN
 dN αe dN βe 
,
.
WN (α, β) = WN
N
N
We assume that the sequence of WN converges to a graphon W in cut norm as the number of
agents N goes to infinity, which is crucial for the convergence analysis of cooperative MARL in
Section 3.
Assumption 2.1 The sequence (WN )N ∈N converges in cut norm to some graphon W ∈ W
such that
kWN − W k → 0.
Some common examples of graphons include
1) Erdős Rényi: W (α, β) = p, 0 ≤ p ≤ 1, α, β ∈ I;
2) Stochastic block model:

W (α, β) =

p
q

if 0 ⩽ α, β ⩽ 0.5 or 0.5 ⩽ α, β ⩽ 1,
otherwise,

where p represents the intra-community interaction and q the inter-community interaction;
3) Random geometric graphon: W (α, β) = f (min(|β − α|, 1 − |β − α|)), where f : [0, 0.5] →
[0, 1] is a non-increasing function.
3

2.2

Cooperative MARL with Nonuniform Interactions

In this section, we facilitate the analysis of MARL by considering a particular class of MARL
with nonuniform interactions, where each agent interacts with all other agents via the aggregated
weighted mean-field effect of the population of all agents.
Recall that we use the weighted graph GN = (VN , EN ) to represent the multi-agent system,
in which agents are cooperative and coordinated by a central controller. They share a finite
state space S and take actions from a finite action space A. We denote by P(S) and P(A) the
space of all probability measures on S and A, respectively. Furthermore, denote by B(S) the
space of all Borel measures on S.
For each agent i, the neighborhood empirical measure is given by
N
µi,W
(·) :=
t

1 X N
ξi,j δsj (·),
t
N

(2.1)

j∈VN

N
where δsj denotes Dirac measure at sjt , and ξi,j
describing the interaction between agents i and
t
j is taken as either
N
ξij
= WN (

i j
, )
N N

(C1)

or
N
ξi,j
∼ Bernoulli(WN (

i j
, )).
N N

(C2)

At each step t = 0, 1, · · · , if agent i, i ∈ [N ] at state sit ∈ S takes an action ait ∈ A, then she
will receive a reward


i
N
r sit , µi,W
,
a
i ∈ [N ],
(2.2)
t
t ,
where r : S × B(S) × A → R, and she will change to a new state sit+1 according to a transition
probability such that


N
sit+1 ∼ P · sit , µi,W
, ait , i ∈ [N ], si0 ∼ µ ∈ P(S),
(2.3)
t
where P : S × B(S) × A → P(S).
(2.2)-(2.3) indicate that the reward and the transition probability of agent i at time t depend
N
.
on both her individual information (sit , ait ) and neighborhood empirical measure µi,W
t
Furthermore, the policy is assumed to be stationary for simplicity and takes the Markovian
form

ait ∼ π i ·|sit ∈ P(A), i ∈ [N ],
(2.4)
which maps agent i’s state to a randomized action. For each agent i, the space of all policies is
denoted as Π.
N
Remark 2.2 When ξij
≡ 1, i, j ∈ [N ], it corresponds to classical mean-field theory with uniform interactions [9, 24]. Furthermore, our framework is flexible enough to include the nonP
N
uniform interactions of actions via νti,WN = N1 j∈VN ξi,j
δaj (·), and also to include heterogenet
ity of agents by allowing r and P to rely on the agent types i.

4

The objective of the multi-agent system (2.1)-(2.4) is to maximize the expected discounted
accumulated reward averaged over all agents, i.e.,
VN (µ)

=

sup

JN (µ, π1 , . . . , πN )

(2.5)

(π 1 ,...,π N )∈ΠN

"∞
N
X

1 X
N
γ t r sit , µi,W
E
, ait
:=
sup
t
(π 1 ,...,π N )∈ΠN N i=1
t=0

#
si0 ∼ µ, ait ∼ π i (·|sit )

,

subject to (2.1)-(2.4) with a discount factor γ ∈ (0, 1).
Definition 2.3 An ε-Pareto optimality of cooperative MARL (2.1)-(2.5) for ε > 0 is defined as
(π 1,∗ , . . . , π N,∗ ) ∈ ΠN such that
∗
JN (µ, π1∗ , . . . , πN
)≥

2.3

JN (µ, π1 , . . . , πN ) − ε.

sup

(2.6)

(π 1 ,...,π N )∈ΠN

Graphon Mean-Field Control

We expect the cooperative MARL (2.1)-(2.5) to become a GMFC problem as N → ∞. In
GMFC, there is a continuum of agents α ∈ I, and each agent with the index/label α ∈ I
follows
α,W
α
α
α
α
α
α
sα
, aα
0 ∼ µ , at ∼ π (·|st ), st+1 ∼ P (·|st , µt
t ),

(2.7)

α,W
α
α
is defined as the
where µα
t = L(st ), α ∈ I denotes the probability distribution of st , and µt
neighborhood mean-field measure of agent α:
Z
α,W
µt
=
W (α, β)µβt dβ ∈ B(S),
(2.8)
I

with the graphon W given in Assumption 2.1.
To ease the sequel analysis, define the space of state distribution ensembles M := P(S)I :=
{f : I → P(S)} and the space of policy ensembles Π := P(A)S×I . Then µ := (µα )α∈I and
π := (π α )α∈I are elements in M and Π , respectively.
The objective of GMFC is to maximize the expected discounted accumulated reward averaged over all agents α ∈ I
V (µ ) :

=

sup J(µ , π )
"∞
Z
X

α,W
sup E
γ t r sα
, aα
t , µt
t

(2.9)

π ∈Π

=

π ∈Π

I

#
α α
α
α
sα
0 ∼ µ , at ∼ π (·|st )

dα.

t=0

3

Main Results

3.1

Reformulation of GMFC

In this section, we show that GMFC (2.7)-(2.9) can be reformulated as a MDP with deterministic
dynamics and continuous state-action space M × Π .
Theorem 3.1 GMFC (2.7)-(2.9) can be reformulated as
V (µ ) = sup

∞
X

π ∈Π t=0

γ t R(µ t , π ),

5

(3.1)

subject to
α
α
α
µα
t+1 (·) = Φ (µ t , π )(·), t ∈ N, µ0 = µ , α ∈ I,

(3.2)

where the aggregated reward R : M × Π → R and the aggregated transition dynamics Φ :
M × Π → M are given by
Z XX
R(µ , π ) =
r(s, a, µα,W )π α (a|s)µα (s)dα,
(3.3)
I s∈S a∈A

Φ α (µ , π )(·) =

XX

P (·|s, µα,W , a)π α (a|s)µα (s).

(3.4)

s∈S a∈A

The proof of Theorem 3.1 is similar to the proof of Lemma 2.2 in [23]. So we omit it here.
(3.4) and (3.2) indicate the evolution of the state distribution ensemble µ t over time. That
is, under the fixed policy ensemble π , the state distribution µα
t+1 of agent α at time t + 1 is
fully determined by the policy ensemble π and the state distribution ensemble µ t at time t.
Note that the state distribution of each agent α is fully coupled with state distributions of the
population of all agents via the graphon W .
With the reformulation in Theorem 3.1, the associated Q function starting from (µ , π ) ∈
M × Π is defined as
Q(µ , π )

= R(µ , π ) + sup
π 0 ∈Π

∞
hX

γtR µt, π 0



i
α α
α
α
sα
0 ∼ µ , a0 ∼ π (·|s0 ) .

(3.5)

t=1

Hence its Bellman equation is given by
Q(µ , π ) = R(µ , π ) + γ sup Q(Φ (µ , π ), π 0 ).

(3.6)

π 0 ∈Π

Remark 3.2 (Label-state formulation) GMFC (2.7)-(2.9) can be viewed as a classical MFC
S×I
with extended state space S × I, action space A,
, mean-field information
R policy π̃ ∈ P(A)
µ̃ ∈ P(S × I), reward r̃((s, α), µ̃, a) := r(s, I W (α, β)µ̃(·, β)dβ, a), transition dynamics of
(s̃t , αt ) such that
Z
s̃t+1 ∼ P (·|s̃t , ãt , W (αt , β)µ̃t (·, β)dβ), αt+1 = αt , ãt ∼ π̃(·|s̃t , αt ), s̃0 ∼ µ0 , α̃0 ∼ U nif (0, 1).
I

It is worth pointing out such a label-state formulation has also been studied in GMFG [29, 15].

3.2

Approximation

In this section, we show that GMFC (2.7)-(2.9) provides a good approximation for the cooperative multi-agent system (2.1)-(2.5) in terms of the value function and the optimal policy
ensemble. To do this, the following assumptions on W , P , r, and π are needed.
Assumption 3.3 (graphon W ) There exists LW > 0 such that for all α, α0 , β, β 0 ∈ I


|W (α, β) − W (α0 , β 0 )| ≤ LW · |α − α0 | + |β − β 0 | .
Assumption 3.3 is common in graphon mean-field theory [21, 15, 29]. Indeed, the Lipschitz
continuity assumption on W in Assumption 3.3 can be relaxed to piecewise Lipschitz continuity
on W .
6

Assumption 3.4 (transition probability P ) There exists LP > 0 such that for all s ∈
S, a ∈ A, µ1 , µ2 ∈ B(S)
kP (·|s, µ1 , a) − P (·|s, µ2 , a)k1 ≤ LP · kµ1 − µ2 k1 ,
where k · k1 denotes L1 norm here and throughout the paper.
Assumption 3.5 (reward r) There exist Mr > 0 and Lr > 0 such that for all s ∈ S, a ∈ A,
µ1 , µ2 ∈ B(S),
|r(s, µ, a)| ≤ Mr , |r(s, µ1 , a) − r(s, µ2 , a)| ≤ Lr · ||µ1 − µ2 ||1 .
Assumption 3.6 (policy π ) There exists LΠ > 0 such that for any policy ensemble π :=
(π α )α∈I ∈ Π is Lipschitz continuous, i.e.
max kπ α (·|s) − π β (·|s)k1 ≤ LΠ |α − β|.
s∈S

Assumptions 3.4-3.6 are standard and commonly used to bridge the multi-agent system and
mean-field theory.
To show approximation properties of GMFC in the large-scale multi-agent system, we need
to relate policy ensembles of GMFC to policies of the multi-agent system. On one hand, one
can see that any π ∈ Π leads to a N -agent policy tuple (π 1 , . . . , π N ) ∈ ΠN with
i

ΓN : Π 3 π 7→ (π 1 , . . . , π N ) ∈ ΠN , with π i := π N .

(3.7)

On the other hand, any N -agent policy tuple (π 1 , . . . , π N ) ∈ ΠN can be seen as a step policy
ensemble π N in Π :
π N,α :=

N
X

π i 1α∈( i−1 , i ] ∈ Π .
N

N

(3.8)

i=1

Theorem 3.7 (Approximate Pareto Property) Assume Assumptions 2.1, 3.3, 3.4, 3.5 and
3.6. Then under either the condition (C1) or (C2), we have for any initial distribution µ ∈ P(S)
|VN (µ) − V (µ)| → 0, as N → ∞.

(3.9)

Moreover, if the graphon convergence in Assumption 2.1 is at rate O( √1N ), then |VN (µ) −
V (µ)| = O( √1N ). As a consequence, for any ε > 0, there exists an integer Nε such that when
N ≥ Nε , the optimal policy ensemble of GMFC denoted as π ∗ (if it exists) provides an ε-Pareto
optimality (π 1,∗ , . . . , π N,∗ ) := ΓN (π ∗ ) for the multi-agent system (2.5), with ΓN defined in (3.7).
Directly learning Q function of GMFC in (3.6) will lead to high complexity. Instead, we will
introduce a smaller class of GMFC with a lower dimension in the next section, which enables a
scalable algorithm.

3.3

Algorithm Design

This section will show that discretizing the graphon index α ∈ I of GMFC enables to approximate Q function in (3.6) by an approximated Q function in (3.10) below defined on a smaller
space, which is critical for designing efficient learning algorithms.
7

m
Precisely, we choose uniform grids αm ∈ IM := { M
, 0 ≤ m ≤ M } for simplicity, and
f M := P(S)IM ,
approximate each agent α ∈ I by the nearest αm ∈ IM close to it. Introduce M
e M := P(A)S×IM . Meanwhile, µ̃ := (µ̃αm )m∈[M ] ∈ M
f M and π̃ := (π̃ αm )m∈[M ] ∈ Π
e M can be
Π
viewed as a piecewise constant state distribution ensemble in M and a piecewise constant policy
ensemble in Π , respectively. Our arguments can be easily generalized to nonuniform grids.
Consequently, instead of performing algorithms according to (3.6) with a continuum of
graphon labels directly, we work with GMFC with M blocks called block GMFC, in which
agents in the same block are homogeneous. The Bellman equation for Q function of block
GMFC is given by

e (µ̃ , π̃ ), π̃ 0 ),
e , π̃ ) = R(µ̃
e , π̃ ) + γ sup Q(
eΦ
Q(µ̃

(3.10)

eM
e0 ∈Π
π

fM × Π
e M → R and
e:M
where the neighborhood mean-field measure, the aggregated reward R
e
f
e
f
the aggregated transition dynamics Φ : M M × Π M → M M are given by
M −1

µ̃αm ,W

=

1 X
W (αm , αm0 )µ̃αm0 , m ∈ [M ],
M 0

(3.11)

m =0
M −1

e , π̃ )
R(µ̃

=

e αm (µ̃ , π̃ )(·)
Φ

=

1 X XX
r(s, a, µ̃αm ,W )µ̃αm (s)π̃ αm (a|s),
M m=0
s∈S a∈A
XX
P (·|s, a, µ̃αm ,W )µ̃αm (s)π̃ αm (a|s).

(3.12)
(3.13)

s∈S a∈A

e and continWe see from (3.10) that block GMFC is a MDP with deterministic dynamics Φ
f
e
uous state-action space M M × Π M . The following Theorem shows that there exists an optimal
eM.
policy ensemble of block GMFC in Π
Theorem 3.8 (Existence of Optimal Policy Ensemble) Given Assumptions 3.4, 3.5, ase M that maximize
sume γ ·(LP +1) < ∞, then for any fixed integer M > 0, there exists an π̃ ∗ ∈ Π
f
e
Q(µ̃ , π̃ ) in (3.10) for any µ̃ ∈ M M .
Furthermore, we show that with sufficiently fine partitions of the graphon index I, i.e.,
M is sufficiently large, block GMFC (3.10)-(3.13) well approximates the multi-agent system in
Section 2.2.
Theorem 3.9 Assume γ · (LP + 1) < 1 and Assumptions 2.1, 3.3, 3.4, 3.5 and 3.6. Under
either (C1) or (C2), for any ε > 0, there exists Nε , Mε such that for N ≥ Nε , the optimal policy ensemble π̃ ∗ of block GMFC (3.10) with Mε blocks provides an ε-Pareto optimality
(π̃ 1,∗ , . . . , π̃ N,∗ ) := ΓN (π̃ ∗ ) for the multi-agent system (2.5) with N agents.
Theorem 3.9 shows that the optimal policy ensemble of block GMFC is near-optimal for all
sufficiently large multi-agent systems, meaning that block GMFC provides a good approximation
for the multi-agent system.
Recall that block GMFC can be viewed as a MDP with deterministic dynamics and continuous state-action space. To learn block GMFC, one can adopt a similar kernel-based Q learning
method in [24] for MFC, a uniform discretization method or deep reinforcement algorithms like
DDPG [37] for MFC in [9] with theoretical guarantees. Since block GMFC has a higher dimension than classical MFC, we choose to adapt DRL algorithm Proximal Policy Optimization
(PPO) [47] to block GMFC and then apply the learned policy ensemble of block GMFC to the
8

multi-agent system to validate our theoretical findings. We describe the deployment of block
GMFC in the multi-agent system in Algorithm 1, which we call it N-agent GMFC.
Algorithm 1 N-agent GMFC
Input Initial state distribution µ0 , number of agents N , episode length T , the learned policy
e M learned by PPO
π̃ ∈ Π
Initialize si0 ∼ µ0 , i ∈ [N ]
for t = 1 to T do
for i = 1 to N do
m
|
Choose m(i) = arg min | Ni − M
m∈[M ]

Sample action ait ∼ π̃ αm(i) (·|sit ), observe reward rti and new state sit+1
end for
end for

4

Proofs of Main Results

In this section, we will provide proofs of Theorems 3.7-3.9.

4.1

Proof of Theorem 3.7

To prove Theorem 3.7, we need the following two Lemmas. We start by defining the step state
N,α
)α∈I for notational simplicity
distribution µ N
t := (µt
X
(4.1)
µN,α
(·) =
δsit (·)1α∈( i−1 , i ] .
t
N

N

i∈VN

Lemma 4.1 shows the convergence of the neighborhood empirical measure to the neighborhood mean-field measure.
Lemma 4.1 Assume Assumptions 2.1, 3.3, 3.4 and 3.6. Under either condition (C1) or (C2),
for any policy ensemble π ∈ Π , we have
N Z
X
i=1

i
( i−1
N ,N ]



N
E kµi,W
− µα,W
k1 dα → 0, as N → ∞,
t
t

(4.2)

where µit = µα
t ≡ µ ∈ P(S).
Moreover, if the graphon convergence in Assumption 2.1 is at rate O( √1N ), then
N Z
X
i=1



1
N
E kµi,W
− µα,W
k1 dα = O( √ ).
t
t
i
N
( i−1
N ,N ]

Proof of Lemma 4.1 We first prove (4.2) under the condition (C1) and then show (4.2) also
holds under the condition (C2).
R
N
N
Case 1: ξi,j
= WN ( Ni , Nj ). Note that under the condition (C1), µi,W
= I WN ( Ni , β)µN,β
dβ
t
t

9

by the definition of µN,α
in (4.1). Then
t
N Z
X
i=1

i
( i−1
N ,N ]



N
E kµi,W
− µα,W
k1 dα
t
t

N Z
X

≤

Z
i
h Z
i
β
dα
dβ
−
W
(α,
β)µ
dβ
E
WN ( , β)µN,β
t
t
i−1 i
N
1
I
I
i=1 ( N , N ]
Z
N Z
i
h Z
X
i
i
N,β
dα
E
WN ( , β)µt dβ − WN ( , β)µβt dβ
i−1 i
N
N
1
I
I
i=1 ( N , N ]
Z
N Z
i
h Z
X
i
β
+
dα
E
WN ( , β)µt dβ − W (α, β)µβt dβ
i−1 i
N
1
I
I
i=1 ( N , N ]

=

: I1 + I2 .

=

For the term I1 , we adopt Theorem 2 in [15] and have that under the policy ensemble π and
N -agent policy (π 1 , . . . , π N ) := ΓN (π ), with ΓN defined in (3.7)
Z
h Z
i
i
i
N,β
I1 = E
WN ( , β)µt dβ − WN ( , β)µβt dβ
→ 0, as N → ∞.
N
N
1
I
I
Moreover, if the graphon convergence in Assumption 2.1 is at rate O( √1N ), then the term I1 is
also at rate O( √1N ).

By noting that WN (α, β) = WN dNNαe , dNNβe ,
I2

=

N Z
X
i=1

=

i
( i−1
N ,N ]

N Z
X
i
( i−1
N ,N ]

Zi=1 Z
=
=

Z
dN αe  β
WN
, β µt dβ − W (α, β)µβt dβ dα
N
1
I
I
Z
Z

WN α, β µβt dβ − W (α, β)µβt dβ dα
Z

I

I

1

Z


WN α, β µβt dβ − W (α, β)µβt dβ dα
1
I
I
I
Z
XZ Z
 β
WN α, β µt (s)dβ − W (α, β)µβt (s)dβ dα

s∈S

I

I

I

→ 0,
where the last inequality is from the fact in [39] that the convergence of kWN − W k → 0 is
equivalent to the convergence of
Z Z

kWN − W kL∞ →L1 := sup
WN (α, β) − W (α, β) g(β)dβ dα → 0.
kgk∞ ≤1

I

I

Combining I1 and I2 , we prove (4.2) under the condition (C1).

10

N
Case 2: ξi,j
are random variables with Bernoulli(WN ( Ni , Nj )).
N Z
X
i=1

=

i
( i−1
N ,N ]

N Z
X
i=1

+

N
− µα,W
k1 dα
Ekµi,W
t
t

N

N Z
X
i=1

≤

i
( i−1
N ,N ]

1 X N
ξ δj−
N j=1 ij st

E

N

i
( i−1
N ,N ]

1 X N
ξ δj−
N j=1 ij st

E

N Z
X
i=1

Z

i
( i−1
N ,N ]

WN (

E
I

Z
I

W (α, β)µβt dβ 1 dα

Z
WN (
I

i
, β)µN,β
dβ 1 dα
t
N

i
, β)µN,β
dβ −
t
N

Z
I

W (α, β)µβt dβ 1 dα

=: I1 + I2 .
Note from Case 1 that I2 → 0 as N → ∞ and I2 = O( √1N ) if the graphon convergence in
Assumption 2.1 is at rate O( √1N ). Therefore, it is enough to estimate I1 .
N

I1

=

E

1 X N
ξ δj−
N j=1 ij st

h h
≤ E E

Z
WN (
I

N
n1 X

sup
f :S→{−1,1}

N j=1

i
, β)µN,β
dβ 1
t
N

N
ξij
f (sjt ) −

N
o
ii
1 X
i j
WN ( , )f (sjt ) s1t , . . . , sN
.
t
N j=1
N N

We proceed the same argument as in the proof of Theorem 6.3 in [24]. Precisely, condioN
n
N
ξij
f (sjt ) − WN ( Ni , Nj )f (sjt )
tioned on s1t , . . . , sN
is a sequence of independent meant ,
j=1

N
zero random variables bounded in [−1, 1] due to E[ξi,j
] = WN ( Ni , Nj ). This implies that each
j
j
j
N
ξij
f (st ) − WN ( Ni , N )f (st ) is a sub-Gaussian with variance bounded by 4. As a result, conn P
oN
PN
N
j
j
j
1
i
1
N
ditioned on s1t , . . . , sN
is a mean-zero subt ,
j=1 ξij f (st ) − N
j=1 WN ( N , N )f (st )
N
i=1

Gaussian random variable with variance N4 . By the equation (2.66) in [51], we have
I1

≤

h h
E E

N
n1 X

sup
f :S→{−1,1}

p
≤

N j=1

N
ξij
f (sjt ) −

N
o
ii
1 X
i j
WN ( , )f (sjt ) s1t , . . . , sN
t
N j=1
N N

8 ln(2)|S|
√
.
N

N
Therefore, combining I1 and I2 in Case 2, we show that when ξi,j
are random variables with
j
i
Bernoulli(WN ( N , N )), (4.2) holds under the condition (C2).
2

Lemma 4.2 shows the convergence of the state distribution of N -agent game to the state
distribution of GMFC.
Lemma 4.2 Assume Assumptions 2.1, 3.3, 3.4 and 3.6. For any uniformly bounded family G
of functions g : S → R, we have
sup

N Z
X

g∈G i=1

i
( i−1
N ,N ]

E[g(sit ) − g(sα
t )] → 0,

11

(4.3)

where si0 ∼ µ0 , sα
0 ∼ µ0 . Moreover, if the graphon convergence in Assumption 2.1 is at rate
O( √1N ), then
N Z
X
1
sup
E[g(sit ) − g(sα
t )] = O( √ ).
i−1
i
N
g∈G i=1 ( N , N ]
Proof of Lemma 4.2 The proof is by induction as follows. To do this, first introduce
XX
lg (s, µ, π) :=
g(s0 )P (s0 |s, µ, a)π(a|s).
a∈A s0 ∈S

(4.3) holds obviously at t = 0. Suppose that (4.3) holds at t. Then for any uniformly bounded
function g with |g| ≤ Mg at t + 1
N Z
X
i
( i−1
N ,N ]

i=1

=

N Z
X
i
( i−1
N ,N ]

i=1

≤

N Z
X
i
( i−1
N ,N ]

i=1

+





N
E lg (sit , µi,W
, π i ) − E lg (sit , µα,W
, π i ) dα
t
t

i
( i−1
N ,N ]

N Z
X
i
( i−1
N ,N ]

i=1

=





α,W
N
E lg (sit , µi,W
, π i ) − E lg (sα
, π α ) dα
t
t , µt

N Z
X
i=1

+

E[g(sit+1 ) − g(sα
t+1 )] dα





α,W
E lg (sit , µα,W
, π i ) − E lg (sα
, π i ) dα
t
t , µt




α,W
α,W
E lg (sα
, π i ) − E lg (sα
, π α ) dα
t , µt
t , µt

: I + II + III,

(4.4)

where the first equality is by the law of total expectation.
First term of (4.4)
I

=

N Z
X
i=1

≤

i
( i−1
N ,N ]

Mg LP





N
E lg (sit , µi,W
, π i ) dα
, π i ) − E lg (sit , µα,W
t
t

N Z
X
i=1

i
( i−1
N ,N ]



N
E kµi,W
− µα,W
k1 dα
t
t

→ 0, as N → ∞
where the second inequality is from the continuity of P , and the last inequality is from Lemma
4.1.
Second term of (4.4) One can view lg (s, µα,W
, π i ) as a function of s ∈ S for any fixed µα,W
t
t
α,W
i
i
and π i .
and π . Note that |lg (s, µt , π )| ≤ Mg , where Mg is a constant independent of µα,W
t
Since (4.3) holds at t, then
II

=

N Z
X
i=1

i
( i−1
N ,N ]





α,W
E lg (sit , µα,W
, π i ) − E lg (sα
, π i ) dα
t
t , µt

→ 0, as N → ∞.
12

Third term of (4.4)
III

N Z
X

=

i
( i−1
N ,N ]

i=1

≤





α,W
α,W
E lg (sα
, π i ) − E lg (sα
, π α ) dα
t , µt
t , µt

N Z
X

Mg

i
( i−1
N ,N ]

i=1



α α
E kπ i (sα
t ) − π (st )k1 dα

N Z
X

≤

Mg LΠ

=

1
O( ),
N

i=1

max

i α∈( i−1 , i ]
( i−1
N
N
N ,N ]

|

i
− α|dα
N

where the second inequality is by the uniform boundedness of g and the third inequality is from
Assumption 3.6.
2
Now we are ready to prove Theorem 3.7. We start by defining rb the aggregated reward over
all possible actions under the policy π
X
rb(s, µ, π) :=
r(s, µ, a)π(a|s).
a∈A

Proof of Theorem 3.7
|VN (µ) − V (µ)|
"∞
#
#
"∞
Z
N
X
X


1 X
α,W
i,W
γ t r sα
, aα
dα
E
γ t r sit , µt N , ait − sup E
= sup
t , µt
t
π
∈Π
ΠN N i=1
I
t=0
t=0
"∞
# Z
"∞
#
N
X
X


1 X
i,WN
α,W
t
i
i
t
α
α
≤ sup
E
γ r st , µt
, at − E
γ r st , µt , at dα
π ∈Π N i=1
I
t=0
t=0
∞
N Z
 
X
X

 α α,W

i
α
N
γt
= sup
E rb(sit , µi,W
,
π
)
−
E
r
b
(s
,
µ
,
π
)
dα
t
t
t
π ∈Π t=0

≤ sup

∞
X

i=1

γt

N Z
X

π ∈Π t=0

+ sup

i=1

∞
X

γ

t

π ∈Π t=0

+ sup

∞
X

π ∈Π
Π t=0

i
( i−1
N ,N ]

i
( i−1
N ,N ]

N Z
X
i=1

γt

 

 i α,W

i
i
N
E rb(sit , µi,W
,
π
)
−
E
r
b
(s
,
µ
,
π
)
dα
t
t
t

i
( i−1
N ,N ]

N Z
X
i=1

i
( i−1
N ,N ]

 



α,W
, π i ) dα
, π i ) − E rb(sα
E rb(sit , µα,W
t
t , µt
 



α,W
α,W
E rb(sα
, π i ) − E rb(sα
, π α ) dα
t , µt
t , µt

:= I + II + III,

(4.5)

where we use (3.8) in the second inequality.
First term of (4.5)
I

≤ sup Lr
π

∞
X
t=0

γt

N Z
X
i=1

i
( i−1
N ,N ]

1
= O( √ ),
N

N
Ekµi,W
− µα,W
k1 dα
t
t

(4.6)

13

where√the last equality is from Lemma 4.1 when the convergence in Assumption 2.1 is at rate
O(1/ N ).
From Lemma 4.2, we have II = O( √1N ).

Second term of (4.5)
Third term of (4.5)
III

≤ sup
π

∞
X
t=0

≤ LΠ sup
π

γt

N Z
X
i=1

∞
X
t=0

γt

i
( i−1
N ,N ]

max kπ i (s) − π α (s)k1 dα
s∈S

N Z
X
i=1

i
− π α |dα
i
N
( i−1
,
]
N
N
|

1
= O( ).
N
2

Therefore, combining I, II and III yields the desired result.

4.2

Proof of Theorem 3.8

First, we see that (3.10) corresponds to the following optimal control problem
Ve M (µ̃ ) := sup J˜M (µ̃ , π̃ )
eM
π̃ ∈Π

"∞
#
M
X
 αm
1 X
αm ,W
αm
αm
αm
αm
t
αm
αm
γ r s̃t , µ̃t
, ãt
s̃0 ∼ µ̃ , ãt ∼ π̃ (·|s̃t ) .(4.7)
E
= sup
e M M m=1
π̃ ∈Π
t=0
The associated Q function of (4.7) is defined as
"∞
#
M
X

1 X
α
,W
α
α
α
α
α
E
γ t r s̃t m , µ̃t m , ãt m s̃0 m ∼ µ̃αm , ã0 m ∼ π̃ αm (·|s̃t m )
Q̃(µ̃ , π̃ ) = sup
π̃ 0 M m=1
t=0
= R(µ̃ , π̃ ) + sup

∞
X

γ t R̃(µ̃ t , π̃ 0 ),

(4.8)

e M t=1
π̃ 0 ∈Π

e (µ̃ , π̃ ), µ̃ = µ̃ .
subject to µ̃ t+1 = Φ
t
0
We first show the verification result and then prove the continuity property of Q̃ in (4.8),
which thus leads to Theorem 3.8.
Lemma 4.3 (Verification) Assume Assumption 3.5. Then Q̃ in (4.8) is the unique function
satisfying the Bellman equation (3.10). Furthermore, if there exists π̃ ∗ ∈ arg maxΠe M Q̃(µ̃ , π̃ ) for
f M , then π̃ ∗ is an optimal stationary policy ensemble.
each µ̃ ∈ M
The proof of Lemma 4.3 is standard and very similar to the proof of Proposition 3.3 in [24].
Mr
fM × Π
eM →
Proof of Lemma 4.3 First, define 1−γ
-bounded function space Q := {f : M
Mr
Mr
[− 1−γ
, 1−γ
]}. Then we define a Bellman operator B : Q → Q

e (µ̃ , π̃ ), π̃ 0 ),
e , π̃ ) + γ sup q(Φ
(Bq)(µ̃ , π̃ ) := R(µ̃
eM
π̃ 0 ∈Π

14

One can show that B is a contraction operator with the module-γ. By Banach fixed point
theorem, B admits a unique fixed point. As Q̃ function of (4.8) satisfies B Q̃ = Q̃, Q̃ is unique
solution of (3.10).
0
e M with
We next define Bπ̃ : Q → Q under the policy ensemble π̃ 0 ∈ Π
0
e (µ̃ , π̃ ), π̃ 0 ).
e , π̃ ) + γq(Φ
(Bπ̃ q)(µ̃ , π̃ ) := R(µ̃
0

Similarly, we can show that Bπ̃ is a contraction map with the module-γ and thus admits a
0
unique fixed point, which is denoted as Q̃π̃ . From this, we have
∗

Q̃π̃ (µ̃ , π̃ )

e , π̃ ) + γ Q̃π̃ ∗ (Φ̃ (µ̃ , π̃ ), π̃ ∗ )
= R(µ̃
e (µ̃ , π̃ ), π̃ 0 ) = Q̃(µ̃ , π̃ ),
e , π̃ ) + γ sup Q̃(Φ
= R(µ̃
eM
π̃ 0 ∈Π

which implies π̃ ∗ is an optimal policy ensemble.

2

Lemma 4.4 Let Assumptions 3.4, 3.5 hold. Assume further γ · (1 + LP ) < 1. Then Q̃ in (4.8)
is continuous.
R
Proof of Lemma
4.4 We will show that as µ̃ n → µ̃ , π̃ n → π̃ in the sense that I kµ̃α −
R
α
α
µ̃α
n k1 dα + I maxs∈S kπ̃ − π̃n k1 dα → 0,
Q̃(µ̃ n , π̃ n ) → Q̃(µ̃ , π̃ ).
From (4.8),

≤

|Q̃(µ̃ n , π̃ n ) − Q̃(µ̃ , π̃ )|
∞
∞
X
X
e n , π̃ n ) + sup
e , π̃ ) + sup
γ t R̃(µ̃ n,t , π̃ 0 )
γ t R̃(µ̃ t , π̃ 0 ) − R(µ̃
R(µ̃
e M t=1
π̃ 0 ∈Π

e M t=1
π̃ 0 ∈Π

≤

∞
X

e , π̃ ) − R(µ̃
e n , π̃ n ) + sup
R(µ̃

γ t R̃(µ̃ n,t , π̃ 0 ) − R̃(µ̃ t , π̃ 0 )

e M t=1
π̃ 0 ∈Π

Z
≤

Lr ·

kµ̃α,W − µ̃α,W
k1 dα + Mr ·
n

I

+ sup

kµ̃α − µ̃α
n k1 dα + Mr ·





γ t · Lr ·

Z

kµ̃α,W
− µ̃α,W
t
n,t k1 dα + Mr ·

I

Z

Lr + Mr ·
∞
X

Z

max kπ̃ α − π̃nα k1 dα

I s∈S

α
kµ̃α
t − µ̃n,t k1 dα



I

kµ̃α − µ̃α
n k1 dα + Mr ·

Z

I

+ sup

Z

I
∞
X

e M t=1
π̃ 0 ∈Π

≤

Z

Z


γ t · Lr + Mr ·

max kπ̃ α − π̃nα k1 dα

I s∈S

α
kµ̃α
t − µ̃n,t k1 dα.

I

e M t=1
π̃ 0 ∈Π

By induction, we obtain
Z
Z
Z
α
α
α
(t−1)
α
kµ̃α
−
µ̃
k
dα
≤
(L
+
1)
·
kµ̃
−
µ̃
k
dα
≤
.
.
.
≤
(L
+
1)
kµ̃α
1
P
1
P
t
n,t
t−1
n,t−1
1 − µ̃n,1 k1 dα.
I

I

I

Therefore, if γ · (1 + LP ) < 1, then
|Q̃(µ̃ n , π̃ n ) − Q̃(µ̃ , π̃ )| ≤ C

Z

kµ̃α − µ̃α
n k1 dα +

I

where C is a constant depending on Lr , Mr , LP .
15

Z


max kπ̃ α − π̃nα k1 dα .

I s∈S

2

Now we prove Theorem 3.8.
e M , there exists π̃ ∗ ∈
Proof of Theorem 3.8 By Lemma 4.4, along with the compactness of Π
∗
e M such that π̃ ∈ arg max Q(µ̃ , π̃ ). By Lemma 4.3, there exists an optimal policy ensemble
Π
eM
π̃ ∈Π

eM.
π̃ ∗ ∈ Π

4.3

2

Proof of Theorem 3.9

We first prove the following Lemma, which shows that GMFC and block GMFC become increasingly close to each other as the number of blocks becomes larger.
Lemma 4.5 Under Assumptions 3.3, 3.4 and 3.6, we have
M Z
X

h
i L + 2L L + L
2LW
Π
P W
W
αm ,W
t
+
,
kµα,W
−
µ̃
k
dα
≤
(1
+
L
)
−
1
1
P
t
t
m−1 m
M
M
m=1 ( M , M ]
M Z
h
i L + 2L L + L
X
Π
P W
W
αm
t
(1
+
L
)
−
1
k
dα
≤
.
kµα
−
µ̃
P
1
t
t
m−1 m
M
(
,
]
M
M
m=1
Proof of Lemma 4.5
M Z
X
m
( m−1
M ,M ]

m=1

≤

M Z
X

(4.9)
M

m
( m−1
M ,M ]

m=1

m ,W
kµα,W
− µ̃α
k1 dα
t
t

m ,W
kµα,W
− µα
k1 dα +
t
t

1 X αm ,W
m ,W
kµ
− µ̄α
k1
t
M m=1 t

M

+

1 X αm ,W
m ,W
kµ̄
− µ̃α
k1 ,
t
M m=1 t

PM
1
αm0
0
where µ̄αm ,W := M
.
m0 =1 W (αm , αm )µ
α,W
αm ,W
m ,W
By the definition of µt , µt
in (2.8), µ̃α
in (3.11) and µ̄αm ,W , together with the
t
Lipschitz continuity of W in Assumption 3.3,
M Z
X
m=1

m
( m−1
M ,M ]

m ,W
kµα,W
− µα
k1 dα
t
t

≤

M Z
X
m
( m−1
M ,M ]

m=1

M
X

1
m ,W
kµαm ,W − µ̄α
k1
t
M m=1 t

≤

LW
,
M

≤

1 X αm
m
kµ − µ̃α
t k1 .
M m=1 t

M

1 X αm ,W
m ,W
kµ̄
− µ̃α
k1
t
M m=1 t

αm
kµα
t − µt k1 dα +

M

Plugging these into (4.9),
M Z
X
m=1

where At :=

PM

m
( m−1
M ,M ]

kµα,W
− µ̃tαm ,W k1 dα ≤ At +
t

2LW
,
M

PM
αm
αm
1
α
m
k1 dα + M
− µ̃α
m kµt − µt
t k1 .
m=1 kµt
m=1 ( m−1
M ,M ]
R

16

LW
,
M

On the other hand,
M Z
X
m=1

≤

m
( m−1
M ,M ]

M

M Z
X
m=1

αm
kµα
t − µ̃t k1 dα

m
( m−1
M ,M ]

αm
kµα
t − µt k1 dα +

1 X αm
m
kµ − µ̃α
t k1 = At .
M m=1 t

Therefore, it is enough to estimate At . We next estimate At+1 by an inductive way. Note that
A0 = 0.

=

At+1
M Z
X
m=1

=

M

m
( m−1
M ,M ]

αm
kµα
t+1 − µt+1 k1 dα +

M Z
X
m=1

+

XX

m
( m−1
M ,M ]

1 X αm
m
kµ
− µ̃α
t+1 k1
M m=1 t+1


αm ,W
αm
α
α
αm
(a|s)
P (·|s, µα,W
,
a)π
(a|s)µ
(s)
−
P
(·|s,
a,
µ
)µ
(s)π
t
t
t
t

s∈S a∈A

M

1 X XX
αm ,W
αm
m ,W
m
m
P (·|s, µα
, a)π αm (a|s)µα
)µ̃α
(a|s)
t
t (s) − P (·|s, a, µ̃t
t (s)π̃
M m=1
1
s∈S a∈A

≤

dα
1

M Z
X
m=1



m
( m−1
M ,M ]

LP · kµα,W
− µαm ,W k1 +
t


LΠ
αm
+ kµα
t − µt k1 dα
M

M

+


1 X
m ,W
m
m
LP · kµα
− µ̃αm ,W k1 + kµα
− µ̃α
t
t
t k1
M m=1

≤ (1 + LP )At + (LΠ + 2LP LW + LW )

1
,
M

where the second equality is from (3.4) and (3.13), and we use Assumptions 3.3, 3.4 and 3.6 in
the third inequality.
By induction, we have
h
i L + 2L L + L
Π
P W
W
At+1 ≤ (1 + LP )t − 1
.
M
2
Based on Lemma 4.5, we have the following Proposition.
Proposition 4.6 Assume Assumptions 3.3, 3.4, 3.5, 3.6, and γ · (LP + 1) < 1. Then we have
for any µ ∈ P(S)
sup J˜M (µ, π ) − J(µ, π ) → 0, as M → +∞,
π ∈Π

where J˜M and J are given in (4.7) and (2.9), respectively.
Proof of Proposition 4.6 Recall from (3.11) that
JeM (µ, π̃ )

=

∞
X
t=0

17

e t , π̃ ),
γ t R(µ̃

(4.10)

m
e αm (µ̃αm , π̃ αm ), t ∈ N+ , µ̃α ≡ µ, and µ̃αm ,W given in (3.11).
subject to µ̃α
t
t
0
t+1 = Φ

J(µ, π )

=

∞
X

γ t R(µ t , π ),

t=0
α,W
α
α
α
subject to µα
given in (2.8). Since π̃ := (π̃ αm )m∈[M ] ∈
t+1 = Φ (µt , π ), t ∈ N+ , µ0 ≡ µ, and µt
e M can be viewed as a piecewise-constant projection of π ∈ Π onto Π
e M . Then,
Π
α

sup J˜M (µ, π ) − J(µ, π )
π ∈Π

≤

sup

∞
X

π ∈Π t=0

≤

sup

∞
X

π ∈Π t=0

γ t R̃(µ̃ t , π̃ ) − R(µ t , π )
t

µt , π̃ ) − R(µ
µt , π̃ ) + sup
γ R̃(µ̃

∞
X

π ∈Π t=0

µt , π̃ ) − R(µ
µt , π )
γ t R(µ

:= I + II.
In terms of the term I, we first estimate R̃(µ̃ t , π̃ ) − R(µ t , π̃ ) :
R̃(µ̃ t , π̃ ) − R(µ t , π̃ )
M Z
X

=

m=1

−

XX

m
( m−1
M , M ] s∈S a∈A

M Z
X

m
( m−1
M , M ] s∈S a∈A

m=1

m=1

−

XX

m
( m−1
M , M ] s∈S a∈A

M Z
X

XX

M Z
X

+

m=1

−

XX

m
( m−1
M , M ] s∈S a∈A

M Z
X
m=1

m
( m−1
M , M ] s∈S a∈A

m=1

m
( m−1
M ,M ]

αm
m
(a|s)dα
)µ̃α
r(s, a, µα,W
t (s)π̃
t

αm
m
r(s, a, µα,W
)µ̃α
(a|s)dα
t
t (s)π̃

XX

M Z
X

Lr ·

αm
r(s, a, µα,W
)µα
(a|s)dα
t
t (s)π̃

αm
m ,W
m
r(s, a, µ̃α
)µ̃α
(a|s)dα
t
t (s)π̃

m
( m−1
M , M ] s∈S a∈A

m=1

≤

XX

M Z
X

≤

αm
m ,W
m
r(s, a, µ̃α
)µ̃α
(a|s)dα
t
t (s)π̃

αm
)µα
(a|s)dα
r(s, a, µα,W
t
t (s)π̃

kµα,W
− µ̃tαm ,W k1 dα + Mr ·
t

M Z
X
m=1

m
( m−1
M ,M ]

αm
kµα
t − µ̃t k1 dα.

By Lemma 4.5,
I≤

C(γ, LΠ , LP , LW , Lr , Mr )
.
M

For the term II,
sup

∞
X

π ∈Π t=0

γ t R(µ t , π̃ ) − R(µ t , π )

≤

sup

∞
X

γ t Mr

π ∈Π t=0

≤

LΠ Mr 1
.
1−γ M
18

M Z
X
m=1

m

max kπ α − π α k1 dα

m
s∈S
( m−1
M ,M ]

2
e M ⊂ Π and (π 1,∗ , . . . , π N,∗ ) ∈ ΠN are optimal
Proof of Theorem 3.9 Suppose that π̃ ∗ ∈ Π
policies of the problems (4.7) and (2.5), respectively. From Proposition 4.6, for any ε > 0, there
exists sufficiently large Mε > 0
|J˜Mε (µ, π̃ ∗ ) − J(µ, π̃ ∗ )| ≤

ε
,
3

PN
where by (3.8), π N,∗ := i=1 π i,∗ 1α∈( i−1 , i ] .
N
N
From Theorem 3.7, for any ε > 0, there exists Nε such that for all N ≥ Nε
|JN (µ, π̃ 1,∗ , . . . , π̃ N,∗ ) − J(µ, π̃ ∗ )| ≤

ε
ε
, |JN (µ, π 1,∗ , . . . , π N,∗ ) − J(µ, π N,∗ )| ≤ .
3
3

Then we have
JN (µ, π̃ 1,∗ , . . . , π̃ N,∗ ) − JN (µ, π 1,∗ , . . . , π N,∗ )
≥ JN (µ, π̃ 1,∗ , . . . , π̃ N,∗ ) − J(µ, π̃ ∗ ) + J(µ, π̃ ∗ ) − J˜Mε (µ, π̃ ∗ )
{z
} |
{z
}
|
I1

I2

+ J˜Mε (µ, π̃ ∗ ) − J˜Mε (µ, π N,∗ ) + J˜Mε (µ, π N,∗ ) − JN (µ, π 1,∗ , . . . , π N,∗ )
|
{z
} |
{z
}
I3

≥

I4

ε ε ε
− − − = −ε.
3 3 3

where I3 ≥ 0 due to the optimality of π̃ ∗ for Ṽ Mε . This means that the optimal policy of block
∗
) := ΓN (π̃ ∗ ).
GMFC provides an ε-optimal policy for the multi-agent system with (π̃1∗ , . . . , π̃N
2

5

Experiments

In this section, we provide an empirical verification of our theoretical results, with two examples
adapted from existing works on learning MFGs [16, 10] and learning GMFGs [15].

5.1

SIS Graphon Model

We consider a SIS graphon model in [16] under a cooperative setting. In this model, each agent
α ∈ I shares a state space S = {S, I} and an action space A = {C, N C}, where S is susceptible,
I is infected, C represents keeping contact with others, and N C means keeping social distance.
The transition probability of each agent α is represented as follows
P (st+1 = I|st = S, at = C, µα,W
)
t

= β1 µα,W
(I),
t

P (st+1 = I|st = S, at = N C, µα,W
)
t

= β2 µα,W
(I),
t

P (st+1 = S|st = I, µα,W
)
t

= δ,

where β1 is the infection rate with keeping contact with others, β2 is the infection rate under
social distance, and δ is the fixed recovery rate. We assume 0 < β2 < β1 , meaning that keeping
social distance can reduce the risk of being infected. The individual reward function is defined
as
r(s, µα,W
, a) = −c1 1{I} (s) − c2 1{N C} (a) − c3 1{I} (s)1{C} (a),
t
19

where c1 represents the cost of being infected such as the cost of medical treatment, c2 represents
the cost of keeping social distance, and c3 represents the penalty of going out if the agent is
infected.
In our experiment, we set β1 =0.8, β2 =0, δ = 0.3 for the transition dynamics and c1 =2,
c2 =0.3, c3 = 0.5 for the reward function. The initial mean field µ0 is taken as the uniform
distribution. We set the episode length to 50.

5.2

Malware Spread Graphon Model

We consider a malware spread model in [10] under a cooperative setting. In this model, let
S = {0, 1, . . . , K − 1}, K ∈ N, denote the health level of the agent, where st = 0 and st = K − 1
represents the best level and the worst level, respectively. All agents can take two actions:
at = 0 means doing nothing, and at = 1 means repairing. The state transition is given by

st + b(K − st )χt c, if at = 0,
st+1 =
0,
if at = 1,
where χt , t ∈ N are i.i.d. random variables with a certain probability distribution. Then after
taking action at , agent α will receive an individual reward
i)st /K − c2 at .
, at ) = −(c1 + hµα,W
r(st , µα,W
t
t
Here considering the heterogeneity of agents, we use W (α, β) to denote the importance effect
R
P
i := β∈I s∈S sW (α, β)µβt (s)dβ is the risk of being infected by
of agent β on agent α. hµα,W
t
other agents and c2 is the cost of taking action at .
In our experiment, we set K=3, c1 =0.3, and c2 =0.5. In addition, to stabilize the training
of the RL agent, we fix χt to a static value, i.e., 0.7. In this model, we set the episode length to
10.

5.3

Performance of N-agent GMFC on Multi-Agent System

For both models, we use PPO [47] to train the block GMFC agent in the infinite-agent environment and obtain the policy ensembles and further use Algorithm 1 to deploy them in the
finite-agent environment. We test the performance of N-agent GMFC with 10 blocks to different
numbers of agents, i.e., from 10 to 100. For each case, we run 1000 times of simulations and
show the mean and standard variation (Green shadows in Figure 1 and Figure 2) of the mean
episode reward. We can see that in both scenarios and for different types of graphons, the mean
episode rewards of the N-agent GMFC become increasingly close to that of block GMFC as
the number of agents grows. (See Figure 1 and Figure 2). This verifies our theoretical findings
empirically.

Figure 1: Experiments for different graphons in SIS finite-agent environment

20

Figure 2: Experiments for different graphons in Malware Spread finite-agent environment

5.4

Comparison with Other Algorithms

For different types of graphons, we compare our algorithm N-agent GMFC with three existing
MARL algorithms, including two independent learning algorithms, i.e., independent DQN [40],
independent PPO [47] and a powerful centralized-training-and-decentralized-execution(CTDE)based algorithm QMIX [46]. We test the performance of those algorithms with different numbers
of blocks, i.e., 2, 5, 10, to the multi-agent systems with 40 agents. The results are reported in
Table 1 and Table 2.
In the SIS graphon model, N-agent GMFC shows dominating performance in most cases
and outperforms independent algorithms by a large margin. Only QMIX can reach comparable
results. And in the malware spread graphon model, N-agent GMFC outperforms other algorithms in more than half of the cases. Only independent DQN has comparable performance in
this environment. And we can see that in both environments, the performance gap between
N-agent GMFC and other MARL algorithms is shrinking as the number of blocks goes larger.
This is mainly because the action space of block GMFC increases more quickly than MARL
algorithms as the block number increases. And it is hard to train RL agents when the action
space is too large.
Beyond the visible results shown in Tables 1 and 2, when the number of agents N grows
larger, classic MARL methods become infeasible because of the curse of dimensionality and the
restriction of memory storage, while N-agent GMFC is trained only once and independent of the
number of agents N , hence is easier to scale up in a large-scale regime and enjoys a more stable
performance. We can see that N-agent GMFC shows more stable results when N increases as
shown in Figure 1 and Figure 2.
Table 1: Mean Episode Reward for SIS with 40 agents
Graphon Type

Erdős Rényi

Stochastic Block

Random Geometric

Algorithm

M
2
5
10
2
5
10
2
5
10

N-agent GMFC

I-DQN

I-PPO

QMIX

-15.37
-15.74
-15.67
-13.58
-13.67
-13.57
-12.45
-9.82
-10.52

-17.58
-16.17
-17.55
-16.05
-15.91
-15.52
-17.93
-12.81
-11.68

-20.63
-20.42
-21.38
-18.38
-20.13
-14.87
-14.82
-12.99
-12.66

-20.51
16.94
-14.45
-17.69
-13.79
-13.86
-14.52
-10.84
-12.60

21

Table 2: Mean Episode Reward for Malware Spread with 40 agents
Graphon Type

Erdős Rényi

Stochastic Block

Random Geometric

5.5

Algorithm

M
2
5
10
2
5
10
2
5
10

N-agent GMFC

I-DQN

I-PPO

QMIX

-5.21
-5.21
-5.21
-5.16
-5.10
-5.09
-5.02
-4.85
-4.82

-5.11
-5.30
-5.14
-5.21
-5.19
-5.05
-5.21
-5.03
-4.83

-5.31
-5.26
-5.27
-5.37
-5.31
-5.28
-5.27
-5.04
-5.14

-6.05
-6.13
-5.21
-5.88
-5.70
-5.27
-5.35
-5.05
-4.83

Implementation Details

We use three graphons in our experiments: (1) Erdős Rényi: W (α, β) = 0.8; (2) Stochastic
block model: W (α, β) = 0.9, if 0 ⩽ α, β ⩽ 0.5 or 0.5 ⩽ α, β ⩽ 1, W (α, β) = 0.4, otherwise;
(3)
x
Random geometric graphon: W (α, β) = f (min(|β − α|, 1 − |β − α|)), where f (x) = e− 0.5−x .
For the RL algorithms, we use the implementation of RLlib [36] (version 1.11.0, Apache-2.0
license). For PPO used to learn an optimal policy ensemble in block GFMC, we use a 64dimensional linear layer to encode the observation and 2-layer MLPs with 256 hidden units per
layer for both value network and actor network. For independent DQN and independent PPO,
we use the default weight-sharing model with 64-dimensional embedding layers. We train the
GMFC PPO agent for 1000 iterations, and other three MARL agents for 200 iterations. The
specific hyper-parameters are listed in Table 3.
Table 3: RL Algorithm Settings
Algorithms
Learning rate
Learning rate decay
Discount factor
Batch size
KL coefficient
KL target
Buffer size
Target network update frequency

6

GMFC PPO

I-DQN

I-PPO

QMIX

0.0005
True
0.95
128
0.2
0.01
-

0.0005
True
0.95
128
2000
2000

0.0001
True
0.95
128
0.2
0.01
-

0.00005
False
0.95
128
2000
1000

Conclusion

In this work, we have proposed a discrete-time GMFC framework for MARL with nonuniform
interactions on dense graphs. Theoretically, we have shown that under suitable assumptions,
GMFC approximates MARL well with approximation error of order O( √1N ). To reduce the
dimension of GMFC, we have introduced block GMFC by discretizing the graphon index and
shown that it also approximates MARL well. Empirical studies on several examples have verified
the plausibility of the GMFC framework. For future research, we are interested in establishing
22

theoretical guarantees of the PPO-based algorithm for block GMFC, learning the graph structure of MARL and extending our framework to MARL with nonuniform interactions on sparse
graphs.

References
[1] Adler, J.L., Blue, V.J., 2002. A cooperative multi-agent transportation management and route
guidance system. Transportation Research Part C: Emerging Technologies 10, 433–454.
[2] Angiuli, A., Fouque, J.P., Laurière, M., 2022. Unified reinforcement Q-learning for mean field
game and control problems. Mathematics of Control, Signals, and Systems , 1–55.
[3] Bayraktar, E., Chakraborty, S., Wu, R., 2020. Graphon mean field systems. arXiv preprint
arXiv:2003.13180 .
[4] Bet, G., Coppini, F., Nardi, F.R., 2020. Weakly interacting oscillators on dense random graphs.
arXiv preprint arXiv:2006.07670 .
[5] Caines, P.E., Huang, M., 2019. Graphon mean field games and the GMFG equations: ε-Nash
equilibria, in: 2019 IEEE 58th conference on decision and control (CDC), IEEE. pp. 286–292.
[6] Cardaliaguet, P., Lehalle, C.A., 2018. Mean field game of controls and an application to trade
crowding. Mathematics and Financial Economics 12, 335–363.
[7] Carmona, R., Cooney, D.B., Graves, C.V., Lauriere, M., 2022. Stochastic graphon games: I. the
static case. Mathematics of Operations Research 47, 750–778.
[8] Carmona, R., Delarue, F., 2013. Probabilistic analysis of mean-field games. SIAM Journal on
Control and Optimization 51, 2705–2734.
[9] Carmona, R., Laurière, M., Tan, Z., 2019. Model-free mean-field reinforcement learning: mean-field
MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802 .
[10] Chen, Y., Liu, J., Khoussainov, B., 2021. Agent-level maximum entropy inverse reinforcement
learning for mean field games. arXiv preprint arXiv:2104.14654 .
[11] Choi, J., Oh, S., Horowitz, R., 2009. Distributed learning and cooperative control for multi-agent
systems. Automatica 45, 2802–2814.
[12] Cortes, J., Martinez, S., Karatas, T., Bullo, F., 2004. Coverage control for mobile sensing networks.
IEEE Transactions on Robotics and Automation 20, 243–255.
[13] Cui, J., Liu, Y., Nallanathan, A., 2019. Multi-agent reinforcement learning-based resource allocation for UAV networks. IEEE Transactions on Wireless Communications 19, 729–743.
[14] Cui, K., Koeppl, H., 2021. Approximately solving mean field games via entropy-regularized deep reinforcement learning, in: International Conference on Artificial Intelligence and Statistics, PMLR.
pp. 1909–1917.
[15] Cui, K., Koeppl, H., 2022. Learning graphon mean field games and approximate Nash equilibria,
in: International Conference on Learning Representations.
[16] Cui, K., Tahir, A., Sinzger, M., Koeppl, H., 2021. Discrete-time mean field control with environment states, in: 2021 60th IEEE Conference on Decision and Control (CDC), IEEE. pp. 5239–5246.
[17] Delarue, F., 2017. Mean field games: A toy model on an Erdös-Renyi graph. ESAIM: Proceedings
and Surveys 60, 1–26.
[18] Elie, R., Perolat, J., Laurière, M., Geist, M., Pietquin, O., 2020. On the convergence of model free
learning in mean field games, in: Proceedings of the AAAI Conference on Artificial Intelligence,
pp. 7143–7150.
[19] Esunge, J.N., Wu, J., 2014. Convergence of weighted empirical measures. Stochastic Analysis and
Applications 32, 802–819.
[20] Flyvbjerg, H., Sneppen, K., Bak, P., 1993. Mean field theory for a simple model of evolution.
Physical review letters 71, 4087.

23

[21] Gao, S., Caines, P.E., 2019a. Graphon control of large-scale networks of linear systems. IEEE
Transactions on Automatic Control 65, 4090–4105.
[22] Gao, S., Caines, P.E., 2019b. Spectral representations of graphons in very large network systems
control, in: 2019 IEEE 58th conference on decision and Control (CDC), IEEE. pp. 5068–5075.
[23] Gu, H., Guo, X., Wei, X., Xu, R., 2019. Dynamic programming principles for mean-field controls
with learning. arXiv preprint arXiv:1911.07314 .
[24] Gu, H., Guo, X., Wei, X., Xu, R., 2021. Mean-field controls with Q-learning for cooperative
MARL: convergence and complexity analysis. SIAM Journal on Mathematics of Data Science 3,
1168–1196.
[25] Guo, X., Hu, A., Xu, R., Zhang, J., 2019. Learning mean-field games. Advances in Neural
Information Processing Systems 32.
[26] Hadikhanloo, S., Silva, F.J., 2019. Finite mean field games: fictitious play and convergence to
a first order continuous mean field game. Journal de Mathématiques Pures et Appliquées 132,
369–397.
[27] Huang, M., Malhamé, R.P., Caines, P.E., 2006. Large population stochastic dynamic games:
closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle. Communications
in Information & Systems 6, 221–252.
[28] Hughes, E., Leibo, J.Z., Phillips, M., Tuyls, K., Dueñez-Guzman, E., García Castañeda, A.,
Dunning, I., Zhu, T., McKee, K., Koster, R., et al., 2018. Inequity aversion improves cooperation
in intertemporal social dilemmas. Advances in neural information processing systems 31.
[29] Lacker, D., Soret, A., 2022. A label-state formulation of stochastic graphon games and approximate
equilibria on large networks. arXiv preprint arXiv:2204.09642 .
[30] Lasry, J.M., Lions, P.L., 2007. Mean field games. Japanese Journal of Mathematics 2, 229–260.
[31] Lee, J.W., Park, J., Jangmin, O., Lee, J., Hong, E., 2007. A multiagent approach to Q-learning
for daily stock trading. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems
and Humans 37, 864–877.
[32] Lee, J.W., Zhang, B.T., 2002. Stock trading system using reinforcement learning with cooperative
agents, in: Proceedings of the Nineteenth International Conference on Machine Learning, pp.
451–458.
[33] Leibo, J.Z., Zambaldi, V., Lanctot, M., Marecki, J., Graepel, T., 2017. Multi-agent reinforcement
learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037 .
[34] Lerer, A., Peysakhovich, A., 2017. Maintaining cooperation in complex social dilemmas using deep
reinforcement learning. arXiv preprint arXiv:1707.01068 .
[35] Li, Y., Wang, L., Yang, J., Wang, E., Wang, Z., Zhao, T., Zha, H., 2021. Permutation invariant policy optimization for mean-field multi-agent reinforcement learning: A principled approach.
arXiv preprint arXiv:2105.08268 .
[36] Liang, E., Liaw, R., Nishihara, R., Moritz, P., Fox, R., Goldberg, K., Gonzalez, J.E., Jordan,
M.I., Stoica, I., 2018. RLlib: Abstractions for distributed reinforcement learning, in: International
Conference on Machine Learning (ICML).
[37] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D., 2016.
Continuous control with deep reinforcement learning, in: International Conference on Learning
Representations.
[38] Lin, Y., Qu, G., Huang, L., Wierman, A., 2021. Multi-agent reinforcement learning in stochastic
networked systems. Advances in Neural Information Processing Systems 34, 7825–7837.
[39] Lovász, L., 2012. Large networks and graph limits. volume 60. American Mathematical Soc.
[40] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.,
2013. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 .
[41] Mondal, W.U., Agarwal, M., Aggarwal, V., Ukkusuri, S.V., 2022a. On the approximation of
cooperative heterogeneous multi-agent reinforcement learning (MARL) using mean field control
(MFC). Journal of Machine Learning Research 23, 1–46.

24

[42] Mondal, W.U., Aggarwal, V., Ukkusuri, S.V., 2022b. Can mean field control (MFC) approximate
cooperative multi agent reinforcement learning (MARL) with non-uniform interaction? arXiv
preprint arXiv:2203.00035 .
[43] Motte, M., Pham, H., 2022. Mean-field Markov decision processes with common noise and openloop controls. The Annals of Applied Probability 32, 1421–1458.
[44] Pasztor, B., Bogunovic, I., Krause, A., 2021. Efficient model-based multi-agent mean-field reinforcement learning. arXiv preprint arXiv:2107.04050 .
[45] Qu, G., Wierman, A., Li, N., 2020. Scalable reinforcement learning of localized policies for multiagent networked systems, in: Learning for Dynamics and Control, PMLR. pp. 256–266.
[46] Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., Whiteson, S., 2018. QMIX:
Monotonic value function factorisation for deep multi-agent reinforcement learning, in: International Conference on Machine Learning, PMLR. pp. 4295–4304.
[47] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O., 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
[48] Subramanian, J., Mahajan, A., 2019. Reinforcement learning in stationary mean-field games, in:
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,
pp. 251–259.
[49] Vasal, D., Mishra, R., Vishwanath, S., 2021. Sequential decomposition of graphon mean field
games, in: 2021 American Control Conference (ACC), IEEE. pp. 730–736.
[50] W Axhausen, K., Horni, A., Nagel, K., 2016. The multi-agent transport simulation MATSim.
Ubiquity Press.
[51] Wainwright, M.J., 2019. High-dimensional statistics: A non-asymptotic viewpoint. volume 48.
Cambridge University Press.
[52] Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., Wang, J., 2018. Mean field multi-agent reinforcement learning, in: International Conference on Machine Learning, PMLR. pp. 5571–5580.
[53] Yin, H., Mehta, P.G., Meyn, S.P., Shanbhag, U.V., 2010. Learning in mean-field oscillator games,
in: 49th IEEE Conference on Decision and Control (CDC), IEEE. pp. 3125–3132.

25

