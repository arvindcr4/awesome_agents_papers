1

Learning Efficient Communication Protocols for
Multi-Agent Reinforcement Learning

arXiv:2511.09171v1 [cs.MA] 12 Nov 2025

Xinren Zhang, Jiadong Yu, IEEE Member, Zixin Zhong
Abstractâ€”Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for modeling complex interactions among
autonomous entities in distributed environments. In Multi-Agent
Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since
agents may generate redundant or non-essential messages. While
prior work has focused on boosting task performance with
information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of
communication protocols (communication topology and message).
To fill this gap, we introduce a generalized framework for learning
multi-round communication protocols that are both effective and
efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the
learning process: the Information Entropy Efficiency Index (IEI)
and Specialization Efficiency Index (SEI) for efficiency-augmented
optimization, and the Topology Efficiency Index (TEI) for explicit
evaluation. We integrate IEI and SEI as the adjusted loss functions
to promote informative messaging and role specialization, while
using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments,
we demonstrate that our learned communication protocol can
significantly enhance communication efficiency and achieves better
cooperation performance with improved success rates.

I. I NTRODUCTION
Multi-Agent Systems (MAS) provide a framework for designing and analyzing distributed autonomous entities that interact within shared environments to achieve individual or collective goals [1]. Multi-Agent Reinforcement Learning (MARL)
extends traditional single-agent Reinforcement Learning (RL)
to such multi-agent settings, enabling agents to develop adaptive cooperative or competitive behaviors that effectively model
complex systems where single-agent approaches prove insufficient [2]â€“[5]. However, this extension introduces significant
challenges, including nonstationarity from concurrent learning
dynamics, scalability concerns with increasing agent numbers,
and the intricacy of achieving effective coordination [6].
These challenges in MARL are fundamentally rooted in partial observability [7], where agents cannot access the complete
system state and thus struggle to determine which information
is task-relevant or beneficial to other agents. Meanwhile, as the
number of agents in future wireless networks grows, the complexity of coordination intensifies significantly, especially when
handling high-dimensional data in uncertain environments. In
this context, introducing communication protocols in MARL
offers a promising solution by enabling agents to share information to collectively reduce environmental uncertainty [8]â€“[12].
X. Zhang, J. Yu, and Z. Zhong are with the Hong Kong University of Science
and Technology (Guangzhou).

The communication protocol in MAS defines the rules and
mechanisms governing agent interactions, encompassing two
fundamental dimensions: communication topology (the connectivity structure and temporal coordination among agents) and
message content (the information encoded for transmission).
Communication protocols can be broadly categorized into three
paradigms: engineered, hybrid, and learned approaches.
Engineered protocols rely on technical specifications and
predefined structures grounded in rigorous theoretical foundations [13]. Early work established technical methods through
state transition models [14] and recoverability analysis using Petri Nets [15], which are later extended to systematic
analysis of network control protocols via switched-system
approaches [16]. These foundations enable diverse applications
including cryptographic authentication for UAV networks [17],
secure routing with intrusion detection [18], and event-triggered
formation control [19]. While providing reliability and theoretical guarantees, engineered protocols suffer from limited
adaptability to dynamic environments.
Hybrid approaches integrate structured frameworks with
adaptive elements to address this limitation [20]. Recent advances incorporate higher-level intelligence through
communication-centric architectures for LLM-based MAS that
balance predefined structures with adaptive capabilities [21],
and AI/ML-driven semantic protocols enable distributed sensing and intelligent resource allocation in next-generation networks [22]. Despite these advances, hybrid methods remain
fundamentally constrained by their dependence on predetermined rules, unable to fully escape the limitations of their
structured foundations.
Learned protocols, as employed in MARL, enable agents
to autonomously develop communication strategies through
continuous interaction, fundamentally overcoming the rigidity
of traditional approaches. Unlike predefined communication
protocols, learned approaches enable agents to automatically
discover and transmit task-specific representations, selectively
encoding only the information from local observations that is
relevant to achieving their objectives. The evolution of learned
communication protocols has progressed through several architectural innovations. Early approaches introduced differentiable
broadcasting mechanisms [8], [23], enabling all agents to
share information through gradient-based optimization. Subsequent developments incorporated selective communication
via context-dependent gating [11], allowing agents to dynamically determine when to communicate based on environmental
conditions. More recent architectures leverage attention-based
message prioritization [24] to weight information relevance,

2

and support heterogeneous agent structures [25] that enable
specialized roles and adaptive coordination. This learning-based
paradigm adapts dynamically to environmental changes and
evolving task requirements, making it particularly suitable for
complex, uncertain scenarios where predefined communication
rules prove insufficient.
However, application of MARL algorithms with communication protocols faces critical resource constraints, as realworld limitations in bandwidth, energy, and computational
capacity [26] directly impact system feasibility and scalability.
Traditional MARL evaluation focuses primarily on task success
rates and convergence speed, often overlooking the communication resources required to achieve these outcomes. This
gap is problematic as excessive communication overhead can
negate the benefits of learned protocols, making communication
efficiency a fundamental dimension for transitioning MARL
from theoretical research to practical deployment under realworld resource limitations.
Communication efficiency assessment hinges on three key
aspects. First, information entropy per successful task completion reveals how effectively agents minimize redundancy while
maintaining superior performance [27]. Achieving objectives
with minimal information transfer maximizes the utility of
each transmitted bit. We propose the Information Entropy
Efficiency Index (IEI) to quantify how effectively agents
encode task-relevant information into compact representations.
Second, information diversity during task completion significantly affects system performance [28]. In complex cooperative tasks, heterogeneous roles and specialized sharing
lead to superior outcomes, while highly similar information
creates redundancy that fails to leverage distributed multiagent perception. Diverse communication enables agents to
partition the information space, with each agent contributing
complementary insights for effective division of the task. We
propose the Specialization Efficiency Index (SEI) to assess
agent differentiation through message diversity.
Third, communication cost-effectiveness reveals how efficiently agents extract value from limited resources [29]. This
return-on-investment directly impacts scalability and deployment feasibility, as inefficient protocols cause exponential
bandwidth growth in large-scale applications. We propose the
Topology Efficiency Index (TEI) to quantify task success
relative to communication costs.
Together, these three communication efficiency metrics
(CEMs)-IEI, SEI, and TEI-form a comprehensive framework
for evaluating communication efficiency in MAS. By incorporating IEI and SEI directly into the policy update process
through refined loss functions, agents are guided toward developing more efficient communication protocols, while using TEI
as an evaluation metric to assess overall communication costeffectiveness. This approach addresses a critical gap in current
MARL research, where communication efficiency is typically
an afterthought rather than an explicit optimization objective.
The contribution of this paper can be summarized as follows:
â€¢ Develop a generalized framework for MARL that accom-

modates multi-round communication scenarios, providing
a unified approach for analyzing diverse MARL architectures.
â€¢ Design three comprehensive metrics (IEI, SEI, and TEI)
that quantify different dimensions of communication efficiency in MARL systems, enabling systematic comparison
across algorithms.
â€¢ Propose loss function refinements that explicitly optimize Î¦IEI and Î¦SEI to enable learning of efficiencyaugmented communication protocols, with performance
validated through comprehensive Î¦TEI assessment.
â€¢ Conduct comprehensive experiments demonstrating that
our learned communication protocols substantially improve communication efficiency while achieving enhanced
cooperative performance.
The paper is organized as follows: Section II reviews related
literature on the learned communication protocol in MARL.
Section III introduces our generalized MARL model that accommodates multi-round communication scenarios. Section IV
proposes three novel metrics for evaluation of communication
efficiency and presents comparative experimental results across
different baseline algorithms. Section V incorporates these
metrics into loss functions during policy update to enhance both
communication efficiency and task performance. Section VI
presents the experiment results of the proposed efficiencyaugmented method. Finally, Section VII summarizes our contributions and findings.
II. R ELATED W ORKS
The existing literature offers numerous MARL algorithms
with learned communication protocols that enable agents to autonomously develop coordination strategies through experience.
These learned protocols comprise communication topology and
message content. We systematically review these approaches
by first examining how different protocol aspects are learned,
and next discussing the technical mechanisms that enable this
learning, with particular focus on attention mechanisms which
is an increasingly prevalent tool for enhancing protocol design.
A. Learned Communication Protocols
Learned communication protocols in MARL can be categorized based on which aspects of coordination they address:
topology control, message content design, or both of them.
1) Topology Control: Communication topology determines
the structure and timing of information flow, addressing when
agents should communicate and with whom. Foerster et al. [23]
pioneered this direction with RIAL and DIAL, two foundational
frameworks that established learnable topology as a component of coordination strategy. RIAL combined deep recurrent
Q-networks with independent Q-learning, conditioning each
agentâ€™s decisions on hidden states, observations, and received
messages through cross-timestep broadcasting. DIAL advanced
this by enabling gradient flow among agents during centralized
learning, using real-valued communication vectors during training that are later discretized during execution.

3

Recognizing that maintaining permanent communication
links may be inefficient, Singh et al. [11] developed IC3Net,
which incorporates a gating mechanism that enables agents
to learn when communication is necessary across cooperative,
competitive, and mixed scenarios. This framework employs
independent LSTM controllers with individualized reward signals, utilizing a gating function to modulate the weighted
aggregation of other agentsâ€™ hidden states. This approach
represents a significant shift from uniform broadcasting toward
selective, context-dependent communication activation. Building upon this concept, SchedNet [30] further advances selective
communication by proposing a weight-based scheduler that
dynamically determines which agents should broadcast their
messages at each time step.
Niu et al. [24] further advanced topology control through
MAGIC, which models communication as a learnable directed
graph. The framework integrates a scheduler to determine when
and to whom to communicate, producing adjacency matrices
for targeted message passing through multiple rounds, enabling
selective message conveyance and aggregation based on both
local and global context.
2) Message Content Design: Message content design determines what information is transmitted and how it is encoded,
fundamentally affecting the informativeness and efficiency of
inter-agent coordination. The predominant paradigm focuses
on hidden state sharing, where communication is realized by
broadcasting internal representations.
Sukhbaatar et al. [8] introduced CommNet, exemplifying
this approach through a neural network framework enabling
fully cooperative agents to learn communication alongside their
policy. Each agent broadcasts its hidden state as a continuous
vector message, with these messages being averaged to form
communication input. This end-to-end differentiable system can
be trained via standard backpropagation and be combined with
RL algorithms for partially observable environments. While
simple and effective in small cooperative tasks, hidden state
sharing tends to produce redundant signals as agent populations grow, motivating the development of more selective
mechanisms that apply additional transformations to hidden
states. In HetNet [25], each agent generates its hidden state
by processing class-specific observations through a tailored
module comprising a CNN/fully-connected layer and an LSTM
cell. This hidden state is then transformed via a class-specific
weight matrix and encoder to form the communication message.
In order to enhance bandwidth efficiency, the message can be
further binarized using the Gumbel-Softmax function. Finally,
receiving agents decode incoming messages and aggregate them
using class-adapted attention coefficients, which are subsequently fused with their own hidden states.
3) Joint Topology-Content Learning: Recent approaches address both topology and content simultaneously, recognizing
their interdependence in effective communication protocols.
Das et al. [12] developed TarMAC, implementing targeted
communication where agents learn both what messages to send
(content) and whom to address (topology). Communication

messages consist of a signature encoding properties of intended recipients and a value containing the actual message
content. The model implements multi-round communication
where agents use query vectors to compute attention weights,
creating targeted information flow that adapts dynamically to
task context.
Liu et al. [10] proposed G2ANet, advancing joint learning
through a two-stage design combining hard and soft attention.
The framework employs hard attention to identify relevant
agent interactions (topology selection) and soft attention to
learn relationship importance (message weighting). Using a
graph neural network structure, the model determines which
interactions to maintain and their relative importance, enabling
efficient communication across diverse scenarios.
B. Attention Mechanisms in MARL Communication
Attention mechanisms have emerged as a key technical
approach for improving communication protocols, enabling selective processing of information for both topology control and
message design. These mechanisms overcome the inefficiency
of indiscriminate broadcasting by introducing selectivity and
targeting into learned protocols.
1) Attention for Topology Selection: Attention mechanisms
enable dynamic determination of communication partners based
on task context. ATOC [9] employs an attention mechanism to
decide if an agent should communicate in its observable field.
MAGIC [24] employs attention-based scheduling to produce
adjacency matrices that specify directed communication links,
allowing agents to selectively determine whom to communicate
with at each timestep. G2ANet [10] implements hard attention
for topology selection, using binary gating to identify relevant agent interactions while pruning irrelevant connections.
This selective activation reduces communication overhead by
establishing sparse connectivity patterns that adapt to task
requirements.
2) Attention for Message Aggregation: Beyond topology
selection, attention mechanisms enable intelligent processing
and integration of received messages. TarMAC [12] implements
signature-based soft attention where agents compute attention weights through dot products between query vectors and
message signatures, enabling targeted information extraction
from multi-agent communications. G2ANet [10] employs soft
attention to weight the importance of different agent contributions, processing communication through weighted sums of
hidden states. MAGIC [24] employs attention-based message
processor to help agents integrate messages for intelligent
decision making. This attention-weighted aggregation allows
agents to prioritize relevant information while filtering out less
pertinent signals, improving both communication efficiency and
coordination quality.
Despite these advances in learned protocols and attention
mechanisms, evaluation of these frameworks often lacks practical insights for real-world deployment where resource constraints play critical roles. Meanwhile, some approaches employ
one-round communication while others implement multiple

4

rounds, highlighting the need for systematic evaluation that
considers both communication efficiency and task performance
in resource-constrained environments.
III. MARL F RAMEWORK WITH C OMMUNICATION
P ROTOCOL L EARNING
To provide a unified formulation for existing MARL communication protocol learning approaches, we propose a generalized framework grounded in a partially observable multi-agent
Markov game with an ğ¿-round communication protocol. We
formulate the framework as a tuple âŸ¨S, A, ğ‘‡, O, Î©, ğ‘…, ğ‘, ğ›¾, ğ¿âŸ©.
Here, S represents the global state space of the environment,
A encompasses the action space of agents, ğ‘‡ : S Ã— A â†’ S
denotes the transition function, O constitutes the observation
space of agents, Î© : S â†’ O signifies the observation function,
ğ‘… : S Ã— A â†’ R characterizes the reward function, ğ‘ indicates
the number of agents, ğ›¾ is the discount factor, and ğ¿ counts
the rounds of communication protocol.
The proposed generalized model employs the Centralized
Training with Decentralized Execution (CTDE) paradigm over
multi-round communication. It operates in two distinct stages:
centralized training stage and decentralized execution stage.
Each stage comprises three fundamental components: (1) observation processing, (2) multi-round communication protocol,
and (3) policy optimization or decision making. During the
centralized training stage, global information is leveraged to
optimize policies and communication mechanisms simultaneously, while in the decentralized execution stage, agents
operate independently using only local observations and learned
communication protocols. The overall architecture of MARL
with ğ¿-round communications is shown in Fig. 1.1
A. Centralized Training Stage
During the training phase, global information is leveraged
to facilitate more effective learning. As shown in Fig. 1(a),
the centralized training framework consists of the following
integrated components:
1) Observation Processing: Each agent ğ‘– âˆˆ [0, ğ‘ âˆ’ 1]
receives its local observation ğ‘œğ‘–ğ‘¡ from the environment with
global state ğ‘ ğ‘¡ . During the centralized training phase, these
observations are processed collectively. The collective observations oğ‘¡ = [ğ‘œ0ğ‘¡ , ğ‘œ1ğ‘¡ , . . . , ğ‘œ ğ‘¡ğ‘ âˆ’1 ] are transformed through a shared
observation encoder into initial hidden states:
hğ‘¡
ğ‘¡ (0)

ğ‘¡ (0)

ğ‘¡ (0)

(0)

= ğ‘“ğœ½ğ‘œobs (oğ‘¡ ),

(1)

ğ‘¡ (0)

where h
= [â„0 , â„1 , . . . , â„ ğ‘ âˆ’1 ] represents the collection
of all agentsâ€™ initial hidden states, and ğœ½ obs is the parameter set
used to encode observations. ğœ½ obs is shared across all agents
to promote consistency in feature extraction while maintaining
the ability to process agent-specific observations. These initial
hidden states serve as the foundation for the subsequent communication rounds.
1 Note that ğ¿ may differ between training and execution: there are ğ¿
1
communication rounds during centralized training and ğ¿2 rounds during
decentralized execution, where ğ¿1 â‰  ğ¿2 in general.

2) Multi-Round Communication Protocol: The training process incorporates ğ¿ 1 communication rounds of information
exchange among agents before action selection. For each communication round ğ‘™ âˆˆ {1, 2, Â· Â· Â· , ğ¿ 1 }, agents engage in the
following steps:
â€¢ Step 1: Message Encoding. Each agent encodes its current hidden state from the previous round into an initial
message vector:
mâˆ—ğ‘¡

(ğ‘™)

(ğ‘™)

(ğ‘™)

(ğ‘™âˆ’1)

= ğ‘“ğœ½ğ‘šmsg (hğ‘¡
(ğ‘™)

),

(2)

(ğ‘™)

where mâˆ—ğ‘¡ = [ğ‘š 0âˆ—ğ‘¡ , ğ‘š 1âˆ—ğ‘¡ , . . . , ğ‘š âˆ—ğ‘¡
ğ‘ âˆ’1 ] represents the
collection of all agentsâ€™ initial messages. The function
ğ‘“ğœ½ğ‘šmsg can take various forms such as an identity transform
(ğ‘™âˆ’1)

(ğ‘™)

mapping ğ‘š ğ‘–âˆ—ğ‘¡ = â„ğ‘– ğ‘¡
, a linear transformation where
(ğ‘™âˆ’1)
(ğ‘™)
ğ‘š ğ‘–âˆ—ğ‘¡ = ğ‘Š ğ‘™ â„ğ‘– ğ‘¡
+ ğ‘ ğ‘™ with learnable weights ğ‘Š ğ‘™ and bias
ğ‘ ğ‘™ , or an attention-based transform operating on the collective hidden states to capture cross-agent dependencies.
â€¢ Step 2: Communication Topology Selection. The centralized controller determines the communication topology
(ğ‘™)
ğº ğ‘¡ âˆˆ R ğ‘ Ã— ğ‘ based on the latest hidden state:
ğºğ‘¡

(ğ‘™)

topo

= ğ‘“ğœ½ topo (hğ‘¡

(ğ‘™âˆ’1)

),

(3)

(ğ‘™)

where ğº ğ‘–,ğ‘¡ ğ‘— = 1 indicates that agent ğ‘– establishes a
communication channel with agent ğ‘—. During training,
this topology can be predefined based on domain knowledge [8], determined dynamically through an attention
mechanism that weighs the importance of different communication links [9], [11], [24], or learned via a dedicated
neural network component that adapts the communication
structure according to the current state of the environment
and agents [31].
â€¢ Step 3: Message Aggregation. Based on the initial message
vectors and the communication topology, the information
can be aggregated from connected agents:
mğ‘¡

(ğ‘™)

aggr

(ğ‘™)

(ğ‘™)

(ğ‘™)

(ğ‘™)

= ğ‘“ğœ½ aggr (mâˆ—ğ‘¡ , ğº ğ‘¡ ),
(ğ‘™)

(4)

(ğ‘™)

where mğ‘¡ = [ğ‘š 0ğ‘¡ , ğ‘š 1ğ‘¡ , . . . , ğ‘š ğ‘¡ğ‘ âˆ’1 ] represents the collection of all agentsâ€™ aggregated messages. The function
aggr
ğ‘“ğœ½ aggr can implement various aggregation mechanisms such
Ã
(ğ‘™)
(ğ‘™)
as simple summation where ğ‘š ğ‘–ğ‘¡ = ğ‘— âˆˆ N ğ‘¡ (ğ‘™) ğ‘š âˆ—ğ‘¡ğ‘—
[8],
ğ‘–
[11], graph attention that assigns different weights to mesÃ
(ğ‘™)
(ğ‘™)
(ğ‘™)
sages via ğ‘š ğ‘–ğ‘¡ = ğ‘— âˆˆ N ğ‘¡ (ğ‘™) ğ›¼ğ‘–ğ‘¡ ğ‘— ğ‘š âˆ—ğ‘¡ğ‘— with learned attention
ğ‘–

(ğ‘™)

coefficients ğ›¼ğ‘–ğ‘¡ ğ‘— [10], [12], and more complex attentionweighted functions that take both message content and
graph structure into account to determine the importance
of each message within the agentâ€™s receptive field [24],
[31], [32].
â€¢ Step 4: Hidden State Update. Following message aggregation, each agent updates its hidden state based on
its previous hidden state and the aggregated message
information:
hğ‘¡

(ğ‘™)

= ğ‘“ğœ½hsu
(hğ‘¡
hsu

(ğ‘™âˆ’1)

(ğ‘™)

, mğ‘¡ ),

(5)

5

â‘  Observation Processing

st

â‘  Observation Processing

ot

o0t

Obs. Enc.

Obs. Enc.

ht

( 0)

...

hï€°t

hit

Msg. Enc.

Msg. Content Comm. Topo.
(1)
(1)
Gt G ï€½ ï€±
m*t

Topo. Selec.

(ï€°)

(ï€°)

Msg. Enc.

Msg. Content Comm. Topo.

mï€°*t

ij

Gij ï€½ ï€°

(ï€±)

Obs. Enc.
hNt ï€­ï€±

â‘¡ Multi-Round Communication Protocol

Topo. Selec.

Gt ïƒ ï‚¡ N ï‚´N

...

Obs. Enc.

(ï€°)

â‘¡ Multi-Round
Communication Protocol
Msg. Enc.

oNt ï€­1

oit

Msg. Content Comm. Topo.
(ï€±)
Git ïƒ ï‚¡ï€±ï‚´ N
mi*t

(ï€±)

( ï€±)

Gï€°t ïƒ ï‚¡ï€±ï‚´ N

(ï€± )

...

i, j ïƒ [ï€±, N ]

Msg. Enc.

Topo. Selec.

Topo. Selec.

Msg. Content Comm. Topo.
(ï€± )
GNt ï€­ï€± ïƒ ï‚¡ï€±ï‚´ N
m*tN ï€­ï€±
(ï€± )

...

...

N Agents

N Agents

N Agents

...

...

...

...

N Agents

...

Recv. Msg. Set ï»mtj ï½ j ï‚¹0

Recv. Msg. Set ï»mtj ï½ j ï‚¹i

Recv. Msg. Set ï»mtj ï½ j ï‚¹ N ï€­1

Msg. Aggr.

Msg. Aggr.

Msg. Aggr.

Hidden State
Update

Hidden State
Update

Hidden State
Update

(1)

hNt ï€­ï€±

(1)

...

ht

Msg. Aggr.

...

(l )

(l )

...

...

...

Hidden State
Update

( l ï€«ï€±)

(ï€± )

...

hï€°t

G

t ( L1 )

Msg. Enc. Topo. Selec.

Comm. Topo.
( L2 )
G0t

ï» ï½

Msg. Content
(L )

mi*t

t ( L2 )

h

t ( L1ï€­1)

Recv. Msg. Set m j

Msg. Aggr.

Msg. Aggr.

Hidden State
Update
ht

Hidden State
Update

( L1 )

Comm. Topo.

ï€²

Git

( L2 )

ï» ï½

Msg. Enc. Topo. Selec.
Msg. Content
( Lï€² )
mN*t ï€­ï€±

( L2 ï€­1)

h0t

Msg. Aggr.

( L2 )

hit

â‘¢ Policy Optimization

( L2 )

GNt ï€­1
( L2 )

j ï‚¹i

hit

Msg. Aggr.

( L2 ï€­1)

( L2 ï€­1)

hNt ï€­1

Hidden State
Update

Hidden State
Update

h0t

Comm. Topo.

Recv. Msg. Set ï»mtj ï½ jï‚¹Nï€­1

t ( L2 )

Recv. Msg. Set m j

j ï‚¹0

Used for next
hidden state
update

...

Msg. Content
( Lï€² )
mï€°*t

Msg. Content Comm. Topo.

(ï€±)

Used for next
hidden state
update

...

...

Msg. Enc. Topo. Selec.

Topo. Selec.

*t ( L1 )

L2 Iterations

hit

...

Used for next
hidden state
update

L1 Iterations

Msg. Enc.
m

...

m t , ht

...

(1)

...

ht

Used for next
hidden state
update

(1)

(1)

( L2 )

( L2 )

hNt ï€­1

â‘¢ Decision Making

ï°

ï°0

at

a0t

ï° N ï€­1

ï°i
ait

...

aNt ï€­1

...

(b)

(a)

Fig. 1: General MARL Framework of Multi-round Communications. (a) Centralized Training, (b) Decentralized Execution.
(ğ‘™)

(ğ‘™)

(ğ‘™)

(ğ‘™)

where hğ‘¡ = [â„0ğ‘¡ , â„1ğ‘¡ , . . . , â„ğ‘¡ğ‘ âˆ’1 ] represents the updated hidden states for all agents. The function ğ‘“ğœ½hsu
can be
hsu
implemented using recurrent networks [8], [11], [24], or
other architectures that effectively integrate temporal and
communication information [10], [12].
This process repeats for ğ¿ communication rounds. Although
the topology generation parameters ğœ½ topo remain fixed through(ğ‘™)
out all rounds, the communication topology ğº ğ‘¡
evolves
dynamically across different communication rounds because it
(ğ‘™âˆ’1)
takes as input the continuously updated hidden states hğ‘¡

from the previous round. Consequently, while the topology
generation mechanism is shared, the actual communication
topologies adapt to the evolving information state of the system.
3) Policy Optimization: Upon completion of all ğ¿ 1 communication rounds, the final hidden states can be used to formulate
policies and select actions for all agents:
ağ‘¡ = ğ‘“ğœ½ğœ‹ğœ‹ (hğ‘¡

( ğ¿1 )

),

(6)

where ağ‘¡ = [ğ‘ 0ğ‘¡ , ğ‘ 1ğ‘¡ , . . . , ğ‘ ğ‘¡ğ‘ âˆ’1 ] represents the joint action of all
agents.

6

During the centralized training phase, we optimize both the
individual policies and the communication protocols simultaneously. This is achieved through a centralized critic that has
access to global state information: ğ‘„(ğ‘ ğ‘¡ , ağ‘¡ ) = ğ‘“ğœ½ğ‘„ğ‘„ (ğ‘ ğ‘¡ , ağ‘¡ ).
The training process optimizes the following comprehensive
objective function as follows:
L ğ‘¡ = ğ‘™ ağ‘¡ + ğ‘¤ ğ‘ ğ‘™ ğ‘„ğ‘¡ ,

(7)

where ğ‘™ ağ‘¡ is the policy loss and ğ‘™ ğ‘„ğ‘¡ is the critic loss.
The network parameters are updated using gradient descent:
ğœ½ â† ğœ½ âˆ’ ğœ‚âˆ‡ğœ½ Lğ‘¡ ,

(8)

where ğœ½ = {ğœ½ obs , ğœ½ msg , ğœ½ topo , ğœ½ aggr , ğœ½ hsu , ğœ½ ğœ‹ , ğœ½ ğ‘„ } represents the
collective parameters of all network components, and ğœ‚ is the
learning rate.
This centralized training approach enables agents to learn
sophisticated communication protocols and the learned parameters can later be deployed in a fully decentralized manner during
execution.
B. Decentralized Execution Stage
As shown in Fig. 1 (b), during the decentralized execution
stage, each agent ğ‘– âˆˆ [0, ğ‘ âˆ’ 1] operates independently using
only local observations and the fixed parameters ğœ½ learned from
centralized training. Unlike the training phase where global
information is accessible, agents must now rely exclusively on
their own observations and received messages.
1) Observation Processing: Each agent ğ‘– processes its local
observation ğ‘œ ğ‘–ğ‘¡ independently:
â„ğ‘–ğ‘¡

(0)

= ğ‘“ğœ½ğ‘œobs (ğ‘œ ğ‘–ğ‘¡ ),

(9)

where ğœ½ obs represents the fixed parameters of the observation
encoder from training.
2) Multi-Round Communication Protocol: The execution
phase follows the ğ¿ 2 -round communication structure, with
each agent operating autonomously without access to global
information. For each round ğ‘™ âˆˆ {1, 2, Â· Â· Â· , ğ¿ 2 }:
â€¢ Message Encoding: Agent ğ‘– encodes its hidden state into
a message:
(ğ‘™)
(ğ‘™âˆ’1)
ğ‘š ğ‘–âˆ—ğ‘¡ = ğ‘“ğœ½ğ‘šmsg (â„ğ‘–ğ‘¡
).
(10)
â€¢ Topology Selection: Agent ğ‘– independently determines its

communication neighbors:
ğº ğ‘–ğ‘¡

(ğ‘™)

topo

= ğ‘“ğœ½ topo (â„ğ‘–ğ‘¡

(ğ‘™âˆ’1)

),

(11)

(ğ‘™)

where ğº ğ‘–,ğ‘¡ ğ‘— = 1 indicates that agent ğ‘– establishes a
communication channel with agent ğ‘—.
â€¢ Message Reception and Aggregation: Agent ğ‘– receives
(ğ‘™)
messages from agents ğ‘— where ğº ğ‘¡ğ‘—,ğ‘– = 1, denoted as
Nğ‘–ğ‘¡

(ğ‘™)

(ğ‘™)

= { ğ‘— | ğº ğ‘¡ğ‘—,ğ‘– = 1}, and aggregates them:
ğ‘š ğ‘–ğ‘¡

(ğ‘™)

aggr

(ğ‘™)

(ğ‘™)

= ğ‘“ğœ½ aggr ({ğ‘š âˆ—ğ‘¡ğ‘— } ğ‘— âˆˆ N ğ‘¡ (ğ‘™) , ğº ğ‘–ğ‘¡ ).
ğ‘–

(12)

â€¢ Hidden State Update: Agent ğ‘– updates its hidden state:

â„ğ‘–ğ‘¡

(ğ‘™)

= ğ‘“ğœ½hsu
(â„ğ‘–ğ‘¡
hsu

(ğ‘™âˆ’1)

(ğ‘™)

, ğ‘š ğ‘–ğ‘¡ ).

(13)

All function parameters {ğœ½ msg , ğœ½ topo , ğœ½ aggr , ğœ½ hsu } remain fixed
(ğ‘™)
from training. The communication topology ğº ğ‘–ğ‘¡ may vary in
rounds as it depends on evolving hidden states.
3) Decision Making: After ğ¿ 2 communication rounds, each
agent ğ‘– independently selects its action:
ğœ‹ğ‘– = ğ‘“ğœ½ğœ‹ğœ‹ (â„ğ‘–ğ‘¡

( ğ¿2 )

),

ğ‘ ğ‘–ğ‘¡ âˆ¼ ğœ‹ğ‘– ,

(14a)
(14b)

where ğœ½ ğœ‹ represents the fixed policy parameter. This decentralized decision-making integrates both local observations and
collective knowledge acquired through multi-round communication, enabling informed action selection without requiring
global state information.
In particular, unlike during the training phase, no parameter
updates occur during execution. All network components are
deployed with their fixed, learned parameters (ğœ½ obs , ğœ½ msg , ğœ½ topo ,
ğœ½ aggr , ğœ½ hsu , and ğœ½ ğœ‹ , respectively). The centralized critic with
parameters ğœ½ ğ‘„ is not used during execution, as it is only
needed for training, and is not needed during execution. The
comprehensive CTDE process for MARL with communication
can be found in Algorithm 1.
IV. P ROPOSED C OMMUNICATION E FFICIENCY M ETRICS
Communication efficiency represents a critical yet often
overlooked dimension in MARL systems. While existing research predominantly evaluates algorithms through task performance, this focus neglects a fundamental trade-off: systems
achieving high performance through excessive communication
become impractical in large-scale, resource-constrained deployments [33]. The current evaluation framework is without standardized metrics necessary to characterize this performanceefficiency trade-off.
To address this gap, we propose three novel CEMs that
explicitly incorporate communication efficiency alongside task
performance, providing a comprehensive evaluation framework
for multi-agent coordination.
A. Information Entropy Efficiency Index (IEI)
IEI quantifies the amount of entropy required per unit of
success:
ğ»ğ‘¡
,
(15)
Î¦IEIğ‘¡ =
ğ’®ğ‘¡
where ğ»ğ‘¡ is the average message entropy across all agents and
communication rounds in epoch ğ‘¡:
!
ğ¿
ğ‘ âˆ’1
(ğ‘™)
1 âˆ‘ï¸ 1 âˆ‘ï¸
ğ»ğ‘¡ =
ğ» (ğ‘€ğ‘–ğ‘¡ ) .
(16)
ğ¿ ğ‘™=1 ğ‘ ğ‘–=0
The entropy of each agentâ€™s communication message is calculated as:
âˆ‘ï¸
(ğ‘™)
ğ‘¡ (ğ‘™)
ğ‘¡ (ğ‘™)
ğ» (ğ‘€ğ‘–ğ‘¡ ) = âˆ’
ğ‘(ğ‘š ğ‘–,ğ‘˜
) log2 ğ‘(ğ‘š ğ‘–,ğ‘˜
),
(17)
ğ‘˜

7

Algorithm 1 Learning Communication Protocols for MultiAgent Systems via Centralized Training and Decentralized
Execution
1: Initialize: Parameters replay buffer D, parameter set
ğœ½ = {ğœ½ obs , ğœ½ msg , ğœ½ topo , ğœ½ aggr , ğœ½ hsu , ğœ½ ğœ‹ , ğœ½ ğ‘„ }.
2: /* Phase 1: Centralized Training */
3: while not converged do
4:
Agents collect observation batch oğ‘¡ from environment
with global state ğ‘ ğ‘¡ .
(0)
5:
Generate initial hidden states: hğ‘¡ = ğ‘“ğœ½ğ‘œobs (oğ‘¡ ).
6:
for communication round ğ‘™ = 1 to ğ¿ 1 do
(ğ‘™)
7:
Generate the initial message vector mâˆ—ğ‘¡
=
(ğ‘™âˆ’1)
ğ‘“ğœ½ğ‘šmsg (hğ‘¡
).
Get the communication topology ğº ğ‘¡

8:

(ğ‘™)

topo

= ğ‘“ğœ½ topo (hğ‘¡

(ğ‘™âˆ’1)

).

(ğ‘™)
(ğ‘™)
(ğ‘™)
aggr
Aggregate message mğ‘¡ = ğ‘“ğœ½ aggr (mâˆ—ğ‘¡ , ğº ğ‘¡ ).
(ğ‘™)
(ğ‘™âˆ’1)
(ğ‘™)
Update hidden state hğ‘¡ = ğ‘“ğœ½hsu
(hğ‘¡
, mğ‘¡ ).
hsu

9:
10:
11:
12:

end for
(ğ¿ )
ağ‘¡ = ğ‘“ğœ½ğœ‹ğœ‹ (hğ‘¡ 1 ).
13:
Execute ağ‘¡ , observe rewards rğ‘¡ and next state ğ‘ ğ‘¡+1 .
14:
Store transition in D and sample mini-batch.
15:
Lğ‘¡ = ğ‘™ ağ‘¡ + ğ‘¤ ğ‘ ğ‘™ ğ‘„ğ‘¡ {Compute loss with critic ğ‘„(ğ‘ ğ‘¡ , ağ‘¡ ) =
ğ‘„
ğ‘“ğœ½ ğ‘„ (ğ‘ ğ‘¡ , ağ‘¡ )}.
16:
Update parameters: ğœ½ â† ğœ½ âˆ’ ğœ‚âˆ‡ğœ½ Lğ‘¡ .
17: end while
18: /* Phase 2: Decentralized Execution */
19: for each timestep ğ‘¡ during execution do
20:
for each agent ğ‘– âˆˆ {0, 1, ..., ğ‘ âˆ’ 1} in parallel do
21:
22:
23:
24:
25:

Receive observation ğ‘œğ‘–ğ‘¡ and encode: â„ğ‘– = ğ‘“ğœ½ğ‘œobs (ğ‘œ ğ‘–ğ‘¡ ).
end for
for communication round ğ‘™ = 1 to ğ¿ 2 do
for each agent ğ‘– âˆˆ {0, 1, ..., ğ‘ âˆ’ 1} in parallel do
(ğ‘™)
(ğ‘™âˆ’1)
Generate the initial message ğ‘š ğ‘–âˆ—ğ‘¡ = ğ‘“ğœ½ğ‘šmsg (â„ğ‘–ğ‘¡
).
Get
topo

the

communication

topology

(ğ‘™âˆ’1)

ğº ğ‘–ğ‘¡

(ğ‘™)

=

ğ‘“ğœ½ topo (â„ğ‘–ğ‘¡
).
27:
end for
28:
for each agent ğ‘– âˆˆ {0, 1, ..., ğ‘ âˆ’ 1} in parallel do
(ğ‘™)
(ğ‘™)
=
29:
Receive messages {ğ‘š âˆ—ğ‘¡ğ‘— } ğ‘— âˆˆ N ğ‘¡ (ğ‘™) where Nğ‘–ğ‘¡
{ ğ‘— | ğº ğ‘—,ğ‘– = 1} based on communication topology.
Aggregate
messages
(ğ‘™)
(ğ‘™)
aggr
ğ‘“ğœ½ aggr ({ğ‘š âˆ—ğ‘¡ğ‘— } ğ‘— âˆˆ N ğ‘¡ (ğ‘™) , ğº ğ‘–ğ‘¡ ).
ğ‘–

(ğ‘™)

(ğ‘™)
ğ‘š ğ‘–ğ‘¡

where ğœ‰ğ‘¡ is the average cosine similarity between messages
from different agents across all communication rounds in epoch
ğ‘¡:
ğ¿
ğ‘
âˆ’2 ğ‘
âˆ’1
âˆ‘ï¸
âˆ‘ï¸
(ğ‘™) Âª
2
1 âˆ‘ï¸ Â©
ğœ‰ğ‘¡ =
cos(ğœƒ ğ‘–,ğ‘¡ ğ‘— ) Â®
(19)
Â­
ğ¿ ğ‘™=1 ğ‘ (ğ‘ âˆ’ 1) ğ‘–=0 ğ‘—=ğ‘–+1
Â«
Â¬
The cosine similarity between messages from agents ğ‘– and ğ‘— is
calculated as:
(ğ‘™)
cos(ğœƒ ğ‘–,ğ‘¡ ğ‘— ) =

(ğ‘™âˆ’1)

ğ‘š ğ‘–ğ‘¡

(ğ‘™)

Â· ğ‘š ğ‘¡ğ‘—

(ğ‘™)

(ğ‘™)

(ğ‘™)

âˆ¥ğ‘š ğ‘–ğ‘¡ âˆ¥ âˆ¥ğ‘š ğ‘¡ğ‘— âˆ¥

.

(20)

A lower Î¦SEIğ‘¡ indicates that agents achieve success while
developing more diverse communication patterns, reflecting
greater functional specialization.

TEI measures the effective utilization of communication by
relating task success to communication frequency:
Î¦TEIğ‘¡ =

ğ’®ğ‘¡
,
ğ¶ğ‘¡

(21)

where ğ’®ğ‘¡ represents the success rate at epoch ğ‘¡, and ğ¶ğ‘¡ denotes
the total number of communications. During each epoch ğ‘¡, ğ¶ğ‘¡
is calculated as:
âˆ’1
âˆ’1 ğ‘
ğ¿ ğ‘
âˆ‘ï¸
âˆ‘ï¸
âˆ‘ï¸
(ğ‘™)
ğº ğ‘–,ğ‘¡ ğ‘— ,
(22)
ğ¶ğ‘¡ =
ğ‘™=1 ğ‘–=0 ğ‘—=0
ğ‘—â‰ ğ‘–
(ğ‘™)

=
(ğ‘™)

Update hidden state â„ğ‘–ğ‘¡ = ğ‘“ğœ½hsu
(â„ğ‘–ğ‘¡
, ğ‘š ğ‘–ğ‘¡ ).
hsu
32:
end for
33:
end for
34:
for each agent ğ‘– âˆˆ {0, 1, ..., ğ‘ âˆ’ 1} in parallel do
(ğ¿ )
35:
ğ‘ ğ‘–ğ‘¡ âˆ¼ ğ‘“ğœ½ğœ‹ğœ‹ (â„ğ‘–ğ‘¡ 2 ).
36:
end for
37: end for
31:

(ğ‘™)

SEI reflects the degree of functional specialization among
agents by measuring the similarity of their communications
relative to task success:
ğœ‰ğ‘¡
(18)
Î¦SEIğ‘¡ = ,
ğ’®ğ‘¡

ğ‘–

ğ‘¡ (ğ‘™)

30:

B. Specialization Efficiency Index (SEI)

C. Topology Efficiency Index (TEI)

ğ‘¡ (0)

26:

A lower Î¦IEIğ‘¡ indicates that agents achieve success while
exchanging information with lower entropy, suggesting more
efficient encoding of task-relevant information.

ğ‘¡
where ğ‘(ğ‘š ğ‘–,ğ‘˜
) represents the normalized probability distribution of agent ğ‘–â€™s message components in communication round
ğ‘™.

where ğº ğ‘–,ğ‘¡ ğ‘— âˆˆ {0, 1} indicates whether agent ğ‘– communicates
with agent ğ‘— in round ğ‘™ of epoch ğ‘¡.
A higher Î¦TEIğ‘¡ value indicates that agents achieve better task
performance with fewer communication acts, indicating greater
communication parsimony.
D. Comparative Study of CEMs
1) Summary and Comparison of CEMs: The three proposed
CEMs provide complementary perspectives to evaluate communication efficiency in MAS. Both the IEI and the SEI capture
more implicit aspects of communication efficiency. IEI evaluates the entropy efficiency of transmitted messages, revealing
how effectively agents encode task-relevant information into
compact representations. SEI, meanwhile, assesses the degree
of functional differentiation among agents through message

8

(a)

25

(b)
3

10
TEI

15

2

0

500

1000 1500 2000

8

1.5

6

1

4

10
5

0.5

(c)

12

2.5
SEI

IEI

20

10-7

14

0

500

Epoch

2

1000 1500 2000

0

500

1000

1500

2000

Epoch
MAGIC

CommNet

TarMAC

GA-Comm

IC3Net

Fig. 2: Comparison of Î¦IEI , Î¦SEI , and Î¦TEI for different algorithms in the TJ environment with ğ¿ = 1.
diversity, indicating whether agents have developed specialized
communication roles that contribute to coordination.
In contrast, the TEI represents an explicit communication
efficiency metric that directly refers to resource consumption, including computational costs, bandwidth utilization, and
temporal overhead. By quantifying task success relative to
communication frequency, TEI offers a straightforward measure
of communication parsimony that aligns with practical deployment constraints in resource-limited environments.
Altogether, these metrics form a comprehensive framework
for evaluating communication efficiency in MAS, integrating
both implicit message encoding and coordination mechanisms
with explicit resource constraints that characterize successful
multi-agent learning.
2) Experimental Comparison of CEMs: Based on the aforementioned metrics, we conduct a comprehensive comparative
study of 5 MARL algorithms with different learned communication protocolsâ€”MAGIC, CommNet, TarMAC, GA-Comm,
and IC3Netâ€”within the Traffic Junction (TJ) environment for
5 agents [11]. Our evaluation applies Î¦IEI , Î¦SEI and Î¦TEI
to systematically assess MASâ€™ communication quality from
different dimensions. The performance of these 5 algorithms
is examined under both one-round (ğ¿ = 1) and two-round
(ğ¿ = 2) communication scenarios, which allows us to study
how communication efficiency affects learning dynamics and
final performance. The experiments are conducted using Python
3.10.16 with PyTorch on an Intel (R) Core (TM) i7-14650HX
processor and NVIDIA GeForce RTX 4060Ti Laptop GPU.
Fig. 2 presents a comparison of three CEMs for 5 aforementioned MARL algorithms in the TJ environment with oneround communication (ğ¿ = 1). Firstly, Fig. 2 (a) evaluates Î¦IEI ,
where lower values indicate more focused communication. All
algorithmsâ€™ Î¦IEI converge from high initial values to stable
low levels, revealing an inverse relationship between convergence speed and communication compactness. TarMACâ€™s Î¦IEI
converges earliest but achieves the highest final value, while
IC3Net and GA-Commâ€™s Î¦IEI converge slower yet attain the

TABLE I: Convergence Epoch and Final Convergence Value
Comparison (ğ¿ = 1)
Algorithm
MAGIC [24]
CommNet [8]
TarMAC [12]
GA-Comm [10]
IC3Net [11]

Î¦IEI

Î¦TEI (Ã—10 âˆ’7 )

Î¦SEI

Epoch

Value

Epoch

Value

Epoch

Value

750
1700
400
1300
1300

6.5
7.5
7.5
5.0
5.0

750
1700
400
1300
1300

0.9
0.6
1.0
0.75
0.5

750
1000
400
1300
1300

10.8
9.2
8.0
12.5
12.5

lowest value. Secondly, Fig. 2 (b) examines Î¦SEI , where lower
values reflect more specialized communication. All algorithms
reduce initial redundancy following distinct trajectories, exhibiting a similar inverse pattern: TarMACâ€™s Î¦SEI converges
fastest but stabilizes at the highest redundancy, while IC3Netâ€™s
Î¦SEI achieves the lowest redundancy despite slower convergence. CommNet and GA-Commâ€™s Î¦SEI show intermediate
performance. Thirdly, Fig. 2 (c) compares Î¦TEI , representing
integrated efficiency balancing task performance with communication overhead. Unlike IEI and SEI, Î¦TEI increases during
training for all algorithms. TarMACâ€™s Î¦TEI stabilizes earliest,
followed by MAGIC and CommNetâ€™s Î¦TEI , with GA-Comm
and IC3Netâ€™s Î¦TEI converging last. However, GA-Comm and
IC3Net achieve the highest final Î¦TEI , followed by MAGIC,
CommNet, and TarMAC.
Across all metrics, a consistent inverse relationship emerges
between convergence speed and final performance. IC3Net
and GA-Comm converge slowly yet achieve superior values
across IEI, SEI, and TEI. Conversely, TarMACâ€™s rapid convergence yields the poorest final values, suggesting premature optimization. MAGIC maintains intermediate performance.
These patterns indicate that rapid convergence often signals
suboptimal strategy commitment, whereas extended exploration
enables more effective task-communication trade-offs. Detailed
convergence epochs and final values are shown in Table I.

9

(a)

8

10-6)

SEI

0.6

1

TEI

(

4

0.4

et
3N

m
om

IC

C
A
G

A

-C

rM
Ta

m

N

et

IC
m

Co

M

A
G

et
3N

m
om

-C
A

G

IC

C
A

et

rM

N

Ta

m
m

A
G

Co

M

3N

om
-C

G

A

IC

C
A

et
N

rM
Ta

m
m

A
G

Co

0.5

0
IC

0
et

0
m

0.2

IC

2

M

(c)

1.5

0.8

6
IEI

(b)

1

Fig. 3: Comparison of Î¦IEI , Î¦SEI , Î¦TEI for different algorithms under one-round and two-round communication scenario in the
TJ environment.

Fig. 3 compares the CEMs for all 5 algorithms under oneround (ğ¿ = 1) and two-round (ğ¿ = 2) communication scenarios
in the TJ environment. Overall trends reveal a critical tradeoff between communication coordination and overhead: while
two-round communication achieves superior information focus
and specialization (lower Î¦IEI and Î¦SEI ), it incurs substantial communication overhead penalties (lower Î¦TEI ) compared
to one-round communication. Fig. 3 (a) demonstrates that
two-round communication universally reduces Î¦IEI relative to
one-round scenarios, indicating that additional communication
round enables agents to develop more focused information exchange protocols. This improvement is particularly pronounced
for CommNet, MAGIC and TarMAC, while GA-Comm and
IC3Net show more moderate reductions. Fig. 3 (b) reveals a
similar advantage for two-round communication in specialization. Most algorithms achieve lower Î¦SEI under ğ¿ = 2, reflecting enhanced role differentiation among agents. GA-Comm
exhibit comparable performance across both communication
rounds. Fig. 3 (c) presents a contrasting pattern: one-round
communication consistently leads to superior Î¦TEI across all
algorithms. GA-Comm and IC3Net experience the largest Î¦TEI
losses under two-round communication (approximately 50%
and 47% reductions, respectively), while MAGIC and TarMAC
show moderate penalties (41% and 38%), and CommNet exhibits the smallest degradation (29%).
The overall findings suggest that while additional communication round enhances encoding and coordination quality, it
introduces communication overhead that significantly diminish
overall task-communication efficiency in resource-constrained
environments.
V. P ROTOCOL L EARNING WITH E FFICIENCY
AUGMENTATION
The above findings reveal a critical dilemma: while tworound communication improves message encoding (Î¦IEI ) and
coordination specialization (Î¦SEI ), it severely degrades overall

task-communication efficiency (Î¦TEI ) due to increased overhead. However, our experiments demonstrate that Î¦IEI and Î¦SEI
naturally decline during training even with one-round communication, with two-round communication merely accelerating
this convergence rather than fundamentally altering it. This
observation suggests a key insight that the superior encoding
and coordination quality achieved by multi-round communication may be attainable within a one-round framework by
explicitly guiding the natural convergence process. We therefore
propose the direct incorporation of Î¦IEI and Î¦SEI directly
into the training loss function as regularization terms. This
approach leverages and accelerates their inherent downward
trend, enabling agents to achieve multi-round-level encoding
compactness and coordination specialization while maintaining
one-round communication. By exploiting this natural learning
dynamic rather than adding communication rounds, the MAS
obtains superior task-communication efficiency without incurring overhead penalties.
The standard RL training loss function integrated with additional communication efficiency terms can be written as
Lğ‘¡ = ğ‘™ağ‘¡ + ğ‘¤ ğ‘ ğ‘™ ğ‘„ğ‘¡ + ğ‘¤ IEIğ‘¡ Î¦IEIğ‘¡ + ğ‘¤ SEIğ‘¡ Î¦SEIğ‘¡ ,

(23)

where Î¦IEIğ‘¡ corresponds to the Information Entropy Efficiency
Index, Î¦SEIğ‘¡ corresponds to the Specialization Efficiency Index,
and ğ‘¤ ğ‘ , ğ‘¤ IEIğ‘¡ , when ğ‘¤ SEIğ‘¡ are hyperparameters that balance the
different objectives.
It is noted that while our framework involves three efficiency
metrics, we involve merely IEI and SEI in the loss function.
This decision stems from both architectural and methodological
considerations. Several baseline algorithms in our evaluation
(e.g., CommNet, MAGIC) employ fully connected communication topologies where ğ¶ğ‘¡ remains constant, making TEI
optimization ineffective. Furthermore, while IEI and SEI provide informative gradients about message content and diversity, TEI offers limited training signals for neural network
optimization. Our approach therefore strategically focuses on

10

Algorithm 2 Dynamic Regularization Weight Adjustment for
Efficiency-Augmented Training
1: Input: Original loss Lğ‘¡ = ğ‘™ ağ‘¡ + ğ‘¤ ğ‘ ğ‘™ ğ‘„ğ‘¡ , average message

entropy ğ»ğ‘¡ and average message similarity ğœ‰ğ‘¡ across all
agents and communication rounds in epoch ğ‘¡, success rate
ğ’®ğ‘¡ .
2: Set minimum success rate threshold T , scaling factor
ğ›½, regularization target ratio ğ›¼, minimum weight ğœ† min ,
maximum weight ğœ† max , and small constant ğœ–.
3: if ğ’®ğ‘¡ < T then
4:
ğ’®ğ‘¡ â† T .
5: end if
6: Compute the parameters and metrices in epoch ğ‘¡:
Î¦IEIğ‘¡ = ğ»ğ‘¡ Â· (1.0 âˆ’ ğ›½ Â· ğ’®ğ‘¡ ), Î¦SEIğ‘¡ = ğœ‰ğ‘¡ Â· (1.0 âˆ’ ğ›½ Â· ğ’®ğ‘¡ ),
(dynamic weight)
ğ‘¤ IEIğ‘¡ = ğ›¼ Â· Lğ‘¡ /(Î¦IEIğ‘¡ + ğœ–), ğ‘¤ SEIğ‘¡ = ğ›¼ Â· Lğ‘¡ /(Î¦SEIğ‘¡ + ğœ–),
(Constrain weight range)
ğ‘¤ IEIğ‘¡ = max(ğœ†min , min(ğœ†max , ğ‘¤ IEIğ‘¡ )),
ğ‘¤ SEIğ‘¡ = max(ğœ†min , min(ğœ†max , ğ‘¤ SEIğ‘¡ )).
7: Update total loss: Lğ‘¡ = Lğ‘¡ + ğ‘¤ IEIğ‘¡ Â· Î¦IEIğ‘¡ + ğ‘¤ SEIğ‘¡ Â· Î¦SEIğ‘¡ .
8: Output: Updated total loss Lğ‘¡ for efficiency augmentation.

Note: The formulation Î¦IEIğ‘¡ = ğ»ğ‘¡ Â· (1.0 âˆ’ ğ›½ Â· ğ’®ğ‘¡ ) and Î¦SEIğ‘¡ =
ğœ‰ğ‘¡ Â· (1.0 âˆ’ ğ›½ Â· ğ’®ğ‘¡ ) provides a smoother regularization mechanism compared to the direct division by success rate (ğ»ğ‘¡ /ğ’®ğ‘¡
and ğœ‰ğ‘¡ /ğ’®ğ‘¡ ). This approach reduces sensitivity to success rate
fluctuations-applying milder regularization when the success
rate approaches 1, while preventing excessive regularization
intensity when success rates are low.

improving message content efficiency and agent specialization
through the learning process using IEI and SEI, while using
TEI as an evaluation metric to assess the resource efficiency of
communication.
To address the challenge of balancing task performance
with communication efficiency in Eq. (23), we implement a
dynamic regularization weight adjustment mechanism. This
approach ensures that communication efficiency is adequately
emphasized without compromising the primary objectives of
the task.
Our dynamic weighting strategy adaptively balances task
performance and communication efficiency by modulating regularization intensity based on agent success rates. The mechanism operates with a simple principle: high task performance increases emphasis on efficiency optimization, while
performance deterioration automatically reduces regularization
pressure to prioritize task completion. This prevents excessive regularization from impeding learning during challenging
phases while refining communication protocols during stable
periods. The success-rate-dependent scaling maintains proportionality between task objectives and efficiency terms, ensuring
optimization stability. The implementation of our dynamic

TABLE II: Simulation parameters
Parameters

Value

Parameters

Value

ğœ–
ğ›½
ğœ†min

10âˆ’10

T
ğ›¼
ğœ†max

0.05
0.01
5 Ã— 10 âˆ’3

0.5
10âˆ’5

regularization weight adjustment mechanism is presented in
Algorithm 2, which outlines the step-by-step procedure for
calculating both IEI and SEI regularization weights based on
current performance metrics.
VI. E XPERIMENT R ESULTS
To validate our proposed efficiency-augmented loss function,
we conducted comparative experiments in the TJ environment
with 5 agents. We evaluated the MASâ€™ performance before
and after implementing our loss function adjustment. Table II
details the key experimental parameters used throughout our
evaluation.
Fig. 4 presents a comprehensive comparison of 5 communication algorithms under three following experimental conditions:
â€¢ ğ¿ = 1 (w/o Effic. Aug.): Baseline configuration with
one communication round per decision cycle, using standard training objectives without efficiency augmentation
in Eq. (7).
â€¢ ğ¿ = 1 (w/ Effic. Aug.): Configuration with one communication round but enhanced with our proposed efficiencyaugmented loss function (Eq. 23). This explicitly optimizes
for both communication efficiency and task performance.
â€¢ ğ¿ = 2 (w/o Effic. Aug.): Baseline configuration with two
communication rounds per decision cycle, using standard
training objectives without efficiency augmentation.
The quantitative results based on Fig. 4 are detailed in Table III. Across all evaluated algorithms, a consistent pattern
emerges: our proposed loss adjustment mechanism simultaneously enhances both task success rate (ğ’®ğ‘¡ ) and communication
efficiency (Î¦TEI ). This dual improvement challenges the conventional intuition that performance enhancement necessarily
requires increased communication overhead, instead revealing
the possibility of synergistic optimization through refined training objectives. We now elaborate more details as below.
A. Efficiency Augmentation in ğ¿ = 1
The 5 algorithms exhibit varying degrees of responsiveness
to loss adjustment, which can be categorized into two tiers:
1) Strong Responders (IC3Net, CommNet, MAGIC): IC3Net
demonstrates the most transformative response-transitioning
from suboptimal plateaus without adjustment to consistently
near-perfect performance with adjustment, representing a fundamental capability shift rather than mere acceleration. CommNet shows substantial gains despite its communication-intensive
architecture. MAGIC similarly achieves near-perfect ğ’®ğ‘¡ earlier
while maintaining higher Î¦TEI values.

11

(a)

10-6

TEI

0.8
0.6

(b)

1
0.5

0.4
0.2

0
0

500

1000 1500

0

500

Epoch

1000 1500

Epoch

(c)

10-6

0.8

(d)

B. Multi-Round Communication

TEI

1

0.6

0.5
0.4
0.2

0
0

500

1000 1500

0

500

Epoch

1000 1500

Epoch

(e)

10-6

(f)

1
TEI

0.8
0.6

0.5

0.4
0.2

0
0

500

1000 1500

0

500

Epoch

1000 1500

Epoch

(g)

10-6

(h)

1
TEI

0.8
0.6

Although the magnitude of improvement varies, all algorithms demonstrate consistent improvements in both ğ’®ğ‘¡ and
Î¦TEI . This universal benefit reveals a key insight: beyond
architectural modifications, optimizing the training objective
function offers an effective approach to enhance both task
performance and communication efficiency. This objectivelevel strategy is architecture-agnostic, complementing architectural innovations as an alternative pathway to improve taskcommunication trade-offs in multi-agent learning.

0.5

When comparing ğ¿ = 1 (w/ Effic. Aug.) against ğ¿ = 2
(w/o Effic. Aug.), algorithm-specific patterns emerge with the
following practical implications.
1) Superior Performance of One-Round Communication:
For GA-Comm, MAGIC, and IC3Net, ğ¿ = 1 (w/ Effic. Aug.)
consistently outperforms ğ¿ = 2 (w/o Effic. Aug.) from the
dimensions of both convergence speed and convergence values
for ğ’®ğ‘¡ and Î¦TEI metrics. This indicates that the adjusted
communication structure can outperform the increased communication overhead for these architectures.
2) Marginal Advantages of Two-Round Communication:
CommNet and TarMAC exhibit slightly superior performance
with ğ¿ = 2 (w/o Effic. Aug.) compared to ğ¿ = 1 (w/
Effic. Aug.). However, these modest performance advantages
incur substantial efficiency penalties, requiring significantly
additional communication overhead to achieve incremental improvements. This trade-off highlights the importance of contextspecific optimization strategies.

0.4
0.2

0
0

500

1000 1500

0

500

Epoch

Epoch

(i)

10

1.5
TEI

0.8
0.6

1000 1500

-6

(j)

1
0.5

0.4
0.2
0

500

1000 1500

0
0

500

1000 1500

Epoch

Epoch

L=1 (w/ Effic. Aug.)
L=2 (w/o Effic. Aug.)
GAComm
MAGIC

L=1 (w/o Effic. Aug.)
CommNet
TarMAC
IC3Net

Fig. 4: Comparison of succcess rate and Î¦TEI for different
algorithms in the TJ environment of one-round communication
scenario and two-round communication scenario.
2) Moderate Responders (TarMAC, GA-Comm): Both algorithms exhibit meaningful but more modest improvements.
TarMAC demonstrates notably greater gains in both ğ’®ğ‘¡ and
Î¦TEI compared to GA-Comm, whose improvements are the
most limited among all evaluated algorithms.

C. Implications for Practical Deployment
The above findings yield critical insights for deploying MAS
under varying communication constraints.
1) Resource-Constrained Scenarios: When communication
constraints are stringent, the better approach is ğ¿ = 1 (w/
Effic. Aug.). Because it consistently outperforms it alternative
configurations ğ¿ = 1 (w/o Effic. Aug.) across all evaluated
algorithms. It achieves near-optimal task performance while
maintaining significantly enhanced communication efficiency.
Therefore, ğ¿ = 1 (w/ Effic. Aug.) enables practical deployment
in communication-constrained environments without performance degradation.
2) Unconstrained Scenarios: When communication constraints are less stringent, the better approach becomes
algorithm-dependent. CommNet and TarMAC marginally benefit from additional communication rounds ğ¿ = 2 (w/o Effic.
Aug.) in unconstrained settings, while GA-Comm, MAGIC,
and IC3Net perform optimally with ğ¿ = 1 (w/ Effic. Aug.)
regardless of constraint levels.
These results challenge the presumed trade-off between
communication efficiency and task performance in MAS. By
incorporating efficiency into the optimization objective, our
approach enables agents to develop dense communication protocols while maintaining or enhancing task performance. This

12

TABLE III: Performance Comparison of MARL Algorithms under Different Communication Rounds in TJ Environment
ğ¿ = 2 (w/o Effic. Aug.)

ğ¿ = 1 (w/o Effic. Aug.)
Algorithm

Î¦TEI

ğ’®ğ‘¡

Epoch

Value

Epoch

25

25

20

20

15
10

15
10

5

5
0

500

1000

1500

0

500

Epoch

1000

1500

Epoch

3

3

2

2

SEI

SEI

(a)

1
0

1

0

500

1000

0

1500

0

500

Epoch

w/ Loss Adjust

Î¦TEI

ğ’®ğ‘¡
Value

Epoch

Value

Epoch

Value

0.72 1000 8.79 730 (-770) 0.99 (+.28) 730 (-270) 6.71 (-2.08) 1200 (-300) 0.99 (+.28) 1200 (-200) 13.47 (+4.69)
0.96 1300 12.35 1200 (-100) 0.95 (-.02) 1200 (-100) 6.19 (-6.16) 1100 (-200) 0.99 (+.03) 1100 (-200) 12.73 (+.38)
0.79 480 8.14 1000 (+520) 0.97 (+.18) 630 (+150) 5.04 (-3.10) 1300 (+820) 0.95 (+.16) 1300 (+820) 9.88 (+1.74)
0.89 750 10.88 1500 (+750) 1.00 (+.10) 1500 (+750) 6.60 (-4.28) 700 (-50) 1.00 (+.11) 1100 (+350) 12.22 (+1.35)
0.88 1300 12.39 500 (-800) 0.88 (+.01) 500 (-800) 6.36 (-6.03) 800 (-500) 1.00 (+.12) 800 (-500) 14.48 (+2.09)

IEI

IEI

CommNet [8] 1500
G2Comm [10] 1300
TarMAC [12] 480
MAGIC [24] 750
IC3Net [11] 1300

Î¦TEI

ğ’®ğ‘¡

Epoch Value Epoch Value

ğ¿ = 1 (w/ Effic. Aug.)

1000

1500

Epoch

w/o Loss Adjust

CommNet

IC3Net

(b)

Fig. 5: Comparison of (a) Î¦IEI and (b) Î¦SEI for CommNet and
IC3Net in the TJ environment of one-round communication
scenario.

reveals that communication inefficiency arises primarily from
poorly designed optimization objectives rather than the inherent
information needs of the task itself. The consistent patterns
observed across architecturally diverse algorithms demonstrate
that optimizing objective functions provides a more generalizable solution than modifying network architectures.

of loss adjustment. The loss-adjusted CommNet model reaches
stable IEI values 59% faster with a 36% decrease in terminal
IEI, while IC3Net shows a 58% acceleration in convergence
with a 19% reduction in convergence values. These consistent
improvements across architecturally distinct algorithms suggest
fundamental improvements in information encoding rather than
algorithm-specific optimizations.
This reduction in IEI values demonstrates that explicitly
incorporating IEI into training objectives leads agents to develop more precise encoding of task-relevant information while
minimizing redundant data transmission. This advancement is
particularly valuable for MAS with communication constraints.
GAComm, TarMAC, and MAGIC exhibit similar efficiency
patterns, which are not shown due to space limitations.
Fig. 5 (b) examines the values of SEI for CommNet and
IC3Net with and without our proposed loss adjustment in oneround communication scenarios. Both algorithms demonstrate
substantial improvements in communication diversity after implementing loss adjustment. With loss adjustment, CommNet
shows a 48% acceleration in convergence rate and a 40%
reduction in terminal SEI values, while IC3Net exhibits a
56% faster convergence with 36% lower convergence value.
These significant reductions in architecturally distinct algorithms suggest that loss adjustment fundamentally transforms
communication dynamics rather than optimizing algorithmspecific parameters.
Lower SEI values indicate that agents develop more heterogeneous, specialized communication protocols when efficiency
is explicitly incorporated into training objectives. This enhanced
diversity enables agents to establish functionally differentiated
roles, facilitating more sophisticated coordination while maintaining or improving task performance. GAComm, TarMAC,
and MAGIC still exhibit similar specialized communication
patterns, which are not shown due to space limitations.
VII. C ONCLUSIONS

D. IEI and SEI Enhancement Comparison
Fig. 5 (a) compares the values of IEI for CommNet and
IC3Net with and without our proposed loss adjustment in
one-round communication scenarios. Both algorithms show
significant efficiency improvements after the implementation

This work fills a critical gap in communication research
by introducing a framework for evaluating and learning communication protocol efficiency in MAS, particularly focusing
on MARL systems. Our proposed CEMsâ€“IEI, SEI, and TEIâ€“
provide multidimensional insight into the efficient utilization of

13

information compactness, agent specialization, and communication resources. Our experiments in various MARL algorithms
with different learned communication protocols imply that
traditional approaches often achieve task success at the expense
of communication efficiency, while additional communication
rounds fail to improve overall efficiency despite potential
performance benefits. Most importantly, our refinement of the
loss function that incorporates efficiency considerations yields
substantial improvements in both communication efficiency and
task performance in all evaluated algorithms, demonstrating that
these objectives can be optimized simultaneously rather than
traded against each other. As MAS transitions to real-world
deployment under practical constraints, our framework provides
valuable tools for developing efficient communication protocols. Future work should explore the generalizability of these
approaches across diverse environments and communication
architectures, including human-agent systems where efficiency
constraints are critical.
R EFERENCES
[1] M. Wooldridge, An introduction to multiagent systems. John wiley &
sons, 2009.
[2] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
â€œValue-decomposition networks for cooperative multi-agent learning,â€
arXiv preprint arXiv:1706.05296, 2017.
[3] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
S. Whiteson, â€œMonotonic value function factorisation for deep multi-agent
reinforcement learning,â€ J. Mach. Learn. Res., vol. 21, no. 178, pp. 1â€“51,
2020.
[4] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,
â€œCounterfactual multi-agent policy gradients,â€ in Proc. AAAI Conf. Artif.
Intell., vol. 32, no. 1, 2018.
[5] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, â€œQtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement
learning,â€ in Proc. Int. Conf. Mach. Learn. PMLR, 2019, pp. 5887â€“5896.
[6] Z. Ning and L. Xie, â€œA survey on multi-agent reinforcement learning and
its application,â€ Journal of Automation and Intelligence, vol. 3, no. 2, pp.
73â€“91, 2024.
[7] E. J. Sondik, The optimal control of partially observable Markov processes. Stanford University, 1971.
[8] S. Sukhbaatar, R. Fergus et al., â€œLearning multiagent communication with
backpropagation,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 29, 2016.
[9] J. Jiang and Z. Lu, â€œLearning attentional communication for multi-agent
cooperation,â€ in Proc. Adv. Neural Inf. Process. Syst., vol. 31, 2018.
[10] Y. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, â€œMulti-agent game
abstraction via graph attention neural network,â€ in Proc. AAAI Conf. Artif.
Intell., vol. 34, no. 05, 2020, pp. 7211â€“7218.
[11] A. Singh, T. Jain, and S. Sukhbaatar, â€œLearning when to communicate
at scale in multiagent cooperative and competitive tasks,â€ arXiv preprint
arXiv:1812.09755, 2018.
[12] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and
J. Pineau, â€œTarMAC: Targeted multi-agent communication,â€ in Proc. Int.
Conf. Mach. Learn. PMLR, 2019, pp. 1538â€“1546.
[13] A. K. Chopra, M. P. Singh et al., â€œAn evaluation of communication protocol languages for engineering multiagent systems,â€ Journal of Artificial
Intelligence Research, vol. 69, pp. 1351â€“1393, 2020.
[14] G. Bochmann and C. Sunshine, â€œFormal methods in communication
protocol design,â€ IEEE Trans. Commun., vol. 28, no. 4, pp. 624â€“631,
2003.
[15] P. Merlin and D. Farber, â€œRecoverability of communication protocolsimplications of a theoretical study,â€ IEEE Trans. Commun., vol. 24, no. 9,
pp. 1036â€“1043, 2003.
[16] L. Zou, Z. Wang, J. Hu, Y. Liu, and X. Liu, â€œCommunication-protocolbased analysis and synthesis of networked systems: Progress, prospects
and challenges,â€ Int. J. Syst. Sci., vol. 52, no. 14, pp. 3013â€“3034, 2021.

[17] J. Miao, Z. Wang, X. Ning, A. Shankar, C. Maple, and J. J. Rodrigues,
â€œA UAV-assisted authentication protocol for internet of vehicles,â€ IEEE
Trans. Intell. Transp. Syst., vol. 25, no. 8, pp. 10 286â€“10 297, 2024.
[18] S. A. Shifani, G. V. Rajkumar, S. Maheshwari, A. A. Mary, M. M. Metilda
et al., â€œMANET: A secured and logical routing protocol development over
mobile adhoc networks for intelligent data communication,â€ in 2024 5th
International Conference on Electronics and Sustainable Communication
Systems (ICESC). IEEE, 2024, pp. 741â€“748.
[19] B. Zhou, B. Huang, Y. Su, and C. Zhu, â€œInterleaved periodic eventtriggered communications-based distributed formation control for cooperative unmanned surface vessels,â€ IEEE Trans. Neural Netw. Learn. Syst.,
2024.
[20] M. Habiba and N. I. Khan, â€œRevisiting gossip protocols: A vision for
emergent coordination in agentic multi-agent systems,â€ arXiv preprint
arXiv:2508.01531, 2025.
[21] B. Yan, Z. Zhou, L. Zhang, L. Zhang, Z. Zhou, D. Miao, Z. Li, C. Li,
and X. Zhang, â€œBeyond self-talk: A communication-centric survey of
llm-based multi-agent systems,â€ arXiv preprint arXiv:2502.14321, 2025.
[22] E. C. Strinati, G. C. Alexandropoulos, N. Amani, M. Crozzoli, G. Madhusudan, S. Mekki, F. Rivet, V. Sciancalepore, P. Sehier, M. Stark et al.,
â€œToward distributed and intelligent integrated sensing and communications for 6G networks,â€ IEEE Wireless Commun., vol. 32, no. 1, pp.
60â€“67, 2025.
[23] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, â€œLearning to
communicate with deep multi-agent reinforcement learning,â€ in Proc. Adv.
Neural Inf. Process. Syst., vol. 29, 2016.
[24] Y. Niu, R. R. Paleja, and M. C. Gombolay, â€œMulti-agent graph-attention
communication and teaming,â€ in Proc. of the 20th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS), London,
United Kingdom, May, 2021, pp. 964â€“973.
[25] E. Seraj, Z. Wang, R. Paleja, D. Martin, M. Sklar, A. Patel, and
M. Gombolay, â€œLearning efficient diverse communication for cooperative
heterogeneous teaming,â€ in Proc. of the 21st International Conference on
Autonomous Agents and Multiagent Systems, 2022, pp. 1173â€“1182.
[26] L. Yu, Q. Wang, Y. Qiu, J. Wang, X. Zhang, and Z. Han, â€œEffective multiagent communication under limited bandwidth,â€ IEEE Trans. Mobile
Comput., vol. 23, no. 7, pp. 7771â€“7784, 2024.
[27] A. ReÌnyi, â€œOn measures of entropy and information,â€ in Proceedings of
the 4th Berkeley symposium on mathematical statistics and probability,
vol. 4. University of California Press, 1961, pp. 547â€“562.
[28] H.-M. Chen, P. K. Varshney, and M. K. Arora, â€œPerformance of mutual
information similarity measure for registration of multitemporal remote
sensing images,â€ IEEE Trans. Geosci. Remote Sens., vol. 41, no. 11, pp.
2445â€“2454, 2003.
[29] M. Chafii, S. Naoumi, R. Alami, E. Almazrouei, M. Bennis, and M. Debbah, â€œEmergent communication in multi-agent reinforcement learning for
future wireless networks,â€ IEEE Internet Things Mag., vol. 6, no. 4, pp.
18â€“24, 2023.
[30] D. Kim, S. Moon, D. Hostallero, W. J. Kang, T. Lee, K. Son, and
Y. Yi, â€œLearning to schedule communication in multi-agent reinforcement
learning,â€ arXiv preprint arXiv:1902.01554, 2019.
[31] S. Li, J. K. Gupta, P. Morales, R. Allen, and M. J. Kochenderfer, â€œDeep
implicit coordination graphs for multi-agent reinforcement learning,â€
arXiv preprint arXiv:2006.11438, 2020.
[32] J. Jiang, C. Dun, T. Huang, and Z. Lu, â€œGraph convolutional reinforcement learning,â€ arXiv preprint arXiv:1810.09202, 2018.
[33] D. Chen, K. Zhang, Y. Wang, X. Yin, Z. Li, and D. Filev,
â€œCommunication-efficient decentralized multi-agent reinforcement learning for cooperative adaptive cruise control,â€ IEEE Trans. Intell. Veh.,
2024.

