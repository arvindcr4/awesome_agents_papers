
--- Page 1 ---
Deep Q-Learning with Gradient Target Tracking
Bumgeun Parkâˆ—
Electrical Engineering, KAIST
j4t123@kaist.ac.kr
Taeho Lee âˆ—
Electrical Engineering, KAIST
eho0228@kaist.ac.kr
Donghwan Lee
Electrical Engineering, KAIST
donghwan@kaist.ac.kr
Abstract
This paper introduces Q-learning with gradient target tracking, a novel reinforce-
ment learning framework that provides a learned continuous target update mecha-
nism as an alternative to the conventional hard update paradigm. In the standard
deep Q-network (DQN), the target network is a copy of the online networkâ€™s
weights, held fixed for a number of iterations before being periodically replaced via
a hard update. While this stabilizes training by providing consistent targets, it intro-
duces a new challenge: the hard update period must be carefully tuned to achieve
optimal performance. To address this issue, we propose two gradient-based target
update methods: DQN with asymmetric gradient target tracking (AGT2-DQN)
and DQN with symmetric gradient target tracking (SGT2-DQN). These methods
replace the conventional hard target updates with continuous and structured updates
using gradient descent, which effectively eliminates the need for manual tuning.
We provide a theoretical analysis proving the convergence of these methods in
tabular settings. Additionally, empirical evaluations demonstrate their advantages
over standard DQN baselines, which suggest that gradient-based target updates
can serve as an effective alternative to conventional target update mechanisms in
Q-learning.
1 Introduction
Recently, reinforcement learning (RL) [27] has seen remarkable success in solving sequential decision-
making problems [21, 18, 25, 24, 7, 8, 28, 30, 1]. In particular, deep Q-network (DQN) algorithm [21]
is a widely used approach for value-based RL, and has demonstrated human-level performance in
various complex tasks, such as playing Atari games. However, standard DQN suffers from tuning
challenges due to the periodic hard update of the target network. Specifically, the target Q-network is
updated only periodically via a hard update, where every few thousand iterations, the target networkâ€™s
weights are replaced by those of the online network. This strategy stabilizes learning but introduces a
new challenge: the hard update period must be carefully tuned to achieve optimal performance. An
alternative to hard updates is incremental updates, where the target network is updated gradually at
each step. Lillicrap et al. [18] popularized this approach in deep RL, using a soft update (or Polyak
averaging), where the target parameters move toward the online parameters with a small learning rate.
While this method mitigates instability, it still requires careful tuning of the averaging weight.
To address these challenges, we introduce a novel gradient-based target update framework for Q-
learning. Our approach replaces the standard hard target update with gradient-based target learning
approach, which continuously updates the target Q-function using gradient descent. In particular,
âˆ—Equal contribution.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).
arXiv:2503.16700v3  [cs.LG]  18 Jul 2025
--- Page 2 ---
we develop DQN with asymmetric gradient target tracking (AGT2-DQN) and DQN with symmetric
gradient target tracking (SGT2-DQN). AGT2-DQN replaces the hard update with a continuous,
gradient-based update for the target network. Instead of copying weights infrequently, the target
networkâ€™s parameters are treated as learnable, and they are adjusted at every step using gradient
descent on a mean squared loss between the target Q-network and online Q-network. SGT2-DQN
also uses continuous gradient-based target updates, but with a symmetric coupling between the online
and target networks. Both networks are updated with loss terms that drive them toward each otherâ€™s
predictions. Morever, SGT2-DQN adds an extra regularization term to the loss functions of both
networks that penalizes the difference between the online Q-value and target Q-value for the same
state-action. This means the online and target networks influence each otherâ€™s updates, which enforces
a more balanced interaction (each network â€œknowsâ€ about the otherâ€™s objective).
Unlike standard DQN, where the target network is updated only periodically, our methods ensure
continuous adaptation of the target function through gradient-based learning. The advantages of our
approach are supported by experiments on benchmark reinforcement learning tasks, which show that
AGT2-DQN and SGT2-DQN achieve learning performance comparable to standard DQN without
significant tuning steps.
Finally, we provide theoretical analysis, which shows that both the asymmetric and symmetric
algorithms converge to the optimal Q-values in the tabular setting. In fact, for the tabular versions
of AGT2-DQN/SGT2-DQN, called Q-learning with asymmetric gradient tracking (AGT2-QL) and
Q-learning with symmetric gradient tracking (SGT2-QL), we prove that under standard assumptions,
the online and target Q estimates will converge to Qâˆ— (the true optimal Q-function) with probability
one.
2 Related works
In the original DQN [21], the target network is a copy of the online networkâ€™s weights that is held
fixed for a number of iterations and then replaced (â€œhard updatedâ€) periodically. Researchers have
looked into several alternatives to this hard update mechanism: soft (Polyak) updates have been
introduced in [18]. Instead of copying the network every few thousand steps, an incremental update
can be done every step that updates the target parameters with a small learning rate towards the online
parameters. In [14, 15], the effect of the soft updates has been theoretically investigated for tabular
TD-learning and tabular Q-learning respectively.
DeepMellow [12] removes the target network entirely by modifying the Bellman update and achieves
faster learning without the target network. They replace the conventional max operator with a
Mellowmax operator, a smooth approximation of max that prevents over-optimistic value estimates.
Similarly, in [22], the authors propose adding a functional regularization (FR) therm to the DQN loss
and removing the target network, which is calle FR-DQN. In their FR-DQN algorithm, the Q-network
is trained with an extra loss term that measures the difference between the current Q-values and
those of a prior network (which is a periodically updated snapshot of the online network). This acts
similarly to a target network, but instead of using the prior networkâ€™s values as fixed targets, it uses
them as a reference to restrain the current networkâ€™s updates.
In summary, several strategies have been explored as alternatives to hard target updates. All these
methods share the goal of providing stability to Q-learning without the downsides of an abruptly
updated target. The new gradient target tracking approaches proposed in this paper contribute to this
landscape by offering a learned, continuous target update mechanism as yet another alternative to the
hard update paradigm.
3 Preliminaries
3.1 Markov decision problem
We consider the infinite-horizon discounted Markov decision problem and Markov decision process,
where the agent sequentially takes actions to maximize cumulative discounted rewards. In a Markov
decision process with the state-space S := {1, 2, . . . ,|S|} and action-space A := {1, 2, . . . ,|A|},
the decision maker selects an action a âˆˆ Aat the current state s âˆˆ S, then the state transits to the
next state sâ€² âˆˆ Swith probability P(sâ€²|s, a), and the transition incurs a rewardr(s, a, sâ€²) âˆˆ R, where
2
--- Page 3 ---
P(sâ€²|s, a) is the state transition probability from the current state s âˆˆ Sto the next state sâ€² âˆˆ Sunder
action a âˆˆ A, and r(s, a, sâ€²) is the reward function. Moreover, |S| and |A| denotes cardinalities of S
and A, respectively. For convenience, we consider a deterministic reward function and simply write
r(sk, ak, sk+1) =: rk+1, kâˆˆ {0, 1, . . .}. A policy Ï€ maps a state s âˆˆ Sto an action distribution
Ï€ : S â†’âˆ†A, where âˆ†A denotes the set of all probability distributions over the action space A. The
objective of the Markov decision problem is to find an optimal policy,Ï€âˆ—, such that the cumulative
discounted rewards over infinite time horizons is maximized, i.e.,
Ï€âˆ— := arg max
Ï€âˆˆÎ˜
E
" âˆžX
k=0
Î³krk+1
Ï€
#
,
where Î³ âˆˆ [0, 1) is the discount factor, Î˜ is the set of all policies, (s0, a0, s1, a1, . . .) is a state-action
trajectory generated by the Markov chain under policy Ï€, and E[Â·|Ï€] is an expectation conditioned on
the policy Ï€. Moreover, Q-function under policy Ï€ is defined as
QÏ€(s, a) = E
" âˆžX
k=0
Î³krk+1
s0 = s, a0 = a, Ï€
#
, (s, a) âˆˆ S Ã— A,
and the optimal Q-function is defined as Qâˆ—(s, a) = QÏ€âˆ—
(s, a) for all (s, a) âˆˆ S Ã— A. Once Qâˆ— is
known, then an optimal policy can be retrieved by the greedy policy Ï€âˆ—(s) = arg maxaâˆˆA Qâˆ—(s, a).
Throughout, we assume that the Markov decision process is ergodic so that the stationary state
distribution exists.
4 Deep Q-learning with gradient target tracking
Before introducing the main results, we first briefly review the standard DQN in [21]. DQN considers
the two optimal Q-function estimates, QÎ¸ called the online Q-network, and QÎ¸â€² called the target
Q-network. Here, Î¸ and Î¸â€² are called the online and target parameters, respectively. To update these
parameters, the following loss function is considered:
L(Î¸; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
(y âˆ’ QÎ¸(s, a))2
where B is the mini-batch uniformly sampled from the replay buffer D, |B| denotes the size of the
mini-batch, and y, fixed as a constant, is called the target, which is defined as
y = r + 1(sâ€²)Î³max
aâˆˆA
QÎ¸â€²(sâ€², a)
Here, 1(sâ€²) is an indicator function defined as
1(s) :=

0 if s = terminal state
1 else
The online Q-network QÎ¸ is then updated through the gradient descent step
Î¸ â† Î¸ âˆ’ Î±âˆ‡L(Î¸; B)
where the target parameter Î¸â€² is fixed as a constant. Then, the target Q-network QÎ¸â€² is updated
periodically by the online parameter with period C >0
Î¸â€² â† Î¸
Figure 1 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Cartpole
environment (Open AI gym). The results show that C = 10 gives the best learning performance
among the other choices of C. As the results indicate, achieving better learning performance requires
careful selection of C, which demands extensive trials and time.
In this paper, we propose two gradient-based target learning methods for DQN: the first algorithm
is called DQN with asymmetric gradient target tracking (AGT2-DQN), and the second is DQN
with symmetric gradient target tracking (SGT2-DQN). In AGT2-DQN, we employ two distinct Q-
networks, QÎ¸1 and QÎ¸2 , referred to as the online and target Q-networks, respectively. The parameters
Î¸1 and Î¸2 are called the online and target parameters, respectively. The corresponding loss functions
are defined as
L1(Î¸1; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
(y1 âˆ’ QÎ¸1 (s, a))2
3
--- Page 4 ---
Figure 1: Comparison of reward curves of DQN with different C in Cartpole environment.
and
L2(Î¸2; B) := Î²
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
(y2 âˆ’ QÎ¸2 (s, a))2
where Î² >0 is a constant weight, y1 and y2 are targets defined as
y1 = r + 1(sâ€²)Î³max
aâˆˆA
QÎ¸2 (sâ€², a), y 2 = QÎ¸1 (s, a)
Both Î¸1 and Î¸2 are updated through the gradient descent steps
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1(Î¸1; B), Î¸ 2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2(Î¸2; B),
where Î± >0 is the step-size. The overall algorithm is summarized in Appendix A. Note that the
online parameter update is identical to that in the standard DQN. The primary distinction arises from
the target parameter update, which employs the following gradient descent step for the loss function
L2:
Î¸2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2(Î¸2; B) = Î¸2 + Î± 1
|B|
X
(s,a,r,sâ€²)âˆˆB
(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))âˆ‡Î¸2 QÎ¸2 (s, a)
The above update treats the target network in DQN as a learnable parameter updated via gradient
descent, rather than updating it only periodically [ 21] or by polyak averaging [ 18]. This yields a
form of continuous update where the target slowly learns to predict the online Q-values (or some
combination thereof), instead of being replaced outright.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 2: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î². Cartpole
environment in OpenAI Gym is used here.
4
--- Page 5 ---
Figure 3: Comparison of reward curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50. Cartpole environment in OpenAI Gym is used here.
As noted in the previous section, the updates for the online and target parameters are asymmetric.
This asymmetry might lead to inefficiencies in finding the optimal solution since each update plays a
distinct role and does not consider the influence of the other.
The algorithm can be conceptualized as a two-player game, where the first player (online parameter
update) strives to solve the Bellman equation, while the second player (target parameter update)
aims to track the first playerâ€™s actions. In this setup, the utility functions of the two players are
asymmetric and do not consider the objectives of each other. This mismatch can lead to instability
and inefficiencies during the learning process. A potential alternative would be to design a game
where each playerâ€™s utility function explicitly incorporates the other playerâ€™s goal, leading to a more
balanced interaction. In such a framework, the utility functions would be symmetric, which could
potentially enhance the stability and efficiency of the algorithm.
Motivated by the previous discussions, we propose the second algorithm, referred to as SGT2-
DQN. Similar to AGT2-DQN, this algorithm also employs two Q-networks QÎ¸1 and QÎ¸2 , which are
called the online and target Q-networks, respectively. However, SGT2-DQN introduces a symmetric
approach to updating these networks, that aims to harmonize the interaction between them. In
particular, SGT2-DQN employs the following loss functions:
L1(Î¸1; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
[(y1 âˆ’ QÎ¸1 (s, a))2 + Î²(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2]
and
L2(Î¸2; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
[(y2 âˆ’ QÎ¸2 (s, a))2 + Î²(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2].
where Î² >0 is a constant weight representing the regularization weight, and the targets y1 and y2
are defined as
y1 = r + 1(sâ€²)Î³ max
aâˆˆA
QÎ¸2 (sâ€², a), y 2 = r + 1(sâ€²)Î³max
aâˆˆA
QÎ¸1 (sâ€², a).
Next, both Î¸1 and Î¸2 are updated through the gradient descent steps
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1(Î¸1; B), Î¸ 2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2(Î¸2; B),
where Î± >0 is the step-size. The overall algorithm is summarized in Appendix B. In SGT2-DQN,
both the online and target networks interact and adjust towards each other using gradient steps.
Moreover, this algorithm echoes the idea of double Q-learning in a gradient setting. Double Q-
learning maintains two estimates that learn from each otherâ€™s predictions [9]. SGT2-DQN similarly
lets two function approximators co-evolve and correct one another.
Figure 2 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100} in Cartpole environment. The results demonstrate that their learning
efficiency is comparable to DQN. However, one can observe that they are less sensitive to the
hyperparameter Î².
5
--- Page 6 ---
Figure 3 illustrates the learning curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50, where the hyperparameter for each method has been selected to achieve approximately the
best performance among the tested grid points.
The results show that while DQN exhibits slightly better learning efficiency than the proposed methods
in this environment. However, as mentioned before, the latter require less efforts for hyperparameter
tuning in this case.
We conducted similar experiments across multiple environments, and the results varied depending on
the environment. For instance, comparisons of learning curves are presented in Figure 4, where the
hyperparameters were roughly tuned to achieve optimal learning performance for each method.
As shown in Figure 4(a), DQN with C = 50, AGT2-DQN with Î² = 0.1, and SGT2-DQN with Î² = 1
demonstrate comparable performance in the Acrobot environment. In contrast, Figure 4(b) illustrates
that AGT2-DQN with Î² = 0.1 and SGT2-DQN with Î² = 0.1 exhibit better learning efficiency than
DQN with C = 500. A full comparison of results, including those with different hyperparameter
settings, is provided in the Appendix J. Moreover, Appendix J also includes comparisons in other
environments, which exhibit similar trends.
Overall, DQN, AGT2-DQN, and SGT2-DQN exhibited comparable learning performance on average.
In some environments, DQN outperformed the other two methods, while in others, AGT2-DQN and
SGT2-DQN achieved better learning performance than DQN. However, we observed that AGT2-DQN
and SGT2-DQN were slightly less sensitive to hyperparameters than DQN on average. Therefore, it
is difficult to conclude that the proposed AGT2-DQN and SGT2-DQN outperform DQN. However,
we argue that they offer interesting alternatives to the hard update paradigm in DQN.
(a) Acrobot: DQN with C = 50, AGT2-DQN
with Î² = 0.1, and SGT2-DQN with Î² = 1.
(b) Pendulum: DQN with C = 500, AGT2-
DQN with Î² = 0.1, and SGT2-DQN with Î² =
0.1.
Figure 4: Comparison of simulated reward curves of AGT2-DQN,SGT2-DQN, DQN
Beyond the tuning issue, our experience suggests that AGT2-DQN and SGT2-DQN achieve faster
learning speeds than DQN during the early learning phase. However, they frequently encounter
performance collapse after this phase. Similar to policy gradient methods, this collapse appears to
result from overshooting in the gradient target update, a phenomenon that may not occur in standard
DQN. We believe this issue can be mitigated by incorporating stabilization techniques, such as
clipping the loss function, similar to the approach used in proximal policy optimization (PPO) in [24].
5 Analysis of optimality
The proposed AGT2-DQN and SGT2-DQN aim to minimize their respective loss functions through
gradient descent steps. However, it is not currently clear how minimizing these errors in the loss
functions ensures that the computed Q-estimates are closer to the optimal Q-function. Therefore,
further investigation is required to establish a more definitive link between the reduction of loss
function errors and the accuracy of the Q-estimates in approximating the optimal Q-function. This
6
--- Page 7 ---
issue is the main goal of this section and will be critical for validating the effectiveness of the proposed
methods.
To simplify the analysis, let us assume that by minimizing the loss functions of AGT2-DQN, we can
approximately minimize the following expected loss functions:
L1(Î¸1) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a))2i
L2(Î¸2) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2

where U(S Ã—A) means the uniform distribution over the setS Ã—A. Note that we employ the uniform
distribution over S Ã— Ato simplify the analysis. In practice, this would correspond to the average
distribution of S Ã— Ain the replay buffer.
Next, suppose that the loss functions are minimized with
L1(Î¸1) â‰¤ Îµ, L 2(Î¸2) â‰¤ Îµ,
where Îµ >0 is a small number representing the maximum error of the two loss functions. Then, we
can derive the following error bounds on the corresponding solution:
Theorem 1. Suppose that the loss functions are minimized with
L1(Î¸1) â‰¤ Îµ, L 2(Î¸2) â‰¤ Îµ.
For the expected loss functions of AGT2-DQN, we have
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž â‰¤2
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
Its proof can be found in Appendix H. We can observe that the error converges to 0 as Îµ â†’ 0.
Moreover, the errors also depend on Î² >0 and as Î² â†’ âˆžthe error tends to be reduced.
Similarly, let us assume that by minimizing the loss functions of SGT2-DQN, we can approximately
minimize the following expected loss function:
L1(Î¸1) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
"
r(s, a, sâ€²) + Î³max
aâˆˆA
QÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)
2
+Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

,
L2(Î¸2) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
"
r(s, a, sâ€²) + Î³ max
aâˆˆA
QÎ¸1 (sâ€², a) âˆ’ QÎ¸2 (s, a)
2
+Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2

where U(S Ã— A) means the uniform distribution over the set S Ã— A. Then, we can establish the
following error bounds on the corresponding solution:
Theorem 2. Suppose that the loss functions are minimized with
L1(Î¸1) â‰¤ Îµ, L 2(Î¸2) â‰¤ Îµ
For the expected loss functions of SGT2-DQN, we have
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž, âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
Its proof can be found in Appendix I. As in the AGT2-DQN case, the error vanishes as Îµ â†’ 0.
7
--- Page 8 ---
6 Convergence analysis for tabular case
In this section, we provide theoretical convergence analysis of AGT2-DQN and SGT2-DQN in tabular
cases for conceptual investigations, where the tabular versions of AGT2-DQN and SGT2-DQN are
called Q-learning with asymmetric gradient tracking (AGT2-QL) and Q-learning with symmetric
gradient tracking (SGT2-QL), respectively. Let us first consider the following update of AGT2-QL:
QA
k+1(sk, ak) = QA
k (sk, ak) + Î±k

rk+1 + 1(sâ€²
k)Î³ max
aâˆˆA
QB
k (sâ€²
k, a) âˆ’ QA
k (sk, ak)

,
QB
k+1(sk, ak) = QB
k (sk, ak) + Î±kÎ²(QA
k (sk, ak) âˆ’ QB
k (sk, ak)).
where Î±k > 0 is the learning rate, Î² > 0 is a constant weight, and sâ€²
k can be interpreted in the
following two ways: 1) in the Markovian observation model, sâ€²
k = sk+1, i.e., the next state and 2) in
the i.i.d. observation model, sâ€²
k is the next state sampled at the current time, while it is not identical
to sk+1. In the second i.i.d. observation model, sâ€²
k is sampled independently from some distribution
d. In this paper, we consider the i.i.d. observation model for our convergence analysis to simplify the
overall analysis. However, we note that the analysis presented in this paper for the i.i.d. observation
model can be extended to the Markovian observation model with some modifications [19]. We also
acknowledge that such more sophisticated analyses are not the main focus in this paper. The main
goal of the convergence analysis in this section is to provide conceptual insights into the convergence
of the proposed algorithms. We also note that simulation results are provided in the Appendix G to
demonstrate the validity and convergence of AGT2-QL. Empirically, the results show that AGT2-QL
tends to converge faster for larger values ofÎ² >0.
We note that this algorithm is similar to the so-called averaging Q-learning developed in [15], which
can be seen as a precursor. The averaging Q-learning in [ 15] maintains two separate estimates,
the target estimate QB
k and the online estimate QA
k : the online estimate QA
k is for approximating
Q-function and updated through an online manner, whereas the target estimate QB
k is for computing
the target values and updated through taking Polyakâ€™s averaging. Only the difference is the update
rule for QB
k which depends on the transition (sk, ak).
AGT2-QL can be seen as a tabular version of AGT2-DQN due to the following insights: if we
consider the two loss functions
lA(QA) := 1
2(y1 âˆ’ QA(sk, ak))2, l B(QB) := Î²
2 (y2 âˆ’ QB(sk, ak))2
with
y1 = rk+1 + 1(sâ€²
k)Î³ max
aâˆˆA
QB
k (sk+1, a), y 2 = QA
k (sk, ak)
then each update in AGT2-QL can be interpreted as the gradient descent steps
QA
k+1(sk, ak) = QA
k (sk, ak) âˆ’ Î±k
âˆ‚lA(QA)
âˆ‚QA(sk, ak) (1)
and
QB
k+1(sk, ak) = QB
k (sk, ak) âˆ’ Î±k
âˆ‚lB(QB)
âˆ‚QB(sk, ak), (2)
In other words, AGT2-QL can be seen as an tabular and online implementation of AGT2-DQN,
whose convergence can be established as follows:
Theorem 3. Let us consider AGT2-QL and assume that the step-size satisfies
0 â‰¤ Î±k â‰¤ 1,
âˆžX
k=0
Î±k = âˆž,
âˆžX
k=0
Î±2
k < âˆž. (3)
Assume that (sk, ak, sâ€²
k) are i.i.d. samples, where ak is sampled from the fixed behavior policy b and
sk is sampled from a fixed state distribution d. Then for any Î² >0, QA
k â†’ Qâˆ— and QB
k â†’ Qâˆ— with
probability one.
The corresponding convergence analysis is given in Appendix E. In AGT2-QL, the update for the
target estimate QB
k and the online estimate QA
k are different, resulting in asymmetric updates for the
two values. We can extend the idea in AGT2-QL by symmetrizing the update rule. In particular, by
adding the two targets y1 and y2 and switching the roles of the target and online estimates, one can
8
--- Page 9 ---
construct the following loss functions:
lA(QA) := 1
2(y1 âˆ’ QA(sk, ak))2 + Î²
2 (QB(sk, ak) âˆ’ QA(sk, ak))2
and
lB(QB) := 1
2(y2 âˆ’ QB(sk, ak))2 + Î²
2 (QA(sk, ak) âˆ’ QB(sk, ak))2
with the targets
y1 = rk+1 + 1(sâ€²
k)Î³max
aâˆˆA
QB
k (sâ€²
k, a), y 2 = rk+1 + 1(sâ€²
k)Î³max
aâˆˆA
QA
k (sâ€²
k, a)
where Î² >0 is a constant representing the regularization weight. Applying the gradient steps in (1)
and (2) results in the following SGT2-QL:
QA
k+1(sk, ak) =QA
k (sk, ak) + Î±k{rk+1 + 1(sâ€²
k)Î³ max
aâˆˆA
QB
k (sâ€²
k, a) âˆ’ QA
k (sk, ak)
+ Î²(QB
k (sk, ak) âˆ’ QA
k (sk, ak))}
QB
k+1(sk, ak) =QB
k (sk, ak) + Î±k{rk+1 + 1(sâ€²
k)Î³ max
aâˆˆA
QA
k (sâ€²
k, a) âˆ’ QB
k (sk, ak)
+ Î²(QA
k (sk, ak) âˆ’ QB
k (sk, ak))}
which can be seen as a tabular counterpart of SGT2-DQN.
We can observe that now the updates for the online and target estimatesQB
k and QA
k are symmetric.
We also note that this variant can be seen as a Q-learning counterpart of the double TD-learning
developed in [14]. In this paper, we establish its convergence as follows:
Theorem 4. Let us consider SGT2-QL and assume that the step-size satisfies (3). Assume that
(sk, ak, sâ€²
k) are i.i.d. samples, where ak is sampled from the fixed behavior policyb and sk is sampled
from a fixed state distribution d. Then for any Î² >0, QA
k â†’ Qâˆ— and QB
k â†’ Qâˆ— with probability one.
The proof is provided in the Appendix F. We note that the convergence of SGT2-QL and its analysis
have not yet been investigated in the literature. Additionally, empirical results demonstrating the
convergence of this algorithm are included in the Appendix G. These results show that SGT2-QL
generally converges faster than AGT2-QL. Moreover, they indicate that the convergence speed of
SGT2-QL is less sensitive to the weight Î² >0 compared to AGT2-QL.
7 Conclusion
This paper introduces gradient target tracking frameworks as alternatives to the hard target update in
DQN. By replacing the periodic target updates with a continuous gradient-based tracking mechanism,
the proposed approach mitigates tuning challenges. Theoretical analysis establishes the convergence
of the proposed methods in tabular settings. These findings suggest that the gradient-based target
tracking is a promising alternative to conventional target update mechanisms in reinforcement
learning.
References
[1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and
stabilization for deep reinforcement learning. In International conference on machine learning,
pages 176â€“185, 2017.
[2] Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size Q-learning.
Systems & Control letters, 61(12):1203â€“1208, 2012.
[3] Shalabh Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic recursive algorithms for
optimization: simultaneous perturbation methods, volume 434. Springer, 2012.
[4] Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447â€“469, 2000.
[5] Zaiwei Chen, Siva T Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A lya-
punov theory for finite-sample guarantees of markovian stochastic approximation. Operations
Research, 72(4):1352â€“1367, 2024.
9
--- Page 10 ---
[6] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning.Journal of machine learning
Research, 5(Dec):1â€“25, 2003.
[7] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In International conference on machine learning, pages 1587â€“1596, 2018.
[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. In International
conference on machine learning, pages 1861â€“1870, 2018.
[9] Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,
pages 2613â€“2621, 2010.
[10] Morris W Hirsch and Hal Smith. Monotone dynamical systems. In Handbook of differential
equations: ordinary differential equations, volume 2, pages 239â€“357. Elsevier, 2006.
[11] Hassan K Khalil. Nonlinear systems. Upper Saddle River, 2002.
[12] Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: re-
moving the need for a target network in deep q-learning. In Proceedings of the twenty eighth
international joint conference on artificial intelligence, 2019.
[13] Harold Kushner and G. George Yin. Stochastic approximation and recursive algorithms and
applications, volume 35. Springer Science & Business Media, 2003.
[14] Donghwan Lee and Niao He. Target-based temporal-difference learning. In International
Conference on Machine Learning, pages 3713â€“3722, 2019.
[15] Donghwan Lee and Niao He. A unified switching system perspective and convergence analysis
of q-learning algorithms. Advances in Neural Information Processing Systems, 33:15556â€“15567,
2020.
[16] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asyn-
chronous q-learning: Sharper analysis and variance reduction. IEEE Transactions on Informa-
tion Theory, 68(1):448â€“473, 2021.
[17] Daniel Liberzon. Switching in systems and control. Springer Science & Business Media, 2003.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
[19] Han-Dong Lim and Donghwan Lee. Finite-time analysis of asynchronous q-learning under
diminishing step-size from control-theoretic view. IEEE Access, 2024.
[20] Hai Lin and Panos J Antsaklis. Stability and stabilizability of switched linear systems: a survey
of recent results. IEEE Transactions on Automatic control, 54(2):308â€“322, 2009.
[21] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[22] Alexandre PichÃ©, Joseph Marino, Gian Maria Marconi, Christopher Pal, and Moham-
mad Emtiyaz Khan. Beyond target networks: Improving deep q-learning with functional
regularization.
[23] AndrÃ© Platzer. Vector barrier certificates and comparison systems. In Formal Methods: 22nd
International Symposium, FM 2018, Held as Part of the Federated Logic Conference, FloC
2018, Oxford, UK, July 15-17, 2018, Proceedings, volume 10951, page 418, 2018.
[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
10
--- Page 11 ---
[25] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
tering the game of go with deep neural networks and tree search. nature, 529(7587):484â€“489,
2016.
[26] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine
learning, 3(1):9â€“44, 1988.
[27] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT Press,
1998.
[28] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
[29] Wolfgang Walter. Ordinary differential equations (graduate texts in mathematics). Springer,
1998.
[30] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.
Dueling network architectures for deep reinforcement learning. In International conference on
machine learning, pages 1995â€“2003, 2016.
11
--- Page 12 ---
A Deep Q-learning with asymmetric gradient target tracking
A detailed pseudo code of the proposed AGT2-DQN is given in Algorithm 1.
Algorithm 1 Deep Q-learning with asymmetric gradient target tracking (AGT2-DQN)
1: Initialize replay memory D to capacity |D|
2: Randomly initialize the online parameter Î¸1 âˆˆ Rm
3: Set Î¸2 = Î¸1
4: Set k = 0
5: for Episode i âˆˆ {1, 2, . . .} do
6: Observe s0
7: for t âˆˆ {0, 1, . . . , Ï„âˆ’ 1} do
8: Take an action at according to
at =

arg maxaâˆˆA QÎ¸1 (st, a) with probability 1 âˆ’ Îµ
a âˆ¼ uniform(A) with probability Îµ
9: Observe rt+1, st+1
10: Store the transition (st, at, rt+1, st+1) in D
11: Sample uniformly a random mini-batch B of transitions (s, a, r, sâ€²) from D
12: Set
L1(Î¸1; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
(y1 âˆ’ QÎ¸1 (s, a))2
and
L2(Î¸2; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
(y2 âˆ’ QÎ¸2 (s, a))2
where
y1 = r + 1(sâ€²)Î³ max
aâˆˆA
QÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)
and
y2 = QÎ¸1 (s, a)
13: Perform a gradient descent step
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1(Î¸1; B), Î¸ 2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2(Î¸2; B)
14: Set k â† k + 1
15: end for
16: end for
12
--- Page 13 ---
B Deep Q-learning with symmetric gradient target tracking
A detailed pseudo code of the proposed SGT2-DQN is given in Algorithm 2.
Algorithm 2 Deep Q-learning with symmetric gradient target tracking (SGT2-DQN)
1: Initialize replay memory D to capacity |D|
2: Randomly initialize the online parameter Î¸1 âˆˆ Rm
3: Set Î¸2 = Î¸1
4: Set k = 0
5: for Episode i âˆˆ {1, 2, . . .} do
6: Observe s0
7: for t âˆˆ {0, 1, . . . , Ï„âˆ’ 1} do
8: Take an action at according to
at =

arg maxaâˆˆA QÎ¸1 (st, a) with probability 1 âˆ’ Îµ
a âˆ¼ uniform(A) with probability Îµ
9: Observe rt+1, st+1
10: Store the transition (st, at, rt+1, st+1) in D
11: Sample uniformly a random mini-batch B of transitions (s, a, r, sâ€²) from D
12: Set
L1(Î¸1; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
[(y1 âˆ’ QÎ¸1 (s, a))2 + Î²(QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2]
and
L2(Î¸2; B) := 1
2
1
|B|
X
(s,a,r,sâ€²)âˆˆB
[(y2 âˆ’ QÎ¸2 (s, a))2 + Î²(QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2]
where
y1 = r + 1(sâ€²)Î³ max
aâˆˆA
QÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)
and
y2 = r + 1(sâ€²)Î³ max
aâˆˆA
QÎ¸1 (sâ€², a) âˆ’ QÎ¸2 (s, a)
13: Perform a gradient descent step
Î¸1 â† Î¸1 âˆ’ Î±âˆ‡Î¸1 L1(Î¸1; B), Î¸ 2 â† Î¸2 âˆ’ Î±âˆ‡Î¸2 L2(Î¸2; B)
14: Set k â† k + 1
15: end for
16: end for
13
--- Page 14 ---
C Convergence analysis with ODE model
C.1 Basics of nonlinear system theory
Let us consider the nonlinear system
d
dtxt = f(xt), x 0 = z, t âˆˆ R+, (4)
where xt âˆˆ Rn is the state, f : Rn â†’ Rn is a nonlinear mapping, and R+ denotes the set of
nonnegative real numbers. For simplicity, we assume that the solution to (4) exists and is unique. In
fact, this holds true so long as the mapping f is globally Lipschitz continuous.
Lemma 1 ([11, Theorem 3.2]). Let us consider the nonlinear system(4) and assume that f is globally
Lipschitz continuous, i.e.,
âˆ¥f(x) âˆ’ f(y)âˆ¥ â‰¤Lâˆ¥x âˆ’ yâˆ¥, âˆ€x, yâˆˆ Rn, (5)
for some L >0 and norm âˆ¥ Â· âˆ¥, then it has a unique solution x(t) for all t â‰¥ 0 and x(0) âˆˆ Rn.
An important concept in dealing with the nonlinear system is the equilibrium point. A point x = xe
in the state space is said to be an equilibrium point of (4) if it has the property that whenever the
state of the system starts at xe, it will remain at xe [11]. For (4), the equilibrium points are the
real roots of the equation f(x) = 0. The equilibrium point xe is said to be globally asymptotically
stable if for any initial state x0 âˆˆ Rn, xt â†’ xe as t â†’ âˆž. Now, we provide a vector comparison
principle [29, 10, 23] for multi-dimensional O.D.E., which will play a central role in the analysis
below. We first introduce the quasi-monotone increasing function, which is a necessary prerequisite
for the comparison principle.
Definition 1 (Quasi-monotone function) . A vector-valued function f : Rn â†’ Rn with f :=
[f1 f2 Â·Â·Â· fn]T is said to be quasi-monotone increasing if fi(x) â‰¤ fi(y) holds for all i âˆˆ
{1, 2, . . . , n} and x, yâˆˆ Rn such that xi = yi and xj â‰¤ yj for all j Ì¸= i.
An example of a quasi-monotone increasing function f is f(x) = Ax where A is a Metzler matrix,
which implies the off-diagonal elements ofA are nonnegative. Now, we introduce a vector comparison
principle. For completeness, we provide a simple proof tailored to our interests.
Lemma 2 (Vector comparison principle [10, Theorem 3.2]) . Suppose that f and f are globally
Lipschitz continuous. Let vt be a solution of the system
d
dtxt = f(xt), x 0 âˆˆ Rn, âˆ€ t â‰¥ 0,
assume that f is quasi-monotone increasing, and let vt be a solution of the system
d
dtvt = f(vt), v 0 < x0, âˆ€t â‰¥ 0, (6)
where f(v) â‰¤ f(v) holds for any v âˆˆ Rn. Then, vt â‰¤ xt for all t â‰¥ 0.
Proof. Instead of (6), first consider
d
dtvÎµ(t) = f(vÎµ(t)) âˆ’ Îµ1n, v Îµ(0) < x(0), âˆ€t â‰¥ 0
where Îµ >0 is a sufficiently small real number and 1n is a vector where all elements are ones, where
we use a different notation for the time index for convenience. Suppose that the statement is not true,
and let
tâˆ— := inf{t â‰¥ 0 : âˆƒi such that vÎµ,i(t) > xi(t)} < âˆž,
and let i be such index. By the definition of tâˆ—, we have that vÎµ,i(tâˆ—) = xi(tâˆ—) and vÎµ,j(tâˆ—) â‰¤ xj(tâˆ—)
for any j Ì¸= i. Then, since f is quasi-monotone increasing, we have
fi(vÎµ(tâˆ—)) â‰¤ fi(x(tâˆ—)). (7)
On the other hand, by the definition of tâˆ—, there exists a small Î´ >0 such that
vÎµ,i(tâˆ— + âˆ†t) > xi(tâˆ— + âˆ†t)
for all 0 < âˆ†t < Î´. Dividing both sides by âˆ†t and taking the limit âˆ†t â†’ 0, we have
Ë™vÎµ,i(tâˆ—) â‰¥ Ë™xi(tâˆ—) = fi(x(tâˆ—)). (8)
14
--- Page 15 ---
By the hypothesis, it holds that
d
dtvÎµ(t) = f(vÎµ(t)) âˆ’ Îµ1n < f(vÎµ(t)) â‰¤ f(vÎµ(t))
holds for all t â‰¥ 0. The inequality implies Ë™vÎµ,i(t) < fi(vÎµ(t)), which in combination with (8) leads
to fi(vÎµ(tâˆ—)) > fi(x(tâˆ—)). However, it contradicts with (7). Therefore, vÎµ(t) â‰¤ x(t) holds for all
t â‰¥ 0. Since the solution vÎµ(t) continuously depends on Îµ >0 [29, Chap. 13], taking the limit Îµ â†’ 0,
we conclude v0(t) â‰¤ x(t) holds for all t â‰¥ 0. This completes the proof.
C.2 Switching system theory
Let us consider the particular nonlinear system, the switched linear system,
d
dtxt = AÏƒtxt, x 0 = z âˆˆ Rn, t âˆˆ R+, (9)
where xt âˆˆ Rn is the state, Ïƒ âˆˆ M:= {1, 2, . . . , M} is called the mode, Ïƒt âˆˆ Mis called the
switching signal, and {AÏƒ, Ïƒâˆˆ M}are called the subsystem matrices. The switching signal can be
either arbitrary or controlled by the user under a certain switching policy. Especially, a state-feedback
switching policy is denoted by Ïƒ(xt). To prove the global asymptotic stability of the switching
system, we will use a fundamental algebraic stability condition of switching systems reported in [20].
More comprehensive surveys of stability of switching systems can be found in [20] and [17].
Lemma 3 ([20, Theorem 8]). The origin of the linear switching system (9) is the unique globally
asymptotically stable equilibrium point under arbitrary switchings, Ïƒt, if and only if there exist a full
column rank matrix , L âˆˆ RmÃ—n, m â‰¥ n, and a family of matrices, Â¯AÏƒ âˆˆ RmÃ—n, Ïƒâˆˆ M, with the
so-called strictly negative row dominating diagonal condition, i.e., for each Â¯AÏƒ, Ïƒâˆˆ M, its elements
satisfying
[ Â¯AÏƒ]ii +
X
jâˆˆ{1,2,...,n}\{i}
|[ Â¯AÏƒ]ij| < 0, âˆ€i âˆˆ {1, 2, . . . , m},
where [Â·]ij is the (i, j)-element of a matrix (Â·), such that the matrix relations
LAÏƒ = Â¯AÏƒL, âˆ€Ïƒ âˆˆ M,
are satisfied.
A more general class of systems is the affine switching system
d
dtxt = AÏƒtxt + bÏƒt, x 0 = z âˆˆ Rn, t âˆˆ R+, (10)
where bÏƒt âˆˆ Rn is the additional input vector. Due to the additional input bÏƒk , its stabilization
becomes much more challenging.
C.3 ODE-based stochastic approximation
Due to its generality, the convergence analyses of many RL algorithms rely on the ODE (ordinary
differential equation) approach [3, 13]. It analyzes convergence of general stochastic recursions by
examining stability of the associated ODE model based on the fact that the stochastic recursions with
diminishing step-sizes approximate the corresponding ODEs in the limit. One of the most popular
approach is based on the Borkar and Meyn theorem [4]. We now briefly introduce the Borkar and
Meynâ€™s ODE approach for analyzing convergence of the general stochastic recursions
Î¸k+1 = Î¸k + Î±k(f(Î¸k) + Îµk+1) (11)
where f : Rn â†’ Rn is a nonlinear mapping. Basic technical assumptions are given below.
Assumption 1.
1. The mapping f : Rn â†’ Rn is globally Lipschitz continuous and there exists a function
fâˆž : Rn â†’ Rn such that
lim
câ†’âˆž
f(cx)
c = fâˆž(x), âˆ€x âˆˆ Rn.
2. The origin in Rn is an asymptotically stable equilibrium for the ODE Ë™xt = fâˆž(xt).
15
--- Page 16 ---
3. There exists a unique globally asymptotically stable equilibrium Î¸e âˆˆ Rn for the ODE
Ë™xt = f(xt), i.e., xt â†’ Î¸e as t â†’ âˆž.
4. The sequence {Îµk, Gk, kâ‰¥ 1} with Gk = Ïƒ(Î¸i, Îµi, iâ‰¤ k) is a Martingale difference
sequence. In addition, there exists a constant C0 < âˆž such that for any initial Î¸0 âˆˆ Rn, we
have E[âˆ¥Îµk+1âˆ¥2|Gk] â‰¤ C0(1 + âˆ¥Î¸kâˆ¥2), âˆ€k â‰¥ 0.
5. The step-sizes satisfy Î±k > 0, Pâˆž
k=0 Î±k = âˆž, Pâˆž
k=0 Î±2
k < âˆž.
Lemma 4 ([4, Borkar and Meyn theorem]) . Suppose that Assumption 1 holds. For any initial
Î¸0 âˆˆ Rn, supkâ‰¥0 âˆ¥Î¸kâˆ¥ < âˆž with probability one. In addition, Î¸k â†’ Î¸e as k â†’ âˆžwith probability
one.
The Borkar and Meyn theorem states that under Assumption 1, the stochastic process (Î¸k)âˆž
k=0
generated by (11) is bounded and converges to Î¸e almost surely.
D Assumptions and definitions
In this paper, we focus on the following setting: {(sk, ak, sâ€²
k)}âˆž
k=0 are i.i.d. samples under the
behavior policy Î², where the time-invariant behavior policy is the policy by which the RL agent
actually behaves to collect experiences. Note that the notation sâ€²
k implies the next state sampled at
the time step k, which is used instead of sk+1 in order to distinguish sâ€²
k from sk+1. In this paper, the
notation sk+1 indicate the current state at the iteration step k + 1, while it does not depend on sk.
For simplicity, we assume that the state at each time is sampled from the stationary state distribution
p, and in this case, the state-action distribution at each time is identically given by
d(s, a) = p(s)b(a|s), (s, a) âˆˆ S Ã— A.
In this paper, we assume that the behavior policy b is time-invariant, and this scenario excludes the
common method of using the Îµ-greedy behavior policy with Îµ >0 because the Îµ-greedy behavior
policy depends on the current Q-iterate, and hence is time-varying. Moreover, the proposed analysis
cannot be easily extended to the analysis of Q-learning with the Îµ-greedy behavior policy. However,
we note that this assumption is common in the ODE approaches for Q-learning and TD-learning [26].
This assumption can be relaxed by considering a time-varying distribution. However, this direction
will not be addressed in this paper to simplify the presentation of the proofs.
Throughout, we make the following assumption for convenience.
Assumption 2. d(s, a) > 0 holds for all (s, a) âˆˆ S Ã— A.
This assumption ensures that every state-action pair is visited infinitely often for sufficient exploration.
It is applied when the state-action occupation frequency is given and has also been considered in [16]
and [5]. The work in [2] introduces an alternative exploration condition, known as the cover time
condition, which states that within a certain time period, every state-action pair is expected to be
visited at least once. Slightly different variations of the cover time condition have been used in [6]
and [16] for convergence rate analysis.
Throughout the paper, we will use the following matrix notations for compact dynamical system
representations:
P :=
ï£®
ï£¯ï£°
P1
...
P|A|
ï£¹
ï£ºï£», R:=
ï£®
ï£¯ï£°
R1
...
R|A|
ï£¹
ï£ºï£», Q:=
ï£®
ï£¯ï£°
Q(Â·, 1)
...
Q(Â·, |A|)
ï£¹
ï£ºï£»,
Da :=
ï£®
ï£¯ï£°
d(1, a)
...
d(|S|, a)
ï£¹
ï£ºï£», D:=
ï£®
ï£¯ï£°
D1
...
D|A|
ï£¹
ï£ºï£»,
where Pa = P(Â·, a,Â·) âˆˆ R|S|Ã—|S| , Q(Â·, a) âˆˆ R|S|, aâˆˆ Aand Ra(s) := E[r(s, a, sâ€²)|s, a]. Note that
P âˆˆ R|SÃ—A|Ã—|S| , R âˆˆ R|SÃ—A|, Q âˆˆ R|SÃ—A|, and D âˆˆ R|SÃ—A|Ã—|SÃ—A| . In this notation, Q-function
is encoded as a single vector Q âˆˆ R|SÃ—A|, which enumerates Q(s, a) for all s âˆˆ Sand a âˆˆ A. In
particular, the single value Q(s, a) can be written as
Q(s, a) = (ea âŠ— es)T Q,
16
--- Page 17 ---
where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. Note also that under Assumption 2, D is a
nonsingular diagonal matrix with strictly positive diagonal elements.
For any stochastic policy, Ï€ : S â†’âˆ†A, where âˆ†A is the set of all probability distributions over A,
we define the corresponding action transition matrix as
Î Ï€ :=
ï£®
ï£¯ï£¯ï£¯ï£°
Ï€(1)T âŠ— eT
1
Ï€(2)T âŠ— eT
2
...
Ï€(|S|)T âŠ— eT
|S|
ï£¹
ï£ºï£ºï£ºï£» âˆˆ R|S|Ã—|SÃ—A| , (12)
where es âˆˆ R|S|. Then, it is well known that PÎ Ï€ âˆˆ R|SÃ—A|Ã—|SÃ—A| is the transition probability
matrix of the state-action pair under policy Ï€. If we consider a deterministic policy, Ï€ : S â†’ A, the
stochastic policy can be replaced with the corresponding one-hot encoding vector âƒ— Ï€(s) := eÏ€(s) âˆˆ
âˆ†A, where ea âˆˆ R|A|, and the corresponding action transition matrix is identical to (12) with
Ï€ replaced with âƒ— Ï€. For any given Q âˆˆ R|SÃ—A|, denote the greedy policy w.r.t. Q as Ï€Q(s) :=
arg maxaâˆˆA Q(s, a) âˆˆ A. We will use the following shorthand frequently: Î Q := Î Ï€Q.
E Convergence of AGT2-QL
In this section, we study convergence of AGT2-QL. The full algorithm is described in Algorithm 3.
We now analyze the convergence of AGT2-QL using the same switching system approach [15].
Algorithm 3 AGT2-QL
1: Initialize QA
0 and QB
0 randomly.
2: for iteration k = 0, 1, . . .do
3: Sample (s, a)
4: Sample sâ€² âˆ¼ P(Â·|s, a) and r(s, a, sâ€²)
5: Update QA
k+1(s, a) = QA
k (s, a) + Î±k{r(s, a, sâ€²) + Î³ maxaâˆˆA QB
k (sâ€², a) âˆ’ QA
k (s, a)}
6: Update QB
k+1(s, a) = QB
k (s, a) + Î±kÎ²(QA
k (s, a) âˆ’ QB
k (s, a))
7: end for
In particular, the main theoretical tool is the Borkar and Meyn theorem [4]. To apply this framework,
it is crucial to study the corresponding ODE model and its asymptotic stability. Therefore, in the next
subsection, we first derive the ODE model for AGT2-QL.
E.1 Original system
Using the notation introduced in Appendix D, the update of can be rewritten as
QA
k+1 =QA
k + Î±k
n
(ea âŠ— es)(ea âŠ— es)T R
+Î³(ea âŠ— es)(esâ€²)T max
aâˆˆA
QB
k (Â·, a) âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k

QB
k+1 =QB
k + Î±kÎ²
n
(ea âŠ— es)(ea âŠ— es)T QA
k âˆ’ (ea âŠ— es)(ea âŠ— es)T QB
k
o
where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. The above update can be further expressed
as
QA
k+1 =QA
k + Î±k{DR + Î³DP Î QB
k
QB
k âˆ’ DQA
k + ÎµA
k+1}
QB
k+1 =QB
k + Î±k{Î²DQA
k âˆ’ Î²DQB
k + ÎµB
k+1}
where
ÎµA
k+1 =(ea âŠ— es)(ea âŠ— es)T R + Î³(ea âŠ— es)(esâ€²)T Î QB
k
QB
k
âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k âˆ’ (DR + Î³DP Î QB
k
QB
k âˆ’ DQA
k )
17
--- Page 18 ---
ÎµB
k+1 =(ea âŠ— es)(ea âŠ— es)T Î²QA
k âˆ’ (ea âŠ— es)(ea âŠ— es)T Î²QB
k âˆ’ Î²(DQA
k âˆ’ DQB
k )
As discussed in Appendix C.3, the convergence of AGT3-QL can be analyzed by evaluating the
stability of the corresponding continuous-time ODE
d
dt

QA
t
QB
t

=
 âˆ’D Î³DP Î QB
t
Î²D âˆ’Î²D

QA
t
QB
t

+

DR
0

,

QA
0
QB
0

âˆˆ R2|S||A|, , (13)
Using the Bellman equation (Î³DP Î Qâˆ— âˆ’ D)Qâˆ— + DR = 0, the above expressions can be rewritten
by
d
dt

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

=
 âˆ’D Î³DP Î QB
t
Î²D âˆ’Î²D

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

+
 Î³DP (Î QB
t
âˆ’ Î Qâˆ—)Qâˆ—
0

,

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

=z âˆˆ R2|S||A|. (14)
The above system is a linear switching system discussed in Appendix C.2. More precisely, let Î˜ be
the set of all deterministic policies, let us define a one-to-one mapping Ï† : Î˜ â†’ {1, 2, . . . ,|Î˜ Ã— Î˜|}
from two deterministic policies (Ï€A, Ï€B) âˆˆ Î˜ Ã— Î˜ to an integer in {1, 2, . . . ,|Î˜ Ã— Î˜|}, and define
Ai =

âˆ’D Î³DP Î Ï€B
Î²D âˆ’Î²D

âˆˆ R2|SÃ—A|Ã—2|SÃ—A|, b i =

Î³DP (Î Ï€B âˆ’ Î Ï€âˆ—
)Qâˆ—
0

âˆˆ R2|SÃ—A|
for all i = Ï†(Ï€A, Ï€B) and (Ï€A, Ï€B) âˆˆ Î˜ Ã— Î˜. Then, the above ODE can be written by the affine
switching system
d
dtxt = AÏƒ(xt)xt + bÏƒ(xt), x 0 = z âˆˆ R|S||A|,
where
xt :=

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

is the state, Ïƒ : R|S||A| â†’ {1, 2, . . . ,|Î˜ Ã— Î˜|} is a state-feedback switching policy defined by
Ïƒ(xt) := Ïˆ(Ï€QA
t
, Ï€QB
t
), and Ï€Q(s) = arg maxaâˆˆA Q(s, a).
Until now, we have derived an ODE model of AGT2-QL. The next goal is to prove its asymptotic
stability which is essential step to apply the Borkar and Meyn theorem [4]. Notably, proving the global
asymptotic stability of the above switching system without the affine term is relatively straightforward
using Lemma 3. However, existing theories do not support switching systems that include affine
terms.
To address this issue, we construct two comparison systems by leveraging the special structure of the
switching system and the greedy policy, and we establish their global asymptotic stability. Building
on the vector comparison principle introduced in Lemma 2, we then prove the asymptotic stability of
the desired affine switching system. We note that this approach was first proposed in [15], and we
follow its framework with modifications tailored to the proposed algorithms.
To proceed, defining the vector functions
f(x1, x2) :=

f1(x1, x2)
f2(x1, x2)

:=

âˆ’D Î³DP Î x2+Qâˆ—
Î²D âˆ’Î²D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
0

,
(14) can be written by the systems
d
dt

xt,1
xt,2

=

f1(xt,1, xt,2)
f2(xt,1, xt,2)

, z 0 =

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

, âˆ€t â‰¥ 0 (15)
with 
xt,1
xt,2

=

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

In the following, we present several lemmas to support the main analysis results.
Lemma 5. We have
f(x1, x2) =

âˆ’Dx1 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

18
--- Page 19 ---
Proof. It can be proved through
f(x1, x2) =

âˆ’D Î³DP Î x2+Qâˆ—
Î²D âˆ’Î²D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
0

=

âˆ’Dx1 + Î³DP Î x2+Qâˆ—x2 + Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

=

âˆ’Dx1 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

,
which completes the proof.
Lemma 6. f is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function forf1 and f2, sepa-
rately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and anithe element
of âˆ†x1 is zero. For f1, using Lemma 5, we have
eT
i f1(x1 + âˆ†x1, x2 + âˆ†x2)
= âˆ’ eT
i D(x1 + âˆ†x1) + Î³eT
i DP Î x2+âˆ†x2+Qâˆ—(x2 + âˆ†x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
= âˆ’ eT
i Dx1 + Î³eT
i DP Î x2+âˆ†x2+Qâˆ—(x2 + âˆ†x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
â‰¥ âˆ’eT
i Dx1 + Î³eT
i DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
=eT
i f1(x1, x2),
where the second line is due to âˆ’eT
i Dâˆ†x1 = 0 . Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eT
i f2(x1 + âˆ†x1, x2 + âˆ†x2) =Î²eT
i D(x1 + âˆ†x1) âˆ’ Î²eT
i D(x2 + âˆ†x2)
=Î²eT
i D(x1 + âˆ†x1) âˆ’ Î²eT
i Dx2
â‰¥Î²eT
i Dx1 âˆ’ Î²eT
i Dx2
=eT
i f2(x1, x2),
where the second line is due to eT
i Dâˆ†x2 = 0. Therefore, f is quasi-monotone increasing.
Lemma 7. f is globally Lipshcitz continuous.
Proof. From Lemma 5, one gets
f(x1, x2) =

âˆ’D 0
Î²D âˆ’Î²D

x1
x2

+

Î³DP Î x2+Qâˆ—(x2 + Qâˆ—)
0

+

âˆ’Î³DP Î Qâˆ—Qâˆ—
0

Therefore, we have the inequalities
âˆ¥f(x1, x2) âˆ’ f(y1, y2)âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D 0
Î²D âˆ’Î²D

y1
y2

âˆž
+ âˆ¥Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î y2+Qâˆ—(y2 + Qâˆ—)âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î y2+Qâˆ—(y2 + Qâˆ—)âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆž


x1
x2

âˆ’

y1
y2

âˆž
indicating that f is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Our main goal here is to establish the asymptotic stability of the system given in (14). To this end,
we will apply the tools introduced in Appendix C.2. In particular, we will derive the upper and lower
comparison systems as discussed before.
19
--- Page 20 ---
E.2 Upper comparison system
We consider the following system which is called the upper comparison system:
d
dt

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

=
âˆ’D Î³DP Î QB,u
t âˆ’Qâˆ—
Î²D âˆ’Î²D

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

,

QA,u
0 âˆ’ Qâˆ—
QB,u
0 âˆ’ Qâˆ—

>

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

âˆˆ R2|S||A|,
(16)
where â€˜>â€™ in the above equation implies the element-wise inequality.
Defining the vector functions
h(x1, x2) :=

h1(x1, x2)
h2(x1, x2)

:=

âˆ’D Î³DP Î x2
Î²D âˆ’Î²D

x1
x2

the upper comparison system can be written by the system
d
dt

xt,1
xt,2

=

h1(xt,1, xt,2)
h2(xt,1, xt,2)

, x 0 >

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

, âˆ€t â‰¥ 0
with 
xt,1
xt,2

=

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

.
In the following lemmas, we establish key properties of the upper comparison system.
Lemma 8. h is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for h1 and h2, sepa-
rately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and anithe element
of âˆ†x1 is zero. For h1, we have
eT
i h1(x1 + âˆ†x1, x2 + âˆ†x2) = âˆ’ eT
i D(x1 + âˆ†x1) + Î³eT
i DP Î x2+âˆ†x2 (x2 + âˆ†x2)
= âˆ’ eT
i Dx1 + Î³eT
i DP Î x2+âˆ†x2 (x2 + âˆ†x2)
â‰¥ âˆ’eT
i Dy1 + Î³eT
i DP Î x2 x2
=eT
i h1(x1, x2),
where the second line is due to âˆ’eT
i Dâˆ†x1 = 0 . Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eT
i h2(x1 + âˆ†x1, x2 + âˆ†x2) =Î²eT
i D(x1 + âˆ†x1) âˆ’ Î²eT
i D(x2 + âˆ†x2)
=Î²eT
i D(x1 + âˆ†x1) âˆ’ Î²eT
i Dx2
â‰¥Î²eT
i Dx1 âˆ’ Î²eT
i Dx2
=eT
i h2(x1, x2),
where the second line is due to eT
i Dâˆ†x2 = 0. Therefore, h is quasi-monotone increasing.
Lemma 9. h is globally Lipshcitz continuous.
Proof. We complete the proof through the inequalities
âˆ¥h(x1, x2) âˆ’ h(y1, y2)âˆ¥âˆž
=


âˆ’D Î³DP Î x2
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D Î³DP Î y2
Î²D âˆ’Î²D

y1
y2

âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D 0
Î²D âˆ’Î²D

y1
y2

âˆž
+ âˆ¥Î³DP Î x2 x1 âˆ’ Î³DP Î y2 y2âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥x1 âˆ’ y2âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆž


x1
x2

âˆ’

y1
y2

âˆž
indicating that h is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
20
--- Page 21 ---
Lemma 10. h(x1, x2) â‰¥ f(x1, x2) for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|, where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. Using Î³DP (Î x2 âˆ’ Î Qâˆ—)Qâˆ— â‰¤ 0 for all x2 âˆˆ R|S||A|, we obtain
f(x1, x2) =

âˆ’D Î³DP Î x2+Qâˆ—
Î²D âˆ’Î²D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
0

â‰¤

âˆ’D Î³DP Î x2+Qâˆ—
Î²D âˆ’Î²D

x1
x2

â‰¤

âˆ’D Î³DP Î x2
Î²D âˆ’Î²D

x1
x2

=h(x1, x2)
for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|. This completes the proof.
Based on the previous lemmas, we are now ready to prove that the solution of the upper comparison
system indeed upper-bounds the solution of the original system, which is the reason why (16) is
named as such.
Lemma 11. We have 
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â‰¥

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0,
where â€˜â‰¥â€™ denotes the element-wise inequality
Proof. The desired conclusion is obtained by Lemma 2 with Lemmas 7 to 10.
Next, we prove that the upper comparison system (16) is globally asymptotically stable at the origin.
Notably, the proof is simpler than that of the original system, as the upper comparison system does
not include the affine term.
Lemma 12. For any Î² >0, the origin is the unique globally asymptotically stable equilibrium point
of the upper comparison system (16).
Proof. For notational convenience, we define Î Ïƒ, Ïƒ âˆˆ Mas Î QB
t
such that Ïƒ = Ïˆ(Ï€QB
t
). Then, for
the upper comparison switching system, we apply Lemma 3 with
AÏƒ =

âˆ’D Î³DP Î Ïƒ
Î²D âˆ’Î²D

and
L =
I 0
0 Î³1/2I

,
which satisfies
LAÏƒ =
 I 0
0 Î³1/2I

âˆ’D Î³DP Î Ïƒ
Î²D âˆ’Î²D

=
 âˆ’D Î³DP Î Ïƒ
Î³1/2Î²D âˆ’Î³1/2Î²D

= Â¯AÏƒ
 I 0
0 Î³1/2I

with
Â¯AÏƒ =

âˆ’D Î³ 1/2DP Î Ïƒ
Î³1/2Î²D âˆ’Î²D

.
To check the strictly negative row dominating diagonal condition, fori âˆˆ {1, 2, . . . ,|S||A|}, we have
[ Â¯AÏƒ]ii +
X
jâˆˆ{1,2,...,n}\{i}
|[ Â¯AÏƒ]ij| =[âˆ’D]ii + Î³1/2[D]ii
X
jâˆˆ{1,2,...,n}\{i}
|[PÎ Ïƒ]ij|
â‰¤[âˆ’D]ii + Î³1/2[D]ii
â‰¤(âˆ’1 + Î³1/2)[D]ii < 0.
21
--- Page 22 ---
For i âˆˆ {|S||A|+ 1, |S||A| + 2, . . . ,2|S||A|}, we also have
[ Â¯AÏƒ]ii +
X
jâˆˆ{1,2,...,n}\{i}
|[ Â¯AÏƒ]ij| â‰¤ âˆ’[D]iiÎ² + [D]iiÎ²Î³1/2 = [D]iiÎ²(âˆ’1 + Î³1/2) < 0
for any Î² > 0. Therefore, the strictly negative row dominating diagonal condition is satisfied.
By Lemma 3, the origin of the upper comparison system is globally asymptotically stable.
E.3 Lower comparison system
Let us consider the so-called lower comparison system
d
dt

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

=

âˆ’D Î³DP Î Ï€Qâˆ—
Î²D âˆ’Î²D

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

,

QA,l
0 âˆ’ Qâˆ—
QB,l
0 âˆ’ Qâˆ—

<

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

âˆˆ R2|S||A|,
(17)
where â€˜<â€™ in the above equation implies the element-wise inequality. Here, we note that the lower
comparison system is simply a linear system. Therefore, its analysis is much simpler than the upper
and original system. Defining the vector functions
g(x1, x2) :=

g1(x1, x2)
g2(x1, x2)

=

âˆ’D Î³DP Î Ï€Qâˆ—
Î²D âˆ’Î²D

x1
x2

,
the lower comparison system can be written by
d
dt

xt,1
xt,2

=

g1(xt,1, xt,2)
g2(xt,1, xt,2)

, x 0 <

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

,
for all t â‰¥ 0.
In the sequel, we present key properties of the lower comparison system.
Lemma 13. g is globally Lipschitz continuous.
Proof. It is straightforward from the linearity. In particular, we complete the proof through the
inequalities
âˆ¥g(x1, x2) âˆ’ g(y1, y2)âˆ¥âˆž =


âˆ’D Î³DP Î Qâˆ—
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D Î³DP Î Qâˆ—
Î²D âˆ’Î²D

y1
y2

âˆž
â‰¤


âˆ’D Î³DP Î Qâˆ—
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
indicating that g is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 14. f(x1, x2) â‰¥ g(x1, x2) for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|, where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. We obtain
f(x1, x2) =

âˆ’D Î³DP Î x2+Qâˆ—
Î²D âˆ’Î²D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
0

=

âˆ’Dx1 + Î³DP Î x2+Qâˆ—x2 + Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

=

âˆ’Dx1 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

â‰¥

âˆ’Dx1 + Î³DP Î Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
Î²Dx1 âˆ’ Î²Dx2

=

âˆ’Dx1 + Î³DP Î Qâˆ—x2
Î²Dx1 âˆ’ Î²Dx2

=

âˆ’D Î³DP Î Qâˆ—
Î²D âˆ’Î²D

x1
x2

=g(x1, x2)
22
--- Page 23 ---
for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|. This completes the proof.
Similar to the upper comparison system, we prove that the solution of the lower comparison system
indeed provides a lower bound for the solution of the original system.
Lemma 15. We have 
QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

â‰¥

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0,
where â€˜â‰¥â€™ denotes the element-wise inequality.
Proof. The desired conclusion is obtained by Lemma 2with Lemmas 6, 7, 13 and 14.
Moreover, the next lemma proves that the lower comparison system is also globally asymptotically
stable at the origin.
Lemma 16. For any Î² >0, the origin is the unique globally asymptotically stable equilibrium point
of the lower comparison system (17).
Proof. The proof follows the same procedure as that of the upper comparison system. Therefore, the
detailed proof is omitted here to avoid repetition.
So far, we have established several key properties of the upper and lower comparison systems,
including their global asymptotic stability. In the next subsection, we prove the global asymptotic
stability of the original system based on these results.
E.4 Stability of the original system
We establish the global asymptotic stability of (14).
Theorem 5. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium
point of the original system (14). Equivalently,

Qâˆ—
Qâˆ—

is the unique globally asymptotically stable
equilibrium point of the original system (13).
Proof. By Lemmas 11 and 15, we have
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â‰¥

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

â‰¥

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0
Moreover, by Lemma 16 and Lemma 12, we have
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â†’ 0,

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

â†’ 0
as t â†’ âˆž. Therefore, the state of the original system also asymptotically converges to the origin.
This completes the proof.
E.5 Numerical example
In this subsection, we present a simple example to illustrate the validity of the properties of the upper
and lower comparison systems established in the previous sections. To this end, let us consider an
MDP with S = {1, 2}, A = {1, 2}, Î³ = 0.9,
P1 =

0.2 0 .8
0.3 0 .7

, P 2 =

0.5 0 .5
0.7 0 .3

,
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2, P[a = 2|s = 1] = 0.8,
P[a = 1|s = 2] = 0.7, P[a = 2|s = 2] = 0.3,
and
R1 =

3
1

, R 2 =

2
1

23
--- Page 24 ---
Simulated trajectories of the ODE model of AGT2-QL including the upper and lower comparison
systems are depicted in Figure 5 for QA
t part and Figure 6 for QB
t part. The results empirically prove
that the ODE model associated with AGT2-QL is asymptotically stable. Moreover, they illustrate that
the solutions of the upper and lower comparison systems bound the solution of the original system,
as established by the theory.
Figure 5: Trajectories of the O.D.E. model of AGT2-QL and the corresponding upper and lower
comparison systems ( QA
t part). The solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.
E.6 Convergence of AGT2-QL
We can now apply the Borkar and Meyn theorem, Lemma 4, to prove Theorem 3. First of all, note
that the system in (15) corresponds to the ODE model in Assumption 1. The proof is completed by
examining all the statements in Assumption 1:
E.6.1 Step 1:
AGT2-QL can be expressed as the stochastic recursion in (11) with
f

Î¸1
Î¸2

:=

âˆ’D Î³DP Î Î¸2
Î²D âˆ’Î²D

Î¸1
Î¸2

+

DR
0

.
Moreover, f is globally Lipschitz continuous according to Lemma 7.
To prove the first statement of Assumption 1, we note that
fâˆž

Î¸1
Î¸2

= lim
câ†’âˆž
f

cÎ¸1
cÎ¸2

c
24
--- Page 25 ---
Figure 6: Trajectories of the original O.D.E. model of AGT2-QL and the corresponding upper and
lower comparison systems (QB
t part). The solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.
= lim
câ†’âˆž

âˆ’D Î³DP Î cÎ¸2
Î²D âˆ’Î²D

cÎ¸1
cÎ¸2

+

DR
0

c
= lim
câ†’âˆž

âˆ’D Î³DP Î cÎ¸2
Î²D âˆ’Î²D

Î¸1
Î¸2

=

âˆ’D Î³DP Î Î¸2
Î²D âˆ’Î²D

Î¸1
Î¸2

where the last equality is due to the homogeneity of the policy, Ï€cÎ¸2 (s) = arg maxaâˆˆA cÎ¸2(s, a) =
arg maxaâˆˆAÎ¸2(s, a), where Î¸2(s, a) denotes the entry in the parameter vector Î¸2 corresponding to
the state-action pair (s, a) âˆˆ S Ã— A.
E.6.2 Step 2:
Let us consider the system
d
dt

Î¸1,t
Î¸2,t

= fâˆž

Î¸1,t
Î¸2,t

=

âˆ’D Î³DP Î Î¸2,t
Î²D âˆ’Î²D

Î¸1,t
Î¸2,t

.
Its global asymptotic stability around the origin can be easily proved following similar lines as in the
proof of the upper comparison system in Lemma 12.
E.6.3 Step 3:
According to Theorem 5, the ODE model of AGT2-QL is globally asymptotically stable around the
optimal solution

Î¸1
Î¸2

=

Qâˆ—
Qâˆ—

.
25
--- Page 26 ---
E.6.4 Step 4:
Next, we prove the remaining parts. Recall that AGT2-QL is expressed as
Â¯Qk+1 = Â¯Qk + Î±k{f( Â¯Qk) + Îµk+1}
where Â¯Qk :=

QA
k
QB
k

and
Îµk+1 :=

âˆ’(es âŠ— ea)(es âŠ— ea)T Î³(es âŠ— ea)eT
sâ€²Î QB
Î²(es âŠ— ea)(es âŠ— ea)T âˆ’Î²(es âŠ— ea)(es âŠ— ea)T

QA
QB

+

(es âŠ— ea)r(s, a, râ€²)
0

âˆ’

âˆ’D Î³DP Î QB
Î²D âˆ’Î²D

QA
QB

âˆ’

R
0

and
f

QA
QB

:=

âˆ’D Î³DP Î QB
Î²D âˆ’Î²D

QA
QB

+

DR
0

,
To proceed, let us define the history
Gk := (Â¯Îµk, Â¯Îµkâˆ’1, . . . ,Â¯Îµ1, Â¯Qk, Â¯Qkâˆ’1, . . . ,Â¯Q0)
Moreover, let us define the corresponding process (Mk)âˆž
k=0 with Mk := Pk
i=1 Îµi. Then, we can
prove that (Mk)âˆž
k=0 is Martingale. To do so, we can easily prove E[Îµk+1|Gk] = 0 by
E[Îµk+1|Gk] =E
 âˆ’(es âŠ— ea)(es âŠ— ea)T Î³ âˆ’ (es âŠ— ea)eT
sâ€²Î QB
Î²(es âŠ— ea)(es âŠ— ea)T âˆ’Î²(es âŠ— ea)(es âŠ— ea)T

QA
QB
Gk

âˆ’

âˆ’D Î³DP Î QB
Î²D âˆ’Î²D

QA
QB

=0
Using this identity, we have
E[Mk+1|Gk] =E
"k+1X
i=1
Îµi
Gk
#
= E[Îµk+1|Gk] + E
" kX
i=1
Îµi
Gk
#
=E
" kX
i=1
Îµi
Gk
#
=
kX
i=1
Îµi = Mk.
Therefore, (Mk)âˆž
k=0 is a Martingale sequence, and Îµk+1 = Mk+1 âˆ’ Mk is a Martingale difference.
Moreover, it can be easily proved that the fourth condition of Assumption 1 is satisfied by algebraic
calculations. Therefore, the fourth condition is met.
F Convergence of SGT2-QL
In this section, we study convergence of SGT2-QL. The full algorithm is described in Algorithm 4.
Similar to ATG2-GL, we analyze the convergence of SGT2-QL based on the same switching system
Algorithm 4 SGT2-QL
1: Initialize QA
0 and QB
0 randomly.
2: for iteration k = 0, 1, . . .do
3: Sample (s, a)
4: Sample sâ€² âˆ¼ P(Â·|s, a) and r(s, a, sâ€²)
5: Update QA
k+1(s, a) = QA
k (s, a) + Î±k{r(s, a, sâ€²) + Î³ maxaâˆˆA QB
k (sâ€², a) âˆ’ QA
k (s, a)} +
Î±kÎ²(QB(s, a)k âˆ’ QA
k (s, a))
6: Update QB
k+1(s, a) = QB
k (s, a) + Î±k{r(s, a, sâ€²) + Î³ maxaâˆˆA QA
k (sâ€², a) âˆ’ QB
k (s, a)} +
Î±kÎ²(QA(s, a)k âˆ’ QB
k (s, a))
7: end for
approach [15].
26
--- Page 27 ---
F.1 Original system
As before, our first goal is to derive an ODE model of SGT2-QL to apply the Borkar and Meyn
theorem Lemma 4. To begin with, the update rule of SGT2-QL in Algorithm 4 can be rewritten as
follows:
QA
k+1 =QA
k + Î±k
n
(ea âŠ— es)(ea âŠ— es)T R
+ Î³(ea âŠ— es)(esâ€²)T max
aâˆˆA
QB
k (Â·, a) âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k
+Î²((ea âŠ— es)(ea âŠ— es)T QB
k âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k )
o
,
QB
k+1 =QB
k + Î±k
n
(ea âŠ— es)(ea âŠ— es)T R
+ Î³(ea âŠ— es)(esâ€²)T max
aâˆˆA
QA
k (Â·, a) âˆ’ (ea âŠ— es)(ea âŠ— es)T QB
k
+Î²((ea âŠ— es)(ea âŠ— es)T QA
k âˆ’ (ea âŠ— es)(ea âŠ— es)T QB
k )
o
,
where es âˆˆ R|S| and ea âˆˆ R|A| are s-th basis vector (all components are 0 except for the s-th
component which is 1) and a-th basis vector, respectively. The above update can be further expressed
as
QA
k+1 = QA
k + Î±k{DR + Î³DP Î QB
k
QB
k âˆ’ DQA
k + Î²(QB
k âˆ’ QA
k ) + ÎµA
k+1}
QB
k+1 = QB
k + Î±k{DR + Î³DP Î QB
k
QA
k âˆ’ DQB
k + Î²(QA
k âˆ’ QB
k ) + ÎµB
k+1}
where
ÎµA
k+1 =(ea âŠ— es)(ea âŠ— es)T R + Î³(ea âŠ— es)(esâ€²)T Î QB
k
QB
k
âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k + (ea âŠ— es)(ea âŠ— es)T Î²(QB
k âˆ’ QB
k )
âˆ’ (DR + Î³DP Î QB
k
QB
k âˆ’ DQA
k + Î²(QB
k âˆ’ Qa
k))
ÎµA
k+1 =(ea âŠ— es)(ea âŠ— es)T R + Î³(ea âŠ— es)(esâ€²)T Î QB
k
QB
k
âˆ’ (ea âŠ— es)(ea âŠ— es)T QA
k + (ea âŠ— es)(ea âŠ— es)T Î²(QB
k âˆ’ QB
k )
âˆ’ (DR + Î³DP Î QB
k
QB
k âˆ’ DQA
k + Î²(QB
k âˆ’ Qa
k))
As discussed in Appendix C.3, the convergence of SGT3-QL can be analyzed by evaluating the
stability of the corresponding continuous-time ODE model
d
dt

QA
t
QB
t

=
 âˆ’(1 + Î²)D Î³DP Î QB
t
+ Î²D
Î³DP Î QA
t
+ Î²D âˆ’(1 + Î²)D

QA
t
QB
t

+

DR
DR

,

QA
0
QB
0

âˆˆ R2|S||A|.
(18)
Using the Bellman equation (Î³DP Î Qâˆ— âˆ’ D)Qâˆ— + DR = 0, the above expressions can be rewritten
by
d
dt

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

=
 âˆ’(1 + Î²)D Î³DP Î QB
t
+ Î²D
Î³DP Î QA
t
+ Î²D âˆ’(1 + Î²)D

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

+
 Î³DP (Î QB
t
âˆ’ Î Qâˆ—)Qâˆ—
Î³DP (Î QA
t
âˆ’ Î Qâˆ—)Qâˆ—

,

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

= z âˆˆ R2|S||A|. (19)
The above system is a linear switching system. More precisely, let Î˜ be the set of all deterministic
policies, let us define a one-to-one mapping Ï† : Î˜ â†’ {1, 2, . . . ,|Î˜ Ã— Î˜|} from two deterministic
policies (Ï€A, Ï€B) âˆˆ Î˜ Ã— Î˜ to an integer in {1, 2, . . . ,|Î˜ Ã— Î˜|}, and define
Ai =

âˆ’(1 + Î²)D Î³DP Î Ï€B + Î²D
Î³DP Î Ï€A + Î²D âˆ’(1 + Î²)D

âˆˆ R2|SÃ—A|Ã—2|SÃ—A|,
bi =

Î³DP (Î Ï€B âˆ’ Î Ï€âˆ—
)Qâˆ—
Î³DP (Î Ï€A âˆ’ Î Ï€âˆ—
)Qâˆ—

âˆˆ R2|SÃ—A|
27
--- Page 28 ---
for all i = Ï†(Ï€A, Ï€B) and (Ï€A, Ï€B) âˆˆ Î˜ Ã— Î˜. Then, the above ODE can be written by the affine
switching system
d
dtxt = AÏƒ(xt)xt + bÏƒ(xt), x 0 = z âˆˆ R|S||A|,
where
xt :=

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

is the state, Ïƒ : R|S||A| â†’ {1, 2, . . . ,|Î˜ Ã— Î˜|} is a state-feedback switching policy defined by
Ïƒ(xt) := Ïˆ(Ï€QA
t
, Ï€QB
t
), and Ï€Q(s) = arg maxaâˆˆA Q(s, a).
For convenience of analysis, let us define the following vector functions:
f(x1, x2) :=

f1(x1, x2)
f2(x1, x2)

:=

âˆ’(1 + Î²)D Î³DP Î x2+Qâˆ— + Î²D
Î³DP Î x1+Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
Î³DP (Î x1+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—

.
Then, (19) can be written by the system
d
dt

xt,1
xt,2

=

f1(xt,1, xt,2)
f2(xt,1, xt,2)

, z 0 =

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

, âˆ€t â‰¥ 0,
where 
xt,1
xt,2

=

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

is the state vector. Next, we present several useful lemmas that aid in the analysis of the ODE model.
Lemma 17. The function f can be represented by
f(x1, x2) =

âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1+Qâˆ—(x1 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—

.
Proof. The proof follows from the following algebraic manipulations:
f(x1, x2) =

âˆ’(1 + Î²)D Î³DP Î x2+Qâˆ— + Î²D
Î³DP Î x1+Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
Î³DP (Î x1+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—

=

âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2+Qâˆ—x2 + Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1+Qâˆ—x1 + Î³DP (Î x1+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—

=

âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1+Qâˆ—(x1 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—

.
This completes the proof.
Lemma 18. f is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function forf1 and f2, sepa-
rately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and anithe element
of âˆ†x1 is zero. For f1, using Lemma 17, we have
eT
i f1(x1 + âˆ†x1, x2 + âˆ†x2)
= âˆ’ (1 + Î²)eT
i D(x1 + âˆ†x1) + Î²D(x2 + âˆ†x2)
+ Î³eT
i DP Î x2+âˆ†x2+Qâˆ—(x2 + âˆ†x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
= âˆ’ (1 + Î²)eT
i Dx1 + Î²D(x2 + âˆ†x2) + Î³eT
i DP Î x2+âˆ†x2+Qâˆ—(x2 + âˆ†x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
â‰¥ âˆ’(1 + Î²)eT
i Dx1 + Î²Dx2 + Î³eT
i DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³eT
i DP Î Qâˆ—Qâˆ—
=eT
i f1(x1, x2),
where the second line is due to âˆ’(1 + Î²)eT
i Dâˆ†x1 = 0. Similarly, assuming that âˆ†x1 âˆˆ R|S|||A|
and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eT
i f2(x1 + âˆ†x1, x2 + âˆ†x2) â‰¥ eT
i f2(x1, x2),
by symmetry. Therefore, h is quasi-monotone increasing.
28
--- Page 29 ---
Lemma 19. f is globally Lipshcitz continuous.
Proof. From Lemma 17, one has
f(x1, x2) =

âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

x1
x2

+

Î³DP Î x2+Qâˆ—(x2 + Qâˆ—)
Î³DP Î x1+Qâˆ—(x1 + Qâˆ—)

+

âˆ’Î³DP Î Qâˆ—Qâˆ—
âˆ’Î³DP Î Qâˆ—Qâˆ—

.
sing this relation, we derive the following sequence of inequalities:
âˆ¥f(x1, x2) âˆ’ f(y1, y2)âˆ¥âˆž
â‰¤


âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

x1
x2

âˆ’

âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

y1
y2

âˆž
+ âˆ¥Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î y2+Qâˆ—(y2 + Qâˆ—)âˆ¥âˆž
+ âˆ¥Î³DP Î x1+Qâˆ—(x1 + Qâˆ—) âˆ’ Î³DP Î y1+Qâˆ—(y1 + Qâˆ—)âˆ¥âˆž
â‰¤


âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î y2+Qâˆ—(y2 + Qâˆ—)âˆ¥âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥Î x1+Qâˆ—(x1 + Qâˆ—) âˆ’ Î y1+Qâˆ—(y1 + Qâˆ—)âˆ¥âˆž
â‰¤


âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥x1 âˆ’ y1âˆ¥âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥x2 âˆ’ y2âˆ¥âˆž
â‰¤


âˆ’(1 + Î²)D Î²D
Î²D âˆ’(1 + Î²)D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ 2âˆ¥Î³DP âˆ¥âˆž


x1
x2

âˆ’

y1
y2

âˆž
indicating that f is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
F.2 Upper comparison system
We consider the following system which is called the upper comparison system:
d
dt

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

=
 âˆ’(1 + Î²)D Î³DP Î QB,u
t âˆ’Qâˆ— + Î²D
Î³DP Î QA,u
t âˆ’Qâˆ— + Î²D âˆ’(1 + Î²)D

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

,

QA,u
0 âˆ’ Qâˆ—
QB,u
0 âˆ’ Qâˆ—

>

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

âˆˆ R2|S||A|. (20)
Defining the following vector function:
h(x1, x2) :=

h1(x1, x2)
h2(x1, x2)

:=

âˆ’(1 + Î²)D Î³DP Î x2 + Î²D
Î³DP Î x1 + Î²D âˆ’(1 + Î²)D

x1
x2

the upper comparison system can be written by the system
d
dt

xt,1
xt,2

=

h1(xt,1, xt,2)
h2(xt,1, xt,2)

, x 0 >

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

, âˆ€t â‰¥ 0
where 
xt,1
xt,2

=

QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

is the state vector. Similar to the original ODE model, we derive several useful properties of the upper
comparison system.
Lemma 20. h is quasi-monotone increasing.
Proof. We will check the condition of the quasi-monotone increasing function for h1 and h2, sepa-
rately. Assume that âˆ†x1 âˆˆ R|S|||A| and âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and anithe element
29
--- Page 30 ---
of âˆ†x1 is zero. For h1, we have
eT
i h1(x1 + âˆ†x1, x2 + âˆ†x2)
= âˆ’ (1 + Î²)eT
i D(x1 + âˆ†x1) + Î²D(x2 + âˆ†x2) + Î³eT
i DP Î x2+âˆ†x2 (x2 + âˆ†x2)
= âˆ’ (1 + Î²)eT
i Dx1 + Î²D(x2 + âˆ†x2) + Î³eT
i DP Î x2+âˆ†x2 (x2 + âˆ†x2)
â‰¥ âˆ’(1 + Î²)eT
i Dx1 + Î²D(x2) + Î³eT
i DP Î x2 x2
=eT
i h1(x1, x2),
where the second line is due to âˆ’eT
i Dâˆ†x1 = 0 . Similarly, assuming that âˆ†x1 âˆˆ R|S|||A| and
âˆ†x2 âˆˆ R|S|||A| are nonnegative vectors, and an ithe element of âˆ†x2 is zero, we get
eT
i h2(x1 + âˆ†x1, x2 + âˆ†x2) â‰¥ eT
i h2(x1, x2),
by symmetry. Therefore, h is quasi-monotone increasing.
Lemma 21. h is globally Lipshcitz continuous.
Proof. We complete the proof through the inequalities
âˆ¥h(x1, x2) âˆ’ h(y1, y2)âˆ¥âˆž
=


âˆ’D Î³DP Î x2
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D Î³DP Î y2
Î²D âˆ’Î²D

y1
y2

âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

x1
x2

âˆ’

âˆ’D 0
Î²D âˆ’Î²D

y1
y2

âˆž
+ âˆ¥Î³DP Î x2 x1 âˆ’ Î³DP Î y2 y2âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆžâˆ¥x1 âˆ’ y2âˆ¥âˆž
â‰¤


âˆ’D 0
Î²D âˆ’Î²D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
+ âˆ¥Î³DP âˆ¥âˆž


x1
x2

âˆ’

y1
y2

âˆž
indicating that h is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 22. h(x1, x2) â‰¥ f(x1, x2) for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|, where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. The proof follows from the inequalities
f(x1, x2) =

âˆ’(1 + Î²)D Î³DP Î x2+Qâˆ— + Î²D
Î³DP Î x1+Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

+

Î³DP (Î x2+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—
Î³DP (Î x1+Qâˆ— âˆ’ Î Qâˆ—)Qâˆ—

â‰¤

âˆ’(1 + Î²)D Î³DP Î x2+Qâˆ— + Î²D
Î³DP Î x1+Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

â‰¤

âˆ’(1 + Î²)D Î³DP Î x2 + Î²D
Î³DP Î x1 + Î²D âˆ’(1 + Î²)D

x1
x2

=g(x1, x2)
for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|.
Based on the previous lemmas, we are now ready to prove that the solution of the upper comparison
system indeed upper-bounds the solution of the original system, which is the reason why (20) is
named as such.
Lemma 23. We have 
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â‰¥

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0,
where â€˜â‰¥â€™ denotes the element-wise inequality.
Proof. The desired conclusion is obtained by Lemma 2 with Lemmas 19 to 22.
30
--- Page 31 ---
Next, we prove that the upper comparison system (20) is globally asymptotically stable at the origin.
Lemma 24. For any Î² >0, the origin is the unique globally asymptotically stable equilibrium point
of the upper comparison system (20).
Proof. For notational convenience, we define Î Ïƒ, Ïƒ âˆˆ Mas Î QB
t
such that Ïƒ = Ïˆ(Ï€QB
t
). Then, for
the upper comparison switching system, we apply Lemma 3 with
AÏƒ1,Ïƒ2 =

âˆ’(1 + Î²)D Î³DP Î Ïƒ1 + Î²D
Î³DP Î Ïƒ2 + Î²D âˆ’(1 + Î²)D

and
L =

I 0
0 I

,
which satisfies
LAÏƒ1,Ïƒ2 = Â¯AÏƒ1,Ïƒ2 L
with
Â¯AÏƒ1,Ïƒ2 =

âˆ’(1 + Î²)D Î³DP Î Ïƒ1 + Î²D
Î³DP Î Ïƒ2 + Î²D âˆ’(1 + Î²)D

To check the strictly negative row dominating diagonal condition, fori âˆˆ {1, 2, . . . ,|S||A|}, we have
[ Â¯AÏƒ1,Ïƒ2 ]ii +
X
jâˆˆ{1,2,...,n}\{i}
|[ Â¯AÏƒ1,Ïƒ2 ]ij| = âˆ’ (1 + Î²)[D]ii + Î³[D]ii
X
jâˆˆ{1,2,...,n}\{i}
|[PÎ Ïƒ]ij| + Î²[D]ii
â‰¤ âˆ’(1 + Î²)[D]ii + (Î³ + Î²)[D]ii
=(Î³ âˆ’ 1)[D]ii
<0
For i âˆˆ {|S||A|+ 1, |S||A| + 2, . . . ,2|S||A|}, by symmetry, we also have the same result
[ Â¯AÏƒ1,Ïƒ2 ]ii +
X
jâˆˆ{1,2,...,n}\{i}
|[ Â¯AÏƒ1,Ïƒ2 ]ij| < 0
for any Î² > 0. Therefore, the strictly negative row dominating diagonal condition is satisfied.
By Lemma 3, the origin of the upper comparison system is globally asymptotically stable.
F.3 Lower comparison system
Let us consider the so-called lower comparison system
d
dt

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

=
 âˆ’(1 + Î²)D Î³DP Î Ï€Qâˆ— + Î²D
Î³DP Î Ï€Qâˆ— + Î²D âˆ’(1 + Î²)D

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

,

QA,l
0 âˆ’ Qâˆ—
QB,l
0 âˆ’ Qâˆ—

<

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

âˆˆ R2|S||A|, (21)
We note that the lower comparison system is simply a linear system. Defining the vector function
g(x1, x2) :=

g1(x1, x2)
g2(x1, x2)

=
 âˆ’(1 + Î²)D Î³DP Î Ï€Qâˆ— + Î²D
Î³DP Î Ï€Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

,
the lower comparison system can be written by
d
dt

xt,1
xt,2

=

g1(xt,1, xt,2)
g2(xt,1, xt,2)

, x 0 <

QA
0 âˆ’ Qâˆ—
QB
0 âˆ’ Qâˆ—

,
for all t â‰¥ 0. Below, we present several key properties of the lower comparison system.
Lemma 25. g is globally Lipschitz continuous.
Proof. It is straightforward from the linearity. In particular, we complete the proof through the
inequalities
âˆ¥g(x1, x2) âˆ’ g(y1, y2)âˆ¥âˆž â‰¤

 âˆ’(1 + Î²)D Î³DP Î Ï€Qâˆ— + Î²D
Î³DP Î Ï€Qâˆ— + Î²D âˆ’(1 + Î²)D

âˆž


x1
x2

âˆ’

y1
y2

âˆž
31
--- Page 32 ---
indicating that g is globally Lipschitz continuous with respect to the âˆ¥ Â· âˆ¥âˆž norm. This completes the
proof.
Lemma 26. f(x1, x2) â‰¥ g(x1, x2) for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|, where â€˜â‰¥â€™ denotes the
element-wise inequality.
Proof. The proof follows directly from the inequalities
f(x1, x2) =

âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î x2+Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î x1+Qâˆ—(x1 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—

â‰¥

âˆ’(1 + Î²)Dx1 + Î²Dx2 + Î³DP Î Qâˆ—(x2 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—
âˆ’(1 + Î²)Dx2 + Î²Dx1 + Î³DP Î Qâˆ—(x1 + Qâˆ—) âˆ’ Î³DP Î Qâˆ—Qâˆ—

=
 âˆ’(1 + Î²)D Î³DP Î Ï€Qâˆ— + Î²D
Î³DP Î Ï€Qâˆ— + Î²D âˆ’(1 + Î²)D

x1
x2

=g(x1, x2)
for all (x1, x2) âˆˆ R|S||A| Ã— R|S||A|. This completes the proof.
Similar to the upper comparison system, we prove that the solution of the lower comparison system
indeed provides a lower bound for the solution of the original system.
Lemma 27. We have 
QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

â‰¥

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0,
where â€˜â‰¥â€™ denotes the element-wise inequality.
Proof. The desired conclusion is obtained by Lemma 2with Lemmas 18, 19, 25 and 26.
Moreover, the next lemma proves that the lower comparison system is also globally asymptotically
stable at the origin.
Lemma 28. For any Î² >0, the origin is the unique globally asymptotically stable equilibrium point
of the lower comparison system (21).
Proof. For the proof, one can apply the same procedure as in the proof of the upper comparison
system.
So far, we have established several key properties of the upper and lower comparison systems,
including their global asymptotic stability. In the next subsection, we prove the global asymptotic
stability of the original system based on these results.
F.4 Stability of the original system
We establish the global asymptotic stability of (19).
Theorem 6. For any Î² > 0, the origin is the unique globally asymptotically stable equilibrium
point of the original system (19). Equivalently,

Qâˆ—
Qâˆ—

is the unique globally asymptotically stable
equilibrium point of the original system (18).
Proof. By Lemmas 23 and 27, we have
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â‰¥

QA
t âˆ’ Qâˆ—
QB
t âˆ’ Qâˆ—

â‰¥

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

, âˆ€t â‰¥ 0.
Moreover, by Lemma 28 and Lemma 24, we have
QA,u
t âˆ’ Qâˆ—
QB,u
t âˆ’ Qâˆ—

â†’ 0,

QA,l
t âˆ’ Qâˆ—
QB,l
t âˆ’ Qâˆ—

â†’ 0
as t â†’ âˆž. Therefore, the state of the original system also asymptotically converges to the origin.
This completes the proof.
32
--- Page 33 ---
F.5 Numerical example
In this subsection, we provide a simple example to illustrate the validity of the properties of the
upper and lower comparison systems established in the previous sections. Let us consider the MDP
previously considered for AGT2-QL with S = {1, 2}, A = {1, 2}, Î³ = 0.9,
P1 =

0.2 0 .8
0.3 0 .7

, P 2 =

0.5 0 .5
0.7 0 .3

,
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2, P[a = 2|s = 1] = 0.8,
P[a = 1|s = 2] = 0.7, P[a = 2|s = 2] = 0.3,
and the expected reward vectors
R1 =

3
1

, R 2 =

2
1

Solutions of the O.D.E. model of SGT2-QL and the upper and lower comparison systems are depicted
in Figure 7 for QA
t part and Figure 8 for QB
t part.
Figure 7: Trajectories of the O.D.E. model of SGT2-QL and the corresponding upper and lower
comparison systems (QA
t part). he solution of the ODE model (black line) is upper and lower bounded
by the upper and lower comparison systems, respectively (red and blue lines, respectively). This
result provides numerical evidence that the bounding rules hold.
As before, the simulation study empirically proves that the ODE model associated with SGT2-QL is
asymptotically stable. Moreover, they illustrate that the solutions of the upper and lower comparison
systems bound the solution of the original system, as established by the theory.
So far, we have examined key properties of the ODE model of SGT2-QL and proved its global
asymptotic stability, which is essential for establishing the convergence of SGT2-QL. In the next
subsection, we will analyze its convergence in detail.
33
--- Page 34 ---
Figure 8: Trajectories of the original O.D.E. model of SGT2-QL and the corresponding upper and
lower comparison systems (QB
t part). he solution of the ODE model (black line) is upper and lower
bounded by the upper and lower comparison systems, respectively (red and blue lines, respectively).
This result provides numerical evidence that the bounding rules hold.
F.6 Convergence of SGT2-QL
In this subsection, we apply the Borkar and Meyn theorem, Lemma 4, to prove the convergence of
SGT2-QL in Theorem 4. First of all, note that the system in (15) corresponds to the ODE model
in Assumption 1. The proof is completed by examining all the statements in Assumption 1:
F.6.1 Step 1:
SGT2-QL can be expressed as the stochastic recursion in (11) with
f

Î¸1
Î¸2

:=

âˆ’(1 + Î²)D Î³DP Î Î¸2 + Î²D
Î³DP Î Î¸1 + Î²D âˆ’(1 + Î²)D

Î¸1
Î¸2

+

DR
DR

.
Moreover, f is globally Lipschitz continuous according to Lemma 19.
To prove the first statement of Assumption 1, we note that
fâˆž

Î¸1
Î¸2

= lim
câ†’âˆž
f

cÎ¸1
cÎ¸2

c
= lim
câ†’âˆž

âˆ’(1 + Î²)D Î³DP Î cÎ¸2 + Î²D
Î³DP Î cÎ¸1 + Î²D âˆ’(1 + Î²)D

Î¸1
Î¸2

+

DR
DR

c
= lim
câ†’âˆž

âˆ’(1 + Î²)D Î³DP Î cÎ¸2 + Î²D
Î³DP Î cÎ¸1 + Î²D âˆ’(1 + Î²)D

Î¸1
Î¸2

34
--- Page 35 ---
=

âˆ’(1 + Î²)D Î³DP Î Î¸2 + Î²D
Î³DP Î Î¸1 + Î²D âˆ’(1 + Î²)D

Î¸1
Î¸2

where the last equality is due to the homogeneity of the policy, Ï€cÎ¸i(s) = arg maxaâˆˆAcÎ¸i(s, a) =
arg maxaâˆˆAÎ¸i(s, a) for i âˆˆ {1, 2}, where Î¸i(s, a) denotes the entry in the parameter vector Î¸i
corresponding to the state-action pair (s, a) âˆˆ S Ã— A.
F.6.2 Step 2:
Let us consider the system
d
dt

Î¸1,t
Î¸2,t

= fâˆž

Î¸1,t
Î¸2,t

=

âˆ’(1 + Î²)D Î³DP Î Î¸2,t + Î²D
Î³DP Î Î¸1,t + Î²D âˆ’(1 + Î²)D

Î¸1,t
Î¸2,t

.
Its global asymptotic stability around the origin can be easily proved following similar lines as in the
proof of the upper comparison system in Lemma 12.
F.6.3 Step 3:
According to Theorem 5, the ODE model of SGT2-QL is globally asymptotically stable around
Î¸1
Î¸2

=

Qâˆ—
Qâˆ—

.
F.6.4 Step 4:
Next, we prove the remaining parts. Recall that SGT2-QL is expressed as
Â¯Qk+1 = Â¯Qk + Î±k{f( Â¯Qk) + Îµk+1}
where Â¯Qk :=

QA
k
QB
k

and
Îµk+1 :=

âˆ’(1 + Î²)(es âŠ— ea)(es âŠ— ea)T Î³(es âŠ— ea)eT
sâ€²Î QB + Î²(es âŠ— ea)(es âŠ— ea)T
Î³(es âŠ— ea)eT
sâ€²Î QA + Î²(es âŠ— ea)(es âŠ— ea)T âˆ’(1 + Î²)(es âŠ— ea)(es âŠ— ea)T

QA
QB

+

(es âŠ— ea)(es âŠ— ea)T r(s, a, sâ€²)
(es âŠ— ea)(es âŠ— ea)T r(s, a, sâ€²)

âˆ’

âˆ’(1 + Î²)D Î³DP Î QB + Î²D
Î³DP Î QA + Î²D âˆ’(1 + Î²)D

QA
QB

+

R
R

and
f

QA
QB

:=

âˆ’(1 + Î²)D Î³DP Î QB + Î²D
Î³DP Î QA + Î²D âˆ’(1 + Î²)D

QA
QB

+

DR
DR

.
To proceed, let us define the history
Gk := (Â¯Îµk, Â¯Îµkâˆ’1, . . . ,Â¯Îµ1, Â¯Qk, Â¯Qkâˆ’1, . . . ,Â¯Q0)
Moreover, let us define the corresponding process (Mk)âˆž
k=0 with Mk := Pk
i=1 Îµi. Then, we can
prove that (Mk)âˆž
k=0 is Martingale. To do so, we can easily proveE[Îµk+1|Gk] = 0. Using this identity,
we have
E[Mk+1|Gk] =E
"k+1X
i=1
Îµi
Gk
#
= E[Îµk+1|Gk] + E
" kX
i=1
Îµi
Gk
#
=E
" kX
i=1
Îµi
Gk
#
=
kX
i=1
Îµi = Mk.
Therefore, (Mk)âˆž
k=0 is a Martingale sequence, and Îµk+1 = Mk+1 âˆ’ Mk is a Martingale difference.
Moreover, it can be easily proved that the fourth condition of Assumption 1 is satisfied by algebraic
calculations. Therefore, the fourth condition is met.
G Experiments on AGT2-QL and SGT2-QL
In this section, we empirically demonstrate the convergence of the proposed AGT2-QL and SGT2-QL
algorithms.
35
--- Page 36 ---
First of all, we consider the simple MDP considered in the previous sections with S = {1, 2},
A = {1, 2}, Î³ = 0.9,
P1 =

0.2 0 .8
0.3 0 .7

, P 2 =

0.5 0 .5
0.7 0 .3

,
a behavior policy Î² such that
P[a = 1|s = 1] = 0.2, P[a = 2|s = 1] = 0.8,
P[a = 1|s = 2] = 0.7, P[a = 2|s = 2] = 0.3,
and the expected reward vectors
R1 =

3
1

, R 2 =

2
1

.
Simulated errors |QA
k (s, a) âˆ’ Qâˆ—(s, a)|, |QB
k (s, a) âˆ’ Qâˆ—(s, a)|, (s, a) âˆˆ S Ã— Aof AGT2-QL are
depicted in Figure 9 for the QA
k part and in Figure 10 for the QB
k part. The results are presented for
different weight values Î² âˆˆ {0.01, 0.05, 0.1, 0.2, 0.5} and the diminishing step-size Î±k = 80
200+k .
The results demonstrate the convergence of AGT2-QL to the optimalQâˆ— over time. Moreover, the
results illustrate the convergence trends for varying weight Î². In particular, the larger the Î², the faster
the convergence.
Figure 9: Simulated errors of AGT2-QL (QA
k part)
Similarly, for the same environment, simulated errors |QA
k (s, a) âˆ’ Qâˆ—(s, a)|, |QB
k (s, a) âˆ’
Qâˆ—(s, a)|, (s, a) âˆˆ S Ã— A of AGT2-QL are depicted in Figure 11 for the QA
k part and in Fig-
ure 12 for the QB
k part with different weight values Î² âˆˆ {0.01, 0.05, 0.1, 0.2, 0.5}. The results
also demonstrate the convergence, but with different convergence trends for varying weight Î². In
particular, the error evolutions show that SGT2-QL generally converges faster than AGT2-QL, and
the convergence speeds are less sensitive to Î².
36
--- Page 37 ---
Figure 10: Simulated errors of AGT2-QL (QB
k part)
Additionally, we conduct experiments using grid world environments, including Taxi, FrozenLake,
and CliffWalk, in OpenAI Gym. Figure 13 presents reward curves for the Taxi environment, which
shows trends similar to previous results: AGT2-QL exhibits better learning performance for higher
values of Î², whereas SGT2-QL is not significantly affected by the choice of Î².
Similarly,Figure 14 presents reward curves for the FrozenLake environment, whileFigure 15 shows
reward curves for the CliffWalk environment. All results demonstrate that the proposed approaches,
AGT2-QL and SGT2-QL, are valid and effectively learn the optimal policy. Moreover, the learning
speeds of AGT2-QL and SGT2-QL are comparable to that of standard Q-learning.
37
--- Page 38 ---
Figure 11: Simulated errors of SGT2-QL (QA
k part)
38
--- Page 39 ---
Figure 12: Simulated errors of SGT2-QL (QB
k part)
39
--- Page 40 ---
(a) Reward curves of AGT2-QL with
different Î²
(b) Reward curves of SGT2-QL with dif-
ferent Î²
(c) Reward curves of AGT2-QL, SGT2-
QL, Q-learning, and double Q-learning
Figure 13: Comparison of simulated reward curves of AGT2-QL, SGT2-QL, Q-learning, double
Q-learning in Taxi environment
(a) Reward curves of AGT2-QL with
different Î²
(b) Reward curves of SGT2-QL with dif-
ferent Î²
(c) Reward curves of AGT2-QL, SGT2-
QL, Q-learning, and double Q-learning
Figure 14: Comparison of reward curves of AGT2-QL, SGT2-QL, Q-learning, double Q-learning in
Frozenlake environment
40
--- Page 41 ---
(a)
 (b)
(c)
Figure 15: Comparison of simulated reward curves of AGT2-QL, SGT2-QL, Q-learning, double
Q-learning in CliffWalk environment
41
--- Page 42 ---
H Proof of Theorem 1
Let us assume that by minimizing the loss functions of AGT2-DQN, we can approximately minimize
the following expected loss function
L1(Î¸1) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a))2i
L2(Î¸2) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2

where U(S Ã— A) means the uniform distribution over the set S Ã— A. Let us suppose that the loss
functions are minimized with the error
L1(Î¸1) â‰¤ Îµ, L 2(Î¸2) â‰¤ Îµ
To proceed further, note that using the law of iterated expectations, the expected loss functions can be
written by
L1(Î¸1) = E(s,a)âˆ¼U(SÃ—A)
h
Esâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a))2
s, a
ii
and
L2(Î¸2) = E(s,a)âˆ¼U(SÃ—A)

Esâ€²âˆ¼P(Â·|s,a)
 Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2
s, a

For convenience, let us define the Bellman operator
(T Q)(s, a) := R(s, a) + Î³
X
sâ€²âˆˆS
P(sâ€²|s, a)max
aâˆˆA
Q(sâ€², a)
Using Jensenâ€™s inequality, we can prove that
L1(Î¸1) â‰¥E(s,a)âˆ¼U(SÃ—A)
h 
Esâ€²âˆ¼P(Â·|s,a) [r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)]
2i
=E(s,a)âˆ¼U(SÃ—A)
h
((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2
i
â‰¥ 1
|S||A| ((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2
for all (s, a) âˆˆ S Ã— A, which implies
|(T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
p
Îµ|S||A|, âˆ€(s, a) âˆˆ S Ã— A
Similarly, for the loss function L2(Î¸2), we have
L2(Î¸2) = E(s,a)âˆ¼U(SÃ—A)
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

â‰¥ 1
|S||A|
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2
for all (s, a) âˆˆ S Ã— A, which implies
|QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
r2Îµ
Î² |S||A|, (s, a) âˆˆ S Ã— A. (22)
Based on the above results, we will establish bounds on QÎ¸1 and QÎ¸2 in the sequel.
H.1 Bound on QÎ¸1
First of all, using the reverse triangle inequality leads to
||QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|| â‰¤
p
Îµ|S||A|, (s, a) âˆˆ S Ã— A. (23)
To proceed, we consider the two cases:
Case 1: Let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| > |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|
Then, one can derive from (23)
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’ âˆ¥T QÎ¸2 âˆ’ T Qâˆ—âˆ¥âˆž
42
--- Page 43 ---
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’Î³âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž âˆ’ Î³âˆ¥QÎ¸2 âˆ’ QÎ¸1 âˆ¥âˆž
â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž âˆ’ Î³
r2Îµ
Î² |S||A|,
where (22) is used in the last line. This leads to
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž +
p
Îµ|S||A| + Î³
r2Îµ
Î² |S||A|
Taking the maximum on the left-hand side over (s, a) âˆˆ S Ã— Ayields
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤ Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž +
p
Îµ|S||A| + Î³
r2Îµ
Î² |S||A|
Arranging terms leads to
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
Case 2: Let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤ |(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|,
which leads to
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤|(T QÎ¸2 )(s, a) âˆ’ (T Qâˆ—)(s, a)|
â‰¤âˆ¥T QÎ¸2 âˆ’ T Qâˆ—âˆ¥âˆž
â‰¤Î³âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž
â‰¤Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž + Î³âˆ¥QÎ¸2 âˆ’ QÎ¸1 âˆ¥âˆž
â‰¤Î³âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž + Î³
r2Îµ
Î² |S||A|.
where the last inequality uses (22). After simple manipulations, we have
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤ Î³
1 âˆ’ Î³
r2Îµ
Î² |S||A|
Combining the two cases, one gets the following bound:
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
H.2 Bound on QÎ¸2
Next, using the reverse triangle inequality, we also havep
Îµ|S||A|
1 âˆ’ Î³ â‰¥|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a) + Qâˆ—(s, a) âˆ’ QÎ¸2 (s, a)|
â‰¥||QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| âˆ’ |QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)||. (24)
Now, we consider the two cases:
Case 1: Let us first consider the following case:
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| < |QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)|.
In this case, the inequality in (24) leads to
|QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + |QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)|
Applying the bound on |QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)|, we can derive
|QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤2
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
43
--- Page 44 ---
Now, taking the maximum on the left-hand side over (s, a) âˆˆ S Ã— Aleads to
âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž â‰¤ 2
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î² .
Case 2: Next, let us consider the case
|QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| â‰¥ |QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)|
In this case, the inequality in (24) results in
|QÎ¸2 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤ |QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)| â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
where the last inequality is due to the bound on |QÎ¸1 (s, a) âˆ’ Qâˆ—(s, a)|. Taking the maximum on the
left-hand side over (s, a) âˆˆ S Ã— Aleads to
âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î²
Combining the two cases, we have the desired conclusion.
I Proof of Theorem 2
Let us assume that by minimizing the loss functions of SGT2-DQN, we can approximately minimize
the following expected loss function
L1(Î¸1) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
"
r(s, a, sâ€²) + Î³ max
aâˆˆA
QÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)
2
+Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

L2(Î¸2) =E(s,a)âˆ¼U(SÃ—A),sâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸1 (sâ€², a) âˆ’ QÎ¸2 (s, a))2
+Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2

where U(S Ã— A) means the uniform distribution over the set S Ã— A. Suppose that the loss functions
are minimized with
L1(Î¸1) â‰¤ Îµ, L 2(Î¸2) â‰¤ Îµ.
For convenience, let us define the Bellman operator
(T Q)(s, a) := R(s, a) + Î³
X
sâ€²âˆˆS
P(sâ€²|s, a) max
aâˆˆA
Q(sâ€², a)
Moreover, note that using the law of iterated expectations, the expected loss functions can be expressed
as
L1(Î¸1) =E(s,a)âˆ¼U(SÃ—A)
h
Esâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a))2ii
+ E(s,a)âˆ¼U(SÃ—A)
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

,
and
L2(Î¸2) =E(s,a)âˆ¼U(SÃ—A)
h
Esâ€²âˆ¼P(Â·|s,a)
h
(r(s, a, sâ€²) + Î³maxaâˆˆAQÎ¸1 (sâ€², a) âˆ’ QÎ¸2 (s, a))2ii
+ E(s,a)âˆ¼U(SÃ—A)
Î²
2 (QÎ¸1 (s, a) âˆ’ QÎ¸2 (s, a))2

Next, applying Jensenâ€™s inequality to L1(Î¸1), we can prove that
L1(Î¸1) â‰¥E(s,a)âˆ¼U(SÃ—A)
"
Esâ€²âˆ¼P(Â·|s,a)

r(s, a, sâ€²) + Î³ max
aâˆˆA
QÎ¸2 (sâ€², a) âˆ’ QÎ¸1 (s, a)
2#
44
--- Page 45 ---
+ E(s,a)âˆ¼U(SÃ—A)
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

=E(s,a)âˆ¼U(SÃ—A)

((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2
+ E(s,a)âˆ¼U(SÃ—A)
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2

â‰¥ 1
|S||A| ((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2
+ 1
|S||A|
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2
for all (s, a) âˆˆ S Ã— A, which implies
1
|S||A| ((T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a))2 â‰¤ Îµ, 1
|S||A|
Î²
2 (QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a))2 â‰¤ Îµ,
and equivalently
|(T QÎ¸2 )(s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
p
Îµ|S||A|, |QÎ¸2 (s, a) âˆ’ QÎ¸1 (s, a)| â‰¤
s
2Îµ|S||A|
Î²
Next, applying the same procedures as in the proof for AGT2-DQN, we arrive at
âˆ¥QÎ¸1 âˆ’ Qâˆ—âˆ¥âˆž â‰¤
p
Îµ|S||A|
1 âˆ’ Î³ + Î³
1 âˆ’ Î³
s
2Îµ|S||A|
Î² .
By symmetry, one can obtain the same bound for âˆ¥QÎ¸2 âˆ’ Qâˆ—âˆ¥âˆž. This completes the proof.
45
--- Page 46 ---
J Additional comparison results among DQN, AGT2-DQN, and SGT2-DQN
J.1 Cartpole environment
Figure 16: Comparison of reward curves of DQN with different C in Cartpole environment. The
results show that C = 10 gives the best learning performance among the other options.
Figure 16 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Cartpole
environment. The results show that C = 10 gives the best learning performance among the other
choices of C.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 17: Comparison of reward curves of AGT2-DQN and SGT2-DQN with differentÎ² in Cartpole
environment.
Figure 17 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}in Cartpole environment. The results demonstrate that their learning ef-
ficiency is comparable to DQN but without requiring extensive tuning, as they appear to be less
sensitive to Î².
Figure 18 illustrates the learning curves of DQN with C = 10, AGT2-DQN and SGT2-DQN with
Î² = 50 in Cartpole environment. The results show that while DQN exhibits slightly better learning
efficiency than the proposed methods in this environment, while as mentioned before the latter require
less efforts for hyperparameter tuning.
46
--- Page 47 ---
Figure 18: Comparison of reward curves of DQN with C = 10, AGT2-DQN with Î² = 50 and
SGT2-DQN with Î² = 50 in Cartpole environment.
47
--- Page 48 ---
J.2 Acrobot environment
Figure 19: Comparison of reward curves of DQN with different C in Acrobot environment.
Figure 19 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Acrobot
environment.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 20: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in Acrobot
environment.
Figure 20 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}. The results demonstrate that their learning efficiency is comparable
to DQN and slightly less sensitive to the choice of the hyperparameter Î².
Figure 21 illustrates the learning curves of DQN with C = 50, AGT2-DQN with Î² = 0.1, and
SGT2-DQN with Î² = 1, where each hyperparameter has been selected to approximately yield the
best results. The results show that DQN exhibits slightly better learning efficiency than the proposed
methods.
48
--- Page 49 ---
Figure 21: Comparison of reward curves of DQN with C = 50, AGT2-DQN with Î² = 0.1 and
SGT2-DQN with Î² = 1 in Acrobot environment.
49
--- Page 50 ---
J.3 Pendulum environment
Figure 22: Comparison of reward curves of DQN with different C in Pendulum environment.
Figure 22 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Pendulum
environment, which are sensitive to the choice of the hyperparameter C.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 23: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Pendulum environment.
Figure 23 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}. The results demonstrate that their learning efficiency is comparable to
DQN. Moreover, compared to DQN, AGT2-DQN is less sensitive to the choice of the hyperparameter
Î², while SGT2-DQN is more sensitive to the choice of the hyperparameter.
Figure 24 illustrates the learning curves of DQN with C = 500, AGT2-DQN with Î² = 0.1, and
SGT2-DQN with Î² = 0.1, where each hyperparameter has been selected to approximately yield the
best results. The results show that in this case, AGT2-DQN and SGT2-DQN exhibit better learning
efficiency than DQN.
50
--- Page 51 ---
Figure 24: Comparison of reward curves of DQN with C = 500, AGT2-DQN with Î² = 0.1 and
SGT2-DQN with Î² = 0.1 in Pendulum environment.
51
--- Page 52 ---
J.4 Mountaincar environment
Figure 25: Comparison of reward curves of DQN with different C in Mountaincar environment.
Figure 25 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Pendulum
environment.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 26: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Mountaincar environment.
Figure 26 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}.
Figure 27: Comparison of reward curves of DQN with C = 200, AGT2-DQN with Î² = 50 and
SGT2-DQN with Î² = 10 in Mountaincar environment.
52
--- Page 53 ---
Figure 27 illustrates the learning curves of DQN with C = 200, AGT2-DQN with Î² = 50, and
SGT2-DQN with Î² = 10, where each hyperparameter has been selected to approximately yield the
best results.
Overall, the results reveal that DQN, AGT2-DQN, and SGT2-DQN provide similar learning efficien-
cies and sensitivity to the hyperparameters.
53
--- Page 54 ---
J.5 Lunarlander environment
Figure 28: Comparison of reward curves of DQN with different C in Lunarlander environment.
Figure 28 shows the learning curves of DQN for different C âˆˆ {1, 10, 100, 200, 500} in Lunarlander
environment.
(a) AGT2-DQN
 (b) SGT2-DQN
Figure 29: Comparison of reward curves of AGT2-DQN and SGT2-DQN with different Î² in
Lunarlander environment.
Figure 29 illustrates the learning curves of AGT2-DQN and SGT2-DQN for different Î² âˆˆ
{0.01, 0.1, 1, 10, 50, 100}.
Figure 30: Comparison of reward curves of DQN with C = 10, AGT2-DQN with Î² = 0.01 and
SGT2-DQN with Î² = 0.01in Lunarlander environment.
54
--- Page 55 ---
Figure 30 illustrates the learning curves of DQN with C = 10, AGT2-DQN with Î² = 0.01, and
SGT2-DQN with Î² = 0.01, where each hyperparameter has been selected to approximately yield the
best results.
Overall, the results indicate that DQN, AGT2-DQN, and SGT2-DQN exhibit similar learning effi-
ciencies and sensitivity to hyperparameters.
55