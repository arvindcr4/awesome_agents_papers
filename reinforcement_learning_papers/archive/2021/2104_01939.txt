NQMIX: Non-monotonic Value Function Factorization for Deep
Multi-Agent Reinforcement Learning
Quanlin Chen

Abstract
Multi-agent value-based approaches recently make great progress, especially value
decomposition methods. However, there are still a lot of limitations in value function
factorization. In VDN, the joint action-value function is the sum of per-agent actionvalue function while the joint action-value function of QMIX is the monotonic mixing
of per-agent action-value function. To some extent, QTRAN reduces the limitation of
joint action-value functions that can be represented, but it has unsatisfied performance
in complex tasks. In this paper, in order to extend the class of joint value functions that
can be represented, we propose a novel actor-critic method called NQMIX. NQMIX
introduces an off-policy policy gradient on QMIX and modify its network architecture,
which can remove the monotonicity constraint of QMIX and implement a nonmonotonic value function factorization for the joint action-value function. In addition,
NQMIX takes the state-value as the learning target, which overcomes the problem in
QMIX that the learning target is overestimated. Furthermore, NQMIX can be extended
to continuous action space settings by introducing deterministic policy gradient on itself.
Finally, we evaluate our actor-critic methods on SMAC domain, and show that it has a
stronger performance than COMA and QMIX on complex maps with heterogeneous
agent types. In addition, our ablation results show that our modification of mixer is
effective.

1

Introduction

Multi-agent reinforcement learning (MARL) can be used to solve many real-world
problems such as coordination of robot swarms [1] and autonomous cars [2]. But two
crucial challenges remain open in MARL. One is credit assignment problem[3]: we
only know the global reward returned from environment, but it is difficult for us to
deduce the local reward for each agent. The other is that joint action space of MARL
grows exponentially in the number of agents.
To cope with the credit assignment problem, VDN[4]and QMIX[5] learn to
maximize the individual action-value function ğ‘„ğ‘ conditioned on per-agent local
observation history, and then modeling the joint action-value function ğ‘„ğ‘¡ğ‘œğ‘¡ as the
mixing of individual value functions. In this way, global reward can be directly used to
train the joint action-value function. To address the complexity of joint action space,
QMIXâ€™s individual value function only takes each agentâ€™s local observation and action
as input while its joint value function only takes per-agent action-value as input.

But QMIX still has some limitations: (1) The monotonicity constraint of QMIX
between ğ‘„ğ‘¡ğ‘œğ‘¡ and each ğ‘„ğ‘ will restricts the class of joint action-value functions that
can be represented. (2) QMIX uses Q-learning to train the critic, which will
overestimate the learning target [6]. (3) Itâ€™s hard for QMIX to extend itself to continuous
action space settings. Actually, if the action space is continuous, then ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™) is
continuous function. Besides, ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™) is represented by neural networks, so
ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™) is a continuous nonconvex function that is hard for us to obtain the maximum
point.
To address these limitations, we propose a novel actor-critic method called NQMIX:
(1) NQMIX hybridizes off-policy policy-gradient and value-based methods so as to
implement non-monotonic value function factorization for the joint action-value
function. (2) NQMIX takes the state-value as the learning target to avoid overestimating
the learning target. (3) we apply deterministic policy gradient on NQMIX to extend it
to continuous action space settings.

2

Related Work

The simplest approach to extract policies in MARL is using Q-learning [7] to learn
an individual action-value function independently for each agent (independent Qlearning [8]) or using actor-critic to learn an individual policy and critic for each agent
(independent actor-critic[9]). However, this approach has the credit assignment
problem and non-stationarity problem without representing interactions between agents.
Another approach is policy-based methods. They always learn a fully centralized
value function which is used to train decentralized actor for each agent, which can
represent interactions between agents and overcome the non-stationarity problem. For
example, MADDPG[10] extends DDPG[11] to the MARL setting by introducing a
centralized critic, but it still has the credit assignment problem. Similarly, COMA[9]
also learns a centralized critic and then applies it to estimate a counterfactual baseline
which can handle the credit assignment problem. Besides, COMA proposes the
counterfactual multi-agent policy gradient, but it is an on-policy policy gradient which
limits the speed of learning.
In addition, some approaches are value-based methods. They always model the
centralized value function as mixing of individual value functions. VDN decomposes
the centralized action-value function into the sum of per-agent action-value function
while QMIX decomposes the centralized action-value function into the monotonic
combination of per-agent action-value function. Moreover QTRAN[12] proposes a
more general factorization than VDN or QMIX.

3

Background

A fully cooperative multi-agent task can be described as a Dec-POMDP[13] defined
as a tuple ğº = âŒ©ğ´, ğ‘†, ğ‘ˆ, ğ‘ƒ, ğ‘, ğ‘‚, ğ‘…âŒª. ğ´ = {1, â€¦ , ğ‘›} is a set of n agents. ğ‘† is true state

space of the environment. ğ‘ˆ =Ã—ğ‘–âˆˆğ´ ğ‘ˆğ‘– is the space of joint actions. P is a transition
function describing the probability ğ‘ƒ(ğ‘ â€²|ğ‘ , ğ®) . ğ‘ =Ã—ğ‘–âˆˆğ´ ğ‘ğ‘– is the space of joint
observations. O is an observation function describing the probability ğ‘ƒ(ğ‘œğ‘¡ |ğ‘ ğ‘¡ , ğ®ğ‘¡âˆ’1 ).
At each time step t, each agent ğ‘ âˆˆ ğ´ â‰¡ {1, â€¦ , ğ‘›} choose an action ğ‘¢ğ‘¡ğ‘ , which results
in a joint action ğ® âˆˆ ğ‘ˆ . The joint action will cause a transition of environment
described by transition function P. And the environment returns joint observations ğ’ =
(ğ‘œğ‘¡1 , ğ‘œğ‘¡2 , â‹¯ , ğ‘œğ‘¡ğ‘› ) and a global reward ğ‘Ÿ~ğ‘…(ğ‘ , ğ®). But each agent a can only receive its
local observation ğ‘œğ‘¡ğ‘ . Besides, Dec-POMDP use ğœ ğ‘ to represent an actionğ‘
observation history for agent a. At specific time step t, ğœğ‘¡ğ‘ = (ğ‘¢0ğ‘ , ğ‘œ1ğ‘ â€¦ , ğ‘¢ğ‘¡âˆ’1
, ğ‘œğ‘¡ğ‘ ). And
then a stochastic policy ğœ‹ ğ‘ for each agent a is modeled as a mapping from actionobservation history ğœ ğ‘ to the probability of action ğ‘¢ğ‘ . In other words,
ğœ‹ ğ‘ (ğ‘¢ğ‘ |ğœ ğ‘ , ğœ½): ğ‘‡ Ã— ğ‘ˆ â†’ [0,1].
QMIX is a value-based method that learns a monotonic joint action-value function
ğ‘„ğ‘¡ğ‘œğ‘¡ as a mixing of per-agent action-value. First, QMIX estimates per-agent actionvalue by a GRU network that takes local action-observation history ğœ ğ‘ as input. Peragent action-value function is denoted by ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™). Each agent a choose greedy actions
with respect to its ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™) and get the ğ‘„ğ‘ (ğœ ğ‘ , ğ‘¢ğ‘ ) . A mixing network with nonnegative weights is responsible for combining ğ‘„ğ‘ (ğœ ğ‘ , ğ‘¢ğ‘ ) into ğ‘„ğ‘¡ğ‘œğ‘¡ monotonically.
And non-negative weights ensure

ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ•ğ‘„ğ‘

â‰¥ 0, âˆ€ğ‘ âˆˆ ğ´, which guarantees that
argmax ğ‘„1 (ğœ 1 , ğ‘¢1 )
ğ‘¢1

argmax ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‰, ğ®) = [
]
â‹®
ğ®
ğ‘›
ğ‘›
argmax ğ‘„1 (ğœ , ğ‘¢ )

(1)

ğ‘¢ğ‘›

Policy Gradient[14] is a policy-based method that learns parameterized policy that
can select actions without consulting a value function. We use ğœ½ âˆˆ ğ‘…ğ‘‘ to represent the
policyâ€™s parameter vector. Then we write ğœ‹(ğ‘¢|ğ‘ , ğœ½) = ğ‘ƒğ‘Ÿ(ğ‘ˆğ‘¡ = ğ‘¢| ğ‘†ğ‘¡ = ğ‘ , ğœ½ğ‘¡ = ğœ½) as
parameterized policy. The return ğºğ‘¡ is the sum of discounted reward from time-step
ğ‘˜âˆ’ğ‘¡
t, ğºğ‘¡ = âˆ‘âˆ
ğ‘Ÿ(ğ‘ ğ‘˜ , ğ‘¢ğ‘˜ ) where 0 < Î³ < 1. Value functions are defined to be the
ğ‘˜=ğ‘¡ ğ›¾
expected return,ğ‘‰ ğœ‹ (ğ‘ ) = ğ¸(ğºğ‘¡ |ğ‘†ğ‘¡ = ğ‘ ; ğœ‹) and ğ‘„ğœ‹ (ğ‘ , ğ‘¢) = ğ¸(ğºğ‘¡ |ğ‘†ğ‘¡ = ğ‘ , ğ‘ˆğ‘¡ = ğ‘¢; ğœ‹) .
The agentâ€™s goal is to obtain a policy which maximizes the return from the start state,
denoted by the performance objective ğ½(ğœ‹) = ğ‘‰ ğœ‹ (ğ‘ 0 ) . From the policy gradient
theorem[14], we know that
âˆ‡ğœ½ ğ½(ğœ‹) = âˆ‘ ğ‘‘ ğœ‹ (ğ‘ ) âˆ‘ âˆ‡ğœ½ ğœ‹(ğ‘¢|ğ‘ , ğœ½)ğ‘„ğœ‹ (ğ‘ , ğ‘¢)
ğ‘ 

(2)

ğ‘¢

ğ‘¡
where ğ‘‘ ğœ‹ (ğ‘ ) = âˆ‘âˆ
ğ‘¡=0 ğ›¾ ğ‘ƒğ‘Ÿ{ğ‘†ğ‘¡ = ğ‘ |ğ‘ 0 , ğœ‹}.
Off-policy actor-critic[15] gives the off-policy policy-gradient

âˆ‡ğœ½ ğ½(ğœ½) â‰ˆ ğ‘”(ğœ½) = âˆ‘ ğ‘‘ ğ‘ (ğ‘ ) âˆ‘ âˆ‡ğœ½ ğœ‹(ğ‘¢|ğ‘ , ğœ½)ğ‘„ğœ‹ (ğ‘ , ğ‘¢)
ğ‘ 

(3)

ğ‘¢

where b denotes the behavior policy that is used to generate behavior and ğœ‹ denotes
the target policy that is evaluated and improved.

4

Methodology
4.1 Policy Gradient for agent a

The monotonicity constraint is to ensure the consistency of greedy policy, so a natural
thought is to replace greedy policy by parameterized policy. But there are more
problems to solve with the introduction of actor, such as the credit assignment.
Conventional single-agent policy gradient canâ€™t solve the credit assignment, such as
REINFORCE[16] whose form is as follows:
âˆ‡ğœƒ ğœ‹(ğ‘¢ğ‘¡ |ğ‘ ğ‘¡ , ğœ½)
ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ›¾ ğ‘¡ ğºğ‘¡
(4)
ğœ‹(ğ‘¢ğ‘¡ |ğ‘ ğ‘¡ , ğœ½)
where ğºğ‘¡ is difficult for us to compute, because we only know the global reward r, but
we donâ€™t know the local reward for each agent. Although COMA can cope with the
credit assignment problem in which the gradient is:
ğ‘” = ğ¸ğœ‹ [âˆ‡ğœƒ log ğœ‹ ğ‘ (ğ‘¢ğ‘ |ğœ ğ‘ ) ğ´ğ‘ (ğ‘ , ğ®)]

(5)

ğ´ğ‘ (ğ‘ , ğ®) = ğ‘„(ğ‘ , ğ®) âˆ’ âˆ‘ ğœ‹ ğ‘ (ğ‘¢â€²ğ‘ |ğœ ğ‘ ) ğ‘„(ğ‘ , (ğ®âˆ’ğ‘ , ğ‘¢â€²ğ‘ ))

(6)

ğ‘¢â€²ğ‘

yet COMA must be trained on-policy, which prevents us from using replay buffer and
limits the learning speed.
In order to learn off-policy, we use the all-actions method from the exercise 13.2 of
[16] (my solution is at Appendix A), in which we rewrite Equation 2 and 3 as an
expectation when there's a discount ğ›¾:
âˆ

âˆ‡ğœƒ ğ½(ğœ‹) = âˆ‘ ğ›¾ ğ‘¡ ğ¸ğ‘  [âˆ‘ âˆ‡ğœƒ ğœ‹(ğ‘¢|ğ‘ , ğœ½)ğ‘„ğœ‹ (ğ‘ , ğ‘¢)]
ğ‘¡=0

(7)

ğ‘¢

Equation 7 not only overcomes the credit assignment problem as it doesnâ€™t refer to the
global reward, but also is an off-policy policy gradient because we only instantiate
expectation using the sampled ğ‘ ğ‘¡ ~ğ‘ rather than instantiate expectation using the
sample action u. And then we apply this formula to perform gradient-ascent algorithm.
In detail, we instantiate the expectation using the sampled ğœğ‘¡ğ‘ and get stochastic
gradient-ascent formula:
ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ›¾ ğ‘¡ âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğœğ‘¡ğ‘ )ğ‘„ğœ‹ (ğœğ‘¡ğ‘ , ğ‘¢)

(8)

ğ‘¢

In addition, if the action space is continuous or very large, then we can use
deterministic policy gradient[17]:
âˆ‡ğœƒ ğ½(ğœ‡ğœƒ ) = ğ¸ğ‘ ~ğœŒğœ‡ [âˆ‡ğœƒ ğœ‡ğœƒ (ğ‘ )âˆ‡ğ‘¢ ğ‘„ğœ‡ (ğ‘ , ğ‘¢)|ğ‘¢=ğœ‡ğœƒ(ğ‘ ) ]

(9)

We also instantiate this expectation using sampled ğœğ‘¡ğ‘ and get another gradient-ascent
formula:

ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ›¾ ğ‘¡ âˆ‡ğœƒ ğœ‡ğœƒ (ğœğ‘¡ğ‘ )âˆ‡ğ‘¢ ğ‘„ğœ‡ (ğ‘ , ğ‘¢)|ğ‘¢=ğœ‡ğœƒ(ğœğ‘¡ğ‘ )

(10)

4.2 Remove the monotonicity constraint
The policy of QMIX is a greedy policy, so it needs monotonicity to ensure that each
agent always contributes to joint action-value. But when we learn a parameterized
policy, it is possible for us to choose to increase or decrease per-agent performance
based on the sign of

ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ•ğ‘„ğ‘

. In detail, for per agent a, if

ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ•ğ‘„ğ‘

> 0 , then we should

increase the performance ğ½(ğœ‹ ğ‘ ) of the agent a, which means that we should perform
gradient-ascent algorithm:
ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ›¾ ğ‘¡ âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğœğ‘¡ğ‘ )ğ‘„ğœ‹ (ğœğ‘¡ğ‘ , ğ‘¢)

(11)

ğ‘¢

On the contrary, if

ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ•ğ‘„ğ‘

< 0, then we should decrease the performance ğ½(ğœ‹ ğ‘ ) of agent

a, which means that we should perform gradient-descent algorithm:
ğœƒğ‘¡+1 = ğœƒğ‘¡ âˆ’ ğ›¼ğ›¾ ğ‘¡ âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğœğ‘¡ğ‘ )ğ‘„ğœ‹ (ğœğ‘¡ğ‘ , ğ‘¢)

(12)

ğ‘¢

In a word, Equation 11 and 12 can be rewrote as
ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ‘ ğ‘”ğ‘› (

ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡ ğ‘¡
) ğ›¾ âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğœğ‘¡ğ‘ )ğ‘„ğœ‹ (ğœğ‘¡ğ‘ , ğ‘¢)
ğœ•ğ‘„ğ‘

(13)

ğ‘¢

Similarly, for the continuous action space setting, we have
ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡ ğ‘¡
ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ›¼ğ‘ ğ‘”ğ‘› (
(14)
) ğ›¾ âˆ‡ğœƒ ğœ‡ğœƒ (ğœğ‘¡ğ‘ )âˆ‡ğ‘¢ ğ‘„ğœ‡ (ğœğ‘¡ğ‘ , ğ‘¢)|ğ‘¢=ğœ‡ğœƒ(ğœğ‘¡ğ‘ )
ğœ•ğ‘„ğ‘
In this way, we can remove the monotonicity constraint. Figure 1 illustrates the overall
setup.
For each agent a, we use DRQNs to encode per-agent observation ğ‘œğ‘¡ğ‘ and last action
ğ‘
ğ‘¢ğ‘¡âˆ’1
into a hidden state â„ğ‘¡ğ‘ representing the action-observation history ğœğ‘¡ğ‘ . And then
the hidden state â„ğ‘¡ğ‘ is not only passed into an MLP estimating per-agent action-value,
but also passed into a parameterized policy outputting the probability of available
actions, as show in Figure 1b. If we plan to reuse the DRQN for each agent, we need to
encode the â€œagent idâ€ into the input of DRQN, and then the input changes to
ğ‘
(ğ‘œğ‘¡ğ‘ , ğ‘¢ğ‘¡âˆ’1
, ğ‘).
The mixing network of QMIX is replaced by a MLP estimating the joint action-value
and the extra state ğ‘ ğ‘¡ is also directly passed into the MLP because there is no
monotonicity constraint, as show in Figure 1a.
Note that agents can reuse the critic network but canâ€™t reuse the actor policy network.
Because âˆ‘ğ‘¢ âˆ‡ğœ‹ ğ‘ (ğ‘¢|ğœğ‘¡ğ‘ , ğœ½)ğ‘„ğ‘ (ğœğ‘¡ğ‘ , ğ‘¢) is local for the agent a. If the term is mixed and
âˆ‘ğ‘¢ âˆ‡ğœ‹ ğ‘ (ğ‘¢|ğœğ‘¡ğ‘ , ğœ½)ğ‘„ğ‘ (ğœğ‘¡ğ‘ , ğ‘¢)
is reused by all agents, then the
and
ğ‘
ğ‘
ğ‘
âˆ‘ğ‘ âˆ‘ğ‘¢ âˆ‡ğœ‹ (ğ‘¢|ğœğ‘¡ , ğœ½)ğ‘„ğ‘ (ğœğ‘¡ , ğ‘¢) may have opposite signs, which will affect the gradient-

ascent or gradient-descent algorithm.
ğ‘„ğ‘ (ğœğ‘ , ğ‘¢ğ‘ğ‘¡ )

ğ‘„ğ‘¡ğ‘œğ‘¡ (ğ‰, ğ’–)

ğœ‹

MLP

ğ‘„ğ‘ (ğœ ğ‘ ,âˆ™)

ğ‘„1 (ğœğ‘¡1 , ğ‘¢ğ‘¡1 ), â‹¯ , ğ‘„ğ‘› (ğœğ‘¡ğ‘› , ğ‘¢ğ‘¡ğ‘› ), ğ‘ ğ‘¡
Agent 1

Agent N

(ğ‘œ1ğ‘¡ , ğ‘¢1ğ‘¡âˆ’1 )

(ğ‘œğ‘›ğ‘¡ , ğ‘¢ğ‘›ğ‘¡âˆ’1 )

MLP
â„ğ‘ğ‘¡âˆ’1

GRU
MLP

(ğ‘œğ‘ğ‘¡ , ğ‘¢ğ‘ğ‘¡âˆ’1 )
(a)

(b)

Figure 1: (a) The overall architecture of NQMIX. (b) Agent network structure

4.3 Algorithm
Algorithm 1 NQMIX
Input: an evaluate policy parameterization ğœ‹ ğ‘ (ğ‘¢|ğœ, ğœ½) for each agent a
Input: a target policy parameterization ğœ‹ ğ‘ (ğ‘¢|ğœ, ğœ½â€²) for each agent a
Input: an evaluate agent network reused by all agents
Input: a target agent network reused by all agents
Input: an evaluate mixing network
Input: a target mixing network
1: Loop
2:
Generate an episode following: ğ‘¢ğ‘¡ğ‘ ~ğœ‹ ğ‘ (âˆ™ |ğœğ‘¡ğ‘ , ğœ½) âˆ€ğ‘
3:
Add the episode to replay buffer
4:
Sample a random mini-batch of N episodes from replay buffer
5:
ğ¼â†1
6:
for each time step t:
ğ‘ )
7:
ğ‘„ğ‘ (ğœğ‘¡ğ‘ ,âˆ™) â† Agent_eval(ğ‘œğ‘¡ğ‘ , ğ‘¢ğ‘¡âˆ’1
âˆ€ğ‘
ğ‘
ğ‘)
â€² (ğœ ğ‘
8:
ğ‘„ğ‘ ,âˆ™) â† Agent_target(ğ‘œğ‘¡+1 , ğ‘¢ğ‘¡ âˆ€ğ‘
ğ‘ )
ğ‘
ğ‘
9:
ğ‘‰ğ‘â€² (ğœğ‘¡+1
â† âˆ‘ğ‘¢ ğœ‹ ğ‘ (ğ‘¢|ğœğ‘¡+1
, ğœ½â€²)ğ‘„ğ‘â€² (ğœğ‘¡+1
, ğ‘¢) âˆ€ğ‘
10:
ğ‘„ğ‘¡ğ‘œğ‘¡ â† Mixing_eval[ğ‘„1 (ğœğ‘¡1 , ğ‘¢ğ‘¡1 ), â‹¯ , ğ‘„ğ‘› (ğœğ‘¡ğ‘› , ğ‘¢ğ‘¡ğ‘› ), ğ‘ ğ‘¡ ]
ğ‘› ),
â€²
1 ),
11:
ğ‘‰ğ‘¡ğ‘œğ‘¡
â† Mixing_target[ğ‘‰1â€² (ğœğ‘¡+1
â‹¯ , ğ‘‰ğ‘›â€² (ğœğ‘¡+1
ğ‘ ğ‘¡+1 ]
â€²
12:
ğ›¿ â† ğ‘… + ğ›¾ğ‘‰ğ‘¡ğ‘œğ‘¡ âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡
13:
Update critic by minimizing the TD-error: ğ›¿
14:
Update the actor policy:
ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ½ â† ğœ½ + ğ›¼ğ¼ âˆ™ sign (
) âˆ‘ âˆ‡ğœ‹ ğ‘ (ğ‘¢|ğœğ‘¡ğ‘ , ğœ½)ğ‘„ğ‘ (ğœğ‘¡ğ‘ , ğ‘¢) âˆ€ğ‘
ğœ•ğ‘„ğ‘
ğ‘¢

â„ğ‘ğ‘¡

15:
16:

ğ¼ â† ğ›¾ğ¼
Update the target networks:
ğœ½â€² â† ğœğœ½ + (1 âˆ’ ğœ)ğœ½â€²
ğ’˜â€²ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ â† ğœğ’˜ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ + (1 âˆ’ ğœ)ğ’˜â€²ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡
ğ’˜â€²ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ â† ğœğ’˜ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ + (1 âˆ’ ğœ)ğ’˜â€²ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ
17: end for
18: end Loop
A new sampled episode will be put int replay buffer, and NQMIX sample a random
min-batch from replay buffer. NQMIX learned off-policy, so it can use previous
experience to train current actor and critic and mini-batch gradient descent and ascent
make each update more stable. The ability to use the replay buffer is one of the reasons
why NQMIX performs better than COMA.
The learning target of NQMIX is state value (9th code), which can avoid
overestimating learning target.
NQMIX uses soft update rather than replace target networks directly when updating
target networks (16th code).
Algorithm 2 NQMIX for continuous action space
Input: an evaluate policy parameterization ğœ‡ğ‘ (ğœ|ğœ½) for each agent a
Input: a target policy parameterization ğœ‡ğ‘ (ğœ|ğœ½â€²) for each agent a
Input: an evaluate agent network reused by all agents
Input: a target agent network reused by all agents
Input: an evaluate mixing network
Input: a target mixing network
1: Loop
2:
Generate an episode following: ğ‘¢ğ‘¡ğ‘ = ğœ‡ğ‘ (ğœ|ğœ½) âˆ€ğ‘
3: Add the episode to replay buffer
4:
Sample a random mini-batch of N episodes from replay buffer
5:
ğ¼â†1
6:
for each time step t:
ğ‘ )
7:
ğ‘„ğ‘ (ğœğ‘¡ğ‘ ,âˆ™) â† Agent_eval(ğ‘œğ‘¡ğ‘ , ğ‘¢ğ‘¡âˆ’1
âˆ€ğ‘
ğ‘
ğ‘)
â€² (ğœ ğ‘
8:
ğ‘„ğ‘ ,âˆ™) â† Agent_target(ğ‘œğ‘¡+1 , ğ‘¢ğ‘¡ âˆ€ğ‘
9:
ğ‘„ğ‘¡ğ‘œğ‘¡ â† Mixing_eval[ğ‘„1 (ğœğ‘¡1 , ğ‘¢ğ‘¡1 ), â‹¯ , ğ‘„ğ‘› (ğœğ‘¡ğ‘› , ğ‘¢ğ‘¡ğ‘› ), ğ‘ ğ‘¡ ]
10:

ğ‘›
ğ‘› |ğœ½â€²)),
â€²
1
1 |ğœ½â€²)),
ğ‘„ğ‘¡ğ‘œğ‘¡
â† Mixing_target[ğ‘„1â€² (ğœğ‘¡+1
, ğœ‡1 (ğœğ‘¡+1
â‹¯ , ğ‘„ğ‘›â€² (ğœğ‘¡+1
, ğœ‡ğ‘› (ğœğ‘¡+1
ğ‘ ğ‘¡+1 ]

11:
12:
13:

â€²
ğ›¿ â† ğ‘… + ğ›¾ğ‘„ğ‘¡ğ‘œğ‘¡
âˆ’ ğ‘„ğ‘¡ğ‘œğ‘¡
Update critic by minimizing the TD-error: ğ›¿
Update the actor policy:
ğœ•ğ‘„ğ‘¡ğ‘œğ‘¡
ğœ½ â† ğœ½ + ğ›¼ğ¼ âˆ™ sgn (
) âˆ‡ğœƒ ğœ‡ğ‘ (ğœğ‘¡ğ‘ |ğœ½)âˆ‡ğ‘¢ ğ‘„ğ‘ (ğœğ‘¡ğ‘ , ğ‘¢)|ğ‘¢=ğœ‡ğ‘ (ğœğ‘¡ğ‘) âˆ€ğ‘
ğœ•ğ‘„ğ‘
ğ¼ â† ğ›¾ğ¼
Update the target networks:
ğœ½â€² â† ğœğœ½ + (1 âˆ’ ğœ)ğœ½â€²
â€²
ğ’˜ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ â† ğœğ’˜ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡ + (1 âˆ’ ğœ)ğ’˜â€²ğ‘ğ‘”ğ‘’ğ‘›ğ‘¡

14:
15:

ğ’˜â€²ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ â† ğœğ’˜ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ + (1 âˆ’ ğœ)ğ’˜â€²ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ
16: end for
17: end Loop
In addition, QMIX is difficult to learn in a continuous action space setting, but
NQMIX can adapt to such settings, as show in Algorithm 2. At this time, NQMIX
adopts deterministic policy gradient and the policy is denoted as ğœ‡ğ‘ (ğœ|ğœ½): ğ‘‡ â†’ ğ‘ˆ. And
the learning target and the actorâ€™s updating formula need to be modified accordingly.

5

Experimental Results

5.1 Main Results
We test our algorithm on StarCraft II maps from the SMAC benchmark[18]. We
pause training every 5,000 time-steps and run 32 evaluation episodes with learned
parameterized policy or greedy policy. After training, we plot the mean win rate across
3 runs for each method on selected maps. Appendix B contains additional experimental
details. We compare our NQMIX, QMIX and COMA on several SMAC maps. And in
all maps, we set the difficulty to 7(very hard).

(a)
(b)
(c)
Figure 2. Win rates for QMIX, COMA and NQMIX on three different combat maps.
Figure 2(b) and 2(c) show the win rates of NQMIX on complex maps are better
than QMIX and COMA while Figure 2(a) show the win rates of NQMIX on simple
maps are between QMIX and COMA. Although NQMIX and QMIX have different
mixers, yet the number of parameters is similar for both mixers. As the number of
parameters in mixer gradually increases from Figure 2(a) to Figure 2(b), NQMIX will
outperform QMIX, especially on the maps with complex heterogeneous agent types.

5.2 Ablation Results

(a)
(b)
Figure 3. Win rates for QMIX, NQMIX, NQMIX-M on two different combat maps.
The ablation experiment is to explore whether the NQMIX should directly replace
the original mixer by MLP. First, a method called NQMIX-M is added in this section
and NQMIX-M continues to use the mixer of QMIX but remove the absolute activation
function of mixer.
Figure 3a shows that NQMIX-M has slightly lower performance than NQMIX but
still learns faster than QMIX. Figure 3b shows that NQMIX-M performs worse than
both NQMIX and QMIX. Ablation results show that NQMIX can obtain better
performance by modifying the mixer.

6

Conclusion and Future work

In this paper, we propose a novel method called NQMIX that can implement nonmonotonic value function factorization by introducing an actor on QMIX. Our results
in SMAC domain show that NQMIX has a stronger performance than QMIX and
COMA on maps with complex heterogeneous agent types. In the future work, we aim
to evaluate NQMIX on additional maps from SMAC to observe whether NQMIX will
be have a satisfactory performance on some other maps and whether the actor
introduced will become a new performance bottleneck.

7

Acknowledgements

References
[1] HÃ¼ttenrauch M, Adrian S, Neumann G. Deep reinforcement learning for swarm
systems[J]. Journal of Machine Learning Research, 2019, 20(54): 1-31.
[2] Cao Y, Yu W, Ren W, et al. An overview of recent progress in the study of
distributed multi-agent coordination[J]. IEEE Transactions on Industrial
informatics, 2012, 9(1): 427-438.
[3] Chang Y, Ho T, Kaelbling L. All learning is Local: Multi-agent Learning in Global
Reward Games[J]. Advances in Neural Information Processing Systems, 2003, 16:
807-814.
[4] Sunehag P, Lever G, Gruslys A, et al. Value-Decomposition Networks For
Cooperative Multi-Agent Learning Based On Team Reward[C]//PROCEEDINGS
OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS
AGENTS AND MULTIAGENT SYSTEMS (AAMAS'18). ASSOC
COMPUTING MACHINERY, 2018, 3: 2085-2087.
[5] Rashid T, Samvelyan M, Schroeder C, et al. Qmix: Monotonic value function
factorisation for deep multi-agent reinforcement learning[C]//International
Conference on Machine Learning. PMLR, 2018: 4295-4304.
[6] Van Hasselt H, Guez A, Silver D. Deep reinforcement learning with double qlearning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2016,
30(1).
[7] Watkins C J C H. Learning from delayed rewards[D]. University of Cambridge,
1989.
[8] Tan M. Multi-agent reinforcement learning: Independent vs. cooperative
agents[C]//Proceedings of the tenth international conference on machine learning.
1993: 330-337.
[9] Foerster J, Farquhar G, Afouras T, et al. Counterfactual multi-agent policy
gradients[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2018,
32(1).
[10] Lowe R, Wu Y, Tamar A, et al. Multi-agent actor-critic for mixed cooperativecompetitive environments[C]//Proceedings of the 31st International Conference on
Neural Information Processing Systems. 2017: 6382-6393.
[11] Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement
learning[C]//ICLR (Poster). 2016.
[12] Son K, Kim D, Kang W J, et al. Qtran: Learning to factorize with transformation
for cooperative multi-agent reinforcement learning[C]//International Conference
on Machine Learning. PMLR, 2019: 5887-5896.
[13] Oliehoek F A, Amato C. A concise introduction to decentralized POMDPs[M].
Springer, 2016.
[14] Sutton R S, McAllester D A, Singh S P, et al. Policy gradient methods for
reinforcement learning with function approximation[C]//NIPs. 1999, 99: 10571063.

[15] Degris T, White M, Sutton R. Off-Policy Actor-Critic[C]//International Conference
on Machine Learning. 2012.
[16] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press,
2018.
[17] Silver D, Lever G, Heess N, et al. Deterministic policy gradient
algorithms[C]//International conference on machine learning. PMLR, 2014: 387395.
[18] Samvelyan M, Rashid T, Schroeder de Witt C, et al. The StarCraft Multi-Agent
Challenge[C]//Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems. 2019: 2186-2188.

A. Solutions
My solutions to exercise 13.2 of [9]:
From Equation 3, we have
âˆ‡ğœƒ ğ½(ğœ‹) = âˆ‘ ğ‘‘ ğ‘ (ğ‘ ) âˆ‘ âˆ‡ğœƒ ğœ‹(ğ‘¢|ğ‘ , ğœ½)ğ‘„ğœ‹ (ğ‘ , ğ‘¢)
ğ‘ 
âˆ

ğ›¾ ğ‘¡ ğ‘ƒ{ğ‘†ğ‘¡ = ğ‘ |ğ‘ 0 , ğ‘} âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğ‘ )ğ‘„ğœ‹ (ğ‘ , ğ‘¢)

= âˆ‘âˆ‘
ğ‘ 
âˆ

=âˆ‘
ğ‘¡=0

ğ‘¢

ğ‘¡=0

ğ‘¢

ğ›¾ ğ‘¡ âˆ‘ ğ‘ƒ{ğ‘†ğ‘¡ = ğ‘ |ğ‘ 0 , ğ‘} [âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğ‘ )ğ‘„ğœ‹ (ğ‘ , ğ‘¢)]
ğ‘ 

ğ‘¢

âˆ

= âˆ‘ ğ›¾ ğ‘¡ ğ¸ğ‘  [âˆ‘ âˆ‡ğœ‹(ğ‘¢|ğ‘ )ğ‘„ğœ‹ (ğ‘ , ğ‘¢)]
ğ‘¡=0

ğ‘¢

B. Experimental Setup
B.1. Architecture of NQMIX
All agents reuse a Deep Recurrent Q-Networks which consist of a 64-dimensional
fully-connected layer, a GRU recurrent layer and a |ğ‘ˆ|-dimensional fully-connected
layer. In other words, this part of NQMIX is the same as QMIX. But each agent has its
local policy that is a 64-dimensional fully-connected layer with ReLU activation before
it. Each local policy takes the hidden state of GRU as input. In order to make the
numbers of parameters of mixers for both NQMIX and QMIX similar, the mixer of
NQMIX is a MLP consisting of two fully-connected layers where the number of units
of first layer is 32 Ã— (ğ´ + 4) âˆ’ ğ´ (A is the number of agents) and the second layer has
one unit.
We used Ï„ = 0.001 for the soft target updates and Î³ = 0.99 for a discount factor. We
use RMSprop for learning critic parameters with a learning rate of 5 Ã— 10âˆ’4 . We use
RMSprop for learning each actor policy parameters with a learning rate of 5 Ã— 10âˆ’4 .

