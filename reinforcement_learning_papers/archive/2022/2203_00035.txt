Can Mean Field Control (MFC) Approximate Cooperative Multi Agent
Reinforcement Learning (MARL) with Non-Uniform Interaction?

Washim Uddin Mondal1, 2

Vaneet Aggarwal1

Satish V. Ukkusuri2

1

arXiv:2203.00035v2 [cs.LG] 1 Jun 2022

2

School of Industrial Engineering, Purdue University, West Lafayette, Indiana, USA 47907
Lyles School of Civil Engineering, Purdue University, West Lafayette, Indiana, USA 47907

Abstract
Mean-Field Control (MFC) is a powerful tool
to solve Multi-Agent Reinforcement Learning
(MARL) problems. Recent studies have shown that
MFC can well-approximate MARL when the population size is large and the agents are exchangeable.
Unfortunately, the presumption of exchangeability
implies that all agents uniformly interact with one
another which is not true in many practical scenarios. In this article, we relax the assumption of
exchangeability and model the interaction between
agents via an arbitrary doubly stochastic matrix.
As a result, in our framework, the mean-field â€˜seenâ€™
by different agents are different. We prove that, if
the reward of each agent is an affine function of
the mean-field seen by that agent, then one can
approximate such a non-uniform MARL problem
via its associated
p MFCpproblem within an error of
e = O( âˆš1N [ |X | + |U|]) where N is the population size and |X |, |U| are the sizes of state and
action spaces respectively. Finally, we develop a
Natural Policy Gradient (NPG) algorithm that can
provide a solution to the non-uniform MARL with
an error O(max{e, }) and a sample complexity
of O(âˆ’3 ) for any  > 0.

1

INTRODUCTION

Multi-Agent Systems (MAS) are ubiquitous in the modern world. Many engineered systems such as transportation
networks, power distribution and wireless communication
systems can be modeled as MAS. Modeling, analysis and
control of such systems to improve the overall performance
is a central goal of research across multiple disciplines.
Multi-Agent Reinforcement Learning (MARL) is a popular
approach to achieve that target. In this article, we primarily
focus on cooperative MARL where the goal is to determine

policies for each individual agent such that the aggregate
cumulative reward of the entire population is maximized.
However, the sizes of joint state, and action spaces of the
population grows exponentially with the number of agents.
This makes the computation of the solution prohibitively
hard for large MAS.
Two major computationally efficient approaches have been
developed to tackle this problem. The first approach restricts
its attention to local policies. In other words, it is assumed
that each individual agent makes its decision solely based
on its local state/observation. Algorithms that fall into this
category are independent Q-learning (IQL) [Tan, 1993], centralised training and decentralised execution (CTDE) based
algorithms such as VDN [Sunehag et al., 2017], QMIX
[Rashid et al., 2018], WQMIX [Rashid et al., 2020], etc. Unfortunately, none of these algorithms can provide theoretical
convergence guarantees. The other approach is called meanfield control (MFC) [Angiuli et al., 2022]. It is grounded
on the idea that in an infinite population of homogeneous
agents, it is sufficient to study the behaviour of only one
representative agent in order to draw accurate conclusions
about the whole population. Recent studies have shown that,
if the agents are exchangeable, then MFC can be proven to
be a good approximation of MARL [Gu et al., 2021].
Unfortunately, the idea of exchangeability essentially states
that all agents in a population uniformly interact with each
other (uniform means that all pairwise interactions are the
same). This is not true in many practical scenarios. For example, in a traffic control network, the congestion at an intersection is highly influenced by the control policies adopted
at its immediate neighbouring intersections. Moreover, the
influence of an intersection on another intersection rapidly
diminishes with increase of their separation distance. Nonuniform interaction is a hallmark characteristic of many
other MASs such as social networks, wireless networks etc.
In the absence of uniformity of the interaction between the
agents, the framework of MFC no longer applies, and the
problem becomes challenging. In this paper, we come up a
new result which assures that even with non-uniform interac-

Accepted for the 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022).

tions, MFC is a good choice for approximating MARL if the
reward of each agent is an affine function of the mean-field
distributions â€˜seenâ€™ by that agent. We note that the behaviour
of agents in multitude of social and economic networks can
be modeled via affine rewards (refer the examples given in
[Chen et al., 2021]), and thus for many cases of practical
interest, MFC can approximate MARL with non-uniform
interactions.
1.1

CONTRIBUTIONS

We consider a non-uniform MARL setup where the pairwise
interaction between the agents is described by an arbitrary
doubly stochastic matrix (DSM). As a result of non-uniform
interaction, the so-called mean-field effect of the population
on an agent is determined by the identity of the agent. This
is in stark contrast with other existing works [Gu et al., 2021,
Mondal et al., 2022] where the presumption of exchangeability washes away the dependence on identity. We demonstrate that, if the reward of each agent is an affine function
of the mean-field distribution â€˜seenâ€™ by that agent, then the
standard MFC approach can approximate the
p non-uniform
p
MARL with an error bound of e , O( âˆš1N [ |X | + |U|]),
where N is the number of agents and |X |, |U| indicate the
sizes of state and action spaces of individual agent.
We would like to emphasize the importance of this result.
MFC is traditionally seen as an approximation method of
MARL when the agents are exchangeable and hence their
interactions are uniform. Uniformity allows us to solve MFC
problems by tracking only one representative agent. In this
paper, we show that, under certain conditions, a non-uniform
MARL can also be approximated by the MFC approach.
Thus, although the non-uniform interaction is a major part
of the original MARL problem, the assumed affine structure
of the reward function allows us to evade non-uniformity
while obtaining an approximate solution. The key result is
established in Lemma 8 (Appendix B.2) where, using the
affine structure of the reward function, we show that the instantaneous reward generated from non-uniform MARL can
be closely approximated by MFC-generated instantaneous
reward.
Finally, using the results of [Liu et al., 2020], in section 5,
we design a natural policy-gradient based algorithm that
can solve MFC within an error of O() for any  > 0, with
a sample complexity of O(âˆ’3 ). Invoking our approximation result, we prove that the devised algorithm can yield a
solution that is O(max{e, }) error away from the optimal
MARL solution, with a sample complexity of O(âˆ’3 ) for
any  > 0.
1.2

RELATED WORKS

Single Agent RL: The classical algorithms in single agent

learning include tabular Q-learning Watkins and Dayan
[1992], SARSA Rummery and Niranjan [1994], etc. Although they provide theoretical guarantees, these algorithms
can only be applied to small state-action space based systems due to their large memory requirements. Recently Neural Network (NN) based Q-iteration Mnih et al. [2015], and
policy gradient Mnih et al. [2016] algorithms have becomes
popular due to the large expressive power of NN. However,
they cannot be applied to large MAS due to the exponential
blow-up of joint state-space.
MFC as an Approximation to Uniform MARL: Recently,
MFC is gaining traction as a scalable approximate solution
to uniform MARL. On the theory side, recently it has been
proven that MFCâˆšcan approximate uniform MARL within
an error of O(1/ N ) [Gu et al., 2021]. However, the result
relies on the assumption that all agents are homogeneous.
Later, this approximation result was extended to heterogeneous agents [Mondal et al., 2022]. We would like to clarify
that the idea of heterogeneity is different from the idea of
non-uniformity. In the first case, the agents are divided into
multiple classes. However, the identities of different agents
within a given class are irrelevant. In contrast, non-uniform
interaction takes the identity of each agent into account.
Graphon Approximation: One possible approach to consider non-uniform agent interaction is the notion of Graphon
mean-field, which is recently gaining popularity in the noncooperative MARL setup [Caines and Huang, 2019, Cui
and Koeppl, 2021]. The main idea is to approximate the
finite indices of the agents as a continuum of real numbers
and the discrete interaction graph between agents as a continuous, symmetric, measurable function, called graphon, in
the asymptotic limit of infinite population. The unfortunate
consequence of this approximation is that one is left to deal
with an infinite dimensional mean-field distribution. In order
to obtain practical solution from graphon-approximation,
one must therefore discretise the continuum of agent indices
[Cui and Koeppl, 2021], which limits the use of this approximation. Our paper establishes that for affine reward functions, we do not need to go to the complexity of Graphon
approximation.
Applications of MFC: Alongside the theory, MFC has also
become popular as an application tool. It has been used in
ride-sharing [Al-Abbasi et al., 2019], epidemic management
[Watkins et al., 2016], congestion control in road network
[Wang et al., 2020] etc.
Learning Algorithms for MFC: Both model-free [Angiuli
et al., 2022, Gu et al., 2021] and model-based [Pasztor et al.,
2021] Q-learning algorithms have been proposed in the literature to solve uniform MARL via MFC with homogeneous
agents. Recently, [Mondal et al., 2022] proposed a policygradient algorithm for heterogeneous-MFC.

2

W (i, j) = 1/N , âˆ€i, j âˆˆ [N ], then âˆ€i âˆˆ [N ], Âµi,N
= ÂµN
t
t ,
i,N
N
and Î½ t = Î½ t , which forces our framework to collapse
onto that described in the above mentioned papers.

COOPERATIVE MARL WITH
NON-UNIFORM INTERACTION

We consider a system comprising of N interacting agents.
The (finite) state and action spaces of each agent are denoted
as X , and U respectively. Time is assumed to belong to the
discrete set, T , {0, 1, 2, Â· Â· Â· }. The state and action of i-th
agent at time t are symbolized as xit and uit . The empirical
state and action distributions of the population of agents at
N
time t are denoted by ÂµN
t , and Î½ t respectively, and defined
as follows.
N

ÂµN
t (x) ,

1 X
Î´(xit = x), âˆ€x âˆˆ X , âˆ€t âˆˆ T
N i=1

Î½N
t (u) ,

1 X
Î´(uit = u), âˆ€u âˆˆ U, âˆ€t âˆˆ T
N i=1

(1)

N

(2)

where Î´(Â·) is the indicator function.
Each agent, i âˆˆ [N ] , {1, Â· Â· Â· , N } is endowed with a
reward function r and a state transition function P that are of
the following forms: r : X Ã—U Ã—P(X )Ã—P(U) â†’ R and P :
X Ã—U Ã—P(X )Ã—P(U) â†’ P(X ) where P(Â·) is the set of all
Borel probability measures over its argument. In particular,
r, P take the followings as arguments: (a) the state, xit and
action, ait of the corresponding agent and (b) the weighted
state distribution Âµi,N
and the weighted action distribution
t
Î½ i,N
of
the
population
as seen from the perspective of the
t
agent. The terms Âµi,N
and
Î½ i,N
are defined as follows.
t
t
Âµi,N
t (x) ,

N
X

W (i, j)Î´(xjt = x), âˆ€x âˆˆ X , âˆ€t âˆˆ T (3)

At time t âˆˆ T, each agent is also presumed to have a policy
function Ï€t : X Ã— P(X ) â†’ P(U) that maps (xit , Âµi,N
t ) to
a distribution over the action space, U. In simple words, a
policy function Ï€t is a rule that (probabilistically) dictates
what action must be chosen by an agent given its current
state and the mean-distribution of the population as observed
by the agent. Note that the policy function is presumed to be
the same for all the agents as the reward function, r and the
transition function, P is taken to homogeneous across the
population. Homogeneity of r, P is a common assumption
in the mean-field literature [Gu et al., 2021, Vasal et al.,
2021].
For a given set of initial states x0 , {xi0 }iâˆˆN , the value of
the sequence of policies, Ï€ , {Ï€t }tâˆˆT , for the i-th agent is
defined as follows.
h 
i
X
i,N
vi (x0 , Ï€) ,
Î³ t E r xit , uit , Âµi,N
,
Î½
(6)
t
t
tâˆˆT
i,N
where Âµi,N
are defined by (3), (4), respectively, and
t , Î½t

the expectation is computed over all the state-action trajectories generated by the transition function P and the sequence
of policy functions, Ï€. The term, Î³ âˆˆ [0, 1] is called the
time discount factor. We would like to emphasize that the
value function vi is dependent on the interaction matrix W
i,N
(because so are Âµi,N
t , and Î½ t ). However, such dependence
is not explicitly shown to keep the notation uncluttered. The
average value function of the entire population is expressed
as below.
N

j=1

Î½ i,N
t (u) ,

N
X

vMARL (x0 , Ï€) =
W (i, j)Î´(ujt = u), âˆ€u âˆˆ U, âˆ€t âˆˆ T

(4)

j=1

The function W : [N ] Ã— [N ] â†’ [0, 1] dictates the influence
of one agent on another. In particular, W (i, j) specifies
how j-th agent influences i-th agentâ€™s reward and transition
i,N
functions. Observe that, for Âµi,N
to be probability
t , and Î½ t
distributions, W must be right-stochastic i.e.,
N
X

W (i, j) = 1, âˆ€i âˆˆ {1, Â· Â· Â· , N }

(5)

j=1

In summary, the reward received by the i-th agent at time
i,N
t can be expressed as r(xit , uit , Âµi,N
t , Î½ t ). Moreover, the
state of the agent at time t + 1 is decided by the following
i,N
probability law: xit+1 âˆ¼ P (xit , uit , Âµi,N
t , Î½ t ). We would
like to point out that, in contrast to our framework, existing
works assume reward and state transition to be functions
N
of ÂµN
t , Î½ t , thereby making the influence of population to
be identical for every agent [Mondal et al., 2022, Gu et al.,
2021]. If we take the influence function W to be uniform i.e.,

1 X
vi (x0 , Ï€)
N i=1

(7)

The goal of MARL is to maximize vMARL (x0 , .) over all
policy sequences Ï€. Such an optimization is hard to solve
in general, especially for large N .
Before concluding this section, we would like to point out
two important observations that will be extensively used in
many of our forthcoming results.
Remark 1. âˆ€t âˆˆ T, the random variables {uit }iâˆˆ[N ] are
conditionally independent given {xit }iâˆˆ[N ] . In other words,
given current states, each agent chooses its action independent of each other.
Remark 2. âˆ€t âˆˆ T, the random variables {xit+1 }iâˆˆ[N ] are
conditionally independent given {xit }iâˆˆ[N ] , and {uit }iâˆˆ[N ] .
In other words, given current states and actions, the next
state of each agent evolves independent of each other.

3

MEAN-FIELD CONTROL

MFC is an approximation method of N âˆ’agent MARL that
takes away many of the complexities of the later. The main

idea of MFC is to consider an infinite population of homogeneous agents, instead of a finite population as considered in
MARL. The advantage of such presumption is that it allows
us to draw accurate inferences about the whole population
by tracking only a single representative agent. Unfortunately,
as stated before, such approximation method is known to
work [Gu et al., 2021] when the interactions between different agents are uniform, i.e., W (i, j) = 1/N , âˆ€i, j âˆˆ [N ]. In
this article, we shall show that, under certain conditions, we
can show MFC as an approximation of MARL, even with
non-uniform W . Below we describe the MFC method.
As explained above, in MFC, we only need to track a single
representative agent. Let the state and action of the agent at
time t be denoted as xt , and ut respectively. Also, let Âµt ,
Î½ t be the state, and action distributions of the infinite population at time t. The reward and state transition laws of the
representative at time t are denoted as r(xt , ut , Âµt , Î½ t ) and
P (xt , ut , Âµt , Î½ t ), respectively. For a given policy sequence
Ï€ , {Ï€t }tâˆˆT , the action distribution Î½ t can be expressed
as a deterministic function of the state distribution, Âµt as
follows.
X
Î½ t = Î½ MF (Âµt , Ï€t ) ,
Ï€t (x, Âµt )Âµt (x)
(8)
xâˆˆX

In a similar fashion, the state distribution at time t + 1 can
be written as a deterministic function of Âµt as follows.
Âµt+1 = P MF (Âµt , Ï€t )
XX
,
P (x, u, Âµt , Î½ MF (x, Âµt ))

(9)

xâˆˆX uâˆˆU

Ã— Ï€t (x, Âµt )(u)Âµt (x)
For an initial state distribution Âµ0 , the value of a sequence
of policies Ï€ , {Ï€t }tâˆˆT , is defined as written below.
X
vMF (Âµ0 , Ï€) ,
Î³ t rMF (Âµt , Ï€t ) ,

under certain conditions, vMARL is well-approximated by
vMF . Therefore, in order to solve MARL, it is sufficient to
solve its associated MFC.
It is worthwhile to point out that Âµt , Î½ t can be thought of as
N
limiting values of the empirical distributions ÂµN
t , Î½ t in the
N
asymptotic limit of infinite population. Note that, ÂµN
t , Î½t
and thereby, Âµt , Î½ t are NOT dependent on W . This makes
the MFC problem agnostic of W . In contrary, agents in the
N âˆ’agent MARL problem are influenced by the weighted
i,N
mean-field distribution {Âµi,N
t , Î½ t }iâˆˆ[N ] which do depend
on W via (3), (4). Therefore, unlike in the existing works,
the mean-field representative in our case cannot be described
as a randomly chosen typical agent in the limit N â†’ âˆž.
The concept of mean-field representative, in our work, is a
useful construct that, under certain conditions, can provide
well-approximated solution to MARL.
In the next section, we describe how these seemingly incompatible frameworks, namely non-uniform MARL where the
behaviour of agents are dependent on W , and the framework
of W -agnostic MFC, can be merged together.

4

MFC AS AN APPROXIMATION TO
NON-UNIFORM MARL

Before formally stating our main result, we would like to
describe the assumptions that the result is grounded upon.
Our first assumption is on the structure of state-transition
function.
Assumption 1. The state-transition function P is Lipschitz
continuous with parameter LP with respect to the meandistribution arguments. Mathematically, the inequality,
|P (x, u,Âµ1 , Î½ 1 ) âˆ’ P (x, u, Âµ2 , Î½ 2 )|1
â‰¤ LP [|Âµ1 âˆ’ Âµ2 |1 + |Î½ 1 âˆ’ Î½ 2 |1 ]
holds âˆ€x âˆˆ X , âˆ€u âˆˆ U, âˆ€Âµ1 , Âµ2 âˆˆ P(X ) and âˆ€Î½ 1 , Î½ 2 âˆˆ
P(U). The symbol | Â· |1 denotes L1 norm.

tâˆˆT

where r

MF

(Âµt , Ï€t ) ,

XX

r(x, u, Âµt , Î½ MF (Âµt , Ï€t ))

xâˆˆX uâˆˆU

Ã— Ï€t (x, Âµt )(u)Âµt (x)
(10)
The term rMF (Âµt , Ï€t ) indicates the average reward of the
population. Alternatively, it can also be expressed as the ensemble average of the reward of the representative agent i.e.,
rMF (Âµt , Ï€t ) = E[r(xt , ut , Âµt , Î½ t )] where the expectation
is computed over all possible states xt âˆ¼ Âµt , and actions
ut âˆ¼ Ï€t (xt , Âµt ) at time t. The mean distributions Âµt , Î½ t are
sequentially determined by (8), (9) from a given initial state
distribution, Âµ0 .
The goal of MFC is to maximize vMF (Âµ0 , Â·) over all policy
sequences. In the next section, we shall demonstrate that,

Assumption 1 states that the transition function, P , is Lipschitz continuous with respect to its mean-field arguments.
Essentially, this implies that if the state-distribution changes
from Âµ to Âµ + âˆ†Âµ, then the corresponding change in the
transition-function can be bounded by a term proportional to
|âˆ†Âµ|1 . Similar property holds for the change in the actiondistribution. This useful assumption commonly appears in
the mean-field literature [Gu et al., 2021, Mondal et al.,
2022, Carmona et al., 2018].
The second assumption is on the structure of r, the reward
function.
Assumption 2. The reward function, r is affine with respect
to mean-distribution arguments. Mathematically, for some
a âˆˆ R|X | , b âˆˆ R|U | , and f : X Ã— U â†’ R, the equality,
r(x, u, Âµ, Î½) = aT Âµ + bT Î½ + f (x, u)

holds âˆ€x âˆˆ X , âˆ€u âˆˆ U, âˆ€Âµ âˆˆ P(X ), and âˆ€Î½ âˆˆ P(U).
Assumption 2 dictates that the reward is an affine function
of the mean-field distributions. Although this assumption
does not allow us to encapsulate a large variety of reward
functions, we would like to point out that the behaviour of
agents in multitude of social and economic networks can
be modeled via affine rewards (refer the examples given in
[Chen et al., 2021]). We shall provide one explicit example
at the end of this section. We would also like to reiterate that
the benefit of this seemingly restrictive assumption of affine
reward is it allows us to apply the principles of MFC to an
arbitrarily interacting N -agent system which is notoriously
complex to solve in general.
The immediate corollary of Assumption 2 is that the reward
function is bounded and Lipschitz continuous. The formal
proposition is given below.
Corollary 1. If the reward function, r satisfies Assumption
2, then for some MR , LR > 0, the following holds
(a)|r(x, u, Âµ1 , Î½ 1 )| â‰¤ MR ,
(b)|r(x, u, Âµ1 , Î½ 1 ) âˆ’ r(x, u, Âµ2 , Î½ 2 )|
â‰¤ LR [|Âµ1 âˆ’ Âµ2 |1 + |Î½ 1 âˆ’ Î½ 2 |1 ]
âˆ€x âˆˆ X , âˆ€u âˆˆ U, âˆ€Âµ1 , Âµ2 âˆˆ P(X ), and âˆ€Î½ 1 , Î½ 2 âˆˆ P(U).

The third assumption concerns the set of allowable policy
functions.
Assumption 3. The set of allowable policy functions, Î , is
such that each of its element is Lipschitz continuous with
respect to its mean-state distribution argument. Mathematically, âˆ€Ï€ âˆˆ Î , the following inequality holds
|Ï€(x, Âµ1 ) âˆ’ Ï€(x, Âµ2 )|1 â‰¤ LQ |Âµ1 âˆ’ Âµ2 |1
for some LQ > 0 and âˆ€x âˆˆ X , âˆ€Âµ1 , Âµ2 âˆˆ P(X ).
Assumption 3 states that the allowable policy functions must
be Lipschitz continuous with respect to its state-distribution
argument. Such requirement typically holds for neural network based policies and are commonly presumed to be true
in the literature [Gu et al., 2021, Cui and Koeppl, 2021,
Pasztor et al., 2021].
The final assumption imposes some constraints on the interaction function, W .
Assumption 4. The interaction function, W is such that,
N
X

W (i, j) = 1, âˆ€j âˆˆ {1, Â· Â· Â· , N }

(11)

i=1

In conjunction with (5), this assumption implies that W is
doubly-stochastic.

Assumption 4 requires W to be an N Ã— N doubly stochastic
matrix (DSM). Such presumption is commonly applied in
many multi-agent tasks, e.g., distributed consensus [Alaviani and Elia, 2019a], distributed optimization [Alaviani
and Elia, 2019b], and multi-agent learning [Wai et al., 2018].
We now state our main result.
Theorem 1. Let, x0 , {xi0 }iâˆˆ[N ] be the initial states in an
N -agent non-uniform MARL problem and Âµ0 be its associated empirical distribution defined by (1). Assume Î  to be
a set of policies that obeys Assumption 3, and Ï€ , {Ï€t }tâˆˆT
is a sequence of policies such that Ï€t âˆˆ Î , âˆ€t âˆˆ T. If
Assumption 1, 2 and 4 hold, then
p
|U| 1
|vMARL (x0 , Ï€) âˆ’ vMF (Âµ0 , Ï€)| â‰¤ CR âˆš
N 1âˆ’Î³


h
i
p
p
1
SR CP
1
1
+âˆš
|X | + |U|
âˆ’
SP âˆ’ 1 1 âˆ’ Î³SP
1âˆ’Î³
N
(12)
whenever Î³SP < 1 where SP , (1 + LQ ) + LP (2 + LQ ),
SR , MR (1+LQ )+LR (2+LQ ), CP , 2+LP , and CR ,
|b|1 + MF . The parameters LP , b, LQ , LR , MR have been
defined in Assumption 1, 2, 3, and Corollary 1, respectively.
The term MF is such that |f (x, u)| â‰¤ MF , âˆ€x âˆˆ X , âˆ€u âˆˆ
U where f is stated in Assumption 2. The functions vMARL ,
and vMF are defined in (7), (10) respectively.
Theorem 1 has an important implication. Specifically, it
states that, if reward and transition functions respectively
are affine and Lipschitz continuous functions of the meandistributions, and the interaction between the agents is described
âˆš by a DSM, then the solution of MFC is at most
O(1/ N ) error away from the solution of the non-uniform
MARL problem. Therefore, the larger the number of agents,
the better is the MFC-based approximation. It also describes
how the approximation error changes with the sizes of the
state, and action spaces. Specifically, if all other
p parameters
p
are kept fixed, then the error increases as O( |X | + |U|).
In other words, if individual state and action spaces are large,
then MFC may not be a good approximation to non-uniform
MARL.
Now we shall discuss one example where the reward, transition function and the interaction function satisfy Assumption
1, 2, and 4 respectively.
Example 1. A version of this model has been adapted in
[Subramanian and Mahajan, 2019] and [Chen et al., 2021].
Consider a network of N firms operated by a single operator.
All of the firms produce the same product but with varying
quality. A discrete set X , {1, 2, Â· Â· Â· , Q} (state-space) describes the possible levels of quality of the product. At each
time instant, each firm decides whether to invest to improve
the quality of its product which leads to the following action
set: U = {0, 1}. If at time t, the i-th firm decides to invest,

i.e., uit = 1, its current quality, xit , improves according to
the following transition law.
$
!
%
ï£±
i,N
ï£´
ï£²xi + Ï‡ 1 âˆ’ ÂµÌ„t
i
(Q âˆ’ xt ) if uit = 1,
t
Q
xit+1 =
ï£´
ï£³ i
xt
otherwise
where Ï‡ is a uniform random variable between [0, 1], and
ÂµÌ„i,N
is average product quality of its K < N neighbouring
t
firms. The intuition is that improving product quality might
be difficult if the quality maintained in the local economy is
high. Formally, we assume that each firm equally influences
and is influenced by K other firms. Hence, W (i, j) = 1/K
for all i, j âˆˆ [N ] that influence each other and W (i, j) = 0
otherwise. The
product quality is computed
P local average
i,N
,
xÂµ
(x)
as, ÂµÌ„i,N
where
Âµi,N
is given in (3). At
t
t
t
xâˆˆX
time t, the i-th firm earns a positive reward, Î±R xit due to
its revenue, a negative reward, Î²R ÂµÌ„i,N
due to the average
t
local quality, and a cost Î»R uit due to investment. Hence, the
total reward can be expressed as follows.
i,N
i,N
i
r(xit , uit , Âµi,N
âˆ’ Î»R uit
t , Î½ t ) = Î±R xt âˆ’ Î²R ÂµÌ„t

Clearly, in this example, Assumption 1, 2, and 4 are satisfied.

5

SOLUTION OF MFC VIA NATURAL
POLICY GRADIENT ALGORITHM

In this section, we develop a Natural Policy Gradient (NPG)
algorithm to solve the MFC problem. By virtue of Theorem
1, it provides an approximate solution to the non-uniform
MARL problem. Recall from section 3 that, in MFC, it is
sufficient to track only one representative agent. At time t,
that agent takes its decision ut based on its own state xt , and
the mean-field state distribution Âµt . Thus, MFC essentially
reduces to a single-agent Markov Decision Problem (MDP)
with extended state space X Ã— P(X ) and action space U.
To solve MFC, it is therefore sufficient to consider only
stationary policies [Puterman, 2014].
Let the set of stationary policies be denoted by Î  and its elements be parameterized by Î¦ âˆˆ Rd . For a given policy Ï€Î¦ âˆˆ
Î , we shall define its sequence as Ï€ Î¦ , {Ï€Î¦ , Ï€Î¦ , Â· Â· Â· }. Let,
QÎ¦ be the Q-function associated with policy Ï€Î¦ . We define
QÎ¦ (x, Âµ, u) for arbitrary x âˆˆ X , Âµ âˆˆ P(X ), and u âˆˆ U,
as follows.
QÎ¦ (x, Âµ, u) ,
"âˆž
#
X
E
Î³ t r(xt , ut , Âµt , Î½ t ) x0 = x, Âµ0 = Âµ, u0 = u
t=0

(13)
4.1

PROOF OUTLINE

In this subsection, we shall provide a brief sketch of the
proof of Theorem 1.
Step 0: The difference between vMARL and vMF is essentially the time-discounted sum of differences between the
average N -agent reward and average mean-field (MF) reward at time t. Our first goal, therefore, is to estimate the
difference between these rewards.
Step 1: Average N -agent reward at t depends on weighted
i,N
empirical distributions {Âµi,N
t }iâˆˆ[N ] , {Î½ t }iâˆˆ[N ] whereas
average MF reward depends on the distributions Âµt , Î½ t . To
estimate their difference, we first compute the difference
between average N -agent reward at t and average MF reward at the same instant generated from the distribution
ÂµN
t . This estimate is provided by Lemma 8 in the Appendix.
Assumption 2 is invoked to establish this result.
Step 2: Next we estimate the difference between the average MF reward generated by ÂµN
t and that generated by Âµt .
Lemma 4 in the Appendix bounds this difference by a term
proportional to |ÂµN
t âˆ’ Âµt |.
Step 3: Using Lemma 3 and 7, we now establish a recursive
relation on |ÂµN
t âˆ’ Âµt |. Via induction, we can now write this
difference as a function of t.
Step 4: Finally, by computing a time-discounted sum of all
the upper bounds described above, we arrive at the desired
result.

where the expectation is over ut+1 âˆ¼ Ï€Î¦ (xt+1 , Âµt+1 ), and
xt+1 âˆ¼ P (xt , ut , Âµt , Î½ t ), âˆ€t âˆˆ T. The mean-field distributions {Âµt+1 , Î½ t }tâˆˆT are updated via deterministic update
equations (8), and (9). We now define the advantage function as follows.
AÎ¦ (x, Âµ, u) , QÎ¦ (x, Âµ, u) âˆ’ E[QÎ¦ (x, Âµ, u)]

(14)

where the expectation is over u âˆ¼ Ï€Î¦ (x, Âµ).
âˆ—
Let, vMF
(Âµ0 ) = supÎ¦âˆˆRd vMF (Âµ0 , Ï€ Î¦ ) where vMF is the
value function of MFC problem and is defined in (10). Let,
{Î¦j }Jj=1 be a sequence of parameters that are generated by
the NPG algorithm [Liu et al., 2020, Agarwal et al., 2021]
as follows.

Î¦j+1 = Î¦j + Î·wj , wj , arg minwâˆˆRd LÎ¶ Î¦j (w, Î¦j )
Âµ0

(15)
The term Î· is defined as the learning parameter. The function
Î¦
LÎ¶ Î¦j and the distribution Î¶Âµ0j are defined below.
Âµ0

LÎ¶ÂµÎ¦0 (w, Î¦) , E(x,Âµ,u)âˆ¼Î¶ÂµÎ¦0

h

AÎ¦ (x, Âµ, u)
2 i
âˆ’ (1 âˆ’ Î³)wT âˆ‡Î¦ log Ï€Î¦ (x, Âµ)(u)
,
0

0
Î¶ÂµÎ¦0 (x, Âµ, u) ,

0

âˆž
X

Î³ Ï„ P(xÏ„ = x, ÂµÏ„ = Âµ, uÏ„ = u|

Ï„ =0

x0 = x, Âµ0 = Âµ, u0 = u, Ï€ Î¦0 )(1 âˆ’ Î³)

(16)

(17)

NPG update (15) indicates that, at each iteration, one must
solve another minimization problem to obtain the gradient
direction. It can be solved by applying a stochastic gradient
descent (SGD) approach. In particular, the update equation,
in this case, turns out to be the following: wj,l+1 = wj,l âˆ’
Î±hj,l [Liu et al., 2020]. The term Î± is the learning rate for
this sub-problem. The update direction hj,l can be defined
as follows.
hj,l ,
âˆ’

(18)

1
AÌ‚Î¦ (x, Âµ, u) âˆ‡Î¦j log Ï€Î¦j (x, Âµ)(u)
1âˆ’Î³ j
Î¦

where (x, Âµ, u) âˆ¼ Î¶Âµ0j , and AÌ‚Î¦j is a unbiased estimator of
AÎ¦j . The process to obtain the samples and the estimator has
been detailed in Algorithm 2 in the Appendix I. We would
like to point out that Algorithm 2 is based on Algorithm 3
of [Agarwal et al., 2021]. We summarize the whole NPG
process in Algorithm 1.
Algorithm 1 Natural Policy Gradient
Input: Î·, Î±: Learning rates, J, L: Number of execution steps
w0 , Î¦0 : Initial parameters, Âµ0 : Initial state distribution
Initialization: Î¦ â† Î¦0
1: for j âˆˆ {0, 1, Â· Â· Â· , J âˆ’ 1} do
2:
wj,0 â† w0
3:
for l âˆˆ {0, 1, Â· Â· Â· , L âˆ’ 1} do
Î¦
4:
Sample (x, Âµ, u) âˆ¼ Î¶Âµ0j and AÌ‚Î¦j (x, Âµ, u) using
Algorithm 2
5:
Compute hj,l using (18)
wj,l+1 â† wj,l âˆ’ Î±hj,l
6:
end for
1 PL
7:
wj â†
wj,l
L l=1
8:
Î¦j+1 â† Î¦j + Î·wj
9: end for
Output: {Î¦1 , Â· Â· Â· , Î¦J }: Policy parameters
The global converge of NPG is stated in Lemma 1 which is
a direct consequence of Theorem 4.9 of [Liu et al., 2020].
However, the following assumptions are needed to establish
the Lemma. These are similar to Assumptions 2.1, 4.2, 4.4
respectively in [Liu et al., 2020].
Assumption 5. âˆ€Î¦ âˆˆ Rd , âˆ€Âµ0 âˆˆ P(X ), for some Ï‡ > 0,
FÂµ0 (Î¦) âˆ’ Ï‡Id is positive semi-definite where FÂµ0 (Î¦) can
be expressed as follows.
h
FÂµ0 (Î¦) , E(x,Âµ,u)âˆ¼Î¶ÂµÎ¦ {âˆ‡Î¦ Ï€Î¦ (x, Âµ)(u)}
0
i
T
Ã— {âˆ‡Î¦ log Ï€Î¦ (x, Âµ)(u)}
Assumption 6. âˆ€Î¦ âˆˆ Rd , âˆ€Âµ âˆˆ P(X ), âˆ€x âˆˆ X , âˆ€u âˆˆ U,
|âˆ‡Î¦ log Ï€Î¦ (x, Âµ)(u)|1 â‰¤ G

Assumption 7. âˆ€Î¦1 , Î¦2 âˆˆ Rd , âˆ€Âµ âˆˆ P(X ), âˆ€x âˆˆ X ,
âˆ€u âˆˆ U,
|âˆ‡Î¦1 log Ï€Î¦1 (x, Âµ)(u) âˆ’ âˆ‡Î¦2 log Ï€Î¦2 (x, Âµ)(u)|1
â‰¤ M |Î¦1 âˆ’ Î¦2 |1
for some positive constant M .

T
wj,l
âˆ‡Î¦j log Ï€Î¦j (x, Âµ)(u)

!

for some positive constant G.

Assumption 8. âˆ€Î¦ âˆˆ Rd , âˆ€Âµ0 âˆˆ P(X ),
âˆ—
âˆ—
LÎ¶ÂµÎ¦âˆ— (wÎ¦
, Î¦) â‰¤ bias , wÎ¦
, arg minwâˆˆRd LÎ¶ÂµÎ¦ (w, Î¦)
0

0

where Î¦âˆ— is the parameter of the optimal policy.
Lemma 1. Let {Î¦j }Jj=1 be the sequence of policy parameters obtained from Algorithm 1. If Assumptions 5âˆ’8 hold,
then the following inequality holds for some Î·, Î±, J, L,
1
âˆ—
vMF
(Âµ0 ) âˆ’

J
X

J j=1

âˆš
vMF (Âµ0 , Ï€Î¦j ) â‰¤

bias
+ ,
1âˆ’Î³

for arbitrary initial parameter Î¦0 and initial state distribution Âµ0 âˆˆ P(X ). The parameter bias is a constant. The
sample complexity of Algorithm 1 is O(âˆ’3 ).
The bias bias turns out to be small for rich neural network
based policies [Liu et al., 2020]. Intuitively, it indicates the
expressive power of the policy class, Î .
Lemma 1 establishes that Algorithm 1 can approximate the
optimal mean-field value function with an error bound of ,
and a sample complexity of O(âˆ’3 ). Using Theorem 1, we
can now state the following result.
Theorem 2. Let x0 , {xi0 }iâˆˆ[N ] be the initial states in
an N -agent system and Âµ0 their associated empirical distribution. Assume that {Î¦j }Jj=1 are the policy parameters
generated from Algorithm 1, and the set of policies, Î  satisfies Assumption 3. If Assumptions 1, 2, 4, 5 - 8 are satisfied,
then, any  > 0, the following inequality holds for certain
choices of Î·, Î±, J, L
J

1X
vMF (Âµ0 , Ï€Î¦j )
J j=1
Î¦âˆˆRd
âˆš
bias
â‰¤
+ C max{e, }
1âˆ’Î³
p i
1 hp
where e , âˆš
|X | + |U|
N
(19)
sup vMARL (x0 , Ï€Î¦ ) âˆ’

whenever Î³SP < 1 where SP is given in Theorem 1. The
term, C is a constant and the parameter bias is defined in
Lemma 1. The sample complexity of the process is O(âˆ’3 ).

Proof. Note that following inequality,
J

sup vMARL (x0 , Ï€Î¦ ) âˆ’
Î¦âˆˆRd

1X
vMF (Âµ0 , Ï€Î¦j )
J j=1

âˆ—
(Âµ0 )
â‰¤ sup vMARL (x0 , Ï€Î¦ ) âˆ’ vMF
Î¦âˆˆRd

J

âˆ—
(Âµ0 ) âˆ’
+ vMF

1X
vMF (Âµ0 , Ï€Î¦j )
J j=1

Using Theorem 1, the first term can be bounded by C 0 e
for some constant C 0 . The second term can be bounded by
âˆš
bias /(1 âˆ’ Î³) +  with a sample complexity of O(âˆ’3 )
(Lemma 1). Assigning C = 2 max{C 0 , 1}, we conclude the
result.
Theorem 2 guarantees that Algorithm 1 can yield a policy
such that its associated value is O(max{e, }) error away
from the optimal value of the non-uniform MARL problem.
Moreover, it also dictates such a policy can be obtained with
a sample complexity of O(âˆ’3 ).

6

EXPERIMENTS

Let the policy sequence that maximizes the mean-field value
function vMF (Âµ0 , Â·) be denoted as Ï€ âˆ—MF where Âµ0 indicates
the empirical distribution of the initial joint state, xN
0 . We
define the percentage error as follows.
error ,

âˆ—
âˆ—
vMARL (xN
0 , Ï€ MF ) âˆ’ vMF (Âµ0 , Ï€ MF )
Ã— 100%
âˆ—
vMF (Âµ0 , Ï€ MF )
(20)

We can approximately obtain Ï€ âˆ—MF using Algorithm 1. Fig. 1
plots the value of error (defined in (20)) as a function of N
for the reward, transition function, and interaction model described in Example 1. The values of various parameters used
in this numerical experiment are provided in the description
of Fig. 1. Evidently, the error decreases with N . Notice that
the reward function stated in Example 1 (thereby that is used
for generating Fig. 1) is linear in its mean-field distribution
argument. In Fig. 2, we exhibit the error as a function of N
with the following non-linear reward function.
i,N
i,N Ïƒ
i
i
r(xit , uit , Âµi,N
t , Î½ t ) = Î±R xt âˆ’ Î²R (ÂµÌ„t ) âˆ’ Î»R ut
(21)

The term Ïƒ is a measure of non-linearity. All other parameters are same as stated in Example 1. Observe that if Ïƒ = 1,
the reward function stated above turns out to be identical to
the reward function given in Example 1. In Fig. 2a, and 2b
we plot error for Ïƒ = 1.1, 1.2 respectively. In both of these
scenarios, we see the error to be a decreasing function of N .

Figure 1: Percentage error (defined by (20)) as a function
of N . Reward, state transition, and agent interaction matrix
are same as stated in Example 1. The bold line and the halfwidth of the shaded region respectively denote the mean, and
the standard deviation values of the error obtained over 25
random seeds. The values of various system parameters used
in the experiment are as follows: K = 5, Î±R = 1, Î²R =
Î»R = 0.5, and Q = 10. The hyperparameter values used in
Algorithm 1 are as follows: Î± = Î· = 10âˆ’3 , J = L = 102 .
We use a feed forward neural network with a single hidden
layer as the policy approximator.
This indicates that although our MFC-based approximation
results are theoretically proven for affine rewards only, they
empirically hold for non-affine rewards as well.
The codes for generating these results are publicly available
at: https://github.com/washim-uddin-mondal/UAI2022

7

CONCLUSION

In this article, we consider a multi-agent reinforcement learning (MARL) problem where the interaction between agents
is described by a doubly stochastic matrix. We prove that,
if the reward function is affine, one can well-approximate
this non-uniform MARL problem via an associated MeanField Control (MFC) problem. We obtain an upper bound
of the approximation error as a function of the number of
agents, and also propose a natural policy gradient (NPG) algorithm to solve the MFC problem with polynomial sample
complexity. The obvious drawback of our approach is the
restriction on the structure of the reward function. Therefore,
extension of our techniques to non-affine reward functions
is an important future goal.
Acknowledgements
W. U. M., and S. V. U. were partially funded by NSF Grant
No. 1638311 CRISP Type 2/Collaborative Research: Criti-

(a) Ïƒ = 1.1

(b) Ïƒ = 1.2

Figure 2: Percentage error when the reward function is given by (21). All other parameters are same as in Fig. 1.
cal Transitions in the Resilience and Recovery of Interdependent Social and Physical Networks.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav
Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. Journal
of Machine Learning Research, 22(98):1â€“76, 2021.
Abubakr O Al-Abbasi, Arnob Ghosh, and Vaneet Aggarwal. Deeppool: Distributed model-free algorithm for
ride-sharing using deep reinforcement learning. IEEE
Transactions on Intelligent Transportation Systems, 20
(12):4714â€“4727, 2019.
S Sh Alaviani and Nicola Elia. Distributed average consensus over random networks. In 2019 American Control
Conference (ACC), pages 1854â€“1859. IEEE, 2019a.
Seyyed Shaho Alaviani and Nicola Elia. Distributed
multiagent convex optimization over random digraphs.
IEEE Transactions on Automatic Control, 65(3):986â€“998,
2019b.
Andrea Angiuli, Jean-Pierre Fouque, and Mathieu LauriÃ¨re.
Unified reinforcement q-learning for mean field game and
control problems. Mathematics of Control, Signals, and
Systems, pages 1â€“55, 2022.
Peter E Caines and Minyi Huang. Graphon mean field
games and the gmfg equations: Îµ-nash equilibria. In 2019
IEEE 58th conference on decision and control (CDC),
pages 286â€“292. IEEE, 2019.
RenÃ© Carmona, FranÃ§ois Delarue, et al. Probabilistic Theory
of Mean Field Games with Applications I-II. Springer,
2018.

Yang Chen, Jiamou Liu, and Bakhadyr Khoussainov. Agentlevel maximum entropy inverse reinforcement learning
for mean field games. arXiv preprint arXiv:2104.14654,
2021.
Kai Cui and Heinz Koeppl. Learning graphon mean field
games and approximate nash equilibria. arXiv preprint
arXiv:2112.01280, 2021.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Meanfield controls with Q-learning for cooperative MARL:
convergence and complexity analysis. SIAM Journal on
Mathematics of Data Science, 3(4):1168â€“1196, 2021.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An
improved analysis of (variance-reduced) policy gradient
and natural policy gradient methods. Advances in Neural
Information Processing Systems, 33:7624â€“7636, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529â€“533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous
methods for deep reinforcement learning. In International conference on machine learning, pages 1928â€“1937.
PMLR, 2016.
Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal,
and Satish V Ukkusuri. On the approximation of cooperative heterogeneous multi-agent reinforcement learning
(marl) using mean field control (mfc). Journal of Machine
Learning Research, 23(129):1â€“46, 2022.

Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient model-based multi-agent mean-field reinforcement
learning. arXiv preprint arXiv:2107.04050, 2021.
Martin L Puterman. Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
2014.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder,
Gregory Farquhar, Jakob Foerster, and Shimon Whiteson.
Qmix: Monotonic value function factorisation for deep
multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295â€“4304. PMLR,
2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon
Whiteson. Weighted qmix: Expanding monotonic value
function factorisation for deep multi-agent reinforcement
learning. Advances in neural information processing
systems, 33:10199â€“10210, 2020.
Gavin A Rummery and Mahesan Niranjan. On-line Qlearning using connectionist systems, volume 37. Citeseer, 1994.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field games. In Proceedings of the 18th International Conference on Autonomous
Agents and MultiAgent Systems, pages 251â€“259, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo,
Karl Tuyls, et al.
Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296, 2017.
Ming Tan. Multi-agent reinforcement learning: Independent
vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330â€“337,
1993.
Deepanshu Vasal, Rajesh Mishra, and Sriram Vishwanath.
Sequential decomposition of graphon mean field games.
In 2021 American Control Conference (ACC), pages 730â€“
736. IEEE, 2021.
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi
Hong. Multi-agent reinforcement learning via double
averaging primal-dual optimization. Advances in Neural
Information Processing Systems, 31, 2018.
Xiaoqiang Wang, Liangjun Ke, Zhimin Qiao, and Xinghua
Chai. Large-scale traffic signal control using a novel
multiagent reinforcement learning. IEEE transactions on
cybernetics, 51(1):174â€“187, 2020.
Christopher JCH Watkins and Peter Dayan. Q-learning.
Machine learning, 8(3):279â€“292, 1992.

Nicholas J Watkins, Cameron Nowzari, Victor M Preciado, and George J Pappas. Optimal resource allocation
for competitive spreading processes on bilayer networks.
IEEE Transactions on Control of Network Systems, 5(1):
298â€“307, 2016.

A

PROOF OF COROLLARY 1

Lemma 4. If rMF (., .) is defined by (10), then âˆ€Âµ1 , Âµ2 âˆˆ
P(X ), âˆ€Ï€ âˆˆ Î , the following inequality holds.

The following inequalities hold âˆ€x âˆˆ X , âˆ€u âˆˆ U, âˆ€Âµ1 âˆˆ
P(X ), and âˆ€Î½ 1 âˆˆ P(U).
|r(x, u, Âµ1 , Î½ 1 )| â‰¤ |aT Âµ1 | + |bT Î½ 1 | + |f (x, u)|
â‰¤ |a|1 |Âµ1 |1 + |b|1 |Î½ 1 |1 + |f (x, u)|
(a)

|rMF (Âµ1 , Ï€) âˆ’ rMF (Âµ2 , Ï€)|1 â‰¤ SR |Âµ1 âˆ’ Âµ2 |1
where SR , MR (1 + LQ ) + LR (2 + LQ ).
The terms MR , LR , and LQ are defined in Corollary 1 and
Assumption 3 respectively.

= |a|1 + |b|1 + |f (x, u)|
B.2

Equality (a) follows from the fact that both Âµ1 and Î½ 1 are
probability distributions. As the sets X , U are finite, there
must exist MF > 0 such that, |f (x, u)| â‰¤ MF , âˆ€x âˆˆ X ,
âˆ€u âˆˆ U. Taking MR = |a|1 + |b|1 + MF , we can establish
proposition (a).
Proposition (b) follows from the fact that âˆ€x âˆˆ X , âˆ€u âˆˆ U,
âˆ€Âµ1 , Âµ2 âˆˆ P(X ), âˆ€Î½ 1 , Î½ 2 âˆˆ P(U), the following relations
hold.
|r(x, u,Âµ1 , Î½ 2 ) âˆ’ r(x, u, Âµ2 , Î½ 2 )|
â‰¤ |aT (Âµ1 âˆ’ Âµ2 )| + |bT (Î½ 1 âˆ’ Î½ 2 )|
â‰¤ |a|1 |Âµ1 âˆ’ Âµ2 |1 + |b|1 |Î½ 1 âˆ’ Î½ 2 |1
Taking LR = max{|a|1 , |b|1 }, we conclude the result.

B

APPROXIMATION RESULTS

The following Lemma 6, 7, 8 establish that the state, action distributions and the average reward of an N -agent
system closely approximate their mean-field counterparts
when N is large. All of these results use Lemma 5 as the
key ingredient.
Lemma 5. [Mondal et al., 2022] Assume that âˆ€m âˆˆ [M ],
{Xm,n }nâˆˆ[N ] are independent random variables that lie
in
P the interval [0, 1], and satisfy the following constraint:
mâˆˆ[M ] E[Xm,n ] = 1, âˆ€n âˆˆ [N ]. If {Cm,n }mâˆˆ[M ],nâˆˆ[N ]
are constants that obey |Cm,n | â‰¤ C, âˆ€m âˆˆ [M ], âˆ€n âˆˆ [N ],
then the following inequality holds.
X
âˆš
E Cm,n (Xm,n âˆ’ E[Xm,n ]) â‰¤ C M N
mâˆˆ[M ]

PROOF OF THEOREM 1

The following results are necessary to establish the theorem.
B.1

LIPSCHITZ CONTINUITY

In the following three lemmas, we shall establish that the
functions, Î½ MF , P MF and rMF defined in (8), (9) and (10)
are Lipschitz continuous. In all of these lemmas, the term Î 
denotes the set of policies that satisfies Assumption 3. The
proofs of these lemmas are delegated to Appendix C, D, and
E respectively.
Lemma 2. If Î½ MF (., .) is defined by (8), then âˆ€Âµ1 , Âµ2 âˆˆ
P(X ), âˆ€Ï€ âˆˆ Î , the following inequality holds.
|Î½ MF (Âµ1 , Ï€) âˆ’ Î½ MF (Âµ2 , Ï€)|1 â‰¤ (1 + LQ )|Âµ1 âˆ’ Âµ2 |1
where LQ is defined in Assumption 3.
Lemma 3. If P MF (., .) is defined by (9), then âˆ€Âµ1 , Âµ2 âˆˆ
P(X ), âˆ€Ï€ âˆˆ Î , the following inequality holds.
|P MF (Âµ1 , Ï€) âˆ’ P MF (Âµ2 , Ï€)|1 â‰¤ SP |Âµ1 âˆ’ Âµ2 |1
where SP , (1 + LQ ) + LP (2 + LQ ).
The terms LP , and LQ are defined in Assumption 1, and 3
respectively.

The proofs of Lemma 6, 7, and 8 have been delegated to
Appendix F, G, and H respectively.
N
Lemma 6. Assume {ÂµN
t , Î½ t }tâˆˆT are empirical state and
action distributions of an N -agent system defined by (1),
and (2) respectively. If these distributions are generated
by a sequence of policies Ï€ = {Ï€t }tâˆˆT , then âˆ€t âˆˆ T the
following inequality holds.
p
|U|
N
MF
N
E|Î½ t âˆ’ Î½ (Âµt , Ï€t )|1 â‰¤ âˆš
N

where Î½ MF is defined in (8).
N
Lemma 7. Assume {ÂµN
t , Î½ t }tâˆˆT are empirical state and
action distributions of an N -agent system defined by (1),
and (2) respectively. If these distributions are generated
by a sequence of policies Ï€ = {Ï€t }tâˆˆT , then âˆ€t âˆˆ T the
following inequality holds.

p i
CP hp
MF
E|ÂµN
(ÂµN
|X | + |U|
t+1 âˆ’ P
t , Ï€t )|1 â‰¤ âˆš
N
where P MF is defined in (9), CP , 2 + LP , and LP is
given in Assumption 1.
N
Lemma 8. Assume {ÂµN
t , Î½ t }tâˆˆT are empirical state and
action distributions of an N -agent system defined by (1),
i,N
and (2) respectively. Also, âˆ€i âˆˆ [N ], let {Âµi,N
t , Î½ t } be

weighted state and action distributions defined by (3), (4).
If these distributions are generated by a sequence of policies
Ï€ = {Ï€t }tâˆˆT , then âˆ€t âˆˆ T the following inequality holds.
N

1 X
i,N
MF
E
r(xit , uit , Âµi,N
(ÂµN
t , Î½t ) âˆ’ r
t , Ï€t )
N i=1
p
|U|
â‰¤ CR âˆš
N
where rMF is given in (10), CR , |b|1 + MF and MF is
such that |f (x, u)| â‰¤ MF , âˆ€x âˆˆ X , âˆ€u âˆˆ U. The function
f (., .) and the parameter b are defined in Assumption 2. We
would like to mention that MF always exists since X , U are
finite.
B.3

PROOF OF THE THEOREM

Inequality (a) follows from Lemma 7 and Eq. (9) while (b)
is a result of Lemma 3. Finally, inequality (c) can be derived
by recursively applying (b). Therefore, the term J2 can be
upper bounded as follows.


p i SR CP
1 hp
1
1
J2 â‰¤ âˆš
|X | + |U|
âˆ’
SP âˆ’ 1 1 âˆ’ Î³SP
1âˆ’Î³
N
This concludes the theorem.

C

PROOF OF LEMMA 2

The following inequalities hold true.
|Î½ MF (Âµ1 , Ï€) âˆ’ Î½ MF (Âµ2 , Ï€)|1
X

=

Note that,

xâˆˆX

|vMARL (x0 , Ï€) âˆ’ vMF (Âµ0 , Ï€)|
(a)

=

âˆ’

Ï€(x, Âµ1 )Âµ1 (x) âˆ’

âˆž
N
X
1 X

N

=

t=0
i=1
âˆž
X
t MF

Î³ r

X X

â‰¤

Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’

X X

+

X

Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’

X

âˆž
X

+

Ï€(x, Âµ2 )(u)Âµ1 (x)

xâˆˆX

X

Ï€(x, Âµ2 )(u)Âµ1 (x) âˆ’

uâˆˆU xâˆˆX

â‰¤

Ï€(x, Âµ2 )(u)Âµ2 (x)

xâˆˆX

uâˆˆU xâˆˆX

t=0

N
1 X
i,N
MF
J1 ,
Î³tE
[r(xit , uit , Âµi,N
(ÂµN
t , Î½ t )] âˆ’ r
t , Ï€t )
N
t=0
i=1
p
(a)
|U| 1
â‰¤ CR âˆš
N 1âˆ’Î³

1

X

uâˆˆU xâˆˆX

(Âµt , Ï€t ) â‰¤ J1 + J2

Equality (a) directly follows from the definitions (7) and
(10). The first term J1 can be written as follows.

Ï€(x, Âµ2 )Âµ2 (x)

xâˆˆX

X X

i,N
Î³ t E[r(xit , uit , Âµi,N
t , Î½ t )]

X

Ï€(x, Âµ2 )(u)Âµ2 (x)

xâˆˆX

Âµ1 (x)

X

|Ï€(x, Âµ1 )(u) âˆ’ Ï€(x, Âµ2 )(u)|

xâˆˆX

uâˆˆU

X

|Âµ1 (x) âˆ’ Âµ2 (x)|

xâˆˆX

X

Ï€(x, Âµ2 )(u)

uâˆˆU

(a)

â‰¤ LQ |Âµ1 âˆ’ Âµ2 |1

X

Âµ1 (x) + |Âµ1 âˆ’ Âµ2 |1

xâˆˆX

Equation (a) is a result of Lemma 8. The second term can
be expressed as follows.
J2 ,
(a)

âˆž
X

MF
Î³ t E|rMF (ÂµN
(Âµt , Ï€t )|
t , Ï€t ) âˆ’ r

t=0
âˆž
X

â‰¤ SR

Î³ t |ÂµN
t âˆ’ Âµt |1

t=0

(b)

= (1 + LQ )|Âµ1 âˆ’ Âµ2 |1

Inequality (a) is a consequence of the fact that Ï€ âˆˆ Î  and
Ï€(x, Âµ2 ) is a distribution. Finally, the equality (b) follows
because Âµ1 is a distribution. This concludes the result.

D

PROOF OF LEMMA 3

Inequality (a) follows from Lemma 4. Observe that, âˆ€t âˆˆ T,
|ÂµN
t+1 âˆ’ Âµt+1 |1
MF
MF
â‰¤ |ÂµN
(ÂµN
(ÂµN
t+1 âˆ’ P
t , Ï€t )|1 + |P
t , Ï€t ) âˆ’ Âµt+1 |1
h
i
(a) C
p
p
P
â‰¤ âˆš
|X | + |U|
N
MF
+ |P MF (ÂµN
(Âµt , Ï€t )|1
t , Ï€t ) âˆ’ P
h
i
(b) C
p
p
P
â‰¤ âˆš
|X | + |U| + SP |ÂµN
t âˆ’ Âµt |1
N
hp
(c) C
p i (S t+1 âˆ’ 1)
P
P
â‰¤ âˆš
|X | + |U|
SP âˆ’ 1
N

Note the following inequalities.
|P MF (Âµ1 , Ï€) âˆ’ P MF (Âµ2 , Ï€)|1
=

XX

P (x, u, Âµ1 , Î½ MF (Âµ1 , Ï€))Ï€(x, Âµ1 )(u)Âµ1 (x)

xâˆˆX uâˆˆU

âˆ’

XX

P (x, u, Âµ2 , Î½ MF (Âµ2 , Ï€))Ï€(x, Âµ2 )(u)Âµ2 (x)

xâˆˆX uâˆˆU

â‰¤ J1 + J2
where the term J1 is as follows.

1

XX

J1 ,

Ï€(x, Âµ1 )(u)Âµ1 (x)

xâˆˆX uâˆˆU

â‰¤

Ï€(x, Âµ1 )(u)Âµ1 (x)

xâˆˆX uâˆˆU

Ã— P (x, u, Âµ1 , Î½ MF (Âµ1 , Ï€)) âˆ’ P (x, u, Âµ2 , Î½ MF (Âµ2 , Ï€))
(a) X X

XX

J1 ,
1

Ã— r(x, u, Âµ1 , Î½ MF (Âµ1 , Ï€)) âˆ’ r(x, u, Âµ2 , Î½ MF (Âµ2 , Ï€))
(a) X X

Ï€(x, Âµ1 )(u)Âµ1 (x)

â‰¤

xâˆˆX uâˆˆU

Ï€(x, Âµ1 )(u)Âµ1 (x)

xâˆˆX uâˆˆU

n
o
Ã— LP |Âµ1 âˆ’ Âµ2 |1 + |Î½ MF (Âµ1 , Ï€) âˆ’ Î½ MF (Âµ2 , Ï€)|1

n
o
Ã— LR |Âµ1 âˆ’ Âµ2 |1 + |Î½ MF (Âµ1 , Ï€) âˆ’ Î½ MF (Âµ2 , Ï€)|1

(b)

(b)

â‰¤ LP (2 + LQ )|Âµ1 âˆ’ Âµ2 |1

â‰¤ LR (2 + LQ )|Âµ1 âˆ’ Âµ2 |1

Inequality (a) follows from Assumption 1 whereas (b) uses
Lemma 2 and the fact that Âµ1 , Ï€(x, Âµ1 ) are distributions.
The term J2 is given as follows.
XX

J2 ,

P (x, u, Âµ2 , Î½ MF (Âµ2 , Ï€)) 1

J2 ,

xâˆˆX uâˆˆU

Ã— Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’ Ï€(x, Âµ2 )(u)Âµ2 (x)
(a) X X
=
Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’ Ï€(x, Âµ2 )(u)Âµ2 (x)
xâˆˆX uâˆˆU

X

â‰¤

Âµ1 (x)

xâˆˆX

X

+

X

|Ï€(x, Âµ1 )(u) âˆ’ Ï€(x, Âµ2 )(u)|

|Âµ1 (x) âˆ’ Âµ2 (x)|

xâˆˆX

â‰¤ LQ |Âµ1 âˆ’ Âµ2 |1

X

r(x, u, Âµ2 , Î½ MF (Âµ2 , Ï€))

xâˆˆX uâˆˆU

Ã— Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’ Ï€(x, Âµ2 )(u)Âµ2 (x)
(a)

XX

â‰¤ MR
â‰¤ MR

Ï€(x, Âµ2 )(u)
+ MR

uâˆˆU

(b)

XX

X

Âµ1 (x)

Âµ1 (x) + |Âµ1 âˆ’ Âµ2 |1

uâˆˆU

X

|Âµ1 (x) âˆ’ Âµ2 (x)|

X

Ï€(x, Âµ2 )(u)

uâˆˆU

(b)

â‰¤ MR LQ |Âµ1 âˆ’ Âµ2 |1

X

Âµ1 (x) + MR |Âµ1 âˆ’ Âµ2 |1

xâˆˆX

= (1 + LQ )|Âµ1 âˆ’ Âµ2 |1

(c)

= MR (1 + LQ )|Âµ1 âˆ’ Âµ2 |1

Equality (a) uses the fact that P (x, u, Âµ2 , Î½ MF (Âµ2 , Ï€)) is
a distribution. Inequality (b) follows from Assumption 3
while equation (c) holds because Âµ1 is a distribution.

Inequality (a) uses Corollary 1(a). Inequality (b) follows
from Assumption 3 while equation (c) holds because Âµ1 is
a distribution. This concludes the lemma.

PROOF OF LEMMA 4
F

The following inequalities hold true.

XX

r(x, u, Âµ1 , Î½ MF (Âµ1 , Ï€))Ï€(x, Âµ1 )(u)Âµ1 (x)

xâˆˆX uâˆˆU

XX

r(x, u, Âµ2 , Î½ MF (Âµ2 , Ï€))Ï€(x, Âµ2 )(u)Âµ2 (x)

xâˆˆX uâˆˆU

â‰¤ J1 + J2

PROOF OF LEMMA 6

MF
Applying the definitions of Î½ N
, we can write the
t and Î½
following.

|rMF (Âµ1 , Ï€) âˆ’ rMF (Âµ2 , Ï€)|1

âˆ’

|Ï€(x, Âµ1 )(u) âˆ’ Ï€(x, Âµ2 )(u)|

xâˆˆX

(c)

=

X

xâˆˆX

xâˆˆX

E

Ï€(x, Âµ1 )(u)Âµ1 (x) âˆ’ Ï€(x, Âµ2 )(u)Âµ2 (x)

xâˆˆX uâˆˆU

uâˆˆU

X

Inequality (a) follows from Corollary 1(b) whereas (b) uses
Lemma 2 and the fact that Âµ1 , Ï€(x, Âµ1 ) are distributions.
The term J2 is given as follows.

MF
E|Î½ N
(ÂµN
t âˆ’Î½
t , Ï€t )|1
X
N
=
E|Î½ t (u) âˆ’ Î½ MF (ÂµN
t , Ï€t )(u)|
uâˆˆU

1

N

=

X
uâˆˆU

where the term J1 is given as follows.

E

X
1 X
N
Î´(uit = u) âˆ’
Ï€t (x, ÂµN
t )(u)Âµt (x)
N i=1
xâˆˆX

(22)

Similarly, using the definition of ÂµN
t , we get,
X

Using the definition of L1 norm, we can write the following.
MF
E ÂµN
(ÂµN
t+1 âˆ’ P
t , Ï€t ) 1
X
MF
=
E ÂµN
(ÂµN
t+1 (x) âˆ’ P
t , Ï€t )(x) 1

N
Ï€t (x, ÂµN
t )(u)Âµt (x)

xâˆˆX

=

X

Ï€t (x, ÂµN
t )(u)

xâˆˆX
N

=

xâˆˆX

1 X
Î´(xit = x)
N i=1

1 XX
j
Ï€t (x, ÂµN
t )(u)Î´(xt = x)
N i=1

(23)

xâˆˆX

N
1 X X
=
E
Î´(xit+1 = x)
N
i=1
xâˆˆX

âˆ’

MF
i
N
P (xit , u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)Ï€t (xt , Âµt )(u)

i=1 uâˆˆU

N

1 X
=
Ï€t (xjt , ÂµN
t )
N i=1

N X
X

â‰¤ J1 + J2 + J3
The first term, J1 is given as follows.

Substituting into (22), we obtain the following.

N
1 X X
N
Î´(xjt+1 = x) âˆ’ P (xit , uit , ÂµN
E
t , Î½ t )(x)
N
i=1
xâˆˆX
p
(a)
|X |
â‰¤ âˆš
N

J1 ,

MF
E|Î½ N
(ÂµN
t âˆ’Î½
t , Ï€t )|1
N
1 X X
Î´(ujt = u) âˆ’ Ï€t (xit , ÂµN
E
t )(u)
N
i=1
uâˆˆU
p
(a)
|U|
â‰¤ âˆš
N

=

Inequality (a) is a consequence of Lemma 5. Particularly,
we use the fact that âˆ€u âˆˆ U, the random variables {Î´(uit =
u)}iâˆˆ[N ] lie in [0, 1], are conditionally independent given
xt , {xit }iâˆˆ[N ] (thereby given ÂµN
t ), and satisfy the following constraints.

Inequality (a) follows from Lemma 5. Specifically, we use
the fact that, âˆ€x âˆˆ X , the random variables {Î´(xit+1 =
x)}iâˆˆ[N ] lie in [0, 1], are conditionally independent given
N
xt , {xit }iâˆˆ[N ] , ut , {uit }iâˆˆ[N ] , (thereby given ÂµN
t , Î½t )
and satisfy the following.
N
E[Î´(xit+1 = x)|xt , ut ] = P (xit , uit , ÂµN
t , Î½ t ),

X

E[Î´(xit+1 = x)|xt , ut ] = 1, âˆ€i âˆˆ [N ]

xâˆˆX

The second term J2 can be expressed as follows.



E Î´(uit = u)|xt = Ï€t (xit , ÂµN
t )
X 

E Î´(uit = u)|xt = 1, âˆ€i âˆˆ [N ]

J2 ,

N
1 X X
N
P (xit , uit , ÂµN
E
t , Î½ t )(x)
N
i=1
xâˆˆX

MF
âˆ’ P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)

uâˆˆU

N

â‰¤

G

PROOF OF LEMMA 7

Using the definition of P

MF

MF
N
N
P (x, u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (x, Âµt )(u)Âµt (x)

N i=1

uâˆˆU

|U|
âˆš
N

J3 ,

N
1 X X
MF
E
P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)
N
i=1
xâˆˆX

Ã—

=

p

Inequality (a) follows from Assumption 1 whereas (b) results from Lemma 6. Finally, the term J3 is defined as follows.

xâˆˆX uâˆˆU

N
1 XX

1

(b)

MF
â‰¤ LP E|Î½ N
(ÂµN
t âˆ’Î½
t , Ï€t )| â‰¤ LP

, we get the following.

xâˆˆX uâˆˆU

XX

MF
âˆ’ P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))
(a)

P MF (ÂµN
t , Ï€t )
XX
MF
N
N
=
P (x, u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (x, Âµt )(u)Âµt (x)
=

1 X
N
E P (xit , uit , ÂµN
t , Î½t )
N i=1

N
1 X

N i=1

Î´(xit = x)

MF
i
N
P (xit , u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (xt , Âµt )(u)

âˆ’

N X
X
i=1 uâˆˆU

p
|X |
â‰¤ âˆš
N

(a)

MF
i
N
P (xit , u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)Ï€t (xt , Âµt )(u)

Relation (a) results from Lemma 5. In particular, we use the
MF
fact that âˆ€x âˆˆ X , {P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)}iâˆˆ[N ]
lie in the interval [0, 1], are conditionally independent given
xt , {xit }iâˆˆ[N ] (therefore, given ÂµN
t ), and satisfy the following constraints.

Now the first term can be simplified as follows.
N X
N
X
1 X
a(x)
W (i, j)Î´(xjt = x)
N
i=1 j=1
xâˆˆX

MF
E[P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)|xt ]
X
MF
i
N
=
P (xit , u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)Ï€t (xt , Âµt )(u),

=

xâˆˆX

uâˆˆU

and

N
N
X
X
1 X
a(x)
Î´(xjt = x)
W (i, j)
N
j=1
i=1

(a) X

X

=

MF
E[P (xit , uit , ÂµN
(ÂµN
t ,Î½
t , Ï€t ))(x)|xt ] = 1

N

a(x)

xâˆˆX

1 X
Î´(xjt = x) = aT ÂµN
t
N j=1

xâˆˆX

Equality (a) follows as W is doubly-stochastic (Assumption
4). Similarly, the second term can be simplified as shown
below.

This concludes the Lemma.

H

PROOF OF LEMMA 8

N X
N
X
1 X
b(u)
W (i, j)Î´(ujt = u)
N
i=1 j=1

Note that,
r

MF

uâˆˆU

(ÂµN
t , Ï€t )

XX

=

=

MF
N
N
r(x, u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (x, Âµt )(u)Âµt (x)

N
N
X
X
1 X
b(u)
Î´(ujt = u)
W (i, j)
N
j=1
i=1
uâˆˆU

xâˆˆX uâˆˆU

XX

=

(a) X

MF
N
r(x, u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (x, Âµt )(u)

=

uâˆˆU

xâˆˆX uâˆˆU

Ã—

1
Î´(xit = x)
N i=1

Equality (a) follows from Assumption 4. Therefore, we get,

1 XX
MF
i
N
r(xit , u, ÂµN
(ÂµN
t ,Î½
t , Ï€t ))Ï€t (xt , Âµt )(u)
N
i=1

N

E

uâˆˆU

(a)

=

N
i
1 X Xh T N
i
a Âµt + bT Î½ MF (ÂµN
t , Ï€t ) + f (xt , u)
N
i=1
uâˆˆU

Ã— Ï€t (xit , ÂµN
t )(u)
(b)

T

=a

1 X
Î´(ujt = u) = bT Î½ N
t
N j=1

N
X

N

=

N

b(u)

1 X
i,N
MF
r(xit , uit , Âµi,N
(ÂµN
t , Î½t ) âˆ’ r
t , Ï€t )
N i=1

MF
â‰¤ |b|1 E|Î½ N
(ÂµN
t âˆ’Î½
t , Ï€t )|1
N
N X
X
1 X
i
i
f (xt , ut ) âˆ’
f (xit , u)Ï€t (xit , ÂµN
+ E
t )(u)
N i=1
i=1
uâˆˆU

T MF
ÂµN
(ÂµN
t +b Î½
t , Ï€t )
N

+

1 XX
f (xit , u)Ï€t (xit , ÂµN
t )(u)
N
i=1

Using
p Lemma 6, the first term can be upper bounded by
|b|1 |U|/N . The second term can be bounded as follows.

uâˆˆU

Equality (a) follows from Assumption 2 while (b) uses the
fact that Ï€t (xit , ÂµN
t ) is a distribution. On the other hand,

N
i
1 X h T i,N
i
i
a Âµt + bT Î½ i,N
+
f
(x
,
u
)
t
t
t
N i=1
"
#
N
X
1 X X
i,N
i,N
=
a(x)Âµt (x) +
b(u)Î½ t (u)
N i=1

=

xâˆˆX

+

N i=1

f (xit , uit )

uâˆˆU

uâˆˆU

N
X



1 X
E
f (xit , u) Î´(uit = u) âˆ’ Ï€t (xit , ÂµN
t )(u)
N
i=1
uâˆˆU
p
(a)
|U|
â‰¤ MF âˆš
N
â‰¤

N

1 X
i,N
r(xit , uit , Âµi,N
t , Î½t )
N i=1

N
1 X

N
N X
X
1 X
E
f (xit , uit ) âˆ’
f (xit , u)Ï€t (xit , ÂµN
t )(u)
N i=1
i=1

The term MF > 0 is such that |f (x, u)| â‰¤ MF , âˆ€x âˆˆ X ,
âˆ€u âˆˆ U. Such MF always exists since X , and U are finite.
Equality (a) is a result of Lemma 5. In particular, we use the
following facts to prove this result. The random variables
{Î´(uit = u)}iâˆˆ[N ] are conditionally independent given xt ,
{xit }iâˆˆ[N ] (therefore, given ÂµN
t ), âˆ€u âˆˆ U and they lie in the

interval [0, 1]. Moreover,
|f (xit , u)| â‰¤ MF , âˆ€i âˆˆ [N ], âˆ€u âˆˆ U,
E[Î´(uit = u)|xt ] = Ï€t (xit , ÂµN
t ),
X
E[Î´(uit = u)|xt ] = 1
uâˆˆU

This concludes the Lemma.

I

SAMPLING PROCEDURE

Algorithm 2 Sampling Algorithm
Input: Âµ0 , Ï€ Î¦j , P , r
1: Sample x0 âˆ¼ Âµ0 .
2: Sample u0 âˆ¼ Ï€Î¦j (x0 , Âµ0 )
3: Î½ 0 â† Î½ MF (Âµ0 , Ï€Î¦j ) where Î½ MF is defined in (8).
4: t â† 0
5: FLAG â† FALSE
6: while FLAG is FALSE do
7:
FLAG â† TRUE with probability 1 âˆ’ Î³.
8:
Execute Update
9: end while
10: T â† t
11: Accept (xT , ÂµT , uT ) as a sample.
12: VÌ‚Î¦j â† 0, QÌ‚Î¦j â† 0
13: FLAG â† FALSE
14: SumRewards â† 0
15: while FLAG is FALSE do
16:
FLAG â† TRUE with probability 1 âˆ’ Î³.
17:
Execute Update
18:
SumRewards â† SumRewards + r(xt , ut , Âµt , Î½ t )
19: end while
20: With probability 12 , VÌ‚Î¦j â† SumRewards. Otherwise

QÌ‚Î¦j â† SumRewards.
21: AÌ‚Î¦j (xT , ÂµT , uT ) â† 2(QÌ‚Î¦j âˆ’ VÌ‚Î¦j ).
Output: (xT , ÂµT , uT ) and AÌ‚Î¦j (xT , ÂµT , uT )
Procedure Update:
1: xt+1 âˆ¼ P (xt , ut , Âµt , Î½ t ).
2: Âµt+1 â† P MF (Âµt , Ï€Î¦j ) where P MF is defined in (9).
3: ut+1 âˆ¼ Ï€Î¦j (xt+1 , Âµt+1 )
4: Î½ t+1 â† Î½ MF (Âµt+1 , Ï€Î¦j )
5: t â† t + 1
EndProcedure

